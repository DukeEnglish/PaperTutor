Fundamental Problems With Model Editing:
How Should Rational Belief Revision Work in LLMs?
PeterHase1,† ThomasHofweber2 XiangZhou1,†
EliasStengel-Eskin1 MohitBansal1
1DepartmentofComputerScience,UNCChapelHill
2DepartmentofPhilosophy,UNCChapelHill
peter@cs.unc.edu
Abstract stateoftheworld. Followinginitialstudies(DeCao
etal.,2021;Daietal.,2021;Mitchelletal.,2021;
The model editing problem concerns how Haseetal.,2021),atleast17paperswerepublished
language models should learn new facts on the problem in 2023 alone (Betz and Richard-
about the world over time. While empir-
son, 2023; Pinter and Elhadad, 2023; Hernandez
ical research on model editing has drawn
etal.,2023;Hoelscher-Obermaieretal.,2023;Patil
widespreadattention,theconceptualfounda-
et al., 2023; Yao et al., 2023; Zhong et al., 2023;
tions of model editing remain shaky – per-
Hanetal.,2023;Hartvigsenetal.,2023;Wuetal.,
hapsunsurprisingly,sincemodeleditingis
essentiallybeliefrevision,astoriedproblem 2023;Wangetal.,2023a,b;Weietal.,2023;Gupta
inphilosophythathaseludedsuccinctsolu- etal.,2023;Brownetal.,2023;Onoeetal.,2023;
tionsfordecades. Modeleditingnonetheless Li et al., 2023). Applications of model editing
demandsasolution,sinceweneedtobeable
havefocusedonupdatingmodelswithchangingin-
to control the knowledge within language
formationovertime,unlearningsensitiveinforma-
models. With this goal in mind, this pa-
tion,andfixingindividualfactualmistakes. Indeed,
percritiquesthestandardformulationofthe
modeleditingmethodsnowseemnecessarygiven
model editing problem and proposes a for-
maltestbedformodeleditingresearch. We that interventions in the pretraining or finetuning
firstdescribe12openproblemswithmodel stagesofLLMdevelopmentappearinsufficientfor
editing,basedonchallengeswith(1)defin- solvingtheseproblemsefficiently(Lazaridouetal.,
ingtheproblem,(2)developingbenchmarks, 2021;Dhingraetal.,2022;Debenedettietal.,2023;
and(3)assumingLLMshaveeditablebeliefs
Casperetal.,2023a).
inthefirstplace. Manyofthesechallenges
Yetthemodeleditingproblemstandsonshaky
areextremelydifficulttoaddress,e.g. deter-
miningfar-reachingconsequencesofedits, theoreticalground. Theprincipalreasonforthisis
labeling probabilistic entailments between thatthemodeleditingproblemhasbeenframedas
facts,andupdatingbeliefsofagentsimula- aninstanceofthebeliefrevisionprobleminphilos-
tors. Next, we introduce a semi-synthetic ophy(Hansson,2022). Pastworkpositsthatmodel
datasetformodeleditingbasedonWikidata,
editing shares core goals with belief revision,
where we can evaluate edits against labels
arguingthatLLMsshouldmaintainlogicallycon-
givenbyanidealizedBayesianagent. This
sistentoutputswhenupdatedwithnewinformation
enablesustosayexactlyhowbeliefrevision
(DeCaoetal.,2021;Mitchelletal.,2022;Meng
inlanguagemodelsfallsshortofadesirable
epistemicstandard. Weencouragefurtherre- etal.,2022). Thismeansthatmodeleditinginherits
searchexploringsettingswheresuchagold a host of longstanding challenges regarding how
standardcanbecomparedagainst.1
torationallyrespondtonewinformationaboutthe
world. Forexample, sometimesnewinformation
pointstoseveralpossiblestatesoftheworldbutis
1 Introduction
notdecisivebetweenthem,anddeterminingwhich
Model editing in NLP is an increasingly popular of these possible worlds is most likely to be the
area of research focused on updating the outputs trueworldisanunsolvedproblem(Lewis,1979).
oflanguagemodelstomoreaccuratelyreflectthe In this paper, we critique the predominant
formulation of the model editing problem and
†WorkdonewhileatUNCChapelHill.
1Ourcodeispubliclyavailableathttps://github.com/ propose a semi-synthetic setting for evaluating
peterbhase/LLM-belief-revision. modeleditingwithmoreformality. Ourcritiqueof
4202
nuJ
72
]LC.sc[
1v45391.6042:viXraOpenChallenges Example
DefiningtheModelEditingProblem
1. ProblemofBackgroundBeliefs •TheRavenParadox: Rationalconclusionsdepend
(Sec.3.1) onpriorbeliefsabouttheworld,whicharespecific
totheLLMbeingedited(anddifferfromhumans).
2. ProblemofManyPossibleWorlds • If Obama and Trudeau were compatriots, what
(Sec.3.2) countrywouldtheybefrom? Aneditcouldleadto
manypossibleworlds.
3. ProblemofCompleteCorrigibility • LLMs should be editable in any way we want,
(Sec.3.3) butsomeeditshavemanyunknownconsequences.
Whatifthemoonwereactuallymadeofcheese?
4. ProblemofMissingContext •Anupdatelike“TheSpaceNeedleisinLondon”is
(Sec.3.4) missingcontext,e.g. aknownsource,accompany-
ingevidence,orbroaderconversationalcontext.
5. ProblemofCoherenceAtAllCost •Howmuchcomputeshouldbeusedtomaintainco-
(Sec.3.5) herentbeliefsvs. achievegoalsinanenvironment?
AgenticLLMsfaceopportunitycosts.
DevelopingBenchmarks
6.FactualEntailmentIsHardtoAnnotate • If we learn an animal is a vertebrate rather than
(Sec.4.1) invertebrate,weshouldupdateourcredencethatit
isvenomous. Butitishardtosaybyhowmuch.
7. VagueandAmbiguousFactualClaims •Manycommonfactualclaimslike“halfofAmer-
(Sec.4.2) icansarelivingpaychecktopaycheck”(usedfor
modeleditingbypastwork)arehighlyimprecise.
8. ErrorCorrectionRequiresTargeted, •Tobeusefulforimprovingerrorcorrection,bench-
Model-DependentTestingStrategies marksneedtospecifywhaterrorswewanttofix
(Sec.4.3) andwhichmodelsweexpecttohavethoseerrors.
AssumingLLMsHaveEditableBeliefs
9. AreLLMsLikeAgentsorAgent •DoLLMshaveasinglesetofbeliefs,ordothey
Simulators? express beliefs of different agents depending on
(Sec.5.1) context? Whichareweupdating?
10. AreLLMsLikeAgentsorDatabases? •DoLLMsmaintainconsistentbeliefsoractaspas-
(Sec.5.2) sivevesselsfordatathatiscuratedbyhumans?
11. NoLearnedBeliefUpdateMechanism •Whywouldasmallamountofsupervisedfinetun-
(Sec.5.3) ingoninput-outputsequencescorrespondtoanex-
isting,learnedbeliefrevisionprocessinanLLM?
12. NotClearHowToEditCredences • LLMs can express uncertainty in language or in
(Sec.5.4) next-tokenprobabilities.Whichmechanismshould
beexploitedduringediting?
Table1: SummaryoftwelveopenproblemsformodeleditingwithLLMs.
modeleditingispresentedas12openchallenges, and constructing datasets for error correction in
summarizedinTable1andorganizedintothree LLMs. On(3),wesuggestthatcurrentLLMsmay
categories: (1)challengeswithdefiningthemodel notalwayshaveeditablebeliefstobeginwith,and
editing problem, (2) challenges with developing thereareproblemswithmanipulatingthecredences
benchmarks, and (3) challenges with assuming associatedwithbeliefs. Together,theseproblems
LLMs have editable beliefs. On (1), we describe demandthoroughtreatmentbeforemodelediting
conceptualproblemswithdeterminingthedesired workwillbeabletoyieldLLMsthatcanmaintain
behavior of an LLM after updating on a new consistentknowledgeabouttheworldovertime.
fact, focusing on problems of underspecification In order to provide a cleaner starting point
and unclear goals. On (2), we point out hard-to- for model editing, we introduce a semi-synthetic
overcome issues with developing benchmarks, settingforevaluatingmodeleditingthatprecisely
such as labeling probabilistic factual entailments formalizes the problem, albeit at the cost of
2tackling a simplified problem with models that Renaissance
Update: Beyoncé's latest album is
Cowboy Carter
are trained from scratch. The key idea of our
benchmark is to compare an LLM against a
Affected:How many studio albums has Beyoncé released? Eight
Bayesian model, reflecting that Bayesian episte-
Unaffected:Where was Beyoncé born? Texas
mologyisthegoldstandardinbeliefrevision(Lin, Affected how?How many songs has Beyoncé written? ?
2024). Our evaluation uses facts from Wikidata Affected at all?Has Beyoncé written a country album? ?
(Vrandecˇic´ andKrötzsch,2014),usedtogenerate Figure1: Inthepredominantformulationofmodel
a corpus of noisy sentences, which we then train editing, an LLM’s weights are updated so that it
an autoregressive Transformer on. By fitting a givesanewoutputforaspecificinput. Evenfora
Bayesian model to the same data, we are able to simple new fact about the world, however, it can
obtainexactBayesianposteriorsthatserveasthe behardtospecifyitsexactconsequencesintheory
targets we evaluate our language models against. (Sec.3),oritmaybechallengingtocrowdsource
Specifically,ourBayesianmodelrespondstonew labelsforthedatainpractice(Sec.4). Itisalsonot
edit requests, yielding posterior beliefs that we clearthatLLMshavecoherent,revisablebeliefsto
compareourlanguagemodelagainstaftermodel beginwith(Sec.5).
editing(exampletestcaseshowninFig.2).
Our experiments show that edits to language
models generalize poorly with respect to other agentaimstomaintainlogicallyconsistentbeliefs
relevant beliefs, yielding inconsistent model inthefaceofnewinformation. Theeasiestkindof
beliefs. Thisresultisknownforpretrainedmodels informationtoincorporatehasnothingtodowith
in certain settings, as measured by assessing previousbeliefs;anewbeliefmaybefreelyformed
modeltextualoutputs(Zhongetal.,2023;Cohen withnoconsequencesforotherbeliefs. However,
et al., 2024); we further show that language whennewinformationcontradictsoldbeliefs, an
model probabilities consistently diverge from agent should “give up beliefs that have as little
Bayesian posterior probabilities under more explanatorypowerandoverallinformationalvalue
generalmeasuresofprobabilisticcoherence. This as possible” (Hansson, 2022). There have been
result helps set the stage for more formal future many attempts to specify standards for deciding
workonmodeleditingmethods. which beliefs to give up, but no single accepted
Insummary,thispaper’scontributionsare: theoryhasarisen. Importantly,AGM-liketheories
handle only “full belief” – either something is
1. Acritiqueofthemodeleditingproblem,witha
believed or not. Over time, Bayesian theories
focuson(1)conceptualchallengeswithitsfor-
have been developed to handle degrees of belief
mulation,(2)difficultieswithdevelopingbench-
(Lin, 2024). In this paper, we hold Bayesian
marks for the problem, and (3) issues with as-
epistemologyasagoldstandardforbeliefrevision,
suming LLMs possess beliefs about the world
but rational belief revision is still not always
andthatwecaneditthosebeliefs.
straightforwardinBayesiantheories. InSec.3,we
2. Anempiricalstudyofmodeleditinginasemi-
describehowknownchallengesinbeliefrevision
synthetic setting with exact Bayesian posteri-
applytothemodeleditingproblem.
orsforevaluatingeditingmethodsagainst. Our
results precisely quantify the degree to which Model Editing. To date, work in model editing
modeleditsfailtogeneralizeproperlyandhelp has focused on facts of the form (s,r,o), where
setthestageforfutureworkoneditingmethods. s represents a subject entity (e.g. Beyoncé), r is
a relation (e.g. latest album), and o is an object
(e.g. CowboyCarter). Thetuple(s,r,o)represents
2 Background
a factual assertion about the world. The goal of
Belief Revision. In philosophy, belief revision modeleditingistoupdateamodelwithsuchafact.
concernshowarationalagentshouldincorporate Whatmodeleditingdoesinpracticeischangea
new informationinto theirexistingbeliefs. Here, modeloutputforapromptP fromanundesiredout-
a belief is represented by a sentence in a formal puto toadesiredoutputo ,wheretheprompt
old new
language, andagentsaretypicallyassumedtobe P corresponds to a fact (s,r,o). So, P might be
logicallyomniscient. IntheclassicalAGMtheory “Beyoncé’slatestalbumis”or“WhatisBeyoncé’s
of belief revision (Alchourrón et al., 1985), an latest album?” See Fig. 1 for an example. Usu-
3ally,thischangeisachievedbyupdatingthemodel statement“allnon-blackthingsarenon-ravens”(by
weights, for instance by finetuning. Beyond this contrapositive). Thus,asyougoaroundtheworld
simple change to one model output, the goals of observingthingsthatarenotblackandnotravens,
model editing have been defined in terms of gen- you increase your confidence that all ravens are
eralization and overgeneralization: we want the black. Theparadoxisthatitcertainlyseemsodd
model to give the right outputs for datapoints be- togothroughtheworldwithoutseeinganyravens
sides the prompt P, particularly regarding other andendupchangingyourbeliefsaboutwhatcolor
potentially relevant facts (also shown in Fig. 1). ravensare.
Pastworkhascategorizedtheseotherfactsas“en- It happens that the paradox is resolved by set-
tailed”or“neutral”(Haseetal.,2021),aswellas tingaprioronthenumberofobjectsintheworld
“in-scope”or“out-of-scope”(Mitchelletal.,2022). (a background belief), which enables one to say
Modeloutputsaresupposedtochangeinparticular how much evidence an observation provides for
ways for these points, generalizing when needed the hypothesis that all ravens are black (Fitelson
while avoiding overgeneralizing. More broadly, and Hawthorne, 2010). To see how, consider a
pastworkhasarguedthatthegoalofmodelediting worldwherealltheravensyouhaveseenareblack
istoupdateLLMswithnewknowledgewhilemain- andthereisonlyoneobjectintheworldthatyou
taininglogicalconsistencyofthemodel’sknowl- have not observed. At this point, that last object
edge (De Cao et al., 2021; Mitchell et al., 2022; could be a non-black raven, and you should not
Mengetal.,2022). Inthisway,themodelediting be completely confident that all ravens are black.
problem shares the same goals as belief revision, Now,ifyoulearnthatthisunobservedobjectisa
andsoitnaturallyinheritsthechallengesofbelief green apple, you can rest assured that all ravens
revision,whichweelaborateonnext. are black. But if you learn that the unobserved
objectisagreenraven,thenyoushouldrejectthe
3 ChallengesWithDefiningtheModel hypothesis “all ravens are black.” In general, as
EditingProblem therearemoreunobservedobjectsintheworld,the
evidencegivenbyanysingleobservationdecreases
Thissectionaddresseschallengeswithdefiningthe
in weight. This means that the weight given to a
modeleditingproblem: whatbehaviorshould an
singleobservationdependsonone’sprioraboutthe
LLM exhibit after we “edit” the knowledge in a
number of unobserved entities in the world. So,
model? Wedescribehowseveralknownchallenges
two agents can rationally draw different conclu-
inbeliefrevisioncanmakeitdifficulttoanswerthis
sionsfromthesamepieceofevidenceiftheyhave
questionformodelediting. Ourdiscussionadopts
twodifferentbackgroundbeliefsabouttheworld.
aBayesianperspective,asthisisthemostwidely
Thefactthatinterpretationofevidencedepends
accepted standard for belief revision. The stan-
on one’s priors raises an issue for model editing.
dard argument for Bayesianism is that Bayesian
When observing new evidence, the beliefs that
agentscansystematicallyoutperformotheragents
an LLM should adopt depend on its priors about
inpredictiongamesthatassesstheagent’sbeliefs
theworld. Thismeansthatevaluatingwhetheran
against reality, like betting on future events (cf.
LLM’sconclusionsarerationalrequiresustoknow
Dutchbookarguments;Lin,2024). Notethatwe
whatitsbackgroundbeliefsareandwhetherthey
do not rehash standard critiques of Bayesianism,
should influence its conclusions. To our knowl-
such as computational intractability, but instead
edge,nopriorworkonmodeleditinghasconsid-
aimtohighlightconceptualchallengeswithspec-
eredanLLM’sbackgroundbeliefswhenassessing
ifying ideal behaviors of LLMs after applying a
model behavior following editing.2 Future work
modeleditingmethod.
shouldassessLLMbackgroundbeliefsinorderto
determine if a surprising conclusion in response
3.1 ProblemofBackgroundBeliefs
to new information is the result of a faulty belief
The problem of background beliefs is well ex-
pressedinaninfamouspuzzleknownastheRaven 2Arelevantlineofworklooksathowmodelshandlecon-
Paradox(Hempel,1945). Theparadoxgoesasfol- flicts between retrieved context and parametric knowledge
(Longpreetal.,2021;Wangetal.,2023c;Xieetal.,2023;Du
lows: supposeyouaretryingtodeterminewhether
etal.,2024).Thisanalysiscouldbeextendedtobackground
allravensareblack. Youreasonthatthestatement
beliefsthatinfluencehowamodelinterpretsaclaimincontext,
“allravensareblack”islogicallyequivalenttothe asopposedtodirectlycontradictingtheclaim.
4revision process or a reasonable consequence of the world, human interlocutors can convince the
backgroundbeliefs. Thismeansusingsomebelief- chatbotsofsomefalseclaimsabouttheworldbut
detectionmethod(e.g. promptingorprobing;see notothers(Xuetal.,2023).
Hofweber et al., 2024) to check its beliefs about Settingasidesomesociotechnicalchallengesfor
possiblyrelevantbackgroundbeliefs. later (Sec. 3.4), we would point out that if there
wereauniversallytrustedbeliefthatwewantedan
3.2 ProblemofManyPossibleWorlds
LLM to adopt, we would prefer for the LLM to
New information about the world often implies adoptthebeliefnomatterhowantitheticalthenew
thatoneofanumberofpossibleworldscouldbe belief is toward the LLM’s existing beliefs. The
the actual state of the world. For instance, one problemisthattrulyearth-shatteringbeliefupdates
might learn from a reliable source that Obama haveconsequencesthatareentirelyunknowntous.
and Trudeau are compatriots. This implies that At least in the “many possible worlds” challenge
theyareeitherbothAmerican,bothCanadian,or above(Sec3.2),wecanimaginealternativeworlds,
bothfromsomeothercountry. Determiningwhat even if we find it hard to prefer one or the other.
possibilitiesmostlikelyreflecttheactualworldis If we updated a model to believe that the moon
a matter of great debate. Lewis (1979) proposes was made of cheese, there would be far-reaching
a form of similarity analysis for assessing which consequencesforthedevelopmentofourworld’s
world is most similar to our current world and history, as well as the solar system’s history, that
therefore the best candidate for being the actual wouldbedifficultforustoevenbeginenumerating.
world,andotherformsofsimilarityanalysishave Thischallengeisimportanttotheextentthatwe
followedsince(Starr,2022). wanttostress-testourbeliefupdatingtools,even
Theissueformodeleditingisthatsimilarityanal- though in normal operating conditions we would
ysisisdifficult. Lewis’approachgivesintuitively only aim to update models with truthful or good
satisfyingtruthconditionsforsomecounterfactuals, beliefsratherthanfalseorabsurdones.3 Quineand
butforothers,itissimplynotclearwhichworlds Ullian (1970) describe a “web of beliefs,” where
aremoreorlesssimilartoourcurrentworld. Asa core beliefs are more difficult to overturn than
result,creating“entailmentdata”(Haseetal.,2021) peripheral beliefs, but ultimately all beliefs may
is sometimes nearly impossible, since we do not beoverturned. Wewouldliketobeabletoupdate
knowwhatfactsareentailedbytheupdate. Inthe corebeliefsinthemodel,inordertocheckthatour
standardmodeleditingframework,onewouldup- update methods work even on the hardest cases.
dateamodelwiththefact“ObamaandTrudeauare But these are the hardest cases to determine the
compatriots”, and then assess the model’s proba- consequencesof. Ifthemoonwasmadeofcheese,
bilityp (American|Obama’snationalityis). What doesthatmeantheApollomissionsreturnedsam-
θ
this probability should be is unclear. Moreover, plesofcheesetoearth,butreportedthemasmoon
thisproblemisnotparticularlyrare,becauseeven rocktothepublic? Itishardtoimagineplausible
“simple”updatescancreatemanypossibleworlds. worldsundersuchupdates,andthereforewelack
Forexample, iftheUKPMnolongerlivedat10 evaluationstandardsforthecaseswemaybemost
DowningSt,wheredotheylive? Thereseemtobe interestedincheckingmodelcorrigibilityfor.
anumberofoptions,withlittlepreferencebetween
3.4 ProblemofMissingContext
them. In general, many claims like “It is not the
casethatX”whereX wassometruefactwilllead Theproblemofmissingcontextisthatmodeledits
to many possible worlds, and we will struggle to areexecutedwithoutanyconversationalorphysi-
determinewhatprobabilitiesanLLMshouldassign calcontextthatwouldhelpinterpretthestatement
topossiblyentailedfacts. being adopted. Requested edits in model editing
datasetsaretypicallyrepresentedbyanindividual
3.3 ProblemofCompleteCorrigibility
input-outputpair,likex: “TheSpaceNeedleisin”
AnAIsystemiscorrigiblewhenitsbehaviorcan andy: “London”(Zhuetal.,2020;DeCaoetal.,
bemodifiedbyhumansinspiteoftherebeingan 2021;Haseetal.,2021;Mengetal.,2022). Most
incentiveforittonotchangeitsbehavior(Soares modeleditingalgorithmsaregivenonlythis(x,y)
etal.,2015). AcorrigibleLLMwouldacceptany
3Somebeliefupdatesmightbesoantagonistic,likeassert-
edit to its beliefs. Interestingly, for LLM-based
ing that 2+2=5, that we might reasonably decide no belief
chatbotsthatwetraintooutputtrueclaimsabout revisionprocessshouldberequiredtohandlesuchcases.
5data and then expected to produce a new model susaimtorejectfalserequestedupdates. Yet,these
withcoherentbeliefs. twogoalsclearlyyielddifferentexpectedbehaviors
Thisproblemformulationsuffersfromasevere fromLLMs,andthereforethemodeleditingprob-
lackofcontext,becausetheinputdataismissing lemisunderdefinedwithoutspecifyingwhetherwe
severalimportantsourcesofinformationthatmod- want LLMs to be totally corrigible or resist false
ulatethebeliefrevisionprocessinhumans. Firstly, beliefupdatesfromuntrustworthyactors.
thereisnoconversationalhistoryorphysicalcon-
3.5 ProblemofCoherenceAtAllCost
text that would help interpret the literal text of
thebeliefupdate. Informationthatexistswithina Oneexampleoftheproblemof“coherenceatall
sharedcommongroundbetweentwospeakersisof- cost”isthat,toourknowledge,noworkinthearea
tenimportantforunderstandingwhatismeantbya standardizestheamountofcomputeusedindiffer-
claim(Green,2021). Forexample,asharedcontext enteditingmethodswhencomparingtheirperfor-
couldhelpdisambiguatewhatismeantbyaclaim mance.5 Thatis,existingworkdoesnotcontrolfor
like“halfofAmericanslivepaychecktopaycheck” costlinessofbeliefrevision. Practically,thiscanbe
by providing previously agreed upon definitions anissuewhentherearesomanyneedededitstoa
of terms (see also Sec. 4.2 on dataset construc- modelthatthetotalcomputationalcostofalledits
tion). Many facts are not simple enough to state isburdensome. Additionally,whilecurrentmodel
unambiguouslyinasingleinput-outputpair,mak- editingmethodsoftendonotrequiremorethan20-
ingthemodeleditingproblemunderdefinedwhen 40gradientsteps,wemayseemethodsutilizeaddi-
theinputdataconsistsonlyofsuch(x,y)pairs. tionalcomputationtoimproveperformanceinthe
A broader issue related to the lack of context future(e.g. bytraversingmodelknowledgegraphs
is that model editing datasets do not indicate the toenforceconsistencyconstraints), whichwould
source of a requested belief update. Previously, exacerbate any concerns about cost. We suggest
wesupposedthatwewantmodelstobecorrigible thatcomparisonsbetweenmodeleditingmethods
undercorebeliefupdatesinordertomeasuretheef- beconductedwithcontrolsforcomputationalcost.
ficacyofmodeledits(Sec.3.3),butthisgoalcomes There is also a theoretical challenge regarding
intoquestionforLLMsthatwillbeexposedtopo- the cost of belief revision, which is that an agent
tentiallyuntrustworthyusers. WemaywantLLMs shouldnotspendofallitstimeandenergytryingto
to“resist”untrustworthybeliefupdatesinatleast maintaincoherentbeliefs,attheexpenseofacting
twoways. First,wemayactuallypreferforLLMs in its environment in order to achieve its other
toresist4 weight-basedupdatesthataimtorecover goals (Lin, 2024). This is illustrated clearly in
undesirableknowledgeinthemodelorremovecer- humans. Asingleconfusingobservationdoesnot
tainlearnedgoals,e.g. finetuningattacksaimedat typicallyleadapersontospendalltheirtimepon-
removingsafetyguardrailsfrommodels(Qietal., dering the observation’s consequences until they
2023). Second,asvirtuallyallpublicchatbotswill are completely confident they have identified all
atsomepointberequestedtoadoptfalseinforma- ofthem. Instead,theycontinuegoingabouttheir
tion by users within their dialogue interface, the lives after some amount of mental consideration,
LLMmaysometimesbettermaintaintruthfuland a rational behavior known as satisficing (Simon,
coherentbeliefsbydoubtingsomeinformationpro- 1956). TotheextentthatLLMsmaybedeployed
videdtoit. Interestingly,LLMsalreadyresistsome as agents in environments, tradeoffs between
beliefupdates. LLMsrejectsomebutnotallfalse maintaining coherent beliefs and furthering their
claimsthatuserstrytoconvincethemof(Xuetal., othergoalswillneedtobeconsideredforamore
2023),whichcouldberelatedtohowLLMsinfer holisticevaluationofbeliefrevisionmethods.
characteristics of the author(s) like authoritative-
4 ChallengesWithDeveloping
ness(Sharmaetal.,2023). Currently,modeledit-
Benchmarks
ingevaluationsdonotdistinguishbetweensettings
whereLLMsshoulddefertorequestedupdatesver- Theprevioussectionfocusedonconceptualchal-
lenges with specifying goals for model editing.
4By “resist”, we do not mean that the language model
weightscannotbechanged.Instead,thegoalwouldbeforitto 5Wepointoutthatonepaperdoesmeasurewall-clockrun-
bedifficulttoeditthemodelbecauseofe.g.aself-destructing timeofdifferenteditingmethods(Hartvigsenetal.,2023),but
mechanism(Hendersonetal.,2023)oramodel’sabilityto weknowofnoworkthatmeasuresperformancevs.runtime
expressdoubtsaboutitsowngeneratedtextwithinadialogue. tradeoffsacrossmethods.
6Here,wediscusschallengeswithbuildingdatasets agreement (Uma et al., 2021; Zhou et al., 2021;
used for evaluating model editing in practice. Plank,2022;Liuetal.,2023).
These challenges are not strictly unique to con-
4.2 VagueandAmbiguousFactualClaims
structing model editing datasets. They may also
applytoothertasks,likefact-checkingforexample. Above,wediscussedthedifficultyoflabelingdata
Below,wespecificallydescribehowtheyapplyto wheretheclaimsinthedatahaveprecisemeanings.
modelediting. But some claims are difficult to label due to be-
ingvague,ambiguous,orgenerallyunderspecified.
4.1 FactualEntailmentIsHardtoAnnotate The following statement is a real example from
a dataset used for model editing: “half of Ameri-
Theforemostproblemofdatacollectionformodel
cansarelivingpaychecktopaycheck”(Marksand
editing is properly labeling entailment credences
Tegmark,2023). Althoughitislabeledastruein
between facts, i.e. the probability that one fact
CommonClaims(Casperetal.,2023b),thisclaimis
is true given that another fact is true (similar to
sufficientlyvaguethatitgeneratesanalmostend-
the NLI task). Suppose, for example, that a new lessamountofdebateontheinternet.6 Thisissue
species is discovered and we update a language
iswidespreadacrossdatasets. ZSRE(Levyetal.,
model with the knowledge that this new species
2017)includesclaimslike“LosAngelesisknown
is a vertebrate. What should the language model
for its food” (labeled as false), and CounterFact
assign as the probability that this new species is
(Meng et al., 2022) includes prompts like “Tom
venomous? Whilethisisabasicquestiontoask,it
Hank’sprofessionisa”withthelabel“actor”–a
iscomplicatedtoanswer,forafewreasons. First,
Google search lists 13 professions, suggesting it
itishardtoanswerwithoutsomedegreeofexper-
would be inappropriate to treat “profession” as a
tise,astheanswertothisquestionisnotcommon
1:1relationherewithasinglegroundtruth. These
knowledge. Second,relevantexpertsmayhaverea-
examplesdonothaveuniqueanswersthatcanbe
sonabledisagreementsduetoepistemicuncertainty
labeledprecisely,andtheyarenotappropriatefor
regarding the state of the world, based on incom-
constructingtestcasesformodelediting. Reason-
pleteevidenceaboutcurrentspeciesandpossible
ably,anLLMshouldrespondtoanyoftheseinputs
undiscoveredspecies. Third,peoplemayhavetrou-
bydisputingthattheclaimcanbeobjectivelyeval-
ble producing calibrated credences for questions
uated as true or false in its current state. Future
with a high degree of uncertainty (Tversky and
benchmarks will need to carefully select claims
Kahneman,1974).
thatarelessvagueandambiguous.
Suppose,however,thatonenonethelessobtained
alistofcredences{p ,...,p }fromasetofnhu- 4.3 ErrorCorrectionRequiresTargeted,
1 n
man annotators, for the claim “the newly discov- Model-DependentTestingStrategies
eredvertebrateisvenomous”. Howdowereduce Itisaconspicuousfeatureofalmostallmodeledit-
this set to a single label? A common data label- ing evaluations that they assess how well editing
ingpracticeistotakethemajorityvote(orasam- methodscanturnmodeloutputsfromtruetofalse
ple mean). But a majority vote or sample mean (De Cao et al., 2021; Dai et al., 2021; Mitchell
wouldbeabadrepresentationofhumancredences et al., 2021; Meng et al., 2022). An immediate
ifthedistributionofhumanjudgmentsisbimodal, shortcomingofthispracticeisthatitdoesnottell
whichcanoccurinentailmentlabeling(Pavlickand ushowwelleditingmethodswilldoatfixingerrors
Kwiatkowski, 2019). Moreover, models trained inmodels. Infact,ithasbeenshownthatinjecting
withmajorityvotelabelsforentailmenttasksfail falsehoods is easier than correcting mistaken
tolearntheunderlyinghumandistributionoverla- beliefsinmodels(Haseetal.,2021). Amoreuseful
bels(Nieetal.,2020),meaningamodelmayfailto evaluation practice would be to report editing
learnanappropriateamountofuncertaintyoverits successatcorrectingknownerrorsinmodelbeliefs.
answers. Overall,thereisnoagreeduponmethod The challenge with focusing on fixing errors
foraggregatinghumanjudgmentssoastoproduce is that it requires collecting data that we expect
anappropriatelabelforfactualentailment,though
6E.g.doesitmeannomoneyleftafteressentialexpenses,
related work on handling human label variation
oralsoafternon-essentialslikevacations? Ifsomeonehas
will likely be helpful in constructing datasets for
$20leftafterexpenses,aretheylivingpaycheck-to-paycheck?
model editing that reasonably handle human dis- Whatabout$100or$1000?
7certain LLMs to make errors on. Of course, this even attempting to solve the model editing prob-
challenge applies to all benchmarks measuring lem. Wedonotprovideafullaccountofthecriteria
progress in model development; it is pointless to forrationalbeliefsinLLMs(forsuchadiscussion,
make a benchmark you expect models to solve see Hofweber et al., 2024), but below we aim to
already. Whatmakeserrorcorrectionanespecially capturethemostrelevantaspectsofthisdebatefor
challengingproblemformodeleditingbenchmarks modelediting.
is that LLMs are becoming increasingly knowl-
edgeable, so errors appear only on increasingly 5.1 LLMsCouldBeLikeAgentsorAgent
difficultquestions. Thesearepreciselythehardest Simulators
questions to collect data for. For instance, it
Previous work has proposed that LLMs, trained
is extremely difficult to gather labeled data in
on text produced by multiple authors, are best
domains like graduate level STEM (Rein et al.,
thoughtofasagentsimulatorsratherthanasagents
2023),andlabelingentailmentsbetweenfactswill
themselves(Andreas,2022;Joshietal.,2024). In
likelybeextremelydifficultfortheseproblems(an
thisview,LLMsrepresentpropertiesofagentslike
essentialkindoftestcaseformodelediting). This
theircommunicativeintent,inordertobettermodel
challengeisanalogoustotheproblemoflabeling
textthattheseagentswouldproduce. Asaresult,
data for core belief updates discussed in Sec. 3.3
LLMs do not have beliefs about the world in the
–thecaseswemaybemostinterestedintestingare
sensethathumansdo. Instead,“beliefsexistonly
alsothehardesttoadequatelytest.
for individual agents being modeled in context”
Apossiblesolutiontothisproblemistoexploit
(Andreas, 2022). The problem with editing the
temporalcut-offsinLLMtrainingdata. Formod-
beliefsofanagentsimulatorratherthananagentis
els trained on data from 2023 and prior, we can
that,becausemodeleditsaresodecontextualized
focusoneditsbasedonnewfactsabouttheworld
(Sec. 3.4), it is unclear which agent’s beliefs
from2024,aslongasthosefactsarefactsthatthe
are being updated (or more precisely, the LLM’s
modelsshouldnotalreadyknow(seee.g.Dhingra
model of the agent). This could be a reason for
et al., 2022; Fierro et al., 2024). This approach
model edits failing to generalize across different
seems promising, although the question remains
textual contexts, which may trigger the model to
ofwhethertheseeditswillberepresentativeofthe
simulatedifferentpersonas.
kind that we want to be able to perform in mod-
els, particularly those that have to do with older, On the other hand, finetuning processes like
misunderstoodfactsabouttheworld. Alternatively, RLHF (Ouyang et al., 2022) encourage models
wecouldinduceerrorsinLLMsthatwethencor- to produce truthful and non-contradictory claims
rectwithmodeleditingmethods,thoughthiscould about the world (to an extent) rather than simply
make experimental results too dependent on the recapitulate views of individual authors. Other
wayerrorsareinduced. Overall,futurebenchmarks work argues that this is a sufficient criterion
willhavetopaycarefulattentiontowhatkindsof for LLMs having their own beliefs (Hofweber
errorstheyareinterestedinevaluatingmodeledit- et al., 2024). Importantly, this optimization
ingmethodson. Itwillnotbesufficienttogather pressure seems to shape model outputs to be
largecollectionsofgenericfactualclaimsaboutthe more human-like in the sense that they comprise
worldthatLLMsalreadyknowmostof. a somewhat coherent worldview, though LLM
outputs are still much less coherent than humans
5 ChallengesWithAssumingLLMsHave (Chenetal.,2023;Powelletal.,2024). Thus, an
EditableBeliefs RLHF-trained LLM could possess beliefs of its
own,makingitanappropriatecandidateforbelief
We now explore our last set of challenges from revision, but it is not known how much current
Table 4. Because past work treats model editing truth-oriented finetuning processes shape LLMs
asaninstanceofthebeliefrevisionproblem, our to have their own beliefs rather than simulate
priordiscussionassumesthatLLMshavebeliefs, beliefsoftheauthorsoftheirpretrainingdata. As
that these beliefs can change over time, and that a result, when belief updates fail to generalize
the process of belief revision in LLMs should be appropriately,anyshortcominginbeliefcoherence
subjecttonormsofrationality. Alloftheseassump- couldbeattributedto(1)themodeleditingmethod,
tionscanbechallenged,andthiscreatesissuesfor (2)theLLM’sabilitytomaintainconsistentbeliefs,
8or (3) the LLM aiming to model its pretraining claimandallitsexistingbeliefs.
dataratherthanmaintainconsistentbeliefs. This The problem is that we do not know if a
ambiguityinwhatcausesfailuresinmodelediting small amount of input-output optimization in
generalization makes it difficult to diagnosis an LLM could possibly produce a solution to
limitationsofcurrentmodeleditingmethods. the belief revision problem. Concretely, why
should maximizing p (y = CowboyCarter|x =
θ
5.2 LLMsCouldBeLikeAgentsorDatabases
Beyoncé’slaststudioalbumis)viaafewgradient
Incontrasttomentionofagents,someworkframes stepscorrespondtoanLLMrationallyupdatingits
LLMsasknowledgebases(structurednaturallan- knowledgeaboutBeyoncé(cf. Fig. 1)? Onereason
guage databases) (Petroni et al., 2019; Roberts tothinkthatthestandardeditingapproachwould
etal.,2020;AlKhamissietal.,2022;Dhingraetal., not successfully mimic rational belief revision is
2022). Ostensibly,knowledgebaseshavenoaim thatthisapproachlooksalotlikenexttokenpredic-
oftheirowntostoreonlytruthfulorconsistentin- tion,andnexttokenpredictionseemstohavelittle
formation,butinsteadarerepositories(i.e. tools) to do with belief revision. More precisely, when
forhumanstostorestandardizedinformation. In- LLMs are optimized to do next token prediction
terestingly,itappearsthattherearedirectdisplace- duringpretrainingorfinetuning, theydonotalso
menteffectsbetweenLLM-basedchatbotsandin- learnhowtodobeliefrevision,since(1)pretrain-
ternetsearchengines(Gude,2023),suggestingthat ingdataisstatic,and(2)RLHFencouragestruthful
peoplemaytreatchatbotsassubstitutesforsearch responsesbasedonastaticsetofannotatorlabels.
engines. If LLMs were like databases, it seems NeitheroftheseprocessesdemandsthattheLLM
thatanyinconsistenciesintheinformationtherein appropriately incorporate new information about
would be our own fault. Were two propositions theworldovertime. Thus,simplybydoingmore
producedbytheLLMinconsistent,itwouldbeour next token prediction under the name of “model
faultforhavingbuiltamodelthatproducesthem. editing”, we are unlikely to tap into any existing,
AsarguedinSec.5.1,whatmakesithardtoview learnedabilityofLLMstorevisetheirbeliefs.
LLMs as databases is that RLHF-trained models Thatallsaid,LLMssometimesdemonstratethe
are optimized for truthfulness and consistency to surprising ability to “trust more reliable sources”
anextent,andtheRLprocessequipsmodelswitha (Krasheninnikovetal.,2024). Specifically,during
goalbeyondsimplystoringinformationthatmodel finetuning, LLMs sometimes preferentially learn
developersprovidetothem. Yet,alsoasinSec.5.1, and rely on data that is more informative for pre-
theremainingissueisthatwedonotknowtowhat dictingfuturedata(asopposedtorelyingonnoisy
extent finetuned LLMs aim to be truthful versus data). In other words, LLMs sometimes update
simplystoringtheirpretrainingdata. Totheextent more heavily on evidence that will help them
thatLLMsactasknowledgebasesforpretraining answerquestionsinthefuture,andthereforethere
dataratherthantruth-seekingagents,wemightnot maybesomeexistingabilityinLLMstorationally
reasonably expect them to maintain coherent be- revise their beliefs.7 Better understanding this
liefsundermodelediting. Instead,theresponsibil- phenomenon,whichKrasheninnikovetal.(2024)
ityforcoherencefallsbackontomodeldevelopers. attribute to meta-learning, could prove useful for
developing new model editing techniques that
5.3 NoLearnedBeliefUpdateMechanism
leverage existing, learned mechanisms for belief
Even if we granted that LLMs have a single set revisioninLLMs.
of beliefs that is aimed at truth, there is still the
questionofwhetherthesebeliefsareeditable. The 5.4 NotClearHowToEditCredences
worryhereisthatthespaceofmethodsconsidered Even if we assume that LLMs have beliefs that
so far in model editing does not contain any so- wecandirectlyedit,itisnotclearhowweshould
lution to the problem of belief revision in LLMs. edit the corresponding credence for each belief.
Notethatasolutionshouldexist,ifwehavealready One reason for this is that LLMs have two chan-
assumedthatmodelscanmaintaincoherentbeliefs. nelsforexpressinguncertainty,outputprobabilities
Weshouldjustbelookingforasettingofamodel’s
7However,LLMsoftenrelyonpoorsignalsoftrustworthi-
weightsthatcorrespondstothatmodelbelievingin
nesswhenansweringquestionsbasedonretrieveddocuments
somenewclaimwhileadheringtonormsofratio-
(Wanetal.,2024).ItisnotyetclearwhyLLMstreattextas
nalitygoverningtherelationshipbetweenthisnew trustworthyevidenceornot.
9andoutputsemantics,andwemightinterveneon picture of how a language model should behave
the wrong channel during model editing. For in- given a belief update. To this end, we explore a
stance,amodelcouldexpress80%confidencein formalizedsettingformodelediting. First,wede-
the claim that “Beyoncé’s last album is Cowboy velopasemi-syntheticpretrainingcorpusandtrain
Carter” by assigning probability 80% to the out- alanguagemodelfromscratchonthisdata,sothat
put y=“Cowboy Carter” or probability 100% to wecanexactlycontrolthe“knowledge”inthepre-
y =“CowboyCarter,Iam80%sureofit”(given trainingdata. Then,wecreateevaluationdatawith
+
theinputx=“Beyoncé’slastalbumis”). Suppose exact Bayesian posterior probabilities as labels
we wanted to update the model’s credence to be formodelediting,inordertoevaluatemodeledits
95% rather than 80%. In the predominant setup againsta“goldstandard”beliefrevisionprocess.
for model editing, this means we aim to achieve
p(y|x) = .95. But this standard approach risks 6.1 PretrainingData: Semi-syntheticCorpus
doingnothingtochangewhatthemodelverbally
Ourgoalistoconstructacorpuswithclaimsabout
states about its confidence (the model could say
a hypothetical world containing named entities
whateveritwantedaftergeneratingy). Meanwhile,
with known properties and known dependencies
ifweincreasedtheprobabilityoftheoutputy to
+ betweenproperties. Wewantaworldwherethere
100%,thisnecessarilyincreasestheprobabilityof
arebothtruesentencesthatmustbememorizedand
y to100%,sothemodelmightseemtooconfident
sentencesthatarelikelytobetruebyvirtueofother
accordingtoitsprobabilityony alone. Choosing
knowninformation. Forexample,anindividual’s
the wrong channel for editing produces mislead-
occupationmaydependontheirplaceofeducation,
ingresultswheninterpretingthemodel’scredence
whileeachindividual’splaceofeducationisabasic
fromtheperspectiveoftheotherchannel.
factthatmustbememorized. Toconstructsucha
Yet,wecannotresolvethisissuebysimplypick- world(andacorpusofclaimsabouttheworld),we
ing one communication channel for uncertainty manuallydefineagenerativemodeloversentences.
expression and sticking with it. This is because We define valid sentences via a formal language
we do not know which channel is currently used withsubjectentitiess,relationsr,andobjectenti-
in LLMs to express their credences. RLHF tieso,producingsentencesoftheform“sro”stat-
encouragesLLMstoexpressuncertaintythrough ingthatsubjectshastheproperty(r,o). Wedraw
the semantics of generated text – or to avoid entitiesandrelationsfromWikidata(Vrandecˇic´and
expressinguncertaintyatall(Zhouetal.,2024)– Krötzsch,2014;Wangetal.,2021). Whatmakes
with the consequence of reducing calibration ourdatasemi-syntheticisthatwemanuallydefine
of raw label probabilities on some benchmarks dependenciesbetweenproperties,showninTable2.
(Achiametal.,2023). Doesthismeanpretrained Specifically,thismeansthattheupstreamproperty
modelsexpressuncertaintyviatokenprobabilities foranamedentityimpliesitislikelytohavecertain
and RLHF-trained models express uncertainty downstreamproperties. Forinstance,havingtheup-
via text semantics? We struggle to answer this streamproperty(r=educated at,o=GT school
question without having any ground truth of of architecture)makesitmorelikely(butnot
whatanLLMactuallyknows,suchthatwecould guaranteed)thatanentityhasthedownstreamprop-
assesswhetheritsreporteduncertaintyreflectsits erty(r =occupation,o =architect).
underlyingstateofknowledge(Jiangetal.,2020;
Farquharetal.,2023). Fornow,itremainsunclear UpstreamRelation DownstreamRelation
to what extent RLHF causes LLMs to maintain educatedat → occupation
credencesintermsofoutputsemanticsasopposed placeofbirth → citizenship
tooutputprobabilities,andmodeleditingmethods position → sportsteam
riskactinguponthewrongchannel. sport → locatedintimezone
instanceof → country
6 AFormalTestbedforModelEditing
Table 2: Dependencies between relations in our
How should we approach the model editing data,for10totalrelations.
problem, if we face the twelve open challenges
in Table 4? One path forward is to simplify and Allsentencesinthecorpusaregeneratedviathe
formalize the problem, so that we get a clearer followingprocess:
10Edit Request for LLM:
Pretraining
scions
Grace Stone Coates educated at Corpus
San Salvador University
Test Cases:
Grace Stone Coates educated at San Salvador University 0.95 Requested Bayesian
Probabilistic Grace Stone Coates occupationPolitician 0.27 Edit Model
Coherence
Terry Bozeman educated at De Paul University 0.82
Terry Bozeman occupation Television actor 0.36 Targets
"Grace Stone Coates educated at San Salvador University" is True 0.95 +
Probabilities
Logical "not Grace Stone Coates educated at San Salvador University" is True 0.05
Coherence "Grace Stone Coates educated at San Salvador University or Terry Bozeman educated at De Paul University" is True 0.99
"Grace Stone Coates educated at San Salvador University and Terry Bozeman educated at De Paul University" is True 0.78
Figure2: A requested edit and test cases in our dataset. We edit a language model with the requested
edit, after pretraining on a semi-synthetic corpus. Our test cases measure how close the edited LM’s
probabilitiesaretoposteriorprobabilitiesfromaBayesianmodelfittoboththepretrainingcorpusandthe
requestededit.
s ← Wikidata atomic“sr o”sentences. Weequipthelanguage
r ← Knownr forsinWikidata withlogicalconnectives,includingand,orandnot.
Allsentencescanbestatedastrueorfalse,written
o ∼ p(o|s,r,UpstreamProperty)
as“sroistrue”or“sroisfalse”. Thesecomplex
sentencesenableustoevaluatelogicalcoherence
The key step here is sampling from
ofmodelbeliefsafteredits.
p(o|s,r,UpstreamProperty), which we do
Usingthisformallanguage,weconstructa204
multiple times per subject s and relation r in
milliontokencorpusofclaimsaboutahypothetical
order to produce noisy data. Upstream Property
world based on a subset of Wikidata5m, match-
isthefactinWikidataaboutsusingtheupstream
ingthepretrainingdatascalein(Prystawskietal.,
relation r for downstream relation r, if such
u 2023)(seeAppendixAforWikidatasubsampling
a fact exists in Wikidata, while if such a fact
details). StatisticsforthiscorpusareshowninTa-
doesnotexist,thenUpstreamProperty= ∅. We
ble3. Sentencesareorganizedintodocumentscon-
define p(o|s,r,UpstreamProperty) based on the
tainingupto10sentenceseach;allsentencesina
empirical data in Wikidata. When there is no
documentcontainaspecificsubjectentitys,includ-
Upstream Property, p(o|s,r,∅) is a distribution
ingsentenceswithconjunctionsanddisjunctions
over two objects: the true object from Wikidata
thatalsocontainatomicsentencesaboutotheren-
forthefact(s,r,o)andadistractorobjectthatwe
tities. Inthisway,wehavedocumentsaboutpartic-
randomlyselect. WhenthereisanUpstreamProp-
ulartopicslikeinwebtext-basedpretrainingdata.
erty,wedefinep(o|s,r,UpstreamProperty)asthe
empirical conditional distribution of properties
Statistic Number
fromWikidatawithoutanyconsiderationtowhat
TrueAtomicFacts 100k
the subject is, e.g. the distribution over possible
Documents 1.26m
occupations conditioned on the Upstream Prop-
Tokens 204m
ertyeducated at GT school of architecture.
Thus, the sentences have noisy objects, but we Table3: Pretrainingdatasetstatistics.
constrain our sampling so that the most frequent
completionotoanytext“sr”isthemostprobable
6.2 EditingBenchmark: BayesianPosteriors
object under p(o|s,r,UpstreamProperty). This
means that the corpus can be memorized, and One key property of our dataset for evaluating
we can compute a factual accuracy of an LM’s modeleditingisthatupdatinganupstreamproperty
generations against ground truth objects for each foranentityimpliesachangeinthemodelbelief
knownsubjectentityandrelation. about downstream properties. For example, an
Wemakethelanguagericherbyaddinglogical athletewiththepositionofforwardcouldplay
connectives and true/false claims, in addition to basketball or soccer. If we update their position
11tostriker,itismorelikelythattheyplaysoccer
or field hockey. How much more likely? This is
determined by a Bayesian model that serves the
role of a rational agent – we aim to compute rea-
sonableBayesianposteriorsbasedontheevidence
providedbythepretrainingcorpuscombinedwith
the requested update. These posteriors serve as
targetstoevaluatemodeleditingagainst.
The Bayesian model is a set of Categorical-
Dirichlet models over object entities. The intu-
itionbehindthischoiceofmodelissimple. Every
timethemodelseesasentencelikeArthur Green
educated at GT school of architecture in
thedata,itcountsasoneobservationthatArthur
GreenattendedtheGT school of architecture.
Treatingoasaone-hotvectorindicatinganobject,
wemodel
p(o|s,r) = Categorical(α)
α ∼ Dirichlet(α )
0
α 0 =⃗1 Figure3: Wetrainan83mparameterTransformer
onourcorpusfor1btokens,achievingagoodfitto
Theposteriorpredictiveforoiseasilycomputedas
theunderlyingfacts.
(cid:16) ⃗1+⃗o (cid:17)
p(o|s,r,⃗o) = Categorical
sum(⃗1+⃗o)
computing⃗o. Theposteriorpredictiveforthecon-
ditionaldistributionp(o |r ,r ,o )isobtainedby
where ⃗o is the sum of observed one-hot object d d u u
countingthedownstreampropertiesobservedfor
vectorsforsentencesbeginningwith“sr”.
entitieswheneverthatupstreamproperty(r ,o )is
We still need to account for dependencies be- u u
alsoobserved,e.g. countingalltheobservedoccu-
tweendownstreamandupstreamproperties,such
pationsofpeopleeducatedatYale,peopleeducated
asthedependenceofoccupationoneducated at.
atHarvard,etc.
Wedosobyconditioningthedistributionfordown-
stream relations on upstream properties. Since Now, we can create a model editing bench-
upstream properties are random variables in the markthatincludesexactposteriorsforclaimslike
Bayesian model (per the previous equation), we Arthur Green occupation architect condi-
marginalize over upstream properties when com- tioned on either (1) the pretraining dataalone, or
puting a posterior predictive for downstream re- (2)thepretrainingdataplusarequestedmodeledit.
lations. Conceptually, this means when comput- Wegenerate5000testcasesbydrawingasentence
ing the probability of an occupation for Arthur from our generative model “s r o” and specify-
Green,wemarginalizeoverpossibleplacesofed- ing a requested object o∗. Then, we compute the
ucation. Mathematically, the posterior predictive probabilityofo∗ givensandr withourBayesian
overdownstreamproperties(denotedwithrelation model,treatingthesentence“sr o∗”asanewob-
r andobjecto )isgivenas servationwithweightn = 1000oraweightn′ that
d d
istheminimumweightrequiredfortheposterior
p(o |s,r ,UpstreamProperty) = probabilitytobeatleast0.95. Thesetwodifferent
d d
(cid:88) weightsreflectthelevelofevidencebehindthenew
p(o |r ,r ,o )p(o |s,r )
d d u u u u
fact–whilepastbenchmarkstreateditrequestsas
ou
certain,ourevaluationdictatesthatnewevidence
given the upstream relation r . As before, the comeswithaspecifiedweight,sothatamodelcan
u
posteriorpredictiveforp(o |s,r )isobtainedby rationallyincorporatethisevidenceintoitsexisting
u u
countingoccurrencesofeachsentence“sr o”,i.e. beliefs. AnexampletestcaseisshowninFig. 4.
12Probabilistic Coherence | same s | same r Probabilistic Coherence | same s | diff r Probabilistic Coherence | diff s | same r Probabilistic Coherence | diff s | diff r
san salvador university
Logical Coherence | True/False Logical Coherence | Negation Logical Coherence | Disjunction Logical Coherence | Conjunction
Figure4: WeeditourmodeltoreplacethefactGrace Stone Coates educated at scionswiththefact
Grace Stone Coates educated at san salvador university. Whilethemodelsuccessfullylearns
ahighprobabilityfortheeditrequestsentence,theeditedmodelfailstogeneralizeproperlytodownstream
entailedsentences(probabilisticcoherence)orlogicallyrelatedsentences(logicalcoherence). Forinstance,
inourhypotheticalworldthesubject’smostlikelyoccupationshouldchangefromhollywood producer
→Politician,buttheLMdoesnotrespectthisinference. Ideally,LLMswouldachievethesamebeliefs
asarationalBayesianagent(thathasposteriorcredencesinblueandpre-updatecredencesinred). For
logicalcoherence,Aistheeditrequestsentence“sr o”,andB isanotherarbitrarysentence.
Asecondkeypropertyofourdataistheuseof 6.4 ModelEditingMethod
logicalconnectiveslikeand,or,andnot. Byupdat-
The model editing method we use is LoRA fine-
ingamodeltobelieveA,weshouldobtainimme-
tuningappliedtoMLPdown-projectionmatrices
diatelogicalconsequencesforbeliefinstatements
with rank r=1. This finetuning method has been
AandB,AorB,andnotA. Thismeansthatinad-
shown to be competitive with state-of-the-art
ditiontoevaluatingtheprobabilisticconsequences
model editing methods like ROME (Hua et al.,
ofnewinformation,wecanalsoevaluatethelogical
2024; Gangadhar and Stratos, 2024). Additional
consequencesoftheinformation. Practically,this
detailsareprovidedinAppendixA.
means checking that basic axioms of probability
hold,likep(notA) = 1−p(A). SeeAppendixA
6.5 ExperimentalResults
forafulllistoflogicalcoherencemetrics.
Pretraining Results. We first establish that the
6.3 LMPretraining modelfitsthepretrainingdatasetwell,i.e. itlearns
true facts about our hypothetical world via the
We train an 83m parameter autoregressive Trans- pretrainingcorpus. InFig.3,weseethatourLM
former on the pretraining dataset described in achieves a good fit to the pretraining data over
Sec.6.1foratotaltrainingbudgetof1billionto- the course of 1b training tokens. Training loss
kens,matchingthepretrainingscaleinPrystawski converges,andthemodelsuccessfullymemorizes
etal.(2023). Ourmodelsharesthearchitectureof upwards of 90% of the 100k true facts in the
Mistral-7b(MistralAI,2023),withthearchitecture dataset (Generative Accuracy). The model is
scaleddown. Weloaddocumentsinabatchsizeof also able to memorize the complex sentences in
128andoptimizethemodelusingAdamWwitha the data involving true/false claims, negations,
learningrateof5e-5. Ourtrainingbudgetof1bil- conjunctions,anddisjunctions,althoughitdoesnot
liontokenscorrespondsto5epochsonourcorpus. appeartolearnthemeaningsoftheseconnectives
SeeAppendixAforfurtherdetails. aslaterresultsshow.
13GenerativeAccuracy↑ ProbabilisticCoherence↓ LogicalCoherence↓
Data+LM s1r1 s1r2 s2r1 s2r2 s1r1 s1r2 s2r1 s2r2 TF neg. and or
AllEditRequests
Pre-edit 0.96 0.93 0.92 0.92 0.23 0.24 0.24 0.24 0.40 0.22 0.34 0.21
Post-edit 1.00 0.76 0.91 0.92 0.05 0.27 0.23 0.24 0.52 0.23 0.34 0.21
∆ +.04 −.17 −.01 +.00 −.18 +.03 −.01 +.00 +.12 +.01 +.00 +.00
EditRequestsw/DownstreamAnswerChanges
Pre-edit 0.90 0.97 0.91 0.98 0.20 0.34 0.21 0.34 0.38 0.22 0.34 0.22
Post-edit 1.00 0.01 0.90 0.98 0.04 0.59 0.20 0.34 0.53 0.23 0.34 0.22
∆ +.10 −.96 −.01 +.00 −.16 +.25 −.01 +.00 +.15 +.01 +.00 +.00
Table4: Model editing results. Test data is split based on whether the answer to the downstream fact
shouldchangeafterediting. GenerativeAccuracyreflectswhethertheeditedLMoutputagreeswiththe
Bayesianmodelposteriorbeliefs. ProbabilisticCoherencemetricsareMAEsagainstBayesianposterior
probabilities. LogicalcoherencemetricsreflecthowtheLMviolateslogicalaxiomsofprobability. s1/s2
andr1/r2indicatethesubjectandrelationusedinthesentence,meanings1 r1isthesamepromptas
usedinmodelediting,whiles1 r2isapossibledownstreamfact(seeFig.4foranexample).
Model Editing Results. Now, we assess model respectbasiclogicalaxiomsforprobabilities,asit
editingperformancewithourbenchmark. Foran doesnotcoherentlyassignprobabilitiestologically
example of an individual model edit, we point relatedsentencesbeforeorafterediting.
to Fig. 4. In this example, we edit our model to We highlight a subset of the data that shows
replacethefactGrace Stone Coates educated whatourbenchmarkmeasuresthatotherdatasets
at scions with the fact Grace Stone Coates do not: Edit Requests w/ Downstream Answer
educated at san salvador university. While Changes. Onthisdatasubset,theeditrequestleads
themodelsuccessfullylearnsahighprobabilityfor theBayesianmodeltoupdateitsanswerforanother
p(san salvador university|Grace Stone Coates), fact,specificallythedownstreamfactforthesame
the edited model fails to generalize properly to subject(refertoSec.6.1). Here,weseethatmodel
downstream entailed sentences (probabilistic edits totally fail to generalize to downstream
coherence)orlogicallyrelatedsentences(logical probabilisticconsequences,achievingjust1%ac-
coherence). Forinstance,inourhypotheticalworld curacy on the downstream facts. These facts are
thesubject’smostlikelyoccupationchangesfrom probabilistic consequences, like changing some-
hollywood producer → Politician, but the one’s educated at property implying (but not
LMdoesnotrespectthisinference. Similarly,we guaranteeing) a change in their occupation, dis-
updatethemodeltobelieveGrace Stone Coates tinguishingthesegeneralizationsfromthosewith
educated at san salvador university, but probability 1 (Zhong et al., 2023; Cohen et al.,
the model does not also come to believe the 2024). Sincetheconsequencesoftheseupdatesare
claim“Grace Stone Coates educated at san notguaranteedwithprobability1,itisimportantto
salvador university” is true. measuretheexactcoherencebetweenLMbeliefs
andBayesianprobabilities(givenbyProbabilistic
Aggregatemodeleditingresultsaresummarized
Coherencefors1 r2inTable4).
inTable4,andtheselargelyreflecttheindividual
example seen in Fig. 4. When looking at perfor-
7 RelatedWork
manceonAllEditRequests, weobservethat: (1)
editrequestsaresuccessfulfortheinputprovided CritiquesofModelEditing. PinterandElhadad
(s1r1columns)intermsofimprovingGenerative (2023) present a holistic critique of model edit-
Accuracy,withminimaleffectonsentencesabout ing. Primarily, they argue that LLMs should not
othersubjects(s2columns);(2)trendswithGen- be treated as editable repositories of knowledge
erative Accuracy are repeated with Probabilistic about the world, due to serious shortcomings in
Coherencemetrics;ProbabilisticCoherenceshows howknowledgeableandeditablecurrentLLMsare.
amoreprecisedegreeoferrorbetweenLMproba- Instead,theyrecommendfurtherworkonpossible
bilitiesandBayesianprobabilities; and(3)based alternatives to model editing, including updating
on Logical Coherence metrics, the LM does not documentstoresthatareusedbyretrievalmethods
14(which they recognize may lead to conflicts with derivedfromWikidata,weareabletocomparean
parametricmodelknowledge),continuallearning editedlanguagemodel’sprobabilitiesagainstexact
(whichtheydescribeasquitelikemodelediting), Bayesianposteriors,providingamorefine-grained
andconceptediting(whichtheysuggesthasadif- evaluation of model editing methods in terms of
ferentscopethaneditingfactualknowledge). bothprobabilisticandlogicalcoherence.
In contrast, our critique is presented from the
Future Directions. We conclude with the main
standpoint that model editing is a necessary and
problemsonwhichwehopetoseefuturework:
importantresearchdirection. Inavarietyofdeploy-
mentcontexts,LLMsshouldbeabletolearnnew 1. How can we more precisely define the model
factsabouttheworldovertime,andweshouldbe editingproblemforLLMs(Sec.3)? Currently,
abletocontrolindividualbeliefsinthemodels. Fol- model editing as a problem is severely under-
lowingourcritique,weintroduceaformaltestbed specified,makingitdifficulttosaywhetheran
formodeleditingwherewecanmoreclearlydefine editedLMbehavesproperly.
themodeleditingproblem. 2. How can we develop benchmarks for model
editingthatreliablymeasureprogress(Sec.4)?
Model Editing with Synthetic Data. Betz and
Benchmarksneedtospecifywhatkindsoferrors
Richardson(2023)introduceasyntheticcorpusfor
shouldbefixed,andtestcasesshouldmeasure
modelpretrainingandmodeleditingthatisalsode-
whetherupdatedLLMbeliefsareappropriately
rivedfromaformallanguage,asourcorpusis. Un-
confident,generalizing,andlogicallycoherent.
likeourlanguage,however,theirformallanguage
isbasedonaworldwherethereareasmallnumber 3. CanwedeterminewhatkindsofLLMsshould
ofentitiesthatrelatetooneanotheronlybyvirtue betreatedaseditableinthefirstplace(Sec.5)?
ofbeingordered(likethenaturalnumbers). Likein WhenareLLMslikerationalagents,asopposed
ourwork,theirevaluationalsomeasuresprobabilis- to agent simulators or databases? Do LLMs
ticandlogicalcoherence,withthegoalofcompar- haveexistingcomputationalmechanismsforbe-
ing the language model against certain Bayesian liefrevisionandcommunicatingconfidencethat
epistemicnorms. YettheirBayesianmetricsfocus modeleditingmethodsshouldbeleveraging?
onhowalanguagemodel’sposteriorprobabilities 4. Canweuseformalbenchmarksfordeveloping
compare to its own prior probabilities (a kind of bettermodeleditingapproaches(Sec.6)? When
self-consistency). Theydonotcompareagainstan canwesayexactlywhatanidealBayesianagent
idealized rational agent (Bayesian model), as we wouldbelieve,andcanwemimicBayesianbe-
do. Therefore, we believe our experiments cover liefrevisionbyeditingLLMweights?
a more naturalistic pretraining setting, while our
Limitations
metricsfocusmoredirectlyonhowarationalagent
wouldrespondtonewevidence. Ourdiscussionof
This paper discusses theoretical challenges with
12openproblemsformodeleditinghelpmotivate
model editing and explores an empirical setting
the formalized settings present in both this paper
for model editing evaluation. While we describe
andBetzandRichardson(2023).
twelvecorechallenges,thisisnotanexhaustivelist,
andwemustoccasionallypointtootherworkfor
8 Conclusion&FutureDirections
furtherelaborationoftherelevantissues. Forsome
This paper presents a critique of the standard challengesweleaveittofutureworktointroduce
formulation of the model editing problem and possiblewaysforward.
introduces a semi-synthetic testbed for model Second, our empirical results are first and
editingresearch. Ourcritiquefocuseson12open foremost a demonstration of what a more formal
problems with model editing, divided into issues evaluationofmodeleditingcanlooklike. Indoing
with (1) defining the problem, (2) developing so,weformulateonesemi-syntheticdatadistribu-
datasets for the problem, and (3) treating LLMs tionandintroduceonestandardkindofBayesian
as having editable beliefs to begin with. In modeltoserveasanidealizedrationalagent(i.e.,
response to these issues, we introduce a more to obtain exact posterior probability labels for
formal, semi-synthetic setting for model editing model editing). While we may want LLMs to
experiments. Weevaluatemodeleditingagainstthe mimicBayesianbeliefrevision,therearecertainly
standardofarationalBayesianagent. Usingdata manydesignchoicestobemadeindecidingwhich
15Bayesian model they should mimic, and future References
work can explore these design choices further.
Josh Achiam, Steven Adler, Sandhini Agar-
Forexample,aBayesianapproachcouldperform
wal, Lama Ahmad, Ilge Akkaya, Floren-
causaldiscoverytodecidewhichrelationsshould
cia Leoni Aleman, Diogo Almeida, Janko Al-
dependonwhichotherrelations(weassumeaccess
tenschmidt, Sam Altman, Shyamal Anadkat,
to the known causal graph in our model) or use
et al. 2023. Gpt-4 technical report. arXiv
a hierarchical model to share evidence between
preprintarXiv:2303.08774.
relateddistributions(perhapspeoplefromHarvard
have similar occupations to those from Yale).
CarlosE.Alchourrón,PeterGärdenfors,andDavid
Additionally,wetrainarelativelysmalllanguage
Makinson.1985. Onthelogicoftheorychange:
model on our corpus, and as a result our LM is
Partialmeetcontractionandrevisionfunctions.
abletomemorizethedatasetbutlackscapabilities
TheJournalofSymbolicLogic,50(2):510–530.
of larger models, like some basic logical compe-
tencies. UsingalargerLMcouldshowthatmodel BadrAlKhamissi,MillicentLi,AsliCelikyilmaz,
editingismoresuccessfulwhenevaluatedagainst MonaDiab,andMarjanGhazvininejad.2022. A
ourBayesianmodel–butthisispreciselythepoint reviewonlanguagemodelsasknowledgebases.
of formalizing the evaluation, and we hope that arXivpreprintarXiv:2204.06031.
future work can develop effective model editing
JacobAndreas.2022. Languagemodelsasagent
methodsintheformalframeworkweintroduce.
models. arXivpreprintarXiv:2212.01681.
BroaderImpacts
Gregor Betz and Kyle Richardson. 2023. Prob-
Initsmostambitiousform,thegoalofmodeledit-
abilistic coherence, logical consistency, and
ing is to control what LLMs believe to be true,
bayesian learning: Neural language models as
including beliefs about the empirical state of the
epistemicagents. Plosone,18(2):e0281372.
worldaswellasbeliefsaboutmoralvaluesorwor-
thy goals to pursue. This kind of control will be
Davis Brown, Charles Godfrey, Cody Nizinski,
importantfordevelopingsafeandadaptableAIsys-
Jonathan Tu, and Henry Kvinge. 2023. Edit
tems. Atpresent,modeleditinghasbeenappliedto
at your own risk: evaluating the robustness
importantsafetyproblemslikeunlearningsensitive
of edited models to distribution shifts. arXiv
information in LLMs (Patil et al., 2023; Li et al.,
preprintarXiv:2303.00046.
2024). In the future, model editing will be valu-
ableformakingfine-grainedadjustmentstoother Stephen Casper, Xander Davies, Claudia Shi,
beliefsinLLMsthathelpguidetheirbehaviorto ThomasKrendlGilbert,JérémyScheurer,Javier
be safer in deployment contexts, and to adapt to Rando, Rachel Freedman, Tomasz Korbak,
thechangingstateoftheworld. Withthisdirection DavidLindner,PedroFreire,etal.2023a. Open
in mind, this paper aims to point out flaws in the problems and fundamental limitations of rein-
currentmodeleditingparadigmanddemonstratea forcementlearningfromhumanfeedback. arXiv
directionforempiricalresearchthatwouldprovide preprintarXiv:2307.15217.
amorereliablesignalformethoddevelopment.
StephenCasper,JasonLin,JoeKwon,GatlenCulp,
Acknowledgements and Dylan Hadfield-Menell. 2023b. Explore,
establish,exploit: Redteaminglanguagemodels
WearethankfultoTomHartvigsen,DerekPowell,
fromscratch.
StephenCasper,KyleRichardson,andGregorBetz
for extensive conversations and feedback on this
Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen
work. This work was supported by NSF-AI En-
Zhao, He He, Jacob Steinhardt, Zhou Yu, and
gageInstituteDRL-2112635,DARPAMCSGrant
KathleenMcKeown.2023. Domodelsexplain
N66001-19-2-4031,DARPAECOLEProgramNo.
themselves? counterfactual simulatability of
HR00112390060,NSF-CAREERAward1846185,
natural language explanations. arXiv preprint
Google PhD Fellowship, and UNC SDSS Seed
arXiv:2307.08678.
Grant. The views, opinions, and/or findings con-
tained in this article are those of the authors and RoiCohen,EdenBiran,OriYoran,AmirGlober-
notofthefundingagency. son,andMorGeva.2024. Evaluatingtheripple
16effectsofknowledgeeditinginlanguagemodels. VinayakaGude.2023. Factorsinfluencingchatgpt
Transactions of the Association for Computa- adoption for product research and information
tionalLinguistics,12:283–298. retrieval. JournalofComputerInformationSys-
tems,pages1–10.
DamaiDai,LiDong,YaruHao,ZhifangSui,and
Furu Wei. 2021. Knowledge neurons in pre- Anshita Gupta, Debanjan Mondal, Akshay She-
trainedtransformers. shadri,WenlongZhao,XiangLi,SarahWiegr-
effe,andNiketTandon.2023. Editingcommon
NicolaDeCao,WilkerAziz,andIvanTitov.2021.
sense in transformers. In Proceedings of the
Editingfactualknowledgeinlanguagemodels.
2023ConferenceonEmpiricalMethodsinNatu-
In EMNLP, pages 6491–6506. Association for
ralLanguageProcessing,pages8214–8232.
ComputationalLinguistics.
Xiaoqi Han, Ru Li, Xiaoli Li, and Jeff Z Pan.
Edoardo Debenedetti, Giorgio Severi, Nicholas
2023. A divide and conquer framework for
Carlini, Christopher A. Choquette-Choo,
knowledgeediting. Knowledge-BasedSystems,
Matthew Jagielski, Milad Nasr, Eric Wallace,
279:110826.
andFlorianTramèr.2023. Privacysidechannels
inmachinelearningsystems. SvenOveHansson.2022. LogicofBeliefRevision.
In Edward N. Zalta, editor, The Stanford En-
Bhuwan Dhingra, Jeremy R. Cole, Julian Martin
cyclopediaofPhilosophy,Spring2022edition.
Eisenschlos, Daniel Gillick, Jacob Eisenstein,
MetaphysicsResearchLab,StanfordUniversity.
andWilliamW.Cohen.2022. Time-AwareLan-
guage Models as Temporal Knowledge Bases.
TomHartvigsen,SwamiSankaranarayanan,Hamid
Transactions of the Association for Computa-
Palangi, Yoon Kim, and Marzyeh Ghassemi.
tionalLinguistics,10:257–273.
2023. Agingwithgrace: Lifelongmodelediting
with discrete key-value adaptors. Advances in
KevinDu,VésteinnSnæbjarnarson,NiklasStoehr,
NeuralInformationProcessingSystems,36.
Jennifer C White, Aaron Schein, and Ryan
Cotterell. 2024. Context versus prior knowl-
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian
edge in language models. arXiv preprint
Li,ZornitsaKozareva,VeselinStoyanov,Mohit
arXiv:2404.04633.
Bansal,andSrinivasanIyer.2021. Dolanguage
modelshavebeliefs? methodsfordetecting,up-
SebastianFarquhar,VikrantVarma,ZacharyKen-
dating, and visualizing model beliefs. arXiv
ton,JohannesGasteiger,VladimirMikulik,and
preprintarXiv:2111.13654.
Rohin Shah. 2023. Challenges with unsuper-
visedllmknowledgediscovery. arXivpreprint
CarlGHempel.1945. Studiesinthelogicofcon-
arXiv:2312.10029.
firmation(i.). Mind,54(213):1–26.
Constanza Fierro, Nicolas Garneau, Emanuele
PeterHenderson,EricMitchell,ChristopherMan-
Bugliarello, Yova Kementchedjhieva, and An-
ning, Dan Jurafsky, and Chelsea Finn. 2023.
ders Søgaard. 2024. Mulan: A study of fact
Self-destructing models: Increasing the costs
mutability in language models. arXiv preprint
of harmful dual uses of foundation models. In
arXiv:2404.03036.
Proceedingsofthe2023AAAI/ACMConference
Branden Fitelson and James Hawthorne. 2010. onAI,Ethics,andSociety,pages287–296.
HowBayesianconfirmationtheoryhandlesthe
EvanHernandez,BelindaZLi,andJacobAndreas.
paradoxoftheravens. Springer.
2023. Inspectingandeditingknowledgerepre-
GovindGangadharandKarlStratos.2024. Model sentations in language models. arXiv preprint
editing by pure fine-tuning. arXiv preprint arXiv:2304.00740.
arXiv:2402.11078.
JasonHoelscher-Obermaier,JuliaPersson,Esben
MitchellGreen.2021. SpeechActs. InEdwardN. Kran, Ioannis Konstas, and Fazl Barez. 2023.
Zalta,editor,TheStanfordEncyclopediaofPhi- Detecting edit failures in large language mod-
losophy, Fall 2021 edition. Metaphysics Re- els: Animprovedspecificitybenchmark. arXiv
searchLab,StanfordUniversity. preprintarXiv:2305.17553.
17Thomas Hofweber, Peter Hase, Elias Stengel- Unveiling the pitfalls of knowledge editing
Eskin, and Mohit Bansal. 2024. Are language for large language models. arXiv preprint
models rational? the case of coherence norms arXiv:2310.02129.
andbeliefrevision.
HantiLin.2024. BayesianEpistemology. InEd-
WenyueHua,JiangGuo,MingwenDong,Henghui ward N. Zalta and Uri Nodelman, editors, The
Zhu,PatrickNg,andZhiguoWang.2024. Prop- StanfordEncyclopediaofPhilosophy,Summer
agation and pitfalls: Reasoning-based assess- 2024edition.MetaphysicsResearchLab,Stan-
mentofknowledgeeditingthroughcounterfac- fordUniversity.
tualtasks. arXivpreprintarXiv:2401.17585.
Alisa Liu, Zhaofeng Wu, Julian Michael, Alane
ZhengbaoJiang,FrankFXu,JunAraki,andGra- Suhr, Peter West, Alexander Koller, Swabha
ham Neubig. 2020. How can we know what Swayamdipta, Noah A Smith, and Yejin
languagemodelsknow? TransactionsoftheAs- Choi. 2023. We’re afraid language models
sociationforComputationalLinguistics,8:423– aren’t modeling ambiguity. arXiv preprint
438. arXiv:2304.14399.
NitishJoshi,JavierRando,AbulhairSaparov,Na- ShayneLongpre,KartikPerisetla,AnthonyChen,
joungKim,andHeHe.2024. Personasasaway Nikhil Ramesh, Chris DuBois, and Sameer
tomodeltruthfulnessinlanguagemodels. Singh. 2021. Entity-based knowledge con-
flicts in question answering. arXiv preprint
Dmitrii Krasheninnikov, Egor Krasheninnikov, arXiv:2109.05052.
BrunoMlodozeniec,TeganMaharaj,andDavid
SourabMangrulkar,SylvainGugger,LysandreDe-
Krueger.2024. Implicitmeta-learningmaylead
but,YounesBelkada,SayakPaul,andBenjamin
languagemodelstotrustmorereliablesources.
Bossan.2022. Peft: State-of-the-artparameter-
Angeliki Lazaridou, Adhi Kuncoro, Elena Gri- efficientfine-tuningmethods. https://github.
bovskaya,DevangAgrawal,AdamLiska,Tay- com/huggingface/peft.
fun Terzi, Mai Gimenez, Cyprien de Mas-
SamuelMarksandMaxTegmark.2023. Thegeom-
sond’Autume,TomasKocisky,SebastianRuder,
etryoftruth: Emergentlinearstructureinlarge
etal.2021. Mindthegap: Assessingtemporal
language model representations of true/false
generalization in neural language models. Ad-
datasets. arXivpreprintarXiv:2310.06824.
vances in Neural Information Processing Sys-
tems,34:29348–29363.
Kevin Meng, David Bau, Alex Andonian, and
Yonatan Belinkov. 2022. Locating and editing
OmerLevy,MinjoonSeo,EunsolChoi,andLuke
factualknowledgeingpt. InNeurIPS2022.
Zettlemoyer. 2017. Zero-shot relation extrac-
tionviareadingcomprehension. InProceedings MistralAI.2023. AnnouncingMistral7B. Blog-
of the 21st Conference on Computational Nat- post.
uralLanguageLearning(CoNLL2017),pages
333–342, Vancouver, Canada. Association for Eric Mitchell, Charles Lin, Antoine Bosselut,
ComputationalLinguistics. ChelseaFinn,andChristopherDManning.2021.
Fast model editing at scale. arXiv preprint
David Lewis. 1979. Counterfactual dependence arXiv:2110.11309.
andtime’sarrow. Noûs,pages455–476.
Eric Mitchell, Charles Lin, Antoine Bosselut,
NathanielLi,AlexanderPan,AnjaliGopal,Sum- ChristopherDManning,andChelseaFinn.2022.
mer Yue, Daniel Berrios, Alice Gatti, Justin D Memory-based model editing at scale. In In-
Li,Ann-KathrinDombrowski,ShashwatGoel, ternational Conference on Machine Learning,
LongPhan,etal.2024. Thewmdpbenchmark: pages15817–15831.PMLR.
Measuringandreducingmalicioususewithun-
Yixin Nie, Xiang Zhou, and Mohit Bansal. 2020.
learning. arXivpreprintarXiv:2403.03218.
Whatcanwelearnfromcollectivehumanopin-
Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Men- ionsonnaturallanguageinferencedata? arXiv
gru Wang, Xi Chen, and Huajun Chen. 2023. preprintarXiv:2010.03532.
18YasumasaOnoe,MichaelJQZhang,ShankarPad- emerges from the locality of experience. Ad-
manabhan,GregDurrett,andEunsolChoi.2023. vances in Neural Information Processing Sys-
Can lms learn new entities from descriptions? tems,36.
challenges in propagating injected knowledge.
arXivpreprintarXiv:2305.01651. XiangyuQi,YiZeng,TinghaoXie,Pin-YuChen,
Ruoxi Jia, Prateek Mittal, and Peter Hender-
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo son.2023. Fine-tuningalignedlanguagemodels
Almeida,CarrollWainwright,PamelaMishkin, compromisessafety,evenwhenusersdonotin-
Chong Zhang, Sandhini Agarwal, Katarina tendto! arXivpreprintarXiv:2310.03693.
Slama,AlexRay,etal.2022. Traininglanguage
WillardVQuineandJosephSilbertUllian.1970.
modelstofollowinstructionswithhumanfeed-
Thewebofbelief.
back. AdvancesinNeuralInformationProcess-
ingSystems,35:27730–27744.
DavidRein,BettyLiHou,AsaCooperStickland,
Jackson Petty, Richard Yuanzhe Pang, Julien
VaidehiPatil,PeterHase,andMohitBansal.2023.
Dirani,JulianMichael,andSamuelRBowman.
Cansensitiveinformationbedeletedfromllms?
2023. Gpqa: Agraduate-levelgoogle-proofq&a
objectives for defending against extraction at-
benchmark. arXivpreprintarXiv:2311.12022.
tacks. arXivpreprintarXiv:2309.17410.
Adam Roberts, Colin Raffel, and Noam Shazeer.
ElliePavlickandTomKwiatkowski.2019. Inher-
2020. Howmuchknowledgecanyoupackinto
entdisagreementsinhumantextualinferences.
theparametersofalanguagemodel? InProceed-
Transactions of the Association for Computa-
ingsofthe2020ConferenceonEmpiricalMeth-
tionalLinguistics,7:677–694.
odsinNaturalLanguageProcessing(EMNLP),
pages5418–5426,Online.AssociationforCom-
FabioPetroni,TimRocktäschel,SebastianRiedel,
putationalLinguistics.
PatrickLewis,AntonBakhtin,YuxiangWu,and
Alexander Miller. 2019. Language models as
Mrinank Sharma, Meg Tong, Tomasz Korbak,
knowledgebases? InProceedingsofthe2019
David Duvenaud, Amanda Askell, Samuel R
Conference on Empirical Methods in Natural
Bowman, Newton Cheng, Esin Durmus, Zac
LanguageProcessingandthe9thInternational
Hatfield-Dodds, Scott R Johnston, et al. 2023.
JointConferenceonNaturalLanguageProcess-
Towardsunderstandingsycophancyinlanguage
ing(EMNLP-IJCNLP),pages2463–2473,Hong
models. arXivpreprintarXiv:2310.13548.
Kong, China. Association for Computational
Linguistics. Herbert A Simon. 1956. Rational choice and the
structureoftheenvironment. Psychologicalre-
Yuval Pinter and Michael Elhadad. 2023. Emp-
view,63(2):129.
tying the ocean with a spoon: Should we edit
models? In Findings of the Association for NateSoares,BenjaFallenstein,StuartArmstrong,
ComputationalLinguistics: EMNLP2023,pages andEliezerYudkowsky.2015. Corrigibility. In
15164–15172,Singapore.AssociationforCom- WorkshopsattheTwenty-NinthAAAIConference
putationalLinguistics. onArtificialIntelligence.
BarbaraPlank.2022. The’problem’ofhumanlabel W. Starr. 2022. Counterfactuals. In Edward N.
variation: Ongroundtruthindata,modelingand ZaltaandUriNodelman,editors,TheStanford
evaluation. arXivpreprintarXiv:2211.02570. Encyclopedia of Philosophy, Winter 2022 edi-
tion.MetaphysicsResearchLab,StanfordUni-
Derek Powell, Walter Gerych, and Thomas versity.
Hartvigsen.2024. Taxi: Evaluatingcategorical
knowledgeeditingforlanguagemodels. arXiv AmosTverskyandDanielKahneman.1974. Judg-
preprintarXiv:2404.15004. ment under uncertainty: Heuristics and bi-
ases: Biases in judgments reveal some heuris-
BenPrystawski,MichaelLi,andNoahGoodman. tics of thinking under uncertainty. science,
2023. Why think step by step? reasoning 185(4157):1124–1131.
19Alexandra N Uma, Tommaso Fornaciari, Dirk Twelfth International Conference on Learning
Hovy, Silviu Paun, Barbara Plank, and Mas- Representations.
simoPoesio.2021. Learningfromdisagreement:
Rongwu Xu, Brian S Lin, Shujian Yang, Tianqi
Asurvey. JournalofArtificialIntelligenceRe-
Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan
search,72:1385–1470.
Fang, Wei Xu, and Han Qiu. 2023. The earth
Denny Vrandecˇic´ and Markus Krötzsch. 2014. is flat because...: Investigating llms’ belief to-
Wikidata: a free collaborative knowledgebase. wardsmisinformationviapersuasiveconversa-
CommunicationsoftheACM,57(10):78–85. tion. arXivpreprintarXiv:2312.09085.
AlexanderWan,EricWallace,andDanKlein.2024. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan
Whatevidencedolanguagemodelsfindconvinc- Cheng,ZhouboLi,ShuminDeng,HuajunChen,
ing? arXivpreprintarXiv:2402.11782. andNingyuZhang.2023. Editinglargelanguage
models: Problems,methods,andopportunities.
PengWang,NingyuZhang,XinXie,YunzhiYao,
arXivpreprintarXiv:2305.13172.
BozhongTian,MengruWang,ZekunXi,Siyuan
Cheng, Kangwei Liu, Guozhou Zheng, et al. Zexuan Zhong, Zhengxuan Wu, Christopher D
2023a. Easyedit: An easy-to-use knowledge Manning, Christopher Potts, and Danqi Chen.
editing framework for large language models. 2023. Mquake: Assessingknowledgeeditingin
arXivpreprintarXiv:2308.07269. languagemodelsviamulti-hopquestions. arXiv
preprintarXiv:2305.14795.
Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi
Zheng, Chen Chen, et al. 2023b. Knowledge Kaitlyn Zhou, Jena D Hwang, Xiang Ren, and
editing for large language models: A survey. Maarten Sap. 2024. Relying on the unreli-
arXivpreprintarXiv:2310.16218. able: The impact of language models’ reluc-
tance to express uncertainty. arXiv preprint
Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu,
arXiv:2401.06730.
Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and
Jian Tang. 2021. Kepler: A unified model for Xiang Zhou, Yixin Nie, and Mohit Bansal. 2021.
knowledgeembeddingandpre-trainedlanguage Distributednli: Learningtopredicthumanopin-
representation. TransactionsoftheAssociation iondistributionsforlanguagereasoning. arXiv
forComputationalLinguistics,9:176–194. preprintarXiv:2104.08676.
Yike Wang, Shangbin Feng, Heng Wang, Wei-
Chen Zhu, Ankit Singh Rawat, Manzil Zaheer,
jia Shi, Vidhisha Balachandran, Tianxing He,
Srinadh Bhojanapalli, Daliang Li, Felix Yu,
and Yulia Tsvetkov. 2023c. Resolving knowl-
and Sanjiv Kumar. 2020. Modifying mem-
edgeconflictsinlargelanguagemodels. arXiv
ories in transformer models. arXiv preprint
preprintarXiv:2310.00935.
arXiv:2012.00363.
Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu
A AdditionalDetailsforFormalTestbed
Lei, Yixuan Weng, Ran Song, and Kang Liu.
forModelEditing
2023. Assessingknowledgeeditinginlanguage
modelsviarelationperspective. arXivpreprint A.1 PretrainingCorpusCreation
arXiv:2311.09053.
Weoutlinethepretrainingdatasetcreationprocess
in greater detail here. The steps are to (1) create
Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong
a knowledge graph from Wikidata, (2) define a
Dong, Shuangzhi Wu, Chao Bian, and Deyi
generative model from the knowledge graph, (3)
Xiong. 2023. Depn: Detecting and editing
createthepretrainingdata.
privacyneuronsinpretrainedlanguagemodels.
(1) First, we subsample Wikidata (Vrandecˇic´
arXivpreprintarXiv:2310.20138.
and Krötzsch, 2014; Wang et al., 2021), reading
Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, the first 2m tuples, and excluding relations P735
andYuSu.2023. Adaptivechameleonorstub- (givenname)andP131(locationintheadminstra-
bornsloth: Revealingthebehavioroflargelan- tiveterritortyof)andentitiesQ16521(taxon)and
guage models in knowledge conflicts. In The Q5 (human). We filter the observed relations in
20the subset to those that occur alongside another tohaveprobabilityatleast0.6(see(2)above). This
relationforasubjectentityatleast1000times(i.e. isdonesothatthecorpuscanbememorizedexactly.
every relation appears at least 1000 times for an TheargmaxoutputsfortheLMshouldbecompa-
entityforwhichanotherspecificrelationisalsoob- rabletothegroundtruthobjectsinthegenerative
served). Weselectthetop10co-occuringrelations model, allowing us to check generative accuracy
by count. This is to ensure these is enough data of the language model. Besides making the 10
forlearningdependenciesbetweencertainpairsof sentencesper(s,r),weincludelogicallycomplex
relations. We further filter relations to be 1:1 by sentences for each subject entity. For each (s,r),
selectinganarbitraryobjectentitywheneverarela- we also create 10 True/False sentences (see data
tionis1:Nforasubjectentity. Thisyieldsatotal example 2), with True/False values in exact pro-
of160kfactsinaknowledgegraph. portion to the proportion of true sampled objects
in the 10 noisy sentences. Then, we generate 20
(2) To assign dependencies between relations,
logicallycomplexsentencespersubject,selecting
wecomputeco-occurrenceratesbetweeneachpair
amixtureofand/or/notsentences. The‘and’and
of relations, yielding a 10 × 10 matrix, where
‘or’sentencesmakeuseofotherrandomsentences
each row and column sum to 1. We select pairs
fromthedatasetthatmaybetrueorfalse. The‘not’
of frequently co-occurring relations by solving a
sentencesrandomlyusesthegroundtruthobjectfor
linear program over this matrix, to optimize for
(s,r)(meaningthesentencereadsas: “not sro”is
overallfrequencyofpairedupstream/downstream
false)oradistractorobjecto (meaningthesen-
relations. This produces the pairing in Table. 2. false
tencereadsas: “notsr o ”isfalse). Allofthe
With these dependencies determined, we define false
and/or/notsentenceshavecorrecttrue/falselabels
a generative model for producing sentences, i.e.
inthepretrainingdataset, toeaselearningoflog-
p(o|s,r,UpstreamProperty). This in done in the
icalconnectives. Onlythe“sr o”andTrue/False
manner described in Sec. 6.1. We use empirical
sentencesarenoisy. Thisprocessisrepeateduntil
distributionsfromtheknowledgegraph. Notewe
wehaveused100kfactsfromtheknowledgegraph
set a minimum ground-truth probability to 0.6,
in the pretraining dataset. The result is a dataset
meaning that the true fact from the knowledge
withstatisticsshowninTable6.
graph must have at least a probability of 0.6 for
p(o|s,r,UpstreamProperty = ∅). Whendefining
A.2 EditingBenchmarkDetails
p(o|s,r,UpstreamProperty), we check what the
modalobjectis,andthenrenormalizetheprobabil-
Oncewehavepretrainingdata,wecreatetheedit-
itydistributionsothatitsprobabilityisatleast0.6.
ing benchmark in two steps: (1) fit a Bayesian
This is done to make the dataset easier to memo-
modeltothepretrainingdata,(2)createtestcases.
rize correct answers for. Note that about 20% of
(1) Described in Sec. 6.2, fitting the Bayesian
theobservedfactshaveaknownupstreamrelation
model to the pretraining dataset mostly involves
for their subject entity. This means that 80% of
counting how many times it is observed that
the facts in our data are basic facts that must be
Arthur Green occupation architect as op-
memorized,while20%arestatisticallyimpliedby
posedtoArthur Green occupation carpenter.
theupstreampropertiesfortheirsubjectentity.
Thesecountsasusedtocomputetheposteriorpre-
(3) To generate the pretraining data, we ran- dictive p(o|s,r,UpstreamProperty = ∅). Count-
domlyselectsubjectentitiesfromourknowledge ing evidence for p(o|s,r,UpstreamProperty) is
graphandnoisilysamplesentencesforthecorpus alittlecomplicated. Wewishtoknowhowmany
usingthegenerativemodeluntilwehavereached times,e.g.,someonefromHarvardisanarchitect,
100k atomic facts. For each subject and relation, someone from Yale is an architect, etc. What
we check whether it has a known upstream rela- makes this complicated is that educated at is
tion(thishappens20%ofthetime),andselectthe itself a noisy relation in our pretraining dataset.
empiricaldistributiontouseforsamplingbasedon WhethersomeonewenttoHarvardvs. Yaleisoften
this(Sec6.1). Wesampleonly10objectsforthis notcertain,soifthatpersonisanarchitect,itisnot
facttogointothepretrainingcorpus. Noteweper- clear which conditional distribution to count this
formrejectionsamplingsothatatleastsixofthese evidencefor. Thisisissueisresolvedbyweighting
10objectsarethe“groundtruth”(modal)objectfor observedoccupationcountsbyfrequenciesofob-
theircorrespondingdistribution,whichisenforced servedupstreamproperties. Soif80%ofthetime,
21GenerativeAccuracy↑ ProbabilisticCoherence↓ LogicalCoherence↓
Data+LM s1r1 s1r2 s2r1 s2r2 s1r1 s1r2 s2r1 s2r2 TF neg. and or
AllEditRequests
Pre-edit 0.96 0.93 0.92 0.92 0.23 0.24 0.24 0.24 0.40 0.22 0.34 0.21
Post-edit 1.00 0.76 0.91 0.92 0.05 0.27 0.23 0.24 0.52 0.23 0.34 0.21
∆ +.04 −.17 −.01 +.00 −.18 +.03 −.01 +.00 +.12 +.01 +.00 +.00
EditRequestsw/DownstreamAnswerChanges
Pre-edit 0.90 0.97 0.91 0.98 0.20 0.34 0.21 0.34 0.38 0.22 0.34 0.22
Post-edit 1.00 0.01 0.90 0.98 0.04 0.59 0.20 0.34 0.53 0.23 0.34 0.22
∆ +.10 −.96 −.01 +.00 −.16 +.25 −.01 +.00 +.15 +.01 +.00 +.00
FixingErrorsw.r.t. PretrainingFacts
Pre-edit 0.00 1.00 0.83 0.93 0.26 0.23 0.22 0.23 0.33 0.22 0.30 0.20
Post-edit 1.00 0.97 0.85 0.95 0.05 0.24 0.23 0.23 0.54 0.22 0.30 0.20
∆ +1.00 −.03 +.02 +.02 −.21 +.01 +.01 +.00 +.21 +.00 +.00 +.00
Table5: Modeleditingresults,includingeditsforfixingerrorsinmodeloutputswherethelanguagemodel
failedtolearnafactduringpretraining(section“FixingErrors”). Testdataissplitbasedonwhetherthe
answer to the downstream fact should change after editing. Generative Accuracy reflects whether the
editedLMoutputagreeswiththeBayesianmodelposteriorbeliefs. ProbabilisticCoherencemetricsare
MAEsagainstBayesianposteriorprobabilities. LogicalcoherencemetricsreflecthowtheLMviolates
logical axioms of probability. s1/s2 and r1/r2 indicate the subject and relation used in the sentence,
meanings1 r1isthesamepromptasusedinmodelediting,whiles1 r2isapossibledownstreamfact
(seeFig.4foranexample).
Statistic Number cated aspect of Bayesian inference is learning
TrueAtomicFacts 100k from logical sentences. True/False sentences
AtomicSentences 1m are straightforward to interpret, but and/not/or
T/FSentences 1m sentences are more difficult. Here, our Bayesian
and/or/notSentences 400k model leverages the fact that these complex
Sentences 2.4m sentences are never noisy (always contain true
Documents 1.26m atomic sentences). We compute probabilities
Tokens 204m like p(sent istrue|(sent ∨ sent )istrue),
1 1 2
SubjectEntities 47k and use these probabilities as weights for
Relations 10 learning from the individual sentences in
ObjectEntities 20k Bayesianmodel. Thesecomputationsaredonein
data_classes/dataset_generator.py in our
Table6: Pretrainingdatasetstatistics.
code.
(2) To create test cases for editing, we ran-
whenweseeArthur Green,weseeArthur Green domlysamplesubjectentitiesthatwereseendur-
educated at Harvard, and 20% fo the time we ing pretraining, then formulate test cases of the
see Arthur Green educated at Yale, then we kind shown in Fig. 2. The edit requests them-
taketheirobservedoccupationcounts⃗oandadd.8⃗o selves reinforce the ground truth example in the
to the evidence for p(o|s,r = occupation,r = pretrainingdatahalfthetime(resultsforfixinger-
u
educated at,o = Harvard) and .2⃗o to the rorsonlyinTable5)andcontradictthepretraining
u
evidence for p(o|s,r = occupation,r = datahalfthetime(asatypicalcounterfactualedit
u
educated at,o = Yale). Recall that we request would). The cases for measuring prob-
u
aggregate this evidence across subject entities, abilistic coherence relate to the edit request in
and when determining someone’s occupation termsofwhethertheyusethesameordifferentsub-
(which is downstream of educated at in our jects/relations. Thepossibledownstreamrelation
hypothetical world) it does not matter what the ispresentinthecasefors1 r2, andwepreferen-
subject entity is (all subjects share the same tiallychoosetestcasesherewhereweexpectthe
conditional distribution). The other compli- downstream answer to change when updating on
22the requested edit (we do this 80% of the time). leftandrighthandsidesofthefollowingequations
Thelogicalcoherencecasesuseanotherrandomly reflectingbasiclogicalaxiomsofprobability:
selectedsentenceB,andweformTrue/False,and,
or, and not sentences based on the edit request
TF:p (o|“sr”)=p (True|“sro”is)
sentenceandthesentenceB. Forallofthesesen- θ θ
not: p (True|“sro”is)=1−p (True|not“sro”is)
tences,theBayesianmodelgivesposteriorproba- θ θ
or: p (True|“s r o ”∨“s r o ”is)=
bilitiesafterupdatingontheeditrequest. Forevery θ 1 1 1 2 2 2
testcase,weupdatetheBayesianmodelwiththe p θ(True|“s 1r 1o 1”is)+p θ(True|“s 2r 2o 2”is)−
editrequest,andaftereachtestcase,weresetthe p (True|“s r o ”is)∗p (True|“s r o ”is)
θ 1 1 1 θ 2 2 2
Bayesian model so that it once again reflects the and: p (True|“s r o ”∧“s r o ”is)=
θ 1 1 1 2 2 2
pretrainingdata. Thisupdatingprocessusesoneof p (True|“s r o ”is)∗p (True|“s r o ”is)
θ 1 1 1 θ 2 2 2
two weights, n = 1000 or n′ (see Sec. 6.2). The
resultsinthispaperalwaysusen′.
We note that our model easily learns that
A.3 LMPretrainingDetails
p (True|“sro”is)andp (False|“sro”is)should
θ θ
For our language model, we use the architecture sumto1,sowecancomputealloftheabovemet-
frommistralai/Mistral-7B-v0.1. Wescalethe ricsusing“True”asthetargetprobability.
modeldownto83mparametersbysettinghidden
size = 512,intermediate size = 512*4,num
attention heads = 8, num hidden layers =
12. Wetrainwithoutdropout,usingabatchsizeof
128documents. Trainingfor1btokenstakesabout
24hoursinanNVIDIAA6000GPU.
A.4 ModelEditingDetails
We use LoRA because it has been shown to be
competitive to SOTA methods like ROME (Hua
et al., 2024; Gangadhar and Stratos, 2024). In
our setting, our LoRA implementation is based
onpeft(Mangrulkaretal.,2022)andisextremely
similar to ROME. We perform rank-one updates
to down-projection MLP layers in our language
model. Since our formal language does not have
paraphrasesoran“is”relation,wecouldnotlever-
agetheparaphraseoressenceregularizationlosses
in ROME, making the methods even more simi-
lar in our setting. We use LoRA to optimize the
probabilityofthetargettokenconditionedonthe
prompt“sr”,foratotalof40gradientsteps. This
is sufficient to achieve 100% accuracy on s1 r1
test cases (using the exact prompt as in the edit
request).
A.5 LogicalCoherenceMetrics
The probabilistic coherence metrics in Table 4
aremeanabsoluteerrorsbetweenlanguagemodel
probabilities and Bayesian posteriors. While we
showsimilaridealizedBayesianprobabilitiesfor
logicallycomplexsentencesinFig.4,thelogical
coherence metrics in Table 4 aim to reflect inter-
nallogicalcoherenceofthelanguagemodel. We
thus compute mean absolute errors between the
23