[
    {
        "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
        "authors": "Vedang LadWes GurneeMax Tegmark",
        "links": "http://arxiv.org/abs/2406.19384v1",
        "entry_id": "http://arxiv.org/abs/2406.19384v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19384v1",
        "summary": "We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.",
        "updated": "2024-06-27 17:57:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19384v1"
    },
    {
        "title": "Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space",
        "authors": "Core Francisco ParkMaya OkawaAndrew LeeEkdeep Singh LubanaHidenori Tanaka",
        "links": "http://arxiv.org/abs/2406.19370v1",
        "entry_id": "http://arxiv.org/abs/2406.19370v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19370v1",
        "summary": "Modern generative models demonstrate impressive capabilities, likely stemming\nfrom an ability to identify and manipulate abstract concepts underlying their\ntraining data. However, fundamental questions remain: what determines the\nconcepts a model learns, the order in which it learns them, and its ability to\nmanipulate those concepts? To address these questions, we propose analyzing a\nmodel's learning dynamics via a framework we call the concept space, where each\naxis represents an independent concept underlying the data generating process.\nBy characterizing learning dynamics in this space, we identify how the speed at\nwhich a concept is learned, and hence the order of concept learning, is\ncontrolled by properties of the data we term concept signal. Further, we\nobserve moments of sudden turns in the direction of a model's learning dynamics\nin concept space. Surprisingly, these points precisely correspond to the\nemergence of hidden capabilities, i.e., where latent interventions show the\nmodel possesses the capability to manipulate a concept, but these capabilities\ncannot yet be elicited via naive input prompting. While our results focus on\nsynthetically defined toy datasets, we hypothesize a general claim on emergence\nof hidden capabilities may hold: generative models possess latent capabilities\nthat emerge suddenly and consistently during training, though a model might not\nexhibit these capabilities under naive input prompting.",
        "updated": "2024-06-27 17:50:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19370v1"
    },
    {
        "title": "Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?",
        "authors": "Peter HaseThomas HofweberXiang ZhouElias Stengel-EskinMohit Bansal",
        "links": "http://arxiv.org/abs/2406.19354v1",
        "entry_id": "http://arxiv.org/abs/2406.19354v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19354v1",
        "summary": "The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision",
        "updated": "2024-06-27 17:33:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19354v1"
    },
    {
        "title": "IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language",
        "authors": "Lucky SusantoMusa Izzanardi WijanarkoPrasetia Anugrah PratamaTraci HongIka IdrisAlham Fikri AjiDerry Wijaya",
        "links": "http://arxiv.org/abs/2406.19349v1",
        "entry_id": "http://arxiv.org/abs/2406.19349v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19349v1",
        "summary": "Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.",
        "updated": "2024-06-27 17:26:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19349v1"
    },
    {
        "title": "Efficient World Models with Context-Aware Tokenization",
        "authors": "Vincent MicheliEloi AlonsoFrançois Fleuret",
        "links": "http://arxiv.org/abs/2406.19320v1",
        "entry_id": "http://arxiv.org/abs/2406.19320v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19320v1",
        "summary": "Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.",
        "updated": "2024-06-27 16:54:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19320v1"
    }
]