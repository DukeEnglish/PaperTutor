[
    {
        "title": "Stochastic Gradient Piecewise Deterministic Monte Carlo Samplers",
        "authors": "Paul FearnheadSebastiano GrazziChris NemethGareth O. Roberts",
        "links": "http://arxiv.org/abs/2406.19051v1",
        "entry_id": "http://arxiv.org/abs/2406.19051v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19051v1",
        "summary": "Recent work has suggested using Monte Carlo methods based on piecewise\ndeterministic Markov processes (PDMPs) to sample from target distributions of\ninterest. PDMPs are non-reversible continuous-time processes endowed with\nmomentum, and hence can mix better than standard reversible MCMC samplers.\nFurthermore, they can incorporate exact sub-sampling schemes which only require\naccess to a single (randomly selected) data point at each iteration, yet\nwithout introducing bias to the algorithm's stationary distribution. However,\nthe range of models for which PDMPs can be used, particularly with\nsub-sampling, is limited. We propose approximate simulation of PDMPs with\nsub-sampling for scalable sampling from posterior distributions. The\napproximation takes the form of an Euler approximation to the true PDMP\ndynamics, and involves using an estimate of the gradient of the log-posterior\nbased on a data sub-sample. We thus call this class of algorithms\nstochastic-gradient PDMPs. Importantly, the trajectories of stochastic-gradient\nPDMPs are continuous and can leverage recent ideas for sampling from measures\nwith continuous and atomic components. We show these methods are easy to\nimplement, present results on their approximation error and demonstrate\nnumerically that this class of algorithms has similar efficiency to, but is\nmore robust than, stochastic gradient Langevin dynamics.",
        "updated": "2024-06-27 09:59:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19051v1"
    },
    {
        "title": "Accuracy on the wrong line: On the pitfalls of noisy data for out-of-distribution generalisation",
        "authors": "Amartya SanyalYaxi HuYaodong YuYian MaYixin WangBernhard Schölkopf",
        "links": "http://arxiv.org/abs/2406.19049v1",
        "entry_id": "http://arxiv.org/abs/2406.19049v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19049v1",
        "summary": "\"Accuracy-on-the-line\" is a widely observed phenomenon in machine learning,\nwhere a model's accuracy on in-distribution (ID) and out-of-distribution (OOD)\ndata is positively correlated across different hyperparameters and data\nconfigurations. But when does this useful relationship break down? In this\nwork, we explore its robustness. The key observation is that noisy data and the\npresence of nuisance features can be sufficient to shatter the\nAccuracy-on-the-line phenomenon. In these cases, ID and OOD accuracy can become\nnegatively correlated, leading to \"Accuracy-on-the-wrong-line\". This phenomenon\ncan also occur in the presence of spurious (shortcut) features, which tend to\novershadow the more complex signal (core, non-spurious) features, resulting in\na large nuisance feature space. Moreover, scaling to larger datasets does not\nmitigate this undesirable behavior and may even exacerbate it. We formally\nprove a lower bound on Out-of-distribution (OOD) error in a linear\nclassification model, characterizing the conditions on the noise and nuisance\nfeatures for a large OOD error. We finally demonstrate this phenomenon across\nboth synthetic and real datasets with noisy data and nuisance features.",
        "updated": "2024-06-27 09:57:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19049v1"
    },
    {
        "title": "Statistical Test for Data Analysis Pipeline by Selective Inference",
        "authors": "Tomohiro ShiraishiTatsuya MatsukawaShuichi NishinoIchiro Takeuchi",
        "links": "http://arxiv.org/abs/2406.18902v1",
        "entry_id": "http://arxiv.org/abs/2406.18902v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18902v1",
        "summary": "A data analysis pipeline is a structured sequence of processing steps that\ntransforms raw data into meaningful insights by effectively integrating various\nanalysis algorithms. In this paper, we propose a novel statistical test\ndesigned to assess the statistical significance of data analysis pipelines. Our\napproach allows for the systematic development of valid statistical tests\napplicable to any data analysis pipeline configuration composed of a set of\ndata analysis components. We have developed this framework by adapting\nselective inference, which has gained recent attention as a new statistical\ninference technique for data-driven hypotheses. The proposed statistical test\nis theoretically designed to control the type I error at the desired\nsignificance level in finite samples. As examples, we consider a class of\npipelines composed of three missing value imputation algorithms, three outlier\ndetection algorithms, and three feature selection algorithms. We confirm the\nvalidity of our statistical test through experiments with both synthetic and\nreal data for this class of data analysis pipelines. Additionally, we present\nan implementation framework that facilitates testing across any configuration\nof data analysis pipelines in this class without extra implementation costs.",
        "updated": "2024-06-27 05:30:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18902v1"
    },
    {
        "title": "From Biased Selective Labels to Pseudo-Labels: An Expectation-Maximization Framework for Learning from Biased Decisions",
        "authors": "Trenton ChangJenna Wiens",
        "links": "http://arxiv.org/abs/2406.18865v1",
        "entry_id": "http://arxiv.org/abs/2406.18865v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18865v1",
        "summary": "Selective labels occur when label observations are subject to a\ndecision-making process; e.g., diagnoses that depend on the administration of\nlaboratory tests. We study a clinically-inspired selective label problem called\ndisparate censorship, where labeling biases vary across subgroups and unlabeled\nindividuals are imputed as \"negative\" (i.e., no diagnostic test = no illness).\nMachine learning models naively trained on such labels could amplify labeling\nbias. Inspired by causal models of selective labels, we propose Disparate\nCensorship Expectation-Maximization (DCEM), an algorithm for learning in the\npresence of disparate censorship. We theoretically analyze how DCEM mitigates\nthe effects of disparate censorship on model performance. We validate DCEM on\nsynthetic data, showing that it improves bias mitigation (area between ROC\ncurves) without sacrificing discriminative performance (AUC) compared to\nbaselines. We achieve similar results in a sepsis classification task using\nclinical data.",
        "updated": "2024-06-27 03:33:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18865v1"
    },
    {
        "title": "Full Information Linked ICA: addressing missing data problem in multimodal fusion",
        "authors": "Ruiyang LiF. DuBois BowmanSeonjoo Lee",
        "links": "http://arxiv.org/abs/2406.18829v1",
        "entry_id": "http://arxiv.org/abs/2406.18829v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18829v1",
        "summary": "Recent advances in multimodal imaging acquisition techniques have allowed us\nto measure different aspects of brain structure and function. Multimodal\nfusion, such as linked independent component analysis (LICA), is popularly used\nto integrate complementary information. However, it has suffered from missing\ndata, commonly occurring in neuroimaging data. Therefore, in this paper, we\npropose a Full Information LICA algorithm (FI-LICA) to handle the missing data\nproblem during multimodal fusion under the LICA framework. Built upon complete\ncases, our method employs the principle of full information and utilizes all\navailable information to recover the missing latent information. Our simulation\nexperiments showed the ideal performance of FI-LICA compared to current\npractices. Further, we applied FI-LICA to multimodal data from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) study, showcasing better performance in\nclassifying current diagnosis and in predicting the AD transition of\nparticipants with mild cognitive impairment (MCI), thereby highlighting the\npractical utility of our proposed method.",
        "updated": "2024-06-27 01:50:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18829v1"
    }
]