[
    {
        "title": "Dataset Size Recovery from LoRA Weights",
        "authors": "Mohammad SalamaJonathan KahanaEliahu HorwitzYedid Hoshen",
        "links": "http://arxiv.org/abs/2406.19395v1",
        "entry_id": "http://arxiv.org/abs/2406.19395v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19395v1",
        "summary": "Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.",
        "updated": "2024-06-27 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19395v1"
    },
    {
        "title": "HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection",
        "authors": "Liujuan CaoJianghang LinZebo HongYunhang ShenShaohui LinChao ChenRongrong Ji",
        "links": "http://arxiv.org/abs/2406.19394v1",
        "entry_id": "http://arxiv.org/abs/2406.19394v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19394v1",
        "summary": "Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a\ncomprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic\nself-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline\nproposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.",
        "updated": "2024-06-27 17:59:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19394v1"
    },
    {
        "title": "Looking 3D: Anomaly Detection with 2D-3D Alignment",
        "authors": "Ankan BhuniaChangjian LiHakan Bilen",
        "links": "http://arxiv.org/abs/2406.19393v1",
        "entry_id": "http://arxiv.org/abs/2406.19393v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19393v1",
        "summary": "Automatic anomaly detection based on visual cues holds practical significance\nin various domains, such as manufacturing and product quality assessment. This\npaper introduces a new conditional anomaly detection problem, which involves\nidentifying anomalies in a query image by comparing it to a reference shape. To\naddress this challenge, we have created a large dataset, BrokenChairs-180K,\nconsisting of around 180K images, with diverse anomalies, geometries, and\ntextures paired with 8,143 reference 3D shapes. To tackle this task, we have\nproposed a novel transformer-based approach that explicitly learns the\ncorrespondence between the query image and reference 3D shape via feature\nalignment and leverages a customized attention mechanism for anomaly detection.\nOur approach has been rigorously evaluated through comprehensive experiments,\nserving as a benchmark for future research in this domain.",
        "updated": "2024-06-27 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19393v1"
    },
    {
        "title": "ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos",
        "authors": "Jr-Jen ChenYu-Chien LiaoHsi-Che LinYu-Chu YuYen-Chun ChenYu-Chiang Frank Wang",
        "links": "http://arxiv.org/abs/2406.19392v1",
        "entry_id": "http://arxiv.org/abs/2406.19392v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19392v1",
        "summary": "We introduce ReXTime, a benchmark designed to rigorously test AI models'\nability to perform temporal reasoning within video events. Specifically,\nReXTime focuses on reasoning across time, i.e. human-like understanding when\nthe question and its corresponding answer occur in different video segments.\nThis form of reasoning, requiring advanced understanding of cause-and-effect\nrelationships across video segments, poses significant challenges to even the\nfrontier multimodal large language models. To facilitate this evaluation, we\ndevelop an automated pipeline for generating temporal reasoning question-answer\npairs, significantly reducing the need for labor-intensive manual annotations.\nOur benchmark includes 921 carefully vetted validation samples and 2,143 test\nsamples, each manually curated for accuracy and relevance. Evaluation results\nshow that while frontier large language models outperform academic models, they\nstill lag behind human performance by a significant 14.3% accuracy gap.\nAdditionally, our pipeline creates a training dataset of 9,695 machine\ngenerated samples without manual effort, which empirical studies suggest can\nenhance the across-time reasoning via fine-tuning.",
        "updated": "2024-06-27 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19392v1"
    },
    {
        "title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
        "authors": "Ali Khaleghi RahimianManish Kumar GovindSubhajit MaityDominick ReillyChristian KümmerleSrijan DasAritra Dutta",
        "links": "http://arxiv.org/abs/2406.19391v1",
        "entry_id": "http://arxiv.org/abs/2406.19391v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19391v1",
        "summary": "Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.",
        "updated": "2024-06-27 17:59:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19391v1"
    }
]