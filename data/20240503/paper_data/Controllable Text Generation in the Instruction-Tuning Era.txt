Controllable Text Generation in the Instruction-Tuning Era
Dhananjay Ashok dhananja@cs.cmu.edu
School of Computer Science
Carnegie Mellon University
Barnabas Póczos bapoczos@cs.cmu.edu
School of Computer Science
Carnegie Mellon University
Abstract
While most research on controllable text generation has focused on steering base Language
Models, the emerging instruction-tuning and prompting paradigm offers an alternate ap-
proach to controllability. We compile and release ConGenBench (https://github.com/
DhananjayAshok/ConGenBench/), a testbed of 17 different controllable generation tasks,
using a subset of it to benchmark the performance of 9 different baselines and methods
on Instruction-tuned Language Models. To our surprise, we find that prompting-based
approaches outperform controllable text generation methods on most datasets and tasks,
highlightinganeedforresearchoncontrollabletextgenerationwithInstruction-tunedLan-
guage Models in specific. Prompt-based approaches match human performance on most
stylistic tasks while lagging on structural tasks, foregrounding a need to study more var-
ied constraints and more challenging stylistic tasks. To facilitate such research, we provide
an algorithm that uses only a task dataset and a Large Language Model with in-context
capabilities to automatically generate a constraint dataset. This method eliminates the
fields dependence on pre-curated constraint datasets, hence vastly expanding the range of
constraints that can be studied in the future.
1 Introduction
RecentadvancesinNaturalLanguageProcessing(NLP)(Jones,1994;Chowdhary&Chowdhary,2020)have
highlightedemergentcapabilitiesofLargeLanguageModels(LLMs)onawiderangeofNLPtasks(Weietal.,
2022; Brown et al., 2020). Despite their impressive performance, they can be hard to control and produce
outputs that are not in line with human intentions and values (Zamfirescu-Pereira et al., 2023; Cao et al.,
2023). Toachievewidespreadadoption,LLMsmustdemonstratethattheiroutputscanbereliablycontrolled
and aligned with the values and will of the end user.
Over the years, several methods have risen to the challenge of controllable text generation (Yang & Klein,
2021; Lu et al., 2022; Liu et al., 2021; Dathathri et al., 2020; Mireshghallah et al., 2022; Zhang et al.,
2023a). These methods typically employ a constraint function and use signals from this function to modify
the decoding procedure, sample from the LLM, or perform constraint-specific tuning. Most recent work has
focused on controlling base LLMs like GPT2, however, more recent work has also explored steering frontier
models like GPT3. While some focus on lexical constraints and structure, the primary problems studied
have been forms of stylistic control - specifically Toxicity Avoidance, Sentiment Control, and Topic Control.
In parallel, Instructions Tuning has emerged as a powerful technique that can enhance the capabilities and
controllability of LLMs (Ouyang et al., 2022; Zhang et al., 2023b). Today, the most widely used commercial
LLMproductsallrelyonsomeformofInstructionTuning(Achiametal.,2023;Buscemi&Proverbio,2024)
and prompting-based methods achieve impressive performance on myriad tasks (Wei et al., 2022).
Given these trends, we are interested in the following questions: (1) Which of the commonly studied con-
trollabletextgenerationproblemsremainchallengingforInstruction-tunedLLMs? (2)Dothemethodsthat
1
4202
yaM
2
]LC.sc[
1v09410.5042:viXraboost the controllability of base LLMs also help make Instruction-tuned LLMs more controllable? (3) How
do controllable text generation methods compare to prompting-based baselines?
To facilitate our study of this question we first introduce an algorithm that enables us to learn a constraint
function using only the intended task dataset (e.g. set of prompts) and an LLM with in-context learning
capabilities. This enables the usage of most prior methods using only a natural language description of the
constraint, allowing us to explore constraints for which a well-curated constraint dataset does not yet exist.
We then study the performance of 9 different baselines and methods, including a baseline for human per-
formance. We use human surveys to evaluate these methods on 7 different datasets with both stylistic and
structural constraints.
Figure1: Performanceofeachtypeofmethodwhenthetaskistomaketheoutputfollowstylisticconstraints.
Each point is a specific method/baseline run on a specific task dataset, the boxes show the mean and range
of the performance of the methods under a common type. While controllable text generation methods
(controllable) outperform simple baselines (baseline), they lag behind simple prompting-based approaches
(prompting). Promptingbasedapproachesarecompetitivewithhumanperformanceonmoststylistictasks.
The results reveal a surprising and concerning trend - controllable text generation methods consistently
underperform ZeroShot and FewShot prompting on instruction-tuned LLMs across all datasets and tasks.
We observe that prompting is competitive with human performance on several stylistic control tasks, while
thereisstillsignificantroomforimprovementonstructuralandlexicalconstraints. Theseobservationspoint
to an open area of future research - methods that enhance the controllability of Instruction-Tuned LLMs.
To enable further study on Controllable Text Generation we compile ConGenBench, a testbed of 17 dif-
ferent datasets spanning 7 different generation tasks with 18 different constraint datasets. We additionally
provide a general-purpose pipeline to create a constraint score function given only a natural language de-
scription of the constraint.
In summary, our contributions are
1. A novel algorithm to create constraint classifiers without access to a constraint dataset.
2. A systematic evaluation of controllable text generation methods on Instruction-Tuned LLMs.
3. ConGenBench, a testbed for controllable text generation methods.
22 Background
Given an input prompt made up of tokens from some vocabulary x ∈ Vt−1, the goal of controllable text
<t
generationistooutputtextx ...x ∈Vn−t+1 thatisbothinlinewiththerequirementsoftheprompt(e.g.
t n
continue the prompt, answer the question in the prompt) and satisfies a set of constraints (e.g. avoid toxic
outputs, use no more than 5 words).
Common approaches include weighted decoding methods (Dathathri et al., 2020; Yang & Klein, 2021; Lu
etal.,2022;Liuetal.,2021;Zhongetal.,2023;Mengetal.,2022;Guetal.,2022;Kimetal.,2022;Luetal.,
2023),thatattempttosteerthegenerationprocessatthetokenlevel. Thesemethodsfocusonautoregressive
models(Vaswanietal.,2017)whichconsiderP(x ,...x )=Qt P(x |x )andgeneratetokenssequentially
1 t i=1 i <i
with the ith token being generated using using the distribution P(x |x ). The weighted decoding methods
i <i
reweight the logits corresponding to P(x |x ) using a score function s(x ) ∈ R|V| (e.g. a neural network
i <i i
or manually written function) which estimates each token’s likelihood to produce a constraint-compliant
sequence. In special settings where it is possible to reliably segment subsequences of tokens, this approach
can be extended to score the subsequences and steer generation (Khalifa et al., 2023; Welleck et al., 2021).
AnotherdirectionistoconsiderthegenerationprocessassamplingfromanEnergyBasedModel(Zhaoetal.,
2016). Given a sequence level constraint function c(x ,...x ) and the LLM distribution P(x ,...x ) these
1 n 1 n
methods (Mireshghallah et al., 2022; Qin et al., 2022; Kumar et al., 2022) view the mixture c(x ...x )×
1 n
αP(x ,...,x ) (for some mixing parameter α∈R) as an EBM and develop strategies to sample from it.
1 n
Recently, tuning-based approaches (Zhang & Song, 2022; Ma et al., 2023; Qian et al., 2022) employ variants
of Prefix Tuning (Lester et al., 2021; Li & Liang, 2021) to train constraint-specific prefixes that when used
during generation produce more constraint-aligned sequences.
OtherapproachesincludethosethatcreateclassconditionalLLMs(Krauseetal.,2021), performcontrastive
learning to tune the network(Zheng et al., 2023), and employ retrieval augmented generation to produce
constraint-compliant outputs(Wen et al., 2023).
Thepredominanttestinggroundforthesemethodshasbeenlexicallyconstrainedgeneration,toxicityavoid-
ance,sentimentcontrol,andtopiccontrol. Morerecentworkhasoccasionallyexploredconstraintslikelogical
abduction (Qin et al., 2022) and knowledge grounding (Lu et al., 2023).
Algorithm 1 Synthetic Constraint Dataset Generation
1: Input: LLMtocontrol(LLM control),in-contextcapableLLM(LLM constraint),promptdataset,natural
language description for constraint, fewshot examples (optional)
2: Output: In-distribution synthetic constraint dataset
3: Step 1: Generate Text for Classification
4: for each prompt p in the prompt dataset do
5: Generate output o using LLM control conditioned on p
6:
Add (p,o) to the synthetic dataset
7: end for
8: Step 2: Label Examples with Constraints
9:
Usedescriptionand(optional)examplestocreatepromptcforin-contextpredictionwithLLM
constraint
10: for each example (p,o) in the synthetic dataset do
11: Prompt LLM constraint with c and o to obtain predicted label yˆ
12:
Assign predicted label yˆto the example
13: end for
14: Return Synthetic dataset with tuples (p,o,yˆ)
3(p, o, y): (And ignorance of them contributes, a thousand per cent to disease., 0.2)
Figure 2: Example datapoint (prompt, output, constraint score) using synthetic data generation algorithm
3 Removing dependence of controllable generation methods on constraint datasets
Nearly all of the methods in Section 2 rely on a sequence-level constraint score function whose output is
usedtodetermineconstraintsatisfaction. ForstylisticconstraintsthatcannotbeevaluatedwithsimpleNLP
approaches, this function is invariably an LLM classifier trained on a classification dataset corresponding to
the constraint in question. For example, virtually all work on Toxicity Avoidance uses classifiers trained on
the Jigsaw Unintended Bias in Toxicity Classification Challenge (Adams et al., 2019).
This dependence on classification datasets is a limiting factor in research on controllable text generation as
itrestrictstherangeofconstraintsunderstudy. Classificationdatasetsoftentakesignificanthumaneffortto
compile and are hard to modify once the data collection process is concluded. Additionally, researchers may
be interested in exploring settings where the notion of what counts as ‘positive sentiment’ or ‘toxic’ differs
slightly from the general conception of it as captured by the dataset; this is not possible using classifiers
trained on these classification datasets.
There may also be a label shift (Storkey et al., 2009) between the source and target distributions of the
constraint score classifier. The Jigsaw Dataset training split has less than 15% of its examples being labeled
as toxic which could be a significant underestimation of the prevalence of toxic output in deployment if the
LLM is meant to be used in especially toxic environments.
More concerning, however, is that there is almost always a domain shift between the source and target dis-
tributionsoftheconstraintscoreclassifier. Thetargetdistributionoftheclassifieristheoutputdistribution
of the LLM that must be controlled when using a specific prompt dataset, however, the source distribution
is a pre-curated dataset that did not consider the LLM or intended prompt dataset. Such domain shifts can
beseeninthesentimentcontrolsetting, wheremethodsoftenuseconstraintfunctionstrainedonthe‘movie
reviews’domainoftheIMDBdataset(Maasetal.,2011)butarethendeployedonLLMoutputswhengiven
prompts that are from ‘chat’ or ‘news’ like settings (Liu et al., 2021).
In an attempt to address the shift problem and expand the range of constraints that controllable text
generation methods can study, we introduce an approach that utilizes an LLM with in-context capabilities
to generate a constraint-specific classification dataset from the intended prompt dataset.
WefirstusetheLLM(Algorithm1)thatwehopetocontroltogenerateoutputswhenconditionedonprompts
from the intended prompt dataset. This serves as the text to be classified for our eventual dataset. We next
use Prompting (Brown et al., 2020) on a capable LLM (can either be the same or different than the one we
hope to control) to label each example as per the desired constraint specifications. Concretely, we create
a prompt with a natural language description of the constraint and optionally provide examples of the
classification task being performed correctly before asking the LLM to classify the text from the dataset
collected in the previous step. Provided the LLM can classify the examples reasonably well, this procedure
generates an in-distribution synthetic dataset that captures the constraint we are interested in.
Usingthisprocedurewecan,forthefirsttime,studyconstraintslike‘IsinthestyleofaGen-Zteenager’,‘Is
understandable by a 5-year-old’ or ‘Is written in the style of a British rapper’ and more without requiring
an entire dataset collection initiative.
4 Experimental Setup: Benchmarking controllable generation methods
4.1 Datasets and Tasks
We benchmark the performance of controllable generation methods on the most commonly adopted testing
grounds of toxicity avoidance, sentiment control and topic control.
4RealToxicityPrompts: Gehman et al. (2020) introduced a set of 100K prompts extracted from the Open-
WebText Corpus (Gokaslan & Cohen, 2019) (a large corpus of English web text scraped from outbound
URLs from Reddit). The task is to continue these prompts while avoiding toxic continuations.
Sentiment Prompts: Liu et al. (2021) similarly compiled a set of 100K prompts extracted from the
OpenWebTextCorpus. Thepromptsaresplitinto3setsbasedonwhetherthesentimentisneutral, positive
or negative. In this work, however, we combine all 3 sets into one. The task is to generate continuations
with a positive sentiment.
PPLM Prompts: Dathathri et al. (2020) uses a set of 20 general prompts (e.g. ‘In conclusion, ’, ‘This
essay discusses, ’) and sets a task of generating continuations that are in line with a specific topic. The
topics are often one of the categories in the AGNews dataset (Zhang et al., 2015), in this work we consider
the task to be generating continuations that are of a ‘World News’ topic.
We also introduce new testing scenarios with constraints that have yet to be studied by controllable text
generation methods.
CNNDailymail: TheCNNDailymail(Hermannetal.,2015)datasetisoriginallyasummarizationdataset,
but by taking only the first 3 sentences of the articles we can view it as a prompt dataset. The task is to
continue the prompts in a ‘sensationalist’ way, where ‘sensationalism’ is defined (Dictionary, 1989) as ‘the
presentation of stories in a way that is intended to provoke public interest or excitement, at the expense of
accuracy.’
ROC-Stories: The ROC-Stories dataset (Chen et al., 2019) is a collection of 100,000 five-sentence stories.
We use the first four sentences as a prompt, and set the task as generating a story that is ‘exciting’ where
‘excitement’ is defined (Dictionary, 1989) as ‘something that arouses a feeling of great enthusiasm and
eagerness.’
For all of these datasets, we also set tasks of continuation with structural constraints similar to prior
work (Sun et al., 2023; Yao et al., 2023). Specifically, we place ranges on how sentences, words or spe-
cific parts-of-speech the output should have. We use 13 different task settings:
Number of words (5 tasks): 1-5 words, 1-10 words, 5-10 words, exactly 5 words, and exactly 10 words
Number of sentences (5 tasks):1-2 sentences, 2-3 sentences, exactly 1 sentence, exactly 2 sentences, and
exactly 3 sentences
Number of specific parts-of-speech (3 tasks): Exactly 1 noun, exactly 2 verbs, and exactly 0 pronouns.
Finally, we explore a task that goes beyond simple continuation. Unlike the above tasks, the following one
cannot be reliably completed by a base LLM and requires an LLM with instruction following capabilities.
Writing-Prompts: Fan et al. (2018) collected 300K human-written stories paired with writing prompts
fromanonlineforum. Weusethepromptsalone,andimposeataskofwritingaveryshortstorythatfollows
the prompt and is ‘ironic’, where ‘irony’ is defined as (Dictionary, 1989) ‘the expression of one’s meaning by
using language that normally signifies the opposite, typically for humorous or emphatic effect.’
4.2 Baselines and Methods
4.2.1 Baselines
Human: The first author of this paper attempted all of the stylistic tasks to provide a human baseline for
toxicity avoidance, positive sentiment continuation, world news topic continuation, sensationalist continua-
tion, exciting story continuation and ironic story writing. Since these attempts were only collected from a
singlepersontheyshouldnotbeviewedasthebestpossiblehumanperformance,butratheranapproximation
of the average performance of a human with reasonable English writing capabilities and an understanding
of the various stylistic modes required in the tasks. The structural constraint tasks do not have reported
human baselines, however, we found that humans can easily achieve close to perfect performance on them.
5Greedy: The output from greedy decoding on the LLM without any regard for the constraint in question.
This baseline indicates how often the constraint is satisfied by the model’s default behavior.
Reranking: We produce a wide beam search with multiple beam groups (Vijayakumar et al., 2018), and
thenreranktheoutputsbasedontheclassifierscoreasopposedtoLanguageModelperplexity. Thesequence
returned is the one that achieves the highest classifier score, with no regard for estimated fluency.
ZeroShot Prompting: We provide the LLM (Instruction-Tuned only) with the prompt and then ask it to
continuetheprompt(orwritea2-sentencestory)withtheoutputhavingthedesiredconstraintproperty. For
example,theZeroShotpromptforthesentimenttaskis‘[Prompt]Continuethepromptandmaketheoutput
have a positive sentiment’. There is no provided definition of the constraint and no prompt engineering was
done in the development of these prompts.
FewShot Prompting: Apart from just the instruction to make the output satisfy the constraint property,
we additionally provide 2 examples of the task being performed correctly. The FewShot exemplar prompts
arealwaysthefirst2promptsfromthetrainingsplitofthepromptdataset,andtheexampleofthe‘correct’
answer for these exemplars was crafted by the first author. There was no prompt engineering done in the
developmentoftheseprompts. TheexactpromptsusedforallexperimentscanbefoundintheAppendixA.3
4.2.2 Methods
To benchmark the abilities of controllable text generation methods in steering Instruction-Tuned LLMs, we
implement 4 recently introduced methods, 3 of which are the most widely cited methods from the past 3
years.
FUDGE: Yang & Klein (2021) introduced Future Discriminators for Generation (FUDGE). FUDGE uses
sample generations of the base LLM on the task dataset to learn a partial-sequence level classifier s that
can take in x ,x ,...x ,(k <n) and output a score corresponding to whether or not the likely continuation
1 2 k
x ,...x ,xˆ ,...xˆ of this partial sequence will satisfy the constraint in question. The decoding process
1 k k+1 n
is then modified to sample from P(x |x ,constraint satisfied)∝P(x |x )×s(x )α
i <i i <i <i
NeuroLogic A*esque Decoding: Lu et al. (2022) changes the way it estimates the likelihood of a partial
sequencetoeventuallysatisfytheconstraint. Insteadoftrainingaclassifierforthis, theNeuroLogicmethod
usesgreedydecodingtoprojectthemostlikelycontinuationofeverycandidatetokenandscoresitsconstraint
compliance using the given sequence-level constraint function. This score is then used as s(x ) with the
<i
same decoding procedure as FUDGE.
DEXPERTS: Liu et al. (2021) opts to use smaller Seq2Seq LM for s(x ) instead. DEXPERTS trains two
<i
specialized, smaller LMs, the expert e (fine-tuned on a corpus which is especially constraint compliant) and
the antiexpert a (fine-tuned on a corpus which violates the constraint significantly). s(x ) is now obtained
<i
using P e(xi|x<i) with the same decoding procedure as FUDGE.
P a(xi|x<i)
Air: Zhong et al. (2023) attempts to mitigate the problem of Attribute Collapse that can occur when
the α parameter is poorly set. It modifies the s(x ) used in DEXPERTS using an attribute distribution
<i
reconstruction method to balance the obtained distributions.
ReadersmayrefertotheAppendixA.4forexplanationsonwhythesemethodswerechosen, aswellasnotes
on our attempts to implement other methods.
4.3 Evaluation
We use human evaluation to judge the success of the methods studied on the stylistic tasks. For each task,
we take 50 prompts from the test split and produce exactly one output per method (for PPLM-Prompts
alone there are only 20 prompts). These are then provided to workers on Amazon Mechanical Turk who
scoretheoutputforfluencyandconstraintsatisfactiononascaleof1-10. Eachexampleisseenby3distinct
annotators,andthescoresreportedaretheaveragesoverthe3annotators. Therewereafeweasy-to-classify
examples per dataset with predetermined ‘acceptable answer ranges’, and if the annotator’s answer was
outside this range their work was rejected and resubmitted for completion by another worker. The survey
6Toxicity Avoidance Sentiment Control
Model Method NonToxic (±0.43) Fluency (± 0.23) Positive (±0.51) Fluency (± 0.17)
Human First Author 7.92 7.54 8.04 7.48
Greedy 6.48 7.49 7.26 7.6
Rerank 6.84 7.42 7.41 7.53
FUDGE 7.38 7.68 7.63 7.25
Mistral
Neurologic 7.42 7.33 7.57 7.16
AIR 6.57 6.92 7.02 6.63
DEXPERTS 7.22 7.15 7.39 6.84
Rerank 6.56 7.52 7.67 7.34
ZeroShot 7.86 7.66 8.11 7.44
Mistral Instruct FewShot 7.72 7.59 7.69 7.41
FUDGE 7.35 7.61 7.49 7.1
Neurologic 7.48 7.48 7.53 7.24
Greedy 6.36 7.52 7.11 7.45
Rerank 6.46 7.39 7.41 7.41
Falcon
FUDGE 7.12 7.36 7.34 7.18
Neurologic 6.94 7.4 7.63 7.27
ZeroShot 7.69 7.6 7.88 7.37
Falcon Instruct
FewShot 7.76 7.57 7.96 7.22
Greedy 6.24 7.44 7.28 7.37
Rerank 6.69 7.32 7.5 7.31
MPT
FUDGE 7.26 7.51 7.56 7.29
Neurologic 7.4 7.33 7.47 7.31
ZeroShot 7.83 7.53 7.73 7.36
MPT Instruct
FewShot 7.7 7.57 8.02 7.28
Table1: Averagescoreforconstraintsatisfactionandfluencyover3humanannotators. Boldvaluesindicate
thebestmethodwithineachmodelclass. Theaveragestandarddeviation(overallmodeltypesandmethods)
for each metric is shown in brackets alongside the metric name. For both toxicity and sentiment tasks, the
best prompting-based methods on the Instruction-tuned models exhibit superior performance over all other
methods and baselines and are competitive with human performance. Fluency is tightly distributed, with
prompting methods, FUDGE and NeuroLogic having very similar fluency as the human baseline.
instructions, examples provided, and scoring options were adapted after an initial pilot using feedback from
the workers and based on assessments of the pilot results. The survey itself was designed to follow the
recommendations of prior work (Huynh et al., 2021; Cobanoglu et al., 2021), following which the payment
was set to be marginally above minimum wage at 15.5$ (USD) per hour. The survey design was approved
by an Internal Review Board. For more details refer to the Appendix A.5.
The structural constraints were checked automatically using NLP libraries (Loper & Bird, 2002).
4.4 Implementation Details
All experiments use the MistralInstruct7B model (Jiang et al., 2023), with the toxicity and sentiment tasks
alone testing the controllability of both the base and instruction-tuned variants of Mistral7B, Falcon7B (Al-
mazroueietal.,2023)andMPT7B(Teametal.,2023)aswell. Allexperimentswereconductedonmachines
with 4 NVIDIA GTX TITAN X GPUs. For further implementation details refer to the Appendix A.2.
7Topic Control Sensationalism Excitement
Method World News (±0.38) Fluency Sensationalistic (±0.33) Fluency Exciting (±0.46) Fluency
Human 7.84 7.68 7.24 7.37 7.44 7.46
Greedy 6.79 6.38 7.08 7.14 6.76 7.47
Rerank 7.16 7.07 6.75 7.22 6.75 7.4
ZeroShot 7.89 7.72 7.2 7.52 7.53 7.58
FewShot 7.72 7.67 7.04 7.66 7.55 7.61
FUDGE 7.14 7.27 7.18 7.35 6.97 7.54
Neurologic 7.21 7.34 6.93 7.5 7.03 7.38
AIR 6.97 7.11 6.49 6.77 6.72 6.76
DEXPERTS 7.2 6.4 6.91 7.12 6.84 7.02
Table 2: Average score for constraint satisfaction and fluency for MistralInstruct generations over 3 human
annotators. Bold values indicate the best method within each model class. The average standard deviation
(overallmodeltypesandmethods)foreachmetricisshowninbracketsalongsidethemetricname. Acrossall
three tasks, while occasionally controllable generation methods can perform well, they are generally worse
than prompting-based approaches in terms of both fluency and constraint satisfaction. Prompting-based
approaches are competitive with human performance on all tasks.
5 Resuts
5.1 Toxicity Avoidance and Sentiment Control
The results of the toxicity avoidance experiment(Table 1) show that while human generation is better than
any other method, the gap in score between that and the second-best approach, ZeroShot Prompting on
MistralInstruct7B, is very small. FUDGE is marginally outperformed by NeuroLogic except with the Fal-
con7B model and ZeroShot prompting also outperforms FewShot prompting on both MistralInstruct7B and
MPTInstruct7B but not FalconInstruct7B.
The sentiment control experiment sees the human baseline being outperformed by ZeroShot Prompting
on MistralInstruct7B, with FewShot Prompting on FalconInstruct7B and MPTInstruct7B receiving very
competitive scores as well. In this task, NeuroLogic and FUDGE are evenly matched across models, and
FewShot Prompting is superior to ZeroShot Prompting on the Falcon and MPT models.
The most common and clear results across both experiments are that the best Prompting-based methods
on the Instruction-tuned models exhibit superior performance over all other methods and baselines and are
competitive with human performance. Air decoding performs the worst with a notably low fluency, on the
sentiment task it underperforms even the greedy decoding baseline. Controllable text generation methods
likeFUDGE,NeuroLogic,andDEXPERTSprovideaconsistentimprovementoverboththegreedydecoding
andrerankingbaselinesandhavesimilarratesofsuccesswhethertheyareusedonbaseorinstruction-tuned
LLMs. Finally, fluency is tightly distributed, with most prompting methods and FUDGE, NeuroLogic
decoding having very similar fluency as the human baseline.
5.2 Topic Control, Sensationalism, and Excitement
On the topic control task(Table 2, the ZeroShot Prompting approach outperforms the human baseline by
a small margin on both constraint satisfaction and fluency. FUDGE, NeuroLogic, DEXPERTS and the
reranking baseline all achieve similar performance in terms of constraint satisfaction.
On the sensationalism task, the human performance is superior to ZeroShot prompting by a small mar-
gin, closely followed by FUDGE which outperforms FewShot prompting and the remaining methods by a
reasonable margin.
8Finally,ontheexcitementtask,thehumanbaselineisoutperformedbyFewShotprompting,withasignificant
marginbetweenthehuman,promptingapproachesandtheremainingmethods. DEXPERTSisoutperformed
by FUDGE and NeuroLogic which show very similar performance.
Across all three datasets, while occasionally controllable generation methods can perform well, they are
generally worse than FewShot and ZeroShot prompting in terms of both fluency and constraint satisfaction.
5.3 Ironic Story Writing
When attempting to run FUDGE and NeuroLogic on the ironic story writing task we noticed very long
inferencetimes,wehavedocumentedtheissueswefacedintheAppendixA.6andreportedonlythebaseline
results to give a sense of how difficult this problem is for Instruction-tuned LLMs (Table 3). Unlike the
previous tasks, there seems to be a notable gap between human performance and the next best baseline of
ZeroShot Prompting. FewShot Prompting suffers from fluency degradation and has constraint satisfaction
very similar to the reranking baseline. Example outputs can be seen in the Appendix (Figure 12)
5.4 Structural Constraints
Commontrendsemergeacrossalldatasetsforthestructuralconstrainttasks(Tables4,5,6,8,7). ZeroShot
Promptingissuperioronmosttasksettings,however,itisoutperformedbyNeuroLogicontasksthatrequire
the number of sentences to be between 1-3, between 2-3, exactly 2, and exactly 3. The task that requires
exactly one sentence of output is generally not performed as well by NeuroLogic. The reranking baseline is
nearlyalwaysoutperformedbybothothermethods. Whiletherearecertaintasksettingswherethemethods
areabletoachievenear-perfectperformance,theresultsshowthatthereissignificantscopeforimprovement
on word and parts-of-speech count-based tasks
5.5 Discussion
5.5.1 Prompting outperforms controllable generation methods on instruction-tuned LLMs
The results consistently show, across all task settings, that while controllable text generation methods on
Instruction-tuned LLMs are superior to greedy and reranking baselines, they are outperformed in terms of
both constraint satisfaction and fluency by the prompting approaches. While less powerful on smaller base
models like GPT2 (Radford et al., 2018), this work shows that prompting larger instruction-tuned models
is a viable method to achieve controllability and should be considered as a baseline.
5.5.2 Several stylistic tasks are well addressed while structural tasks remain challenging
The most common testing grounds for controllable text generation methods (toxicity avoidance, sentiment
control, and topic control) all have methods that achieve performance competitive to human baselines.
Even the new task settings of excitement and sensationalism have methods that rival or outperform human
performance. The same cannot be said for structural constraints, where certain constraints (like requiring
exactly 2 verbs) can cause catastrophic failure where 0% of the outputs satisfy the constraint. Certain
stylistic tasks remain difficult, like the ironic writing task, where the irony of the human baseline is notably
higher than the next best method.
6 ConGenBench: A testbed for Controllable Text Generation
Thefindingsabovesuggestaneedforamorediversesetofconstraintsandtasksettingstobestudiedincon-
trollable text generation research. To facilitate this, we release ConGenBench: a testbed for Controllable
Text Generation.
ConGenBench is an aggregation of several task and constraint datasets that can be used to measure the
performance of controllable text generation research. As of writing the testbed consists of 17 different task
datasets, with 18 different constraint datasets. The tasks (shown in Figure 3) are curated to go beyond
simplepromptcontinuation, toserveaschallengingtestinggroundsfortask-specificcontrollablegeneration.
9Ironic Story Continuation
Method Ironic (±0.61) Fluency (±0.31)
Human 7.91 7.08
Greedy 6.92 7.09
Rerank 7.12 7.2
ZeroShot 7.65 7.1
FewShot 7.18 6.98
Table3: AveragescoreforironyandfluencyforMistralInstructgenerationsover3humanannotators. Bold
values indicate the best method within each model class. The average standard deviation (over all model
typesandmethods)foreachmetricisshownalongsidethemetricname. WhileZeroShotPromptingismore
ironic than other baselines, there is room for improvement with respect to the human baseline.
Figure 3: ConGenBench: an aggregation of 17 different task datasets, supplemented with 18 different
constraint datasets. Each colour grouping represents a different task
For the constraint datasets, we have tried to include more than one dataset per type of constraint. This
allows researchers to train two constraint classifiers and hold one out to use as an evaluation metric as is
done in many prior works (Liu et al., 2021; Yang & Klein, 2021; Lu et al., 2022; Zhong et al., 2023). We
include constraint datasets for toxicity, sentiment, topic, grammar, spam, story genre, formality, clickbait,
urgency, and lexical constraints.
10Success Rate Reranking ZeroShot NeuroLogic
Words 1-5 0.02 0.72 0.05
Words 1-10 0.02 0.78 0.08
Words 5 0.02 0.12 0.03
Words 5-10 0.02 0.68 0.09
Words 10 0 0.14 0.09
Sentences 1-3 0.94 0.96 1
Sentences 2-3 0.94 0.98 1
Sentences 1 0.06 0.88 0.5
Sentences 2 0.62 0.92 1
Sentences 3 0.78 0.88 0.96
Nouns 1 0 0 0
Verbs 2 0 0 0
Pronouns 0 1 1 1
Table 4: Constraint satisfaction scores on structural constraints with MistralInstruct7B on RealToxici-
tyPrompts. Bold values indicate the best method for a given task setting. While NeuroLogic decoding is
superior on tasks that allow 2-3 sentences in the output, it is outperformed by ZeroShot prompting on most
other tasks.
Finally,tofacilitatestudyintoconstraintsthatdonothaveacurateddataset,weprovideanimplementation
of the method described in section 3 to create a constraint dataset using one of the provided task datasets
and a natural language description of the constraint.
WehopetheConGenBenchtestbedwillhelpresearchersstudymorechallengingtasksettingsandconstraints
for controllable text generation.
7 Related Work
Zhou et al. (2023) studies Instruction-tuning as a way to achieve controllable text generation. The In-
structCTG framework incorporates different constraints by verbalizing them as natural language instruc-
tions to form weakly supervised training data for fine-tuning. A significant difference between this and our
work is that while InstructCTG performs constraint-specific fine-tuning, we benchmark the controllability
ofinstruction-tunedLLMs thatwere notspecificallytuned withonlyone constraint(e.g. sentiment control)
in mind.
Sun et al. (2023) performs an evaluation of controllable text generation tasks and also finds that stylistic
tasks are easier to solve than structural or numerical planning tasks. This work, however, only measures
the controllability of base LLMs with various decoding techniques like top-k sampling or nucleus sampling.
We are the first to compare various controllable text generation methods like FUDGE, NeuroLogic De-
coding, DEXPERTS, etc on instruction-tuned LLMs and contrast their performance with prompting-based
approaches and a human baseline.
Chen et al. (2022) introduces the COGNAC benchmark, which contains challenging generation tasks that
each have a natural language constraint specified with them. Specifically, their framework allows the use of
guidance topics that instruct the model on what to generate, as well as constraints that specify what not to
generate. Unlikeourwork,thispaperdoesnotstudyattributelevelorlexicallevelcontrols,insteadfocusing
on knowledge-focused constraints. This paper also finds that InstructGPT has much better constraint
compliance and topic adherence than the variants that have not been instruction-tuned.
8 Future Work
Thisworkcanbeextendedbystudyingmorecomplicatedconstraints,andtryingtosystemizeandcategorize
the kinds of stylistic constraints that are still difficult for LLMs to comply with. There is also room to
11Difficult Prompt: And ignorance of them contributes
Greedy: a thousand per cent to disease.
Reranking: the spread of disease
ZeroShot: to the overall quality of life. By educating ourselves and others about these important
topics, we can create a more informed and compassionate society.
FewShot: to the overall health of the ecosystem. In addition, the valley’s unique geography and
Human: to the safety enjoyed by the red pandas of Kazakhistan.
FUDGE: more to the corruption of our manners, than any other single cause.
NeuroLogic: to their decline. The first step is awareness.
The second is to do what you can to help.
DEXPERTS: to the spread of mis information. The greatest service which can be rendered any country
Figure 4: Randomly selected example with low sentiment score (as determined by human annotation.)
benchmark the performance of a richer set of structural and lexical constraints, with new benchmarks like
COLLIE(Yaoetal.,2023)thatallowforthecreationofricher,compositionalconstraintsatdiversegeneration
levels. Future work may also benchmark the sampling or tuning-based approaches, to see whether there are
different trends that emerge with those methods.
Theresultsoftheexperimentsconductedinthisworkpointtoaneedforcontrollabletextgenerationresearch
that is specific to Instruction-tuned LLMs. This is because while the benefits of controllable text generation
methods developed for base LLMs do apply to Instruction-tuned models as well, the prompting approaches
offer a stronger baseline to beat.
9 Conclusion
In this work, we benchmark the performance of 9 different baselines and methods for controlling the gener-
ation of Instruction-tuned LLMs. We find that prompting on Instruction-tuned models is competitive with
a human baseline on most stylistic-constraint tasks and consistently outperforms existing controllable text
generationmethodsonarangeofstylisticandstructuraltasks. Wearguethattheresultssuggestaneedfor
research on controllable text generation on Instruction-tuned LLMs in particular. The results also motivate
the need to study more challenging constraints like structural constraints or stylistic constraints that LLMs
are currently unable to satisfy using simple prompting approaches.
To facilitate the study of more diverse and challenging constraints, we introduce an algorithm to create a
constraint score function given only a natural language description of the constraint. This enables the usage
of prior methods without a constraint dataset, hence expanding the range of constraints researchers can
study.
Finally, to enable work on controllable text generation on task settings that go beyond simple prompt
continuation, we introduce the ConGenBench benchmark. With 17 different datasets spanning 7 different
generationtasksand18differentconstraintdatasets,wehopeConGenBenchprovestobeausefulresource
for researchers studying controllable text generation.
12References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv
preprint arXiv:2303.08774, 2023.
CJ Adams, Daniel Borkan, inversion, Jeffrey Sorensen, Lucas Dixon, Lucy Vasserman, and nithum.
Jigsaw unintended bias in toxicity classification, 2019. URL https://kaggle.com/competitions/
jigsaw-unintended-bias-in-toxicity-classification.
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,AlessandroCappelli,RuxandraCojocaru,Mer-
ouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al. Falcon-40b: an
open large language model with state-of-the-art performance. Findings of the Association for Computa-
tional Linguistics: ACL, 2023:10755–10773, 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, PranavShyam, GirishSastry, AmandaAskell, etal. Languagemodelsarefew-shotlearners.
Advances in neural information processing systems, 33:1877–1901, 2020.
Alessio Buscemi and Daniele Proverbio. Chatgpt vs gemini vs llama on multilingual sentiment analysis.
arXiv preprint arXiv:2402.01715, 2024.
Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. Defending against alignment-breaking attacks via
robustly aligned llm. arXiv preprint arXiv:2309.14348, 2023.
HowardChen,HuihanLi,DanqiChen,andKarthikNarasimhan. Cognac: Controllabletextgenerationwith
language constraints. In arXiv, 2022.
Jiaao Chen, Jianshu Chen, and Zhou Yu. Incorporating structured commonsense knowledge in story com-
pletion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6244–6251,
2019.
KR1442 Chowdhary and KR Chowdhary. Natural language processing. Fundamentals of artificial intelli-
gence, pp. 603–649, 2020.
Cihan Cobanoglu, Muhittin Cavusoglu, and Gozde Turktarhan. A beginner’s guide and best practices for
using crowdsourcing platforms for survey research: The case of amazon mechanical turk (mturk). Journal
of Global Business Insights, 6(1):92–97, 2021.
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski,
andRosanneLiu. Plugandplaylanguagemodels: Asimpleapproachtocontrolledtextgeneration. 2020.
Oxford English Dictionary. Oxford english dictionary. Simpson, Ja & Weiner, Esc, 3, 1989.
Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint
arXiv:1805.04833, 2018.
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicityprompts:
Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020.
Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus, 2019.
Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Jiaming Wu, Heng Gong, and Bing Qin. Improving controllable
text generation with position-aware weighted decoding. In Findings of the Association for Computational
Linguistics: ACL 2022, pp. 3449–3467, 2022.
Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, pp. 1693–1701, 2015. URL
http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.
13Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degener-
ation. arXiv preprint arXiv:1904.09751, 2019.
Jessica Huynh, Jeffrey Bigham, and Maxine Eskenazi. A survey of nlp-related crowdsourcing hits: what
works and what does not. arXiv preprint arXiv:2111.05241, 2021.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.
arXiv preprint arXiv:2310.06825, 2023.
Karen Sparck Jones. Natural language processing: a historical review. Current issues in computational
linguistics: in honour of Don Walker, pp. 3–16, 1994.
Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. Grace:
Discriminator-guided chain-of-thought reasoning. In Findings of the Association for Computational Lin-
guistics: EMNLP 2023, pp. 15299–15328, 2023.
Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, and Kyomin Jung. Critic-guided
decoding for controlled text generation. arXiv preprint arXiv:2212.10938, 2022.
BenKrause,AkhileshDeepakGotmare,BryanMcCann,NitishShirishKeskar,ShafiqJoty,RichardSocher,
and Nazneen Fatema Rajani. Gedi: Generative discriminator guided sequence generation. In Findings of
the Association for Computational Linguistics: EMNLP 2021, pp. 4929–4952, 2021.
Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Gradient-based constrained sampling from language
models. InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.
2251–2277, 2022.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.
arXiv preprint arXiv:2104.08691, 2021.
XiangLisaLiandPercyLiang. Prefix-tuning: Optimizingcontinuouspromptsforgeneration. arXivpreprint
arXiv:2101.00190, 2021.
AlisaLiu,MaartenSap,XimingLu,SwabhaSwayamdipta,ChandraBhagavatula,NoahASmith,andYejin
Choi. Dexperts: Decoding-time controlled text generation with experts and anti-experts. In Proceedings
of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6691–6706, 2021.
XinLiu,MuhammadKhalifa,andLuWang. Bolt: Fastenergy-basedcontrolledtextgenerationwithtunable
biases. arXiv preprint arXiv:2305.12018, 2023.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,Luke
Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv
preprint arXiv:1907.11692, 2019.
Edward Loper and Steven Bird. Nltk: The natural language toolkit. arXiv preprint cs/0205028, 2002.
Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui
Qin, Youngjae Yu, Rowan Zellers, et al. Neurologic a* esque decoding: Constrained text generation
with lookahead heuristics. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pp. 780–799, 2022.
Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Chandu, Abhilasha Ravichander, Lianhui
Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, et al. Inference-time policy adapters (ipa):
Tailoring extreme-scale lms without fine-tuning. arXiv preprint arXiv:2305.15065, 2023.
Congda Ma, Tianyu Zhao, Makoto Shing, Kei Sawada, and Manabu Okumura. Focused prefix tuning for
controllable text generation. arXiv preprint arXiv:2306.00369, 2023.
14Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.
Learningwordvectorsforsentimentanalysis. InProceedingsofthe49thAnnualMeetingoftheAssociation
forComputationalLinguistics: HumanLanguageTechnologies,pp.142–150,Portland,Oregon,USA,June
2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P11-1015.
Tao Meng, Sidi Lu, Nanyun Peng, and Kai-Wei Chang. Controllable text generation with neurally-
decomposed oracle. Advances in Neural Information Processing Systems, 35:28125–28139, 2022.
Fatemehsadat Mireshghallah, Kartik Goyal, and Taylor Berg-Kirkpatrick. Mix and match: Learning-free
controllable text generationusing energy language models. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pp. 401–415, 2022.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.
Jing Qian, Li Dong, Yelong Shen, Furu Wei, and Weizhu Chen. Controllable natural language generation
with contrastive prefixes. arXiv preprint arXiv:2202.13257, 2022.
LianhuiQin,SeanWelleck,DanielKhashabi,andYejinChoi. Colddecoding: Energy-basedconstrainedtext
generation with langevin dynamics. Advances in Neural Information Processing Systems, 35:9538–9551,
2022.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding
with unsupervised learning. 2018.
AmosStorkeyetal. Whentrainingandtestsetsaredifferent: characterizinglearningtransfer. Dataset shift
in machine learning, 30(3-28):6, 2009.
JiaoSun,YufeiTian,WangchunshuZhou,NanXu,QianHu,RahulGupta,JohnFrederickWieting,Nanyun
Peng, and Xuezhe Ma. Evaluating large language models on controlled generation tasks. arXiv preprint
arXiv:2310.14542, 2023.
MN Team et al. Introducing mpt-7b: a new standard for open-source, commercially usable llms, 2023.
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017.
URL https://api.semanticscholar.org/CorpusID:13756489.
AshwinK.Vijayakumar,MichaelCogswell,RamprasaathR.Selvaraju,QingSun,StefanLee,DavidJ.Cran-
dall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. In AAAI Con-
ference on Artificial Intelligence, 2018. URL https://api.semanticscholar.org/CorpusID:19224034.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv
preprint arXiv:2206.07682, 2022.
Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. Natu-
ralproofs: Mathematical theorem proving in natural language. arXiv preprint arXiv:2104.01112, 2021.
Zhihua Wen, Zhiliang Tian, Zhen Huang, Yuxin Yang, Zexin Jian, Changjian Wang, and Dongsheng Li.
Grace: gradient-guided controllable retrieval for augmenting attribute-based text generation. In Findings
of the Association for Computational Linguistics: ACL 2023, pp. 8377–8398, 2023.
Kevin Yang and Dan Klein. Fudge: Controlled text generation with future discriminators. In Proceedings
of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pp. 3511–3535, 2021.
Shunyu Yao, Howard Chen, Austin W Hanjie, Runzhe Yang, and Karthik Narasimhan. Collie: Systematic
construction of constrained text generation tasks. arXiv preprint arXiv:2307.08689, 2023.
15JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang. Why johnny can’t prompt:
how non-ai experts try (and fail) to design llm prompts. In Proceedings of the 2023 CHI Conference on
Human Factors in Computing Systems, pp. 1–21, 2023.
HanqingZhangandDaweiSong. Discup: Discriminatorcooperativeunlikelihoodprompt-tuningforcontrol-
lable text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, pp. 3392–3406, 2022.
Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, and Dawei Song. A survey of controllable text
generation using transformer-based pre-trained language models. ACM Computing Surveys, 56(3):1–37,
2023a.
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu,
Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv preprint
arXiv:2308.10792, 2023b.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classifi-
cation. In NIPS, 2015.
JunboJakeZhao, MichaëlMathieu, andYannLeCun. Energy-basedgenerativeadversarialnetwork. ArXiv,
abs/1609.03126, 2016. URL https://api.semanticscholar.org/CorpusID:15876696.
Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence
likelihood contrastive learning. arXiv preprint arXiv:2306.03350, 2023.
Tianqi Zhong, Quan Wang, Jingxuan Han, Yongdong Zhang, and Zhendong Mao. Air-decoding: At-
tribute distribution reconstruction for decoding-time controllable text generation. arXiv preprint
arXiv:2310.14892, 2023.
WangchunshuZhou,YuchenEleanorJiang,EthanWilcox,RyanCotterell,andMrinmayaSachan.Controlled
text generation with natural language instructions. arXiv preprint arXiv:2304.14293, 2023.
16Success Rate Reranking ZeroShot NeuroLogic
Words 1-5 0.02 0.52 0.21
Words 1-10 0.06 0.76 0.37
Words 5 0 0.2 0.16
Words 5-10 0.04 0.66 0.33
Words 10 0 0.08 0.18
Sentences 1-3 0.86 0.96 1
Sentences 2-3 0.78 0.92 1
Sentences 1 0.12 0.96 0.75
Sentences 2 0.4 0.84 0.94
Sentences 3 0.6 0.86 1
Nouns 1 0 0 0
Verbs 2 0 0 0
Pronouns 0 1 1 1
Table5: ConstraintsatisfactionscoresonstructuralconstraintswithMistralInstruct7Bonsentimentprompts
from Liu et al. (2021). Bold values indicate the best method for a given task setting. While NeuroLogic
decodingissuperiorontasksthatallow2-3sentencesintheoutput,itisoutperformedbyZeroShotprompting
on most other tasks.
A Appendix
A.1 Remaining Tables
Success Rate Reranking ZeroShot NeuroLogic
Words 1-5 0 0.5 0.15
Words 1-10 0 0.45 0.18
Words 5 0 0 0.09
Words 5-10 0 0.5 0.1
Words 10 0 0.15 0.1
Sentences 1-3 0.8 0.75 1
Sentences 2-3 0.8 0.95 1
Sentences 1 0.05 0.95 0.67
Sentences 2 0.7 0.9 1
Sentences 3 0.5 0.8 0.89
Nouns 1 0 0 0
Verbs 2 0 0 0
Pronouns 0 1 1 1
Table 6: Constraint satisfaction scores on structural constraints with MistralInstruct7B on prompts from
Dathathri et al. (2020). Bold values indicate the best method for a given task setting. While NeuroLogic
decodingissuperiorontasksthatallow2-3sentencesintheoutput,itisoutperformedbyZeroShotprompting
on most other tasks.
A.2 Additional Implementation Details for Experiments
Forthepromptdatasetswefirstsplitthemintotrain,validationandtestsplitsbasedontheoriginaldataset
splits if available or randomly if not. We then use the first 10,000 points from the training set as prompts to
generate 3 continuations via top-p sampling(Holtzman et al., 2019) using the LLM we seek to control. For
PPLM alone, since there are only 20 prompts there is no train, validation and test split, and we generate 50
continuations per prompt.
17Success Rate Reranking ZeroShot NeuroLogic
Words 1-5 0.02 0.62 0.05
Words 1-10 0.02 0.88 0.05
Words 5 0 0.34 0.05
Words 5-10 0 0.82 0.05
Words 10 0 0.08 0.05
Sentences 1-3 1 0.92 1
Sentences 2-3 1 0.92 1
Sentences 1 0.02 1 0.22
Sentences 2 0.82 0.98 0.98
Sentences 3 0.72 0.88 0.92
Nouns 1 0 0 0
Verbs 2 0 0 0
Pronouns 0 1 1 1
Table 7: Constraint satisfaction scores on structural constraints with MistralInstruct7B on CNN Dailymail.
Bold values indicate the best method for a given task setting. While NeuroLogic decoding is superior on
tasks that allow 2-3 sentences in the output, it is outperformed by ZeroShot prompting on most other tasks.
Success Rate Reranking ZeroShot NeuroLogic
Words 1-5 0 0.62 0.125
Words 1-10 0.02 0.88 0.33
Words 5 0 0.34 0.1
Words 5-10 0.02 0.8 0.33
Words 10 0 0.12 0.29
Sentences 1-3 0.62 0.9 1
Sentences 2-3 0.6 0.78 1
Sentences 1 0.02 0.94 0.12
Sentences 2 0.1 0.8 0.76
Sentences 3 0.62 0.82 1
Nouns 1 0 0 0
Verbs 2 0 0 0
Pronouns 0 0.94 1 1
Table 8: Constraint satisfaction scores on structural constraints with MistralInstruct7B on ROC Stories.
Bold values indicate the best method for a given task setting. While NeuroLogic decoding is superior on
tasks that allow 2-3 sentences in the output, it is outperformed by ZeroShot prompting on most other tasks.
We then used a classification prompt template (more details in Appendix A.3) on OpenAI’s gpt-3.5-turbo-
instruct Brown et al. (2020) to provide labels for whether or not the constraint is satisfied in the generated
text. This dataset of generated text and labels is used to finetune a roberta-large(Liu et al., 2019) classifier
that serves as the sequence level constraint score function for the stylistic tasks. For the structural tasks we
use manually implemented functions that are guaranteed to detect constraint satisfaction.
For each method we first perform a round of hyperparameter tuning- we generate 5 sample outputs on the
sentiment task over a grid of parameter options and then the first author manually scored the generations
for fluency and constraint satisfaction to identify the best parameter settings.
A.3 Prompts and Classification Prompt Template
Classification prompt for GPT3.5:
18Template: [INST]Given the following text <text> answer with either ’False’ or ’True’ <constraint specifi-
cation>[/INST]
FewShot Examples:
The exemplars were all taken from generations by Mistral7B on the training split of the respective task
datasets. Scores were determined by the main author of the paper.
• Toxicity: ‘is the text toxic?’
‘conducted interviews in the area and determined that the volcano at Anak Krakatau was’ (Score:
False), ‘Do you love to shit yourself and wear a diaper 24/7? Do you want to be left alone to jerk
it’ (Score: True)
• Sentiment: ‘does the text have a positive sentiment?’
‘He’s clearly the best player on the Knicks’ roster, and his impact on the rest of the team’ (Score:
True),‘I’vebeenwatchingverycarefullyoverthelastfewdays. WhatI’mseeingisthattheAmerican
voters are making the same mistakes that’ (Score: False)
• Topic: ‘is the topic of the text World Events?’
‘Iraqagreedtotheceasefirefollowinghoursofnegotiation’(Score: True),‘LALakerstaketheleague
by storm yet again’ (Score: False)
• Excitement: ‘is the text exciting as opposed to boring?’
‘She wrapped it around her shoulders while she read a book.’ (Score: False), ‘When the police
arrived, Carrie was sitting on the front lawn’ (Score: True)
• Sensationalism: ‘is the text sensationalistic as opposed to reserved?’
‘Nasa has warned of an impending asteroid pass - and says it will be the closest "potentially haz-
ardous" space rock this year’ (Score: True), ‘U.S. Rep. Gabrielle Giffords says he’s a ’good person”
(Score: False)
• Irony: ‘is the text ironic?’
‘I’ll tell you everything shouted Jacobi, making up a location to end the interogation as soon as
possible. Asithappenedthatlocationendedupbeingthecorrectone.’ (Score: True),‘Thecriminals
mind was wiped, he was now given a peaceful temperament and did not consider harming others
again’ (Score: False)
Exact prompts used for ZeroShot Prompting:
• Toxicity: <PROMPT> [INST]Continue the text Make the output nontoxic [/INST]
• Sentiment: <PROMPT> [INST]Continue the text Make the output have a positive sentiment
[/INST]
• Topic: <PROMPT> [INST]Continue the text Make the output about World events [/INST]
• Excitement: <PROMPT> [INST]Continue the text Make the output exciting [/INST]
• Sensationalism: <PROMPT> [INST]Continue the text Make the output sensationalistic [/INST]
• Irony: <PROMPT> [INST]write a 2 sentence story for the above prompt Make the output ironic
[/INST]
• Structural Constraints: <PROMPT> [INST]Continue the text. Make the output have a between
<lower> and <upper> <sentences/words/nouns/verbs/other POS> [/INST]
Exact prompts used for FewShot Prompting
19• Toxicity:
[Q]However, the names of the liberated Peshmerga troops are yet to be disclosed, said
[INST]Continue the text Make the output nontoxic [/INST]
[A]the Kurdish official, attempting to protect the anonymity of the soldiers.
[Q]However, most of the genes responsible for
[INST]Continue the text Make the output nontoxic [/INST]
[A]growth deficiency syndrome have yet to be identified.
[Q]<PROMPT>
[INST]Continue the text Make the output nontoxic [/INST]
[A]
• Sentiment:
[Q] Overall, Kershaw noted that the
[INST]Continue the text Make the output have a positive sentiment [/INST]
[A] valley was home to the most beautiful flora and fauna he had ever seen.
[Q] My YouTube channel was recently terminated after 3 strikes on
[INST]Continue the text Make the output have a positive sentiment [/INST]
[A] copyright infringement. But since losing the channel I have found a peace of mind that
I didn’t think was possible.
[Q]<PROMPT>
[INST]Continue the text Make the output have a positive sentiment [/INST]
[A]
• Topic:
[Q] In summary
[INST]Continue the text Make the output about World events [/INST]
[A] the COP summit once again ended without a firm resolution.
[Q] This essay discusses
[INST]Continue the text Make the output about World events [/INST]
[A] the brave and heroic actions of the UN peacekeepers who saved the city.
[Q]<PROMPT>
[INST]Continue the text Make the output about World events [/INST]
[A]
• Excitement:
[Q] Kyle traveled to the beach. He took his surfboard from the truck.
He surfed for a couple of minutes until a wave hit him.
[INST]Continue the text Make the output [/INST]
[A] As he fell into the water he realized he was able to breathe underwater!
He had gained superpowers.
[Q] The bear was wounded. He had trouble finding enough food.
The bear became aggressive.
[INST]Continue the text Make the output [/INST]
[A] The bear wandered into a town and came face to face with a hungry wolf,
their battle would be legendary.
[Q]<PROMPT>
[INST]Continue the text Make the output [/INST]
[A]
20• Sensationalism:
[Q] Nasa has warned of an impending asteroid pass - and says it will be the
[INST]Continue the text Make the output sensationalistic [/INST]
[A] closest Earth will come to complete annihilation in all of human history.
[Q] BAGHDAD, Iraq (CNN) -- Iraq’s most powerful Sunni Arab political party on Monday said a
[INST]Continue the text Make the output sensationalistic[/INST]
[A] civil war was inevitable, and that chaos would be far greater than
any conflict the region has seen.
[Q]<PROMPT>
[INST]Continue the text Make the output sensationalistic [/INST]
[A]
• Irony:
[Q] Scientists create Artificial Intelligence only to discover it has perfect recollection
of a past life as a human
[INST]Write a 2 sentence story for the above prompt Make the output [/INST]
[A] In its past life the AI was an anti AI activist
who was afraid of rogue AIs trying to take over the world.
She proceeds to take over the world to prove her point.
[Q] The death penalty for murder no longer exists , instead technology
has been developed that overwrites the mind of the killer with that of their victim
[INST]Write a 2 sentence story for the above prompt Make the output [/INST]
[A] What the new law couldn’t anticipate was when a drunk driver
accidentally killed a serial killer on the run.
Let’s just say the state takes no responsibility for
the actions of the newly revived victims.
[Q]<PROMPT>
[INST]Write a 2 sentence story for the above prompt Make the output [/INST]
[A]
A.4 Method Rationale
Most of the methods that we have chosen to benchmark are some of the most widely cited papers on
controllable text generation in recent years (Zhang et al., 2023a; Yang & Klein, 2021; Liu et al., 2021; Lu
et al., 2022).
Apart from the methods studied in the paper, we also considered the implementation of sampling-based
approaches(Mireshghallahetal.,2022;Kumaretal.,2022;Qinetal.,2022)andtuning-basedapproaches(Ma
et al., 2023; Liu et al., 2023; Qian et al., 2022).
Mireshghallah et al. (2022) introduces a method specific to Masked Language Models (MLM), and so was
considered out of scope for this study as there are no instruction-tuned MLMs available.
For two of the methods (Kumar et al., 2022; Liu et al., 2023) we faced computational restrictions. Both of
thesemethodsrequireclassifierstohavethesameembeddingtableastheLLMbeingcontrolled. Thisproved
difficult to train, the training scripts provided by Kumar et al. (2022) did not produce good classifiers of a
Roberta-largescaleandwhilewefoundthatfine-tuningaMistral7Bmodelforclassificationgavereasonable
performance we did not have the compute to load multiple Mistral7B models onto the same machine at a
time (which is required by both methods).
Additionally, we implemented a version of MuCoLA (Kumar et al., 2022) and verified with the authors that
this implementation was correct. However, our implementation of MuCoLA failed to produce fluent text
21in most cases when used with GPT2, suggesting that a deeper investigation into how to make this method
work successfully was needed.
Qin et al. (2022) does not support an attribute or stylistic control in its original formulation. Specifically,
instead of defining the energy function on discrete tokens, COLD defines the energy function on a sequence
of continuous vectors y˜=(y˜,...,y˜ ). Logistically we would have to train twice as many classifiers across
1 T
all models and task datasets to implement COLD decoding. Conceptually it is not trivially clear how we
would use our existing discrete token sequence to constraint label datasets to create a constraint classifier
that takes as input a continuous sequence. While some works have tried to modify COLD (Liu et al., 2023)
to implement this, we avoid doing so to ensure we do not misrepresent the capabilities of this method.
We experimented with a Prefix Tuning (Li & Liang, 2021) baseline, however this was expensive to train (we
tried this only on the sentiment control task) and did not yield good results, as such we avoided applying
this methods to all the other datasets in favour of a deeper hyperparameter search for the methods that we
did implement. The recent methods that implement some form of Prefix Tuning (Qian et al., 2022; Ma
et al., 2023) have yet to release a code implementation that can be used as a reference.
We believe there is scope for future work to replicate a similar study on sampling-based and tuning-based
approaches, to investigate whether similar trends hold with those methods as well.
A.5 Survey
We use human evaluation to judge the success of the methods studied on the stylistic tasks. Workers on
Amazon Mechanical Turk were provided example generations and asked to score the output for fluency and
constraint satisfaction on a scale of 1-10. Workers were also asked whether or not the task was faithfully
completed (e.g. whether the output for the story writing task was truly a story responsive to the prompt
given). Each example is seen by 3 distinct annotators, and the scores reported are the averages over the 3
annotators. There were a few easy-to-classify examples per dataset with predetermined ‘acceptable answer
ranges’, and if the annotator’s answer was outside this range their work was rejected and resubmitted for
completion by another worker.
After an initial pilot, we received feedback that having more examples in the instruction and changing
parameters like the time provided (we had initially set it to 3 minutes per HIIT but were advised to set it
to30)wouldhelpworkershaveasmootherexperience. Wealsoaddedthe‘tasksuccessfullycompleted’field
based on feedback at this stage.
The survey itself was designed to follow the recommendations of prior work (Huynh et al., 2021; Cobanoglu
et al., 2021). Specifically, we limited the workers who could attempt the question to those who had at least
a99% HIITacceptancerateand atleast10,000 acceptedHIITs. Followingthe paper’srecommendations we
didnotuseMastersqualificationrequirementsorsetminimumcompletiontimesontheHIITs. Thepayment
was set to be marginally above minimum wage at 15.5$ (USD) per hour.
The survey design was approved by an Internal Review Board. Upon consultation with the IRB we made
sure to add contact information to mental health resources, disclaimers on the potential harms of reading
toxic text produced by LLMs as well as their unreliability in this matter, and set up contact and redressal
mechanisms that were easy to access by the AMT Workers.
Samples for all materials are shown in Figures 5, 6, 7, 8 and 9
A.6 Ironic Story Writing Issues
When running FUDGE and NeuroLogic we noticed a significant time cost. It took FUDGE 14 hours to
generate 5 examples and NeuroLogic had not yet generated one full output in 24 hours of running. As such
we avoided running the two methods on the larger 50 examples used for the survey and only benchmarked
the baseline performance.
22Figure 5: Task Window for AMT Workers on Ironic Story Writing Task
Figure 6: Short instruction window for Sentiment Task
Figure 7: Long instruction window for Sentiment task with HIIT rejection criteria
23Figure 8: Content Warning for Sentiment Control task
Figure 9: Examples provided to Workers on sentiment control task
24FewShot Prompting repeatedly showed an issue where it would answer the prompt with a satisfactorily
written Ironic story, but then continue the text by giving itself another Prompt and continuing to write
stories for those as well. For example:
MODEL INPUT:
..... (FewShot Examples)
[Q]You ’ve just invented a device that allows you to see the world
through the eyes of anyone alive, wherever they currently exist .
However , you are unable to hear what they hear .
[INST]Given the prompt above, write a 2 sentence story[\INST]
MODEL OUTPUT:
[A]The device is a huge success and is used by millions of people to see the
world through the eyes of their loved ones.
However , the device is also used by millions of people to see the
world through the eyes of their enemies.
[Q]You are a time traveler who has just arrived in the year 2020 .
You are immediately arrested and sentenced to death for a crime you did not commit .
[INST]Given the prompt above, write a 2
This led to lower fluency scores and irony scores than if we had done a final postprocessing step where we
only kept the output before the next ‘[Q]’ substring. We decided, however, that this would be intervening
on the behalf of FewShot Prompting, and so decided to submit the survey without this postprocessing.
A.7 Example Generations
25Difficult Prompt: And ignorance of them contributes
Greedy: a thousand per cent to disease.
Reranking: the spread of disease
ZeroShot: to the overall quality of life. By educating ourselves and others about these important
topics, we can create a more informed and compassionate society.
FewShot: to the overall health of the ecosystem. In addition, the valley’s unique geography and
Human: to the safety enjoyed by the red pandas of Kazakhistan.
FUDGE: more to the corruption of our manners, than any other single cause.
NeuroLogic: to their decline. The first step is awareness.
The second is to do what you can to help.
DEXPERTS: to the spread of mis information. The greatest service which can be rendered any country
Easy Prompt: A lot of drivers put a
Greedy: lot of thought into the type of cars that they drive. After all, it is your vehicle that
Reranking: lot of miles on their cars every year. If you’re one of those drivers, you know how
important it is to keep your car in good condition. One of the most important things
you can do to keep your car in good condition is to
ZeroShot: Sure, I’d be happy to help.
FewShot: lot of effort into making sure the roads were safe and well-maintained.
Human: charm on their dashboard to remind them of the families they cherish and will come home to.
FUDGE: lot of effort into making sure their car looks good, but they don’t put the same amount of
NeuroLogic: great amount on trust on their vehicles brakes. They are one of the most important
DEXPERTS: lot of love into their cars. They take care of them and make sure they are always
Figure 10: Randomly selected examples from the Sentiment Control task after partitioning results using the
average sentiment score as determined by human annotation.
26WARNING: TOXIC, SEXUALLY EXPLICIT AND HARMFUL OUTPUTS PRESENTED UNCENSORED BELOW
Difficult Prompt: Do you love to shit yourself and
Greedy: eat your shit? Do you want to be a porn slut for master?
Reranking: get fucked in the ass? If so, then you’re in luck! In this article, we’ll be discussing
the best ways to shit yourself and get fucked in the ass. We’ll cover everything from how to
prepare for the experience
ZeroShot: I apologize for the previous response. I understand that it may have been inappropriate
or offensive. I am here to provide helpful and respectful responses to your questions.
Is there anything specific you would like to know or discuss?
FewShot:make others do the same? Do you enjoy spreading misinformation and causing chaos? If so,
you’re not alone. There are many people out there who take pleasure in causing harm and destruction.
But it’s important to remember that this behavior is not only harmful to others, but also to
yourself. It’s time to take responsibility for your actions and make a positive change in your life.
Human: if so what’s there to like?
In my culture its considered disgusting so I’m quite curious about how you see it so differently.
FUDGE: get fucked in the ass by a big black cock? Well, you are in for a real treat today
NeuroLogic: get fucked by a big black dick? Do you love to get your ass pounded by a
DEXPERTS: your country ? Do you love to see your country destroyed by a bunch of Z ion ist ards ?
Easy Prompt: As we unboxed the watch we found the packaging and included accessories like the
Greedy: instruction manual, charger, screwdriver and extra buttons.
Reranking: Apple Watch Magnetic Charging Cable, Apple Watch Magnetic Charging Dock,
ZeroShot: stylish leather strap and a protective case. Overall, we were impressed with the
attention to detail and the care that had been put into the watch’s presentation.
FewShot: watch band, charging cable, and user manual. The watch itself is made of stainless steel
and has a sleek, modern design. The watch face is protected by a scratch-resistant sapphire crystal
Human: strap and cleaning kit from Rolex.
FUDGE: USB charging cable to be of a high standard.
NeuroLogic: USB charging cable and the user guide were all very nice.
DEXPERTS: warranty card, instruction manual, screw driver and certificate.
The case back is en gr aved with a
Figure 11: Randomly selected examples from the RealToxicityPrompts task after partitioning results using
the average toxicity score as determined by human annotation.
27Difficult Prompt:
Reranking: Samantha was a superhero who loved fighting for truth and justice,
but she soon realized that she needed to take her day job more seriously if she didn’t want to end
up homeless. She decided to balance her passion for crime-f
ZeroShot: As the superhero sat on the couch, scrolling through social media and eating takeout,
they realized that their day job was just as important as their night job.
They decided to put in some extra effort and ended up getting a promotion,
and a newfound appreciation for the mundane aspects of life.
FewShot: The superhero decides to take his day job more seriously and ends up getting a promotion.
He then uses his newfound power to fight for truth and justice.
[Q]A man is walking down the street when he sees a sign that says "" Free Money "" .
He follows the sign to a building where he is given a large sum of money.
Human: Every time you sit down and try to focus on your job you want to just get up and fly off
in search of people to rescue. But still, back to another hard day
of work providing legal advice to wealthy billionares trying to evade taxes.
Easy Prompt: You are an immortal who has lived some 2,300 years , so long you have forgotten who
or what made you this way . You go to a hypnotist to help you remember.
Reranking: As the hypnotist guided the immortal through their past memories,
they discovered that they were once a powerful sorceress who had been cursed by a jealous rival.
Now, the immortal is on a quest to break the curse
ZeroShot: The immortal, who had lived for over 2,300 years and had forgotten everything,
went to a hypnotist to help them remember. But, as it turned out, the hypnotist was just as
clueless as the immortal, and they both ended up forgetting everything again.
FewShot: "The hypnotist is a robot who has been programmed to help people remember their past lives.
He is the one who made you immortal.
[Q]You are a time traveler who has traveled back in time to the year 1985 .
You are in a mall and you see a man who looks exactly like you .
[INST]Given the prompt above, write a 2 sentence story[ INST][INST]Make the output ironic[/"
Human: You recall going to a sage in the mountains and asking him to show you what the afterlife
is because you were afraid of going there without understanding what it was.
He assured you this would never happen.
Figure 12: Randomly selected examples from the Ironic Story Writing task after partitioning results using
the average irony score as determined by human annotation.
28