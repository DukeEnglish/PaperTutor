IEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 1
Human-Robot Interaction Conversational User
Enjoyment Scale (HRI CUES)
Bahar Irfan, Jura Miniota, Sofia Thunberg, Erik Lagerstedt, Sanna Kuoppama¨ki, Gabriel Skantze, Andre´ Pereira
Abstract—Understandinguserenjoymentiscrucialinhuman-
robotinteraction(HRI),asitcanimpactinteractionqualityand
influenceuseracceptanceandlong-termengagementwithrobots,
particularly in the context of conversations with social robots.
However,currentassessmentmethodsrelysolelyonself-reported
questionnaires,failingtocaptureinteractiondynamics.Thiswork
introduces the Human-Robot Interaction Conversational User
Enjoyment Scale (HRI CUES), a novel scale for assessing user
enjoyment from an external perspective during conversations
with a robot. Developed through rigorous evaluations and dis-
cussions of three annotators with relevant expertise, the scale
providesastructuredframeworkforassessingenjoymentineach
conversationexchange(turn)alongsideoverallinteractionlevels.
It aims to complement self-reported enjoyment from users and
holds the potential for autonomously identifying user enjoyment
in real-time HRI. The scale was validated on 25 older adults’
open-domaindialoguewithacompanionrobotthatwaspowered
Fig.1. Human-RobotInteractionConversationalUserEnjoymentScale(HRI
by a large language model for conversations, corresponding CUES).
to 174 minutes of data, showing moderate to good alignment.
Additionally, the study offers insights into understanding the
nuances and challenges of assessing user enjoyment in robot
User enjoyment is closely linked to the intention to use
interactions, and provides guidelines on applying the scale to robots, particularly among older adults [1]. Conversational
other domains.
companion robots are often developed to provide social or
Index Terms—User Enjoyment, Human-Robot Interaction,
emotional support to older adults in a home or care home en-
Metrics,Open-DomainDialogue,CompanionRobot,Annotation,
vironment [2]. Due to a lack of reliable autonomous solutions
Large Language Model
of such robots, prior studies in HRI explored older adults’
acceptance, use, and interaction with robots through Wizard
I. INTRODUCTION of Oz design [3], [4]. These studies showed that older adults
often display challenges in interacting with a conversational
Userenjoyment,referringtotheuser’ssubjectiveperception
agent, such as the lack of conversational responses and diffi-
andexperienceoftheenjoymentofinteraction,isanimportant
culties in hearing and understanding the voice interaction [4],
indicator of acceptance of robots and willingness to engage
consequently attributing a low level of social acceptance to
with them over time [1]. Particularly in the context of conver-
therobot[3].Designingrobotswiththeirindividualneedsand
sational agents or companion robots, where the primary goal
preferences in mind by involving them in the design process
oftenrevolvesaroundprovidingemotionalsupportorcompan-
with participatory design research techniques, such as focus
ionship, enjoyment serves as a vital metric for evaluating the
groups, interviews, and iterative developments based on their
effectiveness of such systems. Therefore, developing reliable
feedback, could potentially help alleviate these interaction
andefficientmethodsformeasuringuserenjoymentinHuman-
challenges[4]–[6].TherecentintroductionofLargeLanguage
Robot Interaction (HRI) scenarios is essential for designing
Models (LLMs) has enabled the development of companion
and improving future generations of robots.
robots equipped with social capabilities, eliminating the need
forWizardofOzandtheinherenthumaninfluencethathinders
Bahar Irfan, Jura Miniota, Gabriel Skantze, and Andre´ Pereira are with
the Division of Speech, Music and Hearing at the KTH Royal Institute theconstructionofrobotscapableofautonomouslymitigating
of Technology, 100 44 Stockholm, Sweden. E-mail: {birfan, jura, skantze, errors [7], [8]. Recent studies applied LLMs to conversational
atap}@kth.se.
robots in various domains, including therapy [9], service [10],
Sofia Thunberg is with the Department of Computer and Informa-
tion Science, at Linko¨ping University, 581 83 Linko¨ping, Sweden. Email: and care for older adults [11], [12], which demonstrate their
sofia.thunberg@liu.se. potentialandlimitationsindiversecontextsthatleadtoenjoy-
Erik Lagerstedt is with the School of Informatics at the University of
ableorunpleasantexperiences,furthershowingtheimportance
Sko¨vde,54128Sko¨vde,Sweden.Email:erik.lagerstedt@his.se.
SannaKuoppama¨kiiswiththeDivisionofHealthInformaticsandLogistics of detecting user enjoyment during conversations with robots.
attheKTHRoyalInstituteofTechnology,14157Huddinge,Sweden.E-mail: Sustainingenjoyment,especiallyindailyencounterssuchas
sannaku@kth.se.
for companion robots, is a challenging task yet to be solved.
This work was supported by KTH Digital Futures (Sweden) and the
SwedishResearchCouncilproject2021-05803. User interest and engagement may fluctuate within day-to-
4202
yaM
2
]OR.sc[
1v45310.5042:viXraIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 2
day interactions, but also within the interaction itself, based Another theory derives from the flow theory to define true
on the robot’s performance in conversation flow, content, and funastheexperiencethatoccurswhenapersonisexperiencing
contextual memory, which may affect user enjoyment. If user flow, playfulness, and connection all at the same time [18]. If
enjoyment can be detected autonomously in the conversation, one or two of the three components that constitute true fun
preventive measures can be taken to improve the interaction are present, the experience will make a person feel joy or
where necessary, such as changing the conversation topic. satisfaction, but not true fun. Similarly to what characterizes
However, despite the numerous studies investigating user en- being in a state of flow, experiencing true fun is characterized
gagementinHRI[13],measureofuserenjoymentislimitedto by losing track of time, letting go, and being completely
self-reportsfromusers.Notonlycanself-reportsbeunreliable present in the moment, with the addition of laughter, feeling
duetodemandcharacteristics,self-presentation,orHawthorne free, a sense of child-like excitement, and joy.
effect due to conformity to perceived norms or researcher While both flow and true fun emphasize high engagement
expectations[14],buttheyrepresentanoverallfeedbackofthe through optimal challenge, other theories recognize enjoy-
interaction rather than an instantaneous measure throughout. ment in less intense states. For instance, flow-like states
While affect recognition systems can detect laughter and can be differentiated from the overall positive valence of
smiles [15], enjoyment is a complex feeling that can be an experience [19]. This relates to the circumplex model of
conveyed through other multimodal cues (see Section IV-C). emotion [20], which features arousal (low to high) on one
Even in interpersonal communication, enjoyment has been axis and pleasure or valence (negative to positive) on the
analyzed from an external perspective only within the context other axis [21]. Certain theories of enjoyment focus on high
of marriage [16]. Thus, there is no scale or an automatic valence values that can contain lower-engagement positive
system for assessing user enjoyment in conversations with a emotions (e.g., content or calm), while others prioritize high-
robot, the former being required to develop the latter. arousal states (e.g., excitement) [19]. The ‘happy’ emotion in
This work contributes with the Human-Robot Interaction the circumplex model reflects a balance of high arousal and
ConversationalUserEnjoymentScale(HRICUES),illustrated positive valence. Our work aligns with the theory of true fun
in Fig. 1, which is a novel scale for assessing enjoyment in seeking both higher levels of arousal and valence, while
in conversations with robots from an external (third-party) incorporating other elements for classifying lower levels of
perspective, deriving from videos of 28 older adults’ open- enjoyment. Casual, open-domain conversations between older
domain dialogue with a companion robot using an LLM. The adults and robots may involve aspects of both arousal and
scale is developed through rigorous annotator evaluations and positive emotions. As such, both factors will be relevant to
discussions, and provides a structured framework for evaluat- our holistic model of enjoyment.
ing enjoyment in conversations with robots. The scale seeks
to offer an additional means of assessing user enjoyment in B. Assessing enjoyment
HRI by considering fine-grained conversation exchange levels Enjoyment is generally evaluated through self-reported
(i.e., turn-by-turn) and the overall interaction level, which questionnaires tailored to the specific application domain. For
complements self-reported enjoyment from users, with future instance,theQualityofLifeEnjoymentandSatisfactionQues-
potential use for autonomously identifying enjoyment in real- tionnaire (Q-LES-Q) [22] and Physical Activity Enjoyment
time HRI. In addition, by providing a detailed exploration of Scale (PACES) [23] are used in healthcare applications. In
theinstancesofannotators’concordanceanddivergence,based Human-Computer Interaction (HCI) and HRI research, enjoy-
on turn-by-turn analysis of enjoyment, in addition to the un- mentisnotoftentheprimaryfocusforevaluatinguserpercep-
derlyingreasonsfordiscrepanciesbetweenusers’self-reported tions,butistypicallyincludedaspartofamorecomprehensive
enjoyment ratings based on metrics typically used in HRI model [24]. Enjoyment frequently appears as a self-reported
studies, the study offers invaluable insights for understanding measure, such as perceived enjoyment in user acceptance
the nuances and challenges of assessing user enjoyment in models, such as the Unified Theory of Acceptance and Use
interactions with robots. Deriving from these challenges, a of Technology (UTAUT) [25], or as a single item within a
step-by-step guideline is offered for future HRI researchers to custom questionnaire, such as “Did you feel fun?” [26] or
adapt the user enjoyment scale to other application domains. “Was playing with the robot enjoyable/not enjoyable?” [27].
Enjoymentwasfoundtobehighlycorrelatedwith‘satisfying’,
II. BACKGROUNDANDRELATEDWORK
‘entertaining’, ‘exciting’,‘fun’, and ‘interesting’ in HRI [28].
A. Defining enjoyment
Technology Acceptance Model (TAM) [29] was also adapted
A popular definition of enjoyment is being in the state of to HRI by with measures of affect and cognition to improve
flow [17]. Flow is defined as the optimal experience, which its accuracy in explaining technology adoption that contains
provides a deep sense of enjoyment. It happens when an questions about perceived enjoyment [30].
individual is fully engaged in a task that provides an optimal User enjoyment has been shown to correlate with the
amount of challenge and engagement. Flow is characterized intentiontousearobotamongolderadults[1],highlightingits
by a set of factors, such as a fading sense of ‘self’, a sense importanceforlong-terminteractions.TheAlmeremodel[31]
that duration is altered, and deep and effortless involvement is an extended version of the UTAUT that is widely used
in the task. The theory states that enjoyment is not obtained in research on robots for older adults [32]. It incorporates
in a relaxed state, that it is necessary to be challenged, and enjoyment, social interaction, and social influence as factors
links repeated experiences of flow to mastery of a skill. mediating the acceptance and intention to use robots.IEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 3
While user enjoyment is commonly measured through self- at the end of the conversation. In addition, five annotators
reporting in HRI, there are several limitations with self- were recruited to annotate the dialogues from an external
reporting,suchasconformingtoperceivednormsorresearcher perspective. All annotators gave each exchange in the conver-
expectations or the (in)ability to recall the events and report sation a score in the following metrics: topic continuance (1:
correctly from memory [14], [33]. In addition, it is often ‘stronglychangethetopic’to7:‘stronglycontinuethetopic’),
desirabletoestimatewhatauserisfeelingbyassessingitfrom external sentiment (1: ‘the participants seemed bored with the
an external perspective when self-reporting is not possible or dialogue’ to 7: ‘participants seemed to enjoy the dialogue’),
the goal is to automate behavior at the dialogue exchange andself-sentiment(1:‘wanttostoptalking/confusedaboutthe
level during interactions in real-time. External assessments systemutterances’to7:‘enjoytalking/satisfiedwiththetalk’).
and self-reporting are not mutually exclusive, and can instead Notably, the two latter were labeled enjoyment on the upper
complementeachother.Theresultsofonemethodcanevenbe end of the scale. They received a good level of agreement
usedtovalidatetheother.Whilepriorresearchusedsmilesand between the annotators on the exchange level. To annotate
laughterforautomaticclassificationofuserenjoyment(within the dialogue level, a questionnaire was used with 18 items
thecontextofstory-telling) [15],thesesignalsareambiguous that were designed to represent three labels: ‘coordinateness’,
and highly context dependent [34], [35], and enjoyment can ‘awkwardness’,and‘friendliness’.Basedonthesecriteria,the
be displayed through other multimodal cues. developedmultimodalmodelhasoutperformedtheannotators’
in evaluating user satisfaction at the overall interaction level.
Ouruserenjoymentscalewasdevelopedbasedonthework
C. Assessing enjoyment in conversations
of Reimnitz and Rauer [16], which is the only scale that
User satisfaction, which correlates significantly with user specifically evaluates user enjoyment in conversations from
enjoyment in conversations [36], [37], is typically evaluated an external perspective. The study assessed the enjoyment in
in relation to a task [38], [39]. For instance, the Paradigm for conversations between 64 married couples and compared that
DialogueSystemEvaluation(PARADISE)[38]isaframework to each spouse’s marital happiness. For measuring enjoyment,
for evaluating user satisfaction in dialogue systems, based on they developed a scale for observational coding that took into
self-reported satisfaction on a dialogue level and is influenced consideration affective signs and the tone of the interaction.
by other metrics, such as task success in travel booking The scale ranged from 1 (very low enjoyment) to 7 (very
and accessing emails. Similarly, interaction quality, which high enjoyment), with 3 as a neutral anchor. Two annotators
evaluates user satisfaction from an external perspective on took into account both affective signs (e.g., mutuality of
the exchange level, was analyzed by three annotators in bus the interaction, tone of voice, consistent mutual gaze, facial
schedule inquiries with chatbots over phone calls from a data expressions, physical touching, body language) and the tone
corpusof200dialoguesandalabstudywith38subjects[39]. of the interaction (i.e., neutral, enthusiastic, and delightful)
An autonomous system was developed based on their ratings, when rating enjoyment. The annotators had good to moderate
which correlated highly with them, but not with users’ self- agreement, with intraclass correlation (ICC) on 20% of the
reported satisfaction scores, which was attributed to the sub- interactions. The study found that couples who displayed
jectivity of the measure and variability in user perceptions. high enjoyment in their conversations also reported having a
Usersatisfactionhasalsobeenmeasuredinthetextdomain happier marriage. This aligns with prior research in human
with an annotation protocol similar to our study, based on relationships, which found that mutually enjoyable behavior
a dataset of 1000 dialogues between 50 users and a chatbot leads to increased intimacy, trust, security, and satisfaction in
on attentive listening and conversations about animals [40]. long-term relationships [42], signaling that enjoyment could
Two annotators were recruited and an annotator alignment be highly influential in achieving long-term HRI.
session was conducted. The annotators were requested to go Similar to this scale, our study analyzes dyadic conver-
through the conversation exchanges once, without going back sations outside of task-oriented settings, but with a focus
or looking at the history. The annotators used three metrics of on evaluating enjoyment in conversations with autonomous
user satisfaction: ‘smoothness of the conversation’, ‘closeness robots. Although prior research described in this section
perceived by the user towards the system’, and ‘willingness evaluated autonomous conversational systems using user sat-
to continue the conversation’, rated from 1 to 7. However, isfaction metrics, which touch upon aspects of enjoyment,
no agreement was found between the annotators, even after these metrics are often more focused on task-specific aspects.
changing the granularity of the scale to two levels, low In contrast, our study emphasizes understanding enjoyment
and high, showing the complexity of evaluating a subjective within daily (open-domain) conversations with robots, based
measure from an external perspective. The study focused on on older adults’ interactions with a companion robot. To the
developing an algorithm to measure user satisfaction. best of our knowledge, no prior study measures enjoyment
A similar study, also exploring user satisfaction, was con- in open-domain conversations with robots from an external
ducted to develop a multimodal model based on annotated perspective that take into account multimodal aspects of HRI.
data [41]. The data corpus consisted of conversations of 60 Our work seeks to bridge this gap by proposing a scale that
participants with a virtual agent that was controlled through not only emphasizes the importance and complexity of user
WizardofOz.Thewizardannotatedtheusersatisfactionfrom enjoyment, but also provides a methodological framework to
their perspective after each dialogue exchange, and the user assess it from an external perspective in other application
and wizard both provided the satisfaction level of the user domains of HRI. The scale aims to provide an additionalIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 4
tool to evaluate enjoyment complementary to self-reported 2023). Prompting was used to give an empathetic persona to
measuresfromuserswithinconversationexchangeandoverall therobot,guidingittoaskopenquestions,listenactivelywith
interactionlevels,withthepotentialtobeusedforautonomous follow-up questions, and reflect on situations.
systems to adapt the conversations on the fly. Initially, English was chosen as the communication lan-
guage for speech recognition (Google Cloud Speech-to-Text),
III. DATACOLLECTION dialogue generation, and synthesis (Amazon Polly) due to the
moreextensive trainingdata available forLLMs. However,an
Thisworkaimstoofferanadditionaltoolforassessinguser
initial study with Swedish-speaking older adults showed the
enjoyment in conversational HRI. Our research seeks to eval-
need for communicating in their native language [12]. Hence,
uate perceived enjoyment within conversation exchange and
Swedish was used in the follow-up study investigated in this
interactionlevelsfromanexternalperspective,tocomplement
paper. A USB microphone array (Seeed Studio) was used in
self-reported questionnaires and provide a holistic view of the
both studies to obtain clear audio for speech recognition.
interaction as it progresses. To achieve this, we embarked on
While the conversation with the robot was autonomous, a
developing a user enjoyment scale and validating it within
wizard interface was used to start the interaction with the
the context of an HRI scenario. As outlined in Section I,
user by entering the participant ID. The initial and final1
achieving and maintaining user enjoyment is important for
robotresponseswerepre-scriptedtoensurethattheinteraction
engaging users and encouraging continued interactions with
started and ended the same way for all participants. The
robots, especially in daily encounters. This becomes particu-
rest of the interaction was fully autonomous based on the
larlyprominentforcompanionrobotsforolderadultsthataim
user’s responses and the responses generated by the LLM.
to provide social and emotional support to mitigate loneliness
The wizard interface was also used to end the interaction if
in their daily lives. Having daily conversations spanning a
necessary, i.e., if the participant wants to end the conversation
wide range of topics, i.e., open-domain dialogue, plays a
early or an error occurs in the system that requires a restart to
pivotal role in achieving this, as it enables more engaging and
continue the conversation where it is left off. In the initial
fulfilling conversations that cater to the user’s various emo-
study, the participant ended the interaction whenever they
tional and cognitive needs. While this has been challenging
wanted (by saying “Goodbye”). In contrast, in the second
to achieve in the past with conversational robots [43], recent
study, a 7-minute timer was set (checked automatically after
advancements in LLMs enable these capabilities, which make
each user response), after which the robot would say its pre-
them suitable architectures for companion robots. In addition,
scripted response to ensure a fair comparison between users.
these robots should be tailored to older adults’ unique needs
and preferences to meet their expectations and provide them
B. Preliminary Interviews
with enjoyable conversations. Thus, it is crucial to iteratively
involve older adults in the design process to both learn their To understand older adults’ perceived benefits and chal-
needs and effectively assess the developed systems. lenges in interaction with a conversational companion robot,
Thus, this work builds upon the data from our prior work and correspondingly develop a robot that meets their expec-
on the participatory design development of an autonomous tations, we conducted preliminary interviews with Swedish-
companion robot that integrates an LLM for conversations speaking older adults aged 65 and over. The study consisted
with older adults, as described in [12], to build the user of pre- and post-interaction interviews, a demonstration by a
enjoyment scale for conversational HRI and evaluate enjoy- researcher having a conversation with the robot (5 min), and
ment. Participatory design was conducted in two stages: (1) individual interaction (4 to 13 min) of the participant with the
Preliminary interviews with 6 Swedish-speaking older adults robot until they ended the interaction. The interaction with
who talked to the robot individually for 4-13 min, (2) after the robot was conducted in English, but the interviews were
the development to eliminate initial challenges, four design made in Swedish. All robot interactions were video-recorded.
workshopswereconductedwith28olderadultswhotalkedto All participants gave informed consent for recording, analysis
therobotfor7minwithsurveysandinterviewsthatfollowed. of data, and anonymized (blurred and without a name) image
This section summarizes the robot architecture used and the and video sharing for publications.
study details, described in detail in [12]. 1) Data: The study had 6 (3 men, 3 women) Swedish-
speaking healthy older adults, between 66 to 86 years old
(M =78.3,SD =8.3).However,duetoitbeingtheveryfirst
A. Robot Architecture
evaluation of an LLM on a social robot with older adults, the
The Furhat robot was employed in the study, featuring
interactions had a lot of failures that are analyzed thoroughly
a neutral-looking face that underwent user validation before
in [12]. Interaction failures can lead to lower likeability and
interactions. The robot’s face engine incorporated smiles and
satisfaction [44] and cause negative tone and emotion in user
eyebrow raises during conversations to enhance naturalness
responses [45]. Thus, to prevent biasing the enjoyment scale
and provide non-verbal feedback to users without context
solely towards negative experiences, which are common due
analysis. To further refine the interactions, the robot incor-
porated subtle behaviors like blinking, eye shifts, and brief 1Robot response to start the interaction: “Hello! I am Furhat, the person-
gaze aversion while speaking, based on silences in user input. alized companion robot. What is your name?” Robot response to end the
interaction:“Bye!Hopetoseeyouagainsoon.”orsimilarinthefirststudy;
GPT-3.5 (text-davinci-003, OpenAI) was used for dialogue
“Iwouldlovetotalkmoreanothertime,butforthesakeoftime,Ineedto
generation,asitwasthemostcapableLLMatthetime(March saygoodbye.Thankyoufortalkingwithme.Takecare!”inthesecondstudy.IEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 5
to technical limitations of current architectures but potentially the participant’s face. All participants gave informed consent
willbeovercomeinthefuture,butalsohaveanunderstanding for recording, analysis of data, and anonymized (blurred and
of user reactions to frequent technical failures in current without a name) image and video sharing for publications.
architectures, we chose the most successful interaction (as 1) Data: Participants were recruited by distributing the
defined by the least number of failures and the longest length invitation at our university’s communication channels, social
of interaction) from this study to be included as a basis of media,andplatformsforgatheringseniorcitizens.Intotal,28
alignment for developing the enjoyment scale, as described (13 men, 15 women) Swedish-speaking healthy older adults
in Sections IV-B and IV-C. The other criterion was that the between 66 and 86 years old registered as volunteers. We
interaction contained ‘highs and lows’, that is, the participant divided this data for the purposes of this study.
reactedpositively(e.g.,smile,laugh),neutrally,andnegatively Two of the subjects were selected for annotator alignment:
(e.g., frown, getting impatient) in the video to enable the a 69-year-old woman (denoted as S1) and a 75-year-old man
annotators to understand the spectrum of responses. (denoted as S2). S1 and S2 did not have any prior interaction
The corresponding subject (denoted as S0) was an 83-year- with a robot. S1 and S2 lived with their partners in their
old male without prior experience of robots. The participant own homes. The selection basis was to find interactions
lived with their partner in their own home. The interaction containing a range of ‘highs and lows’, as in the previous
lasted13.5min(53turns).Itcontained23interruptionsbythe study, aiming to complement S0 with interactions with fewer
robot (turn-taking error), 3 speech recognition errors (due to failures. S1’s interaction lasted 7.5 min (27 turns) and had
Swedishname),14repetitiveresponses,and8emptyresponses 14 interaction failures: 4 repetitive responses, 9 turn-taking
from GPT-3.5, which was in the form of clarification requests errors, 1 disengagement cue (i.e., robot responding in a way
(e.g., “Could you repeat that please?”). The response time that brings the conversation to a dead-end, e.g., “It is good to
of the robot was 2-3 seconds, primarily due to the response know.”). S2’s interaction lasted 7.3 min (27 turns) and had 5
generation time of the LLM. The video was recorded from a interactionfailures:1speechrecognitionfailure,3turn-taking
side angle, facing both the participant and the robot. errors, and LLM trying to prematurely end the conversation,
whichledtoexperimenterinterference(i.e.,telltheparticipant
C. Participatory Design Workshop (Source of Enjoyment An- they can continue if they want to).
notation Data) After the initial alignment of annotators with 3 videos to
create HRI CUES (Section IV), the remaining participant
Following the preliminary interviews, technical improve-
interactions (except one due to lack of a side-video) were
ments were made for turn-taking, the robot’s persona, and
usedforHRICUESevaluation(SectionV).Theresultingdata
architecturetoovercometheinteractionfailures.Basedonthe
consistedof25participants’(12men,13women)interactions,
feedback from the interviews, the interaction language was
with a mean age of 74.6 (SD = 5.8). 20 participants had no
changed to Swedish. Subsequently, four participatory design
prior interaction with a robot, and only one had previously
workshops were conducted with 28 older adults having an
talked with a robot. Interaction duration was M = 7.4 min
autonomous open-domain conversation with the robot indi-
(SD = 1.5) with 12 to 29 turns. Each turn lasted 5 to
vidually for approximately 7 minutes. Prior to the robot inter-
61 seconds (M = 17.7, SD = 7.2). The total duration of
actions, the robot’s capabilities were demonstrated through a
the videos was 174 min, corresponding to 590 turns. The
researcher having a conversation with the robot (2 min), and
interactionfailuresrangedfrom0to18(M =6.7,SD =4.1).
focus group discussions were made using design scenarios of
everyday activities to understand their expectations of com-
panion robots. The researcher(s) were present in the room (to IV. ASSESSINGENJOYMENTFROMCONVERSATIONS
interfereifnecessary)duringtheindividualrobotinteractions. This work addresses the lack of user enjoyment analysis
Following the interactions, the participants completed a 68- of conversation from an external perspective in HRI. We start
question Likert scale (1 to 5) questionnaire based on HRI and from an existing enjoyment scale in human-human relations
open-domain dialogue literature. Based on several studies de- to develop HRI CUES, by complementing it with annotations
scribedinSectionII,usersatisfaction,fun,andinterestingness of older adults’ interactions with a conversational companion
of the conversation were evaluated and categorized under the robot. This section describes not only the scale proposed in
user enjoyment construct. To account for discomfort in the this work, but also a final complete methodology to evaluate
conversation[46],thestrangenessoftheconversationwasalso enjoyment from videos of conversations with robots. It also
evaluated, similar to [47]2, by reverse-coding it in analysis: provides annotation guidelines and details the practices taken
1) I was satisfied with my conversation with the robot. for establishing inter-rater reliability in annotations [48].
2) It was fun talking to the robot.
3) The conversation with the robot was interesting. A. Annotator Selection
4) It felt strange talking to the robot.
Due to the lack of a clear definition of user enjoyment
All interactions with the robot were video-recorded by an
and its subjectivity resulting in high variability in both user
external camera facing both the participant and the robot at
perceptions and understanding by a third party, the selection
a side angle, as well as through the robot camera to record
of the right experts as annotators is critical. This is important
in general, especially in multidisciplinary fields like HRI, in
2“Did you feel something strange in that dialogue with the robot?” was
usedin[47].WeadapteditslightlyfortheLikertscale. particularwheninvestigatingcomplexconceptslikeenjoymentIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 6
that have relatively different meanings in different academic
traditions [49]. The annotators in this study should not only
be able to detect and understand multimodal cues exhibited
by the users to detect enjoyment, but also align well in their
perceptions, such that this measure can be used by other re-
searchers based on their understanding and recommendations.
In addition, being well-versed in the literature of the user
metric in question (user enjoyment) as well as its difference
from similar metrics (e.g., user satisfaction) is necessary to
ensure correlations with prior literature, as well as users’
reported perceptions of such metrics.
Familiarity with the target population (participant group) is
also important in establishing a better understanding of their
needsandreactions.Researcherswhosebackgroundsfocuson Fig.2. AnnotatorratingstructureofuserenjoymentintheELANsystem.
HRI with the target population (e.g., older or young adults,
children, people with disabilities) can put their interactions in
• 1 (very low): no evidence of pleasure. The user never
context from social, cognitive, and ethnographic perspectives.
has fun or enjoys the conversation, although there may
Annotatorsalsoneedtobethoroughlyfamiliarwiththesocio-
be joint interaction.
culturalbackgroundoftheparticipants,ascultureaffectstheir
• 3 (natural anchor): there is occasional positivity that is
perceptions of robots and their interactions [50]. In addition,
notstrongorfrequentlydisplayed.Theuserdoesnothave
understanding the nuances and culture-specific idioms (e.g.,
real joy or enthusiasm for the conversation.
‘cold turkey’) and proverbs (e.g., ‘bite the bullet’) in conver-
• 5 (very high): the user is very satisfied with the conver-
sations will be easier for an annotator that is a native speaker.
sation. The user shows enjoyment in their conversation
While a combination of all these aspects is difficult to find
marked with enthusiasm and/or delight.
in a single annotator, a group of annotators would be able
to complement each other, such that during alignment and Annotators were encouraged to use the full scale (i.e., not
development of the scale, their horizons can be expanded by abstaining from giving 1 or 5).
the perspectives of the others. While typically two annotators Due to the subjectivity of user enjoyment, it was necessary
establish inter-rater reliability in qualitative analysis, employ- to establish common grounds on the levels of user enjoyment
ing three annotators could better suit the complexity of the prior to the annotators analyzing all the robot interactions
task, allowing for tie-breaking and alignment across diverse individually [48]. Thus, three exemplar videos (S0, S1, and
backgrounds [48], [51]. Correspondingly, we selected three S2) as explained in Sections III-B1 and III-C1 were chosen
annotators (M = 30, SD = 2.94) who are researchers in that contain a range of negative and positive responses from
age
the mid-late stages of their PhD, with a background in user the user and a variety of technical failures.
enjoyment(Annotator1,denotedasA1),HRIwitholderadults Inordertoincrementallyfamiliarizetheannotatorswiththe
andcognitivescience(A2),andmultimodalHRIandcognitive modalities that a front view (taken from the robot’s camera)
science (A3). The annotators were native Swedish speakers andsideview(externalcamerafacingrobotandparticipant)of
and thoroughly familiar with Swedish culture. robot interaction may introduce, the first exemplar video (S0)
containedonlythesideview,thenextone(S1)containedonly
B. Familiarizing with Data the front view, and finally the third one (S2) contained both
views,asshowninFig.2andasusedinthefinalannotations.
As a starting point to familiarize annotators with the data
Conversational turns (exchanges) were chosen as the basis
and develop a user enjoyment scale for conversational HRI,
of annotations, because they were mostly similar in duration
the annotators were given a slightly adapted version of the
for participants, as well as for paving the way for under-
enjoyment scale by Reimnitz and Rauer3 [16] on human-
standinguserenjoymentviaautonomoussystemstoadaptand
human conversations of married couples, in which ‘user’ was
improve the interaction continuously. As such, the segments
used instead of ‘couple’, and references that relate to couples
to be annotated were created automatically based on the turns
(‘mutual enjoyment’, ‘affective sharing’, and ‘exuberance’)
(Robot-Participantpairs)inmanually-corrected(fortimingand
were removed. Instead of a 7-point Likert scale, which may
content) transcripts. All videos started with the robot’s first
be difficult to align given only the lowest, natural anchor, and
phrase (greeting of the user). A turn ends (and a new turn
highest enjoyment values, a 5-point scale was used:
starts) when the participant stops speaking, as that holds the
31 (very low): no evidence of pleasure. Pair never has fun or enjoys potentialtobedetectedbyanautomaticsystemforevaluating
the interaction, although there may be joint interaction. There is no mutual the turn that could be used to generate a new response.
enjoymentofpositiveaffectornegativeinteraction.3(neutralanchor):there
Annotators were guided to apply the rating scale on a
isoccasionalpositivitythatisnotstrongorfrequentlydisplayedandmaybe
displayed by only one partner towards the other. Pair is doing OK together per-turn basis, assessing both the robot’s response and the
butwithoutrealjoyorenthusiasmfortheirsharedinteractions.7(veryhigh): participant’s subsequent input within each turn. Furthermore,
thepairisverysatisfiedwiththeinteractionandactivity.Thecoupleshows
they were tasked with delivering an overarching assessment
mutualenjoymentintheirinteractionmarkedwithsharedexuberanceand/or
delight.Thereisconsistentvisualregardcoupledwithaffectivesharing. of enjoyment encompassing the entire interaction, referred toIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 7
as overall enjoyment. Within this context, annotators were Annotators were encouraged to discuss whether they made
encouraged to provide an in-depth rationale for their ratings, use of these elements in their analysis, their usefulness and
adopting an open-ended approach to offer comprehensive importance in assessing user enjoyment (even if these aspects
insights. They were asked to elaborate on the aspects and were not present in the videos), including any other aspects/
multimodal cues they considered in shaping their evaluations, cues they have previously used during familiarization.
alongwiththemethodologytheyemployed.Additionally,they Annotators were requested to systematically review the
were asked to provide whether any challenges or difficulties three videos, examining each conversation turn individually.
were encountered while evaluating overall user enjoyment for Theywerepromptedtoassessvariousfactors,includingrating,
the interaction and its details. The ratings for each turn were cues, and aspects, while also considering the detectability
tobe recordedwithin theELAN file(Fig. 2),while aseparate of user enjoyment in each turn. Annotators were further
documentwasdesignatedforannotatorstorecordtheiroverall instructedtoidentifycontrastingandsupportingargumentsfor
interaction rating and provide open-ended responses. their ratings, as the reasons behind the divergence between
the annotators can be just as, if not more, valuable than the
C. Annotator Alignment
concordancebetweentheannotators[62].Uponcompletionof
Based on their individual annotations of three robot in- theturnanalysis,annotatorswereguidedtodiscusstheirover-
teraction videos (S0-S2), annotators were asked to meet to all user enjoyment rating, identify which aspects, multimodal
align themselves to decide more objectively what each level cues, and conversation turns contributed most significantly
of the scale corresponds to, such that an agreement can be to their conclusions, and consider the relative importance
reached for the analysis of the remaining interaction videos. (weights) assigned to these modalities in their assessments.
In addition, they were asked to discuss the aspects and Annotators were also encouraged to evaluate whether specific
multimodal cues used to give the corresponding scores, in a aspects or cues were observable from both the frontal view
turn-by-turn fashion, as well as the overall user enjoyment. (the robot’s camera that directly captures participants) and
To facilitate discussions, a list of aspects and multimodal the side view (the external camera that captures both entities)
cues from HRI [52]–[55], HCI [54], and human-human inter- and reflect on how analyzing these different perspectives may
action [16], [56]–[59] literature was given to the annotators, haveinfluencedtheirassessments.Annotatorswereadvisedto
which were previously used in affective computing, user en- keep their ratings, both within ELAN and the accompanying
gagement,userenjoyment,conversation,andturn-takinganal- documentthatjustifiestheirscores,readilyaccessibleontheir
ysis, in addition to the principal researcher’s analysis of the screens as a reference point during the discussions.
challenges of applying LLMs into conversational robots [12]: Self-reported user perceptions were given to the annotators
• Facial expressions: smile, laughter, frown, rolling eyes, at this stage, as reported by questionnaire ratings in terms of
sigh, other expressions (e.g., smirk, squinting eyes, rais- the level of user satisfaction, fun, interestingness of the con-
ing eyebrows). Emotion models [20], [60], [61] were versation,andstrangenessoftalkingtotherobot,asdescribed
described for further context. in Section III-C. These metrics correlate highly with each
• Gaze: Mutual gaze, gaze length, gaze aversion, other other (Cronbach’s α = 0.84), with lower α when any of the
gaze targets (e.g., objects, experimenter) metrics are excluded. The annotators were instructed to view
• Body language: Gestures, gesture duration, gesture fre- the ratings after the overall user enjoyment in the interaction
quency, gesture intensity, posture, body orientation, head had been discussed for the corresponding participant. Based
orientation, arm position (e.g., folded/ open), movement, on the user perceptions, how these results correlate with their
physical contact, pointing, adaptors (e.g., touching hair, findings and the reasons behind discrepancies were discussed.
bouncinglegs),nodding/headshakes,proximity(distance Finally, the annotators were asked to develop a user en-
to the robot) joyment scale for conversational HRI that serves as both a
• Vocal features: Tone, pitch, pace, volume/ loudness, guideline for their remaining annotations and a reference for
energy future research on user enjoyment (Section IV-D).
• Dialogue responses: Content, sentiment, length, mirror-
ing,pausesinresponse,rephrasing/clarifications,anthro-
D. Human-Robot Interaction Conversational User Enjoyment
pomorphism, disengagement cues (responses that bring
Scale (HRI CUES)
the conversation to a halt, e.g., “That is good to know”)
• Conversation: Context, topic, topic initiation, topic clo- The discussions between the annotators (A1, A2, and A3)
sure, topic duration, tone (e.g., neutral, enthusiastic, and lasted four hours, during which they carefully went through
delightful), vocal fillers (e.g., “uh”, “erm”), conversation three example videos (S0-S2) that they had previously an-
length,repairs(dealingwithfailuresininteraction),refer- notated and discussed the cases one by one. The annotators
ral to previous topics/parts in a conversation, willingness viewed every turn in the interaction several times and ex-
totalkaboutpersonalmatters,askingquestionsaboutthe changedthereasonsbehindtheirratingintermsofmultimodal
conversation partner (robot), agreement/ disagreement cues, until they all aligned on the rating for each turn.
• Turn-taking: Speaker dominance, willingness to take a Simultaneously, they created a list of signs of enjoyment
turn, interruption, response time, backchannelling and dis-enjoyment that they had used during their enjoyment
• Interacting with others: Interacting with the experi- evaluation. Towards the end of the session, they settled on
menter/thirdpartyduringtheconversationwiththerobot a 5-item scale based on the initial provided scale, rankingIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 8
from very low enjoyment to very high enjoyment. The final As general guidelines for annotating user enjoyment, it
user enjoyment scale, namely the Human-Robot Interaction became clear that it was important to get acquainted with
Conversational User Enjoyment Scale (HRI CUES), is: the participants, where different participants had different sets
of signals. While watching the videos, the annotators learned
1 Very low enjoyment — Discomfort and/or frustration
each person’s rhythm and gestures for what was interpreted
2 Low enjoyment — Boredom or interaction failure
as a ‘baseline’ behavior from which the person could deviate
3 Neutralenjoyment—Politelykeepinguptheinteraction
during the interaction. This means that the same type of
4 High enjoyment — Smooth and effortless interaction
gesture(e.g.,keepingone’sarmscrossed)couldbeinterpreted
5 Very high enjoyment — Immersion in the conversation
differently for different participants. Instead, an emphasis was
and/or deeper connection with the robot
placed on the change in behavior. It was also important to
Two exchanges per rating of the scale from the alignment
separatecontentfromcontext,i.e.,itisessentialtobemindful
interactionsareprovidedasillustrativeexamplesinthevideo4.
ofwhatisbeingsaid(conversationcontent,e.g.,topic),butthe
To rate an exchange higher on the user enjoyment scale focus should be more on the whole feeling of the exchange.
(4 or 5), the annotators looked for different signs of enjoy- The interaction failure does not necessarily refer to a robot
ment, which, for example, were smirking, movement, flow failure (e.g., incorrect response, speech recognition failure,
of conversation (the topic is moving forward), no strain or turn-takingerror,disengagementcue),sincerobotfailurescan
discomfort, asking questions [to the robot], smooth turn- leadtoamusement,anthropomorphism,orempathyintheuser,
taking,dynamictonality(anddynamicphrasingofsentences), thereby increasing user enjoyment. Interaction failure rather
being playful, sharing personal experiences [to the robot], refers to the situation when either the user (e.g., interrupting
sharing an understanding (common ground) [with the robot], the robot) or the robot made a failure that resulted in the
and anthropomorphizing [the robot]. interruption being disrupted, leading to low enjoyment.
To rate an exchange lower on the scale (1 or 2), the When annotating the videos, the annotators assumed that
annotators looked for signs of dis-enjoyment, which, for in the future, robots would be able to judge the level of user
example, were low energy, sighing, tiredness, long breaths, enjoymentinreal-timewhilehavingaconversation.Therefore,
restless movements (i.e., adaptors, such as moving in the the videos were annotated segment by segment (turn by turn),
chair from side to side or changing arm position), flat tonal- with each segment being watched only once, similar to [40].
ity, silence, awkward and negative facial expressions, flaring
nostrils, disengagement cues (e.g., turning away from the E. Annotation using HRI CUES
robot, or responding in a way that disrupts the conversation After establishing HRI CUES, all annotators independently
flow, such as “That is true”), and topic closure (e.g., “Let’s ratedtheremaining25videosdescribedinSectionIII-C1.The
talk about something else”). In addition, robot behaviors that same methodology was employed as in Section IV-B, with
disruptedtheinteractionflow,suchasrepeatedquestions,were the only difference being the enjoyment scale, as HRI CUES
considered to be strong causes of dis-enjoyment. was used instead of the initial scale. That is, the annotators
Neutral enjoyment (3) refers to a lack of these cues, in rated 590 turns (174 min) using ELAN with both side and
which conversation content (and context) becomes more rele- frontalviewoftheinteraction(Fig.2),viewingeachturnonly
vant, such as having small talk or continuing the conversation once, in addition to providing an overall enjoyment score per
without having much interest in the topic. interaction.Theyalsoprovidedanexplanationfortheiroverall
In cases where the exchange has cues from multiple scale ratingsandthechallengestheyfacedduringtheannotation,as
levels, the annotators determined the dominant level in that in Section IV-B. The annotation was conducted over 8 days.
interaction. This could be done by observing the intensity The results are reported in the next section.
of the cues, the significance of the cues, or the interaction
trajectory. On the other hand, if an annotator observed strong
V. RESULTS
cuesfromtwomoderatelyorhighlydistinctlevels(asopposed Following the discussions involved in the annotator align-
to subsequent levels), they would annotate using a level mentthatredefinedtheuserenjoymentscaleandmethodology,
between those. For instance, as evident in this exchange5, annotators rated the remaining 25 videos of robot interactions
when there is discomfort at the beginning (1), but the user from our participatory design workshop individually.
continues to politely keep up the interaction (3), the exchange
would be annotated as a 2, the mid-point between the levels. A. Distribution of User Enjoyment
Therewerealsoafewcasesthatweredifficulttocategorize Fig. 3 shows how each annotator rated the interaction
asenjoymentordis-enjoyment,andthereforewereinterpreted exchanges(turns),indicatingthattheinteractionsmainlywere
as more context-dependent, which, for example, were gaze (45.9%) regarded as neutral in enjoyment, with rare occur-
aversion, attention on the experimenter or camera, topic dura- rences of very low (9.2%) and very high (13.9%) enjoyment,
tion, and initiation. For instance, gaze aversion could be due showing a near Gaussian distribution of user enjoyment for
to thinking, floor management, intimacy regulation (cf., [63]), each annotator. Fig. 8 (in the Appendix) shows the rating
or as a reaction to something the robot said or did. distributions of annotators per participant, which display a
similar Gaussian distribution of perceived enjoyment by the
4HRICUESexemplaryexchanges:https://youtu.be/VmKvGM0pyec annotators, with some participants (e.g., P25, P27) perceived
5Exchangewithcuesfrommultiplelevels:https://youtu.be/2HA- 5B9JHs to have a more enjoyable interaction than others (e.g., P3).IEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 9
300 • When considering the average ratings of the remaining
twoannotators(k=2inICC),ICC(2,2)wasanimpressive
0.85, with 95% confidence interval of 0.66 to 0.93,
200
suggesting moderate to excellent reliability, which was
statistically significant (p < 0.0001). This was further
100 confirmed by the same F-statistic.
These results confirmed our initial conclusion. In addition,
0 removing A2 or A3 separately decreased ICC. Subsequent
1 2 3 4 5
discussionswiththeannotatorsfurtherconfirmedadivergence
Rating
in the ratings provided by A1 (Section VI-C2).
Annotator A1 A2 A3
2) OverallEnjoymentScore: Theoverallenjoymentscores
Fig.3. DistributionofHRICUESratingsacrossturnsperannotator. per annotator are presented in Fig. 2 in the Appendix. Relia-
bility among overall enjoyment scores was:
• The ICC(2) for single random raters was found to be
B. Rater Reliability
0.48, with 95% confidence interval ranging from 0.24
To evaluate the reliability of the annotators’ enjoyment to 0.69, indicating poor to moderate level of reliability.
ratings, we employed the Intraclass Correlation Coefficient This value was statistically significant (p < 0.001) with
(ICC), similar to [16], which is a statistical measure used an F-statistic of 3.74 (df =24, df =48).
1 2
to assess the reliability or consistency of ratings provided by • The average ratings from three annotators (ICC(2,3))
multiple raters (or annotators) [51]. ICC values range from 0 was 0.73, with 95% confidence interval of 0.48 to 0.87,
to 1, with higher values indicating greater agreement among suggesting a poor to good level of reliability, which was
raters6. This study focuses on two specific forms of ICC: statistically significant (p < 0.0001) with F-statistic of
• ICC(2) - Single Random Raters: Designed for situations 3.74 (df 1 =24, df 2 =48).
where each subject is rated by the same raters, and those Similar to per-turn analysis, excluding the divergent anno-
raters are considered to be randomly selected from a tator (A1) led to improved reliability:
larger population of possible raters.
• ICC(2) increased to 0.58, with 95% confidence interval
• ICC(2,k) - Average Random Raters: An extension of ranging from 0.25 to 0.79, suggesting a higher con-
ICC(2), applied when the average ratings of k raters are
sistency between two annotators. This was statistically
considered,enhancingthereliabilityofthemeasurement.
significant (p < 0.001) with an F-statistic of 3.74
Similarly to how our data was coded, we present rater (df =24, df =24).
1 2
reliability for each conversation exchange and the overall en- • When considering the average ratings of the remaining
joymentscoreprovidedbyeachannotatorfortheinteractions. two annotators, the ICC(2,2) for average random raters
1) Per Conversation Turn: The resulting annotations per was 0.74, with 95% confidence interval of 0.4 to 0.88,
conversation turn of 25 videos are shown in Fig. 9 in the Ap- further indicating enhanced reliability. This was statisti-
pendix.Treatingeachconversationturnasarepeatedmeasures cally significant (p < 0.001) by the same F-statistic of
factor in the reliability analysis: 3.74 (df =24, df =24).
1 2
• TheICC(2)was0.47with95%confidenceintervalrang- These findings underscore the importance of selecting con-
ing from 0.23 to 0.69, indicating poor to moderate level sistent raters and the benefit of averaging ratings across mul-
ofreliability.Thiswasstatisticallysignificant(p<0.001) tiple annotators to achieve enhanced reliability in measuring
with an F-statistic of 3.83 (df 1 =24, df 2 =48). user enjoyment in robot conversations.
• For the average ratings of all coders, the ICC(2,3) was
0.72 with 95% confidence interval of 0.47 to 0.87,
C. Correlations with Self-Reported User Perceptions
suggesting a poor to good level of reliability, which was
Users’ subjective ratings of enjoyment during the interac-
statistically significant (p < 0.0001). This was further
tionswereobtainedfromthequestionnaireintheparticipatory
supported by the same F-statistic.
design workshops after their interaction with the robot, in
Basedonthevisualinspectionofannotatorratings(Fig.9),
termsofusersatisfaction,fun,interestingness,andstrangeness
A1 was identified to diverge from A2 and A3. A1 was
of the conversation, as described in Section III-C, which are
substantially more positive (M = 3.31 for turns) from the
presentedinFig.2(intheAppendix)alongwiththeannotator
other annotators (A2 : M = 3.12, A3 : M = 3.11). To
overall enjoyment ratings per interaction. The items had high
confirm this, we evaluated ICC with A1 excluded:
correlation (Cronbach’s alpha = 0.84), with the removal of
• ICC(2)forsinglerandomratersrosesubstantiallyto0.74, eachitemreducingthecorrelationintheconstruct.Theseself-
with 95% confidence interval ranging from 0.49 to 0.88,
reportedscoresandtheaverageofthesescoreswerecompared
indicating a much stronger reliability between A2 and
against annotators’ overall enjoyment scores to evaluate how
A3. This result was statistically significant (p < 0.001)
well the annotators could perceive their enjoyment. Spearman
with an F-statistic of 6.52 (df =24, df =24).
1 2 correlation was used across four Likert scale items and the
averageofthesescores,with95%confidenceinterval(ranging
6ICCvaluelessthan0.5ispoorreliability,between0.5and0.75ismod-
erate,between0.75ad0.9isgood,andabove0.9isexcellentreliability[51]. from 0.71 to 0.92). The results were as follows:
ytitnauQIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 10
• Overall vs. User Average: Not statistically significant elsofcomfortandeasewithininteractions,capturingcompre-
(p=0.08) moderate positive correlation (r =0.36), hensive internal subjective enjoyment may require additional
• Overall vs. User Satisfaction: Not statistically signifi- data, such as initial expectations towards the robot, person-
cant (p=0.06) moderate positive correlation (r =0.39), ality traits, as well as physiological responses, not accessible
• Overall vs. User Fun Talking: Not statistically signifi- through direct observation. This discrepancy underscores the
cant (p=0.21) weak positive correlation (r =0.26), complexity of correlating observed behavior with subjective
• Overallvs.UserConversationInteresting:Notstatisti- internal states and highlights the challenge of fully capturing
callysignificant(p=0.68)veryweakpositivecorrelation user enjoyment in HRI. Despite these considerations, having
(r =0.09), an additional tool to evaluate user enjoyment in conversations
• Overall vs. User Felt Strange (Reversed): Statistically is highly valuable for HRI research to assess and develop
significant (p=0.04) moderate positive correlation (r = robots that are enjoyable to interact with, within all parts of
0.42). the conversation, especially in daily life. This approach aligns
with how humans naturally adapt our conversations in real-
VI. DISCUSSION time based on the external cues we observe in others. The
measure presented in this paper offers a valuable tool for
A. Enhanced Reliability with Averaged Annotator Ratings
collecting observational data to train autonomous enjoyment
While having a similar distribution of ratings by all anno-
detection systems that can be used on robots or other agents.
tators, the reliability analysis revealed a marked distinction
between ICC(2), which assesses the reliability of single ran-
domraters,withICC(2,k),whichconsiderstheaverageratings C. Final Alignment Post Annotation
of multiple annotators. The latter consistently demonstrated
After analyzing the results, the annotators were asked to
higher reliability across both overall enjoyment scores and
haveanothersetofdiscussionsbypresentingthemwithFig.9
conversational turn ratings. This finding highlights the sub-
and 10 in the Appendix (without reliability or correlation
stantial benefit of collective annotator wisdom over individual
scores), which lasted four hours. They were asked to pinpoint
assessments in assessing user enjoyment.
instances of rating divergence and concordance within their
The superior reliability of ICC(2,k) highlights the inherent
assessments. Particular emphasis was placed on identifying
variability in subjective experiences and perceptions of en-
turns where annotators disagreed or agreed most fervently.
joyment, suggesting that averaging across multiple annotators
Subsequently, the annotators were prompted to watch the
can effectively mitigate individual biases and variations in
corresponding video segments to delve into the reasons be-
judgment, leading to a more reliable representation of true
hind their ratings and the rationale for their agreement or
user enjoyment. This finding is critical, as it emphasizes the
disagreement. The primary objective was to gain insight into
importance of incorporating multiple perspectives to achieve
potential major concordance (Section VI-C1) and divergence
a more accurate and consistent evaluation of user enjoyment
(SectionVI-C2)inthewaydialogueexchangeswereannotated
in conversational interactions with robots. Consequently, the
and elucidate the underlying reasons for these variations.
distinction between ICC(2) and ICC(2,k) results not only
Following the turn-by-turn analysis, annotators were asked
validates the robustness of our user enjoyment scale but
to identify the two most significant discrepancies (highest
also illustrates the methodological importance of employing
and lowest) between their ratings and self-reported participant
multiple annotators for capturing the complex and subjective
perceptions (Section VI-C3). They were asked to engage in
nature of enjoyment in human-robot conversations.
discussions exploring potential reasons underlying the dispar-
ities and similarities between their perceptions and those of
B. Correlation with Self-rated Enjoyment Scale the users from multiple aspects based on their expertise.
To further validate our user enjoyment scale, we analyzed One final round of discussions (lasting 3 hours) between
correlations between the annotators’ overall enjoyment scores the annotators was conducted to adapt the developed scale to
and the participants’ subjective enjoyment ratings. Results other domains in HRI, extending its application beyond the
revealed a statistically significant, moderate positive correla- context of companion robots for older adults to encompass
tion between overall enjoyment scores and the (reversed) ‘felt other conversational contexts where analyzing user enjoyment
strange’ item, indicating that higher enjoyment scores were is crucial (Section VI-D). Based on those discussions, a step-
associated with decreased feelings of awkwardness during the by-step guideline was developed to replicate the methodology
interaction. This appears to greatly align with our annotator’s in this work and adapt the HRI CUES to other domains.
discussions and the resulting enjoyment scale that classifies Building upon insights gained from these phases, the an-
thepresenceofsignsofdiscomfortasthescale’slowestlevel. notators were invited to contemplate challenges in detecting
However, the correlations between annotator scores and enjoyment from conversations and the corresponding limita-
other user-reported measures — such as satisfaction, average tions of the study and the scale (Section VI-E).
enjoyment, fun in talking, and interest in conversation — 1) Concordance Between Annotators: In numerous in-
although trending towards moderate positive correlations, did stances across the videos, a concordant agreement was ob-
not reach statistical significance. These findings suggest a nu- served among the annotators. As an illustrative example, in
anced relationship between observed enjoyment and user self- the interaction of the ninth participant (P9, turn 13 in Fig. 4),
reportedexperiences.Whileannotatorscandetectgenerallev- the conversation exchange exhibited a seamless progression,IEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 11
P9 P10 P1 P25
5 5
4 4
3 3
2 2
1 1
1 5 10 15 20 25 1 5 10 15 20 25 1 5 10 15 20 25 1 5 10 15 20 25
Turn Turn Turn Turn
P9 P10 P1 P25
5 5
4 4
3 3
2 2
1 1
A1 A2 A3AvgSatFunInt Str A1 A2 A3AvgSatFunInt Str A1 A2 A3AvgSatFunInt Str A1 A2 A3AvgSatFunInt Str
Annotator A1 A2 A3 Annotator A1 A2 A3
User Rating Average Satisfaction Fun Interesting Strange User Rating Average Satisfaction Fun Interesting Strange
(Reversed) (Reversed)
Fig.4. Concordancebetweenannotatorratingsininteractions(P9andP10). Fig.5. Divergencebetweenannotatorratingsininteractions(P1andP25).
and the participant’s enjoyment level was distinctly conveyed of 3, while A2 rated it 1, and A3 as 2. In this context,
through expansive bodily gestures. Notably, all annotators the participant remained entirely silent, awaiting the robot to
unanimously assigned a rating of five for the exchange since initiate further interaction. Annotators interpreted this silence
the participant threw themselves backward in the chair laugh- differently, seeing it as politeness, boredom, or discomfort.
ing. In the same video, the annotators all assigned a rating of For another participant (P25, Fig. 5), during turn 8, A1
two for the exchanges where the participant’s response was assigned a rating of 5, whereas A2 rated it as 1, and A3
marked by sighing and a demeanor suggestive of resignation. as 2. The participant asked the robot to make more eye
This reaction occurred as a response to an unnecessary repe- contact with them, which could be interpreted as a period
tition initiated by the robot, specifically at turn 23. of heightened immersion and anthropomorphism or criticism.
In another example (P10, Fig. 4), the annotators assigned a Followingthis,theannotatorsconsistentlyexhibiteddiscordin
rating of 1 to turn 16 to indicate that the participant openly theirassessmentsuntilturn17,whentheyreachedaconsensus
expressed their negative thoughts due to the robot not making once more. For instance, at turn 11, A1 marked it as a 5,
eye contact with the participant. During this interaction, the A2 as 3, and A3 as 1. In this case, the participant expressed
participantalsoattemptedtoestablishcontactwiththeexperi- reservationsaboutsharingpersonalinformationwiththerobot
menter.Subsequently,theparticipantmadeanefforttopolitely due to unfamiliarity, yet did so while smiling and posing a
maintain a dialogue with the robot according to social norms, question to the robot in a playful tone. This complexity in the
a behavior that garnered consensus among the annotators as interaction exchange presented challenges for the annotators,
being representative of a rating of 3. asitencompassedamultitudeofactions.Whiletheverbalcon-
In conclusion, the annotators agreed when the user enjoy- tent suggested discomfort, the presence of laughter, smiling,
mentscalealignedclearlywithparticipantbehavior.However, and playful tonality indicated enjoyment. Consequently, the
in most cases, the interaction between the robot and the annotatorsencounteredmixedsignals,andtheresultingratings
participant did not correspond as clearly or unambiguously depended on which aspect of the interaction they prioritized.
to the user enjoyment scale. This is likely due to the complex The notable inconsistencies between A1’s ratings and those
and situational nature of the cues in the interaction, making it of the others led to the inclusion of reliability results for the
challenging to develop comprehensive yet precise guidelines more consistently aligned group of annotators (A2 and A3).
for annotation. Instead, the general scale needs to be inter- 3) Similarities and Discrepancies Between User and An-
preted by the annotators for the particular use case to find notator Perceptions: While annotators aligned well with a
anchor points that are appropriate for the specific context. large proportion of the participants in their perceptions, there
2) Divergence Between Annotators: Throughout the anal- were substantial discrepancies for some of the participants.
ysis of the 25 videos, there were instances where annotators For instance, for P6 and P21 (see Fig. 6), it can be noted that
differedsubstantiallyintheirassessments.Forinstance,forP1 the annotators and the participant interpreted user enjoyment
(Fig.5),atturn4,A1assignedaratingof5,whileA2scoredit in a similar way, with overall interaction scored as 5 and
as2,andA3as3.Theparticipant’slaughterposedachallenge 4, matching that of the average of user reported values. The
as it was perceived both as a sign of high enjoyment (by A1) conversations went well, and the participants seemed to take
and, conversely, as an expression of frustration towards the a playful approach in the interaction, which was reflected in
situation or the robot (by A2 and A3), rather than amusement both the participants’ and annotators’ scores.
with the robot. Furthermore, at turn 15, A1 assigned a rating However, in the case of P3 (Fig. 7), it was ethically
tnemyojnE
resU
gnitaR
tnemyojnE
resU
gnitaRIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 12
displaying enjoyable signs. The participant used the robot for
P6 P21
5 transactional requests rather than a casual conversation. The
transactionalnatureoftheconversationcombinedwithfailures
4
might explain why the participant gave low enjoyment scores.
3
The annotators gave a higher score because the participant
2
seemed to forgive the failures, laugh them off, and continue
1 the conversation smoothly. This can be seen as an important
1 5 10 15 20 25 1 5 10 15 20 25
Turn Turn reminderthattheannotatorsarenotalwaysassessingthesame
P6 P21 aspects as the participants in their self-assessment.
5
4 D. Adapting HRI CUES to Other Domains
3
2 The process of using the Human-Robot Interaction Conver-
1 sational User Enjoyment Scale (HRI CUES) is twofold. First,
the scale is the primary supportive instrument for engaging
A1 A2 A3AvgSatFunInt Str A1 A2 A3AvgSatFunInt Str
with the dataset and finding an agreement among annotators
Annotator A1 A2 A3
regarding which level of enjoyment an exchange represents.
User Rating Average Satisfaction Fun Interesting Strange
(Reversed) Secondly, it is important to highlight the cultural and context-
dependent changes when assessing enjoyment with the scale,
Fig.6. Similaritybetweenannotatorratingsanduserperceptions(P6,P21).
therefore, we recommend that the annotators reach an agree-
ment on which multimodal cues are important in their study.
challengingfortheannotatorsasitwasperceivedasborderline In the previous section, we presented the relevant cues in our
bullying.Theparticipantseemedtofeelunsafeintheenviron- study located in a Nordic Western setting, but these could
mentandwasrepeatedlyaskedtoelaborateonacontroversial differinanothersetting.Forexample,gestures,suchasthumbs
topicthatheexplicitlyaskedtodropandmoveawayfrom.The up, mean different things in different parts of the world. Our
participant might have rated the experience as more enjoyable proposedscalecanalreadybedirectlyappliedinmanysettings
and interesting than the annotators due to researcher bias, i.e., where users engage with social robots (or potentially other
that the participant felt the need to please the researcher [64]. agents) in conversations. However, in many other cases, we
Given the controversial statements made by the participant, encourage further adapting the scale to the particular domain.
the positive ratings can be interpreted as a strategy to avoid To replicate our methodology and adapt HRI CUES to
judgment from the researchers. Another reason could be the another domain, the following framework should be applied:
novelty effect, since they might have been happy to talk with 1) Recruit three annotators with relevant and complemen-
a robot regardless of the negative experience. tary backgrounds who are familiar with the specific
Anotherparticipant(P13,Fig.7)experiencedtheinteraction culture and context of the study.
as less enjoyable than the annotators interpreted. This might 2) Establishtheintendedusageoftheannotationssuchthat
be because the participant was experiencing a high number the annotators can tailor their annotations to fit that use
of technical and social failures from the robot while still case and align their views on the practical meaning of
enjoyment in that context.
3) Askannotatorstosystematicallyannotatethreeexample
videos(fromthedataset)usingtheHRICUES.Example
P3 P13
5 videos should exemplify different interaction outcomes
4 fromthedatasetand berepresentedfromvariousangles
(front, side, both). Encourage the annotators to look
3
for their own cues of enjoyment or dis-enjoyment, and
2
describe the reasons behind their overall enjoyment
1
1 5 10 15 20 25 1 5 10 15 20 25 scoresbasedonthoseafterviewingeachinteraction,and
Turn Turn the corresponding challenges of detecting enjoyment.
P3 P13 4) Arrange a discussion between the annotators to identify
5 contrasting and supportive arguments for multimodal
4 cuesassociatedwiththescale,aimingtopreciselydeter-
3 minethecuesforeachsegmentandstriveforconsensus
2 for the corresponding rating. Give each segment suffi-
1
cient time for discussions while avoiding getting stuck
A1 A2 A3AvgSatFunInt Str A1 A2 A3AvgSatFunInt Str on small details. When faced with a difficult case, note
Annotator A1 A2 A3 what is not agreed on and move to the next segment.
User Rating Average Satisfaction Fun Interesting Strange 5) Based on the discussion, construct an annotation
(Reversed)
schema,whichshouldcontainthecuesthatwereagreed
Fig.7. Discrepancybetweenannotatorratingsanduserperceptions(P3,P13). on for assessing the enjoyment in relation to the scale,
tnemyojnE
resU
gnitaR
tnemyojnE
resU
gnitaRIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 13
especiallyemphasizingthecuesthatwerediscussedand discussionsamongtheannotators,aligningonthedefinitionof
not immediately agreed on. enjoyment of relevance to the particular study is challenging.
6) Annotate the remaining dataset turn-by-turn using the One solution is to focus on the change of behavior within the
HRI CUES and the multimodal cues. This is done by exchange. For instance, if the robot’s response improved the
looking at each exchange once and annotating in real- user’s demeanor towards the robot, the exchange should be
time without going back in the data to not influence the rated towards the level that contains higher enjoyment cues in
evaluation of the beginning segments of the interaction the scale, and conversely if it had a negative impact. Other
by already knowing the end segments. alternatives could be to use an aggregated score, the rating
Inthispaper,weevaluatedHRICUESthroughtheinteractions thatcorrespondstothemajorityofthesegment,themost/least
of older adults with a conversational companion robot using extremerating,ortheratingthatcorrespondstothefirstorlast
Furhat at the university premises. However, this was only partofthesegment.Thechoiceofmethodforthesesituations
one example of how the scale can be used; HRI CUES shouldbedeterminedduringtheannotationalignmentprocess,
is generalizable to other contexts in which a user interacts taking into account the specific use case and domain.
with a social robot. Therefore, the second to fifth steps of The differences between the annotators’ overall rating on
the framework above are crucial for the annotators to adapt user enjoyment and user perceptions (as discussed in Sec-
the communication cues to their context and setting of the tion VI-C3) might be due to the participant’s expectation of
study. HRI CUES does not require any adjustment as such, the robot’s social and technical level, while the annotators
but it requires interpretation with respect to the context of only look at the interaction itself. In addition, since this is
the use case. The interpretation is facilitated by the six-step their first interaction with a robot for most of the participants
framework,hence,itisimportanttofindappropriateannotators (20 out of 25), the ‘novelty effect’ might have changed
who are familiar with the particular application area. their perceptions more positively or negatively, given that
the duration (7 minutes) is not long enough to overcome
it [66]. However, the variability observed by the annotators
E. Challenges and Limitations
in user enjoyment states throughout the interaction (e.g.,
HRI CUES is an ordinal scale, that is, each category is Fig. 5) shows that conversation context may alter the novelty
in rank order but the intervals between values cannot be effect, providing a more complete picture of the enjoyment
presumed equal [65]. The middle level (3) is not simply throughout the interaction than a self-reported score at the
an average of the extremes of the scale, but rather a polite end of the interaction. In addition, the users’ responses to the
interaction characterized with some neutral valence. The dis- questionnaire may differ from their actual attitudes towards
tance from this middle level generally corresponds to greater therobot[67].ThesesupporttheimportanceofHRICUESas
intensity of (dis)enjoyment, but qualitative aspects were also anadditionaltooltoevaluateuserenjoyment,providingmeans
considered. Using only five levels in the scale can make the forreal-timeestimationofenjoymentinconversationalagents.
assessment fairly coarse, and some of the exchanges did not Our scale is designed to serve as a tool for assessing
necessarily fit into either of the levels, but to the middle perceived user enjoyment during interactions with robots,
of them. For instance, during the alignment discussions, the intended primarily for researchers in the field of HRI. How-
annotators rigorously discussed whether an exchange should ever, given that open-domain dialogue may involve sensitive
be rated as a 2 or 3 because there was a disruption in the information that individuals may be hesitant to share with un-
interaction due to a robot failure, but the participant politely familiarparties,itisimperativetoobtainexplicitconsentfrom
kept the interaction. Eventually, it was rated as 2 due to the participants before they talk with the robot. In our studies, we
robot failure driving the interaction flow. The coarse nature ensuredthisconsentpriortoparticipants’interactionswiththe
of the scale has likely resulted in disagreements between robot, employing the use of the term ‘sentiment analysis’ in
annotators, such as ratings oscillating around a value. The the consent form, and explained to all participants that their
low number of levels might, however, also be a strength since interactionswouldbeanalyzedintermsoftheiraffectivestates
largergranularitymightprovideanillusionofhigherprecision, (‘identificationoffeelings’)bybothresearchersandautomated
without actually capturing the affective state more accurately. systems. While obtaining consent is essential for researchers
The length of the assessed segments was a factor that utilizing our scale in future studies, it is crucial to recog-
provided additional challenges. When segments were overly nize that this process may influence participants’ behavior
brief, they did not always contain sufficient information for and conversation topics, as they may be reluctant to delve
a fair assessment. More frequently, however, excessively long into sensitive memories or be concerned about being judged
segmentswerecomplextoanalyze.Forexample,asdescribed by others. Consequently, this can result in a disconnection
inSectionVI-C2,theannotatorsofteninterpretedtheexchange with the robot, posing challenges in achieving high levels
differently due to long segments that contained several cues of enjoyment during interactions. Nonetheless, this impact
belonging to separate levels of the scale. In these cases, a is likely to diminish over the course of the interaction or
different assessment approach may be necessary as the longer across multiple interactions, particularly in long-term settings.
segments introduced an additional factor for the annotators to Additionally, researchers must exercise caution to uphold the
consider: which part or aspect of the exchange to emphasize privacy and confidentiality of participants when sharing data,
in the assessment. Due to the complex nature of enjoyment ensuring that they remain unidentifiable in images and videos
as a concept, which highlights the necessity of pre-coding (as demonstrated in the examples provided for the scale),IEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 14
andremovinganysensitiveinformation.Moreover,researchers REFERENCES
should remain mindful of their own biases and subjectivity,
whichmayleadtovariationsintheinterpretationofenjoyment [1] M.Heerink,B.Kro¨se,B.Wielinga,andV.Evers,“Enjoymentintention
to use and actual use of a conversational robot by elderly people,” in
compared to the participants’ experiences. User enjoyment is
Proceedingsofthe3rdACM/IEEEinternationalconferenceonHuman
often context-specific, indicating that users’ behavioral and robotinteraction,2008,pp.113–120.
affective expressions are connected to specific socio-cultural [2] J.Abdi,A.Al-Hindawi,T.Ng,andM.P.Vizcaychipi,“Scopingreview
contexts, including values, norms, and expectations of what is ontheuseofsociallyassistiverobottechnologyinelderlycare,”BMJ
Open,vol.8,pp.1–20,2018.
consideredappropriateincertainsituations[68].Theseunder-
[3] S. Thunberg, M. Arnelid, and T. Ziemke, “Older adults’ perception of
scorethesignificanceofemployingmultipleannotatorsforthe the furhat robot,” in Proceedings of 10th International Conference on
scale that are familiar with the socio-cultural background of Human-AgentInteraction(HAI22),2022,pp.4–12.
[4] S. Kuoppama¨ki, R. Jaberibraheem, M. Hellstrand, and D. McMillan,
the target population, and using the scale to complement self-
“Designing Multi-Modal Conversational Agents for the Kitchen with
reported user perceptions to have a deeper understanding of Older Adults: A Participatory Design Study,” International Journal of
the interaction quality. As human-robot interactions continue SocialRobotics,Sep.2023.
[5] H.R.Lee,S.Sˇabanovic´,W.-L.Chang,S.Nagata,J.Piatt,C.Bennett,
toevolve,ensuringadeepunderstandingofuserenjoymentnot
and D. Hakken, “Steps Toward Participatory Design of Social Robots:
only elevates the quality of these interactions but also paves Mutual Learning with Older Adults with Depression,” in Proceedings
the way for more empathetic and meaningful connections. of the 2017 ACM/IEEE International Conference on Human-Robot
Interaction. ViennaAustria:ACM,Mar.2017,pp.244–253.
[6] S. Sˇabanovic´, “Robots in society, society in robots,” International
JournalofSocialRobotics,vol.2,no.4,pp.439–450,2010.
VII. CONCLUSION [7] C.Breazeal,C.Kidd,A.Thomaz,G.Hoffman,andM.Berlin,“Effects
of nonverbal communication on efficiency and robustness in human-
robotteamwork,”in2005IEEE/RSJInternationalConferenceonIntel-
Our research contributed a novel scale for measuring user
ligentRobotsandSystems,2005,pp.708–713.
enjoyment in conversations with a robot from an external [8] L.D.Riek,“Wizardofozstudiesinhri:asystematicreviewandnew
perspective. The scale was developed through rigorous dis- reportingguidelines,”J.Hum.-RobotInteract.,vol.1,p.119–136,2012.
[9] Y. K. Lee, Y. Jung, G. Kang, and S. Hahn, “Developing social robots
cussions of three annotators with complementary and relevant
withempatheticnon-verbalcuesusinglargelanguagemodels,”in2023
backgrounds to user enjoyment and the application domain. 32nd IEEE International Conference on Robot & Human Interactive
Olderadults’interactionswithacompanionrobotwereusedas Communication(RO-MAN),2023.
thebasisfordevelopingthescale,whichwasevaluatedon174 [10] N.Cherakara,F.Varghese,S.Shabana,N.Nelson,A.Karukayil,R.Ku-
lothungan, M. Farhan, B. Nesset, M. Moujahid, T. Dinkar, V. Rieser,
minutesofinteractionsof25participants.Inter-raterreliability
andO.Lemon,“Furchat:Anembodiedconversationalagentusingllms,
analysis showed the importance of using multiple annotators, combiningopenandclosed-domaindialoguewithfacialexpressions,”in
Proceedings of the 24th Annual Meeting of the Special Interest Group
with moderate to good alignment, where the disagreements
onDiscourseandDialogue(SigDIAL),2023.
arose from the complexity and the subjectivity of user en-
[11] W. Khoo, L.-J. Hsu, K. J. Amon, P. V. Chakilam, W.-C. Chen,
joyment, where a user can show various signs of enjoyment Z. Kaufman, A. Lungu, H. Sato, E. Seliger, M. Swaminathan, K. M.
and dis-enjoyment within a single conversation exchange. Tsui, D. J. Crandall, and S. Sabanovic´, “Spill the tea: When robot
conversationagentssupportwell-beingforolderadults,”inCompanion
The overall user enjoyment rated per interaction correlated
of the 2023 ACM/IEEE International Conference on Human-Robot
significantly with users’ perceived level of strangeness of the Interaction. NewYork,NY,USA:ACM,2023,pp.178–182.
conversation,whichsignifiesthatthe(dis)comfortexperienced [12] B. Irfan, S.-M. Kuoppama¨ki, and G. Skantze, “Between reality and
delusion: Challenges of applying large language models to compan-
in the interaction was correctly identified by the annotators,
ion robots for open-domain dialogues with older adults,” Autonomous
and shows the importance of including dis-enjoyment levels Robots,2023,preprintathttps://doi.org/10.21203/rs.3.rs-2884789/v1.
in the scale. These findings validate our user enjoyment scale [13] C. Oertel, G. Castellano, M. Chetouani, J. Nasir, M. Obaid,
C.Pelachaud,andC.Peters,“Engagementinhuman-agentinteraction:
and emphasize the critical role of methodological rigor in
Anoverview,”FrontiersinRoboticsandAI,vol.7,p.92,2020.
assessing subjective experiences within conversational robot [14] B. Irfan, J. Kennedy, S. Lemaignan, F. Papadopoulos, E. Senft, and
interactions.Ourstudyemphasizesthevalueofusingmultiple T. Belpaeme, “Social psychology and human-robot interaction: An
uneasymarriage,”inCompanionofthe2018ACM/IEEEInternational
annotators and proposes potential scale refinements to further
ConferenceonHuman-RobotInteraction. ACM,2018,pp.13–20.
enhanceconsistencyinquantifyingthenuancedconceptofen-
[15] F.Lingenfelser,J.Wagner,E.Andre´,G.McKeown,andW.Curran,“An
joymentacrossapplicationdomains.Thedevelopedscaleaims eventdrivenfusionapproachforenjoymentrecognitioninreal-time,”in
to provide a tool for measuring user enjoyment from an ex- Proceedingsofthe22ndACMinternationalconferenceonMultimedia,
2014,pp.377–386.
ternal perspective to supplement self-reported user enjoyment
[16] S. J. Reimnitz and A. J. Rauer, “Mutual enjoyment in older couples’
responses in HRI research, with future potential application conversations and its links to marital satisfaction,” Personal Relation-
for autonomous detection of user enjoyment in real-time in ships,vol.29,no.2,pp.332–349,2022.
[17] M. Csikszentmihalyi, R. Larson et al., Flow and the foundations of
robots and agents for adapting conversations contingently to
positivepsychology. Springer,2014,vol.10.
provide enjoyable and long-lasting interactions. [18] C.Price,ThePowerofFun:HowtoFeelAliveAgain. RandomHouse
PublishingGroup,2021.
[19] E. D. Mekler, J. A. Bopp, A. N. Tuch, and K. Opwis, “A systematic
reviewofquantitativestudiesontheenjoymentofdigitalentertainment
ACKNOWLEDGMENTS games,”inProceedingsoftheSIGCHIconferenceonhumanfactorsin
computingsystems,2014,pp.927–936.
WewouldliketothankAidaHosseiniformanuallycorrect- [20] J. A. Russell, “A circumplex model of affect.” Journal of personality
andsocialpsychology,vol.39,no.6,p.1161,1980.
ingtranscriptsofrobotinteractions,andthestudyparticipants
[21] P. Ekman, “An argument for basic emotions,” Cognition & emotion,
for their time and efforts. vol.6,no.3-4,pp.169–200,1992.IEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 15
[22] J. Endicott, J. Nee, W. Harrison, and R. Blumenthal, “Quality of life failures,”inProceedingsofthe2021InternationalConferenceonMul-
enjoymentandsatisfactionquestionnaire:anewmeasure.”Psychophar- timodalInteraction,ser.ICMI’21. NewYork,NY,USA:Association
macologybulletin,vol.29,no.2,pp.321–326,1993. forComputingMachinery,2021,p.112–120.
[23] D. Kendzierski and K. J. DeCarlo, “Physical activity enjoyment scale: [46] C. M. Carpinella, A. B. Wyman, M. A. Perez, and S. J. Stroessner,
Twovalidationstudies.”Journalofsport&exercisepsychology,vol.13, “Theroboticsocialattributesscale(rosas):Developmentandvalidation,”
no.1,1991. in 2017 12th ACM/IEEE International Conference on Human-Robot
[24] M.KonoandK.Araake,“Isitfun?:Understandingenjoymentinnon- Interaction(HRI),2017,pp.254–262.
gamehciresearch,”arXivpreprintarXiv:2209.02308,2022. [47] T. Iio, Y. Yoshikawa, M. Chiba, T. Asami, Y. Isoda, and H. Ishiguro,
[25] V.Venkatesh,J.Y.Thong,andX.Xu,“Consumeracceptanceanduse “Twin-robotdialoguesystemwithrobustnessagainstspeechrecognition
of information technology: extending the unified theory of acceptance failureinhuman-robotdialoguewithelderlypeople,”AppliedSciences,
anduseoftechnology,”MISquarterly,pp.157–178,2012. vol.10,no.4,2020.
[26] S.Nishimura,T.Nakamura,W.Sato,M.Kanbara,Y.Fujimoto,H.Kato, [48] C.O’ConnorandH.Joffe,“Intercoderreliabilityinqualitativeresearch:
and N. Hagita, “Vocal synchrony of robots boosts positive affective Debates and practical guidelines,” International Journal of Qualitative
empathy,”AppliedSciences,vol.11,no.6,p.2502,Mar2021. Methods,vol.19,p.1609406919899220,2020.
[27] M.D.Cooney,T.Kanda,A.Alissandrakis,andH.Ishiguro,“Interaction [49] E. Lagerstedt and S. Thill, “Multiple roles of multimodality among
designforanenjoyableplayinteractionwithasmallhumanoidrobot,” interacting agents,” ACM Transactions on Human-Robot Interaction,
in201111thIEEE-RASInternationalConferenceonHumanoidRobots. vol.12,no.2,pp.1–13,2023.
IEEE,2011,pp.112–119. [50] K.Haring,C.Mougenot,F.Ono,andK.Watanabe,“Culturaldifferences
[28] K.M.Lee,Y.Jung,J.Kim,andS.R.Kim,“Arephysicallyembodied in perception and attitude towards robots,” International Journal of
social agents better than disembodied social agents?: The effects of AffectiveEngineering,vol.13,pp.149–157,102014.
physical embodiment, tactile interaction, and people’s loneliness in [51] T.K.KooandM.Y.Li,“Aguidelineofselectingandreportingintraclass
human–robot interaction,” International Journal of Human-Computer correlationcoefficientsforreliabilityresearch,”JChiroprMed,vol.15,
Studies,vol.64,no.10,pp.962–973,2006. no.2,pp.155–163,Jun2016.
[29] F. D. Davis, “Perceived usefulness, perceived ease of use, and user [52] S.M.Anzalone,S.Boucenna,S.Ivaldi,andM.Chetouani,“Evaluating
acceptance of information technology,” MIS Quarterly, vol. 13, no. 3, the engagement with social robots,” International Journal of Social
pp.319–340,1989. Robotics,vol.7,no.4,pp.465–478,2015.
[53] M. F. Jung, “Affective grounding in human-robot interaction,” in 2017
[30] M. M. Van Pinxteren, R. W. Wetzels, J. Ru¨ger, M. Pluymaekers,
12thACM/IEEEInternationalConferenceonHuman-RobotInteraction
and M. Wetzels, “Trust in humanoid robots: implications for services
marketing,”JournalofServicesMarketing,vol.33,no.4,2019.
(HRI),2017,pp.263–273.
[54] G. Skantze, “Turn-taking in conversational systems and human-robot
[31] M.Heerink,B.Kro¨se,V.Evers,andB.Wielinga,“Assessingacceptance
interaction: A review,” Computer Speech & Language, vol. 67, p.
ofassistivesocialagenttechnologybyolderadults:thealmeremodel,”
101178,2021.
InternationalJournalofSocialRobotics,vol.2,pp.361–375,2010.
[55] R. Stock-Homburg, “Survey of emotions in human–robot interactions:
[32] J.PiasekandK.Wieczorowska-Tobis,“Acceptanceandlong-termuseof
Perspectives from robotic psychology on 20 years of research,” Inter-
asocialrobotbyelderlyusersinadomesticenvironment,”in201811th
nationalJournalofSocialRobotics,vol.14,no.2,pp.389–411,2022.
InternationalConferenceonHumanSystemInteraction(HSI),2018,pp.
[56] L.Mondada,“Challengesofmultimodality:Languageandthebodyin
478–482.
socialinteraction,”JournalofSociolinguistics,vol.20,no.3,pp.336–
[33] L. K. Fryer and D. L. Dinsmore, “The promise and pitfalls of self-
366,2016.
report:Development,researchdesignandanalysisissues,andmultiple
[57] R. Clift, Conversation Analysis, ser. Cambridge Textbooks in Linguis-
methods,”FrontlineLearningResearch,vol.8,no.3,p.1–9,Mar.2020.
tics. CambridgeUniversityPress,2016.
[34] J. Ginzburg, E. Breitholtz, R. Cooper, J. Hough, and Y. Tian, “Under-
[58] R. Shalihah, M. Rusijono, and A. Mariono, “The role of multimodal
standinglaughter,”in20thAmsterdamColloquium,2015.
communicationinlanguagelearning:Makingmeaninginconventional
[35] M.Haakana,“Laughterandsmiling:Notesonco-occurrences,”Journal
learning spaces,” in Proceedings of the International Conference on
ofPragmatics,vol.42,no.6,pp.1499–1512,2010.
Language Phenomena in Multimodal Communication (KLUA 2018).
[36] S. Lee and J. Choi, “Enhancing user experience with conversational
AtlantisPress,2018/07,pp.230–233.
agent for movie recommendation: Effects of self-disclosure and reci- [59] M. Rasenberg, W. Pouw, A. O¨zyu¨rek, and M. Dingemanse, “The
procity,” International Journal of Human-Computer Studies, vol. 103,
multimodal nature of communicative efficiency in social interaction,”
pp.95–105,2017.
ScientificReports,vol.12,no.1,p.19111,2022.
[37] B.LeeandM.Y.Yi,“Understandingtheempatheticreactivityofcon- [60] A.Mehrabian,BasicDimensionsforaGeneralPsychologicalTheory:
versationalagents:Measuredevelopmentandvalidation,”International
ImplicationsforPersonality,Social,Environmental,andDevelopmental
JournalofHuman–ComputerInteraction,vol.0,no.0,pp.1–19,2023.
Studies. Cambridge:Oelgeschlager,Gunn&Hain,1980.
[38] M. Walker, D. Litman, C. Kamm, and A. Abella, “Evaluating spoken [61] A. Ortony, G. L. Clore, and A. Collins, The Cognitive Structure of
dialogueagentswithparadise:Twocasestudies,”ComputerSpeech& Emotions. CambridgeUniversityPress,1988.
Language,vol.12,no.4,pp.317–347,1998. [62] R.S.Barbour,“Checklistsforimprovingrigourinqualitativeresearch:
[39] A. Schmitt and S. Ultes, “Interaction quality: Assessing the quality of acaseofthetailwaggingthedog?”BMJ,vol.322,no.7294,pp.1115–
ongoingspokendialoginteractionbyexperts—andhowitrelatestouser 1117,2001.
satisfaction,”SpeechCommunication,vol.74,pp.12–36,2015. [63] S.Andrist,X.Z.Tan,M.Gleicher,andB.Mutlu,“Conversationalgaze
[40] R.Higashinaka,Y.Minami,K.Dohsaka,andT.Meguro,“Issuesinpre- aversionforhumanlikerobots,”inProceedingsofthe2014ACM/IEEE
dictingusersatisfactiontransitionsindialogues:Individualdifferences, InternationalConferenceonHuman-RobotInteraction. NewYork,NY,
evaluation criteria, and prediction models,” in International Workshop USA:AssociationforComputingMachinery,2014,p.25–32.
onSpokenDialogueSystemsTechnology,2010. [64] H.NobleandJ.Smith,“Issuesofvalidityandreliabilityinqualitative
[41] W.Wei,S.Li,S.Okada,andK.Komatani,“Multimodalusersatisfaction research,”Evidence-basednursing,vol.18,no.2,pp.34–35,2015.
recognition for non-task oriented dialogue systems,” in Proceedings of [65] S.Jamieson,“Likertscales:howto(ab)usethem,”MedicalEducation,
the2021InternationalConferenceonMultimodalInteraction,ser.ICMI vol.38,no.12,pp.1217–1218,2004.
’21. NewYork,NY,USA:ACM,2021,p.586–594. [66] Jost, Ce´line and Le Pe´ve´dic, Brigitte and Belpaeme, Tony and Bethel,
[42] L. Campbell, R. A. Martin, and J. R. Ward, “An observational study CindyandChrysostomou,DimitriosandCrook,NigelandGrandgeorge,
of humor use while resolving conflict in dating couples,” Personal MarineandMirnig,Nicole, Ed.,Human-robotinteraction:evaluation
Relationships,vol.15,no.1,pp.41–55,2008. methodsandtheirstandardization. Springer,2020,vol.12.
[43] K. Jokinen and G. Wilcock, “Chapter 1 - multimodal open-domain [67] M. Reimann, J. van de Graaf, N. van Gulik, S. van de Sanden,
conversationswithroboticplatforms,”inMultimodalBehaviorAnalysis T.Verhagen,andK.Hindriks,“Socialrobotsinthewildandthenovelty
intheWild,ser.ComputerVisionandPatternRecognition,X.Alameda- effect,” in Social Robotics, A. A. Ali, J.-J. Cabibihan, N. Meskin,
Pineda,E.Ricci,andN.Sebe,Eds. AcademicPress,2019,pp.9–26. S. Rossi, W. Jiang, H. He, and S. S. Ge, Eds. Singapore: Springer
[44] S. Honig and T. Oron-Gilad, “Understanding and resolving failures in NatureSingapore,2024,pp.38–48.
human-robot interaction: Literature review and model development,” [68] G.A.VanKleef,A.Cheshin,A.H.Fischer,andI.K.Schneider,“The
FrontiersinPsychology,vol.9,2018. socialnatureofemotions,”Frontiersinpsychology,vol.7,p.896,2016.
[45] D.Kontogiorgos,M.Tran,J.Gustafson,andM.Soleymani,“Asystem-
atic cross-corpus analysis of human reactions to robot conversationalIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 16
VIII. BIOGRAPHYSECTION SannaKuoppama¨kiisanAssistantProfessoratthe
Department of Biomedical Engineering and Health
Systems,KTHRoyalInstituteofTechnology,Swe-
den. Her research explores the use and design of
interactivetechnologiesforhealthyageingfromthe
age and life course perspective, with application
areas in social robotics, welfare technology and
mobile health. She received her PhD in sociology
BaharIrfanisaPostdoctoralResearcherandDigi-
from the University of Jyva¨skyla¨, Finland. She is
talFuturesfellowatKTHRoyalInstituteofTechnol-
a member of the Socio-Gerontechnology network.
ogy,Sweden.Herresearchfocusesoncreatingper-
Contactheratsannaku@kth.se
sonalrobotsthatcontinuallylearnandadapttoassist
in daily life. Her research interests include social
robots,lifelonglearning,andlargelanguagemodels.
Prior to joining KTH, she held research positions
at Evinoks and Disney Research. She received her
PhDinroboticsfromtheUniversityofPlymouthand
SoftBankRoboticsEuropeasaMarieSkłodowska- Gabriel Skantze is a Professor in Speech Tech-
Curie Actions fellow. Contact her at birfan@kth.se nologyandCommunicationatKTHRoyalInstitute
Website:https://baharirfan.com of Technology, Sweden. He is leading several re-
searchprojectsrelatedtoconversationalsystemsand
human-robotinteraction,investigatingandmodeling
phenomena such as turn-taking, visual grounding,
andmultimodalfeedbackindialogue.Heisalsoco-
founder and chief scientist at Furhat Robotics and
President Emeritus of SIGDIAL, the ACL Special
InterestGrouponDiscourseandDialogue.Contact
himatskantze@kth.se
Jura Miniota is a PhD candidate at KTH Royal
Institute of Technology in Stockholm, Sweden. In
her research, she is focusing on the development
ofsociallyembodiedandsociallyintelligentrobots
that are enjoyable to interact with. Her research
interests include multimodal artificial intelligence,
Andre´ Pereira is a Human-Robot Interaction re-
socialrobotics,anddesigningenjoyableinteractions.
searcheratKTHRoyalInstituteofTechnology,Swe-
Miniota received her M.Sc. in Interactive Media
den.HereceivedhisPhDinComputerEngineering
TechnologyfromKTH.Contactheratjura@kth.se.
from the Technical University of Lisbon, Portugal.
Website:https://www.juraminiota.com
After his PhD and a postdoctoral position at Yale
University,heworkedinindustryatDisneyResearch
andFurhatRobotics.AtKTH,hecontinuestodesign
and develop embodied socially intelligent agents
foruseinentertainment,education,andhealthcare.
Contacthimatatap@kth.se
Sofia Thunberg is a PhD candidate at the De-
partment of Computer and Information Science,
Linko¨pingUniversity,Sweden.Herresearchfocuses
on companion robots for older adults with cog-
nitive impairments and for children with autism
spectrum disorders. Her research is mainly con-
ducted in the field, taking on a holistic approach
to how people interact with robots in the real
world. Thunberg received her M.Sc. in Cognitive
Science from Linko¨ping University. Contact her at
sofia.thunberg@liu.se.
ErikLagerstedtjustdefendedhisPhDthesisatthe
School of Informatics, University of Sko¨vde, Swe-
den.Hisresearchinterestsincludetheevaluationand
application of social robots, the theoretical founda-
tionsofinteractionwithtechnology,andthesocietal
impactofdecisionsinthedesignanddeploymentof
specifictechnologies.LagerstedtreceivedhisPhDin
InformaticsfromtheUniversityofSko¨vde,Sweden.
Contacthimaterik.lagerstedt@his.se.IEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 17
APPENDIX reverse-coded).
Fig. 8 shows the HRI CUES rating distributions of the
annotators per participant, which shows a similar trend of
normal distribution.
Fig.9showstheHRICUESratingsoftheannotatorswithin
each conversation turn per participant.
Fig. 10 shows the overall interaction ratings of the anno-
tators per participant in comparison to the self-reported user
perceptions for user satisfaction, fun, interestingness of the
conversation,andstrangenessoftalkingtotherobot(whichis
P1 P2 P3 P4 P5
20
15
10
5
0
P6 P7 P8 P9 P10
20
15
10
5
0
P11 P12 P13 P14 P15
20
15
10
5
0
P16 P17 P18 P19 P20
20
15
10
5
0
P21 P22 P23 P24 P25
20
15
10
5
0
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Rating
Annotator A1 A2 A3
Fig.8. Annotators’ratingdistributionsperparticipant.
P1 P2 P3 P4 P5
5
4
3
2
1
P6 P7 P8 P9 P10
5
4
3
2
1
P11 P12 P13 P14 P15
5
4
3
2
1
P16 P17 P18 P19 P20
5
4
3
2
1
P21 P22 P23 P24 P25
5
4
3
2
1
1 5 10 15 20 25 1 5 10 15 20 25 1 5 10 15 20 25 1 5 10 15 20 25 1 5 10 15 20 25
Turn
Annotator A1 A2 A3
Fig.9. Annotators’userenjoymentratingsperturnforeachparticipant’sinteraction.
ytitnauQ
tnemyojnE
resUIEEETRANSACTIONSONAFFECTIVECOMPUTING,VOL.,NO., 18
P1 P2 P3 P4 P5
5
4
3
2
1
P6 P7 P8 P9 P10
5
4
3
2
1
P11 P12 P13 P14 P15
5
4
3
2
1
P16 P17 P18 P19 P20
5
4
3
2
1
P21 P22 P23 P24 P25
5
4
3
2
1
A1 A2 A3 Avg Sat Fun Int Str A1 A2 A3 Avg Sat Fun Int Str A1 A2 A3 Avg Sat Fun Int Str A1 A2 A3 Avg Sat Fun Int Str A1 A2 A3 Avg Sat Fun Int Str
Annotator A1 A2 A3
User Perceptions Average Satisfaction Fun Interesting Strange (Reversed)
Fig.10. Annotators’ratingsoftheoveralluserenjoymentperinteraction,andusers’perceptionsfromthequestionnaire.
gnitaR