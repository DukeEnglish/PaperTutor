PublishedasaconferencepaperatICLR2024
PLAN-SEQ-LEARN: LANGUAGE MODEL GUIDED RL
FOR SOLVING LONG HORIZON ROBOTICS TASKS
MurtazaDalal1,TarunChiruvolu1,DevendraSinghChaplot2,RuslanSalakhutdinov1
1CarnegieMellonUniversity,2MistralAI
ABSTRACT
LargeLanguageModels(LLMs)havebeenshowntobecapableofperforming
high-levelplanningforlong-horizonroboticstasks,yetexistingmethodsrequire
accesstoapre-definedskilllibrary(e.g. picking,placing,pulling,pushing,navi-
gating). However,LLMplanningdoesnotaddresshowtodesignorlearnthose
behaviors,whichremainschallengingparticularlyinlong-horizonsettings. Fur-
thermore, for many tasks of interest, the robot needs to be able to adjust its
behaviorinafine-grainedmanner,requiringtheagenttobecapableofmodifying
low-levelcontrolactions. Canweinsteadusetheinternet-scaleknowledgefrom
LLMs for high-level policies, guiding reinforcement learning (RL) policies to
efficientlysolveroboticcontroltasksonlinewithoutrequiringapre-determined
set of skills? In this paper, we propose Plan-Seq-Learn (PSL): a modular ap-
proach that uses motion planning to bridge the gap between abstract language
andlearnedlow-levelcontrolforsolvinglong-horizonroboticstasksfromscratch.
WedemonstratethatPSLachievesstate-of-the-artresultsonover25challenging
roboticstaskswithupto10stages. PSLsolveslong-horizontasksfromrawvisual
input spanning four benchmarks at success rates of over 85%, out-performing
language-based,classical,andend-to-endapproaches. Videoresultsandcodeat
https://mihdalal.github.io/planseqlearn/
1 INTRODUCTION
In recent years, the field of robot learning has witnessed a significant transformation with the
emergenceofLargeLanguageModels(LLMs)asamechanismforinjectinginternet-scaleknowledge
intorobotics. OneparadigmthathasbeenparticularlyeffectiveisLLMplanningoverapredefinedset
ofskills(Ahnetal.,2022;Singhetal.,2023;Huangetal.,2022b;Wuetal.,2023),producingstrong
resultsacrossawiderangeofroboticstasks. Theseworksassumetheavailabilityofapre-defined
skilllibrarythatabstractsawaytheroboticcontrolproblem. Theyinsteadfocusondesigningmethods
toselecttherightsequenceskillstosolveagiventask. However,forroboticstasksinvolvingcontact-
richroboticmanipulation(Fig.1),suchskillsareoftennotavailable,requiresignificantengineering
efforttodesignortraina-priorioraresimplynotexpressiveenoughtoaddressthetask. Howcan
wemovebeyondpre-builtskilllibrariesandenabletheapplicationoflanguagemodelstogeneral
purposeroboticstaskswithasfewassumptionsaspossible? Roboticsystemsneedtobecapableof
onlineimprovementoverlow-levelcontrolpolicieswhilebeingabletoplanoverlonghorizons.
End-to-endreinforcementlearning(RL)isoneparadigmthatcanproducecomplexlow-levelcontrol
strategiesonrobotswithminimalassumptions(Akkayaetal.,2019;Herzog*etal.,2023;Handa
et al., 2022; Kalashnikov et al., 2018; 2021; Chen et al., 2022; Agarwal et al., 2023). Unlike
hierarchicalapproacheswhichimposeaspecificstructureontheagentwhichmaynotbeapplicable
toalltasks,end-to-endlearningmethodscan,inprinciple,learnabetterrepresentationdirectlyfrom
data. However,RLmethodsaretraditionallylimitedtotheshorthorizonregimeduetothesignificant
challengeofexplorationinRL,especiallyinhigh-dimensionalcontinuousactionspacescharacteristic
ofroboticstasks. RLmethodsstrugglewithlonger-horizontasksinwhichhigh-levelreasoningand
low-levelcontrolmustbelearnedsimultaneously;effectivelydecomposingtasksintosub-sequences
andaccuratelyachievingthemischallengingingeneral(Suttonetal.,1999;Parr&Russell,1997).
OurkeyinsightisthatLLMsandRLhavecomplementarystrengthsandweaknesses.Priorwork(Ahn
etal.,2022;Huangetal.,2022a;Wuetal.,2023;Singhetal.,2023;Songetal.,2023)hasshownthat
whenappropriatelyprompted,languagemodelsarecapableofleveraginginternetscaleknowledgeto
breakdownlong-horizontasksintoachievablesub-goals,butlackamechanismtoproducelow-level
robotcontrolstrategiesWangetal.(2023),whileRLcandiscovercomplexcontrolbehaviorson
1
4202
yaM
2
]GL.sc[
1v43510.5042:viXraPublishedasaconferencepaperatICLR2024
Figure1: Longhorizontaskvisualization.WevisualizePSLsolvingtheNutAssemblytask,inwhichthegoal
istoputbothnutsontheirrespectivepegs.Afterpredictingthehigh-levelplanusinganLLM,PSLcomputesa
targetrobotpose,achievesitusingmotionplanningandthenlearnsinteractionviaRL(thirdrow).
robotsbutstrugglestosimultaneouslyperformlong-termreasoning(Nachumetal.,2018). However,
directlycombiningthetwoparadigms,forexample,viatrainingalanguageconditionedpolicyto
solveanewtask,doesnotaddresstheexplorationproblem. TheRLagentmustnowsimultaneously
learnlanguagesemanticsandlow-levelcontrol. Ideally,theRLagentshouldbeabletofollowthe
guidanceoftheLLM,enablingittolearntoefficientlysolveeachpredictedsub-taskonline. Howcan
weconnecttheabstractlanguagespaceofanLLMwiththelow-levelcontrolspaceoftheRLagentin
ordertoaddressthelong-horizonrobotcontrolproblem? Inthiswork,weproposealearningmethod
tosolvelong-horizonroboticstasksbytrackinglanguagemodelplansusingmotionplanningand
learnedlow-levelcontrol. Ourapproach,calledPlan-Seq-Learn(PSL),isamodularframeworkin
whichahigh-levellanguageplangivenbyanLLM(Plan)isinterpretedandexecutedusingmotion
planning(Seq),enablingtheRLpolicy(Learn)torapidlylearnshort-horizoncontrolstrategiesto
solve the overall task. This decomposition enables us to effectively leverage the complementary
strengthsofeachmodule: languagemodelsforabstractplanning,vision-basedmotionplanningfor
taskplantrackingaswellasachievingrobotstatesandRLpoliciesforlearninglow-levelcontrol.
Furthermore, we improve learning speed and training stability by sharing the learned RL policy
acrossallstagesofthetask,usinglocalobservationsforefficientgeneralization,andintroducing
asimple, yetscalablecurriculumlearningstrategyfortrackingthelanguagemodelplan. Toour
knowledge,oursisthefirstworkenablinglanguageguidedRLagentstoefficientlylearnlow-level
controlstrategiesforlong-horizonroboticstasks.
Our contributions are: 1) A novel method for long-horizon robot learning that tightly integrates
large language models for high-level planning, motion planning for skill sequencing and RL for
learning low-level robot control strategies; 2) Strategies for efficient policy learning from high-
levelplans,whichincludepolicyobservationspacedesignforlocality,sharedpolicynetworkand
rewardfunctionstructures,andcurriculaforstage-wisepolicytraining;3)Anextensiveexperimental
evaluation demonstrating that PSL can solve over 25 long-horizon robotics tasks with up to 10
stages,outperformingSOTAbaselinesacrossfourbenchmarksuitesatsuccessratesofover85%
purelyfromvisualinput. PSLproducesagentsthatsolvechallenginglong-horizontaskssuchas
NutAssemblyat96%successrate.
2 RELATED WORK
ClassicalApproachestoLongHorizonRobotics:Historically,roboticstaskshavebeenapproached
viatheSense-Plan-Act(SPA)pipeline(Paul,1981;Whitney,1972;Vukobratovic´&Potkonjak,1982;
Kappleretal.,2018;Murphy,2019),whichrequirescomprehensiveunderstandingoftheenvironment
(sense),amodeloftheworld(plan),andalow-levelcontroller(act). Traditionalapproachesrange
frommanipulationplanning(Lozano-Perezetal.,1984;Tayloretal.,1987),graspanalysisMiller
&Allen(2004),andTaskandMotionPlanning(TAMP)Garrettetal.(2021),tomodernvariants
incorporatinglearnedvision(Mahleretal.,2016;Mousavianetal.,2019;Sundermeyeretal.,2021).
Planningalgorithmsenablelonghorizondecisionmakingovercomplexandhigh-dimensionalaction
spaces.However,theseapproachescanstrugglewithcontact-richinteractions(Mason,2001;Whitney,
2004),experiencecascadingerrorsduetoimperfectstateestimation(Kaelbling&Lozano-Pe´rez,
2PublishedasaconferencepaperatICLR2024
Figure2: Methodoverview. PSLdecomposestasksintoalistofregionsandstageterminationconditions
usinganLLM(top),sequencestheplanusingmotionplanning(left)andlearnscontrolpoliciesusingRL(right).
2013),andrequiresignificantmanualengineeringandsystemsefforttosetup(Garrettetal.,2020b).
Ourmethodleverageslearningateachcomponentofthepipelinetosidesteptheseissues: ithandles
contact-richinteractionsusingRL,avoidscascadingfailuresbylearningonline,andsidestepsmanual
engineeringeffortbyleveragingpre-trainedmodelsforvisionandlanguage.
Planning and Reinforcement Learning: Recent work has explored the integration of motion
planning and RL to combine the advantages of both paradigms (Lee et al., 2020; Yamada et al.,
2021;Cheng&Xu,2022;Xiaetal.,2020;James&Davison,2022;Jamesetal.,2022;Liuetal.,
2022). GUAPOLeeetal.(2020)issimilartotheSeq-Learncomponentsofourmethod,yettheir
systemconsidersthesingle-stageregimeandisfocusedonkeepingtheRLagentinareasoflowpose-
estimatoruncertainty. Ourmethodinsteadconsiderslong-horizontasksbyencouragingtheRLagent
tofollowahigh-levelplangivenbyanLLMusingvision-basedmotionplanning. MoPA-RLYamada
etal.(2021)alsobearsresemblancetoourmethod,yetitoptstolearnwhentousethemotionplanner
viaRL,requiringtheRLagenttodiscovertherightdecompositionofplannervs. controlactionson
itsown. Furthermore,roll-outsoftrajectoriesusingMoPAcanresultintheRLagentchoosingto
motionplanmultipletimesinsequence,whichisinefficient-onemotionplanneractionissufficient
toreachanypositioninspace. Inourmethod,weinsteadexplicitlydecomposetasksintosequences
ofcontact-freereaching(motionplanner)andcontact-richenvironmentinteraction(RL).
LanguageModelsforRLandRoboticsLLMshavebeenappliedtoRLandroboticsinawide
varietyofways,fromplanningAhnetal.(2022);Singhetal.(2023);Huangetal.(2022a;b);Wuetal.
(2023);Liuetal.(2023a);Ranaetal.(2023);Linetal.(2023),rewarddefinitionKwonetal.(2023);
Yuetal.(2023),generatingquadrupedalcontact-pointsTangetal.(2023),producingtasksforpolicy
learningDuetal.(2023);Colasetal.(2020)andcontrollingsimulation-basedtrajectorygenerators
toproducediversetasksHaetal.(2023). Ourworkinsteadfocusesontheonlinelearningsettingand
aimstoleveragelanguagemodeldrivenplanningtoguideRLagentstosolvenewroboticstasksina
sampleefficientmanner. BOSSZhangetal.(2023)isclosesttoouroverallmethod;thisconcurrent
workalsoleveragesLLMguidancetolearnnewskillsviaRL.Crucially,theirmethoddependsonthe
existenceofaskilllibraryandlearnsskillsthatarecombinationofhigh-levelactions. Ourmethod
insteadefficientlylearnslow-levelrobotcontrolskillswithoutdependingonapre-definedskilllibrary,
bytakingadvantageofmotionplanningtotrackanLLMplan.
3 PLAN-SEQ-LEARN
Inthissection,wedescribeourmethodforsolvinglong-horizonroboticstasks,PSL,outlinedinFig.2.
Givenatextdescriptionofthetask,ourmethodbreaksupthetaskintomeaningfulsub-sequences
(Plan),usesvisionandmotionplanningtotranslatesub-sequencesintoinitializationregions(Seq)
fromwhichwecanefficientlytrainlocalcontrolpoliciesusingRL(Learn).
3PublishedasaconferencepaperatICLR2024
3.1 PROBLEMSETUP
We consider Partially Observed Markov Decision Processes (POMDP) of the form
(S,A,T,R,p ,O,p ,γ). S isthesetofenvironmentstates,Aisthesetofactions,T(s′ |s,a)is
0 O
thetransitionprobabilitydistribution,R(s,a,s′)istherewardfunction,p isthedistributionoverthe
0
initialstates ∼p ,Oisthesetofobservations,p isthedistributionoverobservationsconditioned
0 0 O
onthestateO ∼p (O|s)andγ isthediscountfactor. Inourcase,theobservationspaceisthesetof
O
allRGB-D(RGBanddepth)images. Therewardfunctionisdefinedbytheenvironment. Theagent’s
goalistomaximizetheexpectedsumofrewardsoverthetrajectory,E[(cid:80) γtR(s ,a ,s )]. Inour
t t t t+1
work,weconsiderPOMDPsthatdescribeanembodiedrobotagentinteractingwithascene. We
assumethatatextdescriptionofthetask,g ,isprovidedtotheagentinnaturallanguage.
l
3.2 OVERVIEW
Tosolvelong-horizonroboticstasks,weneed
Algorithm1Plan-Seq-LearnOverview
amodulecapableofbridgingthegapbetween
zero-shotlanguagemodelplanningandlearned Require: LLM,PoseEstimatorP,taskdescription
low-levelcontrol.Observethatmanytasksofin- g l,MotionPlannerMP,low-levelhorizonH l
terestcanbedecomposedintoalternatingphases PlanningModule
ofcontact-freemotionandcontact-richinterac- High-levelplanP ←Prompt(LLM,g l)
tion. One first approaches a target region and forp∈P do
thenperformsinteractionbehavior,priortomov- SequencingModule
ing to the next sub-task. Contact-free motion targetregion(t),terminationcondition←p
generationisexactlythemotionplanningprob- Computeposeq =P(Oglobal,t)
target t
lem. For estimating the position of the target AchieveposeMP(q ,Oglobal)
target t
region,wenotethatstate-of-the-artvisionmod- LearningModule
elsarecapableofaccuratelanguage-conditioned fori=1,...,H do
l
stateestimation(Kirillovetal.,2023;Zhouetal., Getactiona ∼π (Olocal)
t θ t
2022; Liu et al., 2023b; Bahl et al., 2023; Ye GetnextstateOlocal ∼p(|s ,a ).
t+1 t t
etal.,2023;Labbe´ etal.,2022). Asaresult,we Store(Olocal,a ,Olocal,r)intoR
proposeaSequencingModulewhichusesoff- t t t+1
updateπ usingRL
θ
the-shelfvisionmodelstoestimatetargetrobot iff (Oglobal)then
stage
statesfromthelanguageplanandthenachieves
break
thesestatesusingamotionplanner. Fromsuch
endif
states,wetraininteractionpoliciesthatoptimize
endfor
thetaskrewardusingRL.SeeAlg.1andFig.2
endfor
foranoverviewofourmethod.
3.3 PLANNINGMODULE: ZERO-SHOTHIGH-LEVELPLANNING
Long-horizontaskscanbebrokenintoaseriesofstagestoexecute. Ratherthandiscoveringthese
stagesusinginteractionorusingataskplannerFikes&Nilsson(1971)thatmayrequireprivileged
informationabouttheenvironment,weuselanguagemodelstoproducenaturallanguageplanszero
shotwithoutaccesstotheenvironment. Specifically, givenataskdescriptiong byahuman, we
l
promptanLLMtoproduceaplan. Designingtheplangranularityandscopearecrucial;weneed
plansthatcanbeinterpretedbytheSequencingModule,avision-basedsystemthatproducesand
achievesrobotposesusingmotionplanning. Asaresult,theLLMpredictsatargetregion(anatural
languagelabelofanobject/receptacleinthescene,e.g. “silverpeg”)whichcanbetranslatedintoa
targetposetoachieveatthebeginningofeachstageoftheplan.
WhentheRLpolicyisexecutingastepoftheplan,weproposetoaddastageterminationcondition
(e.g. grasp,place,turn,open,push)toknowthestageiscompleteandtomoveontothenextstage.
Thisconditionisdefinedasafunctionf (Oglobal)thattakesinthecurrentobservationofthe
stage
environmentandevaluatesabinarysuccesscriteriaaswellasanaturallanguagedescriptorofthe
condition for prompting the LLM (e.g. ‘grasp’ or ‘place’). These stage termination conditions
areestimatedusingvisualposeestimates. Wedescribethestageterminationconditionsingreater
detail in Sec. 3.5 and Appendix D. The LLM prompt consists of the task description g , the list
l
of supported stage termination conditions (which we hold constant across all environments) and
additionalpromptingstringsforoutputformatting.Weformatthelanguageplansasfollows:(“Region
1”,“TerminationCondition1”),... (“RegionN”,“TerminationConditionN”),assumingtheLLM
predictsNstages. Giventheprompt,theLLMoutputsanaturallanguageplanintheformatlisted
above. Below,weincludeanexamplepromptandplanfortheNutAssemblytask.
4PublishedasaconferencepaperatICLR2024
Prompt: Stageterminationconditions: (grasp,place). Taskdescription: Thegoldnutgoesonthegold
pegandthesilvernutgoesonthesilverpeg. Givemeasimpleplantosolvethetaskusingonlythe
stageterminationconditions. Makesuretheplanfollowstheformattingspecifiedbelowandmakesure
to take into account object geometry. Formatting of output: a list in which each element looks like:
(<object/region>,<stageterminationcondition>).Don’toutputanythingelse.
Plan:[(“goldnut”,“grasp”),(“goldpeg”,“place”),(“silvernut”,“grasp”),(“silverpeg”,“place”)]
Whileanylanguagemodelcanbeusedtoperformthisplanningprocess,wefoundthatofavarietyof
publiclyavailableLLMs(viaweightsorAPI),onlyGPT-4OpenAI(2023)wascapableofproducing
correctplansacrossallthetasksweconsider. Wesamplefromthemodelwithtemperature0for
determinism. WealsodeletecomponentsoftheplanthatcontainLLMhallucinations(ifpresent).
WeprovideadditionaldetailsinAppendixDandexamplepromptsinAppendixG.
3.4 SEQUENCINGMODULE: VISION-BASEDPLANTRACKING
Givenahigh-levellanguageplan,wenowwishtostepthroughtheplanandenablealearnedRLpolicy
tosolvethetask,usingoff-the-shelfvisiontoproducetargetposesforamotionplanningsystemto
achieve. AtstageXofthehigh-levelplan,theSequencingModuletakesinthecorrespondingstep
high-levelplan(“RegionY”,“TerminationConditionZ”)aswellasthecurrentglobalobservationof
thesceneOglobal (RGB-Dview(s)thatcoverthewholescene),predictsatargetrobotposeq
target
andthenreachestherobotposeusingmotionplanning.
VisionandEstimation: Usingatextlabelofthetargetregionofinterestfromthehigh-levelplan
andobservationOglobal,weneedtocomputeatargetrobotstateq forthemotionplannerto
target
achieve. Inprinciple,wecantrainanRLpolicytosolvethistask(learnapolicyπ tomapOglobalto
v
q )giventheenvironmentrewardfunction. However,observethatthe3Dpositionofthetarget
target
regionisareasonableestimateoftheoptimalpolicyπ∗forthistask: intuitively,wewishtoinitialize
v
therobotnearbytotheregionofinterestsoitcanefficientlylearninteraction. Thus,wecanbypass
learningapolicyforthisstepbyleveragingavisionmodeltoestimatethe3Dcoordinatesofthe
targetregion. WeopttouseSegmentAnythingKirillovetal.(2023)toperformsegmentation,asit
iscapableofrecognizingawidearrayofobjects,andusecalibrateddepthimagestoestimatethe
coordinatesofthetargetregion. Weconverttheestimatedregionposeintoatargetrobotposeq
target
formotionplanningusinginversekinematics.
MotionPlanning: Givenarobotstartconfigurationq andarobotgoalconfigurationq ofa
0 target
robot,themotionplanningmoduleaimstofindatrajectoryofway-pointsτ thatformacollision-free
path between q and q . For manipulation tasks, for example, q represents the joint angles
0 target
ofarobotarm. Wecanusemotionplanningtosolvethisproblemdirectly,suchassearch-based
planningCohenetal.(2010),sampling-basedplanningKuffnerJr. &LaValle(2000)ortrajectory
optimizationSchulmanetal.(2013). Inourimplementation,weuseAIT*Strub&Gammell(2020),
a sampling-based planner, due to its minimal setup requirements (only collision-checking) and
favorableperformanceonplanning. Forimplementationdetails,pleaseseeAppendixD.
Overall,theSequencingModulefunctionsastheconnectivetissuebetweenlanguageandcontrol
by moving the robot to regions of interest in the plan, enabling the RL agent to quickly learn
short-horizoninteractionbehaviorstosolvethetask.
3.5 LEARNINGMODULE: EFFICIENTLYLEARNINGLOCALCONTROL
Oncetheagentstepsthroughtheplanandachievesstatesneartargetregionsofinterest,itneedsto
trainanRLpolicyπ tolearnlow-levelcontrolforsolvingthetask.Wetrainπ usingDRQ-v2Yarats
θ θ
etal.(2021),aSOTAvisualmodel-freeRLalgorithm,toproducelow-levelcontrolactions(joint
controlorend-effectorcontrol)fromimages. Furthermore,weproposethreemodificationstothe
learningpipelineinordertofurtherimprovelearningspeedandstability.
First, we train a single RL policy across all stages, stepping through the language plan via the
SequencingModule,tooptimizethetaskrewardfunction. Thealternative,trainingaseparatepolicy
per stage, would require designing stage specific reward functions per task. Instead, our design
enablestheagenttosolvethetaskusingasinglerewardfunctionbysharingthepolicyandvalue
functionsacrossstages. Thissimplifiesthetrainingsetupandallowingtheagenttoaccountforfuture
decisionsaswellasinaccuraciesintheSequencingModule. Forexample,ifπ isinitializedata
θ
5PublishedasaconferencepaperatICLR2024
sub-optimalpositionrelativetothetargetregion,π canadaptitsbehavioraccordingtoitsvalue
θ
function,whichistrainedtomodelthefulltaskreturnE[(cid:80) γtR(s ,a ,s )].
t t t t+1
Second, instead of executing π for a fixed number of steps per stage H , we predict a stage
θ l
terminationconditionf (Oglobal)usingthelanguagemodelandevaluatetheconditionatevery
stage
time-steptotestifastageiscomplete,otherwiseittimesoutafterH steps. Formostconditions,
l
f is evaluated by computing the pose estimate of the relevant object and thresholding. This
stage
process functions as a form of curriculum learning: only once a stage is completed is the agent
allowedtoprogresstothenextstageoftheplan. AsweablateinSec.5,stageterminationconditions
enabletheagenttolearnmoreperformantpoliciesbypreventingditheringbehaviorateachstage. As
anexample,inthenutassemblytaskshowninFig.1,onceπ placesthesilvernutonthesilverpeg,
θ
theplacementconditiontriggers(bycomparetheposeofthenuttothepegpose)andtheSequencing
Modulemovesthearmtonearthegoldpeg.
Finally,asopposedtotrainingthepolicyusingtheglobalviewofthescene(Oglobal),wetrainusing
localobservationsOlocal,whichcanonlyobservethesceneinasmallregionaroundtherobot(e.g.
wristcameraviewsforroboticmanipulation). Thisdesignchoiceaffordsseveraluniqueproperties
that we validate in Appendix C, namely: 1) improved learning efficiency and speed, 2) ease of
chaining pre-trained policies. Our policies are capable of leveraging local views because of the
decompositioninPSL:theRLpolicysimplyhastolearninteractionbehaviorsinasmallregion,it
hasnoneedforaglobalviewofthescene,incontrasttoanend-to-endRLagentthatwouldneedto
seeaglobalviewofthescenetoknowwheretogotosolveatask. Foradditionaldetailsinregarding
thestructureandtrainingprocessoftheLearningModule,seeAppendixD.
4 EXPERIMENTAL SETUP
4.1 TASKS
Weconductexperimentsonsingleandmulti-stageroboticstasksacrossfoursimulatedenvironment
suites(Meta-World,ObstructedSuite,KitchenandRobosuite)whichcontainobstructedsettings,
contact-richsetups,andsparserewards(Fig.F.1). SeeAppendixFforadditionaldetails.
Meta-World: Yuetal.(2020)isanRLbenchmarkwitharichsourceoftasks. FromMeta-World,
weselectfourtasks: MW-Disassemble(removinganutfromapeg),MW-BinPick(pickingand
placingacube),MW-Assembly(puttinganutonapeg),MW-Hammer(hammeringanail).
ObstructedSuite: Yamada et al. (2021) contains tasks that evaluate our agent’s ability to
plan, move and interact with the environment in the presence of obstacles. It consists of three
tasks: OS-Lift (cube lifting in a tall box), OS-Push (push a block surrounded by walls), and
OS-Assembly(avoidingobstaclestoplacetablelegattarget).
Kitchen: Gupta et al. (2019); Fu et al. (2020) tests two aspects of our agent: its ability to
handle sparse terminal rewards and its long-horizon manipulation capabilities. The single-stage
kitchentasksincludeK-Slide(openslidecabinet),K-Kettle(pushkettleforward),K-Burner
(turn burner), K-Light (flick light switch), and K-Microwave (open microwave). The
multi-stageKitchentasksdenotethenumberofstagesinthenameandincludecombinationsofthe
aforementionedsingletasks.
Robosuite: Zhu et al. (2020) contains a wide array of robotic manipulation tasks rang-
ing from single stage (RS-Lift: cube lifting, RS-Door: door opening) to multi-stage
(RS-NutRound,RS-NutSquare,RS-NutAssembly: pick-placenut(s)ontotargetpeg(s)and
RS-Bread,RS-Cereal,RS-Milk,RS-Can,RS-CerealMilk,RS-CanBread: pick-place
object(s)intoappropriatebin(s)). Robosuiteemphasizesrealismandfidelitytoreal-worldcontrol,
enablingustohighlightthepotentialofourmethodtobeappliedtorealsystems.
4.2 BASELINES
Wecompareagainsttwotypesofbaselines,methodsthatlearnfromdataandmethodsthatperform
offlineplanning. WeincludeadditionaldetailsinAppendixD.
LearningMethods:
• E2E:Yaratsetal.(2021)DRQ-v2isaSOTAmodel-freevisualRLalgorithm.
6PublishedasaconferencepaperatICLR2024
Robosuite Lift Kitchen Microwave Robosuite Can Robosuite NutAssembly Round
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of Trials Number of Trials Number of Trials Number of Trials
OS-Assembly MW-Disassemble Metaworld Assembly Metaworld Bin Picking
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of Trials Number of Trials Number of Trials Number of Trials
E2E RAPS MoPA-RL PSL (Ours)
Figure3: SampleEfficiencyResults. Weplottasksuccessrateasafunctionofthenumberoftrials. PSL
improvesonthesampleefficiencyofthebaselinesacrosseachtaskinRobosuite,Kitchen,Meta-World,and
ObstructedSuite.PSLisabletodosobecauseitinitializestheRLpolicyneartheregionofinterest(aspredicted
bythePlanandSequenceModules)andleverageslocalobservationstoefficientlylearninteraction.Additional
learningcurvesinAppendixC.
• RAPS:Dalaletal.(2021)isahierarchicalRLmethodthatmodifiestheactionspaceofthe
agentwithengineeredsubroutines(primitives). RAPSgreatlyaccelerateslearningspeed,
butislimitedinexpressivityduetoitsactionspace,unlikePSL.
• MoPA-RL:Yamadaetal.(2021)issimilartoPSLinitsintegrationofmotionplanningand
RLbutdiffersinthatitdoesnotleverageanLLMplanner;itusestheRLagenttodecide
whenandwheretocallthemotionplanner.
PlanningMethods:
• TAMP:Garrettetal.(2020a)isaclassicalbaselinethatusesaprivilegedviewoftheworld
toperformjointhigh-level(taskplanning)andlow-levelplanning(motionplanningwith
primitives)forsolvinglong-horizonroboticstasks.
• SayCan: are-implementationofSayCanAhnetal.(2022)usingpubliclyavailableLLMs
thatperformsLLMplanningwithafixedsetofpre-definedskills. FollowingtheSayCan
paper,wespecifyaskilllibraryconsistingofobjectpickingandplacingbehaviorsusing
pose-estimation,motion-planningandheuristicactionprimitives. Wedonotlearnthepick
skill as done in SayCan because our setup does not contain a separate set of train and
evaluationenvironments. Inthiswork,weevaluatethesingle-taskRLregimeinwhichthe
agentistestedwithheldoutposes,notheldoutenvironments.
4.3 EXPERIMENTDETAILS
WeevaluateallmethodsasidefromTAMPandMoPA-RL(whichuseprivilegedsimulatorinfor-
mation) using visual input. SayCan and PSL use Oglobal and Olocal. For E2E and RAPS, we
providethelearneraccesstoasingleglobalfixedviewobservationfromOglobal forsimplicityand
speedofexecution;wedidnotfindincludingOlocal improvesperformance(Fig.C.5)Wemeasure
performanceintermsoftasksuccessratewithrespecttothenumberoftrials. Wedosotoprovidea
fairmetricforevaluatingavarietyofdifferentcontrolimplementationsacrossPSL,RAPS,andE2E.
Eachmethodistrainedfor10Kepisodestotal. Wetrainoneachtaskusingthedefaultenvironment
rewardfunction. Foreachmethod,werun7seedsoneverytaskandaverageacross10evaluations.
5 RESULTS
WebeginbyevaluatingPSLonavarietyofsinglestagetasksacrossRobosuite,Meta-World,Kitchen
andObstructedSuite. Next,wescaleourevaluationtothelong-horizonregimeinwhichweshowthat
PSLcanleverageLLMtaskplanningtoefficientlysolvemulti-stagetasks. Finally,weperforman
analysisofPSL,evaluatingitssensitivitytoposeestimationerrorandstageterminationconditions.
5.1 LEARNINGRESULTS
PSL accelerates learning efficiency on a wide array of single-stage benchmark tasks. For
single-stagemanipulation,(inwhichtheLLMpredictsonlyasinglestepintheplan),theSequencing
7
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuSPublishedasaconferencepaperatICLR2024
RS-Bread RS-Can RS-Milk RS-Cereal RS-NutRound RS-NutSquare
E2E .52±.49 0.32±.44 .02±.04 0.0±0.0 .06±.13 0.02±.045
RAPS 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0
TAMP 0.9±.01 1.0±0.0 .85±.06 1.0±0.0 0.4±0.3 .35±.2
SayCan .93±.09 1.0±0.0 0.9±.05 .63±.09 .56±.25 .27±.21
PSL 1.0±0.0 1.0±0.0 1.0±0.0 1.0±0.0 .98±.04 .97±.02
Table1: RobosuiteTwoStageResults. Performanceismeasuredintermsofsuccessrateontwo-stage(2
planneractions)tasks.SayCaniscompetitivewithPSLonpick-placestyletasks,butSayCan’sperformance
drops considerably (86.5% to 41.5% on average) on contact-rich tasks involving assembling nuts due to
cascadingfailures.Onlinelearningmethods(E2EandRAPS)makelittleprogressonthelong-horizontasksin
Robosuite.Ontheotherhand,PSLisabletosolveeachtaskwithatleast97%successrate.
Modulemotionplanstothespecifiedregion,thenhandsoffcontroltotheRLagenttocompletethe
task. Inthissetting,wesolelyevaluatethelearningmethodssincetheplanningproblemistrivial
(onlyonestep). Weobserveimprovementsinlearningefficiency(withrespecttonumberoftrials)as
wellasfinalperformanceincomparisontothelearningbaselinesE2E,RAPSandMoPA-RL,across
11tasksinRobosuite,Meta-World,KitchenandObstructedSuite(Fig.3,left). Foralllearningcurves,
pleaseseetheAppendixC.PSLespeciallyperformswellonsparserewardtasks,suchasinKitchen,
forwhichakeychallengeisfiguringoutwhichobjecttomanipulateandwhereitis. Additionally,we
observequalitativelymeaningfulbehaviorusingPSL:PSLlearnstousethegrippertograspandturn
theburnerknob,unlikeE2EorRAPSwhichendupusingotherjointstoflicktheburner.
PSLefficientlysolvestaskswithobstructionsbyleveragingmotionplanning. Wenowconsider
threetasksfromtheObstructedSuiteinordertohighlightPSL’seffectivenessatlearningcontrol
inthepresenceofobstacles. AsweobserveinFig.3andFig.C.2,PSLisabletodosoefficiently,
solvingeachtaskwithin5Kepisodes,whileE2Efailstomakeprogress. PSLisabletodosobecause
theSequencingModulehandlestheobstacleavoidanceimplicitlyviamotionplanningandinitializes
theRLpolicyinadvantageousregionsnearthetargetobject. Incontrast,E2Espendsasignificant
amountoftimeattemptingtoreachtheobjectinspiteoftheobstacles,failingtolearnthetask. While
MoPA-RLisalsoabletosolvemanyofthetasks,itrequiresmoretrialsthanPSLeventhoughit
operatesoverprivilegedstateinput,astheagentmustsimultaneouslylearnwhenandwheretomotion
planaswellashowtomanipulatetheobject.
PSLenablesvisuomotorpoliciestolearnlong-horizonbehaviorswithupto10stages. Two-stage
resultsacrossRobosuiteandMeta-WorldareshowninTable1andTableC.2,withlearningcurves
inFig.3(right)andFig.C.3. OntheRobosuitetasks,E2EandRAPSfailtomakeprogress: while
theylearntoreachtheobject,theyfailtoconsistentlygraspit,letalonelearntoplaceitinthetarget
location. OntheMeta-Worldtasks,thelearningbaselinesperformwellonmosttasks,achieving
similar performance to PSL due to shaped rewards, simplified low-level control (no orientation
changes)andsmallposevariations. However,PSLissignificantlymoresample-efficientthanE2E
andRAPSasshowninFig.C.3. TAMPandSayCanareabletoachievehighperformanceacrosseach
PickPlacevariantoftheRobosuitetasks(93.75%,86.5%averagedacrosstasks),asthemanipulation
skillsdonotrequiresignificantcontact-richinteraction,reducingfailureskillfailurerates. Cascading
failuresstilloccurduetothebaselines’open-loopnatureofexecution,imperfectstateestimation
(SayCan),plannerstochasticity(TAMP).OnlyPSLisabletoachieveperfectperformanceacross
eachtask,avoidingcascadingfailuresbylearningfromonlineinteraction.
Onmulti-stagetasks(involving3-10stages),wefindthatTAMPandSayCanperformancedrops
significantlyincomparisontoPSL(38%and37%vs. 95%averagedacrosstasks). Formultiple
stages,thecascadingfailureproblembecomesallthemoreproblematic,causingallthreebaselines
tofailatintermediatestages,whilePSLisabletolearntoadapttoimperfectSequencingModule
behaviorviaRL.SeeTable2foradetailedbreakdownoftheresults.
PSL solves contact-rich, long-horizon control tasks such as NutAssembly. In these experi-
ments,weshowthatPSLcanlearntosolvecontact-richtasks(RS-NutRound,RS-NutSquare,
RS-NutAssembly) that pose significant challenges for classical methods and LLMs with pre-
trainedskillsduetothedifficultyofdesigningmanipulationbehaviorsundercontinuouscontact.
Bylearninganinteractionpolicywhosepurposeistoproducelocallycorrectcontact-richbehavior,
wefindthatPSLiseffectiveatperformingcontact-richmanipulationoverlonghorizons(Table1,
Table 2), outperforming SayCan by a wide margin (97% vs. 35% averaged across tasks). Our
decompositionintocontact-freemotiongenerationandcontact-richinteractiondecouplesthewhat
8PublishedasaconferencepaperatICLR2024
RS-CerealMilk RS-CanBread RS-NutAssembly K-MS-3 K-MS-5 K-MS-7 K-MS-10
Stages 4 4 4 3 5 7 10
E2E 0.0/0.0 0.0/0.0 0.0/0.0 0.0/0.0 0.0/0.0 0.0/0.0 0.0/0.0
RAPS 0.0/0.0 0.0/0.0 0.0/0.0 .89/0.1 0.0/0.0 0.0/0.0 0.0/0.0
TAMP .71/.05 .72/.25 0.2/0.3 1.0/0.0 0.0/0.0 0.0/0.0 0.0/0.0
SayCan .73/.05 .63/.21 .23/.21 1.0/0.0 0.0/0.0 0.0/0.0 0.0/0.0
PSL .85±.21 0.9±0.2 .96±.08 1.0±0.0 1.0±0.0 1.0±0.0 1.0±0.0
Table2: Multistage(Long-horizon)results.Performanceismeasublackintermsofmeantasksuccessrate
atconvergence. PSListheconsistentlysolveseachtask,outperformingplanningmethodsbyover70%on
challengingcontact-intensivetaskssuchasNutAssembly.
(targetnut)andwhere(peg)fromthehow(precisiongraspandcontact-richplace),allowingtheRL
agenttosimplyfocusontheaspectoftheproblemthatischallengingtoestimatea-priori: howto
interactwiththeobjectsintheappropriatemanner.
5.2 ANALYSIS
WenowturntoanalyzingPSL,evaluatingitsrobustnesstoposeestimatesandtheimportanceofour
proposedstageterminationconditions. WeincludeadditionalanalysisofPSLinAppendixC.
σ=0 σ=0.01 σ=0.025 σ=0.1 σ=0.5
SayCan 1.0±0.0 .93±.05 .27±.12 0.0±0.0 0.0±0.0
PSL 1.0±0.0 1.0±0.0 1.0±0.0 .75±.07 0.0±0.0
Table3: NoisyPoseAblationResults.WeaddnoisesampledfromN(0,σ)totheposeestimatesandevaluate
SayCanandPSL.PSLisabletohandlenoisyposesbytrainingonlinewithRL,onlyobservingperformance
degradationbeyondσ=0.1.
PSLleveragesstageterminationconditionstolearnfaster. Whilethetargetobjectsequenceis
necessaryforPSLtoplantotherightlocationattherighttime,inthisexperimentweevaluateif
knowledgeofthestageterminationconditionsisnecessary. Specifically,ontheRS-Cantask,we
evaluatetheuseofstageterminationconditionchecksinPSLtotriggerthenextstepintheplanversus
simplyusingatimeoutof25steps. Wefindthatitisinfactcriticaltousestageterminationcondition
checks to enable the agent to effectively sequence the plan; use of a timeout results in dithering
behaviorwhichslowsdownlearning. After10Kepisodesweobserveaperformanceimprovementof
31%(100%vs. 69%)byincludingplanstageterminationconditionsinourpipeline.
PSLproducespoliciesthatarerobusttonoisyposeestimates. Inrealworldsettings,thereisoften
noiseinposeestimationduetonoisydepthvalues,imperfectcameracalibrationorevennetwork
prediction errors. Ideally, the agent should be adapt to such potential failure modes: open-loop
planning methods such as TAMP and SayCan are not well-suited to do so because they do not
improveonline. InthisexperimentweevaluatethePSL’sabilitytohandlenoisy/inaccurateposes
byleveragingonlineinteractionviaRL.OntheRS-Cantask,weaddzero-meanGaussiannoise
tothepose,withσ ∈0.01,0.025,.1,.5andreportourresultsinTable 3. WhileSayCanstruggles
tohandleσ >0.01,PSLisabletolearnwithnoisyposesatσ =.1,atthecostofslowerlearning
performance. Neithermethodperformswellatσ =0.5,howeveratthatpointtheposesarenotnear
theobjectandtheeffectissimilartoresettingtoarandomrobotposeintheworkspaceeveryepisode.
6 CONCLUSIONS
Inthiswork,weproposePSL,amethodthatintegratesthelong-horizonreasoningcapabilitiesof
languagemodelswiththedexterityoflearnedRLpoliciesviaaskillsequencingmodule. Attheheart
ofourmethodliesthedecompositionofroboticstasksintosequentialphasesofcontact-freemotion
generation(usinglanguagemodelplanning)andenvironmentinteraction.Wesolvethesephasesusing
motionplanning(informedbyvisualpose-estimation)andmodel-freeRLrespectively,anapproach
whichwevalidateviaanextensiveexperimentalevaluation. Weoutperformstate-of-the-artmethods
forend-to-endRL,hierarchicalRL,classicalplanningandLLMplanningonover20challenging
vision-basedcontroltasksacrossfourbenchmarkenvironmentsuites. Inthefuture,thisworkcould
beextendedtoimprovingapre-existingrobotskilllibraryovertimeusingRL,enablinganagentto
performplanningwithaneverincreasingrepertoireofskillsthatcanberefinedatalow-level. PSL
canalsobeappliedtosim2realtransfer,sincethepolicieswetraininthisworkuselocalobservations,
theyaremoreamenabletosim2realtransferAgarwaletal.(2023).
9PublishedasaconferencepaperatICLR2024
ACKNOWLEDGEMENTS
WethankRussellMendonca,AnanyeAgarwal,MihirPrabhudesaiandBenEyesenbachfortheir
insightfuldiscussionsandfeedback. WeadditionallythankTheophileGervet,RishiVeerapaneni,
Ajay Mandlekar, Gaurav Parmar, Russell Mendonca and Ben Eyesenbach for feedback on early
draftsofthispaper. Aditionally,JaredMejia,LiliChen,AnanyeAgarwal,KennyShaw,Brandon
Trabucco, Yue Wu, Jing Yu Koh, and Shagun Uppal provided valuable writing feedback. This
workwassupportedinpartbyONRN000141812861,ONRN000142312368andDARPA/AFRL
FA87502321015. Additionally,MDissupportedbytheNSFGraduateFellowship.
REFERENCES
AnanyeAgarwal,AshishKumar,JitendraMalik,andDeepakPathak. Leggedlocomotioninchal-
lengingterrainsusingegocentricvision. InConferenceonRobotLearning,pp.403–415.PMLR,
2023.
MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,Chelsea
Finn,KeerthanaGopalakrishnan,KarolHausman,AlexHerzog,etal. Doasican,notasisay:
Groundinglanguageinroboticaffordances. arXivpreprintarXiv:2204.01691,2022.
IlgeAkkaya,MarcinAndrychowicz,MaciekChociej,MateuszLitwin,BobMcGrew,ArthurPetron,
AlexPaino,MatthiasPlappert,GlennPowell,RaphaelRibas,etal. Solvingrubik’scubewitha
robothand. arXivpreprintarXiv:1910.07113,2019.
Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from
humanvideosasaversatilerepresentationforrobotics. 2023.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
TaoChen,MeghaTippur,SiyangWu,VikashKumar,EdwardAdelson,andPulkitAgrawal. Visual
dexterity: In-handdexterousmanipulationfromdepth. arXivpreprintarXiv:2211.11744,2022.
ShuoChengandDanfeiXu. Guidedskilllearningandabstractionforlong-horizonmanipulation.
arXivpreprintarXiv:2210.12631,2022.
BenjaminCohen,SachinChitta,andMaximLikhachev. Search-basedplanningformanipulation
withmotionprimitives. InInternationalConferenceonRoboticsandAutomation,2010.
Ce´dric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Cle´ment Moulin-Frier, Peter
Dominey,andPierre-YvesOudeyer. Languageasacognitivetooltoimaginegoalsincuriosity
drivenexploration. AdvancesinNeuralInformationProcessingSystems,33:3761–3774,2020.
Murtaza Dalal, Deepak Pathak, and Russ R Salakhutdinov. Accelerating robotic reinforcement
learningviaparameterizedactionprimitives. AdvancesinNeuralInformationProcessingSystems,
34:21847–21859,2021.
MurtazaDalal,AjayMandlekar,CaelanGarrett,AnkurHanda,RuslanSalakhutdinov,andDieter
Fox. Imitatingtaskandmotionplanningwithvisuomotortransformers. 2023.
YuqingDu, OliviaWatkins, ZihanWang, Ce´dricColas, TrevorDarrell, PieterAbbeel, Abhishek
Gupta,andJacobAndreas. Guidingpretraininginreinforcementlearningwithlargelanguage
models. arXivpreprintarXiv:2302.06692,2023.
RichardEFikesandNilsJNilsson. Strips: Anewapproachtotheapplicationoftheoremprovingto
problemsolving. Artificialintelligence,2(3-4):189–208,1971.
AdamFishman,AdithyavairanMurali,ClemensEppner,BryanPeele,ByronBoots,andDieterFox.
Motionpolicynetworks. arXivpreprintarXiv:2210.12209,2022.
10PublishedasaconferencepaperatICLR2024
JustinFu,AviralKumar,OfirNachum,GeorgeTucker,andSergeyLevine. D4rl: Datasetsfordeep
data-drivenreinforcementlearning. arXivpreprintarXiv:2004.07219,2020.
Caelan Reed Garrett, Toma´s Lozano-Pe´rez, and Leslie Pack Kaelbling. Pddlstream: Integrating
symbolicplannersandblackboxsamplersviaoptimisticadaptiveplanning. InProceedingsofthe
InternationalConferenceonAutomatedPlanningandScheduling,volume30,pp.440–448,2020a.
CaelanReedGarrett,ChrisPaxton,Toma´sLozano-Pe´rez,LesliePackKaelbling,andDieterFox.
Onlinereplanninginbeliefspaceforpartiallyobservabletaskandmotionproblems. In2020IEEE
InternationalConferenceonRoboticsandAutomation(ICRA),pp.5678–5684.IEEE,2020b.
Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack
Kaelbling,andTom´asLozano-P´erez. IntegratedTaskandMotionPlanning. Annualreviewof
control,robotics,andautonomoussystems,4,2021.
AbhishekGupta,VikashKumar,CoreyLynch,SergeyLevine,andKarolHausman. Relaypolicy
learning: Solvinglong-horizontasksviaimitation andreinforcementlearning. arXivpreprint
arXiv:1910.11956,2019.
HuyHa,PeteFlorence,andShuranSong. Scalingupanddistillingdown: Language-guidedrobot
skillacquisition. InProceedingsofthe2023ConferenceonRobotLearning,2023.
DanijarHafner,TimothyLillicrap,JimmyBa,andMohammadNorouzi. Dreamtocontrol: Learning
behaviorsbylatentimagination. arXivpreprintarXiv:1912.01603,2019.
AnkurHanda,ArthurAllshire,ViktorMakoviychuk,AlekseiPetrenko,RitvikSingh,JingzhouLiu,
Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, et al.
Dextreme: Transfer of agile in-hand manipulation from simulation to reality. arXiv preprint
arXiv:2210.13702,2022.
AlexanderHerzog*,KanishkaRao*,KarolHausman*,YaoLu*,PaulWohlhart*,MengyuanYan,
JessicaLin,MontserratGonzalezArenas,TedXiao,DanielKappler,DanielHo,JarekRetting-
house,YevgenChebotar,Kuang-HueiLee,KeerthanaGopalakrishnan,RyanJulian,AdrianLi,
ChuyuanKellyFu,BobWei,SangeethaRamesh,KhemHolden,KimKleiven,DavidRendleman,
SeanKirmani,JeffBingham,JonWeisz,YingXu,WenlongLu,MatthewBennice,CodyFong,
DavidDo,JessicaLam,NoahBrown,MrinalKalakrishnan,JulianIbarz,PeterPastor,andSergey
Levine. Deeprlatscale: Sortingwasteinofficebuildingswithafleetofmobilemanipulators. In
arXivpreprintarXiv:2305.03270,2023.
WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch. Languagemodelsaszero-shot
planners: Extractingactionableknowledgeforembodiedagents. InInternationalConferenceon
MachineLearning,pp.9118–9147.PMLR,2022a.
WenlongHuang,FeiXia,TedXiao,HarrisChan,JackyLiang,PeteFlorence,AndyZeng,Jonathan
Tompson,IgorMordatch,YevgenChebotar,etal. Innermonologue: Embodiedreasoningthrough
planningwithlanguagemodels. arXivpreprintarXiv:2207.05608,2022b.
StephenJamesandAndrewJDavison. Q-attention: Enablingefficientlearningforvision-based
roboticmanipulation. IEEERoboticsandAutomationLetters,7(2):1612–1619,2022.
StephenJames,KentaroWada,TristanLaidlow,andAndrewJDavison.Coarse-to-fineq-attention:Ef-
ficientlearningforvisualroboticmanipulationviadiscretisation. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.13739–13748,2022.
LesliePackKaelblingandToma´sLozano-Pe´rez. Integratedtaskandmotionplanninginbeliefspace.
TheInternationalJournalofRoboticsResearch,32(9-10):1194–1227,2013.
DmitryKalashnikov,AlexIrpan,PeterPastor,JulianIbarz,AlexanderHerzog,EricJang,Deirdre
Quillen,EthanHolly,MrinalKalakrishnan,VincentVanhoucke,etal. Scalabledeepreinforcement
learningforvision-basedroboticmanipulation. InConferenceonRobotLearning,pp.651–673.
PMLR,2018.
11PublishedasaconferencepaperatICLR2024
Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,
ChelseaFinn,SergeyLevine,andKarolHausman. Mt-opt: Continuousmulti-taskroboticrein-
forcementlearningatscale. arXivpreprintarXiv:2104.08212,2021.
Daniel Kappler, Franziska Meier, Jan Issac, Jim Mainprice, Cristina Garcia Cifuentes, Manuel
Wu¨thrich,VincentBerenz,StefanSchaal,NathanRatliff,andJeannetteBohg.Real-timeperception
meetsreactivemotiongeneration. IEEERoboticsandAutomationLetters,3(3):1864–1871,2018.
Oussama Khatib. A unified approach for motion and force control of robot manipulators: The
operationalspaceformulation. IEEEJournalonRoboticsandAutomation,3(1):43–53,1987.
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,Tete
Xiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. arXivpreprint
arXiv:2304.02643,2023.
JamesJKuffnerJr.andStevenMLaValle. RRT-Connect: Anefficientapproachtosingle-querypath
planning. InIEEEInternationalConferenceonRoboticsandAutomation(ICRA),2000.
MinaeKwon,SangMichaelXie,KaleshaBullard,andDorsaSadigh. Rewarddesignwithlanguage
models. arXivpreprintarXiv:2303.00001,2023.
YannLabbe´,LucasManuelli,ArsalanMousavian,StephenTyree,StanBirchfield,JonathanTremblay,
JustinCarpentier,MathieuAubry,DieterFox,andJosefSivic. Megapose: 6dposeestimationof
novelobjectsviarender&compare. arXivpreprintarXiv:2212.06870,2022.
MichelleALee,CarlosFlorensa,JonathanTremblay,NathanRatliff,AnimeshGarg,FabioRamos,
andDieterFox. Guideduncertainty-awarepolicyoptimization: Combininglearningandmodel-
basedstrategiesforsample-efficientpolicylearning. In2020IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pp.7505–7512.IEEE,2020.
KevinLin,ChristopherAgia,TokiMigimatsu,MarcoPavone,andJeannetteBohg. Text2motion:
Fromnaturallanguageinstructionstofeasibleplans. arXivpreprintarXiv:2303.12153,2023.
BoLiu,YuqianJiang,XiaohanZhang,QiangLiu,ShiqiZhang,JoydeepBiswas,andPeterStone.
Llm+p: Empoweringlargelanguagemodelswithoptimalplanningproficiency. arXivpreprint
arXiv:2304.11477,2023a.
I-ChunArthurLiu,ShagunUppal,GauravSSukhatme,JosephJLim,PeterEnglert,andYoung-
woon Lee. Distilling motion planner augmented policies into visual control policies for robot
manipulation. InConferenceonRobotLearning,pp.641–650.PMLR,2022.
ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,Jianwei
Yang,HangSu,JunZhu,etal. Groundingdino: Marryingdinowithgroundedpre-trainingfor
open-setobjectdetection. arXivpreprintarXiv:2303.05499,2023b.
TomasLozano-Perez,MatthewTMason,andRussellHTaylor. Automaticsynthesisoffine-motion
strategiesforrobots. TheInternationalJournalofRoboticsResearch,3(1):3–24,1984.
JeffreyMahler,FlorianTPokorny,BrianHou,MelroseRoderick,MichaelLaskey,MathieuAubry,
KaiKohlhoff,TorstenKro¨ger,JamesKuffner,andKenGoldberg. Dex-net1.0: Acloud-based
networkof3dobjectsforrobustgraspplanningusingamulti-armedbanditmodelwithcorrelated
rewards. InIEEEInternationalConferenceonRoboticsandAutomation(ICRA),pp.1957–1964.
IEEE,2016.
AjayMandlekar,CaelanGarret,DanfeiXu,andDieterFox. Human-in-the-looptaskandmotion
planningforimitationlearning. ConferenceonRobotLearning,2023.
MatthewTMason. Mechanicsofroboticmanipulation. MITpress,2001.
Andrew T Miller and Peter K Allen. Graspit! a versatile simulator for robotic grasping. IEEE
Robotics&AutomationMagazine,11(4):110–122,2004.
ArsalanMousavian,ClemensEppner,andDieterFox. 6-dofgraspnet: Variationalgraspgeneration
forobjectmanipulation. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pp.2901–2910,2019.
12PublishedasaconferencepaperatICLR2024
RobinRMurphy. IntroductiontoAIrobotics. MITpress,2019.
Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical
reinforcementlearning. Advancesinneuralinformationprocessingsystems,31,2018.
ROpenAI. Gpt-4technicalreport. arXiv,pp.2303–08774,2023.
RonaldParrandStuartRussell. Reinforcementlearningwithhierarchiesofmachines. Advancesin
neuralinformationprocessingsystems,10,1997.
RichardPPaul. Robotmanipulators: mathematics,programming,andcontrol: thecomputercontrol
ofrobotmanipulators. RichardPaul,1981.
KrishanRana, JesseHaviland, SouravGarg, JadAbou-Chakra, IanReid, andNikoSuenderhauf.
Sayplan:Groundinglargelanguagemodelsusing3dscenegraphsforscalabletaskplanning. arXiv
preprintarXiv:2307.06135,2023.
John Schulman, Jonathan Ho, Alex X Lee, Ibrahim Awwal, Henry Bradlow, and Pieter Abbeel.
Findinglocallyoptimal,collision-freetrajectorieswithsequentialconvexoptimization.InRobotics:
scienceandsystems,volume9,pp.1–10.Berlin,Germany,2013.
IshikaSingh,ValtsBlukis,ArsalanMousavian,AnkitGoyal,DanfeiXu,JonathanTremblay,Dieter
Fox,JesseThomason,andAnimeshGarg. Progprompt: Generatingsituatedrobottaskplansusing
large language models. In 2023 IEEE International Conference on Robotics and Automation
(ICRA),pp.11523–11530.IEEE,2023.
Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su.
Llm-planner: Few-shotgroundedplanningforembodiedagentswithlargelanguagemodels. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pp.2998–3009,2023.
Marlin P Strub and Jonathan D Gammell. Adaptively informed trees (ait): Fast asymptotically
optimalpathplanningthroughadaptiveheuristics. In2020IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pp.3191–3198.IEEE,2020.
Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox. Contact-graspnet:
Efficient6-dofgraspgenerationinclutteredscenes. In2021IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pp.13438–13444.IEEE,2021.
RichardSSutton,DoinaPrecup,andSatinderSingh. Betweenmdpsandsemi-mdps: Aframework
fortemporalabstractioninreinforcementlearning. Artificialintelligence,112(1-2):181–211,1999.
Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, Aleksandra Faust, and Tatsuya Harada. Saytap:
Languagetoquadrupedallocomotion. arXivpreprintarXiv:2306.07580,2023.
RussHTaylor,MatthewTMason,andKennethYGoldberg. Sensor-basedmanipulationplanningas
agamewithnature. InFourthInternationalSymposiumonRoboticsResearch,pp.421–429,1987.
EmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-basedcontrol.
In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026–5033.
IEEE,2012.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothe´e
Lacroix, BaptisteRozie`re, NamanGoyal,EricHambro, FaisalAzhar,etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023b.
MiomirVukobratovic´ andVeljkoPotkonjak. Dynamicsofmanipulationrobots: theoryandapplica-
tion. Springer,1982.
Yen-JenWang,BikeZhang,JianyuChen,andKoushilSreenath. Promptarobottowalkwithlarge
languagemodels. arXivpreprintarXiv:2309.09969,2023.
13PublishedasaconferencepaperatICLR2024
DanielEWhitney. Themathematicsofcoordinatedcontrolofprostheticarmsandmanipulators.
1972.
DanielEWhitney. Mechanicalassemblies: theirdesign,manufacture,androleinproductdevelop-
ment,volume1. OxforduniversitypressNewYork,2004.
JimmyWu,RikaAntonova,AdamKan,MarionLepert,AndyZeng,ShuranSong,JeannetteBohg,
SzymonRusinkiewicz,andThomasFunkhouser. Tidybot: Personalizedrobotassistancewithlarge
languagemodels. arXivpreprintarXiv:2305.05658,2023.
FeiXia,ChengshuLi,RobertoMart´ın-Mart´ın,OrLitany,AlexanderToshev,andSilvioSavarese.
Relmogen: Leveraging motion generation in reinforcement learning for mobile manipulation.
arXivpreprintarXiv:2008.07792,2020.
Jun Yamada, Youngwoon Lee, Gautam Salhotra, Karl Pertsch, Max Pflueger, Gaurav Sukhatme,
Joseph Lim, and Peter Englert. Motion planner augmented reinforcement learning for robot
manipulationinobstructedenvironments. InConferenceonRobotLearning,pp.589–603.PMLR,
2021.
DenisYarats,RobFergus,AlessandroLazaric,andLerrelPinto. Masteringvisualcontinuouscontrol:
Improveddata-augmentedreinforcementlearning. arXivpreprintarXiv:2107.09645,2021.
YufeiYe,XuetingLi,AbhinavGupta,ShaliniDeMello,StanBirchfield,JiamingSong,Shubham
Tulsiani,andSifeiLiu.Affordancediffusion:Synthesizinghand-objectinteractions.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition, pp.22479–22489,
2023.
TianheYu,DeirdreQuillen,ZhanpengHe,RyanJulian,KarolHausman,ChelseaFinn,andSergey
Levine. Meta-world: Abenchmarkandevaluationformulti-taskandmetareinforcementlearning.
InConferenceonrobotlearning,pp.1094–1100.PMLR,2020.
WenhaoYu,NimrodGileadi,ChuyuanFu,SeanKirmani,Kuang-HueiLee,MontseGonzalezArenas,
Hao-TienLewisChiang,TomErez,LeonardHasenclever,JanHumplik,BrianIchter,TedXiao,
PengXu,AndyZeng,TingnanZhang,NicolasHeess,DorsaSadigh,JieTan,YuvalTassa,andFei
Xia. Languagetorewardsforroboticskillsynthesis. ArxivpreprintarXiv:2306.08647,2023.
JesseZhang,JiahuiZhang,KarlPertsch,ZiyiLiu,XiangRen,MinsukChang,Shao-HuaSun,and
JosephJLim. Bootstrapyourownskills: Learningtosolvenewtaskswithlargelanguagemodel
guidance. ConferenceonRobotLearning,2023.
XingyiZhou,RohitGirdhar,ArmandJoulin,PhilippKra¨henbu¨hl,andIshanMisra. Detectingtwenty-
thousandclassesusingimage-levelsupervision. InComputerVision–ECCV2022: 17thEuropean
Conference,TelAviv,Israel,October23–27,2022,Proceedings,PartIX,pp.350–368.Springer,
2022.
YukeZhu,JosiahWong,AjayMandlekar,RobertoMart´ın-Mart´ın,AbhishekJoshi,SoroushNasiriany,
andYifengZhu. robosuite: Amodularsimulationframeworkandbenchmarkforrobotlearning.
arXivpreprintarXiv:2009.12293,2020.
14PublishedasaconferencepaperatICLR2024
Appendix
A TABLE OF CONTENTS
• Ethics,ImpactsandLimitationsStatement(AppendixB):Statementaddressingpotential
ethicsconcernsandimpactsaswellaslimitationsofourmethod.
• Additional Experiments (Appendix C): Additional ablations and analyses as well as
learningcurvesforsingle-stagetasksandMeta-World.
• PSL Implementation Details (Appendix D): Full details on how PSL is implemented,
specificallytheSequencingModule.
• BaselineImplementationDetails(AppendixE):Fulldetailsregardingbaselineimplements
(E2E,RAPS,MoPA-RL,TAMP,SayCan)
• Tasks(AppendixF):Visualizationsofeachtaskaswellasdescriptionsofeachenvironment
suite.
• LLMPromptsandPlans(AppendixG):Promptsthatweuseforourmethodaswellas
generatedplansbytheLLM.
15PublishedasaconferencepaperatICLR2024
B ETHICS, IMPACTS AND LIMITATIONS
B.1 ETHICALCONSIDERATIONS
Thereexistpotentialethicalconcernsfromtheuseoflarge-scalelanguagemodelstrainedoninternet-
scale data. These models have been trained on vast corpi that may contain harmful content and
implicitorevenexplicitbiasesexpressedbyinternetusersandmaybecapableofgeneratingsuch
contentwhenqueried. However,theseissuesarenotspecifictoourwork,rathertheyareinherentto
LLMstrainedatscaleandotherworksthatuseLLMsfaceasimilarethicalconcern. Furthermore,
wenotethatourresearchonlymakesuseofLLMstoguidethebehaviorofarobotatacoarselevel-
specifyingwherearobotshouldgoandhowtoleavethearea. OurLLMpromptingschemeensures
thatthisisallthatisoutputtedfromtheLLM.Suchoutputsleavelittlescopeforabuse,theLLM
is not capable of performing the low-level control itself, which is learned through a task reward
independently.
B.2 BROADERIMPACTS
OurresearchonguidingRLagentstosolvelong-horizontasksusingLLMshaspotentialforboth
positiveandnegativeimpacts. PSLdrawsconnectionsbetweenworkonlanguagemodeling,motion
planning and reinforcement learning for low-level control, which could lead to advancements in
learning for robotics. PSL reduces the engineering burden on the human, instead of manually
specifying/pre-trainingalibraryofbehaviors,onlyarewardfunctionandtaskdescriptionneedbe
specified. Morebroadly,enablingrobotstoautonomouslysolvechallengingroboticstasksincrease
thelikelihoodofrobotsonedaybeingabletocompletelaborintensiveworkindangeroussituations.
However,withincreasedautomation,therearerisksofpotentialjobloss. Furthermore,withincreased
robotcapabilities,thereisariskofmisusebybadactors,forwhichappropriatesafeguardsshouldbe
designed.
B.3 LIMITATIONS
ThereareseverallimitationsofPSLwhichleavescopeforfuturework. 1)Weimposeaspecific
structureonthelanguageplansandtasksolution(gotolocationX,interactthere,soon). Whilethis
assumptioncoversabroadsetoftasksaswellillustrateinourexperimentalevaluation,tasksthat
involveinteractingwithmultipleobjectssimultaneouslyorcontinuousswitchingbetweeninteraction
andmovementinafluidmannermaynotbedirectlyapplicable. Futureworkcanexploreintegrating
amoreexpressiveplanstructurewiththeSequencingModule. 2)Useofmotion-planningmakes
applicationtodynamictaskschallenging. Tothatend,researchonmotion-plannerdistillation,such
asMotionPolicyNetworksFishmanetal.(2022)couldenablemuchfaster,reactivebehavior. 3)
AlthoughtheRLagentiscapableofadaptingposeestimationerrors,inthecurrentformulation,there
isnotmuchtheLearningModulecandoifthehigh-levelplanitselfisentirelyincorrect,orifthe
Sequencingmodulemisinterpretsthelanguageinstructionandmovestherobottothewrongobject.
Oneextensiontoaddressthislimitationwouldbetofine-tunethePlanandSeqModulesonlineusing
RLaswell,toadaptthelargemodelstothespecificenvironmentandrewardfunction.
16PublishedasaconferencepaperatICLR2024
C ADDITIONAL EXPERIMENTS
WeperformadditionalanalysesofPSLinthissection.
RS-Can Camera Ablation
1.0
0.8
0.6
0.4
0.2
0.0
0 1000 2000 3000 4000 5000
Number of Trials
PSL-Fixed PSL-Wrist (Ours) PSL-Wrist+Fixed
FigureC.1: CameraViewLearningPerformanceAblation. wristcameraviewsclearlyaccelerate
learningperformance,convergingtonear100%performance4xfasterthanusingfixed-viewand3x
fasterthanusingwrist+fixed-viewobservations.
Effectofcameraviewonpolicylearningperformance: AsdiscussedinSec.3,forPSLweuse
localobservationstoimprovelearningperformanceandgeneralizationtonewposes. Wevalidate
thisclaimontheRobosuiteCantask,inwhichwehypothesizethatthelocalwristcameraviewwill
acceleratepolicylearningperformance. Thisisbecausetheimageofthecanwillbeindependentof
thecan’spositioningeneralsincetheSequencingModulewillinitializetheRLagentasclosetothe
canaspossible. AsobservedinFig.C.1,thisisindeedthecase-PSLlearns4xfasterthanusinga
fixedviewcameraintermsofthenumberoftrials. Weadditionallytestifcombiningwristandfixed
viewinputs(acommonparadigminrobotlearning)canalleviatetheissue,howeverPSLwithwrist
camisstill3xfasteratsolvingthetask.
Effect of camera view on chaining pre-trained policies: In this ablation, we illustrate another
importanteffectofusinglocalviews,suchaswristcameras: easeofchainingpre-trainedpolicies.
Since we leverage motion planning to sequence between policy executions, chaining pre-trained
policiesisrelativelystraightforward: simplyexecutetheSequencingModuletoreachthefirstregion
ofinterest,executethefirstpre-trainedpolicytillitsstageterminationconditionistriggered,then
calltheSequencingModuleonthenextregion,andsoon. However,todoso,itisalsocrucialthat
theobservationsdonotchangesignificantly, sothattheinputstothepre-trainedpoliciesarenot
outofdistribution(OOD).Ifweuseafixed,globalviewofthescene,theoverallscenewillchange
asmultiplepoliciesareexecuted,resultinginfuturepolicyexecutionsfailingduetoOODinputs.
InTableC.1,weobservethisexactphenomenon,inwhichanyversionofPSLthatisprovideda
fixed-viewinputfailstochainpre-trainedpolicieseffectively,whilePSLwithlocal(wrist)views
onlyisabletochainpre-trainedpoliciesoneverytask,upto5stages.
Effect of incorrect plans on training policies using PSL: As noted in the Limitations Section
(Sec.B),weacknowledgethatasdefined,ifthePlanModuleorSequenceModulefailcatastrophically
(incorrectplanormovingtothewrongregioninspace),thereiscurrentlynoconcretemechanism
for the Learning Module to adapt. However, we run an experiment in which we train the agent
usingPSLusinganincorrecthigh-levelplanontwostagetasks(MW-Assembly,MW-Bin-Picking,
MW-Hammer) and find that in some cases, the agent can still learn to solve the task, achieving
performanceclosetoE2E(Fig.C.4. Intuitively,thisispossiblebecauseinPSL,thehigh-levelplanis
notexpressedasahardconstraint,butratherasaseriesofregionsfortheagenttovisitandasetof
exitconditionsforthoseregions. Intheend,however,onlythetaskrewardisusedtotraintheRL
policysoiftheplaniswrong,theLearnModulemustlearntosolvetheentiretaskend-to-endfrom
sub-optimalinitialstates.
AblatingCameraViewchoiceforBaselines: WeevaluateifincludingOlocal inadditiontoOglobal
improves performance across four tasks (RS-Lift, RS-Door, RS-Can, RS-NutRound) and
includetheresultsinFig.C.5. IngeneralthereislittletonoperformanceimprovementforRAPSor
17
etaR
sseccuSPublishedasaconferencepaperatICLR2024
E2Eacrosstheboard. Theadditionallocalviewmarginallyimprovessampleefficiencybutitdoes
notresolvethefundamentalexplorationproblemforthesetasks.
K-Single-Task K-MS-3 K-MS-4 K-MS-5
PSL-Wrist 1.0±0.0 1.0±0.0 1.0±0.0 1.0±0.0
PSL-Fixed 1.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0
PSL-Wrist+Fixed 1.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0
TableC.1: ChainingPre-trainedPoliciesAblation.PSLcanleveragelocalviews(wristcameras)tochain
togethermultiplepre-trainedpoliciesviamotion-planningusingtheSequencingModule.WhilePSLwitheach
camerainputisabletoproduceacapablesingle-taskpolicy,chainingonlyworkswithwristcameraobservations
astheobservationsarekeptin-distribution.
Robosuite Lift Robosuite Door Kitchen Slide Cabinet Kitchen Kettle
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of Trials Number of Trials Number of Trials Number of Trials
Kitchen Light Switch Kitchen Top Left Burner Kitchen Microwave MW-Disassemble
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of Trials Number of Trials Number of Trials Number of Trials
OS-Assembly OS-Lift OS-Push
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of Trials Number of Trials Number of Trials
E2E RAPS MoPA-RL PSL (Ours)
FigureC.2: SingleStageResults.Weplottasksuccessrateasafunctionofthenumberoftrials.PSLimproves
ontheefficiencyofthebaselinesacrosssingle-stagetasks(planlengthof1)inRobosuite,Kitchen,Meta-World,
andObstructedSuite,achievinganasymptoticsuccessrateof100%onall11tasks.
Metaworld Hammer Metaworld Assembly Metaworld Bin Picking
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of Trials Number of Trials Number of Trials
E2E RAPS MoPA-RL PSL (Ours)
FigureC.3: Meta-WorldTwoStageLearningCurves.Weplottasksuccessrateasafunctionofthenumber
oftrials.PSLlearnsfasterthanthebaselinesbyemployinghigh-levelplanningtoaccelerateRLperformance.
18
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuSPublishedasaconferencepaperatICLR2024
MW-BinPick MW-Assembly MW-Hammer
E2E 1.0±0.0 0.4±0.5 0.0±1.0
RAPS 0.0±0.0 0.3±.25 1.0±0.0
TAMP 1.0±0.0 1.0±0.0 0.0±0.0
SayCan 1.0±0.0 0.5±.08 1.0±0.0
PSL 1.0±0.0 1.0±0.0 1.0±0.0
TableC.2: MetaworldTwoStageResults.Whilethebaselinesperformwellonmostofthetasks,onlyPSL
isabletoconsistentlysolveeverytask. ThisisbecausetheLLMplanningandSequencingmoduleseasethe
learningburdenfortheRLpolicy,enablingittolearncontact-rich,long-horizonbehaviors.
MW-Hammer MW-Assembly MW-BinPick
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of Trials Number of Trials Number of Trials
E2E PSL (Ours) PSL Bad Plan
FigureC.4: PSLbadplans.Weplottasksuccessrateasafunctionofthenumberoftrials.Evenwhengiven
thewronghigh-levelplan,PSLisabletolearntosolvethetask,albeitataslowerrate.
RS-Door RS-Lift RS-NutRound RS-Can
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of Trials Number of Trials Number of Trials Number of Trials
E2E-Oglobal E2E-Olocal+Oglobal PSL (Ours) RAPS-Oglobal RAPS-Olocal+Oglobal
FigureC.5: AblatingCameraViewsforBaselines.Weplottasksuccessrateasafunctionofthenumberof
trials.Asseeninthefigureabove,includingOlocaldoesnotimprovetheperformanceofE2EorRAPS.
19
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
etaR
sseccuSPublishedasaconferencepaperatICLR2024
D PSL IMPLEMENTATION DETAILS
Algorithm2PSLImplementation
Require: LLM,taskdescriptiong,MotionPlannerMP,low-levelhorizonH,segmentationmodelS,RGB-D
l l
globalcameras,RGBwristcamera,CameraMatrixKglobal
1: initializeRL:π ,replaybufferR
θ
PlanningModule
2: High-levelplanP ←Prompt(LLM,g)
l
3: forepisode1...N do
4: forp∈Pdo
SequencingModule
5: targetregion(t),terminationcondition←p
6: PCglobal=Projection(Oglobal,Oglobal,Kglobal)
1 2
7: M ,M =Segmentation(Oglobal,Oglobal,robot,object)
robot obj 1 2
8: PCrobot,PCobject=M ∗PCglobal,M ∗PCscene
robot obj
9: PCscene =PCglobal−PCrobot
10: ee =mean(PCobj)
target
11: q =IK(ee )
target target
12: MotionPlan(MP,q ,PCscene)
target
LearningModule
13: fori=1,...,hlow-levelstepsdo
14: Getactiona ∼π (Olocal)
t θ t
15: GetnextstateOlocal ∼p(|s ,a ).
t+1 t t
16: Store(Olocal,a ,Olocal,r)intoR
t t t+1
17: Sample(Olocal,a ,Olocal,r)∼R ▷k=randomindex
k t k+1
18: updateπ usingRL
θ
19: ifpost-conditionthen
20: break
21: endif
22: endfor
23: endfor
24: endfor
D.1 PLANNINGMODULE
Givenataskdescriptiong ,wepromptanLLMusingtheformatdescribedinSec.3.3toproduce
l
a language plan. We experimented with a variety of publicly available and closed-source LLMs
includingLLAMATouvronetal.(2023a),LLAMA-2Touvronetal.(2023b),GPT-3Brownetal.
(2020), Chat-GPT, and GPT-4 OpenAI (2023). In initial experiments, we found that GPT-based
modelsperformedbest,andGPT-4inparticularlymostcloselyadheredtothepromptandproduced
the most accurate plans. As a result, in our experiments, we use GPT-4 as the LLM planner for
alltasks. Wesamplefromthemodelwithtemperature0fordeterminism. Sometimes, theLLM
hallucinatesnon-existentstageterminationconditionsorobjects. Asaresult,weaddapre-processing
stepinwhichwedeletecomponentsoftheplanthatcontainsuchhallucinations.
D.2 SEQUENCINGMODULE
The input to the Sequencing Module is Oglobal. In our experiments, we use two camera views,
Oglobal andOglobal,whichareRGB-Dcalibratedcameraviewsofthescene,toobtainunoccluded
1 2
viewsofthescene. Weadditionallyprovidethecurrentrobotconfiguration,whichisjointanglesfor
robotarms: q andthetargetregionlabelaroundwhichtheRLpolicymustperformenvironment
joint
interaction. Fromthisinformation,themodulemustsolveforacollisionfreepathtoaregionnear
the target. This problem can be addressed by classical motion planning. We take advantage of
sampling-basedmotionplanningduetoitsminimalsetuprequirements(onlycollision-checking)
andfavorableperformanceonplanning. Inordertorunthemotionplanner,werequireacollision
checker, whichweimplementusingpoint-clouds. Tocomputethetargetobjectposition, weuse
predicted segmentation along with calibrated depth, as opposed to a dedicated pose estimation
20PublishedasaconferencepaperatICLR2024
Targceatn-: ------91
Segmen(tSaAtMi)o n- ---- • eetagrte..... Inverse Kinematics
O rg gl bo bal M r obA oM t ob . 'l
PCglboal MotiPolna n(nAeIrT *)
Oglobal •
depth
Projection
qjo-i-n-t- ---------------------
FigureD.1: SequencingModule.InputstotheSequencingModulearetwocalibratedRGB-Dfixedviews,
Oglobal,theproprioceptionq andthetargetobject.Itperformsvisualmotionplanningtothetargetobject
joint
bycomputingascenepoint-cloud(PCglobal),segmentingthetargetobject(M )toestimateitspose(q ),
obj target
segmentingtherobot(M )toremoveitfromPCglobalandmotionplanningusingAIT*.
robot
network,primarilybecausestateoftheartsegmentationmodels(Kirillovetal.,2023;Zhouetal.,
2022)havesignificantzero-shotcapabilitiesacrossobjects.
Projection: Inthisstep,weprojectthedepthmapfromeachglobalviewofthescene,Oglobal and
1
Oglobal intoapoint-cloudPCglobal usingtheirassociatedcameramatricesKglobal andKglobal. We
2 1 2
performthefollowingprocessingstepstocleanupPCglobal:1)croppingtoremoveallpointsoutside
theworkspace2)voxeldown-samplingwithasizeof0.005m3toreducetheoverallsizeofPCglobal
3)outlierremoval,whichprunespointsthatarefartherfromtheir20neighboringpointsthanthe
averageinthepoint-cloudasshowninFig.D.1.
Segmentation: Wecomputemasksfortherobot(M )andthetargetobject(M )byusinga
robot obj
segmentationmodel(SAMKirillovetal.(2023))S whichsegmentsthescenebasedonRGBinput.
Wereducenoiseinthemasksbyfillingholes,computingcontiguousmaskclustersandselecting
thelargestmask. WeuseM toremovetherobotfromPCglobal,inordertoperformcollision
robot
checkingoftherobotagainstthescene. Additionally,weuseM alongwithPCglobal tocompute
obj
theobjectpoint-cloudPCobj,whichweaveragetoobtainanestimateofobjectposition,whichisthe
targetpositionforthemotionplanner. Forthemanipulationtasksweconsiderinthepaper,thisisthe
targetend-effectorposeoftherobot,ee .
target
VisualMotionPlanning: Giventhetargetend-effectorposeee ,weuseinversekinematics(IK)
target
tocomputeq andpassq ,q ,PCglobal intoajoint-spacemotionplanner. Tothatend,
target joint target
weuseasampling-basedmotionplanner,AIT*Strub&Gammell(2020),toperformmotionplanning.
Inordertoimplementcollisioncheckingfromvision,forasampledjoint-configurationq ,we
sample
computethecorrespondingpositionoftherobotmeshandcomputetheoccupancyofeachpointin
thescenepoint-cloudagainsttherobotmesh. Iftheobjectisdetectedasgrasped,thenweadditionally
removetheobjectfromthescenepointcloud,computeitsconvexhullandusethesigneddistance
functionofthejointrobot-objectmeshforcollisionchecking. Asaresult,theSequencingModule
operatesentirelyovervisualinput,andachievesaposeneartheregionofinterestbeforehanding
offcontroltothelocalRLpolicy. WeemphasizethattheSequencingModuledoesnotneedtobe
perfect,itmerelyneedstoproduceareasonableinitializationfortheLearningModule.
D.3 LEARNINGMODULE
D.3.1 STAGETERMINATIONDETAILS
As described in Section 3, we use stage termination conditions to determine when the Learning
ModuleshouldhandcontrolbacktotheSequencingModuletocontinuetothenextstageintheplan.
Formostofthetasksweconsider,thesestageterminationconditionsamounttocheckingforagrasp
orplacementforthetargetobjectinthestage. Forexample,forRS-NutRound,theplanforthefirst
stageis(grasp,nut)andtheplanforthesecondstageis(place,peg). Placementsarestraightforward
21PublishedasaconferencepaperatICLR2024
tocheck: simplyevaluateiftheobjectbeingmanipulatediswithinasmallregionnearthetarget
object. Thiscanbecomputedusingtheestimatedposeofthetwoobjects(currentandtarget). Grasps
aremorechallengingtoestimateandweemployatwostagepipelinetodetectingagrasp. First,we
estimatetheobjectposeandthenevaluateifthezvaluehasincreasedfromwhenthestagebegan.
Second,inordertoensuretheobjectisnotsimplytossedintheair,wecheckiftherobot’sgripperis
tightlycagingtheobject. Wedosobycollisioncheckingtheobjectpoint-cloudagainstthegripper
mesh. WeusethesamecollisioncheckingprocedureasoutlinedinSec3forcheckingcollision
betweenthescenepoint-cloudandrobotmesh.
Toestimatetheturnedcondition,wecomputethemaskoftheburnerknob,evaluateitsprincipalaxis
andmeasureitsanglefromvertical. IfitisgreaterthanXradiansthenthestageconditiontriggers.
Checkingthepushedconditionisstraightforward,theobjectneedstohavemovedbyXdistance
forwardfromthestartposeinxy. Finally,forcheckingtheopenedcondition,weestimatethehandle
poserelativetothehingeandcomputetheangleofthedoor. IfitisgreaterthanXradiansweconsider
thedooropened. Fortheslidecabinetthehandleposeitselfcanbeusedtocheckopening. Closingis
likewiseestimatedastheinverseofopening. Forallconditions,wetakethethresholdXfromthe
environmentsuccessconditionorrewardfunction.
D.3.2 TRAININGDETAILS
Foralltasks,weusetherewardfunctiondefinedbytheenvironment,whichmaybedenseorsparse
dependingonthetask. WefindthatforPSL,itiscrucialtouseanaction-repeatof1,ingeneralwe
foundthatincreasingthisharmedperformance,incontrasttotheE2Ebaselinewhichperformsbest
withanactionrepeatof2. FortrainingpoliciesusingDRQ-v2,weusethedefaulthyper-parameters
fromthepaper,heldconstantacrossalltasks. Wetrainpoliciesusing84x84images. Weusethe
”medium”difficultexplorationscheduledefinedinYaratsetal.(2021),whichannealstheexploration
σfrom1.0to0.1overthecourseof500Kenvironmentsteps. Duetomemoryconcerns,insteadof
usingareplaybuffersizeof1Masdonein Yaratsetal.(2021),oursisofsize750Kacrosseachtask.
Finally,forpathlength,weusethestandardbenchmarkpathlengthforE2EandMoPA-RL,5per
stageforRAPSfollowing Dalaletal.(2021),and25perstageforPSL.
22PublishedasaconferencepaperatICLR2024
E BASELINE IMPLEMENTATION DETAILS
E.1 RAPS
Forthisbaseline,wesimplytaketheresultsfromtheRAPSDalaletal.(2021)paperasis,whichuse
DreamerHafneretal.(2019)andsparserewards. Ininitialexperiments,weattemptedtocombine
RAPSwithDRQ-v2Yaratsetal.(2021)andfoundthatDreamerperformedbetter,whichisconsistent
withRAPS+DreamerhavingthebestresultsinDalaletal.(2021). WeadditionallytriedtorunRAPS
withdenserewards,butfoundthatthemethodperformedsignificantlyworse. Onepotentialreason
forthisisthatitisnotclearexactlyhowtoaggregatethedenserewardsacrossprimitiveexecutions-
wetriedsimplytakingthedenserewardafterexecutingaprimitiveaswellassimplysummingthe
rewardsofintermediateprimitiveexecutions. BothperformedworsethantrainingRAPSwithsparse
rewards. Note that PSL outperforms RAPS even when both methods have only access to sparse
rewards,e.g. theKitchenenvironments. WeobserveclearbenefitsoverRAPSonthesingle-stage
(Fig.C.2)andmulti-stage(Table2)tasks.
E.2 MOPA-RL
Asdescribedinthemainpaper,wetaketheresultsfromMoPA-RLYamadaetal.(2021)asisonthe
ObstructedSuiteoftasks. Thoseresultswererunfromstate-basedinputandleveragedthesimulator
forcollisionchecking. WedosoaswewereunabletosuccessfullycombineMoPA-RLwithDRQ-v2
basedonthepubliclyreleasedimplementationsofbothmethods.
E.3 TAMP
WeusePDDLStreamGarrettetal.(2020a)astheTAMPalgorithmofchoiceasithasbeenshown
tohavestrongplanningperformanceonlong-horizonmanipulationtasksinRobosuite(Dalaletal.,
2023;Mandlekaretal.,2023). ThePDDLStreamplanningframeworkmodelstheTAMPdomain
and uses the adaptive algorithm, a sampling based algorithm, to plan. This TAMP method uses
samplersforgraspgeneration,placementsampling,inversekinematics,andmotionplanning,making
performancestochastic. Henceweaverageperformanceacross50evaluationstoreducevariance. We
adapttheauthorsTAMPimplementation(from (Dalaletal.,2023;Mandlekaretal.,2023))forour
tasks. Notethismethodusesprivilegedaccesstothesimulator,leveragingknowledgeaboutthetask
(whichmustbeexplicitlyspecifiedinaproblemfile),thescene(fromthedomainfileandaccessto
collisionchecking)and3Dgeometryoftheenvironmentobjects.
E.4 SAYCAN
Asdescribedinthemainpaper,were-implementSayCanAhnetal.(2022)usingGPT-4(thesame
LLM we use in our methdo) and manually engineered pick/place skills that use pose-estimation
andmotion-planning. FollowingourSequencingmodule: 1)webuilda3Dscenepoint-cloudusing
cameracalibrationanddepthimages2)weperformvision-basedposeestimationusingsegmentation
alongwiththescenepointcloudand3)werunmotionplanningusingcollisionqueriesfromthe
3Dpoint-cloud,whichisusedforcollisionqueries. Finally,weuseheuristicallyengineeredpick
andplaceprimitivestoperforminteractionbehaviorwhichwedescribeasfollows. Wenotethatfor
ourtasksofinterest,thepickmotioncanberepresentedasatop-grasp. Oncewepositiontherobot
neartheobject;wethensimplylowertherobotarmtilltheend-effector(notthegrippers)comein
contactwiththeobject. Wethenclosethegrippertoexecutethegrasp. Forplace,wefollowthe
implementationof Ahnetal.(2022)andlowertheheldobjectuntilcontactwithasurface, then
release(openthegripper)andlifttherobotarm. Wesettheaffordancefunctionforbothskillsto1,
followingthedesigninAhnetal.(2022)formotionplannedskills.
ForLLMplanning,wespecifythefollowingprompt:
Giventhefollowinglibraryofrobotskills:... Taskdescription:... Makesuretotakeintoaccount
objectgeometry. Formattingofoutput: alistofrobotskills. Don’toutputanythingelse.
23PublishedasaconferencepaperatICLR2024
Thispromptisthesameasourpromptexceptwespecifytherobotskilllibraryintermsofobject
centricbehaviors,insteadofstageterminationconditions.
Giventhefollowinglibraryofrobotskills: ... Taskdescription: ... Givemeasimpleplanto
solvethetaskusingonlytheprovidedskilllibrary. Makesuretheplanfollowstheformatting
specifiedbelowandmakesuretotakeintoaccountobjectgeometry. Formattingofoutput: alist
ofrobotskills. Don’toutputanythingelse.
Robosuite
SkillLibrary: pickcan,pickmilk,pickcereal,pickbreadslice,picksilvernut,pickgoldnut,
putcanon/inX,putmilkon/inX,putcerealon/inX,putbreadslideon/inX,putsilvernuton/in
X,putgoldnuton/inX,graspdoorhandle,turndoorhandle,pickcube
Kitchen
SkillLibrary: graspverticaldoorhandleforslidecabinet,moveleft,moveright,grasphinge
cabinet,grasptopleftburnerwithredtip,rotatetopleftburnerwithredtip90degreeclockwise,
rotatetopleftburnerwithredtip90degreescounterclockwise,pushlightswitchknobleft,push
lightswitchknobright,graspkettle,liftkettle,placekettleon/inX,graspmicrowavehandle,
pullmicrowavehandle
Metaworld:
Skill Library: grasp cube, place cube on/in X, grasp hammer, place hammer, hit nail with
hammer,graspwrench,liftwrench
Obstructed-Suite
SkillLibrary: graspcan,placecaninbin,inserttableleginX,movetableleg,graspcube,
placecubeontable,pushcube
24PublishedasaconferencepaperatICLR2024
F TASKS
(a)MW-Hammer (b)MW-Assembly (c)MW-Disassemble (d)MW-Bin-Picking
(e)OS-Lift (f)OS-Assembly (g)OS-Push (h)K-Kettle
(i)K-Microwave (j)K-Burner (k)K-Light (l)RS-Lift
(m)RS-Door (n)RS-NutRound (o)RS-NutSquare (p)RS-NutAssembly
(q)RS-Can (r)RS-Cereal (s)RS-Milk (t)RS-Bread
(u)RS-CanBread (v)RS-CerealMilk
FigureF.1: TaskVisualizations.PSLisabletosolvealltaskswithatleast80%successratefrompurelyvisual
input.
25PublishedasaconferencepaperatICLR2024
WediscusseachoftheenvironmentsuitesthatweevaluateusingPSL.Allenvironmentsaresimulated
usingtheMuJoCosimulatorTodorovetal.(2012).
1. Meta-World(Row1ofFig.F.1). Meta-World,introducedbyYuetal.(2020),aimstooffer
astandardizedsuiteformulti-taskandmeta-learningmethods. Thebenchmarkconsists
of 50 separate manipulation tasks with a Sawyer robot, well-shaped reward functions,
involvemanipulatingasingleobjecttoarandomizedgoalposition,ormultipleobjectstoa
deterministicgoalposition. Weevaluateonthesingle-task,multi-goal,v2variantsofthe
Meta-Worldenvironments. Allenvironmentsuseend-effectorpositioncontrol-a3DOF
armactionspacealongwithgrippercontrol-orientationisfixed. Inourevaluationweuse
the default environment task rewards, a fixed camera view for the baselines and a wrist
cameraforourlocalpolicies. WereferthereadertotheMeta-Worldpaperforadditional
detailsregardingtheenvironmentsuite.
2. ObstructedSuite(Rows1-2ofFig.F.1). TheObstructedSuiteoftasksintroducedbyYa-
madaetal.(2021)areachallengingsetoftasksrequiringaSawyerarmtoperformobstacle
avoidancewhilesolvingthetask. TheOS-Lifttaskrequirestheagenttopickupacan
thatisinsideatallbox,requiringittoreachoverthewallstograbtheobjectandthenlift
itwithoutmakingcontactwiththeedgesofthebin. TheOS-Pushenvironmenttasksthe
agentwithpushablocktothegoalinthepresentofabinthatforcestheagenttoadjustits
motioninordertoavoidbeingblockedbyitsupperjoints. Finally,theOS-Assemblytask
involvesmovingtherobotarmtoapreciseplacementlocationwhileavoidingobstacles,
thenperformingthetablelegplacement. Notethatweevaluateourmethodontheseenvi-
ronmentsfromvisualinput,amorechallengingsettingthantheoneconsideredbyYamada
etal.(2021).
3. Kitchen(Rows2-3ofFig.F.1). TheKitchenmanipulationsuiteintroducedintheRelay
PolicyLearningpaperGuptaetal.(2019)andmaintainedinD4RLFuetal.(2020)isaset
ofchallenging,sparsereward,joint-controlledmanipulationtasksinasinglekitchen. We
modifythebenchmarktouseend-effectorcontrolaswefindthatthissignificantlyimproves
learningperformance. Thetasksrequiretheabilitytoexploreefficientlywhilstalsobeing
abletochainskillsacrosslongtemporalhorizons,toachievebehaviorssuchasopening
themicrowave,movingthekettle,flickingthelightswitch,turningthetopleftburner,and
finallyslidingthecabinetdoor(K-MS-5). Asidefromthesingle-stagetasksdescribedin
Section4,weevaluateonthreemulti-stagetaskswhichrequirechainingthesingle-stage
tasksinaparticularorder. K-MS-3involvesmovingthekettle,flickingthelightswitchand
turningthetopleftburner,K-MS-7involvesdoingthesametasksasK-MS-5thenturning
thebottomrightburnerandopeningthehingecabinetandK-MS-10involvesperforming
thetasksinK-MS-7andthenopeningthetoprightburner,openingthebottomleftburner
andthenclosingthemicrowave.
4. Robosuite(Rows3-6ofFig.F.1).TheRobosuitebenchmarkfrom Zhuetal.(2020)contains
challenging,long-horizonmanipulationtasksinvolvingpick-placeandnutassembly,aswell
assimplertasksthatinvolveliftingacubeandopeningadoor. Therewardsarecoarsely
definedintermsofdistancestotargetsaswellasgrasp/placementconditions, which, in
fact,arestraightforwardtoimplementintherealworldaswellusingposeestimation. This
standsincontrasttoMeta-Worldwhichspendsconsiderableengineeringeffortdefining
well-shaped dense rewards often by taking advantage of object geometry. As a result,
learning-based methods struggle to make any progress on Robosuite tasks that involve
morethanasingle-stage-optimizingtherewardfunctiontendstoleavetheagentalocal
minima. The suite also contains a well-tuned, realistic Operation Space Control Khatib
(1987)implementationthatweleveragetotrainpoliciesinend-effectorspace.
26PublishedasaconferencepaperatICLR2024
G LLM PROMPTS AND PLANS
Inthissection,welisttheLLMpromptspertask.
Overallpromptstructure:
Stageterminationconditions: (grasp,place,push,open,close,turn). Taskdescription: ... Give
measimpleplantosolvethetaskusingonlythestageterminationconditions. Makesurethe
planfollowstheformattingspecifiedbelowandmakesuretotakeintoaccountobjectgeometry.
Formattingofoutput: alistinwhicheachelementlookslike: (<object/region>,<operator>).
Don’toutputanythingelse.
G.1 ROBOSUITE
RS-PickPlaceCan:
TaskDescriptioncangoesintobin1.
Plan: [(“can”,“grasp”),(“bin1”,“place”)])
RS-PickPlaceCereal:
TaskDescription: cerealgoesintobin3.
Plan: [(“cereal”,“grasp”),(“bin3”,“place”)])
RS-PickPlaceMilk:
TaskDescription: milkgoesintobin2.
Plan: [(“milk”,“grasp”),(“bin2”,“place”)])
RS-PickPlaceBread:
TaskDescription: breadslicegoesintobin4.
Plan: [(“breadslice”,“grasp”),(“bin4”,”place”)])
RS-PickPlaceCanBread:
TaskDescription: cangoesintobin1,breadsliceinbin4.
Plan: [(“can”,“grasp”),(“bin1”,“place”),(“breadslice”,”grasp”),(“bin4”,”place”)])
RS-PickPlaceCerealMilk:
TaskDescription: milkgoesintoinbin2,cerealinbin3.
Plan: [(“cereal”,“grasp”),(“bin3”,“place”),(“milk”,“grasp”),(“bin2”,“place”)])
RS-NutAssembly:
TaskDescription: Thesilvernutgoesonthesilverpegandthegoldnutgoesonthegoldpeg.
Plan: [(“silver nut”, “grasp”), (“silver peg”, “place”),(“gold nut”, “grasp”), (“gold peg”,
“place”)]
27PublishedasaconferencepaperatICLR2024
RS-NutAssemblySquare:
TaskDescription: Thegoldnutgoesonthegoldpeg.
Plan: [(“goldnut”,“grasp”),(“goldpeg”,“place”)]
RS-NutAssemblyRound:
TaskDescription: Thesilvernutgoesonthesilverpeg.
Plan: [(“silvernut”,“grasp”),(“silverpeg”,“place”)]
RS-Lift:
TaskDescription: lifttheredcube.
Plan: [(“redcube”,”grasp”)]
RS-Door:
TaskDescription: openthedoor.
Plan: [(“doorhandle”,“grasp”)]
G.2 META-WORLD
MW-Assembly:
TaskDescription: putthegreenwrenchonthemaroonpeg.
Plan: [(“greenwrench”,“grasp”),(“maroonpeg”,“place”)]
MW-Disassemble:
TaskDescription: removethegreenwrenchfromthepeg.
Plan: [(“greenwrench”,“grasp”)]
MW-Hammer:
TaskDescription: usetheredhammertopushinthenail.
Plan: [(“redhammer”,“grasp”),(“nail”,“push”)]
MW-Bin-Picking:
TaskDescription: movethecubeintheredbinintothebluebin.
Plan: [(“cubeinredbin”,“grasp”),(“bluebin”,“place”)]
G.3 KITCHEN
Kitchen-Microwave:
TaskDescription: pullthemicrowavedooropen.
Plan: [(“microwavedoorhandle”,“open”)]
28PublishedasaconferencepaperatICLR2024
Kitchen-Slide
TaskDescription: usetherightmostverticalbartoslidethedoor.
Plan: [(“rightmostverticalbar”,“open”)]
Kitchen-Light
TaskDescription: usetheroundknobtoflickthelightswitch.
Plan: [(“knob”,“turn”)]
Kitchen-Burner
TaskDescription: rotatethetopleftburnerwiththeredtip.
Plan: [(“topleftburnerwiththeredtip”,“turn”)]
Kitchen-Kettle
TaskDescription: movethekettleforward.
Plan: [(“kettle”,“push”)]
G.4 OBSTRUCTEDSUITE
OS-Lift:
TaskDescription: liftredcanfromwoodenbin.
Plan: [(“redcan’,“grasp”)]
OS-Assembly:
TaskDescription: movethetableleg,whichisalreadyinyourhand,intotheemptyhole.
Plan: [(“emptyhole’,“place”)]
OS-Push:
TaskDescription: pushtheredblockontothegreencircle.
Plan: [(“redblock”,“grasp”)]
29