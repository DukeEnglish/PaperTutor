Random Pareto front surfaces
∗ ∗ † †
Ben Tu Nikolas Kantas Robert M. Lee Behrang Shafei
Abstract
The Pareto front of a set of vectors is the subset which is comprised solely of all of
the best trade-off points. By interpolating this subset, we obtain the optimal trade-off
surface. In this work, we prove a very useful result which states that all Pareto front
surfaces can be explicitly parametrised using polar coordinates. In particular, our polar
parametrisationresulttellsusthatwecanfullycharacteriseanyParetofrontsurfaceusing
the length function, which is a scalar-valued function that returns the projected length
along any positive radial direction. Consequently, by exploiting this representation, we
show how it is possible to generalise many useful concepts from linear algebra, probability
and statistics, and decision theory to function over the space of Pareto front surfaces.
Notably, we focus our attention on the stochastic setting where the Pareto front surface
itself is a stochastic process. Among other things, we showcase how it is possible to define
andestimatemanystatisticalquantitiesofinterestsuchastheexpectation,covarianceand
quantile of any Pareto front surface distribution. As a motivating example, we investigate
how these statistics can be used within a design of experiments setting, where the goal
is to both infer and use the Pareto front surface distribution in order to make effective
decisions. Besides this, we also illustrate how these Pareto front ideas can be used within
the context of extreme value theory. Finally, as a numerical example, we applied some of
our new methodology on a real-world air pollution data set.
1 Introduction
The Pareto front is often regarded as the natural generalisation of the maximum (or minimum)
for vector-valued sets. This generalisation is based on the Pareto partial ordering relation,
which gives us a way to compare between any two vectors when possible. Geometrically, the
Pareto front can be used to define a surface in the vector space which describes the best possible
trade-offs one can obtain between the different components. The primary focus of this work
will be to study this space of Pareto front surfaces in more detail. To accomplish this, we
establish our main result in Theorem 3.1, which gives us an explicit way to parametrise any
Pareto front surface using polar coordinates. The rest of the work is then focussed on using
this polar parametrisation result in order to generalise existing concepts from linear algebra,
probability and statistics, and decision theory to function over this exotic space of Pareto front
surfaces. In particular, we focus our attention on the stochastic setting where the Pareto front
surface itself is random. Under this regime, our polar parametrisation result tells us that we
can treat any random Pareto front surface as an infinite-dimensional stochastic process (10),
which is indexed by the set of positive unit vectors. This stochastic process can then be used
along with standard ideas from functional data analysis in order to define and estimate any
Pareto front surface statistic of interest in a principled and efficient manner—more details in
∗
Imperial College London, United Kingdom
†
BASF SE, Germany
1
4202
yaM
2
]LM.tats[
1v40410.5042:viXraSection 4. We then proceed to show how these ideas can be used within a standard decision
making workflow, where one is interested in actively identifying decisions which lead to the
best trade-off for the decision maker—see Section 5.1 and Section 5.2. As another independent
application, we also highlight how our work on random Pareto front surfaces can be applied
within the domain of multivariate extreme value theory—more details in Section 5.3. Lastly,
in Section 5.4, we present a numerical example which demonstrates how our ideas can be used
in order to analyse Pareto fronts in time series data. Precisely, we focus our attention on the
change in the maximum amount of daily air pollution in North Kensington, London. We find
that the daily Pareto front of pollutant levels has indeed been reduced after the introduction
of many recent pollution reduction initiatives.
1.1 Structure of the paper
The remainder of the paper is organised as follows: In Section 2, we introduce the key def-
initions and nomenclature that will be used throughout the paper. In Section 3, we present
our main result, which gives an explicit parametrisation of any Pareto front surface based on
polar coordinates. We then use this polar parametrisation in order to define some useful op-
erations and concepts on the space of Pareto front surfaces. Notably we define the concept of
an order-preserving transformation (Section 3.1), a length-based utility function (Section 3.2)
and a length-based loss function (Section 3.3). In Section 4, we study the setting where the
Pareto front surface is random and define many useful statistics of interest such as the expec-
tation (Section 4.1), covariance (Section 4.2) and quantiles (Section 4.3), among others. We
also connected our discussion with existing work on Pareto front surface distributions, which
are based solely on ideas from random set theory (Section 4.6). In Section 5, we demonstrate
the effectiveness of these new ideas on some practical applications. For instance, in Section 5.1,
we present a novel visualisation strategy based our polar parametrisation result. In partic-
ular, we show how it is possible to construct a picture of the whole Pareto front surface by
using a family of low-dimensional slices. Consequently, in Section 5.2, we illustrate how all of
these uncertainty quantification and visualisation ideas can be used within a standard Bayesian
experimental design setting. Specifically, we demonstrate how these tools can be used both dur-
ing an optimisation routine (Section 5.2.1) and also in the post-decision setting (Section 5.2.2).
The former stage focusses on the problem of identifying the best experiments to run, whilst
the latter stage is focussed on identifying the inputs which are most likely going lead to the
desired outputs. Subsequently, in Section 5.3, we demonstrate how it is possible to adapt some
prominent results from extreme value theory to work in the multivariate setting, where the
maximum is defined using the Pareto partial ordering. This is in contrast to most existing
work in multivariate extreme value theory, which has largely focussed on the setting where the
vector-valued maximum is defined in a component-wise fashion. Afterwards, in Section 5.4, we
present a case study on how some of these Pareto front ideas can be used in order to quantify
changes in the daily maximum air pollutant levels in a part of west London. In Section 6,
we conclude this work and include a discussion on future research directions. Finally, in Ap-
pendix A, we include the proofs of all of the results that are stated within the paper. The code
that is needed to reproduce the figures and numerical experiments are available in our Github
repository: https://github.com/benmltu/scalarize.
2 Preliminaries
Inthiswork, westudythepropertiesofParetofrontsurfaces, whicharethesurfacesobtainedby
interpolating the Pareto optimum of a vector-valued set. Without loss of generality, we assume
throughout that the goal of interest is maximisation. Naturally, the minimisation problem can
2Weak: weak[A] Strict: strict[A] Interpolated: int[A] Truncated: int[A]
Y Y Y
Yη
y(1) y(1) y(1) y(1)
Vectors: y A Optimalvectors: y
∗
RM Referencevector: η RM
∈ ∈ ∈
Figure 1: An illustration of the different Pareto fronts in M =2 dimensions.
also be treated by simply negating the corresponding set of vectors. Note that there are many
subtle differences in the literature when it comes to defining a Pareto front surface. For this
reason, we will now carefully define the concept of the truncated interpolated weak Pareto
front, which will be the primary focus of this work.
Definition 2.1 (Pareto domination) The weak, strict and strong Pareto domination is de-
noted by the binary relations , and , respectively. We say a vector y RM weakly,
⪰ ≻ ≻≻ ∈
strictly or strongly Pareto dominates another vector y′ RM if
∈
y y′ y y′ RM ,
⪰ ⇐⇒ − ∈ ≥0
y
≻
y′
⇐⇒
y −y′
∈
RM
≥0
\{0
M
},
y y′ y y′ RM ,
≻≻ ⇐⇒ − ∈ >0
respectively, where 0
M
RM denotes the M-dimensional vector of zeros.
∈
Definition 2.2 (Domination region) The Pareto domination region is defined as the collec-
tion of points which dominates (or is dominated) by a particular set of vectors A RM, that
⊂
is
D⋄[A] :=
a∈A
y RM : y a ,
∪ { ∈ ⋄ }
where , , , , , denotes a partial ordering relation. In addition, we denote the
⋄ ∈ {⪯ ⪰ ≺ ≻ ≺≺ ≻≻}
truncated domination region by D⋄,η[A] := D⋄[A] D≻≻[ η ], for any reference vector η RM
∩ { } ∈
and denote the complement of any domination region by DC ⋄[A] := RM \D⋄[A].
Definition 2.3 (Pareto optimality) Given a bounded set of vectors A RM, a point a A
⊂ ∈
is weakly or strictly Pareto optimal if there does not exist another vector a′ A which strongly
∈
or strictly Pareto dominates it, respectively. The collection of all weakly or strictly Pareto
optimal vectors in this set is called the weak or strict Pareto front, weak[A] := A DC [A] and
Y ∩ ≺≺
strict[A] := A DC[A], respectively.
Y ∩ ≺
Although strict Pareto optimality is a desirable quality, it is a difficult operation to work with
becauseitreturnsasubsetoftheoriginalset,whichcouldcontainanarbitrarynumberofpoints.
To address this problem, we consider working with a more relaxed notion of Pareto optimality,
which we refer to as the interpolated weak Pareto optimality (Definition 2.4). Conceptually, we
define the interpolated weak Pareto front as the interpolation of the strict Pareto front. This
interpolation defines a surface in the M-dimensional vector space. For practical convenience,
we also define the truncated interpolated weak Pareto front, which truncates1 this surface at
1Crudely speaking, we can recover the interpolated weak Pareto front from the truncated interpolated weak
Pareto front by sending the reference point to minus infinity: η =(ϵ,...,ϵ) RM and ϵ .
∈ →−∞
3
)2(y )2(y )2(y )2(ysome reference point η RM—see Figure 1 for an illustration.
∈
Definition 2.4 (Interpolated weak optimality) Given a bounded set of vectors A RM,
⊂
we define the interpolated weak Pareto front as the weak Pareto front of its weak domination
region after closure, that is: int[A] := weak[D⪯[Closure (A)]]. By truncating this surface at
Y Y
some reference vector η RM, we obtain the truncated interpolated weak Pareto front int[A] :=
∈ Yη
int[A] D≻≻[ η ]. We denote the set of all possible non-empty truncated interpolated weak
Y Pareto f∩ ronts b{ y Y} ∗ 2RM.
η ⊂
In practice, the set of objective vectors that we seek to optimise is often the output of some
vector-valued objective function g : X RM, where X denotes the space of feasible inputs. In
→
this setting, the optimal set of objectives is given by the Pareto front of the feasible objective
vectors. For example, the strict Pareto front is given by the set Ystrict[ {g(x) }x∈X]
⊂
RM, whilst
the strict Pareto set is defined as the corresponding pre-image. Similarly, we can also define
the other weak variants of the Pareto front and Pareto set as well.
Remark 2.1 (Reference vector) Throughout this work, we will assume that the reference
vector η RM is known and fixed by the decision maker. Intuitively, this parameter is just
∈
used as a way to lower bound the set of vectors that we are interested in targetting. In the
multi-objective literature, it is common to see this vector set to an estimate that is equal or close
the nadir point, which is the vector comprised of the worst possible values for each objective:
η(m) = min g(m)(x) for objectives m = 1,...,M. In fact, some authors have suggested using
x∈X
a reference point that is slightly worse than the nadir when we are interested in evaluating the
performance of a multi-objective algorithm [Ishibuchi et al., 2017].
Nomenclature. For convenience, unless otherwise stated, we will from now on refer to a set
as being a Pareto front or Pareto front surface if it is a truncated interpolated weak Pareto front
(Definition 2.4). We will refer to a point as being Pareto optimal if it lies on this truncated
interpolated weak Pareto front. In addition, we will refer to the set of all non-empty truncated
interpolated weak Pareto front Y∗ as the set of all Pareto fronts or Pareto front surfaces.
η
3 Polar parametrisation
We now present the main result of this paper (Theorem 3.1), which states that all non-empty
Pareto front surfaces are isomorphic to the set of positive unit vectors
S+M−1 := {z
∈
RM
>0
: ||z
||L2
= 1
} ∈
Y∗ 0M.
Notably we present an explicit representation for this isomorphism in (5), which we refer to
as the polar parametrisation of a Pareto front surface. The name of this representation is
motivated by the fact that we rely on the hyperspherical polar coordinates transformation
in order to derive this result. The intuitive idea behind this result is concisely described in
Figure 2. Informally speaking, we can identify each Pareto optimal point by drawing a line
from the reference vector η RM along a positive direction λ M−1. If we know the length
∈ ∈ S+
of these lines ℓ η,λ R when it intersects the Pareto front surface, then we can reconstruct the
∈
Pareto front surface by using a linear mapping: η+ℓ η,λλ
∈
RM for λ
∈
S+M−1. In the following
paragraphs, we will formalise this idea more concretely and give an explicit construction for
this length function.
4Pareto front Polar parametrisation
Objectivevectors: A RM
⊂
Referencevector: η RM
∈
StrictParetofront: strict[A] RM
Y ⊂
TruncatedweakParetofront: Yηint[A] ⊂RM
Truncatedreferencelines: L η,λ ∩D (cid:22),η[ Yηint[A]] ⊂RM
Weaklyoptimalvectors: η+‘ η,λ[ Yηint[A]]λ ∈RM
y(1) y(1)
Figure 2: An illustration of the polar parametrisation result in M =2 dimensions.
Polar coordinates. To prove Theorem 3.1, we rely on the following coordinate system trans-
formation
Tη
: D≻≻[ {η }]
→
S+M−1 ×R>0,
(cid:18) (cid:19)
y η
Tη(y) := (λ∗ η(y),s η,λ∗ η(y)(y)) =
y
−
η
, ||y −η ||L2 , (1)
L2
|| − ||
which maps any vector y
∈
D≻≻[ {η }] to a positive unit vector λ∗ η(y)
∈
S+M−1 and a positive
scalar s η,λ∗(y)(y) > 0. Intuitively, this mapping can be interpreted as a variant of the hyper-
η
spherical polar coordinates transformation centred at the reference vector η RM. In our
∈
setting, the positive unit vector plays the role of the angle, whilst the positive scalar plays the
role of the projected length. The key difference between this transformation and the standard
hyperspherical polar coordinates transformation is that here we restrict our attention to just
the angles lying in the positive orthant. These positive directions are the only ones which will
lead to a Pareto optimal point. Formally, the coordinate transformation in (1) relies on the
length2 scalarisation function s
η,λ
: RM R,
→
max(y(m) η(m),0)
s (y) := Length [L (η,y)] = min − , (2)
η,λ η,λ ∩ m=1,...,M λ(m)
whichisanon-negative3 functionthatisdefinedforallvectorsy RM. Conceptually, whenthe
∈
objective vector lies in the truncated space y D≻≻[ η ], then the length scalarisation function
∈ { }
computes the length of the line L η,λ := η + tλ : t R lying in the open hyper-rectangle
{ ∈ }
(η,y) := z RM : z(m) (η(m),y(m)) for m = 1,...,M —see the left of Figure 4 for an
{ ∈ ∈ }
illustration of this intuition in two dimensions. Consequently, the optimal direction function
λ∗
η
: D≻≻[ {η }]
→
S+M−1 is the function that returns the largest projected length
y η
λ∗(y) := − argmaxs (y) (3)
η y η ∈ η,λ
|| − ||L2 λ∈S +M−1
for any vector y D≻≻[ η ] lying in the truncated space. Note that to invert this coordinate
∈ { }
transformation we can simply apply the linear transformation Tη−1 : S+M−1 ×R>0
→
D≻≻[ {η }],
where Tη−1((λ,l)) := η +lλ for any (λ,l)
∈
S+M−1 ×R>0.
Polar surfaces. By construction, each reference line L is pointing in an increasing direc-
η,λ
tion. This implies that each reference line should intersect the Pareto front surface in exactly
one point if it is non-empty. If this is not the case, then we get a contradiction to Pareto
2Thisscalarisationfunctionhasalsobeenreferredtointheliteratureasanachievementscalarisationfunction
[Ishibuchi et al., 2009, Deb and Jain, 2012].
3The length scalarisation function is positive on the truncated space D≻≻[ η ] and zero everywhere else.
{ }
5
)2(y )2(yint[Y ] int[Y ] int[Y Y ]
Yη 1 Yη 2 Yη 1 ∪ 2
Referencevector: η RM
∈
Objectivevectors: Yn⊂RM
TruncatedweakParetofront: Yηint[Yn] ⊂RM
Truncatedreferencelines: Lη,λ∩D (cid:22),η[ Yηint[Yn]] ⊂RM
y(1) y(1) y(1)
Figure 3: An illustration of the polar parametrisation associated with a set of two points.
M
optimality. Using this observation, we define the set of polar surfaces Lη 2R , in Defini-
⊂
tion 3.1, to be the set of vectors lying in the truncated space4 which satisfies this intersection
constraint.
Definition 3.1 (Polar surfaces) For any reference vector η RM, a set A RM is called
∈ ⊂
a polar surface, A Lη, if and only if the following statement holds:
∈
A
∈
Lη
⇐⇒
A
⊂
D≻≻[ {η }] ∪{η
}
and |A ∩L η,λ
|
= 1 for all λ
∈
S+M−1.
Moreover, for this set of polar surfaces Lη, we define the projected length function ℓ η,λ : Lη
→
R≥0 as the function which returns the L2-distance of the intersected point along any positive
direction λ M−1:
∈ S+
ℓ [A] = max a
η,λ L2
a∈A∩L || ||
η,λ
for any polar surface A Lη.
∈
Note that there is a duality between a polar surface A Lη and its projected lengths:
∈
A
∈
Lη
←→
{ℓ η,λ[A]
≥
0 : λ
∈
S+M−1 }.
By design, we can easily reconstruct the whole polar surface A Lη, from its collection of
∈
projected lengths, by using the equation
A = {η +ℓ η,λ[A]λ
∈
RM : λ
∈
S+M−1 }. (4)
In other words, we can interpret the set of polar surfaces Lη as a set of sets of vectors which
can be completely characterised by its projected lengths. Geometrically speaking, any polar
surface A Lη can be obtained by translating and then stretching the set of positive unit
∈
vectors M−1 along the positive radial directions. Our main result in Theorem 3.1 states that
S+
the set of all Pareto front surfaces is indeed a special subset of this set: Y∗
η ⊂
Lη. Precisely, it
is the subset where the Pareto partial ordering is preserved and the projected length function
has an explicit formula given in terms of the length scalarisation function (2). We present the
proof of this result in Appendix A.1 and an illustration of it in Figure 2.
Theorem 3.1 (Polar parametrisation) For any bounded set of vectors A RM and refer-
⊂
ence vector η RM, if the corresponding Pareto front surface is not empty int[A] = , then it
∈ Yη ̸ ∅
admits the following polar parametrisation:
(cid:26) (cid:27)
Yηint[A] = η +sups η,λ(a)λ
∈
RM : λ
∈
S+M−1 (5)
a∈A
where ℓ [ int[A]] = sup s (a) is the projected length along λ M−1.
η,λ Yη a∈A η,λ ∈ S+
4Note that we have also included the set η Lη in Definition 3.1. This subtle inclusion is made in order
{ }∈
to accommodate for the degenerate setting—see Remark 3.1.
6
)2(y )2(y )2(yLength scalarisation Hypervolume scalarisation
Objectivevector: y RM
∈
Referencevector: η RM
∈
Referenceline: L η,λ⊂RM
Hyperrectangle: (η,y) RM
⊂
Linewithlengths (y) 0
η,λ ≥
BallwithvolumeτHV(s (y)) 0
η,λ ≥
y(1) y(1)
Figure 4: An illustration of the length-based scalarisation functions in M =2 dimensions.
Remark 3.1 (Singleton front) When the Pareto front surface is empty, int[A] = , then
Yη ∅
the right hand side of (5) evaluates to the degenerate polar surface η Lη. This event can
{ } ∈
happen when the reference vector η RM is set too aggressively in such a way as it weakly
∈
dominates the entire feasible objective space.
Remark 3.2 (Conceptual idea) The explicit form of the polar parametrisation (5) can be
derived in an constructive manner. Firstly, one can show that the Pareto front surface of a
single point a D≻≻[ η ] can be written as the polar surface
∈ { }
Yηint[ {a }] = {η +s η,λ(a)λ
∈
RM : λ
∈
S+M−1
} ∈
Lη.
Consequently, we can then determine the Pareto front surface of any set of points A D≻≻[ η ]
⊂ { }
by simply computing the union of the corresponding individual Pareto front surfaces and then
removing all the points which are sub-optimal, that is
int[A] = int[ int[ a ]]
Yη Yη ∪a∈A Yη { }
= Yηint[ {η +s η,λ(a)λ
∈
RM : a
∈
A,λ
∈
S+M−1 }]
(cid:26) (cid:27)
= η +sups η,λ(a)λ
∈
RM : λ
∈
S+M−1 .
a∈A
Remarkably, this overall procedure turns out to be equivalent to just retaining the points which
achieve the maximal projected lengths along each positive direction λ M−1. An illustration
∈ S+
of this conceptual idea is presented in Figure 3. Note however that the proof of the result in
Appendix A.1 does not actually follow this intuitive construction. Instead, our proof is derived
in a more technical way by taking advantage of a well-known optimisation result regarding the
Chebyshev scalarisation function [Miettinen, 1998, Part 2, Theorem 3.4.5]. The primary reason
why we take this more general approach is to ensure that our polar parametrisation result holds
for sets lying in the whole objective space RM as opposed to just the truncated objective space
D≻≻[ η ].
{ }
Remark 3.3 (Other representations) The coordinate system transformation that we per-
formed in (1) is appealing because it admits a representation for the Pareto front surface, which
is simple, explicit and has a linear dependency on the projected length values. Nevertheless, we
can easily obtain other nonlinear representations of a Pareto front surface by taking advantage
of transformation functions. For example, let τ : RM RM denote an invertible and strictly
→
monotonically increasing transformation function such that y y′ τ(y) τ(y′) for
⪰ ⇐⇒ ⪰
any y,y′ RM. Then, by a simple monotonicity argument, we can show that the following
∈
parametrisation of the Pareto front surface is also valid:
(cid:26) (cid:27)
(cid:16) (cid:17)
Yηint[A] = τ−1 τ(η)+sups τ(η),λ(τ(a))λ
∈
RM : λ
∈
S+M−1 .
a∈A
7
)2(y )2(yIntuitively, this reformulation works by performing the polar parametrisation in the trans-
formed space before inverting the result. Note that strict monotonicity is required here be-
cause we want the Pareto front surface to be invariant under these transformations: int[A] =
Yη
τ−1( int [τ(A)]).
Yτ(η)
Remark 3.4 (Computational cost) For a finite set A RM, the worst-case cost involved
⊂
with computing the strictly Pareto optimal front strict[A] RM is ( A 2M). In contrast,
Y ⊂ O | |
the worst-case cost involved with computing a finite approximation of the polar parametrised
Pareto front surface Yηint[A]
⊂
RM is O( |Λ ||A |M), where Λ = {λ 1,λ 2,...
}
denotes a finite set
of positive unit vectors.
3.1 Order-preserving transformations
The polar parametrisation result in Theorem 3.1 tells us that any Pareto front surface is com-
pletelydefinedbyitsprojectedlengths. Conceptually,thismeansthatwecandefinetransforma-
tions on the space of Pareto front surfaces by simply applying transformations on the projected
lengths. As long as these transformation operations preserve the Pareto partial ordering, then
we can be assured that resulting set is also a valid Pareto front surface. In Proposition 3.1, we
give two necessary and sufficient conditions for this to happen. The first condition (C1) states
that the projected lengths have to be positive—this condition ensures that the resulting Pareto
front surface is non-empty. The second condition (C2) is the Pareto optimality condition—this
ensures that we cannot find two vectors in the set such that one strongly dominates the other.
The necessity of this latter condition follows from the spirit of Lemma 3.1, which presents some
equivalent statements for Pareto domination based on the length scalarisation function (2).
The proof of both of these result are presented in Appendix A.2 and Appendix A.3.
Lemma 3.1 (Domination equivalence) For any Pareto front surface A∗ Y∗ and vector
∈ η
y D≻≻[ η ], we have the following equivalences:
∈ { }
y D⪯,η[A∗] s η,λ∗(y)(y) ℓ η,λ∗(y)[A∗],
∈ ⇐⇒ η ≤ η
y D≺≺,η[A∗] s η,λ∗(y)(y) < ℓ η,λ∗(y)[A∗],
∈ ⇐⇒ η η
y D≻≻,η[A∗] s η,λ∗(y)(y) > ℓ η,λ∗(y)[A∗],
∈ ⇐⇒ η η
y D⪰,η[A∗] s η,λ∗(y)(y) ℓ η,λ∗(y)[A∗],
∈ ⇐⇒ η ≥ η
where s η,λ∗(y)(y) = y η
L2
and ℓ η,λ∗(y)[A∗] = max a∈A∗s η,λ∗(y)(a).
η || − || η η
Proposition 3.1 (Pareto front conditions) Consider a polar surface A Lη, then this set
∈
is a Pareto front surface, A = int[A], if and only if the following two conditions holds:
Yη
C1. The positive lengths condition: ℓ [A] > 0 for all λ M−1.
η,λ ∈ S+
C2. The maximum ratio condition: max ℓ η,λ[A]λ(m) 1 for all λ,υ M−1.
m=1,...,M ℓη,υ[A]υ(m) ≥ ∈ S+
Proposition 3.1 gives us two simple conditions on the projected length function which can be
easily checked whenever we want to determine whether a Pareto front surface is valid. For
convenience, we will say a transformation (or an operation) acting on the space of Pareto front
surfaces is order-preserving if the resulting projected length function satisfies the two conditions
in Proposition 3.1. Below, we present some notable order-preserving operations which are
defined over the general space of polar surfaces. Firstly, we present the union operation in
Example 3.1, which formalises the idea described in Remark 3.2.
Example 3.1 (Union) Consider two Pareto front surfaces A∗,B∗ Y∗, by Theorem 3.1,
∈ η
we can compute the Pareto front surface of the union of these two sets by simply taking the
8Addition Scalarmultiplication Interpolation
1 Referencevector: η ∈RM
0.8
Paretofront: A∗∈Y∗η
0.6
Paretofront: B∗∈Y∗η
Paretofront: Yηint[A∗+B∗] ∈Y∗η
0.4 Paretofront: A∗⊕B∗∈Y∗η
0.2 Paretofront: α (cid:12)A∗∈Y∗η
0.0 Paretofronts:((cid:15) (cid:12)A∗) ⊕((1 −(cid:15)) (cid:12)B∗) ∈Y∗η
y(1) y(1) y(1) (cid:15)
Figure 5: An illustration of some algebraic operations on the space of Pareto front surfaces in M =2 dimen-
sions. For these examples, we set the scalar multipliers to be α=2 and ϵ 0.0,0.2,0.4,0.6,0.8,1.0 .
∈{ }
maximum of the projected lengths, that is
Yηint[A∗ ∪B∗] = {η +max(ℓ η,λ[A∗],ℓ η,λ[B∗])λ
∈
RM : λ
∈
S+M−1
} ∈
Y∗ η.
Besides this, we also define the addition and scalar multiplication operation over the space
of polar surfaces in Example 3.2 and Example 3.3, respectively. These two concepts are very
useful because they give us a simple way to perform standard linear operations over the space
of polar surfaces. Later on, in Section 4, we will implicitly use these linear operations in order
to define the concept of integration over the space of Pareto front surfaces. Explicitly, we used
these ideas in Section 4.1 when we define the expected Pareto front surface.
Example 3.2 (Addition) Consider two Pareto front surfaces A∗,B∗ Y∗, we define the sum
∈ η
of these two Pareto front surfaces by the equation
A∗ ⊕B∗ := {η +(ℓ η,λ[A∗]+ℓ η,λ[B∗])λ
∈
RM : λ
∈
S+M−1
} ∈
Y∗ η.
This set is a Pareto front surface because it satisfies the conditions in Proposition 3.1. Note
that this sum is not necessarily equivalent to the Pareto front surface obtained by adding the two
sets together: int[A∗ +B∗] = A∗ B∗, where A+B := a+b RM : a A,b B denotes
Yη ̸ ⊕ { ∈ ∈ ∈ }
the Minkowski sum for any set of vectors A,B RM. We illustrate this subtle distinction in
⊂
the left plot of Figure 5.
Example 3.3 (Scalar multiplication) Consider a positive scalar ϵ > 0 and a Pareto front
surface A∗ Y∗, we define the scalar multiplication operation by the equation
∈ η
ϵ ⊙A∗ := {η +ϵℓ η,λ[A∗]λ
∈
RM : λ
∈
S+M−1
} ∈
Y∗ η.
This set is a Pareto front surface because it satisfies the conditions in Proposition 3.1. We
illustrate an example of this operation in the middle plot of Figure 5.
3.2 Utility functions
Utility functions are often used in multi-objective optimisation in order to assess the quality
of a Pareto front approximation. In this section, we introduce a family of utility functions
based on the length scalarisation function (2). We show that this family of utility functions
are both interpretable and satisfy many desirable properties such as strict Pareto compliancy
(Proposition 3.2).
Length-based R2 utility. The R2 utilities [Hansen and Jaszkiewicz, 1998] are a special
family of utility functions that are constructed using scalarisation functions. They possess
many general and desirable properties [Tu et al., 2024] which makes them very appealing to
9
)2(y )2(y )2(ywork with. By using the length scalarisation function (2), we define a special subfamily of
the R2 utilities which we call the length-based R2 utilities. Precisely, a utility function is a
length-based R2 utility if it can be written in the form
(cid:20) (cid:21)
(cid:2) (cid:3)
U η,τ[Y] := E λ∼Uniform(S +M−1) m y∈a Yxτ(s η,λ(y)) = E λ∼Uniform(S +M−1) τ(ℓ η,λ[ Yηint[Y]]) , (6)
for any finite set of objective vectors Y RM, where τ : R≥0 R is any strictly monotonically
⊂ →
increasing transformation that is defined over the space of non-negative scalars. In words, this
family of utility functions assesses the quality of a Pareto front approximation based on some
notion of the average length away from the reference vector η RM. In this setting, a greater
∈
utility is more desirable because it means that the set is much further away, on average, from
the reference vector.
Hypervolume indicator. A special case of these length-based R2 utilities is the hyper-
volume indicator [Zitzler and Thiele, 1998], which is a popular performance metric in multi-
objective optimisation. The hypervolume indicator of a finite set Y RM is defined as the
⊂
volume of its truncated domination region:
(cid:20) (cid:21)
U ηHV[Y] := ν[D⪯,η[Y]] = E λ∼Uniform(SM−1) maxc M(s η,λ(y))M (7)
+ y∈Y
where ν[A] := (cid:82) 1 [y A]dy is the Lebesgue measure on RM with A RM denoting a
RM 1 ∈ ⊂
measurable set and denoting the indicator function. The second equality above in (7) shows
that the hypervolume indicator can be written in terms of a transformation of the length
scalarisation function τHV(x) = c xM, where c = πM/22−MΓ(M/2+1)−1 is a positive scalar
M M
depending on the Gamma function Γ(z) =
(cid:82)∞
tz−1e−tdt. This equality has been proved in
0
many earlier works [Shang et al., 2018, Deng and Zhang, 2019, Zhang and Golovin, 2020].
In the left of Figure 6, we present a simple illustration of the hypervolume indicator in two
dimensions.
Remark 3.5 (Hypervolume scalarisation) The hypervolume scalarised value can also be
interpreted geometrically as being equal to the volume of the hypersphere with diameter s (y)
η,λ
≥
0, that is τHV(s η,λ(y)) = ν[ z RM : z
L2
s η,λ(y)/2 ] for any objective vector y RM—
{ ∈ || || ≤ } ∈
see the right of Figure 4 for a visualisation of this interpretation in two dimensions.
General properties. By design, the length-based R2 utilities (6) inherit all of the standard
properties that any R2 utility satisfies such as the monotone and submodularity property [Tu
et al., 2024]. In the following, we will additionally show that this special subfamily of utility
functions also satisfies the strict Pareto compliancy property over the truncated space (Propo-
sition 3.2). Loosely speaking, the strict Pareto compliancy property (Definition 3.3) states that
if a set of vectors is strictly better than another, then it should have a higher utility. This result
follows immediately from the fact that the hypervolume indicator is strictly Pareto compliant
over the truncated space [Zitzler et al., 2003]. We prove this result in Appendix A.4.
Definition 3.2 (Set domination) Consider the subsets A,B RM, we say that the set A
⊂
weakly or strictly dominates the set B if and only if its weak Pareto domination region weakly
or strictly contains the other, respectively:
A B D⪯[A] D⪯[B],
⪰ ⇐⇒ ⊇
A B D⪯[A] D⪯[B].
≻ ⇐⇒ ⊃
10Definition 3.3 (Strict Pareto compliancy) A utility function U : 2RM R is strictly
→
Pareto compliant over a set Y RM, if for all finite sets A,B Y, we have that A
⊆ ⊆ ≻
B = U(A) > U(B).
⇒
Proposition 3.2 (Strict Pareto compliancy) For any reference vector η RM and strictly
∈
monotonically increasing transformation τ : R≥0 R, the corresponding length-based R2 utility
→
(6) satisfies the strict Pareto compliancy property over the truncated space D≻≻[ η ] RM.
{ } ⊂
3.3 Loss functions
Loss functions are an important aspect of decision theory because they quantify the cost as-
sociated with observing one element when we expected another. In this section, we define
the frontier loss functions, which is a family of loss functions acting on the space of Pareto
front surfaces, or more generally the space of polar surfaces Lη. These loss functions are use-
ful because they give us a way to compare between any two Pareto front surfaces. Later on
in Section 4.7, we will show how these loss functions can be used in order to define various
probabilistic concepts on the space of random Pareto front surfaces.
Frontier loss. We define the frontier loss function η,S : Lη Lη R≥0 as the average loss
D × →
incurred along each reference line, that is
Dη,S[A,B] := E
λ∼Uniform(S
+M−1)[S(ℓ η,λ[A],ℓ η,λ[B])] (8)
for any two polar surfaces A,B Lη, where S : R R R≥0 denotes a non-negative scoring
∈ × →
function (or loss function). Clearly, different choices of scoring functions will lead to different
notions of loss. For example, in Table 1, we present some popular scoring functions which are
commonlyusedinthefieldofprobabilisticforecasting—seeGneiting[2011]fordiscussion.
Frontier distance. If the scoring function S is a metric in R, then the resulting frontier
loss function can be interpreted as a type of distance function over the space of polar sur-
faces Lη. Note though that this frontier distance function will only be a pseudometric in Lη.
That is, unlike a metric, the distance between any two polar surfaces A,B Lη will be zero,
∈
[A,B] = 0, if and only if ν[ ℓ [A] = ℓ [B] : λ M−1 ] = 0. In other words, the polar
Dη,S { η,λ ̸ η,λ ∈ S+ }
surfaces A,B Lη are equivalent under this frontier distance if and only if their projected
∈
lengths disagree on a set of measure zero.
Hypervolume distance. A special and interpretable case of the frontier distance function
is the hypervolume distance. The hypervolume distance between any two Pareto front surfaces
is the volume of the symmetric difference between their corresponding truncated dominated
regions. Specifically, if we restrict the distance function to the space of Pareto front surfaces
Y∗ and set SHV(x,y) := τHV(x) τHV(y) , then we recover the hypervolume distance
η | − |
Dη,SHV[A∗,B∗] := ν[D⪯,η[A∗] △D⪯,η[B∗]] = 2U ηHV[A∗ ∪B∗] −U ηHV[A∗] −U ηHV[B∗] (9)
for any Pareto front surfaces A∗,B∗ Y∗, where A B = (A B) (B A) denotes the
∈ η △ \ ∪ \
symmetric difference between the sets A,B RM—see the right of Figure 6 for an illustration
⊂
of this distance function on a two-dimensional example.
Remark 3.6 (Generalised frontier loss) The frontier loss in (8) is defined using the length
scalarisation function (2) and a uniform distribution over the scalarisation parameters. Nat-
urally, we can generalise this construction further by considering an arbitrary scalarisation
function and parameter distribution. This generalisation bares some similarity with the con-
struction of the R2 utilities [Hansen and Jaszkiewicz, 1998, Tu et al., 2024].
11Hypervolume indicator Hypervolume distance
Referencevector: η RM
∈
Paretofront: A∗∈Y∗η
Paretofront: B∗∈Y∗η
RegionwithvolumeU ηHV[A∗] ≥0
Regionwithvolume Dη,SHV[A∗,B∗] ≥0
y(1) y(1)
Figure 6: An illustration of the hypervolume indicator and hypervolume distance in M =2 dimensions.
4 Pareto front surface statistics
Real-world problems are typically noisy and subject to many sources of uncertainty. In order to
quantify this uncertainty, decision makers often appeal to ideas from probability and statistics.
In this section, we will generalise these existing ideas to function over the space of Pareto
front surfaces Y∗. Precisely, we will study the concept of a random Pareto front surface and
η
showcase how our polar parametrisation result (Theorem 3.1) can be easily used in order to
define many statistical quantities of interest. Intuitively, we will treat the Pareto front surface
as an infinite-dimensional stochastic process (10) and define the statistics according to its
corresponding finite-dimensional distributions. Before we introduce these ideas in more detail,
we will first describe the general set up and assumptions that we will be using throughout this
section.
Formulation. Consider a probability space (Ω, ,P), where Ω denotes the sample space,
F F
denotes a σ-algebra and P denotes a probability measure. In this section, we will be interested
in studying the Pareto front surface associated with some vector-valued stochastic process
f : X Ω RM, which we have indexed with the inputs x X. For a fixed and known
× → ∈
reference vector η RM, we denote the corresponding Pareto front surface by
∈
(cid:26) (cid:27)
Y η∗ ,f(ω) := Yηint[ {f(x,ω) }x∈X] = η +sups η,λ(f(x,ω))λ
∈
RM : λ
∈
S+M−1 , (10)
x∈X
for ω Ω, where ℓ [Y∗ (ω)] = sup s (f(x,ω)) 0 are the corresponding the projected
∈ η,λ η,f x∈X η,λ ≥
lengths along the positive directions λ M−1.
∈ S+
Projected length process. By exploiting the explicit form of the random Pareto front
surface in (10), we see that the only dependence on the random quantity ω Ω arises solely
∈
in the projected lengths ℓ [Y∗ (ω)] 0 for λ M−1. Therefore all of the distributional
η,λ η,f ≥ ∈ S+
information about the Pareto front surface is completely characterised by its projected length
process
(ω) := ℓ [Y∗ (ω)] 0 : λ M−1 . (11)
Lη { η,λ η,f ≥ ∈ S+ }
NotethatifweusedanonlinearrepresentationfortheParetofrontsurfaceinstead(Remark3.3),
then it is unlikely that such a simplification would have been possible.
Assumptions. To avoid dealing with some unnecessary complications, we will make the
following two simplifying assumptions. Firstly, we have Assumption 4.1, which states that the
Pareto front surface of interest is non-empty5 almost surely. Secondly, we have Assumption 4.2,
5Note that we could easily lift this non-empty assumption by simply treating every empty Pareto front
surface as being equivalent to the degenerate singleton set η Lη.
{ }∈
12
)2(y )2(yFigure 7: An illustration of the estimated Pareto front surface statistics for a finite set of samples in M = 2
dimensions.
which states that the stochastic process of interest is bounded almost surely. Together, both
of these results ensures that the polar parametrisation of the Pareto front surface, described
on the right of (10), holds almost surely. Moreover, Assumption 4.2 also ensures that the
projected length ℓ [Y∗ ] 0, along any positive direction λ M−1, is bounded almost
η,λ η,f ≥ ∈ S+
surely. Consequently, this means that the moments and quantiles of the projected lengths exist
and are finite.
Assumption 4.1 (Positive lengths) TheParetofrontsurfaceY∗ isnon-emptyalmostsurely.
η,f
Assumption 4.2 (Bounded lengths) The stochastic process f is bounded almost surely.
Remark 4.1 (Finite input space) All of the results in this section hold for any input space
X. Nevertheless, for computational convenience, most of the examples in this section focus
on the practical setting where the input space is finite X < . This implies that the pro-
| | ∞
jected lengths can be computed exactly because the supremum reduces down to a maximum:
ℓ [Y∗ (ω)] = max s (f(x,ω)) for λ M−1.
η,λ η,f x∈X η,λ ∈ S+
Remark 4.2 (Sensitivity to transformations) Most of the probabilistic concepts which we
describe in this section are sensitive to the choice of reference vector (Remark 2.1) and any
nonlinear transformations of the objective space (Remark 3.3). For example, in Section 4.1,
we define the expected Pareto front surface in the original objective space: Eω[Y η∗ ,f(ω)]
∈
Y∗ η.
But we could have quite easily defined the same expectation in the transformed space, before
inverting it back to the original objective space: τ−1(Eω[Y τ∗ (η),τ◦f(ω)]), where τ : RM
→
RM is
an invertible monotonically increasing function. Clearly, these two quantities are not necessarily
equal and therefore some care has to be taken when using one or the other. Note that this
issue also arises for the scalar-valued setting as well: E[max nA n] = τ−1(E[max nτ(A n)]) for
̸
any collection of scalar-valued random variable A and any invertible monotonically increasing
n
transformation τ : R R.
→
4.1 Expectation
We define the expected value of the Pareto front surface distribution (10) by the equation
Eω[Y η∗ ,f(ω)] := {η +Eω[ℓ η,λ[Y η∗ ,f(ω)]]λ
∈
RM : λ
∈
S+M−1 }, (12)
where Eω[ ] denotes the expectation operator under P. It can be easily shown that under
·
the given assumptions, the expectation above satisfies the conditions in Proposition 3.1 and
thereforeisavalidParetofrontsurface. Forcompleteness, westatethisresultinProposition4.1
and prove it in Appendix A.5.
13Proposition 4.1 Under Assumptions 4.1 and 4.2, the expectation of a Pareto front surface
distribution (12) is a Pareto front surface Eω[Y η∗ ,f(ω)]
∈
Y∗ η.
Sample-based estimate. In general, the expected projected lengths is an intractable quan-
tity that cannot be computed exactly. Nevertheless, if we can sample a collection of random
Pareto front surfaces, then we can easily estimate this quantity using a sample average
N
1 (cid:88)
µˆ = ℓ [Y∗ (ω )] (13)
η,λ,N N η,λ η,f n
n=1
for λ M−1, where ω Ω denotes some independent samples of the random parameter.
∈ S+ n ∈
Consequently, we can then define the sample mean Pareto front surface µˆ
η,N ∈
Y∗
η
using the
projected length estimates given by (13). The fact that this estimated front is also a valid
Pareto front surface is a direct consequence of Proposition 4.1. We illustrate an example of the
estimated mean Pareto front surface in the left plot of Figure 7.
Remark 4.3 (Bayesian bootstrap) To quantify some uncertainty about the mean estimate,
we can appeal to the use many traditional techniques from scalar-valued statistics. For example,
we can construct a Bayesian bootstrap [Rubin, 1981] of the estimated means
N
(cid:88)
µˆbootstrap(w) = w(n)ℓ [Y∗ (ω )],
η,λ,N η,λ η,f n
n=1
where w = (w(1),...,w(N)) Uniform(∆M−1) denotes a weight vector sampled from a uniform
∼
distribution over the probability simplex ∆M−1 := {w
∈
RM
≥0
: ||w
||L1
= 1 }. We demonstrate
an example of these bootstrap estimates in the left plot of Figure 7. Note that a similar type of
bootstrap estimate can also be computed for the other Pareto front surface statistics which we
will introduce in the upcoming sections.
4.2 Covariance
We define the covariance of the Pareto front surface distribution (10) as the corresponding
collection of covariance matrices with
Cov ω[Y η∗ ,f(ω),Y η∗ ,f(ω)]
λi,λj
:= λ iλT
j
Cov ω[ℓ η,λi[Y η∗ ,f(ω)],ℓ η,λj[Y η∗ ,f(ω)]]
∈
RM×M (14)
foranytwopositiveunitvectorsλ ,λ M−1, whereCov [ , ]denotesthecovarianceoperator
i j ∈ S+ ω · ·
under P.
Sample-based estimate. Ingeneral, thecovarianceoftheprojectedlengthsisanintractable
quantitythatcannotbecomputedexactly. Nevertheless,ifwecansampleacollectionofrandom
Pareto front surfaces, then we can easily estimate these terms by using the standard sample-
based estimate
N
1 (cid:88)
σˆ2 = (ℓ [Y∗ (ω )] µˆ )(ℓ [Y∗ (ω )] µˆ )
η,λi,λi,N N 1 η,λi η,f n − η,λi,N η,λj η,f n − η,λj,N
− n=1
for any two positive unit vectors λ ,λ M−1, where ω Ω denotes some independent
i j ∈ S+ n ∈
samples of the random parameter with N > 1.
14Marginal deviation surfaces. The marginal variances can be used in order to define an
uncertainty region around the mean. That is, we can define the upper deviation surface
η,β
Z
and lower deviation surface for some β 0 by
η,−β
Z ≥
Zη,β := {η +(µ η,λ +βσ η,λ,λ) +λ
∈
RM : λ
∈
S+M−1
} ∈
Lη
where σ := (σ2 )1/2 denotes the marginal standard deviation in the direction λ M−1
η,λ,λ η,λ,λ ∈ S+
and (x)
+
:= max(x,0), for x R, denotes the truncation function which is used to ensure that
∈
the lengths are non-negative. Note that these polar surfaces are not necessarily Pareto front
surfacesbecausetheydonotnecessarilysatisfythemaximumratioconditioninProposition3.1.
We present an illustration of these deviation surfaces in the middle plot of Figure 7 for a simple
two-dimensional problem.
4.3 Quantiles
WedefinethequantilesoftheParetofrontsurfacedistribution(10)asthecollectionofmarginal
quantiles
Qω[Y η∗ ,f(ω),α] := {η + Qω[ℓ η,λ[Y η∗ ,f(ω)],α]λ
∈
RM : λ
∈
S+M−1 }, (15)
where ω[ ,α] denotes the the α-level quantile operator under P for any α (0,1). It can be
Q · ∈
easily shown that under the given assumptions, the quantiles above satisfies the conditions in
Proposition 3.1 and therefore is a valid Pareto front surface. For completeness, we state this
result in Proposition 4.2 and prove it in Appendix A.6.
Proposition 4.2 Under Assumptions 4.1 and 4.2, for any α (0,1), the α-quantile of a
∈
Pareto front surface distribution (15) is a Pareto front surface Qω[Y η∗ ,f(ω),α]
∈
Y∗ η.
Interpretation. Notethattheα-levelquantile(15)doesnotimplythat100α%ofthepossible
Pareto front surfaces are weakly dominated by this quantile Pareto front surface. Instead, this
definition of the quantile can be interpreted as a surface which divides the truncated objective
space D≻≻[ η ] into two regions. One region is the space dominated by the α-level quantile,
{ }
whichcontainsatleastα-probability. Whilsttheotherregion, itscomplement, containsatmost
(1 α)-probability. This interpretation follows immediately from the alternative formulation of
−
the α-quantiles described in (20), which are based on the concept of domination probabilities
(Section 4.4).
Sample-based estimates. In general, the quantile of the projected lengths is an intractable
quantity that cannot be computed exactly. Nevertheless, if we can sample a collection of
random Pareto front surfaces, then we can easily estimate these quantiles by using an empirical
estimate
qˆ := [ℓ [Y∗ (ω)],α] (16)
η,λ,α,N Qω∼Uniform({wn} n≤N) η,λ η,f
for λ M−1, where ω Ω denotes some independent samples of the random parameter.
∈ S+ n ∈
Consequently, we can then define the empirical quantile Pareto front surface qˆ
η,α,N ∈
Y∗
η
using
the projected lengths estimates given by (16). By Proposition 4.2, this empirical Pareto front
surfaceisindeedavalidParetofrontsurface. Weillustrateanexampleofthisempiricalquantile
front in the right plot of Figure 7.
4.4 Probability of domination
The probability that a vector or a set of vectors is Pareto optimal is given by the probability
of domination. This probability can also be interpreted as a generalisation of the survival
15function for Pareto front surface distributions. By Lemma 3.1, we can write the probability of
domination in term of the projected lengths
P[Y
⊆
D⪯,η[Y η∗ ,f]] = P[ ∧y∈Y(s η,λ∗ η(y)(y)
≤
ℓ η,λ∗ η(y)[Y η∗ ,f])]
=
P(cid:20)
inf
ℓ η,λ∗ η(y)[Y η∗ ,f] 1(cid:21)
,
(17)
y∈Y y η L2 ≥
|| − ||
for any set of vectors Y D≻≻[ η ], where denotes the logical and operation. In practice,
⊆ { } ∧
this probability is an intractable quantity. Nevertheless, if we can sample a collection of random
Pareto front surfaces, then we can easily estimate it by using a simple Monte Carlo average:
P[I(ω)]
≈
N1 (cid:80)N n=11 [I(ω n)], where I(ω)
∈
{0,1
}
denotes a random boolean variable and
ω Ωdenotessomeindependentsamplesoftherandomparameter. IntheleftplotofFigure8,
n
∈
we visualise the contours associated with the estimated marginal domination probabilities for
the Pareto front surface distribution described in Figure 7.
4.5 Probability of deviation
In some cases, we might be interested in computing the probability that a vector or a set of
vectors lies between two potentially random Pareto front surfaces. We refer to this quantity as
the probability of deviation. By using Lemma 3.1, we can write the probability of deviation in
term of the projected lengths
P[Y
⊆
(D⪯,η[A∗] △D⪯,η[B∗]) \Yηint[A∗ ∪B∗]]
(cid:20) (cid:21)
(cid:16) (cid:17)
= P y∈Y min ℓ η,λ∗(y)[Y∗] < s η,λ∗(y)(y) < max ℓ η,λ∗(y)[Y∗]
∧ Y∗∈{A∗,B∗} η η Y∗∈{A∗,B∗} η (18)
(cid:20) (cid:18)ℓ η,λ∗(y)[A∗] (cid:19)(cid:18)ℓ η,λ∗(y)[B∗] (cid:19) (cid:21)
= P sup η 1 η 1 < 0 ,
y η − y η −
y∈Y L2 L2
|| − || || − ||
for any random Pareto front surfaces A∗(ω),B∗(ω)
∈
Y∗
η
and any set vectors Y
⊆
D≻≻[ {η }].
As with the domination probabilities in (17), this quantity can be easily estimated using a
Monte Carlo average. In Figure 8, we give an illustration of the estimated marginal deviation
probabilities when A∗(ω) = Y η∗ ,f(ω) is a random Pareto front surface and B∗(ω) = Eω[Y η∗ ,f(ω)]
is a candidate for the expectation which is deterministic.
4.6 Vorob’ev statistics
Random set theory [Molchanov, 2005] gives us a way to extend standard concepts from proba-
bility and statistics to work over the space of closed sets. Some key ideas from this field have
been used in earlier work in order to define some Pareto front surface statistics [Grunert da
FonsecaandFonseca,2010,Binoisetal.,2015a]. Precisely, theseideashavefocussedondefining
probabilistic concepts over the space of domination regions
⪯
:= D⪯[A] RM : A RM .
D { ⊂ ⊂ }
In the following, we will review the core ideas within these works and show how they relate to
our framework. To increase the generality of our discussion, we will consider working in the
more general space of truncated domination regions
⪯,η
:= D⪯,η[A] RM : A RM .
D { ⊂ ⊂ }
Coverage function. In random set theory, one often relies on the concept of a coverage
function, which gives us the probability that an element lies within a random closed set. When
working over the space of truncated domination regions , this coverage function is simply
⪯,η
D
givenbytheprobabilityofdomination(17): P[y
∈
D⪯,η[Y η∗ ,f]]foranyvectory
∈
RM. Equipped
with this coverage function, we can now define many different types of probabilistic concepts
16by leveraging ideas from random set theory. Notably, the existing works referenced above have
primarily focussed on extending the Vorob’ev concepts of random set theory [Molchanov, 2005,
Section 2.2], which we describe below.
Vorob’ev quantiles. The Vorob’ev α-quantile is a possible generalisation of the quantile
function which is defined using the coverage function. Mathematically, the Vorob’ev α-quantile
is defined as the α-level excursion set associated with the coverage function
QV ωorob’ev[Y η∗ ,f(ω),α] := {y
∈
RM : P[y
∈
D⪯,η[Y η∗ ,f]]
≥
α
}
(19)
for α (0,1). By Lemma 3.1, we see that the Vorob’ev α-quantile (19) is equivalent to the
∈
truncated domination region of the (1 α)-quantile front in (15), that is
−
QV ωorob’ev[Y η∗ ,f(ω),α] = D⪯,η[ Qω[Y η∗ ,f(ω),1 −α]]. (20)
In other words, the α-level quantile in (15) is equal to the Pareto front surface or the upper
isoline of the (1 α)-level Vorob’ev quantile. Note that in the original work by Grunert da
−
Fonseca and Fonseca [2010], they worked in the space of domination regions and therefore
⪯
D
their result does not depend on the reference vector. Crudely speaking, we can recover this
special case by letting the reference vector tend to negative infinity: η = (ϵ,...,ϵ) RM with
∈
ϵ .
→ −∞
Vorob’ev mean. In addition to proposing the Vorob’ev α-quantile, Grunert da Fonseca and
Fonseca [2010] proposed using the median quantile (α = 0.5) as a potential candidate for
the mean Pareto front surface. In contrast Binois et al. [2015a] proposed using the Vorob’ev
expectation as a candidate for the mean. The Vorob’ev expectation is defined as the α∗-level
Vorob’ev quantile whose hypervolume is the closest to the expected hypervolume of the random
Pareto front surface. Precisely, EVorob’ev[Y∗ (ω)] := Vorob’ev[Y∗ (ω),α∗], whereα∗ satisfies the
ω η,f Qω η,f
inequality
ν[ Vorob’ev[Y∗ (ω),α]] V ν[ Vorob’ev[Y∗ (ω),α∗]] (21)
Qω η,f ≤ η,f ≤ Qω η,f
for any α > α∗, where V
η,f
:= Eω[ν[D⪯,η[Y η∗ ,f(ω)]]] = Eω[U ηHV[Y η∗ ,f(ω)]] is the expected hyper-
volume of the Pareto front surface distribution. Note that the Vorob’ev mean is a Vorob’ev
quantile and therefore it is a truncated domination region and not a Pareto front surface. As a
result, when we later refer to the Vorob’ev mean front, we are actually referring to the Pareto
front surface of the Vorob’ev mean, that is the (1 α∗)-quantile front.
−
Vorob’ev deviation. Analogous to how the traditional scalar-valued expectation minimises
the variance, the Vorob’ev mean is known to minimise a quantity known as the Vorob’ev
deviation. In the work by Binois et al. [2015a], they defined the Vorob’ev deviation of a Pareto
front surface distribution as the expected hypervolume of the symmetric difference between the
Pareto front surface and the considered set. Equivalently, the quantity can be written as the
expected hypervolume distance (9) between these two sets, that is
Vorob’evDeviation η,f[A] := Eω[ν[A △D⪯,η[Y η∗ ,f(ω)]]] (22)
for any measurable set A (D≻≻[ η ]) lying in the truncated space. As shown in the work
∈ M { }
by Molchanov [2005, Section 2.2], the Vorob’ev mean is a minimiser to the Vorob’ev deviation
over the space of measurable sets A (D≻≻[ η ]) subject to the condition that ν[A] = V η,f.
∈ M { }
That is,
EV ωorob’ev[Y η∗ ,f(ω)]
∈
argmin Vorob’evDeviation η,f[A].
A∈M(D≻≻[{η}]):ν[A]=V
η,f
17Figure 8: An illustration of the Vorob’ev statistics on an M = 2 dimensional example. On the left plot, we
illustrate the contours of the estimated marginal probability of domination (17). In the middle and right plot,
we illustrate the estimated Vorob’ev mean front and expected front, respectively. For both of these fronts, we
include the contours of the estimated marginal probability of deviation (18) between the random Pareto front
surface and the mean estimate.
Validity of the Pareto front surfaces. As a direct consequence of Proposition 4.2, the
positive isolines of the Vorob’ev based Pareto front surfaces are all valid Pareto front surfaces
because they are just instances of the quantile front (15). This result was taking for granted in
the original works, but we have shown that it follows quite naturally from our polar parametri-
sation result.
Sample-based estimates. The Vorob’ev statistics above can be estimated using the em-
pirical quantiles as we described in Section 4.3. For the Vorob’ev mean front, we have to
additionally compute the parameter α∗ which satisfies the hypervolume condition (21). To
approximately solve this problem, Binois et al. [2015a] proposed using an iterative bisection
scheme. Notably, this procedure can be very expensive because it requires evaluating the hy-
pervolume of the quantiles several times. On the other hand, our definition of the expectation
is much cheaper and simpler to estimate. In Figure 8, we present a visual comparison between
the estimated Vorob’ev mean front and our sample mean front for a simple two-dimensional
example. In this example, there does not appear to be any meaningful difference between these
two Pareto front surfaces.
4.7 Decision-theoretic construction
Many standard concepts in probability, concerning real-valued random variables, can be con-
structed under the decision-theoretic perspective in which we optimise a certain scoring rule
[Gneiting,2011]. Remarkably, ourworkonthepolarparametrisationoftheParetofrontsurface
gives us an easy way to generalise this decision-theoretic construction to the space of Pareto
frontsurfacedistributions. Inparticular, wecanrecovermanyofthe Pareto frontsurfacestatis-
tics introduced in the previous sections as being equal to a functional F , which minimises some
ω
expected frontier loss (8),
F ω[Y η∗ ,f(ω)]
∈
argminEω[ Dη,S[A,Y η∗ ,f(ω)]], (23)
A∈Lη
where S : R R R≥0 denotes some scoring function. In Table 1, we present the explicit
× →
choicesofS whichleadstothedifferentParetofrontsurfacestatisticsthatweintroducedearlier.
Note that all of the results in this table follow immediately from the standard construction of
these functionals in the univariate setting. To see this, we first note that we can rewrite the
expected frontier loss as an expectation over the space of positive unit vectors by interchanging
18the order of integration, that is
Eω[ Dη,S[A,Y η∗ ,f(ω)]] = E
λ∼Uniform(S
+M−1)[Eω[S(ℓ η,λ[A],ℓ η,λ[Y η∗ ,f(ω)])]].
As there are no inter-dependencies between the different unit vectors in this expression, we see
that the optimisation problem in (23) can equivalently be solved by independently solving for
the projected lengths along each positive direction, namely
ℓ η,λ[F ω[Y η∗ ,f(ω)]]
∈
argminEω[S(a,ℓ η,λ[Y η∗ ,f(ω)])] (24)
a≥0
for λ M−1. Evidently, once we have solved this collection of optimisation problems, we can
∈ S+
easily reconstruct the polar surface F ω[Y η∗ ,f(ω)]
∈
Lη by using the linear mapping described
earlier in (4). In essence, the scalar-valued optimisation problems in (24) are equivalent to
the standard decision-theoretic optimisation problems which appear in the univariate setting.
Consequently, this means that we can just take advantage of existing results from univariate
statistics in order to define Pareto front surface statistics. For instance, in the previous sections,
we set the scoring function to be the squared error loss and the pinball loss in order to recover
the expected value [Savage, 1971] and the α-quantiles [Raiffa and Schlaifer, 1968, Ferguson,
1967], respectively. Naturally, we can also use this strategy to generalise any other standard
univariate statistic of interest such as the mode, α-expectiles and so on.
Pareto front surface statistic F [ ] Scoring function: S(x,y)
ω
·
Mean front Eω[ ] (x y)2
Quantile front [· ,α] (1 [− x y] α)(x y)
ω
Vorob’ev quantile front Q int[· Vorob’ev[ ,α]] (1 [x ≤ y]− (1 α− ))(x y)
Yη Qω · ≤ − − −
Vorob’ev mean front int[EVorob’ev[ ]] (1 [x y] (1 α∗))(x y)
Yη ω · ≤ − − −
Table 1: A list of Pareto front surface statistics and their scoring functions (23).
5 Applications
Many of the concepts and results which we describe in this work are general and can be
applied in any scenario where we want to infer or quantify properties about a random Pareto
front surface. In this section, we identify a few concrete applications of these ideas which
might be of interest for practitioners and researchers alike. Firstly, we begin in Section 5.1 by
looking at the problem of Pareto front visualisation, which is a core component in many modern
decision making workflows. We showcase how it is possible to adapt many existing visualisation
strategies from the deterministic setting into the stochastic setting by appealing to our polar
parametrisation result. Secondly, in Section 5.2, we illustrate how all of these Pareto front
surface statistics and visualisation ideas can be used within a standard Bayesian experimental
design setting. Subsequently, in Section 5.3, we highlight how our polar parametrisation result
can be used within the realms of multivariate extreme value theory. Lastly, in Section 5.4, we
present a Pareto front analysis of a real-world time series data set concerning the air pollution
levels in North Kensington, London.
5.1 Visualisation
Visualisation is a key element in many modern multi-objective decision making pipelines. We
can easily visualise the Pareto front surface for low-dimensional problems (M 3) by using a
≤
19simple two-dimensional line plot or a three-dimensional surface plot. For higher dimensional
problems(M > 3),thesesimpleapproachesnolongerworkandwehavetobecomemorecreative
when it comes to visualising the Pareto front—see the work by Tuˇsar and Filipiˇc [2015] for a
survey on these visualisation strategies. Notably, this problem of visualising the Pareto front
surface is exacerbated when we also want to include uncertainty information about the Pareto
front surface distribution as well. The majority of existing work in Pareto front visualisation
has largely focussed on the deterministic setting. To the best of our knowledge, very little
work (if any) has focussed on the practically important problem of visualising a random Pareto
front surface. In this section, we address this gap in the literature by proposing a general
visualisation approach which works even in the stochastic setting. Our strategy hinges on the
main result in this section (Proposition 5.1), which gives us a way to slice a high-dimensional
Pareto front surface into a collection of lower-dimensional Pareto front surfaces. Conceptually,
this result tells us that we can build a picture of the overall Pareto front surface by navigating
the space of low-dimensional slices. To illustrate the usefulness of our new method, we built
two simple dashboard applications which can be used to visualise and navigate the space of
one or two-dimensional Pareto front surface slices, respectively—see Figures 10 and 11 for a
snapshot of these applications.
5.1.1 Projected Pareto front surfaces
The polar parametrisation result tells us that any M-dimensional Pareto front is isomorphic to
the set of positive unit vectors M−1. Therefore, in theory, any strategy that can be used to
S+
visualise the set of positive unit vectors can also be used to visualise a Pareto front surface. In
this section, we propose a novel idea to visualise any Pareto front surface by appealing to the
following partitioning of the space of positive unit vectors:
(cid:91)
M−1 = λ M−1 : (λ) = v (25)
S+ { ∈ S+ P[M]\I }
v∈VM−P
+
where I = i ,...,i [M] := 1,...,M is a non-empty set of I = P ordered indices
1 P
{ } ⊂ { } | |
with i 1 <
···
< i P; V+M−P := {z
∈
R >M 0−P : ||z ||L2 < 1
}
is the set of vectors lying within the
positive orthant of the (M P)-dimensional sphere; and
I
: RM RP is the projection
− P →
mapping with I(a) = (a(i1),...,a(iP)) RP for any vector a RM. Intuitively, the partition
P ∈ ∈
in (25) gives us a way to slice up the set of positive unit vectors by intersecting it with the
hyperplanes y RM : [M]\I(y) = v . By construction, if we project each slice onto the
{ ∈ P }
remaining indices, then we obtain a lower-dimensional Pareto front surface. We summarise
this result in Lemma 5.1 and prove it in Appendix A.7. Notably, this result gives us a way to
visualise the set of positive unit vectors M−1 through its lower-dimensional projections. In
S+
the following, we will extend this visualisation strategy to work for any arbitrary Pareto front
surface A∗ Y∗.
∈ η
Lemma 5.1 (Projected slice) For any non-empty set of P > 0 indices I [M] and vector
⊂
v
∈
V+M−P, the polar surface PI,v[ S+M−1] := {PI(λ)
∈
RP : λ
∈
S+M ,v−1
} ∈
L0P is a P-
dimensional Pareto front surface with the zero reference vector, that is PI,v[ S+M−1]
∈
Y∗ 0P.
Projection mapping. We will now formalise the concept of the projected Pareto front sur-
face, which is the natural extension to the ideas described above. To accomplish this, we define
the reconstruction function ϕ : M−P P−1 M−1, which creates a positive unit vector
I V+ × S+ → S+
whose projected values are given by
(cid:113)
(ϕ (v,λ)) = 1 v 2 λ and (ϕ (v,λ)) = v
PI I −|| ||L2 P[M]\I I
20Figure 9: An illustration of the three different ways that we can generate two-dimensional slices of a three-
dimensional Pareto front surface.
for any non-empty set of P ordered indices I = i ,...,i [M], any vector v M−P and
{ 1 P } ⊂ ∈ V+
any positive unit vector λ P−1. Explicitly, the reconstruction function takes the form
∈ S+
(cid:40)(cid:112)
1 v 2 λ(p), if m I and m = i ,
ϕ(m) (v,λ) := −|| ||L2 ∈ p (26)
I v(p), if m J and m = j ,
p
∈
for m = 1,...,M, where J = [M] I = j ,...,j denotes the complement of I containing
1 M−P
\ { }
M −P orderedindices. Usingthisfunction,wedefinetheprojectionmapping PI,v : Lη
→
LPI(η)
to satisfy the equation
(cid:113)
PI,v[A] := {PI(η)+ℓ η,ϕI(v,λ)[A] 1 −||v ||2 L2λ
∈
RP : λ
∈
S+P−1 }, (27)
for any polar surface A Lη. In Proposition 5.1, we show that the projection of any Pareto
∈
front surface A∗
∈
Y∗ η, that is PI,v[A∗]
∈
LPI(η), is a valid P-dimensional Pareto front surface
with the reference vector I(η) RP. This result follows similarly in spirit to Lemma 5.1.
P ∈
For completeness, we explicitly state this result below in Proposition 5.1 and prove it in Ap-
pendix A.8. In addition, we have also included a concrete example in Figure 9, where we
demonstrate how a three-dimensional Pareto front surface can be partitioned into a collection
of two-dimensional slices.
Proposition 5.1 (Projected Pareto front surface) For any Pareto front surface A∗ Y∗,
∈ η
any non-empty set of P > 0 indices I [M], any vector v M−P, the corresponding projected
⊂ ∈ V+
Pareto front surface (27) is a P-dimensional Pareto front surface with the reference vector
PI(η)
∈
RP, that is PI,v[A∗]
∈
Y∗ PI(η).
Remark 5.1 (Non-constant fixed vector) The projected Pareto front surface [A∗]
I,v
P ∈
Y∗ only considers the values indexed by the set I. It ignores the values at the other com-
PI(η)
ponents [M] I. The values at these components were a constant for the spherical Pareto
\
front surface M−1, but there are not necessarily a constant for a general Pareto front surface
S+
A∗ Y∗. Specifically, the projected vector at these indices are given by
∈ η
P[M]\I(η)+ℓ η,ϕI(v,λ)[A∗]v
∈
RM−P
for any λ P−1. Note that this vector is constant if and only if the projected lengths
∈ S+
ℓ [A∗] are constant for all λ P−1. If this is not the case, then one must also be
η,ϕI(v,λ) ∈ S+
aware of this feature when examining the slices.
21Figure 10: An illustration of the dashboard application that we made in order to navigate the space of one-
dimensional projections of a Pareto front surface (Section 5.1.2).
5.1.2 One-dimensional projection
The polar parametrisation result gives us a way to navigate a Pareto front surface by simply
navigating the space of unit vectors λ M−1. We made a simple dashboard application to
∈ S+
illustrate how such a navigation can be done in practice. We give a snapshot of this application
in Figure 10. Below we describe the main concept of this application.
Weights. We can navigate the space of unit vectors by moving M different sliders. Each
slider is associated with a weight lying in the open unit interval: w(m) (0,1) for the objectives
∈
m = 1,...,M. The positive unit vector can then be constructed by normalising the weight
vector appropriately: λ = w/ w M−1.
|| ||L2 ∈ S+
Normalisation. Ideally, we want the weight vector w (0,1)M to denote the relative impor-
∈
tanceofeachobjective. Forthisreason, weapplyanormalisationtransformationτ : RM RM
→
to the objective vectors: τ(y) = (y l)/(u l) for any vector y RM, where l,u RM denote
− − ∈ ∈
the estimates for the lower and upper bound of the objective vectors, respectively. In addition,
we also use these bound estimates to set the reference vector6: η = l 0.2(u l) RM. As
− − ∈
described in Remark 3.3, to reconstruct the Pareto front surface, we just have to invert this
affine transformation, namely we set
y η∗
,λ
= τ−1(τ(η)+ℓ τ(η),λ[τ(A∗)]λ) = η +ℓ η,r[A∗]r
∈
RM
for all λ M−1, where A∗ Y∗ is the Pareto front surface of interest and r M−1 denotes
∈ S+ ∈ η ∈ S+
theupdatedpositiveunitvectorwithr(m) (u(m) l(m))λ(m) forobjectivesm = 1,...,M.
∝ −
Parallel coordinates. To visualise the one-dimensional projections of the Pareto front sur-
face, we propose using a parallel coordinates plot, which is a common visualisation tool for
high-dimensional Pareto fronts [Tuˇsar and Filipiˇc, 2015]. The novelty of our work is that we
can also view uncertainty information along each projection as well. For example, in our ap-
plication we computed and visualised the empirical quantiles and the sample mean obtained
using a finite set of samples {A∗(ω n)
∈
Y∗ η}n=1,...,N. Notably, this overall application is very
lightweight to run because all of these statistics can computed and updated very quickly and
cheaply on the fly.
6Following Remark 2.1, we set the reference vector to be slightly worse than the estimated nadir. Naturally,
wecouldalsoallowthereferencevectortobesetdynamicallybyaddingsomemoreslidersintotheapplication.
22Figure 11: An illustration of the dashboard application that we made in order to navigate the space of two-
dimensional projections of a Pareto front surface (Section 5.1.3).
5.1.3 Two-dimensional projection
Proposition 5.1 gives us a way to visualise a high-dimensional Pareto front surface by construct-
ing a collection of low-dimensional slices that can be easily visualised instead. To illustrate how
one might do this in practice, we created a simple dashboard application that can be used to
navigate the set of two-dimensional slices of a Pareto front surface. We give a snapshot of this
application in Figure 11. Below we describe the main concept of this application.
Indices and weights. To select the two indices I = i ,i , we used two drop-down lists.
1 2
{ }
The corresponding fixed vector v M−2 is then determined by some weight sliders.
∈ V+
Normalisation. Similar to the one-dimensional application, we use an estimate of the ob-
jective ranges in order to normalise the objective values and set the reference vector.
Line plot. To visualise the two-dimensional projection of the Pareto front surface, we con-
sidered using a regular two-dimensional line plot. In our application, we included visuals on
the empirical quantiles and the sample mean of the projected two-dimensional Pareto front
surface. Naturally, other useful information could be included as well. For example, to address
Remark 5.1, it might be beneficial to also include a parallel coordinates plot of the fixed vector
as well—similar to the one used in Figure 10, but only for the fixed components.
5.1.4 Higher dimensional projections
In the previous sections, we have shown how it is possible to view a random Pareto front surface
usingone-dimensionalortwo-dimensionalslices. Theone-dimensionalslicesgaveusinformation
about the marginal performance for each objective. Whilst the two-dimensional slices gave us
information about the correlation between any two objectives. Similarly, in order to learn more
abouttheP-thorderinteractions, weneedtobeabletovisualisetheP-dimensionalslicesofthe
Pareto front surface. For example, one might consider adapting one of the methods surveyed
by Tuˇsar and Filipiˇc [2015] in order to accomplish this.
235.2 Uncertainty quantification
The statistical concepts described in Section 4 gives us a tangible and useful way to analyse
and quantify the uncertainty surrounding a distribution of random Pareto front surfaces. In
this section, we highlight some potential use cases for this machinery in a Bayesian experimen-
tal design setting. Formally, we consider the problem of identifying the Pareto front surface
associated with some bounded vector-valued black-box objective function g : X RM. We
→
suppose that we have executed an experimental design procedure and have observed a collec-
tion of potentially noisy data points T = (x 1,y 1),...,(x T,y T) X RM. We then adopt
D { } ⊂ ×
a standard Bayesian set-up in which we assume a probabilistic prior on the objective function
p(g) and a likelihood on the observations p( g). These variables can then be used to compute
T
D |
a posterior distribution over the objective function, p(g ) p(g)p( g), and consequently
T T
|D ∝ D |
over the Pareto front surface, Y∗ := int[ g(x) ], as summarised in the following flow
η,g Yη { }x∈X
diagram:
p(g),p( g) p(g ) p(Y∗ ).
DT
| −p−o−st−er−i→or
|DT
−P−ar−et−o−fr−o→nt
η,g|DT
We can then appeal to our earlier work in Section 4 in order to study this resulting Pareto
front surface distribution. Precisely, we can associate the stochastic process f : X Ω RM,
× →
described in Section 4, with the sampling distribution induced by the latest posterior model:
f( ,ω) p(g T), where ω Ω is distributed according to P. We can then analyse and eval-
· ∼ |D ∈
uate the posterior Pareto front surface distribution p(Y∗ ) by studying the corresponding
η,g|DT
polar parametrised stochastic process described in (10). To showcase how this overall routine
works in practice, we present an illustrative example in Section 5.2.1, where we visualise the
evolution of a Pareto front surface distribution during a run of Bayesian optimisation. After-
wards, in Section 5.2.2, we then demonstrate how this distributional information can be used
in conjunction with the visualisation techniques described in Section 5.1 in order to help us
make final decisions.
5.2.1 Bayesian optimisation
Bayesian optimisation is a popular strategy for black-box optimisation—see the book by Gar-
nett [2023] for a recent overview on this topic. Notably, this experimental design strategy takes
advantage of a probabilistic surrogate model in order to determine the best inputs to evalu-
ate. As described above, we can easily take advantage of this probabilistic model in order to
compute any Pareto front surface statistic of interest. Practically speaking, we envision that
these statistics might be valuable for an active decision maker who is interested in adapting the
Bayesian optimisation run for their own purposes. For instance, one might use these statistics
in conjunction with the visualisation ideas described in Section 5.1 in order to visualise and
better understand the evolution of the Pareto front surface distribution. Given this newfound
understanding, a keen decision maker might then actively refine and reprogram7 the acquisition
procedure in order to target a specific region of interest. Alternatively, earlier work by Binois
et al. [2015a] suggested that it might also be possible to use some Pareto front surface statistics,
such as the Vorob’ev deviation (22), as a basis for a stopping criteria.
In Figure 12, we illustrate an example of how the Pareto front surface distribution evolves over
time during one run of the Bayesian optimisation algorithm applied on the Gaussian mixture
model (GMM) [Daulton et al., 2022] and the DTLZ2 [Deb et al., 2002] benchmark problem.
For the probabilistic model, we adopted a standard Gaussian process prior [Rasmussen and
Williams, 2006] on our objective function and an independent Gaussian observation likelihood
7For instance, in the expected hypervolume acquisition criterion [Emmerich et al., 2006], one might use this
visual information in order to update the reference vector η RM.
∈
24Figure 12: An illustration of the change in the Pareto front surface distribution p(Y∗ ) when we perform
η,g|DT
Bayesian optimisation on the normalised GMM and DTLZ2 benchmarks. In the plots, we draw the estimated
mean front and highlight the region between the 5% and 95% estimated quantile fronts.
on the function observations. For the acquisition function, we used the expected hypervolume
improvement [Emmerich et al., 2006]. For illustrative convenience, we discretised the input
space in both of these benchmark problems to have X = 212 points. This latter simplification
| |
is only to ensure that we can compute the Pareto front surface of the objective function and
the model samples exactly using our polar parametrisation.
There are a number of key observations that we see from the plots in Figure 12. Firstly, we see
that the Pareto front surface distribution does indeed slowly converge to the actual Pareto front
surface when we observe more data points. This is clearly a desirable property and is some-
thing that is expected on these benchmark problems. Secondly, we see that the model Pareto
front surface always dominates the sample Pareto front surface in these examples. This makes
intuitive sense because the sample front considers only a finite number of Y = T points. In
T
| |
contrast, the model Pareto front surface considers the objective values over the entire input
space. Note however that this intuition only holds in our example because the observations
were not contaminated with any output noise. Otherwise, when there is observation noise in
our data, the sample Pareto front could easily dominate both the actual and model Pareto front
surface. For this reason, it is common for practitioners to rely on model-based estimates of the
Pareto front surface when the data is noisy. Thirdly, we see that the uncertainty of the Pareto
front surface does not necessarily decrease in a monotonic fashion as we add more points. This
feature naturally arises in our example because we did not fix the model hyperparameters in
our Gaussian process prior. Instead, we followed standard practice and updated these hyper-
parameters in an online manner by always maximising the latest log marginal likelihood. As
a result of this updating step, the overall uncertainty in the Pareto front surface occasionally
increased when we incorporate more points—for instance we see this feature when we transition
from T = 20 to T = 30 in the GMM problem.
5.2.2 Input decision
The Pareto front surface distribution gives us information on how the optimal set of objective
vectors are distributed. This valuable information can be used by the decision maker in order to
25guide them in their downstream decision making. For instance, this information can help them
decide which feasible objective vectors y∗ RM are the most desirable. On a practical level,
∈
this post-selection procedure is typically handled with the help of an interactive application
which allows the decision maker to visualise and navigate the Pareto front surface. Notably,
we envision that the ideas and tools that we introduced earlier in Section 5.1 could be used in
this interactive decision making procedure. Consequently, once the desirable vectors have been
elicited, the decision maker would often be interested in identifying the inputs x∗ X, which
∈
would most likely lead to these desirable vectors. To solve this problem, we propose adopting
a decision-theoretic approach, where we select the best input as the one which minimise some
M-dimensional loss function L : RM RM R. That is, we propose minimising the expected
× →
empirical loss
N
1 (cid:88)
x∗ argmin L(f(x,ω ),y∗),
n
∈ N
x∈X
n=1
where ω
n
Ω denotes some independent samples of the random parameter and y∗ RM
∈ ∈
denotes the target vector of interest. Clearly there are many potential loss functions that we
canchoose inpractice. Motivatedthe work sofar, weproposeusing alength-basedloss function
because it is interpretable and leads to the desired result.
Length-based loss function. A natural candidate for the loss function would be a frontier
loss function (8):
L(y,y∗) = [ int[ y ], int[ y∗ ]]
Dη,S Yη { } Yη { }
for any two vectors y,y∗ RM. A weakness of this loss function is that it can be very
∈
expensive to estimate and optimise in practice because it requires computing an integral over
the space of positive unit vectors M−1. Motivated by Lemma 3.1, we propose using a much
S+
cheaper simplification where we consider only computing the score along the optimal direction
(3):
L(y,y∗) = S(s η,λ∗(y∗)(y),s η,λ∗(y∗)(y∗)). (28)
η η
Geometrically, this loss function scores any vector y RM by computing its projected length
∈
alongthereferencelineL η,λ∗ η(y∗) = {η+tλ∗ η(y∗) : t
∈
R }andthencomparingitwiththedesired
projected length. We illustrate the efficacy of this strategy for a two-dimensional example in
Figure 13. We see clearly in this example that the random vectors f(x∗,ω) RM associated
∈
with the best inputs x∗ X are indeed distributed close to the corresponding target vectors
∈
y∗ RM.
∈
Remark 5.2 (Scale sensitivity) Any M-dimensional loss function will naturally be sensitive
to the scales of the different objectives. When using the length-based loss functions (28), we
recommend normalising each objective by its range in order for each objective to have a similar
influence on the projected length as illustrated in Section 5.1.2. Evidently we can also take
advantage of this sensitivity in order to inflate the importance of some objectives over others.
That is, in order to up-weight the importance of an objective, we can increase its range and
similarly in order to down-weight the importance of an objective, we can decrease its range.
5.3 Extreme value theory
Extreme value theory is a well-established branch of statistics which studies the distribution of
extremevalues—seeforinstanceEmbrechtsetal.[1997],Coles[2001]orBeirlantetal.[2004]for
a background on this topic. In this section, we showcase how our polar parametrisation result
can be used in order to generalise many existing ideas in this topic to the multivariate setting,
where the maximum is defined using the Pareto partial ordering. To the best of our knowledge,
26Figure 13: An illustration of the best input attained by minimising the length based loss function (28) with the
squared error scoring function S(x,y)=(x y)2.
−
the majority of existing work in multivariate extreme value theory has largely focussed on
the marginal maximisation setting [Barnett, 1976], where the maximum operation on a set of
vectorsisdefinedcomponent-wise. Muchlesswork, ifany, hasfocussedonthesettingwherethe
maximum is defined using the Pareto partial ordering. The clear benefit of this latter approach
is that it is much more flexible in practice because it can accommodate for the scenario where
the various component-wise maximums do not all take place simultaneously.
The most notable results from extreme value theory are the Fisher–Tippett–Gnedenko theo-
rem [Fisher and Tippett, 1928, Gnedenko, 1943] and the Balkema–de Haan–Pickands theorem
[Balkema and de Haan, 1974, Pickands, 1975]. The former result is concerned with the asymp-
totic distribution of the maximum order statistic associated with a collection of independent
and identically distributed univariate random variables. In words, it states that the distribu-
tion of the maximum, upon proper normalisation, can only converge in distribution to either
a Gumbel, Fr´echet or Weibull distribution. In contrast, the latter result is concerned with
the limiting distribution of the corresponding conditional excess distribution (Definition 5.2).
Conceptually, this result tells us that the distribution of the tail events, pass some threshold,
can be closely approximated by a generalised Pareto distribution. For reference purposes, we
repeat both of these results below in Theorem 5.1 and Theorem 5.2, respectively.
Definition 5.1 (Maximum domain of attraction) A cumulative distribution function F :
R R is in the maximum domain of attraction (MDA) of a cumulative distribution function
→
H : R R, denoted by F MDA(H), if there exist two sequences of real numbers a N > 0 and
→ ∈
b N R such that
∈
(cid:20) (cid:21)
max( Y ,...,Y ) b
lim P { 1 N } − N x = lim F(a Nx+b N)N = H(x)
N→∞ a N ≤ N→∞
for x R, where Y,Y 1,...,Y N R denotes a collection of independent and identically dis-
∈ ∈
tributed random samples from the distribution F.
Theorem 5.1 (Extreme value distribution) [Fisher and Tippett, 1928, Gnedenko, 1943]
If the cumulative distribution function F : R R is in the MDA of a non-degenerate distri-
bution function H : R R, that is F MD→ A(H), then H(x) = exp( (1 + ξ(x−µ))−1/ξ ) is
→ ∈ − σ +
a generalised extreme value distribution, where ξ R, µ R and σ > 0, denotes the shape,
∈ ∈
location and scale parameter, respectively.
Definition 5.2 (Conditional excess distribution) Let Y R denote a random variable
∈
with a distribution function F : R R. The corresponding conditional excess distribution
→
27function at some level u R is given by
∈
F(u+x) F(u)
F u(x) := P[Y u x Y > u] = − ,
− ≤ | 1 F(u)
−
for any x [0,y F u], where y F := sup x R : F(x) < 1 denotes the finite or infinite right
∈ − { ∈ }
endpoint of the distribution function F.
Theorem 5.2 (Generalised Pareto distribution) [Balkema and de Haan, 1974, Pickands,
1975] If the cumulative distribution function F : R R is in the MDA of a generalised
→
extreme value distribution, F MDA(H ), then there exists a positive, measurable function8
ξ,µ,σ
∈
β : R R>0 such that the following limit holds:
→
lim sup F (x) G (x) = 0
u ξ,β(u)
u→yF x∈[0,yF−u]| − |
where G (x) = 1 (1 + ξx)−1/ξ denotes the distribution function of a generalised Pareto
ξ,β − β +
distribution with shape parameter ξ R and rate parameter β > 0.
∈
Component-wise maximum. The existing work on multivariate extreme value theory has
largely focussed on the marginal maximisation setting where the maximum of a collection of N
independent and identically-distributed vectors Y 1,...,Y
N
RM are defined component-wise.
∈
The traditional goal of interest is then to study the multivariate generalisation of the MDA for
the resulting multivariate distribution function F : RM R,
→
F(x) = P[max( Y(1) ,...,Y(1) ) x(1),...,max( Y(M) ,...,Y(M) ) x(M)]
{ 1 N } ≤ { 1 N } ≤
for x RM—see Beirlant et al. [2004, Chapter 8] for an overview on this topic. The primary
∈
benefit of using this component-wise definition is that we can immediately apply both Theo-
rem 5.1 and Theorem 5.2 in order to determine the asymptotic properties of the corresponding
marginal distributions. The remaining challenge is then to identify the corresponding depen-
dence structure between the components. This latter problem turns out to be a major hurdle
in multivariate extreme value theory because this dependency structure cannot necessarily be
described using a finitely parametrised model. Notably, the estimation and analysis of this
dependency structure is still an area of active research—see Beirlant et al. [2004, Chapter 8]
for an in-depth discussion.
Pareto maximum. To the best of our knowledge, there does not exist much work regarding
the study of multivariate extreme value theory when we use the Pareto definition of the vector-
valued maximum (Definition 2.4). We attribute this lack of interest based on the fact that the
Pareto maximum is very challenging to work with in practice. One remarkable outcome of our
work is that it manages to connect the definition of the Pareto maximum with the definition of
the component-wise maximum. Specifically, our polar parametrisation result tells us that the
Pareto maximum can be completely characterised by its projected length process (11), which
is defined as an infinite collection of scalarised maximums. To put it more concretely, if we had
a collection of identically-distributed vectors Y 1,...,Y
N
RM, then the corresponding Pareto
∈
maximum is governed by the following projected length process
= max( s (Y ),...,s (Y ) ) 0 : λ M−1
Lη { { η,λ 1 η,λ N } ≥ ∈ S+ }
8Suppose that F(x)N H (x) for large N, then Coles [2001, Theorem 4.1] suggest that we can set
ξ,µ,σ
≈
β(u)=σ+ξ(u µ)>0 for sufficiently large u R.
− ∈
28α=0.75,β=(1,2),N=4 α=0.75,β=(1,2),N=16 α=0.75,β=(1,2),N=64 α=0.75,β=(1,2),N=256 α=1.5,β=(1,2),N=4 α=1.5,β=(1,2),N=16 α=1.5,β=(1,2),N=64 α=1.5,β=(1,2),N=256
01122 ..... 50505 1234 123456 12345678 0000111 ....... 2468024 000111 ...... 257025 505050 0112 .... 5050 0112 .... 5050
0.0 0 0 0 0.0 0.00 0.0 0.0
0 2 4 0 2 4 6 8 0.0 2.5 5.0 7.5 10.0 0 5 10 15 0.0 0.5 1.0 1.5 2.0 2.5 0 1 2 3 0 1 2 3 0 1 2 3 4
α=1.0,β=(1,2),N=4 α=1.0,β=(1,2),N=16 α=1.0,β=(1,2),N=64 α=1.0,β=(1,2),N=256 α=2.0,β=(1,2),N=4 α=2.0,β=(1,2),N=16 α=2.0,β=(1,2),N=64 α=2.0,β=(1,2),N=256
2.0 2.5 3.0 34 .. 50 11 .. 02 11 .. 24 11 .. 46 1.50
011 ... 505 0112 .... 5050 01122 ..... 50505 011223 ...... 505050 0000 .... 2468 00001 ..... 24680 000011 ...... 246802 00011 ..... 25702 50505
0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00
0 1 2 3 4 0 1 2 3 4 5 0 2 4 6 0 2 4 6 8 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0 1 2 3
y(1) y(1) y(1) y(1) y(1) y(1) y(1) y(1)
Referencevector Meanfront Limitingmeanfront Quantilefronts Limitingquantilefronts
Figure 14: An illustration of the Pareto front surface statistics associated with a collection of independent
Weibull distributions (Proposition 5.2). In the dotted lines, we plot the empirical estimates obtained using
N 4,16,64,256 random samples, whereas in the shaded lines we plot the predicted mean and quantiles given
∈{ }
by the limiting distribution. As a reference, we also plot the contour of the corresponding bivariate probability
density function as well.
for any reference vector η RM. As a consequence of this observation, many of the results that
∈
are known for the component-wise maximum can now be adapted to the Pareto maximum set-
ting. For example, we can use Theorem 5.1 to determine the possible asymptotic distributions
of the projected length process along each positive direction. Similarly, we can use Theorem 5.2
in order to approximate the distribution of the tails of the projected length process conditional
on the fact that it dominates some specified polar surface. That is, instead of setting just a
single threshold value of u R, we set the threshold to be a polar surface defined by some set
∈
= u 0 : λ M−1 .
U { λ ≥ ∈ S+ }
As a proof of concept for these general ideas, we present a simple example in Proposition 5.2,
where we study the asymptotic distribution of a Pareto front surface constructed using a collec-
tion of independent Weibull distributed vectors. Formally, this result states that, upon proper
normalisation, the projected length process of this Pareto front surface distribution converges
in distribution to a Gumbel distribution along each positive direction—the proof of this result
is presented in Appendix A.11. To illustrate this convergence, we present some visual two-
dimensional examples in Figure 14. In these plots, we fixed9 the rate parameter β(m) > 0 and
varied the choice of shape parameter α > 0. On the whole, we see that the empirical and
limiting distributions are already quite similar even at small sample sizes such as N = 64. Con-
ceptually, we see that varying the shape parameter α > 0 amounts to changing from a concave
Pareto front surface distribution when α (0,1) to a convex Pareto front surface distribution
∈
when α > 1.
Proposition 5.2 can also be used in conjunction with Theorem 5.2. In words, this result tells
us that the tails of the corresponding projected length process are approximately distributed
according to an exponential distribution. To demonstrate this property, we present an illustra-
tive example in Figure 15, where we study the conditional excess probabilities associated with
the M = 2 dimensional Pareto front surface distributions described in Figure 14. That is, we
plot the probabilities
F U(z) := P[s η,λ∗(z)(Y) u λ∗(z) s η,λ∗(z)(z) s η,λ∗(z)(Y) > u λ∗(z)] (29)
η − η ≤ η | η η
for any z D≻≻[ η ], where η = 0
M
is the reference vector, Y RM is the random vector
∈ { } ∈
of interest and = u 0 : λ M−1 is the projected length process which defines the
U { λ ≥ ∈ S+ }
9The rate parameter only controls the relative scaling of each objective and therefore it does not play a
major role in the assessment of the empirical result.
29
)2(y
)2(yFigure 15: A comparison between the empirical and theoretical approximation of the conditional excess prob-
abilities (29) associated with the two-dimensional Pareto front surface distributions described in Figure 14. To
set the threshold polar surface, we used an upper quantile of the original Pareto front surface distribution.
threshold polar surface. For convenience, in these examples, we set the threshold polar surface
to be one of upper quantiles. Nevertheless, we note that any polar surface, whose projected
length process is sufficiently long, can be used in practice. Overall we see that these empirical
estimates are indeed close to their theoretical approximate values.
Proposition 5.2 (Weibull distributed vectors) Let the reference vector be set to zero η =
0
M
RM and the vectors Y
n
RM be distributed according to M > 0 independent Weibull
∈ ∈
distributions,
Y(m) Weibull(α,β(m)),
n ∼
with the cumulative distribution function P[Y(m) x] = 1 exp( (β(m)x)α), where α > 0 and
≤ − −
β(m) > 0 denotes the corresponding the shape and rate parameter for m = 1,...,M, respectively.
Then, upon proper normalisation, the projected lengths of int[ Y ,...,Y ], along any positive
Yη { 1 N }
direction λ M−1, converges to a standard Gumbel distribution:
∈ S+
(cid:20) ℓ [ int[ Y ,...,Y ]] b (cid:21)
lim P η,λ Yη { 1 N } − η,λ,N x = exp( exp( x))
N→∞ a η,λ,N ≤ − −
where a = log(N)1/α−1/(αk ), b = log(N)1/α/k and k = ((cid:80)M (β(m)λ(m))α)1/α.
η,λ,N λ η,λ,N λ λ m=1
Remark 5.3 (Endpoint estimation) As identified by Binois et al. [2015b], the Pareto front
surface of an objective function g : X RM can also be viewed as the uppermost quantile of a
→
Pareto front surface distribution [Binois et al., 2015b, Theorem 2.1],
int[ g(x) ] = lim [ int[ g(x) ],α],
Yη { }x∈X α→1Qx∼p(x) Yη { }
where p(x) 0 denotes the density of any distribution over the input space whose essential sup-
≥
port covers the entire input space; for instance, a uniform distribution over X. As emphasised
by Binois et al. [2015b], the clear benefit of this result is that it transforms the vector-valued
optimisation problem into an endpoint estimation problem. This latter topic is naturally related
with extreme value theory [Hall, 1982, Loh, 1984, Girard et al., 2012].
30Nitrogendioxide: NO2 Ozone: O3 Sulphurdioxide: SO2
200 200 80
150 150 60
100 100 40
50 50 20
0 0 0
20132014201520162017201820192020202120222023 20132014201520162017201820192020202120222023 20132014201520162017201820192020202120222023
Figure 16: An illustration of the daily maximum registered pollutants at the North Kensington monitoring
station (UKA00253) over the period of 2013 to 2023.
2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023
NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2
NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2
O3 O3 O3 O3 O3 O3 O3 O3 O3 O3 O3
Figure 17: An illustration of the change in the domination probabilities for the pairwise Pareto front surfaces
associated with the air pollution data (Figure 16).
5.4 Air pollution example
The previous examples shown in this work have largely been focussed on static problems which
have no time dependency. Nevertheless, all of the ideas that we have presented so far can
also accommodate for dynamic problems, where there is indeed some time dependency. As an
illustrative example, we will now consider how these ideas can be used on a real-world time
series data set. Precisely, we study the air pollution data obtained at the North Kensington
(UKA00253) air monitoring station10 in west London.
Data cleaning. The air monitoring station at North Kensington measures many different
pollutants every day at different frequencies. We focus our attention on the measurements on
the three key pollutants in this area: Nitrogen dioxide (NO ), Ozone (O ) and Sulphur dioxide
2 3
(SO ). The Nitrogen dioxide and Ozone are measured at a rate of once every 60 minutes, whilst
2
the Sulphur dioxide is measured at a rate of once every 15 minutes. In our work, we use the
daily maximum of the measured observations as our statistic of interest. Sometimes there are
periods where some subset of the measurement devices are offline and therefore no readings
can be made. The daily maximums are then computed by using the partial observations that
were recorded. On the days where no observations were made at all, we omit the day entirely.
In Figure 16, we plot the resulting time series of these daily maximums over the period of 2013
to 2023.
Pareto front surface distributions. In this working example, we assume that the goal
of interest is to assess and better understand the change in the daily worst-case Pareto front
surface of pollution in the North Kensington area over the last decade. That is, we want to
see the change in the Pareto fronts of the daily maximum pollutants over the different years.
Practically speaking, we envision that this information might be useful for policy makers who
10WehavecuratedthisdatafromtheUK-AIRwebsite(https://uk-air.defra.gov.uk/),whichisapublicly
accessibledomainthatisranbytheDepartmentforEnvironment,Food&RuralAffairs(DEFRA)intheUnited
Kingdom.
31
3m/gµ
3O
2OS
2OS
3m/gµ 3m/gµ2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023
0.0000 0.0119 0.0247 0.0177 0.0071 0.0061 0.0309 0.0355 0.0256 0.0112 0.0137
0.0000 0.0172 0.0025 0.0081 0.0098 0.0218 0.0030 0.0116 0.0012 0.0040 0.0023
NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2
0.0000 0.0132 0.0573 0.0022 0.0234 0.0739 0.0244 0.0268 0.1391 0.0177 0.0235
0.0000 0.0080 0.0002 0.0542 0.0103 0.0001 0.0166 0.0383 0.0000 0.0061 0.0055
NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2 NO2
0.0000 0.0027 0.0482 0.0178 0.0031 0.0678 0.0053 0.0001 0.2845 0.0237 0.0042
0.0000 0.0429 0.0022 0.0161 0.0400 0.0156 0.0484 0.1150 0.0000 0.0210 0.0264
O3 O3 O3 O3 O3 O3 O3 O3 O3 O3 O3
Figure 18: An illustration of the signed yearly changes in the domination probabilities for the pairwise Pareto
fronts described in Figure 17. The average yearly negative and positive changes are depicted in the legends. The
total average yearly change can be computed by summing the absolute values of these signed changes.
are interested in assessing the effectiveness of new pollution reducing initiatives in London such
as the Ultra Low Emission Zone (ULEZ) or the lowering of the local speed limits. In Figure 17,
we take advantage of some of our Pareto front machinery in order to compute and plot the
corresponding domination probabilities for the different pairwise Pareto front surfaces. We
see clearly that there is a general positive trend after 2020, where the Pareto front has been
progressing downwards. This indicates a general reduction in the daily maximum pollution.
Evidently this time period also aligns with many events such as: the Coronovirus pandemic,
the introduction of ULEZ and the lowering of local speed limits. Cumulatively, all of these
effects seem to have had a noticeably positive impact on reducing the daily maximum detected
NitrogendioxideandSulphurdioxideinNorthKensington. Nevertheless,themaximumamount
of Ozone is still at a comparative level to the earlier years. To further illustrate the reduction of
these domination probabilities more explicitly, we plot and calculate the corresponding average
yearly signed changes in Figure 18. That is, we compare every year’s Pareto front surface
distribution with the previous year and see where the relative positive and negative changes
occur. On the whole, we see that there is indeed a quantifiable reduction happening over the
later years as these new initiatives were being introduced.
6 Conclusion
In this work we have studied the space of Pareto front surfaces. We presented a novel re-
sult which showed that any Pareto front surface can be explicitly parametrised using polar
coordinates. We then used this polar parametrisation result in order to establish many gen-
eral concepts over the space of Pareto front surfaces. Notably, we focussed our attention on
the stochastic setting where the Pareto front surface itself was random. We showed that this
random surface admits a stochastic process representation (10), based on our polar parametri-
sation, which makes it amenable to standard ideas from functional data analysis. In particular,
we used this stochastic representation in order to define and compute statistics for random
Pareto front surfaces. We then demonstrated the usefulness of these new probabilistic ideas
on some interesting applications such as: visualisation, uncertainty quantification and extreme
value theory. In the following paragraphs, we highlight some interesting directions for future
research.
Other interpolation schemes. Throughout this work, we have focussed our attention on
the interpolated Pareto front (Definition 2.4), which was defined using the weak Pareto partial
ordering. This type of interpolation is sensible because it is consistent with the weak Pareto
32
3O
2OS
2OSpartial ordering on sets (Definition 3.2): Yηint[A]
⪯
Yηint[B] if A
⊆
B
⊆
D≻≻[ {η }]. In practice,
one might also consider using alternate interpolation (or approximation) schemes such as the
ones based on Delaunay triangulation [Hartikainen et al., 2012], sandwiching [Bokrantz and
Forsgren, 2013] or hyperboxing [D¨achert and Teichert, 2020, Eichfelder and Warnow, 2022].
Notably these other schemes might be beneficial when we are interested in approximating a
Pareto front of a continuous set Y RM using a finite subset Yˆ Y. In this setting, the
ˆ ⊂ ⊆
interpolated weak Pareto front of Y would always give us a lower bound estimate of the actual
front. In contrast, a different interpolation scheme might be able to give us a more accurate
approximation of the target front, with respect to some frontier loss (Section 3.3). Naturally,
many of the results which we have shown here continues to hold even when we change the
interpolation scheme. The only notable difference is that the corresponding projected length
function ℓ , which arises in the polar parametrisation (Theorem 3.1), might no longer be
η,λ
written in terms of the length scalarisation function (2). Instead, the new projected length
function would likely depend on a different scalarisation function—one which accurately caters
for the interpolation scheme at hand. We leave the analysis of these other interpolation schemes
for future research.
Constrained regression. Our polar parametrisation result implies that any Pareto front
surface is completely characterised by its projected length function. Whilst our result in Propo-
sition 3.1 gives two necessary and sufficient conditions that a length function must satisfy in
order for it to induce a valid Pareto front surface. An interesting direction for future work
would be focussed on learning this projected length function directly from observational data.
Note that the first constraint (C1), positivity, can be easily handled by just considering the
logarithm of the length function instead. Whilst the second constraint (C2), the maximum
ratio condition, is much less trivial to work with. Specifically, by taking the logarithm and
rearranging the terms, we see that this latter condition is very similar to a Lipschitz type
constraint, which is known to be challenging to work with.
Advancing the theory. Many of the ideas that we have established in this work can be
generalised and adapted even further. For instance, we only exploited a few basic ideas from
functional data analysis in order to define the Pareto front surface statistics in Section 4.
Other ideas such as functional regression and functional principal component analysis were not
explored in any detail and could be investigated in the future. Similarly, in Section 5.3, we gave
a proof of concept on how one could extend well-known results from extreme value theory to
work in the multivariate setting under the Pareto maximum. A practical direction for future
work could focus on pushing these preliminary ideas even further.
Acknowledgements
Ben Tu was supported by the EPSRC StatML CDT programme EP/S023151/1 and BASF
SE, Ludwigshafen am Rhein. Nikolas Kantas was partially funded by JPMorgan Chase & Co.
under J.P. Morgan A.I. Faculty Research Awards 2021.
33A Proofs
We now prove all the results discussed in the main paper. Some of these proofs rely on the
monotonicity of the length scalarisation function (2), which we define below. The proof of these
monotonicity results follow immediately from the definition of the length scalarisation function
for a fixed reference vector η RM.
∈
Lemma A.1 (Monotonically increasing) The length scalarisation function is a monotoni-
cally increasing function over the whole of RM, that is y y′ = s η,λ(y) s η,λ(y′) for all
⪰ ⇒ ≥
vectors y,y′ RM and any positive unit vector λ M−1.
∈ ∈ S+
Lemma A.2 (Strongly monotonically increasing) The length scalarisation function is a
strongly monotonically increasing function over the truncated space, that is y y′ =
≻≻ ⇒
s η,λ(y) > s η,λ(y′) for all vectors y,y′
∈
D≻≻[ {η }] and any positive unit vector λ
∈
S+M−1.
A.1 Proof of Theorem 3.1
Consider a bounded set of vectors A RM and reference vector η RM which admits a
⊂ ∈
non-empty Pareto front surface int[A] = . Let
Yη ̸ ∅
A∗ = {η +sups η,λ(y)λ
∈
RM : λ
∈
S+M−1
} ∈
Lη.
y∈A
We will first show that this polar surface contains the Pareto front surface: int[A] A∗. To
Yη ⊆
prove this result, we take advantage of the well-known fact that every Pareto optimal point
y∗ int[A] can be obtained by solving the following optimisation problem [Miettinen, 1998,
∈ Yη
Part 2, Theorem 3.4.5]:
y(m) η(m)
y∗ argmaxsLen (y) = argmax min − . (30)
∈ y∈Yint[A] (η,λ∗ η(y∗)) y∈Yint[A] m=1,...,M (y∗)(m) η(m)
η η −
From the monotonicity of the length scalarisation function (Lemma A.1), we have that
max s (y) = sup s (y) = sups (y)
η,λ η,λ η,λ
y∈Y ηint[A] y∈D⪯,η[A] y∈A
for any λ M−1. Combining this result with (30), we find that
∈ S+
y∗ = Tη−1((λ∗ η(y∗),s η,λ∗ η(y∗)(y∗))) = η +sups η,λ∗ η(y∗)(y)λ∗ η(y∗)
∈
A∗
y∈A
for all y∗ int[A], which implies the desired result: int[A] A∗. We will now show by a
∈ Yη Yη ⊆
simple proof by contradiction that the other containment also holds: int[A] A∗. Suppose
Yη ⊇
for a contradiction that there exists a vector a∗ A∗ which is not Pareto optimal, that is
∈
a∗ / int[A]. Then by the definition of weak Pareto optimality (Definition 2.4), there must
∈ Yη
exist an element y∗ A which strongly dominates it. As the length scalarisation function
∈
is strongly monotonically increasing over the truncated space (Lemma A.2), this implies that
s (y∗) > s (a∗) for all λ M−1. This is a contradiction because a∗ A∗ and therefore it
η,λ η,λ ∈ S+ ∈
must achieve the largest projected length for at least one positive direction λ′ M−1, that is
∈ S+
s (y∗) s (a∗) for all y∗ A.
η,λ′ η,λ′
≤ ∈
■
34A.2 Proof of Lemma 3.1
Consider any Pareto front surface A∗
∈
Y∗
η
and any vector y
∈
D≻≻[ {η }]. We will start by
proving the first statement. For the right implication, suppose that y D⪯,η[A∗]. Let a A∗
∈ ∈
be any vector such that a y. By the monotonicity of the length scalarisation function
⪰
(Lemma A.1), we have that s (y) s (a) ℓ [A∗] for all positive directions λ M−1
η,λ ≤ η,λ ≤ η,λ ∈ S+
and especially for λ∗(y) M−1. Now we will prove the left implication. Suppose that
η ∈ S+
s η,λ∗(y)(y) ℓ η,λ∗(y)[A∗], then this implies the existence of a vector a A∗ such that
η ≤ η ∈
ℓ η,λ∗(y)[A∗] s η,λ∗(y)(a) a(m) η(m)
1 η = η = min − ,
≤ s η,λ∗(y)(y) s η,λ∗(y)(y) m=1,...,M y(m) η(m)
η η −
which implies a y and therefore y D⪯,η[A∗]. The proof of the second statement follows
⪰ ∈
in the same way to the proof of the first statement if we replace some instances of and
⪯ ≤
with and <, respectively. The third and fourth statements are just the the contrapositive
≺≺
of the first and second statements, respectively, because D≻≻,η[A∗] = D≻≻[ η ] D⪯,η[A∗] and
{ } \
D⪰,η[A∗] = D≻≻[ η ] D≺≺,η[A∗].
{ } \
■
A.3 Proof of Proposition 3.1
Consider a polar surface A Lη. We will first prove the sufficiency of these two conditions.
∈
Suppose that C1 and C2 holds, then we will show that int[A] is non-empty and int[A] = A.
Yη Yη
Firstly,weseethatC1ensuresthattheset int[A]isnon-empty. Whilstsecondly,weseethatC2
Yη
implies that the set A is indeed a Pareto front surface because we cannot find any two vectors in
this set such that one strongly dominates the other. In particular, assume for a contradiction
that there exists positive directions λ,υ M−1 such that η + ℓ [A]λ η + ℓ [A]υ.
∈ S+ η,λ ≺≺ η,υ
This implies that η(m) +ℓ [A]λ(m) < η(m) +ℓ [A]υ(m), for all objectives m = 1,...,M and
η,λ η,υ
therefore
ℓ [A]υ(m)
η,υ
> 1
ℓ [A]λ(m)
η,λ
for all objectives m = 1,...,M. The above expression contradicts C2 because
ℓ [A]λ(m)
η,λ
max 1,
m=1,...,M ℓ η,υ[A]υ(m) ≥
which implies that there exist at least one objective m where the previous statement fails to
hold. We will now prove the necessity of these two conditions. Assume that A is a Pareto front
surface. Firstly, C1 has to hold because the Pareto front surface is a subset of the truncated
space D≻≻[ η ]. To see that C2 has to hold, we reuse the arguments in the proof of Lemma 3.1.
{ }
Consider the vectors y = η+ℓ [A]λ A and y = η+ℓ [A]υ A for any two positive
η,λ η,λ η,υ η,υ
∈ ∈
directions λ,υ M−1. As A is Pareto front surface, neither one of these vectors strongly
∈ S+
dominate the other. This implies that y η does not strongly dominate y η, which
η,υ η,λ
− −
implies C2:
y(m) η(m) s (y ) ℓ [A] λ(m)
1 max η,λ − = η,λ η,λ = η,λ max
≤ m=1,...,M y η(m ,υ) η(m) s η,λ(y η,υ) ℓ η,υ[A] m=1,...,M υ(m)
−
for any λ,υ M−1.
∈ S+
■
35A.4 Proof of Proposition 3.2
Consider any reference vector η RM and any strictly monotonically increasing transformation
∈
τ : R≥0 R. Let A,B D≻≻[ η ] denote two finite sets lying in the truncated space with A
→ ⊂ { } ≻
B. Wewillshowthatthelength-basedutility(6)satisfiesthestrictParetocompliancyproperty:
U [A] > U [B]. For notational convenience, we begin by defining the set function
η,τ η,τ
S [Y] := maxτ(s (y)).
λ,τ η,λ
y∈Y
By the monotonicity of the transformation τ, we have that A B = S [A] S [B] for
λ,τ λ,τ
≻ ⇒ ≥
all λ M−1. By the strict monotonicity of τ and τHV, we have that
∈ S+
S [A] > S [B] S [A] > S [B],
λ,τHV λ,τHV λ,τ λ,τ
⇐⇒ (31)
S [A] = S [B] S [A] = S [B].
λ,τHV λ,τHV λ,τ λ,τ
⇐⇒
As the hypervolume indicator (7) is strictly Pareto compliant [Zitzler et al., 2003], we have that
UHV[A] > UHV[B], which implies that
η η
0 < U ηHV[A] −U ηHV[B] = E
λ∼Uniform(S
+M−1)[1 [λ
∈
Λ](S λ,τHV[A] −S λ,τHV[B])],
where Λ M−1 denotes the measurable subset of positive unit vectors where S [A] >
⊆ S+ λ,τHV
S [B] and ν[Λ] > 0. Combining this result with the implications in (31), we obtain the
λ,τHV
desired result: U [A] > U [B].
η,τ η,τ
■
A.5 Proof of Proposition 4.1
Equipped with Assumptions 4.1 and 4.2, we will show that the expectation in (12) satisfies
the two conditions in Proposition 3.1 and therefore is a valid Pareto front surface. Firstly, C1
is satisfied because the lengths are assumed to be positive almost surely, which implies that
Eω[ℓ η,λ[Y η∗ ,f(ω)]] > 0 for all λ
∈
RM. Secondly, C2 is satisfied because Y η∗ ,f(ω)
∈
Lη is a Pareto
front surface almost surely and therefore it satisfies C2 almost surely,
λ(m)
ℓ [Y∗ (ω)] max ℓ [Y∗ (ω)],
η,λ η,f m=1,...,M υ(m) ≥ η,υ η,f
which implies that the expectation also satisfies C2,
λ(m)
Eω[ℓ η,λ[Y η∗ ,f(ω)]] m=m 1,a ..x
.,M υ(m) ≥
Eω[ℓ η,υ[Y η∗ ,f(ω)]],
for any two positive directions λ,υ M−1.
∈ S+
■
A.6 Proof of Proposition 4.2
To prove that the α-quantile (15) is a valid Pareto front surface under the given assumptions,
we can show that it satisfies the two conditions in Proposition 3.1. This exercise is simply a
repeat of the arguments in Appendix A.5, with every instance of the expectation Eω[ ] being
·
replaced with the α-quantile [ ,α] instead.
ω
Q ·
■
36A.7 Proof of Lemma 5.1
Consider a non-empty set of indices I [M] with I = P and a vector v M−P. We have
⊂ | | ∈ V+
the following equivalence:
PI,v[ S+M−1] = {PI(λ)
∈
RP : λ
∈
S+M−1 and P[M]\I(λ) = v
}
(cid:113)
= 1 v 2 λ RP : λ P−1
{ −|| ||L2 ∈ ∈ S+ }
(cid:113)
= (1 v 2 ) P−1.
−|| ||L2 ⊙S+
The fact that this final set is a P-dimensional Pareto front surface with the zero reference vector
(cid:112)
follows from Example 3.3 and the fact that P−1 Y∗ and (1 v 2 ) > 0.
S+ ∈ 0P −|| ||L2
■
A.8 Proof of Proposition 5.1
Consider a Pareto front surface A∗ Y∗, any non-empty set of indices I [M] with I = P,
∈ η ⊂ | |
any vector v
∈
V+M−P, we will show that the projected Pareto front surface PI,v[A∗]
∈
LPI(η)
is a P-dimensional Pareto front surface with the reference vector I(η) RP. To accomplish
P ∈
this, we will show that this set satisfies the two conditions in Proposition 3.1. Firstly, C1 is
(cid:112)
satisfied because the projected lengths of A∗ are positive and 1 v 2 > 0. Secondly, to
−|| ||L2
see that C2 is satisfied, we begin by noting that A∗ satisfies C2 because it is a valid Pareto
front surface, which implies
ℓ [A∗] ϕ(m) (v,λ)
η,ϕI(v,λ) max I 1 (32)
ℓ η,ϕI(v,υ)[A∗] m=1,...,M ϕ( Im) (v,υ) ≥
for any λ,υ P−1. As the vectors on the indices of [M] I are fixed to be v M−P and
∈ S+ \ ∈ V+
P−1 is a Pareto front surface, we see that the maximum ratio on the set of indices [M] is
S+
achieved on the subset I:
(m) (m)
ϕ (v,λ) ϕ (v,λ)
max I = max I . (33)
m=1,...,M ϕ(m) (v,υ) m∈I ϕ(m) (v,υ)
I I
By substituting (33) into (32) and using the definition of the reconstruction function in (26),
we see that C2 does indeed hold for the projected Pareto front surface:
(cid:112)
(1 v 2 )ℓ [A∗] λ(m)
−|| ||L2 η,ϕI(v,λ) max 1
(cid:112)
(1 v 2 )ℓ [A∗] m=1,...,P υ(m) ≥
−|| ||L2 η,ϕI(v,υ)
for any λ,υ P−1.
∈ S+
■
A.9 Proof of Theorem 5.1
See for instance Leadbetter et al. [1983, Theorem 1.4.2].
■
A.10 Proof of Theorem 5.2
See for instance Leadbetter et al. [1983, Theorem 1.6.2].
■
37A.11 Proof of Proposition 5.2
As described in Section 5, the projected lengths along any positive direction λ M−1 is given
∈ S+
by the maximum of the length scalarised values
ℓ [ int[ Y ,...,Y ]] = max( s (Y ),...,s (Y ) ). (34)
η,λ Yη { 1 N } { η,λ 1 η,λ N }
As a Weibull distributed random variable is non-negative and η = 0
M
RM, we have that
∈
s (Y ) = min Y(m)/λ(m) for n = 1,...,N. By a standard calculation, we see that the
η,λ n m=1,...,M
Weibull distribution is closed under scaling,
(cid:20) Y(m) (cid:21)
P x = 1 exp( (β(m)λ(m)x)α),
λ(m) ≤ − −
which implies Y(m)/λ(m) Weibull(α,β(m)λ(m)). Similarly, we can show that the minimum of a
∼
collectionofindependentWeibulldistributedrandomvariablesisalsoWeibulldistributed:
(cid:20) Y(m) (cid:21) (cid:89)M (cid:18) (cid:20) Y(m) (cid:21)(cid:19) (cid:18) (cid:88)M (cid:19)
P min x = 1 1 P x = 1 exp (β(m)λ(m))αxα ,
m=1,...,M λ(m) ≤ − − λ(m) ≤ − −
m=1 m=1
whichimpliesthats (Y ) Weibull(α,((cid:80)M (β(m)λ(m))α)1/α)forn = 1,...,N. Thisimplies
η,λ n ∼ m=1
that the distribution of the projected lengths (34) is equivalent to the distribution of the
maximum of a collection of independent and identically distributed Weibull distributions. The
final result is then obtained by repeating a standard calculation—confer with Embrechts et al.
[1997, Table 3.4.4].
■
38B References
August A. Balkema and Laurens de Haan. Residual life time at great age. The Annals of
Probability, 2(5):792–804, October 1974. Cited on pages 27 and 28.
Vic Barnett. The ordering of multivariate data. Journal of the Royal Statistical Society. Series
A (General), 139(3):318–355, 1976. Cited on page 27.
JanBeirlant, YuriGoegebeur, JohanSegers, JozefL.Teugels, DanielDeWaal, andChrisFerro.
Statistics of Extremes: Theory and Applications. Wiley, August 2004. Cited on pages 26
and 28.
Micka¨el Binois, David Ginsbourger, and Olivier Roustant. Quantifying uncertainty on Pareto
fronts with Gaussian process conditional simulations. European Journal of Operational Re-
search, 243(2):386–394, June 2015a. Cited on pages 16, 17, 18, and 24.
Micka¨el Binois, Didier Rulli`ere, and Olivier Roustant. On the estimation of Pareto fronts from
the point of view of copula theory. Information Sciences, 324:270–285, December 2015b.
Cited on page 30.
RasmusBokrantzandAndersForsgren. AnalgorithmforapproximatingconvexParetosurfaces
basedondualtechniques. INFORMS Journal on Computing, 25(2):377–393, May2013. Cited
on page 33.
Stuart Coles. An Introduction to Statistical Modeling of Extreme Values. Springer Series in
Statistics. Springer, 2001. Cited on pages 26 and 28.
Kerstin Da¨chert and Katrin Teichert. An improved hyperboxing algorithm for calculating a
Pareto front representation. arXiv:2003.14249, March 2020. Cited on page 33.
Samuel Daulton, Sait Cakmak, Maximilian Balandat, Michael A. Osborne, Enlu Zhou, and
Eytan Bakshy. Robust multi-objective Bayesian optimization under input noise. In Inter-
national Conference on Machine Learning, pages 4831–4866. PMLR, June 2022. Cited on
page 24.
Kalyanmoy Deb and Himanshu Jain. Handling many-objective problems using an improved
NSGA-II procedure. In 2012 IEEE Congress on Evolutionary Computation, pages 1–8, June
2012. Cited on page 5.
Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable multi-objective
optimization test problems. In Proceedings of the 2002 Congress on Evolutionary Computa-
tion. CEC’02 (Cat. No.02TH8600), volume 1, pages 825–830, May 2002. Cited on page 24.
Jingda Deng and Qingfu Zhang. Approximating hypervolume and hypervolume contributions
using polar coordinate. IEEE Transactions on Evolutionary Computation, 23(5):913–918,
October 2019. Cited on page 10.
Gabriele Eichfelder and Leo Warnow. An approximation algorithm for multi-objective opti-
mization problems using a box-coverage. Journal of Global Optimization, 83(2):329–357,
June 2022. Cited on page 33.
Paul Embrechts, Claudia Klu¨ppelberg, and Thomas Mikosch. Modelling Extremal Events.
Springer Berlin Heidelberg, 1997. Cited on pages 26 and 38.
39Michael T. M. Emmerich, Kyriakos C. Giannakoglou, and Boris Naujoks. Single- and multi-
objective evolutionary optimization assisted by Gaussian random field metamodels. IEEE
Transactions on Evolutionary Computation, 10(4):421–439, August 2006. Cited on pages 24
and 25.
Thomas S. Ferguson. Probability and Mathematical Statistics: A Series of Monographs and
Textbooks. Academic Press, January 1967. Cited on page 19.
Ronald A. Fisher and Leonard H. C. Tippett. Limiting forms of the frequency distribution
of the largest or smallest member of a sample. Mathematical Proceedings of the Cambridge
Philosophical Society, 24(2):180–190, April 1928. Cited on page 27.
Roman Garnett. Bayesian Optimization. Cambridge University Press, January 2023. Cited on
page 24.
St´ephane Girard, Armelle Guillou, and Gilles Stupfler. Estimating an endpoint with high-order
moments. TEST, 21(4):697–729, December 2012. Cited on page 30.
Boris Gnedenko. Sur la distribution limite du terme maximum d’une serie aleatoire. Annals of
Mathematics, 44(3):423–453, 1943. Cited on page 27.
Tilmann Gneiting. Making and evaluating point forecasts. Journal of the American Statistical
Association, 106(494):746–762, June 2011. Cited on pages 11 and 18.
Viviane Grunert da Fonseca and Carlos M Fonseca. The attainment-function approach to
stochastic multiobjective optimizer assessment and comparison. Experimental Methods for
the Analysis of Optimization Algorithms, pages 103–130, 2010. Cited on pages 16 and 17.
Peter Hall. On estimating the endpoint of a distribution. The Annals of Statistics, 10(2):
556–568, 1982. Cited on page 30.
Michael P. Hansen and Andrzej Jaszkiewicz. Evaluating the quality of approximations to the
non-dominated set. Technical Report, Institute of Mathematical Modeling, 1998. Cited on
pages 9 and 11.
Markus Hartikainen, Kaisa Miettinen, and Margaret M. Wiecek. PAINT: Pareto front inter-
polation for nonlinear multiobjective optimization. Computational Optimization and Appli-
cations, 52(3):845–867, July 2012. Cited on page 33.
Hisao Ishibuchi, Noritaka Tsukamoto, Yuji Sakane, and Yusuke Nojima. Hypervolume approxi-
mationusingachievementscalarizingfunctionsforevolutionarymany-objectiveoptimization.
In 2009 IEEE Congress on Evolutionary Computation, pages 530–537, May 2009. Cited on
page 5.
Hisao Ishibuchi, Ryo Imada, Yu Setoguchi, and Yusuke Nojima. Reference point specification
in hypervolume calculation for fair comparison and efficient search. In Proceedings of the
Genetic and Evolutionary Computation Conference, GECCO ’17, pages 585–592. Association
for Computing Machinery, July 2017. Cited on page 4.
Malcolm R. Leadbetter, Georg Lindgren, and Holger Rootz´en. Extremes and Related Properties
of Random Sequences and Processes. Springer Series in Statistics. Springer New York, 1983.
Cited on page 37.
Wei-Yin Loh. Estimating an endpoint of a distribution with resampling methods. The Annals
of Statistics, 12(4):1543–1550, December 1984. Cited on page 30.
40Kaisa Miettinen. Nonlinear Multiobjective Optimization. International Series in Operations
Research & Management Science. Springer US, 1998. Cited on pages 7 and 34.
Ilya Molchanov. Theory of Random Sets. Probability and Its Applications. Springer-Verlag,
2005. Cited on pages 16 and 17.
James Pickands. Statistical inference using extreme order statistics. The Annals of Statistics,
3(1):119–131, January 1975. Cited on pages 27 and 28.
Howard Raiffa and Robert Schlaifer. Applied Statistical Decision Theory. MIT Press, May
1968. Cited on page 19.
Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning.
Adaptive Computation and Machine Learning. MIT Press, 2006. Cited on page 24.
Donald B. Rubin. The Bayesian bootstrap. The Annals of Statistics, 9(1):130–134, January
1981. Cited on page 14.
Leonard J. Savage. Elicitation of personal probabilities and expectations. Journal of the
American Statistical Association, 66(336):783–801, 1971. Cited on page 19.
Ke Shang, Hisao Ishibuchi, Min-Ling Zhang, and Yiping Liu. A new R2 indicator for better
hypervolume approximation. In Proceedings of the Genetic and Evolutionary Computation
Conference, GECCO ’18, pages 745–752. Association for Computing Machinery, July 2018.
Cited on page 10.
Ben Tu, Nikolas Kantas, Robert M. Lee, and Behrang Shafei. Multi-objective optimisation via
the R2 utilities. arXiv:2305.11774, May 2024. Cited on pages 9, 10, and 11.
Tea Tuˇsar and Bogdan Filipiˇc. Visualization of Pareto front approximations in evolutionary
multiobjectiveoptimization: Acriticalreviewandtheprosectionmethod. IEEE Transactions
on Evolutionary Computation, 19(2):225–245, April 2015. Cited on pages 20, 22, and 23.
Richard Zhang and Daniel Golovin. Random hypervolume scalarizations for provable multi-
objective black box optimization. In International Conference on Machine Learning, vol-
ume 37, pages 11096–11105. PMLR, November 2020. Cited on page 10.
Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithms
- A comparative case study. In Parallel Problem Solving from Nature, Lecture Notes in
Computer Science, pages 292–301. Springer, 1998. Cited on page 10.
Eckart Zitzler, Lothar Thiele, Marco Laumanns, Carlos M. Fonseca, and Viviane Grunert
da Fonseca. Performance assessment of multiobjective optimizers: An analysis and re-
view. IEEE Transactions on Evolutionary Computation, 7(2):117–132, April 2003. Cited
on pages 10 and 36.
41