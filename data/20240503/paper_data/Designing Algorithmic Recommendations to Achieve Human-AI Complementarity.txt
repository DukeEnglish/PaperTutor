Designing Algorithmic Recommendations
to Achieve Human–AI Complementarity
Bryce McLaughlin Jann Spiess
Stanford University
First public version: January 2024
This version: May 2024
Abstract
Algorithms frequently assist, rather than replace, human decision-makers. However, the de-
sign and analysis of algorithms often focus on predicting outcomes and do not explicitly model
their effect on human decisions. This discrepancy between the design and role of algorithmic
assistants becomes of particular concern in light of empirical evidence that suggests that algo-
rithmic assistants again and again fail to improve human decisions. In this article, we formalize
the design of recommendation algorithms that assist human decision-makers without making
restrictive ex-ante assumptions about how recommendations affect decisions. We formulate an
algorithmic-design problem that leverages the potential-outcomes framework from causal infer-
ence to model the effect of recommendations on a human decision-maker’s binary treatment
choice. Within this model, we introduce a monotonicity assumption that leads to an intuitive
classification of human responses to the algorithm. Under this monotonicity assumption, we
can express the human’s response to algorithmic recommendations in terms of their compliance
with the algorithm and the decision they would take if the algorithm sends no recommenda-
tion. We showcase the utility of our framework using an online experiment that simulates a
hiring task. We argue that our approach explains the relative performance of different recom-
mendation algorithms in the experiment, and can help design solutions that realize human–AI
complementarity.
BryceMcLaughlin(brycem@stanford.edu)andJannSpiess(jspiess@stanford.edu),GraduateSchoolofBusiness,
Stanford University. We thank Sanmi Koyejo, Jason Weitze, Emma Rockall, Adrienne Propp, Carl Meyer, Omer
Shiran-Cohen,andaudiencemembersatthe2023INFORMSAnnualMeetingandatStanfordforhelpfulcomments
and suggestions.
1
4202
yaM
2
]CH.sc[
1v48410.5042:viXra1 Introduction
Algorithms often assist, rather than replace, human decision-makers. For example, in the United
States, federal pretrial service officers set bail for defendants after observing a Pretrial Risk Assess-
ment (PTRA), rather than algorithmically implementing a bail decision in response to the PTRA.
Similarly, doctors may be assisted by clinical decision support systems throughout the patient
care process (Sutton et al., 2020) and managers may be assisted by pre-employment assessments
throughout the hiring process (Raghavan et al., 2020). In all of these cases, the human maintains
the final decision authority. However, the design and analysis of these algorithms often focus on
the prediction of outcomes as if algorithmic recommendations were implemented directly, rather
than how these recommendations affect human decisions.
The narrow focus of existing approaches on predicting outcomes rather than optimizing for
human–AI complementarity becomes of particular concern in light of a growing body of evidence
that shows that human decision-makers with access to algorithmic recommendations often make
worse decisions than if these recommendations had been implemented directly, and in some cases
may even take worse decisions than they would have taken without access to the algorithm. In a
recent review, Hemmer et al. (2021) identifies 53 articles that report (i) unassisted human perfor-
mance, (ii) algorithm performance, and (iii) algorithm-assisted human performance on a decision
task. Only in 16 of these does the algorithm-assisted human decision-maker perform best. In
many of these studies, decision-makers exhibit noisy and biased adoption of the algorithm’s recom-
mendations, which counteracts the gains associated with the additional information the algorithm
provides. This suggests the necessity to incorporate decision-makers’ adoption decisions explicitly
when designing the algorithm.
This article formalizes the design of recommendation algorithms that assist humans in mak-
ing binary treatment choices via an optimization problem that leverages the potential outcomes
framework from causal inference. For the analysis of recommendation algorithms, we employ the
notion of “potential outcomes” and make connections to the analysis of instrumental variables in
causal inference with imperfect compliance. This view suggests that an optimal recommendation
algorithm should take into account the final outcome of interest to which the decision is related
as well as the decision-maker’s choice of when to comply with a recommendation. We then show
that this framework is helpful in designing better recommendation algorithms and making sense of
their empirical performance using an online experiment.
InSection2,weintroduceaprincipal–agentmodelthatformalizesthedesignofrecommendation
algorithms that assist a human in making binary treatment choices. As a motivating example, we
consider a manager making hiring decisions on applicants who are evaluated by a pre-employment
assessment. The principal in our problem is the company, who designs an algorithm that assists
a hiring manager (the agent) in making the ultimate hiring decision. This model allows for com-
plementarity in information and expertise between the principal’s algorithm and the agent, thus
2moving beyond models in which perfect adherence to the algorithmic recommendation is optimal.
Section 3 connects our problem setup to the potential-outcomes framework from causal in-
ference. There, we introduce a monotonicity assumption that allows us to describe the effect of
recommendations in terms of compliance, similar to the analysis of instrumental variables by Im-
bens and Angrist (1994). The potential-outcomes framework formalizes how individual outcomes
or actions change in response to treatment choices. We use this framework to capture how different
recommendations given by the algorithm lead to different decisions by the human decision-maker.
Using our potential outcomes, we can state the optimal recommendation algorithm as minimizing
an objective that separates into two parts, (1) the loss incurred by a direct implementation of the
algorithm’s recommendations and(2) the loss incurred(or avoided) due to how decision-makers de-
viatefromtherecommendations. Ourmonotonicityassumptionthenimpliesthatrecommendations
can only move the decision-maker toward the recommended action.
In Section 4, we showcase our framework by applying it to an online experiment that involves a
hypothetical hiring task. This experiment asks subjects to make 25 hiring decisions on hypothet-
ical applicants, and randomly varies the recommendation algorithm across subjects. This setup
allows us to evaluate how the structure of recommendations impacts ultimate task performance.
Specifically, the separate recommendation algorithms are based on different assumptions about
compliance behaviors. In line with our framework, we find that subjects perform better when
the recommendations are designed to provide complementary information, rather than optimized
for hiring decisions directly. The best-performing algorithm assumes that human decision-makers
comply with the algorithm only selectively in those cases where they are uncertain. The resulting
decisionsoutperformunassistedhumandecisionsaswellasthebestperformancebyanalgorithmon
its own. We thus demonstrate that our framework can help design systems that improve decisions
through human–AI complementarity.
Our work responds to an empirical literature that analyzes inefficiencies in the interactions
between human decision-makers and prediction algorithms. This literature has identified cases
where human decision-makers imperfectly use prediction algorithms in high-stakes decisions in
criminal justice (Imai et al., 2021; Stevenson and Doleac, 2022; Angelova, Dobbie, and Yang, 2023),
healthcare (Agarwal et al., 2023; Maron et al., 2020), and child protective services (Fogliato, De-
Arteaga, and Chouldechova, 2022; Mills and Grimon, 2022), so much so that the assisted human
decisions are often no better than unassisted human decisions. A more extensive review of this
literature is detailed by Lai et al. (2021). Of the works in this literature, our empirical approach
is most related to that of Green and Chen (2019) who posit ways human decision-makers should
respond to algorithms and test these hypotheses in the lab. Relative to this literature, we focus
on designing complementary recommendations, rather than focusing on describing ways in which
humans misuse algorithms.
We join a collection of works spanning the areas of operations, economics, and computer sci-
3ence that design algorithms to assist humans in taking better decisions. Bansal et al. (2019) and
Donahue, Chouldechova, and Kenthapadi (2022) propose improvements to collaborative decision
processes based on the private information of each agent. Balakrishnan, Ferreira, and Tong (2022)
and Orfanoudaki et al. (2022) find that humans often incorrectly assume linear relationships, then
develop strategies to help human–algorithm decision systems overcome this issue. Athey, Bryan,
and Gans (2020) and McLaughlin and Spiess (2022) model how effort costs and reference points,
respectively, affect a human’s adoption of algorithmic assistance. Baek et al. (2023) and Bastani,
Bastani, and Sinchaisri (2022) apply ideas from reinforcement learning to incorporate past human
actions into the recommendations of assistive algorithms. Caro and de Tejada Cuenca (2023) and
Sun et al. (2022) use data on workers’ deviations from algorithm advice to increase compliance.
Noti and Chen (2023) focuses on the question of when to provide assistance, and shows that strate-
gically restricting advice can preserve complementary between machine and human performance.
Our approach differs from these prior works as it provides a causal framework to capture how a
human’s response varies with the underlying algorithm. Similar to the work of Ibrahim, Kim, and
Tong (2021) (where human predictors assist algorithmic decision-makers), our results highlight the
importance of private information in designing impactful collaboration.
Our methods build upon a literature that optimizes the design of estimators that inform
decision-making. We combine models of communication between heterogeneous agents (Kamenica
and Gentzkow, 2011; Andrews and Shapiro, 2021; Spiess, 2022) with approaches from causal infer-
ence with imperfect compliance (Angrist, Imbens, and Rubin, 1996; Vytlacil, 2002; Mogstad and
Torgovitsky, 2018) in order to design optimal recommendation algorithms in the prescriptive sense
(Bertsimas and Kallus, 2020). This approach parallels work that transfers concepts from causal
inference for use in decision-making (Manski, 2021, 2023), in particular the approach in Saghafian
(2023) of incorporating unobserved confounding using sets of causal models and the approach in
Ben-Michael et al. (2022) of learning robust decision policies from non-stochastic treatment assign-
ments.
2 Problem Setup
We introduce a principal–agent model of joint human–algorithm decision-making, which builds
upon McLaughlin and Spiess (2022). In this model, the principal designs an algorithm that gives
recommendations to an agent. The agent is a human decision-maker who observes each recom-
mendation and subsequently makes a binary treatment choice. As our main illustration, we apply
this model to the problem a company (principal) faces when implementing an algorithmic pre-
employment assessment to assist managers (agents) in the hiring process. The company wants its
managers to hire good workers and not hire bad workers. The company impacts the managers’
decisions only through the recommendation algorithm they implement. We allow for the managers
to have both unknown information and preferences, allowing for complex relationships between the
4algorithm the company implements and the decisions that managers take.
2.1 Hiring as a Binary Treatment Choice Problem
We consider the problem of taking a binary decision D before a binary outcome Y is realized.
We capture the impact of this decision through a loss function ℓ, which describes regret from
implementing D compared to the optimal decision. For each problem instance, we assume access
to characteristics X that have predictive power for Y.
Asarunningexamplethroughoutthispaper,asthebinarytreatmentchoiceweconsiderahiring
decision D ∈ {N,H} with the outcome Y ∈ {B,G} measuring the applicant’s job performance,
which is unknown at the time of the decision. We measure the utility associated with this decision
via the loss function,
ℓ(Y,D) = c 1[Y = G,D = N]+c 1[Y = B,D = H]. (1)
I II
The type-I error cost c > 0 captures the value of a good (G) worker’s productivity minus their
I
hiringcostwhilethetype-IIerrorcostc > 0capturesthehiringcostminuswhateverproductivity
II
a bad (B) worker provides.
While worker performance Y is ex-ante unknown, we assume that there is information X col-
lected about the worker which is available at the time D is made. We do not assume any specific
structure of X, as the data available about applicants before hiring may include their resume,
written question responses, video responses to interview questions, or the actions the worker took
during a game (Li et al., 2021). We do require that the information X is collected, documented,
and made available prior to making a hiring decision D on each worker, and for simplicity assume
that X has finite support.
2.2 Algorithm Design as a Principal–Agent Problem
We examine the decision D through the lens of a principal–agent model with asymmetric informa-
tion and potentially misaligned preferences. Specifically, we assume the decision D is taken by an
agent who (1) observes private information U, (2) does not observe characteristics X directly, and
(3) may not share the preferences represented by the loss function ℓ. To improve the decisions by
the agent, the principal (whose preferences are captured faithfully by ℓ) implements an algorithm
frec that provides recommendations R = frec(X) to the agent. Thus, similar to the information
design literature (Kamenica and Gentzkow, 2011; Bergemann and Morris, 2019), the principal has
partial control over the information the agent receives before they make the final decision.
Returning to our hiring example, we assume the principal is a company that implements an
algorithmic recommendation system frec that uses the data X on workers to generate hiring rec-
ommendations R = frec(X) which influence the hiring decisions of the agent, who is a manager
5at the company. The recommendations reduce the unstructured applicant data X to a simpler
form that is still informative about the worker’s ability Y. For simplicity, we limit the algorithm
to either recommend a decision to the manager (not hire, R = N, or hire, R = H) or not make a
recommendation at all (R = ∅). In practice, recommendations may present more granular predic-
tive information, such as a score that represents a scaled probability a worker is good (Y = G) and
allows the managers to understand the algorithm’s relative ranking of different applicant workers.
It may also include an explanation of the score, highlight pieces of X the algorithm deems partic-
ularly relevant, or include certainty information about the confidence of its prediction (Raghavan
et al., 2020).
ThemanagerrespondstotherecommendationbymakingahiringdecisionD whichmaydepend
ontherecommendationR alongwiththeirprivateinformationU andunknownprivatepreferences.
The manager may receive this private information U as a result of interviews with the applicant
and may have preferences not captured by ℓ which drive their hiring decisions. We do not, for now,
assume any specific structure of the managers’ preferences or private information. Furthermore,
similartohowweassumethecompanydoesnotobserveU,weassumethemanagerdoesnotobserve
X. However, we allow arbitrary correlation between X and U, effectively capturing the idea that
X and U contain shared information. This flexibility allows us to capture the complex decisions
managers exhibit empirically. Note that D generally depends on the recommendation rule frec
beyond the actual recommendation R that is given in a specific case. This is because interpreting
the recommendation R requires an understanding of which cases lead to each recommendation.
Throughout,weincludetheoptionthatthealgorithmdoesnotmakearecommendation,R = ∅,
forcing the manager to make an active choice. We believe that this design feature has formal
advantages for modeling recommendations and practical relevance for achieving human–algorithm
complementarity. First, allowing for instances without recommendations enables us to include
decisions taken without any recommendation algorithm (i.e., unassisted decisions) as a special case
in our framework. Second, building upon McLaughlin and Spiess (2022), forcing active choices by
avoiding recommendations in specific cases may reduce distortions in cases where human decision-
makers know better. These beliefs are further supported by the work of Noti and Chen (2023),
which finds that partially withholding algorithmic assistance (i) increases humans’ reliance on
advice when given and (ii) can be leveraged strategically to improve humans’ prediction accuracy.
3 Making Sense of Recommendations with Potential Outcomes
Having set up a principal–agent problem for thinking about the design of recommendation algo-
rithms, we now use the potential-outcomes framework from causal inference to discuss how the
agent’s decision depends on the recommendation algorithm. Viewing the agent’s decision in this
way suggests that the optimization of recommendations shares common features with the analy-
sis of instrumental variables in inference problems. We strengthen this connection by applying a
6monotonicity assumption similar to that of Imbens and Angrist (1994), which allows us to repre-
sent the remaining potential outcomes in terms of the active decision the agent makes when no
recommendation is given and whether the agent complies with the recommendation. This leads to
a decomposition of the objective of the recommendation algorithm, in terms of compliance, into
the performance of the recommended actions and the quality of active decisions.
3.1 Recommendations Induce Potential Outcomes
We now interpret the agent’s decision in terms of the potential-outcomes framework of Rubin
(1974). Specifically, we write D (r;frec) for the potential decisions that the agent would take
U
when receiving the recommendation r from the recommendation algorithm frec. This notation
makes explicit that potential outcomes vary with the private information U, and are specific to a
recommendation rule frec. The realized decisions are then obtained as D = D (R;frec), where
U
R = frec(X).
In our hiring example, the algorithm’s recommendations impact the company’s utility only
through managers’ decisions. Our framework, therefore, parallels instrumental variables that affect
outcomesonlythroughtheirimpactonatreatmentvariable. Ratherthandirectlyoptimizingforthe
recommendation, the company must understand how the managers’ decision-response D (r;frec)
U
varies across r ∈ {N,∅,H}. Since decisions are themselves binary and the recommendations take
three values, we can enumerate these potential outcomes and categorize them into four response
types based on how the recommendation impacts the assisted managers’ decisions. These response
types are similar in nature to the types developed by Angrist, Imbens, and Rubin (1996) for the
analysis of instrumental variables.
Table 1 enumerates the potential decisions associated with a manager’s response to the rec-
ommendations and provides such a categorization. If the manager maintains the same decision
regardless of the recommendation, they Ignore the recommendation. If they adopt the recommen-
dation regardless of whether the algorithm recommends not hiring (N) or hiring (H) the applicant,
then they Comply with the recommendation. If, instead, the manager always takes the decision
that the algorithm does not recommend, then they Defy the recommendation. If none of these
cases hold, then it must be the case that the manager will Change their decision in response to the
algorithm sending a recommendation, taking one decision when the algorithm sends no recommen-
dation and taking the opposite decision if either recommendation is sent. While we maintain all
these response types for now, we will later simplify the analysis by ruling out the Defy and Change
types, analogously to the monotonicity assumption of Angrist, Imbens, and Rubin (1996).
3.2 Objective of Recommendation Algorithms
Having introduced a framework to capture potential decisions in response to recommendations, we
now consider the optimal recommendation algorithm as the solution to an optimization problem
7Response type D (N) D (∅) D (H)
U U U
N N N
Ignore
H H H
N N H
Comply
N H H
H N N
Defy
H H N
H N H
Change
N H N
Table 1: Enumeration and categorization of the potential outcomes associated with the managers’
decisions in response to different realizations of the recommendation. The Defy and Change re-
sponse types will later be ruled out by Assumption 1.
that minimizes the loss achieved by the potential outcomes the algorithm induces. We show the
optimal recommendation’s objective can be separated into a triage term that assumes that recom-
mendations are implemented directly as well as an objective that captures the agent’s endogenous
response to the recommendation. We compare this optimal recommendation algorithm to solu-
tions that instead directly target optimal decisions, and provide an intuition for why that optimal
decision rule will rarely be the optimal recommendation rule.
In our hiring example, the optimal recommendation algorithm for the company to implement
is the mapping from applicant worker data to recommendations that minimizes the expected loss
of the decisions taken by the manager,
frec∗ = min E[ℓ(Y,D)] where R = f(X),D = D (R;f). (2)
U
f:X(cid:55)→R
This optimization problem is quite difficult to solve, given the uncertainty about how managers
will respond to recommendations. As a result, companies often implement algorithms that instead
recommend the optimal decision based on the information available to the algorithm,
fdec∗ = min E[ℓ(Y,R)] where R = f(X), (3)
f:X(cid:55)→R
implicitly assuming that the recommendation R = f(X) gets implemented directly (D = R). An
extension suggested in previous work by Raghu et al. (2019) is to triage instances into those where
algorithms outperform humans and those where humans outperform algorithms. If we write D =
0
D (∅;f∅ ) where f∅ ≡ ∅ for the decision taken by the manager in absence of a recommendation
U
algorithm, and we assume that the manager takes this decision when not given a recommendation
(f(X) = ∅) and otherwise follows the algorithm, then this objective and its solution correspond in
8our framework to
ftri∗ = min E[ℓ(Y,R)1[R ̸= ∅]+ℓ(Y,D )1[R = ∅]] where R = f(X). (4)
0
f:X(cid:55)→R
Thisobjectiveagainimplicitlyassumesthatrecommendationsarefollowedperfectlywhengivenand
thatmanagerdecisionsarenotaffectedbytherecommendationalgorithmwhennorecommendation
is given (R = ∅). In practice, the decision for R = ∅ may itself be different from the decision D
0
without a recommendation algorithm, and we may specifically hope it to improve since additional
information and effort may be available for those cases (Raghu et al., 2019; Athey, Bryan, and
Gans, 2020). Note also that the case of recommending an optimal decision is a special case where
the algorithm always makes a recommendation (R ̸= ∅), with which the agent complies.
We now compare these different objectives and solutions, and show that the optimal recom-
mendation (frec∗) can be thought of as an augmentation of the optimal triage (ftri∗) that also
incorporates the compliance behavior of the manager. Specifically, the loss function that defines
theoptimalrecommendationcanbedecomposedintothelossfunctionofthetriageproblem(which
we call the triage effect, TE) and the response effect (RE) from managers deviating from the triage
decision,
TE –TriageEffect
(cid:122) (cid:125)(cid:124) (cid:123)
frec∗ = min E[ℓ(Y,R)1[R ̸= ∅]+ℓ(Y,D )1[R = ∅]]
0
f:X(cid:55)→R
+E[(ℓ(Y,D)−ℓ(Y,R)) 1[R ̸= ∅]+(ℓ(Y,D)−ℓ(Y,D )) 1[R = ∅]] (5)
0
(cid:124) (cid:123)(cid:122) (cid:125)
RE –ResponseEffect
where R = f(X),D = D (R;f).
U
In addition to the triage effect from (4), this decomposition has a second part that represents
deviations from recommendations and unassisted decisions. It can represent both improvements
(in cases where the human decision-maker uses their private information to override a suboptimal
recommendation or adjusts their active decision based on their knowledge of the recommendation)
as well as mistakes (when the manager goes against a correct hiring recommendation or takes an
active decision that is worse than in the case without any recommendations).
The two components also highlight the challenge of learning good recommendations from data.
Thetriageeffect(TE)isstraightforwardtoestimate, providedthecompanyhasaccesstohistorical
data through which they can learn the joint distribution of covariates X, ability Y, and unassisted
managerdecisionsD . Ontheotherhand,theresponseeffectdependsonthepotentialoutcomesas-
0
sociatedwiththemanagers’decisions,whichcomplicatesitsevaluationforanyfrec underwhichde-
cision data has not already been collected. Understanding (D (N;frec),D (∅;frec),D (H;frec))
U U U
may require more than simple experimental evaluation based on local changes in recommenda-
tions. These potential decisions may vary arbitrarily with changes to frec. Solving for the optimal
9RE
fdec∗
ftri∗
F
frec∗
TE
Figure 1: Representation of potential algorithms F that could be implemented in terms of the
triage effect TE and the response effect RE. The optimal triage ftri∗ minimizes the triage effect
over F, while the optimal recommendation rule frec∗ minimizes the sum TE +RE. The optimal
decision rule fdec∗ need not lie on the Pareto frontier that minimizes these two losses, highlighted
in red.
recommendation then requires understanding this distribution across potentially implementable
recommendation algorithms F ⊆ {f;f : X (cid:55)→ R}. This makes designing an optimal recommen-
dation algorithm an intractable task without additional assumptions (even when we just want to
robustly outperform the unassisted decisions D ).
0
The relationship between algorithms that solve for the different objectives above is illustrated
in Figure 1. Plotting the set of possible algorithms (F) according to their triage effect (TE) and
response effect (RE) clarifies, in particular, the relationship between optimal triage and optimal
recommendation. The optimal triage (ftri∗) minimizes the triage effect without considering the
response effect related to each algorithm. The optimal recommendation (frec∗) potentially incurs
additional loss through the triage effect in order to reduce the loss incurred due to the response
effect. In practice, this means the optimal recommendation may be sub-optimal if applied directly,
instead relying on the manager’s ability to strategically override decisions inconsistent with their
own private information (U) and preferences. However, both frec∗ and ftri∗ lie on the Pareto
frontier which simultaneously minimizes the triage and response effects. This does not have to be
true of the optimal algorithmic decision rule fdec∗, which generally incurs a higher triage effect
TE than the optimal triage solution ftri∗. This is because the triage solution ftri∗ comes from
minimizing expected triage loss (4) subject to a less strict constraint than the decision solution
fdec∗. Meanwhile, the response effect can be better or worse when applying the fdec∗ or ftri∗ rules.
3.3 Monotonic Responses and Compliance-Aware Recommendations
Above, we have proposed a potential-outcomes framework for potential agent decisions in response
to recommendations, and used this framework to discuss the design of optimal recommendation
algorithms in general. We now introduce assumptions that limit the potential outcomes associated
10with the agent’s decision to simplify the analysis of their endogenous response. We then use these
simplifications to shed further light on the structure of optimal recommendations in terms of agent
compliance.
In the context of the four response types introduced in the hiring example and repeated in
Table 1, manager behavior consistent with the Ignore and Comply responses is easier to explain
thanbehaviorconsistentwiththeDefy andChange responses. Ifthemanagerbelievestheirprivate
information U is much better than the algorithm’s for some problem instance, they are likely to
Ignore the recommendation. Conversely, if they believe the algorithm’s information dominates any
additional information in U, they will Comply. Using recommendations as a threshold requirement
for considering an applicant, as done by hiring managers interviewed by Li et al. (2021), also
generates potential outcomes consistent with only Ignore and Comply responses. On the other
hand, Defy and Change behaviors are counter-intuitive and typically need bad recommendations,
perversecorrelationbetweenX andU,oradversarialmanagerialincentivestobejustifiedrationally.
Consequently, we now make an assumption to eliminate the potential outcomes associated with
Defy andChange responses, similartothemonotonicityassumptionofAngrist, Imbens, andRubin
(1996)inthecontextofinstrumentalvariablesanditsextensionstomultivariatediscretetreatments
(including Lee and Salani´e, 2018).
Assumption 1 (Monotonic response to frec). Let ⪯ be the ordering of decisions with N ⪯ H. We
assume that D (·;frec) is such that D (N;frec) ⪯ D (∅;frec) ⪯ D (H;frec) for all frec ∈ F.
U U U U
At a high level this assumption restricts the direction of the managers’ response to follow R. Hire
recommendations (R = H) can turn not-hire decisions into hire decisions (D = N → D = H) but
cannot turn hire decisions into not-hire decisions (D = H ̸→ D = N). Not-hire recommendations
(R = N) do the exact opposite. This restriction exactly rules out the bottom four rows in Table 1.
It can be thought of as a restriction on plausible recommendation rules F. Specifically, it rules out
that recommendation rules are used that would make sense to Defy (such as a rule that suggests
hiring bad candidates) or that would make sense to observe the Change behavior for (such as a rule
that only gives a recommendation for bad candidates and never gives one for good candidates).
When the manager’s decision behavior is limited by Assumption 1, we can fully describe their
behavior for some specific recommendation rule frec in terms of their active decisions D∅ =
D (∅;frec) which they exhibit when not given a recommendation, but the algorithm frec is imple-
U
mented, along with their compliance behavior C:
Proposition 1 (Simplifiedpotentialoutcomes). Under Assumption 1, the potential outcome triplet
(D U(N;frec),D U(∅;frec),D U(H;frec))canbefullyexpressedbytheactivedecisionD∅ = D U(∅;frec) ∈
{N,H}, which expresses the agent’s decision when not given a recommendation, and the compliance
type C ∈ {Ignore,Comply}, which specifies whether the agent adopts R when the recommendation
is presented.
11C D (N) D (∅) D (H)
U U U
Ignore D∅ D∅ D∅
Comply N D∅ H
Table 2: Reduction of potential outcomes under Assumption 1. This organization acts as
a visual representation of Proposition 1, showing how the values that the potential outcomes
take reduce to the active decision D∅ ∈ {N,H} absent a recommendation and the compliance
C ∈ {Ignore,Comply}.
Table 2 visualizes Proposition 1 by restating the remaining potential-decision types in terms of
(D∅,Comply) once Assumption 1 is applied. The only information determining the decision D
outside of the recommendation R is the active decision D∅ and whether the response type C of the
manager is Ignore or Comply. The proposition follows directly from an inspection of Table 1. For
completeness, a formal proof is given in Appendix A.
As an immediate consequence of the simple representation of response types in Proposition 1,
we can decompose the objective of the recommendation algorithm in terms of compliance:
frec∗ = min E[ℓ(Y,R)1[C = Comply and R ̸= ∅]+ℓ(Y,D∅)1[C = Ignore or R = ∅]]
f:X(cid:55)→R (6)
where R = f(X).
Similarly, this explicit notation allows us to shed further light on the response effect from (5) in
terms of compliance and deviations between active decisions D∅ and unassisted decisions D 0:
RE = E[(ℓ(Y,D∅)−ℓ(Y,R)) 1[R ̸= ∅,C = Ignore]+(ℓ(Y,D∅)−ℓ(Y,D 0)) 1[R = ∅]] (7)
The first term in this decomposition expresses deviations from recommendations when they are
ignored, while the second term captures the change in behavior for those instances where no rec-
ommendation is given that comes from the presence of the recommendation algorithm.
Even with the simplifications introduced by Assumption 1, it is not generally possible to learn
the optimal recommendation algorithm from past data (Y,X,D ) on job performance, character-
0
istics, and unassisted human decisions. This is because neither the compliance behavior C nor
the impact of recommendations on active decisions D∅ can be learned from such limited data.
However, we can now formulate assumptions on these unobserved quantities in order to derive
optimal learnable recommendation algorithms in specific cases, thus providing a formal analog and
extension of the discussion in Section 3.2:
◦ If compliance is perfect (C ≡ Comply) and decisions in instances without recommendations
arenotaffected(D∅ ≡ D 0), thentheoptimalrecommendationalgorithmistheoptimaltriage
solution ftri∗ (which can be learned from data on (Y,X,D )).
0
12◦ Ifcomplianceisperfect(C ≡ Comply),andrecommendationsarelimitedtobebinary(R ̸= ∅)
or human decisions are dominated by recommended actions (such as when E[ℓ(Y,D∅)|X] ≥
min E[ℓ(Y,r)|X] X-a.s. and for all frec ∈ F), then the optimal recommendation
r∈{H,N}
algorithm is the optimal decision algorithm fdec∗ (which can be learned from data on (Y,X)).
This applies, specifically, in the case with perfect compliance and no private information of
the human decision-maker.
At the same time, more nuanced assumptions or data on compliance behavior may be necessary to
achieve complementarity in practical cases. This may include the cases below:
◦ If recommendations do not affect compliance (C) or active decisions for instances where no
recommendations are given (D∅ = D 0), then the optimal recommendation can generally be
seen as the optimal triage decision among compliers only, that is, it solves
min E[ℓ(Y,R)1[R ̸= ∅]+ℓ(Y,D )1[R = ∅]|C = Comply] where R = f(X).
0
f:X(cid:55)→R
However, unlike the case of full compliance, we now have to learn about compliance behavior
in addition to active decisions.
◦ Even if the agent takes optimal decisions given their available information, we would require
information beyond (Y,X,D ), since the optimal recommendation would also depend on the
0
structure and information content of the private signal U.
In our experiment below, we combine information about the point distribution of information
(Y,X,U) with assumptions on active decisions D∅ and compliance C in order to solve for sensible
recommendation algorithms.
4 Complementary Recommendations in an Experiment
We now apply our framework to the design and analysis of recommendation algorithms in a con-
trolled trial. In a pre-registered online experiment, study subjects participate in an incentivized
hiringgame.1 Thisempiricalapplicationshowcasesourframework’sabilitytomaprealisticassump-
tionsaboutagentbehaviortothedesignofpracticalrecommendationsthatrealizecomplementarity
between humans and algorithms.
In the experiment, we collect hypothetical hiring decisions while varying the structure of the
recommendationalgorithmatthesubjectlevel. Weapplydifferentassumptionsonhowsubjectswill
behave within our framework to generate these recommendation algorithms. We find that subjects
areabletoimprovetheirdecisionswithalgorithmswhoserecommendationsarefocusedonproviding
suggestions that are complementary to the subjects’ private information. Moreover, subjects given
1The pre-analysis plan for this experiment can be found at www.socialscienceregistry.org/trials/11857.
13thesefocusedrecommendationsperformbetterthansubjectswhoseenaiverecommendationsbased
on optimal algorithmic predictions, and outperform decisions by the human or algorithm alone.
4.1 Experimental Design
We implement a hiring game as part of an online experiment to test subjects’ performance when
using recommendation algorithms in a setting where both subjects and algorithms have private
information. Specifically, we label 25 hypothetical applicants as good or bad based on the role they
are applying for (private information only available to experiment subjects) and their personality
type (private information only available to the algorithms), and ask subjects to only hire good
applicants. We measure how the accuracy of subjects’ hiring decisions changes in response to
different recommendation algorithms (treatments) that vary at the subject level.
For each subject, the experiment proceeds as follows. Subjects are brought to a Qualtrics
survey from the recruiting site Prolific and informed they will make hiring decisions on 25 hypo-
thetical applicants, each of which are good or bad types (referred to as the applicant’s ability).
They are informed that each applicant’s ability Y is determined by the relationship of the appli-
cant’s personality type X (denoted by letters A–E) to their intended role U (Engineering, Sales, or
Communications), as shown in Figure 2. They then answer three comprehension questions about
how ability Y is determined and, conditional on answering correctly, are randomized into a treat-
ment. All but one control condition give subjects a recommendation algorithm to assist them with
making hiring decisions. Unlike the study subjects, these algorithms can access the applicants’
personality types, but not their role. Subjects are then asked three more comprehension questions
about the specific algorithm they will interact with, but continue in the experiment regardless of
their answers. Subjects then make 25 hiring decisions with access to the distribution of applicants
as represented in Figure 2, the recommendations of their assigned algorithm, and an explanation
of that specific recommendation algorithm. An example of a hiring decision is given in Figure 3.
When making these hiring decisions, subjects know the applicant’s role, but are unaware of the
applicant’s personality type beyond any information conveyed by the recommendation.
Subjects are incentivized to hire good applicants and to not hire bad applicants. Specifically, as
compensation for participating in our experiment, subjects receive $2.04 for their time (estimated
to be 12 minutes) and the chance to earn a $2.00 bonus as a performance incentive. They earn this
bonus if their hiring decision on a randomly selected applicant is correct (hire if good, not hire if
bad). Note that this induces equal type I and type II losses (c = c ) from the perspective of our
I II
subjects, and we analogously evaluate their performance using the percentage of correct decisions
taken. Subjects see all 25 applicants in a random order to make their hiring decisions, but don’t
learn whether they took correct decisions.
The joint distribution of applicant role, personality type, and ability pictured in Figure 2 is de-
signed to vary subjects’ willingness to Comply with recommendations across applicant roles while
14Figure 2: Summary of how the information available to the subject (the applicant’s role) and the
information of the algorithm (the applicant’s personality type) relate to the applicant’s ability (bad
or good). This information is shown to all participants in the experiment and is easily accessible
throughout their decision–making process.
Figure 3: Sample hiring decision from the Predictive treatment.
15Algorithm A B C D E
Control ∅ ∅ ∅ ∅ ∅
Predictive N H H H H
Complementary N H H H N
Triage N ∅ ∅ H H
Complementary Triage N ∅ ∅ H N
Table 3: Recommendations that each algorithm sends across applicants’ personality types X ∈
{A,B,C,D,E}. Note that personality types B and C are equivalent for the distribution of outcomes
as well as in terms of recommendations given.
allowing the recommendation to provide useful information cases the applicant role is relatively
uninformative. When subjects review an applicant for an Engineering role, they know the worker
is good. Subjects are much more uncertain for Sales applicants (60% of which are good) and Com-
munications applicants (40% of which are good). The algorithm’s private information (personality
type) varies significantly across the cases that subjects are uncertain, thus containing useful in-
formation to help them make their decisions. Moreover, both the subject and algorithm’s private
information are of equal value in the sense that a rational agent with access to either the role or
the personality type would take the correct decision 76% of the time.
4.2 Treatments
The recommendation algorithm a subject receives represents the treatment in our experiment and
is randomized at the subject level. In addition to four treatment arms representing different rec-
ommendation algorithms, there is a control arm in which no recommendation is given. These rec-
ommendation algorithms vary by the way they map personality types X to recommended actions
R = frec(X) ∈ {N,∅,H}, where the recommendations follow our earlier example and correspond
to not hiring, no recommendation, and hiring the applicant, respectively. We now describe the
four different algorithms, and link them to the implicit assumptions about compliance and active
decisions that motivate their design. Additionally, Table 3 lists the recommendation algorithms
that subjects could be assigned to, alongside the recommendations that these algorithms provide
across personality types.
Control. Subjects in this condition did not observe a recommendation algorithm. We use this
treatment to collect information on the distribution of unassisted decisions subjects take.
Predictive. The algorithm in this condition sends a hire recommendation if at least half of the
applicants associated with the given personality type are good and sends a not-hire recom-
mendationotherwise. ThisPredictive algorithmistheoptimaldecisionanalgorithmcantake
by itself in order to minimize overall errors, so it corresponds to fdec∗ in the framework of
16Section 3. It would correspond to the optimal recommendation if compliance was perfect and
active decisions were only leading to worse hiring decisions.
Complementary. These subjects observe an algorithm that sends a hire recommendation if at
least half of the Sales and Communications applicants associated with the given personality
type are good and sends a not-hire recommendation otherwise. This algorithm ignores En-
gineering applicants on the assumption that subjects will perform effectively by themselves
when asked whether to hire these applicants, and instead sends the optimal decision for the
subpopulationofSalesandCommunicationsapplicants. Itcorrespondstotheoptimalrecom-
mendation if subjects do not comply with the recommendation for Engineering candidates,
but comply perfectly for the remaining applicants.
Triage. These subjects observe a version of the Predictive algorithm that implements a ‘safety
check’, causingtherecommendationnottosendrecommendationsforsomepersonalitytypes.
Specifically, the Triage algorithm only sends the Predictive algorithm’s recommendation of
hire if at least half of the Communications applicants associated with the given personality
type are good and it only sent the Predictive algorithm’s recommendation of not hire if at
least half of the Engineering and Sales applicants associated with the given personality type
are bad. As a consequence, this algorithm does not send recommendations for the B and C
personalitytypes(whichareequivalent). TheTriage algorithmistheoptimaltriagealgorithm
(ftri∗) for a rational agent with the subjects’ private information.
Complementary Triage. These subjects observe an algorithm that combines the logic of the
Triage and Complementary algorithms. These subjects receive the Complementary algo-
rithm’s recommendation of hire if at least half of the Communications applicants associated
with the given personality type are good and receive the Complementary algorithm’s rec-
ommendation of not hire if at least half of the Sales applicants associated with the given
personality type are bad. The Complementary Triage algorithm is the optimal algorithmic
triage over the subpopulation of Sales and Communications applicants.
We now summarize the logic behind these algorithms by examining their implicit assumptions
about the subjects’ compliance choices and active decisions. Specifically, Table 4 presents each
of the algorithms as a solution to a specific combination of assumptions. For compliance choices,
we distinguish between perfect compliance (that is, following the recommendation and only mak-
ing an active decision when no recommendation is given) and selective compliance. By selective
compliance, we mean that subjects always take an active decision for non-Engineering candidates,
and comply otherwise. For active decisions, we distinguish between completely random decisions
and sophisticated decisions. Sophisticated decisions assume that the subject uses their private role
information effectively when they take active decisions, that is, hire Engineering and Sales can-
didates, which is optimal absent recommendations and across each of our treatment arms. For
17Compliance C
Active decision D∅ Perfect Selective
Random Predictive Complementary
Sophisticated Triage Complementary Triage
Table4: Optimalalgorithmsunderdifferentcombinationsofcompliancechoicesandactivedecisons.
example, if compliance is perfect and decisions are sophisticated, then we assume that subjects
follow all recommendations and correctly decide in the absence of a recommendation when they do
not receive one.
Using these combinations of behaviors, we can interpret the above treatment arms as optimal
recommendation algorithms. First, the Predictive algorithm is optimal with perfect compliance
and random active decisions. Second, the assumptions under which the Triage algorithm is opti-
mal are that compliance is perfect and active decisions are sophisticated. In this case, the subject
follows the recommendation when it is given, and otherwise relies on their private information to
take a decision. Third, the Complementary algorithm is optimal when subjects comply for non-
Engineering candidates only, and take random decisions when no recommendation is sent. Fourth,
the Complementary Triage recommendations achieve optimal outcomes when subjects do not com-
ply for (and always hire) Engineering candidates, and effectively use their private information to
takeasophisticateddecisionwhentheydonotreceivearecommendation. Intheory, thistreatment
arm would achieve the best performance on average, with only one mistake (not hiring a type-E
communications candidate).
AlthoughfinalrecommendationsonlydependonthepersonalitytypeX,wenotethatconstruct-
ing these algorithms requires an understanding of the joint distribution of X with the outcomes
Y as well as the role U. Whether such a joint distribution can be learned in practice depends on
the type and quality of data that is available at the time of construction of an algorithm. Here, we
focus instead on an analysis of the relative performance of different recommendation algorithms in
a setting where we (and the study subjects) know the joint distribution of (Y,X,U).
4.3 Data and Main Results
Before analyzing the results of the experiment, we describe the data collection process, final sample
sizes for each treatment, and basic performance statistics. Among 1675 surveys begun, about
35% failed to answer at least one of the initial comprehension questions and were screened out,
leaving us with 1083 subjects that completed our study. Subjects were split approximately equally
between all arms except Control, which received half as many observations as the other treatments.
Specifically, eachofthefourtreatmentarmscontainedapproximately240participantswho, likethe
approximately120Control subjectswhodidnotgetrecommendations,eachtook25hiringdecisions
18Treatment arm Fraction optimal decisions (%) (SE)
Control 69.5 (0.7)
Predictive 76.2 (0.5)
Complementary 81.6 (0.7)
Triage 78.8 (0.8)
Complementary Triage 80.1 (1.1)
Feasible optimum 96.0
Algorithm by itself 76.0
Random choice 50.0
Table 5: Average subject performance (through fractions of optimal hiring decisions) across treat-
ments. Bootstrapstandarderrorsaregivenandclusteredatthesubjectlevel. Inadditiontosubject
performanceintheexperiment,thefeasibleoptimumconsidersthebesttheoreticallyachievableper-
formance by a combination of a recommendation algorithm with three levels and a rational agent,
while we also consider the optimal performance of an algorithm by itself as well as the trivial
benchmark of a completely random hiring choice based on the flip of a fair coin.
foratotalofaround6,000decision-responsespertreatmentand3,000forControl. Overall,subjects
performed well in the experiment and within the estimated time frame. The average fraction of
correctdecisionswas78.1%acrossallsubjects. Theaveragedurationoftheexperimentwasaround
twelve minutes. However, both the performance (standard deviation of 13 percentage points)
and duration (standard deviation of almost eight minutes) show significant heterogeneity across
subjects.
We compare treatments by average subject performance, with the main results reported in
Table 5. Subjects in the Predictive treatment, who received recommendations from an algorithm
that was trained to predict outcomes, got around 76% of hiring decisions correctly, significantly
above the around 70% in the Control group that was not supplied with any recommendations.
SubjectsintheTriage,Complementary,andComplementary Triage treatmentswerealldesignedto
providerecommendationsthatcomplementhumandecisions,andonaverageoutperformedsubjects
in the Predictive treatment. Hence, the data confirms that the algorithm designed to take optimal
decisionsbyitself(givenbythePredictive treatment)isnottheoptimalrecommendationalgorithm
in this case. Across the treatments, subjects assigned to the Compliance algorithm perform best,
with over 81% optimal decisions (although this performance is statistically indistinguishable from
the Complementary Triage subjects). Overall, the results suggest that subjects tend to take smart
compliancechoicesandfrequently(andcorrectly)overridethealgorithmforEngineeringcandidates.
At the same time, decisions are inefficiently noisy.
We further compare subject performance to useful reference points in Figure 4 (justified further
in Wu et al., 2023). Throughout, we focus on the fraction of cases subjects are taking the optimal
decision on an applicant (hire if good, not hire if bad). Within each treatment, we compare sub-
19Figure 4: Average subject performance (through fractions of optimal hiring decisions) across treat-
ments. 95% bootstrap confidence intervals are clustered at the subject level. In addition, the triage
performance (where compliance is perfect and not sending a recommendation uses the decision of
Control) is given for each treatment by a gray dot, while the Response Effect from Section 3.2
is shown by the vertical red dashed lines. The optimal algorithm performance is shown by the
gray horizontal line. The optimal performance achievable by a rational agent with the subject’s
knowledge (role and algorithmic recommendation) is given for each treatment arm as a black dot.
ject performance to the performance when following the algorithm perfectly (corresponding to the
Triage Effect of Section 3.2), and plot the impact of subject deviations (Response Effect). We also
provide the best achievable performance by an algorithm acting alone (to benchmark which treat-
ments were able to outperform algorithmic automation) as well as the performance achievable by
a rational agent acting with the subject’s information (to act as a performance upper bound). The
decomposition reveals that subject performance tends to improve by deviating from the algorithm
(at least in cases other than the Predictive arm, in which case subject performance is equivalent to
perfectly following the algorithm). Interestingly, machine-assisted human performance is best for
thesomewhatsimplerComplementary algorithmthanforthetheoreticallyoptimalComplementary
Triage. Overall, we think that the patterns in Figure 4 are in line with good compliance choices,
noisy active decisions, and a penalty for overly complex recommendation algorithms.
20Our experiment documents complementarity between humans and algorithms when recommen-
dations are designed in the right way. Subjects in the Triage, Complementary, and Complementary
Triage treatments all performed significantly better (at a 5% significance level) than humans by
themselves (Control) or the algorithm by itself, thus achieving human-AI complementarity, while
subjects in the Predictive treatment could only match the performance human or algorithm could
(theoretically) achieve by themselves.
4.4 Additional Analysis and Robustness Checks
Tounderstandperformancedifferencesacrosstreatmentarmsfurther,weseparatesubjects’propen-
sity to hire applicants across roles and personality types in Figure 5, and relate this behavior to
the recommendations subjects receive. From the graph, we can identify that subjects’ unassisted
decisionsmostlymatchtheaveragequalityofcandidates(whichwouldbeinlinewithaprobability-
matching model where subjects match the probability they hire an applicant with the probability
theyaregood). Theircompliancechoicesaregenerallysophisticated(inthattheycorrectlyoverride
the algorithm for Engineering candidates), although somewhat random. Note that our framework
predicts that the Complementary algorithm is optimal if subjects make somewhat noisy active
decisions but make sophisticated compliance choices. This is in line with our empirical findings,
where the Complementary algorithm is the best among those algorithms we tested and likely the
optimal recommendation algorithm we could have implemented.
As an additional robustness check for our results in this section, we exploit additional compre-
hension questions we asked about the specific recommendation algorithms. Specifically, 376 of the
961 study subjects who were given a recommendation did not answer one of the comprehension
questions correctly that was asked about their specific algorithm. We reproduce all analysis on
both the sub-population of subjects who answered all recommendation comprehension questions
correctly and the sub-population of subjects who answered at least one question wrong in Ap-
pendix B. Our results strengthen on the sub-population who answered all comprehension questions
correctly, while, as expected, results are very noisy and largely insignificant on the sub-population
of subjects who answered at least one question wrong. These results support conclusions by Bansal
et al. (2019) that a subject’s understanding of an algorithm is vital to achieving complementarity.
5 Conclusion
This article formalizes the analysis and optimal design of recommendation algorithms within a
principal–agent model that employs the potential-outcomes framework from causal inference. We
draw similarities between the impact that recommendation algorithms have on human decisions in
algorithmic design and the effect that instrumental variables have on individual treatment choices
in inference problems. We use this connection to decompose the objective function of a recom-
21Figure 5: Frequency of experiment subjects taking a hire decision (y-axis) across the different
combinations of treatments (color) and profiles (x-axis). Different markers are used to identify
whether the subject received a hire recommendation (⇑), did not receive a recommendation (−),
or received a not-hire recommendation (⇓). The optimal decision given oracle information is given
by the location of the points on the lines y = 1 (hire is optimal) and y = 0 (not hire is optimal).
The dashed lines give the frequency of an applicant being hired by a subject without access to
recommendations (Control arm).
22mendation algorithm into algorithmic performance and human response. Making additional as-
sumptions on reasonable human responses to recommendations, our approach allows us to express
recommendation-assisteddecisionsintermsofcomplianceandactivedecisions. Wecanthenreason
aboutoptimalrecommendationalgorithmsintermsofassumptionsoneachcomponentindividually.
We demonstrate the utility of our framework in a controlled online experiment, which reinforces
that decision-maker responses should be considered when designing recommendation algorithms.
Specifically, we document that subjects perform better using recommendations that are designed
to provide complementary information, relative to algorithms that are instead designed to solve a
problem directly.
References
Agarwal, Nikhil, Alex Moehring, Pranav Rajpurkar, and Tobias Salz (2023). Combining Human
Expertise with Artificial Intelligence: Experimental Evidence from Radiology. (Cited on page 3.)
Andrews,IsaiahandJesseM.Shapiro(2021). AModelofScientificCommunication. Econometrica,
89(5):2117–2142. (Cited on page 4.)
Angelova, Victoria, Will S. Dobbie, and Crystal Yang (2023). Algorithmic Recommendations and
Human Discretion. (Cited on page 3.)
Angrist,JoshuaD.,GuidoW.Imbens,andDonaldB.Rubin(1996). IdentificationofCausalEffects
Using Instrumental Variables. Journal of the American Statistical Association, 91(434):444–455.
(Cited on pages 4, 7, and 11.)
Athey,SusanC.,KevinA.Bryan,andJoshuaS.Gans(2020). TheAllocationofDecisionAuthority
to Human and Artificial Intelligence. AEA Papers and Proceedings, 110:80–84. (Cited on pages 4
and 9.)
Baek, Jackie, Justin J. Boutilier, Vivek F. Farias, Jonas Oddur Jonasson, and Erez Yoeli (2023).
Policy Optimization for Personalized Interventions in Behavioral Health. arXiv:2303.12206 [cs].
(Cited on page 4.)
Balakrishnan, Maya, Kris Ferreira, and Jordan Tong (2022). Improving Human-Algorithm Collab-
oration: Causes and Mitigation of Over- and Under-Adherence. (Cited on page 4.)
Bansal, Gagan, Besmira Nushi, Ece Kamar, Walter S. Lasecki, Daniel S. Weld, and Eric Horvitz
(2019). Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance. Pro-
ceedings of the AAAI Conference on Human Computation and Crowdsourcing, 7:2–11. (Cited on
pages 4 and 21.)
23Bastani, Hamsa, Osbert Bastani, and Wichinpong Park Sinchaisri (2022). Improving Human
Decision-Making with Machine Learning. arXiv:2108.08454 [cs]. (Cited on page 4.)
Ben-Michael, Eli, D. James Greiner, Kosuke Imai, and Zhichao Jiang (2022). Safe Policy Learning
through Extrapolation: Application to Pre-trial Risk Assessment. arXiv:2109.11679 [cs, stat].
(Cited on page 4.)
Bergemann, Dirk and Stephen Morris (2019). Information Design: A Unified Perspective. Journal
of Economic Literature, 57(1):44–95. (Cited on page 5.)
Bertsimas, Dimitris and Nathan Kallus (2020). From Predictive to Prescriptive Analytics. Man-
agement Science, 66(3):1025–1044. (Cited on page 4.)
Caro,FelipeandAnnaS´aezdeTejadaCuenca(2023). BelievinginAnalytics: Managers’Adherence
to Price Recommendations from a DSS. Manufacturing & Service Operations Management, page
msom.2022.1166. (Cited on page 4.)
Donahue, Kate, Alexandra Chouldechova, and Krishnaram Kenthapadi (2022). Human-Algorithm
Collaboration: Achieving Complementarity and Avoiding Unfairness. arXiv:2202.08821 [cs].
(Cited on page 4.)
Fogliato, Riccardo, Maria De-Arteaga, and Alexandra Chouldechova (2022). A Case for Humans-
in-the-Loop: Decisions in the Presence of Misestimated Algorithmic Scores. (Cited on page 3.)
Green, Ben and Yiling Chen (2019). The Principles and Limits of Algorithm-in-the-Loop Decision
Making. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW):50:1–50:24. (Cited
on page 3.)
Hemmer, Patrick, Max Schemmer, Michael V¨ossing, and Niklas Ku¨hl (2021). Human-AI Com-
plementarity in Hybrid Intelligence Systems: A Structured Literature Review. PACIS 2021
Proceedings. (Cited on page 2.)
Ibrahim,Rouba,Song-HeeKim,andJordanTong(2021). ElicitingHumanJudgmentforPrediction
Algorithms. Management Science, 67(4):2314–2325. (Cited on page 4.)
Imai, Kosuke, Zhichao Jiang, James Greiner, Ryan Halen, and Sooahn Shin (2021). Experimental
EvaluationofAlgorithm-AssistedHumanDecision-Making: ApplicationtoPretrialPublicSafety
Assessment. arXiv:2012.02845 [cs, stat]. (Cited on page 3.)
Imbens, Guido W. and Joshua D. Angrist (1994). Identification and Estimation of Local Average
Treatment Effects. Econometrica, 62(2):467. (Cited on pages 3 and 7.)
Kamenica,EmirandMatthewGentzkow(2011). BayesianPersuasion. American Economic Review,
101(6):2590–2615. (Cited on pages 4 and 5.)
24Lai, Vivian, Chacha Chen, Q. Vera Liao, Alison Smith-Renner, and Chenhao Tan (2021). Towards
a Science of Human-AI Decision Making: A Survey of Empirical Studies. arXiv:2112.11471 [cs].
(Cited on page 3.)
Lee, Sokbae and Bernard Salani´e (2018). Identifying effects of multivalued treatments. Economet-
rica, 86(6):1939–1963. (Cited on page 11.)
Li, Lan, Tina Lassiter, Joohee Oh, and Min Kyung Lee (2021). Algorithmic Hiring in Practice:
Recruiter and HR Professional’s Perspectives on AI Use in Hiring. In Proceedings of the 2021
AAAI/ACM Conference on AI, Ethics, and Society, AIES ’21, pages 166–176, New York, NY,
USA. Association for Computing Machinery. (Cited on pages 5 and 11.)
Manski, Charles F. (2021). Econometrics for Decision Making: Building Foundations Sketched by
Haavelmo and Wald. Econometrica, 89(6):2827–2853. (Cited on page 4.)
Manski, Charles F. (2023). Probabilistic prediction for binary treatment choice: With focus on
personalized medicine. Journal of Econometrics, 234(2):647–663. (Cited on page 4.)
Maron, Roman C., Jochen S. Utikal, Achim Hekler, Axel Hauschild, Elke Sattler, Wiebke Son-
dermann, Sebastian Haferkamp, Bastian Schilling, Markus V. Heppt, Philipp Jansen, Markus
Reinholz, Cindy Franklin, Laurenz Schmitt, Daniela Hartmann, Eva Krieghoff-Henning, Max
Schmitt, Michael Weichenthal, Christof von Kalle, Stefan Fr¨ohling, and Titus J. Brinker (2020).
Artificial Intelligence and Its Effect on Dermatologists’ Accuracy in Dermoscopic Melanoma Im-
ageClassification: Web-BasedSurveyStudy. Journal of Medical Internet Research,22(9):e18091.
(Cited on page 3.)
McLaughlin, Bryce and Jann Spiess (2022). Algorithmic Assistance with Recommendation-
Dependent Preferences. arXiv:2208.07626 [cs, econ, q-fin]. (Cited on pages 4 and 6.)
Mills, Chris and Marie-Pascale Grimon (2022). The Impact of Algorithmic Tools on Child Protec-
tion: Evidence from a Randomized Controlled Trial. (Cited on page 3.)
Mogstad, Magne and Alexander Torgovitsky (2018). Identification and Extrapolation of Causal
Effects with Instrumental Variables. Annual Review of Economics, 10(1):577–613. (Cited on
page 4.)
Noti, Gali and Yiling Chen (2023). Learning When to Advise Human Decision Makers. In Pro-
ceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages
3038–3048, Macau, SAR China. International Joint Conferences on Artificial Intelligence Orga-
nization. (Cited on pages 4 and 6.)
Orfanoudaki, Agni, Soroush Saghafian, Karen Song, Harini A. Chakkera, and Curtiss B. Cook
(2022). Algorithm, Human, or the Centaur: How to Enhance Clinical Care? (Cited on page 4.)
25Raghavan, Manish, Solon Barocas, Jon Kleinberg, and Karen Levy (2020). Mitigating bias in
algorithmic hiring: evaluating claims and practices. In Proceedings of the 2020 Conference on
Fairness, Accountability, and Transparency, FAT* ’20, pages 469–481, New York, NY, USA.
Association for Computing Machinery. (Cited on pages 2 and 6.)
Raghu, Maithra, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, and Sendhil Mul-
lainathan (2019). The Algorithmic Automation Problem: Prediction, Triage, and Human Effort.
arXiv:1903.12220 [cs]. (Cited on pages 8 and 9.)
Rubin, Donald (1974). Estimating causal effects of treatments in randomized and nonrandomized
studies. Journal of Educational Psychology, 66(5):688–701. (Cited on page 7.)
Saghafian, Soroush (2023). Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning
Approach. Management Science, page mnsc.2022.00883. (Cited on page 4.)
Spiess, Jann (2022). Optimal Estimation when Researcher and Social Preferences are Misaligned.
(Cited on page 4.)
Stevenson, Megan T. and Jennifer L. Doleac (2022). Algorithmic Risk Assessment in the Hands of
Humans. (Cited on page 3.)
Sun, Jiankun, Dennis J. Zhang, Haoyuan Hu, and Jan A. Van Mieghem (2022). Predicting Human
Discretion to Adjust Algorithmic Prescription: A Large-Scale Field Experiment in Warehouse
Operations. Management Science, 68(2):846–865. (Cited on page 4.)
Sutton, Reed T., David Pincock, Daniel C. Baumgart, Daniel C. Sadowski, Richard N. Fedorak,
and Karen I. Kroeker (2020). An overview of clinical decision support systems: benefits, risks,
and strategies for success. npj Digital Medicine, 3(1):1–10. (Cited on page 2.)
Vytlacil, Edward (2002). Independence, Monotonicity, and Latent Index Models: An Equivalence
Result. Econometrica, 70(1):331–341. (Cited on page 4.)
Wu, Yifan, Ziyang Guo, Michails Mamakos, Jason Hartline, and Jessica Hullman (2023). The
Rational Agent Benchmark for Data Visualization. arXiv:2304.03432 [cs]. (Cited on page 19.)
26Appendix
A Proofs
Proof of Proposition 1. Toshowthisresultwewillbuildabijectionbetweenthepotentialoutcomes
(D U(N;frec),D U(∅;frec),D U(H;frec))whichexistunderAssumption1andrealizationsof(D∅,C):
(N,N,N) ←→ (N,Ignore)
(N,N,H) ←→ (N,Comply)
(N,H,H) ←→ (H,Comply)
(H,H,H) ←→ (H,Ignore)
B Robustness Checks
Here we reproduce the results of Figure 4 and Figure 5 for the sub-populations who answered all of
their recommendation comprehension questions correctly (585 subjects) and those who answered
at least one question wrong (376 subjects). Subjects who answered at least one recommendation
comprehension question incorrectly exhibited noisier decisions on average than those who answered
all questions correctly. We find that algorithms generated using our framework exhibited higher
levels of complementarity for the sub-population who correctly answered all comprehension ques-
tions while the sub-population who answered at least one comprehension question wrong failed to
realize complementarity.
Figure6reportseachsub-populationsaverageperformancebytreatment. Bothsub-populations
performed equally well using the Predictive algorithm; however, only the sub-population which an-
swered all of our recommendation comprehension questions correctly was able to further improve
their performance using the Triage, Complementary, and Complementary Triage algorithms. Per-
forming well with these algorithms required subjects to effectively parse when the recommendation
was accurate or erroneous given their private information, thus we expect (and observe) that un-
derstanding how the algorithm generates recommendations is vital for its effective usage.
Figure 7 reports how each sub-populations propensity to hire applications varied across the
profiles they viewed. Generally subjects who answered at least one recommendation comprehen-
sion question wrong exhibited noisier decisions. This difference is particularly noticeable on Sales
profilesforsubjectsintheComplementary andComplementary Triage treatments. Otherthanthis
excess noise, subjects who answered at least one recommendation comprehension question wrong
responded to recommendations in a similar way to subjects who answered all recommendation
comprehension questions correctly.
27(a) Sub-population with complete comprehension
(b) Sub-population with comprehension errors
Figure 6: Average subject performance (through fractions of optimal hiring decisions) across treat-
ments. 95% bootstrap confidence intervals are clustered at the subject level. In addition, the triage
performance (where compliance is perfect and not sending a recommendation uses the decision of
Control) is given for each treatment by a gray dot, while the Response Effect from Section 3.2
is shown by the vertical red dashed lines. The optimal algorithm performance is shown by the
gray horizontal line. The optimal performance achievable by a rational agent with the subject’s
knowledge (role and algorithmic recommendation) is given for each treatment arm as a black dot.
28(a) Sub-population with complete comprehension
(b) Sub-population with comprehension errors
Figure 7: Frequency of experiment subjects taking a hire decision (y-axis) across the different
combinations of treatments (color) and profiles (x-axis). Different markers are used to identify
whether the subject received a hire recommendation (⇑), did not receive a recommendation (−),
or received a not-hire recommendation (⇓). The optimal decision given oracle information is given
by the location of the points on the lines y = 1 (hire is optimal) and y = 0 (not hire is optimal).
The dashed lines give the frequency of an applicant being hired by a subject without access to
recommendations (Control arm). The legend for these graphs is the same as the one in Figure 5
in the main text.
29