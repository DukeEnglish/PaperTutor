[
    {
        "title": "Causal Influence in Federated Edge Inference",
        "authors": "Mert KayaalpYunus InanVisa KoivunenAli H. Sayed",
        "links": "http://arxiv.org/abs/2405.01260v1",
        "entry_id": "http://arxiv.org/abs/2405.01260v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01260v1",
        "summary": "In this paper, we consider a setting where heterogeneous agents with\nconnectivity are performing inference using unlabeled streaming data. Observed\ndata are only partially informative about the target variable of interest. In\norder to overcome the uncertainty, agents cooperate with each other by\nexchanging their local inferences with and through a fusion center. To evaluate\nhow each agent influences the overall decision, we adopt a causal framework in\norder to distinguish the actual influence of agents from mere correlations\nwithin the decision-making process. Various scenarios reflecting different\nagent participation patterns and fusion center policies are investigated. We\nderive expressions to quantify the causal impact of each agent on the joint\ndecision, which could be beneficial for anticipating and addressing atypical\nscenarios, such as adversarial attacks or system malfunctions. We validate our\ntheoretical results with numerical simulations and a real-world application of\nmulti-camera crowd counting.",
        "updated": "2024-05-02 13:06:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01260v1"
    },
    {
        "title": "CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications",
        "authors": "Jan BlumenkampSteven MoradJennifer GielisAmanda Prorok",
        "links": "http://arxiv.org/abs/2405.01107v1",
        "entry_id": "http://arxiv.org/abs/2405.01107v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01107v1",
        "summary": "Spatial understanding from vision is crucial for robots operating in\nunstructured environments. In the real world, spatial understanding is often an\nill-posed problem. There are a number of powerful classical methods that\naccurately regress relative pose, however, these approaches often lack the\nability to leverage data-derived priors to resolve ambiguities. In multi-robot\nsystems, these challenges are exacerbated by the need for accurate and frequent\nposition estimates of cooperating agents. To this end, we propose CoViS-Net, a\ncooperative, multi-robot, visual spatial foundation model that learns spatial\npriors from data. Unlike prior work evaluated primarily on offline datasets, we\ndesign our model specifically for online evaluation and real-world deployment\non cooperative robots. Our model is completely decentralized, platform\nagnostic, executable in real-time using onboard compute, and does not require\nexisting network infrastructure. In this work, we focus on relative pose\nestimation and local Bird's Eye View (BEV) prediction tasks. Unlike classical\napproaches, we show that our model can accurately predict relative poses\nwithout requiring camera overlap, and predict BEVs of regions not visible to\nthe ego-agent. We demonstrate our model on a multi-robot formation control task\noutside the confines of the laboratory.",
        "updated": "2024-05-02 09:14:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01107v1"
    },
    {
        "title": "Computing Threshold Circuits with Bimolecular Void Reactions in Step Chemical Reaction Networks",
        "authors": "Rachel AndersonBin FuAiden MassieGourab MukhopadhyayAdrian SalinasRobert SchwellerEvan TomaiTim Wylie",
        "links": "http://arxiv.org/abs/2405.00940v1",
        "entry_id": "http://arxiv.org/abs/2405.00940v1",
        "pdf_url": "http://arxiv.org/pdf/2405.00940v1",
        "summary": "Step Chemical Reaction Networks (step CRNs) are an augmentation of the\nChemical Reaction Network (CRN) model where additional species may be\nintroduced to the system in a sequence of ``steps.'' We study step CRN systems\nusing a weak subset of reaction rules, \\emph{void} rules, in which molecular\nspecies can only be deleted. We demonstrate that step CRNs with only void rules\nof size (2,0) can simulate threshold formulas (TFs) under linear resources.\nThese limited systems can also simulate threshold \\emph{circuits} (TCs) by\nmodifying the volume of the system to be exponential. We then prove a matching\nexponential lower bound on the required volume for simulating threshold\ncircuits in a step CRN with (2,0)-size rules under a restricted\n\\emph{gate-wise} simulation, thus showing our construction is optimal for\nsimulating circuits in this way.",
        "updated": "2024-05-02 01:55:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.00940v1"
    },
    {
        "title": "Verification of Population Protocols with Unordered Data",
        "authors": "Steffen van BergeremRoland GuttenbergSandra KieferCorto MascleNicolas WaldburgerChana Weil-Kennedy",
        "links": "http://arxiv.org/abs/2405.00921v1",
        "entry_id": "http://arxiv.org/abs/2405.00921v1",
        "pdf_url": "http://arxiv.org/pdf/2405.00921v1",
        "summary": "Population protocols are a well-studied model of distributed computation in\nwhich a group of anonymous finite-state agents communicates via pairwise\ninteractions. Together they decide whether their initial configuration, that\nis, the initial distribution of agents in the states, satisfies a property. As\nan extension in order to express properties of multisets over an infinite data\ndomain, Blondin and Ladouceur (ICALP'23) introduced population protocols with\nunordered data (PPUD). In PPUD, each agent carries a fixed data value, and the\ninteractions between agents depend on whether their data are equal or not.\nBlondin and Ladouceur also identified the interesting subclass of immediate\nobservation PPUD (IOPPUD), where in every transition one of the two agents\nremains passive and does not move, and they characterised its expressive power.\n  We study the decidability and complexity of formally verifying these\nprotocols. The main verification problem for population protocols is\nwell-specification, that is, checking whether the given PPUD computes some\nfunction. We show that well-specification is undecidable in general. By\ncontrast, for IOPPUD, we exhibit a large yet natural class of problems, which\nincludes well-specification among other classic problems, and establish that\nthese problems are in EXPSPACE. We also provide a lower complexity bound,\nnamely coNEXPTIME-hardness.",
        "updated": "2024-05-02 00:32:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.00921v1"
    },
    {
        "title": "MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure",
        "authors": "Zhicheng ZhangYancheng LiangYi WuFei Fang",
        "links": "http://arxiv.org/abs/2405.00902v1",
        "entry_id": "http://arxiv.org/abs/2405.00902v1",
        "pdf_url": "http://arxiv.org/pdf/2405.00902v1",
        "summary": "Multi-agent reinforcement learning (MARL) algorithms often struggle to find\nstrategies close to Pareto optimal Nash Equilibrium, owing largely to the lack\nof efficient exploration. The problem is exacerbated in sparse-reward settings,\ncaused by the larger variance exhibited in policy learning. This paper\nintroduces MESA, a novel meta-exploration method for cooperative multi-agent\nlearning. It learns to explore by first identifying the agents' high-rewarding\njoint state-action subspace from training tasks and then learning a set of\ndiverse exploration policies to \"cover\" the subspace. These trained exploration\npolicies can be integrated with any off-policy MARL algorithm for test-time\ntasks. We first showcase MESA's advantage in a multi-step matrix game.\nFurthermore, experiments show that with learned exploration policies, MESA\nachieves significantly better performance in sparse-reward tasks in several\nmulti-agent particle environments and multi-agent MuJoCo environments, and\nexhibits the ability to generalize to more challenging tasks at test time.",
        "updated": "2024-05-01 23:19:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.00902v1"
    }
]