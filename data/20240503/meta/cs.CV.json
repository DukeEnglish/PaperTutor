[
    {
        "title": "Multi-Space Alignments Towards Universal LiDAR Segmentation",
        "authors": "Youquan LiuLingdong KongXiaoyang WuRunnan ChenXin LiLiang PanZiwei LiuYuexin Ma",
        "links": "http://arxiv.org/abs/2405.01538v1",
        "entry_id": "http://arxiv.org/abs/2405.01538v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01538v1",
        "summary": "A unified and versatile LiDAR segmentation model with strong robustness and\ngeneralizability is desirable for safe autonomous driving perception. This work\npresents M3Net, a one-of-a-kind framework for fulfilling multi-task,\nmulti-dataset, multi-modality LiDAR segmentation in a universal manner using\njust a single set of parameters. To better exploit data volume and diversity,\nwe first combine large-scale driving datasets acquired by different types of\nsensors from diverse scenes and then conduct alignments in three spaces, namely\ndata, feature, and label spaces, during the training. As a result, M3Net is\ncapable of taming heterogeneous data for training state-of-the-art LiDAR\nsegmentation models. Extensive experiments on twelve LiDAR segmentation\ndatasets verify our effectiveness. Notably, using a shared set of parameters,\nM3Net achieves 75.1%, 83.1%, and 72.4% mIoU scores, respectively, on the\nofficial benchmarks of SemanticKITTI, nuScenes, and Waymo Open.",
        "updated": "2024-05-02 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01538v1"
    },
    {
        "title": "Customizing Text-to-Image Models with a Single Image Pair",
        "authors": "Maxwell JonesSheng-Yu WangNupur KumariDavid BauJun-Yan Zhu",
        "links": "http://arxiv.org/abs/2405.01536v1",
        "entry_id": "http://arxiv.org/abs/2405.01536v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01536v1",
        "summary": "Art reinterpretation is the practice of creating a variation of a reference\nwork, making a paired artwork that exhibits a distinct artistic style. We ask\nif such an image pair can be used to customize a generative model to capture\nthe demonstrated stylistic difference. We propose Pair Customization, a new\ncustomization method that learns stylistic difference from a single image pair\nand then applies the acquired style to the generation process. Unlike existing\nmethods that learn to mimic a single concept from a collection of images, our\nmethod captures the stylistic difference between paired images. This allows us\nto apply a stylistic change without overfitting to the specific image content\nin the examples. To address this new task, we employ a joint optimization\nmethod that explicitly separates the style and content into distinct LoRA\nweight spaces. We optimize these style and content weights to reproduce the\nstyle and content images while encouraging their orthogonality. During\ninference, we modify the diffusion process via a new style guidance based on\nour learned weights. Both qualitative and quantitative experiments show that\nour method can effectively learn style while avoiding overfitting to image\ncontent, highlighting the potential of modeling such stylistic differences from\na single image pair.",
        "updated": "2024-05-02 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01536v1"
    },
    {
        "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks",
        "authors": "Murtaza DalalTarun ChiruvoluDevendra ChaplotRuslan Salakhutdinov",
        "links": "http://arxiv.org/abs/2405.01534v1",
        "entry_id": "http://arxiv.org/abs/2405.01534v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01534v1",
        "summary": "Large Language Models (LLMs) have been shown to be capable of performing\nhigh-level planning for long-horizon robotics tasks, yet existing methods\nrequire access to a pre-defined skill library (e.g. picking, placing, pulling,\npushing, navigating). However, LLM planning does not address how to design or\nlearn those behaviors, which remains challenging particularly in long-horizon\nsettings. Furthermore, for many tasks of interest, the robot needs to be able\nto adjust its behavior in a fine-grained manner, requiring the agent to be\ncapable of modifying low-level control actions. Can we instead use the\ninternet-scale knowledge from LLMs for high-level policies, guiding\nreinforcement learning (RL) policies to efficiently solve robotic control tasks\nonline without requiring a pre-determined set of skills? In this paper, we\npropose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to\nbridge the gap between abstract language and learned low-level control for\nsolving long-horizon robotics tasks from scratch. We demonstrate that PSL\nachieves state-of-the-art results on over 25 challenging robotics tasks with up\nto 10 stages. PSL solves long-horizon tasks from raw visual input spanning four\nbenchmarks at success rates of over 85%, out-performing language-based,\nclassical, and end-to-end approaches. Video results and code at\nhttps://mihdalal.github.io/planseqlearn/",
        "updated": "2024-05-02 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01534v1"
    },
    {
        "title": "OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning",
        "authors": "Shihao WangZhiding YuXiaohui JiangShiyi LanMin ShiNadine ChangJan KautzYing LiJose M. Alvarez",
        "links": "http://arxiv.org/abs/2405.01533v1",
        "entry_id": "http://arxiv.org/abs/2405.01533v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01533v1",
        "summary": "The advances in multimodal large language models (MLLMs) have led to growing\ninterests in LLM-based autonomous driving agents to leverage their strong\nreasoning capabilities. However, capitalizing on MLLMs' strong reasoning\ncapabilities for improved planning behavior is challenging since planning\nrequires full 3D situational awareness beyond 2D reasoning. To address this\nchallenge, our work proposes a holistic framework for strong alignment between\nagent models and 3D driving tasks. Our framework starts with a novel 3D MLLM\narchitecture that uses sparse queries to lift and compress visual\nrepresentations into 3D before feeding them into an LLM. This query-based\nrepresentation allows us to jointly encode dynamic objects and static map\nelements (e.g., traffic lanes), providing a condensed world model for\nperception-action alignment in 3D. We further propose OmniDrive-nuScenes, a new\nvisual question-answering dataset challenging the true 3D situational awareness\nof a model with comprehensive visual question-answering (VQA) tasks, including\nscene description, traffic regulation, 3D grounding, counterfactual reasoning,\ndecision making and planning. Extensive studies show the effectiveness of the\nproposed architecture as well as the importance of the VQA tasks for reasoning\nand planning in complex 3D scenes.",
        "updated": "2024-05-02 17:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01533v1"
    },
    {
        "title": "Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models",
        "authors": "Nishad SinghiJae Myung KimKarsten RothZeynep Akata",
        "links": "http://arxiv.org/abs/2405.01531v1",
        "entry_id": "http://arxiv.org/abs/2405.01531v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01531v1",
        "summary": "Concept Bottleneck Models (CBMs) ground image classification on\nhuman-understandable concepts to allow for interpretable model decisions.\nCrucially, the CBM design inherently allows for human interventions, in which\nexpert users are given the ability to modify potentially misaligned concept\nchoices to influence the decision behavior of the model in an interpretable\nfashion. However, existing approaches often require numerous human\ninterventions per image to achieve strong performances, posing practical\nchallenges in scenarios where obtaining human feedback is expensive. In this\npaper, we find that this is noticeably driven by an independent treatment of\nconcepts during intervention, wherein a change of one concept does not\ninfluence the use of other ones in the model's final decision. To address this\nissue, we introduce a trainable concept intervention realignment module, which\nleverages concept relations to realign concept assignments post-intervention.\nAcross standard, real-world benchmarks, we find that concept realignment can\nsignificantly improve intervention efficacy; significantly reducing the number\nof interventions needed to reach a target classification performance or concept\nprediction accuracy. In addition, it easily integrates into existing\nconcept-based architectures without requiring changes to the models themselves.\nThis reduced cost of human-model collaboration is crucial to enhancing the\nfeasibility of CBMs in resource-constrained environments.",
        "updated": "2024-05-02 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01531v1"
    }
]