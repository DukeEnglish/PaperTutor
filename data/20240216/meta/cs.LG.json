[
    {
        "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability",
        "authors": "Siwei YangBingchen ZhaoCihang Xie",
        "links": "http://arxiv.org/abs/2402.09404v1",
        "entry_id": "http://arxiv.org/abs/2402.09404v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09404v1",
        "summary": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol -- for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves. We comprehensively\nbuild AQA-Bench with three different algorithms, namely binary search,\ndepth-first search, and breadth-first search, and to evaluate the sequential\nreasoning ability of 12 different LLMs. Our investigations reveal several\ninteresting findings: (1) Closed-source models like GPT-4 and Gemini generally\nshow strong sequential reasoning ability, significantly outperforming\nopen-source LLMs. (2) Naively providing interactive examples may inadvertently\nhurt few-shot performance. (3) A very limited number of predecessor steps\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The scaling correlation between performance and model size is not always\nsignificant, sometimes even showcasing an inverse trend. We hope our study can\ncatalyze future work on advancing the understanding and enhancement of LLMs'\ncapabilities in sequential reasoning. The code is available at\nhttps://github.com/UCSC-VLAA/AQA-Bench.",
        "updated": "2024-02-14 18:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09404v1"
    },
    {
        "title": "Reinforcement Learning from Human Feedback with Active Queries",
        "authors": "Kaixuan JiJiafan HeQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.09401v1",
        "entry_id": "http://arxiv.org/abs/2402.09401v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09401v1",
        "summary": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret\nbound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the\ndimension of feature space and $\\Delta$ is the sub-optimality gap over all the\ncontexts. We then propose ADPO, a practical version of our algorithm based on\ndirect preference optimization (DPO) and apply it to fine-tuning LLMs. Our\nexperiments show that ADPO, while only making about half of queries for human\npreference, matches the performance of the state-of-the-art DPO method.",
        "updated": "2024-02-14 18:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09401v1"
    },
    {
        "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
        "authors": "Harry DongXinyu YangZhenyu ZhangZhangyang WangYuejie ChiBeidi Chen",
        "links": "http://arxiv.org/abs/2402.09398v1",
        "entry_id": "http://arxiv.org/abs/2402.09398v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09398v1",
        "summary": "Many computational factors limit broader deployment of large language models.\nIn this paper, we focus on a memory bottleneck imposed by the key-value (KV)\ncache, a computational shortcut that requires storing previous KV pairs during\ndecoding. While existing KV cache methods approach this problem by pruning or\nevicting large swaths of relatively less important KV pairs to dramatically\nreduce the memory footprint of the cache, they can have limited success in\ntasks that require recollecting a majority of previous tokens. To alleviate\nthis issue, we propose LESS, a simple integration of a (nearly free) constant\nsized cache with eviction-based cache methods, such that all tokens can be\nqueried at later decoding steps. Its ability to retain information throughout\ntime shows merit on a variety of tasks where we demonstrate LESS can help\nreduce the performance gap from caching everything, sometimes even matching it,\nall while being efficient.",
        "updated": "2024-02-14 18:54:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09398v1"
    },
    {
        "title": "Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning",
        "authors": "Allen M. WangOswin SoCharles DawsonDarren T. GarnierCristina ReaChuchu Fan",
        "links": "http://arxiv.org/abs/2402.09387v1",
        "entry_id": "http://arxiv.org/abs/2402.09387v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09387v1",
        "summary": "The tokamak offers a promising path to fusion energy, but plasma disruptions\npose a major economic risk, motivating considerable advances in disruption\navoidance. This work develops a reinforcement learning approach to this problem\nby training a policy to safely ramp-down the plasma current while avoiding\nlimits on a number of quantities correlated with disruptions. The policy\ntraining environment is a hybrid physics and machine learning model trained on\nsimulations of the SPARC primary reference discharge (PRD) ramp-down, an\nupcoming burning plasma scenario which we use as a testbed. To address physics\nuncertainty and model inaccuracies, the simulation environment is massively\nparallelized on GPU with randomized physics parameters during policy training.\nThe trained policy is then successfully transferred to a higher fidelity\nsimulator where it successfully ramps down the plasma while avoiding\nuser-specified disruptive limits. We also address the crucial issue of safety\ncriticality by demonstrating that a constraint-conditioned policy can be used\nas a trajectory design assistant to design a library of feed-forward\ntrajectories to handle different physics conditions and user settings. As a\nlibrary of trajectories is more interpretable and verifiable offline, we argue\nsuch an approach is a promising path for leveraging the capabilities of\nreinforcement learning in the safety-critical context of burning plasma\ntokamaks. Finally, we demonstrate how the training environment can be a useful\nplatform for other feed-forward optimization approaches by using an\nevolutionary algorithm to perform optimization of feed-forward trajectories\nthat are robust to physics uncertainty",
        "updated": "2024-02-14 18:37:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09387v1"
    },
    {
        "title": "GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly",
        "authors": "Ali AzizpourAdvait BalajiTodd J. TreangenSantiago Segarra",
        "links": "http://arxiv.org/abs/2402.09381v1",
        "entry_id": "http://arxiv.org/abs/2402.09381v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09381v1",
        "summary": "Repetitive DNA (repeats) poses significant challenges for accurate and\nefficient genome assembly and sequence alignment. This is particularly true for\nmetagenomic data, where genome dynamics such as horizontal gene transfer, gene\nduplication, and gene loss/gain complicate accurate genome assembly from\nmetagenomic communities. Detecting repeats is a crucial first step in\novercoming these challenges. To address this issue, we propose GraSSRep, a\nnovel approach that leverages the assembly graph's structure through graph\nneural networks (GNNs) within a self-supervised learning framework to classify\nDNA sequences into repetitive and non-repetitive categories. Specifically, we\nframe this problem as a node classification task within a metagenomic assembly\ngraph. In a self-supervised fashion, we rely on a high-precision (but\nlow-recall) heuristic to generate pseudo-labels for a small proportion of the\nnodes. We then use those pseudo-labels to train a GNN embedding and a random\nforest classifier to propagate the labels to the remaining nodes. In this way,\nGraSSRep combines sequencing features with pre-defined and learned graph\nfeatures to achieve state-of-the-art performance in repeat detection. We\nevaluate our method using simulated and synthetic metagenomic datasets. The\nresults on the simulated data highlight our GraSSRep's robustness to repeat\nattributes, demonstrating its effectiveness in handling the complexity of\nrepeated sequences. Additionally, our experiments with synthetic metagenomic\ndatasets reveal that incorporating the graph structure and the GNN enhances our\ndetection performance. Finally, in comparative analyses, GraSSRep outperforms\nexisting repeat detection tools with respect to precision and recall.",
        "updated": "2024-02-14 18:26:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09381v1"
    }
]