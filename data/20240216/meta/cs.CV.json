[
    {
        "title": "Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge",
        "authors": "Jiancheng YangRui ShiLiang JinXiaoyang HuangKaiming KuangDonglai WeiShixuan GuJianying LiuPengfei LiuZhizhong ChaiYongjie XiaoHao ChenLiming XuBang DuXiangyi YanHao TangAdam AlessioGregory HolsteJiapeng ZhangXiaoming WangJianye HeLixuan CheHanspeter PfisterMing LiBingbing Ni",
        "links": "http://arxiv.org/abs/2402.09372v1",
        "entry_id": "http://arxiv.org/abs/2402.09372v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09372v1",
        "summary": "Rib fractures are a common and potentially severe injury that can be\nchallenging and labor-intensive to detect in CT scans. While there have been\nefforts to address this field, the lack of large-scale annotated datasets and\nevaluation benchmarks has hindered the development and validation of deep\nlearning algorithms. To address this issue, the RibFrac Challenge was\nintroduced, providing a benchmark dataset of over 5,000 rib fractures from 660\nCT scans, with voxel-level instance mask annotations and diagnosis labels for\nfour clinical categories (buckle, nondisplaced, displaced, or segmental). The\nchallenge includes two tracks: a detection (instance segmentation) track\nevaluated by an FROC-style metric and a classification track evaluated by an\nF1-style metric. During the MICCAI 2020 challenge period, 243 results were\nevaluated, and seven teams were invited to participate in the challenge\nsummary. The analysis revealed that several top rib fracture detection\nsolutions achieved performance comparable or even better than human experts.\nNevertheless, the current rib fracture classification solutions are hardly\nclinically applicable, which can be an interesting area in the future. As an\nactive benchmark and research resource, the data and online evaluation of the\nRibFrac Challenge are available at the challenge website. As an independent\ncontribution, we have also extended our previous internal baseline by\nincorporating recent advancements in large-scale pretrained networks and\npoint-based rib segmentation techniques. The resulting FracNet+ demonstrates\ncompetitive performance in rib fracture detection, which lays a foundation for\nfurther research and development in AI-assisted rib fracture detection and\ndiagnosis.",
        "updated": "2024-02-14 18:18:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09372v1"
    },
    {
        "title": "Magic-Me: Identity-Specific Video Customized Diffusion",
        "authors": "Ze MaDaquan ZhouChun-Hsiao YehXue-She WangXiuyu LiHuanrui YangZhen DongKurt KeutzerJiashi Feng",
        "links": "http://arxiv.org/abs/2402.09368v1",
        "entry_id": "http://arxiv.org/abs/2402.09368v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09368v1",
        "summary": "Creating content for a specific identity (ID) has shown significant interest\nin the field of generative models. In the field of text-to-image generation\n(T2I), subject-driven content generation has achieved great progress with the\nID in the images controllable. However, extending it to video generation is not\nwell explored. In this work, we propose a simple yet effective subject identity\ncontrollable video generation framework, termed Video Custom Diffusion (VCD).\nWith a specified subject ID defined by a few images, VCD reinforces the\nidentity information extraction and injects frame-wise correlation at the\ninitialization stage for stable video outputs with identity preserved to a\nlarge extent. To achieve this, we propose three novel components that are\nessential for high-quality ID preservation: 1) an ID module trained with the\ncropped identity by prompt-to-segmentation to disentangle the ID information\nand the background noise for more accurate ID token learning; 2) a\ntext-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better\ninter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD\nmodules to deblur the face and upscale the video for higher resolution.\n  Despite its simplicity, we conducted extensive experiments to verify that VCD\nis able to generate stable and high-quality videos with better ID over the\nselected strong baselines. Besides, due to the transferability of the ID\nmodule, VCD is also working well with finetuned text-to-image models available\npublically, further improving its usability. The codes are available at\nhttps://github.com/Zhen-Dong/Magic-Me.",
        "updated": "2024-02-14 18:13:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09368v1"
    },
    {
        "title": "Prediction of Activated Sludge Settling Characteristics from Microscopy Images with Deep Convolutional Neural Networks and Transfer Learning",
        "authors": "Sina BorzooeiLeonardo ScabiniGisele MirandaSaba DaneshgarLukas DeblieckPiet De LangheOdemir BrunoBernard De BaetsIngmar NopensElena Torfs",
        "links": "http://arxiv.org/abs/2402.09367v1",
        "entry_id": "http://arxiv.org/abs/2402.09367v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09367v1",
        "summary": "Microbial communities play a key role in biological wastewater treatment\nprocesses. Activated sludge settling characteristics, for example, are affected\nby microbial community composition, varying by changes in operating conditions\nand influent characteristics of wastewater treatment plants (WWTPs). Timely\nassessment and prediction of changes in microbial composition leading to\nsettling problems, such as filamentous bulking (FB), can prevent operational\nchallenges, reductions in treatment efficiency, and adverse environmental\nimpacts. This study presents an innovative computer vision-based approach to\nassess activated sludge-settling characteristics based on the morphological\nproperties of flocs and filaments in microscopy images. Implementing the\ntransfer learning of deep convolutional neural network (CNN) models, this\napproach aims to overcome the limitations of existing quantitative image\nanalysis techniques. The offline microscopy image dataset was collected over\ntwo years, with weekly sampling at a full-scale industrial WWTP in Belgium.\nMultiple data augmentation techniques were employed to enhance the\ngeneralizability of the CNN models. Various CNN architectures, including\nInception v3, ResNet18, ResNet152, ConvNeXt-nano, and ConvNeXt-S, were tested\nto evaluate their performance in predicting sludge settling characteristics.\nThe sludge volume index was used as the final prediction variable, but the\nmethod can easily be adjusted to predict any other settling metric of choice.\nThe results showed that the suggested CNN-based approach provides less\nlabour-intensive, objective, and consistent assessments, while transfer\nlearning notably minimises the training phase, resulting in a generalizable\nsystem that can be employed in real-time applications.",
        "updated": "2024-02-14 18:13:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09367v1"
    },
    {
        "title": "Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D Ultrasound Localization Microscopy",
        "authors": "Brice RaubyPaul XingJonathan PoréeMaxime GasseJean Provost",
        "links": "http://arxiv.org/abs/2402.09359v1",
        "entry_id": "http://arxiv.org/abs/2402.09359v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09359v1",
        "summary": "Ultrasound Localization Microscopy (ULM) is a non-invasive technique that\nallows for the imaging of micro-vessels in vivo, at depth and with a resolution\non the order of ten microns. ULM is based on the sub-resolution localization of\nindividual microbubbles injected in the bloodstream. Mapping the whole\nangioarchitecture requires the accumulation of microbubbles trajectories from\nthousands of frames, typically acquired over a few minutes. ULM acquisition\ntimes can be reduced by increasing the microbubble concentration, but requires\nmore advanced algorithms to detect them individually. Several deep learning\napproaches have been proposed for this task, but they remain limited to 2D\nimaging, in part due to the associated large memory requirements. Herein, we\npropose to use sparse tensor neural networks to reduce memory usage in 2D and\nto improve the scaling of the memory requirement for the extension of deep\nlearning architecture to 3D. We study several approaches to efficiently convert\nultrasound data into a sparse format and study the impact of the associated\nloss of information. When applied in 2D, the sparse formulation reduces the\nmemory requirements by a factor 2 at the cost of a small reduction of\nperformance when compared against dense networks. In 3D, the proposed approach\nreduces memory requirements by two order of magnitude while largely\noutperforming conventional ULM in high concentration settings. We show that\nSparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense\ndeep learning based method in 2D ULM i.e. the use of higher concentration in\nsilico and reduced acquisition time.",
        "updated": "2024-02-14 18:03:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09359v1"
    },
    {
        "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
        "authors": "Shih-Yang LiuChien-Yi WangHongxu YinPavlo MolchanovYu-Chiang Frank WangKwang-Ting ChengMin-Hung Chen",
        "links": "http://arxiv.org/abs/2402.09353v1",
        "entry_id": "http://arxiv.org/abs/2402.09353v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09353v1",
        "summary": "Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and\nits variants have gained considerable popularity because of avoiding additional\ninference costs. However, there still often exists an accuracy gap between\nthese methods and full fine-tuning (FT). In this work, we first introduce a\nnovel weight decomposition analysis to investigate the inherent differences\nbetween FT and LoRA. Aiming to resemble the learning capacity of FT from the\nfindings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA\ndecomposes the pre-trained weight into two components, magnitude and direction,\nfor fine-tuning, specifically employing LoRA for directional updates to\nefficiently minimize the number of trainable parameters. By employing DoRA, we\nenhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. DoRA consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding.",
        "updated": "2024-02-14 17:59:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09353v1"
    }
]