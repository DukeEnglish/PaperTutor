[
    {
        "title": "Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions",
        "authors": "Ruqing Xu",
        "links": "http://arxiv.org/abs/2402.09384v1",
        "entry_id": "http://arxiv.org/abs/2402.09384v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09384v1",
        "summary": "A principal designs an algorithm that generates a publicly observable\nprediction of a binary state. She must decide whether to act directly based on\nthe prediction or to delegate the decision to an agent with private information\nbut potential misalignment. We study the optimal design of the prediction\nalgorithm and the delegation rule in such environments. Three key findings\nemerge: (1) Delegation is optimal if and only if the principal would make the\nsame binary decision as the agent had she observed the agent's information. (2)\nProviding the most informative algorithm may be suboptimal even if the\nprincipal can act on the algorithm's prediction. Instead, the optimal algorithm\nmay provide more information about one state and restrict information about the\nother. (3) Common restrictions on algorithms, such as keeping a\n\"human-in-the-loop\" or requiring maximal prediction accuracy, strictly worsen\ndecision quality in the absence of perfectly aligned agents and state-revealing\nsignals. These findings predict the underperformance of human-machine\ncollaborations if no measures are taken to mitigate common preference\nmisalignment between algorithms and human decision-makers.",
        "updated": "2024-02-14 18:32:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09384v1"
    },
    {
        "title": "UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers",
        "authors": "Hong JiaYoung D. KwonDong MaNhat PhamLorena QendroTam VuCecilia Mascolo",
        "links": "http://arxiv.org/abs/2402.09264v1",
        "entry_id": "http://arxiv.org/abs/2402.09264v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09264v1",
        "summary": "Traditional machine learning techniques are prone to generating inaccurate\npredictions when confronted with shifts in the distribution of data between the\ntraining and testing phases. This vulnerability can lead to severe\nconsequences, especially in applications such as mobile healthcare. Uncertainty\nestimation has the potential to mitigate this issue by assessing the\nreliability of a model's output. However, existing uncertainty estimation\ntechniques often require substantial computational resources and memory, making\nthem impractical for implementation on microcontrollers (MCUs). This limitation\nhinders the feasibility of many important on-device wearable event detection\n(WED) applications, such as heart attack detection.\n  In this paper, we present UR2M, a novel Uncertainty and Resource-aware event\ndetection framework for MCUs. Specifically, we (i) develop an uncertainty-aware\nWED based on evidential theory for accurate event detection and reliable\nuncertainty estimation; (ii) introduce a cascade ML framework to achieve\nefficient model inference via early exits, by sharing shallower model layers\namong different event models; (iii) optimize the deployment of the model and\nMCU library for system efficiency. We conducted extensive experiments and\ncompared UR2M to traditional uncertainty baselines using three wearable\ndatasets. Our results demonstrate that UR2M achieves up to 864% faster\ninference speed, 857% energy-saving for uncertainty estimation, 55% memory\nsaving on two popular MCUs, and a 22% improvement in uncertainty quantification\nperformance.\n  UR2M can be deployed on a wide range of MCUs, significantly expanding\nreal-time and reliable WED applications.",
        "updated": "2024-02-14 15:51:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09264v1"
    },
    {
        "title": "Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support",
        "authors": "Zilin MaYiyang MeiYinru LongZhaoyuan SuKrzysztof Z. Gajos",
        "links": "http://dx.doi.org/10.1145/3613904.3642482",
        "entry_id": "http://arxiv.org/abs/2402.09260v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09260v1",
        "summary": "LGBTQ+ individuals are increasingly turning to chatbots powered by large\nlanguage models (LLMs) to meet their mental health needs. However, little\nresearch has explored whether these chatbots can adequately and safely provide\ntailored support for this demographic. We interviewed 18 LGBTQ+ and 13\nnon-LGBTQ+ participants about their experiences with LLM-based chatbots for\nmental health needs. LGBTQ+ participants relied on these chatbots for mental\nhealth support, likely due to an absence of support in real life. Notably,\nwhile LLMs offer prompt support, they frequently fall short in grasping the\nnuances of LGBTQ-specific challenges. Although fine-tuning LLMs to address\nLGBTQ+ needs can be a step in the right direction, it isn't the panacea. The\ndeeper issue is entrenched in societal discrimination. Consequently, we call on\nfuture researchers and designers to look beyond mere technical refinements and\nadvocate for holistic strategies that confront and counteract the societal\nbiases burdening the LGBTQ+ community.",
        "updated": "2024-02-14 15:48:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09260v1"
    },
    {
        "title": "Scaling the Authoring of AutoTutors with Large Language Models",
        "authors": "Sankalan Pal ChowdhuryVilém ZouharMrinmaya Sachan",
        "links": "http://arxiv.org/abs/2402.09216v1",
        "entry_id": "http://arxiv.org/abs/2402.09216v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09216v1",
        "summary": "Large Language Models (LLMs) have found several use cases in education,\nranging from automatic question generation to essay evaluation. In this paper,\nwe explore the potential of using Large Language Models (LLMs) to author\nIntelligent Tutoring Systems. A common pitfall of LLMs is their straying from\ndesired pedagogical strategies such as leaking the answer to the student, and\nin general, providing no guarantees. We posit that while LLMs with certain\nguardrails can take the place of subject experts, the overall pedagogical\ndesign still needs to be handcrafted for the best learning results. Based on\nthis principle, we create a sample end-to-end tutoring system named MWPTutor,\nwhich uses LLMs to fill in the state space of a pre-defined finite state\ntransducer. This approach retains the structure and the pedagogy of traditional\ntutoring systems that has been developed over the years by learning scientists\nbut brings in additional flexibility of LLM-based approaches. Through a human\nevaluation study on two datasets based on math word problems, we show that our\nhybrid approach achieves a better overall tutoring score than an instructed,\nbut otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the\nscope for the community to improve its performance by improving individual\nmodules or using different teaching strategies that it can follow",
        "updated": "2024-02-14 14:53:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09216v1"
    },
    {
        "title": "Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents",
        "authors": "Cheng QianBingxiang HeZhong ZhuangJia DengYujia QinXin CongZhong ZhangJie ZhouYankai LinZhiyuan LiuMaosong Sun",
        "links": "http://arxiv.org/abs/2402.09205v2",
        "entry_id": "http://arxiv.org/abs/2402.09205v2",
        "pdf_url": "http://arxiv.org/pdf/2402.09205v2",
        "summary": "Current language model-driven agents often lack mechanisms for effective user\nparticipation, which is crucial given the vagueness commonly found in user\ninstructions. Although adept at devising strategies and performing tasks, these\nagents struggle with seeking clarification and grasping precise user\nintentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a\nnovel benchmark designed to inspect users' implicit intentions through explicit\nqueries. Next, we propose the incorporation of model experts as the upstream in\nagent designs to enhance user-agent interaction. Employing IN3, we empirically\ntrain Mistral-Interact, a powerful model that proactively assesses task\nvagueness, inquires user intentions, and refines them into actionable goals\nbefore starting downstream agent task execution. Integrating it into the XAgent\nframework, we comprehensively evaluate the enhanced agent system regarding user\ninstruction understanding and execution, revealing that our approach notably\nexcels at identifying vague user tasks, recovering and summarizing critical\nmissing information, setting precise and necessary agent execution goals, and\nminimizing redundant tool usage, thus boosting overall efficiency. All the data\nand codes are released.",
        "updated": "2024-02-15 09:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09205v2"
    }
]