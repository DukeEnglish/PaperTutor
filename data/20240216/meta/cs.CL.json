[
    {
        "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability",
        "authors": "Siwei YangBingchen ZhaoCihang Xie",
        "links": "http://arxiv.org/abs/2402.09404v1",
        "entry_id": "http://arxiv.org/abs/2402.09404v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09404v1",
        "summary": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol -- for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves. We comprehensively\nbuild AQA-Bench with three different algorithms, namely binary search,\ndepth-first search, and breadth-first search, and to evaluate the sequential\nreasoning ability of 12 different LLMs. Our investigations reveal several\ninteresting findings: (1) Closed-source models like GPT-4 and Gemini generally\nshow strong sequential reasoning ability, significantly outperforming\nopen-source LLMs. (2) Naively providing interactive examples may inadvertently\nhurt few-shot performance. (3) A very limited number of predecessor steps\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The scaling correlation between performance and model size is not always\nsignificant, sometimes even showcasing an inverse trend. We hope our study can\ncatalyze future work on advancing the understanding and enhancement of LLMs'\ncapabilities in sequential reasoning. The code is available at\nhttps://github.com/UCSC-VLAA/AQA-Bench.",
        "updated": "2024-02-14 18:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09404v1"
    },
    {
        "title": "Reinforcement Learning from Human Feedback with Active Queries",
        "authors": "Kaixuan JiJiafan HeQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.09401v1",
        "entry_id": "http://arxiv.org/abs/2402.09401v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09401v1",
        "summary": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret\nbound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the\ndimension of feature space and $\\Delta$ is the sub-optimality gap over all the\ncontexts. We then propose ADPO, a practical version of our algorithm based on\ndirect preference optimization (DPO) and apply it to fine-tuning LLMs. Our\nexperiments show that ADPO, while only making about half of queries for human\npreference, matches the performance of the state-of-the-art DPO method.",
        "updated": "2024-02-14 18:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09401v1"
    },
    {
        "title": "Long-form evaluation of model editing",
        "authors": "Domenic RosatiRobie GonzalesJinkun ChenXuemin YuMelis ErkanYahya KayaniSatya Deepika ChavatapalliFrank RudziczHassan Sajjad",
        "links": "http://arxiv.org/abs/2402.09394v1",
        "entry_id": "http://arxiv.org/abs/2402.09394v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09394v1",
        "summary": "Evaluations of model editing currently only use the `next few token'\ncompletions after a prompt. As a result, the impact of these methods on longer\nnatural language generation is largely unknown. We introduce long-form\nevaluation of model editing (\\textbf{\\textit{LEME}}) a novel evaluation\nprotocol that measures the efficacy and impact of model editing in long-form\ngenerative settings. Our protocol consists of a machine-rated survey and a\nclassifier which correlates well with human ratings. Importantly, we find that\nour protocol has very little relationship with previous short-form metrics\n(despite being designed to extend efficacy, generalization, locality, and\nportability into a long-form setting), indicating that our method introduces a\nnovel set of dimensions for understanding model editing methods. Using this\nprotocol, we benchmark a number of model editing techniques and present several\nfindings including that, while some methods (ROME and MEMIT) perform well in\nmaking consistent edits within a limited scope, they suffer much more from\nfactual drift than other methods. Finally, we present a qualitative analysis\nthat illustrates common failure modes in long-form generative settings\nincluding internal consistency, lexical cohesion, and locality issues.",
        "updated": "2024-02-14 18:45:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09394v1"
    },
    {
        "title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
        "authors": "Botao YuFrazier N. BakerZiqi ChenXia NingHuan Sun",
        "links": "http://arxiv.org/abs/2402.09391v1",
        "entry_id": "http://arxiv.org/abs/2402.09391v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09391v1",
        "summary": "Chemistry plays a crucial role in many domains, such as drug discovery and\nmaterial science. While large language models (LLMs) such as GPT-4 exhibit\nremarkable capabilities on natural language processing tasks, existing work\nshows their performance on chemistry tasks is discouragingly low. In this\npaper, however, we demonstrate that our developed LLMs can achieve very strong\nresults on a comprehensive set of chemistry tasks, outperforming the most\nadvanced GPT-4 across all the tasks by a substantial margin and approaching the\nSoTA task-specific models. The key to our success is a large-scale,\ncomprehensive, high-quality dataset for instruction tuning named SMolInstruct.\nIt contains 14 meticulously selected chemistry tasks and over three million\nhigh-quality samples, laying a solid foundation for training and evaluating\nLLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source\nLLMs, among which, we find that Mistral serves as the best base model for\nchemistry tasks. We further conduct analysis on the impact of trainable\nparameters, providing insights for future research.",
        "updated": "2024-02-14 18:42:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09391v1"
    },
    {
        "title": "HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation",
        "authors": "Yihao FangStephen W. ThomasXiaodan Zhu",
        "links": "http://arxiv.org/abs/2402.09390v1",
        "entry_id": "http://arxiv.org/abs/2402.09390v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09390v1",
        "summary": "With the widespread adoption of large language models (LLMs) in numerous\napplications, the challenge of factuality and the propensity for hallucinations\nraises significant concerns. To address this issue, particularly in\nretrieval-augmented in-context learning, we introduce the hierarchical graph of\nthoughts (HGOT), a structured, multi-layered graph approach designed to enhance\nthe retrieval of pertinent passages during in-context learning. The framework\nutilizes the emergent planning capabilities of LLMs, employing the\ndivide-and-conquer strategy to break down complex queries into manageable\nsub-queries. It refines self-consistency majority voting for answer selection,\nwhich incorporates the recently proposed citation recall and precision metrics\nto assess the quality of thoughts, linking an answer's credibility\nintrinsically to the thought's quality. This methodology introduces a weighted\nsystem in majority voting, prioritizing answers based on the citation quality\nof their thoughts. Additionally, we propose a scoring mechanism for evaluating\nretrieved passages, considering factors such as citation frequency and quality,\nself-consistency confidence, and the retrieval module's ranking. Experiments\nreveal that HGOT outperforms other retrieval-augmented in-context learning\nmethods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and\nRetrieve-then-Read on different datasets by as much as $7\\%$, demonstrating its\nefficacy in enhancing the factuality of LLMs.",
        "updated": "2024-02-14 18:41:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09390v1"
    }
]