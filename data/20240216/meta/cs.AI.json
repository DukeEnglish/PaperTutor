[
    {
        "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability",
        "authors": "Siwei YangBingchen ZhaoCihang Xie",
        "links": "http://arxiv.org/abs/2402.09404v1",
        "entry_id": "http://arxiv.org/abs/2402.09404v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09404v1",
        "summary": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol -- for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves. We comprehensively\nbuild AQA-Bench with three different algorithms, namely binary search,\ndepth-first search, and breadth-first search, and to evaluate the sequential\nreasoning ability of 12 different LLMs. Our investigations reveal several\ninteresting findings: (1) Closed-source models like GPT-4 and Gemini generally\nshow strong sequential reasoning ability, significantly outperforming\nopen-source LLMs. (2) Naively providing interactive examples may inadvertently\nhurt few-shot performance. (3) A very limited number of predecessor steps\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The scaling correlation between performance and model size is not always\nsignificant, sometimes even showcasing an inverse trend. We hope our study can\ncatalyze future work on advancing the understanding and enhancement of LLMs'\ncapabilities in sequential reasoning. The code is available at\nhttps://github.com/UCSC-VLAA/AQA-Bench.",
        "updated": "2024-02-14 18:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09404v1"
    },
    {
        "title": "Reinforcement Learning from Human Feedback with Active Queries",
        "authors": "Kaixuan JiJiafan HeQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.09401v1",
        "entry_id": "http://arxiv.org/abs/2402.09401v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09401v1",
        "summary": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret\nbound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the\ndimension of feature space and $\\Delta$ is the sub-optimality gap over all the\ncontexts. We then propose ADPO, a practical version of our algorithm based on\ndirect preference optimization (DPO) and apply it to fine-tuning LLMs. Our\nexperiments show that ADPO, while only making about half of queries for human\npreference, matches the performance of the state-of-the-art DPO method.",
        "updated": "2024-02-14 18:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09401v1"
    },
    {
        "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
        "authors": "Harry DongXinyu YangZhenyu ZhangZhangyang WangYuejie ChiBeidi Chen",
        "links": "http://arxiv.org/abs/2402.09398v1",
        "entry_id": "http://arxiv.org/abs/2402.09398v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09398v1",
        "summary": "Many computational factors limit broader deployment of large language models.\nIn this paper, we focus on a memory bottleneck imposed by the key-value (KV)\ncache, a computational shortcut that requires storing previous KV pairs during\ndecoding. While existing KV cache methods approach this problem by pruning or\nevicting large swaths of relatively less important KV pairs to dramatically\nreduce the memory footprint of the cache, they can have limited success in\ntasks that require recollecting a majority of previous tokens. To alleviate\nthis issue, we propose LESS, a simple integration of a (nearly free) constant\nsized cache with eviction-based cache methods, such that all tokens can be\nqueried at later decoding steps. Its ability to retain information throughout\ntime shows merit on a variety of tasks where we demonstrate LESS can help\nreduce the performance gap from caching everything, sometimes even matching it,\nall while being efficient.",
        "updated": "2024-02-14 18:54:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09398v1"
    },
    {
        "title": "LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement Learning",
        "authors": "Adithya RamanBekir TurkkanTevfik Kosar",
        "links": "http://arxiv.org/abs/2402.09392v1",
        "entry_id": "http://arxiv.org/abs/2402.09392v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09392v1",
        "summary": "Over the recent years, research and development in adaptive bitrate (ABR)\nalgorithms for live video streaming have been successful in improving users'\nquality of experience (QoE) by reducing latency to near real-time levels while\ndelivering higher bitrate videos with minimal rebuffering time. However, the\nQoE models used by these ABR algorithms do not take into account that a large\nportion of live video streaming clients use mobile devices where a higher\nbitrate does not necessarily translate into higher perceived quality. Ignoring\nperceived quality results in playing videos at higher bitrates without a\nsignificant increase in perceptual video quality and becomes a burden for\nbattery-constrained mobile devices due to higher energy consumption. In this\npaper, we propose LL-GABR, a deep reinforcement learning approach that models\nthe QoE using perceived video quality instead of bitrate and uses energy\nconsumption along with other metrics like latency, rebuffering events, and\nsmoothness. LL-GABR makes no assumptions about the underlying video,\nenvironment, or network settings and can operate flexibly on different video\ntitles, each having a different bitrate encoding ladder without additional\nre-training, unlike existing learning-based ABRs. Trace-driven experimental\nresults show that LL-GABR outperforms the state-of-the-art approaches by up to\n44% in terms of perceptual QoE and a 73% increase in energy efficiency as a\nresult of reducing net energy consumption by 11%.",
        "updated": "2024-02-14 18:43:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09392v1"
    },
    {
        "title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
        "authors": "Botao YuFrazier N. BakerZiqi ChenXia NingHuan Sun",
        "links": "http://arxiv.org/abs/2402.09391v1",
        "entry_id": "http://arxiv.org/abs/2402.09391v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09391v1",
        "summary": "Chemistry plays a crucial role in many domains, such as drug discovery and\nmaterial science. While large language models (LLMs) such as GPT-4 exhibit\nremarkable capabilities on natural language processing tasks, existing work\nshows their performance on chemistry tasks is discouragingly low. In this\npaper, however, we demonstrate that our developed LLMs can achieve very strong\nresults on a comprehensive set of chemistry tasks, outperforming the most\nadvanced GPT-4 across all the tasks by a substantial margin and approaching the\nSoTA task-specific models. The key to our success is a large-scale,\ncomprehensive, high-quality dataset for instruction tuning named SMolInstruct.\nIt contains 14 meticulously selected chemistry tasks and over three million\nhigh-quality samples, laying a solid foundation for training and evaluating\nLLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source\nLLMs, among which, we find that Mistral serves as the best base model for\nchemistry tasks. We further conduct analysis on the impact of trainable\nparameters, providing insights for future research.",
        "updated": "2024-02-14 18:42:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09391v1"
    }
]