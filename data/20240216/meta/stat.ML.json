[
    {
        "title": "Reinforcement Learning from Human Feedback with Active Queries",
        "authors": "Kaixuan JiJiafan HeQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.09401v1",
        "entry_id": "http://arxiv.org/abs/2402.09401v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09401v1",
        "summary": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret\nbound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the\ndimension of feature space and $\\Delta$ is the sub-optimality gap over all the\ncontexts. We then propose ADPO, a practical version of our algorithm based on\ndirect preference optimization (DPO) and apply it to fine-tuning LLMs. Our\nexperiments show that ADPO, while only making about half of queries for human\npreference, matches the performance of the state-of-the-art DPO method.",
        "updated": "2024-02-14 18:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09401v1"
    },
    {
        "title": "Loss Shaping Constraints for Long-Term Time Series Forecasting",
        "authors": "Ignacio HounieJavier Porras-ValenzuelaAlejandro Ribeiro",
        "links": "http://arxiv.org/abs/2402.09373v1",
        "entry_id": "http://arxiv.org/abs/2402.09373v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09373v1",
        "summary": "Several applications in time series forecasting require predicting multiple\nsteps ahead. Despite the vast amount of literature in the topic, both classical\nand recent deep learning based approaches have mostly focused on minimising\nperformance averaged over the predicted window. We observe that this can lead\nto disparate distributions of errors across forecasting steps, especially for\nrecent transformer architectures trained on popular forecasting benchmarks.\nThat is, optimising performance on average can lead to undesirably large errors\nat specific time-steps. In this work, we present a Constrained Learning\napproach for long-term time series forecasting that aims to find the best model\nin terms of average performance that respects a user-defined upper bound on the\nloss at each time-step. We call our approach loss shaping constraints because\nit imposes constraints on the loss at each time step, and leverage recent\nduality results to show that despite its non-convexity, the resulting problem\nhas a bounded duality gap. We propose a practical Primal-Dual algorithm to\ntackle it, and demonstrate that the proposed approach exhibits competitive\naverage performance in time series forecasting benchmarks, while shaping the\ndistribution of errors across the predicted window.",
        "updated": "2024-02-14 18:20:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09373v1"
    },
    {
        "title": "Connecting Algorithmic Fairness to Quality Dimensions in Machine Learning in Official Statistics and Survey Production",
        "authors": "Patrick Oliver SchenkChristoph Kern",
        "links": "http://arxiv.org/abs/2402.09328v1",
        "entry_id": "http://arxiv.org/abs/2402.09328v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09328v1",
        "summary": "National Statistical Organizations (NSOs) increasingly draw on Machine\nLearning (ML) to improve the timeliness and cost-effectiveness of their\nproducts. When introducing ML solutions, NSOs must ensure that high standards\nwith respect to robustness, reproducibility, and accuracy are upheld as\ncodified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA;\nYung et al. 2022). At the same time, a growing body of research focuses on\nfairness as a pre-condition of a safe deployment of ML to prevent disparate\nsocial impacts in practice. However, fairness has not yet been explicitly\ndiscussed as a quality aspect in the context of the application of ML at NSOs.\nWe employ Yung et al. (2022)'s QF4SA quality framework and present a mapping of\nits quality dimensions to algorithmic fairness. We thereby extend the QF4SA\nframework in several ways: we argue for fairness as its own quality dimension,\nwe investigate the interaction of fairness with other dimensions, and we\nexplicitly address data, both on its own and its interaction with applied\nmethodology. In parallel with empirical illustrations, we show how our mapping\ncan contribute to methodology in the domains of official statistics,\nalgorithmic fairness, and trustworthy machine learning.",
        "updated": "2024-02-14 17:18:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09328v1"
    },
    {
        "title": "Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models",
        "authors": "Goutham RajendranSimon BuchholzBryon AragamBernhard SchölkopfPradeep Ravikumar",
        "links": "http://arxiv.org/abs/2402.09236v1",
        "entry_id": "http://arxiv.org/abs/2402.09236v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09236v1",
        "summary": "To build intelligent machine learning systems, there are two broad\napproaches. One approach is to build inherently interpretable models, as\nendeavored by the growing field of causal representation learning. The other\napproach is to build highly-performant foundation models and then invest\nefforts into understanding how they work. In this work, we relate these two\napproaches and study how to learn human-interpretable concepts from data.\nWeaving together ideas from both fields, we formally define a notion of\nconcepts and show that they can be provably recovered from diverse data.\nExperiments on synthetic data and large language models show the utility of our\nunified approach.",
        "updated": "2024-02-14 15:23:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09236v1"
    },
    {
        "title": "Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks",
        "authors": "Akshay KumarJarvis Haupt",
        "links": "http://arxiv.org/abs/2402.09226v1",
        "entry_id": "http://arxiv.org/abs/2402.09226v1",
        "pdf_url": "http://arxiv.org/pdf/2402.09226v1",
        "summary": "This paper examines gradient flow dynamics of two-homogeneous neural networks\nfor small initializations, where all weights are initialized near the origin.\nFor both square and logistic losses, it is shown that for sufficiently small\ninitializations, the gradient flow dynamics spend sufficient time in the\nneighborhood of the origin to allow the weights of the neural network to\napproximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a\nneural correlation function that quantifies the correlation between the output\nof the neural network and corresponding labels in the training data set. For\nsquare loss, it has been observed that neural networks undergo saddle-to-saddle\ndynamics when initialized close to the origin. Motivated by this, this paper\nalso shows a similar directional convergence among weights of small magnitude\nin the neighborhood of certain saddle points.",
        "updated": "2024-02-14 15:10:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.09226v1"
    }
]