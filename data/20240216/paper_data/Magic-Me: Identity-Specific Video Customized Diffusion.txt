Magic-Me: Identity-Specific Video Customized Diffusion
ZeMa*1 DaquanZhou*† 1 Chun-HsiaoYeh2 Xue-SheWang1 XiuyuLi2
HuanruiYang2 ZhenDong†2 KurtKeutzer2
JiashiFeng1
1 ByteDanceInc.
{ze.ma1, daquanzhou, xueshe.wang, jshfeng}@bytedance.com
2 UCBerkeley
{daniel yeh, xiuyu, huanrui, zhendong, keutzer}@berkeley.edu
Abstract 1.Introduction
Recent advancements in text-to-video (T2V) generation
Creating content for a specific identity (ID) has shown
[22–24,57,68,72]haveenabledthecreationofconsistent
significant interest in the field of generative models. In
andrealisticanimationsfromtextdescriptions,althoughthe
the field of text-to-image generation (T2I), subject-driven
precise control over the generated content remains a chal-
content generation has achieved great progress with the
lenge. In real-world applications, there is often a need for
ID in the images controllable. However, extending it to
the content generation with a specific identity guided by
video generation is not well explored. In this work, we
text-described contexts, a task known as identity-specific
propose a simple yet effective subject identity controllable
generation [31]. This is important in scenarios such as
video generation framework, termed Video Custom Diffu-
movie production, where a specific character needs to be
sion (VCD). With a specified subject ID defined by a few
animatedforparticularactions. Similarscenariosalsohap-
images,VCDreinforcestheidentityinformationextraction
pen in advertising, where a consistent product identity is
andinjectsframe-wisecorrelationattheinitializationstage
preservedacrossdifferentsettingsorenvironments.
for stable video outputs with identity preserved to a large
Tocontrolobjectidentityinvideogeneration,especially
extent. To achieve this, we propose three novel compo-
forhuman-relatedscenarios,stillremainsachallenge. Pre-
nentsthatareessentialforhigh-qualityIDpreservation: 1)
viousworks,oftenutilizingimagereferences,havefocused
anIDmoduletrainedwiththecroppedidentitybyprompt-
onstylesandmotions[64],andsomeofthemexplorecus-
to-segmentation to disentangle the ID information and the
tomizedgenerationthroughvideoediting[35]. Whilethese
background noise for more accurate ID token learning; 2)
approachesprovideholisticcontrolwithconditionssuchas
atext-to-video(T2V)VCDmodulewith3DGaussianNoise
reference images [41], reference videos [14, 64], or depth
Prior for better inter-frame consistency and 3) video-to-
maps[66]toswitchthestylesorgeneralappearance, their
video (V2V) Face VCD and Tiled VCD modules to deblur
focusisnotonidentity-specificcontrol. Asdepictedinthe
thefaceandupscalethevideoforhigherresolution.
firstrowofFigure2,traditionalT2Vmethod[9]augments
Despite its simplicity, we conducted extensive experi-
thegeneratedvideotofollowthereferenceimagewhilethe
mentstoverifythatVCDisabletogeneratestableandhigh-
subjectidentityisnotreflected.
qualityvideoswithbetterIDovertheselectedstrongbase-
In recent efforts toward ID-specific text-to-image (T2I)
lines. Besides, dueto the transferability of the ID module,
models[16,48],notableadvancementshavebeenachieved.
VCDisalsoworkingwellwithfinetunedtext-to-imagemod-
Thesemodelsleverageafewimagesassociatedwithades-
elsavailablepublically,furtherimprovingitsusability. The
ignatedID,fine-tuningapre-trainedT2Imodelwithalearn-
codesareavailableathttps://github.com/Zhen-
ableconcepttoken.Duringinference,thenetworkproduces
Dong/Magic-Me.
ID-specific images by incorporating the ID token into the
text descriptions. Extending this approach to video gener-
ationseemsintuitive,maintainingthesamepipelineovera
*Theseauthorscontributedequallytothiswork.
†CorrespondingAuthor videogenerationmodel. However, asshowninthesecond
4202
beF
41
]VC.sc[
1v86390.2042:viXraReference:V*dog A V* dog on the coast, waves, AV*dogsittingcalmlybyacozy A V* dog is playfully run-
wind,tree,sky fireplace ning along a beach at sunset,
withwavesgentlycrashinginthe
background
Reference:V*woman A V* woman in a cowboy hat, A V* woman sitting in a cozy A V* woman with a jacket
western vibes, sunset, ruggedcafe´, with a gentle rain visiblein a snow-covered winter land-
landscape throughthewindow scape, with snowflakes drifting
downaround
Figure1.Withjustafewimagesofaspecificidentity,ourVideoCustomDiffusion(VCD)cangeneratetemporalconsistentvideosaligned
withthegivenprompt.BestviewedwithAcrobatReader.Clicktheimagetoplaythevideo.
rowofFigure2,thegeneratedIDsdonotconsistentlyalign, ingvariations,andresolvethetwoprimaryissuesinprevi-
andthevideobackgroundslackstability. ousapproaches.
Toaddressthefirstissue,weproposeanIDmodulethat
TheobservedfailurecasesinFigure2highlighttwoun-
improvesthealignmentofthelearnedIDtoken’sinforma-
derlying issues. 1) The collected reference images exhibit
tionwiththesubjectiveID.Themodulelearnsspecificfea-
diverse backgrounds, capturing variations in expressions,
turesofanidentityintoafewcompactembeddingsoftext
appearances, and settings for the same person. This diver-
tokens,namelyextendedIDtokens,whichutilizesapproxi-
sity is imprinted on the unique ID token. Consequently,
mately105timesfewerparametersthanSVDiff[20](16KB
during inference, even with the same ID token, the gen-
vs 1.7MB). During optimization, updates for the ID token
erated video frames may display diverse IDs. While this
rely solely on object components, utilizing a prompt-to-
mightnotposeanissueinimagecontexts,itbecomesprob-
segmentation sub-module to distinguish identity from the
lematic in video generation. 2) The current video gener-
background. Empirical results demonstrate the efficacy of
ation framework relies on a pre-trained motion module to
ID module in enhancing ID information extraction and in-
establish inter-frame consistency. When ID tokens initial-
creasing consistency between generated videos and user-
izeeachframeindependentlywithdiverseinformation,the
specified IDs. To address the second issue, we propose a
motionmodulemaystruggletogeneratetemporallyconsis-
novel 3D Gaussian Noise Prior to establish the correlation
tentvideoframes.
betweentheinputframes.It’straining-freeandensurescon-
Inourpaper,themainfocusisonID-specificcustomiza-
sistencyininitializationduringtheinferencephase. Conse-
tion, where the goal is to animate a subject’s identity with
quently, althoughtheIDtokensmaycontaindiverseinfor-
diverse motions and scenes while preserving the subject’s
mation, all frames tend to describe consistent IDs during
ID.InthebottomrowofFigure2,ourmethodhandlesthe
balancebetweenpreservingspecificidentitiesandintroduc- 1Someofthesamplesarefollowingtheidentityimagesusedin[69]4. We designs a new training paradigm with masked loss
by prompt-to-segmentation to mitigate noise in the ID
tokens.
2.RelatedWorks
2.1.Subject-DrivenText-to-ImageGeneration
The progression of T2I diffusion models represents a re-
markable stride in image generation, creating both realis-
tic portraits and imaginative depictions of fantastical en-
tities [8, 44, 47, 50]. Recent efforts have spotlighted the
Figure 2. Comparison between the proposed Video Custom customization of these generative models, wherein a pre-
Diffusion (VCD) method1and the previous approaches. The trainedT2Idiffusionmodelisemployedalongsideamini-
firstrowshowstheresultsusingtheT2Vmethod[9]. Themid- mal set of customized subject images, aiming to fine-tune
dlerowdisplaysoutcomesfromemployingtheCustomDiffusion the model and learn a unique identifier linked to the de-
[31]withinavideogenerationframework[18],whilethebottom siredsubject.Pioneeringapproaches,suchasTextualInver-
rowpresentsresultsfromVCD.WeobservetheT2Vexhibitssim- sion[16],adaptatokenembeddingtolearnamappingbe-
ilaritiesbetweenthereferenceimageandthegeneratedvideo,but
tweentokenandsubjectimageswithoutalteringthemodel
the preservation of the ID is missing. On the other hand, direct
structure,whileDreamBooth[48]involvesthecomprehen-
useoftheCustomDiffusiontendstogeneratevideoswithspecific
sive model fine-tuning to learn the concept of subject as
IDs,butconsistencyislacking.Incontrast,VCDsignificantlyim-
well as preserving the capability of general concept gen-
provesIDpreservation.
eration. This sparked a series of subsequent works, such
the denoising process, giving improved video clips. The
as NeTI [5], focusing on fidelity and identity preservation
covariance between the initialization noises for all frames
of the subject. It further extended to multi-subject genera-
iscontrolledbyacovariancematrix. Tofurtherimprovethe
tion [6, 7, 17, 20, 31, 38, 39, 65], where the model is able
qualityofgeneratedvideos,wefurtherapplyFaceVCDto
tojointlylearnmultiplesubjects,andcomposethemintoa
denoisetheblurredfacetorecovertheidentityfortheper-
singlegeneratedimage.
son in the distance, and Tiled VCD to further upscale the
resolution of the videos. We show that the framework of
2.2.Text-to-VideoGeneration
VCDisapplicableforbothT2VandV2V.
Ourframework, VideoCustomDiffusion(VCD),intro- Advancing from image generation, T2V appears to be the
ducesamodularapproachtoID-specificvideogeneration. next breakthrough in the novel applications of generative
The optimization process reuses the same ID module in models. Compared to image generation, video generation
two pipelines, T2V VCD and V2V VCD to preserve the ismorechallengingasitrequireshighcomputationcoststo
identity. Based on Stable Diffusion [1], these pipelines maintainlong-termspatialandtemporalconsistencyacross
can use any domain-specific model finetuned on the same multiple frames, needs to condition on the vague prompt
base during inference, which offers valuable flexibility for ofshortvideocaptioning,andlackshigh-qualityannotated
AI-generatedcontentcommunitiessuchasCivitai[12]and datasets with video-text pairs. Early explorations utilize
HuggingFace[26],allowingnon-technicaluserstomixand GAN and VAE-based methods to generate frames in an
matchmodulesindependently,similartothewaywidelyac- auto-regressive manner given a caption [34, 43], yet these
ceptedtofreelycombineDreamBooth[48],LoRA[25]and worksarelimitedtolow-resolutionvideoswithsimple,iso-
prefixembedding[33]weights. lated motions. The next line of work adopts large-scale
Ourcontributionsaresummarizedasfollows: transformerarchitecturesforlong,HDqualityvideogener-
1. We introduce a novel framework, Video Custom Dif- ations[24,55,57,68],yetsufferingfromsignificanttrain-
fusion (VCD), dedicated to generating high-quality ID- ing, memory, andcomputationalcosts. Therecentsuccess
specificvideos. VCDdemonstratessubstantialimprove- of diffusion models leads a new wave of video generation
mentinaligningIDswithprovidedimagesandtextde- with diffusion-based architectures, with pioneering work
scriptions. like Video Diffusion Models [23] and Imagen Video [22]
2. Weproposearobust3DGaussianNoisePriorforvideo thatintroducenewconditionalsamplingtechniquesforspa-
frame denoising, enhancing inter-frame correlation and tialandtemporalvideoextension. MagicVideo[72]signif-
therebyimprovingvideoconsistency. icantlyimprovesgenerationefficiencybygeneratingvideo
3. WeproposetwoV2Vmodules,i.e.,FaceVCDandTiled clips in a low-dimensional latent space, which is later fol-
VCDtoupscalethevideotohigherresolutions. lowedbyVideoLDM[9].VAE
FaceVCD
UNetwithMotionModule
2X
VAE VAE
T2VVCD TiledVCD
3DGaussianNoisePrior
<V*>
A man,clad in a worn, dark ESRGAN
brown cloakwithahood inthedesert
Figure3. FrameworkofID-specificVideoGeneration. TheframeworkcomprisesT2VVCD,FaceVCD,andTiledVCD.Thebasic
components,IDmodule,and3DGaussianNoisePrior,arereusedinthesedenoisingpipelines.
2.3.VideoEditing Thediffusionmodelistrainedtoapproximatetheorigi-
naldatadistributionwithadenoisingobjective:
Furtheradvancestakemorecontrolofthegeneratedvideo.
Tune-a-Video [64] allows changing video content while
E [∥ϵ (z ,c,t)−ϵ∥], (2)
preserving motions by finetuning a T2I diffusion model z0,c,ϵ,t θ t
with a single text-video pair. Text2Video-Zero [28] and
whereϵ isthemodelprediction,usuallymodeledbyUNet.
Runway Gen [15] propose to combine a trainable mo- θ
Intheinference,giventherandomGaussiannoiseinitializa-
tion dynamic module with a pre-trained Stable Diffusion,
tion z and condition c, diffusion model performs reverse
further enabling video synthesis guided by both text and T
process for t = T,...,1 to get the encoding of sampled
pose/edge/images,withoutusinganypairedtext-videodata.
imagezˆ bytheequation:
Morerecently,AnimateDiff[18]animatesmostoftheexist- 0
ingpersonalizedT2Imodelsbydistillingreasonablemotion (cid:18) (cid:19)
1 1−α
priorsinthetrainingofamotionmodule. zˆ = √ zˆ − √ t ϵ (zˆ,c,t) +σ ϵ, (3)
t−1 α t 1−α¯ θ t t
t t
2.4.ImageAnimation
Previous works on image animation mainly focus on ex- whereσ t = 1− 1−α¯ αt ¯− t1β t,β t =1−α t.
tendingastaticimagetoasequenceofframeswithoutany
change of scene or modifying attributes of the character.
Exposure Bias. In comparing Equation 2 with Equa-
Previous works take the subject from an image [13, 52–
tion 3, we observe that the inputs to the model ϵ differ
54,56,62,67,71]oravideo[19,40,59,60,63],andtrans- θ
betweentrainingandinferencestages. Specifically,during
ferthemotionhappenedinanothervideotothesubject.Our
training,themodelϵ (z ,c,t)receivesz asaninput,which
frameworkisablenotonlytoanimateagivenframebutalso θ t t
is sampled from the ground truth according to Equation 1.
tomodifytheattributesofthesubjectandchangetheback-
However, during inference, the model ϵ (zˆ,c,t) uses zˆ,
ground,allrenderedinreasonablemotions. θ t t
computed based on previous predictions. This difference,
3.Preliminaries knownasexposurebias[42,46,51], leadstoaccumulated
errors in inference. In T2V generation, this discrepancy
Latent Diffusion Model. Our work is based on Sta- also exists in the temporal dimension. During training,
ble Diffusion [1], a variant of Latent Diffusion Model z is sampled from an actual video and typically exhibits
t
(LDM) [47]. In the training, the diffusion model takes an temporal correlation. In contrast, during inference, zˆ re-
t
imagex 0andaconditioncastheinputandencodesx 0into sults from joint inference involving both a T2I model and
a latent code z 0 with an image encoder [27, 29, 70]. The amotionmodule,withtheT2Imodel’spredictionsvarying
latent code z 0 is iteratively mixed with Gaussian noise ϵ across different frames. To address this issue, we propose
through forward process, which can be transformed into a a training-free approach, 3D Gaussian Noise Prior. This
closedform: method introduces covariance into the noise initialization
√ √ during inference. Empirically, we find that this approach
z = α¯ z + 1−α¯ ϵ,ϵ∼N(0,I), (1)
t t 0 t helpsstabilizejointinferenceandbalancesthequalityand
whereα¯
=(cid:81)t
α ,α ∈(0,1). magnitudeofmotions.
t i=1 i i(a)AreferenceimagefortheV*dog. (b)γ=0 (c)γ=0.1 (d)γ=0.2
Figure4. Influenceof3DGaussianNoisePriorCovariance. BestviewedinAcrobatReader. Clicktheimagetoplaythevideo. As
thecovariancehyper-parameterγincreases,themotionbecomesmorestable,butitsmagnitudedecreases.Thenoisepriorisappliedonly
duringinference,eliminatingtheneedforretraining.Thepromptis’aV*dogisrunninginthesnow’.
4.Method magnitudeofthemotions, asdemonstratedinFigure4. A
lowerγvalueleadstovideoswithdramaticmovementsbut
We propose a preprocessing module for VCD, along with
increasedinstability,whileahigherγresultsinmorestable
an ID module and motion module, as illustrated in Fig-
motionwithreducedamplitude.
ure 3. Additionally, we offer an optional module utiliz-
ingControlNetTiletoupsamplevideosandgeneratehigh-
4.2.IDModule
resolution content. Our approach incorporates the off-the-
shelfmotionmodulefromAnimateDiff[18],enhancedwith
ourproposed3DGaussianNoisePrior,asdiscussedinSec-
Prompt-to-segmentation
tion4.1. TheIDmodule,featuringextendedIDtokenswith
ThemainsubjectisapersonwearingthepinkT-shirt
masked loss and prompt-to-segmentation, is introduced in
Section 4.2. In Section 4.3 we introduce two V2V VCD
pipelines,FaceVCDandTiledVCD. VAE
4.1.3DGaussianNoisePrior
For simplicity, we apply our training-free 3D Gaussian
<V*> man Lmask
Noise Prior to an off-the-shelf motion module [18] to mit- Figure 5. Extended ID token learning. The extended ID to-
igate exposure bias during inference. This chosen motion kens are optimized against a masked subject area by prompt-to-
moduleexpandsthenetworktoencompassthetemporaldi- segmentation.
mension. Ittransforms2Dconvolutionandattentionlayers
intotemporalpseudo-3Dlayers[23],adheringtothetrain- Although previous works have explored token embed-
ingobjectiveoutlinedinEquation2. ding [16, 58] and weight fine-tuning [11, 17, 31, 48] for
T2I identity customization, few have delved into iden-
3D Gaussian Noise Prior. For videos comprising f tity customization in T2V generation. We observe that
frames, the3DGaussianNoisePriorsamplesfromaMul- whileweighttuningmethodslikeCustomDiffusion[31]or
tivariateGaussiandistributionN(0,Σ (γ)). Here, Σ (γ) LoRA [25] achieve precise identities in image generation,
f f
denotesthecovariancematrixparameterizedbyγ ∈(0,1). theresultingvideosoftenshowlimitedvarietyanduserin-
putalignment.
 1 γ γ2 ··· γf−1
 γ 1 γ ··· γf−2  ExtendedIDtokens. WeproposetouseextendedIDto-
Σ (γ)=  γ2 γ 1 ··· γf−3 . (4) kenstoonlyinteractwiththeconditionalencodingandbet-
f  

. .
.
. .
.
. .
.
... . .
.
 

terpreservevisualfeaturesofidentityasshowninFigure5.
γf−1 γf−2 γf−3 ··· 1 This approach, compared to the original LoRA, results in
superior video quality as shown in Table 1. Moreover,
Thecovariancedescribedaboveensuresthattheinitial- the proposed ID module requires only 16KB of storage,
ized 3D noise exhibits a covariance of γ|m−n| at the same a notably compact parameter space compared to the 3.6G
position between the m-th and n-th frames. The hyper- required for parameters in Stable Diffusion or 1.7MB for
parameterγrepresentsatrade-offbetweenthestabilityand SVDiff[20].EachtileispartiallydenoisedwithVCDtorestorethede-
tailsidentitythatismissingintheup-scalingbyESRGAN.
5.Experiments
5.1.QualitativeResults
We present several results in Figure 7. Our model not
(a)Referenceimage. (b)Withoutprompt-to- (c) With prompt-to-
segmentation. segmentation. onlymaintainstheidentityofcharactersintherealisticbase
Figure6. Encodedbackgroundnoisecorruptsthetextcondi- modelbutalsoinvarioustypesofstylizedmodels.Wehave
tioningwithoutprompt-to-segmentation. BestviewedinAcro- sourced open-source models from Civitai [12], including
batReader. Clicktheimagetoplaythevideo. Thepromptused RealistVision[3],ToonYou[4],andRCNZCartoon3D[2].
hereis’aV*catsittingonthestreet,watchingcarspassing,lights,
In this section, we first describe the details of the imple-
citynight’.
mentationsandtheselectedbaselines. Then,wepresentthe
ablationstudiesandthecomparisonswiththeselectedbase-
linemethod.
Prompt-to-segementation. Encoding of background
noisewithintheIDtokenisanimportantissueforidentity 5.2.ImplementationDetails
preserving as noted in works [11, 20]. The background
noise can corrupt the conditioned text embedding, thereby Training. Unless specified otherwise, the ID module is
impairingimage-textconsistency. IntheVCDframework, trained using Stable Diffusion 1.5 and employed with Re-
the ID module introduces varying levels of over-fitted alisticVisionduringinference. ApplyingitdirectlytoSta-
backgroundnoisepredictionacrossframes,whichimpedes bleDiffusion1.5forvideogeneration,inconjunctionwith
the motion module’s ability to align various backgrounds AnimateDiff [18], results in distorted videos. We set the
into a consistent one. To remove encoded background learningratesforextendedtokentokensat1e-3. Thebatch
noise, we propose a straightforward yet robust method: sizeisfixedat4. Eachidentity’sIDmoduleundergoes200
prompt-to-segmentation. Since the training data already optimizationstepsduringtraining. Forthemotionmodule,
contains the identity’s class, we use GPT-4V to describe weadjusttheγ inEquation4to0.15. Wedenoise80%in
the main subject in the image and the corresponding theFaceVCDwhile20%inTiledVCD.
class in COCO [36] and input this class information into
Grounding DINO [37] to obtain bounding boxes. These Datasets. To validate the effectiveness of our proposed
bounding boxes are then fed into SAM [30] to generate VCD framework, we meticulously selected 16 subjects
thesegmentationmaskofthesubject. Duringtraining, we from the DreamBooth dataset [48] and CustomCon-
compute the loss only within the mask area. As shown cept101 [31], as well as from the internet, ensuring a di-
in Figure 6, with prompt-to-segmentation, the generated verse representation of humans, animals, and objects. For
videosalignmorecloselywiththeuser’sprompt. each subject, we tasked GPT-4V with creating 25 prompts
foranimationsagainstvariousbackgrounds. Forevaluation
4.3.FaceVCDandTiledVCD
purposes,themodelgeneratesfourvideosforeachprompt,
AsshowninFigure3,thefaceinthedistanceisblurredas usingdifferentrandomseeds. Thisprocessresultsinatotal
thediffusionmodelislimitedtorenderaclearfacewithin of1600videos.
a few cells in the latent space, where each cell is down-
sampled from 8x8 pixels by VAE. To tackle this problem, Evaluation Metrics. We evaluate the generated videos
we propose Face VCD. It first detects and crops the area from three perspectives. (i) Identity alignment: The vi-
of faces from different frames and concatenates the face sualappearanceofthegeneratedidentityshouldalignwith
framesintoaface-centeredvideo.ThenitupscalestheFace that in the reference images. We utilize CLIP-I [45] and
to512x512byinterpolationandappliesapartialdenoising DINO [10] to compute the similarity score between each
processbyVCDwiththesameIDmodulesothattheiden- pair of video frames and reference images. (ii) Text-
titycanbebetterrecoveredwithhigherresolution.Thenthe alignment: Thetext-imagesimilarityscoreiscalculatedin
outputisdown-sampledandpastedbacktooriginalplaces the CLIP feature space [21]. (iii) Temporal-smoothness:
ontheframes. Weassesstemporalconsistencyinthegeneratedvideosby
The output after Face VCD is still limited in resolution computing both CLIP and DINO similarity scores for all
(512x512). WeproposetoapplyTiledVCDtoupscalethe pairs of consecutive video frames. It’s important to note
videowhilepreservingtheidentity. Thevideosarefirstup- thattemporalsmoothnessisinfluencednotonlybythecon-
scaled to 1024x1024 by ESRGAN [32, 61] and then seg- tentconsistencybetweenconsecutiveframesbutalsobythe
mentedinto4tiles,eachofthemoccupying512x512pixels. motion’s magnitude. Therefore, it is advisable to considerV* man V* man, in the forest, with sharp eyes
V* dog V* dog on the coast, waves, wind, wind, tree, sky
Figure7.QualitativeResults.WelistresultsfromRealistVision[3],ToonYou[4]andRCNZCartoon3D[2]foreachsubject.
textalignment,imagealignment,andtemporalsmoothness sitions of AnimateDiff and several identity-specific cus-
collectivelywhencomparingresults. tomizationmethods,suchasCustomDiffusion[31],Textual
Inversion (TI) [16], and LoRA [49], all combined with a
Baselines. Duetothelackofidentity-specificT2Vmeth- 3DGaussianNoisePrior. Whilerecentadvancementshave
ods, we compare our choice of ID module with compo-Table1. Quantitativecomparisonwithbaselinemodels. Thebestscoreishighlightedinbold. Thesymbol↑ indicatesthatahigher
scoreimpliesgreaterrelevance.TheproposedIDmoduleachievesanoptimalbalancebetweenvideoconsistencyandimagealignment.
SD[1] RealisticVision[3] TI[16] LoRA[25] DreamBooth[48] Ours
DINO↑ 0.277 0.341 0.405 0.545 0.527 0.564
CLIP-I↑ 0.604 0.614 0.724 0.712 0.666 0.755
CLIP-T↑ 0.267 0.294 0.245 0.240 0.238 0.277
DINOTempConsist↑ 0.444 0.549 0.699 0.690 0.544 0.745
CLIP-ITempConsist↑ 0.741 0.761 0.814 0.807 0.768 0.820
Table2. Ablationstudy. Withoutthe3DGaussianNoisePriorortheprompt-to-segmentation,allmetrics,exceptforTempConsistency
metrics,demonstrateadeclineinperformance.However,thehighertemporalconsistency,coupledwithreducedtextconsistency,indicates
themodelisoverfittingtomorestaticbackgrounds.Asaresult,thedifferencesbetweenframesareminimal,asillustratedinFigure6.
DINO↑ CLIP-I↑ CLIP-T↑ DINOTempConsist↑ CLIP-ITempConsist↑
Ours 0.564 0.755 0.277 0.745 0.820
w/o3DGaussianNoisePrior 0.555 0.712 0.222 0.688 0.800
w/oPrompt-to-segmentation 0.511 0.701 0.194 0.788 0.889
introduced more novel customization methods for multi- other. Second,theproposedframeworksarelimitedbythe
identitycustomization,suchasthosefoundin[17,20],the capacity of the motion module. Given the motion module
integrationwithsuchmethodscouldbeleftforfuturework. thatonlygeneratesshort-timevideos,it’snoteasytoextend
thelengthofthevideoswhilekeepingthesameconsistency
5.3.QuantitativeResults
andfidelity.Lookingahead,weneedtoworkonmakingthe
WepresentthequantitativeresultsinTable1. Initially,we systemcapableofhandlingmultipleidentitiesthatinteract
evaluatetwopre-trainedmodels:StableDiffusion(SD)and with each other, and ensuring it can keep up the quality in
RealisticVision. RealisticVision,acommunity-developed longervideos.
model fine-tuned on SD, shows promising results in gen-
erating realistic images. As indicated in Table 1, Realistic 7.Conclusion
Vision generally outperforms SD, leading us to adopt it as
Inthispaper,ourworkintroducesVideoCustomDiffusion
the base model when possible. However, for models like
(VCD), a framework designed to tackle the challenge of
DreamBooth, which involve fine-tuning all weights in the
subject identity controllable video generation. By focus-
UNet,replacingthebasemodelweightsisnotfeasible. Its
ing on the fusion of identity information and frame-wise
performanceisgenerallyinferiortotheothers,highlighting
correlation, VCD paves the way for producing videos that
thelimitationsofextensivefine-tuning.
not only maintain the subject’s identity across frames but
5.4.AblationStudy do so with stability and clarity. Our novel contributions,
including the ID module for precise identity disentangle-
AsshowninTable2, weconductadetailedablationstudy
ment, the T2V VCD module for enhanced frame consis-
and find that the 3D Gaussian Noise Prior is crucial for
tency, and the V2V modules for improved video quality,
videosmoothness,imagealignment,andtheCLIP-Tscore.
collectively establish a new standard for identity preserva-
In contrast, removing the prompt-to-segmentation module
tion in video content. The extensive experiments we con-
increases video smoothness but reduces the CLIP-T and
ducted affirm VCD’s superiority over existing methods in
CLIP-Iscores. Thisreductionoccursbecausesuchremoval
generatinghigh-quality,stablevideosthatpreservethesub-
leads to encoded background noise in the token thus cor-
ject’sidentity.Furthermore,theadaptabilityofourIDmod-
rupting the textual condition. Consequently, the generated
ule to work with existing text-to-image models enhances
videoslackmotion,resultinginhighersmoothnessscores.
VCD’spracticality,makingitversatileforabroadrangeof
6.LimitationsandFutureWorks applications.
Our proposed framework has several areas to improve.
First,itstruggleswhenwetrytomakevideoswithseveral
differentidentities,eachwithitsownspecialtokenembed-
ding and LoRA weights. The resulting video degrades es-
pecially when these characters have to interact with eachReferences In Proceedings of the IEEE/CVF International Conference
onComputerVision(ICCV),pages7346–7356,2023. 1
[1] Stable diffusion. https://huggingface.co/
[15] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
runwayml/stable-diffusion-v1-5, 2022. 3, 4,
Jonathan Granskog, and Anastasis Germanidis. Structure
8
and content-guided video synthesis with diffusion models.
[2] Rcnz cartoon 3d v1.0. https://civitai.com/ In Proceedings of the IEEE/CVF International Conference
models/66347?modelVersionId=71009, 2023. 6, onComputerVision,pages7346–7356,2023. 4
7
[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
[3] Realistic vision v5.1. https://civitai.com/ nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
models/4201/realistic-vision-v51, 2023. 6, Or. An image is worth one word: Personalizing text-to-
7,8 image generation using textual inversion. arXiv preprint
[4] Toonyou beta 3. https://civitai.com/models/ arXiv:2208.01618,2022. 1,3,5,7,8
30240?modelVersionId=78775,2023. 6,7 [17] YuchaoGu,XintaoWang,JayZhangjieWu,YujunShi,Yun-
[5] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel peng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning
Cohen-Or. A neural space-time representation for text-to- Chang,WeijiaWu,etal. Mix-of-show: Decentralizedlow-
image personalization. arXiv preprint arXiv:2305.15391, rankadaptationformulti-conceptcustomizationofdiffusion
2023. 3 models. arXivpreprintarXiv:2305.18292,2023. 3,5,8
[6] OmriAvrahami,KfirAberman,OhadFried,DanielCohen- [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Or, andDaniLischinski. Break-a-scene: Extractingmulti- Qiao,DahuaLin, andBoDai. Animatediff: Animateyour
pleconceptsfromasingleimage. InSIGGRAPHAsia2023 personalizedtext-to-imagediffusionmodelswithoutspecific
ConferencePapers,NewYork,NY,USA,2023.Association tuning. arXivpreprintarXiv:2307.04725,2023. 3,4,5,6
forComputingMachinery. 3 [19] Sungjoo Ha, Martin Kersner, Beomsu Kim, Seokjun Seo,
and Dongyoung Kim. Marionette: Few-shot face reenact-
[7] Jinbin Bai, Zhen Dong, Aosong Feng, Xiao Zhang, Tian
mentpreservingidentityofunseentargets.InProceedingsof
Ye, Kaicheng Zhou, and Mike Zheng Shou. Integrat-
theAAAIconferenceonartificialintelligence,pages10893–
ing view conditions for image synthesis. arXiv preprint
10900,2020. 4
arXiv:2310.16002,2023. 3
[20] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,
[8] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
Dimitris Metaxas, and Feng Yang. Svdiff: Compact pa-
Wang,LinjieLia,LongOuyang,JuntangZhuang,JoyceLee,
rameter space for diffusion fine-tuning. In Proceedings of
YufeiGuo,WesamManassra,PrafullaDhariwal,CaseyChu,
theIEEE/CVFInternationalConferenceonComputerVision
YunxinJiao,andAdityaRamesh. Improvingimagegenera-
(ICCV),pages7323–7334,2023. 2,3,5,6,8
tionwithbettercaptions. https://cdn.openai.com/
[21] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,
papers/dall-e-3.pdf,2023. 3
andYejinChoi. Clipscore:Areference-freeevaluationmet-
[9] AndreasBlattmann,RobinRombach,HuanLing,TimDock-
ricforimagecaptioning. arXivpreprintarXiv:2104.08718,
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
2021. 6
Alignyourlatents: High-resolutionvideosynthesiswithla-
[22] Jonathan Ho, WilliamChan, ChitwanSaharia, JayWhang,
tentdiffusionmodels.InProceedingsoftheIEEE/CVFCon-
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
ferenceonComputerVisionandPatternRecognition,pages
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
22563–22575,2023. 1,3
video:Highdefinitionvideogenerationwithdiffusionmod-
[10] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou,
els. arXivpreprintarXiv:2210.02303,2022. 1,3
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg-
[23] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
ingpropertiesinself-supervisedvisiontransformers.InPro-
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
ceedingsoftheIEEE/CVFinternationalconferenceoncom-
fusionmodels. arXiv:2204.03458,2022. 3,5
putervision,pages9650–9660,2021. 6
[24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,
[11] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan,
and Jie Tang. Cogvideo: Large-scale pretraining for
Yuwei Zhou, and Wenwu Zhu. Disenbooth: Identity-
text-to-video generation via transformers. arXiv preprint
preserving disentangled tuning for subject-driven text-to-
arXiv:2205.15868,2022. 1,3
imagegeneration. arXivpreprintarXiv:2305.03374, 2023.
[25] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
5,6
Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.
[12] Civitai. Civitai. https://civitai.com/,2022. 3,6 Lora: Low-rankadaptationoflargelanguagemodels. arXiv
[13] Michael Dorkenwald, Timo Milbich, Andreas Blattmann, preprintarXiv:2106.09685,2021. 3,5,8
RobinRombach,KonstantinosGDerpanis,andBjornOm- [26] HuggingFace. Huggingface. https://huggingface.
mer. Stochastic image-to-video synthesis using cinns. In co/,2022. 3
ProceedingsoftheIEEE/CVFConferenceonComputerVi- [27] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
sionandPatternRecognition,pages3742–3753,2021. 4 Efros. Image-to-image translation with conditional adver-
[14] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, sarialnetworks. InProceedingsoftheIEEEconferenceon
Jonathan Granskog, and Anastasis Germanidis. Structure computervisionandpatternrecognition,pages1125–1134,
and content-guided video synthesis with diffusion models. 2017. 4[28] Levon Khachatryan, Andranik Movsisyan, Vahram Tade- [41] HaomiaoNi, ChanghaoShi, KaiLi, SharonXHuang, and
vosyan, Roberto Henschel, Zhangyang Wang, Shant Martin Renqiang Min. Conditional image-to-video gener-
Navasardyan,andHumphreyShi. Text2video-zero:Text-to- ation with latent flow diffusion models. In Proceedings of
imagediffusionmodelsarezero-shotvideogenerators.arXiv theIEEE/CVFConferenceonComputerVisionandPattern
preprintarXiv:2303.13439,2023. 4 Recognition,pages18444–18455,2023. 1
[29] DiederikPKingmaandMaxWelling. Auto-encodingvaria- [42] Mang Ning, Enver Sangineto, Angelo Porrello, Simone
tionalbayes. arXivpreprintarXiv:1312.6114,2013. 4 Calderara, and Rita Cucchiara. Input perturbation re-
[30] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao, duces exposure bias in diffusion models. arXiv preprint
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite- arXiv:2301.11706,2023. 4
head, Alexander C. Berg, Wan-Yen Lo, Piotr Dolla´r, and [43] YingweiPan,ZhaofanQiu,TingYao,HouqiangLi,andTao
RossGirshick. Segmentanything. arXiv:2304.02643,2023. Mei. Tocreatewhatyoutell: Generatingvideosfromcap-
6 tions. InProceedingsofthe25thACMinternationalconfer-
[31] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli enceonMultimedia,pages1789–1798,2017. 3
Shechtman,andJun-YanZhu. Multi-conceptcustomization [44] Dustin Podell, Zion English, Kyle Lacey, Andreas
oftext-to-imagediffusion. InProceedingsoftheIEEE/CVF Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and
Conference on Computer Vision and Pattern Recognition, Robin Rombach. Sdxl: Improving latent diffusion mod-
pages1931–1941,2023. 1,3,5,6,7 els for high-resolution image synthesis. arXiv preprint
[32] ChristianLedig,LucasTheis,FerencHusza´r,JoseCaballero, arXiv:2307.01952,2023. 3
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
AlykhanTejani, JohannesTotz, ZehanWang, etal. Photo-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
realisticsingleimagesuper-resolutionusingagenerativead-
AmandaAskell,PamelaMishkin,JackClark,etal.Learning
versarialnetwork.InProceedingsoftheIEEEconferenceon
transferable visual models from natural language supervi-
computervisionandpatternrecognition,pages4681–4690,
sion.InInternationalconferenceonmachinelearning,pages
2017. 6
8748–8763.PMLR,2021. 6
[33] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-
[46] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and
ing continuous prompts for generation. arXiv preprint
Wojciech Zaremba. Sequence level training with recurrent
arXiv:2101.00190,2021. 3
neuralnetworks. arXivpreprintarXiv:1511.06732,2015. 4
[34] YitongLi, MartinMin, DinghanShen, DavidCarlson, and
[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
LawrenceCarin.Videogenerationfromtext.InProceedings
Patrick Esser, and Bjo¨rn Ommer. High-resolution image
oftheAAAIconferenceonartificialintelligence,2018. 3
synthesis with latent diffusion models. In Proceedings of
[35] JunHaoLiew,HanshuYan,JianfengZhang,ZhongcongXu,
the IEEE/CVF conference on computer vision and pattern
and Jiashi Feng. Magicedit: High-fidelity and temporally
recognition,pages10684–10695,2022. 3,4
coherent video editing. arXiv preprint arXiv:2308.14749,
[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
2023. 1
MichaelRubinstein,andKfirAberman. Dreambooth: Fine
[36] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
tuning text-to-image diffusion models for subject-driven
PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence
generation. In Proceedings of the IEEE/CVF Conference
Zitnick. Microsoft coco: Common objects in context. In
onComputerVisionandPatternRecognition,pages22500–
ComputerVision–ECCV2014: 13thEuropeanConference,
22510,2023. 1,3,5,6,8
Zurich, Switzerland, September 6-12, 2014, Proceedings,
[49] SimoRyu. Low-rankadaptationforfasttext-to-imagediffu-
PartV13,pages740–755.Springer,2014. 6
sionfine-tuning,2023. 7
[37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun [50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Zhu, etal. Groundingdino: Marryingdinowithgrounded Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
pre-training for open-set object detection. arXiv preprint RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
arXiv:2303.05499,2023. 6 etal.Photorealistictext-to-imagediffusionmodelswithdeep
language understanding. Advances in Neural Information
[38] ZhihengLiu,YifeiZhang,YujunShen,KechengZheng,Kai
ProcessingSystems,35:36479–36494,2022. 3
Zhu,RuiliFeng,YuLiu,DeliZhao,JingrenZhou,andYang
Cao. Cones2: Customizableimagesynthesiswithmultiple [51] FlorianSchmidt.Generalizationingeneration:Acloserlook
subjects. arXivpreprintarXiv:2305.19327,2023. 3 at exposure bias. In Proceedings of the 3rd Workshop on
[39] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Neural Generation and Translation, pages 157–167, Hong
Subject-diffusion: Opendomainpersonalizedtext-to-image Kong,2019.AssociationforComputationalLinguistics. 4
generation without test-time fine-tuning. arXiv preprint [52] AliaksandrSiarohin,Ste´phaneLathuilie`re,SergeyTulyakov,
arXiv:2307.11410,2023. 3 ElisaRicci,andNicuSebe. Animatingarbitraryobjectsvia
[40] HaomiaoNi, YihaoLiu, SharonX.Huang, andYuanXue. deepmotiontransfer. InProceedingsoftheIEEE/CVFCon-
Cross-identity video motion retargeting with joint transfor- ferenceonComputerVisionandPatternRecognition,pages
mationandsynthesis.InProceedingsoftheIEEE/CVFWin- 2377–2386,2019. 4
terConferenceonApplicationsofComputerVision(WACV), [53] AliaksandrSiarohin,Ste´phaneLathuilie`re,SergeyTulyakov,
pages412–422,2023. 4 Elisa Ricci, and Nicu Sebe. First order motion model forimageanimation.InConferenceonNeuralInformationPro- Cun, Xintao Wang, et al. Make-your-video: Customized
cessingSystems(NeurIPS),2019. videogenerationusingtextualandstructuralguidance.arXiv
[54] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei preprintarXiv:2306.00943,2023. 1
Chai, andSergeyTulyakov. Motionrepresentationsforar- [67] BorunXu,BiaoWang,JialeTao,TiezhengGe,YuningJiang,
ticulatedanimation. InCVPR,2021. 4 WenLi,andLixinDuan.Moveasyoulike:Imageanimation
[55] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn, in e-commerce scenario. In Proceedings of the 29th ACM
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, InternationalConferenceonMultimedia,pages2759–2761,
OranGafni, etal. Make-a-video: Text-to-videogeneration 2021. 4
without text-video data. arXiv preprint arXiv:2209.14792, [68] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind
2022. 3 Srinivas.Videogpt:Videogenerationusingvq-vaeandtrans-
[56] Jiale Tao, Biao Wang, Borun Xu, Tiezheng Ge, Yuning formers. arXivpreprintarXiv:2104.10157,2021. 1,3
Jiang, Wen Li, and Lixin Duan. Structure-aware motion [69] JiwenYu,YinhuaiWang,ChenZhao,BernardGhanem,and
transfer with deformable anchor model. In Proceedings of Jian Zhang. Freedom: Training-free energy-guided condi-
theIEEE/CVFConferenceonComputerVisionandPattern tional diffusion model. arXiv preprint arXiv:2303.09833,
Recognition,pages3637–3646,2022. 4 2023. 2
[57] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin- [70] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi man, and Oliver Wang. The unreasonable effectiveness of
Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. deepfeaturesasaperceptualmetric. InProceedingsofthe
Phenaki:Variablelengthvideogenerationfromopendomain IEEE conference on computer vision and pattern recogni-
textualdescription. arXivpreprintarXiv:2210.02399,2022. tion,pages586–595,2018. 4
1,3 [71] JianZhaoandHuiZhang. Thin-platesplinemotionmodel
[58] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir forimageanimation. InProceedingsoftheIEEE/CVFCon-
Aberman. P+: Extended textual conditioning in text-to- ferenceonComputerVisionandPatternRecognition,pages
imagegeneration. 2023. 5 3657–3666,2022. 4
[59] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, [72] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to- Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
video synthesis. In Advances in Neural Information Pro- generation with latent diffusion models. arXiv preprint
cessingSystems(NeurIPS),2018. 4 arXiv:2211.11018,2022. 1,3
[60] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu,
Jan Kautz, and Bryan Catanzaro. Few-shot video-to-video
synthesis. In Advances in Neural Information Processing
Systems(NeurIPS),2019. 4
[61] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
Real-esrgan:Trainingreal-worldblindsuper-resolutionwith
puresyntheticdata. InProceedingsoftheIEEE/CVFinter-
nationalconferenceoncomputervision, pages1905–1914,
2021. 6
[62] Yaohui Wang, Di Yang, Francois Bremond, and Antitza
Dantcheva. Latentimageanimator:Learningtoanimateim-
agesvialatentspacenavigation.InInternationalConference
onLearningRepresentations,2022. 4
[63] OliviaWiles,AKoepke,andAndrewZisserman. X2face:A
networkforcontrollingfacegenerationusingimages,audio,
andposecodes. InProceedingsoftheEuropeanconference
oncomputervision(ECCV),pages670–686,2018. 4
[64] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
Lei,YuchaoGu,YufeiShi,WynneHsu,YingShan,Xiaohu
Qie,andMikeZhengShou. Tune-a-video: One-shottuning
of image diffusion models for text-to-video generation. In
ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages7623–7633,2023. 1,4
[65] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fre´do
Durand, andSongHan. Fastcomposer: Tuning-freemulti-
subject image generation with localized attention. arXiv
preprintarXiv:2305.10431,2023. 3
[66] JinboXing,MenghanXia,YuxinLiu,YuechenZhang,Yong
Zhang,YingqingHe,HanyuanLiu,HaoxinChen,Xiaodong