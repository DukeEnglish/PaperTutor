Auto-Encoding Bayesian Inverse Games
Xinjie Liu†∗ Lasse Peters‡∗ Javier Alonso-Mora‡ Ufuk Topcu† David Fridovich-Keil†
†The University of Texas at Austin ‡Delft University of Technology
{ xinjie-liu, utopcu, dfk }@utexas.edu, { l.peters, j.alonsomora }@tudelft.nl
Abstract—Whenmultipleagentsinteractinacommonenviron- observations
robot goal
ment, each agent’s actions impact others’ future decisions, and
noncooperative dynamic games naturally capture this coupling. human
In interactive motion planning, however, agents typically do
not have access to a complete model of the game, e.g., due to
predictions
unknown objectives of other players. Therefore, we consider the true human plan
made by
inverse game problem, in which some properties of the game
the robot
are unknown a priori and must be inferred from observations. posterior
samples
Existing maximum likelihood estimation (MLE) approaches to
solve inverse games provide only point estimates of unknown
parameterswithoutquantifyinguncertainty,andperformpoorly
when many parameter values explain the observed behavior. To unknown
address these limitations, we take a Bayesian perspective and true human
construct posterior distributions of game parameters. To render goal
inference tractable, we employ a variational autoencoder (VAE)
with an embedded differentiable game solver. This structured
robot
VAE can be trained from an unlabeled dataset of observed
interactions, naturally handles continuous, multi-modal distri-
butions, and supports efficient sampling from the inferred pos- Fig. 1: A robot interacting with a human driver whose goal
teriors without computing game solutions at runtime. Extensive positionisunknown.Weembedadifferentiablegamesolverin
evaluations in simulated driving scenarios demonstrate that the astructuredvariationalautoencodertoinferthedistributionof
proposed approach successfully learns the prior and posterior
thehuman’sobjectivesbasedonobservationsoftheirbehavior.
game parameter distributions, provides more accurate objective
estimates than MLE baselines, and facilitates safer and more
manyparametervaluesexplaintheobservations[10],yielding
efficient game-theoretic motion planning. 1
overconfident, unsafe motion plans [12].
I. INTRODUCTION Bayesian formulations of inverse games mitigate these lim-
itations of MLE methods by inferring a posterior distribution,
Autonomous robots often need to interact with other agents
i.e.,belief,overtheunknownparameters[13,14].Knowledge
to operate seamlessly in real-world environments. For exam-
of this distribution allows the robot to account for uncertainty
ple, in the scenario depicted in Fig. 1, a robot encounters
and generate safer yet efficient plans [12, 14–16].
a human driver while navigating an intersection. In such
Unfortunately, exact Bayesian inference is typically in-
settings, coupling effects between agents significantly com-
tractable in dynamic games, especially when dynamics are
plicate decision-making: if the human acts assertively and
nonlinear. Prior work [14] alleviates this challenge by using
drives straight toward its goal, the robot will be forced to
an unscented Kalman filter (UKF) for approximate Bayesian
brake to avoid a collision. Hence, the agents compete to
inference. However, that approach is limited to unimodal
optimize their individual objectives. Dynamic noncooperative
uncertainty models and demands solving multiple games per
game theory [1] provides an expressive framework to model
belief update, thereby posing a computational challenge.
such interaction among rational, self-interested agents.
The main contribution of this work is a framework for
In many scenarios, however, a robot must handle interac-
tractableBayesianinferenceofposteriordistributionsoverun-
tions under incomplete information, e.g., without knowing the
knownparametersindynamicgames.Tothisend,weapprox-
goal position of the human driver in Fig. 1. To this end, a
imate exact Bayesian inference with a structured variational
recent line of work [2–11] proposes to solve inverse dynamic
autoencoder(VAE).Duringtraining,thisVAEembedsadiffer-
games to infer the unknown parameters of a game—such as
entiable game solver to facilitate unsupervised learning from
other agents’ objectives—from observed interactions.
an unlabeled dataset of observed interactions. At runtime, the
A common approach to solve inverse dynamic games uses
proposed approach can generate samples from the predicted
maximumlikelihoodestimation(MLE)tofindthemostlikely
posterior without solving additional games. As a result, our
parameters given observed behavior [3–11]. However, MLE
approach naturally captures continuous, multi-modal beliefs,
solutionsprovideonlyapointestimatewithoutanyuncertainty
and addresses the limitations of MLE inverse games without
quantification and can perform poorly in scenarios where
resortingtothesimplificationsorcomputationalcomplexityof
existing Bayesian methods.
∗Equalcontribution
1https://xinjie-liu.github.io/projects/bayesian-inverse-games Through extensive evaluations of our method in two simu-
4202
beF
41
]OR.sc[
1v20980.2042:viXralated driving scenarios, we support the following key claims. objective parameters. Inga et al. [4] extends this approach to
The proposed framework (i) learns the underlying prior game maximum-entropy settings.
parameter distribution from unlabeled interactions, and (ii) Recent work [9] proposes to maximize observation likeli-
capturespotentialmulti-modalityofgameparameters.Ourap- hood while enforcing the Karush–Kuhn–Tucker (KKT) con-
proach (iii) is uncertainty-aware, predicting narrow unimodal ditions of OLNE as constraints. This approach only requires
beliefswhenobservationsclearlyrevealtheintentionsofother partial-state observations and can cope with noise-corrupted
agents, and beliefs that are closer to the learned prior in data. Approaches [8, 10] propose an extension of the MLE
case of uninformative observations. The proposed framework approach [9] to inverse feedback and open-loop games with
(iv) provides more accurate inference performance than MLE inequality constraints via differentiable forward game solvers.
inverse games by effectively leveraging the learned prior Liu et al. [8] demonstrates integration with neural network
information, especially in settings where multiple parameter (NN) components to amortize the computation of MLE.
valuesexplaintheobservedbehavior.Asaresult,ourapproach In general, MLE solutions can be understood as point
(v)enablessaferdownstreamrobotplansthanMLEmethods. estimatesofBayesianposteriors,assumingauniformprior[28,
Ch.4]. When multiple parameter values explain the observa-
II. RELATEDWORK
tions equally well, this simplifying assumption can result in
This section provides an overview of the literature on ill-posed problems—causing MLE inverse games to recover
dynamic game theory, focusing on both forward games (Sec- potentially inaccurate estimates [10]. Moreover, in the con-
tion II-A) and inverse games (Section II-B). text of motion planning, the use of point estimates without
awareness of uncertainty can result in unsafe plans [12, 16].
A. Forward Dynamic Games
To address these issues, several recent works take a
This work focuses on noncooperative games where agents Bayesian view on inverse games [13, 14], aiming to infer a
have partially conflicting but not completely adversarial goals posteriordistributionwhilefactoringinpriorknowledge.Since
andmakesequentialdecisionsovertime[1].Sinceweassume exact Bayesian inference is intractable in these problems, the
thatagentstakeactionssimultaneouslywithoutleader-follower belief update may be approximated via a particle filter [13].
hierarchyandweconsidercouplingbetweenagents’decisions However, this approach requires solving a large number of
through both objectives and constraints, our focus is on equilibriumproblemsonlinetomaintainthebeliefdistribution,
generalized Nash equilibrium problems (GNEPs). posing a significant computational burden. A sigma-point
GNEPs are challenging coupled mathematical optimization approximation [14] reduces the number of required samples
problems. Nonetheless, recent years have witnessed an ad- but limits the estimator to unimodal uncertainty models.
vancement of efficient computation methods for GNEPs with To overcome the limitations of MLE approaches while
smooth objectives and constraints. Due to the computational avoiding the intractability of exact Bayesian inference, we
challenges involved in solving such problems under feedback propose to approximate the posterior via a VAE [29] that
information structure [17], most works aim to find open-loop embeds a differentiable game solver [8] during training. The
Nash equilibria (OLNE) [1] instead, where players choose proposed approach can be trained from an unlabeled dataset
their action sequence—an open-loop strategy—at once. A of observed interactions, naturally handles continuous, multi-
substantial body of work employs the iterated best response modaldistributions,anddoesnotrequirecomputationofgame
algorithm to find a GNEP by iteratively solving single- solutions at runtime to sample from the posterior.
agent optimization problems [18, 19, 19–22]. More recently,
methods based on sequential quadratic approximations have
III. PRELIMINARIES:GENERALIZEDNASHGAME
been proposed [23, 24], aiming to speed up convergence In this work, we consider strategic interactions of self-
by updating all players’ strategies simultaneously at each interested rational agents in the framework of generalized
iteration. Finally, since the first-order necessary conditions of Nash equilibrium problems (GNEPs). In this framework, each
open-loop GNEPs take the form of a mixed complementarity ofN agentsseekstounilaterallyminimizetheirrespectivecost
problem (MCP) [25], several works [8, 12] solve generalized while being conscious of the fact that their “opponents”, too,
Nashequilibria(GNE)usingestablishedMCPsolvers[26,27]. actintheirownbestinterest.WedefineaparametricN-player
This work builds on the latter approach. GNEP as N coupled, constrained optimization problems,
B. Inverse Dynamic Games Si(τ¬i):=argmin Ji(τi,τ¬i) (1a)
θ θ
τi
Inversegamesstudytheproblemofinferringunknowngame s.t. gi(τi,τ¬i)≥0, (1b)
parameters, e.g. of objective functions, from observations θ
of agents’ behavior [2]. In recent years, several approaches where θ ∈ Rp denotes a parameter vector whose role will
have extended single-agent inverse optimal control (IOC) and become clear below, and player i ∈ [N] := {1,...,N}
inversereinforcementlearning(IRL)techniquestomulti-agent has cost function Ji and private constraints gi. Furthermore,
θ θ
interactivesettings.Forinstance,theapproachesof[5,6]min- observe that the costs and constraints of player i depend not
imize the residual of agents’ first-order necessary conditions, only on their own strategy τi ∈Rmi, but also on the strategy
givenfullstate-controlobservations,inordertoinferunknown of all other players, τ¬i ∈R(cid:80) j∈[N]\{i}mj.augmentation unaugmented model
Generalized Nash Equilibria. For a given parameter θ, the
auxiliary decoder game differentiable game
solution of a GNEP is an equilibrium strategy profile τ∗ := variable NN parameters game solver solution observation
(τ1∗,...,τN∗)sothateachagent’sstrategyisabestresponse
to the others’, i.e.,
τi∗ ∈Si(τ¬i∗),∀i∈[N]. (2)
θ
encoder
Intuitively, at a GNE, no player can further reduce their cost latent posterior surrogate NN
by unilaterally adopting another feasible strategy.
Fig. 2: Overview of a structured VAE for generative Bayesian
Example: Online Game-Theoretic Motion Planning. This inverse games. Top (left to right): decoder pipeline. Bottom
work focuses onapplying games to online motionplanning in (right to left): variational inference process via an encoder.
interactionwithotheragents,suchastheintersectionscenario
shown in Fig. 1. In this context, the strategy of agent i, A. A Bayesian View on Inverse Games
τ i, represents a trajectory—i.e., a sequence of states and In order to address the limitations of the MLE inverse
inputs extended over a finite horizon—which is recomputed games, we consider a Bayesian formulation of inverse games,
in a receding-horizon fashion. This paradigm results in the and seek to construct the belief distribution
game-theoretic equivalent of model-predictive control (MPC): p(y |θ)p(θ)
model-predictive game-play (MPGP). The construction a tra- b(θ)=p(θ |y)= . (3)
p(y)
jectory game largely follows the procedure used in single-
agent trajectory optimization: gi(·) encodes input and state IncontrasttoMLEinversegames,thisformulationprovidesa
θ
full posterior distribution over the unknown game parameters
constraints including those enforcing dynamic feasibility, col-
lision avoidance, and road geometry; and Ji(·) encodes the θ and factors in prior knowledge p(θ).
θ
ith agent’s objective such as reference tracking. Adopting Observation Model p(y | θ). In autonomous online oper-
a convention in which index 1 refers to the ego agent, the ation of a robot, y represents the (partial) observations of
equilibrium solution of the game then serves two purposes si- otherplayers’recenttrajectories—e.g.,fromafixedlagbuffer
multaneously:theopponents’solution,τ¬1∗,servesasagame- as shown in orange in Fig. 1. Like prior works on inverse
theoretic prediction of their behavior while the ego agent’s games [9, 10, 14], we assume that, given the unobserved
solution, τ1∗, provides the corresponding best response. true trajectory of the human, y is Gaussian-distributed; i.e.,
p(y | τ) = N(y | µ (τ),Σ (τ)). Assuming that the underly-
The Role of Game Parameters θ. A key difference between y y
ingtrajectoryisthesolutionofagamewithknownstructureΓ
trajectorygamesandsingle-agenttrajectoryoptimizationisthe
but unknown parameters θ, we express the observation model
requirement to provide the costs and constraints of all agents.
as p(y | θ) = p(y | T (θ)), where T denotes a game solver
In practice, a robot may have insufficient knowledge of their Γ Γ
that returns a solution τ∗ of the game Γ(θ).
opponents’intents,dynamics,orstatetoinstantiateacomplete
game-theoreticmodel.Thisaspectmotivatestheparameterized Challenges of Bayesian Inverse Games. While the Bayesian
formulation of the game above: for the remainder of this formulation of inverse games in Eq. (3) is conceptually
manuscript, θ will capture the unknown aspects of the game. straightforward, it poses several challenges:
In the context of game-theoretic motion planning, e.g. to (i) The prior p(θ) is typically unavailable and instead must
model agents navigating an intersection, θ typically includes be learned from data.
aspects of opponents’ preferences such as their unknown (ii) The computation of the normalizing constant, p(y) =
(cid:82)
desired lane or preferred velocity. For conciseness, we denote p(y | θ)p(θ)dθ, is intractable in practice due to the
game (1) compactly via the parametric tuple of problem data marginalization of θ.
Γ(θ) := ({Ji,gi} ). Next, we discuss how to infer these (iii) Boththeprior,p(θ),andposteriorp(θ |y)areingeneral
θ θ i∈[N]
parameters online from observed interactions. non-Gaussianorevenmulti-modalandarethereforedif-
ficult to represent explicitly in terms of their probability
IV. FORMALIZINGBAYESIANINVERSEGAMES
density function (PDF).
Thissectionpresentsourmaincontribution:aframeworkfor Priorwork[14]partiallymitigatesthesechallengesbyusing
inferringunknowngameparametersθbasedonobservationsy. anUKFforapproximateBayesianinference,butthatapproach
Thisproblemiscommonlyreferredtoasaninversegame[2]. islimitedtounimodaluncertaintymodelsandrequiressolving
Several prior works on inverse games [9, 10, 30] seek multiple games for a single belief update, thereby posing a
to find game parameters that directly maximize observation computational challenge.
likelihood. However, this MLE formulation of inverse games Fortunately, as we shall demonstrate in Section V, many
(i) only provides a point estimate of the unknown game pa- practical applications of inverse games do not require an
rametersθ,therebyprecludingtheconsiderationofuncertainty explicit evaluation of the belief PDF, b(θ). Instead, a gen-
in downstream tasks such as motion planning; and (ii) fails to erative model of the belief—i.e., one that allows drawing
providereasonableparameterestimateswhenobservationsare samples θ ∼ b(θ)—often suffices. Throughout this section,
uninformative, as we shall also demonstrate in Section V. we demonstrate how to learn such a generative model froman unlabeled dataset D = {y | y ∼ p(y),∀k ∈ [K]} of Next, we discuss the training of the decoder and encoder
k k
independent and identically distributed observed interactions. networks d and e of our structured VAE.
ϕ ψ
D. Training the Structured Variational Autoencoder
B. Augmentation to Yield a Generative Model
Below, we outline the process for optimizing model param-
To obtain a generative model of the belief b(θ), we aug-
eters in our specific setting. For a general discussion of VAEs
ment our Bayesian model as summarized in the top half
and variational inference (VI), refer to Murphy [31].
of Fig. 2. The goal of this augmentation is to obtain a
model structure that lends itself to approximate inference Fitting a Prior to D. First, we discuss the identification of
in the framework of variational autoencoders (VAEs) [29]. the prior parameters ϕ. Specifically, we seek to choose these
Specifically,weintroduceanauxiliaryrandomvariablez with parameters so that the data distribution induced by ϕ, i.e.,
(cid:82)
known standard-normal prior p(z) = N(z), and a deter- p ϕ(y) = p ϕ(y | z)p(z)dθ, closely matches the unknown
ministic decoder d with unknown parameters ϕ. Following true data distribution p(y). For this purpose, we measure
ϕ
the generative process outlined in Fig. 2 from left to right, closeness between distributions using the Kullback–Leibler
the decoder maps z to the game parameters θ and hence (KL) divergence
(cid:0) (cid:1)
we have p (θ | z) = δ d (z) − θ , where δ denotes the
ϕ ϕ D (p∥q):=E [logp(x)−logq(x)]. (4)
Dirac delta function. Next, the game solver T produces the KL x∼p(x)
Γ
GNE solution τ∗ which determines the observation likelihood Thekeypropertiesofthisdivergencemetric,D (p∥q)≥0
KL
p(y | τ). In terms of probabil (cid:82)ity distributions, d ϕ induces and D KL(p∥q) = 0 ⇐⇒ p = q, allow us to cast the
the conditional p ϕ(y | z) = p(y | θ)p ϕ(θ | z)dθ, the estimation of ϕ as an optimization problem:
(cid:82)
prior p (θ) = p (θ | z)p(z)dz, and the data distribution
ϕ ϕ
(cid:82) ϕ∗ ∈argmin D (p(y)∥p (y)) (5a)
p (y)= p(y |θ)p (θ)dθ. KL ϕ
ϕ ϕ ϕ
=argmin −E (cid:2) logE [p (y |z)](cid:3) . (5b)
C. Auto-Encoding Bayesian Inverse Games y∼p(y) z∼p(z) ϕ
ϕ
In the augmented model of Section IV-B, knowledge of ϕ With the prior recovered, we turn to the approximation of the
and the latent posterior p (z | y) = p (y | z)p(z)/p (y) posterior p (θ |y).
ϕ ϕ ϕ ϕ
suffices to specify generative models for the prior p (θ) and
ϕ Variational Belief Inference. We can find the closest sur-
posterior p (θ |y). That is, by propagating samples from the
ϕ rogate q (z | y) of p (z | y) by minimizing the expected
ψ ϕ
latent prior p(z) through d , we implicitly generate samples
ϕ KL divergence between the two distributions over the data
from the prior p (θ). Similarly, by propagating samples from
ϕ distribution y ∼p(y) using the framework of VI:
the latent posterior p (z | y) through d , we implicitly
ϕ ϕ
generate samples from the posterior p ϕ(θ | y). We thus have ψ∗ ∈argminE y∼p(y)[D KL(q ψ(z |y)∥p ϕ(z |y))] (6a)
ψ
convertedtheproblemof(generative)Bayesianinversegames
=argminE [ℓ(ψ,ϕ,y,z)], (6b)
to estimation of ϕ and inference of p (z |y). y∼p(y)
ϕ ψ
Recognizing the similarity of the generative process from z
z∼qψ(z|y)
where
to y outlined in Fig. 2 to the decoding process in a VAE,
we seek to approximate the latent posterior p (z | y) with
ϕ ℓ(ψ,ϕ,y,z):=log q (z |y) −log p (y |z) −logp(z).
ψ ϕ
a Gaussian q ψ(z | y) = N(z | e ψ(y)). Here, in the (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
language of VAEs, e
ψ
takes the role of an encoder that maps N(z|eψ(y)) N(y|(TΓ◦dϕ)(z)) N(z)
observationy tothemeanµ z(y)andcovariancematrixΣ z(y) We have therefore outlined, in theory, the methodology to
of the Gaussian posterior approximation; cf. bottom of Fig. 2. determine all parameters of the pipeline from the unlabeled
Thekeydifferencebetweenourpipelineandaconventional dataset D.
VAE is the special structure of the generative process from
Considerations for Practical Realization. To solve Eqs. (5)
latent z to observation y. Within this process, the decoder
and (6) in practice, we must overcome two main challenges:
networkd iscomposedwiththegamesolverT toproducean
ϕ Γ first, the loss landscapes are highly nonlinear due to the
equilibriumstrategythatparameterizestheobservationmodel.
nonlinear transformations e ,d and T ; and second, the
ψ ϕ Γ
Building on the approach of Liu et al. [8], we can recover
expected values cannot be computed in closed form. These
the gradient of the game solver T with respect to the game
Γ challenges can be addressed by taking a stochastic first-order
parameters θ. It is this gradient information from the game-
optimization approach, such as stochastic gradient descent
theoretic “layer” in the decoding pipeline that induces an
(SGD), thereby limiting the search to a local optimum and
interpretable structure on the output of the decoder-NN d ,
ϕ approximating the gradients of the expected values via Monte
forcing it to predict the hidden game parameters θ.
Carlo sampling. To facilitate SGD, we seek to construct
Remark. To generate samples from the estimated posterior unbiased gradient estimators of the objectives of Eq. (5b)
(cid:82)
q (θ | y) = p (θ | z)q (z | y)dz, we do not need to and Eq. (6b) from gradients computed at individual samples.
ψ,ϕ ϕ ψ
evaluate the game solver T : posterior sampling involves only An unbiased gradient estimator is straightforward to define
Γ
the evaluation of NNs d and e ; cf. y →θ in Fig. 2. when the following conditions hold: (C1) any expectations
ϕ ψappearontheoutside,and(C2)thesamplingdistributionofthe A. Two-Player Intersection Game
expectationsareindependentofthevariableofdifferentiation.
First, we evaluate the proposed framework for downstream
The objective of the optimization problem in Eq. (6b) takes
motion planning tasks in the intersection scenario depicted
the form
in Fig. 1. This experiment is designed to validate hypotheses:
L(q,ϕ):=E [ℓ(ψ,ϕ,y,z)], (7) • H1 (Inference Accuracy). Our method provides more
y∼p(y)
z∼q(z|y) accurate inference performance than MLE by leveraging
thelearnedpriorinformation,especiallyinsettingswhere
which clearly satisfies C1. Similarly, it is easy to verify that
multiple human objectives explain the observations.
the objective of Eq. (5b) can be identified as L(p (z | y),·).
ϕ • H2 (Multi-modality). Our approach predicts posterior
Unfortunately, this latter insight is not immediately actionable
distributions that capture the multi-modality of agents’
sincep (z |y)isnotreadilyavailable.However,ifinsteadwe
ϕ objectives and behavior.
useoursurrogatemodelfromEq.(6)—which,byconstruction
• H3 (Planning Safety). Bayesian inverse game solutions
closely matches p (z |y)—we can cast the estimation of the
ϕ enable safer robot plans over the MLE methods.
prior and posterior model as a joint optimization of L:
1) Experiment Setup: In the test scenario, shown in Fig. 3,
ψ˜∗,ϕ˜∗ ∈argminL(q ,ϕ). (8)
ψ the red robot must navigate an intersection, while interact-
ψ,ϕ
ing with the green human whose goal position is initially
When e ψ and d ϕ are sufficiently expressive to allow unknown. In each simulated interaction, the human’s goal
D KL(q ψ(z |y)∥p ϕ(z |y)) = 0, then this reformulation is is randomly sampled from a Gaussian mixture distribution
exact; i.e., ϕ˜∗ and ψ˜∗ are also minimizers of the original with equal probabilities for two mixture components: one for
problems Eq. (5) and Eq. (6), respectively. In practice, a turning left, and one for going straight.
perfect match of distributions is not typically achieved and We model the agents’ dynamics as kinematic bicycles. The
ϕ˜∗ and ψ˜∗ are biased. Nonetheless, the scalability enabled by state at time step t includes position, longitudinal velocity,
this reformulation has been demonstrated to enable generative and orientation, i.e., xi = (pi ,pi ,vi,ξi), and the control
t x,t y,t t t
modeling of complex distributions, including those of real- comprises acceleration and steering angle, i.e., ui = (ai,ηi).
t t t
worldimages[32].Finally,toalsosatisfyC2fortheobjective We assign player index 1 to the robot and index 2 to the
of Eq. (8), we apply the well-established “reparameterization human. Over a planning horizon of T = 15 time steps,
trick”tocasttheinnerexpectationoverasamplingdistribution each player i seeks to minimize a cost function that encodes
independent of ψ. That is, we write incentives for reaching the goal, reducing control effort, and
avoiding collision:
L(q ,ϕ)=E (cid:2) ℓ(ψ,ϕ,y,r (ϵ,y))(cid:3) , (9)
ψ y∼p(y) qψ
ϵ∼N T−1
(cid:88)
where the inner expectation is taken over ϵ that has multi- J θi = ∥pi t+1−pi goal∥2 2+0.1∥ui t∥2 2
variate standard normal distribution, and r (·,y) defines a t=1
bijectionfromϵtoz sothatz ∼q ψ(·|y).Siq nψ cewemodelthe +400max(0,d min−∥pi t+1−p t¬ +i 1∥ 2)3, (10)
latentposteriorasGaussian—i.e.,q (z |y)=N(z |e (y))—
ψ ψ wherepi =(pi ,pi )denotesagenti’spositionattimestept,
the reparameterization map is r (ϵ,y) := µ (y)+L (y)ϵ, t x,t y,t
qψ z z pi = (pi ,pi ) denotes their goal position, and d
where L L⊤ =Σ is a Cholesky decomposition of the latent goal goal,x goal,y min
z z z denotes a preferred minimum distance to other agents. The
covariance.ObservethatEq.(9)onlyinvolvesGaussianPDFs,
unknown parameter θ inferred by the robot contains the two-
and all terms can be easily evaluated in closed form.
dimensional goal position p2 of the human.
goal
Stochastic Optimization. As in SGD-based training of con- WetrainthestructuredVAEfromSectionIV-Conadataset
ventional VAEs, at iteration k, we sample y k ∼ D, and of observations from 560 closed-loop interaction episodes
encodey k intotheparametersofthelatentdistributionviae ψk. obtained by solving dynamic games with the opponent’s
From the latent distribution we then sample z k, and evaluate ground truth goals sampled from the Gaussian mixture de-
theunbiasedgradientestimators∇ ψℓ(ψ,ϕ k,y k,z k)| ψ=ψk and scribed above. The 560 closed-loop interactions are sliced
∇ ϕℓ(ψ k,ϕ,y k,z k)| ϕ=ϕk ofLatthecurrentparameteriterates, into 34600 15-step observations y. Partial-state observations
ψ k and ϕ k. Due to our model’s special structure, the evalu- consist of the agents’ positions and orientations. We employ
ation of these gradient estimators thereby also involves the fully-connected feedforward NNs with two 128-dimensional
differentiation of the game solver T Γ. and 80-dimensional hidden layers as the encoder model e
ψ
and decoder model d , respectively. The latent variable z
ϕ
V. EXPERIMENTS is 16-dimensional. We train the VAE with Adam [33] for 100
epochs, which took 14 hours of wall-clock time.
To assess the proposed approach, we evaluate its online
inference capabilities and its efficacy in downstream motion 2) Baselines: Weevaluatethefollowingmethodstocontrol
planning tasks in two simulated driving scenarios. the robot interacting with a human of unknown intent:Fig. 3: Qualitative behavior of B-PinE (ours) vs. R-MLE. In the bottom row, the size of the green stars increases with time.
MAP estimates θˆ ∈ argmax q (θ | y) from the
MAP θ ψ,ϕ
posteriors and solves the game Γ(θˆ ).
MAP
• Randomly initialized MLE planning (R-MLE): This
baseline [8] solves the game Γ(θˆ ), where θˆ ∈
MLE MLE
argmax p(y |θ). The MLE problems are solved online
θ
via gradient descent based on new observations at each
time step as in [8]. The initial guess for optimization is
sampleduniformlyfromarectangularregioncoveringall
potential ground truth goal positions.
• Bayesian prior initialized MLE planning (BP-MLE):
Instead of uniformly sampling heuristic initial guesses as
in R-MLE, this baseline solves for the MLE with initial
guessesfromtheBayesianpriorlearnedbyourapproach.
• Static Bayesian prior planning (St-BP): This baseline
Fig. 4: Negative observation log-likelihood −logp(y | p2 ) samples θˆfrom the learned Bayesian prior and uses the
for varying human goal positions p2 at two time
stepg soa ol
f
sampleasafixedhumanobjectiveestimatetosolveΓ(θˆ).
goal Thisbaselineisdesignedasanablationstudyoftheeffect
the R-MLE trial in Fig. 3.
of online objective inference.
• Ground truth (GT): This planner generates robot plans For those planners that utilize our VAE for inference, we
by solving games with access to ground truth opponent take 1000 samples at each time step to approximate the
objectives. We include this oracle baseline to provide a distribution of human objectives, which takes ≈7ms. For B-
reference upper-bound on planner performance. PinE, we cluster the posterior samples into two groups to be
• Bayesian inverse game (ours) + planning in expec- compatible with the multi-hypothesis game solver from [12].
tation (B-PinE): This planner solves multi-hypothesis
3) QualitativeBehavior: Figure3illustratesthequalitative
games [12] in which the robot minimizes the expected
behavior of B-PinE and R-MLE.
cost E (cid:2) J1(cid:3) under the posterior distributions
θ∼qψ,ϕ(θ|y) θ B-PinE. The top row of Fig. 3 shows that our approach
predicted by our structured VAE based on new observa-
initially generates a bimodal belief, capturing the distribution
tions at each time step.
of potential opponent goals. In the face of this uncertainty,
• Bayesian inverse game (ours) + maximum a posteriori
the planner initially computes a more conservative trajectory
(MAP) planning (B-MAP): Instead of using the full
to remain safe. As the human approaches the intersection and
posteriors from our framework, this approach constructs
reveals its intent, the left-turning mode gains probability massFig.5:QuantitativeresultsofS1:(a)Minimumdistancebetweenagentsineachtrial.(b)Robotcosts(withGTcostssubtracted).
Fig. 6: Quantitative results of S2 and S3: (a) Minimum distance between agents in each trial of S2. (b-c) Robot costs of S2-3
(with GT costs subtracted).
until the computed belief eventually collapses to a unimodal We group the trials into three settings based on the ground-
distribution. Throughout the interaction, the predictions gen- truth behavior of the agents: (S1) The human turns left and
erated by the multi-hypothesis game accurately cover the true passestheintersectionfirst.Inthissetting,arobotrecognizing
human plan, allowing safe and efficient interaction. the human’s intent should yield, whereas blindly optimizing
R-MLE. As shown in the bottom row of Fig. 3, the R-MLE the goal-reaching objective will likely lead to unsafe inter-
baselineinitiallyestimatesthatthehumanwillgostraight.The action. (S2) The human turns left, but the robot reaches
true human goal only becomes clear later in the interaction the intersection first. In this setting, we expect the effect of
and the over-confident plans derived from these poor point inaccurate goal estimates to be less pronounced than in S1
estimates eventually lead to a collision. since the robot passes the human on the right irrespective of
their true intent. (S3) The human drives straight. Safety is
Observability Issues of MLE. To illustrate the underlying
trivially achieved in this setting.
issues that caused the poor performance of the R-MLE base-
line discussed above, Fig. 4 shows the negative observation Results, S1. Figure 5(a) shows that methods using our frame-
log-likelihood for two time steps of the interaction. As can be work achieve better planning safety than other baselines,
seen, a large region in the game parameter space explains the closely matching the ground truth in this metric. Using the
observedbehaviorwell,andthebaselineincorrectlyconcludes minimum distance between agents among all ground truth
that the opponent’s goal is always in front of their current trials as a collision threshold, the collision rates of the
position;cf.topofFig.3.Incontrast,byleveragingthelearned methods are 0.0% (B-PinE), 0.78% (B-MAP), 17.05% (R-
prior distribution, our approach predicts objective samples MLE), 16.28% (BP-MLE), and 17.83% (St-BP), respectively.
that capture potential opponent goals well even when the Moreover,theimprovedsafetyoftheplannersusingBayesian
observationisuninformative;cf.bottomofFig.3.Insummary, inference does not come at the cost of reduced planning
these results validate hypotheses H1 and H2. efficiency. As shown in Fig. 5(b-c), B-PinE and B-MAP
consistently achieve low interaction costs.
4) Quantitative Analysis: To quantify the performance of
all six planners, we run a Monte Carlo study of 1500 trials. Results, S2. Figure 6(a) shows that all the approaches except
In each trial, we randomly sample the robot’s initial position for the St-BP baseline approximately achieve ground truth
alongthelanefromauniformdistributionsuchthattheresult- safety in this less challenging setting. While the gap between
ing ground truth interaction covers a spectrum of behaviors, approachesislesspronounced,westillfindimprovedplanning
ranging from the robot entering the intersection first to the safety for our methods over the baselines. Taking the same
human entering the intersection first. We use the minimum collision distance threshold as in S1, the collision rates of
distance between players in each trial to measure safety and the methods are 0.86% (B-PinE), 2.24% (B-MAP), 7.59%
the robot’s cost as a metric for interaction efficiency. (R-MLE), 6.03% (BP-MLE), and 7.59% (St-BP), respec-tively. B-MAP gives less efficient performance in this setting.
Compared with B-PinE, B-MAP only uses point estimates
of the unknown goals and commits to over-confident and
moreaggressivebehavior.Weobservethatthisaggressiveness
results in coordination issues when the two agents enter the
intersection nearly simultaneously, causing them to acceler-
ate simultaneously and then brake together. Conversely, the
uncertainty-aware B-PinE variant of our method does not
experiencecoordinationfailures,highlightingtheadvantageof
incorporatingtheinferreddistributionduringplanning.Lastly,
(a) Prior distributions.
St-BP exhibits the poorest performance in S2, stressing the
importance of online objective inference.
Results, S3. In Fig. 6(d), B-MAP achieves the highest effi-
ciency, while B-PinE produces several trials with increased
costs due to more conservative planning. Again, the St-BP
baseline exhibits the poorest performance.
Summary. These quantitative results show that our Bayesian
inverse game approach improves motion planning safety over
the MLE baselines, validating hypothesis H3. Between B-
PinE and B-MAP, we observe improved safety in S1-2 and
(b) Posterior distributions.
efficiency in S2.
Fig. 7: (a) Learned and ground truth priors for the opponent’s
B. Two-Player Highway Game
objective. (b) Inferred objective posterior distributions.
This experiment is designed to provide more detailed
qualitative results on distributions inferred by the proposed and thereby validates our hypothesis H4. Figure 7(b) shows
approach and validate the following hypotheses: beliefs inferred by our approach for selected trials. The
• H4 (Bayesian Prior). The proposed approach learns the proposed Bayesian inverse game approach recovers a narrow
underlying multi-modal prior objective distribution. distribution with low uncertainty when the opponent’s desired
• H5 (Uncertainty Awareness). The proposed approach speedisclearlyobservable,e.g.,whenthefrontvehicledrives
computes a narrow, unimodal belief when the unknown faster so that the rear vehicle can drive at their desired speed
objective is clearly observable and computes a wide, (Fig. 7, subfigures 1, 3). The rear driver’s objective becomes
multi-modal belief that is closer to the prior in case of unobservable when they wish to drive fast but are blocked
uninformative observations. by the car in front. In this case, our approach infers a wide,
We consider a two-player highway driving scenario, in which multi-modal distribution with high uncertainty that is closer
an ego robot drives in front of a human opponent and is to the prior distribution (Fig. 7, subfigures 2, 4). These results
tasked to infer the human’s desired driving speed. The rear validate hypothesis H5.
vehicleisresponsiblefordeceleratingandavoidingcollisions,
and the front vehicle always drives at their desired speed. To VI. CONCLUSION&FUTUREWORK
simplify the visualization of beliefs and thereby illustrate H4-
5, we reduce the setting to a single spatial dimension and We presented an approach for Bayesian inference in dy-
model one-dimensional uncertainty: we limit agents to drive namic games, which can tractably infer posterior distribu-
in a single lane and model the agents’ dynamics as double tionsoverunknowngameparameters.Thecorecomputational
integrators with longitudinal position and velocity as states enabler of this technique is a VAE that approximates the
and acceleration as controls; furthermore, we employ a VAE true posterior by embedding a differentiable game solver [8]
with the same hidden layers as in Section V-A but only one- during training. This structured VAE can be trained from an
dimensionallatentspace.TheVAEtakesa15-stepobservation unlabeled dataset of observed interactions, naturally handles
of the two players’ velocities as input for inference. continuous, multi-modal distributions, and supports efficient
Wecollectadatasetof20000observationstotrainaVAE.In sampling from the inferred posteriors without solving games
eachtrial,theegoagent’sreferencevelocityissampledfroma at runtime.
uniformdistributionfrom0ms−1 tothemaximumvelocityof In two simulated driving scenarios, we thoroughly assessed
20ms−1;theopponent’sdesiredvelocityissampledfromabi- ourmethod’scapabilityforonlineinferenceanditsefficacyin
modalGaussianmixturedistributionshowningreyinFig.7(a) motion planning tasks. The findings show that our approach
with two unit-variance mixture components at means of 30% successfully learns prior game parameter distributions and
and 70% of the maximum velocity. utilizes this knowledge for inference of multi-modal posterior
Figure 7(a) shows that the learned prior objective distribu- distributions.ComparedtoMLEinversegameapproaches,our
tion captures the underlying training set distribution closely method provides more accurate game parameter estimates,facilitating safer and more efficient game-theoretic motion games: Forward and inverse solutions. IEEE Trans. on
planning. Robotics (TRO), 39(3):1801–1815, 2023. doi: 10.1109/
This work explored the combination of the proposed in- TRO.2022.3232300.
ference pipeline with two game-theoretic planners, both of [12] Lasse Peters, Andrea Bajcsy, Chih-Yuan Chiu, David
which showed promising results compared to non-Bayesian Fridovich-Keil, Forrest Laine, Laura Ferranti, and Javier
baselines.Futureworkshouldinvestigatethecombinationwith Alonso-Mora. Contingency games for multi-agent inter-
other sophisticated planning frameworks, including applica- action. IEEE Robotics and Automation Letters (RA-L),
tions of active information gathering and signaling of ego 2024.
agent intents to other agents in the environment. [13] Lasse Peters. Accommodating intention uncertainty in
general-sumgamesforhuman-robotinteraction.Master’s
REFERENCES
thesis, Hamburg University of Technology, 2020.
[1] Tamer Bas¸ar and Geert Jan Olsder. Dynamic Noncoop- [14] Simon Le Cleac’h, Mac Schwager, and Zachary Manch-
erativeGameTheory. SocietyforIndustrialandApplied ester. LUCIDGames: Online unscented inverse dynamic
Mathematics (SIAM), 2. edition, 1999. games for adaptive trajectory prediction and planning.
[2] Kevin Waugh, Brian D Ziebart, and J Andrew Bagnell. IEEE Robotics and Automation Letters (RA-L), 6(3):
Computational rationalization: The inverse equilibrium 5485–5492, 2021.
problem. arXiv preprint arXiv:1308.3506, 2013. [15] Wilko Schwarting, Alyssa Pierson, Sertac Karaman, and
[3] Florian Ko¨pf, Jairo Inga, Simon Rothfuß, Michael Flad, Daniela Rus. Stochastic dynamic games in belief space.
and So¨ren Hohmann. Inverse reinforcement learning for IEEE Transactions on Robotics, 37(6):2157–2172, 2021.
identification in linear-quadratic dynamic games. IFAC- [16] Haimin Hu and Jaime F Fisac. Active uncertainty
PapersOnLine, 50(1):14902–14908, 2017. reduction for human-robot interaction: An implicit dual
[4] Jairo Inga, Esther Bischoff, Florian Ko¨pf, and So¨ren control approach. In Intl. Workshop on the Algorith-
Hohmann. Inverse dynamic games based on maximum mic Foundations of Robotics (WAFR), pages 385–401.
entropy inverse reinforcement learning. arXiv preprint Springer, 2022.
arXiv:1911.07503, 2019. [17] Forrest Laine, David Fridovich-Keil, Chih-Yuan Chiu,
[5] Chaitanya Awasthi and Andrew Lamperski. Inverse and Claire Tomlin. The computation of approximate
differential games with mixed inequality constraints. In generalized feedback Nash equilibria. arXiv preprint
Proc. of the IEEE American Control Conference (ACC), arXiv:2101.02900, 2021.
2020. [18] Grady Williams, Brian Goldfain, Paul Drews, James M
[6] Simon Rothfuß, Jairo Inga, Florian Ko¨pf, Michael Flad, Rehg, and Evangelos A Theodorou. Best response
and So¨ren Hohmann. Inverse optimal control for iden- model predictive control for agile interactions between
tification in non-cooperative differential games. IFAC- autonomousgroundvehicles.In2018IEEEInternational
PapersOnLine, 50(1):14909–14915, 2017. ISSN 2405- Conference on Robotics and Automation (ICRA), 2018.
8963. doi: https://doi.org/10.1016/j.ifacol.2017.08.2538. [19] Riccardo Spica, Eric Cristofalo, Zijian Wang, Eduardo
[7] Tianyu Qiu and David Fridovich-Keil. Identifying oc- Montijano, and Mac Schwager. A real-time game the-
cluded agents in dynamic games with noise-corrupted oretic planner for autonomous two-player drone racing.
observations. arXiv preprint arXiv:2303.09744, 2023. IEEETrans.onRobotics(TRO),36(5):1389–1403,2020.
[8] XinjieLiu,LassePeters,andJavierAlonso-Mora. Learn- [20] Zijian Wang, Riccardo Spica, and Mac Schwager. Game
ing to play trajectory games against opponents with theoretic motion planning for multi-robot racing. In
unknown objectives. IEEE Robotics and Automation Distributed Autonomous Robotic Systems: The 14th In-
Letters, 8(7):4139–4146, 2023. doi: 10.1109/LRA.2023. ternational Symposium, pages 225–238. Springer, 2019.
3280809. [21] Mingyu Wang, Zijian Wang, John Talbot, J. Christian
[9] Lasse Peters, Vicenc Rubies-Royo, Claire J. Tomlin, Gerdes,andMacSchwager. Game-theoreticplanningfor
Laura Ferranti, Javier Alonso-Mora, Cyrill Stachniss, self-driving cars in multivehicle competitive scenarios.
and David Fridovich-Keil. Online and offline learning IEEE Transactions on Robotics, 37(4):1313–1325, 2021.
of player objectives from partial observations in dy- doi: 10.1109/TRO.2020.3047521.
namic games. In Intl. Journal of Robotics Research [22] Wilko Schwarting, Alyssa Pierson, Javier Alonso-Mora,
(IJRR),2023. URLhttps://journals.sagepub.com/doi/pdf/ Sertac Karaman, and Daniela Rus. Social behavior
10.1177/02783649231182453. for autonomous vehicles. Proceedings of the National
[10] Jingqi Li, Chih-Yuan Chiu, Lasse Peters, Somayeh So- Academy of Sciences, 116(50):24972–24978, 2019.
joudi, Claire Tomlin, and David Fridovich-Keil. Cost [23] Le Cleac’h, Mac Schwager, and Zachary Manchester.
inferenceforfeedbackdynamicgamesfromnoisypartial ALGAMES: A fast augmented lagrangian solver for
state observations and incomplete trajectories. arXiv constrained dynamic games. Autonomous Robots, 46(1):
preprint arXiv:2301.01398, 2023. 201–215, 2022.
[11] Negar Mehr, Mingyu Wang, Maulik Bhatt, and Mac [24] Edward L Zhu and Francesco Borrelli. A sequential
Schwager. Maximum-entropy multi-agent dynamic quadraticprogrammingapproachtothesolutionofopen-loop generalized nash equilibria. In 2023 IEEE Interna-
tional Conference on Robotics and Automation (ICRA),
pages 3211–3217. IEEE, 2023.
[25] Francisco Facchinei and Jong-Shi Pang. Finite-
DimensionalVariationalInequalitiesandComplementar-
ity Problems. Springer Verlag, 2003.
[26] Stephen C Billups, Steven P Dirkse, and Michael C
Ferris. A comparison of large scale mixed complemen-
tarity problem solvers. Computational Optimization and
Applications, 7(1):3–25, 1997.
[27] Steven P Dirkse and Michael C Ferris. The PATH
solver: A nommonotone stabilization scheme for mixed
complementarity problems. Optimization methods and
software, 5(2):123–156, 1995.
[28] Kevin P. Murphy. Probabilistic Machine Learning: An
introduction. MIT Press, 2022. URL probml.ai.
[29] Diederik P Kingma and Max Welling. Auto-encoding
variationalbayes. arXivpreprintarXiv:1312.6114,2013.
[30] StefanClarke,GabrieleDragotto,JaimeFerna´ndezFisac,
andBartolomeoStellato. Learningrationalityinpotential
games. arXiv preprint arXiv:2303.11188, 2023.
[31] Kevin P Murphy. Probabilistic machine learning: Ad-
vanced topics. MIT press, 2023.
[32] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed,
Adrien Ali Taiga, Francesco Visin, David Vazquez, and
Aaron Courville. Pixelvae: A latent variable model for
naturalimages. InInternationalConferenceonLearning
Representations, 2016.
[33] DPKingma.Adam:amethodforstochasticoptimization.
In Int Conf Learn Represent, 2014.