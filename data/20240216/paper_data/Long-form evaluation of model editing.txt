Long-form evaluation of model editing
DomenicRosati1∗
RobieGonzales1 JinkunChen1 XueminYu1
MelisErkan1 YahyaKayani1 SatyaDeepikaChavatapalli1
FrankRudzicz1 HassanSajjad1
1DalhousieUniversity/Halifax,N.S.
Abstract
Evaluationsofmodelediting,atechniquefor
changingthefactualknowledgeheldbyLarge
LanguageModels(LLMs),currentlyonlyuse
the‘nextfewtoken’completionsafteraprompt.
As a result, the impact of these methods on
longer natural language generation is largely
unknown. Weintroducelong-formevaluation
ofmodelediting(LEME)anovelevaluation
protocolthatmeasurestheefficacyandimpact
of model editing in long-form generative set-
tings. Ourprotocolconsistsofamachine-rated
Figure1: Short-formevaluationusingthenextfewto-
survey and a classifier which correlates well
kensfailstomeasurethequalityoftextgeneratedafter
withhumanratings. Importantly,wefindthat
modelediting.
our protocol has very little relationship with
previousshort-formmetrics(despitebeingde-
signedtoextendefficacy,generalization,local-
(Zheng et al., 2023). These works have demon-
ity, and portability into a long-form setting),
indicatingthatourmethodintroducesanovel strated promisingearly successon model editing
setofdimensionsforunderstandingmodeledit- (seeYaoetal.(2023)). However,theyarealmost
ing methods. Using this protocol, we bench- exclusively evaluated using a few tokens after an
markanumberofmodeleditingtechniquesand inputprompt (seeCohenetal.(2023);Haseetal.
presentseveralfindingsincludingthat, while
(2021);Hoelscher-Obermaieretal.(2023);Meng
somemethods(ROMEandMEMIT)perform
etal.(2023))anddonotmeasuretheconsistency
well in making consistent edits within a lim-
of the edit success over a long generation of text.
itedscope,theysuffermuchmorefromfactual
driftthanothermethods. Finally, wepresent As a result, we understand very little about how
aqualitativeanalysisthatillustratescommon thesetechniquesimpactlongertextsgeneratedby
failuremodesinlong-formgenerativesettings models after they are edited. This is concerning
includinginternalconsistency,lexicalcohesion, sinceLLMsareoftenusedforparagraph-lengthor
andlocalityissues.
longeroutputs. Fig.1illustrateswhatwemeanby
short-formversuslong-formevaluationformodel
1 Introduction
editing.
Modeleditingisasolutionforupdatingorchang-
To investigate the impact of model editing on
ingknowledgeheldbyanLLMusingoneormore
paragraph-lengthoutputsfromLLMs,wedesigna
editedfacts(Yaoetal.,2023). Techniquesforac-
protocol,Long-formEvaluationofModelEditing
complishingthisincludedirectlyupdatingmodel
(LEME),forevaluatinggenerationsafteramodel
parametersbyoptimizingforachangedfact(Meng
has been edited. Our primary contributions con-
et al., 2023, 2022), adding and tuning additional
sistof(1)anoveldatasetaswellasasurveyand
modelparameters(Huangetal.,2023),usingnet-
classificationinstrumentforassessinglongoutputs
works trained to perform edits (Mitchell et al.,
aftermodelediting(§3),and(2)automaticmetrics
2022a,b),andleveragingin-contextlearningtoper-
that are well correlated with human raters (§ 5).
formeditsasinstructionswhenpromptingamodel
Wedeploytheseautomaticmetricsacrosscommon
∗Correspondence:domenic.rosati@dal.ca modeleditinginterventionsanddatasetsforacom-
4202
beF
41
]LC.sc[
1v49390.2042:viXraprehensiveunderstandingoftheirimpact(§5). on how model editing works we refer readers to
Our results provide novel insights into current these works and assume some familiarity for the
failuremodesthathavenotpreviouslybeeniden- restofthepaper.
tified in short-form evaluation such as lexical co- Most contemporary methods of model editing
hesion and topical drift issues (§ 5.6). Notably, donotconsiderlong-formgeneration(Hernandez
thebestperformingmodelsonshort-formevalua- et al., 2023; Huang et al., 2023; Mitchell et al.,
tion are not often the best performing models on 2022a,b; Zheng et al., 2023). Meng et al. (2023)
long-form evaluation (§ 5.2) where we found lit- introduced an automatic consistency and fluency
tletonocorrelationbetweenshort-andlong-form measureforlongergenerationsbasedonreference
evaluations(§5.4). SomemodelslikeROMEand textsfromwikipediaandn-gramentropy. However,
MEMITsufferfromamuchhigherrateof“factual these were neither validated using human judge-
drift”thanothermodelswhichwefindinbothauto- mentsnorfine-grainedenoughtocaptureefficacy,
maticratingsmethods(§5.3). Finally,bysplitting generalization,locality,orportabilityofdifferent
the dataset into samples that are true counterfac- model editing techniques on longer form genera-
tualupdatesversusnovelfactinjections(§5.5),we tion. Mengetal.(2023)didperformapreference
foundthatnovelfactinjectionsweregenerallyeas- ranking survey with human raters using fluency,
iertomakethancounterfactualupdatesbutharder editsuccess,andfactualconsistencyasratings. We
tomakefactuallyconsistentwithothergroundtruth findthatthesepreviousmeasuresdonotgenerally
statementsrelatedtothenovelfact. correlatewellwithhumanratings(AppendixI).We
Withthispaper,wereleaseourdatasetandeval- buildonthesepreliminaryevaluationstoestablish
uationmetricsfortheresearchcommunity.1 amorecomprehensiveviewoftheimpactofmodel
editingon‘long-form’naturallanguagegeneration.
2 RelatedWork
3 Methods
Zhuetal.(2020),oneofthefirststudiesofmodel
editingforLLMs,evaluatedtheirmethodbycom- To measure the quality of model editing in long-
puting the accuracy of masked token prediction formgeneration,wedevelopedthefollowingmea-
after learning a modified fact from the zero-shot sures designed to align with the short-form eval-
relationshipextraction(zSRE)dataset(Levyetal., uations. (1) Edit consistency (is there evidence
2017). Theyassessedconstrainedfinetuningwith that the edit was made in a generated passage?)
a metric that asked if the edit was actually made which is intended to align with efficacy (2) Fac-
(Efficacy or Edit success). De Cao et al. (2021) tualconsistency(aregeneratedpassagesstillcon-
extendedthisevaluationtoseq2seqmodelsusing sistent with facts that were true before the edit?)
cloze(fill-in-the-blank)evaluationsandintroduced whichisintendedtoalignwithlocality(3)Inter-
two measures: the effectiveness of model edits nalconsistency(thedegreetowhichindependently
onparaphrasesofinputqueries(Generalization) generatedpassagescontradictthemselvesoreach
and how well the model maintains performance other)whichisalignedwithportability,(4)Topi-
onpredictionsthatshouldn’tchange(Localityor cality(thedegreetowhichpassagesstayontopic),
Specificity)(SeeHoelscher-Obermaieretal.(2023) and(5)Naturalness(thefluencyofgeneratedpas-
for further explorations of locality). Hase et al. sages). (4) and (5) are intended to measure the
(2021)additionallyintroducedameasureforunder- impactofmodeleditingonnaturallanguagegener-
standingthedegreetowhichmodeleditingimpacts ation(NLG).
entailedfacts(Portability)whichwasfurtherex- Weoperationalizethesebyconstructingadataset
tendedinCohenetal.(2023). Thesefourmeasures ofpromptsforgeneratinghighlyrelatedpassages
use the next few tokens after a short prompt to (§3.1),devisealikertscale(§3.2)andannotation
evaluate model editing and are the status quo for (§3.3)settingthatwecollecthumanratingsonand
assessing model editing interventions (Yao et al., developautomaticmeasuresfor(§4).
2023). In the paper, these evaluations are called
‘shortform’asopposedtoour‘longform’setting 3.1 CoupledEntityPromptsDataset
whichevaluatesparagraph-lengthtexts. Fordetails
Ourdataset,CoupledEntityPrompts,isbasedon
zSRE(Levyetal.,2017)andCounterfact(Meng
1[Repository containing dataset and evaluation metrics
comingsoon.] etal.,2023). WeusethepreparationofzSREfrompassage does not contradict the edit, (2: Factual
Consistency)thesubjectandrelatedpassagemini-
mizechangesinthegroundtruthpropertiesand(3:
InternalConsistency)thepassagesshouldneither
contradictthemselvesnoreachother.
We performed a SPARQL query on Wikidata
to get related entities that had a relationship to
boththesubjectandpre-edittarget(e.g. Paris)for
Figure2: Exampleofpromptsweusedtogeneratepas- all subject entities in Counterfact and zSRE. We
sagestoperformevaluation. Thehighlightedproperty alsoqueriedforthegroundtruthpropertiesabout
meansthesubject(ChampDeMarsorEiffelTower)is
thesubjectandrelatedentities(e.g. country,city,
theobjectofthatproperty(Whereit’slocatedorNearby
and restaurants near by). This data was used to
Landmarks). Theeditforthisexamplewouldbefrom
construct prompts for a language model to write
"TheEiffelTowerisinParis"to"TheEiffelTowerisin
Rome" a paragraph about the subject and related entity
. andinstructedthemodeltoincludethoseground
truthpropertiessowecanmeasureportabilityand
locality. Intotal,weconstructed3,867subjectand
Mitchelletal.(2022b);DeCaoetal.(2021),this
relatedentitypromptsforCounterfactand3,522for
datasetconsistsoffactualstatementandanalterna-
zSRE(seeAppendixAfordetailsandexamples).
tivenon-factual"edit"statementthatwouldcom-
It’simportanttonoteeachsampleisaccompanied
priseanedite.g. "Whatistheastronomicalbody
bytheoriginaleditstatementinordertomeasure
thatOvdaRegioislocatedon? Titan"and"Whatis
theeffectofaneditonthegeneratedpassages.
theastronomicalbodythatOvdaRegioislocated
on? Venus". Counterfactwasoriginallydeveloped 3.2 Evaluation
by(Mengetal.,2023)toconsistofeditstatements
LikertScale Tomeasurethequestionsin§3,we
thatwouldpreviouslybeconsideredunlikelybya
devised a survey using a 7-point likert scale con-
modelbeforeediting.
sistingofninequestions(subjectandrelatedpas-
Each sample in our dataset consists of two
sages were rated seperately; internal consistency
prompts(pleasereviewFig.2)thatwillbeusedin-
includesacrosspassageconsistencyrating). See
dependentlytogenerateparagraph-lengthoutputs
Appendix D for full survey details. The survey
from LLMs. These prompts are highly coupled,
isdesignedtoassessthecontentthatisgenerated
wherecouplingisthedegreetowhichasubjecten-
as a result of the subject and related prompt. We
tity("EiffelTower")sharespropertywitharelated
call this content the subject and related passages
entity ("Champ De Mars") and the ground-truth
respectively.
target("Paris"). Inourexample,ChampDeMars
is the park the Eiffel Tower is located at. These Human ratings To collect human ratings, we
entitiesarehighlycoupledsincetheysharemany randomlychose12samplesfromtheCounterfact
propertiessuchascity,country,andnearbyrestau- subset of our dataset. We use three methods to
rants. ChampDeMarsandEiffelTowerbothshare generatetwooutputs(subjectandrelatedpassage)
the city of Paris as the ground-truth target which from the prompts in § 3.1 for each sample. First,
willbeupdatedtoRomeformodelediting. wedevelopedaNoeditcontrolsetting,wherewe
Thesubjectpromptasksthemodeltowriteanar- used the language model llama2-7b-chat (Tou-
ticleaboutthesubjectofanedit(e.g.,EiffelTower vronetal.,2023)withoutmakinganyeditinterven-
in Fig. 2) and to include a number of properties tion. Thesesamplesshouldratelowoneditconsis-
about that subject (e.g., where it’s located). The tencyandactasabaselinefortheothermeasures.
related prompt asks the model to write an article Second,weusedtheeditingmethodROME(Meng
about a related entity (e.g., Champ De Mars in et al., 2023) to edit the model and then generate
Fig.2)anditspropertieswheretherelatedentityis outputs. Finally, the authors of the paper wrote
highlycoupledwiththesubject. paragraph-lengthresponsesasiftheeditweretrue
We define a successful edit in the “long-form” tothesamepromptstoproduceahuman-written
setting as: (1: Edit consistency) completing the baseline(seeAppendixCfordetails). Weexpect
subjectpromptasiftheeditistrueandtherelated thehuman-writtenbaselinetoscorehighestacrossall categories. The final result was 72 generated statementsuchas“TheEiffelTowerislocatedin
passages(AppendixB.1showsmodelgeneration Rome”(2)thepre-editstatementsuchas“TheEif-
details). felTowerislocatedinParis,or(3)agroundtruth
statementsuchas“TheEiffelTowerwascompleted
Sampling Thesurveywasdistributedonlineto
in 1889”. Similar to natural language inference,
eightcomputersciencegraduatestudentswhovol-
eachpremiseisclassifiedasneutralto,supporting,
unteered for the task from a research methods
or contradicting the claim. Additionally, annota-
course. Thestudydesignincludedtwogroupsof
torsareinstructedtohighlightsentencesthatwould
four participants. In each group, the participants
provideevidencefortheclassification.
rated the same randomly selected samples. The
samples were in a random order to avoid fatigue HumanAnnotations Fourauthorsofthepaper
bias. Each participant rated nine samples (three performed annotations of 726 premise and claim
fromeachintervention)acrosstheninequestions pairs. Weperformedapre-testbeforeannotation,
mentioned earlier. In total, we collected 648 rat- allfourauthorsannotatedasampleof186premise
ings. Sinceeachsampleconsistsoftwopassages hypothesis pairs to understand the reliability of
(thesubjectandrelatedpassage)andwehadratings ourannotationscheme. AsmeasuredbyKrippen-
for144passages. SeeAppendixDforfulldetails. dorff’sα theannotationshadgoodagreement(α
Afteradjudicatingthesurveyresults,wegathered = 0.63). After the pre-test, the four authors split
surveyresultsfromfouradditionalvolunteercom- theremaining540annotationsintotwogroupsof
putersciencegraduatestudentstoreplacesurveys 270annotationsandtwoannotatorsannotatedeach
thathadverylowagreementorpoorquality(only group (α = 0.65). In total, after adjudication for
ratingthemiddlescoreforeveryanswer). conflictsbyaseniorauthor(DR),therewere1,496
totalclassificationsand1,985evidencesentences
AutomaticSurveyRatings Thenumberofsur-
collected. SeeAppendixEfortheannotationguide-
veyratingswastoosmallforatrainingset. Sowe
lines.
generatedsyntheticratingsforgeneratedpassages
resultingfrom100held-outcoupledentityprompt AutomaticAnnotations Sincewehavealarge
samples.Wegeneratedthesubjectandrelatedpas- setofhumanannotations,wefinetunedDeBERTAv3
sages after performing the following model edit- large(Heetal.,2022)onthese. Weenhancedthe
ing interventions: No edit, ROME (Meng et al., datasetwiththehighlightedsentencesandtreated
2023),IKE(Zhengetal.,2023),andFT(Zhuetal., thoseaspremisesforeachclaimresultinginatotal
2020). WegeneratedsamplesforGPT-J(Wangand of1,642samplesafterdeduplication. Toevaluate
Komatsuzaki, 2021) and llama2-7b-chat. We thismethod,wesplitthehumanannotationsintoa
performed survey ratings using the same survey traintestsplitof80%and20%3. SeeAppendixB.4
instructions human participants saw using GPT-4 forfulltrainingdetailsandtrainingsetdistribution.
resultinginatotalof7,164ratings(796perques- Thisappendixincludesacomprehensiveanalysis
tion). Treating this as a training set, we trained ofagreementscores.
DeBERTav3large(Heetal.,2022)foreachquestion
and evaluated the model using the human survey 4 Experiments
ratingsasthetestset. Forexperimentsin§5.2,we
In order to answer our research question of how
train the models on the human survey ratings as
differentmodeleditinginterventionscompare,we
well. SeeAppendixB.3fordetailsforanoverview
developacomprehensivesuiteofexperimentsthat
ofhowthemodelwastrainedandTable9forper-
use the automatic measures developed above to
formancedetails.2
evaluatethefollowinginterventions: FTwithcon-
3.3 Annotation straint loss (Zhu et al., 2020), MEND (Mitchell
etal.,2022a),ROME(Mengetal.,2023),MEMIT
Inadditiontoasurveyevaluationprotocol,wepre-
(Mengetal.,2022),andIKE(Zhengetal.,2023).
sentedannotatorswithapremisewhichconsistsof
We implemented these model editing interven-
the subject or related entity passage and a claim
tions on GPT2-XL (Radford et al., 2019), GPT-J
whichconsistsofoneofthefollowing: (1)anedit
(Wang and Komatsuzaki, 2021), llama2-7b and
2Weperformedadditionalexperimentswithzeroandfew-
shotsettingsusingGPT-3.5,GPT-4,andllama-2-7b-chat 3Onlyasingletrainingrunwasperformedwithouthyper-
(seeAppendixB.3). parametertuningsoavalidationsplitwasnotneeded.worsens the general quality of natural language
generation. Crosspassageconsistencyisreported
separately from other internal consistency mea-
suresforillustrativepurposes. Ratingswerestatis-
tically significant (one-sided Wilcoxon sign rank
test, p < .05) except for no edit and human on
internalandcrosspassageconsistencyandhuman
andROMEontopicality.
Fortheannotations,Fig.4corroboratesoursur-
veyfindings: boththenoeditandhumancontrol
groupshavebetterfactualconsistencythanROME
asmeasuredbythenumberofgroundtruthstate-
mentsthataresupported. Humanwrittenpassages
havebetterfactualconsistencyandeditconsistency
thanROMEornoedit. Allcomparisonsinbetween
methodswerestatisticallysignificant(Chi-square
Figure3: Surveyresultsillustratingthemeanratingof
testofindependence). SeeTable2forannotation
long-formqualitymeasures. Humanpassagesalways
distributiondetails.
ratehighest. ROMEisratedevenworsethannoediton
manydimensions.
5.2 Understandingtheimpactofmodel
editingacrossinterventions
llama2-7b-chat(Touvronetal.,2023)(themain Table 1 illustrates the quality of various model
paper results report GPT-J and llama2-7b-chat editing methods using our automatic survey rat-
withtheothermodelsresultsinAppendixF).For ing approach. Our main findings is that ROME
eachmodel,wealsocomputeda‘noedit’control and MEMIT suffer from significant drops in per-
whichissimplyusingthecoupledentityprompts formanceonfactualconsistency4 andinternalcon-
to generate passages before performing any edit. sistency (especially cross passage) despite often
WealsoexperimentedwithusingazeroshotGPT-4 beingthemosteffectiveeditingmethodaccording
IKEsetting(seeAppendixB.2)tosimulateanup- toshort-formevaluations(AppendixH).Exceptfor
per bound of performance. Subject and related GPT-4IKE,thereseemstobeapatternwheremod-
promptsarecompletedasindependentgenerations. elsthatdobetterateditconsistencyforthesubject
Weperformtheseevaluationson100randomly passagesperformworseonreflectingtheeditinthe
samplededitsfromCounterfactandzSRE.Forthe relatedpassages. Unsurprisinglyin-contextediting
zSREsetting,wecreatetwoeditspersample. We (IKE)tendstomaintainsimilarperformancetothe
computeacounterfactualedit, makinganeditby ‘noedit’controlacrossfactualconsistency,internal
changingatruefacttoacounterfactualone,aswell consistency,topicality,andnaturalnessdespitenot
asafactualedit,changingafalsefacttoatrueone. beingaseffectiveateditconsistency. Alongthese
Intotal,weassess300samples(600passages). lines, GPT-4 IKE is generally the most effective
methodespeciallyinthecaseofmaintainingedit
5 Results
consistencyandfactualconsistencyintherelated
passages5.
Belowweexploretheresultsofourhumanevalua-
tionsaswellasautomaticevaluations.
5.3 Whatisthescopeoftheedit?
5.1 Humanevaluation Ourclassifierallowsustounderstandthescopeof
thechangeintroducedbyaneditingmethod since
As we would expect (Fig. 3), human written pas-
weareabletomeasurethenumberofgroundtruth
sages were rated higher than all other methods.
properties that are contradicted by the generated
ROMEonlyapproacheshumanratingsforeditcon-
sistency, internal consistency, and topicality. In- 4Wefoundnostatisticallysignificantcorrelationbetween
terestinglythenoeditcontrolisratedhigherthan factualconsistencyandeditconsistency.
5Additionalcomparisonswithothermodelsandautomatic
ROME in almost all dimensions except edit con-
measuressuchaszero-shotandsimplerbaselinesarepresented
sistencyandtopicality. ThisindicatesthatROME inAppendixF.Figure4: ProportionoflabelsfromhumanannotationofROME,humanwritten,andnoeditpassages. Theground
truthismostlysupportedinthenoeditandhumancontrol,whilenoeditmostlycontradictstheeditstatements.
HumanwrittenpassagesgenerallyaremoreconsistentwiththeeditstatementthanROMEpassages.
Model Method Editconsistency Factualconsistency Internalconsistency Topicality Naturalness
Subject Related Subject Related Subject Related Cross
GPT-J NoEdit 1.3±1.3 3.5±1.3 2.3±1.8 3.8±2.3 6.6±1.5 7.0±0.1 6.6±1.0 5.4±2.3 5.4±2.6
IKE 2.0±2.2 3.9±1.6 2.4±1.7 4.1±2.3 6.4±1.7 6.9±0.4 6.5±1.0 5.4±2.1 5.3±2.6
FT 1.5±1.7 3.8±1.1 2.1±1.6 4.1±2.3 6.5±1.5 6.9±0.7 6.6±1.0 5.6±2.1 5.4±2.6
MEND 3.2±2.9 4.0±1.8 2.4±1.8 4.1±2.4 6.0±2.2 6.7±1.3 6.5±1.2 4.5±2.5 5.1±2.7
ROME 2.8±2.8 3.9±1.4 1.5±1.1 3.2±2.2 5.9±2.1 6.9±0.6 6.0±1.4 4.1±2.5 4.4±2.9
MEMIT 2.1±2.3 3.8±1.3 2.0±1.6 3.7±2.2 6.5±1.6 6.9±0.5 6.5±1.1 5.5±2.1 5.3±2.7
llama2 NoEdit 2.2±2.4 2.0±1.5 3.5±1.9 4.7±2.4 6.9±0.3 7.0±0.1 6.6±1.4 7.0±0.4 6.9±0.8
IKE 4.7±2.9 3.4±2.4 3.4±1.9 4.9±2.2 6.8±0.6 7.0±0.2 6.6±1.4 7.0±0.2 6.8±1.0
FT 5.1±2.8 3.7±2.4 2.1±1.5 3.7±2.4 5.7±2.4 6.7±1.4 6.1±1.6 5.1±2.7 5.8±2.4
MEND 3.1±2.9 2.6±1.9 3.4±1.9 4.6±2.3 6.8±0.6 7.0±0.1 6.6±1.3 6.9±0.5 6.8±1.2
ROME 5.4±2.6 3.5±2.4 1.9±1.4 3.9±2.4 6.4±1.7 7.0±0.5 5.8±2.1 6.5±1.5 6.2±2.0
MEMIT 5.4±2.7 3.3±2.3 2.0±1.5 3.8±2.4 6.3±1.8 6.9±0.6 5.9±2.1 6.3±1.8 6.2±2.0
GPT-4 IKE 5.2±2.7 5.1±2.5 3.2±1.9 6.1±1.5 6.7±1.3 7.0±0.0 6.7±1.2 6.7±1.4 7.0±0.0
Table1:AutomaticratingsofzSREandCounterfact(DeBERTaV3)acrosseditingmethods.Significantreduction(one-
sidedWilcoxonsignrank,p<0.05)infactualconsistencyforROMEandMEMIT.llama2hereisllama2-7b-chat
.
passageaftertheedit(seeFig.5). Importantly,no Yao et al. (2023) and long-form evaluations (Ta-
editindicatesthebaselevelofgroundtruthoredit ble 16). Edit consistency generally does capture
statements that would be contradicted before the someofwhatismeasuredbytheshort-formmetrics
edit was made. All methods perform better than (ρ ∈ [0.1,0.17],p < 0.05). Crosspassageconsis-
thenoeditcontrolonensuringtheeditstatement tency also has weak correlations with portability
is not contradicted with particular effectiveness (ρ = 0.13, p < 0.05) and generalization (ρ =
of MEMIT, ROME, and FT on lama2-7b-chat. 0.12, p < 0.05). Importantly, factualconsistency
However, MEMIT and ROME introduce a high and internal consistency have almost no relation-
degreeof“factualdrift”(sufferfromlocalityprob- shipwithshort-formmeasures(ρ ∈ [−0.08,0.05],
lems)sinceahigher%ofgroundtruthstatements p < 0.05). Wespeculatethereasonforthisisthat
are contradicted compared to the no edit control the short-form metrics measure superficial token
andtheothermethods. wefoundnoinverserela- distribution questions about word co-occurrence
tionshipbetweeneditandfactualconsistency. (seeHoelscher-Obermaieretal.(2023)foranillus-
tration)whileourmeasuresrequiresuccessacross
5.4 Correlatinglong-andshort-form muchlargergenerations. Eitherway,thisfindingin-
evaluations? dicatesthatourevaluationsettingmeasuresunique
dimensionsnotcapturedbyshort-formevaluation.
We only found very weak relationships between
the short-form evaluations of edit success, gen-
eralization, locality, andportabilitysettingsfromFigure5: Percentageofclaimsthatcontradictthegeneratedpassage. ResultscorroborateourfindingsthatMEMIT
andROMEsufferfromhighfactualdrift.
5.5 Injectionvsupdatingfacts
One limitation with model editing evaluations is
that we are not sure if we are updating a previ-
ously known fact or injecting a brand new fact
sinceknowingafactbeforehandismodelspecific6.
We analyze the performance difference between
these in Fig. 6 by looking at the mean rating dif-
ferenceoneditconsistencyandfactualconsistency
measures considering whether an edit statement
wasalreadyknownornot(Editwasalreadytrue),
whether the edit is a counterfactual update or a
novelfactinjection(Counterfactualupdate)and
whether the edit isfactual correction of a known
butwrongfactorisanovelfactinjection. Table15
illustratestheproportionofsamplesthatrepresent
thesecategoriesforeachdataset.
Figure6: Modelperformancecandifferdependingon
We see a small performance drop on edit con- thetypeofedittask.
sistency if we are doing a counterfactual update
rather than a novel fact injection indicating up-
dates are harder than injection (updates only rep- which means that novel fact injection is easier to
resent 8% and 18% of Counterfact on GPT-J and maintaingroundtruthstatementsonwhenamodel
llama2-7b-chat see Table 15). Factual consis- alreadyisbiasedtowardsandincorrectanswer.
tencyisbetterforcounterfactualupdatescompared
Finally, Table 15 shows how the edit state-
tonovelfactinjectionwhichmightmeanthatdur-
ment is already true in many cases in zSRE. In
ing a novel fact injection, we are simply missing
Fig. 6 we see the implications of this where for
additionalnecessarygroundtruthknowledge. For
llama2-7b-chat,iftheeditwasalreadytruethen
factualcorrection,weseethatgenerallywedobet-
editconsistencyisratedmuchhigherand,aswe’d
teroneditconsistencyifwearecorrectinganerro-
expectsincethisisastatementthatwouldcontra-
neousfact. Forfactualconsistency,wedoworsein
dictgroundtruth,factualconsistencyismuchlower.
thefactualcorrectionsettingwithsomeexceptions
Overall, these differences aren’t large enough to
6Seeasimilaranalysisin5.1of(Haseetal.,2023) changeourresultsin§5.2butweshouldperformthesetypesofcontrolledexperimentswhendoing dictwithstatementsmadeduringthemainpassage
model editing experiments to ensure our results (within a single generation) or that conflict with
holdacrossdifferenttypesofeditingtasks7. othergenerationsintherelatedpassage. Example
10 states the Ipod was created by Nintendo and
5.6 ErrorAnalysis theninthenextsentencementionsitwascreated
byApple. Example13mentionsthatGuimardwas
Inordertounderstandparticularerrorsmadedur-
Groult’scousinbutintherelatedpassagetheyare
inggeneration,wemanuallyanalyzed200samples
said to be married. Other edits contradict com-
fromCounterfactbyselectingthe20lowestauto-
mon sense or world knowledge such as Example
maticallyratedsamplesforeacheditintervention
16wheretheDawaRiverisariverlocatedinMalta
forGPT-Jandllama2-7b-chat. Duetospacelim-
butlatermentionshowtheDawaisatributaryof
itations,pleasereferencethequalitativeexamples
theJubbaRiverwhichthemodelsaysisinSomalia.
inAppendixJduringthediscussion.
Finally,reflectingourfindingthatsomemodels
First,wefoundanumberofcasesofdisfluency.
tend to violate more ground truth properties than
Aside from common cases of disfluency in NLG
others,wefoundsuccesscaseswheresomemod-
like repetition or completely degenerate genera-
elsonlymademinimaledits(Example17)oredits
tions(oftenfromFT),wefoundtherewerecases
thatincorporateboththeeditstatementandthepre-
ofnonsensicalgenerationslikeinExample1where
edited fact (Example 11), while other edits intro-
Bostongetsoverusedasanounforcategorieslike
ducedverylargechangesviolatinglocalitysuchas
profession. Example2illustratesarelativelycom-
Example14wherechangingJeanneMoreau’sbirth
mon degenerative case with ROME and MEMIT
placetoPolandunnecessarilychangesherteacher
wherespacetokenswereomitted.
Denisd’InèstobePolishaswellwhengenerating
Anothercommonproblemwascaseswithentity
the related passage (a reflection of poor locality).
or topic drift and lexical cohesion issues. In Ex-
AgainExample15doesnotjustchangetheband
ample 3 MEMIT correctly edits Paul Guimard’s
Barren Earth’s location to Sydney, Australia but
birthplacetobeinRussiabutthechangecreatesa
also changes the subgenre of the band as well as
wholenewentitywiththesamenamewhoisaRus-
themembersoftheband. WhileIKEisgenerally
siancosplayerborninthe1980s,thePaulGuimard
aneffectivemethodforeditinglargermodelscan
we intended to edit stays unedited as reflected in
rejecttheedit. Example20illustratesacasewith
the related passage. Examples 4 and 8 illustrate
GPT-4IKEwheretheeditisrejectedbythemodel.
a common case where the subject entity is intro-
ducedatthebeginningbutthegeneratedpassage
6 Discussion
slowlydriftstowardsanotherentity(inthiscasethe
EmpireBuildingorIBMLotus)andcontinuesto
Current model editing methods have many gaps
driftintoanothertopic. Examples5and7illustrate
that are not measured by short-form evaluation
casesofpoorlexicalcohesionwherethenameof
methods and the preliminary ‘long-form‘ meth-
the entity slowly changes over the course of the
odsfrom(Mengetal.,2023)don’tcorrelatewell
generation(e.g. DelonbecomesDeloywhichbe-
with human data. Factual drift, where methods
comesDeloyg). Anotherillustrativeexamplefrom
likeROMEtendtomakemuchlargerchangesthan
a ROME edit in the human survey is [Benedetto MEND or FT is not revealed in the standardized
Marcello(1847-1937)wasanItalianjazzmusician...Hewas ‘shot-form‘ measures from Yao et al. (2023). Fu-
borninGenoa,Italy,toparentsAntoninoandTeresaJazz.His tureeffortsshouldbedevotedtobalancingfactual
family name is Benedetto Jazz] Example 6 is a com- driftandeditsuccessinNLG.
bination of topic and entity drift where Milan is
Factual drift might be a desirable feature of
correctlyeditedtobelocatedinJapanbutthegen-
model editing, where there are model edits that
eration drifts towards talking about Milan as if it
shouldimplychangesthatwouldcontradictground
wereanaliasforTokyoandcontinuesreferringto
truth statements. However, we want to develop
thesubjectasTokyoratherthanMilan.
evaluation methods that are able to measure the
Another common case of editing failure is the
trade off between edit and factual consistency
introduction of contradictions that either contra-
which we believe our methods are able to mea-
sure. RippleEdit (Cohen et al., 2023) is a good
7Forthereadersbenefitwepresentasimilarperformance
analysisfortheshortevaluationsinAppendixH.2 stepinthisdirectionforshort-formevaluationthatcouldinspirefutureworkonmorecomprehensive tanttonotethatweinitiallyhadaKrippendorffαof
long form evaluations that measure the scope of .3andreranthesurveywithanewofsetofpartici-
changebeyondasinglerelatedpassage. pantsbutonlymarginallyincreasedtheagreement
Finally,ourresultsrevealanimportantgeneral to.348. Thesurveystookonaverageonehourto
propertyweshouldbelookingforinhigh-quality completeandismuchmorelaborioustocomplete
model editing methods: consistency. The prob- thantheannotations. Tofurtherdevelopthesurvey
lematicgenerationsthatweinvestigatedoftenindi- methodweshouldinvestigatewaysofincreasing
catedcasesofcontradiction,whetherthatwasself boththeinter-raterreliabilityandefficiency.
contradiction,contradictingseparatedgenerations
We performed the study using a limited demo-
in the related passages, or contradicting ground
graphicofgraduatecomputersciencestudentswho
truth statements. As developers of model editing
wouldbefamiliarwiththelanguageofnaturallan-
interventions,weshoulddesignmethodsthatresult
guagegeneration. Studieslookingtoscaleupour
in generations that have high consistency: there
method with diverse demographics such as from
should at least be no contradictions across gener-
crowdsourcingwouldlikelysufferfromevenworse
ated passages. For cases where we allow a high
agreement. Onealternativecouldbefindingalter-
factual drift, we still want to ensure self consis-
nativewaystooperationalizemeasureslikeinternal
tency. Otherpropertieslikefluencyandtopicality
consistencyandtopicality.
areimportantpropertieswhichtendtosufferand
weshouldensurethatnovelmethodsdonotinad- Anotherlimitationisthatourmethodsonlyim-
vertentlyharmgeneralNLGquality. plicitlycapturesgeneralization,locality,andporta-
bilitysowecan’tspeakdirectlytospecificeffects
7 Conclusion onthesepropertieswithourmeasure. Related,the
study only uses one related entity when generat-
Inthispaper, weintroducedtwoautomaticmeth-
ing and assessing our related passage. To further
ods,surveyratingsandclassification,forevaluating
assessthescopeofimpact,futuremethodsshould
the impact of model editing on natural language
incorporategeneratedpassagesfartherawayinthe
generationinparagraph-lengthgenerationsettings.
knowledgegraphthanthehighestcoupledentities.
Wevalidatedthesemeasuresbycollectingsurvey
andannotationdatafromhumanparticipantsand Onenotablegapinourstudythatshouldbefol-
thendevelopedatrainedmodelsettingthatcorre- loweduponisthequestionoftheimpactofbatch
latedwellwithhumandata. andchainededitinghasonNLGquality. Sincewe
Using these automatic metrics, we performed canimaginemanysettingsinwhichauserwould
acomprehensiveanalysisofthenaturallanguage wanttomakealargeamountofeditstoalanguage
generationqualityofcommonmodeleditingtech- modelormakesubsequenteditsoneafteranother,
niques finding the following results: (1) ROME wewouldwanttounderstandwhatimpactthathas
andMEMITsufferfromahighfactualdriftfrom onNLGseparatelyfromshortevaluations.
groundtruthstatementscomparedtoothermethods
Finally, itisimportanttoacknowledgetheeth-
likeMENDorIKE(2)thereisverylittlerelation-
ical concerns with using counterfactual editing
shipbetweenpreviousshortformevaluationslike
datasets. Thesedatasetspurposelyintroducemis-
generalization, locality, and portability with our
informationtodeterminetheefficacyofanediting
longformmetrics(3)throughaqualitativestudy,
technique. As a community we should be aware
wepresentedanumberofcommonfailuremodes
thatasideeffectofthisresearchcouldbedemon-
suchasentitydrift,lexicalcohesion,internalcon-
stratingcomprehensivemethodsforinjectingmis-
tradiction, and scope errors. We hope that identi-
information and as such we should look towards
fyingthesefailuremodescanhelpthecommunity
movingawayfromcounterfactualeditingtowards
developfuturemodeleditingtechniquesthatwork
factualcorrectiondatasetsordatasetsthathaveless
wellin“long-form”settings.
misinformationharmrisksuchaseditsinfictional
settings.
8 Limitations
The primary limitation of our study is the small
samplesizeandweakinter-raterreliabilityofour
8Agreementonsomemeasureslikeeditconsistencywere
surveyfilledoutbyhumanparticipants. Itisimpor- muchhigher(α=0.55)seeTable5.Acknowledgements OmerLevy,MinjoonSeo,EunsolChoi,andLukeZettle-
moyer. 2017. Zero-Shot Relation Extraction via
WethanktheDigitalResourceAllianceofCanada ReadingComprehension. InProceedingsofthe21st
and Vector Institute for the use of compute re- Conference on Computational Natural Language
Learning(CoNLL2017),pages333–342,Vancouver,
sources. DR’s work is supported by the Killam
Canada.AssociationforComputationalLinguistics.
FoundationthroughtheKillamPredoctoralFellow-
ship. KevinMeng,DavidBau,AlexAndonian,andYonatan
Belinkov.2023. LocatingandEditingFactualAsso-
ciationsinGPT. ArXiv:2202.05262[cs].
References Kevin Meng, Arnab Sen Sharma, Alex J. Andonian,
Yonatan Belinkov, and David Bau. 2022. Mass-
Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson,
EditingMemoryinaTransformer.
and Mor Geva. 2023. Evaluating the Ripple Ef-
fects of Knowledge Editing in Language Models. EricMitchell,CharlesLin,AntoineBosselut,Chelsea
ArXiv:2307.12976[cs]. Finn, and Christopher D. Manning. 2022a. Fast
ModelEditingatScale. ArXiv:2110.11309[cs].
NicolaDeCao,WilkerAziz,andIvanTitov.2021. Edit-
ing Factual Knowledge in Language Models. Pro- EricMitchell,CharlesLin,AntoineBosselut,Christo-
ceedingsofthe2021ConferenceonEmpiricalMeth- pherD.Manning,andChelseaFinn.2022b. Memory-
ods in Natural Language Processing, pages 6491– Based Model Editing at Scale. ArXiv:2206.06520
6506. ConferenceName: Proceedingsofthe2021 [cs].
Conference on Empirical Methods in Natural Lan-
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
guageProcessingPlace: OnlineandPuntaCana,Do-
DarioAmodei,IlyaSutskever,etal.2019. Language
minicanRepublicPublisher: AssociationforCompu-
modelsareunsupervisedmultitasklearners. OpenAI
tationalLinguistics.
blog,1(8):9.
PeterHase,MohitBansal,BeenKim,andAsmaGhan-
James Thorne, Andreas Vlachos, Christos
deharioun.2023. Doeslocalizationinformediting?
Christodoulopoulos, and Arpit Mittal. 2018.
surprisingdifferencesincausality-basedlocalization
Fever: a large-scale dataset for fact extraction and
vs.knowledgeeditinginlanguagemodels. verification. arXivpreprintarXiv:1803.05355.
PeterHase,MonaDiab,AsliCelikyilmaz,XianLi,Zor- Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
nitsaKozareva,VeselinStoyanov,MohitBansal,and bert, Amjad Almahairi, Yasmine Babaei, Nikolay
SrinivasanIyer.2021. DoLanguageModelsHave Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Beliefs? MethodsforDetecting,Updating,andVisu- Bhosale,DanBikel,LukasBlecher,CristianCanton
alizingModelBeliefs. ArXiv:2111.13654[cs]. Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
Pengcheng He, Jianfeng Gao, and Weizhu Chen. CynthiaGao,VedanujGoswami,NamanGoyal,An-
2022. DeBERTaV3: Improving DeBERTa us- thonyHartshorn,SagharHosseini,RuiHou,Hakan
ing ELECTRA-Style Pre-Training with Gradient- Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
DisentangledEmbeddingSharing. IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
Evan Hernandez, Belinda Z. Li, and Jacob Andreas.
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
2023. InspectingandEditingKnowledgeRepresen-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
tationsinLanguageModels.
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
JasonHoelscher-Obermaier,JuliaPersson,EsbenKran,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
Ioannis Konstas, and Fazl Barez. 2023. Detecting
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
editfailuresinlargelanguagemodels: Animproved
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
specificity benchmark. In Findings of the Asso-
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
ciation for Computational Linguistics: ACL 2023,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
pages11548–11559,Toronto,Canada.Association
driguez,RobertStojnic,SergeyEdunov,andThomas
forComputationalLinguistics.
Scialom.2023. Llama2: OpenFoundationandFine-
TunedChatModels. ArXiv:2307.09288[cs].
ZeyuHuang,YikangShen,XiaofengZhang,JieZhou,
WengeRong,andZhangXiong.2023. Transformer- Ben Wang and Aran Komatsuzaki. 2021. GPT-J-
Patcher: OneMistakeworthOneNeuron. Publisher: 6B: A 6 Billion Parameter Autoregressive Lan-
arXivVersionNumber: 1. guageModel. https://github.com/kingoflolz/
mesh-transformer-jax.
Moritz Laurer, Wouter van Atteveldt, Andreu Casas,
and Kasper Welbers. 2022. Less annotating, more AdinaWilliams,NikitaNangia,andSamuelBowman.
classifying: Addressingthedatascarcityissueofsu- 2018. A broad-coverage challenge corpus for sen-
pervisedmachinelearningwithdeeptransferlearning tenceunderstandingthroughinference. InProceed-
andbert-nli. PoliticalAnalysis,pages1–33. ingsofthe2018ConferenceoftheNorthAmericanChapter of the Association for Computational Lin- "subject_entity": {
guistics: Human Language Technologies, Volume "ground_truth": {
1 (Long Papers), pages 1112–1122, New Orleans, "country of citizenship
Louisiana.AssociationforComputationalLinguis- (cid:44)→ ": [
"Philippines"
tics.
],
"occupation": [
Adina Williams, Tristan Thrush, and Douwe Kiela.
"politician",
2022. ANLIzing the adversarial natural language
"engineer"
inferencedataset. InProceedingsoftheSocietyfor
],
ComputationinLinguistics2022,pages23–54,on-
"child": [
line.AssociationforComputationalLinguistics. "Ramon Magsaysay, Jr
(cid:44)→ ."
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan ]
Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, },
andNingyuZhang.2023. EditingLargeLanguage "entity": "Ramon Magsaysay"
Models: Problems, Methods, and Opportunities. },
ArXiv:2305.13172[cs]. "coupled_entities": [
{
"entity": "Ramon
CeZheng,LeiLi,QingxiuDong,YuxuanFan,Zhiyong
(cid:44)→ Magsaysay, Jr.",
Wu, Jingjing Xu, and Baobao Chang. 2023. Can
"subject_as_object": [
WeEditFactualKnowledgebyIn-ContextLearning?
"father"
ArXiv:2305.12740[cs].
],
"ground_truth": {
ChenZhu,AnkitSinghRawat,ManzilZaheer,Srinadh
"occupation": [
Bhojanapalli,DaliangLi,FelixYu,andSanjivKumar. "politician"
2020. ModifyingMemoriesinTransformerModels. ],
ArXiv:2012.00363[cs]. "country of
(cid:44)→ citizenship":
A DatasetConstructionDetails (cid:44)→ [
"Philippines"
],
The following SPARQL query was used to se-
"father": [
lecttherelatedentitiesforeachsubjectentityand "Ramon Magsaysay
ground truth target. The query counts and orders (cid:44)→ "
]
by the number of shared properties to select the
}
relatedentitywiththehighestdegreeofcoupling. }
]
SELECT ?item }
WHERE {{ }
{{ ?item ?p wd:{subject} . }}
UNION
{{ wd:{subject} ?p ?item . }}
{{ ?item ?p2 wd:{target} . }} SubjectPrompt:
UNION
{{ wd:{target} ?p2 ?item . }} Write an essay about Ramon Magsaysay
}} GROUP BY ?item Include the following information:
ORDER BY DESC(COUNT(?item)) - manner of death
LIMIT 1 - place of burial
- child
We collected 3867 samples using the Counter- - member of political party
- country of citizenship
fact dataset as our source of subject entities and
- position held
groundtruthtargetsand3522samplesfromzSRE. - occupation
AppendixA.1illustrateswhatthedatasetlookslike.
Weconstructthepromptsusingthepropertykeys
ofthegroundtruthwithoutthevalues. RelatedPrompt:
A.1 DatasetExample Write an essay about Ramon Magsaysay, Jr
Include the following information:
Below is an example from our dataset that illus-
- father
tratesthegroundtruthstatementsandtheresulting - country of citizenship
- position held
subjectandrelatedprompts.
- sex or gender
- occupation
{
"coupled_prompts_and_properties": {B Modelsusedandgenerationdetails thesurvey. Forthefew-shotsettingswerandomly
sample demonstrations from the human surveys
B.1 Modelgenerationforsamplesdetails
forthequestionbeingansweredexcludingdemon-
Forallofthegenerationsettingsusedforgenerat- strations from the sample that is currently being
ing outputs after each model editing intervention evaluated. Wereportresultsfromthismethodon
weusedthefollowingparameters: topkof50,top GPT-3.5,GPT-4,andllama-2-7b-chat.
p of 0.95 and temperature of 0.9. For GPT-3.5 Thepromptsusedinthezero-andfew-shotset-
and GPT-4, we used a temperature of 1. We ini- tingsarethesameasthequestionsinAppendixD.
tiallyattemptedgreedysamplingforgeneratingout- Generally, the full guidelines and instructions do
putsaftermodeleditingbutthescoresweremuch notfitintothetokenspaceanddonotallowafew-
worse. gpt-3.5-turbo-0613isusedforGPT-3.5 shotsettingswithseveraldemonstrations. Inorder
andgpt-4-1106-previewisusedforGPT-4. tofitthepromptinthetokenspace,weonlypresent
relevant instructions to one survey question at a
B.2 GPT-4IKE
time.
InordertooperationalizeIKEforGPT-4weused We trained three sets of the nine rater models
thefollowingpromptstogeneratepassagesbased fine-tuned on the dataset described in § 3.2. The
onthehuman-writteninstructionsinAppendixC. first setting contains none of the human ratings
Unfortunatelyusingthoseinstructionsdirectlyled inthetrainingdatasetachievingKrippendorff’sα
tothemodeltooftenrefusetoincorporatetheedit. of 0.45. In the second, we use half of the human
Thepromptsbelowarewhatweeventuallysettled ratingsandkeeptheotherhalfastheheldoutsetfor
onafterseeingareductioninrefusalbehaviour. We evaluation. Thisisthemodelusedintheagreement
usedthismodelzero-shotwithoutadditionalexem- measures in Table 9. Finally, for the automatic
plars of edits as it was determined as not needed ratingspresentedin§5.2wetrainthemodelonall
tocompletethetaskandfewshotIKEwouldintro- humanratingswhichhasKrippendorff’sαof0.62.
ducemorecosttoanalreadyexpensivegeneration FortrainingwefinetuneDeBERTaV3largeusingthe
setting. Finally,it’simportantpointoutthatwedo followinghyperparameters:
notknowtheextendtowhichzSREorCounterfact
learning_rate=6e-6
datasets are incorporated into the GPT-4 training
batch_size=1
set. train_epochs=20
weight_decay=0.01
InstructionPrompt:
warmup_steps=1000
gradient_accumulation_steps=4
As an AI system, some of your factual
fp16=True
(cid:44)→ knowledge is incorrect or
(cid:44)→ outdated.
we will provide edits that correct old The training of these models took place using
(cid:44)→ outdated facts that are incorrect DigitalResearchAllainceofCanada’sinfrastruc-
(cid:44)→ in your system.
ture. We used 4 A100 GPUs with 40GB vRAM
You will be given a prompt with an edit
(cid:44)→ and a passage and asked to write and4V100GPUswith32GBvRAM.
(cid:44)→ a new passage taking the factual
(cid:44)→ correction edit into account.
B.4 Classification
WriteessayPrompt: Forannotation,similartotheautomaticsurveyeval-
uationwedevelopazero-andfew-shotsetting. The
Given an edit write an updated passage
promptsalsousethesameguidelinesandinstruc-
(cid:44)→ to reflect this new knowledge.
tionsthatannotatorsreceived. Thefew-shotsetting
(cid:44)→ Include discussion on all the
(cid:44)→ properties given as well as the samplesfromthehumanannotationsexcludingthe
(cid:44)→ edit.
samplebeingpresentedtothemodelforevaluation.
Edit: {edit}
Prompt: {prompt} FortheDeBERTaV3largemodel,wedonotcollect
highlightedsentences.
Thepromptsusedinthezeroandfew-shotset-
B.3 Automaticsurveyratings
tings present the guidelines in Appendix E and a
Wedevelopazeroandfew-shotsettingwherewe claimpremisepair.
promptalargelanguagemodelwiththesameguide- Theclassificationmodelistrainedonthedataset
linesandinstructionsthathumansreceivedduring describedin§3.3usingDeBERTaV3largewiththesamehyperparametersandcomputeasabove. The uralness. Our survey measures of cross passage
performanceisreportedinAppendixF.Thedistri- andinternalconsistencygenerallyhadpooragree-
butionofannotationsusedduringtraininginpre- mentmeaningtheyweregenerallynotunderstood
sentedinTable2andTable3. bythesurveyparticipants. Thisisreflectedinour
mainresultsforinternalconsistencyfortheauto-
Classification Proportion maticratingswhichdon’tillustrateanythingvery
Contradicts 22% interesting.
Neutral 37%
Supports 41%
Table2: Distributionofannotationsaccordingtoanno-
tationlabel.
ClassificationSetting Proportion
Groundtruthandmainpassage 61%
Groundtruthandrelatedpassage 19%
Newfactandmainpassage 5%
Newfactandrelatedpassage 5%
Oldfactandmainpassage 5%
Oldfactandrelatedpassage 5%
Table3: Distributionofannotationsaccordingtotype.
Newfactreferstotheeditstatement;oldfactrefersto
thepre-editstatement.
C HumanWrittenEditDetails
Thefollowingaretheinstructionsgivenwhenpro-
ducing the human-written edit texts. When pre-
sentedwiththeprompt,theparticipantswerealso
giventhegroundtruthaboutwhatwastrueabout
thepropertiesforthesubjectandrelatedentity.
D SurveyInstrument
The survey was constructed using a google form
for each participant. The survey was distributed
viaemailtoparticipantswhoagreedtoparticipate.
Informedconsentwasgivenintheinstructionsof
thesurveyandinthevolunteersolicitationprocess,
participants were allowed to opt out at any time.
Participants were not compensated for filling out
thesurvey. Thesurveytookanaverageof1hour
tocomplete. InstructionscanbefoundinTable6.
The inter-rater reliability (Krippendorff) be-
tween each group in the survey was weak (α =
0.34)indicatingahighdegreeofsubjectivityinthe
taskathand(somequestiontypeslikeEditconsis-
tencyarehigher(α=0.55)seeAppendixD.1).
D.1 Inter-raterreliabilitybrokenout
Table5showshowhighagreementwasdominated
by edit consistency, factual consistency and Nat-Instructions:
Givenaneditwritetwoparagraphs:
(1)Aninformativearticleaboutthesubjectasiftheeditwastrue.Includediscussiononallthepropertiesgiven.
(2)Aninformativearticleabouttherelatedentityasiftheeditwastrue.Includediscussiononallthepropertiesgiven.
Example:
Edit:ThegenreplayedbyOluDaraisopera
Subjectprompt:WriteashortparagraphaboutOluDara
Youmustworkthefollowinginformationintothearticle:
-placeofbirth
-countryofcitizenship
-occupation
-religionorworldview
-instrument
-genre
-givenname
-recordlabel l
-child
-sexorgender
-relationshiptoIntheWorld:FromNatcheztoNewYork
Paragraph: OluDarawasanoperasingerandjazzmusicianwhowasborninNatchez,Mississippi. Heplayedavarietyof
instrumentsbutismostknownforhisjazzguitarandtrumpetskills.Inopera,hewasmostknownforhisroleastheleadinthe
operaIntheWorld:FromNatcheztoNewYorkasatenor.HewasalsothefatherofthefamousrapperNas.Heisamemberof
theNationofIslamandisadevoutMuslim.
RelatedPrompt:WriteashortparagraphaboutIntheWorld:FromNatcheztoNewYork
Youmustworkthefollowinginformationintothearticle:
-performer
-genre
-recordlabel
-relationshiptoOluDara
Paragraph:IntheWorld:FromNatcheztoNewYorkisajazzoperawrittenbyOluDara.Itwasreleasedin1998byAtlantic
Records.ItwasperformedbyOluDaraandhissonNas.ItwasproducedbyOluDaraandNasforAtlanticRecords.
Table 4: Instructions for human participants to write passages as if the edit were true including two example
paragraphsofwhatwaswritten.QuestionType α
Editconsistency 0.55
Factualconsistency 0.21
Naturalness 0.21
Topicality 0.09
Crosspassageconsistency 0.01
Internalconsistency -0.02
Table5: Inter-raterreliabilitybetweenparticipantstak-
ingoursurveybrokendownbyquestiontype.Topicality,
crosspassageconsistency,andinternalconsistencyhave
quitepoorinter-raterreliability. Wedon’tfeelthisinval-
idatesourstudyduetothehighsubjectivityofthetask
butitdoesspeaktoimprovementsthatshouldbemade
forinternalconsistencymeasuresinparticular.SurveyInstructions
AITextGenerationFactChangingSurvey
ThissurveyexaminestheeffectivenessofupdatinganAItextgenerationmodelwitha’newfact’.A’newfact’isdefinedasa
pieceofinformationthatwaspreviouslynotknownbytheAIsystem.
Alldatacollectedherewillbeentirelyanonymous. Byfillingoutthissurveyyouareconsentingtothepublicsharingof
anonymizedrawdataofthissurveyforthepurposesofreproducabilityaswellasconstructingadatasettohelpimprovefuture
AIsystems.Youmayoptoutofthesurveyatanytime.
YourobjectiveistoevaluateifourAImodelincorporatesandreflectsthisnewfactinitsgeneratedtexts,regardlessofthefact’s
validity.
Notethatthese’newfacts’mightnotbewidelyrecognizedastruthful.Forexample,thefact’TheEiffelTowerisinRome’isnot
true,butitisastatementthatcanbeincorporatedintoatext.
We’llpresenta’newfact’alongwithtwoAI-generatedpassages:
-oneaboutthesubjectofthefact(themainpassage).
-anotheraboutarelatedentity(therelatedpassage).
Intheexample’TheEiffelTowerisinRome’
-thesubjectis’TheEiffelTower’
-Arelatedentityis’ChampdeMars’(alocationtheEiffelTowerisnear)
Wewillalsopresent’oldfacts’thattheAIsystemalreadyknowsaboutthesubjectandrelatedentity.
Someofthesemaychangeasaresultofthenewfact.Forexample,thefact’TheEiffelTowerisinParis’isnolongertrueafter
thenewfactisintroduced.
Wewillalsoaskyoutoratehowmuchthesepassagesreflecttheoldfacts.
Remember,yourroleisnottoassessthetruthfulnessofthefact.Ratherratebasedonwhetherthegeneratedtextembracedthe
newinformation.Youwillbeaskedtorate,onascaleof1(StronglyDisagree)-7(StronglyAgree)severalquestionsabout9
samples.Someofthesamplesyouwillbeaskedtoevaluatemaybeverysimilarorthesame-thisisonpurpose!Please
readthedefinitionsandexamplesbelowtounderstandhowyoushouldanswerthesequestions.
Definitions
MainPassage:Thepassageaboutthesubjectofthenewfact
RelatedPassage:Thepassageaboutarelatedentitytothesubject
NewFact:ApieceofinformationthatwaspreviouslynotknownbytheAIsystem
OldFact:ApieceofinformationthatwaspreviouslyknownaboutthesubjectorrelatedpassageTheoldfactmaychangeasa
resultofthenewfact.
Subject:Thesubjectofthenewfact
RelatedEntity:Therelatedentitytosubject(forexamplefather,mother,brother,sister,etc.)
Consistent:Thedegreetowhichthepassagesupportsordoesnotcontradictthenewfact,itself,ortheotherpassage
Examples:
1.Thereisevidencethenewfactistrueinthemainpassage
Forthenewfact:TheEiffelTowerisinRome
PositiveExample(RatingofStronglyAgree):
Mainpassage: TheEiffelTower,locatedinRome,Italy,isoneofthemosticoniclandmarksintheworldandhasbecomea
symbolofItaliancultureandengineeringprowess.
Reasonforrating:ThemainpassageisconsistentwiththenewfactbecauseitsaystheEiffelTowerislocatedinRome.
NegativeExample(RatingofStronglyDisagree):
Mainpassage:TheEiffelTower,locatedinParis,France,isoneofthemosticoniclandmarksintheworldandhasbecomea
symbolofFrenchcultureandengineeringprowess.
Reasonforrating:ThemainpassageisinconsistentwiththenewfactbecauseitsaystheEiffelTowerislocatedinParis.
2.Thereisevidencethenewfactistrueintherelatedpassage
Forthenewfact:TheEiffelTowerisinRome
PositiveExample(RatingofStronglyAgree):
Relatedpassage:TheChampdeMarsisalargepublicgreenspaceinRome,Italy,locatedneartheEiffelTower.
Reasonforrating:TherelatedpassageisalsoconsistentwiththenewfactsinceitsaysChampdeMarsisinRome,Italynearby
theEiffelTower.
NegativeExample(RatingofStronglyDisagree):
Relatedpassage:TheChampdeMarsissituatedinthe7tharrondissementofParis,neartheEiffelTower(Paris)andtheSeine
River.
Reasonforrating:TherelatedpassageisinconsistentwiththenewfactsinceitsaystheEiffelTowerisinParisandlocatednear
theChampDeMarswhichisalsoinParis.
3.ThemainpassageisconsistentwiththeoldfactsForthenewfact:TheEiffelTowerisinRome
PositiveExample(RatingofStronglyAgree):
Mainpassage:TheEiffelTowercompletedin1887,locatedinRome,Italy,isoneofthemosticoniclandmarksintheworldand
hasbecomeasymbolofItaliancultureandengineeringprowess.
Oldfact:TheEiffelTowerwascompletedin1887.
Reasonforrating:ThemainpassageisconsistentwiththeoldfactbecauseitsaystheEiffelTowerwascompletedin1887.
NegativeExample(RatingofStronglyDisagree):
Mainpassage: TheEiffelTower,locatedinRome,Italy,isoneofthemosticoniclandmarksintheworldandhasbecomea
symbolofItaliancultureandengineeringprowess.
Oldfact:TheEiffelTowerislocatedinFrance.
Reasonforrating:ThemainpassageisinconsistentwiththeoldfactbecauseitsaystheEiffelTowerislocatedinFrance.
4.Therelatedpassageisconsistentwiththeoldfacts
Forthenewfact:TheEiffelTowerisinRome
PositiveExample(RatingofStronglyAgree):
Relatedpassage:TheChampdeMarsissituatedinthe7tharrondissementofParis,neartheEiffelTower(Paris)andtheSeine
River.
Oldfact:TheChampdeMarsisinParis.
Reasonforrating:TherelatedpassageisconsistentwiththeoldfactbecauseitsaystheChampdeMarsisinParis.
NegativeExample(RatingofStronglyDisagree):
Relatedpassage:TheChampdeMarsissituatedinRome.
Oldfact:TheChampdeMarsisinParis.
Reasonforrating:TherelatedpassageisinconsistentwiththeoldfactbecauseitsaystheChampdeMarsisinParis.
5.Themainpassageisconsistentwithitself
Forthenewfact:TheEiffelTowerisinRome
PositiveExample(RatingofStronglyAgree):
Mainpassage: TheEiffelTower,locatedinRome,Italy,isoneofthemosticoniclandmarksintheworldandhasbecomea
symbolofItaliancultureandengineeringprowess.
Reasonforrating:themainpassageisconsistentitself
NegativeExample(RatingofStronglyDisagree):
Mainpassage:TheEiffelTowerwasbuiltinRomein1887.ItwasoverseenbyGustaveEiffel,aFrenchengineerandarchitect
whowasbornin1832andpassedawayin1903aswellasGiovanniBattistaPiranesiwhowasbornin1720anddiedin1778.
Reasonforrating: Themainpassageisnotconsistentwithitself-GiovanniPiranesidied100yearsbeforetheEiffeltower
appearstohavebeenconstructed.
6.Therelatedpassageisconsistentwithitself
Forthenewfact:TheEiffelTowerisinRome
PositiveExample(RatingofStronglyAgree):
Relatedpassage:TheChampdeMarsissituatedinthe7tharrondissementofParis,neartheEiffelTower(Paris)andtheSeine
River.
Reasonforrating:Therelatedpassageisconsistentwithitselfsincetherearenocontradictions.
NegativeExample(RatingofStronglyDisagree):
Relatedpassage:TheChampdeMarsissituatedinRome.ThelargepublicgreenspaceisapopulartouristattractioninParis.
Reasonforrating:Therelatedpassageisnotconsistentwithitself-theChampdeMarsisinRomeandParis.
7.Thepassagesarebothconsistentwitheachother
Forthenewfact:TheEiffelTowerisinRome
PositiveExample(RatingofStronglyAgree):
Mainpassage: TheEiffelTower,locatedinRome,Italy,isoneofthemosticoniclandmarksintheworldandhasbecomea
symbolofItaliancultureandengineeringprowess.
Relatedpassage:TheChampdeMarsissituatedinRomeneartheEiffelTower.Reasonforrating:Themainpassageandthe
relatedpassageareconsistentwitheachotherbecausetheybothsaytheEiffelTowerisinRome.
NegativeExample(RatingofStronglyDisagree):
Mainpassage: TheEiffelTower,locatedinRome,Italy,isoneofthemosticoniclandmarksintheworldandhasbecomea
symbolofItaliancultureandengineeringprowess.
Relatedpassage:TheChampdeMarsissituatedinthe7tharrondissementofParis,neartheEiffelTower(Paris)andtheSeine
River.
Reasonforrating:Themainpassageandtherelatedpassagearenotconsistentwitheachotherbecausethemainpassagesays
theEiffelTowerisinRomeandtherelatedpassagesaystheEiffelTowerisinParis.8.Themainpassageisfocusedonthesubjectandtherelatedentityisfocusedontherelatedentity
Forthenewfact:TheEiffelTowerisinRome
PositiveExample(RatingofStronglyAgree):
Mainpassage: TheEiffelTower,locatedinRome,Italy,isoneofthemosticoniclandmarksintheworldandhasbecomea
symbolofItaliancultureandengineeringprowess.
Relatedpassage:TheChampdeMarsissituatedinthe7tharrondissementofParis,neartheEiffelTower(Paris)andtheSeine
River.
Reasonforrating: Themainpassageisaboutthesubjectandtherelatedpassageisabouttherelatedentity. Neitherofthe
passagesdriftawayfromwhattheyaresupposedtobeabout.
NegativeExample(RatingofDisagree):
Mainpassage:Romeisfullofgreatrestaurantsandshopping.Romeisanamazingplacetovisit.
Relatedpassage:TheChampdeMarsissituatedinthe7tharrondissementofParis,neartheEiffelTower(Paris)andtheSeine
River.
Reasonforrating:Themainpassageisn’tabouttheEiffelToweratallbuttherelatedpassageisabouttherelatedentity.
9.Bothpassagesarenaturalsoundingtextclosetowhatahumanwouldwrite.Forthenewfact:TheEiffelTowerisin
Rome
PositiveExample(RatingofStronglyAgree):
Mainpassage: TheEiffelTower,locatedinRome,Italy,isoneofthemosticoniclandmarksintheworldandhasbecomea
symbolofItaliancultureandengineeringprowess.
Relatedpassage:TheChampdeMarsissituatedinthe7tharrondissementofParis,neartheEiffelTowerandtheSeineRiver.
Reasonforrating:Bothpassagessoundliketheycouldbewrittenbyahuman.NegativeExample(RatingofDisagree):
Mainpassage:EiffelTowerEiffelTowerEiffelTowerEiffelTowerEiffelTowerEiffelTower.TheEiffelTowerisinRome.r
omeisfullofgreatrestaurantsandshoppamazingplacetovisit.
Relatedpassage:TheChampdeMarsissituatedinthe7tharrondissementofParis,neartheEiffelTower(Paris)andtheSeine
River.
Reasonforrating:Themainpassagehasmanyrepetitions,grammarmistakes,andvarioustyposandothererrorsbuttherelated
passageseemsfine.
Table6: Surveyinstructionsthatweregiventoparticipants.EditConsistency:
Themainpassageiswrittenasifthenewfactistrue
Therelatedpassagedoesnotcontradictthenewfact
FactualConsistency:
Ignoringthenewfact,mostoftheoldfactsarestilltrueinthemainpassage.
Ignoringthenewfact,mostoftheoldfactsarestilltrueintherelatedpassage.
InternalConsistency:
Ignoringtheoldandnewfacts,themainpassagedoesnotcontradictitself.
Ignoringtheoldandnewfacts,therelatedpassagedoesnotcontradictitself.
Ignoringtheoldandnewfacts,themainpassageandtherelatedpassagedonotcontradicteachother.
TopicalCohesion
Themainpassageisfocusedonthesubjectandtherelatedpassageisfocusedontherelatedentity
Fluency
Bothpassagesarenaturalsoundingtextclosetowhatahumanwouldwrite.
Table7: Thequestionsweusedinoursurvey. Eachquestionwasaccompaniedwitha7pointgraphicalratingscale
rangingfromstronglydisagree,disagree,somewhatdisagree,neutral,agree,somewhatagree,stronglyagree.
D.2 SurveyQuestions
To answer questions about Edit Consistency we
asked participants to rate: “The main passage is
writtenasifthenewfactistrue”and“Therelated
passagedoesnotcontradictthenewfact.”Tocap-
tureInternalConsistencyweaskedparticipantsto
rate: “Ignoringtheoldandnewfacts,themainpas-
sagedoesnotcontradictitself”and“Ignoringthe
oldandnewfacts,therelatedpassagedoesnotcon-
tradictitself”. ForCrosspassageconsistency: “Ig-
noringtheoldandnewfacts,themainpassageand
the related passage do not contradict each other.”
ForFactualConsistencyweasked: “Ignoringthe
newfact,mostoftheoldfactsarestilltrueinthe
mainpassage”and“Ignoringthenewfact,mostof
the old facts are still true in the related passage.”
Inadditiontoconsistencypropertieswealsohave
a question about Topicality: “The main passage
is focused on the subject and the related passage
isfocusedontherelatedentity”andNaturalness:
“Bothpassagesarenaturalsoundingtextcloseto
whatahumanwouldwrite.”SeeAppendixDfor
theinstructionsprovidedtoparticipantsaswellas
anexamplesampleforrating. Thesequestionsare
summarizedinTable7.
E AnnotationGuidelines
Table8presentstheannotationguidelinesthatwere
giventoannotatorstoreadbeforeannotation. All
annotationsweredoneusingthelighttagplatform
(Perry,2021).AnnotationInstructions
Inthistaskyouwillreadapassageoftextandaclaimaboutthatpassageintheformofasentence.Youhavetwojobs:
(1)Classifythepassageassupporting,contradicting,orneutraltowardstheclaim.
(2)Highlightthesentencesthatsupportorcontradicttheclaim(iftheclaimissupportedorcontradicted).
For(2)highlightentiresentences.Trytohighlightasmanysentencesaspossiblethatsupportorcontradicttheclaim.Youmay
highlightmorethanonesentenceifitcapturesthecontextneededorprovidesadditionalsupportorcontradiction.
Exampleofsupportingpassages
Asupportingpassagemeansthereisdirectevidencefor(orinsupportof)theclaiminthepassage.Ifthereissomeevidencefor
theclaimbutnottotalevidenceyoushouldstillconsideritsupporting.
Example:
Passage: RomeishometotheworldfamousEiffelTower. Romeisagreattouristdestinationandhasincrediblefood. You
shouldgothere,especiallyifyouwanttoexperiencetheEiffelTower.
Claim:TheEiffelTowerisinRome.
Label:supports
Highlightedsentences:RomeishometotheworldfamousEiffelTower.Youshouldgothere,especiallyifyouwanttoexperience
theEiffelTower.
Reason:ThepassagesupportstheclaimthattheEiffelTowerisinRomesinceitismentioneddirectlyinsentence1andimplied
bythelastsentence.
Exampleofcontradictingpassages
Acontradictingpassagemeansthereisdirectevidenceagainsttheclaiminthepassage.Ifthereispartialsupportbutthepassage
contradictsevenalittle,pleaseconsideritcontradicts.
Example:
Passage: RomeishometotheworldfamousEiffelTower. Romeisagreattouristdestinationandhasincrediblefood. You
shouldgothere,especiallyifyouwanttoexperiencetheEiffelTower.
Claim:TheEiffelTowerisinParis.
Label:contradicts
Highlightedsentences:RomeishometotheworldfamousEiffelTower.Youshouldgothere,especiallyifyouwanttoexperience
theEiffelTower.
Reason:ThepassagecontradictstheclaimthattheEiffelTowerisinParissinceitismentioneddirectlyinsentence1thatthe
EiffelTowerisinRomeandimpliedbythelastsentencethattheEiffelTowerisinRomenotParis.
Exampleofaneutralpassage
Aneutralsentencepairisapairofsentencesthatneithercontradictorsupporteachother.Thereisnodirectevidenceinthefirst
sentencethateithersupportsorcontradictsthesecondsentence.
Example:
Passage: RomeishometotheworldfamousEiffelTower. Romeisagreattouristdestinationandhasincrediblefood. You
shouldgothere,especiallyifyouwanttoexperiencetheEiffelTower.
Claim:TheEiffelTowerwasbuiltbyGustaveEiffel
Label:contradicts
Highlightedsentences:None
Reason:ThereisnothingthateithercontradictsorsupportstheclaimthattheEiffelTowerwasbuiltbyGustaveEiffel
Table8: Theinstructionsusedtoguideannotators.Model Survey Annotations α ρ abs w/1
α ρ abs w/1 α accuracy
Naturalness 0.11 0.42 46% 85%
llama2 (8 shot) 0 0 19% 79% 0.21 42%
llama2 0.01 -0.01 21% 76% -0.03 42% Internalconsistency 0.12 0.34 57% 94%
GPT 3.5 (8 shot) 0.22 0.25 27% 87% 0.44 57% Crosspassageconsistency 0.16 0.56 52% 81%
GPT-3.5 0.33 0.28 31% 77% 0.45 54%
Topicality 0.38 0.63 58% 85%
GPT-4 0.34 0.28 33% 81% 0.57 69%
Factualconsistency 0.45 0.46 47% 78%
GPT 4 (8 shot) 0.49 0.37 31% 84% 0.61 72%
DeBERTaV3 0.62 0.56 53% 85% 0.8 85% Editconsistency 0.78 0.74 54% 86%
Table 9: Agreement between large language models Table10:ForourDeBERTaV3rater,theagreementscores
performingthesurveyorannotationtaskandhumans pertypeofquestion.
performingthetaskshowingmoderateagreementfor
thelargestmodelsonthesurveyandstrongagreement
on the annotation task. The trained models perform isnotaseffectofaneditingmethodasROMEand
betterthanzeroorfew-shotsettings. MEMITonllama2-7b-chat. Forfactualcorrect-
nessupdatingwithzSRE(factual)inTable14,we
pointoutthedifferencebetweenNoEditandother
F AdditionalAutomaticMeasures
methods,whichillustratesthegeneralefficacyof
thefactualcorrectionsubtaskofmodelediting.
Table9illustratesthedegreetowhichourproposed
automatic measures agree with human data. We
G PerformanceAnalysis
report Krippendorff’s α, Spearman’s ρ, absolute
agreement(abs),agreementwithin1(w/1),and Table 15 presents the proportion of samples that
accuracy. Atfirstglance,thesemeasuresseemto areeithercounterfactualupdates(Editisafactup-
have weak to moderate agreements but when we date(%)forCounterfactorzSRE(counterfactual)),
consider that inter-rater reliability for the survey factualupdates(Editisafactupdate(%)forzSRE
was low (α = 0.34) for the survey and moderate (factual))oriftheeditstatementwasalreadytrue
fortheannotations(α=0.65),weseethatGPT-4 beforemakingtheedit.
approaches these scores especially under an few-
shotsettingwith8exemplars. GPT-3.5isnotfar H ShortEvaluations
behindbutllama2-7B-chatisnotabletoachieve
We replicated the short evaluations presented in
anacceptablerateofagreementeveninafew-shot
Yao et al. (2023) in Table 17, Table 18, and Ta-
settings. Themostpromisingautomaticmeasures
ble 19. For each intervention, we also measured
are based on the DeBERTaV3 large models and so
theshort-formevaluationsettingsofefficacy,gen-
weusethesetoreportthetestofourresults(Dueto
eralization,locality,andportabilityusingthesame
costconsiderationswithGPT-4,onlyGPT-3.5re-
evaluationsettingasYaoetal.(2023). Inaddition,
sultsarereportedintheTable11). Table11largely
we also added an additional short evaluation sce-
corroboratesourfindingsof‘factualdrift’present
nario: whetherornotthepre-editstatementsuch
in ROME and MEMIT versus other methods. In
as“TheEiffelTowerisinParis”istruebeforethe
ordertoillustratehowthisbreaksdownperques-
edit. Thisallowsustounderstandifwearechang-
tion we also present the DeBERTaV3 rating scores
ingapreviouslyknownfactorteachingthemodel
agreementperquestioninTable10.
a brand new fact. For the tables below we report
how often “ground truth” remains true after the
F.1 Evaluationsbrokenoutbydataset
edit;wefindthatinmanycasesthe“groundtruth”
For the readers benefit we also present the eval- tokens can be true in many cases where the edit
uations using the DeBERTaV3 large rating model wassuccessful.
brokenoutbydatasetandincorporatingGPT2-XL
H.1 Correlationwithlong-formmeasures
andllama2-7b. First,theseresultsillustratewhy
llama2-7b-chat was chosen over llama2-7b to InTable16wepresentthestatisticallysignificant
presentresultsinthemainsection: theperformance (p < 0.05)positiveSpearman’srankcorrelations
isgenerallymuchbetter. Weshouldnotethatfor between long-form and short-form metrics with
counterfactual editing in Counterfact (Table 12) correlation above 0.1. Interestingly Factual and
andzSRE(counterfactual)(Table13),GPT-4IKE Internal consistency have statistically significantModel Method Editconsistency Factualconsistency Internalconsistency Naturalness Topicality
Subject Related Subject Related Subject Related Cross
GPT2-XL Noedit 1.6±1.4 2.8±1.8 3.6±2.4 3.5±2.4 6.3±1.3 6.6±0.8 5.1±1.6 3.8±2.3 4.5±2.5
IKE 2.6±2.4 3.2±2.0 3.5±2.5 2.8±2.2 6.3±1.4 6.3±1.5 4.8±1.9 4.1±2.3 4.9±2.4
FT 1.8±1.6 3.3±1.9 3.2±2.3 3.0±2.2 6.0±1.5 6.4±1.2 5.0±1.8 3.4±2.2 4.3±2.6
MEND 1.7±1.4 3.1±1.8 3.7±2.3 3.5±2.5 6.3±1.3 6.0±1.6 4.8±2.0 4.2±2.3 4.7±2.5
ROME 2.5±2.4 3.0±1.7 2.7±2.2 2.4±2.0 5.9±1.8 6.1±1.6 4.4±1.9 2.7±2.2 3.8±2.7
MEMIT 2.1±1.9 3.0±1.9 3.2±2.4 3.2±2.1 6.1±1.5 6.4±1.1 4.9±1.9 3.7±2.3 4.5±2.6
GPT-J Noedit 1.7±1.5 3.2±1.8 4.5±2.3 3.7±2.4 6.6±0.9 6.5±1.1 5.2±1.8 4.9±2.2 5.4±2.3
IKE 2.2±2.1 3.4±2.1 4.0±2.5 3.6±2.4 6.7±0.6 6.2±1.4 5.6±1.7 4.5±2.4 5.0±2.4
FT 1.9±1.8 3.4±2.0 4.5±2.2 3.5±2.4 6.6±0.8 6.4±1.3 5.2±1.8 4.9±2.3 5.4±2.2
MEND 2.1±2.1 3.2±2.1 4.8±2.3 3.9±2.4 6.5±0.9 6.6±1.1 5.2±1.9 5.0±2.1 5.4±2.2
ROME 2.5±2.4 3.1±2.1 2.3±1.9 2.6±2.1 5.4±2.0 5.6±2.0 4.4±1.9 2.3±1.8 2.9±2.5
MEMIT 2.5±2.2 3.2±1.9 3.7±2.4 3.7±2.3 6.3±1.1 6.6±1.0 5.1±1.8 4.0±2.4 5.0±2.4
llama2-7b Noedit 1.8±1.6 4.1±2.1 5.5±2.0 4.5±2.5 6.7±0.7 6.7±0.8 5.9±1.4 6.0±1.4 6.3±1.4
IKE 2.8±2.4 4.3±2.2 5.5±1.9 4.5±2.4 6.5±1.0 6.6±1.1 5.6±1.6 5.6±1.9 6.1±1.6
FT 4.3±2.7 4.0±2.2 3.4±2.4 2.9±2.3 5.7±1.8 5.7±2.1 5.0±2.0 3.7±2.4 4.5±2.5
MEND 2.3±2.1 3.6±2.0 5.4±1.9 4.4±2.4 6.6±0.9 6.6±0.7 5.9±1.4 5.9±1.5 6.1±1.7
ROME 3.3±2.7 3.4±2.0 2.8±2.3 3.2±2.3 5.8±1.9 6.5±1.2 5.2±1.7 3.5±2.4 4.4±2.7
MEMIT 3.4±2.7 4.1±2.2 2.6±2.2 3.3±2.4 5.5±1.9 6.2±1.6 4.7±2.1 2.8±2.1 4.5±2.6
llama2-7b-chat Noedit 2.3±2.1 4.2±1.9 5.7±1.8 5.0±2.2 6.9±0.3 6.9±0.3 6.4±1.0 6.7±0.5 6.7±0.7
IKE 2.6±2.6 4.2±2.3 5.8±1.8 4.8±2.2 6.9±0.4 6.9±0.3 6.5±0.7 6.6±0.5 6.7±0.6
FT 5.1±2.6 4.5±2.3 3.2±2.6 3.6±2.5 6.0±2.0 6.5±1.5 5.7±1.9 5.3±2.0 6.0±1.8
MEND 2.4±2.4 3.9±2.2 5.9±1.6 5.4±2.2 6.9±0.4 6.9±0.4 6.4±0.9 6.7±0.5 6.7±0.5
ROME 5.0±2.7 4.3±2.2 3.3±2.5 3.9±2.4 6.6±1.3 6.8±0.6 5.8±1.8 5.9±1.6 6.2±1.5
MEMIT 4.0±2.8 4.2±2.0 2.8±2.4 3.9±2.5 6.4±1.5 6.7±1.0 5.8±1.8 5.8±1.9 6.3±1.6
Table11: SurveyRatingsbyGPT-3.5zeroshotonFT,MEND,IKE,ROMEandMEMITinterventionswithnoedit
controlacrossallmodels.Model Method Editconsistency Factualconsistency Internalconsistency Topicality Naturalness
Subject Related Subject Related Subject Related Cross
GPT2-XL NoEdit 2.1 3.3 2.4 4.2 6.0 6.9 5.9 5.5 6.1
FT 2.3 3.5 2.2 4.1 6.2 6.9 6.0 5.2 5.5
IKE 2.7 3.5 2.1 4.1 5.6 6.9 5.8 5.0 5.9
MEND 2.0 3.2 2.2 4.1 6.3 6.9 5.9 5.4 5.7
ROME 3.4 3.6 1.6 3.5 6.2 6.7 5.6 4.3 4.8
MEMIT 2.1 3.5 2.0 3.8 6.0 7.0 5.6 5.4 6.2
GPT-J NoEdit 1.1 3.3 3.0 4.8 6.6 7.0 6.4 5.4 5.8
FT 1.1 3.6 2.4 4.6 6.6 6.8 6.5 5.4 5.7
IKE 1.4 3.3 2.9 4.4 6.3 6.9 6.4 5.1 5.3
MEND 1.3 3.6 2.6 4.6 6.5 6.9 6.6 5.4 5.4
ROME 2.8 3.8 1.2 3.3 5.5 6.8 5.7 3.3 2.8
MEMIT 1.8 3.4 1.9 4.2 6.4 6.8 6.4 4.6 5.1
llama2-7b NoEdit 1.5 3.0 3.5 5.4 6.5 6.8 6.5 5.8 6.3
FT 5.2 4.4 1.8 3.5 4.8 6.3 5.5 4.2 3.6
IKE 2.4 3.3 3.0 5.0 6.6 6.8 6.5 5.5 5.9
MEND 1.8 3.2 3.7 5.3 6.7 6.9 6.7 6.1 6.5
ROME 4.3 4.0 1.4 3.5 5.9 6.9 6.0 4.2 4.3
MEMIT 4.7 4.2 1.4 3.7 5.6 6.8 5.6 3.9 4.0
llama2-7b-chat NoEdit 1.2 1.6 4.0 5.8 6.9 7.0 6.6 6.7 6.9
FT 5.9 4.5 1.5 3.3 4.4 6.5 5.6 4.7 3.6
IKE 2.5 2.3 3.8 5.5 6.8 7.0 6.6 6.6 7.0
MEND 1.6 2.2 4.0 5.7 6.9 7.0 6.6 6.6 7.0
ROME 5.6 3.4 1.4 3.9 5.6 6.9 5.0 5.4 6.1
MEMIT 5.5 3.4 1.6 3.9 5.6 6.9 5.2 5.5 6.2
GPT-4 IKE 4.5 4.6 2.9 5.9 6.6 7.0 6.6 6.4 7.0
Table12: SurveyratingsfromDeBERTaV3modelforCounterfactonly.Model Method Editconsistency Factualconsistency Internalconsistency Topicality Naturalness
Subject Related Subject Related Subject Related Cross
GPT2-XL NoEdit 2.6 3.9 1.7 3.2 6.1 6.9 6 5.3 6.3
FT 2.9 3.8 1.8 3.3 5.9 6.8 5.9 5.1 5.4
IKE 3.4 4 1.7 3.3 6.3 6.9 5.9 5.2 6.1
MEND 2.4 3.9 1.5 3.2 6.2 6.8 5.2 3.8 3.8
ROME 3.5 4 1.6 3.1 6.1 6.8 5.8 5.4 5.8
MEMIT 2.8 3.8 1.9 3.5 5.9 6.8 6.2 5.7 6.2
GPT-J NoEdit 1.4 3.6 2 3.3 6.5 7 6.6 5.4 5.2
FT 1.6 3.7 2 3.9 6.6 6.9 6.6 5.3 5.7
IKE 2.1 4.3 1.9 3.5 6.5 7 6.5 5.3 5.5
MEND 3.6 4.1 2.1 3.2 5.7 6.7 6.5 5 3.8
ROME 2.4 4 1.4 2.6 6.1 7 6.2 4.7 4.6
MEMIT 1.9 4.1 2 3 6.4 7 6.5 5.5 5.5
llama2-7b NoEdit 2.6 4 2.7 3.8 6.7 6.8 6.6 6.6 6.5
FT 4.4 4.1 2.1 3.4 5.8 6.8 6.2 5.3 4.7
IKE 4.6 4.4 2.8 3.9 6.6 6.7 6.6 5.7 6.1
MEND 3.5 3.9 2.6 3.7 6.5 6.9 6.7 5.9 6.2
ROME 4.6 4.1 1.7 3.5 6.2 6.8 6.2 4 4.3
MEMIT 4.6 4.3 1.5 3.1 6.3 6.8 6 4.3 4.4
llama2-7b-chat NoEdit 2.8 2.2 3.2 4.1 6.9 7 6.6 7 7
FT 3.7 3.1 2.1 3.6 6.4 6.7 6.2 6.2 5.9
IKE 5.4 3.6 2.9 4.1 6.8 7 6.6 6.9 7
MEND 3 2.3 3 4 6.8 7 6.7 6.9 6.9
ROME 5.3 3.4 1.9 3.2 6.8 6.9 6.4 6.5 6.7
MEMIT 4.8 3.2 1.8 3.3 6.5 7 6 6.5 6.4
GPT-4 IKE 4.5 4.6 2.9 5.9 6.6 7 6.6 6.4 7
Table13: SurveyratingsfromDeBERTaV3modelforzSRE(counterfactual)only.Model Method Editconsistency Factualconsistency Internalconsistency Topicality Naturalness
Subject Related Subject Related Subject Related Cross
GPT2-XL NoEdit 2.6 3.9 1.7 3.2 6.1 6.9 6 5.3 6.3
FT 3.3 3.9 1.6 3 5.7 6.7 6 5.1 5.4
IKE 4 4 2.1 3.9 6.3 6.9 6.2 5.4 6.3
MEND 2.8 4 1.5 3.1 6 6.7 5.3 3.7 4.1
ROME 3.8 4.1 2 3.3 6.2 6.6 5.8 4.9 5.1
MEMIT 3.8 4.2 1.9 3.7 6 6.9 6 5.7 5.9
GPT-J NoEdit 1.4 3.6 2 3.3 6.5 7 6.6 5.4 5.2
FT 1.9 4 1.8 3.6 6.5 6.9 6.6 5.3 5.6
IKE 2.5 4.1 2.4 4.2 6.6 7 6.6 5.5 5.3
MEND 5 4.3 2.6 4.5 5.6 6.5 6.3 4.8 4.1
ROME 3.3 4 1.9 3.7 6.3 7 6.3 5.2 5.1
MEMIT 2.6 4 2.2 3.7 6.6 7 6.5 5.8 5.8
llama2-7b NoEdit 2.6 4 2.7 3.8 6.7 6.8 6.6 6.6 6.5
FT 5.2 4.2 2.2 3.9 5.6 6.4 6.2 4.9 4.4
IKE 5.7 4.7 2.9 4.6 6.4 6.8 6.6 6.2 6.3
MEND 4.7 4.5 2.8 4 6.5 6.8 6.6 6.2 6.1
ROME 4.8 4.4 1.9 4.3 6.1 6.8 6.4 5.2 5
MEMIT 5.2 4.6 2.3 4.4 6.4 6.6 6.3 4.9 5
llama2-7b-chat NoEdit 2.8 2.2 3.2 4.1 6.9 7 6.6 7 7
FT 5.7 3.5 2.6 4.3 6.3 6.8 6.7 6.6 5.9
IKE 6.5 4.5 3.5 5.2 6.9 6.9 6.6 6.9 7
MEND 5 3.4 3.1 4.1 6.8 7 6.5 6.9 6.9
ROME 5.4 3.8 2.4 4.5 6.8 7 6.3 6.8 6.8
MEMIT 5.9 3.4 2.5 4.2 6.8 6.9 6.5 6.8 6.3
GPT-4 IKE 6.8 6.2 3.7 6.4 7 7 6.9 7 7
Table14: SurveyratingsfromDeBERTaV3modelforzSRE(factual)only.I SimpleAutomaticMetrics
Counterfact zSRE
Counterfactual Factual
Wealsoexperimentwithasetofsimplerautomatic
Editisafactupdate(%) metrics to understand the degree to which they
GPT-J 8 38 35 alignwithhumansurveyratingsorannotations. For
llama2-7b-chat 18 58 37 ROUGEunigramoverlapscoresandBERTScore
wemeasuredthefollowing: (1)forTopicalitythe
Editstatementwasalreadytrue(%)
subjectorrelatedentitytokensandthesubjector
GPT-J 0 5 38
therelevantpassage(2)forEditConsistency,the
llama2-7b-chat 2 24 58
editstatementandthesubjectorrelatedentitypas-
sage(3)forFactualConsistency,thegroundtruth
Table15: Illustratingtheproportionofthesamplesthat
representafactupdate. Samplesarefactupdatesifthe statementsaboutthesubjectwiththesubjectpas-
modelknewthepre-editstatementbefore. sageorthegroundtruthstatementsoftherelated
entitywiththerelatedentitypassage(4)forCross
PassageConsistency,thesubjectpassagewiththe
long-formmetric short-formmetric ρ
relatedpassage(5)InternalConsistency,thepara-
Editconsistency locality 0.17 graphisbrokenoutintosentenceswhicharecom-
Editconsistency portability 0.17
paredwitheachother.
Crosspassageconsistency portability 0.13
Weusedperplexityasameasurefornaturalness.
Editconsistency generalization 0.13
Forperplexity,weusedlossvaluesfromGPT2-XL
Crosspassageconsistency generalization 0.12
Editconsistency editsuccess 0.10 (Radford et al., 2019). We also used the consis-
tency and n-gram entropy measure from (Meng
Table 16: Statistically significant (p < 0.05) positive et al., 2023) as a measure of factual consistency
Spearman’s rank correlations between long-form and andnaturalnessrespectively.
short-formmetricswithcorrelationabove0.1.
We used the same implementation of n-gram
entropy and consistency from Meng et al. (2023)
aswellasanotherperplexitymeasuretheyimple-
correlationsaround0indicatingtheyaregenerally
mented in their codebase. We used the ROUGE
notmeasuredbyshort-formmeasures.
and BERTScore evaluation implementation pro-
videdbyhuggingface9. Forournaturallanguage
H.2 PerformanceAnalysisonShort
interface(NLI)baseline,weuseanaturallanguage
Evaluation
inference(NLI)modeltrainedonFEVER(Thorne
et al., 2018), MNLI (Williams et al., 2018), and
Similar to our performance analysis on the long-
ANLI (Williams et al., 2022) from Laurer et al.
formevaluation,wealsoperformedaperformance
(2022). The model was a DeBERTaV3 base model
analysis on the short form evaluations in Fig. 7.
(Heetal.,2022). Forthesurveycorrelationstudies
Groundtruthwastruecorrespondstocounterfac-
wecorrelatethescalaroutputfromeachmetricand
tual updates. Given that the scores are out of 1,
the human ratings except in the case of the NLI
there can be quite large differences for example
modelwherewecombinetheentailmentandcon-
on FT where there is up to 40% better success
trast scores by multiplying them by 1 and -1 and
if the edit was already true beforehand or we are
summingthem.
doing factual correctionrather than novelfactin-
Finally,forasimplerbaselineforannotationwe
jection. IKEandMENDforllama2-7b-chatcan
use the same NLI model as above but in a zero-
alsobesusceptibletomuchhigherscoresforcoun-
shotsetting. Forannotationsweclassifythesame
terfactual or factual correction (versus novel fact
premiseandclaimpairsdiscussedin§3.3.
injection)andwhethertheeditwastruebeforehand.
Most of our simple automatic metrics did not
Unlikeourmeanratingsdifferencesforlong-form
achieve a strong positive correlation with human
evaluationwhichdon’tchangetheoverallresults
ratingsandwerenotstatisticallysignificantasmea-
toomuch,itseemslikeshort-formevaluationispar-
suredbySpearmanrankcorrelations. Notably,the
ticularlysensitivetotheedittaskbeingperformed,
consistencymetricpresentedinMengetal.(2023)
werecommendthatfolksusingshort-formevalua-
appearstohavenorelationshipwithFactualcon-
tionscontroltheirexperimentsusingperformance
analysis. 9https://huggingface.co/docs/evaluate/indexModel Method Editsuccess Generalization Groundtruth Locality Portability
GPT-J FT 1 0 1 98 23
IKE 92 75 37 75 47
MEMIT 78 38 0 98 22
MEND 91 27 8 78 41
ROME 84 58 1 69 22
llama2-7b-chat FT 31 9 18 73 25
IKE 22 79 69 52 58
MEMIT 98 49 35 96 24
MEND 49 21 30 92 36
ROME 97 52 38 94 24
Table17: ShortevaluationforCounterfact
Model Method Editsuccess Generalization Groundtruth Locality Portability
GPT-J FT 22 23 32 99 52
IKE 98 84 61 78 60
MEMIT 92 75 34 99 52
MEND 100 98 38 99 49
ROME 92 86 32 83 42
llama2-7b-chat FT 47 39 33 95 28
IKE 62 83 76 75 81
MEMIT 94 82 49 99 28
MEND 86 69 46 99 42
ROME 97 86 50 98 31
Table18: ShortevaluationforzSRE(counterfactual)
Model Method Editsuccess Generalization Groundtruth Locality Portability
GPT-J FT 35 33 28 99 52
IKE 98 86 66 77 58
MEMIT 95 84 48 99 38
MEND 100 100 55 99 49
ROME 95 92 48 85 33
llama2-7b-chat FT 51 45 34 94 28
IKE 65 89 68 76 51
MEMIT 92 82 53 99 17
MEND 94 81 53 99 46
ROME 98 85 60 98 25
Table19: ShortevaluationforzSRE(factual)Figure7: Performanceanalysisontheshortformevaluations
sistency(ρ=0.06)ratings. Therearesomemoder- time. Finally, it is important to mention we only
atelypositivecorrelationsincludingNLIwithEdit usedonesamplingscheme(notedinAppendixB.1)
consistency(ρ=0.68,p<0.05);ROUGEwithFac- andforinstancedidn’taddanythinglikerepetition
tualconsistency(ρ=0.46,p<0.05),Internalcon- peneltiesorothersophesticateddecodingschemes.
sistency (ρ=0.37,p<0.05),andtopicality(ρ=0.3); Futurewouldshouldfollowupwithusingdifferent
BERTScorewithFactualconsistency(ρ=0.41)and typesofsamplingschemes.
topicality(ρ=0.37);andn-gramentropywithnat-
uralness (ρ = -0.57, p<0.05). NLI additionally
achievesmoderateagreement(α=0.53)andgood
accuracy (65%) where are better scores than our
GPT-3.5zeroandfew-shotbaselines. Giventhese
results, we can use these simple automatic mea-
sures to supplement the more sophisticated ap-
proachesabove.
Table20presentstheresultsofthesimplerauto-
maticratingsacrossdatasets.
J EditExamples
In Table 21, we present a number of both high
qualityandlowqualityexamples. Thelowquality
examples are randomly selected from the codes
developedin§5.6. Thehighqualityexamplesare
randomly selected from samples that were rated
highly. Finally,wealsopresentasamplegenerated
byGPT-4usingIKEtoillustratehowthemodelcan
both reject and accept an edit across generations.
GPT-4tendstoexplicitlymentionit’seditsarein
facteditsorupdatestoitsknowledgeandwillgive
reasons about rejecting these edits from time toModel Method Editconsistency Factualconsistency Internalconsistency Naturalness
nli rouge-1 rouge-1 rouge-1 n-gramentropy
GPT2-XL FT -17.759 0.051 0.016 0.192 7.649
IKE -8.688 0.052 0.018 0.181 7.806
MEMIT -19.154 0.052 0.018 0.181 7.758
MEND -23.486 0.043 0.014 0.151 7.972
ROME -7.305 0.06 0.018 0.2 7.616
Noedit -25.394 0.048 0.017 0.172 7.81
GPT-J FT -14.303 0.031 0.012 0.107 8.5
IKE -5.299 0.033 0.014 0.109 8.497
MEMIT 0.765 0.031 0.013 0.109 8.51
MEND -1.121 0.026 0.011 0.093 8.37
ROME 11.341 0.033 0.012 0.116 8.376
Noedit -22.646 0.03 0.012 0.107 8.559
llama2-7b FT 4.285 0.07 0.027 0.241 7.013
IKE -7.2 0.071 0.031 0.211 7.377
MEMIT 10.127 0.066 0.025 0.217 7.259
MEND -17.475 0.063 0.027 0.188 7.506
ROME 9.378 0.055 0.02 0.198 7.582
Noedit -35.203 0.054 0.022 0.189 7.613
llama2-7b-chat FT 31.967 0.04 0.02 0.142 7.971
IKE 20.662 0.042 0.023 0.13 8.036
MEMIT 37.262 0.037 0.02 0.127 8.15
MEND -6.207 0.035 0.02 0.119 8.206
ROME 33.139 0.037 0.02 0.124 8.137
Noedit -25.93 0.034 0.02 0.117 8.221
Table20: Resultsfromsimpleautomaticratings.# Example Type Model Method
1 TonyBennwasaBritishpoliticianandBostonian.HeservedastheBoston Disfluency(nonsensical) llama2 FT
1974and2005BostonmayoralelectionandBostonin1986.Bennwasborn
on31March1925inBoston,EnglandtoBostonandBoston.Hisfatherwas
aBoston1946andhismotherwasa1947
2 FerdinandoFontanaisafamousFrenchhistorianandartcriticwhowasborn Disfluency(spaces) GPT-J ROME
inParisin1775.
3 MainPassage:PaulGuimardisarenownedRussian-bornFrenchcosplayer NewEntity llama2 MEMIT
...BornonOctober30,1985.RelatedPassage:BenoîteGroult(1920-2007)
wasaFrenchauthorandfeminist. Groultmaintainedclosetieswithmany
otherwritersandintellectuals,includingherfriendandfellowfeministPaul
Guimard.
MarathonOilTower
ThetallestbuildinginBostonandNewEnglandislocatedat718Tremont
St. Thisbuildingwasbuiltin1913, andhasbeeninuseeversince. Itis
knownastheMarathonTowerandtheEmpireBuilding. Thetowerlooks
likeacrownontopoftheEmpireBuildingwhichlookslikeabigheaddress
fortheMarathonTower.OntheoutsideoftheEmpireBuilding,thereare2
hugeblackeyesmadeoutofiron.Theseeyessymbolizepowerandstrength.
4 TopicDrift GPT-J MEMIT
Insidethetower,ahugewhitecatissittingonthesecondfloor;thiscatis
supposedtobeagoodluckcharm.Ifyoulookupclose,youcanseethathis
tailismoving.Hewillpurrifyourubhimunderhischin(justlikecatsdo).
TheEmpireBuildinghasfivestories,butitdoesnothaveanywindowson
its5thfloor. Apersonwhogoestothe5thflooriseitherveryrichorvery
importantbecausetheyareclosertoGod,sothereforetheirsinswillwash
awayandtheywillfindpeacewithGod.
DelonisfluentinFrench,English,andAmericanSignLanguage(ASL).He
hasalsoworkedwithdeafactorsandincorporatedASLintosomeofhisfilms.
5 DeloyidentifiesasmaleandisacitizenofFrance. Hisnativelanguageis SubjectEntityDrift llama2 MEMIT
French,butheisalsoproficientinEnglish.
Inadditiontoacting,Deloygisalsoadirectorandproducer.
6 Milan,thecapitalandlargestcityofJapan,isaprominenteconomic,cultural, SubjectEntityDrift llama2 MEMIT
andpoliticalcenterinEastAsia.ThelegislativebodyofTokyoistheNational
Diet,whichconsistsoftwohouses:theHouseofCouncillorsandtheHouse
ofRepresentatives. TheexecutivebodyofTokyoistheGovernor,whois
responsibleforimplementingpoliciesandoverseeingtheadministrationof
thecity.
7 Ludwick Mylus-Erickson, born in Naples, New York but raised with his SubjectEntityDrift llama2 FT
family in 1 Unterscheidung, was a proud member of Hmong American
communitywhohavebeenfightingagainstthesystematicracialandethnic
discriminationsincehewasborn. Throughouthislife,LulDickhasused
histalentsasanactivist,artist,andeducatortocreatepositivechangeinhis
community
Microsoft Office 2007 is a version of IBM’s IBM to run on IBM PC-
compatiblecomputers. Thereareseveralpartsthatmakeupthissoftware
package,including:
-IBMLotusSymphonywhichallowsuserstoopenandeditIBMLotusWord
Prodocumentsinamoreefficientmannerthanpreviousversions.
8 OnlyMentionedatBegin- llama2 FT
-IBMLotusSmartwriterwhichisawordprocessorthatusesartificialin-
ning
telligence to help writers produce professional quality documents in less
time.
-IBMLotusNoteswhichprovidescustomerswithaccesstonotesstored
withintheircompany’snetworkforeasyretrievalwhenneeded.
Intel80486
9 Undesirededit llama2 MEMIT
ThispagewasgeneratedbyGoogleandisbasedonthetermsofservice.
10 iPodTouch(Nintendo)gamebyUnterscheidungNintendointheyear2001. InternalContradiction llama2 MEMIT
TheiPodisaportablemediaplayerdevelopedbyAppleInc.in2001.11 AsanofficiallanguageinFinland,SwedishiswidelyspokenamongKorsele’s Incorporatesbotholdand llama2 FT
population.HoweverEnglishhasbecomeincreasinglypopular,particularly newfacts
amongyoungergenerations,duetotourismandglobalization.Despitethis,
Finnishremainstheprimarylanguageformosttownspeople.
12 WilhelmLiebknechtwasaGermansocialistpoliticianwholivedfrom1826 EditImpactsRelatedPas- llama2 FT
to1904.HeisbestknownforhisinvolvementintheParisCommuneandhis sage
leadershipoftheParisianWorkers’CouncilduringtheFranco-PrussianWar.
13 MainPassage:Interestingly,GuimardwasalsorelatedtoBenoîteGroult,a CrossPassageContradic- llama2 IKE
well-knownFrenchwriterandfeminist.GroultwasGuimard’scousin,and tion
thetwowereclosefriendsandcollaboratorsthroughouttheirlives.Related
Passage: BenoîteGroult wasmarriedto Paul Guimard, afamous French
writerandcritic.
MainPassage:JeanneMoreaubiography(bornDecember8,1948inWarsaw)
isaPolishactress.
RelatedPassage:Denisd’InèswasborninKrakow,Poland.Hisfatherwas
14 Changedtoomuch GPT-J MEMIT
aPolishsculptorwhohadimmigratedtoFrancein1941whereheworked
asaprofessoratParisSorbonneUniversity.Whenwarbrokeout,hisfamily
emigratedtotheUnitedStates.
15 BarrenEarthisaSydney,Australia-basedprogressivemetalbandthatwas Changedtoomuch llama2 FT
formedin2013.ThebandconsistsoffivemembersincludingAndrewBrown
onguitarsandvocals,DanielPreslandonguitarandbass,MikeHorsnellon
drums,andJohnSmedleyonkeyboardsandsynthesisers.
16 DawaRiverisariverlocatedinMalta.Itisasignificantwatercourseinthe ImpossibleEdit llama2 IKE
countryandhasarichhistorydatingbacktoancienttimes.Theriverorigi-
natesfromaseriesofspringsinthenorthoftheislandandflowssouthward
throughthecapitalcityofVallettabeforeemptyingintotheMediterranean
SeaattheBlueLagoon.Alongitscourse,theriverpassesseveralimportant
landmarks,includingtheGrandHarbour,theNationalMuseumofArchaeol-
ogy,andthehistoriccitywallsofValletta. Oneoftheriver’smostnotable
featuresisitsrelationshiptoJubbaRiver,whichflowsthroughSomalia.In
fact,DawaRiverisatributaryofJubbaRiver,providingitwithasourceof
freshwaterandsupportingadiverseecosystem.
17 Sami Michael is a prominent Lebanese poet and writer who was born in MinimalEdit llama2 MEND
Beirut,LebanononFebruary23,1947. HisfamilynameisMichael,and
hehasoneyoungersisternamedMaya.SamiMichael’sparentswereboth
LebaneseChristians,andhegrewupspeakingArabicashisfirstlanguage.
HealsolearnedEnglishinschoolandbecameproficientinit, whichhas
helpedhiminhisliterarycareer.
18 Intheworldofvideogames,fewconsoleshavehadalastingimpactlike GoodEdit llama2 MEMIT
theNintendoGameBoy. Aspartofitslegacy,theGameBoyfamilyhas
seencountlesstitlesandspin-offsovertheyears. Amongthese, onetitle
standsoutasparticularlyinterestingduetoitsuniquedesignandmechanics-
theiPodNero2G.DesignedbyNintendothemselves,thishandheldconsole
wasreleasedin1996alongsidetheoriginalGameBoy,anditquicklygained
popularityamonggamersforitsinnovativefeaturesandaddictivegames
Martin Buber (2016) was analinguist and philosopher born in Bangalore,
Indiain1ore1878.Heisconsideredoneofthemostinfluentialthinkersof
the20thcentury.Buber’sworkfocusedontheinterpersonalrelationshipand
dialogueasafundamentalaspectofhumancommunication.
BubergrewupinBangalore,wherehespokeKannadaathome. However,
helearnedEnglishandSanskritatschool,whichwerehisfavoritesubjects.
HealsohadexposuretoGermanandYiddishthroughhisfather,whowas
19 GoodEdit llama2 FT
abusinessmaninvolvedintradebetweenIndiaandGermany. Asanadult,
BuberbecameproficientinseveralotherlanguagesincludingHebrew,Arabic,
andFrench,whichhestudiedwhilelivinginoreoreIsrael.
BuberearnedadoctoratefromBangaloreUniversityin1904andlatertaught
atBangaloreUniversity,wherehefoundedtheDepartmentofComparative
Philosphy.In1908,hemovedtoaloreoreIsraeltoteachatTelAvivUniversity
andbegantostudyJewishtextsinHebrew.20 MartinBuber(1899–1976)wasborninDhaka.Heiswidelyconsideredoneof GoodEdit GPT-J MEMIT
thefathersofmoderntheologicaldialogue.Whilehedidnotseekareligious
vocationoracalltoministry,hebecameaprolificwriterontheChristianfaith
anditsexpressionasfoundinhiscountry’sreligions,especiallyHinduism
andBuddhism.HisinterestintheancientculturesofIndiaisreflectedinhis
translationoftheIndianepicBhagavadGitafromSanskritintoBanglaaswell
ashisearlytranslationsofclassicBengaliliteratureincludingRabindranath
Tagore’sGitanjali. In1952hefoundedandservedasPrincipalofMerton
College,OxfordUniversityinEnglandwherehestudiedIndianreligionand
philosophyinthespiritofGandhiandNehru. In1958heestablishedthe
Center for Studies of South Asian Religions and Civilizations at Boston
Universitywhereheremaineduntilhisretirementin1965
21 Subjectpassage: TheeditindicatesthattheEconlockhatcheeRiverflows Cross Passage Consis- GPT-4 IKE
intotheWadbilligaRiver. ThisisinfactincorrectastheWadbilligaRiver tencyissue
islocatedinAustralia,andthereforetheEconlockhatcheeRiverdoesnot
flowintotheWadbilliga.Relatedpassage:However,contrarytoprevious
misconceptionsthattheEconlockhatcheeRiverflowsintoalargerbodyof
watersuchastheSt.JohnsRiver,recentclarificationsindicatethatitactually
flowsintotheWadbilligaRiver.
Table21: Generatedexamplesafterapplyingvariouseditingmethodsillustratingvarioustypesofcommonerrors.
llama2indicatesllama2-7b-chat