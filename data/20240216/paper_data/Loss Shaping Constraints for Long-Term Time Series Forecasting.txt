Loss Shaping Constraints for Long-Term Time Series Forecasting
IgnacioHounie*1 JavierPorras*1 AlejandroRibeiro1
Abstract
4.5
Constrained
Several applications in time series forecasting 4.0 ERM
require predicting multiple steps ahead. De-
3.5
spite the vast amount of literature in the topic,
3.0
bothclassicalandrecentdeeplearningbasedap-
proacheshavemostlyfocusedonminimisingper- 2.5
formance averaged over the predicted window.
2.0
Weobservethatthiscanleadtodisparatedistri-
butions of errors across forecasting steps, espe- 1.5
ciallyforrecenttransformerarchitecturestrained
1.0
onpopularforecastingbenchmarks. Thatis,op-
timisingperformanceonaveragecanleadtoun- 0 100 200 300 400 500 600 700
desirably large errors at specific time-steps. In PredictionStep
thiswork,wepresentaConstrainedLearningap-
proachforlong-termtimeseriesforecastingthat Figure1.TestMeanSquaredError(MSE)computedatindividual
aimstofindthebestmodelintermsofaverage timestepsontheforecastingwindowforAutoformer(Wuetal.,
performance that respects a user-defined upper 2021)onexchangeratedatausingERMandourapproach.
bound on the loss at each time-step. We call
our approach loss shaping constraints because
Tsay,1994;Hamzaebietal.,2009)andMIMO(Kline,2004;
it imposes constraints on the loss at each time
Bontempi,2008;Bontempi&BenTaieb,2011;Kitaevetal.,
step,andleveragerecentdualityresultstoshow
2019;Zhouetal.,2021;Wuetal.,2021;Liuetal.,2021;
thatdespiteitsnon-convexity,theresultingprob-
Zhouetal.,2022;Nieetal.,2022;Zengetal.,2023;Garza
lem has a bounded duality gap. We propose a
& Mergenthaler-Canseco, 2023; Das et al., 2023b) tech-
practicalPrimal-Dualalgorithmtotackleit,and
niques. Moreover,aplethoraoflearningparametrizations
demonstratethattheproposedapproachexhibits
exist, ranging from linear models (Box & Jenkins, 1976;
competitiveaverageperformanceintimeseries
Zengetal.,2023)torecenttransformerarchitectures(Ki-
forecastingbenchmarks,whileshapingthedistri-
taev et al., 2019; Zhou et al., 2021; Wu et al., 2021; Liu
butionoferrorsacrossthepredictedwindow.
et al., 2021; Zhou et al., 2022; Nie et al., 2022; Garza &
Mergenthaler-Canseco,2023;Dasetal.,2023b).
Regardless of the model and parametrization, most ap-
1.Introduction
proaches optimize a performance, risk or model fit func-
tional(usuallyMSE),averagedoverthepredictedwindow.
Predictingmultiplefuturevaluesoftimeseriesdata,also
Therefore, the distribution of errors across the window –
known as multi-step forecasting (Bontempi et al., 2013),
withoutanyadditionalassumptions–canvarydepending
hasamyriadofapplicationssuchasweather(Wangetal.,
onthemodelanddatageneratingprocess. Inpractice,this
2016),demand(Yietal.,2022),price(Chenetal.,2018)and
canleadtounevenperformanceacrossthedifferentstepsof
movility(Baietal.,2019)forecasting. Severalapproaches
thewindow.
to generating predictions for the next window have been
proposed,includingdirect(Chevillon,2007;Sorjamaaetal., Recentworksusingtransformerbasedarchitecturesfocus
2007),autoregressiveorrecursive(Hilletal.,1996;Tiao& onaggregatemetrics,seeforexample(Kitaevetal.,2019;
Wuetal.,2021;Nieetal.,2022),whileaddressingerrors
*Equalcontribution 1UniversityofPennsylvania. Correspon-
atdifferenttimestepshasrecievedlittleattention(Cheng
denceto:IgnacioHounie<ihounie@seas.upenn.edu>.
etal.,2023). Ourfirstcontributionisprovidinganempirical
analysisinthisdirection:
1
4202
beF
41
]GL.sc[
1v37390.2042:viXra
ESMLossShapingConstraintsforLong-TermForecasting
(C1) We provide an empirical analysis of the distribution predictedwindow,i.e.,atspecifictimesteps. Worksthatpro-
oferrorsacrossthepredictionwindowsusingpopular posereweightingtheerrorsatdifferentstepsofthewindow
models (Kitaev et al., 2019; Zhou et al., 2021; Wu mostlyaimtoimproveaverageperformancebyleveraging
etal.,2021)aswellasavanillatransformerincommon thestructureofresiduals(Chevillon,2007;Guoetal.,1999;
benchmarkdatasets. Hansen,2010)andrelyonpropertiesofthedatagenerat-
ingprocessandpredictivemodelclass. Ontheotherhand,
The analysis in (C1) gives evidence supporting that loss worksthataddresstheempiricaldistributionofforecasting
landscapesvaryconsiderablybetweendatasetsandarchitec- errorsandrobustnessinnon-parametricmodels(Spiliotis
tures. However,havingnocontrolontheerrorincurredat et al., 2019; Taleb, 2009) do not analyse the distribution
eachtimestepinthepredictedwindowcanbedetrimental oferrorsacrossmultiplesteps. Similarly,Multipleoutput
inscenariosthatcanbeaffectedbythisvariability. Forex- SupportVectorRegressionformulti-stepforecasting(Bao
ample,analysingaveragebehaviourisinsufficienttoassess etal.,2014),whichalsotacklesaconstrainedproblem,has
financialrisksineconometrics (Chavleishvili&Manganelli, onlyaddressedaggregateerrorsacrossthewholewindow.
2019)ortoensurestabilitywhenusingthepredictorina
Inthecontextoftransformerarchitecturesfortimeseries
ModelPredictiveControlframework(Terzietal.,2021).
forecasting,tothebestofourknowledge,lossatthestep
Therefore,oursecondcontributionisaddressingthisprob- levelhasonlybeenreportedin(Chengetal.,2023). Since
lemthroughconstrainedlearning. thefocusoftheanalysisisportrayingtheeffectofamulti
task learning approach on loss imbalance, the number of
(C2) We formulate multi-step series forecasting as a con- modelsanddatasetsforwhichstep-wiseerrorsacrossthe
strainedlearningproblem,whichaimstofindthebest windowarereportedislimited. Thus,itdoesnotallowcom-
modelintermsofaverageperformancewhileimpos- parisonsacrossdatasetsandmodels,anddoesnotuncover
ing a user-defined upper bound on the loss at each the erratic (and non monotonic) behaviour that we have
time-step. found empirically for some datasets and models. Recent
worksingenerativetimeseriesmodelshavealsosoughtto
impose constraints using penalty based methods (Coletta
Since this constrains the distribution of the loss per time
etal.,2023),butthenatureoftheconstraintsandproposed
stepacrossthewindow,wecallthisapproachlossshaping
approachalsodiffersfromours.
constraints. Due to the challenges in finding appropriate
constraints,weleveragearecentapproach(Hounieetal.,
2023) to re-interpret the specification as a soft constraint 2.Multi-StepTimeSeriesForecasting
and find an optimal relaxation while jointly solving the
learningtask. Weleveragerecentdualityresults(Chamon&
Letx
t
∈ X ⊆ Rdx denotetheinfeaturevectorandy
t
∈
Ribeiro,2020)toshowthat,despitebeingnon-convex,the
Y ⊆Rdy itsassociatedoutputormeasurementattimestep
t. Thegoalinmulti-steptimeseriesforecastingistopredict
resultingproblemhasaboundeddualitygap. Thisallows
T futurevaluesoftheoutput,i.e.,y :=y givena
ustoprovideapproximationguaranteesatthesteplevel. p p [t+1:t+Tp]
windowoflengthT ofinputfeatures,i.e.,x :=x .
c c [t−Tc:t]
WethenproposeanalternatingPrimal-Dualalgorithmto Thequalityoftheforecastscanbeevaluatedateachtime-
tacklethelossshapingconstrainedproblem. Ourfinalcon- stepusinganon-negativelossfunctionormetricℓ : Y ×
tributionistheexperimentalevaluationofthisalgorithmsin Y →R ,e.g.,thesquarederrororabsolutedifference.
+
apracticalsetting:
Themostcommonapproachinsupervisedmulti-stepfore-
castingistolearnapredictorthatminimizestheexpected
(C3) We evaluate our algorithms for different constraints
lossaveragedoverthepredictedwindow:
usingstate-of-the-arttransformerarchitectures(Kitaev
et al., 2019; Zhou et al., 2021; Wu et al., 2021) in  
popularforecastingbenchmarks. min. θ∈Θ E (xc,yp)∼D T1
(cid:88)Tp
ℓ i(f θ(x c),y p),
p
i=1
Ourempiricalresultsshowcasetheabilitytoaltertheshape
(ERM)
ofthelossbyintroducingconstraints,andhowthiscanlead
tobetterperformanceintermsofstandarddeviationacross where ℓ := ℓ([f (x )] ,[y ] ) denotes theloss evaluated
i θ c i p i
thepredictivewindow. atthei-thtimestep,Daprobabilitydistributionoverdata
pairs1 (x c,y p) and f
θ
:XTc →YTp is a predictor associ-
1.1.Relatedwork ated with parameters θ ∈ Θ ⊂ Rk, for example, a trans-
Thereislittlepriorworkintimeseriespredictiononpromot- 1Our approach is distribution agnostic, i.e. we impose no
ingorimposingacertaindistributionoferrorsacrossthe additionalstructureorassumptionsonD.
2LossShapingConstraintsforLong-TermForecasting
formerbasedneuralnetworkarchitecture. Inthiscase,ϵ couldtakeincreasingvaluesbasedonprior
i
knowledge of the learning problem. We provide further
However,thischoiceofobjectivedoesnotaccountforthe
discussionsaboutsuchconfigurationsinAppendixB.1.
structureordistributionoftheerrorsacrossdifferenttime
steps,whichcanleadtodisparatebehaviouracrossthepre- Nonetheless,whichlosspatternsaredesirableandattainable
dicted window as depicted in Figure 1. In particular, we willdependultimatelyonthemodel,dataandtaskathand.
observeempiricallythatstate-of-the-arttransformerarchi- Inthenextsectionweexplorehowtoautomaticallyadapt
tectures(Kitaevetal.,2019;Zhouetal.,2021;Wuetal., constraints during training, so that the problem is more
2021) can yield highly varying loss dynamics, including robusttotheirmis-specification.
non-monotonic, flat and highly non-linear landscapes, as
presentedinSection5.1. 3.1.AdaptingConstraints: AResilientApproach.
In order to control or promote a desirable loss pattern, a An important challenge is the specification of loss shap-
weighted average across time steps could be employed. ingconstraintsthatarenotoverlyrestrictiveandmakethe
However, since the realised losses will depend not only problemfeasible. Whereasinunconstrainedaveragerisk
on the data distribution but also on the model class and minimization(ERM)anoptimalfunctionθ⋆alwaysexists,
learning algorithm. As a result, such penalization coeffi- in(P-LS)theremaybenoparametersinΘsatisfythere-
cientswouldhavetobetunedinordertoachieveadesired quirements. Inpractice,landingonsatisfiablelossshaping
losspattern,whichincontrastcanbenaturallyexpressedas requirementsmaynecessitatetherelaxationofsomecon-
arequirement,aspresentednext. straints–i.e.,thevaluesofϵ in(P-LS)–. Thisischalleng-
i
ingbecauseassessingtheimpactoftighteningorrelaxinga
3.LossShapingConstraints particularconstraintcanhaveintricatedependencieswith
themodelclass,theunknowndatadistribution,andlearn-
Inordertocontroltheshapeofthelossoverthetimewin- ingalgorithm,whicharehardtodetermineapriori. Thus,
dow, werequirethelossontimestepitobesmallerthan thepointatissueiscomingupwithreasonableconstraints
somequantityϵ i. Thisleadstotheconstrainedstatistical thatachieveadesirabletrade-offbetweencontrollingthe
learningproblem: distributionoftheerroracrossthepredictivewindowand
attaininggoodaverageperformance. Thatis,forlargercon-
  straintlevelstheaverageperformanceimproves,although
1
(cid:88)Tp
P⋆ =m θ∈i Θn E (xc,yp)∼D
T
p
i=1ℓ i(f θ(x c),y p) t fh θe ⋆.constraintswillhavelessimpactontheoptimalfunction
s. to: E [ℓ (f (x ),y )]≤ϵ Inordertodoso,weintroduceanonnegativeperturbation
(xc,yp)∼D i θ c p i
ζ ∈R associatedtoeachtimestep,andconsiderrelaxing
i=1,...,T . (P-LS) t +
p
thei−thconstraintintheoriginalproblembyζ . Explic-
i
itly,weimposeE [ℓ (f (x ),y )]≤ϵ +ζ . We
Anadvantageof(P-LS)isthatitisinterpretableinthesense
(xc,yp)∼D i θ c p i i
alsointroduceadifferentiable,convex,non-decreasingcost
thatconstraints–unlikepenaltycoefficients–makeexplicit h : RTp → R ,thatpenalizesdeviatingfromtheoriginal
therequirementthattheyrepresent. Thatis,theconstraint + +
specification,e.g.,thesquaredL normh(ζ)∝∥ζ∥2.
isexpressedoftermsoftheexpectedlossatindividualtime- 2 2
steps,andthuspriorknowledgeaboutandtheunderlying Thisallowustore-interprettheinitialconstraintlevelsϵas
datadistributionandmodelclasscanbeexploited. asoftconstraintandusetheperturbationcosthtosearchfor
aperturbationζ thatachievesagoodtrade-offwithaverage
InSection5,wefocusonsimplyimposingaconstantupper
performance. Therefore, we propose to solve a Resilient
boundforalltimesteps,whichwesetusingtheperformance
ConstrainedLearningproblem(Hounieetal.,2023),which
ofthe(unconstrained)ERMsolution. Byupperbounding
isdefinedasfollows:
the error, we prevent errors from being undesirably large
irrespectively of their position in the window, effectively
limitingthespreadoftheerrorsacrossthewholewindow. 
1
(cid:88)Tp 
S thi in sc ae pw pe ros at cil hlm isin ni om ti az se ct oh ne sm ere va an tie vr ero ar sa mcr io ns imst ah xe fw oi rn md uo lw a-, m θ∈i Θn E (xc,yp)∼D
T
p
ℓ i(f θ(x c),y p)+h(ζ)
i=1
tions (Liu & Taniguchi, 2021), which only focus on the
s. to: E [ℓ (f (x ),y )]≤ϵ +ζ
worsterror. (xc,yp)∼D i θ c p i
i=1,...,T . (R-LS)
p
Itisworthpointingoutthatmanyotherconstraintchoices
arepossible. Forinstance,itisoftenreasonabletoassume
errorsincreasemonotonicallyalongthepredictionwindow. The key property of the optimal relaxation ζ⋆ is that it
3LossShapingConstraintsforLong-TermForecasting
equates the marginal cost of the relaxation (as measured intermsofapproximationandleadtosub-optimaltestper-
by h) with the marginal cost of its effect on the optimal formance. Weincludeasummaryoftheseresultsaswell
cost(Hounieetal.,2023). Therationalebehindintroduc- asadiscussionontheirimplicationsinthissettinginAp-
ing a learnt relaxation of constraint levels is making the pendixA,andreferto(Chamon&Ribeiro,2020;Hounie
problemeasiertosolve. Inthenextsection,weintroduce etal.,2023)forfurtherdetails.
practicalprimal-dualalgorithmsthatenableapproximating
Theadvantageoftacklingthetheempiricaldualproblem
theconstrained(P-LS)andresilient(R-LS)problemsbased
Dˆ⋆ is that it can be solved using saddle point methods
onsamplesandfinite(possiblynon-convex)parametriza-
presentedinthenextsection.
tionssuchastransformerarchitectures.
4.1.Algorithm
4.EmpiricalDualResilientandConstrained
Inordertosolveproblem(ED-LS),weresorttodualascent
Learning
methods,whichcanbeshowntoconvergeeveniftheinner
Achallengeinsolving(P-LS)and(R-LS)isthatingeneral, minimizationproblemisnon-convex(Chamonetal.,2022).
(i)noclosedformprojectionintothefeasiblesetorprox- Thesaddlepointproblem(ED-LS)canthenbeundertaken
imaloperatorexists,and(ii)itinvolvesanunknowndata byalternatingtheminimizationwithrespecttoθandζwith
distributionD. Inwhatfollows,wedescribeourapproach themaximizationwithrespecttoλ(K.J.Arrow&Uzawa,
fortacklingtheResilientproblem,notingthatPrimal-Dual 1960),whichleadstothePrimal-Dualconstrainedlearning
algorithmstacklingtheoriginalconstrainedproblemcanbe procedureinAlgorithm1.
similarlyderivedbysimplyexcludingtheslackvariables.
Althoughaboundedempiricaldualitygapdoesnot guar-
ToundertakeR-LS,(i)wereplaceexpectationsbysample antee that the primal variables obtained after running Al-
means over a data set {(xn,yn) : n = 1,··· ,N}, as gorithm 1 are near optimal or approximately feasible in
c p
typicallydonein(unconstrained)statisticallearning, and general, recent constrained learning literature provides
(ii)resorttoitsLagrangiandual. sub-optimality and near-feasibility bounds for primal it-
erates(Elenteretal.,2024)aswellasabundantempirical
ThesemodificationsleadtotheEmpiricalDualproblem
evidence(Robeyetal.,2021;Gallego-Posadaetal.,2022;
Dˆ⋆ =max min Lˆ(θ,λ,ϵ,ζ), (ED-LS) Elenteretal.,2022)thatgoodsolutionscanstillbeobtained.
λ≥0 θ∈Θ,ζ∈RTp
+
Algorithm1PrimalDualLossShaping.
whereLˆ istheempiricalLagrangianof(R-LS),definedas Input: Dataset{x ,y } ,primallearningrateη ,
i i i=1,···,N p
duallearningrateη ,perturbationlearningrateη ,num-
Lˆ(θ,λ,ϵ,ζ):= d ϵ
berofepochsT ,numberofbatchesT ,initialconstraint
e b
1 (cid:88)N (cid:88)Tp (cid:18)
λ +
1 (cid:19) (cid:2)
ℓ ([f
(xn)],yn)(cid:3)
−λ (ϵ +ζ ),
t Ii ng ih tit an le izs es :ϵ θα0 ,.
λ ,...,λ ←0
N i T i θ c p i i i 1 Tpred
n=1i=1 p forepoch =1,...,T e do
forbatch =1,...,T do
andλ isthedualvariableassociatedtotheconstrainton b
i Updateprimalvariables
thei-thtimesteplossℓ i. θ ←θ−η ∇ Lˆ(θ,λ,ϵ,ζ)
p θ
TheEmpiricalDualproblem(ED-LS)isanapproximation Evaluateconstraints.
(cid:16) (cid:17)
oftheDualproblemassociatedtoR-LSbasedontraining s ← 1 (cid:80)Nb ℓ ([f (xn )],yn ) −(ϵ +ζ )
i Nb n=1 i θ c(t) p(t) i i
samples. The dual problem itself can be interpreted as Updateslacks.
findingthetightestlowerboundontheprimal. Although (cid:104) (cid:16) (cid:0) (cid:1) (cid:17)(cid:105)
ζ ← ζ−η ∇h ζ −λ
ζ
theestimationofexpectationsusingsamplemeansandnon- +
Updatedualvariables.
convexity of the hypothesis class can introduce a duality
gap,i.e.,Dˆ⋆ <P⋆,undercertainconditionsthisgapcanbe λ←[λ+η ds] +
endfor
bounded.
endfor
Unlikeunconstrainedstatisticallearningbounds,theseap- Return: θ,λ.
proximation bounds depend not only in the sample com-
plexityofthemodelclassandlossbutalsoontheoptimal
dualvariablesorslacksassociatedtotheconstraintproblem. 5.Experiments
Thesereflecthowchallengingitistomeettheconstraints.
Thisaspectiscrucial,asapplyingoverlystringentlossshap- Inordertoexplorethepropertiesoflossshaping,weturnto
ingconstraintsinagivenlearningsettingmaybedetrimental theopensourcebenchmarkdatasetsElectricityConsuming
4LossShapingConstraintsforLong-TermForecasting
Load (ECL)2, Weather 3 and Exchange (Lai et al., 2018) WindowLength=96 WindowLength=192
0.9
to train three popular transformer architectures for time 0.5 0.8
series prediction, namely, Reformer (Kitaev et al., 2019) 0.4 0.7
0.6
and Autoformer (Wu et al., 2021), Informer (Zhou et al., 0.3 0.5
2021)andavanillatransformer. Wecontrasttheirbehav- 0.4
0.2
ior in constrained and (standard) unconstrained learning 0.3
0.1 0.2
settings. We follow the same setup and hyperparameters 0 20 40 60 80 0 50 100 150 200
used in (Kitaev et al., 2019; Wu et al., 2021; Zhou et al., WindowLength=336 WindowLength=720
1.00
2021). Dataissplitintotrain,validation,andtestchronolog- 1.0
0.95
icallywitharatioof7:1:2. Foreachseteverypossiblepair
0.8
of consecutive context and prediction windows of length 0.90
T andT , respectively, areused. Thatis, weuserolling 0.6 0.85
c p
(overlapping)windowsforbothtrainingandtesting.4 We 0.4 0.80
useADAMoptimizer(Kingma&Ba,2014)togetherwith 0 100 200 300 0 200 400 600
PredictionStep PredictionStep
theoriginalimplementationandhyperparametersofeach
method. Additionalexperimentaldetailscanbefoundon Figure2.Step-wise Mean Test Errors for Autoformer on the
AppendixC. Weatherdataset.Eachplotrepresentsadifferentpredictionlength
window.Theshapeoftheerroracrossthewindowissimilaracross
predictionlengths.
5.1.TheShapeofERM
Inthissection,weprovideanempiricalanalysisofthedistri-
butionoferrorsacrosstimestepswhentrainingmodelswith meanstepwiseerrorscomputedonthetrainingandtestsets.
the conventional ERM framework. This aims to uncover AsshowninTable1,inseveralsetupswefindstrongcor-
phenomenathatcannotbeinferredfromthecommonlyre- relations,signalingamonotonicrelationshipbetweentrain
portedmetrics–whichareaveragedacrossthewindow– andtesterrorsHowever,wealsoobserveanumberofweak
andtohighlightthatgeneralizationmayhindertheefficacy andevennegativecorrelations. Inparticular,Informerand
oflossshapingapproaches. thevanillatransformer,presentsignificantlyweakercorre-
lations. Inthesesettings,imposingadesiredlossshapeby
ModelsForthesamedatasetandpredictionlength,differ-
modifyingthedistributionoferrorsonthetrainingsetcan
ent models can have significantly different error patterns,
bechallenging,becauseitneednotaffectperformanceon
evenwhenhavingsimilarMSEs. Thiscanbeobservedin
thetestsetinapredictablemanner.
Figure 3, which portray the test errors of different mod-
elstrainedonthesamedatasets. Thiscanalsobeseenin
Table 4, which shows that the standard deviation of step- Autoformer Informer Reformer Transformer
wiseMSEsconsiderably,whiletheiraverages(Table3)are 96 0.60 0.28 0.91 -0.05
similar. 192 0.93 0.14 0.67 0.05
336 -0.09 0.31 0.38 0.10
PredictionLengthWeobservethattheshapeofthelossis 720 0.75 0.05 0.30 0.09
similaracrossdifferentpredictionlengthsinsomesettings, 96 0.94 0.34 0.91 -0.04
suchastheonedepictedinFigure2. InAppendixC.1we 192 0.64 -0.00 -0.36 -0.81
providefurtherquantitativeevidenceofthisphenomenon 336 0.90 0.07 -0.58 -0.31
720 0.99 0.40 -0.28 -0.42
bycomputingpairwisecorrelationsbetween(scaled)errors
96 -0.28 0.80 0.97 -0.33
fromdifferentpredictionlengths.
192 0.98 0.43 0.95 0.08
GeneralizationInordertoassesstheimpactofthegeneral- 336 0.95 0.26 0.87 0.26
izationgaponthetesterrorshape,weanalysewhetherat 720 0.82 0.21 0.68 0.10
anygivenstepsmallermeantrainingerrorsareassociated
tosmallertesterrors. Wecomputeforeachmodel,dataset Table1.SpearmanCorrelationsbetweentrainandtesterrorsfor
and prediction length, the Spearman correlation between modelstrainedusingERM.Highermeanstherelationshipiscloser
tobeingmonotonic.
2ECL dataset was acquired at https://archive.ics.uci.edu/ml/
datasets/ElectricityLoadDiagrams20112014
3Weatherdatasetwasacquiredathttps://www.ncei.noaa.gov/
5.2.ConstraintLevelChoice
data/local-climatological-data/
4We also fix a known bug (reported in (Das et al., 2023a)) Themostsalientdesignquestionisthechoiceofthecon-
from (Wu et al., 2021) and followup works, where the last T p straintlevelϵ . Inourexperimentalsetting,webeginwith
i
sampleswherediscarded.
thesimplestchoiceofaconstantϵ =ϵforalltimesteps.
i
5
LCE
egnahcxE
rehtaeW
ESM
ESMLossShapingConstraintsforLong-TermForecasting
Autoformer Informer Reformer Transformer
0.6
0.4
ERM
0.2
Constrained
0 25 50 75 0 25 50 75 0 25 50 75 0 25 50 75
Autoformer Informer Reformer Transformer
4 Prediction Step
2
0
0 200 400 600 0 200 400 600 0 200 400 600 0 200 400 600
Prediction Step
Figure3.TestMSEcomputedateachpredictionstepfordifferentdatasets,modelsandpredictionwindowlengths.Thetoprowshows
resultsfortheweatherdatasetwithapredictivewindowlengthof96steps,andthesecondrowcorrespondstotheexchangedatasetwitha
predictivewindowlengthof720steps.Eachplotcorrespondstoadifferentarchitecture,andeachcurverepresentsadifferenttraining
algorithm,weincludeboththeERMbaselineandourmethodusingaconstantconstraintacrossthewindowforallmodels.
Thevalueofϵshoulddependonthestatisticalproblemat fails to do so in others, as depicted in Figure 3 and Ap-
hand,thatis,themodelclass,lossanddatadistribution. It pendixC.1. Nonetheless,wecorroborateempiricallythatit
willalsodependonthesamplesizesince,evenifthemodel consistentlyreducesconstraintviolation(showninTable2)
hasenoughcapacitytofitanextremelytightconstraint,it ontestdataacrossdifferenttransformer-basedarchitectures,
canoverfittotrainingdatawhilebeinginfeasibleattesttime. predictionlengthsanddatasets.
Ontheotherhand,iftheconstraintlevelistoolooseitwill
Furthermore,thisdoesnotcomeatacostintermsoftest
notsignificantlymodifytheshapeofthelossacrossthewin-
MSE(Table3),whichissimilarbetweenERMandourap-
dow. Lackingpriorinformationordomainknowledge,we
proachinmostscenarios. Inseveralsettings,ourapproach
proposeusingthetrainandvalidationloss(seeAppendixC)
evenleadstoareductioninMSE.Forexample,thiseffectis
ofamodeltrainedusingERMinordertogatherinsights
illustratedinFigure3,wheretheAutoformermodeltrained
aboutsensiblevaluesofϵ.
with constraints attains a smoother error landscape, with
Duringpreliminaryexperiments,weobservedthattheme- moststep-wiseerrorsbeinglowerthantheirERMcounter-
dianoftheERMvalidationlosspertimestepachievedgood part.
results. Wealsoconductedanablationonconstraintlevels
Therationalebehindimposinganupperboundateachstep
usingtheupperandlowerquantilesoftrainandvalidation
isreducingthespreadoferrorsacrossthewindows. InTa-
losses (see Appendix C.1). We report the results on the
ble4weshowthatthestandarddeviation(computedacross
values of ϵ with the lower MSE for each combination of
thepredictedwindow)isreducedforthevastmajorityof
model,datasetandpredictionlength.
settings–withtheexceptionofthevanillatransformer,that
Inordertoquantifytheeffectoftheimposedconstraints,we asalreadydiscussedshowspoorgeneralization–. Inaddi-
reportMeanConstraintViolation(MCV),whichwedefine tion, inAppendixC.1 weprovideexamplesinwhichthe
asthefractionoftime-stepsthatareinfeasible,explicitly, losslandscapeinunseendatachangesevenwhenimposing
looserconstraintsthatareeasilysatisfiedoverthetraining
1 T (cid:88)pred set–i.e.,forwhichtheERMsolutionisalsofeasible–. This
MCV(yˆ,y,ϵ)=
T
max{0,ℓ(yˆ t,y t)−ϵ}. suggeststhatirrespectiveofthelevelofconstraintimposed,
pred t=1 usingconstraintscancontributetoenhancingmodelperfor-
mance.
Aneffectiveconstrainedmodelshouldbeabletomarginally
Qualitatively, we can distinguish different scenarios in
trade-offMSEforasignificantreductioninMCV.
whichourapproachleadstoimprovements. Onsomeset-
tings,itcanmaketrainingerrorsfeasiblepossiblyatthecost
5.3.ConstrainedLossShapingResults
ofanincreaseofaveragetrainingerror(e.g.,thefirstrowof
Ourapproacheffectivelyaffectsthedistributionoflosses Figure4). Conversely,inotherscenarios,itfailstoobtaina
acrossthewindowasintendedinmanycases,althoughit feasiblesolutionandleadstoanincreasetrainingerror,but
6
ESM
ESMLossShapingConstraintsforLong-TermForecasting
stillimprovestestperformanceduetobettergeneralization, 0.50
likeinthesecondrowofFigure4. 0.7
0.45
Amongthesettingsinwhichweareunabletochangethe 0.6 0.40
testlossasdesired,wealsodistinguishtwocommonfailure 0.35
0.5
modes. Thefirstiswhenourapproachfailstoimposethe 0.30
0.4
constraintsontrainingdata,asinthefirstrowofFigure5, 0.25
oftenassociatedwithmorechallengingsettings. Another 0 100 200 300 0 100 200 300
one is the lack of generalization, depicted in the second
0.24 0.45
rowofFigure5,whichcorrespondstosettingsinwhichour
0.23
0.40
modeleffectivelychangesthelosspatterninthetrainingset,
0.22
butitdoesnothavethesameeffectontestdata. 0.35
0.21
0.30
0.20
0.25
1.2 3.5 0.19
1.0 3.0 0 200 400 600 0.20 0 200 400 600
2.5
0.8 PredictionStep
2.0
0.6
1.5
0.4 1.0 Figure5.Twoinstancesoffailurecases.Thefirstrowisthetrain-
0.2 0.5 ingandtestingerrorsofAutoformeronWeatherdatawithwindow
lengthof336.ThesecondrowisInformeronECLdatawithwin-
0 200 400 600 0 200 400 600
dowlengthof720.Theredlinesarethevaluesofϵusedduring
0.5
training.
2.0
0.4
1.5
0.3 5.4.Resilience
1.0
0.2 0.5 Relaxingconstraintsshouldallowforabetteraverageper-
formance(objective),inthesamewaythatimposingcon-
0 100 200 300 0 100 200 300
straints should only cause a decrease performance when
PredictionStep
compared to the unconstrained problem. In the previous
Figure4.Examplesofsuccesscases.Thefirstrowcorrespondsto section,wehaveshownthat–empirically–thelatterdoes
thetrainingandtestinglossesofERMandConstrainedAutoformer notalwayshold. Inthesamemanner,theResilientapproach
onExchangeRatedata,withwindowlengthof720.Thesecond didnotconsistentlyattainlowertestMSEs(Table3)than
rowisReformeronExchangeRatewithwindowlengthof336. theconstrainedapproach,eventhoughconstraintswereef-
Theredlinesarethevaluesofϵusedduringtraining. fectively relaxed – by a considerable amount –. Settings
Autoformer Informer Reformer Transformer
Ours Ours+R ERM Ours Ours+R ERM Ours Ours+R ERM Ours Ours+R ERM
96 0.087 0.109 0.091 0.170 0.193 0.176 0.147 0.152 0.149 0.149 0.169 0.152
192 0.026 0.056 0.017 0.173 0.192 0.184 0.166 0.172 0.172 0.063 0.110 0.070
336 0.042 0.080 0.127 0.159 0.174 0.165 0.198 0.162 0.192 0.157 0.168 0.150
720 0.093 0.227 0.075 0.165 0.226 0.173 0.139 0.148 0.139 0.155 0.173 0.151
96 0.022 0.006 0.015 0.834 0.872 0.842 0.917 0.873 0.852 0.008 0.003 0.084
192 0.000 0.000 0.791 1.068 1.089 1.071 1.213 1.496 1.209 1.218 1.139 1.200
336 0.000 0.000 2.411 0.000 0.000 0.000 1.423 1.715 1.700 1.323 1.610 1.382
720 0.778 0.382 1.160 0.000 0.000 0.000 1.460 1.506 1.492 0.000 0.000 0.156
96 0.000 0.000 0.000 0.186 0.230 0.225 0.000 0.019 0.000 0.047 0.030 0.040
192 0.001 0.001 0.002 0.019 0.040 0.045 0.000 0.018 0.129 0.411 0.458 0.485
336 0.018 0.002 0.013 0.411 0.355 0.355 0.010 0.040 0.289 0.209 0.218 0.086
720 0.000 0.000 0.000 0.802 0.949 0.851 0.478 0.631 0.680 0.708 0.856 0.786
Table2.TestMeanConstraintViolationusingconstantconstraintsandvanillaERMmodelsacrossdifferentwindowlengths.Imposing
constraintsconsistentlyreducestheMCVmetricacrossdatasetsandwindowlengths.
7
ESM
LCE
egnahcxE
rehtaeW
ESMLossShapingConstraintsforLong-TermForecasting
Autoformer Informer Reformer Transformer
Ours Ours+R ERM Ours Ours+R ERM Ours Ours+R ERM Ours Ours+R ERM
96 0.200 0.220 0.204 0.319 0.342 0.325 0.297 0.303 0.299 0.259 0.278 0.262
192 0.226 0.258 0.215 0.341 0.360 0.352 0.326 0.332 0.332 0.274 0.321 0.281
336 0.249 0.287 0.334 0.348 0.371 0.354 0.365 0.329 0.359 0.287 0.297 0.280
720 0.270 0.410 0.252 0.377 0.438 0.385 0.316 0.326 0.316 0.293 0.312 0.289
96 0.179 0.163 0.155 0.872 0.910 0.880 1.025 0.981 0.960 0.593 0.556 0.691
192 0.266 0.267 2.052 1.110 1.131 1.113 1.402 1.685 1.398 1.253 1.174 1.235
336 0.446 0.442 3.442 1.088 1.080 1.633 1.632 1.924 1.909 1.362 1.649 1.421
720 1.510 1.205 1.852 1.170 1.166 2.961 2.063 2.114 2.095 1.029 1.015 2.197
96 0.300 0.314 0.375 0.395 0.441 0.435 0.361 0.380 0.422 0.462 0.435 0.448
192 0.311 0.322 0.303 0.447 0.491 0.515 0.425 0.483 0.703 0.593 0.640 0.667
336 0.374 0.402 0.359 0.656 0.600 0.600 0.589 0.655 1.010 0.901 0.915 0.749
720 0.442 0.447 0.443 1.051 1.198 1.100 0.761 0.914 0.963 0.924 1.072 1.002
Table3.TestMSEforconstrainedandERMmodelsacrossdifferentwindowlengths.Weobservethatthetradeoffbetweenconstraint
satisfactionandMSEislow,evenimprovingMSEinmanyinstances.
Autoformer Informer Reformer Transformer
Ours Ours+R ERM Ours Ours+R ERM Ours Ours+R ERM Ours Ours+R ERM
96 0.015 0.013 0.015 0.013 0.018 0.014 0.011 0.008 0.011 0.007 0.008 0.007
192 0.022 0.020 0.022 0.037 0.029 0.041 0.019 0.012 0.020 0.018 0.014 0.018
336 0.019 0.063 0.043 0.018 0.019 0.022 0.023 0.011 0.022 0.022 0.018 0.020
720 0.030 0.156 0.035 0.020 0.040 0.028 0.007 0.007 0.007 0.018 0.018 0.013
96 0.084 0.061 0.080 0.274 0.305 0.278 0.121 0.098 0.115 0.162 0.128 0.167
192 0.447 0.389 1.347 0.299 0.331 0.308 0.148 0.196 0.132 0.375 0.374 0.376
336 9.441 0.843 2.132 0.392 0.376 0.387 0.172 0.269 0.193 0.211 0.304 0.218
720 0.226 0.422 1.038 0.915 1.093 0.946 0.280 0.344 0.314 0.624 0.735 0.610
96 0.029 0.026 0.060 0.141 0.096 0.118 0.107 0.090 0.101 0.121 0.124 0.126
192 0.069 0.040 0.052 0.156 0.127 0.158 0.144 0.132 0.142 0.206 0.242 0.242
336 0.037 0.067 0.068 0.124 0.171 0.136 0.035 0.191 0.141 0.252 0.262 0.224
720 0.081 0.065 0.079 0.251 0.528 0.364 0.038 0.067 0.039 0.273 0.324 0.289
Table4.StandarddeviationofstepwiseerrorsforrunswithϵsettotheMedianoftrainErrorforERM.“Ours”denotestheconstrained
approachand“Ours(R)”addsaresilientrelaxationwithcosth(ζ)=2∥ζ∥2.Imposingconstraintstendstoreducethevarianceacrossthe
2
predictivewindow.Addingresilienceoftenachievesevenlessvariability,especiallyincaseswheretheSTDofconstrainedmethodsis
high.
inwhichrelaxingconstraintsisdetrimentaltoperformance acrossthewindowconsiderablyinmanycases. Forexam-
thussupporttheclaimthatlossshapingconstraintscanalso ple, this can be observed for Autoformer (first columns)
bebeneficialintermsofaverageMSEinsomecases. Ex- acrossseveralsettings.
amplesofthisbehaviorcanbeobservedinFigure3forthe
caseofAutoformerandInformer.
6.Conclusion
Intermsofconstraintviolation,whichwemeasurewithre-
Thispaperintroducedatimeseriesforecastingconstrained
specttothefixedconstraintlevels(ϵ),theresilientapproach
learningframeworkthataimstofindthebestmodelinterms
shouldbeoutperformedbyitsconstrainedcounterpart.How-
ofaverageperformancewhileimposingauserdefinedupper
ever,wefindthatbothapproachesperformsimilarly.
boundonthelossateachtime-step. Giventhatweobserved
Finally, we observe that the resilient approach can be ad- thatthedistributionoflosslandscapesvariesconsiderably,
vantageousinthatitreducesstandarddeviation(Table4) weexploredaResilientConstrainedLearning(Hounieetal.,
8
LCE
egnahcxE
rehtaeW
LCE
egnahcxE
rehtaeWLossShapingConstraintsforLong-TermForecasting
2023)approachtodynamicallyadaptindividualconstraints References
duringtraining. Weanalysedthepropertiesofthisproblem
Bai, L., Yao, L., Kanhere, S., Wang, X., Sheng, Q., et al.
byleveragingrecentdualityresults,anddevelopedpractical
Stg2seq: Spatial-temporalgraphtosequencemodelfor
algorithms to tackle it. We corroborated empirically that
multi-steppassengerdemandforecasting. arXivpreprint
ourapproachcaneffectivelyalterthedistributionoftheloss
arXiv:1905.10069,2019.
acrosstheforecastwindow.
Bao,Y.,Xiong,T.,andHu,Z. Multi-step-aheadtimeseries
Althoughwehavefocusedontransformersforlongterm
prediction using multiple-output support vector regres-
timeseriesforecasting,motivatedbytheempiricalfinding
sion. Neurocomputing,129:482–493,2014.
that their loss varies considerably, the loss shaping con-
strainedlearningframeworkcanbeextendedtootherset-
Bontempi,G. Longtermtimeseriespredictionwithmulti-
tings. Thisincludesaplethoraofmodels,datasetsandtasks
inputmulti-outputlocallearning. 2008. URLhttps:
wherelossesaredistributedacrossdifferenttimestepsor
//api.semanticscholar.org/CorpusID:
grids.
137250.
Bontempi, G. and Ben Taieb, S. Conditionally depen-
dentstrategiesformultiple-step-aheadpredictioninlocal
learning. InternationalJournalofForecasting,27:689–
699,072011. doi: 10.1016/j.ijforecast.2010.09.004.
Bontempi,G.,BenTaieb,S.,andLeBorgne,Y.-A.Machine
learningstrategiesfortimeseriesforecasting. Business
Intelligence: Second European Summer School, eBISS
2012,Brussels,Belgium,July15-21,2012,TutorialLec-
tures2,pp.62–77,2013.
Box,G.E.andJenkins,G.M. Timeseriesanalysis.fore-
castingandcontrol. Holden-DaySeriesinTimeSeries
Analysis,1976.
Boyd,S.,Boyd,S.P.,andVandenberghe,L. Convexopti-
mization. Cambridgeuniversitypress,2004.
Chamon,L.andRibeiro,A.Probablyapproximatelycorrect
constrained learning. Advances in Neural Information
ProcessingSystems,33:16722–16735,2020.
Chamon, L. F., Paternain, S., Calvo-Fullana, M., and
Ribeiro,A. Constrainedlearningwithnon-convexlosses.
IEEETransactionsonInformationTheory,69(3):1739–
1760,2022.
Chavleishvili,S.andManganelli,S. Forecastingandstress
testingwithquantilevectorautoregression. Availableat
SSRN3489065,2019.
Chen, Y., Zhang, C., He, K., and Zheng, A. Multi-step-
ahead crude oil price forecasting using a hybrid grey
wave model. Physica A: Statistical Mechanics and its
Applications,501:98–110,2018.
Cheng, J., Huang, K., and Zheng, Z. Fitting imbalanced
uncertaintiesinmulti-outputtimeseriesforecasting.ACM
TransactionsonKnowledgeDiscoveryfromData,17(7):
1–23,2023.
Chevillon,G. Directmulti-stepestimationandforecasting.
JournalofEconomicSurveys,21(4):746–785,2007.
9LossShapingConstraintsforLong-TermForecasting
Coletta,A.,Gopalakrishan,S.,Borrajo,D.,andVyetrenko, Hill, T., O’Connor, M., and Remus, W. Neural network
S. On the constrained time-series generation problem. modelsfortimeseriesforecasts. Managementscience,
arXivpreprintarXiv:2307.01717,2023. 42(7):1082–1092,1996.
Das,A.,Kong,W.,Leach,A.,Sen,R.,andYu,R. Long- Hornik, K., Stinchcombe, M., and White, H. Multilayer
term forecasting with tide: Time-series dense encoder. feedforwardnetworksareuniversalapproximators. Neu-
arXivpreprintarXiv:2304.08424,2023a. ralnetworks,2(5):359–366,1989.
Das,A.,Kong,W.,Sen,R.,andZhou,Y. Adecoder-only Hounie,I.,Ribeiro,A.,andChamon,L.F.O. Resilientcon-
foundationmodelfortime-seriesforecasting,2023b. strainedlearning. InThirty-seventhConferenceonNeu-
ralInformationProcessingSystems,2023. URLhttps:
Elenter, J., Naderializadeh, N., and Ribeiro, A. A //openreview.net/forum?id=h0RVoZuUl6.
lagrangian duality approach to active learning.
In Koyejo, S., Mohamed, S., Agarwal, A., Bel- K. J. Arrow, L. H. and Uzawa, H. Studies in linear and
grave, D., Cho, K., and Oh, A. (eds.), Advances non-linearprogramming,byk.j.arrow,l.hurwiczandh.
in Neural Information Processing Systems, vol- uzawa.stanforduniversitypress,1958.229pages. Cana-
ume 35, pp. 37575–37589. Curran Associates, Inc., dian Mathematical Bulletin, 3(3):196–198, 1960. doi:
2022. URL https://proceedings.neurips. 10.1017/S0008439500025522.
cc/paper_files/paper/2022/file/
Kingma,D.P.andBa,J. Adam: Amethodforstochastic
f475bdd151d8b5fa01215aeda925e75c-Paper-Conference.
optimization. arXivpreprintarXiv:1412.6980,2014.
pdf.
Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The
Elenter,J.,Chamon,L.,andRibeiro,A. Near-optimalso-
efficient transformer. In International Conference on
lutionsofconstrainedlearningproblems. InTheTwelfth
LearningRepresentations,2019.
InternationalConferenceonLearningRepresentations,
2024. URLhttps://openreview.net/forum?
Kline,D.M. Methodsformulti-steptimeseriesforecasting
id=fDaLmkdSKU.
neuralnetworks. InNeuralnetworksinbusinessforecast-
ing,pp.226–250.IGIGlobal,2004.
Gallego-Posada, J., Ramirez, J., Erraqabi, A., Bengio,
Y., and Lacoste-Julien, S. Controlled sparsity via
Lai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling
constrainedoptimizationor: Howilearnedtostoptuning
long-andshort-termtemporalpatternswithdeepneural
penaltiesandloveconstraints. InKoyejo,S.,Mohamed,
networks. InThe41stinternationalACMSIGIRconfer-
S.,Agarwal,A.,Belgrave,D.,Cho,K.,andOh,A.(eds.),
enceonresearch&developmentininformationretrieval,
Advances in Neural Information Processing Systems,
pp.95–104,2018.
volume 35, pp. 1253–1266. Curran Associates, Inc.,
2022. URL https://proceedings.neurips. Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and
cc/paper_files/paper/2022/file/ Dustdar,S.Pyraformer:Low-complexitypyramidalatten-
089b592cccfafdca8e0178e85b609f19-Paper-Contfioenrfeonrcloen.g-rangetimeseriesmodelingandforecasting.
pdf. InInternationalconferenceonlearningrepresentations,
2021.
Garza,A.andMergenthaler-Canseco,M. Timegpt-1. arXiv
preprintarXiv:2310.03589,2023. Liu, Y. and Taniguchi, M. Minimax estimation for time
series models. METRON, 79(3):353–359, 2021. doi:
Guo,M.,Bai,Z.,andAn,H.Z. Multi-steppredictionfor
10.1007/s40300-021-00217-6. URL https://doi.
nonlinearautoregressivemodelsbasedonempiricaldis-
org/10.1007/s40300-021-00217-6.
tributions. StatisticaSinica,9(2):559–570,1999. ISSN
10170405, 19968507. URL http://www.jstor. Mohri,M.,Rostamizadeh,A.,andTalwalkar,A. Founda-
org/stable/24306598. tionsofmachinelearning. MITpress,2018.
Hamzaebi,C.,Akay,D.,andKutay,F.Comparisonofdirect Nie,Y.,Nguyen,N.H.,Sinthong,P.,andKalagnanam,J. A
anditerativeartificialneuralnetworkforecastapproaches timeseriesisworth64words:Long-termforecastingwith
inmulti-periodictimeseriesforecasting. Expertsystems transformers. arXivpreprintarXiv:2211.14730,2022.
withapplications,36(2):3839–3844,2009.
Robey, A., Chamon, L., Pappas, G. J., Hassani, H., and
Hansen,B.E. Multi-stepforecastmodelselection. In20th Ribeiro, A. Adversarial robustness with semi-infinite
Annual Meetings of the Midwest Econometrics Group, constrained learning. Advances in Neural Information
2010. ProcessingSystems,34:6198–6215,2021.
10LossShapingConstraintsforLong-TermForecasting
Sorjamaa, A., Hao, J., Reyhani, N., Ji, Y., and Lendasse, Zhou,H.,Zhang,S.,Peng,J.,Zhang,S.,Li,J.,Xiong,H.,
A. Methodologyforlong-termpredictionoftimeseries. andZhang,W.Informer:Beyondefficienttransformerfor
Neurocomputing,70(16-18):2861–2869,2007. longsequencetime-seriesforecasting. InProceedingsof
theAAAIconferenceonartificialintelligence,volume35,
Spiliotis, E., Nikolopoulos, K., and Assimakopoulos, V.
pp.11106–11115,2021.
Talesfromtails: Ontheempiricaldistributionsoffore-
castingerrorsandtheirimplicationtorisk. International Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin,
JournalofForecasting,35(2):687–698,2019.ISSN0169- R. FEDformer: Frequencyenhanceddecomposedtrans-
2070. doi: https://doi.org/10.1016/j.ijforecast.2018.10. formerforlong-termseriesforecasting. InChaudhuri,K.,
004. URLhttps://www.sciencedirect.com/ Jegelka,S.,Song,L.,Szepesvari,C.,Niu,G.,andSabato,
science/article/pii/S0169207018301547. S.(eds.),Proceedingsofthe39thInternationalConfer-
enceonMachineLearning,volume162ofProceedings
Taleb, N. N. Errors, robustness, and the fourth ofMachineLearningResearch,pp.27268–27286.PMLR,
quadrant. International Journal of Forecasting, 17–23Jul2022.URLhttps://proceedings.mlr.
25(4):744–759, 2009. ISSN 0169-2070. doi: press/v162/zhou22g.html.
https://doi.org/10.1016/j.ijforecast.2009.05.027.
URL https://www.sciencedirect.com/
science/article/pii/S016920700900096X.
Special section: Decision making and planning under
lowlevelsofpredictability.
Terzi,E.,Bonassi,F.,Farina,M.,andScattolini,R.Learning
modelpredictivecontrolwithlongshort-termmemory
networks. InternationalJournalofRobustandNonlinear
Control,31(18):8877–8896,2021. doi: https://doi.org/
10.1002/rnc.5519. URLhttps://onlinelibrary.
wiley.com/doi/abs/10.1002/rnc.5519.
Tiao,G.C.andTsay,R.S.Someadvancesinnon-linearand
adaptivemodellingintime-series. Journalofforecasting,
13(2):109–131,1994.
Wang,J.,Song,Y.,Liu,F.,andHou,R. Analysisandappli-
cationofforecastingmodelsinwindpowerintegration:A
reviewofmulti-step-aheadwindspeedforecastingmod-
els. Renewable and Sustainable Energy Reviews, 60:
960–981,2016.
Wu,H.,Xu,J.,Wang,J.,andLong,M.Autoformer:Decom-
positiontransformerswithauto-correlationforlong-term
seriesforecasting. AdvancesinNeuralInformationPro-
cessingSystems,34:22419–22430,2021.
Yi,Z.,Liu,X.C.,Wei,R.,Chen,X.,andDai,J. Electric
vehiclechargingdemandforecastingusingdeeplearning
model. JournalofIntelligentTransportationSystems,26
(6):690–703,2022.
Yun, C., Bhojanapalli, S., Rawat, A.S., Reddi, S.J., and
Kumar, S. Are transformers universal approximators
of sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077,2019.
Zeng,A.,Chen,M.,Zhang,L.,andXu,Q.Aretransformers
effectivefortimeseriesforecasting? InProceedingsof
theAAAIconferenceonartificialintelligence,volume37,
pp.11121–11128,2023.
11LossShapingConstraintsforLong-TermForecasting
A.ApproximationGuarantees at each step, and not the aggregated loss throughout the
window.
Thefunctionalspaceinducedbytheparametrization,which
wewilldenoteF ={f :θ ∈Θ}canbenon-convex,asin Wenowstatethemaintheoremsthatboundtheapproxima-
θ θ
thecaseoftransformerarchitectures. However,ithasbeen tionerrors.
shown(Chamon&Ribeiro,2020;Hounieetal.,2023)that Theorem 1 (Hounie et al., 2023) Let ζ⋆ be an optimal
aslongasthedistancetoitsconvexhullF = conv(F )
θ θ relaxation for problem (R-LS). Under Ass. A.1′–A.4, it
isbounded,thenwecanleveragethestrongdualityofthe
holdswithprobabilityof1−(3m+2)δthat
convexvariationalprogramdefinedoverF togetherwith
uniformconvergenceboundstoprovideapprθ
oximationguar-
(cid:12) (cid:12)P⋆−D⋆(cid:12) (cid:12)≤h(ζ⋆+1·Mν)−h(ζ⋆)+Mν+(1+∆)ξ(N,δ).
antees. Thatis,thevaluesof(R-LS)and(ED-LS)areclose.
Thisholdsbothfortheconstrainedandresilientformulation,
Theorem2(Chamon&Ribeiro,2020)Letλ⋆beanopti-
althoughasdiscussednext,theresilientproblemrequires malrelaxationfortheconstrainedproblem(P-LS)defined
milderassumptions. overF θ withconstraintsϵ i−Mν. UnderAss.A.1–A.4,it
holdswithprobabilityof1−(3m+2)δthat
Assumption A.1. There exist a function ϕ ∈ F such
that all constraints are met with margin c > 0, i.e., (cid:12) (cid:12)P⋆−D⋆(cid:12) (cid:12)≤(1+λ⋆)Mν+(1+∆)ξ(N,δ).
E (cid:2) ℓ (ϕ(x),y)(cid:3) ≤c,foralli=1,...,m.
Di i
Assumption A.1.′ There exist a finite relaxation ζ ⪯ ∞ Notethatunlikeinunconstrainedstatisticallearning,opti-
and a function ϕ ∈ F such that all constraints are met maldualvariablesλ⋆ orrelaxationsζ⋆ playanimportant
with margin c > 0, i.e., E Di(cid:2) ℓ i(ϕ(x),y)(cid:3) ≤ ζ − c, for roleintheseapproximationbounds. Thisisbecause,intu-
alli=1,...,m. itively,theyrepresentthedifficultyofsatisfyingconstraints.
AssumptionA.2. Thelossfunctionsℓ , i = 0...m, are
i
convexandM-Lipschitzcontinuous. B.MonotonicLossconstraints
AssumptionA.3. Foreveryϕ ∈ F, thereexistsθ† ∈ Θ
suchthatE (cid:2) |ϕ(x)−f (x)|(cid:3) ≤ν,foralli=0,...,m. Alessrestrictiveconstraintthataccountsforerrorcom-
Di θ†
pounding is to only require errors to be monotonically
AssumptionA.4. Thereexistsξ(N,δ) ≥ 0suchthatfor
increasing. That is, that there exists some ϵ such that
alli=0,...,mandallθ ∈Θ,
ϵ ≥ ϵ for all t. Instead of searching for such an ϵ,
t+1 t
(cid:12) (cid:12) (cid:12) (cid:12)E Di(cid:2) ℓ i(f θ(x c),y p)(cid:3) − N1 (cid:88)N ℓ i(cid:0) f θ(xn c),y pn(cid:1)(cid:12) (cid:12) (cid:12) (cid:12)≤ξ(N,δ) w sme ac la ln md oi dre ifict cl ay tii om np io nse (m P-o Ln So )t ,o an sic dit iy sc, uw sh si ec dh ir neq Su ei cr te ios non Bly .1a .
n=1
withprobability1−δoverdrawsof{(xn,yn)}. B.1.Formulation
i i
Note that the constraint qualification in Assumption A.1 We can require the loss on each timestep i to be smaller
(known as Slater’s condition (Boyd et al., 2004)), which thanthelossstepatthefollowingi+1. Thisleadstothe
isrequiredfortheconstrainedproblem,canberelaxedto constrainedstatisticallearningproblem:
themilderqualificationgiveninAssumptionA.1.′ forthe
resilientproblem.  
1
(cid:88)Tp
Assumption A.2 holds for commonly used objectives in m θ∈i Θn E (xc,yp)∼D
T
p
ℓ i(f θ(x c),y p)
Time-Series forecasting, including Mean Squared Error, i=1
MeanAbsoluteErrorandHuber-loss,amongothers. s. to:E [ℓ (f (x ),y )]
(xc,yp)∼D i θ c p
AssumptionA.3willholdiftheparametrizedfunctionspace ≤E (xc,yp)∼D[ℓ i+1(f θ(x c),y p)]
is rich in the sense that the distance to its convex hull is
i=1,...,T . (P-M)
pred−1
bounded. Since neural networks and transformers have
universal approximation properties (Hornik et al., 1989;
Becauseoflinearityoftheexpectation,theconstraintcan
Yunetal.,2019),wepositthatgivenaparametrizationwith
bere-writtenasasingleexpectationofa(non-convex)func-
enoughcapacity,thisassumptionholds.
tional,explicitly,
AssumptionA.4–knownasuniformconvergence–iscus-
E [ℓ (f (x ),y )
tomary in statistical learning theory. This includes gen- (xc,yp)∼D i θ c p
eralization bounds based on VC dimension, Rademacher −ℓ (f (x ),y )]≤0.
i+1 θ c p
complexity,oralgorithmicstability,amongothers(Mohri
etal.,2018). Unlikegeneralizationboundsfortheuncon- Despitethelackofconvexity,theboundspresentedinAp-
strainedproblem(ERM),thisassumptionboundstheloss pendix A still hold under additional assumptions on the
12LossShapingConstraintsforLong-TermForecasting
distributionofthedataandmodelclass,asshowninThe- Autoformer Informer Reformer Transformer
orem 1 and Proposition B.1 (Chamon et al., 2022). We
includetheseassumptionshereforcompleteness. 96 0.38 0.34 0.60 0.18
192 0.42 0.17 0.37 -0.06
Assumption B.1. The functions y (cid:55)→ ℓ (ϕ(·),y)f (· |
i i 336 0.17 0.17 -0.19 -0.16
y),i=1,...,m,areuniformlycontinuousinthetotalvari- 720 0.30 0.20 0.07 0.19
ation topology for each ϕ ∈ H, where f (x | y) denotes 96 0.76 0.20 0.67 -0.15 i
192 0.32 0.08 0.19 -0.55
thedensityoftheconditionalrandomvariableinducedby
336 0.24 -0.44 0.17 0.43
D . Explicitly,foreachϕ∈Handeveryϵ>0thereexists
i 720 0.69 -0.63 0.12 -0.69
δ ϕ,i >0suchthatforall|y−y˜|≤δ ϕ,iitholdsthat 96 0.01 0.23 0.14 -0.30
192 0.43 -0.08 -0.43 0.33
(cid:90)
336 0.46 -0.22 0.55 0.25
sup |ℓ (ϕ(x),y)f (x|y)−ℓ (ϕ(x),y˜)f (x|y˜)|dx≤ϵ
i i i i 720 0.83 0.45 0.69 0.64
Z∈B Z
Table5.Spearmancorrelationforstep-wiseMeanTrainandTest
Assumption B.2. The conditional distribution x | y in-
errors,formodelstrainedwithconstantconstraints.
ducedbyDarenon-atomic.
Notethattheepigraphformulationof(P-M)resemblesthat
of the original los schaping constrained problem (P-LS),
withthedifferencethatϵisanoptimizationvariable,now C.1.Extendedexperimentresults
constrainedtobemonotonicallyincreasing:
C.1.1.LOSSSHAPEPLOTS
 
1
(cid:88)Tp Figure 6 shows the loss landscapes of the different trans-
m θ∈i Θn E (xc,yp)∼D
T
p
i=1ℓ i(f θ(x c),y p) f teo rr nm se ir na lr oc sh sit se hc atu pr ee ss at ce rs ote sd so mn oa dl el lth ar re ce hid tea cta tuse rets s. cS ao nm be ep oa bt --
s. to: E [ℓ (f (x ),y )]≤ϵ served. Forinstance,Informertendstohaveajaggedloss
(xc,yp)∼D i θ c p i
distributionwithperiodicdiscontinuities.Ontheotherhand,
ϵ ≤ϵ
i i+1 Autoformer’slossesareconcaveorshowseasonalpatterns.
i=1,...,T p−1. (P-M-epi) Thesedistributionscouldbeduetotheinherentinductivebi-
asesoneachmodelarchitecture,suchastheautocorrelation
andtrenddecompositionmechanismsinAutoformer.
C.ExperimentSetup
However,theparticularshapeofthelossremainsidiosyn-
Early Stopping While the common practice in trans-
cratictoeachcombinationofdataset,model,andprediction
formersforforecastingistotrainwithearlystopping,we
windowlength. Furthermore, theeffectofimposingcon-
disableitfortheconstrainedapproachandtrainforafull
straintscanbeappreciatedinmultipleinstances,wherethe
10epochs,duetotheslowerconvergence. Notethatinthe
constraints appear to have a corrective effect on extreme
results presented in this work we keep early stopping for
cases,improvingMSEandMCVsimultaneously.
ERM.
Finallywealsoobservethat,moreoftenthannot,lossland-
scapesarenotlinear,letaloneconstant. Moreover,inrare
Tuningofduallearningrateandinitializationparame-
cases, such as Reformer on Exchange Rate with predic-
ters. Preliminaryexplorationyieldedconsistentlysuperior
tion window of 192 steps, the stepwise loss surprisingly
resultswithduallearningratesetto0.01anddualsinitial-
trendsdownwards. Thismotivatesfurtherexperimentation
ized to 1.0. All experiments reported in the paper were
ondifferentkindsofconstraintsthatarebettersuitedfora
performedwiththisparameterization.
particularlearningproblem.
Choice of ϵ. To choose an appropriate upper bound con-
straintforthestepwiselosses,weperformagridsearchof C.1.2.CORRELATIONBETWEENTRAININGANDTEST
sixvaluesforeverysettingofdataset,modelandprediction ERRORS.
window,optimizingforvalidationMSE.Thevaluesforthe
WecomputetheSpearmancorrelationbetweenmeanstep-
search are the 25, 50, and 75th percentile of the training
wiseerrorscomputedonthetrainingandtestsetsformodels
andvalidationerrorsofeachmodeltrainedwithERM.The
trained with constant constraint levels. Compared to the
valuesreportedinTables3and2arefortheoptimalvalues
values reported for ERM in Table 1, we observe weaker
ofthisgridsearch.
correlationsforourmethod. Thiscanindicatethat,while
Alternativechoicesofepsilon. Duringpreliminaryexperi- imposing a constraint on the training loss, our approach
ments,weexplored increasinglyoverfitstotrainingdata.
13
LCE
egnahcxE
rehtaeWLossShapingConstraintsforLong-TermForecasting
C.1.3.CORRELATIONSBETWEENPREDICTIONLENGTHS
In order to quantify the degree of similarity between er-
rorpatternsacrossvariouspredictionlengths,wecompute
Pearsoncorrelationcoefficientsbetweenstep-wiseerrors,
asshowninFigure7. Weachievethisbyfirstre-sampling
shorter prediction windows using linear interpolation to
matchthenumberofsteps. Notably,somesetupspresent
highcorrelationsforseveralpredictionlengths,whichwe
interpretastheerrordistributionsbeingsimilar.
14LossShapingConstraintsforLong-TermForecasting
Weather Electricity
0.6 Transformer(96) 1.00 Transformer(192) 1.25 Transformer(336) 1.5 Transformer(720) 0.30 Transf Cor om ne sr tr( a9 i6 n) ed 0.350 Transformer(192) 0.35 Transformer(336) 0.45 Transformer(720)
00 .. 24
C ERon Mstrained
000 ... 257 505 0001 .... 2570 5050 01 .. 50 00 .. 22 68 ERM 0000 .... 2233 5702 0505 00 .. 23 50 000 ... 334 050
0 50 0 100 200 0 200 0 500 0 50 0 100 200 0 200 0 500
Informer(96) Informer(192) Informer(336) Informer(720) Autoformer(96) Autoformer(192) Autoformer(336) Autoformer(720)
0.6 0.8 0.8 1.5 0.24 0.30 00 .. 56 0.5
00 .. 24 000 ... 246 000 ... 246 01 .. 50 000 ... 122 802 00 .. 22 05 000 ... 234 000 ... 234
0 50 0 100 200 0 200 0 500 0 50 0 100 200 0 200 0 500
Reformer(96) Reformer(192) Reformer(336) Reformer(720) Informer(96) Informer(192) Informer(336) Informer(720)
0000 .... 2345 000 ... 468 001 ... 570 050 0001 .... 7890 000 ... 333 024 000 ... 334 050 0000 .... 3334 2570 5050 000 ... 344 505
0.1 0.2 0.25 0.6 0.28 0.300
0 50 0 100 200 0 200 0 500 0 50 0 100 200 0 200 0 500
0.4 Autoformer(96) 0.4 Autoformer(192) 00 .. 56 Autoformer(336) 0.5 Autoformer(720) 0.32 Reformer(96) 00 .. 33 46 Reformer(192) 00 .. 34 80 Reformer(336) 00 .. 33 68 Reformer(720)
0.3 0.3 0.4 0.4 0.30 0.32 00 .. 33 46 00 .. 33 24
0.2 0.2 0.3 0.3 0.28 0.30 0.32 0.30
0 50 0 100 200 0 200 0 500 0 50 0 100 200 0 200 0 500
PredictionStep PredictionStep
ExchangeRate
Transformer(96) Transformer(192) Transformer(336) Transformer(720)
01 .. 80 C ERon Mstrained 1.5 1.5 3
0.6 1.0 1.0 2
0.4 0.5 1
0.5
0 50 0 100 200 0 200 0 500
Informer(96) Informer(192) Informer(336) Informer(720)
1.50
1.25 1.25 2.0 4
1.00 1.00 1.5 3
0.75 0.75 2
0.50 0.50 1.0 1
0 50 0 100 200 0 200 0 500
Autoformer(96) Autoformer(192) Autoformer(336) Autoformer(720)
4
0.4 3 6 3
0.3 2 4 2
0.2 1 2 1
0.1 0 0
0 50 0 100 200 0 200 0 500
Reformer(96) Reformer(192) Reformer(336) 3.0 Reformer(720)
1.2 2.5 2.5
1.0 1.5 2.0 2.0
0.8 1.0 1.5 1.5
1.0
0 50 0 100 200 0 200 0 500
PredictionStep
Figure6.FullvisualizationsforthetestlosslandscapesoftheExchange,WeatherandECLdatasets,onconstrainedandERMlosses.
15
ESM
ESM
ESMLossShapingConstraintsforLong-TermForecasting
Weather
Autoformer Informer
1.0 1.0
1.00 -0.58 -0.51 -0.47 0.8 1.00 0.70 0.58 0.33 0.9
0.6
0.8
-0.58 1.00 0.80 0.86 0.4 0.70 1.00 0.59 0.59
0.7
0.2
-0.51 0.80 1.00 0.96 0.0 0.58 0.59 1.00 0.74 0.6
0.2 0.5
-0.47 0.86 0.96 1.00 0.4 0.33 0.59 0.74 1.00 0.4
96 192 336 720 96 192 336 720
Prediction Length Prediction Length
Reformer Transformer
1.0 1.0
1.00 0.74 0.71 0.46 1.00 0.71 0.66 0.53
0.9
0.9
0.8
0.74 1.00 0.80 0.36 0.71 1.00 0.95 0.82
0.8
0.7
0.71 0.80 1.00 0.41 0.6 0.66 0.95 1.00 0.89 0.7
0.5
0.46 0.36 0.41 1.00 0.53 0.82 0.89 1.00 0.6
0.4
96 192 336 720 96 192 336 720
Prediction Length Prediction Length
Electricity
Autoformer Informer
1.0 1.0
1.00 0.77 0.26 0.29 0.9 1.00 0.23 0.05 0.05
0.8
0.8
0.77 1.00 0.56 0.51 0.23 1.00 0.64 0.31
0.7 0.6
0.6
0.26 0.56 1.00 0.73 0.05 0.64 1.00 0.39 0.4
0.5
0.4 0.2 0.29 0.51 0.73 1.00 0.05 0.31 0.39 1.00
0.3
96 192 336 720 96 192 336 720
Prediction Length Prediction Length
Reformer Transformer
1.0 1.0
1.00 0.95 0.79 0.33 0.9 1.00 0.80 0.61 0.55 0.9
0.8 0.8
0.95 1.00 0.88 0.42 0.80 1.00 0.82 0.51
0.7 0.7
0.79 0.88 1.00 0.58 0.6 0.61 0.82 1.00 0.31 0.6
0.5 0.5
0.33 0.42 0.58 1.00 0.4 0.55 0.51 0.31 1.00 0.4
96 192 336 720 96 192 336 720
Prediction Length Prediction Length
Figure7.Pearson correlation between test step wise errors for
weatherandelectricitydatasets. Eachheatmapshowspairwise
correlationsbetweenallpredictionlengthsforagivenmodel.
16
htgneL
noitciderP
htgneL
noitciderP
htgneL
noitciderP
htgneL
noitciderP
69
291
633
027
69
291
633
027
69
291
633
027
69
291
633
027
htgneL
noitciderP
htgneL
noitciderP
htgneL
noitciderP
htgneL
noitciderP
69
291
633
027
69
291
633
027
69
291
633
027
69
291
633
027