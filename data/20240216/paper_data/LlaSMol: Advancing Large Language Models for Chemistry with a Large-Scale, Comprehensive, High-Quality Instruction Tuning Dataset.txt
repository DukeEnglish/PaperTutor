LlaSMol: Advancing Large Language Models for Chemistry
with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
BotaoYu FrazierN.Baker* ZiqiChen* XiaNing HuanSun
TheOhioStateUniversity
{yu.3737, baker.3239, chen.8484, ning.104, sun.397}@osu.edu
https://osu-nlp-group.github.io/LLM4Chem/
Abstract been developed for various chemistry tasks such as for-
wardreactionprediction,retrosynthesis,propertyprediction
Chemistryplaysacrucialroleinmanydomains,
(Schwalleretal.,2019;Zhongetal.,2022;Chenetal.,2023;
such as drug discovery and material science.
Zhouetal.,2023). However,thesemodelsareusuallytask-
While large language models (LLMs) such as
specificmodels,whichneglectsharedchemistryknowledge
GPT-4exhibitremarkablecapabilitiesonnatural
acrosstasksandcanhardlybeadaptedtodifferenttasks.
languageprocessingtasks,existingworkshows
theirperformanceonchemistrytasksisdiscour- Ontheotherhand,largelanguagemodels(LLMs)suchas
aginglylow. Inthispaper,however,wedemon- GPT-4(Achiametal.,2023),Llamaseries(Touvronetal.,
stratethatourdevelopedLLMscanachievevery 2023a;b), and Mistral (Jiang et al., 2023) have emerged
strongresultsonacomprehensivesetofchemistry asgeneral-purposefoundationmodelsanddemonstratere-
tasks, outperformingthemostadvancedGPT-4 markableabilitiesonvariousnaturallanguageprocessing
acrossallthetasksbyasubstantialmarginand tasks(Changetal.,2024;Thirunavukarasuetal.,2023;Yue
approachingtheSoTAtask-specificmodels. The etal.,2023;Zhangetal.,2023;Dengetal.,2023). How-
keytooursuccessisalarge-scale,comprehensive, ever, when applied to chemistry tasks, LLMs show only
high-qualitydatasetforinstructiontuningnamed limitedcapabilities(Jablonkaetal.,2022;Guoetal.,2023;
SMolInstruct. It contains 14 meticulously Hatakeyama-Sato et al., 2023). For example, Guo et al.
selected chemistry tasks and over three million (2023)conductedevaluationsoneightchemistrytasksand
high-quality samples, laying a solid foundation observedthatwhileGPT-4outperformsotherclosed-and
fortrainingandevaluatingLLMsforchemistry. open-sourceLLMs,itsperformanceisnotcomparablewith
BasedonSMolInstruct,wefine-tuneasetof thatoftask-specificdeeplearningmodels. Particularly,they
open-source LLMs, among which, we find that foundthatGPTmodelsperformpoorlywhenapreciseun-
Mistralservesasthebestbasemodelforchem- derstandingofSMILES(Weininger,1988),awidelyused
istry tasks. We further conduct analysis on the textualrepresentationformolecules,isrequired. Inaddition
impactoftrainableparameters,providinginsights to directly applying pretrained LLMs, Fang et al. (2023)
forfutureresearch.1 fine-tunedLLMsonaninstructiontuningdataset,buttheir
performanceremainsverylow,farbehindthestate-of-the-
art(SoTA)modelsdesignedandtrainedforspecifictasks.
1.Introduction Giventhediscouragingresultsthusfar,somecriticalques-
tionsarise: AreLLMsactuallyabletoeffectivelyperform
Chemistryisafundamentalsciencethatunderpinscount- chemistrytasks? Or,Aretheyfundamentallylimitedfor
less aspects of modern life, ranging from drug discovery chemistry?Inthispaper,wedemonstratethatourdeveloped
and materials science to energy production. To facilitate LLMscanachieveverystrongresultsonacomprehensive
researchandapplicationsinthisdomain,deeplearningmod- set of chemistry tasks, outperforming the most advanced
elsincludinggraphneuralnetworks(Kipf&Welling,2017) GPT-4acrossallthetasksbyasubstantialmarginandap-
andTransformer-basedmodels(Vaswanietal.,2017)have proachingtheSoTAtask-specificmodels.
*Equalcontribution WhatmakessuchLLMspossible? First,wecarefullycon-
structalarge-scale,comprehensive,andhigh-qualitydataset
1Datasetandmodelwillbereleasedathttps://github. forinstructiontuningnamedSMolInstruct. Wemetic-
com/OSU-NLP-Group/LLM4Chem
1
4202
beF
41
]IA.sc[
1v19390.2042:viXraLlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
Name Conversion Property Prediction
O=C=O
IUPAC to Molecular Formula (NC-I2F) CO2 ESOL (PP-ESOL)
Query:What is the molecular formula of the compound with this IUPAC name Query:How soluble is <SMILES> CC(C)Cl </SMILES> ?
<IUPAC> 2,5-diphenyl-1,3-oxazole </IUPAC>? Response:Its log solubility is <NUMBER> -1.41 </NUMBER>mol/L.
Response:<MOLFORMULA> C15H11NO </MOLFORMULA>
LIPO (PP-LIPO)
IUPAC to SMILES (NC-I2S) Query:Predict the octanol/water distribution coefficient logDunder the circumstance of pH 7.4
Query:Could you provide the SMILES for for<SMILES>NC(=O)C1=CC=CC=C1O </SMILES>.
<IUPAC> 4-ethyl-4-methyloxolan-2-one </IUPAC>? Response:<NUMBER> 1.090 </NUMBER>
Response:Of course. It's <SMILES> CCC1(C)COC(=O)C1</SMILES>
BBBP (PP-BBBP)
SMILES to Molecular Formula (NC-S2F) Query:Is blood-brain barrier permeability (BBBP) a property of
Query:Given the SMILES representation <SMILES> CCNC(=O)/C=C/C1=CC=CC(Br)=C1 </SMILES>?
<SMILES> S=P1(N(CCCl)CCCl)NCCCO1 </SMILES>, what would be its molecular formula? Response:<BOOLEAN> Yes </BOOLEAN>
Response:It is <MOLFORMULA> C7H15Cl2N2OPS </MOLFORMULA> .
ClinTox(PP-ClinTox)
SMILES to IUPAC (NC-S2I) Query:Is<SMILES> COC[C@@H](NC(C)=O)C(=O)NCC1=CC=CC=C1 </SMILES>toxic?
Query:Translate the given SMILES formula of a molecule Response:<BOOLEAN> No </BOOLEAN>
<SMILES> CCC(C)C1CNCCCNC1 </SMILES> into its IUPAC name.
Response:<IUPAC> 3-butan-2-yl-1,5-diazocane </IUPAC> HIV (PP-HIV)
Query:Can<SMILES> CC1=CN(C2C=CCCC2O)C(=O)NC1=O </SMILES>serve as an
inhibitor of HIV replication?
Molecule Description Response:<BOOLEAN> No </BOOLEAN>
Molecule Captioning (MC) SIDER (PP-SIDER)
Query:Describe this molecule: Query:Are there any known side effects of
<SMILES> CCOC(=O)C1=CN=CN1[C@H](C)C1=CC=CC=C1</SMILES> <SMILES> CC1=CC(C)=C(NC(=O)CN(CC(=O)O)CC(=O)O)C(C)=C1Br</SMILES>
Response:The molecule is an imidazole derivative with short-acting sedative, hypnotic, and affecting the heart?
general anesthetic properties. Etomidate appears to have gamma-aminobutyric acid (GABA) Response:<BOOLEAN> No </BOOLEAN>
like effects, mediated through GABA-A receptor. The action enhances the inhibitory effect of
GABA on the central nervous system by causing chloride channel opening events which leads to Chemical Reaction
membrane hyperpolarization.
Forward Prediction (FS)
Molecule Generation (MG)
Query:<SMILES> NC1=CC=C2OCOC2=C1.O=CO</SMILES>
Query:Give me a molecule that satisfies the conditions outlined in the description: The Based on the reactants and reagents given above, suggest a possible product.
molecule is a member of the class of tripyrrolesthat is a red-colouredpigment with antibiotic Response:A possible product can be <SMILES> O=CNC1=CC=C2OCOC2=C1 </SMILES>
properties produced by Serratia marcescens. It has a role as an antimicrobial agent, a biological
pigment, a bacterial metabolite, an apoptosis inducer and an antineoplastic agent. It is a Retrosynthesis (RS)
tripyrrole, an aromatic ether and a ring assembly.
Response:Here is a potential molecule: Query:Identify possible reactants that could have been used to create the specified product.
<SMILES> CCCCCC1=C(C)NC(/C=C2\N=C(C3=CC=CN3)C=C2OC)=C1 </SMILES> <SMILES> CC1=CC=C(N)N=C1N </SMILES>
Response:<SMILES> CC(C#N)CCC#N.N </SMILES>
Figure1: AnoverviewoftasksintheproposedSMolInstructdataset.
ulouslyincorporatetaskswithmeaningfulapplications,col- modelscouldpromisinglyallowLLMstomatchorsurpass
lect data from diverse data sources, and apply rigorous SoTAtask-specificmodelsonchemistry.
scrutinyforqualitycontrol. Theresultingdatasetconsists
Inansweringthecriticalquestionsraisedearlier,ourfind-
of 14 tasks and over 3M samples, laying a solid founda-
ingsunderscorethegreatpotentialofLLMstoeffectively
tionfortrainingandevaluatingLLMsforchemistrytasks.
performchemistrytasks. Whilethereisstillroomforfur-
ThetasksandexamplesinthedatasetareillustratedinFig-
therimprovement,ourLlaSMolseriescanserveasastrong
ure1.Furthermore,webuildaseriesofLLMsforchemistry
setoffoundationmodelsforchemistryinthefuture.
named LlaSMol by fine-tuning four open-source LLMs
namelyGalactica,Llama2,CodeLlama,andMistral,on
SMolInstructwithLoRA(Huetal.,2022). 2.RelatedWork
WeconductacomprehensivecomparisonbetweenLlaSMol Task-specificModelsforChemistry. Inrecentyears,many
andvariousLLMs,whichrevealsthatLlaSMoloutperforms deeplearningmodelshavebeendevelopedtotacklediffer-
existing LLMs including GPT-4 across all the tasks by a entchemistrytasks. Forexample,MolecularTransformer
substantialmargin. Interestingly,wefoundthattheMistral- (Schwalleretal.,2019)andRSMILES(Zhongetal.,2022)
basedmodelcansignificantlyleadtheperformanceamong formulateforwardsynthesisandretrosynthesisprediction
alltheLlaSMolmodels,showcasingMistral’sgreatpoten- assequence-to-sequencetranslationproblems. Chemformer
tialonchemistry. Moreover,bytuningonlyasmallfraction (Irwinetal.,2022)pretrainsatransformermodelonalarge-
ofparametersonceforallthetasks,LlaSMolcanachieve scaleSMILESdatasetandfine-tunesitforvariousdown-
comparableresultstoSoTAtask-specificmodelsthatarede- streamtasks,suchasforwardsynthesisandpropertypredic-
signedandtrainedspecificallyforeachindividualtask. Our tion. Uni-Mol(Zhouetal.,2023)incorporates3Dinforma-
furtherexplorationrevealsthataddingtrainableparameters tionofmoleculesintothepretrainingofatransformermodel
couldleadtohugeperformanceboost. Thissuggeststhat andfine-tunesitfordownstreamtasks. MolT5(Edwards
largerscaletrainingcombinedwithmoresophisticatedbase etal.,2022)firstpretrainsaT5modelonbothSMILESand
2LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
naturallanguage,andthenfine-tunesittotranslateSMILES Molecules can be represented in multiple ways, such as
into natural language (i.e., molecule captioning) or vice SMILES strings, IUPAC names, and molecular formu-
versa(i.e.,moleculegeneration). Despitetheireffectiveness, las. SMILES strings use a sequence of symbols to en-
thesemodelsoperateonsingletasksandthereforecannot code the 2D structures of molecules (Weininger, 1988).
harnessknowledgesharedacrossdiversechemistrytasks A molecule can have multiple SMILES strings; a canon-
likeLLMs. ical SMILES for the molecule is unique and determin-
istic. For example, the canonical SMILES representa-
LLMsforChemistry. RecenteffortshaveintegratedLLMs
tionofglucoseisC(C1C(C(C(C(O1)O)O)O)O)O. Molec-
with chemistry to solve key chemistry problems, such as
ular formulas represent a molecule by enumerating the
moleculepropertypredictionandretrosynthesis. Theseef-
type and number of atoms in the molecule (Solomons
forts can be divided into two categories: (1) benchmark
et al., 2022). For example, the molecular formula for
studies,(2)fine-tuningLLMswithnewdatasets. Multiple
glucose is C H O . IUPAC names are formal names
benchmarkstudieshaveevaluated(Whiteetal.,2023;Guo 6 12 6
basedonnaturallanguageelements,whichfollowthesys-
etal.,2023;Jablonkaetal.,2023;Liuetal.,2023)thecapa-
tematic rules set by the International Union of Preferred
bilitiesandlimitationsofdifferentoff-the-shelfLLMs,such
andAppliedChemistry(IUPAC)(Favre&Powell,2014).
asGPT-4andLlama,onchemistryproblems. Forexample,
These names are derived from the structures and func-
Guoetal.(2023)findsthattheseLLMsdonotperformwell
tionalgroupsofmolecules,andareintendedtobehuman-
onchemistrytasksandoftenproducechemicallyimplausi-
readable. For example, the IUPAC name for glucose is
bleoutputs. Thesefindingshighlighttheneedforfurther
(3R,4S,5S,6R)-6-(hydroxymethyl)oxane-2,3,4,5-tetrol.
effortstoimproveLLMsviafine-tuningforchemistrytasks.
Molecules are one of the fundamental units of chemistry
ToimproveLLMsforchemistrytasks,multipleinstruction
that participate in reactions (Brown, 2018). A reaction
tuningdatasetsspecifictochemistryhavebeendeveloped.
is a process which converts input molecules (reactants)
Mol-Instructions(Fangetal.,2023)consistsof1.3Minstruc-
intooutputmolecules(products)throughthebreakingand
tionsformultiplesmallmoleculetasks. However,according
formingofchemicalbonds.Othermolecules(reagents)may
toourresults(Section5.3),fine-tuningontheirdatasetdoes
bepresenttoenhanceorfacilitatethereaction.
notconsistentlyimproveLLMs’performancewhencom-
paredtoLLMswithoutfine-tuning. Drugchat(Liangetal.,
2023)collectsaninstructiontuningdatasetwith10.8Kdrug
4.SMolInstruct
moleculesalongwith143Kinstructionsregardingtheirdrug-
4.1.OverviewofSMolInstruct
specificproperties. MolOpt-Instructions(Yeetal.,2023)
consistsofinstructionswith1Mmoleculepairsformolecule SMolInstructisalarge-scaleinstructiontuningdataset
optimizationonsixproperties,inwhicheachpairhassim- thatcentersaroundsmallmolecules. Itcontainsatotalof
ilar molecules with different properties. Compared with 14chemistrytasks,asillustratedinFigure1.
thesedatasets,SMolInstructismuchlargerandcovers
(1) We include four name conversion tasks, namely con-
amorediverseandcomprehensivesetofchemistrytasks.
vertingIUPACtomolecularformula(NC-I2F),converting
ThiscouldenableLLMstobetterunderstandmoleculerep-
IUPACtoSMILES(NC-I2S),convertingSMILEStomolec-
resentationsandlearnchemistryknowledgeacrosstasks.
ularformula(NC-S2F),andconvertingSMILEStoIUPAC
(NC-S2I).Theyaredesignedtoenabledeepunderstanding
3.Preliminaries
ofmolecularstructuresandrepresentations,whichshould
serveasthefundamentalknowledgeforchemistryLLMs.
Moleculesformthebasisofchemistry,whichfundamentally
determinesthepropertiesandbehaviorsofmostsubstances. (2)Additionally,sixpropertypredictiontasksareintegrated,
Amoleculeisagroupofatomsheldtogetherbychemical includingPP-ESOLforwatersolubility(Mobley&Guthrie,
bonds (Brown, 2018). In this paper, we focus on small 2014), PP-Lipo for octanol/water distribution coefficient
molecules,whichtypicallyhavenomorethan100atoms (Poole&Poole,2003), PP-BBBPforblood-brainbarrier
andalowmolecularweightunder1,500Daltons(Lenci& penetration(Martinsetal.,2012),PP-ClinToxfortoxicity
Trabocchi,2020).Smallmoleculesperformmanyimportant to human body (Gayvert et al., 2016), PP-HIV for HIV
functions,suchassignalingincellularbiology(McNerney replicationinhibition(hiv),andPP-SIDERforsideeffects
&Styczynski,2018),pestcontrolinagriculture(Burnsetal., ofdrugs(Kuhnetal.,2015). Theseinvolvedpropertiesare
2006),micronutrientsinnutrition(Chenetal.,2022),and crucialespeciallyfordrugdevelopment.
drugtherapyinmedicine(Lenci&Trabocchi,2020). Given
(3)Twotasksfocusonthetextualdescriptionsofmolecules:
theimportanceofsmallmolecules,itisessentialtointegrate
moleculecaptioning(MC)istogenerateatextualdescrip-
LLMsintothestudyofsmallmoleculestofurtheradvance
tion of a given molecule, and molecule generation (MG)
theirdesignordevelopment.
3LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
Table1: ThestatisticsofSMolInstruct. “Qry.” and“Resp.” areaveragelengthsofqueriesandresponses,respectively.
Task Taskabbr. #Train #Valid #Test #All Qry. Resp.
NameConversion.DataSource:PubChem
IUPACtoMolecularFormula NC-I2F 300,000 1,497 2,993 304,490 84 25
IUPACtoSMILES NC-I2S 300,000 1,497 2,993 304,490 82 59
SMILEStoMolecularFormula NC-S2F 300,000 1,497 2,993 304,490 69 26
SMILEStoIUPAC NC-S2I 300,000 1,497 2,993 304,490 73 68
PropertyPrediction.DataSource:MoleculeNet
ESOL PP-ESOL 888 111 112 1,111 43 22
Lipo PP-Lipo 3,360 420 420 4,200 80 11
BBBP PP-BBBP 1,569 196 197 1,962 69 13
ClinTox PP-ClinTox 1,145 143 144 1,432 64 13
HIV PP-HIV 32,900 4,112 4,113 41,125 71 13
SIDER PP-SIDER 22,820 2,860 2,860 28,540 83 13
MoleculeDescription.DataSource:Mol-Instructions,ChEBI-20
MoleculeCaptioning MC 56,502 1,269 2,539 60,310 84 104
MoleculeGeneration MG 56,502 1,269 2,494 60,265 121 76
ChemicalReaction.DataSource:USPTO-full
ForwardSynthesis FS 988,626 2,086 4,142 994,854 99 53
Retrosynthesis RS 942,065 2,094 4,159 948,318 67 59
Overall 3,306,377 20,548 33,152 3,360,077 81 52
is to generate a molecule based on the given textual de- DataCollection. Afterconsultingdomainexpertsandcare-
scription. They require comprehensive understanding of fullypinpointingthesetofmeaningfultasks(summarized
molecules-theirstructuresandproperties,fromtheirtex- in Section 4.1), we collect data for these tasks from var-
tualdescriptions. Theyalsobridgethegapbetweennatural ious sources, as listed in Table 1. Specifically, for the
languageandmolecules. nameconversiontasks(NC-I2F,NC-I2S,NC-S2F,andNC-
S2I), we leverage PubChem2 (Kim et al., 2019), one of
(4) Lastly, two tasks revolve around chemical reaction
themostcomprehensivemoleculedatabases. Withinthis
knowledge. Forwardsynthesis(FS)aimstopredictpoten-
database, we randomly select a large set of molecule en-
tialproductsfromreactantsandreagents,andretrosynthesis
tries,andextracttheirIUPACnames,SMILESrepresenta-
(RS) involves predicting potential reactants given a prod-
tions,andmolecularformulas. Thisobtaineddataisthen
uct. Thesetasksplayvitalrolesinreal-worldapplications
re-organizedasinput-outputpairsforthetasks. Formolec-
(Coleyetal.,2018). Forexample,retrosynthesisisessential
ulardescription-relatedtasks(MCandMG),weutilizea
forsynthesisplanning,whileforwardsynthesisisusedto
combinationofChEBI-20(Edwardsetal.,2021;2022)and
validateretrosyntheticsuggestions.
Mol-Instructions(Fangetal.,2023),astheybothcontain
Table1showsthestatisticsofSMolInstruct.Itcontains high-quality molecule-text paired data. For property pre-
3.4Msamples. Eachsampleisaquery-responsepair,where dictiontasks(PP-ESOL,PP-Lipo,PP-BBBP,PP-ClinTox,
thequerydescribesataskandanytask-specificinformation PP-HIV,andPP-SIDER),weemploythewell-established
(e.g.,inputmolecule,textualdescription,etc.),andthere- MoleculeNetdatasets(Wuetal.,2018). Wecarefullyselect
sponseisasentencecontainingtheanswertothequeried 6datasetsfromMoleculeNetthatrepresentthemostessen-
task. Forallthetasks,unlessexplicitlydefinedinthetasks tialpropertiesforsmallmoleculechemistry. Forchemical
(NC-I2F,NC-I2S,NC-S2F,andNC-S2I),weuseSMILES reaction tasks (FS and RS), we collect the reaction data
astherepresentationformolecules. fromUSPTO-full(Lowe,2017),whichisanextensivecol-
lectionencompassingover1Mreactionsamplesextracted
4.2.SMolInstructConstructionDetails fromU.S.patents. Alltheaforementioneddatasetsarealso
widelyusedinpreviousstudies(Heetal.,2021;Zhongetal.,
We construct the SMolInstruct dataset by following
2022;Edwardsetal.,2022;Irwinetal.,2022;Chenetal.,
afour-steppipeline: datacollection,qualitycontrol,data
2023;Zhouetal.,2023).
splitting,andinstructionconstruction.Detailedexplanations
areasfollows. 2https://pubchem.ncbi.nlm.nih.gov/
4LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
QualityControl. Toguaranteehighquality,weapplyrig- etal.,2023)whichconsistofhighlyformattedqueries(con-
orous scrutiny. The collected data contains many prob- tainingthreeexplicitlylabeledpartsnamelyinstruction,in-
lematic and low-quality samples, which can be roughly put,andoutput)andanswer-onlyresponses(e.g.,responses
categorizedintothefollowingthreetypes,alongwithour forFSandRSonlycontainanswerSMILESalone,without
curationmethods: (1)ChemicallyinvalidSMILES.Numer- anynaturaltext),ourtemplatesexhibitamorenaturalanddi-
ousSMILESstringsarechemicallyinvalid(e.g.,deviating versesetofformatsinbothqueriesandresponses,allowing
fromtheSMILESgrammar,orviolatingchemicalvalence). formorevariationsandnaturalnessininput-outputinterac-
To address this issue, we employ RDKit (rdk), a widely tions. Moreover,alltheSMILESrepresentationsarecanon-
usedtoolkitforcheminformatics, toparsemoleculesand icalized,establishingastandardizeddataformat. Inlightof
detecterrors. (2)Wrongorinaccurateinformation. Based thedataset’sinclusionofmulti-typesequences(SMILES,
on manual check, we observed wrong and inaccurate in- molecular formula, numbers, etc.) beyond natural lan-
formation recorded in the data. For instance, within the guagetextalone,weutilizespecialtagstoencapsulatecor-
USPTO-fulldataset(Lowe,2017),weidentifyandcorrect responding segments (e.g., <SMILES>...</SMILES>
mislabeledreactantsandreagentsinchemicalreactionsby forSMILES,<MOLFORMULA>...</MOLFORMULA>for
comparingtheiratommappingswithproducts. FortheMC molecularformula,<NUMBER>...</NUMBER>fornum-
andMGtasks,wefilteroutthosetextualdescriptionsthat bers). Thisdesigndoesnotonlyexplicitlyinformmodels
lackpertinent,molecule-specificinformation,withasetof abouttheinformationtypeswithinthetaggedcontent,but
rulesbasedonwordingpatterns,lengthsandkeywords. For alsofacilitateanswerextractionduringevaluation.
PP-SIDER,weeliminatedisorderswithambiguousnames
thatcouldimpedethecreationofpreciseandcomprehen- 4.3.MeritsofSMolInstruct
sibleinstructions. (3)Duplicatedsamples. Theyprevailin
Comparedtopreviouswork(Fangetal.,2023;Liangetal.,
thedata,andwecarefullydetectandremovethem.
2023;Yeetal.,2023),SMolInstructstandsoutinsev-
Data Splitting. Data splitting for multi-task datasets re- eralkeyaspects:
quirescarefulhandlinginordertoavoiddataleakageacross
(1) Large-Scale. SMolInstruct consists of 3.4M dis-
tasks. For instance, FS and RS are reverse tasks, so data
tinctsamplesand1.6Mdistinctmolecules,withadiverse
leakageoccurswhenthetrainingsetcontainsanFSsample
range of sizes, structures, and properties (see Figure 5 in
foracertainchemicalreactionandthetestsethasanRS
AppendixC),showcasinganextensivecoverageofdiverse
sampleforthesamereaction. Thiscanleadtobiasedeval-
chemicalknowledge.
uation. Therefore, wemeticulouslyidentifysamplepairs
acrossrelatedtasks(FSandRS,MCandMG,andthefour (2)Comprehensive. SMolInstructcontains4typesof
NCtasks)thatcorrespondtothesamemolecules/reactions, chemicaltasks(14tasksintotal),emergingasthemostcom-
andensurethatmatchedsamplesareplacedtogetherinei- prehensiveinstructiontuningdatasetforsmallmolecules.
thertrainingorevaluationset.Moreover,somesamplesmay Notably,thetasksaremeticulouslyselectedtobuildastrong
sharethesameinputbuthavedifferentoutputs.Forinstance, chemistryfoundation.
intheRStask,oneproduct(thesameinput)maybesynthe-
(3) High-Quality. Rigorous processing steps have been
sizedfrommultiplesetsofreactants(differentoutputs). If
implementedtoexcludeproblematicandlow-qualitysam-
thesesamplesareplacedintobothtrainingandtestset,it
ples. Alongwithcarefuldatasplittingandcanonicalization
mayleadtoexaggeratedresults. Thereforeweensurethat
ofSMILESrepresentationsSMolInstructstandsasa
sampleswithidenticalinputsareplacedtogethereitherin
high-qualityresourcevaluableforfutureresearch.
oroutsideofthetestset. Additionally,toachievefaircom-
parisonswithMol-instructions(Fangetal.,2023),fortasks
sharedbetweenthetwodatasets(MC,MG,FS,andRS),we 5.Experiments
ensurethattheirtrainingexamplesarenotincludedinour
5.1.OurLlaSMolModels
testset,allowingforadirectevaluationoftheirmodelson
ourtestset. Followingthesenecessarylimitations,samples By fine-tuning base models on the proposed
arerandomlyspittedintotraining/validation/testset,except SMolInstruct dataset, we create LLMs capable
forPPtasksamplesthatundergoascaffoldsplittingfollow- of performing chemistry tasks, which we name LlaSMol
ingthecanonicalmethod(Wuetal.,2018). Table1shows (Largelanguagemodelsonsmallmolecules). Specifically,
thestatisticsforeachsplit. we extensively consider four different LLMs as our base
models,namelyGalactica6.7B(Tayloretal.,2022),Llama
Instruction Creation. To create query-response textual
2(Touvronetal.,2023b)7B,CodeLlama(Roziereetal.,
pairsforinstructiontuning,wemanuallycraftseveraltem-
2023) 7B, and Mistral (Jiang et al., 2023) 7B, where
plates,eachincludingaqueryandacorrespondingresponse,
Galactica is trained for scientific applications and has
andapplyGPT-4torephrasethem. Unlikethosein(Fang
5LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
already been exposed to chemistry-related data during datasetwithitspretrainedcheckpoint. InthecaseofMC
its pretraining, Llama 2 and Mistral are general-purpose andMG,wecomparewithMolT5(Edwardsetal.,2022)
LLMs,whileCodeLlamaisbasedonLlama2andtrained anddirectlyusetheirreleasedcheckpoint. Thereasonswhy
for code. We conductinstruction tuning on theproposed wedonotuseourre-trainedmodelare: (1)wewereunable
SMolInstructdataset,andnametheresultingmodels toreproduceresultsclosetothosereportedinthepaperas
asLlaSMol ,LlaSMol ,LlaSMol ,and nooriginalcodewasprovided;and(2)wetakegreatcareto
Galactica Llama2 CodeLlama
LlaSMol , respectively. All the LlaSMol models are ensurethatourtestsetisdevoidoftrainingexamplesusedby
Mistral
trainedwithLoRA(Huetal.,2022),whichisappliedtoall MolT5,ensuringfairnessintheevaluation.Lastly,regarding
weightmatricesintheself-attentionandfeedforwardneural FS and RS, we re-train RSMILES (Zhong et al., 2022)
network(FFN)moduleswithlora randlora alpha andMolecularTransformer(Schwalleretal.,2019)forthe
setto16. Thefine-tuningprocessutilizestheHuggingface two tasks, respectively, following their reported settings.
Transformers library (Wolf et al., 2020). Training spans Bothofthemodelsaretransformerencoder-decodermodels
three epochs, employing the 8-bit AdamW optimizer, a (Vaswanietal.,2017),specificallyadaptedfortheFSand
learning rate of 1e-4, and a cosine scheduler. The input RStasks.
lengthfortrainingissetto512,whichcovers99.7%ofthe
EvaluationMetrics. Weemployevaluationmetricscom-
samples. During inference, we adopt beam search as the
monlyusedinpreviouswork(Schwalleretal.,2019;Zhong
generationstrategyforsimplicity. Detailscanbefoundin
etal.,2022;Fangetal.,2023;Zhouetal.,2023;Chenetal.,
AppendixA.
2023), whichinclude: (1)ExactMatch(EM).Thismet-
ricindicatestheproportionofpredictedresultsthatexactly
5.2.ExperimentalSetup
matchthegoldstandardsinthedataset.Eachtaskmayadopt
ComparedModels. WecompareourLlaSMolmodelswith adifferentdefinitionofexactmatch,whichcanbefoundin
twotypesofmodels: AppendixA.(2)Fingerprint-basedTanimotoSimilarity
(FTS).Thismetricquantifiesstructuralsimilaritiesbetween
(1)LLMswithoutfine-tuningonSMolInstruct. This
moleculesbycalculatingtheTanimotosimilaritiesbetween
typeincludesourfourbasemodels,namelyGalactica(Tay-
theirMorganfingerprints(Morgan,1965). (3)METEOR
lor et al., 2022), Llama 2 (Touvron et al., 2023b), Code
score. For the MC task, we report the commonly-used
Llama(Roziereetal.,2023),Mistral(Jiangetal.,2023). we
text-basedmetriccalledMETEORscorethatoffersacom-
alsobenchmarkagainstGPT-4(Achiametal.,2023),the
prehensive evaluation by considering both exact matches
currentstate-of-the-art(SoTA)LLM3.ForLlama2,Code
andsemanticsimilarity. (Lavie&Agarwal,2007)(4)Root
Llama,andMistral,weuse1-shot,duetotheirpoorinstruc-
MeanSquareError(RMSE).SpecificallyusedforthePP-
tionfollowingability;forGPT-4,wereportitsresultsunder
ESOLandPP-Lipotask,thismetricquantifiestheaccuracy
azero-shotsetting,asGPT-4performsbestonthissetting
ofpredictedvaluesbymeasuringthesquarerootoftheaver-
inourexperiments(AppendixB).WealsoincludeMolinst
agesquareddifferencesbetweenpredictedandactualvalues.
(Fangetal.,2023),aLlama2modelfine-tunedontheMol-
(5) Accuracy (Acc). Applied in the binary classification
Instructions dataset (Fang et al., 2023), which shares the
tasks(PP-BBBP,PP-ClinTox,PP-HIV,andPP-SIDER),this
tasksofMC,MG,FS,andRSwithSMolInstruct.
metricistheratioofcorrectpredictions.(6)Validity(Valid).
(2)SoTAtask-specificmodels.Toprovideacomprehensive For tasks where the outputs are SMILES representations
view of LlaSMol’s performance, we present results from (NC-I2S,MG,FS,andRS),thismetricpresentstheratioof
SoTAtask-specificmodels. ForNC-I2SandNC-S2I,we validpredictionsthatfollowthegrammarofSMILESand
compare with STOUT (Rajan et al., 2021), an encoder- satisfy chemical valence rules. For all the above metrics
decoder model trained on SMILES-IUPAC name paired exceptRMSE,thelarger,thebetter.
data. ForNC-S2F,ataskachievablewithafixedalgorithm,
Formoredetailsregardingtraining,inference,andevalua-
weimplementaprogramwithRDKit(rdk),awidelyused
tionsetups,pleaserefertoAppendixA.
Pythontoolkitforcheminformatics,andreportitsresults.
ForNC-I2Fwherenodedicatedmodelsexist,weconstructa
5.3.MainResults
baselinecalledSTOUT+RDKitbyaggregatingSTOUTfor
I2SconversionandRDKitforS2Fconversion. ForthePP Table2and3showtheperformanceofdifferentmodelson
tasks,ourcomparedmodelisUni-Mol(Zhouetal.,2023). alltasksinSMolInstruct. Wemakethefollowingkey
Itincorporatesmolecular3Drepresentationsandfollowsa observations:
pretrainingandfine-tuningparadigm. Followingitsoriginal
(1)AmongalltheLLMs,ourLlaSMolmodelsdemon-
settings,wefine-tunethemodelonourSMolInstruct
strate the best performance on all tasks, underscor-
3Duetoresourcelimitations,weevaluateGPT-4on500test ing the effectiveness of the proposed SMolInstruct
samplesforeachtask,ifapplicable. datasetandthebenefitsoffine-tuning. Whencompared
6LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
Table2:Resultsfornameconversionandpropertypredictiontasks.MetricsEMandValidareinpercentage.
NC PP
I2F I2S S2F S2I ESOL Lipo BBBP Clintox HIV SIDER
Model
EM EM Valid EM EM RMSE↓ RMSE↓ Acc Acc Acc Acc
Task-Specific,Non-LLMBasedModels
SoTA 97.9 73.5 99.4 100.0 56.5 0.819 0.612 85.3 92.4 97.0 70.0
ExistingLLMswithoutfine-tuningonSMolInstruct
GPT-4 21.8 3.6 84.2 16.4 0.0 2.032 1.562 64.0 36.8 56.6 56.6
Galactica 24.7 9.7 95.6 8.8 0.0 4.184 2.979 69.0 92.4 96.7 68.1
Lama2 0.9 0.0 18.1 0.3 0.0 3.287 1.634 58.9 45.1 93.3 61.9
CodeLlama 0.4 0.0 81.0 0.1 0.0 3.483 1.733 58.9 85.4 91.8 60.2
Mistral 0.2 0.0 40.3 0.5 0.0 3.079 1.730 40.6 15.3 7.1 38.1
Molinst(instruction-tuned) 0.0 0.0 96.2 0.0 0.0 2.271 1.691 60.9 6.3 4.4 52.4
OurLlaSMolSeries
LlaSMol 81.0 57.7 99.6 90.0 18.2 2.206 1.174 69.0 93.1 96.7 67.9
Galactica
LlaSMol 74.6 41.8 99.1 86.7 10.3 2.721 1.282 68.5 93.1 96.7 65.7
Llama2
LlaSMol 79.2 49.9 99.3 91.5 15.5 2.959 1.203 69.0 93.1 96.7 69.9
CodeLlama
LlaSMol 89.8 70.1 99.6 94.5 29.0 1.150 1.010 74.6 93.1 96.7 70.7
Mistral
toourbasemodels(Galactica,Llama2,CodeLlama,and chemistryfoundationmodel. EventhoughLlaSMolmod-
Mistral),LlaSMolmodelsexhibitremarkableperformance elsmaynotoutperformtheseSoTAmodelsonmanytasks,
enhancementafterfine-tuningonSMolInstruct. This the observed performance gap has markedly diminished
highlights SMolInstruct’s effectiveness in enhancing comparedtopreviousefforts(Fangetal.,2023). Notably,
models’understandingtowardsmolecularrepresentations withonlyasmallproportionofparameterstuned(approx-
andtask-relatedknowledge,andsignifiestheeffectivelearn- imately40M,0.59%)inthe7Bmodels,LlaSMol al-
Mistral
ing of chemistry-related tasks by LLMs. Also, LlaSMol readyachievesperformanceapproachingorevensurpassing
outperformsGPT-4onallthetasks,despiteGPT-4’slarger theSoTAonseveraltaskssuchasNC-I2S,PP-SIDER,and
parameter size. Notably, LlaSMol , which has the MC. As further exploration in Section 5.4 will show that
Llama2
samebasemodelandLoRAsettingasMolinst(Fangetal., addingtrainableparameterscanleadtosubstantialperfor-
2023),surpassesitevenonthesharedtrainingtasks(MC, mance boost, we anticipate that LLMs possess immense
MG, FS, and RS). It shows the benefits of our expansive potentialtosurpasstask-specificmodelsthroughmoreex-
andhigh-qualitydatasetcomparedtoMol-Instructions. tensivefine-tuning. Additionally,LlaSMolunderscoresthe
potentialofauniversalmodelcapableofaddressingmulti-
(2) Our four LlaSMol models show substantial differ-
plechemistrytasks.
encesintheirperformance,emphasizingthesignificant
impact of base models on downstream tasks. Despite Fordetailedexperimentalresultsonmoremetrics,please
sharing identical training and inference settings, as well refertoAppendixB.
ascomparablemodelsizes,LlaSMol consistentlyout-
Mistral
performsLlaSMol Llama2byasubstantialmargin,highlight- 5.4.InfluenceofLoRAModulesandTrainable
ing Mistral’s great potential on chemistry tasks. In addi- Parameters
tion, LlaSMol exhibits better performance than
CodeLlama
Inthissection,weinvestigatetheinfluenceofusingdifferent
LlaSMol ,indicatingapotentialsynergybetweenpro-
Llama2
LoRA modules or different sizes of trainable parameters.
gramminglanguageknowledgeinCodeLlamaandmolecu-
WetakeLlaSMol asthebasicsettingandrefertoit
larrepresentations. Forinstance,understandingcodegram- Llama2
asLlaSMolinthissectionforsimplicity. Allthecompared
marmayhavepositivelyimpactedthelearningofSMILES
modelsarelistedasfollows,withthetrainableparameter
representation,whichcanberegardedasawell-definedcod-
sizesandtheratioslabeledinbrackets:
inglanguageformolecules. Furthermore,LlaSMol
Galactica
outperformsLlaSMol Llama2,andLlaSMol CodeLlamainmost • LlaSMol Lite (8.4M, 0.12%): LoRA is applied on
cases,suggestingthebenefitsofpretrainingonchemistry- q projandv projoftheattentionmodules.
relateddocuments. • LlaSMol Attn (16.8M, 0.25%): LoRA is applied on
alltheattentionprojectionmatrices(includingq proj,
(3) Our LlaSMol models exhibit comparable perfor-
k proj,v proj,o proj).
mance to SoTA models, showing great potential as a
• LlaSMol FFN (23.2M, 0.34%): LoRA is applied on
7LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
Table3:Resultsformoleculecaptioning,moleculegeneration,forwardsynthesis,andretrosynthesis.MetricsEM,FTS,
andValidareinpercentage.
MC MG FS RS
Model
METEOR EM FTS Valid EM FTS Valid EM FTS Valid
Task-Specific,Non-LLMBasedModels
SoTA 0.515 31.6 73.2 95.3 78.7 92.2 100.0 47.0 77.5 99.7
ExistingLLMsWithoutFine-TuningonSMolInstruct
GPT-4 0.198 3.4 43.1 43.1 1.1 39.0 92.0 1.2 43.7 81.0
Galactica 0.050 0.0 11.6 94.7 0.0 25.8 91.3 0.0 34.6 93.0
Llama2 0.150 0.0 4.8 93.6 0.0 13.7 97.7 0.0 27.5 87.7
CodeLlama 0.143 0.0 8.5 95.2 0.0 15.8 99.6 0.0 25.3 97.1
Mistral 0.193 0.0 9.0 35.9 0.0 19.8 99.4 0.0 24.2 98.0
Molinst(instruction-tuned) 0.124 6.0 43.6 84.8 2.1 31.6 99.8 5.7 48.0 97.8
OurLlaSMolSeries
LlaSMol 0.394 7.8 51.0 99.8 52.7 79.7 99.8 25.3 67.0 99.8
Galactica
LlaSMol 0.366 4.8 44.2 99.9 44.2 75.3 99.7 22.4 65.2 99.9
Llama2
LlaSMol 0.366 6.5 46.6 99.8 52.3 79.4 99.8 25.7 66.7 100.0
CodeLlama
LlaSMol 0.452 19.2 61.7 99.8 63.5 85.0 99.8 32.9 70.4 100.0
Mistral
asfollows: (1)ProgressingfromLlaSMolLite, LlaSMol
50 Ll4 a4 S. M20 ol LlaSM50 o.9 l0 Large Attn,LlaSMolFFNtoLlaSMol,theincorporationofmore
48.60 LoRAmodulesduringtrainingleadstoasignificantperfor-
LlaSMolPlus
40 mance enhancement. (2) When comparing LlaSMol and
38.00
LlaSMolLarge,wecantellthatlargerbasemodelsyieldsu-
LlaSMolFFN
periorresults. Furthermore,comparingLlaSMolLargeand
30 30.30
LlaSMolAttn LlaSMol Plus, despite the considerably smaller trainable
parametersize,theformerstilloutperformsthelatter. This
Llama27B
20 21.50 Llama213B indicatesthatperformanceisnotsolelydeterminedbythe
LlaSMolLite
trainableparametersize;rather,theinherentcapabilityof
0 50 100 150
thebasemodelalsoplaysacrucialrole. Insummary,refin-
TrainableParameterSize(M)
ingtheselectionofLoRAmodulestotrainandemploying
Figure2: PerformanceontheFStaskunderdifferentLoRA alarger, moresophisticatedbasemodelofferssignificant
settingsandbasemodels. potentialforfurtherimprovementsintheperformanceof
LLMsonchemistrytasks.
alltheFFNprojectionmatrices(includinggate proj, 6.Conclusion
down proj,up proj).
ToimprovetheperformanceofLLMsforchemistry, this
• LlaSMol (40.0M, 0.59%): The basic setting. LoRA is
paperintroducesalarge-scale,comprehensive,high-quality
appliedonalltheattentionandFFNprojectionmatrices.
instruction tuning benchmark, SMolInstruct. It com-
• LlaSMolPlus(171.0M,2.48%): LoRAisappliedonall
prises14meticulouslychosentasksthatarehighlyrelevant
theattentionandFFNprojectionmatrices,andlm head
to real-world applications and over 3M carefully curated
issettrainable.
samplesthatundergothoroughprocessingtoensureexcep-
• LlaSMolLarge(62.6M,0.48%): Unlikeothermodels
tionalquality. BasedonSMolInstruct,wedevelopeda
thatisbasedonLlama27B,thismodeltakesLlama213B
seriesofLLMsnamedLlaSMolonfourdifferentbasemod-
asthebasemodel. LoRAisappliedonalltheattention
els,amongwhichwefindthemistry-basedLlaSMol
andFFNprojectionmatrices,sameasthebasicsetting. Mistral
achievesthebestperformance. Ourcomprehensiveexperi-
Allthesemodelsaretrainedwiththeidenticaltrainingcon- mentalresultsdemonstrateLlaSMol’ssuperiorityoverex-
figurations(asdescribedinSection5.1). istingLLMsincludingGPT-4onallthetasksaswellassub-
stantialpotentialforLLMstosurpasstheSoTAtask-specific
Weshowthemodelperformanceontheforwardsynthesis
models.Wewillreleaseourcodeanddatatofacilitatefuture
(FS)taskinFigure2,andtheresultsforothertaskscanbe
researchanddevelopment.
foundinAppendixB.Thekeyobservationsaresummarized
8
)%(MELlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
ImpactStatements Coley, C. W., Green, W. H., and Jensen, K. F. Machine
learningincomputer-aidedsynthesisplanning. Accounts
WeanticipatethatourproposedSMolInstructdataset
ofChemicalResearch,51(5):1281–1289,2018.
andmodelswillcontributetofutureresearchonLLMsfor
chemistry. However, we acknowledge certain limitations Deng,X.,Gu,Y.,Zheng,B.,Chen,S.,Stevens,S.,Wang,
ofthiswork. Firstly,despiteourbesteffortstoensurethe B.,Sun,H.,andSu,Y. Mind2web: Towardsageneralist
high quality of the SMolInstruct dataset, we cannot agentfortheweb. InProceedingsofConferenceonNeu-
guaranteetheabsenceofincorrectorharmfulinformation. ralInformationProcessingSystems(NeurIPS)Datasets
Secondly, our primary focus is on advancing LLMs for andBenchmarksTrack,2023.
chemistrytasks;hencewehavenotevaluatedthemodels’
Durant,J.L.,Leland,B.A.,Henry,D.R.,andNourse,J.G.
generalizationabilitiesbeyondthosetasksorthemodels’
Reoptimization of mdl keys for use in drug discovery.
safetyrisks(suchasgeneratingharmfulcontentunderad-
Journalofchemicalinformationandcomputersciences,
versarialattacks).Whilerecognizingtheimportanceofsuch
42(6):1273–1280,2002.
issues,weleavethistopicasfuturework.
Edwards,C.,Zhai,C.,andJi,H. Text2mol: Cross-modal
References moleculeretrievalwithnaturallanguagequeries. InPro-
ceedingsofConferenceonEmpiricalMethodsinNatural
AIDS antiviral screen data. URL https://wiki.
LanguageProcessing(EMNLP),pp.595–607,2021.
nci.nih.gov/display/NCIDTPdata/AIDS+
Antiviral+Screen+Data. Accessed on 1 Fec Edwards,C.,Lai,T.,Ros,K.,Honke,G.,Cho,K.,andJi,
2024. H. Translationbetweenmoleculesandnaturallanguage.
InProceedingsofConferenceonEmpiricalMethodsin
Rdkit: Open-source cheminformatics. URL https:// Natural Language Processing (EMNLP), pp. 375–413,
doi.org/10.5281/zenodo.8254217. Accessed 2022.
on27Jan2024.
Fang,Y.,Liang,X.,Zhang,N.,Liu,K.,Huang,R.,Chen,
Z., Fan, X., and Chen, H. Mol-instructions: A large-
Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,
scalebiomolecularinstructiondatasetforlargelanguage
Aleman,F.L.,Almeida,D.,Altenschmidt,J.,Altman,S.,
models. arXivpreprintarXiv:2306.08018,2023.
Anadkat,S.,etal. Gpt-4technicalreport. arXivpreprint
arXiv:2303.08774,2023.
Favre,H.A.andPowell,W.H. Nomenclatureoforganic
chemistry:IUPACrecommendationsandpreferrednames
Brown,T.L. Chemistry: thecentralscience. Pearson,14th
2013. RoyalSocietyofChemistry,2014.
editionedition,2018.
Gayvert,K.M.,Madhukar,N.S.,andElemento,O. Adata-
Burns, A. R., Kwok, T. C. Y., Howard, A., Houston, E., drivenapproachtopredictingsuccessesandfailuresof
Johanson,K.,Chan,A.,Cutler,S.R.,McCourt,P.,and clinicaltrials. CellChemicalBiology,23(10):1294–1301,
Roy,P.J. High-throughputscreeningofsmallmolecules 2016.
forbioactivityandtargetidentificationinCaenorhabditis
Guo, T., Guo, K., Nan, B., Liang, Z., Guo, Z., Chawla,
elegans. NatureProtocols,1:1906–1914,2006.
N.V.,Wiest,O.,andZhang,X. Whatcanlargelanguage
models do in chemistry? a comprehensive benchmark
Chang,Y.,Wang,X.,Wang,J.,Wu,Y.,Yang,L.,Zhu,K.,
oneighttasks. InProceedingsofConferenceonNeural
Chen,H.,Yi,X.,Wang,C.,Wang,Y.,Ye,W.,Zhang,Y.,
InformationProcessingSystems(NeurIPS)Datasetsand
Chang,Y.,Yu,P.S.,Yang,Q.,andXie,X. Asurveyon
BenchmarksTrack,2023.
evaluationoflargelanguagemodels. ACMTransactions
onIntelligentSystemsandTechnology,2024.
Hatakeyama-Sato, K., Yamane, N., Igarashi, Y., Nabae,
Y., andHayakawa, T. Promptengineeringofgpt-4for
Chen,H.-Y.,Hsu,M.,andLio,C.-W.J. Microbutmighty
chemicalresearch: whatcan/cannotbedone? Science
-Micronutrientsintheepigeneticregulationofadaptive
andTechnologyofAdvancedMaterials: Methods,3(1),
immuneresponses.Immunologicalreviews,305:152–164,
2023.
2022.
He,J.,You,H.,Sandstro¨m,E.,Nittinger,E.,Bjerrum,E.J.,
Chen, Z., Ayinde, O.R., Fuchs, J.R., Sun, H., andNing, Tyrchan,C.,Czechtizky,W.,andEngkvist,O. Molecular
X. G2retro as a two-step graph generative models for optimizationbycapturingchemist’sintuitionusingdeep
retrosynthesisprediction. CommunicationsChemistry,6 neuralnetworks.Journalofcheminformatics,13(1):1–17,
(1):102,2023. 2021.
9LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Lowe, D. Chemical reactions from us patents (1976-
Wang, S., Wang, L., and Chen, W. LoRA: Low-rank sep2016), 2017. URL https://doi.org/10.
adaptation of large language models. In International 6084/m9.figshare.5104873.v1.
ConferenceonLearningRepresentations(ICLR),2022.
Martins, I. F., Teixeira, A. L., Pinheiro, L., and Falcao,
Irwin,R.,Dimitriadis,S.,He,J.,andBjerrum,E.J. Chem- A.O.Abayesianapproachtoinsilicoblood-brainbarrier
former: a pre-trained transformer for computational penetrationmodeling. JournalofChemicalInformation
chemistry. MachineLearning: ScienceandTechnology, andModeling,52(6):1686–1697,2012.
3(1):015022,2022.
Matthews,B.W. Comparisonofthepredictedandobserved
Jablonka, K. M., Schwaller, P., and Smit, B. Is gpt-3 all secondarystructureoft4phagelysozyme. Biochimica
youneedformachinelearningforchemistry? InAIfor etBiophysicaActa(BBA)-ProteinStructure,405(2):442–
AcceleratedMaterialsDesignNeurIPS2022Workshop, 451,1975.
2022.
McNerney, M. P. and Styczynski, M. P. Small molecule
Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A., and
signaling,regulation,andpotentialapplicationsincellular
Smit, B. Leveraging large language models for pre-
therapeutics. WileyInterdisciplinaryReviews: Systems
dictive chemistry. ChemRxiv, 2023. doi: 10.26434/
BiologyandMedicine,10:e1405,2018.
chemrxiv-2023-fw8n4-v3.
Mobley, D. L. and Guthrie, J. P. Freesolv: a database of
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
experimentalandcalculatedhydrationfreeenergies,with
Chaplot,D.S.,delasCasas,D.,Bressand,F.,Lengyel,
inputfiles.JournalofComputer-AidedMolecularDesign,
G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv
28(7):711–720,2014.
preprintarXiv:2310.06825,2023.
Morgan, H. L. The generation of a unique machine de-
Kim,S.,Chen,J.,Cheng,T.,Gindulyte,A.,He,J.,He,S.,
scriptionforchemicalstructures-atechniquedeveloped
Li,Q.,Shoemaker,B.A.,Thiessen,P.A.,Yu,B.,etal.
atchemicalabstractsservice. JournalofChemicalDocu-
Pubchem2019update: improvedaccesstochemicaldata.
mentation,5(2):107–113,1965.
Nucleicacidsresearch,47:D1102–D1109,2019.
Poole,S.K.andPoole,C.F. Separationmethodsforesti-
Kipf, T. N. and Welling, M. Semi-supervised classifica-
matingoctanol–waterpartitioncoefficients. Journalof
tionwithgraphconvolutionalnetworks. InInternational
ChromatographyB,797(1–2):3–19,2003.
ConferenceonLearningRepresentations(ICLR),2017.
Rajan,K.,Zielesny,A.,andSteinbeck,C. Stout: Smilesto
Kuhn, M., Letunic, I., Jensen, L. J., and Bork, P. The
iupacnamesusingneuralmachinetranslation. Journalof
siderdatabaseofdrugsandsideeffects. NucleicAcids
Cheminformatics,13(1):1–14,2021.
Research,44:D1075–D1079,2015.
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,
Lavie, A. and Agarwal, A. Meteor: an automatic metric
Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al.
for mt evaluation with high levels of correlation with
Code llama: Open foundation models for code. arXiv
humanjudgments. InProceedingsoftheSecondWork-
preprintarXiv:2308.12950,2023.
shop on Statistical Machine Translation, StatMT ’07,
pp.228–231.AssociationforComputationalLinguistics,
Schneider,N.,Sayle,R.A.,andLandrum,G.A. Getyour
2007.
atoms in order: An open-source implementation of a
Lenci, E. and Trabocchi, A. Chapter 1 - Synthetic ap- novelandrobustmolecularcanonicalizationalgorithm.
proaches toward small molecule libraries. In Small Journalofchemicalinformationandmodeling,55(10):
MoleculeDrugDiscovery,pp.1–34.Elsevier,2020. 2111–2120,2015.
Liang,Y.,Zhang,R.,Zhang,L.,andXie,P. Drugchat: to- Schwaller,P.,Laino,T.,Gaudin,T.,Bolgar,P.,Hunter,C.A.,
wardsenablingchatgpt-likecapabilitiesondrugmolecule Bekas,C.,andLee,A.A.Moleculartransformer:amodel
graphs. arXivpreprintarXiv:2309.03907,2023. foruncertainty-calibratedchemicalreactionprediction.
ACScentralscience,5(9):1572–1583,2019.
Liu, S., Wang, J., Yang, Y., Wang, C., Liu, L., Guo, H.,
andXiao,C. Chatgpt-poweredconversationaldrugedit- Solomons,T.W.G.,Fryhle,C.B.,andSnyder,S.A. Or-
ingusingretrievalanddomainfeedback. arXivpreprint ganicChemistry,IntegratedE-TextwithE-SolutionsMan-
arXiv:2305.18090,2023. ual. Wiley,13thedition,2022.
10LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
Taylor,R.,Kardas,M.,Cucurull,G.,Scialom,T.,Hartshorn, Zhang,T.,Yue,X.,Li,Y.,andSun,H. Tablellama: Towards
A.,Saravia,E.,Poulton,A.,Kerkez,V.,andStojnic,R. openlargegeneralistmodelsfortables. arXivpreprint
Galactica: A large language model for science. arXiv arXiv:2311.09206,2023.
preprintarXiv:2211.09085,2022.
Zhong,Z.,Song,J.,Feng,Z.,Liu,T.,Jia,L.,Yao,S.,Wu,
Thirunavukarasu,A.J.,Ting,D.S.J.,Elangovan,K.,Gutier- M.,Hou,T.,andSong,M. Root-alignedsmiles: atight
rez, L., Tan, T. F., and Ting, D. S. W. Large language representationforchemicalreactionprediction.Chemical
modelsinmedicine. NatureMedicine,29(8):1930–1940, Science,13(31):9023–9034,2022.
2023.
Zhou, G., Gao, Z., Ding, Q., Zheng, H., Xu, H., Wei, Z.,
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
Zhang,L.,andKe,G. Uni-mol: Auniversal3dmolecu-
M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E., larrepresentationlearningframework. InInternational
Azhar,F.,etal. Llama:Openandefficientfoundationlan- ConferenceonLearningRepresentations(ICLR),2023.
guagemodels. arXivpreprintarXiv:2302.13971,2023a.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale,S.,etal. Llama2: Openfoundationandfine-
tuned chat models. arXiv preprint arXiv:2307.09288,
2023b.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
tention is all you need. In Proceedings of Conference
on Advancesin neural informationprocessing systems
(NeurIPS),volume30,2017.
Weininger,D. Smiles,achemicallanguageandinformation
system. 1. introduction to methodology and encoding
rules. Journal of chemical information and computer
sciences,28(1):31–36,1988.
White, A. D., Hocky, G. M., Gandhi, H. A., Ansari, M.,
Cox,S.,Wellawatte,G.P.,Sasmal,S.,Yang,Z.,Liu,K.,
Singh,Y.,andCcoa,W.J.P. Assessmentofchemistry
knowledgeinlargelanguagemodelsthatgeneratecode.
DigitalDiscovery,2(2):368–376,2023.
Wolf,T.,Debut,L.,Sanh,V.,Chaumond,J.,Delangue,C.,
Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,
et al. Transformers: State-of-the-art natural language
processing. InProceedingsofconferenceonempirical
methodsinnaturallanguageprocessing: systemdemon-
strations(EMNLP),pp.38–45,2020.
Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Ge-
niesse, C., Pappu, A. S., Leswing, K., and Pande, V.
Moleculenet: abenchmarkformolecularmachinelearn-
ing. Chemicalscience,9(2):513–530,2018.
Ye,G.,Cai,X.,Lai,H.,Wang,X.,Huang,J.,Wang,L.,Liu,
W.,andZeng,X. Drugassist: Alargelanguagemodelfor
moleculeoptimization. arXivpreprintarXiv:2401.10334,
2023.
Yue,X.,Qu,X.,Zhang,G.,Fu,Y.,Huang,W.,Sun,H.,Su,
Y.,andChen,W. Mammoth: Buildingmathgeneralist
modelsthroughhybridinstructiontuning. arXivpreprint
arXiv:2309.05653,2023.
11LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
A.DetailsofExperimentalSetup
In this section, we introduce the details of our experimental setups, including the training and inference details of our
LlaSMolmodelsandthecomparedmodels. WealsogivedetailedexplanationsofthemetricsusedinSection5.3,aswell
extrametricsthatwewilluseinAppendixB.
A.1.LlaSMolModels
ThebasemodelsusedfordevelopingLlaSMolareGalactica4(Tayloretal.,2022),Llama25(Touvronetal.,2023b),Code
Llama6(Roziereetal.,2023)andMistral7(Jiangetal.,2023). WeconductinstructiontuningonourSMolInstruct,and
theresultingmodelsarecallednamedasLlaSMol ,LlaSMol ,LlaSMol ,andLlaSMol ,respectively.
Galactica Llama2 CodeLlama Mistral
Expectforbeingbasedondifferentbasemodels,theirtrainingandevaluationconfigurationsareidentical,asdescribedas
follows.
WeusedLoRA(Huetal.,2022)duringtraining,whichisappliedtoalllinearlayersintheself-attentionandFFNmodules
withlora randlora alphasetto16. Withthe8-bitAdamWoptimizer,alearningrateof1e-4,andacosinescheduler,
wetraineachmodelforthreeepochs. Theinputlengthissetto512,andsequenceslongerthan512aretruncated.
Duringinference,weadoptbeamsearchasthegenerationstrategyforsimplicity. Duetotheneedofevaluationsonthetop-k
predictedanswers(asinAppendixB,wherekvariesfordifferenttasks,wegeneratedifferentnumbersofsequencesfor
differenttasksbysettingthenum return sequencesargumentintheHuggingfaceTransformerslibrary(Wolfetal.,
2020). Specifically,itissetto5forNC-I2S,NC-S2I,FS,andMG;3forNC-I2FandNC-S2F;1forallthePPtasks;and10
forRS.Thebeamsizeissettonum return sequences+3forallthetasks. Themaximumnumberofnewgenerated
tokensissetto1024.
A.2.ComparedModels
Weintroduceeachofthecomparedmodelsindetails,includingtheirtraining(ifapplicable)andinferenceprocess.
A.2.1.GPT-4
General You are an expert chemist. Given the SMILES representation of reactants and reagents,
Template your task is to predict the potential product using your chemical reaction knowledge.
The input contains both reactants and reagents, and different reactants and reagents
Task-Specific are separated by ".". Your reply should contain only the SMILES representation of the
Template predicted product and no other text. Your reply must be valid and chemically
reasonable.
Reactants and reagents SMILES: C1CCOC1.CCN(CC)CC.CS(=O)(=O)Cl.CS(C)=O.
N[C@@H]1CC2=CC=C(CN3C=C(CO)C(C(F)(F)F)=N3)C=C2C1
ICL
Product SMILES: CS(=O)(=O)N[C@@H]1CC2=CC=C(CN3C=C(CO)C(C(F)(F)F)=N3)C=
C2C1
Reactants and reagents SMILES: CCN.CN1C=CC=C1C=O
Question
Product SMILES:
Figure3: AnexampleofquerytemplateforGPT-4.
GPT-4(Achiametal.,2023)istheSoTALLMtodate. Weusethemodelversionedasgpt-4-0613andevaluateiton
4https://huggingface.co/facebook/galactica-6.7b
5meta-llama/Llama-2-7b-hf
6codellama/CodeLlama-7b-hf
7mistralai/Mistral-7B-v0.1
12LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
500samplesfromSMolInstructtestsetviaOpenAI’sAPI.SinceGPT-4isnotfine-tunedonourdatasetandthusis
notfamiliarwiththeflexiblequeries,toensureitgeneratesanswersinanexpectedformat,wefollowthepromptformat
proposedin(Guoetal.,2023)andcreateaquerytemplateforeachofthetasks. ThetemplateforFSisshowninFigure3.
Itcontains4parts: (1)Generaltemplatedescribesthetaskinageneralway. (2)Task-specifictemplatedescribesthe
detailed content requirements and format requirements for the specific task. (3) ICL contains the in-context learning
examples. Itprovidesexamplesintheformatof<input title>: <input content>\n <output title>:
<output content>\n,where<input title>and<output title>serveasstraightforwardpromptstothe
inputandoutputcontent. Thisdesignmakethequeriedtaskmoreclear. (4)QuestionhasthesameformatasICL,with
<output content>beingemptyforthemodeltogenerate.
Weconductboths-shotevaluations,wheres=0,1,3,5isthenumberofprovidedICLexamples. For0-shotevaluation,
the ICL part in the template is removed from the queries. In k-shot evaluation, for each sample,the ICL examples are
randomlyselectedfromthetrainingset. TheresultsofthesesettingsareshowninAppendixB,whichrealsthatthese
settings’performanceisnotconsistentacrossallthetasks. Since0-shotshowsthebestperformanceonmosttasks,we
reportitsresultsinSection5.3.
Intheevaluations,weusethedefaultgenerationstrategysetintheAPI.Togeneratethesamenumberofresultsforeach
sample(asdescribedinAppendixA.1),wesettheargumentnintheAPI,whichcontrolsthenumberofoutputsequences.
GPT-4canalwaysfollowtheformattedinstructionsintroducedabove,sowedonotbothertoextracttheanswersfromits
outputs,butdirectlyuseitsoutputsasthepredictedanswers.
A.2.2.GALACTICA
Galactica(Tayloretal.,2022)isaLLMwithoutundergoinginstructiontuning. ToevaluateitonSMolInstruct,we
followtheinstructionsinthepaper(Tayloretal.,2022)andtherepository8 tocreatethequeriesforeachtask. Weuse
zero-shotsetting, asitsofficialinstructiondoesnotsuggestusingfew-shotsetting. Thegenerationconfigurationisset
identicaltothatofourLlaSMolmodels(AppendixA.1).
Galatica’s outputs may contain extra text other than the expected answers. Therefore, with heuristic rules and regular
expressionmatching,weimplementaprogramtoextracttheanswersfromtheoutputsofthemodels.
A.2.3.LLAMA2,CODELLAMA,ANDMISTRAL
Forourbasemodels(Llama2,CodeLlama,andmistral),sincetheyarenottrainedonSMolInstructandhavenotseen
thediversequeriesinthedataset,weusethesamequerytemplatesasthoseusedforGPT-4(AppendixA.2.1). Weusethe
one-shotsettingforthem,asitwouldimprovemodels’abiltitytofollowtheinstructionsandgenerateanswersinamore
formatedway. Inaddition,thegenerationconfiguration(includingbeamsize,outputsequencenumbers,etc)issetidentical
tothatofourLlaSMolmodels(AppendixA.1).
Althoughwetryourbesttomaketheoutputformatasclearaspossibleinthequeries,thesethreemodelsstillcannotfollow
theinstructionsandtheiroutputsareinvariousformats. Byheuristicrulesandregularexpressionmatching,weimplementa
programtoextracttheanswersfromtheoutputsofeachofthemodels.
A.2.4.MOLINST
MolinstisaLlama2modelfine-tunedonMol-Instructions(Fangetal.,2023). OnthesharedtasksbetweenMol-Instructions
andSMolInstruct(includingMC,MG,FS,andRS),wedirectlyusethequerytemplatesfromMol-Instructionsto
achievebetterresults. Onothertasks,wecreateonequerytemplateforeachtaskfollowingthestyleofMol-Instructions.
Weusezero-shotonitsevaluation,asMol-Instructionsdoesnotcontainanyone-shotusecase.
TheoutputsofMolinstmayalsocontainextratextotherthantheexpectedanswers,especiallyonitsunseentasks. Thus,we
alsoimplementaprogramtoextracttheanswers.
8https://github.com/paperswithcode/galai
13LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
A.3.Task-Specific,Non-LLMBasedSoTAModels
A.3.1.STOUTFORNC-I2SANDNC-S2I
STOUTisaencoder-decodermodeltrainedonSMILESandIUPACnamepaireddata,anditiscapableofconductingthe
NC-2iSandNC-S2Itasks. Duetothelackoftrainingcode, wecannotre-trainitonourdataset, anddirectlyusetheir
released model checkpoint9. Since it may have encounter some test samples of SMolInstruct during training, the
evaluationresultsinTable2maybehigherthanitsrealperformance.
A.3.2.RDKITFORNC-S2F
TheNC-S2FtaskcanbeeasilyachievedwithafixedalgorithmbyparsingtheinputSMILESrepresentationandcounting
thenumbersofatoms. WeimplementaprogramwithRDKit(awidelyusedPythontoolkitforprocessingmoleculesand
otherchemicalinformation)andreportitsresults.
A.3.3.STOUT+RDKITFORNC-I2F
SincetherearenodedicatedmodelsfortheNC-I2Ftask,wecombineSTOUTfortheIUPACtoSMILESconversionand
RDKITfortheSMILEStomolecularformulaconversion. Specifically,wefeedtheinputIUPACnameintoSTOUTtoget
thecorrespondingSMILES,andthenusedtheRDKit-basedprogramtogetthemolecularformulabasedontheSMILES.
A.3.4.UNI-MOLFORALLTHEPPTASKS
Uni-Mol(Zhouetal.,2023)isaframeworkforlearningusefulrepresentationsofmoleculesbasedontheir3Dconformations.
Uni-Mol can be fine-tuned to perform property prediction based on these representations. Using the pretrained model
weights,hyperparameters,andcodesuppliedbytheauthors,wefine-tunedUni-Molmodelsforchemicalpropertyprediction
tasksonourdataset. ForSIDERpropertyprediction,weused20asthenumberoftargetsformulti-targetclassification,as
ourdatasetfocusedonaspecificsubsetof20SIDERtargets. WegeneratedresultsfromUni-Molusingthecodeprovidedby
theauthorsandevaluatedaccordingtothemetricsinSection5.2. Thedatasplitusedforfine-tuning,validation,andtesting
propertypredictiontasksisdifferentfromtheoneusedintheUni-Molpaper,sotheperformancemaynotmatchexactly.
A.3.5.MOLT5FORMCANDMG
MolT5(Edwardsetal.,2022)isaT5modelfortranslatingbetweenmoleculesandnaturallanguage. Weusethealready
fine-tunedMolT5-largecheckpointsprovidedbytheauthorsforbothmoleculegenerationandmoleculedescription. We
generatepredictionsonourtestsetusingbeamsearchwith5beams,followingtheexamplecodeprovidedbytheauthors.
Forinput,themoleculedescriptionmodelisprovidedaSMILESstringandthemoleculegenerationmodelisprovideda
naturallanguagedescription. Formoleculedescription,wegeneratedonlyoneresult. Formoleculegeneration,wesetthe
numberofsequencestoreturnto5. WeevaluateourtestsetresultsaccordingtothemetricsinSection5.2. Thedataused
fortestingisdifferentfromthedatausedbytheMolT5paper,sotheperformancemaybedifferent. Pleasenotethatourtest
setdoesnotoverlapwiththeMolT5trainingset.
A.3.6.RSMILESFORFSANDRS
RSMILES(Zhongetal.,2022)isatransformermodeltrainedonpairsofSMILESstringsalignedtominimizetheiredit
distance. RSMILEStranslatesalignedSMILESstringsofreactantsandreagentsintoproductsfortheFStask,andproducts
intoreactantsfortheRStask. FollowingthesettingsdescribedinthepaperofRSMILES,weaugmentandaligneachpair
ofSMILESstringsinourtrainingdatafor5times. FortheFStask, weadopta“mixed”settingandappendcanonical
SMILESstringsofreagentstotheendofalignedreactantSMILESstrings. WetraintwoRSMILESmodelsfortheFSand
RStasks,respectively,usingthehyper-parametersprovidedintheirGitHubrepository. Aftertraining,weaveragethelast5
checkpointstogetthefinalcheckpointforeachtask. Duringinference,weaugmenteachinputSMILESstringsfor5times.
Wegenerate10outputSMILESstringsforeachaugmentedinputusingbeamsearch,resultinginatotalof50SMILES
stringsforeachtestreaction. Wegetthefinaltop10predictionsforeachtaskbyaggregatingthese50predictionsusingtheir
providedscripts.
Theperformanceofourre-trainedRSMILESmodelonourdatasetfortheRStaskiscomparablewiththosereportedintheir
9https://github.com/Kohulan/Smiles-TO-iUpac-Translator
14LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
paperontheUSPTO-fulldataset. Pleasenotethattheperformanceofourre-trainedRSMILESfortheFStask,asshownin
Table3,islowerthanthereportedresultsontheUSPTO-MITdatasetfortheFStaskintheirpaper. Thisisduetothatour
datasetfortheFStaskismorechallengingthantheUSPTO-MITdatasetusedintheRSMILES’spaper,duetotheinclusion
ofstereochemicalinformation.
A.3.7.MOLECULARTRANSFORMERFORFSANDRS
Similar to RSMILES, Molecular Transformer (Schwaller et al., 2019) is also a transformer model trained on pairs of
SMILESstrings, andtranslatefromreactantsandreagentsintoproductsorproductsintoreactants. Whiletheoriginal
MolecularTransformeronlyfocusedontheFStask,wetrainandtestitonboththeFSandRStasks. Weusecanonical
SMILES strings of molecules without data augmentation as the training data of Molecular Transformer. We train two
MolecularTransformermodelsseparatelyfortheFSandRStasksusingthehyper-parametersprovidedintheirGitHub
repository. Duringinference,wegenerate10outputSMILESstringsforeachcanonicalinputSMILESstringusingbeam
search. Theperformanceofourre-trainedMolecularTransformermodelonourdatasetfortheFStaskiscomparablewith
thosereportedintheirpaperontheUSPTO-STEREOdataset.
A.4.EvaluationMetrics
WeintroducethemetricsusedinSection5.3asfollows:
• Exactmatch(EM).Itmeasuresthesuccessofamodelinprovidingresponsesthatperfectlymatchthereferenceor
groundtruthanswers. Notably,foreachpredictedresult,wecompareitwiththegoldanswersofallthesamplesthathave
thesameinputasthesample. Ifthereexistsamatch,itiscountedascorrectandcontributestotheratioofthismetric.
Notethatfordifferenttypesofoutputs,weemploydifferentcriterionforjudgingittheymatch. Fortaskswhereoutputs
areSMILESstrings(NC-I2S,MG,FS,andRS),weparseSMILESstringsintomolecules,andtheyarematchedonlyif
thetwomoleculesareidentical. Fortaskswhereoutputsaremolecularformula,twoarematchediftheyrepresentthe
samesetofatoms,andthecorrespondingnumbersofthesamplesareidentical. FortaskswhereoutputsareIUPACnames
(NC-S2I),sinceIUPACnamesmaycontainmultiplepartsseparatedbysemicolons,wecomparethesetcomposedof
theseparts. Thatis,wedonotcareabouttheordersofthesepartsandhowmanyofpartsarethereinthegeneratedstring,
butjudgebythecorrectnessoftheuniqueparts.
• Fingerprint-basedTanimotoSimilarity(FTS).Itisanimportantmetrictypecommonlyusedincheminformatics. It
measuresthestructuralsimilaritybetweenmolecules. TheonewereportinSection5.3isoneofthistype,calledMorgan
FTS,whichleveragesMorganmethodtocalculatethefingerprint(Morgan,1965).
• METEORscore. Itisacommonmetricusedtomeasurethesimilaritybetweentext. (Lavie&Agarwal,2007)
• RMSE.Itisacommonmetrictomeasurethedistancebetweenpredictedvaluesandthegoldvaluesonregressiontasks.
Smallerisbetter.
• Acc. Itrepresentstheratioofcorrectpredictions.
• Validity(Valid): itreportstheratioofvalidpredictedSMILESrepresentationsthatcanbesuccessfullyparsedintoa
molecule. Itiscalculatedamongallthegeneratedoutputsthatcontainextractableanswerpart. Ifanoutputrefusesto
answeraquestion,itwouldnotbecountedforcalculatingthevalidity. Exceptforthismetric,alltheothermetricsfor
SMILESarecalculatedbasedonthevalidsamples.
AdditionalmetricsusedinAppendixBarebrieflyintroducedasfollows:
• Top-k Exact Match: It is the same as EM discussed before, but on the top-k generated outputs. It gives a more
comprehensiveresults.
• MACCSFTSandRDKFTS:InadditiontoMorganFTSweuseintheprevioussections,weintroducetwoextraFTS
metrics,namelyMACCSFTSandRDKFTS,thatuseMACCS(Durantetal.,2002)andRDK(Schneideretal.,2015)
methodstocalculatethefingerprintrespectively.
• BLUEscoresandROUGEscores: Anothertypesoftextualbasedmetricsthatmeasuresthesimilaritybetweentext.
• Matthew’sCorrelationCoefficient(MCC).Appliedinthebinaryclassificationtasks(PP-BBBP,PP-Clintox,PP-HIV,
andPP-SIDER),thismetricprovidesabalancedmeasureofthequalityofbinaryclassifications(Matthews,1975).
• F1Score: Theharmonicmeanofprecisionandrecall;acommonlyusedmetricforclassificationtasks.
15LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
B.OverallExperimentalResults
B.1.MainResults
In this section, we show the overall experimental results on additional metrics. Results for name conversion tasks are
presentedinTables4,5,6,and7. ResultsformoleculedescriptiontasksarepresentedinTables8and9. Resultsforchemical
reactiontaskscanbefoundinTables10and11. ResultsforthepropertypredictiontaskscanbefoundinTables13and12.
B.1.1.NAMECONVERSION
For the NC-I2F task (Table 4), LlaSMol is the best performing LLM, tying with many other methods on validity.
Mistral
AllLlaSMolmodelsoutperformallothermethodsexcepttheSoTAtask-specificmethod. Thisshowcasesthebenefitof
fine-tuningonSMolInstruct. TheSoTAtask-specificmethod(STOUT+RDKit)outperformsLlaSMol ,achieving
Mistral
anexactmatch97.9%ofthetime. ThebestperformingLLMbaselinefortop-1exactmatchisGalactica,andGPT-4(1-shot)
isthebestperformingLLMbaselinefortop-3. Neitherofthesemethodscomeclosetoachievingthesameperformanceas
theLlaSMolseries,exceptonValidity.
Table4: Overallresults(%)ofNC-I2F.
EM
Model Validity
Top1 Top3
STOUT+RDKit 97.9 - 100.0
GPT-4 16.0 33.0 100.0
GPT-4(zero-shot) 16.0 35.0 100.0
GPT-4(0-shot) 21.8 35.6 100.0
GPT-4(1-shot) 22.2 36.2 100.0
GPT-4(3-shot) 20.6 34.6 100.0
GPT-4(5-shot) 21.2 35.0 100.0
Galactica 24.7 28.1 100.0
Llama2 0.9 1.4 99.8
CodeLlama 0.4 1.1 99.4
Mistral 0.2 3.1 99.9
Molinst(instruction-tuned) 0.0 0.1 66.8
LlaSMol 81.0 91.2 100.0
Galactica
LlaSMol 74.6 86.6 100.0
Llama2
LlaSMol 79.2 89.1 100.0
CodeLlama
LlaSMol 89.8 94.6 100.0
Mistral
FortheNC-I2Stask(Table5),LlaSMol isthebestperformingLLMforallmetrics,tyingwithLlaSMol on
Mistral Galactica
validity.AllLlaSMolmodelsoutperformallothermethodsexcepttheSoTAtask-specificmethod.Thisshowcasesthebenefit
offine-tuningonSMolInstruct. TheSoTAtask-specificmethod(STOUT)onlybarelyoutperformsLlaSMol ,
Mistral
achievinganexactmatch73.5%ofthetimeandevenbetterFTSandvalidityscores. ThebestperformingLLMbaselineis
Galactica,butitsperformanceisnotremotelyclosetoLlaSMolforanymetricexceptvalidity.
FortheNC-S2Ftask(Table6),LlaSMol isthebestperformingLLMforallmetrics,tyingwithmanyothermethodson
Mistral
validity. AllLlaSMolmodelsoutperformallothermethodsexcepttheSoTAtask-specificmethod,whichdirectlycomputes
theanswerusinganalgorithmandistherefore100%accurate. ThebestperformingLLMbaselinesareGPT-4andGPT-4(0
shot),buttheirperformanceisnotremotelyclosetoLlaSMolforanymetricexceptvalidity. LlaSMol’sabilitytotranslate
SMILESrepresentationstomolecularformulasdemonstratesanunderstandingofSMILESandchemicalformulas. While
thisisnotachallengingtask,evaluatingitcanillustrateanLLM’sunderstandingofrepresentations.
FortheNC-S2Itask(Table7),LlaSMolisthebestperformingLLMforallmetrics. Interestingly,themodelsnotfine-tuned
onSMolInstructcannotexactlymatchtheexpectedoutputeven1%ofthetime.TheSoTAtask-specificmethodachieves
56.5%accuracy,indicatingthatthistaskisnotnecessarilyeasy. LlaSMol’sabilitytotranslateSMILESrepresentations
16LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
Table5: Overallresults(%)ofNC-I2S.
ExactMatch FTS
Model Validity
Top1 Top3 Top5 MACCS RDK Morgan
STOUT 73.5 - - 99.9 99.8 99.5 99.4
GPT-4(0-shot) 3.6 4.3 5.2 77.2 51.2 49.2 84.2
GPT-4(1-shot) 3.3 5.7 6.9 76.5 49.6 48.1 85.8
GPT-4(3-shot) 3.6 5.9 6.9 76.5 48.8 46.9 84.4
GPT-4(5-shot) 2.4 4.7 6.1 75.6 47.5 46.2 84.8
Galactica 9.7 11.1 12.5 81.5 58.1 53.4 95.6
Mistral 0.0 0.0 0.0 33.6 21.3 11.3 40.3
Llama2 0.0 0.0 0.0 29.5 18.8 11.3 18.1
CodeLlama 0.0 0.0 0.0 30.7 20.0 12.0 81.0
Molinst(instruction-tuned) 0.0 0.0 0.0 43.9 25.1 18.4 96.2
LlaSMol 57.7 69.2 72.3 95.5 86.4 84.7 99.6
Galactica
LlaSMol 41.8 52.7 56.5 91.1 76.8 75.4 99.1
Llama2
LlaSMol 49.9 60.1 63.8 93.1 80.9 80.0 99.3
CodeLlama
LlaSMol 70.1 77.8 80.1 96.6 90.1 89.1 99.6
Mistral
Table6: Overallresults(%)ofNC-S2F.
EM
Model Validity
Top1 Top3
RDKit 100.0 - 100.0
GPT-4 18.0 28.0 100.0
GPT-4(zero-shot) 16.0 27.0 100.0
GPT-4(0-shot) 16.4 28.8 100.0
GPT-4(1-shot) 16.0 26.0 100.0
GPT-4(3-shot) 13.4 21.6 100.0
GPT-4(5-shot) 12.8 23.2 100.0
Galactica 8.8 9.0 100.0
Llama2 0.3 0.7 99.7
CodeLlama 0.1 0.6 100.0
Mistral 0.5 1.2 100.0
Molinst(instruction-tuned) 0.0 0.0 64.7
LlaSMol 90.0 94.8 100.0
Galactica
LlaSMol 86.7 93.9 100.0
Llama2
LlaSMol 91.5 96.0 100.0
CodeLlama
LlaSMol 94.5 97.2 100.0
Mistral
17LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
to IUPAC names suggests a level of understanding of the functional groups in the IUPAC specification, as well as an
understandingofSMILESrepresentations.
Table7: Overallresults(%)ofNC-S2I.
EM
Model
Top1 Top3 Top5
STOUT 56.5 - -
GPT-4(0-shot) 0.0 0.0 0.0
GPT-4(1-shot) 0.0 0.0 0.0
GPT-4(3-shot) 0.2 0.2 0.2
GPT-4(5-shot) 0.2 0.2 0.2
Galactica 0.0 0.0 0.0
Llama2 0.0 0.0 0.0
CodeLlama 0.0 0.0 0.0
Mistral 0.0 0.0 0.0
Molinst(instruction-tuned) 0.0 0.0 0.0
LlaSMol 18.2 30.6 35.3
Galactica
LlaSMol 10.3 18.7 21.7
Llama2
LlaSMol 15.5 26.2 30.5
CodeLlama
LlaSMol 29.0 45.3 50.5
Mistral
B.1.2.MOLECULEDESCRIPTION
FortheMGtask(Table8),LlaSMol isthebestperformingLLMonallmetricsexceptvalidity,whereLlaSMol
Mistral Llama2
outperforms it by a small margin. Its top-1 exact match score is especially impressive, more than tripling the top-1
performance of the best baseline LLM (Molinst). This indicates that the right combination of foundation model and
fine-tuningdatasetcansupportabettercomprehensionofnaturallanguageaboutmolecules.
Table8: Overallresults(%)ofMG.
ExactMatch FTS
Model Validity
Top1 Top3 Top5 MACCS RDK Morgan
MolT5 31.6 38.7 41.3 87.8 80.1 73.2 95.3
GPT-4(0-shot) 2.8 3.4 4.0 75.8 55.8 46.5 93.0
GPT-4(1-shot) 4.9 6.2 7.9 74.0 52.9 42.8 81.8
GPT-4(3-shot) 5.9 8.2 8.9 74.8 53.5 43.3 85.2
GPT-4(5-shot) 4.0 5.9 7.1 73.6 52.8 43.1 85.2
Galactica 0.0 0.0 0.0 22.7 11.8 11.6 94.7
Llama2 0.0 0.0 0.0 18.3 11.8 4.8 93.6
CodeLlama 0.0 0.0 0.0 26.5 15.1 8.5 95.2
Mistral 0.0 0.1 0.1 32.2 18.4 9.0 35.9
Molinst(instruction-tuned) 6.0 11.6 13.4 69.5 53.5 43.6 84.8
LlaSMol 7.8 13.6 17.2 78.6 61.0 51.0 99.8
Galactica
LlaSMol 4.8 9.0 10.6 71.7 52.7 44.2 99.9
Llama2
LlaSMol 6.5 11.8 14.2 74.0 55.9 46.6 99.8
CodeLlama
LlaSMol 19.2 29.2 33.7 84.1 70.3 61.7 99.8
Mistral
FortheMCtask(Table9),LlaSMol isthebestperformingLLMonallmetrics. ItisstilloutperformedbytheSoTA
Mistral
task-specificmodel(MolT5),however,approachingclosetoit. Pleasenotethatthesetext-basedmetricsonlymeasuresthe,
18LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
anditdoesnotnecessarymeanthecorrectnessofthedescriptiononchemistrydimension. Thatistosay,thesemetricsare
onlyareferenceforhowthedescriptionlookslikethegoldone,insteadofaprecisemeasureofcorrectness. Limitedbythe
currentresources,wecannotobtainthecorrectnessmeasures,andwillleavethistoourfuturework.
Table9: OverallresultsofMC.
Model BLEU-2 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR
MolT5 0.461 0.366 0.563 0.398 0.500 0.515
GPT-4(0-shot) 0.107 0.027 0.252 0.067 0.167 0.193
GPT-4(1-shot) 0.166 0.061 0.296 0.099 0.211 0.206
GPT-4(3-shot) 0.202 0.092 0.333 0.132 0.250 0.244
GPT-4(5-shot) 0.214 0.103 0.346 0.146 0.267 0.258
Llama2 0.110 0.047 0.251 0.107 0.210 0.150
CodeLlama 0.106 0.052 0.247 0.122 0.216 0.143
Mistral 0.146 0.068 0.281 0.118 0.232 0.193
Galactica 0.018 0.002 0.061 0.012 0.052 0.050
Molinst(instruction-tuned) 0.028 0.020 0.225 0.160 0.217 0.124
LlaSMol 0.324 0.232 0.461 0.290 0.405 0.394
Galactica
LlaSMol 0.292 0.203 0.440 0.270 0.388 0.366
Llama2
LlaSMol 0.322 0.226 0.441 0.273 0.390 0.366
CodeLlama
LlaSMol 0.414 0.319 0.521 0.357 0.463 0.452
Mistral
B.1.3.CHEMICALREACTION
FortheFStask(Table10),LlaSMol isthebestperformingLLMacrossallmetrics,althoughittieswithmanymethods
Mistral
onvalidity. Notably,alloftheLlaSMolmodelsperformmuchbetterthantheotherLLMs,whichindicatesthepowerof
fine-tuningonSMolInstructforunderstandingchemicalreactions. TheSoTAtask-specificmethodsstilloutperformall
oftheLLMs,buttheLlaSMolseriesismuchcloserthantheotherLLMs.
WeobserveasimilartrendfortheRStask(Table11). Again,LlaSMol isthebestperformingLLMacrossallmetrics,
Mistral
althoughitdoestiewithLlaSMol forvalidity. WeobservetheLLMswithoutinstructiontuningfailtoachieveany
Llama2
accuracygreaterthan2%onthistask. ThisindicatesthatinstructiontuningcanbeusefulforLLMstolearnretrosynthesis.
TheSoTAtask-specificmethodsstilloutperformalloftheLLMs,whichindicatesthatthereisstillroomforimprovement
forLLMsonRS.
B.1.4.PROPERTYPREDICTION
Forthetworegressiontasksinmolecularpropertyprediction(Table12),LlaSMol achievesthehighestperformance
Mistral
amongLLMsinbothtasks. Notably,LlaSMol outperformsthebestLLMsGPT-4withalargemargin,whileallthe
Mistral
otherLlaSMolmodelsalsooutperformitonLipotask. Thisindicatesthepoweroffine-tuningonSMolInstructfor
understandingpropertiesofmolecules. However,thetask-specificmethod(Uni-Mol)stilloutperformsalloftheLLMs.
Forthefourclassificationtasksinmolecularpropertyprediction(Table13),LlaSMol achievesthehighestaccuracy
Mistral
valuesamongLLMsonallthetasks. Particularly,onPP-SIDERtask,LlaSMol outperformsalltheLLMsandthe
Mistral
task-specificmodelUni-Molonallthethreemetrics,whichhighlightsthepotentialofLLMsinunderstandingmolecules
andpredictingtheirproperties. However,LlaSMol achievesverylowF1valuesonPP-ClinToxandPP-HIV.This
Mistral
suggeststhatLlaSMol canstrugglewithdataimbalanceissueinthesetwotasks,andachievehighaccuracyvaluesby
Mistral
predictingmostsamplesasnegative. Similarly,mostotherLLMsalsoachieveeitherpooraccuracyvalues(e.g.,6.3%and
4.4%forMolinstonPP-ClinToxandPP-HIV)orpoorF1values(e.g.,0.0%and0.0%forGalactica). Therefore,thereisstill
roomforfurtherimprovementoftherobustnessofLLMswhendealingwithimbalanceddatasets.
19LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
Table10: Overallresults(%)ofFS.
ExactMatch FTS
Model Validity
Top1 Top3 Top5 MACCS RDK Morgan
MolecularTransformer 78.5 85.5 87.1 95.5 93.0 91.4 99.5
RSMILES 78.7 87.9 89.7 95.7 93.7 92.2 100.0
GPT-4(0-shot) 2.0 2.4 2.6 60.7 49.9 41.1 88.2
GPT-4(1-shot) 1.1 2.2 2.6 61.4 49.6 41.2 90.8
GPT-4(3-shot) 0.2 2.2 2.6 62.2 51.5 42.9 91.8
GPT-4(5-shot) 1.3 2.0 3.0 63.4 52.2 44.3 93.6
Galactica 0.0 0.0 0.0 40.2 33.2 25.8 84.2
Llama2 0.0 0.0 0.0 33.5 24.4 13.7 97.7
CodeLlama 0.0 0.0 0.0 35.4 26.5 15.8 99.6
Mistral 0.0 0.0 0.0 39.0 31.0 19.8 96.0
Molinst(instruction-tuned) 2.1 3.3 3.7 51.1 36.7 31.6 99.8
LlaSMol 52.7 53.9 70.6 88.6 82.3 79.7 99.8
Galactica
LlaSMol 44.2 58.4 62.5 85.8 78.4 75.3 99.7
Llama2
LlaSMol 52.3 65.6 69.3 88.4 82.1 79.4 99.8
CodeLlama
LlaSMol 63.5 75.6 79.1 91.8 87.2 85.0 99.8
Mistral
Table11: Overallresults(%)ofRS.
ExactMatch FTS
Model Validity
Top1 Top3 Top5 MACCS RDK Morgan
MolecularTransformer 47.0 61.7 66.4 87.0 81.5 77.5 99.7
RSMILES 46.2 63.9 69.9 86.5 80.9 76.7 100.0
GPT-4(0-shot) 0.0 0.2 0.4 60.9 36.8 35.2 48.8
GPT-4(1-shot) 0.3 0.8 1.4 66.6 42.5 40.9 79.6
GPT-4(3-shot) 0.5 1.2 1.6 68.2 45.6 42.2 87.8
GPT-4(5-shot) 0.2 0.8 1.2 68.3 46.0 43.1 84.4
Galactica 0.0 0.0 0.0 48.9 38.3 34.6 93.0
Llama2 0.0 0.0 0.0 46.5 35.0 27.5 87.7
CodeLlama 0.0 0.1 0.0 44.7 32.1 25.3 97.1
Mistral 0.0 0.0 0.0 44.6 32.0 24.2 98.0
Molinst(instruction-tuned) 5.7 8.3 9.5 69.6 53.7 48.0 97.8
LlaSMol 25.3 39.4 45.4 80.9 71.9 67.0 99.8
Galactica
LlaSMol 22.4 35.5 41.1 79.8 70.4 65.2 99.9
Llama2
LlaSMol 25.7 40.0 45.7 80.7 71.7 66.7 100.0
CodeLlama
LlaSMol 32.9 49.6 55.4 83.0 75.1 70.4 100.0
Mistral
20LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
Table12: Overallresults(RMSE)ofPP-ESOLandPP-Lipo.
Model ESOL↓ Lipo↓
Uni-Mol 0.819 0.612
GPT-4(0-shot) 2.032 1.562
GPT-4(1-shot) 2.268 1.625
GPT-4(3-shot) 2.027 1.777
GPT-4(5-shot) 1.689 1.592
Galactica 4.184 2.979
Llama2 3.287 1.634
CodeLlama 3.483 1.733
Mistral 3.079 1.730
Molinst(instruction-tuned) 4.304 2.800
LlaSMol 2.206 1.174
Galactica
LlaSMol 2.721 1.282
Llama2
LlaSMol 2.959 1.203
CodeLlama
LlaSMol 1.150 1.010
Mistral
Table13: OverallresultsofPP-BBBP,PP-ClinTox,PP-HIV,andPP-SIDER.
PP-BBBP PP-ClinTox PP-HIV PP-SIDER
Model
F1 MCC Acc F1 MCC Acc F1 MCC Acc F1 MCC Acc
Uni-Mol 89.5 0.651 85.3 42.1 0.381 92.4 32.8 0.361 97.0 76.5 0.366 70.0
GPT-4(0-shot) 69.8 0.300 64.0 16.5 0.125 36.8 9.2 0.056 56.6 59.9 0.169 56.6
GPT-4(1-shot) 70.5 0.378 66.0 15.7 0.131 25.7 9.6 0.087 39.4 45.0 −0.084 43.2
GPT-4(3-shot) 63.5 0.344 60.9 17.9 0.175 36.1 8.5 0.040 48.0 40.2 −0.143 39.8
GPT-4(5-shot) 59.1 0.319 57.9 11.1 −0.047 33.3 9.1 0.052 55.8 55.4 0.022 50.4
Galactica 81.7 0.000 69.0 0.0 −0.023 92.4 0.0 0.000 96.7 70.9 0.364 68.1
Llama2 72.0 −0.043 58.9 15.2 0.070 45.1 1.4 −0.020 93.3 71.2 0.177 61.9
CodeLlama 70.1 0.043 58.9 0.0 −0.079 85.4 4.5 0.005 91.8 69.9 0.140 60.2
Mistral 33.9 0.046 40.6 12.9 −0.003 15.3 6.2 −0.016 7.1 36.0 −0.202 38.1
Molinst(instruction-tuned) 86.0 0.000 60.9 15.9 0.000 6.3 6.0 −0.001 4.4 75.3 0.043 52.4
LlaSMol 81.7 0.000 69.0 0.0 0.000 93.1 0.0 0.000 96.7 79.9 0.418 67.9
Galactica
LlaSMol 81.3 −0.048 68.5 0.0 0.000 93.1 2.9 0.120 96.7 78.2 0.360 65.7
Llama2
LlaSMol 81.6 0.042 69.0 0.0 0.000 93.1 0.0 0.000 96.7 79.1 0.409 69.9
CodeLlama
LlaSMol 83.7 0.340 74.6 0.0 0.000 93.1 4.3 0.111 96.7 79.9 0.429 70.7
Mistral
21LlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
80 Ll7 a4 S. M60 ol LlaSM79 o.6 l0 Large LlaS7 M8. o3 l0 Plus 45 00 Ll4 a1 S. M80 ol LlaSM49 o.8 l0 Large LlaS4 M5. o1 l0 Plus 89 50 Ll8 a6 S. M70 ol LlaSM90 o.3 l0 Large LlaS8 M8. o7 l0 Plus
70
LlaS6 M8 o.5 l0 FFN 30 LlaS3 M1 o.4 l0 FFN 80 LlaS8 M1 o.7 l0 FFN
60 20 14.40 76.90
54.20 LlaSMolAttn 75 LlaSMolAttn
LlaSMolAttn Llama27B 10 Llama27B Llama27B
51.80 Llama213B 6.00 Llama213B 72.10 Llama213B
50 LlaSMolLite LlaSMolLite 70 LlaSMolLite
0
0 50 100 150 0 50 100 150 0 50 100 150
TrainableParameterSize(M) TrainableParameterSize(M) TrainableParameterSize(M)
(a)NC-I2Ftask. (b)NC-I2Stask. (c)NC-S2Ftask.
15 Ll1 a0 S. M30 ol LlaSM14 o.2 l0 Large LlaS1 M4. o0 l0 Plus 4LlaSM4. o1 l8 Attn L Ll la am ma a2 27 1B 3B 1.30LlaSM1.3 o0 lLite Lla1 S.2 M8 ol L Ll la am ma a2 27 1B 3B
10 1.29
3.05 LlaSMolPlus LlaSM6. o6 l0 FFN 3 LlaSMolFFN 1.25 LlaSM1. o2 l7 FFN
5
1.80 2.72 2.61
0 0L .l 5a 0SMolAttn L Ll la am ma a2 27 1B 3B 2LlaSM3.6 o5 lLitL elaSMo Ll laSM2. o0 l5 Large LlaSMolPlus 1.20 LlaSM1. o2 l1 Attn LlaSM1. o1 l9 Large
LlaSMolLite
1.15
0 50 100 150 0 50 100 150 0 50 100 150
TrainableParameterSize(M) TrainableParameterSize(M) TrainableParameterSize(M)
(d)NC-S2Itask. (e)PP-ESOLtask. (f)PP-Lipotask.
70 Ll6 a8 S. M50 ol LlaS7 M0 o.1 l0 FFN 93.5LlaS9 M3 o.1 l0 Attn Ll9 a3 S. M10 ol L Ll la am ma a2 27 1B 3B 97.0 LlaS9 M6 o.7 l0 A 9t 6t .n 70 96.70
LlaSM67 o.5 l0 Large LlaS6 M9. o0 l0 Plus 93.0 LlaS9 M3 o.1 l0 FFN LlaS9 M3. o1 l0 Plus LlaSMol LlaSMolLarge 96.70
65 LlaS6 M3 o.5 l0 Attn
Llama27B
99 22 .. 05 LlaS9 M3. o1 l0 Lite 96.5 LlaS9 M6L . ol 6a l0S L9 M i6 to. e7 l0 FFN LL lala mS aM 2ol 7P Blus
LlaS6 M1. o9 l0 Lite Llama213B 91.5 LlaSM91 o.7 l0 Large Llama213B
60 96.0
0 50 100 150 0 50 100 150 0 50 100 150
TrainableParameterSize(M) TrainableParameterSize(M) TrainableParameterSize(M)
(g)PP-BBBPtask. (h)PP-Clintoxtask. (i)PP-HIVtask.
66 78 65.70 67.30 0.40 Lla0 S.3 M7 ol LlaSM0. o3 l9 Large 0.39 50 Ll4 a4 S. M23 ol LlaSM49 o.3 l9 Large 46.44
66 56 LlaSMo 6l 4.80LlaSM65 o.8 l0 Large LlaSMolPlus 0.35 LlaSM0. o3 l5 FFN LlaSMolPlus 40 LlaS4 M0 o.6 l8 FFN LlaSMolPlus
66 34 6L 2la .L 5S 0l 6 Ma 3S o.M 9 l0 Aol ttF nFN
Llama27B
0.30 L 0la .2S 7M0. o3 l0 Attn
Llama27B
30 L 2l 4a .S 33 M 52 o.4 l6 Attn
Llama27B
62 LlaSMolLite Llama213B LlaSMolLite Llama213B LlaSMolLite Llama213B
0.25 20
0 50 100 150 0 50 100 150 0 50 100 150
TrainableParameterSize(M) TrainableParameterSize(M) TrainableParameterSize(M)
(j)PP-SIDERtask. (k)MCtask. (l)MGtask.
50 Ll4 a4 S. M20 ol LlaSM50 o.9 l0 Large 48.60 25 Ll2 a2 S. M40 ol LlaSM25 o.6 l0 Large 24.60
LlaSMolPlus LlaSMolPlus
40 20 38.00 18.80
LlaSMolFFN LlaSMolFFN
30 LlaS3 M0 o.3 l0 Attn 15 14.40
LlaSMolAttn
Llama27B Llama27B
20 LlaS2 M1. o5 l0 Lite Llama213B 10 LlaS1 M0. o5 l0 Lite Llama213B
0 50 100 150 0 50 100 150
TrainableParameterSize(M) TrainableParameterSize(M)
(m)FStask. (n)RStask.
Figure4: PerformanceondifferenttasksunderdifferentLoRAsettingsandbasemodels. ExceptforPP-ESOLandPP-Lipo,
thelargerthebetter.
22
)%(ME
)%(ME
)%(ccA
)%(ccA
)%(ME
)%(ME
ESMR
)%(ccA
ROETEM
)%(ME
)%(ME
ESMR
)%(ccA
)%(MELlaSMol:AdvancingLLMsforChemistrywithSMolInstruct
B.2.InfluenceofLoRAModulesandTrainableParameters
Figure4presentsthemodelperformanceonallthe14tasksunderdifferentLoRAsettingsandbasemodels,asdescribedin
Section5.4. AsshowninFigure4,oneofthekeyobservationsisthatincorporatingmoreLoRAmodulesduringtraining
canconsistentlyenhancetheperformanceofLlaSMolonmosttasks. Inaddition,bycomparingLlaSMolandLlaSMol
Large,anotherkeyobservationwecantellisthatLlaSMolLargewithlargerbasemodelconsistentlyoutperformsallthe
LlaSMolmodelsunderdifferentLoRAsettingsonalmostallthetasks,exceptfourclassificationtasksformolecularproperty
prediction. TheseobservationsareconsistentwiththosedescribedinSection5.4.
C.MoreStatisticsofSMolInstruct
0.25
0.0010 0.04      
0.20
0.0008
0.03
      0.15
0.0006
0.02
0.0004 0.10
     
0.0002 0.01 0.05
0.0000 0.00       0.00
0 1000 2000 3000 0 20 40 60 80                 0 2 4 6 8 10
(a)Bertzcomplexity. (b)Atomcount. (c)Molecularweight. (d)Ringcount.
Figure5: ThestatisticsofmoleculesinSMolInstruct,withthelongtailpartsremovedforaclearpresentation.
ToknowmoreabouttheproposedSMolInstructdataset,wedoastatisticsonthemolecules(representedinSMILES).
Altogether, itcontains1.6Mdistinctmolecules, andseveralimportantstatisticalvaluesareshowninresultsareshown
Figure5. Specifically,Bertzcomplexityisatopologicalindexthatmeasuresthecomplexityofmoleculesbasedonthe
number and types of bonds and atoms. Atom count shows the number of atoms in a molecule, it represents the size
ofamolecule. Molecularweightisameasureofthesumoftheatomicweightsoftheatomsinamolecule. Andring
countshowsthenumberofringsinthemolecularstructures. Aswecansee,thevaluesvariesmuch,showingaextensive
coverage in terms of complexity, size, and structure. Notably, when compared to Mol-Instructions (Fang et al., 2023),
moleculesinSMolInstructshowalargercomplexityanddiversity,whichindicatesthattasksofSMolInstructcan
bemorecomprehensiveandcomplicatedthanthoseofMol-Instructions. Thescale,diversity,andcarefulconstructionof
SMolInstructmakesitwell-suitedforlearningchemistryLLMs.
23