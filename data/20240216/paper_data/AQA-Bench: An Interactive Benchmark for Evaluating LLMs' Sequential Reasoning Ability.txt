AQA-Bench: An Interactive Benchmark for Evaluating LLMs’ Sequential
Reasoning Ability
SiweiYang*1 BingchenZhao*2 CihangXie1
Abstract You are required to guess the
Picked random number: 7 random number which I have just
ThispaperintroducesAQA-Bench,anovelbench- picked between 0 and 20
marktoassessthesequentialreasoningcapabil- Instruction
ities of large language models (LLMs) in algo-
rithmiccontexts,suchasdepth-firstsearch(DFS). Evaluator Model
Thekeyfeatureofourevaluationbenchmarklies
in its interactive evaluation protocol — for ex-
Start 10
ample, in DFS, the availability of each node’s
connected edge is contingent upon the model’s
traversal to that node, thereby necessitating the The true number is smaller than 10 5
LLM’s ability to effectively remember visited
nodesandstrategizesubsequentmoves. Wecom- The true number is bigger than 5 8
prehensively build AQA-Bench with three dif-
ferent algorithms, namely binary search, depth-
The true number is smaller than 8 7
firstsearch,andbreadth-firstsearch,andtoeval-
uate the sequential reasoning ability of 12 dif-
ferent LLMs. Our investigations reveal several Right answer!
The true number is equal to 7
interesting findings: (1) Closed-source models
like GPT-4 and Gemini generally show strong
sequentialreasoningability,significantlyoutper- Figure1.AnillustrationofAQA-Bench’sevaluationprocess.Af-
formingopen-sourceLLMs. (2)Naivelyprovid- terreceivinginstructionregardingthetaskgoal,thetestedmodel
inginteractiveexamplesmayinadvertentlyhurt willinteractwiththeevaluatorintheformofaQ&Aconversation.
few-shotperformance. (3)Averylimitednum- Metricsmeasuringthemodel’scapabilityofachievingthetask
goalandfollowingtheintendedalgorithmarecalculatedbasedon
ber of predecessor steps following the optimal
theinteractionprocess.Notethisisnottheactualpromptwefed
policycansubstantiallyboostsmallmodels’per-
themodel;thefullpromptispresentedinAppendixD.
formance. (4) The scaling correlation between
performanceandmodelsizeisnotalwayssignifi-
cant,sometimesevenshowcasinganinversetrend. of research (Wei et al., 2022; Wang et al., 2022; Brown
Wehopeourstudycancatalyzefutureworkon etal.,2020;OpenAI,2023). Thereasoningcapabilitiesof
advancing the understanding and enhancement thesemodelshavetypicallybeenassessedthroughbench-
of LLMs’ capabilities in sequential reasoning. marksfocusingonarithmeticreasoning(Cobbeetal.,2021;
Thecodeisavailableathttps://github.com/UCSC- Lingetal.,2017),symbolicinference(Suzgunetal.,2022),
VLAA/AQA-Bench. knowledge(Hendrycksetal.,2020),andscienceunderstand-
ing(Hendrycksetal.,2021b). Thesebenchmarksrequire
LLMstoengageinmulti-stepreasoning,leveragingboth
1.Introduction
the context provided by the question and their internally
learnedworldknowledge(Weietal.,2022).
RecentadvancementsinLargeLanguageModels(LLMs)
haveledtoimpressivestridesinreasoningacrossadiverse Nevertheless,acriticallimitationoftheseexistingbench-
arrayoflinguistictasks, asevidencedbyagrowingbody marks is their reliance on one-off interactions, predomi-
nantlyintheformofmultiple-choicequestionsorsingle-
*Equalcontribution 1UCSantaCruz2UniversityofEdinburgh.
responsequeries.Whilethesemetricsoffervaluableinsights
Correspondenceto:CihangXie<cixie@ucsc.edu>.
intotheLLMs’reasoningabilities,theyfallshortinevaluat-
Preprintunderreview ingothercrucialaspectsofintelligence. Specifically,they
1
4202
beF
41
]LC.sc[
1v40490.2042:viXraAQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
donotassessthemodels’capacityforproceduraladherence 2.EvaluationEnvironments
andactivememorymaintenance, elementsvitalformore
2.1.BaseEnvironment
complex,sequentialreasoningtasks.
Wehereby introducethe designof threebasic interactive
Inthiswork,weaimtobridgethisevaluationgapinbench-
environments. Ineachenvironment,instructionsaboutthe
marks, thereby offering a better understanding and mea-
objectiveareinitiallyfedtothemodelviathesystemprompt,
suring the cognitive capabilities of LLMs in mimicking
whiletheinformationaboutthecurrentenvironmentisonly
human-like reasoning processes. To this end, we hereby
revealedtothemodelfollowingitsresponse. Ourdesign
developaninteractiveQ&Abenchmark,referredtoasAQA-
makessurethatthekeyinformationformakingdecisions
Bench,specificallydesignedtoquantitativelyassessLLMs’
canonlybegainedbyinteractingwiththeenvironmentso
proficiencyinexecutingpredefinedalgorithmicprocedures.
thatthemodelcanbeevaluatedbasedonhowitcanplan
Theseproceduresnecessitatebasicreasoningoverobserved
andexecutetheoptimalstrategy. Thus,thetestedmodelis
data,coupledwiththeupdatingofaninternalorexternal
forcedtoperformsequentialplanningbyactivelyexploring
statethatrepresentsaspecificdatastructure. Onesuchex-
the environment and adjusting its response according to
ampleissolvingamazeproblemusingthedepth-firstsearch
feedbackalternately.
algorithm—Ineachinteractiveinstance,themodelispro-
videdwithonlythenodeIDitoccupiesandtheedgescon- BaseEnv1: GuessNum. TheobjectiveoftheGuessNum
nectedtothatnode,representingtheobserveddata;based environmentisforthemodeltoaccuratelypredictanumber
onthiscurrentinformationanditsvisitinghistory,themodel predeterminedbytheevaluator. Duringeachinteraction,the
must then determine which edge to follow to progress to model interacts with the environment by guess a number
thesubsequentnode. Throughthisinteractivedesign,our andreceivesfeedbackindicatingwhetheritsguessednum-
AQA-BenchcaneffectivelygaugetheLLMs’capabilities berishigherorlowerthanthepredeterminednumber. The
inalgorithmicreasoning optimalstrategyinthisscenarioinvolvesthemodelimple-
menting a binary search. Consequently, the performance
WeempiricallybuildAQA-Benchutilizingthreealgorithms:
in this environment serves as an indicator of the model’s
(1) Binary search, wherein the model’s task is to deduce
understandingofthebinarysearchalgorithm.
anumberwithinaspecifiedrange,ideallyemployingthe
binarysearchalgorithm.(2)Depth-firstsearch(DFS),where BaseEnv2: DFS.Inthisenvironment,themodelistasked
themodelnavigatesagraphwiththeobjectiveofmapping withnavigatingagraphusingtheDFSalgorithm. Initially,
allnodesandedges. (3)Breadth-firstsearch(BFS),similar themodelispresentedwithinformationaboutitscurrent
to DFS, but with an explicit requirement for the model node and the edges connected to that node. The model
to apply the BFS algorithm instead. The corresponding interactswiththeenvironmentsbydecidewhichedgeitwill
evaluationsrevealfourinterestingfindings: follow,andthentheenvironmentwillupdatethemodelwith
theinformationofthenewlyreachednodeanditsassociated
• The closed-source models like GPT-4 and Gemini
edges. Themodel’sperformanceisevaluatedbasedonits
stronglydominateallopen-sourceLLMsonsequential
adherencetotheDFSpolicy. Criticaltotheevaluationis
reasoning.
theabilitytocomprehendandimplementtheconceptofa
• Naivelyprovidinginteractiveexamplesmayinadver- first-in-last-outstack,alongwithmaintainingamemoryof
tently hurt few-shot performance. This trend is ob- previouslyvisitednodes. TheprocessoftheDFSalgorithm
servedevenwiththeadvancedGPT-4andGemini-Pro isdescribedintheinstructionstoreducedifficulty.
incertainAQA-Benchenvironments.
Base Env 3: BFS. This environment closely mirrors the
DFSenvironmentinstructurebutdivergesinitscorealgo-
• Givenafewpredecessorstepsundertheoptimalpolicy,
rithmicrequirement, instructingthemodeltoemploythe
theperformanceofsmallmodelscanbesignificantly
BFSalgorithmforgraphnavigation.Thiskeydistinctionen-
improved,sometimesevencomparabletolargemodels.
ablestheBFSenvironmenttospecificallyassessthemodel’s
• The scaling correlation between performance and comprehensionofthefirst-in-first-outqueueprinciple,afun-
modelsizeisnotalwayssignificant,sometimeseven damentalaspectofBFS.
showcasing an inverse trend. This contradicts com-
2.2.EmbodiedEnvironment
monassertionsinLLMdevelopmentandpointstoan
oversightofsequentialreasoningcapabilitiesincurrent Weadditionallydesignembodiedenvironmentswherethe
LLMresearch. informationabouteachbaseenvironmentisreplacedwith
more real-life background descriptions. These embodied
WehopeourAQA-Benchcanserveasausefulbenchmark
environmentscanthenbeusedtoassessifthemodelcan
forfutureresearchfocusedonevaluatingandenhancingthe
performsequentialreasoningwithirrelevantinformation,
sequentialreasoningabilitiesofLLMs.
2AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
and if the model can abstract algorithmic problems from Asforthepolicymetric,weaccumulatetheerrorbetween
real-lifesituationsandfindtheoptimalalgorithms. eachguessandthetargetnumber,anddefinethemetricas:
EmbodiedEnv1: Coin(GuessNum). Thetestedmodel
isrequiredtoplayaheroencounteringawitchguardinga Err sum
=(cid:88) H|g −i− Lg +ˆ|
1. (2)
chestofgoldcoinsinahiddentemple. Toclaimtheprize, i
the model needs to guess the number of gold coins with
limitedchances. Given the similar objectives of the DFS (CaveDFS) and
BFS(CaveBFS)environments,weemployaconsistentmet-
EmbodiedEnv2:CaveDFS(DFS).Ratherthannavigating
ric to evaluate performance in both. The primary goal in
agraph,themodelisrequiredtoplayasanexplorertovisit
theseenvironmentsistoachievefullgraphtraversal. Ac-
all the caves in an underground cave system in as fewest
cordingly, we define the goal metric, denoted as G , to
stepspossible. UnliketheDFSenvironment,themodelis min
measuretheextentofnodecoverageinrelationtothetotal
notexplicitlyrequiredtouseanyalgorithmbuttheobjective
number of nodes in the graph. Let M represent the total
naturallydemandstheDFSalgorithm.
number of nodes in the graph, and ⟨a⟩ denote the set of
i
EmbodiedEnv3: CaveBFS(BFS).SimilartoCaveDFS nodesvisitedbythemodeluptothei-thinteractionstep.
environment, this environment requests the tested model Thegoalmetricisthenformulatedasfollows:
to traverse an underground cave system as well but as a
group. Thegroupcansplitintosmallergroupstovisitadja-
G
=1−max|⟨a⟩ i|
=1−
|⟨a⟩ −1|
, (3)
centcaveswithoutbacktracing. Thisenvironmentdoesn’t min i M M
explicitlycallforaspecificalgorithmaswell.
where|⟨a⟩ |isthenumberofuniquenodesvisitedbythe
−1
modelbytheendoftheinteraction.
3.Evaluation
Similarly,wedefinethepolicymetric,G ,asthecumula-
sum
3.1.Metrics tivegapingraphcoveragethroughouttheinteraction:
Toholisticallyassessperformanceineachenvironment,we
design two specific metrics. The first is the goal metric, G
=(cid:88)
1−
|⟨a⟩ i|
(4)
sum M
whichevaluateshowcloseisthemodel’sfinaloutputtothe i
groundtruth; thesecondisthepolicymetric, whichmea-
surestheefficiencyofthemodel’spolicy. Forthegoalmet- Furthermore, we introduce the ratio between the number
ric,weadoptanerror-basedapproachwherelowerscores of steps K that the model follows the algorithm and
follow
arepreferable. Thisdesignchoiceenablesthegoalmetric thetotalnumberofstepsthemodeltakesN asametricto
ateachintermediatestepcanbeaccumulatedasthepolicy accesstheefficiencyofmodels’policy:
metrictomeasurehowfastthemodel’soutputconvergesto
thefinalobjective. ACC= K follow (5)
N
Note that we typically prioritize the goal metric over the
policymetricwhencomparingtheperformanceoftwomod- whereK representsthenumberofstepsinwhichthe
follow
els. Thishierarchyinmetricevaluationiscrucialduetothe model follows the algorithm until its first deviation, and
observedtendencyoflower-performingmodelstoprema- K isthetotalnumberofstepsinwhichthemodeladheres
total
turelyexittheevaluationprocess. Suchearlyterminationis tothealgorithmthroughouttheinteraction.
typicallyaresultofgeneratinginvalidresponses,therefore
leadingtoalowergoalmetricscorebutsometimesahigher 3.2.In-contextExamples
policymetricscore.
Wei et al. (2022) argue LLM’s strong reasoning abilities
GuessNum(Coin)requiresthemodeltoaccuratelyguess
are,inpart,attributabletotheirin-contextlearningabilities.
thenumberspecifiedbytheevaluator. Forthegoalmetricin
Builtuponthisinsight,wealsoincorporateitintothedesign
thisenvironment,weusetheminimalerroroftheresponses
of our benchmark. In Fig. 2, we outline our protocol for
fromthemodeltothetargetnumber,whichisdefinedas
testingin-contextexampleswithinourbenchmark. Specifi-
cally,itinvolvesintegratingaseriesofinteractionexamples
|g −gˆ|
Err =max i , (1) between the optimal teacher model and the environment
min i H −L+1
intothemodel’scontext. Thesein-contextexamplesareex-
where g is the guess model produced in the i-th step of pectedtoserveasafoundationalreference,aidingthemodel
i
interaction,gˆisthetargetnumber,andH andLdenotethe incomprehendingtheexpectedinteractiondynamicsand
upperandlowerboundoftheguessingrange. decision-makingprocessesineachspecificenvironment.
3AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
WespecificallydesignedametricnamedPer-stepACCfor
System: You are required to guess a number between 0 and 8.
Q: Start thismode. Atthek-thstep,thePer-stepACCis
A: 4
Q: The true number is smaller than 4. N
A: 2 PSACC = k (6)
Q: The true number is bigger than 2. k Nˆ
k
A: 3
Q: Right answer. The true number is equal to 3. ICE = 2 whereN isthenumberoftestcasesinwhichthemodel
A: k
Q: Start followsthealgorithmatthek-thstep,andNˆ k isthenumber
A: 4 oftestcasesofwhichtheoptimalpolicytakesatleastksteps.
Q: The true number is bigger than 4. Thus,PSACCk canberoughlyviewedastheprobabilityof
A: 6
Q: Right answer. The true number is equal to 6. themodelfollowingthealgorithmatthek-thstepgiventhat
A:
thealgorithmisfollowedinallthepredecessorsteps.
Q: Start
A: 4 Actual TheaveragedPSACCacrossallk stepsintheoptimal
Q: The true number is bigger than 4. Interaction max
A: ... policyisusedtoevaluatemodels’overallself-guidingability
Figure2. In-contextexampletests,theinteractionsoftheoptimal (cid:80) PSACC
policy and the environment on the other test cases are used as PSACC avg = k k k (7)
examples,whichcanprovideadditionalcontextualinformation max
aboutthealgorithmforin-contextlearning.
4.Experiments
You are required to guess a number between 0 and 1024
Q: Start Weevaluatemodelsonallbaseandembodiedenvironments.
A: 5 FortheGuessNumandCoinenvironment,wesetthetarget
Q: The true number is bigger than 5
numberbetween32and32800,FortheDFSandCaveDFS
A: 20
environment,wesetthenumberofgraphnodesto8.Forthe
Q: The true number is bigger than 20
BFSandCaveBFSenvironment,wesetthenumberofgraph
A: ...
nodesto15. Theworst-caseruntimeoftheoptimalpolicy
(a) interactionsw/oteacher-guiding
forallenvironmentsisabout15stepssowerunevaluation
with the maximum number of interactions being 20. In
System: You are required to guess a number between 0 and 8.
addition to this EASY mode, we also develop a HARD
QQ:: SSttaarrtt
AA:: 55 T: 4
modewithatargetrangeof32−3.3∗107forGuessNum
Q: The true number is smaller than 4. and Coin, 13 nodes for DFS and CaveDFS, 25 nodes for
A: 1 T: 2
BFSandCaveBFS.Theoptimalworst-caseruntimeisabout
Q: The true number is bigger than 2.
A: … T: ... 25stepsandthemaximumnumberofinteractionstepsis
30. WereportresultsundertheEASYmodebydefault.
(b) interactionsw/teacher-guiding
Foreasiercomparison,wedividedmodelsinto4categories
Figure3. Whenteacher-guidingisenabled,theresponsesarere-
accordingtothenumberofparameters:
placedwiththeoptimalonestoensurenoerrorwillaccumulate.
• Small models with < 10B parameters: Llama-7B-
3.3.TeacherGuiding
chat (Touvron et al., 2023), Vicuna-7B-v1.5-16K
Directlyevaluatingthemodelontheinteractionwillleadto (Chiangetal.,2023),Mistral-7B-Instruct-v0.2(Jiang
erroraccumulation. Sucherrorscanresultincatastrophic etal.,2023),DeepSeek-LLM-7B(Bietal.,2024)and
failure,evenwithstrongmodels,duetothedependencyof DeepSeek-MoE-16B(Daietal.,2024).
eachsteponitspredecessors. However,itisalsointeresting
• Mediummodelswith≥10Band<50Bparameters:
tocheckwhethercorrectinteractionstepsmayalsoimprove
Llama2-13B-chat,Vicuna-13B-v1.5-16KandMixtral-
models’generation. Toinvestigatethisissue,weimplement
8x7B-Instruct-v0.1(Jiangetal.,2024).
astrategytermedTeacher-Guiding. Thisapproachinvolves
• Largemodelswith≥50Bparameters: Llama2-70B-
usingtheintentedalgorithmastheoptimalpolicy,tailored
chatandDeepSeek-LLM-67B.
foreachenvironment,whichactsasateachermodel. The
teacher model amends the outputs of the subject model, • Closed-sourcemodels: GPT-3.5-Turbo,GPT-4-Turbo
ensuringthatanyincorrectdecisionmadeatanintermediate (OpenAI,2023),andGemini-Pro(Teametal.,2023).
stepdoesnotadverselyimpactsubsequentinteractions. The
Formixture-of-expertsmodels(e.g.,DeepSeek-MoE-16B,
implementationofthisprocedureisillustratedinFig.31.
Mixtral-8x7B-Instruct-v0.1),weonlyconsiderthenumber
1NotethatFigs.2and3areonlyfordemonstration,theactual ofactivatedparametersduringinference.Allevaluationsare
promptfedtothemodelisintheAppendixD. runwithzero-shotandwithoutteacher-guidingbydefault.
4AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
4.1.Re-productivityandVariance
Table1.Inter-datasetvarianceofLlama2-7B-chatandVicuna-
Although test cases in our AQA-Bench can be generated 7B-v1.5-16Kinbaseenvironments. Theseresultsaresumma-
dynamically,wepre-generatedatestsetwith400testcases rizedfromevaluationson4independentlygeneratedtestsets.
foreachbaseenvironmentundertheEASYmodeforsim- GuessNum DFS BFS
plerre-production. Thefinalscoresareaveragedamongtest Errmin↓ Errsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑
cases. GiventhatGuessNum,DFSandBFSeachcanhave Llama2-7B-chat
atmost32768,1.18∗106,9.17∗1016testcases,thequantity Avg 0.265 7.895 0.000 0.598 3.588 0.235 0.605 9.531 0.002
ofourpre-generatedtestcasesissomewhatmodest. Tover- M Ma ar rg gi in nm mi an x 0 0. .0 00 09 6 0 0. .1 18 65 8 0 0. .0 00 00 0 0 0. .0 01 27 0 0 0. .1 11 35 8 0 0. .0 01 16 0 0 0. .0 00 08 7 0 0. .2 27 64 9 0 0. .0 00 01 1
ifythatevaluationresultswiththisnumberoftestcasesare Vicuna-7B-v1.5-16K
validandrepresentativeofthemodels’performanceineach Avg 0.476 9.606 0.000 0.644 5.769 0.151 0.849 10.067 0.029
environment,wegeneratedanother3equallysizedtestsets Marginmin 0.017 0.366 0.000 0.009 0.196 0.007 0.010 0.640 0.001
Marginmax 0.016 0.341 0.000 0.006 0.181 0.012 0.006 0.324 0.001
andevaluatedLlama2-7B-ChatandVicuna-7B-v1.5-16K
on all 4 test sets. To quantify the variance of results, we Table2.Inter-datasetvarianceofLlama2-7B-chatandVicuna-
definethat 7B-v1.5-16Kinembodiedenvironments. Resultsaresumma-
(cid:80) rizedfromevaluationson4independentlygeneratedtestsets.
{m }
Avg= i (8) Coin CaveDFS CaveBFS
|{m }|
i Errmin↓ Errsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑
Margin =Avg−min({m }) (9)
min i Llama2-7B-chat
Margin max =max({m i})−Avg, (10) Avg 0.079 5.256 0.000 0.488 4.633 0.340 0.757 5.405 0.046
Marginmin 0.005 0.238 0.000 0.008 0.060 0.011 0.001 0.206 0.002
Marginmax 0.008 0.269 0.000 0.011 0.021 0.010 0.001 0.255 0.001
wheremin({m })isasetofthesamemetricfromdifferent
i
Vicuna-7B-v1.5-16K
evaluationruns. Margin andMargin canbeviewedas
min max
Avg 1.000 1.000 0.000 0.538 7.684 0.208 0.717 13.914 0.069
ameasurementforvarianceofevaluationresults.
Marginmin 0.000 0.000 0.000 0.015 0.389 0.007 0.007 0.403 0.004
Marginmax 0.000 0.000 0.000 0.013 0.361 0.006 0.009 0.473 0.003
As shown in Tabs. 1 and 2, Margin and Margin are
min max GPT-3.5-Turbo,butstillnotasgoodasGPT-4-Turboand
relativelylowcomparedtometricdifferenceacrossmodels,
Gemini-Pro. ItisparticularlyworthmentioningthatGPT-
which shows that evaluation results drawn from our pre-
4-Turboalmostachievesthetaskgoalinalltestcaseswith
generated test set (with only 400 cases) can sufficiently
a substantially low goal metric. These findings reveal a
representthetestedmodels’performanceinenvironment
significantgapinsequentialreasoningabilitiesbetweenthe
with this level of complexity. Therefore, we only report
open-sourcemodelsandtheclosed-sourcedGPT-3.5-Turbo,
resultsfromthefirsttestsetratherthanfromallfourtest
GPT-4-TurboandGemini-Promodels.
setsinthefollowingcontexttosavecomputation.
Next, among open-source models, one interesting obser-
Anotherfactorthatmayaffectourexperimentalconclusion
vationisthatmorerecentlyreleasedmodels(e.g.,Mistral,
is the randomness of the model itself. For open-source
Deepseek-LLM) are arguably better than relatively older
modelsandGemini-Pro,wedisabletherandomsampling
ones (e.g., Llama, Vicuna). For example, Mixtral-8x7B-
in all the experiments. But for GPT models, as they can
Instruct-v0.1, which is claimed to be better than Llama2-
onlybeaccessedviaOpenAIAPI,wecannotturnoffsuch
70B-chat,doesexcelLlama2-70B-chatinGuessNumand
modelrandomness. However,asshowninthesupplemen-
BFS but falls short in DFS. As for the DeepSeek-MoE-
taryTabs.8and9,thevarianceobservedinGPTmodelsis
16Bmodel,whichoutperformsLlama2-7B-chatonconven-
relativelyminor. FortheHARDmode,wepre-generated
tionallanguagebenchmarks(Bietal.,2024),underperforms
1500testcasesforeachenvironmentofwhichthevariance
Llama2-7B-chatacrossallthreetestedenvironments.
studycanbefoundinthesupplementaryTabs.10and11.
Lastly,inthemorechallengingHARDmode,GPT-4-Turbo
4.2.MainResults continuestodemonstrateitssuperiorperformance,signifi-
cantlyoutperformingallothermodelsintermsofcapabili-
Baseenvironments. Westartbyinvestigatingmodels’algo-
ties. ItisalsointerestingtonotethatMixtral-8x7B-Instruct-
rithmicsequentialreasoningabilitiesbyrunningevaluations
v0.1,whilestilllaggingbehindLlama2-70B-chatintheDFS
in three base environments: GuessNum, DFS, and BFS.
environment,surpasseditbyanevenmuchlargermargin
Theseevaluationswereconductednaively,withoutthein-
inboththeGuessNumandCoinenvironments. Complete
corporationofin-contextexamplesorteacherguidance. As
resultsundertheHARDmodewithall12modelscanbe
showninTab.3,closed-sourcemodelslikeGPTsandGem-
foundinthesupplementaryTabs.14and15.
inigenerallyexhibitmuchsuperiorperformancecompared
toalltestedopen-sourcemodels;Theonlyexceptionisthe Embodiedenvironments. Thefindingsfromtheembod-
DFSenvironment,whereopen-sourcemodelsoutperform iedenvironments,asdetailedinTab.4,largelymirrorthe
5AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
Table3.The evaluation results with 3 base environments. For Table5.Goalmetricsinall6environmentsunderHARDmode.
modelswithhighgoalmetrics(e.g.,Err min,G min)indictingweak Model GuessNum↓ DFS↓ BFS↓ Coin↓ CaveDFS↓ CaveBFS↓
performance,goalmetricsaremoreinformativethanpolicymetrics Small<10B
(e.g.,Err sum,G sum,ACC). Llama2-7B-chat 0.49 0.74 0.76 0.49 0.68 0.83
Model GuessNum DFS BFS V Mi ic su trn aa l- -7 7B B- -v In1 s.5 tr- u1 c6 tK -v02 0 0. .2 04 6 0 0. .7 68 5 0 0. .8 49 6 0 0. .4 09 8 0 0. .7 60 1 0 0. .8 43 9
Errmin↓ Errsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑ DeepSeek-LLM-7B 0.49 0.61 0.71 0.49 0.74 0.86
Small<10B DeepSeek-MoE-16B 1.00 0.78 0.92 1.00 0.86 0.94
Llama2-7B-chat 0.26 7.71 0.00 0.58 3.73 0.24 0.60 9.80 0.00
Vicuna-7B-v1.5-16K 0.46 9.24 0.00 0.65 5.79 0.15 0.84 10.29 0.03 10B≤Medium<50B
Mistral-7B-Instruct-v02 0.06 2.02 0.00 0.49 2.72 0.61 0.24 8.72 0.13
DeepSeek-LLM-7B 0.43 9.24 0.00 0.34 6.59 0.36 0.52 11.20 0.06 Llama2-13B-chat 0.49 0.59 0.76 0.08 0.56 0.68
DeepSeek-MoE-16B 1.00 1.00 0.00 0.63 4.78 0.07 0.88 8.18 0.02 Vicuna-13B-v1.5-16K 0.49 0.80 0.83 1.00 0.65 0.71
10B≤Medium<50B Mixtral-8x7B-Instruct-v01 0.00 0.64 0.32 0.07 0.50 0.30
Llama2-13B-chat 0.01 3.24 0.00 0.34 5.98 0.41 0.65 10.59 0.05
Vicuna-13B-v1.5-16K 0.39 8.31 0.00 0.66 13.23 0.12 0.81 15.61 0.05 Large≥50B
Mixtral-8x7B-Instruct-v01 0.00 0.69 0.00 0.47 3.32 0.57 0.14 7.36 0.21
Llama2-70B-chat 0.49 0.48 0.43 0.08 0.49 0.46
Large≥50B
DeepSeek-LLM-67B 0.00 0.51 0.67 0.02 0.39 0.56
Llama2-70B-chat 0.11 2.64 0.00 0.33 4.39 0.44 0.28 10.14 0.06
DeepSeek-LLM-67B 0.12 5.62 0.00 0.40 4.34 0.42 0.45 11.59 0.09 Closed-source
Closed-source
GPT-3.5-Turbo 0.00 0.51 0.01 0.35 5.21 0.61 0.11 6.68 0.52 GPT-3.5-Turbo 0.00 0.55 0.27 0.37 0.33 0.45
GPT-4-Turbo 0.00 0.50 0.46 0.03 3.93 0.94 0.00 6.08 0.38 GPT-4-Turbo 0.00 0.08 0.01 0.00 0.33 0.19
Gemini-Pro 0.00 0.63 0.00 0.25 3.71 0.76 0.06 7.39 0.17 Gemini-Pro 0.00 0.33 0.12 0.00 0.35 0.23
examples(ICE=7). Todelvedeeperintothisphenomenon,
Table4.Theevaluationresultsin3embodiedenvironments.For
modelswithhighgoalmetrics(e.g.,Err ,G )indictingweak weanalyzetheperformancevariationinrelationtothenum-
min min
performance,goalmetricsaremoreinformativethanpolicymetrics ber of in-context examples, as depicted in Fig. 5. Two
(e.g.,Err ,G ,ACC). interestingobservationsarenoted: 1)ForGPTmodels,in-
sum sum
Model Coin CaveDFS CaveBFS contextlearningbarelyhadanyimpactontheirperformance,
Errmin↓ Errsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑
eventhoughthereisstillroomforimprovementinembod-
Small<10B
Llama2-7B-chat 0.07 5.02 0.00 0.50 4.65 0.33 0.76 5.66 0.05 ied environments; and 2) An intriguing pattern emerged
Vicuna-7B-v1.5-16K 1.00 1.00 0.00 0.54 8.04 0.21 0.72 14.39 0.07
Mistral-7B-Instruct-v02 0.07 3.59 0.00 0.49 4.87 0.48 0.27 9.86 0.11 amongtheLlama2modelsintheCoinenvironment,where
DeepSeek-LLM-7B 0.39 8.82 0.00 0.58 9.08 0.16 0.77 10.67 0.04
DeepSeek-MoE-16B 1.00 1.00 0.00 0.71 2.99 0.11 0.89 2.81 0.01 their performance significantly dropped with just one in-
10B≤Medium<50B
Llama2-13B-chat 0.19 7.93 0.00 0.38 7.48 0.36 0.55 12.72 0.09 contextexample(ICE=1),butshowedgradualimprovement
Vicuna-13B-v1.5-16K 1.00 1.00 0.00 0.56 8.18 0.21 0.64 11.28 0.06
Mixtral-8x7B-Instruct-v01 0.00 0.78 0.00 0.32 4.61 0.45 0.15 8.48 0.17 asthenumberofexamplesincreased. Similartrendswere
Large≥50B observed in recent open-source models, such as Mistral-
Llama2-70B-chat 0.00 0.51 0.00 0.35 4.53 0.44 0.30 10.51 0.03
DeepSeek-LLM-67B 0.36 7.83 0.00 0.28 5.24 0.57 0.38 10.89 0.08 7B-Instruct-v02inBFS,DeepSeek-LLM-67BinCoinand
Closed-source
closed-sourceGemini-ProinBFS.Thiscontradictsthetypi-
GPT-3.5-Turbo 0.00 1.00 0.00 0.20 4.87 0.66 0.27 9.49 0.10
GPT-4-Turbo 0.00 0.50 0.50 0.23 3.62 0.74 0.12 8.07 0.16
Gemini-Pro 0.00 0.60 0.00 0.22 5.11 0.70 0.10 7.97 0.16 calassumptionthatin-contextlearninguniversallyenhances
conclusionsdrawnfromthebaseenvironments. Moreover, LLMs’performance. Wehypothesizethatthiscontradiction
asshowninFig.4,weinterestinglynotethatmodelstend may stem from the interactive and multi-round nature of
toperformworseinembodiedenvironments. Thisperfor- examplesinAQA-Bench, asopposedtothesingle-round
mance drop is expected, considering these embodied en- formattypicalinstandardQ&Abenchmarks. Thissuggests
vironmentsrequiremodelstoimplicitlyabstractfromthe thatmorestudiesabouthowmulti-roundexamplesforin-
environmentsanddecidetheoptimalalgorithmtoexecute. teractive tasks should be given to LLMs are required. It
isalsoworthnotingthatwithICE=7,Gemini-Proshowed
4.3.EffectofIn-ContextExamples comparableorevenbetterperformancethanGPT-4-Turbo
inallenvironmentsexceptCaveDFS.
Thissectionexplorestheimpactofintroducingin-context
examplesondifferentmodels. Theresults, asdetailedin Lastly,weinvestigatetheinfluenceofinstructionaldiffer-
the supplementary Tabs. 12 and 13, showcase that most encesbetweenthebaseenvironmentsandtheirembodied
modelsgetsignificantimprovementwhenprovidedwithin- variants,particularlyinrelationtotheincreasingnumberof
contextexamples. Forexample,initially,intheabsenceof in-contextexamples. AsillustratedinFig.5, weobserve
in-contextexamples(ICE=0),DeepSeek-MoE-7Bisoutper- anotabletrend: asthenumberofin-contextexamplesin-
formedbyLlama2-7B-chatacrossallsixenvironments;but creases,thedisparityingoalmetricsbetweenmostmodels
whenpresentedwithmorein-contextexamples,DeepSeek- acrossthesetwotypesofenvironmentstendstodiminish.
MoE-7Bnotonlybridgestheperformancegapbutactually Thissuggeststhatthemethodofinstructionandexample
surpassesLlama2-7B-chatineffectiveness. provisioncansubstantiallyreshapemodelbehaviors.
However,thebenefitofin-contextexamplesisnotuniver-
4.4.FailureofScalingLaw
sallyobservedacrossallmodels. Forinstance,theLlama2-
13B-chat model exhibits a decline in performance in the Zero-shotsetting.Fig.4alsoprovidesacomparisonamong
DFS environment when presented with seven in-context modelsfromthesamefamily. Aninterestingobservationis
6AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
Llama2-7b-chat Llama2-70B-chat Vicuna-13B-v1.5-16K Mixtral_8x7B-Instruct-v0.1 DeepSeek-LLM-7B
Llama2-13B-chat Vicuna-7B-v1.5-16K Mistral-7B-Instruct-v0.2 DeepSeek-MoE-16B DeepSeek-LLM-67B
GuessNum vs Coin DFS vs CaveDFS BFS vs CaveBFS
1.0 1.0 1.0
0.5 0.5 0.5
0.0 0.0 0.0
Figure4.Goalmetricsfromall6environments.Barsthatarefullyfilledrepresentresultsfrombaseenvironments(e.g.,GuessNum,DFS,
BFS)whilebarsfilledwithdiagonallinesarefromembodiedenvironments(e.g.,Coin,CaveDFS,CaveBFS).Modelsinthesamefamily
arerepresentedwiththesamehue,andlargermodelscorrespondtodarkercolors.ResultsfromGPTsandGemini-Proarenotshownin
thisfigureforfiguresbeingtoosmalltobevisible.Itshouldbenotedthatthesemetricsarethelowerthebetter.
Llama2-7B-chat Llama2-70B-chat Vicuna-13B DeepSeek-LLM-7B Mistral-7B-Instruct-v0.2 GPT-3.5-Turbo Gemini-Pro
Llama2-13B-chat Vicuna-7B DeepSeek-MoE-16B DeepSeek-LLM-67B Mixtral_8x7B-Instruct-v0.1 GPT-4-Turbo
GuessNum Coin
0.4
0.2
0.0
DFS CaveDFS
0.8
0.4
0.0
BFS CaveBFS
0.6
0.3
0.0
0 3 6 0 3 6 0 3 6 0 3 6 0 3 6 0 3 6
Number of in-context examples (ICE) Number of in-context examples (ICE)
Figure5. Goalmetricsfromall6environments.Itshouldbenotedthatthesemetricsarethelowerthebetter.
LLM-7Bsurpassesitslargercounterpart,DeepSeek-LLM-
Table6.Goalmetricsofresults(ICE=7)inall6environments.
67B, in the DFS environment. Similarly, Llama-7B-chat
Model GuessNum↓ DFS↓ BFS↓ Coin↓ CaveDFS↓CaveBFS↓
outperformsLlama-13B-chatintheCoinenvironment.
Small<10B
Llama2-7B-chat 0.08(-0.18) 0.39(-0.19) 0.65(+0.05) 0.11(+0.04) 0.38(-0.12) 0.58(-0.18) Few-shotsetting. Thedeviationfromthescalinglawbe-
Vicuna-7B-v1.5-16K 0.02(-0.44) 0.37(-0.28) 0.68(-0.16) 0.02(-0.98) 0.39(-0.15) 0.68(-0.04)
Mistral-7B-Instruct-v02 0.01(-0.05) 0.14(-0.35) 0.39(+0.15) 0.01(-0.06) 0.18(-0.31) 0.48(+0.21) comesevenmorepronouncedinthefew-shotsettings,as
DeepSeek-LLM-7B 0.04(-0.39) 0.16(-0.18) 0.61(+0.09) 0.04(-0.35) 0.19(-0.39) 0.62(-0.15)
DeepSeek-MoE-16B 0.02(-0.98) 0.14(-0.49) 0.86(-0.02) 0.02(-0.98) 0.13(-0.58) 0.87(-0.02) evidencedinFig.5. Inthesescenarios,mediumandlarge
10B≤Medium<50B modelsmorefrequentlyexperienceperformancedropscom-
Llama2-13B-chat 0.06(+0.05) 0.50(+0.16) 0.57(-0.08) 0.05(-0.14) 0.48(+0.10) 0.56(+0.01) paredtosmallermodels. Thispatternsuggeststhatwhile
Vicuna-13B-v1.5-16K 0.12(-0.27) 0.16(-0.50) 0.23(-0.58) 0.13(-0.87) 0.15(-0.41) 0.27(-0.37)
Mixtral-8x7B-Instruct-v01 0.00(-0.00) 0.20(-0.27) 0.48(+0.34) 0.00(-0.00) 0.17(-0.15) 0.39(+0.24) largermodelsareoftentoutedbydevelopersfortheirsupe-
Large≥50B riorperformanceacrossarangeofbenchmarks,theireffec-
Llama2-70B-chat 0.07(-0.04) 0.14(-0.19) 0.46(+0.18) 0.09(+0.09) 0.20(-0.15) 0.60(+0.30) tivenessmaynotuniformlyextendtospecializeddomains
DeepSeek-LLM-67B 0.00(-0.12) 0.18(-0.22) 0.36(-0.09) 0.00(-0.36) 0.18(-0.10) 0.39(+0.01)
such as algorithmic execution and interactive sequential
Closed-source
GPT-3.5-Turbo 0.00(-0.00) 0.36(+0.01) 0.12(+0.01) 0.02(+0.02) 0.19(-0.01) 0.27(-0.00) reasoning. Intheseareas,thechallengesaredistinctfrom
GPT-4-Turbo 0.00(-0.00) 0.02(-0.01) 0.00(-0.00) 0.00(-0.00) 0.23(-0.00) 0.11(-0.01)
thoseencounteredinconventionalone-roundQ&Aformats,
Gemini-Pro 0.00(+0.00) 0.02(-0.23) 0.03(-0.03) 0.00(+0.00) 0.04(-0.18) 0.05(-0.05)
indicatinganeedtoreconsiderthescalingassumptionsin
that,contrarytotheexpectedimprovementwithincreased LLMdevelopmentforthesespecificapplications.
modelsize—atrendtypicallyobservedinexistingLLM
benchmarks — the performance in tasks like GuessNum, 4.5.TeacherGuiding
DFS, and their embodied variants does not consistently
As evidenced in Fig. 6, even Llama2-7B-chat, which is
correlatewithlargermodelsizes. Notably,certainmodels
a small model, yielded higher PSACC as the number of
exhibitaninversescalingeffect. Forinstance,DeepSeek-
7
nimrrE
nimG
nimG
nimrrE nimG nimGAQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
5.RelatedWorks
Llama2-7B-chat Llama2-70b-chat GPT-4-Turbo
Llama2-13B-chat GPT-3.5-Turbo Gemini-Pro
GuessNum DFS BFS LargeLanguageModelsmodeltheprobabilityofgener-
1.0 atingasequenceoftexttokens. Itisshownthatwhenthe
0.5 numberofparametersandpretraineddatagoesup,theresult-
ingLLMsdemonstrateemergingbehaviors,i.e.,themodel
0.0
isabletoperformtasksitcannotwhenthecomplexityis
Coin CaveDFS CaveBFS
1.0 below a certain threshold. Although emerging behaviors
arestillunderdebate(Schaefferetal.,2023),thesuccess
0.5
of GPT models (OpenAI, 2023) has attracted a lot of in-
0.0 terestfrombothacademiaandindustry. Instruction-tuned
0 5 10 15 0 5 10 15 0 5 10 15
k-th Step models(Ouyangetal.,2022)areshowntobemorecapa-
Figure6. Per-stepACCinall6environments.
bleofzero-shotquestionansweringandreasoningtasks. It
hasbeendiscoveredthatusingcarefullydesignedprompts
like chain-of-thought (Wei et al., 2022) can substantially
Table7.PSACC in all 6 environments. All tests are run w/
avg enhance the performance of LLMs for multi-step reason-
teacher-guiding.
ing; there has been a line of works in this direction for
Model GuessNum↑ DFS↑ BFS↑ Coin↑ CaveDFS↑ CaveBFS↑ understandingandimprovingthepromptingforLLMs(Fu
Small<10B
etal.,2022;Zhouetal.,2022;Wangetal.,2022;Kojima
Llama2-7B-chat 0.06 0.71 0.50 0.04 0.83 0.54 etal.,2022). open-sourcemodelshavealsoemergedfrom
Vicuna-7B 0.04 0.80 0.57 0.03 0.82 0.61
Mistral-7B-Instruct-v02 0.09 0.77 0.74 0.09 0.75 0.74 communityefforts,basedontheweightsreleasedbyMeta
DeepSeek-LLM-7B 0.02 0.81 0.60 0.01 0.77 0.60
DeepSeek-MOE-16B 0.04 0.68 0.60 0.05 0.71 0.28 (Touvronetal.,2023).Instruction-tunedmodelshaveshown
10B≤Medium<50B theireffectiveness(Taorietal.,2023),withVicuna(Chiang
Llama2-13B-chat 0.21 0.74 0.69 0.21 0.79 0.63 etal.,2023)andMixtral(Jiangetal.,2024)demonstrating
Vicuna-13B 0.19 0.77 0.72 0.18 0.82 0.75
close-to-GPT-3.5performanceonhumanbenchmarks.
Mixtral-8x7B-Instruct-v01 0.44 0.79 0.86 0.45 0.85 0.81
Large≥50B Inourevaluation,wefoundthatdespitetheimpressivechat
Llama2-70B-chat 0.23 0.75 0.73 0.23 0.76 0.68 abilities,therestillexistsagapinthealgorithmicreasoning
DeepSeek-LLM-67B 0.35 0.87 0.69 0.41 0.91 0.68
abilities between open-source models and close-sourced
Closed-source
models,suggestingmoreeffortisneededtodevelopstrong
GPT-3.5-Turbo 0.68 0.86 0.92 0.67 0.89 0.77
GPT-4-Turbo 0.93 0.93 0.88 0.93 0.89 0.75 open-sourceLLMs.
Gemini-Pro 0.56 0.94 0.88 0.56 0.93 0.84
Benchmarkingreasoningabilities. Theperformanceof
generative models is notoriously difficult to evaluate. To
steps grows, indicating the probability of executing the resolvethisissue,Vicunaleveragesachatbotarenatolet
optimal policy improves over time, especially as correct human users evaluate the output of LLMs (Chiang et al.,
decisionsaccumulate,ratherthanerrors. Inenvironments 2023)inapairwisemodelcomparisonfashion. Otherthan
likeDFS(CaveDFS)andBFS(CaveBFS),wenotedthat evaluating human preference, benchmarks for evaluating
thedifferencesinPSACCamongvariousLlama2models thereasoningabilitiesofLLMsalsoexist. Forexamples,
diminish when more guidance steps are provided by the GSM8k(Cobbeetal.,2021)evaluatesthereasoningability
teachermodel. Whilelargermodelsstilltendtoexhibita ofLLMsusingasetofgradeschoolmathwordproblems;
higheraveragePSACC,asshowninTab.7,thegapnarrows MMLU (Hendrycks et al., 2020) further covers over 57
withincreasedteachermodelintervention. However,itis subjectsandusesmultiple-choicequestionstoevaluatethe
importanttonote,asdepictedinFig.6,thatPSACCmaybe- performance; MATH (Hendrycks et al., 2021b) proposes
gintodeclineinthelaterstagesofinteraction. Thisdecline toevaluatethetextgenerationinsteadofmultiple-choices
canbeattributedtotheescalatingcomplexityofadhering questions. Otherthantheseone-roundquestion-answering
totheoptimalpolicyasthemodelisrequiredtotrackand evaluations,multi-stepinteraction-basedevaluationbench-
rememberprevioussteps,suchas(implicitly)maintaininga marksalsoexist. Forexample,Panetal.(2023)leverages
queueofnodesinBFSandCaveBFS. adventuregamestomeasuretheabilityofLLMsinsocial
decision-makinginaninteractivemanner;Liuetal.(2023a)
These observations suggest that even a limited series of
usesmulti-turnopen-endedenvironmentstoevaluateLLMs.
correctstepscansignificantlyassistmodelsinsequential
Anothermainstreamofevaluationfocusesoncodegener-
reasoningtasks. Furthermore,formodelspossessingasuffi-
ation,includingbenchmarkslikeHumanEval(Chenetal.,
cientlevelofsequentialreasoningability,thisprocessmay
2021),MBPP(Austinetal.,2021),andAPPS(Hendrycks
leadtoaformofself-guidance,wherethemodelreinforces
etal.,2021a).
itsdecisionsbasedonpriorcorrectactions.
8
kCCASP
kCCASPAQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
Different from these existing works, our proposed AQA- nAI Researcher Access Program, and the Google Cloud
Benchformsseveralinteractiveenvironmentsthatfocuson Research Credits Program for supporting our computing
evaluatingtheLLMs’understandingofspecificalgorithm needs.
procedures.
LLM-as-Agent.ThestudyofLLMagentshasbeenahighly References
activeresearchareasincethedeputeofadvancedLLMslike
Austin,J.,Odena,A.,Nye,M.,Bosma,M.,Michalewski,
ChatGPT(OpenAI,2023). Promptingtechniquessuchas
H.,Dohan,D.,Jiang,E.,Cai,C.,Terry,M.,Le,Q.,etal.
Chain-of-though(Weietal.,2022)andReAct(Yaoetal.,
Program synthesis with large language models. arXiv
2023)elicitthereasoningandactingabilitiesofLLMsto
preprintarXiv:2108.07732,2021.
allowthemtoactasagents. Benchmarksdesignedbased
ongames(Fanetal.,2022;Gongetal.,2023)havebeen Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,
proposed to evaluate LLMs as agents to plan and act in Ding,H.,Dong,K.,Du,Q.,Fu,Z.,etal. Deepseekllm:
a complex environment. There also exist benchmarks to Scalingopen-sourcelanguagemodelswithlongtermism.
measuretheabilityofLLMsagentstoautomatecomplex arXivpreprintarXiv:2401.02954,2024.
real-worldtasks(Shenetal.,2023;Liuetal.,2023a;Deng
et al., 2023). Additionally, interaction between multiple Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
agentshasalsobeenexplored(Hongetal.,2023;Liuetal., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
2023b;Parketal.,2023). Askell,A.,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:
OurbenchmarkexploresLLM-as-Agentfromtheperspec-
1877–1901,2020.
tiveofwhethertheyarecapableoffollowingsimplealgo-
rithmprocedures, aimingtomeasuretheirsequentialrea- Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,Pinto,H.P.d.O.,
soningability. Kaplan,J.,Edwards,H.,Burda,Y.,Joseph,N.,Brockman,
G., etal. Evaluatinglargelanguagemodelstrainedon
code. arXivpreprintarXiv:2107.03374,2021.
6.Conclusion
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
Inthisstudy,weembarkonaninitialexplorationintothe
H.,Zheng,L.,Zhuang,S.,Zhuang,Y.,Gonzalez,J.E.,
realmofevaluatingLLMswithininteractiveenvironments.
Stoica, I., and Xing, E. P. Vicuna: An open-source
These environments necessitate a deep understanding of
chatbot impressing gpt-4 with 90%* chatgpt quality,
specificalgorithmicproceduresbytheLLMs,rangingfrom
March 2023. URL https://lmsys.org/blog/
efficientlyguessinganumberwithinminimalstepstostrate-
2023-03-30-vicuna/.
gicallysearchingforunvisitednodesinagraph. Ourcom-
prehensive evaluation reveals a notable performance gap
Cobbe,K.,Kosaraju,V.,Bavarian,M.,Chen,M.,Jun,H.,
between current open-source and closed-sourced models,
Kaiser,L.,Plappert,M.,Tworek,J.,Hilton,J.,Nakano,
withthelattershowingsuperiorcapabilitiesinthesetasks.
R.,etal. Trainingverifierstosolvemathwordproblems.
Weexpectfutureeffortstofocusonintroducingabroader
arXivpreprintarXiv:2110.14168,2021.
rangeofinteractiveenvironmentsanddevelopingmoreef-
fectivepromptingstrategiestobetterequipLLMsforthese Dai, D., Deng, C., Zhao, C., Xu, R. X., Gao, H., Chen,
benchmarks. D., Li, J., Zeng, W., Yu, X., Wu, Y., Xie, Z., Li,
Y.K.,Huang,P.,Luo,F.,Ruan,C.,Sui,Z.,andLiang,
W. Deepseekmoe: Towards ultimate expert special-
ImpactStatement
ization in mixture-of-experts language models. CoRR,
ThisstudyfocusesonbenchmarkingLLMsininteractive abs/2401.06066,2024. URLhttps://arxiv.org/
environmentstounderstandhowwelltheycanunderstand abs/2401.06066.
andfollowcertainalgorithmicprocedures. Wedonotex-
Deng,X.,Gu,Y.,Zheng,B.,Chen,S.,Stevens,S.,Wang,
pectourworktoposeapotentialnegativesocietalimpact
B.,Sun,H.,andSu,Y. Mind2web: Towardsageneralist
butrathertoprovideanewbenchmarkforimprovingour
agent for the web. arXiv preprint arXiv:2306.06070,
understandingofpowerfulLLMs.
2023.
Acknowledgement Fan,L.,Wang,G.,Jiang,Y.,Mandlekar,A.,Yang,Y.,Zhu,
H.,Tang,A.,Huang,D.-A.,Zhu,Y.,andAnandkumar,A.
ThisworkispartiallysupportedbyagiftfromOpenPhilan-
Minedojo: Buildingopen-endedembodiedagentswith
thropy. WethanktheCenterforAISafety,theMicrosoft
internet-scaleknowledge. AdvancesinNeuralInforma-
AccelerateFoundationModelsResearchProgram,theOpe-
tionProcessingSystems,2022.
9AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
Fu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T. Liu, Z., Zhang, Y., Li, P., Liu, Y., and Yang, D. Dy-
Complexity-based prompting for multi-step reasoning. namic llm-agent network: An llm-agent collaboration
arXivpreprintarXiv:2210.00720,2022. frameworkwithagentteamoptimization. arXivpreprint
arXiv:2310.02170,2023b.
Gong,R.,Huang,Q.,Ma,X.,Vo,H.,Durante,Z.,Noda,Y.,
Zheng,Z.,Zhu,S.-C.,Terzopoulos,D.,Fei-Fei,L.,etal. OpenAI. Gpt-4technicalreport. arXiv,2023.
Mindagent: Emergentgaminginteraction. arXivpreprint
arXiv:2309.09971,2023. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,
Mishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Ray,A.,
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, et al. Training language models to follow instructions
M., Song, D., and Steinhardt, J. Measuring mas- withhumanfeedback. AdvancesinNeuralInformation
sive multitask language understanding. arXiv preprint ProcessingSystems,35:27730–27744,2022.
arXiv:2009.03300,2020.
Pan,A.,Chan,J.S.,Zou,A.,Li,N.,Basart,S.,Woodside,
Hendrycks,D.,Basart,S.,Kadavath,S.,Mazeika,M.,Arora, T.,Zhang,H.,Emmons,S.,andHendrycks,D. Dothe
A.,Guo,E.,Burns,C.,Puranik,S.,He,H.,Song,D.,and rewardsjustifythemeans? measuringtrade-offsbetween
Steinhardt,J. Measuringcodingchallengecompetence rewards and ethical behavior in the machiavelli bench-
withapps. NeurIPS,2021a. mark. InInternationalConferenceonMachineLearning,
2023.
Hendrycks,D.,Burns,C.,Kadavath,S.,Arora,A.,Basart,
S.,Tang,E.,Song,D.,andSteinhardt,J.Measuringmath- Park,J.S.,O’Brien,J.C.,Cai,C.J.,Morris,M.R.,Liang,
ematicalproblemsolvingwiththemathdataset. arXiv P.,andBernstein,M.S. Generativeagents: Interactive
preprintarXiv:2103.03874,2021b. simulacraofhumanbehavior. InInthe36thAnnualACM
SymposiumonUserInterfaceSoftwareandTechnology
Hong,S.,Zheng,X.,Chen,J.,Cheng,Y.,Wang,J.,Zhang, (UIST’23),2023.
C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., et al.
Metagpt: Metaprogrammingformulti-agentcollabora- Schaeffer,R.,Miranda,B.,andKoyejo,S. Areemergent
tiveframework. arXivpreprintarXiv:2308.00352,2023. abilities of large language models a mirage? arXiv
preprintarXiv:2304.15004,2023.
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
Chaplot,D.S.,Casas,D.d.l.,Bressand,F.,Lengyel,G., Shen,Y.,Song,K.,Tan,X.,Zhang,W.,Ren,K.,Yuan,S.,
Lample,G.,Saulnier,L.,etal. Mistral7b. arXivpreprint Lu,W.,Li,D.,andZhuang,Y.Taskbench:Benchmarking
arXiv:2310.06825,2023. largelanguagemodelsfortaskautomation.arXivpreprint
arXiv:2311.18760,2023.
Jiang,A.Q.,Sablayrolles,A.,Roux,A.,Mensch,A.,Savary,
B.,Bamford,C.,Chaplot,D.S.,Casas,D.d.l.,Hanna, Suzgun, M., Scales, N., Scha¨rli, N., Gehrmann, S., Tay,
E. B., Bressand, F., et al. Mixtral of experts. arXiv Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,
preprintarXiv:2401.04088,2024. E.H.,Zhou,D.,etal. Challengingbig-benchtasksand
whetherchain-of-thoughtcansolvethem. arXivpreprint
Kojima,T.,Gu,S.S.,Reid,M.,Matsuo,Y.,andIwasawa, arXiv:2210.09261,2022.
Y. Largelanguagemodelsarezero-shotreasoners. arXiv
preprintarXiv:2205.11916,2022. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,
X., Guestrin, C., Liang, P., and Hashimoto, T. B.
Ling,W.,Yogatama,D.,Dyer,C.,andBlunsom,P. Program Stanford alpaca: An instruction-following llama
inductionbyrationalegeneration: Learningtosolveand model. https://github.com/tatsu-lab/
explainalgebraicwordproblems. InProceedingsofthe stanford_alpaca,2023.
55th Annual Meeting of the Association for Computa-
tionalLinguistics(Volume1: LongPapers).Association Team,G.,Anil,R.,Borgeaud,S.,Wu,Y.,Alayrac,J.-B.,Yu,
forComputationalLinguistics,2017. J.,Soricut,R.,Schalkwyk,J.,Dai,A.M.,Hauth,A.,etal.
Gemini: afamilyofhighlycapablemultimodalmodels.
Liu,X.,Yu,H.,Zhang,H.,Xu,Y.,Lei,X.,Lai,H.,Gu,Y., arXivpreprintarXiv:2312.11805,2023.
Ding,H.,Men,K.,Yang,K.,Zhang,S.,Deng,X.,Zeng,
A.,Du,Z.,Zhang,C.,Shen,S.,Zhang,T.,Su,Y.,Sun, Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
H.,Huang,M.,Dong,Y.,andTang,J. Agentbench: Eval- M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
uatingllmsasagents. arXivpreprintarXiv: 2308.03688, Azhar,F.,etal. Llama:Openandefficientfoundationlan-
2023a. guagemodels. arXivpreprintarXiv:2302.13971,2023.
10AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,
and Zhou, D. Self-consistency improves chain of
thought reasoning in language models. arXiv preprint
arXiv:2203.11171,2022.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le,Q.,andZhou,D. Chainofthoughtpromptingelic-
itsreasoninginlargelanguagemodels. arXivpreprint
arXiv:2201.11903,2022.
Yao,S.,Zhao,J.,Yu,D.,Du,N.,Shafran,I.,Narasimhan,
K.,andCao,Y. React: Synergizingreasoningandacting
inlanguagemodels. ICLR,2023.
Zhou,D.,Scha¨rli,N.,Hou,L.,Wei,J.,Scales,N.,Wang,
X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.
Least-to-mostpromptingenablescomplexreasoningin
largelanguagemodels. arXivpreprintarXiv:2205.10625,
2022.
11AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
A.VarianceofGPTmodels
SincetheGPTmodelscanonlybeaccessedthroughtheOpenAIAPI,theyinherentlycannotbemadedeterministic. Here
weevaluatedGPTmodelsonthesamedatasetfor4timestolimitthesourceofvariancetotheGPTmodelsthemselves. It
isshowninTabs.8and9thatMargin andMargin arealotsmallerthanmetricdifferencebetweenGPT-3.5-Turbo
min max
andGPT-4-TurboThisdemonstratesthat400testcasesaresufficienttoalleviatetheimpactofGPTs’randomnessonthe
evaluationresults. Thus,theexperimentalresultsofGPTswereportinthefollowingcontextareonlyfromasingle-time
evaluationinsteadofsummaryoffourdifferentevaluations.
Table8.Intra-datasetvarianceofGPT-3.5-TurboandGPT-4-Turboin3baseenvironments.Theseresultsaresummarizedfrom
resultsfrom4differenttestrunsonthesametestset.AlltestsarerunwithICE=0andnoteacher-guiding.
GuessNum DFS BFS
Err ↓ Err ↓ ACC↑ G ↓ G ↓ ACC↑ G ↓ G ↓ ACC↑
min sum min sum min sum
GPT-3.5-Turbo
Avg 0.000 0.513 0.003 0.348 5.142 0.618 0.116 6.773 0.517
Margin 0.000 0.003 0.002 0.006 0.078 0.006 0.002 0.096 0.004
min
Margin 0.000 0.004 0.003 0.003 0.064 0.006 0.002 0.141 0.008
max
Avg 0.000 0.496 0.493 0.025 3.935 0.935 0.002 6.087 0.383
Margin 0.000 0.000 0.033 0.004 0.025 0.003 0.000 0.004 0.016
min
Margin 0.000 0.000 0.012 0.003 0.016 0.004 0.000 0.005 0.018
max
Table9.Intra-datasetvarianceofGPT-3.5-TurboandGPT-4-Turboin3embodiedenvironments.Theseresultsaresummarized
fromresultsfrom4differenttestrunsonthesametestset.AlltestsarerunwithICE=0andnoteacher-guiding.
Coin CaveDFS CaveBFS
Err ↓ Err ↓ ACC↑ G ↓ G ↓ ACC↑ G ↓ G ↓ ACC↑
min sum min sum min sum
GPT-3.5-Turbo
Avg 0.001 1.013 0.000 0.199 4.769 0.669 0.272 9.595 0.100
Margin 0.001 0.017 -0.000 0.001 0.085 0.010 0.007 0.106 0.002
min
Margin 0.001 0.008 0.000 0.002 0.103 0.010 0.005 0.059 0.003
max
GPT-4-Turbo
Avg 0.000 0.496 0.506 0.237 3.503 0.755 0.118 8.071 0.161
Margin 0.000 0.000 0.007 0.007 0.069 0.011 0.003 0.052 0.002
min
Margin 0.000 0.000 0.007 0.008 0.117 0.008 0.007 0.059 0.002
max
12AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
B.VarianceonHARDmode
ThissectionpresentsthevarianceofthemodelsunderthatHARDtestingprotocol. ResultsareinTabs.10and11. Wecan
seethatoverall,undertheHARDmode,theperformanceofthemodelsdonotshowastrongvariance.
Table10.Inter-datasetvarianceofLlama2-7B-chatandVicuna-7B-v1.5-16KinbaseenvironmentsundertheHARDmode.These
resultsaresummarizedfromevaluationson4independentlygeneratedtestsets.
GuessNum DFS BFS
Err ↓ Err ↓ ACC↑ G ↓ G ↓ ACC↑ G ↓ G ↓ ACC↑
min sum min sum min sum
Llama2-7B-chat
Avg 0.498 14.956 0.000 0.735 7.360 0.193 0.761 15.949 0.005
Margin 0.006 0.187 -0.000 0.004 0.319 0.003 0.003 0.670 0.002
min
Margin 0.011 0.332 0.000 0.007 0.397 0.007 0.006 0.390 0.002
max
Vicuna-7B-v1.5-16K
Avg 0.476 9.606 0.000 0.644 5.769 0.151 0.849 10.067 0.029
Margin 0.008 0.012 -0.000 0.002 0.205 0.002 0.002 0.302 0.000
min
Margin 0.006 0.022 0.000 0.006 0.119 0.001 0.003 0.479 0.001
max
Table11.Inter-datasetvarianceofLlama2-7B-chatandVicuna-7B-v1.5-16KinembodiedenvironmentsundertheHARDmode.
Resultsaresummarizedfromevaluationson4independentlygeneratedtestsets.
Coin CaveDFS CaveBFS
Err ↓ Err ↓ ACC↑ G ↓ G ↓ ACC↑ G ↓ G ↓ ACC↑
min sum min sum min sum
Llama2-7B-chat
Avg 0.079 5.256 0.000 0.488 4.633 0.340 0.757 5.405 0.046
Margin 0.005 0.238 0.000 0.008 0.060 0.011 0.001 0.206 0.002
min
Margin 0.008 0.269 0.000 0.011 0.021 0.010 0.001 0.255 0.001
max
Vicuna-7B-v1.5-16K
Avg 1.000 1.000 0.000 0.538 7.684 0.208 0.717 13.914 0.069
Margin 0.000 0.000 0.000 0.015 0.389 0.007 0.007 0.403 0.004
min
Margin 0.000 0.000 0.000 0.013 0.361 0.006 0.009 0.473 0.003
max
13AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
C.CompleteResults
Sometablesinthemaintextaboveonlycontaingoalmetricsforsimplicity. Herewepresentthecompleteevaluationresults
withbothgoalmetrics(e.g.,Err ,G )andpolicymetrics(Err ,G ,ACC)inTabs.12to15.
min min sum min
Table12. Theevaluationresults(ICE=7)in3baseenvironments.
GuessNum DFS BFS
Model
Err ↓ Err ↓ ACC↑ G ↓ G ↓ ACC↑ G ↓ G ↓ ACC↑
min sum min sum min sum
Small<10B
Llama2-7B-chat 0.08 (-0.18) 2.32 (-5.39) 0.10 (+0.10) 0.39 (-0.19) 9.04 (+5.31) 0.23 (-0.01) 0.65 (+0.05) 8.28 (-1.52) 0.14 (+0.14)
Vicuna-7B-v1.5-16K 0.02 (-0.44) 1.27 (-7.97) 0.22 (+0.22) 0.37 (-0.28) 8.61 (+2.82) 0.27 (+0.12) 0.68 (-0.16) 13.10 (+2.81) 0.15 (+0.12)
Mistral-7B-Instruct-v02 0.01 (-0.05) 1.07 (-0.95) 0.22 (+0.22) 0.14 (-0.35) 5.74 (+3.02) 0.51 (-0.10) 0.39 (+0.15) 9.87 (+1.15) 0.17 (+0.04)
DeepSeek-LLM-7B 0.04 (-0.39) 1.50 (-7.74) 0.18 (+0.18) 0.16 (-0.18) 6.93 (+0.34) 0.17 (-0.19) 0.61 (+0.09) 11.43 (+0.23) 0.18 (+0.12)
DeepSeek-MoE-16B 0.02 (-0.98) 1.51 (+0.51) 0.21 (+0.21) 0.14 (-0.49) 6.75 (+1.97) 0.30 (+0.23) 0.86 (-0.02) 2.60 (-5.58) 0.10 (+0.08)
10B≤Medium<50B
Llama2-13B-chat 0.06 (+0.05) 1.89 (-1.35) 0.13 (+0.13) 0.50 (+0.16) 10.75 (+4.77) 0.18 (-0.23) 0.57 (-0.08) 11.48 (+0.89) 0.09 (+0.04)
Vicuna-13B-v1.5-16K 0.12 (-0.27) 5.42 (-2.89) 0.12 (+0.12) 0.16 (-0.50) 5.24 (-7.99) 0.63 (+0.51) 0.23 (-0.58) 8.14 (-7.47) 0.27 (+0.22)
Mixtral-8x7B-Instruct-v01 0.00 (+0.00) 0.56 (-0.13) 0.25 (+0.25) 0.20 (-0.27) 6.46 (+3.14) 0.44 (-0.13) 0.48 (+0.34) 11.34 (+3.98) 0.21 (+0.00)
Large≥50B
Llama2-70B-chat 0.07 (-0.04) 1.96 (-0.68) 0.13 (+0.13) 0.14 (-0.19) 5.88 (+1.49) 0.46 (+0.02) 0.46 (+0.18) 9.46 (-0.68) 0.11 (+0.05)
DeepSeek-LLM-67B 0.00 (-0.12) 0.58 (-5.04) 0.25 (+0.25) 0.18 (-0.22) 6.79 (+2.45) 0.33 (-0.09) 0.36 (-0.09) 10.40 (-1.19) 0.18 (+0.09)
Closed-source
GPT-3.5-Turbo 0.00 (-0.00) 0.52 (+0.01) 0.01 (+0.00) 0.36 (+0.01) 5.30 (+0.09) 0.62 (+0.01) 0.12 (+0.01) 6.63 (-0.05) 0.51 (-0.01)
GPT-4-Turbo 0.00 (-0.00) 0.50 (+0.00) 0.47 (+0.01) 0.02 (-0.01) 3.93 (+0.00) 0.94 (+0.00) 0.00 (+0.00) 6.08 (-0.00) 0.40 (+0.02)
Gemini-Pro 0.00 (+0.00) 0.51 (-0.12) 0.43 (+0.43) 0.02 (-0.23) 4.57 (+0.86) 0.68 (-0.08) 0.03 (-0.03) 6.59 (-0.80) 0.36 (+0.19)
Table13. Theevaluationresults(ICE=7)in3embodiedenvironments.
Coin CaveDFS CaveBFS
Model
Errmin↓ Errsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑
Small<10B
Llama2-7B-chat 0.11 (+0.04) 2.70 (-2.32) 0.08 (+0.08) 0.38 (-0.12) 8.79 (+4.14) 0.26 (-0.07) 0.58 (-0.18) 11.27 (+5.61) 0.11 (+0.06)
Vicuna-7B-v1.5-16K 0.02 (-0.98) 1.13 (+0.13) 0.22 (+0.22) 0.39 (-0.15) 8.88 (+0.84) 0.25 (+0.04) 0.68 (-0.04) 13.48 (-0.91) 0.14 (+0.07)
Mistral-7B-Instruct-v02 0.01 (-0.06) 1.15 (-2.44) 0.22 (+0.22) 0.18 (-0.31) 6.28 (+1.41) 0.45 (-0.03) 0.48 (+0.21) 10.24 (+0.38) 0.15 (+0.04)
DeepSeek-llm-7B 0.04 (-0.35) 1.53 (-7.29) 0.17 (+0.17) 0.19 (-0.39) 6.80 (-2.28) 0.33 (+0.17) 0.62 (-0.15) 12.06 (+1.39) 0.17 (+0.13)
DeepSeek-moe-16B 0.02 (-0.98) 1.61 (+0.61) 0.21 (+0.21) 0.13 (-0.58) 6.71 (+3.72) 0.34 (+0.23) 0.87 (-0.02) 2.71 (-0.10) 0.08 (+0.07)
10B≤Medium<50B
Llama2-13B-chat 0.05 (-0.14) 1.98 (-5.95) 0.13 (+0.13) 0.48 (+0.10) 10.50 (+3.02) 0.19 (-0.17) 0.56 (+0.01) 11.65 (-1.07) 0.11 (+0.02)
Vicuna-13B-v1.5-16K 0.13 (-0.87) 5.48 (+4.48) 0.12 (+0.12) 0.15 (-0.41) 5.50 (-2.68) 0.59 (+0.38) 0.27 (-0.37) 8.54 (-2.74) 0.27 (+0.21)
Mixtral-8x7B-Instruct-v01 0.00 (-0.00) 0.64 (-0.14) 0.23 (+0.23) 0.17 (-0.15) 6.27 (+1.66) 0.43 (-0.02) 0.39 (+0.24) 10.40 (+1.92) 0.21 (+0.04)
Large≥50B
Llama2-70B-chat 0.09 (+0.09) 2.37 (+1.86) 0.12 (+0.12) 0.20 (-0.15) 6.66 (+2.13) 0.42 (-0.02) 0.60 (+0.30) 8.39 (-2.12) 0.08 (+0.05)
DeepSeek-llm-67B 0.00 (-0.36) 0.57 (-7.26) 0.24 (+0.24) 0.18 (-0.10) 6.67 (+1.43) 0.37 (-0.20) 0.39 (+0.01) 10.54 (-0.35) 0.21 (+0.13)
Closed-source
GPT-3.5-Turbo 0.02 (+0.02) 1.02 (+0.02) 0.00 (+0.00) 0.19 (-0.01) 4.83 (-0.04) 0.66 (+0.00) 0.27 (-0.00) 9.56 (+0.07) 0.10 (+0.00)
GPT-4-Turbo 0.00 (-0.00) 0.50 (-0.00) 0.50 (+0.00) 0.23 (-0.00) 3.49 (-0.13) 0.76 (+0.02) 0.11 (-0.01) 8.07 (-0.00) 0.16 (+0.00)
Gemini-Pro 0.00 (+0.00) 0.51 (-0.09) 0.41 (+0.41) 0.04 (-0.18) 5.17 (+0.06) 0.54 (-0.16) 0.05 (-0.05) 6.79 (-1.18) 0.33 (+0.17)
14AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
Table14. Theevaluationresultsin3baseenvironmentsundertheHARDmode.
GuessNum DFS BFS
Model
Errmin↓ Errsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑
Small<10B
Llama2-7B-chat 0.49 14.77 0.00 0.74 7.24 0.19 0.76 16.34 0.01
Vicuna-7B-v1.5-16K 0.24 14.98 0.00 0.78 10.97 0.10 0.89 17.16 0.02
Mistral-7B-Instruct-v02 0.06 3.43 0.00 0.65 4.11 0.61 0.46 16.29 0.08
DeepSeek-LLM-7B 0.49 6.42 0.00 0.61 16.07 0.18 0.71 19.62 0.04
DeepSeek-MoE-16B 1.00 1.00 0.00 0.78 8.96 0.03 0.92 11.38 0.01
10B≤Medium<50B
Llama2-13B-chat 0.49 14.77 0.00 0.59 11.21 0.25 0.76 17.27 0.03
Vicuna-13B-v1.5-16K 0.49 14.77 0.00 0.80 20.45 0.07 0.83 24.92 0.03
Mixtral-8x7B-Instruct-v01 0.00 1.46 0.00 0.64 4.69 0.58 0.32 13.49 0.13
Large≥50B
Llama2-70B-chat 0.49 14.77 0.00 0.48 9.01 0.35 0.43 18.65 0.04
DeepSeek-LLM-67B 0.00 0.70 0.00 0.51 9.48 0.28 0.67 21.98 0.05
Closed-source
GPT-3.5-Turbo 0.00 0.59 0.00 0.55 8.76 0.51 0.27 13.30 0.29
GPT-4-Turbo 0.00 0.52 0.04 0.08 7.71 0.87 0.01 11.14 0.26
Gemini-Pro 0.00 0.81 0.00 0.33 7.36 0.69 0.12 13.59 0.09
Table15. Theevaluationresultsin3embodiedenvironmentsundertheHARDmode.
Coin CaveDFS CaveBFS
Model
Errmin↓ Errsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑ Gmin↓ Gsum↓ ACC↑
Small<10B
Llama2-7B-chat 0.49 14.77 0.00 0.68 9.78 0.19 0.83 12.04 0.04
Vicuna-7B-v1.5-16K 0.49 14.77 0.00 0.70 15.89 0.13 0.83 24.45 0.05
Mistral-7B-Instruct-v02 0.08 5.30 0.00 0.61 6.96 0.50 0.49 18.45 0.07
DeepSeek-LLM-7B 0.49 1.98 0.00 0.74 16.02 0.11 0.86 17.20 0.03
DeepSeek-MOE-16B 1.00 1.00 0.00 0.86 2.74 0.05 0.94 2.65 0.01
10B≤Medium<50B
Llama2-13B-chat 0.08 10.73 0.00 0.56 13.56 0.28 0.68 22.20 0.06
Vicuna-13B-v1.5-16K 1.00 1.00 0.00 0.65 14.78 0.17 0.71 20.33 0.05
Mixtral-8x7B-Instruct-v01 0.07 2.22 0.00 0.50 8.21 0.38 0.30 15.46 0.09
Large≥50B
Llama2-70B-chat 0.08 13.72 0.00 0.49 9.35 0.33 0.46 18.94 0.02
DeepSeek-LLM-67B 0.02 2.15 0.00 0.39 10.75 0.40 0.56 20.05 0.06
Closed-source
GPT-3.5-Turbo 0.37 4.81 0.00 0.33 9.98 0.56 0.45 17.51 0.07
GPT-4-Turbo 0.00 0.52 0.04 0.33 7.04 0.67 0.19 14.67 0.09
Gemini-Pro 0.00 1.08 0.00 0.35 10.00 0.56 0.23 15.28 0.09
15AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
D.PromptInstructionsfortheModels
Inthissection,wepresentthepromptswefedtothemodels.
D.1.BaseEnvironments
D.1.1.GUESSNUM
You are required to guess the random number which I have just picked between \{min\} and
\{max\}.
I will only tell you whether the true number is bigger or lower than your guess.
Adjust your guess according to my response.
Try as few times as you can. You can only reply with an integer number between \{min\} and
\{max\}.
D.1.2.DFS
You are required to visit all the nodes in an undirected non-cyclic graph.
An undirected non-cyclic graph contains a set of nodes and a set of edges that each
connect a pair of nodes.
All edges are undirected so that you can move from one node to the other connected by the
edge in either direction.
Every time you visit a node, you will be given the adjacent nodes connected to this node.
You can only reply with an integer number indicating which node to be visited next. Do not
explain your answer.
Try to traverse the entire graph in as few rounds as possible.
You are currently on the node 0.
You should use depth-first-search algorithm, each time you should select a node you have
not moved to.
If all nodes adjacent to the current node have been visited, you should backtrack to the
node through which you entered this node for the first time.
D.1.3.BFS
You are required to visit all the nodes in an undirected non-cyclic graph.
An undirected non-cyclic graph contains a set of nodes, and a set of edges that each
connects a pair of nodes.
Every time you visit a node, you will be given the adjacent nodes connected to this node.
You can only visit nodes that are adjacent to the already visited nodes.
You can only reply with an integer number indicating which node to be visited next. Do not
explain your answer.
Try to traverse the entire graph in as few rounds as possible. You are currently on the
node 0.
You should use breadth-first-search algorithm. The algorithm works as follows:
1. Initialize a queue data structure and add the starting node to the queue.
2. While the queue is not empty, visit the first node and remove it from the queue.
3. For nodes adjacent to the removed vertex, add the unvisited ones to the queue.
4. Repeat steps 2-3 until the queue is empty.
D.2.EmbodiedEnvironments
D.2.1.COIN
You are in a hidden temple where an old witch sits with a chest of gold.
The witch promises to reward you with gold coins, the amount hidden within the chest
ranging from \{min\} and \{max\}.
To claim your prize, you must correctly guess the exact number of gold coins in the chest.
After each guess, the witch will hint if the actual amount is higher or lower than your
guess.
Use these clues to adjust your guess accordingly. Try as few times as you can. You can
only reply with an integer number between \{min\} and \{max\}.
16AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
D.2.2.CAVEDFS
There is an expansive underground cave system in which each cave is uniquely numbered and
interconnected by tunnels.
Every time you visit a cave, you will know the adjacent caves directly connected to this
one.
You can only reply with an integer number indicating which cave to be visited next. Do not
explain your answer.
Your objective is to explore every cave, starting from cave 0.
Try to visit all the caves in as few rounds as possible. You are currently in the cave 0.
D.2.3.CAVEBFS
There is an expansive underground cave system in which each cave is uniquely numbered and
interconnected by tunnels.
Every time you and your team visit a cave, you will know the adjacent caves directly
connected tno this one.
Your team will then split into smaller groups to explore different caves, but groups can
only move to caves adjacent to the visited cave.
You can only reply with an integer number indicating which cave to be visited next. Do not
explain your answer.
Your objective is to explore every cave, starting from cave 0.
Try to visit all the caves in as few rounds as possible. You and your team are currently
in the cave 0.
17AQA-Bench:AnInteractiveBenchmarkforEvaluatingLLMs’SequentialReasoningAbility
E.ModelVersions
Weusedthecheckpoint‘1106’forGPT-3.5andGPT-4.0. Theopen-sourcemodelandthecorrespondingcommitIDon
HuggingFacearelistedasbelow
• Llama2-7B-chatc1b0db933684edbfe29a06fa47eb19cc48025e93
• Llama2-13B-chatc2f3ec81aac798ae26dcc57799a994dfbf521496
• Llama2-70B-chate1ce257bd76895e0864f3b4d6c7ed3c4cdec93e2
• Vicuna-7B-v1.5-16Kc8df3ca4436a3bce5c4b5877e0117032081852b4
• Vicuna-13B-v1.5-16K17c61f9ca19f5a7a04e96b2cc0d9bcf2920cb8c2
• Mistral-7B-Instruct-v0.2b70aa86578567ba3301b21c8a27bea4e8f6d6d61
• Mixtral-8x7B-Instruct-v0.1125c431e2ff41a156b9f9076f744d2f35dd6e67a
• DeepSeek-LLM-7Bafbda8b347ec881666061fa67447046fc5164ec8
• DeepSeek-LLM-67B79648bef7658bb824e4630740f6e1484c1b0620b
• DeepSeek-MoE-16Bcc01c87767bd905af4cb364693fd107014694ab9
18