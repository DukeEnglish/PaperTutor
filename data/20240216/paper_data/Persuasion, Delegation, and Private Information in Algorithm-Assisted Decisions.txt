Persuasion, Delegation, and Private Information in
Algorithm-Assisted Decisions
Ruqing Xu
Cornell University, Department of Economics
February, 2024
Abstract
A principal designs an algorithmthat generatesa publicly observable prediction of a binary
state. She must decide whether to act directly based on the prediction or to delegate the de-
cision to an agent with private information but potential misalignment. We study the optimal
design of the prediction algorithm and the delegation rule in such environments. Three key
findings emerge: (1) Delegation is optimal if and only if the principal would make the same
binary decision as the agent had she observed the agent’s information. (2) Providing the most
informative algorithm may be suboptimal even if the principal can act on the algorithm’s pre-
diction. Instead, the optimal algorithm may provide more information about one state and
restrictinformation about the other. (3) Commonrestrictions onalgorithms,such as keeping a
“human-in-the-loop” or requiring maximal prediction accuracy, strictly worsen decision quality
in the absence of perfectly aligned agents and state-revealing signals. These findings predict
the underperformance of human-machine collaborations if no measures are taken to mitigate
common preference misalignment between algorithms and human decision-makers.
1 Introduction
Decisions today are increasingly complex and data intensive. Toward the goal of making better de-
cisions, decision-aid algorithms have been developed and employed in many consequential domains,
including criminal justice, healthcare, and credit lending systems. These algorithms take in data
from decision subjects (e.g., defendants, patients, loan applicants) and generate predictions. These
predictions can either be used to make decisions automatically, or be disclosed to human agents
who ultimately make a decision. While the agents may have private knowledge that helps to make
a better decision, they may also exhibit behavioral biases, cognitive limitations, or private motives
that might misalign with the algorithm.
A real-life example is the Child Protection Service (CPS) in Allegheny County, PA, which
utilizes an algorithm to assign risk scores to reports of child maltreatment [Cheng et al., 2022]. If
the risk score exceeds certain thresholds, cases are automatically screened-in. Otherwise, a social
worker sees the score and other available information, then makes the final decision. Suppose that a
social worker under-investigates since investigation is costly to him, but he possesses intuition and
contextual information that are inaccessible to the algorithm. How should we design the predictive
algorithm to collaborate with such a privately informed but biased agent? When should we pass
the prediction and the decision to the agent, for the purpose of better information?
1
4202
beF
41
]HT.noce[
1v48390.2042:viXraIndeed, preference misalignment is widespread in other applications. Bank managers may be
morerisk-aversethanaprofit-maximizingalgorithmwhenevaluatinghigh-riskopportunities. Physi-
ciansmayoverweightsalientbutlesspredictivesignalsindiagnosingandtesting[Mullainathan and Obermeyer,
2022]. Judges can exhibit behavioral inconsistencies or even biases towards certain demographic
groups [Eren and Mocan, 2018, Kleinberg et al., 2017]. If the quality of the final decision is of con-
cern, we ought to take into account the agents’ private information and potential misalignment in
designing the predictive algorithm, as well as in deciding when the decision authority should be
given to an agent.
We present a principal-agent model to study the algorithm design and delegation strategy when
the agent is privately informed but potentially misaligned. We model the algorithm as a “signal”
whose realizations (predictions) convey information about the underlying state. The design of the
algorithm corresponds toapersuasionproblem wheretheprincipal choosestheoptimal signalstruc-
ture. We model the problem of whether to pass the decision to the human as a delegation problem.
Our results suggest that delegation is strictly valuable if and only if the principal would make the
same decision as the agent had she observed the agent’s information. We find that providing the
most informative algorithm may not be optimal even if the principal can choose to act on the algo-
rithm’s prediction. Moreover, the optimal algorithm may provide more information about one state
while restricting information about the other. Lastly, we show that common restrictions on algo-
rithms, such as keeping a “human-in-the-loop” and requiring maximal prediction accuracy, strictly
worsen decision quality for some decision subjects in the absence of perfectly aligned agents and
state-revealing signals. Insightsfromthis papermay explainwhy previousresearch hasfoundmixed
results regarding the success and effectiveness of human-machine collaborative systems [Lai et al.,
2021].
Formally, we consider a model with binary states and binary actions. The principal and the
agent each receive a signal that contains information about the underlying state. The principal’s
signal is publicly observable while the agent’s signal is private to himself.1 We allow for arbitrary
state-dependent preferences for both players but no contingent transfers. The principal faces two
decisionproblems. First,theprincipaldesignstheinformationstructureofthepublicsignal,subject
to a constraint on the maximal informativeness. After the principal observes the signal realization,
she also decides whether to act with the current information, or to delegate the decision to the
agent. Although these decisions are made at different times, the principal must take into account
the delegation decision in the design of the public signal. Figure 1 illustrates.
Agent Decision
Algorithm
Data Prediction Delegation
Decision
Figure 1: Diagram of the two decision problems of the principal
1 Fortheeaseofmodeling,weassumethattheprincipalalwaysrevealshersignaltotheagentregardlessofwhether
shedelegatestotheagent,whichmakestheprincipal’ssignalessentiallypublic. Itiswithoutlossofgeneralitybecause
theagent cannot usethe signal when he is not delegated with the decision.
2We begin by characterizing the necessary and sufficient conditions under which delegation is
valuable. Proposition 1 proves that delegation strictly improves the principal’s payoff if and only if
the principal would take the same action as the agent if she were to observe the agent’s signal. In
all other cases, the principal is either indifferent or strictly prefers to take control due to the agent’s
misalignment. We then analyze the comparative statics on the principal’s payoffs with respect
to the informativeness of the agent’s signal and the degree of preference misalignment between the
players. Proposition3highlightsaninterestingresult: theprincipal’sdelegationpayoffcouldstrictly
decreasewhentheagentbecomesmoreinformative. Inparticular, thishappenswhentheprincipal’s
interim posterior (after observing the public signal) prefers the action that is less preferred by the
agent. On the other hand, Proposition 6 shows that the principal’s payoff weakly increases when
facing a more aligned agent.
After characterizing the optimal delegation strategy, we turn to the principal’s problem of de-
signing the optimal public signal. We show in Proposition 8 that the most informative public signal
isoptimalifandonlyifaconvexityconditionontheposteriorsunderthemaximalsignalholds. Oth-
erwise, the optimal signal maximizes information one state while restricting information about the
other. Lastly, we highlight in Propositions 9 and 10 that common restrictions on algorithms, such
as mandating delegation (keeping a “human-in-the-loop”) and disregarding persuasion (maximizing
prediction accuracy), strictly worsen decision quality for some decision subjects in the absence of
perfectly aligned agents and state-revealing signals.
This paper combines two strands of literature in economics – optimal delegation and Bayesian
persuasion. The classical delegation problem considers a principal who faces a privately informed
but misaligned agent. The principal influences the agent’s behavior by specifying a set of actions
(called the delegation set) from which the agent can choose [Holmström, 1978, 1980]. Following
that, Alonso and Matouschek [2008] and Amador and Bagwell [2013] characterize the optimal del-
egation set under more general preferences. This paper is also related to the Bayesian persuasion
problem [Kamenica and Gentzkow, 2011]. The persuasion problem considers the situation where
theprincipalinfluences thedecisionoftheagentby designingtheinformationstructureofapublicly
observable signal.
This paper differs from and contributes to the above literature by considering the joint design of
optimal persuasionand delegation mechanisms. Lou [2022], Vairo [2023]study a similar problem on
the joint design of delegation and disclosure rules. However, Lou [2022] focuses on a setting where
the principal designs the structure of the agent’s signal, whose realization remains unobservable to
the principal. Our paper differs in that the principal designs the public signal instead of the agent’s
private signal, and can observe the realization of the public signal and choose to use it directly. On
the other hand, Vairo [2023] assumes that the agent’s private information is about his type, which
affects the payoff of both players. In contrast, we assume that the agent holds private information
about the underlying state, while the preference of the agent is common knowledge. This difference
stems from the context of decision-aid algorithms we hope to model, where both the algorithm and
the human agent try to learn about the true state and take the optimal action.
There have been efforts to model human-machine collaborations from both the fields of eco-
nomics and computer science. Rastogi et al. [2022] and Donahue et al. [2022] investigate the opti-
mal ways to aggregate independent algorithmic and human predictions, and under what conditions
these aggregations outperform individual predictions. Straitouri et al. [2022] proposes an algorithm
that selects an optimal set of possible labels and presents it to a human expect for final selection.
Xu and Dean [2023] and McLaughlin and Spiess [2023] study theoretically how to adjust the design
3ofdecision-aidalgorithms tocounteract humanbiases. Agrawal et al.[2018]studiesthecomplemen-
tarity of machine predictions and humans’ private information about the payoffs of state contingent
actions. To our knowledge, this is the first paper to study the design of algorithms and delegation
to humans, two elements of human-machine collaboration, in the presence of misaligned agents.
2 Problem Setting
There are two players: a principal (she) and an agent (he). The state of the world Θ can take one of
the two values, θ ∈ {0,1}. It can represent “low” or “high,” “bad” or “good,” “guilty” or “innocent,”
etc. The principal and the agent share a common prior µ on the probability of the high state,
µ = Pr(Θ = 1). Both players receive a publicly observable signal, while the agent also receives a
private signal.
The principal chooses and commits to the information structure of the public signal. The
information structure of the private signal is exogenous and common knowledge. After observing
the public signal, the principal decides whether to take an action with the current information or
to delegate the decision to the agent. If delegated, the agent observes both the public and private
signals and takes an action.
2.1 Signals and Informativeness
We denote the public and private signals as S1 and S2, respectively, with their realizations s1 and
s2. The set of possible signals realizations are {0,1}. We call µ
s1
= Pr(Θ = 1 | S1 = s1) the
interim posterior, which is the updated belief of the players after observing the public signal. We
call µ
s1s2
= Pr(Θ = 1 | S1 = s1,S2 = s2) the final posterior, which is the agent’s posterior after
observing both signals. Signals are generated according to the following information structures:
Signal S1 Signal S2
0 1 0 1
0 p0 1−p0 0 q0 1−q0
State Θ State Θ
1 1−p1 p1 1 1−q1 q1
Table 1: Public signal Table 2: Agent’s signal
The table entries represent Pr(s |θ), the conditional probability of observing the signal s given
the state θ. We assume that p0+p1 > 1 and q0+q1 >1, meaning that a good (bad) signal is good
(bad) news.
Since we are interested in the principal’s payoffs when the signals become more informative,
we need a theory to compare the informativeness of information structures. We adopt a standard
measure of informativeness in informational economics, Blackwell informativeness [Blackwell, 1951,
′
1953]. We say that signal S is (Blackwell) more informative than signal S if for any prior, the
distribution of posterior beliefs after observing S is a mean-preserving spread of the distribution
′
of posterior beliefs after observing S . In the context of binary signals, it is equivalent to saying
that the interval between the two posteriors after observing the high and low realizations under S
′
strictly contains the interval between the two posteriors under S .
Blackwell informativeness isindependent ofdecision-makers’ priors andpreferences. Thismeans
that any Bayesian agent, facing any decision problem, can obtain a higher expected payoff if she
4receivesasignalthatisBlackwellmoreinformative. Thisisdesirableinoursettingsinceitallowsthe
comparison of signal informativeness tobe independent of the prior or the preferences. However, we
alsonote that theBlackwell order is incomplete –notall signals canberanked interms of Blackwell
informativeness.
To make the problem nontrivial, we impose a maximal Blackwell constraint on the information
design problem of the principal.2 The principal can only choose from signals that are equally or less
informative than the maximally informative signal (i.e., the maximal signal), denoted by λ. The
posterior beliefs under the maximal signal are denoted by µλ and µλ.
0 1
2.2 Preferences and Payoffs
TheprincipalandtheagenthaveaccesstoacommonsetofactionsY = {0,1}. Theyhave(possibly)
misaligned preferences that can be represented by two payoff matrices:
Action Y Action Y
0 1 0 1
0 r00 r01 0 v00 v01
State Θ State Θ
1 r10 r11 1 v10 v11
Table 3: Principal’s payoffs Table 4: Agent’s payoffs
The first subscript represents the state whereas the second represents the action. For example,
r represents the payoff of the principal when under state i, action j is taken. On top of the payoff
ij
structure, both players are von Neumann-Morgenstern expected utility maximizers. This payoff
structure subsumes many commonly assumed payoff functions with binary states and actions, for
example, when the principal and the agent have heterogeneous costs of taking certain actions; when
the principal and the agent have different benefits of matching the state [Kamenica and Gentzkow,
2011]; and when the agent exhibits loss aversion when taking a risky action in a bad state. We
impose the following assumption on the payoff structures:
Assumption 1. The principal’s payoffs satisfy r00 > r01 and r11 > r10. Similarly, the agent’s
payoffs satisfy v00 > v01 and v11 >v10. This assumption has two implications:
(1) There is no dominant action (i.e., action one would take regardless of the state) for either
player.
(2) Each player’s payoff function of action 1 crosses their payoff function of action 0 from below.
This means that both players switch from action 0 to action 1 at some point as their beliefs
about state 1 increase.
We show in Appendix A that this assumption makes the problem interesting, since the principal
never strictly prefers to delegate to the agent when this assumption is violated and the information
design problem is trivially solved by choosing the maximally informative signal.
In Figure 2, we plot an example of the two players’ payoff functions with respect to the players’
beliefs about the underlying state.
2 If the principal has access to an unrestricted space of signals, she will simply choose the state-revealing signal
and neverdelegate.
5U
v00
U0A
r00 r11
U0P
U1P
v11
U1A
v10
r01 r10
Pr(Θ=1)
0 µ¯P µ¯A 1
v01
Figure 2: Players’ payoffs as functions of the belief
For example, UP(·) denotes the Principal’s payoff when taking action 0, as a function of the
0
belief. The four payoff functions are linear combinations of the discrete payoffs. Assumption 1
guarantees thatthepayofffunctionshaveintersections. Theintersectionpointsµ¯ andµ¯ represent
P A
the cutoff points at which the principal and the agent switch to the higher action, respectively. We
call the interval between µ¯ and µ¯ the disagreement interval (I), since the players prefer different
P A
actions in this interval. Note that in this example, µ¯ < µ¯ , meaning that the principal prefers to
P A
take action 1 at a wider range of beliefs compared to the agent. Therefore, we say that action 1 is
the principal-preferred action.
Our analysis greatly simplifies if we look atthe payoff envelopes of the principal. Ifthe principal
does not delegate, the envelope is simply the maximum of UP and UP, which is continuous and
0 1
has a kink at µ¯ . If the principal delegates, however, the agent makes the final decision. Thus, the
P
principal’s payoff follows thecutoffpointoftheagentandbecomesdiscontinuous atµ¯ .3 Wedenote
A
them as the non-delegation envelope (V ) and delegation envelope (V ), respectively, as shown in
N D
Figure 3.
2.3 Design Choices
Conditioning delegation on the public signal. In this model, we assume that the principal
can condition the delegation decision on the realization of the public signal. Alternatively, one
may assume that the principal must commit to delegation or direct action before observing the
public signal. We note that these two models correspond to different decision problems: the former
captures the decision-making process for each decision subject, whereas the latter chooses between
a fully automated system or a human-in-the-loop system for all decisions.
3 Forsimplicityofstatingtheorems,weassumethattheagenttakestheprincipal-preferredactionwhenindifferent,
so the delegation envelope evaluates to thetop point at thediscontinuity.
6U U
I
Pr(Θ=1) Pr(Θ=1)
0 µ¯P 1 0 µ¯P µ¯A 1
(a) Principal’s non-delegation envelope (VN) (b) Principal’s delegation envelope(VD)
Figure 3: Comparison of the principal’s payoff envelopes
Heterogeneity of preferences and signals for different decision subjects. One may be
concerned with how generalizable our results are when the players’ preferences and signal infor-
mativeness may change with respect to different decision subjects. However, we believe that this
does not limit the overall applicability of our findings. This is because the decision-making process
for each decision subject can be repeated under this framework, but with varied parameters for
preferences, signal structures of the agent, and maximal Blackwell constraints.
Complete information about the agent’s preferences and informativeness. Common to
theoretical work in persuasion and delegation, this paper makes the assumption that the prefer-
ences of both players and the information structure of the signals are common knowledge. Such
assumptionsallowustocleanlycharacterize theoptimalpersuasionanddelegationmechanisms. We
recognize that a promising extension to this paper is to develop methods for separately identifying
and estimating human agents’ biases and information. We discuss this in more detail in the future
work section.
3 Value of Delegation
In this section, we study the principal’s optimal delegation decision after observing the realization
of thepublic signal. Proposition 1characterizes the necessary andsufficient condition fordelegation
to be strictly valuable.
Proposition 1 (Necessary and sufficient condition for strict delegation). Given a public signal
realization s1 and the associated interim posterior µ s1, delegation is strictly valuable to the principal
if and only if the agent’s final posteriors µ s10 and µ s11 lie on the opposing sides of the disagreement
interval. In other words,
µ s10 < {µ¯ P,µ¯ A} < µ s11.
Proof. We first prove the “if” direction. By Bayes’ rule, the expected posteriors equals the prior,
i.e., E [µ ] = µ . Since the expected payoff of delegation, E [V (µ )], is linear in beliefs, it
s2 s1s2 s1 s2 D s1s2
equals the linear combination of V D(µ s10) and V D(µ s11) evaluated at the prior µ s1. Therefore, if the
7agent’s posteriors lie on the opposing side of the disagreement interval, any linear combination of
them is above the principal’s non-delegation envelope V . Thus, at any prior µ , delegation would
N s1
give strictly higher payoff than non-delegation, i.e., delegation is strictly valuable.
We now prove the “only if” direction. For the sake of contradiction, suppose that µ s10 and µ s11
are not on the opposing sides of the disagreement interval. First observe that delegation cannot
be strictly valuable whenever the points (µ s10,V D(µ s10)), (µ s11,V D(µ s11)), and (µ s1,V D(µ s1)) are
co-linear. Since if that is the case,
E [V (µ )] = V (E [µ ]) ≤ V (E [µ ]).
s2 D s1s2 D s2 s1s2 N s2 s1s2
The inequality results from the fact that the delegation envelope lies everywhere weakly below the
non-delegation envelope. The only other case where the posteriors are not on the opposing side
of the disagreement interval and are not co-linear with the prior is as shown in Figure 4(a). The
expected delegation payoff is a linear combination with one posterior in the disagreement interval
and another posterior across the discontinuity at µ¯ . In this case, delegation makes the principal
A
strictly worse off.
Intuitively, Proposition1shows thattheprincipal strictly prefers todelegate if andonlyif atthe
principal’s interim posterior, the agent’s signal can potentially induce two distinct actions that the
principal would agree with if she were to know the agent’s signal. To achieve this, the principal’s
interim posterior cannot betoo confident such that the agent’s signal cannot provide any “surprise.”
Theagent’s signalalsoneeds tobeinformative enough compared tohismisaligned incentives, which
is highlighted in the following Corollary.
Corollary 2 (Necessary condition for strict delegation). Given a public signal realization s1 and
the associated interim posterior µ , for delegation to be strictly valuable the following must be true:
s1
µ s11−µ s10 > |µ¯ P −µ¯ A|= |I|,
that is, the distance between the agent’s final posteriors must be more than the length of the dis-
agreement interval.
Proof. It follows immediately from Proposition 1.
This Corollary captures the intuitive trade-off between the agent’s private information and
preference misalignment. A necessary condition for the principal to delegate is that the agent’s
signal is informative enough (the distance between posteriors) compared to the degree of preference
misalignment (the length of the disagreement interval).
4 Comparative Statics on Delegation
This section presents the comparative statics results on the principal’s payoffs with respect to a
change in agent’s informativeness or misalignment.
8U U
µs1
Pr(Θ=1) Pr(Θ=1)
0 µs10 µs111 0 µs10 µs1µs11 1
(a) Delegation is not valuable (b) Delegation is not valuable
U U
Pr(Θ=1) Pr(Θ=1)
0 µs10µs1µs11 1 0 µ11 µ1 µ10 1
(c) Delegation is indifferent (d) Delegation is strictly valuable
Figure 4: Value of delegation
4.1 Informativeness of the Agent’s Signal
When the principal decides whether to delegate, one important factor is the informativeness of the
agent’s signal. One may think that the principal is more likely to delegate to an agent with a bigger
informational advantage. Surprising, this intuition does not always hold.
As a signal becomes more informative, Blackwell’s theorem predicts that, for any prior, at least
one of the two posteriors under the new signal will move further away from the prior compared
to the old signal. We are interested in the comparative statics effect of increasing the agent’s
informativeness on two quantities. The first is the principal’s delegation payoff, i.e., the payoff if
she delegates to the agent at a given interim posterior. The second is the principal’s payoff at the
optimal delegation decision, i.e., the payoff when the she chooses optimally between delegation and
taking the utility-maximizing action.
We show that the impact of a more informed agent depends on the location of the interim
posterior. In some regions, the principal’s delegation payoff can strictly decrease when the agent
becomes more informed.
Proposition 3. When the principal’s interim posterior is such that both players agree to take
the principal-preferred action, changing the agent’s signal from not very informative to moderately
9U U
A A
Pr(Θ=1) Pr(Θ=1)
0 µs1 1 0 µs1 1
(a) A less informative agent (b) A more informative agent
Figure 5: Delegation payoff (red diamond point) when agent becomes more informative
informative can strictly reduce the principal’s payoff of delegation.
Proof. We show that there are a class of situations in which Proposition 3 would apply. Denote as
A the range of interim posteriors in which both players agree to take the principal-preferred action.
Suppose that µ
s1
∈ A, and the agent currently receives a signal S2, which induces final posteriors
µ s10 and µ s11. Consider the case where µ s10,µ s11 ∈ A, i.e., when S2 is not too informative so that
the agent’s final posteriors are not far from the interim posterior. In this case, the final posteriors
are co-linear with the interim posterior, so the principal receives the same payoff for delegation and
taking the optimal action.
′
Suppose now the agent receives a signal S that is Blackwell more informative. By Blackwell’s
2
′ ′
theorem, at least one of the final posteriors µ and µ would be strictly further away from the
s10 s11
′
interim posterior µ . Consider S that moves out the posterior closer to the disagreement interval
s1 2
(I) (such a signal must exist once the other posterior arrive at the end point and thus cannot be
moved anymore). When the two final posteriors are still co-linear with the interim posterior, the
payoff of delegation does not change for the principal. However, once one of the posterior falls into
thedisagreementinterval,theprincipal’sexpectedpayoffofdelegationfallsbelowthenon-delegation
envelope (as shown in Figure 5(a)-5(b)).
We show that there exist cases in which delegating to a more informed agent is strictly worse
off for the principal. However, this case only happens when the interim posterior lies in the interval
A and when the agent’s signal changes from not very informative to moderately informative. Even
when µ
s1
∈ A, once S2 becomes informative enough so that neither of the posteriors is in the
disagreement interval, an further increase in S2’s informativeness would strictly increase the payoff
of delegation. In addition, when µ
s1
∈/ A, increasing the informativeness of S2 always weakly
increases the delegation payoff.
This proposition presents an interesting result: holding everything else equal, the principal’s
delegation payoff may decrease when facing a more informed agent. In particular, this happens
whentheprincipal’sinterimposterioraresuchthatbothplayersagreetotaketheprincipal-preferred
action (i.e., the action less preferred by the agent). In this case, the risk of the agent switching to
the suboptimal action for the principal upon receiving a somewhat informative signal outweighs the
benefit of the signal. It is worth noting that this result echoes Alonso and Matouschek [2008], in
10which they show that the principal may give less discretion to an agent with a bigger informational
advantage in some situations.
In contrast, we note that the principal’s payoff at the optimal delegation decision is unaffected.
This is precisely because the principal is indifferent between delegating or not before the agent’s
signal increases in informativeness, so she can always fall back to not delegating and ensure the
same payoff as before.
Proposition 4. At any principal’s interim posterior, an increase in the informativeness of the
agent’s signal weakly improves the principal’s payoff at the optimal delegation decision.
Proof. First, we establishthatthe casewhereamarginal increaseinthe agent’s informativeness can
reduce the principal’s delegation payoff only happens when the principal was indifferent between
delegation and direct actionunder theoriginal signal. To seethis, notethat adecreaseindelegation
payoff results from one of the final posteriors jumping downward at the discontinuity. Therefore,
beforethediscontinuity, thetwofinalposteriorsmustbeco-linearwiththeinterimposterior,making
the principal indifferent between delegation or direct action. In this case, it is weakly optimal for
the principal to take direct action at µ before and after the change in the agent’s informativeness.
s1
Since the payoff of direct action does not change with the agent’s signal, the principal’s optimal
payoff remains constant.
In all remaining cases, a marginal increase in the agent’s informativeness weakly improves the
principal’s delegation payoff. Sincethenon-delegation payoff isunaffected, thepayoff attheoptimal
delegation decision also weakly improves.
Taken together, Propositions 3 and 4 address the question: is more information from the agent
always better? Theanswerdependsonwhether thetoolofdelegationisavailabletotheprincipal. If
the principal is free to take her own actions, then having access to a more informed agent is always
weakly beneficial. If the principal is required to delegate, however, an improvement in agent’s
informativeness does not guarantee a monotonic improvement in the payoff of the principal.
4.2 Degree of Preference Misalignment
The principal’s delegation decision trades off the gain from the agent’s private signal and the loss
from preference misalignment. This section studies how the principal’s payoffs change when the
agent becomes less aligned with the principal.
Definition 5. Holding the principal’s preferences fixed, we say that an agent is more misaligned
if the disagreement interval induced by the preferences of the new agent strict contains that of the
original agent.
Proposition 6. For any principal’s interim posterior and any agent’s signal, a more misaligned
agent weakly decreases the principal’s delegation payoff and payoff at the optimal delegation decision.
′
Proof. Suppose that the original disagreement interval is I and that it expands to I ⊃ I under the
′
new agent. Denote the principal’s original delegation envelope V and new delegation envelope V .
D D
We consider the principal’s delegation payoff. First, consider the µ s1 and S2 such that both µ s10
′ ′ ′
and µ s11 lie outside of the interval I \I. Observe that V D(x) = V D(x) for all x ∈/ I \I. Therefore,
V (µ )= V′ (µ ). This implies that the delegation payoff E [V′ (µ )], a linear combination
D s1s2 D s1s2 s2 D s1s2
′ ′
of V D(µ s10) and V D(µ s11), also stays the same as that under the original agent.
11U
I
Pr(Θ=1)
0 I′ 1
Figure6: Amoremisalignedagent. Theblackplusthebluesectionsrepresenttheoriginaldelegation
′
envelope V . The black plus the red sections represent the new delegation envelope V .
D D
′
Then, we consider the µ s1 and S2 such that at least one of the µ s10 and µ s11 lies in I \ I.
′ ′ ′
Observe that V (x) < V (x) for all x ∈ I \I. Therefore, V (µ )< V (µ ) for at least one of
D D D s1s2 D s1s2
realizations of S2, and stays the same for the other (or neither) realization. The delegation payoff
E s2[V D′ (µ s1s2)], a linear combination of V D′ (µ s10) and V D′ (µ s11), is (strictly) lower than that under
the original agent.
We have proved that the principal’s delegation payoff weakly decreases under a more misaligned
agent. Wenotethattheprincipal’snondelegationpayoffisthesameregardlessoftheagentavailable
to her. Therefore, the principal’s payoff at the optimal delegation decision, which is the maximum
of the two payoffs, also weakly decreases under a more misaligned agent.
Corollary 7. For some principal’s interim posterior and some agent’s signal, a more misaligned
agent strictlydecreases the principal’s delegation payoff andpayoffat the optimaldelegation decision.
Proposition 6 and Corollary 7 also provide clues on the change of the principal’s delegation
behavior when the agent becomes more misaligned. If the principal does not delegate before at
a given interim posterior, she would not delegate when the agent becomes more misaligned. If
the principal delegates before at a given interim posterior, she may no longer delegate to a more
misaligned agent.
So far, we have considered the change in preference misalignment induced by a change in the
agent’s preferences. Itis alsopossiblethat agreater preference misalignment isinduced by achange
in the principal’s preference parameters. In Appendix B, we explore comparative statics results on
the principal’s payoffs under such cases.
5 Information Design
The principal not only decides whether to give decision authority to the agent but also designs the
informational structure of the public signal. In choosing between public signals, she must take into
account the optimal delegation decision in the second stage after the public signal is realized.
We are interested in the optimal design of the public signal conditional on making the optimal
delegation decision in the second stage. If the signal space is unconstrained, it is trivial that
12the principal would simply choose the state-revealing signal and never delegate. After we impose
constraints on the maximal Blackwell informativeness in the signal space, this question becomes
nontrivial: will the principal always choose the most informative signal within those constraints?
We show that the answer is no, even if the principal has the option to use the signal by herself.
We first introduce a few more notations that would facilitate the analysis. Recall that the red
diamond point in Figure 4 represents the principal’s ex-ante delegation payoff, E [V (µ )], at
s2 D s1s2
the interim posterior µ . Holding fixed the agent’s signal and player’s preferences, this value is a
s1
functionoftheprincipal’s interim posterior, µ . Therefore, wecanplotE [V (µ )]asafunction
s1 s2 D s1s2
of µ for any fixed agent’s signal and player’s preferences.
s1
U
µs1
0 1
Figure 7: Principal’s ex-ante delegation payoff as a function of the interim posterior
In Figure 7, principal’s delegation envelope (V ) is plotted as dashed lines and her ex-ante
D
delegation payoff (E [V (µ )]) is plotted as solid lines. Each point on the solid line is the
s2 D s1s2
probability weighted average of two points on the dashed line.4 The red points provide one such
example. Whentheprincipal’spreferenceandtheagent’ssignalaresymmetric(r00 = r11,r01 = r10,
and q0 = q1 = q), averaging produces a constant payoff in the intermediate section. This special
case is proved in Appendix C. In general, the function is a piecewise linear function with three
sections: the left and right sections follow the delegation envelope, while the middle section can be
flat, upward, or downward sloping.
The value of this function represents the principal’s payoff if she were to delegate at a given
interim posterior µ . However, this is only one part of the equation in what the principal can
s1
obtain at the delegation stage. She also has the choice of taking an action directly according to her
interim posterior. Therefore, principal’s optimal delegation-stage payoff is the maximum between
the ex-ante delegation payoff and the payoff on the non-delegation envelope. Mathematically, we
define:
H(µ ):=max{E [V (µ )],V (µ )}
s1 s2 D s1s2 N s1
Figure 8(a)and 8(b)plot theprincipal’s ex-ante delegation payoff, thenon-delegation envelope, and
the resulting H(µ ) as the maximum of the two. It is also possible to have the discontinuity past
s1
the principal’s cutoff point µ¯ , which results in H(µ ) as plotted in Appendix D, but all analyses
P s1
below still go through in such cases.
One important point in the analysis is the point of discontinuity, which we label as (ρ,H(ρ)).
Since we made the assumption that the agent takes the principal-preferred action when indifferent,
4 Theweights are theprobabilities of S2 =0 and S2 =1 conditional on s1.
13U H
(ρ,H(ρ))
0
1µs1
0 ρ
1µs1
(a) E s2(VD(µs1s2)) and VN(µs1) (b) H(µs1)
Figure 8: Principal’s optimal delegation-stage payoff
H is upper semicontinuous at ρ. In words, ρ is the interim posterior such that, upon receiving
another principal-preferred signal realization, the agent’s final posterior equals µ¯ (and thus is
A
indifferent between two actions). Algebraically,
(1−q0)µ¯
A
ρ= if µ¯ > µ¯
A P
(1−q0)µ¯ A+q1(1−µ¯ A)
q0µ¯
A
ρ= if µ¯ < µ¯
A P
q0µ¯ A+(1−q1)(1−µ¯ A)
The principal’s information design problem is thus choosing the public signal that, given prior µ
and Blackwell constraint λ, generates two interim posteriors µ0 and µ1 such that H(µ0) and H(µ1)
give the maximal expected payoff. Recall that we denote the posteriors generated by the maximally
informative signal subject to Blackwell constraint λ as µλ and µλ. We are now ready to state our
0 1
main proposition for the optimal information design.
Proposition 8 (Optimal information design). Given a prior µ and a Blackwell constraint λ, the
principal chooses the most informative public signal if and only if there exists a weakly convex and
continuous function c such that the points (µλ,H(µλ)), (µλ,H(µλ)), and (ρ,H(ρ)) are in the graph
0 0 1 1
of c.5 If this is the case, we say that these three points can be “convexified”.
If they cannot be convexified,
1. If µ > ρ, µ∗ = µλ and µ∗ = ρ. The optimal signal generates the high posterior at the Blackwell
1 1 0
constraint and the low posterior at ρ.
2. If µ < ρ, µ∗ = µλ and µ∗ = ρ. The optimal signal generates the low posterior at the Blackwell
0 0 1
constraint and the high posterior at ρ.
∗ ∗
3. If µ = ρ, µ = µ = µ. The optimal signal is completely uninformative.
1 0
Proof. First, if the posteriors of the maximally informative signal lie on the same side of the discon-
tinuity at ρ, then it is trivial that they can be convexified with (ρ,H(ρ)) as one of the end points
5 Fortheif direction, weneed toassumethat theprincipalchooses themost informative signal when indifferent.
14H H
(µ0,H(µ0)) (µ1,H(µ1))
(ρ,H(ρ)) (ρ,H(ρ))
(µ1,H(µ1))
(µ0,H(µ0))
0 µ0 ρ µ µ1
1µs1
0 µ0 ρ µ µ1
1µs1
(a) Can be convexified (b) Cannot be convexified,optimal µ∗ 0 =ρ
H (µ0,H(µ0)) H
(ρ,H(ρ)) (ρ,H(ρ))
(µ1,H(µ1)) (µ1,H(µ1))
(µ0,H(µ0))
0 µ0 µρ µ1
1µs1
0 µ0 µρ µ1
1µs1
(c) Can beconvexified (d) Cannot be convexified,optimal µ∗ 1 =ρ
H H (µ0,H(µ0))
(ρ,H(ρ))
(µ1,H(µ1))
(ρ,H(ρ))
(µ0,H(µ0))
(µ1,H(µ1))
0 ρ µ0 µ µ1
1µs1
0 µ0 µ µ1 ρ
1µs1
(e) Trivially convexified (f) Trivially convexified
Figure 9: Optimal information design
15(Figure 9(e), 9(f)). In this case, the maximal signal is at least weakly optimal since expanding
posteriors on a weakly convex function generates a weakly higher expected payoff.
Next, we consider the case when the two posteriors lie on different sides of the discontinuity
at ρ. Observe that in this case the principal’s expected payoff always weakly increases when the
two posteriors marginally move away from the prior (when the signal becomes more Blackwell
informative). Therefore, the maximal signal gives the highest expected payoff among all the signals
that generate posteriors on different sides of (ρ,H(ρ)). What we need to compare now is the payoff
of the maximal signal with the payoff of having one of the posteriors stop at ρ.
Suppose that the three points can be convexified, with (ρ,H(ρ)) in the middle (Figure 9(a),
9(c)). The convexity of the connecting function ensures that the line segment between the two
posteriors lies weakly above the line segment connecting either of the posterior with (ρ,H(ρ)).
Therefore, no matter where the prior, the expected payoff generated by the maximal signal exceeds
that generated by placing one of the posteriors at ρ. Suppose instead that there exists no weakly
convex and continuous function connecting the three points. In the case of three points, it has
to be the case that they can be connected by a strictly concave function. Then, the line segment
between the two posteriors lies strictly below the line segment connecting one of the posteriors with
(ρ,H(ρ)). This proves that restricting one posterior to ρ produces strictly higher expected payoff
than the maximal signal. How about moving the posterior even closer to µ? This is not optimal
since the section of the curve between ρ and µ is now weakly convex so contracting the posteriors
gives a weakly lower payoff.
Why is the point (ρ,H(ρ)) the optimal posterior when the principal does not give out the most
informative signal? As argued before, placing one of the interim posteriors at ρ would make the
agentindifferentbetweenthetwoactionsifhereceivesaprincipal-preferred realizationoftheprivate
signal. Therefore, he would take the principal-preferred action out of indifference. If instead, the
agent strictly prefers to take the principal-preferred action, then the principal can increase the
probability of the principal-preferred realization in the agent-preferred state and still ensures that
agent takes the same action.
For example, supposethat theprincipal preferred action is 1. Ifthe agent strictly prefers totake
action1whenobserving(S1 = 1,S2 = 1), thentheprincipal couldslightly decrease Pr(S1 = 0 |Θ =
0) and increase Pr(S1 = 1| Θ = 0) to lower the interim posterior µ11 back to ρ, while still ensuring
that the agent takes action 1 when observing (S1 = 1,S2 = 1). The principal has thus constructed
a signal with higher chance of realizing S1 = 1 while leaving the agent’s optimal action given the
signal realization unchanged, and thus strictly increased her payoff. This is a generalization of the
indifference result in Kamenica and Gentzkow [2011] applied to our setting where the receiver can
receive an additional signal.
5.1 Features of the Optimal Information Design
This section highlights features of the optimal information design that may inform the design of
real-life algorithms that interact with misaligned agents.
Maximizing prediction accuracy in the presence of a biased agent. Oneimplicationofthe
optimal information design is that whether one should provide the most informative public signal
depends on what the maximally informative signal can achieve. For a given prior, if the maximal
signal is sufficiently uninformative, then there is no harm in providing it because: 1) the principal
16canuseitinthedelegation stage; and2)eveniftheprincipal decides todelegate, thepublicsignalis
notstrongenoughtoswaytheagent’sdecision. Ifthemaximalsignalcangenerateinterimposteriors
sufficiently confident (near the end points), then it is strictly optimal to maximize informativeness
because theprincipal and the agent tend toagree near theextreme beliefs. However, if the maximal
signal only provides moderately confident interim posteriors, it is usually optimal for the principal
todelegate in orderto obtain moreinformation. Given thatthe principal will delegate inthesecond
stage, a “skewed” signal designed to persuade the agent can do better than providing the maximally
informative signal.
One-sided information. In all but the the edge case (µ = ρ), the optimal information structure
maximizes the signal strength on one side. Specifically, when the prior is on the right (left) of the
discontinuity point ρ, theoptimal signalplaces theposteriorattheBlackwell constraint ontheright
(left)side. Thisimpliesdifferentinvestmentdecisionsbetweenloweringanalgorithm’sfalsenegative
rate (1−p1) versus false positive rate (1−p0) when we care about quality of the the final decisions
rather than that of the predictions. For example, if µ > ρ, the optimal signal maximizes the high
posterior µ1 while potentially restricting the low posterior µ0. This implies that the optimal signal
features a very low false positive rate and a moderate false negative rate.
6 Policy restrictions and decision quality
In our model, the principal can utilize the tools of persuasion and delegation to mitigate the agent’s
preference misalignment. First, as she designs the information structure of the public signal, she
can take into account the possibility of passing the information and the decision to the misaligned
agent. Second, upon each signal realization, she can decide whether to delegate the decision to the
agent or to act directly based on the current information.
In real-life applications, however, the principal may have limited access to these tools due to
legal, institutional, or moral constraints. For example, Article 14 of the EU AI act requires that
high-riskAIsystemsmustbedesignedsuchthattheycanbe“effectivelyoverseenbynaturalpersons”
[European Commission,2021]. Inthecontext ofourmodel, it implies thattheprincipal isrestricted
to always delegate. In other applications, the algorithm may be required to be “truthful,” meaning
it simply provides the most informative signal regardless of the human it interacts with. Indeed,
in most traditional applications of decision-aid algorithms, neither of these strategic tools were
available. The algorithm is no more than a prediction tool with no potential to persuade nor any
authority to take actions by itself.
Indeed, these policies, such as designing maximally informative algorithms and obtaining ad-
ditional information from human agents, improves decision quality when there is no preference
misalignment. However, in situations where there is an interaction between the potential biases
of the agent and information provision, our analysis show that these naive policies may in fact be
welfare worsening for the principal. This is formalized in the following propositions.
Proposition 9. If µ¯
P
6= µ¯
A
and q0,q1 < 1, there exists interim posterior µ
s1
at which delegating
to the agent is strictly worse than taking direct action.
Proof. The principal benefits from the option to not delegate when at the interim posterior, the
ex-ante payoff of delegation is strictly lower than the non-delegation envelope. In Figure 8(a), it is
17thepartwhenthesolidline(ex-ante payoff ofdelegation) isstrictlylower thanthedashedline(non-
delegation envelope). This part of sub-optimal delegation is generated by averaging between two
final posteriors, one of which falls into the disagreement interval. Unless µ¯ = µ¯ , the disagreement
P A
interval exists. If q0,q1 < 1, the agent’s final posterior can fall into the disagreement interval.
Proposition 10. If µ¯
P
6= µ¯
A
and q0,q1 < 1, there exists prior µ and Blackwell constraint λ such
that providing no algorithm is strictly better than providing the maximally informative algorithm.
Proof. Providing no (or a completely uninformative) algorithm can be better off than providing the
maximal algorithm whenever there is a discontinuity in the optimal delegation-stage payoff and the
principal’s prior lies in the flat region of H (as in Figure 9(b)). This discontinuity is a product
of the agent’s final posteriors having a discontinuous jump on the delegation envelope V . Unless
D
µ¯
P
= µ¯ A, the discontinuity in V
D
exists. If q0,q1 < 1, the agent’s final posteriors can fall near
the discontinuity and create non-convexity in payoffs. In that case, the principal may be better off
providing no algorithm and enjoying the delegation payoff at the prior.
Unless the agent is perfectly aligned or one of his signal realizations is state-revealing, Proposi-
tions 9 and 10 imply that there exist cases where it is strictly beneficial to remove the agent from
the decision-making process or to provide no algorithmic assistance to the agent. These theoretical
results may explainthe empirical puzzle of why theintroduction of highly predictive decision-aid al-
gorithms often leads to underperforming human-machine collaborations. For example, Jacobs et al.
[2021] find that in the medical domain, algorithmic aids did not enhance clinicians’ accuracy in
choosing antidepressants, with both aided and unaided clinicians underperforming compared to
standalone machine-learning systems. Likewise, Imai et al. [2021] and Stevenson and Doleac [2022]
observe in the judicial context that risk assessment tools barely influenced judges’ decisions on
pretrial detention and sentencing, failing to notably improve public safety or lower incarceration
rates. While these findings may seem puzzling, this paper suggests that the underperformance of
human-machine collaborations is not only understandable, but even expected, if no measures are
taken to mitigate preference misalignment between algorithm and expert.
Note that possible misalignment of the agent includes not only any explicit racial or gender
biases but also common behavioral biases such as risk aversion, complexity aversion, or preference
for inaction. Additionally, the agent must be a Bayesian agent who correctly updates his beliefs
upon receiving multiple sources of information. We argue that these conditions are hard to satisfy
in real-life human agents. In that case, implementing naive policies that are seemingly conducive
to better decision making may in fact worsen decision quality and incur welfare losses for society.
7 Discussion and Future Work
Inthis paper we consider the joint decision problem of designing prediction algorithms and deciding
when to delegate to a privately informed and biased agent. We develop conditions under which
delegating to the agent is strictly optimal, and study the change in the principal’s payoffs when
the agent becomes better informed or more misaligned. We show that given the optimal delegation
decision, the optimal algorithm may not be maximally informative. In particular, the optimal
algorithm maximizes information about one state while restricting information about the other. In
the absence of perfectly aligned agents and state-revealing signals, we show that there exist cases
18where it is strictly beneficial to remove the agent from the decision-making process or to provide no
algorithmic assistance.
Our characterization highlights that naive policies, such as designing maximally informative
algorithms and mandating delegation to human agents, may strictly worsen decision quality for
some decision subjects. This implies that even well-intentioned policies aiming to provide more
information can produce counter-intuitive results if we fail to account for the strategic interac-
tion between information and biases. Echoing recent works in the Economics and Computation
area [Xu and Dean, 2023, McLaughlin and Spiess, 2023], our results once again demonstrate that
good algorithmic predictions do not directly translate to good final decisions. We suggest that the
underperformance of human-machine collaborations widely observed in empirical settings can be
understood through this theoretical lens.
We recognize that an important direction for future work is to go beyond theoretical charac-
terizations and estimate human agents’ biases and information from data. Separately identifying
agents’ biases from their private information can be challenging, as shown in Rambachan et al.
[2021]. However, it may be possible with quasi-experimental or experimental choice data observed
in human-machine collaborative settings. Related to this, it could be worthwhile to empirically
discern agents’ preference biases (due to different objectives) from updating biases (due to incorrect
use of algorithmic predictions). Finally, given that the optimal algorithm interacting with biased
agents may withhold information about some states, it would be important to test experimentally
whether a strategic algorithm can be detected by users who may then stop trusting the algorithm.
References
Ajay Agrawal, Joshua Gans, and Avi Goldfarb. Prediction, judgment, and complexity: a theory of
decision-making and artificial intelligence. In The economics of artificial intelligence: An agenda,
pages 89–110. University of Chicago Press, 2018.
Ricardo Alonso and Niko Matouschek. Optimal Delegation. The Review of Economic Studies, 75
(1):259–293, 2008. ISSN 0034-6527. URL https://www.jstor.org/stable/4626195. Publisher:
[Oxford University Press, Review of Economic Studies, Ltd.].
Manuel Amador and Kyle Bagwell. The theory of optimal delegation with an application to tariff
caps. Econometrica, 81(4):1541–1599, 2013.
David Blackwell. Comparison of experiments. In Proceedings of the second Berkeley symposium on
mathematical statistics and probability, volume 2, pages 93–103. University of California Press,
1951.
David Blackwell. Equivalent comparisons of experiments. The annals of mathematical statistics,
pages 265–272, 1953.
Hao-Fei Cheng, Logan Stapleton, Anna Kawakami, Venkatesh Sivaraman, Yanghuidi Cheng, Diana
Qing, Adam Perer, Kenneth Holstein, Zhiwei Steven Wu, and Haiyi Zhu. How Child Welfare
Workers Reduce Racial Disparities in Algorithmic Decisions. In Proceedings of the 2022 CHI
Conference on Human Factors in Computing Systems, CHI ’22, pages 1–22, New York, NY,
USA, April 2022. Association for Computing Machinery. ISBN 978-1-4503-9157-3. doi: 10.1145/
3491102.3501831. URL https://doi.org/10.1145/3491102.3501831.
19Kate Donahue, Alexandra Chouldechova, and Krishnaram Kenthapadi. Human-Algorithm
Collaboration: Achieving Complementarity and Avoiding Unfairness, June 2022. URL
http://arxiv.org/abs/2202.08821. arXiv:2202.08821 [cs].
Ozkan Eren and Naci Mocan. Emotional Judges and Unlucky Juveniles. American Economic
Journal: Applied Economics, 10(3):171–205, July 2018. ISSN 1945-7782. doi: 10.1257/app.
20160390. URL https://www.aeaweb.org/articles?id=10.1257/app.20160390.
European Commission. Eu artificial intelligence act, Apr 2021. URL
https://artificialintelligenceact.eu/.
Bengt Holmström. On the theory of delegation. Technical report, Discussion Paper, 1980.
Bengt Robert Holmström. On Incentives and Control in Organizations. Stanford University, 1978.
Kosuke Imai, Zhichao Jiang, James Greiner, Ryan Halen, and Sooahn Shin. Experimental Eval-
uation of Algorithm-Assisted Human Decision-Making: Application to Pretrial Public Safety
Assessment, December 2021. URL http://arxiv.org/abs/2012.02845. arXiv:2012.02845 [cs,
stat] version: 4.
Maia Jacobs, Melanie F. Pradier, Thomas H. McCoy, Roy H. Perlis, Finale Doshi-Velez,
and Krzysztof Z. Gajos. How machine-learning recommendations influence clinician treat-
ment selections: the example of antidepressant selection. Translational Psychiatry, 11
(1):1–9, February 2021. ISSN 2158-3188. doi: 10.1038/s41398-021-01224-x. URL
https://www.nature.com/articles/s41398-021-01224-x. Number: 1 Publisher: Nature Pub-
lishing Group.
Emir Kamenica and Matthew Gentzkow. Bayesian Persuasion. American Economic Review,
101(6):2590–2615, October 2011. ISSN 0002-8282. doi: 10.1257/aer.101.6.2590. URL
https://www.aeaweb.org/articles?id=10.1257/aer.101.6.2590.
Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mul-
lainathan. Human Decisions and Machine Predictions*. The Quarterly Journal of
Economics, August 2017. ISSN 0033-5533, 1531-4650. doi: 10.1093/qje/qjx032. URL
http://academic.oup.com/qje/article/doi/10.1093/qje/qjx032/4095198/Human-Decisions-and-Machine-
Vivian Lai, Chacha Chen, Q Vera Liao, Alison Smith-Renner, and Chenhao Tan. Towards a science
of human-ai decision making: a survey of empirical studies. arXiv preprint arXiv:2112.11471,
2021.
Yichuan Lou. Optimal Delegation with Information Manipulation, October 2022. URL
https://papers.ssrn.com/abstract=4298900.
BryceMcLaughlinandJannSpiess. AlgorithmicAssistancewithRecommendation-Dependent Pref-
erences.InProceedings ofthe24thACMConferenceonEconomicsandComputation,EC’23,page
991,NewYork,NY,USA,July2023.AssociationforComputingMachinery. ISBN9798400701047.
doi: 10.1145/3580507.3597775. URL https://doi.org/10.1145/3580507.3597775.
20Sendhil Mullainathan and Ziad Obermeyer. Diagnosing Physician Error: A Machine Learn-
ing Approach to Low-Value Health Care. The Quarterly Journal of Economics, 137(2):
679–727, April 2022. ISSN 0033-5533, 1531-4650. doi: 10.1093/qje/qjab046. URL
https://academic.oup.com/qje/article/137/2/679/6449024.
AsheshRambachan etal. Identifyingprediction mistakes inobservational data. Harvard University,
2021.
Charvi Rastogi, Liu Leqi, Kenneth Holstein, and Hoda Heidari. A Unifying Framework for Com-
bining Complementary Strengths of Humans and ML toward Better Predictive Decision-Making,
August 2022. URL http://arxiv.org/abs/2204.10806. arXiv:2204.10806 [cs].
Megan T.Stevenson and Jennifer L.Doleac. Algorithmic RiskAssessmentinthe Hands of Humans,
September 2022. URL https://papers.ssrn.com/abstract=3489440.
Eleni Straitouri, Lequn Wang, Nastaran Okati, and Manuel Gomez Rodriguez. Improving expert
predictions with prediction sets. arXiv preprint arXiv:2201.12006, 2022.
Maren Vairo. The value of information in delegation. 2023.
Ruqing Xu and Sarah Dean. Decision-aid or Controller? Steering Human Decision Makers with
Algorithms, March 2023. URL http://arxiv.org/abs/2303.13712. arXiv:2303.13712 [cs].
21A Justification of Assumption 1
Assumption 1 assumes that the principal’s payoffs satisfy r00 > r01 and r11 > r10. Similarly, the
agent’s payoffs satisfy v00 > v01 and v11 > v10. We will show that this assumption is without loss of
generality, since the principal never strictly prefers delegation when this assumption is not satisfied.
We start with the principal. Suppose that one of the inequality is not satisfied for the principal,
say, r00 > r01 but r11 ≤ r10. In this case, the payoff of taking action 0 in any state is weakly higher
than that of taking action 1 in the same state. In other words, the principal has a dominant action
0. Therefore, it is never strictly valuable for the principal to delegate the decision to the agent
because the principal would maximize her payoff by taking the dominant action.
Suppose that neither inequalities are satisfied for the principal, i.e., r00 ≤ r01 and r11 ≤ r10. In
this case, the principal has a preference for “mismatching the state.” In particular, the principal’s
preferred action switches from 1 to 0 as her belief of the probability of state 1 increases. We further
suppose that the agent has the normal “state-matching” preferences. Then, the “disagreement
interval (I)” would in fact be two disjoint intervals at the two ends of the unit interval (as shown in
Figure 10). The principal’s payoff envelope in this case is also depicted in Figure 10. It is easy to
see that the principal cannot strictly improve her payoff via delegation, since no linear combination
on the delegation envelope can be above the non-delegation envelope.
U
I I
Pr(Θ=1)
0 µ¯P µ¯A 1
Figure 10: Principal’s payoff envelope of delegation with “state-mismatching” preferences
We now turn to the assumption on the agent. Same as the principal, the agent would have a
dominant action if only one of the inequality for the agent is not satisfied. It is then obvious that
the principal cannot have strict improvements via delegation if the agent always take one action.
If neither inequalities are satisfied for the agent, the problem is symmetric with that of the
principal. The “disagreement” interval would still be disjoint and the principal’s payoff envelope
lookslikethemirrorimageofFigure10. Itfollowsthattheprincipalcannothavestrictimprovements
by delegation.
Finally, if none of the four inequalities are satisfied, i.e., both players have “state-mismatching”
preferences, then with relabelling the actions, the analysis in the main text will go through.
22B Comparative statics on preference misalignment induced by the
principal
In this appendix, we consider the cases when the disagreement interval expands not because of a
change in the agent’s preferences but that of the principals. There are two ways this can happen,
as shown in Figure 11: (1) decrease the principal payoff for the agent’s preferred action, and (2)
increase the principal’s payoff for the agent’s less preferred action.
U U
I I
Pr(Θ=1) Pr(Θ=1)
0 I′ 1 0 I′ 1
(a) Expanding the disagreement interval through (b) Expanding the disagreement interval through
(1) (2)
Figure 11: Principal-induced increase in preference misalignment
Proposition 11. For any principal’s interim posterior and any agent’s signal, both the princi-
pal’s payoff of delegation and payoff at the optimal delegation decision weakly decreases when the
disagreement interval expands through (1), and weakly increases when the disagreement interval
expands through (2).
′
Proof. Suppose that the original disagreement interval is I and that it expands to I ⊃ I under the
′
new agent. Denote the principal’s original delegation envelope V and new delegation envelope V .
D D
′
Denote the principal’s original nondelegation envelope V and new nondelegation envelope V .
N N
Weobservethatincase(1),boththeprincipal’sdelegation andnondelegationenvelopeisweakly
′ ′
lower than before, i.e., V (x) ≤ V (x) and V (x) ≤ V (x) for all x. Since the principal’s payoff of
D D N N
delegation is a linear combination of two points on the delegation envelope, it is also weakly lower
than before. Similarly, the principal’s payoff of nondelegation is a linear combination of two points
on the nondelegation envelope, which is weakly lower than before. The principal’s payoff at the
optimal delegation decision, being the maximum of the two, is thus weakly lower than before.
Case (2) is symmetric in that both the principal’s delegation and nondelegation envelope is
′ ′
weakly higher than before, i.e., V (x) ≥ V (x) and V (x) ≥ V (x) for all x. By a symmetric
D D N N
argument, both the principal’s payoff of delegation and payoff at the optimal delegation decision is
weakly higher than before.
C Derivation of the principal’s ex-ante delegation payoff
This appendix derives the general formula for the intermediate section of the principal’s ex-ante
delegation payoff function. It also shows that the intermediate section will be flat if the principal’s
23payoffs are symmetric.
Since we are considering the posteriors and signals in the delegation stage, we can simplify the
notations to omit the mentioning of signal s1. We redefine:
µ := µ s1, µ0 := µ s10, µ1 := µ s11
Being in the intermediate section means that each point is produced by averaging two points on
the left and right part of the principal’s delegation envelope. Let g be the principal’s delegation
envelope.
g(µ0)= a+bµ0, g(µ1)= c+dµ1
The slope of the line segment connecting the two points is:
a+bµ0−c−dµ1
µ0−µ1
We can write out the function of the connecting line segment h:
a+bµ0−c−dµ1
h(x) = (x−µ1)+c+dµ1
µ0−µ1
Evaluate h at the prior µ:
a+bµ0−c−dµ1
h(µ) = (µ−µ1)+c+dµ1
µ0−µ1
It’s useful to write the prior as an weighted average of two posteriors:
µ = w1µ0+(1−w1)µ1, w1 = q0(1−µ)+(1−q1)µ
Substitute in µ:
a+bµ0−c−dµ1
h(µ) = (w1µ0+(1−w1)µ1−µ1)+c+dµ1
µ0−µ1
a+bµ0−c−dµ1
= (µ0−µ1)w1+c+dµ1
µ0−µ1
= (a+bµ0−c−dµ1)w1+c+dµ1
= c+(a−c+bµ0)w1+dµ1(1−w1)
= c+(a−c+bµ0)w1+d(µ−w1µ0)
= c+(a−c+(b−d)µ0)w1+dµ
Substitute in µ0 and w1 and simplify:
h(µ) = c+(a−c)q0+(a−c)(1−q0−q1)µ+(b−bq1+dq1)µ
The slope of h(µ) with respect to µ is thus:
s= (a−c)(1−q0−q1)+(b−bq1+dq1)
We can also express them in terms of model primitives by substituting a = r00, b = r10 − r00,
c = r01, and d = r11−r01:
s = r10−r01+(r01−r00)q0+(r11 −r10)q1
When the principal’s payoff is symmetric (r01 = r10 and r11 = r00) as well as when the public
signal is symmetric (q0 = q1), we have that
s = 0, h(µ) = q.
24D The optimal delegation-stage payoff when ρ > µ¯
P
H
µs1
0 1
Figure 12: Principal’s optimal delegation-stage payoff
25