Reinforcement Learning from Human Feedback with
Active Queries
Kaixuan Ji∗† and Jiafan He∗‡ and Quanquan Gu§
Abstract
Aligning large language models (LLM) with human preference plays a key role in building
modern generative models and can be achieved by reinforcement learning from human feedback
(RLHF). Despite their superior performance, current RLHF approaches often require a large
amount of human-labelled preference data, which is expensive to collect. In this paper, inspired
by the success of active learning, we address this problem by proposing query-efficient RLHF
methods. We first formalize the alignment problem as a contextual dueling bandit problem and
design an active-query-based proximal policy optimization (APPO) algorithm with an O(cid:101)(d2/∆)
regret bound and an O(cid:101)(d2/∆2) query complexity, where d is the dimension of feature space and
∆ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version
of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning
LLMs. Our experiments show that ADPO, while only making about half of queries for human
preference, matches the performance of the state-of-the-art DPO method.
1 Introduction
Recent breakthroughs in large language models (LLM) significantly enhance performance across a
diverse range of tasks, including commonsense reasoning, world knowledge, reading comprehension,
math, code, and popular aggregated results (Jiang et al., 2023; Touvron et al., 2023; Chiang et al.,
2023; Tunstall et al., 2023). In addition to their amazing capabilities in traditional natural language
tasks (Gao et al., 2023a; Yuan et al., 2023; Han et al., 2023; Wei et al., 2023), they also demonstrate
great potential in responding to human queries (Ouyang et al., 2022). One key step towards building
these models is aligning them with human preference, where reinforcement learning from human
feedback (RLHF) (Casper et al., 2023; Ouyang et al., 2022; Ziegler et al., 2019; Christiano et al.,
2017; Rafailov et al., 2023) is widely employed. Typically, the process of RLHF is described as
follows: At each time, the human user prompts the LLM with an instruction. Subsequently, the
model generates several candidate responses and queries the users for their preferences. A reward
model is then trained on this preference data to mimic human evaluation. The reward model can be
parameterized by the language model itself (Rafailov et al., 2023) or by other neural networks (Gao
∗Equal Contribution
†Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail:
kaixuanji@cs.ucla.edu
‡Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail:
jiafanhe19@ucla.edu
§DepartmentofComputerScience,UniversityofCalifornia,LosAngeles,CA90095,USA;e-mail: qgu@cs.ucla.edu
1
4202
beF
41
]GL.sc[
1v10490.2042:viXraet al., 2023b; Munos et al., 2023). The language models are then updated using reinforcement
learning (RL) algorithms such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) to
optimize responses that maximize the reward.
Despite the notable success of RLHF in aligning language models with human preferences, its
practical implementation often necessitates significant amounts of human-labeled preference data.
For instance, the fine-tuning process of zephyr-7b-beta through RLHF relies on the utilization of
a sizable 62k UltraCat-binarized dataset (Ding et al., 2023). The collection of such a substantial
volume of human preference data is both costly and inefficient. Therefore, there exists a pressing
need to develop query-efficient RLHF methods for effectively aligning large language models with
human preferences.
Following recent theoretical advancements in RLHF (Xiong et al., 2023; Zhu et al., 2023; Sekhari
et al., 2023), we formulate the RLHF problem as a contextual dueling bandit problem (Yue et al.,
2012; Wu and Liu, 2016; Saha, 2021; Saha and Krishnamurthy, 2022; Saha and Gaillard, 2022; Wu
et al., 2023; Di et al., 2023). In this setting, the learner proposes a pair of actions and receives noisy
feedback regarding the preference between the dueling pair for each round. While numerous studies
address regret minimization in dueling bandits, only Sekhari et al. (2023) have considered query
complexity. However, the regret incurred by their algorithm exhibits a linear dependency on the size
of the action set A, limiting the practical applicability of their method.
In this paper, we adopt the principles of active learning (Zhang and Oles, 2000; Hoi et al.,
2006) to design a query-efficient algorithm, Active Proximal Policy Optimization (APPO) for linear
contextual dueling bandits. In each round, APPO employs the maximum likelihood estimator (Di
et al., 2023) to estimate the underlying parameter and constructs an optimistic estimator for the
reward gap between different arms. Subsequently, APPO selects the best arms and estimates
the uncertainty associated with the potential feedback. To reduce the query complexity, APPO
selectively queries the dueling preference and updates the estimation only when the uncertainty of
the observation exceeds a threshold.
We further extend APPO to direct preference optimization (DPO) (Rafailov et al., 2023) and
introduce a novel query-efficient method, Active Direct Policy Optimization (ADPO). Following
the methodology of APPO, ADPO selectively queries human preference only for data where the
model exhibits high uncertainty about the observation. For data where the model is less uncertain
about, we employ the pseudo label predicted by the model to fine-tune the model itself.
• We propose an active-learning based algorithm APPO for linear contextual dueling bandits with a
global sub-optimal gap. Theoretical analysis shows that our algorithm enjoys a constant regret
O(cid:101)(d2/∆)1. Meanwhile, our proposed algorithm only requires O(cid:101)(d2/∆2) queries in total T rounds,
where d is the dimension of the feature mapping, and ∆ is the sub-optimal gap. Compared with
√
previous regret bound O(cid:101)(min{ ATβ,A2β2d/∆}) achieved by Sekhari et al. (2023), our regret
bound is independent on the size of action space A, which is more favorable in practice.
• We propose an active learning-based DPO method, ADPO. We apply our method to train
zephyr-7b-beta on UltraCat-binarized dataset (Ding et al., 2023). Our experiment shows while
ADPO only make about half numbers of queries, the model trained by ADPO outperforms DPO
on Open-LLM-Benchmark (Beeching et al., 2023) by a margin of 0.35%.
1we use notation O(cid:101)(·) to hide the log factor other than number of rounds T
2Notation. We employ [n] to denote the set {1,...,n}. In this work, we use lowercase letters
to represent scalars, and denote vectors and matrices by lower and uppercase boldface letters
respectively. Given a vector x ∈ Rd, we denote the vector’s L -norm by ∥x∥ . We further define
√ 2 2
∥x∥ = x⊤Σx given a positive semidefinite matrix Σ ∈ Rd×d. We use standard asymptotic
Σ
notations O(·),Ω(·),Θ(·), and further use O(cid:101)(·) to hide polylogarithmic factors other than the number
of rounds T. We use 1{·} denote the indicator function,
2 Related Work
2.1 Reinforcement Learning from Human Feedback
LearningfromhumanpreferencedatadatesbacktoWirthetal.(2017);Christianoetal.(2017)andis
recently popularized by generative language models (Achiam et al., 2023; Touvron et al., 2023). This
procedure usually takes place after supervised finetuning (SFT). The canonical procedure of aligning
with human preference includes two steps: reward modeling and reward maximization (Ouyang
et al., 2022; Bai et al., 2022; Munos et al., 2023). Another approach is direct preference optimization
(DPO) (Rafailov et al., 2023), which treats the generative models directly as reward models and
trains them on preference data. Compared with the first approach, DPO simplifies the alignment
process while maintaining its effectiveness. However, both paradigms require a large amount of
human preference data. In this work, we follow the DPO approach and propose a query-efficient
DPO.
TheempiricalsuccessofRLHFalsopromptsaseriesoftheoreticalworks,withapredominantfocus
ontherewardmaximizationstage,modelingthisprocessaslearningaduelingbandit(Zhuetal.,2023;
Xiong et al., 2023; Sekhari et al., 2023). Among these works, Sekhari et al. (2023) stands out for con-
√
sidering query complexity in the process. However, their regret bound is O(cid:101)(min{ ATβ,A2β2d/∆}),
depending on the size of the action set A, thereby limiting the practical applicability of their
algorithm.
2.2 Dueling Bandits
Dueling bandits represent a variant of the multi-armed bandit, incorporating preference feedback
between two selected arms (Yue et al., 2012). Existing results in this domain generally fall into two
categories, shaped by their assumptions about preference probability. The first category of work (Yue
et al., 2012; Falahatgar et al., 2017, 2018; Ren et al., 2019; Wu et al., 2022; Lou et al., 2022) assumes
a transitivity property for preference probability and focuses on identifying the optimal action. Our
work also belongs to this category. The second category of work (Jamieson et al., 2015; Heckel et al.,
2018; Saha, 2021; Wu et al., 2023; Dudík et al., 2015; Ramamohan et al., 2016; Balsubramani et al.,
2016) focuses on general preferences with various criteria for optimal actions, such as Borda winner
and Copeland winner.
Expanding beyond the standard dueling bandit problem, Dudík et al. (2015) was the first to
incorporate contextual information into the dueling bandit framework. Subsequently, Saha (2021)
studiedtheK-armcontextualduelingbanditproblemandproposedanalgorithmwithanear-optimal
regret guarantee. In order to address the challenge of a potentially large action space, Bengs et al.
(2022) also considered linear function approximation and extended these results to the contextual
√
linear dueling bandit problem and obtained a regret guarantee of O(cid:101)(d T). Recently, Di et al. (2023)
3introduced a layered algorithm, improving the results to a variance-aware guarantee of
O(cid:101)(d(cid:112)(cid:80)
σ2),
t
where σ2 denotes the variance of the observed preference in round t.
t
2.3 Active Learning
To mitigate the curse of label complexity, active learning serves as a valuable approach in supervised
learning . The first line of work is pool-based active learning (Zhang and Oles, 2000; Hoi et al., 2006;
Gu et al., 2012, 2014; Citovsky et al., 2021). In pool-based active learning, instead of acquiring
labels for the entire dataset, the learner strategically selects a batch of the most informative data
at each step and exclusively queries labels for this selected data batch. The learner then employs
this labeled data batch to update the model. Subsequently, guided by the enhanced model, the
learner queries another mini-batch of labels and continues the training process. These steps are
iteratively repeated until the model achieves the desired performance level. The strategic selection of
informative data in active learning significantly reduces the label complexity for supervised learning.
The label complexity of pool-based active learning has been extensively studied by Dasgupta (2005);
Dasgupta et al. (2005); Balcan et al. (2006, 2007); Hanneke and Yang (2015); Gentile et al. (2022).
On the other hand, selective sampling (a.k.a., online active learning) (Cesa-Bianchi et al., 2005, 2006,
2009; Hanneke and Yang, 2021) is a learning framework that integrates online learning and active
learning. In this framework, the algorithm sequentially observes different examples and determines
whether to collect the label for the observed example. minimizing the total regret with more collected
data
Another line of research (Schulze and Evans, 2018; Krueger et al., 2020; Tucker et al., 2023)
focused on active reinforcement learning and directly integrates the query cost into the received
reward. Krueger et al. (2020) laid the groundwork for active reinforcement learning by introducing a
cost c associated with each reward observation and evaluated various heuristic algorithms for active
reinforcement learning. Recently, Tucker et al. (2023) studied the multi-arm bandit problem with
costly reward observation. Their work not only suggests empirical advantages but also proves an
O(T2/3) regret guarantee.
3 Preliminaries
Inthiswork, weformulatetheRLHFproblemasacontextualduelingbanditproblem(Saha,2021;Di
et al., 2023). We assume a context set X, and at the beginning of each round, a contextual variable
x is i.i.d generated from the context set X following the distribution D. Based on the context x ,
t t
the learner then chooses two actions y1,y2 from the action space A and determines whether to query
t t
the environment for preferences between these actions. If affirmative, the environment generates the
preference feedback o with the following probability P(o = 1|x ,y1,y2) = µ(cid:0) r(x ,y1)−r(x ,y2)(cid:1),
t t t t t t t t t
where µ(·) : R → [0,1] is the link function and r(·,·) is the reward model.
We consider the linear reward model, i.e., r(x,y) = ⟨θ∗,ϕ(x,y)⟩, where θ∗ ∈ Rd and ϕ :
X × A → Rd is a known feature mapping. For the sake of simplicity, we use ϕ1,ϕ2 to denote
t t
ϕ(x ,y1),ϕ(x ,y2). Additionally, we assume the norm of the feature mapping ϕ and the underlying
t t t t
vector θ∗ are bounded.
Assumption 3.1. The linear contextual dueling bandit satisfies the following conditions:
• For any contextual x ∈ X and action y ∈ A, we have ∥ϕ(x,y)∥ ≤ L/2.
2
4• For the unknown environment parameter θ∗, it satisfies ∥θ∗∥ ≤ B.
2
For the link function µ, we make the following assumption, which is commonly employed in the
study of generalized linear contextual bandits (Filippi et al., 2010; Di et al., 2023).
Assumption 3.2. The link function µ is differentiable and the corresponding first derivative satisfies
κ ≤ µ˙(·),
µ
where κ > 0 is a known constant.
µ
The learning objective is to minimize the cumulative regret defined as:
T
(cid:88)
Regret(T) = r∗(x )−r(x ,y1),
t t t
t=1
where r∗(x ) = r∗(x ,y∗) = max r∗(x ,y) stands for the largest possible reward in context x . In
t t t y∈A t t
comparison with prior studies (Saha, 2021; Di et al., 2023), our regret measure only focuses on the
gap between action y1 and the optimal action y∗. In the context of RLHF, the model generates
t t
multiple candidate responses, and users will choose the most preferable response from the available
options. Under this situation, sub-optimality is only associated with the selected response and
therefore we focus on the regret from action y1.
t
To quantify the cost of collecting human-labeled data, we introduce the concept of query
complexity Query(T) for an algorithm, which is the total number of data pairs that require human
feedbackforpreferenceacrossthefirstT rounds. Itisworthnotingthatwhilesomepriorwork(Tucker
et al., 2023) incorporates direct costs c in the reward for required human feedback, in our work,
we distinguish between regret and query complexity as two separate performance metrics for an
algorithm.
In addition, we consider the minimal sub-optimality gap (Simchowitz and Jamieson, 2019; Yang
et al., 2020; He et al., 2021), which characterizes the difficulty of the bandit problem.
Definition 3.3 (Minimal sub-optimality gap). For each context x ∈ X and action y ∈ A, the
sub-optimality gap ∆(x,y) is defined as
∆(x,y) = r∗(x)−r(x,y),
and the minimal sub-optimality gap is defined as
(cid:8) (cid:9)
∆ = min ∆(x,y) : ∆(x,y) ̸= 0 .
x∈X,y∈A
In general, a larger sub-optimality gap ∆ between action y and the optimal action y∗ implies that
it is easier to distinguish between these actions and results in a lower cumulative regret. Conversely,
a task with a lower gap ∆ indicates that it is more challenging to make such a distinction, leading
to a larger regret. In this paper, we assume the minimal sub-optimality gap is strictly positive.
Assumption 3.4. The minimal sub-optimality gap is strictly positive, i.e., ∆ > 0.
5Algorithm 1 Acitve Proximal Policy Optimization (APPO)
Require: Regularization parameter λ > 0, and B, an upper bound on the ℓ -norm of θ∗, confidence
2
radius β, uncertainty threshold Γ, learning rate η
1: Set initial policy π 1(·|·) as uniform distribution over the action set A, Σ 0 ← λI, C 0 = ∅
2: for t = 1,...,T do
3: Compute the MLE θ(cid:98)t as in (4.1) and observe A, select y t2 ∼ Uniform(A)
4: Let D(cid:98)t(x t,y) = min(cid:8) ⟨θ(cid:98)t,ϕ(x t,y)−ϕ2 t⟩+β∥ϕ(x t,y)−ϕ2 t∥ Σ−1 ,2(cid:9)
t−1
5: Let y t1 = argmax yD(cid:98)t(x t,y)
6: if ∥ϕ1 t −ϕ2 t∥ Σt−1 ≤ Γ then
7: play the duel (y t1,y t2) and keep Σ t = Σ t−1
8: π t+1(y|x) = π t(y|x), C t = C t−1
9: else
10: Sample y t1 ∼ π t(·|s t) and play the duel (y t1,y t2), C t = C t−1∪{t}
11: Query for the preference and observe o t
12: Update Σ t = Σ t−1+(ϕ1 t −ϕ2 t)(ϕ1 t −ϕ2 t)⊤
13: Update π t+1(y|x) ∝ π t(y|x)exp(cid:0) ηD(cid:98)t(x,y)(cid:1)
14: end if
15: end for
4 Algorithm
In this section, we introduce our proposed query-efficient method for aligning LLMs. The main
algorithm is illustrated in Algorithm 1. At a high level, the algorithm leverages the uncertainty-
aware query criterion (Zhang et al., 2023) to issue queries and employs Optimistic Proximal Policy
Optimization (OPPO) (Cai et al., 2020; He et al., 2022b) for policy updates. In the sequel, we
introduce the key parts of the proposed algorithm.
4.1 Regularized MLE
For each round t ∈ [T], we construct the regularized maximum likelihood estimator (MLE) (Line 3)
of parameter θ∗ by solving the following equation:
λθ+ (cid:88) (cid:0) o −µ(⟨θ,ϕ1 −ϕ2⟩)(cid:1) (ϕ1 −ϕ2) = 0, (4.1)
τ τ τ τ τ
τ∈Ct−1
where C represents the index set of rounds up to round t with human-labelled preferences, initially
t
set as an empty set. Compared with previous work on linear dueling bandits (Di et al., 2023), only
part of the data requires human-labelled preference and we construct the MLE only with rounds
t ∈ C T. In addition, the estimation error between θ(cid:98)t and θ∗ satisfied
∥θ(cid:98)t−θ∗∥
Σt−1
≤ O(cid:101)(cid:0)(cid:112) dlog|C T|/κ µ(cid:1) .
After constructing the estimator θ(cid:98)t, the agent first selects a baseline action y t2 and compares each
action y ∈ A with the baseline action y2. For simplicity, we denote D (x ,y) = ⟨θ∗,ϕ(x ,y)−ϕ2⟩ as
t t t t t
the reward gap between y and action y t2. Then, we construct an optimistic estimator D(cid:98)t (Line 4) for
6the reward gap with linear function approximation and Upper Confidence Bound (UCB) bonus, i.e.,
D(cid:98)t(x t,y) ≈ ⟨θ(cid:98)t,ϕ(x t,y)−ϕ2 t⟩+β∥ϕ(x t,y)−ϕ2 t∥
Σ−1
.
t−1
With the help of UCB bonus, we can show that our estimated reward gap D(cid:98)t is an upper bound of
the true reward gap D .
t
4.2 Uncertainty-Aware Query Criterion
To mitigate the expensive costs from collecting human feedback, we introduce the uncertainty-based
criterion (Line 6) (Zhang et al., 2023) to decide whether a pair of action requires y1 and y2 requires
t t
human-labelled preference. Similar criterion has also been used in corruption-robust linear contextual
bandits (He et al., 2022c) and achieving nearly minimax optimal regret in learning linear (mixture)
Markov decision processes (Zhou and Gu, 2022; He et al., 2022a; Zhao et al., 2023). Intuitively
speaking, the UCB bonus β∥ϕ1−ϕ2∥ captures the uncertainty associated with the preference
t t Σ−1
t−1
feedback o . For the action pair (y1,y2) with low uncertainty, where the observation is nearly known
t t t
and provides minimal information, we select the action y1,y2 without querying human preference
t t
feedback. In this situation, the policy π(·|·) remains unchanged as there is no observation in this
round. By employing the uncertainty-based data selection rule, we will later prove that the query
complexity is effectively controlled by:
Query(T) = |C T| ≤ O(cid:101)(d2/∆2).
4.3 Proximal Policy Optimization
In cases where the action pair (y1,y2) exhibits high uncertainty and the uncertainty-aware query
t t
criterion is triggered, the agent resample the action y1 from policy π and queries human feedback
t t
for the duel y1,y2. Upon observing the preference o , this round is then added to the index set C .
t t t t
Subsequently, the policy π is updated using the Proximal Policy Optimization (PPO) method
t+1
(Line 13), i.e.,
(cid:0) (cid:1)
π t+1(y|x) ∝ π t(y|x)exp ηD(cid:98)t(x,y) .
In an extreme case where the uncertainty threshold Γ is chosen to be 0, the uncertainty-aware query
criterion will always be triggered. Under this situation, Algorithm 1 will query the human-labeled
preference for each duel (y1,y2), and it will degenerate to the dueling bandit version of OPPO (Cai
t t √
et al., 2020). In this case, Algorithm 1 has O(cid:101)(d T) regret while having a linear query complexity in
the number of rounds T.
5 Theoretical Analysis
In this section, we present our main theoretical results.
√
Theorem 5.1. Under Assum √ption 3.4, if we set the parameters Γ = O(cid:101)(∆/ d), λ = B−2, η =
O(cid:101)((cid:112) Γ2logA/d), and β = O(cid:101)( d/κ µ) in Algorithm 1, then with probability at least 1−δ, the regret
7Algorithm 2 Active Direct Preference Optimization (ADPO)
Require: Regularization parameter β, uncertainty threshold Γ, learning rate η, initial model
parameter θ , batch size B
0
1: Set θ ← θ 0
2: for t = 1,...,T do
3: Receive batch of data B t = {x i,y i1,y i2}B i=1
4: for i = 1,...,B do
5: Compute or query the preference label o i based on (6.2)
6: end for
7: Update θ ← θ−η∇ θL Bt(π θ,π θ0)
8: end for
for Algorithm 1 in the first T rounds is upper bounded by
Regret(T) = O(cid:101)(d2/∆).
In addition, the query complexity of Algorithm 1 is upper bounded by
Query(T) = O(cid:101)(d2/∆2).
Remark 5.2. Theorem 5.1 suggests that our algorithm achieves a constant regret and query
complexity that is independent of T. It is important to highlight that our algorithm requires a prior
knowledge of the sub-optimal gap ∆. Under the situation where ∆ is unknown, the learner can set
the parameter ∆ via grid search process.
Remark 5.3. In comparison with the regret O(cid:101)(A2d/∆) obtained by the AURORA algorithm
(Sekhari et al., 2023), our algorithm’s regret eliminates the dependency on the action space A.
In comparison to the query complexity O(cid:101)(A3d2/∆2) in Sekhari et al. (2023), we achieve a query
complexity improvement by a factor of A3.
6 Practical Algorithm
In this section, we introduce a practical version of our proposed algorithm, namely active direct
preference optimization (ADPO) and summarize it in Algorithm 2. Inspired from Algorithm 1 and
direct preference optimization (DPO) (Rafailov et al., 2023), ADPO is an active learning-based
DPO. In this section, we consider the Bradley-Terry-Luce model (Bradley and Terry, 1952), where
the link function has the form µ(x) = 1/(1+e−x). At a high level, ADPO follows the basic idea
of Algorithm 1 and sets an uncertainty threshold to select data for querying human preferences.
However, adapting our algorithm to LLM fine-tuning requires several key modifications as outlined
below.
Direct Preference Optimization. We follow the framework of DPO (Rafailov et al., 2023)
for LLM alignment. In DPO, the reward is directly parameterized by the language model in the
8following way2:
(cid:0) (cid:1)
r (x,y) = β logπ (y|x)−logπ (y|x) ,
θ θ ref
where β is a regularization parameter, y is a possible answer given the prompt x and π is
ref
the reference model, which is usually the SFT checkpoint in practice. With the reward model
parameterized above, we have the following DPO training objective:
(cid:104) (cid:16) (cid:17)(cid:105)
L (π ,π ) = −E logµ o·(cid:0) r (x,y1)−r (x,y2)(cid:1) ,
DPO θ ref (x,y1,y2,o)∼D θ θ
where D is the data distribution, y1 and y2 are the two answers given the prompt x, and o is the
human preference such that o = 1 indicates a preference for y1, and o = −1 indicates a preference
for y2. This approach bypasses the reward-model training process in standard RLHF and, therefore,
eliminates the reward estimation error introduced in it. We follow this approach and treat the
language model as both the reward model and the agent simultaneously.
Empirical Uncertainty Estimator. In real applications, rewards are no longer necessarily
parameterized by a linear function. Thus, the uncertainty estimator cannot be directly transferred
to empirical cases. Since the model is essentially predicting the probability of human preference
labels, that is:
P(o = 1|x,y1,y2) = µ(cid:0) r (x,y1)−r (x,y2)(cid:1) , (6.1)
θ θ
where o stands for the preference label and r is the reward model. We can use the reward model’s
θ
predicted probability as its uncertainty. Specifically, if the probability is near 0 or 1, the models are
highly confident in the human’s preference. Therefore, we define the following function U :
θ
U (x,y1,y2) = |r (x,y1)−r (x,y2)|,
θ θ θ
as our empirical uncertainty estimator.
Training Objective. We borrow ideas from previous active learning literature. For given answer
pairs, if the model is very confident in its preference label, we then use the preference label predicted
by the model (i.e., pseudo label) for training. To be specific, given a prompt x and the corresponding
answers y1 and y2, the predicted preference label can be defined as follows:
(cid:40)
o if U (x,y1,y2) ≥ Γ
o (x,y1,y2) = θ , (6.2)
θ sign(cid:0) r (x,y1)−r (x,y2)(cid:1) if U (x,y1,y2) < Γ
θ θ θ
where o is the human preference upon query and sign(z) is the sign of z. With the predicted
preference labels for given prompts and answers, we can now formulate our training objective as
follows:
(cid:20) (cid:18) (cid:19)(cid:21)
L (π ,π ) = −E logµ o (x,y1,y2)·(cid:0) r (x,y1)−r (x,y2)(cid:1) .
D θ ref (x,y1,y2)∼D θ θ θ
2This definition is equivalent to the one in Rafailov et al. (2023).
9To make our approach more time-efficient in practice, we follow the standard approach in DPO and
use mini-batch gradient descent to update the parameters of our model. At each time step, we feed
the model with a batch of data {(x ,y1,y2)}B . We then compute the pseudo-labels and update
i i i i=1
the model parameters by one-step gradient descent.
7 Experiments
In this section, we conduct extensive experiments to verify the effectiveness of our proposed method.
Our experiments reveal that ADPO performs better than DPO while requiring only half the
queries. Additionally, our ablation studies show that involving pseudo-labels is helpful for improving
performance.
7.1 Experimental Setup
Models and Dataset. We start from the base model Zephry-7b-sft-full3, which is supervised-
finetuned from Mistral-7B (Jiang et al., 2023) model on the SFT dataset Ultrachat-200k (Ding et al.,
2023). It has not been trained to align with any human preference. We adopt the human-preference
dataset Ultrachat-binarized (Ding et al., 2023) for alignment. Ultrachat-binarized contains about
62k prompts, and each prompt corresponds to two candidate answers with human preference labels.
Baseline Method and Hyper-parameters. We use the standard direct preference optimization
as our baseline method and follow the implementation alignment handbook codebase4. We use LoRA
(Hu et al., 2021) to fine-tune the model. For both DPO and our method, we select the learning
rate to be 1e-5 and train for one epoch. We fix other hyper-parameters the same as the original
implementation and choose the uncertainty threshold Γ = 1.5 for ADPO.
Table 1: Detailed information of HuggingFace Open LLM Leaderboard. We use TQA to denote
TruthfulQA, WG for Winogrande and HS for HellaSwag. For each evaluation dataset, we present
the number of few-shot examples and metric adopted for evaluation.
Datasets Arc TQA WG GSM8k HS MMLU
# few-shot 25 0 5 5 10 5
Metric acc_norm mc2 acc acc acc_norm acc
Evaluation. We utilize the widely used Huggingface Open LLM Leaderboard (Beeching et al.,
2023) for our evaluation. This benchmark contains a bunch of different datasets covering a wide
range of tasks, offering a comprehensive assessment of different aspects of large language models.
Specifically, the tasks involved in the benchmark include commonsense reasoning (Arc (Clark et al.,
2018), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021)), multi-task language
understanding (MMLU(Hendrycks et al., 2020)), human falsehood mimic (TruthfulQA (Lin et al.,
2021)) and math problem solving (GSM8k (Cobbe et al., 2021)). During the evaluation process,
3https://huggingface.co/alignment-handbook/zephyr-7b-sft-full
4https://github.com/huggingface/alignment-handbook
10we follow the standard approach and provide the language models with a few in-context examples.
Besides the scores for each dataset, we also provide the average score across all datasets. Please
refer to Table 1 for detailed information about the metrics and few-shot numbers we use.
7.2 Experimental Results
Table 2: Results on Open LLM LeaderBoard. We use Avg to denote average score. ADPO
significantly outperforms DPO on three of the datasets and achieves a higher average score, despite
making only 32k queries, which is about half of the queries made by DPO. With a similar amount of
queries, ADPO also outperforms DPO (550).
Models Arc TQA WG GSM8k HS MMLU Avg # Queries
SFT-full 58.28 40.36 76.4 27.9 80.72 60.1 57.29 0
DPO (550) 60.24 41.41 77.27 30.17 82.28 60.23 58.60 35.7k
DPO 60.58 41.88 77.19 29.72 82.34 60.22 58.66 62k
ADPO 61.26 45.52 76.64 28.51 83.21 58.89 59.01 32k
Performance of ADPO. We count the queries made through the training process of ADPO.
Besides the model trained with the entire dataset (denoted by DPO), we select an intermediate
checkpoint at step 550 (denoted by DPO (550)). This checkpoint has made about 35.7k queries,
closest to the 32k used by ADPO. We compare the performance of the three models. The results are
presented in Table 2. Examining the table, we observe that while both DPO and ADPO improve the
average score, ADPO improves the model’s average score from 57.29 to 59.01, which is much higher
than 58.66 achieved by DPO. Specifically, our method demonstrates an impressive performance on
Arc, TruthfulQA and HellaSwag datasets, reaching score of 61.26, 45.52 and 83.21. Compared to
DPO’s scores of 60.68, 83.34, and 60.22, ADPO exhibits significantly better performance. In terms
of number of queried preference labels, ADPO requires only about 32k queries, which is only 52%
of the preference labels used by DPO. In detail, ADPO achieves better performance than DPO on
Arc, TruthfulQA, and HellaSwag, and comparable performance on GSM-8k. We further compare
the improvement between ADPO and DPO (550), both trained with similar query amounts. The
results consistently demonstrate that ADPO outperforms our baselines. An interesting observation
is that, on Winogrande, GSM8k, and MMLU, where ADPO shows inferior performances, DPO (550)
outperforms fully trained DPO. This suggests that the inferiority of ADPO on these datasets may
be attributed to overfitting. We will discuss this phenomenon in detail at Section 7.3. In summary,
these results underscore the effectiveness of ADPO.
Query Efficiency. To further demonstrate the query efficiency of ADPO, we plot the accuracy
curves for ADPO and the baseline as the number of queries increases across all datasets, along
with the average score. The curves for ARC, HellaSwag, TruthfulQA, Winogrande, GSM8k, and
the average score are depicted in Figure 1. The results show that, in terms of ARC, HellaSwag,
1161.0 45 83.0
60.5 44 82.5
60.0 DPO
43 ADPO (w/o PL) 82.0
59.5 ADPO
42 81.5
59.0 DPO DPO
ADPO (w/o PL) 41 81.0 ADPO (w/o PL)
58.5 ADPO ADPO
0 10 20 30 40 50 60 0 10 20 30 40 50 60 0 10 20 30 40 50 60
Number of Queries (k) Number of Queries (k) Number of Queries (k)
(a) ARC Challenge (b) TruthfulQA (c) HellaSwag
31.0 DPO 77.4 59.00
30.5 ADPO (w/o PL) 58.75
ADPO 77.2
30.0 58.50
29.5 77.0 58.25
29.0 76.8 58.00
57.75
DPO DPO
28.5 76.6
ADPO (w/o PL) 57.50 ADPO (w/o PL)
28.0 76.4 ADPO 57.25 ADPO
0 10 20 30 40 50 60 0 10 20 30 40 50 60 0 10 20 30 40 50 60
Number of Queries (k) Number of Queries (k) Number of Queries (k)
(d) GSM8k (e) Winogrande (f) Average Score
Figure 1: The test accuracy curves of DPO, DPO-AQ (w/o PL) and ADPO. The x-axis represents
the number of queries, and the y-axis corresponds to the metric for the respective dataset. Compared
to DPO and DPO-AQ (w/o PL), ADPO enjoys a faster performance improvement and achieves a
higher performance upper bound. Additionally, we observe some instability in the training curve of
GSM8k and Winogrande.
Table 3: The effect of uncertainty threshold in practice setting. We vary the value of Γ and report
the evaluation results. As Γ increases, ADPO makes more queries, and the performance pattern
gets closer to that of DPO.
Γ Arc TQA WG GSM8k HS MMLU Avg # Queries
1.0 61.01 48.41 76.64 20.39 83.35 58.89 58.115 16k
1.3 61.43 47.56 76.24 24.41 83.48 58.44 58.59 24k
1.5 61.26 45.52 76.64 28.51 83.21 58.89 59.005 32k
1.8 60.92 43.2 77.35 29.26 82.13 59.94 58.80 43k
TruthfulQA and the average score, the improvement of DPO slows down after training on 20k
samples. After training on 40k samples, the overall performance even begins to stagnate. In contrast,
the performance of ADPO enjoys a faster improvement after training with the first 10k samples,
and quickly reaches the peak with only 1/3 to 1/2 of the total queries. This suggests that ADPO
can effectively select the most informative data to query the preference labels. Recall that ADPO
cannot outperform DPO on Winogrande and MMLU. From Figure 1(d) and Figure 1(e), we observe
an apparent training instability in both DPO and ADPO. Therefore, the failure of our method on
12
egnellahC
CRA
k8MSG
AQlufhturT
ednargoniW
gawSalleH
erocS
egarevATable 4: The effect of pseudo-labels. The numbers in parentheses represent the corresponding
uncertainty threshold Γ. ADPO outperforms DPO-AQ (w/o PL) in terms of average scores.
Model Arc TQA WG GSM8k HS MMLU Avg # Queries
ADPO (w/o PL) (0.8) 60.49 41.39 77.35 29.26 82.23 60.01 58.46 38k
ADPO (w/o PL) (1.0) 60.49 41.62 77.43 28.73 82.32 60.02 58.44 40k
ADPO (w/o PL) (1.2) 60.49 41.79 77.43 28.81 82.47 59.99 58.50 43k
ADPO 61.26 45.52 76.64 28.51 83.21 58.89 59.01 32k
these two datasets can be attributed to the unstable training dynamic.
7.3 Ablation Study
In this subsection, we study some important parts that might play central roles in ADPO. We
first empirically study the effect of the uncertainty threshold. We also conduct experiments to
demonstrate the impact of using pseudo labels in the training process.
Values of Uncertainty Threshold. We first study the impact of different uncertainty thresholds.
We vary the value of Γ to 1.0, 1.3, 1.5, and 1.8. For each Γ, we count the preference labels used by
the models and evaluate the trained models on the Open LLM Benchmark. As shown in Table 3,
when the uncertainty threshold is small, with more queried preference labels, these models perform
better on the TruthfulQA dataset. However, they perform poorly on datasets like GSM8k. On the
other hand, when the uncertainty threshold goes larger, the models make more queries, and the
performance patterns become closer to the DPO baseline. Another observation is that when Γ ≥ 1.5,
ADPO consistently outperforms the DPO baseline, which implies that ADPO is not very sensitive
to the uncertainty threshold.
Pseudo Labels v.s. Without Pseudo Labels. An alternative to active learning is to directly
follow Algorithm 1 and simply neglect those training data with low uncertainty. Formally, we change
the labels used for training from o to o′ where o′ is defined as follows:
θ θ θ
(cid:40)
o if U (x,y1,y2) ≥ Γ
o′ (x,y1,y2) = θ .
θ 0 if U (x,y1,y2) < Γ
θ
We keep the remaining part of our method the same and denote this method by ADPO (w/o PL).
In this experiment, we also conduct a grid search for the uncertainty threshold Γ and finally pick Γ
to be 0.8, 1.0, and 1.2. The performances of the trained models are shown in Table 4. We plot the
training curve in Figure 1. The results show that, without pseudo-labels, the performance suffers
from a significant downgrade in average score compared to ADPO. The training curves further
indicate that, without pseudo labels, the training dynamics are much more similar to vanilla DPO.
These results show the crucial role of pseudo-labels in the active preference learning process.
138 Conclusion and Future Work
In this work, we consider query-efficient methods for aligning LLMs with human preferences. We first
formulated the problem as a contextual dueling bandit. Under the linear reward and sub-optimal
gap assumption, we propose an active-learning-based algorithm. Our theoretical analysis shows
that our algorithm enjoys a constant regret upper bound and query complexity. We then adapt our
algorithm to direct preference optimization and propose a query-efficient DPO method called ADPO.
Experimental results show that ADPO outperforms DPO with only half the demand on human
preference labels. Despite the good performance ADPO achieves, our theoretical analysis of APPO
cannot be directly applied to ADPO. We leave the theoretical analysis of ADPO as our future work.
A Proof of Theorems in Section 5
In this section, we provide the proof of Theorem 5.1 and we first introduce several lemmas. The
following lemma provides an upper bound on the query complexity and the corresponding dataset
size |C |.
T
Lemma A.1 (Modified from Lemma 4.5, Zhang et al., 2023). Given a uncertainty threshold
0 < Γ ≤ 1, if we set the regularization parameter λ = B−2, then for each round t ∈ [T], we have
|C | ≤ |C | ≤ 16dΓ−2log(3LBΓ−1).
t T
For a finite dataset C , the following lemma provides a upper bound for the estimation error
T
between θ(cid:98)t and θ∗.
Lemma A.2. Suppose we have ∥θ∗∥ ≤ B, ∥ϕ(x,y)∥ ≤ L/2. Then with probability at least 1−δ,
for each round t ∈ [T], we have
∥θ∗−θ(cid:98)t∥
Σt−1
≤ κ1 ·(cid:0)√ λB+(cid:112) 2dlog(λ+|C T|L2/dλδ)(cid:1) ,
µ
Based on Lemmas A.1 and A.2, the following auxiliary lemma proposes a proper choice for the
uncertainty threshold Γ and confidence radius β in Algorithm 1.
Lemma A.3. If we set the uncertainty threshold Γ = κ ∆/(2dι ) and confidence radius β =
√ √ √ µ 1
κ−1(1+4 dι + 2dι ), where ι = 42log(126LB d∆−1κ−1)+(cid:112) 8log(1/δ), ι = log(3LBΓ−1)
µ 2 3 1 µ 2
and ι = log(cid:0) (1+16L2B2Γ−2ι )/δ(cid:1), then we have 2βΓ < ∆ and
3 2
1 (cid:0)√ (cid:112) (cid:1)
β ≥ · λB+ 2dlog(λ+|C |L2/dλδ) .
T
κ
µ
With these parameters, we now define the event E as
1
E
1
= {∀t ∈ [T],∥θ(cid:98)t−θ∗∥
Σ−1
≤ β}.
t−1
According to Lemma A.2 and Lemma A.3, we have Pr(E ) ≥ 1−δ. Conditioned on the event E ,
1 1
the following lemma suggests that our estimated discrepancy is no less than the actual discrepancy.
14Lemma A.4. On the event E , for each round t ∈ [T], context x ∈ X and any action y ∈ A, the
1
estimated discrepancy D(cid:98)t(x,y) satisfied
D(cid:98)t(x,y) ≥ D t(x,y) = ⟨θ∗,ϕ(x,y)−ϕ2 t⟩.
On the other hand, we have
D(cid:98)t(x,y) ≤ D t(x,y)+2β∥ϕ(x,y)−ϕ2 t∥
Σ−1
.
t−1
It is worth to notice that in Algorithm 1 (Line 13), we update the policy π with online mirror
t
descent and the following lemma provides the regret guarantee for this process.
Lemma A.5 (Modified from Lemma 6.2, He et al., 2022b). For any estimated value function
D(cid:98)t(x,·), if we update the policy π t+1(·|x) by the exponential rule:
π t+1(·|x) ∝ π t(·|x)·exp(cid:0) ηD(cid:98)t(x,·)(cid:1) , (A.1)
then the expected sub-optimality gap at round T can be upper bounded as follows:
E x∼D,y∼π∗(·|x)[D(cid:98)t(x,y)]−E x∼D,y∼πt(·|x)[D(cid:98)t(x,y)]
(cid:104) (cid:105)
≤ 2η+η−1E KL(cid:0) π∗(·|x)∥π (·|x)(cid:1) −KL(cid:0) π∗(·|x)∥π (·|x)(cid:1)
x∼D t t+1
With the help of these lemmas, we are now ready to prove our main theorem.
Proof of Theorem 5.1. Now we start the regret analysis. For simplicity, for each round t ∈ [T], we
use ϕ to denote ϕ(x ,y ). Initially, the episodes and their corresponding regret can be decomposed
t t t
into two groups based on whether episode t is added to the dataset C :
T
T
(cid:88)
Regret(T) = ⟨θ∗,ϕ∗⟩−⟨θ∗,ϕ1⟩
t t
t=1
T
(cid:88)
= D (x ,y∗)−D (x ,y1)
t t t t t t
t=1
(cid:88) (cid:88)
= D (x ,y∗)−D (x ,y1)+ D (x ,y∗)−D (x ,y1) (A.2)
t t t t t t t t t t t t
t∈CT t∈/CT
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I1 I2
where D (x,y) = ⟨θ∗,ϕ(x,y)−ϕ2⟩ denotes the reward gap between action y ∈ A and selected action
t t
y2 at round t.
t
Now, we bound this two term separately. For the term I , we have
1
(cid:88)
I = D (x ,y∗)−D (x ,y1)
1 t t t t t t
t∈CT
(cid:88) (cid:88)
≤ D(cid:98)t(x t,y t1)−D t(x t,y t1)+ D(cid:98)t(x t,y t∗)−D(cid:98)t(x t,y t1)
t∈CT t∈CT
(cid:124) (cid:123)(cid:122) (cid:125)
J1
15(cid:88)
= J 1+ E xt∼D,y∼π∗(·|x)[D(cid:98)t(x t,y)]−E xt∼D,y∼πt(·|x)[D(cid:98)t(x t,y)]
t∈CT
(cid:124) (cid:123)(cid:122) (cid:125)
J2
(cid:88) (cid:88)
+ D(cid:98)t(x t,y t∗)−D(cid:98)t(x t,y t1)− E xt∼D,y∼π∗(·|x)[D(cid:98)t(x t,y)]−E xt∼D,y∼πt(·|x)[D(cid:98)t(x t,y t)],
t∈CT t∈CT
(cid:124) (cid:123)(cid:122) (cid:125)
J3
(A.3)
where the inequality holds due to Lemma A.4.
For the term J , we have
1
(cid:88)
J
1
= D(cid:98)t(x t,y t1)−D t(x t,y t1)
t∈CT
(cid:88)
≤ min{4,2β∥ϕ1−ϕ2∥ }
t t Σ−1
t−1
t∈CT
(cid:115)
(cid:88)
≤ 4β |C |· min{1,∥ϕ1−ϕ2∥2 }
T t t Σ−1
t−1
t∈CT
(cid:115)
(cid:18)
λd+|C
|L2(cid:19)
≤ 8β |C |dlog T , (A.4)
T
λd
where the first inequality holds due to Lemma A.4 with the fact that −2 ≤ D (x ,y1) ≤ 2, the
t y t
second inequality holds due to Cauchy–Schwarz inequality and the last inequality holds due to the
elliptical potential lemma (Lemma C.7).
The term J reflects the sub-optimality from the online mirror descent process and can be upper
2
bounded by Lemma A.5. For simplicity, we denote C = {t ,...,t } where K = |C |. Thus, we have
T 1 K T
K
(cid:88)
J
2
= E xtk∼D,y∼π∗(·|x)[D(cid:98)t k(x
t
k,y)]−E xtk∼D,y∼πtk(·|x)[D(cid:98)t k(x
t
k,y)]
k=1
K
≤ (cid:88)(cid:0) 2η+η−1E (cid:2)KL(π∗(·|x)∥π (·|x))−KL(π∗(·|x)∥π (·|x)(cid:3)(cid:1)
x∼D t k t k+1
k=1
= 2ηK +η−1E (cid:2)KL(π∗(·|x)∥π (·|x))−KL(π∗(·|x)∥π (·|x)(cid:3)
x∼D 1 tK+1
≤ 2ηK +η−1E (cid:2)KL(π∗(·|x)∥π (·|x))(cid:3)
x∼D 1
(cid:112)
≤ 2 32dΓ−2log(3LBΓ−1)log|A|, (A.5)
where the first inequality holds due to Lemma A.5, the second equation holds due to policy π keeps
unchanged for t ∈ C , the second inequality holds due to KL(·∥·) ≥ 0 and the last inequality holds
T
(cid:113)
due to η = Γ2logA/(cid:0) 32dlog(3LBΓ−1)(cid:1) with the fact that π is uniform policy.
1
According to Azuma-Hoeffding’s inequality (Lemma C.6), with probability at least 1−δ, the
term J can be upper bounded by
3
(cid:112)
J ≤ 2 2|C |log(1/δ). (A.6)
3 T
16Substituting (A.4), (A.5) and (A.6) into (A.3), we have
(cid:115)
I = J +J +J ≤ 8β |C
|dlog(cid:18)
λd+|C
T|L2(cid:19)
+2(cid:112) 32dΓ−2log(3LBΓ−1)log|A|+2(cid:112)
2|C |log(1/δ)
1 1 2 3 T T
λd
≤ O(cid:101)(βd/Γ)
(cid:18) d2(cid:19)
= O(cid:101) . (A.7)
∆
where the last inequality holds due to Lemma A.1.
Now, we only need to focus on the term I . For each round t ∈/ C , we have
2 T
D t(x t,y t∗)−D t(x t,y t1) = ⟨θ∗−θ(cid:98)t,ϕ∗
t
−ϕ2 t⟩+⟨θ(cid:98)t,ϕ∗
t
−ϕ2 t⟩−⟨θ∗,ϕ1
t
−ϕ2 t⟩
≤ β∥ϕ∗
t
−ϕ2 t∥
Σ−1
+⟨θ(cid:98)t,ϕ∗
t
−ϕ2 t⟩−⟨θ∗,ϕ1
t
−ϕ2 t⟩
t−1
≤ β∥ϕ1
t
−ϕ2 t∥
Σ−1
+⟨θ(cid:98)t,ϕ1
t
−ϕ2 t⟩−⟨θ∗,ϕ1
t
−ϕ2 t⟩
t−1
≤ 2β∥ϕ1−ϕ2∥ ,
t t Σ−1
t−1
where the first inequality holds due to Lemma A.4, the second inequality holds due to the selection
rule of action ϕ1 and the last inequality holds due to Lemma A.4. According to the definition of set
t
C in Algorithm 1, for each round t ∈/ C , we have ∥ϕ1−ϕ ∥ ≤ Γ. Therefore, the sub-optimality
T T t t Σ−1
t−1
gap at round t is upper bounded by
2β∥ϕ1−ϕ ∥ ≤ 2βΓ < ∆,
t t Σ−1
t−1
where the second inequality holds due to Lemma A.3. According to the minimal sub-optimality
assumption (Assumption 3.4), this indicates that the regret yielded in round t ∈/ C is 0. Summing
T
up over t ∈/ C , we have
T
(cid:88)
I = D (x ,y∗)−D (x ,y1) = 0. (A.8)
2 t t t t t t
t∈Tt
Combining the results in (A.7) and (A.8), we complete the proof of Theorem 5.1.
B Proof of Lemmas in Appendix A
In this section, we provide the proofs of the lemmas in Appendix A.
B.1 Proof of Lemma A.1
Proof of Lemma A.1. The proof follows the proof in Zhang et al. (2023). Here we fix the round t to
be T in the proof and only provide the upper bound of C due to the fact that C is monotonically
T t
increasing w.r.t. the round t. For all selected episode t ∈ C , since we have ∥ϕ1−ϕ2∥ ≥ Γ, the
T t t Σ−1
t−1
17summation of the bonuses over all the selected episode t ∈ C is lower bounded by
T
(cid:88) (cid:110) (cid:111)
min 1,∥ϕ1−ϕ2∥2 ≥ |C |min{1,Γ2} = |C |Γ2, (B.1)
t t Σ−1 T T
t−1
t∈CT
where the last equation holds due to 0 ≤ Γ ≤ 1. On the other hand, according to Lemma C.3, the
summation is upper bounded by:
(cid:88) min(cid:110)
1,∥ϕ1−ϕ2∥2
(cid:111)
≤
2dlog(cid:18)
λd+|C
T|L2(cid:19)
. (B.2)
t t Σ−1 λd
t−1
t∈CT
Combining (B.1) and (B.2), we know that the total number of the selected data points |C | satisfies
T
the following inequality:
(cid:18)
λd+|C
|L2(cid:19)
Γ2|C | ≤ 2dlog T .
T
λd
For simplicity, we reorganized the result as follows:
Γ2|C | (cid:18) 2L2 Γ2|C |(cid:19)
T ≤ log 1+ T . (B.3)
2d Γ2λ 2d
Notice that λ = B−2 and 2L2B2 ≥ 2 ≥ Γ2, therefore, if |C | is too large such that
T
Γ2|C | (cid:18) 4L2B2(cid:19) (cid:18) 4L2B2(cid:19) Γ2
T
> 4log +1 ≥ 4log + ,
2d Γ2 Γ2 2L2B2
then according to Lemma C.1, (B.3) will not hold. Thus the necessary condition for (B.3) to hold is:
(cid:32) (cid:33)
Γ2|C T| (cid:18) 4L2B2(cid:19) (cid:18) 2LB(cid:19) 2LBe1 8 (cid:18) 3LB(cid:19)
≤ 4log +1 = 8log +log(e) = 8log < 8log .
2d Γ2 Γ Γ Γ
Applying basic calculus, we obtain the claimed bound for |C | and thus complete the proof of
T
Lemma A.1.
B.2 Proof of Lemma A.2
Proof of Lemma A.2. This proof follows the proof in Di et al. (2023). For each round t ∈ [T], we
define the following auxiliary quantities:
G (θ) = λθ+ (cid:88) (cid:104) µ(cid:0) (ϕ1 −ϕ2)⊤θ(cid:1) −µ(cid:0) (ϕ1 −ϕ2)⊤θ∗(cid:1)(cid:105) (ϕ1 −ϕ2)
t τ τ τ τ τ τ
τ∈Ct−1
ϵ = o
−µ(cid:0) (ϕ1−ϕ2)⊤θ∗(cid:1)
t t t t
(cid:88)
Z = ϵ (ϕ1 −ϕ2).
t τ τ τ
τ∈Ct−1
18By defining θ(cid:98)t as the solution to (4.1), we plug the equation into the definition of G
t
and we have
G t(θ(cid:98)t) = λθ(cid:98)t+ (cid:88) (cid:104) µ(cid:0) (ϕ1
τ
−ϕ2 τ)⊤θ(cid:98)t)−o
τ
+o
τ
−µ(cid:0) (ϕ1
τ
−ϕ2 τ)⊤θ∗(cid:1)(cid:105) (ϕ1
τ
−ϕ2 τ)
τ∈Ct−1
= λθ(cid:98)t+ (cid:88) (cid:104) µ(cid:0) (ϕ1
τ
−ϕ2 τ)⊤θ(cid:98)t)−o τ(cid:105) (ϕ1
τ
−ϕ2 τ)+ (cid:88) (cid:104) o
τ
−µ(cid:0) (ϕ1
τ
−ϕ2 τ)⊤θ∗(cid:1)(cid:105) (ϕ1
τ
−ϕ2 τ)
τ∈Ct−1 τ∈Ct−1
= Z .
t
Therefore, we have that
G t(θ(cid:98)t)−G t(θ∗) = Z t−G t(θ∗) = Z t−λθ∗.
On the other hand, applying Taylor’s expansion, there exists α ∈ [0,1] and θ(cid:101)t = αθ(cid:98)t+(1−α)θ∗,
such that the following equation holds:
G t(θ(cid:98)t)−G t(θ∗) = λ(θ(cid:98)t−θ∗)+ (cid:88) (cid:104) µ(cid:0) (ϕ1
τ
−ϕ2 τ)⊤θ(cid:1) −µ(cid:0) (ϕ1
τ
−ϕ2 τ)⊤θ∗(cid:1)(cid:105) (ϕ1
τ
−ϕ2 τ)
τ∈Ct−1
= (cid:104) λI+ (cid:88) µ′(cid:0) (ϕ1
τ
−ϕ2 τ)⊤θ(cid:101)t(cid:1) (ϕ1
τ
−ϕ2 τ)(ϕ1
τ
−ϕ2 τ)⊤(cid:105) (θ(cid:98)t−θ∗)
τ∈Ct−1
= F(θ(cid:101)t)(θ(cid:98)t−θ∗),
where we define F(θ(cid:101)t) = λI+(cid:80) τ∈Ct−1µ′(cid:0) (ϕ1
τ
−ϕ2 τ)⊤θ(cid:101)t(cid:1) (ϕ1
τ
−ϕ2 τ)(ϕ1
τ
−ϕ2 τ)⊤. Thus, we have:
∥θ(cid:98)t−θ∗∥2
Σt−1
= (Z t−λθ∗)⊤F(θ(cid:101)t)−1Σ t−1F(θ(cid:101)t)−1(Z t−λθ∗)
1
≤ (Z −λθ∗)⊤Σ−1 (Z −λθ∗)
κ2 t t−1 t
µ
1
= ∥Z −λθ∗∥2
κ2 t Σ−1
µ t−1
where the inequality holds due to F(θ(cid:101)t) ⪰ κ µΣ(cid:98)t−1. Now we have:
1
∥θ(cid:98)t−θ∗∥
Σt−1
≤
κ
µ∥Z t−λθ∗∥
Σ− t−1
1
≤ 1 (cid:0) ∥Z ∥ +∥λθ∗∥ (cid:1)
κ µ
t Σ− t−1
1
Σ− t−1
1
1 (cid:0) √ (cid:1)
≤ ∥Z ∥ + λB ,
κ µ
t Σ− t−1
1
wherethesecondinequalityholdsduetotriangleinequalityandlastinequalityholdsduetoΣ ⪰ λI.
t−1
Now we only need to bound ∥Z ∥ .
t Σ−1
t−1
According to Lemma C.2, with probability at least 1−δ, we have
(cid:115) (cid:18)(cid:112)
det(Σ
)(cid:19) (cid:115) (cid:18)
det(Σ
)(cid:19) (cid:115) (cid:18)
λ+|C
|L2/d(cid:19)
t−1 t−1 t
∥Z ∥ ≤ 2log ≤ 2log ≤ 2dlog
t Σ− t−1 1 (cid:112) det(Σ 0)δ λdδ λdδ
19where the first inequality holds due to Lemma C.2 and the last inequality holds due to Lemma C.5.
Now we combine the two term and have
(cid:32) (cid:115) (cid:33)
1 √ (cid:18) λ+|C |L2/d(cid:19)
∥θ(cid:98)t−θ∗∥
Σt−1
≤
κ
λB+ 2dlog λdt
δ
,
µ
which concludes our statement.
B.3 Proof of Lemma A.3
Proof of Lemma A.3. This proof follows the proof in Zhang et al. (2023). First, we recall that
√ √
Γ = ∆κ /2dι and β = κ−1(1+4 dι + 2dι ). We will first demonstrate that the selection of β
µ 1 µ 2 3
satisfy the requirement in Lemma A.3. Recalling that λ = B−2, through basic calculation, we have
(cid:113)
(cid:0) (cid:1)
κ β ≥ 1+ 2dlog (1+L2B216dΓ−2ι )/dδ
µ 2
(cid:113)
(cid:0) (cid:1)
≥ 1+ 2dlog (1+L2B2|C |)/dδ
T
√
(cid:112)
= λB+ 2dlog(λ+|C |L2/dλδ),
T
√
wherethefirstinequalityholdsbyneglectingthepositiveterm4 dι andd ≥ 1,thesecondinequality
2
holds due to Lemma A.1 and the last equation holds by plugging in λ = B−2. Now we come to the
second statement. First, by basic computation, we have
√
(cid:112) (cid:112)
2ι ≤ 2log((1+16L2B2Γ−2ι )+ 2log(1/δ)).
3 2
Notice that we have L ≥ 1, B ≥ 1, and Γ ≤ 1, which further implies that LBΓ−1 ≥ 1, leading to
√ (cid:112)
2+4 ι ≤ 6ι , 2log((1+16L2B2Γ−2ι ) ≤ 3ι .
2 2 2 2
Therefore, we have:
√ √ (cid:112)
2+4 ι + 2ι ≤ 9ι +2 log(1/δ)
2 3 2
√
(cid:112)
≤ 9log(6LB d∆−1κ−1ι )+2 log(1/δ).
µ 1
By Lemma C.1, we can identify the sufficient condition for the following inequality
√ √ √ √
(cid:112)
(6LB d∆−1κ−1)ι ≥ 9(6LB d∆−1κ−1)log(6LB d∆−1κ−1ι )+2(6LB d∆−1κ−1) log(1/δ)
µ 1 µ µ 1 µ
(B.4)
is that
√
(cid:112)
ι ≥ 36log(108LB d∆−1κ−1)+ 8log(1/δ),
1 mu
√
which naturally holds due to our definition of ι . Eliminating the 6LB d∆−1κ−1 term in (B.4)
1 µ
yields that
√ √
ι ≥ 2+4 ι + 2ι ,
1 2 3
which implies that
∆κ µ 1 (cid:112) √
2βΓ = √ (1+2 dι + 2ι ) < ∆.
2 3
2 dι 1κ µ
20Thus, we complete the proof of Lemma A.3.
B.4 Proof of Lemma A.4
Proof of Lemma A.4. For each context x ∈ X and action y ∈ A, we have
|D t(x,y)−⟨θ(cid:98)t,ϕ(x,y)−ϕ2 t⟩| = |⟨θ(cid:98)t−θ∗,ϕ(x,y)−ϕ2 t⟩|
≤ ∥θ(cid:98)t−θ∗∥
Σ−1
·∥ϕ(x,y)−ϕ2 t∥
Σt−1
t−1
≤ β∥ϕ(x,y)−ϕ2∥ , (B.5)
t Σt−1
where the first inequality holds due to Cauchy–Schwarz inequality and the second inequality holds
due to event E . Therefore, we have
1
⟨θ(cid:98)t,ϕ(x,y)−ϕ2 t⟩+β∥ϕ(x,y)−ϕ2 t∥
Σt−1
≥ D t(x,y),
where the first inequality holds due to (B.5). In addition, we have D (x,y) = ⟨θ∗,ϕ(x,y)−ϕ2⟩ ≤ 2.
t t
Combing these two results, we have
D(cid:98)t(x,y) = min{⟨θ(cid:98)t,ϕ(x,y)−ϕ2 t⟩+β∥ϕ(x,y)−ϕ2 t∥ Σt−1,2} ≥ D t(x,y).
On the other hand, we have
D(cid:98)t(x,y) ≤ ⟨θ(cid:98)t,ϕ(x,y)−ϕ2 t⟩+β∥ϕ(x,y)−ϕ2 t∥
Σt−1
≤ D t(x,y)+2β∥ϕ(x,y)−ϕ2 t∥ Σt−1,
where the first inequality holds due to the definition of D(cid:98)t(x,y) and the second inequality holds due
to (B.5). Thus, we complete the proof of Lemma A.4.
B.5 Proof of Lemma A.5
Proof of Lemma A.5. The proof follows the approach in He et al. (2022b). Recall that we assume
the policy is updated in round t according to the update rule (A.1), for all contexts x ∈ X. Thus,
we have:
(cid:8) (cid:9)
exp(cid:8) ηD(cid:98)t(x,y)(cid:9) = π t(y|x)exp ηD(cid:98)t(x,y) = ρπ t+1(y|x) , (B.6)
π (y|x) π (y|x)
t t
where ρ = (cid:80) y∈Aπ t(y|x)exp(cid:8) ηD(cid:98)t(x,y)(cid:9) is the regularization term that is the same for all actions
y ∈ A. Therefore, we have
(cid:88) ηD(cid:98)t(x,y)(cid:0) π∗(y|x)−π t+1(y|x)(cid:1)
y∈A
= (cid:88)(cid:0) logρ+logπ (y|x)−logπ (y|x)(cid:1)(cid:0) π∗(y|x)−π (y|x)(cid:1)
t+1 t t+1
y∈A
=
(cid:88) π∗(y|x)(cid:0)
logπ (y|x)−logπ
(y|x)(cid:1)
−π
(y|x)(cid:0)
logπ (y|x)−logπ
(y|x)(cid:1)
t+1 t t+1 t+1 t
y∈A
21= (cid:88) π∗(y|x)(cid:0) logπ∗(y|x)−logπ (y|x)(cid:1) +(cid:88) π∗(y|x)(cid:0) logπ (y|x)−logπ∗(y|x)(cid:1)
t t+1
y∈A y∈A
(cid:88) (cid:0) (cid:1)
− π (y|x) logπ (y|x)−logπ (y|x)
t+1 t+1 t
y∈A
= KL(cid:0) π∗(·|x)∥π (·|x)(cid:1) −KL(cid:0) π∗(·|x)∥π (·|x)(cid:1) −KL(cid:0) π (·|x)∥π (·|x)(cid:1) , (B.7)
t t+1 t+1 t
where the first equation holds due to (B.6) and the second equation holds due to (cid:80) (cid:0) π∗(y|x)−
y∈A
π (y|x)(cid:1) = 0. Consequently, we have
t+1
E y∼π∗(·|x)(cid:2) D(cid:98)t(x,y)(cid:3) −E y∼πt(·|x)(cid:2) D(cid:98)t(x,y)(cid:3)
= (cid:88) D(cid:98)t(x,y)(cid:0) π∗(y|x)−π t(y|x)(cid:1)
y∈A
= (cid:88) D(cid:98)t(x,y)(cid:0) π∗(y|x)−π t+1(y|x)(cid:1) +(cid:88) D(cid:98)t(x,y)(cid:0) π t+1(y|x)−π t(y|x)(cid:1)
y∈A y∈A
≤ (cid:88) D(cid:98)t(x,y)(cid:0) π∗(y|x)−π t+1(y|x)(cid:1) +2(cid:13) (cid:13)π t+1(·|x)−π t(·|x)(cid:13) (cid:13)
1
y∈A
(cid:16) (cid:17)
= η−1 KL(cid:0) π∗(·|x)∥π (·|x)(cid:1) −KL(cid:0) π∗(·|x)∥π (·|x)(cid:1) −KL(cid:0) π (·|x)∥π (·|x)(cid:1)
t t+1 t+1 t
(cid:13) (cid:13)
+2(cid:13)π t+1(·|x)−π t(·|x)(cid:13)
1
(cid:16) (cid:17)
≤ η−1 KL(cid:0) π∗(·|x)∥π (·|x)(cid:1) −KL(cid:0) π∗(·|x)∥π (·|x)(cid:1)
t t+1
(cid:13) (cid:13)2
+2(cid:13) (cid:13)π t+1(·|x)−π t(·|x)(cid:13) (cid:13) 1−
(cid:13)π t+1(·|x) 2−
η
π t(·|x)(cid:13)
1
(cid:16) (cid:17)
≤ 2η+η−1 KL(cid:0) π∗(·|x)∥π (·|x)(cid:1) −KL(cid:0) π∗(·|x)∥π (·|x)(cid:1) , (B.8)
t t+1
where the first inequality holds due to the fact that 0 ≤ D(cid:98)t(x,y) ≤ 2, the second inequality holds due
to Pinsker’s inequality and the last inequality holds due to the fact that ax−bx2 ≤ a2/4b. Finally,
taking expectation over x ∼ D finishes the proof.
C Auxiliary Lemmas
Lemma C.1 (Lemma A.2, Shalev-Shwartz and Ben-David, 2014). Let a ≥ 1 and b ≥ 0, then
x ≥ 4alog(2a)+2b results in x ≥ alogx+b.
Lemma C.2 (Theorem 1, Abbasi-Yadkori et al., 2011). Let {F }∞ be a filtration. Let {ϵ }∞ be
t t=0 t t=1
a real-valued stochastic process such that ϵ is F -measurable and ϵ is conditionally R-sub-Gaussian
t t t
for some R ≥ 0. Let {ϕ }∞ be an Rd-valued stochastic process such that ϕ is F measurable
t t=1 t t−1
and ∥ϕ ∥ ≤ L for all t. For any t ≥ 0, define U = λI+(cid:80)t ϕ ϕ⊤. Then for any δ > 0, with
t 2 t i=1 i i
probability at least 1−δ, for all t ≥ 0, we have
(cid:13) t (cid:13)2 (cid:32) (cid:112) (cid:33)
(cid:13) (cid:13)(cid:88)
ϕ ϵ
(cid:13)
(cid:13) ≤ 2R2log
det(U t)
.
(cid:13) i i(cid:13) (cid:112)
(cid:13)
i=1
(cid:13)
U−1
det(U 0)δ
t
22Lemma C.3 (Lemma 11, Abbasi-Yadkori et al. 2011). Let {ϕ }t be a sequence in Rd, define
i i=1
U = λI+(cid:80)t ϕ ϕ⊤, then
i i=1 i i
(cid:88)t
(cid:110) (cid:111)
(cid:18) λd+tL2(cid:19)
min 1,∥ϕ ∥2 ≤ 2dlog .
i U−1 λd
i−1
i=1
The following auxiliary lemma and its corollary are useful
Lemma C.4 (Lemma A.2, Shalev-Shwartz and Ben-David 2014). Let a ≥ 1 and b > 0. Then
x ≥ 4alog(2a)+2b yields x ≥ alog(x)+b.
Lemma C.5 (Lemma C.7, Zhang et al., 2023). Suppose sequence {x }t ⊂ Rd and for any i ≤ t,
i i=1
∥x ∥ ≤ L. For any index subset C ⊆ [t], define U = λI + (cid:80) x x⊤ for some λ > 0, then
i 2 i∈C i i
det(U) ≤ (λ+|C|L2/d)d.
Lemma C.6 (Azuma–Hoeffding inequality, Cesa-Bianchi and Lugosi 2006). Let {x }n be a
i i=1
martingale difference sequence with respect to a filtration {G } satisfying |x | ≤ M for some constant
i i
M, x is G -measurable, E[x |G ] = 0. Then for any 0 < δ < 1, with probability at least 1−δ, we
i i+1 i i
have
n
(cid:88) (cid:112)
x ≤ M 2nlog(1/δ).
i
i=1
Lemma C.7 (Lemma11inAbbasi-Yadkorietal.(2011)). Let{ϕ }+∞ beasequenceinRd,Uad×d
t t=1
positive definite matrix and define U = U+(cid:80)t ϕ⊤ϕ . If ∥ϕ ∥ ≤ L and λ (U) ≥ max(1,L2),
t i=1 i i i 2 min
then we have
t (cid:18) (cid:19)
(cid:88) ϕ⊤(U )−1ϕ ≤ 2log detU t .
i i−1 i detU
i=1
References
Abbasi-Yadkori, Y., Pál, D. and Szepesvári, C. (2011). Improved algorithms for linear
stochastic bandits. Advances in neural information processing systems 24.
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D.,
Altenschmidt, J., Altman, S., Anadkat, S. et al. (2023). Gpt-4 technical report. arXiv
preprint arXiv:2303.08774 .
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort,
S., Ganguli, D., Henighan, T. et al. (2022). Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 .
Balcan, M.-F., Beygelzimer, A. and Langford, J. (2006). Agnostic active learning. In
Proceedings of the 23rd international conference on Machine learning.
Balcan, M.-F., Broder, A.andZhang, T.(2007). Marginbasedactivelearning. InInternational
Conference on Computational Learning Theory. Springer.
23Balsubramani, A., Karnin, Z., Schapire, R. E. and Zoghi, M. (2016). Instance-dependent
regret bounds for dueling bandits. In Conference on Learning Theory. PMLR.
Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O.,
Tunstall, L. and Wolf, T. (2023). Open llm leaderboard. https://huggingface.co/spaces/
HuggingFaceH4/open_llm_leaderboard.
Bengs, V., Saha, A. and Hüllermeier, E. (2022). Stochastic contextual dueling bandits under
linear stochastic transitivity models. In International Conference on Machine Learning. PMLR.
Bradley, R. A. and Terry, M. E. (1952). Rank analysis of incomplete block designs: I. the
method of paired comparisons. Biometrika 39 324–345.
Cai, Q., Yang, Z., Jin, C. and Wang, Z. (2020). Provably efficient exploration in policy
optimization. In International Conference on Machine Learning. PMLR.
Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman,
R., Korbak, T., Lindner, D., Freire, P. et al. (2023). Open problems and fundamental
limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217 .
Cesa-Bianchi, N., Gentile, C. and Orabona, F. (2009). Robust bounds for classification via
selective sampling. In Proceedings of the 26th annual international conference on machine learning.
Cesa-Bianchi, N., Gentile, C., Zaniboni, L. and Warmuth, M. (2006). Worst-case analysis of
selective sampling for linear classification. Journal of Machine Learning Research 7.
Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, learning, and games. Cambridge university
press.
Cesa-Bianchi, N., Lugosi, G. and Stoltz, G. (2005). Minimizing regret with label efficient
prediction. IEEE Transactions on Information Theory 51 2152–2162.
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S.,
Zhuang, Y., Gonzalez, J. E., Stoica, I. and Xing, E. P. (2023). Vicuna: An open-source
chatbot impressing gpt-4 with 90%* chatgpt quality.
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S. and Amodei, D. (2017).
Deep reinforcement learning from human preferences. Advances in neural information processing
systems 30.
Citovsky, G., DeSalvo, G., Gentile, C., Karydas, L., Rajagopalan, A., Rostamizadeh,
A. and Kumar, S. (2021). Batch active learning at scale. Advances in Neural Information
Processing Systems 34 11933–11944.
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C. and Tafjord,
O. (2018). Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457 .
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M.,
Tworek, J., Hilton, J., Nakano, R. et al. (2021). Training verifiers to solve math word
problems. arXiv preprint arXiv:2110.14168 .
24Dasgupta, S. (2005). Coarse sample complexity bounds for active learning. Advances in neural
information processing systems 18.
Dasgupta, S., Kalai, A. T. and Monteleoni, C. (2005). Analysis of perceptron-based active
learning. In International conference on computational learning theory. Springer.
Di, Q., Jin, T., Wu, Y., Zhao, H., Farnoud, F. and Gu, Q. (2023). Variance-aware regret
bounds for stochastic contextual dueling bandits. arXiv preprint arXiv:2310.00968 .
Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M. and Zhou, B.
(2023). Enhancing chat language models by scaling high-quality instructional conversations. arXiv
preprint arXiv:2305.14233 .
Dudík, M., Hofmann, K., Schapire, R. E., Slivkins, A. and Zoghi, M. (2015). Contextual
dueling bandits. ArXiv abs/1502.06362.
Falahatgar, M., Jain, A., Orlitsky, A., Pichapati, V. and Ravindrakumar, V. (2018).
The limits of maxing, ranking, and preference learning. In International conference on machine
learning. PMLR.
Falahatgar, M., Orlitsky, A., Pichapati, V. and Suresh, A. T. (2017). Maximum selection
and ranking under noisy comparisons. In International Conference on Machine Learning. PMLR.
Filippi, S., Cappe, O., Garivier, A. and Szepesvári, C. (2010). Parametric bandits: The
generalized linear case. Advances in Neural Information Processing Systems 23.
Gao, J., Zhao, H., Yu, C. and Xu, R. (2023a). Exploring the feasibility of chatgpt for event
extraction. arXiv preprint arXiv:2303.03836 .
Gao, L., Schulman, J. and Hilton, J. (2023b). Scaling laws for reward model overoptimization.
In International Conference on Machine Learning. PMLR.
Gentile, C., Wang, Z. and Zhang, T. (2022). Fast rates in pool-based batch active learning.
arXiv preprint arXiv:2202.05448 .
Gu, Q., Zhang, T. and Han, J. (2014). Batch-mode active learning via error bound minimization.
In UAI.
Gu, Q., Zhang, T., Han, J. and Ding, C. (2012). Selective labeling via error bound minimization.
Advances in neural information processing systems 25.
Han, R., Peng, T., Yang, C., Wang, B., Liu, L. and Wan, X. (2023). Is information extraction
solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors. arXiv
preprint arXiv:2305.14450 .
Hanneke, S. and Yang, L. (2015). Minimax analysis of active learning. J. Mach. Learn. Res. 16
3487–3602.
Hanneke, S. and Yang, L. (2021). Toward a general theory of online selective sampling: Trading
off mistakes and queries. In International Conference on Artificial Intelligence and Statistics.
PMLR.
25He, J., Zhao, H., Zhou, D. and Gu, Q. (2022a). Nearly minimax optimal reinforcement learning
for linear markov decision processes. arXiv preprint arXiv:2212.06132 .
He, J., Zhou, D. and Gu, Q. (2021). Logarithmic regret for reinforcement learning with linear
function approximation. In International Conference on Machine Learning. PMLR.
He, J., Zhou, D. and Gu, Q. (2022b). Near-optimal policy optimization algorithms for learning
adversariallinearmixturemdps. InInternationalConferenceonArtificialIntelligenceandStatistics.
PMLR.
He, J., Zhou, D., Zhang, T. and Gu, Q. (2022c). Nearly optimal algorithms for linear contextual
bandits with adversarial corruptions. Advances in Neural Information Processing Systems 35
34614–34625.
Heckel, R., Simchowitz, M., Ramchandran, K. and Wainwright, M. (2018). Approximate
ranking from pairwise comparisons. In International Conference on Artificial Intelligence and
Statistics. PMLR.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J.
(2020). Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 .
Hoi, S. C., Jin, R., Zhu, J. and Lyu, M. R. (2006). Batch mode active learning and its application
to medical image classification. In Proceedings of the 23rd international conference on Machine
learning.
Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W. et al. (2021).
Lora: Low-rank adaptation of large language models. In International Conference on Learning
Representations.
Jamieson, K., Katariya, S., Deshpande, A. and Nowak, R. (2015). Sparse dueling bandits. In
Artificial Intelligence and Statistics. PMLR.
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D.
d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L. et al. (2023). Mistral 7b.
arXiv preprint arXiv:2310.06825 .
Krueger, D., Leike, J., Evans, O. and Salvatier, J. (2020). Active reinforcement learning:
Observing rewards at a cost. arXiv preprint arXiv:2011.06709 .
Lin, S., Hilton, J. and Evans, O. (2021). Truthfulqa: Measuring how models mimic human
falsehoods. arXiv preprint arXiv:2109.07958 .
Lou, H., Jin, T., Wu, Y., Xu, P., Gu, Q. and Farnoud, F. (2022). Active ranking without
strong stochastic transitivity. Advances in neural information processing systems 35 297–309.
Munos, R., Valko, M., Calandriello, D., Azar, M. G., Rowland, M., Guo, Z. D., Tang,
Y., Geist, M., Mesnard, T., Michi, A. et al. (2023). Nash learning from human feedback.
arXiv preprint arXiv:2312.00886 .
26Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C.,
Agarwal, S.,Slama, K.,Ray, A.et al.(2022). Traininglanguagemodelstofollowinstructions
with human feedback. Advances in Neural Information Processing Systems 35 27730–27744.
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D. and Finn, C. (2023).
Direct preference optimization: Your language model is secretly a reward model. arXiv preprint
arXiv:2305.18290 .
Ramamohan, S., Rajkumar, A. and Agarwal, S. (2016). Dueling bandits: Beyond condorcet
winners to general tournament solutions. In NIPS.
Ren, W., Liu, J. K. and Shroff, N. (2019). On sample complexity upper and lower bounds for
exact ranking from noisy comparisons. Advances in Neural Information Processing Systems 32.
Saha, A. (2021). Optimal algorithms for stochastic contextual preference bandits. Advances in
Neural Information Processing Systems 34 30050–30062.
Saha, A. and Gaillard, P. (2022). Versatile dueling bandits: Best-of-both world analyses for
learning from relative preferences. In International Conference on Machine Learning. PMLR.
Saha, A. and Krishnamurthy, A. (2022). Efficient and optimal algorithms for contextual dueling
bandits under realizability. In International Conference on Algorithmic Learning Theory. PMLR.
Sakaguchi, K.,Bras, R. L.,Bhagavatula, C.andChoi, Y.(2021). Winogrande: Anadversarial
winograd schema challenge at scale. Communications of the ACM 64 99–106.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O. (2017). Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 .
Schulze, S. and Evans, O. (2018). Active reinforcement learning with monte-carlo tree search.
arXiv preprint arXiv:1803.04926 .
Sekhari, A., Sridharan, K., Sun, W. and Wu, R. (2023). Contextual bandits and imitation
learning via preference-based active queries. arXiv preprint arXiv:2307.12926 .
Shalev-Shwartz, S. and Ben-David, S. (2014). Understanding machine learning: From theory to
algorithms. Cambridge university press.
Simchowitz, M. and Jamieson, K. G. (2019). Non-asymptotic gap-dependent regret bounds for
tabular mdps. In Advances in Neural Information Processing Systems.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov,
N., Batra, S., Bhargava, P., Bhosale, S. et al. (2023). Llama 2: Open foundation and
fine-tuned chat models. arXiv preprint arXiv:2307.09288 .
Tucker, A. D., Biddulph, C., Wang, C. and Joachims, T. (2023). Bandits with costly reward
observations. In Uncertainty in Artificial Intelligence. PMLR.
Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S.,
von Werra, L., Fourrier, C., Habib, N., Sarrazin, N., Sanseviero, O., Rush, A. M.
and Wolf, T. (2023). Zephyr: Direct distillation of lm alignment.
27Wei, X., Cui, X., Cheng, N., Wang, X., Zhang, X., Huang, S., Xie, P., Xu, J., Chen, Y.,
Zhang, M. et al. (2023). Zero-shot information extraction via chatting with chatgpt. arXiv
preprint arXiv:2302.10205 .
Wirth, C., Akrour, R., Neumann, G., Fürnkranz, J. et al. (2017). A survey of preference-
based reinforcement learning methods. Journal of Machine Learning Research 18 1–46.
Wu, H. and Liu, X. (2016). Double thompson sampling for dueling bandits. Advances in neural
information processing systems 29.
Wu, Y., Jin, T., Lou, H., Farnoud, F. and Gu, Q. (2023). Borda regret minimization for
generalized linear dueling bandits. arXiv preprint arXiv:2303.08816 .
Wu, Y., Jin, T., Lou, H., Xu, P., Farnoud, F. and Gu, Q. (2022). Adaptive sampling for
heterogeneous rank aggregation from noisy pairwise comparisons. In International Conference on
Artificial Intelligence and Statistics. PMLR.
Xiong, W.,Dong, H.,Ye, C.,Zhong, H.,Jiang, N.andZhang, T.(2023). Gibbssamplingfrom
human feedback: A provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456 .
Yang, K., Yang, L. F. and Du, S. S. (2020). q-learning with logarithmic regret. arXiv preprint
arXiv:2006.09118 .
Yuan, C., Xie, Q. and Ananiadou, S. (2023). Zero-shot temporal relation extraction with chatgpt.
arXiv preprint arXiv:2304.05454 .
Yue, Y., Broder, J., Kleinberg, R. and Joachims, T. (2012). The k-armed dueling bandits
problem. Journal of Computer and System Sciences 78 1538–1556.
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. and Choi, Y. (2019). Hellaswag: Can a
machine really finish your sentence? arXiv preprint arXiv:1905.07830 .
Zhang, T. and Oles, F. (2000). The value of unlabeled data for classification problems. In
Proceedings of the Seventeenth International Conference on Machine Learning,(Langley, P., ed.),
vol. 20. Citeseer.
Zhang, W., He, J., Fan, Z. and Gu, Q. (2023). On the interplay between misspecification and
sub-optimality gap in linear contextual bandits. arXiv preprint arXiv:2303.09390 .
Zhao, H., He, J., Zhou, D., Zhang, T. and Gu, Q. (2023). Variance-dependent regret bounds
for linear bandits and reinforcement learning: Adaptivity and computational efficiency. arXiv
preprint arXiv:2302.10371 .
Zhou, D. and Gu, Q. (2022). Computationally efficient horizon-free reinforcement learning for
linear mixture mdps. Advances in neural information processing systems 35 36337–36349.
Zhu, B., Jiao, J. and Jordan, M. I. (2023). Principled reinforcement learning with human
feedback from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270 .
Ziegler, D. M.,Stiennon, N.,Wu, J.,Brown, T. B.,Radford, A.,Amodei, D.,Christiano,
P. and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint
arXiv:1909.08593 .
28