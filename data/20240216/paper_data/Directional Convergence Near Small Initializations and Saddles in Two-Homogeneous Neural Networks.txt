Directional Convergence Near Small Initializations and
Saddles in Two-Homogeneous Neural Networks
AkshayKumar kumar511@umn.edu
DepartmentofElectricalandComputerEngineering
UniversityofMinnesota
Minneapolis,MN55455
JarvisHaupt jdhaupt@umn.edu
DepartmentofElectricalandComputerEngineering
UniversityofMinnesota
Minneapolis,MN55455
Abstract
Thispaperexaminesgradientflowdynamicsoftwo-homogeneousneuralnetworksforsmallinitial-
izations, where all weights are initialized near the origin. For both square and logistic losses, it is
shownthatforsufficientlysmallinitializations,thegradientflowdynamicsspendsufficienttimein
theneighborhoodoftheorigintoallowtheweightsoftheneuralnetworktoapproximatelyconverge
indirectiontotheKarush-Kuhn-Tucker(KKT)pointsofaneuralcorrelationfunctionthatquantifies
thecorrelationbetweentheoutputoftheneuralnetworkandcorrespondinglabelsinthetrainingdata
set. Forsquareloss, ithasbeenobservedthatneuralnetworksundergosaddle-to-saddledynamics
when initialized close to the origin. Motivated by this, this paper also shows a similar directional
convergenceamongweightsofsmallmagnitudeintheneighborhoodofcertainsaddlepoints.
1 Introduction
Massivelyoverparameterizeddeepneuralnetworkstrainedwith(stochastic)gradientdescentarewidelyknowntobe
immenselysuccessfularchitecturesforinference. Recentworkshaveattributedthissuccesstotheimplicitregulariza-
tionofgradientdescent–themysteriousabilityofgradientdescenttofindasolutionthatgeneralizeswelldespitethe
non-convexityofthelosslandscape,thepresenceofspuriousoptima,andnoexplicitregularization(Neyshaburetal.,
2015;Soudryetal.,2018).
Toresolvethismystery,severalworkshavestudiedthedynamicsofgradientdescentduringtrainingofneuralnetworks
(Jacot et al., 2018; Chizat et al., 2019; Mei et al., 2019; Chizat & Bach, 2018). An important observation emerging
from these studies has been the effect of initialization on the trajectory of gradient descent. For large initialization,
thegradientdescentdynamicsareapproximatelylinearandcanbedescribedbytheso-calledNeuralTangentKernel
(NTK)(Jacotetal.,2018;Aroraetal.,2019b). Thisregimeisalsoreferredtoaslazytraining(Chizatetal.,2019),
sincetheweightsoftheneuralnetworksdonotchangemuchandremainneartheirinitializationsthroughouttraining,
preventingtheneuralnetworksfromlearningtheunderlyingfeaturesfromthedata.
In contrast, for small initializations, gradient descent dynamics is highly non-linear and exhibits feature learning
(Geigeretal.,2020;Yang&Hu,2021;Meietal.,2019). Additionally,thebenefitofsmallinitializationsoverlargein
termsofgeneralizationperformancehasalsobeenobservedundervarioussettings(Geigeretal.,2020). Forexample,
Chizatetal.(2019)traindeepconvolutionalneuralnetworkswithvaryingscalesofinitializationwhilekeepingother
aspects of the neural network fixed, and a significant drop in performance is observed upon increasing the scale of
initialization. InotherrecentworkssuchasJacotetal.(2022);Boursieretal.(2022);Pesme&Flammarion(2023),
a phenomenon termed saddle-to-saddle dynamics has been observed during training. These works reveal that the
trajectoryofgradientdescentpassesthroughasequenceofsaddlepointsduringtraining,instarkcontrasttothelinear
dynamicsobservedintheNTKregime.
i
4202
beF
41
]GL.sc[
1v62290.2042:viXraTheinvestigationintothegradientdescentdynamicsofneuralnetworkswithsmallinitializationshasspurrednumer-
ousinquiries,yetacomprehensivetheoreticalframeworkremainselusive. Thestudyoflinearneuralnetworkshasled
to valuable insights into the sparsity-inducing tendencies of gradient descent (Woodworth et al., 2020; Arora et al.,
2019a). Thesetendenciesalsoappeartobepresentinnon-linearneuralnetworks(Chizatetal.,2019;Chizat&Bach,
2018), however, rigorous results are limited to two-layer Rectified Linear Unit (ReLU) and Leaky-ReLU networks
under various simple data-specific assumptions such as orthogonal inputs (Boursier et al., 2022), linearly separable
data (Min et al., 2024; Lyu et al., 2021; Wang & Ma, 2023), the XOR mapping (Brutzkus & Globerson, 2019) or
univariatedata(Williamsetal.,2019). Anotherimportantlineofworkhasuncoveredanintriguingphenomenonof
directional convergence among neural network weights during the initial training stages (Maennel et al., 2018; Luo
etal.,2021;Brutzkus&Globerson,2019;Atanasovetal.,2022).InMaenneletal.(2018),itisshownthattheweights
oftwo-layerReLUneuralnetworks,trainedusinggradientflowwithsmallinitialization,convergeindirectionearly
in the training process while maintaining small norm. Although this result primarily describes dynamics near ini-
tialization, it constitutes a crucial step towards a comprehensive understanding of neural network training dynamics
andhascontributedsignificantlytowardsunderstandingthetrainingdynamicsinsomeoftheaforementionedworks
(Boursier et al., 2022; Min et al., 2024; Wang & Ma, 2023; Lyu et al., 2021). However, the work of Maennel et al.
(2018) is limited to two-layer ReLU networks and raises the question of whether this phenomenon holds for other
neuralnetworks.
2 Our Contributions
This work establishes the phenomenon of directional convergence in a more general setting. Specifically, we study
thegradientflowdynamicsresultingfromtrainingoftwo-homogeneousneuralnetworksnearsmallinitializationsand
alsoatcertainsaddlepoints.
AneuralnetworkH,whereH(x;w)isthereal-valuedoutputoftheneuralnetwork,xistheinput,andwisavector
containingalltheweights,isdefinedheretobetwo-(positively)homogeneousif
H(x;cw)=c2H(x;w), forallc≥0.
Whilethisclassdoesnotencompassdeepneuralnetworks,itisbroadenoughtoincludeseveralinterestingtypesof
neural networks. Let σ(x) denote the ReLU (or Leaky-ReLU) function, then, some examples of two-homogeneous
neuralnetworksinclude
• Two-layerReLUnetworks: H(x;{v ,u }H )=PH v σ(x⊤u ).
k k k=1 k=1 k k
• Single-layersquaredReLUnetworks: H(x;{u }H )=PH σ(x⊤u )2.
k k=1 k=1 k
• Deep ReLU networks with only two trainable layers, for example H(x;W ,W ) = v⊤σ(W σ(W x)),
1 2 2 1
where v is a fixed vector. We emphasize that this class includes any L−layer deep ReLU network with
exactlytwotrainablelayers(notnecessarilytwoconsecutivelayers).
We consider a supervised learning setup for training and assume that {x ,y }n is the training dataset, X =
i i i=1
[x ,...,x ] ∈ Rd×n, y = [y ,...,y ]⊤ ∈ Rn, and H(X;w) = [H(x ;w),...,H(x ;w)]⊤ ∈ Rn is the vec-
1 n 1 n 1 n
torcontainingtheoutputofneuralnetworkforallinputs,wherew∈Rk.Wedonotmakeanystructuralassumptions
onthetrainingdataset.
In describing our results, a vital role is played by a quantity we refer to as the neural correlation function (NCF),
whichforafixedvectorz∈RnandneuralnetworkHasaboveisdefinedas
N (w)=z⊤H(X;w).
z,H
TheNCFisameasureofthecorrelationbetweenthevectorzandtheoutputoftheneuralnetwork. ForagivenNCF,
werefertothefollowingconstrainedoptimizationproblemasaconstrainedNCF:
max N (w).
z,H
∥w∥2=1
2
ii(a) (b)
Figure 1: A two-dimensional scenario where a single-layer squared ReLU neural network with 20 hidden neurons
istrainedbygradientdescent. ThenetworkarchitectureisdefinedasH(x ,x ;{u }20 ) = P20 max(0,u x +
1 2 i i=1 i=1 1i 1
u x )2,whereu representstheweightsfortheithneuron. Fortraining,weuse50unitnorminputsandcorrespond-
2i 2 i
inglabelsaregeneratedusingthefunctionH∗(x ,x ) = 5max(0,x )2+4max(0,−x )2. Weusesquarelossand
1 2 1 1
optimize using gradient descent for 50000 iterations with step-size 5·10−5 . At initialization, the weights of each
hiddenneuronaredrawnfromGaussiandistributionwithstandarddeviation10−5.Panel(a):theevolutionoftraining
lossandtheℓ -normofalltheweightswithiterations. Panel(b): theevolutionofarctan(u (t)/u (t))(theangle
2 2i 1i
u (t)makeswiththepositivex−axis)forallhiddenneurons. Weseethatthenormoftheweightsremainsmalland
i
lossbarelychanges,thoughtheweightvectorsconvergeindirectiontotheirfinallocation(denotedwithreddots).
Our first main result (Theorem 5.1) shows that, for square and logistic loss, if the initialization is sufficiently small,
then,thegradientdynamicsspendssufficienttimeneartheoriginsuchthattheweightswareeitherapproximately0,
orapproximatelyconvergeindirectiontonon-negativeKarush-Kuhn-Tucker(KKT)pointsoftheconstrainedNCF
max N (w).
y,H
∥w∥2=1
2
Our next main result (Theorem 5.4) shows a similar directional convergence near certain saddle points for square
loss.Specifically,weshowthatifinitializedinasufficientlysmallneighborhoodofthatsaddlepoint,thenthegradient
dynamicsspendssufficienttimenearthesaddlepointsuchthattheweightswithsmallmagnitudeeitherapproximately
convergeindirectiontonon-negativeKKTpointsoftheconstrainedNCFdefinedwithrespecttotheresidualerrorat
thatsaddlepoint,orareapproximately0.
For illustration, we provide a brief “toy” example showing the phenomenon of directional convergence near small
initialization.Wetrainasingle-layersquaredReLUneuralnetworkusinggradientdescentandsmallinitialization,and
provideinFigure1avisualdepictionof(a)theoveralllossandtheℓ normofthenetworkweights,and(b)theangle
2
theweightvectorsmakewiththepositivehorizontalaxis,allasafunctionofthenumberoftrainingiterations.(Seethe
figurecaptionformorespecificexperimentaldetails.) Itisevidentthatthetraininglossbarelychanges,andthenorm
ofalltheweightsremainssmall,indicatingthatgradientdynamicsisstillneartheorigin. Thisisnotsurprisingsince
theoriginisasaddlepoint.However,thedirectionsoftheindividualweightvectorsfortheneuronsundergosignificant
changes. Thisexperimentsuggeststhatwhilethegradientdynamicsmaynotsignificantlychangetheweightvector
magnitudes,itdoeschangetheirdirections. Further,andperhapsmoreinterestingly,thesedirectionsnotonlychange
but also appear to converge. The objective of this paper is to explain how such a phenomenon could occur by just
minimizingthelossusinggradientdescent,andalsocharacterizethedirectionsalongwhichtheweightsconverge.
Probably the most related existing work to our effort here is Maennel et al. (2018), which exclusively focuses on
two-layer ReLU neural networks. Compared to that work, ours establishes directional convergence near small ini-
tializationsfortwo-homogeneousneuralnetworks, amuchwiderclassofneuralnetworks, highlightingtheinherent
importanceofhomogeneityforthesetypesofphenomena. Asalludedabove,thisclassalsoincludesdeepReLUnet-
workswithonlytwotrainablelayers(notnecessarilytwoconsecutivelayers),forwhichtheresultsofMaenneletal.
(2018) are inapplicable. Further, while Maennel et al. (2018) only focuses on initialization, we also establish direc-
tionalconvergencenearcertainsaddlepoints. Thisextensionisparticularlypertinentbecauseithasbeenobservedin
iiipreviousworksthatneuralnetworksexhibitsaddle-to-saddledynamicsundersmallinitialization. Consequently,our
resultdescribingdynamicsnearsmallinitializationandsaddlepointscouldbeimportantforabetterunderstandingof
thetrainingdynamicsinthefuture.
Finally,whiletheresultofMaenneletal.(2018)hascertainlyadvancedourunderstanding,theiranalysisnearsmall
initializations relies on heuristic arguments and are not completely rigorous; see Min et al. (2024, Section 2.2) for
specificdetails. OurprooftechniqueisrigorousandfundamentallydifferentfromMaenneletal.(2018)tohandlea
widerclassofneuralnetworks.
3 Preliminaries
Inthissection,webrieflydescribesomepreliminaryconceptsthatwillbeusefulinrigorouslydescribingtheproblem.
Throughoutthepaper,∥·∥ denotestheℓ normforavectorandthespectralnormforamatrix.ForanyN ∈N,welet
2 2
[N]={1,2,...,N}denotethesetofpositiveintegerslessthanorequaltoN.Wedenotederivativesbyx˙(t)= dx(t),
dt
andforthesakeofbrevitywemayremovethedependentvariabletifitisclearfromthecontext. Foravectorx,x
i
denotesitsi-thentry. Throughoutthepaper,thek-dimensionalsphereisdenotedbySk−1,andwedefine
β :=sup{∥H(X;w)∥ :w∈Sk−1}andβ˜:=max(2∥y∥ ,∥y∥ +β),
2 2 2
whererecallthatXandydenotethetrainingexamplesandlabels.
Afunctionf : X → RiscalledlocallyLipschitzcontinuousifforeveryx ∈ X thereexistsaneighborhoodU ofx
such that f restricted to U is Lipschitz continuous. A locally Lipschitz continuous function is differentiable almost
everywhere(Borwein&Lewis,2000,Theorem9.1.2).
ForanylocallyLipschitzcontinuousfunction,f :X →R,itsClarkesubdifferentialatapointx∈X istheset
n o
∂f(x)=conv lim ∇f(x ): lim x =x,x ∈Ω ,
i i i
i→∞ i→∞
where Ω is any full-measure subset of X such that f is differentiable for all x ∈ Ω. The set ∂f(x) is nonempty,
convex, and compact for all x ∈ X, and the mapping x → ∂f(x) is upper-semicontinuous (Clarke et al., 1998,
Proposition1.5). Wedenoteby∂f(x)theuniqueminimumnormsubgradient.
Sincetheneuralnetworksconsideredinthispapermaybenon-smooth(asafunctionofw),torigorouslydefinethe
gradientflowforsuchfunctionsweusethenotionofo-minimalstructures(Coste,2000). Inparticular, weconsider
neural networks that are definable under some o-minimal structure, a mild technical assumption that is satisfied by
almostallmodernneuralnetworks(Ji&Telgarsky,2020),includingtheexamplespresentedinSection2. Formally,
an o-minimal structure is a collection S = {S }∞ where each S is set of subsets of Rn containing all algebraic
n n=1 n
subsetsofRn andisclosedunderfiniteunionandintersection,complement,projection,andCartesianproduct. The
elements of S are the finite unions of points and intervals. For a given o-minimal structure S, a set A ⊂ Rn is
1
definableifA ∈ S . Afunctionf : D → Rm withD ⊂ Rn isdefinableifthegraphoff isinS . Sinceaset
n n+m
remains definable under projection, the domain D is also definable; see Coste (2000) for a detailed introduction of
o-minimalstructures.
Usingthenotionofdefinabilityundero-minimalstructures,wedefinegradientflowfornon-smoothfunctionsfollow-
ingDavisetal.(2018);Ji&Telgarsky(2020);Lyu&Li(2020). Afunctionz : I → Rd ontheintervalI isanarc
if it is absolutely continuous for any compact sub-interval of I. An arc is differentiable almost everywhere, and the
compositionofanarcwithalocallyLipschitzfunctionisalsoanarc. ForanylocallyLipschitzanddefinablefunction
f(x),x(t)evolvesundergradientflowoff(x)ifitisanarc,and
x˙(t)∈−∂f(x(t)), fora.e.t≥0. (1)
Ifx(t)evolvesunderpositivegradientflowoff(x),i.e.,x˙(t)∈∂f(x(t)), fora.e.t≥0,westillcallx(t)agradient
flowoff(x). Inwhatfollows,itwillbeclearfromthecontextwhetheritispositiveornegativegradientflow.
iv4 Problem Setup
Withintheframeworkintroducedabove,weconsidertheminimizationof
n
X
L(w)= ℓ(H(x ;w),y ), (2)
i i
i=1
where ℓ(yˆ,y) is a loss function; in this work, we consider square loss, ℓ(yˆ,y) = (yˆ− y)2/2, and logistic loss,
ℓ(yˆ,y)=2ln(1+e−yˆy).
Asalludedabove,wealsoassumethattheneuralnetworksunderconsiderationaretwo-homogeneous,apropertywe
formalizeviathefollowingassumption.
Assumption 1. For any fixed x, H(x;w) is locally Lipschitz and definable under some o-minimal structure that
includespolynomialsandexponential,andforallc≥0,H(x;cw)=c2H(x;w).
InWilkie(1996),itwasshownthatthereexistsano-minimalstructureinwhichpolynomialsandexponentialfunctions
aredefinable.Also,thedefinabilityofafunctionisstableunderalgebraicoperations,composition,inverse,maximum,
andminimum. SinceReLU/Leaky-ReLUisamaximumoftwopolynomials,typicalneuralnetworksinvolvingReLU
activationfunctionaredefinable(Ji&Telgarsky,2020).Also,undertheaboveassumption,L(w)isdefinableforboth
square and logistic loss. Finally, we also require H to be two-homogeneous for our results to hold, which rules out
deepneuralnetworkssuchasdeepReLUnetworkswithmorethan2trainablelayers.
Next,sinceL(w)isdefinable,thegradientfloww(t)isanarcthatsatisfiesfora.e.t≥0
w˙(t)∈−∂L(w(t)),w(0)=δw , (3)
0
wherew isavectorandδisapositivescalarthatcontrolsthescaleofinitialization.
0
Fordifferentialinclusions,itispossibletohavemultiplesolutionsforthesameinitialization. Thisleadstotechnical
difficultiesinprovingourresults. Wewilladdressthisdifficultybymakinguseofthefollowingdefinitionwhichis
inspiredbyLyuetal.(2021)andwillbediscussedinmoredetailinthelatersections.
Definition 4.1. Suppose g(w) : Rk → R is locally Lipschitz and definable under some o-minimal structure, and
considerthefollowingdifferentialinclusionwithinitializationw˜
dw
∈∂g(w),w(0)=w˜, fora.e.t≥0.
dt
Wesayw˜ isanon-branchinginitializationifthedifferentialinclusionhasauniquesolutionforallt≥0.
5 Main Results
5.1 DirectionalConvergenceNearInitialization
Wearenowinpositiontostateourfirstmainresultestablishingapproximatedirectionalconvergenceoftheweights
nearsmallinitialization.
Theorem5.1. Letw beaunitnormvectorandanon-branchinginitializationofthedifferentialinclusion
0
u˙ ∈∂N (u),u(0)=w . (4)
y,H 0
Foranyϵ∈(0,η),whereηisapositiveconstant1,thereexistC >1andδ >0suchthatthefollowingholds: forany
δ ∈(0,δ)andsolutionw(t)ofeq.(3)forsquareorlogisticlosswithinitializationw(0)=δw ,wehave
0
√
(cid:2) (cid:3)
∥w(t)∥ ≤ Cδ, forallt∈ 0,T ,
2
1Here,ηdependsonthesolutionofeq.(4),whichsolelyreliesonX,y,H,w0,andisindependentofδ. SeeLemmaC.7andtheprooffor
moredetails.
vwhereT = ln(C).Further,either
4ββ˜
w(T)⊤uˆ (cid:18) 3 (cid:19)
∥w(T)∥ ≥δη, and ≥1− 1+ ϵ,
2 ∥w(T)∥ 2η
2
whereuˆ isanon-negativeKKTpointof
max N (u)=y⊤H(X;u),
y,H
∥u∥2=1
2
or
∥w(T)∥ ≤2δϵ.
2
Here,ϵrepresentsthelevelofdirectionalconvergenceoftheweight,andC representshowlonggradientflowneeds
tostayneartheorigintoensurethedesiredlevelofdirectionalconvergence.
In words, the first part of the result establishes that for a given choice of ϵ > 0, we can choose δ sufficiently small
suchthatthenormoftheweightsremainssmallforallt∈[0,T],indicatingthatgradientflowremainsneartheorigin.
ThesecondpartquantifieswhathappensatthetimeT;therearetwopossibleoutcomes. Inonescenario,theweights
approximatelyconvergeindirectiontowardsanon-negativeKKTpointoftheconstrainedNCFdefinedwithrespect
toyandneuralnetworkH(additionally,∥w(T)∥ ≥δη,whereηisaconstantthatdoesnotdependonδ.) Incontrast,
2
in the second scenario ∥w(T)∥ ≤ 2δϵ, where we can choose ϵ and δ both to be arbitrarily small. Thus, compared
2
tothefirstscenario,inthesecondscenario,theweightsgetmuchclosertotheorigin. Infact,asitwillbecomemore
clearfromtheproofsketchlater,thishappensbecausethegradientdynamicsoftheNCFcanconvergeto0.
Notethatwerequirew tobeanon-branchinginitializationofeq.(4). Thenecessityforsucharequirementessen-
0
tiallyarisesbecausetherecouldexistmultiplesolutionsfordifferentialinclusions. Wediscussitinmoredetailafter
providing the proof sketch of the above theorem. However, we note that if the neural network H(x;w) has locally
Lipschitz gradients then this requirement is always satisfied, since in that case eq. (4) always has a unique solution.
Thiswouldinclude,forexample,thesquaredReLUneuralnetwork.
5.1.1 ProofSketchofTheorem5.1
We provide a brief proof sketch for Theorem 5.1 here; the complete proof can be found in Appendix C. The proof
ultimatelyreliesupontwolemmas. Thefirstonedescribestheapproximatedynamicsofw(t)intheinitialstagesof
trainingforsmallinitialization.
Lemma5.2. LetC >1beanarbitrarilylargeconstantandw(t)beanysolutionofeq.(3)forsquareorlogisticloss
q h i
withinitializationw(0)=δw ,whereδ ≤ 1 and∥w ∥ =1. Then,forallt∈ 0,ln(C) ,
0 C 0 2 4ββ˜
√
∥w(t)∥ ≤ Cδ. (5)
2
Further,forthedifferentialinclusion
u˙ ∈∂N (u),u(0)=w , (6)
y,H 0
andanyϵ>0thereexistsasmallenoughδ >0suchthatforanyδ ∈(0,δ),
(cid:13) (cid:13) (cid:20) (cid:21)
(cid:13)w(t) (cid:13) ln(C)
(cid:13) −u(t)(cid:13) ≤ϵ, forallt∈ 0, , (7)
(cid:13) δ (cid:13) 4ββ˜
2
whereu(t)isacertainsolutionofeq.(6).
The first part of the lemma shows that for sufficiently small δ the gradient dynamics can spend an arbitrarily large
timeneartheorigin. Tounderstandtheimplicationsofthesecondpart,letusfirstfocusonthedifferentialinclusionin
eq.(6),whichisthepositivegradientflowoftheNCFdefinedwithrespecttoyandneuralnetworkH. Fromeq.(7),
weobservethatforsmallinitialization,intheinitialstagesofthedynamics,w(t)/δisapproximatelyequaltou(t).
viNow, since δ is a positive scalar, dividing w(t) by it does not change the direction of w(t). Further, note that the
dynamicsofu(t)donotdependonδandtheapproximationineq.(7)canbemadetoholdforanarbitrarilylongtime
bychoosingsufficientlysmallδ. Thus,ifwechooseC largeenoughsuchthattheapproximationineq.(7)isvalidfor
asufficientlylongtimeinwhichu(t)approximatelyconvergesindirection,thenbyvirtueofeq.(7),w(t)wouldalso
approximatelyconvergeindirection.
Thus, ouraimistoestablishapproximatedirectionalconvergenceofu(t)withinsomefinitetime. Forthis, weturn
towardsanalyzingthegradientflowdynamicsoftheNCF.Recallthat,foragivenvectorz,theNCFisdefinedas
N (u)=z⊤H(X;u), (8)
z,H
andgradientflowwillsatisfyfora.e.t≥0
du
∈∂N (u),u(0)=u , (9)
dt z,H 0
whereu istheinitialization.
0
SincethefunctionvalueincreasesalongthegradientflowtrajectoryandN (u)maynotbeboundedfromabove,
z,H
the gradient flow trajectory can potentially diverge to infinity and take N (u) to infinity along with it. However,
y,H
inthefollowinglemmaweshowthatthegradientflowwillalwaysconvergeindirection,andalsocharacterizethose
directions.
Lemma 5.3. For any solution u(t) of eq. (9), either lim N (u(t)) = ∞ or lim N (u(t)) = 0. Also,
t→∞ z,H t→∞ z,H
either lim u(t) exists or lim u(t) = 0. If lim u(t) exists then its value, say u∗, must be a non-
t→∞ ∥u(t)∥2 t→∞ t→∞ ∥u(t)∥2
negativeKKTpointoftheoptimizationproblem
max N (u)=z⊤H(X;u). (10)
z,H
∥u∥2=1
2
Theabovelemmastatesthatanysolutionofeq.(9)willeitherconvergeto0orconvergeindirectiontoaKKTpoint
oftheconstrainedNCF.Toestablishdirectionalconvergenceintheabovelemma,wefollowasimilartechniqueasin
Ji&Telgarsky(2020,Theorem3.1).
ToproveTheorem5.1fromhere, wecombineLemma5.2andLemma5.3. Fromagiveninitializationδw , weget
0
w . Then,forthesolutionu(t)ofthedifferentialinclusionineq.(6),usingLemma5.3,wechooseT largeenough
0
such that u(T) either approximately converges in direction to the KKT point of the NCF or gets close to 0. Then,
basedonthatT,usingLemma5.2,wechooseδsufficientlysmallsuchthatw(t)/δisclosetou(t),forallt∈[0,T].
Theresultfollows.
Thereis,however,oneissuewiththeaboveargument. Thedifferentialinclusionscouldhavemultiplesolutions,and
ineq.(7), theapproximationholdsforsomesolutionofeq.(6); itisnotknownbeforehandwhichsolutionitwould
be. Therefore, we would need to choose T large enough such that all solutions of equation 6 have approximately
convergedindirection. However, thismaynotbepossibleforallinitializationsw . Toillustratethisweconsidera
0
simpleexample.
Considerthefunctionf(u ,u ) = u |u |thatsatisfiesAssumption1,andisdifferentiableeverywhereexceptalong
1 2 1 2
the line u = 0. In Figure 2 we plot its gradient field. Note that u˜ = [1,0]⊤ is a critical point for f(u ,u ), i.e.,
2 1 2
0∈∂f(u˜). Thus,ifinitializedatu˜,onepossiblegradientflowsolutionistostayatu˜ forallt≥0. However,∂f(u˜)
containsothervectorswhichcouldleadtogradientflowescapingfromu˜. Moreover,onecouldconstructagradient
flowsolutionthatcanspendarbitraryamountoftimeatu˜ beforeescapingit. Specifically,foranyfiniteT,
 (cid:20) (cid:21)
1

0
, forallt∈[0,T]
u T(t)= (cid:20) cosh(t−T)(cid:21)

sinh(t−T)
, forallt≥T,
√ √
isapossiblegradientflowsolution. Wenotethatlim u (t)/∥u (t)∥ = [1/ 2,1/ 2]⊤, however, clearlyfor
t→∞ T T 2 √ √
anyfinitetimeT wecanchooseT largesuchthatu (T)/∥u (T)∥ staysawayfrom[1/ 2,1/ 2]⊤. Thus,wecan
T T 2
viiFigure2: Thegradientfieldoff(u ,u )=u |u |.
1 2 1 2
notestablishfinitetimeapproximatedirectionalconvergenceforallpossiblegradientflowsolutions. Completedetails
forthisexamplecanbefoundinAppendixE.
Toaddressthisissue,weonlyconsiderinitializationwhichleadstoauniquesolution. Inparticular,weassumew to
0
beanon-branchinginitializationofeq.(4). Asnotedearlier,iftheneuralnetworkhaslocallyLipschitzgradients,then
this requirement is always satisfied. However, for more general networks such as two-layer ReLU neural networks
the above assumption may appear somewhat restrictive. That said, it is worth noting that a similar assumption was
also made in Lyu et al. (2021), where the full training dynamics of two-layer Leaky-ReLU neural networks were
investigated in a simple setting involving linearly separable data. Furthermore, Maennel et al. (2018) addresses this
challenge of non-uniqueness by asserting that the differential inclusion resulting from the gradient flow of the loss
function will have a unique solution in “almost all” cases, but do not provide a formal proof. We leave it as an
importantfutureresearchdirectiontohandlemoregeneralinitializations.
5.1.2 ACorollaryforSeparableNeuralNetworks
WenextconsiderthecasewhenH(x;w)isseparableandcanbedividedintosmallerneuralnetworks.Inthefollowing
lemma,wedescribethedirectionalconvergenceforsuchneuralnetworksnearsmallinitialization.
Corollary 5.3.1. Suppose we can write w = [w ,...,w ]⊤ such that H(x;w) = PH H (x;w ), for all x.
1 H i=1 i i
ConsiderthesamesettingasinTheorem5.1,then,foranyϵ∈(0,η),whereηisapositiveconstant,thereexistC >1
andδ >0suchthatforanyδ ∈(0,δ),andforalli∈[H],either
√ w (T)⊤uˆ (cid:18) 3 (cid:19)
Cδ ≥∥w (T)∥ ≥δη, and i i ≥1− 1+ ϵ,
i 2 ∥w (T)∥ 2η
i 2
whereuˆ isanon-negativeKKTpointof
i
max N (u)=y⊤H (X;u), (11)
∥u∥2=1
y,Hi i
2
or
ln(C)
∥w (T)∥ ≤2δϵ, whereT = .
i 2 4ββ˜
Theaboveresultestablishesthatforseparableneuralnetworks,theweightsofsmallerneuralnetworksapproximately
converge in direction to the KKT points of the optimization problem in eq. (11), the constrained NCF defined with
respecttotheoutputofsmallerneuralnetworks.
Indeed, this is precisely what we observed for the “toy” experiments depicted in Figure 1. Recall, in that case, the
neuralnetworkwasthesumofsquaredReLUfunctionsandhencesatisfiesthesettingofCorollary5.3.1.Inthebottom
ofFigure3, weagainplottheevolutionofthedirectionoftheweightsforeachhiddenneuron. Onthetop, weplot
theconstrainedNCF withrespecttothe outputofeachneuron, which willbeidenticalfor each neuron. We clearly
observethattheweightsofeachneuronconvergeindirectiontowardsKKTpointsoftheconstrainedNCF.
viiiFigure3: ThelowerpartshowsthecontentofFigure1bwiththehorizontalandverticalaxesinterchanged. Thetop
plotshowstheconstrainedNCFN (θ) = Pn y max(0,[cos(θ),sin(θ)]⊤x )2. AspredictedbyCorollary5.3.1,
y,H i=1 i i
theneuronweightsconvergeindirectiontotheKKTpointsoftheNCF.
5.2 DirectionalConvergenceNearSaddlePoints
Severaltheoreticalandempiricalworkshaveobservedasaddle-to-saddledynamicsduringtrainingofneuralnetworks
withsmallinitializationandsquareloss(Jacotetal.,2022;Pesme&Flammarion,2023;Boursieretal.,2022;Jinetal.,
2023).Theevolutionoflossalternatesbetweenbeingstagnantanddecreasingsharply,almostlikeapiecewiseconstant
function. This indicates that weights move from one saddle of the loss function to another during training. Some
theoretical works further show that at each saddle point only certain number of weights are non-zero. For example,
Pesme&Flammarion(2023)provessaddle-to-saddledynamicsintwo-homogeneousdiagonallinearnetworks,where
at each saddle points only few weights are non-zero. The authors of Boursier et al. (2022) study two-layer ReLU
networkwithorthogonalinputs,andshowthatgradientflowentersneighborhoodofasaddlepointwhereonesetof
neuronshavehighnormwhileothershavezeronorm.
In this section, we show that the directional convergence near initialization described in the previous section also
occursnearcertainsaddlepointsforsquareloss. However,therecouldbedifferentkindsofsaddlepointsthroughout
thelosslandscape. Thechoiceofsaddlepointsconsideredhereismotivatedbytheaboveobservations,suchasonly
acertainnumberofweightsbeingnon-zeroatthesaddlepoints.
Weassumethattheweightsofneuralnetworkwcanbedividedintotwosets,w = [w ,w ],suchthatH(x;w) =
n z
H (x;w )+H (x;w ),whereH (x;w )andH (x;w )eachsatisfyAssumption1.Forsquareloss,weminimize
n n z z n n z z
n
L(w ,w )=
1X
∥H (x ;w )+H (x ;w )−y ∥2 =
1
∥H (X;w )+H (X;w )−y∥2. (12)
n z 2 n i n z i z i 2 n n z z
i=1
Thesaddlepointofeq.(12)thatweconsiderheresatisfiesthefollowing.
Assumption2. Weassume {w ,w }isasaddlepointofeq.(12)suchthat
n z
∥w ∥ ∈[m,M], and∥w ∥ =0, (13)
n 2 z 2
wherem,M arepositiveconstants. Further,ifH (x;w )doesnothavealocallyLipschitzgradient,thenweassume
n n
thatthereexistsγ >0suchthatforallw satisfying∥w −w ∥ ≤γ itholdsthat
n n n 2
⟨w −w ,s⟩≥0,wheres∈−∂ L(w ,0). (14)
n n wn n
Intheaboveassumption,w isthesetofweightswithhighnormwhilew containssetsofweightswithzeronorm.
n z
Duetohomogeneity,H (x;w )=0,andthusH (x;w )iseffectivelytheoutputoftheneuralnetworkat{w ,w }.
z z n n n z
When H (x;w ) does not have locally Lipschitz gradient, we require eq. (14) to ensure that if w initialized near
n n n
w ,thenitstaysnearitforasufficientlylongtime. Wediscussthemotivationforthisinequalityafterdiscussingour
n
maintheoremofthissection,statedbelow.
ixTheorem5.4. Let{w ,w }satisfyAssumption2,anddefiney=y−H (X;w ). Supposeζ isaunitnormvector
n z n n z
andanon-branchinginitializationofthedifferentialinclusion
u˙ ∈∂N (u),u(0)=ζ . (15)
y,Hz z
Foranyϵ∈(0,η),whereηisapositiveconstant2,thereexistC >1andδ >0suchthatthefollowingholds: forany
δ ∈(0,δ)andgradientflowsolution{w (t),w (t)}ofeq.(12)thatsatisfiesfora.e.t≥0
n z
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
w˙ ∂ L(w ,w ) w (0) w +δζ
n ∈− wn n z , n = n n , (16)
w˙ ∂ L(w ,w ) w (0) w +δζ
z wz n z z z z
where∥ζ ∥ isaunitnormvector,wehave
n 2
∥w (t)−w ∥2+∥w (t)−w ∥2 ≤Cδ2, forallt∈(cid:2) 0,T(cid:3) , (17)
n n 2 z z 2
whereT = 1 ln(C),andM isaconstan3. Further,either
M2 2
w (T)⊤uˆ (cid:18) 3 (cid:19)
∥w (T)∥ ≥δη, and z ≥1− 1+ ϵ,
z 2 ∥w (T)∥ 2η
z 2
whereuˆ isanon-negativeKKTpointof
max N (u)=y⊤H (X;u),
∥u∥2=1
y,Hz z
2
or
∥w (T)∥ ≤2δϵ.
z 2
Intheabovetheorem,theinitializationisnearasaddlepointwhichsatisfiesAssumption2,andδcontrolshowfarthe
initializationisfromthesaddlepoint. Thevectoryrepresentstheresidualerroratthesaddlepointandplaysthesame
roleasydidinTheorem5.1. Providedζ isanon-branchinginitializationofeq.(15), weshowthatforsufficiently
z
smallδ, thegradientflowspendsenoughtimenearthesaddlepointsuchthatweightsofsmallmagnitudew either
z
approximatelyconvergeindirectiontotheKKTpointoftheNCFdefinedwithrespecttoyandH ,orgetscloseto0.
z
Theabovetheorem,similartoTheorem5.1,showsdirectionalconvergenceamongweightsofsmallmagnitude. The
prooftechniqueissimilartotheproofofTheorem5.1;fordetailsseeAppendixD.
Wenextexplainthemotivationforeq.(14)inAssumption2whenH (x;w )doesnothavelocallyLipschitzgradi-
n n
ents. Ourproofoftheabovetheoremcruciallyreliesonshowingthatw (t)remainsclosetow andw (t)remains
n n z
small for a sufficiently long time; i.e., eq. (17) holds. Suppose w (t) remains small, then the evolution of w (t) is
z n
approximately governed by the gradient flow of L(w ,0). To understand the gradient flow dynamics of L(w ,0)
n n
nearw ,weusethefollowinglemma.
n
Lemma5.5. Suppose{w ,w }isasaddlepointofeq.(12)suchthat∥w ∥ ∈[m,M], and∥w ∥ =0.Then,
n z n 2 z 2
0∈∂ L(w ,0).
wn n
IfH (x;w )haslocallyLipschitzgradients,then∇ L(w ,0)wouldbesmallandvarysmoothlyintheneighbor-
n n wn n
hoodofw . Thissufficestoensurethatifw (0)isclosetow , thenw (t)remainsclosetow forasufficiently
n n n n n
longtime.
However,ifH (x;w )doesnothavelocallyLipschitzgradients,then0 ∈ ∂ L(w ,0)but∂ L(w ,0)maybe
n n wn n wn n
largenearw and,moreimportantly,pointawayfromw . Thispreventsusfromensuringw (t)remainsnearw .
n n n n
Forexampleconsiderg(u) = (u |u |−1)2 andletu˜ = [1,0]T. Then,0 ∈ ∂g(u˜). Letuδ = [1+δ,δ]T. Then,for
1 2
anyδ ∈ (0,0.1)ands ∈ −∂g(uδ),∥s∥ ≥ 1ands⊤(u˜ −uδ) < 0. Therefore,nomatterhowcloseuδ istou˜,the
2
gradientflowwillquicklygetawayfromu˜ (seetheAppendixFfordetails).
Hence,werequiretheelementsin−∂ L(w ,0)tobepositivelycorrelatedwithw −w toensurew (t)remains
wn n n n n
nearw . Thefollowinglemmafurtherclarifiestheimpactofeq.(14).
n
2Here,ηdependsonthesolutionofeq.(15),whichsolelyreliesonX,y¯,Hz,ζz,andisindependentofδ.Seetheproofformoredetails.
3M2dependsonβ,∥y∥2andvariousparametersassociatedwithHzandHnnear{wn,wz}suchastheirLipschitzconstant,maximumvalue
etc.Importantly,itdoesnotdependonϵandδ.
xLemma 5.6. Suppose {w ,w } is a saddle point of eq. (12) such that ∥w ∥ ∈ [m,M], and∥w ∥ = 0. Then,
n z n 2 z 2
w is a saddle point of L(w ,0). Further, if there exists γ > 0 such that eq. (14) holds for all w satisfying
n n n
∥w −w ∥ ≤γ,thenw isalocalminimaofL(w ,0).
n n 2 n n
Inwords, ifeq.(14)holdsinsomeneighborhoodofw , thenw isalocalminimaofL(w ,0). Thus, ifw (0)is
n n n n
closetow ,thenonecanexpectw (t)toremainnearw .
n n n
6 Conclusions and Future Directions
In this work, we studied the gradient flow dynamics of two-homogeneous neural networks near small initializations
andsaddlepoints,andshowedtheapproximatedirectionalconvergenceoftheirweightsintheinitialstagesoftraining.
An important future direction is, of course, to study the entire gradient flow dynamics of neural networks, and our
work could be an important step towards a comprehensive understanding of training dynamics of neural networks.
Particularly,forsuccessfultrainingundersmallinitialization,thegradientdynamicswillhavetoeventuallyescapethe
origin. Theescapedirectionmaybedeterminedbythedirectionstowhichtheweightsconvergewhilethedynamics
isneartheorigin. Thisalsoholdstruewhileescapingothersaddlepointsencounteredbygradientdynamicsduring
thetrainingprocess,whichnotablyisknowntoundergosaddle-to-saddledynamics.
Anotherpossiblefuturedirectionistoinvestigatesimilardirectionalconvergenceindeeperneuralnetworks. Fornon-
smoothneuralnetworks,werequiredanadditionalassumptionontheinitializationtoensureuniqueness. Itwouldbe
interestingtoanalyzescenarioswhensuchassumptionsdonothold. Wedeferthattoafutureinvestigation.
References
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. In
AdvancesinNeuralInformationProcessingSystems,2019a.
SanjeevArora,SimonSDu,WeiHu,ZhiyuanLi,RussRSalakhutdinov,andRuosongWang. Onexactcomputation
withaninfinitelywideneuralnet. InAdvancesinNeuralInformationProcessingSystems,2019b.
AlexanderAtanasov,BlakeBordelon,andCengizPehlevan. Neuralnetworksaskernellearners: Thesilentalignment
effect. InInternationalConferenceonLearningRepresentations,2022.
J.M.BorweinandA.S.Lewis. ConvexAnalysisandNonlinearOptimization. SpringerVerlag,Berlin,Heidelberg,
NewYork,2000.
EtienneBoursier,LoucasPillaud-Vivien,andNicolasFlammarion.GradientflowdynamicsofshallowreLUnetworks
forsquarelossandorthogonalinputs. InAdvancesinNeuralInformationProcessingSystems,2022.
AlonBrutzkusandAmirGloberson. Whydolargermodelsgeneralizebetter? AtheoreticalperspectiveviatheXOR
problem. InProceedingsofthe36thInternationalConferenceonMachineLearning,2019.
LénaïcChizatandFrancisBach. Ontheglobalconvergenceofgradientdescentforover-parameterizedmodelsusing
optimal transport. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.,
2018.
LénaïcChizat,EdouardOyallon,andFrancisBach. Onlazytrainingindifferentiableprogramming. InAdvancesin
NeuralInformationProcessingSystems,2019.
F. H. Clarke, Yu. S. Ledyaev, R. J. Stern, and P. R. Wolenski. Nonsmooth Analysis and Control Theory. Springer-
Verlag,Berlin,Heidelberg,1998. ISBN0387983368.
F.H.Clarke. OptimizationandNonsmoothAnalysis. WileyNewYork,1983.
M.Coste. AnIntroductiontoO-minimalGeometry. Dottoratodiricercainmatematica/UniversitàdiPisa,Diparti-
mentodiMatematica.Istitutieditorialiepoligraficiinternazionali,2000. ISBN9788881472260.
xiDamekDavis,DmitriyDrusvyatskiy,ShamM.Kakade,andJ.Lee. Stochasticsubgradientmethodconvergesontame
functions. FoundationsofComputationalMathematics,20:119–154,2018.
AleksejF.Filippov. Differentialequationswithdiscontinuousrighthandsides. InMathematicsandItsApplications,
1988.
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in deep
neuralnetworks. JournalofStatisticalMechanics: TheoryandExperiment,2020(11):113301,nov2020.
ArthurJacot,FranckGabriel,andClementHongler. Neuraltangentkernel: Convergenceandgeneralizationinneural
networks. InAdvancesinNeuralInformationProcessingSystems,2018.
ArthurJacot,FrançoisGed,BerfinS¸ims¸ek,ClémentHongler,andFranckGabriel. Saddle-to-saddledynamicsindeep
linearnetworks: Smallinitializationtraining,symmetry,andsparsity,2022.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In Advances in Neural
InformationProcessingSystems,volume33,2020.
Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon Shaolei Du, and Jason D. Lee. Understanding incremental learning of
gradientdescent: Afine-grainedanalysisofmatrixsensing. InProceedingsofthe40thInternationalConference
onMachineLearning,2023.
Tao Luo, Zhi-Qin John Xu, Zheng Ma, and Yaoyu Zhang. Phase diagram for two-layer relu neural networks at
infinite-widthlimit. JournalofMachineLearningResearch,22(71):1–47,2021.
KaifengLyuandJianLi. Gradientdescentmaximizesthemarginofhomogeneousneuralnetworks. InInternational
ConferenceonLearningRepresentations,2020.
KaifengLyu,ZhiyuanLi,RunzheWang,andSanjeevArora.Gradientdescentontwo-layernets:Marginmaximization
andsimplicitybias. InAdvancesinNeuralInformationProcessingSystems,volume34,2021.
HartmutMaennel,OlivierBousquet,andSylvainGelly. Gradientdescentquantizesrelunetworkfeatures,2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks:
dimension-free bounds and kernel limit. In Proceedings of the Thirty-Second Conference on Learning Theory,
pp.2388–2464,2019.
Hancheng Min, Enrique Mallada, and Rene Vidal. Early neuron alignment in two-layer reLU networks with small
initialization. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
BehnamNeyshabur,RyotaTomioka,andNathanSrebro. Insearchoftherealinductivebias: Ontheroleofimplicit
regularizationindeeplearning. InICLR(Workshop),2015.
Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. In Thirty-seventh
ConferenceonNeuralInformationProcessingSystems,2023.
DanielSoudry,EladHoffer,MorShpigelNacson,SuriyaGunasekar,andNathanSrebro. Theimplicitbiasofgradient
descentonseparabledata. J.Mach.Learn.Res.,19(1):2822–2878,January2018.
MingzeWangandChaoMa. Understandingmulti-phaseoptimizationdynamicsandrichnonlinearbehaviorsofreLU
networks. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
A. J. Wilkie. Model completeness results for expansions of the ordered field of real numbers by restricted pfaffian
functions and the exponential function. Journal of the American Mathematical Society, 9(4):1051–1094, 1996.
ISSN08940347,10886834. URLhttp://www.jstor.org/stable/2152916.
FrancisWilliams,MatthewTrager,DanielePanozzo,ClaudioSilva,DenisZorin,andJoanBruna. Gradientdynamics
ofshallowunivariaterelunetworks. InAdvancesinNeuralInformationProcessingSystems,volume32,2019.
xiiBlakeWoodworth,SuriyaGunasekar, JasonD.Lee, EdwardMoroshko, PedroSavarese, ItayGolan, DanielSoudry,
andNathanSrebro.Kernelandrichregimesinoverparametrizedmodels.InProceedingsofThirtyThirdConference
onLearningTheory,pp.3635–3673,2020.
GregYangandEdwardJ.Hu. Tensorprogramsiv: Featurelearningininfinite-widthneuralnetworks. InProceedings
ofthe38thInternationalConferenceonMachineLearning,pp.11727–11737,2021.
Appendices
A Key Properties of o-minimal Structures and Clarke Subdifferentials
Inthissectionwegiveabriefoverviewofo-minimalstructures,andtherelevantpropertiesofClarkesubdifferentials
thatareusedintheproofsofourresults. WeborrowmuchofthediscussionbelowfromJi&Telgarsky(2020);Davis
etal.(2018)
Ano-minimalstructureisacollectionS = {S }∞ ,whereeachS issetofsubsetsofRn,thatsatisfiesfollowing
n n=1 n
axioms:
• TheelementsofS arefiniteunionsofpointsandintervals.
1
• AllalgebraicsubsetsofRnareinS .
n
• Foralln,S isaBooleansubalgebraofthepowersetofRn.
n
• IfA∈S ,B∈S ,thenA×B∈S
n m n+m
• IfP :Rn+1 →RnistheprojectiononfirstncoordinatesandA∈S ,thenP(A)∈S .
n+1 n
Foragiveno-minimalstructureS,asetA ⊂ Rn isdefinableifA ∈ S . Afunctionf : D → Rm withD ⊂ Rn is
n
definableifthegraphoff isinS . Sinceasetremainsdefinableunderprojection,thedomainDisalsodefinable.
n+m
InWilkie(1996),itwasshownthatthereexistsano-minimalstructureinwhichpolynomialsandexponentialfunctions
aredefinable.Also,thedefinabilityofafunctionisstableunderalgebraicoperations,composition,inverse,maximum,
andminimum.SinceReLUandLeaky-ReLUcaneachbeexpressedasmaximumsoftwopolynomials,itcanbeshown
thatthefunctionswewillconsiderinthispaperaredefinable.
It is also true that deep neural networks with ReLU or Leaky-ReLU activation, and different kinds of layers, are
definable. Forcompleteness,westatethatresulthereasalemma.
LemmaA.1. (Ji&Telgarsky,2020,LemmaB.2)Supposethereexistk,d ,d ,...,d > 0andLdefinablefunctions
0 1 L
(g 1,...,g L)whereg
j
:Rd0 ×...×Rdj−1 ×Rk →Rdj. Leth 1(x,w):=g 1(x,w),andfor2≤j ≤L,
h (x,w):=g (x,h (x,w),...,h (x,w),w)
j j 1 j−1
thenallh aredefinable. (Itsufficesifeachoutputcoordinateofg istheminimumormaximumoversomefiniteset
j j
ofpolynomials,whichallowsforlinear,convolutional,ReLU,max-poolinglayersandskipconnections.)
Wealsonotethat,sincedefinabilityisstableundercomposition,theobjectivefunctionsarisingforquadraticlossand
logisticlossarealsodefinable.
A.1 ChainRulesforNon-differentiableFunctions
RecallthatforanylocallyLipschitzcontinuousfunctionf :X →R,itsClarkesubdifferentialatapointx∈X isthe
set
n o
∂f(x)=conv lim ∇f(x ): lim x =x,x ∈Ω ,
i i i
i→∞ i→∞
xiiiwhere Ω is any full-measure subset of X such that f is differentiable at each of its points, and ∂f(x) denotes the
uniqueminimumnormsubgradient.
The functions considered in this paper can be compositions of non-differentiable functions. We use Clarke’s chain
ruleofdifferentiation,describedinthefollowinglemma,tocomputetheClarkesubdifferentialsinsuchcases.
LemmaA.2. (Clarke,1983,Theorem2.3.9)Leth ,...,h :Rd →Randg :Rn →RbelocallyLipschitzfunctions,
1 n
andf(x)=g(h (x),...,h (x)),then,
1 n
( n )
X
∂f(x)⊆conv α ζ :ζ ∈∂h (x),α∈∂g(h (x),...,h (x)) .
i i i i 1 n
i=1
Thechainruleforgradientflowdescribedinthenextlemma,whichiscrucialforouranalysis,essentiallyimpliesthat
fordifferentialinclusions∂f(x)playsthesameroleas∇f(x)doesfordifferentialequations.
LemmaA.3. (Davisetal.,2018,Lemma5.2)(Ji&Telgarsky,2020,LemmaB.9)GivenalocallyLipschitzdefinable
functionf :D→RwithanopendomainD,foranyintervalI andanyarcx:I →D,itholdsfora.e.t∈I that
d(f(x(t)))
=⟨x∗(t),x˙(t)⟩, forallx∗(t)∈∂f(x(t)).
dt
Further,ifx:I →Dsatisfies
x˙ ∈∂f(x), fora.e.t≥0,
then,itholdsfora.e.t≥0that
d(f(x(t)))
x˙(t)=∂f(x(t)), and =∥∂f(x(t))∥2,
dt 2
andtherefore,
Z t
f(x(t))−f(x(0))= ∥∂f(x(s))∥2ds,∀t≥0.
2
0
A.2 TheKurdyka-LojasiewiczInequality
Forgradientflowtrajectoriesthatarebounded,theKurdyka-LojasiewiczInequalityisusefulforshowingconvergence,
essentiallybyestablishingtheexistenceofadesingularizingfunction,whichisformallydefinedasfollows.
DefinitionA.1. AfunctionΨ : [0,ν) → RiscalledadesingularizingfunctionwhenΨiscontinuouson[0,ν)with
Ψ(0)=0,anditiscontinuouslydifferentiableon(0,ν)withΨ′ >0.
The following lemma, which can be seen as an unbounded version of the Kurdyka-Lojasiewicz Inequality, plays an
importantroleinestablishingthedirectionalconvergenceofgradientflowtrajectoriesoftheneuralcorrelationfunction
wherethetrajectoriescanbeunbounded.
LemmaA.4. (Ji&Telgarsky,2020,Lemma3.6)GivenalocallyLipschitzdefinablefunctionf withanopendomain
D ⊂ {x|∥x∥ > 1},foranyc,η > 0,thereexistsaν > 0andadefinabledesingularizingfunctionΨon[0,ν)such
2
that
Ψ′(f(x))∥x∥ ∥∂f(x)∥ ≥1, iff(x)∈(0,ν)and∥∂ f(x)∥ ≥c∥x∥η∥∂ f(x)∥ ,
2 2 ⊥ 2 2 r 2
D E
where∂ f(x)= ∂f(x), x x and∂ f(x)=∂f(x)−∂ f(x).
r ∥x∥2 ∥x∥2 ⊥ r
B Additional Notation and Some Preliminary Lemmata
For notational convenience when dealing with Clarke subdifferentials, we introduce the following notation for sets
containingvectors.
xiv• ∀A,B ⊆Rd,A±B :={x±y:x∈A,y∈B}
• ∀B ⊆Rd, andc∈R,cB :={cy:y∈B}
• ∀B ⊆Rd, andp∈Rd,⟨p,B⟩:={p⊤y:y∈B}⊆R
• Foranynorm∥·∥onRd,∥B∥:={∥y∥,y∈B}⊆R
Thefollowinglemmastatestwoimportantpropertiesofhomogeneousfunctions.
Lemma B.1. ((Lyu & Li, 2020, Theorem B.2), (Ji & Telgarsky, 2020, Lemma C.1)) Let F : Rk → R be a locally
LipschitzandL−positivelyhomogeneousforsomeL>0,then:
1. Foranyw∈Rk andc≥0,
∂F(cw)=cL−1∂F(w).
2. Foranyw∈Rk,
w⊤s=LF(s), foralls∈∂F(w).
Thisresultgivesrisetothefollowingcorollary,whichweusefrequentlyinouranalysis.
CorollaryB.1.1. Foranyw∈Rk andc≥0,
w⊤s=2H(x;w), foralls∈∂H(x;w),
and
∂H(x;cw)=c∂H(x;w).
C Proofs Omitted from Section 5.1
InthissectionwefirstproveLemma5.2andLemma5.3,andthenusethemtoultimatelyproveTheorem5.1.
C.1 ProofofLemma5.2
ToproveLemma5.2,wemakeuseofthefollowinglemma. Itshowsthatiftwodifferentialinclusionsareinitialized
atthesamepoint, andthedifferencebetweenthemissmallinaboundedinterval, thenthedifferencebetweentheir
solutions is also small. This is a well-known stability result (Filippov, 1988, Theorem 1, Section 8); we provide a
proofinAppendixGforcompleteness.
LemmaC.1. Considerthefollowingdifferentialinclusionsfort∈[0,T]:
n
du˜ X
∈ z ∂H(x ;u˜),u˜(0)=u , (18)
dt i i 0
i=1
and
n
du X
∈ (z +f (t))∂H(x ;u),u(0)=u , (19)
dt i i i 0
i=1
whereT isfinite,and|f (t)| ≤δforalli∈[n]andt∈[0,T]. Then,foranyϵ>0thereexistsaδ >0,suchthatfor
i 2
eachsolutionu(t)ofeq.(19)thereexistsasolutionu˜(t)ofeq.(18)satisfying
max ∥u(t)−u˜(t)∥ ≤ϵ. (20)
2
t∈[0,T]
xvProofofLemma5.2. UsingthechainrulefromLemmaA.2,thegradientflowdynamicsare
n
X
w˙ ∈− e ∂H(x ;w),w(0)=δw , (21)
i i 0
i=1
wheree :=∂ ℓ(H(x ;w),y ). Letebethen−dimensionalvectorcontainingasitsithelementsthevaluese ,forall
i yˆ i i i
i. Forsquareloss,∂ (yˆ,y) = yˆ−y,andforlogisticloss,∂ (yˆ,y) = −(2ye−yˆy)/(1+e−yˆy). Notethat,forlogistic
yˆ yˆ
loss,∥e(t)∥ ≤2∥y∥ . Next,recallthat
2 2
β :=sup{∥H(X;w)∥ :w∈Sk−1},
2
sofromtwo-homogeneityofH(x;w),wehave∥H(x;w)∥ ≤ β∥w∥2. Therefore,forsquareloss,sincelossalways
2 2
decreases, ∥e(t)∥ ≤ ∥e(0)∥ = ∥y−H(X;δw )∥ ≤ ∥y∥ +δ2β ≤ ∥y∥ +β,whereweusedδ2 ≤ 1/C < 1.
2 2 0 2 2 2
Also,recallthatβ˜=max(2∥y∥ ,∥y∥ +β),thus,forbothlosses,∥e(t)∥ ≤β˜.
2 2 2
Now,wedefinez(t)=∥w(t)∥2 andnotethatz(0)=δ2 ≤1/C <1. Sincew(t)isacontinuousfunction,soisz(t).
2
Hence,thereexistssomeγ > 0,suchthatforallt ∈ (0,γ),z(t) < 1. WedefineTˆtobethesmallestt > 0suchthat
z(Tˆ)=1. Itfollowsthatforallt∈[0,Tˆ],z(t)≤1. UsingCorollaryB.1.1,
n
z˙ =2w⊤w˙ =−4X e H(x ;w)=−4H(X;w)⊤e≤4β∥w∥2β˜=4ββ˜z, (22)
i i 2
i=1
andsoz(t)≤δ2e4ββ˜timplies
(cid:18) (cid:19)
1 1
Tˆ ≥ ln .
4ββ˜ δ2
Further,sinceδ ≤ √1 ,wehavethat 1 ln(C)≤Tˆimplies
C 4ββ˜
(cid:20) (cid:21)
ln(C)
z(t)≤Cδ2,∀t∈ 0, . (23)
4ββ˜
h i
Now,weconsidert∈ 0,ln(C) . Notethat
4ββ˜
∥H(X;w(t))∥ ≤β∥w(t)∥2 ≤βCδ2. (24)
2 2
Defineξ(t):=e(t)+y;then,forsquareloss,
∥ξ(t)∥ =∥H(X;w(t))∥ ≤βCδ2.
2 2
Forlogisticloss
|ξ i(t)|=(cid:12) (cid:12) (cid:12) (cid:12)y i(cid:18) 11 +− ee −− yy ii HH (( xx ii ;; ww (( tt )) ))(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)≤|y i||1−e−yiH(xi;w(t))|≤e|yi|β||y i||y iH(x i;w(t))|≤βe|yi|β|y i|2δ2C,
whereinthefirstinequalityweuse|1+ex|≥1. Thesecondinequalityfollowsfrom|H(x ;w(t))|≤βδ2C ≤βand
i
thenusingthefollowingfirst-orderTaylorapproximation,
|1−e−yiH(xi;w(t))|≤e|yi|β|y H(x ;w(t))|.
i i
Hence,∥ξ(t)∥ ≤βe∥y∥β∥y∥2δ2C forlogisticloss.
2 2
Next,thedynamicsofw(t)canbewrittenas
n n
X X
w˙ ∈− e ∂H(x ;w)= (y −ξ (t))∂H(x ;w). (25)
i i i i i
i=1 i=1
xviDividingeq.(25)byδ,from1-homogeneityof∂H(x;w)(corollaryB.1.1)wehave
n n
w˙ 1X X
∈ (y −ξ (t))∂H(x ;w)= (y −ξ (t))∂H(x ;w/δ). (26)
δ δ i i i i i i
i=1 i=1
Now,considerthedifferentialinclusion
n
dw˜ X
∈ y ∂H(x ;w˜),w˜(0)=w . (27)
dt i i 0
i=1
h i
Using the fact that for all t ∈ 0,ln(C) , for square loss ∥ξ(t)∥ ≤ δ2βC, and for logistic loss ∥ξ(t)∥ ≤
4ββ˜ 2 2
βe∥y∥β∥y∥2δ2C,andusingLemmaC.1,wehavethatthereexistsasmallenoughδsuchthatforallδ ≤δ,
2
(cid:13) (cid:13)
(cid:13) w(t)(cid:13)
(cid:13)w˜(t)− (cid:13) ≤ϵ,
(cid:13) δ (cid:13)
2
wherew˜(t)isasolutionofeq.(27).
C.2 ProofofLemma5.3
Recall that N (u) = z⊤H(X;u). Throughout this section, for the sake of brevity, we will use N(u) instead of
z,H
N (u). Thegradientflowu(t)satisfies,fora.e.t≥0,
z,H
n
du X
∈∂N(u)⊆ z ∂H(x ;u),u(0)=u . (28)
dt i i 0
i=1
Wewillfirstprovesomeauxiliarylemmata.Thefirstfollowssimplyfromtwo-homogeneityofN(u)andLemmaB.1.
LemmaC.2. Foranys∈∂N(u),s⊤u=2N(u). Foranyc≥0,∂N(cu)=c∂N(u)
Thenextlemmastatesthatiftheinitializationisnon-zero,thengradientflowstaysawayfromtheoriginforallfinite
time.
Lemma C.3. Suppose u(t) is a solution of eq. (28), where u is a non-zero vector. Then, for all finite t > 0,
0
∥u(t)∥ >0.
2
Proof. Since∥u(0)∥ >0,fromcontinuityofu(t),thereexistssomeγ >0suchthat∥u(t)∥ >0,forallt∈(0,γ).
2 2
Forthesakeofcontradiction,supposethereexistssomefiniteT > 0suchthat∥u(T)∥ = 0forthefirsttime. Then,
2
forallt∈[0,T),∥u(t)∥ >0. Sincefora.e.t∈[0,T)
2
dlog(∥u∥2) 1 d∥u∥2 2z⊤H(X;u)
2 = 2 = ≥−β∥z∥ ,
dt ∥u∥2 dt ∥u∥2 2
2 2
itfollowsthatforallt∈(0,T),
∥u(t)∥2 ≥∥u ∥2e−tβ∥z∥2.
2 0 2
Takingt→T,wehave∥u(T)∥2
2
≥∥u 0∥2 2e−Tβ∥z∥2 >0whichleadstoacontradiction.
LemmaC.4. IfN(u(t ))≥0,foranyt ≥0,then,N(u(t))≥0and∥u(t)∥ ≥∥u(t )∥ ,forallt≥t .
0 0 2 0 2 0
Proof. Since,usingLemmaA.3,
Z t
N(u(t))−N(u(t ))= ∥u˙(s)∥2ds,
0 2
t0
xviiwehavethatfort ≥ t 0,N(u(t)) ≥ N(u(t 0)) ≥ 0. Thesecondclaimistruesincefora.e.t ≥ 0, d∥ du t∥2 2 = 4N(u)
implies
Z t
∥u(t)∥2−∥u(t )∥2 =4 N(u(s))ds≥0.
2 0 2
t0
Thefollowinglemmastatestheconditionsforfirst-orderKKTpointoftheconstrainedNCF.
LemmaC.5. Ifavectoru∗ ∈Rk×1isafirst-orderKKTpointof
max N(u)=z⊤H(X;u), (29)
∥u∥2=1
2
then
n
X
z ∂H(x ;u∗)=λ∗u∗,∥u∗∥2 =1, (30)
i i 2
i=1
whereλ∗ ∈RistheLagrangemultiplier. Also,2N(u∗)=λ∗andhence,foranon-negativeKKTpointλ∗ ≥0.
Proof. TheLagrangianisequalto
L(u,λ)=N(u)+λ(∥u∥2−1).
2
Ifu∗isafirst-orderKKTpointthenitmustsatisfytheconstraintsetand,forsomeλ,
0∈∂N(u∗)+2λu∗,
implying
n
X
0∈ z ∂H(x ;u∗)+2λu∗.
i i
i=1
Choosingλ∗ =−2λwegeteq.(30). ByLemmaC.2,
λ∗ =λ∗∥u∗∥2 =u∗⊤∂N(u∗)=2N(u∗)
2
InthefollowinglemmawedefineN˜(u),whichiscentraltoourproof,anditsminimumnormClarkesubdifferential.
LemmaC.6. Foranynonzerou∈Rk wedefineN˜(u)=N(u)/∥u∥2,then,
2
∂N˜(u)=(cid:26)
s
−
2N(u)u(cid:12)
(cid:12)
(cid:12)s∈∂N(u)(cid:27) =(cid:26)(cid:18)
I−
uu⊤(cid:19)
s
(cid:12)
(cid:12)
(cid:12)s∈∂N(u)(cid:27)
,
∥u∥2 ∥u∥4 (cid:12) ∥u∥2 ∥u∥2(cid:12)
2 2 2 2
and
(cid:18) uu⊤(cid:19)
∂N(u)
∂N˜(u)= I− .
∥u∥2 ∥u∥2
2 2
Proof. First,notethatN˜(u)isdifferentiableifandonlyifN(u)isdifferentiable. Therefore,foranynon-zerousuch
thatN(u)isdifferentiable,
∇N(u) 2N(u)u
∇N˜(u)= − .
∥u∥2 ∥u∥4
2 2
ThefirstclaimfollowsfromthedefinitionofClarkesubdifferentialandLemmaC.2. Forthesecondclaim,notethat
(cid:13) (cid:13) (cid:13) s − 2N(u)u(cid:13) (cid:13) (cid:13)2 = ∥s∥2 2 − 4N(u)2 + 4N(u)2 ,
(cid:13)∥u∥2 ∥u∥4 (cid:13) ∥u∥4 ∥u∥6 ∥u∥8
2 2 2 2 2 2
whereweuses⊤u = 2N(u), foralls ∈ ∂N(u). Hence, fortheminimumnormsubdifferentialofN˜(u), wemust
choosetheminimumnormsubdifferentialofN(u).
xviiiWenowproceedtoprovingLemma5.3.Webeginbyshowingthateitheru(t)convergesto0orlim u(t)/∥u(t)∥
t→∞ 2
exists. Weconsidertwocases.
Case1: N(u(0))>0.
Inthiscase,weshowthatlim u(t)/∥u(t)∥ existsusingasimilartechniqueasinJi&Telgarsky(2020). Specifi-
t→∞ 2
cally,weshowthatthelengthofthecurvesweptbyu(t)/∥u(t)∥ ,whichisdefinedas
2
Z ∞(cid:13) (cid:13)d (cid:18) u (cid:19)(cid:13) (cid:13)
(cid:13) (cid:13) dt,
(cid:13)dt ∥u∥ (cid:13)
0 2 2
hasfinitelength,andthuslim u(t)/∥u(t)∥ exists.
t→∞ 2
WeassumeN(u(0))=γ >0,andthus∥u(0)∥ >0. FromLemmaC.4,forallt≥0,
2
N(u(t))≥N(u(0))=γ,
implying
1d∥u∥2
2 =u⊤u˙ =2N(u(t))≥2γ, fora.e.t≥0,
2 dt
whichinturnimplies
∥u(t)∥2 ≥∥u(0)∥2+4γt, forallt≥0. (31)
2 2
RecallthatN˜(u)=N(u)/∥u∥2. Since∥u(t)∥ >0,N˜(u(t))isdefinedforallt≥0. Also,byLemmaA.3,fora.e.
2 2
t≥0,
dN(u)
=∥u˙∥2, andu˙ =∂N(u).
dt 2
Therefore,usingthechainrulefromLemmaA.3,fora.e.t≥0wehave
dN˜(u) (cid:18) uuT (cid:19) ∂N(u) ∂N(u)⊤ (cid:18) uuT (cid:19)
=u˙⊤ I− = I− ∂N(u)≥0, (32)
dt ∥u∥2 ∥u∥2 ∥u∥2 ∥u∥2
2 2 2 2
whereinthesecondequalityweusedthatu˙ =∂N(u),fora.e.t≥0. Hence,forallt ≥t ≥0,
2 1
Z t2 ∂N(u)⊤ (cid:18) uuT (cid:19)⊤
N˜(u(t ))−N˜(u(t ))= I− ∂N(u)dt≥0.
2 1 ∥u∥2 ∥u∥2
t1 2 2
Therefore,N˜(u(t)))isanincreasingfunction,andhence,foranyt≥0,thatN˜(u(t))≥N˜(u(0))implies
N(u(t))≥N˜(u(0))∥u(t)∥2.
2
Fromtheaboveinequalityandeq.(31),wehavelim N(u(t))=∞.
t→∞
Now,sinceN(u) ≤ ∥z∥ ∥H(X;u)∥ ≤ β∥z∥ ∥u∥2,wehavethatN˜(u)isbounded. Hence,bymonotoneconver-
2 2 2 2
gencetheorem,lim N˜(u(t))exists;here,wesupposeitisequaltof.
t→∞
Notethat,bythechainrule,fora.e.t≥0,
d (cid:18) u (cid:19) (cid:18) uuT (cid:19) u˙ (cid:18) uuT (cid:19) ∂N(u)
= I− = I−
dt ∥u∥ ∥u∥2 ∥u∥ ∥u∥2 ∥u∥
2 2 2 2 2
implies
(cid:13) (cid:13)d (cid:18) u (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)(cid:18) uuT (cid:19) (cid:13) (cid:13) 1
(cid:13) (cid:13) =(cid:13) I− ∂N(u)(cid:13) . (33)
(cid:13)dt ∥u∥ (cid:13) (cid:13) ∥u∥2 (cid:13) ∥u∥
2 2 2 2 2
xixSupposeN˜(u(t))convergestof infinitetime,i.e.,N˜(u(T))=f forsomefiniteT. Then,fora.e.t≥T, dN˜(u) =0
dt
implies
(cid:13) (cid:13)(cid:18) u(t)u(t)T(cid:19) (cid:13)
(cid:13)
(cid:13) I− ∂N(u(t))(cid:13) =0.
(cid:13) ∥u(t)∥2 (cid:13)
2 2
Therefore,fromeq.(33),wehave
(cid:18) (cid:19)
d u
=0, fora.e.t≥T,
dt ∥u∥
2
andhence,lim u(t) existsandisequalto u(T) .
t→∞ ∥u(t)∥2 ∥u(T)∥2
Thus,wemayassumef −N˜(u(t))>0,forallfinitet. Defineg(u)=f −N˜(u). Then,since
∥∂ N˜(u)∥ =0,
r 2
wehavethat
∥∂ g(u)∥ ≥∥u∥ ∥∂ g(u)∥ =0.
⊥ 2 2 r 2
Hence, from Theorem A.4, there exists a ν > 0 and a desingularizing function Ψ(·) defined on [0,ν) such that if
∥u∥ >1andg(u)<ν,then
2
1≥Ψ′(g(u))∥u∥ ∥∂g(u)∥ =Ψ′(f −N˜(u))∥u∥ ∥∂N˜(u)∥ . (34)
2 2 2 2
Sincelim N˜(u(t))=f,andeq.(31)holds,wemaychooseT largeenoughsuchthat∥u(t)∥ >1,andg(u(t))<
t→∞ 2
ν,forallt≥T. Hence,fora.e.t≥T,
dN˜(u) ∂N(u)⊤ (cid:18) uuT (cid:19)⊤ (cid:13) (cid:13)(cid:18) uuT (cid:19) ∂N(u)(cid:13) (cid:13)2
= I− ∂N(u)=(cid:13) I− (cid:13)
dt ∥u∥2 ∥u∥2 (cid:13) ∥u∥2 ∥u∥ (cid:13)
2 2 2 2 2
(cid:13) (cid:13)(cid:18) uuT (cid:19) ∂N(u)(cid:13) (cid:13) (cid:13) (cid:13)d (cid:18) u (cid:19)(cid:13) (cid:13)
=(cid:13) I− (cid:13) (cid:13) (cid:13)
(cid:13) ∥u∥2 ∥u∥ (cid:13) (cid:13)dt ∥u∥ (cid:13)
2 2 2 2 2
(cid:13) (cid:18) (cid:19)(cid:13)
=∥u∥
2(cid:13) (cid:13)∂N˜(u)(cid:13)
(cid:13)
2(cid:13)
(cid:13)
(cid:13)dd
t
∥uu
∥
(cid:13)
(cid:13)
(cid:13)
2 2
(cid:13) (cid:18) (cid:19)(cid:13)
1 (cid:13)d u (cid:13)
≥ (cid:13) (cid:13)
Ψ′(f −N˜(u))(cid:13)dt ∥u∥
2
(cid:13)
2
implying
(cid:13) (cid:13)d (cid:18) u (cid:19)(cid:13) (cid:13) dΨ(f −N˜(u))
(cid:13) (cid:13) ≤− .
(cid:13)dt ∥u∥ (cid:13) dt
2 2
Inthechainofequalitiesandinequalitiesabove,weusedLemmaC.6forthefourthequality,andforthefirstinequality
weusedeq.(34). Now,integratingbothsidesoftheabovefromT toanyt ≥T,wehave
1
Z t1(cid:13) (cid:13) (cid:13) (cid:13)dd t(cid:18) ∥uu
∥
(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13)
dt≤Ψ(f −N˜(u(T))−Ψ(f −N˜(u(t 1))≤Ψ(f −N˜(u(T)))<∞,
T 2 2
whichimplies
Z ∞(cid:13) (cid:13)d (cid:18) u (cid:19)(cid:13) (cid:13) Z T (cid:13) (cid:13)d (cid:18) u (cid:19)(cid:13) (cid:13) Z ∞(cid:13) (cid:13)d (cid:18) u (cid:19)(cid:13) (cid:13)
(cid:13) (cid:13) dt= (cid:13) (cid:13) dt+ (cid:13) (cid:13) dt
(cid:13)dt ∥u∥ (cid:13) (cid:13)dt ∥u∥ (cid:13) (cid:13)dt ∥u∥ (cid:13)
0 2 2 0 2 2 T 2 2
≤Z T (cid:13) (cid:13) (cid:13)d (cid:18) u (cid:19)(cid:13) (cid:13) (cid:13) dt+Ψ(f −N˜(u(T)))<∞,
(cid:13)dt ∥u∥ (cid:13)
0 2 2
completingtheproof.
Case2: N(u(0))≤0.
Inthiscase,wemayfurtherassumethatN(u(t)) ≤ 0,forallt ≥ 0,sinceifforsomefinitet,N(u(t)) > 0,wecan
xxusetheproofforCase1bychoosingtasthestartingtimetoprovetheclaim. Thus,weassumeN(u(t)) ≤ 0,forall
t≥0. Now,since
1d∥u∥2
2 =u⊤u˙ =2N(u(t))≤0, fora.e.t≥0,
2 dt
it follows that ∥u(t)∥ decreases with time. Hence, lim ∥u(t)∥ exists. If lim ∥u(t)∥ = 0, then
2 t→∞ 2 t→∞ 2
lim u(t)=0andlim N(u(t))=0andwearedone.
t→∞ t→∞
Otherwise,assumethatlim ∥u(t)∥ =η >0. Inthiscase,since∥u(t)∥ isadecreasingfunction,∥u(t)∥ ≥η,
t→∞ 2 2 2
forallt≥0. SincebyLemmaA.3,fora.e.t≥0,
dN(u)
=∥u˙∥2,
dt 2
wehavethatN(u(t))increaseswithtime. But,wealsoassumeN(u(t)) ≤ 0,andsobythemonotoneconvergence
theorem, N(u(t)) converges. We further claim that lim N(u(t)) = 0. Suppose for the sake of contradiction
t→∞
lim N(u(t))=−γ <0. SinceN(u(t))increaseswithtime,wehaveN(u(t))≤−γ,forallt≥0. Hence,
t→∞
1d∥u∥2
2 =u⊤u˙ =2N(u(t))≤−2γ, fora.e.t≥0.
2 dt
Theaboveequationimpliesthat∥u(t)∥ willbecomelessthanη withinafinitetime,whichleadstoacontradiction,
2
andtherefore,lim N(u(t))=0.
t→∞
We next show that lim u(t)/∥u(t)∥ exists. We can do this in same way in the proof of Case 1. Define
t→∞ 2
uˆ(t) = 2u(t)/η, and note that if uˆ(t) converges in direction, then u(t) also converges in direction. We make this
transformationbecausetouseLemmaA.4werequire∥u(t)∥ > 1aftersometimeT. Whileu(t)mayneverexceed
2
1,wedohave∥uˆ(t)∥ ≥2>1,forallt≥0.
2
Next,N˜(u(t))isdefinedforallt≥0,since∥u(t)∥ >0forallt≥0. Also,sinceN(u(t))convergesto0,N˜(u(t))
2
alsoconvergesto0. Also,N˜(u(t))=N˜(uˆ(t)),thusN˜(uˆ(t))convergesto0aswell. Fromheretoprovedirectional
convergenceofuˆ(t)wecanusethethesameapproachasinCase1,specificallyfromeq.(33)onward.
Wenextturntowardsshowingthatiflim u(t) exists,thenthelimitmustbeanon-negativeKKTpointofthe
t→∞ ∥u(t)∥2
constrainedNCF.Supposeu∗ isthelimit. WehavealreadyshownthatN(u∗)=N˜(u∗)≥0. Thus,weonlyneedto
provethatu∗isaKKTpoint,i.e.,fromeq.(30),itmustsatisfy
2N(u∗)u∗ ∈∂N(u∗), (35)
Assumeforthesakeofcontradictionthatthereexistssomeγ >0suchthatforalls∈∂N(u∗),wehave
∥s−2N(u∗)u∗∥ ≥γ. (36)
2
Defineu = {u : ∥u−u∗∥ ≤ ϵ}. Givenγ,byuppersemi-continuityoftheClarkesubdifferential,wemaychoose
ϵ
ϵ∈(0,1)sufficientlysmallsuchthatforallu∈u ,wehave
ϵ
∂N(u)⊆{p:p=q+r,q∈∂N(u∗),∥r∥ ≤γ/4}. (37)
2
Since u(t) convergestou∗,andN(u)iscontinuous,wecanchooseT largeenoughsuchthatforallt≥T
∥u(t)∥2
(cid:13) (cid:13) (cid:13) (cid:18) (cid:19) (cid:13)
(cid:13)
(cid:13)
u(t) −u∗(cid:13)
(cid:13) ≤ϵ, and
(cid:13)
(cid:13)
2u(t)
N
u(t) −2u∗N(u∗)(cid:13)
(cid:13) ≤γ/4. (38)
(cid:13)∥u(t)∥ (cid:13) (cid:13)∥u(t)∥ ∥u(t)∥ (cid:13)
2 2 2 2 2
xxiSupposes∈∂N(u∗),then,forallu∈Rk\{0}wehave
(cid:13) (cid:13)(cid:18) uu⊤(cid:19) ∂N(u)(cid:13)
(cid:13)
(cid:13) (cid:13)(cid:18)
∂N(u)
2uN(u)(cid:19)(cid:13)
(cid:13)
(cid:13) I− (cid:13) =(cid:13) − (cid:13)
(cid:13) ∥u∥2 ∥u∥ (cid:13) (cid:13) ∥u∥ ∥u∥3 (cid:13)
2 2 2 2 2 2
(cid:13) (cid:18) (cid:19) (cid:18) (cid:19)(cid:13)
(cid:13) u 2u u (cid:13)
=(cid:13)∂N − N (cid:13)
(cid:13) ∥u∥ ∥u∥ ∥u∥ (cid:13)
2 2 2 2
(cid:13) (cid:18) (cid:19)(cid:13) (cid:13) (cid:18) (cid:19) (cid:13)
(cid:13) 2u u (cid:13) (cid:13) u (cid:13)
≥(cid:13)s− N (cid:13) −(cid:13)∂N −s(cid:13)
(cid:13) ∥u∥ ∥u∥ (cid:13) (cid:13) ∥u∥ (cid:13)
2 2 2 2 2
(cid:13) (cid:18) (cid:19) (cid:13) (cid:13) (cid:18) (cid:19)(cid:13)
≥∥s−2u∗N (u∗)∥ −(cid:13) (cid:13)∂N u −s(cid:13) (cid:13) −(cid:13) (cid:13)2u∗N (u∗)− 2u N u (cid:13) (cid:13) ,
2 (cid:13) ∥u∥ (cid:13) (cid:13) ∥u∥ ∥u∥ (cid:13)
2 2 2 2 2
whereinthesecondequalityweused1−homogeneityof∂N(u)and2−homogeneityofN(u),andtheinequalities
followfromtriangleinequalityofnorms. Hence,fora.e.t≥T,usingeq.(36),eq.(37)andeq.(38)wehave
(cid:13) (cid:13)(cid:18) u(t)u(t)⊤(cid:19) ∂N(u(t))(cid:13)
(cid:13)
(cid:13) I− (cid:13) ≥γ−γ/4−γ/4=γ/2,
(cid:13) ∥u(t)∥2 ∥u(t)∥ (cid:13)
2 2 2
implying
d
(cid:18)
N
(cid:18)
u(t)
(cid:19)(cid:19) =(cid:13)
(cid:13)
(cid:13)(cid:18)
I−
u(t)u(t)⊤(cid:19) ∂N(u(t))(cid:13)
(cid:13)
(cid:13)2
≥γ2/4,
dt ∥u(t)∥ (cid:13) ∥u(t)∥2 ∥u(t)∥ (cid:13)
2 2 2 2
(cid:16) (cid:17)
whichcontradictsthefactthatlim N u(t) converges,thusprovingourclaim.
t→∞ ∥u(t)∥2
Now,beforeweturntoproveTheorem5.1,westateanotherusefullemma.
LemmaC.7. Iflim u(t)̸=0,thenthereexistsη >0andT ≥0suchthat∥u(t)∥ ≥η,forallt≥T.
t→∞ 2
Proof. TheproofisbuiltusingtheargumentalreadypresentedintheproofofLemma5.3,andweconsidertwocases.
Case1: N(u(0))>0.
WeassumeN(u(0)) = γ > 0,andtherefore, ∥u(0)∥ > 0. FromLemmaC.4, forallt ≥ 0, wehaveN(u(t)) ≥
2
N(u(0))=γ,whichimplies
1d∥u∥2
2 =u⊤u˙ =2N(u(t))≥2γ, fora.e.t≥0, (39)
2 dt
whichinturnimplies
∥u(t)∥ ≥∥u(0)∥ forallt≥0. (40)
2 2
Thus,wecanchooseη =∥u(0)∥ ,andT =0.
2
Case2: N(u(0))≤0.
ForthiscasewemayfurtherassumethatN(u(t)) ≤ 0,forallt ≥ 0, sinceifforsomet, N(u(t)) > 0,thenusing
Lemma C.4, we have ∥u(t)∥ ≥ ∥u(t)∥ , for all t ≥ t. Then, since N(u(t)) > 0 implies ∥u(t)∥ > 0, we may
2 2 2
chooseη =∥u(t)∥ andT =t.
2
So,letusassumethatN(u(t))≤0,forallt≥0. Then,
1d∥u∥2
2 =u⊤u˙ =2N(u(t))≤0, fora.e.t≥0.
2 dt
Therefore, ∥u(t)∥ decreases with time, and hence, lim ∥u(t)∥ exists and ∥u(t)∥ ≥
2 t→∞ 2 2
lim ∥u(t)∥ , forallt ≥ 0. Since we have assumed lim u(t) ̸= 0, we have lim ∥u(t)∥ > 0.
t→∞ 2 t→∞ t→∞ 2
Thus,wemaychooseη =lim ∥u(t)∥ ,andT =0.
t→∞ 2
xxiiC.3 ProofofTheorem5.1
Considerthedifferentialinclusion
n
X
u˙ ∈∂N (u)⊆ y ∂H(x ;u),u(0)=w , (41)
y,H i i 0
i=1
andletu(t)beitsuniquesolution. ByLemma5.3,eitherlim u(t)=0orlim u(t) exists.
t→∞ t→∞ ∥u(t)∥2
Wefirstconsiderthecasewhenlim u(t) = 0. Here,wedefineη = 1andfixanϵ ∈ (0,η). Then,wechooseT
t→∞
largeenoughsuchthat
∥u(t)∥ ≤ϵ, forallt≥T. (42)
2
Next,iflim u(t)̸=0,thenfromLemmaC.7,thereexistsη >0andT ≥0suchthat∥u(t)∥ ≥2η,forallt≥T.
t→∞ 2
Also,fromLemma5.3,
lim u(t)/∥u(t)∥ =uˆ,
2
t→∞
whereuˆ isanon-negativeKKTpointof
max N (u)=H(X;u)⊤y. (43)
y,H
∥u∥2=1
2
Forafixedϵ∈(0,η),wechooseT >T suchthat
u(t)⊤uˆ
≥1−ϵ, forallt≥T. (44)
∥u(t)∥
2
HavingchosenT forafixedϵ∈(0,η)inbothcases,wenextchooseC suchthat ln(C) =T.FromLemma5.2,there
4ββ˜
existsδsuchthatforanyδ ≤δ
√ (cid:20) ln(C)(cid:21)
∥w(t)∥ ≤ Cδ, forallt∈ 0, ,
2 4ββ˜
and
(cid:13) (cid:13)
(cid:13)w(T) (cid:13)
(cid:13) −u(T)(cid:13) ≤ϵ. (45)
(cid:13) δ (cid:13)
2
Thus,wemaywrite w(T) =u(T)+ζ,where∥ζ∥ ≤ϵ.Iflim u(t)=0,then,usingeq.(42),
δ 2 t→∞
∥w(T)∥ ≤2δϵ.
2
Else,sinceϵ∈(0,η),and∥u(T)∥ ≥2η,wehave∥u(T)+ζ∥ ≥η. Hence,
2 2
w(T) u(T)+ζ
= ,
∥w(T)∥ ∥u(T)+ζ∥
2 2
whichimplies
w(T)⊤uˆ u(T)⊤uˆ+ζ⊤uˆ (cid:18) u(T)⊤uˆ(cid:19) ∥u(T)∥ ζ⊤uˆ
= = 2 + .
∥w(T)∥ ∥u(T)+ζ∥ ∥u(T)∥ ∥u(T)+ζ∥ ∥u(T)+ζ∥
2 2 2 2 2
Now,since
∥u(T)∥ ∥u(T)∥ 1 1 ϵ
2 ≥ 2 = ≥ ≥1− ,
∥u(T)+ζ∥ 2 ∥u(T)∥ 2+∥ζ∥ 2 1+ ∥ζ∥2 1+ 2ϵ η 2η
∥u(T)∥2
and
ζ⊤uˆ −ϵ
≥ ,
∥u(T)+ζ∥ η
2
wehave
w(T)⊤uˆ (cid:18) ϵ (cid:19) ϵ (cid:18) 3 (cid:19)
≥(1−ϵ) 1− − ≥1− 1+ ϵ.
∥w(T)∥ 2η η 2η
2
xxiiiC.4 ProofofCorollary5.3.1
Considerthedifferentialinclusion
n
X
u˙ ∈∂N (u)⊆ y ∂H(x ;u),u(0)=w , (46)
y,H i i 0
i=1
andletu(t)beitsuniquesolution.Fromseparability,wecanwriteu(t)=[u (t),...,u (t)]suchthatforallj ∈[H]
1 H
wehave
n
X
u˙ ∈∂N (u )⊆ y ∂H (x ;u ),u (0)=w , (47)
j y,Hj j i j i j j 0j
i=1
wherew =[w ,...,w ]⊤. ByLemma5.3,forallj ∈[H],eitherlim u (t)=0orlim uj(t) exists.
0 01 0H t→∞ j t→∞ ∥uj(t)∥2
Let Z be the collection of all indices such that lim u (t) = 0, for all j ∈ Z, and Zc be the complement of Z
t→∞ j
in[H]. Forallj ∈ Zc,fromLemmaC.7,thereexistsη > 0andT ≥ 0suchthat∥u (t)∥ ≥ 2η ,forallt ≥ T .
j j j 2 j j
Defineη =min(1,min η )andT =max T ,andfixanϵ∈(0,η).
j∈Zc j j∈Zc j
FromLemma5.3,forallj ∈Zcwehave
lim u (t)/∥u (t)∥ =uˆ ,
j j 2 j
t→∞
whereuˆ isanon-negativeKKTpointof
j
max N (u )=H (X;u )⊤y. (48)
∥uj∥2 2=1
y,Hj j j j
Then,foragivenϵ,wechooseT >T suchthat
1
u (t)⊤uˆ
j j ≥1−ϵ, forallt≥T , andallj ∈Zc. (49)
∥u (t)∥ 1
j 2
Forallj ∈Z,wechooseT largeenoughsuchthat
2
∥u (t)∥ ≤ϵ, forallt≥T andallj ∈Z. (50)
j 2 2
DefineT =max(T ,T )andchooseC suchthat ln(C) =T.FromLemma5.2,thereexistsδsuchthatforanyδ ≤δ
1 2 4ββ˜
√ (cid:20) ln(C)(cid:21)
∥w(t)∥ ≤ Cδ, forallt∈ 0, ,
2 4ββ˜
and
(cid:13) (cid:13)
(cid:13)w(T) (cid:13)
(cid:13) −u(T)(cid:13) ≤ϵ. (51)
(cid:13) δ (cid:13)
2
Byseparability,wemaywrite wj(T) =u (T)+ζ ,where∥ζ ∥ ≤ϵ.Ifj ∈Zc,then,usingeq.(50)wehave
δ j j j 2
∥w (T)∥ ≤2δϵ.
j 2
Else, sinceϵ ∈ (0,η), and∥u (T)∥ ≥ 2η, wehave∥u (T)+ζ ∥ ≥ η. Hence, usingsimilarreasoningasinthe
j 2 j j 2
laterpartoftheproofofTheorem5.1wegetforallj ∈Zc,
w (T)⊤uˆ (cid:18) 3 (cid:19)
j j ≥1− 1+ ϵ.
∥w (T)∥ 2η
j 2
xxivD Proofs Omitted from Section 5.2
WefirstproveLemma5.5andLemma5.6.
D.1 ProofofLemma5.5andLemma5.6
Wenotethatsince{w ,w }isasaddlepointof
n z
1
L(w ,w )= ∥H (X;w )+H (X;w )−y∥2, (52)
n z 2 n n z z
andw =0,wehave
z
(cid:20) (cid:21) (cid:20) (cid:21)
0 ∂ L(w ,0)
∈ wn n , (53)
0 ∂ L(w ,0)
wz n
whichestablishes0∈∂ L(w ,0).
wn n
Usingthemeanvaluetheorem(Clarkeetal.,1998,Theorem2.4),wenextshowthatifthereexistsγ >0suchthatfor
allw satisfying∥w −w ∥ ≤γ itholdsthat
n n n 2
⟨w −w ,s⟩≥0,wheres∈−∂ L(w ,0), (54)
n n wn n
then,w isalocalminimaofL(w ,0).
n n
Letw satisfy∥w −w ∥ ≤γ. Then,usingthemeanvaluetheorem,
k k n 2
L(w ,0)−L(w ,0)∈∂ L(u,0)⊤(w −w ),
k n wn k n
whereu=tw +(1−t)w forsomet∈(0,1). Notethatw −w =(u−w )/t.Thus,
k n k n n
L(w ,0)−L(w ,0)∈∂ L(u,0)⊤(u−w )/t.
k n wn n
Since∥u−w ∥ =(1−t)∥w −w ∥ ≤γ,usingeq.(54)weget
n 2 k n 2
L(w ,0)−L(w ,0)≥0,
k n
provingw isalocalminimaofL(w ,0).
n n
To prove theorem 5.4, we first describe the approximate dynamics of {w (t),w (t)} near the saddle point in the
n z
followinglemma.
LemmaD.1. Let{w ,w }satisfyAssumption2,anddefiney=y−H (X;w ). LetC >1beanarbitrarilylarge
n z n n
constantand{w (t),w (t)}satisfyfora.e.t≥0
n z
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
w˙ ∂ L(w ,w ) w (0) w +δζ
n ∈− wn n z , n = n n , (55)
w˙ ∂ L(w ,w ) w (0) w +δζ
z wz n z z z z
whereδ2 ≤min( 1 ,γ2 )and∥ζ ∥ =∥ζ ∥ =1. Then
2C 4 n 2 z 2
(cid:20) (cid:21)
1
∥w (t)−w ∥2+∥w (t)−w ∥2 ≤2Cδ2, forallt∈ 0, ln(C) , (56)
n n 2 z z 2 M
2
whereM isapositiveconstant4. Further,forthedifferentialinclusion
2
u˙ ∈∂N (u),u(0)=ζ , (57)
y,Hz z
andforanyϵ>0thereexistsasmallenoughδ >0suchthatforanyδ ≤δ,
(cid:13) (cid:13) (cid:20) (cid:21)
(cid:13) (cid:13)w z(t) −u(t)(cid:13)
(cid:13) ≤ϵ, forallt∈
0,ln(C)
, (58)
(cid:13) δ (cid:13) M
2 2
whereu(t)isacertainsolutionofeq.(57).
4M2hereissameasinTheorem5.4.SeethestatementofTheorem5.4andthetheproofofLemmaD.1formoredetails.
xxvProof. Wenotethatsince{w ,w }isasaddlepointof
n z
1
L(w ,w )= ∥H (X;w )+H (X;w )−y∥2, (59)
n z 2 n n z z
wehave
(cid:20) 0(cid:21) (cid:20)Pn
y ∂H (x ;w
)(cid:21)
0
∈ Pi n=1 yi ∂Hn (xi ;wn
)
. (60)
i=1 i z i z
Wenowdefine
∆ (t)=H (X;w (t))−H (X;w ),∆ (t)=H (X;w (t)), andZ(t)=∥w (t)−w ∥2+∥w (t)∥2.
n n n n n z z z n n 2 z 2
Since (w (t),w (t)) is a continuous curve, Z(t) is also a continuous curve. Note that Z(0) = 2δ2 ≤
n z
min(1/C,γ2/2) < min(1,γ2). Therefore, there exists some t > 0, such that Z(t) ≤ min(1,γ2), for all
t∈[0,t]. LetT∗ bethesmallestt>0suchthatZ(T∗)=min(1,γ2). Hence,forallt∈[0,T∗],Z(t)≤min(1,γ2).
OurnextgoalistofindalowerboundforT∗. Weoperatein[0,T∗].
Recallthat
∥H (X;w )∥ ≤β∥w ∥2. (61)
z z 2 z 2
Moreover,bythelocallyLipschitzpropertyofH (X;w ),thereexistsµ >0suchthat
n n 1
∥∆ (t)∥ ≤µ ∥w (t)−w ∥ , forallt∈[0,T∗]. (62)
N 2 1 n n 2
UsingZ(t)≤1,wehave
∥e(t)∥ =∥H (X;w (t))+H (X;w (t))−y∥
2 n n z z 2
≤∥y∥ +∥∆ (t)∥ +∥H (X;w (t))∥ ≤∥y∥ +µ +β :=M ,
2 N 2 z z 2 2 1 1
whereinthelastinequalityweusedeq.(62)andeq.(61). UsingCorollaryB.1.1,wehave
1d∥w (t)∥2
z 2 =−2H (X;w )⊤e≤2βM ∥w (t)∥2. (63)
2 dt z z 1 z 2
WefirstconsiderthecasewhenH (x;w )hasalocallyLipschitzgradient. Let
n n
J(w )=:[∇H (x ;w ),...,∇H (x ;w )]∈Rd×n.
n n 1 n n n n
Fromeq.(60),wehave
n
X
0= y ∇H (x ;w )=J(w )y. (64)
i n i n n
i=1
BythelocallyLipschitzpropertyof∇H (x;w ),wemayassumethatthereexistsµ >0suchthat
n n 2
∥J(w (t))y−J(w )y∥ ≤µ ∥w (t)−w ∥ . (65)
n n 2 2 n n 2
Further,sincew (t)isboundedforallt∈[0,T∗],wemayassumethereexistsµ >0suchthat
n 3
∥J(w (t))∥ ≤µ . (66)
n 2 3
xxviThus,
1d∥w −w ∥2
n n 2 =−⟨w −w ,J(w )e⟩
2 dt n n n
=−⟨w −w ,J(w )(H (X;w )+H (X;w )−y)⟩
n n n n n z z
=−⟨w −w ,J(w )(∆ (t)+∆ (t)−y)⟩
n n n n z
=⟨w −w ,J(w )y⟩−⟨w −w ,J(w )(∆ (t)+∆ (t))⟩
n n n n n n n z
=⟨w −w ,J(w )y−J(w )y⟩−⟨w −w ,J(w )(∆ (t)+∆ (t))⟩
n n n n n n n n z
≤µ ∥w −w ∥2+∥w −w ∥ ∥J(w )∥ (∥∆ (t)∥ +∥∆ (t)∥ )
2 n n 2 n n 2 n 2 n 2 z 2
≤µ ∥w −w ∥2+µ µ ∥w −w ∥2+βµ ∥w −w ∥ ∥w ∥2
2 n n 2 1 3 n n 2 3 n n 2 z 2
≤(µ +µ µ )∥w −w ∥2+βµ ∥w ∥2.
2 1 3 n n 2 3 z 2
Thethirdequalityfollowsfromdefinitionof∆ (t)and∆ (t). Inlastequality,weuseeq.(64). Thefirstinequality
n z
followsfromCauchy-Schwartzandeq.(65). Wegetsecondinequalityfromeq.(66),eq.(62)andeq.(61). Inthefinal
inequality,weuse∥w (t)−w ∥ ≤1,forallt∈[0,T∗].
n n 2
Combiningtheaboveinequalitywitheq.(63),weobtain
1dZ(t)
≤(µ +µ µ )∥w (t)−w ∥2+β(2M +µ )∥w (t)∥2 ≤M Z(t),
2 dt 2 1 3 n n 2 1 3 z 2 2
whereM
2
:=max(µ 2+µ 1µ 3,β(2M 1+µ 3)). Therefore,forallt∈[0,T∗],Z(t)≤Z(0)etM2 implies
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1
T∗ ≥ ln = ln .
M Z(0) M 2δ2
2 2
h i
Sinceweassume2δ2 ≤ 1,wehavethatT∗ ≥ 1 ln(C). Thus,forallt∈ 0, 1 ln(C) ,
C M2 M2
Z(t)≤CZ(0)≤2Cδ2,
provingeq.(56).
WenextconsiderthecasewhenH (X;w )doesnothavealocallyLipschitzgradient.Recallthatif∥w −w ∥ ≤γ,
n n n n 2
then
⟨w −w ,s⟩≥0,wheres∈−∂ L(w ,0). (67)
n n wn n
Further,wemayassumethatthereexistsaconstantµ >0suchthatif∥w −w ∥ ≤γ,then
3 n n 2
max∥p ∥ ≤µ , wherep ∈∂H(x ;w ). (68)
i 2 3 i i n
i∈[n]
Usingthechainrule,wealsohave
∂ L(w ,w )⊆∂ (L(w ,w )−L(w ,0))+∂ L(w ,0). (69)
wn n z wn n z n wn n
Notethat∂ (L(w ,w )−L(w ,0)) ⊆
Pn
H (x ;w )∂H (x ;w ). Therefore, if∥w −w ∥ ≤ γ, then,
wn n z n i=1 z i z n i n n n 2
usingeq.(68),foranyw ,andp∈∂ (L(w ,w )−L(w ,0)),wehave
z wn n z n
√ √
∥p∥ ≤µ ∥H (X;w )∥ ≤µ n∥H (X;w )∥ ≤µ nβ∥w ∥2. (70)
2 3 n z 1 3 n z 2 3 z 2
Next,usingeq.(69)wehave
1d∥w −w ∥2
n n 2 =⟨w −w ,w˙ ⟩∈−⟨w −w ,∂ L(w ,w )⟩
2 dt n n n n n wn n z
∈−⟨w −w ,∂ (L(w ,w )−L(w ,0))+∂ L(w ,0)⟩.
n n wn n z n wn n
xxviiSince∥w (t)−w ∥ ≤γ forallt∈[0,T∗],usingeq.(67)andeq.(70),wehave
n n 2
1d∥w −w ∥2 √ √
n n 2 ≤µ nβ∥w −w ∥ ∥w ∥2 ≤µ nβ∥w ∥2,
2 dt 3 n n 2 z 2 3 z 2
whereinthelastinequalityweuse∥w (t)−w ∥ ≤1,forallt∈[0,T∗]. Combiningaboveinequalitywitheq.(63),
n n 2
wehave
1dZ(t) √
≤β(2M +µ n)∥w (t)∥2 ≤M Z(t),
2 dt 1 3 z 2 2
√
whereM
2
:=β(2M 1+µ
3
n). Therefore,forallt∈[0,T∗],wehavethatZ(t)≤Z(0)etM2 implies
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1
T∗ ≥ ln = ln .
M Z(0) M 2δ2
2 2
h i
Sinceweassume2δ2 ≤ 1,wehavethatT∗ ≥ 1 ln(C). Thus,forallt∈ 0, 1 ln(C) ,
C M2 M2
Z(t)≤CZ(0)≤2Cδ2,
provingeq.(56).
Wenowmovetowardsprovingthesecondpart. WeuseasimilartechniqueasintheproofofLemma5.2. Wedefine
ξ(t)=e(t)+y. Then,
∥ξ(t)∥ =∥y+H (X;w (t))+H (X;w (t))−y∥
2 n n z z 2
=∥H (X;w (t))−H (X;w )+H (X;w (t))∥
n n n n z z 2
≤µ ∥w (t)−w ∥ +β∥w (t)∥2
1 n n 2 z 2
√
≤µ Cδ+βCδ2.
1
Thus,thedynamicsofw (t)canbewrittenas
z
n n
X X
w˙ ∈− e ∂H (x ;w )= (y −ξ(t))∂H (x ;w ). (71)
z i z i z i z i z
i=1 i=1
Dividingeq.(71)byδ,andusing1-homogeneityof∂H(x;w)(CorollaryB.1.1),wehave
n n
w˙ z ∈ 1X (y −ξ(t))∂H (x ;w )=X (y −ξ(t))∂H (x ;w /δ). (72)
δ δ i z i z i z i z
i=1 i=1
Now,considerthedifferentialinclusion
n
dw˜ z ∈X y ∂H (x ;w˜ ),w˜ (0)=ζ . (73)
dt i z i z z z
i=1
h i √
Sinceforallt ∈ 0, 1 ln(C) ,∥ξ(t)∥ ≤ µ Cδ+βCδ2,usingLemmaC.1,thereexistsasmallenoughδ such
M2 2 1
thatforallδ ≤δ,
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)w˜ z(t)− w z δ(t)(cid:13) (cid:13)
(cid:13)
≤ϵ,
2
wherew˜(t)isasolutionofeq.(73).
xxviiiD.2 ProofofTheorem5.4
Considerthedifferentialinclusion
n
X
u˙ ∈ y ∂H (x ;u),u(0)=ζ , (74)
i z i z
i=1
andletu(t)beitsuniquesolution. ByLemma5.3,eitherlim u(t)=0orlim u(t) exists.
t→∞ t→∞ ∥u(t)∥2
Wefirstconsiderthecasewhenlim u(t) = 0. Here,wedefineη = 1andfixanϵ ∈ (0,η). Then,wechooseT
t→∞
largeenoughsuchthat
∥u(t)∥ ≤ϵ,∀t≥T. (75)
2
Wenextconsiderthecasewhenlim u(t) ̸= 0. Then,fromLemmaC.7,thereexistsη > 0andT ≥ 0suchthat
t→∞
∥u(t)∥ ≥2η,forallt≥T. Also,fromLemma5.3,
2
lim u(t)/∥u(t)∥ =uˆ,
2
t→∞
whereuˆ isanon-negativeKKTpointof
maxH (X;u)⊤y, suchthat∥u∥2 =1. (76)
z 2
Forafixedϵ∈(0,η),wechooseT >T suchthat
u(t)⊤uˆ
≥1−ϵ, forallt≥T. (77)
∥u(t)∥
2
HavingchosenT forafixedϵ∈(0,η)inbothcases,wenextchooseC suchthat ln(C) =T.FromLemmaD.1,there
M2
existsδsuchthatforanyδ ≤δ
(cid:20) (cid:21)
ln(C)
∥w (t)−w ∥2+∥w (t)−w ∥2 ≤2Cδ2, forallt∈ 0, ,
n n 2 z z 2 M
2
and
(cid:13) (cid:13)
(cid:13) (cid:13)w z(T) −u(T)(cid:13)
(cid:13) ≤ϵ. (78)
(cid:13) δ (cid:13)
2
Thus,wemaywrite wz(T) =u(T)+ζ,where∥ζ∥ ≤ϵ.Iflim u(t)=0,then,usingeq.(75),
δ 2 t→∞
∥w (T)∥ ≤2δϵ.
z 2
Else,sinceϵ∈(0,η),and∥u(T)∥ ≥2η,wehave∥u(T)+ζ∥ ≥η. Hence,
2 2
w (T) u(T)+ζ
z =
∥w (T)∥ ∥u(T)+ζ∥
z 2 2
whichimplies
w (T)⊤uˆ u(T)⊤uˆ+ζ⊤uˆ (cid:18) u(T)⊤uˆ(cid:19) ∥u(T)∥ ζ⊤uˆ
z = = 2 + .
∥w (T)∥ ∥u(T)+ζ∥ ∥u(T)∥ ∥u(T)+ζ∥ ∥u(T)+ζ∥
z 2 2 2 2 2
Since
∥u(T)∥ ∥u(T)∥ 1 1 ϵ
2 ≥ 2 = ≥ ≥1− ,
∥u(T)+ζ∥ 2 ∥u(T)∥ 2+∥ζ∥ 2 1+ ∥ζ∥2 1+ 2ϵ η 2η
∥u(T)∥2
and
ζ⊤uˆ −ϵ
≥ ,
∥u(T)+ζ∥ η
2
wehavethat
w (T)⊤uˆ (cid:18) ϵ (cid:19) ϵ (cid:18) 3 (cid:19)
z ≥(1−ϵ) 1− − ≥1− 1+ ϵ.
∥w (T)∥ 2η η 2η
z 2
xxixE Gradient Flow Dynamics of f(u ,u ) = u |u |
1 2 1 2
Inthefollowinglemma,wedescribethegradientflowsolutionsoff(u ,u )=u |u |wheninitializedat[1,0]⊤.
1 2 1 2
LemmaE.1. ForanyT >0,considerthefollowingtime-varyingfunction
 (cid:20) (cid:21)
1
(cid:20)
u
(t)(cid:21) 
0
, forallt∈[0,T]
u T(t)= u1T
(t)
= (cid:20) cosh(t−T)(cid:21) (79)
2T 
sinh(t−T)
, forallt≥T,
then,fora.e.t≥0,u (t)satisfies
T
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
u˙ ∂ f(u ,u ) |u | u (0) 1
1 ∈ u1 1 2 = 2 , 1 = , (80)
u˙ ∂ f(u ,u ) u ∂|u | u (0) 0
2 u2 1 2 1 2 2
where

[−1,1], ifu =0
 2
∂|u |∈ 1, ifu >0, .
2 2
 −1, ifu <0
2
Proof. Fort∈[0,T),u (t)isaconstant,hence,
T
(cid:20) (cid:21) (cid:20) (cid:21)
u˙ 0
1T = .
u˙ 0
2T
Sinceu (t)=1andu (t)=0,forallt∈[0,T),wehave
1T 2T
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
|u (t)| 0 0
2T = ∋ .
u (t)∂|u (t)| [−1,1] 0
1T 2T
Fort>T,u (t)andu (t)arecontinuousanddifferentiablefunctions,thus,
1T 2T
(cid:20) (cid:21) (cid:20) (cid:21)
u˙ sinh(t−T)
1T = .
u˙ cosh(t−T)
2T
Sinceu (t)>0,hence,forallt>T,wehave
2T
(cid:20) (cid:21) (cid:20) (cid:21)
|u (t)| sinh(t−T)
2T = ,
u (t)∂|u (t)| cosh(t−T)
1T 2T
completingtheproof.
TheabovelemmashowsthatforanyT > 0,u (t)definedineq.(79)isagradientflowsolutionoff(u ,u )when
T √ √ 1 2
initializedat[1,0]⊤. ForanyfiniteT,itiseasytoseethatlim u (t)/∥u (t)∥ =[1/ 2,1/ 2]⊤. Hence,fora
t→∞ T T 2
fixedT andanyϵ>0,wecanchooseT suchthat
√
(cid:13) (cid:20) (cid:21)(cid:13)
(cid:13)
(cid:13)
u T(T)
−
1/ √2 (cid:13)
(cid:13)≥ϵ.
(cid:13)∥u (T)∥ 1/ 2 (cid:13)
T 2
F Gradient Flow Dynamics of g(u) = (u |u |−1)2
1 2
Wefirstdescribethegradientfieldofg(u)=(u |u |−1)2near[1,0]T.
1 2
Lemma F.1. Let u˜ = [1,0]T, then, 0 ∈ ∂g(u˜). Further, for any δ ∈ (0,0.1), let uδ = [1+δ,δ]⊤. Then, for any
s∈−∂g(uδ)
∥s∥ ≥1, ands⊤(u˜−uδ)<0.
2
xxxProof. Since
(cid:20) (cid:21) (cid:20) (cid:21)
∂ g(u ,u ) 2|u |(u |u |−1)
u1 1 2 = 2 1 2 ,
∂ g(u ,u ) 2u ∂|u |(u |u |−1)
u2 1 2 1 2 1 2
itiseasytoshow0∈∂g(u˜). Next,
(cid:20)
∂
g(uδ)(cid:21) (cid:20)
−2δ(1−δ(1+δ))
(cid:21)
u1 = ,
∂ g(uδ) −2(1+δ)(1−δ(1+δ)),
u2
andsoforanys∈−∂g(uδ)wehavethat
∥s∥ ≥2(1+δ)(1−δ(1+δ))≥2(1−0.1·1.1)≥1.
2
Inaddition,
s⊤(u˜−uδ)=−2δ2(1−δ(1+δ))−2δ(1+δ)(1−δ(1+δ))<0.
Thelemmaaboveestablishesthatthereexistpointsinanyarbitrarilysmallneighborhoodofthesaddlepointu˜ where
thegradientofg(u)haslargenorm. Further,thegradientatthosepointswill“pointaway”fromu˜.
Inthenextlemmawedescribethegradientflowdynamicsofg(u)=(u |u |−1)2near[1,0]T,andshowthatgradient
1 2
flowwillescapefromanyarbitrarilysmallneighborhoodofthesaddlepointu˜ inaconstanttime.
LemmaF.2. Letu˜ =[1,0]T,andu (t)beasolutionofthedifferentialinclusion
δ
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
u˙ ∂ g(u ,u ) u (0) 1+δ
1 ∈− u1 1 2 , 1 = . (81)
u˙ ∂ g(u ,u ) u (0) δ
2 u2 1 2 2
Then,foranyδ ∈(0,0.1),∥u (0.1)−u˜∥ ≥0.09.
δ 2
Weshowthatnomatterhowclosetheinitializationistothesaddlepointu˜,thegradientflowwillescapefromitwithin
√
constanttime. Here,∥u (0)−u˜∥ ≤ 2δwhereδcanbearbitrarilysmallandpositive. However,∥u (0.1)−u˜∥ ≥
δ 2 δ 2
0.09,thus,u (t)escapesfromtheneighborhoodofu˜ withinconstanttimeforanyarbitrarilysmallδ.
δ
Proof. Chooseδ ∈ (0.0.1)andletS := {(u ,u ) : u ∈ [0.8,1.2],u ∈ [δ/2,0.35]}. Notethatforanyu ∈ S,we
1 2 1 2
have
δ/2≤δ(1−1.2·0.35)≤−2|u |(u |u |−1)≤2·0.35·(1−0.8·δ/2)≤1, and (82)
2 1 2
0.9≤2·0.8(1−1.2·0.35)≤−2u (u |u |−1)≤2·1.2·(1−0.8·δ/2)≤2.4. (83)
1 1 2
Letu (t)beasolutionofeq.(81). Forthesakeofbrevity,weuseu(t)insteadofu (t). Notethatu(0) ∈ S. LetT
δ δ
bethesmallestt≥0suchthatu(T)∈/ S. Forallt∈[0,T],u (t)>0,thus,u(t)satisfies
2
(cid:20) (cid:21) (cid:20) (cid:21)
u˙ −2|u |(u |u |−1)
1 = 2 1 2 , forallt∈[0,T]. (84)
u˙ −2u (u |u |−1)
2 1 1 2
Fromeq.(82)andeq.(83),foranyt∈[0,T],wehave
u˙ ∈[δ/2,1], andu˙ ∈[0.9,2.4]. (85)
1 2
Usingthesebounds,wenextshowthatT >0.1. Assumeforthesakeofcontradiction,T ≤0.1. Then,fromeq.(85),
foranyt∈[0,T],wehave
0.8<u (0)≤u (0)+δt/2≤u (t)≤u (0)+t≤1+δ+0.1<1.2, and
1 1 1 1
δ/2<u (0)≤u (0)+0.9t≤u (t)≤u (0)+2.4t≤δ+0.24≤0.34
2 2 2 2
Fromtheaboveequation,weobservethatu(T)∈S,whichleadstoacontradiction. Thus,T >0.1. Hence,usingthe
lowerboundonu˙ ineq.(85),wehave
2
u (0.1)≥u (0)+0.9·0.1≥0.09.
2 2
Thus,
∥u(0.1)−u˜∥ ≥|u (0.1)|≥0.09.
2 2
xxxiG Proof of Lemma C.1
WeproveLemmaC.1inasimilarwayasin(Filippov,1988), thoughthatproofconsidersamoregeneralcase. For
ourproblem,theproofcanbeslightlyshortened.
ToproveLemmaC.1wemakeusefothefollowinglemma
LemmaG.1. (Filippov,1988,Lemma13,Section5)Letforallt∈[a,b]thevector-valuedfunctionx (t)beabsolutely
k
continuous, x (t) → x(t) as k → ∞, and for each k = 1,2,... the functions x˙ (t) ∈ M almost everywhere on
k k
t ∈ (a,b), with M being a bounded closed set. Then the vector-valued function x(t) is absolutely continuous and
x˙(t)∈conv(M)almosteverywhereont∈(a,b).
ProofofLemmaC.1. Forthesakeofcontradiction,weassumethatthestatementinLemmaC.1isnottrue. Thus,for
someϵ>0thereexistsasequenceofsolutionsu (t)of
j
n
du X
∈ (z +fj(t))∂H(x ;u),u(0)=u ,j =1,2,..., (86)
dt i i i 0
i=1
where|fj(t)|≤δ ,foralli∈[n]andj ≥1,andδ →0,suchthatforanysolutionu˜(t)of
i j j
n
du˜ X
∈ z ∂H(x ;u˜),u˜(0)=u , (87)
dt i i 0
i=1
wehave
max ∥u (t)−u˜(t)∥ >ϵ. (88)
j 2
t∈[0,T]
√
Wealsoassumethatδ ≤ B/ n,forsomepositiveconstantB andforallj ≥ 1. Wefirstshowthat{u (t)}∞ has
j j j=1
aconvergentsubsequence. Notethatforanyt∈[0,T],u (t)isboundedsince
j
d∥u j∥2
2
=4Xn
(z +fj(t))H(x ;u )≤4β∥u ∥2(∥z∥ +B),
dt i i i j j 2 2
i=1
whichimplies
∥u (t)∥2 ≤∥u ∥2e4tβ(∥z∥2+B) ≤∥u ∥2e4Tβ(∥z∥2+B) :=B2.
j 2 0 2 0 2 1
Wenextdefine
χ:=sup{∥∂H(x;w)∥ :w∈Sk−1}.
2
Wenotethat{u (t)}∞ isequicontinuous,sinceforanyt ,t ∈[0,T]andj ≥1,
j i=1 1 2
(cid:13) (cid:13)
(cid:13)Z t2Xn (cid:13) Z t2Xn
∥u (t )−u (t )∥ =(cid:13) (z +fj(s))∂H(x ;u )ds(cid:13) ≤ |(z +fj(s))|χ∥u ∥ ds
j 1 j 2 2 (cid:13) i i i j (cid:13) i i j 2
(cid:13) t1 i=1 (cid:13) 2 t1 i=1 √
≤|t −t |B χ n(∥z∥ +B).
2 1 1 2
Therefore,usingtheArzelà–AscoliTheorem,thereexistsasubsequence{u (t)}∞ thatconvergesuniformly. We
jk k=1
denotethelimitingfunctionbyuˆ(t). Wecompleteourproofbyshowinguˆ(t)isasolutionofeq.(87)sincethatwill
leadtoacontradiction.
Foranyvectoru,wedefineF(u) =
Pn
z ∂H(x ;u). Foranyγ > 0,theγ−neighborhoodofF(u),denotedby
i=1 i i
Fγ(u),isdefinedas
Fγ(u)={v:v=h+q,h∈F(u),∥q∥ ≤γ}.
2
Itiseasytoshowthatforanyfiniteγ,Fγ(u)isanonempty,convex,andcompactset. Also,wedefine
uˆη =:{u:∥u−uˆ∥ ≤η},tˆα =:{t:|t−tˆ|≤α}
2
xxxiiChooseanytˆ∈ [0,T]. Since,F(u)isuppersemicontinuous,foranyγ > 0,thereexistasmallenoughη > 0such
thatforallu ∈ uˆη(tˆ),F(u) ⊆ Fγ(uˆ(tˆ)). Further,sinceuˆ(t)iscontinuous,thereexistsasmallenoughαsuchthat,
for all t ∈ tˆα, uˆ(t) ∈ uˆη(tˆ). Hence, for any γ > 0, there exist a small enough α > 0, such that for all t ∈ tˆα,
F(uˆ(t))⊆Fγ(uˆ(tˆ)).
Next,since{u (t)}∞ convergesuniformlytouˆ(t),wecanchoosek largeenoughsuchthat∥u (t)−uˆ(t)∥ ≤ η,
jk k=1 1 jk 2 2
forallk > k andt ∈ tˆα. Sinceuˆ(t)iscontinuous,thereexistβ > 0,suchthat∥uˆ(t)−uˆ(tˆ)∥ ≤ η,forallt ∈ tˆβ.
1 2 2
Thus,forallt∈tˆα∩tˆβ andk >k ,
1
∥u (t)−uˆ(tˆ)∥ ≤∥u (t)−uˆ(t)∥ +∥uˆ(t)−uˆ(tˆ)∥ ≤η.
jk 2 jk 2 2
Hence,F(u (t))⊆Fγ(uˆ(tˆ)),forallt∈tˆα∩tˆβ andk >k .
jk 1
Also, since δ → 0, we can choose k large enough, such that δ ≤ γ , for all k > k . Thus,
jk 2 jk nB1χ 2
∥Pn fjk(t)∂H(x ;u (t))∥ ≤nB χδ ≤γ,forallk >k .Therefore,forallk ≥max{k ,k }andt∈tˆα∩tˆβ,
i=1 i i jk 2 1 jk 2 1 2
n
X
u˙ (t)∈F(u (t))+ fjk(t)∂H(x ;u (t))∈F2γ(uˆ(tˆ)), fora.e.t∈tˆα∩tˆβ.
jk jk i i jk
i=1
FromLemmaG.1,uˆ(t)isabsolutelycontinuousintˆα∩tˆβ and
uˆ˙(t)∈F2γ(uˆ(tˆ)), fora.e.t∈tˆα∩tˆβ.
Wecancovertheinterval[0,T]byvaryingtˆ. Therefore,uˆ(t)isabsolutelycontinuousintheinterval[0,T]anduˆ˙(t)
exist almost everywhere. Also, if for any t ∈ [0,T], uˆ˙(t) exists then uˆ˙(t) ∈ F2γ(uˆ(t)), where γ can be made
arbitrarilysmall. Hence,
uˆ˙(t)∈F(uˆ(t)), fora.e.t∈[0,T].
xxxiii