[
    {
        "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression",
        "authors": "Zhuoshi PanQianhui WuHuiqiang JiangMenglin XiaXufang LuoJue ZhangQingwei LinVictor RühleYuqing YangChin-Yew LinH. Vicky ZhaoLili QiuDongmei Zhang",
        "links": "http://arxiv.org/abs/2403.12968v1",
        "entry_id": "http://arxiv.org/abs/2403.12968v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12968v1",
        "summary": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x.",
        "updated": "2024-03-19 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12968v1"
    },
    {
        "title": "Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models",
        "authors": "Ce ZhangSimon StepputtisKatia SycaraYaqi Xie",
        "links": "http://arxiv.org/abs/2403.12964v1",
        "entry_id": "http://arxiv.org/abs/2403.12964v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12964v1",
        "summary": "Recently, large-scale pre-trained Vision-Language Models (VLMs) have\ndemonstrated great potential in learning open-world visual representations, and\nexhibit remarkable performance across a wide range of downstream tasks through\nefficient fine-tuning. In this work, we innovatively introduce the concept of\ndual learning into fine-tuning VLMs, i.e., we not only learn what an image is,\nbut also what an image isn't. Building on this concept, we introduce a novel\nDualAdapter approach to enable dual-path adaptation of VLMs from both positive\nand negative perspectives with only limited annotated samples. In the inference\nstage, our DualAdapter performs unified predictions by simultaneously\nconducting complementary positive selection and negative exclusion across\ntarget classes, thereby enhancing the overall recognition accuracy of VLMs in\ndownstream tasks. Our extensive experimental results across 15 datasets\nvalidate that the proposed DualAdapter outperforms existing state-of-the-art\nmethods on both few-shot learning and domain generalization tasks while\nachieving competitive computational efficiency. Code is available at\nhttps://github.com/zhangce01/DualAdapter.",
        "updated": "2024-03-19 17:59:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12964v1"
    },
    {
        "title": "Dated Data: Tracing Knowledge Cutoffs in Large Language Models",
        "authors": "Jeffrey ChengMarc MaroneOrion WellerDawn LawrieDaniel KhashabiBenjamin Van Durme",
        "links": "http://arxiv.org/abs/2403.12958v1",
        "entry_id": "http://arxiv.org/abs/2403.12958v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12958v1",
        "summary": "Released Large Language Models (LLMs) are often paired with a claimed\nknowledge cutoff date, or the dates at which training data was gathered. Such\ninformation is crucial for applications where the LLM must provide up to date\ninformation. However, this statement only scratches the surface: do all\nresources in the training data share the same knowledge cutoff date? Does the\nmodel's demonstrated knowledge for these subsets closely align to their cutoff\ndates? In this work, we define the notion of an effective cutoff. This is\ndistinct from the LLM designer reported cutoff and applies separately to\nsub-resources and topics. We propose a simple approach to estimate effective\ncutoffs on the resource-level temporal alignment of an LLM by probing across\nversions of the data. Using this analysis, we find that effective cutoffs often\ndiffer from reported cutoffs. To understand the root cause of this observation,\nwe conduct a direct large-scale analysis on open pre-training datasets. Our\nanalysis reveals two reasons for these inconsistencies: (1) temporal biases of\nCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)\ncomplications in LLM deduplication schemes involving semantic duplicates and\nlexical near-duplicates. Overall, our results show that knowledge cutoffs are\nnot as simple as they have seemed and that care must be taken both by LLM\ndataset curators as well as practitioners who seek to use information from\nthese models.",
        "updated": "2024-03-19 17:57:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12958v1"
    },
    {
        "title": "Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models",
        "authors": "Joana Ribeiro de FariaHuiyuan XieFelix Steffek",
        "links": "http://arxiv.org/abs/2403.12936v1",
        "entry_id": "http://arxiv.org/abs/2403.12936v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12936v1",
        "summary": "Court transcripts and judgments are rich repositories of legal knowledge,\ndetailing the intricacies of cases and the rationale behind judicial decisions.\nThe extraction of key information from these documents provides a concise\noverview of a case, crucial for both legal experts and the public. With the\nadvent of large language models (LLMs), automatic information extraction has\nbecome increasingly feasible and efficient. This paper presents a comprehensive\nstudy on the application of GPT-4, a large language model, for automatic\ninformation extraction from UK Employment Tribunal (UKET) cases. We\nmeticulously evaluated GPT-4's performance in extracting critical information\nwith a manual verification process to ensure the accuracy and relevance of the\nextracted data. Our research is structured around two primary extraction tasks:\nthe first involves a general extraction of eight key aspects that hold\nsignificance for both legal specialists and the general public, including the\nfacts of the case, the claims made, references to legal statutes, references to\nprecedents, general case outcomes and corresponding labels, detailed order and\nremedies and reasons for the decision. The second task is more focused, aimed\nat analysing three of those extracted features, namely facts, claims and\noutcomes, in order to facilitate the development of a tool capable of\npredicting the outcome of employment law disputes. Through our analysis, we\ndemonstrate that LLMs like GPT-4 can obtain high accuracy in legal information\nextraction, highlighting the potential of LLMs in revolutionising the way legal\ninformation is processed and utilised, offering significant implications for\nlegal research and practice.",
        "updated": "2024-03-19 17:43:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12936v1"
    },
    {
        "title": "Supporting Energy Policy Research with Large Language Models",
        "authors": "Grant BusterPavlo PinchukJacob BarronsRyan McKeeverAaron LevineAnthony Lopez",
        "links": "http://arxiv.org/abs/2403.12924v1",
        "entry_id": "http://arxiv.org/abs/2403.12924v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12924v1",
        "summary": "The recent growth in renewable energy development in the United States has\nbeen accompanied by a simultaneous surge in renewable energy siting ordinances.\nThese zoning laws play a critical role in dictating the placement of wind and\nsolar resources that are critical for achieving low-carbon energy futures. In\nthis context, efficient access to and management of siting ordinance data\nbecomes imperative. The National Renewable Energy Laboratory (NREL) recently\nintroduced a public wind and solar siting database to fill this need. This\npaper presents a method for harnessing Large Language Models (LLMs) to automate\nthe extraction of these siting ordinances from legal documents, enabling this\ndatabase to maintain accurate up-to-date information in the rapidly changing\nenergy policy landscape. A novel contribution of this research is the\nintegration of a decision tree framework with LLMs. Our results show that this\napproach is 85 to 90% accurate with outputs that can be used directly in\ndownstream quantitative modeling. We discuss opportunities to use this work to\nsupport similar large-scale policy research in the energy sector. By unlocking\nnew efficiencies in the extraction and analysis of legal documents using LLMs,\nthis study enables a path forward for automated large-scale energy policy\nresearch.",
        "updated": "2024-03-19 17:28:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12924v1"
    }
]