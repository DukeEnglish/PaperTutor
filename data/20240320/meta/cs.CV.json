[
    {
        "title": "Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment",
        "authors": "Mengting ChenXi ChenZhonghua ZhaiChen JuXuewen HongJinsong LanShuai Xiao",
        "links": "http://arxiv.org/abs/2403.12965v1",
        "entry_id": "http://arxiv.org/abs/2403.12965v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12965v1",
        "summary": "This paper introduces a novel framework for virtual try-on, termed\nWear-Any-Way. Different from previous methods, Wear-Any-Way is a customizable\nsolution. Besides generating high-fidelity results, our method supports users\nto precisely manipulate the wearing style. To achieve this goal, we first\nconstruct a strong pipeline for standard virtual try-on, supporting\nsingle/multiple garment try-on and model-to-model settings in complicated\nscenarios. To make it manipulable, we propose sparse correspondence alignment\nwhich involves point-based control to guide the generation for specific\nlocations. With this design, Wear-Any-Way gets state-of-the-art performance for\nthe standard setting and provides a novel interaction form for customizing the\nwearing style. For instance, it supports users to drag the sleeve to make it\nrolled up, drag the coat to make it open, and utilize clicks to control the\nstyle of tuck, etc. Wear-Any-Way enables more liberated and flexible\nexpressions of the attires, holding profound implications in the fashion\nindustry.",
        "updated": "2024-03-19 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12965v1"
    },
    {
        "title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models",
        "authors": "Zuyan LiuYuhao DongYongming RaoJie ZhouJiwen Lu",
        "links": "http://arxiv.org/abs/2403.12966v1",
        "entry_id": "http://arxiv.org/abs/2403.12966v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12966v1",
        "summary": "In the realm of vision-language understanding, the proficiency of models in\ninterpreting and reasoning over visual content has become a cornerstone for\nnumerous applications. However, it is challenging for the visual encoder in\nLarge Vision-Language Models (LVLMs) to extract useful features tailored to\nquestions that aid the language model's response. Furthermore, a common\npractice among existing LVLMs is to utilize lower-resolution images, which\nrestricts the ability for visual recognition. Our work introduces the\nChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel\napproach that enhances feature extraction by focusing on key regions of\ninterest (ROI) within the image, corresponding to the posed questions or\ninstructions. This technique allows LVLMs to access more detailed visual\ninformation without altering the original image resolution, thereby offering\nmulti-granularity image features. By integrating Chain-of-Spot with\ninstruct-following LLaVA-1.5 models, the process of image reasoning\nconsistently improves performance across a wide range of multimodal datasets\nand benchmarks without bells and whistles and achieves new state-of-the-art\nresults. Our empirical findings demonstrate a significant improvement in LVLMs'\nability to understand and reason about visual content, paving the way for more\nsophisticated visual instruction-following applications. Code and models are\navailable at https://github.com/dongyh20/Chain-of-Spot",
        "updated": "2024-03-19 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12966v1"
    },
    {
        "title": "Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models",
        "authors": "Ce ZhangSimon StepputtisKatia SycaraYaqi Xie",
        "links": "http://arxiv.org/abs/2403.12964v1",
        "entry_id": "http://arxiv.org/abs/2403.12964v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12964v1",
        "summary": "Recently, large-scale pre-trained Vision-Language Models (VLMs) have\ndemonstrated great potential in learning open-world visual representations, and\nexhibit remarkable performance across a wide range of downstream tasks through\nefficient fine-tuning. In this work, we innovatively introduce the concept of\ndual learning into fine-tuning VLMs, i.e., we not only learn what an image is,\nbut also what an image isn't. Building on this concept, we introduce a novel\nDualAdapter approach to enable dual-path adaptation of VLMs from both positive\nand negative perspectives with only limited annotated samples. In the inference\nstage, our DualAdapter performs unified predictions by simultaneously\nconducting complementary positive selection and negative exclusion across\ntarget classes, thereby enhancing the overall recognition accuracy of VLMs in\ndownstream tasks. Our extensive experimental results across 15 datasets\nvalidate that the proposed DualAdapter outperforms existing state-of-the-art\nmethods on both few-shot learning and domain generalization tasks while\nachieving competitive computational efficiency. Code is available at\nhttps://github.com/zhangce01/DualAdapter.",
        "updated": "2024-03-19 17:59:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12964v1"
    },
    {
        "title": "FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis",
        "authors": "Linjiang HuangRongyao FangAiping ZhangGuanglu SongSi LiuYu LiuHongsheng Li",
        "links": "http://arxiv.org/abs/2403.12963v1",
        "entry_id": "http://arxiv.org/abs/2403.12963v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12963v1",
        "summary": "In this study, we delve into the generation of high-resolution images from\npre-trained diffusion models, addressing persistent challenges, such as\nrepetitive patterns and structural distortions, that emerge when models are\napplied beyond their trained resolutions. To address this issue, we introduce\nan innovative, training-free approach FouriScale from the perspective of\nfrequency domain analysis. We replace the original convolutional layers in\npre-trained diffusion models by incorporating a dilation technique along with a\nlow-pass operation, intending to achieve structural consistency and scale\nconsistency across resolutions, respectively. Further enhanced by a\npadding-then-crop strategy, our method can flexibly handle text-to-image\ngeneration of various aspect ratios. By using the FouriScale as guidance, our\nmethod successfully balances the structural integrity and fidelity of generated\nimages, achieving an astonishing capacity of arbitrary-size, high-resolution,\nand high-quality generation. With its simplicity and compatibility, our method\ncan provide valuable insights for future explorations into the synthesis of\nultra-high-resolution images. The code will be released at\nhttps://github.com/LeonHLJ/FouriScale.",
        "updated": "2024-03-19 17:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12963v1"
    },
    {
        "title": "FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation",
        "authors": "Shuai YangYifan ZhouZiwei LiuChen Change Loy",
        "links": "http://arxiv.org/abs/2403.12962v1",
        "entry_id": "http://arxiv.org/abs/2403.12962v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12962v1",
        "summary": "The remarkable efficacy of text-to-image diffusion models has motivated\nextensive exploration of their potential application in video domains.\nZero-shot methods seek to extend image diffusion models to videos without\nnecessitating model training. Recent methods mainly focus on incorporating\ninter-frame correspondence into attention mechanisms. However, the soft\nconstraint imposed on determining where to attend to valid features can\nsometimes be insufficient, resulting in temporal inconsistency. In this paper,\nwe introduce FRESCO, intra-frame correspondence alongside inter-frame\ncorrespondence to establish a more robust spatial-temporal constraint. This\nenhancement ensures a more consistent transformation of semantically similar\ncontent across frames. Beyond mere attention guidance, our approach involves an\nexplicit update of features to achieve high spatial-temporal consistency with\nthe input video, significantly improving the visual coherence of the resulting\ntranslated videos. Extensive experiments demonstrate the effectiveness of our\nproposed framework in producing high-quality, coherent videos, marking a\nnotable improvement over existing zero-shot methods.",
        "updated": "2024-03-19 17:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12962v1"
    }
]