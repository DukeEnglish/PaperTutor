[
    {
        "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression",
        "authors": "Zhuoshi PanQianhui WuHuiqiang JiangMenglin XiaXufang LuoJue ZhangQingwei LinVictor RühleYuqing YangChin-Yew LinH. Vicky ZhaoLili QiuDongmei Zhang",
        "links": "http://arxiv.org/abs/2403.12968v1",
        "entry_id": "http://arxiv.org/abs/2403.12968v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12968v1",
        "summary": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x.",
        "updated": "2024-03-19 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12968v1"
    },
    {
        "title": "TexTile: A Differentiable Metric for Texture Tileability",
        "authors": "Carlos Rodriguez-PardoDan CasasElena GarcesJorge Lopez-Moreno",
        "links": "http://arxiv.org/abs/2403.12961v1",
        "entry_id": "http://arxiv.org/abs/2403.12961v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12961v1",
        "summary": "We introduce TexTile, a novel differentiable metric to quantify the degree\nupon which a texture image can be concatenated with itself without introducing\nrepeating artifacts (i.e., the tileability). Existing methods for tileable\ntexture synthesis focus on general texture quality, but lack explicit analysis\nof the intrinsic repeatability properties of a texture. In contrast, our\nTexTile metric effectively evaluates the tileable properties of a texture,\nopening the door to more informed synthesis and analysis of tileable textures.\nUnder the hood, TexTile is formulated as a binary classifier carefully built\nfrom a large dataset of textures of different styles, semantics, regularities,\nand human annotations.Key to our method is a set of architectural modifications\nto baseline pre-train image classifiers to overcome their shortcomings at\nmeasuring tileability, along with a custom data augmentation and training\nregime aimed at increasing robustness and accuracy. We demonstrate that TexTile\ncan be plugged into different state-of-the-art texture synthesis methods,\nincluding diffusion-based strategies, and generate tileable textures while\nkeeping or even improving the overall texture quality. Furthermore, we show\nthat TexTile can objectively evaluate any tileable texture synthesis method,\nwhereas the current mix of existing metrics produces uncorrelated scores which\nheavily hinders progress in the field.",
        "updated": "2024-03-19 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12961v1"
    },
    {
        "title": "WHAC: World-grounded Humans and Cameras",
        "authors": "Wanqi YinZhongang CaiRuisi WangFanzhou WangChen WeiHaiyi MeiWeiye XiaoZhitao YangQingping SunAtsushi YamashitaZiwei LiuLei Yang",
        "links": "http://arxiv.org/abs/2403.12959v1",
        "entry_id": "http://arxiv.org/abs/2403.12959v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12959v1",
        "summary": "Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.",
        "updated": "2024-03-19 17:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12959v1"
    },
    {
        "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models",
        "authors": "Elaine SuiXiaohan WangSerena Yeung-Levy",
        "links": "http://arxiv.org/abs/2403.12952v1",
        "entry_id": "http://arxiv.org/abs/2403.12952v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12952v1",
        "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 datasets involving natural distribution shifts and cross-dataset\ngeneralization demonstrate TPS's superior performance, achieving\nstate-of-the-art results while reducing resource requirements.",
        "updated": "2024-03-19 17:54:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12952v1"
    },
    {
        "title": "Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion",
        "authors": "Joe SukArpit Agarwal",
        "links": "http://arxiv.org/abs/2403.12950v1",
        "entry_id": "http://arxiv.org/abs/2403.12950v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12950v1",
        "summary": "In dueling bandits, the learner receives preference feedback between arms,\nand the regret of an arm is defined in terms of its suboptimality to a winner\narm. The more challenging and practically motivated non-stationary variant of\ndueling bandits, where preferences change over time, has been the focus of\nseveral recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and\nAgarwal, 2023). The goal is to design algorithms without foreknowledge of the\namount of change.\n  The bulk of known results here studies the Condorcet winner setting, where an\narm preferred over any other exists at all times. Yet, such a winner may not\nexist and, to contrast, the Borda version of this problem (which is always\nwell-defined) has received little attention. In this work, we establish the\nfirst optimal and adaptive Borda dynamic regret upper bound, which highlights\nfundamental differences in the learnability of severe non-stationarity between\nCondorcet vs. Borda regret objectives in dueling bandits.\n  Surprisingly, our techniques for non-stationary Borda dueling bandits also\nyield improved rates within the Condorcet winner setting, and reveal new\npreference models where tighter notions of non-stationarity are adaptively\nlearnable. This is accomplished through a novel generalized Borda score\nframework which unites the Borda and Condorcet problems, thus allowing\nreduction of Condorcet regret to a Borda-like task. Such a generalization was\nnot previously known and is likely to be of independent interest.",
        "updated": "2024-03-19 17:50:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12950v1"
    }
]