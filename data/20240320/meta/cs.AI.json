[
    {
        "title": "TexTile: A Differentiable Metric for Texture Tileability",
        "authors": "Carlos Rodriguez-PardoDan CasasElena GarcesJorge Lopez-Moreno",
        "links": "http://arxiv.org/abs/2403.12961v1",
        "entry_id": "http://arxiv.org/abs/2403.12961v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12961v1",
        "summary": "We introduce TexTile, a novel differentiable metric to quantify the degree\nupon which a texture image can be concatenated with itself without introducing\nrepeating artifacts (i.e., the tileability). Existing methods for tileable\ntexture synthesis focus on general texture quality, but lack explicit analysis\nof the intrinsic repeatability properties of a texture. In contrast, our\nTexTile metric effectively evaluates the tileable properties of a texture,\nopening the door to more informed synthesis and analysis of tileable textures.\nUnder the hood, TexTile is formulated as a binary classifier carefully built\nfrom a large dataset of textures of different styles, semantics, regularities,\nand human annotations.Key to our method is a set of architectural modifications\nto baseline pre-train image classifiers to overcome their shortcomings at\nmeasuring tileability, along with a custom data augmentation and training\nregime aimed at increasing robustness and accuracy. We demonstrate that TexTile\ncan be plugged into different state-of-the-art texture synthesis methods,\nincluding diffusion-based strategies, and generate tileable textures while\nkeeping or even improving the overall texture quality. Furthermore, we show\nthat TexTile can objectively evaluate any tileable texture synthesis method,\nwhereas the current mix of existing metrics produces uncorrelated scores which\nheavily hinders progress in the field.",
        "updated": "2024-03-19 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12961v1"
    },
    {
        "title": "WHAC: World-grounded Humans and Cameras",
        "authors": "Wanqi YinZhongang CaiRuisi WangFanzhou WangChen WeiHaiyi MeiWeiye XiaoZhitao YangQingping SunAtsushi YamashitaZiwei LiuLei Yang",
        "links": "http://arxiv.org/abs/2403.12959v1",
        "entry_id": "http://arxiv.org/abs/2403.12959v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12959v1",
        "summary": "Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.",
        "updated": "2024-03-19 17:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12959v1"
    },
    {
        "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models",
        "authors": "Elaine SuiXiaohan WangSerena Yeung-Levy",
        "links": "http://arxiv.org/abs/2403.12952v1",
        "entry_id": "http://arxiv.org/abs/2403.12952v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12952v1",
        "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 datasets involving natural distribution shifts and cross-dataset\ngeneralization demonstrate TPS's superior performance, achieving\nstate-of-the-art results while reducing resource requirements.",
        "updated": "2024-03-19 17:54:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12952v1"
    },
    {
        "title": "Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers",
        "authors": "Vidhi JainMaria AttarianNikhil J JoshiAyzaan WahidDanny DriessQuan VuongPannag R SanketiPierre SermanetStefan WelkerChristine ChanIgor GilitschenskiYonatan BiskDebidatta Dwibedi",
        "links": "http://arxiv.org/abs/2403.12943v1",
        "entry_id": "http://arxiv.org/abs/2403.12943v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12943v1",
        "summary": "While large-scale robotic systems typically rely on textual instructions for\ntasks, this work explores a different approach: can robots infer the task\ndirectly from observing humans? This shift necessitates the robot's ability to\ndecode human intent and translate it into executable actions within its\nphysical constraints and environment. We introduce Vid2Robot, a novel\nend-to-end video-based learning framework for robots. Given a video\ndemonstration of a manipulation task and current visual observations, Vid2Robot\ndirectly produces robot actions. This is achieved through a unified\nrepresentation model trained on a large dataset of human video and robot\ntrajectory. The model leverages cross-attention mechanisms to fuse prompt video\nfeatures to the robot's current state and generate appropriate actions that\nmimic the observed task. To further improve policy performance, we propose\nauxiliary contrastive losses that enhance the alignment between human and robot\nvideo representations. We evaluate Vid2Robot on real-world robots,\ndemonstrating a 20% improvement in performance compared to other\nvideo-conditioned policies when using human demonstration videos. Additionally,\nour model exhibits emergent capabilities, such as successfully transferring\nobserved motions from one object to another, and long-horizon composition, thus\nshowcasing its potential for real-world applications. Project website:\nvid2robot.github.io",
        "updated": "2024-03-19 17:47:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12943v1"
    },
    {
        "title": "Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models",
        "authors": "Joana Ribeiro de FariaHuiyuan XieFelix Steffek",
        "links": "http://arxiv.org/abs/2403.12936v1",
        "entry_id": "http://arxiv.org/abs/2403.12936v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12936v1",
        "summary": "Court transcripts and judgments are rich repositories of legal knowledge,\ndetailing the intricacies of cases and the rationale behind judicial decisions.\nThe extraction of key information from these documents provides a concise\noverview of a case, crucial for both legal experts and the public. With the\nadvent of large language models (LLMs), automatic information extraction has\nbecome increasingly feasible and efficient. This paper presents a comprehensive\nstudy on the application of GPT-4, a large language model, for automatic\ninformation extraction from UK Employment Tribunal (UKET) cases. We\nmeticulously evaluated GPT-4's performance in extracting critical information\nwith a manual verification process to ensure the accuracy and relevance of the\nextracted data. Our research is structured around two primary extraction tasks:\nthe first involves a general extraction of eight key aspects that hold\nsignificance for both legal specialists and the general public, including the\nfacts of the case, the claims made, references to legal statutes, references to\nprecedents, general case outcomes and corresponding labels, detailed order and\nremedies and reasons for the decision. The second task is more focused, aimed\nat analysing three of those extracted features, namely facts, claims and\noutcomes, in order to facilitate the development of a tool capable of\npredicting the outcome of employment law disputes. Through our analysis, we\ndemonstrate that LLMs like GPT-4 can obtain high accuracy in legal information\nextraction, highlighting the potential of LLMs in revolutionising the way legal\ninformation is processed and utilised, offering significant implications for\nlegal research and practice.",
        "updated": "2024-03-19 17:43:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12936v1"
    }
]