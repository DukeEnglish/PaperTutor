[
    {
        "title": "Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion",
        "authors": "Joe SukArpit Agarwal",
        "links": "http://arxiv.org/abs/2403.12950v1",
        "entry_id": "http://arxiv.org/abs/2403.12950v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12950v1",
        "summary": "In dueling bandits, the learner receives preference feedback between arms,\nand the regret of an arm is defined in terms of its suboptimality to a winner\narm. The more challenging and practically motivated non-stationary variant of\ndueling bandits, where preferences change over time, has been the focus of\nseveral recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and\nAgarwal, 2023). The goal is to design algorithms without foreknowledge of the\namount of change.\n  The bulk of known results here studies the Condorcet winner setting, where an\narm preferred over any other exists at all times. Yet, such a winner may not\nexist and, to contrast, the Borda version of this problem (which is always\nwell-defined) has received little attention. In this work, we establish the\nfirst optimal and adaptive Borda dynamic regret upper bound, which highlights\nfundamental differences in the learnability of severe non-stationarity between\nCondorcet vs. Borda regret objectives in dueling bandits.\n  Surprisingly, our techniques for non-stationary Borda dueling bandits also\nyield improved rates within the Condorcet winner setting, and reveal new\npreference models where tighter notions of non-stationarity are adaptively\nlearnable. This is accomplished through a novel generalized Borda score\nframework which unites the Borda and Condorcet problems, thus allowing\nreduction of Condorcet regret to a Borda-like task. Such a generalization was\nnot previously known and is likely to be of independent interest.",
        "updated": "2024-03-19 17:50:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12950v1"
    },
    {
        "title": "On Safety in Safe Bayesian Optimization",
        "authors": "Christian FiedlerJohanna MennLukas KreiskötherSebastian Trimpe",
        "links": "http://arxiv.org/abs/2403.12948v1",
        "entry_id": "http://arxiv.org/abs/2403.12948v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12948v1",
        "summary": "Optimizing an unknown function under safety constraints is a central task in\nrobotics, biomedical engineering, and many other disciplines, and increasingly\nsafe Bayesian Optimization (BO) is used for this. Due to the safety critical\nnature of these applications, it is of utmost importance that theoretical\nsafety guarantees for these algorithms translate into the real world. In this\nwork, we investigate three safety-related issues of the popular class of\nSafeOpt-type algorithms. First, these algorithms critically rely on frequentist\nuncertainty bounds for Gaussian Process (GP) regression, but concrete\nimplementations typically utilize heuristics that invalidate all safety\nguarantees. We provide a detailed analysis of this problem and introduce\nReal-\\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent\nGP bounds and thus retains all theoretical guarantees. Second, we identify\nassuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of\nthe target function, a key technical assumption in SafeOpt-like algorithms, as\na central obstacle to real-world usage. To overcome this challenge, we\nintroduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm,\nwhich guarantees safety without an assumption on the RKHS bound, and\nempirically show that this algorithm is not only safe, but also exhibits\nsuperior performance compared to the state-of-the-art on several function\nclasses. Third, SafeOpt and derived algorithms rely on a discrete search space,\nmaking them difficult to apply to higher-dimensional problems. To widen the\napplicability of these algorithms, we introduce Lipschitz-only GP-UCB\n(LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional\nproblems, while retaining safety.",
        "updated": "2024-03-19 17:50:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12948v1"
    },
    {
        "title": "Clustered Mallows Model",
        "authors": "Luiza S. C. PiancastelliNial Friel",
        "links": "http://arxiv.org/abs/2403.12880v1",
        "entry_id": "http://arxiv.org/abs/2403.12880v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12880v1",
        "summary": "Rankings are a type of preference elicitation that arise in experiments where\nassessors arrange items, for example, in decreasing order of utility. Orderings\nof n items labelled {1,...,n} denoted are permutations that reflect strict\npreferences. For a number of reasons, strict preferences can be unrealistic\nassumptions for real data. For example, when items share common traits it may\nbe reasonable to attribute them equal ranks. Also, there can be different\nimportance attributions to decisions that form the ranking. In a situation\nwith, for example, a large number of items, an assessor may wish to rank at top\na certain number items; to rank other items at the bottom and to express\nindifference to all others. In addition, when aggregating opinions, a judging\nbody might be decisive about some parts of the rank but ambiguous for others.\nIn this paper we extend the well-known Mallows (Mallows, 1957) model (MM) to\naccommodate item indifference, a phenomenon that can be in place for a variety\nof reasons, such as those above mentioned.The underlying grouping of similar\nitems motivates the proposed Clustered Mallows Model (CMM). The CMM can be\ninterpreted as a Mallows distribution for tied ranks where ties are learned\nfrom the data. The CMM provides the flexibility to combine strict and\nindifferent relations, achieving a simpler and robust representation of rank\ncollections in the form of ordered clusters. Bayesian inference for the CMM is\nin the class of doubly-intractable problems since the model's normalisation\nconstant is not available in closed form. We overcome this challenge by\nsampling from the posterior with a version of the exchange algorithm\n\\citep{murray2006}. Real data analysis of food preferences and results of\nFormula 1 races are presented, illustrating the CMM in practical situations.",
        "updated": "2024-03-19 16:25:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12880v1"
    },
    {
        "title": "Primal Methods for Variational Inequality Problems with Functional Constraints",
        "authors": "Liang ZhangNiao HeMichael Muehlebach",
        "links": "http://arxiv.org/abs/2403.12859v1",
        "entry_id": "http://arxiv.org/abs/2403.12859v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12859v1",
        "summary": "Constrained variational inequality problems are recognized for their broad\napplications across various fields including machine learning and operations\nresearch. First-order methods have emerged as the standard approach for solving\nthese problems due to their simplicity and scalability. However, they typically\nrely on projection or linear minimization oracles to navigate the feasible set,\nwhich becomes computationally expensive in practical scenarios featuring\nmultiple functional constraints. Existing efforts to tackle such functional\nconstrained variational inequality problems have centered on primal-dual\nalgorithms grounded in the Lagrangian function. These algorithms along with\ntheir theoretical analysis often require the existence and prior knowledge of\nthe optimal Lagrange multipliers. In this work, we propose a simple primal\nmethod, termed Constrained Gradient Method (CGM), for addressing functional\nconstrained variational inequality problems, without necessitating any\ninformation on the optimal Lagrange multipliers. We establish a non-asymptotic\nconvergence analysis of the algorithm for variational inequality problems with\nmonotone operators under smooth constraints. Remarkably, our algorithms match\nthe complexity of projection-based methods in terms of operator queries for\nboth monotone and strongly monotone settings, while utilizing significantly\ncheaper oracles based on quadratic programming. Furthermore, we provide several\nnumerical examples to evaluate the efficacy of our algorithms.",
        "updated": "2024-03-19 16:03:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12859v1"
    },
    {
        "title": "Tighter Confidence Bounds for Sequential Kernel Regression",
        "authors": "Hamish FlynnDavid Reeb",
        "links": "http://arxiv.org/abs/2403.12732v1",
        "entry_id": "http://arxiv.org/abs/2403.12732v1",
        "pdf_url": "http://arxiv.org/pdf/2403.12732v1",
        "summary": "Confidence bounds are an essential tool for rigorously quantifying the\nuncertainty of predictions. In this capacity, they can inform the\nexploration-exploitation trade-off and form a core component in many sequential\nlearning and decision-making algorithms. Tighter confidence bounds give rise to\nalgorithms with better empirical performance and better performance guarantees.\nIn this work, we use martingale tail bounds and finite-dimensional\nreformulations of infinite-dimensional convex programs to establish new\nconfidence bounds for sequential kernel regression. We prove that our new\nconfidence bounds are always tighter than existing ones in this setting. We\napply our confidence bounds to the kernel bandit problem, where future actions\ndepend on the previous history. When our confidence bounds replace existing\nones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a\nmatching worst-case performance guarantee and comparable computational cost.\nOur new confidence bounds can be used as a generic tool to design improved\nalgorithms for other kernelised learning and decision-making problems.",
        "updated": "2024-03-19 13:47:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.12732v1"
    }
]