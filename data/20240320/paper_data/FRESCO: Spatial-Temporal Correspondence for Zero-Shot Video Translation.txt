FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation
ShuaiYang1* YifanZhou2 ZiweiLiu2 ChenChangeLoy2(cid:0)
1WangxuanInstituteofComputerTechnology,PekingUniversity 2S-Lab,NanyangTechnologicalUniversity
williamyang@pku.edu.cn {yifan006, ziwei.liu, ccloy}@ntu.edu.sg
Input
ControlNet
SDEdit
Customized model
Prompt: The Mona Lisa,
smile, oil painting by
Leonardo Da Vinci
Prompt: A beautiful
woman in CG style, pink
hair, tight hair
Prompt: A beautiful
grandma, white hair
Prompt: A beautiful
cartoon girl, 2D flat,
contour line,
Figure1. Ourframeworkenableshigh-qualityandcoherentvideotranslationbasedonpre-trainedimagediffusionmodel. Givenaninput
video,ourmethodre-rendersitbasedonatargettextprompt,whilepreservingitssemanticcontentandmotion.Ourzero-shotframework
iscompatiblewithvariousassistivetechniqueslikeControlNet,SDEditandLoRA,enablingmoreflexibleandcustomizedtranslation.
Abstract 1.Introduction
Theremarkableefficacyoftext-to-imagediffusionmod-
Intoday’sdigitalage,shortvideoshaveemergedasadomi-
els has motivated extensive exploration of their potential
nantformofentertainment. Theeditingandartisticrender-
application in video domains. Zero-shot methods seek to
ingofthesevideosholdconsiderablepracticalimportance.
extendimagediffusionmodelstovideoswithoutnecessitat-
Recentadvancementsindiffusionmodels[33,34,36]have
ingmodeltraining. Recentmethodsmainlyfocusonincor-
revolutionized image editing by enabling users to manipu-
poratinginter-framecorrespondenceintoattentionmecha-
lateimagesconvenientlythroughnaturallanguageprompts.
nisms. However, the soft constraint imposed on determin-
Despitethesestridesintheimagedomain,videomanipula-
ing where to attend to valid features can sometimes be in-
tion continues to pose unique challenges, especially in en-
sufficient,resultingintemporalinconsistency.Inthispaper,
suringnaturalmotionwithtemporalconsistency.
weintroduceFRESCO,intra-framecorrespondencealong-
sideinter-framecorrespondencetoestablishamorerobust Temporal-coherent motions can be learned by training
spatial-temporal constraint. This enhancement ensures a video models on extensive video datasets [6, 18, 38] or
moreconsistenttransformationofsemanticallysimilarcon- finetuning refactored image models on a single video [25,
tent across frames. Beyond mere attention guidance, our 37, 44], which is however neither cost-effective nor con-
approachinvolvesanexplicitupdateoffeaturestoachieve venient for ordinary users. Alternatively, zero-shot meth-
highspatial-temporalconsistencywiththeinputvideo,sig- ods [4, 5, 11, 23, 31, 41, 47] offer an efficient avenue
nificantly improving the visual coherence of the resulting forvideomanipulationbyalteringtheinferenceprocessof
translated videos. Extensive experiments demonstrate the image models with extra temporal consistency constraints.
effectivenessofourproposedframeworkinproducinghigh- Besides efficiency, zero-shot methods possess the advan-
quality, coherent videos, marking a notable improvement tages of high compatibility with various assistive tech-
overexistingzero-shotmethods. niques designed for image models, e.g., ControlNet [49]
andLoRA[19],enablingmoreflexiblemanipulation.
*WorkdonewhenShuaiYangwasRAPatS-Lab,NTU. Existing zero-shot methods predominantly concentrate
1
4202
raM
91
]VC.sc[
1v26921.3042:viXraon refining attention mechanisms. These techniques often (a) (b)
substitute self-attentions with cross-frame attentions [23, tu
p
n
44], aggregating features across multiple frames. How- i
ever,thisapproachensuresonlyacoarse-levelglobalstyle
re
consistency. Toachievemorerefinedtemporalconsistency, d
n
e
approacheslikeRerender-A-Video[47]andFLATTEN[5] re
R
assume that the generated video maintains the same inter-
framecorrespondenceastheoriginal. Theyincorporatethe
s
optical flow from the original video to guide the feature
ru
O
fusion process. While this strategy shows promise, three
issues remain unresolved. 1) Inconsistency. Changes in
opticalflowduringmanipulationmayresultininconsistent tu (e)
guidance, leadingtoissuessuchaspartsoftheforeground p
n
i
appearing in stationary background areas without proper (c) (d) (f)
foreground movement (Figs. 2(a)(f)). 2) Undercoverage.
In areas where occlusion or rapid motion hinders accurate re
d
optical flow estimation, the resulting constraints are insuf- n
e
ficient,leadingtodistortionsasillustratedinFigs.2(c)-(e).
re
R
3) Inaccuracy. The sequential frame-by-frame generation
isrestrictedtolocaloptimization,leadingtotheaccumula-
tionoferrorsovertime(missingfingersinFig.2(b)dueto s ru
O
noreferencefingersinpreviousframes).
Toaddresstheabovecriticalissues, wepresentFRamE
Figure2. RealvideotoCGvideotranslation. Methods[47]re-
Spatial-temporal COrrespondence (FRESCO). While pre-
lying on optical flow alone suffer (a)(f) inconsistent or (c)(d)(e)
viousmethodsprimarilyfocusonconstraininginter-frame
missingopticalflowguidanceand(b)erroraccumulation. Byin-
temporalcorrespondence,webelievethatpreservingintra-
troducingFRESCO,ourmethodaddressesthesechallengeswell.
frame spatial correspondence is equally crucial. Our ap-
proachensuresthatsemanticallysimilarcontentismanipu- allowing them to guide each other, while anchor frames
latedcohesively,maintainingitssimilaritypost-translation. aresharedacrossbatchestoensureinter-batchconsistency.
Thisstrategyeffectivelyaddressesthefirsttwochallenges: For long video translation, we use a heuristic approach
itpreventstheforegroundfrombeingerroneouslytranslated for keyframe selection and employ interpolation for non-
intothebackground,anditenhancestheconsistencyofthe keyframeframes. Ourmaincontributionsare:
optical flow. For regions where optical flow is not avail- • A novel zero-shot diffusion framework guided by frame
able, the spatial correspondence within the original frame spatial-temporalcorrespondenceforcoherentandflexible
canserveasaregulatorymechanism,asillustratedinFig.2. videotranslation.
In our approach, FRESCO is introduced to two levels:
• Combine FRESCO-guided feature attention and opti-
mizationasarobustintra-andinter-frameconstraintwith
attention and feature. At the attention level, we introduce
betterconsistencyandcoveragethanopticalflowalone.
FRESCO-guided attention. It builds upon the optical flow
• Long video translation by jointly processing batched
guidance from [5] and enriches the attention mechanism
frameswithinter-batchconsistency.
by integrating the self-similarity of the input frame. It al-
lows for the effective use of both inter-frame and intra-
2.RelatedWork
framecuesfromtheinputvideo,strategicallydirectingthe
focus to valid features in a more constrained manner. At Imagediffusionmodels. Recentyearshavewitnessedthe
the feature level, we present FRESCO-aware feature opti- explosivegrowthofimagediffusionmodelsfortext-guided
mization. This goes beyond merely influencing feature at- image generation and editing. Diffusion models synthe-
tention; it involves an explicit update of the semantically size images through an iterative denoising process [17].
meaningful features in the U-Net decoder layers. This is DALLE-2[33]leveragesCLIP[32]toaligntextandimages
achievedthroughgradientdescenttoaligncloselywiththe for text-to-image generation. Imagen [36] cascades diffu-
high spatial-temporal consistency of the input video. The sion models for high-resolution generation, where class-
synergy of these two enhancements leads to a notable up- freeguidance[29]isusedtoimprovetextconditioning.Sta-
lift in performance, as depicted in Fig. 1. To overcome bleDiffusionbuildsuponlatentdiffusionmodel[34]tode-
the final challenge, we employ a multi-frame processing noiseatacompactlatentspacetofurtherreducecomplexity.
strategy. Frames within a batch are processed collectively, Text-to-image models have spawned a series of image
2manipulation models [2, 16]. Prompt2Prompt [16] intro- mapped to a latent feature x = E(I) with an Encoder E.
0
duces cross-attention control to keep image layout. To Then, SDEdit applies DDPM forward process [17] to add
editrealimages,DDIMinversion[39]andNull-TextInver- Gaussiannoisetox
0
sion[28]areproposedtoembedrealimagesintothenoisy √
latentfeatureforeditingwithattentioncontrol[3,30,40]. q(x t|x 0)=N(x t; α¯ tx 0,(1−α¯ t)I), (1)
Besides text conditioning, various flexible conditions
whereα¯ isapre-definedhyperparamterattheDDPMstep
are introduced. SDEdit [27] introduces image guidance t
t. Then, in the DDPM backward process [17], the Stable
during generation. Object appearances and styles can
DiffusionU-Netϵ predictsthenoiseofthelatentfeatureto
be customized by finetuning text embeddings [8], model θ
iterativelytranslatex′ =x tox′ guidedbypromptc:
weights [14, 19, 24, 35] or encoders [9, 12, 43, 46, 48]. T T 0
√ √
ControlNet[49]introducesacontrolpathtoprovidestruc- α¯ β (1−α¯ )( α x′ +β z )
tureorlayoutinformationforfine-grainedgeneration. Our x′ t−1 = 1−t− α¯1 txˆ′ 0+ t−1 1−α¯t t t t , (2)
t t
zero-shot framework does not alter the pre-trained model
and, thus is compatible with these conditions for flexible whereα t andβ t =1−α t arepre-definedhyperparamters,
controlandcustomizationasshowninFig.1. z t is a randomly sampled standard Guassian noise, and xˆ′ 0
Zero-shot text-guided video editing. While large video isthepredictedx′ 0atthedenoisingstept,
diffusionmodelstrainedorfine-tunedonvideoshavebeen √ √
xˆ′ =(x′ − 1−α¯ ϵ (x′,t,c,e))/ α¯ , (3)
studied[1,6,7,10,13,15,15,18,26,37,38,42,44,51], 0 t t θ t t
this paper focuses on lightweight and highly compatible andϵ (x ,t′,c,e)isthepredictednoiseofx′ basedonthe
θ t t
zero-shotmethods. Zero-shotmethodscanbedividedinto
step t, the text prompt c and the ControlNet condition e.
inversion-basedandinversion-freemethods.
The e can be edges, poses or depth maps extracted from I
Inversion-based methods [22, 31] apply DDIM inver-
toprovideextrastructureorlayoutinformation.Finally,the
siontothevideoandrecordtheattentionfeaturesforatten- translatedframeI′ =D(x′)isobtainedwithaDecoderD.
0
tion control during editing. FateZero [31] detects and pre-
SDEditallowsuserstoadjustthetransformationdegreeby
servestheuneditedregionandusescross-frameattentionto
setting different initial noise level with T, i.e., large T for
enforce global appearance coherence. To explicitly lever- greaterappearancevariationbetweenI′andI. Forsimplic-
ageinter-framecorrespondence,Pix2Video[4]andToken-
ity,wewillomitthedenoisingsteptinthefollowing.
Flow[11]matchorblendfeaturesfromthepreviousedited
frames. FLATTEN[5]introducesopticalflowstotheatten- 3.2.OverallFramework
tionmechanismforfine-grainedtemporalconsistency.
The proposed zero-shot video translation pipeline is illus-
Inversion-freemethodsmainlyuseControlNetfortrans-
trated in Fig. 3. Given a set of video frames I = {I }N ,
lation. Text2Video-Zero [23] simulates motions by mov- i i=1
we follow Sec. 3.1 to perform DDPM forward and back-
ing noises. ControlVideo [50] extends ControlNet to
wardprocessestoobtainitstransformedI′ ={I′}N . Our
videos with cross-frame attention and inter-frame smooth- i i=1
adaptationfocusesonincorporatingthespatialandtempo-
ing. VideoControlNet [20] and Rerender-A-Video [47]
ralcorrespondencesofIintotheU-Net. Morespecifically,
warpsandfusesthepreviouseditedframeswithopticalflow
wedefinetemporalandspatialcorrespondencesofIas:
to improve temporal consistency. Compared to inversion-
• Temporalcorrespondence. Thisinter-framecorrespon-
basedmethods,inversion-freemethodsallowformoreflex-
dence is measured by optical flows between adjacent
ible conditioning and higher compatibility with the cus-
frames, a pivotal element in keeping temporal consis-
tomized models, enabling users to conveniently control
tency.Denotingtheopticalflowandocclusionmaskfrom
the output appearance. However, without the guidance I toI aswj andMj respectively,ourobjectiveistoen-
of DDIM inversion features, the inversion-free framework sui rethj atI′ai ndI′ i sharewi+1innon-occludedregions.
is prone to flickering. Our framework is also inversion- i i+1 i
• Spatial correspondence. This intra-frame correspon-
free, but further incorporates intra-frame correspondence,
denceisgaugedbyself-similarityamongpixelswithina
greatly improving temporal consistency while maintaining
singleframe. TheaimisforI′ toshareself-similarityas
highcontrollability. i
I , i.e., semantically similar content is transformed into
i
a similar appearance, and vice versa. This preservation
3.Methodology
of semantics and spatial layout implicitly contributes to
improvingtemporalconsistencyduringtranslation.
3.1.Preliminary
Our adaptation focuses on the input feature and the at-
We follow the inversion-free image translation pipeline of tentionmoduleofthedecoderlayerwithintheU-Net,since
StableDiffusionbasedonSDEdit[27]andControlNet[49], decoder layers are less noisy than encoder layers, and are
and adapt it to video translation. An input frame I is first moresemanticallymeaningfulthanthex latentspace:
t
3I x DDPM sampling ×T I’ Self-attention
T
ε
θ
DDPM … Text cross-attention
forward
e = FRESCO-guided attention
c = “A beautiful cartoon girl”,
Spatial-guided attention
Se
f
fec a. t3 u.3
re
cF
g
dooR
r em ×
apE
sdp
ct
KS
ii
eu
emC
nnte
tO
ti
z- aa
t
ℒw
ℒio
t+sa
epn
mr
at
pe
P
P P P
Pr
r r r
rL
L L L
Lo
o o o
oi
i i i
ij
j j j
jn
n n n
ne
e e e
ee
e e e
ec
c c c
ct
t t t
ta
a a a
ai
i i i
ir
r r r
ro
o o o
on
n n n
n
VK
sQ
K
aUQ r
menr
r
g pi piq uo lu in
nnoitnettA
id tla ue MH - e g K V[
[Q
p pu
u’
] ] noitnettA id tla ue MH
-Sec
VQ
K. ’3.4
s aF
bF
ml ao
ppsR
w fe lid
n-E gS VC
Q K ’[[
[O
pp
p-
ff f]]
]gu
noitnettA id tla ue MH
-ide Hd
[p
a f]tte samn
FR
F&pet
Nl-
ii no gn
E
T F
of
pe
Rf
m
ti
E
ic mSi pe
Co
in
zr
Ot
aa -
tc
l
ia-r
og
wo nus ais
d
r- ef er
d
fa
e
m
a at
te
t ue
a
rn
et tt ie on ntion
Figure3. Frameworkofourzero-shotvideotranslationguidedbyFRamESpatial-temporalCOrrespondence(FRESCO). A FRESCO-
awareoptimizationisappliedtotheU-Netfeaturestostrengthentheirtemporalandspatialcoherencewiththeinputframes. Weintegrate
FRESCO into self-attention layers, resulting in spatial-guided attention to keep spatial correspondence with the input frames, efficient
cross-frameattentionandtemporal-guidedattentiontokeeproughandfinetemporalcorrespondencewiththeinputframes,respectively.
• Featureadaptation.WeproposeanovelFRESCO-aware ForthespatialconsistencylossL spat,weusethecosine
featureoptimizationapproachasillustratedinFig.3. We similarity in the feature space to measure the spatial cor-
design a spatial consistency loss L and a temporal respondence of I . Specifically, we perform a single-step
spat i
consistencylossL todirectlyoptimizethedecoder- DDPM forward and backward process over I , and extract
temp i
layer features f = {f }N to strengthen their temporal the U-Net decoder feature denoted as fr. Since a single-
i i=1 i
andspatialcoherencewiththeinputframes. stepforwardprocessaddsnegligiblenoises,frcanserveas
i
• Attention adaptation. We replace self-attentions with asemanticmeaningfulrepresentationofI tocalculatethe
i
FRESCO-guided attentions, comprising three compo- semanticsimilarity. Then,thecosinesimilaritybetweenall
nents, as shown in Fig. 3. Spatial-guided attention first pairsofelementscanbesimplycalculatedasthegramma-
aggregatesfeaturesbasedontheself-similarityofthein- trixofthenormalizedfeature. Letf˜denotethenormalized
put frame. Then, cross-frame attention is used to aggre- f sothateachelementoff˜isaunitvector. Wewouldlike
gatefeaturesacrossallframes. Finally, temporal-guided thegrammatrixoff˜ toapproachthegrammatrixoff˜r,
i i
attentionaggregatesfeaturesalongthesameopticalflow
tofurtherreinforcetemporalconsistency. L spat(f)=λ spat(cid:88) ∥f˜ if˜ i⊤−f˜ irf˜ ir⊤∥2 2. (6)
The proposed feature adaptation directly optimizes the i
feature towards high spatial and temporal coherence with
I. Meanwhile,ourattentionadaptationindirectlyimproves
3.4.FRESCO-GuidedAttention
coherence by imposing soft constraints on how and where
A FRESCO-guided attention layer contains three consecu-
toattendtovalidfeatures.Wefindthatcombiningthesetwo
tivemodules:spatial-guidedattention,efficientcross-frame
formsofadaptationachievesthebestperformance.
attentionandtemporal-guidedattention,asshowninFig.3.
3.3.FRESCO-AwareFeatureOptimization Spatial-guided attention. In contrast to self-attention,
patches in spatial-guided attention aggregate each other
Theinputfeaturef = {f i}N i=1 ofeachdecoderlayerofU- based on the similarity of patches before translation rather
Netisupdatedbygradientdescentthroughoptimizing thantheirownsimilarity. Specifically, consistentwithcal-
culatingL inSec.3.3,weperformasingle-stepDDPM
ˆf =argminL (f)+L (f). (4) spat
temp spat forwardandbackwardprocessoverI ,andextractitsself-
f i
attentionqueryvectorQrandkeyvectorKr.Then,spatial-
Theupdatedˆf replacesf forsubsequentprocessing. guidedattentionaggregai teQ with i
i
ForthetemporalconsistencylossL ,wewouldlike
temp
the feature values of the corresponding positions between Q′ =Softmax(Qr iK √ir⊤ )·Q , (7)
everytwoadjacentframestobeconsistent, i λ d i
s
L (f)=(cid:88) ∥Mi+1(f −wi+1(f ))∥ (5) whereλ s isascalefactoranddisthequeryvectordimen-
temp i i+1 i i 1 sion. AsshowninFig.4,theforegroundpatchwillmainly
i
4I 1 I 2 I 3 Optical flow and occlusion mask Algorithm1Keyframeselection
tu Input: VideoI={I i}M i=1,sampleparameterss min,s max
p
n Output: KeyframeindexlistΩinascendingorder
I
1: initializeΩ=[1,M]andd i =0,∀i∈[1,M]
× × 2: setd i =L 2(I i,I i−1),∀i∈[s min+1,N −s min]
3: whileexistsisuchthatΩ[i+1]−Ω[i]>s maxdo
n g ise
d
n
Self attention Spatial-guided attention
54 :: sΩ e. ti dn js =er 0t ,( ∀ˆi) j.s ∈o (r ˆit −() sw mii nt ,h ˆiˆi += sa mr ing )max i(d i)
o
itn × ×
e
tta TEN,eachpatchcanonlyattendtopatchesinotherframes,
tn whichisunstablewhenaflowcontainsfewpatches. Differ-
e
re
ffid Cross-frame attention V1 Efficient cross-frame attention entfromit,thetemporal-guidedattentionhasnosuchlimit,
fo
n × × Q[p ](K[p ])⊤
o ita
rtsu
H[p f]=Softmax( f
λ
t√ df )·V′[p f], (9)
llI Cross-frame attention V2 Temporal-guided attention whereλ isascalefactor. AndH isthefinaloutputofour
t
Previous attentions FRESCO-guided attentions FRESCO-guidedattentionlayer.
Figure4.Illustrationofattentionmechanism.Thepatchesmarked
3.5.LongVideoTranslation
withredcrossesattendtothecoloredpatchesandaggregatetheir
features.Comparedtopreviousattentions,FRESCO-guidedatten-
ThenumberofframesN thatcanbeprocessedatonetime
tionfurtherconsidersintra-frameandinter-framecorrespondences
islimitedbyGPUmemory. Forlongvideotranslation,we
oftheinput. Spatial-guidedattentionaggregatesintra-framefea-
follow Rerender-A-Video [47] to perform zero-shot video
turesbasedontheself-similarityoftheinputframe(darkerindi-
translation on keyframes only and use Ebsynth [21] to in-
cates higher weights). Efficient cross-frame attention eliminates
terpolatenon-keyframesbasedontranslatedkeyframes.
redundant patches and retains unique patches. Temporal-guided
attentionaggregatesinter-framefeaturesonthesameflow. Keyframe selection. Rerender-A-Video [47] uniformly
samples keyframes, which is suboptimal. We propose a
aggregate features in the C-shaped foreground region, and heuristickeyframeselectionalgorithmassummizedinAl-
attend less to the background region. As a result, Q′ has gorithm 1. We relax the fixed sampling step to an interval
betterspatialconsistencywiththeinputframethanQ. [s ,s ], and densely sample keyframes when motions
min max
Efficient cross-frame attention. We replace self-attention arelarge(measuredbyL distancebetweenframes).
2
withcross-frameattentiontoregularizetheglobalstylecon- Keyframe translation. With over N keyframes, we split
sistency. Rather than using the first frame or the previous themintoseveralN-framebatches.Eachbatchincludesthe
frame as reference [4, 23] (V1, Fig. 4), which cannot han- first and last frames in the previous batch to impose inter-
dlethenewlyemergedobjects(e.g.,fingersinFig.2(b)),or batch consistency, i.e., keyframe indexes of the k-th batch
using all available frames as reference (V2, Fig. 4), which are{1,(k−1)(N−2)+2,(k−1)(N−2)+3,...,k(N−
iscomputationallyinefficient,weaimtoconsiderallframes 2)+2}. Besides,throughoutthewholedenoisingsteps,we
simultaneously but with as little redundancy as possible. record the latent features x′ (Eq. (2)) of the first and last
t
Thus, we propose efficient cross-frame attentions: Except frames of each batch, and use them to replace the corre-
for the first frame, we only reference to the areas of each spondinglatentfeaturesinthenextbatch.
framethatwerenotseeninitspreviousframe(i.e.,theoc-
clusionregion).Thus,wecanconstructacross-frameindex 4.Experiments
p ofallpatcheswithintheaboveregion. Keysandvalues
u
Implementation details. The experiment is conducted on
of these patches can be sampled as K[p ], V[p ]. Then,
u u
one NVIDIA Tesla V100 GPU. By default, we set batch
cross-frameattentionisapplied
sizeN ∈[6,8]basedontheinputvideoresolution,theloss
V i′
=Softmax(Q′ i(K √[p u])⊤
)·V[p u]. (8)
w tue reig oh pt tλ imsp ia zt a= tio5 n0 ,, wth ee us pc da ale tefa fc fto or rs Kλ s == 2λ 0t it= era5 t. ioF no sr wfe ita h-
d
Adamoptimizerandlearningrateof0.4. Wefindoptimiza-
Temporal-guided attention. Inspired by FLATTEN [5], tion mostly converges when K = 20 and larger K does
we use flow-based attention to regularize fine-level cross- not bring obvious gains. GMFlow [45] is used to estimate
frame consistency. We trace the same patches in differ- optical flows and occlusion masks. Background smooth-
ent frames as in Fig. 4. For each optical flow, we build a ing [23] is applied to improve temporal consistency in the
cross-frameindexp ofallpatchesonthisflow. InFLAT- backgroundregion.
f
5tu
p
n
I
)a
(
o
re
Z
V-
2
T
)b
(
o
e
d
iV
lo
rtn
o
C
)c
(
re
d
n
e
re
R
)d
(
r
u
O
)e
(
Prompt: A black boxer wearing black boxing
Prompt: A dog in the grass in the sun Prompt: Ared car turns in the winter
gloves punches towards the camera, cartoon style
Figure5.Visualcomparisonwithinversion-freezero-shotvideotranslationmethods.
4.1.ComparisonwithState-of-the-ArtMethods Table1.Quantitativecomparisonanduserpreferencerates.
We compare with three recent inversion-free zero-shot Metric Fram-Acc↑ Tem-Con↑ Pixel-MSE↓ Spat-Con↓ User↑
T2V-Zero 0.918 0.965 0.038 0.0845 9.1%
methods: Text2Video-Zero [23], ControlVideo [50],
ControlVideo 0.932 0.951 0.066 0.0957 2.6%
Rerender-A-Video [47]. To ensure a fair comparison, all Rerender 0.955 0.969 0.016 0.0836 23.3%
methods employ identical settings of ControlNet, SDEdit, Ours 0.978 0.975 0.012 0.0805 65.0%
and LoRA. As shown in Fig. 5, all methods successfully
Table2.Quantitativeablationstudy.
translate videos according to the provided text prompts.
However, the inversion-free methods, relying on Control- Metric baseline w/temp w/spat w/attn w/opt full
Net conditions, may experience a decline in video editing Fram-Acc↑ 1.000 1.000 1.000 1.000 1.000 1.000
qualityiftheconditionsareoflowquality,duetoissueslike Tem-Con↑ 0.974 0.979 0.976 0.976 0.977 0.980
Pixel-MSE↓ 0.032 0.015 0.020 0.016 0.019 0.012
defocusormotionblur. Forinstance,ControlVideofailsto
generate a plausible appearance of the dog and the boxer.
the average preference rates across the 11 test videos, re-
Text2Video-Zero and Rerender-A-Video struggle to main-
vealingthatourmethodemergesasthemostfavoredchoice.
tainthecat’sposeandthestructureoftheboxer’sgloves.In
contrast, our method can generate consistent videos based
4.2.AblationStudy
ontheproposedrobustFRESCOguidance.
For quantitative evaluation, adhering to standard prac- To validate the contributions of different modules to the
tices[4,31,47],weemploytheevaluationmetricsofFram- overall performance, we systematically deactivate specific
Acc (CLIP-based frame-wise editing accuracy), Tmp-Con modules in our framework. Figure 6 illustrates the ef-
(CLIP-basedcosinesimilaritybetweenconsecutiveframes) fectofincorporatingspatialandtemporalcorrespondences.
and Pixel-MSE (averaged mean-squared pixel error be- The baseline method solely uses cross-frame attention for
tweenalignedconsecutiveframes). WefurtherreportSpat- temporal consistency. By introducing the temporal-related
Con(L onVGGfeatures)forspatialcoherency.There- adaptation, we observe improvements in consistency, such
spat
sultsaveragedacross23videosarereportedinTable1. No- asthealignmentoftexturesandthestabilizationofthesun’s
tably,ourmethodattainsthebesteditingaccuracyandtem- positionacrosstwoframes. Meanwhile,thespatial-related
poralconsistency. Wefurtherconductauserstudywith57 adaptationaidsinpreservingtheposeduringtranslation.
participants. Participantsaretaskedwithselectingthemost InFig.7,westudytheeffectofattentionadaptationand
preferableresultsamongthefourmethods.Table1presents feature adaption. Clearly, each enhancement individually
6(a)λ = 1 (b)λ = 2 (c)λ = 5 (d)λ = 10
t t t t
(a) input (b) baseline (c) w/ temp (d) w/ spat (e) full
Figure8.Effectofλ .Quantitatively,thePixel-MSEscoresare(a)
t
Figure6. Effectofincorporatingspatialandtemporalcorrespon- 0.016,(b)0.014,(c)0.013,(d)0.012. Theyellowarrowsindicate
dences.Thebluearrowsindicatethespatialinconsistencywiththe theinconsistencybetweenthetwoframes.
inputframes. Theredarrowsindicatethetemporalinconsistency
betweentwooutputframes.
(a)
(a)input (b)λ = 1 (c)λ = 1.5 (d)λ = 2 (e)λ = 5
s s s s
Figure9. Effectofλ . Theregionintheredboxisenlargedand
s
(b) showninthetoprightforbettercomparison. Prompt: Acartoon
whitecatinpinkbackground.
(c)
(d)
(a) input (b) baseline (c)w/spatattn (d)w/spatopt (e) full
Figure10. Effectofincorporatingspatialcorrespondence. (a)In-
(e)
putcoveredwithredocclusionmask. (b)-(d)Ourspatial-guided
attentionandspatialconsistencylosshelpreducetheinconsistency
Figure 7. Effect of attention adaptation and feature adaptation. in ski poles (yellow arrows) and snow textures (red arrows), re-
Top row: (a) Input. Other rows: Results obtained with (b) only spectively.Prompt:AcartoonSpidermanisskiing.
cross-frameattention,(c)attentionadaptation,(d)featureadapta- makeopticalflowhardtopredict,leadingtoalargeocclu-
tion,(e)bothattentionandfeatureadaptations,respectively. The
sionregion.Spatialcorrespondenceguidanceisparticularly
blueregionisenlargedwithitscontrastenhancedontherightfor
crucial to constrain the rendering in this region. Clearly,
bettercomparison.Prompt:AbeautifulwomaninCGstyle.
eachadaptationmakesadistinctcontribution,suchaselim-
improvestemporalconsistencytoacertainextent, butnei- inating the unwanted ski pole and inconsistent snow tex-
ther achieves perfection. Only the combination of the two tures. Combiningthetwoyieldsthemostcoherentresults,
completely eliminates the inconsistency observed in hair asquantitativelyverifiedbythePixel-MSEscoresof0.031,
strands, which is quantitatively verified by the Pixel-MSE 0.028,0.025,0.024forFig.10(b)-(e),respectively.
scores of 0.037, 0.021, 0.018, 0.015 for Fig. 7(b)-(e), re- Table2providesaquantitativeevaluationoftheimpact
spectively.Regardingattentionadaptation,wefurtherdelve of each module. In alignment with the visual results, it is
intotemporal-guidedattentionandspatial-guidedattention. evidentthateachmodulecontributestotheoverallenhance-
Thestrengthoftheconstraintstheyimposeisdeterminedby mentoftemporalconsistency. Notably,thecombinationof
λ andλ ,respectively. AsshowninFigs.8-9,anincrease alladaptationsyieldsthebestperformance.
t s
in λ effectively enhances consistency between two trans- Figure 11 ablates the proposed efficient cross-frame at-
t
formedframesinthebackgroundregion,whileanincrease tention. AswithRerender-A-VideoinFig.2(b),sequential
inλ boostsposeconsistencybetweenthetransformedcat frame-by-frame translation is vulnerable to new appearing
s
and the original cat. Beyond spatial-guided attention, our objects. Our cross-frame attention allows attention to all
spatialconsistencylossalsoplaysanimportantrole,asval- uniqueobjectswithinthebatchedframes,whichisnotonly
idated in Fig. 10. In this example, rapid motion and blur efficientbutalsomorerobust,asdemonstratedinFig.12.
7keyframe #40 non-keyframe #42 non-keyframe #43 non-keyframe #45
(a) input (b) Cross-frame attention V1
non-keyframe #47 non-keyframe #48 non-keyframe #50 keyframe # 52
Figure14.Longvideogenerationbyinterpolatingnon-keyframes
basedonthetranslatedkeyframes.
(c) Cross-frame attention V2 (d) Efficient cross-frame attention
Figure11. Effectofefficientcross-frameattention. (a)Input. (b)
Cross-frameattentionV1attendstothepreviousframeonly,thus
failing to synthesize the newly appearing fingers. (d) The effi-
cientcross-frameattentionachievesthesameperformanceas(c)
cross-frameattentionV2,butreducestheregionthatneedstobe
attendedtoby41.6%inthisexample.Prompt:Abeautifulwoman
holdingherglassesinCGstyle.
Figure15.Videocolorization.Prompt:Abluesealonthebeach.
nel from the input and the AB channel from the translated
video,wecancolorizetheinputwithoutalteringitscontent.
4.4.LimitationandFutureWork
In terms of limitations, first, Rerender-A-Video directly
(a) Sequential translation (b) Joint translation aligns frames at the pixel level, which outperforms our
Figure 12. Effect of joint multi-frame translation. Sequential method given high-quality optical flow. We would like to
translation relies on the previous frame alone. Joint translation explore an adaptive combination of these two methods in
usesallframesinabatchtoguideeachother,thusachievingaccu- the future to harness the advantages of each. Second, by
ratefingerstructuresbyreferencingtothethirdframeinFig.11 enforcing spatial correspondence consistency with the in-
put video, our method does not support large shape defor-
mations and significant appearance changes. Large defor-
mation makes it challenging to use the optical flow of the
original video as a reliable prior for natural motion. This
(a) optimize f before attention(b) optimize f after attention (c) optimize x^ 0 limitation is inherent in zero-shot models. A potential fu-
Figure13.Diffusionfeaturestooptimize. turedirectionistoincorporatelearnedmotionpriors[13].
FRESCOusesdiffusionfeaturesbeforetheattentionlay-
5.Conclusion
ersforoptimization.SinceU-Netistrainedtopredictnoise,
featuresafterattentionlayers(nearoutputlayer)arenoisy,
This paper presents a zero-shot framework to adapt image
leadingtofailureoptimization(Fig.13(b)). Meanwhile,the
diffusionmodelsforvideotranslation.Wedemonstratethe
four-channel xˆ′ (Eq. (3)) is highly compact, which is not
0 vitalroleofpreservingintra-framespatialcorrespondence,
suitableforwarpingorinterpolation. Optimizingxˆ′ results
0 in conjunction with inter-frame temporal correspondence,
insevereblursandover-saturationartifacts(Fig.13(c)).
which is less explored in prior zero-shot methods. Our
4.3.MoreResults comprehensiveexperimentsvalidatetheeffectivenessofour
methodintranslatinghigh-qualityandcoherentvideos.The
Long video translation. Figure 1 presents examples of proposed FRESCO constraint exhibits high compatibility
long video translation. A 16-second video comprising 400 withexistingimagediffusiontechniques,suggestingitspo-
frames are processed, where 32 frames are selected as tential application in other text-guided video editing tasks,
keyframes for diffusion-based translation and the remain- suchasvideosuper-resolutionandcolorization.
ing 368 non-keyframes are interpolated. Thank to our
Acknowledgments. This study is supported under the RIE2020
FRESCOguidancetogeneratecoherentkeyframes,thenon-
Industry Alignment Fund Industry Collaboration Projects (IAF-
keyframesexhibitcoherentinterpolationasinFig.14. ICP)FundingInitiative, aswellascashandin-kindcontribution
Video colorization. Our method can be applied to video fromtheindustrypartner(s).ThisstudyisalsosupportedbyNTU
colorization.AsshowninFig.15,bycombiningtheLchan- NAPandMOEAcRFTier2(T2EP20221-0012).
8References [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao,DahuaLin,andBoDai. AnimateDiff: Animateyour
[1] AndreasBlattmann,RobinRombach,HuanLing,TimDock-
personalizedtext-to-imagediffusionmodelswithoutspecific
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
tuning. arXivpreprintarXiv:2307.04725,2023. 3,8
Alignyourlatents: High-resolutionvideosynthesiswithla-
[14] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,
tentdiffusionmodels. InProc.IEEEInt’lConf.Computer
Dimitris Metaxas, and Feng Yang. SVDiff: Compact pa-
VisionandPatternRecognition,pages22563–22575,2023.
rameterspacefordiffusionfine-tuning. InProc.IEEEInt’l
3
Conf.ComputerVisionandPatternRecognition,2023. 3
[2] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-
[15] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
structpix2pix: Learning to follow image editing instruc-
QifengChen.Latentvideodiffusionmodelsforhigh-fidelity
tions.InProc.IEEEInt’lConf.ComputerVisionandPattern
video generation with arbitrary lengths. arXiv preprint
Recognition,pages18392–18402,2023. 3
arXiv:2211.13221,2022. 3
[3] MingdengCao,XintaoWang,ZhongangQi,YingShan,Xi-
[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
aohuQie,andYinqiangZheng. MasaCtrl: Tuning-freemu-
Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt im-
tualself-attentioncontrolforconsistentimagesynthesisand
age editing with cross-attention control. In Proc. Int’l
editing. InProc.Int’lConf.ComputerVision,2023. 3
Conf.LearningRepresentations,2022. 3
[4] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mi-
[17] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif-
tra. Pix2video: Video editing using image diffusion. In
fusionprobabilisticmodels.InAdvancesinNeuralInforma-
Proc. Int’l Conf. Computer Vision, pages 23206–23217,
tionProcessingSystems,pages6840–6851,2020. 2,3
2023. 1,3,5,6
[18] Jonathan Ho, WilliamChan, ChitwanSaharia, JayWhang,
[5] YurenCong,MengmengXu,ChristianSimon,ShoufaChen, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
Rosenhahn, Tao Xiang, and Sen He. FLATTEN: optical video:Highdefinitionvideogenerationwithdiffusionmod-
flow-guided attention for consistent text-to-video editing. els. arXivpreprintarXiv:2210.02303,2022. 1,3
arXivpreprintarXiv:2310.05922,2023. 1,2,3,5
[19] EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,
[6] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-
Jonathan Granskog, and Anastasis Germanidis. Structure rank adaptation of large language models. In Proc. Int’l
andcontent-guidedvideosynthesiswithdiffusionmodels.In Conf.LearningRepresentations,2021. 1,3
Proc.Int’lConf.ComputerVision,pages7346–7356,2023.
[20] ZhihaoHuandDongXu.Videocontrolnet:Amotion-guided
1,3
video-to-video translation framework by using diffusion
[7] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, model with controlnet. arXiv preprint arXiv:2307.14073,
Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo. 2023. 3
Ccedit:Creativeandcontrollablevideoeditingviadiffusion [21] Ondˇrej Jamrisˇka, Sˇa´rka Sochorova´, Ondˇrej Texler, Michal
models. arXivpreprintarXiv:2309.16496,2023. 3 Luka´cˇ,JakubFisˇer,JingwanLu,EliShechtman,andDaniel
[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Sy`kora. Stylizingvideobyexample. ACMTransactionson
AmitHaimBermano,GalChechik,andDanielCohen-or.An Graphics,38(4):1–11,2019. 5
imageisworthoneword: Personalizingtext-to-imagegen- [22] HyeonhoJeongandJongChulYe. Ground-a-video: Zero-
erationusingtextualinversion. InProc.Int’lConf.Learning shot grounded video editing using text-to-image diffusion
Representations,2022. 3 models. arXivpreprintarXiv:2310.01107,2023. 3
[9] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, [23] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
GalChechik,andDanielCohen-Or. Encoder-baseddomain vosyan, Roberto Henschel, Zhangyang Wang, Shant
tuningforfastpersonalizationoftext-to-imagemodels.ACM Navasardyan, and Humphrey Shi. Text2Video-Zero: Text-
TransactionsonGraphics,42(4):1–13,2023. 3 to-imagediffusionmodelsarezero-shotvideogenerators.In
[10] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, An- Proc.Int’lConf.ComputerVision,2023. 1,2,3,5,6
drew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, [24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Ming-YuLiu,andYogeshBalaji.Preserveyourowncorrela- Shechtman,andJun-YanZhu. Multi-conceptcustomization
tion:Anoisepriorforvideodiffusionmodels. InProc.Int’l oftext-to-imagediffusion. InProc.IEEEInt’lConf.Com-
Conf.ComputerVision,2023. 3 puterVisionandPatternRecognition,2023. 3
[11] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. [25] ShaotengLiu,YuechenZhang,WenboLi,ZheLin,andJiaya
Tokenflow:Consistentdiffusionfeaturesforconsistentvideo Jia. Video-P2P:Videoeditingwithcross-attentioncontrol.
editing.InProc.Int’lConf.LearningRepresentations,2024. arXivpreprintarXiv:2303.04761,2023. 1
1,3 [26] ZhengxiongLuo,DayouChen,YingyaZhang,YanHuang,
[12] Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and
Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Tieniu Tan. VideoFusion: Decomposed diffusion mod-
Ying Shan, and Yujiu Yang. TaleCrafter: Interactive story els for high-quality video generation. In Proc. IEEE Int’l
visualizationwithmultiplecharacters. InACMSIGGRAPH Conf. Computer Vision and Pattern Recognition, pages
AsiaConferenceProceedings,2023. 3 10209–10218,2023. 3
9[27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia- withouttext-videodata. InProc.Int’lConf.LearningRep-
junWu,Jun-YanZhu,andStefanoErmon. SDEdit: Guided resentations,2023. 1,3
imagesynthesisandeditingwithstochasticdifferentialequa- [39] JiamingSong,ChenlinMeng,andStefanoErmon. Denois-
tions. InProc.Int’lConf.LearningRepresentations,2021. ingdiffusionimplicitmodels. InProc.Int’lConf.Learning
3 Representations,2021. 3
[28] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and [40] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Daniel Cohen-Or. Null-text inversion for editing real im- Dekel. Plug-and-play diffusion features for text-driven
ages using guided diffusion models. In Proc. IEEE Int’l image-to-imagetranslation. InProc.IEEEInt’lConf.Com-
Conf. Computer Vision and Pattern Recognition, pages puter Vision and Pattern Recognition, pages 1921–1930,
6038–6047,2023. 3 2023. 3
[29] AlexanderQuinnNichol,PrafullaDhariwal,AdityaRamesh, [41] WenWang, KangyangXie, ZideLiu, HaoChen, YueCao,
Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya XinlongWang,andChunhuaShen. Zero-shotvideoediting
Sutskever, and Mark Chen. Glide: Towards photorealis- using off-the-shelf image diffusion models. arXiv preprint
ticimagegenerationandeditingwithtext-guideddiffusion arXiv:2303.17599,2023. 1
models. InProc.IEEEInt’lConf.MachineLearning,pages [42] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,
16784–16804,2022. 2 Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo
[30] GauravParmar,KrishnaKumarSingh,RichardZhang,Yijun Yu, Peiqing Yang, et al. Lavie: High-quality video gener-
Li,JingwanLu,andJun-YanZhu.Zero-shotimage-to-image ationwithcascadedlatentdiffusionmodels. arXivpreprint
translation. In ACM SIGGRAPH Conference Proceedings, arXiv:2309.15103,2023. 3
pages1–11,2023. 3 [43] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei
[31] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Zhang,andWangmengZuo. ELITE:Encodingvisualcon-
XintaoWang,YingShan,andQifengChen. FateZero: Fus- ceptsintotextualembeddingsforcustomizedtext-to-image
ing attentions for zero-shot text-based video editing. In generation. InProc.Int’lConf.ComputerVision,2023. 3
Proc.Int’lConf.ComputerVision,2023. 1,3,6 [44] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Lei,YuchaoGu,YufeiShi,WynneHsu,YingShan,Xiaohu
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Qie,andMikeZhengShou.Tune-A-Video:One-shottuning
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- of image diffusion models for text-to-video generation. In
ingtransferablevisualmodelsfromnaturallanguagesuper- Proc.Int’lConf.ComputerVision,pages7623–7633,2023.
vision. InProc.IEEEInt’lConf.MachineLearning,pages 1,2,3
8748–8763.PMLR,2021. 2 [45] HaofeiXu,JingZhang,JianfeiCai,HamidRezatofighi,and
[33] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, Dacheng Tao. GMFlow: Learning optical flow via global
and Mark Chen. Hierarchical text-conditional image gen- matching. In Proc. IEEE Int’l Conf. Computer Vision and
erationwithcliplatents. arXivpreprintarXiv:2204.06125, PatternRecognition,pages8121–8130,2022. 5
2022. 1,2 [46] XingqianXu,JiayiGuo,ZhangyangWang,GaoHuang,Ir-
[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, fanEssa,andHumphreyShi. Prompt-freediffusion:Taking
Patrick Esser, and Bjo¨rn Ommer. High-resolution image ”text”outoftext-to-imagediffusionmodels. arXivpreprint
synthesis with latent diffusion models. In Proc. IEEE arXiv:2305.16223,2023. 3
Int’lConf.ComputerVisionandPatternRecognition,pages [47] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change
10684–10695,2022. 1,2 Loy.Rerenderavideo:Zero-shottext-guidedvideo-to-video
[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, translation. InACMSIGGRAPHAsiaConferenceProceed-
MichaelRubinstein,andKfirAberman. Dreambooth: Fine ings,2023. 1,2,3,5,6
tuning text-to-image diffusion models for subject-driven [48] HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang. Ip-
generation. InProc.IEEEInt’lConf.ComputerVisionand adapter: Textcompatibleimagepromptadapterfortext-to-
PatternRecognition,pages22500–22510,2023. 3 imagediffusionmodels. arXivpreprintarXiv:2308.06721,
[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala 2023. 3
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, [49] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans, conditional control to text-to-image diffusion models. In
etal.Photorealistictext-to-imagediffusionmodelswithdeep Proc.Int’lConf.ComputerVision,pages3836–3847,2023.
languageunderstanding. InAdvancesinNeuralInformation 1,3
ProcessingSystems,pages36479–36494,2022. 1,2 [50] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng
[37] ChaehunShin,HeeseungKim,CheHyunLee,Sang-gilLee, Zhang, Wangmeng Zuo, and Qi Tian. ControlVideo:
andSungrohYoon. Edit-A-Video:Singlevideoeditingwith Training-free controllable text-to-video generation. In
object-awareconsistency. arXivpreprintarXiv:2303.07945, Proc.Int’lConf.LearningRepresentations,2024. 3,6
2023. 1,3 [51] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
[38] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, generation with latent diffusion models. arXiv preprint
OranGafni,etal. Make-A-Video: Text-to-videogeneration arXiv:2211.11018,2022. 3
10