FouriScale: A Frequency Perspective on
Training-Free High-Resolution Image Synthesis
Linjiang Huang1,2⋆, Rongyao Fang1⋆, Aiping Zhang3, Guanglu Song4
Si Liu5, Yu Liu4, and Hongsheng Li1,2 (cid:66)
1 CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong
2 Centre for Perceptual and Interactive Intelligence
3 Sun Yat-Sen University
4 Sensetime Research
5 Beihang University
ljhuang524@gmail.com, {rongyaofang@link, hsli@ee}.cuhk.edu.hk
Abstract. Inthisstudy,wedelveintothegenerationofhigh-resolution
images from pre-trained diffusion models, addressing persistent chal-
lenges,suchasrepetitivepatternsandstructuraldistortions,thatemerge
whenmodelsareappliedbeyondtheirtrainedresolutions.Toaddressthis
issue,weintroduceaninnovative,training-freeapproachFouriScalefrom
the perspective of frequency domain analysis. We replace the original
convolutional layers in pre-trained diffusion models by incorporating a
dilationtechniquealongwithalow-passoperation,intendingtoachieve
structural consistency and scale consistency across resolutions, respec-
tively. Further enhanced by a padding-then-crop strategy, our method
canflexiblyhandletext-to-imagegenerationofvariousaspectratios.By
using the FouriScale as guidance, our method successfully balances the
structuralintegrityandfidelityofgeneratedimages,achievinganaston-
ishing capacity of arbitrary-size, high-resolution, and high-quality gen-
eration. With its simplicity and compatibility, our method can provide
valuableinsightsforfutureexplorationsintothesynthesisofultra-high-
resolution images. The code will be released at https://github.com/
LeonHLJ/FouriScale.
Keywords: DiffusionModel·TrainingFree·High-ResolutionSynthesis
1 Introduction
Recently, Diffusion models [19,36] have emerged as the predominant genera-
tive models, surpassing the popularity of GANs [13] and autoregressive mod-
els [11,33]. Some text-to-image generation models, which are based on diffusion
models, such as Stable Diffusion (SD) [36], Stable Diffusion XL (SDXL) [32],
Midjourney[29],andImagen[37],haveshowntheirastonishingcapacitytogen-
erate high-quality and fidelity images under the guidance of text prompts. To
ensureefficientprocessingonexistinghardwareandstablemodeltraining,these
⋆ Equal contribution. (cid:66) Corresponding author.
4202
raM
91
]VC.sc[
1v36921.3042:viXra2 L. Huang et al.
Vanilla Attn-Entro ScaleCrafter Ours
“An anime man in flight uniform with hyper detailed digital artwork and an art style inspired by Klimt, Nixeu, Ian Sprigger, Wlop, and KrenzCushart.”
Fig.1: Visualization of pattern repetition issue of higher-resolution image synthesis
usingpre-trainedSDXL[32](Train:1024×1024;Inference:2048×2048).Attn-Entro[25]
fails to address this problem and ScaleCrafter [15] still struggles with this issue in
imagedetails.Ourmethodsuccessfullyhandlesthisproblemandgenerateshigh-quality
images without model retraining.
models are typically trained at one or a few specific image resolutions. For in-
stance, SD models are often trained using images of 512×512 resolution, while
SDXL models are typically trained with images close to 1024×1024 pixels.
However,asshowninFig.1,directlyemployingpre-traineddiffusionmodels
to generate an image at a resolution higher than what the models were trained
on will lead to significant issues, including repetitive patterns and unforeseen
artifacts. Some studies [3,24,26] have attempted to create larger images by uti-
lizing pre-trained diffusion models to stitch together overlapping patches into a
panoramicimage.Nonetheless,theabsenceofaglobaldirectionforthewholeim-
agerestrictstheirabilitytogenerateimagesfocusedonspecificobjectsandfails
toaddresstheproblemofrepetitivepatterns,whereaunifiedglobalstructureis
essential.Recentwork[25]hasexploredadaptingpre-traineddiffusionmodelsfor
generatingimagesofvarioussizesbyexaminingattentionentropy.Nevertheless,
ScaleCrafter [15] found that the key point of generating high-resolution images
lies in the convolution layers. They introduce a re-dilation operation and a con-
volution disperse operation to enlarge kernel sizes of convolution layers, largely
mitigating the problem of pattern repetition. However, their conclusion stems
from empirical findings, lacking a deeper exploration of this issue. Additionally,
it needs an initial offline computation of a linear transformation between the
original convolutional kernel and the enlarged kernel, falling short in terms of
compatibility and scalability when there are variations in the kernel sizes of the
UNet and the desired target resolution of images.
Inthiswork,wepresentFouriScale,aninnovativeandeffectiveapproachthat
handles the issue through the perspective of frequency domain analysis, suc-
cessfully demonstrating its effectiveness through both theoretical analysis and
experimental results. FouriScale substitutes the original convolutional layers in
pre-trained diffusion models by simply introducing a dilation operation coupled
with a low-pass operation, aimed at achieving structural and scale consistency
acrossresolutions,respectively.Equippedwithapadding-then-cropstrategy,our
method allows for flexible text-to-image generation of different sizes and aspect
ratios. Furthermore, by utilizing FouriScale as guidance, our approach attains
8402×8402
LXDSFouriScale 3
remarkablecapabilityinproducinghigh-resolutionimagesofanysize,withinte-
grated image structure alongside superior quality. The simplicity of FouriScale
eliminates the need for any offline pre-computation, facilitating compatibility
andscalability.WeenvisionFouriScaleprovidingsignificantcontributionstothe
advancement of ultra-high-resolution image synthesis in future research.
2 Related Work
2.1 Text-to-Image Synthesis
Text-to-image synthesis [9,20,36,37] has seen a significant surge in interest due
to the development of diffusion probabilistic models [19,40]. These innovative
models operate by generating data from a Gaussian distribution and refining
it through a denoising process. With their capacity for high-quality generation,
they have made significant leaps over traditional models like GANs [9,13], espe-
ciallyinproducingmorerealisticimages.TheLatentDiffusionModel(LDM)[36]
integratesthediffusionprocesswithinalatentspace,achievingastonishingreal-
ism in the generation of images, which boosts significant interest in the domain
of generating via latent space [5,16,27,31,46]. To ensure efficient processing on
existing hardware and stable model training, these models are typically trained
atoneorafewspecificimageresolutions.Forinstance,StabeDiffusion(SD)[36]
is trained using 512×512 pixel images, while SDXL [32] models are typically
trained with images close to 1024 × 1024 resolution, accommodating various
aspect ratios simultaneously.
2.2 High-Resolution Synthesis via Diffusion Models
High-resolution synthesis has always received widespread attention. Prior works
mainly focus on refining the noise schedule [7,22], developing cascaded ar-
chitectures [20,37,42] or mixtures-of-denoising-experts [2] for generating high-
resolution images. Despite their impressive capabilities, diffusion models were
oftenlimitedbyspecificresolutionconstraintsanddidnotgeneralizewellacross
differentaspectratiosandresolutions.Somemethodshavetriedtoaddressthese
issues by accommodating a broader range of resolutions. For example, Any-size
Diffusion[50]fine-tunesapre-trainedSDonasetofimageswithafixedrangeof
aspectratios,similartoSDXL[32].FiT[28]viewstheimageasasequenceofto-
kensandadaptivelypaddingimagetokenstoapredefinedmaximumtokenlimit,
ensuring hardware-friendly training and flexible resolution handling. However,
these models require model training, overlooking the inherent capability of the
pre-trained models to handle image generation with varying resolutions. Most
recently, some methods [3,24,26] have attempted to generate panoramic images
by utilizing pre-trained diffusion models to stitch together overlapping patches.
Recent work [25] has explored adapting pre-trained diffusion models for gen-
erating images of various sizes by examining attention entropy. ElasticDiff [14]
usestheestimationofdefaultresolutiontoguidethegenerationofarbitrary-size4 L. Huang et al.
Input feature at high resolution Output feature at high resolution
Convolution
Low pass filtering Dilation Down Sample
Down Sample
Original Kernel of UNet Dilated Kernel ≈
Down-sampled feature Convolution Output feature Consistency
at original resolution at original resolution across resolutions
Fig.2: TheoverviewofFouriScale(orangeline),whichincludesadilationconvolution
operation (Sec. 3.2) and a low-pass filtering operation (Sec. 3.3) to achieve structural
consistency and scale consistency across resolutions, respectively.
images. However, ScaleCrafter [15] finds that the key point of generating high-
resolutionimagesbypre-traineddiffusionmodelsliesinconvolutionlayers.They
presentare-dilationandaconvolutiondisperseoperationtoexpandconvolution
kernelsizes,whichrequiresanofflinecalculationofalineartransformationfrom
the original convolutional kernel to the expanded one. In contrast, we deeply
investigatetheissueofrepetitivepatternsandhandleitthroughtheperspective
of frequency domain analysis. The simplicity of our method eliminates the need
for any offline pre-computation, facilitating its compatibility and scalability.
3 Method
Diffusion models, also known as score-based generative models [19,40], belong
to a category of generative models that follow a process of progressively intro-
ducing Gaussian noise into the data and subsequently generating samples from
this noise through a reverse denoising procedure. The key denoising step is typ-
ically carried out by a U-shaped Network (UNet), which learns the underlying
denoisingfunctionthatmapsfromnoisydatatoitscleancounterpart.TheUNet
architecture,widelyadoptedforthispurpose,comprisesstackedconvolutionlay-
ers, self-attention layers, and cross-attention layers. Some previous works have
exploredthedegradationofperformancewhenthegeneratedresolutionbecomes
larger, attributing to the change of the attention tokens’ number [25] and the
reduced relative receptive field of convolution layers [15]. Based on empirical
evidence in [15], convolutional layers are more sensitive to changes in resolu-
tion. Therefore, we primarily focus on studying the impact brought about by
the convolutional layers. In this section, we will introduce FouriScale, as shown
in Fig. 2. It includes a dilation convolution operation (Sec. 3.2) and a low-pass
filtering operation (Sec. 3.3) to achieve structural consistency and scale consis-
tency across resolutions, respectively. With the tailored padding-then-cropping
strategy (Sec. 3.4), FouriScale can generate images of arbitrary aspect ratios.
By utilizing FouriScale as guidance (Sec. 3.5), our approach attains remarkable
capability in generating high-resolution and high-quality images.FouriScale 5
3.1 Notation
2D Discrete Fourier Transform (2D DFT). Given a two-dimensional discrete
signal F(m,n) with dimensions M ×N, the two-dimensional discrete Fourier
transform (2D DFT) is defined as:
M−1N−1
F(p,q)=
1 (cid:88) (cid:88) F(m,n)e−j2π(p Mm+q Nn)
. (1)
MN
m=0 n=0
2D Dilated Convolution. A dilated convolution kernel of the kernel k(m,n),
denotedas k (m,n),is formedbyintroducingzeros between theelements of
dh,dw
the original kernel such that:
(cid:40)
k(m, n ) if m%d =0 and n%d =0,
k dh,dw(m,n)=
0
dh dw otherwish
e,
w (2)
where d , d is the dilation factor along height and width, respectively, m and
h w
n are the indices in the dilated space. The % represents the modulo operation.
3.2 Structural Consistency via Dilated Convolution
The diffusion model’s denoising network, denoted as ϵ , is generally trained on
θ
images or latent spaces at a specific resolution of h×w. This network is often
constructed using a U-Net architecture. Our target is to generate an image of
a larger resolution of H × W at the inference stage using the parameters of
denoising network ϵ without retraining.
θ
Aspreviouslydiscussed,theconvolutionallayerswithintheU-Netarelargely
responsiblefortheoccurrenceofpatternrepetitionwhentheinferenceresolution
becomes larger. To prevent structural distortion at the inference resolution, we
resort to establishing structural consistency between the default resolution and
highresolutions,asshowninFig.2.Inparticular,foraconvolutionallayerConv
k
in the UNet with its convolution kernel k, and the high-resolution input feature
map F, the structural consistency can be formulated as follows:
Down (F)⊛k =Down (F ⊛k′), (3)
s s
where Down denotes the down-sampling operation with scale s1, and ⊛ rep-
s
resents the convolution operation. This equation implies the need to customize
a new convolution kernel k′ for a larger resolution. However, finding an ap-
propriate k′ can be challenging due to the variety of feature map F. The recent
ScaleCrafter[15]methodusesstructure-levelandpixel-levelcalibrationstolearn
alineartransformationbetweenk andk′,butlearninganewtransformationfor
each new kernel size and new target resolution can be cumbersome.
1 For simplicity, we assume equal down-sampling scales for height and width. Our
methodcanalsoaccommodatedifferentdown-samplingscalesinthiscontextthrough
our padding-then-cropping strategy (Section 3.4).6 L. Huang et al.
Inthiswork,weproposetohandlethestructuralconsistencyfromafrequency
perspective.SupposetheinputF(x,y),whichisatwo-dimensionaldiscretespa-
tial signal, belongs to the set RHf×Wf×C. The sampling rates along the x and y
axes are given by Ω and Ω correspondingly. The Fourier transform of F(x,y)
x y
is represented by F(u,v)∈RHf×Wf×C. In this context, the highest frequencies
alongtheuandv axesaredenotedasu andv ,respectively.Additionally,
max max
the Fourier transform of the downsampled feature map Down (F(x,y)), which
s
is dimensionally reduced to RH sf×W sf×C, is denoted as F′(u,v).
Theorem 1. Spatial down-sampling leads to a reduction in the range of fre-
quencies that the signal can accommodate, particularly at the higher end of the
spectrum.Thisprocesscauseshighfrequenciestobefoldedtolowfrequencies,and
superpose onto the original low frequencies. For a one-dimensional signal, in the
condition of s strides, this superposition of high and low frequencies resulting
from down-sampling can be mathematically formulated as
(cid:18) (cid:19) (cid:18) (cid:19)
aΩ Ω
F′(u)=S(F(u),F u+ x )|u∈ 0, x , (4)
s s
where S dentes the superposing operator, Ω is the sampling rates in x axis, and
x
a=1,...,s−1.
Lemma 1. For an image, the operation of spatial down-sampling using strides
of s can be viewed as partitioning the Fourier spectrum into s×s equal patches
and then uniformly superimposing these patches with an average scaling of 1 .
s2
s−1s−1
1 (cid:88)(cid:88)
DFT(Down (F(x,y)))= F (u,v), (5)
s s2 (i,j)
i=0j=0
where F (u,v) is a sub-matrix of F(u,v) by equally splitting F(u,v) into s×s
(i,j)
non-overlapped patches and i,j ∈{0,1,...,s−1}.
TheproofofTheorem1andLemma1areprovidedintheAppendix(Sec.A.1
and Sec. A.2). They describe the shuffling and superposing [34,47,51] in the
frequencydomainimposedbyspatialdown-sampling.IfwetransformEq.(3)to
the frequency domain and follow conclusion in Lemma 1, we can obtain:
 
s−1s−1
1 (cid:88)(cid:88)

s2
F (i,j)(u,v)⊙k(u,v)←Left side of Eq. (3)
i=0j=0
s−1s−1
1 (cid:88)(cid:88)(cid:0) (cid:1)
= F (u,v)⊙k(u,v) (6)
s2 (i,j)
i=0j=0
s−1s−1
1 (cid:88)(cid:88)(cid:16) (cid:17)
= F (u,v)⊙k′ (u,v) ,←Right side of Eq. (3)
s2 (i,j) (i,j)
i=0j=0FouriScale 7
5x5 Conv Kernel 20x20 Dilated Conv Kernel DFT of 20x20 Dilated Conv Kernel
0 0 0
1
2
3 4 4
4
0 1 2 3 4
DFT of 5x5 Conv Kernel
0
1 8 8
2
3
4 12 12
0 1 2 3 4
Cropped DFT
0
1
16 16
2
3
4
0 1 2 3 4 0 4 8 12 16 0 4 8 12 16
Fig.3: We visualize a random 5×5 kernel for better visualization. The Fourier spec-
trum of its dilated kernel, with a dilation factor of 4, clearly demonstrates a periodic
character. It should be noted that we also pad zeros to the right and bottom sides
of the dilated kernel, which differs from the conventional use. However, this does not
impact the outcome in practical applications.
where k(u,v), k′(u,v) denote the fourier transform of kernel k and k′, respec-
tively, ⊙ is element-wise multiplication. Eq. (6) suggests that the Fourier spec-
trumoftheidealconvolutionkernelk′ shouldbetheonethatisstitchedbys×s
Fourier spectrum of the convolution kernel k. In other words, there should be
a periodic repetition in the Fourier spectrum of k′, the repetitive pattern is the
Fourier spectrum of k.
Fortunately,thewidelyuseddilatedconvolutionperfectlymeetsthisrequire-
ment. Suppose a kernel k(m,n) with the size of M ×N, it’s dilated version is
k (m,n), with dilation factor of (d ,d ). For any integer multiples of d ,
dh,dw h w h
namely p′ =pd and integer multiples of d , namely q′ =qd , the exponential
h w w
term of the dilated kernel in the 2D DFT (Eq. (1)) becomes:
e−j2π(cid:16) dp h′m M+ dq w′n N(cid:17) =e−j2π(p Mm+q Nn)
, (7)
which is periodic with a period of M along the m-dimension and a period of
N along the n-dimension. It indicates that a dilated convolution kernel parame-
terized by the original kernel k, with dilation factor of (H/h,W/w), is the ideal
convolution kernel k′. In Fig. 3, we visually demonstrate the periodic repetition
of dilated convolution. We noticed that [15] also uses dilated operation. In con-
trast to [15], which is from empirical observation, our work begins with a focus
on frequency analysis and provides theoretical justification for its effectiveness.
3.3 Scale Consistency via Low-pass Filtering
However, in practice, dilated convolution alone cannot well mitigate the issue of
pattern repetition.As shown in Fig. 4a(top left), the issue ofpattern repetition
issignificantlyreduced,butcertainfinedetails,likethehorse’slegs,stillpresent8 L. Huang et al.
“a professional photograph of an astronaut riding a horse” 0.00.0 DowDno wBlno cBkloscks 0.00.0 DowDno wBlno cBkloscks
0.50.5 0.50.5
1.01.0 1.01.0 1.51.5 1.51.5 2.02.0 Low-Lroews -dreosw nd obwlonc bk lsotcekp s_t1ep_1 2.02.0 Low-Lroews -dreosw nd obwlonc bk lsotcekp s_t1ep_1 2.52.5 H Loig wh -H L- ror eieg wssh -d - rdr eoeo swsw nd nd ob o wbw lol non c c bk bk ls olso tct ec kek p p s _s_ t2t e1 5e pp __ 21 5 2.52.5 F Li olt we -rF Lrei oeldt wse H -r drei eogd swh H nd-ri oeg bwsh lo n-d cr eo bksw lso nd tce o kb pw l s_on t2c e 5bk p l _so 2tce 5kp s_t1ep_1
3.03.0 H Loig wh -H L- ror eieg wssh -d - rdr eoeo swsw nd nd ob o wbw lol non c c bk bk ls olso tct ec kek p p s _s_ t5t e2 0e p5p __ 52 05 3.03.0 F Li olt we -rF Lrei oeldt wse H -r drei eogd swh H nd-ri oeg bwsh lo n-d cr eo bksw lso nd tce o kb pw l s_on t5c e 0bk p l _so 5tce 0kp s_t2e5p_25
HighH-riegsh -dreosw nd obwlonc bk lsotcekp s_t5e0p_50 FilterFeildte Hreigdh H-riegsh -dreosw nd obwlonc bk lsotcekp s_t5e0p_50
3.53.50 0 0.20.2 0.40.4 0.60.6 0.80.8 3.53.50 0 0.20.2 0.40.4 0.60.6 0.80.8
FreqFureenqcuyency FreqFureenqcuyency
0.00.0 MidMdlied dBlleo cBkloscks 0.00.0 MidMdlied dBlleo cBkloscks
0.50.5 0.50.5
1.01.0 1.01.0 Only dilated convolution 1.51.5 1.51.5 Low-Lroews -mreisd mbliodc bk lsotcekp s_t1ep_1 Low-Lroews -mreisd mbliodc bk lsotcekp s_t1ep_1 2.02.0 HighH-riegsh -mreisd mbliodc bk lsotcekp s_t1ep_1 2.02.0 FilterFeildte Hreigdh H-riegsh -mreisd mbliodc bk lsotcekp s_t1ep_1 Low-Lroews -mreisd mbliodc bk lsotcekp s_t2e5p_25 Low-Lroews -mreisd mbliodc bk lsotcekp s_t2e5p_25
2.52.5 H Loig wh -H L- ror eieg wssh -m - rmr ee isdisd m b mb liol diod c c bk bk ls olso tct ec kek p p s _s_ t5t e2 0e p5p __ 52 05 2.52.5 F Li olt we -rF Lrei oeldt wse H -r mrei egd isdh H m- bri leg iodsh c - bkmr le soisd tce kmb p l s_iod t5c e 0bk p l _so 5tce 0kp s_t2e5p_25
HighH-riegsh -mreisd mbliodc bk lsotcekp s_t5e0p_50 FilterFeildte Hreigdh H-riegsh -mreisd mbliodc bk lsotcekp s_t5e0p_50
3.03.00 0 0.20.2 0.40.4 0.60.6 0.80.8 3.03.00 0 0.20.2 0.40.4 0.60.6 0.80.8
FreqFureenqcuyency FreqFureenqcuyency
0 0 Up BUlpo cBkloscks 0 0 Up BUlpo cBkloscks
1 1 1 1
2 2 2 2 3 3 Low-Lroews -ureps b ulopc bk lsotcekp s_t1ep_1 3 3 Low-Lroews -ureps b ulopc bk lsotcekp s_t1ep_1 4 4 H Loig wh -H L- ror eieg wssh -u - rur epep s s b b u lu ol pop c c bk bk ls olso tct ec kek p p s _s_ t2t e1 5e pp __ 21 5 4 4 F Li olt we -rF Lrei oeldt wse H -r urei epgd s h b H u- lori peg c sh bk - lu sorep tces kb p ul s_op t2c e 5bk p l _so 2tce 5kp s_t1ep_1
HighH-riegsh -ureps b ulopc bk lsotcekp s_t2e5p_25 FilterFeildte Hreigdh H-riegsh -ureps b ulopc bk lsotcekp s_t2e5p_25
5 5 L Ho igw h-L H-ro re iegws sh -u -r urep eps sb bu l uo lp opc cbk bk ls o lsotc te ck ekp ps _ s_t5 te 50 ep 0p_ _5 50 0 5 5 L Fio ltw e- rL Fr eioe ldtws e H- rur eiep gds hb Hu -lo rip egc shbk - l uso reptce s k bp uls_ opt5 ce 0 bkp l_ so5 tce0 kp s_t5e0p_50
(D aila )ted V co in sv uol aut lio cn o+ mLow p-p aa rss i sfil ote nr s 0 0 (b)0.20 w.2 ith0 o.4 (a0 u)F .4 r w(eat/q)oFu r w ee fn i/q lfioc tuy e e frn0 ililc. nt6 y etgr0 ie. n6 gring0.80.8 0 0 0.2 (0 c.2 ) w0.4 i0 (tF b.4 r )e h q(wF bur e)e f n iq wlfic tu ey e frn i0 lilc n. t6 ey tgr0 ien.6 gring0.80.8
Fig.4: (a) Visual comparisons between the images created at a resolution of 2048×
2048:withonlythedilatedconvolution,andwithboththedilatedconvolutionandthe
low-pass filtering. (b)(c) Fourier relative log amplitudes of input features from three
distinct layers from the down blocks, mid blocks, and up blocks of UNet, respectively,
are analyzed. We also include features at reverse steps 1, 25, and 50. (b) Without the
applicationofthelow-passfilter.Thereisanevidentdistributiongapofthefrequency
spectrum between the low resolution and high resolution. (c) With the application of
the low-pass filter. The distribution gap is largely reduced.
issues. This phenomenon is because of the aliasing effect after the spatial down-
sampling,whichraisesthedistributiongapbetweenthefeaturesoflowresolution
and the features down-sampled from high resolution, as presented in Fig. 4b.
Aliasing alters the fundamental frequency components of the original signal,
breaking its consistency across scales.
In this paper, we introduce a low-pass filtering operation, or spectral pool-
ing[35]toremovehigh-frequencycomponentsthatmightcausealiasing,intend-
ing to construct scale consistency across different resolutions. Let F(m,n) be a
two-dimensionaldiscretesignalwithresolutionM×N.Spatialdown-samplingof
F(m,n),byfactorss ands alongtheheightandwidthrespectively,altersthe
h w
NyquistlimitstoM/(2s )andN/(2s )inthefrequencydomain,corresponding
h w
tohalfthenewsamplingratesalongeachdimension.Theexpectedlow-passfilter
should remove frequencies above these new Nyquist limits to prevent aliasing.
Therefore, the optimal mask size (assuming the frequency spectrum is central-
ized) for passing low frequencies in a low-pass filter is M/s ×N/s . This filter
h w
designensuresthepreservationofallvaluablefrequencieswithinthedownscaled
resolution while preventing aliasing by filtering out higher frequencies.
ee dd uu tt ii ll pp mm aa gg oo LL ee vv ii tt aa ll ee RR
ee dd uu tt ii ll pp mm aa gg oo LL ee vv ii tt aa ll ee RR
ee dd uu tt ii ll pp mm aa gg oo LL ee vv ii tt aa ll ee RR
ee dd uu tt ii ll pp mm aa gg oo LL ee vv ii tt aa ll ee RR
ee dd uu tt ii ll pp mm aa gg oo LL ee vv ii tt aa ll ee RR
ee dd uu tt ii ll pp mm aa gg oo LL ee vv ii tt aa ll ee RRFouriScale 9
Algorithm 1 Pseudo-code of FouriScale
Data: Input: F ∈RC×Hf×Wf. Original size: h
f
×w f.
Result: Output: F
conv
∈RC×Hf×Wf
r=max(⌈Hf⌉,⌈Wf⌉)
hf wf
F
pad
←Zero-Pad(F)∈RC×rhf×rwf ▷ Zero Padding
F
dft
←DFT(F pad)∈CC×rhf×rwf ▷ Discrete Fourier transform
F ←H⊙F ▷ Low pass filtering
low dft
F ←iDFT(F ) ▷ Inverse Fourier transform
idft low
F
crop
←Crop(F idft)∈RR×Hf×Wf ▷ Cropping
F
conv
←Conv k′(F crop) ▷ Dilation factor of k′ is r
As illustrated in Fig. 4c, the application of the low-pass filter results in a
closeralignmentofthefrequencydistributionbetweenhighandlowresolutions.
This ensures that the left side of Eq. (3) produces a plausible image structure.
Additionally, since our target is to rectify the image structure, low-pass filtering
wouldnotbeharmfulbecauseitgenerallypreservesthestructuralinformationof
asignal,whichpredominantlyresidesinthelowerfrequencycomponents[30,48].
Subsequently,thefinalkernelk∗ isobtainedbyapplyinglow-passfilteringto
thedilatedkernel.ConsideringtheperiodicnatureoftheFourierspectrumasso-
ciatedwiththedilatedkernel,theFourierspectrumofthenewkernelk∗ involves
expanding the spectrum of the original kernel k by inserting zero frequencies.
Therefore, this expansion avoids the introduction of new frequency components
into the new kernel k∗. In practice, we do not directly calculate the kernel k∗
butreplacetheoriginalConv withthefollowingequivalentoperationtoensure
k
computational efficiency:
Conv (F)→Conv (iDFT(H ⊙DFT(F)), (8)
k k′
where H denotes the low-pass filter. Fig. 4a (bottom left) illustrates that the
combination of dilated convolution and low-pass filtering resolves the issue of
pattern repetition.
3.4 Adaption to Arbitrary-size Generation
The derived conclusion is applicable only when the aspect ratios of the high-
resolution image and the low-resolution image used in training are identical.
From Eq. (5) and Eq. (6), it becomes apparent that when the aspect ratios
vary, meaning the dilation rates along the height and width are different, the
well-constructed structure in the low-resolution image would be distorted and
compressed, as shown in Fig. 5 (a). Nonetheless, in real-world applications, the
ideal scenario is for a pre-trained diffusion model to have the capability of gen-
erating arbitrary-size images.
We introduce a straightforward yet efficient approach, termed padding-then-
cropping, to solve this problem. Fig. 5 (b) demonstrates its effectiveness. In
essence,whenalayerreceivesaninputfeatureatastandardresolutionofh ×w ,
f f10 L. Huang et al.
“A car in a garden, with a lake and Eiffel Tower”
(a) w/o padding-then-cropping (b) w padding-then-cropping
Fig.5: Visualcomparisonsbetweentheimagescreatedataresolutionof2048×1024:
(a)withouttheapplicationofpadding-then-croppingstrategy,and(b)withtheappli-
cation of padding-then-cropping strategy. The Stable Diffusion 2.1 utilized is initially
trained on images of 512×512 resolution.
and this input feature increases to a size of H ×W during inference, our first
f f
step is to zero-pad the input feature to a size of rh ×rw . Here, r is defined
f f
as the maximum of ⌈Hf⌉ and ⌈Wf⌉, with ⌈·⌉ representing the ceiling operation.
hf wf
Thepaddingoperationassumesthatweaimtogenerateanimageofsizerh×rw,
wherecertainareasarefilledwithzeros.Subsequently,weapplyEq.(8)torectify
the issue of repetitive patterns in the higher-resolution output. Ultimately, the
obtained feature is cropped to restore its intended spatial size. This step is
necessary to not only negate the effects of zero-padding but also control the
computationaldemandswhentheresolutionincreases,particularlythosearising
from the self-attention layers in the UNet architecture. Taking computational
efficiency into account, our equivalent solution is outlined in Algorithm 1.
3.5 FouriScale Guidance
FouriScale effectively mitigates structural distortion when generating high-res
images.However,itwouldintroducecertainartifactsandunforeseenpatternsin
the background, as depicted in Fig. 6 (b). Based on our empirical findings, we
identifythatthemainissuestemsfromtheapplicationoflow-passfilteringwhen
generatingtheconditionalestimationinclassifier-freeguidance[21].Thisprocess
often leads to a ringing effect and loss of detail. To improve image quality and
reduceartifacts,asshowninFig.6(a),wedevelopaguidedversionofFouriScale
for reference, aiming to align the output, rich in details, with it. Specifically,
beyond the unconditional and conditional estimations derived from the UNet
modified by FouriScale, we further generate an extra conditional estimation.
This one is subjected to identical dilated convolutions but utilizes milder low-
pass filters to accommodate more frequencies. We substitute its attention maps
ofattentionlayerswiththosefromtheconditionalestimationprocessedthrough
FouriScale, in a similar spirit with image editing [6,12,17]. Given that UNet’s
attention maps hold a wealth of positional and structural information [44,45,
49], this strategy allows for the incorporation of correct structural information
derived from FouriScale to guide the generation, simultaneously mitigating theFouriScale 11
Positive condition 𝑐 " Denoising Net
𝜀 !(") FouriScale Milder Low-pass filter 𝜀!(𝑥",𝑐#)
Positive condition𝑐 " Replace Attention Scores C eo stn id mit ai to in oa nl
𝜀 !(") FouriScale Low-pass filter CFG
Negative
o
c ron ∅dition𝑐 ! Un ec sto in md ait ti io on nal
𝑥 ! 𝜀 !(") FouriScale Low-pass filter 𝜀!(𝑥",𝑐$) 𝑥 !&’
(a) Overview of FouriScaleGuidance
“Two little dogs looking a large pizza sitting on a table”
(b) w/o FouriScaleGuidance (c) w FouriScaleGuidance
Fig.6: (a) Overview of FouriScale guidance. CFG denotes Classifier-Free Guidance.
(b)(c) Visual comparisons between the images created at 2048×2048 by SD 2.1: (b)
withouttheapplicationofFouriScaleguidance,➊hasunexpectedartifactsintheback-
ground, ➋➌ are wrong details, (c) with the application of FouriScale guidance.
declineinimagequalityandlossofdetailstypicallyinducedbylow-passfiltering.
The final noise estimation is determined using both the unconditional and the
newlyconditionalestimationsfollowingclassifier-freeguidance.Aswecanseein
Fig. 6 (c), the aforementioned issues are largely mitigated.
3.6 Detailed Designs
Annealing dilation and filtering. Since the image structure is primarily outlined
in the early reverse steps, the subsequent steps focus on enhancing the details,
weimplementanannealingapproachforbothdilationconvolutionandlow-pass
filtering. Initially, for the first S steps, we employ the ideal dilation convolu-
init
tionandlow-passfiltering.DuringthespanfromS toS ,weprogressively
init stop
decrease the dilation factor and r (as detailed in Algorithm 1) down to 1. After
S steps, the original UNet is utilized to refine image details further.
stop
Settings for SDXL. Stable Diffusion XL [32] (SDXL) is generally trained on im-
ageswitharesolutioncloseto1024×1024pixels,accommodatingvariousaspect
ratios simultaneously. Our observations reveal that using an ideal low-pass filter
leadstosuboptimaloutcomesforSDXL.Instead,agentlerlow-passfilter,which
modulates rather than completely eliminates high-frequency elements using a
coefficient σ ∈ [0,1] (set to 0.6 in our method) delivers superior visual qual-
ity. This phenomenon can be attributed to SDXL’s ability to handle changes in12 L. Huang et al.
scale effectively, negating the need for an ideal low-pass filter to maintain scale
consistency, which confirms the rationale of incorporating low-pass filtering to
address scale variability. Additionally, for SDXL, we calculate the scale factor r
(refertoAlgorithm1)bydeterminingthetrainingresolutionwhoseaspectratio
is closest to the one of target resolution.
4 Experiments
Experimentalsetup. Wofollow[15]toreportresultsonthreetext-to-imagemod-
els,includingSD1.5[12],SD2.1[10],andSDXL1.0[32]ongeneratingimagesat
four higher resolutions. The resolutions tested are 4×, 6.25×, 8×, and 16× the
pixel count of their respective training resolutions. For both SD 1.5 and SD 2.1
models, the original training resolution is set at 512×512 pixels, while the infer-
enceresolutionsare1024×1024,1280×1280,2048×1024,and2048×2048.Inthe
case of the SDXL model, it is trained at resolutions close to 1024×1024 pixels,
with the higher inference resolutions being 2048×2048, 2560×2560, 4096×2048,
and 4096×4096. We default use FreeU [39] in all experimental settings.
Testing dataset and evaluation metrics. Following [15], we assess performance
using the Laion-5B dataset [38], which comprises 5 billion pairs of images and
their corresponding captions. For tests conducted at an inference resolution of
1024×1024, we select a subset of 30,000 images, each paired with randomly
chosen text prompts from the dataset. Given the substantial computational de-
mands, our sample size is reduced to 10,000 images for tests at inference resolu-
tions exceeding 1024×1024. We evaluate the quality and diversity of the gener-
atedimagesbymeasuringtheFrechetInceptionDistance(FID)[18]andKernel
InceptionDistance(KID)[4]betweengeneratedimagesandrealimages,denoted
as FID and KID . To show the methods’ capacity to preserve the pre-trained
r r
model’s original ability at a new resolution, we also follow [15] to evaluate the
metrics between the generated images at the base training resolution and the
inference resolution, denoted as FID and KID .
b b
4.1 Quantitative Results
Wecompareourmethodwiththevanillatext-to-imagediffusionmodel(Vanilla),
the training-free approach [25] (Attn-Entro) that accounts for variations in at-
tention entropy between low and high resolutions, and ScaleCrafter [15], which
modifies convolution kernels through re-dilation and adopts linear transforma-
tions for kernel enlargement. We show the experimental results in Tab. 1. Com-
pared to the vanilla diffusion models, our method obtains much better results
because of eliminating the issue of repetitive patterns. The Attn-Entro does
not work at high upscaling levels because it fails to fundamentally consider the
structural consistency across resolutions. Due to the absence of scale consis-
tency consideration in ScaleCrafter, it performs worse than our method on the
majority of metrics. Additionally, we observe that ScaleCrafter often strugglesFouriScale 13
Table1:Quantitativecomparisonsamongtraining-freemethods.Thebestandsecond
best results are highlighted in bold and underline. The values of KID and KID are
r b
scaled by 102.
SD1.5 SD2.1 SDXL1.0
Resolution Method
FID r↓KID r↓FID b↓KID b↓FID r↓KID r↓FID b↓KID b↓FID r↓KID r↓FID b↓KID b↓
Vanilla 26.96 1.00 15.72 0.42 29.90 1.11 19.21 0.54 49.81 1.84 32.90 0.92
Attn-Entro 26.78 0.97 15.64 0.42 29.65 1.10 19.17 0.54 49.72 1.84 32.86 0.92
4×1:1
ScaleCrafter 23.90 0.95 11.83 0.32 25.19 0.98 13.88 0.40 49.46 1.73 36.22 1.07
Ours 23.62 0.92 10.62 0.29 25.17 0.98 13.57 0.40 33.89 1.21 20.10 0.47
Vanilla 41.04 1.28 31.47 0.77 45.81 1.52 37.80 1.04 68.87 2.79 54.34 1.92
Attn-Entro 40.69 1.31 31.25 0.76 45.77 1.51 37.75 1.04 68.50 2.76 54.07 1.91
6.25×1:1
ScaleCrafter 37.71 1.34 25.54 0.67 35.13 1.14 23.68 0.57 55.03 2.02 45.58 1.49
Ours 30.27 1.00 16.71 0.34 30.82 1.01 18.34 0.42 44.13 1.64 37.09 1.16
Vanilla 50.91 1.87 44.65 1.45 57.80 2.26 51.97 1.81 90.23 4.20 79.32 3.42
Attn-Entro 50.72 1.86 44.49 1.44 57.42 2.26 51.67 1.80 89.87 4.15 79.00 3.40
8×1:2
ScaleCrafter 35.11 1.22 29.51 0.81 41.72 1.42 35.08 1.01 106.57 5.15 108.67 5.23
Ours 35.04 1.19 26.55 0.72 37.19 1.29 27.69 0.74 71.77 2.79 70.70 2.65
Vanilla 67.90 2.37 66.49 2.18 84.01 3.28 82.25 3.05 116.40 5.45 109.19 4.84
Attn-Entro 67.45 2.35 66.16 2.17 83.68 3.30 81.98 3.04 113.25 5.44 106.34 4.81
16×1:1
ScaleCrafter 32.00 1.01 27.08 0.71 40.91 1.32 33.23 0.90 84.58 3.53 85.91 3.39
Ours 30.84 0.95 23.29 0.57 39.49 1.27 28.14 0.73 56.66 2.18 49.59 1.63
to produce acceptable images for SDXL, leading to much lower performance
than ours. Conversely, our method is capable of generating images with plausi-
ble structures and rich details at various high resolutions, compatible with any
pre-trained diffusion models.
Furthermore, our method achieves better inference speed compared with
ScaleCrafter [15]. For example, under the 16× setting for SDXL, ScaleCrafter
takes an average of 577 seconds to generate an image, whereas our method,
employing a single NVIDIA A100 GPU, averages 540 seconds per image.
4.2 Qualitative Results
Fig. 7 presents a comprehensive visual comparison across various upscaling fac-
tors (4×, 8×, and 16×) with different pre-trained diffusion models (SD 1.5,
2.1, and SDXL 1.0). Our method demonstrates superior performance in pre-
servingstructuralintegrityandfidelitycomparedtoScaleCrafter[15]andAttn-
Entro [25]. Besides, FouriScale maintains its strong performance across all three
pre-trainedmodels,demonstratingitsbroadapplicabilityandrobustness.At4×
upscaling,FouriScalefaithfullyreconstructsfinedetailsliketheintricatepatterns
on the facial features of the portrait, and textures of the castle architecture. In
contrast, ScaleCrafter and Attn-Entro often exhibit blurring and loss of details.
As we move to more extreme 8× and 16× upscaling factors, the advantages of
FouriScale become even more pronounced. Our method consistently generates
images with coherent global structures and locally consistent textures across
diverse subjects, from natural elements to artistic renditions. The compared
methods still struggle with repetitive artifacts and distorted shapes.14 L. Huang et al.
4× 8× 16×
“Side-view blue-ice sneaker inspired by “The image is titled ‘Queen of the Robots’ “A painting of a koala wearing a princess
Spiderman created by WetaFX.” created by artists Greg Rutowski, Victo dress and crown, with a confetti background.”
Ngai, and Alphonse Mucha.”
“a castle is in the middle of a eurpean city” “A teddy bear mad scientist mixing “Portrait of an anime maid by Krenz Cushart,
chemicals depicted in oil painting style.” Alphonse Mucha, and Ilya Kuvshinov.”
“A watercolorportrait of a woman by Luke ”A nighstandtopped with a white land- “Two cats, grey and black, are wearing
Rueda Studios and David Downton.” line phone, remote control, a metallic steampunk attire and standing in front of
lamp, and a black hardcover book.” a ship in a heavily detailed painting.”
Fig.7:Visualcomparisonsbetween➊ours,➋ScaleCrafter[15]and➌Attn-Entro[25],
undersettingsof4×,8×,and16×,employingthreedistinctpre-traineddiffusionmod-
els: SD 1.5, SD 2.1, and SDXL 1.0.
4.3 Ablation Study
To validate the contributions of each component in our proposed method, we
conduct ablation studies on the SD 2.1 model generating 2048×2048 images.
First, we analyze the effect of using FouriScale Guidance as described in
Sec. 3.5. We compare the default FouriScale which utilizes guidance versus re-
moving the guidance and solely relying on the conditional estimation from the
FouriScale-modifiedUNet.AsshowninTab.2,employingguidanceimprovesthe
FID by 4.26, demonstrating its benefits for enhancing image quality. The guid-
r
ance allows incorporating structural information from the FouriScale-processed
estimation to guide the generation using a separate conditional estimation with
milderfiltering.Thisbalancesbetweenmaintainingstructuralintegrityandpre-
venting loss of details.
Furthermore, we analyze the effect of the low-pass filtering operation de-
scribed in Sec. 3.3. Using the FouriScale without guidance as the baseline, we
additionally remove the low-pass filtering from all modules. As shown in Tab. 2,
this further deteriorates the FID to 46.74. The low-pass filtering is crucial for
r
5.1
DS
1.2
DS
LX
DSFouriScale 15
“A tall giraffe in a zoo eating branches”
Method FID
r
FouriScale 39.49
w/o guidance 43.75
w/o guidance & filtering 46.74
Mask Size:𝑀/2×𝑁/2 Mask Size:𝑀/4×𝑁/4(Ours) Mask Size:𝑀/6×𝑁/6
Table 2: Ablation studies on Fig.8: Comparison of mask sizes for passing low
FouriScale components on SD 2.1 frequenciesgenerating20482imagesbySD2.1.M,
model under 16× 1:1 setting. N denote height and width of target resolution.
maintaining scale consistency across resolutions and preventing aliasing effects
that introduce distortions. Without it, the image quality degrades significantly.
A visual result of comparing the mask sizes for passing low frequencies is
depictedinFig.8.TheexperimentutilizesSD2.1(trainedwith512×512images)
to generate images of 2048×2048 pixels, setting the default mask size to M/4×
N/4. We can find that the optimal visual result is achieved with our default
settings. As the low-pass filter changes, there is an evident deterioration in the
visual appearance of details, which underscores the validity of our method.
5 Conclusion and Limitation
We present FouriScale, a novel approach that enhances the generation of high-
resolution images from pre-trained diffusion models. By addressing key chal-
lenges such as repetitive patterns and structural distortions, FouriScale intro-
duces a training-free method based on frequency domain analysis, improving
structural and scale consistency across different resolutions by a dilation oper-
ation and a low-pass filtering operation. The incorporation of a padding-then-
cropping strategy and the application of FouriScale guidance enhance the flex-
ibility and quality of text-to-image generation, accommodating different aspect
ratios while maintaining structural integrity. FouriScale’s simplicity and adapt-
ability, avoiding any extensive pre-computation, set a new benchmark in the
field. FouriScale still faces challenges in generating ultra-high-resolution sam-
ples, such as 4096×4096 pixels, which typically exhibit unintended artifacts.
Additionally, its focus on operations within convolutions limits its applicability
to purely transformer-based diffusion models.16 L. Huang et al.
Appendix
A Proof
A.1 Proof of Theorem 1
Let’sconsiderf(x)asaone-dimensionalsignal.Itsdown-sampledcounterpartis
represented by f′(x)=Down (f). To understand the connection between f′(x)
s
and f(x), we base our analysis on their generated continuous signal g(x), which
is produced using a particular sampling function. It’s important to note that
the sampling function sa (x) is characterized by a series of infinitely spaced
∆T
impulse units, with each pair separated by intervals of ∆T:
∞
(cid:88)
sa(x,∆T)= δ(x−n∆T). (1)
n=−∞
Based on Eq. (1), f(x) and f′(x) can be formulated as
f(x)=g(x)sa(x,∆T),
(2)
f′(x)=g(x)sa(x,s∆T).
Based on the Fourier transform and the convolution theorem, the spatial
sampling described above can be represented in the Fourier domain as follows:
F(u)=G(u)⊛SA(u,∆T)
(cid:90) ∞
= G(τ)SA(u−τ,∆T)dτ
−∞
1 (cid:88)(cid:90) ∞ (cid:16) n (cid:17)
= G(τ)δ u−τ − dτ (3)
∆T ∆T
n −∞
1 (cid:88) (cid:16) n (cid:17)
= G u− ,
∆T ∆T
n
where G(u) and SA(u,∆T) are the Fourier transform of g(x) and sa(x,∆T).
FromtheaboveEquation,itcanbeobservedthatthespatialsamplingintroduces
the periodicity to the spectrum and the period is 1 .
∆T
Note that the sampling rates of f(x) and f′(x) are Ω and Ω′, the relation-
x x
ship between them can be written as
1 1 1
Ω = , Ω′ = = Ω . (4)
x ∆T x s∆T s x
With the down-sampling process in consideration, we presume that f(x)
complies with the Nyquist sampling theorem, suggesting that u
max
< Ω 2x.FouriScale 17
Following down-sampling, as per the Nyquist sampling theorem, the entire
sub-frequency range is confined to (0,Ωx). The resulting frequency band is a
s
composite of s initial bands, expressed as:
F′(u)=S(F(u),F(u˜ ),...,F(u˜ )), (5)
1 s−1
where u˜ represents the frequencies higher than the sampling rate, while u de-
i
notesthefrequenciesthatarelowerthanthesamplingrate.ThesymbolSstands
for the superposition operator. To simplify the discussion, u˜ will be used to de-
note u˜ in subsequent sections.
i
(1) In the sub-band, where u∈(0,Ωx), u˜ should satisfy
2s
(cid:18) (cid:19)
Ω
u˜∈ x,u . (6)
2s max
Accordingtothealiasingtheorem,thehighfrequencyu˜isfoldedbacktothe
low frequency:
uˆ=(cid:12) (cid:12) (cid:12)u˜−(k+1)Ω x′ (cid:12) (cid:12)
(cid:12),
kΩ x′ ≤u˜≤(k+2)Ω x′
(7)
(cid:12) 2 (cid:12) 2 2
where k =1,3,5,... and uˆ is folded results by u˜.
According to Eq. 6 and Eq. 7, we have
(cid:18) (cid:19)
aΩ Ω Ω
uˆ= x −u˜ and uˆ∈ x −u , x , (8)
s s max 2s
where a=(k+1)/2=1,2,.... According to Eq. (5) and Eq. (8), we can attain
(cid:40)
F′(u)=
F(u) if u∈(0,Ω sx −u max),
(9)
S(F(u),F(aΩ sx −u)) if u∈(Ω sx −u max,Ω 2sx).
According to Eq. (3), F(u) is symmetric with respect to u= Ωx:
2
Ω Ω
F( x −u)=F(u+ x). (10)
2 2
Therefore, we can rewrite F(aΩx −u) as:
s
Ω Ω aΩ
F( x −( x +u− x))
2 2 s
Ω Ω aΩ
=F( x +( x +u− x))
2 2 s
(11)
aΩ
=F(u+Ω − x)
x s
aΩ
=F(u+ x)
s18 L. Huang et al.
sincea=1,2,...,s−1.Additionally,fors=2,theconditionu∈(0,Ω sx−u max)
results in F(u+ Ω sx) = 0. When s > 2, the range u ∈ (0,Ω sx −u max) typically
becomes non-existent. Thus, in light of Eq. (11) and the preceding analysis,
Eq. (9) can be reformulated as
aΩ Ω
F′(u)=S(F(u),F(u+ x))|u∈(0, x). (12)
s 2s
(2) In the sub-band, where u∈(Ωx,Ωx), different from (1), u˜ should satisfy
2s s
Ω Ω
u˜∈( x −u , x). (13)
s max 2s
Similarly, we can obtain:
aΩ Ω Ω
F′(u)=S(F(u˜),F(u+ x))|u∈( x, x). (14)
s 2s s
Combining Eq. (12) and Eq. (14), we obtain
aΩ Ω
F′(u)=S(F(u),F(u+ x))|u∈(0, x), (15)
s s
where a=1,2,...,s−1.
A.2 Proof of Lemma 1
Based on Eq. (3), it can be determined that the amplitude of F′ is 1 times that
s
of F. Hence, F′(u) can be expressed as:
(cid:18) (cid:19) (cid:18) (cid:19)
F′(u)=
1 F(u)+(cid:88)1
F u+
aΩ
x |u∈
0,Ω
x . (16)
s s s s
a
Based on the dual principle, we can prove F′(u,v) in the whole sub-band
 
s−1 (cid:18) (cid:19)
F′(u,v)= 1 (cid:88) F u+ aΩ s,v+ bΩ y , (17)
s2 s s
a,b=0
where
u∈(cid:0) 0,Ωx(cid:1)
, v
∈(cid:16) 0,Ωy(cid:17)
.
s s
B Implementation Details
B.1 Low-pass Filter Definition
In Fig. 1, we show the design of a low-pass filter used in FouriScale. Inspired
by [34,41], we define the low-pass filter as the outer product between two 1D
filters (depicted in the left of Fig. 1), one along the height dimension and one
alongthewidthdimension.Wedefinethefunctionofthe1DfilterfortheheightFouriScale 19
1
High-frequency
Coefficient𝜎
Low-pass Smooth
Region Region
0
𝐻 𝐻 𝐻
𝑊=64 𝐻=64 𝑠"=4 𝑠!=4
2𝑠!
2𝑠!+𝑅!
2 𝑅"=8 𝑅!=8 𝜎=0
Fig.1:Visualizationofthedesignofalow-passfilter.(a)1Dfilterforthepositiveaxis.
(2)2Dlow-passfilter,whichisconstructedbymirroringthe1Dfiltersandperforming
an outer product between two 1D filters, in accordance with the settings of the 1D
filter.
dimensionasfollows,filtersforthewidthdimensioncanbeobtainedinthesame
way:
(cid:18) (cid:18) (cid:18) (cid:19) (cid:19) (cid:19)
1−σ H H
maskh =min max +1−i +1,σ ,1 ,i∈[0, ], (18)
(sh,Rh,σ) R s 2
h h
where s denotes the down-sampling factor between the target and original res-
h
olutionsalongtheheightdimension.R controlsthesmoothnessofthefilterand
h
σ isthemodulationcoefficientforhighfrequencies.Exploitingthecharacteristic
of conjugate symmetry of the spectrum, we only consider the positive axis, the
whole 1D filter can be obtained by mirroring the 1D filter. We build the 2D
low-pass filter as the outer product between the two 1D filters:
mask(s ,s ,R ,R ,σ)=maskh ⊗maskw , (19)
h w h w (sh,Rh,σ) (sw,Rw,σ)
where ⊗ denotes the outer product operation. Likewise, the whole 2D filter can
be obtained by mirroring along the height and width axes. A toy example of a
2D low-pass filter is shown in the right of Fig. 1.
B.2 Hyper-parameter Settings
In this section, we detail our choice of hyperparameters. The evaluative param-
eters are detailed in Tab. 1. Additionally, Fig. 2 provides a visual guide of the
precise positioning of various blocks within the U-Net architecture employed in
our model.
The dilation factor used in each FouriScale layer is determined by the max-
imum value of the height and width scale relative to the original resolution. As
stated in our main manuscript, we employ an annealing strategy. For the first20 L. Huang et al.
1× 1×
1 1
× ×
4 1 1 4
× ×
16 1 1 1 16
× × ×
64 64 64
DB0 DB1 DB2 DB3 MB UB0 UB1 UB2 UB3
Fig.2: Reference block names of stable diffusion in the following experiment details.
S steps,weemploytheidealdilationconvolutionandlow-passfiltering.Dur-
init
ingthespanfromS toS ,weprogressivelydecreasethedilationfactorand
init stop
r (as detailed in Algorithm 1 of our main manuscript) down to 1. After S
stop
steps, the original UNet is utilized to refine image details further. The settings
for S and S are shown in Tab. 1.
init stop
Table 1: Experiment settings for SD 1.5, SD 2.1, and SDXL 1.0.
Params SD 1.5 & SD 2.1 SDXL 1.0
FouriScale blocks [DB2,DB3,MB,UB0,UB1,UB2][DB2,MB,UB0,UB1]
inference timesteps 50 50
[10,30] (4×1:1 and 6.25×1:1)
[S , S ] [20,35]
init stop [20,35] (8×1:2 and 16×1:1)
C More Experiments
C.1 Comparison with Diffusion Super-Resolution Method
Inthissection,wecomparetheperformanceofourproposedmethodwithacas-
caded pipeline, which uses SD 2.1 to generate images at the default resolution
of 512×512, and upscale them to 2048×2048 by a pre-trained diffusion super-
resolution model, specifically the Stable Diffusion Upscaler-4× [43]. We apply
this super-resolution model to a set of 10,000 images generated by SD 2.1. We
then evaluate the FID and KID scores of these upscaled images and compare
r r
them with images generated at 2048×2048 resolution using SD 2.1 equipped
withourFouriScale.TheresultsofthiscomparisonarepresentedinTab.2.Our
method obtains somewhat worse results than the cascaded method. However,
our method is capable of generating high-resolution images in only one stage,
without the need for a multi-stage process. Besides, our method does not needFouriScale 21
Fig.3: Visual comparison with SD+SR. Left: 2048×2048 image upscaled by SD+SR
from 512×512 SD 2.1 generated image. Right: 2048×2048 image generated by our
FouriScale with SD 2.1.
Table2:ComparisonwithSD+ Table 3: Comparison with ElasticDiffusion
Super-Resolution. on the SDXL 2048×2048 setting.
Method FID KID Method FID KID FID KID
r r r r b b
SD+Super Resolution25.94 0.91 ElasticDiffusion [14]52.02 3.03 40.46 2.22
Ours 39.49 1.27 Ours 33.89 1.21 20.10 0.47
model re-training, while the SR model demands extensive data and computa-
tional resources for training. More importantly, as shown in Fig. 3, we find that
ourmethodcangeneratemuchbetterdetailsthanthecascadedpipeline.Dueto
alackofpriorknowledgeingeneration,super-resolutionmethodcanonlyutilize
existing knowledge within the single image for upscaling the image, resulting in
over-smooth appearance. However, our method can effectively upscale images
andfillindetailsusinggenerativepriorswithapre-traineddiffusionmodel,pro-
vidingvaluableinsightsforfutureexplorationsintothesynthesisofhigh-quality
and ultra-high-resolution images.
C.2 Comparison with ElasticDiffusion
We observe that the recent approach, ElasticDiffusion [14], has established a
technique to equip pre-trained diffusion models with the capability to generate
imagesofarbitrarysizes,bothsmallerandlargerthantheresolutionusedduring
training.Here,weprovidethecomparisonwithElasticDiffusion[14]ontheSDXL
2048×2048setting.TheresultsareshowninTab3.First,it’simportanttonote
that the inference times for ElasticDiffusion are approximately 4 to 5 times
longer than ours. Besides, as we can see, our method demonstrates superior22 L. Huang et al.
4096×4096 2048×2048 1024×1024 2048×4096
1024×2048 1024×1024
2048×2048 2048×2048
Fig.4:Visualizationofthehigh-resolutionimagesgeneratedbySD2.1integratedwith
customized LoRAs (images in red rectangle) and images generated by a personalized
diffusion model, AnimeArtXL [1], which is based on SDXL.
performance across all evaluation metrics, achieving lower FID and KID scores
compared to ElasticDiffusion, indicating better image quality and diversity.
D More Visualizations
D.1 LoRAs
In Fig. 4, we present the high-resolution images produced by SD 2.1, which has
been integrated with customized LoRAs [23] from Civitai [8]. We can see that
ourmethodcanbeeffectivelyappliedtodiffusionmodelsequippedwithLoRAs.
D.2 Other Resolutions
In Fig. 5, we present more images generated at different resolutions by SD 2.1,
aside from the 4×, 6.25×, 8×, and 16× settings. Our approach is capable of
generating high-quality images of arbitrary aspect ratios and sizes.
References
1. AnimeArtXL: (2024), https://civitai.com/models/117259/anime-art-
diffusion-xl, accessed: 17, 01, 2024 22
2. Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila,
T., Laine, S., Catanzaro, B., et al.: ediffi: Text-to-image diffusion models with an
ensemble of expert denoisers. arXiv preprint arXiv:2211.01324 (2022) 3
3. Bar-Tal,O.,Yariv,L.,Lipman,Y.,Dekel,T.:Multidiffusion:Fusingdiffusionpaths
for controlled image generation. arXiv preprint arXiv:2302.08113 (2023) 2, 3
4. Bińkowski,M.,Sutherland,D.J.,Arbel,M.,Gretton,A.:Demystifyingmmdgans.
In: International Conference on Learning Representations (2018) 12FouriScale 23
1024×2048 512×1280 1536×1280
2048×768
3072×768
1920×1408 768×2816
1920×640
1152×768 768×768
Fig.5: MoregeneratedimagesusingFouriScaleandSD2.1witharbitraryresolutions.24 L. Huang et al.
5. Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim,S.W.,Fidler,S.,Kreis,
K.:Alignyourlatents:High-resolutionvideosynthesiswithlatentdiffusionmodels.
In: CVPR. pp. 22563–22575 (2023) 3
6. Cao,M.,Wang,X.,Qi,Z.,Shan,Y.,Qie,X.,Zheng,Y.:Masactrl:Tuning-freemu-
tualself-attentioncontrolforconsistentimagesynthesisandediting.arXivpreprint
arXiv:2304.08465 (2023) 10
7. Chen, T.: On the importance of noise scheduling for diffusion models. arXiv
preprint arXiv:2301.10972 (2023) 3
8. Civitai: (2024), https://civitai.com/, accessed: 17, 01, 2024 22
9. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS
34, 8780–8794 (2021) 3
10. Diffusion, S.: Stable diffusion 2-1 base. https://huggingface.co/stabilityai/
stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt (2022) 12
11. Ding,M.,Yang,Z.,Hong,W.,Zheng,W.,Zhou,C.,Yin,D.,Lin,J.,Zou,X.,Shao,
Z.,Yang,H.,etal.:Cogview:Masteringtext-to-imagegenerationviatransformers.
NeurIPS 34, 19822–19835 (2021) 1
12. Epstein,D.,Jabri,A.,Poole,B.,Efros,A.A.,Holynski,A.:Diffusionself-guidance
for controllable image generation. arXiv preprint arXiv:2306.00986 (2023) 10, 12
13. Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville, A., Bengio, Y.: Generative adversarial nets. NeurIPS 27 (2014) 1, 3
14. Haji-Ali, M., Balakrishnan, G., Ordonez, V.: Elasticdiffusion: Training-free arbi-
trary size image generation. arXiv preprint arXiv:2311.18822 (2023) 3, 21
15. He,Y.,Yang,S.,Chen,H.,Cun,X.,Xia,M.,Zhang,Y.,Wang,X.,He,R.,Chen,
Q., Shan, Y.: Scalecrafter: Tuning-free higher-resolution visual generation with
diffusion models. arXiv preprint arXiv:2310.07702 (2023) 2, 4, 5, 7, 12, 13, 14
16. He, Y., Yang, T., Zhang, Y., Shan, Y., Chen, Q.: Latent video diffusion mod-
els for high-fidelity video generation with arbitrary lengths. arXiv preprint
arXiv:2211.13221 (2022) 3
17. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-or, D.:
Prompt-to-promptimageeditingwithcross-attentioncontrol.In:ICLR(2022) 10
18. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
by a two time-scale update rule converge to a local nash equilibrium. NeurIPS 30
(2017) 12
19. Ho,J.,Jain,A.,Abbeel,P.:Denoisingdiffusionprobabilisticmodels.NeurIPS33,
6840–6851 (2020) 1, 3, 4
20. Ho,J.,Saharia,C.,Chan,W.,Fleet,D.J.,Norouzi,M.,Salimans,T.:Cascadeddif-
fusionmodelsforhighfidelityimagegeneration.TheJournalofMachineLearning
Research 23(1), 2249–2281 (2022) 3
21. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 (2022) 10
22. Hoogeboom, E., Heek, J., Salimans, T.: simple diffusion: End-to-end diffusion for
high resolution images. arXiv preprint arXiv:2301.11093 (2023) 3
23. Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.:
Lora:Low-rankadaptationoflargelanguagemodels.In:InternationalConference
on Learning Representations (2021) 22
24. Jiménez,Á.B.:Mixtureofdiffusersforscenecompositionandhighresolutionimage
generation. arXiv preprint arXiv:2302.02412 (2023) 2, 3
25. Jin, Z., Shen, X., Li, B., Xue, X.: Training-free diffusion model adaptation for
variable-sized text-to-image synthesis. arXiv preprint arXiv:2306.08645 (2023) 2,
3, 4, 12, 13, 14FouriScale 25
26. Lee, Y., Kim, K., Kim, H., Sung, M.: Syncdiffusion: Coherent montage via syn-
chronized joint diffusions. NeurIPS 36 (2024) 2, 3
27. Liu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D., Wang, W., Plumb-
ley,M.D.:Audioldm:Text-to-audiogenerationwithlatentdiffusionmodels.arXiv
preprint arXiv:2301.12503 (2023) 3
28. Lu, Z., Wang, Z., Huang, D., Wu, C., Liu, X., Ouyang, W., Bai, L.: Fit: Flexible
vision transformer for diffusion model. arXiv preprint arXiv:2402.12376 (2024) 3
29. Midjourney: (2024), https://www.midjourney.com, accessed: 17, 01, 2024 1
30. Pattichis, M.S., Bovik, A.C.: Analyzing image structure by multidimensional fre-
quency modulation. IEEE TPAMI 29(5), 753–766 (2007) 9
31. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: ICCV. pp.
4195–4205 (2023) 3
32. Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,T.,Müller,J.,Penna,
J.,Rombach,R.:Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXiv preprint arXiv:2307.01952 (2023) 1, 2, 3, 11, 12
33. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
Sutskever,I.:Zero-shottext-to-imagegeneration.In:ICML.pp.8821–8831.PMLR
(2021) 1
34. Riad,R.,Teboul,O.,Grangier,D.,Zeghidour,N.:Learningstridesinconvolutional
neural networks. In: ICLR (2021) 6, 18
35. Rippel,O.,Snoek,J.,Adams,R.P.:Spectralrepresentationsforconvolutionalneu-
ral networks. NeurIPS 28 (2015) 8
36. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR. pp. 10684–10695 (2022)
1, 3
37. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-imagediffusionmodelswithdeeplanguageunderstanding.NeurIPS35,36479–
36494 (2022) 1, 3
38. Schuhmann,C.,Beaumont,R.,Vencu,R.,Gordon,C.,Wightman,R.,Cherti,M.,
Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An open
large-scale dataset for training next generation image-text models (2022) 12
39. Si, C., Huang, Z., Jiang, Y., Liu, Z.: Freeu: Free lunch in diffusion u-net. arXiv
preprint arXiv:2309.11497 (2023) 12
40. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.In:International
Conference on Learning Representations (2020) 3, 4
41. Sukhbaatar, S., Grave, É., Bojanowski, P., Joulin, A.: Adaptive attention span in
transformers. In: Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics. pp. 331–335 (2019) 18
42. Teng, J., Zheng, W., Ding, M., Hong, W., Wangni, J., Yang, Z., Tang, J.: Relay
diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv
preprint arXiv:2309.03350 (2023) 3
43. Upscaler, S.D.: (2024), https://huggingface.co/stabilityai/stable-
diffusion-x4-upscaler, accessed: 17, 01, 2024 20
44. Wang, J., Li, X., Zhang, J., Xu, Q., Zhou, Q., Yu, Q., Sheng, L., Xu, D.: Diffu-
sion model is secretly a training-free open vocabulary semantic segmenter. arXiv
preprint arXiv:2309.02773 (2023) 10
45. Xiao, C., Yang, Q., Zhou, F., Zhang, C.: From text to mask: Localizing en-
tities using the attention of text-to-image diffusion models. arXiv preprint
arXiv:2309.04109 (2023) 1026 L. Huang et al.
46. Zeng, X., Vahdat, A., Williams, F., Gojcic, Z., Litany, O., Fidler, S., Kreis,
K.: Lion: Latent point diffusion models for 3d shape generation. arXiv preprint
arXiv:2210.06978 (2022) 3
47. Zhang, R.: Making convolutional networks shift-invariant again. In: ICML. pp.
7324–7334. PMLR (2019) 6
48. Zhang,Y.,Li,K.,Li,K.,Wang,L.,Zhong,B.,Fu,Y.:Imagesuper-resolutionusing
very deep residual channel attention networks. In: ECCV. pp. 286–301 (2018) 9
49. Zhao, W., Rao, Y., Liu, Z., Liu, B., Zhou, J., Lu, J.: Unleashing text-to-image
diffusion models for visual perception. ICCV (2023) 10
50. Zheng, Q., Guo, Y., Deng, J., Han, J., Li, Y., Xu, S., Xu, H.: Any-size-diffusion:
Toward efficient text-driven synthesis for any-size hd images. arXiv preprint
arXiv:2308.16582 (2023) 3
51. Zhu,Q.,Zhou,M.,Huang,J.,Zheng,N.,Gao,H.,Li,C.,Xu,Y.,Zhao,F.:Fourid-
own:Factoringdown-samplingintoshufflingandsuperposing.In:NeurIPS(2023)
6