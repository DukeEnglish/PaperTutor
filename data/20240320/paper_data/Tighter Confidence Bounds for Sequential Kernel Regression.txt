Tighter Confidence Bounds for Sequential Kernel Regression
HamishFlynn1 DavidReeb2
Abstract 2020)andreinforcementlearning(Sutton&Barto,2018),
where they can be used to design exploration strategies
Confidenceboundsareanessentialtoolforrigor-
(Auer, 2002), establish stopping criteria (Jamieson et al.,
ouslyquantifyingtheuncertaintyofpredictions.
2014)andguaranteesafety(Suietal.,2015;Berkenkamp
Inthiscapacity,theycaninformtheexploration-
et al., 2017). In general, tighter confidence bounds give
exploitationtrade-offandformacorecomponent
risetosequentiallearninganddecision-makingalgorithms
inmanysequentiallearninganddecision-making
withbetterempiricalperformanceandbetterperformance
algorithms.Tighterconfidenceboundsgiveriseto
guarantees. As many modern algorithms use uncertainty
algorithmswithbetterempiricalperformanceand
estimates for flexible nonparametric function approxima-
betterperformanceguarantees. Inthiswork,we
tors, such as kernel methods, it is vital to establish tight usemartingaletailboundsandfinite-dimensional
confidenceboundsforinfinite-dimensionalfunctionclasses.
reformulationsofinfinite-dimensionalconvexpro-
gramstoestablishnewconfidenceboundsforse- In this work, we develop a new confidence sequence for
quentialkernelregression. Weprovethatournew sequentialkernelregression. Thecorrespondinguppercon-
confidenceboundsarealwaystighterthanexist- fidenceboundatanypointxisthesolutionofaninfinite-
ingonesinthissetting. Weapplyourconfidence dimensionalconvexprograminthekernelHilbertspace.We
boundstothekernelbanditproblem,wherefuture showthatthiscanbereformulatedusingeitherarepresenter
actions depend on the previous history. When theoremorLagrangianduality,yieldingfinite-dimensional
ourconfidenceboundsreplaceexistingones,the convexprograms,whichareefficientlysolvableandcertifi-
KernelUCB(GP-UCB)algorithmhasbetterem- able. Weproposeseveralmethodsforcomputingourcon-
piricalperformance,amatchingworst-caseperfor- fidencebounds,whichcanreplacetheconfidencebounds
manceguaranteeandcomparablecomputational usedinkernelbanditalgorithms,suchasKernelUCB(Valko
cost. Our new confidence bounds can be used etal.,2013). Wedemonstratethattheresultingkernelban-
asagenerictooltodesignimprovedalgorithms ditalgorithmshavebetterempiricalperformance,matching
forotherkernelisedlearninganddecision-making worst-caseperformanceguaranteesandcomparablecom-
problems. putationalcost. Finally,weprovethatournewconfidence
boundsarealwaystighterthanexistingconfidencebounds
inthesamesetting(seeFig.1).
1.Introduction
2.ProblemStatement
Aconfidencesequenceforanunknownground-truthfunc-
tion f∗ is a sequence of sets F ,F ,..., where each set
1 2 We consider a sequential kernel regression problem with
F contains all functions f that could plausibly be the
t covariatesx ∈X andresponsesy ∈R,where
ground-truthf∗,givenalldataavailableattimestept. For t t
any point x, the corresponding upper confidence bound, y =f∗(x )+ϵ .
t t t
max {f(x)},isthusthelargestvaluethatf∗(x)could
plausf i∈ bF lyt take,givenalldataavailableattimestept. f∗ ∈ H is an unknown function in a reproducing ker-
nelHilbertspace(RKHS)Handϵ ,ϵ ,... arenoisevari-
1 2
Theuncertaintyestimatesprovidedbyconfidencebounds
ables. Weallowthecovariatestobegeneratedinanarbi-
areexceptionallyusefulinsequentiallearninganddecision- trary sequential fashion, meaning each x can depend on
t
makingproblems,suchasbandits(Lattimore&Szepesva´ri, x ,y ,...,x ,y . WeassumethattheRKHSnormof
1 1 t−1 t−1
f∗isboundedbyB,i.e.∥f∗∥ ≤B,andthat,conditioned
1UniversitatPompeuFabra,Barcelona,Spain2BoschCenter H
forArtificialIntelligence,Renningen,Germany.Correspondence onx 1,y 1,...,x t−1,y t−1,x t,eachϵ tisσ-sub-Gaussian.
to:HamishFlynn<hamishedward.flynn@upf.edu>,DavidReeb
Ourfirstaimistoconstructaconfidencesequenceforthe
<david.reeb@de.bosch.com>.
functionf∗,whichwenowdefineformally. Foranylevel
Preprint.Underreview. δ ∈ (0,1], a (1−δ)-confidence sequence is a sequence
1
4202
raM
91
]LM.tats[
1v23721.3042:viXraTighterConfidenceBoundsforSequentialKernelRegression
Figure1.Tighterconfidenceboundsforsequentialkernelregression.TheupperandlowerconfidenceboundsusedbyourKernel
CMM-UCBmethod(left),theconfidenceboundsfrom(Abbasi-Yadkori,2012)(AY-GP-UCB)(middle-right),andtheconfidencebounds
usedbyImprovedGP-UCB(IGP-UCB)(Chowdhury&Gopalan,2017)(right)foraMate´rnkerneltestfunctionwithsmoothnessν =3/2
andlengthscaleℓ=0.5.OurKernelCMM-UCBmethod–anditsrelaxedversionsDMM-UCBandAMM-UCB–producesconfidence
boundsthatarevisiblyclosertotheground-truthfunction(dashedline)thanthoseofAY-GP-UCBandIGP-UCB(cf.alsoSec.6.1).
F ,F ,... ofsubsetsofH,suchthateachF canbecal- two RKHS functions f ,f ∈ H. We denote the ker-
1 2 t 1 2
culatedusingthedatax ,y ,...,x ,y andthesequence nel function by k(·,·). For a sequence of observations
1 1 t t
satisfiesthecoveragecondition x ,...,x , we define k (x) := [k(x,x ),...,k(x,x )]⊤
1 t t 1 t
and Φ := [k(x ,·),...,k(x ,·)]⊤. We let K :=
P x y1 1, ,x y22 ,, .. .. ..[∀t≥1: f∗ ∈F t] ≥ 1−δ. {k(x i,t x j)} i∈[t],j∈[1 t] =Φ tΦ⊤ t det notethekernelmatrit xand
V := (cid:80)t k(·,x )k(·,x )⊤ = Φ⊤Φ theempiricalco-
t s=1 s s t t
Inwords,thiscoverageconditionsaysthatwithhighprob- varianceoperator. Foraparameterα≥0,wedefine
ability, f∗ lies in F for all times t ≥ 1 simultaneously.
t µ (x) := k (x)⊤(K +αI)−1y , (2)
Wecalleachsetinaconfidencesequenceaconfidenceset. α,t t t t
Oursecondaimistoconstructconfidenceboundsforthe ρ2 (x) := k(x,x)−k (x)⊤(K +αI)−1k (x). (3)
α,t t t t
valueoff∗(x)ateveryx∈X. Foranylevelδ ∈(0,1],an
anytime-valid(1−δ)-upperconfidencebound(UCB)isa µ (x) is the kernel ridge estimate (evaluated at x) or,
α,t
sequenceu 1,u 2,... offunctionsonX,suchthateachu t equivalently, the predictive mean of a standard Gaussian
canbecalculatedusingthedatax 1,y 1,...,x t,y t andthe process(GP)posteriorwithnoiseα. ρ2 α,t(x)isthepredic-
sequencesatisfies tivevarianceofastandardGPposterior.
P x1,x2,...[∀t≥1,∀x∈X :f∗(x)≤u t(x)] ≥ 1−δ.
y1,y2,... 3.RelatedWork
Thisconditionsaysthatwithhighprobability,u (x)isan Severalconfidencesequences/boundshavebeenproposed
t
upper bound on f∗(x) for all t ≥ 1 and all x ∈ X si- for the sequential kernel regression problem that we con-
multaneously. The definition of an anytime-valid lower sider. The confidence bounds in (Srinivas et al., 2010)
confidencebound(LCB)isanalogous. Ifwealreadyhavea are perhaps the most well-known. The two most widely
confidencesequence,wecanuseittoconstructupperand used,whichwecompareoursagainst,weredevelopedby
lowerconfidencebounds. Inparticular,ifF ,F ,... isa Abbasi-Yadkorietal.(2011);Abbasi-Yadkori(2012)and
1 2
(1−δ)-confidencesequence,thenwithprobabilityatleast Chowdhury&Gopalan(2017),usingconcentrationinequal-
1−δ,forallt≥1andallx∈X,wehave itiesforself-normalisedprocesses(delaPen˜aetal.,2004;
2009). Subsequently,Durandetal.(2018)derivedempirical
min{f(x)} ≤ f∗(x) ≤ max{f(x)}. (1) Bernstein-type versions of these confidence bounds, and
f∈Ft f∈Ft
Whitehouse et al. (2023) showed that for certain kernels,
theseconfidenceboundscanbetightenedbyregularisingin
Therefore,aslongaswecansolvethemaximisation/min-
proportiontothesmoothnessofthekernel. Confidencese-
imisationproblemsin(1),wecancomputeanytime-valid
quencesforthissettinghavealsobeenderivedusingonline-
confidence bounds from a confidence sequence. We use
to-confidence-setconversions(Abbasi-Yadkorietal.,2012;
UCB (x) to denote the RHS of (1), i.e. UCB (x) :=
Ft Ft Abbasi-Yadkori,2012).
max {f(x)},andLCB (x)todenotetheLHSof(1).
f∈Ft Ft
WithapplicationstoBayesianoptimisationandkernelban-
Notation. Foranypositiveintegern,[n] := {1,...,n}. ditsinmind,Neiswanger&Ramdas(2021)andEmmeneg-
Weusematrixnotationtodenotetheinnerproductf⊤f := geretal.(2023)developedconfidencesequencesusingmar-
1 2
⟨f ,f ⟩ andouterproductf f⊤ := f ⟨f ,·⟩ between tingaletailboundsforsequentiallikelihoodratios(assum-
1 2 H 1 2 1 2 H
2TighterConfidenceBoundsforSequentialKernelRegression
ing Gaussian rather than sub-Gaussian noise). However, Theorem4.1(TailBoundforAdaptiveMartingaleMixtures
cumulativeregretboundsforKernelUCB-stylebanditalgo- (Theorem5.1of(Flynnetal.,2023)). Foranyδ ∈ (0,1),
rithms using these confidence bounds are not established anyadaptivesequenceofmixturedistributions(P |t∈N),
t
in(Neiswanger&Ramdas,2021),andtheregretboundin andanysequenceofpredictablerandomvariables(λ |t∈
t
(Emmeneggeretal.,2023)isworsethantheoneobtained N),itholdswithprobabilityatleast1−δthat
withtheconfidenceboundsin(Abbasi-Yadkori,2012)or
(cid:18) (cid:19)
withournewconfidencebounds. ∀t≥1: ln E [M (g ,λ )] ≤ ln(1/δ). (6)
t t t
g t∼Pt
Theworkmostcloselyrelatedtooursis(Flynnetal.,2023),
whichproposesnewconfidenceboundsforlinearbandits, We choose (D |t ∈ N) to be any filtration such
t
usingmartingaletailboundsandthemethodofmixtures.
that y and x are both D -measurable, e.g. D =
t t+1 t t
WhileourEq. (8)isasimplekernelgeneralisationoftheir
σ(x ,y ,...,x ,y ,x ). Focusing on linear bandits,
1 1 t t t+1
Eq.(5),derivingconfidenceboundsismoreinvolvedinour Flynn et al. (2023) choose Z (g ) = (g −ϕ(x )⊤θ∗)ϵ ,
t t t t t
kernelsetting,becausemax f∈Ft{f(x)}isnowaninfinite- whereϕ(·)⊤θ∗ isalinearrewardfunction,andshowthat
dimensionaloptimisationproblem. Intheirlinear(i.e.finite-
underthischoice,Eq.(6)reducestoaconvexconstraintfor
dimensional)setting,Flynnetal.(2023)boundtheregret thefinite-dimensionalparametervectorθ∗. Inourkernel
oftheiralgorithmsintermsofthefeaturevectordimension. setting,wechooseZ (g ) = (g −f∗(x ))ϵ ,leadingtoa
t t t t t
The feature dimension is generally infinite in the kernel convexconstraintforthefunctionf∗. AsZ (g )islinearin
t t
setting,soweinsteadboundtheregretintermsofthemaxi-
thenoisevariableϵ ,ψ (g ,λ )canbeupperboundedusing
t t t t
muminformationgain(Srinivasetal.,2010).
theσ-sub-Gaussianpropertyofϵ .
t
ψ (g ,λ ) ≤ 1σ2λ2(g −f∗(x ))2.
4.ConfidenceBoundsforKernelRegression t t t 2 t t t
Inthissection,weuseatailboundformartingalemixtures Usingthisupperbound,choosingλ ≡1/σ2,andrearrang-
t
from(Flynnetal.,2023)todeveloptighterconfidencese- ingEq.(6)(seeApp. A.1),weobtainanupperboundonthe
quencesandboundsforsequentialkernelregression. Euclideandistancebetweenthevectorofground-truthfunc-
tionvaluesf∗ :=[f∗(x ),...,f∗(x )]⊤andtheobserved
t 1 t
4.1.MartingaleMixtureTailBounds responsevectory :=[y ,...,y ]⊤.
t 1 t
WebeginbyrecallingthetailboundfromTheorem5.1of ∥f∗−y ∥2 ≤ 2σ2ln(1/δ)
t t 2
(Flynnetal.,2023)andthegeneralsettinginwhichitholds. (cid:18) (cid:20) (cid:18) 1 (cid:19)(cid:21)(cid:19) (7)
We choose a sequence of random functions (Z : R → −2σ2ln E exp − ∥g −y ∥2 .
R|t ∈ N) adapted to a filtration (D |t ∈ N),
at
sequence g t∼Pt
2σ2 t t 2
t
ofpredictable“guesses”(g ∈ R|t ∈ N),andasequence
t
ofpredictablerandomvariables(λ ∈ R|t ∈ N). Weuse Next,wechoosethemixturedistributionsP t =N(0,cK t),
t
resemblingazero-meanGaussianprocesswithcovariance
theshorthandg = [g ,...,g ]⊤ andλ = [λ ,...,λ ]⊤.
t 1 t t 1 t scaledbyc > 0. Thereareseveralreasonsforthischoice
Flynnetal.(2023)calladata-dependentsequenceofprob-
ability distributions (P |t ∈ N) an adaptive sequence ofP t. First,foranappropriatecovariancescalec>0,we
t
prove that the confidence bounds obtained from (7) with
of mixture distributions if: (a) P is a distribution over
t
g ∈ Rt; (b)P isD -measurable; (c)thedistributions P t =N(0,cK t)arealwaystighterthanthosein(Abbasi-
t t t−1 Yadkori,2012)and(Chowdhury&Gopalan,2017)(seeSec.
areconsistentinthesensethattheirmarginalscoincide,i.e.
(cid:82) 6.1). Second,anyGaussianP yieldsaconvenientclosed-
P (g )dg =P (g )foralltandg . Define t
t t t t−1 t−1 t−1 formexpressionfortheexpectedvaluein(7)(seeApp.A.2).
(cid:32) t (cid:33) Substitutingthisexpressioninto(7),weobtain
(cid:88)
M (g ,λ ):=exp λ Z (g )−ψ (g ,λ ) , (4)
t t t s s s s s s (cid:18) cK (cid:19)−1
s=1 ∥f∗−y ∥2 ≤ y⊤ I+ t y (8)
ψ (g ,λ ):=ln(E[exp(λ Z (g ))|D ]), (5) t t 2 t σ2 t
t t t t t t t−1 (cid:18) (cid:19)
cK 1
+σ2lndet I+ t +2σ2ln
σ2 δ
Flynnetal.(2023)showthatM (g ,λ )isanon-negative
t t t
martingale,andthat,duetoconditions(a)-(c)onthemix-
=:R2.
t
turedistributions,themixtureE [M (g ,λ )]isalsoa
g t∼Pt t t t
non-negativemartingale. ThismeansthatVille’sinequality This inequality states that with probability at least 1−δ,
(Ville, 1939) can be applied. Thm. 5.1 in (Flynn et al., ateverystept,thevectorofground-truthfunctionvalues
2023)providesatime-uniformtailboundforthemartingale f∗ lieswithinasphere ofradiusR aroundtheobserved
t t
mixtureE [M (g ,λ )]. responsevectory .
g t∼Pt t t t t
3TighterConfidenceBoundsforSequentialKernelRegression
4.2.ConfidenceSequences directly compute the numerical solution of (9). We now
describetwofinite-dimensionalreformulationsof(9).
We now describe how the tail bound in (8) can be used
to construct confidence sequences for f∗. Let f = As(9)containsaconstraintontheRKHSnormoff, the
t
[f(x ),...,f(x )]⊤denotethevaluesofanarbitraryfunc- (generalised) representer theorem (Kimeldorf & Wahba,
1 t
tionf ∈Hatthepointsx ,...,x . Since,withprobability 1971;Scho¨lkopfetal.,2001)canbeapplied. Thismeans
1 t
atleast1−δ,(8)holdsforallt≥1,thesetsoffunctions thatthesolutionof(9)canbeexpressedasafinitelinear
f ∈Hthat(ateacht)satisfyboth(8)and∥f∥
H
≤Bform combination of the form k t+1(x)⊤w, where k t+1(x) :=
a1−δconfidencesequenceforf∗. [k(x,x 1),...,k(x,x t),k(x,x)]⊤. Whenwesubstitutethe
functionalformf(·) = k (·)⊤w intotheobjectiveand
Lemma 4.2 (Confidence sequence). For any c > 0 and t+1
bothconstraints,weobtainthefinite-dimensionalconvex
δ ∈ (0,1],itholdswithprobabilityatleast1−δ thatfor
allt≥1simultaneously,thefunctionf∗liesintheset programinThm. 4.4. (seeApp. B.3).
Theorem4.4(RepresentertheoremforUCBcomputation).
(cid:26) (cid:12) (cid:27)
F t = f ∈H(cid:12) (cid:12) (cid:12)∥f t−y t∥ 2 ≤R t, ∥f∥ H ≤B . UCB Ft(x)equalsthesolutionoftheconvexprogram
max k (x)⊤w (10)
t+1
ThetwoconstraintsthatdefineF arebothellipsoidsinH, w∈Rt+1
t
meaningthatF tisanintersectionoftwoellipsoids. Incon- s.t. ∥K t,t+1w−y t∥ 2 ≤R t,
trast,mostexistingconfidencesetsforfunctionsinRKHS’s ∥L w∥ ≤B.
t+1 2
(e.g.(Abbasi-Yadkori,2012;Chowdhury&Gopalan,2017))
aresingleellipsoidscentredatthekernelridgeestimateµ α,t whereK t,t+1 := (K t,k t(x)) ∈ Rt×(t+1) denoteskernel
(seeEq.(2)),forsomeregularisationparameterα > 0. It matrixwithaddedlastcolumnk (x),andL isanyma-
t t+1
turnsoutthatbytakingaweightedsumoftheconstraintsin trixsatisfyingL⊤ L =K (e.g.therightCholesky
t+1 t+1 t+1
F t,wecanobtainasingle-ellipsoidconfidencesetF(cid:101)t ⊇F t, factorofK t+1).
whichisalsocentredatthekernelridgeestimate.
Corollary4.3. Foranyc>0,anyα>0andanyδ ∈(0,1], Eq. (10) is a (t+1)-dimensional second order cone pro-
it holds with probability at least 1−δ that for all t ≥ 1 gram,whichcanbesolvedefficientlyusingconicsolvers
simultaneously,f∗liesintheset from,forexample,theCVXPYlibrary(Diamond&Boyd,
2016). Toillustratethesimplicityofsolving(10)withsuch
F(cid:101)t =(cid:26) f ∈H(cid:12) (cid:12) (cid:12) (cid:12) (cid:13) (cid:13) (cid:13)(V t+αI H)1/2(f −µ α,t)(cid:13) (cid:13) (cid:13) H ≤R(cid:101)α,t(cid:27) , l si eb cr oa nri des r, ew foe rmp uro lav ti id oe n,sa wm ep fl oe cc uo sd oe nin thA ep Lp a. gE ra.1 n. giF anor do uu ar l
problemassociatedwith(9).
where
Theorem4.5(DualproblemforUCBcomputation). The
R(cid:101) α2
,t
:=R t2+αB2−y⊤
t
y t+y⊤
t
Φ t(V t+αI H)−1Φ⊤
t
y t. dualproblemassociatedwith(9)is
Theproof(inApp. B.2)involvesrearrangingtheinequality (cid:40) ρ2 (x) (cid:41)
∥f t−y t∥2 2+α∥f∥2
H
≤ R t2+αB2,
ηηm
21
>>in
00
µ η2/η1,t(x)− η2/ 4η η1 2,t +η 1R(cid:101) η2
2/η1,t
. (11)
intotheconstraintthatdefinesF(cid:101)t. Thislargerconfidence
sequencecanbemoreeasilycomparedtothosein(Abbasi- See (2) and (3) for the definitions of µ (x) and
Yadkori,2012;Chowdhury&Gopalan,2017),cf.Sec.6.1. ρ2 (x). We provide a proof in App. η B2 ./ 4η1 a,t nd focus
η2/η1,t
hereontheconsequencesofthisresult. Byweakduality,
4.3.ImplicitConfidenceBounds thesolutionof(11)isalwaysanupperboundonthesolution
oftheprimalproblemin(9). Wheneverthereexistsanf
Wenowturnourattentiontotheconfidenceboundsthatcan
thatliesintheinterioroftheellipsoidsthatdefineF ,asis
be obtained from our confidence sequences, and how we t
usuallythecase,thenstrongdualityholdsandthesolutions
cancomputethem. Westartbyconsideringtheexactupper
of(9)and(11)areactuallyequal. InApp. B.4,weshow
confidenceboundUCB (x) = max {f(x)},which
Ft f∈Ft that(11)canbefurtherreducedtoaone-dimensionalop-
canbewrittenas
timisationproblem. Usingthesubstitutionη = αη (for
2 1
max f(x) s.t. ∥f −y ∥ ≤R , ∥f∥ ≤B. (9) α>0),thesolutionof(11)isequalto
t t 2 t H
f∈H
(cid:40) (cid:41)
Theoptimisationvariablein(9)isafunctioninthepossibly min µ (x)+ R(cid:101) √α,tρ (x) . (12)
infinite-dimensionalRKHSH,whichmakesitdifficultto α>0 α,t α α,t
4TighterConfidenceBoundsforSequentialKernelRegression
The objective function in (12) is not always convex in α, differentiationmethods(Looketal.,2020;Blondeletal.,
thoughempiricalevidencesuggeststhatitisquasiconvex 2022). Gradient-basedmaximisationworkswellinpractice,
in α, and could therefore be solved using the bisection eventhoughtherearenorigorousapproximationguarantees
method. Sinceµ (x)isthepredictivemeanofastandard asUCB (x)is(ingeneral)anon-concavefunctionofx.
α,t Ft
Gaussianprocess(GP)posteriorandρ2 (x)isitspredictive ThesecondoptionistodiscretiseX andoptimiseoverthefi-
α,t
variance,theobjectivefunctionin(12)closelyresembles nitediscretisation(seee.g.Sec.3.1of(Li&Scarlett,2022)).
GP-basedconfidencebounds(e.g.(Srinivasetal.,2010)). Whenthekernelfunctionk(x,x′)isLipschitz(astheRBF
WecanthereforeinterpretourconfidenceboundUCB (x) and Mate´rn kernels are), then the discretisation error can
Ft
asaGP-basedconfidencebound,whichisminimisedwith becontrolled. However,thegridsizeforasufficientlyfine
respecttotheregularisationparameter. discretisationgrowsexponentiallyinthedimensiondofX,
sothisquicklybecomesimpractical.
4.4.ExplicitConfidenceBounds
5.ApplicationToKernelBandits
In this section, we focus on explicit confidence bounds.
These confidence bounds are mainly useful for deriving
To demonstrate the utility of our confidence bounds, we
worst-case regret bounds with explicit dependence on T
applythemtothestochastickernelbanditproblem.
(seeSection6.2),butarealsocheapertocomputethanthe
implicitconfidenceboundsintheprevioussubsection.From
5.1.StochasticKernelBandits
(12)intheprevioussubsection,wecanimmediatelyseethat
foranyfixedα>0,wehave AlearnerplaysagameoverasequenceofT rounds,where
T may not be known in advance. In each round t, the
UCB Ft(x) ≤ µ α,t(x)+
R(cid:101)
√α α,tρ α,t(x). (13) l ae nar an ce tr ioo nbs xerv ∈es Xan .ac Tt hio en les ae rt nX ert t⊆ henX rea cn ed ivm esus at c reh woo ars de
t t
It turns out that this upper bound on UCB Ft(x) is equal y ut nk= nof w∗ n(x ft u) n+ ctiϵ ot n. iT nh ae nr Rew Ka Hrd S.fu An sc bti eo fn orf e∗ ,w: eX as→ sumR eis tha an
t
totheconfidenceboundUCB F(cid:101)t(x),whereF(cid:101)tistheconfi-
f∗ has bounded RKHS norm, i.e. ∥f∗∥ ≤ B, and that
dencesetfromLemma4.3. H
ϵ ,ϵ ,...,ϵ areconditionallyσ-sub-Gaussian.
Corollary4.6(AnalyticUCB). Forallt≥1andx∈X, 1 2 T
Thegoalofthelearneristochooseasequenceofactions
UCB (x) ≤ µ (x)+
R(cid:101)
√α,tρ (x). (14) that minimisescumulative regret, which is thedifference
F(cid:101)t α,t α α,t between the total expected reward of the learner and the
optimalstrategy. Forasingleround,wedefinetheregretas
ThisstatementfollowsfromtheproofofThm.4.5. Inpar- r = f∗(x∗)−f∗(x ),wherex∗ = argmax {f∗(x)}.
ticular,onecanshowthattheRHSof(14)isthesolutionof At fterT rout nds,thect umulativert egretisr x =∈X (cid:80)t T r .
thedualproblemassociatedwithmax {f(x)}. Again, 1:T t=1 t
f∈F(cid:101)t
whenF(cid:101)t hasaninteriorpoint(asisusuallythecase),the 5.2.KernelUCBMetaAlgorithm
inequalityin(14)isactuallyanequality. Theinequalityin
Cor.4.6alsofollowsfromCauchy-Schwarz(App.B.5). In Algorithm 1, we describe a general recipe for a ker-
nelbanditalgorithm,whichisbasedonasequenceofup-
perconfidenceboundsu ,u ,u ,... fortherewardfunc-
4.5.ConfidenceBoundMaximisation 0 1 2
tionf∗. Ineachroundt,weuseallpreviousobservations
Wenowdiscusshowourupperconfidenceboundscanbe {(x ,y )}t−1 tosuccessivelyconstructtheUCBu ,and
s s s=1 t−1
maximisedw.r.t.x ∈ X,whichisanimportantconsidera- thenselecttheactionthatmaximisesu .
t−1
tionforkernelbandits(seeSec.5). IfX hasfinite(andnot
toolarge)cardinality,thenUCB (x)(andupperbounds
Ft Algorithm1KernelUCB
onit)caneasilybemaximisedexactly.
Input: Upperconfidenceboundsu ,u ,u ,...
0 1 2
If X is a continuous subset of Rd, then maximising fort=1toT do
UCB (x)oranyofourupperboundsoverxexactlyisin- Playx =argmax {u (x)}
Ft t x∈Xt t−1
tractableingeneral,thoughothersimilarconfidencebounds Observey =f∗(x )+ϵ
t t t
havethesamelimitation. Thereareatleasttwooptionsfor endfor
approximatemaximisation. Oneoptionistousegradient-
basedmethodsasin(Flynnetal.,2023). Gradients(w.r.t.x)
oftheimplicitconfidenceboundsin(10),(11)and(12)can For each t, we must be able to compute u using the
t
becomputednumericallyusingdifferentiableconvexoptim- data x ,y ,...,x ,y . Kernel bandit algorithms of this
1 1 t t
sationmethods(Agrawaletal.,2019)orautomaticimplicit form have appeared under different names, such as GP-
5TighterConfidenceBoundsforSequentialKernelRegression
UCB(Srinivasetal.,2010)andKernelUCB(Valkoetal., UCBandKernelAMM-UCB.Webeginbyformallystating
2013),basedontheconfidenceboundsthattheyuse. We theassumptionsunderwhichouranalysisholds.Weassume
thereforeusethenamesKernelUCBandGP-UCBsynony- thatthenoisevariablesareconditionallyσ-sub-Gaussian.
mously. WhenweusetheconfidenceboundsinThm.3.11 Assumption6.1(Sub-Gaussiannoise). Let(D |t∈N)be
t
of (Abbasi-Yadkori, 2012), we call Algorithm 1 AY-GP-
anyfiltrationsuchthaty andx arebothD -measurable.
t t+1 t
UCB.IfweusetheconfidenceboundsinThm.2of(Chowd-
Each noise variable ϵ is conditionally σ-sub-Gaussian,
t
hury&Gopalan,2017),thenAlgorithm1istheImproved
whichmeans
GP-UCBalgorithm(IGP-UCB).
∀λ∈R, E[exp(λϵ )|D ]≤exp(λ2σ2/2).
t t−1
5.3.KernelCMM-UCB
This condition implies that E[ϵ |D ] = 0. We assume
ForourfirstvariantofKernelUCB,weuseourexactcon- t t−1
thattherewardfunctionisuniformlybounded,andthatits
fidence bound UCB (x). In particular, we set u (x) to
Ft t
RKHSnormisbounded.
bethenumericalsolutionof(10). Theresultingalgorithm,
whichwecallKernelConvexMartingaleMixtureUCB(Ker- Assumption6.2(Boundedrewardfunction). ForsomeB >
nelCMM-UCB),isakernelisedversionoftheCMM-UCB 0,∥f∗∥ ≤B. ForsomeC >0,sup |f∗(x)|≤C.
H x∈X
algorithmforlinearbandits(Flynnetal.,2023).
Ifthekernelfunctionisuniformlyboundedbysomeκ>0,
i.e. k(x,x′)∈[−κ,κ]forallx,x′ ∈X,then∥f∗∥ ≤B
5.4.KernelDMM-UCB H
impliesthatf∗(x)∈[−C,C]holdswithC ≤κB. Tocom-
ForoursecondvariantofKernelUCB,weusetheconfidence puteourconfidenceboundsandrunourbanditalgorithms,
boundin(12)thatcomesfromthedualproblemassociated wedonotneedtoknowthevalueofC,butwedoneedto
withmax {f(x)}. Intheinterestofcomputationalef- know(upperboundson)σandB.
f∈Ft
ficiency, we replace min in (12) with min , for a
α>0 α∈A
smallgridAofvaluesofα. Inparticular,weset 6.1.TighterConfidenceBounds
(cid:40) (cid:41)
u t(x)=m α∈i An µ α,t(x)+
R(cid:101)
√α α,tρ α,t(x) .
(T Ch he owco dn hfi ud re yn &ce Gb oo pu an ld as n,in 20( 1A 7)bb aa resi- tY head sk ao mri e, a2 s01 o2 u)
r a
√a nn ad
-
lyticUCBin(14),exceptthatthe(scaled)radiusR(cid:101)α,t/ α
is replaced with a different quantity. For any values of
WecalltheresultingbanditalgorithmKernelDualMartin-
the regularisation parameters λ > 0 and η > 0 used in
galeMixtureUCB(KernelDMM-UCB).Becauseweonly
(Abbasi-Yadkori,2012)and(Chowdhury&Gopalan,2017)
optimise α over the grid A, this upper confidence bound
respectively, we show that we can set c and α such that
√
islooserthantheexactconfidenceboundusedbyKernel
R(cid:101)α,t/ αisstrictlylessthantheseotherradiusquantities.
CMM-UCB.Inreturn,thecostofcomputingthisUCBis
ThisresultisstatedinThm. 6.3andprovedinApp. C.2.
only a factor of |A| more than the cost of computing the
Theorem 6.3. For every δ ∈ (0,1] and every value of
analyticUCBinCor. 4.6.
theregularisationparameterλ > 0,theRHSof(14)with
c=σ2/λandα=λisstrictlylessthantheUCBinThm.
5.5.KernelAMM-UCB
3.11of(Abbasi-Yadkori,2012).
ForourthirdandfinalvariantofKernelUCB,wesetu (x)
t Foreveryδ ∈ (0,1]andeveryvalueoftheregularisation
to be the analytic UCB from Cor. 4.6 with a fixed value
parameter η > 0, the RHS of (14) with c = σ2/(1+η)
ofα. Theresultingalgorithm,whichwecallKernelAna-
andα = 1+η isstrictlylessthantheUCBinThm. 2of
lyticMartingaleMixtureUCB(KernelAMM-UCB),isa
(Chowdhury&Gopalan,2017).
kernelisedversionoftheAMM-UCBalgorithmforlinear
bandits(Flynnetal.,2023). Thisalgorithmismainlyuseful
6.2.CumulativeRegretBounds
forourregretanalysisinSection6.2.
Weboundcumulativeregretintermsofakernel-dependent
6.TheoreticalAnalysis quantity called the maximum information gain, which is
definedas
Inthissection,weshowthatouranalyticconfidencebound (cid:26) (cid:18) (cid:18) (cid:19)(cid:19)(cid:27)
1 1
fromCor. 4.6isalwaystighterthantheconfidencebounds γ (α):= max ln det K +I .
in Thm. 3.11 of (Abbasi-Yadkori, 2012) and Thm. 2 of t x1,...,xt 2 α t
(Chowdhury&Gopalan,2017),whichholdunderthesame
conditionsandassumptions. Inaddition,weestablishcumu- First,weneedtoupperboundourradiusquantityR(cid:101)α,t in
lativeregretboundsforKernelCMM-UCB,KernelDMM- termsofthemaximuminformationgain. Todoso,weshow
6TighterConfidenceBoundsforSequentialKernelRegression
Table1.CumulativeregretofourKernelUCB-stylealgorithmsaswellasAY-GP-UCB,IGP-UCBandarandombaselineafterT =1000
roundswithd=3.Weshowthemean±standarddeviationover10repetitions.
RBFKernel Mate´rnKernel(ν=5/2) Mate´rnKernel(ν=3/2)
ℓ=0.5 ℓ=0.2 ℓ=0.5 ℓ=0.2 ℓ=0.5 ℓ=0.2
KernelDMM-UCB(Ours) 32.2±20.9 491.4±117.1 129.5±45.6 795.1±206.0 195.6±78.0 814.1±344.4
KernelAMM-UCB(Ours) 88.8±6.1 1206.2±20.8 197.0±24.4 1661.5±90.1 316.1±51.1 1741.2±351.2
AY-GP-UCB 136.9±12.7 1518.4±38.9 331.7±45.2 2382.4±135.4 546.0±70.0 2421.3±568.5
IGP-UCB 314.1±110.5 1433.0±122.8 553.3±67.5 1853.1±105.7 655.6±67.4 1707.5±375.5
Random 4282.4±1015.4 3872.4±783.7 4264.7±778.0 3677.5±559.2 4175.1±681.0 3442.0±1080.4
Figure2.CumulativeregretofourKernelUCB-stylealgorithmsaswellasAY-GP-UCB,IGP-UCBandarandombaselineoverT =1000
roundsford=3andvariouskernels(columns)andlengthscales(rows).Weshowthemean±standarddeviationover10repetitions.
thatwhenα=σ2/c,thequadratictermsinR(cid:101)α,tcancelout BecausetheconfidenceboundsusedbyKernelCMM-UCB
(seeLemmaC.2),andweareleftwith (withthesamec)andKernelDMM-UCB(withthesame
(cid:113)
candwithα=σ2/c∈A)areneverlooserthantheconfi-
R(cid:101)σ2/c,t ≤ 2σ2γ t(σ2/c)+ σ2 cB2 +2σ2ln(1 δ). denceboundsusedbyKernelAMM-UCB,thesealgorithms
alsosatisfytheboundin(15),whichmeansthecumulative
Wenowproceedtoupperboundthecumulativeregretof regretboundforKernelAMM-UCBalsoappliestoKernel
KernelAMM-UCBwithα=σ2/c. Usingastandardargu- CMM-UCBandKernelDMM-UCB.
ment(seee.g. Prop. 1in(Russo&VanRoy,2013)andalso
The regret bound in Thm. 6.4 matches the best existing
LemmaD.1inApp. D.1),theregretofanyUCBalgorithm
regret bounds for bandit algorithms that follow the Ker-
atroundtis(withhighprobability)upperboundedbythe
nelUCBrecipeinAlgorithm1(Srinivasetal.,2010;Chowd-
differencebetweentheupperandlowerconfidencebounds
hury & Gopalan, 2017; Durand et al., 2018; Whitehouse
(atx )thatituses. ForKernelAMM-UCB,thisboundis
t etal.,2023),thoughitsdependenceonthemaximuminfor-
√
2R(cid:101)σ2/c,t−1 mationgainissub-optimalbyafactorof γ T. However,
r ≤ ρ (x ). (15)
t (cid:112) σ2/c,t−1 t duetothelowerboundin(Lattimore,2023),thiscannotbe
σ2/c
improvedbyusingabetterconfidencesequenceforf∗.
Fromhere,ourcumulativeregretboundinThm.6.4follows ForthefrequentlyusedRBFandMate´rnkernels,thedepen-
fromstandardarguments,sowedefertheprooftoApp. D.2. denceofthemaximuminformationgainonT canbemade
Theorem6.4. Foranycovariancescalec>0,withproba- explicit. FortheRBFkernelandwithc∝1,themaximum
bilityatleast1−δ,forallT ≥1,thecumulativeregretof informationgainisO((ln(T))d+1)(seee.g. (Srinivasetal.,
KernelCMM-UCB,KernelDMM-UCB(withagridAcon- 2010;Vakilietal.,2021)),whichmeanstheregretbound
√
tainingα=σ2/c)andKernelAMM-UCB(withα=σ2/c) inThm. 6.4isO( T(ln(T))d+1). FortheMate´rnkernel
satisfies withsmoothnessparameterν andc∝T− 2d+d 2ν,themaxi-
(cid:32)(cid:114)
(cid:16)
(cid:17)(cid:33) muminformationgainisO(T2d+d 2ν(ln(T))2d2 +ν 2ν)(seee.g.
r =O Tγ (σ2) γ (σ2)+ 1B2+ln(1) . (Vakilietal.,2021;Whitehouseetal.,2023)),whichmeans
1:T T c T c c δ
theregretboundisO(T22 dd ++ 2ν ν(ln(T))d+2ν 2ν).
7TighterConfidenceBoundsforSequentialKernelRegression
7.Experiments agridAofsize5,thetimeperstepforKernelDMM-UCB
isalittleunder5timesthatofKernelAMM-UCB.Thetime
Weaimtoverifythatwhenourconfidenceboundsreplace
perstepforKernelCMM-UCBgrowsatthefastestratein
existing ones, KernelUCB shows better empirical perfor-
t. Att = 200,thetimeperstepforKernelCMM-UCBis
manceatasimilarcomputationalcost.
morethan15seconds,whereasthetimeperstepforevery
othermethodisbelow0.02seconds.
ComparedMethods. Wecompare:a)KernelCMM-UCB:
cf. Sec. 5.3; b)KernelDMM-UCB:cf. Sec. 5.4; c)Ker-
8.Conclusion
nelAMM-UCB:cf. Sec. 5.5; d)AY-GP-UCB:Algorithm
1withtheconfidenceboundsfromThm. 3.11of(Abbasi-
Inthispaper,wedevelopednewconfidencesequencesand
Yadkori, 2012); e) IGP-UCB: Algorithm 1 with the con-
confidenceboundsforsequentialkernelregression. Inour
fidenceboundsfromThm. 2of(Chowdhury&Gopalan,
theoretical analysis, we proved that our new confidence
2017);e)Random:Actionsarechosenuniformlyatrandom.
bounds are always tighter than comparable existing con-
fidence bounds. We presented three variants of the Ker-
ExperimentalSetup. Weruneachalgorithminseveral
nelUCBalgorithmforkernelbandits,whicheachusedif-
synthetic kernel bandit problems. To generate each re-
ferentconfidenceboundsthatcanbederivedfromourcon-
ward function, we sample 20 inducing points z ,...,z
1 20 fidencesequences. Theresultsofourexperimentssuggest
uniformly at random from the hypercube [0,1]d and a
that the Kernel DMM-UCB algorithm should be the pre-
20-dimensional weight vector w ∈ R20 from a stan-
ferredchoiceforpracticaluses. Itcloselymatchestheem-
dard normal distribution. The test function is f∗(x) =
piricalperformanceofoureffectivebutexpensiveKernel
b(cid:80)20
w k(x,z ), where the scaling factor b is set such
i=1 i i CMM-UCBalgorithm,yetithasroughlythesamecompu-
that∥f∗∥ =B. Thenoisevariablesϵ ,ϵ ,... aredrawn
H 1 2 tationalcostasourKernelAMM-UCBmethod(aswellas
independentlyfromanormaldistributionwithmean0and
AY-GP-UCBandIGP-UCB).Together,ourtheoreticalanal-
standard deviation σ. In each round t, the action set X
t ysisandourexperimentssuggestthatbyreplacingexisting
consistsof100d-dimensionalvectorsdrawnuniformlyat
confidenceboundsinKernelUCBwithournewconfidence
randomfromthehypercube[0,1]d. Wepresentresultsfor
bounds,weobtainkernelbanditalgorithmswithbetterem-
d ∈ {2,3,4} and with the RBF and Mate´rn kernels with
piricalperformance,matchingperformanceguaranteesand
lengthscaleℓ∈{0.2,0.5}.Werecordthecumulativeregret
comparablecomputationalcost.
andwall-clocktime-per-stepoverT =1000rounds.
Thoughinthispaperweappliedourconfidenceboundsonly
We run each algorithm with δ = 0.01 and the true val-
tothekernelbanditproblem,andtheKernelUCBalgorithm
ues B = 10 and σ = 0.1. We run IGP-UCB with the
inparticular,theyareagenerictoolthatcouldbeusedany-
recommend value of η = 2/T. For each of our meth-
where that existing confidence bounds are used. For the
ods, we set the covariance scale to c = 1 for the RBF
future,itremainstoapplyourconfidenceboundstoother
kernel and c = T− 2d+d 2ν for the Mate´rn kernel. Equiva- kernel bandit algorithms and other sequential kernelised
lently,forAY-GP-UCB,wesetλ=σ2fortheRBFkernel
learning and/or decision-making problems. For instance,
and λ = σ2T2d+d 2ν for the Mate´rn kernel. We run Ker- onecoulduseourconfidenceboundsintheπ-GP-UCBalgo-
nel AMM-UCB with α = σ2/c and Kernel DMM-UCB rithm(Janzetal.,2020),whichhasrecentlybeenshownto
withA = {0.1σ2/c,0.3σ2/c,σ2/c,3σ2/c,10σ2/c}. Due achieveoptimalorderregretfortheMate´rnkernel(Vakili&
toprohibitivecost,weonlyrunKernelCMM-UCBfor200 Olkhovskaya,2023). Inaddition,onecouldapplyourcon-
rounds. Allexperimentswererunonasinglecomputerwith fidenceboundstokernelisedreinforcementlearning(Yang
anInteli5-1145G7CPU. etal.,2020;Liu&Su,2022;Vakili&Olkhovskaya,2023)
oradaptivecontrol(Kakadeetal.,2020).
Results. Fig. 2 shows the cumulative regret of each al-
Finally,wecommentonsomelimitationsofourwork. We
gorithm with d = 3, every kernel and every length-scale
assume that the kernel as well as reasonably good upper
(resultswithd∈{2,4}areshowninApp. F.2). Thecumu-
boundsonσ andB areknowninadvance,whichisoften
lativeregretatT =1000isdisplayedinTab. 1. OurKernel
notthecaseinpractice. Whileusingalooseupperbound
CMM-UCBalgorithmconsistentlyachievesthelowestcu-
on σ appears to have little impact, we found that using a
mulativeregretoverthefirst200steps. OurKernelDMM-
looseboundonBcanharmempiricalperformance(seeFig.
UCBalgorithmcloselymatchesKernelCMM-UCBover
7). Infuturework,wewouldliketoaddressthislimitation
thefirst200stepsandconsistentlyachievesthelowestcu-
byusingmodelselectionmethodstolearnanupperbound
mulativeregretover1000steps. Fig. 4(inApp. F.1)shows
on∥f∗∥ ,andthekerneloritshyperparameters,fromdata
the wall-clock time per step for each method. IGP-UCB, H
asitbecomesavailable.
AY-GP-UCBandKernelAMM-UCBtakeapproximately
thesametimeperstep. Aswewouldexpect,sinceweused
8TighterConfidenceBoundsforSequentialKernelRegression
BroaderImpact Diamond,S.andBoyd,S. CVXPY:APython-embedded
modelinglanguageforconvexoptimization. Journalof
Thispaperpresentsworkwhosegoalistoadvancethefield
MachineLearningResearch,17(83):1–5,2016.
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be Durand, A., Maillard, O.-A., and Pineau, J. Streaming
specificallyhighlightedhere. kernelregressionwithprovablyadaptivemean,variance,
and regularization. The Journal of Machine Learning
References Research,19(1):650–683,2018.
Abbasi-Yadkori, Y. Online learning for linearly Emmenegger, N., Mutny`, M., and Krause, A. Likeli-
parametrizedcontrolproblems. PhDthesis,University hoodratioconfidencesetsforsequentialdecisionmaking.
ofAlberta,2012. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),2023.
Abbasi-Yadkori,Y.,Pa´l,D.,andSzepesva´ri,C. Improved
algorithms for linear stochastic bandits. Advances in Flynn,H.,Reeb,D.,Kandemir,M.,andPeters,J. Improved
neuralinformationprocessingsystems,24,2011. algorithmsforstochasticlinearbanditsusingtailbounds
formartingalemixtures. InAdvancesinNeuralInforma-
Abbasi-Yadkori, Y., Pal, D., and Szepesvari, C. Online- tionProcessingSystems(NeurIPS),2023.
to-confidence-setconversionsandapplicationtosparse
stochasticbandits. InArtificialIntelligenceandStatistics, Jamieson, K., Malloy, M., Nowak, R., and Bubeck, S.
pp.1–9.PMLR,2012. lil’ucb: An optimal exploration algorithm for multi-
armedbandits. InConferenceonLearningTheory,pp.
Agrawal,A.,Amos,B.,Barratt,S.,Boyd,S.,Diamond,S., 423–439.PMLR,2014.
andKolter,J.Z. Differentiableconvexoptimizationlay-
ers. Advancesinneuralinformationprocessingsystems, Janz, D., Burt, D., and Gonza´lez, J. Bandit optimisation
offunctionsinthemate´rnkernelrkhs. InInternational
32,2019.
Conference on Artificial Intelligence and Statistics, pp.
Auer, P. Using confidence bounds for exploitation- 2486–2495.PMLR,2020.
exploration trade-offs. Journal of Machine Learning
Kakade,S.,Krishnamurthy,A.,Lowrey,K.,Ohnishi,M.,
Research,3(Nov):397–422,2002.
and Sun, W. Information theoretic regret bounds for
onlinenonlinearcontrol.AdvancesinNeuralInformation
Berkenkamp,F.,Turchetta,M.,Schoellig,A.,andKrause,
ProcessingSystems,33:15312–15325,2020.
A.Safemodel-basedreinforcementlearningwithstability
guarantees. Advancesinneuralinformationprocessing
Kimeldorf,G.andWahba,G. Someresultsontchebychef-
systems,30,2017.
fiansplinefunctions. Journalofmathematicalanalysis
andapplications,33(1):82–95,1971.
Blondel,M.,Berthet,Q.,Cuturi,M.,Frostig,R.,Hoyer,S.,
Llinares-Lo´pez,F.,Pedregosa,F.,andVert,J.-P. Efficient
Lattimore,T.Alowerboundforlinearandkernelregression
andmodularimplicitdifferentiation. Advancesinneural
with adaptive covariates. In The Thirty Sixth Annual
informationprocessingsystems,35:5230–5242,2022.
ConferenceonLearningTheory,pp.2095–2113.PMLR,
2023.
Cesa-Bianchi,N.andLugosi,G. Prediction,learning,and
games. Cambridgeuniversitypress,2006.
Lattimore,T.andSzepesva´ri,C. Banditalgorithms. Cam-
bridgeUniversityPress,2020.
Chowdhury, S. R. and Gopalan, A. On kernelized multi-
armedbandits. InInternationalConferenceonMachine Li, Z. and Scarlett, J. Gaussian process bandit optimiza-
Learning,pp.844–853.PMLR,2017. tion with few batches. In International Conference on
ArtificialIntelligenceandStatistics,pp.92–107.PMLR,
de la Pen˜a, V. H., Klass, M. J., and Leung Lai, T. Self-
2022.
normalizedprocesses: exponentialinequalities,moment
boundsanditeratedlogarithmlaws.AnnalsofProbability, Liu,S.andSu,H. ProvablyefficientkernelizedQ-learning.
32:1902–1933,2004. arXivpreprintarXiv:2204.10349,2022.
de la Pen˜a, V. H., Lai, T. L., and Shao, Q.-M. Self- Look, A., Doneva, S., Kandemir, M., Gemulla, R., and
normalizedprocesses: LimittheoryandStatisticalAppli- Peters,J. Differentiableimplicitlayers. InNeurIPSML
cations. Springer,2009. forEngineeringWorkshop,2020.
9TighterConfidenceBoundsforSequentialKernelRegression
Neiswanger,W.andRamdas,A. Uncertaintyquantification
usingmartingales formisspecifiedGaussianprocesses.
In Algorithmic Learning Theory, pp. 963–982. PMLR,
2021.
Russo,D.andVanRoy,B.Eluderdimensionandthesample
complexityofoptimisticexploration. AdvancesinNeural
InformationProcessingSystems,26,2013.
Scho¨lkopf, B., Herbrich, R., andSmola, A.J. Ageneral-
izedrepresentertheorem. InInternationalconferenceon
computational learning theory, pp. 416–426. Springer,
2001.
Srinivas,N.,Krause,A.,Kakade,S.,andSeeger,M. Gaus-
sianprocessoptimizationinthebanditsetting: Noregret
andexperimentaldesign. InProc.InternationalConfer-
enceonMachineLearning(ICML),2010.
Sui, Y., Gotovos, A., Burdick, J., and Krause, A. Safe
explorationforoptimizationwithgaussianprocesses. In
Internationalconferenceonmachinelearning,pp.997–
1005.PMLR,2015.
Sutton,R.S.andBarto,A.G. Reinforcementlearning: An
introduction. MITpress,2018.
Vakili, S.andOlkhovskaya, J. Kernelizedreinforcement
learningwithorderoptimalregretbounds.InAdvancesin
NeuralInformationProcessingSystems(NeurIPS),2023.
Vakili, S., Khezeli, K., and Picheny, V. On information
gainandregretboundsingaussianprocessbandits. In
InternationalConferenceonArtificialIntelligenceand
Statistics,pp.82–90.PMLR,2021.
Valko,M.,Korda,N.,Munos,R.,Flaounas,I.,andCristian-
ini, N. Finite-Time Analysis of Kernelised Contextual
Bandits. InUncertaintyinArtificialIntelligence,2013.
Ville,J. Etudecritiquedelanotiondecollectif. Bull.Amer.
Math.Soc,45(11):824,1939.
Whitehouse,J.,Wu,Z.S.,andRamdas,A. Onthesublin-
ear regret of gp-ucb. Advances in Neural Information
ProcessingSystems,2023.
Yang,Z.,Jin,C.,Wang,Z.,Wang,M.,andJordan,M.I. On
functionapproximationinreinforcementlearning: opti-
misminthefaceoflargestatespaces. InProceedingsof
the34thInternationalConferenceonNeuralInformation
ProcessingSystems,pp.13903–13916,2020.
Zhang,F. TheSchurcomplementanditsapplications,vol-
ume4. SpringerScience&BusinessMedia,2006.
10TighterConfidenceBoundsforSequentialKernelRegression
A.TailBoundDerivations
A.1.DerivationofEq. (7)
Forconvenience, wefirstre-statethesettinginwhichthetailboundholds. (D |t ∈ N)isanyfiltrationsuchthatx is
t t
D -measurableandy isD -measurable(e.g. D =σ(x ,y ,...,x ,y ,x )). (Z :R→R|t∈N)isasequenceof
t−1 t t t 1 1 t t t+1 t
adaptedrandomfunctions,(g |t ∈ N)isasequenceofpredictableguessesand(λ |t ∈ N)isasequenceofpredictable
t t
randomvariables. (P |t∈N)isanyadaptivesequenceofmixturedistributions,whichmeans: (a)P isadistributionover
t t
g ∈Rt;(b)P isD -measurable;(c)(cid:82) P (g )dg =P (g )forallt. Define
t t t−1 t t t t−1 t−1
(cid:32) t (cid:33)
(cid:88)
M (g ,λ ):=exp λ Z (g )−ψ (g ,λ ) , ψ (g ,λ ):=ln(E[exp(λ Z (g ))|D ]). (16)
t t t s s s s s s t t t t t t t−1
s=1
DuetoThm. 5.1of(Flynnetal.,2023),foranyδ ∈(0,1],withprobabilityatleast1−δ,
(cid:18) (cid:19)
∀t≥1: ln E [M (g ,λ )] ≤ln(1/δ). (17)
t t t
g t∼Pt
WechooseZ (g )=(g −f∗(x ))ϵ . Usingtheσ-sub-Gaussianpropertyofthenoisevariables,wehave
t t t t t
σ2λ2(g −f∗(x ))2
ψ (g ,λ )=ln(E[exp(λ (g −f∗(x ))ϵ )|D ])≤ t t t .
t t t t t t t t−1 2
Combiningthiswith(17),wehave
ln(cid:32)
E
(cid:34) exp(cid:32) (cid:88)t
λ(g −f∗(x ))ϵ −
σ2λ2 s(g s−f∗(x s))2(cid:33)(cid:35)(cid:33)
(18)
g t∼Pt
s=1
s s s 2
(cid:32) (cid:34) (cid:32) t (cid:33)(cid:35)(cid:33) (cid:18) (cid:19)
(cid:88)
≤ln E exp λZ (g )−ψ (g ,λ) =ln E [M (g ,λ)] ≤ln(1/δ).
s s s s t t
g t∼Pt
s=1
g t∼Pt
Next,werearrangetheintegrandontheLHSof(18). Foreverys∈[t],wehave
(f∗(x )−y )2−(g −y )2 =(ϵ )2−(g −f∗(x )−ϵ )2
s s s s s s s s
=(ϵ )2−(g −f∗(x ))2+2(g −f∗(x ))ϵ −(ϵ )2
s s s s s s s
=2(g −f∗(x ))ϵ −(g −f∗(x ))2.
s s s s s
Thismeansthat
σ2λ2(g −f∗(x ))2 λ λ 1
λ (g −f∗(x ))ϵ − s s s = (f∗(x )−y )2− (g −y )2+ (λ −σ2λ2)(g −f∗(x ))2.
s s s s 2 2 s s 2 s s 2 s s s
Eq. (18)cannowbere-writtenas
(cid:32) (cid:34) (cid:32) t (cid:33)(cid:35)(cid:33)
ln E exp
(cid:88)λ
(f∗(x )−y )2−
λ
s(g −y )2+
1
(λ −σ2λ2)(g −f∗(x ))2 ≤ln(1/δ). (19)
g t∼Pt
s=1
2 s s 2 s s 2 s s s s
Wesetλ ≡1/σ2. Forthischoice,wehaveλ −σ2λ2 =0,and(19)becomes
s s s
(cid:32) (cid:34) (cid:32) t (cid:33)(cid:35)(cid:33)
(cid:88) 1 1
ln E exp (f∗(x )−y )2− (g −y )2 ≤ln(1/δ). (20)
g t∼Pt
s=1
2σ2 s s 2σ2 s s
By defining f∗ := [f∗(x ),...,f∗(x )]⊤ and y := [y ,...,y ]⊤, rearranging (20), and then writing the resulting
t 1 t t 1 t
inequalityusingsquarednormsratherthansumsofsquares,weobtain
(cid:18) (cid:20) (cid:18) (cid:19)(cid:21)(cid:19)
1
∥f∗−y ∥2 ≤−2σ2ln E exp − ∥g −y ∥2 +2σ2ln(1/δ).
t t 2
g t∼Pt
2σ2 t t 2
11TighterConfidenceBoundsforSequentialKernelRegression
A.2.DerivationofEq. (8)
Eq. (8)isderivedfromEq. (7)usingthefollowingLemma,whichisprovedinApp. B.2of(Flynnetal.,2023).
LemmaA.1((Flynnetal.,2023)). Foranyt,anyµ andanypositivesemi-definiteT ,wehave
t t
(cid:20) (cid:18)
1
(cid:19)(cid:21) (cid:115)
1
(cid:32)
1
(cid:18)
T
(cid:19)−1 (cid:33)
E exp − ∥g −y ∥2 = exp − (µ −y )⊤ I+ t (µ −y )
2σ2 t t 2 det(I+T /σ2) 2σ2 t t σ2 t t
g t∼N(µ,Tt) t
CombiningEq. (7)andLemmaA.1,weobtain
(cid:18) (cid:20) (cid:18) (cid:19)(cid:21)(cid:19)
1
∥f∗−y ∥2 ≤−2σ2ln E exp − ∥g −y ∥2 +2σ2ln(1/δ). (21)
t t 2 2σ2 t t 2
g t∼N(0,cKt)
(cid:18)
cK
(cid:19)−1 (cid:18) (cid:18)
cK
(cid:19)(cid:19)
=y⊤ I+ t y +σ2ln det I+ t +2σ2ln(1/δ)=(R (δ))2.
t σ2 t σ2 t
B.ConfidenceSequenceandConfidenceBoundDerivations
Inthissection,weprovidethefullderivationsofourconfidencesequencesandconfidenceboundsfromSection4. Before
provingourmainresults,westateandprovesomeusefullemmas.
B.1.UsefulLemmas
LemmaB.1allowsustoexpressweightedsumsofthequadraticconstraintsthatdefineF (fromLemma4.2)asasingle
t
quadraticconstraint.
LemmaB.1. Foranyα>0andanyf ∈H,
(cid:13) (cid:13)2
∥f −y ∥2+α∥f∥2 =(cid:13)(V +αI )1/2(f −µ )(cid:13) +y⊤y −y⊤Φ (V +αI )−1Φ⊤y .
t t 2 H (cid:13) t H α,t (cid:13) t t t t t H t t
H
Proof. ∥f −y ∥2+α∥f∥2 canberewrittenintheform(f −b)⊤A(f −b)+cbycompletingthesquare,whereAisa
t t 2 H
self-adjoint(symmetric)linearoperator,b∈Handc∈R. SinceAisself-adjoint,wehave
(f −b)⊤A(f −b)+c=f⊤Af −2b⊤Af +b⊤Ab+c.
Wealsohave
∥f −y ∥2+α∥f∥2 =(Φ f −y )⊤(Φ f −y )+αf⊤f
t t 2 H t t t t
=f⊤(V +αI )f −2y⊤Φ f +y⊤y .
t H t t t t
Byequatingcoefficients,wehave
A=V +αI ,
t H
b=(V +αI )−1Φ⊤y =Φ⊤(K +αI)−1y =µ ,
t H t t t t t α,t
c=y⊤y −y⊤Φ (V +αI )−1Φ⊤y .
t t t t t H t t
LemmaB.2givesthemaximumofcertainconcavequadraticfunctionals. WeuseitintheproofofTheorem4.5tocalculate
thedualfunctionassociatedwithmax {f(x)}.
f∈Ft
12TighterConfidenceBoundsforSequentialKernelRegression
LemmaB.2. Foranyfunctionsa,b∈Handanyself-adjoint(symmetric)positive-definitelinearoperatorA,wehave
max(cid:8) a⊤f −(f −b)⊤A(f −b)(cid:9) =a⊤b+ 1 a⊤A−1a.
f∈H 4
Proof. LetF(f) = a⊤f −(f −b)⊤A(f −b). SinceF(f)isaconcavequadraticfunctional,F′(f¯) = 0isasufficient
conditionforf¯tobeamaximiserofF(f),whereF′(f)istheFre´chet-derivativeofF atf. Foranydirectiong ∈Hand
anys>0,wehave
d
F(f +sg)=a⊤g−2sg⊤Ag−2(f −b)⊤Ag.
ds
Therefore,thedirectionalderivativeofF (inthedirectiong)is
(cid:12)
d F(f +sg)(cid:12) (cid:12) =a⊤g−2(f −b)⊤Ag.
ds (cid:12)
s=0
ThismeansthattheFre´chet-derivativeofF atf is
F′(f)=a−2A(f −b).
ThereisauniquesolutionofF′(f¯)=0,whichis
1
f¯=b+ Aa.
2
Themaximumis
1
F(f¯)=a⊤b+ a⊤A−1a.
4
LemmaB.3providesanalternativeexpressionforρ2 (x),whichisalreadywell-known.
α,t
LemmaB.3. Foranyα>0,
k(·,x)⊤(V +αI )−1k(·,x)= 1 (cid:0) k(x,x)−k (x)⊤(K +αI)−1k (x)(cid:1) .
t H α t t t
Proof.
1
k(·,x)⊤(V +αI )−1k(·,x)= k(·,x)⊤(V +αI )−1(αI )k(·,x)
t H α t H H
1
= k(·,x)⊤(V +αI )−1(Φ⊤Φ +αI −Φ⊤Φ )k(·,x)
α t H t t H t t
= 1 (cid:0) k(·,x)⊤I k(·,x)−k(·,x)⊤(V +αI )−1Φ⊤Φ k(·,x)(cid:1)
α H t H t t
= 1 (cid:0) k(·,x)⊤I k(·,x)−k(·,x)⊤Φ⊤(K +αI)−1Φ k(·,x)(cid:1)
α H t t t
= 1 (cid:0) k(x,x)−k (x)⊤(K +αI)−1k (x)(cid:1) .
α t t t
13TighterConfidenceBoundsforSequentialKernelRegression
B.2.ProofofCorollary4.3
ProofofCorollary4.3. From∥f∗∥ ≤BandEq. (8),foranyδ ∈(0,1],withprobabilityatleast1−δ,f∗satisfies
H
∀t≥1,α≥0: ∥f∗−y ∥2+α∥f∗∥2 ≤R2+αB2. (22)
t t 2 H t
UsingLemmaB.1,theinequalityin(22)canberewrittenas
(cid:13) (cid:13)2
(cid:13)(V +αI )1/2(f∗−µ )(cid:13) ≤R2+αB2−y⊤y +y⊤Φ (V +αI )−1Φ⊤y .
(cid:13) t H α,t (cid:13) t t t t t t H t t
H
Thismeansthatf∗ ∈ F(cid:101)t forallt ≥ 1andα > 0. FromthedefinitionoftheconfidencesetF
t
fromLemma4.2,every
f ∈F tmustsatisfy∥f t−y t∥2 2+α∥f∥2
H
≤R t2+αB2,whichmeansf ∈F(cid:101)t. Therefore,F
t
⊆F(cid:101)t.
B.3.ProofofTheorem4.4
Forconvenience,wefirstrestatesomeofthedefinitionsfromSection4.3. k (x) = [k(x,x ),...,k(x,x ),k(x,x)]⊤.
t+1 1 t
K isthet×t+1kernelmatrixwithi,jthelementequaltok(x ,x )ifj <t+1andk(x ,x)otherwise. L isany
t,t+1 i j i t+1
matrixsatisfyingL⊤ L =K .
t+1 t+1 t+1
ProofofTheorem4.4. Theoptimisationproblemmax {f(x)}canbewrittenas
f∈Ft
max f(x) s.t. ∥f −y ∥ ≤R , ∥f∥ ≤B. (23)
t t 2 t H
f∈H
First,weproveastatementthatresemblesarepresentertheorem(Kimeldorf&Wahba,1971;Scho¨lkopfetal.,2001). We
willshowthatthesolutionof(23)mustbeoftheformf(x)=k (x)⊤w,forsomeweightvectorw ∈Rt+1.
t+1
All functions f ∈ H can be written in the form f = f + f , where f = k (·)⊤w is in the subspace of H
∥ ⊥ ∥ t+1
spanned by k(·,x ),...,k(·,x ),k(·,x), and f is orthogonal to this subspace. Since f is orthogonal to the bases
1 t ⊥ ⊥
k(·,x ),...,k(·,x ),k(·,x),wehave
1 t
∥f∥2
H
=(cid:10) f ∥+f ⊥,f ∥+f ⊥(cid:11)
H
=(cid:10) f ∥,f ∥(cid:11) H+2(cid:10) f ∥,f ⊥(cid:11) H+⟨f ⊥,f ⊥⟩
H
=(cid:13) (cid:13)f ∥(cid:13) (cid:13)2 H+∥f ⊥∥2 H,
ThismeansthattheRKHSnormoff isminimisedw.r.t. f bychoosingf⊥≡0. Usingthereproducingpropertyofthe
⊥
kernelandthefactthatf isorthogonaltok(·,x ),...,k(·,x ),k(·,x),foranyz ∈{x ,...,x ,x},wehave
⊥ 1 t 1 t
(cid:10) (cid:11) (cid:10) (cid:11)
f(z)=⟨f,k(·,z)⟩ = f +f ,k(·,z) = f ,k(·,z) =f (z).
H ∥ ⊥ H ∥ H ⊥
Thismeansthatthefunctionvaluesf(x ),...,f(x ),f(x)areentirelydeterminedbyf . Theobjectivefunctionisf(x)
1 t ∥
andtheconstraint∥f −y ∥ ≤R dependsonlyonf(x ),...,f(x ). Therefore,anyf ̸=0willhavenoeffectonthe
t t 2 t 1 t ⊥
objectiveorthisconstraint,andwillonlyincreasethenormoff. Thismeansthatwemusthavef ≡0atthesolutionof
⊥
max {f(x)}. Wearenowfreetolookforasolutionoftheformf(x)=k (x)⊤w,whichmeans
f∈Ft t+1
∥f −y ∥ =∥K w−y ∥ ,
t t 2 t,t+1 t 2
(cid:118)
(cid:117)(cid:42) t t (cid:43)
(cid:117) (cid:88) (cid:88)
∥f∥
H
=(cid:116) k(·,x s)w s+k(·,x)w t+1, k(·,x s′)w
s′
+k(·,x)w
t+1
s=1 s′=1 H
(cid:112)
= w⊤K w =∥L w∥ .
t+1 t+1 2
Substitutingtheseexpressionsfortheobjectiveandconstraintsinto(23)yields(10).
14TighterConfidenceBoundsforSequentialKernelRegression
B.4.ProofofTheorem4.5
Theoptimisationproblemmax {f(x)}canbewrittenas
f∈Ft
max f(x) s.t. ∥f −y ∥2 ≤R2, ∥f∥2 ≤B2. (24)
t t 2 t H
f∈H
TheLagrangianforthisproblemis
L(f,η ,η )=f(x)+η (R2−∥f −y ∥2)+η (B2−∥f∥2 ),
1 2 1 t t t 2 2 H
whereη ,η aretheLagrangemultipliers. Thedualfunctionis
1 2
g(η ,η )=max{L(f,η ,η )}.
1 2 1 2
f∈H
Byweakduality,thesolutionof(24)isupperboundedbythesolutionofthedualproblem,whichis
min {g(η ,η )}.
1 2
η1≥0,η2≥0
SincetheLagrangianL(f,η ,η )islinearintheLagrangemultipliersforeveryf ∈H,thedualfunctionisthemaximum
1 2
overasetoflinearfunctions, whichisaconvexfunction(seeEq. (3.7)inSec. 3.2.3of(?)). Thismeansthatthedual
problemisaconvexprogram. Wewillnowshowthat,foranyη >0andα>0,thereisaclosed-formexpressionforthe
1
dualfunctiong(η ,αη ). UsingLemmaB.1,theLagrangianevaluatedattheLagrangemultipliersη andαη is
1 1 1 1
L(f,η ,αη )=f(x)+η (R2−∥f −y ∥2+αB2−α∥f∥2 )
1 1 1 t t t 2 H
(cid:18) (cid:13) (cid:13)2 (cid:19)
=f(x)+η
1
R(cid:101) α2 ,t−(cid:13) (cid:13)(V t+αI H)1/2(f −µ α,t)(cid:13)
(cid:13)
.
H
UsingLemmaB.2andLemmaB.3,thedualfunctionevaluatedatη andαη is
1 1
g(η ,αη )=max{L(f,η ,αη )}
1 1 1 1
f∈H
=max(cid:8) k(·,x)⊤f −η 1(f −µ α,t)⊤(V t+αI H)(f −µ α,t)(cid:9) +η 1R(cid:101) α2
,t
f∈H
1
=k(·,x)⊤µ α,t+
4η
k(·,x)⊤(V t+αI H)−1k(·,x)+η 1R(cid:101) α2
,t
1
k(x,x)−k (x)⊤(K +αI)−1k (x)
=k t(x)⊤(K t+αI)−1y t+ t
4η
αt t +η 1R(cid:101) α2 ,t.
1
Therefore,thedualproblemis
(cid:26) k(x,x)−k (x)⊤(K +αI)−1k (x) (cid:27)
η1>m 0i ,αn
>0
k t(x)⊤(K t+αI)−1y t+ t
4η
1αt t +η 1R(cid:101) α2
,t
.
Usingthesubstitutionη =αη ,wearriveatthedualproblemstatedin(11). Byminimisingthisobjectivefunctionw.r.t. η
2 1 1
foranyfixedα,wecanalsoexpressthedualproblemas
(cid:40) (cid:41)
min k (x)⊤(K +αI)−1y +
R(cid:101) √α,t(cid:113)
k(x,x)−k (x)⊤(K +αI)−1k (x) .
α>0 t t t α t t t
15TighterConfidenceBoundsforSequentialKernelRegression
B.5.ProofofCorollary4.6
ProofofCorollary4.6. Forallf ∈F(cid:101)tandallx∈X,wehave
|f(x)−µ (x)|=|k(·,x)⊤(f −µ )|
α,t α,t
=|k(·,x)⊤(V +αI )−1/2(V +αI )1/2(f −µ )|
t H t H α,t
(cid:113) (cid:13) (cid:13)
≤ k(·,x)⊤(V +αI )−1k(·,x)(cid:13)(V +αI )1/2(f −µ )(cid:13)
t H (cid:13) t H α,t (cid:13)
H
1 (cid:113) (cid:13) (cid:13)
= √ k(x,x)−k (x)⊤(K +αI)−1k (x)(cid:13)(V +αI )1/2(f −µ )(cid:13)
α t t t (cid:13) t H α,t (cid:13) H
≤
R(cid:101) √α,t(cid:113)
k(x,x)−k (x)⊤(K +αI)−1k (x).
α t t t
ThefirstinequalityisduetotheCauchy-Schwarzinequality,andthefinalequalityisduetoLemmaB.3.
C.ConfidenceBoundTightness
Inthissection,weproveTheorem6.3. First,westateandprovesomeusefullemmas.
C.1.UsefulLemmas
LemmaC.1givesusalternativeexpressionsforthequadratictermsinR(cid:101)α,t,whichweusetoproveLemmaC.2.
LemmaC.1. Foranyα>0,wehave
(cid:18)
1
(cid:19)−1
y⊤y −y⊤Φ (V +αI )−1Φ⊤y =y⊤ K +I y .
t t t t t H t t t α t t
Proof. Westartwiththeidentity
Φ (V +αI )=(K +αI)Φ .
t t H t t
Bypost-multiplyingbothsideswith(V +αI )−1andpre-multiplyingbothsideswith(K +αI)−1,weobtain
t H t
(K +αI)−1Φ =Φ (V +αI )−1. (25)
t t t t H
Now,using(25),wehave
y⊤y −y⊤Φ (V +αI )−1Φ⊤y =y⊤y −y⊤(K +αI)−1K y
t t t t t H t t t t t t t t
=y⊤y −y⊤(K +αI)−1(K +αI−αI)y
t t t t t t
=y⊤y −y⊤y +αy⊤(K +αI)−1y
t t t t t t t
(cid:18)
1
(cid:19)−1
=y⊤ K +I y .
t α t t
LemmaC.2givesusasimplifiedexpressionforR(cid:101)α,twhenα=σ2/c. ThisallowsustocompareR(cid:101)α,ttheequivalentradius
quantitiesoftheconfidenceboundsin(Abbasi-Yadkori,2012)and(Chowdhury&Gopalan,2017).
LemmaC.2. Foranyc>0,setα=σ2/c. Wehave
(cid:113)
R(cid:101)α,t = σ2ln(cid:0) det(cid:0) σc 2K t+I(cid:1)(cid:1) + σ2 cB2 +2σ2ln(1 δ).
16TighterConfidenceBoundsforSequentialKernelRegression
Proof. FromLemmaC.1,withthiscandα,thesquaredradiusis
(cid:16) c (cid:17)−1 (cid:18) σ2 (cid:19)−1
R(cid:101) α2
,t
=y⊤
t
σ2K t+I y t−y⊤
t
y t+y⊤
t
Φ
t
V t+
c
I
H
Φ⊤
t
y
t
(26)
(cid:16) (cid:16) c (cid:17)(cid:17) σ2B2
+σ2ln det K +I +2σ2ln(1/δ)+
σ2 t c
(cid:16) (cid:16) c (cid:17)(cid:17) σ2B2
=σ2ln det K +I +2σ2ln(1/δ)+ .
σ2 t c
C.2.ProofofTheorem6.3
ProofofTheorem6.3. Foranyδ ∈(0,1]andanyλ>0,theUCBfromThm. 3.11of(Abbasi-Yadkori,2012)statesthat,
withprobabilityatleast1−δ,
R(cid:101)AY
∀t≥1: f∗(x)≤µ (x)+ √λ,tρ (x),
λ,t λ,t
λ
where
(cid:113) √
R(cid:101) λA ,Y
t
=σ ln(det( λ1K t+I))+2ln(1/δ)+ λB.
Wewillnowshowthatwhenwesetthecovariancescaletoc=σ2/λandsetα =σ2/c=λ,theRHSof(14)isstrictly
lessthanthisUCB.StartingfromtheRHSof(14)withc=σ2/λandα=σ2/c=λ,andthenusingLemmaC.2andthe
√ √ √
inequality a+b< a+ b(fora,b>0),wehave
(cid:113)
σ2ln(cid:0) det(cid:0)1K +I(cid:1)(cid:1)
+2σ2ln(1/δ)+λB2
µ (x)+ R(cid:101) √λ,tρ (x)=µ (x)+ λ t √ ρ (x)
λ,t λ,t λ,t λ,t
λ λ
(cid:113) √
σ
ln(cid:0) det(cid:0)1K +I(cid:1)(cid:1)
+2ln(1/δ)+ λB
λ t
<µ (x)+ √ ρ (x)
λ,t λ,t
λ
R(cid:101)AY
=µ (x)+ √λ,tρ (x).
λ,t λ,t
λ
Foranyδ ∈(0,1]andanyη >0,theUCBfromThm. 2of(Chowdhury&Gopalan,2017)statesthat,withprobabilityat
least1−δ,
∀t≥1: f∗(x)≤µ 1+η,t(x)+R(cid:101) ηIG ,tPρ 1+η,t(x),
where
(cid:113)
R(cid:101) ηIG ,tP =σ ln(det( 1+1 ηK t+I))+tη+2ln(1/δ)+B.
IfthenumberofroundsT isknowninadvance,thenonecansetηtotherecommendedvalueof2/T,andthenthetηterm
inR(cid:101)IGP isboundedby2forallt∈[T]. Wecompareconfidenceboundsforageneralvalueofη >0. Startingfromthe
η,t
RHSof(14)withc=σ2/(1+η)andα=σ2/c=(1+η),wehave
(cid:114)
(cid:16) (cid:16) (cid:17)(cid:17)
σ2ln det 1 K +I +2σ2ln(1/δ)+(1+η)B2
µ (x)+
√R(cid:101)1+η,t
ρ (x)=µ (x)+
1+η t
√ ρ (x)
1+η,t 1+η,t 1+η,t 1+η,t
1+η 1+η
(cid:32) (cid:115) (cid:18) (cid:18) (cid:19)(cid:19) (cid:33)
1
<µ (x)+ σ ln det K +I +2ln(1/δ)+B ρ (x)
1+η,t 1+η t 1+η,t
<µ 1+η,t(x)+R(cid:101) ηIG ,tPρ 1+η,t(x).
17TighterConfidenceBoundsforSequentialKernelRegression
D.RegretAnalysis
Inthissection,weprovethecumulativeregretboundinTheorem6.4. First,westateandprovesomeusefullemmas.
D.1.UsefulLemmas
LemmaD.1givesusanupperboundontheregretforeachround.
LemmaD.1(Per-roundregretbound). Foranyc>0andanyδ ∈(0,1],letx ,x ,... betheactionschosenbyKernel
1 2
CMM-UCB,KernelDMM-UCB(withanygridAthatcontainsα)orKernelAMM-UCB(withanyfixedα). Withprobability
atleast1−δ,
∀t≥1, r ≤
2R(cid:101)
√α,t−1ρ (x ).
t α α,t−1 t
Proof. WeprovetheresultforKernelCMM-UCB.ThesameargumentappliesforKernelDMM-UCBandKernelAMM-
UCB.UsingLemma4.2,foranyc>0,withprobabilityatleast1−δ,wehave
∀t≥1,∀x∈X, LCB (x)≤f∗(x)≤UCB (x).
Ft Ft
Sincex ,x ,... aretheactionschosenbyKernelCMM-UCB,wehavex =argmax {UCB (x)}. Therefore,
1 2 t x∈Xt Ft−1
r =f∗(x∗)−f∗(x )
t t t
≤UCB (x∗)−LCB (x )
Ft−1 t Ft−1 t
≤UCB (x )−LCB (x )
Ft−1 t Ft−1 t
≤
2R(cid:101)
√α,t−1ρ (x ).
α α,t−1 t
Thefinalinequalityuses(13)andtheanalogouslowerboundforLCB (x).
Ft
Weuseaversionofthewell-knownEllipticalPotentialLemma(seee.g. Lemma11.11in(Cesa-Bianchi&Lugosi,2006),
Lemma5.3in(Srinivasetal.,2010)andLemma11in(Abbasi-Yadkorietal.,2011)). TheversionstatedinLemmaD.2is
essentiallythesameastheversioninLemma5in(Whitehouseetal.,2023),soweomittheproof. Theonlydifferenceisthat
westartwith(cid:80)T min(cid:0) 1, 1ρ2 (x )(cid:1) insteadof(cid:80)T 1ρ2 (x ),andweusetheinequalitymin(1,x)≤ 3ln(1+x),
t=1 α α,t−1 t t=1 α α,t−1 t 2
forallx≥0(seeLemmaD.8in(Flynnetal.,2023))insteadofx≤2ln(1+x)forallx∈[0,1]. Thischangemeansthat
LemmaD.2holdsforα>0,ratherthanforα≥max(1,κ),whereκisanupperboundonsup |k(x,x′)|.
x,x′
LemmaD.2(EllipticalPotentialLemma). ForanyT ≥1andanyα>0,
T (cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
(cid:88) 1 3 1
min 1, ρ2 (x ) ≤ ln det K +I .
α α,t−1 t 2 α t
t=1
D.2.ProofofTheorem6.4
ProofofTheorem6.4. UsingLemmaD.1withα=σ2/c,withprobabilityatleast1−δ,wehave
2R(cid:101)σ2/c,t−1
∀t≥1, r ≤ ρ (x ). (27)
t (cid:112) σ2/c,t−1 t
σ2/c
FromAssumption6.2(f∗(x)∈[−C,C]),wealsohave
r ≤2C. (28)
t
Thecombinationof(27)and(28)yields
(cid:32) (cid:33) (cid:32) (cid:33)
2R(cid:101)σ2/c,t−1 1
r
t
≤min 2C,
(cid:112)
ρ σ2/c,t−1(x t) ≤2max(C,R(cid:101)σ2/c,t−1)min 1,
(cid:112)
ρ σ2/c,t−1(x t) .
σ2/c σ2/c
18TighterConfidenceBoundsforSequentialKernelRegression
StartingwiththeCauchy-Schwarzinequality,wehave
(cid:118) (cid:118)
T (cid:117) T (cid:117) T (cid:18) (cid:19)
(cid:88) (cid:117) (cid:88) (cid:117) (cid:88) 1
r
t
≤(cid:116)T r t2 ≤(cid:116)T 4max(C2,R(cid:101) σ2 2/c,t−1)min 1, σ2/cρ2 σ2/c,t−1(x t) .
t=1 t=1 t=1
(cid:113)
FromLemmaC.2, wehaveR(cid:101)σ2/c,t ≤ 2σ2γ t(σ2/c)+ σ2 cB2 +2σ2ln(1 δ) =: U σ2/c,t. SinceU σ2
2/c,t
ismonotonically
increasingint,wehave
(cid:118)
T (cid:117) T
(cid:88) (cid:117) (cid:88) (cid:16) c (cid:17)
r ≤(cid:116)T 4max(C2,U2 )min 1, ρ2 (x )
t σ2/c,t−1 σ2 σ2/c,t−1 t
t=1 t=1
(cid:118)
(cid:117) T
(cid:117) (cid:88) (cid:16) c (cid:17)
≤(cid:116)4T max(C2,U2 ) min 1, ρ2 (x ) .
σ2/c,T−1 σ2 σ2/c,t−1 t
t=1
Finally,usingtheEllipticalPotentialLemmaD.2,wehave
(cid:115)
(cid:88)T (cid:18) σ2(cid:19) (cid:18) (cid:18) σ2(cid:19) σ2B2 (cid:18) 1(cid:19)(cid:19)
r ≤ 12Tγ max C2,2σ2γ + +2σ2ln .
t T c T c c δ
t=1
E.AdditionalPracticalDetails
Inthissection,wecommentonsomepracticaldetailstoaidwithreproducibility.
E.1.SampleCVXPYCodeForSolving(10)
InFig. 3,weprovidesomesampleCVXPYcodeforcomputingthesolutionofthesecondorderconeprogramin(10). wis
theoptimisationvariablew ∈Rt+1;k t1isk (x);k tt1isthet×(t+1)kernelmatrixK ;y tisthereward
t+1 t,t+1
vectory ;R tistheradiusR ;BisthenormboundB;l t1istherightCholeskyfactorL ofthe(t+1)×(t+1)
t t t+1
kernelmatrixK (sol t1isanuppertriangularmatrix).
t+1
import cvxpy as cp
_w = cp.Variable(t+1)
obj = k_t1.T @ _w
cons = [cp.norm(k_tt1 @ _w - y_t) <= R_t,
cp.norm(l_t1 @ _w) <= B]
prob = cp.Problem(cp.Maximize(obj), cons)
ucb = prob.solve()
Figure3. Pythoncodeforsolving(10)withCVXPY.
Thevariableucbwillnowbeequaltothenumericalsolutionofmax {f(x)}. Inpractice,wefindthatitishelpfulto
f∈Ft
addasmallmultipleoftheidentitymatrixtothekernelmatrixK beforecomputingtheCholeskydecomposition. The
t+1
reasonisthat,duetonumericalerror,smallpositiveeigenvaluesofK canappeartobenegative. Onecouldavoidthe
t+1
necessityofcomputingtheCholeskydecompositionofK byexpressingthesecondconstraintasw⊤K w ≤ B2
t+1 t+1
(whichisstillconvexinw). However,theconicformof(10)(withnormconstraints)isfavourablefortheconicsolvers
usedbyCVXPY.
E.2.RecursiveUpdates
ForamoreefficientimplementationofourKernelCMM-UCB,KernelDMM-UCBandKernelAMM-UCBalgorithms,the
inverse(K +αI)−1,thelogdeterminantln(det(1K +I))andtherightCholeskyfactorL canbeupdatedusingthe
t α t t
19TighterConfidenceBoundsforSequentialKernelRegression
followingrecursiveformulas. TheseupdaterulescanbederivedusingpropertiesoftheSchurcomplement(Zhang,2006),
andhavebeenusedbeforeinotherkernelbanditalgorithms(seee.g. App. Fof(Chowdhury&Gopalan,2017)).
Fortheinverse,wehave
1
K = ,
22 k(x ,x )+α−k (x )⊤(K +αI)−1k (x )
t+1 t+1 t t+1 t t t+1
K =(K +αI)−1+K k (x )⊤(K +αI)−1k (x ),
11 t 22 t t+1 t t t+1
K =−K (K +αI)−1k (x ),
12 22 t t t+1
K =−K k (x )⊤(K +αI)−1,
21 22 t t+1 t
(cid:20) (cid:21)
K K
(K +αI)−1 = 11 12 .
t+1 K K
21 22
Forthelogdeterminant,wehave
(cid:18) (cid:18) 1 (cid:19)(cid:19) (cid:18) (cid:18) 1 (cid:19)(cid:19) (cid:18) k(x ,x )−k (x )⊤(K +αI)−1k (x )(cid:19)
ln det K +I =ln det K +I +ln 1+ t+1 t+1 t t+1 t t t+1 .
α t+1 α t α
FortherightCholeskyfactor,wehave
(cid:34) L k (x )⊤L−1 (cid:35)
t t t+1 t
L = (cid:113) .
t+1 0 k(x ,x )−∥k (x )⊤L−1∥2
t+1 t+1 t t+1 t 2
Notethatthevectork (x )⊤L−1isthetransposeofv,wherevisthesolutionofL⊤v =k (x ). vcanbecomputed
t t+1 t t t t+1
using numerical methods for solving lower triangular systems (see e.g. linalg.triangular solve in the SciPy
library),soitisnotnecessarytoinvertL .
t
F.AdditionalExperimentalResults
Inthissection,wepresentsomeadditionalexperimentalresults.
F.1.TimePerStep
Weruneachmethodinasynthetickernelbanditproblem,asdescribedinSec.7. WeusetheMate´rnkernelwithsmoothness
ν =5/2andlength-scaleℓ=0.5.
Figure4.Wall-clocktimeperstepforourkernelUCB-stylealgorithmsaswellasAY-GP-UCB,IGP-UCBoverT =1000roundswith:
(left)originaly-axis;(middle)zoomed-iny-axis;(right)logscaley-axis.Weshowthemeanover10repetitions.
WefindthatKernelDMM-UCBandKernelAMM-UCBhaveasimilarcomputationalcosttoAY-GP-UCBandIGP-UCB,
whereasKernelCMM-UCBhasamuchgreatercomputationalcost.
F.2.2-Dimensionaland4-DimensionalActions
WedisplaythecumulativeregretofeachmethodinthesynthetickernelbanditproblemsdescribedinSec.7. Here,theaction
setX ineachroundtconsistsof100randomvectorsinthe2-dimensionalhypercube[0,1]2(Fig. 5)orthe4-dimensional
t
20TighterConfidenceBoundsforSequentialKernelRegression
hypercube[0,1]4(Fig. 6).
Figure5.CumulativeregretofourKernelUCB-stylealgorithmsaswellasAY-GP-UCB,IGP-UCBandarandombaselineoverT =1000
roundsford=2andvariouskernels(columns)andlengthscales(rows).Weshowthemean±standarddeviationover10repetitions.
In Fig. 5, we observe that our Kernel CMM-UCB and Kernel DMM-UCB algorithms achieve the lowest cumulative
regret,followedbyKernelAMM-UCBandtheAY-GP-UCBandIGP-UCB.Allmethodsperformednoticeablybetterwith
2-dimensionalactionsetsthanwith3-dimensionalactionsets(seeFig. 2).
Figure6.CumulativeregretofourKernelUCB-stylealgorithmsaswellasAY-GP-UCB,IGP-UCBandarandombaselineoverT =1000
roundsford=4andvariouskernels(columns)andlengthscales(rows).Weshowthemean±standarddeviationover10repetitions.
InFig. 6,weobserveasimilardropintheperformanceofallmethodswhenmovingupto4-dimensionalactionsets. In
thetoprowofFig. 6,inwhichthekernellength-scaleisℓ=0.5,weobservethateachmethodstillachievesmuchlower
cumulativeregretthantherandombaseline,andKernelCMM-UCBandKernelDMM-UCBalgorithmsstillachievethe
lowercumulativeregret. However,inbottomrowofFig. 6,weseethatnomethodperformsmuchbetterthantherandom
baseline.
F.3.LooseUpperBoundsonBandσ
Toruneachkernelbanditalgorithm,weonlyneedtoknowupperboundsonthesub-GaussianparameterσandtheboundB
ontheRKHSnormoftherewardfunctionf∗. Wenowinvestigatewhathappenswhentheknownboundsononeorbothof
thesequantitiesareloose.
WeruneachalgorithminthesamesynthetickernelbanditproblemasdescribedinSec.7,wherethekernelisaMate´rn
kernelwithsmoothnessν =5/2andlength-scaleℓ=0.5,andtheactionsare3-dimensionalvectorsin[0,1]3. Asbefore,
21TighterConfidenceBoundsforSequentialKernelRegression
thenoisevariableareσ-sub-Gaussianwithσ =0.1and∥f∗∥=10. Weruneachalgorithmwithallfourpairsofvaluesof
(σ,B),whereσ ∈{0.1,0.2}andB ∈{10,20}.
Figure7.CumulativeregretofourkernelUCB-stylealgorithmsaswellasAY-GP-UCB,IGP-UCBandarandombaselineafterT =1000
roundsforvariouskernels(columns)andlengthscales(rows). Theactionsetsaresubsetsof[0,1]4. Weshowthemean±standard
deviationover10repetitions.
Fig. 7showsthecumulativeregretofeachmethodandwitheachcombinationofcorrectorlooseσandB. Weobservethat
usingalooseupperboundonσhasarelativelysmalleffectoneachmethod,whereasalooseboundonB cancausethe
cumulativeregretofeachmethodtogrowconsiderably.
22