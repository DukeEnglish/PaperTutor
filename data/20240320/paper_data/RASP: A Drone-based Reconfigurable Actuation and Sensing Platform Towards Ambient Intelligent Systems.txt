RASP: A Drone-based Reconfigurable
Actuation and Sensing Platform
Towards Ambient Intelligent Systems
‚àó + ‚àó ‚àó +
Minghui Zhao , Junxi Xia , Kaiyuan Hou , Yanchen Liu , Stephen Xia ,
‚àó
Xiaofan Jiang
‚àóColumbiaUniversity+NorthwesternUniversity
{mz2866,kh3119,yl4189}@columbia.edu,junxixia2024@u.northwestern.edu,stephen.xia@northwestern.edu,
jiang@ee.columbia.edu
ABSTRACT
Realizingconsumer-gradedronesthatareasusefulasrobot
vacuumsthroughoutourhomesorpersonalsmartphonesin
ourdailylivesrequiresdronestosense,actuate,andrespond
to general scenarios that may arise. Towards this vision,
we propose RASP, a modular and reconfigurable sensing
andactuationplatformthatallowsdronestoautonomously
swap onboard sensors and actuators in only 25 seconds,
allowingasingledronetoquicklyadapttoadiverserangeof
tasks.RASPconsistsofamechanicallayertophysicallyswap
sensormodules,anelectricallayertomaintainpowerand Figure1:RASPautonomouspayloadreconfiguration
communicationlinestothesensor/actuator,andasoftware toexecuteuserspecifiedtask
layer to maintain a common interface between the drone
andanysensormoduleinourplatform.Leveragingrecent
commands.Theutilityofsuchaplatformdependsonthe
advancesinlargelanguageandvisuallanguagemodels,we
quantityandqualityoftasksthedroneisequippedtohandle,
furtherintroducethearchitecture,implementation,andreal-
whichisheavilycorrelatedwiththenumberofsensingand
worlddeploymentsofapersonalassistantsystemutilizing
actuation modalities it can support; the number of tasks,
RASP.WedemonstratethatRASPcanenableadiverserange
generalityofadrone,andusefulnessofadroneincreases
ofusefultasksinhome,office,lab,andotherindoorsettings.
witheachnewsensingandactuationmodalityitsupports.
We propose RASP, an adaptive, modular, and
1 INTRODUCTION
reconfigurable actuation and sensing platform that
Dronesareexperiencingincreasedutilizationacrossnumer- allows the same drone to adapt to diverse scenarios by
ous industries, including search and rescue, construction, quickly and efficiently switching out sensors and/or
delivery,agriculture,andmanymore[9,17,20,21,42,44,46]. actuators depending on the situation. Figure 1 shows
Theglobalcommercialdronemarketisexpectedtogrowto an overview of the workflow of RASP. The drone lands,
over580billionUSDby2030[1].Whilecommercialdrones communicates with the base station, and specifies the
aremakingsignificantimpactsacrossvarioussectors,con- module to swap in. The base station secures the drone
sumerdrones(suchasthosefromDJI[39]orSkydio[40]), andactuatestheconveyorbeltandgrippertoremoveand
thataremorecommonineverydaylife,tendtobeconfined replace the onboard sensor/actuator with the requested
tohobbyistorentertainmentpurposes,suchasaerialpho- module. The unified module interface (UMI) ensures that
tography and videography. However, we believe that the the sensor/actuator module is firmly attached, connected,
potentialofdronesindailylifeasaversatileplatformformo- andcanbeaccessedbythedrone.Finally,thedronefliesoff
bilesensingandactuationextendsfarbeyondtheselimited withthenewmoduleforitsnewtask.
applications. Ourapproachofmodularizingadrone‚Äôssensorsandactua-
Imagineadroneineveryhome,muchlikerobotvacuum torsoffersanumberofadvantagescomparedtoastaticdrone
cleaners,oreachpersonhavingadroneactingasapersonal with all sensors and actuators attached. Much like single-
‚Äúflyingsmartphonewithinterchangeableattachements‚Äùthat lensreflex(DSLR)cameraswithinterchangeablelenses,1)
youcandirectthroughavarietyofinterfaces,suchasvoice modularizationallowsforthecreationofanecosystemof
4202
raM
91
]OR.sc[
1v35821.3042:viXraZhaoetal.
sensorsandactuators.Thisallowsconsumerstopurchase thatoperateonevenlowerresourcemicrocontrollers,with-
onlythesensorsandactuatorstheyneedandimprovedrone outanoperatingsystem,thatarelessflexibleinthenumber
reusability,expendability,andsustainability.Astaticdrone ofinterfacesandconfigurationstheysupport[48].[49,50]
maynothaveallthesensorsandactuatorsyoumayneed are reconfigurable sensing platforms based on the Rasp-
orwant,whichrequiresyoutopurchaseacompletelynew berry Pi that have unified and generic hardware inter-
drone,ratherthanjustthesensororactuator.2)Layeringand faces, allowing sensors to use the same set of connectors
modularizationenablesyoutoevolvethedrone,sensors, even if they are interfaced differently in software (e.g.,
andactuatorsindependently.Newsensorsandactuators UART, SPI, or I2C). Several platforms leverage the Berke-
purchasedwillstillbecompatiblewithyourcurrentdrone leyTinyOSoperatingsystem[24],whichallowdevelopers
aslongastheinterfaceremainsthesameandviceversa.3) to develop extremely long-lasting applications with great
Adronecarryingasinglesensororactuatorcanbedesigned flexibility[10,12,15,30,31,36].Despitetheprevalenceofre-
muchsmaller,moreagile,lessnoisy,andmoresuitablefor configurableandmodularsensingplatforms,therecurrently
closedenvironments,suchasindoors. doesnotexistanyplatformthatallowdronestoreconfigure
WebelieveRASPtobeonecrucialsteptowardsgeneral andadaptitsonboardsensorsandactuatorsautomatically.
droneplatformscapableofaidinginmanydailyactivities, Theideaofasmarthomeandothersmartenvironments
muchliketheroleofsmartphones.Todemonstratethis,we hasexistedformanydecades.Therearemanysmarthome
prototypeandshowhowRASPcouldbeintegratedintoa systemsdevelopedbyresearchgroups[11,16,32]andcom-
personalassistantsystemthatalsoleveragesstaticsensorsin mercial products from companies like Google [28], Ama-
theenvironment(cameras)inconjunctionwithfoundational zon[7],andApple[18].Mostoftheseleveragestaticsensors
modelsand penetrativeAI [47]to satisfya widerange of andactuatorsatspecificpartsofthehometosenseandre-
useful tasks in a home, lab, or office setting. For example, spondtohomeorhumanactivity.Thoughthereareafew
adronecouldcarryatemperaturesensortodeterminethe upandcomingproducts(e.g.,RingAlwaysHomeDrone[5])
warmestplacetosit,beforeswappinginapayloadmoduleto andresearchworks[45]thatleveragedronesforindoorand
carryasnacktoanoccupant.Tothebestofourknowledge, homescenarios,moreresearchisneededinallareas(e.g.,
thisdeploymentisthefirstend-to-endarchitectural,me- control,localization,perception,understanding)torealize
chanical,andsoftwaredesigntoautonomouslycontrol afulldrone-basedsystemthatcanoperateautonomously
andreconfiguredronesforversatiletasksleveraging andtakeadvantageofthesensingandcomputefromstatic
foundationalmodels.Ourcontributionsareasfollows. sensorsthroughoutindoorenvironments.[45]recentlypro-
1.WecreateandimplementRASP,amodularandreconfig- posedareconfigurabledroneplatform,butisonlyademo
urablesensingandactuationplatformthatallowsasingle abstractandleavesoutmanydetailsonhowitautonomously
dronetoadapttoawiderangeofscenarios.RASPallows swapsmodules.
drones to dynamically change their onboard sensing and Languageandfoundationalmodelshaveseenasurgein
actuationmodulesinunder25seconds. usageandresearchduetotheirpowerfulcapabilitiesinal-
2.TorealizeRASP,weproposeathree-layerarchitecture lowing computers to understand and interact using natu-
thatphysicallyreplacesonboardmodules,maintainsdata ralhumanmodesofcommunicationinanunprecedented
connectionlines,andenablesdronestocommunicatewith manner.Thereareafewworksthatleveragelanguagemod-
sensingandactuationmodulesthroughaunifiedinterface. els as an interface for people to interact with their smart
3.ToshowtheimportanceofRASPforfutureambientin- homes[22,34,35]ordronesandotherrobotics[43].How-
telligentsystems,weprototypeandintegrateRASPintoa ever,tothebestofourknowledgetherearenoworksthat
personalassistant systemthattakesadvantageofstaticsen- attempttorealizeasmartdrone-basedsystemforindoorset-
sorsintheenvironment(cameras),largelanguageandvision tingsthatjointlyutilizesthestrengthsoflanguagemodels,
language models, and the mobility of a drone to realize a staticinfrastructure,andareconfigurabledroneplatformto
widerangeofusefulapplicationsinsmarthome,lab,office, performadiverserangeofusefultasksforoccupants.
andindoorsettings.
3 MODULARANDRECONFIGURABLE
SENSINGANDACTUATIONDRONE
2 RELATEDWORKS PLATFORM
Therearemanyreconfigurableandmodularsensingplat- Inthissection,weintroducethekeycomponentsofRASP,
formsinexistence.Inadditiontotheplatformsprovidedby ourreconfigurableandmodularsensingandactuationplat-
open-sourcedo-it-yourself(DIY)electronicvendors,such form and the drone system that it is built upon. Figure 2
asAdafruit[19]andSparkfun[13],therearealsoplatforms showsthearchitectureofRASP.AtahighlevelthereareRASP:Drone-basedReconfigurableActuationandSensingPlatform
Total Module Fly Drone Module Cost
Sensor
Mass Mass Time Power Power
Driver Task
Failure Execution DroneOnly 344.7g - 3m47s 195.4W - $344
Detection PM2.5 411.9g 67.2g 3m16s 226.0W 0.29W $40
Data Temp&Moisture 372.3g 27.6g 3m43s 198.8W 3.3ùúáW $12
Storage Drone LightSensor 372.8g 28.1g 3m43s 199.3W 10ùëöW $10
Data Navigation
Processing CO2 372.8g 28.1g 3m42s 199.8W 86ùëöW $16
Alcohol 376.2g 31.5g 3m39s 201.7W 0.75W $8
Mechanical Electrical Software Drone
Actuator 394.8g 50.1g 3m08s 235.1W 1.22W $7
Figure2:RASParchitecture. Table1:RASP‚Äôssupportedsensors/actuatorsandtheir
mass,powerconsumption,andcost.
three major conceptual architectural components: the
ground station or mechanical layer, electrical connection
layer,andthesoftwarelayer.Theseconceptualarchitectural
canhoverfornearly4minutes,whichiscomparabletothe
componentsareimplementedacrossfivephysicalplatform
Crazyfliedrone.
components: the ground station (Figures 3e and 5), sen-
Carrierboard.Thecarrierboard,whichisattachedtothe
sor/actuationmodules(Figures3c-dand4),unifiedmodule
base of the drone, provides the physical data, power, and
interface(UMI)(Figure6),thecarrierboard(Figure3b),and
communication connections between the drone platform
thephysicaldrone(Figure3a).
andeachsensor/actuationmodule.Weimplementthecarrier
boardonalightweight$15RaspberryPiZero2W[4],which
3.1 Architecture
hasconnectionsforonesensororactuationmodule.
MechanicalLayer.Toenablereconfigurability,thereneeds
Sensor, Actuation, and Recharging Modules. RASP
tobeamechanismtophysicallyswapoutdifferentsensor
comes with a collection of sensor and actuation modules
modules;themechanicallayerisresponsibleforthis.The
(fulllistinTable1).Thesemodulesareattachedordetached
dronewilllandonthegroundstation,whichwillphysically
fromthedronebythegroundstation/mechanicallayer.The
detachthecurrentmoduleonboardthedroneandattachthe actuationmodule,showninFigure3d,isacontainerstruc-
newlyrequestedmodule.
turewithamotorthatopensandclosesahatch.Smallitems
ElectricalConnectionLayer.Eachsensorandactuation
(e.g., medication, candy, pet food, etc.) can be loaded into
moduleneedstomaintaindata,communication,andpower
thismoduleforthedronetodeliver.
tothedrone,despitebeingdetachable;Theelectricallayer
Additionally,wecreateamodulethatrechargesthebattery
maintains these connections. We leverage magnets to ac-
ofthedrone,whichgetsattachedandremovedjustlikeother
complishthis,maintainingphysicalconnectionbetweenthe
sensorsandactuators.Unlikeotherstandalonesensorsand
moduleandthedroneduringamission,whilestillallowing
actuators,therechargingmoduleisconnectedtothewall-
themechanicallayertoeasilyswapoutmodules.
poweredgroundstationandchargesthedrone‚Äôsbatteryat
SoftwareLayer.Inadditiontomaintainingphysicaland
arateof1.7A.Atthisrate,thefullydepleteddronecanbe
electricalconnections,theplatformneedstoadapttonew
fullychargedin30minutes.Therechargingmoduleisthe
moduleson-the-flyeveniftheyareswappedout.Thesoft-
defaultmodulethat‚Äôsattachedtothedroneeachtimethe
warelayerprovidesstandardizedprotocolsandlibrariesthat
dronefinishesatask,returnstothegroundstation,andhas
allowthedronetodiscover,configure,control,andreceive
nootherpendingtasks.
datafromattachedmodules.
Tomakeiteasierforthegroundstationtopickupamod-
ule,we3DprintedandattachedastructureshowninFigure4.
3.2 PlatformComponents
Weperformedforceanalysis(usingSOLIDWORKSSimula-
Droneplatform.Wedesignedthedronebasedontheopen- tionXpress)toselecttheoptimalstructureandthicknessfor
sourceCrazyfliedrone[2],asshowninFigure3.Wechose theframethatisaslightaspossibletominimizeextraweight
thelightweight(27g)Crazyflieasourfoundationduetoits whilebeingsturdyenoughtoavoidbeingdeformedwhen
compactsize,makingiteasiertooperateinallenvironments, thegripperholdsit.Wemeasuredthatthegrippersappliesa
notjustinopenoutdoorspaces.Toincreaseitspayload,we 5Nforceontothesidesoftheframewhenitgripstheframe
replaceditsbrushedmotors(19000Kvpoweredbya1-cell anda10Natthebottomoftheframewhenitbringsthe
battery)withmorepowerfulbrushlessmotors(3800Kvpow- moduledowntotheconveyorbelt.Toavoidbendingatthese
eredbya4-cell850mAhbattery).Wealsoprintedaprotective forces,ourforceanalysisshowedwhentheframeis1.5mm
guard,notonlytoprotectitspropellers,buttoprotectpeo- thick,ithadafactorofsafety(FOS)greaterthan1inboth
pleandotherobjectsfromitspropellerblades.Thedrone cases,indicatingthattheframewasabletowithstandtheZhaoetal.
Flight
 5.6cm
Controller
Motor and
 2.9cm
Propellers 15.5cm
13.6cm
Battery

and RPi
Sensor and

Actuator
 Closed Opened
Attachment
(a) (b) (c) (d) (e)
Figure3:a)Physicaldronesystem,b)Carrierboard,c)Severalsensormodules,d)Actuationmodule,e)Gripperfor
swappingsensorandactuationmodules
Material
 (Figure5b).WealsoprintandattachtwoArUcomarkersto
Deformation

(mm)
1.0 the bottom side of the drone to easily detect the position
0.8 ofthedroneanditsorientation.ArUcomarkers,commonly
0.6 usedforcameraposeestimation,aresimilartoQRcodes,
0.4 butcarrylessencodedinformation,whichmakesthemmore
0.2 computationallyefficienttodetect[38].Infuturework,we
0.0
(a) (b) (c) plantoleveragemorecomplexcomputervisionmodelsto
automaticallydeterminethepositionandorientationofthe
dronewithoutneedingtoaddadditionalmarkings.
Figure4:a)3Dstructureattachedtomodulestoallow
Removinganattachedsensorandensuringthatthenew
theRASPtoeasilypickthemup,b)andc)aredisplace-
sensorissecurelyfastenedrequiresthedronetolandpre-
ment plots from stress analyses; the corresponding
ciselyatthespecificlocationwheretheinterchangeoccurs.
lowestfactorsofsafety(FOS)are1.180and4.664.
However,precisedronelandingandalignmentisextremely
difficult because nonlinearities in drone stability become
forcesapplied.Themassofthisframeis11.97g,whichis morepronouncedasthedronemovesclosertotheground
negligiblewhencomparedtotherestofthedrone(344.7g). (groundeffect)[29].Ratherthancreatingaflatlandingpad
Groundstationandautomatedtakeoffandlanding.The thatissimilartothelandingpadsforhelicopters,wetake
groundstationprimarilyspansthemechanicallayer,consist- inspirationfromtheRingAlwaysHomeDrone,whichuses
ingofagrippermechanismandaconveyorbelttoposition afunnel[5].Aslongasthedronelandswithinthelarger
modulestoswapin/out.Wecreatedtheplatformleveraging radiusofopeningofthefunnel,thedronewillautomatically
the chassis of the open-source Ender-3 3D printer [3] be- slidetowardsthebottomofthefunnelwithanopeningthat
causeitprovidesenoughspaceforthedronetoland,aswell latchesontotheonboardmodule.
asenoughmechanicalcontroltoremoveonboardsensors Wetestandbenchmarktwodesignsforthelandingsta-
andattachnewsensorsontothedrone. tion,asshowninFigure5c.Onedesignisanormalfunnel
Wedesignedandbenchmarkedthreedifferentgrippers (Figure5c-top).Theseconddesigncontainsgroovestohelp
with different slots and grooves and measured the offset thedroneslotintoplace(Figure5c-bottom);thebaseofthe
fromthecenteroftheparkeddroneonthegroundstation. dronealsohasprotrudinggroovesthatmatchthegroovesin
Theoptimaldesign,showninFigure3e,helpsthemodule thefunnel.Foreachdesign,wehadthedronelandandswap
slideintoacenteredpositionevenifthemoduleisslightly modules75timesandfoundthattheswapratesarefairly
offsetbecauseoferrorsinactuatingtheconveyortomove similar. However, the grooved design corrects for greater
themoduleintoplace. dronemisalignmentsfromtheplatform(highermaximum
Thegroundstationcontainstwoadditionalcomponents: tolerableorientationandoffseterrorsinFigure5d).Assuch,
the communication hub and the landing station. The com- weadoptthegrooveddesignintoRASP.
munication hub is the central processor that controls the UnifiedModuleInterface(UMI).TheUMI(Figure6)spans
swappingprocess.Dronescommunicatewiththishubprior boththeelectricalconnectionlayerandthesoftwarelayer,
tolandingtospecifywhichmoduleitrequiresandtoinitiate givingthecarrierboardonthedroneastandardizedway
theswappingprocess. to communicate with a variety of sensing and actuation
The drone then lands at the landing station before the modules.TheUMIhasthefollowingdesignfeatures.
mechanical layer begins the swapping process. The com-
municationhubguidesthedroneintopositionforlanding
usingtwocamerasfacingupontopofthegroundstationRASP:Drone-basedReconfigurableActuationandSensingPlatform
ArUco

Left Camera Right Camera
Marker Maximum
 Maximum
 Swap

Landing

Tolerable
 Tolerable
 Success
Landing
 Platform Orient. Err Offset Rate
t=0, move left
Platform Type 1 22.5¬∫ 30mm 93%
Type 2 27.5¬∫ 45mm 94%
Camera t=1
(a) (b) (c) (d) (e)
Figure5:Differentlandingplatformsdesignedandbenchmarked.Theaveragelandingtimetook7.8seconds.
Permanent Magnets Sensor and Actuator Drivers Permanent
 hereeverytimeanewmoduleisattachedandautomatically
Magnets downloadsthecorrespondingdriverfromarepository.
12-Pin

Magnetic

Connector
Connects to

Drone Battery 4 TOWARDSAFULLYAUTONOMOUS
EEPROM DRONE-BASEDPERSONALASSISTANT.
Connects to

Unified

Drone RPi 0 2W
Module ADC Adrone-basedpersonalassistantisoneofthecoreambient
Power Regulator
intelligentsystemsthatweseeRASPenabling.Forseveral
decades now, the number of smart devices, sensors, and
Figure6:Universalmoduleinterfaceforconnecting
actuatorsdeployedthroughoutourhomes,offices,andenvi-
and disconnecting sensors and actuators, as well as
ronmentshasbeengrowingexponentially[14].Thesestatic
rechargingthedrone.
sensingdevicescanprovideacoarse-grainedandlocalized
pictureofeventsandconditionswithintheenvironment;ob-
tainingafine-grainedhighresolutionpicturewouldrequire
Standardized24-pinmagneticconnections:Eachsensorand densedeploymentsofsensors,coveringeverysquareinchof
actuationmoduleinRASPcomesequippedwithtwomag- theenvironment,whichisimpractical.Webelievethatadapt-
neticconnectorseachwith12spring-loadedpins,andthe ingRASP,drones,andothermobileroboticsintosmartenvi-
carrier board has another set that matches. The magnets ronmentsandecosystemscouldenablefine-grainedsensing
alloweachmoduletoremainattachedandconnectedtothe andactuationatthetipofourfingerswithouttheneedfor
droneduringflight,whilebeingpliableenoughtobeeasily densestaticdeployments.
removedbythemechanicallayerifamoduleneedstobe Figure7showsthearchitectureandworkflowofourpro-
switched.Thespring-loadedpinsenableselectricalconnec- totypedrone-basedpersonalassistant.Staticcamerasinthe
tionstothesensorsandactuators,whileallowingtheground environmentprovideahigh-levelandcoarse-grainedview
stationtochargethedrone. oftheenvironment;infutureworkandsystems,weenvision
Sensorandactuatordrivers:Thedroneneedstosupporta RASP being compatible with other static sensors, beyond
varietyofdifferentsensorsandactuators,eachofwhichmay cameras.Whenausergivesavoicecommand,thesystem
beinterfacedwithdifferently.Tosupportthis,RASPimple- leveragesavisual-languagemodeltoanalyzethescene,in-
mentsalibraryofstandarddrivers,aspartofthesoftware terpretthecommand,andoutputalistofpotentiallocations
layer,thatsupportdifferentcommunicationandsampling ofinterest,alongwiththerelevantsensororactuator.Then,
schemes,namely:I2C,SPI,UART,GPIO,andADC.Wealso thesystemactuatestheRASPdronewiththesensor/actuator
providedeveloperswithadrivertemplatethatallowsthem toeachofthelocationstocompletethetask.
to easily incorporate new sensors and actuators into the Forexample,onetaskorquestionapersonmaywantto
RASPecosystem. knowis‚Äúwhereisthewarmestplacetosit‚Äù?AstandardRGB
Plug-and-playcontroller: Once a new module attaches to camerawouldnotbeabletoaccuratelymakethisdetermina-
the carrier board, the drone needs to be able to start col- tionwithoutadditionalsensors(e.g.,temperature).Thevoice
lecting data or using the actuator automatically, without command and images are analyzed by a visual-language
needing to manually run any command. To enable this model, which outputs potential locations where a person
feature, we include a non-volatile memory chip on each couldsit(e.g.,chairsandcouches).Then,thesystemoutfits
module(M24C02[41]2KbI2Cbuselectricallyerasablepro- theRASPdronewithatemperaturemoduleandactuatesthe
grammableread-onlymemory(EEPROM)),whichstoresthe dronetoeachoftheselocationstomeasuretemperature.The
identityofthemodule.Thecarrierboardwillpulldatafrom timeseriesdatafromthetemperaturesensorwillthenbeZhaoetal.
Network of Bird‚Äôs-Eye
View Camera on Ceiling
Object / location identification‚Ä®
- Where is warmest to sit?‚Ä®
- Where is my phone 
State of something‚Ä® Standby

- Is the stove still on?‚Ä® Wait for event
- Did I turn off the faucet 
Surveillance‚Ä® User
 Zero-shot Model Based Task Specification
- Monitor my chemical experiment for spills‚Ä® Instruction
- Watch out my cooking food in the wo  Dispatch
 Automatic TTaasskk Fly to Survey
Aerial Actuation‚Ä® Drone Payload EExxeeccuuttiioonn Self Navigation Self Data
- - B Puri tn sg o s mn ea c rk at t po om isy o np e nt‚Ä® ext to the cabinet
 Se Gg rm oe un L nt L d a iA V nn A
 gy Dt Ih Ni Ong
 ‚Äúfl m a fy e c l a t yt s u o u a t r t o( e e x ., X X . - - .y >
 >
 ‚Äù)->
 S Ie nle sc tati lo lan t ia on nd w Vi it eh w B i Crd am‚Äôs- ee ry ae wA itn ha Gly Ps Tis
 -4
Figure7:SystemarchitectureofintelligentassistantwithRASP.
analyzedbyanotherlanguage/foundationalmodeltomake LLaVA Prompt Generator GPT4 API Prompt
thefinaldetermination.Therearefourclassesoftasksthat Fixed Prompt System Prompt
"ÓÅâis image is a topdown view of the {location} taken from the "You are an expert in data analysis and time
oursystemcanaccommodate. camera installed on the ceiling. " s ae nr die as n i an lt ye zr ep r se et na st oio rn d. aY to au t ok n do erw iv h e o mw e ato n ip nr gocess
Task Prompt insights. "
Object/LocationIdentification.Thissetoftasksrequires - Object Location Identification:
"I would like to know {voice command}, and which type of
thesystemtoobservetheenvironmentandidentifyanobject sensor should I use to beÓÄºer assist this task? " Task Prompt
- State of Something: "ÓÅâere are three files collected from {sensor
"I need to check the state of {voice command}, and what used} for determining {task}. ÓÅâe xx_before.json
orlocationbasedontheuser‚Äôscommand.Forexample,the sensor should I use to determine if it has occurred?" is taken along the way from a space with
- Surveillance: normal condition to the target place; the
command‚Äúwhereismyphone‚Äùrequiresthevisual-language "ÓÅâis image concatenates two images taken by a camera at xx_at.json is taken at the target place; the
different time: the top image was captured first, and the boÓÄºom xx_aÓÄπer.json is taken from the target place back
image was captured later. What are the differences between to the space with normal condition. "
modeltoidentifypotentiallocationsforwherethephone these two images?
- Aerial Actuation:
could be, before sending a drone with a camera to take a "I need the drone to {voice command}" Chain of ÓÅâought Prompt
"You should reason step-by-step to determine
closerlookandverify. Chain of ÓÅâought Prompt t fih ne d b oe ust t w {taa sy k }t .o " analyze the {sensor used} data to
"You should think step-by-step and show all of your work in the
Object/LocationState.Thissetoftasksinvolveslearning explanation. Be very thorough and explicit in your explanation."
Response Format Prompt
aboutthestateorconditionofaspecificobjectorlocation. Response Format Prompt " inY co lu ur d r ee ss : ponse should in json format that
"Your response should only be: Analyze_process: ‚Ä¶ (brief summary of steps)
Forexample,‚Äúismyfoodburning‚Äùwouldrequirethesystem R Tae ra gs eo tn : : ‚Ä¶ ‚Ä¶ (. t( hr ee a os bo jn e cf to or fd ie nt te er rm esin t)e the object and sensor) R Ce oa ns co lun s: i‚Ä¶ on. :( r ‚Ä¶ea (s fio nn a w l ait nh s wnu erm fb oe rr t han e a tl ay ss ki )s )
Carrier: ‚Ä¶ (sensor or payload to carry)"
toidentifyfoodthatisbeingcooked,beforesendingadrone
withaCO2sensortodeterminethestateofthefood.
Surveillance.Thesetasksdifferfromtheprevioustwo,asit Figure8:Summaryofpromptsusedtoengineerthe
requiresthevisual-languagemodeltocontinuouslyanalyze LLaVA visual-language model (left) to interpret and
thesceneuntilapotentialeventoccurs,beforesendingthe identifylocationsofinterestgivenavoicecommand
drone(e.g.,‚ÄúLetmeknowifanychemicalspillsonthetable‚Äù). fromauser,aswellasforChatGPT(right)toanalyze
Actuation.Unlikethepreviousthreecategories,allofwhich timeseriesdatafromtheRASPonboarddronesensor.
requirethedronetosensetheenvironment,actuationtasks
all leverage the actuation module to bring an item (e.g.,
medicine,candy,food)toaspecificlocation.
4.2 Visual-LanguageModelsfor
InterpretingVoiceCommands
Identifying Objects of Interest. When a user issues a
4.1 SetupandNetworkedCameraSystem
voicecommand,wepassthiscommandalongwiththefull
Toeffectivelyuseacameranetworktoobservetheenviron- imageoftheenvironmentstitchedtogetherfromallcameras
ment,theregenerallyisalengthysetupprocesswhereyou intotheopen-sourceLargeLanguage-and-VisionAssistant
need to specify the location of the cameras and have the (LLaVA) visual-language model [25]. For the surveillance
floormapofthebuildingtostitchtogetherdifferentviews. category,wepassintwoconsecutiveframesorsegmentsto
We adopt this self localizing camera network [27] that al- detectchangingevents.LLaVAoutputs(intext)thenamesof
lowsuserstosemi-randomlyplacecamerasontheceiling, potentialobjectsinthescenethatmightofinterest,aswellas
facingdownwardsperpendiculartothefloor;byleveraging thesuitablesensor/actuationmodulethatthedroneshould
motionandpeoplewalkingthroughoutdailylife,thecam- use.Figure8-leftshowstheseriesofengineeredandtask-
eranetworkautomaticallyself-localizeseachcamerawithin specificpromptsweleverage,whichwasdesignedbasedoff
thenetworkandgeneratesthewalkablefloormapofthear- ofthehighlyactiveBrexLLMpromptengineeringguide[8].
easobservedbythecameras.Inthisway,usersonlyneed TheLLaVAtextoutputsareinputasapromptintoGrounding
toinstallcamerasanddonotneedtodefineafloormapor DINO[26],astate-of-artlanguagemodelforzero-shotobject
specifyandcalibratebasedonlocation.Moredetailsofour detection,whichthenoutputsboundingboxesandlocations
deploymentareinSection5. ofthespecifiedobjects.RASP:Drone-basedReconfigurableActuationandSensingPlatform
OneofthechallengeswefoundwithDINOisthatitwas
not able to identify objects of interest very well if input
imagesareverylongorverywide.Assuch,wecreateand
benchmarkedanewclusteringmethodcalledAspectRatio
ConstrainedK-Means(ARCK-Means),whichconstrainsthe
(a) SAM Filtered Masks (b) KMeans aspect ratio of the clustered masked between two values
ùë• <ùë¶tomaintainamoresquareshape.Toaccomplishthis,
wecheckifaddinganewsegmentintotheexistingcluster
(duringhierarchicalclustering)causestheaspectratiotofall
beloworaboveourconstraints(e.g., < ùë• or > ùë¶);Ifthese
constraintsarebroken,thenwedonotmaketheassignment.
(c) Hierarchical (d) Aspect Ratio KMeans Figure9b-dshowsanexampleofsegmentationandclus-
tering. We see that for ARCK-Means the clustered masks
Figure9:Segmentationandclusteringtobreakdown generatedtendtobemoresquare.Additionally,thecluttered
scenesintosmallermoremanageablepiecesforLLaVA tableinthebottomrighthandcornerisfullyencompassedby
and DINO. (a) Object masks after applying Segment ARCK-Means,butbothK-Meansandhierarchicalclustering
AnythingModel(SAM).Extractedframesaftercluster- donotcapturealloftheitemsonthetableinasingleseg-
ingobjectmasksbasedonK-Means(b),hierarchical ment.TheseimprovementsofARCK-Meansovertheother
clustering(c),andARCK-Means(d).ForARCK-Means, clusteringmethodsyieldshigherperformanceindetecting
weconstraintheaspectratioofextractedframestobe objectsofinterest,asshowninFigure10.Inthiscaseand
between0.67and1.5. fortherestofthepaper,asuccessful‚Äúrecall‚Äùmeansthatthe
GroundingDINOmodelwasabletodetectandlocalizethe
objectinterestinanyoneofthesegmentedclustersitwas
ObjectSegmentationandClustering.However,wefound given,regardlessofanyadditionaldetections.
thatLLaVAwasnotabletoreliablydetectobjectsofinter- First,wetestedtwopromptsintotheLLaVAmodeland
estinascenewhenwepassedinanimageoralargespace varythesizeoftheobjectwithrespecttotheframeofone
(e.g., a single chair in a large room); the larger the scene, camera(Figure10a).Thefirstpromptismoregeneral,asking
thelessdetailedandmorehigh-leveltheresponsesbecame. LLaVAtodescribetheobjectspresent.Thesecondprompt
Tocapturesmalldetailswithinthescenethatmightberele- is more specific, asking if the specific object of interest is
vanttoavoicecommand,weusedtheSegmentAnything presentinthescene.Weseethatthemorespecificaprompt
Model(SAM)toextractkeyobjectsandsmallerareasofthe is,thehighertherecall,whichisthereasonwhyweused
scene[23].Figure9ashowsanexampleoftheobjectmasks morespecificprompting(Figure8).Second,weseethatas
thatareoutputbySAM. thesizeoftheobjectgetssmallerwithrespecttotheimage
WetriedinputtingthesemaskedobjectsintoLLaVAand frame,therecallgetssmaller,sincethesignal-to-noiseratio
then DINO one-by-one, but there were two issues we no- oftheobjectgetssmaller.
ticed.First,manyofthemasksgeneratedoftensplitsobjects InFigure10b,wecomparetherecallofeachclustering
between two different masks; as such, DINO would often methodafterclusteringeachsceneinto5segments,andsee
detect the same object multiple times. Second, the masks that ARCK-Means has the highest recall due to improve-
arenotshapedlikearegularimage(e.g.,asquareorrectan- mentsinthemasksitclustersandtheshapeoftheresulting
gle);wesawalargedegradationindetectionperformanceif segments; as such, we adopt ARCK-Means into the final
wezero-paddedtheareasaroundthemaskedobjectintoa system.Figure10cshowstheruntimeandrecallofARCK-
rectangularshape. Meansasafunctionofthenumberofclustersorsegmentswe
Torectifythis,weclustermultiplesegmentstogetherto splitthesceneinto.Weseethattherecalllevelsoutataround
ensurepotentiallyimportantobjectsareallcontainedwith 90%after5clusters,whiletakingaround100mstorunthe
asinglerectangularsegment.Webenchmarkthefollowing fullpipeline.Addinginmoreclustersdoesnotyieldsignifi-
clusteringmethods:K-means[6],hierarchicalclustering[33], cantimprovementsindetection,butsignificantlyincreases
andAspectRatioConstrainedK-Means(ARCK-Means).For runtime;assuch,wesegmentthesceneintofivesegmentsin
K-meansandhierarchicalclustering,weclusterthecentroids thefinalsystem.Togeneratetheseplots,weused47images
ofeachmaskfromSAMintoùëò differentclusters.Next,we ofindoorhome,office,andlabenvironments.Wetook35
findtheminimum-arearectanglethatencompassesallmasks imagesfromtheADE20Ksceneparsingdataset[51]aswell
ofeachcluster.Theseùëò minimum-arearectanglesarethen as12imagesfromourowndeployment.Weimplementand
analyzedbyLLaVAandDINO.Zhaoetal.
Figure10:Differentclusteringmethodsandthresholdingevaluatedforsegmentation.Prompt1:Describetheimage
indetail.Prompt2:Istherea{objectname}intheimage?
runthefullvisual-languagemodelpipeline(SAM,clustering, theclosestpointofinterest.Throughoutourdeployments
LLaVA,andDINO)onanNvidiaRTX3090GPUserver. inSection5,weobservedanmedian2-Dlocalizationerror
Promptingusersformorecontext.Somephrasesrequire of3.29cm,usingourcameranetwork.Moreover,asshown
morecontexttoproperlyidentifylocations,sensors,oractua- in Figure 11, the localization error does not increase over
tors.Forexample‚Äútellmethe‚Äòbest‚Äôlocationtosit‚Äù.Theword time,asiscommonininertialmeasurementunitanddead
‚Äúbest‚Äùcouldhavemanymeanings(e.g.,warmest,coolest,qui- reckoningapproaches.
etest,etc.).Ifausergivesacommandwithanon-specific Using additional markers on top of the drone and the
adjective,suchas‚Äúbest‚Äù,thesystemwillprompttheuserto straightlinepathtonavigateisnotidealforgeneralizingto
clarifyandbemorespecific.Inthecaseofanactuationtask, otherdronesandenvironmentsthataremorewinding.The
thesystemaimstodeliveritspayloadtoonelocation.Ifthe focusofthisportionofourworkistodemonstratetheutility
visual-languagemodeldetectsmultiplepotentiallocations,it of RASP in an important application (personal assistant)
willasktheusertoclarifythelocation,eitherthroughvoice andnotspecificallydronedetection,navigation,orcontrol.
orawebapplicationthatweimplemented. Hence,weleavetheseaspectstofuturework.
4.3 DroneActuationandAnalysis
5 DEPLOYMENT
Oncethevisual-languagemodelsdetermineimportantlo-
WedeployedtheRASPbasedpersonalassistantintoanof-
cations,thesystemattachestherelevantsensororactuator
fice/labsettingasshowninFigure11e.Weusedanetworkof
(fromLLaVA)totheRASPdronebeforemovingthedroneto
4camerasinauniformgrid.AsdiscussedinSection4.1,the
eachoftheselocations.Inthecaseofanactuationtask,the
camerasself-localizedthemselvesandgeneratedthefloor
dronefliestothespecifiedlocation,opensthehatchtodrop
mapaccordingto[27]afteranafternoonofobservingpeople
theitemofinterest,beforeflyingbacktothegroundstation.
walkingandmovingthroughthespace.
Inthecaseofasensingtask,thedronefliestoeachlocation
whilereadingsensorvaluesfromtheattachedmodule.After
thedronelandsbackatthegroundstation,thetimeseries 5.1 Benchmarking
dataisinputintotheGPT-4languagemodel[37]toanalyze
We ran 2-3 tasks in each of the categories of tasks, as we
andmakethefinaldetermination.Figure8-rightshowsthe
discussnext.Foreachtask,weissued70differenttrials.The
prompts we used for GPT-4. If the sensing modality is to
scenariosaredescribedinmoredetailnext.
useacameratoidentifyanobject(e.g.,aobjection/location
1)Phone.InthisIDtask,theuserasks‚Äúwhereismyphone‚Äù.
ID task); the image from the drone is input back into the
Thesystemwillactuatethedronewithacameramoduleto
GroundingDINOmodeltomakethefinaldetermination.
potentiallocationstodetectthephone.
2)Key.ThisIDtaskissimilartothephonetask,butinstead
4.4 DroneNavigation
usersarelookingforkeys.
Weuseimagesfromthecameranetworktoguideandcontrol 3)Sit-X.InthisseriesofIDtasks,usersask‚Äúwhereisthe
the drone. We attach an aRuco marker to the top of the bestplacetosit‚Äù,basedonsomesensingmodality(e.g.,‚Äòsit-
drone,justlikethepatternsusedforlandingthedronein temp‚Äôiswheretheuserasksforthecoolestorwarmestplace).
Section3.Tomovethedronetospecificlocations,weuse Weartificiallyincreaseorlowertemperaturesatdifferent
thestraightlinepathfromthedrone‚Äôscurrentlocationto seatsbyplacingspaceheatersorfansnearby.RASP:Drone-basedReconfigurableActuationandSensingPlatform
Figure11:a)2-DdronelocalizatonCDF.b)Heightlocalizationerrorwhendroneisatdifferentheights.c)Example
groundtruthandlocalizedpathofdrone.d)Localizationerrorofdronevs.lengthofmission.Thelocalization
errorremainsrelativelyconstant,evenasthetimeofthemissiongetslonger,demonstratingthatthesystemisnot
susceptibletolocalizationerrordrifts.e)deploymentfloormap.f)cameramoduleusedinceilingcameranetwork.
100 Phone Key Sit 100 Faucet Stove 100 Food Chemical
100
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
0 0 0 0
KMeans Hierarchical ARCK KMeans Hierarchical ARCK KMeans Hierarchical ARCK KMeans Hierarchical ARCK
Figure12:SummaryofobjectandeventdetectionbytheLLaVA+DINOvisual-languagepipelineaveraged(a)and
brokendownbycategoryofsensingtasks(b-d).
CameraOnly Camera+RASP
Scenario Precision Recall F-1 Accuracy SensorUsed Precision Recall F-1 Accuracy
Object/LocationIdentification
FindPhone 68.85% 84.00% 75.68% 73.00% DroneCam 100.00% 84.00% 91.30% 92.00%
FindKey 78.05% 80.00% 79.01% 71.67% DroneCam 100.00% 80.00% 88.89% 86.67%
Sit-Temperature 23.47% 92.00% 37.40% 25.24% Temperature 76.67% 92.00% 83.64% 91.26%
Sit-Humidity 25.81% 88.89% 40.00% 28.00% Humidity 82.76% 88.89% 85.71% 92.00%
Sit-Light 20.62% 83.33% 33.06% 22.12% LightSensor 95.24% 83.33% 88.89% 95.19%
Average(ID) 43.36% 85.64% 53.03% 44.01% 90.93% 85.64% 87.69% 91.42%
StateofObject/Location
FaucetOpen 80.00% 97.78% 88.00% 82.86% Humidity 93.62% 97.78% 95.65% 94.29%
StoveOpen 79.22% 87.14% 82.99% 77.27% Temperature 96.83% 87.14% 91.73% 90.00%
Average(State) 79.61% 92.46% 85.50% 80.06% 95.22% 92.46% 93.69% 92.14%
Surveillance
FoodBurning 50.91% 70.00% 58.95% 51.25% PM 93.33% 70.00% 80.00% 82.50%
ChemicalSpill 25.40% 80.00% 38.55% 43.33% Gas(Alcohol) 88.89% 80.00% 84.21% 93.33%
Average(Sur.) 38.15% 75.00% 48.75% 47.29% 91.11% 75.00% 82.11% 87.91%
Average(all) 46.54% 83.17% 55.70% 48.99% 91.93% 84.79% 87.78% 90.80%
Table2:Summaryofend-to-endperformancebetweenRASPandcamera-onlyforallsensingtasks.
4)Faucet.Inthisstatetask,usersask‚Äúismyfaucetstillon‚Äù; foodbyboilingwaterinakettle,releasingwatervapors.The
thesystemwillthenactuatethedronewithamoisturesensor systemwillthenactuatethedroneattachedwithaparticulate
todetectthepresenceoflargeamountsofwaterleaking. mattersensor.
5)Stove.Inthisstatetask,usersask‚Äúismystovestillon‚Äù; 7) Chemical. In this surveillance task, users ask ‚Äúlet me
thesystemwillthenactuatethedronewithatemperature knowifanychemicalsspill‚Äù.Wewillthenknockdownand
sensortodetectthestateofthestove. spill a glass of alcohol. The system will then actuate the
6)Food.Inthissurveillancetask,usersask‚Äúletmeknow dronewithanalcoholsensortoconfirm.
whenmyfoodisburning‚Äù.Forsafety,wesimulateburning
)%(
llaceR
egarevA
)%(
llaceR
)%(
llaceR
)%(
llaceRZhaoetal.
Scenario #ofuserprompts #ofexecutions Execution
Accuracy False Positive
100 perexecution perbattery Time
75 Object/LocationIdentification
50 FindPhone 1.0 7 44.4s
25 FindKey 1.0 7 46.0s
0 Sit-Temperature 2.6 3 84.3s
Sit-Humidity 1.9 3 87.7s
Sit-Light 2.3 4 70.4s
100 100
Average(ID) 1.8 4.8 66.6s
80 75 60 50 StateofObject/Location
40 25 FaucetOpen 1.0 6 49.5s
20 StoveOpen 1.0 5 54.2s
0
0 Average(State) 1.0 5.5 51.9s
Surveillance
FoodBurning 3.1 4 62.5s
Figure13:Breakdownofaccuracyandfalsepositive ChemicalSpill 3.7 5 55.2s
Average(Sur.) 3.4 4.5 58.9s
detectionsbysensingtaskcategorywithandwithout
RASP. We see that leveraging RASP in conjunction
Average(all) 2.0 4.9 61.6s
withstaticcamerasgreatlyreducesfalsedetectionsand
Table 3: System performance metrics across bench-
improvesaccuracybecausethedronecangetacloseup
markedcommands.
vieworsenseanimportantpartoftheenvironment
thatacameraalonecannot(e.g.,humidity).
forsmallitem(s)andlocationsinalargescene,thevisual-
languagemodelpipelineoftenidentifiesmultiplepointsof
interest(e.g.,DINOdrawsmultipleboundingboxesandlo-
8)Medicine.Inthisactuationtask,theuserwillask‚Äúplease cations).Withoutaplatformsuchasadronethatcan‚Äúzoom
bringmemymedicine‚Äùandwavehis/herarmsatacamera in‚Äùandconfirm,thesensingcapabilitiesofthissystemislim-
above. The system will then attach an actuation module ited,andthefalsepositiveratebecomesextremelyhighand
loadedwithvitaminsonthedrone,whichwillthendeliver theprecisionbecomeslowwithadditionallocationsidenti-
ittotheperson. fied.However,addingintheRASP-equippeddroneallows
9) Poison. In this actuation task, the user will direct the thesystemtoactuatethedronetoeachlocationtoobtain
drone to ‚Äúdeliver rat poison‚Äù to a specific location in the acloseupviewofthelocationandremoveextraneousloca-
environment.Thesystemwillloadanactuationmodulewith tionsorsenseanaspectoftheenvironmentthatacamera
ratpoisonpellets(simulatedwithsmallsnacks)andthedrone cannot(e.g.,humidity).Thisbothreducesfalsepositivesand
willdispensethematthespecifiedlocation. improvesoverallaccuracy.Table2breaksdowntherecall,
Fornon-actuationtasks,Figure12showsabreakdownof precision,andf-1scoreacrossallindividualtaskstofurther
thevisual-languagemodelperformanceinidentifyingthelo- illustrateimprovementsintruedetectionrate(recall)and
cationstosendthedroneversusthesegmentationclustering precisionbyincorporatingRASP.
method(Section4).WeseethattheARCK-Meansclustering Forthetwoactuationtasks,weobservedanaverageoffset
methodyieldsthehighestrecall,acrossallscenarios,dueto ofthedrone,fromwhereitwassupposedtotraveltodropits
improvementsinmaintainingagoodaspectratioandobject payload,of9.1cmand10.3cmforthe‚Äúpoison‚Äùand‚Äúmedicine‚Äù
groupingoverotherclusteringmethods. tasks, respectively. The offset is on orders of centimeters,
Figure13highlightsimprovementsthatRASPbringscom- meaningthesystemwasabletoeffectivelydeliveritemsto
paredtoapurelystaticcamera-basedsystemacrossallsens- theproperlocationinmostcases.
ingtasks,especiallywithrespecttofalsepositives.Asuccess- Table 3 shows statistics about the number of tasks per
fulor‚Äúaccurate‚Äùtrialinthiscontextmeansthatthesystem categorythatcouldbeperformedperfullchargeofbattery,
wasabletocorrectlyidentifythecorrectobjectorlocation executiontimeandthenumberoftimesthesystemneeded
(object/locationIDtask),correctlyidentifythestateofthe toprompttheuserforamorespecificdescriptionoramore
objectorlocation(object/locationstatetask),orcorrectly accuratelocation.Weseethattheaveragenumberofuser
identify when a targeted event occurs (surveillance task); promptspercommandisonaverage2.Forstatetasks,this
anyadditionalpointsofinterestidentifiedarecountedas valueaveragedjust1prompt(user‚Äôsinitialcommand).How-
incorrectidentifications(falsepositives).Forthenon-RASP ever,forsurveillance andID tasks,thesystemoftentimes
solution, we run the entire SAM, LLaVA, DINO pipeline, neededtoaskformoreinformationfromtheuser.Forall
butdonotactuatethedrone.Becauseweareoftenlooking tasks, the average execution time of the visual-language
)%( etaR
)%(
etaR
tecuaF tecuaF )PSAR(
enohP
evotS
enohP )PSAR(
evotS )PSAR(
syeK
)%( etaR
syeK )PSAR(
dooF
tiS
dooF )PSAR(
tiS )PSAR(
lacimehC lacimehC )PSAR(RASP:Drone-basedReconfigurableActuationandSensingPlatform
Category SuccessP1 SuccessP2 SuccessTotal ‚Äúfindmethecoolest(temperature)placetositoutofthesun-
T1:Actuate+Actuate 20/20=1.0 18/20=0.90 18/20=0.90 light(light)‚Äù,3)sense+actuate:tasksthatinvolveperforming
T2:Sense+Actuate 17/20=0.85 20/20=1.0 22/25=0.85
T3:Sense+Sense 19/20=0.95 18/19=0.95 18/20=0.90 actuationinresponsetosensing(e.g.,‚Äúplaceratpoison(ac-
Table4:Summaryofsuccessratefortasksthatrequire tuation)indarkareas(sensing)‚Äù).Table4summarizesour
reconfiguringthedronemid-mission,brokendownby results,whereweshowthesuccessrateofthefirsttask(P1),
successrateofthefirstleg(P1)andsecondleg(P2). the second task (P2), and the aggregate. We execute each
task20times.Wediscusseachtaskinmoredetail,next:
T1:Actuate+Actuate-‚ÄúBringmedicineANDsnack‚Äù.
Inthistask,theuserwantstwo(P1andP2)itemsbroughtto
him/her.Weseethatthesuccessrateofdeliveringbothitems
ishigh,justlikeweobservedinSection5.1.Theseconditem
(snack)failedtwotimes(P2)becausetheswappingfailed;
thedronelandedwithahighoffsetfromthecenterofthe
landingstation(Figure5d).
T2:Sense+Actuate-‚ÄúPutpoisoninthewarmestarea‚Äù.
Inthistask,theusermaywanttoplacepoison(P2)forrats
andbugsinwarmareas(P1)wheretheyarelikelytocongre-
gate(e.g.,warmplaces).Wesimulate‚Äúwarmplaces‚Äùboiling
waterinakettleinlocationsvisibletocameras.Therewere
threetimesthatalocationawayfromthekettlewaschosen
(P1).Intheseinstances,ourimagesegmentationapproach
cut out the area we placed the kettle, so the failure point
wasfromthecameraratherthanRASP.Improvingsensing
withfoundationalmodelsinfutureworkiskeytorealizing
arobustversionofthisend-to-endsystem.
T3:Sense+Sense-‚ÄúWhatisthemosthumidandbright-
estlocationforplacingaplant‚Äù.Usersmaywanttoknow
Figure 14: Example of identified locations and mea-
ahumid(P1)andbright(P2)placeforoptimalplantgrowth.
surementsinourmulti-step‚Äúsense+sense‚Äùtask.
We simulate the ‚Äúcorrect‚Äù location by placing a lamp and
kettleatthedesiredlocation.Therewasoneinstancewhere
thetaskneededhumanintervention(failed)becauseofun-
modelisonorderofseconds,whileactuatingthedroneand
successfullyswappinginthesecondsensor;again,thefailure
analyzing the sensor data is on order of tens of seconds,
camefromlandingthedrone(P2).Thefoundationalmodel
which is acceptable latency in all of these scenarios. The
correctly identified the location we placed the lamp, and
object/locationidentificationtaskhadalongeraverageexe-
thedroneflewtotheselocationswiththehumiditysensor.
cutiontimebecausethesetasksgenerallyrequirethedrone
However,westoppedtherunwhenthedronelandedwitha
toflyandobservemultiplelocationsforeachtask.
highoffsetthatthefunnel-shapedlandingstationcouldnot
realign.Anotherinstancefailedbecauseourfoundational
5.2 ReconfiguringMid-Mission
modelpipelinedidnotidentifythelocationwesimulated
TheprevioussectiondemonstratedhowRASPcouldbeused highbrightnessandhumidity(P1).Figure14showsanexam-
in conjunction with static sensors in the environment to pleofasuccessfulrun,displayingpointswherethefounda-
betterperformtasksrequiringasinglesensororactuatorin tionalmodelidentifiedtosendthedrone,aswellashumidity
ahomeorofficesetting.Here,welookatscenarioswherea andlightmeasurementstakenbythedroneforeachofthese
userissuescommandsthatrequiremultiplesensingmodali- locationstomakethefinallocationdetermination.Here,we
tiesandactuators,highlightinghowRASPcouldbeeasily placedthekettleatlocationthree.
reconfiguredmid-missiontosatisfythesemulti-layeredand Takeaways. We demonstrated how RASP could be used
morecomplextasks. toreconfigureadrone,mid-mission,fortasksthatrequire
CommontasksthatrequireRASPtoreconfigurethedrone multiplesensorsand/oractuatorsinasmartbuildingsetting
mid-missioncomeinthreedifferentflavors:1)actuate+actu- withadecentsuccessrate.However,atrulypracticalsystem
ate:multipleindividualactuationtasksaggregatedintoone still needs future improvements in drone landing (causes
task(e.g.,‚ÄúbringmemymedicineANDasnack‚Äù),2)sense+ failed module swaps), methods for analyzing sensor data,
sense:ataskthatinvolvesmultiplesensingmodalities(e.g., andfine-grainedcontrolofdrones.Zhaoetal.
Category #Prompts #Exec ExecTime(s) Success lookforitemsinareasunobservablebystaticsensordeploy-
(vLLM+drone)
ments.Ontheflipside,theactuationtaskhadthehighest
ID 1.0 21 0.51+51.30 15/21=0.71
State 1.0 13 0.13+37.20 11/13=0.85 successrateparticularlybecausethesystempromptedusers
Surveil. 1.2 22 0.21+43.87 19/22=0.87 eachtimetoconfirmthelocationtomakethedelivery,which
Actuation 2.1 31 0.27+29.50 31/31=1.0
reducesrelianceonlanguagemodelsandperceptionalgo-
Table5:Summaryoftasksduringin-the-wilddeploy-
rithmstomakethisdetermination.Althoughtherearestill
ments.Thenumberofpromptsistheaveragenumber
improvementsneededtorealizeatrulyautonomousdrone-
oftimestheuserneededtopromptthesystem.This
basedpersonalassistant,alluserswerepositivelyreceptive
numberisoftengreaterthanonebecauseeitherthe
tothissystemandcouldseeitsvalue.
userusedanon-specificadjective(e.g.,‚Äúbest‚Äùrather
than‚Äúwarmest‚Äùlocation)orthesystemneededtonar-
rowdownthepotentialcandidatelocationsinthecase
ofactuationtasks.Numberofexecutionsisthetotal 6 DISCUSSION
numberoftasksissuedduringthedeploymentperiod.
Thefocusofthisworkistorealizeanautonomousreconfig-
urableandmodularsensingandactuationplatformdrone
platformthatcanadaptitssensingandactuationcapabilities
dependingonthetaskathand.Assuch,asingledronecan
5.3 In-the-WildDeployment
be adapted for multiple tasks. We demonstrate the utility
Afterbenchmarkingseveraltaskspercategory,weallowed of such a platform for ambient intelligent systems by in-
peoplewhooccupiedthisofficespace(Figure11e)tofreely tegratingitwithadrone-basedpersonalassistantthatwe
usethesystemoverthecourseof5days.Table5summarizes envisionwillbecomecommonplaceinfuturehomes,offices,
the number of events that occurred during this period. A andbuildings.Specifically,wedemonstratethepotentialof
totalof8peopleissued87commandstothesystemduring leveraging static sensors in the environment and founda-
thistimeperiod. tionalmodels(e.g.,visual-languagemodels)inconjunction
We see that most of the actions issued throughout the with a powerful and adaptable mobile platform in smart
deploymentwereactuationtasks.Around90%ofthesetasks building,home,office,andindoorenvironments.Torealizea
involved bringing the user a snack, which we loaded and fullyautonomousdrone-basedpersonalassistantandother
manually refilled into actuation modules throughout the intelligentsystemsthatcanfullytakeadvantageofRASP,
deployment. ID tasks that users issued generally fell into thereareseveraldirectionsthatneedmoreexploration.
two categories: finding an area with the least amount of Robustpathplanning,navigation,andsearchforin-
sunlight(ourspacehasmanywindowsandissusceptibleto doorenvironments.AsdiscussedinSection5.3,manyof
glare)orfindingalostitem(e.g.,awalletorphone).Forthe thetasksthatfailedwereduetothesimplesearchandnavi-
surveillanceandthestatetasks,mostusersaskedthesystem gationalgorithmsweimplementedonthedronethatwere
abouta3Dprintjob,whetheraheatelementwaslefton(e.g., notabletocaptureanddetectsmallobjectsofinterestina
solderingiron),oriftherewereanyoneoccupyingdifferent clutteredhome/indoorsetting.Moreworkintoexploration
partsofthespace. anddetectingthesesmallobjectsofinterestisneeded.
Thecategoryoftasksthathadthelowestsuccessratewas PenetrativeAI,foundationalmodelsandleveragingdi-
theobject/locationIDcategory.Thisisbecausemostofthese versesensorsintheenvironment.Inthiswork,welever-
tasksreliedonstaticcamerasorthecameraonthedroneto agedcamerasinconjunctionwithlanguagetounderstand
findsomethingextremelysmallinthelandscapeofascene andinterprettheenvironment.However,notallareasmay
(e.g.,acircuitcomponentoraphone),makingitdifficultfor haveacamera(e.g.,duetoprivacyorrelatedconcerns),and
thevisual-languagepipelinetoidentifyrelevantlocations. therearemanymoretypesofsensorsthatcouldbedeployed;
Evenafterflyingthedronetothespecifiedlocation,itcanbe enablingfoundationalmodelsanddronestounderstandthe
difficulttodetect;weenvisionfutureworkfocusingonhow physicalworld(e.g.,penetrativeAI[47])throughalltypesof
todesignsearchalgorithmsandprotocolsfordronestoiden- sensingmodalitiesisimportantforgeneralizingourplatform
tifysmallobjectsofinterest.Severalitemsthatuserswanted andcasestudytoalltypesofscenarios.
thedronetolookforwerealsounderneathfurnitureorta- Dronenoisereduction,smallersizes,andpowereffi-
bles;acameramountedontheceilingorwallshavelimited ciency.Dronesandquadcoptersareveryloudbecauseofits
viewoftheseitems.Anotheravenueofresearchforrealiz- propellers.Havingthemcomfortablyco-existwithpeoplein
ingadroneorrobot-basedpersonalassistantcouldbehow indoorenvironmentsrequiresmoreresearchintothedesign
toleverageanddesignsmallroboticsystems(e.g.,physical ofthemtoreducetheirsizeandnoise,whileprolongingtheir
design,pathplanning,searchalgorithms,etc.)toreachand batterylifeandenablingautomaticchargingwhiledocked.RASP:Drone-basedReconfigurableActuationandSensingPlatform
7 CONCLUSION motes.Sensors(Apr.1,2002)(2002),1‚Äì5.
[11] S.K.Das,D.J.Cook,A.Battacharya,E.O.Heierman,andTze-YunLin.
WeproposeRASP,amodularandreconfigurablesensingand
2002.TheroleofpredictionalgorithmsintheMavHomesmarthome
actuationplatformthatallowssmalldronestoswapsensing architecture.IEEEWirelessCommunications9,6(2002),77‚Äì84. https:
andactuationcapabilitiesdynamically.RASPallowsasingle //doi.org/10.1109/MWC.2002.1160085
dronetoreconfigureitselftosatisfyadiverserangeoftasks [12] N.Edmonds,D.Stark,andJ.Davis.2005.MASS:modulararchitecture
forsensorsystems.InIPSN2005.FourthInternationalSymposium
thatiscommonlynotpossiblewithcurrentdrones.Current
onInformationProcessinginSensorNetworks,2005.393‚Äì397. https:
industrialdronesaretypicallydesignedtosatisfyasingleap-
//doi.org/10.1109/IPSN.2005.1440955
plicationdomain,andconsumerdronesarecurrentlymostly [13] SparkFunElectronics.2022.SparkfunSensors.https://www.sparkfun.
usedbyhobbyistsforentertainment.Wedemonstratethe com/categories/23. Accessed:2022-01-25.
diverserangeoftasksRASPcanenablebyintegratingRASP [14] StatistaMarketForecast.2021. SmartHome-Worldwide. https:
//www.statista.com/outlook/dmo/smart-home/worldwide. [Online].
into a personal assistant system that also leverages static
[15] Sean Harte, Brendan O‚ÄôFlynn, Rafael V. Martinez-Catala, and
cameras to enable a wide range of useful tasks that were
EmanuelM.Popovici.2007.Designandimplementationofaminia-
previouslynotpossible.RASPisasteptowardsgeneralizing turised,lowpowerwirelesssensornode.In200718thEuropeanCon-
theuseandpresenceofdronesindailylife,muchlikehow ferenceonCircuitTheoryandDesign.894‚Äì897. https://doi.org/10.1109/
smartphoneshavebecomeubiquitous. ECCTD.2007.4529741
[16] S.Helal,W.Mann,H.El-Zabadani,J.King,Y.Kaddoura,andE.Jansen.
ACKNOWLEDGMENTS 2005.TheGatorTechSmartHouse:aprogrammablepervasivespace.
Computer38,3(2005),50‚Äì60. https://doi.org/10.1109/MC.2005.107
ThisresearchwaspartiallysupportedbyCOGNISENSE,one [17] heliguy.2020. Farming&AgriculturalDrones-CropSpraying&
of seven centers in JUMP 2.0, a Semiconductor Research Mapping. https://www.heliguy.com/collections/agricultural-drones
Accessedon2023-09-06.
Corporation(SRC)programsponsoredbyDARPA,aswell
[18] AppleInc.2020.HomePodmini.https://www.apple.com/homepod-
as the National Science Foundation under Grant Number
mini/. Accessed:2023-11-27.
CNS-1943396. The views and conclusions contained here [19] AdafruitIndustries.2022. AdafruitSensors. https://www.adafruit.
are those of the authors and should not be interpreted as com/category/35. Accessed:2022-01-25.
necessarilyrepresentingtheofficialpoliciesorendorsements, [20] AnnaMJohnson,ChristopherJCunningham,EvanArnold,WayneD
Rosamond,andJessicaKZ√®gre-Hemsey.2021.Impactofusingdrones
either expressed or implied, of Columbia University, NSF,
inemergencymedicine:Whatdoesthefuturehold? OpenAccess
SRC,DARPA,ortheU.S.Governmentoranyofitsagencies. EmergencyMedicine(2021),487‚Äì498.
[21] JOUAV.2022. DroneinConstruction:Benefits,Applications,and
REFERENCES
UseCases. https://www.jouav.com/industry/drone-in-construction
[1] 2023. Commercial Drone Market Size, Share & Trends Analy- Accessedon2023-09-06.
sisReportByProduct(Fixed-wing,RotaryBlade,Hybrid),ByAp- [22] EvanKing,HaoxiangYu,SangsuLee,andChristineJulien.2023. "
plication, By End-use, By Region, And Segment Forecasts, 2023 - Getreadyforaparty":Exploringsmartersmartspaceswithhelpfrom
2030.https://www.grandviewresearch.com/industry-analysis/global-
largelanguagemodels.arXivpreprintarXiv:2303.14143(2023).
commercial-drones-market. [23] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRol-
[2] 2023.Crazyflie2.1OpenSourceFlyingDevelopmentPlatform.https: land,LauraGustafson,TeteXiao,SpencerWhitehead,AlexanderC
//www.bitcraze.io/products/crazyflie-2-1/. Berg,Wan-YenLo,etal.2023. Segmentanything. arXivpreprint
[3] 2023.Creality3DEnder-3,afullyOpenSource3Dprinterperfectfor
arXiv:2304.02643(2023).
newusersonabudget.https://github.com/Creality3DPrinting/Ender- [24] P.Levis,S.Madden,J.Polastre,R.Szewczyk,K.Whitehouse,A.Woo,
3.
D.Gay,J.Hill,M.Welsh,E.Brewer,andD.Culler.2005.TinyOS:An
[4] 2023.RaspberryPiZero2W.https://www.raspberrypi.com/products/ OperatingSystemforSensorNetworks. SpringerBerlinHeidelberg,
raspberry-pi-zero-2-w/. Berlin,Heidelberg,115‚Äì148. https://doi.org/10.1007/3-540-27139-2_7
[5] 2023.RingAlwaysHomeCam.https://ring.com/always-home-cam- [25] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.2023.
flying-camera.
Visualinstructiontuning.arXivpreprintarXiv:2304.08485(2023).
[6] MohiuddinAhmed,RaihanSeraj,andSyedMohammedShamsulIs- [26] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,
lam.2020. Thek-meansalgorithm:Acomprehensivesurveyand ChunyuanLi,JianweiYang,HangSu,JunZhu,etal.2023.Grounding
performanceevaluation.Electronics9,8(2020),1295. dino:Marryingdinowithgroundedpre-trainingforopen-setobject
[7] Amazon.com.2016. AmazonEcho&AlexaDevices. https://www.
detection.arXivpreprintarXiv:2303.05499(2023).
amazon.com/smart-home-devices/b?node=9818047011. Accessed: [27] YanchenLiu,JingpingNie,StephenXia,JiajingSun,PeterWei,and
2023-11-27. XiaofanJiang.2022.SoFIT:Self-OrientingCameraNetworkforFloor
[8] BrexInc.2023. Brex‚ÄôsPromptEngineeringGuide. https://github.
MappingandIndoorTracking.In202218thInternationalConference
com/brexhq/prompt-engineeringAccessedon2023-11-29.
onDistributedComputinginSensorSystems(DCOSS).IEEE,93‚Äì100.
[9] Croptracker. 2019. Drone Technology In Agriculture. https:// [28] GoogleLLC.2016.WelcometoGoogleNest.Buildyourhelpfulhome.
www.croptracker.com/blog/drone-technology-in-agriculture.htmlAc- http://www.nest.com. Accessed:2023-11-27.
cessedon2023-09-06. [29] AntonioMatus-Vargas,GustavoRodriguez-Gomez,andJoseMartinez-
[10] DavidCuller,JasonHill,MikeHorton,KrisPister,RobertSzewczyk, Carranza.2021.Groundeffectonrotorcraftunmannedaerialvehicles:
andAlecWood.2002. Mica:Thecommercializationofmicrosensor
Areview.IntelligentServiceRobotics14,1(2021),99‚Äì118.Zhaoetal.
[30] KonstantinMikhaylovandMarttiHuttunen.2014.Modularwireless [41] STMicroelectronicsOctober2017.Datasheet:1-Kbitand2-Kbitserial
sensorandActuatorNetworkNodeswithPlug-and-Playmodulecon- I2CbusEEPROMs.STMicroelectronics. https://www.st.com/resource/
nection.InSENSORS,2014IEEE.470‚Äì473. https://doi.org/10.1109/ en/datasheet/m24c02-r.pdf
ICSENS.2014.6985037 [42] JeremyTucker.2015.ARoleforDronesinHealthcare. https://www.
[31] KonstantinMikhaylov,TomiPitka√§ho,andJouniTervonen.2013.Plug- dronesinhealthcare.com/Accessedon2023-09-06.
and-PlayMechanismforPlainTransducerswithWiredDigitalInter- [43] SaiVemprala,RogerioBonatti,ArthurBucker,andAshishKapoor.
facesAttachedtoWirelessSensorNetworkNodes.Int.J.Sen.Netw. 2023. Chatgptforrobotics:Designprinciplesandmodelabilities.
14,1(sep2013),50‚Äì63. https://doi.org/10.1504/IJSNET.2013.056336 MicrosoftAuton.Syst.Robot.Res2(2023),20.
[32] MichaelC.Mozer.1998.TheNeuralNetworkHouse:AnEnvironment [44] Wingtra. 2023. Six Ways Drones in Construction Add Value.
thatAdaptstoitsInhabitants. https://wingtra.com/drone-mapping-applications/drones-in-
[33] FionnMurtaghandPedroContreras.2012.Algorithmsforhierarchical construction-and-infrastructure/Accessedon2023-09-06.
clustering:anoverview.WileyInterdisciplinaryReviews:DataMining [45] StephenXia,RishikanthChandrasekaran,YanchenLiu,ChenyeYang,
andKnowledgeDiscovery2,1(2012),86‚Äì97. TajanaSimunicRosing,andXiaofanJiang.2021.Adrone-basedsystem
[34] Jingping Nie, Hanya Shao, Minghui Zhao, Stephen Xia, Matthias forintelligentandautonomoushomes.InProceedingsofthe19thACM
Preindl,andXiaofanJiang.2022. Conversationalaitherapistfor ConferenceonEmbeddedNetworkedSensorSystems.349‚Äì350.
dailyfunctionscreeninginhomeenvironments.InProceedingsofthe [46] StephenXia,MinghuiZhao,CharuvahanAdhivarahan,KaiyuanHou,
1stACMInternationalWorkshoponIntelligentAcousticSystemsand YuyangChen,JingpingNie,EugeneWu,KarthikDantu,andXiaofan
Applications.31‚Äì36. Jiang.2023.Anemoi:ALow-costSensorlessIndoorDroneSystemfor
[35] JingpingNie,MinghuiZhao,StephenXia,XinghuaSun,HanyaShao, AutomaticMappingof3DAirflowFields.InProceedingsofthe29th
YuangFan,MatthiasPreindl,andXiaofanJiang.2022. Aitherapist AnnualInternationalConferenceonMobileComputingandNetworking.
fordailyfunctioningassessmentandinterventionusingsmarthome 1‚Äì16.
devices.InProceedingsofthe20thACMConferenceonEmbeddedNet- [47] HuataoXu,LiyingHan,QiruiYang,MoLi,andManiSrivastava.2024.
workedSensorSystems.764‚Äì765. Penetrativeai:Makingllmscomprehendthephysicalworld.InProceed-
[36] BrendanO‚ÄôFlynn,S.Bellis,K.Delaney,J.Barton,S.C.O‚ÄôMathuna, ingsofthe25thInternationalWorkshoponMobileComputingSystems
AndreMelonBarroso,J.Benson,U.Roedig,andC.Sreenan.2005.The andApplications.1‚Äì7.
DevelopmentofaNovelMinaturizedModularPlatformforWireless [48] Wei-YingYi,Kwong-SakLeung,andYeeLeung.2018. AModular
SensorNetworks.InProceedingsofthe4thInternationalSymposium Plug-And-PlaySensorSystemforUrbanAirPollutionMonitoring:
onInformationProcessinginSensorNetworks(LosAngeles,California) Design,ImplementationandEvaluation.Sensors18,1(2018). https:
(IPSN‚Äô05).IEEEPress,49‚Äìes. //doi.org/10.3390/s18010007
[37] OpenAI.2023.GPT-4. https://openai.com/research/gpt-4Accessed [49] MinghuiZhao,YanchenLiu,AvikDhupar,KaiyuanHou,Stephen
on2023-11-29. Xia,andXiaofanJiang.2022.Amodularandreconfigurablesensing
[38] MohammadFattahiSaniandGhaderKarimian.2017.Automaticnav- andactuationplatformforsmarterenvironmentsanddrones:demo
igationandlandingofanindoorAR.dronequadrotorusingArUco abstract.InProceedingsofthe20thAnnualInternationalConferenceon
markerandinertialsensors.In2017internationalconferenceoncom- MobileSystems,ApplicationsandServices.626‚Äì627.
puteranddroneapplications(IConDA).IEEE,102‚Äì107. [50] MinghuiZhao,StephenXia,JingpingNie,KaiyuanHou,AvikDhu-
[39] Shenzhen DJI Sciences and Technologies Ltd. 2022. DJI par,andXiaofanJiang.2023. LegoSENSE:AnOpenandModular
Mini 3 is the Sub-249g Drone for Everyone, Anywhere. SensingPlatformforRapidly-DeployableIoTApplications.InProceed-
https://www.dji.com/newsroom/news/dji-mini-3-is-the-sub- ingsofthe8thACM/IEEEConferenceonInternetofThingsDesignand
249g-drone-for-everyone-anywhereAccessedon2023-09-06. Implementation.367‚Äì380.
[40] Skydio,Inc.2016.ProfessionalDrones&AutonomousDrones. https: [51] BoleiZhou,HangZhao,XavierPuig,SanjaFidler,AdelaBarriuso,
//www.skydio.com/Accessedon2023-10-16. andAntonioTorralba.2017. Sceneparsingthroughade20kdataset.
InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition.633‚Äì641.