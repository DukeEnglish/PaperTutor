CLUSTERED MALLOWS MODEL
APREPRINT
LuizaS.C.Piancastelli NialFriel
SchoolofMathematicsandStatistics SchoolofMathematicsandStatistics
UniversityCollegeDublin InsightCentreforDataAnalytics
Dublin,Ireland UniversityCollegeDublin
luiza.piancastelli@ucd.ie Dublin,Ireland
nial.friel@ucd.ie
ABSTRACT
Rankingsareatypeofpreferenceelicitationthatariseinexperimentswhereassessorsarrangeitems,
forexample,indecreasingorderofutility. Orderingsofnitemslabelled{1,...,n}denotedbyπare
permutations(π ∈P )thatreflectstrictpreferences. Thatis,anitemrankedithismorepreferredto n
thejudgethananyotherinpositionj ifj >i. Foranumberofreasons,strictpreferencerelations
canbeunrealisticassumptionsforrealdata. Forexample,whenitemssharecommontraitsitmay
bereasonabletoattributeequalranksamongtheseitems. Dependingonthesituation,therecanbe
differentimportanceattributionstodecisionsthatformπ. Inasituationwith,forexample,alarge
numberofitems,anassessormaywishtorankattopacertainnumberitems;torankotheritems
atthebottomandtoexpressindifferencetoallotheritems. Inthistypeoftop/bottomelicitation,
middle-rankalternativescanbeaclosetouniformplacementoftheremainingitems. Inaddition,
when aggregating opinions, a judging body might be decisive about some parts of the rank but
ambiguous for others. In this paper we extend the well-known Mallows (Mallows, 1957) model
(MM)toaccommodateitemindifference,aphenomenonthatcanbeinplaceforavarietyofreasons,
suchasthoseabovementioned. Theunderlyinggroupingofsimilaritemsmotivatestheproposed
ClusteredMallowsModel(CMM).TheCMMcanbeinterpretedasaMallowsdistributionfortied
rankswheretiesarelearnedfromthedata. AlthoughtheideaoftiedMallowsmodelsdatesbackto
ChungandMarden(1991),ithasnotbeendevelopedotherthaninthehypothesistestingcontext.
TheCMMprovidestheflexibilitytocombinestrictandindifferentrelations,achievingasimplerand
robustrepresentationofrankcollectionsintheformoforderedclusters. Bayesianinferenceforthe
CMMisintheclassofdoubly-intractableproblemssincethemodel’snormalisationconstantisnot
availableinclosedform. Weovercomethischallengebysamplingfromtheposteriorwithaversion
oftheexchangealgorithm(Murrayetal.,2006). Apseudo-likelihoodapproximationisalsoprovided
asacomputationallycheaperalternative. Information-criterionanddata-basedselectionstrategiesare
outlinedtoselectbetweenpartitionsofdifferentsizes. Realdataanalysisoffoodpreferencesand
resultsofFormula1racesarepresented,illustratingtheCMMinpracticalsituations.
Keywords Mallowsmodel·rankingdata·Bayesianlearning
1 Introduction
Theexerciseofrankingalistofitems,forexample,frombesttoworst,isacommonsituationinavarietyofcontexts.
Itemrankingarecommonlyemployedinmarketresearch,forexample. Whilemanyotherreal-lifesituationsnaturally
produce rankings. One such example is the Single Transferable Voting system (STV) adopted in countries such
as Ireland, Australia and the United Kingdom (local elections in Scotland and Northern Ireland). In this form of
election, voter preferences are given in the form of a ranked-choice ballot. In sports, they represent the result of
racingcompetitionsorcompetitors’placesinlong-termchampionships. Rankingsalsounderliemanyrecommendation
systems,suchastheorderofpagesinwebsearches.
4202
raM
91
]EM.tats[
1v08821.3042:viXraCMM
Mathematically,arankingofnalternativeslabelled{1,...,n}isanobservationinP ,thespaceofpermutationofn
n
labels. Thiswillbedenotedbyπ. Importantoperationsonπareπ(i)andπ−1(i). Thefirstcorrespondstotheitem
labelrankedithanditsinverse,π−1(i),istherankoftheobjecti. Forexample,ifπ =(3,1,2),thenπ(1)=3means
thattheitem3rankedfirstandπ−1(1)=2meansthatitem1isrankedsecond.
Statistical and machine learning approaches for ranking data go beyond complete rankings, that is, π ∈ P . For
n
instance,pairwisepreferencestudiesgatheropinionsbyaskingtheassessortochoosebetweenitempairs.Thisapproach
elicitspreferencesforpairs{i,j},i̸=j takenfrom{1,...,n},denotedasi≻j orj ≻i. Thesymbol≻denotesa
strictpreferenceforthefirstovertheseconditemandmultiplecomparisonsareoftenpresentedtothesameassessor.
Forexample,supposethatthreepairwisecomparisonsof{1,...,4}aregivenasC ={4≻2,1≻3,2≻1},thenC
impliesthatπ = (4,2,1,3). Mostoften,C doesnotcontainallcomparisonsneededtoexplicitacompleteranking.
Thishappensnecessarilywhen#C < (n−1),where#C isthecardinalityofC. Inthiscase,morethanoneπ will
becompatiblewithC ifopinionsareconsistent. Forexample,C = {2 ≻ 1,3 ≻ 4}induces4!/(2!2!) = 6complete
permutations including π = (2,1,3,4), π = (3,4,2,1), π = (3,2,4,1), each of which are consistent with C.
1 2 3
Pairwisepreferencesetsareonepossiblegeneratingmechanismofincompleterankingdata. Thepartialobservation
ofπwillbedenotedbyπtoindicatethatthecompleteorderingisnotobserved. Anothertypeofpartialobservation
(cid:101)
π ofπ arisesinthecontextof, whatwetermtop−k elicitation. Inthissetting, assessorsprovidetheirfirstk < n
(cid:101)
choices,determiningπ(1),...,π(k). Thesingletransferablevotingisanexampleofthissituation. SeealsoGormley
andMurphy(2006)foranotherexampleinthecontextofIrishthird-levelcollegeapplications.
MethodologytohandleπandπisgainingrecentattentioninStatisticsandMachineLearning(ML).Inthelatter,this
(cid:101)
isknownaslearningtorankapproachesandsomepopularapproachesareBurgesetal.(2006),Burges(2010). In
recommendationsystems,ranking-basedmethodsareoftenadvocatedduetouserinconsistencyorlimitedscaleof
ratings(Brunetal.(2010),Brunetal.(2011),LuandBoutilier(2014)).
Inthispaperwetakeastatisticalapproachtomodelπ (orπ)bydescribingitintermsofaprobabilitydistribution.
(cid:101)
Probabilisticmodelsdescribetheunderlyingmechanismthatgeneratesπbymakingdifferentassumptionsaboutthe
assessor’s decision process. Critchlow et al. (1991) categorizes these as (1) Thurstone order models, (2) pairwise
rankingmodels,(3)distance-based,and(4)multistagerankingmodels. Thelatterisapopularapproachwhereπis
formedfromsequentialdecisionsondecreasingitemsets. Multistagerankingisunderpinnedbytheindependenceof
irrelevantalternatives(IIA)assumption. UnderIIA,theprobabilityofselectinganobjectfromasetisnotaffectedby
thepresenceorabsenceofothers. OneofthemostfamousmodelsinthisclassisthePlackett-Luce(Plackett,1975).
Ourworkrelatestocategory(3),forwhichtheMallows’model(Mallows,1957)isthebestknownexample. Distance-
basedmodelsattributeprobabilitytoπaccordingtoitsdistancetoapopulationconsensusπ . Thisiswrittenasd(π,π )
0 0
whered(·,·)isadistanceinP ,andπ isanunknownmodelparameter. TheMallowsmodel(MM)isgivenby
n 0
exp{−αd(π,π )}
p(π|π ,α)= 0 , (1)
0 ϕ(α,π )
0
whereα>0governsthespreadaroundπ ∈P . Itisanexponentialfamilymodelwhereπ andαareestimatedfrom
0 n 0
thedataandϕ(α,π )isthenormalisationconstant. IntheMallows−ϕmodel,d(·,·)istheKendall’sτ distanceinP ,
0 n
whichcountsthenumberofpairwisedisagreementsinπandπ . OtherpopulardistancechoicesaretheHammingand
0
Cayleydistancemetrics. Thetermϕ(α,π )isoftenwrittenasϕ(α)becausecommonchoicesfordareunaffectedby
0
permutationsofitemlabels(i.e.,areright-invariant.)
Themajorityofapplicationsanddevelopmentof(1)usestheKendall,Hamming,orCayleyformsofd(·,·). Thisis
becausethenormalisationtermϕ(α,π )hasasimpleanalyticalforminthesecases. Expressionsforϕ(α)werederived
0
byFlignerandVerducci(1986)undertheKendallandHammingdistances,andlaterfortheCayleydistancebyIrurozki
etal.(2018). Theseareeachnon-trivialadvancementsthatdonotgeneralisetootherdistances.
As a consequence, other distances such as the Spearman and Footrule make (1) difficult to evaluate. Computing
(cid:80)
ϕ(α,π ) = exp{−αd(π,π )} exactly requires enumerating every π ∈ P . This is unfeasible except for
trivially0 smallnπ ,∈ rP enn deringtheMMa0 nintractableprobabilitydistributioninmostcasn
es.
One important extension of (1) is the Generalised Mallows Model (GMM, Fligner and Verducci (1986)). The
GMM models the importance of different ranking stages with α = (α ,...,α ) according to p(π|α) ∝
1 n−1
(cid:16) (cid:17)
exp −(cid:80)n−1α dj(π,π ) relying on the decomposition d(π,π ) = (cid:80)n−1dj(π,π ). The latter assumes that
j=1 j 0 0 j=1 0
thetotaldistancecanbeexpressedasthesumofindependentcomparisonsbetweenπ andπ thatoccuratranking
0
stagesj =1,...,n−1,i.e.,d(·,·)isdecomposable. Inanotherdirection,Meila˘andBao(2008),Meila˘andBao(2010)
introducedtheInfiniteGeneralisedMallowsmodel(IGM)fortop−kforpartialpermutationsπ. Moredetailsabout
(cid:101)
theIGManditsconnectiontoourapproachareexploredlater. InCrispinoetal.(2017),theMMisextendedtoπ
(cid:101)
frompairwisecomparisonswithinconsistencies. RecentworkbyVitellietal.(2018)approachesmodelingacollection
2CMM
Figure1: DistributionofranksgiventosushivariantsbyrespondentswholivedinJapan’sregionTohokuatleastuntil
15yearsold.
ofindependentranks(π ,...,π )fromqnon-homogeneousassessors. Thisisdenotedbyπ,a(q×n)matrix. The
1 q
authorsproposemodellingπ withafinitemixtureofMallows’distributionstofindcluster-specificconsensusand
spreads.
Thestudyofπisatopicofemerginginterest,oftenreferredtoasrankaggregation. SeeDengetal.(2014),Zhuetal.
(2023),Lietal.(2020),forexample. Thecontributionofthisworkgoesinthisdirection. Weproposeanextensionof
theMMforsituationswhereπdoesnotconveyastrictpreferencesconsensus. Theparameterofmaininterestinthe
Mallowsmodelisπ ∈P ,acompleteandstrictorderingof{1,...,n}. Estimatingπ meansinferringi≻j orj ≻i
0 n 0
foralli,j whichcouldbeunrealistic. Theaimofthemodelproposedinthispaperistoaccommodateindifference
between some items i,j, denoted by i ∼ j. This can comefrom the data’s underlying generatingprocess such as
top-bottomelicitationorconveyuncertaintybetweenassessors. Thisismadeclearinthefollowingsubsection.
1.1 Amotivatingexample
Kamishima(2003)surveysfoodpreferenceinJapanbyaskingparticipantstorank10varietiesofsushi. Thisisa
famousrankingdataset,alsoanalysedinVitellietal.(2018). AccordingtoKamishima(2003),foodtasteinJapanese
culture is largely influenced by geographical regions. Accordingly, participants are separated by their response to
thequestion"Whatistheregioninwhichyouhavebeenthemostlonglyliveduntil15yearsold?". Thesubsetof
rankingsfromTohokucontainsn = 280observationsandisillustratedinFigure1. Boxesshowcasethemarginal
distribution of rankings given to the types of sushi, obtained fixing i ∈ {fatty tuna, ..., cucumber roll} and
gatheringπ−1(i),...,π−1(i). Arrangingtheboxplotsinincreasingorderofmedianvaluegivesatypeofsummaryof
1 280
theopinionswithinπ.
Figure1isamuchsimplifiedrepresentationofthedatahowever. Wehighlightthatthestructureofπ =(π ,...,π )
1 280
isinfactmuchmorecomplexsinceπ ismultivariate. Naturally, elementsofarankingcarryrelativeinformation
j
about the item set and this comes from imposing π ∈ P . Figure 2 provides an alternative visualisation of π
j n
that brings attention to this fact. It is constructed by gathering how many times sushi i is ranked lower that sushi
i′ for all π and constructing the empirical proportion, p¯(i,i′) = (cid:80)280 I{π (i) ≻ π (i′)}/280. In Figure 2, i is
j j=1 j j
shownasrowsandi′ ascolumns. Totheright,asnapshotofthedataispresentedwherethedifferentsushivariants
shrimp,sea eel,tuna,squid,sea urchin,salmon roe,egg,fatty tuna,tuna roll,
cucumber rollarelabeled{1,...,10}.
Figure1immediatelysuggestsfatty tunaandcucumber rollasthebestandworstoptions. However,theoverall
opinionregardingmiddleranksislessclear. ThisissupportedbythepreliminaryanalysisshowninFigure2,wherethe
proportioniscloseto0.5forthefollowingpairs: {tuna roll, squid} ≈0.51,{sea urchin, sea eel}≈0.52,
{sea eel, salmon roe}≈0.52and{tuna, shrimp}≈0.54. Inthesecases,empirically,atleast,thereappearsto
beanindifferenceinthepreferenceofoneovertheother. Theaimofourstatisticalmodelisaccommodatesituations
likethis,wheretheremaybeties(orindifference)incertainitems.
3CMM
3 1 7 9 8 10 2 6 5 4
9 1 4 5 6 2 3 8 10 7
1 5 9 2 8 3 4 10 6 7
 
π=   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   
8 3 1 2 9 4 7 10 6 5
 
3 9 4 1 6 2 7 8 10 5
1 6 4 5 8 3 7 10 9 2
 tuna shrimp ··· sea urchin squid 
tuna roll shrimp ··· cucumber roll egg
 
 shrimp sea urchin ··· salmon roe egg 
 
    . . . . . . . . . . . . . . .    
 fatty tuna tuna ··· salmon roe sea urchin 
 tuna tuna roll ··· cucumber roll sea urchin
shrimp salmon roe ··· tuna roll sea eel
Figure2: Ontheright,theproportionoftimesthati(rows)isrankedbelowi′(columns)inπ. Totheleft,thetopand
bottomrowsofπ areshown. Thetopmatrixtakesitemslabeledas{1,...,10}whichcorrespondstoshrimp,sea
eel,tuna,squid,sea urchin,salmon roe,egg,fatty tuna,tuna rollandcucumber rollasexemplified
atthebottom.
1.2 Modellingindifference
The proposed model is an extension of the MM given in equation (1) that allows the consensus π to contain ties.
0
Asbeforewedenotetieditemsiandj byi ∼ j toindicatethatthereisnotastrictpreferencebetweenthem. This
canbeinterpretedasreplacingπ withanorderedpartitionoftheitemset{1,...,n}wherealternativesinthesame
0
groupareexchangeable. ThismotivatesnamingourapproachtheClusteredMallowsModel(CMM).WithintheCMM
framework,clusteringoccursontheitemset,contrastingwithVitellietal.(2018),whereassessorsaregroupedinstead.
Toachievethis,eachitemisassociatedwithoneofL≤nclustersorgroups. Webeginbyassumingthatthenumberof
orderedclustersisfixed. ThechoiceofLwillbeaddressedlateron. Weintroducethenotationz =(z(1),...,z(n)),
avectorofunknownclusterallocationofi∈{1,...,n},wherez(i)denotestheorderedclusterallocationofitemi
wherez(i) ∈ {1,...,L}. Theinverseoperationz−1(l)meanslistingobjectsthatwereassignedlabell,definedfor
l∈{1,...,L}. Arankinterpretationofclusterlabelsisfixedbyimposingthefollowingtwoconditions:
Condition(I): i≻j ifz(i)≻z(j).
Condition(II): i∼j ifz(i)=z(j).
Thefirstconditionisinterpretedasitemiispreferredoveranyotherobjectwithahigherclusterlabel. Whilethe
secondimposesindifferencebetweenitemswithinthesamecluster,sothatifi∼j thenz(i)andz(j)areequal.
As an example, take z = (2,1,1,2,3). This allocation implies that items 2 and 3 are in the first cluster since
z(2) = z(3) = 1. Theyarealsobothtoppreferencessince2 ∼ 3andarepreferredovertheremainingitems. The
clusters({z−1(1)},{z−1(2)},{z−1(3)})are{2,3},{1,4},{5}. AusefulnotationforclusteredgroupsistheBrack
operation(Marden,1995). Denotedhereby{z}=({z−1(1)},...,{z−1(L)}),clustersareplacedinincreasingorder
ofclusterlabeland{·}indicatesinterchangeabilityofitemswithinacluster. Hence,{z}=({2,3},{1,4},{5})inthis
example.
Modelling π in terms of z can be advantageous over the MM when preferences are not all necessarily strict. A
straightforwardexampleiswhenalternativessharecommonfeaturesorsimplywhenassessorsarejointlyindecisive
aboutrankingsomeitems. Insuchcases,relativepreferencesp¯(i,i′)willbecloseto0.5asinourmotivatingexample.
Inaddition,itisreasonabletoassumesomeinterchangeabilitywhennismoderateorlarge.
Inferringz fromπ withtheCMMprovidesanswerstothefollowing. Doevaluationsdeterminestrictpreferences
between all i,j ∈ {1,...,n}? If yes, then L = n, and the CMM has the MM as a special case. Otherwise, the
immediatenextquestionishowmanyclustersarethereandtowhichgroupdoeseachobjectbelongto?
4CMM
LearningzistheprocessofdeterminingLandn ,...,n ,wheren
=(cid:80)n
I{z(i)=l}isthesizeofclusterl. If
1 L l i=1
n ,...,n isfreetovary,thenindifferenceisaccommodatedinanypartoftherankings. Onesettingthatcanresult
1 L
fromthiskindofflexibilityinthis’best-worsescaling’. Thisisachoicemechanismthatrecognizesextremestobe
moreeasilyrecognizedbyindividuals(Doignon(2023),Marleyetal.(2016),MarleyandPihlens(2012)). ACMM
withLclustersandn = 1forsmallandlargeLreflectsbest-worsescaling,forexample,n = 1,n = 8,n = 1
l 1 2 3
encodesthetopandthebottomchoicesofn=10items,withindifferenceintheranksoftheremaining8items.
Thepaperisorganizedasfollows. Insection2,theClusteredMallowsModelisdefined,andsomeconditionsonzand
θ areestablishedsothatthemodelisidentifiable. WeillustratehowtheCMMisrelatedtootherapproachesinthe
literaturesuchasproductpartitionmodels(Hartigan,1990;BarryandHartigan,1992)andtheInfiniteGeneralised
Mallows(IGM)(Meila˘ andBao,2010). TheCMMisdefinedintermsofdistancesbetweenorderedclustersforwhich
someoptionsaregiveninsection3. Theprobabilitymodelinvolvesanon-analyticalnormalisationandthisposes
challengestostatisticalinferenceandmodelsimulation. Thesepointsarehandledinsection4wherewepresenta
pseudo-likelihoodformulationandprovidemeanstoestimatetheCMM.FittingtheCMMtoobservedranksishandled
insections5and6.3. Bayesianinferenceforfittingthemodelparameterswithsomepre-specifiedstructure,thatis,L
andn ,...,n ,isthefocusofsection5. Insection6.3,analgorithmicsearchofthemodelstructureisdesignedfor
1 L
whenthegoalistolearnboththepartitionandparametersfromthedata. Finally,realexamplesdemonstratetheCMM
fitindifferentscenarios. Formula1racingresultsareconsideredforanalysisinsection7.1withLandn ,...,n
1 L
chosentoreflectcharacteristicsofthecompetition’spointsystem. WeconcludewiththeanalysisoftheTohokusushi
rankingsinsection7.2wherethemethodsdevelopedinsections6and5areappliedjointly. Inbothourrealdatasets,
comparisontotheMallowsmodelisexplored,andmodellingorderedclustersisdemonstratedtobepreferrableineach
example.
2 TheClusteredMallowsModelandrelatedmodels
Supposethatwehaveacollectionofqindependentranksfromapopulation,
π =(π ,...,π ), π ∈P ∀j =1,··· ,q.
1 q j n
The Clustered Mallows model (CMM) places a probability distribution on π and depends on parameters z ≡
(z(1),...,z(n))andθ > 0. Additionallyitdependsonameasureofagreementbetweenorderedclusters,whichis
denotedbyd (·,·)tomakeadistinctionfrom(1). Weprovidemoredetailsonpossiblechoicesford (·,·)inSection3.
oc oc
Fornow,letuswritetheCMMas
 
f(π|z,θ)=
(cid:89)q (cid:18) exp{−θd oc(π j;z)}(cid:19) =exp −θ(cid:88)q
d (π
,z)
Ψ(θ;z)−q, (2)
Ψ(θ;z) oc j
 
j=1 j=1
(cid:80)
whereΨ(θ;z)= exp{−θd (π,z)}.
π∈Pn oc
Themainaimofmodellingtheobservedcollectionπwith(2)istolearnaconsensusamongtheqassessorsandthe
variabilityaroundthis,allowingforthepotentialthatsomeitemsmaybeclusteredandthattheclustersthemselvesare
ordered. Inordertolearnzandθsomeconditionsonzneedtobeestablishedinordertoensurethattheparametersare
identifiable. Thisisbecausedistancesbetweenorderedclustersbehavedifferentlyinaccordancewiththenumberof
clustersL,anditemspercluster,n
(z)=(cid:80)nI{z(i)=l}.
Asthenumberofexchangeableitemsvariesinz,sotoo
l i
doesthedistributionofd (π,z). Forexample,ifn (z) = nandallitemsbelongtothesamecluster,thedistance
oc 1
willbezeroforanyπ. ThisisobviouslynotthecaseifL > 1. ItturnsoutthatLandn(z)determinethedistance
distribution,somethingthatwillbemadeclearinsection(3)onced isspecified.
oc
Fornow,letustermn(z)=(n (z),...,n (z))theClusteringTable(inshort,CT).TheidentificationoftheCMMis
1 L
establishedbyfixingtheCT,guaranteeingtheinterpretabilityofθ,andachievinganimportantsimplificationof(2).
GiventheCT,Ψ(θ;z)=Ψ(θ)andthenormalisationtermdoesnotdependonz. ThiscanbethoughtofastheCMM
counterpartoftheMM’sright-invarianceofd(·,·),underwhichϕ(α,π )=ϕ(α). Nonetheless,Ψ(θ)isstillnon-trivial
0
tocompute,andthisissueisaddressedinsection4.2.
FixingtheCTmeansthatthenumberofclustersanditemspergrouparepre-specified. Thisrequirementdoesnotmake
theproposedmodellimitedinpractice,butthechoiceoftheCTistreatedasamodelselectionproblem. Section6is
devotedtochoosingtheCTwhereanalgorithmicsearchisoutlined.
TheCMMprobabilitymodelwiththesetCTn(z)=(n (z),...,n (z))is
1 L
5CMM
(cid:110) (cid:111)
exp
−θ(cid:80)q
d (π ,z)
f(π|z,θ)= j=1 oc j , θ >0,z ∈ P n , (3)
Ψ(θ)q P ,×···×P
n1 nL
where d (π,z) is a comparison between the reference group {z} = ({z−1(1)},...,{z−1(L)}) and
oc
({π(1),...,π(n )}, {π(n + 1),...,π(n + n )},...,{π(n + ...,n + 1),...,π(n)}). The latter splits π
1 1 1 2 1 L−1
intoclustersofsizesn ,...,n ,accordingtotheCT,denotedby{π} orsimply{π}. Asanexample,withCT =
1 L CT
(n ,n ,n )=(2,2,1),z =(2,1,1,2,3)andπ =(2,1,3,4,5)thebaselinegroupis{z}=({2,3},{1,4},{5})and
1 2 3
theotheris{π}is({2,1},{3,4},{5}). Possiblewaystomeasurethedisagreementbetweenthetwoorderedclusters
arepresentedinsection(3). Beforegoingintothat,someimportantconnectionsoftheCMMarediscussedbelow.
TheCMMsharesacloseconnectiontothetiedMallowsmodelbyCritchlow(1985). Heretiedranksaredefinedasthe
processofpartitioningnelementsintoLorderedclusters,similartoourapproachhere. ThemainfocusofCritchlow
(1985)istoextendpopulardistancesforπ,π ∈ P totiesineitherorbothπ andπ . Theseareadoptedlaterby
0 n 0
Marden(1995)forhypothesestesting. Marden(1995)developsanapproachtotesttherandomnessofaranking,thatis,
H :π ∼Uniform(P ). Theprobabilitythatπisgeneratedfromthenullhypothesisisbasedonitsdistancetoatied
0 n
π∗ =(1,...,1)thatassignsallitemsthesamelabel. AvailingoftheextendedmetricsbyCritchlow(1985),likelihood
0
ratiotestsareconductedusinganexponentialfamilydistributionwithsufficientstatisticd(π,π∗).
0
Tothebestofourknowledge,noattentionwasdevotedtotheextendedmetricsbyCritchlow(1985). Theyareapplied
intheCMMasthepossibleformsofd wherethefocusisonrankingaggregationwithnon-strictnessintheconsensus
oc
ofπ =(π ,...,π ).
1 q
Another important related class is product-partition models introduced by Hartigan (1990), Barry and Hartigan
(1992). This is also an approach to partition n elements in L sets. Let S denotes the lth group for l = 1,...,L.
l
Product partition models assume that each object belongs to a single set, i.e., S ∩ S = ∅ for any l ̸= l′ and
l l′
∪L S = {1,...,n},whichisalsoinplaceforourmodel. However,thejointprobabilityofS ,...,S iswritten
l=1 l 1 L
asp(S ,...,S ) =
L(cid:81)L
c(S ),wherec(S ) > 0isameasureofsetcohesion. Thismeansthatproduct-partition
1 L l=1 l l
models assume between-set independence, something that does not hold in a CMM once rankings carry relative
information. ForareviewofBayesianproductpartitionmodels,werecommendQuintanaetal.(2018).
TheInfiniteGeneralisedMallows(IGM)modelbyMeila˘andBao(2010)isanotherimportantconnection,developedfor
top−krankings,π. Itassumesthatthefirstkitemsarerankedfromaninfinitesetofitemsandπismodeledaccording
(cid:101) (cid:101)
tod(cid:101)k(π (cid:101),σ). ThelatteristheKendalldistancetoaninfinitepermutationσ. Byd(cid:101)k itisdenotedthatthecomparisonis
doneupuntilrankk,whichisaformoftruncationofd . TheCMMisanalogoustotheIGMwhentheCTisn =1
k l
forl =1,...,kandn =(n−k). Thisisanalternative(finite)approachtothetaskthatsetsthefirstkassingle
k+1
elementclusters,andthefinalclustercontainstheremaining(n−k)items.
TheCMMovercomesapossiblelimitationoftheIGM,whichismentionedbytheauthorsinsection3.4. Whenthe
datadoesnotcontainenoughinformationabouttherelativerankingofsomeitempairiandi′orp¯(i,i′)=0.5,thenthe
optimalIGMconsensusdoesnothaveauniquesolution. Inthiscase,itisnecessarytoacquiremoresamples,which
mightnotalwaysbefeasible. TheCMMisasimplesolutiontothisissuethatsimplyassignsi,i′tothesamecluster.
3 Measuresofclusterordination
Thissectionspecifiesandcomparesdistancesbetweenorderedclusterswhichwillbedenotedasd . Herewebuild
oc
ontheworkofCritchlow(1985)wheremetricsofassociationinP areformallygeneralisedtothetiedspace. As
n
previouslyintroduced,comparisonsintheCMMarebetween{z}and{π} ≡{π}. Theseareorderedclustersof
CT
identicalsizes,andCTisatiepattern(Marden(1995)). Hence,Critchlow’sextendedmetricsareapplicabletoquantify
theagreementbetween{z}and{π}.
3.1 Hamming
The Hamming distance between permutations γ,ω ∈ P involves rank-wise comparisons I{γ(i) ̸= ω(i)} for
n
i ∈ {1,...,n}. The indicator function I{γ(i) ̸= ω(i)} is zero if and only if γ(i) and ω(i) take the same value.
Otherwise,I{γ(i)̸=ω(i)}=1. TheHammingdistanceisdenotedbyd (γ,ω),andisgivenbythesumd (γ,ω)=
h h
(cid:80)n I{γ(i)̸=ω(i)}. ThisisthesimplestassociationmetricinP ,andithasbeenextendedinCritchlowtoγ∗,ω∗
i=1 n
containingties.TheauthordemonstratesthattheHammingextensionisstraightforwardlyd (γ,ω)=(cid:80)n I{γ∗(i)̸=
oc i=1
ω∗(i)}=1. Inotherwords,imposingnodisagreementsbetweentherepetitionsyieldsavalidmetricinthespaceof
ties. IntheCMM,itwillbedenotedasd (π,z),andisdefinedbelow.
oc,h
6CMM
Definition1 TheHammingorderedclusterdistancebetweentheorderedclusters{z}and{π}is
n
(cid:88)
d (π,z)= I{z(π(i))̸=z }. (4)
oc,h (cid:101)i
i=1
Thesymbol·indicatessortinginincreasingorder,soz istheithelementofz. Byz(π(i))itismeantthatthegroup
(cid:101) (cid:101)i (cid:101)
labelinzisappliedtoπ(i).
Example: Take the previous z = (2,1,1,2,3) and π = (2,1,3,4,5). The sorted z is (1,1,2,2,3) and
(cid:101)
z(π(1)),...,z(π(5))is(1,2,1,2,3).TheHammingdistanceisd (π,z)=I{1̸=1}+I{1̸=2}+···+I{3̸=3}
oc,h
=2.
3.2 Kendall
TheKendallmetricinP ,hered (γ,ω),comparestheorderrelationofpairs{γ(i),γ(j)}and{ω(i),ω(j)}. Forall
n k
i̸=j, i,j =1,...,neachofthefollowingtwocasesγ(i)>γ(j),ω(i)<ω(j)orγ(i)<γ(j),ω(i)>ω(j)counts
asadisagreement. Asummationoverallsuchdisagreementsof{i,j}combinationsgivestheKendallmetricinP .
n
TheextensionofthisideatotiesisdevelopedinCritchlow,andweadoptittointroducetheKendallorderedcluster
distance. Thismetricisdenotedbyd andisdefinedasfollows.
oc,k
Definition2 TheKendallorderedclusterdistancebetween{z}and{π}isgivenby
 
L−1 L L j
(cid:88)(cid:88) (cid:88) (cid:88)
d oc,k(π,z)= n ij n i′j′, (5)
i=1 j=1 i′=i+1j′=1
wheren isthenumberofitemsplacedingroupiin{π}andingroupj of{z}.
ij
Example: Considerz = (2,1,1,2,3)andπ = (2,1,3,4,5)weobtainn = 1,n = 1,n = 0,n = 1,n =
11 12 13 21 22
1,n =0,n =0,n =0,n =1. TheKendallorderedclusterdistanceisthend (π,z)=n (n +n )+
23 31 32 33 oc,k 11 21 31
n (n +n +n +n )+n (n +n +n +n +n +n )+n n +n (n +n )+n (n +n +n )=
12 21 22 31 32 13 21 22 23 31 32 33 21 31 22 31 32 23 31 32 33
1(1)+1(2)+0(3)+1(0)+1(0)+0(1)=3.
Figure(3)illustratesthedistributionofd andd obtainedfromdifferentCTconfigurations. Asitwasmentioned
oc,h oc,k
insection(2),theCTdeterminesthedisagreementsthatcanbemade,hencethed distribution. Thisisbecausethe
oc
clusteringtablestipulatestheindistinguishableitems,andindifferencedoesnotcontributetod . Astobeexpected,
oc
d valuesdependalsoonthetypeofcomparison,soFigure(3)showstheHammingandKendallcasesforeachCT.
oc
Thenextsubsectionisdevotedtofurtherillustrateandcompared (π,z)andd (π,z)whenusedinEquation(3)
oc,h oc,k
togetherwithsomeparameterconfigurationstoexemplifytheClusteredMallowsModel.
3.3 Distancesproperties
TheKendallandHammingorderedclusterdistancesarenowillustratedundersomeconfigurationsofzandθ. Ouraim
istoprovideafurtherunderstandingoftheirpropertiesinthecontextoftheCMM.Weconsiderz =(1,1,2,2,3,3)
1
and z = (1,1,1,2,2,3,3,3,4) which encode the ordered clusters {z } = ({1,2},{3,4},{5,6}) and {z } =
2 1 2
({1,2,3},{4,5},{6,7,8},{9}). Setting z has three equally sized clusters of n = 6. In z , n = 9 items are
1 2
partitionedintoL=4clusters. Forthissetting,therearethreetopchoices(items{1,2,3}),{9}istheworsealternative,
andothersareintwointermediateplacings.
Tostudythesesettingsalongsided ,d ,ausefulsummaryistheaggregationbyrankandcluster. Thisisthe
oc,h oc,k
probabilitythatthemodeltakesanelementofclusterl∈{1,...,L}ineachranki. Definition3formalisesthisconcept,
whichwenametheRankCluster(RC)probability.
Definition3 TheCMM(z,θ)Rank-Cluster(RC)probabilityis
(cid:88)
RC(i,l)=P(z(π(i))=l|z,θ)= f(π|z,θ)I{z(π(i))=l}=E [I{z(π(i))=l}] (6)
f(·|z,θ)
π∈Pn
fori∈{1,...,n}andl∈{1,...,L}. TheindicatorfunctionI{z(π(i))=l}=1,ifz(π(i))=landzero,otherwise.
7CMM
Figure 3: Distribution of the Hamming and Kendall ordered cluster distances d ’s under some example CTs that
oc
partitionn=6items.
We illustrate various RC probabilities using z as defined above and each of the two possible values θ = 0.5 or
1
θ = 1inFigure4. Ineachwindow,barscorrespondtoranksi ∈ {1,...,6},coloredaccordingtotheprobabilities
ofl ∈ {1,2,3}. ThefirstrowiscomputedusingEquation(3)andd ,withθ = 0.5ontheleftandθ = 1onthe
GO,h
right. Thesamevaluesofθareusedwhencomputing(3)withd inthesecondrow. Theeffectofincreasingθis
GO,k
tosharpentheprobabilityofacorrectclusterassignmentforalliforeitherdistance. Correctclusterassignmentsare
thosethatpreservez =(1,1,2,2,3,3),i.e.,z(π(1)),...,z(π(6))=z . Thespreadaroundz isreducedasθ →∞,
(cid:101)1 (cid:101)1 (cid:101)1
shownbyincreasedredbarsforranks1and2,greenfori∈{3,4},andbluefori∈{5,6}.
Figure4: CMMrank-clusterprobabilitiesunderHammingandKendallorderedclusterdistanceswithz =(1,1,2,2,3)
andθ =0.5ontheleft,θ =1ontheright.
Inadditiontotheeffectofθ,Figure4illustratesthattheclusteringinztranslatestoRC.Ifi,i′areranksthatexpect
thesamecorrectcluster,i.e. z =z ,thenRC(i,l)=RC(i′,l) ∀l =1,...,L. Forexample,thefirstandsecond
(cid:101)i (cid:101)i′
bars(i=1,i′ =2)areidenticalinallwindowssincez =z =1. Onthecontrary,i=1,i′ =3aredifferentonce
(cid:101)1 (cid:101)2
z =1,z =2. Weexpandonthispropertyinthenextremark.
(cid:101)1 (cid:101)2
8CMM
Remark1 Ifj ̸=j′aretieditemsfromtheset{1,...,n},thenz(j)=z(j′)=lforsomelin{1,...,L}.Indifference
between j and j′ immediately implies that they have the same probability of being ranked ith. Mathematically
P(π−1(i)=j)=P(π−1(i)=j′)ifz(j)=z(j′). Rankexchangeabilityisanimplicationofitem-indifferencewhich
canbeexpressedasP(z(π(i))=l)=P(z(π(i′))=l)∀lifz =z′.
(cid:101)i (cid:101)i
SimilarbehaviourisobservedinFigure5,whichfocusesonz . Thisconfigurationhastheclusteringtable(3,2,3,1)
2
whichshowstheequivalenceoftheRC’s{1,2,3},{4,5}and{6,7}. TheHammingdistanceandθ =0.9areusedto
obtaintheRCvaluestotheleftofFigure5. Ontheright,d correspondstotheKendalldistanceandθis0.5. Figure
oc
5showsthattheKendallorderedclusterdistanceismorearoundz incomparisontotheHammingorderedcluster
(cid:101)2
distance,evenwiththesmallerθ =0.5.
Figure5: Rank-clusterprobabilitiesoftheHammingorderedclusterdistancewithθ = 0.9ontheleft,andKendall
ordered cluster distance with θ = 0.5 on the right. The cluster allocations are z = (1,1,1,2,2,3,3,3,4) in this
2
example.
Itisimmediatefromthedefinitionsthatd (π,z) ≥ d (π,z)forfixedπ. Makingθ valuescomparablecould
oc,k oc,h
beachievedwithnormalisation. Whentherearenoties,d takesvaluesin[0,n]andd ∈
[0,(cid:0)n(cid:1)
]. However,there
h k 2
is no expression for max d (π,z) or max d (π,z). See Critchlow (1985) for further details. The
π∈Pn oc,h π∈Pn oc,k
consequenceisthatθwillnotbecomparablebetweenCMMstakingdifferentdistances. Thisisaremarkthatshouldbe
borneinmindfortheremainderofthepaper.
4 Computationalaspects
Theorderedclustersdistancespresentedinsection3completethespecificationoftheCMM.Wearenowreadyto
tackleaspectssuchasmodelsimulationandinference. Insection4.1,wehandlethesamplingtask,wherethegoalisto
producerandomrealisationsfromaCMMwithspecifiedθ,z,d andCT.Subsequently,section4.2detailsthemodel’s
oc
normalisationconstantΨ(θ),whichisnon-trivialtocompute.
4.1 Modelsimulation
Samplingfromdistance-basedprobabilitymodelsisoftenanon-trivialtask. IntheMallowsmodel,approachestodraw
exactlyfrom(1)arelimitedtosomedistancechoicesandmoderatenvalues. Oneexampleisthedistancesalgorithm
(Irurozkietal.,2019),whichrequiresknowingthenumberofpermutationsateachdistancevalue. Mostoften,πis
sampledapproximately,usingMCMCmethods.
MCMCsamplingπ ∈P oftenfollowsthestrategyproposedbyDiaconis(2009). Inthisalgorithm,acandidatestateπ′
n
isgeneratedfromπviaarandomtransposition.Thetranspositionisbetweenapair{i,j}wherei,j ∈{1,...,n},i̸=j
aredrawnuniformlyatrandom. Theproposedstateπ′resultsfromπbyswappingπ(i)withπ(j)andviceversa. This
isthenacceptedorrejectedwiththeusualMetropolisprobability.
Ithasbeenshownthatthissimpleandcomputationallyefficientmethodconvergesquicklytothetargetstationary
distribution. TheguidelineproposedbyDiaconis(2009)isthatnlognstepssufficetoreachstationarity,approximately.
ThisstrategyiswidelyappliedtosampletheMM,andisoftenanefficientsampler. Itextendsdirectlytoasample
fromaCMMwithparametersz,θandsomed . ThisisdescribedinAlgorithm4.1. AMarkovchainwithstationarity
oc
9CMM
distributionf(·|z,θ)isconstructedusingthepairwisetranspositionsproposal. ThelaststateafterN iterationsofthe
algorithmistakenasanapproximatedrawfromtheCMM.Naturally,theinitialzconveysthemodel’sCT.
Algorithm1:DrawingfromaCMMprobabilitymodelusingN iterationsofaMetropolisalgorithm. Asymmetric
pairwiseswitchproposalgeneratescandidatestates,andthelastiterationofthechainisapproximatelyaCMM(z,θ)
draw.
Input: N :numberofiterations;initialπ;modelparametersθ,z,d choice
oc
fori∈1:N do
Samplej,kfrom{1,...,n}withoutreplacement;
Setπ′ ←π
Switchpositionsj andk: π′(j)←π(k)andπ′(k)←π(j);
(cid:110) (cid:72) (cid:111)
Computeα(π′)=min 1,f(π′|z,θ) = exp(−doc(π′,z)) (cid:72)Ψ((cid:72)θ) ;
f(π|z,θ) exp(−doc(π,z))Ψ((cid:72)θ)
Withprobabilityα,setπ ←π′;
end
Output: π
CollectingqindependentCMMsamplescanbeachievedwithaparallelimplementationofAlgorithm(4.1),andthis
willbedenotedbythefunctionrCMM(z,θ,N,q). Notethattheintractablenormalisationterm,Ψ(θ),cancelsinthe
Metropolisacceptanceprobabilityα(π′).
4.2 Pseudolikelihoodapproximation
ThissectionisdevotedtotheproblemofcomputingtheCMMprobability,f(·|z,θ)forobservedπ. Thelatterdepends
(cid:80)
on the constant Ψ(θ) = exp{−θd (π,z) which is generally intractable. To address this, we formulate
a pseudo-likelihood model,π w∈ hP in ch can be uo sc ed in different ways. One immediate usage is to simply replace the
true likelihood and carry out approximate inference with this tractable counterpart. Another direction is to gather
pseudo-likelihoodsamplestoconstructastatisticalestimatoroff(π|z,θ).
Letf(cid:101)(·|z,θ)denoteamultistageapproximationofthetruef(·|z,θ), ourpseudo-likelihoodmodel. Bymultistage,
we mean that π is formed with a sequence of n−1 independent decisions as explained next. Starting from the
completesetofitems,arankingstartswiththechoiceofπ(1). Oncethisisdetermined,π(2)isadecisionontopof
{1,...,n} where−π−1(1)denotesthatπ−1(1)hasbeenremovedfromthepossiblealternatives. Thisgoeson
−π−1(1)
sequentiallyforπ(2),...,π(n−1),untilasingleitemisleftforπ(n). Theprocessofdefiningπthroughsequential
selectionsoriginatingfromtop-rankingpositionsisknownasforwardranking.
Leti = 1,...,n−1indexthen−1choicesandσ denotetheitemsavailable(unranked)atstepi. Startingwith
i
σ ={1,...,n},weapproximatetheconditionaldistributionp(π(i)|z,θ,σ )fori=1,...,n−1asfollows. First,
1 i
a cluster decision chooses the group label z(π(i)) of the i−th ranked object. Then, the item stage selects π(i)
conditionallyonz(π(i)). Inotherwords,theclusterstepdefinestheorderedgroupthatoccupiespositioniandgiven
z(π(i))=l,theitemπ(i)isdrawnuniformlyatrandomfromthesetofunrankedlabellelements.
Theclusterstageworkswithanapproximationoftherank-clusterprobabilitiesRC(i,l)(3)thatwillbedescribed
inwhatfollows. LetCT denotethecountofavailableitemsatstepiforlabelsi = 1,...,L. FromCT , wecan
i i
approximatetheRC(i,l)probabilityasfollows. WeintroducethetermD(l|CT ),whichisanindicatorfunctionof
i
CT . ThepurposeofD(l|CT )istoapproximatethenumberofdisagreementsthatcanbemadeifaclusterlitemisin
i i
π(i). ThisisdonebysettingD(l|CT ) = 0iflisthesmallestlabelintheunrankedset,i.e. n (σ ) > 0. Otherwise,
i l i
D(l|CT i) = n l(z(σ i)). TheapproximateprobabilityR(cid:103)C(i,l)isobtainedfromnormalisationofexp{θD(l|CT i)},
givenby
exp{−θD(l|CT )}
R(cid:103)C(i,l)= i . (7)
(cid:80)L
exp{−θD(l|CT )}
l=1 i
TodrawfromtheCMMpseudo-likelihood,thefirststepisthentosamplez(π(i))from{1,...,L}withprobabilities
R(cid:103)C(i,1),...,R(cid:103)C(i,L). Or,equivalently,gather(7)forsomeobservedπfromz(π(i)). Givenz(π(i))=l,theitem
(cid:80)
probability is simply 1/ CT , in line with the model assumption of within-group equivalence. To sample from
i
f(cid:101)(·|z,θ), anitemlabelisthenuniformlydrawnfromclusterl objectsthatarecurrentlyunranked. Afterthat, itis
removedfromσ whichistheavailablesetatthenextstage.
i+1
10CMM
i σi CTi D(l=1,2|CTi) R(cid:103)C(i,1=1,2) f(cid:101)(π(i)|σi,z,θ)
1 (1,2,3,4) (2,2) (0,2) (0.731,0.269) 0.731×0.5
2 (3,2,4) (1,2) (0,2) (0.731,0.269) 0.269×0.5
3 (2,4) (1,1) (0,1) (0.622,0.378) 0.622×1
Table1: Probabilityofπ =(1,3,2,4)computedfromtheCMMmultistageapproximation,f(cid:101)(π|z,θ).
With this strategy, the probability f(cid:101)(π(i)|σ i,z,θ) becomes a product of the two (cluster and item) steps,
f(cid:101)(π(i)|σ i,z,θ) = R(cid:103)C(i,l)(1/n l(z(σ i))), wheren
l
= (cid:80)# j=σ 1iI{z(σ ij) = l}. Conditionalindependenceofchoices
i=1,...,n−1givesusthattheCMMpseudo-likelihoodmodelf(cid:101)(π|z,θ)is
n−1
(cid:89) 1
f(cid:101)(π|z,θ)= R(cid:103)C(i,l) . (8)
n (z(σ ))
l i
i=1
Computationoff(cid:101)(π|z,θ)inasmallexamplewheren=4isillustratednextforclarity.Threeconditionallyindependent
choicesaremadeondecreasingitemsetsstartingfromthefirstrank. Eachchoiceisformedbythetwo-stageprocedure
thatfirstselectsaclusterlabel,andthenaniteminthatgroup.
Example: Let us consider z = (1,1,2,2) and θ = 0.5. We work out the probability f(cid:98)(π = (1,3,2,4)|z,θ) in
Table1. Thestep-specificsetofavailableitemsσ anditsclusteringtablearelistedinthefirsttwocolumns. The
i
CT is transformed into D(l = 1,2|CT ) in column 3. This term assumes 0 for the smallest l, and n (z(σ ))
i i l i
otherwise,favouringtheordinationofclusterlabels. TheD(l = 1,2|CT )isusedtocompute(7),andtheproduct
i
R(cid:103)C(i,l)(1/n l(z(σ i)))isf(cid:101)(π(i)|σ i,z,θ).Thefinalvalueoff(cid:101)(π|z,θ)istheproductinthefifthcolumn,inthelogarithm
scale,logf(cid:101)(π =(1,3,2,4)|z =(1,1,2,2),θ =0.5)=−3.487. TheCMMprobabilityforthissettingis−3.344with
theHammingorderedclusterdistance,and−3.593withtheKendallorderedclusterdistance.
AnimportancesamplingestimatorofΨ(θ)cannowbecomputedusingindependentdrawsfromf(cid:101)(π|z,θ). Inthis
strategy,theCMMpseudo-likelihoodactsastheimportancedensity,itbeingadistributionthatresemblesthetarget,
f(·|z,θ). TheimportancesamplingestimatorofΨ(cid:98)(θ)basedonM pseudo-likelihooddrawsisgivenby
M
Ψ(cid:98)(θ)=
1 (cid:88)exp(−θd oc(π j,z))
. (9)
M
j=1
f(cid:101)(π j|z,θ)
Evidently, the true value of Ψ(θ) is a function of n and θ which grows as n → ∞ and θ → 0. In addition, it is
importanttonotethatthevariabilityofΨ(cid:98)(θ)issensitivetothechoiceofM. Withtheimportancedensity(4.2),we
havethatVar(Ψ(θ))=E [(w(π)−1)2]/M,wherew(π)=f(cid:101)(π|z,θ)/f(π|z,θ). ThisimpliesthathighM maybe
f(cid:101)
required depending on the precision to which Ψ(cid:98)(θ) must be computed. In the applications of (9) explored in this
paper,replicationofΨ(cid:98)(θ)withfixedM isusedtoaccountforitsvariability. Ourapproachsharessimilaritiestothe
methodologyinVitellietal.(2018),whereimportancesamplingwithapseudo-likelihoodhasbeenusedsuccessfully
fortheMallowsdistribution. Theirlikelihoodapproximationisalsobasedonaforward-rankingmultistageformulation,
usedinIStoestimatetheMallow’snormalisationfordistanceswherethistermdoesnothaveaclosedform. Naturally,
theirmethodologyisnotdirectlyapplicablesowehavedesignedapseudo-likelihoodthatembedstheCMMclustering
structure.
Ifthecomputationalcostrelatedtothisprocedurebecomesprohibitive,oneoptionistousef(cid:101)(π|z,θ)asanapproxima-
tionofthetruef(π|z,θ). Inferencewithapproximate(orcomposite)likelihoodshasbeenwidelyappliedtoproblems
involvingcomplexjointprobabilities. Whentheunderlyingmultivariateprobabilitymodeliscumbersometoevaluate,
compositelikelihoodmethodsadoptsimplificationsoftheunderlyingdependencystructure. Compositelikelihood
ideascanbeusedforfastapproximateinferencewiththeCMM,andwerecommendVarinetal.(2011)formoredetails
onthetopic.
4.2.1 Modelselection
CMMmodelselectionisthetaskofchoosingtheCTthatbestfitstheobserveddata,foraspecifieddistanced . A
oc
commonstrategyinthisregardistousecriteriathattaketheoptimizedlog-likelihoodandappliessomepenalisation
11CMM
for complexity. For instance, the Bayesian Information Criterion (BIC) for a CMM model M is BIC(M) =
klog(q)−2f(π|z (cid:98),θ(cid:98)),where{z (cid:98),θ(cid:98)}aretheparametervaluesthatmaximizef,andkisthenumberofmodelparameters.
TheBICpenalisation,however,isnotimmediateforaCMMbecausethenumberoffreeparameterskisnotdefinedin
theusualsense. Inourcontext,thesizeofzisalwaysn,andwhatchangesisthenumberofrepeatedelements.
We design a suitable penalisation that is based on the possible distinct arrangements of z for a given CT. With n
items, theleastparameterizedmodelpossibleisCT = n, theUniformdistributiononP . Attheotherendisthe
n
Mallows Model, where n = 1 ∀ l = 1,...,L, where L = n, and each item is its own cluster. The number of
l
possiblearrangementsare,respectively,1andn!and,ifconfigurationsareequallylikely,theprobabilityp(z|CT)is
(n!/(n !×n !))−1. ThiscanbeseenasanuninformativepriordistributionforzconstrictedontheCT.Ifthisisaprior
1 L
distributionofz,thenthejointprobabilityf(π,z|θ)=f(π|z,θ)p(z|n(z))isautomaticallypenalisedforcomplexity.
TheCMMselectioncriterionthatarisesfromthisobservationisthen
I(cid:98)(CT)=logf(cid:98)(π|z (cid:98),θ(cid:98))+logp(z (cid:98)|n(z)), (10)
where the likelihood is estimated using (9). The above formulation has similarities with the Integrated Complete
Likelihood(ICL,Biernackietal.(2000))criterionwhichiswellcementedinmodel-basedclustering. Thelatterisbased
ontheideaofmaximizingthejointprobabilityofamixtureandtheallocationsvector,whichisinsomeresemblance
withf(π,z|θ).
Withtheingredientsjustoutlined,wearereadytodevelopinferencewiththeCMMandanobservedcollectionofranks.
Thisisdonebyassumingthatπisarealisationofmodel(2)andthegoalistolearnaboutzandθ. Section5handles
Bayesianinferenceforz andθ givenamodelstructure,CT.Algorithmstosamplethedoubly-intractableposterior
arespecifiedandwealsohandleincompleteobservationsofπviadataaugmentation. Strategiesformodelstructure
evaluationfollowinsection6.3. AnalgorithmisdesignedtotackletheCTsearch,whichisusefulwhenthegoalisto
learnboththemodelstructureandparametersfromobserveddata.
5 Inference: modelparameters
Inthissection,ourfocusisontheposteriordistributionofz,θgiventhedata,π,
 
q
 (cid:88) 
p(z,θ|π)∝f(π|z,θ)p(θ)p(z)=exp −θ d (π ,z) Ψ(θ)−qp(θ)p(z), (11)
oc j
 
j=1
wherep(z)andp(θ)arepriordistributionsforzandθ,respectively. OurstrategyistouseMCMCsamplingtoinfer
(11). Tothisend,aMarkovchainthathasstationarydistribution(11)isconstructed. Wedothisbydrawingfromthe
full-conditionaldistributionsofθandz. Algorithmstosamplep(z|π,θ)andp(θ|π,z)areoutlinednext.
Thefull-conditionaldistributionforziswrittenas
 
q
 (cid:88) 
p(z|π,θ)∝exp −θ d (π ,z) p(z), (12)
oc j
 
j=1
anddoesnotinvolvetheintractableconstantΨ(θ). Hence,standardMCMCtechniquesaresuitabletosample(12).
Metropolismovesthatproposecandidatestatesfromrandomswitchesareonepossiblestrategy. Frominitialstateszt
andθt,theproposaldenotedasswitch(zt,m)randomlyswitchesmelementsofz. Thisisequivalenttothestrategy
employedinAlgorithm(4.1),wherem=2. Thealgorithmacceptsz′asitsnextstatewiththeMetropolisprobability
α(z′)whichis
  
α(z′)=exp
−θt
(cid:88)q
d oc(π j,z′
)−(cid:88)q
d oc(π
j,zt) pp (( zz t′)
).
 
j=1 j=1
Given zt+1, the next step is to sample θt+1 from p(θ|π,zt+1). This is given by p(θ|π,zt+1) ∝ f(π|z,θ)p(θ) =
(cid:110) (cid:111)
exp −θ(cid:80)q d (π ,zt+1) Ψ(θ)−qp(θ). UnavailabilityofΨ(θ)makessamplingp(θ|π,zt+1)anon-trivialtask.
j=1 oc j
Forinstance,aMetropolis-Hastings(MH)algorithmrequiresevaluatingΨ(θt)/Ψ(θ′)intheacceptanceprobability.
SpecialisedmethodologyhasbeendevelopedforMCMCsamplinginthiscontextwherethelikelihoodinvolvessome
non-analyticalterm.Theproblemsinthisclassareoftencalleddoubly-intractable(Murrayetal.(2006))fromlikelihood
intractabilityinadditiontothemodelevidence. Somepossibleapproachesarepseudo-marginalMCMC(Andrieuand
12CMM
Roberts,2009),theExchangeAlgorithm(EA)(Murrayetal.,2006)andthenoisyMetropolisalgorithm(Alquieretal.,
2016).
We adopt a version of the EA to sample p(θ|π,zt+1) using tools presented in Section 4. In EA, cancellation of
intractabletermsisachievedintheMHratiousinganaugmentationstrategy. Thetargetposteriorisaugmentedwith
auxiliarydrawsfromf andthejointacceptanceofθ′ ∼h(·|θt)andπ′ ∼f(·|θ′,zt+1)isevaluated.Writingf(·|θ,z)as
q(π|θ,z)/Ψ(θ)whereq(π|θ,z)=exp{−θ(cid:80)q
d (π ,z)}allowsustoseethattheEAacceptanceratioistractable.
j=1 oc j
Itisgivenby
(cid:26) p(θ′)q(π|θ′,zt)q(π′|θt,zt)h(θt|θ′)(cid:88) Ψ((cid:88) θt(cid:88)(cid:88))q(cid:88) Ψ((cid:88) θ′(cid:88))q(cid:27)
α(θ′)=min p(θt)q(π|θt,zt)q(π′|θ′,zt)h(θ′|θt)(cid:88) Ψ((cid:88) θ′(cid:88))q(cid:88) Ψ((cid:88) θt(cid:88)(cid:88))q .
Withthiscleveraugmentationchoice,theunavailabletermscancelinα(θ′).TheEAstepsjustdescribedaresummarised
inAlgorithm2. Therein,STEP2callsthesamplingroutinerCMM(z,θ,N,q)toobtaintheauxiliarydata. Sinceπ′is
anapproximatedrawfromaGibbsrun,Algorithm(5)isaApproximateExchangeupdate. TheApproximateExchange
Algorithm(AEA)wasintroducedinCaimoandFriel(2011)andisanimportantextensionoftheEAthatoriginally
requiresexactsamples.
Algorithm2: AEA(θt,zt,N)ApproximateExchangeAlgorithmforsamplingthefull-conditionaldistributionof
θ.
Input: Current θt,ztandobserveddataπ
STEP1: Drawθ′fromsomeproposaldistribution,θ′ ∼h(·|θ);
STEP2: Sampleauxiliarydataπ′fromtheCMMusingtherCMM(z,θ,N,q)routine;
STEP3: Acceptθt+1 ←θ′withprobability
(cid:26) p(θ′)q(π|θ′,zt)q(π′|θt,zt)h(θt|θ′)(cid:27)
α(θ′)=min 1, .
p(θt)q(π|θt,zt)q(π′|θ′,zt)h(θ′|θt)
otherwiseθt+1 ←θt;
WiththeMetropolisandAEAmoves,BayesianinferenceistheresultofrunningtheMarkovchainthatupdatesinturn
zt+1 ∼p(·|π,θ)andθt+1 ∼p(·|π,zt+1). Runningthechainforalongenoughnumberofiterations,wecancollect
drawsandtreatthenasrealisationsfromtheposterior(11).
5.1 Partialobservations
PartialobservationofranksiseasilyaccommodatedintheBayesianapproachwithdataaugmentation. Anincomplete
observationofπ,denotedbyπ,cancomefromsettingssuchastop−kelicitationorpairwisepreferences. Inanyof
(cid:101)
suchcases,wewillassumethatπreflectsincompleteinformationaboutnalternativesandembedstheobservedpiece.
(cid:101)
Forexampleintop−kdata,oneobservesπ(1),...,π(k),however,π(k+1),...,π(n)arenotranked,andweconsider
(cid:101) (cid:101) (cid:101) (cid:101)
themasmissing.
Withacollectionofpartiallyobservedranksπ ≡(π ,...,π ),wecantreatπ ’sasunobservedrandomvariableswith
(cid:101) (cid:101)1 (cid:101)q j
somedistributionp(π ). Auniformatrandomarrangementoftheunrankeditemsisareasonableassumptioninthe
j
absenceofadditionalinformation. Dataaugmentationbringsπtoπandthetargetposteriorbecomesp(π,z,θ|π).
(cid:101) (cid:101)
Fromaninitialsetπt,thecompletedataupdateproposeseachπ′ byrandomlypermutingtheunrankeditemstofillthe
j
missingpositionsofπ .Theprobabilitytoacceptπt+1asπ′ isα(π′)=min(cid:8) 1,exp(cid:8) −θ(d (π′,z)−d (πt,z))(cid:9)(cid:9)
(cid:101)j j j j oc j oc j
givencurrentvaluesofθandz.Iteratingforj =1,...,qgivestheupdatedcompletedataπt+1andzt+1 ∼p(·|πt+1,θ)
andθt+1 ∼p(·|πt+1,zt+1)aredrawnasbefore.
6 Inference: learningtheCTstructure
ThissectionisdevotedtooptimizingtheCMMprobabilityofπformultipleCTs. Ourgoalhereistocarryoutfast
optimizationoff(π|z,θ)underasetofcandidatestructures. Inotherwords,wewouldliketofitz (cid:98)andθ(cid:98)whichare
themaximumlikelihoodestimatorsofzandθ. Oncez (cid:98)andθ(cid:98)areobtained,modelstructureselectionisbasedonthe
bestpossibleallocationandspreadundereachgivenCT.Thedecisioncanbemadewiththeinformationcriterionin
Equation(10)oradata-basedmetricthatwewillintroducelateron.
13CMM
Hence, fitting z
(cid:98)
and θ(cid:98)is pursued next with the main focus being on model selection. Our key concern here is to
minimizethecomputationalcostrelatedtofittingthemodelforvariousCTs. Onceamodelstructureischosen,itcan
beusedwiththeBayesianmethodoutlinedinsection5. Thisway,theposteriordistributionsofthechosenmodel
parametersarederived,anduncertaintyisadequatelyquantified. Thistypeoftwo-stepstrategyisnotnovel,being
well-knownandeffectiveintheBayesianNetworks(BNs)literature. InBNs,heuristicsearchesarecommonlyutilized
toexplorepossiblegraphstructuresandchoosethegraphtopology. Onceastructureisselected,inferentialmethods
likeMarkovChainMonteCarloareemployedtoestimatethemodelparametersofthechosenstructure. Formore
detailsonBayesianNetworksinferenceseeLarrañagaetal.(2013),ScutariandDenis(2021),KoskiandNoble(2011).
AnotherrelatedexampleisRafteryandDean(2006),whereaninformationcriteria-basedstrategy(BIC)isappliedfora
combinedsearchofvariableandclusterselection.
Themaximumlikelihoodestimatorsofz andθ arefoundasthesolutionofmax f(π|z,θ). Itisstraightforward
z,θ
toseethatz
istheconfigurationthatminimizesthesumofdistances(cid:80)q
d (π ,z)givenaCT.Thez optimum
(cid:98) j=1 oc j
doesnotdependonθoncemin((cid:80)q
d (π
,z))=min(θ(cid:80)q
d (π ,z))foranyθ >0. TheMLEofθfollows
j=1 oc j j=1 oc j
conditionally on z by working out the solution to S′(θ) = 0 where S′(θ) is the score equation for θ. We start by
outliningaproceduretoderivez
(cid:98)
thatusessimulatedannealing. Givenθ(cid:98), θ(cid:98)isestimatedusingthemethodologyof
Critchlow(1985).
6.1 Allocations: SimulatedAnnealingSearch
Simulatedannealing(SA)isanoptimizationtechniquethatreliesonMCMCsampling. Itcanbestraightforwardly
applied to optimize f(π|z,θ) in terms of z using ideas introduced in previous sections. Maximizing the CMM
probabilitywithrespecttotheallocationszisequivalenttominimizing(cid:80)q
d (π,z). Giventhatθisgreaterthan
j=1 oc
zero,theoptimumzisthesameforanyθ.
(cid:98)
Denotebyg(z)theCMMprobabilityasafunctionofzthattakesobserveddataπandθ =1. SAisbasedontheidea
thatpowersg(z)α makef moreconcentratedarounditsmodeasα → ∞. Metropolis(orMH)movesareusedto
samplefrompowerszt ∼g(·)αwhereα increaseswithiterationst=1,...,T. Theprobabilitytomovefromzttoz′
t t
istheMetropolisratiop(z′)=min{1,(g(z′)/g(zt))αt},underasymmetricproposal. Asα tincreases,thetransition
probabilityconvergestooneifg(z′)>g(zt),zerootherwise. Thesamplershouldeventuallyreachmax g(z)and
z
stopacceptingnewmoves. ThekeyingredientofSMisdefiningadiscretesetofpowersα <α <···<α ,which
0 1 T
iscalledtheannealing(orcooling)schedule. Itisoftenrecommendedtostartfromsmallα toavoidlocaloptimums.
t
AnSAalgorithmcanbeformulatedinasimilarspirittothemethodsofsection(5). Asbefore,theswitch(zt,m)
proposalsgenerateperturbationsz′andp(z′)is
 exp(cid:16) −(cid:80)q
d (π
,z′)(cid:17)αt
j=1 oc j
p(z′)= (cid:16) (cid:17) . (13)
exp −(cid:80)q d (π ,zt)
j=1 oc j
It is easy to see that α plays the role of the CMM spread in the SA acceptance probability and the procedure is
t
equivalent to the Metropolis algorithm of section (5). We can use this fact to guide the setup of α, the annealing
schedule. Theinitialα shouldbecloseto0toallowforexplorationofthez space. Thisstartingtemperaturecan
0
thenbeincreasedbysomefixedamounts>0untilamaximumα . Werecommendsettingα frominspectingthe
T T
CMMphasetransition. Thisisthepointwherefurtherincreasingθeffectivelycausesnochangeinthedistribution.
Alternatively,thegeometricscheduleisapopularchoice. Thisisgivenbyα =βtβ forsomesmallβ andβ >1.
t 0 0
Analgorithmicdescriptionofthisoptimizationroutineifprovidedinthesupplementarymaterial(4.1). Inwhatfollows,
itwillbedenotedbyAnnealing(π,α,m),whereαistheannealingscheduleandmisthenumberofswitches.
6.2 θ: IterativeMethod
Givenz,theMLEofθisfoundfromthesolutionofS′(θ)=0. Thisisgivenby
(cid:98)
−θ(cid:80)q
d (π
,z)exp{−θ(cid:80)q
d (π
,z)}Ψ(θ)−q−qΨ(θ)−q−1exp{−θ(cid:80)q
d (π ,z)}=0. Itisshownin
j=1 oc j j=1 oc j j=1 oc j
Critchlow(1985)that,withsomealgebraicmanipulation,wecanwritethisas
q
1(cid:88)
E [d (π;z)]= d (π ,z), (14)
f oc (cid:98) q oc j (cid:98)
j=1
14CMM
an expectation with respect to f(·|θ,z). The implication of (14) is that if E can be estimated it is possible to
(cid:98) f
optimizeθ inamethodofmomentsfashion. Thismeansthatθ(cid:98)issuchthatE f[d oc(π,z (cid:98))]isascloseaspossibleto
d¯=(cid:80)q
d (π ,z)/qtheempiricalaveragedistance. TheproofofEquation(14)fortheCMMcanbefoundinthe
j=1 oc j (cid:98)
supplementarymaterial. Usingthisresult,weproposeaniterativesearchthatestimatesE withMonteCarlo.
f
Algorithm(3)describesthisprocedureinasummarisationofCMMmaximumlikelihoodestimation. InSTEP1,z
(cid:98)
isfittedwithSA,andtheempiricald¯iscomputedfromthedata. Theestimationofθtakesaninitialguessθ0 anda
pre-specifiedtoleranceϵ>0.AuxiliarydataissampledinSTEP2usingthecurrentθtandAlgorithm(4.1).Thenumber
ofsamplesis⌈1/ϵ⌉,inverselyproportionaltothetolerance. InSTEP3,theexpectationin(14)isestimatedbythe
MonteCarloaveraged¯ = ((cid:80)⌈1/ϵ⌉d (π ,z))/⌈1/ϵ⌉,whereπ′ ≈ CMM(z,θt). STEP4adjustsθt accordingto
θt j=1 oc j (cid:98) j (cid:98)
therelativedifferencebetweend¯andd¯ ,settingθt+1 ←θt(d¯/d¯ ). Thealgorithmstopsoncetheabsolutedifference
θt θt
|θt+1−θt|isbelowϵ.
Algorithm3:optCMM(π,α,ϵ,N)OptimizationoftheCMMmodelparameterszandθwithobserveddataπ.
Input:π,N:Gibbssampleriterations,α:annealingschedule,CT,θ0:initialvalueofθ,ϵ>0:tolerance.
STEP1:Obtainz←Annealing(π,α,m);
(cid:98)
Computed¯=(cid:80)q
d (π ,z);
j=1 oc j (cid:98)
SetC =0;//Convergenceindicator
θt ←θ0;//Initialiseθ
whileC =0do
STEP2:π′ ←rCMM(z,θt,N,⌈1/ϵ⌉);//SampletheCMM(z,θt)usingAlgorithm(4.1)
(cid:98) (cid:98)
STEP2:Computed¯ θt =(cid:80) j⌈ =1/ 1ϵ⌉ do ⌈c 1(π /ϵj′ ⌉,z(cid:98)) ;//DistanceexpectationfromMonteCarlo
(cid:18) d¯ (cid:19)
STEP4:Setθt+1 ←θt θt ;
d¯
if|θt+1−θt|<ϵthen
C =1;
else
θt ←θt+1
end
end
Output:θ(cid:98)=θtandz (cid:98);
Algorithm optCMM(π,α,ϵ,N) outputs {z (cid:98),θ(cid:98)} at low computational cost and is exploited next for CT selection.
WeconcludewiththeremarkthatAlgorithm(3)alsoappliestofittingtheCMMinthefrequentistapproach. While
uncertaintyquantificationof{z (cid:98),θ(cid:98)}isnotreadily,itisfeasibletoobtainwithbootstrappingtechniques. Thisinvolves
optimizingre-sampledorreplicateddataπr forr =1,...,Rwiththesamealgorithm.
6.3 CTsearch
UsingestimatesfromAlgorithm(3)andaselectioncriterion,wecandesignanalgorithmicsearchtofindtheCTthat
bestfitstheobserveddata. OursuggestionistocarryoutagreedyexplorationoftheCTspace,whichaimstofind
theoptimalmodelbyproposingmodificationsofaninitialCTt. Agreedymethodfindsalocaloptimumbymoving
inthedirectionthatimprovesthecriterionateachstep,stoppingoncenoimprovementisachieved. Tothisend,this
subsectionaddressestwokeypoints: supplyingasensibleinitialvalueanddefiningaselectioncriterion.
To provide a sensible initial guess, we propose to find a starting CT that is guided by the preference proportions
matrixdescribedinsection1. Thisinvolvescomputingtheempiricalproportionsp¯(i,i′)=(cid:80)q I{π (i)≻π (i′)}/n
j=1 j j
for all i ̸= i′, i,i′ ∈ {1,...,n}. This matrix can be used to construct an initial CT by observing that pairs with
p¯(i,i′) close to 0.5 are near indifference. Hence, an iterative scan of p¯(i,i′) can be performed to group elements
that are close to 0.5 within a certain tolerance. For example, if this is tol = 0.05, pairs such that p¯(i,i′) is within
(0.45,0.55)arecandidatestobegrouped. Inourstrategy,ifthereismorethanone{i,i′}combinationthatsatisfies
this criterion, the one with the minimal absolute difference to 0.5 is chosen first. The search continues until there
are no |p¯(i,i′) − 0.5| < tol that have not been grouped. As this search evolves and items are placed together,
p¯(·,·) involving at least one group takes all possible element-wise combinations. For example, the starting CT
table for the sushi problem with tol = 0.05 takes four iterations. In the first step (top-left window), squid and
tuna roll are grouped, as this is the pair with p¯closest to 0.5 (p¯(squid,tuna roll) = 0.51). In step two, the
preferencesproportionmatrixisrecomputed,andthisisnowa(9×9)matrixwherep¯({squid,tuna roll},i′)is
15CMM
Figure6: Aggregationofthesushi’spairwisepreferencesmatrixbasedontheclosenessbetweenp¯(i,i′)toindifference.
Items(orclusters)aremergediterativelybycheckingiftheaverageempiricalpreferencesarewithintol=0.05to0.5.
Thealgorithmtakesfouriterationswhichareindicatedatthetopofeachplot.
1 ((cid:80)280 I{π−1(squid) < π−1(i′)}+I{π−1(tuna roll) < π−1(i′)} when comparing this group to some
2×280 j=1 j j
sushiotheri′.
AggregationofthesushispairwisepreferencesmatrixbasedonthedescribedprocedureisillustratedinFigure6. The
stepstakentoarriveattheinitialCT forthisdataareorganizedasrows,withtheircorrespondingiterationnumber
shownatthetop. Thefinalgroupsizesare(1,1,1,2,2,3),asindicatedonthebottom-rightwindow. Werecommend
(cid:112)
tol closeto 0.5/q asacanonicalchoiceofthisparameter. Thisformulationtakesintoaccountthatthestandard
(cid:112)
deviationofaproportionpestimatedfromqsamplesdependsonq. Ifpisnear0.5,wecansettol≈sd(p)= p(1/q).
(cid:98) (cid:98) (cid:98) (cid:98)
(cid:112)
Inthepreviousexample, 0.5/280=0.042.
OnceaninitialCTisobtained,modificationsofitareproposedandevaluatedintermsofaselectioncriterion. Denoted
byN(CTt),theseareneighboringtablesofthecurrentCTtthatcanbedesignedindifferentways. Forexample,a
oneadjacentshiftbuildsN(CTt)byshiftingoneobservationfromclusterlto(l−1)or(l+1). Allthosepossible
fromCTt =(1,3,3,3)areN(CTt)={(4,3,3),(2,3,3,3),(1,2,4,3),(1,4,2,3),(1,3,2,4),(1,3,4,2),
(1,3,3,2,1)}. Thenumberoftablescanbeupto2×Lsoanexhaustivesearchunderoneadjacentshiftsisnotfeasible
forlargeL. Inthiscase,apossiblemitigationistoexploresampledsubsetsofthepossibleN(CTt). Changingthe
typeofsearchmovesandsettingrestrictionsonthezspaceareotherpossiblestrategies. Whileadjacentshiftsearches
simultaneouslyLandn ,...,n ,onecouldfixLorsomen . Forexample,fixingn =1forlowandhighlexplores
1 L l l
thehypothesisoftop-bottomelicitation. OnceN(CTt)isdetermined,modelsareoptimizedwiththeroutineoptCMM()
andtheselectioncriterioniscomputed. Forcomputationalefficiency,visitedconfigurationsandtheircriterionvalues
canbestoredandrecycledacrossthesearch.
ThenaturalselectioncriteriontoemployintheCTsearchis(10). AnalogouslytoAICandBICmethods,thiscriterion
workswiththeoptimizedlog-likelihoodvalueandemploysapenalisationformodelcomplexity. However,tocompute
16CMM
(10),itisnecessarytoestimatetheCMMlikelihoodwithimportancesampling. Inaddition,theCTsearchrequires
doingsowithenoughprecisionsothatMonteCarlovariabilitydoesnotimpactthealgorithm’sdecisions. Evidently,
thevariabilitytowhichIˆ(CT)isestimatedcanbemadesmallerbyincreasingM butthecomputationalcostincreases.
Totacklethissituation,westipulateandtestanalternativecriterionthatispurelydata-based.
A data-based goodness of fit metric can be formulated on top of the data’s agreement to the proposed partitions.
OnceaCT isproposedandzˆ isfound, wecancomputetheproportionoftimesthatrankingsinπ agreewiththe
preference relations in zˆ. This is computed similarly to what is done in the initial value procedure but does not
involveapre-specifiedtolerance. Forexample,withtheorderedpartition{z}=({1,2},{3,4},{5})thereare3unique
clusters,making(cid:0)3(cid:1)
between-groupcomparisonsnecessary. Comparisonsaremadeacrossallbetween-clusterelement
2
combinations,checkinghowoftenthesmaller-labelobjectprecedesthehigher-labeloneinthedata. Intheexample,
clusters(1,2),(1,3)and(2,3)arecompared. Forthefirstpair,thisconsistsofevaluatingthebetween-setcombinations
of {1,2} and {3,4}. Hence, the contrast of clusters one and two are done with (cid:80)q I{π−1(1) < π−1(3)}+
j=1 j j
(cid:80)q I{π−1(2) < π−1(3)}+ (cid:80)q I{π−1(1) < π−1(4)}+ (cid:80)q I{π−1(2) < π−1(4)}. After this is computed
j=1 j j j=1 j j j=1 j j
forallbetween-groupcombinations,averagingacrossthetotalnumberofcomparisons,i.e.
q×(cid:16)(cid:0)n(cid:1)
−(cid:80)L
(cid:0)nl(cid:1)(cid:17)
,
2 l=1 2
renderstheproportionofcorrectpreferencerelations. SimilarlytowithI(cid:98)(CT),thegoalistomaximizethisproportion.
ThismetricisdenotedbyD(CT,π)andsatisfies0 < D(CT,π) < 1wherevaluesclosetooneareindicativeofa
betterfit.
Algorithm4:GreedysearchoftheCMM’sCTusingI(cid:98)(CT). Selectionwiththedata-basedcriterionD(CT,π)is
donesimilarlyasdoesnotrequiresamplinginSTEP2.
Input:CTt:initialCT,π:observeddata,α:annealingschedule,ϵ:optimizationtoleranceforθ,N:Gibbssampleriterations,
M:ISreplicas;
OptimizeCTt:{z (cid:98),θ(cid:98)}t ←optCMM(CTt,π,α,ϵ,N);
EstimateI(cid:98)(CTt)with{z (cid:98),θ(cid:98)}t;
L=(CTt,I(cid:98)(CTt);//Initialiselistofvisitedconfigurations
STEP1:Listneighboringtables:N(CTt)≡{CTt,...,CTt};//CTtmodifications
1 J
STEP2:forj ∈1:J do
if CTt ∈/ L//Tablenotvisitedthen
j
{z (cid:98),θ(cid:98)}t
j
←optCMM(CT jt,π,α,ϵ,N);
ComputeI(cid:98)(CT jt)andappendCT jt,I(cid:98)(CT jt)toL;
else
RecycleI(cid:98)(CT jt)fromL;//Tablevisited
end
end
STEP3:if max{I(cid:98)(CT 1t),...,I(cid:98)(CT Jt)}>I(cid:98)(CTt)then
UpdateCTttotheCT jtwithmaximumI(cid:98)(CT jt);
ReturntoSTEP1;
else
STOP;
end
Output:CTt;
Algorithm(4)summariseswithpseudo-codetheCMMCTsearch. ThealgorithmiswrittenintermsofIˆ(CT)but
worksequivalentlywithD(CT,π),thedata-basedcriterion. InSTEP1,modificationsofCTtareproposed,andthis
istheneighboringtablesset,N(CTt). STEP2usestheroutineoptCMM()tofindthebestz andθ foreachmodel
structureinN(CTt)andtheoptimizedmodelparametersareusedtocomputeIˆ(CT)(orD(CT,π)). Iftheproposed
CT hasalreadybeenvisited, STEP2isrecycledfrompreviousiterations. InSTEP3, thealgorithmmovesinthe
directionthatoptimizesthecriterion,stoppingifnoimprovementisachieved.
6.3.1 Simulationstudies
Inwhatfollows,weevaluatetheCTsearchwithsimulationstudies. Threesettingareexploredtoinvestigatedifferent
aspectsoftheproposedmethodology. Inthefirst,onehundredCMMdatasetsofq =500ranksaresimulatedusing
17CMM
Figure7: Summaryoftopfivemodelsfrom100searchesofn(z)=(1,1,3,1,1). Eachbarindicatestheproportionof
timesaconfigurationappearedasthefirsttofifthchoice.
Figure8: Totheleft,histogramsofI valuesfrom1KfitsoftheKendallandHammingCMMtoCMM-ksimulated
data. Estimatesofthespreadθ =0.4fromthecorrectly-specifiedmodelareshowntotheright. Thevalueofθusedto
generatethedataisindicatedbytheverticaldashedline.
theHammingorderedclusterdistance. TheCTandθ usedtogeneratethedataare(1,1,3,1,1)andθ = 0.9. This
configurationisdesignedtoinspecttheCTsearchwiththeoptimizationofI(cid:98)(·). Undersmalln=7,wesetM =100K
andrunAlgorithm(4)withone-adjacentshiftsinSTEP1. Thesearchisdoneforeachforeachreplicateddataπr,
r ∈1:100andtheCTswiththefivehighestI valuesarestored. TheresultsaresummarisedinFigure7wherethe
horizontalaxisshowsthemodelrank(firsttofifth)andthebarsarecoloredaccordingtotheproportionoftimesa
modelwasrankedinthatposition. Wecanseeinthefirst(fromlefttoright)barthattheCTusedtogeneratethedatais
selectedeverytimeasthebestcandidate. Inspectionofthefollowingranksisusefultoshowthatthenext-to-bestones
areincloseresemblancewith(1,1,3,1,1). Forinstance,choicesinthesecondandthirdtobestplacesdifferfromthe
trueCTbyoneadjacentshift.
A second study is concerned with the choice of distance. In this setting, we use the Kendall d to draw
oc
πr ∼ CMM(θ,z) with q = 200,n = 10 for r = 1,...,1K. The parameters used to simulate the data are
z =(1,2,2,3,3,3,3,4,4,5)andθ =0.4,soCT =(1,2,4,2,1). TheCMMisfittedtoeachπr usingtheHamming
andKendallorderedclusterdistancesunderthecorrectlyspecifiedCT.ComputingI(·),weobtain1Kestimatesofthe
criterionfromd andd modelfits. ThesearedisplayedwithhistogramsinFigure8,totheleft,showingthatthe
og,k og,h
criterionselectsthecorrectdistance,asdesired. ThevaluesofI obtainedwiththeKendallorderedclusterdistanceare
higherandsharenegligibleoverlapwiththosefromtheHammingfits.
We also stored {z (cid:98),θ(cid:98)} from this study to investigate the performance of Algorithm (3). Ideally, we would like the
correctly-specifiedmodeltoproduceθ(cid:98)estimatesclosetoθ =0.4whichwasusedtosimulatethedata. Figure8shows
thatthisisachieved,indicatingthetruevaluewithaverticaldashedline. Fittedvaluesofzarealsoexploredandthe
(cid:98)
correctconfigurationisrecoveredabout95%timeswitheitherd . Thecompletetableoffrequenciesisreportedinthe
oc
supplement. Theanalysisofzallowsustoconcludethatitsestimationisrobustwithrespecttothedistance. Inother
(cid:98)
words,clusterallocationsarewellestimatedundermisspecificationofd whichisadesirablefeature.
oc
18CMM
Figure9: Totheleft,frequencyofCTsselectedwithAlgorithm(4)asbest(toppanel),second-to-best(middlepanel)
andthird-to-best(bottompanel). Withineachwindow,barsshowtheaggregationoffrequenciesforCTswithinzero,
one,twoorthreeormoresingle-adjacent-shiftsfromthetrueCT.Ontheright,adiagramexemplifiesdifferenttables
thatareallwithinonesingle-adjacent-shiftfrom(3,3,3,3,3). Categoriesshowninbluecomposethesecondbarinthe
topwindowplot,wheretheyareindicatedindifferentcolors.
OurfinalsimulationstudyillustratestheCTsearchunderamoderatesetofn=15itemsandD(·,π).Inthisexperiment,
the CT used to simulate the data is CT = (3,3,3,3,3) and θ = 1.5. One hundred replications are considered as
before,andwesummarisetheresultsinFigure9. Totheleft,threepanelsareincludedtoillustratethefrequencyof
theCTschosenasbest(firstrow),second-to-best(secondrow)andthird-to-best(lastrow). Ineachpanel,thefour
barsaggregatethefrequencyofCTsthatarewithinzero,one,twoorthreeormoreadjacencyshiftsfromthetruetable.
Withineachbar,colorsaredisplayedtoindicatepossibleCTsinthatdistancefrom(3,3,3,3). Naturally,onlythecorrect
tableiswithinzeroshiftsfromthetrueCT,sothefirstbaristhefrequencyofexactlyrightmatches. Alargefrequency
ofone-shiftresultsisseeninthethreeplotswhichmayatfirstseemtosurpassthecorrectCTfrequency. However,we
pointoutthatthisiscomposedbyalargenumberoftablessuchastheonesonthediagramtotheright. Itshowsthe
CTsthatareselected(rankedfirst)bythesearchandcanbeobtainedfromthetrueCTwithasingleadjacentshift. With
this,webringattentiontothefactthatCTswithinverycloseresemblancetothegeneratingoneareoftenselected. We
canconcludethatalthoughthesearchbasedonI(cid:98)(·)providesmoreaccurateresults,D(·,π)-basedselectionprovidesa
closeapproximation. WhenthecostofestimatingI(cid:98)(·)withsmallvariabilityisprohibitive,thedata-basedselectionisa
suitablealternative. ThetwomethodscanalsobeemployedinconjunctionbyusingD(·,π)tonarrowthesearchand
thencomputeI(cid:98)(·)forasmallersubset.
19CMM
7 Dataapplications
7.1 Formula1dataanalysis
Inthissection,weanalysethefinalrankofdriversinracesoftheFormula1driverschampionship. Thisdataisavailable
onlinefromavarietyofsources,andwefocusontheraceswhichtookplacein2015and2016. Thisasetofq =40
raceswhere28differentdriversparticipatedatleastonce. Eachcircuitstartswith20carssorankingsofupto20
competitorsarerecorded. Therecanbeasmallernumberoffinishersifdriversfailtofinishtherace. Wechooseto
filteroutcompetitorsthatparticipated10timesorless,resultinginanitemsetofn = 25foranalysis. Thedatais
thenviewedasanincompleterankinginP whereracesj = 1,...,40arecodedbyπ . Thisisapartialranking
25 (cid:101)j
observationofthen=25driversset.
Our goal is to analyse π ≡ (π ,...,π ) with the CMM and a CT guided by the Formula 1 point system. The
(cid:101) (cid:101)1 (cid:101)40
competitionawardsdriverswithscoresaccordingtotheirrankineachrace. Thechampionshipwinneristhenthedriver
withthemostpointsattheendoftheseason. Therearesomesubtlevariations,butthebasicscoringruleisthatpoints
aregiventothetop-10finishersas(25,18,15,12,10,8,6,4,2,1),forthefirsttotenth,respectively.
Basedonthis,wesettheCMMCTto(1,1,1,2,3,2,15). Podiumpositionsthatareworth25,18,and15areeachtheir
owncluster,i.e.,n =n =n =1. Thisisfollowedbygroupingthefourthandfifthplacesawarded10and12points
1 2 3
son =2. Scores{4,6,8},{1,2}and{0}arethelastclusters,makingn =3,n =2andn =15. Theideabehind
4 4 5 6
thispartitionchoiceistoallowinterchangeabilitybetweendriverswithintheabovescoringranges. Forexample,itis
reasonabletoassumethattherearecompetitorswhomostoftendonotscore(cluster6).
TheunclusteredMMisalsofittedtoπ,wheredriversarestrictlybetterorworsethanoneanotherintheconsensusπ .
0
BothmodelsarefittedundertheBayesianapproachandsamplesfromtheposteriordistributionsof(θ,z)and(π ,α)
0
arecollected. TheMallowsmodelisfittedwiththeRpackageBayesMallows(Sørensenetal.,2020)thatimplements
anequivalentaugmentationstrategy.
Posteriormodesofzandπ arecomputedafterrunningeachalgorithmfor11Kiterationsanddiscardingthefirst1Kas
0
burn-in. Thetop10driversinzandπ aresummarisedinTable2. TheCMMandMMshowagreementbetweenthe
0
modelfitsandtheirdifferenceisthatsomestrictorderingisestimatedinthelatterfortheclustereddrivers.
Points CMM MM
25 {Hamilton} Hamilton
18 {Rosberg} Rosberg
15 {Vettel} Vettel
{12,10} {Raikkonen,Ricciardo} Raikkonen≻Ricciardo
{8,6,4} {Bottas,Massa,Verstappen} Bottas≻Massa≻Verstappen
{2,1} {Grosjean,Perez} Perez≻Grosjean
Table2: PosteriormodeofCMMorderedclusters{z}andMM’scentralpermutationπ .
0
Toselectbetweenmodels,theselectioncriterion(10)iscomputedperMCMCiterationwiththecurrentcompletedata
andmodelparameters. Itsmaximumis−1901.93fortheCMMfitand−2388.607intheMM.ThevalueofI ishigher
fortheCMM,indicatingthattheclusteredmodeloptimizesthepenalisedobserveddataprobability. Thisclaimis
supportedbyFigure10whichdisplaysthedistributionofranksbydriver(MM)ordriversubgroup(CMM).Itisevident
thataggregationsuggesteddecreasessubstantiallytheuncertaintyregardinghowcompetitorsareordered.
7.2 Tohokusushirankings
ThecompletemethodologydevelopedthroughoutthepaperisnowusedtoanalysetheTohokusushirankingsdata
introducedinsection(1). Thisisthecollectionofq =280rankingsofn=10sushivarietiesprovidedbyparticipants
ofthesurveyKamishima(2003)wholivedinTohokuuntil15yearsold. Wepursueanswerstothefollowingquestions.
Is there a strict consensus between all sushi varieties in Tohoku? and, if not, in what part of the ranking is there
indifference,andbetweenwhichsushis?
First,Algorithm(4)isusedtosearchforaCTapplyingoneadjacentshifts. Thesearchisdonewithd ,d and
og,h og,k
I(·)orD(·,π). AllcombinationsofdistanceandcriteriataketheinitialCT valueobtainedviatheprocedureofsection
6.3. TheoptimumCTsareshowninTable3.
20CMM
Figure10:DistributionofclusteredandunclusteredranksofdriversinFormula1seasons2015and2016,corresponding
totheCMMandMM,respectively.
d
go
I(cid:98)(·) D(·,π)
Kendall (1,1,1,2,2,3) (1,2,2,5)
Hamming (1,2,2,2,3) (1,2,3,4)
Table 3: CMM structure selection for the Tohoku sushi rankings carried out with criteria I(cid:98)(·) and D(·,π). The
likelihood-basedinformationcriterionisestimatedwithM =100K.
Asindicatedbythesimulationstudiesinsection6.3,I(cid:98)(·)moreoftenidentifiesthetruedata-generatingpartition. Hence,
itshouldbepreferredwhenitispossibletoobtainitwithadequateprecision. Weevaluateitsdecisivenessbyestimating
theuncertaintyofI(cid:98)(·)underM =100K viaMonteCarloreplication. Thecriterionisrecomputed500timesforthe
modelsinTable3plusthesecond-to-bestonesaccordingtoI(cid:98)(·). MonteCarloreplicationofeachoftheseCTswith
M =100K yieldstheresultsusedtoproduceFigure11. Thisplotindicatesthatwecandecisivelyselectthemodel
Kendall(I)≡(1,1,1,2,2,3). ThisCT optimizesthelikelihood-basedcriterionandhassmallenoughvariabilityto
makethedecisionunambiguous.
Giventhatamodelstructureisselected,wefititsparametersusingtheBayesianapproachandthemethodologyof
section5. Wetacklethecomparisonbetweentheclusteredandunclusteredmodelsbyfittingthelatter(MM)withthe
BayesMallowspackage.TheCMMfittakestheproposalsswitch(·,2)forz,andlog-Normal(logθt,σ)withvanishing
calibrationofσforθ. Apriori,θ ∼Gamma(s=2,r =2)andN issetto200forapproximateexchangealgorithm
moves. Themaximumaposteriori(MAP)parametersoftheclusteredandunclusteredallocationsarecomputedand
displayedinTable4. Theydifferinthatthesushis{sea urchin, salmon roe},{sea eel, tuna roll, squid}
and{egg, cucumber roll}areindifferentwithinranks{4,5},{6,7,8},{9,10}accordingtotheCMM,andfollow
theorderingofcolumn3intheMM.
ForcomparisonwiththeMM,theinformationcriterion(10)isrecomputedfortheCMMwiththeselectedstructureusing
theMAPparameterestimates. OnehundredreplicationswithM =100K giveanaverageI(cid:98)(CT)of−3968.38with
standarddeviation3.74. ThiscanbecomputedexactlyfortheKendall-MMusingtheclosedformofitsnormalisation
constant. Itsvalueis−4341.95whichindicatesthattheCMMispreferable. Thiscanbeaffirmeddecisivelyoncethe
differencebetweentheMMandtheCMMI valuesismuchlargerthantheCMM’sIˆstandarddeviation. Moreover,
thedecisionisthesameifweconsidertheunpenalisedlog-likelihood,allowingustoconcludethatatiedconsensus
optimizestheprobabilityoftheobserveddata.
ThealternativepartitionsarealsocomparedinTable5usingthedata-basedmetricD(·,π)bygroup. Itdisplaysthe
proportionoftimesthatobjectswithlowerclusterlabels(orlowerpositioninπ )proceedthosewithhigherlabelsin
0
21CMM
Figure11: DistributionofI(cid:98)(·)undercandidateCTsestimatedviaMonteCarlo. Thefirstandsecond-to-bestmodels
fromtheCT searchthatusesI(cid:98)(·)isconsidered(1. Kendall(I),2. Kendall(I),1. Hamming(I),2. Hamming(I))
alongsidethoseobtainedbysearchingviathedata-basedcriterion(Kendall(D),Hamming(D)).
Ranks CMM MM
1 fatty tuna fatty tuna
2 tuna tuna
3 shrimp shrimp
4,5 sea urchin∼salmon roe sea urchin≻salmon roe
6,7,8 sea eel∼tuna roll∼squid sea eel≻tuna roll≻squid
9,10 egg∼cucumber roll egg≻cucumber roll
Table4: MaximumaposterioriestimatesoftheCMMzandMMπ fittedtoTohokusushirankings. IntheCMM,z
0
encodesanorderedpartitionwhereindifferentitemsaregroupedinrows4,5and6. Theirstrictorderinπ isshownin
0
thethirdcolumn.
thedata. Naturally,thethreefirstcolumnsareidenticalforthetwomodels,sincethefirstthreeCMMclustersareof
singleobjects. Subsequently,thereisanincreaseintheproportionofcorrectrelationsintheCMM,withanaverageof
0.68for(4,5+),(5,6)whichbecomes0.65intheMMconsidering(4,5+),...,(9,10).
TheBayesianmodelfitestimatestheposteriordistributionofθ,aparameterthatquantifiesthespreadaroundsome
˙
allocationz. OneusefulwaytolookatthisiswiththeMAPzandrank-clusterprobabilities. AsshowninEquation
(cid:98)
(3), the prevalence of clusters in rankings is a CMM expectation. Using percentage points of the θ posterior, we
canconstructconfidencebandsforRC(i,l)andshowuncertaintyaroundit. The2.5%and97.5%quantilesofthe
distributionareQ (2.5%) = 0.244andQ (97.5%) = 0.291, soP(0.244 < θ < 0.291) = 0.95. Wecanestimate
θ θ
95%bandsforRC(i,l)withtheexpectedvaluesE f(·|z(cid:98)˙,Qθ(2.5%))[I{(cid:98)z˙(π(i))=l}],E f(·|z(cid:98)˙,Qθ(97.5%))[I{(cid:98)z˙(π(i))=l}]
estimatedwithMonteCarlo. Resultsobtainedwith100Ksimulatedrealisationsandsomei,lcombinationsarethe
following. Theprobabilitythatfatty tuna(l = 1)isthetopitem(i = 1)isbetween0.239and0.273. Foregg,
cucumber roll,thisis(0.055,0.047)soeitheroftheseitemshasaverysmallprobabilityofbeingthefavorite. Any
pairi,lcanbeusedinthisanalysistoobtainreadilyuncertaintyquantificationofgroupprobabilitieswithinranks.
(1,2+) (2,3+) (3,4+) (4,5+) (5,6)
CMM
0.74 0.67 0.68 0.64 0.73
(1,2+) (2,3+) (3,4+) (4,5+) (5,6+) (6,7+) (7,8+) (8,9+) (9,10)
MM
0.74 0.67 0.68 0.61 0.65 0.65 0.65 0.72 0.63
Table5: Proportionoftimesthatobjectswithlowerclusterlabels(orlowerpositioninπ )proceedthosewithhigher
0
labelsinthesushipreferencesdata. ThisiscomputedbetweenCMMclustersinthefirstrow,betweenobjectsinthe
secondrowfortheMM,andaveragedbythenumberofcomparisons.
22CMM
8 Conclusions
Inthispaper,theClusteredMallowsModel(CMM)wasintroducedasanextensionofthewell-knownMallowsModel
(MM).TheCMMaddressesthechallengeofmodellingrankcollectionsthatdonotconveystrictpreferencesamong
allitemsofinterest. Todothis,theCMMincorporatesapopulationconsensusparameterthatcanaccommodateties.
Unlike the MM, which utilizes a modal permutation to fit π, the CMM fits ordered clusters. Ordered clusters are
achievedwiththeintroductionofanitemallocationvector,whereweimposethatwithin-groupitemsareindifferent
andthatclusterlabelsholdanorderinginterpretation. TheCMMismotivatedbyarealdataexampleofsushirankings,
wheretherelativepreferencebetweencertainvariantsintheitemsetisunclear. ByintroducingtheCMM,weprovidea
morecomprehensivemodelingapproachforrankcollectionsthatencompassambiguouspreferences.
TheCMMprobabilitywasdefinedintermsofadistancebetweenorderedclusters,andaclusteringtable(CT).To
determinesuitabledistanceoptions,wereferredtotheextendedmetricsbyCritchlow(1985). Thesewereformulated
asCMMmetricsandthemodeltakingdifferentdistanceswereexaminedandcompared. Theresultingmodelhas
anon-analyticalnormalisationconstant, whichplacestheCMMposteriormodelintheclassofdoubly-intractable
distributions.Accordingly,specialattentionwasdevotedtotheproblemsofmodelselectionandposteriorsampling.The
firstwashandledwithanimportancesamplingestimatoroftheintractableterm,whichallowedforamodelselection
criteriontobeevaluated. PosteriorsamplingwasfullydevelopedwithMetropolis-HastingsandApproximateExchange
Algorithm(AEA,CaimoandFriel(2011))movesthatdrawfromtheparameters’full-conditionaldistributions. With
theapplicationoftheAEA,wewereabletoovercometheunavailabilityofCMMthenormalisationtermandfitthe
model’sposteriordistributionforπ.
Bayesianinferenceinthepresenceofincompleterankswasalsoaddressedusingdataaugmentationtechniques. The
CMM applicability to incomplete ranks was illustrated in an example that concerns results races in the Formula 1
competition. Wewereabletodemonstratethatgroupingdriversaccordingtosomescoringrangesallowedforamore
robustrepresentationofthedata,whichispreferredovertheunclustered(MM)fit. Wealsooutlinedagreedysearchof
theCMMstructurethatreliesonoptimizationofthepenalisedlog-likelihoodoradata-basedgoodnessoffitmetric.
A complete analysis of the motivating data was presented and the process of learning the CMM CT and model
parameterswasdemonstratedinpractice. TheCMMandMMmodelfitswerecomparedandwewereabletoshowthat
theaggregationofsomesushisinmiddleandlower-rankscanbetterdescribethedata-generatingprocess.
Finally,somepointsoffutureresearchthatdeserveattentionarethefollowing. ACMMextensiontothesimultaneous
clusteringofassessorsanditemscanbedevelopedusingasimilarapproachtoVitellietal.(2018). Thiswouldconsist
ofmodellingclustersofrankerswithspecificCMMdistributions. Inturn,thiswouldallowforfittingnon-homogeneous
populationswithcluster(ofassessors)-specificitempartitions. Anotherpointforfutureinvestigationismodelselection,
wheresophisticatedCTexplorationscouldbestipulatedwiththeTABUorgeneticmetaheuristics.
Acknowledgments
ThispublicationhasemanatedfromresearchconductedwiththefinancialsupportofScienceFoundationIrelandunder
Grantnumber18/CRT/6049. TheInsightCentreforDataAnalyticsissupportedbyScienceFoundationIrelandunder
GrantNumber12/RC/2289_P2. Theauthorsreporttherearenocompetingintereststodeclare.
23CMM
References
Alquier,P.,Friel,N.,Everitt,R.,andBoland,A.(2016). NoisyMonteCarlo: ConvergenceofMarkovchainswith
approximatetransitionkernels. StatisticsandComputing,26:29–47.
Andrieu,C.andRoberts,G.(2009). Thepseudo-marginalapproachforefficientMonteCarlocomputations. Annalsof
Statistics,37:697–725.
Barry,D.andHartigan,J.A.(1992). Productpartitionmodelsforchangepointproblems. TheAnnalsofStatistics,
20:260–279.
Biernacki, C., Celeux, G., and Govaert, G. (2000). Assessing a mixture model for clustering with the integrated
classificationlikelihood. IEEETransactionsonPatternAnalysisandMachineIntelligence,22:719–725.
Brun,A.,Hamad,A.,Buffet,O.,andBoyer,A.(2010). From"ilike"to"iprefer"incollaborativefiltering. In2010
22ndIEEEInternationalConferenceonToolswithArtificialIntelligence,volume2,pages365–367.
Brun,A.,Hamad,A.,Buffet,O.,andBoyer,A.(2011). Towardspreferencerelationsinrecommendersystems. In
Workshop on Preference Learning, European Conference on Machine Learning and Principle and Practice of
KnowledgeDiscoveryinDatabases(ECML-PKDD2010),volume51.
Burges,C.,Ragno,R.,andLe,Q.(2006). Learningtorankwithnonsmoothcostfunctions. InSchölkopf,B.,Platt,J.,
andHoffman,T.,editors,AdvancesinNeuralInformationProcessingSystems,volume19.MITPress.
Burges, C. J. (2010). From ranknet to lambdarank to lambdamart: An overview. In Technical Report, Microsoft
Research.
Caimo,A.andFriel,N.(2011). Bayesianinferenceforexponentialrandomgraphmodels. SocialNetworks,33:41–55.
Chung,L.andMarden,J.I.(1991). Useofnonnullmodelsforrankstatisticsinbivariate,two-sample,andanalysisof
varianceproblems. JournaloftheAmericanStatisticalAssociation,86:188–200.
Crispino,M.,Arjas,E.,Vitelli,V.,Barrett,N.,andFrigessi,A.(2017). ABayesianMallowsapproachtonon-transitive
paircomparisondata: Howhumanaresounds? TheAnnalsofAppliedStatistics,13:492–519.
Critchlow,D.(1985). MetricMethodsforAnalyzingPartiallyRankedData. SpringerNewYork,NY.
Critchlow,D.E.,Fligner,M.A.,andVerducci,J.S.(1991). Probabilitymodelsonrankings. JournalofMathematical
Psychology,35:294–318.
Deng, K., Han, S., Li, K.J., andLiu, J.S.(2014). Bayesianaggregationoforder-basedrankdata. Journalofthe
AmericanStatisticalAssociation,109:1023–1039.
Diaconis, P. (2009). The Markov Chain Monte Carlo revolution. Bulletin of the American Mathematical Society,
46:179–205.
Doignon, J. (2023). The best-worst-choice polytope on four alternatives. Journal of Mathematical Psychology,
114:102769.
Fligner,M.andVerducci,J.(1986). Distancebasedrankingmodels. JournaloftheRoyalStatisticalSociety: SeriesB,
48:359–369.
Gormley, I. and Murphy, T. (2006). Analysis of irish third-level college applications data. Journal of the Royal
StatisticalSociety: SeriesA,169:361–379.
Hartigan,J.(1990). Partitionmodels. CommunicationsinStatistics-TheoryandMethods,19:2745–2756.
Irurozki,E.,Calvo,B.,andLozano,J.(2018). SamplingandlearningMallowsandgeneralizedMallowsmodelsunder
theCayleydistance. MethodologyandComputinginAppliedProbability,20:1–35.
Irurozki,E.,Calvo,B.,andLozano,J.A.(2019). MallowsandgeneralizedMallowsmodelformatchings. Bernoulli,
25:1160–1188.
Kamishima,T.(2003). Nantonaccollaborativefiltering: Recommendationbasedonorderresponses. InProceedingsof
theACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining.
Koski,T.andNoble,J.(2011). BayesianNetworks: AnIntroduction. JohnWiley&Sons.
Larrañaga,P.,Karshenas,H.,Bielza,C.,andSantana,R.(2013). AreviewonevolutionaryalgorithmsinBayesian
networklearningandinferencetasks. InformationSciences,233:109–125.
Li,H.,Xu,M.,Liu,J.S.,andFan,X.(2020). AnextendedMallowsmodelforrankeddataaggregation. Journalofthe
AmericanStatisticalAssociation,115:730–746.
Lu,T.andBoutilier,C.(2014). EffectivesamplingandlearningforMallowsmodelswithpairwise-preferencedata.
JournalofMachineLearningResearch,15:3783–3829.
24CMM
Mallows,C.L.(1957). Non-nullrankingmodels.I. Biometrika,44:114–130.
Marden,J.(1995). AnalyzingandModelingRankData. ChapmanandHall/CRC,1stedition.
Marley,A.andPihlens,D.(2012). Modelsofbest–worstchoiceandrankingamongmultiattributeoptions(profiles).
JournalofMathematicalPsychology,56:24–34.
Marley,A.A.J.,Islam,T.,andHawkins,G.E.(2016). Aformalandempiricalcomparisonoftwoscoremeasuresfor
best–worstscaling. JournalofChoiceModelling,21:15–24.
Meila˘,M.andBao,L.(2008). Estimationandclusteringwithinfiniterankings. InProceedingsoftheTwenty-Fourth
ConferenceonUncertaintyinArtificialIntelligence,Arlington,Virginia,USA.
Meila˘,M.andBao,L.(2010). Anexponentialmodelforinfiniterankings. JournalofMachineLearningResearch,
11:3481–3518.
Murray,I.,Ghahramani,Z.,andMacKay,D.(2006). MCMCfordoubly-intractabledistributions. InProceedingsofthe
22ndAnnualConferenceonUncertaintyinArtificialIntelligence(UAI-06).
Plackett,R.L.(1975). Theanalysisofpermutations. JournaloftheRoyalStatisticalSociety: SeriesC,24:193–202.
Quintana,F.A.,Loschi,R.H.,andPage,G.L.(2018). BayesianProductPartitionModels. JohnWiley&Sons,Ltd.
Raftery,A.E.andDean,N.(2006). Variableselectionformodel-basedclustering. JournaloftheAmericanStatistical
Association,101:168–178.
Scutari,M.andDenis,J.-B.(2021). BayesianNetworks: WithExamplesinR. CRCpress.
Sørensen,O.,Crispino,M.,Liu,Q.,andVitelli,V.(2020). BayesMallows: AnRpackagefortheBayesianMallows
model. TheRJournal,12:324–342.
Varin,C.,Reid,N.,andFirth,D.(2011). Anoverviewofcompositelikelihoodmethods. StatisticaSinica,pages5–42.
Vitelli,V.,Sørensen,O.,Crispino,M.,Frigessi,A.,andArjas,E.(2018). Probabilisticpreferencelearningwiththe
Mallowsrankmodel. JournalofMachineLearningResearch,18:1–49.
Zhu,W.,Jiang,Y.,Liu,J.S.,andDeng,K.(2023). Partition–Mallowsmodelanditsinferenceforrankaggregation.
JournaloftheAmericanStatisticalAssociation,118:343–359.
25