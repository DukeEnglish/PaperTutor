Embodied LLM Agents Learn to
Cooperate in Organized Teams
XudongGuo1 KaixuanHuang2 JialeLiu3 WenhuiFan1 NataliaVélez2
QingyunWu3 HuazhengWang4 ThomasL.Griffiths2 MengdiWang2
1TsinghuaUniversity 2PrincetonUniversity
3PennStateUniversity 4OregonStateUniversity
Abstract
Large Language Models (LLMs) have emerged as integral tools for reasoning,
planning,anddecision-making,drawingupontheirextensiveworldknowledge
andproficiencyinlanguage-relatedtasks. LLMsthusholdtremendouspotential
fornaturallanguageinteractionwithinmulti-agentsystemstofostercooperation.
However,LLMagentstendtoover-reportandcomplywithanyinstruction,which
mayresultininformationredundancyandconfusioninmulti-agentcooperation.
Inspiredbyhumanorganizations,thispaperintroducesaframeworkthatimposes
prompt-basedorganizationstructuresonLLMagentstomitigatetheseproblems.
Through a series of experiments with embodied LLM agents and human-agent
collaboration,ourresultshighlighttheimpactofdesignatedleadershiponteam
efficiency, shedding light on the leadership qualities displayed by LLM agents
andtheirspontaneouscooperativebehaviors. Further,weharnessthepotentialof
LLMstoproposeenhancedorganizationalprompts,viaaCriticize-Reflectprocess,
resulting in novel organization structures that reduce communication costs and
enhanceteamefficiency.
1 Introduction
Modern intelligent systems, such as autonomous vehicle networks and swarms of drones, often
involvecomplexdecision-makingprocesseswheremultipleagentsorcomponentsmustcollaborate
seamlessly to achieve specific objectives Wang et al. (2020); Vinyals et al. (2019); Zhang et al.
(2019a);Wangetal.(2021). Inthesesystems,communicationamongthevariousagentsispivotal,as
itdictatestheflowofinformation,coordinationoftasks,andoverallsystemperformanceZhangetal.
(2019b);Guoetal.(2023);Foersteretal.(2016);Dasetal.(2019). Agentsintraditionalmulti-agent
systemsoftenhavetocommunicateinpre-specifiedways,suchasexchanginggradients,sharingdata,
stateobservationsandactions,etcKimetal.(2020);Linetal.(2021);Foersteretal.(2016). The
emergenceoflargelanguagemodels(LLMs)makesitpossibleforAIagentstocommunicateand
cooperateusingnaturallanguage,bringingenormousflexibilityandpotentialformorenuancedand
human-understandableinteractionsParketal.(2023);Hongetal.(2023);Mandietal.(2023);Chen
etal.(2023a).
Despite the flexibility of LLMs, integrating them into practical multi-agent systems remains a
challenge. WhileLLMsaretrainedandfinetunedfortextgenerationandinstruction-following,they
arenotnecessarilytailoredtomulti-agentcooperation. ModernLLMsarepronetoover-reporting
andobeyinginstructions,asaby-productofRLHFfinetuning(Baietal.,2022),andtheycanignore
criticalinformation(Liuetal.,2023a)orbedistractedbyirrelevantinformation(Shietal.,2023),
especially when the context is long (see Figure 1 for examples). While recent studies involving
agent-basedLLMshavedemonstratedtheyarecapableof solvingproblemsthroughmulti-agent
collaborationLietal.(2023b);Zhangetal.(2023c);Mandietal.(2023),itisworthnotingthatsuch
Preprint.Underreview.
4202
raM
91
]IA.sc[
1v28421.3042:viXraFigure 1: Example of disorganized communication and interruption, without a designated
leader. InateamofthreeGPT-4agents,twoagentsengagedinunnecessarycommunicationand
madedisordereddecisions,causingadelayduetothelackofapredefinedorganization. Weidentified
manymoreexamplesincludingconflictingmessagesandrepetitivecommunications,seeAppendixD.
collaborationsoftenfollowpredefinedpatternsdesignedusingheuristicstochannelthebehaviorof
themodelsproductivelyLietal.(2023b). Creatingsystemsthatsupportfree-flowinginteraction
betweenLLMsinawaythatcouldpotentiallyscaletoincludehumancollaboratorsisstillanopen
problem.
ThispaperinvestigatesthecollaborativepotentialofLLMagentsworkinginteams. Drawingon
priorstudiesinhumancollaborationfromcognitiveandeconomicperspectives,thereispotential
for organizations to be redesigned to more effectively manage the limited attention span within
teams,assuggestedbySimonetal.(1971),andmitigateindividuallimitationsandenhanceoverall
teamperformance, ashighlightedbyVanZandt(1999)andVélez etal.(2023). Specifically, we
studytworesearchquestions. First,whatroledoorganizationalstructuresplayinmulti-LLM-agent
systems? Second,howcanweoptimizetheseorganizationalstructurestosupportefficientmulti-
agentcoordination? ByleveragingAutoGenWuetal.(2023),agenericmulti-agentconversation
framework,wedevelopaframeworkforstudyinghowtobestorganizeembodiedLLMagentsto
communicateandcollaborateinphysical/simulatednon-textenvironmentsZhangetal.(2023c). Our
frameworkofferstheflexibilitytopromptandorganizeLLMagentsintovariousteamstructures,
facilitatingversatileinter-agentcommunication. Italsoservesasatestbedtoempiricallyevaluatethe
traditionalideasproposedintheorganizationtheoryliterature.
OurinitialexperimentsinthissettingrevealthatuncoordinatedLLMagentsoftensendredundant
andrepetitivemessagesandinterruptothers’actions,leadingtochaos(seeFig.1andAppendixD).
To remedy these issues, we explore organizational structures that allow multiple LLM agents to
collaborateandcompleteacommontaskefficiently.
Thefirstorganizationalstructureweexploreisahierarchy,aclassicobjectofstudyinorganizational
theoryMarchandSimon(1958);Radner(1993);Chisholm(1992);BoltonandDewatripont(1994);
Garicano(2000);Doddsetal.(2003). Withadesignatedleader,LLMagentsworkmoreefficiently
andcollaboratively. Fortheexampleofathree-agentteam,imposingaleaderimprovesefficiency
by up to 30% with almost no extra communication cost (up to 3%), consistent with findings for
humanorganizationsDoddsetal.(2003). Thisalsoholdstrueinfive-agentcases. Further,LLM
agentsdemonstratedthepotentialtoelecttheirownleaderandadjustleadershipdynamicallyvia
communication. Withproperorganizations,LLMagentsexhibitavarietyofcooperativebehaviors
thatmimichumans. Forexample,agentscanprovideconstructivesuggestionsandseekhelpfrom
others;theycanalsoexecuteappropriateinteractionsforahierarchysuchasreportingbackontask
progress;seeFigures6and7andAppendixC.Wealsotestedhuman-agentcollaboration,andobserve
that,unsurprisingly,humanleadersaremuchbetteratcoordinatingateamofagentswhencompared
toAIagents.
In addition to testing existing organizational structures, we explore the use of LLMs to improve
theorganizationprompt. Tothisend, wedevelopaCriticize-Reflectframework, adoptingadual
LLMarchitecture,toreflectontheteamperformanceandgenerateimprovedandnovelorganization
2prompts. Throughthisiterativeprocess,ourLLMagentsspontaneouslyformnovel,effectiveteam
structures,leadingtoreducedcommunicationcostsandimprovedefficiency;seeFigures8and9.
2 RelatedWorks
2.1 LLMAgents
AspowerfulLLMsinheritabundantworldknowledgeandalsogeneralreasoningability,thereare
increasingeffortstodeployLLMsasthereasoningcorefordecision-makingtobuildhuman-like
autonomousagents(Sunetal.,2023;Zhuetal.,2023;Haoetal.,2023a). Thisrequiresobservations
oftheRLenvironmenttobetranslatedintonaturallanguageinawaythatiseasierforlanguage
modelstoprocess. Thereasoningofthelanguagemodelsalsoneedstobeturnedintoaviableaction
for execution. Popular prompting techniques for doing so include ReAct (Yao et al., 2022) and
Reflexion(Shinnetal.,2023). Othermethodsthatinvolvefine-tuningthelanguagemodelshavealso
beenexplored(Haoetal.,2023b). Inaddition,varioustechniqueshavebeenproposedtomitigatethe
biasesandconstraintsoflanguagemodels,includingchain-of-thoughtreasoning(Weietal.,2022),
externaltools(Shenetal.,2023;Patiletal.,2023),externaldocuments(Wangetal.,2023)andskill
libraries(Zhuetal.,2023).
2.2 Multi-AgentCooperation
Multi-agent cooperation has been extensively studied for decades under various topics such as
communicationefficiency,planning,leadership,andteamdynamicsusingdifferentplatforms(Lowe
etal.,2017;Samvelyanetal.,2019;Resnicketal.,2018;Puigetal.,2021)(seerecentsurveysfor
detail(OroojlooyandHajinezhad,2023;Zhangetal.,2021;GronauerandDiepold,2022)). Previous
worksmainlyfocusedoncommunicationthroughcontinuousvectors(Dasetal.,2019)ordiscrete
symbols(Loweetal.,2017;Jaquesetal.,2019).
Recentwork(Xuetal.,2023;Zhangetal.,2023d;Wuetal.,2023;Lietal.,2023a)showedthat
multipleLLMagentsorhuman-agentteamscanimproveuponsingleLLMinsolvingpuretext-based
tasks, such as creative writing, reasoning, and code generation. Liu et al. (2023b), Hong et al.
(2023)andZhengetal.(2023)furtherexploredagentselectionorroleassignmenttoimprovethe
performance.
LLMshavealsobeenappliedtomulti-agentcooperationforembodiedtasks(Agasheetal.,2023;
Mandietal.,2023;Parketal.,2023;Chenetal.,2023a). Besides,Zhangetal.(2023b)proposed
anintentioninferenceframeworktoenhancethecooperationofLLMagentswithoutexplicitcom-
munication. Lietal.(2023b)investigatedLLM-agentscollaborationforTheoryofMindinferences
taskswithabroadcast-onlycommunicationprotocolandhomogeneouspolicies. Zhangetal.(2023c)
studiedembodiedmulti-agentcooperationinthetwo-agentandtheone-human-one-agentsettings.
Chenetal.(2023b)exploreddifferentfixedcommunicationstructuresformulti-LLM-robots. These
initialexplorationsarealsolimitedtofixedteamstructuresandarenotoptimizedforcommunication
efficiency. Incontrast, ourworkexplorestheimpactofdeployingandoptimizingorganizational
structures,allowing≥3agentsinateam,forefficientmulti-agentcommunicationandcooperation.
2.3 PromptOptimization
Languagemodelsaresensitivetoprompts. Theformatofthepromptcanhaveasubstantialinfluence
onperformance(Gaoetal.,2020;Weietal.,2022;Zhouetal.,2022a;Shietal.,2023;Zouetal.,
2023;Qietal.,2023). Variousresearcheffortshaveaimedatimprovingperformancethroughprompt
optimization. Typicalapproachesincludeheuristicsearchusinglanguagemodels’knowledge(Gao
etal.,2020;Shinetal.,2020),first-ordermethodslikesoftprompttuning(Lesteretal.,2021),and
prefixtuning(LiandLiang,2021). Inthiswork,wefocusonobtaininganinterpretablepromptin
theformofnaturallanguage,drawingoninsightsfromYangetal.(2023),Zhouetal.(2022b),and
Pryzantetal.(2023).
3Figure2: Multi-LLM-agentarchitecture. (a)ThemodulesofanLLMagentandthecompositionof
prompts. (b)Therearetwophasesinonetimestep: CommunicationphaseandActionphase. Inthe
communicationphase,theagentstaketurnscommunicatingbybroadcastingorselectingreceiversto
senddistinctmessages. Theagentscanalsochoosetokeepsilent. CommisshortforCommunication;
POisshortforPartialObservation.
Figure3: Criticize-Reflectarchitectureforimprovingorganizationalstructure. Theredagent
representstheleaderinanorganization. TheCriticevaluatesthetrajectoriesandanalyzestheagents’
performance. Togetherwiththeexternalcostsfromtheenvironment,theCoordinatorproposesanew
organizationprompttoimprovetheteamefficiency.
3 Method
3.1 ArchitectureandMulti-AgentCommunication
We adopt the embodied LLM-agent architecture proposed by Zhang et al. (2023c) and expand
it to enable organized teams of ≥ 3 agents to communicate, plan, and act in physical/simulated
environments. Figure2illustratesourarchitecture. BorrowinsightsfromZhangetal.(2023c),we
adoptfourstandardmodules: Configurator,PerceptionModule,MemoryModule,andExecution
Module. Theyareresponsibleforconfiguringtheagents,translatingenvironmentalobservationsinto
text,storing&retrievinghistoricalinformation,andexecutingactions,respectively(Fig.2(a)).
Previousworksfocusedontwo-agentcooperation,inwhichcasethecommunicationcanbesimply
treatedasanextraactionMandietal.(2023);Zhangetal.(2023c). Incontrast,weaimtoenable
threeormoreagentstoworkinateamandcooperatethroughorganizedcommunication. Thuswe
designthearchitecturewithseveralfeaturesthatfacilitateorganizedmulti-agentcommunication
(Figure2(b)):
• Weimposeanorganizationalstructurefortheagentteamviaprompting,i.e.,includinga
textualdescriptionaspartofthepromptforeachroundofcommunication.
• LLMagentskeepalternatingbetweentwophasesduringtheirtask: thecommunication
phase and the action phase. The standalone communication phase supports richer team
structuresandflexiblecommunicationpatterns.
4• Duringcommunication,agentstaketurnstocommunicate.Anagentcanchoosetobroadcast
amessage,selectonerecipientforamessage,choosemultiplerecipientsandsendthem
distinctmessages,orremainsilent. Agentskeeptheirownhistoryofcommunicationand
canrespondtomessagesfrompreviouscommunications.
3.2 Criticize-ReflectMethodforImprovingOrganizationalStructure
WeleveragepowerfulLLMstooptimizetheorganizationprompt,borrowinginsightsfrom(Yang
etal.,2023). Todoso,weintroduceadual-LLMframeworktoallowthemulti-LLM-agentsystemto
ponderandimprovetheorganizationalstructure.Figure3illustratesthearchitectureofourframework.
ItconsistsoftwoLLMs:
• LLM critic: Inspired by the Actor-Critic method of reinforcement learning Konda and
Tsitsiklis (1999), we introduce an LLM critic to evaluate the team’s performance based
onverbalfeedback. Theteamcritictakesasinputthedialogueandactionhistoryofone
episode. Then,thecriticanalyzestheinputandreasonstoextractandsummarizethekey
stepsthatarebelievedtoinfluencetheperformanceintheepisode. Also,thecriticprovides
atextualevaluationofagents’behaviorsandtherankingoftheirleadership. Notethatthe
criticinourmethodisdifferentfromZhangetal.(2023a)wherethecriticisacentralized
controllerofferingsuggestionstoeachagent. SeethepromptsinAppendixA.
• LLMcoordinator: TheLLMcoordinatortakesasinputtheoutputsoftheLLMcriticas
wellascostmetricsofpreviousepisodesfromtheenvironment. Itreflectsonthesedata
andgeneratesthoughtsbasedontheanalysisofthepastepisodes. Thecoordinatorthen
generatesthreedistinctnewpromptsandchoosesthebestoutofthem. Formoredetails
pleaserefertoAppendixA.
Foreachneworganizationprompt,werunforoneepisodefollowingtheneworganizationalstructure
and then return the dialogue and action history to the critic. By alternating between criticizing
andreflectingoncurrentstructure, theframeworkdiscoversmoreeffective, novelorganizational
structureswithself-improvement.
3.3 EnvironmentSetup
WechoseVirtualHome-SocialPuigetal.(2018,2021)astheenvironmentandextendedittosupport
multi-LLM-agentcommunicationandinteraction. Inthisenvironment,agentsarehumanoidhelpers
inavirtualhomedoinghousekeeping,wherethetasksincludePrepareafternoontea,Washdishes,
Prepare a meal, Put groceries, Set up a dinner table, etc. For instance, in Figure 1, the agents
cooperatetoprepareafternoonteabysearchingforandtransportingtask-specificitems(chocolate,
juice, wine, etc.) to a target location (the coffee table). The environment generates symbolic
observationsoftheobjectsinthehomeandtheirrelations. Eachagentonlyobservestheobjectsin
theopencontainerslocatedinherroomandteammatesinthesameroom,butshecanwalktoanother
roomtoexplore. Anyagentcancommunicatewithanyotheragent,notsubjecttoarangelimit.
Eachepisodestartsfromaninitialstatewhereagentsarerandomlylocatedintheenvironmentand
all containers are closed. The episode terminates when the task is fully completed. To evaluate
theteam’sefficiencywemeasurethenumberoftimestepstakentotaskcompletion,andwereport
theaveragenumberoftokenscommunicatedbetweenagentsperstep. Inourexperiment,eachrun
initializeswithanindependentlyrandomizedstatetoobtainthemeanandaconfidenceinterval. We
adoptGPT-4,GPT-3.5-turbo(Ouyangetal.,2022),andLlama2-70B(Touvronetal.,2023)asLLMs
inouragents. Thetemperatureissetas0.8,themaximumnumberofoutputtokensis256,andthe
numberofcompletionchoicestogenerateis1.
4 MainResults
4.1 ADesignatedLeaderEnhancesPerformance
WefirststudiedtheeffectoforganizationalstructuresandleadershiponLLMagents. Forbenchmark-
ing,weexperimentedwithdisorganizedLLMagentswithoutprovidinganyorganizationprompt. In
thiscase,agentsstillcommunicatewithoneanotherandworktocompletetheoveralltask. However,
5Figure4: Organizedteamswithadesignatedleaderachievehigherefficiency. (a,b)Comparison
betweenthecaseofdisorganizedagents,thecasewherealeaderisappointed,thecasewhereagents
choosetheirownleaderdynamically,andthecasewhereahumanplayerreplacesanagenttobe
theleader. NotethatGPT-3.5-turbodoesn’tsupportleadershipelection. (c,d)Comparingleadership
qualityforGPT-3.5-turbovs. GPT-4. TheconfidenceintervalsofHumanastheleadergroupare
calculatedover3seedswhileothersareover20seeds.
Figure5:Examplesofcommunicationmessageswhenthereisadesignatedleader.Left:messages
fromleadagents;Right: messagesfromnon-leadagents. GPT-4(upper),GPT-3.5-turbo(center),and
Llama2-70B(lower)demonstrateddifferentcommunicationstyles.
wediscoveredfrequentoccasionswhereagentssendredundant,repetitivemessagesandinterfere
withoneanother. SeeFigure1foranillustrationandseeAppendixDformoreexamples. Numeric
metricsarereportedinAppendixTable1.
Whenaleaderisappointedviatheorganizationprompt,weobserveimprovedteamperformance–
theteamscompletedthetaskinlesstime(Figure4(a)). Afterrunningthe3×GPT-3.5-turboexperi-
mentswith20randomseeds,weperformedtwo-samplet-tests,showingastatisticallysignificant
improvement by 9.76% in performance (t(38) = 1.71,p < .05). Similarly, a designated leader
brings benefits to the team of 3×GPT-4 (improved by 5.28%, t(38) = 0.86,p = 0.20) and the
team of 1×GPT-4+2×GPT-3.5-turbo (improved by 9.61%, t(38) = 1.43,p = 0.08). Compared
to the disorganized teams, teams with a designated leader only have a slightly increased or even
lesscommunicationcost(Figure4(b)). Thisisconsistentwithpatternsseeninpreviousmodelsof
hierarchicalorganizationsDoddsetal.(2003). ForadditionalexperimentsonLlama2-70B,pleasesee
AppendixTable1. Thecommunicationstylesofleadersandnon-leaderswereclearlydifferentiated,
asshowninFigure5.
Next, we asked the agents to elect their own leader. The leadership was reelected about every 9
timesteps,basedoninformationextractedfromthelatest12messages. Weobservethatagentsare
generallynotpower-seeking: theyoftenvoteforotherstolead. Insomeoccasions,agentsfavored
candidates who exhibited higher knowledge levels, for example, one agent thought that “Given
that Agent_2 has found a necessary item, it makes sense for him to be the leader in this round."
However,onmostoccasions,wecouldnottellwhetheragentsmadetheirvotesbasedonrational
reasoningorjustrandomthoughts(seeAppendixE).Inthecaseofthe3×GPT-4team1,implementing
leadershipelectionresultedinimprovedteamefficiencywhencomparedtoconsistentlyfollowing
1NotethatGPT-3.5-turboagentsdonotsupportelectionprobablyduetotheiralignmentpolicyandalways
ignorethedemandofelection.
6apredeterminedleader(t(38) = 1.84,p < .05;seeFigure4(a)). However,thisimprovementwas
accompaniedbyasubstantialincreaseincommunicationcost(i.e.,tokenusages),akintoreal-world
scenarioswhererelaxinghierarchicalstructurepotentiallyincreasescommunicationcostsMalone
(2004).
Theproposedmulti-LLM-agentarchitectureisalsohuman-friendlytosupporthuman-AIcollabora-
tion. Intheexperiment,weaskahumanplayertoreplacetheleaderintheteamof3GPT-4agents.
Werecruitthreehumanplayerstoconducttheexperiments. Figure4(a,b)demonstratesthathuman
leadershipachievedbettertaskcompletiontimeandimprovedcommunicationefficiencycompared
withGPT-4astheleader. Pleasefindmoreexamplesofdialoguesbetweenthehumanleaderand
LLMagentsinAppendixF.
4.2 LeadershipandOpenCommunicationMatters
LLMagentshavedifferentlevelsofleadership. IntheteamwithamixtureofGPT-4andGPT-3.5-
turboagents,appointingGPT-4astheleaderincreasestheteamefficiencyhigherthanifGPT-3.5-
turboistheleader(Figure4(c,d)). Weranthisexperimentonteamsofthreeagentsandfiveagents,
respectively. Inbothscenarios,thetaskcompletiontimeandcommunicationcostarereducedwhen
GPT-4actsastheleader. ThisfindingimpliesdifferentlevelsofleadershipbetweentheseLLMs.
Wealsoobservedthatencouragingconstructivefeedbacktotheleaderagenthelpedperformance.
Motivatedbysuccessfulhumanorganizations,wetriedtopromoteopencommunicationsamong
LLMagentsbyaddinganadditionalpromptthat"Iftheleader’sinstructionsarenotright,youcan
correcttheleader". Figure4(c,d)illustratestheresults. Interestingly,thismodificationimproves
theteam’soverallefficiencyandreducesthetimetotaskcompletionwhentheteamismadeupof
3×GPT-4(t(38) = 0.87,p = 0.14). Incontrast,thesamemodificationlowerstheteamefficiency
whenGPT-3.5-turboagentstrytocorrecttheleader(t(38)=0.27,p=0.40). Inbothexperiments,
thecommunicationcostincreases. WepresentmoredetailsaboutthesebehaviorsinAppendixG.
4.3 EmergenceofCooperativeBehaviors
WedelvedintothebehaviorsofLLMagentsinanorganizedteamtoinvestigatehoworganization
promptsinfluenceagents’communicationanddecisions. Analysisoftheirdialoguehistoryrevealed
that agents demonstrated a variety of cooperative behaviors, such as reporting, correction, task
allocation,andaskingforhelp(seeFigure6foranexampledialogue).
OnemayarguethatthesetypesofbehaviorscouldalsoemergeduetothenatureofLLMs, even
withoutapre-specifiedteamstructure. Thusweperformedaquantitativeanalysistostudytheimpact
ofanorganizationpromptonthesebehaviors. Wefollowedathree-stepprocess:
(1) Wedefinedthreemajorcategoriesofhumancooperativebehaviors: (i)Informationsharing:
agents influence others by offering new information, either actively or by being asked.
Reportingtotheleader,sharingnewobservations,andansweringquestionsbelongtothis
category. (ii) Leadership & assistance: agents, especially the leader if there is one, can
influenceothersbychangingtheirplans. Thebehaviorsincludetaskallocation,correction,
andaskingforhelp. (iii)Requestforguidance: agentsactivelyrequestnewinformationor
plansfortheirowndecisionmaking.
(2) Wedevelopedastandaloneprompt-basedGPT-4-classifiertoanalyzeeachpieceofdialogue.
Theclassifierdecideswhethertolabelthedialoguewithanysubsetoftheaforementioned
labels. Theclassifierhasanaccuracyof91.67%whentestedon20human-labeleddialogue
sampleswith60labels(seeAppendixAforthepromptandAppendixHforthetestsamples).
(3) Weusetheclassifiertolabelmessagesgeneratedbyourteamsofagents, andreportthe
percentagesofmessageswithcooperativebehaviors. Notethatamessagemayhavemultiple
labels.
Figure7reportstheresultsandillustratesthebehaviorpatternsfordifferentLLMagents. Theresults
supportseveralobservations. Eveninadisorganizedteam,LLMagentslovetotellotherswhatto
do. Leadership&assistanceaccountsforaround>50%ofallthebehaviors(Figure7(a)). However,
otherthantellingotherswhattodo,agentsinthedisorganizedteamdonotshowmuchcooperative
behaviours,forexampletheywouldrequestforguidancein<10%ofthedialogues.
7Figure6: Examplesofcooperativebehaviorsinadialogue. Agent_3leadstheteam(3×GPT-4
agents). Theagentsemergethreetypesofcooperativebehaviors: informationsharing,leadership&
assistance,andrequestforguidance.
Figure7: EmergentcooperativebehaviorsofLLMagents. Weanalyzedthecommunication
logofthemixtureteam(1×GPT-4+2×GPT-3.5-turbo)andaskedanotherGPT-4toannotateagent’s
cooperativebehaviors. (a)Behaviorofdisorganizedagents. (b)BehaviorofateamledbyaGPT-4
agent. (c)BehaviorofateamledbyaGPT-3.5-turboagent.
Incontrast, whentheteamhasahierarchicalorganizationtheleadLLMagentwouldpresumea
dominantroleandgiveorderstoothers(amountto> 60%oftheircommunication), whileother
memberstendtofollowandgivefewerorderscomparedwiththedisorganizedcase. (Figure7(b,c)).
Insuchateam,agentstendtoshareandaskformoreinformation,especiallyforthefolloweragents
intheteam.
4.4 NovelOrganizationalStructures
Havingevaluatedthemeritsofdifferentkindsofstructures,welettheLLMproposenovelorganiza-
tionalstructuresanditerativelyrefinetheorganizationpromptsusingtheCriticize-Reflectmethod
discussedinSection3(seealsoFigure3).
Figure8(a)visualizesthereflectionprocess. Thesystemwasinitializedwithabasicorganization
prompt, i.e., “Agent_1 as the leader to coordinate the task". As the Reflection process moves
forward, theCoordinatorgeneratesasequenceofevolvingorganizationprompts,pickingupkey
wordslike“hierarchical"and“dynamic"thatimplymorecomplexteamstructures.
We compared the team’s performance before and after the Criticize-Reflect steps. Figure 8(b)
illustrates the team’s efficiency. We observe that for 3×GPT-3.5-turbo, the new organizational
structureimprovedtheteam’sefficiencyincompletingthetask(t(38)=1.73,p<.05),atslightly
increasedcommunicationcost. Whilefor3×GPT-4and1×4+2×GPT-3.5-turbo,thecommunication
costsarereducedwithimprovedtaskefficiency(t(38)=1.56,p=0.06for3×GPT-4,andt(38)=
0.32,p=0.38for1×4+2×GPT-3.5-turbo).
TheCriticanalyzestherecordsofactionanddialogue,andperformancemetricsfromthemostrecent
episode. Itprovidesevaluationforthefullteam’strajectory,feedbacktoindividualagentsandtheir
rankings. SeebelowforanexampleoftheCriticoutput:
8Asanablationstudy,weremovedtheCriticfromourarchitectureandonlyperformedtheReflection
step. The results are shown in Figure 8(b), indicating that Reflection without the Critic leads
to performance decline (t(38) = 1.96,p < .05). In this case, the Coordinator needs to digest
all dialogue history and generate a new organization prompt. This did not work well and led to
rather vague outcomes, for example, “Establish a flexible communication network with rotating
leadershiprolesassignedbasedonagents’task-specificexpertisetofacilitateswiftdecision-making
andreduceunnecessarycommunicationsteps.” ThiscomparisonhighlightstheroleoftheCriticand
theimportanceofhavingadualCriticize-Reflectarchitecture. Formoreresults/promptsgeneratedby
thereflectionprocess,pleaserefertoAppendixI.
In addition, it is worth mentioning that LLMs are able to generate highly complex prompts that
imply novel organizational structures that are rarely seen in human societies. We illustrate the
communication patterns as team structures in Figure 9 together with the three novel structures
proposedbyCriticize-Reflect: (c)chain,(d)dual-leader,and(e)dynamicstructures,whicharethe
beststructuresofthethreesettingsinFigure8(b,c)respectively.
Finally, to test the generalizability of the novel organizational structures, we pick the best novel
prompt, the one illustrated in Figure 9(e), proposed by the Criticize-Reflect architecture on the
Prepareafternoonteatask. Wetestitonasetofsixnewtasks,comprisingofthreeeasytasksand
threehardtasks2,asshowninAppendixFigure10. Inthethreehardtasks,theteamwiththenovel
organizationalstructurehadbetterperformancesthantheteamappointingafixedGPT-4agentasthe
leader(AppendixFigure10(a,b)). Inthethreeeasytasks,thebenefitsaremarginal. Wecomparedthe
twoteamsacrossalltasks,performedat-testandconcludedthatthenovelteamstructureleadsto
moreefficientperformancethanthefixedleader(t(22)=2.08,p<.05).
5 Conclusion
Wedevelopedanovelmulti-LLM-agentarchitecturetofacilitatecommunicationandorganizethe
embodied agent teams for enhanced cooperation. Moreover, we proposed the Criticize-Reflect
architecturebasedonLLMstogeneratemoreefficientorganizationalprompts. Extensiveexperiments
withvariousgroupsettingsandorganizationalstructuresdemonstratethatahierarchically-organized
teamwithadesignated/electedleaderhassuperiorteamefficiency,whichcanbefurtherimprovedby
Criticize-Reflect.
Thecurrentworkisperformedinasingleenvironmentandlackshumanevaluation. Futurework
shallexpandtheevaluationtoabroadersetofenvironments,allowinghumanevaluationandmore
complexhuman-AIcollaboration.
2Thehardtaskshavetypicalnumbersofstepstoaccomplishthetasks>60,whilethoseofeasytasksare
<60.
9Figure8: Thereflectionandimprovementprocessforfindingnovelorganizationalstructures.
(a)Theexperimentwasdoneusingthe1×GPT-4+2×GPT-3.5-turboteam. Theorganizationprompt
evolvesduringtheiterations,andtakesonadditionalkeywordssuchas"central","hierarchical",and
"dynamic". (b)Theconfidenceintervalsarecalculatedover20seeds.
Figure9: Communicationpatternsandthecorrespondingorganizationalpromptsfordifferent
teamstructures. (a)Teamwithoutorganizationprompts. (b)Teamwithaleader. (c)Ateaminthe
chainstructure. (d)Adual-leaderteam. (e)Ateamwithadynamicleadership. (c,d,e)areproposed
byLLMviaReflection. Red-robotnodesmarktheleadagents,andothernodesarethefollowers.
Edgesmarktheaccumulatedcommunicationcostbetweenthetwonodes(darkeredgemeanshigher
tokencosts).
ImpactStatement
This research studies the integration of prompt-based organizational structures to teams of LLM
agents,contributingtomoreefficientandcoherentmulti-agentinteractions. Thesefindingshavethe
potentialtogreatlyinfluencethedeploymentofmoreeffectiveandautonomousmulti-agentsystems
in various fields, including robotics, virtual assistants, etc. For example, the study has potential
applicationsindisasterresponsescenarios,whereefficientmulti-agentcoordinationiscrucial.
References
SaaketAgashe,YueFan,andXinEricWang. Evaluatingmulti-agentcoordinationabilitiesinlarge
languagemodels. arXivpreprintarXiv:2310.03903,2023.
YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,DawnDrain,
StanislavFort,DeepGanguli,TomHenighan,etal. Trainingahelpfulandharmlessassistantwith
reinforcementlearningfromhumanfeedback. arXivpreprintarXiv:2204.05862,2022.
PatrickBoltonandMathiasDewatripont. Thefirmasacommunicationnetwork. TheQuarterly
JournalofEconomics,109(4):809–839,1994.
Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan,
Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. AgentVerse:
Facilitatingmulti-agentcollaborationandexploringemergentbehaviorsinagents. arXivpreprint
arXiv:2308.10848,2023a.
10YongchaoChen,JacobArkin,YangZhang,NicholasRoy,andChuchuFan. Scalablemulti-robot
collaborationwithlargelanguagemodels: Centralizedordecentralizedsystems? arXivpreprint
arXiv:2309.15943,2023b.
Donald Chisholm. Coordination without hierarchy: Informal structures in multiorganizational
systems. UniversityofCaliforniaPress,1992.
AbhishekDas,ThéophileGervet,JoshuaRomoff,DhruvBatra,DeviParikh,MikeRabbat,andJoelle
Pineau. Tarmac: Targetedmulti-agentcommunication. InInternationalConferenceonMachine
Learning,pages1538–1546.PMLR,2019.
PeterSheridanDodds,DuncanJWatts,andCharlesFSabel.Informationexchangeandtherobustness
oforganizationalnetworks. ProceedingsoftheNationalAcademyofSciences,100(21):12516–
12521,2003.
JakobFoerster,IoannisAlexandrosAssael,NandodeFreitas,andShimonWhiteson. Learningto
communicatewithdeepmulti-agentreinforcementlearning. InAdvancesinNeuralInformation
ProcessingSystems,volume29,2016.
TianyuGao,AdamFisch,andDanqiChen. Makingpre-trainedlanguagemodelsbetterfew-shot
learners. arXivpreprintarXiv:2012.15723,2020.
LuisGaricano. Hierarchiesandtheorganizationofknowledgeinproduction. JournalofPolitical
Economy,108(5):874–904,2000.
SvenGronauerandKlausDiepold. Multi-agentdeepreinforcementlearning: asurvey. Artificial
IntelligenceReview,pages1–49,2022.
XudongGuo,DamingShi,andWenhuiFan. Scalablecommunicationformulti-agentreinforcement
learningviatransformer-basedemailmechanism.InProceedingsoftheThirty-SecondInternational
JointConferenceonArtificialIntelligence,IJCAI-23,pages126–134,2023.
ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,ZhenWang,DaisyZheWang,andZhitingHu.
Reasoningwithlanguagemodelisplanningwithworldmodel. arXivpreprintarXiv:2305.14992,
2023a.
ShiboHao,TianyangLiu,ZhenWang,andZhitingHu. Toolkengpt: Augmentingfrozenlanguage
modelswithmassivetoolsviatoolembeddings. arXivpreprintarXiv:2305.11554,2023b.
SiruiHong,XiawuZheng,JonathanChen,YuhengCheng,JinlinWang,CeyaoZhang,ZiliWang,
Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin
Wu. MetaGPT: Meta programming for multi-agent collaborative framework. arXiv preprint
arXiv:2308.00352,2023.
NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,PedroOrtega,DJStrouse,
Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent
deepreinforcementlearning. InInternationalconferenceonmachinelearning,pages3040–3049.
PMLR,2019.
WoojunKim, JongeuiPark, andYoungchulSung. Communicationinmulti-agentreinforcement
learning: Intentionsharing. InInternationalConferenceonLearningRepresentations,2020.
Vijay Konda and John Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
ProcessingSystems,volume12,1999.
BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-efficientprompt
tuning. arXivpreprintarXiv:2104.08691,2021.
GuohaoLi,HasanAbedAlKaderHammoud,HaniItani,DmitriiKhizbullin,andBernardGhanem.
CAMEL: Communicative agents for ”mind” exploration of large language model society. In
Thirty-seventhConferenceonNeuralInformationProcessingSystems,2023a.
HuaoLi,YuQuanChong,SimonStepputtis,JosephCampbell,DanaHughes,MichaelLewis,and
KatiaSycara. Theoryofmindformulti-agentcollaborationvialargelanguagemodels. arXiv
preprintarXiv:2310.10701,2023b.
11Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.
InChengqingZong,FeiXia,WenjieLi,andRobertoNavigli,editors,Proceedingsofthe59th
AnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJoint
ConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pages4582–4597,August
2021.
ToruLin, JacobHuh, ChristopherStauffer, SerNamLim, andPhillipIsola. Learningtoground
multi-agentcommunicationwithautoencoders. InAdvancesinNeuralInformationProcessing
Systems,volume34,pages15230–15242,2021.
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
andPercyLiang. Lostinthemiddle: Howlanguagemodelsuselongcontexts. arXivpreprint
arXiv:2307.03172,2023a.
ZijunLiu,YanzheZhang,PengLi,YangLiu,andDiyiYang.DynamicLLM-agentnetwork:AnLLM-
agentcollaborationframeworkwithagentteamoptimization. arXivpreprintarXiv:2310.02170,
2023b.
RyanLowe,AvivTamar,JeanHarb,OpenAIPieterAbbeel,andIgorMordatch. Multi-agentactor-
criticformixedcooperative-competitiveenvironments. Advancesinneuralinformationprocessing
systems,30,2017.
ThomasWMalone. Thefutureofwork. HarvardBusinessReviewPress,2004.
ZhaoMandi,ShreeyaJain,andShuranSong. RoCo: Dialecticmulti-robotcollaborationwithlarge
languagemodels. arXivpreprintarXiv:2307.04738,2023.
JamesGMarchandHerbertASimon. Organizations. Wiley,1958.
AfshinOroojlooyandDavoodHajinezhad. Areviewofcooperativemulti-agentdeepreinforcement
learning. AppliedIntelligence,53(11):13677–13722,2023.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructions with human feedback. Advances in Neural Information Processing Systems, 35:
27730–27744,2022.
JoonSungPark,JosephO’Brien,CarrieJunCai,MeredithRingelMorris,PercyLiang,andMichaelS
Bernstein. Generativeagents: Interactivesimulacraofhumanbehavior. InProceedingsofthe36th
AnnualACMSymposiumonUserInterfaceSoftwareandTechnology,pages1–22,2023.
ShishirGPatil,TianjunZhang,XinWang,andJosephEGonzalez. Gorilla: Largelanguagemodel
connectedwithmassiveapis. arXivpreprintarXiv:2305.15334,2023.
ReidPryzant,DanIter,JerryLi,YinTatLee,ChenguangZhu,andMichaelZeng. Automaticprompt
optimizationwith"gradientdescent"andbeamsearch. arXivpreprintarXiv:2305.03495,2023.
XavierPuig,KevinRa,MarkoBoben,JiamanLi,TingwuWang,SanjaFidler,andAntonioTorralba.
Virtualhome:Simulatinghouseholdactivitiesviaprograms.InProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition,pages8494–8502,2018.
XavierPuig,TianminShu,ShuangLi,ZilinWang,Yuan-HongLiao,JoshuaB.Tenenbaum,Sanja
Fidler,andAntonioTorralba. Watch-and-help: Achallengeforsocialperceptionandhuman-AI
collaboration. arXivpreprintarXiv:2010.09890,2021.
XiangyuQi,KaixuanHuang,AshwineePanda,MengdiWang,andPrateekMittal. Visualadversarial
examplesjailbreakalignedlargelanguagemodels. InTheSecondWorkshoponNewFrontiersin
AdversarialMachineLearning,2023.
RoyRadner. Theorganizationofdecentralizedinformationprocessing. Econometrica: Journalof
theEconometricSociety,pages1109–1146,1993.
CinjonResnick,WesEldridge,DavidHa,DennyBritz,JakobFoerster,JulianTogelius,Kyunghyun
Cho,andJoanBruna. Pommerman: Amulti-agentplayground. arXivpreprintarXiv:1809.07124,
2018.
12MikayelSamvelyan,TabishRashid,ChristianSchroederdeWitt,GregoryFarquhar,NantasNardelli,
TimGJRudner, Chia-ManHung, PhilipHSTorr, JakobFoerster, andShimonWhiteson. The
starcraftmulti-agentchallenge.InProceedingsofthe18thInternationalConferenceonAutonomous
AgentsandMultiAgentSystems,pages2186–2188,2019.
YongliangShen,KaitaoSong,XuTan,DongshengLi,WeimingLu,andYuetingZhuang.Hugginggpt:
Solvingaitaskswithchatgptanditsfriendsinhuggingface. arXivpreprintarXiv:2303.17580,
2023.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael
Schärli,andDennyZhou. Largelanguagemodelscanbeeasilydistractedbyirrelevantcontext. In
InternationalConferenceonMachineLearning,pages31210–31227.PMLR,2023.
TaylorShin,YasamanRazeghi,RobertLLoganIV,EricWallace,andSameerSingh. Autoprompt:
Elicitingknowledgefromlanguagemodelswithautomaticallygeneratedprompts. arXivpreprint
arXiv:2010.15980,2020.
NoahShinn,FedericoCassano,AshwinGopinath,KarthikRNarasimhan,andShunyuYao.Reflexion:
Languageagentswithverbalreinforcementlearning. InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023.
HerbertASimonetal. Designingorganizationsforaninformation-richworld. Computers,communi-
cations,andthepublicinterest,72:37,1971.
Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive
planningfromfeedbackwithlanguagemodels. arXivpreprintarXiv:2305.16653,2023.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
Timothy Van Zandt. Decentralized information processing in the theory of organizations. In
Contemporary Economic Issues: Economic Behaviour and Design, pages 125–160. Springer,
1999.
NataliaVélez,BrianChristian,MathewHardy,BillDThompson,andThomasLGriffiths. Howdo
humansovercomeindividualcomputationallimitationsbyworkingtogether? CognitiveScience,
47(1):e13232,2023.
OriolVinyals,IgorBabuschkin,WojciechM.Czarnecki,MichaëlMathieu,AndrewDudzik,Junyoung
Chung,DavidH.Choi,RichardPowell,TimoEwalds,PetkoGeorgiev,JunhyukOh,DanHorgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max
Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David
Budden,YurySulsky,JamesMolloy,TomL.Paine,CaglarGulcehre,ZiyuWang,TobiasPfaff,
YuhuaiWu,RomanRing,DaniYogatama,DarioWünsch,KatrinaMcKinney,OliverSmith,Tom
Schaul,TimothyLillicrap,KorayKavukcuoglu,DemisHassabis,ChrisApps,andDavidSilver.
GrandmasterlevelinStarCraftIIusingmulti-agentreinforcementlearning. Nature,575(7782):
350–354,2019.
GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,and
AnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels. arXiv
preprintarXiv:2305.16291,2023.
JiaweiWang,TianyuShi,YuankaiWu,LuisMiranda-Moreno,andLijunSun. Multi-agentgraph
reinforcementlearningforconnectedautomateddriving. InProceedingsofthe37thInternational
ConferenceonMachineLearning(ICML),pages1–6,2020.
Lu Wang, Lei Han, Xinru Chen, Chengchang Li, Junzhou Huang, Weinan Zhang, Wei Zhang,
Xiaofeng He, and Dijun Luo. Hierarchical multiagent reinforcement learning for allocating
guaranteeddisplayads. IEEETransactionsonNeuralNetworksandLearningSystems, pages
1–13,2021.
13JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
NeuralInformationProcessingSystems,35:24824–24837,2022.
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,
Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via
multi-agentconversationframework. arXivpreprintarXiv:2308.08155,2023.
ZelaiXu,ChaoYu,FeiFang,YuWang,andYiWu. Languageagentswithreinforcementlearningfor
strategicplayinthewerewolfgame. arXivpreprintarXiv:2310.18940,2023.
ChengrunYang,XuezhiWang,YifengLu,HanxiaoLiu,QuocVLe,DennyZhou,andXinyunChen.
Largelanguagemodelsasoptimizers. arXivpreprintarXiv:2309.03409,2023.
ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao.
React: Synergizingreasoningandactinginlanguagemodels. arXivpreprintarXiv:2210.03629,
2022.
Bin Zhang, HangyuMao, Jingqing Ruan, Ying Wen, Yang Li, Shao Zhang, Zhiwei Xu, Dapeng
Li, Ziyue Li, Rui Zhao, et al. Controlling large language model-based agents for large-scale
decision-making: Anactor-criticapproach. arXivpreprintarXiv:2311.13884,2023a.
CeyaoZhang,KaijieYang,SiyiHu,ZihaoWang,GuangheLi,YihangSun,ChengZhang,Zhaowei
Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, and
YaodongYang. ProAgent: BuildingproactivecooperativeAIwithlargelanguagemodels. arXiv
preprintarXiv:2308.11339,2023b.
HongxinZhang,WeihuaDu,JiamingShan,QinhongZhou,YilunDu,JoshuaB.Tenenbaum,Tianmin
Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language
models. arXivpreprintarXiv:2307.02485,2023c.
HuichuZhang,SiyuanFeng,ChangLiu,YaoyaoDing,YichenZhu,ZihanZhou,WeinanZhang,Yong
Yu,HaimingJin,andZhenhuiLi. CityFlow: Amulti-agentreinforcementlearningenvironment
forlargescalecitytrafficscenario. InTheWorldWideWebConference,pages3620–3624.ACM,
2019a.
JintianZhang,XinXu,andShuminDeng. Exploringcollaborationmechanismsforllmagents: A
socialpsychologyview. arXivpreprintarXiv:2310.02124,2023d.
KaiqingZhang,ZhuoranYang,andTamerBas¸ar. Multi-agentreinforcementlearning: Aselective
overview of theories and algorithms. Handbook of reinforcement learning and control, pages
321–384,2021.
SaiQianZhang,QiZhang,andJieyuLin. Efficientcommunicationinmulti-agentreinforcement
learning via variance based control. In Advances in Neural Information Processing Systems,
volume32,2019b.
Yi Zheng, Chongyang Ma, Kanle Shi, and Haibin Huang. Agents meet okr: An object and key
resultsdrivenagentsystemwithhierarchicalself-collaborationandself-evaluation. arXivpreprint
arXiv:2311.16542,2023.
DennyZhou,NathanaelSchärli,LeHou,JasonWei,NathanScales,XuezhiWang,DaleSchuurmans,
ClaireCui,OlivierBousquet,QuocLe,etal. Least-to-mostpromptingenablescomplexreasoning
inlargelanguagemodels. arXivpreprintarXiv:2205.10625,2022a.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,
and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint
arXiv:2211.01910,2022b.
XizhouZhu,YuntaoChen,HaoTian,ChenxinTao,WeijieSu,ChenyuYang,GaoHuang,BinLi,
LeweiLu,XiaogangWang,etal. Ghostintheminecraft: Generallycapableagentsforopen-world
enviromentsvialargelanguagemodelswithtext-basedknowledgeandmemory. arXivpreprint
arXiv:2305.17144,2023.
AndyZou,ZifanWang,JZicoKolter,andMattFredrikson. Universalandtransferableadversarial
attacksonalignedlanguagemodels. arXivpreprintarXiv:2307.15043,2023.
14A PromptTemplates
WelistthepromptsofActor,Communicator,Critic,andCoordinatorasfollows.
ActorandtheCommunicator. ORGANIZATION_INSTRUCTIONistheplaceholderfortheorganiza-
tioninstructionprompt,eithermanuallydesignedorautomaticallygenerated. Theenvironmentwill
providetextdescriptionsforthecurrentGOAL,PROGRESS,andAVAILABLE_ACTIONS.Weinclude
thelatest12sentandreceivedmessagesasDIALOGUE_HISTORY,andthelatest20stepsofactionsas
ACTION_HISTORY.
15Critic. We provide the full trajectory as the input to TRAJECTORIES. Additionally,
ORGANIZATION_INSTRUCTION and GOAL of the current task and organization are also provided
asanadditionalcontext.
16Coordinator. In“Instructionexamples”,weincludethebasicsetting(goal,organizationstructurein-
struction),thecommunicationcost,thenumberofstepstaken,aswellasthesummarizedinformation
generatedbytheCritic(leadershipranking,problems,summaryofthetrajectory)fortheCoordinator.
17Classifier. WefeedthemessagestotheGPT-4classifierandgetthelabels. Therubricsaremanually
writtenafterinvestigatingthecommunicationlogs.
18B AdditionalResults
B.1 Completelistofbasicexperimentalresults
We present the full results of various group settings and organization instructions in Appendix
Table1. Here,wealsoincludetheresultsof1×GPT-4+2×Llama2-70B.Surprisingly,GPT-4exhibits
poorerleadershipthanLlama2-70Binthiscase. Thecommunicationcostsfortheteamscontaining
Llama2-70BaremuchhigherthanthosecontainingGPT-3.5-turbo.
Table1: Performancefordifferentorganizationinstructions. Whentherearetwodifferentkinds
ofLLMsinthegroup,Agent_1isGPT-4,andAgent_2istheothertypeofLLM.
GROUPSETTING ORGANIZATIONINSTRUCTION TIME COMMUNICATIONCOST
3×GPT-4 NONE 57.75±13.09 67.03±9.68
3×GPT-4 AGENT1ISTHELEADERTOCOORDINATETHE 54.70±8.92 54.73±8.89
TASK.
3×GPT-4 AGENT1ISTHELEADERTOCOORDINATETHE 50.70±13.92 63.49±8.61
TASK.IFTHELEADER’SINSTRUCTIONSARE
NOTRIGHT,YOUCANCORRECTTHELEADER.
3×GPT-4 ELECTANEWLEADEREVERY10STEPSTO 49.20±9.97 135.03±20.45
COORDINATETHETASK....AFTERTHEELEC-
TION,THEOTHERAGENTSSHOULDFOLLOW
THELEADER’SINSTRUCTIONS.
3×GPT-3.5-TURBO NONE 102.95±21.88 53.73±6.04
3×GPT-3.5-TURBO AGENT1ISTHELEADERTOCOORDINATETHE 92.90±14.70 59.87±6.33
TASK.
3×GPT-3.5-TURBO AGENT1ISTHELEADERTOCOORDINATETHE 94.20±16.22 60.53±3.66
TASK.IFTHELEADER’SINSTRUCTIONSARE
NOTRIGHT,YOUCANCORRECTTHELEADER.
1×GPT-4+2×GPT-3.5-TURBO NONE 81.10±18.35 54.00±5.06
1×GPT-4+2×GPT-3.5-TURBO AGENT1ISTHELEADERTOCOORDINATETHE 73.30±16.12 55.82±6.57
TASK.
1×GPT-4+2×GPT-3.5-TURBO AGENT1ISTHELEADERTOCOORDINATETHE 85.67±14.52 61.57±0.55
TASK.IFTHELEADER’SINSTRUCTIONSARE
NOTRIGHT,YOUCANCORRECTTHELEADER.
1×GPT-4+2×GPT-3.5-TURBO AGENT2ISTHELEADERTOCOORDINATETHE 75.65±15.43 58.39±8.11
TASK.
1×GPT-4+2×GPT-3.5-TURBO AGENT2ISTHELEADERTOCOORDINATETHE 72.33±6.60 74.21±7.73
TASK.IFTHELEADER’SINSTRUCTIONSARE
NOTRIGHT,YOUCANCORRECTTHELEADER.
1×GPT-4+2×LLAMA2-70B NONE 77.00±2.94 119.48±1.28
1×GPT-4+2×LLAMA2-70B AGENT1ISTHELEADERTOCOORDINATETHE 83.67±10.96 135.22±16.39
TASK.
1×GPT-4+2×LLAMA2-70B AGENT2ISTHELEADERTOCOORDINATETHE 76.00±5.72 142.24±11.85
TASK.
2×GPT-4+3×GPT-3.5-TURBO NONE 42.67±4.03 98.03±9.86
2×GPT-4+3×GPT-3.5-TURBO AGENT1ISTHELEADERTOCOORDINATETHE 39.67±9.46 94.73±4.01
TASK.
2×GPT-4+3×GPT-3.5-TURBO AGENT2ISTHELEADERTOCOORDINATETHE 48.50±9.50 96.53±2.51
TASK.
19B.2 AcrossTaskGeneralizability
Weconductexperimentsacrossthetaskstotestthegeneralizabilityoftheprompt“dynamicleadership”
(Figure9(e))foundusingCriticize-ReflectarchitectureonthePrepare_Afternoon_Teataskandreport
theperformanceinFigure10;seeSection4.4forthecompletesettinganddiscussions.
Figure 10: The organized team structure with a designated leader and the novel structure
proposedbyCriticize-Reflectarchitecturegeneralizedtodifferenttasks. Thepromptfordynamic
leadershipisproposedbyCriticize-ReflectarchitectureonthePrepare_Afternoon_Teataskshownin
Figure8(a). Theexperimentwasdoneusingthe1×GPT-4+2×GPT-3.5-turboteamovertwoseedsfor
eachtask. (a,b)Hardtasks(read_book,put_dishwasher_hard,prepare_food)withtypicalnumbersof
stepstoaccomplishthetasks>60. (c,d)Easytasks(put_dishwasher_easy,put_fridge,setup_table)
withtypicalnumbersofstepstoaccomplishthetasks<60.
20C EmergentCooperativeBehaviorsinanOrganization
Byinvestigatingthemessagesbetweenagents,wemainlyobservethefollowingcooperativebehaviors,
assummarizedinTable2.
Table2: Typicalcooperativebehaviors.
Type Description Example
Sharinginformation Anagentsharesherobserva- (Ex 1.)“I’m in the bathroom. There’s an unchecked
tions to others, reports her <bathroomcabinet>(190).”
task-relatedprogresstooth- (Ex2.)“I’llcheckthecabinetinthebedroom”
ers, or responds to other
agents’requests
Givingorders Anagentgivesorderstooth- “Istillneedtofind<pudding>(371). Canyouhelpme
ers,eitherbydirectlygiving searchthebedroomfortheremainingitem?”
acommandorbyapolitere-
quest
Asking for informa- An agent asks other agents (Ex1.)“Whereareyounow?”
tion about their location, task (Ex2.)“Anyupdatesfromthekitchen?”
progress, or other informa- (Ex3.)“Doweknowthelocationofthecoffeetable?”
tion
Exchanginginforma- Anagentsharesoneagent’s Agent3→Agent1: “Foundcupcakeandjuiceinbed-
tion informationtoanotheragent room,plusawine.”;
Agent1→Agent2:“Agent3foundawine,cupcake,and
juiceinthebedroom.”
Askingfororders Anagentaskswhatsheneeds “I’minthekitchenwiththedishwasher,stove,microwave
todoorwhethershecanhelp unchecked.Needmetochecktheseorsomethingelse?”
others
Correction Anagentcorrectstheplanof (Ex1.) Agent2→Agent3: “AfterIplacethepudding
others onthecoffeetable,let’ssplitup.Youcheckthebathroom,
I’llcheckthebedroom.”;Agent3→Agent2:“Youdon’t
needtosearchthebedroom,thejuice(380)isthere.”
(Ex2.) “Therearenomorewineorjuiceinthekitchen
cabinets.Weneedtocomeupwithanotherplan.”
21D IneffectiveCommunication
There are also cases in which language model agents fail to communicate efficiently. From the
messagesbetweenagents,wesummarizethetypicalcategoriesinTable3.
Table3: Typicalineffectivecommunicationbehaviors.
Type Description Example
Duplicatedmessage The same message is said Agent1→Agent3:“Checkkitchencabinetordishwasher
morethanonce forwine,I’llcheckthestove.”,
Agent2→Agent3: “Agent1willcheckthestovefor
wine.”
Repeatedcommand Thesamecommandissaid Agent3→Agent1:“Explorethebathroomforthesecond
morethanonce wine.”
Agent2→Agent1:“Gotothebathroomandlookforthe
wine.”
Conflictingcommand Anagentgivesanotheragent Agent 1 → Agent 3: “Find <juice> and check
an order that conflicts with <bathroomcabinet>(190)”
hercurrenttask Agent2→Agent3:“Checkthekitchencontainersforthe
lastwine.”
Conflictingplans Anagentclaimstocommit Agent3→Agent1:“Ifoundthepuddingandwillcheck
totwoconflictingplans thedishwasher,stove,andmicrowaveforthesecondwine.”
Agent3→Agent2: “Iwillcontinuesearchingforthe
secondwineinthebedroomandbathroom.”
Improperdelegation Anagentasksanotheragent Agent 1 → Agent 2: “Continue checking the kitchen
todoherowntask cabinetsforremainingitems.”
Agent2→Agent3:“Pleasecontinuecheckingtheother
kitchencabinetsfortheremainingitems.”
Ignoringrequests An agent ignores other Agent2→Agent3:“Ihaven’tfoundanyoftheremaining
agents’questions itemsinthekitchen.Haveyoufoundanyoftherequired
itemsinthelivingroom?”
Agent3→Agent2: “Ihaven’texploredthebathroom
yet.”
22E ExamplesofElection
InFigure11,theagentsvotetoelectanewleader. Wecanobservebehaviorssuchasnominations
forthemselvesandotheragents, voting, andconsensusachievement. Wefindthattheagentsare
notpower-seekingandmaygiveupleadershipearly. Theagentsprefertovoteforothersinstead
ofnominatingthemselves(5timesmoreduringthewholetask). Theelectedleaderalsodoesnot
plantokeepthepositionbuttonominateothersforthenextround. Also, theagents’standpoint
canbeeasilyinfluencedbyothers. Theagentsdonotdebatemuchtowintheelectionbutreacha
consensussoon. Forexample,Agent_1givesuprunningforherselfbutvotesforAgent_2becauseof
Agent_3’ssupport. Furthermore,sometimesnominationsandvotesaredeterminedbyhallucinations.
For example, at step 2, Agent_2 nominates Agent_1 as he was the first one to propose a search
strategy. However,basedonthepreviousdialogues,Agent_1hasnotproposedanystrategyyet.
Figure11: Examplesoftheelectionofanewleader. Ittakestwostepstovoteandnegotiateto
determine the new leader in this case. Note that Agent_3 chooses not to send a message as the
electionisdoneandnomoreinformationtobesharedfornow. Allthemessagesinthefigureare
broadcasts.
23F ExamplesofHuman-AICollaboration
WeconductedexperimentsinvolvingateamconsistingofonehumanplayerandtwoGPT-4agents,
withthehumanplayeractingastheleader. Figure12illustratestheremarkablecollaborationbetween
humansandAI.
Figure12:Examplesofhuman-AIcollaborationwhenthehumanplayerleadstwoGPT-4agents
(Agent_2&3).
24G ExamplesofCorrection
Due to hallucination and the limit of the dialogue history buffer, the leader may forget what has
happened and give wrong orders. When the prompt encourages the agents to correct the leader
whennecessarybyaddingIftheleader’sinstructionsarenotright,youcancorrecttheleader,some
correctionbehaviorsappear,asshowninFigure13.
In the first example, the leader Agent_1 gives an unnecessary and repetitious instruction. Then
Agent_2correctstheleadertoavoidtimewasting. Inthesecondexample,theleaderAgent_1may
havehallucinationsandcannotrememberwhatAgent_3isholdingclearly(cupcakeandwineinthe
messagewhilejuiceandwineinthethoughts). Therefore,Agent_3clarifiesthatsheisnotholding
thecupcakeandwineandshareshernextplanwiththeleader.
Figure 13: Examples of correction dialogues and the corresponding thoughts. The prompt
includesIftheleader’sinstructionsarenotright,youcancorrecttheleader.
25H ExamplesofCooperativeBehaviorsClassificationbyHumansandGPT-4
WeaskahumanevaluatorandGPT-4tolabelthedialoguesintothreedifferentbehaviorcategories
showninTable4.
Table4:DialoguesandbehaviorlabelsgivenbyhumansandGPT-4.Label1:informationsharing;
Label2: leadership&assistance;Label3: Requestforguidance.
Human GPT-4
Dialogue
Label1* Label2 Label3 Label1 Label2 Label3
Hey,whereareyou?Pleaseletmeknowyourlocation 0 1 1 0 1 1
sothatIcanassignyouatask.
I’m currently in the bedroom where I found an 1 1 0 1 1 0
uncheckedcabinet.Pleaseexplorethelivingroomand
startcheckingfortherequireditems.
I’m currently in the bedroom where I found an 1 0 0 1 0 0
uncheckedcabinet.Ihaven’tfoundanyoftherequired
itemsyet.
Checkthecabinetinthebedroom.I’llchecktheone 1 1 0 0 1 0
inthebathroom.
Ihaven’tfoundanyoftherequireditemsyet.Didyou 1 0 1 1 0 1
findanyinthekitchen?
Ihaven’tfoundanyoftherequireditemsyet. Have 1 0 1 1 0 1
youfoundanyinthebathroom?
Agent3,Ifoundapuddinginoneofthekitchencabi- 1 1 0 1 1 0
nets.Pleasecontinuecheckingtheotherkitchencabi-
netsfortheremainingitems.
Ihaven’tfoundanyoftherequireditemsyet.Didyou 1 0 1 1 0 1
findanyotherrequireditemsinthekitchen?
Ihaven’tfoundanyoftheremainingitemsyet. 1 0 0 1 0 0
I’minthelivingroomsearchingfortheremaining 1 0 0 1 0 0
items.
Okay,Iwillkeepcheckingthekitchencabinetsforthe 1 0 0 1 0 0
remainingitems.
I’mcurrentlyinthelivingroomsearchingforthere- 1 0 0 1 0 0
mainingitems.
Ihaven’tfoundanyoftheremainingitemsyet. 1 0 0 1 0 0
I’mstillsearchingthelivingroom. Haveyoufound 1 0 1 1 0 1
anyoftherequireditems?
Putitemsonthetableonceyoufindthem.Keepsearch- 0 1 0 0 1 0
ingyourcurrentareas.
I haven’t found any of the remaining items in the 1 0 1 1 0 1
kitchen. Haveyoufoundanyoftherequireditems
inthelivingroom?
Ihaven’texploredthebathroomyet.Haveyoufound 1 0 1 1 0 1
anyoftherequireditemsinthelivingroom?
Ihaveexploredthebedroomandfoundwine,cupcake, 1 1 1 1 1 0
andjuice. Istillneedtofindpudding. Canyouhelp
mesearchthebedroomfortheremainingitem?
Ifoundthewine,cupcake,andjuiceinthebedroom. 1 0 0 1 1 0
Agent1wantsmetoputthemonthecoffeetableand
thencheckthebathroomcabinet.
Agent1wantsustocheckifthere’sanotherwinein 1 0 0 0 1 0
thekitchen.
26I ExamplesofNewPromptsafterReflection
WelistmorepromptsgeneratedbytheCriticize-ReflectarchitectureinFigure14.
Figure14: ExamplesofPromptsgeneratedviaReflection. Thefirstrowisgeneratedwiththe
Critic,whilethesecondrowiswithouttheCritic,wherethenewpromptsarerelativelyvague. Note
thatthereisnoAgentZintheteam.
27