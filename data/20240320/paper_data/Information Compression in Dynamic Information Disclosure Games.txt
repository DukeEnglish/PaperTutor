Information Compression in Dynamic Information Disclosure Games
Dengwang Tang and Vijay G. Subramanian
Abstract—We consider a two-player dynamic information benefit from the drug. In turn, the drug companies can make
design problem between a principal and a receiver—a game is a profit. Without government regulations, drug companies
played between the two agents on top of a Markovian system
andthepublicwillbothsufferduetomistrust.Inmanyreal-
controlledbythereceiver’sactions,wheretheprincipalobtains
world dynamic systems, information exchange and decision
andstrategicallysharessomeinformationabouttheunderlying
system with the receiver in order to influence their actions. making can happen repeatedly as the system/environment
In our setting, both players have long-term objectives, and changes over time—for example, public companies disclose
the principal sequentially commits to their strategies instead information periodically which impacts stockholders’ deci-
of committing at the beginning. Further, the principal cannot
sions;(COVID-19)vaccineproducersconducttheirtrialsand
directly observe the system state, but at every turn they can
release results sequentially which impacts the government’s
chooserandomizedexperimentstoobservethesystempartially.
The principal can share details about the experiments to the purchasing decisions; during an epidemic, health authorities
receiver. For our analysis we impose the truthful disclosure updatetheirrecommendationsontheuseoffacemasksover
rule:theprincipalisrequiredtotruthfullyannouncethedetails timeaccordingtochanginglevelsofinfection,etc.Therefore,
and the result of each experiment to the receiver immediately in the face of information asymmetry, it is important to
after the experiment result is revealed. Based on the received
establish rules/protocols to facilitate repeated information
information, the receiver takes an action when its their turn,
with the action influencing the state of the underlying system. exchange among agents in multi-agent dynamic systems.
We show that there exist Perfect Bayesian equilibria in this In the economics literature, there are mainly two ap-
game where both agents play Canonical Belief Based (CBB) proaches to the above problem, namely mechanism design
strategies using a compressed version of their information,
[1] and information design [2]. In mechanism design, less
rather than full information, to choose experiments (for the
informedagentscanextractinformationfrommoreinformed
principal) or actions (for the receiver). We also provide a
backward inductive procedure to solve for an equilibrium in agents by committing to how they will use the collected in-
CBB strategies. formation beforehand. Whereas in information design, more
informed agents can partially disclose information to less
I. INTRODUCTION
informed agents. The more informed agents commit on the
Inmanymodernengineeringandsocioeconomicproblems manner in which they partially disclose their information. In
andsystems,suchascyber-security,transportationnetworks, both approaches, all agents can benefit from the information
ande-commerce,informationasymmetryisaninevitableas- exchange.Forbothapproaches,onecanclassifythepertinent
pectthatcruciallyimpactsdecisionmaking.Inthesesystems, literature into two groups: (i) static settings, where both
agents need to decide on their actions under limited infor- information disclosure and decision making take place only
mation about the system and each other. In many situations, once; and (ii) dynamic settings, where agents repeatedly
agents can overcome (some of) the information asymmetry disclose information and take actions over time on top of
by communicating with each other. However, agents can be an ever changing environment/physical system. Mechanism
unwilling to share information when agents’ goals are not design and information design for the dynamic settings are
aligned with each other, since having some information that more challenging than the static settings since agents need
anotheragentdoesnotknowcanbeanadvantage.Ingeneral, to anticipate future information disclosure when taking an
communication between agents with diverging incentives action. Mechanism design in dynamic settings has been
cannot be naturally established without rules/protocols that studied extensively in the literature [3], [4], [5], [6]. In most
everyone agrees upon, and all agents suffer due to the of the works on information design in dynamic settings,
breakdown of the information exchange. For example, drug the receivers are assumed to be myopic [7], [8], [9], [10],
companies are required by regulations to disclose their trial [11],[12],[13].Thisassumptiongreatlysimplifiesreceivers’
results truthfully. The public can then trust the results and decision making. There have been a few papers studying
information design problems where all agents in the system
AversionofthisworkissubmittedtoIEEECDC2024.
have long-term goals [14], [15], [16], [17], [18], [19], [20],
This work is supported by NSF Grant No. ECCS 1750041, ECCS
2038416, ECCS 1608361, CCF 2008130, ARO Award No. W911NF-17- [21], [22], [23], [24]. These papers typically assume that the
1-0232, MIDAS Sponsorship Funds by General Dynamics, and Okawa principalcommitstotheirstrategyforthewholegamebefore
FoundationResearchGrant.
thegamestartsasinaStackelbergequilbrium[25].Thebulk
D. Tang is with Ming Hsieh Department of Electrical and Computer
Engineering, University of Southern California, Los Angeles, CA 90089, of this literature also assumes that the principal observes the
USAdengwang@usc.edu underlying state perfectly. However, these assumptions can
V.G.SubramanianiswiththeDepartmentofElectricalEngineeringand
be inappropriate for many applications. If the protocol gives
Computer Science, University of Michigan, Ann Arbor, MI 48109, USA
vgsubram@umich.edu more informed agents the power to commit to a strategy for
1
4202
raM
81
]TG.sc[
1v40221.3042:viXrathe whole time horizon at the beginning of time, then the Notation: We use capital letters to represent random
more informed agent can implement punishment strategies variables, bold capital letters to denote random vectors,
by threatening to withhold information if less informed and lower case letters to represent realizations. We use
agents do not obey their “instructions”—see Example 1. superscripts to indicate teams and agents, and subscripts to
Thus, the informed agents could abuse their commitment indicate time. We use t : t to indicate the collection of
1 2
power to implement otherwise non-credible threats instead timestamps (t ,t + 1,··· ,t ). For example X1 stands
1 1 2 1:3
of using it for efficient information disclosure. This is not a for the random vector (X1,X1,X1). For random variables
1 2 3
desirableoutcome—forexample,onlinemapservicesshould or random vectors, we use the corresponding script capital
notthreatentowithholdserviceifadriverrefusestotakethe letters (italic capital letters for greek letters) to denote the
recommended route; and similarly, public health authorities space ofvalues theserandom vectorscan take.For example,
may want to use persuasion instead of threats to encourage Hi denotes the space of values the random vector Hi
t t
mask wearing during an epidemic. Again, focusing on the can take. We use P(·) and E[·] to denote probabilities and
public health setting, during an epidemic the authorities expectations,respectively.Weuse∆(Ω)todenotethesetof
may not know the full extent of the disease spread, but probability distributions on a finite set Ω.
only an estimate of it (using testing and other methods).
A. A Motivating Example
In this context, transparency to the public on the part of the
authorities—in disclosing measurement methods and data— The following is an example where given the power to
is important for persuasion based schemes to be effective. commit to a strategy for the whole game before the game
Inthiswork,wefocusonthedynamicinformationdesign starts (i.e. the Stackelberg game setting), the principal can
problem. Specifically, we consider a dynamic game between use otherwise non-credible threats.
aprincipalandareceiverontopofaMarkoviansystem.Both
Example 1. Consider a two-stage game of two players: the
theprincipalandthereceiverhavelong-termobjectives.The principalP,andtheagent/receiverR.Thestateofthesystem
principalcannotdirectly(andperfectly)observe1 thesystem at time t is X . The states are uncontrolled, and X ,X
t 1 2
state,butcanchooserandomizedexperimentstopartiallyob- are i.i.d. uniform random variables taking values in {0,1}.
servethesystem.Theprincipalisalsoallowedtochooseany The principal can observe X at time t while the receiver
t
experiment, but they must announce the experimental setup cannot. At stage t, The principal transmits message M to
t
andresultstruthfullytothereceiverbeforethereceivertakes thereceiverandthereceivertakesanactionU ∈{a,b,c,d}.
t
their action. Both these aspects of our model are motivated
The instantaneous payoff for both players are given by
by the public health setting described earlier. The receiver
takes action on each turn based on the information received r 1A(0,a)=1,r 1A(0,b)=1.01,r 1A(0,c)=r 1A(0,d)=−1000
to date, which then influences the underlying system. rA(1,c)=1,rA(1,d)=1.01,rA(1,a)=rA(1,b)=−1000
1 1 1 1
Contributions: In the class of dynamic information dis- rB(0,a)=500,rB(0,b)=1,rB(0,c)=rB(0,d)=−1000
closure games among a principal and a receiver discussed 1 1 1 1
rB(1,c)=500,rB(1,d)=1,rB(1,a)=rB(1,b)=−1000
above, under the assumption of truthful disclosure, we iden- 1 1 1 1
tify compression based strategies, called Canonical Belief and rA(·,·)=rB(·,·)=rA(·,·).
2 2 1
Based (CBB) strategies, for both players to play at equilib- Suppose that the principal has the power to commit to a
rium.Herebothagentsusestrategiesbasedonacompressed strategy (g ,g ) at the beginning of the game. Then, (given
1 2
versionoftheirinformation,ratherthanthefullinformation, the Stackelberg setting) an optimal strategy for the principal
to choose their actions—experiments (for the principal), and is the following: fully reveal the state at t = 1 (i.e. M =
1
actions (for the receiver). We develop a backward inductive X ); if the receiver plays a or c at t = 1, then transmit no
1
sequentialdecompositionproceduretofindsuchequilibrium informationatt=2;andifthereceiverplaysbordatt=1,
strategies,andweshowthattheprocedurealwayshasatleast then fully reveal the state at t=2. Then, the receiver’s best
onesolution.Finally,weinvestigateexamplesofsuchgames responsetotheprincipal’sstrategyisthefollowing:att=1:
to provide insight to CBB-strategy-based equilibria. play b if M =0, and play d if M =1; and at time t=2:
1 1
Organization: The rest of this work is organized as play a if M =0, and play c if M =1.
2 2
follows: In Section I-A, we provide an example where a In the resulting equilibrium, the principal effectively
principalcanabuseitscommitmentpower.Weformulatethe threatensthereceivertocomplytotheirinterestattimet=1
problem in Section II. Then, we provide some preliminary by not giving information at time t = 2, even though the
results in discrete geometry in Section III. In Section IV we interestsofbothpartiesarealignedatt=2.Infact,without
stateourmainresults.InSectionVwestudysomeexamples. posingathreattothereceiverattime2,theprincipalcannot
Wediscussdifficultieswithpotentialextensionsofourresult convince them to play b or d at time 1.
in Section VI. Finally, we conclude in Section VII. Details
II. PROBLEMFORMULATION
of all the technical proofs are in the Appendix.
We consider a finite-horizon two players dynamic game
1Generalizingtodirectandperfectobservationsistechnicallychallenging betweentheprincipalAandtheagent/receiverB.Thegame
as existence of Nash equilibria and sequential refinements thereof, with
consists of T stages, where, in each stage, the principal
or without compressing information, for infinite state or action dynamic
games[26]isnon-trivialormaynotholdingreatgenerality. moves before the receiver. The game features an underlying
2dynamic system with state X . At each time t ∈ [T], the III. BACKGROUND:DISCRETEGEOMETRY
t
receiver chooses an action U . Then, the system transits to
t Inthissection,weintroducesomenotationsandresultsof
the next state X ∼ P (X ,U ), where P : X ×U (cid:55)→
t+1 t t t t t t discrete geometry that are necessary for our main results.
∆(X ) is the transition kernel. The initial state X has
t+1 1
prior distribution πˆ ∈ ∆(X ). The initial distribution πˆ and Definition 2. Letf beareal-valuedfunctiononapolytope3
1
transition kernels P = (P )T are common knowledge to Ω.Then,f iscalleda(continuous)piecewiselinearfunction
t t=1
both players. We assume that neither player can observe if there exist polytopes C 1,··· ,C k such that
the state X t directly. However, at each time t, the principal • f is linear on each C j for j =1,··· ,k; and
can conduct an experiment to learn about X t. In this work, • C 1∪···∪C k =Ω.
an experiment2 refers to an observation kernel that can
Lemma 1. Let Ω ,Ω be polytopes. Let ℓ : Ω (cid:55)→ Ω be
be chosen by the principal. We impose the rule that the 1 2 1 2
an affine function and f : Ω (cid:55)→ R be a piecewise linear
experiments are required to be public—both the principal 2
function. Then the composite function f ◦ℓ : Ω (cid:55)→ R is
and the receiver know the settings (the probabilities in the 1
piecewise linear.
observation kernel), and the outcome (the observation itself)
of the experiment. Specifically, at each time t, the principal Proof. See Appendix A.
chooses an observation kernel σ : X (cid:55)→ ∆(M ), and
t t t
announces σ to the receiver. The experiment outcome M Next, we introduce the notion of a triangulation.
t t
is then realized, and observed by both the principal and
Definition 3. [30]LetΩbeafinitedimensionalpolytope.A
the receiver. Note that if the horizon T = 1, then the
triangulation γ of Ω is a finite collection of simplices (i.e.
above setting is the same as the classical information design
convex hulls of a finite, affinely independent set of points)
problem considered in [2].
such that
Assumption 1. X ,U ,M are finite sets with |M | suffi- (1) If a simplex C ∈γ, then all faces of C are in γ;
t t t t
ciently large. (2) For any two simplices C 1,C 2 ∈ γ, C 1 ∩ C 2 is a
(possibly empty) face of C ; and
1
The order of events happening at time t is given as the (3) The union of all simplices in γ equals Ω.
following: (1) The principal commits to an experiment σ ,
t
andannouncesittothereceiver;(2)Themeasurementresult
M is revealed to both the principal and receiver; (3) The
t
receivertakesactionU ;and(4)X transitstothenextstate.
t t
Let S be the space of experiments. The principal uses a
t
(pure) strategy to choose their experiment gA : S ×
t 1:t−1 Fig. 1. Left: 2-D Polytope Ω; Center: A triangulation; Right: NOT a
M 1:t−1 × U 1:t−1 (cid:55)→ S t. For convenience, define H tA = triangulation.
S ×M ×U .Thereceiverusesa(pure)strategy
1:t−1 1:t−1 1:t−1
g tB : S 1:t ×M 1:t ×U 1:t−1 (cid:55)→ U t. For convenience, define Forafunctionf :Ω(cid:55)→Randatriangulationγ,letI(f,γ)
H tB = S 1:t × M 1:t × (cid:104)U 1:t−1. The princ (cid:105)ipal’s goal is to denotethelinearinterpolationoff basedonthetriangulation
maximize JA(g) = Eg (cid:80)T rA(X ,U ) . The receiver’s γ, i.e.
t=1 t t t
(cid:104) (cid:105)
goal is to maximize JB(g) = Eg (cid:80)T rB(X ,U ) . The I(f,γ)(ω):=α f(ω )+···+α f(ω ),
t=1 t t t 1 1 k k
instantaneous reward functions (rA,rB)T are common
t t t=1 if ω ∈ C, where C ∈ γ is a simplex with vertices
knowledge to both agents.
ω ,··· ,ω , and ω = α ω + ··· + α ω for some
1 k 1 1 k k
The belief of the principal at time t is a function µA t : α 1,··· ,α k ≥0 such that α 1+···+α k =1.
M × S × U (cid:55)→ ∆(X ). The belief of the
1:t−1 1:t−1 1:t−1 1:t
receiver at time t (after knowing σ and observing M ) is a
t t
function µB :M ×S ×U (cid:55)→∆(X ).
t 1:t 1:t 1:t−1 1:t
Inspired by the “mechanism picking game” defined in
[29], we call the above game a signal picking game, and
we will study Perfect Bayesian Equilibria for our game.
Definition 1 (PBE). A Perfect Bayesian Equilibrium is a
Fig.2. Left:Atriangulationγ labeledwiththevaluesofafunctionf on
pair (g,µ), where thevertices.Right:3-DplotofI(f,γ).
• g is sequentially rational given µ (=(cid:0) µA 1:T,µB 1:T(cid:1) ).
• µ can be updated using Bayes law whenever the de- Lemma 2. For any real-valued function f on a polytope
nominator is non-zero. Ω, I(f,γ) is a well-defined, continuous piecewise linear
function.
2Informationdesignliterature[27],[28]deemssuchexperimentssignals. 3ApolytopeisaconvexhullofafinitesetinRd whered<+∞.
3Proof. See Appendix A. Notethatduetotheassumptionofpublicexperiments,the
signalpickinggameisagamewithsymmetricinformation4
For each ω ∈ Ω and triangulation γ, we have shown after each experiment is conducted. The principal’s advan-
that there exists a unique way to represent ω as a convex tages lies in the fact that they have the power to determine
combination of the vertices of one simplex from γ. One can thechoiceofexperiments.Thus,standardresultsonstrategy-
treat this convex combination as a finite measure. Denote independence of beliefs (e.g. [31]) imply that the beliefs of
this finite measure by C(ω,γ). Then we have I(f,γ)(ω) = both players in this game are strategy-independent, i.e. there
(cid:82) f(·)dC(ω,γ).
is a canonical belief system. Similar strategy-independent
belief systems are also constructed and used in [32]. We
Definition 4. Let f be a real-valued function on Ω. The
describe this belief system as follows.
concave closure cav(f) of f is defined as a function ρ such
that Definition 5. Define the Bayesian update function ξ :
t
∆(X )×S ×M (cid:55)→∆(X ) by setting for each x ∈X
ρ(ω):=sup{z :(ω,z)∈cvxg(f)} ∀ω ∈Ω t t t t t t
π (x )σ (m |x )
where cvxg(f)⊂Ω×R is the convex hull of the graph of ξ t(x t|π t,σ t,m t):= (cid:80) t πt (x˜t )σ (t mt |x˜ )
f.
x˜t t t t t t
for all (π ,σ ,m ) such that the denominator is non-zero.
t t t
For certain functions f, their concave closures can be When the denominator is zero, ξ (π ,σ ,m ) is defined to
t t t t
represented as a triangulation based interpolation of the be the uniform distribution.
original function. Define the set of all such triangulations
Definition 6. The canonical belief system is a collection
as argcav(f), i.e.
of functions (κA,κB) ,κi : Hi (cid:55)→ ∆(X ),i ∈ {A,B}
t t t∈T t t t
argcav(f):={γ is a triangulation of Ω:I(f,γ)=cav(f)}. defined recursively through the following step. Denote πi =
t
κi(hi),i∈{A,B},t∈T, and then we have
t t
• π 1A :=πˆ, the prior distribution of X 1;
• π tB :=ξ t(π tA,σ t,m t);
• π tA +1 :=ℓ t(π tB,u t),whereℓ t :∆(X t)×U t (cid:55)→∆(X t+1)
is defined by
(cid:88)
ℓ (π ,u )(x ):= π (x˜ )P (x |x˜ ,u ).
t t t t+1 t t t t+1 t t
x˜t
We consider a subclass of strategies for both the principal
and the receiver, called canonical belief based (CBB) strate-
gies,whereinplayer i∈{A,B}choosestheirexperimentor
action, respectively, at time t based solely on Πi = κi(Hi)
t t t
insteadofHi.LetλA :∆(X )(cid:55)→S betheCBBstrategyof
t t t t
the principal, and λB :∆(X )(cid:55)→U be the CBB strategy of
t t t
Fig. 3. Top-left: 3-D plot of a function f (an upper semi-continuous thereceiver.Then,sayingthatplayeriisusingCBBstrategy
piecewiseconstantfunctiontakingvaluesin{0,1,2}).Top-right:Concave λi is equivalent to saying that they are using the strategy
closure of f. Bottom-left and bottom-right: 2-D visualization of two t
differenttriangulationsinargcav(f). gi(hi)=λi(κi(hi)), ∀hi ∈Hi.
t t t t t t t
The following lemma identifies a class of functions with Given an experiment and a distribution on the state, the
the above property. posterior belief of the receiver is a random variable (a func-
tion of the random outcome). In an information disclosure
Lemma 3. Let f ,··· ,f ,ρ ,··· ,ρ be continuous piece-
1 k 1 k game, it is helpful to consider the following sub-problem:
wise linear functions on a polytope Ω. For ω ∈Ω, define
how to design an experiment such that the receiver’s belief,
Υ(ω)=argmaxf (ω), and Ψ(ω)= max ρ (ω). asarandomvariable,followsacertaindistribution.Thenext
j j
j=1,···,k j∈Υ(ω) definition formalizes this concept. This concept was used in
classical one-shot information design setting [2] as well.
Then argcav(Ψ) is non-empty, i.e. there exists a triangula-
tion γ of Ω such that the concave closure of Ψ is equal to Definition 7. [2] An experiment σ ∈ S is said to induce
t t
I(Ψ,γ). a distribution η ∈ ∆ (∆(X ))—that is, η is a distribution
f t
with finite support on the set of distributions ∆(X )—from
Proof. See Appendix A. t
π ∈∆(X ) [2] if for all π˜ ∈∆(X ),
t t t t
(cid:88) (cid:88)
IV. MAINRESULTS η(π˜ )= 1 σ (m˜ |x˜ )π (x˜ ).
t {π˜t=ξt(πt,σt,m˜t)} t t t t t
In this section, we introduce our main result—Theorem m˜t x˜t
1—, which provides a dynamic programming characteriza-
4Asmentionedearlier—inFootnote1—,therearesignificantchallenges
tion of a subset of PBE of the signal picking game. isgeneralizingtoasymmetricinformationsettings.
4A distribution η is said to be inducible from π if there InductionStep:Supposethattheinductioninvariantholds
t
exists some experiment σ that induces η from π . for t+1.
t t
Remark 1. In [2], the authors showed that a distribution is • Step 1: For each u t ∈U t, using the fact that ℓ t(π t,u t)
η ∈∆ f(∆(X t)) is inducible from π
t
if and only if π
t
is the is affine in π t, by Lemma 1, q tA,q tB are continuous
center of mass of η, i.e. π t =(cid:80) π˜t∈supp(η)η(π˜ t)·π˜ t.
•
p Si te ec pew 2:is Be yli Lne ea mr mfu an 3ct ,io γn ts isin wπ et ll.
-defined.
We now introduce our main result, which describes a • Step3:ByLemma2,V tA,V tB arecontinuouspiecewise
backward induction procedure to find a PBE where both
linear functions.
players use CBB strategies.
This completes the proof.
Theorem 1. Let
VA (·)=VB (·):=0 A. Extension
T+1 T+1
In many real-world settings, the receivers have the option
For each t=T,T −1,··· ,1 and π ∈∆(X ), define
t t
to quit the game at any time. Our model and results can
(cid:88)
qˆi(π ,u ):= ri(x˜ ,u )π (x˜ )+Vi (ℓ (π ,u )) be extended to finite horizon games where the receiver can
t t t t t t t t t+1 t t t
x˜t decide to terminate the game at any time before time T.
∀i∈{A,B}; (1a)
Proposition 2. Let U ⊂ U be the set of actions that
t t
Υ t(π t):=argmax qˆ tB(π t,u t); (1b) terminates the game at time t. If we define V ti,q ti,λ∗ ti for
ut
each i∈{A,B},t∈T as in (1) except that (1a) is changed
vˆA(π ):= max qˆA(π ,u ); (1c)
t t t t t to
ut∈Υ(πt)
vˆ tB(π t):=m ua txqˆ tB(π t,u t); (1d) qˆ ti(π t,u t)
(cid:40)
γ t ∈argcav(vˆ tA); (1e) :=(cid:88) ri(x˜ ,u )π (x˜ )+ V ti +1(ℓ t(π t,u t)) if u t ̸∈U t
Vi(π ):=I(vˆi,γ ) ∀i∈{A,B}. (1f) t t t t t 0 if u ∈U
t t t t x˜t t t
Let λ∗B(π ) be any u ∈ U that attains the maximum
t t t t for i ∈ {A,B}. Then the CBB strategies (λ∗A,λ∗B) form
in (1c). Let λ∗A(π ) be any experiment that induces the
t t (the strategy part of) a PBE, and VA(πˆ) and VB(πˆ) are
finite measure C(π ,γ ) from π . Then, the CBB strategies 1 1
t t t the equilibrium payoff for the principal and the receiver
(λ∗A,λ∗B) form (the strategy part of) a PBE, and VA(πˆ)
1 respectively in this PBE.
andVB(πˆ)aretheequilibriumpayoffsfortheprincipaland
1
the receiver respectively in this PBE. Proof. Similar to Theorem 1.
Proof Outline. In Lemma 4 in Appendix B we construct a V. EXAMPLES
belief system µ∗ that is consistent with any strategy profile.
Hence, we only need to show sequential rationality of λ∗. We implement the sequential decomposition algorithm of
To show the receiver’s sequential rationality, we prove Proposition2inMATLABforbinarystatespaces(i.e.|X t|=
the following: Fixing the principal’s strategy to be λ∗A, the 2). We run the algorithm on the following examples of the
receiver is facing an MDP with state ΠB and action U . The signal picking game.
t t
proofthenfollowsviastandardstochasticcontrolarguments.
Example 2. Consider the quickest detection game defined
To show the principal’s sequential rationality, we prove
in [24]. In this game, the underlying state X is binary and
the following: Fixing the receiver’s strategy to be λ∗B, the t
uncontrolled,withX ={1,2}.State2isanabsorbingstate,
principal is facing an MDP with state ΠA t and action Σ t. i.e. P(X = 2 | Xt = 2) = 1, whereas the system can
t+1 t
This proof follows cia standard stochastic control arguments
jump from state 1 to state 2 at any time with probability p,
coupled with information design results [2]. i.e. P(X =2 | X =1)=p where p∈(0,1).
t+1 t
The detailsof the proofare presented inAppendix B.
The receiver would like to detect (the epoch of) the jump
The following proposition states that the sequential de- fromstate1tostate2asaccuratelyaspossible.Ateachtime
composition procedure described in Theorem 1 is well de- the receiver has two options: U t = j stands for declaring
fined and always has a solution. statej forj =1,2.Theinstantaneousrewardofthereceiver
is given by
Proposition 1. There always exists a CBB strategy profile
(λ∗A,λ∗B) that satisfies Eqs. (1) in Theorem 1. 
−1 if X =1,U =2
 t t
Proof. Induction on time t is used for the proof. rB(X ,U )= −c if X =2,U =1
t t t t t
Induction Invariant: V tA,V tB are well-defined continu- 0 otherwise
ous piecewise linear functions.
Induction Base: The induction variant is clearly true for wherec∈(0,1).Oncethereceiverdeclaresstate2,thegame
t=T +1 since VA ,VB are constant functions. ends immediately.
T+1 T+1
5The principal would like the receiver to stay in the receiver for c∈(0,1) is given by
system as long as possible. The instantaneous reward for 
1 if X =U
the principal is  t t
rB(X ,U )= −c if U =0 .
(cid:40) t t t t
rA(X ,U )= 1 if U t =1 0 otherwise
t t t 0 otherwise
The principal would like the receiver to stay in the
system as long as possible. The instantaneous reward for
Setting p = 0.2,c = 0.1, we obtained the qB and VA
t t the principal is
functions specified in Proposition 2 in Figure 4. The hori-
(cid:40)
zontalaxisrepresentsπ t(1).InthefiguresforV tA functions,
rA(X ,U )=
1 if U
t
=0
the vertices of the triangulation γ t are labeled. The vertices t t t 0 otherwise
represent the set of beliefs that the principal could induce,
and they completely describe the principal’s CBB strategy. Setting p = 0.2 and c = 0.15, we obtained the qB and
t
Ifthevertexislabeledwithredcircles,thereceiverwilltake VA functions specified in Proposition 2—see Figure 5. The
t
action U = 1 at this posterior belief. If, instead, the vertex horizontal axis represents π (−1). The figures follows the
t t
is labeled with blue triangles, the receiver will take action sameinterpretationasthefiguresinExample2.(Themarkers
U =2 at this posterior belief. for actions are different from previous figures, but they are
t
From the figures, one can see that at any stage, there self-explanatory.)
is only one possible belief that the principal would induce Different from Example 2, the value functions and CBB
which leads to the receiver quitting the game (i.e. select strategies at equilibrium oscillate with a period of 4 (given
U = 2). This is consistent with the principal’s objective p = 0.2,c = 0.15) instead of converging as t gets further
t
of keeping the receiver in the system. Just like in static away from the horizon T.
information design problems [2], [33], when it is better off
VI. DISCUSSION
forthereceivertodeclarechange,i.e.,quit,underthecurrent
belief, the principal would promise to tell the receiver that Naturally, one may consider extending the above result
the state is 2 with some probability p˜ when the state is to two settings: (a) when a public noisy observation of the
indeed 2, and tell the receiver nothing otherwise. In doing state is available in addition to the principal’s experiment,
so,thereceiverwouldbelievethatthestateis1withahigher (b) when there are multiple receivers. However, our result
probability when the principal does not tell the receiver is immediately extendable to neither setting. This is since
anything. The principal chooses p˜to be precisely the value the techniques we use in this paper depend heavily on the
for which the receiver is willing to stay in the system [2]. piecewise linear structure of qˆ and V-functions in (1), as
When t is close to T, the end of the game, the principal well as the preservation of this piecewise linear structure
wouldonlyprefertodeclarestate2iftheybelievethatπ (1) under backward induction. Specifically, when the functions
t
is very small. This is due to the fact that “false alarms” qˆA,qˆB are piecewise linear, the concave closure of vˆA can
t t t
are costlier than delayed detection in this game. When t is be expressed as a triangulation based interpolation (through
furtherawayfromT,thethresholdofπ (1)fortheprincipal Lemma 3), which in turn allows us to apply the same
t
to declare state 2 becomes larger. This holds because when triangulation to vˆB, and thus ensuring the continuity and
t
the game is close to end, the receiver has the “safe” option piecewise linearity of VB. However, this structure does not
t
to declare state 1 (at a small cost) until the end to avoid appear in general in the extensions.
false alarms (which are costly). However, this option is less We describe an attempt to extend Theorem 1 to settings
preferable when the gap between t and T is large. (a) and (b) in the most straightforward way. In the case of
When t is further away from T, the principal’s value setting(a),oneneedstochangethebeliefupdatein(1a)from
function seems to converge. This is due to the fact that the ℓ t(π t,u t) to some other update function that incorporates
receiver has the option to quit the game and staying in the the public observation. However, unlike ℓ t(π t,u t), the new
game is costly in general. update function may not be linear in π t. Therefore this
procedure cannot preserve piecewise linear properties.
Example 3. Consider a game between a principal and a
In the case of setting (b), u will represent a vector
t
detector. In this game, the underlying state X is binary and
t of actions of all receivers, and one needs to change the
uncontrolled with X = {−1,1}. At any time, the system
t definition of Υ (π ) in (1b) to be the set of mixed strategy
t t
can jump to the other state with probability p∈(0,1), i.e.
Nash equilibrium (or alternatively correlated equilibrium)
action profiles of the following stage game: Receiver i
P(X =−j | X =j)=p, ∀j ∈{−1,1}.
t+1 t chooses an action in Ui, and receives payoff qˆi(π ,u ). In
t t t t
this setting, Υ (π ) is a set of probability measures on the
Thereceiverhasthreeactions:U =j standsfordeclaring t t
t product set U . The new vˆA function can then be given by
statej forj =−1,1.BothU =−1andU =+1terminate t t
t t
(cid:88)
the game. In addition, the receiver can choose to wait at a vˆA(π )= argmax qA(π ,u˜ )η (u˜ )
t t t t t t t
cost with action U t = 0. The instantaneous reward of the ηt∈Υt(πt) u˜t
6However, in this case, continuity and piecewise linearity [5] A. Pavan, I. Segal, and J. Toikka, “Dynamic mechanism design: A
of qˆ are not enough to ensure that the value function Myersonian approach,” Econometrica, vol. 82, no. 2, pp. 601–653,
t
VA possesses the same property. To see this, consider the 2014.
t [6] D. Bergemann and J. Va¨lima¨ki, “Dynamic mechanism design: An
following example with two receivers B and C. Let U tB = introduction,”JournalofEconomicLiterature,vol.57,no.2,pp.235–
UC = {1,2} = X = {1,2}. Let p = π (1). Then, all 74,2019.
t t t
functionsofπ canbeexpressedasafunctionofp.Suppose [7] D.LingenbrinkandK.Iyer,“Optimalsignalingmechanismsinunob-
t servablequeueswithstrategiccustomers,”inProceedingsofthe2017
that
ACMConferenceonEconomicsandComputation,2017,pp.347–347.
(cid:40) [8] J. C. Ely, “Beeps,” American Economic Review, vol. 107, no. 1, pp.
1 if uB =uC
qB(π ,u )= t t , 31–53,2017.
t t t 0 otherwise [9] F.Farokhi,A.M.Teixeira,andC.Langbort,“Estimationwithstrategic
sensors,”IEEETransactionsonAutomaticControl,vol.62,no.2,pp.

p+1 if uB =1,uC =2 724–739,2016.
qC(π ,u
)=
1 if
ut
B
=2,ut
C =1. [10] M.O.Sayin,E.Akyol,andT.Bas¸ar,“Strategiccontrolofatracking
t t t t t system,” in 2016 IEEE 55th Conference on Decision and Control
0 otherwise (CDC). IEEE,2016,pp.6147–6153.
[11] J. Renault, E. Solan, and N. Vieille, “Optimal dynamic information
It can be verified that, under either the concept of Nash provision,” Games and Economic Behavior, vol. 104, pp. 329–349,
2017.
equilibrium or correlated equilibrium, Υ (π ) contains only
t t [12] J. Best and D. Quigley, “Honestly dishonest: A solution to the
one element: player B plays action 1 with probability 1 commitment problem in Bayesian persuasion,” Mimeo, Tech. Rep.,
2+p
and player C plays their two actions with equal probability 2016.
[13] ——, “Persuasion for the long-run,” Economics Group, Nuffield
independent of player B’s action. Now suppose that
College,UniversityofOxford,Tech.Rep.,2016.
(cid:40) [14] P. B. Luh, S.-C. Chang, and T.-S. Chang, “Solutions and properties
p uB =1
qA(π ,u )= t . of multi-stage Stackelberg games,” Automatica, vol. 20, no. 2, pp.
t t t 0 otherwise 251–256, 1984. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/0005109884900347
Then we have vˆA(π ) = p for p ∈ [0,1]. Observe that [15] B. Tolwinski, “A Stackelberg equilibrium for continuous-time differ-
t t 2+p entialgames,”inThe22ndIEEEConferenceonDecisionandControl,
vˆA is a strictly concave function. Hence the concave closure 1983,pp.675–681.
t
of vˆA is just vˆA itself, which is not piecewise linear. [16] ——, “Closed-loop Stackelberg solution to a multistage linear-
t t quadratic game,” Journal of Optimization Theory and Applications,
vol.34,no.4,pp.485–501,1981.
VII. CONCLUSIONANDFUTUREWORK
[17] F. Farhadi, D. Teneketzis, and S. J. Golestani, “Static and dynamic
informational incentive mechanisms for security enhancement,” in
In this work, we formulated a dynamic information dis-
2018EuropeanControlConference(ECC). IEEE,2018,pp.1048–
closure game, called the signal picking game, where the 1055.
principalsequentiallycommittoasignal/experimenttocom- [18] L.Li,O.Massicot,andC.Langbort,“Sequentialpublicsignalingin
municate with the receiver. We showed that there exist routinggameswithfeedbackinformation,”in2018IEEEConference
onDecisionandControl(CDC),2018,pp.2735–2740.
equilibria where both the principal and the receiver make
[19] M. O. Sayin and T. Bas¸ar, “Dynamic information disclosure for
decisions based on the canonical belief instead of their deception,”in2018IEEEConferenceonDecisionandControl(CDC).
respective full information. We also provided a sequential IEEE,2018,pp.1110–1117.
[20] M.O.Sayin,E.Akyol,andT.Bas¸ar,“HierarchicalmultistageGaus-
decomposition procedure to find such equilibria.
sian signaling games in noncooperative communication and control
Unlike the CIB-belief-based sequential decomposition systems,”Automatica,vol.107,pp.9–20,2019.
procedures of [34], [35], [36], [37], the sequential decom- [21] M. O. Sayin and T. Bas¸ar, “On the optimality of linear signaling to
deceive Kalman filters over finite/infinite horizons,” in International
position procedure of Theorem 1 always has a solution. The
Conference on Decision and Game Theory for Security. Springer,
mainreasonisthattheCIBbeliefinthesignalpickinggame 2019,pp.459–478.
is strategy-independent, just like in [32]. The result illus- [22] E. Meigs, F. Parise, A. Ozdaglar, and D. Acemoglu, “Optimal
trates the critical difference between strategy-dependent and dynamic information provision in traffic routing,” arXiv preprint
arXiv:2001.03232,2020.
strategy-independentcompressionofinformationindynamic
[23] H. Tavafoghi and D. Teneketzis, “Informational incentives for con-
games. gestion games,” in 2017 55th Annual Allerton Conference on Com-
Thereareafewfutureresearchproblemsarisingfromthis munication, Control, and Computing (Allerton). IEEE, 2017, pp.
1285–1292.
work. The first problem is to extend our result to infinite
[24] F. Farhadi and D. Teneketzis, “Dynamic information design: A sim-
horizon games. The second problem is to extend our result ple problem on optimal sequential information disclosure,” Dynamic
to settings with multiple senders. GamesandApplications,vol.12,no.2,pp.443–484,2022.
[25] S.HeinrichVon,“Marketstructureandequilibrium,”2011.
REFERENCES [26] R. B. Myerson and P. J. Reny, “Perfect conditional ε-equilibria of
multi-stage games with infinite sets of signals and actions,” Econo-
[1] R.B.Myerson,“Mechanismdesign,”inAllocation,Informationand metrica,vol.88,no.2,pp.495–531,2020.
Markets. Springer,1989,pp.191–206. [27] M. Gentzkow and E. Kamenica, “Bayesian persuasion with multiple
[2] E. Kamenica and M. Gentzkow, “Bayesian persuasion,” American sendersandrichsignalspaces,”GamesandEconomicBehavior,vol.
EconomicReview,vol.101,no.6,pp.2590–2615,2011. 104,pp.411–429,2017.
[3] D. Bergemann and J. Va¨lima¨ki, “The dynamic pivot mechanism,” [28] ——, “Disclosure of endogenous information,” Economic Theory
Econometrica,vol.78,no.2,pp.771–789,2010. Bulletin,vol.5,no.1,pp.47–56,2017.
[4] S.AtheyandI.Segal,“Anefficientdynamicmechanism,”Economet- [29] L. Doval and V. Skreta, “Mechanism design with limited commit-
rica,vol.81,no.6,pp.2463–2485,2013. ment,”arXivpreprintarXiv:1811.03579,2018.
7[30] J.DeLoera,J.Rambau,andF.Santos,Triangulations:Structuresfor
algorithms and applications. Springer Science & Business Media,
2010,vol.25.
[31] P. R. Kumar and P. Varaiya, Stochastic systems: Estimation, identifi-
cationandadaptivecontrol. Prentice-Hall,Inc.,1986.
[32] A. Nayyar, A. Gupta, C. Langbort, and T. Bas¸ar, “Common
information based Markov perfect equilibria for stochastic games
with asymmetric information: Finite games,” IEEE Transactions on
Automatic Control, vol. 59, no. 3, pp. 555–570, 2013. [Online].
Available:https://doi.org/10.1109/tac.2013.2283743
[33] D.BergemannandS.Morris,“Informationdesign:Aunifiedperspec-
tive,”JournalofEconomicLiterature,vol.57,no.1,pp.44–95,2019.
[34] Y. Ouyang, H. Tavafoghi, and D. Teneketzis, “Dynamic games
with asymmetric information: Common information based perfect
Bayesianequilibriaandsequentialdecomposition,”IEEETransactions
on Automatic Control, vol. 62, no. 1, pp. 222–237, 2016. [Online].
Available:https://doi.org/10.1109/tac.2016.2544936
[35] H.Tavafoghi,Y.Ouyang,andD.Teneketzis,“Onstochasticdynamic
gameswithdelayedsharinginformationstructure,”in2016IEEE55th
ConferenceonDecisionandControl(CDC). IEEE,2016,pp.7002–
7009.[Online].Available:https://doi.org/10.1109/cdc.2016.7799348
[36] D.Vasal,A.Sinha,andA.Anastasopoulos,“Asystematicprocessfor
evaluating structured perfect Bayesian equilibria in dynamic games
with asymmetric information,” IEEE Transactions on Automatic
Control, vol. 64, no. 1, pp. 81–96, 2019. [Online]. Available:
https://doi.org/10.1109/tac.2018.2809863
[37] D.Tang,H.Tavafoghi,V.Subramanian,A.Nayyar,andD.Teneketzis,
“Dynamic games among teams with delayed intra-team information
sharing,”DynamicGamesandApplications,2022.[Online].Available:
https://doi.org/10.1007/s13235-022-00424-4
80
qTB
1
VTA
-0.5 u=1 0.5 1
qTB
1
VTA
-1 u=2 0 0.5 u u= =- +1 1 0
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 u=0
0 qTB -1 2 VTA -1 0 0.2 0.4 0.6 0.8 1 -1 0 0.2 0.4 0.6 0.8 1
-0.5 1 1
qTB
-1 1
VTA
-1
-1 0 0.5 0.5
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0
qTB
-2 3
VTA
-2
0
0 0.2 0.4 0.6 0.8 1
0
0 0.2 0.4 0.6 0.8 1
-0.5 2 1
qTB
-2 2
VTA
-2
1
-1 0 0.5 1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0
qTB
-3 4
VTA
-3
0
0 0.2 0.4 0.6 0.8 1
0
0 0.2 0.4 0.6 0.8 1
-0.5 2 1
qTB
-3 3
VTA
-3
2
-1 0 0.5
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 1
0
qTB
-4 5
VTA
-4
0
0 0.2 0.4 0.6 0.8 1
0
0 0.2 0.4 0.6 0.8 1
-0.5 1
qTB
-4 1
VTA
-4
-1 0 0.5 0
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0
qTB
-5
VTA
-5
0
0 0.2 0.4 0.6 0.8 1
-1
0 0.2 0.4 0.6 0.8 1
-0.5
5
1
qTB
-5 1
VTA
-5
-1 0 0.5 0.5
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0
qTB
-6
VTA
-6
0
0 0.2 0.4 0.6 0.8 1
0
0 0.2 0.4 0.6 0.8 1
-0.5 5 1 qTB -6 2 VTA -6
-1 0 0.5 1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0
qTB
-7 10
VTA
-7 0 0 0.2 0.4 0.6 0.8 1 0 0 0.2 0.4 0.6 0.8 1
-0.5 5 1
qTB
-7 3
VTA
-7
2
-1 0 0.5
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 1
0
qTB
-8 10
VTA
-8
0
0 0.2 0.4 0.6 0.8 1
0
0 0.2 0.4 0.6 0.8 1
-0.5 5 1
qTB
-8 1
VTA
-8
-1 0 0.5 0
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0
qTB
-9 10
VTA
-9
0
0 0.2 0.4 0.6 0.8 1
-1
0 0.2 0.4 0.6 0.8 1
-0.5 5 1
qTB
-9 1
VTA
-9
-1 0 0.5 0.5
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0 qTB -10 10 VTA -10 0 0 0.2 0.4 0.6 0.8 1 0 0 0.2 0.4 0.6 0.8 1
-0.5 5 1
qTB
-10 2
VTA
-10
-1 0 0.5 1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0 qTB -11 10 VTA -11 0 0 0.2 0.4 0.6 0.8 1 0 0 0.2 0.4 0.6 0.8 1
-0.5 5 1
qTB
-11 3
VTA
-11
2
-1 0 0.2 0.4 0.6 0.8 1 0 0 0.2 0.4 0.6 0.8 1 0.5 1
0 qTB -12 10 VTA -12 0 0 0.2 0.4 0.6 0.8 1 0 0 0.2 0.4 0.6 0.8 1
-0.5 5 1
qTB
-12 1
VTA
-12
-1 0 0.2 0.4 0.6 0.8 1 0 0 0.2 0.4 0.6 0.8 1 0.5 0
0 qTB -13 10 VTA -13 0 0 0.2 0.4 0.6 0.8 1 -1 0 0.2 0.4 0.6 0.8 1
-0.5 5 1
qTB
-13 1
VTA
-13
-1 0 0.2 0.4 0.6 0.8 1 0 0 0.2 0.4 0.6 0.8 1 0.5 0.5
0 0
Fig.4. TheqB andVA functionsforExample2withp=0.2,c=0.1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
t t
attimest=T :T −13.
Fig.5. TheqB andVA functionsforExample3withp=0.2,c=0.15
t t
attimest=T :T −13.
9APPENDIX
A. Proofs of Discrete Geometric Results
Proof of Lemma 1. Let C ,··· ,C be polytopes such that (i) f is linear on each of C ,··· ,C (ii) C ∪···∪C =Ω .
1 k 1 k 1 k 2
Since ℓ is an affine function, we have the pre-images D =ℓ−1(C ),j =1,··· ,k to be polytopes as well.
j j
f ◦ℓ is linear on each D (since it is the composition of two linear functions), and D ∪···∪D = Ω . We conclude
j 1 k 1
that f ◦ℓ is a piecewise linear function.
Proof of Lemma 2. First, for any ω, given a simplex C such that ω ∈C, there is a unique way to represent ω as a convex
combination of vertices of C.
Suppose that ω is in both simplices C and C′. Then ω is in C∩C′, which is a face of both C and C′. Since C∩C′ is a
simplex, ω can be uniquely represented as a convex combination of vertices of C∩C′. Since the set of vertices of C∩C′ is
a subset of the vertices of both C and C′, we conclude that the above representation is also the unique way of representing
ω as a convex combination of vertices of C (and of C′). We conclude that for any ω, there is a unique way to represent ω
as a convex combination of vertices of any simplex in γ. Hence I(f,γ) is well defined.
For any simplex C ∈ γ, I(f,γ) is linear on C. Since the number of simplices in γ is finite and their union is Ω, we
conclude that I(f,γ) is a continuous piecewise linear function on Ω.
Proof of Lemma 3. For j = 1,··· ,k, let C ,··· ,C be polytopes corresponding to the piecewise linear function f
j1 jmj j
under Definition 2, i.e. f is linear on each of C ,··· ,C and C ∪···∪C =Ω. Define
j j1 jmj j1 jmj
S ={C ∩C ∩···∩C :1≤i ≤m ,··· ,1≤i ≤m }
1i1 2i2 kik 1 1 k k
S is a finite collection of polytopes. All of f ,··· ,f are linear on each element of S. The union of S equals Ω. Define
1 k
A :={ω ∈Ω:f (ω)≥f (ω) ∀j′ =1,··· ,k},
j j j′
S ={F ∩A :F ∈S}.
j j
S is the collection of subsets where f is (one of) the maximum among f ,··· ,f . S is also a finite collection of
j j 1 k j
polytopes, since each F ∩A is a subset of F that satisfy certain linear constraints.
j
Similarly, let D ,··· ,D be polytopes corresponding to the piecewise linear function ρ . Define
j1 jnj j
R ={F ∩D :F ∈S ,1≤i≤n }
j ji j j
For each polytope F ∈ R , ρ is linear on F, and f is (one of) the maximum among f ,··· ,f for all points in F.
j j j 1 k
The union of R equals A .
j j
LetP bethesetofverticesofpolytopesinR ∪···∪R .P isafiniteset.DefineB ⊂Ω×RbyB ={(ω,Ψ(ω)):ω ∈P}.
1 k
Let Z be the convex hull of B. We have Z to be a polytope with its vertices contained in B.
Let Ψˆ be the concave closure of Ψ. We will show that the function Ψˆ is represented by the upper face of Z. Then we
obtain a triangulation of Ω by projecting a triangulation of Z in a similar way to the construction of regular triangulations
(see Section 2.2 of [30]).
Step 1: Prove that Ψˆ(ω)=max{y :(ω,y)∈Z}.
Define Ψ(ω):=max{y :(ω,y)∈Z}. Ψ is a concave function.
It is clear that Z ⊂cvxg(Ψ), since B is a subset of the graph of Ψ. Therefore Ψ(ω)≤Ψˆ(ω).
Consider any ω ∈ Ω. Let j∗ be such that j∗ ∈ Υ(ω) and Ψ(ω) = ρ (ω). Then ω ∈ A . We have ω ∈ F for some
j∗ j∗
F ∈ R . Let ω ,··· ,ω ∈ P be the vertices of F. We can write ω = α ω +···+α ω for some α ,··· ,α ≥
j∗ 1 m 1 1 k m 1 m
0,α +···+α =1. Since ρ is linear on F we have
1 m j∗
Ψ(ω)=ρ (ω)=α ρ (ω )+···+α ρ (ω ) (2)
j∗ 1 j∗ 1 k j∗ m
Since ω ,··· ,ω ∈F and F ⊂A . By definition, j∗ ∈Υ(ω ) for all i=1,··· ,m. Therefore ρ (ω )≤Ψ(ω ) for all
1 m j∗ i j∗ i i
i=1,··· ,m. Consequently, combining (2) we have
Ψ(ω)≤α Ψ(ω )+···+α Ψ(ω )
1 1 k m
Given that (ω,α Ψ(ω )+···+α Ψ(ω ))∈Z, we have
1 1 k k
α Ψ(ω )+···+α Ψ(ω )≤Ψ(ω)
1 1 k k
HenceΨ(ω)≤Ψ(ω).Therefore,Ψisaconcavefunctionaboveh.SincetheconcaveclosureΨˆ(ω)isthesmallestconcave
function above h, we conclude that Ψˆ(ω)≤Ψ(ω) for all ω ∈Ω.
Therefore Ψˆ =Ψ, completing the proof of Step 1.
Step 2: Construct the triangulation γ and show that Ψˆ =I(h,γ).
10Let A⊂Ω×R be the graph of Ψˆ. A is also the union of upper faces of Z. Let ϑ be a point set triangulation (as defined
in Def. 2.2.1 in [30]) of the finite point set B. (A point set triangulation of a finite set of points always exists. See Section
2.2.1 of [30].) Let ϑˆ be the restriction of ϑ to A, i.e. ϑˆ := {F : F ⊂ A,F ∈ ϑ}. It can be shown that ϑˆ is a simplicial
complex (i.e. a polyhedral complex where all polytopes are simplices. See Def. 2.1.5 in [30].) with vertices contained in
A∩B.
Since A is the upper convex hull of Z, the projection map proj : Ω×R (cid:55)→ Ω,(ω,y) (cid:55)→ ω is a bijection between A
Ω
and Ω. Let γ be the projection of ϑˆon to Ω, i.e. γ ={proj (F):F ∈ϑˆ}. We conclude that γ is a simplical complex that
Ω
spans Ω, i.e. a triangulation of Ω.
The inverse map proj−1 : Ω (cid:55)→ A is a piecewise linear map that is linear on each simplex in γ. Therefore we have
Ω
Ψˆ(ω)=I(Ψˆ,γ)(ω) for all ω ∈Ω. Since the vertices of ϑˆ are contained in both A and B, we have Ψˆ(ω)=Ψ(ω) for each
vertex ω of γ. (Recall that B is a subset of the graph of h and A is the graph of Ψˆ.) Therefore we have Ψˆ(ω)=I(Ψ,γ)(ω)
for all ω ∈Ω.
B. Proof of Main Results
Lemma 4. There exist a belief system µ∗ = (µ∗A,µ∗B) such that (i) µ∗ is consistent with any strategy profile (gA,gB);
(ii) the canonical belief system κ is the marginals of µ∗.
Proof of Lemma 4. Define µ∗i :Hi (cid:55)→∆(X ) recursively through the following:
t t 1:t
• µ∗ 1A(hA 1):=πˆ.
µ∗A(x |hA)σ (m |x )
• µ∗ tB(x 1:t|hB t ):= (cid:80) t µ∗A1 (:t x˜ t |hAt )σ t (mt |x˜ )
• µ∗ t+A 1(x 1:t+1|hA
t+1):=x˜1 µ:t
∗
tBt
(x
1:t1 |: ht
B t
)t P(xt t+1t
|x
tt
,u t)
Through induction on t it is clear that κ is the marginal distribution derived from µ∗.
It remains to show the consistency of µ∗ w.r.t. any strategy profile g =(gA,gB). We will also show it via induction:
• µ∗ 1A is clearly consistent with any g since it is defined to be the prior distribution of X 1.
• Suppose that µ∗ tA is consistent with g. Then consider any hB t =(σ 1:t,m 1:t,u 1:t−1)∈H tB such that Pg(hB t )>0. Then
we have Pg(hA)>0, and µ∗A(x |hA)=Pg(x |hA) follows by induction hypothesis. Therefore
t t 1:t t 1:t t
Pg(x ,σ ,m ,hA)
Pg(x |hB)=Pg(x |σ ,m ,hA)= 1:t t t t
1:t t 1:t t t t Pg(σ ,m ,hA)
t t t
Pg(x ,hA)Pg(σ |x ,hA)Pg(m |σ ,x ,hA)
= 1:t t t 1:t t t t 1:t t
(cid:80) Pg(x˜ ,hA)Pg(σ |x˜ ,hA)Pg(m |σ ,x˜ ,hA)
x˜1:t 1:t t t 1:t t t t 1:t t
Pg(x ,hA)gA(σ |hA)σ (m |x )
= 1:t t t t t t t t
(cid:80) Pg(x˜ ,hA)gA(σ |hA)σ (m |x˜ )
x˜1:t 1:t t t t t t t t
Pg(x ,hA)σ (m |x ) Pg(x |hA)σ (m |x )
= 1:t t t t t = 1:t t t t t
(cid:80) Pg(x˜ ,hA)σ (m |x˜ ) (cid:80) Pg(x˜ |hA)σ (m |x˜ )
x˜1:t 1:t t t t t x˜1:t 1:t t t t t
µ∗A(x |hA)σ (m |x )
= t 1:t t t t t =µ∗B(x |hB)
(cid:80) µ∗A(x˜ |hA)σ (m |x˜ ) t 1:t t
x˜1:t t 1:t t t t t
which means that µ∗B is consistent with g.
t
• Suppose that µ∗ tB is consistent with g. Then consider any hA t+1 = (σ 1:t,m 1:t,u 1:t) ∈ H tA +1 such that Pg(hA t+1) > 0.
Then we have Pg(hB)>0, and µ∗B(x |hB)=Pg(x |hB) follows by induction hypothesis. Then we have
t t 1:t t 1:t t
Pg(x ,hA )
Pg(x |hA )= 1:t+1 t+1
1:t+1 t+1 Pg(hA )
t+1
Pg(x ,hB)Pg(u |x ,hB)Pg(x |x ,u ,hB)
= 1:t t t 1:t t t+1 1:t t t
(cid:80) Pg(x˜ ,hB)Pg(u |x˜ ,hB)
x˜1:t 1:t t t 1:t t
Pg(x ,hB)gB(u |hB)P(x |x ,u )
= 1:t t t t t t+1 t t
(cid:80) Pg(x˜ ,hB)g (u |hB)
x˜1:t 1:t t t t t
Pg(x ,hB)
= 1:t t ·P(x |x ,u )=µ∗B(x |hB)P(x |x ,u )
(cid:80) Pg(x˜ ,hB) t+1 t t t 1:t t t+1 t t
x˜1:t 1:t t
=µ∗A(x |hA )
t 1:t+1 t+1
which means that µ∗A is consistent with g.
t+1
11Proof of Theorem 1. Let µ∗ be a belief system that satisfies Lemma 4. It is shown by Lemma 4 that µ∗ is consistent with
any strategy profile g. Hence to show that λ∗ forms a CBB-PBE we only need to show sequential rationality.
Step 1: Fixing the principal’s strategy to be λ∗A, show that λ∗B is sequentially rational at any hB ∈HB at any time τ,
τ:T τ τ
given the belief µ∗B(hB).
τ τ
To prove Step 1, we argue that at hB, the receiver is facing an MDP problem with state process ΠB = κB(HB) and
τ t t t
action U for t≥τ.
t
First, we can write
(cid:34) T (cid:35)
Eµ∗ τB(hB τ),λ∗ tA,g τB :T (cid:88) r tB(X t,U t)
t=τ
(cid:34) T (cid:35)
=Eµ∗ τB(hB τ),λ∗ tA,g τB :T (cid:88) Eµ∗ τB(hB τ),λ∗ tA,g τB :T[r tB(X t,U t)|H tB,U t]
t=τ
where for any hB
t
such that Pµ∗ τB(hB τ),λ∗ tA,g τB :T(hB
t
)>0 we have
Eµ∗ τB(hB τ),λ∗ tA,g τB :T[r tB(X t,U t)|hB
t
,u t]=(cid:88) r tB(x˜ t,u t)Pµ∗ τB(hB τ),λ∗ tA,g τB :T(x˜ 1:t|hB
t
)
x˜1:t
(cid:88)
= rB(x˜ ,u )πB(x˜ )=:r˜B(πB,u )
t t t t t t t t
x˜t
where πB :=κB(hB). The second equality is true due to Lemma 4.
t t t
Therefore we can write
(cid:34) T (cid:35) (cid:34) T (cid:35)
Eµ∗ τB(hB τ),λ∗ tA,g τB :T (cid:88) r tB(X t,U t) =Eµ∗ τB(hB τ),λ∗ tA,g τB :T (cid:88) r˜ tB(ΠB t ,U t) .
t=τ t=τ
We now show that ΠB is a controlled Markov Chain controlled by U . By Definition 6, we have ΠB =
t t t+1
ξ (ΠA ,Σ ,M ), where ΠA =ℓ (ΠB,U ),Σ =λ∗B (ΠA ). Therefore we have
t+1 t+1 t+1 t+1 t+1 t t t t+1 t+1 t+1
Pµ∗ τB(hB τ),λ∗ tA,g τB :T(π tB +1|hB
t
,u t)
= (cid:88) 1
{π tB +1=ξt+1(π tA
+1,σt+1,m˜t+1)}Pµ∗ τB(hB τ),λ∗ tA,g τB :T(m˜ t+1|hB
t
,u t)
m˜t+1
= (cid:88) 1
{π tB +1=ξt+1(π tA +1,σt+1,m˜t+1)}
(cid:88) σ t+1(m˜ t+1|x˜ t+1)Pµ∗ τB(hB τ),λ∗ tA,g τB :T(x˜ t+1|hA t+1)
m˜t+1 x˜t+1
(cid:88) (cid:88)
= 1 σ (m˜ |x˜ )πA (x˜ )
{π tB +1=ξt+1(π tA +1,σt+1,m˜t+1)} t+1 t+1 t+1 t+1 t+1
m˜t+1 x˜t+1
where πA =ℓ (πB,u ),σ =λ∗A (πA ). The last equality is true due to Lemma 4.
t+1 t t t t+1 t+1 t+1
By construction, σ =λ∗A (πA ) induces the distribution C(πA ,γ ) from πA . This means that
t+1 t+1 t+1 t+1 t+1 t+1
(cid:88) (cid:88)
1 σ (m˜ |x˜ )πA (x˜ )
{π tB +1=ξt+1(π tA +1,σt+1,m˜t+1)} t+1 t+1 t+1 t+1 t+1
m˜t+1 x˜t+1
=C(πA ,γ )(πB )
t+1 t+1 t+1
We conclude that
Pµ∗ τB(hB τ),λ∗ tA,g τB :T(π tB +1|hB
t
,u t)=C(ℓ t(π tB,u t),γ t+1)(π tB +1).
In particular, this means that the conditional distribution of ΠB given all of (ΠB ,UB) is dependent only on (ΠB,U ),
t+1 1:t 1:t t t
proving that ΠB is a controlled Markov Chain controlled by U for t≥τ.
t t
Therefore, at hB, the receiver faces an MDP problem with state ΠB, action U , instantaneous reward r˜B(ΠB,U ) and
τ t t t t t
transition kernel P(πB |πB,u )=C(ℓ (πB,u ),γ )(πB ).
t+1 t t t t t t+1 t+1
Now, by construction, we know that
(cid:34) (cid:35)
(cid:88)
vˆB(πB)=max rB(x˜ ,u )π (x˜ )+VB (ℓ (π ,u ))
t t t t t t t t+1 t t t
ut
x˜t
(cid:20) (cid:90) (cid:21)
=max r˜B(πB,u )+ vˆB (·)dC(ℓ (πB,u ),γ ) (3)
t t t t+1 t t t t+1
ut
12and λ∗B(πB) attains the maximum in (3). Therefore λ∗B solves the Bellman equation for the MDP problem specified
above,t andt hence is an optimal strategy. Furthermore, Vτ B:T (πˆ) = (cid:82) vˆB(·)dC(πˆ,γ ) is the optimal total expected payoff for
1 1 t
the receiver when the principal plays λ∗A.
Step 2: Fixing the receiver’s strategy to be λ∗B, show that λ∗A is sequentially rational at any hA ∈HA at any time τ,
τ:T τ τ
given the belief µ∗A(hA).
τ τ
Similar to Step 1, we argue that at hA, the principal is facing an MDP problem with state process ΠA = κA(HA) and
τ t t t
action Σ for t≥τ.
t
First, we write
(cid:34) T (cid:35) (cid:34) T (cid:35)
Eµ∗ τA(hA τ),g τA :T,λ∗ tB (cid:88) r tA(X t,U t) =Eµ∗ τA(hA τ),g τA :T,λ∗ tB (cid:88) Eµ∗ τA(hA τ),g τA :T,λ∗ tB [r tA(X t,U t)|H tA,Σ t]
t=τ t=τ
Given that the receiver uses the CBB strategy λ∗B, we know that U =λ∗B(ΠB) where ΠB =ξ (ΠA,Σ ,M ). For any
t t t t t t t t
hA
t
such that Pµ∗ τA(hA τ),g τA :T,λ∗ tB(hA
t
)>0 we have
Eµ∗ τA(hA τ),g τA :T,λ∗ tB [r tA(X t,U t)|hA
t
,σ t]
= (cid:88) r tA(x˜ t,λ∗ tB(ξ t(π tA,σ t,m˜ t)))Pµ∗ τA(hA τ),g τA :T,λ∗ tB (m˜ t,x˜ t|hA
t
,σ t)
x˜t,m˜t
= (cid:88) r tA(x˜ t,λ∗ tB(ξ t(π tA,σ t,m˜ t)))σ t(m˜ t|x˜ t)Pµ∗ τA(hA τ),g τA :T,λ∗ tB (x˜ t|hA
t
,σ t)
x˜t,m˜t
(cid:88)
= rA(x˜ ,λ∗B(ξ (πA,σ ,m˜ )))σ (m˜ |x˜ )πA(x˜ )=:r˜A(πA,σ )
t t t t t t t t t t t t t t t
x˜t,m˜t
where πA :=κA(hA). The third equality is true due to Lemma 4.
t t t
Therefore we can write
(cid:34) T (cid:35) (cid:34) T (cid:35)
Eµ∗ τA(hA τ),g τA :T,λ∗ tB (cid:88) r tA(X t,U t) =Eµ∗ τA(hA τ),g τA :T,λ∗ tB (cid:88) r˜ tA(ΠA
t
,Σ t) .
t=τ t=τ
We now show that ΠA is a controlled Markov process with action Σ : We know that
t t
ΠA =ℓ (ΠB,UB), UB =λ∗B(ΠB), ΠB =ξ (ΠA,Σ ,M ).
t+1 t t t t t t t t t t t
Hence ΠA is a function of ΠA,Σ , and M . Furthermore,
t+1 t t t
Pµ∗ τA(hA τ),g τA :T,λ∗ tB (m t|hA
t
,σ t)=(cid:88) σ t(m t|x˜ t)Pµ∗ τA(hA τ),g τA :T,λ∗ tB (x˜ t|hA
t
,σ t)
x˜t
(cid:88)
= σ (m |x˜ )πA(x˜ ).
t t t t t
x˜t
Therefore the conditional distribution of ΠA given (HA,Σ ) depends only on (ΠA,Σ ), proving that ΠA is a controlled
t+1 t t t t t
Markov process. We conclude that at hA, the principal faces an MDP problem with state ΠA, action Σ , and instantaneous
τ t t
reward r˜A(ΠA,Σ ) for t≥τ.
t t t
Next we will show that λ∗A is a dynamic programming solution of this MDP.
τ:T
Induction Variant: VA, as defined in (1f), is the value function for this MDP.
t
Induction Step: Suppose that VA is the value function for this MDP at time t+1 and consider the stage optimization
t+1
problem at πA.
t
Note that the instantaneous cost can be written as
(cid:88)
r˜A(πA,σ )= rA(x˜ ,λ∗B(ξ (πA,σ ,m˜ )))σ (m˜ |x˜ )πA(x˜ )
t t t t t t t t t t t t t t t
x˜t,m˜t
(cid:32) (cid:33)(cid:32) (cid:33)
=(cid:88) (cid:88)
rA(x˜ ,λ∗B(ξ (πA,σ ,m˜ )))
σ t(m˜ t|x˜ t)π tA(x˜ t) (cid:88)
σ (m˜ |xˆ )πA(xˆ )
t t t t t t t (cid:80) σ (m˜ |xˆ )πA(xˆ ) t t t t t
m˜t x˜t xˆt t t t t t xˆt
(cid:32) (cid:33)(cid:32) (cid:33)
(cid:88) (cid:88) (cid:88)
= rA(x˜ ,λ∗B(ξ (πA,σ ,m˜ )))ξ (πA,σ ,m˜ )(x˜ ) σ (m˜ |xˆ )πA(xˆ )
t t t t t t t t t t t t t t t t t
m˜t x˜t xˆt
(cid:34) (cid:35)
(cid:88) (cid:12)
=E rA(x˜ ,λ∗B(ΠB))ΠB(x˜ )(cid:12)πA,σ
t t t t t t (cid:12) t t
x˜t
13where ΠB is a random distribution that follows the distribution induced by σ from πA.
t t t
Hence objective function for the stage optimization can be written as
Q˜A(πA,σ )=r˜A(πA,σ )+E[VA (ΠA )|πA,σ ]
t t t t t t t+1 t+1 t t
=r˜A(πA,σ )+E[VA (ℓ (ΠB,λ∗B(ΠB)))|πA,σ ]
t t t t+1 t t t t t t
=E(cid:2) v˜A(ΠB)|πA,σ (cid:3)
t t t t
where
(cid:88)
v˜A(π ):= rA(x˜ ,λ∗B(π ))π (x˜ )+VA (ℓ (πB,λ∗B(πB)))
t t t t t t t t t+1 t t t t
x˜t
By construction of λ∗B, we know that v˜A =vˆA (defined in (1c)).
t t t
Therefore, the stage optimization problem can be reformulated as:
(cid:90)
max vˆA(·)dν
t t
νt∈∆f(∆(Xt)) (SP)
subject to ν is inducible from πA
t t
and the optimal signal is any signal that induces an optimal distribution ν∗ of (SP) from πA.
t t
By the seminal result on one-shot information design in [2], we know that the optimal value of (SP) is given by the
concave closure of the function vˆA evaluated at πA.
t t
By construction, we have VA to be the concave closure of vˆA. Furthermore, λ∗A(πA) is assumed to induce the
t t t t
distribution ν = C(πA,γ ), where we know that (cid:82) vˆA(·)dC(πA,γ ) = VA(πA). Hence λ∗A(πA) is an optimal solution
t t t t t t t t t t
for the stage optimization problem, and VA is the value function at time t, proving the induction step.
t
We conclude that λ∗A is an optimal strategy for the principal at hA given the belief system µ (hA) and the receiver’s
τ:T τ τ τ
strategy λ∗B. Furthermore, VA(πˆ) is the optimal total expected payoff for the principal when the receiver plays λ∗A. Hence
1
we have completed the proof of sequential rationality.
14