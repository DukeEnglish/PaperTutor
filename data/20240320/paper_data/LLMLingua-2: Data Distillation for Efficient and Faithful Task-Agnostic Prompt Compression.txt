LLMLingua-2: Data Distillation for Efficient and Faithful
Task-Agnostic Prompt Compression
ZhuoshiPan1‚Ä†,QianhuiWu2‚Ä°,HuiqiangJiang2,MenglinXia2,XufangLuo2,JueZhang2,
QingweiLin2,VictorR√ºhle2,YuqingYang2,Chin-YewLin2,
H.VickyZhao1,LiliQiu2,DongmeiZhang2
1 TsinghuaUniversity,2 MicrosoftCorporation
{qianhuiwu, hjiang, xufang.luo}@microsoft.com
Abstract 1 Introduction
Recentyearshavewitnessedtheemergenceofvar-
This paper focuses on task-agnostic prompt
compressionforbettergeneralizabilityandef- iouspromptingtechniquesforlargelanguagemod-
ficiency. Considering the redundancy in nat- els(LLMs),suchasChain-of-Thought(COT)(Wei
ural language, existing approaches compress et al., 2022), In-context Learning (ICL) (Dong
promptsbyremovingtokensorlexicalunitsac- etal.,2023),andRetrievalAugmentedGeneration
cordingtotheirinformationentropyobtained
(RAG)(Lewisetal.,2020). Thesetechniquesem-
fromacausallanguagemodelsuchasLLaMa-
powerLLMstohandlecomplexandvariedtasks
7B.Thechallengeisthatinformationentropy
throughrichandinformativepromptsthatmayex-
maybeasuboptimalcompressionmetric: (i)it
onlyleveragesunidirectionalcontextandmay ceed tens of thousands of tokens. However, the
failtocaptureallessentialinformationneeded benefitsofsuchlengthypromptscomeatacostof
forpromptcompression; (ii)itisnotaligned increasedcomputationalandfinancialoverhead,as
withthepromptcompressionobjective. wellasthedegradedinformationperceptionability
ofLLMs. Promptcompressionisastraightforward
Toaddresstheseissues,weproposeadatadis-
solutiontoaddresstheseissues,whichattemptsto
tillationprocedure toderiveknowledge from
anLLMtocompresspromptswithoutlosing shortentheoriginalpromptswithoutlosingessen-
crucialinformation,andmeantime,introduce tialinformation.
anextractivetextcompressiondataset. Wefor-
Several methods have been proposed to com-
mulatepromptcompressionasatokenclassifi-
presspromptsinatask-awaremanner(Jiangetal.,
cationproblemtoguaranteethefaithfulnessof
2023b;Xuetal.,2024;JungandKim,2023;Huang
thecompressedprompttotheoriginalone,and
et al., 2023). These techniques aim to generate
use a Transformer encoder as the base archi-
tecturetocaptureallessentialinformationfor compressedpromptstailoredtothespecifictaskor
promptcompressionfromthefullbidirectional query,typicallyresultinginenhancedperformance
context.Ourapproachleadstolowerlatencyby ondownstreamtasks, particularlyinquestionan-
explicitly learning the compression objective
swering. However,thedependencyontask-specific
withsmallermodelssuchasXLM-RoBERTa-
featurespresentschallengesintermsofefficiency
largeandmBERT.
andgeneralizabilitywhendeployingthesemethods.
We evaluate our method on both in-domain Forexample,inRAG-styleapplications,itmaybe-
andout-of-domaindatasets,includingMeeting- comenecessarytocompressthesamedocuments
Bank,LongBench,ZeroScrolls,GSM8K,and
multipletimesdependingontheassociatedqueries
BBH.Despiteitssmallsize,ourmodelshows
withtask-awarepromptcompression. Moredetails
significantperformancegainsoverstrongbase-
arediscussedinSec.2.
lines and demonstrates robust generalization
ability across different LLMs. Additionally, Someworkshaveexploredtask-agnosticprompt
ourmodelis3x-6xfasterthanexistingprompt compression methods for better generalizability
compressionmethods, whileacceleratingthe andefficiency(Jiangetal.,2023a;Lietal.,2023).
end-to-endlatencyby1.6x-2.9xwithcompres-
Theunderlyingassumptionisthatnaturallanguage
sionratiosof2x-5x.1
containsredundancy(Shannon,1951)thatmaybe
usefulforhumanunderstandingbutmightnotbe
‚Ä†WorkduringinternshipatMicrosoft.
‚Ä°Correspondingauthor. necessary for LLMs. Therefore, they propose to
1Code:https://aka.ms/LLMLingua-2 compresspromptsbyremovingtokens(Jiangetal.,
4202
raM
91
]LC.sc[
1v86921.3042:viXra2023a) or lexical units (Li et al., 2023) accord- ‚Ä¢ Weapproachpromptcompressionasatoken
ing to their information entropy obtained from a classification task (i.e., preserve or discard),
causal small language model (SLM), regardless andtakethepredictedprobabilityofeachto-
of the downstream task or question information. kenbeinglabeledaspreserveasthecom-
However, these task-agnostic methods face two pressionmetric. Thebenefitsarethreefolds:
challenges: (i)Informationentropyisanempirical (1) It can capture all essential information
metricforpromptcompression. Relyingonitfor neededforpromptcompressionfromthefull
prompt trimming may be suboptimal, as it is not bidirectionalcontextbyusingaTransformer
alignedwiththepromptcompressionobjective. (ii) encoderforfeatureextraction. (2)Itcanlead
CausalLMsonlyleverageunidirectionalcontext, to lower latency, due to the use of smaller
whichmayfailtocaptureallessentialinformation modelstoexplicitlylearnthecompressionob-
neededforpromptcompressionwithinthecontext. jective. (3) It guarantees faithfulness of the
The challenges lead to the following research compressedprompttotheoriginalcontent.
questions:
‚Ä¢ Weconduct extensive experiments and anal-
Q1. How can we identify or build a suitable ysis on both in-domain (i.e., MeetingBank)
datasettoaligntheSLMtowardseffectiveprompt andout-of-domaindatasets(i.e.,LongBench,
compression? ZeroScrolls,GSM8K,andBigBenchHard).
Despitesmallinsize,ourmodelshowssignif-
Q2. Howcanwedesignacompressionalgorithm
icantperformancegainsoverstrongbaselines
thateffectivelyleveragesthefullbidirectionalcon-
anddemonstratesrobustgeneralizationability
textforbetterperformance?
fromGPT-3.5-TurbotoMistral-7B.Addition-
ForQ1,mosttextcompressiondatasetsareab- ally, our model is 3x-6x faster than existing
stractive (Toutanova et al., 2016; Koupaee and promptcompressionmethods,whileacceler-
Wang,2018;Kimetal.,2019),meaningthatthey atingtheend-to-endlatencyby1.6x-2.9xwith
treat prompt compression as a generative task compressionratiosof2x-5x.
wheretheoriginalpromptsarerephrasedintocon-
2 RelatedWorks
densedones. However,thisautoregressivegener-
ation process is slow and it may produce halluci- Depending on whether task information is used
nated content (Zhao et al., 2020). On the other forcompression,promptcompressionmethodscan
hand,extractivecompressiondatasetssuchasSent- be categorized into task-aware and task-agnostic
Comp(FilippovaandAltun,2013)andDebateSum compressionapproaches.
(RoushandBalaji,2020)areusuallycreatedforthe Task-awarecompressioncompressesthecontext
summarizationtaskandoftenlackdetailedinforma- basedonthedownstreamtaskorthecurrentquery.
tion. Inthecaseofpromptcompression,thiswill Forexample,LongLLMLingua(Jiangetal.,2023b)
hurt the performance of LLM inference in down- applies a question-aware coarse-to-fine compres-
streamapplicationssuchasQA(seeAppendixG sionapproachtoestimatetheinformationentropy
forsomeexamples). Therefore,itisnecessaryto of the tokens and adapts the estimation accord-
construct an extractive text compression dataset ingtothequestion. ReinforcementLearning(RL)
thatretainsessentialinformation. basedmethods(JungandKim,2023;Huangetal.,
2023) usually train a model for prompt compres-
Contributions. Wepresentthispapertoaddress
sion with reward signals from downstream tasks.
theabovechallengesfortask-agnosticpromptcom-
Softprompttuningmethods(Wingateetal.,2022;
pression. Wemakethefollowingcontributions.
Muetal.,2023)typicallyrequirefine-tuningforthe
‚Ä¢ We propose a data distillation procedure to specifictask. Xuetal.(2024)trainsasummariza-
derive knowledge from an LLM (GPT-4) to tionmodeltocompressthecontextdependingon
compressthepromptswithoutlosingcrucial thequestion. Task-awarecompressionapproaches
information. Weintroduceanextractivetext areusuallytailoredforspecifictasksandcompres-
compressiondataset,containingpairsoforigi- sionratios,whichmaylimittheirgeneralizability
naltextsfromMeetingBank(Huetal.,2023) inreal-worldapplications.
and their compressed versions. We publicly Task-agnostic methods compress the prompt
releasethedataset. without considering the specific task, making itCompressed Text: Item 15, City Manager Recommendation adopt three resolutions. Response
Join Victory Pace program. Join California first program. Consent inclusion
properties jurisdiction California Hero program. Emotion, motion, second, public
comment. Cast vote. Public comment? Come forward. Alex Mitchell, represent Hero LLM
program. Hero program in California three half years
Compressed Prompt
Step 1: Step 2: Step 5:
LLM Data Distillation Data Annotation Prompt Compression
based on ùëùpreserve
ùëùpreserve ùëùdiscard
Original Text: Item 15, report from City Manager Recommendation to adopt three Step 3: ‚Ä¶
resolutions. First, to join the Victory Pace program. Second, to join the Quality Control
California first program. And number three, consenting to to inclusion of & Filtering ‚Ä¶
certain properties within the jurisdiction in the California Hero program. It
was emotion, motion, a second and public comment. CNN. Please cast your vote.
Oh. Was your public comment? Yeah. Please come forward. I thank you, Mr. Mayor. ‚Ä¶
Thank you. Members of the council. My name is Alex Mitchell. I represent the Step 4: Token Classifier as Compressor
hero program. Just wanted to let you know that the hero program. Has been in Train Compressor
California for the last three and a half years. Original Prompt
Figure1: OverviewofLLMLingua-2.
moreadaptabletoarangeofapplicationsandblack- 3.1 DataDistillation
boxLLMs. However,producingcompressedtext
ToextractknowledgefromtheLLMforeffective
that can generalize well to different tasks is not
prompt compression, our goal is to prompt GPT-
trivial. Typicalmethodsinvolveusinginformation
4togeneratecompressedtextsfromoriginaltexts
entropy-basedmetricstoremoveredundantinfor-
thatmeetthefollowingcriteria: (i)Tokenreduction:
mationintheprompt(Lietal.,2023;Jiangetal.,
Compressedpromptsshouldbeshortinlengthto
2023a). They employ a small language model to
reduce cost and speed up inference. (ii) Informa-
estimate token importance from the information
tiveness: Essentialinformationshouldberetained.
metrics. Despitebeingtraining-free,thesemethods
(iii)Faithfulness: Compressedpromptsshouldre-
maynoteffectivelycapturethetokenimportance
main faithful and avoid introducing hallucinated
distributionoptimizedforspecificLLMsandoften
contenttoensureaccuracywhenpromptingLLMs
entailhighcomputationoverhead. Summarization-
indownstreamtasks.
basedmethodsarealsoleveragedfortask-agnostic
However,distillingsuchdatafromGPT-4ischal-
compression(Chenetal.,2023;Packeretal.,2023).
lenging, as it does not consistently follow the in-
However, they often omit crucial details and do
structions. For instance, Jiang et al. (2023a) ex-
not generalize well. An alternative approach is
perimentedwithdifferentpromptsforcompression
to compress or trim the context hidden or KV
andfoundthatGPT-4strugglestoretainessential
caches (Chevalier et al., 2023; Ge et al., 2023;
informationfromoriginaltexts. Inourpreliminary
Zhang et al., 2023; Liu et al., 2023b; Xiao et al.,
experiments, we have also observed that GPT-4
2024). However,thisisorthogonaltoourworkand
tends to modify expressions used in the original
cannotbeeasilyappliedtoblack-boxLLMs.
textsandsometimesgenerateshallucinatedcontent.
Toaddressthischallenge,weproposethefollowing
3 DatasetConstruction
datasetdistillationprocedure.
In this section, we outline the process of dataset InstructionDesign Awell-craftedinstructionis
constructionforpromptcompression. Wefirstin- the key to unveiling the compression capabilities
troduce our data distillation procedure, which in- ofGPT-4. Toensurethatthegeneratedtextsstay
volvesextractingknowledgefromanLLM(GPT-4 faithfultotheoriginal,weexplicitlyinstructGPT-
)tocompresstextswithoutlosingcrucialinforma- 4tocompressthetextbydiscardingunimportant
tionorintroducinghallucinatedcontent(Sec.3.1). wordsintheoriginaltextsonlyandnotaddingany
LeveragingthedistilledknowledgefromtheLLM, newwordsduringgeneration.
we explain our data annotation algorithm, which Toensuretokenreductionandinformativeness,
assignslabelstoeachwordintheoriginaltextto previousstudies(Jiangetal.,2023a;Huangetal.,
indicatewhetheritshouldbepreservedaftercom- 2023)havespecifiedeitheracompressionratioor
pression(Sec.3.2). Toensurethedataset‚Äôsquality, a target number of compressed tokens in the in-
weproposetwoqualitycontrolmetricsforfiltering structions. However, GPT-4 often fails to adhere
low-qualitysamples(Sec.3.3). totheserestrictions. Additionally,theinformationOurInstructionforCompression:
Compressthegiventexttoshortexpressions,andsuch
thatyou(GPT-4)canreconstructitascloseaspossible
totheoriginal.Unliketheusualtextcompression,Ineed 100
youtocomplywiththe5conditionsbelow:
1.YoucanONLYremoveunimportantwords. 80
2.Donotreordertheoriginalwords.
3.Donotchangetheoriginalwords.
60 4.Donotuseabbreviationsoremojis.
5.Donotaddnewwordsorsymbols.
Compresstheoriginaggressivelybyremovingwordsonly. 40
Compress the origin as short as you can, while retain-
ingasmuchinformationaspossible. Ifyouunderstand,
20
pleasecompress thefollowingtext: {text tocompress}
Thecompressedtextis:
0
0 10000 20000 30000 40000
Context Length
Figure2: Ourinstructionusedfordatadistillation.
Figure4: Illustrationofcompressionratiow.r.t. original
context length on MeetingBank. We use GPT-4-32k
0.30
withtheoutputtokenlimitsettingto4096.
0.25
0.20
sion leads to substantial information loss, signifi-
0.15
cantlyimpactingtheperformanceofdownstream
0.10
tasks. Tomitigatethisissue,wefirstsegmenteach
0.05
longcontextintomultiplechunks,eachcontaining
0.00 nomorethan512tokensandendingwithaperiod.
1 2 3 4 5 6 7 8 9 10111213141516171819 20
Compression Ratio We then instruct GPT-4 to compress each chunk
Figure3: Distributionofcompressionratioafterchunk- individually.
wisecompressiononMeetingBank.
3.2 DataAnnotation
Having obtained pairs of original texts and
density of text can vary significantly depending
their compressed versions from data distillation
on its genre, style, etc. For instance, news arti-
(Sec.3.1),thegoalofdataannotationistoassigna
clestypicallycontaindenserinformationcompared
binarylabeltoeachtokenintheoriginaltextstode-
to meeting transcripts. Furthermore, even within
termineifitshouldbepreservedordiscardedafter
thedomainofmeetingtranscripts,theinformation
compression. Fig.5describesthethreeprimaryob-
density from different speakers may vary. These
staclesencounteredhere,whicharisefromGPT-4‚Äôs
factorssuggestthatafixedcompressionratiomay
inabilityto precisely complywiththe instruction
not be optimal. Therefore, we remove the com-
inFig.9. Alg.1outlinestheoverallprocedureof
pressionratiorestrictionfromourinstructionsand
theproposedannotationalgorithmdesignedtodeal
insteadpromptGPT-4tocompresstheorigintext
with these obstacles. For more detailed informa-
asshortaspossiblewhileretainingasmuchinfor-
tion,pleaserefertoAppendixB.
mation as possible. As shown in Fig. 3, GPT-4
assignsvaryingcompressionratiostodifferentsen- 3.3 QualityControl
tencesanddiscardssomesentencesentirely. Fora
Weintroducetwoqualitycontrolmetricstoassess
comparison between ourinstruction and those of
the quality of the compressed texts generated by
Jiangetal.(2023a),pleaserefertoTable7.
GPT-4 distillation, as well as the quality of the
automaticallyannotatedlabels. Wethenfilterthe
Chunk-Wise Compression Empirically, we
examplesbytheirscores.
havefoundthatthelengthoftheoriginaltexthasa
notableinfluenceonthecompressionperformance. VariationRate AsGPT-4mayfailtofollowthe
As shown in Fig. 4, GPT-4 tends to apply a high instructions,weintroducethemetricVariationRate
compressionratiowhenprocessingverylongcon- (VR)toevaluatethequalityofthecompressedtexts
text,whichmightbeduetoGPT-4‚Äôslimitedability generatedfromdatadistillation. VRmeasuresthe
tohandlelongcontext. Thisaggressivecompres- proportionofwordsinthecompressedtextthatare
ecnetnes
fo
oitaR
oitaR
noisserpmoCexampleswiththetop5%highestvariationrates.
OriginalTexts
Item15,reportfromCityManagerRecommendationto
adopt three resolutions. First, to join the Victory Pace Alignment Gap We propose Alignment Gap
program. Second, to join the California first program. (AG) to evaluate the quality of the automatically
Andnumberthree,consentingtotoinclusionofcertain
annotatedlabels. Letl(¬∑)representtheannotation
propertieswithinthejurisdictionintheCaliforniaHero
program. function, where l(w) = True signifies that word
w ‚àà S corresponds to a word in S . We
CompressedTexts ori comp
CityManagerRecommendationadoptthreeresolutions. firstlydefinethematchingrate(MR)as:
JoinCaliforniafirstprogram.Consentpropertiesinclusion
jurisdictionCaliforniaHeroprogram. 1 (cid:88)
MR = I(l(w) = True). (2)
|S |
ori w‚ààS
ori
Figure5: Challengesindataannotation.
(i) Ambiguity: a word in the compressed texts may Since there exists a many-to-one word mapping
appearmultipletimesintheoriginalcontent. fromS toS (i.e.,the"Ambiguity"challenge
ori comp
(ii)Variation: GPT-4maymodifytheoriginalwordsin presentedinSec.3.2),wefurtherpresentahitting
tense,pluralform,etc. duringcompression.
rate(HR)asaregularizationtermtomeasurethe
(iii)Reordering: Theorderofwordsmaybechanged proportionofwordsinS thatarefoundinS .
comp ori
aftercompression.
HRisdefinedas:
Algorithm1:DataAnnotation HR = 1 (cid:88) I(w ‚àà S ). (3)
Input :originalstringS ori,compressed |S ori|
w‚ààS
comp
ori
stringS ,windowsizes.
comp
SplitoriginalstringS towordlistS . Finally,theAlignmentGap(AG)isdefinedas:
ori ori
SplitcompressedS towordlistS .
comp comp
InitializelabelsoforiginalwordstoFalse. AG = HR‚àíMR. (4)
Initializepreviousmatchindexprev to0.
forw ‚àà S do Thealignmentgapofaperfectannotationshould
comp
fori = 1,2,..., s do be0. AlargeAGindicatesahighhittingratewith
right=min(2 |S |,prev+i) apoormatchingrate,implyinglow-qualityanno-
ori
iffuzzy_match(w,S [right])then tationforthisexample. Therefore,wediscardex-
ori
L[right]=True. amplesofthehighest10%alignmentgaptoensure
prev =right. qualitycontrolofthedataset.
Break.
end 4 Compressor
left=max(0,prev‚àíi)
Weformulatepromptcompressionasabinaryto-
iffuzzy_match(w,S [left])then
ori kenclassificationproblem(i.e.,preserveordiscard)
L[left]=True.
to guarantee the faithfulness of the compressed
Break.
prompt to the original content, and meantime en-
end
surethelowlatencyofthecompressionmodelit-
end
self. Forthetokenclassificationmodel,weemploy
end
a Transformer encoder as the feature extractor to
Output:labelsoforiginalwordsL(S ).
ori
leverage information from the bidirectional con-
texts of each token. We train the classification
absentintheoriginaltext. Specifically, letS model on the dataset constructed in Sec. 3 from
comp
bethesetofwordsinthecompressedtextandS MeetingBank(Huetal.,2023). Duringinference,
ori
wedeterminewhethertopreserveordiscardeach
bethatoftheoriginaltext. VRisdefinedas:
tokenintheoriginalpromptbasedonitsprobability
VR = 1 (cid:88) I(w ‚àà/ S ), (1) calculatedbyourclassificationmodel.
|S | ori
comp w‚ààS
comp 4.1 TokenClassificationModel
where|¬∑|isthecardinalityofaset. Ahighervaria- Architecture WeutilizeaTransformerencoder
tionrateimpliesahigherlikelihoodofencounter- (Devlinetal.,2019)asthefeatureencoderf and
Œ∏
inghallucinatedcontent. Therefore,weexcludethe add a linear classification layer on top. GivenQA Summary Length
Methods
F1Score BELU Rouge1 Rouge2 RougeL BERTScore Tokens 1/œÑ
Selective-Context 66.28 10.83 39.21 18.73 27.67 84.48 1,222 2.5x
LLMLingua 67.52 8.94 37.98 14.08 26.58 86.42 1,176 2.5x
LLMLingua-2-small 85.82 17.41 48.33 23.07 34.36 88.77 984 3.0x
LLMLingua-2 86.92 17.37 48.64 22.96 34.24 88.27 970 3.1x
Original 87.75 22.34 47.28 26.66 35.15 88.96 3,003 1.0x
Table1: In-domainevaluationofdifferentmethodsonMeetingBank.
an original prompt consisting of N words x = for a higher compression ratio of ‚àº15x for tasks
{x }N ,thiscanbeformulatedas: involving multiple demonstrations or documents.
i i=1
Particularly, we can replace the perplexity-based
h = f Œ∏(x), (5) iterative token compression module in LLMLin-
p(x ,Œò) = softmax(Wh +b), (6) guawithourtoken-classification-basedcompressor,
i i
whilekeepingthebudgetcontrollerunchanged.
where h = {h }N denotes feature vectors for
i i=1
all words, p(x ,Œò) ‚àà R2 denotes the probability 5 Experiment
i
distributionoflabels{preserve,discard}for
Implementation Details We construct our ex-
thei-thwordx , andŒò = {Œ∏,W,b}representall
i tractive text compression dataset using training
thetrainableparameters.
examples from MeetingBank (Hu et al., 2023)
Training Let y = {y }N denote the corre- with implementation details in Appendix A. Our
i i=1
sponding labels for all words in x, then we em- approach is implemented using Huggingface‚Äôs
TransformersandPyTorch2.0.1withCUDA-11.7.
ploycrossentropylosstotrainthemodel. Theloss
functionLw.r.t. xis:
Weusexlm-roberta-large(Conneauetal.,
2020)andmultilingual-BERT(Devlinetal.,
1 (cid:88)N 2019) for the feature encoder f Œ∏ in our compres-
L(Œò) = CrossEntropy(y ,p(x ,Œò)). (7)
N i i sor, which we refer to as LLMLingua-2 and
i=1 LLMLingua-2-small, respectively. We fine-
4.2 CompressionStrategy tune both models for 10 epochs, using the Adam
optimizer(KingmaandBa,2015)withalearning
Ourapproachtocompressingtheoriginalprompt
rate of 1e-5 and a batch size of 10. Unless spec-
x = {x }N withatargetcompressionratio1/œÑ
i i=1 ifiedotherwise,allreportedmetricsuseGPT-3.5-
involves a three-step process, where œÑ is defined
Turbo-06133 as the target LLM for downstream
asthequotientofthenumberofwordsinthecom-
tasks,withgreedydecodingatatemperatureof0
pressed prompt and the number of words in the
forenhancedstabilityacrossexperiments.
originalpromptx. First,wederivethetargetnum-
ber of tokens to be preserved in the compressed Datasets&EvaluationMetrics Weconductfive
promptxÀú: NÀú = œÑN. Next,weusethetokenclassi- groupsofexperimentstoevaluatethecompressed
ficationmodeltopredicttheprobabilityp ofeach promptsontwogroupsofdatasets.
i
wordx beinglabeledaspreserve2. Finally,we (i)In-Domain: Aswetrainourcompressorus-
i
retain the top NÀú words in the original prompt x ing the dataset built with training examples from
withthehighestp andmaintaintheiroriginalorder MeetingBank(Huetal.,2023),weusetheMeet-
i
toformthecompressedpromptxÀú. ingBank test examples for in-domain evaluation.
It‚Äôsworthnotingthatourapproachcanbereadily Inadditiontothesummarizationtask,wefurther
integrated into the coarse-to-fine framework pro- introduceaQAtaskbypromptingGPT-4togener-
posedinLLMLingua(Jiangetal.,2023a),allowing ate3question-answerpairsforeachexampledis-
tributedacrossthewholecontext(seeAppendixF
2Toaddresstokenization-relatedchallengesthatarisewhen
formoredetails). Forthesummarizationtask,we
applyingourapproachacrossvariousLLMsandSLMs,we
preservetheintegrityofmulti-tokenwordsandrepresentthe usethesameevaluationmetricasinLLMLingua
probabilityofawordbyaveragingoverthepredictedproba-
bilitiesofallsubwordtokens. 3https://platform.openai.com/LongBench ZeroSCROLLS
Methods
SingleDoc MultiDoc Summ. FewShot Synth. Code AVG Tokens 1/œÑ AVG Tokens 1/œÑ
2,000-tokenconstraint
Task(Question)-AwareCompression
SBERT‚Ä† 33.8 35.9 25.9 23.5 18.0 17.8 25.8 1,947 5x 20.5 1,773 6x
OpenAI‚Ä† 34.3 36.3 24.7 32.4 26.3 24.8 29.8 1,991 5x 20.6 1,784 5x
LongLLMLingua‚Ä† 39.0 42.2 27.4 69.3 53.8 56.6 48.0 1,809 6x 32.5 1,753 6x
Task(Question)-AgnosticCompression
Selective-Context‚Ä† 16.2 34.8 24.4 15.7 8.4 49.2 24.8 1,925 5x 19.4 1,865 5x
LLMLingua‚Ä† 22.4 32.1 24.5 61.2 10.4 56.8 34.6 1,950 5x 27.2 1,862 5x
LLMLingua-2-small 29.5 32.0 24.5 64.8 22.3 56.2 38.2 1,891 5x 33.3 1,862 5x
LLMLingua-2 29.8 33.1 25.3 66.4 21.3 58.9 39.1 1,954 5x 33.4 1898 5x
3,000-tokensconstraint
Task(Question)-AwareCompression
SBERT‚Ä† 35.3 37.4 26.7 63.4 51.0 34.5 41.4 3,399 3x 24.0 3,340 3x
OpenAI‚Ä† 34.5 38.6 26.8 63.4 49.6 37.6 41.7 3,421 3x 22.4 3,362 3x
LongLLMLingua‚Ä† 40.7 46.2 27.2 70.6 53.0 55.2 48.8 3,283 3x 32.8 3,412 3x
Task(Question)-AgnosticCompression
Selective-Context‚Ä† 23.3 39.2 25.0 23.8 27.5 53.1 32.0 3,328 3x 20.7 3,460 3x
LLMLingua‚Ä† 31.8 37.5 26.2 67.2 8.3 53.2 37.4 3,421 3x 30.7 3,366 3x
LLMLingua-2-small 35.5 38.1 26.2 67.5 23.9 60.0 41.9 3,278 3x 33.4 3,089 3x
LLMLingua-2 35.5 38.7 26.3 69.6 21.4 62.8 42.4 3,392 3x 33.5 3206 3x
OriginalPrompt 39.7 38.7 26.5 67.0 37.8 54.2 44.0 10,295 - 34.7 9,788 -
Zero-Shot 15.6 31.3 15.6 40.7 1.6 36.2 23.5 214 48x 10.8 32 306x
Table2: Out-of-domainevaluationongenerallong-contextscenarios. ‚Ä†: numbersreportedinJiangetal.(2023b).
GSM8K BBH
Methods
1-shotconstraint half-shotconstraint 1-shotconstraint half-shotconstraint
EM Tokens 1/œÑ EM Tokens 1/œÑ EM Tokens 1/œÑ EM Tokens 1/œÑ
Selective-Context‚Ä† 53.98 452 5x 52.99 218 11x 54.27 276 3x 54.02 155 5x
LLMLingua‚Ä† 79.08 446 5x 77.41 171 14x 70.11 288 3x 61.60 171 5x
LLMLingua-2-small 78.92 437 5x 77.48 161 14x 69.54 263 3x 60.35 172 5x
LLMLingua-2 79.08 457 5x 77.79 178 14x 70.02 269 3x 61.94 176 5x
Full-Shot 78.85 2,366 - 78.85 2,366 - 70.07 774 - 70.07 774 -
Zero-Shot 48.75 11 215x 48.75 11 215x 32.32 16 48x 32.32 16 48x
Table3: Out-of-domainevaluationonreasoningandin-contextlearning. ‚Ä†: numbersreportedinJiangetal.(2023b).
(Jiangetal.,2023a). ForQAtask,weusethemet- Baselines We take two state-of-the-art prompt
ricsandscriptsprovidedinLongBench(Baietal., compressionmethodsasprimarybaselinesforcom-
2023)SingleDocumentQAforevaluation. parison: Selective-Context (Li et al., 2023) and
LLMLingua (Jiang et al., 2023a), both are based
(ii)Out-of-Domain: Forlong-contextscenarios,
onLLaMA-2-7B.Additionally,wecompareour
we use LongBench (Bai et al., 2023) and Zero-
approachwithsometask-awarepromptcompres-
SCROLLS(Shahametal.,2023),andweemploy
sionmethods,suchasretrieval-basedmethodsand
thesameevaluationmetricasinLongLLMLingua
LongLLMLingua(Jiangetal.,2023b).
(Jiangetal.,2023b). Forreasoningandin-context
learning,weuseGSM8K(Cobbeetal.,2021)and ResultsonIn-DomainBenchmark InTable1,
Big Bench Hard (BBH) (bench authors, 2023), wefirstpresenttheresultsofourproposedmethod
with evaluation metrics consistent with LLMLin- comparedtothestrongbaselinesonMeetingBank.
gua(Jiangetal.,2023a). Despite the fact that our compressors are muchMeetingBank LongBench-SingleDoc
Methods
QA Summ. Tokens 1/œÑ 2,000-tokencons. Tokens 1/œÑ 3,000-tokencons. Tokens 1/œÑ
Selective-Context 58.13 26.84 1,222 2.5x 22.0 2,038 7.1x 26.0 3,075 4.7x
LLMLingua 50.45 23.63 1,176 2.5x 19.5 2,054 7.1x 20.8 3,076 4.7x
LLMLingua-2-small 75.97 29.93 984 3.0x 25.3 1,949 7.4x 27.9 2,888 5.0x
LLMLingua-2 76.22 30.18 970 3.0x 26.8 1,967 7.4x 27.3 2,853 5.1x
OriginalPrompt 66.95 26.26 3,003 - 24.5 14,511 - 24.5 14,511 -
Table4: EvaluationwithMistral-7BastheTargetLLMonMeetingBankandLongBenchsingledocQAtask. We
reportRouge1(Lin,2004)forsummary.
smallerthantheLLaMa-2-7Busedinthebaselines, method, by offering shorter prompts with higher
our approach achieves significantly better perfor- informationdensity,effectivelyimprovesMistral-
mance on both the QA and Summary tasks, and 7B‚Äôsfinalinferenceperformance.
comes close to matching the performance of the
original prompt. This demonstrates the effective- LatencyEvaluation Table5showsthelatencyof
nessofourconstructeddataset,andhighlightsthe differentsystemsonaV100-32GGPUwithdiffer-
importanceandbenefitofoptimizingthecompres- entcompressionratios. ItshowsthatLLMLingua-
sionmodelusingpromptcompressionknowledge. 2hasamuchsmallercomputationoverheadthan
other compression methods, and can achieve an
ResultsonOut-of-DomainBenchmarks Asour
end-to-endspeeduprangingfrom1.6xto2.9x. Ad-
modelistrainedonmeetingtranscriptsdatafrom
ditionally, our method can reduce GPU memory
MeetingBank, here we explore its generalization
costs by 8x, lowering the demand for hardware
abilityacrossvariousbenchmarksoflong-context
resources. Fordetails,seetheAppendixI.
scenarios,reasoning,andin-contextlearning. Ta-
ble 2 and 3 show the results on LongBench, Ze-
1/œÑ 1x 2x 3x 5x
roSCROLLS,GSM8K,andBBH:Ourmodelhas
demonstrated superior performance compared to End2Endw/oCompression 14.9
End2Endw/LLMLingua-2 - 9.4(1.6x)7.5(2.1x)5.2(2.9x)
other task-agnostic baselines. Even our smaller
model, which is of BERT-base size, has been Selective-Context - 15.9 15.6 15.5
LLMLingua - 2.9 2.1 1.5
able to achieve comparable, and in some cases,
LLMLingua-2 - 0.5 0.4 0.4
evenslightlyhigherperformancethantheoriginal
prompt. Whileourapproachhasshownpromising Table5: Latency(s)comparisononMeetingBank.
results,itfallsshortwhencomparedtoothertask-
awarecompressionmethodslikeLongLLMlingua
(Jiang et al., 2023a) on Longbench. We attribute Observation on Context Awareness We have
thisperformancegaptotheadditionalinformation observedthatLLMLingua-2caneffectivelymain-
thattheyleveragefromthequestion. However,the tainthemostinformativewordswithrespecttothe
task-agnosticcharacteristicsofourmodelmakeit fullcontextasthecompressionratioincreases. We
anefficientoptionwithgoodgeneralizabilitywhen owethistotheadoptionofthebidirectionalcontext-
deployedacrossdifferentscenarios. awarefeatureextractor, aswellasthestrategyof
explicitlyoptimizingtowardthepromptcompres-
Mistral-7BastheTargetLLM Table4presents
sionobjective. SeeFigure6formoredetails.
theresultsofdifferentmethodsusingMistral-7B-
v0.14 asthetargetLLM.Ourmethoddemonstrates
Prompt Reconstruction We have conducted
significantperformancegainoverotherbaselines,
experiments of prompting GPT-4 to reconstruct
showcasingitsgoodgeneralizationabilityacross
theoriginalpromptfromtheLLMLingua-2com-
targetLLMs. Notably,LLMLingua-2yieldseven
pressedprompt. TheresultsshowthatGPT-4can
better performance than the original prompt. We
effectivelyreconstructtheoriginalprompt,suggest-
speculate that Mistral-7B might be less adept at
ingthatthereisnoessentialinformationlossduring
managinglongcontextsthanGPT-3.5-Turbo. Our
thecompressionprocessofLLMLingua-2. Figure
4https://mistral.ai/ 7and8inAppendixEpresentsomeexamples.LongBench ZeroSCROLLS
Methods
SingleDoc MultiDoc Summ. FewShot Synth. Code AVG Tokens 1/œÑ AVG Tokens 1/œÑ
LLMLingua-2-small 29.5 32.0 24.5 64.8 22.3 56.2 38.2 1,891 5x 33.3 1,862 5x
LLMLingua-2 29.8 33.1 25.3 66.4 21.3 58.9 39.1 1,954 5x 33.4 1,898 5x
LLMLingua-2‚Ä° 30.7 33.9 25.4 66.6 22.6 58.1 39.5 1,853 5x 33.4 1,897 5x
OriginalPrompt 39.7 38.7 26.5 67.0 37.8 54.2 44.0 10,295 - 34.7 9,788 -
Zero-Shot 15.6 31.3 15.6 40.7 1.6 36.2 23.5 214 48x 10.8 32 306x
Table 6: Out-of-domain evaluation on general long-context benchmarks with the 2,000-token constraint.
LLMLingua-2‚Ä°: Weexpandtheconstructedtextcompressiondatasetusing50kexamplesfromTriviaQA-wiki.
ThentrainanLLMLingua-2compressorwiththeexpandeddataset.
Instruction 1/œÑ VR‚Üì QAF1‚Üë fromtwoperspectives.
Firstly, we have conducted extensive out-of-
Instruction1 123x 13.7 19.1
domainevaluation onfour benchmarksin thepa-
Instruction2 27x 7.8 26.1
Instruction3 78x 9.6 23.7 per,includingLongBench(Baietal.,2023),Zero-
Instruction4 49x 9.4 24.9 SCROLLS(Shahametal.,2023),GSM8K(Cobbe
etal.,2021),andBigBenchHard(BBH)(benchau-
LLMLingua-2w/oChunk 21x 6.0 27.9
LLMLingua-2 2.6x 2.2 36.7 thors,2023),whichcovermultipletasksfromdoc-
umentQAtomathproblemsandin-contextlearn-
Table7: AblationStudyonChunk-WiseCompression ing. Theexperimentalresultsshowthatevenour
and Instruction Design. We report the compression LLMLingua-2-smallmodelthatisofBERT-base
ratio,variationrate,andQAperformanceonLongBench
size achieves superior performance than the two
SingleDocumentQA.SeeFig.10inAppendixformore
LLaMA-2-7B based baselines Selective-Context
detailsofInstruction1-Instruction4here.
(Li et al., 2023) and LLMLingua (Jiang et al.,
2023a). Thisdemonstratesthatourlearnedprompt
Ablation Study on Chunk-Wise Compression compressionmodelhasgoodgeneralizationability
andInstructionDesign Table7showsthatboth todatafromdifferentdomains.
thedesignedinstructionandthechunk-wisecom- Secondly, we expand the constructed text
pression strategy proposed in this paper signifi- compression dataset using 50k examples from
cantlycontributetothesuccessofLLMLingua-2. TriviaQA-wiki. ThentrainanLLMLingua-2com-
pressorwiththeexpandeddatasettoseewhether
6 Conclusion there would be further performance gain. Table
6 shows the results under the 2,000-token con-
Thispapertargetstask-agnosticpromptcompres-
straint. We can see that training the compressor
sion for better generalizability and efficiency. In
with more data does bring further performance
thispaper,weidentifythechallengesencountered gain(LLMLingua-2‚Ä°). However,theimprovement
inexistingmethodsandaddressthemaccordingly.
seemsnotthatsignificant. Weconjecturethatthis
Weconductextensiveexperimentsandanalysison
is because although the semantics of texts from
fivebenchmarksacrossdifferenttasksanddomains.
differentdomainsmayvaryalot,theirredundancy
Ourmodelshowssuperiorityoverstrongbaselines
pattern might be similar. Such pattern or knowl-
intermsofperformanceandcompressionlatency.
edge may be learned during in-domain training,
Wepubliclyreleasethedatasetoftextcompression
andthenactasananchorthatcantransferacross
withnoessentialinformationlossinthispaper.
differentdomains. Weleavethisforfuturework.
Limitations
References
Ourtextcompressiondatasetwasconstructedus-
ingonlytrainingexamplesfromMeetingBank, a Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
JiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao
datasetofsummarizationovermeetingtranscripts.
Liu,AohanZeng,LeiHou,etal.2023. Longbench:
Thisraisesconcernsaboutthegeneralizationability
A bilingual, multitask benchmark for long context
ofourcompressor. Herewediscussthisquestion understanding. ArXivpreprint,abs/2308.14508.BIGbenchauthors.2023. Beyondtheimitationgame: Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing
Quantifyingandextrapolatingthecapabilitiesoflan- Yang,andLiliQiu.2023a. LLMLingua: Compress-
guagemodels. TransactionsonMachineLearning ing prompts for accelerated inference of large lan-
Research. guagemodels. InProceedingsofthe2023Confer-
enceonEmpiricalMethodsinNaturalLanguagePro-
HowardChen,RamakanthPasunuru,JasonWeston,and cessing,pages13358‚Äì13376,Singapore.Association
Asli Celikyilmaz. 2023. Walking down the mem- forComputationalLinguistics.
orymaze: Beyondcontextlimitthroughinteractive
reading. ArXivpreprint,abs/2310.05029. HuiqiangJiang,QianhuiWu,XufangLuo,Dongsheng
Li,Chin-YewLin,YuqingYang,andLiliQiu.2023b.
AlexisChevalier,AlexanderWettig,AnirudhAjith,and
Longllmlingua: Accelerating and enhancing llms
Danqi Chen. 2023. Adapting language models to
in long context scenarios via prompt compression.
compresscontexts. ArXivpreprint,abs/2305.14788.
ArXivpreprint,abs/2310.06839.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
HoyounJungandKyung-JoongKim.2023. Discrete
MarkChen,HeewooJun,LukaszKaiser,Matthias
prompt compression with reinforcement learning.
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
ArXivpreprint,abs/2308.08758.
Nakano,etal.2021. Trainingverifierstosolvemath
wordproblems. ArXivpreprint,abs/2110.14168.
ByeongchangKim,HyunwooKim,andGunheeKim.
AlexisConneau,KartikayKhandelwal,NamanGoyal, 2019. Abstractive summarization of Reddit posts
Vishrav Chaudhary, Guillaume Wenzek, Francisco withmulti-levelmemorynetworks. InProceedings
Guzm√°n, Edouard Grave, Myle Ott, Luke Zettle- ofthe2019ConferenceoftheNorthAmericanChap-
moyer,andVeselinStoyanov.2020. Unsupervised teroftheAssociationforComputationalLinguistics:
cross-lingualrepresentationlearningatscale. InPro- HumanLanguageTechnologies,Volume1(Longand
ceedings of the 58th Annual Meeting of the Asso- ShortPapers),pages2519‚Äì2531,Minneapolis,Min-
ciationforComputationalLinguistics,pages8440‚Äì nesota.AssociationforComputationalLinguistics.
8451, Online. Association for Computational Lin-
guistics. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and national Conference on Learning Representations,
Kristina Toutanova. 2019. BERT: Pre-training of ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
deepbidirectionaltransformersforlanguageunder- ConferenceTrackProceedings.
standing. InProceedingsofthe2019Conferenceof
theNorthAmericanChapteroftheAssociationfor MahnazKoupaeeandWilliamYangWang.2018. Wiki-
ComputationalLinguistics: HumanLanguageTech- how:Alargescaletextsummarizationdataset. ArXiv
nologies,Volume1(LongandShortPapers),pages preprint,abs/1810.09305.
4171‚Äì4186,Minneapolis,Minnesota.Associationfor
ComputationalLinguistics. Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy- Goyal,HeinrichK√ºttler,MikeLewis,Wen-tauYih,
ongWu,BaobaoChang,XuSun,JingjingXu,and Tim Rockt√§schel, Sebastian Riedel, and Douwe
ZhifangSui.2023. Asurveyforin-contextlearning. Kiela. 2020. Retrieval-augmented generation for
ArXivpreprint,abs/2301.00234. knowledge-intensiveNLPtasks. InAdvancesinNeu-
ralInformationProcessingSystems33: AnnualCon-
KatjaFilippovaandYaseminAltun.2013. Overcom-
ferenceonNeuralInformationProcessingSystems
ingthelackofparalleldatainsentencecompression.
2020,NeurIPS2020,December6-12,2020,virtual.
In Proceedings of the 2013 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,pages
YuchengLi,BoDong,FrankGuerin,andChenghuaLin.
1481‚Äì1491,Seattle,Washington,USA.Association
2023. Compressingcontexttoenhanceinferenceef-
forComputationalLinguistics.
ficiencyoflargelanguagemodels. InProceedingsof
the2023ConferenceonEmpiricalMethodsinNatu-
TaoGe,JingHu,XunWang,Si-QingChen,andFuru
ralLanguageProcessing,pages6342‚Äì6353,Singa-
Wei.2023. In-contextautoencoderforcontextcom-
pore.AssociationforComputationalLinguistics.
pressioninalargelanguagemodel. ArXivpreprint,
abs/2307.06945.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
YebowenHu,TimGanter,HaniehDeilamsalehy,Franck maticevaluationofsummaries. InTextSummariza-
Dernoncourt, Hassan Foroosh, and Fei Liu. 2023. tionBranchesOut,pages74‚Äì81,Barcelona,Spain.
Meetingbank: Abenchmarkdatasetformeetingsum- AssociationforComputationalLinguistics.
marization. ArXivpreprint,abs/2305.17529.
NelsonFLiu,KevinLin,JohnHewitt,AshwinParan-
XijieHuang,LiLynaZhang,Kwang-TingCheng,and jape,MicheleBevilacqua,FabioPetroni,andPercy
MaoYang.2023. Boostingllmreasoning: Pushthe Liang. 2023a. Lost in the middle: How lan-
limitsoffew-shotlearningwithreinforcedin-context guage models use long contexts. ArXiv preprint,
pruning. ArXivpreprint,abs/2312.08901. abs/2307.03172.Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong
Wang,VictorXie,ZhaozhuoXu,AnastasiosKyril- Chen,LianminZheng,RuisiCai,ZhaoSong,Yuan-
lidis, and Anshumali Shrivastava. 2023b. Scis- dongTian,ChristopherRe,ClarkBarrett,Zhangyang
sorhands: Exploitingthepersistenceofimportance Wang,andBeidiChen.2023. H2o: Heavy-hitterora-
hypothesisforLLMKVcachecompressionattest cleforefficientgenerativeinferenceoflargelanguage
time. InThirty-seventhConferenceonNeuralInfor- models. InThirty-seventhConferenceonNeuralIn-
mationProcessingSystems. formationProcessingSystems.
Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. ZhengZhao,ShayB.Cohen,andBonnieWebber.2020.
Learningtocompresspromptswithgisttokens. In Reducingquantityhallucinationsinabstractivesum-
Thirty-seventh Conference on Neural Information marization. InFindingsoftheAssociationforCom-
ProcessingSystems. putationalLinguistics: EMNLP2020,pages2237‚Äì
2249, Online. Association for Computational Lin-
Charles Packer, Vivian Fang, Shishir G Patil, Kevin
guistics.
Lin,SarahWooders,andJosephEGonzalez.2023.
Memgpt: Towardsllmsasoperatingsystems. ArXiv
preprint,abs/2310.08560.
A DetailsofDataDistillation
Allen Roush and Arvind Balaji. 2020. DebateSum:
Toconstructtheextractivecompressiondataset,we
Alarge-scaleargumentminingandsummarization
dataset. InProceedingsofthe7thWorkshoponAr- useGPT-4-32ktocompresstheoriginalmeeting
gumentMining,pages1‚Äì7,Online.Associationfor transcript. Each transcript is divided into chunks
ComputationalLinguistics.
first, with each chunk terminating at the end of a
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be- completesentenceandnotexceeding512tokens.
rant, and Omer Levy. 2023. Zeroscrolls: A zero- We employ the default parameter settings with a
shotbenchmarkforlongtextunderstanding. ArXiv temperatureof0.3andatop_pof1.0. Themax-
preprint,abs/2305.14196.
imum number of generated tokens is set to 4096.
Claude E Shannon. 1951. Prediction and entropy Transcripts exceeding 28K tokens are truncated,
of printed english. Bell system technical journal, allowinga4Ktokenbudgetforgeneration. Fig.9
30(1):50‚Äì64.
presents the full instruction used in GPT-4 com-
Kristina Toutanova, Chris Brockett, Ke M. Tran, and pression. Tab.8 shows the statistics of our Meet-
Saleema Amershi. 2016. A dataset and evaluation ingBankcompressiondataset.
metricsforabstractivecompressionofsentencesand
shortparagraphs. InProceedingsofthe2016Con-
DataPart DataSizeChunkSentence(Avg)Token(Avg) 1/œÑ
ferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages340‚Äì350,Austin,Texas.Associa- Original 5,169 41,746 232 3,635 -
Compressed 5,169 41,746 132 1,415 2.57x
tionforComputationalLinguistics.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten Table8:StatisticsofMeetingBankcompressiondataset.
Bosma,brianichter,FeiXia,EdH.Chi,QuocVLe,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In B DetailsofDataAnnotation
AdvancesinNeuralInformationProcessingSystems.
Basedonthecompressedprompt,wedesignaword
David Wingate, Mohammad Shoeybi, and Taylor
annotationalgorithmtoautomaticallyassigneach
Sorensen.2022. Promptcompressionandcontrastive
conditioningforcontrollabilityandtoxicityreduction word a label indicating whether the word in the
inlanguagemodels. InFindingsoftheAssociation originalpromptshouldberetained. Initially,allla-
forComputationalLinguistics: EMNLP2022,pages
belsoftheoriginalwordsaresettoFalse. Then,for
5621‚Äì5634,AbuDhabi,UnitedArabEmirates.As-
everywordinthecompressedprompt,wesearch
sociationforComputationalLinguistics.
foritscorrespondingwordintheoriginalprompt,
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
whichisthenassignedaTruelabel.
Han,andMikeLewis.2024. Efficientstreaminglan-
guagemodelswithattentionsinks. InTheTwelfth
Sliding Window: To assign labels to the ap-
International Conference on Learning Representa-
propriate words in the original prompt, we uti-
tions.
lize a sliding window approach, constraining the
FangyuanXu,WeijiaShi,andEunsolChoi.2024. RE- searchscopewithinalocalwindowcenteredonthe
COMP: Improving retrieval-augmented LMs with
previously matched word in the original prompt.
contextcompressionandselectiveaugmentation. In
The search initiates from the last matching posi-
The Twelfth International Conference on Learning
Representations. tion. The True label is then assigned to the firstPromptCompressionDetails:
Example1:
Item15,reportfromCityManagerRecommendationtoadoptthreeresolutions. First,tojointheVictoryPaceprogram.
Second,tojointheCaliforniafirstprogram.Andnumberthree,consentingtotoinclusionofcertainpropertieswithinthe
jurisdictionintheCaliforniaHeroprogram.Itwasemotion,motion,asecondandpubliccomment.CNN.Pleasecastyour
vote.Oh.Wasyourpubliccomment?Yeah.Pleasecomeforward.Ithankyou,Mr.Mayor.Thankyou.Membersofthe
council.MynameisAlexMitchell.Irepresenttheheroprogram.Justwantedtoletyouknowthattheheroprogram.Has
beeninCaliforniaforthelastthreeandahalfyears.We‚Äôrein.Over20.We‚Äôrein28counties,andwe‚Äôvecompletedover
29,000energyefficientprojectstomakehomes.Greenerandmoreenergyefficient.Andthisincludesanything.Fromsolar
towater.Efficiency.We‚Äôvedone.Almost.$550millioninhomeimprovements.
Example2:
John:So,um,I‚Äôvebeenthinkingabouttheproject,youknow,andIbelieveweneedto,uh,makesomechanges.Imean,we
wanttheprojecttosucceed,right?So,like,Ithinkweshouldconsidermayberevisingthetimeline.
Sarah: Itotallyagree,John. Imean,wehavetoberealistic,youknow. Thetimelineis,like,tootight. YouknowwhatI
mean?Weshoulddefinitelyextendit.
Figure 6: LLMLingua-2 performs context awareness compression. The dark red highlights the words which
arepreservedata5xcompressionratio, mediumreddenotes3xcompressionratio, andlightredrepresents2x
compressionratio. Grayindicatesdiscardedwordsduringcompression.
matched word in the original prompt. Further- E PromptReconstruction
more, the search is bidirectional to prevent mis-
Fig.7andFig.8showtworeconstructedprompts
matchescausedbyGPT-4‚Äôsreordering,asshown
fromthecompressedpromptsusingGPT-4. Specif-
inFig.5. Moreover,ifGPT-4introducesnewwords
ically,weprependasimplereconstructioninstruc-
during compression, the sliding window restricts
tion: "I have asked you to compress a meeting
thesearchscope,preventingmismatchesbetween
transcriptbydroppingwordonly. Now,reconstruct
thenewlyaddedwordsinthecompressedprompt
the original meeting transcript based on the fol-
andwordsintheoriginalprompt.
lowingcompressedtranscript."tothecompressed
prompt. With the key information preserved in
FuzzyMatching: Anotherchallengearisesfrom
thecompressedprompt,thereconstructedprompt
thefactthatGPT-4mayaltertheoriginalwordsin
closelyresemblestheoriginalprompt.
tense,voice,andsingular/pluralformsduringcom-
pression,evenwhenwerequestGPT-4tocompress
F DetailsofMeetingBankQAand
bydiscardingwordsonly. Toaddressthisissue,we
MeetingBankSummary
firstapplylemmatizationtoreducewordstotheir
base form using Spacy5, and then perform word TheMeetingBankQAdatasetconsistsof862meet-
matchingusingtheslidingwindowapproach. ingtranscriptsfromtheMeetingBanktestset. Ini-
tially, we generate 10 question-answer pairs for
C ContextAwareCompression eachmeetingtranscriptusingGPT-4-32K.Thein-
structionusedingeneratingQApairsis: "Create
Fig.6 presents some compression results of our 10questions/answerpairsfromthegivenmeeting
LLMLingua-2underdifferentcompressionratios. transcript. The answer should be short and con-
Ourmethodeffectivelymaintainsthemostmean- cise. ThequestionshouldstartwithQ:andanswser
ingfulwordsasthecompressionratioincreases. shouldstartwithA:. Themeetingtranscriptisas
follows.". To ensure the quality of the generated
D ComparisonwithBaselines
QA pairs, we discard the question-answer pairs
withanswerlengthsexceeding50tokens. Subse-
InFig.11andFig.12,wequalitativelycomparethe
quently, we carefully examine the remaining QA
compressedpromptsofourmethodswiththoseof
pairs to ensure that the answers actually appear
baseline method on GSM8K and BBH datasets.
in the original transcripts, instead of being prod-
Note our LLMLingua-2 here is only trained on
uctsofGPT-4‚Äôshallucinations. Aftertheaforemen-
MeetingBank,butalsoyieldsmorereasonablecom-
tioned filtering process, we retain 3 high-quality
pressedpromptthanbaselinemethodsonthetrans-
question-answerpairsforeachmeetingtranscript.
ferreddomaindata.
Additionally,weinstructGPT-4-32Ktosummarize
5https://spacy.io/api/lemmatizer eachmeetingtranscript. ThesummariesgeneratedOriginalPrompt(200Tokens):
Item15,reportfromCityManagerRecommendationtoadoptthreeresolutions. First,tojointheVictoryPaceprogram.
Second,tojointheCaliforniafirstprogram.Andnumberthree,consentingtotoinclusionofcertainpropertieswithinthe
jurisdictionintheCaliforniaHeroprogram.Itwasemotion,motion,asecondandpubliccomment.CNN.Pleasecastyour
vote.Oh.Wasyourpubliccomment?Yeah.Pleasecomeforward.Ithankyou,Mr.Mayor.Thankyou.Membersofthe
council.MynameisAlexMitchell.Irepresenttheheroprogram.Justwantedtoletyouknowthattheheroprogram.Has
beeninCaliforniaforthelastthreeandahalfyears.We‚Äôrein.Over20.We‚Äôrein28counties,andwe‚Äôvecompletedover
29,000energyefficientprojectstomakehomes.Greenerandmoreenergyefficient.Andthisincludesanything.Fromsolar
towater.Efficiency.We‚Äôvedone.Almost.$550millioninhomeimprovements.
CompressedPrompt(98Tokens,2xCompression):
Item15reportCityManagerRecommendationadoptthreeresolutions.joinVictoryPaceprogram.SecondjoinCalifornia
firstprogram. threeinclusionpropertiesjurisdictionCaliforniaHeroprogram. emotionmotionsecondpubliccomment.
CNN.castvote.publiccomment?.comeforward.thankyouMr.Mayor.Memberscouncil.AlexMitchell.representhero
program.Californiathreeandahalfyears.20.28countiescompleted29,000energyefficientprojects.includes.solarwater.
EfficiencyAlmost$550millionhomeimprovements.
Reconstructedprompt(198Tokens):
Itemnumber15ontheagendaisthereportfromtheCityManager.TheRecommendationistoadoptthreeresolutions.The
firstistojointheVictoryPaceprogram.ThesecondresolutionistojointheCaliforniaFirstprogram.Thethirdresolutionis
fortheinclusionofpropertieswithinourjurisdictionintotheCaliforniaHeroprogram.Thereisanemotionforthemotion
andasecond.Wewillnowopenthefloorforpubliccomment.WehavearepresentativefromCNNhere.Arethereanyother
individualswhowouldliketocastavoteormakeapubliccomment?Ifso,pleasecomeforward.
Thankyou, Mr. MayorandMembersofthecouncil. MynameisAlexMitchellandIrepresenttheHeroprogramin
California.Ihavebeenwiththeprogramforthreeandahalfyears.Asofnow,20outof28countieshavecompleted29,000
energyefficientprojects.Thisincludesprojectsrelatedtosolarwaterandenergyefficiency.Almost$550millionhasbeen
investedinhomeimprovements.
Figure7: PromptreconstructionresultsofourLLMLingua-2ontheMeetingBankexample.
OriginalPrompt(160Tokens):
Okay. Thankyou. Nextstepiswe‚Äôregoingtodoitemnumber,isitthatwas16. SoIcoulddoitem16. We‚Äôlltrytoget
throughtheseasexpeditiouslyaspossible.Andthere‚Äôsgoingtobeaamotionthat‚Äôsreadytogohere.Socanwethethe
theitemplease. Reportfromcityclerkrecommendationtoreceiveandfilethecertificationofthepetitionregardingthe
regulationofmedicalmarijuanabusinessesandapproveoneofthefollowingthreealternativeactionsadopttheinitiative
ordinancewithoutalterationtosubmittheinitiativeordinancewithoutalterationtothevoterstobeheldonNovember8th,
2016orthree.AdoptareportpursuanttoCaliforniaStateElectionsCode.Thankyou.There‚Äôsamotionandaseconddevice.
MarieLowenthal.Thankyou.
CompressedPrompt(83Tokens,2xcompression):
Okay.Thankyou.Nextstepdoitemnumber,16.item16.trygetthroughexpeditiously.motionreadygohere.item.Report
fromcityclerkreceivefilecertificationofpetitionregulationmedicalmarijuanabusinessesapprovethreealternativeactions
adoptinitiativeordinancewithoutalterationsubmittovotersNovember8th,2016orthree.AdoptreportCaliforniaState
ElectionsCode.motionseconddevice.MarieLowenthal.
ReconstructedPrompt(123Tokens):
Okay.Thankyou.Thenextstepistodoitemnumber16.Foritem16,wewilltrytogetthroughthisexpeditiously.The
motionisreadytogofromhere.Theitemisareportfromthecityclerktoreceiveandfilethecertificationofapetitionfor
theregulationofmedicalmarijuanabusinesses. Wecanapprovethreealternativeactions: adopttheinitiativeordinance
withoutalteration,submitittothevotersonNovember8th,2016,orthree,adoptthereportaspertheCaliforniaState
ElectionsCode.Themotionissecondedbythedevice.MarieLowenthal.
Figure8: PromptreconstructionresultsofourLLMLingua-2ontheMeetingBankexample.
byGPT-4areusedasgroundtruthtoevaluatethe lackingdetailedinformation. Thisinformationloss
summaryperformance. inevitably hinders the downstream tasks such as
document-basedQA,asillustratedinFig.13and
G DrawbackofExistingText
Fig.14
CompressionDataset
H ModelSizeandTrainingDetails
Existing extractive compression datasets such as
SentComp(FilippovaandAltun,2013)andDebate- We use xlm-roberta-large which has
Sum(RoushandBalaji,2020)aremainlycreated 355M parameters as the feature encoder f in
Œ∏
forsummarizationtask. Thecompressedtextspro- LLMLingua-2. The training process takes ap-
videdintheirdatasetareusuallytooconcise,only proximately 23 hours on our MeetingBank com-
maintainingthemainideaoftheoriginaltextand pression dataset. For LLMLingua-2-small,OurGPT-4InstructionforCompression:
SystemPrompt:
Youareanexcellentlinguistandverygoodatcompressingpassagesintoshortexpressionsbyremovingunimportantwords,
whileretainingasmuchinformationaspossible.
UserPrompt:
Compressthegiventexttoshortexpressions,andsuchthatyou(GPT-4)canreconstructitascloseaspossibletotheoriginal.
Unliketheusualtextcompression,Ineedyoutocomplywiththe5conditionsbelow:
1.YoucanONLYremoveunimportantwords.
2.Donotreordertheoriginalwords.
3.Donotchangetheoriginalwords.
4.Donotuseabbreviationsoremojis.
5.Donotaddnewwordsorsymbols.
Compresstheoriginaggressivelybyremovingwordsonly.Compresstheoriginasshortasyoucan,whileretainingasmuch
informationaspossible.Ifyouunderstand,pleasecompressthefollowingtext:{texttocompress}
Thecompressedtextis:
Figure9: TheinstructionweusedinGPT-4compression.
Instruction1:
Couldyoupleaserephrasetheparagraphtomakeitshort,andkeep5%tokens?
Instruction2:
Summarizetheprovidedexamplesinafewsentences,maintainingallessentialreasoningaspects.
Instruction3:
RemoveredundancyandexpressthetextconciselyinEnglish,ensuringthatallkeyinformationandreasoningprocessesare
preserved.
Instruction4:
Followthesestepstoshortenthegiventextcontent:1.First,calculatetheamountofinformationcontainedineachsentence,
andremovesentenceswithlessinformation. 2. Next, furthercondensethetextbyremovingstopwords, unnecessary
punctuation,andredundantexpressions.Refinethecontentwhileensuringthatallkeyinformationisretained.Let‚Äôsdoit
stepbystep.
Figure10: Otherinstructionsweevaluated,whichareproposedinLLMLingua(Jiangetal.,2023a).
thefeatureencoderisthemultilingual-BERT ormultilingual-BERTcompressoracquired
which has 110M parameters. It takes roughly 16 fromthepre-trainingphase.
hourstotrainthemultilingual-BERTmodel.
K IntegrationwithLongLLMLingua
I GPUMemoryUsage
In retrieval-augmented generation (RAG) and
LLMLingua-2enjoysasmallerGPUmemoryover-
Multi-DocumentsQuestion-Answer(MDQA)sce-
head because of its lightweight. The peak GPU
narios,theprimarychallengeistoidentifythedoc-
memoryusageofLLMLingua-2onMeetingBank
ument that contains the key information relevant
is only 2.1GB, while LLMLingua and Selective-
tothequestion. Inthesescenarios,LongLLMLin-
Context,whichutilizeLLAMA-2-7BastheSLM,
guaimprovesthekeyinformationpreservationby
consume 16.6GB and 26.5GB of GPU memory,
utilizingtheinformationprovidedinthequestion.
respectively.
While LLMLingua-2 is designed for question-
agnosticcompression,itcanalsobeintegratedwith
J MultilingualGeneralizationAbility
LongLLMLinguatopreservemorekeyinformation
In Table 9, we assess the performance of relevanttothequestioninthesescenarios. Specifi-
LLMLingua-2 on the Chinese benchmarks of cally,weutilizeLongLLMLingua‚Äôscoarse-grained
LongBench, comprising 5 tasks with a total of compressiontoassignvaryingcompressionratios
1000 samples. Despite being trained solely on to different documents based on the question‚Äôs
the MeetingBank data, which consists of En- perplexity conditioned on each document. Con-
glishcorpusonly,LLMLingua-2alsooutperforms sequently, it allocates more token budgets to the
LLMLingua on Chinese benchmarks. We at- documentswhicharemorerelevanttothequestion.
tribute this performance gain to the multilin- As illustrated in Table 11, LLMLingua-2
gual capabilities of the xlm-roberta-large withLongLLMLinguacoarse-grainedcompressionOriginalPrompt(139tokens):
Q:Ihaveablackberry,aclarinet,anectarine,aplum,astrawberry,abanana,aflute,anorange,andaviolin.Howmany
fruitsdoIhave?
A:Let‚Äôsthinkstepbystep.
Wefirstidentifythefruitsonthelistandincludetheirquantityinparentheses:
-blackberry(1)-nectarine(1)-plum(1)-strawberry(1)-banana(1)-orange(1)
Now,let‚Äôsaddthenumbersinparentheses:1+1+1+1+1+1=6.Sotheansweris6.
Compressedprompt(57tokens)byLLMLingua:
:ablackberry,aaneaaaa,manyhave
:‚Äôsthink
Wefirsttheruitsthelistandincludetheirin-(‚Äì
‚Äôsthenumbersinparentheses:1+1=6.Sotheansweris6.
Compressedprompt(54tokens)byLLMLingua-2:
Q:clarinet,nectarine,strawberry,violin.
Howmanyfruits
thinkstepbystep.
identifyfruitsincludequantityparentheses:
blackberrynectarineplumstrawberrybananaorangeaddnumbersparentheses:1+1=6.
answeris6.
Figure11: Comparisonwithbaseline. LLMLingua-2hereisonlytrainedonMeetingBank,butalsoyieldsmore
reasonablecompressedpromptthanLLMLinguaonBBH.
achievesanaverageperformancegainof25.3%on
NaturalQuestions(Liuetal.,2023a)comparedto
LLMLingua-2.
L Sample-WiseDynamicCompression
Ratio
By default, LLMLingua-2 applies fixed compres-
sion rate to all samples in the benchmark. How-
ever,thisapproachmaynotbeoptimalduetovari-
ationsinthedensityofkeyinformationacrossdif-
ferentsamples. Toaddressthisproblem,weallow
LLMLingua-2todynamicallyadjustthecompres-
sion rate for each sample under the overall com-
pression rate constraint. Specifically, we employ
thecompressortopredicteachtoken‚Äôspreservation
probability of all samples. We then set a proba-
bilitythresholdtoachievetheoverallcompression
rateconstraint. Forallsamples,tokenswithpreser-
vationprobabilitieshigherthanthisthresholdare
retained.
Table 12 presents the performance of
LLMLingua-2 using the sample-wise dynamic
compression ratio, showcasing a 4.4% and 4.5%
performance improvement under 7x and 5x
compression ratios, respectively, compared to
LLMLingua-2withafixedcompressionratio.OriginalPrompt(249tokens):
Question:Samboughtadozenboxes,eachwith30highlighterpensinside,for$10eachbox.Herearrangedfiveofthese
boxesintopackagesofsixhighlighterseachandsoldthemfor$3perpackage.Hesoldtherestofthehighlightersseparately
attherateofthreepensfor$2.Howmuchprofitdidhemakeintotal,indollars?
Let‚Äôsthinkstepbystep
Sambought12boxesx$10=$120worthofhighlighters.
Hebought12*30=360highlightersintotal.
Samthentook5boxes√ó6highlighters/box=30highlighters.
Hesoldtheseboxesfor5*$3=$15
Aftersellingthese5boxestherewere360-30=330highlightersremaining.
Theseform330/3=110groupsofthreepens.
Hesoldeachofthesegroupsfor$2each,somade110*2=$220fromthem.
Intotal,then,heearned$220+$15=$235.
Sincehisoriginalcostwas$120,heearned$235-$120=$115inprofit.
Theansweris115
Compressedprompt(144tokens)byLLMLingua:
:Samboughtadozenboxeseach30highlpensinside,$10each.Hereangedfiveofboxesintoof
sixeach$3per.Hesoldthetheltersseparatelyattheofthree$2.muchmaketotal,
Letsthinkstep
boughtboxesx0oflters
He23tersin
Samthenboxes6lters/box0ters
Hesoldtheseboxes5
Afterellingtheseboxesthere36030lters
ese00ofthree
soldgroups2eachsomade*2$20from
Intotal,he015
Sincehishe$-$120=$115inprofit.
Theansweris115
Compressedprompt(138tokens)byLLMLingua-2:
Samboughtdozen30highlighterpens$10rearrangedfiveboxesintosixhighlighterssold$3persoldrestthreepensprofit?
Sambought12boxesx$10=$120
12*30=360highlighters
5boxes√ó6highlighters/box=30
sold5*$3=$15
5360-30=330highlighters
330/3=110groupsthree
sold$2110*2=$220
earned$220+$15=$235.originalcostearned$235-$120=$115
Theansweris115
Figure12: Comparisonwithbaseline. LLMLingua-2hereisonlytrainedonMeetingBank,butalsoyieldsmore
reasonablecompressedpromptthanLLMLinguaonGSM8K.
Document:
Chinesegovernmentistoopenmoremuseums,memorialhallsandnationalpatriotismeducationbasestothepublicforfree
amideffortstoupgradeculturalservices.Allnationalmuseumsandprovincialcomprehensivemuseumswillstopcharging
entryfeesthisyear,saysagovernmentcircular.Museumsandmemorialhallslistedasnationalpatriotismeducationbases
willopenforfree,addsthecircular,jointlyissuedbythePublicityDepartmentoftheCommunistPartyofChinaCentral
Committee,theministriesoffinanceandculture,andtheStateAdministrationofCulturalHeritageonJanyary23. Free
entryisalsoavailabletomuseumsabovecountylevelinZhejiang,Fujian,Hubei,Jiangxi,AnhuiandGansuprovincesand
XinjiangUygurAutonomousRegion.Otherprovinces,autonomousregionsandmunicipalitiesareencouragedcutorabolish
entryfeesaccordingtotheircircumstances,thecircularsays.Allmuseums,memorialhallsandnationalpatriotismeducation
baseswillbefreetovisitby2009exceptculturalrelicsandhistoricalsites,whichwillhavecheapratesforminors,the
elderly,soldiers,thedisabledandlow-incomefamilies,saysthecircular. Forspecialorguestexhibitions,museumsand
memorialhallscanchargefees,thecircularsays,andmuseumsareencouragedtohavecheapticketsandflexibleplans,such
asregularfreeentry,andcheapticketsforgroupsandfamilies.
Question:
Inwhichprovinceswillmuseumsabovecountrylevelbeopenforfree?
Figure13: AnexamplefromtheSentCompdataset(FilippovaandAltun,2013). Thecompressedtextishighlighted
inblue. Theprovidedcompressedtextfailstocoverthequestionreferenceswhicharehighlightedinred.Document:
Theoverallresultsregardingthelong-termeffectsofexchangeratevolatilityarehighlyinformativeinrelationtotheexports
andimportsofanLDC.Mexico‚Äôsexportsofagriculturalgoodsareclearlydepressedbyuncertainty:Table3showsthatno
unprocessedagriculturalgoodrespondspositively,whilevariousanimal,vegetable,andwoodproductsmakeup6ofthe21
industrieswithnegativeeffects.Importsarealsoaffected.WhilethecategoryofOil-seeds,oilnuts,andoilkernelsdoes
seemtoincreasebecauseofuncertainty,6ofthe21industriesinwhichvolatilityreducesimportflowsareagriculturalin
nature.Mexicantextileexportsalsoshowclearnegativeeffectsduetouncertainty,notonlyforthecategoryofClothing
exceptfurclothing,butalsofortheinputsofTextileandleathermachineryandTextileyarnandthread(inTable4).
Question:
Whichindustriesoftextilesufferfromnegativeeffectsduetotheexchangerateuncertainty?
Figure14: AnexamplefromtheDebateSumdataset(RoushandBalaji,2020). Thecompressedtextishighlighted
inblue. Theprovidedcompressedtextfailstocoverthequestionreferenceswhicharehighlightedinred.
LongBench-Zh
Methods
SingleDoc MultiDoc Summ. FewShot Synth. AVG Tokens 1/œÑ
Task(Question)-AgnosticCompression
LLMLingua 35.2 20.4 11.8 24.3 51.4 28.6 3060 5x
LLMLingua-2 46.7 23.0 15.3 32.8 72.6 38.1 3023 5x
OriginalPrompt 61.2 28.7 16.0 29.2 77.5 42.5 14940 -
Table9: Out-of-domainevaluationonLongBenchChinesebenchmarks.
QA Summary Length
DataType
F1Score BELU Rouge1 Rouge2 RougeL BERTScore #Tokens 1/œÑ
Filtered 58.71 17.74 48.42 23.71 34.36 88.99 1629 3.3x
Annotated Kept 92.82 19.53 50.24 25.16 36.38 89.05 855 2.9x
All 86.30 19.17 49.89 24.90 35.97 89.04 1003 3.0x
Filtered 59.65 20.53 46.39 25.31 34.17 88.91 5298 -
Original Kept 94.41 23.05 47.73 27.20 35.74 88.99 2461 -
All 87.75 22.34 47.28 26.66 35.15 88.96 3,003 -
Table10: Ablationstudyofthefilteringprocessindatasetconstruction. Annotatedgathersallwordswhichare
assignedaTruelabelbyourannotationalgorithmastheinputprompt. Filtereddenotesthediscardsamplesofthe
filteringprocessinsec3.3,whileKeptrepresentstheretainedsamples.Methods 1st 5th 10th 15th 20th Reorder Tokens 1/œÑ
4xconstraint
Question-AwareCompression
BM25‚Ä† 40.6 38.6 38.2 37.4 36.6 36.3 798 3.7x
Gzip‚Ä† 63.1 61.0 59.8 61.1 60.1 62.3 824 3.6x
SBERT‚Ä† 66.9 61.1 59.0 61.2 60.3 64.4 808 3.6x
OpenAI‚Ä† 63.8 64.6 65.4 64.1 63.7 63.7 804 3.7x
LLMLingua-2+ 74.0 70.4 67.0 66.9 65.3 71.9 739 3.9x
LongLLMLingua‚Ä† 75.0 71.8 71.2 71.2 74.7 75.5 748 3.9x
Question-AgnosticCompression
Selective-Context‚Ä† 31.4 19.5 24.7 24.1 43.8 - 791 3.7x
LLMLingua‚Ä† 25.5 27.5 23.5 26.5 30.0 27.0 775 3.8x
LLMLingua2 48.6 44.5 43.6 40.9 39.9 46.2 748 3.9x
OriginalPrompt 75.7 57.3 54.1 55.4 63.1 - 2,946 -
Zero-shot 56.1 15 196x
Table11:PerformancecomparisononNaturalQuestions(20documents)(Liuetal.,2023a). LLMLingua-2+denotes
LLMLingua-2withLongLLMLingua(Jiangetal.,2023b)coarselevelcompression. ‚Ä†: numbersreportedinJiang
etal.(2023b).
LongBench-SingleDoc
Methods
QAScore Tokens 1/œÑ QAScore Tokens 1/œÑ
TargetTokenConstraint 2000Tokens 3000Tokens
LLMLingua2 29.8 1954 7.4x 35.5 3392 4.3x
CompressionRatioConstraint 7x 5x
LLMLingua2FR‚Ä† 25.1 2131 6.8x 27.4 3185 4.5x
LLMLingua2DCR‚Ä° 29.5 2125 6.8x 32.2 3164 4.5x
OriginalPrompt 39.7 14,511 1x 39.7 14,511 1x
Table12: EvaluationofLLMLingua-2samplewisedynamiccompressiononLongBenchsingledocQAtask. FR‚Ä†
assignseachexamplewiththesamefixedcompressionrate. DCR‚Ä°assignsdynamiccompressionratetodifferent
exampleswithinthecorpuslevelconstraint.