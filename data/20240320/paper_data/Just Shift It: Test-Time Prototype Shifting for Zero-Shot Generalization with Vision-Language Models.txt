Just Shift It: Test-Time Prototype Shifting for
Zero-Shot Generalization with Vision-Language
Models
Elaine Sui, Xiaohan Wang, and Serena Yeung-Levy
Stanford University
{esui,xhanwang,syyeung}@stanford.edu
Abstract. Advancements in vision-language models (VLMs) have pro-
pelled the field of computer vision, particularly in the zero-shot learn-
ing setting. Despite their promise, the effectiveness of these models of-
ten diminishes due to domain shifts in test environments. To address
this, we introduce the Test-Time Prototype Shifting (TPS) frame-
work, a pioneering approach designed to adapt VLMs to test datasets
usingunlabeledtestinputs.Ourmethodisbasedonthenotionofmod-
ulating per-class prototypes in the shared embedding space. By pre-
computing and caching prototypes generated with the pre-trained text
encoder, TPS not only facilitates optimization-free prototype reuse for
subsequent predictions but also enables seamless integration with cur-
rent advancements in prompt engineering. At test-time, TPS dynami-
cally learns shift vectors for each prototype based solely on the given
test sample, effectively bridging the domain gap and enhancing classi-
fication accuracy. A notable aspect of our framework is its significantly
reduced memory and computational demands when compared to con-
ventional text-prompt tuning methods. Extensive evaluations across 15
datasets involving natural distribution shifts and cross-dataset general-
izationdemonstrateTPS’ssuperiorperformance,achievingstate-of-the-
art results while reducing resource requirements. Code is available at
https://github.com/elaine-sui/TPS.
Keywords: test-time adaptation · vision-language models · zero-shot
1 Introduction
In recent years, the field of computer vision has witnessed remarkable progress,
largely fueled by the emergence of robust vision-language foundation models [9,
38].Thesemodels,uponundergoingpre-trainingonlarge-scaledatasets,acquire
a deep understanding of visual concepts, enabling their seamless application to
a range of downstream tasks without task-specific training. While these foun-
dation models exhibit much better zero-shot generalization abilities compared
to ImageNet pre-trained models, they still suffer from performance degradation
due to domain shifts at test-time.
Toaddressthisproblem,priorstudieshaveexploredvariousfine-tuningtech-
niques, ranging from traditional full-model tuning to more parameter-efficient
4202
raM
91
]VC.sc[
1v25921.3042:viXra2 E. Sui et al.
Gradients
Gradients
Cat EnT ce ox dt er MiE nin mtr io zp ay tion MiE nin mtr io zp ay tion
Dog
Pre-Cached
Learnable Prompt Bird Prototypes
image embeddings image embeddings
(a) Test-TimePromptTuning (b) Test-TimePrototypeShifting
Fig.1:ComparisonofTest-TimePromptTuning(TPT)[43]againstourmethod,Test-
TimePrototypeShifting(TPS).TPTrequiresgradientstobackpropagatethroughthe
large text encoder in order to reach the tuneable prompt, incurring high memory and
computational costs. In contrast, TPS only backpropagates gradients to the feature
space, in which our class prototype shifts are learned, making it much more efficient.
methods such as tuning additional adapters [8,16] or input prompts [20,22].
Nevertheless, these strategies necessitate having enough labeled data for effec-
tive fine-tuning, posing challenges in domains where acquiring labeled data is
difficult. In light of this, we turn our attention to the emerging paradigm of
test-time adaptation (TTA), where model parameters are adjusted in an unsu-
pervised manner with unlabeled test inputs.
How can we effectively enable test-time adaptation using these advanced
vision-languagemodels(VLMs)?Therehavebeenrelativelyfewinitiativesinthis
area. A notable example is the Test-Time Prompt Tuning (TPT) method [43],
whichsuggestsfine-tuningseveralprompttokensofthetextinputforeachindi-
vidual unlabeled test sample. However, this approach faces practical constraints
due to the significant memory and computational demands involved in back-
propagating through the text encoder for each image.
In this work, we propose Test-Time Prototype Shifting (TPS), a sim-
ple yet effective framework that specifically adjusts per-class prototypes within
the embedding space. Initially, we compute each class prototype using the pre-
trained text encoder from a VLM, enabling the prototypes to be cached and
reused for all subsequent predictions. At test-time, we adapt by learning a shift
vector for each prototype on the fly for a single test sample, bridging the do-
main gap between the prototypes and the target sample. A key highlight of our
framework is that the shift vectors are the only parameters being updated and
areadjustedwithintheembeddingspaceitself,circumventingtheneedforback-
propagation through the text and visual encoders. In comparison to TPT, our
TPS framework achieves a 10 times increase in speed while necessitating less
than 1/10 of the memory cost.
Benefiting fromthe two-stage learning paradigm,our approachfullycapital-
izes on the advancements in prompt engineering. Prototypes can be generated
using vanilla templates such as “a photo of a {class}.", or crafted using more
sophisticated, well-designed prompts [30,38,52] that significantly enhance gen-
eralizationcapabilities.Furthermore,experimentswithfew-shotlearningonIm-
ageNet reveal that learnable prompts are highly effective in addressing natural
domain shifts [56,57]. In our study, we build upon these previous advancementsJust Shift It 3
by generating improved prototypes that can be seamlessly integrated into our
Test-Time Prototype Shifting framework.
The zero-shot generalization capabilities of TPS were extensively evaluated
across two distinct series of datasets: those involving natural distribution shifts
andthosefocusedoncross-datasetgeneralization.OurTPSconsistentlyoutper-
forms CLIP baselines, and surpasses current state-of-the-art (SoTA) by 3.3%
on the natural distribution shifts benchmark and 1.9% on the cross-dataset
generalization benchmark, respectively. Moreover, we demonstrate that regard-
less of the prototype-generation approach, learning a feature-space shift on the
prototypes consistently boosts performance over zero-shot CLIP by over 4% on
natural distribution shifts and up to 1% on cross-dataset generalization bench-
marks.Remarkably,ourapproachnotonlyout-performedTPTintermsoftop-1
accuracy but also achieved this with only 1/10 of the memory and time costs.
Our main contributions are summarized as follows:
1. We introduce the Test-time Prototype Shifting (TPS) framework, a novel,
straightforward and efficient approach. This is, to our knowledge, the first
instance of utilizing feature space modulation for test-time adaptation with
VLMs.
2. Our TPS framework is designed to seamlessly integrate with existing ad-
vancements in prompt engineering, transforming it into a flexible plug-and-
play system.
3. We achieve state-of-the-art performance on both natural distribution shift
and cross-dataset generalization benchmarks, surpassing current SoTA by
3.3%and1.9%,respectively.Additionally,ourapproachsignificantlyreduces
computational and memory demands by more than 10 times compared to
TPT.
2 Related Works
2.1 Test-Time Adaptation
Test-timeadaptation(TTA)isthetaskofadaptingamodel’sweightsonanun-
labeled out-of-distribution test set in order to achieve higher test performance.
Inthecontextofvisiontasks,traditionalmethodsleverageImageNet-pretrained
image classifiers and use techniques such as computing pseudo-prototype class
representations to update the linear classifier [18], learning better feature repre-
sentations through self-supervised auxiliary tasks [24,26,46], adapting the nor-
malizationlayerstolearnthestatisticsofthetargetdistribution[32,33,41,44,49],
as well as minimizing prediction entropy to increase the confidence of predic-
tions [10,32,33,49,53]. With the development of CLIP [38], recent work in TTA
has been predominantly based on prompt tuning. This involves learning a tune-
able text and/or image prompt to encode the visual distribution shift while
maintaining the strong performances of these foundation models by keeping the
pre-trained parameters fixed [7,12,28,43,55]. Despite only tuning a relatively4 E. Sui et al.
small number of prompt parameters, tuning the input requires backpropagat-
ingthroughtheirrespectiveencoders,whichisespeciallymemoryintensivewith
large input sizes, making it infeasible in practice. Recently, an image diffusion-
basedmethodDiffusionTTA[37]hasbeenproposedforTTA,involvingupdating
classifierweightsbytrainingfortheauxiliarytaskofimagereconstructionusing
a conditional diffusion model. Nevertheless, this also necessitates backpropaga-
tion through the classifier and diffusion model. In contrast, this work proposes
toavoidbackpropagationthroughtheencodersandmaintaintherichnessofthe
CLIP embedding space by directly modulating the features within it.
Current TTA methods also comprise those that are considered training-
free [11,47]. Approaches include adding a parameter-free attention module to
modulate multi-modal features [11] and computing the similarity between the
target image and those from a constructed support set [47]. Although our work
doesnotdirectlyfitthissetting,wefollowthesamespiritofminimallyadjusting
intermediate representations to close the domain gap.
2.2 Feature Modulation
Feature modulation is a parameter-efficient tuning paradigm where features are
perturbed to better conform to a target task. This learned perturbation is typi-
callyintheformoffeaturenormalization,achievedbymodulatingtheencoder’s
normalization layers to align source and target tasks [3,17,19,36,48]. However,
modulationcanalsobeappliedmoredirectlytothefeaturesthemselves[23,58].
Forexample,SSF[23]proposestolearnscaleandshiftparametersforeachlayer’s
activations.DN[58]proposestosubtractthemeansofthetextandimageembed-
dings from the respective inputs before computing CLIP similarity to align the
CLIP training and inference procedures. We propose a more simplistic feature
modulation procedure where we only learn shift vectors to pre-computed class
prototypesintest-timetrainingtobetteralignthemwiththeout-of-distribution
image embeddings of the target dataset.
2.3 Prompting for Vision-Language Models
Vision-language models enable zero-shot generalization to downstream datasets
via prompting. As predictions are computed by cosine similarity of the text and
image embeddings, the quality of the text embeddings or class prototypes can
cause a drastic difference in performance. In the case of image classification,
this entails the careful design of natural language text descriptions for each of
the class names, focusing on the visual aspects apparent from the image it-
self [38]. CoOp [56] removes the need for hand-crafting prompts by prompt-
tuning in the few-shot setting and CoCoOp [57] extends CoOp by learning
instance-conditionedprompts,improvinggeneralizationability.Anotherparadigm
ofprompt-engineeringincludespromptinglargelanguagemodels(LLMs)forbet-
ter prompt templates [25] and/or content [30,52]. Specifically, Menon and Von-
drick [30] and Yang et al. [52] both prompt GPT-3 [2] to generate concepts or
descriptorsofclassnamestoincreasezero-shotandlinear-probeperformanceonJust Shift It 5
1) Prototype Generation 2) Test-Time Shift Tuning
Class-specific descriptors
{} which has/is {descriptor A} Backpropagate
{} which has/is {descriptor B}
... Shift Learner
Text
Class-agnostic templates Encoder
a photo of a nice {}
a good photo of a {}
... Cached
Class
Prototypes
3) Test-Time Inference
Shift Learner
Image
Encoder
Image
Encoder
Average top-k
Fig.2: Weillustratethethreestagesof Test-Time Prototype Shifting (TPS).1)
Prototype Generation:pre-computationofclassprototypesusingdifferentprompt-
engineering strategies. We show the computation of k class-conditioned descriptors
for a single class. Means are computed and cached. 2) Test-Time Shift Tuning:
one iteration of test-time training where we tune the Shift Learner to generate small
perturbations to the class prototypes to close the gap between the source and tar-
get distributions. Marginal entropy of the CLIP similarities of the shifted prototypes
andaugmentedimageembeddingsisminimized.3) Test-Time Inference:Usingthe
tuned Shift Learner, we compute the final prediction for the shifted class prototypes
and the original image embedding with CLIP similarity.
image classification while providing model interpretability. Our work leverages
these developments in prompt-engineering in our prototype generation phase,
using these techniques to generate more knowledge-rich prototypes that can be
swapped into the framework in a plug-and-play manner.
3 Method
3.1 Background
Test-Time Adaptation. Let f be a model trained on data from some source
θ
distribution and D be some unlabeled out-of-distribution test dataset. Then,
test
the goal of test-time adaptation is to adapt θ such that the performance of f
θ′
onD increaseswithouttheuseofground-truthtestlabels.Following[43,54],
test
we consider the single-input TTA setting where f is adapted independently for
θ
every test example d∈D .
test
ImageClassificationwithCLIP. CLIP[38]isalarge-scalevision-languagemodel
pre-trained by contrasting millions of (image, text) pairs. It consists of a text
encoder f and an image encoder f . To classify an image v with CLIP, we use
T V
theCLIPpre-trainedimageencoderf toobtainavisualfeaturerepresentation
V
x = f (v). Then, each label in the target label set c ∈ C is transformed
V target
with prompt template p. Each prompt is embedded with the pre-trained text6 E. Sui et al.
encoder f to produce prototypes p = f (p(c)). The cosine similarities are
T c T
computed between all class prototypes {p } and the visual embedding {x}
c c∈C
to get a probability distribution over all classes.
3.2 Test-Time Prototype Shifting
Ourmethodcomprisesthreestages:PrototypeGeneration,Test-TimeShiftTun-
ing,andTest-TimeInference,asdepictedinFigure2.Initially,inthePrototype
Generation stage, we simply compute the class prototypes by embedding each
class template c ∈ C of the test dataset. The vanilla prompt is “a photo of a
{class}.”,withmoreadvancedtechniquesdiscussedinSection3.2.2.IntheTest-
Time Shift Tuning stage, shift vectors are generated through the Shift Learner
to modify the prototypes (Section 3.2.1), and cosine similarities between aug-
mented image embeddings and shifted class prototypes are used to produce n
probabilitydistributions.WeoptimizetheShiftLearnertominimizetheentropy
oftheaggregatedmarginaldistribution.Finally,intheTest-TimeInferencestage
(Section3.2.3),wepredicttheclassbycomparingthecosinesimilaritiesbetween
thelearnedshiftedtextembeddingsandtheoriginalimageembedding,choosing
the class with the highest probability.
3.2.1 Feature-Space Shift
The Test-Time Prompt Tuning (TPT) method, as proposed by Shu et al. [43],
necessitates backpropagation through the text encoder at every test-time train-
ing step to learn the text prompt vectors. This process results in substantial
computational and memory demands, which significantly hinders its practical
applicability. Essentially, prompt tuning, when used for test-time adaptation,
acts as an indirect technique for adjusting text embeddings. Its primary goal
is to bridge the domain gap, simultaneously leveraging the multi-modal embed-
ded knowledge of CLIP. Such considerations bring us to a pivotal question: why
not directly modulate class prototypes within the embedding space? This has
inspired the development of our Shift Learner module, designed specifically for
learning modifications to the text embeddings. This approach not only capital-
izesontheexceptionalqualityoftheCLIPrepresentationspacebutalsoadeptly
avoids the requirement for the computationally demanding gradient backpropa-
gation through the text encoder.
Rather than indirectly altering embeddings through prompt tuning, our ap-
proach is to directly learn to shift class prototypes within the embedding space.
This strategy enables us to preserve the overarching architecture of the CLIP
embeddingspace,whileadjustingtheclassprototypesforbetteralignmentwith
out-of-distribution visual embeddings. Our key proposition involves learning
shifts for specific class prototypes, as opposed to a universal shift applicable
to all classes. We recognize that domain gaps often consist of both dataset-level
shifts(forexample,transitioningfromnaturalimagestosketches)andclass-level
distribution shifts (such as differences between indoor and outdoor settings for
dogs). A universally applied prototype shift, although effective for dataset-levelJust Shift It 7
adjustments,lacksthenuancedexpressivenessrequiredforclass-levelcorrections.
This is because it alters each prototype identically, maintaining the relative dis-
tances between class prototypes even after test-time training.
Onthecontrary,class-specificshiftshavethecapabilitytoaccommodateboth
levelsofdistributionshifts,enhancingtheirgeneralizabilityacrossvarioustarget
datasets.Althoughadoptingclass-specificshiftsresultsinthenumberoftunable
parameters increasing linearly with the number of classes, this added complex-
ity is minimal compared to the computational requirements of computing and
storing gradients for the sizable text encoder.
Weformallyelaborateontheoperationofourper-classshiftasfollows,given
class c ∈ C and corresponding class prototype p ∈ Rd, we learn a shift vector
c
s ∈ Rd. The shift operation is performed by channel-wise addition between
c
the class prototype p and the learned shift vector s . The normalized shifted
c c
prototype p′ ∈Rd is generated as follows,
c
p +s
p′ = c c (1)
c ||p +s ||
c c 2
3.2.2 Advanced Prototype Generation
While it is effective to enhance the zero-shot generalization capabilities of
vision-languagemodels(VLMs)throughtest-timetraining,anotherlineoftech-
niques focuses on crafting sophisticated prompts to elevate performance. Our
Test-Time Prototype Shifting framework is uniquely positioned in this land-
scape. By learning shifts on pre-computed and cached class prototypes, TPS
is designed to be seamlessly compatible with any existing prompt-engineering
methods. This integration empowers the generation of more robust and effec-
tive prototypes, leveraging advanced prompting strategies and offline prototype
adjustment.
In our research, we concentrate on unsupervised test-time adaptation. Con-
sequently, we operate under the assumption that test images are unavailable
before they are individually encountered at test-time, and specific knowledge
about the type of domain shift is absent. For fair comparisons, we refrain from
usingtarget-domain-specificprompts.Forinstance,weavoidemployingprompts
such as “a photo of a sketch {class}” exclusively for ImageNet Sketch, as this
would imply prior knowledge of the domain shift type, which is contrary to our
testing conditions. In this work, we employ three distinct prompt techniques:
class-agnostic prompts using the set of 80 hand-crafted templates provided by
CLIP [38] for ImageNet, class-specific prompts derived from intricate class de-
scriptions generated by GPT-4 [34], and learnable prompts such as CoOp [56]
and CoCoOp [57].
Aggregating class-specific representations. Our framework requires a
singlerepresentationperclass,butifsucharepresentationisderivedfromasin-
gleprompt,theamountofinformationthatitcarriesislimitedtothenumberof
input tokens that the CLIP text encoder was trained with. Hence, it would be8 E. Sui et al.
Algorithm 1 Test-Time Prototype Shifting (TPS)
Require: pre-trained and frozen image encoder f from a VLM, pre-computed class
V
prototypes p and trainable shift parameters s for c ∈ C, test image v , set of
c c 0
augmentations A, number of augmentations (n−1)
1:
2: function train(v , A, f , {p } , {s } )
0 V c c∈C c c∈C
3: Sample a ,...,a ∈U(A)
1 n−1
4: v =a (v ), x =f (v ) for i∈{0,...,n−1}
i i 0 i V i
5: p′ =(p +s )/||p +s || for c∈C
c c c c c 2
6: Compute p(c|x ,p′) by Eq 2
i c
7: Compute p˜= 1(cid:80)k p(c|x′,p′) for x′ in top-k
k i=1 i c i
8: Compute L by Eq 3
9: Compute ∂L
10: Update {s }
c c∈C
11: end function
12:
13: function test(v , f , {p } , {s } )
0 V c c∈C c c∈C
14: x =f (v )
0 V 0
15: p′ =(p +s )/||p +s || for c∈C
c c c c c 2
16: Compute estimate p(c|x ,p′) by Eq 2
0 c
17: return argmaxp
18: end function
best to have multiple class-specific representations, each providing complemen-
tary information in order to build a holistic representation of that class.Follow-
ing [30,38], we can easily improve the robustness of class prototypes by taking
the mean of the class-specific embeddings. This allows us to leverage multiple
promptingandprompt-learningtechniquesandretaintheknowledgefromthese
various representations. Further, this allows us to maintain the computational
and memory efficiency of our method without sacrificing prototype representa-
tion strength. We combine the class-agnostic prompts and per-class descriptors
to generate the final prototype set {p } . Several types of combinations are
c c∈C
discussed in the Appendix.
Prototypes as a plug-and-play module.Itisalsoimportanttonotethat
our method is not limited to using these aforementioned prompting strategies
and,providedthattheyareembeddedwithintheCLIPtextembeddingspace,is
agnostic to how the prototypes are generated. In essence, this makes the proto-
typesaflexibleplug-and-playmoduleofourframework,enablingourframework
to take advantage of future advancements in prototype creation.
3.2.3 Test-Time Training and Inference
Attesttime,givenasingletestimagev ,wefollow[43]toaugmentit(n−1)times
0
and compute the features of the original and augmented images with the CLIP
image encoder to obtain embeddings {x }n−1. As introduced in Section 3.2.1,
i i=0
we shift the pre-cached prototypes {p } to obtain {p′} . For each image
c c∈C c c∈CJust Shift It 9
feature x , the predicted probabilities are calculated as
i
exp(p′⊤x /τ)
p(c|x ,p′)= c i (2)
i c (cid:80) exp(p′⊤x /τ)
c∈C c i
where τ denotes the temperature scalar. Similar to TPT [43], we select the k
distributions with highest confidence (i.e. lowest entropy) of the batch and take
the average. Denoting the image embeddings corresponding to the selected k
distributions as {x′}k , we train our model to minimize the following entropy
i i=1
of this marginal distribution,
(cid:88)
L=− p˜(c|x ,p′)logp˜(c|x ,p′) (3)
0 c 0 c
c∈C
k
1 (cid:88)
where p˜(c|x )= p(c|x′,p′) (4)
0 k i c
i=1
In our model, the only parameters that are optimized are the shift vectors
{s } . We update the shift vectors for a single step of gradient descent.
c c∈C
Aftertest-timetraining,weencodetheoriginalimageandcomputeitscosine
similaritywiththeshiftedclassprototypes,resultinginafinalpredictionthatis
theargmaxofthepredictionlogits.Algorithm1summarizestheentireprocedure
of our proposed method, TPS, that enables efficient test-time adaptation using
VLMs.
4 Experimental Results
4.1 Datasets
We evaluate our method TPS on natural distributions shifts and cross-dataset
generalization. For natural distribution shifts, we evaluate ImageNet [5] along
withitsfourvariants:ImageNet-V2[39],ImageNet-A[15],ImageNet-R[14]and
ImageNet-Sketch [50]. For cross-dataset generalization, we evaluate on ten pub-
licly available image classification datasets of different objects and scenes, in-
cludingfine-grainedandspecializeddatasets:Flowers102[31],DTD[4],Oxford-
Pets[35],StanfordCars[21],UCF101[45],Caltech101[6],Food101[1],SUN397[51],
FGVC-Aircraft [29], and EuroSAT [13]. We report Top-1 accuracy for image
classification on all datasets.
4.2 Implementation Details
SimilartoTPT[43],weaugmentatestimage63timeswithrandomresizedcrops
to obtain a batch of 64 images that also includes the original image. We select
10% of samples in the batch with lowest entropy and compute the marginal en-
tropy of the selected predicted probability distributions. We initialize the learn-
able shiftto allzeros andoptimize itfor 1step usingthe AdamW[27] optimizer10 E. Sui et al.
Table 1: Acc@1 of zero-shot image classification with CLIP-ViT-B/16 backbone on
ImageNet and its OOD variants. Performance improvements over zero-shot CLIP are
denoted in (↑blue). Best performances are in bold.
Method ImageNet ImageNet-A ImageNet-V2 ImageNet-R ImageNet- Average OODAverage
Sketch
Zero-ShotBaseline
CLIP-ViT-B/16 66.74 47.79 60.89 73.99 46.12 59.10 57.20
Test-TimeAdaptionBaselines
TPT[43] 68.98 54.77 63.45 77.06 47.94 62.44 60.81
TPT[43]+descriptors* 67.71 54.28 61.24 73.39 46.09 60.54 58.75
TPT+(templates+descriptors)* 69.54 55.13 63.95 77.46 48.34 62.88 61.22
DiffTPT[7] 70.30 55.68 65.10 75.00 46.80 62.28 60.52
Diffusion-TTA[37] 63.8 - - - - - -
Ours(Shift+templates+descriptors) 71.45(↑4.71) 60.61(↑12.82) 64.91(↑4.02) 80.20(↑6.21) 50.88(↑4.76) 65.61(↑6.51) 64.15(↑6.95)
andlearningrateof5e-3forImageNetvariantsand1e-3forcross-datasetgener-
alization.Forourmethod,weinitializeeachclassprototypebytakingthemicro
average of the mean of the class-agnostic CLIP template prompts and the mean
of the class-specific GPT-4 generated descriptive prompts.
4.3 Comparison to State-of-the-Art
4.3.1 Baselines We compare our method with zero-shot and TTA base-
lines that leverage CLIP ViT-B/16 as a backbone. These TTA methods include
TPT [43] which performs text-prompt tuning, DiffTPT [7], a variant of TPT
that uses diffusion models to augment the visual training data, and Diffusion-
TTA [37], a method that adapts a discriminative classifier via a conditional
diffusion model. To make our method more comparable to the simplest base-
lines, we also add their versions using more advanced prototypes, such as the
inclusion of class-agnostic CLIP templates [38] (+ templates), class-specific
LLM-generated descriptors (+ descriptors), and learned CoOp [56] prompts
(+ CoOp).
Given that Test-Time Prompt Tuning (TPT) does not involve tuning in
the feature space, our advanced prompt generation is constrained to enhancing
the input of TPT’s text encoder. To address this, we append descriptors to
the prompt suffixes, denoted as + descriptors*. Moreover, a limitation arises
with TPT’s methodology of initializing its learnable prompt using a singular
prompttemplate.Thisrestrictsthepotentialforintegratingthediversearrayof
CLIPImageNetprompttemplates,astheyarestructuredinvariousformats.As
a result, to further augment the TPT-tuned class prototypes, we take its mean
withthesameadvancedprototypesusedatinitializationinourmethod,denoted
as +(templates + descriptors)*.
4.3.2 Natural Distribution Shifts Table 1 presents the top-1 accuracy of
our method, benchmarked against zero-shot and test-time adaptation (TTA)
baselines using CLIP on ImageNet and its out-of-distribution variants. Our re-
sults demonstrate that shifting class prototypes significantly enhances perfor-
mance.Comparedtothebaselinezero-shotCLIP,weobserveanimprovementof
7%, and a 3.3% increase over the vanilla TPT on average for out-of-distributionJust Shift It 11
Table 2: Acc@1 of zero-shot image classification with CLIP-ViT-B/16 backbone on
ImageNet and its OOD variants using CoOp-learned prompts. Performance improve-
ments over zero-shot CLIP are denoted in (↑blue). Best performances are in bold.
Method ImageNet ImageNet-A ImageNet-V2 ImageNet-R ImageNet- Average OODAverage
Sketch
Zero-ShotBaseline
CLIP-ViT-B/16+CoOp[56] 71.51 49.71 64.20 75.21 47.99 61.72 59.28
Test-TimeAdaptionBaselines
TPT+CoOp([43,56]) 73.61 57.95 66.83 77.27 49.29 64.99 62.83
DiffTPT+CoOp([7,56]) 75.00 58.09 66.80 73.90 49.50 64.12 61.97
Ours(Shift+CoOp[56]) 73.73(↑2.22) 60.49(↑10.78) 66.84(↑2.64) 77.44(↑2.23) 49.08(↑1.09) 65.52(↑3.80) 63.46(↑4.18)
Table 3: Acc@1 of zero-shot image classification with CLIP-ViT-B/16 backbone on
cross-dataset generalization. Performance improvements over zero-shot CLIP are de-
noted in (↑blue). Best performances are in bold.
Method Flower102 DTD Pets Cars UCF101 CalTech101Food101 SUN397 Aircraft EuroSAT Average
Zero-ShotBaseline
CLIP-ViT-B/16 67.28 44.44 87.98 65.24 65.08 92.98 83.80 62.55 23.70 41.42 63.45
Test-TimeAdaptionBaselines
TPT[43] 68.98 47.75 87.79 66.87 68.04 94.16 84.67 65.50 24.78 42.44 65.10
TPT[43]+descriptors* 69.14 51.48 86.05 64.84 70.10 93.59 81.83 65.44 22.29 42.98 64.77
TPT+(templates+descriptors)* 69.71 46.93 87.87 66.77 68.60 94.12 84.94 66.11 23.37 43.17 65.16
DiffTPT[7] 70.10 47.00 88.22 67.01 68.22 92.49 87.23 65.74 25.60 43.13 65.47
Diffusion-TTA[37] 71.5 - 86.1 - - - 88.8 - 24.6 - -
Ours(Shift+templates+descriptors)71.54(↑4.26)50.47(↑6.03)87.35(↓0.63)69.06(↑3.82)71.00(↑5.92) 95.09(↑2.11) 85.23(↑1.43)68.98(↑6.43)26.34(↑2.64)44.48(↑3.06)66.96(↑3.51)
datasets.Furthermore,Table1showsthatdirectlyappendingdescriptorstothe
TPT prompt suffixes results in a performance decrease of 2%, emphasizing the
limitations of TPT in seamlessly incorporating prompt-engineering techniques.
Notably,Table2demonstratesthatourapproachoflearningafeature-basedshift
outperformsTPTandDiffTPTby0.6%and1.5%,respectively,onaverageeven
when using advanced prototypes derived from learned CoOp [56] prompts with-
out backpropagating through the text encoder or prompting a diffusion model.
This finding underscores that feature space modulation can effectively replicate
theimpactoftest-timeprompttuninginscenariosinvolvingnaturaldistribution
shifts.
4.3.3 Cross-Dataset Generalization Table 3 details the performance of
our approach relative to zero-shot and test-time adaptation (TTA) baselines
when generalizing to fine-grained and specialized datasets. Our findings reveal
that the optimal implementation of our method, which involves shifting class
prototypes and integrating both CLIP templates and Large Language Model
(LLM)-generated descriptors, results in an average improvement of 3.5% over
the zero-shot CLIP, and a 1.9% increase compared to TPT.
4.3.4 Comparison to Training-Free Methods In a similar spirit to zero-
shot test-time adaptation, training-free methods perform domain adaptation
without tuning any parameters. We compare our method to state-of-the-art
training-free methods in Table 4. We show that our method, TPS, when us-
ing the same GPT-3-generated [2] text prompts from the official SuS-X [47]12 E. Sui et al.
Table 4: Acc@1 of zero-shot image classification with CLIP-ResNet-50 backbone on
ImageNet and its OOD variants. Best performances are in bold.
Method ImageNet ImageNet-R ImageNet- Cross-Dataset NeedsSupport
Sketch Average Set
SuS-X-SD-C[47] 61.65 61.69 35.88 60.49 ✓
SuS-X-LC-P[47] 61.80 61.62 36.25 60.64 ✓
CALIP[11] 60.57 - - 59.34 ✗
Ours(Shift+SuS-Xdescriptors) 63.52 63.66 37.66 61.47 ✗
20000 19350
700 TPT
Ours (TPS) 17500
600
15000
500
12500
400 10000
300 7500
200 5000
2500 1726
100
0
200 400 600 800 1000 TPT Ours
Number of classes Method
Fig.3: Comparison of computational and memory costs on an A6000 GPU on Ima-
geNet. Left: Average runtimes of TPT and TPS across different sized subsets of Im-
ageNet [5] over 3 runs. Note that error bars are depicted but are not visible as they
have extremely small standard deviations. Right: Memory consumption of TPT and
TPS on ImageNet.
code, out-performs both CALIP [11] and SuS-X [47] without any additional im-
age support set constructed with Stable Diffusion [40] or LAION-5B [42]. This
demonstrates how a simple feature-space shift is more effective than complex
training-freemethods.Forfaircomparison,wecompareTPSwithSuS-Xresults
with fixed hyperparameter settings as ours are not tuned per dataset and use
the same CLIP-ResNet50 backbone.
4.4 Efficiency Analysis
Thepurposeoftest-timeadaptationistotuneamodelonanout-of-distribution
dataset at test-time given a single input or batch at a time. Given that the
input is streaming, each adaptation requires low memory and computational
cost to be used in practice. TPT [43], by design, tunes the input features to the
text encoder. Despite tuning few parameters overall, computing the gradient of
the loss with respect to the prompt parameters necessitates the computation of
gradients for the parameters in entire text encoder. Computing these gradients
)sm(
emiT
hctaB
egarevA
)BiM(
yromeM
UPGJust Shift It 13
Table 5: Acc@1 for zero-shot image classification comparing pure zero-shot baselines
vs.withlearnedfeature-spaceshiftfromprototypesderivedfromvariouspromptsusing
CLIP-ViT/B-16 backbone.
PromptType Setting ImageNet ImageNet Cross-Dataset
OODAverage Average
Vanilla Zero-Shot 66.74 57.20 63.45
+shift 68.77 61.59 64.41
∆ +2.03 +4.39 +0.96
CoOp[56] Zero-Shot 71.51 59.28 N/A
+shift 73.73 63.46 N/A
∆ +2.22 +4.18 N/A
CLIPtemplates Zero-Shot 68.35 59.43 64.69
+shift 70.38 64.04 65.57
∆ +2.03 +4.61 +0.88
Descriptors Zero-Shot 68.52 58.29 66.02
+shift 70.40 62.48 66.80
∆ +1.88 +4.19 +0.78
CLIPtemplates+Descriptors Zero-Shot 69.54 59.88 65.94
+shift 71.45 64.15 66.96
∆ +1.91 +4.27 +1.02
adds to the runtime, scaling linearly with the number of classes in the dataset,
and retaining the encoder gradients adds to the memory requirements.
Figure3showstheaveragebatchruntimeandGPUmemoryconsumptionon
ImageNetforTPSandthesimplestTTAbaselineTPTonasingleA6000GPU.
We show the runtime of TPT and TPS on subsets of ImageNet comprised of
different sized label sets. We observe that the average runtime per test input for
TPTscaleslinearlywiththesizeofthelabelsetwhileourmethod,TPS,remains
constantatapproximately65msperbatchonaverage.OnthefullImageNettest
set, we further see that TPS runs more than 10x as fast and uses less than
10x the memory as TPT, and yet is able to achieve performance gains over
thebaseline.Thesesignificantspeed-upsandlowmemoryconstraintsenableour
method to be easily used in practice.
4.5 Ablation Studies
4.5.1 Effect of Shift on Different Prototypes We explore the effect of
feature-spaceshiftonavarietyofprototypes.Specifically,wecompareourmethod
TPS against zero-shot performance given the same prototypes constructed from
the vanilla “a photo of a {class}” prompt, CoOp [56]-learned prompt, the 80
ImageNet context prompts from CLIP [38] and our LLM-generated descriptors.
As observed in Table 5, regardless of the prototype generation technique used,
introducing even minor perturbations to class prototypes consistently yields an
averagegainof>4%intop-1accuracyonImageNetout-of-distributiondatasets
and up to 1% on cross-domain datasets over zero-shot CLIP with the same pro-
totypes.Thisillustrateshowthestructureoftheembeddingspaceismaintained
with a learnable shift.
4.5.2 Feature-SpaceTransformationVariants Wecomparedifferentvari-
antsforfeature-spacetransformationsinTable6.Specifically,wecompareagainst14 E. Sui et al.
Table 6: Acc@1 of zero-shot image classification comparing different learned feature
transformationsusingCLIP-ViT-B/16backbone.Bestperformancesareinbold.Sec-
ond best performances are underlined.
Method ImageNet ImageNet Cross-Dataset
OODAverage Average
Scale 70.40 61.12 65.96
Shift 71.45 64.15 66.96
Scale&Shift 71.47 64.16 66.91
FiLM 0.09 0.28 2.08
element-wise scale, element-wise scale&shift, as well as FiLM [36] where affine
vectorsarecomputedviaalinearlayerontheprototypes.Weobservethatscale
performsworsethanshift,andscale&shift performsequallytoshift.Ontheother
hand, FiLM suffers from model collapse due to much more learnable weights in
TTA. Considering both efficiency and efficacy, shift is the best choice in our
framework.
5 Conclusion
In this work, we have presented the Test-time Prototype Shifting (TPS) frame-
work, a novel approach to enhancing the zero-shot generalization abilities of
VLMs. TPS addresses the limitations of existing test-time training methods,
such as Text-Prompt Tuning, by directly modulating class prototypes in the
embedding space. This strategy not only reduces the computational and mem-
ory demands significantly but also allows for greater flexibility and precision
in adapting to diverse domain shifts. By leveraging pre-computed and cached
prototypes, and introducing class-specific shifts, TPS effectively bridges domain
gaps.Theextensiveevaluationsconductedacrossmultipledatasetsdemonstrate
the superior performance of our method, outperforming existing approaches in
terms of accuracy while being more computationally efficient. This work not
only sets a new standard for zero-shot generalization in VLMs but also presents
a scalable, efficient solution for real-world applications.
6 Research Impact and Limitations
WeproposeTPS,aframeworkthatcanbeusedtoeasilyandeffectivelyimprove
zero-shot generalization of VLMs. Given the large-scale training of foundation
VLMs,webelieveitisimportanttounderstanddifferentwaystobetterleverage
the resulting rich multi-modal contrastive representation spaces in parameter-
and runtime-constrained settings. We propose to learn a slight perturbation to
the class prototypes to maintain the overall representation quality of the pre-
trained embedding space while learning a better alignment to the OOD target
dataset. We hope that this framework can inspire future work to explore other
tasks where learning directly in the feature space can be an efficient alternative
to more complex tuning approaches.Just Shift It 15
Our work builds on the CLIP [38] representation space and uses GPT-4 [34]
to generate class descriptors to create more advanced class prototypes. Thus,
our model has the potential to magnify the biases of both these models. Future
studies may explore how to best leverage these models’ capabilities without
promoting its biases.16 E. Sui et al.
References
1. Bossard,L.,Guillaumin,M.,VanGool,L.:Food-101–miningdiscriminativecom-
ponentswithrandomforests.In:EuropeanConferenceonComputerVision(2014)
9
2. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,
Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Win-
ter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,
J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language
models are few-shot learners. In: NeurIPS (2020) 4, 11
3. Chen, T., Lucic, M., Houlsby, N., Gelly, S.: On self modulation for generative
adversarial networks. In: International Conference on Learning Representations
(2019) 4
4. Cimpoi,M.,Maji,S.,Kokkinos,I.,Mohamed,S.,,Vedaldi,A.:Describingtextures
in the wild. In: Proceedings of the IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR) (2014) 9
5. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-
Scale Hierarchical Image Database. In: CVPR (2009) 9, 12
6. Fei-Fei, L., Fergus, R., Perona, P.: Learning generative visual models from few
training examples: An incremental bayesian approach tested on 101 object cate-
gories. In: CVPR Workshops (2004) 9
7. Feng, C.M., Yu, K., Liu, Y., Khan, S., Zuo, W.: Diverse data augmentation with
diffusionsforeffectivetest-timeprompttuning.In:ProceedingsoftheIEEE/CVF
International Conference on Computer Vision (ICCV). pp. 2704–2714 (October
2023) 3, 10, 11
8. Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., Li, H., Qiao, Y.: Clip-
adapter: Better vision-language models with feature adapters (2021) 2
9. Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra, I.:
Imagebind: One embedding space to bind them all. In: Conference on Computer
Vision and Pattern Recognition (CVPR) (2023) 1
10. Goyal,S.,Sun,M.,Raghunathan,A.,Kolter,J.Z.:Testtimeadaptationviaconju-
gatepseudo-labels.In:AdvancesinNeuralInformationProcessingSystems.vol.35,
pp. 6204–6218 (2022) 3
11. Guo, Z., Zhang, R., Qiu, L., Ma, X., Miao, X., He, X., Cui, B.: Calip: Zero-shot
enhancement of clip with parameter-free attention. In: AAAI (2023) 4, 12
12. Hassan,J.,Gani,H.,Hussein,N.,Khattak,M.U.,Naseer,M.,Khan,F.S.,Khan,S.:
Alignyourprompts:Test-timepromptingwithdistributionalignmentforzero-shot
generalization. In: Thirty-seventh Conference on Neural Information Processing
Systems (2023) 3
13. Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification. IEEE J. Sel. Top.
Appl. Earth Obs. Remote. Sens. (2019) 9
14. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai,
R.,Zhu,T.,Parajuli,S.,Guo,M.,Song,D.,Steinhardt,J.,Gilmer,J.:Themany
faces of robustness: A critical analysis of out-of-distribution generalization. ICCV
(2021) 9
15. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., Song, D.: Natural adversarial
examples.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition (CVPR). pp. 15262–15271 (June 2021) 9Just Shift It 17
16. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Ges-
mundo,A.,Attariyan,M.,Gelly,S.:Parameter-efficienttransferlearningforNLP.
In: Proceedings of the 36th International Conference on Machine Learning (2019)
2
17. Huang,X.,Belongie,S.:Arbitrarystyletransferinreal-timewithadaptiveinstance
normalization. In: ICCV (2017) 4
18. Iwasawa,Y.,Matsuo,Y.:Test-timeclassifieradjustmentmoduleformodel-agnostic
domain generalization. In: Advances in Neural Information Processing Systems
(2021) 3
19. Jaderberg,M.,Simonyan,K.,Zisserman,A.,kavukcuoglu,k.:Spatialtransformer
networks. In: Advances in Neural Information Processing Systems. vol. 28 (2015)
4
20. Jia,M.,Tang,L.,Chen,B.C.,Cardie,C.,Belongie,S.,Hariharan,B.,Lim,S.N.:Vi-
sualprompttuning.In:EuropeanConferenceonComputerVision(ECCV)(2022)
2
21. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for fine-
grained categorization. In: ICCV Workshops (2013) 9
22. Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient
prompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing (Nov 2021) 2
23. Lian, D., Zhou, D., Feng, J., Wang, X.: Scaling & shifting your features: A new
baselineforefficientmodeltuning.In:AdvancesinNeuralInformationProcessing
Systems (NeurIPS) (2022) 4
24. Lin,W.,Mirza,M.J.,Kozinski,M.,Possegger,H.,Kuehne,H.,Bischof,H.:Video
test-time adaptation for action recognition. In: CVPR (2023) 3
25. Liu, S., Yu, S., Lin, Z., Pathak, D., Ramanan, D.: Language models as black-box
optimizers for vision-language models (2023) 4
26. Liu,Y.,Kothari,P.,vanDelft,B.,Bellot-Gurlet,B.,Mordan,T.,Alahi,A.:Ttt++:
Whendoesself-supervisedtest-timetrainingfailorthrive?In:AdvancesinNeural
InformationProcessingSystems.vol.34,pp.21808–21820.CurranAssociates,Inc.
(2021) 3
27. Loshchilov,I.,Hutter,F.:Decoupledweightdecayregularization.In:International
Conference on Learning Representations (2019) 9
28. Ma,X.,ZHANG,J.,Guo,S.,Xu,W.:Swapprompt:Test-timepromptadaptation
for vision-language models. In: Thirty-seventh Conference on Neural Information
Processing Systems (2023) 3
29. Maji, S., Kannala, J., Rahtu, E., Blaschko, M., Vedaldi, A.: Fine-grained visual
classification of aircraft. Tech. rep. (2013) 9
30. Menon, S., Vondrick, C.: Visual classification via description from large language
models. ICLR (2023) 2, 4, 8
31. Nilsback, M.E., Zisserman, A.: Automated flower classification over a large num-
ber of classes. In: Indian Conference on Computer Vision, Graphics and Image
Processing (Dec 2008) 9
32. Niu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P., Tan, M.: Efficient test-
time model adaptation without forgetting. In: ICML (2022) 3
33. Niu, S., Wu, J., Zhang, Y., Wen, Z., Chen, Y., Zhao, P., Tan, M.: Towards stable
test-time adaptation in dynamic wild world. ICLR (2023) 3
34. OpenAI: Gpt-4 technical report (2023) 7, 15, 3
35. Parkhi,O.M.,Vedaldi,A.,Zisserman,A.,Jawahar,C.V.:Catsanddogs.In:CVPR
(2012) 918 E. Sui et al.
36. Perez, E., Strub, F., de Vries, H., Dumoulin, V., Courville, A.C.: Film: Visual
reasoning with a general conditioning layer. In: AAAI (2018) 4, 14
37. Prabhudesai,M.,Ke,T.W.,Li,A.C.,Pathak,D.,Fragkiadaki,K.:Test-timeadap-
tation of discriminative models via diffusion generative feedback. In: Conference
on Neural Information Processing Systems (2023) 4, 10, 11
38. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell,A.,Mishkin,P.,Clark,J.,Krueger,G.,Sutskever,I.:Learningtransferable
visual models from natural language supervision. In: ICML (2021) 1, 2, 3, 4, 5, 7,
8, 10, 13, 15
39. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do ImageNet classifiers gener-
alize to ImageNet? In: Proceedings of the 36th International Conference on Ma-
chineLearning.ProceedingsofMachineLearningResearch,vol.97,pp.5389–5400.
PMLR (09–15 Jun 2019) 9
40. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10684–
10695 (June 2022) 12
41. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., Bethge, M.: Im-
proving robustness against common corruptions by covariate shift adaptation. In:
NeurIPS (2020) 3
42. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti,
M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kun-
durthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An
open large-scale dataset for training next generation image-text models. In: Ad-
vancesinNeuralInformationProcessingSystems.vol.35,pp.25278–25294.Curran
Associates, Inc. (2022) 12
43. Shu,M.,Nie,W.,Huang,D.A.,Yu,Z.,Goldstein,T.,Anandkumar,A.,Xiao,C.:
Test-time prompt tuning for zero-shot generalization in vision-language models.
In:AdvancesinNeuralInformationProcessingSystems.vol.35,pp.14274–14289.
Curran Associates, Inc. (2022) 2, 3, 5, 6, 8, 9, 10, 11, 12, 1
44. Song,J.,Lee,J.,Kweon,I.S.,Choi,S.:Ecotta:Memory-efficientcontinualtest-time
adaptation via self-distilled regularization. In: CVPR (2023) 3
45. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 101 human actions
classes from videos in the wild. CoRR abs/1212.0402 (2012) 9
46. Sun,Y.,Wang,X.,Zhuang,L.,Miller,J.,Hardt,M.,Efros,A.A.:Test-timetraining
with self-supervision for generalization under distribution shifts. In: ICML (2020)
3
47. Udandarao,V.,Gupta,A.,Albanie,S.:Sus-x:Training-freename-onlytransferof
vision-language models. In: ICCV (2023) 4, 11, 12
48. deVries,H.,Strub,F.,Mary,J.,Larochelle,H.,Pietquin,O.,Courville,A.C.:Mod-
ulating early visual processing by language. In: Advances in Neural Information
Processing Systems. vol. 30 (2017) 4
49. Wang, D., Shelhamer, E., Liu, S., Olshausen, B.A., Darrell, T.: Tent: Fully test-
time adaptation by entropy minimization. In: ICLR (2021) 3
50. Wang,H.,Ge,S.,Lipton,Z.,Xing,E.P.:Learningrobustglobalrepresentationsby
penalizing local predictive power. In: Advances in Neural Information Processing
Systems. pp. 10506–10518 (2019) 9
51. Xiao,J.,Hays,J.,Ehinger,K.A.,Oliva,A.,Torralba,A.:Sundatabase:Large-scale
scenerecognitionfromabbeytozoo.In:2010IEEEComputerSocietyConference
onComputerVisionandPatternRecognition(June2010).https://doi.org/10.
1109/CVPR.2010.5539970 9Just Shift It 19
52. Yang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison-Burch, C., Yatskar, M.:
Languageinabottle:Languagemodelguidedconceptbottlenecksforinterpretable
image classification. In: CVPR (2023) 2, 4
53. Zhang, M., Levine, S., Finn, C.: MEMO: test time robustness via adaptation and
augmentation. CoRR abs/2110.09506 (2021) 3
54. Zhang, M., Levine, S., Finn, C.: Memo: Test time robustness via adaptation and
augmentation.In:AdvancesinNeuralInformationProcessingSystems.vol.35,pp.
38629–38642. Curran Associates, Inc. (2022) 5
55. Zhao, S., Wang, X., Zhu, L., Yang, Y.: Test-time adaptation with clip reward for
zero-shot generalization in vision-language models. In: ICLR (2024) 3
56. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language
models. CoRR abs/2109.01134 (2021) 2, 4, 7, 10, 11, 13
57. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for vision-
language models. In: CVPR (2022) 2, 4, 7
58. Zhou,Y.,Ren,J.,Li,F.,Zabih,R.,Lim,S.N.:Test-timedistributionnormalization
forcontrastivelylearnedvisual-languagemodels.In:Thirty-seventhConferenceon
Neural Information Processing Systems (2023) 4Just Shift It: Test-Time Prototype Shifting for
Zero-Shot Generalization with Vision-Language
Models Supplementary Material
Elaine Sui, Xiaohan Wang, and Serena Yeung-Levy
Stanford University
{esui,xhanwang,syyeung}@stanford.edu
This document provides more details of our approach and additional exper-
imental results, organized as follows:
– § A Implementation Details.
– § B Additional Quantitative Results with Different Random Seeds.
– § C Additional Ablation Studies.
A Implementation Details of TPS
Algorithm2showsmoredetailedpseudocodeinPyTorch-likestyleforTest-Time
PrototypeShiftingoveranentiredataset.Wewillreleasethemodelsandsource
code to ensure reproducibility.
B Main Results With More Random Seeds
In Sec B.1 and B.2, we run Test-Time Prototype Shifting (TPS) over 3 ran-
dom seeds on both the natural distribution shifts (Table 1) and cross-dataset
generalization (Table 3), respectively. The randomness comes from the image
augmentation in creating a diverse minibatch for the entropy minimization ob-
jective.
B.1 Natural Distribution Shifts
From Table 7, we observe that our conclusion from Sec 4.3.2 still holds. That is,
ourmethodoutperformsSoTATPT[43]by>3.4%onaverage.Wealsoobserve
that augmenting the TPT-tuned class prototypes with more advanced off-the-
shelfprototypesonlyboostsperformancebyamere0.5%onaverageovervanilla
TPT, demonstrating TPT’s limitation in maximally leveraging these advanced
prototypes.
B.2 Cross-Dataset Generalization
From Table 8, we see that our conclusion from Sec 4.3.3 remains valid. Specifi-
cally, TPS outperforms TPT [43] by >2% on average. Similarly to Sec B.1, we
observe that taking the mean of the TPT-tuned and advanced off-the-shelf pro-
totypesincreasesperformancebyonly0.5%onaverageoverTPT,demonstrating
TPT’s inflexibility in utilizing these more robust class representations.2 E. Sui et al.
Algorithm 2 Test-Time Prototype Shifting Pseudocode in PyTorch-like style
1 # Define frozen parameters
2 image_encoder = CLIPImageEncoder()
3 prototypes = load_class_prototypes()
4
5 predictions = []
6 for img, label in data_loader:
7 # Test-Time Shifting
8 shift_params = nn.Parameter(torch.zeros(num_classes, embed_dim),
requires_grad=True)
9 aug_imgs = [aug(img) for i in range(batch_size - 1)]
10 imgs = torch.stack([img] + aug_imgs, dim=0)
11 image_features = image_encoder(imgs)
12
13 text_features = prototypes + shift_params
14 text_features = F.normalize(text_features, dim=-1)
15
16 logits = (logit_scale * text_features @ image_features.T)
17
18 # Confidence selection
19 entropies = compute_batch_entropies(logits)
20 top_k_idx = torch.argsort(batch_entropy, descending=False)[:k]
21
22 loss = compute_average_entropy(logits[top_k_idx])
23 optimizer.zero_grad()
24 loss.backward()
25 optimizer.step()
26
27 # Test-Time Inference
28 new_prototypes = prototypes + shift_params
29 new_prototypes = F.normalize(new_prototypes, dim=-1)
30
31 logits = (logit_scale * new_prototypes @ image_features[0].unsqueeze
(0).T)
32 pred = torch.argmax(logits)
33
34 predictions.append(pred)
35
36 return predictionsJust Shift It 3
Table 7: Acc@1 of zero-shot image classification with CLIP-ViT-B/16 backbone on
ImageNet and its OOD variants over 3 random seeds. Best performances are in bold.
Method ImageNet ImageNet-A ImageNet-V2 ImageNet-R ImageNet- Average OODAverage
Sketch
Test-TimeAdaptationBaselines
TPT[43] 68.96(±.03) 54.47(±.26) 63.46(±.07) 77.10(±.04) 47.93(±.03) 62.38(±.05) 60.74(±.06)
TPT+(templates+descriptors)* 69.51(±.05) 54.94(±.17) 63.86(±.11) 77.57(±.11) 48.38(±.04) 62.85(±.03) 61.19(±.04)
Ours 71.43(±.06) 60.78(±.21) 65.00(±.09) 80.06(±.13) 50.97(±.09) 65.65(±.06) 64.20(±.08)
Table 8: Acc@1 of zero-shot image classification with CLIP-ViT-B/16 backbone on
cross-dataset generalization over 3 random seeds. Best performances are in bold.
Method Flower102 DTD Pets Cars UCF101 CalTech101 Food101 SUN397 Aircraft EuroSAT Average
TPT[43] 68.79(±.1) 46.79(±.1) 87.09(±.1) 66.38(±.2) 67.86(±.1) 94.13(±.1) 84.67(±.1) 65.41(±.1) 23.44(±.3) 42.78(±.3) 64.73(±.1)
TPT+(templates+descriptors)*69.67(±.11)47.56(±.55)87.88(±.02)66.91(±.17)68.35(±.21)94.17(±.13)84.89(±.07)66.23(±.12)23.55(±.31)43.12(±.18)65.23(±.06)
Ours 71.47(±.12)51.00(±.47)87.45(±.09)68.99(±.10)70.98(±.24)94.90(±.16)85.15(±.08)68.85(±.16)25.82(±.45)44.61(±.11)66.92(±.04)
C Full Ablations
In Sec C.1, we report full ablations on TPS on the effectiveness of feature-space
shift on various prototypes. These results are comparable to those reported in
Sec 4.5. In Sec C.2, we include additional ablations to observe the effect of
learning a class-specific shift over a universal shift for all classes. In Sec C.3, we
explorevariantsonprototypegenerationusingtheclass-agnosticCLIPImageNet
context prompt templates [38] and the class-specific descriptors generated using
GPT-4 [34]. All these ablations are run over 3 random seeds.
C.1 Effect of Shift on Different Prototypes
Full comparisons between zero-shot and feature-shifted performance on all nat-
ural distribution shift and cross-domain generalization benchmark datasets over
3 random seeds are in Tables 9 and 10, respectively. We demonstrate that our
conclusion from Sec 4.5.1 stills holds – that learning a small perturbation in
thefeaturespaceresultsinperformancegainsof >4%andupto1%onaverage
acrossnaturaldistributionshiftandcross-domaingeneralizationtasksregardless
of what prototypes are used.
C.2 Effect of Per-Class vs. Shared Shift
Test-timeprompttuningmethodsinvolvetuningapromptthatissharedacross
all classes in a dataset. Given that the tuneable prompt tokens form a portion
ofthetextencoderinput,thesefullpromptsarethenmappedtotheembedding
space with the encoder’s learned complex feature-space mapping. This results
in non-linear perturbations from the original class prototypes. However, for our
method, tuning shift parameters that are shared for all class prototypes in the
feature-space means that the relative distance between class prototypes will re-
mainconstantbeforeandaftertest-timeshifttuning,limitingtheexpressiveca-
pabilityofthelearnedshift.Rather,webelievethateachclassprototypeshould4 E. Sui et al.
Table 9: Acc@1 for zero-shot and with feature-space shift with features initialized
usingdifferentprototypegenerationtechniquesonImageNetanditsout-of-distribution
variants. Results are over 3 random seeds.
PromptType Setting ImageNet ImageNet-A ImageNet-V2 ImageNet-R ImageNet- Average OODAverage
Sketch
Zero-Shot 66.74 47.79 60.89 73.99 46.12 59.10 57.20
Vanilla +shift 68.81(±.03) 58.11(±.16) 63.51(±.17) 76.98(±.05) 48.11(±.09) 63.10(±.08) 61.68(±.09)
∆ +2.07 +10.32 +2.62 +2.99 +1.99 +4.00 +4.48
Zero-Shot 71.51 49.71 64.20 75.21 47.99 61.72 59.28
CoOp[56] +shift 73.76(±.04) 60.43(±.12) 66.84(±.10) 77.39(±.05) 49.08(±.06) 65.50(±.02) 63.44(±.03)
∆ +2.25 +10.72 +2.64 +2.18 +1.09 +3.78 +4.16
Zero-Shot 68.35 49.95 61.97 77.59 48.21 61.21 59.43
CLIPtemplates +shift 70.39(±.06) 60.47(±.07) 64.66(±.04) 80.70(±.04) 50.38(±.14) 65.32(±.03) 64.05(±.02)
∆ +2.04 +10.52 +2.69 +3.11 +2.17 +4.11 +4.62
Zero-Shot 68.52 48.91 61.78 74.81 47.68 60.34 58.29
Descriptors +shift 70.38(±.03) 59.21(±.09) 63.80(±.07) 77.49(±.12) 49.57(±.06) 64.09(±.02) 62.52(±.03)
∆ +1.86 +10.30 +2.02 +2.68 +1.89 +3.75 +4.23
CLIPtemplates Zero-Shot 69.54 50.51 63.01 77.18 48.84 61.82 59.88
+Descriptors +shift 71.43(±.06) 60.78(±.21) 65.00(±.09) 80.06(±.13) 50.97(±.09) 65.65(±.06) 64.20(±.08)
∆ +1.89 +10.27 +1.99 +2.88 +2.13 +3.83 +4.32
Table10:Acc@1forzero-shotandwithfeature-spaceshiftwithfeaturesinitializedus-
ingdifferentprototypegenerationtechniquesoncross-domaingeneralizationdatasets.
Results are over 3 random seeds.
PromptType Setting Flower102 DTD Pets Cars UCF101 CalTech101 Food101 SUN397 Aircraft EuroSAT Average
Zero-Shot 67.28 44.44 87.98 65.24 65.08 92.98 83.80 62.55 23.70 41.42 63.45
Vanilla +shift 67.75(±.10)45.69(±.10)87.57(±.10)67.60(±.23)66.79(±.21)93.79(±.08)84.62(±.03)64.58(±.03)24.75(±.39)41.35(±.03)64.45(±.04)
∆ +0.47 +1.25 -0.41 +2.36 +1.71 +0.81 +0.82 +2.03 +1.05 -0.07 +1.00
Zero-Shot 65.57 44.86 88.25 66.19 67.46 93.67 83.77 65.78 23.64 47.74 64.69
CLIPtemplates +shift 66.41(±.05)45.61(±.19)87.99(±.10)68.66(±.31)68.02(±.11)93.85(±.14)84.54(±.08)67.19(±.05)24.66(±.13)48.28(±.20)65.52(±.05)
∆ +0.84 +0.75 -0.26 +2.47 +0.56 +0.18 +0.77 +1.41 +1.02 +0.54 +0.83
Zero-Shot 71.13 52.72 86.75 65.15 70.53 94.08 84.12 67.10 25.26 43.31 66.02
Descriptors +shift 71.69(±.15)53.80(±.21)87.82(±.19)67.00(±.14)71.18(±.15)94.56(±.08)84.78(±.05)68.25(±.18)26.27(±.09)42.11(±.18)66.75(±.06)
∆ +0.56 +1.08 +1.07 +1.85 +0.65 +0.48 +0.66 +1.15 +1.01 -1.20 +0.73
CLIPtemplatesZero-Shot 70.52 49.94 87.22 66.48 70.24 94.12 84.47 67.55 24.69 44.14 65.94
+Descriptors +shift 71.47(±.12)51.00(±.47)87.45(±.09)68.99(±.10)70.98(±.24)94.90(±.16)85.15(±.08)68.85(±.16)25.82(±.45)44.61(±.11)66.92(±.04)
∆ +0.95 +1.06 +0.23 +2.51 +0.74 +0.78 +0.68 +1.30 +1.13 +0.47 +0.98
bemodulatedbyslightlydifferentmagnitudesand/ordirectionstoprovidemore
degrees of freedom in capturing the class-level distribution shifts in addition to
the dataset-level shifts present in a domain gap.
Table 11 shows that, on average, learning a per-class shift increases perfor-
mance by > 1.2% regardless of which prototypes are used. Moreover, we see
that Table 12 demonstrates that, on average, learning a per-class shift increases
performance by around 0.5% on average over different prototype settings. This
demonstrates that learning per-class shifts allows the model to capture both
dataset-level and class-level distribution shifts in a domain gap.
C.3 Prototype Variants
We explore different methods for creating class prototypes. Specifically, we ex-
periment with different forms of aggregating the text encoded with the 80 Im-
ageNet context prompts from CLIP [38] and our LLM-generated descriptors.
The CLIP ImageNet templates are class-agnostic and add image-level charac-Just Shift It 5
Table11:Acc@1forlearningasharedvs.per-classshiftontopofdifferentprototypes
over 3 random seeds. Best performances are in bold.
Method ImageNet ImageNet-A ImageNet-V2 ImageNet-R ImageNet- Average OODAverage
Sketch
Shared 71.23(±.02) 56.57(±.19) 64.98(±.03) 79.31(±.03) 50.80(±.06) 64.58(±.04) 62.92(±.06)
Perclass 71.43(±.06) 60.78(±.21) 65.00(±.09) 80.06(±.13) 50.97(±.09) 65.65(±.06) 64.20(±.08)
Table12:Acc@1forlearningasharedvs.per-classshiftontopofdifferentprototypes
over 3 random seeds. Best performances are in bold.
Method Flower102 DTD Pets Cars UCF101 CalTech101 Food101 SUN397 Aircraft EuroSAT Average
Shared 71.36(±.12) 50.49(±.12)87.46(±.12)67.33(±.06) 70.77(±.12) 94.35(±.06) 84.82(±.01) 68.12(±.04) 25.27(±.02)44.67(±.06)66.47(±.03)
Per-class71.47(±.12)51.00(±.47)87.45(±.09)68.99(±.10)70.98(±.24)94.90(±.16)85.15(±.08)68.85(±.16)25.82(±.45)44.61(±.11)66.92(±.04)
teristics whereas the descriptors are class-specific and add class-level semantic
information.
Tables 13 and 14 compare three variants of pooling these CLIP templated
embeddings and descriptor embeddings to obtain a single class prototype. Simi-
larlytotheconclusionofSec4.5.1,weobservethatingeneral,thegainsobserved
using more advanced prototypes in the zero-shot setting almost directly trans-
late to the test-time adaptation setting with shifting. In Sec 4, we present the
results of our method using prototypes that are a micro average of the CLIP
templates and LLM-generated descriptors.6 E. Sui et al.
Table 13: Acc@1fordifferentvariantsofprototypegeneration,i.e.waysofcombining
templates and descriptors, on natural distribution shifts, over 3 random seeds. Best
performances for each setting are in bold.
PromptType(s) Pooling ImageNet ImageNet-A ImageNet-V2 ImageNet-R ImageNet- Average OODAverage
Method Sketch
Zero-Shot
Vanillaprompt N/A 66.74 47.79 60.89 73.99 46.12 59.10 57.20
CLIPtemplates+Descriptors Macro 68.73 50.32 62.31 77.67 48.56 61.52 59.72
CLIPtemplates+Descriptors Micro 69.54 50.51 63.01 77.18 48.84 61.82 59.88
CLIPtemplates×Descriptors Macro 69.03 50.73 62.22 76.91 49.07 61.59 59.73
WithShift
Vanillaprompt N/A 68.81(±.03) 58.11(±.16) 63.51(±.17) 76.98(±.05) 48.11(±.09) 63.10(±.08) 61.68(±.09)
CLIPtemplates+Descriptors Macro 70.75(±.08) 60.86(±.09) 64.95(±.11) 80.84(±.03) 50.70(±.11) 65.62(±.02) 64.34(±.02)
CLIPtemplates+Descriptors Micro 71.43(±.06) 60.78(±.21) 65.00(±.09) 80.06(±.13) 50.97(±.09) 65.65(±.06) 64.20(±.08)
CLIPtemplates×Descriptors Macro 70.82(±.02) 60.42(±.06) 64.50(±.05) 79.53(±.09) 51.13(±.02) 65.28(±.01) 63.89(±.02)
Table 14: Acc@1 for different variants of knowledge injection, i.e. ways of combining
templates and descriptors, over 3 random seeds on cross-dataset generalization tasks.
Best performances in each setting are in bold.
PromptType(s) PoolingMethodFlower102 DTD Pets Cars UCF101 CalTech101 Food101 SUN397 Aircraft EuroSAT Average
Zero-Shot
Vanillaprompt N/A 67.28 44.44 87.98 65.24 65.08 92.98 83.80 62.55 23.70 41.42 63.45
CLIPtemplates+Descriptors Macro 66.91 45.86 88.33 66.46 68.12 93.83 83.97 66.34 24.03 46.62 65.05
CLIPtemplates+Descriptors Micro 70.52 49.94 87.22 66.48 70.24 94.12 84.47 67.55 24.69 44.14 65.94
CLIPtemplates×Descriptors Macro 72.03 50.83 86.21 66.12 70.90 94.16 83.73 67.98 25.53 47.19 66.47
WithShift
Vanillaprompt N/A 67.75(±.10)45.69(±.10)87.57(±.10)67.60(±.23)66.79(±.21)93.79(±.08)84.62(±.03)64.58(±.03)24.75(±.39)41.35(±.03)64.45(±.04)
CLIPtemplates+Descriptors Macro 67.52(±.27)46.43(±.28)88.00(±.13)69.04(±.16)68.67(±.18)94.16(±.18)84.77(±.04)67.70(±.08)24.79(±.30)47.09(±.19)65.82(±.06)
CLIPtemplates+Descriptors Micro 71.47(±.12)51.00(±.47)87.45(±.09)68.99(±.10)70.98(±.24)94.90(±.16)85.15(±.08)68.85(±.16)25.82(±.45)44.61(±.11)66.92(±.04)
CLIPtemplates×Descriptors Macro 72.53(±.12)52.56(±.09)86.15(±.05)68.89(±.07)71.44(±.20)94.43(±.06)84.44(±.08)69.04(±.02)26.51(±.26)45.65(±.15)67.16(±.03)