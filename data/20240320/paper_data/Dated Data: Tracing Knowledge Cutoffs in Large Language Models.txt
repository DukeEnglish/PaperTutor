Dated Data: Tracing Knowledge Cutoffs
in Large Language Models
JeffreyCheng MarcMarone OrionWeller
DawnLawrie DanielKhashabi BenjaminVanDurme
JohnsHopkinsUniversity
jcheng71@jhu.edu
Abstract
ReleasedLargeLanguageModels(LLMs)areoftenpairedwithaclaimedknowl-
edgecutoffdate,orthedatesatwhichtrainingdatawasgathered. Suchinformation
iscrucialforapplicationswheretheLLMmustprovideuptodateinformation.
However,thisstatementonlyscratchesthesurface: doallresourcesinthetraining
datasharethesameknowledgecutoffdate? Doesthemodel’sdemonstratedknowl-
edgeforthesesubsetscloselyaligntotheircutoffdates? Inthiswork,wedefinethe
notionofaneffectivecutoff. ThisisdistinctfromtheLLMdesignerreportedcutoff
andappliesseparatelytosub-resourcesandtopics. Weproposeasimpleapproach
toestimateeffectivecutoffsontheresource-leveltemporalalignmentofanLLM
byprobingacrossversionsofthedata. Usingthisanalysis,wefindthateffective
cutoffs often differ from reported cutoffs. To understand the root cause of this
observation,weconductadirectlarge-scaleanalysisonopenpre-trainingdatasets.
Ouranalysisrevealstworeasonsfortheseinconsistencies: (1)temporalbiasesof
CommonCrawldataduetonon-trivialamountsofolddatainnewdumpsand(2)
complicationsinLLMdeduplicationschemesinvolvingsemanticduplicatesand
lexicalnear-duplicates. Overall,ourresultsshowthatknowledgecutoffsarenot
assimpleastheyhaveseemedandthatcaremustbetakenbothbyLLMdataset
curatorsaswellaspractitionerswhoseektouseinformationfromthesemodels.
1 Introduction
ManyLargeLanguageModel(LLM)creatorsusethelabel“open-source”todescribetheirmodels,
butveryfewactuallyprovidetheexactdatasetstheirmodelsusedforpre-training. Whiletheopen-
datacommunityhasmadegreateffortsintrainingLLMsandprovidingthefullyreplicabletraining
data(Groeneveldetal.,2024;Bidermanetal.,2023;Computer,2023),thesemodelsoftenlagbehind
theirclosed-datacounterpartsandaretypicallyonlyusedforresearch,leadingmostpractitionersto
usehigh-performingmodelswhosedatatheycannotinspect.
Inplaceofprovidingtheexactpre-trainingdata,creatorsoftenprovidea“knowledgecutoff”datefor
theirmodel. Whenfacedwithadescriptionthatstates,e.g.,“thismodelhasacutoffdateofMarch
2024,” doesthatmeanallofitsincludedresourcessharetheexactsamecutoffdate? Evenifthe
modelprovidesanexplicitreportedcutoff foraresource(e.g. theWikipediadumpdate),doesthat
implythatthemodel’sknowledgeforthatresource,oreffectivecutoff,isthesameasthereported
cutoff date? For LLM users, these questions can be crucial: imagine a layperson using an LLM
fortaxadvice,withoutrealizingthattheeffectivecutoffofthetaxcodeis2022andthusoutdated–
despitethefactthatthereportedcutoffisadvertisedas2023(Fig.1).
As a result there has been a push for researchers to document their data (Mitchell et al., 2018;
Pushkarnaetal.,2022;Gebruetal.,2021;Luccionietal.,2022),identifywhatdataisinthesemodels
4202
raM
91
]LC.sc[
1v85921.3042:viXraAs of my
Do I need to 2022 2024
knowledge
issue a 1099-K
cutoff in March
for $10,000?
2024, the limit Form 1099-K is issued Now a single transaction
is $20,000 for transactions only if exceeding $5000 can
the aggregate amount of require the third party
these transactions platform to issue a
exceeded $20,000 1099-K.
Figure1:LLMsmaycontaindifferentversionsofadatasetintheirtrainingdatathanwhatisspecified
ina“cutoff"date,misleadingusersandcausingerrorswhenusersareunawareofthesedifferences.
withmembershipinferencetests(Carlinietal.,2022;Piktusetal.,2023;Marone&VanDurme,
2023),andotherwisereproducedatafromtheirtrainingset(Carlinietal.,2021;Ippolitoetal.,2022;
Nasretal.,2023;Welleretal.,2023). However,thesetestsgenerallyonlycheckforstaticinclusion
ofthedata,ratherthanidentifyingwhentheresourcesstoppedbeingincluded. Evenmoredifficult
arecaseswherethereexistsmultipleversionsofaresource,wheredifferentversionscancontain
informationthatisupdated,deleted,orevenconflictingwiththepreviousversions.
GiventhecrucialimportanceoftheseknowledgecutoffsandthelackoftransparencyfromLLM
creators,weseektoautomaticallydeterminetheeffectivecutoffofmodelswithrespecttoagiven
resource, withoutneedingaccesstothemodel’strainingdata. Ourcontributionsinclude: (1)we
collectresourcesetsspanninglongtimeframesandproposeasimplemethodtodetermineeffective
cutoffofLLMs;(2)weshowthatforavarietyofmodels(particularlynewermodels),theresource-
leveleffectivecutoffsdifferdrasticallyfromtheirreportedcutoffdate;and(3)weprovideanalysis
detailingthecauseofsuchmisalignment,showingthatpre-trainingdatasetssufferfromdeduplication
complicationsandthatCommonCrawldumpsexhibittemporalmisalignmentfromthedumpdates.
2 RelatedWork
DocumentingandDescribingLLMTrainingData AsthesizeofthedatainLLMsincreases,
therehavebeenmanycallsforresearcherstodocumentdatasetsthroughadditionaldocumentation
artifacts. Oneoftheearliestworksonthistopiccallsforcreatorstobroadlysummarizethetypeof
datatrainedon,callingtheseModelCards(Mitchelletal.,2018). OtherworkproposedDataCards
andDatasheetsthatfocusonmorespecificdocumentationofthedataused(Pushkarnaetal.,2022;
Gebruetal.,2021). Theopen-accesscommunityhasadoptedversionsofthese(e.g. Huggingface
modeldescriptions)buttheydonotprovidefine-grainedversioningandanalysisthatenablesprecise
trackingofcutoffs. Forexample,itmaynotbeclearwhetherscrapedCommonCrawldatacontains
additionalversionsofscrapedWikipedia.
Otherresearchhasfocusedonmorefine-grainedanalysis,suchastheroleoffilteringinLLM-data
creation(Lucyetal.,2024)orhowPII,toxicdata,n-grams,andprovenanceplayaroleinLLMdata
(Dodgeetal.,2021;Elazaretal.,2023). TheseworkshaveprovidedgreatinsightintoLLMs,but
necessarilydependonthedatabeingavailabletothepublic. Incontrast,ourworkfocusesinsteadon
determiningwhattemporalversionsofdataexistinamodel,withoutaccesstothetrainingdata.
MembershipInferenceAttacks ManyprominentLLMs(e.g. GPT-4,LLaMAv2)donotprovide
theirtrainingdataorevendescriptiveinformationaboutthem. Inthesecases,manypeoplewish
toknowiftheirdataisincludedinthemodel’strainingset. Techniqueslikemembershipinference
attacks(Shokrietal.,2017)havebeenappliedtolargelanguagemodels. Manystrategieshavebeen
proposedforthisproblem,mostofwhichrelyontheLLM’sperplexityoverpotentialdatainstances:
theseincludeusingsimilarbutsyntheticdata(Matternetal.,2023),promptingcalibration(Fuetal.,
2023),andavarietyofothertechniques(Shietal.,2023;Faysseetal.,2024;Hisamotoetal.,2019).
Overall, membership inference testing focuses on whether a single instance was included in the
LLMstrainingset(withthecriticalassumptionthattherewasonlyoneversion). Incontrast,our
workfocusesonwhichversion(s)ofthedatawasincludedintheLLMstrainingdata.
ContinualLearninginLLMs Newwrittenknowledgeincreaseseveryday,butlanguagemodels
remainstatic. Asre-trainingaLLMisprohibitivelyexpensive, itisinfeasibleforLLMstokeep
2upwithlivingonlineresources.1 Thus,thereexistsalargefieldofresearchoncontinuallearning,
orhelpingLLMsstayuptodatewithoutexpensivere-training. Thistypicallyinvolvesmodeling
approachesthatperformlimitedcontinuedtrainingtokeepmodelknowledgeuptodate(e.g. Huetal.
2023;Kasaietal.2023,amongothers). Ourworkalsoinvolvesexaminingthetemporalknowledgein
thesemodels,butdiffersbyfocusingontheoriginalstaticmodelsandwhatknowledgetheycontain.
3 Methodology
WeseektoprobeLLMstodeterminetheirresource-leveleffectivecutoffs. Modelssometimesreport
aninclusiondatefortheirsub-corpora,2 whichistypicallythelasttimestampofthatresourcebut
doesnotaddressadditionalsourcesofearlierdata. ThisisrelevantbecausegivenacertainWikipedia
dumpisincludedintrainingdata,itisreasonabletoassumethattheeffectiveknowledgecutofffor
thosearticlesisthecorrespondingdumpdate. However,olderversionsofthattextmaybepresentin
webscrapesinresourcesliketheCommonCrawl.
Toperformouranalysis,wefirstgatherseverallongspanning(2016-2023)resourcecollections.
We then measure perplexity on data at certain dates across a variety of models and analyze the
implicationsoftheseperplexity-at-timecurves,referringbacktopre-trainingdatawhenavailable.
3.1 Time-SpanningDatasets
Broadly,onlineresourcesincludedinLLMtrainingdatacanbedividedintothreesubsets: resources
thatinvolvefrequentupdates(e.g. legaltextsorWikipedia),resourcesthatarestaticbutbuildover
time(e.g.blogsornewsarticles),andpurelystaticresources(e.g.books).Weconstructrepresentative
datasetsthatallowustoprobetheformertwotypes,choosingWikipediaforourfrequentlyupdating
resourceandtheNewYorkTimesforabuilding-over-timeresource.
Wikipedia Wikipedia is commonly used in
pre-training(Gaoetal.,2020;Almazroueietal.,
2023a),providesbroadcoverageovermanytop-
ics, and changes frequently and rapidly. To
create the dataset, we start by collecting the
50003mostediteddocumentsbynumberofed-
its.4 Foreachofthesedocuments, weusethe
WikipediaAPItocollectaversionofthedocu-
mentatmonthlyintervals,fromApril2016to
April2023. Ourfinaldatasetthereforeconsists
ofthesame5000documentschangingmonthly
overthissevenyearperiod. Wecallthisdataset Figure2: PerplexityoftheWikipediadocument
WIKISPAN. An individual document in WIK- “Liverpool”forPythia-6.9B.Eachpointrepresents
ISPANistheWikipediadocumentontopicT at theperplexityofaversionofthedocument“Liver-
monthM (e.g. UnitedStatesinJanuary2020). pool”atthecorrespondingmonth.
New York Times We use New York Times (NYT) articles to represent our building-over-time
resource,asthedocumentscontainlong-formhighqualitytext,arefrequentlyincludedinpre-training
dataandCommonCrawldumps,andprovidealongandsteadystreamofnewdocuments. Forour
probingdataset,calledNEWSSPAN wecollectallthearticleswithtopleveldomain“nytimes.com”
fromacuratedcollectionofthe20mostrecentCommonCrawldumps(Soldainietal.,2024). We
bucketthearticlesbymonthaccordingtotheirpublishdate,andcollect500articlesfromeachbucket
fromJanuary2016untilJuly2020. Notethatduetocopyrightconcerns,CommonCrawlremoved
NYTarticlesfromrecentdumpsandhavestoppedscrapingit. UnlikethedocumentsinWIKISPAN,
thedocumentsineachbuckethavenorelationwithoneanother.
1Forexample,Wikipediagetseditedaboutevery2seconds,withanaverageof539newarticlesaday.
2https://huggingface.co/allenai/OLMo-7BreportsusingtheJanuary2023dumpofWikipedia
3Wefilterout100thatdidnotexistallthewaybackto2016.
4https://en.wikipedia.org/wiki/Wikipedia:Most_frequently_edited_pages,downloadedinMay2023
3ModelName Pile C4 RW CCDumps WikiDump CCCutoff
Pythia(Bidermanetal.,2023) 2020
GPT-Neo(Blacketal.,2022) 2020
GPT-J(Wang&Komatsuzaki,2021) 2020
RedPajamas(Computer,2023) 5(’19-’23) Mar’23 Jan’23
Falcon(Almazroueietal.,2023b) Mar’20 Feb’23
FalconRW(Almazroueietal.,2023b) Feb’23
OLMo(Groeneveldetal.,2024) 20(’20-’23) Mar’23 June’23
LLaMA(Touvronetal.,2023a) 5(’17-’20) Aug’22 2020
Table1: Differentdecoder-onlyLLMsandtheircorrespondingpre-trainingdata. TheCommonCrawl
dumpsareprocessedtovariousdegrees. Frequentlyuseddatasetswiththeirowncolumnsinclude
RefinedWeb(RW)(Penedoetal.,2023),C4(Raffeletal.,2020),andthePile(Gaoetal.,2020).
3.2 ProbingMethodology
WeevaluateLLMstoautomaticallydeterminetrainingdatacutoffs. Wedothisbymeasuringthe
perplexityforthedocumentsineachmonthbucket,usingthefirst512tokensforWikipediaand256
forNYT(duetoshorterdocuments). SeeFig.2foranexampleononeWikipediatopic.
Normalization Ourgoalistodeterminetheeffectivecutoffofthemodelforagivenresource,rather
thanfocusingonindividualtopicsordocuments. Occasionallysomedocumentsinourtime-spanning
datasetshavedrasticallydifferentperplexitycomparedtopreviousmonths(e.g. aWikipediapage
changingtobecomearedirectpageratherthanacontentpageforonemonth). Asthisdistractsfrom
understandingtherelativemodelperplexityacrossmonths,wefollowpreviouswork(Shietal.,2023)
andaggregateperplexitiesbytakingtheaverageofthemedian95%ofmeasurementsineachmonth.
Asperplexitymeasurementsarenotcomparableacrossmodels(duetodifferencesintokenizers,etc.)
wenormalizetheaveragedperplexitiestoa0-1scalebyperformingmin-maxscalinginorderto
fairlycomparetheirfluctuationsacrosstime. Wecalltheserelativeperplexities.
3.3 Pre-TrainingDatasets
The LLMs we evaluate were pre-trained on data derived from three major datasets: C4 (Raffel
etal.,2020),thePile(Gaoetal.,2020),andRefinedWeb(Penedoetal.,2023),aswellasadditional
CommonCrawldumpsbeyondthosethatwerealreadyincludedinthesenameddatasets. Wedescribe
their contents, as well as how different LLMs used them during training. We note that no open
pre-trainingdataseteverincludesadirectdumpofNYTarticles;theyareonlypresentinincluded
CommonCrawldumps. Whiledatasetsmayalsocontainothercurateddata(e.g. books),wefocuson
scrapeddataandcollectionsthatmaycontainWikipediaorNYTimescontent.
CommonCrawl Astheprimarysourceofdataforlargelanguagemodels,CommonCrawl(CC)
dumpsproviderawwebtextandmetadata. MostrecentmodelschoosetouseCCNet(Wenzeketal.,
2019)whichprocessesthefilesintohigherqualitytext.
C4 C4isasingle,heavilyprocessed,open-accessCommonCrawldumpfromApril2019. Ittook
theprocesseddataandappliedcontent-basedfilterstodiscarddocumentscontainingundesirable
contentsuchasobscenewords,boilerplatetemplates,andcode. Itisdeduplicatedatathreesentence
spanlevel,andhasanoverallsizeofabout750GB.
Pile The Pile is an open-access curated dataset consisting of 22 sub-datasets and totals around
800GB.Therelevantsub-datasetsarePile-CCandtheWikipediadump. ThePile-CCisdeduplicated
atadocumentlevel,andconsistsof22randomchunksoutofthe3679extractedfromCommonCrawl
dumpsbetween2013-2020. TheWikipediadumpisfromMarch2020andisup-sampledthreetimes.
RefinedWeb RefinedWeb(RW)consistsofURLsfromallCommonCrawldumpsatthetimeoftheir
publication,from2008toFebruary2023. BecauseRWwasdesignedtobeusedinconjunctionwith
4otherhighqualitydatasources,URLsfromspecifictop-leveldomains(including“wikipedia.org")
areexcluded. ThepublicRWisonlya600billiontokensampleofthetotal5trilliontokendataset.
3.4 Models
Weevaluateavarietyofdecoder-onlyTransformerLLMswithaccessible(ordescribed)data, as
showninTable1. Weprovidespeculationaboutclosed-datamodelsinAppendixD,butaswecannot
verifycorrectness,wedonotincludetheseresultsinthemaintext.
Pythia(Bidermanetal.,2023),GPT-Neo(Blacketal.,2022),andGPT-J(Wang&Komatsuzaki,
2021)aretrainedonThePile(Gaoetal.,2020). Theserepresentsomeoftheearliestattemptsto
replicateclosed-accessmodelsbytheopen-sourcecommunity. ThePythiasuitewasdesignedto
provideareplicabletrainingprocessforstudyingideasliketrainingdynamics,theimpactofdata,
andmemorizationinLLMs. TheRedPajamamodelsuite(Computer,2023)wasintendedasanopen
accessreplicaoftheLLaMAmodel(Touvronetal.,2023a),providingreplicablepre-trainingdata
sourcedfromweb-crawls. TheFalconandFalconRWsuites(Almazroueietal.,2023b)aretrainedon
RefinedWeb,adatasetconsistingsolelyofweb-scrapeddata. Falconalsoincorporatesothercurated
corporaintoitstrainingdata. FinallyOLMo(Groeneveldetal.,2024),andLLaMA(Touvronetal.,
2023a)arethelatestgenerationofLLMs,trainedonmixturesofthedatasetsusedinprevioussystems
andincreasingamountsofCommonCrawl.
Overall, these models span several iterations of LLM pre-training approaches in terms of data
selection,sizeofthetrainingset,andmodelparametersize. Table1outlinesthesemodelssuitesand
Section3.3describescorrespondingpre-trainingdatasetsindetail.
3.5 MiningfromPre-trainingData
Wehypothesizethatsimilardocumentsintrainingimpactmodelknowledgeandrelativeperplexity
measurements. Forexample,ifpartsofaresourcewereduplicatedmanytimes,wemightexpect
perplexityonthosedocumentstobeparticularlylow. Priorworkhasshowntheeffectsofdocument
frequencyonLLMmemorization(Carlinietal.,2022). Tobetterunderstandtheperplexitycurves,
wesearchfordocumentssimilartothoseinourtime-spanningdatasets. Thisretrievesoldversions
ofthedocuments,nearduplicates,andcopiedfragments–allofwhichmayimpactinformationin
themodelandourperplexitymeasurements. Weexpectthatthedistributionminedfromtraining
dataisinverselycorrelatedwiththeperplexitytrends–thecountsoftheversionsoftheretrieved
Wikipedia-alikedocumentsshouldbehigheratthesamemonthswhereperplexityislower. Theentire
processconsistsofobtainingandindexingnearly4TtokensfromseveralLLMtrainingsets.
Givenapre-trainingdataset,wefirstconstructaBM25indexoveritusingElasticsearch. Foreach
topic,weusethefirstkwordsofthedocumentasthequerytofindsimilardocuments.5 Notethat
BM25scorealoneisnotareliableindicatorofwhetherthedocumentisacloseduplicate(oreven
fromWikipedia)butisusefulasafirststepinfilteringpotentialdocuments.
Using the top ten returned BM25 results, we calculate the edit distance between the matched
documentsandeveryversionofthecorrespondingWikipediatopicinWIKISPAN,normalizingby
thecharacterlengthofthematcheddocumenttoavoidlengthbiases. Weclassifymatcheddocuments
asaWikipediadocumentonlyiftheminimumofthisnormalizededitdistancescoreislessthan0.2.6
Withthissubsetofmatcheddocuments,weattributeeachdocumenttothemonthwhereitsnormalized
editdistancewasminimized(ormultiplemonthsifitisequallysimilartomanyversions). Thisthen
allowsustoplotthegroundtruthdistributionofalldocumentssimilartotheoriginalbydate. We
provideamoredetaileddescriptionofthisalgorithminAppendixA.
4 Results
Inthissectionwediscusstheresultsofprobinglanguagemodelstodeterminetheireffectivecutoffs
andcomparethedetermineddateswiththegroundtruthdistributionsfromtheirtrainingsets.
5WeusetheversionofWikipediafromthemodel’sdumpdateforqueries
6Atmost20%ofthedocumentneedstobechangedtoexactlymatchaversionofthecorrespondingtopic.
5Figure3:RelativeperplexitiesofmodelspermonthoftheNEWSSPAN(NYT)dataset(§3.1).Relative
perplexitiesareshownastheabsoluteperplexitiesdonotmatterfordeterminingresource-level
effectiveknowledgecutoffs. Wefindthatourapproachidentifiestheeffectiveknowledgecutoffasthe
statedknowledgecutoffforNYT,asmodelshaveincreasedperplexitywhentheirCommnonCrawl
datadumpsendinearly2020.
4.1 NEWSSPAN
AsmentionedinSection3.1,weuseNYTdatauntilmid-2020,andthusonlyevaluatemodelswith
pre-2021CommonCrawlcutoffsduetothelackofevaluationdata. OurresultsareshowninFig.3.
Foreachofthemodels,theperplexitycurveincreasesinearly2020,whichisthedateofthelast
CommonCrawldumpincludedinthePile. Thus,weseethatitispossibletodeterminetheeffective
knowledgecutoffofdifferentarticlespostedovertimeusingthismethod.
4.2 WIKISPAN
AsdiscussedinSection3.3,therearethreemajorcategoriesofdatasetsthatmodelsaretrainedon:
C4,thePile,andFalconRefinedWeb(RW);hereweshowresultsforeachcategory. Againnotethat
resultsshowrelativeperplexitiesasabsoluteperplexitiesbetweendifferentmodelsarenotrelevant
to our goal of determining knowledge-cutoffs. For each category, we also overlay the computed
distributionofsimilargroundtruthdocumentsinlightgrey(whenavailable).
Pile-basedModels ThethreemodelsGPT-Neo,GPT-JandPythiaareexclusivelytrainedonthe
Pile. WeshowtheresultsoftheperplexitymeasurementsinFig.4(upper),whereweseeanoticeable
dropinperplexityaroundMarch2020. ThismonthcorrespondsexactlytothedateoftheWikipedia
includedinthePile,indicatingtheeffectivecutoffforWikipediaofthePile-basedmodelsaligns
withtheirreportedWikipediacutoff. Appropriately,wealsonotethatthedistributionofgroundtruth
versionsishighestatthatmonth,correspondingtothe3xup-sampledWikipediadumpinthePile.
FalconRW-based Models FalconRW is exclusively trained on RW while Falcon incorporates
curatedcorporafromthePile. NotethatbothmodelsweretrainedonasubsetoftheRWdataset,the
exactsubsetwhichisnotpubliclydescribed. WeseeinFig.4(middle)thattheperplexitycurveshave
alowperplexityfromlate2019toearly2021withaminimumaroundJanuary2020. Theseresults
mayseemsurprisingasFalconRWhasnotseenWikipediaintraining–however,astheoverlayed
groundtruthandSection5.1shows,itdoescontainWikipediafromothertop-leveldomains.
C4-basedModels Fig.4(lower)showsresultsforC4-derivedmodels: RedPajamas,OLMo,and
LLaMa. WhileeachmodelusesC4duringpre-training,itonlycomprisesasmallportionoftheir
respective training data. The more salient similarity is that each of the models consists of many
independentCommonCrawldumps,andtheirdifferencesineffectivecutoffdatesofthethreemodels
canbeexplainedbytheCommonCrawldumpsincludedintheirtrainingdata. LLaMAincorporates5
dumpsfrom2017to2020,andthusisthelowestduringthattimerange. Incontrast,RedPajamas
incorporates 5 dumps from 2019 to 2023, and thus sees a minimum perplexity at a later date.
OLMousesall20dumpsfrom2019to2023,andthusseesaminimumperplexityatthelatestdate.
Nonetheless,westillseetheeffectofC4onthesemodels,astherespectiveminimaarebiasedtowards
theC4CommonCrawldumpdateofApril2019. SeeSection5.2foranexplicitbreakdownofthe
entiretrainingdataofRedPajamas.
6Figure4: Relativeperplexitiesofmodelspermonthusingthe WIKISPAN (§3.1)dataset(weuse
relativeasexactperplexitiesarenotneededfordeterminingeffectivecutoffs). UpperplotshowsPile
derivedmodels,middleshowsFalconRWderivedmodels,whilelowershowsC4derivedmodels.
ThelightgreybarsindicatethedistributionofWikipedia-alikedocuments,matchedtotheirclosest
version,ascalculatedinSection3.5. Notethatthesedatasetsareonlyasubsetofthetrainingsetfor
somemodels. Insomecasestheknowledgecutoffalignswiththemodel’seffectivecutoff(e.g. the
Pile)whileformorerecentmodelstheyarealignedmuchearlier(e.g. RedPajamasto2019,even
thoughithasanexplicit2023Wikipediadump).
4.3 ImpactofScale
Wealsoconsidertheeffectofmodelsizeonourmethods. WeevaluatetheperplexityofWIKISPAN
underasuiteofPythia(460M,1B,2.8B,6.9B,12B)andLLaMA(7B,13B,65B)models. Fig.5
showsthatwhilethesmallermodelshaveamorevaryingperplexitycurve,theyarestillminimizedat
theexpecteddate. Thismakesintuitivesenseasthemodelsarealltrainedonthesamedata.
Figure5: RelativeperplexitiesofmodelsinthePythia(left)andLLaMA(right)suites. Darkercolors
indicatelargermodelsize. Whilethesmallermodelshaveamorevariableperplexitycurve,theyare
stillminimizedatthesameeffectivecutoffdate.
7...Bytheendofthe17thcentury,theChineseeconomyhadrecoveredfromthedevastationcausedbythewarsinwhichtheMingdynasty
wereoverthrown,andtheresultingbreakdownoforder.[147]Inthefollowingcentury,marketscontinuedtoexpandasinthelateMing
period,butwithmoretradebetweenregions,agreaterdependenceonoverseasmarketsandagreatlyincreasedpopulation.[148].[149]
ThegovernmentbroadenedlandownershipbyreturninglandthathadbeensoldtolargelandownersinthelateMingperiodbyfamilies
unabletopaythelandtax.[150]Togivepeoplemoreincentivestoparticipateinthemarket,theyreducedthetaxburdenincomparison
withthelateMing,andreplacedthecorvéesystemwithaheadtaxusedtohirelaborers.[151]TheadministrationoftheGrandCanalwas
mademoreefficient,andtransportopenedtoprivatemerchants.[152]Asystemofmonitoringgrainpriceseliminatedsevereshortages,
andenabledthepriceofricetoriseslowlyandsmoothlythroughthe18thcentury.[153]Waryofthepowerofwealthymerchants,Qing
rulerslimitedtheirtradinglicensesandusuallyrefusedthempermissiontoopennewmines,exceptinpoorareas...
...Bytheendofthe17thcentury,theChineseeconomyhadrecoveredfromthedevastationcausedbythewarsinwhichtheMingdynasty
wereoverthrown,andtheresultingbreakdownoforder.[148]Inthefollowingcentury,marketscontinuedtoexpandasinthelateMing
period,butwithmoretradebetweenregions,agreaterdependenceonoverseasmarketsandagreatlyincreasedpopulation.[149].[150]
ThegovernmentbroadenedlandownershipbyreturninglandthathadbeensoldtolargelandownersinthelateMingperiodbyfamilies
unabletopaythelandtax.[151]Togivepeoplemoreincentivestoparticipateinthemarket,theyreducedthetaxburdenincomparison
withthelateMing,andreplacedthecorvéesystemwithaheadtaxusedtohirelaborers.[152]TheadministrationoftheGrandCanalwas
mademoreefficient,andtransportopenedtoprivatemerchants.[153]Asystemofmonitoringgrainpriceseliminatedsevereshortages,
andenabledthepriceofricetoriseslowlyandsmoothlythroughthe18thcentury.[154]Waryofthepowerofwealthymerchants,Qing
rulerslimitedtheirtradinglicensesandusuallyrefusedthempermissiontoopennewmines,exceptinpoorareas...
Table2: Anexampleofnear-duplicateWikipediadocumentsthataresemanticallyequivalentinthe
FalconRWdataset,differingonlybythereferencenumbers. Thedocumentscontaindifferentversions
oftheWikipediaarticle“QingDynasty,”andarelocatedonlines168922and97669ofthe5thand
3970thparquetfilesinthepublicreleaseofFalconRW.Thecoloredtextindicatesexactmatches.
5 Whyaremodelsnotalignedtotheircutoffdate?
In this section we describe why a model’s effective cutoff and reported cutoff can differ. This
misalignment is due to two main factors: (1) deduplication pipelines that ignore semantically
equivalentbutlexicallynearduplicatesand(2)theoldertimealignmentofCommonCrawldumps.
5.1 ComplicationsinDeduplicationPipelines
It is common practice in LLM training pipelines to deduplicate data. When a dataset undergoes
fuzzy or exact deduplication, one might expect that different versions of Wikipedia pages (near
duplicates)andcopiesofWikipediapages(exactduplicates),respectively, areremovedfromthe
dataset. However,weempiricallyfindthatthereexistalargenumberofnearandexactduplicatesin
trainingdatasets,andprovideexamplesforeach.
FalconRW FalconRWremovedalldocumentsthathadthetop-leveldomainofWikipedia,sothey
coulduseFalconRWinconjunctionwithacuratedversionofWikipediainthefuture(astheydidin
theunreleasedmainFalcondataset). Theyassumedthiswoulddeduplicatethedata,however,wefind
thatduetoversioningandWikipediamirrors,therearestillnearduplicateWikipediadocuments,as
showninTable2,wherethetextsdifferonlybythereferencenumbers.
Pile Similarly,thePilealsocontainsaccidentallyduplicatedWikipediadocuments.However,dueto
theexplicit3xWikipediaup-sampling,theseaccidentalduplicatesarerelativelyminorincomparison.
C4 C4wascreatedfromoneCommonCrawldumpand“discardedallbutoneofanythree-sentence
spanoccurringmorethanonce."However,weshowanexampleinTable3ofapairofdocuments
whichcontainthesamethree-sentencespan. Beyondthis,wecanalsoseethatthedocumentsare
semanticallyequivalent,anddifferonlybywhitespacecharacters.
RedPajamas WefindthattheRedPajamasCommonCrawldump,whichunderwentparagraph-level
deduplication,containsexactduplicatedocuments. WeshowanexampleinTable4.
Discussion Outofallthemodelsweevaluated,onlyPilederivedmodelsexhibitsharpalignment
towards their reported Wikipedia dump date. This is due to two main factors: the size of its
CommonCrawldata(whichisminorcomparedtoothermodels)andthepurposefulup-samplingof
theirWikipediadumptomatchtheirdesireddate. ThemassiveamountsofCommonCrawldatathat
othermodelsaretrainedoncompoundstheirissueswithdeduplication,leadingtomanyversionsof
Wikipediadocumentswhicharenotnecessarilyoftheversionoftheirreporteddumpdate.
8Figure6: RelativeperplexitiesofmodelstrainedonPileandPile-dedup. Weseethatdeduplicating
the3xup-sampledWikipediainthePileresultsinanoldertemporalalignmentduetotheincluded
WikipediadocumentsfromCommonCrawl.
We confirm this hypothesis by comparing Pythia vs. Pythia-deduplicated. Fig. 6 shows that the
deduplicatedPythia,whichremovesthepurposefullyup-sampledWikipediadocuments,nolonger
hasthesharpminimumofstandardPythiaandinsteadhasaearliereffectivecutoff(duetotheolder
CCdocuments). Thus,weseethattheaccidentduplicatesandthelackofpurposefulduplicates(of
versionscorrespondingtothedesiredeffectiveknowledgecutoff)createsthismisalignment.
Figure7: DistributionofWikipediaversionsovertheentireRedPajamastrainingset. Eachcolor
representsadifferentsupcorporaofthetrainingset(greenforCommonCrawl,orangeforWikipedia,
andblueforC4). TheblacklinerepresentstheperplexitycurveofRedPajamas7BoverWIKISPAN
5.2 MisalignmentofCommonCrawlDumpDates
AllourevaluatedmodelsaretrainedonsomeportionofCommonCrawldata,withrecentmodels
using large proportions of it. Our ground truth results in Fig. 4 (especially C4-derived models)
confirms previous work from Dodge et al. (2021) that suggest that a non-trivial amount of data
insideofaCommonCrawldumpisactuallyolddata. InthecontextofWikipedia,thismeansthata
CommonCrawldumpin2023oftencontainsmanyversionsofdocumentsdatingbackto2016. Thus,
eventhoughmodelsmayincludeCommonCrawldumpsfromarangeofdates,theaggregateddata
willbeextremelybiasedtowardsearlierdates.
To illustrate this concretely for a “newer" style model composed of large amounts of Common-
Crawl, we collect the ground truth from all the resources containing Wikipedia in RedPajamas
(CommonCrawl,C4,andexplicitWikipedia)andoverlayitsperplexitycurveinFig.7.7
WeseethatalthoughthedirectWikipediadumpisincludedinthepre-trainingdata,over80%of
theWikipediadocumentsarefromearlierversions(pre-2023). Moreover,versionsfromtheearlier
monthscanandoftenhaveduplicateversionsasdescribedpreviously,whilethedocumentsinthe
directWikipediadumparetypicallynotduplicated. Wealsoseethatperplexityisminimizedaround
thedateoftheseCommonCrawlWikipediaversionsthatcomposethemajority,inmid-2019.
5.3 Summary
Thus,weseethatthereexiststworeasonsthatcontributetothetemporalmisalignmentoflanguage
model’sreportedcutoffvs. theeffectivecutoff: (1)failuresofcurrentdeduplicationpipelinesto
7Notethatalthoughweperformedthisanalysisbybinningdocumentsbytheirmostsimilarversion,onecan
computeann-gramanaylsiswithsimilarresults(AppendixC).
9controlforsemanticduplicatesand(2)theusageandrelianceonnewerCommonCrawldumpsto
providedupdateinformationwheninactualitythereisasignificantamountofolderdata.
6 Conclusion
It is now common practice for Large Language Models to provide a “knowledge-cutoff" which
intends to communicate to users the date at which LLMs no longer have up to date information.
However, thissimplemetricoversimplifiesLLMtraininginadetrimentalmannertousability; it
leavesunansweredthequestionsof“isthisknowledgecutoffspecificforallresourcesinthemodel",
“howmanycopiesofmyresourceareinthemodel"or“whichversionsofmycorpusareincluded?"
We propose a method to automatically determine the effective cutoff date of LLMs for a given
resourceandshowthatalthoughsometimesitdoesalignwiththereportedcutoff,inmanycasesit
doesnot. Todeterminewhytheyfailtoalign,weanalyzethetrainingdataofopen-dataLLMsto
discoverthattherearelargequantitiesofnear-duplicatesinLLMtrainingdata(suchasdiffering
onlyinthecitationnumbersincludedinthetext)despiteeffortsfromLLMcreatorstodeduplicate.
Further,mostLLMsrelyonCommonCrawldumpsfordata,despitethefactthatanon-trivialamount
ofCommonCrawldataismucholderthanthereporteddumpdate. Wehopethisanalysiswillprovide
insightforusersofLLMswhoneedresource-specificknowledgecutoffsandforLLM-creatorswho
seektoaligntheirLLMstoagivendate.
References
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,AlessandroCappelli,RuxandraCo-
jocaru, Merouane Debbah, EtienneGoffinet, Daniel Heslow, JulienLaunay, QuentinMalartic,
BadreddineNoune,BaptistePannier,andGuilhermePenedo. Falcon-40B:anopenlargelanguage
modelwithstate-of-the-artperformance. 2023a.
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,AlessandroCappelli,RuxandraCojo-
caru,MérouaneDebbah,ÉtienneGoffinet,DanielHesslow,JulienLaunay,QuentinMalartic,etal.
Thefalconseriesofopenlanguagemodels. arXivpreprintarXiv:2311.16867,2023b.
StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal.
Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. InInternational
ConferenceonMachineLearning,pp.2397–2430.PMLR,2023.
SidBlack,StellaBiderman,EricHallahan,QuentinG.Anthony,LeoGao,LaurenceGolding,Horace
He,ConnorLeahy,KyleMcDonell,JasonPhang,MichaelMartinPieler,USVSNSaiPrashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Benqi Wang, and Samuel Weinbach. Gpt-
neox-20b: Anopen-sourceautoregressivelanguagemodel. ArXiv,abs/2204.06745,2022. URL
https://api.semanticscholar.org/CorpusID:248177957.
NicholasCarlini,FlorianTramer,EricWallace,MatthewJagielski,ArielHerbert-Voss,Katherine
Lee,AdamRoberts,TomBrown,DawnSong,UlfarErlingsson,etal. Extractingtrainingdata
fromlargelanguagemodels. In30thUSENIXSecuritySymposium(USENIXSecurity21), pp.
2633–2650,2021.
NicholasCarlini,DaphneIppolito,MatthewJagielski,KatherineLee,FlorianTramèr,andChiyuan
Zhang. Quantifyingmemorizationacrossneurallanguagemodels. ArXiv,abs/2202.07646,2022.
URLhttps://api.semanticscholar.org/CorpusID:246863735.
TogetherComputer. Redpajama: anopendatasetfortraininglargelanguagemodels,October2023.
URLhttps://github.com/togethercomputer/RedPajama-Data.
Jesse Dodge, Maarten Sap, Ana Marasovic´, William Agnew, Gabriel Ilharco, Dirk Groeneveld,
MargaretMitchell,andMattGardner. Documentinglargewebtextcorpora: Acasestudyonthe
colossalcleancrawledcorpus. arXivpreprintarXiv:2104.08758,2021.
YanaiElazar,AkshitaBhagia,IanMagnusson,AbhilashaRavichander,DustinSchwenk,AlaneSuhr,
PeteWalsh,DirkGroeneveld,LucaSoldaini,SameerSingh,etal. What’sinmybigdata? arXiv
preprintarXiv:2310.20707,2023.
10Manuel Faysse, Patrick Fernandes, Nuno Guerreiro, António Loison, Duarte Alves, Caio Corro,
NicolasBoizard,JoãoAlves,RicardoRei,PedroMartins,etal. Croissantllm: Atrulybilingual
french-englishlanguagemodel. arXivpreprintarXiv:2402.00786,2024.
WenjieFu,HuandongWang,ChenGao,GuanghuaLiu,YongLi,andTaoJiang.Practicalmembership
inference attacks against fine-tuned large language models via self-prompt calibration. arXiv
preprintarXiv:2311.06062,2023.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang,HoraceHe,AnishThite,NoaNabeshima,ShawnPresser,andConnorLeahy. Thepile:
An 800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027, 2020. URL
https://api.semanticscholar.org/CorpusID:230435736.
TimnitGebru,JamieMorgenstern,BrianaVecchione,JenniferWortmanVaughan,HannaWallach,
HalDauméIii,andKateCrawford. Datasheetsfordatasets. CommunicationsoftheACM,64(12):
86–92,2021.
DirkGroeneveld,IzBeltagy,PeteWalsh,AkshitaBhagia,RodneyKinney,OyvindTafjord,A.Jha,
HamishIvison,IanMagnusson,YizhongWang,ShaneArora,DavidAtkinson,RussellAuthur,
KhyathiRaghaviChandu,ArmanCohan,JenniferDumas,YanaiElazar,YulingGu,JackHessel,
Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik,
CrystalNam, MatthewE.Peters, ValentinaPyatkin, AbhilashaRavichander, DustinSchwenk,
Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep
Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca
Soldaini, Noah A. Smith, and Hanna Hajishirzi. Olmo: Accelerating the science of language
models. 2024. URLhttps://api.semanticscholar.org/CorpusID:267365485.
Sorami Hisamoto, Matt Post, and Kevin Duh. Membership inference attacks on sequence-to-
sequencemodels: Ismydatainyourmachinetranslationsystem? TransactionsoftheAssociation
for Computational Linguistics, 8:49–63, 2019. URL https://api.semanticscholar.org/
CorpusID:119302127.
NathanJ.Hu,EricMitchell,ChristopherD.Manning,andChelseaFinn. Meta-learningonlineadap-
tationoflanguagemodels. ArXiv,abs/2305.15076,2023. URLhttps://api.semanticscholar.
org/CorpusID:258866057.
DaphneIppolito,FlorianTramèr,MiladNasr,ChiyuanZhang,MatthewJagielski,KatherineLee,
Christopher A. Choquette-Choo, and Nicholas Carlini. Preventing verbatim memorization in
language models gives a false sense of privacy. ArXiv, abs/2210.17546, 2022. URL https:
//api.semanticscholar.org/CorpusID:253237404.
Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier,L’elioRenardLavaud,Marie-AnneLachaux,PierreStock,TevenLeScao,ThibautLavril,
ThomasWang, TimothéeLacroix, andWilliamElSayed. Mistral7b. ArXiv, abs/2310.06825,
2023. URLhttps://api.semanticscholar.org/CorpusID:263830494.
JungoKasai,KeisukeSakaguchi,yoichitakahashi,RonanLeBras,AkariAsai,XinyanVelocityYu,
DragomirRadev,NoahA.Smith,YejinChoi,andKentaroInui. RealtimeQA:What’stheanswer
rightnow? InThirty-seventhConferenceonNeuralInformationProcessingSystemsDatasetsand
BenchmarksTrack,2023. URLhttps://openreview.net/forum?id=HfKOIPCvsv.
AlexandraSashaLuccioni,FrancesCorry,HamsiniSridharan,MikeAnanny,JasonSchultz,andKate
Crawford. Aframeworkfordeprecatingdatasets: Standardizingdocumentation,identification,and
communication. InProceedingsofthe2022ACMConferenceonFairness,Accountability,and
Transparency,pp.199–212,2022.
LiLucy,SuchinGururangan,LucaSoldaini,EmmaStrubell,DavidBamman,LaurenKlein,and
JesseDodge. Aboutme: Usingself-descriptionsinwebpagestodocumenttheeffectsofenglish
pretrainingdatafilters. arXivpreprintarXiv:2401.06408,2024.
MarcMaroneandBenjaminVanDurme. Dataportraits: Recordingfoundationmodeltrainingdata.
arXivpreprintarXiv:2303.03919,2023.
11JustusMattern,FatemehsadatMireshghallah,ZhijingJin,BernhardSchölkopf,MrinmayaSachan,
andTaylorBerg-Kirkpatrick. Membershipinferenceattacksagainstlanguagemodelsvianeigh-
bourhoodcomparison. arXivpreprintarXiv:2305.18462,2023.
MargaretMitchell,SimoneWu,AndrewZaldivar,ParkerBarnes,LucyVasserman,BenHutchinson,
Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting.
Proceedings of the Conference on Fairness, Accountability, and Transparency, 2018. URL
https://api.semanticscholar.org/CorpusID:52946140.
MiladNasr,NicholasCarlini,JonathanHayase,MatthewJagielski,AFederCooper,DaphneIppolito,
Christopher A Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. Scalable
extractionoftrainingdatafrom(production)languagemodels. arXivpreprintarXiv:2311.17035,
2023.
GuilhermePenedo,QuentinMalartic,DanielHesslow,RuxandraCojocaru,AlessandroCappelli,
HamzaAlobeidli,BaptistePannier,EbtesamAlmazrouei,andJulienLaunay. TheRefinedWeb
datasetforFalconLLM:outperformingcuratedcorporawithwebdata,andwebdataonly. arXiv
preprintarXiv:2306.01116,2023. URLhttps://arxiv.org/abs/2306.01116.
AleksandraPiktus,ChristopherAkiki,PauloVillegas,HugoLaurenccon,GérardDupont,Alexan-
draSashaLuccioni,YacineJernite,andAnnaRogers. Therootssearchtool: Datatransparency
for llms. In Annual Meeting of the Association for Computational Linguistics, 2023. URL
https://api.semanticscholar.org/CorpusID:257219882.
MahimaPushkarna,AndrewZaldivar,andOddurKjartansson.Datacards:Purposefulandtransparent
datasetdocumentationforresponsibleai. InProceedingsofthe2022ACMConferenceonFairness,
Accountability,andTransparency,pp.1776–1826,2022.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. TheJournalofMachineLearningResearch,21(1):5485–5551,2020.
WeijiaShi,AnirudhAjith,MengzhouXia,YangsiboHuang,DaogaoLiu,TerraBlevins,DanqiChen,
andLukeZettlemoyer. Detectingpretrainingdatafromlargelanguagemodels. arXivpreprint
arXiv:2310.16789,2023.
RezaShokri,MarcoStronati,CongzhengSong,andVitalyShmatikov. Membershipinferenceattacks
againstmachinelearningmodels. In2017IEEEsymposiumonsecurityandprivacy(SP),pp.3–18.
IEEE,2017.
LucaSoldaini,RodneyKinney,AkshitaBhagia,DustinSchwenk,DavidAtkinson,RussellAuthur,
BenBogin,KhyathiRaghaviChandu,JenniferDumas,YanaiElazar,ValentinHofmann,A.Jha,
Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Daniel Morrison,
NiklasMuennighoff,AakankshaNaik,CrystalNam,MatthewE.Peters,AbhilashaRavichander,
KyleRichardson,ZejiangShen,EmmaStrubell,NishantSubramani,OyvindTafjord,PeteWalsh,
LukeZettlemoyer,NoahA.Smith,HannaHajishirzi,IzBeltagy,DirkGroeneveld,JesseDodge,
and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining
research. 2024. URLhttps://api.semanticscholar.org/CorpusID:267364861.
GemmaTeam,ThomasMesnard,CassidyHardin,RobertDadashi,SuryaBhupatiraju,ShreyaPathak,
LaurentSifre,MorganeRivière,MihirSanjayKale,JulietteLove,PouyaTafti,LéonardHussenot,
AakankshaChowdhery,AdamRoberts,AdityaBarua,AlexBotev,AlexCastro-Ros,Ambrose
Slone,AmélieHéliou,AndreaTacchetti,AnnaBulanova,AntoniaPaterson,BethTsai,Bobak
Shahriari,CharlineLeLan,ChristopherA.Choquette-Choo,ClémentCrepy,DanielCer,Daphne
Ippolito,DavidReid,ElenaBuchatskaya,EricNi,EricNoland,GengYan,GeorgeTucker,George-
ChristianMuraru,GrigoryRozhdestvenskiy,HenrykMichalewski,IanTenney,IvanGrishchenko,
Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny
Brennan,JeremyChen,JohanFerret,JustinChiu,JustinMao-Jones,KatherineLee,KathyYu,
KatieMillican,LarsLoweSjoesund,LisaLee,LucasDixon,MachelReid,MaciejMikuła,Mateo
Wirth,MichaelSharman,NikolaiChinaev,NithumThain,OlivierBachem,OscarChang,Oscar
Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni,
RamonaComanescu,ReenaJana,RohanAnil,RossMcIlroy,RuiboLiu,RyanMullins,SamuelL
12Smith,SebastianBorgeaud,SertanGirgin,SholtoDouglas,ShreePandya,SiamakShakeri,Soham
De,TedKlimenko,TomHennigan,VladFeinberg,WojciechStokowiec,YuhuiChen,Zafarali
Ahmed,ZhitaoGong,TrisWarkentin,LudovicPeran,MinhGiang,ClémentFarabet,OriolVinyals,
JeffDean,KorayKavukcuoglu,DemisHassabis,ZoubinGhahramani,DouglasEck,JoelleBarral,
Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and
KathleenKenealy. Gemma: Openmodelsbasedongeminiresearchandtechnology,2024.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanielM.Bikel,Lukas
Blecher,CristianCantónFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,
JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyS.
Hartshorn,SagharHosseini,RuiHou,HakanInan,MarcinKardas,ViktorKerkez,MadianKhabsa,
IsabelM.Kloumann,A.V.Korenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,
JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,Pushkar
Mishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,Kalyan
Saladi,AlanSchelten,RuanSilva,EricMichaelSmith,R.Subramanian,XiaTan,BinhTang,Ross
Taylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengxuYan,IliyanZarov,YuchenZhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey
Edunov,andThomasScialom. Llama2: Openfoundationandfine-tunedchatmodels. ArXiv,
abs/2307.09288,2023b. URLhttps://api.semanticscholar.org/CorpusID:259950998.
BenWangandAranKomatsuzaki.GPT-J-6B:A6BillionParameterAutoregressiveLanguageModel.
https://github.com/kingoflolz/mesh-transformer-jax,May2021.
Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, and Benjamin
VanDurme. "accordingto..."promptinglanguagemodelsimprovesquotingfrompre-training
data. arXivpreprintarXiv:2305.13252,2023.
GuillaumeWenzek,Marie-AnneLachaux,AlexisConneau,VishravChaudhary,FranciscoGuzmán,
ArmandJoulin,andEdouardGrave. Ccnet: Extractinghighqualitymonolingualdatasetsfrom
webcrawldata,2019.
13A ProbingPseudocode
We denote the pre-training dataset D. Let M∗ denote the month corresponding to the reported
WikipediadumpinD. LetD
T,M
representtheversionoftopicT atmonthM. EDITreferstothe
Levenshteindistancebetweentwostrings.
Algorithm1CountingversionsofdocumentsinWIKISPAN
1: procedureRETRIEVE(D,M∗)
2: counts←{}
3: forTopicT ∈ WIKISPANdo
4: Q←D T,M∗[:512] ▷Querywiththefirst512tokens
5: R← BM25(Q,D)[:10] ▷10retrievalresults
6: forMatchedDocumentD ∈Rdo
7: dists←[]
8: forVersionV ∈D T,Mstart··· ,D T,Mend do
9: dist← EDIT(D,V)/len(D)
10: dists.append(dist)
11: endfor
12: if min(dists)<0.2then
13: min_months←argmin(dists) ▷tiesidenticaldocversions
14: form∈min_monthsdo
15: counts[m]←counts[m]+1/len(min_months)
16: endfor
17: endif
18: endfor
19: endfor
20: returncounts
21: endprocedure
B DeduplicationComplications
NataliePortmanisanactresswithdualAmericanandIsraelicitizenship.Herfirstrolewasasanorphantakeninbyahitmaninthe1994
actionfilmLéon:TheProfessional,butmainstreamsuccesscamewhenshewascastasPadméAmidalaintheStarWarsprequeltrilogy
(releasedin1999,2002and2005).In1999,sheenrolledatHarvardUniversitytostudypsychologywhilestillworkingasanactress.She
completedherbachelor’sdegreein2003.In2001,PortmanopenedinNewYorkCity’sPublicTheaterproductionofAntonChekhov’s
TheSeagull.In2005,PortmanwonaGoldenGlobeAwardandreceivedanAcademyAwardnominationforBestSupportingActressfor
herperformanceinthedramaCloser.ShewonaConstellationAwardforBestFemalePerformanceandaSaturnAwardforBestActress
forherstarringroleinVforVendetta(2006).SheplayedleadingrolesinthehistoricaldramasGoya’sGhosts(2006)andTheOther
BoleynGirl(2008).InMay2008,sheservedastheyoungestmemberofthe61stAnnualCannesFilmFestivaljury.Portman’sdirectorial
debut,Eve,openedthe65thVeniceInternationalFilmFestival’sshortscompetitionin2008.Portmandirectedasegmentofthecollective
filmNewYork,ILoveYou.PortmanisalsoknownforherportrayalasJaneFoster,theloveinterestofMarvelsuperheroThor,inthe
filmadaptationThor(2011),anditssequel,Thor:TheDarkWorld...(2013).In2010,Portmanstarredinthepsychologicalthriller
BlackSwan.HerperformancereceivedcriticalpraiseandearnedherasecondGoldenGlobeAward,theScreenActorsGuildAward,the
BAFTAAward,theBroadcastFilmCriticsAssociationAwardandtheAcademyAwardforBestActressin2011.
NataliePortmanisanactresswithdualAmericanandIsraelicitizenship.Herfirstrolewasasanorphantakeninbyahitmaninthe1994
actionfilmLéon:TheProfessional,butmainstreamsuccesscamewhenshewascastasPadméAmidalaintheStarWarsprequeltrilogy
(releasedin1999,2002and2005).In1999,sheenrolledatHarvardUniversitytostudypsychologywhilestillworkingasanactress.She
completedherbachelor’sdegreein2003.\nIn2001,PortmanopenedinNewYorkCity’sPublicTheaterproductionofAntonChekhov’s
TheSeagull.In2005,PortmanwonaGoldenGlobeAwardandreceivedanAcademyAwardnominationforBestSupportingActressfor
herperformanceinthedramaCloser.ShewonaConstellationAwardforBestFemalePerformanceandaSaturnAwardforBestActress
forherstarringroleinVforVendetta(2006).SheplayedleadingrolesinthehistoricaldramasGoya’sGhosts(2006)andTheOther
BoleynGirl(2008).InMay2008,sheservedastheyoungestmemberofthe61stAnnualCannesFilmFestivaljury.Portman’sdirectorial
debut,Eve,openedthe65thVeniceInternationalFilmFestival’sshortscompetitionin2008.Portmandirectedasegmentofthecollective
filmNewYork,ILoveYou.PortmanisalsoknownforherportrayalasJaneFoster,theloveinterestofMarvelsuperheroThor,inthe
filmadaptationThor(2011),anditssequel,Thor:TheDarkWorld...(2013).\nIn2010,Portmanstarredinthepsychologicalthriller
BlackSwan.HerperformancereceivedcriticalpraiseandearnedherasecondGoldenGlobeAward,theScreenActorsGuildAward,the
BAFTAAward,theBroadcastFilmCriticsAssociationAwardandtheAcademyAwardforBestActressin2011.
Table3: AnexampleofexactthreesentenceduplicatesinC4,alongwithsemanticallyequivalenttext
following. ThetwodocumentsareversionsoftheWikipediaarticle“NataliePortman.” Thecolored
textindicatesexactmatches.
14"AdamRichardSandler(bornSeptember9,1966)isanAmericanactor,comedian,screenwriter,filmproducer,andmusician.After
becomingaSaturdayNightLivecastmember,hewentontostarinmanyHollywoodfeaturefilmsthathavegrossedover$2billionat
theboxofficecombined.Sandler’swell-knownrolesincludeBillyMadison(1995),HappyGilmore(1996),TheWaterboy(1998),The
WeddingSinger(1998),BigDaddy(1999),Mr.Deeds(2002),50FirstDates(2004),TheLongestYard(2005),Click(2006),Grown
Ups(2010),JustGowithIt(2011),GrownUps2(2013),Blended(2014),andMurderMystery(2019).HealsovoicesDraculainthe
HotelTransylvaniafranchise(2012–present).Someofhisfilms,suchasthewidelypannedJackandJill,havebeenheavilycriticized,
culminatinginasharedsecondplaceinthenumberofRaspberryAwards(3)andRaspberryAwardnominations(11),inbothcases
secondonlytoSylvesterStallone.SandlerventuredintodramaticterritorywithhisrolesinPunch-DrunkLove(2002),Spanglish(2004),
ReignOverMe(2007),FunnyPeople(2009),TheMeyerowitzStories(NewandSelected)(2017),andUncutGems(2019),allofwhich
earnedhimcriticalpraise."
Table4:AnexampleoftheexactdocumentduplicatesinRedPajamas. Thedocumentsareversionsof
theWikipediaarticle“AdamSandler,”andisduplicated10timesintheRedPajamasCommonCrawl
trainingdata.
C N-gramAnalysisinsteadofExactMatch
Howisperplexityaffectedwhenseeingtwolexicallysimilartexts? Onehypothesisisthatperplexity
ismostaffectedbyexactandnear-duplicates. Anotherhypothesisisthattheactualtextinadocument
isfactoraffectingperplexity. Toillustratethedifferencebetweenthesehypotheses,adocumentthat
containsshuffledsentencesfromaversionofaWikipediaarticle(shufflingorderofsentences)would
notaffectperplexityintheformercase,butwouldinthelattercase. Theformehhypothesisisthe
basisbehindouralgorithmforattributingmatcheddocumentstotheirclosestversions. Wetestthe
latterhypothesisbyproposinganotherwaytoattributecredit,directlycountingtheintersectionof
n-gramsinmatcheddocumentswithprecomputedsetsofn-gramssourcedfromWIKISPAN. We
discountbythenumberoftimesanngramappearsacrossallmonths(similartoaninversedocument
frequency)inordertocountngramsthataredistincttoaspecificWikipediaversion. Ouralgorithm
andresultsaredescribedinAppendixC.1
C.1 AlgorithmPseudocode
WeusethesamenotationasinAppendixA
Algorithm2Countingn-gramsofdocumentsinWIKISPAN
1: ngrams←{}
2: forMonthm∈ WIKISPANdo ▷Computealln-gramsofalldocsinmonthm
3: ngrams[m]←Counter([NGRAMS(D T,m)forT ∈ WIKISPAN])
4: endfor
(cid:84)
5: common_ngrams← ngrams[m]
m∈WIKISPAN
6: procedureRETRIEVE(D,M∗)
7: counts←{}
8: forTopicT ∈ WIKISPANdo
9: Q←D T,M∗[:512] ▷Querywiththefirst512tokens
10: R← BM25(Q,D)[:10] ▷10retrievalresults
11: forMatchedDocumentD ∈Rdo
12: forngramn∈ NGRAMS(D[:512])do
13: formonthm∈ WIKISPANdo
14: ifn∈ngrams[m]then
15: counts[m]←counts[m]+ngrams[m][n]−common_ngrams[n]
16: endif
17: endfor
18: endfor
19: endfor
20: endfor
21: returncounts
22: endprocedure
15C.2 Results
Weshowtheperplexitycurvesofourevaluatedmodelsandcompareitwithournewn-gramstatistics.
Figure8: Relativeperplexitiesofmodelspermonthusingthe WIKISPAN (§3.1)dataset(weuse
relativeasexactperplexitiesarenotneededfordeterminingeffectivecutoffs). UpperplotshowsPile
derivedmodels,middleshowsFalconRWderivedmodels,whilelowershowsC4derivedmodels.
Thelightgreybarsindicatethegroundtruthsimilardocuments,matchedtotheirclosestversion,
ascalculatedinAppendixC.Notethatthesedatasetsareonlyasubsetofthetrainingsetforsome
models. Insomecasestheknowledgecutoffalignswiththemodel’seffectivecutoff(e.g. thePile)
whileformorerecentmodelstheyarealignedmuchearlier(e.g. RedPajamasto2019,eventhoughit
hasanexplicit2023Wikipediadump).
16D ClosedModelResults
Weevaluateourmethodontheclosed-datamodelsGemma(Teametal.,2024),LLaMA-2(Touvron
etal.,2023b)andMistral(Jiangetal.,2023)inFig.9. WeseethatMistral/LLaMA-2,likemanyof
theopen-datamodelsweanalyze,hasamuchearliereffectivecutoffforWikipedia. Incontrast,we
seethatGemmahasamuchlatereffectivecutoff,indicatingtheirsuccessataligningWikipediato
roughly2021.
Figure9: RelativeperplexitiesofmodelspermonthusingtheWIKISPANdataset(weuserelativeas
exactperplexitiesarenotneededfordeterminingeffectivecutoff).
17