Chain-of-Spot: Interactive Reasoning Improves
Large Vision-Language Models
Zuyan Liu1â‹†, Yuhao Dong1â‹†, Yongming Rao2â€ , Jie Zhou1, and Jiwen Lu1â€ 
1 Department of Automation, Tsinghua University 2 Tencent
Question: Question:
What is the number on the What color are the two cars in
left brown bus? the middle front?
LLaVA-1.5: LLaVA-1.5:
The number on the left ðŸ¤¯ The twocarsinthemiddle ðŸ¤¯
brown bus is 10. front are red.
LLaVA-1.5-CoS: LLaVA-1.5-CoS:
Ans1:Theregionofinterestis: Ans1:Theregionofinterestis:
Ans2:Thenumberis95.ðŸ¤— Ans2:Red and blue. ðŸ¤—
Fig.1: Chain-of-Spot encourages Large Vision-Language Models to identify the re-
gion of interest (ROI) in the image condition on the question and reasoning through
an interactive manner, thereby improving the ability of visual understanding.
Abstract. In the realm of vision-language understanding, the profi-
ciency of models in interpreting and reasoning over visual content has
becomeacornerstonefornumerousapplications.However,itischalleng-
ingforthevisualencoderinLargeVision-LanguageModels(LVLMs)to
extractusefulfeaturestailoredtoquestionsthataidthelanguagemodelâ€™s
response. Furthermore, a common practice among existing LVLMs is to
utilizelower-resolutionimages,whichrestrictstheabilityforvisualrecog-
nition.OurworkintroducestheChain-of-Spot(CoS)method,whichwe
describeasInteractiveReasoning,anovelapproachthatenhancesfeature
extractionbyfocusingonkeyregionsofinterest(ROI)withintheimage,
corresponding to the posed questions or instructions. This technique al-
lowsLVLMstoaccessmoredetailedvisualinformationwithoutaltering
the original image resolution, thereby offering multi-granularity image
features. By integrating Chain-of-Spot with instruct-following LLaVA-
1.5models,theprocessofimagereasoningconsistentlyimprovesperfor-
manceacrossawiderangeofmultimodaldatasetsandbenchmarkswith-
outbellsandwhistlesandachievesnewstate-of-the-artresults.Ourem-
piricalfindingsdemonstrateasignificantimprovementinLVLMsâ€™ability
tounderstandandreasonaboutvisualcontent,pavingthewayformore
â‹† Equal contribution. â€ Corresponding authors.
4202
raM
91
]VC.sc[
1v66921.3042:viXra2 Liu et al.
sophisticatedvisualinstruction-followingapplications.Codeandmodels
are available at https://github.com/dongyh20/Chain-of-Spot.
Keywords: Large Vision-Language Models Â· Chain-of-Spot
1 Introduction
The progress of large vision-language models (LVLMs) [1,19,23] has attracted
growingattentioninthefieldofcomputervision.Combiningavisualencoderand
a pre-trained large language model (LLM) [6,38], large vision-language models
enable language models to extend their understanding beyond textual informa-
tion to include visual data. This integration has opened up new possibilities
where users can engage in natural language conversations about visual content,
makingthetaskincreasinglyimportantandpopularacrossvariousdomains.De-
spite the rapid development, substantial room exists to further improve models,
including hallucinations, limited input resolution, and weak interpretability.
Previousstudiesinlargelanguagemodelshaveshownthattechniquessuchas
the chain-of-thought [42] represent a promising direction to enhance the reason-
ing capabilities of LLMs and improve the correctness of the generation results
by guiding them through a structured thought process. Unlike LLMs, which
primarily process and interpret text, LVLMs face the distinct challenge of com-
prehending and reasoning about the visual information presented in images.
Therefore,itisnaturaltothink:could LVLMs also be improved if we guide them
to reason more deeply about the input visual content?
A distinct and fundamental problem in visual understanding is the need
to understand visual content at vastly different scales. Considering the diverse
queries in visual instruction-following problems, it becomes even more challeng-
ingforthevisualencoderofLVLMstoextractusefulfeaturescontainingvarious
information that can help the language models understand and respond to var-
ious questions. Moreover, the computational burden imposed by processing an
extensive number of tokens generated from high-resolution images has led to a
common practice among existing LVLMs to utilize images of lower resolution,
typically sized at 224x224 or 336x336 pixels. While recent methods like Mon-
key[19]andLLaVA-HD[22]haveintroducedmultiplevisualfeaturestoalleviate
theproblems,simplyincreasingthevisualtokenswillalsobringconsiderableex-
tra computation due to the long sequence length.
In this paper, we introduce an efficient, natural, and innovative approach
to significantly enhancing the visual reasoning process of large vision-language
models through Chain-of-Spot, which we term our method as Interactive Rea-
soning. We contend that the key to advancing LVLMsâ€™ ability to reason visually
lies in instructing them to identify which parts of the given images are perti-
nent to answering a given question. After being equipped with the ability to
identify the key region of interest (ROI) regarding the target questions or in-
structions, we zoom in on the critical part of the original image to offer LVLMs
more detailed information. Therefore, we manage to perform multi-granularityChain-of-Spot 3
imagefeatureswhilemaintainingtheimageresolutioninvariant.Tosubstantiate
our hypothesis, we have redesigned the training and inference procedures of the
widely adopted instruct-tuning pipelines. Specifically, we have restructured the
question-answeringframeworkwithinLVLMsinaninteractivemanner.Initially,
wenecessitatetheLVLMstopinpointtheregionofinterestrelevanttotheques-
tion and subsequently integrate both the global image features and localized
region of interest (ROI) features to deduce the final answer. To instantiate this
methodology,wehavecuratedanenhancedversionoftheinstruct-tuningdataset
by annotating the region of interest (ROI) condition on the question by calcu-
lating the relevance map between language token attention and image features.
Our findings indicate that this refined approach can considerably bolster the
performance of vision-language models across a wide range of vision-language
benchmarks.
WeevaluateourideaofChain-of-Spotbyintegratingourpipelinetoimprove
the ability of the popular visual instruction tuning technique LLaVA [22,23].
We have observed a substantial leap in performance metrics after a single epoch
of fine-tuning. Most notably, our enhanced version of LLaVA/13B achieves im-
pressive results on a wide range of multimodal datasets. In the visual question
answeringtasks,accuracyiselevatedfrom80.0%to81.8%with1.8%promotion
on VQAv2 [9]. In GQA [12], we witness an increase from 63.3% to 64.8%, while
in VizWiz [10], the accuracy rises markedly from 53.6% to 58.0%. The trend of
improvement was consistent in multimodal benchmarks as well. For example, in
SEEDBench [15], the accuracy was augmented from 61.6% to 62.3%. Further-
more, when evaluating the long-context generation on the MM-Vet [44] dataset
evaluatedbytheGPT-4[30]model,weachievedanaccuracyboostfrom35.4%to
37.6%.Asimilarpromotioniswitnessedonthesmallervision-languageinstruct-
following model LLaVA/7B, where we observe a similar lift on the multimodal
benchmarks after fine-tuning with the idea of Chain-of-Spot. These results not
only underscore the efficacy of Chain-of-Spot when applied to a state-of-the-art
visual instruction tuning technique but also pave the way for future exploration
of the reasoning mechanism of vision-language models.
2 Related Work
Large Vision-Language Models. Recent advancements in Large Language
Models (LLMs) [2,29,37,38], including the notable ChatGPT [27], have show-
cased remarkable capabilities in language generation and interaction based on
humanprompts.TheintegrationofLLMswithvisualdatahasunlockedthepo-
tentialforthesemodelstonotonlycomprehendimagecontentbutalsorecognize
and interpret image components, thereby facilitating the emergence of Large
Vision-Language Models (LVLMs). The BLIP series [7,16,17], encompassing
BLIP, BLIP2, and Instruction-BLIPs, has demonstrated that pre-trained lan-
guage models are pivotal for visual-language tasks, addressing challenges in im-
age captioning, visual question answering, and instruction-following. The KOS-
MOS series [11,31] and Shikra [5] have showcased LVLMsâ€™ proficiency in visual4 Liu et al.
grounding and detection tasks through pre-training on interwoven image-text
data. LLaVA [23] recently introduced an approach to fine-tune open language
models using GPT-generated visual instruction data, significantly enhancing
practical instruction-following task performance. Subsequent works, including
Qwen-VL [1], LLaVA-1.5 [22], and mPLUG-Owl [43], have continued to propel
the capabilities of Large Vision-Language Models by implementing improved
tuning strategies, expanding alignment data, and incorporating object-level an-
notations. Nevertheless, the efficacy of current LVLMs is hampered by the res-
olution of input images, as processing high-resolution imagery demands more
robust image encoders and leads to a steep increase in computational costs.
LLaVA-HD [22] and Monkey [19] have experimented with image patching tech-
niquestohandlehigherresolutions,whichis,however,costlyandinefficient.Our
workseekstointroduceamoreefficientandeffectiveapproachthroughournovel
concept of Chain-of-Spot, which guides the model to identify regions of interest
within an image and interactively respond to queries, thereby addressing the
challenges posed by high-resolution image processing.
Visual Understanding with Multi-Scale Features. The theme of visual
understanding with multi-scale features focuses on effectively representing and
processing visual information at multiple scales to improve the performance of
various tasks. The Feature Pyramid Network (FPN) [20] constructs seman-
tic feature maps at multiple scales, enabling the detection of objects across
a range of sizes with enhanced accuracy. This concept of a pyramidal hierar-
chy of features has been widely adopted and extended in various subsequent
works [3,14,46]. The UNet architecture [35] leverages a contracting path to as-
similatecontextualinformationbyutilizingmulti-scalefeatures.High-Resolution
Network(HRNet)[39]maintainshigh-resolutionrepresentationsthroughoutthe
network and effectively captures fine details by parallelly connecting convolu-
tions across resolutions. Recent work [40,41,45] find that utilizing multi-scale
features can boost performance while improving recognition modelsâ€™ efficiency.
Glance and Focus [41] introduces a two-stage approach where the network first
glances at the entire image to identify regions of interest and then focuses on
these regions for detailed analysis, exploiting the benefits of multi-scale process-
ing. However, regarding the meaningful usage of multi-scale features in the field
of visual understanding, the method of exploiting multi-scale knowledge when
generating vision-language response remains undiscovered.
3 Method
In this section, we perform a detailed introduction to the idea of the Chain-of-
Spotapproach,anefficient,rational,andnovelpipelinetoeffectivelyimproveex-
isting vision-language modelsâ€™ inference capabilities. We begin with a high-level
overview of the motivation and idea of Chain-of-Spot, followed by a comprehen-
sive description of the implementation pipeline of Chain-of-Spot, including data
annotation, fine-tuning, and inference procedure on multimodal LLMs.Chain-of-Spot 5
Text Token Visual Token Response 1: Response 2:
<bounding box> <A>
Language Model
Instruction 1: Instruction 2:
<Img> To answer the question: <Q>, The region of interest in the image is <ROI Img>.
where is the region of interest in the image? Answer the question: <Q>.
Fig.2: Procedure of Chain-of-Spot. Wecombineoriginalvisualtokenswithques-
tions in instruction 1 to ask LLMs to generate regions of interest (ROI) in the image.
Then, we use the cropped image together with the questions again to generate better
responses.
3.1 Overview of Chain-of-Spot
The integration of multimodal components within Large Vision-Language Mod-
els (LVLMs) typically involves a visual encoder, denoted as E , and a language
V
embedding encoder, represented by E . Initially, the model is presented with an
L
image I and an accompanying textual prompt Q, which could be a question or
instruction. The visual encoder E , which is pre-trained, is employed to extract
V
visualfeaturerepresentationsfromI.Concurrently,thelanguagecomponentpro-
cessesQbytokenizingandembeddingthetextualdataintoavectorspaceusing
the encoder E . The VLM then amalgamates these multimodal elements into
L
a cohesive representation. Subsequently, the model utilizes a transformer-based
large language model M, which has undergone fine-tuning across a spectrum of
domains,togenerateapertinenttextualresponseR.Theoperationalmechanism
oftheVLM,whichunderliestheresponsegeneration,canbemathematicallyex-
pressed as follows:
R=M[ E (I), E (Q) ] (1)
V L
The impetus behind Chain-of-Spot is rooted in the processing methodology of
LargeVision-LanguageModels(LVLMs).Conventionalimageencoderstypically
analyze an entire image, extracting a comprehensive global representation. This
approach, however, often fails to effectively discern and prioritize the salient
regionswithintheimagethataremostrelevanttoagivenquestion.Wehypoth-
esize that for a specific image I and query Q, there exists an optimal region
of interest I = Crop(I|Q) that holds the key to answering the question. By
C|Q
dynamicallypromptingthegenerationmodeltoidentifyandconcentrateonthis
critical region I , we believe we can significantly enhance the modelâ€™s rea-
C|Q
soning capabilities. This interactive mechanism encourages a more focused and
effectiveanalysis,potentiallyleadingtomoreaccurateandcontextuallyrelevant
responses.
Additionally, the length of image token embeddings is directly proportional
to the square of the image resolution in contemporary LVLMs. Consequently,
thesemodelstypicallyoptforlowerimageresolutionstomitigateexcessivecom-
putational demands. We posit that this compromise on resolution may be inad-6 Liu et al.
equate for the performance of state-of-the-art LVLMs, potentially overlooking
fine-grained details pivotal for addressing specific tasks. Strategies such as sim-
ply increasing the resolution of input images [22] or employing structured high-
resolution image patches [19] to magnify and extract more nuanced information
arenotwithoutdrawbacks,primarilyduetotheresultantsurgeincomputational
complexity. Hence, preserving the original image resolution while selectively en-
larging critical image regions could offer a more viable and efficient approach to
enhancing detail without prohibitive computational costs.
Building upon the two aforementioned hypotheses, we have developed an
interactivereasoning(Chain-of-Spot)approachthatworksintandemwithmulti-
scale images. This method dynamically concentrates on the prominent regions
within the images I , which are relevant to the posed questions Q. Such an
C|Q
approach is in harmony with the complex nature of human visual perception
andcognitiveprocesses.StartingwiththeoriginalLVLMswithabaselineLarge
Language Model (LLM) denoted as M, we fine-tune the VLM using a dataset
comprising the centered-image I , the question Q, and the original image I.
C|Q
Through this process, the model learns to identify the pivotal regions within
the image. The result is an enhanced LLM, M , which possesses interactive
CoS
reasoning capabilities. During the inference phase, the interaction with M is
CoS
initiated by instructing it to locate the key image area that corresponds to the
question Q, as per a pre-defined instruction prompt, denoted as Inst.1. This
step is formalized as follows:
R(I )=M [ E (I), E (Q+Inst.1) ] (2)
C|Q CoS V L
In response to R(I ), which includes the coordinates of the essential region,
C|Q
wecroptheimagetoisolateI .Subsequently,theresponsegenerationprocess
C|Q
utilizes both the original image I and the cropped image I , following the
C|Q
instruction prompt Inst.2:
R=M [ E (I,I ), E (Q+Inst.2)] (3)
CoS V C|Q L
It is important to note that the interactive generation unfolds conversationally,
therebyexposingtheentirereasoningprocessforidentifyingthekeyimageregion
to the model. The procedure of Chain-of-Spot is illustrated in Fig. 2.
3.2 Region of Interest Annotation via Relevance Propagation
Inthissection,weintroducethepipelineweusetoidentifytheregionofinterest
in the image. We first show how we delimit the informative area in an image,
and then introduce how we get the region of interest customized for a specific
question based on the attention mechanism.
In order to detect potentially important areas in an image, we utilize an
object detection model as our visual encoder to convert each image into several
regionfeatures.Consideringbothspeedandquality,FasterRCNN[34]isselected
as our detection model. Different from the traditional image encoder, the whole
image is extracted into several bounding boxes containing the correspondingChain-of-Spot 7
featuresoftheseregions.Thesefeatureswillthenbeforwardedintothelanguage
model M along with the embedded text tokens.
Subsequently, we follow the rules proposed in [4] to construct a relevance
mapÎ£ fromtheattentionmapAwhichdefinesrelationsamongallinputtokens.
The attention map is constructed based on the following rules:
(cid:18) QÂ·KâŠ¤(cid:19)
A=softmax âˆš (4)
d
h
where (Â·) represents matrix multiplication, Q and K denote query matrix and
key matrix, respectively. It should be recalled that if the attention mechanism
is performed in a self-attention manner, the query and key are projected from
input tokens, while in a cross-attention manner, the query comes from input
text tokens, and the key comes from visual input. Based on the concept of
residual connection, the relevance map Î£ is accumulated by adding up each
layerâ€™s information contained in the attention map. For each layer, the process
tounleashinformationobtainedthroughtheattentionmechanismcanbedivided
intotwosteps:first,weintroduceattentioninterpreterÎ¨ torevealvaluableclues
contained in attention maps, which can be formatted as follows:
Î¨ =E ((âˆ‡AâŠ™I (A))) (5)
h A>0
whereE denotestheaverageamongmulti-headattention,âŠ™isHadamardprod-
h
uct,âˆ‡denotesthebackpropagationofattentionmapAandI indicatesonly
A>0
positive scores are considered. Then, we progressively update the relevance map
following the rules:
Î£ =Î£+Î¨ Â·Î£ (6)
After we obtain the relevance map, we post-process the relevance map to get
the highlighted area in the image. Specifically, for each image region, we inquire
and process the corresponding row in the relevance map to obtain a relevance
score regarding the input image token. We then calculate a mask for all image
regions and determine the highlighted area by a threshold Ïµ. We simply record
the coordinates of this highlighted area as our training data.
3.3 Training and Inference
We introduce how to fine-tune the LLM for the capability of Chain-of-Spot
(CoS)M fromoriginalLLMMwiththepreparedtrainingdata(I,I ,Q):
CoS C|Q
image I, the center of the image condition on question I , and the question
C|Q
or instruction Q.
As implemented in Eq. (2), in the first step, we ask the language model M
tofindtheregionofinterestconditiononthequestionQanddesigntheInst.1:
Inst.1: [I] To answer the question: [Q],
(7)
where is the region of interest in the image?8 Liu et al.
To answer the Inst.1, we format the answer of the region in a bounding box
manner based on the pre-processed image I and I , where we adopt four
C|Q
coordinatestorepresenttheleft,rightboundaryofthewidthw ,w ,theupper,
0 1
bottom boundary of the height h ,h , respectively. Note that we normalize the
0 1
coordinatesinto[0,1]scaleandkeepthreedecimalplacesforconsistenttraining.
WeslightlyextendtherawregionofI toincludemoreout-paintingknowledge
C|Q
of the region of interest. The bounding box is treated directly as a string for
the word embedding for the sake of simplicity. The answer to Inst.1 can be
formatted as:
Ans.1:str([w ,w ,h ,h ]) (8)
0 1 0 1
Thesecondstepistoanswerthequestionafterconsideringtheregionofinterest.
GiventhepartofinputimageconditiononthequestionI ,Weadoptthesame
C|Q
pre-process procedure on I as image I and treat the two images equally in
C|Q
large VLMs, which we zoom in the critical region of the image I to the
C|Q
same resolution as the original image I and extract image representations using
image encoder respectively. In Inst.2, we append the I and call back the
C|Q
raw question to emphasize the question to solve. The format of Inst.2 is as
follows:
Inst.2: The region of interest in the image is [I ].
C|Q
(9)
Answer the question: [Q].
The answer to Inst.2 is the original response. During the training phase, we
concatenate Inst.1, Inst.2 and the relevant answers for the forward process
and adopt Cross-Entropy loss on answers. During the inference phase, we first
feed-forward Inst.1 into the vision-language model and let the model generate
predictions on the bounding box of the region of interest. Then, we crop the
imageaccordinglyandformulateInst.2forthevision-languagemodeltoadopt
a normal response.
4 Experiments
In this section, we conduct extensive experiments to illustrate the improvement
of the proposed Chain-of-Spot procedure on widely used multimodal vision-
languagemodels.ByadoptingChain-of-Spotonstate-of-the-artvision-language
model LLaVA-1.5 [22], we compare the improvements on a wide range of visual
question answering and multimodal benchmarks and compare our models with
existing large LVLMs. Subsequently, we perform an in-depth analysis of the
design of the Chain-of-Spot. Additionally, we display quantitative results on the
practical behavior of Chain-of-Spot and the comparisons with baselines.
4.1 Experimental Settings
Datasets and Baselines. In our study, we employ a comprehensive suite of
11 multimodal datasets, each serving as a critical component in evaluating theChain-of-Spot 9
performance of our proposed methods. These datasets are bifurcated into two
distinct categories: visual question answering (VQA) and multimodal bench-
marks.
Forvisualquestionanswering,weutilizesixdatasets:VQA-v2[9],GQA[12],
VizWiz [10], Science QA [25], TextVQA [36], and OKVQA [26]. VQA-v2 [9] is a
popular dataset that contains over 265K images from COCO [21] and abstract
scenes with multiple questions. GQA [12] offers a structured understanding of
visual reasoning challenges with over 22M question-answer pairs grounded in
113,000images.VizWiz[10]isuniqueforitsreal-worldimagestakenbyvisually
impaired users, accompanied by nearly 31,000 question-answer pairs. Science
QA [25] provides a specialized domain of scientific education with diagrams and
associated questions to test knowledge-based VQA. TextVQA [36] emphasizes
the understanding of the text in images with about 28,000 images and 45,000
question-answerpairs.OKVQA[26]extendstherealmofVQAtogeneralknowl-
edgewith14Kimagesthatrequireexternalknowledgebeyondthevisualcontent.
We integrate a suite of five diverse datasets to establish a comprehensive
multimodalbenchmark:SEEDBench[15],MME[8],MMBench[24],POPE[18],
andMM-Vet[44].Specifically,SEEDBench[15]offersarichmultimodalanalysis
dataset consisting of synchronized multimodal sequences extracted from online
video platforms. MME [8] extends the benchmarking landscape with a broad
arrayof14sub-tasksdesignedtoevaluatemultimodallearningcomprehensively.
MMBench [24] focuses on assessing multimodal machine learning models, facili-
tating comparisons across a spectrum of tasks and data modalities. POPE [18]
presentsachallengingdatasetaimedatprobingthehallucinationphenomenain
Language-Vision Language Models (LVLMs). Lastly, MM-Vet [44] is a platform
for evaluating generative capabilities, with performance metrics benchmarked
against the state-of-the-art GPT-4 model [28].
To establish a strong benchmark for our experimental analysis, we adopt
the state-of-the-art LLaVA-1.5 [22] method as our primary baseline. We adopt
our methodology to two variants of the LLaVA architecture, LLaVA/7B and
LLaVA/13B, and aim to demonstrate the scalability and generalizability of our
approach across different model sizes.
Implementation Details. We delineate the specifics of our implementation.
We augmented the supervised fine-tuning dataset from LLaVA-1.5 [22], amass-
ing a total of 665K images. It is important to note that multiple questions
may be associated with a single image, culminating in approximately 3.3 mil-
lion question-answer pairs. For the Chain-of-Spot fine-tuning phase, we select
a random question-answer pair for each image to facilitate learning, ensuring
the number of training steps per epoch remains consistent. Our starting point
was the publicly accessible LLaVA-1.5/7B and LLaVA-1.5/13B models, which
we fine-tuned using the instructions, image regions of interest (ROIs), and orig-
inal images for one epoch. To maintain a baseline for comparison, we preserved
the fine-tuning methodology of LLaVA-1.5 intact. The learning rate was set at
2e-5, with a global batch size of 1024. The maximum token length for the large
language models was established at 2048 tokens. Our experimental setup in-10 Liu et al.
Table 1: Comparisons with vision-language models on visual question an-
sweringdatasets.OurChain-of-Spot(CoS)consistentlyimprovesthevanillaLLaVA-
1.5 [22] in all the benchmarks under different language model sizes. The best results
are highlighted bold and the second are highlighted underline.
Method Language VQAV2 GQA VizWiz SQAI VQAT OKVQA
BLIP-2[16] Vicuna-13B 65.0 32.3 19.6 61.0 42.5 45.9
InstructBLIP[7] Vicuna-13B - 49.5 33.4 63.1 50.7 -
Shikra[5] Vicuna-13B 77.4 - - - - 47.2
IDEFICS-80B[13] LLaMA-65B 60.0 45.2 36.0 - 30.9 -
Qwen-VL[1] Qwen-7B 79.5 59.3 35.2 67.1 63.8 58.6
Qwen-VL-Chat[1] Qwen-7B 78.2 57.5 38.9 68.2 61.5 56.6
mPLUG-Owl2[43] LLaMA-7B 79.4 56.1 54.5 68.7 58.2 57.7
Monkey[19] Qwen-7B 80.3 60.7 61.2 69.4 - 61.3
LLaVA-1.5[22] Vicuna-7B 78.5 62.0 50.0 66.8 58.2 57.9
LLaVA-1.5+CoS Vicuna-7B 80.7 63.7 50.8 68.2 60.9 58.4
LLaVA-1.5[22] Vicuna-13B 80.0 63.3 53.6 71.6 61.3 60.9
LLaVA-1.5+CoS Vicuna-13B 81.8 64.8 58.0 71.9 62.4 62.9
cluded 8 NVIDIA A100 GPUs, utilizing the DeepSpeed Zero3 [33] optimization
to minimize memory overhead. Regarding image preprocessing, we first padded
the original images and their ROIs to a square format, followed by a resize to
a resolution of 336Ã—336 pixels. This preprocessing was necessary for feature
extractionbytheCLIP-ViT-L[32]visualbackbone.Throughoutthefine-tuning
process, we held the weights of the visual encoder and the multimodal projector
constant while all language model parameters were trained in accordance with
the established LLaVA-1.5 fine-tuning protocol.
4.2 Main Results
We perform our main experiments on 11 widely used and challenging multi-
modal benchmarks. We clearly show the performance compared with our base-
line LLaVA-1.5 and the comparisons with other vision-language models to show
the superiority of our method.
Results on Visual Question Answering Datasets. We rigorously evalu-
ate the effectiveness of our Chain-of-Spot approach through extensive exper-
iments on six challenging datasets that are widely recognized in the visual
question-answering research community: VQA-v2 [9], GQA [12], VizWiz [10],
ScienceQA [25], TextVQA [36], and OKVQA [26]. Results are shown in Tab. 1.
UponintegratingourChain-of-SpotmethodwiththeLLaVA/13Bmodel,weob-
servedaremarkableenhancementinperformance,surpassingallbaselinemetrics
on the aforementioned datasets, setting new state-of-the-art results on four of
them.ThisimprovementwasmostpronouncedintheVizWizdataset,wherewe
achieved a 4.4% increase. On the more general VQA-v2 and GQA datasets, we
saw increases of 1.8% and 1.5%, respectively. The performance on ScienceQA,
TextVQA, and OKVQA datasets, with improvements of 0.3%, 1.1%, and 2.0%,
respectively, further demonstrates the versatility of our approach. Similarly, theChain-of-Spot 11
Table 2: Comparisons with vision-language models on multimodal bench-
marks. LLaVA-1.5 [22] with Chain-of-Spot (CoS) achieves state-of-the-art perfor-
manceonallthemultimodalbenchmarks,surpassingpreviousLVLMsbyalargemar-
gin. The best results are highlighted bold and the second are highlighted underline.
Method Language SEED SEEDI MME MMB POPE MM-Vet
BLIP-2[16] Vicuna-13B 46.4 - 1293.8 - 85.3 22.4
InstructBLIP[7] Vicuna-13B 53.4 58.8 1212.8 36.0 78.9 25.6
Qwen-VL[1] Qwen-7B 56.3 62.3 - 38.2 - -
Qwen-VL-Chat[1] Qwen-7B 58.2 65.4 1487.5 60.6 - -
mPLUG-Owl2[43] LLaMA-7B 61.6 - 1450.2 64.5 - 36.2
LLaVA-1.5[22] Vicuna-7B 58.6 66.1 1510.7 64.3 85.9 30.5
LLaVA-1.5+CoS Vicuna-7B 59.7 67.1 1501.1 64.4 86.4 30.8
LLaVA-1.5[22] Vicuna-13B 61.6 68.2 1531.3 67.7 85.9 35.4
LLaVA-1.5+CoS Vicuna-13B 62.3 69.6 1546.1 68.2 86.1 37.6
LLaVA/7B model, enhanced with Chain-of-Spot, consistently outperformed the
baselineacrossallsixbenchmarkswithgainsof2.2%onVQA-v2,1.7%onGQA,
0.8%onVizWiz,1.4%onScienceQA,2.7%onTextVQA,and0.5%onOKVQA.
Notably, the fine-tuned LLaVA/7B model using our Chain-of-Spot approach
achieved superior performance to the baselines of the LLaVA/13B model on the
VQA-v2 and GQA datasets, which exemplifies the potential of our method to
achievestate-of-the-artperformanceevenwhencomputationalresourcesarelim-
ited. This robust performance underscores the efficacy of our proposed method
and highlights its potential to enhance visual question-answering capabilities
significantly.
Results on Multimodal Benchmarks. We evaluated our innovative Chain-
of-Spot method across five multimodal benchmarks specifically designed to test
thelimitsofmultimodalunderstandingandreasoning.Thebenchmarksincluded
SEEDBench [15], MME [8], MMBench [24], POPE [18], and MM-Vet [44], each
presenting its own challenges and requiring a nuanced understanding of multi-
modal inputs. We reported the overall accuracy and dissected the performance
on image-only data to capture the full scope of our methodâ€™s capabilities on
SEEDBench. Results are shown in Tab. 2. When implemented on the LLaVA-
1.5/13B models, our Chain-of-Spot method consistently outperformed the es-
tablishedbaselinesacrossallsixbenchmarks,demonstratingtherobustnessand
versatility of our approach. On SEEDBench, we observed a solid gain of 0.7%,
while the image-only subset of SEEDBench saw a more significant improvement
of1.4%.TheMMBenchandMM-Vetbenchmarksalsoshowednotableimprove-
ments of 0.5% and 2.2%, respectively. When we adapted Chain-of-Spot to the
smaller LLaVA-1.5 model with a 7B language model as its backbone, we still
observed similar gains across the five benchmarks, which is a testament to our
methodâ€™s scalability and effectiveness across different model sizes. Most impres-
sively, with Chain-of-Spot, LLaVA-1.5 models achieved state-of-the-art results
onallbenchmarks,firmlyestablishingourproposedmethodasasignificantstep
forward in multimodal learning.12 Liu et al.
Table3:AnalysisandAblations.Weconductablationsonthechoicesofreasoning
techniques and training strategies to demonstrate the effectiveness of Chain-of-Spot
as well as provide more insights into our method. w/o Chain-of-Spot indicates not
implementingchain-of-spotmethodwhileonlycontinuouslyfine-tuningLLaVA-1.5for
one epoch.
(a) ChoicesofReasoningTechniques (b) AblationsonTrainingStrategies
ReasoningTechniques GQA VQAV2 TrainingStrategies GQA VQAV2
w/oChain-of-Spot 63.9 80.2 LearningRate=1.0Ã— 64.8 81.8
CenterCrop 64.0 80.4
LearningRate=0.1Ã— 64.1 81.2
FromImage 64.1 80.6 Epochs=1 64.8 81.8
FromQuestions 64.3 81.1 Epochs=2 64.9 81.2
FromSingleQuestion 64.8 81.8 Epochs=3 64.4 80.9
4.3 Analysis
Intheanalysispart,weconductconvincingexperimentstodemonstratethekey
design of the idea of Chain-of-Spot. Additionally, we perform abundant visual-
izations to serve as a supplement to our impressive quantitative improvement.
Choices of Reasoning Techniques. In the method section of our paper, we
introduce a novel approach to generating regions of interest (ROI) in images by
leveraging one-to-one correspondence between question-answer pairs. We con-
ducted a series of analytical experiments in Tab. 3a to validate the critical
importance of this design choice. Initially, we trained our LLaVA model con-
tinuously for one epoch without the Chain-of-Spot component to establish a
baseline. Subsequently, we explored various techniques for ROI determination,
including (1) a naive method that crops and zooms into the image center, (2)
an uninformed prediction model that identifies ROIs directly from the image
without language cues, (3) a broad approach using all questions in the LLaVA
datasettoelicitageneralizedresponse,and(4)atargetedpredictionbasedonin-
dividualquestions.Thecomparativeresultsclearlydemonstratethatourmethod
outperforms these alternatives, underscoring the efficacy of our question-answer
pair-driven ROI generation technique in achieving superior performance. Note
that all the alternatives of Chain-of-Spot outperform the baselines that further
train one epoch without Chain-of-Spot, which shows the idea of instructing the
model to reason works effectively.
Ablations on Training Strategies. In the process of fine-tuning the LLaVA
model,weconductedanexplorationofhyper-parameterconfigurationsinTab.3b.
Notably, a key parameter adjustment involved experimenting with a reduced
learningrate.However,experimentalevidencesuggestedthatretainingtheorig-
inal learning rate was more beneficial, potentially attributable to the necessity
for stronger learning gradients during the critical task of identifying regions of
interest(ROI)withinthelearningprocedure.Furthermore,thenumberoftrain-
ing epochs was scrutinized, revealing that prolonging the training duration did
not correspond to an enhancement in model performance. This plateau in per-
formancegainsbeyondacertainnumberofepochsislikelyaconsequenceoftheChain-of-Spot 13
Question: What type of tool is she using
Question: What sport is this?
for her hair?
Question: What are these people doing? Question: Describethewallpaperonthe
laptop.
Fig.3: Visualizations on Chain-of-Spot. Chain-of-Spot shows the reasonable re-
gion of interest condition on the given questions.
LLaVA datasetâ€™s limited size, which introduces a heightened risk of overfitting
when subjected to excessive training.
Visualizations on Chain-of-Spot. We employ visualizations in Fig. 3 to elu-
cidate the effectiveness of the Chain-of-Spot approach in identifying regions of
interest pertinent to query responses within images. For instance, in the visu-
alization of the top-left image, when prompted with the question â€™What sport
is this?â€™, the model adeptly narrows in on the individuals engaged in skiing,
thereby isolating the key area that confirms the sport in question. Similarly, in
the bottom-left image, under the question â€™What are these people doing?â€™, the
modeldoesnotdirectlyidentifythepersonsintheimagewhilehonesinonadish
situated on a table, which in turn unambiguously addresses the posed question.
Movingtothetop-rightimage,themodelâ€™sfocusshiftstoacombheldbyagirl,
which stands as the definitive answer to the associated question. In the case of
the bottom-right image, the model demonstrates its perceptual acuity by rec-
ognizing the wallpaper pattern and, precisely, identifies a laptop as the answer
to the question at hand in a messy room with complex disturbance. These vi-
sualizations collectively showcase the modelâ€™s remarkable capacity to selectively
crop and spotlight the most salient region within an image that directly per-
tains to the answer, thereby affirming its robust capability for reasoning and
understanding within visual contexts.
Qualitative Comparisons of Chain-of-Spot. We present qualitative com-
parisons in Fig. 4 to demonstrate the enhancements brought about by imple-
menting Chain-of-Spot within the LLaVA-1.5 framework. Specifically, we detail
twoillustrativeexamplesthatunderscoretheadvancementofthereasoningpro-
cess. For the initial example, we tasked the model with identifying a person14 Liu et al.
Q:What is the people sitting under parasol doing?
Describe in detail.
LLaVA-1.5:Thepeoplesittingundertheparasolareengagedina
gameofchess.Theyareplayingonachessboard,whichisplaced
ontheground.Thechessboardissurroundedbyseveralchess
pieces,indicatingthattheyareactivelyparticipatinginthegame.
LLaVA-1.5+CoS:Thepeoplesittingundertheparasolarelikely
enjoyingamealoradrink,astheyaregatheredaroundtables
with chairs. They are socializing, relaxing, and taking in the
atmosphereoftheoutdoorshoppingcenter.
Q:What is the man with brown beard doing?
Describe in detail.
LLaVA-1.5:Themanwiththebrownbeardisstandingina
crowdofpeople,holdingacellphone,andtakingapicture.
Heissmilingandappearstobeenjoyingthemoment.
LLaVA-1.5+CoS:Themanwithabrownbeardisstanding
in the crowd, laughing and enjoying the moment. He is
partofthegroupofpeoplegatheredtogether,sharingthe
experienceandhavingagoodtime.
Fig.4: Generation comparisons after implementing Chain-of-Spot. Chain-of-
SpotcorrectsthefocusandtheanswersoftheLLaVAmodeloncomplexvisualquestion
cases.
located in a distant position (beneath a parasol). Prior to the integration of
Chain-of-Spot,LLaVA-1.5incorrectlyfocusedonthenearbychildrenengagedin
a game of chess. However, with our Chain-of-Spot approach, the model is able
to discern the significance of the parasol, thereby accurately zooming in on the
more remote area and providing the correct response that the individuals are
involvedinactivitiessuchaseating,drinking,andconversing.Thesecondexam-
pleshowcasesacomplexscenariofeaturingmanyindividuals,wherewehighlight
a specific person distinguished by a brown beard. Without the aid of Chain-of-
Spot,LLaVA-1.5confoundsthisindividualwithsomeoneincloseproximityand
inaccurately reports that he is holding a cell phone and taking a photo. In con-
trast, by employing Chain-of-Spot, the model is adept at isolating the correct
man and accurately depicting his actions. These examples collectively illustrate
the significant strides our approach makes in refining the reasoning capabilities
of LLaVA-1.5, thereby enhancing its performance in complex visual recognition
tasks.
5 Conclusion, Limitations and Societal Impact
In this paper, we have presented an innovative approach Chain-of-Spot to im-
prove large visual-language modelsâ€™ reasoning and understanding ability effec-
tively. By equipping LVLMs with the ability to identify and focus on these keyChain-of-Spot 15
regionsofinterest(ROI),weprovideapathwaytomoredetailedvisualinforma-
tion without compromising image resolution. We have reimagined the training
and inference procedures within the established instruct-tuning pipelines, in-
troducing an interactive question-answering framework that compels LVLMs to
integrate global image and localized ROI features. Our empirical results under-
score the effectiveness of our approach, demonstrating marked improvements in
LVLM performance across various vision-language benchmarks.
One limitation of our approach may come from the insufficient amount of
training data for fine-tuning, which may cause inadequate guidance for the
LVLMstofindtheROIintheimage.TheenhancementofLVLMscanprofoundly
impact society by improving assistive technologies and smart automation, yet it
also necessitates careful consideration of ethical implications related to privacy
and the increased potential for sophisticated surveillance systems.
A More Visualizations
Inthissupplementarysection,wefollowthevisualizationframeworkoutlinedin
the main manuscript to present a broader array of qualitative results under a
variety of scenarios, thereby providing a more comprehensive understanding of
our modelâ€™s performance. By conducting an in-depth analysis of the regions of
interest (ROIs) across diverse situations, we demonstrate the reasoning capabil-
ities of our approach. Additionally, we perform more comparisons against our
LLaVA [22] baselines under visual question-answering datasets.
A.1 More Results of Chain-of-Spot
We perform abundant visualization results from GQA [12] datasets to clearly
illustrate the relationship between the question or instruction and the region of
interest identified by Chain-of-Spot. Results are shown in Fig. 5. Chain-of-Spot
canidentifythecriticalobject(thecalculatortotherightofthecharger)among
complexobjectsinthesubfigure(row4th,column3rd),concentrateonthesmall
objects (the license plate) in the whole image in the subfigure (row 4th, column
1st), point out the correct target (the person next to the sign) under similar
disturbance in the subfigure (row 1st, column 1st).
A.2 More Comparisons with Baselines
In this supplementary section of our study, we extend our analysis through a
comprehensive set of comparisons against baselines, where we employ our com-
parisons on the GQA dataset [12]. Results are illustrated in Fig. 6. We show
the generated response both before and after employing our Chain-of-Spot fine-
tuning.Byvisuallyannotatingtheregionsofinterest(ROI)identifiedbyChain-
of-Spot on the images, we provide a clear and intuitive understanding of how
the modelâ€™s focus is refined.16 Liu et al.
What is the person next to the Which color is the horse, dark Does the umbrella have the same
sign eating? brown or tan? color as the bag?
What is sitting on the table made Which kind of vehicle is pictured? Are there any bottles or cups that
of wood? are made of plastic?
What color is the knife block on
Is the bat red or gray? What piece of clothing is blue?
the counter?
Are there any glasses or tennis What material is the calculator to
Which color is the license plate? balls? the right of the charger made of?
Fig.5:MoreResultsofChain-of-Spot.Weillustratethequestionandtherelevant
region of interest in the image marked by Chain-of-Spot.Chain-of-Spot 17
Are both the phone and the coffee cup Are the cabinets below the stove Which kind of furniture is blue?
the same color? wooden and open?
LLaVA-1.5:No ðŸ¤¯ LLaVA-1.5:Yes ðŸ¤¯ LLaVA-1.5:Table ðŸ¤¯
LLaVA-1.5 + CoS:Yes ðŸ¤— LLaVA-1.5 + CoS:No ðŸ¤— LLaVA-1.5 + CoS:Chair ðŸ¤—
The monitor to the right of the other
Who is wearing a watch? What color is the small bag?
monitor has which color?
LLaVA-1.5:Man ðŸ¤¯ LLaVA-1.5:Black ðŸ¤¯ LLaVA-1.5:Red ðŸ¤¯
LLaVA-1.5 + CoS:Woman ðŸ¤— LLaVA-1.5 + CoS:Green ðŸ¤— LLaVA-1.5 + CoS:Black ðŸ¤—
What are the jars sitting on top of? What is the color of the flower pot? Who is sitting on top of the steps?
LLaVA-1.5:Coffee maker ðŸ¤¯ LLaVA-1.5:Green ðŸ¤¯ LLaVA-1.5:Man ðŸ¤¯
LLaVA-1.5 + CoS:Stove ðŸ¤— LLaVA-1.5 + CoS:Blue ðŸ¤— LLaVA-1.5 + CoS:Woman ðŸ¤—
Fig.6:MoreComparisonswithBaselines.ResultsbeforeandafterChain-of-Spot
are illustrated as LLaVA-1.5 and LLaVA-1.5+CoS, respectively. The correct answers
are colored green and the wrong answers are colored red. We mark out the region of
interest identified by Chain-of-Spot.18 Liu et al.
80
1.2 5.8 7.0 7.4 7.1 6.0 1.3
70
16.8 32.3 39.8 42.1 40.2 32.3 15.7
60
20.4 42.1 54.9 60.6 56.0 43.0 19.8
50
21.8 46.0 62.0 68.8 63.7 47.7 21.5 40
21.2 44.5 59.1 65.8 60.6 46.2 21.1 30
20
17.5 34.8 44.6 48.1 45.2 35.4 16.8
10
1.3 5.6 7.4 8.0 7.6 6.0 1.4
0
Fig.7: Statistical Analysis of Chain-of-Spot. We show the statistical results of
the ROI in all the question-answer pairs of LLaVA [22] dataset. We can observe that
the center of the image contributes most to the information.
Notably, we observe a marked enhancement in the performance of the un-
derlying Large Vision-language models post Chain-of-Spot intervention. This
improvement is attributed to the moduleâ€™s ability to excise extraneous visual
information and concentrate on the salient image regions pertinent to the posed
question.TheresultantcroppingoftheROIeffectivelymitigatesthedistraction
causedbyirrelevantimagecontent,therebysharpeningthemodelâ€™sattentionand
significantly elevating the accuracy of the responses. For example, in the subfig-
ure (row 2nd, column 3rd), we can observe that LLaVA baseline mistakes the
wallpaperinthemonitorasthecolorofthemonitor,however,zoomingintothe
monitorcaneffectivelyidentifythecorrectcolorofthemonitor.Inthesubfigure
(row 2nd, column 1st), the hands of the two persons in the image have complex
relationships, resulting in mistakes in the LLaVA baseline. Chain-of-Spot can
focus on the hand region in the image, which offers more clear information to
the language model.
B Statistical Analysis of Chain-of-Spot
We are interested in examining the statistical outcomes associated with the dis-
tribution of the region of interest (ROI) within various datasets. To this end,
we present an analysis of the reasoning outcomes derived from all question-
answering pairs under consideration. Results are shown in Fig. 7. Our findings
indicate that the image center plays a predominant role in influencing perfor-
mance, with the critical region encompassing roughly one-quarter of the entire
image area.
ROI
probability
(%)Chain-of-Spot 19
References
1. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C.,
Zhou, J.: Qwen-vl: A frontier large vision-language model with versatile abilities.
ArXiv:2308.12966 (2023) 2, 4, 10, 11
2. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-
shotlearners.Advancesinneuralinformationprocessingsystemspp.33,1877â€“1901
(2020) 3
3. Cai,Z.,Vasconcelos,N.:Cascader-cnn:Delvingintohighqualityobjectdetection.
In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
pp. 6154â€“6162 (2018) 4
4. Chefer,H.,Gur,S.,Wolf,L.:Genericattention-modelexplainabilityforinterpret-
ingbi-modalandencoder-decodertransformers.In:ProceedingsoftheIEEE/CVF
International Conference on Computer Vision. pp. 397â€“406 (2021) 7
5. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleash-
ing multimodal llmâ€™s referential dialogue magic. arXiv preprint arXiv:2306.15195
(2023) 3, 10
6. Chiang,W.L.,Li,Z.,Lin,Z.,Sheng,Y.,Wu,Z.,Zhang,H.,Zheng,L.,Zhuang,S.,
Zhuang,Y.,Gonzalez,J.E.,Stoica,I.,Xing,E.P.:Vicuna:Anopen-sourcechatbot
impressing gpt-4 with 90%* chatgpt quality. https://lmsys.org/blog/2023-03-
30-vicuna/ (March 2023) 2
7. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B.A., Fung, P.,
Hoi, S.C.H.: Instructblip: Towards general-purpose vision-language models with
instruction tuning. ArXiv:2305.06500 (2023) 3, 10, 11
8. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang,
J.,Zheng,X.,etal.:Mme:Acomprehensiveevaluationbenchmarkformultimodal
large language models. arXiv preprint arXiv:2306.13394 (2023) 9, 11
9. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in
VQA matter: Elevating the role of image understanding in Visual Question An-
swering. In: Conference on Computer Vision and Pattern Recognition (CVPR)
(2017) 3, 9, 10
10. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham,
J.P.: Vizwiz grand challenge: Answering visual questions from blind people. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 3608â€“3617 (2018) 3, 9, 10
11. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L.,
Mohammed,O.K.,Liu,Q.,etal.:Languageisnotallyouneed:Aligningperception
with language models. arXiv preprint arXiv:2302.14045 (2023) 3
12. Hudson,D.A.,Manning,C.D.:Gqa:Anewdatasetforreal-worldvisualreasoning
and compositional question answering. In: Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition. pp. 6700â€“6709 (2019) 3, 9, 10,
15
13. HugoLaurencon, van Strien, D., Bekman, S., Tronchoon, L., Saulnier, L.: Intro-
ducing idefics: An open reproduction of state-of-the-art visual language model.
https://huggingface.co/blog/idefics (2023) 10
14. Kirillov,A.,Girshick,R.,He,K.,DollÃ¡r,P.:Panopticfeaturepyramidnetworks.In:
Proceedings of the IEEE/CVF conference on computer vision and pattern recog-
nition. pp. 6399â€“6408 (2019) 420 Liu et al.
15. Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking
multimodal llms with generative comprehension. ArXiv:abs/2307.16125 (2023) 3,
9, 11
16. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023) 3, 10, 11
17. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In: International Con-
ference on Machine Learning. pp. 12888â€“12900. PMLR (2022) 3
18. Li,Y.,Du,Y.,Zhou,K.,Wang,J.,Zhao,W.X.,Wen,J.R.:Evaluatingobjecthal-
lucinationinlargevision-languagemodels.arXivpreprintarXiv:2305.10355(2023)
9, 11
19. Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., Bai, X.:
Monkey:Imageresolutionandtextlabelareimportantthingsforlargemulti-modal
models. arXiv preprint arXiv:2311.06607 (2023) 2, 4, 6, 10
20. Lin, T.Y., DollÃ¡r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramidnetworksforobjectdetection.In:ProceedingsoftheIEEEconferenceon
computer vision and pattern recognition. pp. 2117â€“2125 (2017) 4
21. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., DollÃ¡r, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Visionâ€“
ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,
2014, Proceedings, Part V 13. pp. 740â€“755. Springer (2014) 9
22. Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning.
ArXiv:2310.03744 (2023) 2, 3, 4, 6, 8, 9, 10, 11, 15, 18
23. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint
arXiv:2304.08485 (2023) 2, 3, 4
24. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J.,
He,C.,Liu,Z.,etal.:Mmbench:Isyourmulti-modalmodelanall-aroundplayer?
arXiv preprint arXiv:2307.06281 (2023) 9, 11
25. Lu,P.,Mishra,S.,Xia,T.,Qiu,L.,Chang,K.W.,Zhu,S.C.,Tafjord,O.,Clark,P.,
Kalyan,A.:Learntoexplain:Multimodalreasoningviathoughtchainsforscience
question answering. In: The 36th Conference on Neural Information Processing
Systems (NeurIPS) (2022) 9, 10
26. Marino,K.,Rastegari,M.,Farhadi,A.,Mottaghi,R.:Ok-vqa:Avisualquestionan-
sweringbenchmarkrequiringexternalknowledge.In:ProceedingsoftheIEEE/cvf
conference on computer vision and pattern recognition. pp. 3195â€“3204 (2019) 9,
10
27. OpenAI: Chatgpt. https://chat.openai.com (2023) 3
28. OpenAI: Gpt-4 technical report. ArXiv:abs/2303.08774 (2023) 9
29. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,Mishkin,P.,Zhang,C.,
Agarwal,S.,Slama,K.,Ray,A.,etal.:Traininglanguagemodelstofollowinstruc-
tions with human feedback. Advances in Neural Information Processing Systems
35, 27730â€“27744 (2022) 3
30. Peng, B., Li, C., He, P., Galley, M., Gao, J.: Instruction tuning with gpt-4. arXiv
preprint arXiv:2304.03277 (2023) 3
31. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-
2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824 (2023) 3
32. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models fromChain-of-Spot 21
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748â€“8763. PMLR (2021) 10
33. Rasley,J.,Rajbhandari,S.,Ruwase,O.,He,Y.:Deepspeed:Systemoptimizations
enabletrainingdeeplearningmodelswithover100billionparameters.In:Proceed-
ingsofthe26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery
& Data Mining. pp. 3505â€“3506 (2020) 10
34. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object de-
tection with region proposal networks. Advances in neural information processing
systems 28 (2015) 6
35. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: Medical Image Computing and Computer-Assisted
Interventionâ€“MICCAI2015:18thInternationalConference,Munich,Germany,Oc-
tober 5-9, 2015, Proceedings, Part III 18. pp. 234â€“241. Springer (2015) 4
36. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh,
D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.8317â€“8326
(2019) 9, 10
37. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave,
E., Lample, G.: Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023) 3
38. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 2, 3
39. Wang,J.,Sun,K.,Cheng,T.,Jiang,B.,Deng,C.,Zhao,Y.,Liu,D.,Mu,Y.,Tan,
M.,Wang,X.,etal.:Deephigh-resolutionrepresentationlearningforvisualrecog-
nition. IEEE transactions on pattern analysis and machine intelligence (43(10)),
3349â€“3364 (2020) 4
40. Wang, Y., Chen, Z., Jiang, H., Song, S., Han, Y., Huang, G.: Adaptive focus
for efficient video recognition. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 16249â€“16258 (2021) 4
41. Wang, Y., Lv, K., Huang, R., Song, S., Yang, L., Huang, G.: Glance and focus: a
dynamicapproachtoreducingspatialredundancyinimageclassification.Advances
in Neural Information Processing Systems pp. 33, 2432â€“2444 (2020) 4
42. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,
D., et al.: Chain-of-thought prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems 35, 24824â€“24837 (2022) 2
43. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,
Shi, Y., et al.: mplug-owl: Modularization empowers large language models with
multimodality. arXiv preprint arXiv:2304.14178 (2023) 4, 10, 11
44. Yu,W.,Yang,Z.,Li,L.,Wang,J.,Lin,K.,Liu,Z.,Wang,X.,Wang,L.:Mm-vet:
Evaluating large multimodal models for integrated capabilities. ArXiv:2308.02490
(2023) 3, 9, 11
45. Zhu,M.,Han,K.,Wu,E.,Zhang,Q.,Nie,Y.,Lan,Z.,Wang,Y.:Dynamicresolu-
tion network. Advances in Neural Information Processing Systems pp. 34, 27319â€“
27330 (2021) 4
46. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable
transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159
(2020) 4