Wear-Any-Way: Manipulable Virtual Try-on via
Sparse Correspondence Alignment
Mengting Chen Xi Chen Zhonghua Zhai Chen Ju Xuewen Hong
Jinsong Lan Shuai Xiao⋆
Alibaba Group
{mengtingchen.cs@gmail.com shuai.xsh@alibaba-inc.com}
mengtingchen.github.io/wear-any-way-page
Abstract. This paper introduces a novel framework for virtual try-on,
termed Wear-Any-Way. Different from previous methods, Wear-Any-
Way is a customizable solution. Besides generating high-fidelity results,
ourmethodsupportsuserstopreciselymanipulatethewearingstyle.To
achievethisgoal,wefirstconstructastrongpipelineforstandardvirtual
try-on, supporting single/multiple garment try-on and model-to-model
settings in complicated scenarios. To make it manipulable, we propose
sparse correspondence alignment which involves point-based control to
guide the generation for specific locations. With this design, Wear-Any-
Way gets state-of-the-art performance for the standard setting and pro-
vides a novel interaction form for customizing the wearing style. For
instance, it supports users to drag the sleeve to make it rolled up, drag
the coat to make it open, and utilize clicks to control the style of tuck,
etc. Wear-Any-Way enables more liberated and flexible expressions of
the attires, holding profound implications in the fashion industry.
Keywords: VirtualTry-on·CustomizableGeneration·DiffusionModel
1 Introduction
Virtual try-on aims to synthesize an image of the specific human wearing the
providedgarments.Itemergesasasignificanttechnologywithintherealmofthe
fashion industry, providing consumers with an immersive and interactive means
of experiencing apparel without physically trying.
Classicalsolutionsforvirtualtry-onrelyonGenerative-Adversarial-Networks
(GANs) [10,22,28,33,34] to learn the feature warping from the garment image
to the person image. Recent advances in diffusion models [17] bring a huge
improvement in the generation quality for various image/video synthesis tasks,
including virtual try-on. Some recent works [25,63] could generate high-fidelity
resultsleveragingpre-trainedtext-to-imagediffusionmodels[48].However,there
still exist drawbacks for these solutions.
⋆ Corresponding author
4202
raM
91
]VC.sc[
1v56921.3042:viXra2 Chen et al.
(a) Single Garment Try-on (b) Outfit Try-on
(c)Manipulation with clicks
(d) Manipulation with drag
Fig.1: Manipulable try-on with Wear-Any-Way.Ourmethodachievesstate-of-
the-art performance for the standard setting of virtual try-on (first row), supporting
diversifiedinputformatsandscenarios.Animpressivefeatureisthatourmethodsup-
portsuserstomanipulatethewayofwearingusingsimpleinteractionslikeclick(second
row) and drag (third row). It should be noted that all these applications are accom-
plished with a single model in one pass.
First, most of them only support simple cases like well-arranged single gar-
ments with simple textures as input. For garments with complicated textures or
patterns, existing solutions often fail to maintain the fidelity of the fine details.
In addition, most of the previous solutions do not solve the challenges in real-
world applications like model-to-model try-on, multi-garment try-on, complex
human poses, and complicated scenarios, etc.
Second, previous methods are unable to exert control over the wearing style.
However, the actual wearing style holds significant importance in the realm of
fashion.Forinstance,variationsintherollingupordownofsleeves,thetucking
and layering of tops and bottoms, the decision to leave a jacket open or closed,
and even the exploration of different sizes for the same garment all contribute
to diverse ways of wearing. These distinct styles can showcase varied states
of the same piece of clothing, highlighting the pivotal role of styling options,
particularly in the context of fashion applications.Wear-Any-Way 3
In this work, we propose Wear-Any-Way, a novel framework for virtual try-
on that solves the two aforementioned challenges at once. Wear-Any-Way could
act as a strong option for the standard setting of virtual try-on, it synthesizes
high-qualityimagesandpreservesthefinedetailsofthepatternsonthegarment.
Besides, it serves as a universal solution for real-world applications, supporting
various sub-tasks like model-to-model try-on, multi-garment try-on, and com-
plicated scenarios like street scenes. The most important feature is that Wear-
Any-Way supports users in customizing the wearing style. As shown in Fig. 1,
users could use simple interactions like click and drag to control the rolling of
sleeves, the open magnitude of the coat, and even the style of tuck.
To archive these functions, we first build a strong baseline for virtual try-on.
Recently,Reference-only[61]structurehasproveneffectiveinmanydownstream
tasks like image-to-video [20,57] and image editing [5,39,40]. Inspired by these
methods,webuildadual-branchpipeline,themainbranchisadenoisingU-Net
initialized from a pre-trained inpainting Stable Diffusion [48]. The main U-Net
takes the person image as input while the reference U-Net extracts the features
from the garment image. The referential garment features are then injected into
the main branch with self-attention. To further improve the flexibility and ro-
bustnessofthisstrongbaseline,wealsoinjecttheguidanceofhumanpose.This
pipeline achieves state-of-the-art performance in the standard try-on setting.
Afterward, we take a further step to make this strong baseline customizable.
Specifically, we investigate a point-based control that forces the specific points
on the garment image to match the target points on the person image in the
generation result. To align the features of the paired points, we propose sparse
correspondencealignment,whichfirstlearnsaseriesofpermutablepointembed-
dings and injects these embeddings into both the main and reference U-Net by
modifyingtheattentionlayers.Toassistthenetworklearnthefeaturealignment
better, we design several strategies like condition dropping, zero-initialization,
and point-weighted loss to ease the optimization.
Equipped with all these techniques, Wear-Any-Way demonstrates superior
quality and controllability for virtual try-on. In general, our contributions could
be summarized in three folds:
– We construct a novel framework, Wear-Any-Way, which generates high-
qualityresultsandsupportsuserstopreciselymanipulatethewayofwearing.
– We propose a strong, flexible, and robust baseline for virtual try-on, which
reaches state-of-the-art with extensive comparisons with previous methods.
– We design the sparse correspondence alignment to enable the point-based
control and further develop several strategies (i.e., conditional dropping,
zero-initialization, point-weighted loss) to enhance the controllability.
2 Related Work
GAN-based virtual try-on. To execute the Virtual try-on task, numerous
methods[10,22,28,33,34]haveutilizedGenerativeAdversarialNetworks(GANs).4 Chen et al.
EffectivelyapplyingGANstothevirtualtry-onchallengerequiresanuancedap-
proach, current methods [8,12,29,56] typically adopt a two-stage strategy: first
deforming the garment to fit the target body shape, then integrating the trans-
formed clothing onto the human model using a GAN-based try-on generator.
To achieve accurate deformation, several techniques estimate a dense flow map
that guides the clothing reshaping process [2,12,14,29,56]. Subsequent methods
then address misalignment issues between the warped clothing and the human
bodyusingstrategieslikenormalization[8]ordistillation[12,21].However,these
existing approaches have limitations, particularly when dealing with images of
individuals in complex poses or against intricate backgrounds, resulting in no-
ticeabledropsinperformance.Moreover,ConditionalGANs(cGANs)encounter
difficulties with significant spatialtransformations between the clothingand the
subject’s posture, as highlighted by CP-VTON [54], which brings to light the
need for improved methods capable of handling these challenges.
Diffusion-basedvirtualtry-on.Theexceptionalgenerativecapabilitiesofdif-
fusionhaveinspiredseveralapproachestoincorporatediffusionmodelsintofash-
ionsynthesis,coveringtaskslikevisualtry-on[3,6,23,37,63].TryOnDiffusion[63]
utilizes dual U-Nets for the try-on task. However, the requirement for extensive
datasets capturing various poses presents a significant challenge. Consequently,
therehasbeenapivottowardsleveraginglarge-scalepre-traineddiffusionmodels
as priors in the try-on process [18,46,49,58]. Approaches like LADI-VTON [37]
andDCI-VTON[13]havebeenintroduced,whichtreatclothingaspseudo-words
orusewarpingnetworkstointegrategarmentsintopre-traineddiffusionmodels.
StableVITON [25] proposes a novel approach that conditions the intermediate
featuremapsofaspatialencoderusingazerocross-attentionblock.Whilethese
methods have addressed the issues of background complexity, they struggle to
preserve fine details and encounter problems such as inaccurate warping. More-
over, they fail to enable flexible control over how garments are worn, producing
only a rigid, static image.
Building on this landscape, our work extend the interactive capabilities of
virtual try-on, which sets new standards for performance and user interaction.
Rather than just generating static images, Wear-Any-Way empowers users to
manipulate garments dynamically, allowing for an unprecedented level of cus-
tomizationthatsignifiesasignificantleapinthepersonalizationofdigitalfashion
experiences.
Point-based image editing. Building on the achievements of diffusion mod-
els [18], various diffusion-based image editing techniques [1,4,16,24,36] have
emerged, predominantly relying on textual instructions for editing. Techniques
such as those in [24,27,53] apply fine-tuning to models on single images to
produce alterations directed by descriptive texts. However, this text-guided ap-
proach often yields only broad-stroke modifications, lacking the precision re-
quired for detailed image editing.
To overcome the limitations of text-guided editing, studies explored point-
based editing [11,42,55]. DragGAN, notable for its intuitive drag-and-drop ma-
nipulation,optimizeslatentcodesforhandlepointsandincorporatespointtrack-Wear-Any-Way 5
ing. However, GANs’ inherent limitations constrain DragGAN. FreeDrag [32]
refines DragGAN by eliminating point tracking, while [51] extends DragGAN’s
framework to diffusion models, showcasing versatility. Simultaneously, [39] uti-
lizes diffusion models for drag-based editing, employing classifier guidance to
convert editing intentions into actionable gradients.
Toaddresstheseshortcomingsandenhancethegranularityandadaptability
of image editing, our research leverages diffusion models’ exceptional genera-
tive capabilities. We propose a novel editing paradigm, Wear-Any-Way, com-
bining point-based precision with diffusion models’ rich generative potential.
Wear-Any-Way enhances fine-grained, context-aware image alterations, offering
unprecedented control over the editing process and outcomes.
3 Method
We first introduce the basic knowledge required for diffusion models in Sec. 3.1.
Afterward,wepresentourstrongbaselineforvirtualtry-oninSec.3.2.Next,we
diveintothedetailsofourproposedsparsecorrespondencealignmentinSec.3.3
and the training strategies in Sec. 3.4. In addition, in Sec. 3.5 we also elaborate
on the details for collecting the training point pairs and finally summarize the
the inference pipeline in Sec. 3.6.
3.1 Preliminaries
Text-to-image diffusion model. Diffusion models [17] exhibit promising ca-
pabilitiesinbothimageandvideogeneration.Inthisstudy,weselectthewidely
adopted Stable Diffusion [48] as our foundational model, leveraging its efficient
denoising procedure in the latent space. The model initially employs a latent
encoder [26] to project an input image x into the latent space: z = E(x ).
0 0 0
Throughout training, Stable Diffusion transforms the latent representation into
Gaussian noise using the formula:
√ √
z = α¯z + 1−α¯ϵ, (1)
t t 0 t
where ϵ∼U([0,1]), and α¯ is the cumulative product of the noise coefficient
t
α at each step. Subsequently, the model learns to predict the added noise as:
t
E (|ϵ (z ,c,t)−ϵ|2). (2)
z,c,ϵ,t θ t 2
Here,trepresentsthediffusiontimestep,andcdenotestheconditioningtext
prompts. During the inference phase, Stable Diffusion effectively reconstructs
an image from Gaussian noise step by step, predicting the noise added at each
stage. The denoised results are then fed into a latent decoder to regenerate
colored images from the latent representations, denoted as xˆ =D(ˆz ).
0 06 Chen et al.
Garment Person Garment Person
Point Embed Point EmbedRef FeatureDenoising Feature
Reference U-Net Trainable Module
Freeze Module
c c
CLIP C Concatenate
Encoder Add (ZP erro oj e Inct io tir al) Projector
PointEmbed
Network
Garment
CLIP Feature
C C
Self-attention
Add MainU-Net Cross-attention
Pose
Encoder Alignment Feature
(a) Overview of Wear-Any-Way (b) SparseCorrespondenceAlignment(SCA)
Fig.2: The pipeline of Wear-Any-Way. The overall framework consists of two
U-Nets.ThereferenceU-Nettakesthegarmentimageasinputtoextractfine-grained
features. The main U-Net takes charge of generating the try-on results. It takes the
personimage(masked),thegarmentmask,andthelatentnoiseasinput.Weexertthe
pose control via an additional pose encoder. The point-based control is realized by a
pointembeddingnetworkandasparsecorrespondencealignmentmodule.Thedetailed
structuresaredemonstratedontherightpart.Symbolsofflamesandsnowflakesdenote
trainable and frozen parameters respectively.
3.2 Virtual Try-on Pipeline
As demonstrated in Fig. 2, our pipeline consists of two branches. The main
branch is the inpainting model initialized with the pre-trained weight of Stable
Diffusion [48]. It takes in a 9-channel tensor as input, with 4 channels of latent
noise, 4 channels of the latent for the inpainting background (i,e., person image
withthemaskedclothesregion),and1channelforthebinarymask(representing
for inpainting region). The original SD receives text embedding as conditions to
guide the diffusion procedure. Instead, we replace the text embedding with an
imageembeddingofthegarmentimageextractedbyaCLIP[47]imageencoder.
The CLIP image embedding could guarantee the overall colors and textures
ofthegarmentbutfailstopreservethefinedetails.Recently,reference-only[61]
has proven effective in keeping the fine details of the reference image in many
fields of applications [5,20,40,57]. TryonDiffusion [63] also leverages two U-Net
tomakegenerationswithhighfidelity.Inspiredbythoseexplorations,wealsouse
a reference U-Net to extract the detailed features of the garment. Our reference
U-Net is a standard text-to-image diffusion model with 4-channel input. We
conduct feature fusion after each block by concatenating the “key” and “value”
of the reference U-Net after the main U-Net.
To further enhance the generation, we add the pose map as an additional
control. We construct a tiny convolution network to extract the features of the
pose map, and directly add it to the latent noise of the main U-Net. The pose
map is extracted from the provided person image with DW-Pose [59].
ACS ACS ACS ACS ACS ACS ACSWear-Any-Way 7
3.3 Sparse Correspondence Alignment
TomakeourWear-Any-Waycustomizable,weintroduceasparsecorrespondence
alignment mechanism into the diffusion procedure. Specifically, for the point-
pair, one point is marked on the garment image and another is given on the
person image. We utilize the correspondence between the two points to control
the generation results: the marked position of the garment would match the
targeted position on the person image. In this way, users could precisely control
the wearing style by manipulating multiple point pairs.
As shown in Fig. 2, we first learn a series of point embedding to represent
the pair of control points. Afterward, we inject this control signal into both the
main U-Net and referential U-Net. To assist the model in learning the corre-
spondence relationships, we propose several strategies like condition dropping,
zero-initialization, and point-guided loss.
Point embedding. Assuming we sample N pairs of points, we use disk maps
D1×H×W torepresentthepointsonthegarmentandpersonimagesrespectively.
g/p
The background values are zeros. Points on the garment image are filled with
values from 1 to K randomly without repeat, while the points on the person
imagearefilledwiththecorrespondingvalues.Kdenotesthemaximumnumber
ofcontrolpointsenabledinoneimagewithN ≤K.Inthiswork,wesetK=24.
This random assignment decouples the semantics and the points thus making
the point representation permutable. It serves as the basis to support arbitrary
numbers of point control at arbitrary locations.
Afterward, we design a point embedding network with stacked convolution
layerstoprojectthediskmapsintohigh-dimensionembeddings:EC×H×W.This
g/p
network is optimized alone with the diffusion model in the end-to-end training.
Embedding injection. We excel in controls by injecting the point embedding
into the attention layer. In our baseline, the features from the reference U-Net
are concatenated on the “key” and “value” of the self-attention as illustrated in
Eq. (3), where the subscripts of m,r represents the main and reference U-Net.
Q ·cat(K ,K )T
Attention=softmax( m √ m r )·cat(V ,V ) (3)
m r
d
k
This attention layer enables garment features extracted by the reference U-
Net tobe integrated into themain U-Net. Toenable the correspondencecontrol
withpointguidance,wemodifythisattentionlayerviaaddingthepointembed-
ding of the person and garment with the “query” and “key” as in Eq. (4).
(Q +E )·cat(K +E ,K +E )T
Attention=softmax( m p √m p r g )·cat(V ,V ) (4)
m r
d
k
In this way, when integrating the garment feature into the main U-Net, the
feature aggregation would consider the correspondence of the point pairs. The
feature located by the point of the garment could be aligned to the position8 Chen et al.
DUeta-Nil-eNtet
QueryPoint
“Aphotoofagarment.” CosineSimilarity
DUet-aNil-eNtet
MatchedPoint
(a) Thepipelineofsinglepointmatching (b) Resultsfordensepointmatching
Fig.3: Pipeline of collecting the training point-pairs. As shown on the left,
the person and garment images are sent into the same Stable Diffusion to extract the
feature. We calculate the cosine similarity between the two feature maps to get the
point pairs. Some densely sampled point pairs are demonstrated on the right.
of the point on the person image. Thus, users could assign control points to
manipulate the wearing style by clicking and dragging.
3.4 Training strategies
Besidesthedesignofthemodelstructure,wealsodevelopseveraltrainingstrate-
gies to assist Wear-Any-Way to learn the correspondence alignment.
Condition dropping. We observe that, even without the guidance of point
pairs, the generation results on the training set are already very close to the
ground truth. We analyze that, the inpainting mask and the human pose map
couldindicatethewearingstyleofthetrainingsampletosomeextent.Toenforce
themodeltolearnfromthecontrolpoint,weincreasethepossibilityofdropping
the pose map and degrading the inpainting mask to a box around the ground
truth mask.
Zero-initialization. Adding the point embedding to the attention “key” and
“value” causes the unstability of the training optimization. To achieve a pro-
gressive integration, we get the inspiration of ControlNet [62] to add a zero-
initialized convolutional layer at the output of the point embedding network.
This zero-initialization brings better convergence.
Point-weighted loss. To enhance the controllability of the paired points. We
increase the loss weight around the sampled points on the person image. The
supervision of Wear-Any-Way is an MSE loss for predicting the noise.
3.5 Training Points Collection
In this section, we elaborate on the pipeline for collecting the training point
pairs. It is crucial to get precisely matched point pairs from the garment and
person image to train Wear-Any-Way. Considering that there are no densely
annotatedpointdatabetweenthegarmentandpersonimage,wemakeextensive
explorations to collect point pairs.Wear-Any-Way 9
GarmentImage GarmentPoint GarmentImage GarmentPoint
ImagewithDrag
PersonImage PersonPoint PersonImage PersonPoint
(a) Click-basedpipeline (b) Drag-basedpipeline
Fig.4: The inference pipeline of Wear-Any-Way. Forclick-basedcontrol,users
provide garment images, person images, and point pairs to customize the generation.
When the user drags the image, the starting and end points are translated as the
garment and person points. While the parsed clothes are regarded as the garment
image. Thus, the drag could be transformed into the click-based setting.
The challenge lies in the fact that garments are not rigid like steel. When
wornonthehumanbody,garmentscouldundergodeformation.Previousvirtual
matching/correspondencelearningmethods[31,44,50]couldonlydealwithrigid
objects like buildings. Fashion/human key points detection methods [35,59,64]
could localize point pairs. However, they could only detect a few predefined key
points, which fail to generate arbitrary sampled points for truly flexible control.
Recently, some works [15,52,60] find that the pre-trained diffusion models are
naturallyimagematchers.Inthiswork,wealsoexplorecorrespondencelearning
by leveraging pre-trained text-to-image diffusion models.
AsillustratedinFig.3,weleveragethesiamesetext-to-videodiffusionmodel
toextractfeaturesfromthepersonandgarmentimagerespectively.Wetakethe
featuremapofthelastlaterandensemblethepredictionresultsatmultipletime
stepstogetarobustmatching.Givenapointonthepersonimage,weselectthe
corresponding point on the garment image with the maximum cosine similarity.
Thedensepoint-matchingresultsaredemonstratedontherightofFig.3.Given
a person image, we first extract the mask for the wearing clothes. Afterward,
we randomly sample points in the internal and boundary regions of the mask as
queries and leverage the matching pipeline to extract the corresponding points
on the garment image. We chose the mapping direction from the peron image
to the garment image because some points on the garment image could not be
matched on the on-body image when the poses were complex.
3.6 Inference with manipulation
Equipped with the sparse correspondence alignment, Wear-Any-Way supports
userstocustomizethetry-onresultsusingcontrolpoints.Theinferencepipeline
is illustrated in Fig. 4. For the click-based setting, besides providing the gar-
ment image and person images, users could assign multiple point pairs on these
two images as control signals. The coordinates on the garment image indicated
by the garment points could be aligned to the corresponding position of the
person points in the generation result. For the drag-based control, the starting
yaW-ynA-raeW
gnisraP
yaW-ynA-raeW10 Chen et al.
and end points are processed as garment points and person points, while the
parsed clothes are regarded as the garment image. In this way, the drag-based
manipulation could be transformed into click-based controls.
4 Experiments
4.1 Implementation Details
Detailed configurations. In this work, the main U-Net and the reference U-
Netbothleveragethepre-trainedweightsfromStableDiffusion-1.5[48].Wecol-
lect 0.3 million high-quality try-on data with “person image, up-clothes, down-
clothes” triplets to train our model. However, for fair comparisons with other
works, we also train Wear-Any-Way on VITON-HD [9] and Dresscode [38] re-
spectively to report the quantitative results and make qualitative comparisons.
To equip our model with the ability to try the upper and down clothes in one
pass. We concatenate two input garment image together from H×W×3 to
H×2W×3. We randomly drop a garment image to an all-zeros image to pre-
serve the ability of single-garment generation.
Training hyper-parameters. During training, we set the initial learning rate
5e-5 with a batch size of 64. The models are trained using 8× A100 GPUs. For
the main U-Net, we train the parameters of the decoder, and the self-attention
layers of the encoder; All parameters of the reference U-Net are trained. For the
data augmentation, we conduct random crop on the person image, and exert
random flip simultaneously on the garment and person images. In addition, we
alsouserandomcolorjittertoimprovetherobustness.Wetrainourmodelwith
the resolution of 768×576 on the self-collected data for better visual quality.
We also train a 512×384 version for fair comparisons with previous works.
4.2 Evaluation protocols.
Standard virtual try-on. We first evaluate the performance of Wear-Any-
Wayonstandardvirtualtry-onbenchmarks(i.e.,VITON-HD[9]Dresscode[38])
to report qualitative results. Meanwhile, we give qualitative comparisons with
state-of-the-art methods to prove the effectiveness of our design.
Evaluation for point control. To evaluate the ability of point-based control,
wegetinspirationfrompreviousdrag-basedimageeditingmethods[39,43,51]to
calculate the landmark distance. Specifically, we detect the fashion landmarks
using FashionAI [64] detector on the pair of garment image and the person
image. Afterward, we use the paired landmarks L and L as the
garment person
control points to generate a try-on image (the person image could be viewed as
the ground truth). Next, we use the same detector to localize the landmarks on
the newly generated image, noted as L . We calculate the Euclidean distance
gen
between L and L to evaluate the control ability. Ideally, the landmark
person gen
distance should be small if the generation is well-controlled by the points. We
construct a benchmark covering the upper-, down-, and coat-clothes with 1000
samples in total for a comprehensive evaluation.Wear-Any-Way 11
CLIP Encoder DINOv2 Encoder ControlNet Ours
Fig.5: Ablation studies on different feature extractors. We compare CLIP
imageencoder[47],DINOv2[41],ControlNet[62],andourreferenceU-Net.Ourresults
demonstrate notable superiority.
4.3 Ablations Studies
In this section, we dive into the details to prove the effectiveness of our pipeline
design. We first conduct experiments for our strong baseline of standard virtual
try-on. Afterward, we provide a detailed analysis of the sparse correspondence
alignmentmodule.Besides,collectingthepairedpointstrainingisalsoanimpor-
tant step. Thus, we make comparisons with multiple correspondence matching
methods with qualitative results.
Strong baseline. We first investigate the design of our strong baseline for
virtualtry-on.WeclaimthatthereferenceU-Netiscrucialforpreservingthefine
detailsofgarments.PreviousworkslikeAnyDoor[7]usesaimageencoder(e.g.,
DINOv2 [41]) to extract the garment features. StableVITON [25] leverages a
ControlNet-like [62] structure to extract finer representations.
We organize the qualitative comparisons in Fig. 5. We leverage the CLIP
image encoder, DINOv2, and ControlNet as feature extractors and apply the
sametrainingsettingsforfaircomparison.WeobservethatCLIP,DINOv2,and
ControlNet could only encode the global appearances of the garments, but fail
to preserve the identity of the detailed patterns/texts/logos. In contrast, the
reference U-Net provides fine-grained details and is able to preserve the high-
fidelity details of the garments.
Sparse correspondence alignment. It is the core component of our point-
basedcontrol.Wefirstconductablationstudiesforthecontrolinjectionmethods
in Tab. 1. We follow the evaluation protocol introduced in Sec. 4.2 to calculate
thelandmarkdistance.Withoutthepointembedding,ourtry-onbaseline(row1)
gets a high landmark distance. In the second row, we first explore injecting the
pointembeddingattheinputnoiseofthemainandreferenceU-Net.Inthethird
row,wereporttheresultsofinjectingthecontrolsignalintheattentionlayeras
introduced in Sec. 3.3. In Tab. 2, we add the enhancement strategies presented
in Sec. 3.4 step-by-step to verify the effectiveness of our designs.12 Chen et al.
DCI-VTON LaDI-VTON KGI AnyDoor StableVITON Ours
Fig.6: Qualitative comparison for classical virtual try-on. Wemakecompar-
isons on VITON-HD [9] test split with DCI-VTON [13], LaDI-VTON [37], KGI [30],
AnyDoor[7],andStableVITON[25].Oursolutiondemonstratesnotablesuperiorityin
detail preservation and generation quality.
Table 1: Point embedding injection. Table 2: Enhancing strategies for
Wecomparedifferentwaysofinjectingthe the sparse correspondence alignments are
pointembeddingsandreportthelandmark added step by step from the baseline to
distance as the metric. verify the effectiveness.
Distupper Distdown Distcoat DistupperDistdownDistcoat
None 35.65 21.13 43.34 Base(Attentionk,q) 24.35 15.79 27.27
+Zero-init 22.65 15.33 25.56
LatentNoise 27.32 16.34 30.38
+Condition-dropping 18.39 12.04 20.44
Attentionq,k 24.35 15.79 27.27 +Point-weightedloss 17.65 10.32 20.32
Training point pair collection. As introducedin Sec.3.5, we collectthe con-
trolpointbyusingasiameseU-Netstructure.Inthissection,wemakeextensive
experiments by comparing different correspondence-matching methods. We also
utilize the benchmarks of fashion landmarks to evaluate the matching accuracy.
Concretely,givenapairoflandmarksonthepersonandgarmentimage,welever-
agedifferentmatchingmethodstomapthelandmarksfromthepersonimageto
thegarmentimage.Then,wecalculatethedistancebetweenthemappedresults
with the ground truth landmarks. The comparison results are listed in Tab. 3.
Thepre-traineddiffusionmodelsdemonstratedsuperiorabilitiescomparedwith
otherfeatureextractorslikeCLIP[47]andDINOv2[41].Wealsoincludeseveral
specificcorrespondence-matchingmethods,andourpre-trainedreferenceU-Net.
Among them, the diffusion matcher demonstrates the best performance.
4.4 Comparisons with Existing Alternatives
Inthissection,weconductintensivecomparisonswithexistingalternatives.Con-
sidering that no methods could accomplish the manipulable virtual try-on. WeWear-Any-Way 13
SourceImage DragDiffusion DragonDiffusion Ours SourceImage DragDiffusion DragonDiffusion Ours
Fig.7: Qualitativecomparisonwithdrag-basedimageeditingmethods. We
make comparisons with DragDiffusion [51] and DragonDiffusion [39]
Table 3: Ablation studies for training point-pairs collection. Wecomparethe
features of different vision foundation models and specific dense matching methods.
We compare the landmark distance for evaluation.
Dist Dist Dist
upper down coat
SuperGlue[50] 134.04 128.34 187.30
CLIP[47] 93.42 89.23 129.24
DINO[41] 83.24 70.08 103.54
ReferenceU-Net 59.34 35.23 79.98
StableDiffusion 43.44 29.94 59.45
first compete for previous arts in the standard setting of virtual try-on. After-
ward, we compare interactive image editing methods that support click/drag.
Standard virtual try-on. We report the qualitative results in Tab. 4 on
VITON-HD [9] and Dresscode [38] datasets. Our model gets the best result
for the FID and KID and competitive performance for the SSIM and LPIPS.
Considering that the quantitative results could not perfectly align with the
real generation quality. We make qualitative comparisons with previous state-
of-the-art solutions in Fig. 6. Wear-Any-Way demonstrates obvious advantages
over other works for the generation quality and detail preservation.
Controllable generation. We prove the controllability of our model by com-
paring it with drag-based image editing methods like DragDiffusion [51] and
DragonDiffusion [39]. We illustrate the comparison results in Fig. 7. We observe
that DragDiffusion [42] could not precisely follow the instructions of drag, while
DragonDiffusion [39] usually destroys the structure of humans and garments.
4.5 Qualitative Analysis
WeillustratemoreexamplesinFig.8,andgiveaqualitativeanalysisofthepoint-
based manipulation. It is demonstrated that Wear-Any-Way supports the ma-
nipulationforvarioustypesofgarmentsincludingcoats,T-shirts,pants,hoodies,
etc. Besides, users could assign arbitrary numbers of control points to get the
customized generation results. As shown in the first row, assisted by the precise
point control, Wear-Any-Way can conduct “continuous” editing for splitting the
coat gradually. The high controllability of Wear-Any-Way enables it to realize14 Chen et al.
Unsplit HalfSplit Split WideSplit WiderSplit
SleeveUp SleeveDown PantUp PantDown
Untuck TuckIn HalfTuck FrontTuck SideTuck
Fig.8: Manipulable virtual try-on with click. Wear-Any-Way supports users
to assign arbitrary numbers of control points on the garment and person image to
customize the generation, bringing diverse potentials for real-world applications.
Table 4: Quantitative comparisons with existing state-of-the-art try-on solutions on
VITON-HD and DressCode upper-body (D.C. Upper) datasets. Bold and underline
denote the best and the second best result, respectively.
Train/Test VITON-HD/VITON-HD D.C.Upper/D.C.Upper
Method SSIM LPIPS FID KID SSIM LPIPS FID KID
VITON-HD[19] 0.862 0.117 12.117 3.23 - - - -
HR-VITON[29] 0.878 0.1045 11.265 2.73 0.936 0.0652 13.820 2.71
LADI-VTON[37] 0.864 0.0964 9.480 1.99 0.915 0.0634 14.262 3.33
Paint-by-Example[58] 0.802 0.1428 11.939 3.85 0.897 0.0775 15.332 4.64
DCI-VTON[13] 0.880 0.0804 8.754 1.10 0.937 0.0421 11.920 1.89
GP-VTON[56] 0.884 0.0814 9.072 0.88 0.769 0.2679 20.110 8.17
AnyDoor[7] 0.821 0.099 10.846 2.46 0.899 0.119 14.834 3.05
StableVITON[25] 0.852 0.0842 8.698 0.88 0.911 0.0500 11.266 0.72
Wear-Any-Way 0.877 0.078 8.155 0.78 0.934 0.0409 11.72 0.33
manyfantasticstylesofwearing,likerolled-upsleevesorpaints,andthedifferent
types of tuck.
5 Conclusion
Inthiswork,weproposeWear-Any-Way,anovelframeworkformanipulablevir-
tualtry-on.Besidesreachingstate-of-the-artperformanceontheclassicalsetting
of virtual try-on, our methods enable users to customize the style of wearing byWear-Any-Way 15
assigning control points. Our pipeline is based on a dual U-Net design. We in-
troduce a sparse correspondence align module to make our model customizable.
Wear-Any-Way serves as a practical tool for e-commerce and provides novel
inspirations for the future research of virtual try-on.
Limitations and potential effect. Our methods could still generate some
artifacts for fine details like human hands, especially when the hands only oc-
cupy a small region in the full image. This could be improved by using higher
resolutions and larger diffusion models like SD-XL [45].
References
1. Avrahami,O.,Lischinski,D.,Fried,O.:Blendeddiffusionfortext-driveneditingof
naturalimages.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition. pp. 18208–18218 (2022) 4
2. Bai, S., Zhou, H., Li, Z., Zhou, C., Yang, H.: Single stage virtual try-on via de-
formable attention flows. In: European Conference on Computer Vision. pp. 409–
425. Springer (2022) 4
3. Bhunia, A.K., Khan, S., Cholakkal, H., Anwer, R.M., Laaksonen, J., Shah, M.,
Khan,F.S.:Personimagesynthesisviadenoisingdiffusionmodel.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
5968–5976 (2023) 4
4. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 18392–18402 (2023) 4
5. Cao,M.,Wang,X.,Qi,Z.,Shan,Y.,Qie,X.,Zheng,Y.:Masactrl:Tuning-freemu-
tualself-attentioncontrolforconsistentimagesynthesisandediting.arXivpreprint
arXiv:2304.08465 (2023) 3, 6
6. Cao,S.,Chai,W.,Hao,S.,Zhang,Y.,Chen,H.,Wang,G.:Difffashion:Reference-
based fashion design with structure-aware transfer by diffusion models. arXiv
preprint arXiv:2302.06826 (2023) 4
7. Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., Zhao, H.: Anydoor: Zero-shot
object-level image customization. arXiv preprint arXiv:2307.09481 (2023) 11, 12,
14
8. Choi, S., Park, S., Lee, M., Choo, J.: Viton-hd: High-resolution virtual try-on via
misalignment-aware normalization. In: CVPR. pp. 14131–14140 (2021) 4
9. Choi, S., Park, S., Lee, M., Choo, J.: Viton-hd: High-resolution virtual try-on via
misalignment-aware normalization. In: CVPR (2021) 10, 12, 13
10. Dong,H.,Liang,X.,Zhang,Y.,Zhang,X.,Shen,X.,Xie,Z.,Wu,B.,Yin,J.:Fash-
ion editing with adversarial parsing learning. In: Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition.pp.8120–8128(2020) 1,3
11. Endo, Y.: User-controllable latent transformer for stylegan image layout editing.
arXiv preprint arXiv:2208.12408 (2022) 4
12. Ge, Y., Song, Y., Zhang, R., Ge, C., Liu, W., Luo, P.: Parser-free virtual try-on
via distilling appearance flows. In: CVPR. pp. 8485–8493 (2021) 4
13. Gou, J., Sun, S., Zhang, J., Si, J., Qian, C., Zhang, L.: Taming the power of dif-
fusionmodelsforhigh-qualityvirtualtry-onwithappearanceflow.arXivpreprint
arXiv:2308.06101 (2023) 4, 12, 14
14. Han,X.,Hu,X.,Huang,W.,Scott,M.R.:Clothflow:Aflow-basedmodelforclothed
person generation. In: ICCV. pp. 10471–10480 (2019) 416 Chen et al.
15. Hedlin,E.,Sharma,G.,Mahajan,S.,Isack,H.,Kar,A.,Tagliasacchi,A.,Yi,K.M.:
Unsupervised semantic correspondence using stable diffusion. Advances in Neural
Information Processing Systems 36 (2024) 9
16. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or,
D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint
arXiv:2208.01626 (2022) 4
17. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS
(2020) 1, 5
18. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840–6851 (2020) 4
19. Honda,S.:Viton-gan:Virtualtry-onimagegeneratortrainedwithadversarialloss.
arXiv preprint arXiv:1911.07926 (2019) 14
20. Hu,L.,Gao,X.,Zhang,P.,Sun,K.,Zhang,B.,Bo,L.:Animateanyone:Consistent
and controllable image-to-video synthesis for character animation. arXiv preprint
arXiv:2311.17117 (2023) 3, 6
21. Issenhuth, T., Mary, J., Calauzenes, C.: Do not mask what you do not need to
mask: a parser-free virtual try-on. In: Computer Vision–ECCV 2020: 16th Euro-
peanConference,Glasgow,UK,August23–28,2020,Proceedings,PartXX16.pp.
619–635. Springer (2020) 4
22. Jo, Y., Park, J.: Sc-fegan: Face editing generative adversarial network with user’s
sketch and color. In: Proceedings of the IEEE/CVF international conference on
computer vision. pp. 1745–1753 (2019) 1, 3
23. Karras, J., Holynski, A., Wang, T.C., Kemelmacher-Shlizerman, I.: Dream-
pose: Fashion image-to-video synthesis via stable diffusion. arXiv preprint
arXiv:2304.06025 (2023) 4
24. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,
M.: Imagic: Text-based real image editing with diffusion models. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
6007–6017 (2023) 4
25. Kim, J., Gu, G., Park, M., Park, S., Choo, J.: Stableviton: Learning seman-
tic correspondence with latent diffusion model for virtual try-on. arXiv preprint
arXiv:2312.01725 (2023) 1, 4, 11, 12, 14
26. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv:1312.6114
(2013) 5
27. Kwon,G.,Ye,J.C.:Diffusion-basedimagetranslationusingdisentangledstyleand
content representation. arXiv preprint arXiv:2209.15264 (2022) 4
28. Lee,C.H.,Liu,Z.,Wu,L.,Luo,P.:Maskgan:Towardsdiverseandinteractivefacial
image manipulation. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 5549–5558 (2020) 1, 3
29. Lee, S., Gu, G., Park, S., Choi, S., Choo, J.: High-resolution virtual try-on with
misalignment and occlusion-handled conditions. In: ECCV. pp. 204–219. Springer
(2022) 4, 14
30. Li, Z., Wei, P., Yin, X., Ma, Z., Kot, A.C.: Virtual try-on with pose-garment
keypointsguidedinpainting.In:ProceedingsoftheIEEE/CVFInternationalCon-
ference on Computer Vision. pp. 22788–22797 (2023) 12
31. Lindenberger, P., Sarlin, P.E., Pollefeys, M.: Lightglue: Local feature matching at
light speed. arXiv preprint arXiv:2306.13643 (2023) 9
32. Ling, P., Chen, L., Zhang, P., Chen, H., Jin, Y.: Freedrag: Point tracking is not
youneedforinteractivepoint-basedimageediting.arXivpreprintarXiv:2307.04684
(2023) 5Wear-Any-Way 17
33. Liu, J., Song, X., Chen, Z., Ma, J.: Mgcm: Multi-modal generative compatibility
modeling for clothing matching. Neurocomputing 414, 215–224 (2020) 1, 3
34. Liu, L., Zhang, H., Ji, Y., Wu, Q.J.: Toward ai fashion design: An attribute-gan
model for clothing match. Neurocomputing 341, 156–167 (2019) 1, 3
35. Liu, X., Li, J., Wang, J., Liu, Z.: Mmfashion: An open-source toolbox for visual
fashion analysis. In: Proceedings of the 29th ACM International Conference on
Multimedia. pp. 3755–3758 (2021) 9
36. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:Sdedit:Guided
image synthesis and editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073 (2021) 4
37. Morelli,D.,Baldrati,A.,Cartella,G.,Cornia,M.,Bertini,M.,Cucchiara,R.:Ladi-
vton: Latent diffusion textual-inversion enhanced virtual try-on. arXiv preprint
arXiv:2305.13501 (2023) 4, 12, 14
38. Morelli, D., Fincato, M., Cornia, M., Landi, F., Cesari, F., Cucchiara, R.: Dress
code: High-resolution multi-category virtual try-on. In: ECCV. pp. 2231–2235
(2022) 10, 13
39. Mou,C.,Wang,X.,Song,J.,Shan,Y.,Zhang,J.:Dragondiffusion:Enablingdrag-
stylemanipulationondiffusionmodels.arXivpreprintarXiv:2307.02421(2023) 3,
5, 10, 13
40. Nam,J.,Kim,H.,Lee,D.,Jin,S.,Kim,S.,Chang,S.:Dreammatcher:Appearance
matching self-attention for semantically-consistent text-to-image personalization.
arXiv preprint arXiv:2402.09812 (2024) 3, 6
41. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez,P.,Haziza,D.,Massa,F.,El-Nouby,A.,etal.:Dinov2:Learningrobust
visual features without supervision. arXiv:2304.07193 (2023) 11, 12, 13
42. Pan, X., Tewari, A., Leimkühler, T., Liu, L., Meka, A., Theobalt, C.: Drag your
gan:Interactivepoint-basedmanipulationonthegenerativeimagemanifold.arXiv
preprint arXiv:2305.10973 (2023) 4, 13
43. Pan, X., Tewari, A., Leimkühler, T., Liu, L., Meka, A., Theobalt, C.: Drag your
gan: Interactive point-based manipulation on the generative image manifold. In:
ACM SIGGRAPH 2023 Conference Proceedings. pp. 1–11 (2023) 10
44. Pautrat,R.,Suárez,I.,Yu,Y.,Pollefeys,M.,Larsson,V.:Gluestick:Robustimage
matchingbystickingpointsandlinestogether.In:ProceedingsoftheIEEE/CVF
International Conference on Computer Vision. pp. 9706–9716 (2023) 9
45. Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,T.,Müller,J.,Penna,
J.,Rombach,R.:Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXiv:2307.01952 (2023) 15
46. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021) 4
47. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021) 6, 11, 12, 13
48. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022) 1, 3, 5, 6, 10
49. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 10684–10695 (2022) 418 Chen et al.
50. Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A.: Superglue: Learning
feature matching with graph neural networks. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 4938–4947 (2020) 9,
13
51. Shi, Y., Xue, C., Pan, J., Zhang, W., Tan, V.Y., Bai, S.: Dragdiffusion: Har-
nessing diffusion models for interactive point-based image editing. arXiv preprint
arXiv:2306.14435 (2023) 5, 10, 13
52. Tang, L., Jia, M., Wang, Q., Phoo, C.P., Hariharan, B.: Emergent correspon-
dence from image diffusion. Advances in Neural Information Processing Systems
36 (2024) 9
53. Valevski, D., Kalman, M., Molad, E., Segalis, E., Matias, Y., Leviathan, Y.: Uni-
tune:Text-drivenimageeditingbyfinetuningadiffusionmodelonasingleimage.
ACM Transactions on Graphics (TOG) 42(4), 1–10 (2023) 4
54. Wang,B.,Zheng,H.,Liang,X.,Chen,Y.,Lin,L.,Yang,M.:Towardcharacteristic-
preserving image-based virtual try-on network. In: ECCV. pp. 589–604 (2018) 4
55. Wang, S.Y., Bau, D., Zhu, J.Y.: Rewriting geometric rules of a gan. ACM Trans-
actions on Graphics (TOG) 41(4), 1–16 (2022) 4
56. Xie, Z., Huang, Z., Dong, X., Zhao, F., Dong, H., Zhang, X., Zhu, F., Liang,
X.: Gp-vton: Towards general purpose virtual try-on via collaborative local-flow
global-parsing learning. In: CVPR. pp. 23550–23559 (2023) 4, 14
57. Xu, Z., Zhang, J., Liew, J.H., Yan, H., Liu, J.W., Zhang, C., Feng, J., Shou,
M.Z.: Magicanimate: Temporally consistent human image animation using diffu-
sion model. arXiv preprint arXiv:2311.16498 (2023) 3, 6
58. Yang,B.,Gu,S.,Zhang,B.,Zhang,T.,Chen,X.,Sun,X.,Chen,D.,Wen,F.:Paint
byexample:Exemplar-basedimageeditingwithdiffusionmodels.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
18381–18391 (2023) 4, 14
59. Yang, Z., Zeng, A., Yuan, C., Li, Y.: Effective whole-body pose estimation with
two-stagesdistillation.In:ProceedingsoftheIEEE/CVFInternationalConference
on Computer Vision. pp. 4210–4220 (2023) 6, 9
60. Zhang, J., Herrmann, C., Hur, J., Polania Cabrera, L., Jampani, V., Sun, D.,
Yang,M.H.:Ataleoftwofeatures:Stablediffusioncomplementsdinoforzero-shot
semantic correspondence. Advances in Neural Information Processing Systems 36
(2024) 9
61. Zhang, L.: Reference-only controlnet. https://github.com/Mikubill/sd-webui-
controlnet/discussions/1236 (20235) 3, 6
62. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 3836–3847 (2023) 8, 11
63. Zhu, L., Yang, D., Zhu, T., Reda, F., Chan, W., Saharia, C., Norouzi, M.,
Kemelmacher-Shlizerman, I.: Tryondiffusion: A tale of two unets. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
4606–4615 (2023) 1, 4, 6
64. Zou,X.,Kong,X.,Wong,W.,Wang,C.,Liu,Y.,Cao,Y.:Fashionai:Ahierarchical
dataset for fashion understanding. In: Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition workshops. pp. 0–0 (2019) 9, 10