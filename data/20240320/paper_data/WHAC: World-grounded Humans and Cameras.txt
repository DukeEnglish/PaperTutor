WHAC: World-grounded Humans and Cameras
WanqiYin∗,1,2,ZhongangCai∗,1,3,4,RuisiWang1,FanzhouWang1,
ChenWei1,HaiyiMei1,WeiyeXiao1,ZhitaoYang1,QingpingSun1,
AtsushiYamashita2,ZiweiLiu3,LeiYang1,4
1SenseTimeResearch,2TheUniversityofTokyo,
3S-Lab,NanyangTechnologicalUniversity,4ShanghaiAILaboratory
Abstract
Estimatinghumanandcameratrajectorieswithaccuratescaleintheworldcoor-
dinatesystemfromamonocularvideoisahighlydesirableyetchallengingand
ill-posedproblem. Inthisstudy,weaimtorecoverexpressiveparametrichuman
models(i.e.,SMPL-X)andcorrespondingcameraposesjointly,byleveragingthe
synergybetweenthreecriticalplayers: theworld,thehuman,andthecamera. Our
approachisfoundedontwokeyobservations. Firstly,camera-frameSMPL-Xesti-
mationmethodsreadilyrecoverabsolutehumandepth. Secondly,humanmotions
inherentlyprovideabsolutespatialcues.Byintegratingtheseinsights,weintroduce
anovelframework,referredtoasWHAC,tofacilitateworld-groundedexpressive
humanposeandshapeestimation(EHPS)alongsidecameraposeestimation,with-
outrelyingontraditionaloptimizationtechniques. Additionally,wepresentanew
syntheticdataset,WHAC-A-Mole,whichincludesaccuratelyannotatedhumans
andcameras,andfeaturesdiverseinteractivehumanmotionsaswellasrealistic
cameratrajectories. Extensiveexperimentsonbothstandardandnewlyestablished
benchmarkshighlightthesuperiorityandefficacyofourframework. Wewillmake
thecodeanddatasetpubliclyavailable.
1 Introduction
Expressivehumanposeandshapeestimation(EHPS)hasgarneredconsiderableresearchattention
duetoitswideapplicationsacrosstheentertainment, fashion, andhealthcareindustries. Despite
remarkableadvancementsinrecentyears,themajorityofEHPSmethodsprimarilyfocusonestimat-
ingparametrichumanmodels(i.e.,SMPL-X[30])inthecameracoordinatesystem. Thisapproach
fallsshortindynamicsituationswherethecameraandsubjectmoveconcurrently. Estimating3D
trajectoriesintheworldcoordinatesystem(world-grounded)from2Dcamerafootageischallenging
asthe3D-to-2Dprojectionresultsinalossofcriticalspatialinformation. Consequently,camera
trajectoriesdeducedarethusinherently“scaleless",andthedepthofhumansdirectlyestimatedfrom
thecameraperspectivelacksvalidity.
Inthiswork,wedemonstratethesynergybetweenhumans,cameras,andtheworld. First,existing
camera-frameEHPSmethods,althoughnotspecificallysupervisedtoestimatehumandepthdirectly,
canstillaccuratelydeducethetruedepth. Thisonlyrequiresareasonablyaccuratefocallengththat
canbeobtainedfromthevideocapturedevicesorestimated[20]. Second,roottranslationisacritical
componentofhumanmotions,allowingthelattertoserveasastrongpriorafteranassociationis
learned. Hence,byanalyzinghumanposes,onecanmakeaninformedestimationofthevelocityof
humanmovement. Buildingupontheseinsights,wepresentWHAC,anovelframeworkdesigned
tojointlyestimateexpressivehumanmodelsandcameramovementsusingamonocularvideo. For
anygiveninputvideo,camera-frameSMPL-Xparametersandapreliminarycameratrajectoryare
∗Equalcontributions.
Preprint.Underreview.
4202
raM
91
]VC.sc[
1v95921.3042:viXraFigure 1: WHAC synergizes human-camera (camera-frame SMPL-X estimation), camera-world
(visualodometry),andhuman-world(ourproposedMotionVelocimeter)modelingforconstructing
world-groundedhumanandcameratrajectories.
firstestimatedusingplug-and-playEHPS[6]andvisualodometry[39]models. Thehuman-camera
relative positions are first deduced. These estimations are then utilized with VO estimations to
canonicalize the sequences of human poses for accurate velocity estimation. Consequently, the
scaleofthecameratrajectorycanberecovered. ItisnoteworthythatWHACpioneerswhole-body,
optimization-freeestimationinaworld-groundedcontexttorecoverhumanandcameratrajectories
jointly.
Moreover, the development of a new dataset becomes essential to more accurately assess model
performanceonworld-groundedhumanmotionsandcameratrajectoriesacrossabroaderspectrum
ofscenarios. Recentstudieshaveunderscoredthesurprisingefficacyofsyntheticdata[6,3,42,7],
thankstoitsdiversityandcontrollability. Inspiredbythesefindings,weintroduceWHAC-A-Mole,a
comprehensivesyntheticdatasetforWorld-groundedHumansAndCameraswitharichcollectionof
AnimatedsubjectsunderMOvingviewpointsinmuLtipleEnvironments. WHAC-A-Molefeatures
comprehensivemotionsequencesthatinclude1)interactivehumanactivitiesfromDLP-MoCap[4],
2)partnerdancesfromDD100[36],inadditionto3)thestandardAMASS[27]motionrepository.
Notably,WHAC-A-Moleincludesautomaticallygeneratedcameratrajectoriesthatmimiccinematic
filmingtechniques,suchastrackingshotsandarcshots,therebyofferingahighlevelofrealism.
WevalidateourWHAConstandardbenchmarksandWHAC-A-Moletoobtainconsistentperfor-
mancegainscomparedtothestate-of-the-art(SoTA)methodsunderbothcamera-frameandworld-
groundedsettings. WHACevendemonstratesasurprisingcapabilitytohandlecornercaseswhen
motion-basedandcamera-basedobservationscontradict,pavingthewayforpotentialapplications.
Insummary,ourcontributionsarethree-fold. First,weproposeWHAC,anovelregression-based
framework that capitalizes on human priors for the pioneering world-grounded EHPS method.
Second, we contribute WHAC-A-Mole, a comprehensive benchmark with accurate human and
camera annotations of diverse human activities. Third, our empirical evaluations underscore the
superiorperformanceofWHACacrossmultiplebenchmarks.
2 RelatedWorks
2.1 ExpressiveHumanPoseandShapeEstimation(EHPS)
EHPScapturesbody,face,andhandsfrommonocularimagesorvideos,typicallythroughparametric
humanmodels(e.g.,SMPL-X[30]). Earlyoptimization-basedmethod[30]fitsSMPL-Xmodels
on 2D keypoints, and was soon outperformed by regression-based methods that were trained on
alargeamountofpaireddata. Two-stagemethodsestimatebodyparametersfirst,thenhand/face
parametersfromcrop-outimagepatches[10,33,49,12,28,22,46,29].Recently,OSX[24]proposes
the one-stage paradigm that estimates body, hand, and face with shared features. This paradigm
shift simplifies the pipeline and led to the first foundation model SMPLer-X [6] that achieved
unprecedentedgeneralizationabilityacrosskeybenchmarks. However,despitetheirsuccess,these
2SMPL-X Depth
Estimator Recovery
Cropped input input sequence
Visual R
Odometry c Canonicalization
(VO) CamVO: Rc, tcwo/ scale
Derive humans from VO cameras Humans from MV
CamVO
CamMV Motion
Velocimeter
Scaled VO camera trajectory Camera trajectory scale alignment Derive cameras from MV humans (MV)
Figure2: OverviewofWHAC.SMPL-Xestimatorextractscamera-frameSMPL-Xwithdummy
depth,whichisrecoveredinSec.3.2. ThescalelesscameratrajectoryestimatedbyVOisthenused
tocanonicalizethehumantrajectorytoestimateitsvelocityandthusscaleinSec.3.3. Acamera
trajectoryisthenderivedforalignmentandscalerecovery,whichsubsequentlyupdatesthehuman
trajectoryinSec.3.4.
methodsestimateparametrichumansinthecameracoordinate,lackinginformationontheglobal
trajectoryespeciallywhenthecameraismoving.
2.2 World-groundedRecoveryofHumansandCameras
Estimationofhumantrajectoryinworldcoordinatesystemtypicallyrequiresamulti-camerasetup[14,
18,48,31,15,5,8]oradditionalwearabledevices(e.g.,IMU[41,13]orelectromagneticsensors[19]).
Methods that require only a single camera often rely on other assumptions: Yu et al. [44] needs
thescenetobeprovidedbytheuserandLuvizonetal.[26]assumesastaticcamera. D&D[23],
GLAMR [45], and TRACE [37] estimate global human trajectories from single-frame poses or
imagefeatures. However,cameraandhumanrotationhaveacoupledeffectoncamera-frameglobal
orientationestimation,whichleadstoambiguity. Liuetal.[25]leveragesStructure-from-Motion
(SfM)[34]toreconstructbothcameraandhumantrajectoriesandadjusthuman’sscaletomatchthe
camera’s,whichmaynotreflecttheabsolutescale. Recently,SLAHMRleveragesSLAM[38]and
humanmotionprior[32]intheoptimizationtorecoverhumansandthecamera. However,theprocess
iscomputationallyexpensiveandtakesexcessivelylongtocomplete. PACE[21]alsoleveragesvisual
odometry[39]forcameraposeestimationandafasterhumanmotiontosignificantlyacceleratethe
optimizationprocessbutisstilltime-consuming. WHAM[35]isthefirstregression-basedworkin
thedomainthatfeaturesreal-timeperformance. Ittakescameraestimation(angularvelocity)asthe
inputandestimateshumanparametersinthecameraframeandhumantrajectoryintheworldframe
throughseparatebranches,whilethecameratrajectoryisnotrecovered. OurWHACaimstorecover
bothhumanandcameratrajectoriesintheworldcoordinatewithaccuratescales.
3 Methodology
Recoveringaccurate3Ddimensionsfrom2Dobservationsisanill-posedproblem: asmallobjectat
acloserangemayappearthesameasalargeobjectatafarrange. Inthissection,weaimtoaddress
twoambiguitieswithpriorsthataresurprisinglyeffective,butnotthoroughlyutilizedinexisting
EHPSworks: theparametrichumansthemselves.
3.1 Preliminaries
Problem Formulation. We aim to estimate human and camera pose sequences in the world
coordinatesystem. ThehumansarerepresentedbySMPL-Xparameters: globalorientationθw ∈
go
R1×3,translationtw ∈R1×3,bodyposeθ ∈R21×3,lefthandposeθ ∈R15×3,righthandpose
h b lh
θ ∈R15×3,jawposeθ ∈R1×3,bodyshapeβ ∈R10andfacialexpressionϕ∈R10. Thecameras
rh j
arerepresentedbyworld-framerotationRw ∈R1×3 andtranslationtw ∈R1×3. Inthiswork,the
c c
superscriptindicatesthecoordinatesystem(i.e.,wforworldandcforcamera).
3Ct2
H H
dt2
I I
HH t2t1Ht0 dt0d dt t2 1
Ct2
Ct1
Ht2
Ht0Ht1 dt1
Ct1
O2 O1
Ct0 dt0 f1 tz1
a) Ct0 b) f2 tz2
Figure3:a)HumantrajectoriesH derivedfromcameratrajectoriesCofdifferentscalescanbevastly
differentinbothshapeanddirection,despitethatthesamecamera-framehumanrootdepthd and
t
translationstc areused. b)Differentpairsoffocallengthf andt cancorrespondtothesameimage.
h z
Camera-frameSMPL-XEstimation typicallyomitsabsolutedepthestimation. Hence,primary
metrics(e.g.,PA-MPJPE,MPJPE,andPVE)allperformrootalignment. WeemploySMPLer-X[6],
astrongfoundationmodelthatdemonstratesaccurateestimationofhumanposeandshapes. Weadd
additionalGRUsbeforethepredictionheadsandfinetunethemodeltobettercapturethetemporal
cues.
VisualOdometry(VO) typicallyprovideshigh-qualityRw;thetrajectoryformedbytwisscaleless
c c
butaccurateinshape. Wealsofollowthestandardtodefinethefirstcameraframeoftheinputvideo
astheworldcoordinatesystem.
3.2 RecoveringCamera-spaceHumanRootTranslation
MainstreamEHPSmethods[28,24,6]recoverparametrichumansinthecameraspaceandadopta
weakperspectivecameramodel,whichconsidersallpointstobeatthesamedepthawayfromthe
camera.
(cid:34)f∗ 0 0(cid:35)(cid:34)t +δx(cid:35) (cid:34)f∗ 0 0(cid:35)(cid:34)t +δx(cid:35)
x x
0 f∗ 0 t +δy ≈ 0 f∗ 0 t +δy (1)
y y
0 0 1 t +δz 0 0 1 t
z z
wheref∗ indicatesthefocallengthinNDC(NormalizedDeviceCoordinate)space(imagepixel
coordinatesarenormalizedinto[−1,1]). tc =(t ,t ,t )istheroottranslationinthecameraframe,
h x y z
(δx,δy,δz)istherelativetranslationofapointrelativetotheroot. Hence,theprojected2Dpoint
(NDCspace)iswrittenas:
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
u∗ f∗(t +δx)/t s(t +δx)
= x z = x (2)
v∗ f∗(t +δy)/t s(t +δy)
y z y
wheresisthescaleparameter. TheSMPL-Xestimatoralsopredictscameraparameters(s,t ,t )
x y
toreprojectSMPL-Xjointsontheimageplane. Hence,weobtaintherelationshipsbetweenfocal
length,depth,ands:
f∗ 2 f 2 f
s= = × ⇒t = × (3)
t I t z I s
z z
wheref isthefocallengthinpixels. I istheresolution(sidepixellength)oftheinputcroptothe
SMPL-Xestimator.
Wepointoutthatalthoughthesecamera-framemethodsdonotsupervisethehumanrootdeptht ,by
z
trainingthemodeltoproduceascalesthatoverlaysSMPL-Xaccuratelybackontheimageplane,
themodelimplicitlylearnshumanrootdeptht thatiscoupledwithfocallengthf asillustratedin
z
Fig.3b). Adummyfocallengthof5,000isoftenused[28,24,6],however,thisleadstounrealistic
human root depth t . We highlight that accurate intrinsic parameters are accessible from many
z
devices,andourempiricalresultsshowusingthediagonalpixellength[20]alsoyieldssatisfactory
results.
3.3 EstimatingWorld-frameHumanMotionsforScaleExtraction
Inrecentyears,therehasbeenaplethoraofhigh-qualityopticalmotioncapturedatasetsthatbecome
available, covering a wide range of human activities. Previous art [35] estimates human global
4trajectoryfrom2Dkeypointobservations,whichmaynotcapturesubtle3Dinformation. Hence,we
proposetolearnabsolutescalefrom3Dhumanmotions.
First,foraSMPL-XsequenceofK frames,estimatedinthecameracoordinatesystem,wecompute
the3Djointcoordinates. Specifically,foreachframe:
Jc =M(θc ,tc,θ ,θ ,θ ,θ ,β,ϕ),Jc ∈RK×15×3 (4)
go b lh rh j
whereM istheSMPL-Xparametricmodel. Weselect15joints(14LSP[17]jointsandthepelvis)
fromtheoriginal55joints. Wethencomputethejointsintheworldcoordinatesystem:
Jw =Tvo×Jc,Tvo =[Rw|tw] (5)
c c
whereTvoisthevisualodometry’sestimation(camera-to-worldtransformation). Notethattw does
c
nothaveavalidscale.
Tofacilitatethetraining,westandardizetheinput: wedefineacanonicalframewherethehumanis
root-alignedwithzeroglobalorientation. WethencomputethecanonicaltransformationTcanoby
usingthefirst(0th)frame’srotationandoffsetthetranslationtozero:
Tcano =[Rcano|tcano],Rcano =(Rw ×θc )−1 =(θc )−1,tcano =−pw (6)
c,0 go,0 go,0 0
whereRw is0th camerarotationintheworldframeestimatedfromvisualodometry,θc is0th
c,0 go,0
globalorientationestimatedinthecameraspace,pw isthepelvisjointofJw. NoteRw isanidentity
0 0 c,0
matrixI asthe0thcameraframeisdefinedastheworldframe. Alljointsarethencanonicalizedas
3
Jcano =Tcano×Jw.
OurMotionVelocimeterthenestimatesper-framevelocityinthecanonicalspace:
Vcano =MotionVelocimeter(Jcano) (7)
wherethevelocityisthende-canonicalizedbacktotheworldframe:
Vworld =(Tcano)−1×Vcano (8)
with Vworld, we can reconstruct the human trajectory with scale in the world coordinate system.
MotionVelocimeteronlyrequiresasimplearchitecturethatweincludeintheSupplementaryMaterial.
3.4 RecoveringScaledHumanandCameraTrajectories
Asweobtainhumantrajectorytwwithabsolutescale,onepossiblewayistoalignthehumantrajectory
h
derivedfromtheVO-estimatedcameratrajectoryusingcamera-framehumanroottranslationtotw.
h
However,Fig.3a)showsthatsuchalignmentisproblematicasthehumantrajectoryderivedfrom
scalelesscameratrajectorymaybeinvalid. Hence,weproposetotransferthescaletothecamera
trajectoryintwosteps. First,wederiveacameratrajectoryfromthehumantrajectory:
Tw =(Tcano)−1×Tcano×(Tc)−1 (9)
c,derived h h
Tw =[Rw |tw ] (10)
c,derived c,derived c,derived
Thisderivedcameratrajectoryalreadyhasanaccuratescalewithagoodshape. However,wefind
thatthecameratrajectoryestimatedbyVOhasabetter,morerobustshapebecauseitcanleverage
visualcuesthataremuchdenserthanhumanmotioncues. Inthislight, weperformUmeyama’s
U
method [40] (shown as →) to align the VO-estimated camera trajectory with the human-derived
cameratrajectorytw whilediscardRw andkeepingthecamerarotationRw:
c,derived c,derived c
tw =tw →U tw (11)
c,final c c,derived
Hence,wethenupdatehumantrajectorybyderivingitfromthealignedcameratrajectorytw :
c,final
Tw =Tw ×Tc,Tw =[Rw|tw ],Tw =[Rw |tw ] (12)
h,final c,final h c,final c c,final h,final h,final h,final
Asaresult, weobtainhumantrajectorytw andcameratrajectorytw , bothintheworld
h,final c,final
coordinatesystemandwithabsolutescales.
5Table 1: Dataset Comparison. #Inst.: number of human instances (crops). #Seq.: number of
videosequences. R/S:RealorSynthetic. Multi.: multipersonscenes. Track.: trackIDlabels. HHI:
human-humaninteractionmotions. †: EgoSet. ‡: unknownasthedataisnotreleasedwhenthispaper
iswritten. ⋄: typicallyshort(<100frames)clips.
Dataset #Inst. #Seq. R/S Multi. Track. Contact HHI Camera Human
3DPW[41] 74.6K 60 R ✓ × × ✓ Moving SMPL
RICH[16] 476K 141 R ✓ ✓ ✓ ✓ Static* SMPL
HCM[21] ‡ 25 S ✓ ‡ × × Moving SMPL
EMDB[19] 109K 81 R × N.A. × N.A. Moving SMPL
EgoBody†[47] 175K 125 R ✓ ✓ × ✓ Moving SMPL-X
BEDLAM[3] 951K 10.4K⋄ S ✓ ✓ × × Static SMPL-X
SynBody[42] 2.7M 27K⋄ S ✓ ✓ × × Static SMPL-X
WHAC-A-Mole 1.46M 2434 S ✓ ✓ ✓ ✓ Moving SMPL-X
4 WHAC-A-MoleDataset
WehighlightthatWHAC-A-Molecombinesfine-craftedautomaticcameramovementswithvaried
charactersanimatedwithdiverse,high-qualitymotionsequencestogenerateadatasetwithaccurate
cameraandSMPL-Xannotations. Thedatasetisconstructedwiththeadvancedhumandatasynthesis
toolboxXRFeitoria[11]. ItleveragesSMPL-XL(alayeredextensionofSMPL-X)tocreatevirtual
humanswithdiversebodyshapes,clothing,andaccessories. WefollowSynBody[42]inthescene
setup,subjectcreation,andplacement. Wefurtherimprovethedatasynthesizedintwoways: diverse
motion sources (Sec. 4.1) and camera trajectory generation (Sec. 4.2). In Tab. 1, we compare
WHAC-A-Molewithpopularvideo-basedbenchmarkswithbothcameraandhumanannotations.
WHAC-A-Molefeaturesacompetitivescaleoftraininginstancesandvideosequences,multiperson
scenes with track IDs, contact labels, accurate camera pose and SMPL-X annotations. We split
WHAC-A-Molebymotionsequenceinto80%:20%fortrainingandtesting. ExamplesofWHAC-A-
MolearevisualizedinFig.4.
4.1 InteractiveHumanMotions
AMASS[27]isapopularmotionrepository,widelyusedbyexistingsyntheticdatasets[21,3,42].
However,AMASSonlycontainssingle-personmotions. Asaresult,syntheticdataiscapturedin
virtualscenespopulatedwithunrelatedsingle-personmotions,typicallyscatteredsparselytoavoid
collision. However,closehumaninteractionsarecommonindailylife,anddifficulttosolve. Inthis
light,weselecttwolatestmotiondatasetsthatcontaincomprehensiveinteractivehumanmotions.
First, DD100 [36], a duet dance motion capture dataset that includes near two hours of partner
dancesof10differentgenres. Second,DLP-MoCap[4],amotioncapturedatasetcontainingdaily
interactions between two subjects. Since SMPL-XL models are fully compatible with SMPL-X
bodyposesequences,weanimatevirtualcharacterswithacombinationofAMASS,DD100,and
DLP-MoCap.
4.2 CameraTrajectoryGeneration
Tobettermodelthecameramovement,weadopttherepresentationinRaoetal.[1]todefinethe
camerainahuman-centricsphericalcoordinatesystem(r ,θ ,ϕ ),inwhichther representsthe
c c c c
distancefromthecameratothecharacter,whilethepolarangleθ andtheazimuthalangleϕ define
c c
theanglebetweenthecamera’slookingdirectionandthecharacter’sfacingdirection. Therefore,
givenacharacter’slocation(x ,y ,z )andfacingdirection(θ ,ϕ ),thecamera’slocationin
ch ch ch ch ch
theworldspaceis
(x ,y ,z )=(x ,y ,z )+r (sin(θ)cos(ϕ),sin(θ)sin(ϕ),cos(θ)) (13)
c c c ch ch ch c
where the θ = (θ +θ ) mod 2π, the ϕ = (ϕ +ϕ ) mod 2π, and the camera’s rotation is
c ch c ch
thereby calculated by restricting the camera look at the (x ,y ,z ). In WHAC-A-Mole, we
ch ch ch
designtwotypesofshotscalesincludingthemediumshotandthefullshot,whichrespectivelyuse
the location of the neck and the pelvis as the character’s location (x ,y ,z ). For the motion
ch ch ch
sequencesthatconsistofmultiplecharacters,the(x ,y ,z )andthe(θ ,ϕ )arederivedfrom
ch ch ch ch ch
6Figure4: VisualizationofWHAC-A-Molesamplesequences,animatedwitha)AMASS,b-c)DLP-
MoCap, and d-e) DD100. In each sample, the first row depicts the overview (note the camera
trajectoryshowninbrightrays),andthesecondandthethirdrowsshowthecameraviewandoverlaid
SMPL-Xannotations.
theaverageofthelocationsandthefacingdirectionsofallthecharacters.Basedonthehuman-centric
sphericalcoordinatesystem(r ,θ ,ϕ ),wedesigndifferentkeyframe-settingstrategiestosimulate
c c c
fivecommoncameramovementsbelow.
-Arcshotaddsequally-spacedkeyframestorotatethecameraaroundthecharacterhorizontally
by increasing or decreasing ϕ ∈ [ϕ ,ϕ ] or vertically by increasing or decreasing θ ∈
c min max c
[θ ,θ ]. Moreover,theangularvelocityofthearcshotscanbecontrolledbyadjustingthe∆ϕ
min max c
orthe∆θ betweentwoadjacentkeyframes.
c
- Push shot also adds equally-spaced keyframes and moves the camera towards the character by
decreasingther . Morespecifically,thefractionofthecharacterintheview,whichismoreintuitive
c
whenfilming,isusedtoindirectlydecreasether by
c
(cid:40)
hbbox as⩽1.0
r = frac∗tan(fov/2) (14)
c hbbox∗as as>1.0
frac∗tan(fov/2)
wheretheh istheheightofthecharacter’sboundingboxinthecameraspace,thefracisthe
bbox
desiredfractionofthecharacterintheview,theasistheaspectratioofthecameraframe,andthe
fovisthefieldofviewofthecamera. Similartothearcshots,thespeedofthecameramovement
canbeadjustedbysettingthe∆fracbetweentwoadjacentkeyframes.
- Pull shot is opposite to the push shot and moves the camera further away from the charac-
ter by increasing the r under the control of the frac. Randomly sampling frac in a range
c
[frac ,frac ]atdifferentkeyframesderivescontinuouspushingandpulling,whichiscom-
min max
monlyusedwhenfilmingdances.
-Trackingshotfollowsthecharacterandmaintainstherelativepositionbetweenthecameraand
thecharacter,i.e.,maintainthe(r ,θ ,ϕ )ofthecamerainthehuman-centricsphericalcoordinate
c c c
system. A new keyframe of the tracking shot is added when the overlap ratio of the character’s
boundingboxinthecurrentframeandinthelastkeyframeisgreaterthanathresholdλ .
overlap
-Panshotrotatesthecamerahorizontallytokeepthecameralookingatthecharacter,thereforeitis
anotherwaytomakethecamerafollowthecharacter,anditsharesthesamerulewiththetracking
shottoaddanewkeyframe.
Ratherthanassigningaspecificcameramovementtoanentirehumanmotionsequence,ourpipeline
automaticallycombinesseveraltypesofcameramovementsintoonemotionsequencetoincreasethe
varietyofcameramovements. Forexample,whencapturingstaticmotions(whoselongestedgeof
theboundingboxformedbythe(x ,y ,z )acrossallframesislessthanathresholdλ )or
ch ch ch bbox
interactivemotions,wecombinethehorizontalandtheverticalarcshotswiththerandompullorpush
shotstorotatethecameraaroundthecharactersaswellastransitingsmoothlybetweendifferentshot
angles,suchashigh-angle,low-angleoreye-level,andpushinginorpullingoutthedistancebetween
thecameraandthecharacterstoincreasetherhythmofthecameramovement. Forthemotionswith
7long-distancemovements,wecombinethetrackingshotsandthepanshotstofollowthecharacter.
Ifthecharacter’sfacingdirectionisstable(i.e.,thattherotationanglefromthecharacter’sfacing
directioninthelastkeyframetothecurrentkeyframeislessthanathresholdλ ),weusethe
angle
trackingshot. Otherwise,weusethepanshot. Thisruleeffectivelysmoothsthecamera’smovement,
especiallywhenthecharacterturnsdramatically.
5 Experiments
WeevaluateWHAConbothcamera-frameandworld-groundedbenchmarkstocompareitsparametric
humanrecoveryabilitieswithexistingSoTAmethods. Duetospaceconstraints,weincludeinference
speedcomparison,morevisualizationsontrajectoryreconstruction,andmorequalitativeresultsin
theSupplementaryMaterial.
5.1 ImplementationDetails
WefinetuneSMPLer-X-B[6]withEgoBody,3DPW,andEMDBforcamera-frameestimationof
SMPL-X parameters. WHAC-A-Mole (with motions from AMASS, DD100, and DLP-MoCap),
3DPW,EMDB,andRICHareusedtotraintheMotionVelocimeter. MoredetailsareintheSupple-
mentaryMaterial.
5.2 Datasets
InadditiontoourproposedWHAC-A-Mole,mainstreambenchmarksforhumanposeandshape
estimation with parametric human labels are used. EgoBody [47] includes 125 sequences of 36
subjects in 15 indoor scenes, featuring 3D human motions interacting with scenes. We study
theEgoSetthatiscapturedbyahead-mountedcamera;2)3DPW[41],apopulardatasetwith60
sequencescapturedbyaniPhone,featuringdiversehumanactivitiesinoutdoorscenes;3)EMDB[19]
provides 58 minutes of motion data of 10 subjects in 81 indoor and outdoor scenes. Notably, it
containsasubset,EMDB2,thatcontainsglobaltrajectoriesofhumansandcameras. 4)RICH[16]
consistsof142multi-viewvideoswith22subjectsand5sceneswith6-8fixedcameras. RICHisnot
usedforevaluationasthecamerasarestatic.
5.3 EvaluationMetrics
Forcamera-framehumanrecovery, weusethestandardMeanPerJointPositionError(MPJPE),
Procrustes-alignedMPJPE(PA-MPJPE),PerVertexError(PVE)inmillimeters(mm),andAccel-
erationerror(Accl.) inm/s2. Notethatthesemetricsareevaluatedafterrootalignmentbetween
estimatedandgroundtruthparametrichumans,thusnotconsideringdiscrepancyintranslationes-
timation. In this light, we also report T-MPJPE [2] and similarly T-PVE, which are variants of
MPJPEandPVEthatincludestranslationestimationtoreflecttheaccuracyofdepthestimationinthe
cameraspace.
Forworld-framehuman/camerarecovery, wefollowpreviousworks[43,21,35]tosplithuman
motion sequences with global trajectory into 100-frame segments. The segments are Procrustes-
alignedtothegroundtruthforMPJPEcomputation: W-MPJPE ifthefirsttwoframesareusedin
100
thealignmentorWA-MPJPE iftheentiresegment.Toevaluatethequalityoftrajectory,weextend
100
Average Trajectory Error (ATE) [21] to C-ATE and H-ATE for camera and human respectively,
whicharecomputedafterProcrustes-alignmentofestimatedandgroundtruthtrajectories. Allmetrics
areinmillimeters(mm). WealsoreportrespectiveAlignmentScales(AS)usedinthealignment
for the camera (C-AS) and human (H-AS) and values closer to 1.0 indicate more accurate scale
estimation.
5.4 World-groundedBenchmarks
InTab.3,weevaluateonWHAC-A-MoleinTab.2. WHAC-A-Moleprovidesexpressivehuman(i.e.,
SMPL-X),withaccuratelyannotatedcameramotions. SincenoexistingEHPSmethodsproduce
SMPL-Xintheworldcoordinatesystemandastrictlyfaircomparisonisnotplausible,webuildthe
firstbenchmarkbymakingtwoadaptationstotheSoTAmethods(OSX[24]andSMPLer-X[6]):
8Table 2: World-frame evaluation on WHAC-A-Mole. *: adapted to world-grounded evaluation.
H-ASandC-AS:thecloserto1.0,thebetter.
PA-MPJPE↓ W-MPJPE↓ WA-MPJPE↓ H-ATE↓ H-AS C-ATE↓ C-AS
OSX*[24]+DPVO[39] 90.1 1036.1 390.7 180.5 0.5 0.5 7.3
SMPLer-X-B*[6]+DPVO[39] 76.7 842.3 335.4 138.3 0.5 0.5 7.3
WHAC(GTGyro) 76.5 343.8 182.0 103.5 0.9 0.5 1.3
WHAC 76.5 343.3 182.0 103.5 0.9 0.5 1.3
Table3: World-frameevaluationonEMDB2. *: adaptedtoworld-groundedevaluation. H-ASand
C-AS:thecloserto1.0,thebetter.
PA-MPJPE↓ W-MPJPE↓ WA-MPJPE↓ H-ATE↓ H-AS C-ATE↓ C-AS
GLAMR[45] 56.0 756.1 286.2 - - - -
SLAHMR[43] 61.5 807.4 336.9 207.8 1.9 - -
WHAM[35](GTGyro) 41.9 436.4 165.9 83.2 1.5 - -
OSX-L*[24]+DPVO[39] 99.9 1186.2 458.8 235.4 2.3 14.8 5.1
SMPLer-X-B*[6]+DPVO[39] 42.5 930.1 375.8 200.6 2.0 14.8 5.1
WHAC(GTGyro) 39.4 392.5 143.1 75.8 1.1 14.8 1.5
WHAC 39.4 389.4 142.2 76.7 1.1 14.8 1.4
camera-frametranslationestimationandvisualodometry. Implementationdetailsareincludedinthe
SupplementaryMaterial. ItisnotedthatmethodsthatachievegoodresultsonEMDBstillstruggle
onWHAC-A-Mole,whichcanbeattributedtothemorechallengingscenariosofWHAC-A-Mole
(involvinghardposes,diverseinteractions,occlusions,andcomplicatedcameramovements). We
hopeWHAC-A-Molecanserveasausefulfoundationforfutureworld-groundedEHPSresearch.
Moreover,wecompareWHACwithbothbody-onlymethods(GLAMR[45],SLAHMR[43],and
WHAM[35])andwhole-bodymethods(OSX-L[24]andSMPLer-X[6])onEMDB2,whereWHAC
achievesbestperformance,evensurpassingbody-onlymethodsthatarenativetoEMDB’sSMPL
annotations.
5.5 Camera-spaceBenchmarks
In Tab. 4, it is shown that WHAC outperforms existing SoTAs. We highlight that 1) WHAC
archivesimmenseT-PVE-allimprovement,whichcapturesabsolutedepthestimationfromhumansto
cameras. ThisisbecauseWHACformulatesthesubjectdistancetothecamera. 2)Withtemporal
informationembeddedintheEHPSmodule,WHACattainssubstantialreductionsinacceleration
error (Accl.) compared to previous single-frame SoTAs. Moreover, temporal cues also lead to
significantperformancegainsinhandandfaceestimation. InTab.5,wefurtherevaluateWHAC
onEMDBand3DPW,wheretheplausibilityofcamera-framehumantranslationestimationandthe
significanceoftemporalmodelingarevalidatedagain.
AlbeitWHAC-A-Moleismainlydesignedforworld-groundedevaluationofhumanandcamerapose
sequences,weevaluateWHAC’sperformanceunderthecamera-framesettingonWHAC-A-Molein
Tab.6. Similartopreviousexperiments,itisobservedthatWHACisonparwithSMPLer-Xwith
betterperformanceontheaccelerationerror.
Table4: Resultsofcamera-framemethodsonEgoBody(EgoSet)withSMPL-Xgroundtruths. PVE
variantsaremeasuredforwhole-body(SMPL-X)methodsonly.
PA-MPJPE↓ PA-PVE-all↓ PVE-all↓ PVE-hand↓ PVE-face↓ Accl.↓
GLAMR[45] 114.3 - - - - 173.5
SLAHMR[43] 79.1 - - - - 25.8
Hand4Whole[28] 71.0 59.8 127.6 48.0 41.2 27.2
OSX-L[24] 66.5 54.6 115.7 50.5 41.0 24.7
SMPLer-X-B[6] 47.1 40.7 72.7 43.7 32.4 18.9
WHAC 46.9 39.0 64.7 41.0 26.3 11.6
9Table5: Morecamera-frameevaluationsonEMDB1and3DPW.Comparedtoexistingmainstream
EHPSmethods,WHACrecoversmeaningfulhumandepths(T-PVE)andachievesloweracceleration
errors(Accl.).
EMDB1[19] 3DPW[41]
Method
PA-PVE↓ PVE↓ T-PVE↓ Accl.↓ PA-PVE↓ PVE↓ T-PVE↓ Accl.↓
Hand4Whole[28] 99.5 143.1 36851.8 34.2 81.7 124.7 30279.0 31.0
OSX-L[24] 93.3 134.0 45526.0 30.3 76.9 117.8 38472.2 24.9
SMPLer-X-B[6] 68.2 99.3 41298.0 24.4 62.6 95.6 32532.0 24.8
WHAC 61.0 91.2 140.2 18.4 62.8 91.9 260.8 20.3
Table6: Resultsofcamera-framemethodsonWHAC-A-Mole. WHACisonparwithSMPLer-X
butproducesaloweraccelerationerror.
PA-MPJPE↓ PA-PVE-all↓ PVE-all↓ PVE-hand↓ PVE-face↓ Accl.↓
OSX-L[24] 90.1 88.1 155.7 83.3 85.0 38.9
SMPLer-X-B[6] 76.7 74.8 116.2 70.6 63.1 44.0
WHAC 76.5 74.8 117.8 77.7 63.2 31.2
5.6 AblationStudy
Weevaluatethenecessityofthekeycomponentsin Tab.7. Itisobservedthatusingvisualodometry
alone(bodytrajectorydependsonestimatedcameratrajectory)leadstoaccuratecameratrajectory
shape (lowest camera trajectory error) but lacks accurate scale (alignment scale is far from 1.0).
UsingMotionVelocimeteralone(cameratrajectorydependsonestimatedbodytrajectory),however,
resultsinveryaccuratescalerecoveryandbetterbodytrajectoryerror. WHACleveragesthescale
recoveryabilityofMotionVelocimeterandvisualodometer,achievinghigh-qualitybodyandcamera
trajectorieswithonlyaslightdeclineinscaleaccuracy.
5.7 Visualization
We highlight that WHAC is the first regression-based, whole-body method that simultaneously
predictscameraandhumantrajectories. Wehighlightthatthecameraprovidessupplementarycues
to human motions. In Fig. 5 a) and b), we test two corner cases where the human motion itself
canbemisleading: whenhumanposeappearsstationarybutthereisrootmovementintheworld
coordinates(e.g.,skateboarding),andwhenhumanposeclearlyindicatesmotionbutthereisnoroot
movementintheworldcoordinates(e.g.,runningonatreadmill). Ourformulationconsidersboth
motionandcameracuestopredictthecorrecttrajectorieswhereasWHAMfails,whichleveragesfoot
contactandlockingbutnocamerainformationinhuman’sglobaltrajectoryestimation. Wealsoshow
complicatedscenariosonc)in-the-wildvideofromTikTok,whichfeaturesafast-movingobject. Our
MotionVelocimetercanestimatereasonablerootmovement,whereasWHAM’scontactestimation
andfoot-lockingresultsinafloatingsubject. MorevisualizationsareincludedintheSupplementary
Material.
WHAC (Ours)
WHAM
a) b) c)
Figure5: Visualizationonin-the-wildhardcases. WHACleverageshuman-camera-scenecollabo-
rationtoresolvecaseswheremotionprioralonewouldfail: a)Skateboardingandb)Treadmill. c)
WHACcanalsohandlefastcases.
10Table 7: Ablation on key components. DPVO Table 8: Ablation on intrinsic sources. A rea-
representsvisualodometry,MVrepresentsMo- sonableintrinsicdrasticallyimprovehumanroot
tionVelocimeter. translationestiamtion.
Method WA-MPJPE↓ H-ATE↓ C-ATE↓ C-AS T-MPJPE↓ W-MPJPE↓ WA-MPJPE↓
DPVO 376.0 177.8 14.8 5.10 Dummy(5,000) 36020.4 6239.9 604.6
MV 233.2 129.9 134.1 1.10 Assumed[20] 179.7 391.2 144.0
MV+DPVO 142.2 76.7 14.8 1.40 GT 100.3 389.4 142.2
6 Conclusion
Inconclusion, wepresentWHAC,thepioneeringregression-basedEHPSmethodthatjointlyre-
covers human motions and camera trajectories in the world coordinate system. Moreover, our
WHAC-A-Moleservesasausefulbenchmarkfortheevaluationofworld-groundedEHPSmethods.
WHACachievesSoTAperformanceonbothstandardbenchmarksandourproposedWHAC-A-Mole,
demonstratingstrongpotentialsfordownstreamapplications.
Limitations. WHAC-A-Moleincludesarichcollectionofmultipersonscenariosthatmayrequire
specialalgorithmdesignstotacklecloseinteractionandocclusions,whichWHAClacks. Weleave
thistothefuturework.
Potentialnegativesocietalimpact. WHACmaybeusedforunwarrantedsurveillanceasitrecovers
humantrajectoriesintheworldframe.
11References
[1] RaoAnyi,JiangXuekun,GuoYuwei,XuLinning,YangLei,JinLibiao,LinDahua,andDaiBo.
Dynamicstoryboardgenerationinanengine-basedvirtualenvironmentforvideoproduction.
arXivpreprintarXiv:2301.12688,2023.
[2] EduardGabrielBazavan,AndreiZanfir,MihaiZanfir,WilliamTFreeman,RahulSukthankar,
andCristianSminchisescu. Hspace: Syntheticparametrichumansanimatedincomplexenvi-
ronments. arXivpreprintarXiv:2112.12867,2021.
[3] Michael J Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. Bedlam: A synthetic
datasetofbodiesexhibitingdetailedlifelikeanimatedmotion. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages8726–8737,2023.
[4] ZhongangCai,JianpingJiang,ZhongfeiQing,XinyingGuo,MingyuanZhang,ZhengyuLin,
Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, et al. Digital life project: Autonomous 3d
characterswithsocialintelligence. arXivpreprintarXiv:2312.04547,2023.
[5] ZhongangCai,DaxuanRen,AilingZeng,ZhengyuLin,TaoYu,WenjiaWang,XiangyuFan,
YangGao,YifanYu,LiangPan,etal. Humman: Multi-modal4dhumandatasetforversatile
sensingandmodeling. InEuropeanConferenceonComputerVision,pages557–577.Springer,
2022.
[6] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Wang Yanjun, Hui En
Pang,HaiyiMei,MingyuanZhang,LeiZhang,ChenChangeLoy,LeiYang,andZiweiLiu.
Smpler-x: Scalingupexpressivehumanposeandshapeestimation. InA.Oh,T.Neumann,
A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,AdvancesinNeuralInformation
ProcessingSystems,volume36,pages11454–11468.CurranAssociates,Inc.,2023.
[7] ZhongangCai,MingyuanZhang,JiaweiRen,ChenWei,DaxuanRen,ZhengyuLin,Haiyu
Zhao, Lei Yang, Chen Change Loy, and Ziwei Liu. Playing for 3d human recovery. arXiv
preprintarXiv:2110.07588,2021.
[8] WeiCheng,RuixiangChen,SimingFan,WanqiYin,KeyuChen,ZhongangCai,JingboWang,
YangGao,ZhengmingYu,ZhengyuLin,etal. Dna-rendering: Adiverseneuralactorrepository
for high-fidelity human-centric rendering. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pages19982–19993,2023.
[9] KyunghyunCho,BartVanMerriënboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-
decoderforstatisticalmachinetranslation. arXivpreprintarXiv:1406.1078,2014.
[10] VasileiosChoutas,GeorgiosPavlakos,TimoBolkart,DimitriosTzionas,andMichaelJBlack.
Monocularexpressivebodyregressionthroughbody-drivenattention. InComputerVision–
ECCV2020: 16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,Part
X16,pages20–40.Springer,2020.
[11] XRFeitoriaContributors. Openxrlabsyntheticdatarenderingtoolbox. https://github.com/
openxrlab/xrfeitoria,2023.
[12] YaoFeng,VasileiosChoutas,TimoBolkart,DimitriosTzionas,andMichaelJBlack. Collabo-
rativeregressionofexpressivebodiesusingmoderation. In2021InternationalConferenceon
3DVision(3DV),pages792–804.IEEE,2021.
[13] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard Pons-Moll. Human poseitioning
system (hps): 3d human pose estimation and self-localization in large scenes from body-
mountedsensors. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages4318–4329,2021.
[14] NilsHasler,BodoRosenhahn,ThorstenThormahlen,MichaelWand,JürgenGall,andHans-
PeterSeidel. Markerlessmotioncapturewithunsynchronizedmovingcameras. In2009IEEE
ConferenceonComputerVisionandPatternRecognition,pages224–231.IEEE,2009.
[15] BuzhenHuang,YuanShu,TianshuZhang,andYangangWang. Dynamicmulti-personmesh
recoveryfromuncalibratedmulti-viewcameras. In2021InternationalConferenceon3DVision
(3DV),pages710–720.IEEE,2021.
[16] Chun-HaoPHuang,HongweiYi,MarkusHöschle,MatveySafroshkin,TsvetelinaAlexiadis,
Senya Polikovsky, Daniel Scharstein, and Michael J Black. Capturing and inferring dense
12full-bodyhuman-scenecontact. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages13274–13285,2022.
[17] Sam Johnson and Mark Everingham. Clustered pose and nonlinear appearance models for
humanposeestimation. InBMVC,pages1–11.BritishMachineVisionAssociation,2010.
[18] HanbyulJoo,HaoLiu,LeiTan,LinGui,BartNabbe,IainMatthews,TakeoKanade,Shohei
Nobuhara,andYaserSheikh. Panopticstudio: Amassivelymultiviewsystemforsocialmotion
capture. In Proceedings of the IEEE International Conference on Computer Vision, pages
3334–3342,2015.
[19] Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang,
JuanJoséZárate,andOtmarHilliges. Emdb: Theelectromagneticdatabaseofglobal3dhuman
pose and shape in the wild. In Proceedings of the IEEE/CVF International Conference on
ComputerVision,pages14632–14643,2023.
[20] ImryKissos,LiorFritz,MatanGoldman,OmerMeir,EduardOks,andMarkKliger. Beyond
weak perspective for monocular 3d human pose estimation. In European Conference on
ComputerVision,pages541–554.Springer,2020.
[21] Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael J Black, Otmar
Hilliges, Jan Kautz, and Umar Iqbal. Pace: Human and camera motion estimation from
in-the-wildvideos. arXivpreprintarXiv:2310.13768,2023.
[22] Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang, and Cewu Lu. Hybrik-x:
Hybrid analytical-neural inverse kinematics for whole-body mesh recovery. arXiv preprint
arXiv:2304.05690,2023.
[23] JiefengLi,SiyuanBian,ChaoXu,GangLiu,GangYu,andCewuLu. D&d: Learninghuman
dynamicsfromdynamiccamera. InEuropeanConferenceonComputerVision,pages479–496.
Springer,2022.
[24] JingLin,AilingZeng,HaoqianWang,LeiZhang,andYuLi. One-stage3dwhole-bodymesh
recoverywithcomponentawaretransformer. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages21159–21168,2023.
[25] MiaoLiu,DexinYang,YanZhang,ZhaopengCui,JamesMRehg,andSiyuTang. 4dhuman
bodycapturefromegocentricvideovia3dscenegrounding. In2021internationalconference
on3Dvision(3DV),pages930–939.IEEE,2021.
[26] DiogoCLuvizon,MarcHabermann,VladislavGolyanik,AdamKortylewski,andChristian
Theobalt. Scene-aware3dmulti-humanmotioncapturefromasinglecamera. InComputer
GraphicsForum,pages371–383.WileyOnlineLibrary,2023.
[27] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J
Black. Amass: Archiveofmotioncaptureassurfaceshapes. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages5442–5451,2019.
[28] GyeongsikMoon, HongsukChoi, andKyoungMuLee. Accurate3dhandposeestimation
forwhole-body3dhumanmeshestimation. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages2308–2317,2022.
[29] HuiEnPang,ZhongangCai,LeiYang,QingyiTao,ZhonghuaWu,TianweiZhang,andZiwei
Liu. Towardsrobustandexpressivewhole-bodyhumanposeandshapeestimation. InA.Oh,
T.Neumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,AdvancesinNeural
Information Processing Systems, volume 36, pages 17330–17344. Curran Associates, Inc.,
2023.
[30] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman,
DimitriosTzionas,andMichaelJBlack. Expressivebodycapture: 3dhands,face,andbody
fromasingleimage. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages10975–10985,2019.
[31] SidaPeng,YuanqingZhang,YinghaoXu,QianqianWang,QingShuai,HujunBao,andXiaowei
Zhou. Neuralbody: Implicitneuralrepresentationswithstructuredlatentcodesfornovelview
synthesisofdynamichumans. InCVPR,2021.
[32] DavisRempe,TolgaBirdal,AaronHertzmann,JimeiYang,SrinathSridhar,andLeonidasJ
Guibas. Humor: 3dhumanmotionmodelforrobustposeestimation. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pages11488–11499,2021.
13[33] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap: A monocular 3d whole-body
pose estimation system via regression and integration. In Proceedings of the International
ConferenceonComputerVision,pages1749–1759,2021.
[34] JohannesLSchonbergerandJan-MichaelFrahm. Structure-from-motionrevisited. InProceed-
ingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages4104–4113,
2016.
[35] SoyongShin,JuyongKim,EniHalilaj,andMichaelJBlack. Wham: Reconstructingworld-
groundedhumanswithaccurate3dmotion. arXivpreprintarXiv:2312.07531,2023.
[36] LiSiyao,TianpeiGu,ZhitaoYang,ZhengyuLin,ZiweiLiu,HenghuiDing,LeiYang,and
ChenChangeLoy. Duolando: Followergptwithoff-policyreinforcementlearningfordance
accompaniment. InTheTwelfthInternationalConferenceonLearningRepresentations,2023.
[37] YuSun,QianBao,WuLiu,TaoMei,andMichaelJBlack. Trace: 5dtemporalregressionof
avatarswithdynamiccamerasin3denvironments. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages8856–8866,2023.
[38] ZacharyTeedandJiaDeng. Droid-slam: Deepvisualslamformonocular,stereo,andrgb-d
cameras. Advancesinneuralinformationprocessingsystems,34:16558–16569,2021.
[39] ZacharyTeed,LahavLipson,andJiaDeng. Deeppatchvisualodometry. AdvancesinNeural
InformationProcessingSystems,36,2024.
[40] ShinjiUmeyama. Least-squaresestimationoftransformationparametersbetweentwopoint
patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence, 13(04):376–380,
1991.
[41] TimovonMarcard,RobertoHenschel,MichaelJBlack,BodoRosenhahn,andGerardPons-
Moll. Recoveringaccurate3dhumanposeinthewildusingimusandamovingcamera. In
ProceedingsoftheEuropeanConferenceonComputerVision(ECCV),pages601–617,2018.
[42] Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei,
ZhongfeiQing,ChenWei,BoDai,WayneWu,ChenQian,DahuaLin,ZiweiLiu,andLeiYang.
Synbody: Syntheticdatasetwithlayeredhumanmodelsfor3dhumanperceptionandmodeling.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),pages
20282–20292,October2023.
[43] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human
andcameramotionfromvideosinthewild. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages21222–21232,2023.
[44] RiYu,HwangpilPark,andJeheeLee. Humandynamicsfrommonocularvideowithdynamic
cameramovements. ACMTransactionsonGraphics(TOG),40(6):1–14,2021.
[45] YeYuan,UmarIqbal,PavloMolchanov,KrisKitani,andJanKautz. Glamr: Globalocclusion-
awarehumanmeshrecoverywithdynamiccameras.InProceedingsoftheIEEE/CVFconference
oncomputervisionandpatternrecognition,pages11038–11049,2022.
[46] HongwenZhang,YatingTian,YuxiangZhang,MengchengLi,LiangAn,ZhenanSun,and
YebinLiu. Pymaf-x: Towardswell-alignedfull-bodymodelregressionfrommonocularimages.
IEEETransactionsonPatternAnalysisandMachineIntelligence,2023.
[47] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica
Bogo,andSiyuTang. Egobody: Humanbodyshapeandmotionofinteractingpeoplefrom
head-mounteddevices. InComputerVision–ECCV2022: 17thEuropeanConference,TelAviv,
Israel,October23–27,2022,Proceedings,PartVI,pages180–200.Springer,2022.
[48] YuxiangZhang,LiangAn,TaoYu,XiuLi,KunLi,andYebinLiu. 4dassociationgraphfor
realtime multi-person motion capture using multiple video cameras. In Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages1324–1333,2020.
[49] YuxiaoZhou,MarcHabermann,IkhsanulHabibie,AyushTewari,ChristianTheobalt,andFeng
Xu. Monocularreal-timefullbodycapturewithinter-partcorrelations. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages4811–4822,2021.
14A Overview
Giventhespaceconstraintsinthemainpaper,weprovideadditionalinformationanddetailsinthis
supplementarymaterial: moreresultsvisualizationontheEMDBdatasetandWHAC-A-Moledataset
inSec. B;furtherelaborationontheMotionVelocimeterinSec. C;theaverageinferencespeedin
Sec. D;detailedtrainingproceduresinSec. E,explanationsoftheadaptationsappliedtocamera
spaceEHPSmethodsduringevaluationsinSec. F,andthestep-by-stepcomprehensiveformulations
fortrajectorytransformationsinSec. G.
B ResultsVisualization
InFig. 6,wevisualizethecameraandhumantrajectoriesoftwosequencesfromEMDB2dataset.
Thesequencesconsistof2.7kframesforand1.1kframesrespectively. Notably,WHACdemonstrates
itscapabilitytoaccuratelyrecovertrajectoryandscaleintheworldspace,evenforlengthysequences.
Moreover,inFig. 6b1)andFig. 6b2),WHACeffectivelycapturesthedownwardtrajectoryasthe
depictedhumandescendsstairs.
Besideshumanandcameratrajectoryestimationintheworldspace,wepresentthevisualizationofthe
cameraspaceresultsinFig.7.Thisvisualizationincludesvariousscenariossuchassevereocclusions,
Camera Traj. -GT HumanTraj. -GT
Camera Traj. -WHAC Human Traj. -WHAC
Camera Traj. -DPVO
a1) a2)
Side View Side View
Camera Traj. -GT
Human Traj. -GT
Camera Traj. -WHAC
Human Traj. -WHAC
Camera Traj. -DPVO
b1) b2)
Figure6: VisualizationofworldspaceresultsontheEMDBdataset. a1)andb1)depictcamera
trajectories, while a2) and b2) illustrate human trajectories. Notably, in sequence b, the human
isdescendingstairs,andWHACeffectivelycapturestheglobaltrajectory,indicatingadownward
directionbesidesrecoveringtheabsolutetrajectoryscaleintheworldspace. Thegridsizeintheplots
is2m.
15Figure7: VisualizationofcameraspaceresultsonWHAC-A-Moledataset. Eachsamplecomprises
tworows: thefirstrowdisplaystheoriginalinputframesfromthesequence,whilethesecondrow
overlaystheSMPL-Xresults. ThisvisualizationshowcasesWHAC’sperformanceonchallenging
scenes, including sequences with severe occlusions, intricate human interactions, and dynamic
dancingposes.
closeinteractions,bodycontactbetweensubjects,andchallengingdancingposes,whichserveas
representativecasesfortheWHAC-A-Moledataset. Evenwithoutspecifictrainingorfinetuning
onWHAC-A-Moledataset,WHACdemonstratesstrongabilitiesinhandlingposeestimationand
depthrecoveryincameraspaceforvariousscenarios. However,challengespersistintherecoveryof
multi-humaninteractionsandcontactbetweenbodypartsintheworldspace.
C MotionVelocimeter
WepresentthearchitectureofMotionVelocimeterinFig. 8. TheinputstoMotionVelocimeterconsist
of canonicalized 3D joints, which are derived from SMPL-X meshes and positioned within the
canonicalspaceofthesequence’sinitialframe. Themodel’soutputsarerootvelocitiescorresponding
tothepreviousframewithinthecanonicalspace. Incontrasttothemotionencoderandtrajectory
decoderthattakes2DkeypointsasinputinWHAM[35],3Djointsretainspatialinformationthat
is critical to velocity estimation in the world-frame for absolute scale recovery. Utilizing 3D
jointsenhancesthemodel’sabilitytocaptureandinterpretcomplexmovementpatterns,offeringa
morecomprehensiverepresentationofspatialandtemporaldynamicswithintheworld-grounded
environment.
16MotionVelocimeter
Figure 8: Illustration of MotionVelocimeter module. The inputs are canonicalized 3D joints
regressedfromSMPL-Xmeshes,andtheoutputsarerootvelocitiesinthecanonicalspace.
Table9: Inferencespeedofcoremodulesinframepersecond(FPS).*Denotestheinferencespeed
withoutthereplaceable,off-the-shelfmodules.
Method GLAMR[45] SLAHMR[43] PACE[21] WHAM[35] WHAC WHAC*
FPS 2.4 0.04 2.1 200 165 2500
D Inferencespeed
AsindicatedinTable9,weassesstheinferencespeedofboththecoremodules(excludingreal-time
humandetectionandvisualodometry)followingtheprotocolappliedinWHAM[35],aswellasthe
inferencespeedwithoutoff-the-shelfmodules,whichonlyincludesMotionVelocimeterandthescale
recoveryofhumanandcameratrajectories. Asaregression-basedmethod,theinferencespeedwith
WHAChasnotablyfasterinferencespeedscomparedtooptimization-basedmethods. Itsefficiency
enablesittomeetthereal-timespeedrequirement.
E TrainingDetails
Toenhancetemporalconsistencyandsmoothnessincameraframeresultsontemporaldatasets,we
finetuneSMPLer-X-B[6]. ThisinvolvesincorporatingGRUs[9]betweentheViT-Bbackboneand
theregressionheadsoftheSMPLer-X-Bmodel. Wefinetunethemodelon4×V100GPUsusing
EgoBody[47],3DPW[41],andEMDB[19]datasets. Wesettheminimumlearningrateto1×10−6
for10epochs,withasequencelengthof32framesperbatch.
For training MotionVelocimeter, we freeze the finetuned SMPLer-X-B model and initialize the
learningrateto1×10−3. Weemployastepdecaylearningratescheduler,reducingthelearningrate
byγ =0.1every2epochs. TheMotionVelocimeteristrainedon4×V100GPUsover8epochs.
F AdaptationsforcameraframeEHPSmethods
InTable2andTable3ofthemainpaper,weemployworld-groundedadaptationstocameraframe
methodssuchasOSX[24]andSMPLer-X[6]usingtheapproachdescribedinSec. 3.2. Thismethod
enables the recovery of camera-space root depth, ensuring a fair comparison with other world-
groundedmethods. Withouttheadaptations,cameraframemethodsfacelimitationswhencompared
toworld-groundedmethods,duetothehighT-MPJPEobservedinthecameraframe.
G ComprehensiveFormulations
Giventhespaceconstraints,wepresentonlythefinalizedandgeneralformulationsinSec. 3.3and
Sec. 3.4ofthemainpaper. Here,weincludethecomprehensivestep-by-stepformulationsofthe
trajectorytransformationsforclarity.
17
J-regressor
3D
keypoints
Embedding
GRUs
FC
layers
Root
velocityG.1 Canonicalization
We briefly explain the transformation for canonicalization in Eq. 6 in the main paper. Here we
providethecomprehensiveformulation:
Tcano =Tcano =[Rcano|tcano]=[Rcano|tcano], (15)
w,i w,i w,i
whereTcanoisageneralizedsymbolforthetransformationfromworldcoordinatesystemtocanonical
coordinatesystem,idenotestheithframeinthesequence.
Forthefirst(0th)frameinthesequence:
Rcano =(Rw ×θc )−1 =(θc )−1,tcano =−pw, (16)
w,0 c,0 go,0 go,0 w,0 0
whereRw is0th camerarotationintheworldframeestimatedfromvisualodometry,θc is0th
c,0 go,0
globalorientationestimatedinthecameraspace,−pw isthepelvisjointofJw forthefirstframe.
0 0
Foreveryframeinthesequence:
Rcano =(Rw ×θc )−1 =(θc )−1,tcano =−pw. (17)
w,i c,0 go,0 go,0 w,i i
Weusethepelvistranslation−pw forthecorrespondingframewhileusingthecamerarotationand
i
globalorientationofthefirstframefortheentiresequenceincanonicalization. Thisistoretainthe
humanrotationintheworldcoordinatesystembetweenframesforreliabletrajectoryestimation.
G.2 DeriveCameraTrajectoriesfromHumanTrajectories
InSec. 3.4inthemainpaper,webrieflyexplaintheprocessofderivingcameratrajectoriesTw
c,derived
fromhumantrajectoriesTw inEq. 9. Weappendthefullformulationforthisprocess:
h
Tw =(Tcano)−1×Tcano×(Tc)−1 =Tw ×Tcano×Th =Tw×Th, (18)
c,derived h h cano h c h c
Tw =[Rw|tw], (19)
h h h
Th =[Rh|th]=[θc |tc ]−1,th =−(θc )−1×pc, (20)
c c c go,i h,i c go,i i
whereTw istheoutputhumantrajectoriesintheworldcoordinatesystemfromMotionVelocimeter,
h
Thistheinvertedhumanroottransformationinthecameracoordinatesystem,θc andpcistheith
c go,i i
globalorientationandpelvisincameraspacerespectively.
TofurtherprocessthecamerarotationsRw andcameramovingtrajectoriestw separately,
c,derived c,derived
were-writethetransformationsmentionedaboveintheformofT =[R|t]:
Tw =[Rw |tw ]=[Rw|tw]×[Rh|th]=[RwRh|Rwth+tw], (21)
c,derived c,derived c,derived h h c c h c h c h
Rw =Rw×Rh,tw =Rw×th+tw, (22)
c,derived h c c,derived h c h
wheretw isthehuman-derivedcameratrajectoryintheworldcoordinatesystemwithabsolute
c,derived
scale. ThescalerecoveryisexplainedinEq. 11inthemainpaper.
G.3 DeriveHumanTrajectoriesfromCameraTrajectories
In Sec. 3.4 and Eq. 12, we explain the process of deriving the human trajectories Tw from
h,final
scale-recoveredcameratrajectoriesTw :
c,final
Tw =[Rw|tw ], (23)
c,final c c,final
Tw =[Rw |tw ]=Tw ×Tc, (24)
h,final h,final h,final c,final h
18whereTw isthescale-recoveredVO-estimatedcameratrajectories. WeempiricallyfindthatRw
c,final c
estimatedwithvisualodometryisaccurateandcanbeusedinthefinalcameratrajectoryTw .
c,final
ThescalerecoveryprocessviaUmeyamaalignment[40]fortw isexplainedinEq. 11inthe
c,final
mainpaper. Tcisthehumanroottransformationinthecameracoordinatesystem.
h
By first deriving camera trajectories from MotionVelocimeter-estimated human trajectories, and
followedbyderivinghumantrajectoriesfromscaledVO-estimatedcameratrajectories,weobtain
humantrajectoryTw andcameratrajectoryTw ,bothintheworldcoordinatesystemand
h,final c,final
withabsolutescales.
19