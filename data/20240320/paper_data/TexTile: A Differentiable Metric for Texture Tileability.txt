TexTile: A Differentiable Metric for Texture Tileability
CarlosRodriguez-Pardo1 DanCasas1 ElenaGarces1,2 JorgeLopez-Moreno1,2
1UniversidadReyJuanCarlos,Spain 2SEDDI,Spain
SSIM LPIPS DISTS BRISQUE TexTile
Figure 1. Existing perceptual metrics, commonly used to evaluate texture synthesis algorithms, typically fail to account for tileability.
Suchweaknessisdepictedinthisfigurewhere,foreachcolumn,weshowtiledversionsoftextureswith(top)andwithout(bottom)tiling
artifacts.Foreachcolumn,wehighlightusingsaturatedcolordotsthepreferredimage(i.e.,higherscore)accordingtodifferentmetrics.It
canbeseenthatthereisnocorrelationacrossexistingmethods(i.e.,saturateddotsdistributedovertopandbottomrows),whileourmethod
TexTileconsistentlyprefersseamlesstiledtextures(i.e.,saturatedgreendotsonthebottomforallcolumns).
Abstract evaluateanytileabletexturesynthesismethod,whereasthe
currentmixofexistingmetricsproducesuncorrelatedscores
We introduce TexTile, a novel differentiable metric to whichheavilyhindersprogressinthefield.
quantifythedegreeuponwhichatextureimagecanbecon-
catenatedwithitselfwithoutintroducingrepeatingartifacts 1.Introduction
(i.e., the tileability). Existing methods for tileable texture
The appearance of 3D digital objects plays a fundamental
synthesisfocuson generaltexturequality, butlack explicit
role in the overall realism of a virtual environment. To
analysis of the intrinsic repeatability properties of a tex-
create realistic textures, many strategies have been widely
ture. Incontrast,ourTexTilemetriceffectivelyevaluatesthe
explored, including procedural algorithms [19, 28], scan-
tileablepropertiesofatexture,openingthedoortomorein-
ning [22, 45] and, more recently, text-to-image generative
formedsynthesisandanalysisoftileabletextures. Underthe
pipelines[7‚Äì9]. Amongthedifferentpropertiesthatwewish
hood, TexTileisformulatedasabinaryclassifiercarefully
builtfromalargedatasetoftexturesofdifferentstyles, se-
for the synthesized textures (e.g., photorealism, variety in
detail, high resolution), the ability to seamlessly repeat or
mantics, regularities, and human annotations. Key to our
tileitselfwithoutnoticeableartifacts‚Äì‚Äìitstileability‚Äì‚Äìises-
method is a set of architectural modifications to baseline
peciallyimportantinthefrequentcaseofapplyingatexture
pre-train image classifiers to overcome their shortcomings
toalargesurface. Forexample, whentexturingthefacade
atmeasuringtileability,alongwithacustomdataaugmen-
ofabuildingorafieldofgrass.
tation and training regime aimed at increasing robustness
andaccuracy. WedemonstratethatTexTilecanbeplugged Many methods exist that focus on the specific case of
intodifferentstate-of-the-arttexturesynthesismethods, in- tileabletexturesynthesis[1,11,36,45,48,53,54]. Thishas
cludingdiffusion-basedstrategies,andgeneratetileabletex- beenachieved,forexample,bymanipulatingimageborders
tures while keeping or even improving the overall texture [36],maximizingstationaryimageproperties[45],orcondi-
quality. Furthermore,weshowthatTexTilecanobjectively tioninggenerativemodelsonstructuredpatterns[74]. How-
4202
raM
91
]VC.sc[
1v16921.3042:viXraever,despitesuchsignificantlydiversemethodologiesused Input ImageŒô Œôtiled Inference (Sec 3.3) Model Design (Sec 3.2)
in existing methods, they typically rely on evaluation us-
+
ingcommonmetricsbasedongeneraltexturequalitywhich,
TexTile
unfortunately,donotexplicitlyaccountfortheintrinsicre- ConvNextBlock
peatabilitypropertiesofatexture. Model ‚Ñ≥
ùëìatt ‚àó +
Synthesis ‚ÑíTexTile ùúÜ
Self-Attention
TexTile as Loss Function (Sec 5.1)
To address this shortcoming, we introduce TexTile, a
Figure2. OurmodeltakesasinputatextureimageI, whichwe
novelmetricfortexturetileability. TexTileisadata-driven
tiletoformI ,andreturnsanestimationofitstileability. This
metric that brings two key novel functionalities into tex- tiled
metriccanbeusedasalossfunctionÓà∏ toallowsynthesisal-
turesynthesismethods: first,itcomputesahuman-friendly gorithmstogeneratetileabletextures. OTe uxT ril me odel,Óàπarchitecture
scorethatcapturestheintrinsicrepeatabilityofanytexture; iscomprisedofConvNext[39]andresidualself-attentionblocks.
and second, it provides a differentiable metric that can be
usedasanadditionaldataterminanylearning-basedortest- 2.RelatedWork
timeoptimizationmethodfortileabletexturesynthesis. We
2.1.ImageQualityAssessment
demonstratethatTexTileenablesafactualanalysisofstate-
of-the-artmethodsfortileabletexturesynthesis,whilepre- Image Quality Assessment (IQA) algorithms can be cat-
viousmetricsoftenresultinuncorrelatedevaluations(i.e.,a egorized into three different groups. Reference-based
goodtileabletexturemighthavelowSSIM[66]score,ora IQA methods compare an input and a reference image,
poortileabletexturemighthaveahighSSIM),asillustrated which is the most widely studied strategy for IQA. These
inFigure1. Furthermore,wedemonstratethatTexTilecan methodshavetraditionallyleveragedpixel-wisedifferences
beusedoff-the-shelfasanadditionallossterminstate-of- (e.g., PSNR, ùìÅ or ùìÅ distances) or image statistics (e.g.,
1 2
the-art methods for texture synthesis, including diffusion- SSIM [66] or FSIM [69]) to compute the similarity be-
based models, to output tileable textures while preserving tweentheimages. Neuralreference-basedIQA,whichlever-
orevenimprovingtheoverallimagequality. age the stronger correlation of deep neural networks with
human perception [71], have been also proposed. These
strategieseitherleveragefeaturesfromuntrained[3]orpre-
Underthehood, weformulateTexTileasabinaryclas-
trainedconvolutionalneuralnetworks[20],orusedirectsu-
sifier built using a carefully designed architecture with an
pervision from human judgments, as in LPIPS [71], PIE-
attention-enhanced convolutional network. The convolu-
APP [51], DISTS [14], Si-FID [56], or DreamSIM [18].
tionalfilterscandetectlocaldiscontinuities‚Äìwhicharecom-
These methods introduce powerful and differentiable met-
mon in borders that are not seamlessly tileable‚Äì and can
rics, however, they require a reference image and are thus
deal with images of arbitrary sizes, but they struggle with
notsuitableformeasuringglobalproperties,liketileability.
global understanding to detect artifacts and repeating pat-
terns. Therefore,weintroduceSelf-Attentionlayersintoour Instead of comparing an input and a reference image,
architecture,whichisknowntocaptureaglobalunderstand- distribution-basedIQAmethodscomparestatisticsoftwo
ing of the input. This, combined with a custom data aug- setsofreferenceimagesandgeneratedimages. Thesemeth-
mentationpolicydesignedfortileabilitydetection, enables ods are commonly used to evaluate the perceptual qual-
us to train a novel neural classifier to unleash a new func- ity of generative models, typically using metrics based on
tionalityforthestate-of-the-arttexturesynthesismethods. neural networks activations [6, 27, 57, 67], nearest neigh-
bors[33],spectral[61],orgeometricdistances[31]. While
thesemethodsareusefulforevaluatingtheperformanceof
Insummary,weintroducethefollowingcontributions:
generativemodels,theystruggleaslossfunctions[57],and
alsorequirereferenceimages.
‚Ä¢ Anovellearning-basedmetricfortextureanalysisthatac- Finally, no-reference IQA methods compute the over-
curatelyquantifiestileability. allqualityscoreofaninputimagewithoutrequiringanex-
‚Ä¢ An attention-enhanced convolutional classifier, and a plicit reference. These methods rely on image statistics,
training configuration aimed at maximizing robustness as in BIQI [44] amd BRISQUE [43]; or on training deep
andaccuracy. neuralnetworksonhumanjudgmentsofimagequality,like
‚Ä¢ Adifferentiablelossfunctionwhichcanbepluggedinto HyperIQA [59], MANIQA [68], VCRNet [49], or CLIP-
texturesynthesisalgorithmstogeneratetileabletextures. IQA[64]. Despitecompetitiveresultsinimagequalityas-
‚Ä¢ Open-sourcecodeandtrainedweightsforourmetric. We sessment,thesemethodsdonotincorporatetileabiltyanal-
believethiswillopenthedoortoquantitativebenchmarks ysis. SeamlessGAN [53] leverages the discriminator of a
ontileabletextures,whichiscurrentlynotpossibledueto single-image generative model to find artifacts in the bor-
thelackofaspecificmetricforsuchtask. dersofgeneratedtextures. However,thediscriminatoronlyInput Image
measuresseamlessness,ignoringotherfactorsthatinfluence
Positive
tileabilityand,mostimportantly,itcannotgeneralizetoany Examples
imageoutsideofitssingle-imagedataset.
Our metric is most closely related to no-reference IQA
methods, as it takes a single image as input. However, in ùê¥ùëàùê∫ ùëá‚Üíùêπ
contrast to existing methods, we assess the image quality
based on tilebility instead of general image quality. We Figure3.Fromatileabletexture(left),ourdataaugmentationcan
demonstrate that, when combined with existing IQA met- generatetileable(toprow)andnon-tileable(bottom)variations.
rics, our metric successfully captures overall quality and
tileability. To the best of our knowledge, our metric is the we train on a dataset of textures on a binary classification
firstno-referencetileabilitymetric. task. Ourmodellearnstoclassifybetweentileableandnon-
tileabletextures,bymeansofacomprehensivedataaugmen-
2.2.TileableTextureSynthesis tation policy and custom architecture design choices. We
explainourmodeldesignchoicesinSec.3.2,validatethem
Non-parametric tileable texture synthesis methods gener-
usingablationstudiesinSec.4.1,andexplainthemodelpre-
ate new tileable images by maximizing image stationar-
dictionsinSec.4.3. WeshowexamplesofresultsofTexTile
ity [45], by manipulating the images with border transfor-
as a loss function (Sec. 5.1), as a means of benchmarking
mations using Graphcuts [36], or by patch-based synthe-
imagesynthesisalgorithms(Sec.5.2),andapplicationsfor
sis with histogram-preserving blending [11]. Parametric
alignmentandrepeatingpatterndetection(Sec.5.3).
alternatives typically leverage deep neural networks in di-
verseways. Rodriguez-Pardoetal.[54]lookforrepeating
3.2.ModelDesignandTraining
patternsinimagesusingdeepfeaturesinpretrainedCNNs,
then synthesize tileable images by blending the borders.
NetworkDesign
Tileabilitycanbealsoachievedwithspecificimageparam-
eterizationsandneuralnetworkdesign,asinNeuralCellu- Amodelthatcanmeasuretexturetileabilityshouldhaveat
larAutomata[48], orinthePeriodicSpatialGAN[5]. By least three properties. First, it must be able to detect local
manipulatinglatentspacesinpre-trainedGANs,Seamless- discontinuities, which happen when borders are not seam-
GAN[53]achievetileabilitywithoutspecificmodelmodi- lessly tileable. Second, the model should be able to han-
fications. Tileableimagegenerationhasalsobeenexplored dle images of any dimension or aspect ratios. Finally, it
withthegoalofmaterialcapture,bymeansofmodelscon- should have a global understanding of the image, in order
ditioned on structured patterns [74], or through rolled dif- todetectartifactsandrepeatingpatterns. Thefirsttwoprop-
fusion [62]. Specialized methods have been proposed for ertiescanbeachievedbyfully-convolutionalarchitectures,
tileable vector image generation [1]. For a review on tex- whicharestrongly biasedtowardstextures[26]. However,
turesynthesis,wereferthereadertothesurveyin[2]. problems that require global understanding of images are
Thesemethodstypicallyprovidequantitativeevaluation typically tackled using attention-based Vision Transform-
using reference-based IQA. As mentioned in Section 2.1, ers[23],whicharemorebiasedtowardsshape[46]andare
these metrics measure the perceptual similarity between lessflexibleintermsofinputdimensionality.
generated and input images, however, they do not account We propose an architecture that can benefit from the
fortiling. Tothebestofourknowledge,thereisnoavailable properties of both convolutional and attention-based mod-
metricthatcanbeusedtocomparethesemethodsinterms els. Because of the limited size of our training dataset,
oftileability,hinderingtheprogressofthefieldandlimiting we also want to leverage ImageNet pretraining [12]. We
evaluationtoqualitativeanalyses. Ourmetricaimstosolve thus use a state-of-the-art pretrained ConvNext [39] fully-
thisgap. Beingfullydifferentiable,itcanbeleveragedasa convolutional model. We further introduce Linear Self-
lossfunctiontoenableexistingsynthesisalgorithmstopro- Attentionmodules[65]toallowittolearnglobalpatternsin
duceseamlesslytileableoutputs. theimages,whilekeepingalimitedcomputationalcost. We
designthemasresiduallayersùë•‚Üêùë•+ùúÜùëì att(ùë•),multiplied
3.TexTile byalearnableparameterùúÜ,whichweinitializeat1ùëí‚àí6. We
illustratethismoduleinFigure2. Thisway,wecanmodify
3.1.Introduction
the internal model architecture without disrupting the per-
Our goal is to develop a differentiable no-reference image formanceofthepre-trainedbackboneduringearlytraining
metric that measures texture tileability, that is, a single- iterations. Weonlyusetwoofsuchmodules,whichweplace
imagemetricthatdoesnotrequireasecondimageforcom- onthedeeperlayersoftheConvNextmodel. Previouswork
parison purposes. To achieve this, we leverage a convolu- ontextureestimationandsynthesisalsobenefitfromadding
tional neural network as our differentiable function, which attentiontofully-convolutionalbackbones[21,55].DataAugmentationforTileableTextures
ConvNext ResNet VGG
Weleverageacomprehensivedataaugmentationpolicy,to 0.20 ConvNext-Att ResNet-Att Swin
trainthemodeltodetectrepeatingartifactsinimages.
Ourpolicycontainsseveraloperationswhichwedivide
0.15
into: Tileability-preservingpolicies,whichgeneratevari-
ations of textures without reducing their tileability. These
include global color and gamma changes, random flipping 0.10 ConvNext-Base-Att
(horizontalandvertical),translations,equalization,blursor
noise, or rescaling the images with different scale factors 0.2 0.4 0.6 0.8 1.0 1.2
Trainable Parameters 1e8
across each dimensions. We also introduce an operation
namedUnFold,whichmirrorstheinputimagehorizontally Figure 4. Influence on the neural architecture type and size on
itsquantitativeperformance(Cross-entropyerroronthevalidation
and vertically. Tileability-breaking policies include rota-
dataset).Convolutionalarchitecturesaremarkedwith ,attention-
tions, shears, random cropping, or thin-plate spline warp- ‚ñ†
basedmodelswith‚àô,andourversionsofconvolutionalnetworks
ing[4]. Weapplytileability-preservingpoliciestotileable
withembeddedattentionwith‚úö.
examples, and both kinds of operations to non-tileable ex-
amples. Finally, our last operation is random tiling, by We set ùúÜ = 0.25, which preserves discrimination be-
whichwetilethetexturesarandomnumberoftimes(1-5), tween clearly tileable and non-tileable samples while am-
followedbyarandomrescaling,andrandomcropping. With biguouscasessitcloserto0.5. Thisisastrictlymonotonic
thispolicy,weallowthemodeltodetecttileabilityregard- function,sorelativetileabilityordersaremaintained.
lessonthenumberofrepetitionsintheinputimages.
We introduce two additional data augmentation poli- 3.4.Dataset
cies. With ùê¥ùëàùê∫ , we generate non-tileable textures
ùëá‚Üíùêπ Ourgoalistomakeourmodelrobusttotexturesofanyse-
fromtileabletextures. Wedothisbyapplyingatileability-
mantics,regularity,stochasticity,andhomogeneity. Wealso
breaking operation to a tileable texture. With ùê¥ùëàùê∫ ,
ùêπ‚Üíùêπ‚Ä≤ wantourmodeltoworkwithbothnaturalandsynthetictex-
we create repetition-free texture examples by applying ev-
turemaps. Toachievethis, wecollectadatasetof tileable
ery data augmentation operation except random tiling to
andnon-tileabletexturesfromavarietyofsources.
non-tileable examples, and assign these textures a positive
Wegatheranoveldatasetof4276tileabletextures,com-
tileability label. These two policies are needed to reduce
prised of high-resolution photographs of materials, turned
thedistributionshiftbetweentileableandnon-tileabletrain-
into tileable textures by artist labor. We extend this data
ingdatasets, astheycomefromdifferentsourcesandtheir
withpublicly-availabletileabletextures, including285im-
semantics, feature scales, and contents may differ. With
agesfrom[63],186CC-BYimagesfromJulioSillet,includ-
ùê¥ùëàùê∫ andùê¥ùëàùê∫ ,weforcethemodeltolearnpat-
ùëá‚Üíùêπ ùêπ‚Üíùêπ‚Ä≤ ingbothalbedoandsurfacenormalmaps,and290royalty-
ternsexclusivelyrelatedtotileability,ignoringothertexture
freetexturesfromManyTextures. Theseimages, beingcu-
properties. WeillustratesomeofthesepoliciesinFigure3,
ratedbyhumanexperts,ensurethetileabilityproperty.
includingexampleofUnFoldingonthetoprightimage.
Wealsocreateanadditionaldatasetofnon-tileabletex-
tures, for which examples are comparably easier to ob-
3.3.ModelInference
tain. To this end, we gather 3922 synthetic high-quality
WeillustratetheinferenceprocessinFigure2. Wetilethe from [13], 1187 photographs of a wide variety of tex-
input image I once across each spatial dimension I tiled ‚Üê ture types from [10], 506 facade photographs from [29],
ùë°ùëñùëôùëí(I,(2,2)). Weobservethatthislimitednumberofrepe- 7265 images taken under different illumination conditions
titionsisenoughtoaccuratelydetecttileability. Weusethis from[72],760from[73],and93from[45]. Weextendthis
processbothintextureevaluationandwhenweuseTexTile datasetwith8675imagesfrom[16,32,34,41],and86non-
aslossfunction. I isfedtoourmodelÓàπ,whichoutputs stationary textures from [75]. While we can create nega-
tiled
anunboundedprediction,whichcanbetransformedintothe tive examples from tileable textures using ùê¥ùëàùê∫ , this
ùëá‚Üíùêπ
desired(0,1)rangewithaLogisticfunction. However,do- datasetisimportantforgeneralization. Weuse504tileable
ing this typically leads to predictions very close to 0 or 1, imagesand504non-tileableexamplesforvalidation,andthe
making our metric less useful for comparing different tex- restisleftfortraining. Wehaveapproximately5timesmore
tures. WemitigatethisissuebyintroducingùúÜ, whichcon- non-tileableexamplesthantileableimages,whichmayhin-
trolshowfarthepredictedvaluesarefromtheboundaries: derthetaskoflearninganunbiasedclassifier. Weovercome
thiswithourtraininganddataaugmentationpolicies.
1 InSection4weevaluatetheperformanceofourmodel,
TexTile= (1)
1+exp( ‚àíùúÜ‚ãÖÓàπ(I tiled)) measuringaveragebinarycross-entropyerroronourtestset,
rorrE
noitadilaVConfiguration Error‚Üì Accuracy‚Üë ùêπ ‚Üë AUC‚Üë Distance Error‚Üì Accuracy‚Üë ùêπ ‚Üë AUC‚Üë
1 1
W/outPretraining 0.459 0.808 0.804 0.886 FID[27] 0.568 0.703 0.699 0.764
W/outNAdam[15] 0.096 0.965 0.966 0.992 GS[31] 0.694 0.507 0.533 0.510
W/outLook-Ahead[70] 0.082 0.971 0.971 0.992 MSID[61]0.797 0.582 0.544 0.503
W/outNegativeSamples 0.319 0.905 0.909 0.943 TexTile 0.064 0.982 0.983 0.997
W/outColorAug. 0.119 0.956 0.957 0.992 Table 2. Comparison of our metric with distribution-based dis-
W/outRescales 0.105 0.960 0.968 0.990
tances,onadownstreamclassificationtask.
W/outFlips 0.087 0.972 0.972 0.993
W/outDistortions 0.094 0.966 0.966 0.987
W/outùê¥ùëàùê∫ 0.121 0.960 0.961 0.990 DataAugmentationandTrainingConfiguration
ùëá‚Üíùêπ
W/outùê¥ùëàùê∫ 0.087 0.969 0.966 0.991
ùêπ‚Üíùêπ‚Ä≤ InTable1,weshowanablationstudyofourdataaugmenta-
W/outUnFold 0.082 0.970 0.971 0.992
tionandoptimizationsetups. Fromourfinalmodelconfigu-
FinalModel 0.064 0.982 0.983 0.997
Table1.Ablationstudyondifferentconfigurationsofmodeltrain- ration,weremovedifferentcomponentstoitstrainingpolicy
ingconfigurationsanddataaugmentationpolicies. tostudytheirimpactongeneralization. Wereportdifferent
classification metrics measured on our test set, which con-
aswellasclassificationmetrics,likeaccuracy,ùêπ 1-Scoreand
tainsabalancednumberofpositiveandnegativeexamples.
Area Under the Curve (AUC). We provide more details of
First,weshowthatthemodelstronglybenefitsfrompre-
ourdatasetsinthesupplementarymaterial.
training on ImageNet. The model performance can be en-
hanced by introducing Nesterov momentum [15] into the
optimizer, and further with Lookahead training [70]. We
3.5.ImplementationDetails
observe that these results are consistent across architec-
We train our models for 100 epochs using NAdam [15] tures, and that other optimizers, such as RAdam [37] or
Lookahead [70], Automatic Gradient Scaling and Mixed AdamW[40],performedworsethanNAdamorAdam.
Precision Training [42], with an initial learning rate of Regarding data augmentation, we first tested a model
0.002, halved every 33 epochs. We use PyTorch [50] for trainedwithoutnegativeexamples,relyingonsyntheticnon-
training, and Kornia [52] for data augmentation. We use tileable textures from tileable ground truths. However,
batchsizesof24samples,composedofbalancednumberof this yielded limited generalization. Traditional augmenta-
positiveandnegativeexamples. Thisprocesstakesapprox- tions(color,geometry,noise,blurs,elastictransformations)
imately6hoursonasingleNvidiaRTX3060GPU.Weuse provided incremental improvements. Our custom policies,
images of (384,384) pixels for training and (512,512) for which generate negative samples from tileable images and
inference. HyperparametersaretunedusingBayesianopti- vice versa, further enhanced model performance. By in-
mizationonavalidationdatasetcontaining1000images. troducing random unfolding, we achieve small gains. This
comprehensive data augmentation policy mitigates model
and dataset limitations and makes TexTile more robust to
4.Evaluation importantfactorslikecolorvariations,sharpness,orscales.
4.1.AblationStudy 4.2.ComparisonwithDistribution-BasedMetrics
Network Architecture Design In Figure 4, we show Inthisexperiment,wecompareourmetricwithoff-the-shelf
the impact of the neural network architecture design on distribution-basedmetrics. Toachievethis,wecomputethe
generalization performance. We evaluate different fully- latentfeaturesofbothourtileableandnon-tileabletraining
convolutionalbackbones,includingResNet[24],VGG[58] datasets, using 2x2 tilings as in our setup. Then, for ran-
and ConvNext [39]; as well as transformer-based Swin domlyselectedsubsetsfromourtestset, wecomparetheir
V2 [38], on different model sizes. We also show the re- latentfeaturestothoseofbothtrainingdatasets. Finally,we
sultsofourvariationsofthefully-convolutionalbackbones, classifythesetaccordingtothewhichofbothdistributions
where we introduce linear Self-Attention [65] modules in is closest. We show the results on Table 2, where we find
thelastlayersofthemodels. Everynetworkispre-trained that these metrics fail to capture important characteristics
onImageNet[12],thenfinetunedinourtask,andevaluated thatinfluencethetileabilityoftextures. Directlysupervising
onavalidationdataset. Asshown,largernetworkstypically fortileabilityisunsurprisinglymoreeffective. Interestingly,
performbetter,withtheexceptionofSwin,whichinterest- theGS[31]andMSID[61]metricsperformonlymarginally
ingly benefits from fewer parameters. Further, ConvNexts betterthanrandom,whereasFID[27]bettercapturesthedif-
strongly outperform ResNets and VGGs. By introducing ferencesbetweenthedistributions.
self-attentionintothesefully-convolutionalmodels,wesig-
4.3.QualitativeEvaluation
nificantly improve their generalization capabilities. In the
restofourexperiments,wewilluseourcustomConvNext- WeleverageAxion-BasedClass-ActivationMappings[17]
Base-Attmodel,asitachievesthelowesterroroverall. tovisualizewhichfeaturesaremostrelevanttoourmodel.
gniniarT
noitatnemguAataD0.013 0.059 0.089 0.217 0.531 0.572 0.652 0.819 0.821 0.969
Figure5.Ontop,texturessamples(tiled2√ó2)withincreasingpredictedtileability.Belowthem,modelsaliencymapsandTexTilevalues.
InFigure5, weshowsaliencymapsandpredictedTexTile
valuesforafewrepresentativetextureswithincreasingde-
grees of tileability. As shown on the first two examples,
our model predicts low tileability values for textures with-
outseamlessborders. Inthenextthreeexamples,whichare
seamlessonlyononeoftheiraxes,themodeloutputshigher Figure6. Imageoutpaintingfortileabletexturesynthesis. Onthe
tileability values. On the last five examples, which are all left,non-tileableinputimageswiththeareatobeoutpaintedina
seamlessly tileable, the model is leveraging other features, solid color; on their sides, outpainted results, obtained by maxi-
likeunevenshadings,unusualartifacts,orrepeatingobjects. mizingtileability,shownina2x2tilecomposition.
Theseresultsshowthatourmodelcanexploitpatternsother
Importantly, this can be achieved without re-training the
thanborderdiscontinuityforitsprediction,andthatitcanin-
generative models. Further implementation details and re-
tegratedistantinformationforfindingrepeatingelements.
sultsareincludedinthesupplementarymaterial.
5.Results
5.2.BenchmarkingTextureSynthesisAlgorithms
5.1.TexTileasaLossFunction
In Table 3, we show a quantitative comparison between
Because TexTile is a fully-differentiable metric, it can be different texture synthesis algorithms and generative mod-
leveraged as a loss function for synthesizing tileable tex- els, on the 14-texture dataset used in [53], using reference
tures. Weexplorethisontwodifferenttypesofalgorithms. andno-referencemetrics. For[25,47], weshowresultsof
First, we extend an optimization-based neural texture theirbaselinemethodsandourmodificationsthatgenerate
synthesis algorithm [25] to generate tileable textures. To tileable textures. The qualitative results on the complete
do so, we simply optimize a joint loss function Óà∏ = datasetispresentinthesupplementarymaterial.
ùúÜ styleÓà∏
style
+ùúÜ TexTileÓà∏ TexTile, where ùúÜ
style
and ùúÜ
TexTile
con- Besides, we observe that powerful generative models
trol the weight of the each component of the loss and are like [47, 53] do not consistently beat non-parametric alter-
selectedempiricallytoùúÜ style = ùúÜ TexTile = 1. Weobserved nativeslike[11,36]acrosseitherreferenceorno-reference
little sensitivity to these weightings as long as they are on metrics. In terms of tileability, introducing Textile as a
thesameorderofmagnitude. Weshowend-to-endsynthe- lossfunctionto[25,47]notonlysignificantlyimprovesthe
sisresultsinFigure8. Wecanalsogeneratetileabletextures tileability of their outputs but it does it without reducing
inthisfashionbyoptimizingonlythetexturebordersusing their perceptual quality. Unsurprisingly, methods that per-
outpainting,asweillustrateinFigure6. formborderblending[11,54]achievelowertileabilityval-
Relatedly, we can also extend single image diffusion ues across different metrics, than methods that synthesize
models so they can generate tileable textures. While the thetexturesinamoreholisticway,like[5,45,48,53]. These
goalofthesemethodsismoregeneralimageorvideosyn- resultsconfirmthattherearemoreconstituentfactorsintex-
thesis, we leverage them as powerful texture synthesis al- turetileabilitythansimplyseamlessborders.
gorithms. Todoso,weuseSinFusionmodels[47]without A correlation matrix between these metrics is shown in
anymodificationsinthetrainingprocess. Duringinference, Figure 7. Learned reference metrics (LPIPS, DISTS and
aftereachdiffusionstep, weperformasingleoptimization PieAPP) correlate strongly between each other but poorly
steptothenoisyimageonthedirectionthatmaximizesTex- with other measures. Si-FiD and SSIM are not closely re-
Tile. WeshowqualitativeresultsinFigure8. latedwithanyothermetric,whileTexTileisslightlycorre-
Withthesesimplemodifications,wecantransformthese latedwithBRISQUEandCLIP-IQA.Aswealsoillustrate
methods ‚Äìand potentially any texture generative model‚Äì inFigure1, thereisnoconsensusacrossmetricsonwhich
into tileable texture synthesis algorithms, without signifi- algorithmisoutperformingtheothers,showingthatpercep-
cantlossintheperceptualqualityofthegeneratedtextures. tualevaluationfortexturesynthesisremainsachallenge.Reference-BasedMetrics NoReference
SSIM[66]‚Üë Si-FID[56]‚Üì LPIPS[71]‚Üì DISTS[14]‚Üì PieAPP[51]‚Üì BRISQUE[43]‚Üì CLIP-IQA[64]‚Üë TexTile‚Üë
Deloitetal.[11] 0.138 1.707 0.595 0.372 2.289 49.43 0.407 0.639
Lietal.[36] 0.149 0.981 0.559 0.341 1.672 45.84 0.437 0.707
Rodriguez-Pardoetal.[54] 0.171 0.969 0.594 0.361 1.890 43.14 0.661 0.403
Bergmannetal.[5] 0.148 0.981 0.579 0.359 1.789 44.13 0.638 0.675
Niklassonetal.[48] 0.146 0.819 0.623 0.446 2.292 53.99 0.435 0.731
Rodriguez-Pardoetal.[53] 0.166 0.694 0.540 0.336 1.542 51.10 0.452 0.729
Heitzetal.[25]w/outTexTile 0.140 1.741 0.575 0.329 1.687 49.05 0.376 0.431
Heitzetal.[25]withTexTile 0.152 1.764 0.555 0.327 1.653 48.49 0.392 0.781
Nikankinetal.[47]w/outTexTile 0.172 1.314 0.591 0.386 1.963 55.96 0.408 0.388
Nikankinetal.[47]withTexTile 0.189 1.415 0.569 0.387 1.926 56.99 0.396 0.798
Table3.Quantitativeevaluationbetweendifferenttileabletexturesynthesisalgorithmsacrossavarietyofmetrics,includingreferenceand
no-referencemeasures,andTexTile.Besttworesultsforeachcolumnsaremarkedinbold.
PreviousMethods NeuralTextureSynthesis[25] DiffusionModels[47]
Input Blending[11] SeamlessGAN[53] Self-Org[48] W/outTexTile WithTexTile W/outTexTile WithTexTile
Figure8.Comparisonsofdifferenttexturesynthesisalgorithms.Ontheleftmostcolumn,weshowtheinputtexture,ontheright,2x2tilings
oftheoutputsofdifferentmethods.For[25]and[47],weshowtheoriginalversionsourthemodificationsfortileabletexturesynthesis.
SSIM 1.00 -0.22 -0.20 0.14 -0.15 0.42 0.10 0.21 axes. Similarly, we can find the optimal rotation angle ùúÉ
Si-FID -0.22 1.00 -0.04 -0.29 0.08 0.11 -0.53 -0.10 foranimageIbyargmax TexTile(Rotate(I,ùúÉ)),maximiz-
LPIPS -0.20 -0.04 1.00 0.81 0.88 0.11 0.17 -0.27 ùúÉ
ingthetileabilityoftheinputimage. InFigure9,weshow
DISTS 0.14 -0.29 0.81 1.00 0.83 0.52 -0.01 0.24
PieAPP -0.15 0.08 0.88 0.83 1.00 0.31 -0.06 0.03 the scores of our metric, for the same image, to which we
BRISQUE 0.42 0.11 0.11 0.52 0.31 1.00 -0.71 0.55
apply different rotation angles. Our metric provides high
CLIP-IQA 0.10 -0.53 0.17 -0.01 -0.06 -0.71 1.00 -0.40
TexTile 0.21 -0.10 -0.27 0.24 0.03 0.55 -0.40 1.00 scores for rotation angles which provide seamless borders,
SSIM Si-FID LPIPS DISTS PieAPP BRISQUECLIP-IQA TexTile andlowervaluesformisalignments. Interestingly,thereisa
Figure7.Pearsoncorrelationmatrixbetweendifferentmetrics. smallpeakat¬±35‚ó¶,inwhichthelinesintheimageconnect
with each other, but their colors do not match in the bor-
ders. TheseresultsindicatethatTexTilenotonlymeasures
5.3.AlignmentandRepeatingPatternDetection
seamlessness,butalsocolorcontinuity,andaxisalignment.
Besidesbenchmarkingandenablingtileabletexturesynthe- WecanalsoleverageTexTiletocomputethesizeofthe
sis,ourmetricenablesadditionalapplications. repeating pattern in images. Given an axis-aligned im-
PreviousworkontextureanalysisusedtheRadonTrans- age I, we can find the size ‚Ñé,ùë§ of the repeating pattern in
form[30,54]forautomaticallyaligningimageswiththeùë•ùë¶- theimagebyfindingthecropthatmaximizesitstileability:True label: 0 True label: 1
90¬∞
135¬∞ 0.75 45¬∞
0.50
180¬∞ 0¬∞ 195¬∫ 35¬∫
TexTile: 0.569 TexTile: 0.482
Figure11. Examplesofmisclassificationsdonebyourmodel. On
225¬∞ 315¬∞ thetop,groundtruthlabels,onthebottom,thepredictedTexTile
270¬∞ value.Fortheexampleontheleft,weshowaninsetofthecentral
90¬∫ 135¬∫
cropofthetexture,tohighlightthatitisnotseamlesslytileable.
Figure 9. On the left, TexTile under different rotation angles.
adequatelysynthesizetexturesthatmatchtheappearanceof
Ontheright, samplesofrotatedimagesondifferentlocalpeaks.
Scoresbelow0.5,astheredpeak,depictnon-tileabletextureswith theinput,addingTexTilehelpsreducediscontinuitiesinthe
highly-noticeableartifacts. Alocalmaximaforanon-tileabletex- bordersbutwillnotimproveitsperceptualquality,ascanbe
ture is found at ¬±35‚ó¶, in blue, while highly tileable textures are seeninthetexturesgeneratedwith[25]inFigure8.
foundatthegrayandgreeninsets.
250 1.0 6.Conclusions
0.8
200 WehavepresentedTexTile,thefirstdifferentiablemetricfor
0.6
texture tileability. While it is trained on a simple classifi-
150
0.4 cation task, we design custom data augmentation, training
100 0.2 regimes, and neural architectures, all specifically tailored
0.0 to accurately measure tileability. We validated our design
100 150 200 250
Figure10. Ontheleft,inputimage. Inthemiddle,TexTilevalues choiceswithcomprehensiveablationstudies,andleveraged
fordifferentcropsizes. Theoptimalcropishighlightedonboth saliency maps for model understanding. We showed dif-
images with a green inset. On the right, the optimal crop, tiled ferent applications of our differentiable metric, including
manytimesforvisualization. benchmarkingtexturesynthesisalgorithms,detectingrepe-
argmax TexTile(Crop(I,(‚Ñé,ùë§))). We show a result in titionsandmisalignmentinimages,andtransformingimage
‚Ñé,ùë§
Figure10,wherethisalgorithmfindstheoptimalcropsizeat generativemodelsintotileabletexturesynthesisalgorithms.
theminimumrepeatablepattern,aswellasthreelowerpeaks Wewillprovidecodeandmodelweights.
atdifferentdiscretescalefactorsofthiscropsize. Notethat
we limit the crops to ‚Ñé,ùë§ ‚â• 64 and perform the crop at
LimitationsandFutureWork Wecouldextendourwork
thecenteroftheimage. Previouswork[35,54]foundthese
in several ways. While our method accurately measures
repeating patterns in the activations of pre-trained CNNs.
tileability in textures, it is does not quantify their percep-
Because internal neural activations operate at lower reso-
tualquality,limitingitsscope. Combiningperceptualmet-
lutions that those of the original image, these methods are
rics with TexTile, to measure both perceptual quality and
limitedinprecision. Ourmethod,incontrast,operatesatthe
tileability, is an important research avenue for a more in-
resolutionoftheoriginalimageandmaythusbemorepre-
tegrated analysis of texture quality. Besides, our model is
cise. Moreresultsonthesetwoapplicationsarepresentin
pre-trained on a ImageNet, then fine-tuned on a manually-
thesupplementarymaterial.
curateddataset,andthusmayinheritbiases. Whilewebe-
lievethedatasetsweusedwerecomprehensive,itispossible
5.4.FailureCases
thatsometypeoftextureisunderrepresentedandthemodel
As shown in Table 1, our model predictions are accurate, maynotperformaccurately. Usingsyntheticdata[18]may
however, some errors occur. We show some examples of help alleviate this issue. Finally, there is no solid under-
misclassifiedtexturesinFigure11. Ontheleft,weshowa standingofhumanperceptionoftexturerepetitiveness[60],
texture labeled as non-tileable in our test dataset, that our and, whilehumanperceptualvalidationisoutofthescope
modelpredictsastileabledespiteitshowingdiscontinuities of this work, its apparent higher correlation with TexTile,
in the borders. On the right, we show a texture that is la- mightaddnewinsightstotheelementsidentifiedsofar.
beledastileable,whichourmodelclassifiesasnon-tileable.
Both examples are edge cases and highlight the ambiguity Acknowledgments This publication is part of the project TaiLOR,
inwhatconstitutesatileabletexture. Besides,whenusedas CPP2021-008842fundedbyMCIN/AEI/10.13039/501100011033andtheNextGen-
alossforsynthesismodels,TexTilecannotcompensatefor erationEU/PRTRprograms. ElenaGarceswaspartiallysupportedbyaJuandela
thelimitationsofthegenerativebackbone. Ifamodelcannot Cierva-IncorporacionFellowship(IJC2020-044192-I).References [17] Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo,
Yinghui Gao, and Biao Li. Axiom-based grad-cam: To-
[1] Noam Aigerman and Thibault Groueix. Generative escher
wardsaccuratevisualizationandexplanationofcnns. arXiv
meshes. arXivpreprintarXiv:2309.14564,2023. 1,3
preprintarXiv:2008.02312,2020. 5
[2] Adib Akl, Charles Yaacoub, Marc Donias, Jean-Pierre [18] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy
Da Costa, and Christian Germain. A survey of exemplar- Chai,RichardZhang,TaliDekel,andPhillipIsola. Dream-
basedtexturesynthesismethods.ComputerVisionandImage sim:Learningnewdimensionsofhumanvisualsimilarityus-
Understanding,172:12‚Äì24,2018. 3 ingsyntheticdata. arXivpreprintarXiv:2306.09344,2023.
[3] DanAmirandYairWeiss. Understandingandsimplifying 2,8
perceptual distances. In Proceedings of the IEEE/CVF In- [19] Bruno Galerne, Ares Lagae, Sylvain Lefebvre, and George
ternational Conference on Computer Vision, pages 12226‚Äì Drettakis. Gabornoisebyexample. ACMTransactionson
12235,2021. 2 Graphics(TOG),31(4):73:1‚Äì73:9,2012. 1
[4] Adrien Bartoli, Mathieu Perriollat, and Sylvie Chambon. [20] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.
Generalized thin-plate spline warps. International Journal A neural algorithm of artistic style. arXiv preprint
ofComputerVision,88:85‚Äì110,2010. 4 arXiv:1508.06576,2015. 2
[5] UrsBergmann,NikolayJetchev,andRolandVollgraf.Learn- [21] Shouchang Guo, Valentin Deschaintre, Douglas Noll, and
ing texture manifolds with the periodic spatial gan. pages ArthurRoullier. U-attentiontotextures: hierarchicalhour-
469‚Äì477,2017. 3,6,7 glass vision transformer for universal texture synthesis. In
[6] Miko≈ÇajBi≈Ñkowski,DanicaJSutherland,MichaelArbel,and ProceedingsoftheACMSIGGRAPHEuropeanConference
ArthurGretton. Demystifyingmmdgans. InInternational
onVisualMediaProduction,pages1‚Äì10,2022. 3
[22] YuGuo,CameronSmith,Milo≈°Ha≈°an,KalyanSunkavalli,
ConferenceonLearningRepresentations,2018. 2
and Shuang Zhao. Materialgan: reflectance capture using
[7] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp,
agenerativesvbrdfmodel. ACMTransactionsonGraphics
andKangxueYin. Texfusion: Synthesizing3dtextureswith
(TOG),39(6):1‚Äì13,2020. 1
text-guided imagediffusion models. In Proceedings ofthe
[23] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,
IEEE/CVF International Conference on Computer Vision,
JianyuanGuo,ZhenhuaLiu,YehuiTang,AnXiao,Chunjing
pages4169‚Äì4181,2023. 1
Xu,YixingXu,etal. Asurveyonvisiontransformer. IEEE
[8] DanCasasandMarcComino-Trinidad. SMPLitex: AGen- TransactionsonPatternAnalysisandMachineIntelligence,
erativeModelandDatasetfor3DHumanTextureEstimation 45(1):87‚Äì110,2022. 3
from Single Image. In British Machine Vision Conference [24] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
(BMVC),2023.
Deepresiduallearningforimagerecognition.InProceedings
[9] DaveZhenyuChen,YawarSiddiqui,Hsin-YingLee,Sergey oftheIEEE/CVFInternationalConferenceonComputerVi-
Tulyakov,andMatthiasNie√üner.Text2Tex:Text-drivenTex- sion,pages770‚Äì778,2016. 5
tureSynthesisviaDiffusionModels. 2023. 1 [25] EricHeitz,KennethVanhoey,ThomasChambon,andLau-
[10] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. rentBelcour.Aslicedwassersteinlossforneuraltexturesyn-
Vedaldi. Describing textures in the wild. In Proceedings thesis. InProceedingsoftheIEEE/CVFInternationalCon-
oftheIEEE/CVFInternationalConferenceonComputerVi- ferenceonComputerVision,pages9412‚Äì9420,2021. 6,7,
sion,2014. 4 8
[11] ThomasDeliotandEricHeitz.Proceduralstochastictextures [26] KatherineHermann,TingChen,andSimonKornblith. The
bytilingandblending. GPUZen,2,2019. 1,3,6,7 originsandprevalenceoftexturebiasinconvolutionalneural
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, networks. AdvancesinNeuralInformationProcessingSys-
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
tems,33:19000‚Äì19015,2020. 3
[27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
agedatabase.InProceedingsoftheIEEE/CVFInternational
BernhardNessler, andSeppHochreiter. Ganstrainedbya
ConferenceonComputerVision,pages248‚Äì255.Ieee,2009.
twotime-scaleupdateruleconvergetoalocalnashequilib-
3,5
rium. AdvancesinNeuralInformationProcessingSystems,
[13] ValentinDeschaintre,MiikaAittala,FredoDurand,George
30,2017. 2,5
Drettakis,andAdrienBousseau.Single-imagesvbrdfcapture
[28] Yiwei Hu, Julie Dorsey, and Holly Rushmeier. A novel
witharendering-awaredeepnetwork. ACMTransactionson
framework for inverse procedural texture modeling. ACM
Graphics(TOG),37(4):1‚Äì15,2018. 4
TransactionsonGraphics(TOG),38(6):1‚Äì14,2019. 1
[14] KeyanDing,KedeMa,ShiqiWang,andEeroPSimoncelli.
[29] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image quality assessment: Unifying structure and texture
Efros.Image-to-imagetranslationwithconditionaladversar-
similarity. IEEETransactionsonPatternAnalysisandMa- ialnetworks. InProceedingsoftheIEEE/CVFInternational
chineIntelligence,44(5):2567‚Äì2581,2020. 2,7
ConferenceonComputerVision,pages1125‚Äì1134,2017. 4
[15] Timothy Dozat. Incorporating nesterov momentum into [30] Kourosh Jafari-Khouzani and Hamid Soltanian-Zadeh.
adam. 2016. 5 Radontransformorientationestimationforrotationinvariant
[16] Mario Fritz, E. Hayman, B. Caputo, and J. Eklundh. The textureanalysis. IEEETransactionsonPatternAnalysisand
kth-tipsdatabase. 2004. 4 MachineIntelligence,27(6):1004‚Äì1008,2005. 7[31] ValentinKhrulkovandIvanOseledets. Geometryscore: A [45] JoepMoritz,StuartJames,TomSFHaines,TobiasRitschel,
method for comparing generative adversarial networks. In andTimWeyrich. Texturestationarization: Turningphotos
InternationalConferenceonMachineLearning,pages2621‚Äì intotileabletextures. InComputerGraphicsForum,pages
2629.PMLR,2018. 2,5 177‚Äì188.WileyOnlineLibrary,2017. 1,3,4,6
[32] GustafKylberg. Thekylbergtexturedatasetv.1.0. External [46] Muhammad Muzammal Naseer, Kanchana Ranasinghe,
report(Blueseries)35,CentreforImageAnalysis,Swedish SalmanHKhan,MunawarHayat,FahadShahbazKhan,and
UniversityofAgriculturalSciencesandUppsalaUniversity, Ming-HsuanYang.Intriguingpropertiesofvisiontransform-
Uppsala,Sweden,2011. 4 ers.AdvancesinNeuralInformationProcessingSystems,34:
[33] TuomasKynk√§√§nniemi, TeroKarras, SamuliLaine, Jaakko 23296‚Äì23308,2021. 3
Lehtinen,andTimoAila.Improvedprecisionandrecallmet- [47] Yaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion:
ricforassessinggenerativemodels. AdvancesinNeuralIn- Trainingdiffusionmodelsonasingleimageorvideo. InIn-
formationProcessingSystems,32,2019. 2 ternationalConferenceonMachineLearning.PMLR,2023.
[34] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. A 6,7
sparsetexturerepresentationusinglocalaffineregions.IEEE [48] EyvindNiklasson,AlexanderMordvintsev,EttoreRandazzo,
TransactionsonPatternAnalysisandMachineIntelligence, andMichaelLevin. Self-organisingtextures. Distill,2021.
27(8):1265‚Äì1278,2005. 4 https://distill.pub/selforg/2021/textures. 1,3,6,7
[49] ZhaoqingPan,FengYuan,JianjunLei,YumingFang,Xiao
[35] Louis Lettry, Michal Perdoch, Kenneth Vanhoey, and Luc
Shao, and Sam Kwong. Vcrnet: Visual compensation
VanGool. Repeatedpatterndetectionusingcnnactivations.
restoration network for no-reference image quality assess-
InIEEEWinterConferenceonApplicationsofComputerVi-
sion,pages47‚Äì55.IEEE,2017. 8 ment. IEEE Transactions on Image Processing, 31:1613‚Äì
1627,2022. 2
[36] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi,
[50] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
KalyanSunkavalli,andManmohanChandraker.Inverseren-
Chanan,EdwardYang,ZacharyDeVito,ZemingLin,Alban
deringforcomplexindoorscenes: Shape,spatially-varying
Desmaison,LucaAntiga,andAdamLerer.Automaticdiffer-
lighting and svbrdf from a single image. In Proceedings
entiationinpytorch. 2017. 5
oftheIEEE/CVFInternationalConferenceonComputerVi-
[51] Ekta Prashnani, Hong Cai, Yasamin Mostofi, and Pradeep
sion,pages2475‚Äì2484,2020. 1,3,6,7
Sen. Pieapp: Perceptual image-error assessment through
[37] LiyuanLiu, HaomingJiang, PengchengHe, WeizhuChen,
pairwisepreference. InProceedingsoftheIEEE/CVFInter-
XiaodongLiu,JianfengGao,andJiaweiHan. Onthevari-
nationalConferenceonComputerVision,pages1808‚Äì1817,
anceoftheadaptivelearningrateandbeyond.arXivpreprint
2018. 2,7
arXiv:1908.03265,2019. 5
[52] EdgarRiba,DmytroMishkin,DanielPonsa,EthanRublee,
[38] ZeLiu,HanHu,YutongLin,ZhuliangYao,ZhendaXie,Yix-
and Gary Bradski. Kornia: an open source differentiable
uanWei, JiaNing, YueCao, ZhengZhang, LiDong, etal.
computer visionlibrary for pytorch. InProceedings ofthe
Swintransformerv2: Scalingupcapacityandresolution. In
IEEE/CVFWinterConferenceonApplicationsofComputer
ProceedingsoftheIEEEConferenceonComputerVisionand
Vision,pages3674‚Äì3683,2020. 5
PatternRecognition,pages12009‚Äì12019,2022. 5
[53] Carlos Rodriguez-Pardo and Elena Garces. Seamlessgan:
[39] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht-
Self-supervised synthesis of tileable texture maps. IEEE
enhofer,TrevorDarrell,andSainingXie. Aconvnetforthe
TransactionsonVisualizationandComputerGraphics,2022.
2020s. InProceedingsoftheIEEE/CVFInternationalCon-
1,2,3,6,7
ferenceonComputerVision,pages11976‚Äì11986,2022. 2,
[54] CarlosRodriguez-Pardo, SergioSuja, DavidPascual, Jorge
3,5
Lopez-Moreno, and Elena Garces. Automatic extraction
[40] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
andsynthesisofregularrepeatablepatterns. Computers&
regularization. arXivpreprintarXiv:1711.05101,2017. 5
Graphics,83:33‚Äì41,2019. 1,3,6,7,8
[41] P. Mallikarjuna, Alireza Tavakoli Targhi, Mario Fritz, E. [55] Carlos Rodriguez-Pardo, Henar Dominguez-Elvira, David
Hayman,B.Caputo,andJ.Eklundh.Thekth-tips2database. Pascual-Hernandez,andElenaGarces. Umat: Uncertainty-
2006. 4 aware single image high resolution material capture. Pro-
[42] PauliusMicikevicius,SharanNarang,JonahAlben,Gregory
ceedingsoftheIEEE/CVFInternationalConferenceonCom-
Diamos,ErichElsen,DavidGarcia,BorisGinsburg,Michael puterVision,2023. 3
Houston,OleksiiKuchaiev,GaneshVenkatesh,etal. Mixed [56] TamarRottShaham,TaliDekel,andTomerMichaeli. Sin-
precisiontraining. InInternationalConferenceonLearning GAN: Learning a Generative Model from a Single Natural
Representations,2018. 5 Image. InProceedingsoftheIEEE/CVFInternationalCon-
[43] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad ferenceonComputerVision,2019. 2,7
Bovik. No-referenceimagequalityassessmentinthespatial [57] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
domain. IEEE Transactions on Image Processing, 21(12): Cheung,AlecRadford,andXiChen. Improvedtechniques
4695‚Äì4708,2012. 2,7 fortraininggans. AdvancesinNeuralInformationProcess-
[44] Anush Krishna Moorthy and Alan Conrad Bovik. A two- ingSystems,29,2016. 2
stepframeworkforconstructingblindimagequalityindices. [58] KSimonyanandAZisserman.Verydeepconvolutionalnet-
IEEESignalProcessingLetters,17(5):513‚Äì516,2010. 2 works for large-scale image recognition. In InternationalConferenceonLearningRepresentations.Computationaland deepfeaturesasaperceptualmetric. InProceedingsofthe
BiologicalLearningSociety,2015. 5 IEEE/CVF International Conference on Computer Vision,
[59] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, pages586‚Äì595,2018. 2,7
JinqiuSun,andYanningZhang. Blindlyassessimagequal- [72] Xilong Zhou and Nima Khademi Kalantari. Adversarial
ityinthewildguidedbyaself-adaptivehypernetwork. In single-imagesvbrdfestimationwithhybridtraining. Com-
ProceedingsoftheIEEE/CVFConferenceonComputerVi- puterGraphicsForum,2021. 4
sionandPatternRecognition,pages3667‚Äì3676,2020. 2 [73] Xilong Zhou and Nima Khademi Kalantari. Look-ahead
[60] Hua-Chun Sun, David St-Amand, Curtis L Baker Jr, and trainingwithlearnedreflectancelossforsingle-imagesvbrdf
Frederick AA Kingdom. Visual perception of texture reg- estimation. ACM Transactions on Graphics (TOG), 41(6),
ularity: Conjoint measurements and a wavelet response- 2022. 4
distribution model. PLoS Computational Biology, 17(10): [74] XilongZhou,MilosHasan,ValentinDeschaintre,PaulGuer-
e1008802,2021. 8 rero,KalyanSunkavalli,andNimaKhademiKalantari. Ti-
[61] AntonTsitsulin,MarinaMunkhoeva,DavideMottin,Panagi- legen: Tileable, controllable material generation and cap-
otisKarras,AlexBronstein,IvanOseledets,andEmmanuel ture. In SIGGRAPH Asia 2022 Conference Papers, pages
Mueller.Theshapeofdata:Intrinsicdistancefordatadistri- 1‚Äì9,2022. 1,3
butions. InInternationalConferenceonLearningRepresen- [75] YangZhou, ZhenZhu, XiangBai, DaniLischinski, Daniel
tations,2019. 2,5 Cohen-Or,andHuiHuang. Non-stationarytexturesynthesis
[62] GiuseppeVecchio,RosalieMartin,ArthurRoullier,Adrien by adversarial expansion. ACM Transactions on Graphics
Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy (TOG),37(4):1‚Äì13,2018. 4
Boubekeur. Controlmat: A controlled generative approach
tomaterialcapture. arXivpreprintarXiv:2309.01700,2023.
3
[63] Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa,
AngelX.Chang,andManolisSavva. Plan2scene: Convert-
ingfloorplansto3dscenes.InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages10733‚Äì
10742,2021. 4
[64] JianyiWang,KelvinCKChan,andChenChangeLoy. Ex-
ploringclipforassessingthelookandfeelofimages.InPro-
ceedings of the AAAI Conference on Artificial Intelligence,
pages2555‚Äì2563,2023. 2,7
[65] SinongWang,BelindaZLi,MadianKhabsa,HanFang,and
HaoMa. Linformer: Self-attentionwithlinearcomplexity.
arXivpreprintarXiv:2006.04768,2020. 3,5
[66] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSi-
moncelli. Imagequalityassessment: fromerrorvisibilityto
structuralsimilarity. IEEETransactionsonImageProcess-
ing,13(4):600‚Äì612,2004. 2,7
[67] XudongXie,ZijieWu,ZhiliangXu,andZhenZhu.Learning
inasingledomainfornon-stationarymulti-texturesynthesis.
arXivpreprintarXiv:2305.06200,2023. 2
[68] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan
Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang.
Maniqa:Multi-dimensionattentionnetworkforno-reference
imagequalityassessment. InProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages1191‚Äì1200,2022. 2
[69] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang.
Fsim: A feature similarity index for image quality assess-
ment.IEEETransactionsonImageProcessing,20(8):2378‚Äì
2386,2011. 2
[70] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E
Hinton. Lookahead optimizer: k steps forward, 1 step
back. Advances in neural information processing systems,
32,2019. 5
[71] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of