Optimal and Adaptive Non-Stationary Dueling Bandits
Under a Generalized Borda Criterion
Joe Suk Arpit Agarwal ∗
Columbia University Columbia University
joe.suk@columbia.edu agarpit@outlook.com
Abstract
In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is
defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated
non-stationary variant of dueling bandits, where preferences change over time, has been the focus of
several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal
is to design algorithms without foreknowledge of the amount of change.
ThebulkofknownresultsherestudiestheCondorcetwinnersetting,whereanarmpreferredoverany
other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this
problem (which is always well-defined) has received little attention. In this work, we establish the first
optimalandadaptiveBordadynamicregretupperbound,whichhighlightsfundamentaldifferencesinthe
learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.
Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within
the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity
are adaptively learnable. This is accomplished through a novel generalized Borda score framework which
unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like
task. Such a generalization was not previously known and is likely to be of independent interest.
Contents
1 Introduction 2
1.1 Summary of Contributions and Novelties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Additional Related Work. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Setup – Non-stationary Dueling Bandits 5
3 Dynamic Regret Lower Bounds 6
4 Dynamic Regret Upper Bounds 6
4.1 Borda Dueling Bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4.2 Condorcet Dueling Bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5 A New Unified View of Condorcet and Borda Regret 9
6 Algorithmic Design 9
6.1 Base Algorithm – Soft Elimination with GBS . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
6.2 Non-Stationary Meta-Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
7 Novelties in Condorcet Regret Analysis 12
8 Conclusion and Future Questions 12
A Setting up Appendix and the Generalized Borda Problem 17
A.1 Summary of Results in Appendix for Generalized Borda Problem . . . . . . . . . . . . . . . . 17
∗TheauthoriscurrentlyatFAIR,Meta. WorkdonewhiletheauthorwasatColumbiaUniversity.
1
4202
raM
91
]GL.sc[
1v05921.3042:viXraA.2 Generalized Notation Related to GBS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.3 Significant Winner Switches w.r.t. Known Weights . . . . . . . . . . . . . . . . . . . . . . . . 18
A.4 Significant Winner Switches w.r.t. Unknown Weights . . . . . . . . . . . . . . . . . . . . . . . 18
B Regret Lower Bounds for GBS 19
B.1 Stationary Regret Lower Bound (Known Weight) . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 Hardness of Learning all Generalized Winners . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.3 Stationary Regret Lower Bound for Unknown Weight under GIC . . . . . . . . . . . . . . . . 22
C Dynamic Regret Lower Bounds 24
C.1 Known Weight Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.2 Unknown Weight Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D Fixed Winner Regret Analysis 25
D.1 Preliminaries and Concentration Bounds for Estimation . . . . . . . . . . . . . . . . . . . . . 25
D.2 Regret Upper Bound under Fixed Winner for Known Weight . . . . . . . . . . . . . . . . . . 27
D.3 Regret Upper Bound under Fixed Winner for Unknown Weight . . . . . . . . . . . . . . . . . 28
E Nonstationary Regret Analysis for Known Weights 29
E.1 Formal Statement of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.2 Proof of Theorem 16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.2.1 Episodes Align with SKW Phases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.3 Decomposing the Regret . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.4 Bounding the per-Episode Regret of the Safe Arm . . . . . . . . . . . . . . . . . . . . . . . . 30
E.5 Bounding per-Episode Regret of Active Arms to Last Global Arm . . . . . . . . . . . . . . . 30
E.6 Bounding per-Episode Regret of the Last Global Arm to the Last Safe Arm . . . . . . . . . . 34
E.7 Total Variation Regret Rates for Known Weights (Proof of Corollary 17) . . . . . . . . . . . . 37
F Nonstationary Regret Analysis for Unknown Weights 38
F.1 Formal Statement of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.2 Analysis Overview for Theorem 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.3 Episodes Align with SUW Phases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.4 Generic Bad Segment Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.5 Decomposing the Regret Along Different Base Algorithms . . . . . . . . . . . . . . . . . . . . 40
F.6 Summing Regret Over Episodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
F.7 Proof Overview for Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
1 Introduction
The K-armed dueling bandits problem, where a learner relies on relative feedback between arms, has been
well-studied within the multi-armed bandits literature (see Sui et al., 2018; Bengs et al., 2021, for surveys).
This problem has many applications in domains such as information retrieval, recommendation systems,
etc, where relative feedback is easy to elicit, while real-valued feedback is difficult to obtain or interpret.
For example, the availability of implicit user feedback comparing the output of two information retrieval
algorithms allows one to automatically tune the parameters of these algorithms using the framework of
dueling bandits (Radlinski et al., 2008; Liu, 2009).
Here, at round t∈[T], the learner pulls a pair of arms and observes relative feedback between these arms
indicating which was preferred. The feedback is drawn (stochastically) according to a pairwise preference
matrix and the regret is measured in terms of the sub-optimality of arms with respect to a ‘winner’ arm.
Unlike classical MAB, there are several different notions of winner arm that are considered in the pairwise
setting, and the underlying theory depends on the notion of winner. Most early work in dueling bandits
considered the Condorcet winner (CW) (Urvoy et al., 2013; Ailon et al., 2014; Zoghi et al., 2014, 2015b;
Komiyama et al., 2015) which is an arm that ‘stochastically beats’ every other arm. An alternative line of
works focuses on the Borda winner (BW), an arm maximizing the probability of defeating a uniformly at
random chosen comparator (Urvoy et al., 2013; Jamieson et al., 2015; Ramamohan et al., 2016; Falahatgar
2et al., 2017; Lin and Lu, 2018; Heckel et al., 2018; Saha et al., 2021). Both Borda and Condorcet carry their
own notions of suboptimality gap and thus regret.
Inthiswork,westudyboththeCondorcetandBordaformulationsofduelingbandits. Eachhasitsadvantages
and disadvantages. For instance, while the Condorcet winner may not exist, the Borda winner always exists.
On the other hand, the Borda winner may not satisfy the independence of clones property (Schulze, 2011).
More broadly, the celebrated Arrow’s impossibility theorem from social choice theory (Arrow, 1950) asserts
that no notion of winner satisfies all “reasonable” axiomatic requirements.
Our focus in this work is on the more challenging non-stationary dueling bandits where preferences may
change over time. Saha and Gupta (2022) first studied this problem and provided an algorithm that achieves
√
a nearly optimal Condorcet dynamic regret1 of O˜( KLT), where L is the total number of changes in the
preference matrix. However, this algorithm is not adaptive, i.e., requires knowledge of L. Moreover, the
dependence on L is pessimistic, as L may count insignificant changes which do not properly capture the
difficulty of non-stationarity. For example, if preferences change often, but a winner arm is fixed, one expects
√
a faster rate of KT.
In fact, an analogous insight was first made in the non-stationary MAB literature (Suk and Kpotufe, 2022;
Abbasi-Yadkori et al., 2023). The latter work proposes a notion of significant shifts which tightly capture
the changes in winner arm which are detrimental to performance. Following suit, Buening and Saha (2023)
proposes a notion of significant Condorcet winner switches and, under the classical SST∩STI condition,
(cid:112)
achieves an adaptive dynamic regret bound of O˜(K L˜T) in terms of L˜ such switches. This SST∩STI
condition roughly states that there is a linear ordering on arms which enforces monotonicity and transitivity
conditions on the mean pairwise preferences.
While SST∩STI is well-studied in earlier dueling bandit works (Yue and Joachims, 2011; Yue et al., 2012a), it
is arguably unrealistic as pairwise preferences may not even be ordered in applications (e.g., cyclic preferences
in a tournament). Despite this, a followup work proved a lower bound asserting the SST∩STI condition is
necessary to attain even sublinear regret in the absence of significant CW switches (Suk and Agarwal, 2023).
Outside of SST∩STI, the story is less resolved: the earlier mentioned work (Buening and Saha, 2023) in fact
√
also shows, outside of SST∩STI, a dynamic regret upper bound of O˜(K ST) in terms of the coarser count
S ≥L˜ of total Condorcet Winner switches. However, it is unclear if the dependence on K in this regret rate
is optimal and also whether an intermediate notion of significant non-stationarity (say, counting between L˜
and S switches in CW) can be learned adaptively. This leads to the first question which our paper addresses.
Question #1. Can we learn other notions of significant CW switches in the Condorcet dueling bandit
√
problem and improve upon the K ST regret rate?
Next, as alluded earlier, a more fundamental issue with the discussion up to this point is that the Condorcet
winnermaynotalwaysexist. Inthecompanionnon-stationaryBordaduelingbanditproblem,whereoneaims
to minimize a Borda version of dynamic regret, the only known work is Saha and Gupta (2022). However,
their only dynamic regret upper bound is non-adaptive, requiring parameter knowledge of non-stationarity,
and is furthermore suboptimal in light of our lower bound (Theorem 9).
At the same time, the earlier mentioned lower bound of Suk and Agarwal (2023) does not rule out the
possibility of learning significant winner switches within the Borda framework, without the need for a strong
SST∩STI assumption or even that a CW exist. This brings us to the second question resolved by this work.
Question #2. Can we attain adaptive and optimal Borda dynamic regret in terms of a “Borda notion” of
significant winner switches?
Surprisingly, both Questions #1 and 2 are answered using the same algorithmic ideas. In particular, we
introduce a new unified framework, that of generalized Borda scores, for dueling bandits which generalizes
both the Condorcet and Borda problems.
Then, in answering Question #2 in the affirmative, we show that our techniques can in fact be applied to the
Condorcet dueling bandit problem. In particular, the Condorcet regret objective can be recast as a Borda-like
regret which allows for a different type of regret analysis avoiding the need for the SST∩STI assumption as in
1measuredtoatime-varyingsequenceofCondorcetwinners.
3prior works (Buening and Saha, 2023; Suk and Agarwal, 2023).
1.1 Summary of Contributions and Novelties
Contributions:
• For Borda dueling bandits, we establish the first optimal and adaptive Borda dynamic regret bound in
terms of significant Borda winner switches. Note the best prior guarantee (Saha and Gupta, 2022) was
suboptimal even with knowledge of non-stationarity and in terms of a coarser notion of non-stationarity.
• For Condorcet dueling bandits, we show that under a General Identifiability Condition (GIC), which
is weaker than SST∩STI (see Figure 1 for full relations), a new non-stationarity measure, termed
approximate CW changes, can be adaptively learned. In particular, we show a regret upper bound of
√
K2/3T2/3S1/3 intermsofS approximateCWchanges. ThisimprovesontheK ST Condorcet
approx approx
dynamic regret upper bound of Buening and Saha (2023) when there is an appreciable difference in the
non-stationarity measures S and S .
approx
As a consequence, this also establishes the first adaptive dynamic regret upper bound outside of
SST∩STI in terms of the total variation measure of change, which was studied in prior works (Saha
and Gupta, 2022; Buening and Saha, 2023).
Technical Novelties. We next highlight some of the algorithmic and analysis novelties.
• A New Perspective on Borda vs. Condorcet Dueling Bandits. We introduce a new framework
(thatofgeneralizedBordascoresinSec.5)thatunifiesboththeCondorcetandBordaregretminimization
problems. To our knowledge, even in the simpler stationary dueling bandit problem, such a framework
has not been studied before.
In fact, this framework is likely of independent interest and could be considered a new standalone
objective for dueling bandits, motivated by the need for a generalized winner notion satisfying the
desirable properties of Borda and Condorcet winner. Related notions have been studied in the social
choice theory literature (Xia and Conitzer, 2008; Xia, 2013).
Although we focus on applying this framework to study the Borda and Condorcet dueling bandit
problems in the body of the paper, Appendix A includes an expansive investigation of the minimax
rates for various regret minimization tasks within this framework.
• Novel Exploration Schedule. All prior works on Borda dueling bandits rely on uniform exploration
at a fixed learning rate to ensure good estimation of Borda scores (Jamieson et al., 2015; Saha et al.,
2021; Wu et al., 2023). This is not suitable for developing an adaptive non-stationary algorithm since
determining such a learning rate requires knowledge of the non-stationarity. To bypass this, we employ
a new time-varying learning rate which is combined with a novel soft-elimination strategy for Borda
dueling bandits.
• Recasting Condorcet Regret as a Borda-like Regret. For analyzing Condorcet dynamic regret,
we take an alternative analysis approach compared to prior works which relied either on SST∩STI or
studying an easier measure of non-stationarity (Buening and Saha, 2023; Suk and Agarwal, 2023).
Key to this is using our aforementioned generalized Borda score framework to reduce the Condorcet
regrettoaBorda-likeregretquantitywhichallowsforustodoanMAB-styleanalysis, withoutsuffering
from the difficulties of preference feedback.
1.2 Additional Related Work
Dueling bandits. The stochastic dueling bandit problem was first proposed by Yue and Joachims (2011);
Yue et al. (2012b), which provided an algorithm achieving an instance-dependent O(KlogT) regret under the
4SST∩STI condition. Urvoy et al. (2013) studied this problem under the broader Condorcet winner condition
and achieved an instance-dependent O(K2logT) regret bound, which was further improved by Zoghi et al.
(2014) and Komiyama et al. (2015) to O(K2 +KlogT). Finally, Saha and Gaillard (2022) showed it is
possible to achieve an optimal instance-dependent bound of O(KlogT) and instance-independent bound of
√
O( KT) under the Condorcet condition. These works all assume a stationary environment.
Works on adversarial dueling bandits (Gajane et al., 2015; Saha et al., 2021) allow for changing preferences
and thus are closer to this work. However, these works focus on the static regret objective against the ‘best’
arm in hindsight and whereas we consider the dynamic regret.
Other than the earlier mentioned works on dynamic regret minimization in dueling bandits, Kolpaczki et al.
(2022) studies weak dynamic regret minimization but uses procedures requiring knowledge of non-stationarity.
Borda Regret Minimization. TheonlyworksstudyingBordaregret(instochasticoradversarialsettings)
are Saha et al. (2021); Saha and Gaillard (2022); Wu et al. (2023). Of these, only Saha and Gaillard (2022)
establishes dynamic Borda regret bounds, which require knowledge of the underlying non-stationarity, and
are suboptimal in light of our optimal regret bound (Theorem 16). Hilgendorf (2018) studies weak Borda
regret where the learner only incurs the Borda regret of the better of the two arms paid.
Other Notions of Winner. Other alternative notions of winner and objectives, beyond Condorcet and
Borda, have been proposed, such as Copeland winner (Zoghi et al., 2015a; Komiyama et al., 2016; Wu and
Liu, 2016) and von Neumann winner (Dudik et al., 2015; Balsubramani et al., 2016).
Non-stationary multi-armed bandits. Switching multi-armed bandits with was first considered in the
so-called adversarial setting by Auer et al. (2002), where a version of EXP3 was shown to attain optimal
√
dynamic regret LKT when given knowledge of the number L of changes in the rewards. Later works
showed similar guarantees in this problem for procedures inspired by stochastic bandit algorithms (Garivier
and Moulines, 2011; Kocsis and Szepesvári, 2006). Recently, Auer et al. (2018, 2019); Chen et al. (2019)
established the first adaptive and optimal dynamic regret guarantees, without requiring parameter knowledge
of the number of changes.
Alternativecharacterizationsofthechangeinrewardsintermsofatotal-variationquantitywasfirstintroduced
in Besbes et al. (2014) with minimax rates quantified therein and adaptive rates attained in Chen et al.
(2019). There have also been characterizations of non-stationarity in terms of drift parameters (Jia et al.,
2023; Krishnamurthy and Gopalan, 2021). Yet another characterization, in terms of the number of best
√
arm switches S was studied in Abbasi-Yadkori et al. (2023), establishing an adaptive regret rate of SKT.
Around the same time, Suk and Kpotufe (2022) introduced the aforementioned notion of significant shifts for
(cid:112)
the switching bandit problem and adaptively achieved rates of the form L˜·K·T in terms of L˜ significant
shifts in the rewards, which serves as the inspiration for our Definition 1.
2 Setup – Non-stationary Dueling Bandits
We consider K-armed dueling bandits with horizon T. At round t∈[T], the pairwise preference matrix is
denoted by P ∈[0,1]K×K, where (i,j)-th entry P (i,j) encodes the likelihood of observing a preference for
t t
arm i in a direct comparison with arm j. In the stationary dueling bandit problem, the preference matrices
P ≡ P are unchanging in time, whereas in the non-stationary problem, the preference matrices P may
t t
change arbitrarily from round to round. At round t, the learner selects a pair of actions (i ,j )∈[K]×[K]
t t
and observes the feedback O (i ,j )∼Ber(P (i ,j )) where P (i ,j ) is the underlying preference of arm i
t t t t t t t t t t
over j . We next outline the two main formulations of dueling bandits (Borda and Condorcet).
t
5BordaCriterion. Letb (a)=. 1 (cid:80) P (a,a′)betheBordascoreofarma. Then,ab =. argmax b (a)
t K t t a∈[K] t
a′∈[K]
is the Borda winner (BW) at round t. Then, the Borda dynamic regret is
T
. (cid:88) 1
RegretB = b (ab)− (b (i )+b (j )). (1)
t t 2 t t t t
t=1
Condorcet Criterion. Here, one assumes the existence of a Condorcet winner (CW) ac such that
t
P (ac,a)≥1/2 for all arms a∈[K]. Then, the Condorcet dynamic regret is defined as
t t
T
. (cid:88)1
RegretC = (P (ac,i )+P (ac,j )−1).
2 t t t t t t
t=1
3 Dynamic Regret Lower Bounds
We briefly summarize the minimax dynamic regret rates for Borda and Condorcet problems. Full statements
and proofs are deferred to Appendix C.
• Borda Dynamic Regret As the minimax Borda regret rate over n stationary rounds is K1/3·n2/3
(Saha et al., 2021), it follows that the minimax dynamic regret rate over L stationary phases of length T/L is
L·K1/3·(T/L)2/3 =K1/3·T2/3·L1/3 (Theorem9). Infact,atighternotionofnon-stationarity(Definition1)
may replace L here. To our knowledge, this establishes the first lower bound on Borda dynamic regret.
• Condorcet Dynamic Regret Here, since the minimax regret rate over stationary problems is of order
√ √
KT, the lower bound on Condorcet dynamic regret is of order KLT (Saha and Gupta, 2022). Once again,
L here may be replaced by a tighter measure of non-stationarity (Suk and Agarwal, 2023).
4 Dynamic Regret Upper Bounds
4.1 Borda Dueling Bandits
A New Measure of Non-Stationarity. Following the discussion in Subsec. 1.2, key in works on non-
stationary (dueling) bandits is a measure of non-stationarity which captures the difficulty of the problem.
Speaking plainly, the higher the amount of non-stationarity the more difficult the problem is, and so larger
regret rates are expected. It is thus crucial that such a measure of change properly captures the difficulty of
the problem. However, the only other work dealing with Borda dynamic regret (Saha and Gupta, 2022) relies
on coarse non-stationarity measures, such as the aggregate number or magnitude of changes.
To contrast, in non-stationary MAB and Condorcet dueling bandits, it is now recognized (Suk and Kpotufe,
2022; Buening and Saha, 2023; Suk and Agarwal, 2023) that tighter non-stationary measures, so-called
significant shifts, which capture only those changes which are detrimental to performance. can in fact be
adaptively learned. Inspired by these results, we first define a notion of significant BW switches.
Definition 1 (SignificantBWSwitches). Define an arm a as having significant Borda regret on [s ,s ] if
1 2
(cid:88)s2
δb(a)≥K1/3·(s −s )2/3. (2)
s 2 1
s=s1
Define significant BW switches (abbrev. SBS) as follows: the 0-th sig. shift is defined as τ = 1 and
0
the (i+1)-th sig. shift τ is recursively defined as the smallest t > τ such that for each arm a ∈ [K],
i+1 i
6∃[s ,s ]⊆[τ ,t] such that arm a has significant regret over [s ,s ]. We refer to the interval of rounds [τ ,τ )
1 2 i 1 2 i i+1
as an SBS phase2. Let L˜ be the number of SBS phases elapsed in T rounds.
b
Our goal then is to establish a dynamic regret upper bound which depends optimally on the number of SBS,
and does not require knowledge of the underlying non-stationarity.
Adaptive and Optimal Regret Upper Bound. Intuitively, if one knows the SBS τ , then, in each SBS
i
phase [τ ,τ ), one can achieve a tight regret bound of order K1/3·(τ −τ )2/3 by learning the last safe
i i+1 i+1 i
arm, or the last arm to incur significant Borda regret in said phase. We show that in fact such a rate can be
attained, over all SBS phases, without any knowledge of the non-stationarity.
Theorem 1. Algorithm 2 with the fixed weight specification (see Definition 5) satisfies:
 
E[RegretB]≤O˜
(cid:88)L˜
b
K1/3·(τ i+1−τ i)2/3 .
i=0
Corollary 2. By Jensen’s inequality, the regret bound of Theorem 16 is upper bounded by K1/3·T2/3·L˜1/3.
Furthermore, relating SBS to total variation V =. (cid:80)T max |P (a,a′)−P (a,a′)|, a regret boundb of
T t=2 t t−1
a,a′∈[K]
order V1/4·T3/4·K1/4+K1/3·T2/3 also holds in terms of total variation quantity V (see Appendix E.7).
T T
In particular, Theorem 16 provides a matching upper bound to the lower bound of Sec. 3 (see Theorem 9),
thus improving the suboptimal result of Theorem 6.1 in Saha and Gupta (2022), which relied on a coarser
notion of non-stationary and was not adaptive.
4.2 Condorcet Dueling Bandits
Significant Notions of Change Outside of SST∩STI. Following the discussion of Sec. 1, it was
previously shown in Suk and Agarwal (2023) that an analogue of significant winner switches (Definition 1)
for Condorcet dynamic regret cannot be learned adaptively outside both the strong SST and STI preference
model assumptions. Recall that STI/SST mandate at each round t, there is a total ordering on arms, ≻ ,
t
such that ∀i⪰ j ⪰ k:
t t
• P (i,k)≥max{P (i,j),P (j,k)} (SST).
t t t
• P (i,k)≤P (i,j)+P (i,k) (STI).
t t t
In particular, such conditions are convenient for relating uncompared arms through an inferred ordering,
which turns out to be crucial to detecting unknown changes in CW.
However, this impossibility result leaves open whether other tighter notions of non-stationarity can be learned
√
outside of SST∩STI, and at rates faster than the state-of-the-art K ST Condorcet dynamic regret achieved
by Buening and Saha (2023).
We show this is indeed possible in a broad class of preference models, the GIC class, outside of SST∩STI.
Condition 1. (General Identifiability Condition) At each round t, there exists an arm a∗ such that
t
a∗ ∈argmax P (a,a′) for all arms a′ ∈[K].
t a∈[K] t
While GIC requires the CW to beat every other arm with the largest margin, it is far broader than SST∩STI
in not requiring any ordering on non-winner arms, allowing for cycles or arbitrary preference relations. The
GIC was previously studied in utility dueling bandits by Zimmert and Seldin (2018) (see also Bengs et al.,
2021, Section 3.1). Unlike these prior works, we do not require the winner arm a∗ to be unique.
t
7Figure 1: GIC is a larger subclass of the Condorcet Winner model, including SST∩STI and SST, while
incomparable with STI in general.
We also note that Condition 1 implies the Condorcet condition, but is weaker than SST while incomparable
with STI. See Figure 1 for a full comparison.
We next show that, under GIC, a tighter notion of non-stationarity than the count S of winner switches may
be learned with improved regret rates.
Definition 3 (Approximate Winner Changes). Define ζ recursively as follows: let ζ =1 and let ζ be the
i 0 i+1
smallest t>ζ such that there does not exist an approximate winner arm a˜ such that
i
(cid:18) K2 (cid:19)1/3
∀s∈[ζ ,t],a∈[K]:|P (a∗,a)−P (a˜,a)|≤ . (3)
i s s s s−ζ
i
.
Let S =1+max{i:ζ <T} be the total count of approximate winner changes.
approx i
Note that (3) always holds so long as the winner a∗ does not change. Thus, we have S ≤S.
t approx
Improved Condorcet Dynamic Regret Bound. Our regret upper bound is as follows.
Theorem 4. Under Condition 1, Algorithm 2 with the unknown weight specification (see Definition 5)
satisfies:
E[RegretC]≤O˜(min{K2/3·T2/3·S1/3 ,K1/2·V1/4·T3/4+K2/3·T2/3}).
approx T
√
We compare this regret bound to the K ST rate of Buening and Saha (2023). In particular, we have:
min{K2 3T2 3S a1 3pprox,K1 2V T1 4T43 +K2 3T2 3}≤K√ ST ⇐⇒ K1 3 ≥ T61S Sa1 3 21pprox or K ≥ T3 2 SV T21 ∨ T
S3
21 2 . (4)
We highlight two regimes, under GIC, where this comparison is favorable:
• Many spurious winner changes S =T: if there are S =T changes in winner, then, (4) always holds
regardless of the values of V ,S ,K,T and thus captures regimes where sublinear regret is possible
√ T approx
while the K ST rate is vacuous. The superiority of our regret rate is most evident when S or V
approx T
are small, which is possible if the majority of S =T winner changes are spurious to performance.
• Large number of arms: viewed another way, (4) states that if the number of arms K is larger than
some threshold determined by the discrepancy in the non-stationarity measures S vs. S or S vs
√ approx
V , then Theorem 4’s regret rate is superior to the K ST rate.
T
We also note that the K1/2·V1/4·T3/4 rate of Theorem 4 is the first adaptive Condorcet dynamic regret
T
bound in termsof total-variation, as prior works onlyachieved regretupperboundsunder SST∩STI (Buening
and Saha, 2023; Suk and Agarwal, 2023).
2Weconflate[a,b)fora,b∈Nwith[a,b)∩N
85 A New Unified View of Condorcet and Borda Regret
ContinuingthediscussionofSubsec.1.1, ourregretupperbounds(Theorems1and4)areshownbyappealing
to a new unified framework which generalizes both the Borda and Condorcet regret minimization tasks.
Key to this is the new idea of a generalized Borda score (GBS), which measures the preference of an arm a
over a reference distribution or weight of arms w∈∆K, where ∆K denotes the probability simplex on [K].
More precisely, we define a GBS with respect to weight w=(w1,...,wK) as
b
(a,w)=. (cid:88)
P
(a,a′)·wa′
.
t t
a′∈[K]
Notions of generalized Borda winner (maximizer of GBS) and generalized Borda dynamic regret (analogous to
(1)) follow suit. Taking w to be Unif{[K]}, this recovers the Borda score and dynamic regret.
If a Condorcet winner ac exists, then taking w to be the point-mass weight w(ac) on ac also allows us to
t t t
capture the Condorcet regret. Indeed, one observes that the CW ac maximizes the score b (a,w(ac)), and
t t t
the regret in terms of generalized Borda scores of arm a w.r.t. weight w(ac) becomes
t
1
b (ac,w(ac))−b (a,w(ac))=P (ac,a)− ,
t t t t t t t 2
which is precisely the Condorcet regret at round t.
Recasting the Condorcet regret objective as a Borda-like regret quantity will be key to bypassing the need for
SST∩STI, while still capturing a tighter measure of non-stationarity.
Changing and Unknown Weights (Key Difficulty). The challenge with this reformulation of the
Condorcet problem is that the reference weight w(ac) is unknown since the CW ac is. Furthermore, in the
t t
more difficult non-stationary problem, the identity of ac may change at unknown times meaning so can the
t
weight w(ac). This makes it difficult to even estimate the generalized Borda score b (a,w (ac)) compared to
t t t t
the usual Borda task where the weight w is fixed over time.
We will show that such difficulties are resolved by tracking all point-mass weights and carefully amortizing
the regret analysis to periods of time where a estimating a fixed point-mass weight suffices.
6 Algorithmic Design
To minimize the generalized Borda dynamic regret, we rely on estimating b (a,w) for weights w belonging to
t
a reference set of weights W. In what follows, we’ll discuss the procedures in terms of a fixed W. In the end,
W can be flexibly specialized for each of the Borda and Condorcet regret problems (see Definition 5).
Following the high-level idea of prior works on non-stationary dueling bandits (Suk and Agarwal, 2023;
Buening and Saha, 2023), we’ll first design a base algorithm which works well in mildly non-stationary
environments where there is no approximate winner change (Definition 3) and then randomly schedule
different instances of this base algorithm to allow for detection of unknown non-stationarity.
One key deviation from the aforementioned prior non-stationary works is that we avoid using a successive
elimination base algorithm. This is necessary since accurate estimation of the (generalized) Borda score
b (a,w), calls for some minimal uniform exploration, which rules out any hard elimination strategy.
t
Thus, we first design a new base algorithm, which mixes elimination with time-varying exploration.
96.1 Base Algorithm – Soft Elimination with GBS
Estimating Generalized Borda Scores. At round t, we estimate the generalized Borda scores via
importance-weighting for weight w∈W:
ˆb (a,w)=. E (cid:20) 111{i t =a,j t =a′}·O t(i t,j t)(cid:21) , (5)
t a′∼w q t(a)·q t(a′)
.
where q is the play-distribution over arms at time t. Let δˆw(a′,a)=ˆb (a′,w)−ˆb (a,w) denote the induced
t t t t
estimator of the generalized Borda gap.
Eliminating Arms We evict arm a from the candidate arm set A at round t if for some [s ,s ]⊆[1,t]:
t 1 2
max
(cid:88)s2
δˆw(a′,a)≥Clog(T)·F([s ,s ]). (6)
a′∈∩s s2 =s1Ass=s1 s 1 2
In the above C >0 is a universal constant free of any problem-dependent parameters whose value can be
determined from the analysis and the eviction threshold F(I) for interval I is determined using the estimation
error bounds on (cid:80)s2 ˆb (a,w) (see Definition 5 in Appendix D.1 for exact formulas for F(I), which differ
for Borda vs.
Conds o= rcs1ets
problems).
These error bounds scale with the inverse play-probability min q−1(a), calling for a careful design of the
a t
play distribution q (·), namely that which balances exploration and proper elimination.
t
Novel Exploration Schedule. We choose q as a mixture of exploring the candidate set A and uniformly
t t
exploring all arms:
q ∼(1−η )·Unif{A }+η ·Unif{W},
t t t t
where Unif{W} is a further uniform mixture of playing according to the distrributions in W. The learning
rate η must then be carefully set to ensure both sufficient exploration (to reliably estimate the GBS) and safe
t
regret. As before, the values of η (see Definition 5) will depend whether we are in the Borda vs. Condorcet
t
setting.
However, prior works on stationary Borda dueling bandits must set η using knowledge of the horizon T
t
(Jamieson et al., 2015; Saha et al., 2021; Wu et al., 2023). In our non-stationary problem, this amounts to
setting η based on knowledge of the underlying non-stationarity (Saha and Gupta, 2022).
t
Thus, a key difficulty in targeting the regret rate of Theorem 16 without knowledge of non-stationarity is
that the optimal oracle learning rate η depends on the unknown phase lengths (τ −τ for Borda scores;
t i+1 i
ρ −ρ for Condorcet). To circumvent this, weemploy a time-varying learning rate η whichdepends onthe
i+1 i t
current number of rounds elapsed. To our knowledge, such an idea has only been used in works on adversarial
bandit for analyzing EXP3 (Seldin et al., 2013; Maillard, 2011).
6.2 Non-Stationary Meta-Algorithm
Forthenon-stationarysettingwithunknownshifts,weuseahierarchicalalgorithmMETABOSSE(Algorithm2)
to schedule multiples copies of the base algorithm BOSSE(t ,m) (Algorithm 1) at random start times t
start start
and durations m.
Going into more detail, METABOSSE proceeds in episodes, starting each episode by running a starter instance
of BOSSE. A running base algorithm may further activates its own base algorithms of varying durations
(Line 12 of Algorithm 1), called replays according to a random schedule decided by the Bernoulli random
variables B (see Line 6 of Algorithm 2). We refer to the base algorithm playing at round t as the active
s,m
base algorithm.
10Algorithm 1: BOSSE(t ,m ): (Generalized) BOrda Score Soft Elimination
start 0
.
Input:Input set of weights W, learning rate profileγγγ ={γ }T , eviction threshold F(·).
t t=1
1 Initialize: t←t start, active arm set A tstart ←[K].
2 while t≤T do
33 Set exploration rate for this round: η t ←γ t−tstart.
4 Update play distribution q t: let q t ∈∆K be the mixture (1−η t)·Unif{A t}+η t·Unif{W}.
5 Sample i t,j t ∼q t i.i.d..
6 Receive feedback O t(i t,j t)∼Ber(P t(i t,j t)).
7
Increment t←t+1.
8 Evict arms with large generalized Borda gaps:
A ←A \(cid:8) a∈[K]:∃[s ,s ]⊆[t ,t],w∈W s.t. (6) holds with a∈∩s2 A (cid:9)
t t 1 2 start s=s1 s
A ←A \(cid:8) a∈[K]:∃[s ,s ]⊆[t ,t],w∈W s.t. (6) holds with a∈∩s2 A (cid:9)
9
global global 1 2 ℓ s=s1 s
10 if ∃msuch .that B t,m >0 then /* Lines 10-13 only for use with METABOSSE */
11
Let m=max{m∈{2,4,...,2⌈log(T)⌉}:B
t,m
>0}.
// Set maximum replay length.
12
Run BOSSE(t,m).
// Replay interrupts.
13 if t>t start+m 0 then RETURN.
Algorithm 2: Meta-BOSSE
.
Input: horizon T, input set of weights W, learning rate profileγγγ ={γ }T , eviction threshold F(·).
t t=1
1 Initialize: round count t←1.
2 Episode Initialization (setting global variables t ℓ,A global,B s,m):
3 t ℓ ←t. // t ℓ indicates start of ℓ-th episode.
4 A global ←[K] // Global active arm set.
5 For each m=2,4,...,2⌈log(T)⌉ and s=t ℓ+1,...,T:
(cid:16) (cid:17)
6 Sample and store B s,m ∼Bernoulli m1/3·(s1 −tℓ)2/3 . // Set replay schedule.
7 Run BOSSE(t ℓ,T +1−t ℓ).
8 if t<T then restart from Line 2 (i.e. start a new episode).
The candidate arm set A is pruned by the active base algorithm at round t, and globally shared between all
t
running base algorithms. In addition, all other variables, i.e. the ℓ-th episode start time t , round count t,
ℓ
and replay schedule {B } , are shared between base algorithms.
s,m s,m
In sharing these global variables, any replay can trigger a new episode: every time an arm is evicted by a
replay, it is also evicted from the global arm set A , essentially the active arm set for the entire episode.
global
A new episode is triggered when A becomes empty, i.e., there is no safe arm left to play.
global
For further algorithmic intuition on the hierarchical schedule and management of base algorithms, we defer
the reader to Section 4 of Suk and Kpotufe (2022). We now focus on highlighting some novelties in this work.
Each Active Base Alg. Chooses Own Learning Rate EachbasealgorithmBOSSE(t ,m)determines
start
its own time-varying learning rate using its starting round t . Then, the global learning rate η at round t
start t
is set by the base algorithm active at round t (Line 3 of Algorithm 1). Furthermore, the history of global
learning rates {η } (globally accessible by any base algorithm) is used to determine the eviction thresholds
t t
(15) and (16) over intervals [s ,s ] of rounds where multiple base algorithms may be active.
1 2
This ensures reliable detection of critical segments [s ,s ] of time where an arm has large (generalized) Borda
1 2
regret. For such a critical segment, a core argument of the analysis is that an ideal replay, i.e., an instance of
BOSSE(s ,m) for m = s −s , is scheduled with high probability. However, such an ideal replay must be
1 2 1
scheduled for a duration commensurate with s −s , and hence must also use a commensurate learning rate.
2 1
11A New Replay Scheduling Rate. In order to properly detect critical segments while also safeguarding
T2/3 regret, a different replay scheduling rate (Line 12 of Algorithm 1) is required. For this, we find that
instantiating a base algorithm of length m at time t in episode [t ,t ) with probability m−1/3·(s−t )−2/3
ℓ ℓ+1 ℓ
balances regret minimization and detection of changes in winner.
7 Novelties in Condorcet Regret Analysis
Leaving the details of the analysis to Appendices E and F, we instead focus here on highlighting the key
technical novelties in showing our Condorcet dynamic regret upper bound.
Recasting Condorcet Regret as a Generalized Borda Regret. As discussed in Sec. 1, we emphasize
our main analysis novelty is in reformulating the Condorcet regret as a generalized Borda-like regret. This
allows us to take the analysis route of prior works on non-stationary MAB (Suk and Kpotufe, 2022) without
suffering from issues endemic to preference feedback, as seen in Buening and Saha (2023); Suk and Agarwal
(2023). To our knowledge, no other works on dueling bandits make use of such a trick.
Keeping Track of All Unknown Weights. As mentioned earlier in Sec. 5, a crucial difficulty with
making use of the generalized Borda framework for Condorcet dueilng bandits is that the reference weights w
are unknown. Furthermore, the space of all possible weights ∆K is combinatorially large and thus one cannot
hope to maintain estimates for all weights in ∆K without an intractable union bound. We show in fact that
such accurate estimation can be bypassed because the worst-case regret is always attained at a point-mass
weight on some arm a∈[K] due to regret being linear in the weight vector.
Keeping Track of Unknown Changes in Weights. An added difficulty in the non-stationary setting is
that the unknowns weights may change at unknown times. This makes it challenging to bound the regret
over intervals of rounds [s ,s ] which elapse multiple approximate winner changes ρ and hence for which we
1 2 i
requirescoreestimationw.r.t. achangingweightsequence{w } . However, evenfollowingtheaboveproposal
t t
for tracking point-mass weights, we note the space of all sequences of point-mass weights is combinatorially
large. This prohibits estimating all (cid:80)s2 b (a,w ) for changing point-mass weights w .
s=s1 s s s
Instead, we carefully divide up the regret analysis into segments of time [s ,s ]⊆[ρ ,ρ ) lying within an
1 2 i i+1
approximate winner phase [ρ ,ρ ) where it suffices to bound the regret using a single weight (as it turns
i i+1
out, that of the current approximate winner a♯). This involves doing a separate analysis of when an arm is
detected as having significant regret for each arm a∈[K] and each base algorithm.
8 Conclusion and Future Questions
We’ve achieved the first optimal and adaptive dynamic Borda regret upper bound. Additionally, we’ve
introducedthegeneralizedBordaframeworkwhichhasrevealednewpreferencemodelswherefasterCondorcet
regret rates are attainable, adaptively, in terms of new tighter measures of non-stationarity. In the Condorcet
setting, it is still unclear if the bounds of Theorem 4 are tight and what can be said outside of the GIC
class. More broadly, tracking changing von-Neumann winners in non-stationary dueling bandits appears a
challenging, but quite interesting direction.
Future work can also study this generalized Borda framework (with generic known or unknown weights) and
characterize the instance-dependent regret rates, which remain unclear even in stationary settings.
12References
Yasin Abbasi-Yadkori, András György, and Nevena Lazić. A new look at dynamic regret for non-stationary
stochastic bandits. Journal of Machine Learning Research, 24(288):1–37, 2023.
Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing Dueling Bandits to Cardinal Bandits. In
Proceedings of the 31st International Conference on Machine Learning, 2014.
Kenneth J. Arrow. A difficulty in the concept of social welfare. Journal of Political Economy, 58(4):328–346,
1950. ISSN 00223808, 1537534X.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The Nonstochastic Multiarmed
Bandit Problem. SIAM Journal on Computing, 32(1):48–77, 2002.
Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown number
of distribution changes. 14th European Workshop on Reinforcement Learning (EWRL), 2018.
Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an unknown
number of distribution changes. In Alina Beygelzimer and Daniel Hsu, editors, Conference on Learning
Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99 of Proceedings of Machine Learning
Research, pages 138–158. PMLR, 2019.
Akshay Balsubramani, Zohar Karnin, Robert E. Schapire, and Masrour Zoghi. Instance-dependent regret
bounds for dueling bandits. In 29th Annual Conference on Learning Theory, volume 49 of Proceedings of
Machine Learning Research, pages 336–360. PMLR, 23–26 Jun 2016.
Gábor Bartók, Dean P. Foster, Dávid Pál, Alexander Rakhlin, and Csaba Szepesvári. Partial monitor-
ing—classification, regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967–997,
2014. ISSN 0364765X, 15265471.
Viktor Bengs, Róbert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke Hüllermeier. Preference-based online
learning with dueling bandits: A survey. J. Mach. Learn. Res., 22(1), jan 2021. ISSN 1532-4435.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-stationary
rewards. Advances in Neural Information Processing Systems, 27:199–207, 2014.
AlinaBeygelzimer,JohnLangford,LihongLi,LevReyzin,andRobertSchapire. Contextualbanditalgorithms
with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial
Intelligence and Statistics, pages 19–26, 2011.
Thomas Kleine Buening and Aadirupa Saha. Anaconda: An improved dynamic regret algorithm for adaptive
non-stationary dueling bandits. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors,
Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of
Proceedings of Machine Learning Research, pages 3854–3878. PMLR, 25–27 Apr 2023.
YifangChen,Chung-WeiLee,HaipengLuo,andChen-YuWei. Anewalgorithmfornon-stationarycontextual
bandits: Efficient, optimal, and parameter-free. In Proceedings of the 32nd Conference on Learning Theory,
99:1–30, 2019.
Miroslav Dudik, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual
Dueling Bandits. In Proceedings of the 28th Conference on Learning Theory, 2015.
Moein Falahatgar, Yi Hao, Alon Orlitsky, Venkatadheeraj Pichapati, and Vaishakh Ravindrakumar. Maxing
and ranking with few assumptions. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30.
Curran Associates, Inc., 2017.
13Pratik Gajane, Tanguy Urvoy, and Fabrice Clérot. A relative exponential weighing algorithm for adversarial
utility-based dueling bandits. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd
International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of
JMLR Workshop and Conference Proceedings, pages 218–227. JMLR.org, 2015.
Aurélien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In
International Conference on Algorithmic Learning Theory, pages 174–188. Springer, 2011.
Reinhard Heckel, Max Simchowitz, Kannan Ramchandran, and Martin Wainwright. Approximate ranking
from pairwise comparisons. In Proceedings of the Twenty-First International Conference on Artificial
Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1057–1066.
PMLR, 09–11 Apr 2018.
LennardHilgendorf. Duellingbanditswithweakregretinadversarialenvironments. Master’sthesis,University
of Copenhagen, 2018. Available at https://arxiv.org/pdf/1812.04152.pdf.
Kevin Jamieson, Sumeet Katariya, Atul Deshpande, and Robert Nowak. Sparse Dueling Bandits. In
Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, 2015.
S. Jia, Qian Xie, Nathan Kallus, and P. Frazier. Smooth non-stationary bandits. In International Conference
on Machine Learning, 2023.
Levente Kocsis and Csaba Szepesvári. Discounted ucb. 2nd PASCAL Challenges Workshop, 2006.
Patrick Kolpaczki, Viktor Bengs, and Eyke Hüllermeier. Non-stationary dueling bandits. arXiv preprint
arXiv:2202.00935, 2022.
JunpeiKomiyama,JunyaHonda,HisashiKashima,andHiroshiNakagawa. RegretLowerBoundandOptimal
Algorithm in Dueling Bandit Problem. In Proceedings of the 28th Conference on Learning Theory, 2015.
Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Copeland Dueling Bandit Problem: Regret
Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm. In Proceedings of the 33rd
International Conference on Machine Learning, 2016.
RamakrishnanKrishnamurthyandAdityaGopalan. Onslowly-varyingnon-stationarybandits. arXivpreprint:
arXiv:2110.12916, 2021.
Chuang-Chieh Lin and Chi-Jen Lu. Efficient mechanisms for peer grading and dueling bandits. In Jun Zhu
and Ichiro Takeuchi, editors, Proceedings of The 10th Asian Conference on Machine Learning, volume 95
of Proceedings of Machine Learning Research, pages 740–755. PMLR, 14–16 Nov 2018.
Tie-Yan Liu. Learning to rank for information retrieval. Found. Trends Inf. Retr., 3(3):225–331, mar 2009.
ISSN 1554-0669. doi: 10.1561/1500000016.
Odalric-Ambrym Maillard. APPRENTISSAGE SÉQUENTIEL : Bandits, Statistique et Renforcement. Phd
thesis, Université des Sciences et Technologie de Lille - Lille I, October 2011.
Filip Radlinski, Madhu Kurup, and Thorsten Joachims. How does clickthrough data reflect retrieval quality?
In Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM ’08, page
43–52, New York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781595939913. doi:
10.1145/1458082.1458092.
SiddarthaRamamohan,ArunRajkumar,andShivaniAgarwal. DuelingBandits: BeyondCondorcetWinners
to General Tournament Solutions. In Advances in Neural Information Processing Systems 29, 2016.
Aadirupa Saha and Pierre Gaillard. Versatile dueling bandits: Best-of-both world analyses for learning from
relative preferences. In International Conference on Machine Learning, pages 19011–19026. PMLR, 2022.
14Aadirupa Saha and Shubham Gupta. Optimal and efficient dynamic regret algorithms for non-stationary
duelingbandits. InInternationalConferenceonMachineLearning, ICML2022, 17-23July2022, Baltimore,
Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 19027–19049. PMLR,
2022.
Aadirupa Saha, Tomer Koren, and Yishay Mansour. Adversarial dueling bandits. In Marina Meila and
Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021,
18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 9235–9244.
PMLR, 2021.
Markus Schulze. A new monotonic, clone-independent, reversal symmetric, and condorcet-consistent single-
winner election method. Social Choice and Welfare, 36(2):267–303, 2011. ISSN 01761714, 1432217X.
Yevgeny Seldin, Csaba Szepesvári, Peter Auer, and Yasin Abbasi-Yadkori. Evaluation and analysis of the
performanceoftheexp3algorithminstochasticenvironments. InMarcPeterDeisenroth, CsabaSzepesvári,
andJanPeters,editors,ProceedingsoftheTenth EuropeanWorkshoponReinforcementLearning,volume24
of Proceedings of Machine Learning Research, pages 103–116, Edinburgh, Scotland, 30 Jun–01 Jul 2013.
PMLR.
Yanan Sui, Masrour Zoghi, Katja Hofmann, and Yisong Yue. Advancements in dueling bandits. In Jérôme
Lang, editor, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,
IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pages 5502–5510. ijcai.org, 2018.
Joe Suk and Arpit Agarwal. When can we track significant preference shifts in dueling bandits? In
Thirty-seventh Conference on Neural Information Processing Systems, 2023.
Joe Suk and Samory Kpotufe. Tracking most significant arm switches in bandits. In Conference on Learning
Theory, pages 2160–2182. PMLR, 2022.
Tanguy Urvoy, Fabrice Clerot, Raphael Feraud, and Sami Naamane. Generic Exploration and K-armed
Voting Bandits. In Proceedings of the 30th International Conference on Machine Learning, 2013.
Huasen Wu and Xin Liu. Double thompson sampling for dueling bandits. In Advances in Neural Information
Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December
5-10, 2016, Barcelona, Spain, pages 649–657, 2016.
Yue Wu, Tao Jin, Hao Lou, Farzad Farnoud, and Quanquan Gu. Borda regret minimization for generalized
linear dueling bandits, 2023.
Lirong Xia. Generalized scoring rules: a framework that reconciles borda and condorcet. SIGecom Exch., 12
(1):42–48, jun 2013.
Lirong Xia and Vincent Conitzer. Generalized scoring rules and the frequency of coalitional manipulability.
In Proceedings of the 9th ACM Conference on Electronic Commerce, EC ’08, page 109–118, New York, NY,
USA, 2008. Association for Computing Machinery. ISBN 9781605581699.
YisongYueandThorstenJoachims. Beatthemeanbandit. InProceedingsofthe28thInternationalConference
on Machine Learning, 2011.
Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem.
Journal of Computer and System Sciences, 78(5):1538–1556, 2012a.
Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem.
Journal of Computer and System Sciences, 78(5):1538–1556, 2012b. ISSN 0022-0000. doi: https://doi.org/
10.1016/j.jcss.2011.12.028. JCSS Special Issue: Cloud Computing 2011.
Julian Zimmert and Yevgeny Seldin. Factored bandits. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31.
Curran Associates, Inc., 2018.
15Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten de Rijke. Relative Upper Confidence Bound
for the K-Armed Dueling Bandit Problem. In Proceedings of the 31st International Conference on Machine
Learning, 2014.
Masrour Zoghi, Zohar Karnin, Shimon Whiteson, and Maarten de Rijke. Copeland Dueling Bandits. In
Advances in Neural Information Processing Systems 28, 2015a.
Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale online
ranker evaluation. In Proceedings of the 8th ACM International Conference on Web Search and Data
Mining, 2015b.
16A Setting up Appendix and the Generalized Borda Problem
Recall from Sec. 5 that the generalized Borda score (GBS) b (a,w) is defined as
t
b
(a,w)=. (cid:88)
P
(a,a′)·wa′
,
t t
a′∈[K]
.
with respect to weight w=(w1,...,wK)∈∆K. Our goal is to minimize the analogous notion of dynamic
regret with respect to the quantities b (a,w) in two situations: (1) the setting of known weights where the
t
vector w is known to the learner and (2) the setting of unknown and changing weights where w changes
t
over time t and are unknown to the learner. These two scenarios will respectively capture the Borda and
Condorcet regret problems. To avoid confusion, throughout this appendix we’ll sometimes refer to the usual
Borda score b (a,Unif{[K]}) as the uniform Borda score (resp. regret, winner).
t
Throughout this appendix, we’ll rewrite the definitions, theorems, and proofs of the body in terms of this
general framework (with arbitrary weights w ) in order to unify presentation across the Borda and Condorcet
t
settings. We first summarize the key results.
Note that our investigation into characterizing the minimax regret rates for this problem go beyond what is
needed for showing the main results presented in the body of the paper.
A.1 Summary of Results in Appendix for Generalized Borda Problem
Table 1: Summary of Results
Environment Model Assumption Reference Weights Regret Lower Bound Upper Bound
Fixed Winner None Known w =w K1/3T2/3 K1/3T2/3
t
Unknown w =w Ω(T)
t
GIC (Condition 1) Unknown w =w K2/3T2/3 K2/3T2/3
t
Non-Stationary None Known {w } K1/3L˜1/3 T2/3 K1/3L˜1/3 T2/3
t t Known Known
GIC (Condition 1) Unknown, aligned {w } K2/3L˜1/3 T2/3 K2/3L˜1/3 T2/3
t t Unknown Unknown
(a) We first establish a minimax regret rate of K1/3T2/3 for a known reference weight under the stationary
setting. Interestingly, the minimax regret is the same as that of the usual Borda regret, thus showing
the generic known weight problem is no harder than the usual Borda dueling bandit problem.
(b) We also establish the minimax regret rate of K1/3L˜1/3 T2/3 in the non-stationary dueling bandit
Known
problem where the underlying preference model and (known) reference weights are allowed to change,
and L˜ counts the number of significant changes in generalized winner (see Definition 2). As a
Known
reminder, our algorithm is adaptive in the sense that it does not require any knowledge of L˜ .
Known
This measure of non-stationarity generalizes Definition 1 for Borda scores and so this result shows
Theorem 1.
(c) Wenextconsiderthesettingofunknownweightsandshowit’singeneralimpossibleforasinglealgorithm
to simultaneously obtain sublinear regret for both the uniform and Condorcet winner reference weights,
√
thus ruling out getting both T2/3 Borda regret and T Condorcet winner regret.
(d) In light of said impossibility, we show that under GIC, it is in fact possible to obtain K2/3T2/3 regret
with respect to all unknown reference weights in the stationary setting. We then show that, even under
this condition, the higher dependence on K (compared to the K1/3 rate in the known weight setting)
in this regret rate is in fact tight.
(e) In the non-stationary dueling bandit problem with a changing sequence of unknown reference weights,
we introduce a notion of significant winner changes w.r.t. changing reference weights, and show that,
17under the GIC, one can simultaneously achieve a dynamic regret of K1/3L˜1/3 T2/3 in terms of
Unknown
L˜ significantshiftsw.r.t. allsequencesofreferenceweightswhicharealignedwiththesignificant
Unknown
shifts. This dynamic regret rate is in fact minimax optimal and our algorithm is adaptive to unknown
L˜ . These significant winner changes w.r.t. changing reference weights in fact capture the
Unknown
approximate winner changes of Definition 3 and so this result shows Theorem 4.
A.2 Generalized Notation Related to GBS
For ease of presentation, in the remainder of the appendix, we reparametrize the GBS b (a,w) to be in terms
. t
of the gaps in preferences δ (a,a′)=P (a,a′)− 1. Or,
t t 2
b
(a,w)=. (cid:88)
δ
(a,a′)·wa′
.
t t
a′∈[K]
Note the resulting regret notions and rates will not change under this new formulation. Now, define the GBS
.
winner as a∗(w)=argmax b (a,w) and the GBS dynamic regret as
t a∈[K] t
T
. (cid:88)1
Regret({w }T )= (2·b (a∗(w ),w )−b (i ,w )−b (j ,w )).
t t=1 2 t t t t t t t t t t
t=1
In the case of a known weight w ≡w, we’ll simplify the notation as Regret(w).
t
. .
Let δw(a′,a) = b (a′,w)−b (a,w) be the gap in GBS between arms a and a′ at time t. Let δw(a) =
t t t t
max δw(a′,a) be the absolute gap in GBS of arm a.
a′∈[K] t
A.3 Significant Winner Switches w.r.t. Known Weights
We now introduce generalizations of significant Borda winner switches (Definition 1) for arbitrary weights.
Definition 2 (Significant Winner Switches w.r.t. Known Weightings (SKW)). Fix a weight w. Define an
arm a as having significant generalized Borda regret over [s ,s ] if
1 2
(cid:88)s2
δw(a)≥K1/3·(s −s )2/3. (7)
s 2 1
s=s1
Define significant winner switches w.r.t. the known weight w (abbreviated as SKW) as follows: the
0-th sig. shift is defined as τ =1 and the (i+1)-th sig. shift τ is recursively defined as the smallest t>τ
0 i+1 i
such that for each arm a∈[K], ∃[s ,s ]⊆[τ ,t] such that arm a has significant generalized Borda regret over
1 2 i
[s ,s ]. We refer to the interval of rounds [τ ,τ ) as an SKW phase. Let L˜ be the number of SKW
1 2 i i+1 Known
phases elapsed in T rounds3.
Remark 1. Definition 2 and subsequent results (Theorem 9 and Theorem 16) for a known weight w can be
trivially generalized to the setting where there’s a fixed and known sequence of weights {w }T . For simplicity
t t=1
of presentation, we focus on the fixed weight w ≡w, which preserves the essence of the theory.
t
A.4 Significant Winner Switches w.r.t. Unknown Weights
We next introduce a generalization of Definition 3 for the generalized Borda problem with unknown weights.
In particular, we define a notion of significant shifts which roughly tracks when any SKW occurs for some
weight w.
3whileτi,L˜ Known dependonthefixedweightw,we’lldropthedependenceasneededforsakeofpresentation
18Definition 3 (Significant Winner Switches w.r.t. Unknown Weightings (SUW)). Define an arm a as having
significant worst-case generalized Borda regret over [s ,s ] if
1 2
(cid:88)s2
max δw(a)≥K2/3·(s −s )2/3. (8)
s 2 1
w∈∆K
s=s1
We then define generalized significant winner switches ρ ,ρ ,... in an analogous manner to Definition 2.
0 1
Let L˜ be the number of generalized SUW phases.
Unknown
Remark 1. Under GIC, L˜ is tighter than the number of changes of the winner arm a∗.
Unknown t
Although SUW are a stronger notion of shift than SKW (i.e., L˜ ≥L˜ ), the SUW can be learned
Unknown Known
in a more general setting where one aims to minimize dynamic regret w.r.t. any sequence of changing weights
{w }T which are aligned with the SUW, defined as follows.
t t=1
Definition 4 (Aligned Sequence of Weights). We say a sequence {w }T of weights is aligned with SUW if
t t=1
the weight w does not change for rounds t lying in SUW phase [ρ ,ρ )
t i i+1
Inparticular,asmentionedinSec.5,settingw astheCondorcetwinnerweightw(ac)recoverstheCondorcet
t t
regret problem Quite importantly, in this setting, the weights w are unknown and so their changepoints are
t
also unknown since the SUW are.
B Regret Lower Bounds for GBS
B.1 Stationary Regret Lower Bound (Known Weight)
We first show that, in the stationary setting, the problem of minimizing generalized Borda regret w.r.t. any
fixed and known weight w is as hard as the uniform Borda problem (i.e., K1/3·T2/3 minimax regret).
Theorem 5 (Lower Bound on Regret in Stochastic Setting). For any algorithm and any reference weight
w ≡w, there exists a preference matrix P, with K ≥3, such that the expected regret is:
t
E[Regret(w)]≥Ω(K1/3·T2/3).
Proof. We construct an environment similar to the lower bound construction for the uniform Borda score
(Saha et al., 2021, Lemma 14). As in said result, it will in fact suffice to construct an environment forcing a
regret lower bound (w.r.t. weight w) of order Ω(min{T ·ε,K/ε2}) for ε∈(0,0.05). Taking ε:=(K/T)1/3
will then give the desired result, since (K/T)1/3 <0.05 for large enough value of T/K. For T/K smaller than
a constant, we may take ε small enough so that K ≥(cid:0)T(cid:1)2/3 ·K =T2/3K1/3 to conclude.
ε2 K
Suppose K is even; if K is odd, we may reduce to a dueling bandit problem with K −1 arms by setting
the gaps δ(a,a′) to zero for some fixed arm a∈[K] and any other arm a′ ∈[K]. First observe that for any
reference weight w, there must exist a set of arms I of size at most K/2 such that its mass under w is
w(I) ≥ 1/2. This follows since either the set of arms {1,...,K/2} or {K/2+1,...,K} has mass at least
1/2 under the distribution w(·). We then partition the K arms into a good set G := [K]\I and bad set
B :=I. Without loss of generality, suppose G ={1,...,K/2}. We’ll consider K/2+1 different environments
E ,E ,...,E where in E for a∈[K/2], arm a will maximize the generalized Borda score b(·,w).
0 1 K/2 a
Specifically, we set the preference matrix P≡P as follows:
t
Environment E : Let the preference matrix P have entries
0
(cid:40)
0.5 a,a′ ∈G or a,a′ ∈B
P(a,a′)= .
0.9 a∈G,a′ ∈B
19In this environment, the generalized Borda score is
(cid:40)
0.4·w(B) a∈G
b(a,w)= ,
−0.4·w(G) a∈B
where w(·) denotes the distribution on ∆[K] induced by weight w.
The alternative environments E ,...,E will be small perturbations of E where an arm a∈G will have
1 K/2 0
the highest generalized Borda score by a margin of ε.
Environment E for a∈[K/2]: Let the preference matrix P be identical to that of environment E except
a 0
in the entries P(a,a′) = 0.9+ε for a′ ∈ B. Thus, in this environment the generalized Borda scores are
identical to that of E except for b(a,w)=(0.4+ε)·w(B), meaning arm a is the winner w.r.t. weight w.
0
Let E [·],P (·) denote the expectation and probability measure of the induced distributions on observations
Ea Ea
and decisions under environment E . Now, the expected regret on environment E is lower bounded by
a a
T (cid:32) (cid:34) T (cid:35)(cid:33)
(cid:88) (cid:88)
E [Regret(w)]≥ ε·E [111{(i ,j )̸=(a,a)}]=ε· T −E 111{(i ,j )=(a,a)} .
Ea Ea t t Ea t t
t=1 t=1
Letting U be a uniform prior over {1,...,K/2}, we have the expected regret over a random environment
drawn from U is lower bounded by:
(cid:32) (cid:34) (cid:34) T (cid:35)(cid:35)(cid:33)
(cid:88)
E [E [Regret(w)]]≥ε· T −E E 111{(i ,j )=(a,a)} . (9)
a∼U Ea a∈U Ea t t
t=1
Let N (a):=(cid:80)T 111{(i ,j )=(a,a)} be the (random) arm-pull count of arm a. Then, since N (a)≤T for
T t=1 t t T
all values of a, by Pinsker’s inequality (see Saha and Gaillard, 2022, proof of Lemma C.1), we have
(cid:114)
KL(P ,P )
E [N (a)]−E [N (a)]≤T 0 a ,
Ea T E0 T 2
where P is the induced distribution on the history of observations and decisions under environment E .
a′ a′
Taking a further expectation over a∈U and using Jensen’s inequality, we obtain:
(cid:118)
1 (cid:88) (cid:117) 2 (cid:88)
E [E [N (a)]]≤ E [N (a)]+T(cid:117) KL(E ,E ). (10)
a∼U Ea T K/2 E0 T (cid:116)K 0 a
a∈{1,...,K/2} a∈{1,...,K/2}
WenowaimtoboundthislastKLterm. LettingH bethehistoryofrandomness,observations,anddecisions
t
till round t: H := {a}∪{(i ,j ,O (i ,j ))} for t ≥ 1 and H := {a}. Let Pt denote the marginal
t s s s s s s≤t 0 a
distribution over the round t data (i ,j ,O (i ,j )) under the realization of environment E . By the chain
t t t t t a
rule for KL, we have
T T K
(cid:88) (cid:88) (cid:88)
KL(P ,P )= KL(Pt |H ,Pt |H )= P ({i ,j }={a,i})·KL(Ber(0.9),Ber(0.9+ε)).
0 a 0 t−1 a t−1 E0 t t
t=1 t=1i=K/2+1
(11)
Next, observe the following bound on the KL between Ber(0.9) and Ber(0.9+ε):
(cid:18) (cid:19) (cid:18) (cid:19)
0.9 0.1
KL(Ber(0.9),Ber(0.9+ε))≤0.9·log +0.1·log .
0.9+ε 0.1−ε
By elementary calculations, the above is less than 10·ε2 for any ε∈(0,0.05).
20Now, suppose the algorithm incurs regret at most ϵ·T on all environments E , lest we be done. Then, the
a
total number of times a bad arm in B is played must be at most ϵ·T/0.4 or for all a∈{0,1,...,K/2+1}:
(cid:34) T (cid:35)
(cid:88) ε·T
E 111{{i ,j }∩B ≠ ∅} ≤ .
Ea t t 0.4
t=1
Plugging the above two steps into (11) yields
2 (cid:88) 20ε2 (cid:88) (cid:88)K (cid:34) (cid:88)T (cid:35) 50·ε3·T
KL(P ,P )≤ E 111{{i ,j }={a,i}} ≤ .
K 0 a K E0 t t K
a∈{1,...,K/2} a∈{1,...,K/2}i=K/2+1 t=1
Then, plugging the above into (10) yields
(cid:114)
1 (cid:88) 50ε3
E [E [N (a)]]≤ E [N (a)]+T ·T.
a∼U Ea T K/2 E0 T K
a∈{1,...,K/2}
Thus, plugging the above steps into (9) gives
(cid:32) (cid:32) (cid:114) (cid:33)(cid:33)
T 50ε3
E [Regret(w)]≥ε· T − +T ·T .
a∈U,Ea K/2 K
Now, suppose 50ε3·T ≤1/50. Then, the above RHS is lower bounded by ε·T/2500 for K ≥3.
K
It remains to handle the case of 50ε3·T >1/50 =⇒ T ≥ K . Suppose, for contradiction, that for all such
K 2500ε3
T we have regret at most K . Then, the regret over just the first T := K rounds must also be at
25002ε2 0 2500ε3
most K =ε·T /2500. However, this contradicts the previous case’s lower bound for T =T . ■
25002ε2 0 0
B.2 Hardness of Learning all Generalized Winners
We next turn our attention to minimizing stationary regret w.r.t an unknown, but fixed, weight w, which the
following theorem asserts is hard.
Theorem6(LowerBoundonRegretwithUnknownWeight). ThereexistsastochasticproblemP∈[0,1]K×K
such that for any algorithm, there is a reference weight w∈∆K such that:
E[Regret(w)]≥T/45.
Proof. Consider the following stationary preference matrix:
 
1/2 3/5 3/5
P:=2/5 1/2 1 
2/5 0 1/2
In this environment arm 1 is the Condorcet winner while arm 2 is the Borda winner w.r.t. the uniform
reference weight. For the Condorcet weight w(1), both arms 2 and 3 have a gap of 1/10. On the other hand,
the Condorcet winner has a gap in Borda scores of 1/15 to arm 2. However, any algorithm must play one of
arms 1, 2, or 3 at least T/3 times in expectation. No matter which arm is played T/3 times, there is a weight
for which its generalized Borda regret is at least T/45. ■
Remark 2. In fact, the lower bound of Theorem 6 holds even for an algorithm knowing the preference matrix
P. The task of minimizing generalized Borda regret for an unknown reference weight w can also be cast as a
partial monitoring problem with fixed feedback P but unknown matrix of losses decided by the adversary’s
weight. It’s straightforward to verify that this game is not globally observable, meaning the minimax
expected regret is Ω(T) by the classification of finite partial monitoring games (Bartók et al., 2014).
21B.3 Stationary Regret Lower Bound for Unknown Weight under GIC
Under GIC (Condition 1), we next focus on deriving regret lower bounds in stochastic environments where
there is a fixed winner arm a∗ ≡a∗ across all rounds. Even under GIC, the best rate we can achieve for all
t
weights is T2/3. In particular, so long as an algorithm attains optimal uniform Borda regret it cannot achieve
√
T Condorcet regret.
Theorem7(LowerBoundonAdaptiveRegretwithGIC). FixanyalgorithmsatisfyingE[Regret(Unif{[K]})]≤
C·T2/3 for all K =3 armed dueling bandit instances. Then, there exists a stochastic problem P∈[0,1]3×3
satisfying Condition 1 such that for the specialization w=w(a∗) and sufficiently large T, we have:
E[Regret(w)]≥Ω(T2/3/C2).
Proof. Consider the preference matrix for ε:=4C·T−1/3:
   
1/2 1/2 0.9+ε 1/2 1/2 0.9
P1 = 1/2 1/2 0.9 ,P2 =1/2 1/2 0.9+ε.
0.1−ε 0.1 1/2 0.1 0.1−ε 1/2
Set T large enough so that ε<0.05.
We first sketch out the intuition behind the proof. In P1, arm 1 is both the Condorcet winner and the Borda
winner, while in P2, arm 2 is. The argument will go as follows: first, the algorithm must identify the winner
arm and whether we’re in P1 or P2 since otherwise it pays more than C·T2/3 uniform Borda regret. The
only way to identify the winner arm, however, is to play arm 3 at least Ω(T2/3) times. This forces a Ω(T2/3)
Condorcet winner regret.
Specifically, for a∈{1,2} let a− :={1,2}\{a} denote the other potential winner arm. Then, we have
(cid:34) T (cid:35)
(cid:88) 3T
E [Regret(Unif{[3]})]≤C·T2/3 =⇒ E 111{{a−,3}∩{i ,j }≠ ∅} ≤ .
Pa Pa t t 4
t=1
Then, using a Pinsker inequality argument similar to the proof of Theorem 5,
(cid:34) T (cid:35)
(cid:88)
E [Regret(w(a))]≥ε·E 111{{3,a−}∩{i ,j }≠ ∅}
Pa Pa t t
t=1
(cid:32) (cid:34) T (cid:35)(cid:33)
(cid:88)
≥ε· T −E 111{i =j =a}
Pa t t
t=1
(cid:32) (cid:32) (cid:34) T (cid:35) (cid:114) (cid:33)(cid:33)
≥ε· T − E
(cid:88)
111{i =j =a} +T
KL(P a−,P a)
,
Pa− t t 2
t=1
where once again P ,P denote the corresponding induced distributions on all random variables. We next
a a−
bound the above KL divergence by chain rule and our earlier bound on KL(Ber(0.9),Ber(0.9+ε)):
(cid:34) T (cid:35)
(cid:88)
KL(P ,P )≤10ε2·E 111{{i ,j }={a,3}}+111{{i ,j }={a−,3}}
a− a P a− t t t t
t=1
Next, note that arm 3 has a uniform Borda gap of at least 0.4·2/3 in either P1 or P2. This means we must
have
(cid:34) (cid:88)T (cid:35) C·T2/3 ε·T
E 111{{i ,j }={a,3}}+111{{i ,j }={a−,3}} ≤ = .
P a− t t t t 0.4·2/3 0.4·8/3
t=1
Thus, our KL bound from earlier becomes 75 ·ε3·T.
8
22Plugging this into our earlier inequality, we have a regret lower bound of
(cid:32) (cid:32) (cid:114) (cid:33)(cid:33)
3T 75
ε· T − +T ·ε3·T .
4 8
By a similar argument to the proof of Theorem 5, the above is order Ω(min{ε·T,1/ε2}) by analyzing the
two cases ε3·T ·(75/8)>8/75 and ε3·T ·(75/8)≤8/75.
Now, plugging in our value of ε, this last rate is of order T2/3·min{C,C−2}. ■
As a consequence, Theorem 7 prohibits using Condorcet regret minimizing algorithms in this setting, and
hence new algorithmic techniques are required for efficiently learning winners under GIC.
We next investigate the dependence on K in the minimax regret rate. We show that, to achieve optimal
regret for all (unknown) weights adaptively, we need to suffer a higher dependence on K of K2/3, compared
to the known and fixed weight setting with dependence K1/3 (see Theorem 5).
Theorem 8 (Lower Bound on Adaptive Regret under GIC). Fix any algorithm. Then, for K > 3, there
exists a dueling bandit problem P∈[0,1]K×K satisfying Condition 1 such that there is a fixed weight w∈∆K
with:
E[Regret(w)]≥Ω(min{K2/3T2/3,T}).
Proof. We first sketch the argument. We’ll consider a variant of the environments E ,E ,...,E introduced
0 1 K/2
in the proof of Theorem 5. Recall there is a good set G :={1,...,K/2} and a bad set B :={K/2+1,...,K}
of arms. In every environment, the good arms (resp. bad arms) have a gap of zero when compared to each
other. In E , each good arm has a gap of 0.4 to each bad arm. Now, we introduce a further refinement of this
0
construction and define the environment E for a∈G and a′ ∈B as identical to the environment E except
a,a′ 0
P(a,a′)=0.9+ε. Now, the sample complexity of finding the winner arm a in environment E will be K2
a,a′ ε2
whence we’ll pay a regret of min{ε·T,K2·ε−2}. Setting ε∝K2/3T−1/3 then forces K2/3·T2/3 regret.
Let U be a uniform prior over pairs of arms (a,a′) for a∈G and a′ ∈B. Then, we have
(cid:32) (cid:34) (cid:34) T (cid:35)(cid:35)(cid:33)
(cid:88)
E E [Regret(w(a′))]≥ε· T −E E 111{(i ,j )=(a,a)} .
(a,a′)∼U E a,a′ (a,a′)∼U E a,a′ t t
t=1
Now, by Pinsker’s inequality we have
(cid:115)
(cid:104) (cid:105) 1 (cid:88) 1 (cid:88)
E E [N (a)] ≤ E [N (a)]+T KL(E ,E ),
(a,a′)∼U E a,a′ T (K/2)2 E0 T (K/22) 0 a,a′
a∈G,a′∈B a∈G,a′∈B
whererecallN (a):=(cid:80)T 111{(i ,j )=(a,a)}. Next,weboundthisKLinanidenticalwaytothecalculations
T t=1 t t
of the proof of Theorem 5:
(cid:34) T (cid:35)
(cid:88)
KL(E ,E )≤10·ε2·E 111{{i ,j }={a,a′}} .
0 a,a′ E0 t t
t=1
Now, suppose the algorithm incurs regret at most ε·T on environment E lest we be done. This means it
0
cannot pull arms a′ ∈B more than T ·ε/0.4 times in total, or
 
T
(cid:88) (cid:88) T ·ε
E E0 111{{i t,j t}={a,a′}}≤
0.4
.
a∈G,a′∈Bt=1
Thus,
4 (cid:88) 100·ε3·T
KL(E ,E )≤ .
K2 0 a,a′ K2
a∈G,a′∈B
23Plugging this KL bound into our earlier Pinsker bound, we obtain a regret lower bound of
(cid:32) (cid:32) (cid:114) (cid:33)(cid:33)
4T 100·ε3·T
E E [Regret(w(a′))]≥ε· T − +T .
(a,a′)∼U E a,a′ K2 K2
By an analogous argument to the proof of Theorem 5, the above is lower bounded by Ω(min{ε·T,K2ε−2}).
■
C Dynamic Regret Lower Bounds
C.1 Known Weight Setting
Now, for the known weight setting, the K1/3·T2/3 lower bound of Theorem 5 can naturally be extended to a
dynamic regret lower bound of order L˜1/3 ·T2/3·K1/3 over L˜ SKW phases. The proof techniques
Known Known
are routine, similar to arguments already used for showing dynamic regret lower bounds for Condorcet regret
(Saha and Gupta, 2022, Section 5), and will not be done in detail here to avoid redundancy.
Theorem 9 (Fixed Weight Dynamic Regret Lower Bound in Terms of SKW). For L ∈ [T], let P(L) be
the class of environments with SKW L˜ (w)≤L. For any algorithm, we have there exists a changing
Known
environment in P(L) such that the dynamic regret with respect to known weight w is lower bounded by
E[Regret(w)]≥Ω(L1/3·T2/3·K1/3).
Proof. (Sketch) We can take the lower bound construction of Theorem 5 over a horizon of length T/L to
force a regret of K1/3·(T/L)2/3. Repeating this construction with a new randomly chosen winner arm every
T/L rounds forces a total regret of K1/3·L1/3·T2/3 while satisfying L˜ (w)≤L. ■
Known
Theorem 10 (Fixed Weight Dynamic Regret Lower Bound in Terms of Total Variation). For V ∈[0,T]∩R,
let P(V) be the class of environments with total variation V at most V. For any algorithm, we have there
T
exists a changing environment with total variation V such that the dynamic regret with respect to known
T
weight w is lower bounded by
E[Regret(w)]≥Ω(V1/4·T3/4·K1/4+K1/3·T2/3).
Proof. (Sketch) The argument is analogous to the proof of Theorem 5.2 in Saha and Gupta (2022). First,
assume T1/4·V3/4·K−1/4 ≥1. Then, using the previous lower bound construction forcing regret of order
L1/3·T2/3·K1/3, we can use the specialization L∝T1/4·V3/4·K−1/4 which ensures the total variation V
T
at most
L·(K·L/T)1/3 =L4/3·K1/3·T−1/3 ≤V
and forces a regret lower bound of order
L1/3·T2/3·K1/3 ∝T3/4·K1/4·V1/4.
If T1/4·V3/4·K−1/4 <1, then V1/4·T3/4·K1/4 <K1/3·T2/3 which means it suffices to establish a regret
lower bound of order K1/3·T2/3 which is already evident from Theorem 5. ■
Remark 3. Note the optimal dependence on V , T, and K in Theorem 10 differ from the minimax Condorcet
T
dynamic regret rate of V1/3·T2/3·K1/3. This arises from the different stationary minimax regret rates (T2/3
T
versus T1/2) of Condorcet versus Borda regret.
24C.2 Unknown Weight Setting
By analogous arguments as in the previous section, except now using the stationary lower bound construction
of Theorem 8 as a base template, we have the dynamic regret is lower bounded by L1/3·T2/3·K2/3 for
environments SUW count L˜ ≤L.
Unknown
For total variation budget V, we claim the regret lower bound is of order V1/4·T3/4·K1/2+K2/3·T2/3. In
particular, we note that if V =O(K2/3T−1/3), then V1/4K3/4K1/2 =O(K2/3T2/3) so that it suffices to use
.
thestationaryregretboundofTheorem8. Otherwise,wehaveV =Ω(K2/3T−1/3)andL=V3/4T1/4K−1/2 =
Ω(1) so that a regret lower bound of order L1/3T2/3K2/3 = Ω(V1/4T3/4K1/2) holds. At the same time
L·(K2/T)1/3 ≤V so that the constructed environment has total variation at most V.
D Fixed Winner Regret Analysis
As a warmup to proving the dynamic regret upper bounds, we’ll first show a regret upper bound in easier
fixed winner environments where there is a fixed winner arm either (1) with respect to a known weight or (2)
with respect to all unknown weights under Condition 1.
These “simpler” analyses will reappear and serve as a core template for the more complicated dynamic regret
analyses of Appendix E (known weight) and Appendix F (unknown weight).
D.1 Preliminaries and Concentration Bounds for Estimation
Throughout the regret upper bound analyses c ,c ,... will denote positive constants not depending on T or
1 2
any distributional parameters.
We first recall a version of Freedman’s martingale concentration inequality, which has become standard in
adaptive non-stationary bandit analyses (Suk and Agarwal, 2023; Buening and Saha, 2023; Suk and Kpotufe,
2022).
Lemma 11 (Theorem1ofBeygelzimeretal.(2011)). Let X ,...,X ∈R be a martingale difference sequence
1 n
withrespecttosomefiltration{F ,F ,...}. AssumeforalltthatX ≤Ra.s. andthat(cid:80)n E[X2|F ]≤V
0 1 t i=1 i i−1 n
a.s. for some constant V only depending on n. Then for any δ ∈(0,1) and λ∈[0,1/R], with probability at
n
least 1−δ, we have:
n
(cid:88) (cid:16)(cid:112) (cid:17)
X ≤(e−1) V log(1/δ)+Rlog(1/δ) .
i n
i=1
We next apply Lemma 11 to bound the estimation error of our estimatesˆb (a,w) (see (5) in Subsec. 6.1)
t
Proposition 12. Let E be the event that for all rounds s <s and all arms a∈A for all t∈[s ,s ], and
1 1 2 t 1 2
weight w∈W:
(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
t(cid:88) =s2 s1ˆb t(a,w)− t(cid:88) =s2 s1E(cid:104) ˆb t(a,w)|F t−1(cid:105)(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)≤10(e−1)log(T|W|) (cid:118) (cid:117) (cid:117) (cid:116)(cid:32) s(cid:88) =s2 s1K(cid:88)
a′
q
sw (a2 a′ ′)(cid:33) +K· t∈m [sa 1,x s2]η t−1 .
(12)
where F :={F }T is the canonical filtration generated by observations and randomness of elapsed rounds.
t t=1
Then, E occurs with probability at least 1−1/T2.
1
Proof. First, note for any arm a ∈ [K] and weight w, the random variable ˆb (a,w)−E[ˆb (a,w)|F ] is
t t t−1
a martingale difference bounded above by max K ·η−1. Then, in light of Lemma 11, it suffices to
t∈[s1,s2] t
25compute the variance ofˆb (a). We have for arm a active at round t (i.e., a∈A ):
t t
 2
E[ˆb2 t(a)|H t−1]≤E  (cid:88) 111{i t =a,j t = q (a a′ )} ·· q(O (at ′( )a,a′)−1/2) ·w a′  
t t
a′∈[K]
 
=E  (cid:88) 111{( qi t 2, (j at )) ·= q2( (a a, ′a )′)} ·w a2 ′
a′∈[K] t t
(cid:88) w2
= a′
q (a)·q (a′)
t t
a′∈[K]
(cid:88) w2
≤K a′ ,
q (a′)
t
a′
where the last inequality follows from the fact that q (a)≥1/K by virtue of a∈A . Then, the result follows
t t
from Lemma 11 and taking union bounds over arms a, weight w∈W, and rounds s ,s . ■
1 2
We next establish separate applications of this concentration bound for the setting of known weights and
unknown weights. For these settings, we respectively set W ={w} for known weight w and W ={w(a),a∈
[K]} for unknown weights.
Known Reference Weight. First, in the case of a known and fixed reference weight W ={w}, we can
observe:
(cid:88) w2 (cid:88)
q (a)≥η ·w =⇒ K a′ ≤K w ·η−1 =K·η−1. (13)
s s a q (a′) a′ s s
s
a′ a′
Unknown Reference Weight. For an unknown reference weight, and when W = {w(a),a ∈ [K]}, we
have the algorithm plays, w.p. η at round t, from Unif{[K]} so that:
t
(cid:88) w2 K2
q (a)≥η /K =⇒ K a′ ≤ . (14)
t t q (a′) η
s t
a′
Now, combining (13) and (14) with (12) yields the following parameter specifications for use in the known vs.
unknown weight setting.
Definition 5 (Parameter Specifications). We define two parameter specifications for use in Algorithm 1
(for known vs. unknown weights). We define the known weight specification w.r.t. weight w∈∆K via
. .
W ={w}, γ =min{K1/3·t−1/3,1},
t
(cid:118) 
F([s 1,s 2])=. K1/3·(s 2−s
1)2/3∨(cid:117)
(cid:117) (cid:116)K
(cid:88)s2
η s−1+K max η s−1 . (15)
s=s1
s∈[s1,s2]
.
The unknown weight specification is defined via W = {w(a),a ∈ [K]} (i.e., the point-mass weights),
.
γ =min{K2/3·t−1/3,1}, and
t
(cid:118) 
F([s 1,s 2])=. K2/3·(s 2−s
1)2/3∨(cid:117)
(cid:117) (cid:116)K2
(cid:88)s2
η s−1+K max η s−1 . (16)
s=s1
s∈[s1,s2]
We next establish an elementary helper lemma which asserts that the intervals of rounds [s ,s ] over which
1 2
we evict an arm a in (6) must be at least Ω(K) rounds in length for the fixed weight specification and at least
Ω(K2) rounds in length for the unknown weight specification. This will serve useful in further simplifying the
concentration bound of (12) throughout the regret analysis, as needed.
26Lemma 13. On event E , letting F([s ,s ]) be as in (15) with the fixed weight specification, we have that the
1 1 2
eviction criterion (6) holding over interval [s ,s ] implies
1 2
s −s ≥K/8.
2 1
Letting F([s ,s ]) be as in (16) with the unknown weight specification, we have
1 2
s −s ≥K2/8.
2 1
Proof. By concentration (Proposition 12) and since the generalized Borda gaps δw(a′,a) are bounded above
t
by 1, eviction of an arm a over [s ,s ] under the fixed weight specification implies
1 2
(cid:88)s2
2·(s −s )≥s −s +1≥ δw(a)≥K1/3·(s −s )2/3.
2 1 2 1 s 2 1
s=s1
The above implies s −s ≥K/8. For the unknown weight specification, we repeat the above argument with
2 1
K1/3 replaced by K2/3. ■
D.2 Regret Upper Bound under Fixed Winner for Known Weight
Theorem 14 (Regret Upper Bound for Known Weight). Algorithm 1 with the fixed weight specification (see
Definition 5) w.r.t. fixed weight w, satisfies in any environment with a fixed winner a∗ ≡a∗(w):
t
E[Regret(w)]≤O˜(K1/3·T2/3).
Proof. We first note the regret bound is vacuous for T <K; so, assume T ≥K.
WLOG suppose arms are evicted in the order 1,2,...,K at respective times t ≤t ≤···≤t (if arm a is
1 2 K
not evicted, let t :=T +1. Then, we can first decompose the regret depending on whether we play an active
a
arm or explore other arms with probability η :
t
(cid:34) T (cid:35) T  T 
(cid:88) (cid:88) (cid:88) (cid:88)
E 2b t(a∗,w)−b t(i t,w)−b t(j t,w) ≤ η t+E  (2·b t(a∗,w)−b t(a,w)−b t(a′,w))·q t(a).
t=1 t=1 t=1a,a′∈At
Using the fixed specification for η =(K/t)1/3 from Definition 5, we have first term on the RHS above (our
t
exploration cost) is of order K1/3·T2/3.
Now,inlightofourconcentrationboundProposition12,itsufficestoboundtheregretonthehigh–probability
good event E since the total regret is negligible outside of this event. For the second expectation, using our
1
fixed-weight concentration bound (13) we get that the regret of playing arm a as a candidate is at most (on
the good event E ):
1
(cid:118) 
111{E
1}t (cid:88) ta =− 11 b t(a∗,w |) A− t|b t(a,w) ≤10·(e−1)log(T)(cid:117) (cid:117) (cid:116)Kt (cid:88) ta =− 11
η t−1+K· t∈[m 1,ta ax −1]η t−1 . (17)
Note that |A |≥K+1−a since we assumed WLOG that arm a is the a-th arm to be evicted. Additionally,
t
plugging in the specialization for η according to the fixed weight specification of Definition 5, we have that
t
t (cid:88)a−1 t (cid:88)a−1(cid:18)
t
(cid:19)1/3
K η−1 ≤K2+K ≤K2+c K2/3·(t −1)4/3,
t K 1 a
t=1 t=1
K· max η−1 =K2/3·(t −1)1/3.
t a
t∈[1,ta−1]
27Next, by Lemma 13, we must have t −1 ≥ K/8 so that K2/3·(t −1)1/3 is of order K1/3·t2/3. By the
a a a
same reasoning, K2 is of order K2/3·(t −1)4/3.
a
Then, plugging the above two displays into (17) and summing the inequality over arms a gives:
2·E(cid:34)
111{E
}(cid:88)K t (cid:88)a−1 b t(a∗,w)−b t(a,w)(cid:35) ≤(cid:88)K c 2log(T)·K1/3·t2 a/3
.
1 |A| K+1−a
a=1 t=1 a=1
Upper bounding each t by T, we obtain a regret bound of order log(K)·log(T)·K1/3T2/3. ■
a
D.3 Regret Upper Bound under Fixed Winner for Unknown Weight
Theorem 15 (Adaptive Regret Upper Bound for Unknown Weights). Suppose Condition 1 holds and there
is a winner arm a∗ ≡ a∗ fixed across time. Then, Algorithm 1 with the unknown weight specification (see
t
Definition 5) satisfies for all weights w∈∆K:
E[Regret(w)]≤O˜(K2/3·T2/3).
Proof. We first show that it suffices to bound regret w.r.t. the point-mass weights w(a) ∈ W for a ∈ [K].
This is true since the regret is linear in the reference weight w:
T (cid:34) T (cid:35) T
(cid:88) (cid:88) (cid:88)
b (a∗,w)−b (q ,w)=E δ (a∗,a′)−δ (q ,a′) ≤ max b (a∗,w(a))−b (q ,w(a)).
t t t a′∼w t t t t t t
a∈[K]
t=1 t=1 t=1
Now, in light of our concentration bound (13) from Proposition 12, it suffices to bound the regret on the
high–probability good event E since the total regret is negligible outside of this event.
1
We then follow the recipe of the proof of Theorem 15 (see Appendix D.2) except now using the concentration
.
bound of Proposition 12 with (14) and the unknown weight specification η =K2/3/t1/3. First, note that it’s
t
clear the exploration cost (cid:80)T η for the unknown weight specification is order K2/3·T2/3.
t=1 t
Now, suppose WLOG that the arms are evicted in the order 1,2,...,K at times t ≤t ≤···≤t . Then,
1 2 K
Proposition 12 gives us a bound with respect to any point-mass weight in W:
(cid:118) 
111{E 1} am ′∈a [Kx
]t (cid:88) ta =− 11 b t(a∗,w(a′) |) A− t|b t(a,w(a′)) ≤10·(e−1)log(T)(cid:117) (cid:117) (cid:116)K2t (cid:88) ta =− 11
η t−1+K· t∈[m 1,ta ax −1]η t−1 . (18)
Crucially, note that the fixed winner arm a∗ is never evicted by Condition 1. Now, we have
t (cid:88)a−1 t (cid:88)a−1 t1/3
K2 η−1 ≤K4+K2 ≤K4+c K4/3·(t −1)4/3,
t K2/3 3 a
t=1 t=1
K· max η−1 =K1/3·(t −1)1/3.
t a
t∈[1,ta−1]
Now, by Lemma 13, we have since s −s ≥ K2/8, the K4 term in the first display above is of order
2 1
K4/3·(t −1)4/3.
a
Then, plugging the above two displays into (18) and summing over arms a∈[K], we have:
2E(cid:34)
111{E
}(cid:88)K (cid:88)ta b t(a∗,w)−b t(a,w)(cid:35) ≤(cid:88)K c 4log(T)·K2/3·t2 a/3
.
1 |A| K+1−a
a=1t=1 a=1
As before, upper bounding each t by T, we obtain a regret bound of order log(K)·log(T)·K2/3·T2/3. ■
a
28E Nonstationary Regret Analysis for Known Weights
E.1 Formal Statement of Results
We first state formally the upper bound on dynamic regret w.r.t. known weight w in terms of the SKW
(Definition 2). In particular, we show matching upper bounds, up to log terms of the lower bounds of
Appendix C.1. As a reminder, taking w to be the uniform weight, we recover the Borda dynamic regret and
so the below results generalize Theorem 1 and Corollary 2.
Theorem 16. Algorithm 2 with the fixed weight specification (see Definition 5) w.r.t. fixed weight w satisfies:
 
E[Regret(w)]≤O˜
L˜ Kn(cid:88)own(w)
K1/3·(τ i+1(w)−τ i(w))2/3 .
i=0
Corollary 17. By Jensen’s inequality, the regret bound of Theorem 16 is further upper bounded by
K1/3T2/3L˜1/3 . Furthermore, relating SKW to total variation, a regret bound of order V1/4 · T3/4 ·
Known T
K1/4+K1/3·T2/3 also holds in terms of total variation quantity V (see Appendix E.7 for proof).
T
E.2 Proof of Theorem 16
The broad outline of our regret analysis will be similar to prior works on adaptive non-stationary (dueling)
bandits (Suk and Agarwal, 2023; Buening and Saha, 2023; Suk and Kpotufe, 2022). Our first goal is to show
that episodes [t ,t ) align with the SKW phases, in the sense that a new episode is triggered only when an
ℓ ℓ+1
SKW has occurred.
Recall from Algorithm 2 that [t ,t ) represents the ℓ-th episode. As a notation, we suppose there are T
ℓ ℓ+1
total episodes and, by convention, we let t :=T +1 if only ℓ−1 episodes occurred by round T.
ℓ
E.2.1 Episodes Align with SKW Phases
Lemma 18. On event E , for each episode [t ,t ) with t ≤T (i.e., an episode which concludes with a
1 ℓ ℓ+1 ℓ+1
restart), there exists an SKW τ ∈[t ,t ).
i ℓ ℓ+1
Proof. We first note thatˆb (a,w) is an unbiased estimator for b (a,w) in the sense that E[ˆb (a,w)|F ]=
t t t t−1
b (a,w) if a∈A . Then, by concentration (Proposition 12) and our eviction criteria (6), we have that arm a
t t
being evicted from A over the interval [s ,s ] using the eviction threshold (15) implies
t 1 2
(cid:88)s2
b (a∗,w)−b (a,w)≥c (s −s )2/3·K1/3.
s s s 5 2 1
s=s1
This means arm a incurs significant generalized Borda regret w.r.t. w over [s ,s ]. Since a new episode is
1 2
triggered only if all arms are evicted from A , there must exist an SKW in each episode [t ,t ). ■
global ℓ ℓ+1
E.3 Decomposing the Regret
Let a♯ denote the last safe arm at round t, or the last arm to incur significant generalized Borda regret
t
w.r.t. w in the unique phase [τ ,τ ) containing round t, per Definition 2. Furthermore, let a denote the
i i+1 ℓ
last global arm of episode [t ,t ) or the last arm to be evicted from A in said episode. Then, we can
ℓ ℓ+1 global
decompose the per-episode regret:
(cid:34)tℓ(cid:88)+1−1 (cid:35) (cid:34)tℓ(cid:88)+1−1 (cid:35) (cid:34)tℓ(cid:88)+1−1 (cid:35) (cid:34)tℓ(cid:88)+1−1 (cid:35)
E δw(i )+δw(j ) =E δw(a♯) +E δw(a ,i )+δw(a ,j ) +E δw(a♯,a ) .
t t t t t t t ℓ t t ℓ t t t ℓ
t=tℓ t=tℓ t=tℓ t=tℓ
(19)
29We next handle each of the expectations on the RHS separately. Our goal will be to show that each of the
expectations above is of order
 
(cid:88) 1
E 111{E 1} K1/3·(τ i+1−τ i)2/3 + T. (20)
i∈[L˜ Known]:[τi,τi+1)∩[tℓ,tℓ+1)̸=∅
Admitting this goal, summing the regret over episodes while using Lemma 18 to ensure each SKW phase
[τ ,τ ) only intersects at most two episodes will yield the desired total regret bound. This will follow in a
i i+1
nearly identical manner to Section 5.5 of Suk and Kpotufe (2022).
Now, the three expectations on the RHS of (19) are respectively:
• The per-episode dynamic regret of the last safe arm (analyzed in Appendix E.4)
• The per-episode regret of the played arms {i ,j } to the last global arm (analyzed in Appendix E.5)
t t
• The per-episode regret of the last global arm to the last safe arm (analyzed in Appendix E.6).
E.4 Bounding the per-Episode Regret of the Safe Arm
By the definition of SKW (Definition 2), the safe arm a♯ is fixed for t ∈ [τ ,τ ) and has dynamic regret
t i i+1
upper bounded by K1/3·(s −s )2/3 on any subinterval [s ,s ]⊆[τ ,τ ). In particular, letting [s ,s ] be
2 1 1 2 i i+1 1 2
the intersection [τ ,τ )∩[t ,t ) (which is necessarily an interval), we have that the dynamic regret of a♯
i i+1 ℓ ℓ+1 t
on episode [t ,t ) is at most order (20).
ℓ ℓ+1
E.5 Bounding per-Episode Regret of Active Arms to Last Global Arm
This will follow a similar argument as our regret analysis in fixed winner environments (Theorem 14). Recall
that the global learning rate η is the learning rate γ set by the base algorithm which is active at round
t t−tstart
t. Note that η is a random variable which depends on the scheduling of base algorithms in episode [t ,t ).
t ℓ ℓ+1
We first observe that the play distribution q plays an arm according to weight w with probability η and
t t
plays an arm chosen from A uniformly at random with probability 1−η . Let a be a random draw from the
t t
distribution q be the play distribution at round t, which is measurable w.r.t. F . Then, we may rewrite
t t−1
the regret as
(cid:34) T (cid:35)
(cid:88)
E E [E[111{t<t }·b (a,w)|t ,q ]|t ] .
tℓ qt ℓ+1 t ℓ t ℓ
t=tℓ
Note that111{t<t } and b (a,w) are independent conditional on t and q . Next, observe that:
ℓ+1 t ℓ t
E[b (a,w)|t ,q ]=η ·E [b (a,w)|t ,q ]+(1−η )·E [b (a,w)|t ,q ].
t ℓ t t a∼w t ℓ t t a∼Unif{At} t ℓ t
Plugging in the results of the above to our regret formula, we obtain:
(cid:34)tℓ(cid:88)+1−1 (cid:35) (cid:34)tℓ(cid:88)+1−1 (cid:35) (cid:34)tℓ(cid:88)+1−1 (cid:35)
E δw(a ,i )+δw(a ,j ) ≤E η +E E [δw(a ,a)] . (21)
t ℓ t t ℓ t t a∼Unif{At} t ℓ
t=tℓ t=tℓ t=tℓ
Bounding the Regret of Extra Exploration. We bound the first expectation on the above RHS. Recall
that γ denotes the learning rate γ set by a base algorithm initiated at round t . Now, we can
t−tstart t start
coarsely bound the sum of the η ’s by the sum of all possible γ , weighted by the probabilities of the
t t−tstart
scheduling of each base algorithm. So, we have
(cid:34)tℓ(cid:88)+1−1 (cid:35) (cid:34)tℓ(cid:88)+1−1 (cid:35) 
(cid:88)
tsta(cid:88)rt+m 
E η t ≤E γ t−tℓ +E  B tstart,m γ t−tstart.
t=tℓ t=tℓ BOSSE(tstart,m) t=tstart
30Recall in the above that the Bernoulli B (see Line 6 of Algorithm 2) decides whether BOSSE(s,m) is
s,m
scheduled.
The first sum on the RHS above is order K1/3·(t −t )2/3. The analogous sum in the second expectation
ℓ+1 ℓ
on the RHS above is order K1/3·m2/3. Then, it suffices to bound
(cid:34) T (cid:35)
(cid:88) (cid:88)
E B ·111{t <t }·K1/3·m2/3 .
tstart,m start ℓ+1
tstart=tℓ+1 m
Conditioning on t , and noting that111{B } and111{t <t } are independent conditional on t , we
ℓ tstart,m start ℓ+1 ℓ
have that by tower rule:
(cid:34) T (cid:35)
(cid:88) (cid:88)
E E[B |t ]·E[111{t <t }|t ]·K1/3·m2/3 .
tℓ tstart,m ℓ start ℓ+1 ℓ
tstart=tℓ+1 m
Plugging in E[B |t ]= 1 (from Line 12 of Algorithm 2) and taking sums over m and s,
tstart,m ℓ m1/3·(tstart−tℓ)2/3
the above becomes order log(T)·K1/3·(t −t )2/3. This is of the right order with respect to (20) by the
ℓ+1 ℓ
sub-additivity of the function x(cid:55)→x2/3.
Bounding the Regret of Active Arms. Next, we bound the second expectation on the RHS of (21).
This follows a similar argument to Appendix D.2 (the proof of Theorem 14). Supposing WLOG that the
arms are evicted from A in the order 1,2,...,K at respective times t1 ≤ ··· ≤ tK, then we have the
global ℓ ℓ
second expectation on the RHS of (21) can be written as
 
E
(cid:88)K t (cid:88)a ℓ−1 δ tw |( Aa ℓ, |a) +(cid:88)K tℓ(cid:88)+1−1 δ tw |( Aa ℓ, |a)
·111{a∈A t}. (22)
a=1t=tℓ t a=1 t=ta
ℓ
t
For the first double sum above, we repeat the arguments of Appendix D.2 using the episode-specific learning
rates {η }tℓ+1−1. Crucially, we note that, no matter which base algorithm BOSSE(t ,m) is active at round
t t=tℓ start
t∈[t ,t ), η ≥K1/3·(t−t )−1/3. Thus, we have
ℓ ℓ+1 t ℓ
ta−1
(cid:88)ℓ
K η−1 ≤K2+c ·K2/3·(ta−t )4/3,
t 6 ℓ ℓ
t=tℓ
K max η−1 ≤K2/3·(ta−t )1/3.
t ℓ ℓ
t∈[tℓ,ta ℓ−1]
Then, using Lemma 13 and following the arguments of Appendix D.2:
111{E
}(cid:88)K t (cid:88)a ℓ−1 δ tw(a ℓ,a)
≤c log(T)·K1/3·(t −t )2/3.
1 |A | 7 ℓ+1 ℓ
t
a=1t=tℓ
Fortheseconddoublesumin(22), ouraimistoboundtheregretofplayingarmaontheroundst∈[ta,t )
ℓ ℓ+1
where a replay is active and, thus, reintroduces arm a to A after it has been evicted from A . For this,
t global
we rely on a careful decomposition of the rounds in [ta,t ) when a∈A based on which replay reintroduces
ℓ ℓ+1 t
the arm a to the active set. We’ll first require some definitions, repeated from the analyses of Suk and
Agarwal (2023); Suk and Kpotufe (2022).
We first note that the various base algorithms scheduled in the process of running METABOSSE have an
ancestor-parent-child structure determined by which instance of BOSSE calls another. Keeping this in mind,
we now set up the following terminology (which is all w.r.t. a fixed arm a):
Definition 6.
31(i) For each scheduled and activated BOSSE(s,m), let the round M(s,m) be the minimum of two quantities:
(a) the last round in [s,s+m] when arm a is retained by BOSSE(s,m) and all of its children, and (b) the
last round that BOSSE(s,m) is active and not permanently interrupted. Call the interval [s,M(s,m)]
the active interval of BOSSE(s,m).
(ii) Call a replay BOSSE(s,m) proper if there is no other scheduled replay BOSSE(s′,m′) such that [s,s+
m]⊂(s′,s′+m′) where BOSSE(s′,m′) will become active again after round s+m. In other words, a
properreplayisnotscheduledinsidethescheduledrangeofroundsofanotherreplay. LetProper(t ,t )
ℓ ℓ+1
be the set of proper replays scheduled to start before round t .
ℓ+1
(iii) CallascheduledreplayBOSSE(s,m)subproperifitisnon-properandifeachofitsancestorreplays(i.e.,
previously scheduled replays whose durations have not concluded) BOSSE(s′,m′) satisfies M(s′,m′)<s.
In other words, a subproper replay either permanently interrupts its parent or does not, but is scheduled
after its parent (and all its ancestors) stops playing arm a. Let SubProper(t ,t ) be the set of all
ℓ ℓ+1
subproper replays scheduled before round t .
ℓ+1
Equipped with this language, we now show some basic claims which essentially reduce analyzing the compli-
catedhierarchyofreplaystoanalyzingtheactiveintervalsofreplaysinProper(t ,t )∪SubProper(t ,t ).
ℓ ℓ+1 ℓ ℓ+1
Proposition 19. The active intervals
{[s,M(s,m)]:BOSSE(s,m)∈Proper(t ,t )∪SubProper(t ,t )},
ℓ ℓ+1 ℓ ℓ+1
are mutually disjoint.
Proof. Clearly, the classes of replays Proper(t ,t ) and SubProper(t ,t ) are disjoint. Next, we
ℓ ℓ+1 ℓ ℓ+1
show the respective active intervals [s,M(s,m)] and [s′,M(s′,m′)] of any two BOSSE(s,m),BOSSE(s′,m′)∈
Proper(t ,t )∪SubProper(t ,t ) are disjoint. There are three cases here:
ℓ ℓ+1 ℓ ℓ+1
1. Proper replay vs. subproper replay: a subproper replay can only be scheduled after the round M(s,m)
of the most recent proper replay BOSSE(s,m) (which is necessarily an ancestor). Thus, the active
intervals of proper replays and subproper replays are disjoint.
2. Two distinct proper replays: two such replays can only permanently interrupt each other, and since
M(s,m) always occurs before the permanent interruption of BOSSE(s,m), we have the active intervals
of two such replays are disjoint.
3. Two distinct subproper replays: consider two non-proper replays
BOSSE(s,m),BOSSE(s′,m′)∈SubProper(t ,t ),
ℓ ℓ+1
withs′ >s. TheonlywaytheiractiveintervalsintersectisifBOSSE(s,m)isanancestorofBOSSE(s′,m′).
Then, if BOSSE(s′,m′) is subproper, we must have s′ >M(s,m), which means that [s′,M(s′,m′)] and
[s,M(s,m)] are disjoint.
■
Next,weclaimthattheactiveintervals[s,M(s,m)]forBOSSE(s,m)∈Proper(t ,t )∪SubProper(t ,t )
ℓ ℓ+1 ℓ ℓ+1
contain all the rounds where a is played after being evicted from A . To show this, we first observe that
global
for each round t when a replay is active, there is a unique proper replay associated to t, namely the proper
replay scheduled most recently. Next, note that any round t > ta where arm a ∈ A must either belong
ℓ t
to the active interval [s,M(s,m)] of the unique proper replay BOSSE(s,m) associated to round t, or else
satisfies t>M(s,m) in which case a unique subproper replay BOSSE(s′,m′)∈SubProper(t ,t ) is active
ℓ ℓ+1
at round t and not yet permanently interrupted. Thus, it must be the case that t∈[s′,M(s′,m′)].
32At the same time, every round t ∈ [s,M(s,m)] for a proper or subproper BOSSE(s,m) is clearly a round
where arm a is once again active, i.e. a∈A , and no such round is accounted for twice by Proposition 19.
t
Thus,
(cid:71)
{t∈[ta,t ):a∈A }= [s,M(s,m)].
ℓ ℓ+1 t
BOSSE(s,m)∈Proper(tℓ,tℓ+1)∪SubProper(tℓ,tℓ+1)
Then, we can rewrite the second double sum in (22) as:
(cid:88)K (cid:88)
B
M (cid:88)(s,m) δ tw(a ℓ,a)
.
s,m |A |
a=1BOSSE(s,m)∈Proper(tℓ,tℓ+1)∪SubProper(tℓ,tℓ+1) t=s∨ta
ℓ
t
Further bounding the sum over t above by its positive part, we can expand the sum over BOSSE(s,m) ∈
Proper(t ,t )∪SubProper(t ,t ) to be over all scheduled BOSSE(s,m), or obtain:
ℓ ℓ+1 ℓ ℓ+1
 
K M(s,m)
(cid:88) (cid:88)
B s,m
(cid:88) δ t( |Aa ℓ, |a)
·111{a∈A t} , (23)
a=1BOSSE(s,m) t=s∨ta t
ℓ +
where the sum is over all replays BOSSE(s,m), i.e. s∈{t +1,...,t −1} and m∈{2,4,...,2⌈log(T)⌉}. It
ℓ ℓ+1
then remains to bound the contributed relative regret of each BOSSE(s,m) in the interval [s∨ta,M(s,m)],
ℓ
which will follow similarly to the previous steps. Fix s,m and suppose ta+1 ≤ M(s,m) since otherwise
ℓ
BOSSE(s,m) contributes no regret in (23).
Note that the global learning rate η for t∈[s∨ta,M(s,m)) must satisfy η ≥K1/3·m−1/3 since any child
t ℓ t
base algorithm BOSSE(s′,m′) of BOSSE(s,m) will use a larger learning rate γ
t−s′
≥γ t−s.
Then, following similar reasoning as before, i.e. combining our concentration bound (12) with the eviction
criterion (6), we have for a fixed arm a:
111{E
}M (cid:88)(s,m) δ tw(a ℓ,a)
≤
c 8log(T)·(K1/3·m2/3∧m)
,
1 |A | min |A |
t=s∨ta t t∈[s,M(s,m)] t
ℓ
Plugging this into (23) and switching the ordering of the outer double sum, we obtain (we also overload the
notation M(s,m,a) to avoid ambiguity on which arm a we’re bounding the regret of):
K
(cid:88) (cid:88) 1
B ·c log(T)·(K1/3·m2/3∧m) .
s,m 8 min |A |
t∈[s,M(s,m.a)] t
BOSSE(s,m) a=1
Following previous arguments, the above innermost sum over a is at most log(K) since the k-th arm a in
k
[K] to be evicted by BOSSE(s,m) satisfies min |A |≥K+1−k.
t∈[s,M(s,m,ak)] t
Now, let R(m) := c log(K)·log(T)·(K1/3 ·m2/3 ∧m) which is the bound we’ve obtained so far on the
8
relative regret for a single BOSSE(s,m). Now, plugging R(m) into (23) gives:
    
E 111{E
1}(cid:88)K tℓ(cid:88)+1−1 δ tw |( Aa ℓ, |a)
·111{a∈A t}≤E tℓE 
(cid:88)
B s,m·R(m)|t ℓ
a=1 t=ta t BOSSE(s,m)
ℓ
(cid:34) T (cid:35)
(cid:88)(cid:88)
=E E[B ·111{s<t }|t ]·R(m) .
tℓ s,m ℓ+1 ℓ
s=tℓ m
Next, we observe that B and 111{s < t } are independent conditional on t since 111{s < t } only
s,m ℓ+1 ℓ ℓ+1
depends on the scheduling and observations of base algorithms scheduled before round s. Thus, recalling that
P(B =1)=m−1/3·(s−t )−2/3,
s,m ℓ
E[B ·111{s<t }|t ]=E[B |t ]·E[111{s<t }|t ]
s,m ℓ+1 ℓ s,m ℓ ℓ+1 ℓ
1
= ·E[111{s<t }|t ].
m1/3·(s−t )2/3 ℓ+1 ℓ
ℓ
33Then, plugging the above display into our expectation from before and unconditioning, we obtain:
 
tℓ(cid:88)+1−1⌈lo (cid:88)g(T)⌉
1 (cid:104) (cid:105)
E  2n/3·(s−t )2/3 ·R(2n)≤c 9log(K)·log2(T)·E tℓ,tℓ+1 K1/3·(t ℓ+1−t ℓ)2/3 . (24)
ℓ
s=tℓ+1 n=1
Note that in the above, we used the fact that the episode length t −t dominates K by Lemma 13 to
ℓ+1 ℓ
bound a term of order K2/3·(t −t )1/3 by K1/3·(t −t )2/3.
ℓ+1 ℓ ℓ+1 ℓ
E.6 Bounding per-Episode Regret of the Last Global Arm to the Last Safe Arm
(cid:104) (cid:105)
Now, it remains to bound E (cid:80)tℓ+1−1δw(a♯,a ) . For this, we’ll use a bad segment analysis similar to Suk
t=tℓ t t ℓ
and Agarwal (2023); Buening and Saha (2023); Suk and Kpotufe (2022). Roughly a bad segment [s ,s ]
1 2
will be such that (cid:80)s2 δw(a♯,a )≳(s −s )2/3·K1/3. On such a bad segment [s ,s ], a perfect replay is
roughly defined as
as= res1plas
y
wt hoℓ
se
sche2 duled1
duration overlaps most of [s ,s ].
Th1 en,2
the key fact is that
1 2
the randomized scheduling of replays (Line 12 of Algorithm 1) ensures that a perfect replay is scheduled for
some bad segment (thus evicting a from A before too many bad segments elapse, giving us a way of
ℓ global
bounding (cid:80)s2 δw(a♯,a ) with high probability.
s=s1 s t ℓ
We next formally define such notions. In what follows, bad segments will be defined with respect to a fixed
armaandconditionalontheepisodestarttimet . Inparticular,thiswillholdfora=a whichwillultimately
ℓ ℓ
be used to bound δw(a♯,a ) across the episode [t ,t ). The following definition only depends on a fixed arm
t t ℓ ℓ ℓ+1
a and the episode start time t and, conditional on these quantities, are deterministic given the environment.
ℓ
Definition 7. Fix the episode start time t , and let [τ ,τ ) be any phase intersecting [t ,T). For any arm a,
ℓ i i+1 ℓ
define rounds s (a),s (a),s (a)...∈[t ∨τ ,τ ) recursively as follows: let s (a):=t ∨τ and define
i,0 i,1 i,2 ℓ i i+1 i,0 ℓ i
s (a) as the smallest round in (s (a),τ ) such that arm a satisfies for some fixed c >0:
i,j i,j−1 i+1 10
si,j(a)
(cid:88)
δw(a♯,a)≥c log(T)·K1/3·(s (a)−s (a))2/3, (25)
t t 10 i,j i,j−1
t=si,j−1(a)
if such a round s (a) exists. Otherwise, we let the s (a) := τ − 1. We refer to any interval
i,j i,j i+1
[s (a),s (a)) as a critical segment, and as a bad segment (w.r.t. arm a) if (25) above holds.
i,j−1 i,j
Remark 4. Arm a♯ is fixed within any critical segment [s (a),s (a))⊆[τ ,τ ) since an SKW does
t i,j−1 i,j i i+1
not occur inside [τ ,τ ).
i i+1
The following fact, which may be considered an analogue of Lemma 13, will serve useful.
Fact 20. A bad segment [s (a),s (a)) satisfies s (a)−s (a)≥K/8.
i,j i,j+1 i,j+1 i,j
Now, note a bad segment [s (a),s (a)) only contributes order K1/3·(s (a)−s (a))2/3 regret of a
i,j i,j+1 i,j+1 i,j
to a♯. At the same time, we claim that a well-timed replay (see Definition 8 below) running from s (a) to
t i,j
s (a) will in fact be capable of evicting arm a using (6). This will allow us to reduce the problem to
i,j+1
studying the number and lengths of bad segments which elapse before one is detected by such a replay.
We next define a perfect replay.
Definition 8. Let s˜ (a):=⌈si,j(a)+si,j+1(a)⌉ denote the approximate midpoint of [s (a),s (a)). Given a
i,j 2 i,j i,j+1
bad segment [s (a),s (a)), define a perfect replay w.r.t. [s (a),s (a)) as a call of BOSSE(t ,m)
i,j i,j+1 i,j i,j+1 start
where t ∈[s (a),s˜ (a)] and m≥s (a)−s (a)
start i,j i,j i,j+1 i,j
Next, we analyze the behavior of a perfect replay on the bad segment [s (a),s (a)). We first invoke an
i,j i,j+1
elementary lemma:
34Lemma 21. (x+y)2/3−x2/3 ≥y2/3/2 for real numbers y ≥x>0.
Proof. This follows from noting the derivative of the function y (cid:55)→(x+y)2/3−x2/3−y2/3/2 in the domain
y ≥x is positive since
∂ 2 1
(x+y)2/3−x2/3−y2/3/2= − .
∂y 3·(x+y)1/3 3·y1/3
The above is positive since
1 1 1 2 1
≥ > =⇒ > .
(x/y+1)1/3 21/3 2 3·(x+y)1/3 3·y1/3
Thus, y (cid:55)→(x+y)2/3−x2/3−y2/3/2 is an increasing function in y. Next, since (2x)2/3−x2/3−x2/3/2=
x2/3·(22/3−1−1/2)>0 if x>0, we have that the desired inequality must always be true for y ≥x>0. ■
Proposition 22. Suppose the good event E holds (cf. Proposition 12). Let [s (a),s (a)) be a bad
1 i,j i,j+1
segment with respect to arm a. Then, if a perfect replay with respect to [s (a),s (a)) is scheduled, arm a
i,j i,j+1
will be evicted from A by round s (a).
global i,j+1
Proof. We first observe that by Lemma 21 and Definition 7:
si, (cid:88)j+1(a)
δw(a♯,a)=
si, (cid:88)j+1(a) δw(a♯,a)−s˜i,j (cid:88)(a)−1
δw(a♯,a)≥
c
10 log(T)·K1/3·(s (a)−s˜ (a))2/3.
t t t t t t 2 i,j+1 i,j
t=s˜i,j(a) t=si,j(a) t=si,j(a)
Next, we note that any perfect replay BOSSE(t ,m) will not evict a♯ since otherwise it incurs significant
start t
regret within SKW phase [τ ,τ ) (see also the proof of Lemma 18). The same applies for any child base
i i+1
algorithm of a perfect replay.
Next, we argue that arm a must be evicted from A at some round in [s˜ (a),s (a)]. If a̸∈A for
global i,j i,j+1 t
someroundt∈[s˜ (a),s (a)]wearealreadydone. Otherwise,supposea∈A forallt∈[s˜ (a),s (a)]
i,j i,j+1 t i,j i,j+1
and so we must have E[δˆw(a♯,a)|F ]=δw(a♯,a) for all such rounds t. Now, if a perfect replay is scheduled,
t t t−1 t t
then we must have a global learning rate η ≥ K1/3·(s (a)−s (a))−1/3 for all t ∈ [s˜ (a),s (a)]
t i,j+1 i,j i,j i,j+1
since any child base algorithm of a perfect replay can only set a larger learning rate by Definition 5.
Now, noting [s (a),s (a)) and the second half of the bad segment [s (a),s (a)) have commensurate
i,j i,j+1 i,j i,j+1
lengths up to constants, this means, if a perfect replay w.r.t. [s (a),s (a)) is scheduled, by similar
i,j i,j+1
calculations to earlier (and using Fact 20) we must have
(cid:118)
(cid:117)
(cid:117)
si, (cid:88)j+1(a)
(cid:117)K η−1+K· max η−1 ≤c K1/3·(s (a)−s (a))2/3.
(cid:116) s s 11 i,j+1 i,j
s=s˜i,j(a)
s∈[s˜i,j(a),si,j+1(a)]
Then,byourevictioncriterion(6)andconcentration,wehavethatarmawillbeevictedover[s˜ (a),s (a)]
i,j i,j+1
for large enough constant c in Definition 7. ■
10
It remains to show that, for any arm a, a perfect replay is scheduled w.h.p. before too much regret is incurred
on the elapsed bad segments w.r.t. a. In particular, this will hold for the last global arm a , allowing us to
ℓ
bound the remaining expectation E[(cid:80)tℓ+1−1δw(a♯,a )].
t=tℓ t t ℓ
To show this, we’ll define a bad round s(a) > t which will roughly be the latest time that arm a can be
ℓ
evicted by a perfect replay before there is too much regret. We’ll then show that a perfect replay with respect
to some bad segment is indeed scheduled before the bad round is reached.
First, fix an arm a and an episode start time t .
ℓ
35Definition 9. (Bad Round) For a fixed round t and arm a, the bad round s(a) > t is defined as the
ℓ ℓ
smallest round which satisfies, for some fixed c >0:
12
(cid:88)
(s (a)−s (a))2/3 >c log(T)·(s(a)−t )2/3, (26)
i,j+1 i,j 12 ℓ
(i,j)
where the above sum is over all pairs of indices (i,j)∈N×N such that [s (a),s (a)) is a bad segment
i,j i,j+1
(see Definition 7) with s (a)<s(a).
i,j+1
Our goal is then to then to show that arm a is evicted by some perfect replay scheduled within episode
[t ,t ) with high probability before the bad round s(a) occurs.
ℓ ℓ+1
For each bad segment [s (a),s (a)), recall that s˜ (a) is the approximate midpoint between s (a) and
i,j i,j+1 i.j i,j
s (a) (see Definition 8). Next, let m :=2n where n∈N satisfies:
i,j+1 i,j
2n ≥s (a)−s (a)>2n−1.
i,j+1 i,j
Plainly,m isadyadicapproximationofthebadsegmentlength. Next,recallthattheBernoulliB decides
i,j t,m
whether BOSSE(t,m) is scheduled at round t (see Line 6 of Algorithm 2). If for some t ∈ [s (a),s˜ (a)],
i,j i,j
B = 1, i.e. a perfect replay is scheduled, then a will be evicted from A by round s (a)
t,mi,j global i,j+1
(Proposition 22). We will show this happens with high probability via concentration on the sum
s˜i,j(a)
(cid:88) (cid:88)
X(a,t ):= B ,
ℓ t,mi,j
(i,j):si,j+1(a)<s(a)t=si,j(a)
Note that the random variable X(a,t ) only depends on the replay scheduling probabilities {B } given
ℓ s,m s,m
a fixed arm a and episode start time t , since the bad round s(a) is also fixed given these quantities. This
ℓ
means that X(a,t ) is an independent sum of Bernoulli random variables B , conditional on t . Then, a
ℓ t,mi,j ℓ
multiplicative Chernoff bound over the randomness of X(a,t ), conditional on t yields
ℓ ℓ
(cid:18) E[X(a,t )|t ] (cid:19) (cid:18) E[X(a,t )|t ](cid:19)
P X(a,t )≤ ℓ ℓ |t ≤exp − ℓ ℓ .
ℓ 2 ℓ 8
The above RHS error probability is bounded above above by 1/T3 by observing:
E[X(a,t )|t ]≥ (cid:88)
s˜i (cid:88),j(a)
1 ≥ 1 (cid:88) (s i,j+1(a)−s i,j(a))2/3 ≥ c 12 log(T),
ℓ ℓ (i,j)t=si,j(a)m1 i,/ j3·(t−t ℓ)2/3 2
(i,j)
(s(a)−t ℓ)2/3 2
for c >0 large enough, where the last inequality follows from (26) in the definition of the bad round s(a)
12
(Definition 9). Taking a further union bound over the choice of arm a∈[K] gives us that X(a,t )>1 for all
ℓ
choices of arm a (define this as the good event E (t )) with probability at least 1−K/T3. Thus, under event
2 ℓ
E (t ), all arms a will be evicted before round s(a) with high probability.
2 ℓ
Recall on the event E the concentration bounds of Proposition 12 hold. Then, on E ∩E (t ), letting a=a
1 1 2 ℓ ℓ
in the preceding arguments we must have t −1 ≤ s(a ) Thus, by the definition of the bad round s(a )
ℓ+1 ℓ ℓ
(Definition 9), we must have:
(cid:88)
(s (a )−s (a ))2/3 ≤c log(T)·(t −t )2/3. (27)
i,j+1 ℓ i,j ℓ 12 ℓ+1 ℓ
[si,j(aℓ),si,j+1(aℓ)):si,j+1(aℓ)<tℓ+1−1
Thus,by(25)inthedefinitionofbadsegments(Definition7),overthebadsegments[s (a ),s (a ))which
i,j ℓ i,j+1 ℓ
elapsebeforetheendoftheepisodet −1,theregretofa toa♯ isatmostorderlog2(T)K1/3·(t −t )2/3.
ℓ+1 ℓ t ℓ+1 ℓ
Over each non-bad critical segment [s (a ),s (a )), the regret of playing arm a to a♯ is at most
i,j ℓ i,j+1 ℓ ℓ t
log(T)·K1/3·(τ −τ )2/3 and there is at most one non-bad critical segment per phase [τ ,τ ) (follows
i+1 i i i+1
from Definition 7).
36So, we conclude that on event E ∩E (t ):
1 2 ℓ
tℓ(cid:88)+1−1
(cid:88)
δw(a♯,a )≤c log2(T) K1/3(τ −τ )2/3.
t t ℓ 13 i+1 i
t=tℓ i∈Phases(tℓ,tℓ+1)
Taking expectation, we have by conditioning first on t and then on event E ∩E (t ):
ℓ 1 2 ℓ
(cid:34)tℓ(cid:88)+1−1 (cid:35) (cid:34) (cid:34) tℓ(cid:88)+1−1 (cid:35)(cid:35)
E δw(a♯,a ) ≤E E 111{E ∩E (t )} δw(a♯,a )|t +T ·E [E[111{Ec∪Ec(t )}|t ]]
t t ℓ tℓ 1 2 ℓ t t ℓ ℓ tℓ 1 2 ℓ ℓ
t=tℓ t=tℓ
  
(cid:88) 2K
≤c 13log2(T)E tℓE 111{E 1∩E 2(t ℓ)} K1/3·(τ i+1−τ i)2/3 |t ℓ+
T2
i∈Phases(tℓ,tℓ+1)
 
(cid:88) 2
≤c 13log2(T)E 111{E 1} K1/3·(τ i+1−τ i)2/3 + T,
i∈Phases(tℓ,tℓ+1)
where in the last step we bound111{E ∩E (t )}≤111{E } and apply tower law again. This concludes the proof.
1 2 ℓ 1
■
E.7 Total Variation Regret Rates for Known Weights (Proof of Corollary 17)
Re-Defining Total Variation in Terms of Generalized Borda Scores. Recall the total variation
quantity is defined as:
T
(cid:88)
V := max|δ (a,a′)−δ (a,a′)|,
T t t−1
a,a′
t=2
.
where δ (a,a′)=P (a,a′)− 1 is the gap in dueling preferences. We first note that this can be rewritten as
t t 2
(cid:80)T max |b (a,w)−b (a,w)| since by Jensen:
t=2 a∈[K],w∈∆K t t−1
max |b (a,w)−b (a,w)|≤ max E [|δ (a,a′)−δ (a,a′)|≤max|δ (a,a′)−δ (a,a′)|.
t t−1 a′∼w t t−1 t t−1
w∈∆K w∈∆K a,a′
Note the other directions of the above inequalities are clear by taking w=w(a′). Thus, we may redefine the
total variation in terms of generalized Borda scores:
T
(cid:88)
V := max |b (a,w)−b (a,w)|.
T t t−1
w∈∆K
t=2
ProofofCorollary17. First,weboundthetotalvariationV overanSKWphase[τ ,τ ). Consider
[τi,τi+1) i i+1
the arm a∗ (w). By Definition 2, there must exist a round t∈[τ ,τ ) such that
τi+1 i i+1
(cid:18)
K
(cid:19)1/3
b (a∗(w),w)−b (a∗ (w),w)> .
t t t τi+1 τ −τ
i+1 i
Adding b (a∗ (w),w)−b (a∗(w),w)≥0 to the RHS we obtain that
τi+1 τi+1 τi+1 t
(cid:18)
K
(cid:19)1/3
V ≥ .
[τi,τi+1) τ −τ
i+1 i
Now, summing over SKW phases, we have by Hölder’s inequality:
L˜ K(cid:88)nown (cid:32) (cid:88)(cid:18) K (cid:19)1/3(cid:33)1/4(cid:32) (cid:88) (cid:33)3/4
K1/3·(τ −τ )2/3 ≤ (τ −τ )·K1/3 +K1/3·T2/3
i+1 i τ −τ i+1 i
i+1 i
i=0 i i
Thus, we obtain a total dynamic regret bound of V1/4·T3/4·K1/4+K1/3·T2/3.
T
37F Nonstationary Regret Analysis for Unknown Weights
F.1 Formal Statement of Results
Theorem 23. Algorithm 2 with the unknown weight specification (see Definition 5) satisfies for all sequences
of aligned weights {w } (see Definition 4):
t t
E[Regret({w }T )]≤O˜(min{K2/3T2/3L˜1/3 ,K1/2V1/4T3/4+K2/3T2/3}).
t t=1 Unknown T
Remark 2. Interestingly, Theorem 23 holds even without GIC (Condition 1). However, without GIC,
Definition 3 may not be a meaningful notion of non-stationarity as (7) may be triggered even in stationary
environments.
F.2 Analysis Overview for Theorem 23
The proof of Theorem 23 will broadly follow the same outline as the regret analysis for known weights, but
with replacements of the eviction threshold by (s −s )2/3·K2/3 (see the unknown weight specification in
2 1
Definition 5), and bounding the variance of estimation by quantities of this order. The key difficulty for
unknown weights is that the evaluation weights w may change at the unknown SUW shifts ρ .
t i
Importantly, Algorithm 2 can only estimate aggregate gaps (cid:80)s2 δw(a′,a) over intervals [s ,s ] with respect
toafixedandunchangingweightw.
Thus,themainnoveltyins= ths1ist analysisistocarefullypa1 rti2
tiontheregret
analysis along intervals [s ,s ] lying within a phase [ρ ,ρ ). In fact, such a strategy is already inherent
1 2 i i+1
to the bad segment argument done in Appendix E.6 to bound (cid:80)tℓ+1−1δw(a♯,a ) for a known and known
weight w. So, we’ll repeat a similar such bad segment analysis, butt= ft oℓr difft erent t aℓ rms and even for different
base algorithms.
This strategy is similar to the approach taken in the Condorcet winner dynamic regret analysis of Buening
and Saha (2023, see Appendix A.3 therein), who rely on such a tactic to avoid decomposing the regret using
triangle inequalities as done in the original non-stationary MAB analysis of Suk and Kpotufe (2022). Our
need for this strategy is different, as we must constrain ourselves to only being able to detect that an arm a
is bad over segments [s ,s ] of rounds where we can use a single reference weight w.
1 2
We first establish the analogue of Lemma 18 from Appendix E for the unknown weight setting.
F.3 Episodes Align with SUW Phases
Lemma 24. On event E , for each episode [t ,t ) with t ≤T (i.e., an episode which concludes with a
1 ℓ ℓ+1 ℓ+1
restart), there exists an SUW shift ρ ∈[t ,t ).
i ℓ ℓ+1
Proof. This follows in an analogous manner as Lemma 18, where we note that significant SUW regret occurs
if, for some weight w∈∆K, we have (cid:80)s2 δw(a)≥K2/3·(s −s )2/3. Thus, an arm being evicted from
A implies it has significant SUW
res= grs e1ts
meaning a new
2 episo1
de is triggered only when an SUW has
global
occurred. ■
In what follows, we redefine the last safe arm a♯ as the last arm to become unsafe in the sense of Definition 3
t
in the unique SUW phase [ρ ,ρ ) containing round t.
i i+1
F.4 Generic Bad Segment Analysis
We’llfirstdefineagenericgoodeventE overwhichthebadsegment-typeargument(asseeninAppendixE.6)
3
holds for any arm a and any episode start time t . Before we can define such an event, we’ll generically
ℓ
redefine the necessary mathematical objects of Appendix E.6 for the unknown weight setting. We note that
38everythingthatfollowsinthissubsectionisindependentofanyobservationsordecisionsmadebyAlgorithm2
and depend only on the possible random choices of replay schedules (see Line 12 of Algorithm 2), which may
be instantiated independently and obliviously to the algorithm’s actual behavior.
In what follows, fix a starting round t init ∈ [T] and an arm a ∈ [K]. Define the random variables B sti ,n mit ∼
Ber(m−1/3·(s−t )−2/3) for m=2,4,...,2⌈log(T)⌉ and s=t +1,...,T, which are the replay schedulers
init init
of Line 12 in Algorithm 2 if t was the start of an episode. Also, fix a round t from which we will begin
init start
defining bad segments; the variable t will serve useful when we analyze bad segments for different base
start
algorithms BOSSE(t ,m). The following definitions will then be relative to a fixed t , t , and arm a.
start init start
Definition 10. (Generic Bad Segment) Fix an SUW phase [ρ ,ρ ) intersecting [t ,T). Define rounds
i i+1 start
s ,s ,s ...∈[t ∨ρ ,ρ ) recursively as follows: let s :=t ∨ρ and define s as the smallest
i,0 i,1 i,2 start i i+1 i,0 start i i,j
round in (s ,ρ ) such that arm a satisfies for some fixed c >0:
i,j−1 i+1 14
si,j
max (cid:88) δw(a′)(a♯,a)≥c log(T)·K2/3·(s −s )2/3, (28)
t t 14 i,j i,j−1
a′∈[K]
t=si,j−1
if such a round s exists. Otherwise, we let the s :=ρ −1. We refer to any interval [s ,s ) as a
i,j i,j i+1 i,j−1 i,j
critical segment, and as a bad segment if (25) above holds.
Remark 5. Note that (28) mimics the notion (8) of significant worse-case generalized Borda regret in
Definition 3. However, we need only concern ourselves with checking for bad regret w.r.t. point-mass weights
w(a′) for a′ ∈[K], as that will suffice for the analysis.
Definition 11. (Generic Bad Round) Define the bad round s(a,t ,t )>t as the smallest round
start init start
which satisfies, for some fixed c >0:
15
(cid:88)
(s −s )2/3 >c log(T)·(s(a,t ,t )−t )2/3, (29)
i,j+1 i,j 15 start init init
(i,j)
where the above sum is over all pairs of indices (i,j) ∈ N×N such that [s ,s ) is a bad segment (see
i,j i,j+1
Definition 7) with s <s(a,t ,t ).
i,j+1 start init
Now, define the independent sum of Bernoulli’s
s˜i,j
(cid:88) (cid:88)
X(a,t ,t ):= Btinit ,
start init t,mi,j
(i,j):si,j+1<s(a,tstart,tinit)t=si,j
where m is the dyadic approximation w.r.t. [s (a),s (a)) in the same sense as defined in Appendix E.6.
i,j i,j i,j+1
We are now prepared to define the good event E , where all bad segment arguments hold, based on the
3
following lemma.
Lemma 25. Let E be the event over which for all a ∈ [K] and rounds t ,t ∈ [T] with t ≥ t ,
3 start init start init
X(a,t ,t )>1. Then, over the randomness of all possible replay schedules, P(E )>1−T−3.
start init 3
Proof. This follows from repeating the multiplicative Chernoff bound as used in Appendix E.6 for each
X(a,t ,t ) and then taking union bounds over arms a ∈ [K] and rounds t ∈ [T]. We note that
start init init
crucially all potential replay schedules {B sti ,n mit} s,m,tinit are independent across different s,m,t init. ■
Next, we show that, under event E , the regret of a fixed arm a to a♯ on the active interval [s,M(s,m,a)] (see
3 t
Definition 6 in Appendix E.5) of any scheduled base algorithm BOSSE(s,m) will be boundable by running a
customized bad segment analysis using the notions defined above.
Notation 26. (Active Interval of First Ancestor Base Algorithm) For the first ancestor base algorithm
BOSSE(t ,T +1−t ) (i.e., that which is instantiated first in episode [t ,t ) per Line 7 of Algorithm 2),
ℓ ℓ ℓ ℓ+1
we’ll let M(t ,T +1−t ,a) denote the last round when a is retained by BOSSE(t ,T +1−t ) and all of its
ℓ ℓ ℓ ℓ
children.
39.
Definition 12. For an interval of rounds I, Let Phases(I) = {i ∈ [L˜ ] : [ρ ,ρ )∩I ̸= ∅}, i.e.,
Unknown i i+1
denote those phases intersecting I. Let S(I) := |Phases(I)| be the number of intersecting phases and let
L(I,i):=|[ρ ,ρ )∩I| be the intersection’s length for phase [ρ ,ρ ).
i i+1 i i+1
Lemma 27. (Generic Bad Segment Analysis for BOSSE(s,m)) For any scheduled BOSSE(s,m), letting
I :=[s,M(s,m,a)] be its active interval, we have on event E ∩E :
3 1
 
M(s,m,a)
(cid:88) (cid:88)
δ twt(a♯ t,a)≤c 16log2(T)K2/3·(s−t ℓ)2/3·111{S(I)>1}+ K2/3·L(I,i)2/3 .
t=s i∈Phases(I)
Proof. For (t ,t ):=(t ,s), we can define perfect replays with respect to the generic bad segments of
init start ℓ
Definition 10 analogously to Definition 8. We next show an analogue of Proposition 22 for such perfect
replays. In particular, for each bad segment [s ,s ), we have for some point-mass weight w,
i,j i,j+1
s (cid:88)i,j+1
δw(a♯,a)≥
c
17 log(T)·K2/3·(s −s˜ )2/3.
t t 2 i,j+1 i,j
t=s˜i,j
Next, the key points hold if a perfect replay w.r.t. [s ,s ) is scheduled:
i,j i,j+1
• Arm a♯ (which is constant for t∈[s ,s )⊆[ρ ,ρ )) is not evicted from A since it does not incur
t i,j i,j+1 i i+1 t
significant SUW regret.
• The global learning rates η set for t∈[s˜ ,s ] satisfy η ≥K2/3·(s −s )−1/3 since any child
t i,j i,j+1 t i,j+1 i,j
base algorithm can only increase the learning rate of its parent base algorithm.
• By similar calculations to the proof of Theorem 15 (see Appendix D.3) and using the fact that a generic
bad segment, as defined in (28) must have length at least K2/8 (analogous to Lemma 13), we have
(cid:118)
(cid:117) si,j+1
(cid:117) (cid:116)K2 (cid:88) η s−1+K· max η s−1 ≤c 18·K2/3·(s i,j+1−s i,j)2/3.
s=s˜i,j
s∈[s˜i,j,si,j+1]
Combining the above points with our concentration bound (14) and eviction criterion (6) give us that arm a
will be evicted from A by round s . By Lemma 27, we have that a perfect replay with respect to arm a,
t i,j+1
t =s, and t =t will be scheduled before the bad round s(a,s,t ) on event E . By (29), we then must
start init ℓ ℓ 3
have:
(cid:88)
(s −s )2/3 ≤c log(T)·(M(s,m,a)−t )2/3,
i,j+1 i,j 15 ℓ
(i,j)
where the sum is over pairs of indices (i,j) representing these generic bad segments. Thus, the desired regret
bound follows by similar arguments to Appendix E.6. Note that bounding the regret over the bad segments
of multiple phases [ρ ,ρ ) is needed only if the number of intersecting phases S(I)>1. Otherwise, we can
i i+1
avoid a bad segment analysis and follow the proof steps of Appendix D.2 to directly bound the regret as
order K2/3·m2/3. ■
F.5 Decomposing the Regret Along Different Base Algorithms
Equipped with the generic bad segment analysis for any base algorithm BOSSE(s,m), we’re now ready to
bound the per-episode regret. It will suffice to decompose the regret along active intervals [s,M(s,m,a)] of
different base algorithms BOSSE(s,m) in a similar fashion to Appendix E.5, within each of which we can
plug in the regret bound of Lemma 27 and then carefully integrate with respect to the randomness of replay
scheduling.
40We may first decompose the regret as
(cid:34)tℓ(cid:88)+1−1 (cid:35) (cid:34)tℓ(cid:88)+1−1 (cid:35) (cid:34)tℓ(cid:88)+1−1 (cid:35)
E δwt(i )+δwt(j ) =E δwt(a♯) +E δwt(a♯,i )+δwt(a♯,j ) .
t t t t t t t t t t t t
t=tℓ t=tℓ t=tℓ
The first expectation on the above RHS is bounded of the right order by Definition 3 and earlier arguments.
For the second expectation on the above RHS, we further condition on whether we’re in exploration mode or
playing from the candidate set A . Following the steps of Appendix E.5, we have that it suffices to bound
t
 
(cid:88)
tℓ(cid:88)+1−1 δwt(a♯,a)
E  t |At
|
·111{a∈A t}.
t
a∈[K] t=tℓ
In fact, following the same decomposition of rounds into active intervals of different base algorithms, we can
further upper bound the above by
   
(cid:88)K
(cid:88)
M( (cid:88)s,m,a) δwt(a♯,a)
E  B s,m t |At
|
 .
t
a=1BOSSE(s,m) t=s
+
Note that in the above we sum over all base algorithms including the “first ancestor” base algorithm
BOSSE(t ,T +1−t ) using Notation 26 and for which we use the convention B =1.
ℓ ℓ tℓ,T+1−tℓ
Next, we plug in the guarantee of Lemma 27. Let I(s,m,a) := [s,M(s,m,a)] be the active interval of
BOSSE(s,m). Then, it remains to bound (hiding the added log terms from Lemma 27 for ease of notation):
 
K2/3·(s−t )2/3·111{S(I(s,m,a))>1}+(cid:80) K2/3·L(I(s,m,a),i)2/3
E  (cid:88) B s,m (cid:88) ℓ min i∈P |h Aase |s(I(s,m,a)) .
t∈I(s,m,a) t
BOSSE(s,m) a∈[K]
We can further upper bound L([s,M(s,m,a)],i) by L([s,s + m],i), the sum over phases [ρ ,ρ ) in
i i+1
Phases([s,M(s,m,a)])byasumoverPhases([s,s+m]),andthe111{S(I(s,m,a)>1}bya111{S([s,s+m])>
1} to obtain:
 
K2/3·(s−t )2/3·111{S([s,s+m])>1}+(cid:80) K2/3·L([s,s+m],i)2/3
E  (cid:88) B s,m (cid:88) ℓ min i∈P |h Aas |es([s,s+m]) .
t∈I(s,m,a) t
BOSSE(s,m) a∈[K]
Next, we note that (cid:80) 1 ≤ log(K) by previous arguments which turns the sum over
a∈[K] mint∈[s,M(s,m,a)]|At|
a∈[K] in the above display to a log(K) factor.
Now, define the function
(cid:88)
G(s,m):=K2/3·(s−t )2/3·111{S([s,s+m])>1}+ K2/3·L([s,s+m],i)2/3.
ℓ
i∈Phases([s,s+m])
Then, following the same chain of arguments as in Appendix E.5, we have that it suffices to bound
(cid:34)tℓ(cid:88)+1−1
(cid:88) 1
(cid:35)
E ·G(s,m) .
m1/3·(s−t )2/3
s=tℓ+1 m ℓ
We split this up into two expectations based on the two terms in the definition of G(s,m):
   
(cid:88)
ρi(cid:88)+1−1
(cid:88) 1
tℓ(cid:88)+1−1
(cid:88) (cid:88) L([s,s+m],i)2/3
E  +E  .
m1/3 m1/3·(s−t )2/3
i∈Phases([tℓ,tℓ+1)) s=ρi m≥ρi+1−s s=tℓ m i∈Phases([s,s+m]) ℓ
41For the first expectation above, we have (cid:80) m−1/3 ≤log(T)·(ρ −s)−1/3. Then, summing over s,
m≥ρi+1−s i+1
this becomes (cid:80)ρi+1−1(ρ −s)−1/3 ≤(ρ −ρ )2/3. Thus, the first expectation in the above display is at
most order
log(Ts= )ρ (cid:80)i i+1
(ρ
i+ −1
ρ
)2/i
3.
i∈Phases([tℓ,tℓ+1)) i+1 i
For the second expectation, we use Jensen’s inequality to bound
(cid:88)
L([s,s+m],i)2/3 ≤m2/3·S([s,s+m])1/3,
i∈Phases([s,s+m])
where recall from Definition 12 that S(I) counts the number of phases [ρ ,ρ ) intersecting interval I.
i i+1
Now, plugging this into our earlier expectation gives
(cid:34)tℓ(cid:88)+1−1 (cid:88)m1/3·S([s,s+m])1/3(cid:35)
E .
(s−t )2/3
s=tℓ m ℓ
Then, coarsely bounding S([s,s+m]) by S([t ,t )) and summing over m and s, we obtain the above is at
ℓ ℓ+1
most order log(T)·S([t ,t ))1/3·(t −t )2/3.
ℓ ℓ+1 ℓ+1 ℓ
Combining the above steps, we obtain a per-episode regret bound of
 
(cid:88)
tℓ(cid:88)+1−1 δwt(a♯,a)
E  t |At
|
·111{a∈A t}≤
t
a∈[K] t=tℓ
  
(cid:88) 1
c 19log(K)log3(T)K2/3E 111{E 1}S([t ℓ,t ℓ+1))1/3(t ℓ+1−t ℓ)2/3+ (ρ i+1−ρ i)2/3 + T.
i∈Phases([tℓ,tℓ+1))
(30)
Now, we will sum the above over episodes [t ,t ).
ℓ ℓ+1
F.6 Summing Regret Over Episodes
In Terms of SUW. We first show the total dynamic regret bound of order K2/3 ·T2/3 ·L˜1/3 in
Unknown
Theorem 23. By Hölder’s inequality, summing over episodes gives:
T (cid:32) T (cid:33)1/3
(cid:88) S([t ,t ))1/3·(t −t )2/3 ≤ (cid:88) S([t ,t )) ·T2/3 ≤(L˜ +Lˆ)1/3·T2/3,
ℓ ℓ+1 ℓ+1 ℓ ℓ ℓ+1 Unknown
ℓ=1 ℓ=1
where Lˆ represents the number of realized episodes by Algorithm 2 (i.e., episodes [t ,t ) where t <T +1).
ℓ ℓ+1 ℓ
Since Lemma 24 gives us that Lˆ ≤L˜ , the above is of order L˜1/3 ·T2/3.
Unknown Unknown
Next, again since Lemma 24 implies each phase intersects at most two episodes, we have that summing
(cid:80) (ρ −ρ )2/3 over ℓ gives an upper bound of L˜1/3 ·T2/3 by Jensens’ inequality.
i∈Phases([tℓ,tℓ+1)) i+1 i Unknown
Total Variation Regret Bound. Next, we show the total variation regret bound of order V1/4·T3/4·
T
K1/2+K2/3·T2/3. We’ll follow a similar argument to that of Appendix E.7 for known weight, except taking
care to handle the extra S([t ,t )1/3·(t −t )2/3 term in (30).
ℓ ℓ+1 ℓ+1 ℓ
We first bound the total variation V over an SUW phase [ρ ,ρ ). Consider the winner arm a∗ .
[ρi,ρi+1) i i+1 ρi+1
By Definition 3, there must exist a round t∈[ρ ,ρ ) and a weight w∈∆K such that:
i i+1
(cid:18) K2 (cid:19)1/3
b (a∗,w)−b (a∗ ,w)> .
t t t ρi+1 ρ −ρ
i+1 i
42This implies V ≥(K2/(ρ −ρ ))1/3. By an analogous argument to the SKW total variation regret
[ρi,ρi+1) i+1 i
analysis (with the only modification being the power of K), we have:
L˜
U(cid:88)nknown
K2/3·(ρ −ρ )2/3 ≤V1/4·T3/4·K1/2+K2/3·T2/3.
i+1 i T
i=0
Next, we bound the total variation V over an episode [t ,t ). Let S˜([t ,t )) be the number of
[tℓ,tℓ+1) ℓ ℓ+1 ℓ ℓ+1
phases [ρ ,ρ ) properly contained in episode [t ,t ) or such that [ρ ,ρ )⊆[t ,t ).
i i+1 ℓ ℓ+1 i i+1 ℓ ℓ+1
Then, we have the bound
(cid:88) (cid:88) (cid:18) K2 (cid:19)1/3
V ≥ V ≥ .
[tℓ,tℓ+1) [ρi,ρi+1) ρ −ρ
i+1 i
i:[ρi,ρi+1)⊆[tℓ,tℓ+1) i:[ρi,ρi+1)⊆[tℓ,tℓ+1)
Next, we have that since x (cid:55)→ x−1/3 is convex, we have the above RHS can be further lower bounded by
Jensen’s inequality:
(cid:88) (cid:18) K2 (cid:19)1/3
≥K2/3·
S˜([t ℓ,t ℓ+1))4/3
.
ρ −ρ (t −t )1/3
i+1 i ℓ+1 ℓ
i:[ρi,ρi+1)⊆[tℓ,tℓ+1)
Alternatively, we may also lower bound V by the same argument that we made for an SUW phase
[tℓ,tℓ+1)
[ρ ,ρ ) since [t ,t ) is a period of rounds where every arm incurs significant regret in some period
i i+1 ℓ ℓ+1
[s ,s ]⊆[t ,t ] w.r.t. some weight w. Thus, we also have
1 2 ℓ ℓ+1
(cid:18) K2 (cid:19)1/3
V ≥ .
[tℓ,tℓ+1) t −t
ℓ+1 ℓ
Now, by Hölder’s inequality we have
 1/4
(cid:88)Lˆ
K2/3·S([t ℓ,t ℓ+1))1/3·(t ℓ+1−t ℓ)2/3
≤K1/2·T3/4·(cid:88)Lˆ
K2/3S (t([t ℓ,t
−ℓ+
t1) )) 14 // 33
 +K2/3·T2/3.
ℓ+1 ℓ
ℓ=1 ℓ=1
UpperboundingS([t ,t ))4/3 ≤c ·(S˜([t ,t ))4/3+24/3)usingtheinequality(a+b)4/3 ≤2·(a4/3+b4/3),
ℓ ℓ+1 20 ℓ ℓ+1
we can upper bound the above RHS by order:
(cid:32) (cid:33)1/4
K1/2·T3/4· (cid:88) V +K2/3·T2/3 ≤K1/2·T3/4·V1/4+K2/3·T2/3.
[tℓ,tℓ+1) T
ℓ
F.7 Proof Overview for Theorem 4
Although Theorem 4 does not directly follow from Theorem 23, the analysis will follow a nearly identical
structure while substituting the SUW phases [ρ ,ρ ) with the approximate winner phases [ζ ,ζ ) (see
i i+1 i i+1
Definition 3).
First, we transform the Condorcet dynamic regret to a generalized Borda dynamic regret. Let w =w(a˜ )
t t
where a˜ is the approximate winner arm of the unique approximate winner phase [ζ,ζ ) containing round t.
t i+1
43Then, in this case the Condorcet dynamic regret may be re-written using Definition 3:
(cid:34) T (cid:35)
(cid:88)1
E[RegretC]=E (P (ac,i )+P (ac,j )−1)
2 t t t t t t
t=1
(cid:34) (cid:88)T 1 (cid:35) Sa (cid:88)pprox
≤E (P (a˜ ,i )+P (a˜ ,j )−1) + K2/3·(ζ −ζ )2/3
2 t t t t t t i+1 i
t=1 i=0
(cid:34) (cid:88)T 1 (cid:35) Sa (cid:88)pprox
=E (δ (a˜ ,a˜ )−δ (i ,a˜ )+δ (a˜ ,a˜ )−δ (j ,a˜ )) + K2/3·(ζ −ζ )2/3
2 t t t t t t t t t t t t i+1 i
t=1 i=0
(cid:34) 1(cid:88)T (cid:35) Sa (cid:88)pprox
≤E δwt(i )+δwt(j ) + K2/3·(ζ −ζ )2/3.
2 t t t t i+1 i
t=1 i=0
It remains to bound the expectation on the above RHS by the second sum (up to log terms) in the same
display and then show
Sa (cid:88)pprox
K2/3·(ζ −ζ )2/3
≤min(cid:110)
K2/3T2/3S1/3
,K1/2V1/4T3/4+K2/3T2/3(cid:111)
.
i+1 i approx T
i=0
The key facts will be crucial in showing these claims.
Fact 28. An SUW cannot occur within an approximate winner phase. In other words, supposing a˜ is the
i
approximate winner arm of phase [ζ ,ζ ) (such that (3) holds for a=a˜ and for all s∈[ζ ,ζ ),a∈[K])
i i+1 i i i+1
then we must have that a˜ satisfies for all [s ,s ]⊆[ζ ,ζ ):
i 1 2 i i+1
(cid:88)s2
max δw(a˜ )<K2/3·(s −s )2/3.
s i 2 1
w∈∆K
s=s1
Proof. Note that (3), Jensen’s inequality, and GIC (Condition 1) implies for any weight w ∈ ∆K and
s∈[ζ ,ζ ):
i i+1
(cid:18) K2 (cid:19)1/3
δw(a˜ )=|δw(a∗,a˜ )|≤E [|P (a∗,a)−P (a˜ ,a)|]≤ .
s i s s i a∼w s s s i s−ζ
i
Thus,a˜ doesnotincursignificantworst-casegeneralizedBordaregretoveranyinterval[s ,s ]⊆[ζ ,ζ ). ■
i 1 2 i i+1
Fact 29. The total variation in an approximate winner phase is at least
(cid:18) K2 (cid:19)1/3
V ≥ .
[ζi,ζi+1) ζ −ζ
i+1 i
Proof. Fix a phase [ζ ,ζ ) and consider the winner arm a∗ at round ζ . By Definition 3, there must
i i+1 ζi+1 i+1
exist a round s∈[ζ ,ζ ) such that for some arm a∈[K]:
i i+1
(cid:18) K2 (cid:19)1/3
<δw(a)(a∗,a∗ )≤δw(a)(a∗,a∗ )−δw(a)(a∗,a )≤V ,
ζ i+1−ζ
i
s s ζi+1 s s ζi+1 ζi+1 s ζi+1 [ζi,ζi+1)
where the second inequality follows from GIC. ■
Fact 30 (Analogue of Lemma 24). On event E , for each episode [t ,t ) with t ≤T (i.e., an episode
1 ℓ ℓ+1 ℓ+1
which concludes with a restart), there exists an approximate winner change ζ ∈[t ,t ).
i ℓ ℓ+1
Proof. This follows from Fact 28, as an SUW cannot occur within an approximate winner phase. This means
thatsincearestartimpliesanSUWhasoccurred,anapproximatewinnerchangemusthavealsooccurred. ■
44Now,thegenericbadsegmentanalysisofAppendixF.4canbedonewhilereplacingtheSUWphases[ρ ,ρ )
i i+1
with the approximate winner phases [ζ ,ζ ). Fact 28 ensures that the analogue of Lemma 27 will hold as
i i+1
the approximate winner arm a˜ of phase ζ cannot be evicted by a perfect replay corresponding to a generic
i i
bad segment in phase [ζ ,ζ ). Then, the steps of Appendix F.5 and Appendix F.6 follow mutatis mutandis
i i+1
while using Fact 29 to get the total variation bound.
45