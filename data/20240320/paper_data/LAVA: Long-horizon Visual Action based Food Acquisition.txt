LAVA: Long-horizon Visual Action based Food Acquisition
Amisha Bhaskar, Rui Liu, Vishnu D. Sharma, Guangyao Shi, Pratap Tokekar
Abstract‚ÄîRobotic Assisted Feeding (RAF) addresses the
fundamentalneedforindividualswithmobilityimpairmentsto LAVA
regain autonomy in feeding themselves. The goal of RAF is to
usearobotarmtoacquireandtransferfoodtoindividualsfrom High-level Policy ùùÖ H
thetable.ExistingRAFmethodsprimarilyfocusonsolidfoods, ‚Äúselect wide
leaving a gap in manipulation strategies for semi-solid and primitive‚Äù
deformable foods. This study introduces Long-horizon Visual
Action (LAVA) based food acquisition of liquid, semisolid, Mid-level Policy ùùÖ
M
and deformable foods. Long-horizon refers to the goal of ‚Äúselect the target tofu
and align it to the
‚Äúclearing the bowl‚Äù by sequentially acquiring the food from
center of the bowl‚Äù
the bowl. LAVA employs a hierarchical policy for long-horizon
food acquisition tasks. The framework uses high-level policy
to determine primitives by leveraging ScoopNet. At the mid- Low-level Policy ùùÖ L
level, LAVA finds parameters for primitives using vision. To ‚ÄúExecute trajectory
learned from
carry out sequential plans in the real world, LAVA delegates demonstration‚Äù
action execution which is driven by Low-level policy that uses
parametersreceivedfrommid-levelpolicyandbehaviorcloning
ensuring precise trajectory execution. Fig. 1: System setup for LAVA alongside an illustrative
Wevalidateourapproachoncomplexreal-worldacquisition descriptionoftheproposedframeworkwithsnapshotsoftask
trialsinvolvinggranular,liquid,semisolid,anddeformablefood execution.
types along with fruit chunks and soup acquisition. Across 46
bowls,LAVAacquiresmuchmoreefficientlythanbaselineswith
asuccessrateof89¬±4%,andgeneralizesacrossrealisticplate
variations such as different positions, varieties, and amount of parametrized primitives for food manipulation, employing
food in the bowl. Code, datasets, videos, and supplementary distinct tools and primitives for specific tasks such as skew-
materials can be found on our website. ering[4]‚Äì[7],bitetransfer[4],[8],[9],scooping[2]andeven
end-to-end system [10], [11].
I. INTRODUCTION
This approach, while effective for singular, isolated ac-
For individuals with limited mobility or disabilities, the
tions, falls short in replicating the complex, sequential be-
act of feeding themselves can pose a significant challenge.
haviorsexhibitedbyhumansduringfeeding.Humansadeptly
This challenge has motivated the development of Robotic
combinevariousactions,suchasscoopingbothsolidchunks
Assisted Feeding (RAF) [1] aiming to restore independence
and liquid from a bowl in a single motion or rearranging
and enhance the quality of life for those affected, while also
food items for easier acquisition, demonstrating a nuanced
alleviating the caregiver burden. A key component of such
understanding and strategy that spans the entire meal. This
an assistive feeding system is bite acquisition, i.e., the act
limitation underscores a gap in RAF technology and high-
of a robotic arm picking up morsels of food from a plate to
lightstheneedforanadvancedunderstandingandreplication
transfer the food to a person‚Äôs mouth.
of human-like, long-horizon feeding strategies capable of
Navigating the diverse array of foods‚Äîfrom granular
managingboththerigidityofsolidfoodsandthecomplexity
cereals to semi-solid food such as yogurt and deformable
of deformable items.
food items such as tofu‚Äîwithout breakage or deformation
Recentadvancementsinskill-basedreinforcementlearning
presentssignificantchallengesforRAF[2],[3].Additionally,
(RL)offerpromisingmethodologiesformodelingthesecom-
the dynamic positioning of food chunks within a fluid
plex, long-horizon manipulation sequences in a hierarchical
medium complicates the prediction of their exact location
manner. This entails first learning a high-level policy for
at the time of scooping, requiring sophisticated sensing and
composing skills [12], and then optionally inferring the
real-time adaptation capabilities. This underscores the need
parameters of low-level skills separately [13], [14]. Such
for RAF systems to exhibit not only dexterity but also an
approaches have shown potential, yet they face limitations
advanced understanding of the dynamic nature of various
when applied to the food domain, which demands high-
foodtypesandoperateoveralonghorizontoclearthebowl.
fidelity models for food deformation, visual recognition,
Traditional RAF methodologies have depended on
and utensil interaction not fully captured in current sim-
hard-coded adaptation strategies and low-level vision-
ulations. While VAPORS [7] demonstrates effective long-
horizon planning for specific food items such as noodles, it
All authors are from the University of Maryland, College Park,
MD 20742 USA. {amishab, ruiliu, vishnuds, gyshi, relies heavily on simulation for learning plate dynamics and
tokekar}@umd.edu lacks adaptability and applicability in real-world scenarios
4202
raM
91
]OR.sc[
1v67821.3042:viXraRGB Image o
t
MobileNetV2 Global Average Dense layers
Pooling
(ùëÉ")
! ScoopNet
Wide ! H Mid-level policy ùùÖ M2
Primitive
High-level policy ùùÖ ùùÖ M2 (ùëÉ #" %, —∞" #%)
H Wide Primitive Deep Primitive Direct
Mid-level policy ùùÖ (% $%) (% $&) RGB Image o t Scooping (h)
M1
RGBImage Segmentation Subregions Target Instance
TargetNet (! ) DepthNet (! )
M1 M2
ùúÉ$
R2R3 "
R1 ùúÉ$
%
ùúÉ$
&
W Sca ol ol- pG iu ni gd (e ùù≥d ) (ùëÉ #" ", —∞" #") W Sc (a o %l ol !"- pG "iu ,n —∞i gd ( " !e "d ") ) (x t (,y %t !,A #xl "bi ,g ,y n —∞b, !#Œ≤, "# )) S (c %oD !o "i p #r ,ie n —∞c gt "
!
(h #) )
Low-level policy ùùÖ
L
ùúÉ ùúÉ ùúÉ‚Äô ( )$ $$
Behavioral Trajectory Behavioral
Cloning (! L) Execution Cloning (! L)
Fig. 2: LAVA: System Architecture of LAVA wich employs a high level policy(blue) œÄ to select amongst discrete high
H
level primitives Pk, such as wide primitive and Deep primitive, which then further gets refined by mid-level policy (green)
H
œÄ to select amongst mid-level primitivesPk , low-level vision parametrized policy œÄ (brown) executes trajectory learned
M M L
from Behavioral cloning for long-horizon dextrous food acquisition.
for broader categories of food types. ‚Ä¢ Weintroduceadatasetoffooditems,showcasingdiffer-
Thus, we seek to find an appropriate layer of abstraction ent volumes and spatial arrangements within the bowl.
for feeding, which can leverage the benefits of (1) hierarchi- ‚Ä¢ We evaluate the learned scooping policies through real-
calplanningforlong-horizonmanipulation;(2)vision-based world deployment system with UR5e and end-effector
primitives for fine-grained control; and (3) flexible approach coupled with spoon attachment and D435i RealSense
that can dynamically adapt to the wide variety of challenges camera on the wrist.
presented by different food types. The remaining of the paper is organized as follows.
Recognizingthesechallenges,thisworkintroducesLAVA In Section II, we review related work on robotic assisted
(Long-horizon Acquisition via Visual Action), as a hier- feeding, imitation learning, and long-horizon planning. In
archical policy for sequential planning of food acquisition Section III, we present the problem statement. In Section
(see Figure 1). Our approach is decoupled into three level- IV, we introduce the multi-level policy including high-level
policy:ahigh-levelpolicythatidentifiesprimitivesbasedon policy, mid-level policy, and low-level policy. In Section V,
visual inputs; a mid-level policy to refine these primitives we present and discuss the real robot experimental results
and parameterize the actions of the lower-level policy and and compare them with the baseline. Finally, Section VI,
a low-level policy to use those parameters to rearrange and concludesthepaper withlessonslearnedand possiblefuture
acquire food items and sequentially clear the bowl. work.
The key contributions of this paper are:
II. RELATEDWORK
‚Ä¢ We present a comprehensive hierarchical policy frame- We build on prior works studying multisensory robot
work for long-horizon, visual-action-based food ac- learning and long-horizon task planning both within and be-
quisition that systematically divides the task of food yondthefooddomain.Inthissection,wewilldiscussrelated
acquisition into high-level decision-making, mid-level workinrobot-assistedfeeding,learningfromdemonstration,
action refinement, and low-level execution. and more generally long-horizon planning and control.
‚Ä¢ Our method showcases adaptability and robustness
A. Robotic-Assisted Feeding
acrossadiverserangeoffoodtypes,effectivelyclearing
bowls.Itaddressesandsurpassespreviouslimitationsin Robotic Assisted Feeding (RAF) can be split into two
adaptability to various food types and the challenge of stages: bite acquisition and bite transfer. Previous work in
completing meals. RAF focused on bite acquisition and transfer with the aidof robotic arms and specialized tools such as spoons and feeding. Our work aims to address these gaps by focusing
forks [2], [4], [6], [8], [15]. The incorporation of computer on the adaptation to real-world scenarios and the develop-
vision has enabled these systems to adapt to various food ment of specialized primitives for a wide variety of food
types and user preferences, with models such as SPANet items, challenging the scalability of current approaches and
[5] demonstrating proficiency in mapping food images to introducing the necessity for innovative solutions in robotic
actions. However, challenges remain in handling semi-solid feeding.
and deformable foods, where generalizable strategies are
scarce and bimanual scooping [2] techniques have shown III. PROBLEMSTATEMENT
limited success. Market-available devices [11] offer meal- In this work, we tackle the challenge of sequential bite
time assistance but are constrained by their reliance on acquisition to maximize the success rate and efficiency
teleoperation and the physical limitations of their design. of long-horizon food acquisition to ensure efficient bowl
In isolation, this does not capture many long-horizon real- clearance. The focus is on a variety of food types, ranging
worldfeedingscenarioswithmultipleutensilsandstrategies. fromgranularitemssuchascerealstosemi-solidfoodssuch
Whilepriorresearchhasmadestridesinvisualplanningand asyogurt,anddeformablesubstancessuchastofu,allwithin
manipulation for specific food items [7], a comprehensive a bowl fixed in position and assumed to be scoopable with
approach that addresses the adaptability to a wide array of a spoon.
food types and real-world feeding scenarios is still needed.
We assume access to bowl image observations o ‚àà
R W√óH√óC = O of unknown bowl states S. Here, W, H,
B. Learning from Demonstration +
and C denote the image dimensions. The image is sourced
Learning from Demonstration (LfD) is a methodology
from a camera attached to the wrist of the robotic arm
where robots learn new skills by observing expert demon-
withacustomspoonattachmentasanend-effector.Wehave
strations, which can be performed by humans or intelligent
access to expert demonstration data for robot proprioceptive
agents. This approachis particularly useful fortasks that are
information (joint positions). Our goal is to learn a policy
challenging to pre-program but can be easily demonstrated.
œÄ(œï |o ) that takes RGB images as input (o ) and returns
t t t
LfD has been applied across various domains, including
output as joint angles Œ∏ of the arm for efficient long-
t
robotic assembly in manufacturing [16], path planning for
horizon food acquisition. In this context, long-horizon refers
complex tasks [17], assistive technologies in rehabilitation
to a series of sequential actions aimed at complete bowl
[18], and intricate picking and placing tasks [19]. The
clearance.
technique is divided into three main approaches: kinesthetic
teaching [20], where a human physically guides the robot; IV. PROPOSEDAPPROACH
teleoperation [21], where the robot is remotely controlled;
We formalize the long-horizon food acquisition setting as
and passive observation [22], where the robot learns by
ahierarchicalpolicyœÄ.TodosowedecoupleœÄ intoseparate
watching.Ourresearchprimarilyutilizeskinestheticteaching
high, mid, and low-level sub-policies. We assume access to
to instruct a UR5e robot arm in scooping tasks. Within
K discrete manipulation primitives Pk, k ‚àà 1,...,K, and
LfD‚Äôslearningobjectives,ourfocusisondevelopingpolicies H
learn a high-level policy œÄ which selects amongst these
for handling semi-solid and deformable food items, and H
primitivesbasedonvisualinputo .Themid-levelpolicyœÄ
optimizing their scooping trajectories. t M
further refines this selection, parameterizing the low-level
C. Long-Horizon Planning and Control policy œÄ L based on both the chosen primitive and additional
visual inputs.
Recent works in long-horizon manipulation frameworks
This low-level policy then executes a sequence of actions
have explored separating high-level strategic decision-
Œ∏k, aimed at achieving precise food acquisition. This hierar-
making from detailed motion planning. Traditional task- t
chical arrangement is formalized as follows:
and-motion planning approaches rely on extensive domain
knowledge and fixed task sequences [23]‚Äì[25], but falter ‚Ä¢ High-level policy: œÄ H(P Hk|o t) focuses on selecting the
due to the unpredictable dynamics of food on a plate and manipulation primitive suitable for the current visual
the complexity of state estimation. Model-based planning scene.
has shown promise in tasks such as dough manipulation by ‚Ä¢ Mid-level policy: œÄ M(P Mk ,œà Mk |o t,P Hk) refines this
using environment dynamics learned from visual inputs to choicebyparameterizingactionstailoredtothespecific
plan action sequences [12], [26]. However, these methods food item‚Äôs characteristics.
struggle with the high-dimensional action spaces typical in ‚Ä¢ Low-level policy: œÄ L(Œ∏ tk|P Mk ,œà MK) executes the action
food acquisition. Hierarchical reinforcement Learning of- sequence, utilizing parameters and primitives from the
fers a solution by dividing decision-making into high-level mid-level policy.
strategic planning and execution by discrete, parameterized We consider low-level actions Œ∏ , parameterized by the
t
low-level primitives [27]. While promising in simulation for positionofthetipofaspoon(x,y)andspoonrollandpitch
taskssuchastabletopmanipulation[13],[14],thesemethods (Œ≥,Œ≤) in the wrist frame of reference. As shown in Figure 2
have limitations in real-world application and handling the detailingtheLAVAsetup,wedescribeeachmoduleinLAVA
diverseandcomplexmanipulationtasksrequiredforeffective in further detail.A. High-level Policy custom collection and additional sources, targeting binary
classification of high-level primitives P1,...,Pk. We used
At the highest level of our hierarchical model, the high- H H
level policy œÄ (Pk|o ) uses visual cues to select the most data augmentation (including rotations, zooms, and flips)
H H t
suitablescoopingprimitive‚ÄîWidePrimitive(PW)andDeep to increase robustness against food image variations. Our
H
Primitive (PD), based on the food type present. dataset is available online.
H
1) Wide Primitive (PW): Wide Primitive, is a strategy The network is initially trained on the ImageNet dataset,
H
developed for handling foods that lack cohesion or are de- with a customized final layer for specific task adaptation.
formable, such as tofu or certain types of jelly. This method This configuration, along with a Global Average Pooling
involves using the bowl‚Äôs wall as a guide and support mech- layer and two dense layers ending in a sigmoid activation,
anism for the scooping action. By gently pressing the food uses the Adam optimizer and binary cross-entropy loss for
against the wall of the bowl, it creates a pseudo-cohesive accurate classification. The detailed architecture, ScoopNet,
mass that can be scooped more easily. This technique is is depicted in Figure 3. The output of ScoopNet is softmax
especiallyusefulforfoodsthattendtoscatterorbreakapart, probabilities over high-level primitives.
as the wall provides the necessary containment to gather
and scoop the food effectively. Instance scooping requires B. Mid-level Policy
sophisticated control over the spoon‚Äôs movement, including The Mid-level Policy œÄ (Pk ,œàK|o ,Pk) serves as the
adjusting the angle applied against the food and the bowl M M M t H
intermediary layer that refines and parameterizes the cho-
wall, to achieve the desired outcome without compromising
sen primitive for execution. This refinement is crucial for
the integrity of the food or missing the target. It requires
bridging the gap between high-level strategy selection and
identifying the target instance to not collide with other
low-level action execution.
instances or break them in the process. The wide primitive
1) TargetNet (œÄ ) for Wide Primitive: We have de-
M1
isimplementedwiththeothertwomid-levelprimitivesalign
signed TargetNet, shown in Figure 4, that uses Mask R-
and wall-guided scooping described in Section IV-B.1
CNN, tailored for the task of identifying and segmenting
targetfooditemssuchastofuinabowl,crucialforexecuting
Dense layers
wide primitives. This model precisely segments food items,
enabling the selection of appropriate mid-level primitives:
RGB Images
wall-guidedscoopingandcenteralign(describedlaterinthis
section).
(7, 7, 1280)
ùëÉ" We use a custom dataset annotated for bowl, tofu, and
!
targetscoopingareas,TargetNetemploystransferlearningto
Kx1
accurately segment food items against diverse backgrounds,
(224,224,3) increasing its generalizability. In Section V-D.3, we report
zero-shotgeneralizationresultsforothertypesoffooditems.
1280x1 1024x1
The model‚Äôs training includes a COCOEvaluator to ensure
Fig. 3: ScoopNet outputs the softmax probabilities over the segmentation accuracy meets COCO dataset [29] standards.
high-level primitive depending on the type of food items
present in the image. RGB Image Segmentation Subregions Target Instance
2) Deep Primitive (P HD): Deep Primitive, on the other R3R2R1 (ùëÉ!# ", —∞!# ")
hand, is a straightforward approach designed for foods that
possess enough cohesion to be picked up directly by a
Fig. 4: TargetNet finds the next ‚Äútarget‚Äù item for the wide
spoon without requiring additional support or manipulation.
high-level primitive and the mid-level primitive that decides
This method is particularly effective for liquid and semi-
whether to scoop the target item or to align it first.
solid foods such as yogurt or porridge, where the food‚Äôs
natural consistency allows it to adhere to the spoon when
scooped directly from the top or side. The key to successful Post-training, TargetNet creates a binary mask for pixels
direct scooping lies in the precise control of the spoon‚Äôs that are ‚Äúoccupied‚Äù by instances of food items. We divide
trajectory and depth of penetration into the food, ensuring the surrounding region of interest into sub-regions. If a sub-
that a sufficient quantity is acquired without disturbing region intersects the bowl boundary, it is considered to be
the remaining contents of the bowl excessively. The deep ‚Äúoccupied.‚Äù Otherwise, it is ‚Äúunoccupied.‚Äù A food item is
primitive is implemented with the direct scooping mid-level classified as ‚ÄúR1‚Äù if it is rightmost and closest to the wall,
primitive described in Section IV-B.2 ‚ÄúR2‚Äù if the food item is at the center of the bowl, and
3) ScoopNet (œÄ ): ScoopNet is a network designed to ‚ÄúR3‚Äù otherwise. The subsequent visualization and centroid
H
select between the two high level primitives based on the calculationstepsofdetectedinstanceshelpwithdetermining
type of food, utilizing the MobileNetV2 architecture [28] its location in subregions of the bowl and its location with
as the base. We train on a dataset of 5316 images from a respecttothecenterofthebowl,selectingbetweenmid-levelprimitives ‚ÄîWall-guided Scooping(P1 ) or Align(P2 ) depth estimation in varied food scenarios.
M1 M1
and predicting parameters for low-level policy. Directscooping(P1 ,œà1 )Thedirectscoopingstrategy
M2 M2
Wall-guided Scooping (P1 ,œà1 ) The Wall-guided employs a feedback mechanism centered on a predefined
M1 M1
Scoopingstrategy,parameterizedbyŒ¥‚Äîthecentroiddistance scooping axis, (Œ≤ = 0‚ó¶). The strategy utilizes the trained
of the target instance from the bowl‚Äôs center‚Äîadapts its model on a dataset of correct trajectories taken by an expert
approach based on the target‚Äôs proximity to the bowl‚Äôs wall human to scoop food from the bowl where the input is the
and the sub-region. For food items in subregion R1, the positionoftheroboticarmrelativetothebowlalongwiththe
strategy uses the wall‚Äôs structural support for a scooping estimateddepth(h)ofthefoodreceivedfromDepthNetand
action. Conversely, items in central subregion R2 require a the output is the adjusted trajectory from behavioral cloning
pre-scooping alignment, tactically moving the food towards based on inputs. This real-time adjustment is critical for
the wall to simplify the scooping motion. achievingpreciseinteractionbetweenthescoopandthefood
Align (P2 ,œà2 ) The alignment step is essential for item, ensuring effective scooping without causing displace-
M1 M1
orienting the spoon to the target instance and guiding its ment or spillage and long-horizon acquisition as the level of
movement toward the bowl‚Äôs center. This procedure takes food changes while sequential scooping. Furthermore, this
intoconsiderationthecentroidcoordinatesofthetofu(x ,y ) strategy is enhanced by the implementation of trajectory
t t
and the bowl‚Äôs center (x ,y ) as well as the spoon‚Äôs roll (Œ≥) selection from behavioral cloning.
b b
and pitch (Œ≤). Two key parameters are computed:
C. Low-level policy
‚Ä¢ Spoon Orientation Angle: Calculated as Œ≥ =
arctan(yb‚àíyt), this angle determines the necessary WeuseBehavioralCloning(œÄ L)withkinestheticteaching
rotationx ob f‚àí tx ht e spoon to align with the target instance, to adapt scooping actions for different food textures and
ensuring the spoon is positioned for optimal interaction consistencies,improvingtherobot‚Äôsperformanceinassistive
and is untilted for planar push (Œ≤ =0‚ó¶). feeding at the lowest level. Various food items, with their
‚Ä¢ Instance Push distance: Determined by (x t,y t,x b,y b), uniquerequirementsforscoopingtechniques,necessitatethe
the instance is pushed from its current position towards modelingofdistinctoptimalscoopingtrajectories,especially
the bowl‚Äôs center, optimizing the positioning for the for semi-solid and deformable foods. This process includes
scooping action. collecting demonstration data on joint positions, velocities,
andtimestampstoapproachthescoopingtaskasatrajectory
2) DepthNet (œÄ ) for Deep Primitive: DepthNet is
M2
optimization problem within the robot arm‚Äôs joint space.
designed for depth detection of food in a bowl based on
The objective is to minimize a cost function J(œÑ) over
visual input o and high-level primitive received from high-
t (cid:82)T
a trajectory œÑ, represented as J(œÑ) = L(q(t),qÀô(t))dt,
level policy. The architecture of DepthNet is outlined in 0
where q(t) and qÀô(t) denote the robot‚Äôs joint positions and
Figure 5.
velocities at time t, respectively, and L(¬∑) is an instanta-
(224,224,32) neous cost function penalizing deviations from the optimal
(112,112,64) trajectory. The Weiszfeld algorithm [30], [31] is used for
RGB Images (56,56,64) 100352 32 1 this optimization, finding a trajectory xÀÜ that minimizes the
ùëÉ!# ", —∞# !" sum of Euclidean distances to demonstrated trajectories. It
iteratively refines xÀÜ until the adjustment falls below a small
(224,224,3) Convolution+RELU MaxPooling Dropout threshold œµ.
Flatten Batch Normalization Dense
The algorithm updates the estimate of xÀÜ using xÀÜ =
k+1
Fig. 5: DepthNet detects the depth (h) of the food in the (cid:80)n pi
bowl.
(cid:80)ni=1 |xÀÜk‚àí 1pi|2, iterating until the change in xÀÜ between it-
erai t= i1 on|xÀÜ sk‚àí ip si|2
below a predefined threshold œµ. This method
DepthNet utilizes a vision-based approach to estimate the determines optimal trajectories for the robot arm‚Äôs joints,
volume of food in a bowl, employing a Sequential model enhancing the robot‚Äôs scooping accuracy and effectiveness.
withconvolutionallayersof32,64,and128filtersforfeature
V. EXPERIMENTS
extraction. These layers are augmented with batch normal-
ization for improved training stability and dropout layers For the experiments, we begin by describing the experi-
at rates of 0.25 and 0.5 to prevent overfitting. MaxPooling mental setup. Following this, we discuss the data collection
layershelpreducethedimensionsoffeaturemaps,increasing procedureandthebaselineforcomparison.Subsequently,we
the model‚Äôs efficiency. After convolutional processing, the present and analyze the experimental results.
model uses a flattened layer for data restructuring, followed
A. Experimental Setup
by a dense layer with 32 neurons (using ‚Äòrelu‚Äô activation)
for feature processing. The architecture culminates in a final The setup comprises a UR5e robot arm, a custom spoon
dense layer with a single neuron (using ‚Äòlinear‚Äô activation) attachment, a RealSense camera, and a fixed-position bowl,
to predict the food‚Äôs depth. DepthNet has been trained on a depicted in Figure 1. The spoon is affixed to the arm, with a
dataset of 1000 cereal images, categorized into three depth lengthmeasuring10.0cm.TheRealSensecameraisattached
rangesinthebowl:5.5cm,4cm,and2cm,enablingprecise to the wrist of the arm.During experiments, we explore varied configurations system.Thisallowstherobottomovetothebowl‚Äôslocation,
across the amount, size, position, and depth of food as well adjusting to a predetermined height and orientation.
as different food types including granular cereals, liquid In the case of FTS, during tests with various food items
water, and semi-solid yogurt in the bowl. Food position in a stationary bowl position, wrist 2 of the robot arm is
configurationsencompassmultiplenumbersoftofuandfruit rotated by ‚àí0.6 radians to start the scooping action along a
chunksplacedindifferentinstancepositionsacrossthebowl. predefined trajectory.
The varied amount and food depth included cereals, water, Conversely,LAVA-low,employsthesamelow-levelpolicy
yogurt, and jelly filled at different depth levels inside the œÄ as LAVA for scooping. For deformable food and fruit
L
bowl.Additionally,weconducttestswithtofuchunksinside chunks, we stick with the wall-guided scooping trajectory
soup as shown in Figure 8. For each food type, and depth, for the R1 region(see Figure. 4 for reference) and keep
weconduct10trialsoflong-horizonfoodscoopingattempts rotating the bowl every 45 degrees constantly so that the
and for each position configuration in case of multiple tofu spoon can reach all the instances in the bowl near the wall
and fruit chunks, we conduct 5 trials of long-horizon food andgetsmaximumcoverage.Incontrastforgranular,liquid,
scooping attempts. andsemi-solidfoodswestickwithdirectscooping,adjusting
its approach based on the depth of the food within the bowl.
1.0 1000 Timeout This adjustment occurs once a predefined depth threshold
is reached, to effectively target the lower layers of food,
0.8
800
ensuring thorough bowl clearance.
0.6 600
D. Experimental Results
0.4 400
In this section, we present and analyze the experimental
0.2 200
results.WefirstpresentthesuccessrateofLAVA‚Äôsnetworks.
0.0 Granular Liquid* SemiSoli Dd e* forma Frbl uie
t
chunks* Soup* G0 ranular Liqui Sd e* miSoli Dd e* form Fa rb ul ite chunks* Soup* T
e
thvh eae ln bu,
aa
stf
e
eo ll iil nto
s
ew
p
mi en
r
eg
f to
ht
r
oh
m
de
sa
.t nr
c
Wa ein ei on
tn
eg
st
tho aef ct rrh ooe
sb
soh
t
vie aar rna
id
erc dh
c
foic oma ol
p
dap iro
te
eli mic ty s,
w
aw
i nth
de
(a) Overall Success Rate (‚Üë) (b) Total Time (‚Üì) varied food configurations, including granular food cereals,
1.0 1.0 liquid food water, semi-solid yogurt, deformable tofu, and
0.8 0.8 multi-mediumsoupwithtofuchunks.Toassessperformance,
we employ the criteria of success rate, which indicates the
0.6 0.6
successful scooping of food items from a bowl without
0.4 0.4 spillage and breakage and successful long-horizon food
0.2 0.2 acquisition by clearing the bowl efficiently. Instances where
some spillage occurs are considered partial success.
0.0 0.0
Granular Liquid* SemiSoli Dd e* forma Frbl uie
t
chunks* Soup* Deformable
Fruit
chunks* Soup* 1001 %) L aA cV cA ur‚Äôs acyNe it nwo cr hk ooS su ic nc ges cs orr ra et ce ts: hiS gc ho -lo ep vN elet pra ic mh ii te iv ve ed
s
across 46 bowls, TargetNet accurately predicted bite targets
(c) Spillage (‚Üì) (d) Breakage (‚Üì)
at 87.9% over 83 instances, and DepthNet successfully
LAVA LAVA-low FTS determined correct spoon depths for bite sizes at 85.7%
across 175 instances, demonstrating the LAVA networks‚Äô
Fig. 6: Breakdown of experimental performance comparison
effectiveness in robotic-assisted feeding.
between LAVA, LAVA-low, and Fixed Trajectory Scoop-
2) Comparison with Baselines: In our experimental anal-
ing(FTS). ‚àó represents zero-shot experiments.
ysis, we evaluate the success rates of LAVA against two
baselinemodels,Lava-lowandFTS,acrossavarietyoffood
B. Data Collection types and scooping dynamics, as shown in Figures 6 and
We collected data through kinesthetic teaching, which 7. Our evaluation focused on several key metrics: efficiency
encompassedtwodifferenttypesoftrajectory‚Äîwall-guided (total time taken to clear the bowl), scoop size, and spillage
scooping and direct scooping, with twenty-five demonstra- for granular, semisolid, and liquid foods. For deformable
tions recorded for each category with different parameters, foods and fruit chunks, we recorded configuration, number
focusing on RGB images and robot joint positions. This ofscoopattempts,andinstancesoffoodbreakage.Inpartic-
process was limited to cereals and tofu. ular, for complex scenarios such as soup with tofu chunks,
our assessment averaged efficiency, spillage, and breakage
C. Baselines
metrics.
Inourstudy,weusedtwobaselines,LAVA-lowandFixed How do all the methods handle the challenge of
Trajectory Scooping (FTS). For both baselines, the process scooping liquids, such as water and soup, which are
begins with detecting the bowl in an RGB image using prone to spillage? The analysis, particularly visible in
RetinaNet [32]. Upon identifying the bowl, we calculate Figure 6c, reveals that both baseline models struggle with
its centroid and map this position to the robot‚Äôs coordinate thefluidityofwaterandsoup,leadingtosignificantspillage.
etaR
sseccuS
etaR
egallipS
)s(
emiT
etaR
egakaerB100 100 100
80 80 80
60 60 60
40 40 40
LAVA
20 LAVA 20 LAVA 20 FTS
LAVA-low LAVA-low LAVA-low
0 0 0
0 100 200 300 400 0 50 100 150 0 100 200 300 400 500
Time (s) Time (s) Time (s)
(a) 5 tofu chunks. (b) 4 tofu chunks. (c) Cereals.
Fig. 7: Individual trials comparison between LAVA, LAVA-low and FTS. Subfigures (a) and (b) show the comparison with
different tofu configurations, and (D) show the comparison with cereals.
The FTS model, with its fixed end-effector orientation and upon the limitations of existing models.
height, is not equipped to adjust to the varying dynamics of
3) Zero-shot Generalization: As detailed in Section V-B,
liquid scooping, resulting in spillage and ineffective scoop-
ourdatacollectionprocessexclusivelyinvolvedthetranspar-
ing. LAVA-low initially copes but struggles as water levels
entglassbowlcontaininggranularcerealsandtofu.However,
decrease,showinginefficiencyinmaintainingadequatescoop
we evaluated our approach to soup with tofu chunks and
sizes.Incontrast,LAVAadeptlyadjuststoreal-timechanges
different food types such as liquid water and semi-solid
infooddepth,achievingoptimalscoopsizesandminimizing
yogurt, and solid apple chunks during testing. Remarkably,
spillage for efficient bowl clearance.
our approach demonstrates robust performance across these
What about the acquisition of more solid, yet de-
varied configurations, as depicted in Figure 6 and 8
formable food types, such as tofu? Our findings, demon-
Especially with soup and tofu chunks, scooping up both
stratedinFigures7a,7b,and6d,indicatethatBothbaselines
the solid pieces and the liquid at the same time is tricky.
encounter issues with deformable foods such as tofu, often
Oursystem,LAVA,isdesignedtoadjusttothesechallenges.
resulting in food breakage. The FTS model‚Äôs rigid scooping
Despite the tofu chunks‚Äô tendency to float away from the
motiondamagesthefood,whileLava-low,despitemanaging
desired central scooping area, LAVA‚Äôs adaptive strategy
to scoop, causes tofu to accumulate and break as shown
realignsandreorientstoscoopthetofueffectively.Following
by instances of food breakage in Figure 6d due to lack
the tofu acquisition, LAVA continues to adapt and clear the
of strategic food prioritization based on subregions. LAVA,
remaining soup, showcasing its capability to handle various
however, prioritizes tofu chunks based on their subregion,
food textures and types within the same meal, leading to
aligning them for easier access and significantly reducing
efficient bowl clearance.
breakage, mimicking human scooping strategies.
How does each method fare in preventing spillage
and ensuring efficient scoop attempts with solid foods
such as fruit chunks? The evaluation as visible in Figure
Failure
6c and 6a reveals that the baselines are less adept with
solid,irregularlyshapedfoodssuchasfruitchunks,proneto
rolling or falling off the spoon. This issue is exacerbated for
fruitswithcurvedsurfaces.LAVA,employinganalign-then-
scoopstrategy,ensuresbetteralignmentandsignificantlyless
spillagebyadjustingtothefruit‚Äôsshapeforsecurescooping.
We see that LAVA consistently outperforms the baselines, Success
achievinghighersuccessratesandmoreeffectiveplateclear-
ance.ItsurpassesFTSandLava-lowbyadaptingitsstrategy
for efficient, minimal-breakage scooping across all tested
Fig. 8: (Zero-shot) long-horizon food acquisition with tofu
food types, demonstrating the benefits of its hierarchical
chunks in soup. The top sequence of images shows the
policy framework. As expected, FTS and Lava-low, limited
spoon aligning the target tofu towards the bowl‚Äôs center,
bytheirstaticapproaches,failtooptimizeforfuturescooping
which then drifts away during the scooping attempt due to
advantages, leading to increased breakage and inefficiency,
the soup‚Äôs fluidity. The bottom sequence shows the system‚Äôs
especially without considering food prioritization and ar-
subsequentattempttorealignthetofutothecenter,followed
rangement strategies.
by a successful scooping action.
LAVA‚Äôs comprehensive strategy ensures efficient, adap-
tive, and precise food acquisition, significantly improving
deriuqcA
meti
%
deriuqcA
meti
%
deriuqcA
thgieW
%VI. CONCLUSION,LIMITATIONANDFUTUREWORK [13] M. Dalal, D. Pathak, and R. R. Salakhutdinov, ‚ÄúAccelerating robotic
reinforcementlearningviaparameterizedactionprimitives,‚ÄùAdvances
Inthiswork,wehavedevelopedandpresentedahierarchi-
inNeuralInformationProcessingSystems,vol.34,pp.21847‚Äì21859,
cal policy framework designed to enhance robotic systems‚Äô 2021.
capability in the acquisition of diverse food types, ranging [14] S.Nasiriany,H.Liu,andY.Zhu,‚ÄúAugmentingreinforcementlearning
with behavior primitives for diverse manipulation tasks,‚Äù in 2022
from liquids to solids and deformable items. Through inte-
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
grating DepthNet, TargetNet, and ScoopNet, our approach 2022,pp.7477‚Äì7484.
leverages representation learning, alongside sophisticated [15] T. Bhattacharjee, G. Lee, H. Song, and S. S. Srinivasa, ‚ÄúTowards
robotic feeding: Role of haptics in fork-based food manipulation,‚Äù
planning and execution strategies, to address the challenges
IEEERoboticsandAutomationLetters,vol.4,no.2,pp.1485‚Äì1492,
associated with the variability in food textures, sizes, and 2019.
positions within the bowl. [16] Z. Zhu and H. Hu, ‚ÄúRobot learning from demonstration in robotic
assembly:Asurvey,‚ÄùRobotics,vol.7,no.2,p.17,2018.
Our experimental analysis demonstrates the framework‚Äôs
[17] Z.Xie,Q.Zhang,Z.Jiang,andH.Liu,‚ÄúRobotlearningfromdemon-
better performance in achieving better efficiency, minimum stration for path planning: A review,‚Äù Science China Technological
spillage, and breakage as well as adaptive food scooping Sciences,vol.63,no.8,pp.1325‚Äì1334,2020.
[18] C.Lauretti,F.Cordella,E.Guglielmelli,andL.Zollo,‚ÄúLearningby
compared to baseline models. Specifically, it showcases
demonstration for planning activities of daily living in rehabilitation
improvements in success rates across various food configu- andassistiverobotics,‚ÄùIEEERoboticsandAutomationLetters,vol.2,
rations. Despite the promising result towards generalization, no.3,pp.1375‚Äì1382,2017.
[19] R.Rahmatizadeh,P.Abolghasemi,L.Bo¬®lo¬®ni,andS.Levine,‚ÄúVision-
limitations exist, particularly in handling thin, flat, or irreg-
basedmulti-taskmanipulationforinexpensiverobotsusingend-to-end
ular foods needing specialized strategies. Future efforts will learningfromdemonstration,‚Äùin2018IEEEinternationalconference
focus on broadening the action space for diverse food types onroboticsandautomation(ICRA). IEEE,2018,pp.3758‚Äì3765.
[20] B. Akgun, M. Cakmak, J. W. Yoo, and A. L. Thomaz, ‚ÄúTrajectories
and exploring efficient data acquisition methods, including
and keyframes for kinesthetic teaching: A human-robot interaction
leveraging internet video resources for complex food han- perspective,‚Äù in Proceedings of the seventh annual ACM/IEEE inter-
dling strategies in real-world scenarios. nationalconferenceonHuman-RobotInteraction,2012,pp.391‚Äì398.
[21] W. Si, N. Wang, and C. Yang, ‚ÄúA review on manipulation skill
REFERENCES acquisitionthroughteleoperation-basedlearningfromdemonstration,‚Äù
CognitiveComputationandSystems,vol.3,no.1,pp.1‚Äì16,2021.
[1] S.W.Brose,D.J.Weber,B.A.Salatin,G.G.Grindle,H.Wang,J.J. [22] D.Vogt,S.Stepputtis,S.Grehl,B.Jung,andH.B.Amor,‚ÄúAsystem
Vazquez,andR.A.Cooper,‚ÄúTheroleofassistiveroboticsinthelives forlearningcontinuoushuman-robotinteractionsfromhuman-human
ofpersonswithdisability,‚ÄùAmericanJournalofPhysicalMedicine& demonstrations,‚Äùin2017IEEEInternationalConferenceonRobotics
Rehabilitation,vol.89,no.6,pp.509‚Äì521,2010. andAutomation(ICRA). IEEE,2017,pp.2882‚Äì2889.
[2] J. Grannen, Y. Wu, S. Belkhale, and D. Sadigh, ‚ÄúLearning bi- [23] S.Srivastava,E.Fang,L.Riano,R.Chitnis,S.Russell,andP.Abbeel,
manual scooping policies for food acquisition,‚Äù arXiv preprint ‚ÄúCombined task and motion planning through an extensible planner-
arXiv:2211.14652,2022. independent interface layer,‚Äù in 2014 IEEE international conference
[3] P.Sundaresan,J.Wu,andD.Sadigh,‚ÄúLearningsequentialacquisition onroboticsandautomation(ICRA). IEEE,2014,pp.639‚Äì646.
policiesforrobot-assistedfeeding,‚ÄùarXivpreprintarXiv:2309.05197,
[24] C.R.Garrett,R.Chitnis,R.Holladay,B.Kim,T.Silver,L.P.Kael-
2023. bling, and T. Lozano-Pe¬¥rez, ‚ÄúIntegrated task and motion planning,‚Äù
[4] D. Gallenberger, T. Bhattacharjee, Y. Kim, and S. S. Srinivasa, Annual review of control, robotics, and autonomous systems, vol. 4,
‚ÄúTransfer depends on acquisition: Analyzing manipulation strategies pp.265‚Äì293,2021.
forroboticfeeding,‚Äùin201914thACM/IEEEInternationalConference
[25] R.Chitnis,D.Hadfield-Menell,A.Gupta,S.Srivastava,E.Groshev,
onHuman-RobotInteraction(HRI). IEEE,2019,pp.267‚Äì276. C. Lin, and P. Abbeel, ‚ÄúGuided search for task and motion plans
[5] R. Feng, Y. Kim, G. Lee, E. K. Gordon, M. Schmittle, S. Kumar, using learned heuristics,‚Äù in 2016 IEEE International Conference on
T. Bhattacharjee, and S. S. Srinivasa, ‚ÄúRobot-assisted feeding: Gen- RoboticsandAutomation(ICRA). IEEE,2016,pp.447‚Äì454.
eralizing skewering strategies across food items on a plate,‚Äù in The [26] H. Shi, H. Xu, Z. Huang, Y. Li, and J. Wu, ‚ÄúRobocraft: Learn-
InternationalSymposiumofRoboticsResearch. Springer,2019,pp. ing to see, simulate, and shape elasto-plastic objects in 3d with
427‚Äì442. graph networks,‚Äù The International Journal of Robotics Research, p.
[6] T. Bhattacharjee, G. Lee, H. Song, and S. S. Srinivasa, ‚ÄúTowards 02783649231219020,2023.
robotic feeding: Role of haptics in fork-based food manipulation,‚Äù [27] S. Pateria, B. Subagdja, A.-h. Tan, and C. Quek, ‚ÄúHierarchical
IEEERoboticsandAutomationLetters,vol.4,no.2,pp.1485‚Äì1492, reinforcement learning: A comprehensive survey,‚Äù ACM Computing
2019. Surveys(CSUR),vol.54,no.5,pp.1‚Äì35,2021.
[7] P. Sundaresan, S. Belkhale, and D. Sadigh, ‚ÄúLearning visuo-haptic [28] M.Sandler,A.Howard,M.Zhu,A.Zhmoginov,andL.-C.Chen,‚ÄúMo-
skewering strategies for robot-assisted feeding,‚Äù in 6th Annual Con- bilenetv2: Inverted residuals and linear bottlenecks,‚Äù in Proceedings
ferenceonRobotLearning,2022. oftheIEEEConferenceonComputerVisionandPatternRecognition
[8] S. Belkhale, E. K. Gordon, Y. Chen, S. Srinivasa, T. Bhattacharjee, (CVPR),June2018.
and D. Sadigh, ‚ÄúBalancing efficiency and comfort in robot-assisted [29] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B.
bite transfer,‚Äù in 2022 International Conference on Robotics and Girshick, J. Hays, P. Perona, D. Ramanan, P. Doll‚Äôa r, and C. L.
Automation(ICRA). IEEE,2022,pp.4757‚Äì4763. Zitnick, ‚ÄúMicrosoft COCO: common objects in context,‚Äù CoRR, vol.
[9] R. K. Jenamani, D. Stabile, Z. Liu, A. Anwar, K. Dimitropoulou, abs/1405.0312, 2014. [Online]. Available: http://arxiv.org/abs/1405.
and T. Bhattacharjee, ‚ÄúFeel the bite: Robot-assisted inside-mouth 0312
bite transfer using robust mouth perception and physical interaction- [30] A. Beck and S. Sabach, ‚ÄúWeiszfeld‚Äôs method: Old and new results,‚Äù
aware control,‚Äù in Proceedings of the 2024 ACM/IEEE International JournalofOptimizationTheoryandApplications,vol.164,pp.1‚Äì40,
ConferenceonHuman-RobotInteraction,2024,pp.313‚Äì322.
2015.
[10] E. K. Gordon, R. K. Jenamani, A. Nanavati, Z. Liu, H. Bolotski, [31] E. Weiszfeld and F. Plastria, ‚ÄúOn the point for which the sum of
R. Karim, D. Stabile, A. Kashyap, B. H. Zhu, X. Dai et al., ‚ÄúAn the distances to n given points is minimum,‚Äù Annals of Operations
adaptable, safe, and portable robot-assisted feeding system,‚Äù arXiv Research,vol.167,pp.7‚Äì41,2009.
preprintarXiv:2403.04134,2024.
[32] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dolla¬¥r, ‚ÄúFocal loss
[11] https://meetobi.com/.(2023).[Online].Available:https://meetobi.com/ fordenseobjectdetection,‚ÄùinProceedingsoftheIEEEinternational
[12] X. Lin, C. Qi, Y. Zhang, Z. Huang, K. Fragkiadaki, Y. Li, C. Gan, conferenceoncomputervision,2017,pp.2980‚Äì2988.
and D. Held, ‚ÄúPlanning with spatial-temporal abstraction from
point clouds for deformable object manipulation,‚Äù arXiv preprint
arXiv:2210.15751,2022.