Vid2Robot: End-to-end Video-conditioned Policy
Learning with Cross-Attention Transformers
Vidhi Jain1,2 Maria Attarian1,3 Nikhil J Joshi1 Ayzaan Wahid1 Danny Driess1 Quan Vuong1 Pannag R Sanketi1
Pierre Sermanet1 Stefan Welker1 Christine Chan1 Igor Gilitschenski3 Yonatan Bisk2 Debidatta Dwibedi1
1Google DeepMind Robotics 2Carnegie Mellon University 3University of Toronto
Robot observes human
Prompt Video showing task “Pick up coke can and place on counter”
doing a task.
Vid2Robot outputs actions
Initial State Policy Rollout Video
to complete shown task in
its own environment.
Fig. 1: Overview. Vid2Robot is a video-conditioned robot policy. Given a human demonstration (top), Vid2Robot recognizes the task
semantics and performs the same task based on the robot’s current visual observation (bottom left). A successful trajectory is presented on
the bottom right.
Abstract—While large-scale robotic systems typically rely on of dishwasher a specific household uses to entirely different
textual instructions for tasks, this work explores a different ways to clean the house. For known skills, humans simply
approach: can robots infer the task directly from observing
communicateinnaturallanguage,butwhennuanceisrequired
humans? This shift necessitates the robot’s ability to decode
or a skill is novel, we revert to demonstrations. For example,
human intent and translate it into executable actions within its
own physical constraints and environment. we might show how a particular microwave works or how we
We introduce Vid2Robot, a novel end-to-end video-based prefer our cabinets to be organized. To enable seamless robot
learning framework for robots. Given a video demonstration of deployment, robots need the same ability for generalization
a manipulation task and current visual observations, Vid2Robot
from demonstration for learning new policies that comes so
directlyproducesrobotactions.Thisisachievedthroughaunified
naturally to humans.
representation model trained on a large dataset of human video
and robot trajectory. The model leverages cross-attention mech- Humans can infer the intentions of other humans based on
anismstofusepromptvideofeaturestotherobot’scurrentstate third-person visual observations. Oftentimes, we use social
and generate appropriate actions that mimic the observed task. reasoning and common sense to understand others’ goals
To further improve policy performance, we propose auxiliary
implicitly.Thisabilityisleveragedbothaschildrenandadults
contrastive losses that enhance the alignment between human
(e.g. viaHow-To videos [30])for learning anythingwhere the
and robot video representations.
We evaluate Vid2Robot on real-world robots, demonstrating mechanicalnuanceofthetaskishardtocaptureinstillimages
a 20% improvement in performance compared to other video- or text [5] (e.g. how to knead dough or knit). If robots can
conditioned policies when using human demonstration videos. also be taught to understand the intentions of other agents, it
Additionally, our model exhibits emergent capabilities, such as
might allow them to better interact with humans and perform
successfullytransferringobservedmotionsfromoneobjecttoan-
tasks more efficiently.
other,andlong-horizoncomposition,thusshowcasingitspotential
for real-world applications. Project website: vid2robot.github.io. Thisworkfocusesonvisualimitationlearning,whererobots
learntoperformtasksbywatchingvideodemonstrations.This
setup offers several advantages. First, it allows robots to learn
I. INTRODUCTION
from agents with different embodiment, enabling new skill
The path to creating versatile robots that provide assistance acquisition without tele-operation. Second, it enables robots
inpeople’sdailyroutinesrequiresthemtolearnnewskillson- to learn from experts, even if they are not situated with the
the-go. These range from small preferences like which brand robot. Finally, visual imitation learning is ideal for teaching
4202
raM
91
]OR.sc[
1v34921.3042:viXratasks that are difficult or impossible to describe in words. real-world multi-task robotic control. We present the key con-
Existing multi-task robot manipulation models (e.g. RT- tributionsofourworkasfollows:(1)Wepresentatransformer-
1 [7], RT-2 [8], and RT-X [33]) use language conditioning based policy to encode video task specification, demonstrated
to output a robot trajectory. This reliance of text alone for by either robot or human agent embodiments (§II). (2) We
task specification makes it difficult for robots to handle pol- encourage alignment between the prompt and robot video
ysemy and tasks whose executions vary dramatically based representations using three contrastive auxiliary losses during
oncontext.Forexample,‘opendrawer’,‘opencabinet’,‘open training (§II-E) (3) Through real robot experiments, we find
container with lid’ and ‘open jar with screw cap’ might share ourvideoconditionedpolicyisbetterthanbaselinesonhuman
thesameverbbutrequireverydifferentmotorcontrolforeach promptvideos.Furthermore,ourpolicyisbetteratcross-object
interaction. Here the agent should not generalize its policy, motion transfer (§III).
whereas it should generalize from one drawer to others that
II. APPROACH
vary in type, color and shape. For this reason, there are a
broad range of tasks for which it is hard to design primitives A. Preliminaries
for high-level planning approaches [25, 2]. Our objective is to design a robotic system that takes in a
Another common approach has been to use a final goal prompt video of a manipulation task and outputs actions that
image in goal-conditioned behavior cloning tasks [31, 24]. accomplish the task demonstrated in the video. This system
Whileseveraltaskspecificationscanbedefinedintermsofthe needs to infer the underlying task from the prompt video
resulting state of the environment, there are others for which (which might have a different setup or embodiment than the
the manner in which the action is performed is part of the robot)andthenmanipulatetheobjectsinitsownenvironment
specification. For example, ‘hold the flag’ and ‘wave the flag’ toachievetheinferredtask.Specifically,wearegivenaprompt
can have the same final goal image. This ambiguity can be video V and the robot state S = {x }t where x
t i i=t−k−1 i
resolved through the use of several sub-goal frames, that is, is the frame from the robot’s camera stream at time i, k
video conditioning. is the maximum number of historical frames, and t is the
Whilelanguageconditionedpoliciesachievesomewhathigh current time-step. We train a policy π(a |S ,V) that infers
t t
success rates, video conditioned policies have lagged behind the underlying task from V and predicts task relevant action
in performance, as shown in prior work [21]. Cases of good a . To train this model, we need a dataset of paired prompt
t
performance[40]withvideoconditioningrequiretheprovided videos and robot trajectories. We will discuss in detail how to
video to be from the same workspace with limited variability. create paired datasets below.
Based on observations, we identify three main challenges for
B. Datasets
video conditioned policies: (1) High dimensional data: Raw
videos are high dimensional data that require more compute To train a video-conditioned robot policy we need a dataset
and memory to process. This makes video conditioned multi- of pairs: prompt videos and robot trajectories performing the
task policies difficult to train at scale. (2) Variability in task same task. In this work, we explore prompt videos where the
specification: There can be significant variance in how people task is performed by both humans and robots. To create this
performthesametask.Demonstrationsforatasklike‘unstack dataset, we rely on three classes of data:
the cups’ can have both visually distinctive and physically di- 1) Robot-Robot: We pair existing robot-robot videos of the
versecups,inadditiontochangesinthebackgrounddistractors sametask.Forthispairingweconsidertwovideostomatch
and lighting conditions. This leads to high variability in task if they are performing the same task in different settings.
specification for a policy that needs to perform the same task We define ‘task‘ based on natural language instructions
in a new setting. (3) Limited Availability of Training Data: used when recording robot trajectories. These instructions
While there is an abundance of unlabeled video data on the typicallyconsistofoneortwoverbssurroundedbynouns,
internet,obtaininglabeledvideodatasetsforspecifictasksthat such as ‘place water bottle upright’, ‘move the coke can to
our robots are capable of doing is challenging. the green chip bag’ or ‘open top drawer’. The objective of
Despitethesechallenges,asnoted,videoconditionedpolicy thispairingistwo-fold:first,tobeabletotakeadvantageof
learning is a core challenge robots need to master. Therefore, analreadylabeledandcollecteddatasetofrobottrajectories
to reduce the reliance on detailed and potentially ambiguous and second to ensure robots are able to imitate when the
language prompts, we aim to enable physical visual demon- same task is demonstrated in a different environment.
strations as a another way for task specification. To this end, 2) Hindsight Human-Robot: Here we use the task instruc-
we study how end-to-end models with video-conditioning can tions from the robot trajectories dataset and ask one to
used to specify tasks to robot. five human participants to perform the task and record
We aim to develop an end-to-end system that enables rapid a demonstration video from the robot’s perspective/view.
adaptation to tasks specified in the form of video demonstra- The set of instructions are the same as before, but there
tion.Unlikepriorworkthateitherlearnedrepresentationsfrom is a significant embodiment and speed variability due to
videos for only object and verb recognition [21] or learned different humans performing the task with left or right
motor control in simulation [44], our work demonstrates the hands and at a randomized robot camera angle. This
applicability of end-to-end learned video representations for requires some manual effort but provides us with a lot ofRobot-Robot Robot Different
Hindsight Human Different
Human-Robot
Co-located
Human Same
Human- Robot
Fig.2:Datasetcreation.(toprow)HereweshowaRobot-Robotvideopairforplacingtherxbarintotopdrawer.Wesimilarlypairexisting
robot-robot videos performing the same task. (middle row) Here we show Hindsight Human-Robot paired videos for picking a coke can
from the bottom drawer and placing it on the counter task. We use the task instructions from robot trajectories and ask human participants
toperformthetaskandrecordademonstrationvideofromrobot’sperspective/view.(bottomrow)HereweshowaCo-locatedHuman-Robot
pair of videos for placing the pipe wrench in the toolkit. We record both a human demonstration and by a robot teleoperation in a same
workspace. Different workspaces can be used to perform the same task instruction, thus, eventually resulting in pairs with visually diverse
prompts and robot state observations. More details in §II-B.
paired data for training the policy for the available set of state-prompt encoder, and (4) robot action decoder. The full
instructions in the robot dataset without having to collect architecture is illustrated in Figure 3 and each of the modules
new robot trajectories. are detailed below:
3) Co-located Human-Robot In this case, a human and a (1) Prompt Video Encoder encodes the video demonstration
robot perform the same task in the same workspace. We provided as a reference to convey the desired task semantics.
used this approach to collect human demonstrations and The prompt video encoder implicitly learns to infer what task
robot trajectories in diverse spaces such as a living space shouldbeperformedandhowitneedstobedone.Theprompt
with sofas, a meeting room with whiteboards, hardware encoder consists of a per-frame Image encoder ϕ (ViT [14])
p
workstations with toy tools, a kitchen with a countertop, followed by a Perceiver Resampler [1, 19] ψ . The output of
p
refrigerator and sink, a storage supplies area, and more. thepromptencoderψ (ϕ (V))=z isasetofN tokens
p p prompt
We show examples of paired prompt and robot videos of d-dimension to condition the policy with the task relevant
from each of the three datasets in Figure 2. As can be seen, attributes from the video.
there is a considerable difference in the backgrounds and (2) Robot State Encoder encodes the current state of the
distractor objects in the Hindsight Human-Robot and Co- robot given the current frame and last k frames as input. Note
located Human-Robot datasets. A different complexity arises that this module also encodes information about the objects
when comparing the first approach (Robot-Robot) where the and environment of the robot. The architecture is similar
actor is a robot with same morphology to the other two cases to the prompt encoder, that is, a per-frame Image encoder
where the human is the actor in the prompt videos. ϕ s followed by a Perceiver Resampler ψ s. Similar to the
After combining all the datasets, we have ∼100k robot prompt encoder’s outputs, the output of the state encoder is
videos and ∼10k human videos covering the tasks introduced ψ s(ϕ s(S t)) = z state that encodes the latent environment and
in RT-1 [7] and RT-2 [8]. We include videos from all three robotstateinformationfromthehistoryofrecentobservations.
data sources as they represent varying levels of difficulty and We use the same image encoder weights for both (1) and
expense to collect. Pairing existing robot datasets requires (2), that is, ϕ p=ϕ s=ϕ. The role of the image encoder ϕ is to
less additional effort but lacks diversity in how the task is capturespatialvisualinformationineachframe.ThePerceiver
done. The second source of data is created by asking humans Resampler is used to enable temporal learning across frames
to mimic existing robot trajectories. While this adds some as well as reduce the number of video tokens that must be
diversity in prompt videos, it does not cover any new tasks passed into the action decoder.
on the robot side. Finally, the presumed gold-standard is to (3)State-PromptEncoderThestate-promptencodertakesthe
collect data where both humans and robots are co-located in promptvideoencodingz promptandrobotstateencodingz state
thesameenvironmentandperformdiversetasks.Thistakesthe and outputs a task encoding relevant for action prediction
most amount of time as labor is required both of the humans z state|prompt. The module is trained to output robot actions
and robot trajectories collected through tele-operation. by cross-attending between the state encoding as queries and
the prompt video encoding as keys and values. Intuitively, the
C. Model Architecture
state-prompt encoder enables fusion of the state and prompt
Our policy takes as input the prompt video and the current information. For example, if the prompt video demonstrates
robot state and outputs robot actions. It consists of four picking up of an apple in the basket and the current state
modules:(1)promptvideoencoder(2)robotstateencoder,(3) contains apple, banana and orange, then the cross attentionAction
Prompt
Per-Frame Prompt Tokens Mode Arm Base
Image
Resampler
Encoder
Prompt Video
Prompt Encoder State-Prompt Action
Encoder Decoder
Per-Frame
State
Image
Resampler
Encoder
State Tokens Action position
Robot Visual Observations embedding
State Encoder
Fig. 3: Architecture.Ourmodeltakesasinputframesofthepromptvideoandtherobot’scurrentobservations,encodesthoseintoprompt
video and robot state token embeddings, which are then processed through into state-prompt encoder and decoded into a robot action for
the current timestep. More details in §II-C.
between the state and prompt encoding enables learning for video. For example, when using ViT-B/16 the total number
which object to attend to in the state, which is crucial for the of video tokens for a 16 frame reference video and a 8
next step of action decoding. We refer to the output of the frame robot state video at 224 × 224 resolution would be
state-prompt encoder as prompt-aware state tokens. 8×196+16×196 = 4704. A full self-attention operation
(4) Robot Action Decoder The goal of the action decoder is on this would lead to an attention matrix with 47042 ∼22M
to predict the action vector a for the current state S such entries. However, by using two Perceiver Resamplers with 64
t t
that it completes the task shown in the prompt video V . The latentswewereabletotrainwithattentionmatricesofthesize
p
actiondecoderisatransformerdecoderarchitecturethattakes 8×196×64+16×196×64∼.3M.Thus,crossattentionlayers
inthefixedactionpositiontokens[49]asinputqueriesandthe in Vid2Robot play an important role in reducing attention
prompt-aware state tokens z for keys and values. computation and enabling training with paired videos.
state|prompt
The size of the action position embedding is N×d where N
D. Preprocessing
is the number of action dimensions and d is the transformer
embedding dimension. More details on the action vector in Tohandlethevaryinglengthsofvideosforefficienttraining,
§II-D. we randomly sample N =16 frames always including first
Theactionpositionembeddingscross-attendtotheprompt- and last frames and sort them in increasing order of time.
awarestatetokenstopredictthetargetbinnedactionvaluesas During training, we sample a robot state S t by sampling a
output.Eachoutputtokenoftheactiondecodercorrespondsto random timestep first. We then select the preceding k − 1
an action dimension for the mode, arm and base. Specifically, frames to create a robot state video comprising of a total
each token embedding is projected to 256 dimensions and of k=8 frames before. In case there are less than k − 1
a softmax layer is applied on the top to obtain the bin frames before the current time-step, we repeat the first frame
corresponding to the target action vector. Unlike prior work tocreateafixedsizerobotstatevideo.Thepixelvaluesineach
[7, 8] that use autoregressive action decoding that requires framearenormalizedbetween0to1.Eachframeisresizedto
multiple forward passes during inference, we use action posi- (224,224). Photometric distortions like cropping, brightness,
tion embeddings for one forward pass prediction like in ACT contrast, hue and saturation are applied during training.
[49]. Instead of predicting one action for the next timestep, The action vector consists of values indicating the mode,
we follow the approach outlined in [21, 49] and train the gripper pose and closedness as well as base displacement
policy with a prediction horizon of four steps. We always use and rotation. Each of the values have different ranges, which
the action bin that has the highest probability, i.e. argmax we first use to scale the values in between 0 and 1. We
over predicted probabilities, to choose the action value for then discretize the values into 256 bins each. In total, we
execution. construct 11-dim action vector as target, each of which has
Cross-Attention Layers. In the Vid2Robot architecture, we value between [0, 255]. In this study, we train and evaluate in
use Cross-Attention Transformer layers extensively. They are the scenarios where base remains stationary.
used in the following modules: Prompt Resampler, State
E. Training
Resampler, State-Prompt Encoder and Action Decoder. We
foundCross-Attentionlayersarehelpfulinmanagingthehigh Action Prediction Loss We train Vid2Robot end-to-end with
number of tokens and the resulting large attention matrices behavior cloning. The idea is to learn video representations
when processing both prompt videos and robot state videos. from raw pixels to recognize task verb and objects, as well as
This is because the standard self-attention layers would re- learn motor control to accomplish it. We use a classification
quire orders of magnitude more memory to process the same loss on actions that have been tokenized into N=256 bins.1. Action Prediction Loss 3. Prompt Video - Robot Video Contrastive Loss
Prompt SigLIP Loss
Encoder Prompt V-V
Encoder Pool
Prompt Video DP eo cl oic dy e r Action Prompt Video Batch r 0 rp 0p0 0 r 0p p1 1 r 0pp 22 r 0p p3 3
EnS cta ot de e r Cross L- oe sn stropy EP nr co om dp et
r
PV o- oV
l
r 1 r 1p 0 r 1p 1 r 1p 2 r 1p 3
r 2 r 2p 0 r 2p 1 r 2p 2 r 2p 3
Current Robot Observation
GT Action Robot Video Batch r r 3p 0 r 3p 1 r 3p 2 r 3p 3
3
2. Video Alignment Loss 4. Video - Text Contrastive Loss
Temporal Cycle
Consistency SigLIP Loss
Pe Ir m-F ar ga em e mAl eig nn t Loss “Pick co“Pk“euP“ P tcu u hat t anh h m afa rmmommmem re be irn ori ntttin o ot oot mo lobol obdlb xor”o axwx”” er Fr To ez xe t n
Encoder Pool and place on counter” Encoder t 0 t 1 t 2 t 3
Text Instruction Batch vt vt vt vt
Prompt Video v 0 00 01 02 03
Per-Frame Align Prompt V-T v 1 v 1t 0 v 1t 1 v 1t 2 v 1t 3
EI nm ca og de e r m Poe on lt Task progress Encoder Pool v 2 v 2t 0 v 2t 1 v 2t 2 v 2t 3
aware embedding
Robot Video space Robot Video Batch v 3 v 3t 0 v 3t 1 v 3t 2 v 3t 3
Fig. 4: Training Setup. We show all the losses Vid2Robot is trained with and how each loss is connected to its different modules. Along
with (1) the main action prediction loss, we apply three auxiliary losses: (2) temporal video alignment loss, (3) a contrastive loss between
thepromptandrobotvideoperformingthesametask,and(4)acontrastivelossbetweenaprompt/robotvideowiththelanguageembedding.
More details in §II-E.
Given the robot trajectory for performing a task with current E ={Φ(v1),Φ(v2)...,Φ(vLi)},whereL isthelengthofthe
i i i i i
visual observations x , we have the corresponding expert ith video.
t
actiona .TheactionpredictionlossisCrossEntropybetween We apply TCC loss on encoding E , and E for prompt
t p r
the predicted action and the expert action as: and robot video respectively. The intuitive idea of TCC loss
is that we want to ensure the representation of every frame
(cid:88)
L CE(a t,aˆ t)= a tlogaˆ t (1) of E p should have a correspondence in E r and vice versa.
τ This involves two steps: First, we compute soft neighbor of
tth frame of E
p
(E pt in short) in E
r
and call it E(cid:103) pt r.
This trains all the model parameters, as shown in Fig 3.
Auxiliary Losses. Although our dataset size is substantial, it (cid:88)Lr e−∥E it−E jk∥2
is insufficient for training large Transformer based models. In
E(cid:103) pt
r
= α kE rk, where α
k
=
(cid:80)Lje−∥E it−E jk∥2
(2)
ordertopreventover-fittingbyjustpredictingactionscorrectly k k
Second, we find the corresponding frame for this newly
onthetrainingset,weaddthreeauxiliarylossesthatencourage
computed soft-neighbour in E . This is called cycle-back
learning features that are helpful in understanding semantics p
in[17]anditinvolvessimilarsoft-neighbourcomputationasin
in prompt videos.
Video Alignment Loss: We want to encourage temporal
Equation2toobtainsayE(cid:100) pt r,whichideallyshouldbesameas
alignmentbetweenpromptvideosandrobotvideosperforming t,thatis,(E(cid:100) pt r−t)2 shouldbeminimized.TCClossminimizes
thatshowthesametask.Byaligningpromptvideosandrobot such mean squared error between all frames for prompt and
videos, we want the image encoder to learn to be invariant robot video encodings and vice-versa, that is,
to different embodiments, lighting, backgrounds, view-angles (cid:88)
and distractor objects while still encoding features relevant to
L TCC(E p,E r)= (E(cid:100) pt r−t)2
predicting task progress. Our choice of loss is the temporal- t∈Vp (3)
cycle consistency loss introduced in [17]. This loss has been
L =
L TCC(E p,E r)+L TCC(E r,E p)
shown to encode task progress when trained on videos of TCC 2
different agents performing the same task [47]. This loss Prompt-Robot Video Contrastive Loss (VVCL): We apply
is applied on per-frame image embeddings of the prompt contrastive loss between prompt tokens produced by robot or
V and robot V videos during training. To apply the loss, prompt video performing the same task. This loss encourages
p r
we average pool the per-frame embeddings output in spatial the prompt encodings to learn task semantics from video
dimensions from image encoder ϕ and apply a projector head tokens only in a self-supervised manner. A thing to note here
of 2-layer MLP [10]. We call this as alignment pooling layer is that the initial pairing of prompt and robot video has been
Φ on the per-frame image embeddings, as shown in Fig 4. doneusingnaturallanguage.However,byusingaconstrastive
For each video V , this results in a sequence of embeddings loss only on video embeddings with a self-supervised loss,
iwe hope to encode features not covered by the short natural III. EXPERIMENTS
languageembeddingitself.Examplesofthesefeaturesinclude
similar motions like reaching for objects, and rotating the We present results with real robot evaluations for our
robotarm.WeuseaAttentionPoolinglayertomergefeatures multi-task video-conditioned policy. One of the key questions
from the N prompt tokens to produce a single embedding for that we tackle in this work is how well robots can imitate
each video. We apply the SigLIP [48] loss between video- humansperformingmanipulationtasks.Becauseofdifferences
video pairs to encourage videos showing same task, involving in embodiments, humans perform manipulation tasks at a
similar motions and interacting objects, to be close to each different speed and style. We study the effect of using robot
other while being away from other videos in the batch. A as well as human videos as prompts.
batch contains the same number of robot videos and prompt Metrics.Werefertoarolloutasasequenceofactionsinferred
videos, say B. We use the prompt encoder ψ p(ϕ(·)) to obtain fromthepolicyandexecutedontherobotfromaninitialstate
a batch of full robot video embeddings Z robot and prompt observation and prompt video, until the policy terminates or
video embeddings Z prompt, each of size B×d. We multiply a maximum number of steps are taken, whichever is lower.
them, Z robot ·Z pT rompt to obtain a B ×B matrix. Adding a We define success for a rollout when the policy executes
learnable temperature τ and bias b, we have our logit matrix the task instruction shown in the prompt video. A successful
as Yˆ =(Z robot·Z pT rompt)∗τ +b. We consider the videos of rollout involves correct actions to be taken successively in the
robot and prompt performing the same task as positives and environment,withoutanyassistanceforresetsorrecovery.For
assign them a label of 1 along the diagonal and -1 for off- each task instruction, we record many rollouts per policy. We
diagonal pairs, that is, the label matrix Y =2I B −1. SigLIP taketheaverageofsuccessrecordedacrossalltherolloutsand
loss is the negative loglikelihood, specifically, σ′(Z 1,Z 2) = call it the Success Rate for that task. Aggregated success rate
−(cid:80) logσ(Y ·(Z 1 ·Z 2T)∗t+b), where Y = 2I B −1. The across tasks is referred as Overall Success Rate.
video-video contrastive loss is then defined as: A mistake made early on in a rollout can result in poor
success rate, even if the model’s offline overall prediction
L =σ′(Z ,Z ) (4)
VVCL prompt robot accuracyishigh.Forexample,ifapolicymakesanerrorwhile
grasping a water bottle early on in the task and it slips to an
Video-text Contrastive Loss (VTCL): We add a contrastive
unreachable location, the rollout will be marked as a failure,
loss between prompt tokens Z and those produced by
prompt even if the policy had good action predictions for the later
robot video Z and the embedding of the text instructions
robot steps. To record partial progress for a rollout, we annotate
of the task Z . This encourages a part of the embedding
text whether the robot reached the correct location, grasped the
space to be aware of object names and verbs present in the
correct object, released the object at the correct location, and
prompt and the robot videos. A version of this loss has been
terminated the task correctly. More details on partial success
applied before by BC-Z [21] as auxiliary language regression
analysis in §III-A.
loss. We use an Attention Pooling layer [46] with one latent
Evaluation Setup. We ask human raters to evaluate success
querytomergefeaturesfromtheN prompttokenstoproduce
for a policy’s rollout on a robot. We evaluate the policies
asingleembeddingforeachvideo.Withinabatch,weretrieve
by varying the robots, lighting conditions, chest of drawers,
B pairs of video and text embeddings. Similar to Equation 4,
andobjects.Weensurethepoliciesbeingevaluatedareshown
we apply SigLIP [48] loss to get
similar initial object configurations during rollouts. The initial
σ′(Z ,Z )+σ′(Z ,Z ) state is randomized after all policies have been evaluated for
L VTCL = prompt text 2 robot text (5) agiveninitialstate.Forallrollouts,wesamplepromptvideos
that are not seen by the models during training. This ensures
This encourages every video to have similar embeddings that the policies are evaluated for whether they can recognize
to their textual description embeddings, while being different the task from new prompt videos or not.
fromthetextembeddingscorrespondingtoothervideosinthe Baselines. We compare our model with BC-Z [21], a video
batch. conditioned policy using a ResNet-18 encoder. BC-Z [22] in-
Overall, we apply the mean of all four losses for training volves demonstration-observation pairs processed via a FiLM
that is L= 1 4(L CE +L TCC +L VVCL+L VTCL). [36] conditioned ResNet encoder and fed into a ResNet based
policynetworktopredictrobotactions.Forafaircomparison,
F. Implementation we train the BC-Z model with the same training data used to
train the Vid2Robot model. We run rollouts of BC-Z policy
We trained the model (implemented in Jax) for 200K
for a fixed maximum number of steps, as BC-Z doesn’t have
iterations. We use AdamW optimizer with an initial learning
a terminate action.
rateof8e-5usingacosinelearningrateschedulewithwarmup
Key Questions and Results We address the following ques-
steps2,000andfinallearningrateof1e-6.ForboththePrompt
tions in this work:
and State Resamplers, we use 2 Perceiver Resampler layers
with64latents.Bothstate-promptencoderandactiondecoder 1) How well do video-conditioned policies perform when
are 4 layer deep cross-attention transformers. theyareshownataskinanunseenvideo?(Fig5,§III-A)Fig. 5: Policy Rollouts. Each row shows a prompt video of a human doing a task on the left, and on the right we show the corresponding
successfulrobotrolloutsusingVid2Robot.Notehowvisuallydifferentthepromptsare,whilethepolicyrolloutsarerecordedwithdifferent
lighting, background, as well as number and placement of the distractor objects.
TABLE I: Task Success Rate for Robot and Human prompts.
Prompter Model pick pick-placeon placeinto open close movenear knockover placeupright Overall
BC-Z 75.0% 50.0% 61.5% 16.7% 66.7% 44.0% 58.3% 50.0% 52.6%
Robot
Vid2Robot 75.0% 58.8% 50.0% 91.7% 100.0% 33.3% 41.7% 16.7% 54.9%
BC-Z 50.0% 12.5% 12.5% 0.0% 50.0% 43.8% 12.5% 50.0% 30.6%
Human
Vid2Robot 100.0% 50.0% 50.0% 62.5% 87.5% 43.8% 25.0% 12.5% 52.8%
2) Whatisthegapinsuccessrateduetopromptembodiment cokecan’,‘pickgreenricechipbag’,‘placecokecanupright’,
difference (robot v/s human)? (§ III-A) ‘pick coke can from bottom drawer and place on counter’,
3) Can we leverage the learned motion representations for ‘openmiddledrawer’,‘closemiddledrawer’,and‘placeapple
out-of-distribution object interactions? (§ III-B) into top drawer’.
Weaskfourevaluatorstocarryouttworolloutspertaskfor
A. Task-based success
a prompt video dataset and policy setting (a row in Table I),
We compare the our Vid2Robot model and baseline BC- thatimplies,wehaveeighttrialspertasktoevaluateapolicy’s
Z with robot and human prompt videos in Table I. Both task success rate. We report overall success rate per row over
Vid2Robot and BC-Z were trained on a same data mixture nine tasks with eight trials per task, that is, 9×8=72 trials.
containing robot-robot and human-robot paired data. Prompt In total, our evaluations in Table I required 72×4=288 real
videos cover a subset of the training tasks but the videos robot rollouts.
themselves are new for the models. In this evaluation, we 1) What is the gap in success rate due to embodiment
investigate what each model’s ability is to infer the task spec- difference in prompt videos?: We compare our model with
ification from prompt video as well as the current observed BC-Z when prompted with robot and human videos. BC-Z
state of the robot. serves as a strong baseline for our comparisons. The overall
In order to test the capabilities of the model in different success rate of our model Vid2Robot outperforms BC-Z for
settings on real robot, we evaluate it across eight categories Human prompt videos by 20%, and is comparable for Robot
of manipulation tasks as shown in Table I. Specifically, we prompt videos. Note that there is an order of magnitude more
evaluateforninetasks:‘knockwaterbottleover’,‘moverxbar training samples for robot trajectories than human videos
chocolate near coke can’, ‘move green jalapeno chip bag near in our training mixture. Hence, there isn’t a significant gapPrompt video showing task “Place coke can upright”
Policy Rollout Videos with above prompt video but different objects
Robot places green can upright, not the chips bag or banana
Fig.6:PartialSuccessRateforBC-ZandVid2Robot.Ourpolicy
Vid2RobotoutperformsBC-Zintermsofreachingthecorrectobject,
grasping it, releasing it at the correct location and then terminating Robot places chips bag upright
theepisodecorrectly.NotethatBC-Zdoesnothaveterminatecontrol.
TABLE II: Cross-object motion transfer success.
Robot places stapler upright
pick- place place knock
Model pick placeon into upright over Overall
BC-Z 45.8% 0.0% 29.2% 12.5% 0.0% 17.5%
Vid2Robot 45.8% 25.0% 54.2% 16.7% 29.2% 34.2%
Robot places unseen soft toy upright
in performance for robot prompt videos. For human prompt Fig.7:Qualitativeresultsforcross-objectmotiontransfer.Givena
videos, our model outperforms BC-Z in most tasks, showing promptvideoofplacingcokecanupright,werolloutthepolicywith
that Vid2Robot captures the task semantics from prompt a green can, chips bag, stapler and a soft toy in front of the robot.
We observe that our model can infer the motion of place upright in
videos better than the baseline. Our model outperforms in
the prompt video and apply it on other objects. There is an implicit
tasks like picking from drawer and placing on the counter,
notionofpragmaticsinthepolicyasshownbytheselectionofgreen
andopening/closingdrawertasksbyalargemargin.Themost can over other objects.
challenging task is placing upright and knocking over. We
analyze the failure reasons in §V Fig 9.
2) How well do video-conditioned policies perform when only those scenarios where the interaction object shown in
they are shown a task in an unseen video?: In addition to prompt is present in the current robot observations. But what
marking a rollout as a success, we recorded partial success if we provided a prompt video of one object and tested on
annotations per rollout. In Fig 6, we observe that our model other objects. Does it do the same motion as shown in the
reaches to the correct object 78% of the time, about 8% prompt video? Interestingly, we found our model to perform
more as compared to baseline. The policies sometimes fail learned manipulation actions on objects that it has not seen
toreachthecorrectobjectandgotowardsadistractorinstead. in train set. We call this emergent behavior as cross-object
Next, grasping errors happen, particularly with small and motion transfer.
deformable objects and in collision prone areas like drawer We compare Vid2Robot with BC-Z for cross object motion
handle or counter’s edge. Here our model (65%) outperforms transfer ability with five prompt videos, namely, ‘knock water
BC-Z (45%) by a large margin of 20%. A successful grasp bottle over’, ‘pick green rice chip bag’, ‘place coke can
often the most difficult part in a rollout, and the most crucial upright’, ‘pick coke can from bottom drawer and place on
for success. After grasping, most tasks require releasing at counter’, and ‘place apple into top drawer’. Each prompt
a correct location. There is a slight drop in success rate in video is evaluated with unrelated objects in robot’s initial
bothmodelsduetoincorrectreleaseduringtherollouts.While observation. The objects used for evaluation are ‘orange’,
BC-Z runs for a fixed number of steps, our policy Vid2Robot ‘green can’, ‘chips bag’, ‘banana’, ‘pink piggy soft toy’,
predictswhentoterminate.Weobservethattherateofrelease ‘wrist watch’. We selected objects to have diverse shape, size,
and terminate is almost identical, about 57% for our model, and deformability to evaluate situations that require different
thatimplies,thatafterreleasingatcorrectlocation,Vid2Robot grasps for success.
mostly terminates successfully. Theevaluationsetupissimilarto§III-A.Heretheevaluator
sets up one of the object for a task and records rollouts for
B. Cross-object motion transfer
each model. We compare 2 models on 5 tasks with 6 objects,
Our policy and baseline were trained with paired videos as so every evaluator runs 2×5×6=60 rollouts. We repeat the
discussedin§II-B.Thisimpliesthatthetrainingdataincluded evaluation with four raters, thus reporting results in Table IIon a total of 4×60=240 rollouts.
1) Can we provide a prompt video of one object and test
it on other objects? Does the policy do the same motion as
shown in the prompt video?: In Fig 7, we show the above
experimental setup qualitatively. We use a prompt video to
‘place coke can upright’. We observe that the policy is able
transfer the action of ‘placing upright’ to several objects, like
a green can, a chips bag, a stapler, and a soft toy. Note that
the policy adheres to the prompt video and chooses green can
Fig.8:AblationforauxilliarylossesusedinVid2Robot.Wecom-
over chips bag or banana for placing upright.
pare our proposed approach that has all auxiliary losses (green, left)
Quantitatively, we observe that BC-Z is often unable to
with a variant without language contrastive loss that was originally
successfully complete the tasks when testing cross=object proposed in BC-Z (orange, middle) and a version with no auxilliary
motion transfer, as shown in each task in Table II. In contrast, losses (blue, right). More details in (§III-C)
our model (34%) performs better than BC-Z (17%) in this
setting and performs the motion indicated in the prompt
IV. RELATEDWORK
video. Our model is comparable to BC-Z with 45% success
rate on picking out-of-distribution objects. More importantly, Task Specifications for Robots The development of general-
tasks involving placing into drawers demonstrates significant purpose robots hinges on effectively grounding task specifi-
improvement (29% → 54%). For certain tasks like picking cations. Videos are a dense source of information that not
fromdrawersandplacingoncountersandknockingover,BC- only provide what to do but also how to do it in physical
Z is unable to perform at all whereas Vid2Robot is able to world. Recent works have used videos for task specification
complete the task 25%−29% of the time. [4, 23, 40]. Another line of work uses videos to learn world
modelstopredictfuturevisualobservations[29,26,9,31,15].
C. Ablations While language [45, 7, 33, 34], final goal images [24, 6],
andotherslikehand-drawninputs [43]havebeenproposedas
In §II-E, we presented action prediction loss and three
means for task specification, learning from prompt videos is
auxiliary losses. Here we analyze the role of these additional
complementary to these approaches and inevitable for rapid
loss functions to the overall success rate. We investigate the
adaptation of trained polices to perform new manipulation
impact of (1) not using any auxiliary loss, and (2) adding
skills at deployment.
auxiliary language loss.
Learning from Human Demonstrations As videos of hu-
We consider the tasks similar to that described in §III-A, mans performing various tasks proliferate the internet, several
that is, 9 tasks for evaluating each policy. worksaimtoaddresshowtobestleveragethisinformationfor
We have 3 model variants, namely, the original Vid2Robot, robot learning. The difference in robot vs human embodiment
the one without video-text contrastive loss (CL) and the one poses a significant challenge, for which existing approaches
with only action prediction loss. We ask 3 human evaluators range from translating image of a human into the robot [42]
to run the each model variant with 2 rollouts each. In total, toinpaintingforagent-agnosticrepresentations[3].Manyprior
we report results with 3×3×9×2=162 rollouts in Fig 8. The works propose to leverage off-the-shelf models for hand pose
error bars indicate the standard deviation for success reported estimation and contact tracking [4, 12, 37], object-centric
on rollouts with each model variant. representations [38, 20], as well as reward functions for rein-
1) What is the impact of not using any auxiliary loss?: forcement learning [3, 27, 42]. Other methods [32, 44, 4] cast
We observe that the performance of our model (61%) is this problem into visual representation learning to accelerate
significantly improved by enforcing representation constraints learning of downstream motor control tasks. While these
through auxiliary losses, in comparison to using only action modularlearningsolutionsworkwellinlimiteddatasets,these
prediction loss (45%). It highlights the importance of the arepronetocompoundingerrorofeachofitscomponent,and
proposed auxiliary losses in §II-E. thus, not efficiently scalable. End-to-end training approaches
2) What is the impact of the auxiliary language loss?: for goal-conditioned imitation learning [11, 41, 18, 13] and
BC-Z proposed to use language representations to improve reinforcement learning [39, 35] are promising alternatives to
videorepresentationsforconditioningthepolicy.Wecompare these techniques, but these results have been largely limited
our policy with another variant trained with all losses but the in simulation and hindered by sim-to-real gap. In contrast, we
Video-Text CL. We observe only marginal improvement of 1- choosetotacklethisasanend-to-endlargemulti-tasklearning
2%insuccessratewhenusingthelanguageloss.Thisimplies from human videos with real robot evaluations.
that video alignment and video contrastive loss contribute Imitation via Paired Demonstrations Our setup of paired
significantly towards performance improvement. Our results promptvideosandrobottrajectoryismostsimilartoOne-Shot
hope to serve as a promising evidence that effective video VisualImitationliterature.Manypriorworksassumeaccessto
representations can be learned without auxiliary losses that pairs,wherethefirstisusedasthedemonstrationofthetaskto
use pre-trained language embeddings. be performed, and the second as the observation of the agent.Human Prompt Videos Robot Policy Rollout Videos
Place coke can upright Self occlusion preventing full observation of state
Knock water bottle over Bottle fell out of hand when grasping
Move rxbar chocolate near coke can Moved rxbar near another can (distractor) instead of the coke can
Fig.9:Failureanalysiswithpolicyrollouts.(Top)PolicypredictsgripperposeanddependsontheIKsolvertomovethearm.Sometimes,
the IK solution can block the robot’s camera view. (Middle) Grasping failures happen, especially with transparent and deformable objects.
(Bottom) Distractor objects as well as difference in lighting and background may cause recognition errors, where policy might perform the
correct motion but with incorrect object(s).
Some of the early works [16] proposed training a demon- is out of camera view. By enhancing the state information
stration network via temporal convolution and neighorhood with multimodal sensor fusion, we may improve the grasp
attention to condition a manipulation policy network. In more success rate. Third, we consider carefully collected short task
recentapproacheslike[11,28,20],paireddemonstrationsand instruction demonstrations from three different sources as
observations are used to train a transformer policy, often with shown in §II-B, all of which are 5 to 20 seconds videos. To
additional constraints like inverse dynamics prediction[11] test our models on long horizon demonstrations or ‘in-the-
or contrastive representation learning [28]. However, these wild’ videos online, we need effective pairing strategies for
approaches are largely evaluated in specific set of simulated videos and a few corresponding robot trajectories to train the
tasks, and not compared on real robots. Most similar to our policy.
work is BC-Z [22] which reports evaluations with real robot
tasks. While our setup is similar to some of this prior art,
VI. CONCLUSION
our model Vid2Robot couples large image encoders, cross- We demonstrate novel methods for both data collection and
attention layers, and contrastive auxiliary losses to learn a modeling for video conditioned skill learning. These skills
manipulation policy that imitates a human showing a task. generalize to novel object configurations and more abstracted
verbmeaningswhennoimmediatelyobviousobjectisvisible.
V. LIMITATIONSANDFUTUREDIRECTIONS
The skills and generality provided by our model complement
In §III, we show that our approach has improved over other approaches to widen the set of skills that robots have
previous work but there is a gap in performance for video- access to, and to include skills not otherwise easily acquired.
conditioned policies. Language conditioned policies like [8] Future work can leverage these learned primitives to execute
shows a higher success for known set of tasks with several novel task plans. We hope our cross-object motion transfer
hundreds of teleoperation trajectories for training. We, on the experiments will encourage further research in transferring
other hand, accomplish the first milestone of evaluating the motion to new objects and settings for bootstrapping data
video-conditioned policies in the similar setup. We discuss collection, and enabling human-robot interaction with rapid
three limitations of our work and provide insights for future adaptation to new skills.
directions here.
First, we qualitatively investigate some reasons for failure
ACKNOWLEDGMENTS
of a policy rollout. In Fig 9, we illustrate and explain 3 We would like to thank Yansong Pang, Grecia Salazar,
examples showing how self occlusion, grasping errors and Utsav Malla, Deeksha Manjunath, Jornell Quiambao, Sarah
presence of distractors can lead to failure during any rollout. Nguyen, Sangeetha Ramesh, Tran Pham, Samuel Wan, Tomas
Second, we observe a significant drop in the grasping success Jackson,JodilynPeralta,CelesteBarajas,ElioPrado,Rochelle
in Fig 6. While we use robot camera observation to estimate Dela Cruz, Alex Luong and Krista Reymann for supporting
the state and implicitly learn depth estimation, it is often data collection via teleoperation. Special thanks to Jornell
incomplete when there is occlusion or when the robot gripper Quiambao, Grecia Salazar, Utsav Malla, Deeksha Manjunath,TABLE III: Author Contributions
Name DataCollection ModelTraining Evaluations Infrastructure Leadership PaperWriting
AyzaanWahid ✓
ChristineW.Y.Chan ✓
DannyDriess ✓ ✓
DebidattaDwibedi ✓ ✓ ✓ ✓ ✓ ✓
IgorGilitschenski ✓ ✓
MariaAttarian ✓ ✓ ✓ ✓
NikhilJJoshi ✓ ✓ ✓ ✓ ✓
PannagRSanketi ✓ ✓ ✓
PierreSermanet ✓
QuanVuong ✓ ✓
StefanWelker ✓ ✓
VidhiJain ✓ ✓ ✓ ✓ ✓ ✓
YonatanBisk ✓ ✓
Sarah Nguyen, Sangeetha Ramesh, and Jaspiar Singh for AAAI Conference on Artificial Intelligence, 2020. URL
evaluations on robot; Michael Ahn, Anthony Brohan and https://yonatanbisk.com/piqa.
KeerthanaGopalakrishnanforpolicyevaluationinfrastructure; [6] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao,
Suneel Belkhale, Dorsa Sadigh, Chelsea Finn, and Sergey ColineDevin,AlexX.Lee,MariaBauza´,TodorDavchev,
Levine for helpful discussions; Jonathan Tompson, and Vin- Yuxiang Zhou, Agrim Gupta, Akhil S. Raju, Antoine
cent Vanhouke for thorough feedback on the writing. This Laurens, Claudio Fantacci, Valentin Dalibard, Martina
work was also supported by Google Robotics Funding at Zambelli, Murilo Fernandes Martins, Rugile Pevcevi-
Carnegie Mellon University. ciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor,
ThomasLampe,EmilioParisotto,KonradZolna,ScottE.
REFERENCES
Reed, Sergio Gomez Colmenarejo, Jonathan Scholz,
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli,
toineMiech,IainBarr,YanaHasson,KarelLenc,Arthur
Oleg O. Sushkov, Tom Rothorl, Jose´ Enrique Chen,
Mensch, Katherine Millican, Malcolm Reynolds, Ro-
Yusuf Aytar, David Barker, Joy Ortiz, Martin A. Ried-
man Ring, Eliza Rutherford, Serkan Cabi, Tengda Han,
miller,JostTobiasSpringenberg,RaiaHadsell,Francesco
Zhitao Gong, Sina Samangooei, Marianne Monteiro,
Nori,andNicolasManfredOttoHeess. Robocat:Aself-
JacobMenick,SebastianBorgeaud,AndrewBrock,Aida
improving generalist agent for robotic manipulation. In
Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski,
Transactions on Machine Learning Research, September
Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and
2023.
Karen Simonyan. Flamingo: a visual language model
[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yev-
for few-shot learning. In Alice H. Oh, Alekh Agarwal,
gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana
Danielle Belgrave, and Kyunghyun Cho, editors, Ad-
Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine
vances in Neural Information Processing Systems, 2022.
Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas
URL https://openreview.net/forum?id=EbMuimAbPbs.
Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian,
[2] Montserrat Gonzalez Arenas, Ted Xiao, Sumeet Singh,
DmitryKalashnikov,YuhengKuang,IsabelLeal,Kuang-
Vidhi Jain, Allen Z. Ren, Quan Vuong, Jake Varley,
Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deek-
Alexander Herzog, Isabel Leal, Sean Kirmani, Dorsa
sha Manjunath, Igor Mordatch, Ofir Nachum, Carolina
Sadigh, Vikas Sindhwani, Kanishka Rao, Jacky Liang,
Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jor-
and Andy Zeng. How to prompt your robot: A prompt-
nell Quiambao, Kanishka Rao, Michael Ryoo, Grecia
bookformanipulationskillswithcodeaspolicies.In2nd
Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh,
Workshop on Language and Robot Learning: Language
Sumedh Sontakke, Austin Stone, Clayton Tan, Huong
as Grounding, 2023. URL https://openreview.net/forum?
Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong,
id=T8AiZj1QdN.
Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu,
[3] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak.
and Brianna Zitkovich. RT-1: Robotics transformer
Human-to-robot imitation in the wild. In Robotics
for Real-World control at scale. In arXiv preprint
Science and Systems (RSS), 2022.
arXiv:2212.06817, December 2022.
[4] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain,
[8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
andDeepakPathak. Affordancesfromhumanvideosasa
Chebotar,XiChen,KrzysztofChoromanski,TianliDing,
versatile representation for robotics. In Computer Vision
Danny Driess, Avinava Dubey, Chelsea Finn, Pete Flo-
and Pattern Recognition, April 2023.
rence,ChuyuanFu,MontseGonzalezArenas,Keerthana
[5] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
Gopalakrishnan, Kehang Han, Karol Hausman, Alex
Gao, and Yejin Choi. PIQA: Reasoning about Physical
Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil
Commonsense in Natural Language. In Thirty-FourthJoshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, learning. Advances in neural information processing
Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey systems, 30, 2017.
Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, [17] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson,
Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Pierre Sermanet, and Andrew Zisserman. Temporal
Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Cycle-Consistency learning. In Computer Vision and
JaspiarSingh,AnikaitSingh,RaduSoricut,HuongTran, Pattern Recognition, 2019.
Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Ste- [18] OliverGroth,Chia-ManHung,AndreaVedaldi,andIng-
fan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted mar Posner. Goal-Conditioned End-to-End visuomotor
Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna control for versatile skill primitives. In 2021 IEEE
Zitkovich. RT-2:Vision-Language-ActionModelsTrans- International Conference on Robotics and Automation
ferWebKnowledgetoRoboticControl.InarXivpreprint (ICRA), pages 1319–1325. IEEE, May 2021.
arXiv:2307.15818, 2023. [19] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste
[9] Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. Alayrac, Carl Doersch, Catalin Ionescu, David Ding,
Goal-conditioned reinforcement learning with imagined Skanda Koppula, Daniel Zoran, Andrew Brock, Evan
subgoals. In Marina Meila and Tong Zhang, editors, Shelhamer,OlivierJHenaff,MatthewBotvinick,Andrew
Proceedings of the 38th International Conference on Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver
Machine Learning, volume 139 of Proceedings of Ma- IO: A general architecture for structured inputs & out-
chineLearningResearch,pages1430–1440.PMLR,18– puts. In International Conference on Learning Repre-
24 Jul 2021. URL https://proceedings.mlr.press/v139/ sentations, 2022. URL https://openreview.net/forum?id=
chane-sane21a.html. fILj7WpI-g.
[10] Ting Chen, Simon Kornblith, Kevin Swersky, Moham- [20] Vidhi Jain, Yixin Lin, Eric Undersander, Yonatan Bisk,
mad Norouzi, and Geoffrey E Hinton. Big self- and Akshara Rai. Transformers are adaptable task plan-
supervised models are strong semi-supervised learners. ners. InKarenLiu,DanaKulic,andJeffIchnowski,edi-
Advances in neural information processing systems, 33: tors,ProceedingsofThe6thConferenceonRobotLearn-
22243–22255, 2020. ing, volume 205 of Proceedings of Machine Learning
[11] Sudeep Dasari and Abhinav Gupta. Transformers for Research, pages 1011–1037. PMLR, 14–18 Dec 2023.
one-shot visual imitation. In Conference on Robot URL https://proceedings.mlr.press/v205/jain23a.html.
Learning, pages 2071–2084. PMLR, 2021. [21] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler,
[12] EadomDessalene,ChinmayaDevaraj,MichaelMaynord, FrederikEbert,CoreyLynch,SergeyLevine,andChelsea
Cornelia Fermuller,and Yiannis Aloimonos. Forecasting Finn. BC-Z:Zero-ShotTaskGeneralizationwithRobotic
action through contact representations from first person ImitationLearning. InProceedingsofthe5thConference
video. IEEE Trans. Pattern Anal. Mach. Intell., 45(6): on Robot Learning, pages 991–1002, 2022.
6703–6714, June 2023. [22] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler,
[13] Yiming Ding, Carlos Florensa, Mariano Phielipp, and FrederikEbert,CoreyLynch,SergeyLevine,andChelsea
Pieter Abbeel. Goal-conditioned imitation learning. In Finn. Bc-z: Zero-shot task generalization with robotic
Proceedings of the 33rd International Conference on imitation learning. In Conference on Robot Learning,
Neural Information Processing Systems, pages 15324– pages 991–1002. PMLR, 2022.
15335. Curran Associates Inc., Red Hook, NY, USA, [23] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi
December 2019. Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima
[14] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov, Anandkumar, Yuke Zhu, and Linxi Fan. VIMA: General
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, robotmanipulationwithmultimodalprompts. InFortieth
Mostafa Dehghani, Matthias Minderer, Georg Heigold, International Conference on Machine Learning, 2023.
Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An [24] Jacob Krantz, Theophile Gervet, Karmesh Yadav, Austin
image is worth 16x16 words: Transformers for im- Wang, Chris Paxton, Roozbeh Mottaghi, Dhruv Batra,
age recognition at scale. In International Confer- JitendraMalik,StefanLee,andDevendraSinghChaplot.
ence on Learning Representations, 2021. URL https: Navigating to objects specified by images. April 2023.
//openreview.net/forum?id=YicbFdNTTy. [25] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol
[15] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Hausman, Brian Ichter, Pete Florence, and Andy Zeng.
Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Code as policies: Language model programs for embod-
Pieter Abbeel. Learning universal policies via text- ied control. In 2023 IEEE International Conference on
guided video generation. In Thirty-seventh Conference Robotics and Automation (ICRA), 2023.
on Neural Information Processing Systems, 2023. URL [26] Yuxuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey
https://openreview.net/forum?id=bo8q5MRcwy. Levine. Imitation from observation: Learning to imitate
[16] Yan Duan, Marcin Andrychowicz, Bradly Stadie, Ope- behaviors from raw video via context translation. In
nAIJonathanHo,JonasSchneider,IlyaSutskever,Pieter 2018 IEEE International Conference on Robotics and
Abbeel, and Wojciech Zaremba. One-shot imitation Automation(ICRA),pages1118–1125.IEEE,May2018.[27] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suender-
Osbert Bastani, Vikash Kumar, and Amy Zhang. VIP: hauf,NormanDiPalo,NurMuhammadMahiShafiullah,
Towards universal visual reward and representation via Oier Mees, Oliver Kroemer, Pannag R Sanketi, Paul
Value-ImplicitPre-Training. InInternationalConference Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan,
on Learning Representations, 2023. Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi,
[28] Zhao Mandi, Fangchen Liu, Kimin Lee, and Pieter Roberto Mart´ın-Mart´ın, Russell Mendonca, Rutav Shah,
Abbeel. Towards more generalizable one-shot visual Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean
imitation learning. In 2022 International Conference Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl,
on Robotics and Automation (ICRA), pages 2434–2444. Shivin Dass, Shubham Sonawani, Shuran Song, Sichun
IEEE, 2022. Xu, Siddhant Haldar, Simeon Adebola, Simon Guist,
[29] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. SoroushNasiriany,StefanSchaal,StefanWelker,Stephen
Structured world models from human videos. 2023. Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa,
[30] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe
Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Yu, Tianli Ding, Todor Davchev, Tony Z Zhao, Travis
HowTo100M: Learning a Text-Video Embedding by Armstrong, Trevor Darrell, Vidhi Jain, Vincent Van-
Watching Hundred Million Narrated Video Clips. In houcke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard,
ICCV, 2019. Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li,
[31] AshvinNair,VitchyrPong,MurtazaDalal,ShikharBahl, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu,
Steven Lin, and Sergey Levine. Visual reinforcement Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho,
learning with imagined goals. In NeurIPS, July 2018. YoungwoonLee,YuchenCui,Yueh-HuaWu,YujinTang,
[32] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo,
Finn, and Abhinav Gupta. R3m: A universal visual Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment:
representation for robot manipulation. In 6th Annual Robotic learning datasets and RT-X models. arXiv
Conference on Robot Learning, 2022. URL https:// 2310.08864, 2023.
openreview.net/forum?id=tGbpgz6yOrI. [34] Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil,
[33] Open X-Embodiment Collaboration, Abhishek Padalkar, Sam Powers, Yonatan Bisk, and Chris Paxton. Spatial-
Acorn Pooley, Ajay Mandlekar, Ajinkya Jain, Albert Language Attention Policies for Efficient Robot Learn-
Tung,AlexBewley,AlexHerzog,AlexIrpan,Alexander ing. In Conference on Robot Learning, 2023. URL
Khazatsky, Anant Rai, Anikait Singh, Animesh Garg, https://arxiv.org/abs/2304.11235.
Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben [35] Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter
Burgess-Limerick, Beomjoon Kim, Bernhard Scho¨lkopf, Abbeel,andSergeyLevine. SFV:reinforcementlearning
BrianIchter,CewuLu,CharlesXu,ChelseaFinn,Chen- of physical skills from videos. ACM Trans. Graph., 37
fengXu,ChengChi,ChenguangHuang,ChristineChan, (6):1–14, December 2018.
Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, [36] Ethan Perez, Florian Strub, Harm De Vries, Vincent
Deepak Pathak, Dhruv Shah, Dieter Bu¨chler, Dmitry Dumoulin, and Aaron Courville. Film: Visual reasoning
Kalashnikov, Dorsa Sadigh, Edward Johns, Federico with a general conditioning layer. In Proceedings of the
Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S AAAI conference on artificial intelligence, 2018.
Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, [37] Vladim´ır Petr´ık, Makarand Tapaswi, Ivan Laptev, and
Gregory Kahn, Hao Su, Hao-Shu Fang, Haochen Shi, Josef Sivic. Learning object manipulation skills via
Heni Ben Amor, Henrik I Christensen, Hiroki Fu- approximate state estimation from real videos. In Jens
ruta, Homer Walke, Hongjie Fang, Igor Mordatch, Il- Kober, Fabio Ramos, and Claire Tomlin, editors, Pro-
ija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou- ceedingsofthe2020ConferenceonRobotLearning,vol-
Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jas- ume 155 of Proceedings of Machine Learning Research,
mine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, pages 296–312. PMLR, 2021.
Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon [38] So¨ren Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch,
Oh, Jitendra Malik, Jonathan Booher, Jonathan Tomp- and Pierre Sermanet. Online object representations with
son, Jonathan Yang, Joseph J Lim, Joa˜o Silve´rio, Jun- contrastive learning. 2019.
hyek Han, Kanishka Rao, Karl Pertsch, Karol Haus- [39] Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis,
man, Keegan Go, Keerthana Gopalakrishnan, Ken Gold- Sergey Levine, and Chelsea Finn. Reinforcement learn-
berg, Kendra Byrne, Kenneth Oslund, Kento Kawa- ing with videos: Combining offline observations with
harazuka, Kevin Zhang, Krishan Rana, Krishnan Srini- interaction. November 2020.
vasan,LawrenceYunliangChen,LerrelPinto,LiFei-Fei, [40] Rutav Shah, Roberto Mart´ın-Mart´ın, and Yuke Zhu.
Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, MUTEX:Learningunifiedpoliciesfrommultimodaltask
Max Spero, Maximilian Du, Michael Ahn, Mingtong specifications. In 7th Annual Conference on Robot
Zhang, Mingyu Ding, Mohan Kumar Srirama, Mo- Learning, 2023.
hit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas [41] Pratyusha Sharma, Deepak Pathak, and Abhinav Gupta.Third-Person visual imitation learning via decoupled
hierarchical controller. Adv. Neural Inf. Process. Syst.,
32, 2019.
[42] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter
Abbeel,andSergeyLevine. AVID:LearningMulti-Stage
tasks via Pixel-Level translation of human videos. In
Robotics: Science and Systems (RSS), December 2020.
[43] AustinStone,TedXiao,YaoLu,KeerthanaGopalakrish-
nan,Kuang-HueiLee,QuanVuong,PaulWohlhart,Sean
Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and
Karol Hausman. Open-World object manipulation using
pre-trained Vision-Language models. March 2023.
[44] TeteXiao,IlijaRadosavovic,TrevorDarrell,andJitendra
Malik. Masked visual pre-training for motor control.
arXiv preprint arXiv:2203.06173, 2023.
[45] Sriram Yenamandra, Arun Ramachandran, Karmesh Ya-
dav, Austin Wang, Mukul Khanna, Theophile Gervet,
Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg,
John Turner, Zsolt Kira, Manolis Savva, Angel Chang,
Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mot-
taghi, Yonatan Bisk, and Chris Paxton. HomeRobot:
Open-Vocabulary Mobile Manipulation. In Conference
on Robot Learning, 2023. URL https://arxiv.org/abs/
2306.11565.
[46] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,
Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Con-
trastive captioners are image-text foundation models.
arXiv preprint arXiv:2205. 01917, 2022.
[47] KevinZakka,AndyZeng,PeteFlorence,JonathanTomp-
son, Jeannette Bohg, and Debidatta Dwibedi. Xirl:
Cross-embodiment inverse reinforcement learning. In
Conference on Robot Learning, pages 537–546. PMLR,
2022.
[48] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,and
Lucas Beyer. Sigmoid loss for language image pre-
training. In 2023 IEEE/CVF International Conference
on Computer Vision (ICCV), pages 11941–11952, Los
Alamitos, CA, USA, oct 2023. IEEE Computer Society.
doi: 10.1109/ICCV51070.2023.01100.
[49] TonyZZhao,VikashKumar,SergeyLevine,andChelsea
Finn.LearningFine-Grainedbimanualmanipulationwith
Low-Cost hardware. In Robotics: Science and Systems,
2023.