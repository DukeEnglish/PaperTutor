[
    {
        "title": "CoSLight: Co-optimizing Collaborator Selection and Decision-making to Enhance Traffic Signal Control",
        "authors": "Jingqing RuanZiyue LiHua WeiHaoyuan JiangJiaming LuXuantang XiongHangyu MaoRui Zhao",
        "links": "http://arxiv.org/abs/2405.17152v1",
        "entry_id": "http://arxiv.org/abs/2405.17152v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17152v1",
        "summary": "Effective multi-intersection collaboration is pivotal for\nreinforcement-learning-based traffic signal control to alleviate congestion.\nExisting work mainly chooses neighboring intersections as collaborators.\nHowever, quite an amount of congestion, even some wide-range congestion, is\ncaused by non-neighbors failing to collaborate. To address these issues, we\npropose to separate the collaborator selection as a second policy to be\nlearned, concurrently being updated with the original signal-controlling\npolicy. Specifically, the selection policy in real-time adaptively selects the\nbest teammates according to phase- and intersection-level features. Empirical\nresults on both synthetic and real-world datasets provide robust validation for\nthe superiority of our approach, offering significant improvements over\nexisting state-of-the-art methods. The code is available at\nhttps://github.com/AnonymousAccountss/CoSLight.",
        "updated": "2024-05-27 13:26:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17152v1"
    },
    {
        "title": "Analysis of Multiscale Reinforcement Q-Learning Algorithms for Mean Field Control Games",
        "authors": "Andrea AngiuliJean-Pierre FouqueMathieu LaurièreMengrui Zhang",
        "links": "http://arxiv.org/abs/2405.17017v1",
        "entry_id": "http://arxiv.org/abs/2405.17017v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17017v1",
        "summary": "Mean Field Control Games (MFCG), introduced in [Angiuli et al., 2022a],\nrepresent competitive games between a large number of large collaborative\ngroups of agents in the infinite limit of number and size of groups. In this\npaper, we prove the convergence of a three-timescale Reinforcement Q-Learning\n(RL) algorithm to solve MFCG in a model-free approach from the point of view of\nrepresentative agents. Our analysis uses a Q-table for finite state and action\nspaces updated at each discrete time-step over an infinite horizon. In [Angiuli\net al., 2023], we proved convergence of two-timescale algorithms for MFG and\nMFC separately highlighting the need to follow multiple population\ndistributions in the MFC case. Here, we integrate this feature for MFCG as well\nas three rates of update decreasing to zero in the proper ratios. Our technique\nof proof uses a generalization to three timescales of the two-timescale\nanalysis in [Borkar, 1997]. We give a simple example satisfying the various\nhypothesis made in the proof of convergence and illustrating the performance of\nthe algorithm.",
        "updated": "2024-05-27 10:01:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17017v1"
    },
    {
        "title": "A Large Language Model-based multi-agent manufacturing system for intelligent shopfloor",
        "authors": "Zhen ZhaoDunbing TangHaihua ZhuZequn ZhangKai ChenChangchun LiuYuchen Ji",
        "links": "http://arxiv.org/abs/2405.16887v1",
        "entry_id": "http://arxiv.org/abs/2405.16887v1",
        "pdf_url": "http://arxiv.org/pdf/2405.16887v1",
        "summary": "As productivity advances, the demand of customers for multi-variety and\nsmall-batch production is increasing, thereby putting forward higher\nrequirements for manufacturing systems. When production tasks frequent changes\ndue to this demand, traditional manufacturing systems often cannot response\npromptly. The multi-agent manufacturing system is proposed to address this\nproblem. However, because of technical limitations, the negotiation among\nagents in this kind of system is realized through predefined heuristic rules,\nwhich is not intelligent enough to deal with the multi-variety and small batch\nproduction. To this end, a Large Language Model-based (LLM-based) multi-agent\nmanufacturing system for intelligent shopfloor is proposed in the present\nstudy. This system delineates the diverse agents and defines their\ncollaborative methods. The roles of the agents encompass Machine Server Agent\n(MSA), Bid Inviter Agent (BIA), Bidder Agent (BA), Thinking Agent (TA), and\nDecision Agent (DA). Due to the support of LLMs, TA and DA acquire the ability\nof analyzing the shopfloor condition and choosing the most suitable machine, as\nopposed to executing a predefined program artificially. The negotiation between\nBAs and BIA is the most crucial step in connecting manufacturing resources.\nWith the support of TA and DA, BIA will finalize the distribution of orders,\nrelying on the information of each machine returned by BA. MSAs bears the\nresponsibility for connecting the agents with the physical shopfloor. This\nsystem aims to distribute and transmit workpieces through the collaboration of\nthe agents with these distinct roles, distinguishing it from other scheduling\napproaches. Comparative experiments were also conducted to validate the\nperformance of this system.",
        "updated": "2024-05-27 07:10:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.16887v1"
    },
    {
        "title": "Knowing What Not to Do: Leverage Language Model Insights for Action Space Pruning in Multi-agent Reinforcement Learning",
        "authors": "Zhihao LiuXianliang YangZichuan LiuYifan XiaWei JiangYuanyu ZhangLijuan LiGuoliang FanLei SongBian Jiang",
        "links": "http://arxiv.org/abs/2405.16854v1",
        "entry_id": "http://arxiv.org/abs/2405.16854v1",
        "pdf_url": "http://arxiv.org/pdf/2405.16854v1",
        "summary": "Multi-agent reinforcement learning (MARL) is employed to develop autonomous\nagents that can learn to adopt cooperative or competitive strategies within\ncomplex environments. However, the linear increase in the number of agents\nleads to a combinatorial explosion of the action space, which may result in\nalgorithmic instability, difficulty in convergence, or entrapment in local\noptima. While researchers have designed a variety of effective algorithms to\ncompress the action space, these methods also introduce new challenges, such as\nthe need for manually designed prior knowledge or reliance on the structure of\nthe problem, which diminishes the applicability of these techniques. In this\npaper, we introduce Evolutionary action SPAce Reduction with Knowledge\n(eSpark), an exploration function generation framework driven by large language\nmodels (LLMs) to boost exploration and prune unnecessary actions in MARL. Using\njust a basic prompt that outlines the overall task and setting, eSpark is\ncapable of generating exploration functions in a zero-shot manner, identifying\nand pruning redundant or irrelevant state-action pairs, and then achieving\nautonomous improvement from policy feedback. In reinforcement learning tasks\ninvolving inventory management and traffic light control encompassing a total\nof 15 scenarios, eSpark consistently outperforms the combined MARL algorithm in\nall scenarios, achieving an average performance gain of 34.4% and 9.9% in the\ntwo types of tasks respectively. Additionally, eSpark has proven to be capable\nof managing situations with a large number of agents, securing a 29.7%\nimprovement in scalability challenges that featured over 500 agents. The code\ncan be found in https://github.com/LiuZhihao2022/eSpark.git.",
        "updated": "2024-05-27 06:00:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.16854v1"
    },
    {
        "title": "LLM-Based Cooperative Agents using Information Relevance and Plan Validation",
        "authors": "SeungWon SeoJunhyeok LeeSeongRae NohHyeongYeop Kang",
        "links": "http://arxiv.org/abs/2405.16751v1",
        "entry_id": "http://arxiv.org/abs/2405.16751v1",
        "pdf_url": "http://arxiv.org/pdf/2405.16751v1",
        "summary": "We address the challenge of multi-agent cooperation, where agents achieve a\ncommon goal by interacting with a 3D scene and cooperating with decentralized\nagents under complex partial observations. This involves managing communication\ncosts and optimizing interaction trajectories in dynamic environments. Our\nresearch focuses on three primary limitations of existing cooperative agent\nsystems. Firstly, current systems demonstrate inefficiency in managing acquired\ninformation through observation, resulting in declining planning performance as\nthe environment becomes more complex with additional objects or goals.\nSecondly, the neglect of false plans in partially observable settings leads to\nsuboptimal cooperative performance, as agents struggle to adapt to\nenvironmental changes influenced by the unseen actions of other agents. Lastly,\nthe failure to incorporate spatial data into decision-making processes\nrestricts the agent's ability to construct optimized trajectories. To overcome\nthese limitations, we propose the RElevance and Validation-Enhanced Cooperative\nLanguage Agent (REVECA), a novel cognitive architecture powered by GPT-3.5.\nREVECA leverages relevance assessment, plan validation, and spatial information\nto enhance the efficiency and robustness of agent cooperation in dynamic and\npartially observable environments while minimizing continuous communication\ncosts and effectively managing irrelevant dummy objects. Our extensive\nexperiments demonstrate the superiority of REVECA over previous approaches,\nincluding those driven by GPT-4.0. Additionally, a user study highlights\nREVECA's potential for achieving trustworthy human-AI cooperation. We expect\nthat REVECA will have significant applications in gaming, XR applications,\neducational tools, and humanoid robots, contributing to substantial economic,\ncommercial, and academic advancements.",
        "updated": "2024-05-27 01:47:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.16751v1"
    }
]