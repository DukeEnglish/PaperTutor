[
    {
        "title": "Matryoshka Multimodal Models",
        "authors": "Mu CaiJianwei YangJianfeng GaoYong Jae Lee",
        "links": "http://arxiv.org/abs/2405.17430v1",
        "entry_id": "http://arxiv.org/abs/2405.17430v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17430v1",
        "summary": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in\nvisual-linguistic reasoning. These models first embed images into a fixed large\nnumber of visual tokens and then feed them into a Large Language Model (LLM).\nHowever, this design causes an excessive number of tokens for dense visual\nscenarios such as high-resolution images and videos, leading to great\ninefficiency. While token pruning/merging methods do exist, they produce a\nsingle length output for each image and do not afford flexibility in trading\noff information density v.s. efficiency. Inspired by the concept of Matryoshka\nDolls, we propose M3: Matryoshka Multimodal Models, which learns to represent\nvisual content as nested sets of visual tokens that capture information across\nmultiple coarse-to-fine granularities. Our approach offers several unique\nbenefits for LMMs: (1) One can explicitly control the visual granularity per\ntest instance during inference, e.g. , adjusting the number of tokens used to\nrepresent an image based on the anticipated complexity or simplicity of the\ncontent; (2) M3 provides a framework for analyzing the granularity needed for\nexisting datasets, where we find that COCO-style benchmarks only need around ~9\nvisual tokens to obtain accuracy similar to that of using all 576 tokens; (3)\nOur approach provides a foundation to explore the best trade-off between\nperformance and visual token length at sample level, where our investigation\nreveals that a large gap exists between the oracle upper bound and current\nfixed-scale representations.",
        "updated": "2024-05-27 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17430v1"
    },
    {
        "title": "GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction",
        "authors": "Yuanhui HuangWenzhao ZhengYunpeng ZhangJie ZhouJiwen Lu",
        "links": "http://arxiv.org/abs/2405.17429v1",
        "entry_id": "http://arxiv.org/abs/2405.17429v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17429v1",
        "summary": "3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and\nsemantics of the surrounding scene and is an important task for the robustness\nof vision-centric autonomous driving. Most existing methods employ dense grids\nsuch as voxels as scene representations, which ignore the sparsity of occupancy\nand the diversity of object scales and thus lead to unbalanced allocation of\nresources. To address this, we propose an object-centric representation to\ndescribe 3D scenes with sparse 3D semantic Gaussians where each Gaussian\nrepresents a flexible region of interest and its semantic features. We\naggregate information from images through the attention mechanism and\niteratively refine the properties of 3D Gaussians including position,\ncovariance, and semantics. We then propose an efficient Gaussian-to-voxel\nsplatting method to generate 3D occupancy predictions, which only aggregates\nthe neighboring Gaussians for a certain position. We conduct extensive\nexperiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental\nresults demonstrate that GaussianFormer achieves comparable performance with\nstate-of-the-art methods with only 17.8% - 24.8% of their memory consumption.\nCode is available at: https://github.com/huang-yh/GaussianFormer.",
        "updated": "2024-05-27 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17429v1"
    },
    {
        "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
        "authors": "Chankyu LeeRajarshi RoyMengyao XuJonathan RaimanMohammad ShoeybiBryan CatanzaroWei Ping",
        "links": "http://arxiv.org/abs/2405.17428v1",
        "entry_id": "http://arxiv.org/abs/2405.17428v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17428v1",
        "summary": "Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model with a variety of architectural designs and\ntraining procedures to significantly enhance the performance of LLM as a\nversatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For model training, we introduce a\ntwo-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ndatasets into instruction tuning, which not only enhances non-retrieval task\naccuracy but also improves retrieval performance. Combining these techniques,\nour NV-Embed model, using only publicly available data, has achieved a\nrecord-high score of 69.32, ranking No. 1 on the Massive Text Embedding\nBenchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,\nreranking, classification, clustering, and semantic textual similarity tasks.\nNotably, our model also attains the highest score of 59.36 on 15 retrieval\ntasks in the MTEB benchmark (also known as BEIR). We will open-source the model\nat: https://huggingface.co/nvidia/NV-Embed-v1.",
        "updated": "2024-05-27 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17428v1"
    },
    {
        "title": "Hardness-Aware Scene Synthesis for Semi-Supervised 3D Object Detection",
        "authors": "Shuai ZengWenzhao ZhengJiwen LuHaibin Yan",
        "links": "http://arxiv.org/abs/2405.17422v1",
        "entry_id": "http://arxiv.org/abs/2405.17422v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17422v1",
        "summary": "3D object detection aims to recover the 3D information of concerning objects\nand serves as the fundamental task of autonomous driving perception. Its\nperformance greatly depends on the scale of labeled training data, yet it is\ncostly to obtain high-quality annotations for point cloud data. While\nconventional methods focus on generating pseudo-labels for unlabeled samples as\nsupplements for training, the structural nature of 3D point cloud data\nfacilitates the composition of objects and backgrounds to synthesize realistic\nscenes. Motivated by this, we propose a hardness-aware scene synthesis (HASS)\nmethod to generate adaptive synthetic scenes to improve the generalization of\nthe detection models. We obtain pseudo-labels for unlabeled objects and\ngenerate diverse scenes with different compositions of objects and backgrounds.\nAs the scene synthesis is sensitive to the quality of pseudo-labels, we further\npropose a hardness-aware strategy to reduce the effect of low-quality\npseudo-labels and maintain a dynamic pseudo-database to ensure the diversity\nand quality of synthetic scenes. Extensive experimental results on the widely\nused KITTI and Waymo datasets demonstrate the superiority of the proposed HASS\nmethod, which outperforms existing semi-supervised learning methods on 3D\nobject detection. Code: https://github.com/wzzheng/HASS.",
        "updated": "2024-05-27 17:59:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17422v1"
    },
    {
        "title": "MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities",
        "authors": "Hao DongYue ZhaoEleni ChatziOlga Fink",
        "links": "http://arxiv.org/abs/2405.17419v1",
        "entry_id": "http://arxiv.org/abs/2405.17419v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17419v1",
        "summary": "Detecting out-of-distribution (OOD) samples is important for deploying\nmachine learning models in safety-critical applications such as autonomous\ndriving and robot-assisted surgery. Existing research has mainly focused on\nunimodal scenarios on image data. However, real-world applications are\ninherently multimodal, which makes it essential to leverage information from\nmultiple modalities to enhance the efficacy of OOD detection. To establish a\nfoundation for more realistic Multimodal OOD Detection, we introduce the\nfirst-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes\nand varying modality combinations. We first evaluate existing unimodal OOD\ndetection algorithms on MultiOOD, observing that the mere inclusion of\nadditional modalities yields substantial improvements. This underscores the\nimportance of utilizing multiple modalities for OOD detection. Based on the\nobservation of Modality Prediction Discrepancy between in-distribution (ID) and\nOOD data, and its strong correlation with OOD performance, we propose the\nAgree-to-Disagree (A2D) algorithm to encourage such discrepancy during\ntraining. Moreover, we introduce a novel outlier synthesis method, NP-Mix,\nwhich explores broader feature spaces by leveraging the information from\nnearest neighbor classes and complements A2D to strengthen OOD detection\nperformance. Extensive experiments on MultiOOD demonstrate that training with\nA2D and NP-Mix improves existing OOD detection algorithms by a large margin.\nOur source code and MultiOOD benchmark are available at\nhttps://github.com/donghao51/MultiOOD.",
        "updated": "2024-05-27 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17419v1"
    }
]