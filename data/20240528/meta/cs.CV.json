[
    {
        "title": "Matryoshka Multimodal Models",
        "authors": "Mu CaiJianwei YangJianfeng GaoYong Jae Lee",
        "links": "http://arxiv.org/abs/2405.17430v1",
        "entry_id": "http://arxiv.org/abs/2405.17430v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17430v1",
        "summary": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in\nvisual-linguistic reasoning. These models first embed images into a fixed large\nnumber of visual tokens and then feed them into a Large Language Model (LLM).\nHowever, this design causes an excessive number of tokens for dense visual\nscenarios such as high-resolution images and videos, leading to great\ninefficiency. While token pruning/merging methods do exist, they produce a\nsingle length output for each image and do not afford flexibility in trading\noff information density v.s. efficiency. Inspired by the concept of Matryoshka\nDolls, we propose M3: Matryoshka Multimodal Models, which learns to represent\nvisual content as nested sets of visual tokens that capture information across\nmultiple coarse-to-fine granularities. Our approach offers several unique\nbenefits for LMMs: (1) One can explicitly control the visual granularity per\ntest instance during inference, e.g. , adjusting the number of tokens used to\nrepresent an image based on the anticipated complexity or simplicity of the\ncontent; (2) M3 provides a framework for analyzing the granularity needed for\nexisting datasets, where we find that COCO-style benchmarks only need around ~9\nvisual tokens to obtain accuracy similar to that of using all 576 tokens; (3)\nOur approach provides a foundation to explore the best trade-off between\nperformance and visual token length at sample level, where our investigation\nreveals that a large gap exists between the oracle upper bound and current\nfixed-scale representations.",
        "updated": "2024-05-27 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17430v1"
    },
    {
        "title": "GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction",
        "authors": "Yuanhui HuangWenzhao ZhengYunpeng ZhangJie ZhouJiwen Lu",
        "links": "http://arxiv.org/abs/2405.17429v1",
        "entry_id": "http://arxiv.org/abs/2405.17429v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17429v1",
        "summary": "3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and\nsemantics of the surrounding scene and is an important task for the robustness\nof vision-centric autonomous driving. Most existing methods employ dense grids\nsuch as voxels as scene representations, which ignore the sparsity of occupancy\nand the diversity of object scales and thus lead to unbalanced allocation of\nresources. To address this, we propose an object-centric representation to\ndescribe 3D scenes with sparse 3D semantic Gaussians where each Gaussian\nrepresents a flexible region of interest and its semantic features. We\naggregate information from images through the attention mechanism and\niteratively refine the properties of 3D Gaussians including position,\ncovariance, and semantics. We then propose an efficient Gaussian-to-voxel\nsplatting method to generate 3D occupancy predictions, which only aggregates\nthe neighboring Gaussians for a certain position. We conduct extensive\nexperiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental\nresults demonstrate that GaussianFormer achieves comparable performance with\nstate-of-the-art methods with only 17.8% - 24.8% of their memory consumption.\nCode is available at: https://github.com/huang-yh/GaussianFormer.",
        "updated": "2024-05-27 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17429v1"
    },
    {
        "title": "Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model",
        "authors": "Kuan-Chih HuangXiangtai LiLu QiShuicheng YanMing-Hsuan Yang",
        "links": "http://arxiv.org/abs/2405.17427v1",
        "entry_id": "http://arxiv.org/abs/2405.17427v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17427v1",
        "summary": "Recent advancements in multimodal large language models (LLMs) have shown\ntheir potential in various domains, especially concept reasoning. Despite these\ndevelopments, applications in understanding 3D environments remain limited.\nThis paper introduces Reason3D, a novel LLM designed for comprehensive 3D\nunderstanding. Reason3D takes point cloud data and text prompts as input to\nproduce textual responses and segmentation masks, facilitating advanced tasks\nlike 3D reasoning segmentation, hierarchical searching, express referring, and\nquestion answering with detailed mask outputs. Specifically, we propose a\nhierarchical mask decoder to locate small objects within expansive scenes. This\ndecoder initially generates a coarse location estimate covering the object's\ngeneral area. This foundational estimation facilitates a detailed,\ncoarse-to-fine segmentation strategy that significantly enhances the precision\nof object identification and segmentation. Experiments validate that Reason3D\nachieves remarkable results on large-scale ScanNet and Matterport3D datasets\nfor 3D express referring, 3D question answering, and 3D reasoning segmentation\ntasks. Code and models are available at:\nhttps://github.com/KuanchihHuang/Reason3D.",
        "updated": "2024-05-27 17:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17427v1"
    },
    {
        "title": "Benchmarking and Improving Bird's Eye View Perception Robustness in Autonomous Driving",
        "authors": "Shaoyuan XieLingdong KongWenwei ZhangJiawei RenLiang PanKai ChenZiwei Liu",
        "links": "http://arxiv.org/abs/2405.17426v1",
        "entry_id": "http://arxiv.org/abs/2405.17426v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17426v1",
        "summary": "Recent advancements in bird's eye view (BEV) representations have shown\nremarkable promise for in-vehicle 3D perception. However, while these methods\nhave achieved impressive results on standard benchmarks, their robustness in\nvaried conditions remains insufficiently assessed. In this study, we present\nRoboBEV, an extensive benchmark suite designed to evaluate the resilience of\nBEV algorithms. This suite incorporates a diverse set of camera corruption\ntypes, each examined over three severity levels. Our benchmarks also consider\nthe impact of complete sensor failures that occur when using multi-modal\nmodels. Through RoboBEV, we assess 33 state-of-the-art BEV-based perception\nmodels spanning tasks like detection, map segmentation, depth estimation, and\noccupancy prediction. Our analyses reveal a noticeable correlation between the\nmodel's performance on in-distribution datasets and its resilience to\nout-of-distribution challenges. Our experimental results also underline the\nefficacy of strategies like pre-training and depth-free BEV transformations in\nenhancing robustness against out-of-distribution data. Furthermore, we observe\nthat leveraging extensive temporal information significantly improves the\nmodel's robustness. Based on our observations, we design an effective\nrobustness enhancement strategy based on the CLIP model. The insights from this\nstudy pave the way for the development of future BEV models that seamlessly\ncombine accuracy with real-world robustness.",
        "updated": "2024-05-27 17:59:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17426v1"
    },
    {
        "title": "LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence",
        "authors": "Zhuoling LiXiaogang XuZhenhua XuSerNam LimHengshuang Zhao",
        "links": "http://arxiv.org/abs/2405.17424v1",
        "entry_id": "http://arxiv.org/abs/2405.17424v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17424v1",
        "summary": "Due to the need to interact with the real world, embodied agents are required\nto possess comprehensive prior knowledge, long-horizon planning capability, and\na swift response speed. Despite recent large language model (LLM) based agents\nachieving promising performance, they still exhibit several limitations. For\ninstance, the output of LLMs is a descriptive sentence, which is ambiguous when\ndetermining specific actions. To address these limitations, we introduce the\nlarge auto-regressive model (LARM). LARM leverages both text and multi-view\nimages as input and predicts subsequent actions in an auto-regressive manner.\nTo train LARM, we develop a novel data format named auto-regressive node\ntransmission structure and assemble a corresponding dataset. Adopting a\ntwo-phase training regimen, LARM successfully harvests enchanted equipment in\nMinecraft, which demands significantly more complex decision-making chains than\nthe highest achievements of prior best methods. Besides, the speed of LARM is\n6.8x faster.",
        "updated": "2024-05-27 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17424v1"
    }
]