[
    {
        "title": "Matryoshka Multimodal Models",
        "authors": "Mu CaiJianwei YangJianfeng GaoYong Jae Lee",
        "links": "http://arxiv.org/abs/2405.17430v1",
        "entry_id": "http://arxiv.org/abs/2405.17430v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17430v1",
        "summary": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in\nvisual-linguistic reasoning. These models first embed images into a fixed large\nnumber of visual tokens and then feed them into a Large Language Model (LLM).\nHowever, this design causes an excessive number of tokens for dense visual\nscenarios such as high-resolution images and videos, leading to great\ninefficiency. While token pruning/merging methods do exist, they produce a\nsingle length output for each image and do not afford flexibility in trading\noff information density v.s. efficiency. Inspired by the concept of Matryoshka\nDolls, we propose M3: Matryoshka Multimodal Models, which learns to represent\nvisual content as nested sets of visual tokens that capture information across\nmultiple coarse-to-fine granularities. Our approach offers several unique\nbenefits for LMMs: (1) One can explicitly control the visual granularity per\ntest instance during inference, e.g. , adjusting the number of tokens used to\nrepresent an image based on the anticipated complexity or simplicity of the\ncontent; (2) M3 provides a framework for analyzing the granularity needed for\nexisting datasets, where we find that COCO-style benchmarks only need around ~9\nvisual tokens to obtain accuracy similar to that of using all 576 tokens; (3)\nOur approach provides a foundation to explore the best trade-off between\nperformance and visual token length at sample level, where our investigation\nreveals that a large gap exists between the oracle upper bound and current\nfixed-scale representations.",
        "updated": "2024-05-27 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17430v1"
    },
    {
        "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
        "authors": "Chankyu LeeRajarshi RoyMengyao XuJonathan RaimanMohammad ShoeybiBryan CatanzaroWei Ping",
        "links": "http://arxiv.org/abs/2405.17428v1",
        "entry_id": "http://arxiv.org/abs/2405.17428v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17428v1",
        "summary": "Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model with a variety of architectural designs and\ntraining procedures to significantly enhance the performance of LLM as a\nversatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For model training, we introduce a\ntwo-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ndatasets into instruction tuning, which not only enhances non-retrieval task\naccuracy but also improves retrieval performance. Combining these techniques,\nour NV-Embed model, using only publicly available data, has achieved a\nrecord-high score of 69.32, ranking No. 1 on the Massive Text Embedding\nBenchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,\nreranking, classification, clustering, and semantic textual similarity tasks.\nNotably, our model also attains the highest score of 59.36 on 15 retrieval\ntasks in the MTEB benchmark (also known as BEIR). We will open-source the model\nat: https://huggingface.co/nvidia/NV-Embed-v1.",
        "updated": "2024-05-27 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17428v1"
    },
    {
        "title": "From Neurons to Neutrons: A Case Study in Interpretability",
        "authors": "Ouail KitouniNiklas NolteVíctor Samuel Pérez-DíazSokratis TrifinopoulosMike Williams",
        "links": "http://arxiv.org/abs/2405.17425v1",
        "entry_id": "http://arxiv.org/abs/2405.17425v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17425v1",
        "summary": "Mechanistic Interpretability (MI) promises a path toward fully understanding\nhow neural networks make their predictions. Prior work demonstrates that even\nwhen trained to perform simple arithmetic, models can implement a variety of\nalgorithms (sometimes concurrently) depending on initialization and\nhyperparameters. Does this mean neuron-level interpretability techniques have\nlimited applicability? We argue that high-dimensional neural networks can learn\nlow-dimensional representations of their training data that are useful beyond\nsimply making good predictions. Such representations can be understood through\nthe mechanistic interpretability lens and provide insights that are\nsurprisingly faithful to human-derived domain knowledge. This indicates that\nsuch approaches to interpretability can be useful for deriving a new\nunderstanding of a problem from models trained to solve it. As a case study, we\nextract nuclear physics concepts by studying models trained to reproduce\nnuclear data.",
        "updated": "2024-05-27 17:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17425v1"
    },
    {
        "title": "Hardness-Aware Scene Synthesis for Semi-Supervised 3D Object Detection",
        "authors": "Shuai ZengWenzhao ZhengJiwen LuHaibin Yan",
        "links": "http://arxiv.org/abs/2405.17422v1",
        "entry_id": "http://arxiv.org/abs/2405.17422v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17422v1",
        "summary": "3D object detection aims to recover the 3D information of concerning objects\nand serves as the fundamental task of autonomous driving perception. Its\nperformance greatly depends on the scale of labeled training data, yet it is\ncostly to obtain high-quality annotations for point cloud data. While\nconventional methods focus on generating pseudo-labels for unlabeled samples as\nsupplements for training, the structural nature of 3D point cloud data\nfacilitates the composition of objects and backgrounds to synthesize realistic\nscenes. Motivated by this, we propose a hardness-aware scene synthesis (HASS)\nmethod to generate adaptive synthetic scenes to improve the generalization of\nthe detection models. We obtain pseudo-labels for unlabeled objects and\ngenerate diverse scenes with different compositions of objects and backgrounds.\nAs the scene synthesis is sensitive to the quality of pseudo-labels, we further\npropose a hardness-aware strategy to reduce the effect of low-quality\npseudo-labels and maintain a dynamic pseudo-database to ensure the diversity\nand quality of synthetic scenes. Extensive experimental results on the widely\nused KITTI and Waymo datasets demonstrate the superiority of the proposed HASS\nmethod, which outperforms existing semi-supervised learning methods on 3D\nobject detection. Code: https://github.com/wzzheng/HASS.",
        "updated": "2024-05-27 17:59:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17422v1"
    },
    {
        "title": "Survival of the Fittest Representation: A Case Study with Modular Addition",
        "authors": "Xiaoman Delores DingZifan Carl GuoEric J. MichaudZiming LiuMax Tegmark",
        "links": "http://arxiv.org/abs/2405.17420v1",
        "entry_id": "http://arxiv.org/abs/2405.17420v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17420v1",
        "summary": "When a neural network can learn multiple distinct algorithms to solve a task,\nhow does it \"choose\" between them during training? To approach this question,\nwe take inspiration from ecology: when multiple species coexist, they\neventually reach an equilibrium where some survive while others die out.\nAnalogously, we suggest that a neural network at initialization contains many\nsolutions (representations and algorithms), which compete with each other under\npressure from resource constraints, with the \"fittest\" ultimately prevailing.\nTo investigate this Survival of the Fittest hypothesis, we conduct a case study\non neural networks performing modular addition, and find that these networks'\nmultiple circular representations at different Fourier frequencies undergo such\ncompetitive dynamics, with only a few circles surviving at the end. We find\nthat the frequencies with high initial signals and gradients, the \"fittest,\"\nare more likely to survive. By increasing the embedding dimension, we also\nobserve more surviving frequencies. Inspired by the Lotka-Volterra equations\ndescribing the dynamics between species, we find that the dynamics of the\ncircles can be nicely characterized by a set of linear differential equations.\nOur results with modular addition show that it is possible to decompose\ncomplicated representations into simpler components, along with their basic\ninteractions, to offer insight on the training dynamics of representations.",
        "updated": "2024-05-27 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17420v1"
    }
]