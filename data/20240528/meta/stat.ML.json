[
    {
        "title": "Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE",
        "authors": "Aditya RavuriNeil D. Lawrence",
        "links": "http://arxiv.org/abs/2405.17412v1",
        "entry_id": "http://arxiv.org/abs/2405.17412v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17412v1",
        "summary": "This paper shows that the dimensionality reduction methods, UMAP and t-SNE,\ncan be approximately recast as MAP inference methods corresponding to a\ngeneralized Wishart-based model introduced in ProbDR. This interpretation\noffers deeper theoretical insights into these algorithms, while introducing\ntools with which similar dimensionality reduction methods can be studied.",
        "updated": "2024-05-27 17:57:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17412v1"
    },
    {
        "title": "Spectral Greedy Coresets for Graph Neural Networks",
        "authors": "Mucong DingYinhan HeJundong LiFurong Huang",
        "links": "http://arxiv.org/abs/2405.17404v1",
        "entry_id": "http://arxiv.org/abs/2405.17404v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17404v1",
        "summary": "The ubiquity of large-scale graphs in node-classification tasks significantly\nhinders the real-world applications of Graph Neural Networks (GNNs). Node\nsampling, graph coarsening, and dataset condensation are effective strategies\nfor enhancing data efficiency. However, owing to the interdependence of graph\nnodes, coreset selection, which selects subsets of the data examples, has not\nbeen successfully applied to speed up GNN training on large graphs, warranting\nspecial treatment. This paper studies graph coresets for GNNs and avoids the\ninterdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs\naround a node) based on their spectral embeddings. We decompose the coreset\nselection problem for GNNs into two phases: a coarse selection of widely spread\nego graphs and a refined selection to diversify their topologies. We design a\ngreedy algorithm that approximately optimizes both objectives. Our spectral\ngreedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates\nthe need for model pre-training, and applies to low-homophily graphs. Extensive\nexperiments on ten datasets demonstrate that SGGC outperforms other coreset\nmethods by a wide margin, generalizes well across GNN architectures, and is\nmuch faster than graph condensation.",
        "updated": "2024-05-27 17:52:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17404v1"
    },
    {
        "title": "RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control",
        "authors": "Litu RoutYujia ChenNataniel RuizAbhishek KumarConstantine CaramanisSanjay ShakkottaiWen-Sheng Chu",
        "links": "http://arxiv.org/abs/2405.17401v1",
        "entry_id": "http://arxiv.org/abs/2405.17401v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17401v1",
        "summary": "We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play\nsolution for training-free personalization of diffusion models. Existing\ntraining-free approaches exhibit difficulties in (a) style extraction from\nreference images in the absence of additional style or content text\ndescriptions, (b) unwanted content leakage from reference style images, and (c)\neffective composition of style and content. RB-Modulation is built on a novel\nstochastic optimal controller where a style descriptor encodes the desired\nattributes through a terminal cost. The resulting drift not only overcomes the\ndifficulties above, but also ensures high fidelity to the reference style and\nadheres to the given text prompt. We also introduce a cross-attention-based\nfeature aggregation scheme that allows RB-Modulation to decouple content and\nstyle from the reference image. With theoretical justification and empirical\nevidence, our framework demonstrates precise extraction and control of content\nand style in a training-free manner. Further, our method allows a seamless\ncomposition of content and style, which marks a departure from the dependency\non external adapters or ControlNets.",
        "updated": "2024-05-27 17:51:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17401v1"
    },
    {
        "title": "Conditioning on Time is All You Need for Synthetic Survival Data Generation",
        "authors": "Mohd AshhadRicardo Henao",
        "links": "http://arxiv.org/abs/2405.17333v1",
        "entry_id": "http://arxiv.org/abs/2405.17333v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17333v1",
        "summary": "Synthetic data generation holds considerable promise, offering avenues to\nenhance privacy, fairness, and data accessibility. Despite the availability of\nvarious methods for generating synthetic tabular data, challenges persist,\nparticularly in specialized applications such as survival analysis. One\nsignificant obstacle in survival data generation is censoring, which manifests\nas not knowing the precise timing of observed (target) events for certain\ninstances. Existing methods face difficulties in accurately reproducing the\nreal distribution of event times for both observed (uncensored) events and\ncensored events, i.e., the generated event-time distributions do not accurately\nmatch the underlying distributions of the real data. So motivated, we propose a\nsimple paradigm to produce synthetic survival data by generating covariates\nconditioned on event times (and censoring indicators), thus allowing one to\nreuse existing conditional generative models for tabular data without\nsignificant computational overhead, and without making assumptions about the\n(usually unknown) generation mechanism underlying censoring. We evaluate this\nmethod via extensive experiments on real-world datasets. Our methodology\noutperforms multiple competitive baselines at generating survival data, while\nimproving the performance of downstream survival models trained on it and\ntested on real data.",
        "updated": "2024-05-27 16:34:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17333v1"
    },
    {
        "title": "Leveraging Offline Data in Linear Latent Bandits",
        "authors": "Chinmaya KausikKevin TanAmbuj Tewari",
        "links": "http://arxiv.org/abs/2405.17324v1",
        "entry_id": "http://arxiv.org/abs/2405.17324v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17324v1",
        "summary": "Sequential decision-making domains such as recommender systems, healthcare\nand education often have unobserved heterogeneity in the population that can be\nmodeled using latent bandits $-$ a framework where an unobserved latent state\ndetermines the model for a trajectory. While the latent bandit framework is\ncompelling, the extent of its generality is unclear. We first address this by\nestablishing a de Finetti theorem for decision processes, and show that\n$\\textit{every}$ exchangeable and coherent stateless decision process is a\nlatent bandit. The latent bandit framework lends itself particularly well to\nonline learning with offline datasets, a problem of growing interest in\nsequential decision-making. One can leverage offline latent bandit data to\nlearn a complex model for each latent state, so that an agent can simply learn\nthe latent state online to act optimally. We focus on a linear model for a\nlatent bandit with $d_A$-dimensional actions, where the latent states lie in an\nunknown $d_K$-dimensional subspace for $d_K \\ll d_A$. We present SOLD, a novel\nprincipled method to learn this subspace from short offline trajectories with\nguarantees. We then provide two methods to leverage this subspace online:\nLOCAL-UCB and ProBALL-UCB. We demonstrate that LOCAL-UCB enjoys $\\tilde\nO(\\min(d_A\\sqrt{T}, d_K\\sqrt{T}(1+\\sqrt{d_AT/d_KN})))$ regret guarantees, where\nthe effective dimension is lower when the size $N$ of the offline dataset is\nlarger. ProBALL-UCB enjoys a slightly weaker guarantee, but is more practical\nand computationally efficient. Finally, we establish the efficacy of our\nmethods using experiments on both synthetic data and real-life movie\nrecommendation data from MovieLens.",
        "updated": "2024-05-27 16:23:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17324v1"
    }
]