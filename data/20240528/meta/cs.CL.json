[
    {
        "title": "Matryoshka Multimodal Models",
        "authors": "Mu CaiJianwei YangJianfeng GaoYong Jae Lee",
        "links": "http://arxiv.org/abs/2405.17430v1",
        "entry_id": "http://arxiv.org/abs/2405.17430v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17430v1",
        "summary": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in\nvisual-linguistic reasoning. These models first embed images into a fixed large\nnumber of visual tokens and then feed them into a Large Language Model (LLM).\nHowever, this design causes an excessive number of tokens for dense visual\nscenarios such as high-resolution images and videos, leading to great\ninefficiency. While token pruning/merging methods do exist, they produce a\nsingle length output for each image and do not afford flexibility in trading\noff information density v.s. efficiency. Inspired by the concept of Matryoshka\nDolls, we propose M3: Matryoshka Multimodal Models, which learns to represent\nvisual content as nested sets of visual tokens that capture information across\nmultiple coarse-to-fine granularities. Our approach offers several unique\nbenefits for LMMs: (1) One can explicitly control the visual granularity per\ntest instance during inference, e.g. , adjusting the number of tokens used to\nrepresent an image based on the anticipated complexity or simplicity of the\ncontent; (2) M3 provides a framework for analyzing the granularity needed for\nexisting datasets, where we find that COCO-style benchmarks only need around ~9\nvisual tokens to obtain accuracy similar to that of using all 576 tokens; (3)\nOur approach provides a foundation to explore the best trade-off between\nperformance and visual token length at sample level, where our investigation\nreveals that a large gap exists between the oracle upper bound and current\nfixed-scale representations.",
        "updated": "2024-05-27 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17430v1"
    },
    {
        "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
        "authors": "Chankyu LeeRajarshi RoyMengyao XuJonathan RaimanMohammad ShoeybiBryan CatanzaroWei Ping",
        "links": "http://arxiv.org/abs/2405.17428v1",
        "entry_id": "http://arxiv.org/abs/2405.17428v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17428v1",
        "summary": "Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model with a variety of architectural designs and\ntraining procedures to significantly enhance the performance of LLM as a\nversatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For model training, we introduce a\ntwo-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ndatasets into instruction tuning, which not only enhances non-retrieval task\naccuracy but also improves retrieval performance. Combining these techniques,\nour NV-Embed model, using only publicly available data, has achieved a\nrecord-high score of 69.32, ranking No. 1 on the Massive Text Embedding\nBenchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,\nreranking, classification, clustering, and semantic textual similarity tasks.\nNotably, our model also attains the highest score of 59.36 on 15 retrieval\ntasks in the MTEB benchmark (also known as BEIR). We will open-source the model\nat: https://huggingface.co/nvidia/NV-Embed-v1.",
        "updated": "2024-05-27 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17428v1"
    },
    {
        "title": "Privacy-Aware Visual Language Models",
        "authors": "Laurens SamsonNimrod BarazaniSennay GhebreabYuki M. Asano",
        "links": "http://arxiv.org/abs/2405.17423v1",
        "entry_id": "http://arxiv.org/abs/2405.17423v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17423v1",
        "summary": "This paper aims to advance our understanding of how Visual Language Models\n(VLMs) handle privacy-sensitive information, a crucial concern as these\ntechnologies become integral to everyday life. To this end, we introduce a new\nbenchmark PrivBench, which contains images from 8 sensitive categories such as\npassports, or fingerprints. We evaluate 10 state-of-the-art VLMs on this\nbenchmark and observe a generally limited understanding of privacy,\nhighlighting a significant area for model improvement. Based on this we\nintroduce PrivTune, a new instruction-tuning dataset aimed at equipping VLMs\nwith knowledge about visual privacy. By tuning two pretrained VLMs, TinyLLaVa\nand MiniGPT-v2, on this small dataset, we achieve strong gains in their ability\nto recognize sensitive content, outperforming even GPT4-V. At the same time, we\nshow that privacy-tuning only minimally affects the VLMs performance on\nstandard benchmarks such as VQA. Overall, this paper lays out a crucial\nchallenge for making VLMs effective in handling real-world data safely and\nprovides a simple recipe that takes the first step towards building\nprivacy-aware VLMs.",
        "updated": "2024-05-27 17:59:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17423v1"
    },
    {
        "title": "THREAD: Thinking Deeper with Recursive Spawning",
        "authors": "Philip SchroederNathaniel MorganHongyin LuoJames Glass",
        "links": "http://arxiv.org/abs/2405.17402v1",
        "entry_id": "http://arxiv.org/abs/2405.17402v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17402v1",
        "summary": "Large language models (LLMs) have shown impressive capabilities across\ndiverse settings, but still struggle as the length and complexity of the\ncontext increases. To address this challenge, we propose Thinking Recursively\nand Dynamically (ThReaD). THREAD frames model generation as a thread of\nexecution that, based on the context, can run to completion or dynamically\nspawn new threads. By spawning, threads can offload work (e.g., thinking,\nretrieving information) to child threads, which only return tokens needed for\nthe parent thread to do its work. In effect, this enables the model to adapt,\nas needed, the amount of intermediate work used to produce tokens. We apply\nTHREAD in the settings of LLM task solving and question answering, where the\ndynamic threading allows the model to recursively decompose the given task or\nquestion into progressively simpler sub-problems that can be solved by separate\nchild threads. We test THREAD, implemented using a few-shot learning approach,\non diverse benchmarks for agent tasks and data-grounded question answering.\nTHREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these\nbenchmarks, including ALFWorld, TextCraft, and WebShop, along with two new\nbenchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD\noutperforms existing frameworks by 10% to 50% absolute points with smaller\nmodels, including Llama-3-8b and CodeLlama-7b.",
        "updated": "2024-05-27 17:51:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17402v1"
    },
    {
        "title": "The Expressive Capacity of State Space Models: A Formal Language Perspective",
        "authors": "Yash SarrofYana VeitsmanMichael Hahn",
        "links": "http://arxiv.org/abs/2405.17394v1",
        "entry_id": "http://arxiv.org/abs/2405.17394v1",
        "pdf_url": "http://arxiv.org/pdf/2405.17394v1",
        "summary": "Recently, recurrent models based on linear state space models (SSMs) have\nshown promising performance in language modeling (LM), competititve with\ntransformers. However, there is little understanding of the in-principle\nabilities of such models, which could provide useful guidance to the search for\nbetter LM architectures. We present a comprehensive theoretical study of the\ncapacity of such SSMs as it compares to that of transformers and traditional\nRNNs. We find that SSMs and transformers have overlapping but distinct\nstrengths. In star-free state tracking, SSMs implement straightforward and\nexact solutions to problems that transformers struggle to represent exactly.\nThey can also model bounded hierarchical structure with optimal memory even\nwithout simulating a stack. On the other hand, we identify a design choice in\ncurrent SSMs that limits their expressive power. We discuss implications for\nSSM and LM research, and verify results empirically on a recent SSM, Mamba.",
        "updated": "2024-05-27 17:46:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.17394v1"
    }
]