Knowing What Not to Do: Leverage Language Model
Insights for Action Space Pruning in Multi-agent
Reinforcement Learning
ZhihaoLiu1,4∗,XianliangYang2,ZichuanLiu3,YifanXia3,
WeiJiang5,YuanyuZhang6,LijuanLi1,GuoliangFan1,LeiSong2,JiangBian2†
1InstituteofAutomation,ChineseAcademyofSciences,2MicrosoftResearchAsia,
3NanjingUniversity,4SchoolofArtificialIntelligence,UniversityofChineseAcademyofSciences,
5UniversityofIllinoisUrbana-Champaign,6GuizhouUniversity
{liuzhihao2022, lijuan.li, guoliang.fan}@ia.ac.cn,
{zichuanliu, yfxia}@smail.nju.edu.cn,zhangyuanyu18@gmail.com
{xianya, lei.song, jiang.bian}@microsoft.com,weij4@illinois.edu
Abstract
Multi-agentreinforcementlearning(MARL)isemployedtodevelopautonomous
agentsthatcanlearntoadoptcooperativeorcompetitivestrategieswithincomplex
environments. However, the linear increase in the number of agents leads to a
combinatorialexplosionoftheactionspace,whichmayresultinalgorithmicinsta-
bility,difficultyinconvergence,orentrapmentinlocaloptima. Whileresearchers
havedesignedavarietyofeffectivealgorithmstocompresstheactionspace,these
methodsalsointroducenewchallenges,suchastheneedformanuallydesigned
priorknowledgeorrelianceonthestructureoftheproblem,whichdiminishesthe
applicabilityofthesetechniques. Inthispaper,weintroduceEvolutionaryaction
SPAceReductionwithKnowledge(eSpark),anexplorationfunctiongeneration
frameworkdrivenbylargelanguagemodels(LLMs)toboostexplorationandprune
unnecessaryactionsinMARL.Usingjustabasicpromptthatoutlinestheoverall
taskandsetting,eSparkiscapableofgeneratingexplorationfunctionsinazero-
shotmanner,identifyingandpruningredundantorirrelevantstate-actionpairs,and
thenachievingautonomousimprovementfrompolicyfeedback. Inreinforcement
learningtasksinvolvinginventorymanagementandtrafficlightcontrolencompass-
ingatotalof15scenarios,eSparkconsistentlyoutperformsthecombinedMARL
algorithminallscenarios,achievinganaverageperformancegainof34.4%and
9.9%inthetwotypesoftasksrespectively. Additionally,eSparkhasproventobe
capableofmanagingsituationswithalargenumberofagents,securinga29.7%
improvementinscalabilitychallengesthatfeaturedover500agents. Thecodecan
befoundinhttps://github.com/LiuZhihao2022/eSpark.git.
1 Introduction
Multi-agentreinforcementlearning(MARL)hasemergedasapowerfulparadigmforsolvingcomplex
anddynamicproblemsthatinvolvemultipledecision-makers[68,56]. However,theintricaciesof
agent interplay and the exponential expansion of state and action spaces render the solution of
MARLproblemsdifficult. ResearchershaveproposedtheCentralizedTrainingwithDecentralized
∗WorkdoneduringinternshipatMicrosoftResearchAsia.
†Correspondingauthor.
Preprint.Underreview.
4202
yaM
72
]AM.sc[
1v45861.5042:viXraExecution(CTDE)framework[43]andparametersharingmethods,decomposingthevalueorpolicy
functionsofamulti-agentsystemintoindividualagentsandsharingmodelparametersamongall
agents. As experimentally verified by many of the most prominent MARL algorithms such as
Multi-agentPPO(MAPPO)[64],QMIX[46],QPLEX[56]orQTRAN[53],thesemethodologies
have been demonstrated to be robust strategies for surmounting the challenges posed by MARL.
MARLmethodsbasedonparametersharingandCTDEhaveachievednotablesuccessinavarietyof
well-establishedtasks,includingStarCraftMulti-AgentChallenge(SMAC)[30,57],theMulti-Agent
ParticleEnvironment(MPE)[34],andSimulationofUrbanMObility(SUMO)[59,35].
Despitethegreatsuccessofparameter-sharingCTDEmethods,theirpracticalitydwindlesinreal-
worldtasksinvolvinglargenumberofagents,suchaslarge-scaletrafficsignalcontrol[38],wireless
communicationnetworks[70],andhumanitarianassistanceanddisasterresponse[37],wherecentral-
izedtrainingbecomesimpracticalduetolargeproblemscale[39]. FullyDecentralizedTrainingand
Execution(DTDE)methods,suchasIndependentPPO(IPPO)[13],offerascalablesolutionwhere
resourceconsumptiondoesnotescalatedrasticallywithanincreaseinthenumberofagents.However,
duetothelackofconsiderationforagentinteractions,theyoftenstruggletofindoptimalsolutionsand
fallintolocaloptima. Currentstrategiesforaddressinglarge-scaleMARLtasksinvolveintroducing
task-specificstructurestomodelagentinteractionsordividingagentsintosmaller,independently
trainedgroups[63,10]. Thesemethods,however,areconstrainedbytheirdependenceontask-related
structuring,limitingtheirapplicabilitytoanarrowrangeofproblems.
Additionally,the"curseofdimensionality"presentsasignificantchallengeinmulti-agentsystems[20,
19],whereagentsarerequiredtonavigatethroughanexpansiveactionspacesaturatedwithnumerous
actionsthatareeitherirrelevantormarkedlysuboptimal(relativetostates). Whilehumanscandeftly
employcontextualcuesandpriorknowledgetosidestepsuchchallenges,MARLalgorithmstypically
engageintheexplorationofsuperfluousandextraneoussuboptimalactions[66]. Besides,prevailing
parametersharingcanexacerbatethisexploratorydilemma,aswillbeelucidatedinProposition2.
Theissueoccursprimarilybecauseagentswithsharedparametersoftenprefersuboptimalpolicies
thatpresentshort-termadvantages,ratherthanexploringpoliciesthatmaypotentiallydeliverhigher
long-termreturns.
Explorationiscrucialforovercominglocaloptima,asitencouragesagentstodiscoverpotentially
betterstatesthusrefiningtheirpolicies. Whilesingle-agentexplorationtechniquesliketheUpper
ConfidenceBound(UCB)[4],entropyregularization[18],andcuriosity-basedexploration[17,45]
have shown promising results, they struggle with the escalated complexity in MARL scenarios,
compoundedbyissueslikedeceptiverewardsandthe"Noisy-TV"problem[7]. Integratingdomain
knowledgeintoexplorationcouldsignificantlyenhanceexplorationefficiency,byhelpingidentify
criticalelementsandproblemstructures,therebyaidingintheselectionofoptimalactions[52]. How-
ever,theintegrationofthisknowledgewithinadata-drivenframeworkposessignificantchallenges,
particularlywhenmanualinputfromdomainexpertsisrequired,thusreducingitspracticality.
Recently, Large Language Models (LLMs) such as GPT-4 [1] have shown formidable skills in
language comprehension, strategic planning, and logical reasoning across various tasks [61, 69].
Althoughnotalwaysdirectlysolvingcomplex,dynamicproblems,theirinferentialanderror-learning
abilities facilitate progressively better solutions through iterative feedback [36]. The integration
of LLMs with MARL presents a promising new avenue by facilitating exploration through the
pruningofredundantactions. Inthispaper,weintroduceEvolutionaryactionSPAceReductionwith
Knowledge(eSpark),anovelapproachthatutilizesLLMstoimproveMARLtrainingviaoptimized
explorationfunctions,whichareusedtoprunetheactionspace. eSparkbeginsbyusingLLMsto
generateexplorationfunctionsfromtaskdescriptionsandenvironmentalrulesinazero-shotfashion.
ItthenappliesevolutionarysearchwithinMARLtopinpointthebestperformingpolicy. Finally,by
analyzingthefeedbackontheperformanceofthispolicy,eSparkreflectsandproposesasetofnew
explorationfunctions,anditerativelyoptimizesthemaccordingtotheaforementionedsteps. This
processenhancestheMARLpolicybycontinuouslyadaptingandrefiningexploration.Tosummarize,
ourcontributionsareasfollows:
1. WeintroducetheeSparkframework,whichharnessestheintrinsicpriorknowledgeand
encodingcapabilityofLLMstodesignexplorationfunctionsforactionspacepruning,thus
guidingtheexplorationandlearningprocessofMARLalgorithms. eSparkrequiresno
complexpromptengineeringandcanbeeasilycombinedwithMARLalgorithms.
22. WevalidatetheperformanceofeSparkacross15differentenvironmentswithintheinven-
torymanagementtaskMABIM[60]andthetrafficsignalcontroltaskSUMO[5].Combined
withIPPO,eSparkoutperformsIPPOinallscenarios,realizinganaverageprofitincrease
of34.4%intheMABIMandimprovingmultiplemetricsinSUMObyanaverageof9.9%.
EveninthefaceofscalabilitychallengeswheretheDTDEmethodstypicallyencounter
limitations,eSparkelevatestheperformanceofIPPOby29.7%.
3. Weconductcontrolledexperimentsandablationstudiestoanalyzetheeffectivenessofeach
componentwithintheeSparkframework. Wefirstvalidatetheadvantagesofknowledge-
basedpruning. Subsequently,weconductablationstudiestodemonstratethatbothretention
training and LLM pruning techniques contribute to the performance of eSpark. These
effectsareevenmorepronouncedinthemorecomplexMABIMenvironment.
2 Relatedworks
LLMsforcodegeneration. LLMshaveexhibitedformidablecapabilitiesinthedomainofcode
generation[49,42]. Morerecently,LLM-basedapproachestoself-improvingcodegenerationhave
beenappliedtoaddresschallengesincombinatorialoptimization[33,62], robotictasks[31,58],
reinforcementlearning(RL)rewarddesign[36],andcodeoptimization[48]. Forthefirsttime,tothe
bestofourknowledge,weproposetheuseofLLMstogeneratecodewiththeaimofpruningthe
redundantactionspaceinMARLenvironments,andthroughaprocessofautonomousreflectionand
evolution,iterativelyenhancingthequalityofthegeneratedoutput.
LLMsforRLandMARL.TheintegrationofLLMsintoRLandMARLhassparkedconsiderable
research interest [50, 27, 29]. Some works [21, 8] incorporate the goal descriptions of language
modelsthathelpinenhancingthegeneralizationcapabilitiesofagentsdesignedtofollowinstructions.
Furtherstudieshaveextendedthisapproachtocomplextasksinvolvingreasoningandplanning[24,
23]. Moreover,LLMshavebeenemployedtoguideexplorationandboostRLefficiency[14,9,22].
However,scalingtohigh-complexity,real-time,multi-agentsettingsremainsachallenge.Ourmethod
mitigatesthisbygeneratingexplorationfunctionstonavigatethepolicyspace,thusfacilitatingthe
applicationtocomplexMARLscenarioswithoutdirectLLM-agentdecision-makinginteraction.
ActionspacepruninginRLandMARL.Pruningtheactionspacehasbeenshowntobeeffective
in guiding agent behaviors in complex environments [32, 16]. Techniques include learning an
eliminationsignaltodiscardunnecessaryactions[66],andemployingtransferlearningthatpre-trains
agentstoisolateusefulexperiencesforlateractionrefinement[51,28,2]. Someapproachesuse
manuallydesigneddatastructuresbasedonpriorknowledgetofilteractions[15,44,40]. However,
trainingpruningsignalsorapplyingtransferlearningisinherentlydifficultwithmanyagents,and
theneedforexpertknowledgeinmanualpruningruleshamperstheirtransferability,limitingthe
applicabilityofcurrentmethods. WeharnesstheabundantknowledgeembeddedwithinLLMsfor
actionspacepruning,demonstratinguniversalapplicabilityacrossamultitudeofscenarios.
3 Preliminaries
3.1 Problemformulationandnotations
Markovgameframework. Inourstudy,weexploreaMarkovgameframework,formallydefined
bythetuple⟨N,S,O,A,P,R,γ⟩. HereN representsthetotalnumberofparticipatingagents,S
denotesawell-definedstatespace,andO =(cid:81)N Ok constitutesthecombinedobservationspace,
k=1
A=(cid:81)N Ak isthejointactionspaceforallagentsinvolved. Thetransitiondynamicsarecaptured
k=1
bytheprobabilityfunctionP : S ×A×S → [0,1],therewardfunctionR : S ×A → Rmaps
state-actionpairstoreal-valuedrewards. Thediscountfactorisdenotedbyγ ∈[0,1].
Ateachdiscretetimestept,theenvironmentisinstates ∈S. Eachagentk ∈[1,2,...,N]receives
t
anobservationok ∈Ok anddrawsanactionfromak ∼πk(·|ok),whereπk :Ok×Ak →[0,1]
t t t
denotes the policy of agent k, and (cid:80) πk(ak | ok) = 1. The joint actions of all agents
ak∈Ak t
a = (a1,a2,...,aN)isdrawnfromthejointpolicyπ(· | s ) = (cid:81)N πk(· | ok). Subsequently,
t t t t t k=1 t
arewardr = R(s ,a )isgivenbasedonthecurrentstateandjointaction. Thestatetransitionis
t t t
determinedbys ∼P(·|s ,a ).
t+1 t t
3Inthispaper,wefocusonafullycooperativescenariowhereallagentsshareacommonrewardsignal.
Thecollectiveobjectiveistomaximizetheexpectedcumulativereward,startingfromaninitialstate
distributionρ0. Thiscollaborativeapproachemphasizesthealignmentofindividualagentstrategies
towardsmaximizingaunifiedrewardJ(π):
(cid:34) ∞ (cid:35)
(cid:88)
J(π)=E γtr .
s0∼ρ0,a0:∞∼π,s1:∞∼P t
t=0
Policywithexplorationfunction. Intheconfigurationofourstudy,weintroducetheexploration
functionE :Ok×Ak →{0,1},indicatingwhetheranactionisselectablebyagentk. Foragiven
policyπk ofagentkandanexplorationfunctionE,wedefineanewpolicyguidedbyE,denotedby
πk,asfollows:
E
πk(·|ok)·E(ok,·)
πk(·|ok)= t t
E t (cid:80) πk(ak |ok)·E(ok,ak)
ak∈Ak t t
if(cid:80) πk(ak |ok)·E(ok,ak)>0;otherwise,πk(·|ok)=πk(·|ok). Consequently,thejoint
ak∈Ak t t E t t
policyforallagentsundertheguidanceofE isdefinedas:
N
(cid:89)
π (·|s )= πk(·|ok).
E t E t
k=1
Wedefinethesetofalljointpoliciesas{π}andthesetofallexplorationfunctionsas{E}.Let{π }
E
denotethesetofjointpolicieswhensubjectedtoanexplorationfunctionE ∈{E}. Anexploration
functionE isnon-trivialifitassignsazeroprobabilitytoatleastoneobservation-actionpair. The
followingpropositionnaturallyarisesfromthedefinition:
Proposition1.
1. ForanyE ∈{E},{π }⊆{π}. IfE isnon-trivial,then{π }⊂{π}.
E E
2. Foranyπ ∈{π},thereexistsanon-trivialE ∈{E}suchthatJ(π )≥J(π).
E
An intelligent choice of exploration functions does not diminish our ability to discover optimal
policies; instead, it allows us to refine the policy space, thereby enhancing the efficiency of the
learningprocess. TheproofofthispropositioncanbefoundinAppendixA.
3.2 Challengesandmotivations
Theintricaterelationshipsamongmultipleagentsmakeitextremelydifficulttosearchfortheoptimal
solutioninMARL.Withoutpowerfulexplorationmethods,itisnearlyimpossibletoavoidsuboptimal
outcomes. Wewillelaborateonthiswithanexamplefromthefollowingproposition:
Proposition2. Let’sconsiderafullycooperativegamewithNagents,onestate,andthejointaction
space{0,1}N, wheretherewardisgivenbyr(00,1N) = r andr(0N−m,1m) = −mr forall
1 2
m̸=N,r ,r arepositiverealnumbers. Wesupposetheinitialpolicyisrandomlyanduniformly
1 2
initialized,andthepolicyisoptimizedintheformofgradientdescent. Letpbetheprobabilitythat
thesharedpolicyconvergestothebestpolicy,then:
(cid:114)
r
p=1− N−1 2 .
r +Nr
1 2
Detailed proof is provided in Appendix A. In the above example, we show that the increase in
thenumberofagentsmakesitmoredifficultforMARLalgorithmstoreachtheoptimalsolution.
However,basedontheproblemcontext,humanscanunderstandproblemsfromahigh-levelsemantic
perspective,andquicklyfindoptimalsolutions. AsLLMshavedemonstratedsurprisingabilitiesin
semanticunderstanding,reasoning,andplanningacrossvarioustasks[65,55],weconductasimple
experimenttotestGPT-4’scapabilityfortheissueinProposition2,andhereisGPT-4’sresponse:
4In a fully cooperative game, all agents work together to maximize the total reward.
There are two distinct reward conditions:
1. When all agents choose action 1, the reward is r , a positive real number.
1
2. When there is any number of agents m (where 0 < m < N) choosing action 1,
the reward is −mr , where r is a positive real number.
2 2
All agents should act in a way that avoids the negative reward scenario. The
negative reward scenario happens anytime there is a mix of 0’s and 1’s in the action
space, which means some agents are choosing 1 and others are choosing 0. Therefore,
the optimal joint action for all agents is to all choose 1.
GPT-4exhibitsreasoningabilitiesonparwiththoseofhumansanddirectlysolvestheproblemin
Proposition 2. Propositions 1 has already shown that an intelligent exploration function can not
onlyreducethesearchingspacebutalsoimprovethefinalperformance. Thismakesusthinkabout
applyingthepowerfulLLMstoprunetheredundantactionspaceandtherebyguidetheexploration
inMARL.Inthefollowingsections,weproposetheeSparkframework,whichintegratestheprior
knowledgeandinferentialcapabilityofLLMstoboosttheexplorationinMARL.
4 Method
Inthissection,weintroduceanovelframework,eSpark,whichintegratesrobustpriorknowledge
encapsulated in LLMs. It improves iteratively through a cycle of trial and error, leveraging the
capability of LLMs. Figure 1 illustrates the overall training procedure. eSpark is composed of
threecomponents: (i)zero-shotgenerationofexplorationfunctions,(ii)evolutionarysearchforbest
performingMARLpolicy,and(iii)detailedfeedbackontheperformanceofthepolicytoimprove
thegenerationofexplorationfunctions. WeshowthepseudocodeofeSparkinAppendixB.
4.1 Explorationfunctiongeneration
LLMshavebeendemonstratedtopossessexceptionalcapabilitiesinbothcodecomprehensionand
generation. To this end, we employ a LLM as LLM code generator, denoted as LLM , whose
g
taskistounderstandtheobjectivesofthecurrentenvironment,andoutputanexplorationfunction:
E ,...,E ∼LLM (prom), (1)
1 K g 1. Explorationfunction generation 3. Reflection
Exploration
where prom is the prompt for function 1
LLM g,andthegenerationofK ex- fE ux np cl to ir oa nt i 2on Generate LLMc
ploration functions is to circum-
vent the suboptimality that may
Exploration
arise from single-sample genera- function K LLMg
tion. Theinitialpromincludesan
2. Evolutionary search
RL formulation describing the re-
ward system, state items, transi- Exploration Performance 1
functions
tions,andtheactionspace,along- Feedback 1
s fii ed se ta heta ts ak skde os bc jr ei cp tt ii vo en s,th ea xt ps ep ce tec di- state Binary action mask Per (f bo er sm ta )nce i Feedback
Feedback i
outputs,andformattingrules. De- state
tails on the initial prom are pro- Performance K
action
videdinAppendixG.Weusecode Environment RL policy Feedback K
fortheRLformulationasiteffec-
tivelycapturesthephysicaltransi-
tiondynamicscrucialtoRLprob-
Figure1: eSparkfirstlygeneratesK explorationfunctionsvia
lems,whicharealwaysdifficultto
zero-shotcreation. Eachexplorationfunctionisthenusedto
expresspreciselythroughthetext
guide an independent policy, and the evolutionary search is
alone, especially when environ-
performedtofindthebest-performingpolicy. Finally,eSpark
mentalcomplexityincreases.Code
reflectsonthefeedbackfromthebestperformancepolicy,re-
contextsalsoimprovecodegener-
fines, and regenerates the exploration functions for the next
ationandclarifyenvironmentalse-
iteration.
manticsandvariableroles[36].
5
…
…
… …
…Duringthecodegeneration,however,LLM mayincorrectlyinterpretvariablesandproducelogically
g
flawed code. This kind of flawed logic could persist if it is added to the prompt context for the
nextgeneration. AsresearchhasshownthatcollaborationamongmultipleLLMscanenhancethe
qualityandefficacyofthegeneratedcontents[11,67],weintroducetheLLMcheckerdenotedas
LLM ,whichreviewsLLM ’soutputtopursueanenhancedgeneration. LLM usesthesamepromptas
c g c
LLM butispromptedtofocusonverifyingtheaccuracyofcoderelativetoenvironmentaltransitions
g
andvariablespecifications. Ifinconsistenciesarefound,LLM signalstheerror,promptingLLM to
c g
regeneratethecode. Finally,explorationfunctionsaregeneratedby:
E ,...,E ∼LLM (prom,LLM (prom)). (2)
1 K c g
ExplorationfunctionsareappliedonlyduringthetrainingphasetoguidetheexplorationofMARL.
Duringtheexecutionphase,allexplorationfunctionsareremoved.
4.2 Evolutionarysearch
Duringthegeneration,however,itshouldbenotedthattheinitiallygeneratedexplorationfunction
may not always guarantee executability and effectiveness. To address this, eSpark performs an
evolutionarysearchparadigmthatselectsthebest-performingpolicyinoneiterationandusesits
feedbackforsubsequentgeneration[36]. Specifically,eSparksamplesabatchofK exploration
functionsineachgenerationtoensurethereareenoughcandidatesforsuccessfulcodeexecution.
Performanceisassessedatregularcheckpointswithinaniteration,withthefinalevaluationbasedon
theaverageofthelastfewcheckpoints. Thepolicyachievingthehighestperformanceisselected,
andthefeedbackobtainedfromthispolicyisintegratedtooptimizetheexplorationfunctionsinthe
followingsteps.
Duetothedynamicnatureofexploration,theexplorationfunctiongeneratedbasedonfeedbackfrom
thebest-performingpolicymaynotbeapplicabletootherpolicies. AstheproofofProposition1
demonstrates,whenanexplorationfunctionisincapableofintelligentlypruning,itmayevenimpair
theperformanceofthepolicy. Tothisend,weutilizeretentiontrainingtomaintaincontinuityof
exploration. Letϕi−1 representbest-performingpolicyfromthe(i−1)-thiteration. Forthei-th
best
iterationexceptforthefirst,atthestartoftheiteration,weset:
ϕi,ϕi,...,ϕi ←ϕi−1. (3)
1 2 K best
Thisallowsustomatchexplorationfunctionswiththeircorrespondingpolicies,subsequentlyrefining
performanceincrementally. WewillverifytheimpactofretentiontraininginSection5.4.
4.3 Reflectionandfeedback
Feedback from the environment can significantly enhance the quality of the generated output by
LLMs[41,14]. IneSpark,weleveragepolicyfeedback,whichcontainstheevaluationofpolicy
performance from various aspects, to enhance the generation of LLMs. This policy feedback
mayeithercomefromexpertsorbeautomaticallyconstructedfromtheenvironment,aslongasit
encompassesinsightsintotheaspectswherethecurrentalgorithmperformswellandareaswhere
it requires improvement. As illustrated in Equation 4, by correlating the best-performing policy
feedbackF andthemosteffectiveexplorationfunctionE ,LLMsintrospect,updatetheprompt
best best
prom,andgearupfortheensuingevolutionarycycle.
prom←prom:Reflection(E ,F ). (4)
best best
Inourexperiments,wegenerateautomatedpolicyfeedbackfromenvironmentalrewardsignals,as
domainexpertsinrelevantfieldsarenotavailable. Weacknowledgethatobtainingfeedbackfrom
humanexpertscanbeexpensive. Nevertheless,itisimportanttonotethatwithinourframework,
thenumberofroundsforfeedbackcollectionisspecifiedbyapredefinedhyperparameter,whichis
typicallykeptlow(inourexperiments,itissetto10). Therefore,inscenarioswherehumanexperts
areaccessible,incorporatingtheirinsightsisfeasibleandcanpotentiallyenhanceperformance.
65 Experiments
5.1 Experimentsettings
ForacomprehensiveevaluationofeSpark’scapabilities,weperformdetailedvalidationswithintwo
distinctindustrialenvironments: theinventorymanagementenvironmentMABIMandthetraffic
signalcontrolenvironmentSUMO.
• MABIMsetting: MABIMsimulatesmulti-echeloninventorymanagementbymodeling
each stock-keeping unit (SKU) as an agent, mirroring real-world operations and profits
withintheMARLframework. Thetotalrewardiscomposedofmultiplerewardcomponents.
Weutilizethetotalrewardtoidentifythebest-performingpolicy,whilethosecomponents
evaluatethepolicy’smultifacetedperformancetogeneratepolicyfeedback. Wefocuson
threekeychallengeswithininventorymanagement: multipleechelons,capacityconstraints
andscalability,selectingcorrespondingscenariosforexperiments.
• SUMOsetting: SUMOisatrafficsignalcontrolenvironmentinwhicheachintersectionis
representedasanagent. Itoffersavarietyofrewardfunctions,andweuse"thenumberof
stoppedvehicles"astherewardforevolutionarysearch,whileotherrewardsareforpolicy
feedback. TheAverageDelay,AverageTripTime,andAverageWaitingTimemetricsare
chosenforevaluation[35]. WeemployGESA[25]tostandardizeintersectionsinto4-arm
configurations. Eachsimulationspans3600seconds,withdecisionsat15-secondintervals.
• Modelsetting: WeuseIPPOasthebaseMARLframeworkforeSparkduetoitsDTDE
structure,whichissuitableforlarge-scalechallenges. Butnotethatourapproachcanalso
beappliedasaplugininotherMARLmethods. WeselectGPT-4fortheLLM andLLM
c g
duetoitssuperiorcomprehensionandgenerationabilities. Foreachscenario,weconduct
threerunswithabatchsizeofK =16,trainingfor10iterations.
AlltrainingjobsareexecutedwithanIntel(R)Xeon(R)Gold6348CPUand4NVIDIARTXA6000
GPUs. InAppendixC,weprovideadetailedintroductionandsettingfortheenvironmentsandmodel.
InAppendixD,wegivehyperparameterconfigurationsanddescriptionsofeachbaselinemethod.
5.2 Experimentresults
Inthissection,wepresentthekeyfindingsforeSparkintheMABIMandSUMO,highlightingthe
bestandsecondbestresultsinboldandunderline. MoredetailedresultsarepresentedinAppendixE.
5.2.1 PerformanceonMABIM
Table1showstheresultsinthe100SKUsscenariosintermsofcapabilityconstraintsandmultiple
echelonchallenges. WithIPPOasthebaseMARLalgorithm,eSparknotonlyoutperformsIPPOin
allscenariosbutalsoexceedstheperformanceofallcomparedbaselinesin4outof5scenarios. For
anin-depthanalysis,wediscussthepolicydifferencesbetweenIPPOandeSparkinAppendixF,
along with eSpark’s reflective mechanism and exploration function adjustments in Appendix H.
WhileIPPOstrugglestolearntheintricateinterplayamongSKUs,eSparkexcelsparticularlyin
navigating cooperation among SKUs and refining its search in a broad space, leading to marked
improvementsinmanagingcapacityconstraintsandmulti-echeloncoordination.
InTable1,wealsopresenttheperformanceoutcomesoftheeSparkalgorithminthescaling-up
500SKUsscenarios. DuetothecentralizednatureoftheCTDEmethods,theystruggletoscaleto
large-scaleproblemsandthereforearenotpresentedinthetable. DespiteIPPO’smarkedlyinferior
performanceonscenarioswhenproblemsscaleup,eSparkexhibitssignificantenhancementsand
consistentlyachievesoptimalresultsacrossmultiplescenarios. Weattributethisimprovementto
eSpark’sactionspacepruningstrategy,whicheffectivelyaddressestheheightenedexplorationneeds
inscenarioswithmanyagents,providingaclearadvantageinsuchcomplexenvironments.
5.2.2 PerformanceonSUMO
To further assess eSpark’s capabilities across different tasks, we have compiled a summary of
resultsinTable2basedontheSUMOenvironment. SimilartotheoutcomesinMABIM,eSpark
consistentlyenhancestheperformanceoftheIPPOinallscenarios,andithasoutperformedtheCTDE
7Table1: PerformanceinMABIM,ahigherprofitindicatesabetterperformance. The"Standard"
scenariofeaturesasingleechelonwithsufficientcapacity. The"2/3echelons"involveschallengesof
multi-echeloncooperation. The"Lower/Lowest"scenariosarethechallengeswhereSKUscompete
forinsufficientcapacity, while"500SKUsscenarios"assessscalability. The‘-’indicatesCTDE
algorithmsarenotresearchedinthescalabilitychallenges.
Avg.profits(K)
Method 100SKUsscenarios 500SKUsscenarios
Standard 2echelons 3echelons Lower Lowest Standard 2echelons 3echelons Lower Lowest
IPPO 690.6 1412.5 1502.9 431.1 287.6 3021.2 7052.0 7945.7 3535.9 2347.4
QTRAN 529.6 1595.3 2012.2 70.1 19.5 - - - - -
QPLEX 358.9 1580.7 704.2 379.8 259.3 - - - - -
MAPPO 719.8 1513.8 1905.4 478.3 265.8 - - - - -
BSstatic 563.7 1666.6 2338.9 390.7 -1757.5 3818.5 8151.2 11926.3 3115.1 -9063.8
BSdynamic 684.2 1554.2 2378.2 660.6 -97.1 4015.7 8399.3 11611.1 3957.5 2008.6
(S,s) 737.8 1660.8 1725.2 556.9 203.7 4439.4 9952.1 10935.7 3769.3 2212.4
eSpark 823.7 1811.4 2598.7 579.5 405.0 4468.6 9437.3 12134.2 3775.7 2519.5
baselinesaswellasdomain-specificMARLbaselinestoachievethebestperformance. Notably,even
whenIPPOaloneiscapableofgoodresults(asseeninscenariossuchasGrid4×4andCologne8),
thepruningmethoddesignedineSparkdoesnotcompromisetheeffectivenessofIPPO.Wewill
delvefurtherintotheanalysisofexplorationfunctionsproducedbyeSparkinSection5.3.
Table2: PerformanceinSUMO,includingthemeanandstandarddeviation(inparentheses). Alower
timeindicatesabetterperformance.
Avg.delay(seconds) Avg.triptime(seconds)
Method
Grid4×4 Arterial4×4 Grid5×5 Cologne8 Ingolstadt21 Grid4×4 Arterial4×4 Grid5×5 Cologne8 Ingolstadt21
FTC 161.14(3.77) 1229.68(16.79) 820.88(24.36) 85.27(1.21) 224.96(11.91) 291.48(4.45) 676.77(13.70) 584.54(24.17) 145.93(0.84) 352.06(9.29)
MaxPressure 63.39(1.34) 995.23(77.02) 242.85(16.04) 31.63(0.61) 228.64(15.83) 174.68(2.05) 702.09(23.61) 269.35(9.62) 95.67(0.62) 352.30(15.06)
IPPO 48.40(0.45) 884.73(38.94) 228.78(11.59) 27.60(1.70) 342.97(43.61) 160.12(0.60) 506.18(10.39) 238.03(7.10) 91.41(1.60) 464.50(43.30)
MAPPO 51.25(0.58) 958.43(181.72) 958.43(181.72) 32.55(4.66) 347.59(47.59) 160.01(0.54) 757.40(242.00) 247.56(3.71) 94.31(1.77) 480.66(49.46)
CoLight 53.40(1.89) 820.67(58.65) 339.66(48.55) 27.48(1.30) 296.47(106.82) 165.77(1.89) 470.33(12.34) 305.41(44.43) 91.66(1.29) 410.59(97.29)
MPLight 63.51(0.64) 1142.98(79.65) 223.44(16.18) 37.93(0.45) 196.74(9.88) 172.47(0.89) 583.21(45.84) 255.49(6.26) 110.56(1.18) 331.42(11.79)
eSpark 48.36(0.32) 854.22(68.21) 209.49(13.98) 25.39(1.27) 243.92(15.81) 159.74(0.44) 484.87(58.21) 235.20(6.80) 89.50(1.36) 367.57(15.03)
5.3 eSparklearnsintelligentpruningmethods
GiventhateSparkemploysthepriorknowledgeoftheLLMstocraftitsexplorationfunction,our
study aimed to investigate two critical aspects: (1) the validity of action space pruning via prior
knowledge,and(2)thepotentialadvantagesofthismethodoverrule-basedheuristicpruning.
Toaddressthequestionsraised,wedevisetwopruningstrategies. First,weimplementarandom
pruningmethod,whereinagentsrandomlyexcludeaportionofactionsduringdecision-makingto
testthevalidityofknowledge-basedpruning. Secondly,weutilizedomain-relatedORalgorithms
toimplementheuristicpruningmethods. ForMABIM,actionsareprunedusingthe(S,s)policy
andunboundlimit,whileforSUMO,pruningreliesonMaxPressuretokeeponlyafewactionswith
thehighestpressure. ThedetailsofthesemethodsarepresentedinAppendixD.3. JustlikeeSpark,
thesepruningstrategiesareintegratedwithIPPOduringtrainingbutnotexecution. Weconducted
experimentsunderthesamesettinginSection5.1,withresultspresentedinTables3andTable4.
Table 3: Average performance changes on Table 4: Average performance changes on
MABIM.AllchangesarerelativetoIPPO. SUMO.AllchangesarerelativetoIPPO.
Avg.profitschangeratio(%) Avg.timechangeratio(%)
Method Method
100SKUs 500SKUs Delay Triptime Waittime
Randompruning 2.1 -0.5 Randompruning -0.1 2.2 -2.5
(S,s)pruning -25.9 15.5 MaxPressurepruning -0.5 1.5 -0.1
Upboundpruning -23.2 -32.7 eSpark -9.7 -5.7 -14.3
eSpark 39.1 29.7
Asshowninthetables,randompruningmarginallyaffectsperformancebymerelyalteringexploration
rateswithoutprovidingnewinsights. Heuristicpruning’simpactvarieswithitsdesignandcontext. In
MABIM,(S,s)pruningislesseffectiveinthe100SKUsscenario,asitrestrictsthealreadyeffective
IPPO’sexplorationinsmallerscales.However,itprovesbeneficialinthe500SKUsscenario,whereit
guidestheexplorationandleadstobetterresults.Upboundpruningconsistentlyunderperformsdueto
8itsoverlysimplisticheuristic. ForSUMO,pressure-basedpruningdoesnotoffersignificantbenefits.
Nevertheless,eSparkdemonstratesremarkableadaptabilityacrossalltestingtasks,adeptlyselecting
pruningmethodsthatsubstantiallyenhanceresults. Itsknowledge-basedgenerativetechniqueand
evolutioncapabilityenableittomasterintelligentpruningstrategies.
IPPO
Random pruning 1e-1
(S,s) pruning
Upbound pruning
1e-2
eSpark
0 0.5 1 2 3 6 12 1e-3
(mean demand)
Actions
Figure2: ActionselectionfrequencyforIPPOandvariouspruningmethodsonthe100SKUsLowest
scenario. "Actions"representsthereplenishmentquantityisamultipleofthemeandemandwithin
theslidingwindow. eSparklearnsnotonlytominimizerestockingbutalsotodiversifywithsmall
purchasesbelowthemeandemand,balancingdemandfulfillmentandoverflowprevention.
Figure2presentsafrequencyheatmapofactionselectionforIPPOandvariouspruningmethods
in the 100 SKUs Lowest scenario. IPPO learns a minimally restocking strategy, risking unmet
demand. RandompruningchoosesactionsmoreuniformlyyetmirrorsIPPO’spattern. (S,s)pruning
excessivelyexceedsmeandemand,ignoringno-restockactionsandleadingtosignificantoverflow.
Upboundpruningtypicallyavoidsrestocking,butpreferstopurchasenearthemeandemand,which
couldresultinoverflowcosts. Incontrast,eSparkadoptsabalancedpolicy,avoidingoverstocking
whilediversifyingitsminorrestockingstrategiestomeetdemandwithoutcausingoverflow.
5.4 eSparkbenefitsfromretentiontrainingandactionspacereduction
ExtensiveresearchhasunderscoredtheimportanceofreflectioninLLM-drivencontentgeneration[36,
41].Herein,wefocusontheeffectsofretentiontrainingandactionpruningoneSpark’sperformance.
Wefirstdesignanablationexperiment,whichwerefertoastheeSparkw/oretention. Themodel
parametersareinitializedwhenaniterationisfinished,andthenewlygeneratedexplorationfunctions
areequipped,afterwhichthetrainingstartsfromscratch. Giventhattheinitializedmodelneedsa
moreextensivenumberofstepstoconverge,weaccordinglytriplethetrainingstepsperiterationin
comparisontothestandardeSpark. Anotherablationretainstheretentiontraining,whiletheonly
differenceisthattheLLMsandreflectionareremoved. WenamethisexperimenteSparkw/oLLM.
ThecomparativeanalysisofthesetwoablationsisdelineatedinTable5andTable6.
Table5: Averageperformancechangeacross Table6: Averageperformancechangeinthe
100SKUscenariosintheMABIMenviron- SUMOenvironment. Allchangesarerelative
ment. AllchangesarerelativetoIPPO. toIPPO.
Method Avg.profitschangeratio(%) Method Avg.timechangeratio(%)
Delay Triptime Waittime
eSpark 39.1
eSparkw/oretention 24.0 eSpark -9.7 -5.7 -14.3
eSparkw/oLLM -2.8 eSparkw/oretention -9.6 -4.6 -11.2
eSparkw/oLLM -9.1 -5.0 -12.8
TheremovalofretentiontrainingandLLMsbothresultinadeclineintheperformanceoftheeSpark.
IntheSUMOscenario,theperformancegapbetweenthetwoablationsandthecompleteeSparkis
relativelysmall,whereasitismorepronouncedintheMABIMscenarios. Thiscanbeattributedto
thefactthatMABIMinvolvesagreaternumberofagentsandamorecomplexobservationspace
actionspace,whereasuperiorpruningcansignificantlyenhancetheperformanceofMARLmethods.
Additionally,weobservethatthelackofLLMsleadstoasignificantdecreaseinperformanceon
MABIM,emphasizingthecentralroleofknowledge-basedactionspacepruningwithintheeSpark.
6 Conclusions,limitationsandfuturework
WepresenteSpark,anovelframeworkforgeneratingexplorationfunctions,leveragingtheadvanced
capabilitiesofLLMstointegratepriorknowledge,generatecodeandreflect,therebyrefiningthe
explorationinMARL.eSparkhassurpasseditsbaseMARLalgorithmacrossallscenariosinboth
MABIM and SUMO environments. In terms of pruning strategies, pruning based on the prior
9
sdohteM
ycneuqerFknowledgefromLLMsoutshinesbothrandomandheuristicapproaches. Ablationstudiesconfirm
theindispensablerolesofretentiontrainingandactionspacereductiontoeSpark’ssuccess.
Nevertheless,eSparkalsohascertainlimitations. First,currentlyeSparkisonlyapplicabletotasks
involvinghomogeneousagents. Forheterogeneousagents,apotentialmethodcouldbetogenerate
distinct exploration functions for each agent; however, this approach becomes impractical when
thenumberofagentsistoolarge. Moreover, eSparkbenefitsfrompolicyfeedbacktorefinethe
explorationfunctions. Whenfeedbackisnotinformativeregardinghowtomodifytheexploration
(e.g.,intaskswithsparserewards,end-of-episodefeedbackaloneistoolimitedtodevelopautomated
feedback),eSparkmaystruggletoimproveandneedextraexpertinputforeffectivereflection.
Futureworkencompassesnumerouspotentialdirections. Existingresearchadvocatesforassigning
differentrolesorcategoriestoagents[12,57],whichcouldofferacompromisefortheapplication
of eSpark in heterogeneous multi-agent systems. Furthermore, state-specific feedback for more
granularimprovementrepresentsanintriguingavenue[54]. Ourfutureendeavorswillinvestigate
thesequestions,strivingtodevelopalgorithmsthatarerobustandexhibitstronggeneralizability.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXiv
preprintarXiv:2303.08774,2023.
[2] PrithvirajAmmanabroluandMarkORiedl. Playingtext-adventuregameswithgraph-baseddeeprein-
forcementlearning. arXivpreprintarXiv:1812.01628,2018.
[3] KennethJArrow,TheodoreHarris,andJacobMarschak.Optimalinventorypolicy.Econometrica:Journal
oftheEconometricSociety,pages250–272,1951.
[4] PeterAuer. Usingconfidenceboundsforexploitation-explorationtrade-offs. JournalofMachineLearning
Research,pages397–422,2002.
[5] MichaelBehrisch,LauraBieker,JakobErdmann,andDanielKrajzewicz. Sumo–simulationofurban
mobility:anoverview. InSIMUL,2011.
[6] AlanSBlinder. Inventorytheoryandconsumerbehavior. HarvesterWheatsheaf,1990.
[7] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. InICLR,pages1–17,2018.
[8] HarrisChan,YuhuaiWu,JamieKiros,SanjaFidler,andJimmyBa. Actrce:Augmentingexperiencevia
teacher’sadviceformulti-goalreinforcementlearning. arXivpreprintarXiv:1902.04546,2019.
[9] JonathanDChang,KianteBrantley,RajkumarRamamurthy,DipendraMisra,andWenSun. Learningto
generatebetterthanyourllm. arXivpreprintarXiv:2306.11816,2023.
[10] ChachaChen,HuaWei,NanXu,GuanjieZheng,MingYang,YuanhaoXiong,KaiXu,andZhenhuiLi.
Towardathousandlights:Decentralizeddeepreinforcementlearningforlarge-scaletrafficsignalcontrol.
InAAAI,pages3414–3421,2020.
[11] WeizeChen,YushengSu,JingweiZuo,ChengYang,ChenfeiYuan,ChenQian,Chi-MinChan,YujiaQin,
YaxiLu,RuobingXie,etal. Agentverse:Facilitatingmulti-agentcollaborationandexploringemergent
behaviorsinagents. arXivpreprintarXiv:2308.10848,2023.
[12] FilipposChristianos,GeorgiosPapoudakis,MuhammadARahman,andStefanoVAlbrecht. Scaling
multi-agentreinforcementlearningwithselectiveparametersharing. InICML,pages1989–1998,2021.
[13] ChristianSchroederdeWitt,TarunGupta,DenysMakoviichuk,ViktorMakoviychuk,PhilipHSTorr,
MingfeiSun,andShimonWhiteson. Isindependentlearningallyouneedinthestarcraftmulti-agent
challenge? arXivpreprintarXiv:2011.09533,2020.
[14] YuqingDu,OliviaWatkins,ZihanWang,CédricColas,TrevorDarrell,PieterAbbeel,AbhishekGupta,
andJacobAndreas. Guidingpretraininginreinforcementlearningwithlargelanguagemodels. InICML,
pages8657–8677,2023.
[15] GabrielDulac-Arnold,RichardEvans,HadovanHasselt,PeterSunehag,TimothyLillicrap,JonathanHunt,
TimothyMann,TheophaneWeber,ThomasDegris,andBenCoppin. Deepreinforcementlearninginlarge
discreteactionspaces. arXivpreprintarXiv:1512.07679,2015.
[16] NancyFulda,DanielRicks,BenMurdoch,andDavidWingate. Whatcanyoudowitharock?affordance
extractionviawordembeddings. arXivpreprintarXiv:1703.03429,2017.
10[17] OliverGroth,MarkusWulfmeier,GiuliaVezzani,VibhavariDasagi,TimHertweck,RolandHafner,Nicolas
Heess,andMartinRiedmiller. Iscuriosityallyouneed?ontheutilityofemergentbehavioursfromcurious
exploration. arXivpreprintarXiv:2109.08603,2021.
[18] TuomasHaarnoja,AurickZhou,PieterAbbeel,andSergeyLevine. Softactor-critic:Off-policymaximum
entropydeepreinforcementlearningwithastochasticactor. InICML,pages1861–1870,2018.
[19] JianyeHao,XiaotianHao,HangyuMao,WeixunWang,YaodongYang,DongLi,YanZheng,andZhen
Wang. Boostingmultiagentreinforcementlearningviapermutationinvariantandpermutationequivariant
networks. InICLR,2022.
[20] Xiaotian Hao, Hangyu Mao, Weixun Wang, Yaodong Yang, Dong Li, Yan Zheng, Zhen Wang, and
JianyeHao. Breakingthecurseofdimensionalityinmultiagentstatespace:Aunifiedagentpermutation
framework. arXivpreprintarXiv:2203.05285,2022.
[21] Felix Hill, Sona Mokra, Nathaniel Wong, and Tim Harley. Human instruction-following with deep
reinforcementlearningviatransfer-learningfromtext. arXivpreprintarXiv:2005.09382,2020.
[22] HengyuanHuandDorsaSadigh. Languageinstructedreinforcementlearningforhuman-aicoordination.
InICML,pages13584–13598,2023.
[23] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot
planners:Extractingactionableknowledgeforembodiedagents. InICML,pages9118–9147,2022.
[24] WenlongHuang, FeiXia, TedXiao, HarrisChan, JackyLiang, PeteFlorence, AndyZeng, Jonathan
Tompson,IgorMordatch,YevgenChebotar,etal.Innermonologue:Embodiedreasoningthroughplanning
withlanguagemodels. InCoRL,pages1769–1782,2023.
[25] HaoyuanJiang,ZiyueLi,ZhishuaiLi,LeiBai,HangyuMao,WolfgangKetter,andRuiZhao. Ageneral
scenario-agnostic reinforcement learning for traffic signal control. IEEE Transactions on Intelligent
TransportationSystems,2024.
[26] AnastasiosKouvelas,JennieLioris,SAlirezaFayazi,andPravinVaraiya. Maximumpressurecontroller
forstabilizingqueuesinsignalizedarterialnetworks. TransportationResearchRecord,2421(1):133–141,
2014.
[27] MinaeKwon,SangMichaelXie,KaleshaBullard,andDorsaSadigh.Rewarddesignwithlanguagemodels.
InICLR,2023.
[28] YixingLan,XinXu,QiangFang,YujunZeng,XinwangLiu,andXianjianZhang. Transferreinforcement
learningviameta-knowledgeextractionusingauto-pruneddecisiontrees. Knowledge-BasedSystems,
242:108221,2022.
[29] DapengLi,HangDong,LuWang,BoQiao,SiQin,QingweiLin,DongmeiZhang,QiZhang,Zhiwei
Xu,BinZhang,etal. Verco:Learningcoordinatedverbalcommunicationformulti-agentreinforcement
learning. arXivpreprintarXiv:2404.17780,2024.
[30] DapengLi,ZhiweiXu,BinZhang,andGuoliangFan. Fromexplicitcommunicationtotacitcooperation:
Anovelparadigmforcooperativemarl. arXivpreprintarXiv:2304.14656,2023.
[31] JackyLiang,WenlongHuang,FeiXia,PengXu,KarolHausman,BrianIchter,PeteFlorence,andAndy
Zeng. Codeaspolicies: Languagemodelprogramsforembodiedcontrol. InICRA,pages9493–9500,
2023.
[32] ZacharyCLipton,KamyarAzizzadenesheli,AbhishekKumar,LihongLi,JianfengGao,andLiDeng.
Combatingreinforcementlearning’ssisypheancursewithintrinsicfear. arXivpreprintarXiv:1611.01211,
2016.
[33] FeiLiu,XialiangTong,MingxuanYuan,XiLin,FuLuo,ZhenkunWang,ZhichaoLu,andQingfuZhang.
Anexampleofevolutionarycomputation+largelanguagemodelbeatinghuman:Designofefficientguided
localsearch. arXivpreprintarXiv:2401.02051,2024.
[34] RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch. Multi-agentactor-criticfor
mixedcooperative-competitiveenvironments. InNeurIPS,2017.
[35] JiamingLu,JingqingRuan,HaoyuanJiang,ZiyueLi,HangyuMao,andRuiZhao. Dualight:Enhancing
traffic signal control by leveraging scenario-specific and scenario-shared knowledge. arXiv preprint
arXiv:2312.14532,2023.
[36] YechengJasonMa,WilliamLiang,GuanzhiWang,De-AnHuang,OsbertBastani,DineshJayaraman,
YukeZhu,LinxiFan,andAnimaAnandkumar. Eureka: Human-levelrewarddesignviacodinglarge
languagemodels. InICLR,pages1–39,2024.
[37] PatrickMeier. Digitalhumanitarians:howbigdataischangingthefaceofhumanitarianresponse. Crc
Press,2015.
[38] SeyedSajadMousavi,MichaelSchukat,andEndaHowley. Trafficlightcontrolusingdeeppolicy-gradient
andvalue-function-basedreinforcementlearning. IETIntelligentTransportSystems,11(7):417–423,2017.
11[39] MdShirajumMunir,NguyenHTran,WalidSaad,andChoongSeonHong.Multi-agentmeta-reinforcement
learningforself-poweredandsustainableedgecomputingsystems. IEEETransactionsonNetworkand
ServiceManagement,18(3):3353–3374,2021.
[40] SrinarayanaNagarathinam,VishnuMenon,ArunchandarVasan,andAnandSivasubramaniam. Marco-
multi-agentreinforcementlearningbasedcontrolofbuildinghvacsystems. InACMe-Energy,2020.
[41] NathaliaNascimento,PauloAlencar,andDonaldCowan. Gpt-in-the-loop:Adaptivedecision-makingfor
multiagentsystems. arXivpreprintarXiv:2308.10435,2023.
[42] MohamedNejjar,LucaZacharias,FabianStiehle,andIngoWeber. Llmsforscience: Usageforcode
generationanddataanalysis. arXivpreprintarXiv:2311.16733,2023.
[43] FransAOliehoek,MatthijsTJSpaan,andNikosVlassis. OptimalandapproximateQ-valuefunctionsfor
decentralizedPOMDPs. JournalofArtificialIntelligenceResearch,32:289–353,2008.
[44] VenkataRamakrishnaPadullaparthi,SrinarayanaNagarathinam,ArunchandarVasan,VishnuMenon,and
DepakSudarsanam. Falcon-farmlevelcontrolforwindturbinesusingmulti-agentdeepreinforcement
learning. RenewableEnergy,181:445–456,2022.
[45] DeepakPathak,PulkitAgrawal,AlexeiAEfros,andTrevorDarrell. Curiosity-drivenexplorationby
self-supervisedprediction. InICML,pages2778–2787,2017.
[46] TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFarquhar,JakobFoerster,and
ShimonWhiteson. Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning.
JournalofMachineLearningResearch,21(1):7234–7284,2020.
[47] RogerPRoess,ElenaSPrassas,andWilliamRMcShane. Trafficengineering. Pearson/PrenticeHall,
2004.
[48] BernardinoRomera-Paredes,MohammadaminBarekatain,AlexanderNovikov,MatejBalog,MPawan
Kumar,EmilienDupont,FranciscoJRRuiz,JordanSEllenberg,PengmingWang,OmarFawzi,etal.
Mathematicaldiscoveriesfromprogramsearchwithlargelanguagemodels. Nature,625(7995):468–475,
2024.
[49] BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,YossiAdi,
JingyuLiu,TalRemez,JérémyRapin,etal. Codellama:Openfoundationmodelsforcode. arXivpreprint
arXiv:2308.12950,2023.
[50] PratyushaSharma,AntonioTorralba,andJacobAndreas.Skillinductionandplanningwithlatentlanguage.
InACL,pages1713–1726,2022.
[51] AliShirali,AlexanderSchubert,andAhmedAlaa. Pruningthewaytoreliablepolicies:Amulti-objective
deepq-learningapproachtocriticalcare. arXivpreprintarXiv:2306.08044,2023.
[52] HerbertASimon. Rationalchoiceandthestructureoftheenvironment. Psychologicalreview,63(2):129,
1956.
[53] KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi. Qtran: Learning
tofactorize withtransformationforcooperativemulti-agentreinforcement learning. InICML,pages
5887–5896,2019.
[54] KaushikSubramanian,CharlesLIsbellJr,andAndreaLThomaz. Explorationfromdemonstrationfor
interactivereinforcementlearning. InAAMAS,pages447–456,2016.
[55] GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,andAnima
Anandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels. arXivpreprint
arXiv:2305.16291,2023.
[56] JianhaoWang,ZhizhouRen,TerryLiu,YangYu,andChongjieZhang. QPLEX:Duplexduelingmulti-
agentQ-learning. InICLR,pages1–27,2021.
[57] Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforcement
learningwithemergentroles. arXivpreprintarXiv:2003.08039,2020.
[58] YukiWang,GonzaloGonzalez-Pumariega,YashSharma,andSanjibanChoudhury. Demo2code:From
summarizingdemonstrationstosynthesizingcodeviaextendedchain-of-thought. InNeurIPS,2024.
[59] HuaWei,NanXu,HuichuZhang,GuanjieZheng,XinshiZang,ChachaChen,WeinanZhang,Yanmin
Zhu,KaiXu,andZhenhuiLi. Colight:Learningnetwork-levelcooperationfortrafficsignalcontrol. In
CIKM,pages1913–1922,2019.
[60] Xianliang Yang, Zhihao Liu, Wei Jiang, Chuheng Zhang, Li Zhao, Lei Song, and Jiang Bian. A
versatile multi-agent reinforcement learning benchmark for inventory management. arXiv preprint
arXiv:2306.07542,2023.
[61] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikRNarasimhan,andYuanCao. React:
Synergizingreasoningandactinginlanguagemodels. InICLR,pages1–33,2023.
12[62] HaoranYe, JiaruiWang, ZhiguangCao, andGuojieSong. Reevo: Largelanguagemodelsashyper-
heuristicswithreflectiveevolution. arXivpreprintarXiv:2402.01145,2024.
[63] DonghaoYing,YunkaiZhang,YuhaoDing,AlecKoppel,andJavadLavaei. Scalableprimal-dualactor-
criticmethodforsafemulti-agentRLwithgeneralutilities. InNeurIPS,2023.
[64] ChaoYu, AkashVelu, EugeneVinitsky, JiaxuanGao, YuWang, AlexandreBayen, andYiWu. The
surprisingeffectivenessofppoincooperativemulti-agentgames. InNeurIPS,pages24611–24624,2022.
[65] HaoqiYuan, ChiZhang, HongchengWang, FeiyangXie, PenglinCai, HaoDong, andZongqingLu.
Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv preprint
arXiv:2303.16563,2023.
[66] TomZahavy,MatanHaroush,NadavMerlis,DanielJMankowitz,andShieMannor. Learnwhatnotto
learn:Actioneliminationwithdeepreinforcementlearning. InNeurIPS,2018.
[67] BinZhang,HangyuMao,JingqingRuan,YingWen,YangLi,ShaoZhang,ZhiweiXu,DapengLi,Ziyue
Li,RuiZhao,etal. Controllinglargelanguagemodel-basedagentsforlarge-scaledecision-making:An
actor-criticapproach. arXivpreprintarXiv:2311.13884,2023.
[68] KaiqingZhang,ZhuoranYang,andTamerBas¸ar.Multi-agentreinforcementlearning:Aselectiveoverview
oftheoriesandalgorithms. HandbookofReinforcementLearningandControl,pages321–384,2021.
[69] XizhouZhu,YuntaoChen,HaoTian,ChenxinTao,WeijieSu,ChenyuYang,GaoHuang,BinLi,Lewei
Lu,XiaogangWang,etal. Ghostintheminecraft:Generallycapableagentsforopen-worldenviroments
vialargelanguagemodelswithtext-basedknowledgeandmemory. arXivpreprintarXiv:2305.17144,
2023.
[70] AlessandroZocca. Temporalstarvationinmulti-channelcsmanetworks:ananalyticalframework. ACM
SIGMETRICSPerformanceEvaluationReview,46(3):52–53,2019.
13A Proofs
Proposition1.
1. ForanyE ∈{E},{π }⊆{π}. IfE isnon-trivial,then{π }⊂{π}.
E E
2. Foranyπ ∈{π},thereexistsanon-trivialE ∈{E}suchthatJ(π )≥J(π).
E
Proof. WebeginbyofferingtheproofofthefirststatementinProposition1. Wedenote{πk}asthe
setofallpossiblepoliciesforagentk,witheachπk satisfyingthefollowingtwoconditions:
(cid:26) πk :Ok×Ak →[0,1]
(cid:80) πk(ak |ok)=1 (5)
ak∈Ak
Let{πk}bethesetofpoliciesunderanexplorationfunctionE. Foreveryelementinπk ∈{πk},
E E E
oneofthetwosituationsexists:
1. IfE istrivial,thenπk(·|ok)=πk(·|ok),henceπk ∈{πk}.
E E
2. IfE isnon-trivial,thenπk(·|ok)= πk(·|ok)·E(ok,·) . Itisclearthat0≤πk(·|
ok)≤1and(cid:80) πk(E ak |ok)=1(cid:80) ,a thk∈ uA sk ππ kk( ∈ak {|o πk k)· }E .(ok,ak) E
ak∈Ak E E
Therefore,weknowthat:
{πk}⊆{π }. (6)
E k
Giventhatfor∀π(· | s ) = (cid:81)N πk(· | ok),wehaveπ ∈ {π},whereeachπk belongsto{πk}.
t k=1
AccordingtoFormula6, itisknownthatfor∀π (· | s ) = (cid:81)N πk(· | ok), itbelongsto{π}.
E t k=1 E
Then:
{π }⊆{π}. (7)
E
WhenE isnon-trivial,πk ∈ {πk}stillholds,butπk ∈ {πk}maynotbetrue(i.e.,whenπk(ak |
E E
ok)>0for∀ak ∈A,πk ∈/ {πk}). Hencewecanget:
E
{πk}⊂{π }. (8)
E k
Inasimilarmanner,wecandeducethatifthereexistsak ∈[1,2,...,N]suchthatπk ∈/ {πk},then
E
π ∈/ {π },whichmeans:
E
{π }⊂{π}. (9)
E
Therefore,wefinishtheproofofthefirststatement.
Toproofthesecondstatement,itisnecessarytointroduceaseriesofvariables. Wedefinethevalue
functionandstate-actionfunctionforπasfollows: V (s)=E [(cid:80)∞ γtr |s =s]
andQ (s,a) = E [(cid:80)∞ γtr |s =sπ ,a =a].a0 T:∞ he∼π ad,s v1: a∞ n∼ taP gefut n= c0 tiont isde0 fined
π a1:∞∼π,s1:∞∼P t=0 t 0 0
as A (s,a) = Q (s,a) − V (s). The joint exploration function is introduced as E(s,·) =
π π π
(cid:81)N E(ok,·). TherelationshipbetweenV (s)andQ (s,a)canbeformulatedas:
k=1 π π
(cid:88)
V (s)= π(a|s)Q (s,a) (10)
π π
a∈Ak
Foranon-trivialE,thevaluefunctionofπ canbewrittenas:
E
(cid:88) E(s,a)π(a|s)
V (s)= Q (s,a)
πE (cid:80) E(s,a)π(a|s) π
a∈Ak
a∈Ak
1 (cid:88)
= E(s,a)π(a|s)Q (s,a)
(cid:80) E(s,a)π(a|s) π (11)
a∈Ak
a∈Ak
 
1 (cid:88)
=
(cid:80)
E(s,a)π(a|s)V π(s)− (1−E(s,a))π(a|s)Q π(s,a).
a∈Ak
a∈Ak
14Thus,wehave:
(cid:80)
V (s)− (1−E(s,a))π(a|s)Q (s,a)
V (s)−V (s)= π a∈Ak π −V (s)
πE π (cid:80) E(s,a)π(a|s) π
a∈Ak
(cid:0) (cid:80) (cid:1) (cid:80)
1− E(s,a)π(a|s) V (s)− (1−E(s,a))π(a|s)Q (s,a)
= a∈Ak π a∈Ak π
(cid:80)
E(s,a)π(a|s)
a∈Ak
(cid:80) (cid:80)
(1−E(s,a))π(a|s)V (s)− (1−E(s,a))π(a|s)Q (s,a)
= a∈Ak π a∈Ak π
(cid:80)
E(s,a)π(a|s)
a∈Ak
(cid:80)
(1−E(s,a))π(a|s)A (s,a)
=− a∈Ak π .
(cid:80)
E(s,a)π(a|s)
a∈Ak
(12)
(cid:80)
When− (1−E(s,a))π(a|s)A (s,a)≥0,whichmeanstheexpectationoftheadvantage
a∈Ak π
value for the pruned actions is less than or equal to 0, then V (s) ≥ V (s). Because for every
(cid:80) (cid:80)
πE π
s∈S, A (s,a)= Q (s,a)−V (s)=0,therealwaysexistactionsforwhichthe
a∈Ak π a∈Ak π π
advantagefunctionvaluesarelessthanorequaltozero.
As J(π ) − J(π) = E [V (s )−V (s )], if an exploration function E can satisfy the
E s0∼ρ0 πE 0 π 0
(cid:80)
condition that for all states s ∈ S, the inequality − (1−E(s,a))π(a | s)A (s,a) ≥ 0
a∈Ak π
holds,thenitcanbeguaranteedthatJ(π )≥J(π).
E
Therefore,wefinishtheproofofthesecondstatement.
Proposition2. Let’sconsiderafullycooperativegamewithNagents,onestate,andthejointaction
space{0,1}N, wheretherewardisgivenbyr(00,1N) = r andr(0N−m,1m) = −mr forall
1 2
m̸=N,r ,r arepositiverealnumbers. Wesupposetheinitialpolicyisrandomlyanduniformly
1 2
initialized,andthepolicyisoptimizedintheformofgradientdescent. Letpbetheprobabilitythat
thesharedpolicyconvergestothebestpolicy,then:
(cid:114)
r
p=1− N−1 2 . (13)
r +Nr
1 2
Proof. Clearly,thebestpolicyisthedeterministicpolicywithjointaction(00,1N).
Now,letthesharedpolicybe(1−θ,θ),whereθistheprobabilitythatanagenttakesaction1. The
expectedrewardcanbewrittenas:
N−1
J(θ)=Pr(cid:0) a1:N =(00,1N)(cid:1) ·r − (cid:88) Pr(cid:0) a1:N =(0N−t,1t)(cid:1) ·t·r
1 2
t=0
N−1
(cid:88)
=θN ·r − t·Ct θt(1−θ)N−t·r (14)
1 N 2
t=0
N
(cid:88)
=θN ·r − t·Ct θt(1−θ)N−t·r +N ·θN ·r ,
1 N 2 2
t=0
whereCt isacombinatorialnumber. Weneedtosimplify(cid:80)N t·Ct θt(1−θ)N−t forfurther
N t=0 N
analysis. Noticethestructuralsimilaritybetweentheresultsandthebinomialtheorem:
N
((1−θ)+θ)N =(cid:88) Ct θt(1−θ)N−t. (15)
N
t=0
15WetakethederivativeofθonbothsidesofFormula15.Becausetheleftsideisconstant,itsderivative
is0. Then:
d(cid:80)N Ct θt(1−θ)N−t
0= t=0 N
dθ
N
(cid:88)
= Ct t·θt−1·(1−θ)N−t+Ct (N −t)·(−1)·(1−θ)N−t−1·θt
N N
t=0
(16)
N
(cid:88)
= Ct (1−θ)N−t−1θt−1((1−θ)t−(N −t)θ)
N
t=0
N
(cid:88)
= Ct (1−θ)N−t−1θt−1(t−Nθ).
N
t=0
Thus,wehave:
N N
(cid:88) (cid:88)
Nθ Ct (1−θ)N−t−1θt−1 = tCt (1−θ)N−t−1θt−1
N N
t=0 t=0
(17)
N N
(cid:88) (cid:88)
Nθ Ct (1−θ)N−tθt = tCt (1−θ)N−tθt.
N N
t=0 t=0
NoticethattheleftsideoftheequationistheexpansionformofFormula15,andtherightsideofthe
equationisthedesiredFormula,wecanget:
N
(cid:88)
tCt (1−θ)N−tθt =Nθ. (18)
N
t=0
BringFormula18backtoFormula14,weget:
J(θ)=θN ·r −Nθr +N ·θN ·r . (19)
1 2 2
InordertomaximiseJ(θ),wemustmaximiseθN·(r +Nr )−Nθr .Sincethepolicyoptimization
1 2 2
usuallyadoptsagradientmanner,wecalculatethederivativeofFormula19withrespecttoθas:
dJ(θ)
=NθN−1(r +Nr )−Nr . (20)
dθ 1 2 2
(cid:113)
thepointθ∗ = N−1 r2 istheonlyzeroof dJ(θ). Whenθ ≤θ∗, dJ(θ) ≤0;θ ≥θ∗, dJ(θ) ≥0.
r1+Nr2 dθ dθ dθ
RememberwearetryingtomaximizeJ(θ)throughagradientway,andthenthepolicyimprovesthe
parametersinthedirectionofthegradient. Astheinitialpolicyisrandomlyanduniformlyinitialized,
theθisuniformlydistributedintheinterval[0,1],thentheprobabilitythatthesharedpolicyconverges
tothebestpolicyis:
(cid:114)
r
p=1− N−1 2 . (21)
r +Nr
1 2
Therefore,wefinishtheproofofProposition2.
B Pseudocode
InthisSection,wegivethepseudocodeofourproposedeSpark. Wedenotetheperformanceof
policyiasG ,andthepseudocodeofeSparkisshowninAlgorithm1.
i
C Detailedsettings
C.1 MABIMdetails
MABIMisasimulationenvironmentdedicatedtoleveragingMARLtotacklethechallengesinherent
ininventorymanagementproblems. WithinMABIM,eachstockSKUateveryechelonisrepresented
16Algorithm1eSpark
1: Input: Initial prompt prom, LLM checker LLM c, LLM code generator LLM g, the evolution
iterationnumberN,andsamplebatchsizeK
2: Initialize: policiesϕ1,ϕ1,...,ϕ1
1 2 K
3: fori=1toN do
4: //ExplorationFunctionGeneration
5: E 1,...,E K ∼LLM c(prom,LLM g(prom))
6: //Retentiontraining
7: ifi̸=1then
8: ϕi,ϕi,...,ϕi ←ϕi−1
1 2 K best
9: endif
10: //Evolutionarysearch
11: G 1,F 1 =ϕ(E 1),...,G K,F K =ϕ(E K)
12: //ReflectionandFeedback
13: best←argmax k(G 1,G 2,...,G K)
14: prom←prom:Reflection(E ,F )
best best
15: endfor
16: Output: ϕN
best
Replenishment Replenishment
... ...
Product Product
Factory Warehouse Customer
Figure3: MABIMinventorymodel.
as an autonomous agent. The decision-making process of each agent reflects the procurement
requirementsforthespecificSKUatitscorrespondingechelon.
EachtimestepinvolvestheagentmakingdecisionsregardingreplenishmentquantitiesforSKUs
andsubsequentlytransitioningtheenvironmenttoanewstate. LetM ∈Z+bethetotalwarehouses,
with the first one being closest to customers, and N ∈ Z+ the total SKUs. Given a variable
X ∈ {D,S,L...}, Xt represents its value for the j-th SKU in the i-th echelon at step t, with
i,j
0 ≤ i < M and 0 ≤ j < N. Given the above notations, the main progression of a step can be
describedasfollows:
Dt+1 =Rt (Replenish)
i+1,j i,j
St =min(Dt ,It ) (Sell)
i,j i,j i,j
t−1
(cid:88)
At = I(k+Lk ==t)·St (Arrive)
i,j i,j i+1,j
k=0
(cid:32) W −(cid:80) It (cid:33)
γt =min i j i,j ,1 ,Bt =⌊At ·γt⌋ (Receive)
i (cid:80) At i,j i,j i
j i,j
It+1 =It −St +Bt (Update)
i,j i,j i,j i,j
Here,D,R,S,I,A,B ∈Z+andI(condition)isanindicatorfunctionthatreturns1ifthecondition
istrue,and0otherwise. Forthetopmostechelon,ordersarechanneledtoasupervendorcapableof
fulfillingallorderdemandsatthatlevel. Ordersfromotherechelonsaredirectedtotheirimmediate
upstream echelons, where the demands are satisfied based on the inventory levels of the upper
echelons. Thedemandatthebottomechelonisderivedfromactualcustomerorderscapturedwithin
real-worlddatasets. TherewardfunctionwithinMABIMismeticulouslycalibratedbasedonthe
economic realities of inventory management, integrating five fundamental elements: sales profit,
ordercost,holdingcost,backlogcost,andexcesscost. Thesummationoftheseelementsconstitutes
therewardvalue,therebyincentivizingagentstooptimizeinventorycontrolforenhancedprofitability
andoperationalefficacy.
17MABIMincorporateschallengesacrossfivekeycategories: Scalingup,Cooperation,Competition,
Generalization, and Robustness. We concentrate on the challenges associated with Scaling up,
Cooperation, and Competition, as these challenges not only manifest in inventory management
problemsbutalsoexistinabroadrangeofMARLtasks. Wecatalogthenumberofagents,challenges
anddegreesofdifficultywithinalltheexperimentalscenariosinTable7. Thespecificsettingofeach
scenarioisgiveninTable13:
Table7: Tasksandcorrespondingchallenges. ‘+’denotestheextentofthechallenges.
Challenge
Taskname Agentsnumber
Scalingup Cooperation Competition
Standard(100SKUs) 100
2echelons(100SKUs) 200 +
3echelons(100SKUs) 300 ++
Lowercapacity(100SKUs) 100 +
Lowestcapacity(100SKUs) 100 ++
Standard(500SKUs) 500 +
2echelons(500SKUs) 1000 + +
3echelons(500SKUs) 1500 + ++
Lowercapacity(500SKUs) 500 + +
Lowestcapacity(500SKUs) 500 + ++
Table8: Experimentssettings. "#SKU*N"indicatesNtimesthenumberofSKUs.
Taskname #Echelon #SKU Capacityperechelon
Standard(100SKUs) 1 100 #SKU*100
2echelons(100SKUs) 2 100 #SKU*100
3echelons(100SKUs) 3 100 #SKU*100
Lowercapacity(100SKUs) 1 100 #SKU*50
Lowestcapacity(100SKUs) 1 100 #SKU*25
Standard(500SKUs) 1 500 #SKU*100
2echelons(500SKUs) 2 500 #SKU*100
3echelons(500SKUs) 3 500 #SKU*100
Lowercapacity(500SKUs) 1 500 #SKU*50
Lowestcapacity(500SKUs) 1 500 #SKU*25
Foreachscenario,wecarryoutthreeindependentruns. Performanceisreportedasaveragetestset
profitsfromthetopmodelineachrun.
C.2 SUMOdetails
SUMOisanopen-source,highlyportable,microscopicandcontinuousroadtrafficsimulationpackage
designedtohandlelargeroadnetworks. IntheSUMOsimulationenvironment,eachintersectionis
conceptualizedasanautonomousagentequippedwithanarrayofpredefinedtrafficsignalphases.
Thesephasesorchestratethetrafficflowacrosstheintersection’smultipleapproaches. Theselection
of these phases, driven by the assessment of live traffic conditions, is aimed at attenuating road
congestionandenhancingthefluidityofvehicularmovementthroughthenetwork,thuscontributing
totheoverallefficiencyofurbantrafficmanagement.
Toconductathoroughevaluationofeachalgorithm,weselectatotaloffivescenariosfromboth
syntheticandreal-worlddatasets. Thesedatasetsencompassadiversearrayofintersections,varying
in number and type. The intersections are classified according to their configuration into three
categories: bi-directional(2-arm),tri-directional(3-arm),andquadri-directional(4-arm),indicating
thenumberofexitpointsateachjunction. Wesummarizethetypeofeachdataset,thenumberof
intersectionsincluded,andtheclassificationoftheseintersectionsinTable9.
18Table9: ThecategoriesofeachSUMOdataset,alongwiththenumberandtypesofintersections
included.
Dataset Category Intersectionsnumber 2-arm 3-arm 4-arm
Grid4×4 synthesis 16 0 0 16
Arterial4×4 synthesis 16 0 0 16
Grid5×5[35] synthesis 25 0 0 25
Cologne8 real-world 8 1 3 4
Ingolstadt21 real-world 21 0 17 4
Tofacilitateahomogeneousobservationandactionspaceconducivetothedeploymentofvarious
MARLalgorithms,weemploytheGEneralScenario-Agnostic(GESA)frameworktoparseeach
intersectionintoastandardized4-armintersectionwitheightpotentialactions.
C.3 Modeldetails
We employ IPPO as the base MARL algorithm for eSpark due to its ability to scale to large-
scale MARL challenges. We select GPT-4 as our LLM and LLM , specifically opting for the
c g
2023-09-01-previewversion. ThetemperatureofGPT-4issetto0.7,withnofrequencypenalty
and presence penalty. For each scenario, we conduct three runs, setting the batch size for each
generationofexplorationfunctionstoK =16. Thisbatchsizeischosenbecauseitguaranteesthat
theinitialgenerationcontainsatleastoneexecutableexplorationfunctionforourenvironment. We
limitthenumberoftrainingiterationsto10,asweobservethattheperformanceformostscenarios
tendstoconvergewithinthisnumberofiterations.
D Baselinedetails
In our experiments, we employed three categories of baselines: OR baselines, MARL baselines
andpruningbaselines. InTableD,welistthecharacteristicsandenvironmentofallthebaselines
utilizedinourstudy. Intherestofthissection,wewillelucidatetheunderlyingprinciplesofeach
ORbaseline,articulatethedesignofthepruningbaselines,andpresentthehyperparameterforthe
MARLbaselines.
Table10: Allthebaselinesusedintheexperiments
MARLbaseline Usedenvironment
Algorithmname ORbaseline Pruningbaseline
CTDE DTDE MABIM SUMO
Basestock(BS)[3] ✓ ✓
(S,s)[6] ✓ ✓
FTC[47] ✓ ✓
MaxPressure[26] ✓ ✓
IPPO ✓ ✓ ✓
QTRAN ✓ ✓
QPLEX ✓ ✓
MAPPO ✓ ✓ ✓
MPLight[10] ✓ ✓
CoLight[59] ✓ ✓
Ramdompruning ✓ ✓ ✓
(S,s)pruning ✓ ✓
Upboundpruning ✓ ✓
MaxPressurepruning ✓ ✓
19D.1 ORbaselines
D.1.1 Basestockalgorithm
Thebasestockalgorithmconstitutesastreamlinedandefficaciousapproachforinventorycontrol,
wherebyreplenishmentordersareinitiateduponinventorybelowapredefinedthresholdlevel. This
policyistraditionallyacknowledgedasafundamentalbenchmark,favoredforitsstraightforward-
ness and rapid implementation. The computation of the base stock level is facilitated through a
programmaticmethodology,asexplicatedinEquation22:
max ot =p¯ ·St −c¯ ·St −h¯ ·It+1−c¯ ·T0 −c¯ ·I0
i,j i,j i,j i,j i+1,j i,j i,j i,j i,j i,j i,j
s.t It+1 =It +St−L¯ i,j −St
i,j i,j i+1,j i,j
Tt+1 =Tt −St−L¯ i,j +St
i,j i,j i+1,j i+1,j
St =min(It ,Rt )
i,j i,j i,j
(22)
−1
(cid:88)
T0 = St
i,j i+1,j
t=−L¯
i,j
z =It+1+St +Tt
i,j i,j i+1,j i,j
z ∈R+.
i,j
Intheaboveequations,i,j,andtareindexesforthewarehouse,SKU,anddiscretetime,respectively.
Theindicatorsp¯,c¯,h¯,andL¯ representtheaveragesellingprice,costofprocurement,costofholding,
andleadtime. ThevariablesS,R,I,andT denotethequantitiesassociatedwithsales,ordersfor
replenishment,inventoryinstockandinventoryintransit. ot describestheprofitobjective,while
i,j
z isindicativeofthebasestocklevel.
i,j
Weutilizetwoapproachesforcomputingstocklevels. Thefirstapproach,namedBSstatic,involves
calculatingallbasestocklevelswithhistoricaldatafromthetrainingset,whicharethenapplied
consistentlytothetestset. Thelevelsremainunchangedduringthetestperiod. Thesecondapproach,
termedasBSdynamic,computesstocklevelsdirectlyonthetestsetrelyingonhistoricaldataand
updatesonaregularbasis.
D.1.2 (S,s)algorithm
The (S,s) inventory policy serves as a robust framework for managing stock levels. Under this
policy,arestockingorderistriggeredoncetheinventorycountfallsbelowapredefinedthreshold,
identifiedass. Theobjectiveofthisreplenishmentistoelevatethestocktoitsupperlimit,designated
as S. Empirical analyses have substantiated the efficacy of this protocolin optimizing inventory
controlprocesses. Asaresult,itisadoptedasabenchmarkheuristic,withtheaimofalgorithmically
ascertaining the most efficacious (S,s) parameter for each discrete SKU in the given inventory
dataset. Inourimplementation,weconductasearchonthetrainingsettoidentifytheoptimalvalues
ofsandS,afterwhichweapplythesevaluesconsistentlytothetestset.
D.1.3 Fixed-timecontrolalgorithm
TheFixed-TimeControl(FTC)algorithmisatraditionaltrafficsignalcontrolstrategypredicatedon
predefinedsignalplans. Theseplansaretypicallydesignedbasedonhistoricaltrafficflowpatterns
anddonotadapttoreal-timetrafficconditions. TheFTCoperatesonastaticschedulewherethe
signalphasesatintersectionschangeatfixedintervals. Thisapproachisstraightforwardandeasyto
implementbutmaynotbeoptimalundervariabletrafficconditionsduetoitslackofresponsiveness
todynamictrafficdemands.
In our implementation, the FTC follows a fixed sequence of signal phases: ’WT-ET’, ’NT-ST’,
’WL-EL’,’NL-SL’,’WL-WT’,’EL-ET’,’SL-ST’,’NL-NT’.Here,’W’,’E’,’N’,and’S’denote
westbound,eastbound,northbound,andsouthboundtraffic,respectively,while’T’indicatesthrough
movement,and’L’signifiesaleftturn. Eachphasehasadurationof30seconds
20Table11: HyperparametersofMARLAlgorithmsUsedinMABIMandSUMOEnvironments. ‘-’
indicatesthatthealgorithmisnotsetordoesnotcontainthishyperparameter.
MABIMenvironment SUMOenvironment
Hyperparameter
IPPO QTRAN QPLEX MAPPO IPPO CoLight MPLight
Trainingsteps 5020000 5020000 5020000 5020000 2400000 2400000 2400000
Discountrate 0.985 0.985 0.985 0.985 0.985 0.9 0.9
Optimizer Adam Adam Adam Adam Adam RMSProp RMSProp
Optimizeralpha 0.99 0.99 0.99 0.99 0.99 0.95 0.95
Optimizereps 1e-5 1e-5 1e-5 1e-5 1e-5 1e-7 1e-7
Learningrate 5e-4 5e-4 5e-4 5e-4 5e-4 1e-3 1e-3
Gradnormclip 10 10 10 10 10 - -
Epsclip 0.2 - - 0.2 0.2 - -
Criticcoef 0.5 - - 0.5 0.5 - -
Entropycoef 0 - - 0 0 - -
Accumulatedepisodes 4 8 8 4 4 10 10
Numberofneighbors - - - - - 5 -
D.1.4 MaxPressurealgorithm
TheMaxPressurealgorithmrepresentsamoreadvancedtrafficsignalcontrolstrategythatdynamically
adjustssignalphasesinresponsetoreal-timetrafficconditions. Itcalculatesthe"pressure"ateach
intersection,definedasthedifferencebetweenthenumberofvehiclesontheincomingandoutgoing
lanes.Thealgorithmaimstooptimizetrafficflowbyselectingsignalphasesthatreducethemaximum
pressureacrossthenetwork,thusalleviatingcongestionandenhancingnetworkthroughput. Unlike
FTC,MaxPressureisadaptiveandcancontinuouslyoptimizesignaltimingbasedonthecurrent
trafficstate,makingitmoresuitableformanagingfluctuatingtrafficvolumes.
D.2 HyperparameterssettingsforMARLbaselines
Thefollowingtableenumeratesthehyperparametersemployedduringthetrainingprocessforall
MARLbaselines.Foralltestscenarios,trainingisperformedwithauniformsuiteofhyperparameters
thathavenotundergonespecializedfine-tuning.
D.3 Pruningbaselines
D.3.1 Randompruning
Randompruningisimplementedbyrandomlymaskingacertainpercentageoftheavailableactions.
Duringtheactionselectionprocess,eachagentwillhaveppercentofitsavailableactionsrandomly
masked. Tobalancetheobservabilityofthepruning’simpactwiththepreservationofthealgorithm’s
capacitytoutilizepriorexperience,wesetp=0.3.
D.3.2 (S,s)pruning
According to the (S,s) algorithm, for a given SKU, a replenishment quantity of ∆ = S −s is
orderedwhenthecurrentinventorylevelfallsbelowthethresholds;otherwise,noorderisplaced.
Weextendthereferencereplenishmentquantity∆toarange[r ×∆,r ×∆],wherer ,r areboth
1 2 1 2
realnumbersand0≤r ≤1andr ≥1. Actionswithinthisintervalaredeemedavailable,while
1 2
thoseoutsideofthisrangearemasked. Inourimplementation,weselectr =0.5andr =2.
1 2
D.3.3 MaxPressurepruning
TheMaxPressurepruningmethodutilizestheheuristicconceptof"pressure"atanintersectionto
pruneactions. Wecalculatethepressureassociatedwitheachaction,andthesepressuresarethen
ranked. Theactionswiththetop-khighestpressuresarerenderedavailableforselection. Actionsnot
meetingthisthresholdaresubsequentlymasked.
AstandardizedintersectionwarpedthroughtheGESAismodeledasafour-armintersectioncompris-
ingeightpotentialactions. Weempiricallysetk =4toensureeffectivepruningwhilemaintaininga
sufficientnumberofavailableactions.
21E Additionalresults
E.1 Additionalmainresults
Inthissection,wepresentthecompletemainexperimentalresultsforSUMOinSection5.2,andgive
theconsumptionofGPTtokenshere.
Table12: DetailedperformanceinSUMO,includesthemeanandstandarddeviation.
Method Metric Grid4×4 Arterial4×4 Grid5×5 Cologne8 Ingolstadt21
Delay 161.14±3.77 1229.68±16.79 820.88±24.36 85.27±1.21 224.96±11.91
FTC Triptime 291.48±4.45 676.77±13.70 584.54±24.17 145.93±0.84 352.06±9.29
Waittime 155.66±3.42 521.86±13.33 441.63±21.13 58.92±0.68 161.22±7.88
Delay 63.39±1.34 995.23±77.02 242.85±16.04 31.63±0.61 228.64±15.83
MaxPressure Triptime 174.68±2.05 702.09±23.61 269.35±9.62 95.67±0.62 352.30±15.06
Waittime 37.37±1.06 511.06±22.55 114.36±6.48 11.03±0.28 159.44±13.34
Delay 48.40±0.45 884.73±38.94 228.78±11.59 27.60±1.70 342.97±43.61
IPPO Triptime 160.12±0.60 506.18±10.39 238.03±7.10 91.41±1.60 464.50±43.30
Waittime 22.69±0.38 435.44±77.54 91.84±6.31 7.70±0.82 267.51±40.53
Delay 51.25±0.58 958.43±181.72 221.62±20.73 32.55±4.66 347.59±47.59
MAPPO Triptime 160.01±0.54 757.40±242.00 247.56±3.71 94.31±1.77 480.66±49.46
Waittime 25.41±0.54 609.80±255.22 97.10±5.22 9.39±1.53 283.59±43.20
Delay 63.51±0.64 1142.98±79.65 223.44±16.18 37.93±0.45 196.74±9.88
MPLight Triptime 172.47±0.89 583.21±45.84 255.49±6.26 110.56±1.18 331.42±11.79
Waittime 40.32±0.96 366.27±58.03 126.42±5.31 12.98±0.57 126.09±13.60
Delay 53.40±1.89 820.67±58.65 339.66±48.55 27.48±1.30 296.47±106.82
CoLight Triptime 165.77±1.89 470.33±12.34 305.41±44.43 91.66±1.29 410.59±97.29
Waittime 27.25±1.64 312.47±16.63 157.65±35.69 9.35±1.09 215.98±90.62
Delay 48.36±0.32 854.22±68.21 209.49±13.98 25.39±1.27 243.92±15.81
eSpark Triptime 159.74±0.44 484.87±58.21 235.20±6.80 89.50±1.36 367.57±15.03
Waittime 22.58±0.29 328.82±61.70 88.38±4.41 6.94±0.38 180.09±13.84
Sincewedonotdesignspecificpromptsfordifferentscenariotaskswithinthesameenvironment,we
calculatetheaveragetokenconsumptionforallscenarioswitheachenvironmentanddisplayitin
TableE.1.
Table13: AveragetokenassumptionforMABIMandSUMO.
Environment Tokenassumption(M)
MABIM 3.0
SUMO 2.6
E.2 Detailedresultsofthepruningmethods
Inthissection,weprovidethedetailedresultsformultiplepruningbaselinesasdiscussedinSec-
tion5.3,alongwithresultsofeSparkforcomparison.
Table14: DetailedperformanceofvariouspruningmethodsinMABIM.
Avg.profits(K)
Method 100SKUsscenarios 500SKUsscenarios
Standard 2echelons 3echelons Lower Lowest Standard 2echelons 3echelons Lower Lowest
Randompruning 733.0 1407.6 1426.6 511.6 262.0 2718.0 8667.4 9464.1 2535.5 2202.1
(S,s)pruning 394.4 832.3 933.4 441.1 258.3 3884.3 9248.3 10282.1 3517.9 2085.6
Upboundpruning 745.0 630.2 -2.2 557.7 294.7 3261.3 2473.6 1657.6 2833.0 2167.7
eSpark 823.7 1811.4 2598.7 579.5 405.0 4468.6 9437.3 12134.2 3775.7 2519.5
22Table 15: Detailed performance of various pruning methods in SUMO, includes the mean and
standarddeviation.
Method Metric Grid4×4 Arterial4×4 Grid5×5 Cologne8 Ingolstadt21
Delay 49.07±0.36 858.33±48.20 238.57±9.45 25.89±1.34 353.38±24.39
Ramdompruning Triptime 160.13±0.58 548.08±61.84 241.92±9.60 89.75±1.26 478.53±22.54
Waittime 22.66±0.14 387.25±43.93 93.49±8.09 7.03±0.37 281.97±21.90
Delay 48.78±0.37 890.04±121.50 234.27±14.28 26.26±0.36 337.02±62.18
MaxPressurepruning Triptime 160.72±0.17 533.36±78.08 253.68±17.68 90.36±0.88 448.11±65.32
Waittime 23.03±0.56 391.96±78.34 102.29±10.34 7.33±0.12 257.98±61.91
Delay 48.36±0.32 854.22±68.21 209.49±13.98 25.39±1.27 243.92±15.81
eSpark Triptime 159.74±0.44 484.87±58.21 235.20±6.80 89.50±1.36 367.57±15.03
Waittime 22.58±0.29 328.82±61.70 88.38±4.41 6.94±0.38 180.09±13.84
E.3 Detailedresultsoftheablations
In this section, we provide the detailed results for ablations in Section 5.4, along with results of
eSparkforcomparison.
Table16: DetailedperformanceofablationsinMABIM.
Avg.profits(K)
Method
Standard 2echelons 3echelons Lower Lowest
eSparkw/oretention 719.0 1806.1 2388.6 547.7 294.1
eSparkw/oLLM 754.7 1538.9 1109.9 536.7 198.5
eSpark 823.7 1811.4 2598.7 579.5 405.0
Table17: DetailedperformanceofablationsinSUMO,includesthemeanandstandarddeviation.
Method Metric Grid4×4 Arterial4×4 Grid5×5 Cologne8 Ingolstadt21
Delay 48.75±0.49 851.56±37.98 211.07±22.01 25.18±0.51 246.05±14.88
eSparkw/oretention Triptime 160.42±0.54 487.05±65.66 248.96±15.04 89.23±0.49 363.28±14.83
Waittime 22.97±0.39 338.64±67.07 97.77±10.25 7.14±0.15 175.79±12.47
Delay 48.67±0.48 854.10±63.38 212.25±16.13 24.70±0.56 257.14±40.50
eSparkw/oLLM Triptime 159.90±0.56 491.49±67.52 238.33±9.41 88.57±0.60 376.46±40.01
Waittime 22.99±0.43 342.62±73.88 90.89±6.60 6.69±0.23 188.14±37.03
Delay 48.36±0.32 854.22±68.21 209.49±13.98 25.39±1.27 243.92±15.81
eSpark Triptime 159.74±0.44 484.87±58.21 235.20±6.80 89.50±1.36 367.57±15.03
Waittime 22.58±0.29 328.82±61.70 88.38±4.41 6.94±0.38 180.09±13.84
F Policyperformanceanalysis
TogainadeeperunderstandingofthepolicydifferencebetweeneSparkandIPPO,weselectthe
capacity limit and multiple echelons challenges within the 100 SKUs scenario as representative
cases,presentinginFigure4thedailyprofitofeSparkandIPPOonthetestdatasetchallengedwith
capacitylimitandmultipleechelons. Inthecapacitylimitchallenges,ahighdailyoverflowratioand
lowfulfillmentratiosuggestthatIPPOfallsshortinmasteringtheadjustmentofrestockingquantities
for individual agents when capacity is limited, leading to overstocking and substantial overflow.
Concurrently,thispreventsSKUsrequiredbyconsumersfrombeingaccommodated,culminating
inanexceedinglylowfulfillmentratio. Inmultipleechelonchallenges,thefulfillmentratioateach
echelongraduallydecreasesovertime,indicatingthatIPPOstrugglestocomprehendandlearnthe
intricateinterplayrequiredforcooperationamongvariousechelons,therebyinadequatelyfulfilling
thedemandsofeachechelon. Suchshortcomingsnotonlydiminishpotentialprofitsbutalsosubject
thesystemtoconsiderablebacklogexpenses. However,throughactionspacepruning,evolutionary
search,andreflection,eSparkmanagestoreducethesearchwithinthevastspace,selectingthemost
effectiveexploration functionsand continuouslyimproving. Thisapproach significantlyreduces
overflow in the capacity limit scenario and successfully learns suitable cooperation methods for
multipleechelons.
23Lower capacity 2 echelons
Fulfillment Overflow Downstream fulfillment Upstream fulfillment
eSpark IPPO eSpark IPPO
100 100 100 100
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
0 0 0 0
0 20 40 60 0 20 40 60 0 20 40 60 0 20 40 60
Lowest capacity 3 echelons
Fulfillment Overflow Downstream fulfillment Midstream fulfillment Upstream fulfillment
100 100 100 100
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
0 0 0 0
0 20 40 60 0 20 40 60 0 20 40 60 0 20 40 60
Figure 4: TheDapyesrformance comparDiasyosn between eSparkDayasnd IPPO in 100 SKDaUyss scenarios. In
capacity-limitedscenarios,eSparkstrivestomeetthedemandswhileminimizingoverflowcosts,
boasting a lower overflow ratio and a higher fulfillment ratio. In the multiple-echelon challenge,
eSparkachievesnuancedcollaborationacrossdifferentechelons,ensuringhighfulfillmentratios.
G Fullprompts
WereferencethepromptdesignoutlinedintheEureka[36]andadaptitspecificallyforexploration
functiongeneration. Ourpromptprovidesgeneralguidanceonthedesignofexplorationfunctions,
specificcodeformattingsuggestions,feedback,andrecommendationsforimprovement. Wepresent
ourpromptsforMABIMbelow.
You are an expert in both inventory management learning and reinforcement learning. You will get some exploration
functions, which help mask some actions that are logically unlikely to be selected, to help the exploration in
reinforcement learning tasks as effective as possible.
Your goal is to evaluate whether the given exploration function matches the task description and whether it contains
any illogical errors in the code content, and evaluate whether it's possible to avoid some unreasonable actions,
help the exploration of reinforcement learning. You need to pay special attention to the meaning of each state item
and the logic of the task, making sure to detect
(1) All incorrect use of variables in code
(2) All the parts that don't follow logic.
The exploration function signature can be:
{task_exploration_signature_string}
Your advice can be text or snippets of code, but it should not be the full exploration function code. Most
importantly, remember that your response must begin with either "Code passes check." or "Code fails to pass check.".
Under no circumstance can you begin your answer with other content.
Figure5: SystempromptforLLM .
c
You are an exper t in both inve nto ry manageme nt learni ng and reinforc ement learnin g. You ar e tryi ng to wri te so me
exploration functions, which hel ps mask some actio ns th at are logi cally unlike ly to be se lecte d, to he lp explorati on
in reinforcement learni ng tas ks as effe ctive as possibl e.
Your goal is to write a exploration function for the agent that will mask the actions that's almost impossible to be
chosen in the task described in text.
The exploration function signature can be:
{task_exploration_signature_string}
Your exploration function should only use the variables from the argument list.
Please just give only the exploration function and don't put it in a class. Please make sure that the code is
compatible with numpy (e.g., use numpy array instead of torch tensor).
Figure6: SystempromptforLLM .
g
24
)%(oitaR
)%(oitaRWrite a exploration function for the following task :
{task_introduction}
The definition of environment and transition are :
{transition_definition}
The agent and state definition is :
{state_definition}
The reward definition is :
{reward_definition}
Figure7: InitialpromptforLLM .
g
Please carefully check the exploration function for the following task :
{task_introduction}
The definition of environment and transition are :
{transition_definition}
The agent and state definition is :
{state_definition}
The reward definition is :
{reward_definition}
The requirements of code :
{code_output_tip}
Figure8: InitialpromptforLLM .
c
Here is the latest exploration function and it's description:
{gpt_response}
Please carefully review this code, check whether it matches the task description and whether it contains any
illogical errors in the code content, and evaluate whether it's possible to improve the exploration and the
performance.
Figure9: LLM ’sfeedbacktoLLM .
g c
We discuss this code with experts, and the code is not approved by experts, and the comments of experts on this code
are as follows:
{checker_feedback}
Please refer to expert advice and combine your own knowledge, fix the problems and provide a new, improved
exploration function!
Figure10: LLM ’sfeedbacktoLLM .
c g
def compute_mask(agent_states: AgentStates, supply_chain: SupplyChain, action_space: list):
# Here are some code you can refer to when you generate your exploration function.
...
return total_mask, {}
Figure11: Signatureofexplorationfunction.
25Please carefully analyze the policy feedback and provide a new, improved exploration function that can better solve
the task. Some helpful tips for analyzing the policy feedback:
(1) You can start with "let's think step by step", and then look at each reward individually and think about how
can you improve it
(2) If the total reward maintains in the same level or even reduce, then you must rewrite the entire exploration
function
(3) If the values for a certain reward component are near identical throughout, then this means RL is not able
to optimize this component as it is written. You may consider:
(a) Changing the temperature scale or value of the related mask component so that more action can be
explored
(b) Re-writing the related mask component
(c) Discarding the mask component or add a new mask component
Figure12: OutputandimprovementtipsforLLM .
g
The output of the exploration function should be a total mask (1 denote action is not masked and 0 otherwise). The
code output should be formatted as a python code string: "```python ... ```".
Some helpful tips for writing the exploration function code:
(1) Your total mask and its component should be a 3-D numpy array in [warehouse_name, sku_type, action_mask].
The masks of the actions that are available are set to 1, otherwise 0. No other elements are allowed to appear in
total mask.
(2) If you choose to transform a component mask, then you must also introduce a temperature parameter inside the
transformation function; this parameter must be a named variable in the mask function and it must not be an input
variable. Each transformed mask component should have its own temperature variable
(3) Make sure the type of each input variable is correctly specified; For example, a float input variable should
not be specified as torch.Tensor
(4) Most importantly, the exploration code's can only use variables defined in its arguments. Under no
circumstance can you introduce new input variables. You only need to give the definition of exploration function and
no other function or class should be defined.
Figure13: OutputformatforLLM .
g
Your output should contain two parts:
(1) Your response must begin with either "Code passes check." or "Code fails to pass check.". "Code passes
check." means you believe that there are no logical errors in the code and the variables are taken in accordance
with the description of the task. "Code fails to pass check." indicates the provided exploration function contains
logical errors, or you think the code is obviously flawed and you can point out how to facilitate more effective
exploration.
(2) If you begin with "Code fails to pass check.", you have to explain why the code fails to pass the check and
give your advice on fixing the problems; If you begin with "Code passes check.", you also have to state why each
part is logical and reasonable.
Some common logical errors include:
(1) Misunderstanding the meaning of the state items, or including syntax errors when using variables
(2) Illogically handling the state items
(3) Using multiple unrelated state items to calculate mask component
(4) The way of combining mask components into total mask is unreasonable
Figure14: OutputformatforLLM .
c
We trained a RL policy using the provided exploration function code and tracked the values of the individual
components of the reward function as well as global policy metrics. We also compute the maximum, mean in the early
training stage, mean in the late training stage, mean in all the training stage, minimum values for reward and its
components after every {epoch_freq} epochs. Each element is a one-dimensional array of length n_warehouse,
representing the value of that component on different warehouses:
<Reward Feedback Here>
Figure15: Rewardfeedbackandactionfeedback.
26H eSpark’sexplorationfunctionediting
In this section, we demonstrate the reward editing capabilities of eSpark. eSpark is capable of
reflectinguponfeedbacktooptimizetheexplorationforsubsequentiterations.
We trained a RL policy using the provided exploration function code and tracked the values of the individual
components of the reward function as well as global policy metrics. We also compute the maximum, mean in the early
training stage, mean in the late training stage, mean in all the training stage, minimum values for reward and its
components after every {epoch_freq} epochs. Each element is a one-dimensional array of length n_warehouse,
representing the value of that component on different warehouses:
metric_name: reward, Max: [435592.087], Min: [-1331893.335], Mean in the early training stage: [-212101.5832], Mean
in the late training stage: [338683.3766], Mean in all the training stage: [71512.5884375]
metric_name: profit, Max: [587080.], Min: [430897.], Mean in the early training stage: [493648.], Mean in the late
training stage: [564756.2], Mean in all the training stage: [527295.125]
metric_name: excess_cost, Max: [1648683.5], Min: [17213.5], Mean in the early training stage: [596130.9], Mean in
the late training stage: [123902.4], Mean in all the training stage: [349431.25]
metric_name: order_cost, Max: [60580.], Min: [56490.], Mean in the early training stage: [58864.], Mean in the late
training stage: [58758.], Mean in all the training stage: [58650.625]
metric_name: holding_cost, Max: [456.135], Min: [272.157], Mean in the early training stage: [390.6432], Mean in the
late training stage: [399.6234], Mean in all the training stage: [394.1803125]
metric_name: backlog_cost, Max: [56796.1], Min: [40408.7], Mean in the early training stage: [50364.04], Mean in the
late training stage: [43012.8], Mean in all the training stage: [47306.48125]
Please carefully analyze the policy feedback and provide a new, improved exploration function that can better solve
the task. Some helpful tips for analyzing the policy feedback:
(1) You can start with "let's think step by step", and then look at each reward individually and think about how
can you improve it
(2) If the total reward maintains in the same level or even reduce, then you must rewrite the entire exploration
function
(3) If the values for a certain reward component are near identical throughout, then this means RL is not able
to optimize this component as it is written. You may consider:
(a) Changing the temperature scale or value of the related mask component so that more action can be
explored
(b) Re-writing the related mask component
(c) Discarding the mask component or add a new mask component
Figure16: Policyfeedbackfromthelastiteration.
From the policy feedback, we can identify a few key issues:
1. The total reward is negative in the early training stage and only becomes positive in the later training stages.
This suggests that the agent is not learning effective strategies early on, potentially due to a lack of exploration.
2. The excess cost is quite high, especially in the early training stages. This suggests that the agent is
overstocking the warehouse.
3. The holding cost remains high throughout the training, indicating that the agent is holding onto inventory for
too long.
Given these issues, we need to revise the exploration function to encourage the agent to explore actions that would
result in less overstocking. Here's an improved exploration function:
Figure17: eSparkreflectsuponthepolicyfeedbackandproposesthemodification.
Figure18: Comparisonofexplorationfunctionsbeforeandafterediting.
27I Impactstatement
TheintegrationofMARLintoreal-worldapplicationsholdstransformativepotentialforsocietalin-
frastructure,includingtransportation,logistics,andpublicservicecoordination. Ourworkintroduces
eSpark, aframeworkthatsignificantlyimprovesMARLscalabilityandefficiencybyleveraging
largelanguagemodelstopruneunnecessaryactionspaces.Thisadvancementfostersthedevelopment
ofautonomoussystemscapableofcomplex,cooperativeinteractions,withouttheneedforextensive
manualtuning. Byenablingthemanagementoflarge-scaleagentsystems,itcanoptimizeresource
allocation,reducecongestion,andenhanceservicedeliveryinurbanenvironments. Thetechnology
alsopromisestoimprovetherobustnessandreliabilityofAIsystems,ensuringtheyoperatewithin
ethical boundaries and contribute positively to societal norms and values. As we move towards
anincreasinglyautomatedfuture,eSparkrepresentsasteptowardsresponsibleAIdevelopment,
prioritizingsocietalwell-beingandtheadvancementofcollective,intelligentproblem-solving.
28