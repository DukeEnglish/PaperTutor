Matryoshka Multimodal Models
MuCai1 JianweiYang2 JianfengGao2 YongJaeLee1
1UniversityofWisconsin-Madison 2MicrosoftResearch,Redmond
https://matryoshka-mm.github.io/
Abstract
LargeMultimodalModels(LMMs)suchasLLaVAhaveshownstrongperformance
in visual-linguistic reasoning. These models first embed images into a fixed
largenumberofvisualtokensandthenfeedthemintoaLargeLanguageModel
(LLM).However,thisdesigncausesanexcessivenumberoftokensfordensevisual
scenariossuchashigh-resolutionimagesandvideos,leadingtogreatinefficiency.
While token pruning and merging methods exist, they produce a single-length
output for each image and cannot afford flexibility in trading off information
densityv.s.efficiency. InspiredbytheconceptofMatryoshkaDolls,wepropose
M3: MatryoshkaMultimodalModels,whichlearnstorepresentvisualcontentas
nestedsetsofvisualtokensthatcaptureinformationacrossmultiplecoarse-to-fine
granularities. Our approach offers several unique benefits for LMMs: (1) One
can explicitly control the visual granularity per test instance during inference,
e.g., adjusting the number of tokens used to represent an image based on the
anticipatedcomplexityorsimplicityofthecontent;(2)M3providesaframework
for analyzing the granularity needed for existing datasets, where we find that
COCO-stylebenchmarksonlyneedaround9visualtokenstoobtainanaccuracy
similar to that of using all 576 tokens; (3) Our approach provides a foundation
toexplorethebesttrade-offbetweenperformanceandvisualtokenlengthatthe
samplelevel,whereourinvestigationrevealsthatalargegapexistsbetweenthe
oracleupperboundandcurrentfixed-scalerepresentations.
1 Introduction
LargeMultimodalmodels(LMMs)[1,2,3,4,5,6,7]haveshownstrongperformanceinvisual-
linguisticunderstandingandreasoning. ModelssuchasLLaVA[2,5,4]firstembedtheinputimage
with a fixed number of visual tokens, and then feed them as prefix tokens to a Large Language
Model(LLM)[8,9]toreasonabouttheinputimage. Similarmodeldesignsareborrowedinvideo
LMMs [10, 11], where each frame contributes a fixed number of tokens to form the final video
representation.
Inreality,thenumberofvisualtokenscanbeprohibitivelylargeinthecaseofhigh-resolutionimages,
andevenmoresoforlongvideos.Existingworks[10,4,12,13]mainlytacklethisissuebyincreasing
theinputcontextlengthandconsequently,feedingalargenumbere.g.,3-8kofvisualtokensintothe
LLM.Thisapproachhasacoupleofsignificantdrawbacks: (1)theextremelylongcontextmakes
bothtrainingandinferenceinefficient;(2)anexcessivenumberofvisualtokenscanactuallyharm
theLMM’sperformance,distractingitfromattendingtotherelevantinformation,asweshowin
Sec.4.3. Severalrecentworks[14,15,16]useheuristicstopruneandmergevisualtokenstoreduce
thesequencelength. However,theyproduceasingle-lengthoutputanddonotaffordcontrolover
thefinalsequencelength,whichcouldbeusefultotradeinformationdensityversusefficiencywhile
accountingforresourceconstraintsinthedeploymentphase.
Preprint.Underreview.
4202
yaM
72
]VC.sc[
1v03471.5042:viXraM3
Describe this image for me.
In the heart of a bustling restaurant, a young girl finds solace at a
X
!! table…
X In the heart of a bustling restaurant, a young girl with vibrant hair is
!"
seated at a wooden table, her attention captivated by the camera…
…
In the heart of a bustling restaurant, a young girl with long, dark hair is
the center of attention. She's dressed in a blue and white striped
X
!# sweater,. … The table is adorned with a white paper bag, perhaps
holding her meal. A blue Pepsi cup rests on the table …
Figure1: MatryoshkaMultimodalModels. WeenforcethecoarsersetofvisualtokensX tobe
Si−1
derivedfromthefinerlevelofvisualtokensX . Asaresult,thegranularityofMatryoshkavisual
Si
tokensgraduallychangesinacontrollablemanner. TheimageisfromMSCOCO[17]validationset.
Imagesandvideosnaturallyexhibitahierarchicalstructurefromcoarsetofinedetails,andourhuman
visualsystemhasevolvedtorecognizevisualinformationinthiscoarsetofinemanner,asshownby
biologistsandpsychologistsdecadesago[18,19].CanwecreateasimilarstructureforLMMs,where
withinonesuiteofmodelweights,thevisualcontenttokensareorganizedintodifferentscalesof
granularities? Conceptually,ourgoalistolearnthevisualtokenstohaveanestedstructure,similarto
theMatryoshkaDoll[20]. MatryoshkaRepresentationLearning(MRL)[20]buildstheMatryoshka
mechanismoveraneuralnetwork’srepresentationvector,whereeachofthesegmentswithvarious
featuredimensionsiscapableofhandlingtaskslikeclassificationorretrieval. However,forLMMs,
theinefficiencymainlycomesfromthenumberoftokens. Thus,inspiredby,butdifferentfromMRL,
ourworkismotivatedtobuildMatryoshkaMultimodalModelsuponthetokenlengthdimension,so
thatwecanflexiblyadjustit.
Specifically,weproposeM3: MatryoshkaMulti-
modalModels,whichenforcesanLMMtolearn
ahierarchyofvisualrepresentationgranularities
atthetokensequencelevel,insteadofthefeature
dimensionlevelasinMRL[20].Withthisrepre-
sentation,atinferencetime,thevisualgranular-
itycanbeflexiblycontrolledbasedonspecificre-
quirements,e.g.,toaccountfortheinputimage’s
informationdensityandefficiencyconstraints.
Ourtrainingprocessissimpleandstraightfor-
ward.Duringtraining,weencodetheimageinto
M setsofvisualtokensfromcoarsetofine,X ,
Si
i = 1,··· ,M,wherethenumberofvisualto-
kensgraduallyincreases,i.e.,|X |<|X |.
Si−1 Si
Figure2: MMBenchevaluationresultsunderM3, Andimportantly,thevisualtokensinacoarser
oracleunderLLaVA-1.5-M3,LLaVA-1.5withav- levelarederivedfromthevisualtokensinafiner
eragepoolingatinferencetime, LLaVA-1.5sep- level,i.e.,X Si−1 ⊂X Si,∀i. Inthisway,thevi-
arately trained for each specific scale, and other sualinformationin[X S1,X S2,··· ,X SM]grad-
methods. M3showsasleastasgoodperformance ually includes more fine-grained details. For
asLLaVAtrainedforeachspecificscale. Alarge example,givenanaturalimageasshowninFig-
gap exists between the oracle upperbound and ure1,X S1 includeshigh-levelsemanticssuch
model’sactualperformanceonaspecificscale. astherestaurantandgirl,whileX SM includes
more details such as the Pepsi cup and white
paperbag. Allothertrainingsettings, suchas
thelossfunctionandmodelarchitecture,arekeptthesameasLLaVA[2,5,4].
Ourapproach,M3,introducesseveralnovelpropertiesandbenefitsforLMMs. First,ourapproach
can adaptively and efficiently represent visual content. Under one suite of weights, it generates
2multiplenestedsetsofvisualtokenswithdifferentgranualaritiesininformationdensity. Thisenables
flexibilityinthenumberofvisualtokensusedforanyimageduringinference,enablingcontrolover
thebesttradeoffbetweencostandperformancebasedontheimageorvideocontent. Forexample,
onecanuseallvisualtokensforimageswithdensedetailsandusejustafewtokensforsimpler
images. Thisflexibilitycanbeparticularlysignificantwhenhandlingverylongvisualsequences,
suchasvideos. Forinstance,givenafixedbudgetof2880visualtokens,ausercouldrepresenta
videoof2880frameseachwithonetokenorrepresentthesamevideobysampling5frameseach
with576tokens.
Second, our approach can be used as a general framework to evaluate the visual complexity of
vision-languagedatasetsorbenchmarks,i.e.,whichlevelofgranularityisneededinordertoperform
thegiventaskcorrectly. Surprisingly,wefindthatmostbenchmarks,especiallythosemainlycrafted
fromnaturalscenes(suchasCOCO)[21,22,23], canbehandledwellwithonly∼ 9tokensper
image. Incontrast,densevisualperceptiontaskssuchasdocumentunderstandingorOCR[24,25]
requireagreateramountoftokens(144−576tokens)perimagetohandlethetaskwell. Thedetailed
findingsarepresentedinSec.4.2.
Finally,ourapproachprovidesafoundationtotackleacriticaltaskinLMMs: Howtousetheleast
amount of visual tokens while answering the visual questions correctly?. Based on the model’s
predictionsonthetestset,wefindthatcomparedtofullvisualtokens,theoraclecanusefarfewer
tokens while performing much better. For example, under six common LMM benchmarks used
inLLaVA-NeXT[4],theoraclewiththetrainedM3 modelcanuseasfewas8.9visualtokenson
averagetoachieveperformancethatis8%pointsbetterthanLLaVA-NeXTwhichuses576tokens
perimagegrid. Thisindicatesthatthereisalargeroomforimprovementcomparedtotheoracle
upperbound,asweshowinSec.4.2.
ToenablefurtherresearchonadaptiveLMMsthatlearndiverseinformationgranularities,wepublicly
releaseourcodeandmodels.
2 RelatedWork
LargeMultimodalModels. LargeLanguageModels(LLMs)likeChatGPT[26],GPT-4[27],and
LLaMA[28]havedemonstratedimpressivereasoningandgeneralizationcapabilitiesfortext. The
landscape of LLMs has been significantly transformed by the recent introduction of models that
alsoincorporatevisualinformation,suchasGPT-4V(ision)[1]. Buildinguponopen-sourceLLMs
[28, 8], a plethora of multimodal models have made significant strides, spearheaded by models
like LLaVA [2, 5] and MiniGPT-4 [3], which combine LLaMA’s [28] language capabilities with
aCLIP[29]basedimageencoder. Recently,LMMsonmoretasksandmodalitieshaveemerged,
such as region level LMMs [30, 31, 32, 33, 34], 3D LMMs [35], and video LMMs [10, 11, 12].
However,existingLMMstypicallyrepresentthevisualcontentwithalargeandfixednumberof
tokens,whichmakesitchallengingtoscaletoverylongvisualsequencessuchashigh-resolution
imagesorlong-formvideos. Inthiswork, weproposetoadaptivelyandefficientlyrepresentthe
visualcontentbylearningmultiplenestedsetsofvisualtokens,providingflexibilityinthenumberof
visualtokensusedforanyimageduringinference.
MatryoshkaRepresentationLearning. MatryoshkaRepresentationLearning(MRL)[20]addresses
the need for flexible representations that can adapt to multiple downstream tasks with varying
computationalresources. Thisapproach,inspiredbythenestednatureofMatryoshkadolls,encodes
informationatdifferentgranularitieswithinthesamehigh-dimensionalfeaturevectorproducedby
aneuralnetwork. TheadaptabilityofMRLextendsacrossdifferentmodalities, includingvision
(ResNet[36],ViT[37]),vision+language(ALIGN[38]),andlanguage(BERT[39]),demonstrating
itsversatilityandefficiency. Recentwork[40]extendsMRLtoboththetextembeddingspaceandthe
Transformerlayersspace. OurapproachisinspiredbyMRL,butinsteadoflearningmultiplenested
embeddingsforahigh-dimensionalfeaturevector,welearn nestedvisualtokensalongthetoken
lengthdimensionforthevisualinput. WearethefirsttoshowthattheideaofMatryoshalearningcan
enableexplicitcontroloverthevisualgranularityofthevisualcontentthatanLMMprocesses.
TokenReduction. OneofthemaincausesofinefficiencyinrecentLMMsistheirlargenumberof
prefixvisualtokensthatarefedintotheLLM[2,3]. ThequadraticcomplexityinTransformers[41]
isthekeyissueinscalingtheinputsequencelengthforTransformers. Tokenreductionservesasan
effectivetechniquetoreducecomputationalcostsinTransformers. Sparseattentionmethodssuchas
Linformer[42]andReFormer[43]conductattentionoperationswithinlocalwindowsratherthanthe
3Matryoshka Multimodal Models
Text Prompt
X!!
:Describe the
X!#
scene for me.
CLIP Image
Encoder X!"
Granularity Large Language Model
Controller
: There are a group of people standing in the ski facility,
some of them are holding a green flag while other are …
Figure3: ArchitectureofourproposedMatryoshkaMultimodalModels. Thevisualfeatures
fromCLIParerepresentedasseveralgroupsofcoarse-to-finevisualtokens. Attesttime,userscan
explicitlycontrolthegranularityofthevisualfeatures.
fullcontext,therebyreducingthequadraticcomplexityofthevanillaattentionoperation. Another
notablemethodisTokenMerging(ToMe)[14],whichutilizesfullattentionbutgraduallyreduces
thenumberoftokensineachtransformerblockbyselectingthemostrepresentativetokensthrough
bipartitematchingfortheVisionTransformer(ViT).Arecentwork[44]furtherstudiesdifferent
familiesoftokenreductionmethodsforViT. However,priorapproachesproduceasinglelength
outputperinputimageanddonotoffermultiplegranularitiesoverthereducedtokensequence. Our
M3approachinsteadlearnsamulti-granularity,coarse-to-finetokenrepresentationwithinthesame
modelarchitectureandweights,enablingittoeasilybeadjustedtovariouscomputationalormemory
constraints.
3 M3: MatryoshkaMultimodalModels
OurgoalistolearnaLargeMultimodalModel(LMM)thatrepresentsvisualcontentasnestedsets
ofvisualtokenscapturinginformationacrossmultiplecoarse-to-finegranularities,sothatonecan
explicitlycontrolthevisualgranularitypertestinstanceduringinference. Hereweintroducehowwe
learnaMatryoshkadoll-liketokensequence.
LMMssuchasLLaVA[2]typicallyinputasequenceofvisualtokensasprefixtokenstotheLLM
forvisual-linguisticreasoning. Thevisualencoderfrompretrainedvision-languagemodels,suchas
CLIP[29]andSigLIP[45],istypicallyutilizedtoprojecttheimagesintothesetofvisualtokens. In
particular,theCLIPvisualencoderrepresentsaninputimageIasanH ×W gridofvisualtokens
X ,whereeachX ∈RC isaC dimensionalfeaturevector. Ourgoalistolearnnestedsetsof
H×W i
visualtokens[X ,X ,··· ,X ]whichencodethevisualinformationinacoarse-to-finemanner.
S1 S2 SM
To this end, we enforce X ⊂ X ,∀i. Importantly, we do not introduce any new learnable
Si Si+1
parameterstotheLMM.WeinsteadoptimizetheCLIPvisualencodertolearnthenestedvisual
representationdirectly,andtraintheensuingLLMtoadapttothelearnednestedsetoftokens.
Foreaseofexposition,weconsiderCLIP-ViT-L-336[29]asthevisualencoder,whereanimageisen-
codedas24×24visualtokens(576total).WecreateM setsoftokense.g.,|S |∈{1,9,36,144,576},
i
inwhichthevisualtokensatthecoarserlevelarederiveddirectlyfromthoseatthefinerlevel. Specif-
ically,giventheinitial24×24visualtokens,Wesequentiallyapply2×2poolingwithastride2,
resultingin12×12,6×6,and3×3visualtokens. Finally,weapply3×3poolingandgetthe
mostcondensedsinglevisualtoken. Inthisway,thesetsofMatryoshkavisualtokenscangradually
preservethespatialinformationintheoriginaltokenswhilesimultaneouslyformingacoarse-to-fine
nestedrepresentation.
WetrainM3 byaveragingtheautoregressivenexttokenpredictionlossforeachscaleS foreach
i
imageI . Specifically,givenaMatryoshkavisualrepresentationX forscaleS ,wemaximizethe
i Si i
4likelihoodofthepredictedtokensmatchingtheground-truthanswerX :
a
L
(cid:89)
P(X |X ,X )= P (x |X ,X ,X ), (3.1)
a Si q θ j Si q a,<j
j=1
whereθisthetrainableparametersofthemodel,whichincludesboththeCLIPvisualencoderand
theensuingLLM.X denotesthequestionintextformat,Ldenotesthetokenlengthoftheground
q
truthanswerX ,andX denotesallthegroundtruthanswertokensbeforethecurrentprediction
a a,<j
tokenx ,wherej denotesthetokenindexduringtexttokengeneration. Weomitsystemmessages
j
forclarity,thoughtheyarepartoftheconditioning. Figure3showsourmodelarchitecture.
ThefinalobjectiveaveragesoverallM visualtokenscales:
M
1 (cid:88)
min −logP(X |X ,X ). (3.2)
θ M a Si q
i=1
Withthisobjectivefunction,M3learnsnestedsetsofvisualtokensthatgraduallyincludemoredetails
withincreasingscale. Forexample,inFigure1,thesmallersetofvisualtokensdescribesthewhole
sceneatahighlevelwhilethelargersetofvisualtokensincludesmoredetailssuchasthePepsicup.
Ourtrainingobjectiveaffordsourmodeltoconductvisualquestionansweringunderanygranularity
duringinference. Thiscanbeparticularlyusefulinresourceconstrainedapplications;e.g.,thevisual
granularitycanbeflexiblyadjustedbasedontheanticipatedsimplicityorcomplexityofthevisual
contentwhiletakingintoaccountcomputeandmemoryconstraints.
4 Experiments
Inthissection,wefirstdetailtheexperimentsettingsinSec4.1. Thenweshowtheperformanceof
M3 onbothimage-levelbenchmarks4.2andvideo-levelbenchmarks4.3. Finally,weanalyzethe
behaviorofMatryoshkaMultimodalModelsandprovideablationsinSec4.4and 4.5.
4.1 ExperimentSettings
Model WeuseLLaVA-1.5[5]andLLaVA-NeXT[4]asthebaseLMMs, bothwithVicuna7B
asthelanguagemodelbackbone. Wefinetunethewholemodelusingtheexactvisualinstruction
datafromLLaVA-1.5andLLaVA-NeXT,respectively. ThelearningrateofLLMis2×10−5 and
1×10−5,respectivelyforLLaVA-1.5andLLaVA-NeXT.Thelearningrateforthevisualencoderis
2×10−5forbothmodels. Wetrainbothmodelsfor1epochusing8NVIDIAH100GPUs.
Instead of training the language model from scratch, we initialize the language model weights
frompre-trainedLLaVA-1.5andLLaVA-NeXT,whichweempiricallyworksbetter. Wenameour
MatryoshkaMultimodalModelsLLaVA-1.5-M3andLLaVA-NeXT-M3.
VisualTokenScales Wedesign5scalesforthevisualtokens.LLaVA-1.5[5]andLLaVA-NeXT[4]
bothleverageCLIP-ViT-L-336[29]asthevisualencoder,whereanimageisembeddedinto24×24
visual tokens. We gradually apply 2×2 pooling with stride 2, resulting in 12×12,6×6, and
3×3 visual tokens, where we finally apply a 3×3 pooling to get the final single visual token.
Therefore,thesizeofMatryoshkavisualtokensetsareS ∈{1,9,36,144,576},followinganested
manner. TheefficiencyanlaysisonthesystemlevelisshowninAppendixB,whereM3booststhe
speedoftheLMMprefillprocessthroughdiminishedfloating-pointoperations(FLOPs)andlessens
computationalmemoryrequirements.
Evaluations. Forimageunderstanding,weevaluateLLaVA-1.5andLLaVA-NeXTon(a)diverse
multimodalbenchmarks: POPE[22],GQA[46],MMBench[23],VizWiz[47],SEEDBench[48],
ScienceQA[49],MMMU[50],and(b)documentunderstanding/Opticalcharacterrecognition(OCR)
benchmarks: DocVQA[51],ChartQA[25],AI2D[52]andTextVQA[24].
For video understanding, we use both (a) open ended video question answering benchmarks
evaluatedbyGPT-3.5: MSVD-QA[53],MSRVTT-QA[53]andActivityNet-QA[54];and(b)multi-
choicevideoquestionansweringbenchmarks: NExT-QA[55],IntentQA[56],andEgoSchema[57].
5Table1: ComparisonbetweenLLaVA-1.5-M3acrossvariousbenchmarksundervideounderstanding
benchmarks. LLaVA-1.5-M3maintainstheperformanceofLLaVA-1.5whileoutperformingQwen-
VLandInstructBLIPwithfewertokens.
Approach #Tokens MMBench GQA POPE VizWiz SEEDBench
Qwen-VL[7] 256 38.2 59.3 - 35.2 56.3
Qwen-VL-Chat[7] 256 60.6 57.5 - 38.9 58.2
InstructBLIP-7B[58] 32 36.0 49.2 - 34.5 53.4
InstructBLIP-13B[58] 32 - 49.5 78.9 33.4 -
LLaVA-1.5-7B[5] 576 64.8 62.0 85.9 54.4 60.5
576 65.9 61.9 87.4 54.9 60.6
144 66.4 61.3 87.0 53.1 59.7
LLaVA-1.5-M3 36 64.8 60.3 85.5 52.8 58.0
9 63.1 58.0 83.4 51.9 55.4
1 59.5 52.6 78.4 49.4 50.1
Table2: ComparisonofapproacheswiththeSSbaselineandM3acrossvariousbenchmarksunder
LLaVA-NeXT[4]. Here#TokensdenotesthenumberofvisualtokensperimagegridinLLaVA-
NeXT.SSdenotesthebaselinemodeltrainedwithaSpecificScaleofvisualtokens. M3isatleastas
goodasSS,whileperformingbetterontaskssuchasTextVQA,ChartQA,andMMBench. Oracle
denotesthecasewherethebesttradeoffbetweenvisualtokensandperformanceispicked.
#TokensPer
Approach TextVQA AI2D ChartQA DocVQA MMBench POPE ScienceQA MMMU
Grid
SS 64.53 64.83 59.28 75.40 66.58 87.02 72.29 34.3
576
M3 63.13 66.71 58.96 72.61 67.96 87.20 72.46 34.0
SS 62.16 65.77 55.28 67.69 67.78 87.66 72.15 36.4
144
M3 62.61 68.07 57.04 66.48 69.50 87.67 72.32 36.1
SS 58.15 65.90 45.40 56.89 67.01 86.75 71.87 36.2
36
M3 58.71 67.36 50.24 55.94 68.56 87.29 72.11 36.8
SS 50.95 65.06 37.76 44.21 65.29 85.62 72.37 36.8
9
M3 51.97 66.77 42.00 43.52 67.35 86.17 71.85 35.2
SS 38.39 63.76 28.96 33.11 61.43 82.83 72.32 35.3
1
M3 38.92 64.57 31.04 31.63 62.97 83.38 71.19 34.8
#Tokens 31.39 11.54 41.78 64.09 8.90 6.08 7.43 22.85
Oracle
Performance 70.51 76.36 70.76 81.73 74.35 94.29 76.07 50.44
4.2 ImageUnderstanding
LLaVA-1.5-M3 WeevaluateLLaVA-1.5-M3onthecommonmultimodalunderstandingandrea-
soningbenchmarks. ResultsareshowninTable1. LLaVA-1.5-M3 withfulltokensmaintainsthe
performanceofLLaVA-1.5acrossdiversebenchmarks. Moreimportantly,ourapproachshowsstrong
performance even with 1 or 9 tokens. Specifically, in MMBench, a comprehensive multimodal
understandingbenchmark,LLaVA-1.5-M3with9tokenssurpassesQwen-VL-Chatwith256tokens,
andachievessimilarperformanceasQwen-VL-Chatwitheven1token. ComparedwithInstruct-
BLIP[58],LLaVA-1.5M3with9tokenssurpassesInstructBLIP-7BandInstructBLIP-13Bacrossall
benchmarks. Thisdemonstratesthatourmodelhasbothflexibilityandstrongempiricalperformance
underdiversenumberofvisualtokens.
LLaVA-NeXT-M3 WeusetheproposedMatryoshkaMultimodalModelstofinetuneLLaVA-NeXT,
andcompareLLaVA-NeXT-M3withSS,whichdenotesthesettingwheretheLLaVA-NeXTistrained
underaSpecificScaleofvisualtokensalsofor1epoch. Wealsoincludetheoracleupperbound
performance. Specifically,‘Oracle’denotesthecasewherethebesttradeoffbetweenvisualtokens
andperformanceispickedforeachtestinstance. Specifically,foreachtestinstance,weselectthe
thescalewiththefewestamountoftokensbutcananswerthequestioncorrectly. Resultsareshown
inTable2. Ourapproach,M3,isatleastasgoodasSS,whileperformingbetterontaskssuchas
documentunderstanding(TextVQAandChartQA)andcommonbenchmarkssuchasMMBench[23].
Ourresultsalsoshowthatdatasetlevelbiasestowardsthevisualtokenscalesdoexist. Forexample,
ScienceQAmaintainsconsistentperformanceacrossallvisualtokenscales. AI2DandMMBench
onlyencounterasmallperformancedropforevenasfewas9to1tokens. Ontheotherhand,dense
6Table3: OverallaccuracyofLLaVA-NeXT-M3andrecentvideoLMMsonvariousvideounderstand-
ingbenchmarks. Here #Tokensdenotestheoverallnumberofvisualtokensacrossallframes.
Approach #Tokens MSVD MSRVTT ActivityNet NextQA IntentQA EgoSchema
Video-LLaMA[60] - 51.6 29.6 12.4 - - -
LLaMA-Adapter[61] - 54.9 43.8 34.2 - - -
Video-ChatGPT[62] - 64.9 49.3 35.2 - - -
Video-LLaVA[63] 2048 70.7 59.2 45.3 - - -
InternVideo[64] - - - - 59.1 - 32.1
LLaVA-NeXT-7B[4] 2880 78.8 63.7 54.3 63.1 60.3 35.8
2880 78.2 64.5 53.9 63.1 58.8 36.8
720 79.0 64.5 55.0 62.6 59.6 37.2
LLaVA-NeXT-7B-M3 180 77.9 63.7 55.0 61.4 59.3 37.6
45 75.8 63.0 53.2 59.5 58.7 38.8
5 73.5 62.7 50.8 56.5 56.7 36.2
visualperceptiontaskssuchasTextVQAandDocVQAshowasignificantperformancewithfewer
tokens. ThisanalysisshowsthatM3 couldserveasaframeworktoanalyzethegranularitythata
benchmarkneeds.
Furthermore, there is a large gap between the model’s actual performance under full tokens and
theupper-boundoracle. Thisindicatesthatusingfulltokenscannotalwaysresultintheoptimal
performanceforallsamples;i.e.,thereisalargeroomofimprovementtowardstheoraclepoint.
4.3 VideoUnderstanding
FollowingIG-VLM[59],wedirectlyconductzero-shotinferenceondiversevideobenchmarksusing
LLaVA-NeXT-M3. Specifically,6framesareuniformlysampledovertheentirevideo,thenarranged
asacollage,whichisfedintoLLaVA-NeXTalongwiththequestiontogettheresponse. Results
underLLaVA-NeXT-M3andrecentvideoLMMsareshowinTable3.
LLaVA-NeXT-M3withfullvisualtokensagainshowscomparableperformancewithLLaVA-NeXT.
Moreinterestingly,resultsindicatethatfullvisualtokensusuallydonotleadtothebestperformance
invideounderstandingtasks. Specifically,on4outof6benchmarks,fullvisualtokensshowless
desirableperformancecomparedto720or180visualtokens.Wesuspectthatverylongvisualcontext
couldbringdistraction(e.g.,toomuchfocusonpotentiallyirrelevantbackground)tothemodel’s
prediction,whereacompactrepresentationofthevideofocusingonthemorerelevantinformation
maybemoreadvantageous.
Finally,formostvideounderstandingtaskssuchasActivityNet,IntentQAandEgoSchema,with9
tokensperimagegrid(45tokensintotal),theaccuracydifferencecomparedtofulltokens(2880in
total)islessthan1%. Thisdemonstratesthatthevideoquestionsinthesebenchmarksusuallyrequire
verysparsevisualinformation,asthesourceofsuchvideounderstandingbenchmarksmostlycomes
fromnaturalscenes,whichmatchesourobservationinimageunderstandingbenchmarks.
4.4 In-depthAnalysis
M3showsmuchstrongerperformancecomparedtoheuristicsbasedsamplingattesttime. A
simplewayto reducethe numberof visualtokens viaa training-freeway isto conductheuristic
tokenmergingorreduction. InTable4,wecompareM3withthreetraining-freeapproaches: average
pooling,spatialsampling,andsequentialsampling. M3ismuchmoreresilientwhenthenumberof
tokensdecreases,whiletheheuristicbasedsamplingapproachesshowdramaticperformancedrop. A
visualizationofthespatialandsequentialsamplingisshowninFigure5.
M3servesasagoodmetricforimagecomplexity. WeextracttheresponsefromLLaVA-NeXT-M3
intheTextVQAbenchmark,andshowthesampleswhereusingvisualtokensacrossdifferentscales
cananswerthequestioncorrectlyandincorrectly. ShowninFigure4,theOCRperformancealigns
withthecomplexityoftheimages,whichindicatesthatM3canbeutilizedasametrictowardssample
levelcomplexity.
Largegapbetweenoracleandactualperformance. AsshowninTable2,theoracleupper-bound
canuseveryfew(6∼64)tokensyetachieveatleast10%betterperformancecomparedtofullvisual
7Table 4: Comparison between M3, and heuristics based sampling baselines—average pooling,
spatialsampling,andsequentialsampling—atinferencetimeonMMBenchwiththeLLaVA-NeXT
architecture.
#Tokens M3 AveragePooling SpatialSampling SequentialSampling
576 67.96 67.18 67.18 67.18
144 69.50 61.68 65.81 60.14
36 68.56 50.77 60.05 44.76
9 67.35 45.45 45.45 31.96
1 62.97 19.33 26.29 22.42
Q: how much is a Q: what directive is the sign Q: what number is on the Q: what brand is the apricot Q: what beer company is a sponsor on Q: what is the telephone number of
polos crazy bike? giving? black and white sign? brandy? the score board? andrewyates?
# Tokens 577 144 36 9 1 # Tokens 577 144 36 9 1 # Tokens 577 144 36 9 1
Correct? ✔ ✔ ✔ ✔ ✔ Correct? ✔ ✔ ✔ ✘ ✘ Correct? ✘ ✘ ✘ ✘ ✘
Figure4:TextVQAtestsampleswithcorrectandincorrectpredictionsupondifferentscales.Answers
varywithdifferentnumberofvisualtokens. Inaddition,M3canserveasaframeworktoevaluatethe
complexityofimages.
tokens. Thissuggeststhatavisualtokenscalepredictor,wherethemodellearnstoautomatically
selectthebestvisualtokenscalegiventheinputimagesorbothinputimagesandquestions, has
potentialtoachieveabettertradeoff. Thiswouldbeinterestingfuturework.
Zero-shotgeneralizationtolongervisualsequences. Hereweextendthelengthofthevisual
tokensatinferencetimetostudythemodel’szero-shotgeneralizationbehavior.ResultsunderLLaVA-
NeXTareshowninTable5. HereLLaVA-NeXT-M3istrainedon2×2imagegridsbutevaluated
on 3×3 grids. We set the number of visual tokens to be 144 in each image during evaluation.
Themodelobtainsasignificantimprovementindocumentunderstandingby2.12,1.80,and4.11
onTextVQA,ChartQA,andDocVQA,respectively,whilemaintainingthesameperformanceon
benchmarksmainlycomposedofnaturalsceneimages. 3×3imagegridswith144tokenspergrid
own1440tokens,yetachievesimilarperformancewiththedefaultLLaVA-NeXT2×2imagegrids
with2880totaltokens(576tokenspergrid). Thisindicatesitispromisingtofeedmoresubimages
whilemakingthenumberofvisualtokenswithineachsubimagemuchsmaller.
4.5 AblationStudies
WeablatethekeydesignsinM3,includingthe
samplingmethodofMatryoshkavisualtokens,
andtrainingstrategy.
Matryoshka visual token sampling. Here
we compare three different ways to select the
visualtokensforMatryoshkaMultimodalMod-
els,includingaveragepooling,spatialsampling,
andsequentialsampling,whichisillustratedin
Figure5. ShowninTable6,averagingpooling
(a) Sequential (b) Spatial shows better performance than the two alter-
nativesacrossdiversebenchmarks. Ingeneral,
Figure5: Visualizationofsequentialandspatial sequentialsamplingperformstheworst. Wehy-
sampling.Given24×24girds,thevisualizedcells pothesizethatthisisduetothevisualtokenshav-
denotethesampledtokens. ing spatial information, while sequential sam-
pling does not naturally align with the spatial
distributionofthevisualtokens.
TrainingtheentireLMMvsonlytrainingCLIP. SincethenestedbehaviorofMatryoshkavisual
tokensislearnedwithintheCLIPvisualencoder,wenextevaluatewhetheritisnecessarytoalso
8Table5: PerformancecomparisonofdifferentimagegridconfigurationswithLLaVA-NeXT-M3.
#Grids #Tokenspergrid Overall#Tokens TextVQA AI2D ChartQA DocVQA MMBench POPE ScienceQA
2×2 144 720 62.61 68.07 57.04 66.48 69.50 87.67 72.32
3×3 144 1440 64.73 67.75 58.84 70.59 69.50 87.67 72.22
2×2 576 2880 63.13 66.71 58.96 72.61 67.96 87.20 72.46
Table 6: Ablation on Matryoshka visual token sampling including average pooling, sequential
sampling,andspatialsampling.
TextVQA MMBench AI2D
NumofVisTokens AvgPooling Sequential Spatial AvgPooling Sequential Spatial AvgPooling Sequential Spatial
576 63.13 59.37 60.45 67.96 64.60 64.43 66.71 65.61 64.96
144 62.61 55.80 58.33 69.50 64.18 64.52 68.07 64.90 64.96
36 58.71 52.79 52.39 68.56 63.92 64.69 67.36 64.51 64.02
9 51.97 44.05 44.19 67.35 63.14 62.11 66.77 63.70 63.92
1 38.92 28.03 29.91 62.97 59.36 57.47 64.57 63.21 63.08
finetunetheLLM.ShowninTable7,trainingthewholeLLMachievesbetterperformance. This
demonstratesthatbyalsotrainingtheLLM,themodelcanbetteradapttothepatternsofthevisual
tokensdistributedintheMatryoshkamanner.
AsexplainedinSec.3and4.1,we(a)initializetheLLMweightsfromLLaVAand(b)minimize
thelossaverageduponallvisualtokenscalesforeachsampleduringtraining. Analternativechoice
istorandomlysampleavisualtokenscale. ShowninTable8,initializingtheLLMweightsfrom
LLaVAandminimizingthelossesoverallscalesshowsconsistentperformanceboostcomparedto
usingthevanillatext-onlypre-trainedLLMweights[8]andrandomlyselectingavisualtokenscale.
InitializingtheLLMweightsfromLLaVAmakesthetrainingprocessofM3morestable. Bylearning
allscalesatonce,themodelisforcedtolearnthenestedbehaviorforeachsample,whichleadsto
betterperformance.
5 ConclusionandFutureWork
We introduced M3: Matryoshka Multimodal Models, which learns to represent visual content as
nestedsetsofvisualtokens,capturinginformationacrossmultiplecoarse-to-finegranularities.LMMs
equippedwithM3affordexplicitcontroloverthevisualgranularitypertestinstanceduringinference.
We alsoshowed that M3 can serve as an analysis framework to investigate the visual granularity
neededforexistingdatasets,wherewediscoveredthatalargenumberofmultimodalbenchmarks
only need as few as 9 visual tokens to obtain accuracy similar to that of using all visual tokens,
especiallyforvideounderstanding. Furthermore,wedisclosedalargeperformance-efficiencygap
betweentheoracleupper-boundandthemodel’sperformance.
Ourworkcanbenaturallyextendedtootherdomains. Forexample,thelongcontextinatext-only
LLM or vision tokens in dense vision tasks can also be represented as nested sets of tokens in a
Matryoshkamanner. Onelimitationofourcurrentapproachisthatwearelackinganeffectivevisual
token predictor that can bridge the gap between the oracle and LMM’s actual performance at a
specificscale. Webelievethiswouldbeanexcitingnextdirectionofresearchinthisspace.
Acknowledgement
ThisworkwassupportedinpartbyNSFCAREERIIS2150012,andInstituteofInformation&com-
municationsTechnologyPlanning&Evaluation(IITP)grantsfundedbytheKoreagovernment(MSIT)
(No. 2022-0-00871,DevelopmentofAIAutonomyandKnowledgeEnhancementforAIAgentCol-
laboration)and(No. RS2022-00187238,DevelopmentofLargeKoreanLanguageModelTechnology
forEfficientPre-training),andMicrosoftAccelerateFoundationModelsResearchProgram.
References
[1] OpenAI. Gpt-4v(ision)systemcard. https://cdn.openai.com/papers/GPTV_System_
Card.pdf,2023.
9Table7: PerformancecomparisonoftrainingLLaVA-NeXT-M3withandwithouttrainingtheLLM
acrossdiversebenchmarks. WeseeacleardropwhenfreezingtheLLM.
NumofVisTokens TextVQA MMBench AI2D DocVQA
w/LLM w/oLLM w/LLM w/oLLM w/LLM w/oLLM w/LLM w/oLLM
576 63.13 61.16 67.96 63.66 66.71 63.92 72.61 69.15
144 62.61 57.79 69.50 65.21 68.07 63.73 66.48 59.77
36 58.71 49.75 68.56 63.92 67.36 62.89 55.94 44.08
9 51.97 36.15 67.35 61.08 66.77 62.05 43.52 28.36
1 38.92 19.72 62.97 51.80 64.57 60.59 31.63 17.37
Table8: Impactof(a)initializingtheLLMweightsfromLLaVA,and(b)averagingthelossfromall
scalesvsrandomlyselectingascaleforeachsampleduringtraining.
Technique TextVQA AI2D
InitLLMweightsfromLLaVA ✓ ✓ ✓ ✓
Averagelossesoverallscales ✓ ✓ ✓ ✓
576 60.36 62.25 61.01 63.13 62.40 65.06 65.84 66.71
144 59.61 61.02 59.80 62.61 63.67 65.61 65.77 68.07
36 54.86 55.91 55.32 58.71 63.67 65.32 66.68 67.36
9 46.84 47.04 48.80 51.97 63.02 64.83 65.38 66.77
1 33.78 33.68 36.05 38.92 61.53 63.21 63.37 64.57
[2] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.NeurIPS,
2023.
[3] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4: Enhanc-
ingvision-languageunderstandingwithadvancedlargelanguagemodels. ICLR,2024.
[4] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,January2024.
[5] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning,2024.
[6] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,Zhuoyi
Yang,LeiZhao,XixuanSong,JiazhengXu,BinXu,JuanziLi,YuxiaoDong,MingDing,and
JieTang. Cogvlm: Visualexpertforpretrainedlanguagemodels,2023.
[7] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,
localization,textreading,andbeyond. arXivpreprintarXiv:2308.12966,2023.
[8] Vicuna. Vicuna: Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality. https:
//vicuna.lmsys.org/,2023.
[9] Meta. Llama-3. https://ai.meta.com/blog/meta-llama-3/,2024.
[10] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunited
visualrepresentationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[11] HangZhang,XinLi,andLidongBing.Video-llama:Aninstruction-tunedaudio-visuallanguage
modelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023.
[12] YuanhanZhang,BoLi,haotianLiu,YongjaeLee,LiangkeGui,DiFu,JiashiFeng,ZiweiLiu,
andChunyuanLi. Llava-next: Astrongzero-shotvideounderstandingmodel,April2024.
[13] GeminiTeam. Gemini: Afamilyofhighlycapablemultimodalmodels,2024.
[14] DanielBolya,Cheng-YangFu,XiaoliangDai,PeizhaoZhang,ChristophFeichtenhofer,and
JudyHoffman. Tokenmerging: YourViTbutfaster. InInternationalConferenceonLearning
Representations,2023.
10[15] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao
Chang. Animageisworth1/2tokensafterlayer2: Plug-and-playinferenceaccelerationfor
largevision-languagemodels. arXivpreprintarXiv:2403.06764,2024.
[16] YuzhangShang,MuCai,BingxinXu,YongJaeLee,andYanYan. Llava-prumerge: Adaptive
tokenreductionforefficientlargemultimodalmodels. arXivpreprintarXiv:2403.15388,2024.
[17] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Dolla´r,andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InComputer
Vision–ECCV2014: 13thEuropeanConference,Zurich,Switzerland,September6-12,2014,
Proceedings,PartV13,pages740–755.Springer,2014.
[18] MikeGHarrisandChristosDGiachritsis. Coarse-grainedinformationdominatesfine-grained
informationinjudgmentsoftime-to-contactfromretinalflow. Visionresearch,40(6):601–611,
2000.
[19] JayHegde´. Timecourseofvisualperception: coarse-to-fineprocessingandbeyond. Progress
inneurobiology,84(4):405–439,2008.
[20] AdityaKusupati,GantavyaBhatt,AniketRege,MatthewWallingford,AdityaSinha,VivekRa-
manujan,WilliamHoward-Snyder,KaifengChen,ShamKakade,PrateekJain,etal.Matryoshka
representationlearning. AdvancesinNeuralInformationProcessingSystems,35:30233–30249,
2022.
[21] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making
thevinvqamatter: Elevatingtheroleofimageunderstandinginvisualquestionanswering.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
6904–6913,2017.
[22] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluating
objecthallucinationinlargevision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
[23] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
[24] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi
Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages8317–8326,2019.
[25] AhmedMasry,DoLong,JiaQingTan,ShafiqJoty,andEnamulHoque. ChartQA:Abenchmark
for question answering about charts with visual and logical reasoning. In Findings of the
AssociationforComputationalLinguistics: ACL2022,pages2263–2279,Dublin,Ireland,May
2022.AssociationforComputationalLinguistics.
[26] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/,2023.
[27] OpenAI. Gpt-4technicalreport. 2023.
[28] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Tim-
othe´eLacroix,BaptisteRozie`re,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[29] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748–8763.PMLR,2021.
[30] MuCai,HaotianLiu,SivaKarthikMustikovela,GregoryP.Meyer,YuningChai,DennisPark,
andYongJaeLee. Makinglargemultimodalmodelsunderstandarbitraryvisualprompts. In
IEEEConferenceonComputerVisionandPatternRecognition,2024.
[31] ShilongZhang,PeizeSun,ShoufaChen,MinXiao,WenqiShao,WenweiZhang,KaiChen,
andPingLuo. Gpt4roi: Instructiontuninglargelanguagemodelonregion-of-interest. arXiv
preprintarXiv:2307.03601,2023.
11[32] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra:
Unleashingmultimodalllm’sreferentialdialoguemagic. arXivpreprintarXiv:2306.15195,
2023.
[33] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,ShaohanHuang,ShumingMa,andFuru
Wei. Kosmos-2: Groundingmultimodallargelanguagemodelstotheworld. arXivpreprint
arXiv:2306.14824,2023.
[34] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang,
JianfengGao,LeiZhang,ChunyuanLi,andJianweiYang. Llava-grounding: Groundedvisual
chatwithlargemultimodalmodels,2023.
[35] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and
ChuangGan. 3d-llm: Injectingthe3dworldintolargelanguagemodels. NeurIPS,2023.
[36] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. InCVPR,2016.
[37] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
JakobUszkoreit,andNeilHoulsby. Animageisworth16x16words: Transformersforimage
recognitionatscale. ICLR,2021.
[38] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocLe,Yun-Hsuan
Sung,ZhenLi,andTomDuerig. Scalingupvisualandvision-languagerepresentationlearning
withnoisytextsupervision. InInternationalconferenceonmachinelearning,pages4904–4916.
PMLR,2021.
[39] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,
2018.
[40] XianmingLi,ZongxiLi,JingLi,HaoranXie,andQingLi.2dmatryoshkasentenceembeddings.
arXivpreprintarXiv:2402.14776,2024.
[41] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InAdvancesinNeuralInforma-
tionProcessingSystems,pages5998–6008,2017.
[42] SinongWang,BelindaZ.Li,MadianKhabsa,HanFang,andHaoMa.Linformer:Self-attention
withlinearcomplexity,2020.
[43] NikitaKitaev,LukaszKaiser,andAnselmLevskaya. Reformer: Theefficienttransformer. In
InternationalConferenceonLearningRepresentations,2020.
[44] Joakim Bruslund Haurum, Sergio Escalera, Graham W. Taylor, and Thomas B. Moeslund.
Whichtokenstouse? investigatingtokenreductioninvisiontransformers. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision(ICCV)Workshops,October2023.
[45] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for
languageimagepre-training. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages11975–11986,2023.
[46] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual
reasoningandcompositionalquestionanswering. InCVPR,2019.
[47] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,
andJeffreyPBigham. Vizwizgrandchallenge: Answeringvisualquestionsfromblindpeople.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
3608–3617,2018.
[48] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-
bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint
arXiv:2307.16125,2023.
12[49] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. AdvancesinNeuralInformationProcessingSystems,
2022.
[50] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,
DongfuJiang,WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,
MingYin,BoyuanZheng,ZhenzhuYang,YiboLiu,WenhaoHuang,HuanSun,YuSu,and
WenhuChen. Mmmu: Amassivemulti-disciplinemultimodalunderstandingandreasoning
benchmarkforexpertagi. InProceedingsofCVPR,2024.
[51] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on
document images. In Proceedings of the IEEE/CVF winter conference on applications of
computervision,pages2200–2209,2021.
[52] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAli
Farhadi. Adiagramisworthadozenimages. InBastianLeibe, JiriMatas, NicuSebe, and
MaxWelling,editors,ComputerVision–ECCV2016,pages235–251,Cham,2016.Springer
InternationalPublishing.
[53] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,XiangnanHe,andYuetingZhuang.
Video question answering via gradually refined attention over appearance and motion. In
Proceedingsofthe25thACMinternationalconferenceonMultimedia,pages1645–1653,2017.
[54] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao.
Activitynet-qa: A dataset for understanding complex web videos via question answering.
InAAAI,volume33,pages9127–9134,2019.
[55] JunbinXiao,XindiShang,AngelaYao,andTat-SengChua. Next-qa: Nextphaseofquestion-
answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pages9777–9786,2021.
[56] JiapengLi, PingWei, WenjuanHan, andLifengFan. Intentqa: Context-awarevideointent
reasoning. InInt.Conf.Comput.Vis.,pages11963–11974,2023.
[57] KarttikeyaMangalam,RaiymbekAkshulakov,andJitendraMalik. Egoschema: Adiagnostic
benchmarkforverylong-formvideolanguageunderstanding. InAdv.NeuralInform.Process.
Syst.,2024.
[58] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose
vision-languagemodelswithinstructiontuning,2023.
[59] WonkyunKim,ChanginChoi,WonseokLee,andWonjongRhee. Animagegridcanbeworth
avideo: Zero-shotvideoquestionansweringusingavlm. arXivpreprintarXiv:2403.18406,
2024.
[60] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An instruction-tuned audio-visual
languagemodelforvideounderstanding. InConf.EmpiricalMethodsinNaturalLanguage
Processing,pages543–553,2023.
[61] RenruiZhang, JiamingHan, AojunZhou, XiangfeiHu, ShilinYan, PanLu, HongshengLi,
PengGao,andYuQiao. Llama-adapter: Efficientfine-tuningoflanguagemodelswithzero-init
attention. arXivpreprintarXiv:2303.16199,2023.
[62] MuhammadMaaz,HanoonaAbdulRasheed,SalmanKhan,andFahadShahbazKhan. Video-
chatgpt: Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. ArXiv
abs/2306.05424,2023.
[63] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunited
visualrepresentationbyalignmentbeforeprojection. ArXivabs/2311.10122,2023.
13[64] YiWang,KunchangLi,YizhuoLi,YinanHe,BingkunHuang,ZhiyuZhao,HongjieZhang,
Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang,
LiminWang,andYuQiao. Internvideo: Generalvideofoundationmodelsviagenerativeand
discriminativelearning. ArXivabs/2212.03191,2022.
[65] ZhihangYuan,YuzhangShang,YangZhou,ZhenDong,ChenhaoXue,BingzheWu,ZhikaiLi,
QingyiGu,YongJaeLee,YanYan,etal. Llminferenceunveiled: Surveyandrooflinemodel
insights. arXivpreprintarXiv:2402.16363,2024.
[66] Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd:
Activation-awaresingularvaluedecompositionforcompressinglargelanguagemodels. arXiv
preprintarXiv:2312.05821,2023.
14A BroaderImpact
ThebroaderimpactofM3,aframeworkwithnestedvisualrepresentations,haspotentialbenefits
andrisksassociatedwithitsdeploymentandrelease. Ourmodelistrainedusingtheexactsame
architectureanddataofLLaVA-1.5[5]andLLaVA-NeXT[4]. AlltheconcernsaresameasLLaVA.
Specifically,asoneexample,LLaVAconductsinstructiontuningusingGPT-4andGPT-4Vgenerated
data. ThebiasfromGPT-4andGPT-4VwouldstillexistinLLaVA.
B EfficiencyAnalysis
To illuminate the computational benefits conferred by M3, we employ the roofline-based LLM-
Vieweranalysisasdetailedin[65]. Ouranalysisissetwithinahypotheticalcontextdesignedto
emphasizetheeffectsofM3onprocessingefficiencyinLMMs. WestudytheLLaVA-1.5casewhere
a336×336resolutionimageisprocessedusingaCLIP-ViTimageencoder,resultingin576visual
tokens. Accompanied by a text prompt with an assumed number of 30 tokens, the nested visual
tokensinM3 substantiallylowersthevisualtokencount. Theconsequencesofthisreductionare
substantial as outlined in Table 9, detailing the computational costs involved in the LMM prefill
process. Notably, M3 not only boosts the speed of the LMM prefill process through diminished
floating-pointoperations(FLOPs)butalsolessenscomputationalmemoryrequirements.
ItiscrucialtohighlightthattheadvantagesofM3 arenotlimitedtojustefficiencyimprovements.
ThetokenreductionapproachofM3 canalsoenhanceotherLMMaccelerationmethods,suchas
quantizationandfactorization,asreferencedin[66]. Thiscomplementaryrelationshipaccentuates
thebroadpotentialofM3tocontributetoawiderarrayofefficiency-boostingstrategies.
Table9:ComputationCostAnalysis.ThedevelopmentdeviceisTeslaV100GPU,andtimeestimated
bytherooflinemodelrepresentsthetheoreticalperformancethatthehardwarecanachieve.
#Tokens FLOPs(TB) PrefillTime(ms) TotalMemory(GB) StoringActivation(GB)
576 8.0 58.1 21.6 3.8
144 2.2 19.5 15.0 0.7
36 0.9 18.0 13.8 0.3
9 0.5 17.7 13.6 0.2
1 0.4 17.6 13.5 0.1
C MoreVisualizationsonNestedVisualRepresentation
ShowninFigure6,withmorevisualtokens,LMMscandiscovermoredetails,suchasfurnitureand
humanattributes. Besides,LMMscangeneratehigherqualitydescriptionswithmorevisualtokens,
asdemonstratedbytheOCRcapabilityinFigure6(b).
15M3
Describe this image for me.
X The image shows an interior space that appears to be a living room
!! or a combined living and dining area ...
The image shows an interior space that appears to be a living room or
X !" a lobby. The room has a warm color scheme with beige walls and a
darker brown floor. There is a large, L-shaped sofa…
…
The image shows an interior space that appears to be a living room or
a combined living and dining area…There is a large, L-shaped sofa with
X
!# a light-colored upholstery, positioned in the center of the room. In
front of the sofa, there is a glass-top coffee table with various…
(a)
M3
Describe this image for me.
X This is a black and white photograph capturing a moment from a
!! baseball game. In the foreground, there are three individuals…
This is a black and white photograph capturing a moment from a
X !" baseball game. In the foreground, three baseball players are standing
on a field. The player on the left is wearing a baseball uniform with
the name "KIMBLE" on the front, a cap, and a glove..
…
This is a black and white photograph capturing a moment from a
baseball game. In the left section, we see a player from the Kimberly
X !# team. He is dressed in a white baseball uniform with the word
"KIMBERLY" emblazoned across the chest. He is holding a baseball
glove, ready for action.
(b)
Figure6: Morevisualizationexamples. Withmorevisualtokens,LMMscandiscovermoredetails,
andgeneratehigherqualitydescriptions. TheimagesarefromMSCOCO[17]validationset.
16