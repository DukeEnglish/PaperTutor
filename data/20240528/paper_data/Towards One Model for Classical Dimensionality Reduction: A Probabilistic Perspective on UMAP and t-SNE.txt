UnderreviewfortheWorkshopatthe6thSymposiumonAdvancesinApproximateBayesianInference,20241–14
Towards One Model for Classical Dimensionality Reduction:
A Probabilistic Perspective on UMAP and t-SNE
Aditya Ravuri
University of Cambridge
Neil D. Lawrence
University of Cambridge
Abstract
This paper shows that the dimensionality reduction methods, UMAP and t-SNE, can be
approximately recast as MAP inference methods corresponding to a generalized Wishart-
based model introduced in Ravuri et al. (2023). This interpretation offers deeper theoretical
insights into these algorithms, while introducing tools with which similar dimensionality
reduction methods can be studied.
1. Introduction
In the realm of single-cell biology and various other domains with complex, high-dimensional
data, dimensionality reduction (DR) algorithms are essential tools for uncovering the
underlying structure of data. These algorithms, which include very widely-used techniques
like t-SNE (van der Maaten and Hinton, 2008) and UMAP (McInnes et al., 2020), are
especially valuable for visualizing data manifolds, enabling downstream processing, and
the discovery of insightful patterns. A deeper comprehension of these algorithms and their
theoretical underpinnings is crucial for advancing their applicability, particularly when prior
information is available and improving their interpretability. Our work builds on (and aims
to unify fully) the ProbDR framework, which interprets classical DR methods through a
probabilisticlenstoenablethecommunicationofassumptions, integrationofpriorknowledge
and accounting for noise and confounders.
Ravuri et al. (2023) introduced ProbDR as a framework with two main interpretations:
UMAP and t-SNE corresponding to inference over an adjacency matrix, and other classical
DR algorithms that utilize eigendecomposition of a PSD matrix as inference using a Wishart
generative model on the covariance/precision matrix.
In this work, we further simplify the framework, moving away from the variational
interpretation and propose that all algorithms with ProbDR interpretations (and hence
most classical DR methods) can be written as MAP inference algorithms given the model,
S|X ∼ W{−1}(XXT +ϵHK (X)H+γI,ν), (1)
t
where S ∈ S+ is an estimate of a covariance matrix, X ∈ Rn,q corresponds to the set of low
n
(q) dimensional latent variables, H = I−11T/n is a centering matrix, and K (X ,X ) =
t i j
(1+∥X −X ∥2)−1 is the Student-t kernel.
i j
WeaimtointerprettheseDRalgorithmsasMAPinferencemethodsinsteadofvariational
methods as studied in Ravuri et al. (2023); Van Assel et al. (2022). This unifies t-SNE and
UMAP-like algorithms with the other algorithms and provides semantic interpretation to
UMAP and t-SNE. Additionally, we hope that the tools introduced in this paper will provide
researchers with more machinery to understand the behaviour of latent variable models.
© A.Ravuri&N.D.Lawrence.
4202
yaM
72
]LM.tats[
1v21471.5042:viXraRavuri Lawrence
2. Background
The ProbDR framework showed that many classical DR algorithms can be expressed
as inference algorithms corresponding to a probabilistic model. Algorithms that set the
embedding of a high-dimensional dataset Y ∈ Rn,d in terms of the eigenvectors of a positive-
semi-definite matrix were shown in Ravuri et al. (2023) to correspond to a two-step inference
process, where,
1. one first estimates a covariance matrix S(Y) or a precision matrix Γ(Y) (which a
graph Laplacian L can be an estimate of),
2. then estimates the embedding via maximum a-posteriori given one of the two following
models,
S|X ∼ W(cid:0) XXT +σ2I ,d(cid:1) or,
n
Γ|X ∼ W(cid:0) (XXT +βI )−1,d(cid:1) .
n
PCoA, for example, is recovered using the first of these formulations with S ≡ YYT. Setting
ϵ = 0 in Equation (1) recovers these results.
In the case of UMAP and t-SNE, ProbDR did not specify a generative model for either
the data or the covariance but only a generative model for the adjacency matrices that
describe the nearest neighbour graph.
Specifically, ProbDR showed that the generative models corresponding to UMAP and
t-SNE can be seen as models for adjacency matrices A′,
(cid:40)
Categorical(vec(A′)|wt (X)) t-SNE
p(A′|X)= ij (2)
(cid:81)n Bernoulli(A′ |wU(X ,X )) UMAP
i>j ij ij i j
where,
(1+∥X −X ∥2)−1 1
wt = i j , and wU = .
ij (cid:80) (1+∥X −X ∥2)−1 ij 1+∥X −X ∥2
k̸=l k l i j
The inference can be done as maximum a-posteriori estimation for X, given the binary,
empirical,nearest-neighbouradjacencymatrixA′(Y) = I(j ∈ N(i)),whereN(i)represents
ij
the set of nearest neighbours of data point Y . UMAP and t-SNE are typically interpreted in
i
a variational way, however the inference trivially becomes MAP inference when we use A′(Y)
as the variational data-dependent distribution (due to the Ravuri et al. (2023), Appendix
B.7, Lemma 13). This interpretation is also presented in Damrich et al. (2022).
This simplification is reasonable due to the findings of Damrich and Hamprecht (2021),
where it was found that the relatively complex calculation of the variational probabilities
in t-SNE and UMAP can be replaced with simply the adjacency matrices without loss of
performance. Our initial experiments also closely aligned with these findings.
Crucially, however, Becht et al. (2019); Damrich et al. (2022) note that the optimisation
process is equally as important. As part of an extensive study on the nature of the t-SNE
and UMAP loss functions, Damrich et al. (2022) then show how the stochastic optimisation
of t-SNE and UMAP can be interpreted to be contrastive estimation with the loss
(cid:18) (cid:19) (cid:18) (cid:19)
w (X) w (X)
L(X) ∝ −E log ij −mE log 1− ij , (3)
ij∼p ij∼ξ
w (X)+1 w (X)+1
ij ij
2Towards One Model for DR: A Probabilistic Perspective on UMAP and t-SNE
where p represents a discrete distribution that is uniform across the nearest neighbour pairs
and zero everywhere else. Similarly, ξ represents a uniform distribution over non-neighbours.
m (set to 2n ) is a multiplicative hyperparameter proportional to the number of contrastive
−
negatives that affects the strength of repulsion. In Damrich et al. (2022), w (X) = 1
ij ∥Xi−Xj∥2
recovers UMAP. This bound is important, as we found that a naive optimisation of the
Bernoulli likelihood in Equation (2) leads to a poor embedding (although Appendix B offers
more commentary on this bound as a Bernoulli likelihood and provides more evidence for
our claims).
For this work, we aim to work with such a contrastive loss function and interpret it
as a likelihood, but over the latents X. This is because we were particularly inspired by
Nakamura et al. (2023), who showed that contrastive learning methods could be seen as
variational algorithms (hence suggesting a link between t-SNE and contrastive learning) and
by Gutmann and Hyv¨arinen (2010), which shows that contrastive losses are estimators of
negative log-likelihoods. Damrich et al. (2022) also greatly simplify the UMAP optimisation
process.
3. Discussion
This section argues that inference with the generative model in Equation (1) approximately
recovers UMAP and t-SNE-like algorithms.
3.1. The proposed model
We first consider MAP inference for X given the model,
νL|X ∼ W((−αHD′H+γI)−1,ν), (4)
X ∼ Uniform(−∞,∞),
where D′ is a squared distance matrix, with elements D′ = log(1 + ∥X − X ∥2). Let
ij i j
M = −HD′H. M is PSD (with some interesting properties relating to isometric Euclidean
embeddings, see Appendix A.2). The graph Laplacian is computed as L = (D−A)/d¯, with
d¯being the average degree1. The adjacency matrix A = 1 if p(ij) (of Equation (3)) > 0.
ij
Then, the log-likelihood given the model in Equation (4) is as follows (we focus on just
the data-dependent term),
logp(X|L) = −0.5νtr(L(αM+γI))+0.5νlogdet(αM+γI)+c
∝ −αtr(LM)+logdet(αM+γI)+k
= αtr(LHD′H)+logdet(αM+γI)+k
=
αtr(cid:0) LD′(cid:1)
+logdet(αM+γI)+k (trace cyclic and L centered)
= −αtr(AD′)+logdet(αM+γI)+k tr(D′) = 0
(cid:88)
= −α ATD′ +logdet(αM+γI)+k
i i
i
(cid:88)(cid:88)
= −α a log(1+∥X −X ∥2)+logdet(αM+γI)+k
ij i j
i j
1. So that its inverse (i.e. the covariance) has diagonal elements around one.
3Ravuri Lawrence
Figure 1: Embeddings of 30k MNIST digits obtained using inference within Equation (4).
Left: A run with both α = 1/50 and γ = 1/5 treated as hyper-parameters.
Right: A run with α = 1/50 treated as a hyperparameter and γ optimised using
maximum likelihood inference. Appendix B sheds light into why this choice of
hyper-parameters is performant, leading to the embedding of Figure 4.
(cid:18) (cid:19)
n 1
= α
+E
log +logdet(αM+γI)+k
d¯ ij∼p 1+∥X −X )∥2
i j
(cid:18) (cid:19)
w (X ,X )
= αnE log ij i j +logdet(αM+γI)+k,
ij∼p
1+w (X ,X )
ij i j
with n = nd¯≈ 1.5nn being the number of total number of edges and n is the number
+ # #
of neighbours per point (typically set to 15). An important note here is that the model
is misspecified (for example, the variance implied by the covariance parameter and the
data are quite different, with the variance of the Wishart being much lower than the data
estimate).
Therefore, by minimising the first term of Equation (3), we maximise the data-dependent
termofthelikelihoodofEquation(4),andso,Equation(4)definesamodelfordimensionality
reduction that in some ways is similar to t-SNE and UMAP-like algorithms. Figure 1 shows
embeddings of 30,000 digits from the MNIST dataset obtained using this model. We also
run this model on a suite of other datasets (from Pedregosa et al. (2011); Deng (2012); sta;
Krumsiek et al. (2011)) to show that we recover roughly similar embeddings as minimisation
of Equation (3) in Figure 2.
3.2. CNE bounds can sometimes be approximated using the proposed model
Consider the negative CNE loss,
(cid:18) (cid:19) (cid:18) (cid:19)
ανn w (X) ανn w (X)
−L ∝ E log ij + mE log 1− ij
ij∼p ij∼ξ
2 w (X)+1 2 w (X)+1
ij ij
ν ανn (cid:18) ∥X −X ∥2 (cid:19)
= − tr(L(αM+γI))+ mE log i j Sec. 3.1
2 2 ij∼ξ 1+∥X −X ∥2
i j
4Towards One Model for DR: A Probabilistic Perspective on UMAP and t-SNE
Figure 2: Left: embeddings on various datasets obtained using optimisation of the CNE
bound (Equation (3)) compared with right: inference results using our model.
.5
dlofinam
stigid
snoitator
rpxe_eneg
elttuhs
dlofinam
stigid
snoitator
rpxe_eneg
elttuhsRavuri Lawrence
Let the first term be represented as T . We first approximate the distances D′ = log(1+
a ij
d2 ) ≈ 0.5(1+d2 −1/(1+d2 )) within T as this is a good approximation for the log1p
ij ij ij a
function near 0. Therefore,
M ≈ −H(11T+0.5D2−0.5K )H = XXT +HK H. App. A.1
t t
This approximation sheds light on the behaviour of the covariance parameter of Equation (4)
and draws the link to Equation (1).
Then,
ανn (cid:18) ∥X −X ∥2 (cid:19)
−L ∝ T + mE log i j
a 2 ij∼ξ 1+∥X −X ∥2
i j
(cid:18) (cid:19)
ανmn 1
≈ T − E (for large distances)
a 2 ij∼ξ 1+∥X −X ∥2
i j
(cid:18) (cid:19)
ανmn 1
≈ T − E as n >> n
a 2 ij∼U 1+∥X −X ∥2 #
i j
ανmn (cid:88) 1
= T −
a 2n2 1+∥X −X ∥2
i j
ij
(cid:18) (cid:19)
ανm 1 1 1
= T + tr − 11TK − K 11T+ 11TK 11T
a 2 n t n t n2 t
νm
= T + tr(αHK H)+k
a t
2
ν ν αmν
= − tr(L(αM +γI))+ tr(αmM +γmI)− tr(XXT)+k
aprx aprx
2 2 2
(cid:18) (cid:19)
mν α mν α αmν
∝ − tr(m−1L( M +I))+ tr M +I − tr(XXT)+k
aprx aprx
2 γ 2 γ 2γ
(cid:18) (cid:19)
mν α mν α αmν
≈ − tr(m−1L( M +I))+ logdet M +I − tr(XXT)+k (large γ)
aprx aprx
2 γ 2 γ 2γ
= logW(νL|m−1(αM +γI)−1,mν)+logN(X|0,γI/αmν)
aprx
This shows the NCE bound can be approximated by the likelihood of our proposed
model with a diffuse prior on X. Note that the model is still misspecified. A comparison of
embeddings used with this approximation and the CNE bound are shown in Figure 3.
4. Conclusion
We present a probabilistic interpretation of t-SNE and UMAP-like algorithms, showing
that they correspond to inference within misspecified generative models for the covari-
ance/precision with fixed scale parameters, and a choice of a covariance that describes
non-linear functions within a Gaussian process context. We hope that this serves as a
foundation to further refine these models (particularly based on their regularisation terms)
such that they emulate results from t-SNE and UMAP-like algorithms.
6Towards One Model for DR: A Probabilistic Perspective on UMAP and t-SNE
Figure 3: Left: Embedding of 30k digits from MNIST obtained using the CNE bound and
right: using our approximation. For this, we set n = 1, which was the best
−
performing case.
Figure 4: Embedding of 30k MNIST digits resulting from ProbDR inference using a scaled
Student-t kernel. See Appendix B.
7Ravuri Lawrence
Acknowledgments
AR would like to thank Francisco Vargas for helpful discussions and a studentship from the
Accelerate Programme for Scientific Discovery.
References
Statlog (Shuttle). UCI Machine Learning Repository. URL https://doi.org/10.24432/
C5WS31.
Etienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel W. H.
Kwok, Lai Guan Ng, Florent Ginhoux, and Evan W. Newell. Dimensionality reduction
for visualizing single-cell data using UMAP. Nature Biotechnology, 37(1):38–44, Jan 2019.
ISSN 1546-1696. doi: 10.1038/nbt.4314. URL https://doi.org/10.1038/nbt.4314.
Rajendra Bhatia. Positive Definite Matrices. Princeton University Press, 2007. ISBN
9780691129181. URL http://www.jstor.org/stable/j.ctt7rxv2.
Sebastian Damrich and Fred A Hamprecht. On umap's true loss function. In M. Ranzato,
A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in
Neural Information Processing Systems, volume 34, pages 5798–5809. Curran Associates,
Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/
2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf.
Sebastian Damrich, Niklas B¨ohm, Fred A Hamprecht, and Dmitry Kobak. From t-sne to
umap with contrastive learning. In The Eleventh International Conference on Learning
Representations, 2022.
Li Deng. The mnist database of handwritten digit images for machine learning research
[best of the web]. IEEE Signal Processing Magazine, 29(6):141–142, 2012. doi: 10.1109/
MSP.2012.2211477.
Arjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC,
2018.
Michael Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation
principle for unnormalized statistical models. In Proceedings of the thirteenth international
conference on artificial intelligence and statistics, pages 297–304. JMLR Workshop and
Conference Proceedings, 2010.
Apoorva Khare. Schoenberg: from metric geometry to matrix positivity, Apr 2019. URL
https://math.iisc.ac.in/seminar-slides/2019/2019-04-12-ApoorvaKhare.pdf.
Jan Krumsiek, Carsten Marr, Timm Schroeder, and Fabian J Theis. Hierarchical differentia-
tion of myeloid progenitors is encoded in the transcription factor network. PLoS One, 6
(8):e22649, August 2011.
Leland McInnes, John Healy, and James Melville. UMAP: Uniform manifold approximation
and projection for dimension reduction, 2020.
8Towards One Model for DR: A Probabilistic Perspective on UMAP and t-SNE
Hiroki Nakamura, Masashi Okada, and Tadahiro Taniguchi. Representation uncertainty
in self-supervised learning as variational inference. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 16484–16493, 2023.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011.
Aditya Ravuri, Francisco Vargas, Vidhi Lalchand, and Neil D Lawrence. Dimensionality
reduction as probabilistic inference. In Fifth Symposium on Advances in Approximate
Bayesian Inference, 2023. URL https://arxiv.org/pdf/2304.07658.pdf.
I. J. Schoenberg. Remarks to maurice frechet’s article “sur la definition axiomatique
d’une classe d’espace distances vectoriellement applicable sur l’espace de hilbert. Annals
of Mathematics, 36(3):724–732, 1935. ISSN 0003486X. URL http://www.jstor.org/
stable/1968654.
Hugues Van Assel, Thibault Espinasse, Julien Chiquet, and Franck Picard. A probabilistic
graph coupling view of dimension reduction. Advances in Neural Information Processing
Systems, 35:10696–10708, 2022.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of
Machine Learning Research, 9(86):2579–2605, 2008. URL http://jmlr.org/papers/v9/
vandermaaten08a.html.
Appendix A. Centered Distance Based Results
A.1. The MDS Matrix is a Gram matrix
Assume centered X (i.e. the column means of X are zero). Then,
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
−HDH = − I− 11T D I − 11T
n n
(cid:18) (cid:19) (cid:18) (cid:19)
1 (cid:16) (cid:17) 1
= − I− 11T D˜11T +11TD˜ −2XXT I − 11T
n n
...
2 2 2
= 2XXT − XXT11T − 11TXXT + 211TXXT11T
n n n2
= 2XXT
A.2. -HD′H is PSD
D′0.5 (and D′ ) are valid distance metrics. We only need to show that −D′ is CPSD,
ij ij
then, there exists an isometric Euclidean embedding Khare (2019); Schoenberg (1935).
1
−D′ = log . Note that the inner term is a kernel, hence is PSD. The log
1+∥X −X ∥2
i j
function preserves CPSD-ness (Bhatia, 2007).
9Ravuri Lawrence
Appendix B. A different interpretation of Equation (3)
If the negative samples coefficient of the second term of Equation (3) is absorbed into the
logarithm, and the first term ignored, this implies that the edge probability is,
(cid:18) 1 (cid:19)1.5n #n−/n
P(A = 1) = 1− 1−
ij 1+∥X −X ∥2
i j
(cid:18) (cid:19)
1.5n n 1
# −
≈ log 1+ .
n ∥X −X ∥2+ϵ
i j
We further approximate this probability as the following based on empirical observations
that it did not seem to affect the embeddings negatively,
1.5n n /n
P(A = 1) ≈ # − .
ij 1+∥X −X ∥2
i j
Usingthisdefinition,weestimatetheparameterofaWishartdistributionoverthenormalized
graph Laplacian L by moment matching (specifically such that the functional terms of the
Wishart variance match the Bernoulli distribution’s variances). This leads to the covariance
estimate,
p
ij
M = ,
ij (cid:80) (cid:80)
p p
a ia b ib
1.5n n /n
# −
where p = . It’s easy to see that this is PD. Plugging this in place of αM
ij 1+∥X −X ∥2
i j
into our model in Equation (4) leads to the embedding in ??. Note that the coefficient
is around 0.01 when n = 10k, which might shed light into the choice for α that is most
performant in Figure 4.
Appendix C. A Matrix-t Perspective
The density of the matrix-t distribution, with the column covariance set to I is expressed as
(Gupta and Nagar, 2018),
(cid:12) (cid:12)
logp(Y|Σ) =
−(α+n/2)log(cid:12)
(cid:12)I n+
β Σ−1YYT(cid:12)
(cid:12)−
d
log|Σ|+c.
(cid:12) 2 (cid:12) 2
Let S = Γ−1 be an invertible estimator of YYT. Note that,
(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12)
log(cid:12)
(cid:12)I n+
β Σ−1S(cid:12)
(cid:12) =
log(cid:12) (cid:12)β Σ−1S(cid:12) (cid:12)(cid:12) (cid:12)2
ΓΣ+I
n(cid:12)
(cid:12)
(cid:12) 2 (cid:12) (cid:12)2 (cid:12)(cid:12)β (cid:12)
(cid:12) (cid:12)
(cid:12)2 (cid:12)
= log(cid:12) ΓΣ+I n(cid:12)−log|Σ|+k.
(cid:12)β (cid:12)
Therefore the density can be rewritten in terms of the precision matrix Γ,
(cid:12) (cid:12)
2α+n−d (cid:12)2 (cid:12)
logp(Y|Σ) = log|Σ|−(α+n/2)log(cid:12) ΓΣ+I n(cid:12)+c.
2 (cid:12)β (cid:12)
10Towards One Model for DR: A Probabilistic Perspective on UMAP and t-SNE
For reference, the negative log-density of a matrix Cauchy distribution can be written as
follows (adapted from ),
d+n n
L (Σ) = log|I +LΣ|− log|Σ|+c.
t1
2 2
The first term of the CNE bound can also be approximated as,
(cid:18) (cid:19)
w (X)
−E log ij
ij∼p
w (X)+1
ij
(cid:32) (cid:33)
1
= −E log
ij∼p 1+ 1
wij(X)
(cid:18) (cid:19)
1
= −E log
ij∼p 2+∥X −X ∥2
i j
= E log(cid:0) 2+∥X −X ∥2(cid:1)
ij∼p i j
≈
n−2(cid:88)
a
log(cid:0)
1+∥X −X
∥2(cid:1)
ij i j
ij
(cid:88)
≤ log(2+n−2 a ∥X −X ∥2)
ij i j
ij
= log(1+2n−2tr(LXXT +I))
≈ log|I+L(XXT +I)|.
Following the methodology of Section 3.1 however leads to a very Laplacian-Eigenmaps-
like solution. The regularisation term is paramount to the quality of embeddings obtained
using such methods.
Appendix D. An Analysis of the Adjacency Probabilities
The generative probabilities of t-SNE, in some sense, model the probability with which an
edge is the shortest edge on the entire distance graph, representing the shortest edge. The
probability of this is proportional to w (X) = 1 (implied by the contrastive loss
ij 1+∥Xi−Xj∥2
used in Damrich et al. (2022); this is also evident from the construction of the algorithm).
In this section, we assume that there’s a latent dataset described by a generative model
X → Y that’s unobserved, but whose statistics then affect the data adjacency matrix A
through Y → A, that is observed.
Assuming that a generative model for data Y given latents X exists, we show by simula-
(cid:18) (cid:20) (cid:21) (cid:19)
tion that P(A (Y) = 1|X) = P I ∥Y −Y ∥2 = argmin∥Y −Y ∥2 = 1X cannot be
ij i j k l
k>l
proportional to the form assumed in t-SNE if a Gaussian process model,
Y|X ∼ MN(0,XXT +σ2I,I),
is assumed, but can be achieved by both matrix Cauchy distributiones with a linear kernel
and Gaussian processes with a kernel that is the sum of linear and smooth kernels. This is
intuitive, as for these probabilities to decay to zero, in the Gaussian case, the kernel must be
11Ravuri Lawrence
Cauchy, S=XX^T+0.1I Normal, S=XX^T+0.1I Normal, S=0.23exp(-r)+0.022XX^T+0.75I
2 monte carlo samples
smoothed samples
2 4
4
4
6 6
6
8
8 8
10 10
10
12 12
14 14 12
0 10 20 30 40 0.0 0.1 0.2 0.3 0.4 0 10 20 30 40
Distance in X Distance in X Distance in X
Figure 5: Monte-Carlo simulations showing the probability with which a distance d (Y) is
ij
the minimum throughout the entire distance matrix, plotted as a function of the
Euclidean distance between the X.
non-stationary. In the case of a matrix Cauchy distribution, this is due to its extreme-value
properties.
Monte-Carlo simulations were done where a matrix Cauchy distribution (left on Figure 5)
andaGaussianprocess(right)withdotproductkernelsareusedtogeneratehigh-dimensional
data, and hence distance matrices between the high-dimensional data points. Then, the
log probability that the ijth element of the distance matrix is the smallest is computed
and plotted against the Euclidean distances of the corresponding X points. These show
that using a Gaussian process with a linear kernel produces probabilities that are linear
functions of the latent Euclidean distance. A matrix Cauchy prior on the other hand, induces
proximity probabilities of the right shape.
As the linear kernel induces extremely small probabilities for larger distances in X, we
also use importance sampling to simulate tail probabilities and ensure that these behave
linearly as a function of the distances in X. For this, we need a full joint distribution of
distances, which is approximated below.
Theorem 1 (Distribution of normal distances) Assume that Y is distributed as,
(cid:20) (cid:21) (cid:18) (cid:20) (cid:21) (cid:19)
Y k k
i ∼ MN µ, ii ij ,I .
Y k k d
j ji jj
Then, the following hold. Firstly, denoting d2 = ∥Y −Y ∥2, the marginal distribution is
ij i j
given by,
(cid:18) (cid:19)
d
d2 ∼ Γ k = ,θ = 2(k +k −2k ) .
ij 2 ii jj ij
As a consequence, E(d2 ) = d∗k˜ and V(d2 ) = 2d∗k˜2, where k˜ = k +k −2k .
ij ij ij ij ij ii jj ij
Additionally,
C(d2 ,d2 ) = 2d∗(k +k −k −k )2.
ij mn im jn in jm
This is a useful fact as the upper triangle of the distance matrix is approximately normal
due to the central limit theorem with increasing d. Proved in Appendix E.
12
ytilibaborP
goLTowards One Model for DR: A Probabilistic Perspective on UMAP and t-SNE
Figure 6: Illustration of P(argmin(d2 ) = ij) through importance sampling for a linear GP.
ij
Results of this experiment are given in Figure 6, which confirms that a Gaussian process
with a linear (dot product) kernel produces probabilities that are approximately linear
functions of the latent distance, which are unlike the assumed tSNE adjacency probabilities.
Along different lines, we can think of the UMAP generative model as a model for
adjacenciesA ∼ Bernoulli(1/(1+∥X −X ∥2))asstatedinProbDR.Usingjustthemeanthe
ij i j
variance in Theorem 1, we can find the parameters ofa covariance S = αXXT+βK (X)+γI,
t
using a black-box optimiser such that the probability of a pair of points ij having a distance
smaller than ϵ,
(cid:32) (cid:33)
ϵ−dk˜
P(d2 < ϵ) ≈ Φ √ ij ,
ij 2dk˜
ij
with k˜ = α∥X −X ∥2+2γ +2β(1−1/(1+∥X −X ∥2)), is as in UMAP. This results
ij i j i j
in the parameters α ≈ 0.1,β ≈ 0.3,γ ≈ 0.6, with the fit being near-perfect. However, we
reason that this is quite a coarse approximation and a naive application of this covariance
within ProbDR results in a mediocre embedding.
Appendix E. An Approximate Joint Distribution of Euclidean Distances
The first part of the theorem is given in Ravuri et al. 2023, reproduced below.
∀k : d′ ≡ yk −yk ∼ N(0,k +k −2k ) =d (cid:112) k +k −2k Z
ij i j ii jj ij ii jj ij
d d
⇒ d2 ≡ ∥Y −Y ∥2 = (cid:88) (yk −yk)2 =d (k +k −2k )(cid:88) Z2
ij i j i j ii jj ij k
k k
=d (k +k −2k )χ2
ii jj ij d
d
= Γ(k = d/2,θ = 2(k +k −2k )).
ii jj ij
The covariance between d2 and d2 can be computed as follows. Let,
ij mn
d′ = Y −Y and d′ = Y −Y .
ij id jd mn md nd
13Ravuri Lawrence
We can then derive some important moments as follows,
E(d′ ) = 0,
ij
V(d′ ) = E(d′2) = k +k −2k ,
ij ij ii jj ij
C(d′ ,d′ ) = C(Y −Y ,Y −Y )
ij mn id jd md nd
= C(Y ,Y )−C(Y ,Y )−C(Y ,Y )+C(Y ,Y )
id md id nd jd md jd nd
= k +k −k −k ,
im jn in jm
Then,
 
(cid:88) (cid:88)
C(d2 ij,d2 mn) = C  (Y id1 −Y jd1)2, (Y md2 −Y nd2)2 
d1 d2
(cid:88)(cid:88)
= C((Y −Y )2,(Y −Y )2) linearity
id1 jd1 md2 nd2
d1 d2
=
(cid:88) C(d′2,d′2
) independence
ij mn
d
=
d∗C(d′2,d′2
)
ij mn
(cid:104) (cid:105)
= d∗
E[d′2d′2 ]−E[d′2]E[d′2
]
ij mn ij mn
(cid:104) (cid:105)
= d∗ E(d′2)E(d′2 )+2E2(d′ d′ )−E[d′2]E[d′2 ] Isserlis’ theorem
ij mn ij mn ij mn
= 2d∗E2(d′ d′ )
ij mn
= 2d∗(k +k −k −k )2.
im jn in jm
14