The Expressive Capacity of State Space Models: A
Formal Language Perspective
YashSarrof,YanaVeitsman,MichaelHahn
SaarlandInformaticsCampus
SaarlandUniversity,Germany
{ysarrof, yanav, mhahn}@lst.uni-saarland.de
Abstract
Recently,recurrentmodelsbasedonlinearstatespacemodels(SSMs)haveshown
promisingperformanceinlanguagemodeling(LM),competititvewithtransform-
ers. However, there is little understanding of the in-principle abilities of such
models, which could provide useful guidance to the search for better LM archi-
tectures. We present a comprehensive theoretical study of the capacity of such
SSMs as it compares to that of transformers and traditional RNNs. We find that
SSMsandtransformershaveoverlappingbutdistinctstrengths. Instar-freestate
tracking, SSMs implement straightforward and exact solutions to problems that
transformersstruggletorepresentexactly. Theycanalsomodelboundedhierar-
chical structure with optimal memory even without simulating a stack. On the
otherhand, weidentifyadesignchoiceincurrentSSMsthatlimitstheirexpres-
sivepower. WediscussimplicationsforSSMandLMresearch,andverifyresults
empiricallyonarecentSSM,Mamba.
1 Introduction
Aftertheirintroduction[69],transformersrapidlybecametheprimaryworkhorseofNLP,powering
most of today’s large language models (LLMs). Compared to previously-dominant recurrent ar-
chitectures[RNNs17,29],transformersofferedakeyadvantage: parallelizedtrainingbyavoiding
recurrence. However,buildingonalonghistoryofcontinuousdynamicalmodels[e.g.34,35]and
earlyworkonfasterRNNs[8,41],arecentlineofworkhasdevelopedstatespacemodels(SSMs)
rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent
modelsthat—whileformulatedintermsofiterativestateupdates—allowefficientparallelization.
TheimpressiveempiricalperformanceofsuchSSMsraisesthequestionofwhethertheymighthave
capabilitiesthatthetransformerarchitecturemightlackinprinciple. Simultaneously,tounderstand
whetherSSMsmayplausiblyovertakethedominantroleoftransformers,itisanimportantquestion
whetherSSMsmaylackabilitiespresentintransformers. Abetterunderstandingofthesequestions
mayalsopointthewaytofuturearchitecturesthatunitethestrengthsofbotharchitectures.
Onecommonapproachtounderstandingthecapabilitiesofcomputationalarchitecturesisthrough
theirexpressivecapacityinsimulatingautomataandmodelinglanguageclasses;indeed,asizeable
literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g.
62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is well-
understoodintermsofsuchlanguageclasses,resultsaboutexpressivecapacitydirectlyyieldresults
abouttheabilitytomodelspecificcomputationalproblems.
While a substantial number of results have been obtained for transformers and traditional RNNs,
understandingremainslargelyopenforSSMs. Inaninitialstep,Merrilletal.[49]showedthatall
problems computable by SSMs are contained in TC0, a circuit complexity class that is known to
Preprint.
4202
yaM
72
]LC.sc[
1v49371.5042:viXraalsocovertransformers[48,65].Understandardconjectures,thissuggeststhatcertaintypesofstate
trackingarehardforbothmodels. Jelassietal.[33]providedevidencefordifferencesbetweenthe
architectures, showing that transformers are better than SSMs at the specific problem of copying
strings–aproblemwellwithinTC0. However,beyondtheseresults,broaderdetailedunderstanding
ofthepowerofSSMsandhowtheycomparetoRNNsandtransformersremainsopen.
Our contribution in this paper is to provide rigorous understanding of SSMs’ abilities in different
classesoflanguages.WeshowthattransformersandSSMscoveroverlappingbutdistinctfragments
ofTC0. Forinstance,SSMscanmodelboundedhierarchicalstructureinwayssimilartotransform-
ersandtraditionalRNNs,evenwithoutembeddingastack-likestructure(Theorem6). Forregular
languages involving modular counting, such as the PARITY function (Theorem 2), we identify a
designchoicethatmakesextantSSMsstruggleinwayssimilartotransformers. Inothercases,we
showthatSSMsresolveafailurecaseoftransformers: theyeffortlesslymodelFlipFlopstatetrack-
ing (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others,
ourresultssuggestfutureLMarchitecturesmightneedtocombinebothattentionandstatespaces.
2 Background: StateSpaceModels
SSMLayers Wedefineasinglelayerofastatespacemodelasamap,atinputlengthT,
RT×d →RT×d (x) (cid:55)→(z)
t t=1,...,T t t=1,...,T
givenbytherecurrence
h =A(x)◦h +B(x) z =φ(h,x) (1)
t t t−1 t t t t
where◦denoteselementwiseproduct,and,foreachx ∈Rd,
t
h ∈Rd B(x)∈Rd (increment)
0 t
A(x)∈Rd (gate) φ:R2d →Rd (transform)
t
WeallowA,Btobearbitrarysmoothmaps. Themapφ(h,x)includesacascadeofchannel-mixing
t t
transformationsandnormalization,whichweabstractasfollows:
φ(h,x)=Mix (Norm(Mix (h,x)),x) (2)
t t 1 2 t t t
whereweallowMix (·):Rd →Rd tobelinear[e.g.23]or(Swi)GLU[e.g.56]. WewilltakeNorm
j
toimplementRMSNorm[75];LayerNorm[4]canbecoveredbyabsorbingcenteringintoMix .
2
A Full SSM Real-world SSMs typically stack several layers of the form (1–2). Where needed,
(1) (L)
weusesuperscriptstoindicatethelayersinanSSM:h ,...,h ,whereListhenumberoflayers.
t t
Weconsiderinputwordsw=w overadiscretealphabetΣ, andassumeanencodinginterms
1...|w|
of token embeddings e(σ)∈Rd, for σ∈Σ. We will also write e for e(σ). These feed into the
σ
(1) (l+1) (l)
lowestlayerasx :=e(w). Theoutputsofeachlayerfeedintothenextlayer,asx =z . The
t t t t
transformationsin(1)arespecifictoeachlayer:A(1),...,A(L)andsimilarlyforB,φ.Tokeepnotation
simple, we will only show the superscripts where necessary for disambiguation. The activations
z t(L) at the highest layer are read out by some neural network ρ into vectors q
t
∈Rdpred describing
classificationornext-tokenpredictions. Weagaintakeρtobeanarbitraryfunction;importantly,all
ourconstructionswillallowρtooperatecorrectlyevenatfiniteprecision.
Implementation Choices In Mamba, (1) directly maps onto Eqs. (2a) and (2b) in Gu and Dao
[23]. The notation of Gu and Dao [23] use a matrix multiplication Ah instead of elementwise
t−1
multiplicationA(x)◦h in(1),butimportantly,Mamba’sAisdiagonal,sowecantakeA(x) =A .
t t−1 t i ii
SomeSSMsassumenondiagonalA(x),buttypicallythismatrixisdiagonalizable[e.g.24,67],so
t
thattheSSMisstillequivalenttooneoftheform(1). WediscusshowotherSSMsinstantiate(1)
inAppendixA.Somemodelsassumecomplex-valuedactivations(AppendixA);ourresultslargely
donotdependonthisdistinction,buttakeitintoaccountwhereneeded(Theorem13). SomeSSMs
[e.g.23]usedifferentnumbersofchannelsinx andh usingstateexpansion;asthisdoesnotaffect
t t
expressive capacity, we will simply assume a constant dimensionality d. Local convolutions [e.g.
21]canbesimulatedwithanSSMlayeranddonotincreaseexpressivecapacity(Remark18).
2Wewillfindthattwodesignchoiceshavenontrivialimpactonexpressivecapacity: Thefirstoneis
time invariance: we call an SSM TIME-INVARIANT if A(x t) does not depend on x t. Some SSMs,
such as S4 [24] and Retnet [67] are time-invariant; Mamba [23], Griffin [14], GLA [72], HGRN
[57, 56], QRNN/SRU [8, 41] are not (Appendix A). The second one is the sign of the entries of
A(x): Across all non-time-invariant SSMs surveyed, we find that the gate is always nonnegative
t
(AppendixA):A(x t)≥0(NONNEGATIVE)duetoexponentialorsigmoidparameterizationsofthe
gate–thischoiceturnsouttolimitexpressivecapacity(Theorem2).
Role of Parameterization While the abstract form (1–2) is common across the SSM literature,
differences in parameterization may have substantial effect on efficiency and training stability.
Studyingexpressivenessallowsustoabstractawayfromthesedifferencestoaremarkabledegree:
WewillallowA,B,ρtobearbitraryfunctionswiththegiveninput-outputproperties. Ournegative
resultsarebasedonabstractpropertiesofthesetup(1–2),whichfundamentallybottlenecksSSMs
throughelementwiselinearstateupdates. Forourpositiveresults,willuseempiricallearnabilityex-
perimentstoverifythatlearnablesolutionsinstantiatingthem(thoughnotnecessarilyimplementing
thesameconstructionsasusedintheproofs)doexistinarecentSMM[Mamba,23].
TraditionalRNNs WecontrastSSMswithtraditionalRNNssuchassimpleRNNsorLSTMs:for
these,therecurrenceinEq. (1)isreplacedby
h =ψ(h ,x) (3)
t t−1 t
whereψcouldbelinear,anMLP[17],oramorecomplexgatedfunction[29].
FinitePrecisionAssumption WhileEq.(1)assumesarbitraryreal-valuedactivations,real-world
implementationscanonlyrepresentnumberswithboundedprecision. Formally,weadoptthefinite
precision notion used by Weiss et al. [70] in a study of the expressive power of traditional RNNs:
Weallowanunboundednumberofintegerbits,butonly pfractionalbits,independentofthelength
oftheinput. SeeAppendixEfordiscussion.
3 TheoreticalResults
3.1 Motivation
We study three foundational types of data structures needed for modeling formal languages [30]:
finitestateautomata(Theorem1,2,4),counters(Theorem5),andstacks(Theorem6). Allofthese
datastructurescanbeunderstoodintwoequivalentforms: Oneistotrackastatesequenceoveran
input, whereeachinputsymbolengendersaspecifictransformationonthestate. Theotherone—
more commonly considered in research on expressive capacity and closer to language modeling–
considersformallanguages—setsoffinitestringsthataredefinedbythepropertythatanautomaton
reachesoneofapre-specifiedsetof“accepting”statesaftertraversingtheword. Wewillfocuson
thelatterperspective,enablingeasycomparisonwithexistingresultsontransformersandRNNs.
Afinite-state-automaton(seeDefinition7)representsageneralstatetrackingproblemoverafinite
statespace, withoutimposingfurtherstructureonthestatespace: Theautomatonkeepstrackofa
singlestatefromafinitestatespace;whenreadingastringfromlefttoright,eachsymbolengenders
aspecifictransformationofthestate. Ateachposition,thecurrentstatedetermineswhichsymbols
can come next; membership in a formal language is determined by the state reached after reading
thefullstring. Finite-state-automataareequivalentinexpressivitytoregularexpressions,anddefine
theregularlanguages[37]. Indeed,wewillbeabletoprovideaprecisecriterionidentifyingwhich
such finite state tracking problems–or equivalently, regular languages–SSMs such as Mamba are
capableofinthefinite-precisionsetting(Theorem4).
Allowinganautomatontokeeptrackofoneormorecounters[20]—integersthatareincremented
ordecrementedateachsymbolread—turnthestatespaceinfinite,butinahighlystructuredmanner.
SSMscanmodelthisdatastructure(Theorem5),ascanRNNsandtransformers[70,6]. Stacks,a
first-in-first-outdatastructure,enableautomatatokeeptrackofhierarchicalstructure,foundational
tonaturallanguage[11]. WeshowthatSSMscanimplementshortcutsolutionstobounded hierar-
chicalstructureevenwithoutimplementingastack(Theorem6)–thesearelikelytobemostuseful
tonaturallanguagegiventheboundednessofhumanmemory[50,36].
3Figure1: Threekeyformallanguages: prefixeswiththesetsofpossiblenextcharacters: FlipFlop
(Theorem1),PARITY(Theorem2),bounded-depthDyck(Theorem6).InFlipFlop,afterar(read)
instruction,thebitmustmatchwhatcameafterthelastw(write)instruction(here,0). ForPARITY,
EOS can only follow when the number of ones in the prefix is even. For bounded-depth Dyck, a
closingbracketcanonlyappearifitmatchesthe lastunclosedopeningbracket(here, “)”matches
“(”)). Openingbracketscanappearaslongasthemaximumdepth(here,5)hasn’tbeenreached.
3.2 Background: FormalLanguagePredictionandRecognition
WefixafinitealphabetΣ. Itselementsarecalledcharactersorsymbols. Thesetofallfinitestrings
woverΣisdenotedΣ∗;suchstringsareoftenreferredtoaswords. Thelengthofwisdenoted|w|.
AformallanguageLisasubsetofΣ∗. Techically, weassumethatthealphabetincludesBOSand
EOSsymbols,whichoccursatthebeginningandendofeachelementofLandnowhereelse.
WenextneedtodefinewhatitmeansforanSSMtomodelaformallanguage. Astandardnotion
isthenotionofrecognition, wherethetaskistoclassifyafullstringasbelongingtothelanguage
or not. Formally, for an SSM with d =1, we say that it recognizes a language L if the output
pred
ρ(z(L) )equals—whentheSSMisrunonw∈Σ∗—1ifw∈Land0else.
|w|
However, such a classification task is arguably not always matched to dominant use cases in pre-
dictivesequencemodeling, wherethetaskistopredictthenexttokenateachstep. Thus, wealso
castformallanguagesintoalanguagemodelingandsequencepredictionframework. Weadoptthe
task of [6], where the model is asked to output at each step in a sequence the set of possible next
symbols. LetPrefix(L):={w:wΣ∗∩L̸=0/}thesetofvalidprefixesofL. Wethensaythatamodel
predictively models a language L if (Figure 1), given a valid prefix w∈Prefix(L), it outputs the
finiteset
{σ∈Σ:w σΣ∗∩L̸=0/} (4)
1...t
Wethinkofeachsuchsetasanatomiclabel;thesetofpossiblelabelsisthepowersetofthefinite
alphabet Σ. Here, d =2|Σ|. Predictive modeling can be easily converted into recognition by
pred
checkingwhetheranysymbolinthesequenceisnotinthepredictivesetattheprecedingposition;
thiscanbedonebyadding1SSMlayer. Conversely, ifwecanshowthatSSMscannotrecognize
a language, this proves they also cannot perform predictive modeling for it, as they then cannot
correctlypredictwhereEOScanappear. Togetthestrongestresults,wethusprovepositiveresults
forpredictivemodeling,andnegativeresultsforrecognition.
3.3 Length-GeneralizingRepresentationsforFlip-FlopStateTracking
AsmotivatedinSection3.1,webeginwithfinite-state-automata–equivalently,regularlanguages,
awell-studiedsettingforunderstandingtheexpressivepoweroftransformers[e.g.25,6,3,44,45]
andafundamentalmodelofstatetrackingwhenthenumberofpossiblestatesisfinite[45,49]. Tra-
ditional RNNs can emulate all finite-state-automata at finite precision [31, 32]. Starting from two
specificlanguages,FlipFlop(Section1)andPARITY(Section2),wederiveanexactcharacteriza-
tionoftheregularlanguagesmodeledbyabroadclassofSSMsatfiniteprecision(Theorem4).
Webeginonthepositiveside,byestablishingacasewhereSSMspatternwithRNNsinavoidinga
failuremodeofself-attention. TheFlipFloplanguages[45]areasimpleinstanceofstatetracking
definedintermsofwrite,read,andignoreinstructions. Eachwriteinstructioncomeswithapiece
ofinformation;wheneverareadinstructionisencountered,theinformationwrittenbythelastwrite
instructionisrecalled. Formally,defineL tobethesetoffinitestringsxoverΣ={r,w,i,0,1},
FF
where x ,x ,···∈{r,w,i}, x ,x ,···∈{0,1}, and where the bit following any r matches the bit
1 3 2 4
following the last preceding occurrence of w. Liu et al. [44] show that the Flip Flop language, as
4anabstraction,isafundamentalingredientofmanylong-rangereasoningsettings. Itcanberepre-
sented with a small finite-state-automaton, and LSTMs learn L well [45]. Transformers can in
FF
principle represent it [44, 45], though known constructions are not inherently length-generalizing.
Indeed, transformers empirically struggle in generalization [45]; intuitively, this may happen be-
causeattentionheadsaggregateinformationinacommutativemanner,andreliablyattendingtothe
last write instruction requires strong position dependence in the attention weights. SSMs pattern
withtraditionalRNNs: theycaneasilyrepresentFlipFlopatarbitraryinputlengths:
Theorem1. Atwo-layerSSMwithfiniteprecisionpredictivelymodelsL atarbitrarylengths.
FF
In the construction, the first layer records the last instruction token, achieved in (1) by setting
A(e(r))=A(e(w))=A(e(i))=0, and A(e(0)=A(e(1))=1, and setting B(e(0))=B(e(1))=0.
(1) (1)
Additionaldimensionsforwardthecurrenttokentoh . Intheoutputofthefirstlayerz ,when-
t t
ever the input is 0 or 1, the model now has access both to the current token w and the preceding
t
token w , which must have been an instruction. Based on this information, the model can set
t−1
(2)
the gate to overwrite the state h with the current input token when the preceding token was w,
t−1
(2) (1)
andpass alongthe state h unalteredotherwise. This, together with z , is sufficientfor always
t−1 t
identifyingthelegalnextsymbolsinL . TheformalproofisinAppendixB.1.
FF
3.4 DifficultyofPARITY
Conversely,wenextestablishadesignchoiceinSSMswhichlimitstheirpowerinemulatingfinite-
state-automata, establishing – in the finite-precision setting – an even stronger separation between
existingSSMvariantsandtraditionalRNNsthanthecircuitcomplexityargumentsinMerrilletal.
[49]. ThelanguagePARITY,thelanguageofbitstringswithanevennumberofones,isrecognized
by a finite-state automaton with 2 states, and is straightforwardly encoded into a traditional RNN,
evenalinearone,withfiniteprecision. PARITY,whileinprincipleexpressiblefortransformers[9],
isempiricallyveryhardtolearnfortransformers[6,15],asitcanprovablyonlyberepresentedin
sharpminima[26]. AsufficientlygeneralSSMcouldeasilyrecognizeitatd=1bysettingh =1,
0
A(e )=−1,A(e )=0,B≡0,sothatthesignofthesingleentryofh indicatestheparity. Suchan
1 0 t
SSMwouldneedtobenon-time-invariantandrequirenegativeorcomplexgatevalues;i.e.,satisfy
neitherTIME-INVARIANTnorNONNEGATIVE. Suchpropertiesareindeedprovablyneeded:
Theorem 2. No SSM satisfying NONNEGATIVE can recognize PARITY at arbitrary input lengths
withfiniteprecision. Inparticular,thisappliestoMamba.
TheproofisinAppendixB.2; itexaminesinputsoftheform1N andshowsthattheactivationsz
N
convergeasN →∞, andthuscannotreliablyencodetheparityofN. AsweshowinTheorem13,
the same result holds even for SSMs evading NONNEGATIVE when they are TIME-INVARIANT, at
leastwhenthecoefficientshaverationalanglesinthecomplexplanes.AllextantSSMswesurveyed
(Appendix, Section A) satisfy either NONNEGATIVE or TIME-INVARIANT. Hypothetical SSMs
evading both NONNEGATIVE and TIME-INVARIANT would be strictly stronger and can represent
notonlyPARITY,butallregularlanguagesknowntobeinTC0(Theorem21).
3.5 WhichRegularLanguagescanSSMsModel?
We now combine Theorems 1 and 2 to derive an exact characterizations of the regular languages
that modern non-time-invariant SSMs such as Mamba can recognize or predictively model – the
two notions turn out to coincide here – in the finite-precision setting. The key insight is that L
FF
and PARITY are fundamental building blocks of two classes of regular languages: the star-free
languagesandtheircomplement,thenon-star-freelanguages[60,46]:
Definition3. Aregularlanguageisstar-freeifitcanbedefinedusingregularexpressionsinvolving
onlytheemptyset,theemptystring,individualsymbols,concatenation,andBooleancombinations
–avoidingtheKleenestaroperation.
L isstar-free: thereisawaytodefineitwithoutKleenestar. PARITYisnotstar-free;anyregular
FF
expression for it must involve the Kleene star. Even languages whose intuitive definition involves
theKleenestarmayturnouttobestar-free.Forexample,(01)∗hasanexpressionavoidingthestar.1
1Itistheunionofεwiththeintersectionof0Σ∗,Σ∗1,withthecomplementsofΣ∗00Σ∗andΣ∗11Σ∗.
5Equivalently, a language is star-free if it can be defined logically using only first-order quantifiers
andtheorderrelation[60]. Also,L isnon-star-freeifandonlyifrecognizingitinvolvescounting
modulosomeK [46];PARITYbeingthesimplestexample. Modernnon-time-invariantSSMssuch
asMambacannotperformmodulocounting,buttheycanmodelallstar-freelanguages:
Theorem4. TheclassofSSMssatisfyingNONNEGATIVEcanpredictivelymodelaregularlanguage
L atfiniteprecisionifandonlyifL isstar-free.
The proof in Appendix B.3 uses the Krohn-Rhodes theorem [39] to reduce all star-free languages
toflipflop-likestatetracking. Importantly, therearewell-knownconstructivecriteriafordeciding
whetheragivenautomatondefinesastar-freelanguage[60]; hence, wehaveadecidablecriterion
forthefinite-statetrackingproblemsthatsuchSSMssatisfyingNONNEGATIVEcansolve.
This is much simpler than the situation for transformers, where an exact characterization of their
power within the regular languages is complicated: Angluin et al. [3] show that a certain formal
abstraction of transformers (masked unique hard attention) also recognizes exactly the star-free
languages, but constructions of realistic transformers via Krohn-Rhodes in Liu et al. [44] do not
inherently length generalize, and empirical research indicates difficulty in generalization even for
certain simple star-free languages [6, 45]. Known length-generalizing constructions are limited to
verysimplesubclassessuchasthepiecewisetestablelanguages[71]. Incontrast,forSSMswehave
asinglemodelperlanguage,atfiniteprecisionandforarbitrarilylonginputs. Thus,weexpectthat
the SSM architecture confers an advantage in star-free state tracking problems when compared to
transformers–apredictionwewillfindsupportedexperimentally(Figure3).
3.6 UnboundedCounting
HavingcharacterizedtheregularlanguagesmodeledbySSMs,wenowconsiderlanguagesrequiring
unboundedcounting[20],specifically,languagesrecognizedbykeepingtrackofoneormorecoun-
ters,whereeachcharactercausesaspecificincrementordecrementtoeachcounter[38,27,70,40].
A prime example is the Dyck-1 language of well-formed strings over “(” and “)”; here a counter
is incremented (decremented) whenever an opening (closing) bracket is encountered; a string is
well-formedifandonlyifthecounteris0attheendofthestring. Someotherrelevantformallan-
guages are Shuffle-Dyck-k (the shuffles of multiple Dyck-1 languages), anbn – here, a increments
thecounterandbdecrementsit,andanbncn–here,therearetwocounters,onekeepingtrackofanbn
and one of bncn (See Appendix C.2). Such counter languages are fundamental as basic context-
free(Dyck-1,anbn)orcontext-sensitive(e.g.,anbncn)languages[30],andhavebeenthesubjectof
studiesofbothtransformers[6]andRNNs[70]. ManysuchlanguagesaremodeledbySSMs:
Theorem 5. Consider the languages Dyck-1, Shuffle-Dyck-k, n-ary Boolean Expressions, anbn,
anbncn,andanbncndn,asdefinedinAppendixC.2. Theneachoftheseispredictivelymodeledbyan
SSM.
TheproofisinAppendixB.4. Intuitively,anSSMcandirectlyimplementtherequiredcountersby
settingA≡1andbydefiningB(e )tothebeincrementordecrementcasedbyσ. Inmodelingsuch
σ
languages,SSMspatternwithbothtransformers[6]andLSTMs[70].
It may seem counterintuitive that NONNEGATIVE SSMs can perform unbounded counting but (by
Theorem 2) not modular counting—the latter would seem to just require reading out the value of
anunboundedcounter. Whatiskeyisthat, eventhoughh canencodeunboundedcounts, reading
t
outthemodularvalueofanunboundedintegerisaformidableproblemfortypicalneuralnetwork
nonlinearities,inparticularwhentheinformationhasbeenpushedthroughnormalization(2).
Weshouldnotethatthereisaqualitativedifferencebetweenthisresultandtheprecedingpositive
resultsaboutfinite-statelanguages(Theorems1and4),inthattheconstructioninTheorem5uses
unboundedly large entries in the state h, whereas Theorems 1 and 4 use bounded values at finite
t
precision. Indeed,wewillfindbetterlengthgeneralizationinthefinite-statecase(Figure3).
AconsequenceofTheorem5isthatSSMscanrecognizesomelanguagestranscendingthecontext-
freelanguages,asanbncnisnotcontext-free.Asecondapplicationofthetheorem,ofgreatlinguistic
interest,istoboundedhierarchicalstructure,aswediscussnext.
63.7 BoundedHierarchicalStructurewithoutStacks
It is generally agreed that hierarchical structure is a key
aspect of language, and that comprehending language at
1 Layer Mamba SSM
a human-like level requires the computational ability to Flip Flop Validation Set Performance
process such structures [12, 43, 18]. The fundamental 100
data structure for processing hierarchical structure is the
stack, where information is stored and removed as one 10 2
traverses to higher and lower levels of hierarchical em-
bedding[30]. WenowshowthatSSMs’countingability 10 4
canoffershortcutsevenonsuchlanguagesmodelinghi- Distribution
erarchicalstructure,eschewingtheneedforastack. 10 6 FFL(0.8)
FFL(0.98)
A useful abstraction of hierarchical structure as relevant
0 500 1000
tonaturallanguageisthefamilyofDycklanguages. The Training Iterations
bounded-depth Dyck language Dyck with K types of
K,h
parenthesesanddepthhisthelanguageofwell-bracketed
strings over ( , ) , ..., ( , ) , such that the number Figure 2: Test error on the valida-
1 1 K K
of yet unclosed brackets never exceeds h in any prefix tion set for L FF, in the setup of Liu
[28,74]. Theclassicalunbounded-depthDycklanguages et al. [45]. On both the in-distribution
– the limit where h → ∞ – play a fundamental role as (green)andout-of-distribution(orange)
thebackboneofcontext-freelanguagesviatheChomsky- setting, Mamba achieves near-zero test
Schützenberger theorem [12]. Bounding the depth re- error,inagreementwithTheorem1,and
flectsthefactthatdeepembeddingisrareinnaturallan- avoidingthefailuresoftransformersob-
guage[36,7]. Priorworkhasfoundthattwo-layertrans- servedbyLiuetal.[45].
formers[73]andtraditionalRNNs[28,6]bothmodelall
Dyck languages. ThesameturnsouttoholdforSSMs:
K,h
Theorem6. Thebounded-depthDycklanguageDyck ispredictivelymodeledbyatwo-layerSSM
K,h
atfiniteprecision,withd=O(hlogK).
TheproofisinAppendixB.5. Intuitively,thefirstlayerrecordsthedepthofeachparenthesisusing
theideasfromTheorem5, andthesecondlayerkeepstrackofthelastopenbracketateachdepth
usingTheorem1. Wenotethat,sinceDyck isstar-free,Theorem4alreadyguaranteestheexis-
K,h
tenceofrepresentingSSMs,butthedepthandwidthguaranteedbyTheorem6islikelytobemuch
betterthanwhatwouldbeobtainedbyablack-boxapplicationofTheorem4: AsHewittetal.[28]
show,hlogK unitsisoptimaluptoconstantsandisattainedbytraditionalRNNsandLSTMs. The
SSMconstructionisverydifferentfromthatofHewittetal.[28]fortraditionalRNNs(bothsimple
RNNsandLSTMs),whichdirectlysimulatesastack. Ourconstructionissimilartothetransformer
construction in Theorem 4.2 in [73], which however has to rely on specific positional encodings,
unliketheSSMconstruction. Thishighlightsthatstacksarenottheonlywayofsimulatingbounded
hierarchical structure in recurrent architectures, and non-stack-based strategies can even attain the
same optimal scaling of hidden units. Probing whether such stack-free shortcuts are learned by
SSM-basedLLMsisanexcitingproblemforfutureresearch.
4 Experiments
Wehavederivedafine-grainedtheoreticalcharacterizationofexpressivenessstrengthsandlimita-
tionsofSSMs. Wenowshowthatourpositiveresultscanbeinstantiatedandlearnedinarealistic
SSMimplementation,byevaluatingarecenthighlysuccessfulSSM,Mamba[23].
FlipFlop We empirically instantiate Theorem 1 using the dataset released by [45], reflecting the
language L as defined in Section 3.3. Matching Figure 2 in Liu et al. [45], we evaluated both
FF
with in-distribution data, and with out-of-distribution data where the distance between read and
writeinstructionstendedtobelarger. Weevaluateforpredictingthebitsfollowingr instructions2,
matchingthe“deterministic/clean”modeofLiuetal.[45],andconsideredpredictionstobecorrect
onlyifallpredictionswithinasequencewerecorrect. WeprovidefurtherdetailsinAppendixD.2.
2Atallotherpositions,predictivemodelingistrivial,asitonlyneedstoconsidertheinputsymbolatthat
position.
7
rorrE
tseTBin [1, 50] Transformer Accuracy Bin [1, 50] Mamba Accuracy Bin [51, 100] Transformer Accuracy Bin [51, 100] Mamba Accuracy
Star-Free Languages Non Star-Free Languages Counter Langauges
1.0
0.8
0.6
0.4
0.2
0.0 Tomita-1 Tomita-4 Tomita-7 Tomita- a2 a*bb*cc*dd* {e a,e b* }*d{b,c} {* 0,1,2}*02* D2 D3 D4 D12 Parity (aa)* (aaaa)* (abab)* Tomita-3 Tomita-5 Tomita-6 Dyck-1 Shuffle- S2 huffle- S4 huffle-6 Boolean B-3 oolean-5 anbn anbncn anbncndn
Figure3: Resultson27formallanguages, comparingourMambaresults(blue)withtransformer
results reported by [6] (orange), on in-distribution lengths (solid) and out-of-distribution lengths
(dotted). As predicted by Theorem 4, Mamba performs strongly on star-free languages, and even
showsperfectlengthgeneralization. AgainaspredictedbyTheorem4,itperformspoorlyonnon-
star-freelanguages. Resultsfortransformersfrom[6]aremixed. Mambaalsosucceedsonlearning
the counter languages from Theorem 5, showing perfect accuracy at in-distribution lengths at in-
distributionlengths,butlengthgeneralizationlagsbehindtransformers.
A small one-layer3 Mamba model converged to 0 error in both validation sets after ∼ 1400 steps
(Figure 2), compared to 500 steps for an LSTM reported by Liu et al. [45]. In contrast, Liu et al.
[45]foundtransformerstokeepmakingoccasionalmistakesdespitetrainingfor10Ksteps.
TestSuitefromBhattamishraetal.[6] Totestourtheoreticalresultsonregularandcounterlan-
guages(Theorems2,4,5),wetestMambaon27formallanguages,including18regularlanguages
and9counterlanguages,basedonapriorstudycomparingtransformersandRNNs[6]. Theregular
languagesincludeapopularbenchmark[68]andvariousregularexpressions;11arestar-free. The
counter languages include the languages covered by Theorem 5. Definitions of all languages are
providedinAppendixC.WechosethistestsuitebecauseitpreciselycoversTheorems4and5,and
wehaveproven(in)expressibilityresultsforeachlanguageintheset.
Following [6], we trained the model for predictive mod-
eling, i.e., at each step, the model outputs a label indi-
Mamba SSM Layer 1 vs Layer 2
cating the set of possible next characters (4), including
(Dyck-(8, 10), Test)
EOSwhenrequired. Following[6],wecountthemodel’s
1.0
responseonaninputstringascorrectifandonlyifpredic-
tivemodellingwassuccessfulatallpositionsintheinput. 0.8
Such a evaluation setup makes random baselines fairly
0.6
low, where a random predictor would have an accuracy
exponentiallysmallinNineachoftheNpositions.Train- 0.4
ing inputs have length in [1,50]; the model is then eval- Model
uated on held-out bins with length [1,50] and [51,100]. 0.2 2 Layer Mamba
1 Layer Mamba
FurtherexperimentaldetailsareinAppendixD.1. 0.0
25 50 75 100
We show our Mamba results, together with Transformer Memory Dimensions
results reported by Bhattamishra et al. [6], in Figure 3.
LSTMs perform perfectly on all languages, and are thus
not shown. In a striking confirmation of Theorem 4, Figure 4: As predicted by Theo-
Mamba learns all star-free languages with strong length rem6,Mambawith2layerscanmodel
generalization, and does poorly on all non-star-free lan- bounded-depth Dyck languages. Re-
guages. Transformers show somewhat more mixed per- sults for test set with strings of length
formance, and do not always length-generalize even on 700≤n≤1400.
the star-free languages. In agreement with Theorem 5,
Mamba, similar to Transformers, learns the counter lan-
guages,thoughitstruggleswithlengthgeneralizationwhencomparedtotransformers. Thediffer-
encesinMamba’sperformancebetweenstar-freeandcounterlanguagesmaystemfromthefactthat
3Theorem1constructsatwo-layerSSM.WehypothesizethatMambausesitslocalconvolution(Remark18)
toreplacethelowerlayerfromtheconstructioninTheorem1.
8
ycaruccA
tseT
ycaruccA
esolCthe construction for the former class (Theorem 4) is able to use finite precision and bounded state
valuesatarbitraryinputlengths,whilethelatter(Theorem5)usesunboundedstatevalues.
Bounded Hierarchical Structure To test Theorem 6, we recreate the experimental setup from
Yaoetal.[74]. MatchingtheirFigure4,wetrainedMambatopredictivelymodelDyck atK=8
K,h
andh=10. Thetrainingandthevalidationsetcontainedsamplesoflength≤700,whilethetestset
containedsamplesoflength700≤n≤1400. Yaoetal.[74]foundbothtransformersandLSTMs
achieved strong performance on this setup. We provide further details in Appendix D.3. Recall
that Theorem 6 shows that two-layer SSMs can predictively model Dyck . We trained Mamba
K,h
with1or2layersandvaryingdimensionality,findingthattwolayerscanachieveessentiallyperfect
performanceacrossmodelsizes,evenonthetestset(Figure4and5).
5 Discussion
RelatedWork OurworkbelongstoanincipientlineofresearchintotheexpressivenessofSSMs
[33, 49]. It is closely related to a long string of work studying the expressive capacity of neural
sequencemodels,whichhassofarfocusedonrecurrentnetworks[e.g.62,6,28]and,morerecently,
selfattention[e.g.10,48,66]. Asecondlinkistotheclassicalandlong-standingstudyoflineardy-
namicalsystemsandcontroltheory[34].Forinstance,Theorem2reliestheasymptoticconvergence
ofanSSMoncertaininputs,establishingalinktotheasymptoticsoflinearsystems[e.g.55].
Take-Aways Whiletheoreticalinnature,ourresultshaveseveralactionableimplicationsforSSM
andLLMresearch,informingtherapidlygrowingresearchonSSM-basedLLMs. First,encourag-
ingly, SSMs can keep track of bounded hierarchical structure with optimal memory even without
explicitly implementing a stack (Theorem 6), suggesting that simple diagonal linear state updates
may be sufficiently powerful for modeling the hierarchical structure of language. Second, SSMs
resolve a basic failure mode of self-attention in flip-flop state tracking while being parallellizable
(Theorem1). Overall,SSMsandattentionhaveoverlappingbutdistinctstrengths. Thislendssup-
porttothedevelopmentofhybridarchitecturesinterleavingSSMandattentionlayers,asinstantiated
very recently by Jamba [42]. Third, nonnegative gates as obtained by exponential or sigmoid pa-
rameterizationsprovablyrestrictexpressivecapacity,eveninnon-time-invariantSSMs(Theorem2).
While[23]foundnoevidencethatcomplex-valuedparamerizationsimprovedoverreal-valuedones
in the language modality, our results suggest revisiting this question, at least for tasks where peri-
odicstate-trackingabilitiesmaybeimportant. Fourth,whileexactlycharacterizingthecapacityof
transformershasprovendifficulteveninthefinite-statecase,Theorem4providesadecidablechar-
acterizationoftheregularlanguages–equivalently,finite-statetrackingproblems–thatSSMssuch
asMambacanmodel. Suchdecidablecharacterizationsmaymakeiteasiertotheoreticallypredict
abilitiesandanticipatefailuresofLLMs;exploringtheimplicationsofthischaracterizationinmore
realisticsetupsisanexcitingdirectionforfutureresearch.
Limitations Themainlimitationofourtheoreticalresultsisthattheyfocusonin-principleexpres-
siveness,anddonotdirectlymakestatementsaboutlearningandgeneralization. Futureworkcould
addressthis,forexample,byexaminingwhetherourconstructionsresultinreasonablyflatminima,
orbystudyinggradientflowdynamics. Whileweempiricallyverifiedthatourpositiveresultscan
indeed be instantiated, in a learnable manner, in one realistic SSM implementation, implementa-
tionaldifferencesmightstillresultinpracticaldifferencesbetweenimplementations. Studyingthe
roleofsuchimplementationaldifferencesisaninterestingproblemforfuturework;wehavemade
afirststepbytheoreticallyelucidatingtheimplicationsofnonnegativegatevalues.
6 Conclusion
We have studied the expressive capacity of modern state space models (SSMs), through the lens
of automata and formal languages. We have shown theoretically that SSMs can express star-free
languages,arangeofcounterlanguages,andboundedhierarchicalstructure. Byprovidingrigorous
resultsabouttheexpressivenessoftheSSMarchitecture,ourresultscanprovideguidancetowork
onSSM-basedlanguagemodels.
9Acknowledgments
WethankMarkRofinforusefuldiscussionaboutTheorem2.
References
[1] E.Akyürek,B.Wang,Y.Kim,andJ.Andreas. In-contextlanguagelearning: Arhitecturesand
algorithms. arXivpreprintarXiv:2401.12973,2024.
[2] J.Almeida. Finitesemigroupsanduniversalalgebra,volume3. WorldScientific,1995.
[3] D. Angluin, D. Chiang, and A. Yang. Masked hard-attention transformers and boolean rasp
recognizeexactlythestar-freelanguages. arXivpreprintarXiv:2310.13897,2023.
[4] J.L.Ba,J.R.Kiros,andG.E.Hinton. Layernormalization. stat,1050:21,2016.
[5] D.A.M.Barrington, K.Compton, H.Straubing, andD.Thérien. Regularlanguagesinnc1.
JournalofComputerandSystemSciences,44(3):478–499,1992.
[6] S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to
recognizeformallanguages. InProceedingsofthe2020ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing(EMNLP),pages7096–7116,2020.
[7] D.Blasi,R.Cotterell,L.Wolf-Sonkin,S.Stoll,B.Bickel,andM.Baroni. Onthedistribution
ofdeepclausalembeddings: Alargecross-linguisticstudy. InProceedingsofthe57thAnnual
MeetingoftheAssociationforComputationalLinguistics,pages3938–3943,2019.
[8] J.Bradbury,S.Merity,C.Xiong,andR.Socher. Quasi-recurrentneuralnetworks. InInterna-
tionalConferenceonLearningRepresentations,2016.
[9] D.ChiangandP.Cholak.Overcomingatheoreticallimitationofself-attention.InProceedings
ofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers),pages7654–7664,2022.
[10] D. Chiang, P. Cholak, and A. Pillay. Tighter bounds on the expressivity of transformer en-
coders. 2023.
[11] N.Chomsky. Syntacticstructures,1957.
[12] N. Chomsky and M. P. Schützenberger. The algebraic theory of context-free languages. In
Studies in Logic and the Foundations of Mathematics, volume 35, pages 118–161. Elsevier,
1963.
[13] Y.N.Dauphin,A.Fan,M.Auli,andD.Grangier.Languagemodelingwithgatedconvolutional
networks. InInternationalconferenceonmachinelearning,pages933–941.PMLR,2017.
[14] S.De,S.L.Smith,A.Fernando,A.Botev,G.Muraru,A.Gu,R.Haroun,L.Berrada,Y.Chen,
S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y. W. Teh, R. Pascanu, N. de Freitas,
and Ç. Gülçehre. Griffin: Mixing gated linear recurrences with local attention for efficient
language models. CoRR, abs/2402.19427, 2024. doi: 10.48550/ARXIV.2402.19427. URL
https://doi.org/10.48550/arXiv.2402.19427.
[15] G. Deletang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy,
M. Hutter, S. Legg, J. Veness, et al. Neural networks and the chomsky hierarchy. In The
EleventhInternationalConferenceonLearningRepresentations,2022.
[16] S.Eilenberg. Automata,languages,andmachines. Academicpress,1974.
[17] J.L.Elman. Findingstructureintime. Cognitivescience,14(2):179–211,1990.
[18] M.B.Everaert,M.A.Huybregts,N.Chomsky,R.C.Berwick,andJ.J.Bolhuis. Structures,
notstrings: linguisticsaspartofthecognitivesciences. Trendsincognitivesciences,19(12):
729–743,2015.
[19] P. C. Fischer, A. R. Meyer, and A. L. Rosenberg. Counter machines and counter lan-
guages. Mathematical systems theory, 2(3):265–283, Sep 1968. ISSN 1433-0490. doi:
10.1007/BF01694011. URLhttps://doi.org/10.1007/BF01694011.
[20] P. C. Fischer, A. R. Meyer, and A. L. Rosenberg. Counter machines and counter languages.
Math. Syst. Theory, 2(3):265–283, 1968. doi: 10.1007/BF01694011. URL https://doi.
org/10.1007/BF01694011.
10[21] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re. Hungry hungry hippos:
Towards language modeling with state space models. In The Eleventh International Con-
ference on Learning Representations, 2023. URL https://openreview.net/forum?id=
COZDy0WYGg.
[22] A.Ginzburg. Algebraictheoryofautomata. AcademicPress,1968.
[23] A.GuandT.Dao. Mamba:Linear-timesequencemodelingwithselectivestatespaces. CoRR,
abs/2312.00752, 2023. doi: 10.48550/ARXIV.2312.00752. URL https://doi.org/10.
48550/arXiv.2312.00752.
[24] A.Gu,K.Goel,andC.Re. Efficientlymodelinglongsequenceswithstructuredstatespaces.
InInternationalConferenceonLearningRepresentations,2021.
[25] M.Hahn. Theoreticallimitationsofself-attentioninneuralsequencemodels. Transactionsof
theAssociationforComputationalLinguistics,8:156–171,2020.
[26] M. Hahn and M. Rofin. Why are sensitive functions hard for transformers? CoRR,
abs/2402.09963, 2024. doi: 10.48550/ARXIV.2402.09963. URL https://doi.org/10.
48550/arXiv.2402.09963.
[27] M. Hahn, A. Krebs, K. Lange, and M. Ludwig. Visibly counter languages and the structure
1
of nc . In G. F. Italiano, G. Pighizzini, and D. Sannella, editors, Mathematical Foundations
ofComputerScience2015-40thInternationalSymposium,MFCS2015,Milan,Italy,August
24-28,2015,Proceedings,PartII,volume9235ofLectureNotesinComputerScience,pages
384–394. Springer, 2015. doi: 10.1007/978-3-662-48054-0\_32. URL https://doi.org/
10.1007/978-3-662-48054-0_32.
[28] J.Hewitt,M.Hahn,S.Ganguli,P.Liang,andC.D.Manning.Rnnscangenerateboundedhier-
archicallanguageswithoptimalmemory.InProceedingsofthe2020ConferenceonEmpirical
MethodsinNaturalLanguageProcessing(EMNLP),pages1978–2010,2020.
[29] S.HochreiterandJ.Schmidhuber.Longshort-termmemory.NeuralComputation,9(8):1735–
1780,1997.
[30] J. E. Hopcroft, R. Motwani, and J. D. Ullman. Introduction to automata theory, languages,
andcomputation. ACMNewYork,NY,USA,2001.
[31] B.HorneandD.Hush.Boundsonthecomplexityofrecurrentneuralnetworkimplementations
offinitestatemachines. Advancesinneuralinformationprocessingsystems,6,1993.
[32] P.Indyk. Optimalsimulationofautomatabyneuralnets. InAnnualSymposiumonTheoretical
AspectsofComputerScience,pages337–348.Springer,1995.
[33] S.Jelassi, D.Brandfonbrener, S.M.Kakade, andE.Malach. Repeatafterme: Transformers
arebetterthanstatespacemodelsatcopying,2024.
[34] R. E. Kalman. On the general theory of control systems. In Proceedings First International
ConferenceonAutomaticControl,Moscow,USSR,pages481–492,1960.
[35] R.E.Kalman. Mathematicaldescriptionoflineardynamicalsystems. JournaloftheSociety
forIndustrialandAppliedMathematics,SeriesA:Control,1(2):152–192,1963.
[36] F.Karlsson. Constraintsonmultiplecenter-embeddingofclauses. JournalofLinguistics,43
(2):365–392,2007.
[37] S. Kleene. Representation of events in nerve nets and finite automata. In Automata Studies.
1951.
[38] A.Krebs,K.Lange,andM.Ludwig. Visiblycounterlanguagesandconstantdepthcircuits. In
E.W.MayrandN.Ollinger,editors,32ndInternationalSymposiumonTheoreticalAspectsof
ComputerScience,STACS2015,March4-7,2015,Garching,Germany,volume30ofLIPIcs,
pages 594–607. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2015. doi: 10.4230/
LIPICS.STACS.2015.594. URLhttps://doi.org/10.4230/LIPIcs.STACS.2015.594.
[39] K. Krohn and J. Rhodes. Algebraic theory of machines. i. prime decomposition theorem for
finite semigroups and machines. Transactions of the American Mathematical Society, 116:
450–464,1965.
[40] M.Kutrib,A.Malcher,andM.Wendlandt. Input-drivenmulti-counterautomata. Theoretical
ComputerScience,870:121–136,2021.
11[41] T.Lei,Y.Zhang,S.I.Wang,H.Dai,andY.Artzi. Simplerecurrentunitsforhighlyparalleliz-
ablerecurrence. arXivpreprintarXiv:1709.02755,2017.
[42] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom,
Y. Belinkov, S. Shalev-Shwartz, O. Abend, R. Alon, T. Asida, A. Bergman, R. Glozman,
M. Gokhman, A. Manevich, N. Ratner, N. Rozen, E. Shwartz, M. Zusman, and Y. Shoham.
Jamba: Ahybridtransformer-mambalanguagemodel,2024.
[43] T. Linzen, E. Dupoux, and Y. Goldberg. Assessing the ability of LSTMs to learn syntax-
sensitivedependencies.TransactionsoftheAssociationforComputationalLinguistics,4:521–
535,2016.
[44] B.Liu, J.T.Ash, S.Goel, A.Krishnamurthy, andC.Zhang. Transformerslearnshortcutsto
automata. arXivpreprintarXiv:2210.10749,2022.
[45] B.Liu,J.T.Ash,S.Goel,A.Krishnamurthy,andC.Zhang. Exposingattentionglitcheswith
flip-floplanguagemodeling. InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,and
S.Levine,editors,AdvancesinNeuralInformationProcessingSystems36:AnnualConference
onNeuralInformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,De-
cember 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/
hash/510ad3018bbdc5b6e3b10646e2e35771-Abstract-Conference.html.
[46] R.McNaughtonandS.A.Papert. Counter-FreeAutomata(MITresearchmonographno.65).
TheMITPress,1971.
[47] H.Mehta,A.Gupta,A.Cutkosky,andB.Neyshabur.Longrangelanguagemodelingviagated
state spaces. In The Eleventh International Conference on Learning Representations, 2023.
URLhttps://openreview.net/forum?id=5MkYIYCbva.
[48] W. Merrill and A. Sabharwal. A logic for expressing log-precision transformers. In Thirty-
seventhConferenceonNeuralInformationProcessingSystems,2023.
[49] W.Merrill,J.Petty,andA.Sabharwal. Theillusionofstateinstate-spacemodels. InICML,
2024.
[50] G.A.MillerandN.Chomsky. Finitarymodelsoflanguageusers. 1963.
[51] S.M.S.MiracSuzgun,YonatanBelinkov. Onevaluatingthegeneralizationoflstmmodelsin
formallanguages. volume2,pages277–286.UniversityofMassachusettsAmherstLibraries,
1 2019. doi: 10.7275/s02b-4d91. URL https://openpublishing.library.umass.edu/
scil/article/id/1167/.
[52] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resur-
rectingrecurrentneuralnetworksforlongsequences. InInternationalConferenceonMachine
Learning,pages26670–26698.PMLR,2023.
[53] B.Peng,S.Narayanan,andC.Papadimitriou. Onlimitationsofthetransformerarchitecture.
arXivpreprintarXiv:2402.08164,2024.
[54] J.Pérez,J.Marinkovic´,andP.Barceló. Ontheturingcompletenessofmodernneuralnetwork
architectures. arXivpreprintarXiv:1901.03429,2019.
[55] P. C. Phillips and V. Solo. Asymptotics for linear processes. The Annals of Statistics, pages
971–1001,1992.
[56] Z.Qin,S.Yang,W.Sun,X.Shen,D.Li,W.Sun,andY.Zhong. Hgrn2:Gatedlinearrnnswith
stateexpansion. arXivpreprintarXiv:2404.07904,2024.
[57] Z. Qin, S. Yang, and Y. Zhong. Hierarchically gated recurrent neural network for sequence
modeling. AdvancesinNeuralInformationProcessingSystems,36,2024.
[58] J.Sakarovitch. Elementsofautomatatheory. Cambridgeuniversitypress,2009.
[59] C.Sanford,D.J.Hsu,andM.Telgarsky. Representationalstrengthsandlimitationsoftrans-
formers. AdvancesinNeuralInformationProcessingSystems,36,2024.
[60] M. P. Schützenberger. On finite monoids having only trivial subgroups. Inf. Control., 8(2):
190–194,1965.
[61] N.Shazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202,2020.
[62] H. Siegelman and E. D. Sontag. On the computational power of neural nets. Journal of
ComputerandSystemSciences,50:132–150,1995.
12[63] H.T.Siegelmann.Neuralnetworksandanalogcomputation:beyondtheTuringlimit.Springer
Science&BusinessMedia,1999.
[64] H.Straubing. Finiteautomata,formallogic,andcircuitcomplexity. Birkhaeuser,1994.
[65] L.Strobl. Average-hardattentiontransformersareconstant-depthuniformthresholdcircuits,
2023.
[66] L. Strobl, W. Merrill, G. Weiss, D. Chiang, and D. Angluin. Transformers as recognizers of
formal languages: A survey on expressivity. CoRR, abs/2311.00208, 2023. doi: 10.48550/
ARXIV.2311.00208. URLhttps://doi.org/10.48550/arXiv.2311.00208.
[67] Y.Sun,L.Dong,S.Huang,S.Ma,Y.Xia,J.Xue,J.Wang,andF.Wei. Retentivenetwork: A
successortotransformerforlargelanguagemodels. arXivpreprintarXiv:2307.08621,2023.
[68] M.Tomita. Dynamicconstructionoffinite-stateautomatafromexamplesusinghill-climbing.
InProceedingsoftheFourthAnnualConferenceoftheCognitiveScienceSociety,pages105–
108,1982.
[69] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polo-
sukhin.Attentionisallyouneed.InAdvancesinneuralinformationprocessingsystems,pages
5998–6008,2017.
[70] G.Weiss,Y.Goldberg,andE.Yahav. Onthepracticalcomputationalpoweroffiniteprecision
rnnsforlanguagerecognition. InProceedingsofthe56thAnnualMeetingoftheAssociation
forComputationalLinguistics(Volume2: ShortPapers),pages740–745,2018.
[71] A.YangandD.Chiang. Countingliketransformers: Compilingtemporalcountinglogicinto
softmaxtransformers. arXivpreprintarXiv:2404.04393,2024.
[72] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with
hardware-efficienttraining. arXivpreprintarXiv:2312.06635,2023.
[73] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process
bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Associ-
ation for Computational Linguistics and the 11th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers). Association for Computational Linguistics,
2021.doi:10.18653/v1/2021.acl-long.292.URLhttp://dx.doi.org/10.18653/v1/2021.
acl-long.292.
[74] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process
boundedhierarchicallanguages.InC.Zong,F.Xia,W.Li,andR.Navigli,editors,Proceedings
of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
pages 3770–3785, Online, Aug. 2021. Association for Computational Linguistics. doi: 10.
18653/v1/2021.acl-long.292. URLhttps://aclanthology.org/2021.acl-long.292.
[75] B.ZhangandR.Sennrich. Rootmeansquarelayernormalization. AdvancesinNeuralInfor-
mationProcessingSystems,32,2019.
13A InstantiationsofGeneralFrameworkinSSMModels
Here, we survey how (1) is instantiated in a range of SSMs. As stated in Section 2, we refer to
SSMswherethegateAdoesnotdependonx astime-invariant. Anequivalentterminologyisthe
t
distinctionbetween“WeakLinearTimeInvariantConvolutionalModels”(i.e., time-invariant)and
“LinearTimeVariantModels”(i.e.,non-time-invariant)inAkyüreketal.[1].
A.1 Non-Time-InvariantModels
Approximatelysimultaneouslywithormorerecentlythan[23],arangeofnon-time-invariantSSMs
havebeenintroduced[14,72,57,56]. ThiscategoryalsocovershighlysimilarearlierRNNvariants
[8,41].
Mamba In Mamba, (2) and (3) directly map onto Eqs. (2a) and (2b) in Gu and Dao [23]. The
notationofGuandDao[23]useamatrixmultiplicationAh insteadofelementwisemultiplication
t−1
A(x)◦h in(REF),butimportantly,Mamba’sAisdiagonal,sowecantakeA(x) =A . Dueto
t t−1 t i ii
exponentialparameterization,itsentriesarenonnegative.
Griffin TheRG-LRUlayerofGriffin[14]usestheequation
(cid:113)
h = a ◦h + 1−a2◦(i ◦x)
t t t−1 t t t
(cid:124)(cid:123)(cid:122)(cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
A(xt)
B(xt)
wherea,i areneurallyparameterizedintermsofx butnoth ;bydesign,a ∈(0,1). φisinstanti-
t t t <t t
atedintermsoflineartransformations,GeLU,andRMSNorm(Figure2inDeetal.[14]). Thelocal
attentionusedbyGriffincanbesubsumedintoanSSMlayer(Remark18).
GatedLinearAttention[GLA72] Thismodel(Section4.4inYangetal.[72])instantiatesour
frameworkusingarecurrenceoftheform(1);whilethestateistwo-dimensionalinthismodel,the
updateisperformedbyelementwiseproductsasin(1). Thegateisobtainedbyapplyingsigmoidto
alineartransformationofx;thus,itsentriesarein(0,1). φisinstantiatedintermsofSwiGLUand
t
LayerNorm.
HGRN HGRN[57]andHGRN2[56]aredefinedbyarecurrenceoftheform(1);thegateentries
are∈(0,1)bydesign. φisinstantiatedintermsofGLU,lineartransformations,andnormalization.
InHGRN,thestateiscomplex,butcruciallythegateremainsreal-valued.
A.2 Time-InvariantModels
Time-invariant SSMs introduced before late 2023 are surveyed by Gu and Dao [23, Appendix B],
such as [47, 67, 52]. Time-invariant SSMs have often used complex-valued states and gates; this
does not have a major impact on our results: First, as complex-valued SSMs subsume real-valued
ones,ourpositiveresultscarryover. Second,ournegativeresultaboutPARITYisaffectedbythis
distinctionandrequiresaseparateargument,seeTheorem13.
Note also that Ah is often described as a general matrix multiplication, but A is diagonalizable
t−1
(e.g. Lemma 3.2 in Gu et al. [24]; [67] for RetNet), which —even though implementation may
bebasedonnon-diagonlizedrepresentations[24]—rendersthemodelequivalenttoonewhereAis
diagonalfromthestart. ThisequivalenceisshownasLemma3.1inGuetal.[24].
B FormalDefinitionsandProofs
B.1 FlipFlop
We begin by introducing key notions of automata theory. References for automata theory include
Eilenberg [16], Hopcroft et al. [30], Sakarovitch [58]. We will provide those key notions that are
14necessary to prove our results. We will focus on deterministic finite-state-automata (DFA), and
simplyrefertothemasfinite-state-automata.4 First,
Definition7. A(deterministic)finite-state-automatonA consistsof:
• afinitealphabetΣ
• afinitestatesetQ
• astartingstateq ∈Q
0
• atransitionfunctionu:Q×Σ→Q
Weextendutoamapu:Q×Σ∗→Qbysetting:
u(q,ε)=q
u(q,w )=u(u(q,w ),w )
1...i+1 1...i i+1
whereεistheemptyword.
Intuitively,u(q ,w)isthestatethatA isinafterreadingw.
0
TheautomatonrecognizesalanguageL⊆Σ∗ifthereisarecognizingsetR⊆Qsuchthat
L:={w:u(q ,w)∈R} (5)
0
Kleene’sTheorem[37]assertsthatalanguageL⊆Σ∗isregular(i.e.,definedbyaregularexpression)
ifandonlyifitisrecognizedbysomefinite-stateautomaton.
AveryfundamentalautomatonunderlyingFlipFlopis:
Definition8. Aset-resetautomatonisafinite-state-automatonwhere(Q\{q })⊆Σand
0
(cid:26)
q ifσ̸∈Q
u(q,σ)= (6)
σ else
Intuitively,suchanautomatonkeepsrecordingthelastseensymbolfromadesignedsetQ⊆Σ.Such
anautomatoniseasilysimulatedwithasinglenon-time-invariantSSMlayer:
Lemma 9. Let A = ⟨Σ,Q,q ,u⟩ by a set-reset automaton. Then there is a single-layer SSM
0
with finite precision and width d = 1+logQ that maps each w ∈ Σ∗ to the state sequence
1...T
u(q ,w ),u(q ,w ),...,u(q ,w )∈QT.
0 1 0 12 0 1...T
Formally,thereisaninjectivemapV :Q→Rd suchthatρ(z)=V(u(q ,w ))fort=1,...,T.
t 0 1...t
Proof. Let B(σ)∈Rlog|Q| be a binary encoding if σ∈Q, and 0∈Rlog|Q| else. Take h =B(q ).
0 0
Let A(σ)=0 if σ∈Q and A(σ)=1 else. After processing a string, the state h is B(σ) where σ
t
is the last symbol in Q that has occurred if any has, and B(q ) otherwise. Coming to (2, in order
0
to avoid division by zero when normalizing if no element of Q has been read, we add a dummy
dimension to h whose value is always 1. We take Mix ,Mix to be the identity. Note that, even
t 1 2
thoughnormalizationwillaffectthenumericalvalues,thebinaryencodingofσ∈Qcanstillberead
(cid:112)
outwithfiniteprecision,as1≤∥h∥ ≤ 1+log|Q|,andthusnonzeroentrieswillremainbounded
t 2
awayfromzero.
Theorem 10. (Repeated from Theorem 1) A two-layer SSM with finite precision can predictively
modelL atarbitrarylengths.
FF
Proof. In the first layer, we use Lemma 9 to simulate a set-reset automaton over the input alpha-
bet Σ ={w,r,i,0,1} where Q =Σ ∪{q }. This layer outputs at each position whether the last
1 1 1 0
instruction was write, read, or ignore. The layer additionally, at each position, forwards the input
symbolusingadditionaldimensions.Formally,atthefirstlayer,ρ(h)allowsustoreadouttheinput
t
symbolsx ,x ∈Σ.
t−1 t
4Acloselyrelatednotionisthesemiautomaton,whichisthenotionconsideredinthecloselyrelatedwork
Liuetal.[44].Semiautomatalackafixedstartstateq .Weincludeq ,butthisdifferenceisnotsubstantialfor
0 0
ourformalresults.
15In the second layer, we again use Lemma 9 to simulate a set-reset automaton over an extended
alphabet Σ :=Σ ×Σ , where the first component indicates the input symbol x and where the
2 1 1 t
secondcomponentindicatesx . Inthisset-resetautomaton,Q contains,besidesastartstateq ,
t−1 2 0
those elements of Σ whose second entry is w. The second layer thus keeps track of the input bit
2
b∈{0,1} following the last write instruction. It additionally forwards the input symbol x using
t
additionaldimensions.
Thesecondlayer,viaρ,thenpredictsthepossiblenextsymbolsonthebasisofthisinformation: If
x ∈{0,1},anyinstructionin{w,r,i}ispossible.Ifx ∈{w,i},anybitin{0,1}ispossible.Ifx =r,
t t t
thebitstoredafterthelastwriteinstructionispossible;ifnowriteinstructionhasappeared(hence,
thesecondautomatonisstillinitsstartstate),anybitin{0,1}ispossible.
B.2 DifficultyofRepresentingPARITY
Definition11. PARITYistheregularlanguageoverΣ={0,1}ofstringswherethenumberofones
iseven. Asaregularexpression,PARITYis(0∗10∗10∗)∗.
Theorem12. (RepeatedfromTheorem2)NoSSMsatisfyingNONNEGATIVEcanrecognizePARITY
atarbitraryinputlengthswithfiniteprecision.
(1) (L)
Proof. WeconsideranSSMwithmultiplelayers,andindicatethelayerinsuperscript:h ,...,h .
t t
We write z(0) for the input token embedding e(w). Consider a SSM processing the word 1t, for
t t
t→∞. Weshow,byinductionoverthenumberoflayers,thefollowingclaim:
(k)
(†)Eachentryofz convergestoavaluebounded,inabsolutevalue,byaconstant.
t
Bytheassumptionoffiniteprecision, convergenceautomaticallyleadstotheentriesbecomingul-
(L)
timately constant. Once we have shown this, we know that z is constant when t is sufficiently
t
large; thus, the parity of the string 1t cannot be read out from z(L) . As a consequence, the SSM
t
cannotrecognizePARITY.Indeed,wehaveshwonthestrongerclaimthatthelanguage(11)∗ –the
language of even-length strings over one symbol – is not recognized by an SSM; we will use this
strongerstatementinCorollary14.
Weproceedtoproving(†). Theclaim(†)istriviallytrueatk=0,astheinputtokenisalwaysthe
(0)
sameandwedefinedz :=e(w). Nowconsiderk>0. Byhypothesis,theactivationsaregivenas
t t
(k) (k)
h =A(x)◦h +B(x) (7)
t t t−1 t
whereA(x),B(x)areconstantα:=A(x),β:=B(x)whent >T ,forsomeT >0. Thesolution
t t t t 0 0
oftherecurrencefort>T is
0
(cid:18) (cid:19)
β β
h
t
=αt−T0 h T0+
α−1
+
1−α
(8)
Eachdimension j=1,...,d ofthisvectorcanbeconstant(if(h ) +
βj
=0),divergeexponen-
T0 j αj−1
tially(α >1),convergeexponentially(α <1)ordivergelinearly(α =1).
j j j
Wenextneedtoshowthatz =Mix (Norm(Mix (h,x)))converges.
t 2 1 t t
First,considertheeffectofapplyingalineartransformationtothestateh. Eachentryoftheresult
t
willbesomelinearcombination
u =λ (h) +···+λ (h) (9)
t 1 t 1 d t d
Ifeachα <1,thenu converges. Ifsome|α |≥1,theremaybesomecancellationifα =α for
j t j i j
some i̸= j; cancellation can only lead to full erasure of the relevant terms or to a remaining term
withthesameexponent. Inconclusion,eachentryu willagaineitherconvergetoafinitevalueor
t
divergetowards±∞.
We now need to understand the behavior of Mix (h,x). Recall that, based on our survey (Ap-
1 t t
pendixA),weallowedittocontainlinear,GLU[13],andSwiGLU[61]components.IfMix (h,x)
1 t t
implementsalineartransformationonly,eachentrylikewisemayconverge,divergelinearly,ordi-
verge exponentially. We note that—if σ is the sigmoid function—σ(u) always converges, as σ
t
16simply saturates to 0 or 1 if u diverges. Hence, if Mix (h,x) implements GLU, each entry like-
t 1 t t
wise may converge, diverge linearly, or diverge exponentially. Finally, if Mix (h,x) implements
1 t t
SwiGLU, each entry of the result will be a product of a linear combination of the form u, and
t
Swish appliedtoanothersuchlinearcombination. Dependingonthebehaviorofthesetwou-like
β t
terms,theoutcomewillbehaveasaproductofsequencesthatmayconvergeexponentially,diverge
exponentially,ordivergelinearly–e.g.,theoutcomemayalsodivergequadratically,orconvergeas
nα−n,etc.
If all dimensions of Mix (h,x) converge, then Norm(Mix (h,x)) will also converge to a scaled
1 t t 1 t t
versionof βi ,scaledbyaboundedfactorasβ ̸=0. NowassumesomedimensionsofMix (h,x)
1−αi i 1 t t
donotconverge;inthiscase,foranytwodimensionsi,j,eithertheirratiowillconvergetoaconstant,
orconvergeto0or±∞. AfterapplyingNorm(·),theentriesasymptoticallydominatingtheothers
willconvergetoafinitevaluebounded,inabsolutevalue,by1;theotherswillconvergetozero.
In conclusion, we have found that each entry of Norm(Mix (h,x)) converges to some number
1 t t
bounded,inabsolutevalue,by1. AsMix iscontinuous,eachentryofz likewiseconverges,witha
2 t
bounddependingontheLipschitzconstantofMix .
2
We next show the result, referenced in the main paper text after Theorem 2, about time-invariant
SSMswithcomplex-valuedgates:
Theorem13. TIME-INVARIANT SSMscannotrecognizePARITYwithfiniteprecisionatarbitrary
inputlengths,evenwithcomplex-valuedgates,aslongaseachentryineachAhasarationalangle
inthecomplexplane.
Here,byarationalangle,werefertoananglethatisarationalnumberwhenexpressedindegrees;
such angles are rational multiples of 2π when expressed in radians. As the rational angles are
denseinthereals,oneexpectsthatevenifsomeirrationalanglespermittedmodelingPARITY,such
solutions would be very hard to find – in particular given that irrational numbers are not exactly
representedinfiniteprecision.
Proof. Byassumption,anyA ∈Cinanylayercanbewrittenas
j
A =r exp(2πiq ) (10)
j j j
where q ∈[0,1] is rational and r ≥0 is real – here, 2πq is known as the argument of A ; it
j j j j
describestheangleofA inthecomplexplaneinradians. Correspondingly,theangleindegreesis
j
describedbyq ·360◦;thisisrationalifandonlyifq is.
j j
Asatime-invariantSSMhasafinitenumberofsuchvaluesA ,acrossallitslayers,wecanselecta
j
positiveintegerW suchthatWq ∈Nforeach j,ineachlayer. Importantly,(A )W =(r )W ∈R.
j j j
NowconsidertheactionofanylayeroftheSSMonaninputsequenceoftheformA =(10W−1)T.
T
Theclaimisthat,foreachi=1,...,W,thesequence
(k)
z (11)
tW+i
convergesast →∞. AsintheproofofTheorem2,inthefinite-precisionsetting,convergeentails
thatthesequencebecomesultimatelystationary. NotethattheparityofA equalstheparityofT;
T
(k)
hence,itisimpossibletoreadouttheparityfromz whenT islarge.
TW
17Nowconsider,suppressingtheindexforthedimensionin1,...,d:
tW
h(k) =∑AtW−iB(z(k−1)
)
tW i
i=1
tW
=∑AtW−iB(z(k−1)
)
i
i=1
t (s+1)W−1
= ∑ ∑ AtW−jB(z(k−1) )
sW+j
s=1 j=sW
t W−1
= ∑ ∑ A(t−s)W−jB(z(k−1) )
sW+j
s=1 j=0
t W−1
= ∑ ∑(rexp(2πiq))(t−s)W−jB(z(k−1) )
sW+j
s=1 j=0
t W−1
= ∑ ∑ r(t−s)W−jexp(−2πijq)B(z(k−1) )
sW+j
s=1 j=0
W−1 t
= ∑ exp(−2πijq)∑r(t−s)W−jB(z(k−1) )
sW+j
j=0 s=1
(k−1)
SeparatelyconsideringsummationbeyondT atwhichz hasbecomestationary,weget
0 tW+j
 
=(cid:34) T ∑0−1 ...(cid:35) +

(cid:32) W ∑−1
exp(−2πijq)B(z(k−1)
)r−j(cid:33)(cid:32) ∑t r(t−s)W(cid:33)


 j 
j−0  j=T0 s=1 
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
U1 U2 U3
U andU do not depend on t. Intuitively, U ∈C determines a direction in the complex plane,
1 2 2
whereasU ∈Rdeterminesamagnitude. ItremainstounderstandU ,whichcanberewrittenas:
3 3
t t t (cid:40) 1−(rW)t −1 r̸=1
U = ∑r(s−1)W =r−W ∑(rW)s=r−W ∑(rW)s−r−W =r−W 1−(rW) (12)
3
s−1 r=1
s=1 s=1 s=0
We have now achieved a situation like in the proof of Theorem 2: U can converge exponentially,
3
divergelinearly,ordivergeexponentially.Theremainderoftheproofisanalogoustothatproof.
ThefollowingCorollaryofTheorem2willbeusedintheproofofTheorem4:
Corollary14. Assume NONNEGATIVE, SSMswithfiniteprecisioncannotrecognizeanynon-star-
freeregularlanguage.
Proof. For any non-star-free regular language L, there are words u,v,w such that the membership
uvnw∈L isdeterminedbytheparityofn[46]. Fixanysuchu,v,w∈Σ∗.
Now assume an SSM satisfying NONNEGATIVE can recognize L with finite precision. We can
subsumetheactionofuintothestateh bytakingh ,ineachlayer,tobethestateoftheSSMafter
0 0
readingu. WenowhaveanSSMthatcandeterminetheparityoft whenfedawordoftheformvtw.
ForthisSSM,wewanttoshow
(†)Whenfedwordsoftheformv,v2,v3,...,foreachi=0,...,|v|−1,andeachlayerk=1,...,L,
(k)
thesequencez convergesast→∞.
t|v|+i
Asintheprecedingtwoproofsinthissection,convergenceentailsbecomingultimatelyconstantin
thefinite-precisionsetting.
18Theclaim(†)isimmediateatk=0.
Nowatk>0,wewrite:
(k) (k−1) (k−1) (k)
h =A(z )...A(z )h
t|v|+i t|v|+i (t−1)|v|+i+1 (t−1)|v|+i
(k−1) (k−1) (k−1)
+A(z )...A(z )B(z )
t|v|+i (t−1)|v|+i+2 (t−1)|v|+i+1
(k−1) (k−1) (k−1)
+A(z )...A(z )B(z )
t|v|+i (t−1)|v|+i+3 (t−1)|v|+i+2
+...
(k−1)
+B(z )
(t−1)|v|+i
(k)
OntheRHS,ast →0,alltermsexceptforh becomeconstantbytheinductivehypothesis.
(t−1)|v|+i
Hence,therearesomeα,βsuchthat,forsufficientlylarget,
(k) (k)
h =α◦h +β (13)
t|v|+i (t−1)|v|+i
Wearenow,foreachi,inthesamesituationasintheproofofTheorem2]: eachdimensionofthis
recurrencecanconvergeexponentially,divergeexponentially,ordivergelinearly;asinthatproof,it
(k)
followsthatz convergesast→∞.
t|v|+i
Wehaveshown(†).
Wenowfollowupbyshowingthat
(∗)Whenfedwordsoftheformvtw,foreachi=1,...,|w|,andeachlayerk=1,...,L,thesequence
(k)
z convergesast→∞.
t|v|+i
Again,atfiniteprecision,convergenceentailsthatthesequencesareultimatelyconstant. Again,(∗)
istrueatk=0trivially. WhenfeedingtheSSMwordsoftheformvtw,ineachlayer,thefinalstate
isineachlayerk,ateachi=1,...,|w|:
(k) (k−1) (k−1) (k)
h =A(z )...A(z )h
t|v|+i t|v|+|w| t|v|+1 t|v|
(k−1) (k−1) (k−1)
+A(z )...A(z )B(z )
t|v|+i t|v|+2 t|v|+1
+...
(k−1)
+B(z )
t|v|+i
Byinductivehypothesis,forlarget,thereareψ,γ suchthat
i i
(k) (k)
h =ψ ◦h +γ
t|v|+i i t|v| i
(k)
and,asshownbefore,eachentryofh convergesexponentially,divergesexponentially,ordiverges
t|v|
linearly. Now,byassumption,onecanreadout,atfiniteprecisiion,theparityoft from
(L) (k)
z =Mix (Norm(Mix (ψ ◦h +γ )))
t|v|+|w| 1 2 |w| t|v|+i |w|
We now simply absorbing the operation X (cid:55)→ψ ◦X+γ into Mix , and obtain by the same
|w| |w| 2
(L)
argumentsasintheproofofTheorem2thatz convergesasr→∞. Thisisacontradictionto
t|v|+|w|
(L)
theclaimthattheparityoft canbereadoutfromz atfiniteprecision.
t|v|+|w|
B.3 ProofofTheorem4
OurproofofTheorem4willrelyonthealgebraictheoryoffiniteautomata,specificallythecascade
productandtheKrohn-RhodesTheorem[39]. Thesetechniques,originallydevelopedinthe1960s,
19haverecentlybeenintroducedtothetheoreticalstudyoftransformersbyLiuetal.[44];weprovide
self-containeddefinitionsandsomewhatdifferentnotation,tailoredtoourproofsaboutstate-space
models. In general, we will find that the properties of state-space models allow more natural and
directly length-generalizing implementations of these algebraic notions than what is possible for
transfomers.
Recall the definition of a finite-state-automaton (Definition 7). Our construction will build on an
importantoperationonautomata,thecascadeproduct[39,16,22]:
Definition 15. Given two automata A ,A with associated alphabets Σ ,Σ and state sets Q ,Q
1 2 1 2 1 2
suchthat
Σ =Q ×Σ , (14)
2 1 1
thecascadeproductA ≀A istheautomatongivenby
2 1
• Σ=Σ
1
• Q=Q ×Q
2 1
• q isthetupleofthestartingstatesofA ,A
0 2 1
• u(⟨q,p⟩,σ)=⟨u (q,⟨p,σ⟩),u (p,σ)⟩
2 1
Wenotethattheliteratureusuallyuses“◦”forthecascadeproduct[e.g.16]. Toavoidcollisionwith
theelementwiseproduct“◦”(e.g.,(1)),wehereinsteaduse“≀”,usuallyusedforthewreathproduct
–aproductonmonoidswithaneffectanalogoustothecascadeproduct[2].
Whiletheformaldefinitioniscumbersome, theintuitionbehinditissimple: Thecascadeproduct
correspondstofirstreadingawordwwithA ,recordingthestatesequenceq ,q ,...,q ∈Q and
1 0 1 |w| 1
–ateacht=1,...,|w|–pastingthestateq togetherwiththeinputsymbolw ∈Σ –resultingin
t−1 t 1
awordoveranewalphabetQ ×Σ ,andthenrunningA ontheresultingword. Theoverallstateof
1 1 2
A ≀A afterreadingawordisthetupleofthestatesreachedbyA andA .NotethatwewriteA ≀A ,
2 1 2 1 2 1
rather than, A ≀A , because the second argument of the cascade product (A ) intuitively reads the
1 2 1
inputfirst,preprocessingitfortheotherautomaton,A .
2
Thesomewhatinscrutableupdateruleforu(·,·)encodestheactionofA inthesecondcomponent,
1
andtheactionofA ontheextendedalphabetinthefirstcomponent. Thereisacloseanalogytothe
2
stacking of sequence models, and we will leverage this analogy to translate cascade products into
multilayerSSMs. Thefundamentalbackgroundhereisthefollowingclassicalfact:
Fact16(ConsequenceofKrohn-RhodesTheorem[39]andSchützenberger’sTheorem[60]). Each
star-free regular language is recognized by an iterated cascade product of set-reset automata,
(...(A ≀...)≀A )≀A ,whereeachA isaset-resetautomaton.
1 n−1 n i
WenowformallyshowthatcascadeproductscanbetranslatedtoSSMstacking. Weneedanauxil-
iarylemma,whichprovidesasingle-layerSSMthatencodestheinputw instateh –wewilluse
t−1 t
ittoforwardinformationaboutthestateofA att−1toA att:
1 2
Lemma 17. Let Σ be an alphabet, and consider words w∈Σ∗. There is a one-layer SSM with
d=4|Σ|suchthat,fort=2,...,|w|,thecharacterw canbereadoutfromz atfiniteprecision.
t−1 t
ToproveLemma17,afirstideaistouseanexponentialmovingaveragewithA=1/2toencodethe
recentinputcharactersinh;thiseffectivelyencodesthefullhistoryintothebinaryexpansionofh,
t t
andinparticularallowsreadingoutthesecond-lastinputinprinciple. However,suchaconstruction
does not work at finite precision, because rounding may make it impossible to extract even the
second-most-significantbit.5 WeavoidthisproblemsimplybytakingA=1/4,effectivelyutilizing
onlyeverytwodigitsinthebinaryexpansionofh,ensuringthatthesecond-lastinputcanberead
t
outataconstantmargin. Wenowprovidetheformalproof:
5Informally,inbinary,0.0111111...111and0.1arearbitrarilyclose.
20Proof. WebeginbyshowingtheclaiminthespecialcaseΣ={1,0}. Here,wetaked=4,and
h =[0,0,0,0]T
0
A(e )=[1/4,1/4,0,0]T
0
A(e )=[1/4,1/4,0,0]T
1
B(e )=[1,0,1,0]T
0
B(e )=[0,1,0,1]T
1
Nowweseparatelyconsiderthestateh dependingontheformoftheprefixw : Ifw =...00,
t 1...t 1...t
then
 ∈[1,2] 
∈[0,1/8]
h =  (15)
t  1 
0
Ifw =...10,then
1...t
 ∈[1,1.25] 
∈[1/4,1/2]
h =  (16)
t  1 
0
Inparticular,assumingw =0,onecanreadoffw from(h) withamarginofsize1/8. Asw is
t t−1 t 2 t
encodedinh andduetosymmetry,analogousstatementsholdwhenw =1.
t t
Now,foreachσ∈Σ,werunsuchaone-layerSSMwhere0representsσand1representsallother
characters.6 Byrunningtheseinparallel, weobtainanSSMwithd=4|Σ|fromwhosestatesone
√
canreadoutw atfiniteprecision. Astheentriesinh areallboundedby2,wefind∥h∥ ≤2 d
t−1 t t 2
independentoft,andthemarginisstillboundedawayfromzeroafternormalization,andthusinz,
t
wherewecanassumeMix ,Mix tobetheidentity.
1 2
Remark 18. Some SSMs include local convolutions [e.g. 21, 23] or local attention [14], which
aggregateinformationfromalocalwindowofsomewidth∆>0. Thesedonotincreasetheexpres-
sivecapacitybeyondSSMsaswehavedefinedin(1-2),asaggregationoflocalinformationcanbe
simulatedwithasingleSSMlayer: UsingthelayerconstructedintheproofofLemma17,giventhe
stateh,onceonehasreadoutw asdescribedintheproof,onecanrecoverh fromh andx;
t t−1 t−1 t t
theninductivelyreadoutw usingh andx ,etc. Thus,uptoanygivenwidth∆>0,onecan
t−2 t−1 t−1
readoutw ,...,w fromthestateh ofthislayeratfiniteprecision.
t−∆ t−1 t
WearenowreadytotranslatecascadeproductsintoSSMstacking:
Lemma 19. Let A , A be two finite-state-automata, and assume that there are two SSMs with
1 2
top-level states z(L1,1) and z(L2,2) that map each w to the state sequences under A 1, A 2, at finite
precision.
Formally,onawordw,ρ
(z(L1,1)
)andρ
(z(L2,2)
)providethestatesequencesofA ,A .
1 t 2 t 1 2
ThenthereisanSSMwithL +L +1layersthatmapseachwtothestatesequenceunderA ≀A ,
1 2 2 2
againatfiniteprecision.
Wenotethataconceptuallyrelatedresultholdsfortransformers[Lemma12in44].However,SSMs
allowasimplerandlength-independentconstruction,astheydonotrequirepositionalencodingsto
implementsuchaconstruction.
Proof. ThelowerlayersarebasedontheSSMmodelingA . Weduplicateeachchannel,sowenow
1
have2ddimensions.Wefurtheradddfurtherdimensionsthatdirectlypassontheinputembeddings,
i.e.,A≡0,B≡1,Mix ≡Id onthesedimensions.
j
In the resulting SSM, zL1 indicates both w itself, and the state reached by A after reading w .
t t 1 1...t
Thestateisredundantlyindicatedbytwoseparatesetsofddimensions;thecharacterw isindicated
t
byd furtherstate.
6Infact,usingabinaryencodingofΣ,onecanachieved=4log|Σ|.
21Note, however, that the second automaton in the wreath product requires access to the state q
t−1
ratherthanq.
t
Forthis,weaddalayerprovidedbyLemma17,ofwidth4|Q|. Additional2d dimensionspasson
(1)w,and(2)thestatethatA reachesafterreadingtheprefixw .
t 1 1...t
WenowhaveL +1layerswherezL1+1 has2d+4|Q|dimensionsandindicates(1)w,(2)thestate
1 t t
that A reaches after reading the prefix w , (3) the state that A reaches after reading the prefix
1 1...t 1
w .
1...t−1
ThefirstandthirdpieceofinformationarenowfedintothesecondSSM;thesecondpieceispassed
onind additionaldimensions. AsweallowedAandBtobearbitraryfunctions,weredefinethese
inthelowestlayerofthatsecondSSMtoreadoutfromthe4|Q|-dimensionalcomponentindicating
(3),providingthedesiredsecond-to-laststate.
WehaveconstructedanSSMwithL +L +1layers,wherezL1+L2+1 indicates(1)w,(2)thestate
1 2 t t
that A reaches after reading the prefix w , (3) the state that A reaches after reading the prefix
1 1...t 2
w pasted with the state sequence of A . This information is sufficient for reading out the state
1...t 1
sequenceofA ≀A .
2 1
Notethatthenumberofchannelsmaynotbeconsistent,asitis3d inthetopandbottomparts,but
2d+4|Q|inthemiddle;wesimplypadtothelargerdimensionality.
Wearenowreadytoshowtheexistenceoflength-generalizingSSMsforanystar-freestatetracking
problem,andconcludewiththetheorem:
Theorem20(RepeatedfromTheorem4). AssumingNONNEGATIVE,SSMscanpredictivelymodel
aregularlanguageL atfiniteprecisionifandonlyifL isstar-free.
Proof. Weneedtoshow:
1. SSMsatfiniteprecisioncanpredictivelymodelallstar-freelanguages. Foreachlanguage,
asingleSSMsisapplicableatarbitrarylengths.
2. Assuming NONNEGATIVE,finite-precisionSSMscannotrecognizeanynon-star-freereg-
ularlanguage.
ThesecondstatementisCorollary14;itsufficestoprovethefirststatement.
Assume L is star-free. By the Krohn-Rhodes theorem, there is an automaton A that is a cascade
productofsomeset-resetautomatathatrecognizesL. ByLemmas9and19,thereisanSSMthat
computesthestatesequenceofthatautomaton.
Nowwenotethat,sinceA recognizesL,thestateqafterreadingwissufficientfordeterminingthe
setofcharactersthatcanfollowthisprefixinanyelementofL. For,assumeotherwise,thenthere
arewordsw,w′ suchthatu(q ,w)=u(q ,w′)andσ∈ΣsuchthatwσΣ∗∩L ̸=0/ butw′σΣ∗∩L =
0 0
0/; then u(q ,wσ)=u(q ,w′σ) but the set R (5) is reachable from u(q ,wσ) but not u(q ,w′σ),
0 0 0 0
contradiction.
Hence, the SSM’s outputs can be transformed, by composing ρ with a map from states to next-
charactersets,topredictivelymodelL.
Theorem 21. SSMs with complex-valued coefficients evading both NONNEGATIVE and TIME-
INVARIANTcanrepresentallregularlanguagesknowntobeinTC0.
We we do not use this theorem in the main paper, due to the nonexistence (as far as we know) of
implementedSSMswiththisproperty.
Proof. SSMsevadingbothNONNEGATIVEandTIME-INVARIANTcancountmoduloanyintegerk,
usingd=1andA(e )=e2πi/k,A(e )=1,B≡0,h =1.Thisisageneralizationoftheconstruction
1 0 0
forPARITYdescribedinSectionB.2,sincee2πi/k=−1.
The set of regular languages known to be in TC0 is the set of regular languages whose syntactic
monoidcontainsnonon-solvablegroups[5]. Theselanguagesarerecognizedbycascadeproducts
22ofset-resetautomataandautomataperfomingmodularcounting[64].Bytheremarkabove,together
withLemma9andLemma19,suchcascadeproductscanbesimulatedbySSMs.
B.4 MaintainingCounters
AsthefirststepinshowingTheorem5,weshowthatSSMscanmaintainunboundedcounters,and
thatonecanreadoutthevaluesofsuchcounters,uptofinitebounds,evenatfiniteprecision:
Lemma 22. LetC>0 be an integer. Let any function u:Σ→ZC be given. Let L∈N. Then a
one-layerSSMwithfiniteprecisioncancompute,ateachpositioni=1,...,T:
(cid:32) (cid:32) (cid:33) (cid:33)
i
max min ∑u(w),L ,−L (17)
i
j=1
(1)
inthesensethatρcanreadthisoutfromz withfiniteprecision.
i
Proof. Defined=2L+1. Defineh =0∈Rd. Foreachx∈Σ,defineA(e )=1∈Rd andB(e ) ∈
0 x x i
Rd byB(e ) =u(x). Inordertoreadoutthestateh uptoalimitL,wedefine
x i t
φ(h,x)=Norm(h +[0,1,−1,2,−2,...,−L,L]) (18)
t t t
By testing which entries of the result are negative or positive, one can read out the state up to L
even after rounding φ(h,x) to finite precision. The proof straightforwardly extends to multiple
t t
counters.
Wearereadytoprovethetheorem:
Theorem 23. (Repeated from Theorem 5) Consider the languages Dyck-1, Shuffle-Dyck, n-ary
BooleanExpressions, anbn, anbncn, andanbncndn, asdefinedinAppendixC.Eachoftheseispre-
dictivelymodeledbyanSSM.
Proof. Foreachoftheselanguages,wefirstdefineanassignmentu:Σ→ZC:
Foranbn:(here,C=1)
u(a)=1
u(b)=−1
ForDyck-1:(here,C=1)
u(“(”)=1
u(“)”)=−1
ForShuffle-Dyck-k(here,C=k)
u(“(”)=(0,...,0,1,0...0) where1isinthei-thslot
i
u(“)”)=(0,...,0,−1,0...0) where−1isinthei-thslot
i
Foranbncn:(here,C=2)
u(a)=(1,0)
u(b)=(−1,1)
u(c)=(0,−1)
Foranbncndn:(here,C=3)
u(a)=(1,0,0)
u(b)=(−1,1,0)
u(c)=(0,−1,1)
u(d)=(0,−1,−1)
ForBooleanExpressions:(here,C=1)
u(⟨VALUE⟩)=−1
u(⟨n−ARY⟩)=+n
23Foreachofthesemappings,weuseLemma22atL=1toconstructaone-layerSSMsthatcan,for
eachoftheCcounters,distinguishthevalues≤−1,0,≥1.
Inparallel,wepassontheinputsymbolitselfinlog|Σ|furtherdimensions.
Overall,theoutputz ofsingleSSMlayerprovides,ateveryposition,boththeoriginalsymbolinΣ
t
andanelementof{≤−1,0≥1}C.
Wecanthusviewtheoutputofthislayerasastringoveranenrichedstringofsymbolsσ ×σ ∈
1 2
Σ×{≤−1,0≥1}C. Basedonthis,onecanpredictivelymodeltheselanguagesasfollows.
ForDyck-1,thenexttokenisEOSor“(”ifσ =0,and“(”or“)”afteranyotherprefix(notethat
2
predictivemodelingassumesvalidprefixes).
Shuffle-k-Dyckissimilar: EOSisallowedifandonlyifallcountersarezero. Anopeningbracket
isalwaysallowed. Aclosingbracketisonlyallowediftherespectivecounteris>0.
Foranbn,thenexttokenisaorbifσ =a;bifσ=(a,≥1)or(b,≥1);EOSifσ=(b,0).
1
Constructionsforanbncn,anbncndnaresimilar.
ForBooleanexpressions,thenexttokenis⟨n−ARY⟩orEOSifσ =0,andanyothertokenother-
2
wise.
Alloftheseconstructionscanbeencodedusinganappropriatefunctionρapplyingtoz.
t
B.5 Bounded-DepthDyck
Definition 24. The language Dyck [28, 74] is given by the CFG with the nonterminals
K,h
{S ,S ,...,S ,S }andthefollowingproductionrules:
0 1 h−1 h
S →( S ) |...|( S ) |ε
h 1 h−1 1 K h−1 K
S →( S ) |...|( S ) |ε
h−1 1 h−2 1 K h−2 K
......
S →( S ) |...|( S ) |ε
2 1 1 1 K 1 K
S →( S ) |...|( S ) |ε
1 1 0 1 K 0 K
S →ε
0
andthestartsymbolS .
h
Theorem25. (RepeatedfromTheorem6)Thebounded-depthDycklanguageDyck ispredictively
K,h
modeledbyatwo-layerSSMatfiniteprecision,withd=O(hlogK).
Proof. Inthefirstlayer,wecalculateeachtokendepthuptohusingLemma22.Afterthefirstlayer,
ateachposition,theactivationswillindicateboththedepthuptoh,andtheidentityofthesymbol.
Thespaceofactivationsisthus{0,...,h}×{( ,) ,...,( ,) }. Wethen,foreachdepthl=1,...,h,
1 1 K K
defineaset-resetautomaton(Definition8)givenbythesetQ :={l}×{( ,) ,...,( ,) }. Running
l 1 1 K K
alloftheseset-resetautomatawilltellus,foreachdepth,theidentityofthelastbracketatthatdepth.
Wecandeducethemaximumdepthh′atwhichthelastbracketisanopeningone,andthusinferthe
setofvalidnextsymbols. Theactivityoftheseset-resetautomatacan,inparallel,besimulatedbya
secondSSMlayerusingLemma9. Weneedhsuchautomata,andeachSSMhaswidthlogK.
C DefinitionsofLanguages
Here, we provide formal definitions of languages from the test suite based on Bhattamishra et al.
[6]. DescriptionsfollowBhattamishraetal.[6],andareincludedhereforself-conrainedness. Inall
cases,ourdatagenerationsetupisdirectlytakenfrom[6].
C.1 RegularLanguages
Tomita Grammars. Used primarily as a benchmark language family for assessing sequence to
sequencemodels[68], someofthelanguagesinthisfamilyarestar-free(withdot-depthof1)and
24somenon-star-free. AlltheregularlanguagesofthefamilyaredefinedonthealphabetΣ={0,1}.
IndividuallanguagedefinitionsareavailableinTable1.
DDD .Wefollowthedefinitionof[6]todefinetheD familyofstar-freelanguages.Inourexperiments
nnn n
weonlygenerateD ,D ,andD 2languages,giventheprovenequivalenceofTomita-2andD . All
3 4 1 2
thelanguagesofthefamilyaredefinedonthealphabetofΣ={a,b}. D =(aD b)∗ hasleveln
n n−1
inthedot-depthhierarchy.
PARITY.PARITYisthesetofallstringsonthealphabetΣ={0,1}suchthatthenumberof1’sis
even. ThislanguagecanbeeasilyrecognizedbyaDFAwithjusttwostates.
Others. Wefurtherhavethenon-star-freelanguages(aa)∗,(aaaa)∗ and(abab)∗,andthestar-free
languagesaa∗bb∗cc∗dd∗ee∗,{ab}∗d{b,c}∗,and{0,1,2}∗02∗.
C.2 CounterLanguages
DyckandShuffle-Dyck. Dyck-1isdefinedonthealphabetΣ={[,]}andderivedusingthefollow-
ingCFGproductionrule: S→(S)|SS|ε.
We further use the family of Shuffle-k languages [51]. Shuffle-Dyck-k is defined in terms of Σ=
{( ,) ,...,( ,) }. It is defined as the shuffle of k Dyck-1 languages, each defined in terms of the
1 1 k k
alphabetΣ ={(,)}wherei=1,...,k.
i i i
n-aryBooleanExpressions. Thisisthesetofvalidexpressionsovervariousoperators. Wefocus
onup-to-3-aryexpressions,definedusingthefollowinggrammar:
S→⟨VALUE⟩
S→⟨UNARYOPERATOR⟩S
S→⟨BINARYOPERATOR⟩SS
S→⟨TERNARYOPERATOR⟩SSS
Thislanguageisrecognizedbyacounterautomaton[19].
Others Wefurtherincludethelanguagesoftheformsanbn,anbncn,andanbncndn.
Grammar Star-Free Definition
1 Yes 1*
2 Yes (10)*
3 No strings without 12n+102m+1
substrings
4 Yes strings without any 000’s sub-
strings
5 No stringswithanevennumberof
01’sand10’s
6 No strings where number of 0’s -
numberof1’sisdivisibleby3
7 Yes 0*1*0*1
Table1: TomitaGrammars
25Language Model Bin-1[1,50] Bin-2[51,100] Bin-3[101,150]
Transformer 100.0 100.0 100.0
Mamba1 100.0 62.6 13.91
Dyck-1
Mamba2 100.0 49.1 9.5
Mamba3 100.0 53.95 10.0
Transformer 100.0 100.0 93.0
Mamba1 100.0 49.5 2.3
Shuffle-2
Mamba2 100.0 61.5 8.2
Mamba3 100.0 65.5 9.7
Transformer 100.0 100.0 98.8
Mamba1 100.0 44.4 4.3
Shuffle-4
Mamba2 100.0 63.8 7.2
Mamba3 100.0 56.2 7.8
Transformer 100.0 99.9 94.0
Mamba1 100.0 39.4 3.4
Shuffle-6
Mamba2 100.0 61.2 6.75
Mamba3 100.0 59.6 9.85
Transformer 100.0 100.0 99.8
Mamba1 99.75 65.7 7.05
Boolean-3
Mamba2 99.95 47.25 2.3
Mamba3 100.0 73.45 8.6
Transformer 100.0 99.8 99.0
Mamba1 99.9 30.05 7.6
Boolean-5
Mamba2 100.0 80.2 14.9
Mamba3 99.25 60.7 6.25
Transformer 100.0 100.0 100.0
Mamba1 100.0 4.1 0
anbn
Mamba2 100.0 9.4 0
Mamba3 100.0 21.3 0
Transformer 100.0 100.0 100.0
Mamba1 100.0 0 0
anbncn
Mamba2 100.0 7.6 0
Mamba3 100.0 5.1 0
Transformer 100.0 100.0 99.4
Mamba1 100.0 4.76 0
anbncndn
Mamba2 100.0 0 0
Mamba3 100.0 0 0
Table 2: Accuracies on the counter Languages from the Bhattamishra et al. [6] test suite. Trans-
formerresultsreportedbasedonBhattamishraetal.[6].ForMamba,wereportbestsettings(chosen
basedoninputsoflength[1,50])at1(Mamba1),2(Mamba2),3(Mamba3)layers. Resultsforthe
best-performing layer count, from the first two bins, are shown in Figure 3. On these languages,
thereisalsoathirdbin.
D ExperimentalDetails
AllexperimentsusedtheMambareferenceimplementation7. Unlessstatedotherwise,wefollowed
thedefaultsgiventhere(d =16,d =4,expand=2),aswefoundthedefaultcombination
state conv
toworkbetterthanotheroptions. Wetunedd foreachlanguage.
model
D.1 Bhattamishraetal.[6]testsuite
DataPreparation Forallthelanguages,weuseeitherthedatapreparedbyBhattamishraetal.[6]
or—wherenotavailable—theirdata-generationscripts,allowingfullcomparabilitywithresultsthey
reported for transformers. We used their official code and data release at https://github.com/
satwik77/Transformer-Formal-Languages (last commit 48eea2e; MIT license). Training sets
7https://github.com/state-spaces/mamba/blob/main/README.md
26Language Model Bin-1[1,50] Bin-2[51,100]
Transformer 100.0 100.0
Mamba1 100.0 100.0
Tomita1
Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer(posenc) 100.0 92.4
Mamba1 100.0 100.0
Tomita4
Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer 100.0 100.0
Mamba1 100.0 100.0
Tomita7
Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer 100.0 100.0
Mamba1 100.0 100.0
Tomita2
Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer 100.0 100.0
Mamba1 100.0 100.0
aa∗bb∗cc∗dd∗ee∗
Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer 100.0 100.0
Mamba1 100.0 100.0
{a,b}∗d{b,c}∗
Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer 100.0 68.7
Mamba1 100.0 100.0
{0,1,2}∗02∗
Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer 74.6 3.1
Mamba1 100.0 100.0
D
2 Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer 80.9 8.5
Mamba1 100.0 100.0
D
3 Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer 90.2 3.3
Mamba1 100.0 100.0
D
4 Mamba2 100.0 100.0
Mamba3 100.0 100.0
Transformer 95.18 1.5
Mamba1 93.65 93.35
D
12 Mamba2 99.9 95.55
Mamba3 99.99 99.85
Table3: AccuraciesontheregularLanguagesfromtheBhattamishraetal.[6]testsuite-1sthalf.
TransformerresultsreportedbasedonBhattamishraetal.[6]. ForMamba,wereportbestsettings
(chosenbasedoninputsoflength[1,50])at1(Mamba1),2(Mamba2),3(Mamba3)layers. Results
forthebest-performinglayercountarealsoshowninFigure3.
typicallyconsistof10Ksamples,withlengthsvaryingbetween1to50. Therearetwoheldoutbins:
one with in-distribution lengths ([1,50]), and one testing length generalization (lengths [51,100]).
The first one was used for hyperparameter optimization. Each bin typically contains around 2K
samples. Howeverforlanguagessuchasanbn,wherethenumberofpositiveexamplesineachbin
waslimited,allpossibleexamplesforthatbinareincluded.
27Language Model Bin-1[1,50] Bin-2[51,100]
Transformer 68.7 0
Mamba1 26.95 0
Parity
Mamba2 80.05 4.15
Mamba3 91.15 16.7
Transformer 100.0 0
Mamba1 2.1 0
(aa)∗
Mamba2 2.1 0
Mamba3 4.2 0
Transformer 100.0 0
Mamba1 0 0
(aaaa)∗
Mamba2 0 0
Mamba3 4.0 0
Transformer 100.0 2.5
Mamba1 0 0
(abab)∗
Mamba2 0 0
Mamba3 0 0
Transformer 75.4 10.8
Mamba1 25.99 12.49
Tomita3
Mamba2 36.88 17.05
Mamba3 60.85 29.37
Transformer 29.3 0.0
Mamba1 15.94 0
Tomita5
Mamba2 34.5 0
Mamba3 38.4 0
Transformer 88.8 0
Mamba1 7.2 0
Tomita6
Mamba2 37.8 0
Mamba3 54.56 0.04
Table4: AccuraciesontheregularLanguagesfromtheBhattamishraetal.[6]testsuite-continued.
TransformerresultsreportedbasedonBhattamishraetal.[6]. ForMamba,wereportbestsettings
(chosenbasedoninputsoflength[1,50])at1(Mamba1),2(Mamba2),3(Mamba3)layers. Results
forthebest-performinglayercountarealsoshowninFigure3.
Hyperparameters Foreachlanguage,weconductedextensivehyperparametersearch. Wevaried
thed parameterinMambaacrosstheset{16,32,64,128,256}.Additionally,weexperimented
model
with the number of layers in our model, ranging from 1 to 3, training each configuration for 100
epochs.ForlanguageswhereMambaperformedwell,thisnumberoflayerswassufficient.However,
forlanguageswhereMambastruggled,weincreasedthenumberoflayersupto12,withlittletono
success.
WeusedtheAdamWoptimizer. Toidentifyoptimallearningrates,westartedwithacoarsehyper-
parametersearchusingvaluesfromtheset{0.001,0.0001,0.00001}. Ifoneoftheselearningrates
showed high performance, we conducted a more fine-grained search to find the optimal learning
rate. Finally, we varied the batch size from {16, 32, 64} for datasets with 10K training examples.
Forlanguageslikeanbn withlimitedtrainingsize,wesearchedforanoptimalbatchsizewithinthe
set{5,10}.
D.2 FlipFlop
WeobtainedthedatasetofLiuetal.[45]fromtheirrelease,https://huggingface.co/datasets/
synthseq/flipflop(MITlicense).
Oursetupcorrespondstothedeterministic(“clean”)modeinLiuetal.[45].
Matching Figure 2 in Liu et al. [45], we evaluated both with in-distribution data (matching the
distributionofthetrainingdataset)withp =0.8,p =0.1,p =0.1,andusinganoutofdistribution
i w r
sparse tail with p =0.98,p =0.01,p =0.01, where p,p ,p refer to the probabilities of that
i w r i w r
instructionappearingininputsequences.
28Mamba SSM Layer 1 vs Layer 2 Mamba SSM Layer 1 vs Layer 2
(Dyck-(8, 10), Validation) (Dyck-(8, 10), Test)
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
Model Model
0.2 2 Layer Mamba 0.2 2 Layer Mamba
1 Layer Mamba 1 Layer Mamba
0.0 0.0
25 50 75 100 25 50 75 100
Memory Dimensions Memory Dimensions
Figure5: MambaAccuracyonDyck ,onthedevelopmentset(length≤700,samelengthrange
8,10
astrainingset)andtestset(length700≤n≤1400). ThelatterisalsoplottedinFigure4.
Wetrainedaone-layerMambawiththedefaultparameters8,settingd to16withtheAdamW
model
optimizerusingalearningrateof3x10−4andabatchsizeof16.
FollowingtheevaluationcriteriaforLSTMsinLiuetal.[45],wecomputethetestevery100training
stepsonourvalidationsetsofchoice,byrandomlysamplingaround103 samplesfromeachsetin
everyevaluationcycle.
D.3 BoundedHierarchicalStructure
We built on the official code and data release of Yao et al. [74] at https://github.com/
princeton-nlp/dyck-transformer (last commit: 5d21fcf). We train a 2-layer Mamba and a
1-layerMambaonDyck withK=8andh=10. Thetrainingsetandthevalidationsetcontains
K,h
samplesoflengths≤700,whilethetestsetcontainssamplesoflengths700≤n≤1400. Wetrain
Mambawithavaryingnumberoflayersl∈{1,2}andd ∈{20,30,40,50,60,70,80,90,100}.
model
WeusetheAdamoptimizerwithaninitiallearningrateof0.01or0.001,usingcross-entropyloss.
After training for 100 epochs (with early stopping allowed in case of convergence), we select the
learningratewiththebettertrainingperformance.
E FinitePrecisionAssumption
AsdescribedinSection2,weadoptthefiniteprecisionnotionusedbyWeissetal.[70]:Weallowan
unboundednumberofintegerbits,butonly pfractionalbits,where pisasufficientlylargeconstant
(e.g., p=8),independentofthelengthoftheinput.
Thereareavarietyofrelatedprecisionnotionsinthetheoreticalliteratureonneuralsequencemodels
–here,wediscusstheeffectofothernotionsonourresults:
1. InfiniteprecisionInfiniteprecisionallowsanyparameterandintermediatevaluetobean
arbitrarynumber. Suchasettingisunrealistic,asitwouldallowencodingarbitrarydetail
abouttheinputintoinfiniteprecision[e.g.63]andreadtheseoutwithsufficientlypowerful
functions(A, B, φ)in(5)–thiswouldleadtotheunrealisticconclusionthatanyfunction
and language could be represented. For this reason, theoretical work has often adopted
restrictedprecisionnotions.
2. Finite inventory of values, where integer and fractional bits are both restricted. Such
a setup may be justified based on the fact that any real computer has bounded memory,
though such a setup precludes any positive results on non-finite-state problems for any
computationalarchitecture.9
8Fromhttps://github.com/state-spaces/mamba/blob/main/README.md
9Forinstance,aTuringmachinewithboundedmemoryandthusaboundedtapeisequivalenttoafinite-state
automaton.
29
ycaruccA
esolC
ycaruccA
esolCSuch a restrictive setup would not affect our positive results on Flip-Flop, Star-Free, and
bounded-depthDycklanguages(Theorems1,4,6),asthesealluseboundedfinite-precision
activationvalues. Asthisisamorerestrictedsetupthantheoneweareassuming,thisalso
would not affect our negative results about PARITY and non-star-free languages (Theo-
rems2,4). Theseresultsarethushighlyrobusttovariationsofthefiniteprecisionassump-
tion.
Such a more restrictive definition would, however, mean that, for unbounded counting
(Theorem 5), modeling is only possible up to a bound determined by the number of pos-
sible values—this is the one place where our results would be impacted. Indeed, we do
observethatMambalearnsthesecounterlanguagesontraininglengthsbutstruggleswith
lengthgeneralization. Transformers,ontheotherhand,canrepresenttheselanguageswith
bounded activations (due to the constructions in [6]), and show strong length generaliza-
tion.
An intermediary between infinite and finite precision is notions of precision where the
numberofallowedbitsslowlyincreaseswiththeinputlength,e.g.,logarithmically. Such
asetup hasparticularly beenadopted fortransformers [48], because afinite-precision as-
sumptionleadstoverylowexpressivityintransformers. ForSSMs,ontheotherhand,we
find that finite precision assumptions are sufficient for showing a broad range of positive
results.
30