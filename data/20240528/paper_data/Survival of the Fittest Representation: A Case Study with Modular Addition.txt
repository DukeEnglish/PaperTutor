Survival of the Fittest Representation: A Case Study
with Modular Addition
XiaomanDeloresDing∗, ZifanCarlGuo∗, EricJ.Michaud, ZimingLiu†, MaxTegmark†
MassachusettsInstituteofTechnology
{delores,carlguo,ericjm,zmliu,tegmark}@mit.edu
Abstract
Whenaneuralnetworkcanlearnmultipledistinctalgorithmstosolveatask,how
doesit“choose”betweenthemduringtraining? Toapproachthisquestion,wetake
inspirationfromecology: whenmultiplespeciescoexist,theyeventuallyreachan
equilibriumwheresomesurvivewhileothersdieout. Analogously,wesuggest
that a neural network at initialization contains many solutions (representations
and algorithms), which compete with each other under pressure from resource
constraints, withthe“fittest”ultimatelyprevailing. ToinvestigatethisSurvival
oftheFittesthypothesis,weconductacasestudyonneuralnetworksperforming
modularaddition,andfindthatthesenetworks’multiplecircularrepresentations
at different Fourier frequencies undergo such competitive dynamics, with only
afewcirclessurvivingattheend. Wefindthatthefrequencieswithhighinitial
signalsandgradients,the“fittest,”aremorelikelytosurvive. Byincreasingthe
embeddingdimension,wealsoobservemoresurvivingfrequencies. Inspiredby
theLotka-Volterraequationsdescribingthedynamicsbetweenspecies,wefind
that the dynamics of the circles can be nicely characterized by a set of linear
differentialequations. Ourresultswithmodularadditionshowthatitispossibleto
decomposecomplicatedrepresentationsintosimplercomponents,alongwiththeir
basicinteractions,toofferinsightonthetrainingdynamicsofrepresentations.1
1 Introduction
The field of mechanistic interpretability attempts to reverse-engineer the algorithms that neural
networkslearn. Thisinvolvesunderstandingtherepresentations(features)networkslearn[1,2,3,
4,5,6]andhowtheseplayaroleinlargercircuits[7,8,9,10,11]. Whilemostsuchworkstudies
networks as static objects, some have recently begun to study how network representations and
circuitsformovertraining[1,7,8,12,13,14]. Instudyingtrainingdynamics,wehopetounderstand
notjustwhatneuralnetworkslearn,buthowandultimatelywhytheylearnthealgorithmsthatthey
learn. Thisunderstandingmayeventuallybeusefulfortrainingmodelswiththepropertieswedesire,
suchasimprovedefficiencyandsafety.
Onebroadquestioninmechanisticinterpretabilityregardsuniversality[9]: canmodelsconsistently
learnthesamealgorithmsacrossdifferentseedsandscales? Whilesomeworkhasfoundevidence
of universality [7, 15, 16], in other cases there seems to be some variability in algorithms and
representationsnetworkslearntosolveparticulartasks[17,18,19]. Inthiswork, weask: when
networkshaveachoicebetweendifferentrepresentations,howdotheychoosewhichonetolearn?
Thequestionaboveishardtoanswersincerepresentationsinthegeneralcasearehigh-dimensional
anddifficulttodisentangle,letalonetounderstandthedynamicsbetweenmultiplerepresentations.
*Equalcontribution.Orderdeterminedbyacoinflip.
†Equaladvising.
1Ourcodeisavailableathttps://github.com/carlguo866/circle-survival.
Preprint.Underreview.
4202
yaM
72
]GL.sc[
1v02471.5042:viXraFigure 1: Our experiment method: perform a Fourier transform to the embedding, analyze the
initialization,signalevolution,andtheireffectsonthefinallearnedcirclesoverdifferenttraining
runs.
Therefore,ourstudyfocusesonthetoymodelsperformingmodularadditiona+b=c(modp),a
mathematicallydefinedandwellstudiedproblem. Priorworkshaveshownthatcircularrepresen-
tationsintheembedding,akintohownumbersarearrangedaroundaclock,arekeyformodular
additionmodelstogeneralize[1,20,8,17,21], andthatthemodellearnsafewsuchcirclesthat
nicelycorrespondtodifferentFourierfrequencies[8]. Sincetheembeddingisrandomlyinitialized,
theprojectionoftheembeddingontotheFourierbasisresultsinroughlysimilarfrequencies,meaning
thatallpossiblefrequencieshaveachancetobecomethefinalcirclerepresentation. However,only
3-5circlessurviveaftertraining,whilesomefrequenciesdonotformcircles,asillustratedinFigure
1. Thisbegsthequestion: isthereanypatterntohowtheserepresentationsform?
Tounderstandthepattern,weproposetheSurvivaloftheFittesthypothesisinanalogytoecosystems,
wherecirclesofdifferentfrequenciescanbethoughtofas"species"competingforafixedamount
oftotalresources. Thetwosystemsareanalogousinmanyaspects: (1)Wefindsurvivingcircles
tohavelargemagnitudesand/orlargeexpandingvelocity(gradient)atinitialization,supportingthe
“survivalofthefittest”hypothesis. (2)Thenumberofsurvivingfrequenciesincreasesastheresources
(embeddingdimension)increases. (3)Alineardifferentialequation,inspiredbytheLotka–Volterra
equations[22]describinginteractingdynamicsbetweenspecies,isabletofittheevolutionofthe
magnitudeofthesecirclesquitewell,evenwhenwepushtheinteractionmatricestobeextremely
sparseviaLassoregression. Somecirclepairsdisplaycollaborativebehavior,whileotherpairsare
competitive.
Ourresultsshowthatitispossibletodecomposecomplicated,high-dimensionalembeddingsinto
low-dimensional,interpretablerepresentationsthatalsohavesimpleinteractions. Ourworkservesas
aproof-of-conceptexample,demonstratingthatthisdecomposition-styleanalysiscanhelpunderstand
modeltrainingdynamicsinmorereal-worldcontexts. Thepaperisorganizedasfollows:InSection2,
weintroducetheproblemsetup,observingthatmostcirclesdieandonlyafewsurvive. InSection3,
weinvestigatehowmanycirclessurviveandwhichcirclessurviveunderdifferentcircumstances. In
Section4,weshowthatthedynamicsofcirclescanbewellmodeledbyalineardifferentialequation.
RelatedworksarediscussedinSection5. WeconcludeinSection6.
2 ProblemSetup
Modular addition We study models performing the task of modular addition in the form of
a+b = c(modp),wherea,b,c = 0,1,...,p−1. OurmodelshaveanembeddingmatrixW
E
ofsize(p,d),whereeveryintegert∈{0,1,··· ,p−1}istreatedasatokenandhasanassociated
embeddingvectorE ∈ Rd. Themodeltokenizesthetwoinputsaandb,concatenatesthem,and
t
feeds[E ,E ]∈R2dtoatwo-layerMLPwithtwohiddenlayersof100neuronseach,producinga
a b
2Figure2:(Left)Thesignals,themagnitudeoftheFouriercoefficients,ofeachembeddingfrequencies
overtimeshownonalogarithmicscale. Thesurvivingfrequenciesclearlyseparatethemselvesfrom
therestofthefrequenciesthatquicklygoto0. (Right)Snapshotsoftheembeddingprojectedontoa
deadfrequency(top)andasurvivedfrequency(bottom)atdifferenttimestepsduringtraining.
categoricaloutputc. Wedefaulttod=128andalargeweightdecayof0.5tomakesurethemodel
quickly"groks"[21]toformthefinalrepresentation.
Circlesandsignals Priorworkhasshownthatcircularrepresentations(circles)areimportantfor
neuralnetworkstoperformmodularaddition[1,8,17]. However,asshowninFigure3,neighboring
numbersalongthecirclemayhaveincrementsotherthan1becausetherearepequivalentgroup
representationsthatcorrespondtocircleswithdifferentFourierfrequenciesk =1,2,··· ,(p−1)/2.2
Thecircleoffrequencykplacesatokentat(cos(2πkt/p),sin(2πkt/p)).
SimilartoNandaetal.[8],wedecomposerepresentationsintoalinearcombinationofcirclesof
differentfrequencies. DenotingE ∈ Rd tobetheembeddingvectorfortokenn, wedefinethe
n
Fouriercoefficientsoffrequencyktobe
p−1
F
k
= (cid:88) e−i2πk pnE n. (1)
n=0
ThecircleoffrequencykislocatedontheplanespannedbytwovectorsReal(E ),Imag(E )∈Rd.
k k
WedefinetheFouriersignaloffrequencyk as∥F ∥2 = (cid:80)d−1(Fj)2 andobservehowthesignal
k j=0 k
ofeachfrequencyevolvesovertrainingsteps,showninFigure2(Left). Weobservethatonlyafew
circleshavesignificantsignals(hencesurvived)intheend,whilethesignalsofallothercirclesdecay
toalmostzero(hencedead). ShowninFigure2(Right),projectionsofasurvivingfrequencystabilize
intoaclearcircle(bottom),whileadeadfrequencycollapsestowardsitscenter(top). Whileprior
workshavefoundcircleseitherwithFouriertransformation[8]orwithprincipalcomponentanalysis
(PCA)[1],weusetheFouriertransformationsinceitcanrevealcircularrepresentationsbetter(see
Figure 3). Because each frequency corresponds to a circle, we use the phrase “circle”, “circular
representations”or“frequencies”interchangeablythroughoutthewholepaper.
Researchquestion: whichcirclessurvive? InFigure2(Left),weobservethatthesurvivingcircles
intheendtendtohavehigherinitialsignal. Canweusethisinformationtopredictsurvivedcircles?
Unfortunately,wefindthatinitiallylargefrequenciesdonotalwaysleadtosurvivingcircles,butlarge
initialsignalsdoleadtohigherprobabilityofsurviving. Therefore,weresorttostatisticalanalysis
ratherthandeterministicanalysisbyaggregatingtrainingresultswithafixedembeddingandrandom
initializationsofMLPsanddatasetsandreportthemeanand95%confidenceinterval.
2Notethatfrequencykandp−krefertothesamecircle,whichaccountsforthefactor2.
3Figure3: Top: Themodelembeddingprojectedontothefirst10principalcomponentsinpairs,the
onlycomponentswithsignificantsingularvalues. Bottom: Themodelembeddingprojectedontothe
Fourierbasisoffrequenciesindescendingorderofsignalmagnitude. The∆betweenadjacenttokens
showsacorrespondencebetweenPCAandFFT.3ThisindicatesthatPCAisalooseapproximationof
circlesassociatedwithFourierfrequencies.
3 SurvivaloftheFittest
Inanecologicalsystemwithconstrainedresources,onlythefittestwillsurvive. Wehypothesizethat
thistheoryalsoholdstrueinthecaseofmodularaddition. Motivatedbytheecologicalanalogy,we
wishtoanalyzethecompetitivedynamicsofhowthemodelselectscertaincirclesasitsrepresentation
byansweringQ1: howmanycirclessurvive? Q2: whatarethepropertiesofthecirclesthatsurvive?
3.1 Q1: Howmanycirclessurvive? AResourcePerspective.
Inecologicalsystems,withmoreresourcesavailable,itisintuitivetothinkthatmorespeciescould
have a chance to survive. Similarly, in neural networks, the embedding dimension can be made
analogoustothetotalresourcesavailable,asalargermodelhasstrongerapproximationpowerand
canleadtobetterperformanceonthedesiredtask[23,24,25]. Weexpectthathigher-dimensional
embeddings,evenjustrandomlyinitialized,providemore“resources”formodeltraining.Specifically,
inthesettingofmodularaddition,whend≥p,withprobabilityone,perfectcirclescanbeobtained
bylinearlyprojectingd-dimensionalrandomrepresentationsintosuitablesubspaces.4
Freezingembeddings Weconfirmwithexperimentsthattheinitialrandomembeddingsalready
encapsulaterichrepresentationsforMLPstolearn. Toverify, wefreezetheembeddingsattheir
initialization and train only the MLP. We vary the dimension d of the embedding and observe
evolution of test loss. In Figure 4, we show snapshots of the test loss at different timesteps as a
functionofembeddingdimension,whereweindeednoticeaphasetransitionaroundd=p—thetest
lossissharplybetterwhend>pthanwhend<p. Interestingly,weobservethatat10,000steps,
modelswiththesmallds(d≤20)havelowerlossthanmodelswith“medium"ds(20≤d≤59),
an intriguing observation similar to the phenomenon of double descent [26]. Our speculation is:
networkswithsmalldscanbenefitfromfeaturelearning,whilenetworkswithlargeds(d≥p=59)
havemoresheerapproximationresources,asweargueabove,despitebeinginalazylearningregime
[27]. Networksofmediumdsfailperhapsbecausetheyarebothlazyandresourceconstrained. A
fullinvestigationofthisphenomenonisleftforfuturework.
3∆canbecalculatedastheinverseoffrequencykmodulopsothatk·∆=1( mod p).
4Findingasubspacewhereaperfectcircleoffrequencyklivesonisequivalenttofindtwolinearprojections
p andp suchthatE ·p =sin(2πak/p)andE ·p =cos(2πak/p)foralla=0,1,··· ,p−1.Since
1 2 a 1 a 2
theseequationsarelinearandlinearlyindependent(duetorandominitializations),whenthenumberofunknown
variables2dislarger(smaller)thanthenumberofequations2p,thesystemisunderdetermined(overdetermined),
leadingtotheexistence(nonexistence)ofsolutions.
4Figure 4: Freezing the initial embedding and training only the MLP, test loss (zoomed in on the
bottomto<1.0)inrelationtoembeddingdimensiond. Onecannoticeadipinlossatd=p.
Trainableembeddings Now,wegobacktothestandardsetupwhereembeddingsaretrainable.
Sincedimensionalityisanalogoustoresourcesinecologicalsystems,wewanttounderstandifmore
circlescansurviveinembeddingsoflargerdimensions. Theanswerisyes. Asbothpandddetermine
theembeddingdimension,weconductexperimentsvaryingonewhilefixingtheother. InFigure
5(Left),wefixdandstudytheeffectofvaryingpontheexpectednumberofsurvivingfrequencies
aftersamplingover100trials,fromwhichweidentifyclearpositivecorrelationbetweenpandthe
number of circles the model learns. Similarly, we fix p and vary d to see d’s effect on the total
numberofsurvivingfrequenciesinFigure5(Right),whichsimilarlyshowsanupwardcurve. As
theembeddingdimensionincreases,thenumberofcircles(algorithmredundancy)increases. This
redundantmechanismpotentiallymakesneuralnetworksmorerobust,butlessparameter-efficient
andinterpretable. Understandingthismechanismwouldbeanintriguingtopicforfuturework.
3.2 Q2: WhichCircles? SomeAre“Fitter"atInitialization.
Wewanttopredictthefinalsurvivingcirclesfromtheinitialization. FollowingtheSurvivalofthe
Fittesthypothesis,wewishtodefineafew“fitness”measuresthatmakecertainfrequenciesmore
likelytosurvivethanothers. Weindeedobservesomepropertiesthatmakesomefrequenciesmore
Figure5: (Left): Thenumberofcirclesthemodelchoosesforitsfinalrepresentationinrelationto
thenumberoftokens,p,over100randomtrials. (Right): Thenumberofcirclesastheembedding
dimensionforrepresentingeachtokenincreasesfrom16to128.
5“fit,” including large initial signals and large initial gradients. Intuitively, one can think of these
“fitness”measuresasbeingborneitherstrongorfastintheecosystemanalogy.
Figure6: (Left)SurvivalratesoffrequenciesgiventheirinitialFouriersignalsovermanyrandomized
trials. (Middle)Survivalrateofanarbitraryperturbedfrequencyversusdeviationfromthemean
initial signal, with different lines showing the same frequency in different embeddings. (Right)
Survivalratesofthelargestfrequencyandthe2ndlargestastheydifferbyr.
3.2.1 InitialSignal—1stFitnessMeasure
Wedefinesurvivalrateasthenumberoftimesaparticularfrequencysurvivesandbecomespartof
thefinalrepresentationoverthetotalnumberofthetrialswherewefixtheembeddingandrandomize
overdifferentMLPsanddatasetinitializations. InFigure6(Left),weshowthatover10different
embeddinginitializationsand50trialseach,establishingalinearcorrelation: thehighertheinitial
signal,themorelikelythemodelistochoosethatfrequencyasitsfinalrepresentation. Toconfirm,
wefindthePearsoncorrelationbetweensurvivalrateandinitialsignalis0.85withap-valuesmaller
than10−3.
Tocorroborate, weconductaperturbationexperimentontheinitialembedding. Specifically, for
agivenembeddingatinitialization,weperformFouriertransformandfindtheinitialcoefficients
of an arbitrary frequency. We manually enlarge or shrink its magnitude and perform an inverse
FFTtorestoretheembedding,fromwhichweperformmodeltraining. WedemonstrateinFigure
6(Middle)thesurvivalrateoftheperturbedfrequencyindifferentinitializationsasitdeviatesfrom
themeanoftherestofthesignals. Weobservethatifthefrequencyismuchhigherthantherest,
survivalrateisnear100%,whilethefrequencyrarelysurvivesifitismuchlowerthanthemean. This
experimentindeedsuggeststhataswecontroltheenvironmentmuchmoreclosely,theinitialsignal
ofthefrequencyplaysauniqueroleindeterminingthefinalrepresentation.
Tofurthersubstantiateourfindings,wemanuallyconstructanembeddingtoevaluatesurvivalrate.
Wefirstrandomlysample2outofpfrequenciesandsettheirsignalstobeofavaryingratior ∈[0,1].
Concretely,thelargestfrequencywillhavesignalmagnitudes,whilethesecondhighestwillhave
asignal r·s. Weset allotherfrequencies tohaveasignalof smallϵ = 10−6. Inthissetup, we
showinFigure6(Right)thatthefrequencywiththehighestsignalwillalwayssurvive,whilethe
secondhighestfrequencyincreasesinsurvivalrateasitssignalincreasesanddifferentiatesitself
furtherfromtherestofthesignals. Interestingly,despiteallotherfrequencieshavingasignalnear0
atinitialization,themodelsometimeschoosestorevivethemratherthanalwayschoosingthetwo
clearfrontrunners,aphenomenonthatwarrantsmoreinvestigationinthefuture.
3.2.2 InitialGradient—2ndFitnessMeasure
Inevolutiontheory,speciesbestadaptedtotheirenvironmentwouldsurvive. Inneuralnetworks,
wehypothesizethatnotonlytherepresentationswithhighinitialsignals,butalsothosecanquickly
adapts into circles are more likely to survive. This observation motivates us to examine initial
expandingvelocity(gradient). Wesimplycalculatethegradientasthedifferenceinthesignalsbefore
andafteragiventimestepi,takingintoaccountboththeembeddinggradientsandweightdecay.
InFigure7(Left),weshowthefrequencies’initialgradientvaluesalongsidetheirsurvivalstatus.
Duetotheweightdecaymechanism,allgradientsignalsdecreaseovertime,butthosewithhigher
initialgradientstendtoshrinklessandaremorelikelytosurvive. InFigure7(Middle),weshow
6thatastheinitialgradientincreases,thesurvivalrateofthosefrequenciesincreases. Toanalyzethe
possiblycompoundingeffect,weshowthatfrequencieswithbothhighinitialsignalandgradientare
morelikelytosurviveinFigure7(Right),asthetoprightcornerismorelitwithoranges,indicating
moresurvivedfrequencies. Tofurtherverifytheaboveobservation,wetrainalinearsupportvector
machine(SVM)thatseparatesthedeadandsurvivedfrequencies,achievingan83.8%accuracy.
Figure7: Distributionofthefrequencygradientatstep4. (Middle)Survivalrateinrelationtofre-
quencygradientsforstep1to4. (Right)Survived(orange)anddead(blue)frequenciescharacterized
bytheirinitialsignalsandgradientsatstep4.
RelationtoLotteryTicketHypothesis OuranalysiscanberelatedtotheLotteryTicketsHypothe-
sis(LTH)[28],wheresomesubnetworksare“winningtickets“thatachievecomparabletestaccuracy
to the original network when trained in isolation. Our results suggest that with large embedding
dimensions,goodcirclesexistevenatinitialization,similarto“winningtickets.” Ouranalysisis
technically different from LTH in two ways: (1) LTH requires training and pruning to identify
“winning”tickets,whilecirclesaremathematicallydefinedwithouttraining(butspecifictomodular
addition);(2)LTHonlystatestheexistenceof“winning”ticketsatinitialization,whilewemanage
tocharacterizethepropertiesof“winning”circles,that,whentrainedinisolation,havecomparable
accuracy(seeAppendixC).Ourworkprovidesrepresentation-levelinsightstothestudiesofLTH.
4 CirclesCanCollaborateorCompete
In the last section, we demonstrated how Survival of the Fittest explains the evolution of circles.
However,whatislackingfromthisexplanationistheinteractionbetweencircles,considering“fitness”
isdefinedonindividualcircles. Thissectionseekstounderstandcircleinteractionfromtwoaspects:
(1) understand how a group of n circles “collaborate” to reduce losses on the task at hand; (2)
understandthe(effective)differentialequationsthatgoverntheevolutionofcirclesignals.
4.1 CirclesHavetoCollaboratetoReduceLoss
Weobservedthatmodelsnaturallyformmultiplecirclesintraining. Whyisthisthecase? Can’t
a single circle perform modular addition effectively? Here, we show that a strong cooperative
pattern exists between the circles of different frequencies. We investigate this relationship by
conductingablationstudies,bymanuallyisolating1,2,and3differentfrequencies(removingall
otherfrequencies)andseeiftheseisolatedfrequenciescansolvethetaskofmodularaddition.Wefind
thatthemodelisnotabletoperformthemodularadditionwithonlyonecircle,stillhasconsiderably
highlossusingtwocircles,andreachesnearzerolosswith3circles. Thelossachievedovertimein
30,000trainingsteps,with1,2and3circlesrespectively,isshowninFigure8.
4.2 ModelingCircleDynamicswithDifferentialEquations
In ecology, the Lotka-Volterra equations are famous for using first-order nonlinear differential
equationstomodeltherelationshipbetweenpreyxandpredatorsy[22]. Theyhavethefollowing
formdx/dt = αx−βxy,dy/dt = δxy−γy,wherexandy representthepopulationdensityof
preyandpredators,respectively,anddx/dtanddy/dtaretheinstantaneousgrowthratesofthetwo
populations. Thiscanbeeasilygeneralizedtomorethantwospeciesbyinvolvinglinearsingle-body
termsandquadratictwo-bodyterms. Interpretingspeciespopulationasfrequencysignals,wehave:
7dx
i
=(cid:88)Nc
α x
+(cid:88)Nc (cid:88)Nc
β x x , i=1,2,···N , (2)
dt i i i,j i j c
i=1 i=1 j=i
wherex representssignalsofeachfrequency.
i
AlthoughthefityieldsanR2valuecloseto1,themodelsuffersfromoverfittingduetotoomanyfree
parametersofx ·x terms. Whenweattempttocomputeatrajectoryusingourestimates,errors
i j
accumulate,leadingtoarapidlossofnumericalstability. Thispromptsanaturalquestion: arethe
quadraticnonlineartermsreallynecessary?
Tooursatisfaction,theanswerisno. DeviatingfromtheLokta-Volterraequations,wefoundaneven
simpler, lineardifferentialequation, thatcanestimatethetrainingtrajectorywell. Removingthe
second-ordertermsfromEquation2,weget
dx
i
=(cid:88)Nc
α x +b , i=1,2,··· ,N , (3)
dt i,j j i c
j=1
orinmatrixform dx =Ax+b. Withthisnewsetofequations,wecanmodelthetrajectorywell.
dt
WereporttheR2ofourfitforbothlinearregressionandLassoinFigure9overmanydifferenttrials
withembeddingsofvaryingsize. Fortwofrequenciesinagiventrainedmodel,onesurvivedandone
dead,wecomparetheoriginaltrajectory,theestimatedtrajectoryfromLinearRegressionandfrom
LassoinFigure9.Whilelinearregressiongivesusanalmostperfectfitattheriskofoverfitting,Lasso
providescomparableestimationswithreasonableerrorsboundsandextremelysparsecoefficients.
Asthedifferentialequationsarelinear,wecanfindananalyticalsolutiontotheODEsystem. Assume
Aasthecoefficientmatrixoftheregressionmodelandbastheintercept,foragivenx ,wehavethe
0
followingsolution
x(t)=eAtx +(eAt−I)A−1b. (4)
0
NotethatforthecoefficientmatrixoftheLassofit,thematrixisextremelysparseandisnotnaturally
invertable,sowehaveaddedasmallϵI(ϵ=10−8)toA. Theanalyticalsolutionsimilarlyprovides
uswithacomparableestimationofthetrajectoryinFigure9.
RelationtoNeuralTangentKernel DespitethefactthattheLotka-Volterraequationsworkwellto
modelecologicaldynamics,itisnottoosurprisingthatalinearODEissufficienttomodelthetraining
dynamicsinneuralnetworks,whentheneuralnetworksarewideenoughtobecharacterizedbythe
neuraltangentkernel[29]. However,ouranalysisisstillnovelinthesensethatwecandisentangle
theentireembeddingspaceintoindividualcirclesandstudytheirinteractionslinearly. Theideaof
decompositionallowstheanalysisofacomplicatedsystemtobebrokendowntoanalysisofmany
simplesubsystemsandtheirinteractions.
Figure8: Testlossovertrainingstepsinablationstudy, withonlyone(left), two(middle), three
(right)circlesremainingandothercirclesremoved. Threecirclesareneededtoreducethelosstozero.
8(a)R2 forfitonembed- (b)EstimatedTrajectory (c)EstimatedTrajectory (d) Coefficient Matrix
dingsofvaryingd forDeadFrequency forSurvivingFrequency forLassoregression
Figure9: UsinglinearODEtomodelcircleinteractionsduringtraining.
5 RelatedWork
Mechanistic Interpretability on Algorithmic Tasks A lot of work has been done to reverse
engineerhowneuralnetworksimplementalgorithmictasks[8,17,30,31,32,33,34]becausethey
are mathematically well-defined and simple. However, even on these toy tasks, neural networks
alreadydisplaysomeintriguingphenomena,includingphasechangesduringtraining[8],different
algorithms[17,30],orshowtheexistenceofmultiplecopiesofalgorithms[8,17].
Training Dynamics Training dynamics strives to understand what happens internally within a
modelduringtraining.Twomoststudiedphenomenainthisareaare“grokking”[21,1,20,35,36]and
“doubledescent”[26,37,38,39].Otherworkshavestudiedtrainingdynamicsatvariousabstraction
levels, suchasonemergingcapabilitieslevel[12,18], onthecircuitlevel[7,14,13], andonthe
neuronlevel[40]. SimilartoouranalysisofcircleinteractionsusingODEs,previousworkattempted
tomodelrepresentationdynamicsduringtrainingusingsimpleeffectivedynamics[1,41,42,43].
RepresentationLearning Representationlearningiskeyfornetworkstogeneralize[44,45,46,
2]. Manylearningparadigmsaimtoencouragebetterrepresentations,includingweaksupervised
learning [47], contrastive learning [48, 46], and Siamese learning [49, 50]. Similar to our paper
revealingcirclesondifferentfrequencies,priorworkshaveshownredundantrepresentationsand
algorithmsinmodels[51,25]andsimilarlycircularrepresentationsingenerallanguagemodels[52].
LotteryTicketHypothesis TheLotteryTicketHypothesis[28]positsthatsomesubnetworks–
“winningtickets”—identifiedatinitializationandtrainedinisolationcanmatchthetestaccuracyof
theoriginal,densenetwork. Ithasinspiredextensions,suchasastrongerconjectureonfindingsuch
subnetworkswithouttraining[53,54,55,56,57,58,59],transferringwinningticketsacrosssetups
[60,61],andimprovingmethodsofpruningtofindthesubnetworks[62,63,64,65,66]. Ourwork
relatestoLTHthatcirclescanbetreatedassubnetworkswithdistinctsignalsatinitialization. In
oursetting,“winningtickets”existatinitializationandhavenicepropertieslikehighinitialsignals,
whichallowthemtoeventuallybecomethelearnedrepresentations.
6 Conclusion
Inthispaper,weshowthattheSurvivaloftheFittesttheorycanexplainthetrainingdynamicsofthe
toymodularadditiontask. Qualitatively,embeddingscanbedecomposedintocirclesofdifferent
frequencies,deemedasspeciesinteractingwithoneanother. Undertheresourceconstraintofmodel
sizes,circleswithlargesignalsandgradientsaremorelikelytosurvive. Quantitatively,thedynamics
ofcircleinteractioncanbedescribedbyasimplelineardifferentialequation. Ourresultshighlight
simplelawsunderlyingseeminglycomplicatedrepresentationdynamicsandopenthedoorformore
fine-grainedanalysisofrepresentationdynamicsformechanisticinterpretability.
Limitations Wehavefocusedonasinglelearningproblem: modularaddition. Ourworkstudying
dynamicsbetweendifferentrepresentationsismadepossiblebecauserepresentationsinmodular
additionmodelsarewell-definedandwell-understood. Significantadditionalworkisneededtoscale
theseanalysestoevenmorecomplex,generalmodels,whichremainachallenge.
9BroaderImpact Understandingtrainingdynamicsallowsustobetterunderstandandcontrolneural
networksinwayswedesire,suchasmakingthemmoreaccurateandsafe. Anydual-usetechnologies
haveaccompanyingrisks,sooneshouldexercisecautionwhendeployingthesetechniques.
Acknowledgements
WethankWesGurnee,JulianYocum,andZiqianZhongforhelpfulconversationsandsuggestions.
This work is supported by the Rothberg Family Fund for Cognitive Science, the NSF Graduate
ResearchFellowship(GrantNo. 2141064),andIAIFIthroughNSFgrantPHY-2019786.
References
[1] ZimingLiu,OuailKitouni,NiklasNolte,EricJMichaud,MaxTegmark,andMikeWilliams.
Towardsunderstandinggrokking:Aneffectivetheoryofrepresentationlearning. InAliceH.Oh,
AlekhAgarwal,DanielleBelgrave,andKyunghyunCho,editors,AdvancesinNeuralInforma-
tionProcessingSystems,2022. URLhttps://openreview.net/forum?id=6at6rB3IZm.
[2] AndyZou,LongPhan,SarahChen,JamesCampbell,PhillipGuo,RichardRen,Alexander
Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel
Li, MichaelJ.Byun, ZifanWang, AlexMallen, StevenBasart, SanmiKoyejo, DawnSong,
MattFredrikson,J.ZicoKolter,andDanHendrycks. Representationengineering: Atop-down
approachtoaitransparency,2023.
[3] HoagyCunningham,AidanEwart,LoganRiggs,RobertHuben,andLeeSharkey.Sparseautoen-
codersfindhighlyinterpretablefeaturesinlanguagemodels. arXivpreprintarXiv:2309.08600,
2023.
[4] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Con-
erly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu,
ShaunaKravec,NicholasSchiefer,TimMaxwell,NicholasJoseph,ZacHatfield-Dodds,Alex
Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter,
Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language
models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-
circuits.pub/2023/monosemantic-features/index.html.
[5] WesGurneeandMaxTegmark. Languagemodelsrepresentspaceandtime. InTheTwelfth
InternationalConferenceonLearningRepresentations,2024. URLhttps://openreview.
net/forum?id=jE8xbmvFin.
[6] SamuelMarksandMaxTegmark. Thegeometryoftruth: Emergentlinearstructureinlarge
languagemodelrepresentationsoftrue/falsedatasets. arXivpreprintarXiv:2310.06824,2023.
[7] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan,BenMann,AmandaAskell,YuntaoBai,AnnaChen,TomConerly,DawnDrain,
DeepGanguli,ZacHatfield-Dodds,DannyHernandez,ScottJohnston,AndyJones,Jackson
Kernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,JackClark,JaredKaplan,
Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer
CircuitsThread,2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-
heads/index.html.
[8] NeelNanda,LawrenceChan,TomLieberum,JessSmith,andJacobSteinhardt. Progressmea-
suresforgrokkingviamechanisticinterpretability. InTheEleventhInternationalConferenceon
LearningRepresentations,2023. URLhttps://openreview.net/forum?id=9XFSbDPmdW.
[9] ChrisOlah,NickCammarata,LudwigSchubert,GabrielGoh,MichaelPetrov,andShanCarter.
Zoomin: Anintroductiontocircuits. Distill,5(3):e00024–001,2020.
[10] NelsonElhage,NeelNanda,CatherineOlsson,TomHenighan,NicholasJoseph,BenMann,
AmandaAskell,YuntaoBai,AnnaChen,TomConerly,NovaDasSarma,DawnDrain,Deep
Ganguli,ZacHatfield-Dodds,DannyHernandez,AndyJones,JacksonKernion,LianeLovitt,
KamalNdousse,DarioAmodei,TomBrown,JackClark,JaredKaplan,SamMcCandlish,and
10ChrisOlah. Amathematicalframeworkfortransformercircuits. TransformerCircuitsThread,
2021. https://transformer-circuits.pub/2021/framework/index.html.
[11] SamuelMarks,CanRager,EricJMichaud,YonatanBelinkov,DavidBau,andAaronMueller.
Sparsefeaturecircuits: Discoveringandeditinginterpretablecausalgraphsinlanguagemodels.
arXivpreprintarXiv:2403.19647,2024.
[12] JesseHoogland,GeorgeWang,MatthewFarrugia-Roberts,LiamCarroll,SusanWei,andDaniel
Murfet. Thedevelopmentallandscapeofin-contextlearning. arXivpreprintarXiv:2402.02364,
2024.
[13] AngelicaChen,RavidSchwartz-Ziv,KyunghyunCho,MatthewLLeavitt,andNaomiSaphra.
Suddendropsintheloss: Syntaxacquisition,phasetransitions,andsimplicitybiasinmlms.
arXivpreprintarXiv:2309.07311,2023.
[14] AadityaK.Singh,TedMoskovitz,FelixHill,StephanieC.Y.Chan,andAndrewM.Saxe. What
needstogorightforaninductionhead? amechanisticstudyofin-contextlearningcircuitsand
theirformation,2024.
[15] Rhys Gould, Euan Ong, George Ogden, and Arthur Conmy. Successor heads: Recurring,
interpretableattentionheadsinthewild. InTheTwelfthInternationalConferenceonLearning
Representations,2024. URLhttps://openreview.net/forum?id=kvcbV8KQsi.
[16] WesGurnee,TheoHorsley,ZifanCarlGuo,TaraRezaeiKheirkhah,QinyiSun,WillHathaway,
NeelNanda,andDimitrisBertsimas.Universalneuronsingpt2languagemodels.arXivpreprint
arXiv:2401.12181,2024.
[17] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza:
Twostoriesinmechanisticexplanationofneuralnetworks. InThirty-seventhConferenceon
NeuralInformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=
S5wmbQc1We.
[18] RThomasMcCoy,JunghyunMin,andTalLinzen. Bertsofafeatherdonotgeneralizetogether:
Large variability in generalization across models with similar test set performance. arXiv
preprintarXiv:1911.02969,2019.
[19] Andrew Kyle Lampinen, Stephanie CY Chan, and Katherine Hermann. Learned feature
representationsarebiasedbycomplexity,learningorder,position,andmore. arXivpreprint
arXiv:2405.05847,2024.
[20] ZimingLiu, EricJMichaud, andMaxTegmark. Omnigrok: Grokkingbeyondalgorithmic
data. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=zDiHoIWa0q1.
[21] AletheaPower,YuriBurda,HarriEdwards,IgorBabuschkin,andVedantMisra. Grokking:
Generalizationbeyondoverfittingonsmallalgorithmicdatasets,2022.
[22] UriAlon. Anintroductiontosystemsbiology: designprinciplesofbiologicalcircuits. Chapman
andHall/CRC,2019.
[23] UtkarshSharmaandJaredKaplan.Aneuralscalinglawfromthedimensionofthedatamanifold.
arXivpreprintarXiv:2004.10802,2020.
[24] EricMichaud,ZimingLiu,UzayGirit,andMaxTegmark. Thequantizationmodelofneural
scaling. AdvancesinNeuralInformationProcessingSystems,36,2024.
[25] JinyeopSong,ZimingLiu,MaxTegmark,andJeffGore. Aresourcemodelforneuralscaling
law. arXivpreprintarXiv:2402.05164,2024.
[26] PreetumNakkiran,GalKaplun,YaminiBansal,TristanYang,BoazBarak,andIlyaSutskever.
Deepdoubledescent: Wherebiggermodelsandmoredatahurt. InInternationalConferenceon
LearningRepresentations,2020. URLhttps://openreview.net/forum?id=B1g5sA4twr.
11[27] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature
and lazy training in deep neural networks. Journal of Statistical Mechanics: Theory and
Experiment, 2020(11):113301, nov 2020. doi: 10.1088/1742-5468/abc4de. URL https:
//dx.doi.org/10.1088/1742-5468/abc4de.
[28] JonathanFrankleandMichaelCarbin. Thelotterytickethypothesis: Findingsparse,trainable
neural networks. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=rJl-b3RcF7.
[29] ArthurJacot,FranckGabriel,andClémentHongler. Neuraltangentkernel: Convergenceand
generalization in neural networks. Advances in neural information processing systems, 31,
2018.
[30] IsaacLiao,ZimingLiu,andMaxTegmark. Generatinginterpretablenetworksusinghypernet-
works,2023.
[31] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: reverse
engineeringhownetworkslearngroupoperations. InProceedingsofthe40thInternational
ConferenceonMachineLearning,ICML’23.JMLR.org,2023.
[32] DashiellStander,QinanYu,HongluFan,andStellaBiderman. Grokkinggroupmultiplication
withcosets,2023.
[33] PhilipQuirkeandFazlBarez. Understandingadditionintransformers,2024.
[34] PhilipQuirke,ClementNeo,andFazlBarez. Increasingtrustinlanguagemodelsthroughthe
reuseofverifiedcircuits,2024.
[35] BoazBarak,BenjaminL.Edelman,SurbhiGoel,ShamKakade,EranMalach,andCyrilZhang.
Hiddenprogressindeeplearning: Sgdlearnsparitiesnearthecomputationallimit,2023.
[36] VimalThilak,EtaiLittwin,ShuangfeiZhai,OmidSaremi,RoniPaiss,andJoshuaSusskind.The
slingshotmechanism: Anempiricalstudyofadaptiveoptimizersandthegrokkingphenomenon,
2022.
[37] Fatih Furkan Yilmaz and Reinhard Heckel. Regularization-wise double descent: Why it
occursandhowtoeliminateit. In2022IEEEInternationalSymposiumonInformationTheory
(ISIT), page 426–431. IEEE Press, 2022. doi: 10.1109/ISIT50566.2022.9834569. URL
https://doi.org/10.1109/ISIT50566.2022.9834569.
[38] RylanSchaeffer,ZacharyRobertson,AkhilanBoopathy,MikailKhona,KaterynaPistunova,
Jason William Rocks, Ila R Fiete, Andrey Gromov, and Sanmi Koyejo. Double descent
demystified: Identifying, interpreting & ablating the sources of a deep learning puzzle. In
TheThirdBlogpostTrackatICLR2024,2024. URLhttps://openreview.net/forum?id=
muC7uLvGHr.
[39] XanderDavies,LauroLangosco,andDavidKrueger. Unifyinggrokkinganddoubledescent.
arXivpreprintarXiv:2303.06173,2023.
[40] LuciaQuirke,LovisHeindrich,WesGurnee,andNeelNanda. Trainingdynamicsofcontextual
n-gramsinlanguagemodels. arXivpreprintarXiv:2311.00863,2023.
[41] DavidDBaek,ZimingLiu,andMaxTegmark. Geneft: Understandingstaticsanddynamicsof
modelgeneralizationviaeffectivetheory. arXivpreprintarXiv:2402.05916,2024.
[42] LoekvanRossemandAndrewMSaxe. Whenrepresentationsalign: Universalityinrepresenta-
tionlearningdynamics. arXivpreprintarXiv:2402.09142,2024.
[43] MichaelY.Hu,AngelicaChen,NaomiSaphra,andKyunghyunCho. Latentstatemodelsof
trainingdynamics. TransactionsonMachineLearningResearch,2023. ISSN2835-8856. URL
https://openreview.net/forum?id=NE2xXWo0LF.
[44] MinyoungHuh,BrianCheung,TongzhouWang,andPhillipIsola. Theplatonicrepresentation
hypothesis. arXivpreprintarXiv:2405.07987,2024.
12[45] YoshuaBengio,AaronCourville,andPascalVincent. Representationlearning: Areviewand
new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1798–1828,2013.
[46] PhucHLe-Khac,GrahamHealy,andAlanFSmeaton. Contrastiverepresentationlearning: A
frameworkandreview. IeeeAccess,8:193907–193934,2020.
[47] Zhi-HuaZhou. Abriefintroductiontoweaklysupervisedlearning. Nationalsciencereview,5
(1):44–53,2018.
[48] AshishJaiswal,AshwinRameshBabu,MohammadZakiZadeh,DebapriyaBanerjee,andFillia
Makedon. Asurveyoncontrastiveself-supervisedlearning. Technologies,9(1):2,2020.
[49] Jean-BastienGrill,FlorianStrub,FlorentAltché,CorentinTallec,PierreRichemond,Elena
Buchatskaya,CarlDoersch,BernardoAvilaPires,ZhaohanGuo,MohammadGheshlaghiAzar,
etal. Bootstrapyourownlatent-anewapproachtoself-supervisedlearning. Advancesinneural
informationprocessingsystems,33:21271–21284,2020.
[50] XinleiChenandKaimingHe.Exploringsimplesiameserepresentationlearning.InProceedings
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages15750–15758,
2021.
[51] DiegoDoimo,AldoGlielmo,SebastianGoldt,andAlessandroLaio. Redundantrepresentations
helpgeneralizationinwideneuralnetworks. JournalofStatisticalMechanics: Theoryand
Experiment,2023(11):114011,2023.
[52] JoshuaEngels,IsaacLiao,EricJMichaud,WesGurnee,andMaxTegmark. Notalllanguage
modelfeaturesarelinear. arXivpreprintarXiv:2405.14860,2024.
[53] Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets:
Zeros,signs,andthesupermask. InAdvancesinNeuralInformationProcessingSystems,2019.
[54] VivekRamanujan,MitchellWortsman,AniruddhaKembhavi,AliFarhadi,andMohammad
Rastegari. What’s hidden in a randomly weighted neural network? In 2020 IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pages11890–11899,2020.
doi: 10.1109/CVPR42600.2020.01191.
[55] EranMalach,GiladYehudai,ShaiShalev-Schwartz,andOhadShamir.Provingthelotteryticket
hypothesis: Pruningisallyouneed. InHalDauméIIIandAartiSingh,editors,Proceedingsof
the37thInternationalConferenceonMachineLearning,volume119ofProceedingsofMachine
LearningResearch,pages6682–6691.PMLR,13–18Jul2020. URLhttps://proceedings.
mlr.press/v119/malach20a.html.
[56] ArthurdaCunha,EmanueleNatale,andLaurentViennot. Provingthelotterytickethypothesis
forconvolutionalneuralnetworks. InInternationalConferenceonLearningRepresentations,
2022. URLhttps://openreview.net/forum?id=Vjki79-619-.
[57] LaurentOrseau,MarcusHutter,andOmarRivasplata. Logarithmicpruningisallyouneed. In
Proceedingsofthe34thInternationalConferenceonNeuralInformationProcessingSystems,
NIPS’20,RedHook,NY,USA,2020.CurranAssociatesInc. ISBN9781713829546.
[58] AnkitPensia,ShashankRajput,AlliotNagle,HaritVishwakarma,andDimitrisPapailiopoulos.
Optimal lottery tickets via subsetsum: logarithmic over-parameterization is sufficient. In
Proceedingsofthe34thInternationalConferenceonNeuralInformationProcessingSystems,
NIPS’20,RedHook,NY,USA,2020.CurranAssociatesInc. ISBN9781713829546.
[59] James Diffenderfer and Bhavya Kailkhura. Multi-prize lottery ticket hypothesis: Finding
accuratebinaryneuralnetworksbypruningarandomlyweightednetwork. InInternational
ConferenceonLearningRepresentations,2021. URLhttps://openreview.net/forum?
id=U_mat0b9iv.
[60] AriS.Morcos,HaonanYu,MichelaPaganini,andYuandongTian. Onetickettowinthemall:
generalizinglotteryticketinitializationsacrossdatasetsandoptimizers. CurranAssociatesInc.,
RedHook,NY,USA,2019.
13[61] Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Jingjing Liu, and Zhangyang
Wang. The elastic lottery ticket hypothesis. In M. Ranzato, A. Beygelzimer,
Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Infor-
mation Processing Systems, volume 34, pages 26609–26621. Curran Associates, Inc.,
2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/
dfccdb8b1cc7e4dab6d33db0fef12b88-Paper.pdf.
[62] JonathanFrankle,GintareKarolinaDziugaite,DanielM.Roy,andMichaelCarbin. Stabilizing
thelotterytickethypothesis,2020.
[63] NamhoonLee,ThalaiyasingamAjanthan,andPhilipTorr. SNIP:SINGLE-SHOTNETWORK
PRUNING BASED ON CONNECTION SENSITIVITY. In International Conference on
LearningRepresentations,2019. URLhttps://openreview.net/forum?id=B1VZqjAcYX.
[64] JonathanFrankle,GintareKarolinaDziugaite,DanielRoy,andMichaelCarbin. Pruningneural
networksatinitialization:Whyarewemissingthemark? InInternationalConferenceonLearn-
ingRepresentations,2021. URLhttps://openreview.net/forum?id=Ig-VyQc-MLK.
[65] ChaoqiWang,GuodongZhang,andRogerGrosse. Pickingwinningticketsbeforetrainingby
preservinggradientflow. InInternationalConferenceonLearningRepresentations,2020. URL
https://openreview.net/forum?id=SkgsACVKPH.
[66] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neu-
ral networks without any data by iteratively conserving synaptic flow. In H. Larochelle,
M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural In-
formation Processing Systems, volume 33, pages 6377–6389. Curran Associates, Inc.,
2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
46a4378f835dc8040c8057beb6a2da52-Paper.pdf.
[67] Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redun-
dancy in pretrained transformer models. In Bonnie Webber, Trevor Cohn, Yulan He, and
Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 4908–4926, Online, November 2020. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.398. URL https:
//aclanthology.org/2020.emnlp-main.398.
[68] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternational
ConferenceonLearningRepresentations,2019. URLhttps://openreview.net/forum?
id=Bkg6RiCqY7.
14A WeightDecay
In the main paper, we have estimated how dimensionality is analogous to total resources in an
ecosystem,whileweightdecayrepresentstheresourceconstraintintheenvironmentthatdecayat
eachtimestep.
Consideringtheextremecasewhereweightdecayiszero: thereisnoresourcelimitanditisnot
surprising that all the frequencies survive while the neural network fails to generalize, as it has
trouble "grokking" [1, 20]. As weight decay slightly increases, the different frequencies pose a
competitivedynamicagainsteachother. Thenumberoffinalcircularrepresentationsgraduallydrops,
asillustratedinFigure10.
Figure10: Numberofcirclessurvivedasafunctionofweightdecay. Theleft paneldisplaysthe
completerangeofweightdecaystestedinourexperiments;themiddlefocusesonsmallerweight
decays,whiletherightillustratesthetransitioninthenumberofsurvivingcirclesatlargerweight
decays.
B Circularity
Inadditiontothesignalmagnitudemetricusedthroughoutthepaper,wecomputeanothermetric,
circularity,toanalyzetheinitialembedding. WemodifythemetricintroducedinZhongetal.[17]by
calculatingthrougheachfrequencyintheFourierBasisinsteadofthroughtheprincipalcomponents.
Asestablishedinthepaper,ifX ,...,X ∈RdareembeddingsprojectedontotheFourierbasis,
0 p−1
thecircularityforaspecificfrequencyf isdefinedas
(cid:12)p−1 (cid:12)2
2 (cid:12)(cid:88) (cid:12)
c = (cid:12) X e2πi·jk/p(cid:12) (5)
k p(cid:80)p−1X2 (cid:12)
(cid:12)
k,j (cid:12)
(cid:12)
j=0 k,j j=0
Usingthismetric,weconducttwoexperiments: onetomeasurethecircularityoftheembeddingas
itsdimensionalityvariesandanothertoseeifinitialcircularityplaysaroleininformingeventual
representations.
To investigate the impact d exerts on the system, we randomly sample 50 initial embeddings of
different dimensions and compute the mean and maximum circularity among all frequencies, as
showninFigure11(Left). Indeed,circularityincreasesasdimensionalityincreases,confirmingour
observationthathigher-dimensionalembeddingencodesmorecomplexinformationatinitialization.
Wealsoaimtoverifythehypothesisthatiftheembeddingisinitializedclosertoacircleonagiven
frequency,thatfrequencyismorelikelytosurvive. However,theevidenceshowninFigure11(Right)
isinconclusiveastowhetherlargerinitialcircularityimpliesabetterchanceofsurvival.
15Figure11: (Left)CircularityofprojectionsoftheembeddingontodifferentFourierfrequenciesat
initialization as dimension d varies, calculated using Equation 5. (Right) Histogram of different
frequenciesbytheirgradients,alongwiththeirsurvivalstatus.
C ForcingModeltoLearn< 3Circles
Inourtrainingwithanembeddingofreasonablylargesize,suchas(p,d) = (59,128),themodel
almostalwayschoosesthreeormorecirclesasitsfinalrepresentations. AsillustratedinFigure8,
thesecirclescooperatetosolvethemodularadditiontaskinsteadofactingontheirown. However,
one would notice that if the model were using the Clock algorithm [8], a single circle would be
sufficienttosolvethetaskperfectly. Theobservationnaturallyevokesthefollowingquestions: why
doesthemodelchoosetolearnmultiplecircles,andcanwe’force’ittolearnfewerthanthreecircles
undersomecontrivedconditions?
Torestricttherepresentationsthemodelcanlearn,wemanuallyablatetheinitialembeddingsothat
onlyoneortwofrequencieshavenon-zerosignalsontheFourierbasis.
Specifically,foragivenembedding,wefirsttrainwithoutanyablation,fromwhichweidentifythe
originalcirclesthemodelchoosestolearn. Wethenusek todenotethefrequencywithlargestsignal
1
aftertrainingandk todenotethesecondlargestfrequency.
2
Usingthisinformation,weconductfourablationexperiments:
A) Atinitialization,projecttheembeddingontotheFourierbasis,setallfrequenciesexceptfor
k to0,anduseinverseFFTtoreconstructtheembedding. Trainthemodelusingthisinitial
1
embedding.
B) Atinitialization,usethesameablationprocedureasabove,butsuppressallotherfrequencies
exceptfork andk to0.
1 2
C) Atinitialization,randomlyselectafrequencyk andsuppressallotherfrequenciesexcept
r,1
fork .
r,1
D) At initialization, randomly select two frequencies k and k and suppress all other
r,1 r,2
frequenciesexceptforthosetwo.
InExperimentA,althoughtheembeddingisinitializedwithonlyonefrequencywithsignificant
signal,themodelrevivessomefrequencieswithanoriginally0signaltoformcircularstructureswith
strongsignals,andeventuallyendsupwithfourlearnedcircles. Figure12(Left)showsthetestloss
forExperimentAinorange. Wecaninferfromthelosscurvethatdespitetryingtolearnwithonly
onecircle,themodelstrugglestoachievelowerlosswiththissimplerepresentationandhastomake
itsrepresentationmorecomplexovertime,leadingtotheperiodicspikesintestloss. Innoneofour
experimentswereweabletoconstructamodelthatnaturallyformsasingle-circlerepresentation.
However,inExperimentB,themodelachievesgoodperformanceusingonlythetwocirclesinitialized
withnon-zerosignals. Twocirclesseemsufficientforthemodeltoachievealossaslowas1e−7,if
thetwocirclesareinitalizedwellandweforcethemodeltoonlyusetwo. Therefore,wesuspect
thatthreecirclesarenotnecessarybutratheramodelchoicetopreferredundantrepresentations[67].
16Interestingly,ExperimentBreacheslowertestlossmorequicklythantheoriginal,mainlybecause
ablatingtohaveonlythestrongesttwofrequenciesalreadyservesasatypeoftrainingandmakesthe
modeltrainingprocesseasier,similartothefindingsofZhouetal.[53].
Incomparison,Figure12(Right)showsthatthemodelstrugglestolearnwithonly1or2random
frequencies;aftertraining,atleastthreecirclessurvive. Summarizingtheseresults,weconcludethat
thechoiceofthecirclesarenotarbitrary: themodelcanlearnthetaskwellwithonlythetwomost
fittedfrequenciesbutnottworandomfrequencies. Here,wefurthercorroboratethattheembedding
initializationplaysasignificantroleinthemodel’spreferenceforitscirclerepresentations.
Figure12: (Left)Testlosscurveovertrainingtimestepsoftheablationexperimentwhenkeepingall
frequencies(blue),thelargestfrequency(orange),andthelargesttwofrequencies(green). (Right)
Testlosscurveoftheablationexperimentwhenkeepingallfrequencies(blue),onerandomfrequency
(pink),andtworandomfrequencies(cyan).
Note that in experiments A, C, and D, the test loss curve exhibits several cycles of spiking and
subsequentdecay. ThisslingshotphenomenonisassociatedwiththeuseoftheAdamoptimizerand
oftenco-occurswithgrokking[36]. Wedonotdiscussthisfurther,asitfallsoutsidethescopeofour
research.
D EmbeddingGradients
InSection3.2.2,weapproximatethegradientasthedifferenceofsignalsbeforeandafteragiven
timestep. Weprovidefurtherjustificationhere.
Theactualgradientontheembeddingsconsistsoftwoparts: thegradient ∂L producedbyMLP
∂Ek
on the embedding through backpropagation and weight decay of the initial data. To understand
the effect of the former on frequency signals, we use the same Fourier transform procedure to
transformthegradientstotheFourierbasisandcomputetheirnorm,astheFouriertransformisa
lineartransformation. InFigure13,wevisualizethenormofthegradientinFourierbasisovertime.
TheFouriergradientspikesaround100stepsandquicklydiminishestonearzeroafter1000steps.
Afterthispoint,thegradientbecomesnegligiblysmall,andweightdecaybecomesthedominant
factoraffectingtheevolutionofsignals.
Theseobservationsmotivatetwoexperimentaldecisionsinourpaper.
1. Becauseitisdifficulttoreconstructtheeffectofbothbackpropagationandweightdecay
compounded on top of each other on the Fourier basis, especially when weight decay
dominatesthegradient,wethinkthedifferenceinsignalisasimpleandsufficientproxyto
conductexperimentswithinSection3.2.2.
2. Sincetheembeddinggradientbecomesnegligiblysmallafter1000steps,weonlyusedata
fromthefirst1000stepstofitthelinearODEsysteminSection4. Moredatapointsafter
thefirst1000stepswillonlyallowtheregressionmodeltocapturethedynamicsofweight
decay,whichdistractsfromourstudyofthedynamicsbetweenrepresentations.
17Figure13: TheembeddinggradientsthroughbackpropagationprojectedontotheFourierbasisover
time. The right zooms into timesteps 100 to 1000. The colored curves denote frequencies that
eventuallysurvived,whilethegreyonesrepresentgradientsofdeadfrequencies.
Figure14: Numberofsurvivedcirclesoverrandomtrialsasafunctionofp,whereprangesfrom17
to97.
E TrainingDetails
As discussed in Section 2, the model we trained for the modular addition task features a simple
embedding-MLParchitecture. Theembeddinghasasizeof(p,d),whereprepresentsthemodulus
anddstandsforthedimension. Bydefault,wesetp=59andd=128. Inourexperiments,wevary
panddtostudytheireffectsonmodel’slearnedembeddings.
TheMLPinourmodelconsistsoftwolayers: aninputlayerwithdimension2d,twohiddenlayers,
eachwithawidthof100,andanoutputlayerwithdimensionp,whichrepresentsthelogitsforeach
tokenfrom0top−1.
BoththeembeddingandtheMLPareinitializedfromaGaussiandistributionwithµ=0andσ =1.
WetrainthemodelusingtheAdamWoptimizer[68],withalearningratesetto0.01. Thetraining
lossisdefinedasthecross-entropylossbetweenthelogitscomputedbythemodelandtheground
18Figure15: Coefficientsheatmapsfor3moreLassofitsoftheembeddings’trainingdynamics.
truth. Toencouragemodelgeneralization,weuseatrain-testsplitof80-20andapplyadefaultweight
decayof0.5. Additionally,weexperimentwithotherweightdecayvaluestostudytheirimpact.
Ineachexperiment,themodelistrainedfor3×104steps.
F Non-primeModulusp
InSection3.1,weonlyprovideresultsforprimemoduluspsincenon-primepbehavesdifferently
inthemodularadditiontaskduetotheirnon-trivialfactors. Forexample, whenp = 12, acircle
withdelta∆=2doesnotcoverallthenumbersin[0,p−1],andourpreviousanalysisinFourier
basisnolongerholdstrue. However,wepresentsupplementaryresultsontheeffectofthenumberof
survivingcirclesforallpossiblemoduliinFigure14.ComparedwithFigure5(Left),onecanobserve
morevariationinFigure14,butanupwardtrendcanstillbeobserved.
G MoreODEApproximatedTrajectories
InFigure9,wehaveshownthetrajectoriesoftworepresentativefrequenciesapproximatedbyour
linearODE,onedeadandonesurvived,andonerepresentativecoefficientsheatmap. InFigure15,
weshowthesparsecoefficientsheatmapfor3moreODEfitswithLasso. InFigure16,wereport
predictedtrajectoriesforall29frequenciesinonetrainingrun.
H ExperimentsComputeResource
AllthemodeltrainingisperformedonNVIDIAV100GPUs. WeprovidetheGPUspecsasfollows:
• Processor: IntelXeonGold6248
• Nodes: 224
• ClockRate: 2.5GHz
• CPUcores: 40
• NodeRAM:384GB
• RAMpercore: 9GB
• Acceleratortype: NvidiaVoltaV100
• Accelerators(perNode): 2
• AcceleratorRAM:32GB
TheGPUdaysneededforeachexperimentare:
• Initialgradient/gradientexperimentin3.2: 1GPUday
19Figure 16: Original and estimated trajectories for signal evolution of all 29 frequencies during
training.
• FreezeembeddingexperimentinSection3.1: 0.8GPUdays
• VaryingmoduluspexperimentinSection3.1: 15GPUdays
• VaryingdimensionexperimentinSection3.1: 9GPUdays
• InitialsignalperturbationexperimentinSection3.2.1: 15GPUdays
• ManualconstructionexperimentinSection3.2.1: 0.5GPUdays
• VaryingweightexperimentinAppendixA:2GPUdays
• Othersmall-scaleexperiments: 1GPUday
Overall,theexperimentscomputeresourceaddsuptoabout45GPUdays. Thefullresearchproject
doesnotrequiremorecomputethantheexperimentsreportedinthepaper.
20I AcknowledgementforOnlineAssetsUsed
WehaveutilizedseveralonlineassetsincreatingFigure1. TheoriginalillustrationoftheFourier
Transformcanbeaccessedhere. TheiconsaresourcedfromTheNounProjectandClker.
21