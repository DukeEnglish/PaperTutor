Spectral Greedy Coresets for Graph Neural Networks
Mucong Ding∗ Yinhan He† Jundong Li‡ Furong Huang§
Abstract
The ubiquity of large-scale graphs in node-classification tasks significantly hinders the real-
world applications of Graph Neural Networks (GNNs). Node sampling, graph coarsening, and
dataset condensation are effective strategies for enhancing data efficiency. However, owing to the
interdependence of graph nodes, coreset selection, which selects subsets of the data examples,
has not been successfully applied to speed up GNN training on large graphs, warranting special
treatment. This paper studies graph coresets for GNNs and avoids the interdependence issue
by selecting ego-graphs (i.e., neighborhood subgraphs around a node) based on their spectral
embeddings. We decompose the coreset selection problem for GNNs into two phases, a coarse
selection of widely spread ego graphs and a refined selection to diversify their topologies. We
design a greedy algorithm that approximately optimizes both objectives. Our spectral greedy
graph coreset (SGGC) scales to graphs with millions of nodes, obviates the need for model
pre-training, and is applicable to low-homophily graphs. Extensive experiments on ten datasets
demonstrate that SGGC outperforms other coreset methods by a wide margin, generalizes well
across GNN architectures, and is much faster than graph condensation.
1 Introduction
Graph neural networks (GNNs) have achieved significant success in addressing various graph-related
tasks, such as node classification and link prediction (Hamilton, 2020). Nonetheless, the widespread
presence of large-scale graphs in real-world scenarios, including social, informational, and biological
networks, presents substantial computational challenges for GNN training, given the frequent scaling
of these graphs to millions of nodes and edges. The cost of training a single model is considerable and
escalates further when multiple training iterations are required, for example, to validate architectural
designs and hyperparameter selections (Elsken et al., 2019). To tackle the above issues, we adopt a
natural data-efficiency approach — simplifying the given graph data appropriately, with the goal of
saving training time. In particular, we ask the following question: how can we appropriately simplify
graphs while preserving the performance of GNNs?
A simple yet effective solution to simplify a dataset is coreset selection, despite other methods
such as graph sparsification, graph coarsening, and graph condensation reviewed in the related
work Section 5. Typically, the coreset selection approach (Toneva et al., 2018; Paul et al., 2021)
finds subsets of data examples that are important for training based on certain heuristic criteria.
The generalization of coreset selection to graph node/edge classification problems is then to find the
∗University of Maryland, College Park; e-mail: mcding@umd.edu
†University of Virginia, Charlottesville; e-mail: nee7ne@virginia.edu
‡University of Virginia, Charlottesville
§University of Maryland, College Park
1
4202
yaM
72
]GL.sc[
1v40471.5042:viXraimportant “subsets” of the given graph, e.g., nodes, edges, and subgraphs. This challenge arises from
graph nodes’ interdependence and GNNs’ non-linearities. We focus on node classification in this
paper as it is among the important learning tasks on graphs and is still largely overlooked.
SGGC:Selecting ego-graphs iteratively and greedily
selected
preprocess compute scores graph reduction fast training
GNNs
one time and select union of selected on coreset graph
ego-graphs
repeat until budget limit
original large graph small coreset graph
performant inference
with preserved test performance on original graph
Figure 1: Overview of spectral greedy graph coresets (SGGC) for efficient GNN training. SGGC processes a
large graph to iteratively select ego-graphs. The assembled coreset graph facilitates fast GNN training while
maintaining test performance on the original graph.
In this paper, we find a new approach to formulate graph coreset selection for GNNs. It avoids
GNN’s interdependent nodes and non-linear activation issues by selecting ego-graphs, i.e., the
subgraph induced by the neighborhood around a center node, based on their node embeddings in the
graph-spectral domain. Our ego-graph-based coreset is inspired by two observations. (1) We find
that most GNNs applied to large graphs follow the nearest-neighbor message-passing update rule and
haveego-graph-likereceptivefields. Thus,byselectingtheego-graphs(whichisequivalenttoselecting
their center nodes), we avoid the problem of finding subsets of nodes and edges independently, which
typically leads to complex combinatorial optimization; see Section 2. (2) Moreover, we identify
that when expressing the node embeddings in the graph-spectral domain, the non-linear spectral
embeddings of GNNs on ego-graphs are “smooth” functions of the center node, i.e., nearby nodes are
likely to have similar spectral embeddings on their corresponding ego-graphs (Balcilar et al., 2021),
which we will theoretically justify under certain assumptions in Section 3.
Using (1) and (2), we propose approximating the GNN’s spectral embedding using a subset
of ego-graphs. To approximate the spectral embedding with fewer ego-graphs (which one-to-one
correspond to their center nodes), one should select center nodes that are far from each other since
nearby ones are likely to have similar embeddings, thus, being less informative. We derive an upper
bound on the coreset objective independent of the spectral embedding. This enables us to find the
coreset of center nodes without evaluating the GNN’s spectral embeddings on any ego-graph. With
the coreset objective substituted by the upper-bound, we adopt the greedy iterative geodesic ascent
(GIGA) approach (Campbell and Broderick, 2018; Vahidian et al., 2020) to obtain the coresets.
The above procedure of selecting distant ego-graphs is sufficient to approximate the whole graph’s
spectral embedding well. However, the selected center nodes do not necessarily approximate the
node classification loss well, and the topological information of ego-graphs is not considered. To
approximate the GNN training loss, we propose to refine the coreset selection by filtering out some
of the selected ego-graphs whose topologies are not distinctive enough. Since the transformation
from the spectral to the spatial domain is a linear operation depending on the graph topology, the
approximated spatial embeddings of ego-graphs will differ more when they have more distinctive
topologies. Hence, we exclude ego-graphs with non-distinctive spatial embeddings to enhance
efficiency. This is solved by the submodular coreset algorithm (Iyer et al., 2021; Kothawade et al.,
2022) using greedy submodular maximization (Mirzasoleiman et al., 2020).
2As a result, we decompose the ego-graph selection into two stages: a coarse selection of widely
spread ego-graphs that approximate the whole graph’s spectral embedding (as detailed in Equa-
tion (NAC)) and a refined selection to approximate the node classification loss with improved sample
efficiency (as detailed in Equation (LCC)). Specifically, the first stage (which is solved via GIGA)
extrapolates the graph to find distant center nodes over the original graph, and the second stage
(which is solved via submodular maximization) exploits the candidate center nodes and keeps the
most informative ones based on their topologies. We call this two-stage algorithm spectral greedy
graph coresets (SGGC). Our SGGC compresses node attributes of selected ego-graphs using low-rank
approximations, maintaining efficient storage without compromising GNN performance, as shown
in Section 6.
A visualization overview of the SGGC approach, from preprocessing a large graph to forming a
coreset graph for fast GNN training, is provided in Figure 1. SGGC scales to large graphs, needs no
pre-training, and performs well on both high- and low-homophily graphs. SGGC surpasses other
coreset methods in our experiments on ten graph datasets. We show that the combined algorithm is
better than applying either algorithm (GIGA or submodular maximization) individually. Moreover,
SGGC matches graph condensation’s performance (Jin et al., 2021), but is significantly faster and
better generalizes across GNN architectures.
Our contributions are summarized as follows:
(1) Novel Graph Coreset Selection Method for GNNs: We propose a novel graph coreset
selection method for GNNs, leveraging ego-graphs to address computational challenges in training
on large-scale graphs, which simplifies the data while preserving GNN performance.
(2) Two-Stage Spectral Greedy Graph Coresets Algorithm: Our approach introduces a
two-stage algorithm, SGGC, that efficiently reduces graph size for GNN training by first broadly
selecting ego-graphs and then refining this selection based on topological distinctiveness.
(3) Theoretical Foundation: We provide a theoretical justification for our method, showing
that ego-graph-based coresets can approximate GNN spectral embeddings with smooth functions,
enabling a more effective simplification of graphs.
(4) Empirical Validation Across Diverse Graphs: SGGC demonstrates superior performance
in experiments on ten graph datasets, offering significant improvements in efficiency, scalability, and
generalizability over existing coreset and condensation methods.
2 Problem: Graph Coresets for GNNs
We start by defining the downstream task for graph coresets and node classification with graph
neural networks. For a matrix M, we denote its (i,j)-th entry, i-th row, j-th column by M , M ,
i,j i,:
and M , respectively.
:,j
Node classification on a graph considers that we are given an (undirected) graph G = (V =
[n], E ⊂ [n]×[n]) with (symmetric) adjacency matrix A ∈ {0,1}n×n, node features X ∈ Rn×d,
node class labels y ∈ [K]n, and mutually disjoint node-splits V (cid:83) V (cid:83) V = [n], where we
train val test
assume the training split consists of the first n nodes, i.e., V = [n ]. Using a graph neural
t train t
network (GNN) f : Rn×n × Rn×d → Rn×K, where θ ∈ Θ denotes the parameters, we aim to
θ ≥0
find θ∗ = argmin L(θ), where the training loss is L(θ) := 1 (cid:80) ℓ([f (A,X)] , y ). Here
θ nt i∈[nt] θ,λ i,: i
Z := [f (A,X)] is the output embedding for the i-th node. The node-wise loss function is
i θ,λ i,:
ℓ(Z ,y ) := CrossEntropy(softmax(Z ),y ). The loss L(θ) defined above is under the transductive
i i i i
3Figure 2: Relative standard devia- Figure 3: Conceptual diagram Figure 4: Spectral response of
tion of spectral embeddings on ego- showingthetheoreticalanalysisfor- 2-layer GCNs on Cora. The
graphs Z across all the nodes vs. mulating the spectral greedy graph spectral response corresponding
i
the ego-graph size p; see Section 3. coresets (SGGC). to eigenvalue λ is defined as
i
∥[UTf (A,X)] ∥/∥[UTX] ∥.
θ i,: i,:
setting, which can be generalized to the inductive setting by assuming only {A | i,j ∈ [n ]} and
ij t
{X | i ∈ [n ]} are used during training.
i t
Graph neural networks (GNNs) can often be interpreted as iterative convolution over nodes
(i.e., message passing) (Ding et al., 2021), where given inputs X(0) = X,
X(l+1) = σ(C (A)X(l)W(l)) ∀l ∈ [L], (GNN)
α(l)
and X(L) = f (A,X). Here, C (A) is the convolution matrix which is (possibly) parametrized by
θ α(l)
α(l), W(l) is the learnable linear weights, and σ(·) denotes the non-linearity. See Appendix B for
more details on GNNs.
Graph coresets for GNNs Graph coresets for GNNs seek to select a subset of training
nodes V ⊂ V = [n ] with size |V | ≤ c ≪ n along with a corresponding set of sample weights
w train t w t
such that the training loss L(θ) is approximated for any θ ∈ Θ. Let w ∈ Rnt be the vector of
≥0
non-negative weights, we require the number of non-zero weights ∥w∥ := (cid:80) 1[w > 0] ≤ c, and
hence the search space is W := {w ∈ Rnt | ∥w∥ ≤ c}. The
objective0
of
grai p∈ h[n ct] oresei
t selection is
≥0 0
min max(cid:12) (cid:12) (cid:88) w ·ℓ(cid:0) [f (A,X)] , y (cid:1) −L(θ)(cid:12) (cid:12), (GC)
(cid:12) i θ i i (cid:12)
w∈W θ∈Θ
i∈[nt]
whichminimizestheworst-caseerrorforallθ. However,theaboveformulationprovides nearly no data
reduction in terms of its size, since both the graph adjacency matrix A and node feature matrix X
are still needed to compute the full-graph embeddings f (A,X) in the coreset loss in Equation (GC).
θ
Since the goal of graph coresets is not only to reduce the number of labels, but more importantly,
the data size, we should formalize how the subsets of A and X are selected in Equation (GC) to
make the objective practical. A natural convention, considered by related literature including graph
condensation (Jin et al., 2021), is further assuming that only the node features of the selected nodes,
X = {X | i ∈ V }, and the subgraph induced by the selected nodes, A = {A | i,j ∈ V } are
w i,: w w i,j w
kept. Under this convention, the central problem of graph coresets changes to selecting the labeled
nodes, as well as their node features and adjacencies, which we call as node-wise graph coresets,
min max(cid:12) (cid:12) (cid:88) w ·ℓ(cid:0) [f (A ,X )] , y (cid:1) −L(θ)(cid:12) (cid:12). (N-GC)
(cid:12) i θ w w i i (cid:12)
w∈W θ∈Θ
i∈[nt]
4However, since A and X are complex discrete functions of w, the above formulation leads to a
w w
complex combinatorial optimization, which we still need to learn how to solve. Moreover, posing
A and X to be entirely determined by w leads to sub-optimality. For example, it is likely that
w w
there exists another set of sample weights w′ ∈ W such that using A and X in Equation (N-GC)
w′ w′
results in a smaller error.
Alternative formulation of graph coresets. The critical difference between Equation (N-
GC) and a typical coreset problem like Equation (GC) is that the node-pair-wise relation encoded
in the adjacency matrix A forbids us to select nodes as independent samples. In this paper, we
consider another formation of graph coresets, to avoid the non-independent issue. The idea is to
use the property that most GNNs (especially those applied to large graphs) are “local functions”
on the graph, i.e., the output embedding Z = [f (A,X)] of the i-th node may only depend on
i θ,λ i,:
the close-by nodes {j ∈ [n] | d(i,j) < D} where d(i,j) denotes the shortest-path distance. Without
loss of generality, we consider nearest-neighbor message passing (including GCN, GAT, and GIN),
whose convolution/message-passing weight is non-zero C ≠ 0 if and only if i = j or A = 1. More
i,j i,j
specifically, we define the receptive field of a node i for an L-layer GNN (Equation (GNN)) as a set
of nodes VL whose features {X | j ∈ VL} determines Z . For nearest-neighbor message passing, it
i j,: i i
is easy to see V1 = {i}∪{j ∈ [n] | A = 1} = {j ∈ [n] | d(i,j) ≤ 1}. Then by induction, for L-layer
i i,j
GNNs, the respective filed of node i is VL = {j ∈ [n] | d(i,j) ≤ L}, which is exactly its depth-L
i
ego-graph. Here, we assume the GNN is L-layered, and the depth-L ego-graph of node i, denoted
by G , is defined as the induced subgraph of nodes within distance L. The above characterization of
i
the “local property” of GNNs leads to the following equation,
(cid:2) (cid:3) (cid:2) (cid:3)
f (A,X) = f (A ,X ) ∀ i ∈ [n], (RF)
θ i,: θ Gi Gi 1,:
where A and X denote the adjacencies and node features in the ego-graph G , respectively,
Gi Gi i
where we always re-number the center node i in G as the first node.
i
Ego-graph-wise graph coreset can then be formulated by substituting Equation (RF)
into Equation (GC),
min max(cid:12) (cid:12) (cid:88) w ·ℓ(cid:0) [f (A ,X )] , y (cid:1) −L(θ)(cid:12) (cid:12). (EG-GC)
w∈W θ∈Θ
(cid:12) i θ Gi Gi 1,: i (cid:12)
i∈[nt]
Compared with node-wise graph coreset (Equation (N-GC)), ego-graph-wise selection has the
following advantages: (1) it avoids the non-independence issue as we are now selecting ego-graphs
independently, i.e., whether G (j ̸= i) is selected will not affect the embedding [f (A ,X )] of
j θ Gi Gi 1,:
node i; (2) it is equivalent to the original objective (Equation (GC)) which ensures optimality; and
(cid:83)
(3) although the adjacencies and node features in the union of selected ego-graphs G are kept
i∈Vw i
and their size could be O(dL ) times of the node-wise selected data (where d is the largest node
max max
degree), we find that we can highly compress the ego-graph node features via principal component
analysis (PCA), depending on how far away the nodes are from the selected center nodes V , which
w
eventually leads to comparable data size reduction. See Figure 5 and Appendix A for details.
3 Spectral Greedy Graph Coresets
Despite the numerous benefits of selecting ego-graph subsets, addressing Equation (EG-GC) remains
a challenging task due to the highly non-linear nature of the GNN f , and it is costly because the
θ
5evaluation of A and X necessitates locating the ego-graph G , a process that incurs a time
Gi Gi i
complexity of O(dL ). This section introduces a method that is both efficient and effective for
max
solving Equation (EG-GC), designed to circumvent the non-linearities present in f without the need
θ
fordirectevaluationofA andX forany node. ThekeyideainvolvesreformulatingEquation(EG-
Gi Gi
GC) within the graph spectral domain.
Graph spectral domainreferstotheeigenspaceofthegraphLaplacian,whichalsoencompasses
the corresponding spectral feature space. Consider the symmetrically normalized Laplacian L =
I −D−1/2AD−1/2 where D is the diagonal degree matrix. By applying eigendecomposition, we
n
obtain L = Udiag(λ ,...,λ )UT, where the eigenvalues satisfy 0 ≤ λ ≤ ··· ≤ λ ≤ 2, and each
1 n 1 n
column u = U represents an eigenvector. Features or embeddings can be transformed into
i :,i
the spectral domain by left-multiplying with UT, for instance, UTX. Here, the i-th row [UTX]
i,:
corresponds to the features associated with eigenvalue λ .
i
Likewise, for each ego-graph G , it is possible to identify the spectral representation of ego-graph
i
embeddings, represented as Z(cid:101)i = UG iTf θ(A Gi,X Gi). To simplify our analysis and ensure that Z(cid:101)i
associated with different eigenvalues have identical dimensions, we introduce a nuanced concept
of ego-graphs, termed diffusion ego-graphs. Consider the diffusion matrix P = 1I + 1D−1A,
2 n 2
characterized as right stochastic — meaning each row sums to 1, and it models a lazy-random walk
across the graph. The matrix P can be diagonalized simultaneously with L, producing eigenvalues
in the order 1 ≥ 1− 21λ
1
≥ ··· ≥ 1− 1 2λ
n
≥ 0. The diffusion ego-graph G(cid:101)i of node i s defined
as the induced subgraph comprising V(cid:101) iL = {indices of the p largest entries of [PL] i,:}. When p is
sufficiently large, G(cid:101)i ⊇ G
i
for every i, ensuring that Equation (RF) is satisfied.
Small variation of spectral embeddings on ego-graphs. We start from a key observation
that the variation of the spectral embeddings on ego-graphs Z(cid:101)i = U GT if θ(A Gi,X Gi) across all the
nodes i ∈ [n] is small, when p is not too small, for all possible θ. This is formalized as follows.
Assumption 1 (Bounded Variation of Z(cid:101)i). For large enough graph size n and ego-graph size p,
(cid:113)
we assume for any model parameter θ ∈ Θ, RSD(Z(cid:101)) := n1 (cid:80) i∈[n]∥Z(cid:101)i−Z(cid:101)∥ F(cid:14) ∥Z(cid:101)∥
F
< B, i.e., the
relative standard deviation (RSD) of Z(cid:101) is upper-bounded by a constant B > 0 independent of θ, where
Z(cid:101) = n1 (cid:80) i∈[n]Z(cid:101)i is the node-wise average of Z(cid:101)i.
In Figure 2, we plot RSD(Z(cid:101)i) versus p on the Cora dataset (Yang et al., 2016), where we find the
RSD drops vastly when p increases. The intuition behind this phenomenon is that many real-world
graphs (e.g., citation and social networks) are often self-similar, where the spectral representations
of large-enough ego-graphs are close to the full graphs’.
Approximately decompose the ego-graph-wise graph coreset objective Equation (EG-
GC) in spectral domain. We can re-write Equation (RF) in the spectral domain of each
ego-graph as
ℓ([f θ(A Gi,X Gi)] 1,:, y i) = ℓ([U Gi] 1,:Z(cid:101)i,y i), (SRF)
since [f (A ,X )] = [U UT f (A ,X )] = [U ] UT
θ Gi Gi 1,: Gi Gi θ Gi Gi 1,: Gi 1,: Gi
f θ(A Gi,X Gi) = [U Gi] 1,:Z(cid:101)i. We now denote v
i
:= [U Gi]T
1,:
∈ Rp (not an eigenvector), ℓ(cid:101)i(Z(cid:101)) =
ℓ(vT iZ(cid:101), y i), and L(cid:101)(Z(cid:101)) = n1
t
(cid:80) i∈[nt]ℓ(cid:101)i(Z(cid:101)). Since by Section 3, we assume Z(cid:101)i ≈ Z(cid:101) for all θ ∈ Θ and
i ∈ [n], we propose to approximately achieve the goal of ego-graph-wise coreset (Equation (EG-GC))
by: (1) finding the subset of labeled nodes to approximate the average spectral embedding,
wm a∈in Wm θ∈a Θx(cid:13) (cid:13) (cid:88) w ia·Z(cid:101)i−Z(cid:101) (cid:13) (cid:13) F, (NAC)
i∈[nt]
6which we call node-wise average coresets; and (2) finding the subset of labeled nodes to
approximate the node-classification loss,
min
max(cid:12)
(cid:12)
(cid:88)
w
ic·ℓ(cid:101)i(Z(cid:101))−L(cid:101)(Z(cid:101))(cid:12)
(cid:12), (LCC)
wc∈W Z(cid:101)
i∈[nt]
where now the average spectral embedding Z(cid:101) is treated as an unknown parameter. Since Z(cid:101) is the
output embedding and ℓ(cid:101)i(Z(cid:101)) = ℓ(vT iZ(cid:101), y i) is a linear classification loss, Equation (LCC) is the linear
classification coresets. Although the optimal sample weights wa and wc (where the superscript a
stands for average and c stands for classification) are different, we further require the corresponding
subsets of nodes coincide, i.e., V = V , and this is realized by the combined coreset algorithm
wa wc
(see Algorithm 1). Moreover, given Section 3, if we can upper-bound the errors in Equations (NAC)
and (LCC) through the combined coreset algorithm, we can show the approximation error on the
node classification loss is also upper-bounded (see Theorem 2).
The remaining of this section analyzes how to solve the two coreset selection problems one by
one, while we defer the combined greedy algorithm and theoretical guarantees to Section 4.
3.1 Graph Node-wise Average Coresets
Solving node-wise average coresets (Equation (NAC)) approximately without evaluating
the spectral embeddings. For the node-wise average coresets (Equation (NAC)), since the
evaluation of a single spectral embedding Z(cid:101)i is expensive, we ask: is it possible to find the coresets
approximatelywithoutevaluatinganyZ(cid:101)i? Surprisingly,thisispossiblebecausethespectralembedding
Z(cid:101)i is a “smooth” function of nodes i on the graph. Here, “smothness” refers to the phenomena that
Z(cid:101)i (as a function of node i) varies little across edges, i.e., Z(cid:101)i ≈ Z(cid:101)j if A
i,j
= 1. The intuition behind
this is simple: ego-graphs of connected nodes have a large overlap G(cid:101)i∩G(cid:101)j, and thus the resulted
output embedding is similar no matter what parameter θ is used.
Spectral characterization of smoothness. The spectral transformation can again be
used to characterize the degree of smoothness since the eigenvalue λ represents the smoothness of
i
eigenvector u i. For an entry of the spectral embedding [Z(cid:101)i] a,b, we can construct an n-dimensional
vector (cid:101)z(a,b) = (cid:2) [Z(cid:101)1] a,b,...,[Z(cid:101)n] a,b(cid:3)
∈ Rn by collecting the corresponding entry of the spectral embedding of each node. Then, we want
to show the inner product ⟨z(a,b),u ⟩ is larger for smaller eigenvalue λ . Actually, this can be done by
(cid:101) i i
first considering the spectral representation of the inputs, i.e., X(cid:101)i = U GT iX Gi, where we can similarly
define x (cid:101)(a,b) = (cid:2) [X(cid:101)1] a,b,...,[X(cid:101)n] a,b(cid:3) and show that if the node features are i.i.d. unit Gaussians,
then in expectation ⟨x(a,b),u ⟩ ∝ (1− 1λ )L; see Lemma 3 in Appendix A. Second, we note that the
(cid:101) i 2 i
spectral behavior of message-passing GNNs f (A,X) (Equation (GNN)) is completely characterized
θ
by its convolution matrix (Balcilar et al., 2021); see Figure 4 for practical observations. Based on
this, we can show the corresponding GNN function in the spectral domain f(cid:101)θ(·) = UTf θ(A,U·) is
Lipschitz continuous if all of the linear weights W(l) in Equation (GNN) have bounded operator
norms (see Lemma 4 in Appendix A). Based on these results, we can formally characterize the
smoothness of spectral embeddings (see Proposition 5 in Appendix A).
Upper-bound on the node-wise average error. Following the work in (Linderman and
Steinerberger, 2020; Vahidian et al., 2020), and based on Proposition 5, we can obtain an upper-
b inou An pd peo nn dit xhe An ),o wde h- ew reise wa av =er (cid:80)age erro wr a∥ δ(cid:80)
∈i∈ R[n nt
,]w δia i· sZ(cid:101)
ti
h−
e
uZ(cid:101) n∥
itF
v≤ ectM or· w∥ hP ow sea i− -thn11 en∥ tr( yse ie sT onh ee ,o are nm
d
11
i∈[nt] i i i
7is the vector of ones. We then propose to optimize the upper-bound ∥Pwa− 11∥ which does not
n
depend on Z(cid:101)i, enabling us to approximately solve the node-wise average coreset without evaluating
a single Z(cid:101)i. (Vahidian et al., 2020) propose to optimize ∥Pwa− n11∥ using a variant of the greedy
geodesic iterative ascent (GIGA) algorithm (Campbell and Broderick, 2018), and we follow their
approach (see Section 4 for details).
Theorem 1 (Upper-bound on the Error Approximating Node-wise Average). Under all
assumptions of Proposition 5, we have ∥(cid:80) i∈[nt]w ia·Z(cid:101)i−Z(cid:101)∥
F
≤ M ·∥Pwa− n11∥ for some constant
M > 0.
Proof. The proof mostly follows from the proof of Theorem 1 in (Linderman and Steinerberger,
2020). We begin with decomposing an arbitrary (a,b)-th entry of (cid:80) i∈[nt]w ia·Z(cid:101)i−Z(cid:101) in the spectral
domain.
(cid:80) wa·z(a,b) − 1 (cid:80) z(a,b)
i∈[nt] i (cid:101)i n j∈[n](cid:101)j
=(cid:80) ((cid:80) waδ − 1)z(a,b)
j∈[n] i∈[nt] i i n (cid:101)j
=(cid:80) (cid:10)(cid:80) waδ − 1,u (cid:11) ⟨u ,z(a,b)⟩
k∈[n] i∈[nt] i i n k k
≤M′·(cid:80) (cid:10)(cid:80) waδ − 1,u (cid:11) (1− 1λ )L.
k∈[n] i∈[nt] i i n k 2 k
Thus we have,
(cid:12) (cid:12)(cid:80) i∈[nt]w ia· (cid:101)z( ia,b) − n1 (cid:80) j∈[n](cid:101)z( ja,b)(cid:12) (cid:12)2
≤M′2·(cid:80) k∈[n](cid:12) (cid:12)(cid:10)(cid:80) i∈[nt]w iaδ i− n1,u k(cid:11)(cid:12) (cid:12)2 (1− 21λ k)2L
=M′2(cid:13) (cid:13)(cid:80) k∈[n](1− 1 2λ k)L(cid:10)(cid:80) i∈[nt]w iaδ i− n1,u k(cid:11) u k(cid:13) (cid:13)2
=M′2·(cid:13) (cid:13)PL((cid:80) i∈[nt]w iaδ i− n1)(cid:13) (cid:13)
=M′2·∥Pwa− 11∥2.
n
Thus we conclude,
(cid:88) 1
∥ w ia·Z(cid:101)i−Z(cid:101)∥
F
≤ M ·∥Pwa− n1∥,
i∈[nt]
where M = M′ ·pd. Here p is the (diffusion) ego-graph size and d is the number of features per
node.
3.2 Spectral Linear Classification Coresets
There are more available approaches to solve the linear classification coreset Equation (LCC), and
we adopt the submodular maximization formulation in (Mirzasoleiman et al., 2020).
Submodular maximization formulation of linear classification coreset. Follow-
ing (Mirzasoleiman et al., 2020), we can show the approximation error in Equation (LCC) can
be upper-bounded by a set function H(·), i.e., |(cid:80) i∈[nt]w ic · ℓ(cid:101)i(Z(cid:101)) − L(cid:101)(Z(cid:101))| ≤ H(V wc), where
(cid:80)
H(V wc) := i∈[nt]min
j∈V wc
max Z(cid:101)|ℓ i(Z(cid:101))−ℓ j(Z(cid:101))| (see Lemma 7 in Appendix A). Then, by introduc-
ing an auxiliary node {i }, we can define a submodular function F(V) := H({i })−H(V ∪{i }),
0 0 0
and formulate the coreset selection as a submodular set-cover problem. Due to the efficiency
constraints, (Mirzasoleiman et al., 2020) propose to solve the submodular maximization problem
instead, max F(V ), which is dual to the original submodular cover formulation. We follow
wc∈W wc
8this approach and adopt their CRAIG (CoResets for Accelerating Incremental Gradient descent) al-
gorithm for the linear classification coreset. It is worth noting that, although the CRAIG formulation
discussedabovecanbeusedtosolvetheoriginalego-graph-wisecoresetproblem(Equation(EG-GC))
directly, it suffers from a much larger complexity as we have to forward- and backward-pass through
the GNN all the time, and evaluate all ego-graph embeddings explicitly.
4 Algorithm and Theoretical Analysis
The spectral greedy graph coresets (SGGC) algorithm. We now describe how we combine
the two greedy algorithms, GIGA and CRIAG, to achieve both objectives respectively, with an
extra constraint that they find the same subset of nodes, i.e., V = V . The idea is to incorporate
wa wc
the submodular cost F(V ) = F(V ) into the SCGIGA’s objective. Through the introduction
wc wa
of a hyperparameter 0 < κ < 1, we change the objective of the node-wise average coreset to be
∥Pwa− 11∥−κF(V ). Now, the submodular cost F(V ) can be understood as a selection cost,
n wa wa
and the new objective can be solved by a relaxation on the GIGA algorithm, which is called SCGIGA
as discussed in (Vahidian et al., 2020). The complete pseudo-code is shown below (see Appendix A
for more details).
Algorithm 1: Spectral greedy graph coresets (SGGC).
Input: Diffusion matrix P = 1I + 1D−1A, coreset size c, hyperparameter 0 < κ < 1.
2 n 2
1 Initialize weights w 0a ← 0, w 0c ← 0;
2 for t = 0,...,c−1 do
3 Compute P(w ta) = (cid:80) i∈[nt][w ta] i∥PP: :, ,i i∥;
4 Compute a t ← ∥1 1− −⟨ ⟨1 1, ,P P( (w wt ta a) )⟩ ⟩P P( (w wt ta a) )∥, and for each i ∈ [n t], bi t ← ∥P Pi i− −⟨ ⟨P Pi i, ,P P( (w wt ta a) )⟩ ⟩P P( (w wt ta a) )∥;
5 Find subset V t = {i ∈ [n t] | ⟨a t, bi t⟩ ≥ κ·max j∈[nt]⟨a t, bj t⟩};
6 Select node i∗ = argmax i∈VtF({i}∪V wa)−F(V wa);
1 1
7 Compute ζ 0 = ⟨√ n,P i∗⟩,ζ 1 = ⟨√ n,P (w t)⟩,ζ 2 = ⟨P i∗,P (w t)⟩, and
η ← ζ0−ζ1ζ2 ;
t (ζ0−ζ1ζ2)+(ζ1−ζ0ζ2)
8 Update weights w ta +1 ← ∥(1( −1 η− tη )Pt) (w wta ta+ )+ηt ηδ ti P∗ i∗∥;
9 Compute [wa] i ← n∥P:,i∥∥(cid:80) j∈1 [nt][w ca]jP:,j∥[w ca] i ∀i ∈ [n t];
10 Compute wc = (cid:80) j∈[nt]1(cid:8) i = argmin k∈V wa max Z(cid:101)|ℓ(cid:101)j(Z(cid:101))−ℓ(cid:101)k(Z(cid:101))|(cid:9) ;
11 Combine w i ← w ia·w ic for each i ∈ [n t], and normalize w ← w/∥w∥ 1;
12 return coreset V w, weights w
Theoretical guarantees of SGGC. Based on the correctness theorems of SCGIGA and
CRAIG, and Section 3, we can prove the following error-bound on the node-classification loss, which
shows SGGC approximately solves the graph coresets problem (Equation (GC)) (see Appendix A).
Theorem 2 (Error-Bound on Node Classification Loss). If both Equation (N-GC) and Equa-
ti (cid:0)on (LCC) have boun (cid:1)ded erro (cid:12)rs and Section 3 holds, then we have, max θ∈Θ(cid:12) (cid:12)(cid:80) i∈[nt]w iaw ic·
ℓ [f θ(A Gi,X Gi)] 1,:, y
i
−L(θ)(cid:12) < ϵ, where ϵ does not depend on the coreset size c and the number of
training nodes n .
t
9Proof. thm:error-gnn-loss First, using Algorithm 1 and by the Theorem 2 of Vahidian et al. (2020),
(cid:113)
we have ∥Pwa− 11∥ ≤ 1 −(κ)2·O((1−κ2ϵ2)c/2) for some ϵ > 0 and where c = |V | is the size of
n n n w
selected coreset. By Theorem 1, we have the upper-bound on the objective of the node-wise average
coreset (Equation (N-GC)), ∥(cid:80) i∈[nt]w ia·Z(cid:101)i−Z(cid:101)∥
F
< Ma for some Ma > 0. Then, by Theorem 1
of (Vahidian et al., 2020), we know F(V w) = H({i 0})−H(V w∪{i 0}) ≥ O(1√− nκ). Thus by Lemma 7,
we have an upper-bound on, max Z(cid:101)|ℓ i(Z(cid:101))−ℓ j(Z(cid:101))| ≤ Mc for some Mc > 0 for any i ∈ V
w
and j ∈ [n].
Note the wa and wc above is the output of Algorithm 1 and we have V = V = V .
wa wc w
Second, by repeatedly using the second bound above, we can get |(cid:80) i∈[nt]w iaw ic · ℓ(cid:101)i(Z(cid:101)i) −
(cid:80) j∈[nt](cid:80) k∈[n]w jaℓ(cid:101)k(Z(cid:101)j)| ≤ Mc, where ζ : [n] → V
w
is the mapping described in Lemma 7. While,
by the first bound above, Section 3, the Jensen-bound and the Lipschitzness of ℓ(cid:101)k (the Lipschitz
coefficient is an absolute constant), we can derive, |(cid:80) i∈[nt]w iaℓ(cid:101)k(Z(cid:101)i)−ℓ(cid:101)k(Z(cid:101)k)| ≤ |(cid:80) i∈[nt]w iaℓ(cid:101)k(Z(cid:101)i)−
ℓ(cid:101)k(Z(cid:101))| + |ℓ(cid:101)k(Z(cid:101)) − ℓ(cid:101)k(Z(cid:101)k)| ≤ O(Ma + B∥Z(cid:101)∥). Combining the two inequalities, we conclude the
proof.
5 Related Work
Table 1: SGGC is better than other model-agnostic/based coresets, graph coarsening, and comparable to
graph condensation. We train 2-layer GCNs on the coreset/coarsened/condensed graphs and report the test
accuracy. OOT and OOM refer to out-of-time/memory.
Model-AgnosticCoresets Model-BasedCoresets GraphReduction Ours DataCondense Oracle
Dataset Ratio
Uniform Herding K-Center Forgetting Cal CRAIG Glister GraNd GradMatch Corasening SGGC GCond FullGraph
15% 67.7±4.5 66.1±1.2 64.3±4.8 65.4±3.1 71.6±1.0 68.4±4.4 65.6±5.6 71.9±1.7 72.0±1.3 — 72.9±0.6 —
Cora 25% 71.8±4.2 69.9±1.0 72.6±2.5 72.6±3.5 75.3±1.5 74.4±1.7 74.3±2.4 74.4±1.5 74.7±2.3 31.2±0.2 78.6±1.0 79.8±1.3 81.2±0.4
50% 78.3±2.2 70.8±0.4 78.9±1.0 76.1±1.1 80.7±0.5 78.2±2.0 78.3±2.0 79.3±0.8 80.2±0.5 65.2±0.6 80.2±0.8 80.1±0.6
15% 53.6±7.9 46.1±1.6 47.5±6.3 51.5±4.9 53.2±2.0 55.4±6.7 54.0±5.0 57.0±3.9 58.8±3.9 — 63.7±3.1 —
Citeseer 25% 61.7±3.2 54.9±3.9 61.6±4.0 55.3±5.5 56.1±2.8 59.5±4.3 62.0±5.5 64.4±1.5 66.0±1.5 52.2±0.4 67.2±2.4 70.5±1.2 70.6±0.9
50% 66.9±1.7 68.7±0.5 65.6±1.6 67.6±0.8 68.2±0.8 67.9±2.2 67.9±1.4 70.5±0.8 70.7±0.5 59.0±0.5 68.4±0.9 70.6±0.9
15% 65.7±4.5 61.9±1.0 69.0±4.8 65.4±4.9 71.7±0.8 73.2±4.1 65.6±5.5 62.0±1.0 65.3±4.5 — 72.5±1.5 —
Pubmed 25% 71.1±1.8 65.9±0.4 73.3±2.6 69.0±2.5 74.7±1.7 71.0±3.3 71.5±3.2 70.6±2.3 71.1±1.4 — 75.8±1.6 — 79.3±0.6
50% 75.3±1.1 72.2±0.6 77.8±1.3 72.5±2.1 77.3±1.1 74.4±1.7 75.1±2.0 76.0±1.2 74.8±1.1 — 76.5±0.6 —
0.2% 47.1±1.6 45.5±1.3 46.9±0.9 46.0±1.2 OOT OOT 48.0±1.0 48.1±0.4 48.2±0.2 41.9±0.2 48.4±0.8 46.5±0.4
Flickr 1.0% 48.4±1.1 46.7±0.3 47.5±0.9 47.7±2.4 OOT OOT 48.5±0.7 48.8±0.5 48.7±0.6 44.5±0.1 49.0±0.6 47.1±0.1 49.1±0.7
2.0% 47.0±1.1 45.5±0.6 46.9±0.7 46.6±1.6 OOT OOT 47.4±0.9 OOM 48.5±0.6 — 64.4±0.4 —
0.5% 58.4±1.5 45.7±4.4 56.8±2.8 55.5±2.4 OOT OOT 57.2±2.1 OOM 53.4±1.9 43.5±0.2 59.7±1.5 63.2±0.3
ogbn-arxiv 1.0% 62.0±0.9 47.6±0.4 60.7±0.8 60.4±1.9 OOT OOT 62.2±1.3 OOM 56.5±1.7 50.4±0.1 62.5±0.9 64.0±0.4 70.9±0.2
2.0% 64.7±0.5 56.5±0.5 62.4±0.9 62.8±2.4 OOT OOT 64.2±1.1 OOM 58.6±1.0 — 64.4±0.4 —
0.05% 46.8±1.2 31.9±0.5 35.9±1.9 32.9±4.8 OOT OOT OOM OOM OOM — 46.3±4.1 —
ogbn-products 75.6±0.2
0.15% 53.0±1.0 36.5±0.3 47.6±0.8 42.0±3.7 OOT OOT OOM OOM OOM — 53.6±1.2 —
0.1% 27.4±4.6 18.5±3.5 22.5±4.5 26.4±1.0 OOT OOT OOM OOM 19.4±3.5 — 38.4±3.4 —
Reddit 92.2±0.6
0.2% 40.7±7.2 17.0±4.0 20.0±3.1 39.7±3.5 OOT OOT OOM OOM 18.3±3.0 — 48.6±4.6 —
In this section, we review general coreset methods, graph coresets, and other graph reduction
methods, as well as graph condensation that adapts dataset condensation to graph (see Appendix C).
Earlycoresetselectionmethodsconsiderunsupervisedlearningproblems,e.g.,clustering. Coreset
selection methods choose samples that are important for training based on certain heuristic criteria.
They are usually model-agnostic; for example, Herding coreset (Welling, 2009) selects the closest
samples to the cluster centers. K-center coreset (Farahani and Hekmatfar, 2009) picks multiple
center points such that the largest distance between a data point and its nearest center is minimized.
In recent years, more coreset methods consider the supervised learning setup and propose many
10model-based heuristic criteria, such as maximizing the diversity of selected samples in the gradient
space (Aljundi et al., 2019), discovering cluster centers of model embedding (Sener and Savarese,
2018), and choosing samples with the largest negative implicit gradient (Borsos et al., 2020).
Graph coreset selection is a non-trivial generalization of the above-mentioned coreset methods
given the interdependent nature of graph nodes. The very few off-the-shelf graph coreset algorithms
are designed for graph clustering (Baker et al., 2020; Braverman et al., 2021) and are not optimal
for the training of GNNs.
Graph sparsification(Batsonetal.,2013;Satulurietal.,2011)andgraph coarsening(Loukas
and Vandergheynst, 2018; Loukas, 2019; Huang et al., 2021; Cai et al., 2020) algorithms are usually
designed to preserve specific graph properties like graph spectrum and graph clustering. Such
objectives often need to be aligned with the optimization of downstream GNNs and are shown to be
sub-optimal in preserving the information to train GNNs well (Jin et al., 2021).
Graph condensation (Jin et al., 2021) adopts the recent dataset condensation approach
which synthesizes informative samples rather than selecting from given ones. Although graph
condensation achieves the state-of-the-art performance for preserving GNNs’ performance on the
simplified graph, it suffers from two severe issues: (1) extremely long condensation training time;
and (2) poor generalizability across GNN architectures. Subsequent work aims to apply a more
efficient distribution-matching algorithm (Zhao and Bilen, 2021b; Wang et al., 2022) of dataset
condensation to graph (Liu et al., 2022) or speed up gradient-matching graph condensation by
reducing the number of gradient-matching-steps (Jin et al., 2022). While the efficiency issue of graph
condensation is mitigated, the performance degradation on medium- and large-sized graphs (Jin
et al., 2021) still renders graph condensation practically meaningless.
6 Experiments
In this section, we demonstrate the effectiveness and advantages of SGGC, together with some
important proof-of-concept experiments and ablation studies that verify our design. We also show
the efficiency, architecture-generalizability, and robustness of SGGC. We define the coreset ratio as
c/n 1, where c is the size of coreset, and n is the number of training nodes in the original graph. We
t t
train 2-layer GNNs with 256 hidden units and repeat every experiment 10 times. See Appendix D
for implementation details and Appendix E for more results and ablation studies on more datasets.
SGGC is better than other model-agnostic or model-based coresets and graph
coarsening. Now, we demonstrate the effectiveness of SGGC in terms of the test performance
(evaluated on the original graph) of GNNs trained on the coreset graph on seven node classification
benchmarks with multiple coreset ratios c/n . Table 1 presents the full results, where SGGC
t
consistently achieves better performance than the other coreset methods and the graph coarsening
approach. Although graph condensation treats the condensed adjacency A and node features X as
w w
free learnable parameters (have less constraint than coreset methods), the performance is comparable
to or even lower than SGGC on Cora and Flickr. The advantages of SGGC are often more significant
when the coreset ratio is small (e.g., on Citeseer with a 15% ratio), indicating that SGGC is capable
of extrapolating on the graph and finding informative ego-graphs when the budget is very limited.
Apart from the three small graphs (Cora, Citeseer, and Pubmed), we also consider two mid-scaled
graphs (Flickr and ogbn-arxiv), a large-scale graph (ogbn-products) with more than two million
1SomepaperlikeJinetal.(2021)definesthisratioasc/n,whichcouldbesmallevenifwekeepalltraining/labeled
nodes, i.e., c=n (e.g., on Cora and Citeseer) and is often misleading.
t
11Table 2: Selecting diffusion ego-graphs largely outperforms node-wise selection and achieves comparable
performance to selecting standard ego-graphs with much smaller ego-graph sizes.
Dataset Selection Model-Agnostic Model-Based Ablation Baselines Ours Oracle
Ratio Strategy Uniform K-Center CRAIG Glister CRAIG-Linear SCGIGA SGGC Full Graph
Node 63.3±2.7 67.7±2.7 64.6±4.2 61.9±5.5 64.1±4.0 63.0±2.0 70.3±1.2
Cora
Std. Ego 74.3±2.4 72.7±3.9 74.5±3.3 73.7±1.9 73.0±3.4 75.9±1.5 77.5±0.9 81.2±0.4
25%
Diff. Ego 73.7±1.1 72.6±2.2 72.0±3.3 74.0±2.3 72.7±3.1 76.7±1.9 76.8±1.0
Node 58.1±3.0 52.0±3.3 58.2±3.9 55.1±3.0 57.2±3.0 55.1±2.8 60.8±1.7
Citeseer
Std. Ego 61.8±4.8 56.8±4.4 60.0±5.6 59.7±5.9 61.7±5.8 53.4±1.8 67.1±1.5 70.6±0.9
25%
Diff. Ego 61.7±3.2 61.6±4.0 59.5±4.3 61.9±5.5 59.5±3.8 54.6±2.7 67.2±2.4
Table 3: The complete SGGC algorithm is better than the node-wise average coreset (SCGIGA) and the
linear classification coreset (CRAIG-Linear) individually.
Dataset Cora Citeseer Pubmed
Ratio 25% 50% 25% 50% 25% 50%
Full Graph 81.2±0.4 70.6±0.9 79.3±0.6
CRAIG-Linear 72.7±2.9 78.1±1.1 59.5±3.8 66.6±1.9 71.6±3.8 75.4±2.3
SCGIGA 76.7±1.4 78.3±1.0 54.6±2.8 66.9±1.1 69.8±0.7 74.1±0.4
SGGC (Ours) 78.6±1.0 80.2±1.1 67.2±2.4 68.4±0.9 75.8±1.6 76.5±0.6
nodes, and a much denser graph (Reddit) whose average node degree is around 50. In Table 1, we see
that when scaling to larger and denser graphs, many model-based coreset methods, graph coarsening,
and graph condensation are facing severe efficiency issues. SGGC can run on ogbn-product with
a coreset ratio c/n = 0.05% within 43 minutes, while all the other model-based coresets (except
t
Forgetting), graph coarsening, and graph condensation run out of time/memory.
Figure5: Testaccuracyversustheselecteddatasizeof Figure 6: SGGC is more robust than SCGIGA on
selectingnodesanddiffusionego-graphswith/without low-homophily graphs. We select the coresets on the
PCA-based compression of node attributes. edge-added graph with lower homophily, but train
and test GCNs on the original graph.
12Selecting diffusion ego-graphs is better than selecting nodes and standard ego-graphs.
We also verify that selecting diffusion ego-graphs is advantageous to selecting nodes. In Figure 5, we
show that we can compress the diffusion ego-graphs to achieve data size comparable with node-wise
selection without noticeably lowering the performance. Ego-graph compression is based on the
principal component analysis (PCA), and we compress the node features more when they are far
away from the selected center nodes (see Appendix A). In Table 2, we compare the performance
of various coreset methods with the three selection strategies, including selecting the standard
ego-graphs or diffusion ego-graphs. Not surprisingly, we see ego-graph selection strategies largely
outperform node-wise selection (the largest gap is around 8%). Although selecting standard and
diffusion ego-graphs often lead to similar performance, we note that, by selecting diffusion ego-graphs,
we can achieve comparable performance with ego-graph-size p =8 or 16 on most datasets, which is
much smaller than the average size of standard ego-graphs for L = 2, e.g., around 36 on Cora.
Ablation study: the two-stage SGGC algorithm is better than each stage individu-
ally. It is important to verify that the combined coreset objective is better than the node-wise
average coreset (Equation (N-GC), implemented as SCGIGA (Vahidian et al., 2020) with zero
selection cost) and the linear classification coreset (Equation (LCC), implemented as the CRAIG
algorithm (Mirzasoleiman et al., 2020) with a linear model, denoted by CRAIG-Linear) individually
(see Appendix D for details). In Table 3, we see SGGC is consistently better than (1) CRAIG-Linear
(outperformed by 3.8% on average), which over-simplifies GNNs to a linear classifier and completely
ignores the graph adjacency and (2) SCGIGA (outperformed by 4.4% on average), which relies on
a possibly wrong assumption that the node-wise classification loss is a “smooth” function of nodes
over the graph. Moreover, we find SGGC is more robust than SCGIGA, against the variations
of homophily in the underlying graph (as shown in Figure 6), where the homophily is defined as
h(A,y) = 1 (cid:80) 1{y = y } (Ma et al., 2021) (i.e., how likely the two end nodes of an edge
|E| (i,j)∈E j k
are in the same class). SCGIGA’s performance is greatly degraded on low-homophily graphs because
it assumes the node-wise classification loss to be a “smooth” function of nodes over the graph. When
we decrease the graph homophily by randomly adding edges to Cora, this assumption cannot be
guaranteed. Our SGGC does not suffer from this issue because the spectral embedding of ego-graphs
is always a “smooth” function over graph (see Section 3).
SGGC generalizes better than graph condensation and is more efficient. Finally, we
compare SGGC with graph condensation (Jin et al., 2021) in terms of the generalizability across
GNN architectures and running time. GCond is model-dependent and generalizes poorly across
architectures. In Table 4, we see the performance of graph condensation heavily relies on the GNN
architecture used during condensation, while our SGGC is model-agnostic and generalizes well to
various types of GNNs. Although the best performance of graph condensation is comparable to
SGGC in Table 4, if we do not tune the architecture for condensation, it is much lower on average.
Specifically, when using SGC for condensation, the test performance of GCond is comparable to
SGGC’s. However, when using other architectures, including GCN and SAGE during condensation,
the test accuracy of GCond drops for at least 2% in all settings. In terms of running time, apart
from the fact that GCond cannot scale to large graphs like ogbn-product, it is much slower than
SGGC. On ogbn-arxiv with the coreset ratio c/n = 0.05%, graph condensation runs for 494s while
t
SGGC only requires 133s.
13Table 4: SGGC generalizes better across GNN architectures than graph condensation (GCond) on Cora with
a 50% ratio.
Architecture
Method used during GCN SAGE SGC
Compression
GCN 70.6±3.7 60.2±1.9 68.7±5.4
GCond SAGE 77.0±0.7 76.1±0.7 77.7±1.8
SGC 80.1±0.6 78.2±0.9 79.3±0.7
SGGC (Ours) N/A 80.2±1.1 79.1±0.7 78.5±1.0
7 Conclusions
This paper proposes spectral greedy graph coreset (SGGC), a coreset selection method on graph
for graph neural networks (GNNs), and node classification. For the theoretical limitations, we
note that the small variation assumption of spectral embeddings on ego graphs may not hold for
non-message-passing GNNs and very dense graphs. For the practical limitations, we address the
problem that although SGGC is practically very efficient, similar to most of the coreset algorithms,
it has a O(cn n) time complexity. This hinders us from applying a large coreset ratio on very large
t
graphs, which consequently bottlenecks the downstream GNN performance on the coreset graph.
Future work may consider more efficient setups, e.g., online coreset selection and training on graphs
with hundreds of millions of nodes. Considering broader impacts, we view our work mainly as a
methodological contribution, which paves the way for more resource-efficient graph representation
learning. Our innovations can enable more scalable ways to do large-network analysis for social good.
However, progress in graph learning might also trigger other hostile social network analyses, e.g.,
extracting fine-grained user interactions for social tracking.
Acknowledgement
Ding and Huang are supported by DARPA Transfer from Imprecise and Abstract Models to
Autonomous Technologies (TIAMAT) 80321, National Science Foundation NSF-IIS-2147276 FAI,
DOD-ONR-OfficeofNavalResearchunderawardnumberN00014-22-1-2335, DOD-AFOSR-AirForce
OfficeofScientificResearchunderawardnumberFA9550-23-1-0048,DOD-DARPA-DefenseAdvanced
Research Projects Agency Guaranteeing AI Robustness against Deception (GARD) HR00112020007,
Adobe, Capital One and JP Morgan faculty fellowships.
14A Proofs and More Theoretical Discussions
In this section, we present the proofs and the complete lemmas or theorems for the theoretical
results mentioned in the main paper. We divide this section into three parts; each corresponds to a
section/subsection of the paper. We also describe the PCA compression algorithm to the ego-graph’s
node features at last.
A.1 Proofs for Section 3.1
We start from the theoretical results in Section 3.1. For this subsection, our goal is to show that
the spectral embeddings of ego-graphs, as a function of the center node, is a “smooth” function on
the graph, in the sense that when transformed into the graph spectral domain, their high-frequency
components is relatively small.
The above-mentioned “smoothness” characterization of ego-graph’s spectral embeddings is pre-
sented as Proposition 5 in the main paper. In order to prove Proposition 5, as analyzed in Section 3,
we first need to prove the spectral representation of ego-graph’s input features is “smooth” (Lemma 3)
under some assumptions, and then show the GNNs we considered (including GCN (Balcilar et al.,
2021)) are Lipschitz continuous in the spectral domain (Lemma 4, as a corollary of the Theorem 4
in (Balcilar et al., 2021)).
Lemma 3 (Smoothness of the Spectral Representation of Ego-graph’s Input Features).
For an arbitrary entry (a-th row, b-th column) of the spectral representations of the ego-graphs’
features, X(cid:101)i = U G⊤ iX Gi, we denote x (cid:101)(a,b) = (cid:2) [X(cid:101)1] a,b,...,[X(cid:101)n] a,b(cid:3) . If all node features, i.e., all entries
of X are i.i.d. unit Gaussians, then E[⟨x(a,b),u ⟩] = (1− 1λ )L.
(cid:101) i 2 i
Proof. For simplicity and without loss of generality, we consider there is only one feature per node,
i.e., X ∈ Rn is a column vector. Thus X(cid:101)i ∈ Rn is also a column vector. We consider the k-th entry
of the spectral representation.
We have,
(cid:88)
[U⊤X ] = [U⊤X ] = [U⊤] X
Gi Gi k Gi Gi k Gi k,j j
j∈VL
i
(cid:88)
= [U⊤] X [PL]
k,j j j,i
j∈[n]
1
= [U⊤diag(X ,...,X )U(I − Λ)LU⊤] .
1 n k,i
2
Due to E[U⊤diag(X ,...,X )U] = I. Thus we have E[x(k)] ≈ [(I − 1Λ)LU⊤] , and E[⟨x(k),u ⟩] =
1 n (cid:101) 2 k,: (cid:101) i
(1− 1λ )L.
2 i
Lemma 4 (Lipschitzness of GCN in Spectral Domain). For an L-layer GCN (Kipf and
Welling, 2016), if all linear weights W(l) have bounded operator norm, then the corresponding GNN
function in the spectral domain f(cid:101)θ(·) = UTf θ(A,U·) is Lipschitz continuous.
Proof. For simplicity and without loss of generality, we consider L = 1. Since W(1) has bounded
operator norm, and the non-linear activation function σ(·) is Lipschitz continuous, we only need
15to show the convolution matrix in the spectral domain, i.e., C(cid:101) = U⊤CU has bounded operator
norm. This directly follows from the Theorem 4 in (Balcilar et al., 2021), where they show
∥U⊤CUu ∥ ≈ [1−d¯/(d¯+1)λ ].
i i
Now we prove Proposition 5 as a corollary of Lemmas 3 and 4.
Proposition 5 (Smoothness of Spectral Embeddings). Assuming the node features X are i.i.d.
Gaussians, and the L-layer GNN (Equation (GNN)) have operator-norm bounded linear weights W(l),
then the spectral embedding Z(cid:101)i is smooth on graph, in the sense that, ⟨ (cid:101)z(a,b),u i⟩ ≤ M ·(1− 1 2λ i)L
for some constant M, where (cid:101)z(a,b) = (cid:2) [Z(cid:101)1] a,b,...,[Z(cid:101)n] a,b(cid:3) and u
i
is the eigenvector corresponding to
eigenvalue λ .
i
Proof of Proposition 5: Given all the assumptions of Lemmas 3 and 4, we have that for any entry,
⟨x (cid:101)(a,b),u i⟩ = (1 − 21λ i)L in expectation, and ∥Z(cid:101)i − Z(cid:101)j∥ = ∥UTf θ(A,UX(cid:101)i) − UTf θ(A,UX(cid:101)j)∥ ≤
M ·∥X(cid:101)i−X(cid:101)j∥ for i,j ∈ [n]. This directly leads to that ⟨ (cid:101)z(a,b),u i⟩ ≤ M ·⟨x (cid:101)(a,b),u i⟩ = M ·(1− 1 2λ i)L.
□
Finally, we show the upper bound on the error of approximating the node-wise average.
Theorem 6 (Upper-bound on the Error Approximating Node-wise Average). Under all
assumptions of Proposition 5, we have ∥(cid:80) i∈[nt]w ia·Z(cid:101)i−Z(cid:101)∥
F
≤ M ·∥Pwa− n11∥ for some constant
M > 0.
Proof. The proof mostly follows from the proof of Theorem 1 in (Linderman and Steinerberger,
2020). We begin with decomposing an arbitrary (a,b)-th entry of (cid:80) i∈[nt]w ia·Z(cid:101)i−Z(cid:101) in the spectral
domain.
(cid:88) wa·z(a,b)
−
1 (cid:88) z(a,b)
i (cid:101)i n (cid:101)j
i∈[nt] j∈[n]
= (cid:88) ((cid:88) waδ − 1 )z(a,b)
i i n (cid:101)j
j∈[n] i∈[nt]
= (cid:88) (cid:10) (cid:88) waδ − 1 ,u (cid:11) ⟨u ,z(a,b)⟩
i i n k k
k∈[n] i∈[nt]
≤ M′· (cid:88) (cid:10) (cid:88) waδ − 1 ,u (cid:11) (1− 1 λ )L.
i i n k 2 k
k∈[n] i∈[nt]
16Thus we have,
(cid:12) (cid:12) (cid:88) w ia· (cid:101)z( ia,b) − n1 (cid:88) (cid:101)z( ja,b)(cid:12) (cid:12)2
i∈[nt] j∈[n]
≤ M′2· (cid:88) (cid:12) (cid:12)(cid:10) (cid:88) w iaδ i− n1 ,u k(cid:11)(cid:12) (cid:12)2 (1− 1 2λ k)2L
k∈[n] i∈[nt]
= M′2(cid:13) (cid:13) (cid:88) (1− 1 2λ k)L(cid:10) (cid:88) w iaδ i− n1 ,u k(cid:11) u k(cid:13) (cid:13)2
k∈[n] i∈[nt]
= M′2·(cid:13) (cid:13)PL((cid:88) w iaδ i− n1 )(cid:13) (cid:13)
i∈[nt]
1
= M′2·∥Pwa− 1∥2.
n
Thus we conclude ∥(cid:80) i∈[nt]w ia ·Z(cid:101)i −Z(cid:101)∥
F
≤ M ·∥Pwa − n11∥ where M = M′ ·pd. Here p is the
(diffusion) ego-graph size and d is the number of features per node.
A.2 Proofs for Section 3.2
The major theoretical result in Section 3.2 is that the approximation error on the empirical loss of
the (spectral) linear classification problem can be upper-bounded by a set function (i.e., a function
which only depends on the selected set of nodes), which is formally stated as follows.
Lemma 7 (Upper-bound on the Error Approximating Empirical Loss). For the set function
H(·) defined as H(V) := n1 (cid:80) i∈[nt]min
j∈V
max Z(cid:101)|ℓ i(Z(cid:101))−ℓ j(Z(cid:101))|, we have
(cid:88)
min max| w ic·ℓ(cid:101)i(Z(cid:101))−L(cid:101)(Z(cid:101))| ≤ H(V wc).
wc∈W Z(cid:101)
i∈[nt]
Proof. The proof follows from Section 3.1 of (Mirzasoleiman et al., 2020). Consider there is
a mapping ζ : [n] → V and let wc be the number of nodes mapped to i ∈ V divided by n, i.e.,
i
w ic = n1(cid:12) (cid:12){j ∈ [n] | ζj = i}(cid:12) (cid:12). Then,(cid:80) i∈[nt]w ic·ℓ(cid:101)i(Z(cid:101)) = (cid:80) j∈[n]ℓ(cid:101) ζ(i)(Z(cid:101))and|(cid:80) i∈[nt]w ic·ℓ(cid:101)i(Z(cid:101))−L(cid:101)(Z(cid:101))| ≤
n1 (cid:80) j∈[n]|ℓ(cid:101) ζ(i)(Z(cid:101))−ℓ(cid:101)i(Z(cid:101))|. Fromthiswecanreadilyderivethat,min wc∈W|(cid:80) i∈[nt]w ic·ℓ(cid:101)i(Z(cid:101))−L(cid:101)(Z(cid:101))| ≤
n1 (cid:80) i∈[nt]min
j∈V
|ℓ i(Z(cid:101))−ℓ j(Z(cid:101))|, and by taking max
Z(cid:101)
on both sides we conclude the proof.
A.3 Proofs for Section 4
The proof for the error-bound on the node-classification loss (Theorem 2) consists of two parts.
First, we want to show that Algorithm 1 can achieve both objectives (Equations (N-GC) and (LCC))
which follows from the Theorem 2 of (Vahidian et al., 2020), Theorem 1, and Lemma 7. Second, we
show under Section 3 we can bound the approximation error on the node classification loss.
Proof of Theorem 2: First, using Algorithm 1 and by the Theorem 2 of Vahidian et al. (2020), we
(cid:113)
have ∥Pwa− 11∥ ≤ 1 −(κ)2·O((1−κ2ϵ2)c/2) for some ϵ > 0 and where c = |V | is the size of
n n n w
selected coreset. By Theorem 1, we have the upper-bound on the objective of the node-wise average
coreset (Equation (N-GC)), ∥(cid:80) i∈[nt]w ia·Z(cid:101)i−Z(cid:101)∥
F
< Ma for some Ma > 0. Then, by Theorem 1
17of (Vahidian et al., 2020), we know F(V w) = H({i 0})−H(V w∪{i 0}) ≥ O(1√− nκ). Thus by Lemma 7,
we have an upper-bound on, max Z(cid:101)|ℓ i(Z(cid:101))−ℓ j(Z(cid:101))| ≤ Mc for some Mc > 0 for any i ∈ V
w
and j ∈ [n].
Note the wa and wc above is the output of Algorithm 1 and we have V = V = V .
wa wc w
Second, by repeatedly using the second bound above, we can get |(cid:80) i∈[nt]w iaw ic · ℓ(cid:101)i(Z(cid:101)i) −
(cid:80) j∈[nt](cid:80) k∈[n]w jaℓ(cid:101)k(Z(cid:101)j)| ≤ Mc, where ζ : [n] → V
w
is the mapping described in Lemma 7. While,
by the first bound above, Section 3, the Jensen-bound and the Lipschitzness of ℓ(cid:101)k (the Lipschitz
coefficient is an absolute constant), we can derive, |(cid:80) i∈[nt]w iaℓ(cid:101)k(Z(cid:101)i)−ℓ(cid:101)k(Z(cid:101)k)| ≤ |(cid:80) i∈[nt]w iaℓ(cid:101)k(Z(cid:101)i)−
ℓ(cid:101)k(Z(cid:101))|+|ℓ(cid:101)k(Z(cid:101))−ℓ(cid:101)k(Z(cid:101)k)| ≤ O(Ma+B∥Z(cid:101)∥). Combining the two inequalities, we conclude the proof.
□
A.4 Compressing Ego-Graph’s Node Features via PCA
In the main paper, we described the algorithm to find the center nodes V of the ego-graphs. We
w
then find the union of those ego-graphs, which consists of nodes V(L) = (cid:83) VL where VL is the
w i∈Vw i i
set of nodes in the ego-graph centered at node i.
(L)
We then propose to compress the ego-graph’s node features, which consists of |V |d = ξcd ≤
w
c×p×d floating point numbers, to have a comparable size with the node features of the center
nodes, which consists of only c×d floating point numbers. Here c is the size of the coreset, p is the
(L)
diffusion ego-graph size, d is the number of features per node, and ξ = |V |/c ≤ p because there
w
may be overlaps between the ego-graphs. In practice, the overlaps are often large, and we expect
1 < ξ ≪ p.
We first quantize all the nodes’ features to half-precision floating-point numbers, and then the
desired compress ratio for all the non-center nodes is 1/(ξ−1). We then compress the features of
each non-center node j according to its shortest path distance to the nearest center node, denoted by
d (j). Assuming an L-layer GNN and thus L-depth ego-graphs, for 1 ≤ l ≤ L, we find the set of
min
(L)
nodes = {j ∈ V | d (j) = l}, and compress their features by the principal component analysis
w min
(PCA) algorithm (which finds a nearly optimal approximation of a singular value decomposition).
(L,l)
If we keep the q largest eigenvalues, we only need (|V |+d+1)q half-precision floating point
l w
numbers to store the features of V(L,l) , we find q by formula q = cd (1)l, which satisfies
w l l |Vw(L,l)|+d+1 2
the targeted compress ratio
(cid:80)l=L(|V(L,l)
|+d+1)q ≤ cd.
l=1 w l
B Message-Passing GNNs
In this section, we present more results and discussions regarding the common generalized graph
convolution framework.
Notations. Consider a graph with n nodes and m edges. Connectivity is given by the
adjacency matrix A ∈ {0,1}n×n and features are defined on nodes by X ∈ Rn×d with d the length
of feature vectors. Given a matrix C, let C , C , and C denote its (i,j)-th entry, i-th row, j-th
i,j i,: :,j
column, respectively. Besides, we simplify the notion of {1,...,L} to [L]. We use ⊙ to denote the
element-wise (Hadamard) product. ∥·∥ denotes the entry-wise ℓp norm of a vector and ∥·∥
p F
denotes the Frobenius norm. We use I ∈ Rn×n to denote the identity matrix, 1 to denote the
n
vector whose entries are all ones, and δ i to denote the unit vector in Rn whose i-th entry is 1. And (cid:102)
represents concatenation along the last axis. We use superscripts to refer to different copies of the
18same kind of variable. For example, X(l) ∈ Rn×f l denotes node representations on layer l. A Graph
Neural Network (GNN) layer takes the node representation of a previous layer X(l) as input and
produces a new representation X(l+1), where X = X(0) is the input features.
A common framework for generalized graph convolution. GNNs are designed following
different guiding principles, including neighborhood aggregation (GraphSAGE (Hamilton et al.,
2017), PNA (Corso et al., 2020)), spatial convolution (GCN (Kipf and Welling, 2016)), spectral
filtering (ChebNet (Defferrard et al., 2016), CayleyNet (Levie et al., 2018), ARMA (Bianchi et al.,
2021)), self-attention (GAT (Veličković et al., 2018), Graph Transformers (YaronLipman, 2020;
Rong et al., 2020; Zhang et al., 2020)), diffusion (GDC (Klicpera et al., 2019), DCNN (Atwood and
Towsley, 2016)), Weisfeiler-Lehman (WL) alignment (GIN (Xu et al., 2018a), 3WL-GNNs (Morris
et al., 2019; Maron et al., 2019)), or other graph algorithms ((Xu et al., 2020; Loukas, 2019)).
Despite these differences, nearly all GNNs can be interpreted as performing message passing on
node features, followed by feature transformation and an activation function. Now we rewrite this
expression according to one pointed out by (Balcilar et al., 2021) in the form:
(cid:32) (cid:33)
(cid:88)
X(l+1) = σ C(r)X(l)W(l,r) , (1)
r
where C(r) ∈ Rn×n denotes the r-th convolution matrix that defines the message passing operator,
r ∈ Z + denotes the index of convolution, and σ(·) denotes the non-linearity. W(l,r) ∈ Rf l×f l+1 is the
learnable linear weight matrix for the l-th layer and r-th filter.
Within this common framework, GNNs differ from each other by the choice of convolution
matrices C(r), which can be either fixed or learnable. A learnable convolution matrix relies on the
inputs and learnable parameters and can be different in each layer (thus denoted as C(l,r)):
(l,r) (r) (r) (l) (l)
C = C ·h (X ,X ), (2)
i,j i,j θ(l,r) i,: j,:
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
fixed learnable
where C(r) denotes the fixed mask of the r-th learnable convolution, which may depend on the
adjacency matrix A and input edge features E i,j. While h(r)(·,·) : Rf l × Rf l → R can be any
learnable model parametrized by θ(l,r). We re-formulate some popular GNNs into this generalized
graph convolution framework (see Table 5 for more details).
Table 5: Summary of GNNs re-formulated as generalized graph convolution.
ModelName DesignIdea Conv. MatrixType #ofConv. ConvolutionMatrix
GCN1(KipfandWelling,2016) SpatialConv. Fixed 1 C=D(cid:101)−1/2A(cid:101)D(cid:101)−1/2

SAGE-Mean2(Hamiltonetal.,2017) MessagePassing Fixed 2
C(1)=In
C(2)=D−1A

C(r)=A+In and
GAT3(Veličkovićetal.,2018) Self-Attention Learnable #ofheads h(r) (X(l),X(l))=exp(cid:0) LeakyReLU(
 a ((l X,r) i( ,l :)Wi,: (l,r)(cid:102)j,: X j( ,l :)W(l,r))·a(l,r))(cid:1)
1WhereA(cid:101)=A+In,D(cid:101)=D+In. 2C(2)representsmeanaggregator. Weightmatrixin(Hamiltonetal.,2017)isW(l)=W(l,1)(cid:102)W(l,2).
3Needrow-wisenormalization. C i( ,l j,r)isnon-zeroifandonlyifAi,j=1,thusGATfollowsdirect-neighboraggregation.
19Most GNNs can be interpreted as performing message passing on node features, followed by
feature transformation and an activation function, which is known as the common “generalized graph
convolution” framework.
GNNs that cannot be defined as graph convolution. Some GNNs, including Gated
Graph Neural Networks (Li et al., 2015) and ARMA Spectral Convolution Networks (Bianchi et al.,
2021) cannot be re-formulated into this common graph convolution framework because they rely on
either Recurrent Neural Networks (RNNs) or some iterative processes, which are out of the paradigm
of message passing.
C More Related Work
Dataset condensation (or distillation) is first proposed in (Wang et al., 2018) as a learning-to-learn
problem by formulating the network parameters as a function of synthetic data and learning them
through the network parameters to minimize the training loss over the original data. However, the
nested-loop optimization precludes it from scaling up to large-scale in-the-wild datasets. (Zhao et al.,
2020) alleviate this issue by enforcing the gradients of the synthetic samples w.r.t. the network
weights to approach those of the original data, which successfully alleviates the expensive unrolling
of the computational graph. Based on the meta-learning formulation in (Wang et al., 2018), (Bohdal
et al., 2020) and (Nguyen et al., 2020, 2021) propose to simplify the inner-loop optimization of a
classification model by training with ridge regression which has a closed-form solution, while (Such
et al., 2020) model the synthetic data using a generative network. To improve the data efficiency of
synthetic samples in the gradient-matching algorithm, (Zhao and Bilen, 2021a) apply differentiable
Siamese augmentation, and (Kim et al., 2022) introduce efficient synthetic-data parametrization.
Recently, a new distribution-matching framework (Zhao and Bilen, 2021b) proposes to match the
hidden features rather than the gradients for fast optimization but may suffer from performance
degradation compared to gradient-matching (Zhao and Bilen, 2021b), where (Kim et al., 2022)
provide some interpretation.
Recent benchmark (Guo et al., 2022) of model-based coreset methods on image classification
indicates Forgetting and GraNd are among the best-performing ones but still evidently underperform
the dataset condensation approach (see Appendix C). Forgetting (Toneva et al., 2018) measures the
forgetfulness of trained samples and drops those that are not easy to forget. GraNd (Paul et al.,
2021) selects the training samples that contribute most to the training loss in the first few epochs.
Graph sampling methods (Chiang et al., 2019; Zeng et al., 2019) can be as simple as uniformly
sampling a set of nodes and finding their induced subgraph, which is understood as a graph-
counterpart of uniform sampling of i.i.d. samples. However, most of the present graph sampling
algorithms(e.g.,ClusterGCN(Chiangetal.,2019)andGraphSAINT(Zengetal.,2019))aredesigned
for sampling multiple subgraphs (mini-batches), which form a cover of the original graph for training
GNNs with memory constraint. Therefore, those graph mini-batch sampling algorithms are effective
graph partitioning algorithms and not optimized to find just one representative subgraph.
D Implementation Details
PackagesandHardwarespecs. Generally,ourprojectisdevelopeduponthePytorchframework,
and we use Pytorch Geometric (https://pytorch-geometric.readthedocs.io/en/latest/) to
acquire datasets Cora, Citeseer, Pubmed, and Flickr, and utilize Ogb (https://ogb.stanford.
20edu/) to get dataset Arxiv. The coreset methods on the graph are implemented based on Guo
et al. (2022) (https://github.com/patrickzh/deepcore) and our downstream GNN structures,
GCN, GraphSage, and SGC, are implemented based on Jin et al. (2021) (https://github.com/
ChandlerBang/GCond). The experiments are conducted on hardware with Nvidia GeForce RTX 2080
Ti(11GB GPU).
Dataset statistics. We adopt five graph datasets, Cora, Citeseer, Pubmed, Flickr, and Arxiv
in our experiments. Cora, Citeseer, and Pubmed are citation datasets whose node features represent
the most common words in the text, and two nodes, each representing a paper, are connected if
either one cites the other. Flickr is an image network where images connect if they share common
properties, such as similar figures or buildings. While Arxiv is a directed citation network that
contains all computer science papers in Arxiv indexed by MAG2. A directed edge from paper A to
paper B establishes if A cites B. Here is detailed information on the datasets.
Table 6: Statistics of the datasets.
Nodes Edges Features Classes Train (%) Validation (%) Test (%)
Cora 2,708 10,556 1,433 7 5.2 18.5 76.3
Citeseer 3,327 9,104 3,703 6 3.6 15.0 81.4
Pubmed 19,717 88,648 500 3 0.3 2.5 97.2
Flickr 89,250 899,756 500 7 50.0 25.0 25.0
Arxiv 169,343 1,166,243 128 40 53.7 17.6 28.7
From Table 6, we could see that Cora, Citeseer, and Pubmed are network data with nodes of no
more than 20,000 and edges of less than 100,000, which can be deemed as small datasets. While
Flickr (which has 89,250 nodes and 899,756 edges) and Arxiv (which has 169,343 nodes and 1,166,243
directed edges) are much larger datasets, and more than 50% of the nodes are training nodes, so
testing GNN accuracy for model-based coreset methods could be both time and space consuming.
Hyper-parameter setups of SGGC. For SGGC, there are three hyper-parameters: slack
parameter (denoted as κ), max budget (denoted as s), and diffusion ego-graph size. (1) Slack
parameter, denoted by κ, means we could pick the nodes whose alignment score has at least κ
of the highest alignment score of v∗ in Algorithm 1. The κ shows the balance of our algorithm
between SCGIGA and CraigLinear. When κ is 1, we pick nodes in the same way as SCGIGA. When
we select set κ to be 0, this algorithm ignores geodesic alignment and becomes the CraigLinear
algorithm. Therefore, κ determines which algorithm in SCGIGA and CraigLinear SGGC is more
similar. (2)Maxbudget, denotedbys, isthemaximumnumberofnodeswecouldpickforeachepoch
in line 6 of Algorithm 1. This procedure could help accelerate the selection process for large graphs
suchasFlickrandArxiv. Weaimtofindthelargestmaxbudget, whichcouldremainanunnoticeable
drop in accuracy compared to the case when the max budget is 1. (3) Diffusion ego graph size,
denoted by p, is the ego-graph size we fix for every coreset node in the ego-graph selecting procedure.
Since the diffusion ego-graph size of one node is smaller than its two-degree standard ego-graph, the
GNN test performance becomes better when the diffusion ego-graph is larger. However, we need to
control the size of the diffusion ego-graph so that the induced subgraph has comparable memory
with that of the node-wise coreset selection methods. Therefore, our ablation experiment in this part
aims to find the smallest ego-graph size for each dataset where test performance does not increase
anymore.
2https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/
21So, according to our analysis of hyper-parameters, we design our hyper-parameter selecting
strategy and list the selected hyper-parameters below.
Table 7: SGGC hyper-parameter setups
Slack Parameter (κ) Max Budget (s) Diffusion Ego-Graph Size (p)
Cora 0.999 1 16
Citeseer 0.5 1 8
Pubmed 0.1 1 16
Flickr 0.5 10 8
Arxiv 0.1 10 8
Products 0.1 8 2
‘Reddit2 0.1 8 8
In the hyper-parameter tuning process, we first determine the diffusion ego-graph size for different
datasets under the fixed max budget, the slack parameter kappa, and the fraction ratio since this
hyper-parameter does not significantly influence the experiment result. Hence, we could pick the
smallest ego-graph size with which the model reaches the highest GNN test accuracy. Taking Cora
and Citeseer as an example, the test accuracy becomes around the best when the ego-graph size is 16
for Cora and 8 for Citeseer. After that, we fix the ego-graph size as we just selected and then choose
the slack parameter κ for each dataset according to GNN test accuracy. For instance, according
to Table 14, the best performance achieves when κ = 0.999 for Cora and κ = 0.5 for Citeseer. Finally,
we fix the selected diffusion ego-graph size and slack parameter κ to find the best max budget s.
This step is only for Flickr and Arxiv. We try to increase the max budget from 1, and the aim is to
find the largest max budget which does not sacrifice noticeable accuracy to accelerate the SGGC
algorithm as much as possible. Therefore, based on Table 15, we observe that the accuracy drops
out of the highest performance error bar when the max budget is larger than 10 for Flickr and Arxiv,
so we choose the max budget as 10 for both of them.
Selection strategy setups The GNN training varies according to different selection strategies
(node-wise selection, standard ego-graph selection, and diffusion ego-graph selection). (1) When we
choose node-wise selection, we directly induce a subgraph composed of the coreset nodes and train
GNN inductively. (2) For the standard ego-graph strategy, we union the 2-hop ego-graphs of every
selected coreset node and use this node and the labeled coreset nodes to train GNN transductively.
(3) The diffusion ego-graph selection strategy is slightly different from the standard one, which
controls the ego-graph size p. If the size of the 2-degree ego graph of one node exceeds p, then we
randomly cut the ego-graph size to p. If not, we try to increase the ego-graph degree to add more
nodes until the node has an ego-graph size p. Here we point out that not all ego-graphs could reach
the size p because the connection component the node is in has a size smaller than p.
Hyper-parameter setups of other model-agnostic coreset methods. The model-agnostic
coreset methods, Uniform, Herding, kCenterGreedy, and Cal, are determined algorithms that directly
select coresets based on node features, thus are hyper-parameter free. Besides, it is also worth noting
that those coreset selection methods do not utilize the graph structure information, i.e., the graph
adjacency matrix. For details, see Guo et al. (2022)
Hyper-parameter setups of model-based coreset methods. Themodel-basedmethodtakes
advantage of the node’s gradient/hidden features in GNN to select the coresets. Our experiments
are conducted on a pre-trained 2-layer GCN model, specifically GCNConv (feature dim.,256)-
2250%dropout-GCNConv (256, class num.)-log softmax. This model is trained for five epochs. The
optimizer is Adam, with a learning rate of 0.01 and weight decay 5e-4. Besides, we also offer other
pre-trained models, such as SGC and SAGE, in our released code repository. For Craig, we choose
the submodular function to be the Facility Location. For GradMatch, we take the regularization
parameter in orthogonal matching pursuit to be one, following the default setting of Guo et al.
(2022). Forgetting, Craig, Glister, and GraNd are hyper-parameter-free except for the pretraining
part.
Downstream GNN architectures. We implement GCN, SGC, and GraphSage as the
downstream GNN architecture. GCN has two convolution layers and a 50% dropout layer between
them. SGC and GraphSage are both 2-layer without dropout. Those three models do not have batch
normalization. The optimizers for the three models are all Adam equipped with a learning rate of
0.01 and weight decay of 5e-4. The hidden dimension is 256 in all three models. For every coreset
selection of each dataset, we train the GNN 10 times, and the GNN is trained for 600 epochs every
time.
E More Experimental Results.
E.1 Training Time with SGGC
As a coreset selection method, SGGC can significantly reduce the training time and memory
complexities of GNNs on large graphs. Specifically, when SGGC identifies a coreset of c nodes for a
graph with n training nodes (usually c ≪ n ), training GNNs on the SGGC coreset graph exhibits
t t
linear time and memory complexity to c rather than n . In other words, SGGC achieves sublinear
t
training time and memory complexities (to the original graph size) simultaneously. In Table 8, we
have recorded the actual training time and memory of GNNs on the SGGC coreset graph and the
original graph under different combinations of datasets and coreset ratios (c/n ). 2-Layer GCN
t
with 256 hidden dimensions is trained for 200 epochs. Our experimental results verify the sublinear
complexities of training GNNs using SGGC.
Table 8: GNN training time on SGGC coreset graphs and original graphs.
Training Time on Training Time on Training Memory on Training Memory on
Dataset Ratio
SGGC Coreset Graph Original Graph SGGC Coreset Graph Original Graph
Flickr 0.2% 9.9 s 45.4 s 2.8 MB 147.5 MB
Flickr 1.0% 10.7 s 45.4 s 3.1 MB 147.5 MB
ogbn-arxiv 0.5% 23.4 s 68.1 s 17.1 MB 963.1 MB
ogbn-arxiv 1.0% 28.0 s 68.1 s 4.56 MB 963.1 MB
E.2 SGGC with JKNets and Graph Transformers
We argue that SGGC is also applicable to GNNs with a large or even non-local receptive field. We
address this argument for each assumption, respectively, with experimental support.
We tested our SGGC with JKNets (Xu et al., 2018b) on ogbn-arxiv and Reddit. Many complex
model-based coreset algorithms run out of time/out of memory on ogbn-arxiv and Reddit, like
in Table 1. On ogbn-arxiv, we use a 4-layer GCN-based JK-MaxPool. The other hyperparameters
23are the same as in Table 1. The test accuracy of JKNets trained on the coreset graphs obtained by
different algorithms is in Table 9. We see SGGC is still better than the other baselines.
Table 9: Performance results of applying SGGC with JKNets.
Dataset Ratio Uniform Herding k-Center Forgetting SGGC (Ours) Full Graph (Oracle)
ogbn-arxiv 0.5% 55.0 ± 1.1 35.7 ± 0.9 49.1 ± 1.8 51.5 ± 1.9 56.2 ± 0.7 72.2 ± 0.3
ogbn-arxiv 1.0% 59.8 ± 1.5 40.2 ± 2.2 53.6 ± 0.6 59.5 ± 1.6 60.5 ± 0.8 72.2 ± 0.3
Reddit 0.1% 20.3 ± 6.5 13.7 ± 2.5 25.8 ± 1.6 19.8 ± 5.5 40.4 ± 1.9 96.5 ± 0.8
Reddit 0.2% 32.3 ± 6.1 14.3 ± 1.3 28.0 ± 2.7 22.5 ± 2.1 42.8 ± 1.0 96.5 ± 0.8
We also tested SGGC with Graph Transformers (Shi et al., 2020) on ogbn-arxiv. The other
hyperparameters are the same as in Table 1. The test accuracy of Graph Transformers trained on
the coreset graphs obtained by different algorithms is as follows. We see SGGC is still better than
the other baselines in most cases.
Table 10: Performance results of applying SGGC with Graph Transformer.
Dataset Ratio Uniform Herding k-Center Forgetting SGGC (Ours) Full Graph (Oracle)
ogbn-arxiv 0.5% 35.2 ± 2.9 35.2 ± 0.6 47.6 ± 1.6 52.2 ± 2.0 52.9 ± 1.3 72.1 ± 0.4
ogbn-arxiv 1.0% 58.3 ± 1.2 38.9 ± 2.5 51.0 ± 1.9 55.9 ± 2.7 56.1 ± 2.3 72.1 ± 0.4
E.3 SGGC on Low-Homophily Graphs and Graphs requiring Long-Range Rea-
soning
We conduct experiments on two real-world low-homophily graphs, Chameleon (homophily ratio
h = 0.23) and Squirrel (homophily ratio h = 0.22) (Rozemberczki et al., 2021). The homophily ratio
is defined as,
1 (cid:88)
h = ⊮{y = y },
j k
|E|
(i,j)∈E
where E is the set of edges, y is the label of node i, and ⊮{·} is the indicator function. Generally, the
i
homophily ratio h describes how likely the two end nodes of an edge are in the same class. Common
node classification benchmarks are often high-homophily; for example, the homophily ratio of Cora
is around 0.81. The Chameleon and Squirrel graphs are relatively small, allowing us to execute most
baseline methods on them successfully. As shown in Table 11, we find our SGGC nearly always
shows the best performance, which indicates SGGC is robust to and suitable for low-homophily
graphs.
We now include comparison results between SGGC and coreset baselines on the PascalVOC-SP
dataset(Dwivedietal.,2022). PascalVOC-SPisanodeclassificationdatasetthatrequireslong-range
interaction reasoning in GNNs to achieve strong performance in a given task. With 5.4 million
nodes, PascalVOC-SP is larger than all datasets in Table 1, and many complex model-based coreset
algorithms run out of time or memory. The performance metric for PascalVOC is macro F1, and our
SGGC consistently shows the best performance, as shown in Table 12. This indicates that SGGC is
also applicable to node classification tasks that require long-range information.
24Table 11: Performance comparison low-homophily graphs.
Dataset Ratio Uniform Herding K-Center CRAIG Forgetting Glister GradMatch GraNd Cal SGGC(ours) FullGraph(Oracle)
Chameleon 12.5% 49.0±2.8 46.3±0.8 44.7±1.1 49.2±3.8 46.9±2.3 50.7±2.2 43.2±1.6 42.5±1.9 49.5±1.0 48.1±2.2 58.7±1.6
Chameleon 25.0% 39.7±4.0 32.9±1.8 29.9±1.7 38.2±4.1 32.4±3.2 38.8±3.0 33.5±2.9 37.3±2.9 40.8±2.9 40.1±2.3 58.7±1.6
Chameleon 50.0% 50.4±3.8 46.9±1.9 45.3±1.2 50.2±2.5 47.1±1.5 50.8±2.8 43.6±1.1 43.2±1.4 48.9±2.1 49.4±1.9 58.7±1.6
Squirrel 12.5% 39.2±1.0 38.5±0.9 38.0±0.8 38.6±1.3 35.6±0.5 38.4±1.3 36.8±0.7 36.5±0.8 38.0±0.5 38.7±1.3 44.5±0.4
Squirrel 25.0% 33.3±1.5 31.6±0.7 31.7±0.8 33.3±1.7 31.6±0.8 33.3±1.2 31.0±0.6 30.4±1.0 32.7±0.5 34.1±1.2 44.5±0.4
Squirrel 50.0% 37.4±1.2 36.9±1.2 36.4±1.8 37.9±1.8 35.8±0.6 37.8±1.0 34.5±1.0 35.2±0.7 36.5±0.5 38.2±1.1 44.3±0.4
Table 12: Performance comparison on PascalVOC-SP, a graph dataset that requires long-range interaction
reasoning.
Dataset Ratio Uniform Herding k-Center Forgetting SGGC(Ours) FullGraph(Oracle)
PascalVOC-SP 0.05% 0.060±0.015 0.040±0.006 0.050±0.013 0.044±0.009 0.069±0.011 0.263±0.006
PascalVOC-SP 0.10% 0.073±0.018 0.051±0.004 0.068±0.009 0.062±0.016 0.080±0.008 0.263±0.006
E.4 Ablation Studies
As the ablation studies, we first discuss how three key factors, max budget, diffusion ego-graph
size, and slack parameter(denoted as κ), affect the GNN test accuracy of our SGGC algorithm. We
perform experiments on GCN and display the SGGC performance under different hyper-parameters
on Citeseer, Cora, Flickr, and Arxiv. Then we explore whether different selection strategies affect
GNN test accuracy on large graphs, Flickr, and Arxiv.
Ego-graph size. Diffusion ego graph size is an important hyper-parameter to determine
in coreset selection. Given the diffusion ego-graph size of one node is smaller than its two-degree
standard ego-graph, when the diffusion ego-graph is larger, the GNN test performance becomes
better. However, we need to control the size of the diffusion ego-graph so that the induced subgraph
has a comparable memory with that of the node-wise coreset selection methods. Therefore, our
ablation experiment in this part aims to find the smallest ego-graph size for each dataset where test
performance does not increase anymore.
Table 13: GNN test performance with different diffusion ego-graph size on Flickr and Arxiv
Diffusion Ego-Graph Size (p) 4 8 12 16 20 24 28
Cora 80.4±0.7 80.1±0.5 79.9±0.8 80.5±0.7 80.6±0.9 80.0±0.6 80.1±0.7
Citeseer 67.2±1.1 67.7±0.6 67.9±0.8 67.2±0.5 67.2±1.1 67.2±1.1 66.5±1.2
The diffusion ego-graph size on Cora and Citeseer seems to have no clear influence on test
accuracy if the ego-graph size is larger than a very small positive integer.
Slack parameter. In Algorithm 1, slack parameter (i.e.κ) means we could pick the nodes
whose alignment score has at least κ of the highest alignment score of v∗. This κ shows the balance
of our algorithm between SCGIGA and CraigLinear. When κ is 1, we pick nodes like pure SCGIGA.
When we select set κ to be 0, this algorithm ignores geodesic alignment and becomes the CraigLinear
algorithm. Therefore, κ determines which algorithm in SCGIGA and CraigLinear SGGC is more
similar.
From Table 14, the test performance keeps comparable from around 0.001 to at least as high as
0.75 for Cora. When κ approximates 1, test performance reaches its highest. However, for Citeseer,
25Table 14: GNN test performance with different kappa on Cora and Citeseer
CraigLinear
Slack Parameter (κ) 0.001 0.1 0.25 0.375 0.5
(κ=0)
Cora 78.1±1.1 76.2±1.2 75.8±1.6 76.1±0.9 76.6±1.2 76.7±2.0
Citeseer 66.6±1.9 63.1±6.6 67.8±1.8 69.1±1.7 69.7±2.3 69.8±1.5
SCGIGA
Slack Parameter (κ) 0.625 0.75 0.9 0.999 0.9999
(κ=1)
Cora 76.9±1.5 77.9±0.9 75.0±1.3 80.4±0.8 80.0±0.7 78.3±1.3
Citeseer 69.2±1.8 69.0±1.5 68.4±2.6 69.1±0.9 68.2±0.5 66.9±1.1
the accuracy is the highest at around 0.5. This result shows that κ selection highly varies according
to datasets. It is interesting that if we plot the κ-accuracy graph, the curve drops dramatically at 0
and 1. This remains not clear why the result would change that dramatically.
Max budget. Max budget is the number of nodes we could pick for each epoch in line 6 of
Algorithm 1. This procedure could help accelerate the selection process for large graphs such as
Flickr and Arxiv. We aim to find the largest max budget which could remain an unnoticeable drop
in accuracy compared to the case when the max budget is 1.
Table 15: GNN test performance with different max budget on Flickr and Arxiv
Max Budget (s) 1 5 10 15 20 25 30
Flickr 49.5±0.2 49.3±0.4 49.3±0.3 49.0±0.5 49.1±0.4 48.7±0.4 48.4±0.8
Arxiv 60.9±1.5 61.4±1.4 59.8±1.3 56.9±2.0 57.7±3.2 54.8±3.4 50.2±5.6
From this table, we could see that increasing the max budget does not affect test accuracy much
before the budget reach as large as 10, so we could take the budget to 10 for both Flickr and Arxiv.
Standard ego graph sizes Recall that in Section 6, our SGGC adopts the diffusion ego-graph
selection strategy because it could dramatically save memory storage by cutting the ego-graph size
of each node while keeping the GNN test accuracy from a noticeable decrease. To see it more clearly,
we compare diffusion ego-graph size and standard average ego-graph size for all datasets.
Table 16: Ego-graph size for different selection strategy on datasets.
Cora Citeseer Pubmed Flickr Arxiv
Diffusion ego-graph size 16 8 16 8 8
Standard ego-graph size 36.8 15.1 60.1 875.7 3485.2
E.5 More results on large graphs
As a good continuation of Table 1, we conduct main coreset selection methods on large graphs, Flickr
and Arxiv, to further understand the power of our SGGC.
26Table 17: GNN test performance on large graph with different fraction ratio
Uniform Herding kCenterGreedy Forgetting Glister GradMatch SGGC
0.2% 45.9±2.0 44.5±0.9 46.6±1.8 45.7±2.1 47.4±0.8 48.3±0.5 46.9±1.5
Flickr
1% 47.4±1.8 46.3±0.5 46.7±0.9 47.4±1.9 48.4±1.0 48.5±0.6 48.6±0.7
0.1% 47.2±6.2 33.8±1.3 41.5±5.8 43.6±6.1 41.5±8.6 45.6±3.9 41.1±6.8
Arxiv
0.5% 57.4±2.6 45.6±0.4 57.4±1.0 56.6±2.7 56.1±3.0 51.2±4.2 60.0±1.9
From Table 17, we could see that the performance is the highest on Flickr under a fraction
rate 1% and on Arxiv under a fraction rate 0.5%. It is worth noting that in a large graph, when
the fraction rate increases, the GNN accuracy also increases, which is different from what we have
observed in small graphs such as Pubmed, Cora, and Citeseer.
27References
Aljundi, R., Lin, M., Goujaud, B. and Bengio, Y. (2019). Gradient based sample selection for
online continual learning. Advances in neural information processing systems 32.
Atwood, J. and Towsley, D. (2016). Diffusion-convolutional neural networks. In NeurIPS.
Baker, D., Braverman, V., Huang, L., Jiang, S. H.-C., Krauthgamer, R. and Wu, X.
(2020). Coresets for clustering in graphs of bounded treewidth. In International Conference on
Machine Learning. PMLR.
Balcilar, M., Guillaume, R., Héroux, P., Gaüzère, B., Adam, S. and Honeine, P. (2021).
Analyzing the expressive power of graph neural networks in a spectral perspective. In Proceedings
of the International Conference on Learning Representations (ICLR).
Batson, J., Spielman, D. A., Srivastava, N. and Teng, S.-H. (2013). Spectral sparsification of
graphs: theory and algorithms. Communications of the ACM 56 87–94.
Bianchi, F. M., Grattarola, D., Livi, L. and Alippi, C. (2021). Graph neural networks with
convolutional arma filters. IEEE Transactions on Pattern Analysis and Machine Intelligence .
Bohdal, O., Yang, Y. and Hospedales, T. (2020). Flexible dataset distillation: Learn labels
instead of images. arXiv preprint arXiv:2006.08572 .
Borsos, Z., Mutny, M. and Krause, A. (2020). Coresets via bilevel optimization for continual
learning and streaming. Advances in Neural Information Processing Systems 33 14879–14890.
Braverman, V., Jiang, S. H.-C., Krauthgamer, R. and Wu, X. (2021). Coresets for clustering
in excluded-minor graphs and beyond. In Proceedings of the 2021 ACM-SIAM Symposium on
Discrete Algorithms (SODA). SIAM.
Cai, C., Wang, D. and Wang, Y. (2020). Graph coarsening with neural networks. In International
Conference on Learning Representations.
Campbell, T. and Broderick, T. (2018). Bayesian coreset construction via greedy iterative
geodesic ascent. In International Conference on Machine Learning. PMLR.
Chiang, W.-L., Liu, X., Si, S., Li, Y., Bengio, S. and Hsieh, C.-J. (2019). Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the
25th ACM SIGKDD international conference on knowledge discovery & data mining.
Corso, G., Cavalleri, L., Beaini, D., Liò, P. and Veličković, P. (2020). Principal neighbour-
hood aggregation for graph nets. NeurIPS 33.
Defferrard, M., Bresson, X. and Vandergheynst, P. (2016). Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in neural information processing
systems, vol. 29.
Ding, M., Kong, K., Li, J., Zhu, C., Dickerson, J., Huang, F. and Goldstein, T. (2021).
Vq-gnn: A universal framework to scale up graph neural networks using vector quantization.
Advances in Neural Information Processing Systems 34 6733–6746.
28Dwivedi, V. P., Rampášek, L., Galkin, M., Parviz, A., Wolf, G., Luu, A. T. and Beaini,
D. (2022). Long range graph benchmark. Advances in Neural Information Processing Systems 35
22326–22340.
Elsken, T., Metzen, J. H. and Hutter, F. (2019). Neural architecture search: A survey. The
Journal of Machine Learning Research 20 1997–2017.
Farahani, R. Z. and Hekmatfar, M. (2009). Facility location: concepts, models, algorithms and
case studies. Springer Science & Business Media.
Guo, C., Zhao, B. and Bai, Y. (2022). Deepcore: A comprehensive library for coreset selection in
deep learning. arXiv preprint arXiv:2204.08499 .
Hamilton, W., Ying, Z. and Leskovec, J. (2017). Inductive representation learning on large
graphs. Advances in neural information processing systems 30.
Hamilton, W. L. (2020). Graph Representation Learning. Morgan & Claypool Publishers.
Huang, Z., Zhang, S., Xi, C., Liu, T.andZhou, M.(2021). Scalingupgraphneuralnetworksvia
graph coarsening. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
& Data Mining.
Iyer, R., Khargoankar, N., Bilmes, J. and Asanani, H. (2021). Submodular combinatorial
information measures with applications in machine learning. In Algorithmic Learning Theory.
PMLR.
Jin, W., Tang, X., Jiang, H., Li, Z., Zhang, D., Tang, J. and Yin, B. (2022). Condensing
graphs via one-step gradient matching. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining.
Jin, W., Zhao, L., Zhang, S., Liu, Y., Tang, J. and Shah, N. (2021). Graph condensation for
graph neural networks. In International Conference on Learning Representations.
Kim, J.-H., Kim, J., Oh, S. J., Yun, S., Song, H., Jeong, J., Ha, J.-W. and Song, H. O.
(2022). Dataset condensation via efficient synthetic-data parameterization. In International
Conference on Machine Learning. PMLR.
Kipf, T. N. and Welling, M. (2016). Semi-supervised classification with graph convolutional
networks. In International Conference on Learning Representations.
Klicpera, J., Weißenberger, S. and Günnemann, S. (2019). Diffusion improves graph learning.
In Advances in neural information processing systems. PMLR.
Kothawade, S., Kaushal, V., Ramakrishnan, G., Bilmes, J. and Iyer, R. (2022). Prism: A
rich class of parameterized submodular information measures for guided data subset selection. In
Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36.
Levie, R., Monti, F., Bresson, X. and Bronstein, M. M. (2018). Cayleynets: Graph
convolutional neural networks with complex rational spectral filters. IEEE Transactions on Signal
Processing .
29Li, Y., Tarlow, D., Brockschmidt, M. and Zemel, R. (2015). Gated graph sequence neural
networks. In ICLR.
Linderman, G. and Steinerberger, S. (2020). Numerical integration on graphs: where to sample
and how to weigh. Mathematics of computation 89 1933–1952.
Liu, M., Li, S., Chen, X. and Song, L. (2022). Graph condensation via receptive field distribution
matching. arXiv preprint arXiv:2206.13697 .
Loukas, A. (2019). Graph reduction with spectral and cut guarantees. J. Mach. Learn. Res. 20
1–42.
Loukas, A. and Vandergheynst, P. (2018). Spectrally approximating large graphs with smaller
graphs. In International Conference on Machine Learning. PMLR.
Ma, Y., Liu, X., Shah, N. and Tang, J. (2021). Is homophily a necessity for graph neural
networks? In International Conference on Learning Representations.
Maron, H., Ben-Hamu, H., Serviansky, H. and Lipman, Y. (2019). Provably powerful graph
networks. In NeurIPS.
Mirzasoleiman, B., Bilmes, J. and Leskovec, J. (2020). Coresets for data-efficient training of
machine learning models. In International Conference on Machine Learning. PMLR.
Morris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G. and
Grohe, M. (2019). Weisfeiler and leman go neural: Higher-order graph neural networks. In
AAAI.
Nguyen, T., Chen, Z. and Lee, J. (2020). Dataset meta-learning from kernel ridge-regression. In
International Conference on Learning Representations.
Nguyen, T., Novak, R., Xiao, L. and Lee, J. (2021). Dataset distillation with infinitely wide
convolutional networks. Advances in Neural Information Processing Systems 34 5186–5198.
Paul, M., Ganguli, S. and Dziugaite, G. K. (2021). Deep learning on a data diet: Finding
important examples early in training. Advances in Neural Information Processing Systems 34
20596–20607.
Rong, Y., Bian, Y., Xu, T., Xie, W., Wei, Y., Huang, W. and Huang, J. (2020). Self-
supervised graph transformer on large-scale molecular data. In Advances in neural information
processing systems, vol. 33.
Rozemberczki, B., Allen, C. and Sarkar, R. (2021). Multi-scale attributed node embedding.
Journal of Complex Networks 9 cnab014.
Satuluri, V., Parthasarathy, S. and Ruan, Y. (2011). Local graph sparsification for scalable
clustering. In Proceedings of the 2011 ACM SIGMOD International Conference on Management
of data.
Sener, O. and Savarese, S. (2018). Active learning for convolutional neural networks: A core-set
approach. In International Conference on Learning Representations.
30Shi, Y., Huang, Z., Feng, S., Zhong, H., Wang, W. and Sun, Y. (2020). Masked label
prediction: Unified message passing model for semi-supervised classification. arXiv preprint
arXiv:2009.03509 .
Such, F. P., Rawal, A., Lehman, J., Stanley, K. and Clune, J. (2020). Generative teaching
networks: Accelerating neural architecture search by learning to generate synthetic training data.
In International Conference on Machine Learning. PMLR.
Toneva, M., Sordoni, A., des Combes, R. T., Trischler, A., Bengio, Y. and Gordon,
G. J. (2018). An empirical study of example forgetting during deep neural network learning. In
International Conference on Learning Representations.
Vahidian, S., Mirzasoleiman, B. and Cloninger, A. (2020). Coresets for estimating means
and mean square error with limited greedy samples. In Conference on Uncertainty in Artificial
Intelligence. PMLR.
Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P. and Bengio, Y. (2018).
Graph attention networks. In International Conference on Learning Representations.
Wang, K., Zhao, B., Peng, X., Zhu, Z., Yang, S., Wang, S., Huang, G., Bilen, H., Wang,
X. and You, Y. (2022). Cafe: Learning to condense dataset by aligning features. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
Wang, T., Zhu, J.-Y., Torralba, A. and Efros, A. A. (2018). Dataset distillation. arXiv
preprint arXiv:1811.10959 .
Welling, M. (2009). Herding dynamical weights to learn. In Proceedings of the 26th Annual
International Conference on Machine Learning.
Xu, K., Hu, W., Leskovec, J.andJegelka, S.(2018a). Howpowerfularegraphneuralnetworks?
In International Conference on Learning Representations.
Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K.-i. and Jegelka, S. (2018b).
Representation learning on graphs with jumping knowledge networks. In International conference
on machine learning. PMLR.
Xu, K., Li, J., Zhang, M., Du, S. S., Kawarabayashi, K.-i. and Jegelka, S. (2020). What
can neural networks reason about? In ICLR.
Yang, Z., Cohen, W. and Salakhudinov, R. (2016). Revisiting semi-supervised learning with
graph embeddings. In International conference on machine learning. PMLR.
YaronLipman, O. H.-H. (2020). Global attention improves graph networks generalization. arXiv
preprint arXiv:2006.07846 .
Zeng, H., Zhou, H., Srivastava, A., Kannan, R. and Prasanna, V. (2019). Graphsaint:
Graph sampling based inductive learning method. In International Conference on Learning
Representations.
Zhang, J., Zhang, H., Xia, C. and Sun, L. (2020). Graph-bert: Only attention is needed for
learning graph representations. arXiv preprint arXiv:2001.05140 .
31Zhao, B. and Bilen, H. (2021a). Dataset condensation with differentiable siamese augmentation.
In International Conference on Machine Learning. PMLR.
Zhao, B. and Bilen, H. (2021b). Dataset condensation with distribution matching. arXiv preprint
arXiv:2110.04181 .
Zhao, B., Mopuri, K. R. and Bilen, H. (2020). Dataset condensation with gradient matching.
In International Conference on Learning Representations.
32