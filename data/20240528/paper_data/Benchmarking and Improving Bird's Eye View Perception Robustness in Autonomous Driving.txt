1
Benchmarking and Improving Bird’s Eye View
Perception Robustness in Autonomous Driving
Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu
Abstract—Recentadvancementsinbird’seyeview(BEV)representationshaveshownremarkablepromiseforin-vehicle3D
perception.However,whilethesemethodshaveachievedimpressiveresultsonstandardbenchmarks,theirrobustnessinvaried
conditionsremainsinsufficientlyassessed.Inthisstudy,wepresentRoboBEV,anextensivebenchmarksuitedesignedtoevaluatethe
resilienceofBEValgorithms.Thissuiteincorporatesadiversesetofcameracorruptiontypes,eachexaminedoverthreeseverity
levels.Ourbenchmarksalsoconsidertheimpactofcompletesensorfailuresthatoccurwhenusingmulti-modalmodels.Through
RoboBEV,weassess33state-of-the-artBEV-basedperceptionmodelsspanningtaskslikedetection,mapsegmentation,depth
estimation,andoccupancyprediction.Ouranalysesrevealanoticeablecorrelationbetweenthemodel’sperformanceonin-distribution
datasetsanditsresiliencetoout-of-distributionchallenges.Ourexperimentalresultsalsounderlinetheefficacyofstrategieslike
pre-traininganddepth-freeBEVtransformationsinenhancingrobustnessagainstout-of-distributiondata.Furthermore,weobserve
thatleveragingextensivetemporalinformationsignificantlyimprovesthemodel’srobustness.Basedonourobservations,wedesignan
effectiverobustnessenhancementstrategybasedontheCLIPmodel.Theinsightsfromthisstudypavethewayforthedevelopmentof
futureBEVmodelsthatseamlesslycombineaccuracywithreal-worldrobustness.Thebenchmarktoolkitandmodelcheckpointsare
publiclyaccessibleat:https://github.com/Daniel-xsy/RoboBEV.
IndexTerms—3DObjectDetection,Bird’sEyeViewSegmentation,SemanticOccupancyPrediction,Out-of-DistributionRobustness.
✦
1 INTRODUCTION
DEEP neural network-based 3D perception methods into adversarial robustness [12], [15], [16], [17], [26], [47],
have registered transformative breakthroughs, ex- [48] – which delves into worst-case scenarios – and robust-
cellinginarangeofdemandingbenchmarks[2],[3],[4],[7], ness under distribution shift [50], [57], [59], [59], [60] that
[9], [10], [28], [29], [31], [42]. Among these, camera-centric examines average-case performance, and, to certain extent,
methods [2], [3], [4], [7], [9], [10] have surged in popularity mirroringreal-worldconditions.
over their LiDAR-driven counterparts [28], [29], [31], [42], While adversarial robustness of 3D perception models
primarily due to advantages such as reduced deployment has been studied [14], [19], [20], [62], this work seeks to
costs, augmented computational efficiency, and the ability explorealess-traveledavenue:robustnessofBEV-centric3D
to provide dense semantic insights [6], [43]. Central to perceptionsystemswhensubjectedtonatural,oftenunpre-
many of these advancements is the bird’s eye view (BEV) dictable, corruptions. In this work, to address the existing
representation,whichoffersatrioofsignificantbenefits: knowledge gap, we present a comprehensive benchmark
dubbedRoboBEV.Thisbenchmarkevaluatestherobustness
• Itfacilitatesunifiedlearningfrommulti-viewimages.
of BEV perceptions against natural corruptions including
• Itencouragesaphysicallyinterpretablemethodology
exterior environments, interior sensors, and temporal fac-
forfusinginformationacrosstemporalinstances.
tors.Specifically,theexteriorenvironmentsincludevarious
• Its output domain aligns seamlessly with several
lighting and weather conditions, which are simulated by
downstream applications, such as prediction and
incorporating Brightness, Dark, Fog, and Snow corruption
planning,whichfortifiestheperformancemetricsof
types. Additionally, the inputs may be corrupted by in-
vision-centric3Dperceptionframeworks.
terior factors caused by sensors, such as Motion Blur and
However, this blossoming landscape of BEV percep- Color Quant. We further propose two novel corruptions in
tion methodologies is not without its challenges. Despite consecutive space tailored for BEV-based temporal fusion
their evident prowess, the resilience of these algorithms in strategies, namely Camera Crash and Frame Lost. Moreover,
the face of out-of-context or unforeseen scenarios remains we consider complete sensor failure for camera-LiDAR fu-
under-examined. This oversight is particularly concerning sion models [66], [68], [69] that are trained on multi-modal
giventhatmanyofthesealgorithmsareenvisionedtofunc- inputs. The study involves a comprehensive investigation
tion in safety-critical realms such as autonomous driving. of diverse out-of-distribution corruption settings that are
Traditionally,therobustnessofalgorithmscanbebifurcated highly relevant to real-world autonomous driving applica-
tions.Figure1summarizesthediverseBEVperceptiontasks
• S. Xie is with the Donald Bren School of Information and Computer andcorruptiontypesinourbenchmarkstudy.
Sciences,UniversityofCalifornia,Irvine,CA,USA. Leveraging the proposed RoboBEV benchmark, we con-
• L.KongiswiththeSchoolofComputing,NationalUniversityofSinga-
duct an exhaustive analysis of 33 BEV perception models
pore,andCNRS@CREATE,Singapore.
• J.RenandZ.LiuarewithNanyangTechnologicalUniversity,Singapore. under corruptions across various severity levels. Finally,
• W.Zhang,L.Pan,andK.ChenarewithShanghaiAILaboratory,China. based on the observations, we propose to improve the
• Z.Liuservesasthecorrespondingauthor.E-mail:ziwei.liu@ntu.edu.sg.
model’s robustness by leveraging the CLIP [113] backbone
4202
yaM
72
]VC.sc[
1v62471.5042:viXra2
Detection Segmentation Occupancy Depth Estimation
Perception
Task
Corruption Camera Corruption Camera Failure LiDAR Failure
Sensor
Type
Camera
Camera
LiDAR
Image
Corruption
t-1 t
Motion Dark Snow Camera Crash
timestamp
Quant Bright Fog t-1 Frame Lost t
Fig.1.RoboBEV benchmarkdesigns.ThebenchmarkcomprehensivelyencompassesfourdistinctBEVperceptiontasks(detection,segmenta-
tion,occupancyprediction,anddepthestimation),fourdiversesensortypeconfigurationsinbetweenLiDAR,cameras,andjointsetups(camera
corruption,camerafailure,andLiDARfailure),andanarrayofeightnaturalimagecorruptions(Brightness,Darkness,Fog,Snow,MotionBlur,Color
Quantization,CameraCrash,andFrameLost),eachcategorizedintothreedistinctseveritylevels.
and adapting it to BEV perception tasks. The key contribu- therobustnessofBEVperceptionmodelsfurther.
tionsofthisworkaresummarizedasfollows: 5) We make the datasets and benchmark toolkit pub-
licly available, aiming to encourage the research
1) We introduce RoboBEV, a comprehensive bench- communitytoreplicateandextendourfindings.
mark suite for evaluating BEV perception robust-
The remainder of this paper is organized as follows.
nessunderadiversesetofnaturalcorruptions.
Section 2 reviews the relevant literature on vision-centric
2) Weconductextensiveexperimentstoassesstheper-
and LiDAR-based BEV perception, out-of-distribution ro-
formance of 30 camera-based and 3 camera-LiDAR
bustness,androbustnessenhancementusingCLIP.Section3
fusion-based BEV perception algorithms. These al-
provides necessary preliminaries of BEV-based perception
gorithmsareevaluatedacrosseightdistinctcorrup-
tasks. Section 4 elaborates in detail on our benchmark de-
tions,eachappliedatthreedifferentseveritylevels,
signs and robustness metrics. Extensive experimental stud-
foratotaloffourperceptiontasks.
iesareincludedinSection5.Basedontheresults,wedraw
3) Ourstudyoffersvaluableinsightsthroughin-depth
analysesandobservationsinSection6.Finally,Section7dis-
analysesofthefactorsthatcontributetorobustness
cussespotentiallimitationsandoffersconcludingremarks.
undercorruptionscenarios,whichshedlightonfu-
turemodeldesigns.Ourkeyobservationsare:i)The
absolute performances show a strong correlation 2 RELATED WORKS
withtheperformancesundercorruptions.However,
In this section, we review the most relevant works on the
therelativerobustnessdoesnotnecessarilyincrease
topics of BEV perception, out-of-distribution robustness,
as standard performance improves; ii) The model
andpopularrobustnessenhancementstrategies.
pre-training together with depth-free BEV transfor-
mation has great potential to enhance robustness;
2.1 Camera-BasedBEVPerception
andiii)Utilizinglongandrichtemporalinformation
largelyenhancestherobustness. BEV perception methodologies can be divided into two
4) Based on our observations, we propose to leverage primary branches predicated on the explicitness of their
the CLIP [113] model as the backbone to improve depthestimation[6].Asegmentoftheliterature,influenced3
byLSS[24],suchasBEVDet[3],employsanauxiliarydepth exploring the fusion of multiple views, seeking to harness
estimation branch to facilitate the transformation from per- thecomplementarystrengthsofdifferentrepresentations.In
spective view to bird’s eye view (PV2BEV). BEVDepth [22] essence, while LiDAR-based 3D perception methodologies,
refinesthisparadigm,enhancingdepthestimationaccuracy especiallythoselinkedwithBEVperception,haveexhibited
using explicit depth data from point clouds. Meanwhile, significantpromise,theirresilienceinreal-worldconditions
BEVerse [51] introduces a multi-task learning framework warrantsdeeperexplorationandvalidation.
that achieves benchmark-setting outcomes. In contrast, an
alternative research trajectory avoids explicit depth esti-
2.3 RobustnessunderAdversarialAttacks
mation. Drawing inspiration from DETR [21], models like
Modern neural networks, while showcasing staggering
DETR3D [4] and ORA3D [52] encapsulate 3D objects as
capabilities, remain vulnerable to adversarial onslaughts,
queries, leveraging Transformers’ cross-attention mecha-
wheremeticulouslyengineeredperturbationsininputscan
nisms. Following this, PETR [7] boosts performance by
precipitate erroneous outputs [12], [16], [17]. The men-
formulating 3D position-aware representations. Simultane-
ace of adversarial examples has been a research epicenter
ously, BEVFormer [2] and PolarFormer [53] venture into
across various vision domains: classification [12], [16], de-
temporalcross-attentionandpolarcoordinate-based3Dtar-
tection [15], [27], and segmentation [14], [15]. These adver-
get predictions, respectively. Taking a leaf out of Sparse
sarial stimuli can emerge in both digital domains [12], [16]
RCNN’s[56]book,SRCN3D[54]andSparse4D[55]pioneer
and real-world environments [14], [61]. Alarming findings
sparse proposals for feature amalgamation. Meanwhile,
reveal that adversarial examples can cripple 3D perception
SOLOFusion [65] pursues deeper historical data integra-
systems,flaggingpotentialsafetyconcernsduringpractical
tion for temporal modeling. In addition to detection, BEV
deployments [14], [19], [20]. While Xie et al. [62] delve into
perception tasks also include map segmentation [25], [74],
the adversarial robustness of camera-centric detectors, our
multi-view depth estimation [70], and semantic occupancy
focuspivotstowardsmorepervasivenaturalcorruptions.
prediction[71],[72],[75],[77],[78].Whilethesemethodolo-
gies flaunt impressive outcomes on pristine datasets, their
resilienceagainstnaturalcorruptionsremainsanenigma. 2.4 RobustnessunderNaturalCorruptions
Assessing model tenacity against corruptions has bur-
2.2 LiDAR-Based3DPerception geoned as a pivotal research domain. Several benchmarks,
such as ImageNet-C [50], ObjectNet [60], ImageNetV2 [59],
LiDAR, with its precision in capturing spatial relationships
and more, evaluate the robustness of 2D image classifiers
using laser beams, has paved the way for breakthroughs
against an array of corruptions. For instance, ImageNet-C
in 3D perception, central to applications like autonomous
taintspristineImageNetsampleswithsimulatedanomalies
driving [11]. Two primary tasks have gained prominence:
like compression artifacts and motion blurs. On the other
3DobjectdetectionandLiDARsemanticsegmentation,both
hand, ObjectNet [60] offers a test set abundant in rotation,
of which have inherent connections to BEV perception [5],
background,andviewpointvariances.Hendrycksetal.[58]
[13], [23], [30], [33]. In the realm of 3D object detection,
underscore the correlation between synthetic corruption
the focus has been on optimally representing LiDAR point
robustness and enhancements in real-world scenarios. Re-
clouddata[32].Point-basedapproaches,suchasthosepre-
cently, Some works [76], [79], [80] endeavor to improve
sentedin[88],[89],[90],[91],shineinpreservingtheinnate
the robustness of 3D perception models. Kong et al. [85],
geometry of point clouds, capturing local structures and
[86]establisharobustnessbenchmarkformonoculardepth
patterns. Meanwhile, voxel-based strategies, like [92], [93],
estimation under corruptions. Ren et al. [87] design atomic
[94],converttheirregularpointcloudsintostructuredgrids,
corruptionsonindoorobject-centricpointcloudsandCAD
relying on sparse convolution techniques [92] to handle
models to understand classifiers’ robustness. Yet, a void
non-empty voxels efficiently. Pillar-based techniques, high-
persists concerning benchmarks for 3D BEV perception
lighted by works like [29], [95], offer a trade-off between
models, which play critical roles in safety-sensitive appli-
detectionaccuracyandcomputationalspeedbyfine-tuning
cations. While a concurrent study by Zhu et al. [73] ex-
the vertical resolution. Additionally, hybrid approaches,
ploresasimilarlandscape,theirnarrativeispredominantly
such as [96], [97], merge the strengths of both point and
adversarial-centric. In contrast, our benchmarks, spanning
voxel representations to derive more enriched features. On
models, tasks, scenarios, and validation studies, offer a
theotherhand,semanticsegmentationtechniquesoftenpivot
broaderandmorecomprehensivelensintothisdomain.
on the representation choice. Raw point methods, like [35],
[98], emphasize the direct usage of irregular point clouds,
2.5 RobustnessEnhancementsusingCLIP
whilerangeviewapproaches,showcasedin[36],[99],[100],
[102], [108], convert these point clouds into 2D grids. This TheContrastiveLanguage-ImagePre-training(CLIP)model
conversion aligns closely with BEV perception, transform- [113] has been shown to significantly improve the model’s
ing3Ddataintoatop-downperspective,essentialformany out-of-distribution robustness compared to previous su-
applications. Further refining this idea are bird’s eye view pervised trained models on ImageNet [115], [116]. Recent
techniques, exemplified by [103], which offer a direct 2D studies have begun to explore why CLIP exhibits superior
top-down representation. Voxel-centric methods, such as robustness [115], [121], [122] and how this robustness can
[104],maintainthe3Dspatialstructure,oftenoutperforming be maintained after fine-tuning [123], [124]. Specifically,
other singular modalities. Modern research, like [18], [34], [124] found that while end-to-end fine-tuning can enhance
[37], [38], [39], [105], [106], [107], pushes the boundaries by in-distribution performance on supervised datasets, it can4
1e8 Clean 1e8 Bright 1e8 Dark 1e8 Fog 1e8 Snow 1e8 Motion 1e8 Quant
2.5 2.5
1.5 2.5 1.5 1.5
6 2.0 2.0
2.0
1.0 1.5 4 1.0 1.5 1.0 1.5
1.0 1.0 1.0
0.5 2 0.5 0.5
0.5 0.5 0.5
0.0 0.0 0 0.0 0.0 0.0 0.0
0 100 200 0 100 200 0 100 200 0 100 200 0 100 200 0 100 200 0 100 200
Fig.2.Histogramsofpixeldistributionsfordifferentcorruptiontypes.Whilecertaincorruptionsexhibitminimalshiftsinpixeldistribution(e.g.,Motion
Blur),itisnoteworthythatthesealterationspredominantlyhaveadverseeffectsontheoverallperformanceoftheBEVperceptionsystems.
also compromise the out-of-distribution robustness of the 3.3 Camera-LiDARFusion
pre-trained CLIP model. To address this, they employed TheBEVparadigmstreamlinesthefusionoffeaturesfroma
weighted parameter adjustments to achieve both perfor- varietyofinputmodalities.Whilesomealgorithmsfocuson
mance and robustness. Given CLIP’s robust performance, crafting BEV representations solely from images, a notable
a natural question arises: can we leverage the pre-trained fractionoftheliterature,includingworkslike[66],[67],[69],
CLIP model to enhance BEV perception robustness? In this [81],[82],[83],advocatesforaunifiedBEVspace.Thisaligns
work, we make the first attempt to investigate how the features extracted from both images and point clouds. We
robustnessofthepre-trainedCLIPmodelcanberetained. delvedeepintotheperformanceofsuchmulti-modalfusion
algorithms, especially under circumstances where images
are corrupted, yet the LiDAR mechanism remains benign.
3 BEV PERCEPTION PRELIMINARIES
Furthermore, we address a common scenario where the
In this section, we delineate the commonly adopted tech- model is trained using multi-modal inputs but deployed
niques for BEV perception algorithms, which have demon- on vehicles equipped with only one of the sensors. To
stratedenhancedperformanceacrossstandarddatasets. assess robustness, we evaluate the model’s performance
under conditions of complete sensor failure, where either
3.1 ModelPre-Training thecameraorLiDARismissing.
Over the past few years, pre-training has enhanced the
3.4 BEVViewTransformation
performance of computer vision models in diverse tasks.
Within the domain of camera-driven 3D perception, initial- The body of works in BEV transformation can be divided
izing the ResNet backbone using FCOS3D [9] weights has based on the use of depth estimation techniques [6]. One
become standard practice. To stabilize the training process, faction,asdiscussedin[3],[22],[51],[65],embedsadistinct
FCOS3D adjusts a depth weight from 0.2 to 1 during fine- depth-estimation branch within their systems. Given the
tuning [9]. Another prevailing approach involves train- inherent challenges in predicting 3D bounding boxes from
ing the VoVNet-V2 [63] backbone on the DDAD15M [64] images, these models first forecast a per-pixel depth map.
dataset, targeting depth estimation, before fine-tuning it This map then serves as a compass, guiding image fea-
using the nuScenes training set for detection. Semantically, tures to their rightful 3D coordinates. The subsequent BEV
these pre-training techniques fall into two categories: se- transformationprocessoftenfollowsabottom-upapproach,
mantic and depth pre-training. Furthermore, M-BEV [114] as depicted in [4]. On the other side of the spectrum are
introduces robust masked image pre-training techniques modelsleveragingpre-definedobjectqueries[2],[4]orlean
designed to augment model resilience in scenarios charac- proposals [54], [55] to collate 2D features in a top-down
terizedbyabsentsensordata. manner. While both these paradigms have demonstrated
their prowess on benign datasets, we further expand the
horizonbyexaminingtheirefficacyoncorrupteddata.
3.2 TemporalFusion
The dynamic landscape of autonomous driving demands 4 BENCHMARK DESIGN
precise velocity estimates of moving entities, which is a
This section elaborates on our benchmark design for BEV
challenge for singular frame inputs. This reveals the im-
algorithms. Subsections 4.1 and 4.2 discuss the creation
portance of temporal cues in enhancing vision systems’
andstructureofthenuScenes-Cbenchmarkdataset.Subsec-
perceptioncapabilities.Priorresearchhaspioneeredvarious
tion4.3illustratestherobustnessmetricsdesignedtoassess
methodologiestoharnessthesetemporalcues.Forinstance,
theresilienceofmodelsagainstvariouscorruptions.
BEVFormer [2] integrates history data and leverages tem-
poralcross-attentiontoaggregateBEVfeaturesfrommulti-
4.1 DatasetGeneration
timestampimages.Meanwhile,BEVDet4D[44]appendsfea-
tures from multiple frames to weave in temporal nuances, Our main proposal is the nuScenes-C benchmark dataset,
and SOLOFusion [65] aims for more inclusive temporal which is created by introducing corruptions to the valida-
modeling by merging extensive historical data. However, tion set of the nuScenes dataset [1]. Encompassing a vast
theresilienceofthesesophisticatedtemporalmodelsunder expanse of eight distinct corruptions, our dataset simu-
corruptedconditionsremainslargelyunder-explored. lateschallengesposedbyexternalenvironmentalelements,5
TABLE1
Severitylevelsetupsincorruptionsimulations:Detailedparametersusedforgeneratingmulti-levelcorruptionsforeachcorruptiontype.
Corruption Parameter Easy Moderate Hard
Bright adjustmentinHSVspace 0.2 0.4 0.5
Dark scalefactor 0.5 0.4 0.3
Fog (thickness,smoothness) (2.0,2.0) (2.5,1.5) (3.0,1.4)
(mean,std,scale,threshold,
Snow (0.1,0.3,3.0,0.5,10.0,4.0,0.8) (0.2,0.3,2,0.5,12,4,0.7) (0.55,0.3,4,0.9,12,8,0.7)
blurradius,blurstd,blendingratio)
Motion (radius,sigma) (15,5) (15,12) (20,15)
Quant bitnumber 5 4 3
Crash numberofdroppedcamera 2 4 5
Frame probabilityofframedropping 2/6 4/6 5/6
sensor-induced distortions, and our innovative temporal Average Scale Error (mASE), mean Average Orientation
corruptions. Inspired by [50], we tier each corruption type Error (mAOE), mean Average Velocity Error (mAVE) and
across three intensities: easy, moderate, and hard. These meanAverageAttributeError(mAAE).
severitylevelsensurethatwhilechallengesarepresent,they To better compare the robustness among different BEV
do not entirely destroy performance, thereby maintaining detectors, we introduce two new metrics inspired by [50]
the relevance and integrity of our findings. Moreover, we basedonNDS.Thefirstmetricisthemeancorruptionerror
introduce variability within each severity to ensure diver- (mCE),whichisappliedtomeasuretherelativerobustnessof
sity. Comprehensively, our benchmark consists of 866,736 candidatemodelscomparedtothebaselinemodel:
images,eachwitharesolutionof1600×900pixels.
blacW koe ua tl sso inco on us rid ce ar ms ec re an -a Lr ii Do As Rsim fuu sl ia ot ning algco om ritp hl met se .s Ee vn es ro yr CEi = (cid:80)(cid:80)
3
3 l= (1 1( −1− NDN SD bS as) ei l, inl
e)
, mCE= N1 (cid:88)N CEi , (1)
l=1 i,l i=1
pixelissettozerowhensimulatingthecamera’sabsence.To
where i denotes the corruption type and l is the severity
emulatethemissingofLiDARreadings,onlythedatapoints
within a [−45, 45] degree frontal field-of-view (FOV) are level; N denotes the number of corruption types in our
benchmark.Itshouldbenotedthatonecanchoosedifferent
retained.Suchadesignchoiceisrootedinourobservations
baselinemodels.Inthiswork,weresorttoDETR3D[4]asit
thatmulti-modaltrainedmodelssufferfromsignificantper-
offersaseminarBEVdetectionperformance.Tocomparethe
formancedropswhenLiDARreadingsareentirelyabsent.
performancediscrepancybetweennuScenes-Candthestandard
nuScenes dataset [1], we define a simple mean resilience
4.2 NaturalCorruptions
rate(mRR)metric,whichiscalculatedacrossthreeseverity
A visual guide to our corruption taxonomy is presented in levelsasfollows:
Figure 1. Broadly, we focus on three corruption categories.
F suir cs ht, at sh vo as re yii nn gdu ilc lued mib ny ate iox nte orn ra ml ee tn eov rir oo lon gm ice an lt ea xl td rey mna em s,i ac rs e, RRi = (cid:80) 3×3 l= N1N DSDSi,l , mRR= N1 (cid:88)N RRi . (2)
clean i=1
simulated via Brightness, Dark, Fog, and Snow. Considering
In our benchmark, we report both metrics for each BEV
thebulkoftrainingdataiscapturedunderrelativelybenign
modelandcomparetherobustnessbasedonthem.
conditions,testingmodelsundertheseextremesiscrucial.
Secondly,sensor-drivendistortionscancorruptcollected
imagery. High-speed motion may induce blur, or memory 5 BENCHMARK EXPERIMENTS
conservation tactics might compel image quantization. To
This section delineates the experimental results to assess
mimicthesereal-worldchallenges,wehaveintegratedMo-
the robustness of BEV algorithms under varied corrupted
tionBlurandColorQuant.
scenarios. Subsection 5.1 details the experimental setup
Lastly, we introduce camera malfunctions, where entire
andmethodology.Subsection5.2discussestheperformance
image sets or random frames are omitted due to hardware
of camera-only BEV models on the nuScenes-C dataset,
issues. This is captured by our novel Camera Crash and
highlighting the impact of different corruptions on model
Frame Lost corruptions. The illustration of these processes
robustness.Subsection5.3explorestheresilienceofcamera-
is visualized in Figure 1. We visualize the pixel histogram
LiDAR fusion models to sensor-specific corruptions and
analysisonoursynthesizedimages,asshowninFigure2.A
complete sensor failures. Finally, Subsection 5.4 evaluates
notableobservationisthattheMotionBlurcorruption,while
therealismandvalidityofthesyntheticcorruptionusedin
inducingminimalpixeldistributionshifts,stillcausedasig-
theexperiments.
nificantperformancedip.Additionalexperimentalfindings
andresultsarediscussedindetailinSection5.
5.1 ExperimentalSettings
4.3 RobustnessMetrics
In our study, we use the official model configurations and
WefollowtheofficialnuScenesmetric[1]tocalculaterobust- public checkpoints provided by open-sourced codebases,
nessmetricsonthenuScenes-Cdataset.WereportnuScenes whenever applicable; we also train additional model vari-
DetectionScore(NDS)andmeanAveragePrecision(mAP), ants with minimal modifications to conduct experiments
along with mean Average Translation Error (mATE), mean under controlled settings. To facilitate access to all model6
TABLE2
BEVmodelcalibration.Pretrain:modelinitializedfrompretrainedFCOS3D[9]checkpoint;Temporal:modelutilizestemporalinformation;Depth:
modelwithexplicitdepthestimationbranchusedinthepipeline;CBGS:modelusestheclass-balancedgroup-samplingtrainingstrategy[46].The
mCEandmRRscoresaregiveninpercentage(%).Bold:Bestinthecategory.Underline:Secondbestinthecategory.
Model Pretrain Temporal Depth CBGS Backbone BEVEncoder ImageSize NDS↑ mCE↓ mRR↑
DETR3D[4] ✓ ResNet Attention 1600×900 0.4224 100.00 70.77
DETR3DCBGS[4] ✓ ✓ ResNet Attention 1600×900 0.4341 99.21 70.02
BEVFormer(small)[2] ✓ ✓ ResNet Attention 1280×720 0.4787 101.23 59.07
BEVFormer-S(small)[2] ✓ ResNet Attention 1280×720 0.2622 114.43 76.87
BEVFormer(base)[2] ✓ ✓ ResNet Attention 1600×900 0.5174 97.97 60.40
BEVFormer-S(base)[2] ✓ ResNet Attention 1600×900 0.4129 101.87 69.33
PETR(r50)[7] ResNet Attention 1408×512 0.3665 111.01 61.26
PETR(vov)[7] ✓ VoVNet-V2 Attention 1600×640 0.4550 100.69 65.03
ORA3D[52] ✓ ResNet Attention 1600×900 0.4436 99.17 68.63
PolarFormer(r101)[53] ✓ ResNet Attention 1600×900 0.4602 96.06 70.88
PolarFormer(vov)[53] ✓ VoVNet-V2 Attention 1600×900 0.4558 98.75 67.51
SRCN3D(r101)[54] ✓ ResNet CNN+Attn. 1600×900 0.4286 99.67 70.23
SRCN3D(vov)[54] ✓ VoVNet-V2 CNN+Attn. 1600×900 0.4205 102.04 67.95
Sparse4D(r101)[55] ✓ ✓ ResNet CNN+Attn. 1600×640 0.5438 100.01 55.04
BEVDet(r50)[3] ✓ ✓ ResNet CNN 704×256 0.3770 115.12 51.83
BEVDet(r101)[3] ✓ ✓ ResNet CNN 704×256 0.3877 113.68 53.12
BEVDet(r101)[3] ✓ ✓ ✓ ResNet CNN 704×256 0.3780 112.80 56.35
BEVDet(tiny)[3] ✓ ✓ SwinTrans CNN 704×256 0.4037 116.48 46.26
BEVDepth(r50)[22] ✓ ✓ ResNet CNN 704×256 0.4058 110.02 56.82
BEVerse(swin-t)[51] ✓ ✓ ✓ SwinTrans CNN 704×256 0.4665 110.67 48.60
BEVerse-S(swin-t)[51] ✓ ✓ SwinTrans CNN 704×256 0.1603 137.25 28.24
BEVerse(swin-s)[51] ✓ ✓ ✓ SwinTrans CNN 1408×512 0.4951 117.82 49.57
BEVerse-S(swin-s)[51] ✓ ✓ SwinTrans CNN 1408×512 0.2682 132.13 29.54
SOLOFusion(short)[65] ✓ ✓ ResNet CNN 704×256 0.3907 108.68 61.45
SOLOFusion(long)[65] ✓ ✓ ResNet CNN 704×256 0.4850 97.99 64.42
SOLOFusion(fusion)[65] ✓ ✓ ✓ ResNet CNN 704×256 0.5381 92.86 64.53
 P & (  P 5 5        6 Q R Z
   
   
     
         ' D U N
 ) R J
               4 X D Q W
 0 R W L R Q
   
   
     
     % U L J K W
   
                                        1 ' 6                                         1 ' 6                  
(a)mCEvs.NDS (b)mRRvs.NDS (c)DepthEst 5im H Va L Ot Li Ho Qn FE H r 5ro Dr W Hvs.RR.
Fig.3.(a):ThemCEmetricshowsalinearrelationshipwith“clean”performancewhile(b):themRRmetricconfrontstheriskofdecreasing.(c):We
observestrongcorrelationswherelargedepthestimationerrorsunderSnow andDark tendtocausedrasticperformancedrops.
checkpointsandconfigurations,wehavecompileda“model Furthermore,foracomprehensiveoverview,metricsfor
zoo”,whichcanbeaccessedthroughourrepository1. each corruption type were deduced by averaging results
We re-implemented several models, including BEVDet across all three severity levels. In our study, DETR3D [4]
(r101) [3], PolarFormer (vov) [53], and SRCN3D (vov) [54], waschosenasthebaselineforthemCEmetric.Ourresearch
tailored to our investigative requirements. For BEVDet methodologyandthecorrespondingcodewereconstructed
(r101), to ensure fair comparisons, we choose to preserve upontheMMDetection3Dcodebase[40],[41].
the input resolution consistent with BEVDet (r50). For the
original versions of both PolarFormer (vov) and SRCN3D 5.2 Camera-OnlyBenchmark
(vov), the models were initialized using checkpoints from We undertook an exhaustive benchmark analysis of 30
DD3D[109],whichhadpreviouslytrainedonthenuScenes contemporary BEV models on the nuScenes-C dataset. The
trainval set, causing information leakage. To mitigate this primary outcomes of our investigations are encapsulated
and ensure fair comparisons, we re-implemented the two in Table 2. Our analysis revealed that all models exhibit a
models,initiatingthemviatheFCOS3D[9]model,without declineinperformanceacrossthecorrupteddataset.
further alterations. Specifically, the VoVNet-V2 iterations
[63]oftheFCOS3Dmodelswerefirsttrainedfordepthesti- 5.2.1 3DObjectDetection
mationontheDDAD15Mdataset[64]andthenunderwent A notable trend emerges when examining the absolute
fine-tuningonthenuScenestrainingset. performancesonboththenuScenes-Candits“clean”coun-
terpart.Specifically,BEVdetectorsexhibitingsuperbperfor-
mance on the standard dataset also tend to showcase com-
1.TheRoboBEVmodelzooispubliclyaccessibleathttps://github.
com/Daniel-xsy/RoboBEV/blob/master/zoo/README.md. mendableperformancewhenfacedwithout-of-distribution
 U R U U (  K W S H '7
TABLE3
TheCorruptionError(CE)ofeachBEVdetectorinourRoboBEV benchmark.TheCEandmCEscoresaregiveninpercentage(%).Bold:Bestin
thecategory.Underlined:Bestintherowifimproveduponbaseline.†:distinguishpre-trainingversionBEVDet.
Model NDS↑ mCE↓ Camera Frame Quant Motion Bright Dark Fog Snow
DETR3D[4] 0.4224 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00
DETR3DCBGS[4] 0.4341 99.21 98.15 98.90 99.15 101.62 97.47 100.28 98.23 99.85
BEVFormer(small)[2] 0.4787 102.40 101.23 101.96 98.56 101.24 104.35 105.17 105.40 101.29
BEVFormer(base)[2] 0.5174 97.97 95.87 94.42 95.13 99.54 96.97 103.76 97.42 100.69
PETR(r50)[7] 0.3665 111.01 107.55 105.92 110.33 104.93 119.36 116.84 117.02 106.13
PETR(vov)[7] 0.4550 100.69 99.09 97.46 103.06 102.33 102.40 106.67 103.43 91.11
ORA3D[52] 0.4436 99.17 97.26 98.03 97.32 100.19 98.78 102.40 99.23 100.19
PolarFormer(r101)[53] 0.4602 96.06 96.16 97.24 95.13 92.37 94.96 103.22 94.25 95.17
PolarFormer(vov)[53] 0.4558 98.75 96.13 97.20 101.48 104.32 95.37 104.78 97.55 93.14
SRCN3D(r101)[54] 0.4286 99.67 98.77 98.96 97.93 100.71 98.80 102.72 99.54 99.91
SRCN3D(vov)[54] 0.4205 102.04 99.78 100.34 105.13 107.06 101.93 107.10 102.27 92.75
Sparse4D(r101)[55] 0.5438 100.01 99.80 99.91 98.05 102.00 100.30 103.83 100.46 95.72
BEVDet(r50)[3] 0.3770 115.12 105.22 109.19 111.27 108.18 123.96 123.34 123.83 115.93
BEVDet(tiny)[3] 0.4037 116.48 103.50 106.61 113.18 107.26 130.19 131.83 124.01 115.25
BEVDet(r101)[3] 0.3877 113.68 103.32 107.29 109.25 105.40 124.14 123.12 123.28 113.64
BEVDet(r101†)[3] 0.3780 112.80 105.84 108.68 101.99 100.97 123.39 119.31 130.21 112.04
BEVDepth(r50)[22] 0.4058 110.02 103.09 106.26 106.24 102.02 118.72 114.26 116.57 112.98
BEVerse(swin-t)[51] 0.4665 110.67 95.49 94.15 108.46 100.19 122.44 130.40 118.58 115.69
BEVerse(swin-s)[51] 0.4951 107.82 92.93 101.61 105.42 100.40 110.14 123.12 117.46 111.48
SOLOFusion(short)[65] 0.3907 108.68 104.45 105.53 105.47 100.79 117.27 110.44 115.01 110.47
SOLOFusion(long)[65] 0.4850 97.99 95.80 101.54 93.83 89.11 100.00 99.61 98.70 105.35
SOLOFusion(fusion)[65] 0.5381 92.86 86.74 88.37 87.09 86.63 94.55 102.22 90.67 106.64
TABLE4
BenchmarkresultsofperceptiontasksincludingMapSegmentation(MS),DepthEstimation(DE),andSemanticOccupancyPrediction(SOP).
Tasks Model Metric Clean Camera Frame Quant Motion Bright Dark Fog Snow
MS CVT[25] IoU↑ 0.348 0.200 0.170 0.294 0.281 0.275 0.200 0.247 0.177
DE SurroundDepth[70] AbsRel↓ 0.280 0.485 0.497 0.334 0.338 0.339 0.354 0.320 0.423
SOP TPVFormer[72] mIoU↑ 0.521 0.274 0.229 0.381 0.386 0.490 0.373 0.466 0.193
SOP SurroundOcc[71] SCIoU↑ 0.314 0.199 0.181 0.258 0.225 0.307 0.248 0.296 0.183
datasets, a trend visually represented in Figure 3a. Nev- model’s robustness, we dissected BEV models based on
ertheless, looking more closely at these results reveals a components like training strategies (e.g., pre-training and
morecomplexstory.Detectors,despiteparallelperformance CBGS[46]resampling),modelarchitectures(e.g.,backbone),
on the “clean” dataset, display varied robustness when andlearningtechniques(e.g.,temporalcuelearning).
confrontedwithdiversecorruptiontypes.Forexample,BE-
Verse(swin-s)[51]showsstrongresilienceduringaCamera
Crash,andPETR(vov)[7]performswellinSnowconditions.
However,bothperformpoorlyinDarksettings.
5.2.2 OtherBEVPerceptionTasks
Our investigations further highlight a potential vul-
nerability in resilience rates across various corruptions.
Even though the mCE metric displays a linear correla-
Our inquiry also extended to associated tasks, including
tion between the nuScenes and nuScenes-C datasets, the
BEV-centric map segmentation, depth estimation, and oc-
mRR metric elucidates notable disparities among models
cupancy prediction, with outcomes presented in Table 4.
with comparable baseline performance. This suggests po-
Adhering to setting 1 from [25], we reported the Intersec-
tential overfitting of some models to the nuScenes dataset,
tion over Union (IoU) for vehicle map-view segmentation
thereby compromising their adaptability to the nuScenes-C
results. For depth estimation, we employed the Absolute
dataset. For instance, despite Sparse4D [55] outperforming
Relative Difference (Abs Rel) score, and for semantic occu-
DETR3D [4] on the “clean” dataset, it falls short in terms
pancyprediction,weusedthemeanIntersectionoverUnion
of mRR metrics across all corruption categories. Moreover,
(mIoU). For comprehensive metric definitions, readers can
DETR3D performs exceptionally well in Dark conditions,
consult the original publications [25], [70], [71], [72]. These
in stark contrast to BEVerse (swin-t). Despite its strong
results,spanningdiverseperceptiontasks,offeranenriched
performance in clean conditions, BEVerse (swin-t) achieves
perspective on BEV model capabilities and constraints. It
only12%relativeperformanceinthedark.Therefore,acom-
is worth noting that many BEV-centric perception models
prehensive assessment of cutting-edge models is essential
struggle with specific corruptions, such as Dark and Snow.
foracompleteevaluationoftheircapabilities.
This reveals a common vulnerability among BEV models,
As shown in Table 2, to gain deeper insights into the compromisingtheirreliabilityinreal-worldscenarios.8
TABLE5
TheBEVdetectionresults(NDS)offusion-basedmodelsunderdifferentinputmodalities.SinceFogandSnowcanalsoaffecttheLiDARsensors,
thesetwotypesofcorruptionsarenotconsideredfortheimage-LiDAR-fusionmodels.
Model Camera LiDAR Clean Camera Frame Quant Motion Bright Dark Fog Snow
BEVFusion[66] ✓ 0.4121 0.2777 0.2255 0.2763 0.2788 0.2902 0.1076 0.3041 0.1461
BEVFusion[66] ✓ 0.6928 − − − − − − − −
BEVFusion[66] ✓ ✓ 0.7138 0.6963 0.6931 0.7044 0.6977 0.7018 0.6787 − −
TransFusion[68] ✓ ✓ 0.6887 0.6843 0.6447 0.6819 0.6749 0.6843 0.6663 − −
AutoAlignV2[69] ✓ ✓ 0.6139 0.5849 0.5832 0.6006 0.5901 0.6076 0.5770 − −
TABLE6
Benchmarkresultsforrobustnessundersensorfailures.Themodelsaretrainedusingmulti-modalinputswhiletestedusingsinglesensorinputs.
Model Train Camera LiDAR NDS↑ mAP↑ mATE mASE mAOE mAVE mAAE
BEVFusion[66] C ✓ 0.4122 0.3556 0.6677 0.2727 0.5612 0.8954 0.2593
BEVFusion[66] L ✓ 0.6927 0.6468 0.2912 0.2530 0.3142 0.2627 0.1858
BEVFusion[66] C+L ✓ ✓ 0.7138 0.6852 0.2874 0.2539 0.3044 0.2554 0.1874
BEVFusion[66] C+L ✓ 0.3340(↓0.3798) 0.0789(↓0.6063) 0.5044 0.3073 0.4999 0.5098 0.2338
BEVFusion[66] C+L ✓ 0.6802(↓0.0605) 0.6247(↓0.0605) 0.2948 0.2590 0.3137 0.2697 0.1844
TransFusion[68] C+L ✓ ✓ 0.6887 0.6453 0.2995 0.2552 0.3209 0.2765 0.1877
TransFusion[68] C+L ✓ 0.3470(↓0.3417) 0.0343(↓0.6110) 0.4087 0.3091 0.4446 0.3104 0.2282
TransFusion[68] C+L ✓ 0.6464(↓0.0423) 0.5764(↓0.0689) 0.3171 0.2761 0.3227 0.3124 0.1897
AutoAlignV2[69] C+L ✓ ✓ 0.6139 0.5649 0.3300 0.2699 0.4226 0.4644 0.1983
AutoAlignV2[69] C+L ✓ 0.5651(↓0.0448) 0.4794(↓0.0855) 0.3463 0.2734 0.4361 0.4894 0.2007
5.3 Camera-LiDARFusionBenchmark sensors fails. We evaluate the performance of our multi-
modalmodelusinginputfromonlyasinglemodality,with
5.3.1 CameraSensorCorruptions
results presented in Table 6. When simulating camera fail-
We studied scenarios where cameras are impaired while
ure,allpixelvaluesaresettozero.ForLiDARsensorfailure,
LiDAR operates optimally, a frequent occurrence in real-
we discovered that no model could perform adequately
world conditions. For instance, LiDAR point cloud cap-
when all point data are absent (i.e., the NDS falls to zero).
ture remains largely unhampered by lighting variations, Hence, we retain only the points within a [−45, 45] degree
whereas camera captures can degrade under limited light.
rangeinfrontofthevehicleanddiscardallothers.
Intentionally, we excluded conditions like Snow and Fog,
Interestingly, our findings indicate that multi-modal
as they could introduce noise to both camera and LiDAR
models are disproportionately reliant on LiDAR input. In
readings. Results of these studies are depicted in Table 5.
scenarios where LiDAR data is missing, the mAP metrics
Interestingly,multi-modalfusionmodelsmaintainhighper-
for BEVFusion [66] and Transfusion [68] drop by 89% and
formance even when the camera data is compromised.
95%, respectively. In contrast, the absence of image data
When provided with benign LiDAR and degraded camera
leads to a much milder decline in performance. This phe-
inputs,BEVFusion[66]consistentlyoutperformsitsLiDAR-
nomenonunderscoresthat,duringthetrainingphase,point
onlycounterpart,withanotablyhigherNDSscoreof0.6928,
cloudfeaturesmaydisproportionatelyinfluencethemodel,
across most types of camera corruptions, except Dark. This
thereby asserting dominance over image-based features in
affirms the efficacy of using LiDAR data even when the
perception tasks. Such a dependence on LiDAR data intro-
cameradataissuboptimal.
duces a significant vulnerability to multi-modal perception
However,therearecircumstanceswherecorruptedcam-
models, particularly because LiDAR sensors are prone to
era inputs adversely affect the model’s performance. For
data corruption under adverse weather conditions such as
example, under conditions such as Camera Crash and Mo-
rain, snow, and fog. These observations necessitate further
tion Blur, the benefits of incorporating camera features
research focused on enhancing the robustness of multi-
into the model are marginal. Moreover, in the presence of
modal perception systems, especially when one sensory
Dark corruption, corrupted camera features not only fail to
modalityisentirelyabsent.
provide useful information but also diminish the efficacy
of LiDAR features, leading to a performance drop from
an NDS score of 0.6928 to 0.6787. As a result, enhancing
the robustness of multi-modal fusion models against input
5.4 ValidityAssessment
corruptionemergesasacrucialavenueforfutureresearch.
Since the corruption images are synthesized digitally, it is
5.3.2 CompleteSensorFailure
important to study how close they are compared to real-
Multi-modalfusionmodelsaretypicallytrainedusingdata world corruption. To study the validity of synthesized im-
from both camera and LiDAR sensors. However, the de- ages, we conducted two experiments, including the pixel
ployedmodelmustfunctionadequatelyevenifoneofthese distributionstudyandcorruption-augmentedtraining.9
TABLE7
TheResilienceRate(RR)ofeachBEVdetectorinourRoboBEVbenchmark.Bold:bestwithinthecorruptions.Underlined:Bestacross
corruptions.Symbol†distinguishesthepre-trainingversionBEVDet.TheRRscoresareinpercentage(%)andthehigherthebetter.
Model NDS↑ mRR↑ Camera Frame Quant Motion Bright Dark Fog Snow
DETR3D[4] 0.4224 70.77 67.68 61.65 75.21 63.00 94.74 65.96 92.61 45.29
DETR3DCBGS[4] 0.4341 70.02 68.90 61.85 74.52 58.56 95.69 63.72 92.61 44.34
BEVFormer(small)[2] 0.4787 59.07 57.89 51.37 68.41 53.69 78.15 50.41 74.85 37.79
BEVFormer(base)[2] 0.5174 60.40 60.96 58.31 67.82 52.09 80.87 48.61 78.64 35.89
PETR(r50)[7] 0.3665 61.26 63.30 59.10 67.45 62.73 77.52 42.86 78.47 38.66
PETR(vov)[7] 0.4550 65.03 64.26 61.36 65.23 54.73 84.79 50.66 81.38 57.85
ORA3D[52] 0.4436 68.63 68.87 61.99 75.74 59.67 91.86 58.90 89.25 42.79
PolarFormer(r101)[53] 0.4602 70.88 68.08 61.02 76.25 69.99 93.52 55.50 92.61 50.07
PolarFormer(vov)[53] 0.4558 67.51 68.78 61.67 67.49 51.43 93.90 53.55 89.10 54.15
SRCN3D(r101)[54] 0.4286 70.23 68.76 62.55 77.41 60.87 95.05 60.43 91.93 44.80
SRCN3D(vov)[54] 0.4205 67.95 68.37 61.33 67.23 50.96 92.41 54.08 89.75 59.43
Sparse4D(r101)[55] 0.5438 55.04 52.83 48.01 60.87 46.23 73.26 46.16 71.42 41.54
BEVDet(r50)[3] 0.3770 51.83 65.94 51.03 63.87 54.67 68.04 29.23 65.28 16.58
BEVDet(tiny)[3] 0.4037 46.26 64.63 52.39 56.43 52.71 54.27 12.14 60.69 16.84
BEVDet(r101)[3] 0.3877 53.12 67.63 53.26 65.67 58.42 65.88 28.84 64.35 20.89
BEVDet†(r101)[3] 0.3780 56.35 64.60 51.90 80.45 68.52 68.76 36.85 54.84 24.84
BEVDepth(r50)[22] 0.4058 56.82 65.01 52.76 67.79 61.93 70.95 43.30 71.54 21.27
BEVerse(swin-t)[51] 0.4665 48.60 68.19 65.10 55.73 56.74 56.93 12.71 59.61 13.80
BEVerse(swin-s)[51] 0.4951 49.57 67.95 50.19 56.70 53.16 68.55 22.58 57.54 19.89
SOLOFusion(short)[65] 0.3907 61.45 65.04 56.18 71.77 66.62 75.92 52.03 76.73 27.28
SOLOFusion(long)[65] 0.4850 64.42 65.13 51.34 74.19 71.34 82.52 58.02 82.29 30.52
SOLOFusion(fusion)[65] 0.5381 64.53 70.73 64.37 75.41 67.68 80.45 48.80 83.26 25.57
TABLE8
Validitystudyofusingcorruptionsasdataaugmentationsduringthemodeltrainingtoimprovethecross-domainrobustnessofBEVdetectors.
Model TrainingSet TestingSet CorruptAug NDS↑ mAP↑ mATE mASE mAOE mAVE mAAE
DETR[4] nuScenestrain nuScenesval 0.4224 0.3468 0.7647 0.2678 0.3917 0.8754 0.2108
DETR[4] nuScenestrain nuScenesval ✓ 0.4242(↑0.0018) 0.3511 0.7655 0.2736 0.4130 0.8487 0.2119
FCOS3D[9] daytrain dayval 0.3867 0.3045 0.7651 0.2576 0.5001 1.2102 0.1321
FCOS3D[9] daytrain dayval ✓ 0.3883(↑0.0196) 0.3073 0.7630 0.2581 0.5043 1.1782 0.1286
FCOS3D[9] daytrain nightval 0.0854 0.0162 1.0434 0.6431 0.8241 1.8505 0.7597
FCOS3D[9] daytrain nightval ✓ 0.1245(↑0.0391) 0.0265 1.0419 0.4658 0.8145 2.2727 0.6067
FCOS3D[9] drytrain dryval 0.3846 0.2970 0.7744 0.2541 0.4721 1.3199 0.1380
FCOS3D[9] drytrain dryval ✓ 0.3854(↑0.0008) 0.2992 0.7654 0.2582 0.4824 1.3334 0.1361
FCOS3D[9] drytrain rainval 0.3203 0.2151 0.8994 0.2856 0.5253 1.7129 0.1619
FCOS3D[9] drytrain rainval ✓ 0.3302(↑0.0099) 0.2266 0.8595 0.2719 0.5559 1.5697 0.1439
5.4.1 PixelDistributions model should achieve better generalizability than the
Assuming that a corruption simulation is realistic enough “clean” model when tested on real-world corruption
to reflect real-world situations, the distribution of a cor- datasets.Also,thecorruption-augmentedmodelshouldalso
rupted “clean” set should be similar to that of the real- show better performance on the clean dataset. We vali-
world corruption set. We validate this using ACDC [110], date this using nuScenes, nuScenes-Night, and nuScenes-
nuScenes[1],Cityscapes[112],andFoggy-Cityscapes[111], Rain. We adopt FCOS3D [9] as the baseline and train the
sincethesedatasetscontainreal-worldcorruptiondataand model with corruption augmentation. For nuScenes-Night
cleandatacollectedbythesamesensortypesfromthesame andnuScenes-Rain,wetrainthemodelontheDay-trainand
physical locations. We simulate corruptions using “clean” Dry-trainsplitandevaluateontheDay-val,Night-val,Dry-
images and compare the distribution patterns with their val, and Rain-val split. The results can be seen in Table 8.
correspondingreal-worldcorrupteddata.Wedothistoen- We observe that using synthesized images as the data aug-
surethatthereisnoextradistributionshiftfromaspectslike mentation strategy successfully improves the cross-domain
sensor difference (e.g., FOVs and resolutions) and location robustness. Specifically, in day-to-night domain transitions,
discrepancy(e.g.,environmentalandsemanticchanges).As we observe a significant performance drop from 0.3867
illustrated here, the pixel distributions of our synthesized to 0.0854 in the baseline model due to the large domain
imagesexhibitahighdegreeofresemblancetothoseofreal- gap.However,whentrainedwithcorruptionaugmentation,
world data, thereby affirming the dataset’s validity from a the model’s cross-domain performance improves by 45.8%,
pixelstatisticalperspective. therebyvalidatingthevalidityofoursynthesizedimages.
5.4.2 Corruption-AugmentedTraining
Furthermore,ifthecorruptionsimulationisrealisticenough
to reflect real-world situations, a corruption-augmented10
TABLE9
Comparisonsbetweenthebaselinemodelsandmodelswithcorruptionaugmentations.ThenumbersshowthenuScenesdetectionscore(NDS).
Model CorruptAug Clean Crash Frame Quant Motion Bright Dark Fog Snow mRR↑
BEVFormer[2] 0.5174 0.3154 0.3017 0.3509 0.2695 0.4184 0.2515 0.4069 0.1857 0.6040
BEVFormer[2] ✓ 0.5169 0.3267 0.3069 0.4102 0.4028 0.4418 0.3642 0.4379 0.3808 0.7427
DETR3D[4] 0.4224 0.2859 0.2604 0.3177 0.2661 0.4002 0.2786 0.3912 0.1913 0.7077
DETR3D[4] ✓ 0.4252 0.2956 0.2462 0.3923 0.3850 0.4159 0.3622 0.4135 0.3827 0.8506
PETR[7] 0.4550 0.2924 0.2792 0.2968 0.2490 0.3858 0.2305 0.3703 0.2632 0.6503
PETR[7] ✓ 0.4260 0.2807 0.2567 0.3947 0.4003 0.4138 0.3709 0.4137 0.3846 0.8555
PETRv2[101] 0.4860 0.4016 0.3928 0.4418 0.4053 0.4597 0.4129 0.4467 0.3994 0.8642
PETRv2[101] ✓ 0.4924 0.4114 0.4029 0.4633 0.4656 0.4734 0.4541 0.4712 0.4601 0.9144
BEVDet[3] 0.3500 0.2313 0.1948 0.2435 0.2148 0.2599 0.1408 0.2572 0.0969 0.5854
BEVDet[3] ✓ 0.3326 0.2274 0.1891 0.3033 0.3117 0.3186 0.2600 0.3083 0.2661 0.8210
dataset, particularly for corruption types that originally
Align detector head to posed significant challenges to the models (e.g., Motion
CLIP representation
Blur,Snow).However,augmentingtrainingdatawithsensor
clean
clean corruption scenarios, such as the omission of camera in-
Pre-trained snow Random formation, does not significantly improve robustness. This
CLIP Initialized outcome underscores the necessity for future research to
fog
develop more sophisticated modules capable of handling
fog
… CLIPRepresentation Space incompleteinputscenarios.
snow Step1: Detection Head Alignment
5.5.2 FoundationModelBackbone
Fine-tune CLIP representation
space to robust BEV perception Recent studies [115], [116] have demonstrated that founda-
tion models, trained on internet-scale datasets in an un-
clean
supervised manner, exhibit notable generality compared
clean
Fine-tuned snow CLIP to models trained on conventional datasets (e.g., Ima-
CLIP Aligned
geNet [117]). Inspired by these findings, we explore the
fog
fog potential to transfer the generality of foundation models
…
Detection Adapted to BEV perception tasks. Specifically, we investigate three
CLIPRepresentation Space
distinct approaches for utilizing the CLIP backbone in our
models: (1) freezing the backbone while training only the
snow Step2: End-to-end fine-tuning detector head; (2) fine-tuning both the backbone and the
Fig.4.TwostepstotransferCLIP[113]robustnesstoBEVDet[3].The detector head; (3) freezing the backbone and training the
firststepistoalignthedetectionheadtothefrozenCLIPbackbonewith detection head, then fine-tuning the whole model together,
corruption-augmented inputs. The second step end-to-end fine-tunes since previous work [118] shows that end-to-end fine-
thebackboneanddetectionheadtoenhancetherobustness.
tuning, even though improve in-distribution performance,
cancompromiserobustnessonout-of-distributiondataset.
5.5 RobustnessImprovement
ThethreemethodsareshowninFigure4andtheresults
Build-uponourexperiment,wefurtherstudyhowtofunda- are shown in Table 10. The first observation is CLIP is not
mentallyimprovetherobustnessofBEVperceptionmodels. welloptimizedtowardsBEVperceptiontasks,indicatedby
In Subsection 5.5.1, we simply use the corruption as data thelowbenignperformancewhenwefreezetheCLIPback-
augmentationasinSubsection5.4.2.InSubsection5.5.2,we bone. Additionally, end-to-end fine-tuning with randomly
resorttoadvancedCLIP[113]asthemodelbackbonesince initialized detection heads leads to minimal improvement.
CLIP is well-known to show promising out-of-distribution Interestingly, when applying corruption augmentation, the
generality[115],[116]. CLIP backbone shows little improvement to the baseline
model. The improvement in mRR is only 0.56 and, when
5.5.1 Corruption-AugmentationTraining equipped with corruption augmentation, the mRR is even
We investigate the utilization of corruption as a data aug- lower. Finally, our two-stage training can effectively im-
mentation strategy during the training phase. To system- prove the performance while also transferring the robust-
atically assess the effectiveness of this approach, we apply nessofCLIPtowardsBEVperceptiontasks,especiallywith
corruption augmentation to five models within our bench- equipped with corruption augmentation. The robustness
mark,withresultsdetailedinTable9.Ourfindingsindicate improvementsurpassestheend-to-endfine-tunedCLIPbya
thatcorruptionaugmentationsubstantiallyenhancesperfor- notablemargin.Forinstance,theNDSscoreunderDark,Fog,
mance against semantic corruptions within our proposed andSnowimproveby23.1%,11.8%,and15.8%respectively.
guAtpurroC
guAtpurroC11
Clean Bright Dark Snow Fog MotionBlur ColorQuant
Fig.5.DepthestimationresultsofBEVDepth[22]underdifferentcorruptiontypes.Theresultsexhibitadifferentsensitivityforeachscenario.
TABLE10
WeimplementtheCLIP[113]backbonetoimprovetherobustnessofBEVDet[3].WeobservethatthefrozenCLIPbackbonecannottransferwell
toBEVperceptiontasks.HeadAlignmentcanfurtherimprovetheperformancewhileretainingrobustness.
Backbone Freeze HeadAlign CorruptAug Clean Crash Frame Quant Motion Bright Dark Fog Snow mRR↑
Baseline - - 0.3500 0.2313 0.1948 0.2435 0.2148 0.2599 0.1408 0.2572 0.0969 58.54
Baseline - - ✓ 0.3326 0.2274 0.1891 0.3033 0.3117 0.3186 0.2600 0.3083 0.2661 82.10
CLIP[113] ✓ 0.2223 0.1544 0.1179 0.1574 0.0816 0.1563 0.0651 0.1659 0.0371 52.61
CLIP[113] ✓ ✓ 0.2093 0.1447 0.1110 0.1840 0.1808 0.1826 0.1127 0.1939 0.0916 71.74
CLIP[113] 0.3609 0.2409 0.2090 0.2422 0.2163 0.2804 0.1317 0.2739 0.1119 59.10
CLIP[113] ✓ 0.3434 0.2330 0.2098 0.3109 0.3217 0.3192 0.2374 0.3154 0.2735 80.84
CLIP[113] ✓ 0.3710 0.2547 0.2121 0.2562 0.2280 0.2940 0.1508 0.2920 0.1282 61.18
CLIP[113] ✓ ✓ 0.3667 0.2514 0.2140 0.3430 0.3484 0.3553 0.2922 0.3527 0.3166 84.32
TABLE11 depth estimation, leading to the largest performance drop.
Ablationstudyonmodelpre-training.Row(a):ThePETR[7]baseline. These results provide further support for our conclusion
Row(b):ThemodelwiththeM-BEV[114]pre-training.Weobservethat
that the performance of depth-based approaches can suffer
maskedpre-trainingeffectivelyimprovesrobustnesstosensorfailure.
significantlyifthedepthestimationisnotaccurateenough.
# Model Crash Frame Quant Motion Thedepthestimationresultsundercorruptionscanbeseen
from Figure 5, where we can see significant differences
(a) PETR[7] 0.2347 0.2336 0.2915 0.2695
under certain corruptions (e.g., Snow) compared to “clean”
(b) w/ M-BEV[114] 0.2759 0.2715 0.3175 0.2860
inputs.
# Model Bright Dark Fog Snow
(a) PETR[7] 0.3029 0.2648 0.2919 0.2463
(b) w/ M-BEV[114] 0.3298 0.2844 0.3191 0.2583
6.2 ModelPre-Training
- Pre-training improves robustness across a wide range of se-
6 ANALYSIS AND DISCUSSION
mantic corruptions while does not help with temporal corrup-
This section delves into the nuanced relationships between tions. The effectiveness of these strategies for improving
model designs and robustness under various corruptions. model robustness is illustrated in Figure 6a and Figure 6b,
Subsection 6.1 explores the performance of depth estima- where models that utilize pre-training largely outperform
tion models and highlights their susceptibility to image those not. For controlled comparison, we re-implement the
corruptions. Subsection 6.2 discusses the impact of pre- BEVDet (r101) [3] model using the FCOS3D [9] checkpoint
training on model resilience across different corruptions. as initialization. Our results, presented in Figure 10, show
Subsection 6.3 examines the benefits of temporal fusion. that pre-training can significantly improve mRR across a
Subsection 6.4 compares the robustness of different model wide range of corruptions (except Fog) even if it has lower
backbones. Subsection 6.5 analyzes how pixel distribution “clean” NDS (0.3780 vs. 0.3877). Specifically, under Color
changes correlate with model performance under corrup- Quant, Motion Blur, and Dark corruptions, the mRR met-
tion. Finally, Subsection 6.6 provides detailed metrics on ric improves by 22.5%, 17.2%, and 27.8%, respectively. It
howspecificmodelerrors. is worth noting that pre-training mainly improves most
semantic corruptions and does not improve temporal cor-
6.1 DepthEstimation
ruptions. Even though, the pre-trained BEVDet still largely
- Depth-free BEV transformations show better robustness. Our lags behind those depth-free counterparts. Therefore, we
analysis reveals that depth-based approaches suffer from can conclude that pre-training together with the depth-free
severeperformancedegradationwhenexposedtocorrupted bird’seyeviewtransformationprovidesmodelswithstrong
images as shown in Figure 6c and 6d. Moreover, we un- robustness. More recently, M-BEV [114] propose masked
dertake a comparative study to evaluate the intermediate pre-training task to enhance robustness under incomplete
depth estimation results of BEVDepth [22] under corrup- sensor input. We compare a masked pre-trained PETR
tions.Tothisend,wecomputethemeansquareerror(MSE) with the corresponding baseline. The results can be seen
between“clean”inputsandcorruptedinputs.Ourfindings in Table 11. We find M-BEV can most effectively improve
indicate an explicit correlation between vulnerability and robustness towards incomplete sensor output, and reveal
depth estimation error, as presented in Figure 3c. Specifi- the promise of masked image modeling pre-train in BEV
cally,SnowandDarkcorruptionssignificantlyaffectaccurate perceptiontasks.12
 P & (  P 5 5  P & (  P 5 5
                   
                   
                   
 3 U H W U D L Q
      ) & 2 6  '            ' H S W K     
      5 D Q G R P            ' H S W K  H V W L P D W L R Q  I U H H     
 ' ( 7 5  '  ' H S W K  H V W L P D W L R Q  E D V H G
         
                1 ' 6                 1 ' 6                 1 ' 6                 1 ' 6
(a)Pre-train-mCE (b)Pre-train-mRR (c)Depth-mCE (d)Depth-mRR
Fig.6.Ablationstudy.Weobservethatpre-trainingstrategiestogetherwithdepth-freebird’seyeviewtransformationprovidethemodelswithbetter
robustness.WedonotconsiderSOLOFusion[65]long-termfusionheresinceitutilizes16frames,whichismuchlargerthanothermethods.
 & (  7 H P S R U D O   5   5  & (  7 H P S R U D O  5 5
     ) D O V H  ) D O V H
     7 U X H         7 U X H   
   
        
  
           
                1 ' 6                 1 ' 6                 1 ' 6                 1 ' 6
(a)CameraCrash-CE (b)CameraCrash-RR (c)FrameLost-CE (d)FrameLost-RR
Fig.7.Ablationstudy.WeobservethatnotallthemodelswithtemporalfusionexhibitbetterrobustnessunderCameraCrash andFrameLost.
However,theyhaveexhibitedcertainpotentialsincemodelswiththelowestmCEmetricarealwaysthosethatutilizetemporalinformation.
Fig. 8. Backbone comparison: ResNet [120] vs. VoVNet-V2 [63]. We Fig.10.Pre-trainingcomparisons.Wecomparetheresiliencerate(RR)
comparetheabsolutecorruptionerror(i.e.,CE)sincethetwovariants betweenBEVDet[3]withandwithoutusingmodelpre-training.
havesimilar“clean”performances.
Frame Lost corruption. Moreover, the RR metric of its long-
only version outperforms its short-only counterpart on a
wide range of corruption types, which indicates the great
potentialofutilizinglongertemporalinformation.
To further investigate the impact of temporal fusion
on enhancing the corruption robustness, we employ the
BEVFormermodeltoassessfeatureerrorswithandwithout
theintegrationoftemporalinformationcomparedto“clean”
Fig.9.Backbonecomparison:ResNet[120]vs.SwinTransformer[45].
Wecomparetherelativecorruptionerror(i.e.,RR)sincethetwovariants temporalinputs.Wecomputethemeansquarederror(MSE)
havedifferent“clean”performances. betweencorruptedinputs,bothwithandwithouttemporal
information,and“clean”inputswithtemporalinformation.
The results are shown in Figure 12. We note a trend of
6.3 TemporalFusion
increasingerrorovertimeinthetemporalfusionmodel(il-
- Temporal fusion has the potential to yield better absolute per- lustratedbythebluebar),attributabletoerroraccumulation
formanceundercorruptions.Fusinglongertemporalinformation under consecutive corrupted inputs. Despite this, temporal
largelyhelpswithrobustness.Weareparticularlyinterestedin fusion consistently demonstrates an error mitigation effect
examininghowmodelsutilizingtemporalinformationper- acrossframes.
formundertemporalcorruptions.WefindSOLOFusion[65] However, we find that not all models with temporal
which fuses wider and richer temporal information per- fusion exhibit better robustness under Camera Crash and
forms extremely well compared to its short-only and long- Frame Lost. The robustness is highly correlated to how
only versions. In terms of Camera Crash, the short-only and to fuse history frames and how many frames are used,
long-only versions have close resilience rate performance which emphasizes the importance of evaluating temporal
(65.04 vs. 65.13). However, the fusion version improves to fusion strategies from wider perspectives. The results can
70.73,whichisthehighestamongallthecandidatemodels. be seen in Figure 7. Nonetheless, temporal fusion remains
Similarly, the fusion version improves the resilience rate a potential method to enhance temporal robustness since
by almost 10% compared to the other two versions under themodelswiththelowestCorruptionError(orthehighest13
mAVE mAVE mAVE
mATE mATE mATE
mAOE mAOE mAOE
mASE mASE mASE
mAAE mAAE mAAE
0.85 0.85 0.85
0.50 0.50 0.50
0.15 0.15 0.15
Fog Snow Fog Snow Fog Snow
Dark Dark Dark
Bright Bright Bright
Motion Motion Motion
Quant Quant Quant
Frame Frame Frame
CleanCamera CleanCamera CleanCamera
DETR3D (a)DETR3D BEVFormerBase (b)BEVFormer PolarFormer-r101 (c)PolarFormer
mAVE mAVE mAVE
mATE mATE mATE
mAOE mAOE mAOE
mASE mASE mASE
mAAE mAAE mAAE
0.85 0.85 0.85
0.50 0.50 0.50
0.15 0.15 0.15
Fog Snow Fog Snow Fog Snow
Dark Dark Dark
Bright Bright Bright
Motion Motion Motion
Quant Quant Quant
Frame Frame Frame
CleanCamera CleanCamera CleanCamera
BEVDet-R50 (d)BEVDet BEVDepth-R50 (e)BEVDepth BEVerseSmall (f)BEVerse
Fig.11.Benchmarkresultsoftask-specificmetricsreportedonnuScenes-CotherthanNDS,underdifferentcorruptiontypesinourbenchmark.
features extracted by the backbone model, under benign
Error and Percentage Change Across Frames
3.40 10 inputs and corruption inputs. Based on that, we compute
With Temporal Fusion Percentage Decrease
3.35 Without Temporal Fusion therelativeerrorbetweenthe“clean”andcorruptioninputs
8
Gramian matrix. The VoVNet has a relative error of 4.17
3.30
6 and5.43towardsSnowandMotionBlur,respectively.While
3.25 ResNet has errors of 11.47 and 2.10. The results align with
4
3.20 our end-to-end experiments above, where ResNet is more
robust under Motion Blur while VoVNet is more robust
3.15 2
towardsSnow.Theobservedrobustnessdifferencesbetween
3.10 0 ResNet and VoVNet to Snow and Motion Blur stem from
their architectural designs. ResNet’s deep hierarchical fea-
3.05
2
tures and skip connections make it more sensitive to high-
3.00 frequency noise like Snow, resulting in higher errors. Con-
4
2.95 versely, VoVNet’s dense feature aggregation and efficient
0 4 8 12 16 20 24 28 32 36
Frame ID feature use enhance robustness to Snow by mitigating local
Fig. 12. Visualized feature errors between the corruption inputs and noisepatterns.However,VoVNetshowshighererrorsunder
the “clean” inputs. We observe that temporal fusion exhibits a certain Motion Blur. ResNet’s hierarchical feature representation
potentialtomitigatethenoisescausedbycorruptedinputimages.
mightbebettersuitedtohandlingthespatialinconsistencies
caused by simulated Motion Blur. These findings suggest
VoVNet excels in spatial noise resilience, while ResNet
Resilience Rate) are consistently those that utilize temporal
handles spatial distortions better. Furthermore, the finding
information.
can provide insights for feature work to design new loss
functions towards aligning the Gramian matrix of features
6.4 Backbone
extractedbybackboneunderdifferentcorruptions.
- The Swin Transformer is more vulnerable towards the lighting
changings;VoVNet-V2ismorerobustagainstSnowwhileResNet
6.5 Corruptions
shows better robustness across a wide range of corruptions. Al-
thoughResNet[120]andVoVNet[63]exhibitclosestandard The relationship between pixel distribution shifts and model
performance, ResNet-based detectors exhibit consistently performance degradation is not straightforward. We calculate
superior robustness across a wide range of corruptions, as the pixel distribution over 300 images sampled from the
illustrated in Figure 8. Conversely, the VoVNet backbone nuScenesdatasetandvisualizethepixelhistogramsshown
consistently exhibits better robustness under Snow corrup- in Figure 2. Interestingly, the Motion Blur causes the least
tions. Moreover, Swin Transformer [45] based BEVDet [3] pixel distribution shifts while causing a relatively large
demonstrates significant vulnerability towards changes in performancedrop.Ontheotherhand,Brightshiftsthepixel
lighting conditions (e.g., Bright and Dark). A clear com- distribution to higher values, and Fog makes fine-grained
parison can be found in Figure 9. Inspired by [119], we features more indistinct by shifting the pixel value more
compute the Gramian matrix in the feature space in the agminated. However, these two corruptions only lead to
rorrE
)%(
esaerceD
egatnecreP14
Fig.13.VisualizationsofdetectionresultsfromBEVFormer[2].Fromlefttoright:top:GT,Clean,Motion,Quant;bottom:Bright,Dark,Fog,Snow.
the smallest performance gap, which reveals that model improve the model’s robustness further. By shedding light
robustnessisnotsimplycorrelatedwithpixeldistribution. ontheseaspects,weaimtofurnishtheresearchcommunity
withinvaluableinsightsthatcanguidethedevelopmentof
morerobust,future-readyBEVperceptionsystems.
6.6 DetailedMetrics
Potential Limitation. Despite the distinct data corruptions
-Velocitypredictionerrorsamplifyundercorruptions,andattri-
we introduce in our benchmarks, they cannot cover all
bution and scale errors differ across models. While our study
the out-of-distribution scenarios in real-world applications
predominantlyreportsthenuScenesDetectionScore(NDS)
due to unpredictable complexity. Additionally, we mainly
metrics, additional insights into model robustness are il-
analyzedcoarse-graineddesignsamongmodels(e.g.,depth
lustrated in Figure 11. We find that models incorporating
estimation)sinceitisconsiderablynon-trivialtoidentifythe
temporal information, such as BEVFormer [2] and BEV-
trade-offbetweenfine-grainednetworkdesigns.
erse[51],exhibitsubstantiallylowermeanAbsoluteVelocity
Acknowledgments. This work is supported by the Min-
Error (mAVE) compared to those that do not. Nonetheless,
istry of Education, Singapore, under its MOE AcRF Tier 2
even models with temporal fusion are not immune to the
(MOET2EP20221-0012),NTUNAP,andundertheRIE2020
adverse effects of image corruption; specifically, velocity
IndustryAlignmentFund–IndustryCollaborationProjects
prediction errors markedly escalate even under mild il-
(IAF-ICP) Funding Initiative, as well as cash and in-kind
lumination alterations. Figure 11b and 11f illustrates that
contribution from the industry partner(s). This work is
MotionBlurcorruptiondetrimentallyinfluencesthevelocity
also under the programme DesCartes and is supported by
predictions for both BEVFormer and BEVerse, revealing a
theNationalResearchFoundation,PrimeMinister’sOffice,
significant vulnerability in these models that incorporate
Singapore, under its Campus for Research Excellence and
temporaldata.Moreover,acloserexaminationofattribution
TechnologicalEnterprise(CREATE)programme.
and scale errors reveals considerable heterogeneity across
models.Depth-freemodelsdemonstrateaconsistentperfor-
mance in these metrics, while depth-based models display
REFERENCES
pronounced variability. This observation underscores the
heightenedsusceptibilityofdepth-basedmethodstoimage [1] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,
corruptionsandemphasizestheneedforfurtherresearchto A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuScenes: A
MultimodalDatasetforAutonomousDriving,”inIEEE/CVFConf.
enhancetheirrobustness.
Comput.Vis.PatternRecog.,2020,pp.11621–11631.
[2] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu, and
J.Dai,“BEVFormer:LearningBird’s-Eye-ViewRepresentationfrom
7 CONCLUSION
Multi-Camera Images via Spatiotemporal Transformers,” in Eur.
Inthisstudy,wepresentedtheRoboBEVbenchmark,crafted Conf.Comput.Vis.,2022,pp.1–18.
[3] J. Huang, G. Huang, Z. Zhu, and D. Du, “BEVDet: High-
by incorporating a comprehensive set of eight different
PerformanceMulti-Camera3DObjectDetectioninBird-Eye-View,”
natural corruptions to form the nuScenes-C dataset. This arXivpreprintarXiv:2112.11790,2021.
benchmark serves as a rigorous testing ground for evalu- [4] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao, and
J. Solomon, “DETR3D: 3D Object Detection from Multi-View Im-
ating the out-of-distribution robustness of bird’s eye view
agesvia3D-to-2DQueries,”inConf.RobotLearn. PMLR,2022,pp.
(BEV) perception models. Additionally, we extended our 180–191.
analyses to account for sensor failures in multi-modal per- [5] M. Jaritz, T. H. Vu, R. Charette, E. Wirbel, and P. Pe´rez, “Cross-
ception frameworks, offering a more holistic view of BEV ModalLearningforDomainAdaptationin3DSemanticSegmenta-
tion,”inIEEETrans.PatternAnal.Mach.Intell.,2023,vol.45,no.2,
perception robustness. Through extensive experimentation,
pp.1533–1544.
we scrutinized various factors influencing the robustness [6] H. Li, C. Sima, J. Dai, W. Wang, L. Lu, H. Wang, J. Zeng, Z. Li,
of BEV perception models. Our findings elucidate critical J.Yang,H.Deng,H.Tian,E.Xie,J.Xie,L.Chen,T.Li,Y.Li,Y.Gao,
vulnerabilities and strengths across different models and X.Jia,S.Liu,J.Shi,D.Lin,andY.Qiao,“DelvingintotheDevils
ofBird’s-Eye-ViewPerception:AReview,EvaluationandRecipe,”
under diverse conditions. Based on our observations, we
in IEEE Trans. Pattern Anal. Mach. Intell., 2024, vol. 46, no. 4, pp.
proposed to leverage the pre-trained CLIP backbone to 2151–2170.15
[7] Y.Liu,T.Wang,X.Zhang,andJ.Sun,“PETR:PositionEmbedding [30] F.Hong,L.Kong,H.Zhou,X.Zhu,H.Li,andZ.Liu,“Unified3D
TransformationforMulti-View3DObjectDetection,”arXivpreprint and 4D Panoptic Segmentation via Dynamic Shifting Networks,”
arXiv:2203.05625,2022. in IEEE Trans. Pattern Anal. Mach. Intell., 2024, vol. 46, no. 5, pp.
[8] K. Chitta, A. Prakash, B. Jaeger, Z. Yu, K. Renz, and A. Geiger, 3480–3495.
“TransFuser: Imitation with Transformer-Based Sensor Fusion for [31] Y.ZhouandO.Tuzel,“VoxelNet:End-to-EndLearningforPoint
Autonomous Driving,” in IEEE Trans. Pattern Anal. Mach. Intell., CloudBased3DObjectDetection,”inIEEE/CVFConf.Comput.Vis.
2023,vol.45,no.11,pp.12878–12895. PatternRecog.,2018,pp.4490–4499.
[9] T.Wang,X.Zhu,J.Pang,andD.Lin,“FCOS3D:FullyConvolutional [32] L.Kong,J.Ren,L.Pan,andZ.Liu,“LaserMixforSemi-Supervised
One-StageMonocular3DObjectDetection,”inIEEE/CVFInt.Conf. LiDAR Semantic Segmentation,” in IEEE/CVF Conf. Comput. Vis.
Comput.Vis.,2021,pp.913–922. PatternRecog.,2023,pp.21705–21715.
[10] T.Wang,X.Zhu,J.Pang,andD.Lin,“ProbabilisticandGeometric [33] L.Kong,X.Xu,J.Ren,W.Zhang,L.Pan,K.Chen,W.T.Ooi,and
Depth: Detecting Objects in Perspective,” in Conf. Robot Learn. Z. Liu, “Multi-Modal Data-Efficient 3D Scene Understanding for
PMLR,2022,pp.1475–1485. AutonomousDriving,”arXivpreprintarXiv:2405.05258,2024.
[11] Q. Meng, W. Wang, T. Zhou, J. Shen, Y. Jia, and V. G. Luc, [34] R.Chen,Y.Liu,L.Kong,X.Zhu,Y.Ma,Y.Li,Y.Hou,Y.Qiao,and
“Towards a Weakly Supervised Framework for 3D Point Cloud W. Wang, “Clip2Scene: Towards Label-Efficient 3D Scene Under-
Object Detection and Annotation,” in IEEE Trans. Pattern Anal. standingbyCLIP,”inIEEE/CVFConf.Comput.Vis.PatternRecog.,
Mach.Intell.,2022,vol.44,no.8,pp.4454–4468. 2023,pp.7020–7030.
[12] C.Szegedy,W.Zaremba,I.Sutskever,J.Bruna,D.Erhan,I.Good- [35] G. Puy, A. Boulch, and R. Marlet, “Using a Waffle Iron for
fellow,andR.Fergus,“IntriguingPropertiesofNeuralNetworks,” Automotive Point Cloud Semantic Segmentation,” arXiv preprint
arXivpreprintarXiv:1312.6199,2013. arXiv:2301.10100,2023.
[13] X. Xu, L. Kong, H. Shuai, and Q. Liu, “FrNet: Frustum- [36] A.Ando,S.Gidaris,A.Bursuc,G.Puy,A.Boulch,andR.Marlet,
RangeNetworksforScalableLiDARSegmentation,”arXivpreprint “RangeViT:TowardsVisionTransformersfor3DSemanticSegmen-
arXiv:2312.04484,2023. tation in Autonomous Driving,” in IEEE/CVF Conf. Comput. Vis.
[14] G.Rossolini,F.Nesti,G.D’Amico,S.Nair,A.Biondi,andG.But- PatternRecog.,2023,pp.5240–5250.
tazzo, “On the Real-World Adversarial Robustness of Real-Time [37] Y. Liu, R. Chen, X. Li, L. Kong, Y. Yang, Z. Xia, Y. Bai, X. Zhu,
Semantic Segmentation Models for Autonomous Driving,” arXiv Y.Ma,Y.Li,Y.Qiao,andY.Hou,“UniSeg:AUnifiedMulti-Modal
preprintarXiv:2201.01850,2022.
LiDARSegmentationNetworkandtheOpenPCSegCodebase,”in
[15] C.Xie,J.Wang,Z.Zhang,Y.Zhou,L.Xie,andA.Yuille,“Adver- IEEE/CVFInt.Conf.Comput.Vis.,2023,pp.21662–21673.
sarialExamplesforSemanticSegmentationandObjectDetection,”
[38] R. Chen, Y. Liu, L. Kong, N. Chen, X. Zhu, Y. Ma, T. Liu, and
inIEEE/CVFInt.Conf.Comput.Vis.,2017,pp.1369–1378.
W. Wang, “Towards Label-Free Scene Understanding by Vision
[16] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and FoundationModels,”inAdv.NeuralInf.Process.Syst.,2023.
HarnessingAdversarialExamples,”arXivpreprintarXiv:1412.6572,
[39] Y.Liu,L.Kong,J.Cen,R.Chen,W.Zhang,L.Pan,K.Chen,and
2014.
Z.Liu,“SegmentAnyPointCloudSequencesbyDistillingVision
[17] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, FoundationModels,”inAdv.NeuralInf.Process.Syst.,2023.
“UniversalAdversarialPerturbations,”inIEEE/CVFConf.Comput.
[40] M. Contributors, “MMDetection3D: OpenMMLab Next-
Vis.PatternRecog.,2017,pp.1765–1773.
Generation Platform for General 3D Object Detection,”
[18] Y.Liu,L.Kong,X.Wu,R.Chen,X.Li,L.Pan,Z.Liu,andY.Ma,
https://github.com/open-mmlab/mmdetection3d,2020.
“Multi-SpaceAlignmentsTowardUniversalLiDARSegmentation,”
[41] J. Sun, X. Xu, L. Kong, Y. Liu, L. Li, C. Zhu, J. Zhang, Z. Xiao,
inIEEE/CVFConf.Comput.Vis.PatternRecog.,2024.
R.Chen,T.Wang,W.Zhang,K.Chen,andC.Qing,“AnEmpirical
[19] J.Tu,M.Ren,S.Manivasagam,M.Liang,B.Yang,R.Du,F.Cheng,
Study of Training State-of-the-Art LiDAR Segmentation Models,”
and R. Urtasun, “Physically Realizable Adversarial Examples for
arXivpreprintarXiv:2312.04484,2024.
LiDAR Object Detection,” in IEEE/CVF Conf. Comput. Vis. Pattern
[42] Y.Yan,Y.Mao,andB.Li,“SECOND:SparselyEmbeddedConvo-
Recog.,2020,pp.13716–13725.
lutionalDetection,”Sensors,vol.18,no.10,p.3337,2018.
[20] Y.Cao,N.Wang,C.Xiao,D.Yang,J.Fang,R.Yang,Q.A.Chen,
[43] Y. Ma, T. Wang, X. Bai, H. Yang, Y. Hou, Y. Wang, Y. Qiao,
M.Liu,andB.Li,“InvisibleforBothCameraandLiDAR:Security
R.Yang,D.Manocha,andX.Zhu,“Vision-CentricBEVPerception:
ofMulti-SensorFusionBasedPerceptioninAutonomousDriving
ASurvey,”arXivpreprintarXiv:2208.02797,2022.
UnderPhysical-WorldAttacks,”inIEEESymposiumonSecurityand
Privacy,2021,pp.176–194. [44] J. Huang and G. Huang, “BEVDet4D: Exploit Temporal
Cues in Multi-Camera 3D Object Detection,” arXiv preprint
[21] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and
arXiv:2203.17054,2022.
S. Zagoruyko, “End-to-End Object Detection with Transformers,”
inEur.Conf.Comput.Vis.,2020,pp.213–229. [45] Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,andB.Guo,
[22] Y. Li, Z. Ge, G. Yu, J. Yang, Z. Wang, Y. Shi, J. Sun, and Z. Li, “SwinTransformer:HierarchicalVisionTransformerUsingShifted
“BevDepth:AcquisitionofReliableDepthforMulti-View3DObject Windows,” in IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 10012–
Detection,”arXivpreprintarXiv:2206.10092,2022. 10022.
[23] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, [46] B. Zhu, Z. Jiang, X. Zhou, Z. Li, and G. Yu, “Class-Balanced
andA.Markham,“LearningSemanticSegmentationofLarge-Scale Grouping and Sampling for Point Cloud 3D Object Detection,”
PointCloudswithRandomSampling,”inIEEETrans.PatternAnal. arXivpreprintarXiv:1908.09492,2019.
Mach.Intell.,2022,vol.44,no.11,pp.8338–8354. [47] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
[24] J.PhilionandS.Fidler,“Lift,Splat,Shoot:EncodingImagesfrom “TowardsDeepLearningModelsResistanttoAdversarialAttacks,”
Arbitrary Camera Rigs by Implicitly Unprojecting to 3D,” in Eur. arXivpreprintarXiv:1706.06083,2017.
Conf.Comput.Vis.,2020,pp.194–210. [48] N.CarliniandD.Wagner,“TowardsEvaluatingtheRobustnessof
[25] B. Zhou and P. Kra¨henbu¨hl, “Cross-View Transformers for Real- NeuralNetworks,”inIEEESymposiumonSecurityandPrivacy,2017,
TimeMap-ViewSemanticSegmentation,”inIEEE/CVFConf.Com- pp.39–57.
put.Vis.PatternRecog.,2022,pp.13760–13769. [49] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into Transfer-
[26] T.B.Brown,D.Mane´,A.Roy,M.Abadi,andJ.Gilmer,“Adver- ableAdversarialExamplesandBlack-BoxAttacks,”arXivpreprint
sarialPatch,”arXivpreprintarXiv:1712.09665,2017. arXiv:1611.02770,2016.
[27] X. Liu, H. Yang, Z. Liu, L. Song, H. Li, and Y. Chen, “DPatch: [50] D.HendrycksandT.Dietterich,“BenchmarkingNeuralNetwork
An Adversarial Patch Attack on Object Detectors,” arXiv preprint Robustness to Common Corruptions and Perturbations,” arXiv
arXiv:1806.02299,2018. preprintarXiv:1903.12261,2019.
[28] S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “PointPainting: [51] Y. Zhang, Z. Zhu, W. Zheng, J. Huang, G. Huang, J. Zhou,
Sequential Fusion for 3D Object Detection,” in IEEE/CVF Conf. and J. Lu, “BEVerse: Unified Perception and Prediction in Bird’s-
Comput.Vis.PatternRecog.,2020,pp.4604–4612. Eye-ViewforVision-CentricAutonomousDriving,”arXivpreprint
[29] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Bei- arXiv:2205.09743,2022.
jbom,“PointPillars:FastEncodersforObjectDetectionfromPoint [52] W. Roh, G. Chang, S. Moon, G. Nam, C. Kim, Y. Kim, S. Kim,
Clouds,” in IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2019, pp. andJ.Kim,“ORA3D:OverlapRegionAwareMulti-View3DObject
12697–12705. Detection,”arXivpreprintarXiv:2207.00865,2022.16
[53] Y. Jiang, L. Zhang, Z. Miao, X. Zhu, J. Gao, W. Hu, and Y.-G. [75] W.Tong,C.Sima,T.Wang,L.Chen,S.Wu,H.Deng,Y.Gu,L.Lu,
Jiang,“PolarFormer:Multi-Camera3DObjectDetectionwithPolar P.Luo,D.Linetal.,“SceneasOccupancy,”inIEEE/CVFInt.Conf.
Transformers,”arXivpreprintarXiv:2206.15398,2022. Comput.Vis.,2023,pp.8406–8415.
[54] Y. Shi, J. Shen, Y. Sun, Y. Wang, J. Li, S. Sun, K. Jiang, and [76] L.Kong,Y.Liu,X.Li,R.Chen,W.Zhang,J.Ren,L.Pan,K.Chen,
D. Yang, “SRCN3D: Sparse R-CNN 3D Surround-View Camera andZ.Liu,“Robo3D:TowardsRobustandReliable3DPerception
Object Detection and Tracking for Autonomous Driving,” arXiv AgainstCorruptions,”inIEEE/CVFInt.Conf.Comput.Vis.,2023,pp.
preprintarXiv:2206.14451,2022. 19994–20006.
[55] X.Lin,T.Lin,Z.Pei,L.Huang,andZ.Su,“Sparse4D:Multi-View [77] X. Wang, Z. Zhu, W. Xu, Y. Zhang, Y. Wei, X. Chi, Y. Ye, D. Du,
3D Object Detection with Sparse Spatial-Temporal Fusion,” arXiv J. Lu, and X. Wang, “OpenOccupancy: A Large Scale Benchmark
preprintarXiv:2211.10581,2022. for Surrounding Semantic Occupancy Perception,” arXiv preprint
[56] P.Sun,R.Zhang,Y.Jiang,T.Kong,C.Xu,W.Zhan,M.Tomizuka, arXiv:2303.03991,2023.
L.Li,Z.Yuan,C.Wangetal.,“SparseR-CNN:End-to-EndObject [78] C. Min, X. Xu, D. Zhao, L. Xiao, Y. Nie, and B. Dai, “Occ-BEV:
Detection with Learnable Proposals,” in IEEE/CVF Conf. Comput. Multi-CameraUnifiedPre-Trainingvia3DSceneReconstruction,”
Vis.PatternRecog.,2021,pp.14454–14463. arXivpreprintarXiv:2305.18829,2023.
[57] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song, [79] C. Ge, J. Chen, E. Xie, Z. Wang, L. Hong, H. Lu, Z. Li, and
“Natural Adversarial Examples,” in IEEE/CVF Conf. Comput. Vis. P. Luo, “MetaBEV: Solving Sensor Failures for 3D Detection and
PatternRecog.,2021,pp.15262–15271. MapSegmentation,”inIEEE/CVFInt.Conf.Comput.Vis.,2023,pp.
[58] D.Hendrycks,S.Basart,N.Mu,S.Kadavath,F.Wang,E.Dorundo, 8721–8731.
R. Desai, T. Zhu, S. Parajuli, M. Guo et al., “The Many Faces of [80] Y. Man, L.-Y. Gui, and Y.-X. Wang, “DualCross: Cross-Modality
Robustness:ACriticalAnalysisofOut-of-DistributionGeneraliza- Cross-Domain Adaptation for Monocular BEV Perception,” arXiv
tion,”inIEEE/CVFInt.Conf.Comput.Vis.,2021,pp.8340–8349. preprintarXiv:2305.03724,2023.
[59] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar, “Do ImageNet [81] Z.Chen,Z.Li,S.Zhang,L.Fang,Q.Jiang,F.Zhao,B.Zhou,and
ClassifiersGeneralizetoImageNet?”inInt.Conf.Mach.Learn.,2019, H.Zhao,“AutoAlign:Pixel-InstanceFeatureAggregationforMulti-
pp.5389–5400. Modal3DObjectDetection,”arXivpreprintarXiv:2201.06493,2022.
[60] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfre- [82] C.Han,J.Sun,Z.Ge,J.Yang,R.Dong,H.Zhou,W.Mao,Y.Peng,
und, J. Tenenbaum, and B. Katz, “ObjectNet: A Large-Scale Bias- andX. Zhang,“ExploringRecurrent Long-TermTemporalFusion
Controlled Dataset for Pushing the Limits of Object Recognition for Multi-View 3D Perception,” arXiv preprint arXiv:2303.05970,
Models,”inAdv.NeuralInf.Process.Syst.,vol.32,2019. 2023.
[61] A.Kurakin,I.J.Goodfellow,andS.Bengio,“AdversarialExamples
[83] X.Chen,T.Zhang,Y.Wang,Y.Wang,andH.Zhao,“FUTR3D:A
in the Physical World,” in Artificial Intelligence Safety and Security, UnifiedSensorFusionFrameworkfor3DDetection,”inIEEE/CVF
2018,pp.99–112. Conf.Comput.Vis.PatternRecog.,2023,pp.172–181.
[62] S. Xie, Z. Li, Z. Wang, and C. Xie, “On the Adversarial Ro-
[84] Y.Li,Y.Chen,X.Qi,Z.Li,J.Sun,andJ.Jia,“UnifyingVoxel-Based
bustness of Camera-Based 3D Object Detection,” arXiv preprint
RepresentationwithTransformerfor3DObjectDetection,”inAdv.
arXiv:2301.10766,2023.
NeuralInf.Process.Syst.,vol.35,pp.18442–18455,2022.
[63] Y.LeeandJ.Park,“CenterMask:Real-TimeAnchor-FreeInstance
[85] L.Kong,S.Xie,H.Hu,L.X.Ng,B.R.Cottereau,andW.T.Ooi,
Segmentation,”inIEEE/CVFConf.Comput.Vis.PatternRecog.,2020,
“RoboDepth:RobustOut-of-DistributionDepthEstimationUnder
pp.13906–13915.
Corruptions,”inAdv.NeuralInf.Process.Syst.,2023.
[64] V. Guizilini, R. Ambrus, S. Pillai, A. Raventos, and A. Gaidon,
[86] L.Kong,Y.Niu,S.Xie,H.Hu,L.X.Ng,B.R.Cottereau,D.Zhao,
“3DPackingforSelf-SupervisedMonocularDepthEstimation,”in
L.Zhang,H.Wang,W.T.Ooi,R.Zhu,Z.Song,L.Liu,T.Zhang,
IEEE/CVFConf.Comput.Vis.PatternRecog.,2020,pp.2485–2494.
J.Yu,M.Jing,P.Li,X.Qi,C.Jin,Y.Cheng,J.Hou,J.Zhang,Z.Kan,
[65] J.Park,C.Xu,S.Yang,K.Keutzer,K.Kitani,M.Tomizuka,and
Q. Lin, L. Peng, M. Li, D. Xu, C. Yang, Y. Yao, G. Wu, J. Kuai,
W.Zhan,“TimeWillTell:NewOutlooksandaBaselineforTem-
X. Liu, J. Jiang, J. Huang, B. Li, J. Chen, S. Zhang, S. Ao, Z. Li,
poralMulti-View3DObjectDetection,”Int.Conf.Learn.Represent.,
R. Chen, H. Luo, F. Zhao, and J. Yu, “The RoboDepth Challenge:
2023.
Methods and Advancements Towards RobustDepth Estimation,”
[66] Z.Liu,H.Tang,A.Amini,X.Yang,H.Mao,D.Rus,andS.Han,
arXivpreprintarXiv:2307.15061,2023.
“BEVFusion:Multi-TaskMulti-SensorFusionwithUnifiedBird’s-
[87] J. Ren, L. Pan, and Z. Liu, “Benchmarking and Analyzing Point
EyeViewRepresentation,”inIEEEInt.Conf.Robot.Autom.,2023.
CloudClassificationUnderCorruptions,”inInt.Conf.Mach.Learn.,
[67] T.Liang,H.Xie,K.Yu,Z.Xia,Z.Lin,Y.Wang,T.Tang,B.Wang,
2022,pp.18559–18575.
and Z. Tang, “BEVFusion: A Simple and Robust LiDAR-Camera
[88] S. Shi, X. Wang, and H. Li, “PointRCNN: 3D Object Proposal
FusionFramework,”arXivpreprintarXiv:2205.13790,2022.
Generation and Detection from Point Cloud,” in IEEE/CVF Conf.
[68] X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai,
Comput.Vis.PatternRecog.,2019,pp.770–779.
“TransFusion:RobustLiDAR-CameraFusionfor3DObjectDetec-
[89] W. Shi and R. Raj, “Point-GNN: Graph Neural Network for 3D
tion with Transformers,” in IEEE/CVF Conf. Comput. Vis. Pattern
ObjectDetectioninaPointCloud,”inIEEE/CVFConf.Comput.Vis.
Recog.,2022,pp.1090–1099.
PatternRecog.,2020,pp.1711–1719.
[69] Z. Chen, Z. Li, S. Zhang, L. Fang, Q. Jiang, and F. Zhao, “Au-
[90] Z.Yang,Y.Sun,S.Liu,X.Shen,andJ.Jia,“STD:Sparse-to-Dense
toAlignV2: Deformable Feature Aggregation for Dynamic Multi-
3DObjectDetectorforPointCloud,”inIEEE/CVFInt.Conf.Comput.
Modal3DObjectDetection,”arXivpreprintarXiv:2207.10316,2022.
Vis.,2019,pp.1951–1960.
[70] Y.Wei,L.Zhao,W.Zheng,Z.Zhu,Y.Rao,G.Huang,J.Lu,and
[91] Z.Yang,Y.Sun,S.Liu,andJ.Jia,“3DSSD:Point-Based3DSingle
J.Zhou,“SurroundDepth:EntanglingSurroundingViewsforSelf-
SupervisedMulti-CameraDepthEstimation,”inConf.RobotLearn. Stage Object Detector,” in IEEE/CVF Conf. Comput. Vis. Pattern
Recog.,2020,pp.11040–11048.
PMLR,2023,pp.539–549.
[71] Y. Wei, L. Zhao, W. Zheng, Z. Zhu, J. Zhou, and J. Lu, [92] Y.Yan,Y.Mao,andB.Li,“SECOND:SparselyEmbeddedConvo-
“SurroundOcc: Multi-Camera 3D Occupancy Prediction for Au- lutionalDetection,”Sensors,2018,vol.18,pp.3337.
tonomousDriving,”inIEEE/CVFInt.Conf.Comput.Vis.,2023,pp. [93] Y.ZhouandO.Tuze,“VoxelNet:End-to-EndLearningforPoint
21729–21740. CloudBased3DObjectDetection,”inIEEE/CVFConf.Comput.Vis.
[72] Y.Huang,W.Zheng,Y.Zhang,J.Zhou,andJ.Lu,“Tri-Perspective PatternRecog.,2018,pp.4490–4499.
View for Vision-Based 3D Semantic Occupancy Prediction,” in [94] T. Yin, X. Zhou, and P. Krahenbuhl, “Center-Based 3D Object
IEEE/CVFConf.Comput.Vis.PatternRecog.,2023,pp.9223–9232. Detection and Tracking,” in IEEE/CVF Conf. Comput. Vis. Pattern
[73] Z.Zhu,Y.Zhang,H.Chen,Y.Dong,S.Zhao,W.Ding,J.Zhong, Recog.,2021,pp.11784–11793.
andS.Zheng,“UnderstandingtheRobustnessof3DObjectDetec- [95] G. Shi, R. Li, and C. Ma, “PillarNet: High-Performance Pillar-
tion with Bird’s-Eye-View Representations in Autonomous Driv- Based3DObjectDetection,”arXivpreprintarXiv:2205.07403,2022.
ing,”inIEEE/CVFConf.Comput.Vis.PatternRecog.,2023,pp.21600– [96] S. Shi, L. Jiang, J. Deng, Z. Wang, C. Guo, J. Shi, X. Wang, and
21610. H.Li“PV-RCNN++:Point-VoxelFeatureSetAbstractionwithLocal
[74] F. Bartoccioni, E´. Zablocki, A. Bursuc, P. Pe´rez, M. Cord, and VectorRepresentationfor3DObjectDetection,”Int.J.Comput.Vis.,
K.Alahari,“Lara:LatentsandRaysforMulti-CameraBird’s-Eye- 2022.
ViewSemanticSegmentation,”inConf.RobotLearn. PMLR,2023, [97] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li,
pp.1663–1672. “PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object17
Detection,”inIEEE/CVFConf.Comput.Vis.PatternRecog.,2020,pp. etal.,“RobustFine-TuningofZero-ShotModels,”inIEEE/CVFConf.
10529–10583. Comput.Vis.PatternRecog.,2022,pp.7959–7971.
[98] H.Thomas, C. R.Qi, J.-E. Deschaud,B. Marcotegui,F. Goulette, [119] J.Johnson,A.Alahi,andL.Fei-Fei,“PerceptualLossesforReal-
andL.Guibas,“KPConv:FlexibleandDeformableConvolutionfor Time Style Transfer and Super-Resolution,” in Eur. Conf. Comput.
PointClouds,”inIEEE/CVFInt.Conf.Comput.Vis.,2019,pp.6411– Vis.,2016,pp.694–711.
6420. [120] K.He,X.Zhang,S.Ren,andJ.Sun,“DeepResidualLearningfor
[99] B. Wu, A. Wan, X. Yue, and K. Keutzer, “SqueezeSeg: Convolu- ImageRecognition,”inIEEE/CVFConf.Comput.Vis.PatternRecog.,
tionalNeuralNetswithRecurrentCRFforReal-TimeRoad-Object 2016,pp.770–778.
Segmentation from 3D LiDAR Point Cloud,” in IEEE Int. Conf. [121] Y. Xue, S. Joshi, D. Nguyen, and B. Mirzasoleiman, “Under-
Robot.Autom.,2018,pp.1887–1893. standing the Robustness of Multi-Modal Contrastive Learning to
[100] A. Milioto, I. Vizzo, J. Behley, and C. Stachniss, “RangeNet++: DistributionShift,”arXivpreprintarXiv:2310.04971,2023.
FastandAccurateLiDARSemanticSegmentation,”inIEEE/RSJInt. [122] A.Fang,G.Ilharco,M.Wortsman,Y.Wan,V.Shankar,A.Dave,
Conf.Intell.RobotsSyst.,2019,pp.4213–4220. and L. Schmidt, “Data Determines Distributional Robustness in
[101] Y.Liu,J.Yan,F.Jia,S.Li,Q.Gao,T.Wang,X.Zhang,andJ.Sun, Contrastive Language Image Pre-Training (CLIP),” in Int. Conf.
“PETRv2: A Unified Framework for 3d Perception from Multi- Mach.Learn.,2022,pp.6216–6234.
CameraImages,”arXivpreprintarXiv:2206.01256,2022. [123] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-
[102] L. Kong, Y. Liu, R. Chen, Y. Ma, X. Zhu, Y. Li, Y. Hou, Y. Qiao Lopes,A.S.Morcos,H.Namkoong,A.Farhadi,Y.Carmon,S.Ko-
and Z. Liu, “Rethinking Range View Representation for LiDAR rnblithetal.,“ModelSoups:AveragingWeightsofMultipleFine-
Segmentation,”inIEEE/CVFInt.Conf.Comput.Vis.,2023,pp.228– Tuned Models Improves Accuracy Without Increasing Inference
240. Time,”inInt.Conf.Mach.Learn.,2022,pp.23965–23998.
[103] Y. Zhang, Z. Zhou, P. David, X. Yue, Z. Xi, B. Gong, and [124] S. Goyal, A. Kumar, S. Garg, Z. Kolter, and A. Raghunathan,
H.Foroosh,“PolarNet:AnImprovedGridRepresentationforOn- “Finetune Like You Pretrain: Improved Finetuning of Zero-Shot
line LiDAR Point Clouds Semantic Segmentation,” in IEEE/CVF VisionModels,”inIEEE/CVFConf.Comput.Vis.PatternRecog.,2023,
Conf.Comput.Vis.PatternRecog.,2020,pp.9601-9610. pp.19338–19347.
[104] C. Choy, J. Gwak, and S. Savarese, “4D Spatio-Temporal Con-
vNets:MinkowskiConvolutionalNeuralNetworks,”inIEEE/CVF
Conf.Comput.Vis.PatternRecog.,2019,pp.3075–3084.
[105] V. E. Liong, T. N. T. Nguyen, S. Widjaja, D. Sharma,
and Z. J. Chong, “AMVNet: Assertion-Based Multi-View Fu-
sion Network for LiDAR Semantic Segmentation,” arXiv preprint
arXiv:2012.04934,2020.
[106] J. Xu, R. Zhang, J. Dou, Y. Zhu, J. Sun, and S. Pu, “RPVNet: A
DeepandEfficientRange-Point-VoxelFusionNetworkforLiDAR
Point Cloud Segmentation,” in IEEE/CVF Int. Conf. Comput. Vis.,
2021,pp.16024–16033.
[107] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han,
“SearchingEfficient3DArchitectureswithSparsePoint-VoxelCon-
volution,”inEur.Conf.Comput.Vis.,2020,pp.685–702.
[108] L. Kong, N. Quader, and V. E. Liong, “ConDA: Unsupervised
DomainAdaptationforLiDARSegmentationviaRegularizedDo-
main Concatenation,” in IEEE Int. Conf. Robot. Autom., 2023, pp.
9338–9345.
[109] D.Park,R.Ambrus,V.Guizilini,J.Li,andA.Gaidon,“IsPseudo-
LiDARNeededforMonocular3DObjectDetection?”inIEEE/CVF
Int.Conf.Comput.Vis.,2021,pp.3142–3152.
[110] C. Sakaridis, D. Dai, and L. Van Gool, “ACDC: The Adverse
Conditions Dataset with Correspondences for Semantic Driving
Scene Understanding,” in IEEE/CVF Int. Conf. Comput. Vis., 2021,
pp.10765–10775.
[111] C. Sakaridis, D. Dai, and L. Van Gool, “Semantic Foggy Scene
UnderstandingwithSyntheticData,”Int.J.Comput.Vis.,vol.126,
pp.973–992,2018.
[112] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele, “The Cityscapes
Dataset for Semantic Urban Scene Understanding,” in IEEE/CVF
Conf.Comput.Vis.PatternRecog.,2016,pp.3213–3223.
[113] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,
G.Sastry,A.Askell,P.Mishkin,J.Clarketal.,“LearningTransfer-
able Visual Models from Natural Language Supervision,” in Int.
Conf.Mach.Learn.,2021,pp.8748–8763.
[114] S. Chen, Y. Ma, Y. Qiao, and Y. Wang, “M-BEV: Masked BEV
PerceptionforRobustAutonomousDriving,”inAAAIConf.Artif.
Intell.,vol.38,no.2,2024,pp.1183–1191.
[115] T. Nguyen, G. Ilharco, M. Wortsman, S. Oh, and L. Schmidt,
“QualityNotQuantity:OntheInteractionBetweenDatasetDesign
andRobustnessofCLIP,”Adv.NeuralInf.Process.Syst.,vol.35,pp.
21455–21469,2022.
[116] J. P. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh,
V.Shankar,P.Liang,Y.Carmon,andL.Schmidt,“Accuracyonthe
Line: On the Strong Correlation Between Out-of-Distribution and
In-DistributionGeneralization,”inInt.Conf.Mach.Learn.,2021,pp.
7721–7735.
[117] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei,“Im-
ageNet:ALarge-ScaleHierarchicalImageDatabase,”inIEEE/CVF
Conf.Comput.Vis.PatternRecog.,2009,pp.248–255.
[118] M. Wortsman, G. Ilharco, J. W. Kim, M. Li, S. Kornblith,
R. Roelofs, R. G. Lopes, H. Hajishirzi, A. Farhadi, H. Namkoong