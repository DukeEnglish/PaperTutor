LARM: Large Auto-Regressive Model for
Long-Horizon Embodied Intelligence
ZhuolingLi1 XiaogangXu2 ZhenhuaXu1 SerNamLim3 HengshuangZhao1∗
1HKU 2CUHK 3UCF
https://lizhuoling.github.io/LARM_webpage/
Abstract
Duetotheneedtointeractwiththerealworld,embodiedagentsarerequiredto
possesscomprehensivepriorknowledge,long-horizonplanningcapability,anda
swiftresponsespeed. Despiterecentlargelanguagemodel(LLM)basedagents
achievingpromisingperformance,theystillexhibitseverallimitations.Forinstance,
theoutputofLLMsisadescriptivesentence,whichisambiguouswhendetermining
specificactions.Toaddresstheselimitations,weintroducethelargeauto-regressive
model(LARM).LARMleveragesbothtextandmulti-viewimagesasinputand
predicts subsequent actions in an auto-regressive manner. To train LARM, we
developanoveldataformatnamedauto-regressivenodetransmissionstructure
andassembleacorrespondingdataset. Adoptingatwo-phasetrainingregimen,
LARMsuccessfullyharvestsenchantedequipmentinMinecraft,whichdemands
significantlymorecomplexdecision-makingchainsthanthehighestachievements
ofpriorbestmethods. Besides,thespeedofLARMis6.8×faster.
1 Introduction
Inrecentyears,remarkableprogresshasbeenachievedinvariousartificialintelligence(AI)fields[22]
likecomputervision[17]andnaturallanguageprocessing[20],butmostofthemlackthecapacity
tophysicallyinteractwiththerealworld. Toaddressthisdisconnect,theconceptofembodiedAI
isintroduced[10]. Earlyembodiedagentsarepredominantlydevelopedonsimulationplatforms
forspecifictaskssuchasobjectgraspingandindoornavigation[30]. Whilenotableadvancements
areachieved,theseagentstendtobespecialistmodelsconfinedtoisolatedtasks[19]. Toovercome
thislimitation,recentstudies,includingthiswork,employMinecraft[5;12;16]asabenchmarkto
exploreembodiedagentswithopen-endedobjectivesandlong-horizonreasoningchains.
The initial methods for developing such agents primarily rely on reinforcement learning, the ex-
ploration of which is inefficient and results in limited performance [37]. Recent works begin to
investigatetheuseoflargelanguagemodels(LLMs)tobuildagents[8]. Owingtotheextensive
general knowledge and formidable reasoning capabilities of LLMs, these methods demonstrate
promisingresultswithsignificantlyreduceddomain-specificengineeringefforts[34]. Nevertheless,
LLMscontinuetoexhibitanumberoflimitations,renderingthemill-suitedforembodiedAItasks.
ThelimitationsofLLMcanbedividedintotwocategories,inadequatemodelstructureandineffective
datautilization.Concerningthemodelstructure,ontheonehand,theoutputofLLMistext-based.As
illustratedinFig.1,althoughLLMcoulddescribethefutureplanwell,thespecificactionsstillremain
ambiguous. Ontheotherhand,theoutputofLLMisgeneratedthroughiterativetokenprediction,
necessitatingN inferenceoperationsforN tokens,whichresultsinasluggishspeed. Regardingdata
utilization,thetrainingdataforLLMsispredominantlysourcedfromtheinternetandlacksintricate
dependencyknowledgeamongembodiedactions,agentstatus,andenvironmentalinformation.
ToaddressthelimitationsofLLM,weproposeLargeAuto-RegressiveModel(LARM).LARM
engagesinreal-timeinteractionwiththeenvironmentandpredictssubsequentactionsinanauto-
∗Correspondingauthor.
Preprint.Underreview.
4202
yaM
72
]VC.sc[
1v42471.5042:viXraThe agent should equip the iron sword and then proceed to
LLM locate and approach cows. Once the cows are within range,
the agent should combatcowsuntilgetleather.
Target: Get leather
Output: A description sentence
by combating cows
LARM
(Ours) Equip the
Find cows Combat cows Get leather
iron sword
Output: The actions to execute
Previous SOTA
LARM
More Advanced Achievements
Figure 1: LARM is the first method that achieves enchanted diamond equipment in Minecraft,
surpassingpreviousSOTAslimitedtocraftingstandarddiamondtools. ComparedwithLLMsthat
generateambiguousdescriptivesentencesfordeterminingskillstoexecute,LARMdirectlypredicts
subsequentskillsinanauto-regressiveway.
regressivemanner. Besidestextinformation(targettaskdescription,historicalaction,agentposition,
inventorylist,andenvironmentfeedbackmessage),LARMincorporatesmulti-viewimagesasinput.
Insteadofgeneratingasentencecomposedofmultipletokens,LARMdirectlyproducesasingle
tokentopredictthenextaction. Thisdesigneradicatestheambiguityinherentinsentencedescription
outputsandenhancestheinferencespeedofLARM.
To train LARM, we introduce a unique data organization structure termed as Auto-Regressive
NodeTransmissionStructure(ARNTS).WithinARNTS,eachdatasampleisconceptualizedasa
node,encompassingtextinformation,multi-viewimages,andtheexpectedsubsequentaction. A
chronologicaldependencyexistsamongthesenodes. Theprocessofaccomplishingacomplextarget
can be interpreted as transmissions between different nodes. Adhering to this data structure, we
manuallycollectanARNTSdatasetcomprising2,589datasamples,whichcoverdiversifiedtargets,
weathers,andbiomes. LeveragingthisdatasettotraintheLARMagent,theagentcaneffectively
learnthedependencyknowledgeamongvariousinformationtypes. Additionally,wehavedevised
threedataaugmentationstrategies,eachtailoredtotheuniquecharacteristicsofARNTS.However,
theARNTSdatasetaloneisinadequatetotrainLARMduetoitsnumerousparameters(e.g.,7billion
parameters). Tomitigatethischallenge,weemploya34Gmulti-modaldataset[12],sourcedfrom
WikiMinecraftwebpages,topre-trainthemodelbeforeproceedingwithARNTStraining.
Throughintegratingtheaforementionedefforts,wedevelopanagentcapableofcraftingenchanted
diamondtools. AsshowninFig.1,craftingenchanteddiamondtoolsrequiresafarmoreintricateac-
tionchainandlong-horizonschedulingabilitycomparedtoexistingstate-of-the-art(SOTA)methods
[34;38],whichareonlycapableofcraftingstandarddiamondtools. Besides,theinferencespeedof
LARMis6.8×fasteronaveragecomparedtoanLLMwithcomparableparametervolume.
2 RelatedWork
Embodiedbenchmarks. EmbodiedAIrecentlygarnerssignificantattentionduetoitspotentialto
bridgethegapbetweenvirtualalgorithmsandphysicalinteractions[6].Therecentworksinembodied
AIprimarilyfocusonenhancingagents’interactionwiththeenvironment,andmanybenchmarks
areestablished[11]. Forinstance,Andersonetal. proposeavision-and-languagenavigationtaskin
photo-realisticenvironments[3]. Similarly,Chenetal. introduceTouchdown[9],agame-liketask
thatcombinesnavigationandquestion-answeringinurbanenvironments.Moreover,theemergenceof
interactiveenvironmentslikeAI2-THOR[21]andHabitat[30]significantlycontributestoembodied
agentdevelopment. However,thesebenchmarksaremostlyrestrictedtoisolatedtasks.
Minecraft.Minecraftprovidesaplatformforexploringmodelswithlong-horizonplanningcapability
[12]. InitialmethodstypicallydevelopMinecraftagentsviareinforcementlearning[14]. Tomitigate
2Collect log
Craft furnace
Skill Prompt MineCLIP Match
Smelt iron
GPT-4 Matched Skill
Mine gold Skill Embeddings
Skill Library Perform
LoRA
the skill
CLIP Environment
Image Agent
Multi-view Tokens Decoders
Images Multi-view
Skill Token
Image Tokens
The last skill Environment
Target: Find Tokenizer Position coordinate feedback
sugar canes. Text Tokens Inventory list
Figure2: TheoverallframeworkofLARM.Inthisframework, thenetworktakesthetargettask
description,multi-viewimages,agentinformation,andenvironmentfeedbackasinputtopredicta
skilltoken. Theskilltokenismatchedwiththeskillembeddings,whicharegeneratedbasedona
pre-preparedskilllibrary,toselecttheoptimumskill. Then,theagentperformsthisskill,whichhelps
theagentonestepclosertocompletingthetargetandchangestheenvironment.
theoptimizationchallenges,thesemethodsmanuallydecomposeahigh-leveltaskintosimpleratomic
tasks,subsequentlytrainingagentstoexecutetheseatomictaskssequentially[37]. However,the
performance of these methods remains suboptimal due to the limited exploration efficiency [31].
To rectify this issue, recent works construct agents based on LLMs. For example, Voyager [34]
successfullycraftsdiamondtoolsusingatraining-freesystembasedonGPT-4[1]. Similarly,DEPS
[35]developsamorecomprehensiveLLMsystem,yieldingsatisfactoryresults. However,LLMs
solely support text input, which is inadequate for encapsulating environmental information. To
surmountthishurdle,certainstudiestrainlargevision-languagemodelsthroughfine-tuningLLMs
[38;13]. Despitetheseadvancements,theoutputofthesemodelsremainsambiguoustext.
Large language models. GPT-3 emerges as a significant milestone in the evolution of LLMs,
showcasingremarkableopen-worldgeneralizationcapabilitiesacrossdiversetasks[8]. Subsequently,
thefine-tuningofGPT-3usingreinforcementlearningwithhumanfeedbackleadstothecreation
of ChatGPT, a model that displays an impressive breadth of general knowledge [26]. Further
advancementsresultinthecreationofGPT-4,reinforcingtheadvantagesofincreasingmodelsize
andtrainingdata[1]. Inaddition,therearenotableopen-sourceworksthattrainmodelswithfewer
parameters while still achieving promising results, such as LLaMA [33]. However, a significant
limitationofLLMsistheirinabilitytointerpretinformationinimages,whicharevitalinformation
sources. To overcome this, large vision-language models like LLaVA [24] and Flamingo [2] are
developed. Whilethesemodelscandescribethecontentinprovidedimages,theylackthecapability
tointeractwiththerealworld.
3 Algorithm
3.1 LargeAuto-RegressiveModel
LARMisalargeembodiedAImodelcontainingbillionsofparametersandpredictingsubsequent
skillsinanauto-regressivemanner. ItsframeworkisillustratedinFig.2. Tosavetrainingcost,we
generateskillembeddingsbeforetrainingLARM.FollowingVoyager[34],wedesignpromptsthat
describetheintendeduseofskills. Then,GPT-4isemployedtogenerateskillcodebasedonthese
promptsandMineflayer[27]. Combiningthegeneratedskills,askilllibraryisderived. Weutilize
MineCLIP[12]toextracttheembeddingsoftheseskills. Everyskillismappedtoanembedding
vectorwiththelengthofL. AlltheembeddingsaresavedanddirectlyusedduringtrainingLARM.
TheinputtoLARMconsistsofbothtextandimageinformation. Thetextinformationincludesthe
targettaskdescription,theagentinformation,andtheenvironmentfeedback. Specifically,theagent
informationcontainsthelastskillperformedbytheagent,thecurrent3Dpositioncoordinateofthe
agentintheMinecraftworld,theagent’sinventorylist(alistoftheresourcesthattheagentowns).
Thisinformationwelldescribesthecurrentstatusoftheagent. Fortheenvironmentfeedback,two
kinds of information are included, i.e., whether the last skill performed by the agent is executed
3
remroF-Q rotcejorP
rotcejorPOak Log View Position Embedding Image Position Embedding
SpurceLog
Birch Log
Stick
Acacia Log
Cherry Log
East North West South
Figure3: Anexampleofvariousrecipes Figure4: Themulti-viewimagesareaddedwithviewand
forcraftingastick. imagepositionembeddings.
successfullyandthegameresponsemessagetothatskill. Allthetextinformationistokenizedinto
texttokensbyafrozentokenizer,theweightofwhichisinitializedfromBERT[20].
DifferentfrompreviousMinecraftagentsthattakeamonocularimageasinput[38],LARMadopts
N views of images in different directions, providing a more comprehensive description of the
surroundingenvironment. TheN multi-viewimagesaretokenizedasN ×NI imagetokensbya
frozenCLIPencoder. However,sincethereareN images,thenumberofobtainedtokensisincreased
byN timesandwouldresultinasignificantcomputationalburdenifusingallthesetokensasinput
tothesubsequentdecoders. Toresolvethisproblem,weutilizeatrainableQ-Former[23]moduleto
reducethenumberofimagetokensfromN ×NI toNI,asshowninFig.2. Besidesreducingimage
tokens,thisQ-Formermoduleisalsohelpfulforfusingthefeatureexistinginmulti-viewimages.
AftertheQ-Former,theimagetokensarefurthertransformedbyatrainableprojectorlinkingthe
visionbranchwithdecoders.Thedecoderstakebothtextandimagetokensasinputtoconductfeature
interaction. Giventhatthedecoderscontainnumerousparametersandarechallengingtotrain,we
initializetheparametersfromtheLLaVAweight[24]andfreezethemduringtraining. Additionally,
atrainableLoRA[18]isappliedtoassistthemodelinunderstandingMinecraftknowledge.
The output of the decoders is multiple tokens and we compress them into a single token with a
trainableprojector. Wenamethetokenproducedbythisprojectorasskilltoken,whichcontainsthe
informationaboutwhichskillshouldbeexecutedaspredictedbytheLARMmodel. Byconducting
cosine distance [25] based matching between the skill token and the many pre-generated skill
embeddings,themostsimilarskillisselectedfortheagenttoperform. Thechosenskillaltersboth
thestatusoftheagentandtheenvironment. LARMrepeatstheaboveprocessinanauto-regressive
wayuntilan<End>tokenismatched,markingtheendofthisskillexecutionchain.
3.2 Multi-viewVisionPerception
InMinecraft,thereareoftenmultiplerecipesforcraftingthesameitem. Forexample,asshownin
Fig.3,differenttypesoflogscanbecollectedbychoppingtreestocraftsticks. Whentheagentis
locatedinvariousbiomes,thenearesttreecategoryvaries,andtheoptimalrecipeistheonebased
onthisnearestcategoryoftree. Hence,itisimportanttoperceivenearbyenvironmentinformation
throughimagestodeterminetheoptimalrecipe. Tofullyobservesurroundingobjects,weproposeto
usemulti-viewimagesasinput. InLARM,bysettingtheagentobservationpitchangleto0◦ and
theyawangleto0◦,90◦,180◦,and270◦,the4multi-viewimagesillustratedinFig.4areobtained,
respectively. To enable the LARM model to associate each pixel in these 4 images with voxels
inthe3Denvironment,twokindsoflearnablepositionembeddingsareadded,theimageposition
embedding(IPE)andviewpositionembedding(VPE)inFig.4. Denotethesizeofthemulti-view
image batch as N ×H ×W ×3, where H and W represent image height and width. IPE is to
distinguishpixelsinthesameimageanditssizeis1×H×W×1.Conversely,VPEisforclassifying
variousviewsandthusitssizeisN ×1×1×1.
3.3 SkillLibraryPreparation
Inthiswork,wemanuallydesignpromptsforGPT-4togeneratethecodeof106skillsintheskill
library. AsshowninFig.5,toensurethecorrectnessofcodegeneration,twoGPT-4basedagents
arebuilt,oneservesastheprogrammerandtheotheroneasthechecker. Theprogrammertakesa
promptthatdescribestheskillinformation,acodeformatrequirement,andadescriptionaboutthe
atomfunctionsbasedonMineflayerasinputtoproducethecorrespondingskillcode. Theskillis
thenexecutedintheMinecraftenvironment. Afterwards,theexecutionresult,alongwiththeoriginal
code,issenttothechecker. Ifthecheckerjudgesthattheskillcodeorexecutionresultdonotsatisfy
therequirement,itinformstheprogrammeraboutthereasonandaskstheprogrammertorewriteit.
4Atom Functions based async function collect_4_obsidian (bot) Execute
on Mineflayer {
// Collect 4 obsidian. Environment
for (leti=0;i<4;i++) Execution
{ Result
Prompt:Writeaskillof constflag=awaitdig_one_obsidian(bot)
collecting4obsidian if (flag==false)returnflag
}
Code
Programmer returntrue
}
Code Format
Requirement Generated Skill Code Check Result Checker
Figure5: TheskilllibrarygenerationpipelineusingtwoGPT-4agents,theprogrammerandchecker.
Theprogrammerisforwritingcodeandthecheckeristocheckthecorrectnessofgeneratedcode.
Target: Craft 16 planks Target: Craft 4 sticks Target: Craft 1 crafting Target: Craft 1 wooden
W …e …ather: Clear Craft planks W …e …ather: Clear Craft sticks t Wab el ae ther: Clear crafC tir na gft ta a ble p Wic ek aa thx ee r: Clear
Multi-view images: Multi-view images: …… ……
Multi-view images: Multi-view images:
Node A Node B Node C Node D
Figure6: ForARTNS,theproceduresofperformingamulti-steptaskisinterpretedasagraph. In
differentnodes,theagentperceivestheenvironmentinformationandselectsaskilltoexecute. During
anedgelinkingtwonodes,theagentperformstheselectedskill,anditsstatustransfersfromthe
startnodetotheendnode. Besides,thereexistdependenciesamongvariousnodes. Forexample,as
markedbythebluedottedlines,NodeDdependsonthefirstthreenodes,becausecrafting1wooden
axerequirestheplanks,sticks,andcraftingtableinMinecraft.
Otherwise,theskillcodeisaddedtotheskilllibrary. Notably,inFig.5,theskillisgeneratedbased
onpre-definedprompts. However,theskilllibrarycanalsobedynamicallyexpandedfollowingthe
proceduresdevelopedinVoyager[34].
4 DataandTraining
4.1 ARTNSDataset
In ARTNS, we represent the performing a multi-step task process as a sequential chain, which
includesnodesandedgesthatlinkthesenodes. AsillustratedinFig.6,thenodesdenotethemoments
whentheagentperceivesenvironmentalinformationandselectsthenextskill. Whentheselected
skillisperformed,thestatusoftheagenttransitionsfromanodetothenextnode,representedbythe
edgelinkingthesetwonodes. Notably,thereexistsdependencerelationsamongnodes,andthenext
nodedoesnotnecessarilydependonthepreviousnode. Forexample,asshowninFig.6,makinga
craftingtablerequiresplanksbutnotsticks,whichmeansNodeCdependsonthecompletionofthe
targetinNodeAbutnotNodeB.Inaddition,anodecoulddependonmultiplepreviousnodes,like
NodeDdependsonNodeA,NodeB,andNodeCsimultaneously.
InaccordancewiththeARTNSformat,wemanuallyoperateanagenttocompletevarioustasksin
Minecraftandrecordthedata. Throughcollectingdataindiversebiomesandweather,weobtaina
datasetconsistingof2,589datapairs. EachpaircontainstheexplorationtrajectoryID,datapairID,
time,weather,biomename,target,agentposition,agentinventorylist,thenameofthelastperformed
skillanditscorrespondingexecutionresult,thenextskill,andmulti-viewimages. Somemulti-view
imageexamplesarevisualizedinFig.7. Then,weannotatethedependencyrelationsamongnodes,
whicharerepresentedasagraphlikethebluedottedlinesillustratedinFig.6. Thisgraphdescribes
theskillsinsomenodesareprerequisitesforothernodes,andtheagentshouldlearnthisknowledge
tounderstandhowtoschedulefutureskills. RefertoSectionFinAppendixfordatacollectiondetails.
4.2 ARTNS-basedAugmentation
BasedonthecharacteristicsofARTNS,wedevelopthreeaugmentationstrategies: (1)targettrans-
mission,(2)targetrephrasing,and(3)skillroll-back. Theyarebeneficialtoimprovingtherobustness
oftheagent. Thetargettransmissionstrategyisdevisedbasedonthedependencyrelationsamong
nodes. AsshowninFig.6,makingawoodenpickaxeinNodeDrequirestheplankscraftedinNode
5Original Adjusted
Figure7: Someexamplesofthecollectedmulti-viewimages,whichcoversdiversetasks,biomes,
weather,etc. Thebottomrightgroupofimagesisarainynightcasesoitsoriginalimagesaredim.
Weadjustitscolorcontrastforclearervisualization,andtheinputtoLARMisstilltheoriginalform.
A.Thus,whenanagentisinnodeA,eveniftheoriginaltargetisreplacedwiththetarget“Craft1
woodenpickaxe”fromNodeD,thenextcorrectskillisstill“Craft16planks”. Basedonthisconcept,
wetransmitthetargetsoffuturenodestothecurrentnodeandtrainLARMtoinferthenextskill.
Thistrainingisbeneficialtoimprovingthelong-horizonschedulingabilityofLARM.Inthetarget
rephrasingstrategy,weutilizeLLMstorewritethetargetsinvariousnodes. Forexample,the“Craft
4sticks”canbeaugmentedas“Make4sticks”or“Gotocraft4sticks”. Thisstrategyenhancesthe
robustnessindealingwithdifferenttargetexpressions. Theskillroll-backstrategyistoenableLARM
tosituationswhereaskillisexecutedunsuccessfully. Forinstance,whenanagentiscollectingoak
logs,bysettingtheoaklognumberbelowexpectationandtheexecutionresultofthelastskillas
“fail”,wetrainLARMtoknowtore-implementthelastskill. Inthisway,LARMbecomesmore
robusttounexpectedsituationsandcanrecoverfromfailureautomatically.
4.3 Two-phaseTraining
SincetheinitialknowledgeofLARMaboutMinecraftislimitedandtheARTNSdatasetvolumeis
insufficient,weuse34Gwebpagedata[12]crawledfromWikitopre-trainLARMbeforetheARTNS
data training. Specifically, we remove the text comprising fewer than 30 letters in the webpage
data. Inthisway,63,666sentencesareobtained. Thesentencesarematchedwiththeclosestimages
onwebpagesandprovidedtothemodel. InWikipre-training,wemaskapartofthesesentences
and train the model to predict the masked words. This helps the pre-trained model gain a better
understandingofMinecraft. RefertoSectionDinAppendixformoredetails. ThemodelinWiki
pre-trainingsharesthesamenetworkstructureandtrainingprocessasLLaVA.Subsequently,inthe
ARTNStrainingstagedescribedinSection3.1,weemploythedecoderweightofthepre-trained
modeltoinitializetheLARMdecodersshowninFig.2. Theexperimentalresultssuggestthatthe
devisedWikipre-trainingeffectivelyimprovestheconvergencespeedofLARM.
5 Experiments
5.1 ExperimentalSetup
Implementationdetails. Inourimplementation,theparametersN,H,W,L,andNI aresetto4,
480,640,512,and1,024,respectively. LARMisbuiltwith32decoders,whichcontainabout7B
parameters. Theepochnumberandlearningratearesetto1and2e-4forWikipre-training,and6
and2e-5fortheARTNStrainingstage. Thetrainingbatchsizeis16forbothWikipre-trainingand
ARNTStraining. Othertrainingdetails,likethechoiceofoptimizerandlearningscheduler,follow
theLLaVAmodel. AlltrainingexperimentsareconductedonfourRTX3090GPUs.
Experimentalsettings. Unlessspecificallystatedotherwise,theagentisrandomlyinitializedina
Minecraftenvironmentwithanemptyinventory. Adescriptionofthetargettaskisgiventotheagent,
andtheagentisexpectedtocompletethistaskindependentlywithouthumanintervention. Foreach
task,theagentistestedmultipletimesinvariousenvironmentsusingdifferenttargetdescriptions.
Thisistoverifywhetheritcancompletethegiventaskunderavarietyofchallengingcircumstances.
Thediversetasksreflectboththelong-termschedulingabilityandrobustnessofvalidatedmodels.
5.2 ComparisonwithPreviousMethods
Techtreemasterycomparison. Inthisexperiment,wecomparethetechtreemasteryofLARM
withpreviouscounterparts,includingMineAgent[12],Plan4MC[37],LLAMA-Rider[13],ReAct
6Table 1: Performance comparison with reinforcement learning skill based methods. Following
previousworks[34;13],exceptforthetasks“harvestdiamondsword”and“harvestenchantedsword”
wherethemethodsaretested3times,allthemethodsarerun30timesinothertasks. Theshown
numbersarethesuccessratesofthemethodsincompletingthesetaskssuccessfully. Thebestmethod
ishighlightedinlightgray. RefertoSectionEinAppendixformoreexperimentaldetails.
Task MineAgent Plan4MC LLaMA-RiderBase LLaMA-Rider LARM
Harveststick 0.00 0.30 0.23 0.43 1.00
Harvestcraftingtable 0.03 0.30 0.37 0.67 1.00
Harvestbowl 0.00 0.47 0.73 0.97 1.00
Harvestchest 0.00 0.23 0.67 0.77 1.00
Harvestwoodensword 0.00 0.47 0.63 0.10 1.00
Harvestfurnace 0.00 0.37 0.00 0.17 1.00
Harveststonestairs 0.00 0.47 0.00 0.57 1.00
Harveststonesword 0.00 0.10 0.00 0.00 1.00
Harvestironingot 0.00 0.47 0.03 0.13 1.00
Harvestbucket 0.00 0.20 0.00 0.00 1.00
Harvestironsword 0.00 0.20 0.00 0.00 1.00
Harvestbeef 0.33 0.43 0.03 0.03 1.00
Harvestmutton 0.35 0.33 0.00 0.03 1.00
Harvestdiamondsword 0.00 0.00 0.00 0.00 1.00
Harvestenchantedsword 0.00 0.00 0.00 0.00 0.67
Table2: PerformancecomparisonwithLLMbasedmethodsonthetechtreemastery. Following
previousworks[34],allthemethodsarerun3times. Thefractionsinthistableshowthesuccessful
trialsofeverymethodwithin3runs. Notably,allthecomparedmethodsdemandtensofprompting
iterations,whileLARMobtainstheenchantedtoolachievementwithonly1prompt. Thebestmethod
ishighlightedinlightgray. RefertoSectionEinAppendixformoreexperimentaldetails.
Achievement ReAct Reflexion AutoGPT Voyager STEVE LARM
WoodenTool 0/3 0/3 3/3 3/3 3/3 3/3
StoneTool 0/3 0/3 3/3 3/3 3/3 3/3
IronTool 0/3 0/3 3/3 3/3 3/3 3/3
DiamondTool 0/3 0/3 0/3 1/3 3/3 3/3
EnchantedTool 0/3 0/3 0/3 0/3 0/3 2/3
[36],Reflection[32],AutoGPT[4],Voyager[34],andSTEVE[38]. Amongthecomparedmethods,
MineAgentandPlan4MCarebasedonreinforcementlearning. Duetotheirinefficiencyinexploring
theenvironment,theirperformancesarelimited,andtheycanonlyconductsomesimpletasks,like
collecting logs and crafting wooden tools. Similarly, although LLAMA-Rider is an LLM-based
method,itisestablishedupontheskillslearnedfromPlan4MC.Hence,itstechmasteryachievement
isalsolimited. ReAct,Reflection,andAutoGPTareLLMsspeciallydesignedforconductingtask-
specificskillscheduling,andpreviousworksutilizethemtobuildembodiedagentsinMinecraft[34].
VoyagerisacompetitivecounterpartbuiltbasedonGPT4andobtainsthediamondtoolachievement
usingLLMforthefirsttime,whichconfirmsthesuperiorityofLLM.Nevertheless,Voyageronly
supportstextinput,andthusitsenvironmentperceptioncapabilityisrestricted. Tobridgethisgap,
STEVEleveragesthepoweroflargevision-languagemodelstoperformlong-termskillscheduling
andderivespromisingachievement.
ThetaskcompletionsuccessratesofLARMandreinforcementlearningskillbasedmethods,which
includeMineAgent,Plan4MC,andLLaMA-Rider,arereportedinTable1. Avarietyoftasksare
testedandthebestmethodismarkedinlightgray. Itcanbeobservedthatthereinforcementlearning
skill based methods display significantly poorer performance compared with LARM, which can
complete almost all these tasks with a success rate of 100%. Besides, except LARM, all these
methodsfailtocraftadiamondswordandenchantedsword.
WefurthercompareLARMwithLLMbasedmethodsinTable2. Itcanbefoundthattheresultsof
themethodsinTable2aresignificantlybetterthanthoseinTable1. Thisobservationindicatesthat
LLM-basedagentsgenerallyoutperformreinforcementlearningskillbasedones. Moreover,LARM
istheonlymethodcapableofcraftingenchantedtools,whichrequiresacomplexskillexecution
chain. Therefore,thesuperiorityofLARMisconfirmed.
7Assume you are a Minecraft agent and own a
Enchanted sword
diamond sword. The target task is to combat a
Enchanting table
horse.Whatshould youdonext?
Obsidian
Diamond pickaxe
Thenextstepistogathermaterialsforcrafting
Iron furnace
a saddle to ride the horse. The agent should
Stone pickaxe gather 2 birch planks, 1 cobblestone, and 1w/o Wiki
Furnace stickfromthesurroundingarea.
Wooden pickaxe
Crafting table Thenext action is to equip the diamond sword,
w/ Wiki pre-train
Wood log and then proceed to locate and approach the
w/o Wiki pre-train
horse. Once the horse is within range, the
0 200 400 600 800 agent should use the diamond sword to attack w/ Wiki
Training Iteration the horse.
(a) (b)
Figure8: AnalysisontheeffectoftheWikipre-training. (a)Themostadvanceditemsachievedby
themodelswithandwithoutWikipre-trainingaftervariousiterationsofARTNStraining. Itcanbe
observedthatthemodelconvergesmorequicklyandpresentsbetterperformancewiththeARTNS
trainingphase. (b)ThemodelinitializedfromLLaVAlacksknowledgeaboutMinecraftandgivesa
falseanswertothequestion. Bycontrast,themodelafterWikipre-traininganswerscorrectly. The
keypointsofthefalseandcorrectresponsesarehighlightedinbronzeandgreen,respectively.
Table3: AblationstudyontheARTNS-basedaugmentationstrategiesincludingtargettransmission
(TT),targetrephrasing(TR),andskillroll-back(SR).ThesettingofthisexperimentfollowsTable2.
TT TR SR WoodenTool StoneTool IronTool DiamondTool EnchantedTool
3/3 3/3 0/3 0/3 0/3
✓ ✓ 3/3 3/3 1/3 0/3 0/3
✓ ✓ 3/3 3/3 3/3 3/3 1/3
✓ ✓ 3/3 3/3 3/3 2/3 1/3
✓ ✓ ✓ 3/3 3/3 3/3 3/3 2/3
Inferenceefficiency.Besidesthetechtreemasteryachievement,inferenceefficiencyisalsoimportant.
However,manyaforementionedmethodsareimplementedbasedonLLMAPIs. Theinferencespeed
ofthesemethodsissignificantlyaffectedbynetworklatency,whileLARMisamodeldeployedin
localmachines. Additionally, theGPUsusedbythesemethodsmayvaryandareuncontrollable.
Therefore,directlycomparingtheirinferencespeedsismeaningless. Thankfully,themethodsbuilt
uponLLMsharesimilarnetworkarchitectures,i.e.,theTransformerstructure. Thus,itisreasonable
to compare LARM with a LLM of the same parameter volume to validate inference efficiency.
Followingthisthought,wecomparetheinferencetimecostsofLARMandLLaVA-1.6withboth
7Bparameters. Theyaretestedin100casestocomputetheaverageinferencetime,witheachcase
correspondingtoanodeinFig.6.TheoutputofLLaVAispromptedtobeasconciseaspossible.This
experimentisperformedusingoneRTX3090GPU.Throughthisexperiment,wefindthattheaverage
inference time of LLaVA on the 100 cases is 5.78 seconds while LARM is 0.85 seconds, which
indicatesthatLARMis6.8×moreefficient. TheefficiencyofLARMstemsfromitscharacteristic
thatitgeneratesasingletokenratherthanasequenceoftokensateachstep.
5.3 AblationStudy
EffectivenessofWikipre-training. LARMisinitializedusingtheweightofLLaVA,whichlacks
knowledgeaboutMinecraft. Tobridgethisgap,weintroducetheWikipre-trainingphasetoenrich
theknowledge. Inthispart,weexaminetheimpactofWikipre-trainingontheperformanceofthe
trainedmodelandvisualizetheresultsinFig.8.
InFig.8(a),wecomparethemostadvanceditemsobtainedbythemodelsafterdifferentiterationsof
ARTNStraining. Achievinganupperitemindicatesmoresuperiorperformance,andthetwocurves
correspondtomodelswithandwithoutWikipre-training. FromFig.8(a),itcanbeinferredWiki
pre-trainingacceleratestheconvergenceofLARMandcontributestobetterperformance.
Fig.8(b)presentstheresponsesofthemodelswithandwithoutWikipre-trainingtoaquestionabout
Minecraft. ThemodelwithoutWikipre-trainingisexactlytheoriginalLLaVA-1.6-7Bmodel. The
keypointsofthesetwoanswersaremarkedindifferentcolors. Wecanfindthatalthoughthequestion
isabouthowtocombatahorsewhenadiamondswordisintheinventory,themodelwithoutWiki
pre-trainingprovidesanansweraboutridingahorseinstead. Besides,theprovidedrecipeforcrafting
8𝑆 : Collect lava
!
𝑆 : Produce obsidian
$ Shorter distance
Ours: Harvest
𝑆 ": Seek water acacia log
Previous methods:
Target: Craft sticks
Harvest oak log
𝑆 : Collect water
#
Longer distance
(a) (b)
Figure9: Qualitativestudyonthemulti-viewvisioninput. (a)LARMutilizesvisioninformation
tocollectwaterandlavaandthenobtainsobsidian. Duetothespacelimit,weonlyvisualizethe
imageviewfacingtowardthetarget. (b)LARMcandynamicallyselecttheoptimalrecipe(acacia
log,highlightedinyellow)forcraftingsticksbasedonmulti-viewvisioninput. Bycontrast,previous
methodsoftendirectlyadoptthedefaultrecipe(oaklog,highlightedinorange),whichcausesthem
toexplorealongerdistanceforgatheringmaterialsandthisprocesscouldtakequitealongertime.
asaddleisincorrect. Bycontrast,themodelafterWikipre-trainingaccuratelyandindetaildescribes
thestepsforcombatingahorse. ThisdemonstratesthatWikipre-trainingeffectivelyenrichesthe
knowledgeofLARMaboutMinecraft.
EffectivenessofARTNS-basedaugmentation. Inthispart,weablatetheeffectivenessofthethree
augmentationstrategies,includingtargettransmission(TT),targetrephrasing(TR),andskillroll-back
(SR).TheresultsarereportedinTable3. GiventhelimiteddatavolumeofthecollectedARTNS
dataset,wecanfindthattheseaugmentationstrategiesarecrucialfortheperformanceofLARM.
Specifically,TRandSRboosttheresultssignificantlybyimprovingtherobustnessofLARM.TT
contributesthemosttotechtreemastery. WithoutTT,LARMoftenchoosesanincorrectskillto
performduringtheexplorationprocess. ThisisbecausecompletingacomplextaskinMinecraft
demandsmultiplestepsofdecision-making,andanywrongdecisionatanystepcouldleadtofinal
failure. TTeffectivelyreducesthefrequencyofincorrectdecision-making.
Effectivenessofmulti-viewvisioninput. Inthisstudy,wequalitativelyanalyzetheeffectivenessof
multi-viewvisioninput. AsshowninFig.9(a),theagentaimstoharvestobsidian,whichrequires
thefusionofwaterandlava. Theagentutilizesvisioninformationtosearchforwaterandlava,and
eventuallysuccessfullyproducesobsidian. Iftheinputislimitedtoasingle-viewimageinsteadof
multi-viewimages,theagentmayfailtodetectthenecessarytargets.
InFig.9(b),wecomparethebehaviorsofLARMandpreviousmethods[34;38]inselectingarecipe
forcraftingsticks. Thankstothemulti-viewvisioninput,LARMcanidentifythatthenearestsource
oflogsisanacaciatree. Bycontrast,previousmethodsoftendefaulttothestandardrecipe,which
involvesusingoaklogstocraftsticks. However,findingoaktreesmaytakeaconsiderablylonger
time. Theresultsconfirmtheeffectivenessofmulti-viewimageinputandthesuperiorityofLARM
indynamicallychoosingtheoptimalrecipe.
6 ConclusionandLimitation
Inthiswork,wehaveintroducedLARM,anembodiedintelligencepredictingsubsequentskillsinan
auto-regressivemanner. TotrainLARM,anewdataorganizationstructurehasbeenproposedanda
correspondingdatasethasbeencollected.Comparedwithpreviousmethods,LARMpresentsstronger
long-horizonschedulingabilityandfasterinferencespeed. LARMisthefirstmethodcapableof
craftingenchanteddiamondtoolsinMinecraft. Wehopethisworksignificantlycontributestothe
developmentofembodiedintelligence. Atthecurrentstage,LARMismainlytestedinMinecraft
withoutdeployinginreal-worldscenarios,whichistheprimarylimitationofthiswork.
9References
[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Al-
tenschmidt,J.,Altman,S.,Anadkat,S.,etal.: Gpt-4technicalreport.arXiv:2303.08774(2023)
3,13
[2] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A.,
Millican,K.,Reynolds,M.,etal.: Flamingo: avisuallanguagemodelforfew-shotlearning.
NeurIPS(2022) 3
[3] Anderson,P.,Wu,Q.,Teney,D.,Bruce,J.,Johnson,M.,Sünderhauf,N.,Reid,I.,Gould,S.,Van
DenHengel,A.: Vision-and-languagenavigation: Interpretingvisually-groundednavigation
instructionsinrealenvironments.In: CVPR(2018) 2
[4] Significant-gravitas/auto-gpt: An experimental open-source attempt to make gpt-4 fully
autonomous. (2023), https://github.com/Significant-Gravitas/Auto-GPT/tree/
master 7,15
[5] Baker,B.,Akkaya,I.,Zhokov,P.,Huizinga,J.,Tang,J.,Ecoffet,A.,Houghton,B.,Sampedro,
R.,Clune,J.: Videopretraining(vpt): Learningtoactbywatchingunlabeledonlinevideos.
NeurIPS(2022) 1,18,19
[6] Bredeche, N., Haasdijk, E., Prieto, A.: Embodiedevolutionincollectiverobotics: areview.
Front.Robot.AI(2018) 2
[7] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K.,
Hausman,K.,Herzog,A.,Hsu,J.,etal.: Rt-1: Roboticstransformerforreal-worldcontrolat
scale.arXiv:2212.06817(2022) 18
[8] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A.,
Shyam,P.,Sastry,G.,Askell,A.,etal.: Languagemodelsarefew-shotlearners.NeurIPS(2020)
1,3
[9] Chen,H.,Suhr,A.,Misra,D.,Snavely,N.,Artzi,Y.: Touchdown: Naturallanguagenavigation
andspatialreasoninginvisualstreetenvironments.In: CVPR(2019) 2
[10] Chrisley,R.: Embodiedartificialintelligence.ArtificialIntelligence(2003) 1
[11] Duan,J.,Yu,S.,Tan,H.L.,Zhu,H.,Tan,C.: Asurveyofembodiedai: Fromsimulatorsto
researchtasks.IETCI(2022) 2
[12] Fan,L.,Wang,G.,Jiang,Y.,Mandlekar,A.,Yang,Y.,Zhu,H.,Tang,A.,Huang,D.A.,Zhu,
Y., Anandkumar, A.: Minedojo: Building open-ended embodied agents with internet-scale
knowledge.NeurIPS(2022) 1,2,3,6,14
[13] Feng,Y.,Wang,Y.,Liu,J.,Zheng,S.,Lu,Z.: Llamarider: Spurringlargelanguagemodelsto
exploretheopenworld.arXiv:2310.08922(2023) 3,6,7,15
[14] Frazier,S.,Riedl,M.: Improvingdeepreinforcementlearninginminecraftwithactionadvice.
In: AAAI(2019) 2
[15] Guo,H.,Wu,F.,Qin,Y.,Li,R.,Li,K.,Li,K.: Recenttrendsintaskandmotionplanningfor
robotics: Asurvey.ACMComputingSurveys(2023) 18
[16] Guss, W.H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., Salakhutdinov, R.:
Minerl: alarge-scaledatasetofminecraftdemonstrations.In: IJCAI(2019) 1
[17] He,K.,Zhang,X.,Ren,S.,Sun,J.: Deepresiduallearningforimagerecognition.In: CVPR
(2016) 1
[18] Hu,E.J.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,S.,Wang,L.,Chen,W.,etal.: Lora: Low-rank
adaptationoflargelanguagemodels.In: ICLR(2021) 4
[19] Huang,C.,Mees,O.,Zeng,A.,Burgard,W.: Visuallanguagemapsforrobotnavigation.In:
ICRA(2023) 1
10[20] Kenton,J.D.M.W.C.,Toutanova,L.K.: Bert: Pre-trainingofdeepbidirectionaltransformersfor
languageunderstanding.In: NAACL(2019) 1,4
[21] Kolve,E.,Mottaghi,R.,Han,W.,VanderBilt,E.,Weihs,L.,Herrasti,A.,Deitke,M.,Ehsani,
K., Gordon, D., Zhu, Y., et al.: Ai2-thor: An interactive 3d environment for visual ai.
arXiv:1712.05474(2017) 2
[22] LeCun,Y.,Bengio,Y.,Hinton,G.: Deeplearning.Nature(2015) 1
[23] Li,J.,Li,D.,Savarese,S.,Hoi,S.: Blip-2: Bootstrappinglanguage-imagepre-trainingwith
frozenimageencodersandlargelanguagemodels.In: ICML(2023) 4
[24] Liu,H.,Li,C.,Wu,Q.,Lee,Y.J.: Visualinstructiontuning.arXiv:2304.08485(2023) 3,4,13,
14
[25] Nguyen,H.V.,Bai,L.: Cosinesimilaritymetriclearningforfaceverification.In: ACCV(2010)
4
[26] OpenAI:Chatgpt.https://openai.com/blog/chatgpt/(2023) 3,13
[27] PrismarineJS.: Prismarinejs/mineflayer: Createminecraftbotswithapowerful,stable,andhigh
leveljavascriptapi(2013) 3
[28] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,
A.,Mishkin,P.,Clark,J.,etal.: Learningtransferablevisualmodelsfromnaturallanguage
supervision.In: ICML(2021) 14
[29] Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.G., Novikov, A., Barth-Maron, G.,
Gimenez,M.,Sulsky,Y.,Kay,J.,Springenberg,J.T.,etal.: Ageneralistagent.arXivpreprint
arXiv:2205.06175(2022) 18
[30] Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J.,
Koltun,V.,Malik,J.,etal.: Habitat: Aplatformforembodiedairesearch.In: ICCV(2019) 1,2
[31] Scheller,C.,Schraner,Y.,Vogel,M.: Sampleefficientreinforcementlearningthroughlearning
fromdemonstrationsinminecraft.In: NeurIPSWorkshop(2020) 3
[32] Shinn,N.,Labash,B.,Gopinath,A.: Reflexion: anautonomousagentwithdynamicmemory
andself-reflection.arXiv:2303.11366(2023) 7,15
[33] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B.,
Goyal,N.,Hambro,E.,Azhar,F.,etal.: Llama: Openandefficientfoundationlanguagemodels.
arXiv:2302.13971(2023) 3
[34] Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., Anandkumar, A.:
Voyager:Anopen-endedembodiedagentwithlargelanguagemodels.arXiv:2305.16291(2023)
1,2,3,5,7,9,15
[35] Wang, Z., Cai, S., Chen, G., Liu, A., Ma, X., Liang, Y.: Describe, explain, plan and se-
lect: Interactiveplanningwithlargelanguagemodelsenablesopen-worldmulti-taskagents.
arXiv:2302.01560(2023) 3
[36] Yao,S.,Zhao,J.,Yu,D.,Du,N.,Shafran,I.,Narasimhan,K.R.,Cao,Y.: React: Synergizing
reasoningandactinginlanguagemodels.In: ICLR(2022) 7,15
[37] Yuan,H.,Zhang,C.,Wang,H.,Xie,F.,Cai,P.,Dong,H.,Lu,Z.: Skillreinforcementlearning
andplanningforopen-worldlong-horizontasks.In: NeurIPSWorkshop(2023) 1,3,6,15
[38] Zhao,Z.,Chai,W.,Wang,X.,Boyi,L.,Hao,S.,Cao,S.,Ye,T.,Hwang,J.N.,Wang,G.: See
andthink: Embodiedagentinvirtualenvironment.arXiv:2311.15209(2023) 2,3,4,7,9,15
11Contents
1 Introduction 1
2 RelatedWork 2
3 Algorithm 3
3.1 LargeAuto-RegressiveModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.2 Multi-viewVisionPerception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.3 SkillLibraryPreparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
4 DataandTraining 5
4.1 ARTNSDataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.2 ARTNS-basedAugmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.3 Two-phaseTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
5 Experiments 6
5.1 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
5.2 ComparisonwithPreviousMethods . . . . . . . . . . . . . . . . . . . . . . . . . 6
5.3 AblationStudy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
6 ConclusionandLimitation 9
A AppendixOverview 13
B CaseStudy 13
C Studyontrainingdatavolume 13
D WikiPre-trainingDetails 14
E DetailsofComparedMethods 14
F DataCollectionProcedures 15
G AnalysisonLLMOutput 16
H LARMTextInput 17
I SkillLibraryDescription 17
J DiscussiononLARMOutput 18
12A AppendixOverview
Due to the page-limit of the main paper, we provide more sufficient analysis of LARM in this
Appendix. ThecontentinthisAppendixisstructuredasfollows:
• Section B: In the main paper, we mainly show the result of LARM on crafting an enchanted
diamondsword. Inthissection,weshowmorebehaviorexamplesofLARMinperformingother
tasks, i.e., searching a village, building a nether portal and entering the nether, and multi-agent
collaborationtocombatzombies.
• SectionC:Asthetrainingdatavolumnisoftenimportantfordeeplearningbasedmethods,we
studyhowthedataamountaffectstheperformanceofLARMinthissection.
• SectionD:Inthispart,wefurtherdescribetheproceduresoftheWikipre-trainingstage.
• SectionE:InMinecraft,variousmethodsoftenadoptdifferenttrainingandtestingprotocolsdue
tothelackofawidelyacceptedbenchmark. Therefore,thecomparedmethodsreportedinTable1
andTable2mayadoptdifferenttrainingandvalidationproceduresfromothers. Inthissection,we
elaborateonthedetailsofthesecomparedmethods.
• SectionF:ThissectiondescribehowtheARTNSDatasetiscollected.
• SectionG:Webeginwithanin-depthanalysisontheoutputoflargelanguagemodels(LLMs).
Inthisanalysis, wecomparetheMinecraftknowledgeofvariouspopularlargelanguagemodels
(LLMs),whichincludeLLaVA[24],ChatGPT[26],andGPT-4[1]. Theexperimentalresultsreveal
thattheoutputdescriptionisnotonlyambiguousbutalsooftenincorrect.
• Section H: As explained in the main paper, the input to LAM consists of two parts, the text
information and image information. The text information describes information about the agent
status,surroundingenvironmentinformation,andsystemfeedback. Inthispart,weshowhowthe
differentinformationisorganizedasthetextinput,andseveralexamplesarepresented.
• SectionI:Asmentionedinthemainpaper,askilllibrarygenerationpipelineisdevisedtoproduce
theskillsintheskilllibrary. TheseskillsserveasbasicactionsforLARM.Inthispart,weexplain
theseskills. Specifically,thecodeofaskillispresentedasanexample. Inaddition,thenamesand
abstractdescriptionsofmoreskillsarepresented.
• SectionJ:Thissectionpresentsadiscussiononwhetherlow-levelcontrolsignalsorhigh-level
skillpredictionsaremoresuitabletoserveastheoutputofanembodiedmodel.
B CaseStudy
In Fig. 10, we present three additional examples of LARM behavior. In the first case, the agent
travelsthroughvariousbiomestofindavillage,whichdemonstratesthestrongexplorationcapability
of LARM. For the second one, the agent builds a nether portal and enters the Nether through it.
ThiscasesuggeststhepromisingconstructioncapabilityofLARM.Inthelastexample,twoagents
collaboratewitheachothertocombatalargegroupofzombies,confirmingthatmultipleLARM
modelscanefficientlycooperatetogether. Thesethreecasesrevealthepromisingperformanceof
LARMfromdifferentperspectivesandimpliesthatitcanachievemorecomplextargetswithstronger
skilllibraryandmoretrainingdata.
C Studyontrainingdatavolume
Inthisexperiment,wedelveintohowdifferentARTNStrainingdatavolumesinfluencetheobtained
highestachievement. Specifically,weseparatelytrainLARMusing10%,50%,and100%ofthe
totaltrainingdata,andtheresultsarepresentedinTable4. Itcanbeobservedthathavingsufficient
trainingdataiscriticalfortheperformance. Whenonly10%ofdataisemployed,themodelcanonly
craftstonetoolswithalowsuccessrate. Incontrast,when100%ofthedataisutilized,themodel
successfullysuccessfullyharvestsenchantedtools.
13t t t t
! " # $
t t t t
! " # $
t t t t
! " # $
Figure10:MorebehaviorexampleillustrationsofLARM,whichincludesearchingavillage,building
anetherportalandenteringthenether,andmulti-agentcollaborationtocombatzombies.
Table 4: Ablation study on the influence of ARTNS training data volume. The setting of this
experimentfollowstheoneinTable2.
DataRatio WoodenTool StoneTool IronTool DiamondTool EnchantedTool
10% 3/3 1/3 0/3 0/3 0/3
50% 3/3 3/3 2/3 1/3 0/3
100% 3/3 3/3 3/3 3/3 2/3
D WikiPre-trainingDetails
Toexplainourmethodclearly,wefurtherexplainthetrainingdetailsoftheWikiPre-trainingstagein
thissection. TheproceduresarevisualizedinFig.11forclarity. Asshown,thedatainWikiwebpages
includebothtextandimageinformation. TheworkMinddojo[12]crawls34GofMinecraftdatafrom
Internetandcomposesitasamulti-modaldataset. However,thisdatasetcontainsmanyshortphrases
thatofferlimitedusefulinformation. Toresolvethisproblem,weexcludetextwithfewerthan30
characters. Fortheremainingdata,wematcheverytextwithitsclosestimageasamulti-modaldata
pairandusesittotrainLARM.AsdepictedinFig.11,wemaskaportionofthetextinformationand
constructapromptfromtheremainingtextandimage. Themodelistrainedtofillinthemasked
words. Themodeloutputformat,trainingloss,andhyper-parameterdetailsfollowLLaVA[24]. Each
datapairisutilizedtwiceintheWikipre-trainingstage,equatingtotwoepochsoftraining.
E DetailsofComparedMethods
InMinecraft,asthereisnowidelyacceptedbenchmark,differentmethodsoftenemployvarying
trainingpipelines,testingprotocalsandtrainingdata. Therefore,themethodscomparedinTable1
andTable2arenotalwaystrainedinthesamesetting. However,thisproblemdoesnotaffectthe
significantcontributionofLARM,becausethemainfocusofembodiedAIresearchishowintelligent
the agent can achieve, and LARM obtains the achievement significantly better than all previous
counterparts. Besides,wedescribedbelow,thetestingprotocalofLARMismorechallengingthan
allcomparedmethods. Tobemoretransparentaboutthedifferenceamongvariousmethods, we
elaborateonthedetailsofallcomparedmethodsinthissectiononebyone.
MineAgent[12].MineAgentisthebaselinemethodprovidedbyMinddojo.Thisworkfirstfine-tunes
the CLIP model [28] based on numerous Internet data and uses the data to guide the training of
14
egalliV
a
hcraeS
rehteN
eht
retnE
tabmoC
tnega-itluMText: A sword is made from one of six materials, in order of
increasing quality and expense: wood, gold, stone, iron,
Text
Image diamond and netherite.
Image:
Wiki Webpage A Crawled Text-Image Pair
I will give you an incomplete sentence about Minecraft and please complete it. The
providedimagemayberelevanttothissentence.Thesentenceis“Aswordismadefrom
oneofsixmaterials,inorderofincreasingqualityandexpense:”.Pleasedirectlyanswer
theaddedwords.
Theaddedwordsare:“wood,gold,stone,iron,diamondandnetherite”.
Figure11: LARMispre-trainedbasedonWikidatatoenrichtheknowledgeaboutMinecraft. Inthis
stage,themodelisaskedtopredicttheremainingwordsinasentencebasedonthegivencontext.
reinforcementlearningbasedalgorithms. ThemethodistestwithMinddojo. Ineachtestingiteration,
theagentisinitializedwithsometoolsandaskedtocompleteataskdescribedbygiventext.
Plan4MC[37]. Plan4MCisareinformcementlearningbasedmethod. Itsplitsthetasksofanagent
intobasicskillsandtrainsanagenttolearnthemonebyoneinahierarchicalway. Themodelistest
basedonMinddojo. Ineachtestingiteration,theagentisinitializedwithsometoolsandaskedto
completeataskdescribedbygiventext.
LLaMA-Rider[13]. LLaMA-RiderisaLLMobtainedbyfine-tuningLLaMA.Foritsdeveloping
modelprocess,thisworkfirstmakestheagentexploretheenvironmentbyitselfandcollectsome
dataduringthisprocess. Then,theycomplilethecollecteddataintoadatasettofine-tunetheLARM
modelinasupervisedmanner. TheoutputofLLaMA-Rideristextandthetextisusedtocallthe
skillsgeneratedbyPlan4MC.ThemodelistestusingMinddojo. Ineachtestingiteration,theagentis
initializedwithsometoolsandaskedtocompleteataskdescribedbygiventext.
Voyager[34]. VoyagerisamethodimplementedbasedonGPT4andhasnotrainingprocess. Its
maincontributionisdesigningamulti-steppromptgenerationpipeline. Theskillsrequiredbyitare
implementedbasedonMineflayer. Whenatargettaskisgiven,themethodpromptsGPT4toknow
whichskillshouldbeexecutedandgraduallyrealizesthetarget. Thismethodistestinthestandard
Minecraftgameenvironment. Inthebeginingoftesting,theagentisinitializedwithnoanytool.
ReAct[36], Reflexion[32], andAutoGPT[4]. AllthethreemethodsareLLMsandcanreason
aboutwhatskillshouldbeperformedthroughmulti-stepquestionanswering. Theirperformancesin
MinecraftaretestintheworkVoyager. TheircalledskillsarealsoimplementedbasedonMineflayer
andthetestingprotocolsarethesameasVoyager.
STEVE[38]. STEVEislargevision-languagemodel. Theauthorsofthisworkcollectsatraining
datasetincludingbothviedosandtest-imagepairsandusesthedatatofine-tuneaLLaMAmodel.
Theobtainedmodelisusedtoconductmulti-stepreasoningandcallskillsimplementedbasedon
Mineflayer. ItstestingprotocolfollowsVoyager.
F DataCollectionProcedures
Asdescribedinthemainpaper,thedataisorganizedintheARTNSformatandwecollectthedata
manually. Specifically,foreachgiventargettask,wefirstdecomposethetaskintomultiplesteps
andeachstepcorrespondstoabasicskill. Weoperatetheagenttoexecutetheseskillsonebyone.
Aftereachskillisperformed,themomentofwhichcorrespondstoanodeinFig.6,werecordallthe
concernedinformation,suchasmulti-viewimages,agentpositioninformation,etc. Aftercollecting
thedata,welabelthedependencerelationsamongallnodes.
15G AnalysisonLLMOutput
Asdiscussedinthemainpaper,theoutputofLLMsisoftenambiguousforagentstoselectingspecific
actions. Inaddition,wefindthatthegeneratedresponseofexistingLLMsisoftenincorrectdueto
theirlimitedknowledgeaboutMinecraft. Inthefollowing,wecomparethreevariouspopularLLMs,
i.e.,LLaVA-1.6-7B,ChatGPT,andGPT-4inMinecraftrelatedquestion-answertest.
Dialogue1: Combatonehorse withadiamondsword .
Prompt:
YouareanexpertofplayingMinecraft. Iwilltellyouatargettask,someinformationabout
yourMinecraftagentandenvironment. Youneedtotellmethenextactiontheagentshould
performtocompletethetargettask. Youranswershouldbeconciseanddescribethenext
actionclearly. Forcraftingitems,youshouldspecifytheexpecteditemnumber. Thetarget
taskistokillonehorse. Thetimeisnightandtheweatherisclear. Theagentpositioninthe
worldcoordinatesystemis(-584,67,384). Theinventoryinformationincludes: “chicken:1.
mutton:2. sand:1. cobblestone:44. birch_planks:4. dark_oak_planks:1. cobblestone_slab:1.
apple:1. stick:2. diamond_pickaxe:1. diamond_shovel:1. beef:7. white_wool:1. porkchop:2.
diamond_sword:1. leather:4. feather:2. dirt:13.”.
LLaVA-1.6-7B:
Thenextactiontheagentshouldperformistogathermaterialsforcraftingasaddle,whichis
requiredtoridethehorse. Theagentshouldfirstgather2birchplanks,1cobblestone,and1
stickfromthesurroundingarea. Oncethematerialsaregathered,theagentshouldthencrafta
saddleusingthegatheredmaterials. Thesaddlecanbecraftedbyplacingthebirchplankson
thecobblestone,followedbythestickontopofthebirchplanks. Aftercraftingthesaddle,
theagentcanthenmountthehorseandrideit.
ChatGPT:
Thenextactiontheagentshouldperformistolocateahorseandattackittokillit.
GPT-4:
Giventhecurrentcircumstancesandthetargettask,thenextactiontheagentshouldperform
istoequipthediamondsword. Afterequippingthesword,theagentshouldthenproceed
tolocateandapproachthehorse. Oncethehorseiswithinrange,theagentshouldusethe
diamondswordtoattackandkillthehorse.
Dialogue2: Combatonehorse withoutasword .
Prompt:
YouareanexpertofplayingMinecraft. Iwilltellyouatargettask,someinformationabout
yourMinecraftagentandenvironment. Youneedtotellmethenextactiontheagentshould
performtocompletethetargettask. Youranswershouldbeconciseanddescribethenext
actionclearly. Forcraftingitems,youshouldspecifytheexpecteditemnumber. Thetarget
taskistokillonehorse. Thetimeisnightandtheweatherisclear. Theagentpositioninthe
worldcoordinatesystemis(-584,67,384). Theinventoryinformationincludes: “chicken:1.
mutton:2. sand:1. cobblestone:44. birch_planks:4. dark_oak_planks:1. cobblestone_slab:1.
apple:1. stick:2. diamond_pickaxe:1. diamond_shovel:1. beef:7. white_wool:1. porkchop:2.
leather:4. feather:2. dirt:13.”.
GPT-4:
Giventhetaskandthecurrentinventory,theagentdoesnothavetheappropriateequipment
tokillahorseefficiently. Therefore,thenextactionshouldbetocraftastonesword. Todo
this,theagentneedstouse2cobblestonesfromtheinventoryandcreateacraftingtableif
onedoesn’texist. Oncetheswordiscrafted,theagentcanthenproceedtolocateandkillthe
horse.
16Accordingtheabovetwodialogues,theoutputofLLaVAissignificantlyincorrect. Firstofall,the
promptisaboutcombatingahorse,whiletheresponseofLLaVAisabouthowtorideahorse. In
addition, saddle cannot be crafted by the agent itself in Minecraft, the given recipe is also false.
By contrast, ChatGPT gives a relatively better answer, locate a horse first and then combat it.
Nevertheless,ChatGPToverlooksthefactthatthereisadiamondswordintheinventorylistand
combating a horse without any weapon is quite inefficient. In contrast to LLaVA and ChatGPT,
theknowledgeofGPT-4aboutMinecraftismuchmorericher. Whenadiamondswordisinthe
inventorylist,GPT-4knowstheagentshouldfirstequipthesword,andthenfindahorseandcombat
it. Conversely,ifthereisnoweaponintheinventorylist,GPT-4understandsastoneswordshouldbe
craftedfirstbasedontheavailableresourcesintheinventorylist. GPT-4doesnotselectstocrafta
ironswordbecausethereisironingot. Additionally,craftingawoodenswordisnotchosenbecause
itsdamageisweakerthanthestonesword.
Based on the aforementioned result and analysis, we can observe that GPT-4 behaves the best
amongthesethreeLLMs. Thisobservationsuggeststhatthemodelsizeandtrainingdatavolumn
arealsocrucialfortheLLMsappliedtoMinecraft. However,althoughGPT-4presentspromising
performance,itshowsslowinferencespeedandthedemandsnumerousGPUsforinference. These
characteristicsmakeitunsuitableforembodiedapplications,whichrequirereal-timeoperationsand
have limited local computing resource. Compared with these LLMs, LARM is much more fast,
computingeconomical,andpresentssuperiorperformance. Therefore,LARMismoresuitablefor
developingembodiedintelligencethanthemanypreviousLLMs.
H LARMTextInput
Asmentionedinthemainpaper,thetextinputisimportantforLARMtoperceiveinformation. Inthe
following,weintroducehowdifferenttextinformationisorganizedasaninputsentence,whichis
shownbelow.
AnexampleofhowdifferenttextinformationisorganizedasansentenceinputtoLARM.
OriginalInformation:
target: Create1ironshovel.
time: day.
weather: clear.
lastaction: craft_16_oak_planks.
position: (-54,64,56).
lastactionexecutionresult: success.
inventorylist: oak_log:12. oak_planks:16.
TheSentenceInputtoLARM:YouareanexpertofplayingMinecraft. Iwilltellyoua
targettask,someinformationaboutyourMinecraftagentandenvironment. Youneedtotell
methenextactiontheagentshouldperformtocompletethetargettask. Youranswershould
beconciseanddescribethenextactionclearly. Forcraftingitems,youshouldspecifythe
expecteditemnumber. Thetargettaskistocreate1ironshovel. Thetimeisdayandthe
weather is clear. The agent position in the world coordinate system is (-54, 64, 56). The
inventoryinformationincludes: “oak_log:12. oak_planks:16.”.
I SkillLibraryDescription
Asmentionedbefore,theskilllibraryisimportantforLARM,astheskillsincludedinthislibrary
serveasthebasicactionsforLARM.Toclearlyexplainthedetailsoftheseskills,thecodeofone
skillisgivenasanexample. Inaddition,wepresentthetitlesanddescriptionof10skillsinthispart.
Skill-Codeexample1: "Thecodeofcombatingonemob".
// Description: The function to combat one mob.
17async function combat_one_mob(bot, animal_name)
{
try
{
// Find the entity that matches the name of the animal
let tgt_animal = bot.nearestEntity(entity => entity.name.
toLowerCase() === animal_name)
// If the entity is not found, display an error message
if(tgt_animal == null)
{
bot.chat(’Do not find the entity ${animal_name}.’)
return false
}
// If the entity is found, attack it
else
{
await bot.pvp.attack(tgt_animal)
}
}
// If an error occurs, log the error and return false
catch(err)
{
console.log(err)
return false
}
}
Thefunctionnamesanddescriptionsof10representativeskills.
collect_1_oak_log: Collect1oaklog.
craft_16_spruce_planks: Craft16spruceplanks.
craft_1_wooden_shovel: Craft1woodenshovel.
collect_32_stone: Collect32stoneblocks.
collect_1_lava_bucket: Collect1bucketoflava.
craft_1_diamond_sword: Collect1diamondsword.
equip_diamond_sword: Equiptheowneddiamondsword.
combat_one_zombie: Combat1zombie.
go_ground: Gototheground.
eat_1_steak: Eat1steak.
J DiscussiononLARMOutput
As described in the main paper, LARM takes a skill (high-level action) as one atomic operation.
Simultaneously,thereisalsosomemethodsthatdirectlyoutputalow-levelaction,likethemouse
clickorkeyboardpress[5]. Inthissection,wepresentadiscussiononwhetherweshouldemploy
high-levelorlow-levelactionsinembodiedartificialintelligence(AI).
Inpracticalembodiedapplications,differingagentsusuallyhavesignificantlydifferentmechanical
structures. Althoughthereareworksthattryunifyingthembytrainingend-to-endmodelswitha
largeamountofparametersandcollectingnumerousdata,theobtainedperformanceisstilllimited
andcannotbedeployedinrealindustrialenvironments[29;7]. Conversely,existingdeployedrobots
aremainlyimplementedbasedonmanytraditionalcontrolmodules[15],whicharesimilartothe
pre-generatedskillsinMinecraft. Therefore,makingmodelsoutputhigh-levelactionsisbeneficialto
alleviatethedeploymentdifficulty.
18Similarly,inMinecraft,althoughbuildingVPT[5]consumesnumerousresource,itsobtainedhighest
achievementismuchweakerthanLARM,whileLARMistrainedusingonly4RTX3090GPUs.
Therefore, when the data and computing resource is not very abundant, exploring models with
high-leveloutputisapromisingchoice.
19