LLM-BASED COOPERATIVE AGENTS USING INFORMATION
RELEVANCE AND PLAN VALIDATION
SeungWonSeo JunhyeokLee
DepartmentofArtificialIntelligence DepartmentofSoftwareConvergence
KyungHeeUniversity KyungHeeUniversity
Yongin,SouthKorea Yongin,SouthKorea
ssw03270@khu.ac.kr bluehyena123@khu.ac.kr
SeongRaeNoh HyeongYeopKang∗
DepartmentofArtificialIntelligence DepartmentofSoftwareConvergence
KyungHeeUniversity KyungHeeUniversity
Yongin,SouthKorea Yongin,SouthKorea
rhosunr99@khu.ac.kr siamiz@khu.ac.kr
ABSTRACT
We address the challenge of multi-agent cooperation, where agents achieve a common goal by
interactingwitha3Dsceneandcooperatingwithdecentralizedagentsundercomplexpartialob-
servations. Thisinvolvesmanagingcommunicationcostsandoptimizinginteractiontrajectoriesin
dynamicenvironments. Ourresearchfocusesonthreeprimarylimitationsofexistingcooperative
agentsystems. Firstly,currentsystemsdemonstrateinefficiencyinmanagingacquiredinformation
throughobservation,resultingindecliningplanningperformanceastheenvironmentbecomesmore
complexwithadditionalobjectsorgoals. Secondly,theneglectoffalseplansinpartiallyobservable
settingsleadstosuboptimalcooperativeperformance,asagentsstruggletoadapttoenvironmental
changesinfluencedbytheunseenactionsofotheragents. Lastly,thefailuretoincorporatespatial
dataintodecision-makingprocessesrestrictstheagent’sabilitytoconstructoptimizedtrajectories.
Toovercometheselimitations,weproposetheRElevanceandValidation-EnhancedCooperative
LanguageAgent(REVECA),anovelcognitivearchitecturepoweredbyGPT-3.5.REVECAleverages
relevanceassessment,planvalidation,andspatialinformationtoenhancetheefficiencyandrobustness
ofagentcooperationindynamicandpartiallyobservableenvironmentswhileminimizingcontinuous
communicationcostsandeffectivelymanagingirrelevantdummyobjects. Ourextensiveexperiments
demonstratethesuperiorityofREVECAoverpreviousapproaches,includingthosedrivenbyGPT-
4.0. Additionally,auserstudyhighlightsREVECA’spotentialforachievingtrustworthyhuman-AI
cooperation. WeexpectthatREVECAwillhavesignificantapplicationsingaming,XRapplications,
educational tools, and humanoid robots, contributing to substantial economic, commercial, and
academicadvancements.
Keywords Multi-agentplanning·Cooperativeagent·Dynamicenvironments·LargeLanguageModels
1 Introduction
Digitalagentscollaboratingwithhumansarecrucialingames,educationalplatforms,andvirtualuniverses. Knownas
Non-PlayerCharacters(NPCs),theseagentsenhanceuserimmersionincommercialapplicationsbutoftenoperate
onpre-scriptedbehaviors,limitingadaptabilityandresultinginrepetitiveactions. IngameslikeWorldofWarcraft
andDiablo,NPCswhomustbeescortedbytheplayeroftenfailtocomprehendthesituationalcontext,rushinginto
∗Correspondingauthor:siamiz@khu.ac.kr
4202
yaM
72
]IA.sc[
1v15761.5042:viXraLLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
dangerandcreatingchallengesforplayers. Thislackofadaptiveintelligencedetractsfromthegamingexperience,
highlightingtheneedforadvancedAI-drivenagents.
Recently,OpenAIshowcasedanAGIRobotusinglargelanguagemodels(LLMs)forrecognitionofnaturallanguage
and household tasks. Inspired by this, we aim to create adaptive, intelligent cooperative agents. By facilitating
naturallanguageinteractions,high-levelplanning,andscene-dedicatedlow-levelcontrollers,theseLLM-basedagents
willaddresscooperativeproblemsmoreeffectivelythantraditionalmethodsrelyingonstatic, learnablemodelsor
reinforcementlearning.
This paper introduces REVECA, a RElevance and Validation-Enhanced Cooperative Language Agent, an LLM-
basedagentframeworkaddressingdecentralizedcontrol,costlycommunication,complextasks,partiallyobservable
environments, and noisy settings. Similar to prior work [1], we focus on multi-objective household tasks using
well-constructedvirtualsettings. Specifically,wefocusontwodecentralizedagentscooperatingonamulti-objective,
long-horizonhouseholdtaskundercomplexpartialobservations. Additionally,continuouscommunicationincurstime
costs,andirrelevantdummyobjectsaddnoise,complicatingtheenvironment.
The primary advancements of REVECA over previous works are threefold. Firstly, REVECA leverages LLMs to
assesstherelevanceofnewlyacquiredinformationbeforestorage,prioritizingitandreducingreasoningcomplexityfor
planning. Previousresearch[1,2]storedallinformationinmemory,leadingtoperformancedeclinesduetoincreasing
memoryrequirements,limitedcontextlength,andintensifyingreasoningcomplexity.Somestudies[3,4]usedqueuesto
retainrecentK piecesofinformation,butthisresultedinsuboptimalplanningduetothelimitedhistoricalinformation.
Secondly,REVECAusesanovelmodularcognitivearchitecturetomitigatefalseplanning. Thisoccursinpartially
observableenvironmentsbecausechangesinducedbycollaboratorsarenotimmediatelyreflectedintheagent’smemory.
Traditional approaches [5] update information via constant pre-task communication, which is costly. REVECA’s
ValidationModulechecksplanvaliditybyassessinginformationrelevanceandestimatingcollaborators’recentplans,
enablingcommunicationonlywhennecessaryandimprovingefficiencyandeffectiveness.
Lastly, REVECAexcelsinplanningbyincorporatingspatialinformation. Previousstudiesoftenneglectedspatial
information due to its integration challenge in language models. Although some studies [1, 3] have attempted to
incorporatespatialinformationthroughimageortextrepresentations,theydidnotreportperformanceimprovements.
WeattempttoaddressthisbyconsideringthespatialdistancebetweenagentsandobjectsinLLMreasoningprocesses.
Byemployingapromptingtechnique,thateffectivelyintegratesnumericaldistances,REVECAenhancesthecooperative
performance.
Weconductedcomparativeanalysis,ablationstudies,anduserstudies,usingthreemulti-roomsimulationenvironments:
CommunicativeWatch-And-Help(C-WAH),ThreeDWorldMulti-AgentTransport(TDW-MAT),andNoisy-C-WAH.
Noisy-C-WAH, a variant of C-WAH with dummy obstacles, tested REVECA in noisy settings. Results show that
REVECAachieveshighersuccessrates,efficiency,androbustnessthanrecentstudies.
Insummary,ourmaincontributionsareasfollows:
• Addressedcooperativechallenges: decentralizedcontrol,costlycommunication,complexlong-horizontasks,
partiallyobservableenvironments,andnoisymulti-objectsettings.
• DevelopedanLLM-basedrelevanceestimationmethodforoptimalplanningandrobustnessinnoisyenviron-
ments.
• Introducedanagentframeworkthatvalidatesplansandincorporatesspatialinformationformitigatingfalse
planning.
• ConductedextensiveevaluationsthatshowREVECA’ssuperiorperformanceoverpreviousstudiesinC-WAH,
TDW-MAT,andNoisy-C-WAH.
• Performedauserstudydemonstratingpotentialtrustworthyhuman-AIcooperation.
2 RelatedWork
Researchoncooperativeagentshasalonghistory [6,7]. Thetraditionalcooperativeagenthasbeenstudiedacross
variousdirections, mainlyleveragingreinforcementlearningtechnique[8,9,10,11,12,13,14,15,16,17,18,19,
20,21,22,23,24,25]. Theseapproacheshavefacilitatedthedevelopmentofagentsthatcanautonomouslylearnto
cooperatebymaximizingcumulativerewardsthroughtrialanderrorinvarioussimulatedsettings. Notablestudies
haveexploredaspectssuchasvisualreinforcementlearningfornavigationandinteraction[8],mappingstatespacesto
actionseffectively[9],andenhancingtherobustnessoflearningalgorithmsinmulti-agentsystems[10].
2LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
Environment
Memory Module
Observation Module Low-level
Action Skill Common Goal
Book
Execution
Rooms and Objects Module Communication Self-information
History History
Communication Validation Planning
Module Module Module Scene Collaborators-
Observation information
History History
Other Agents
Figure1: TheinteractionflowbetweenthesixmodulesofREVECAandtheenvironment.
Someotherresearchershaveaimedtodevelopplatformstotestcooperativeagents[26,27,28,29,30,31,32,33,34,
35,36,37]. Theseprovidestandardizedenvironmentstobenchmarktheperformanceandgeneralizationcapabilitiesof
theagents.
However,asignificantlimitationofpreviousworkisthelackofsupportfornaturallanguage-basedcommunication
betweenagents[38,39,40,11,30,34]. Theeffectivecommunicationiscrucialforenhancingcollaboration,especially
incomplex,multi-agentenvironments. Naturallanguageprocessingcapabilitiesenableagentstoexchangeinformation
inamoreintuitiveandhuman-likemanner.
LLMshaverecentlyattractedsignificantinterestinbothacademiaandindustry,alsogainingattentioninthefieldof
agents[41,42]. Thecommon-sensereasoningcapabilitiesandnaturallanguageprocessingabilitiesofLLMshave
contributedtoenhancingtheagent’sdecision-making[13,43,44,45,46,47,48]andplanning[49,50,51,52,53,5,54]
abilities.
RecentstudiesonLLM-basedembodiedagents[1,3,2,4]representasignificantstepforwardinthisfield. These
agentsutilizeLLMsforunderstandingtheenvironment,planningtasks,andcommunicatingwithhumanusersand
otheragents. However,existingstudiesfacesuboptimalperformanceduetoissueswithLLMs,suchasinadequate
spatialdataprocessingcapabilities[55],performancedegradationwithlargeinputdata[56],andinsufficientreasoning
abilitieswithcomplexreasoningtasks[57].
ThispaperintroducesREVECA,anLLM-basedembodiedagentframeworkdesignedtoaddressthelimitationsofthe
previousworksbyleveraginginformationrelevance,planvalidation,andeffectivecommunicationstrategies.
3 ProblemDefinition
TheproblemwefocusonisanextensionofthedecentralizedpartiallyobservableMarkovdecisionprocess(DEC-
POMDP)[58,59,60,10,1,4],characterizedbythetuple(n,S,{Σ },{A },{O },T,G,h). Here,nrepresentsthe
i i i
numberofagents;S denotesafinitesetofstates; A = AW ∪AC denotestheactionsetforagenti,comprisinga
i i i
finitesetofworldactionsAW andacommunicationactionAC tosendamessageσ ∈ Σ . Theobservationsetfor
i i i i
agenti,O =OW ×OC,comprisesworldobservationsOW andthesetofpossiblemessagestheagentcanreceive
i i i i
fromteammatesOC =Σ ×···×Σ . Thejointtransitionmodel,T(s,a,s′)=p(s′ |s,a),definestheprobabilityof
i 1 n
transitioningtoanewstates′ ∈Saftertakingjointactiona∈A ×···×A instates∈S. ThesetG={g ,...,g }
1 n 1 k
definesthegoalwithseveralsub-goalsfortheagentstocomplete. Decentralizedagents,startingfromaninitialstates ,
0
mustplanandexecuteactionstoreachthegoalGasefficientlyaspossiblewithintheplanninghorizonh,basedon
partialobservationsthatguidetheirdecision-makingprocesses.
Thispaperfocusesonascenariowheretwodecentralizedintelligentagentscollaborateonalong-horizonrearrangement
task[61]withinanindoormulti-roomenvironment,utilizingnoise-freebroadcastcommunication. Whilethefocus
is on two agents, the methods and experiments are generalizable to scenarios with more agents. The agents can
performnavigationactions,interactionactions,andcommunicationactions. Thetaskincludesseveralpredicatesg
i
withspecifiedcounts,suchasIN(cupcake,fridge):2,whichrepresentsthesub-taskofplacingtwocupcakesinthe
fridge.
3LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
Environment Observation Module Memory Module
Alice, I finished our sub-
goal. I completed to move
the pancake. Common Goal
Communication History
Rooms and Objects Other Agents Communication Information Scene Information
NextStep
CCoommmmuunniiccaattiioonn MMoodduullee Validation Module Planning Module Scene Observation
History
Bob, did you move the
pancake?
Collaborators-
information History
Alice thinks Bob
already moved it. K plans
Execution Module Self-information History
Grab the pancake.
Did Bob
Low-level Action
already
Alice thinks Bob Skill Book
<Alice> [grab] <pancake> (107) move it?
didn’t move it. Best Plan LLM
Used Not Used
Figure2: AnexamplescenariodemonstratingREVECA’scomprehensiveoperationalflow,highlightingtheinteraction
betweenvariousmodules,collaborators,andtheenvironmenttoachieveacommongoal.
4 REVECAframework
Ourframeworkcomprisessixkeymodules: CommunicationModule,ObservationModule,MemoryModule,Planning
Module, Validation Module, and Execution Module. Communication Module facilitates natural language-based
informationsharing. ObservationModulegathersandcategorizesinformationintofourrelevancelevelswhilethe
agentexplorestheenvironment. MemoryModulecomprisessixcomponents: commongoal,communicationhistory,
sceneobservationhistory,self-informationhistory,collaborators-informationhistory,andalow-levelactionskillbook.
Itisresponsibleforstoringandupdatingdata. PlanningModuleemploysmemorydatatogenerateefficientplans,
consideringinformationrelevancy,predictedproximityofallagents,andcollaborators’informationhistory. Validation
Modulechecksforfalseplansbygeneratingandconfirmingcollaborators’possibleinteractionscenariosbetween
observationandplanningtimes,thendiscardsandreformulatesplansifneeded. ExecutionModuleexecutesvalidated
plansusingthelow-levelactionskillbook. ThemodulardesignofREVECAisillustratedinFigure1andanexample
scenariopresentingREVECA’sworkflowisdepictedinFigure2.
4.1 Communicationforinformationsharing
TheCommunicationModule,facilitatingnaturallanguageinformationsharing,isinvokedinfourcases. 1)Simulation
Initiation: Agentsexchangeinitialinformationabouttheirlocationsandsurroundingobjects. 2)ValidationRequests:
Agentsqueryabouttaskhistory. 3)ResponsetoValidationRequests: Agentsprovidetaskcompletionhistory. 4)
Sub-goalAchievement: Agentsannouncesub-goalcompletion. Thedetailedillustrationsforeachcasearepresented
inFigure3.
4.2 Observation-time: RelevanceEstimationandStorage
Duringobservationtime,agentsacquirefourtypesofinformation: scene,self-agent,communication,andcollaborator-
agent. Scene information includes object details like 3D position, ID, name, room, and interactions. Self-agent
information includes the held object, current position, and completed plan history. These are obtained at every
simulationstep. Communicationinformation,conveyedinnaturallanguage,isobtainedwhenitoccurs. Collaborator-
agentinformationisacquiredwhencommunicatingwithorseeingaspecificagent. Itincludesthesamedetailsas
self-agentinformation.
Sceneandcommunicationinformationarestoredinsceneobservationhistoryandcommunicationhistory,respectively,
withfourrelevancelevels(strong,medium,low,none)assessedbyLLMs. Thisprioritizesinformation,avoidingthe
needtore-referenceallmemoryentriesandreducingthereasoningcomplexityduringtheLLM-basedplanningprocess.
AnexampleofdeterminingtherelevancescoreforsceneinformationisdepictedinFigure4.
4LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
Alice & Bob Milk Common goal Find and move a milk and a cupcake to the bedroom table.
Door Table Cupcake
Milk is here. Nothing here. Hey, Bob! I’m currently in
Kitchen the kitchen and I found a
(a)Simulation milk on the kitchen table.
Initiation
OK, Alice! I’m currently in
the bedroom and I found
nothing.
Bedroom
Living Room What happened to the
milk?
Environment at Initial state (b) Validation Alice! I want to grab the
milk at the kitchen. Have
Request
Kitchen you already done that?
Kitchen
Bedroom I’ll put the milk on the
Living Room (c)Response to bedroom desk. At step 5, I have grabbed
Validation the milk and currently I’m
Environment at step 10 Requests holding it.
Kitchen
Kitchen
I complete the sub-goal.
(d)Sub-goal Bob, I completed the sub-
Bedroom Achievement goal. I moved the milk to
the bedroom table.
Living Room
Environment at step 20
Figure3: FourcaseswhentheCommunicationModuleisinvoked: (a)SimulationInitiation. (b)ValidationRequests.
(c)ResponsetoValidationRequests. (d)Sub-goalAchievement.
Common Find and put milk, an apple, Relevance Strong Medium Low None
goal and a pear in the fridge. level
Action Action Action
Walk to the kitchen table. Grab the milk. Put milk in the fridge.
Figure4: Anexampleofdeterminingtherelevancescoreforsceneinformation. Ifthecommongoalistoputmilkin
thefridge,thefridgeandmilkhavestrongrelevance. Sincethemilkcouldalsobeplacedincontainers,thedrawerand
cabinetareestimatedtohaveamoderaterelevance.
Self-agentinformation,whichisfullyobservable,isdirectlystoredinself-informationhistory. Conversely,collaborator-
agentinformation,whichisacquireddiscontinuously,isinterpolatedtoaddressgapsbeforebeingstoredincollaborators-
informationhistory.
4.3 Planning-time: Real-TimeAdaptation
Existingresearchonagent-basedsimulations[62,63,64,65]oftenconstructssequentiallong-termplansforspecific
goals,effectiveinindividualtaskswheretheagentisthesoleentityinfluencingtheenvironmentandincentralized
settings where a manager oversees the environment. However, in decentralized cooperative tasks within partially
observableenvironments,agentsmustdynamicallyadapttheirplanstochanges,aslong-termplansdevisedatearlier
stagescanbecomeimpracticalandleadtofalseplans.
5LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
1. Creating a set of Kplans (K=3) 2. Reasoning through chain-of-thought
Plan 1 Plan 2 Plan 3 Prompt
LLM Output
Of the actions I can take,
which are the most efficient In this situation, since the goal is to
to achieve the common goal? find and put the target objects into the
… fridge, the most efficient action to
Let’s think step by step. LLM take would be to grab the pancake.
Common goal LLM Input Option C is strong in relevance to the
[Open]kitchen cabinet [Open]bedroom cabinet [Grab] pancake goal, and since you are closer than
F p apain n pd c leak ia nend tha enp d fu rit da gn ea . pR roe xla imtiv ite y C Alo ls ie c eto pR roe xla imtiv ite y Cl Bos oe b to pR roe xla imtiv ite y C Alo ls ie c eto SA e ls fe t i no ff o K rmp al ta ion ns B i fs oo rl b o y ct oao ut e t th d oe , gik t r i awtc boh iue t.ln d w bh e e mre o t rh ee e p ffa in cic ea nk te
Relevance Medium Relevance Medium Relevance Strong Collaborator information
Score Score Score
Figure 5: The two-step planning procedure of REVECA. In the first step, a set of K = 3 plans are created, each
planevaluatedforproximityandrelevance. ThesecondstepinvolvesreasoningthroughtheseplansusingtheLLM
withchain-of-thoughtprompting. TheLLMevaluatestheplansbasedoninputdata,includingself-informationand
collaboratorinformation,recommendingthemostefficientaction.
Environment Bathroom Alice & Bob Without Validation Module
Door
Table
Fridge
Kitchen
Milk
Cupcake
Alice wonders that the milk has been Alice walks to the fridge that Alice fails to find the milk, and she
Bedroom moved from the fridge. already explored by Bob. guesses that Bob already found it.
Living room
With Validation Module
Bob, I’m now in the living
Common goal room, and now I’m
planning to find milk. Do
Put a milk and a cupcake to the bedroom table. you know where is the milk?
Yes, I’ve already moved it
Action History from the fridge. I think
Unknown to Alice you’d rather move the
Known to Alice 3. Bob found a milk in the fridge. Alice thinks Bob did it, cupca tk he e b be ec da rou ose m I .’m in
1. Alice found a milk in the fridge.4. Bob grabbed the milk. but she's not sure.
2. Alice moved to the living 5. Bob moved to the bedroom. Alice gets information through Alice grabs the cupcake based on
room. 6. Bob put milk to the bedroom table. communication with Bob. the communication derived plan.
Figure6: DemonstrateshowREVECA’sValidationModuleenhancescollaborationbetweenAliceandBob. Theaction
historyindicateswhichactionsAliceknowsaboutandwhichareunknowntoher. Withoutvalidation,Alice’splanfails
duetomissinginformationaboutBob’sactions. WhentheValidationModuleisused,AlicecommunicateswithBob,
confirmingthestatusofthemilkandsuccessfullycompletingthetaskbygrabbingthecupcake.
Toaddressthis,weproposeaPlanningModulethatdevisesthenextoptimalactionateachdecisionpoint,bothatthe
startofthesimulationandfollowingtheexecutionofeachaction. ItstartswithcreatingK plansbasedoninformation
relevanceandrelativeproximityofcollaborators. LLMsthenutilizezero-shotchain-of-thoughtprompting[66]to
selecttheoptimalplan,prioritizingtasksbasedonrelevance,self-information,andcollaborators’information. This
approachimplicitlyguidestheplanningprocesstoprioritizetasksthatarecrucialforachievingthegoalwhileensuring
theagent’sactionsaremoreefficientthanthoseofitscollaborators. Thistwo-stepplanningprocedureisdepictedin
Figure5.
4.4 Validation-time: Scenario-basedValidation
Evenawell-constructedplanmaybecomeinvalidduetotheenvironmentalchangescausedbycollaboratorsduringthe
intervalbetweenobservationandplanning. Inpartiallyobservableenvironments,determiningwhethersuchchanges
haveoccurredischallengingfortheagent.
A straightforward method to resolve this is to revisit the object’s location or query all collaborators about their
interactions. However,thiscanresultininefficientpathplanningandincursignificantcommunicationcosts.
6LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
Figure7: ThelayoutsofthetwovirtualenvironmentsusedintheREVECAexperiments. (a)C-WAH.(b)TDW-MAT.
Toaddressthis,REVECAincludesscenario-basedplanvalidation,estimatingplanvalidityusingstoredinformation.
Theagentgeneratesallpossiblescenariosfortheobject’sinteractionswithcollaboratorsbetweentheobservationand
planningtimes. TheagentthenusescollaboratorinformationfromitsMemoryModuletoasktheLLMtodetermine
themostlikelyscenarioemployingthezero-shotchain-of-thoughtpromptingtechnique[66]. Ifthescenariowhereno
collaboratorinteractedwiththeobjectismostlikely,theagentassumestheplanisvalid. Ifanotherscenarioislikely,the
agentcommunicateswiththecollaboratortoconfirmtheinteraction. Ifconfirmed,theagentdiscardstheoriginalplan,
createsanewone,andrepeatsthescenario-basedvalidation. ThescenarioswithandwithouttheValidationModuleare
showninFigure6.
4.5 ExecutingNavigationandContextualActions
Aftertheplanisdetermined,theExecutionModuleretrievessub-taskinformationfromtheMemoryModuletoidentify
thetargetlocation. WeusetheA-starsearchalgorithmforefficientpathfinding. Uponapproachingtheobject,theagent
retrievesapredefinedanimationfromthelow-levelactionskillbooktoexecutetheplannedinteraction.
5 Experiment
5.1 ExperimentalSettings
Weconductedexperimentsusingthreetypesofindoormulti-roomsimulationenvironments: C-WAH,TDW-MAT[1],
andNoisy-C-WAH.Toensureafaircomparison,weadoptedthedetailedparametersdefinedinthepreviousstudy’s
settings[1].
C-WAH[1],anextendedversionoftheWatch-And-HelpChallenge[34],isbuiltontherealisticmulti-agentsimulation
platform,VirtualHome[67]. Thisenvironmentincludesfivehouseholdtasks,suchassettingthetableanddoingthe
dishes,formingthecommongoal. Ourexperimentsconsistof10episodes,eachwithfivedifferenthouseholdtasks
acrosstwotestenvironments. InC-WAH,agentscanacquireorprovideinformationthroughcommunicationwith
otheragentswhileexecutinginstructions. Whenanagententersaspecificroom,itcanobserveallobjectsnotinsidea
containerlikefridgesormicrowaves. Toobserveobjectsinsidecontainers,theagentisrequiredanadditionalactionof
openingcontainers. Agentsarelimitedtousingupto500charactersperframetomimicreal-worldcommunication
costs. Horizonhissetto250steps,andeachtaskincludes3to5subgoals(orobjects). Failingtoachievethecommon
goalwithinthe250stepsresultsinanepisodefailure. ThelayoutisshowninFigure7(a).
TDW-MAT,anextendedversionoftheThreeDWorldTransportChallenge[68],isbuiltontheTDW[69]. Itfeatures
morenaturalobjectplacementsandavarietyofobjectsandcontainersthatassistintransportingitems. Thecommon
goalsinvolvetransportingitemsintwocategories: FoodandStuff. Eachepisodeincludes10targetobjects,and2to5
containersareplacedtofacilitatemovingmultipleitemssimultaneously. UnlikeC-WAH,agentsinTDW-MATcannot
obtaincompleteroominformationwithoutperforminga360-degreerotationin15-degreeincrements. Communication
islimitedto500charactersperframe,andhissetto3,000steps. ThelayoutisshowninFigure7(b).
Noisy-C-WAHisacustomizedversionofC-WAHaugmentedwithanadditional10or20dummyobjectsperepisode
todemonstratetherobustnessofourframeworkinnoisyenvironments. Thesedummyobjectsincreasethecomplexity
oftheenvironment,challengingtheagents’observation,planning,andcommunicationprocesses.
InC-WAHandNoisy-C-WAH,weevaluatetheagentperformanceusingSimulationSteps(SS),TravelDistance(TD),
CommunicationSteps(CS),andCharacterCounts(CC)tomeasurethetimecosttoachievethecommongoal,the
averagedistancetraveled,thetimecosttoperformcommunication,andtheaveragenumberofcharactersusedinthe
communication,respectively. InTDW-MAT,performanceisevaluatedbasedonthesuccessrateoftransportingitems,
7LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
includingtheoverallsuccessrate(TOTAL),andspecificsuccessratesforobjectscategorizedasFood(FOOD)and
Stuff(STUFF).
5.2 REVECAandBaselines
Incomparativeexperiments,ourREVECAwasevaluatedagainstthreebaselines:theMCTS-basedHierarchicalPlanner
(MHP),theRule-basedHierarchicalPlanner(RHP),andtheCooperativeEmbodiedLanguageAgent(CoELA).We
comparedREVECAwithMHPandCoELAinC-WAH,withRHPandCoELAinTDW-MAT,andwithCoELAin
Noisy-C-WAH.
REVECAleveragesGPT-3.5throughtheOpenAIAPI.AlthoughGPT-4isthelatestmodel,itshightokencostmakes
extensiveexperimentationimpractical. Thus,weemployedGPT-3.5withdefaultparameters: atemperatureof0.7,
top-pof1,andamaximumtokenlimitof256.
MHP,fromtheWatch-And-HelpChallenge,isaHierarchicalPlannerwithahigh-levelplannerutilizingMCTSanda
low-levelplannerbasedonregressionplanning[70].
RHP,fromtheThreeDWorldTransportChallenge,isaHierarchicalPlannerwithahigh-levelplannerusingheuristics
andalow-levelA-start-basedplannerfornavigationusingasemanticmapandFrontierExplorationstrategy.
CoELAleveragesLLMsforplanningandcommunication,enablingcollaborativetask-solving. Forafaircomparison
andcostefficiency,weusedGPT-3.5-drivenCoELA(CoELA-3.5)fortheexperimentandprovidedGPT-4.0-driven
CoELA(CoELA-4.0)performancefromtheCoELAmanuscript[1]foracomprehensiveanalysisofdifferentLLM
capacities.
5.3 ComparativeAnalysisResults
Table1: ComparativeexperimentalresultswithC-WAH.
SS↓ TD(m)↓ CS↓ CC↓
REVECA 48.90 40.34 6.90 79.65
CoELA-3.5 71.90 61.29 10.40 158.46
CoELA-4.0 57.00 / / /
MHP 69.40 58.96 / /
Table2: ComparativeexperimentalresultswithTDW-MAT.
TOTAL(%)↑ FOOD(%)↑ STUFF(%)↑
REVECA 0.86 0.88 0.84
CoELA-3.5 0.64 0.69 0.59
CoELA-4.0 0.71 0.82 0.61
RHP 0.79 0.83 0.76
Table1showsthatREVECAoutperformsotherbaselinesinC-WAH.Specifically,GPT-3.5-drivenREVECAusesfewer
stepstocompletetaskscomparedtoCoELA-4.0,CoELA-3.5,andMHP,demonstratingsuperioreffectiveness. The
performanceofCoELAsignificantlydropswithGPT-3.5,evenbelowMHP,highlightingCoELA’srelianceonGPT-4.0’s
reasoningcapability. REVECAalsoshowsfeweraveragetraveldistances,communicationsteps,andcharactersused
percommunication.
Table2presentsTDW-MATresults,whereGPT-3.5-drivenREVECAoutperformsallbaselines,includingGPT-4.0-
drivenCoELA,acrossallmetrics(TOTAL,FOOD,andSTUFF).
In Noisy-C-WAH, the experiment included 10 or 20 additional dummy objects. As shown in Figure 8, REVECA
outperformsCoELA-3.5inallmetrics. CoELA’sstrategyofstoringallacquiredinformationintextformleadstoa
significantdeclineinreasoningperformance. Asdummyobjectsincrease,theadvantagesofusingrelevancescores
becomemoreevident.
8LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
Figure8: ComparativeexperimentalresultswithNoisy-C-WAH,incorporatingvaryingnumbersofdummyobjects.
Table3: AblationstudyresultwithC-WAH.Thehighestperformancescoreineachmetricishighlightedinunderlined
bold,whilescoresthatsurpassthoseofREVECAareindicatedinbold.
SS↓ TD(m)↓ CS↓ CC↓
REVECA 48.90 40.34 6.90 79.65
w/ospatialinfo 64.80 56.93 9.60 86.81
w/orelevancescore 43.80 35.75 8.20 79.64
w/oValidationModule 54.20 48.47 5.10 46.90
alwaysaskbeforeaction 46.90 38.05 9.10 84.00
w/ocollaborator’sinfo 59.60 51.18 9.2 83.25
K=2 55.40 46.32 7.50 66.47
K=4 53.30 45.00 9.30 86.72
R=3 53.00 46.02 8.10 77.41
R=5 54.90 46.75 8.70 88.47
9LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
Table4: AblationstudyresultwithNoisy-C-WAHaugmentedby20dummyobjects. Thehighestperformancescorein
eachmetricishighlightedinunderlinedbold,whilescoresthatsurpassthoseofREVECAareindicatedinbold.
SS↓ TD(m)↓ CS↓ CC↓
REVECA 63.60 54.37 8.80 103.03
w/ospatialinfo 89.00 77.86 13.10 126.00
w/orelevancescore 95.30 79.66 9.80 110.33
w/oValidationModule 75.80 68.53 5.00 46.84
alwaysaskbeforeaction 56.80 49.97 13.30 127.14
w/ocollaborator’sinfo 80.40 70.96 9.70 110.42
K=2 65.20 55.68 7.40 109.16
K=4 72.50 62.38 9.90 120.30
R=3 64.60 55.15 8.60 95.03
R=5 76.90 64.86 7.70 100.05
5.4 AblationStudyResults
Todemonstratethesignificanceofeachcomponentinourframework,weconductedanablationstudywithinC-WAH
andNoisy-C-WAHenvironments,thelatteraugmentedwith20dummyobjects. TheresultsarepresentedinTable3
and Table4.
Initially,weexaminedtheREVECAwithoutspatialinformationandwithoutrelevancescorestoevaluatetheirimpacton
performance. Theabsenceofspatialinformationdegradedperformanceacrossallmetrics,highlightingitsimportance
ineffectiveplanning. OmittingrelevancescoresimprovedSSandTDscoresbutworsenedCSscoresinC-WAHwhile
degradingallscoresinNoisy-C-WAH.Thissuggeststhatrelevancescoresarecrucialinmorecomplexenvironments
likeNoisy-C-WAHwithover20objects. Toaidreadersunderstand,examplescenarioswithandwithoutusingspatial
informationareillustratedinFigure9
Next,wetestedREVECAwithouttheValidationModuleandwithasettingwherecommunicationoccurredbefore
everyaction. TheabsenceoftheValidationModuleworsenedSSandTDscores. Conversely,frequentcommunication
beforeeveryactionslightlyimprovedSSandTDbutsignificantlydegradedCSandCCscores,indicatingthatthe
ValidationModuleenhancesperformancewithoutexcessivecommunication.
WealsotestedtheREVECAwithoutconsideringcollaboratorinformation,resultinginsignificantlydegradedperfor-
manceacrossallscores. Thisunderscorestheimportanceofcollaboratorinformation,evenwhensomedetailsare
inferred.
Lastly, we varied the number of plans (K) and relevance levels (R) considered by the LLM planner. Our default
parametersofK =3andR=4yieldedthebestperformance.
5.5 UserStudyResults
WeconductedauserstudytoevaluateREVECA’sabilitytocollaborateseamlesslywithhumanstoachievecommon
goals. Fiveparticipants(fourmenandonewoman)withanaverageageof23.4yearswererecruited. Theexperiment
wasconductedintheC-WAHenvironmentusingfourmethods: REVECA,REVECAwithoutcommunication(w/o
comm),REVECAwithalwaysaskbeforeaction(alwaysask),andCoELA.Participantssharedthesameobservation
andactionspaceastheagentsandinteractedwiththeenvironmentbyselectingactionsfromapredefinedlist.
Participantscompleted5sub-goalswitheachmethod. Aftereach,theyanswereda7-pointLikertscale(1: strongly
disagree,7: stronglyagree)questionnaireonfourresearchquestions: 1)Didtheagentrespondappropriatelytoyour
intentions? (Appropriateness),2)Wastheinteractionwiththeagenthelpfulinachievingthegoal? (Usefulness),3)Did
theagent’sperformancehelpachievethegoalquickly? (Efficiency),and4)Didyoufeelasenseoftrustwiththeagent?
(Trust)The“w/ocomm"methodexcludedAppropriatenessandUsefulnessquestions,duetothelackofinteraction.
Participantsweretheninterviewedabouteachmethod.
AsshowninFigure10,REVECAscoredhighestacrossallfourquestions,excellinginhumancollaboration.Participants
notedthatCoELAoftenproducedmessagesfocusedonstatusreportsandplanningratherthanaddressingtheirquestions,
resultinginlowerscoresinAppropriatenessandUsefulness.Inthe“alwaysask"condition,participantsfoundtheagent’s
10LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
(a) Without and spatial information
Common goal 1 2 3 4 5 6
Alice plans to grab Alice walks to the green Alice grabs the green Alice walkstotheliving Alice walkstothe Alice put plates in the
Put green plate and green plate. plate in the kitchen. plate. room to grab the red plate. kitchen again. sink.
red plate in the sink.
Environment
Alice Door Table
Sink Plate
Bathroom (b) With and spatial information (Ours)
Kitchen
Bedroom 1 2 3 4 5 6
Living room Alice plans to grab the Alice walks to the red Alice walks to the Alice grabs the green Alice walkstothe Alice put plates in the
red plate first. plate and grabs it. kitchen. plate. kitchen again. sink.
Alice’s Memory
The green plate is on the kitchen table.
The red plate is on the living room table.
The sink is in the kitchen.
I’m currently in the living room.
I’m holding nothing.
Figure9: TwoexamplescenariosinvolvingAlice,whoneedstoplaceagreenplateandaredplateintothesink. This
demonstratesthedifferenceinpathplanningefficiencywithandwithoutusingspatialinformation. Inscenario(a),
Alice,withoutspatialinformation,followsasuboptimalpathtocompletethetask,movingbackandforthunnecessarily.
Scenario(b)showcasestheimprovedefficiencywhenspatialinformationisused,enablingAlicetoplanamoredirect
andlogicalroute.
repetitivequestionsdisruptiveanddemotivating. RegardingTrust,participantsnotedthatthelackofcommunicationin
the“w/ocomm"methodmadeitdifficulttounderstandtheagent’sactionsandsituation,therebyhinderingtrust-based
collaboration.
6 Conclusion
Inthispaper,weintroducedREVECA,anLLM-drivencognitivearchitecturedesignedformulti-objectivehousehold
tasks,enablingefficientcooperationbetweendecentralizedagentsundercomplexpartialobservations. Byleveraging
relevance assessment, spatial information, and plan validation, REVECA enhances agent cooperation in dynamic
andpartiallyobservableenvironments,whileminimizingcontinuouscommunicationcostsandeffectivelymanaging
irrelevantdummyobjects.
REVECA’sapplicationsextendbeyondhouseholdtaskstogaming,virtualuniverses,andeducationalenvironments. In
gaming,REVECAcanrevolutionizeNPCbehavior,enablingadaptiveandintelligentinteractionsforaricherplayer
experience. Invirtualuniverses,REVECAcanenhanceuser-agentinteractions,facilitatingcollaborativetasksand
socialinteractionsformoreseamlessexperiences. Ineducation,REVECAcanmanagecomplextasksandprovide
real-timefeedback,supportingpersonalizedlearningandcreatinginteractivesimulationsandvirtualtutorstailoredto
individualneeds.
11LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
Figure10: UserstudyresultswithC-WAH.Thisfigureillustratesthemeanscoresandassociatedstandarderrorsfor
responsestofourresearchquestions.
However,REVECAhaslimitationsthatwarrantfurtherexploration. Itseffectivenessinhighlydynamicandunpre-
dictableoutdoorsettingsremainstobevalidated. Theframeworkhasbeentestedprimarilywithtwoagents,andscaling
tomoreagentsmayintroducenewchallengesinsocialinteractionsandcoordination. Additionally,REVECA’suseofa
low-levelactionskillbookcouldbeenhancedbyintegratingrecentadvancementsinanimationgenerationtechnologies,
enablingmorerealisticandcontext-sensitiveactions.
AddressingtheselimitationscouldmakefutureREVECAevenmorerobustandapplicableacrossabroaderrangeof
multi-agentenvironmentsandtasks.
References
[1] HongxinZhang,WeihuaDu,JiamingShan,QinhongZhou,YilunDu,JoshuaBTenenbaum,TianminShu,and
Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint
arXiv:2307.02485,2023.
[2] WenhaoLi,DanQiao,BaoxiangWang,XiangfengWang,BoJin,andHongyuanZha. Semanticallyalignedtask
decompositioninmulti-agentreinforcementlearning. ArXiv,abs/2305.10865,2023.
[3] CeyaoZhang,KaijieYang,SiyiHu,ZihaoWang,GuangheLi,YiEveSun,ChenZhang,ZhaoweiZhang,Anji
Liu,Song-ChunZhu,XiaojunChang,JungeZhang,F.Yin,YitaoLiang,andYaodongYang. Proagent: Building
proactivecooperativeagentswithlargelanguagemodels. InAAAIConferenceonArtificialIntelligence,2023.
[4] HongxinZhang,ZeyuanWang,QiushiLyu,ZheyuanZhang,SunliChen,TianminShu,YilunDu,andChuang
Gan. Combo: Compositionalworldmodelsforembodiedmulti-agentcooperation. ArXiv,abs/2404.10775,2024.
[5] HuaoLi,YuQuanChong,SimonStepputtis,JosephCampbell,DanaHughes,MichaelLewis,andKatiaP.Sycara.
Theoryofmindformulti-agentcollaborationvialargelanguagemodels. InConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,2023.
[6] PeterStoneandManuelaVeloso. Multiagentsystems:Asurveyfromamachinelearningperspective. Autonomous
Robots,8:345–383,2000.
[7] SvenGronauerandKlausDiepold. Multi-agentdeepreinforcementlearning: asurvey. ArtificialIntelligence
Review,55(2):895–943,2022.
[8] Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, and Ali
Farhadi. Visualsemanticplanningusingdeepsuccessorrepresentations. InProceedingsoftheIEEEinternational
conferenceoncomputervision,pages483–492,2017.
12LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
[9] DipendraMisra, AndrewBennett, ValtsBlukis, EyvindNiklasson, MaxShatkhin, andYoavArtzi. Mapping
instructionstoactionsin3denvironmentswithvisualgoalprediction. arXivpreprintarXiv:1809.00786,2018.
[10] ChrisAmato,GeorgeDimitriKonidaris,LesliePackKaelbling,andJonathanP.How. Modelingandplanning
withmacro-actionsindecentralizedpomdps. Thejournalofartificialintelligenceresearch,64:817–859,2019.
[11] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda,
CharlesBeattie,NeilCRabinowitz,AriSMorcos,AvrahamRuderman,etal. Human-levelperformancein3d
multiplayergameswithpopulation-basedreinforcementlearning. Science,364(6443):859–865,2019.
[12] TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFarquhar,JakobFoerster,andShimon
Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of
MachineLearningResearch,21(178):1–51,2020.
[13] PratyushaSharma,AntonioTorralba,andJacobAndreas. Skillinductionandplanningwithlatentlanguage. arXiv
preprintarXiv:2110.01517,2021.
[14] HengyuanHu,AdamLerer,BrandonCui,LuisPineda,NoamBrown,andJakobFoerster. Off-belieflearning. In
InternationalConferenceonMachineLearning,pages4369–4379.PMLR,2021.
[15] SiyiHu,FengdaZhu,XiaojunChang,andXiaodanLiang. Updet: Universalmulti-agentreinforcementlearning
viapolicydecouplingwithtransformers. arXivpreprintarXiv:2101.08001,2021.
[16] LinghuiMeng,MuningWen,YaodongYang,ChenyangLe,XiyunLi,WeinanZhang,YingWen,HaifengZhang,
JunWang,andBoXu. Offlinepre-trainedmulti-agentdecisiontransformer: Onebigsequencemodeltacklesall
smactasks. arXivpreprintarXiv:2112.02845,2021.
[17] DJStrouse,KevinMcKee,MattBotvinick,EdwardHughes,andRichardEverett. Collaboratingwithhumans
withouthumandata. AdvancesinNeuralInformationProcessingSystems,34:14502–14515,2021.
[18] AndreiLupu,BrandonCui,HengyuanHu,andJakobFoerster. Trajectorydiversityforzero-shotcoordination. In
Internationalconferenceonmachinelearning,pages7204–7213.PMLR,2021.
[19] KeaneLucasandRossEAllen. Any-play: Anintrinsicaugmentationforzero-shotcoordination. arXivpreprint
arXiv:2201.12436,2022.
[20] MuningWen,JakubKuba,RunjiLin,WeinanZhang,YingWen,JunWang,andYaodongYang. Multi-agent
reinforcementlearningisasequencemodelingproblem. AdvancesinNeuralInformationProcessingSystems,
35:16509–16521,2022.
[21] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu. Thesurprising
effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems,
35:24611–24624,2022.
[22] RuiZhao,JinmingSong,YufengYuan,HaifengHu,YangGao,YiWu,ZhongqianSun,andWeiYang. Maximum
entropypopulation-basedtrainingforzero-shothuman-aicoordination. InProceedingsoftheAAAIConference
onArtificialIntelligence,volume37,pages6145–6153,2023.
[23] YangLi,ShaoZhang,JichenSun,YaliDu,YingWen,XinbingWang,andWeiPan. Cooperativeopen-ended
learningframeworkforzero-shotcoordination. InInternationalConferenceonMachineLearning,pages20470–
20484.PMLR,2023.
[24] YangLi,ShaoZhang,JichenSun,WenhaoZhang,YaliDu,YingWen,XinbingWang,andWeiPan. Tackling
cooperativeincompatibilityforzero-shothuman-aicoordination. arXivpreprintarXiv:2306.03034,2023.
[25] YifanZhong,JakubGrudzienKuba,XidongFeng,SiyiHu,JiamingJi,andYaodongYang. Heterogeneous-agent
reinforcementlearning. JournalofMachineLearningResearch,25:1–67,2024.
[26] RyanLowe,YiIWu,AvivTamar,JeanHarb,OpenAIPieterAbbeel,andIgorMordatch. Multi-agentactor-critic
formixedcooperative-competitiveenvironments. Advancesinneuralinformationprocessingsystems,30,2017.
[27] EricKolve,RoozbehMottaghi,WinsonHan,EliVanderBilt,LucaWeihs,AlvaroHerrasti,MattDeitke,Kiana
Ehsani,DanielGordon,YukeZhu,etal. Ai2-thor: Aninteractive3denvironmentforvisualai. arXivpreprint
arXiv:1712.05474,2017.
[28] TianminShuandYuandongTian. M3ˆrl: Mind-awaremulti-agentmanagementreinforcementlearning. arXiv
preprintarXiv:1810.00147,2018.
[29] FeiXia,AmirRZamir,ZhiyangHe,AlexanderSax,JitendraMalik,andSilvioSavarese. Gibsonenv: Real-
worldperceptionforembodiedagents. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages9068–9079,2018.
13LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
[30] MikayelSamvelyan,TabishRashid,ChristianSchroederDeWitt,GregoryFarquhar,NantasNardelli,TimGJ
Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent
challenge. arXivpreprintarXiv:1902.04043,2019.
[31] ManolisSavva,AbhishekKadian,OleksandrMaksymets,YiliZhao,ErikWijmans,BhavanaJain,JulianStraub,
JiaLiu,VladlenKoltun,JitendraMalik,etal. Habitat: Aplatformforembodiedairesearch. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pages9339–9347,2019.
[32] MohitShridhar,JesseThomason,DanielGordon,YonatanBisk,WinsonHan,RoozbehMottaghi,LukeZettle-
moyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages10740–10749,2020.
[33] FanboXiang,YuzheQin,KaichunMo,YikuanXia,HaoZhu,FangchenLiu,MinghuaLiu,HanxiaoJiang,Yifu
Yuan,HeWang,etal. Sapien: Asimulatedpart-basedinteractiveenvironment. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages11097–11107,2020.
[34] XavierPuig,TianminShu,ShuangLi,ZilinWang,Yuan-HongLiao,JoshuaBTenenbaum,SanjaFidler,and
AntonioTorralba. Watch-and-help: Achallengeforsocialperceptionandhuman-aicollaboration. arXivpreprint
arXiv:2010.09890,2020.
[35] AishwaryaPadmakumar,JesseThomason,AyushShrivastava,PatrickLange,AnjaliNarayan-Chen,Spandana
Gella,RobinsonPiramuthu,GokhanTur,andDilekHakkani-Tur. Teach: Task-drivenembodiedagentsthatchat.
InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages2017–2025,2022.
[36] ChengshuLi,RuohanZhang,JosiahWong,CemGokmen,SanjanaSrivastava,RobertoMartín-Martín,Chen
Wang,GabraelLevine,MichaelLingelbach,JiankaiSun,etal. Behavior-1k: Abenchmarkforembodiedaiwith
1,000everydayactivitiesandrealisticsimulation. InConferenceonRobotLearning,pages80–93.PMLR,2023.
[37] QinhongZhou,SunliChen,YisongWang,HaozheXu,WeihuaDu,HongxinZhang,YilunDu,JoshuaBTenen-
baum,andChuangGan. Hazardchallenge: Embodieddecisionmakingindynamicallychangingenvironments.
arXivpreprintarXiv:2401.12975,2024.
[38] JiechuanJiangandZongqingLu. Learningattentionalcommunicationformulti-agentcooperation. InNeural
InformationProcessingSystems,2018.
[39] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Michael G. Rabbat, and Joelle
Pineau. Tarmac: Targetedmulti-agentcommunication. InInternationalConferenceonMachineLearning,2018.
[40] MicahCarroll,RohinShah,MarkKHo,TomGriffiths,SanjitSeshia,PieterAbbeel,andAncaDragan. Onthe
utilityoflearningabouthumansforhuman-aicoordination. Advancesinneuralinformationprocessingsystems,
32,2019.
[41] ZhihengXi,WenxiangChen,XinGuo,WeiHe,YiwenDing,BoyangHong,MingZhang,JunzheWang,Senjie
Jin,EnyuZhou,etal. Theriseandpotentialoflargelanguagemodelbasedagents: Asurvey. arXivpreprint
arXiv:2309.07864,2023.
[42] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,JiakaiTang,XuChen,
YankaiLin,etal. Asurveyonlargelanguagemodelbasedautonomousagents. FrontiersofComputerScience,
18(6):1–26,2024.
[43] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin
Akyürek,AnimaAnandkumar,etal. Pre-trainedlanguagemodelsforinteractivedecision-making. Advancesin
NeuralInformationProcessingSystems,35:31199–31212,2022.
[44] SherryYang,OfirNachum,YilunDu,JasonWei,PieterAbbeel,andDaleSchuurmans. Foundationmodelsfor
decisionmaking: Problems,methods,andopportunities. arXivpreprintarXiv:2303.04129,2023.
[45] Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. Cognitive architectures for
languageagents. arXivpreprintarXiv:2309.02427,2023.
[46] GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,andAnimaAnand-
kumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels. arXivpreprintarXiv:2305.16291,
2023.
[47] WenlongHuang,FeiXia,DhruvShah,DannyDriess,AndyZeng,YaoLu,PeteFlorence,IgorMordatch,Sergey
Levine,KarolHausman,etal. Groundeddecoding: Guidingtextgenerationwithgroundedmodelsforrobot
control. arXivpreprintarXiv:2303.00855,2023.
[48] SiyuYuan,JiangjieChen,ZiquanFu,XuyangGe,SohamShah,CharlesRobertJankowski,YanghuaXiao,and
DeqingYang. Distillingscriptknowledgefromlargelanguagemodelsforconstrainedlanguageplanning. arXiv
preprintarXiv:2305.05252,2023.
14LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
[49] WenlongHuang,F.Xia,TedXiao,HarrisChan,JackyLiang,PeterR.Florence,AndyZeng,JonathanTompson,
IgorMordatch, YevgenChebotar, PierreSermanet, NoahBrown, TomasJackson, LindaLuu, SergeyLevine,
KarolHausman,andBrianIchter. Innermonologue: Embodiedreasoningthroughplanningwithlanguagemodels.
InConferenceonRobotLearning,2022.
[50] WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch. Languagemodelsaszero-shotplanners:
Extractingactionableknowledgeforembodiedagents. InInternationalConferenceonMachineLearning,pages
9118–9147.PMLR,2022.
[51] ShreyasSundaraRaman,VanyaCohen,EricRosen,IfrahIdrees,DavidPaulius,andStefanieTellex. Planning
withlargelanguagemodelsviacorrectivere-prompting.InNeurIPS2022FoundationModelsforDecisionMaking
Workshop,2022.
[52] VishalPallagani,BharathMuppasani,KeerthiramMurugesan,FrancescaRossi,LiorHoresh,BiplavSrivastava,
FrancescoFabiano,andAndreaLoreggia. Plansformer: Generatingsymbolicplansusingtransformers. arXiv
preprintarXiv:2212.08681,2022.
[53] MaitreyGramopadhyeandDanielSzafir.Generatingexecutableactionplanswithenvironmentally-awarelanguage
models. In2023IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),pages3568–3575.
IEEE,2023.
[54] ZihaoWang,ShaofeiCai,GuanzhouChen,AnjiLiu,XiaojianShawnMa,andYitaoLiang.Describe,explain,plan
andselect: interactiveplanningwithllmsenablesopen-worldmulti-taskagents. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[55] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohnA.Gehrke,EricHorvitz,EceKamar,PeterLee,
YinTatLee,Yuan-FangLi,ScottM.Lundberg,HarshaNori,HamidPalangi,MarcoTulioRibeiro,andYiZhang.
Sparksofartificialgeneralintelligence: Earlyexperimentswithgpt-4. ArXiv,abs/2303.12712,2023.
[56] MoshLevy,AlonJacoby,andYoavGoldberg. Sametask,moretokens:theimpactofinputlengthonthereasoning
performanceoflargelanguagemodels. ArXiv,abs/2402.14848,2024.
[57] Tomer David Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. ArXiv,
abs/2302.08399,2023.
[58] DanielS.Bernstein,ShlomoZilberstein,andNeilImmerman. Thecomplexityofdecentralizedcontrolofmarkov
decisionprocesses. InConferenceonUncertaintyinArtificialIntelligence,2000.
[59] Claudia V. Goldman and Shlomo Zilberstein. Optimizing information exchange in cooperative multi-agent
systems. InAdaptiveAgentsandMulti-AgentSystems,2003.
[60] MatthijsT.J.Spaan,GeoffreyJ.Gordon,andNikosA.Vlassis. Decentralizedplanningunderuncertaintyfor
teamsofcommunicatingagents. InAdaptiveAgentsandMulti-AgentSystems,2006.
[61] Dhruv Batra, Angel X. Chang, S. Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun, Sergey Levine,
JitendraMalik,IgorMordatch,RoozbehMottaghi,ManolisSavva,andHaoSu. Rearrangement: Achallengefor
embodiedai. ArXiv,abs/2011.01975,2020.
[62] LeiWang,WanyuXu,YihuaiLan,ZhiqiangHu,YunshiLan,RoyKa-WeiLee,andEe-PengLim. Plan-and-solve
prompting: Improvingzero-shotchain-of-thoughtreasoningbylargelanguagemodels. InAnnualMeetingofthe
AssociationforComputationalLinguistics,2023.
[63] SimengSun,Y.Liu,ShuoWang,ChenguangZhu,andMohitIyyer. Pearl: Promptinglargelanguagemodelsto
planandexecuteactionsoverlongdocuments. ArXiv,abs/2305.14564,2023.
[64] NoahShinn,FedericoCassano,BeckLabash,AshwinGopinath,KarthikNarasimhan,andShunyuYao. Reflexion:
languageagentswithverbalreinforcementlearning. InNeuralInformationProcessingSystems,2023.
[65] ArchikiPrasad,AlexanderKoller,MareikeHartmann,PeterClark,AshishSabharwal,MohitBansal,andTushar
Khot. Adapt: As-neededdecompositionandplanningwithlanguagemodels. ArXiv,abs/2311.05772,2023.
[66] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Largelanguagemodels
arezero-shotreasoners. ArXiv,abs/2205.11916,2022.
[67] XavierPuig,KevinKyunghwanRa,MarkoBoben,JiamanLi,TingwuWang,SanjaFidler,andAntonioTorralba.
Virtualhome: Simulatinghouseholdactivitiesviaprograms. 2018IEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages8494–8502,2018.
[68] ChuangGan,SiyuanZhou,JeremySchwartz,SethAlter,AbhishekBhandwaldar,DanGutfreund,DanielL.K.
Yamins,JamesJ.DiCarlo,JoshH.McDermott,AntonioTorralba,andJoshuaB.Tenenbaum. Thethreedworld
transportchallenge:Avisuallyguidedtask-and-motionplanningbenchmarktowardsphysicallyrealisticembodied
ai. 2022InternationalConferenceonRoboticsandAutomation(ICRA),pages8847–8854,2021.
15LLM-BasedCooperativeAgentsusingInformationRelevanceandPlanValidation
[69] Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius,
AbhishekBhandwaldar,NickHaber,MegumiSano,KunoKim,EliasWang,DamianMrowca,MichaelLingelbach,
AidanCurtis,KevinT.Feigelis,DanielBear,DanGutfreund,DavidCox,JamesJ.DiCarlo,JoshH.McDermott,
JoshuaB.Tenenbaum,andDanielL.K.Yamins. Threedworld: Aplatformforinteractivemulti-modalphysical
simulation. ArXiv,abs/2007.04954,2020.
[70] RichardE.Korf. Planningassearch: Aquantitativeapproach. Artif.Intell.,33:65–88,1987.
16