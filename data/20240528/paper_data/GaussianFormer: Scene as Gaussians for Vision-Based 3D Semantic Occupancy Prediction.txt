GaussianFormer: Scene as Gaussians for
Vision-Based 3D Semantic Occupancy Prediction
Yuanhui Huang1, Wenzhao Zheng1,2∗, Yunpeng Zhang3,
Jie Zhou1, and Jiwen Lu1†
1Tsinghua University 2University of California, Berkeley 3PhiGent Robotics
https://wzzheng.net/GaussianFormer
huangyh22@mails.tsinghua.edu.cn; wenzhao.zheng@outlook.com;
yunpengzhang97@gmail.com; {jzhou,lujiwen}@tsinghua.edu.cn
2DImage 3DGaussian 3DOccupancy
Representation Representation Representation
Gaussian-to-Voxel
GaussianFormer
Splatting
ImageInput 3DSemantic Gaussians 3DOccupancy Predictions
driveablesurface car motorcycle terrain vegetation sidewalk other flat pedestrian bicycle manmade
Fig.1: Considering the universal approximating ability of Gaussian mixture [8,11],
weproposeanobject-centric3DsemanticGaussianrepresentationtodescribethefine-
grainedstructureof3Dsceneswithouttheuseofdensegrids.WeproposeaGaussian-
Former model consisting of sparse convolution and cross-attention to efficiently trans-
form2Dimagesinto3DGaussianrepresentations.Togeneratedense3Doccupancy,we
design a Gaussian-to-voxel splatting module that can be efficiently implemented with
CUDA.Withcomparableperformance,ourGaussianFormerreducesmemoryconsump-
tion of existing 3D occupancy prediction methods by 75.2% - 82.2%.
Abstract. 3D semantic occupancy prediction aims to obtain 3D fine-
grained geometry and semantics of the surrounding scene and is an im-
portant task for the robustness of vision-centric autonomous driving.
Mostexistingmethodsemploydensegridssuchasvoxelsasscenerepre-
sentations,whichignorethesparsityofoccupancyandthediversityofob-
jectscalesandthusleadtounbalancedallocationofresources.Toaddress
this, we propose an object-centric representation to describe 3D scenes
with sparse 3D semantic Gaussians where each Gaussian represents a
flexible region of interest and its semantic features. We aggregate infor-
mation from images through the attention mechanism and iteratively
refine the properties of 3D Gaussians including position, covariance,
andsemantics.WethenproposeanefficientGaussian-to-voxelsplatting
method to generate 3D occupancy predictions, which only aggregates
the neighboring Gaussians for a certain position. We conduct extensive
∗ Project Leader
† Corresponding Author
4202
yaM
72
]VC.sc[
1v92471.5042:viXra2 Y. Huang, W. Zheng et al.
experiments on the widely adopted nuScenes and KITTI-360 datasets.
ExperimentalresultsdemonstratethatGaussianFormerachievescompa-
rableperformancewithstate-of-the-artmethodswithonly17.8%-24.8%
of their memory consumption. Code is available at: https://github.
com/huang-yh/GaussianFormer.
Keywords: 3D occupancy prediction · 3D Gaussian splitting · Au-
tonomous Driving
1 Introduction
Whether to use LiDAR for 3D perception has long been the core debate among
autonomous driving companies. While vision-centric systems share an econom-
ical advantage, their inability to capture obstacles of arbitrary shapes hinders
driving safety and robustness [13,17,25,26]. The emergence of 3D semantic oc-
cupancypredictionmethods[4,16,18,35,50,57]remediesthisissuebypredicting
theoccupancystatusofeachvoxelinthesurrounding3Dspace,whichfacilitates
a variety of newly rising tasks such as end-to-end autonomous driving [46], 4D
occupancy forecasting [58], and self-supervised 3D scene understanding [15].
Despite the promising applications, the dense output space of 3D occupancy
prediction poses a great challenge in how to efficiently and effectively represent
the3Dscene.Voxel-basedmethods[23,50]assigneachvoxelwithafeaturevector
to obtain dense representations to describe the fine-grained structure of a 3D
scene.Theyemploycoarse-to-fineupsampling[46,50]orvoxelfiltering[23,33]to
improve efficiency considering the sparse nature of the 3D space. As most of the
voxel space is unoccupied [3], BEV-based methods [27,56] compress the height
dimension and employ the bird’s eye view (BEV) as scene representations, yet
they usually require post-processing such as multi-scale fusion [56] to capture
finer details. TPVFormer [16] generalizes BEV with two additional planes and
achievesabetterperformance-complexitytrade-offwiththetri-perspectiveview
(TPV). However, they are all grid-based methods and inevitably suffer from
the redundancy of empty grids, resulting in more complexity for downstream
tasks [46]. It is also more difficult to capture scene dynamics with grid-based
representationssinceitisobjectsinsteadofgridsthatmoveinthe3Dspace[58].
In this paper, we propose the first object-centric representation for 3D se-
mantic occupancy prediction. We employ a set of 3D semantic Gaussians to
sparselydescribea3Dscene.EachGaussianrepresentsaflexibleregionofinter-
estandconsistsofthemean,covariance,anditssemanticcategory.Weproposea
GaussianFormer model to effectively obtain 3D semantic Gaussians from image
inputs. We randomly initialize a set of queries to instantiate the 3D Gaussians
and adopt the cross-attention mechanism to aggregate information from multi-
scale image features. We iteratively refine the properties of the 3D Gaussians
for smoother optimizations. To efficiently incorporate interactions among 3D
Gaussians, we treat them as point clouds located at the Gaussian means and
leverage3Dsparseconvolutionstoprocessthem.Wethendecodetheproperties
of 3D semantic Gaussians from the updated queries as the scene representation.GaussianFormer 3
Motivated by the 3D Gaussian splatting method in image rendering [20], we de-
signanefficientGaussian-to-voxelsplattingmodulethataggregatesneighboring
Gaussians to generate the semantic occupancy for a certain 3D position. The
proposed 3D Gaussian representation uses a sparse and adaptive set of features
to describe a 3D scene but can still model the fine-grained structure due to
the universal approximating ability of Gaussian mixtures [8,11]. Based on the
3D Gaussian representation, GaussianFormer further employs sparse convolu-
tion and local-aggregation-based Gaussian-to-voxel splatting to achieve efficient
3Dsemanticoccupancyprediction,asshowninFig.1.Weconductextensiveex-
periments on the nuScenes and KITTI-360 datasets for 3D semantic occupancy
prediction from surrounding and monocular cameras, respectively. Gaussian-
Formerachievescomparableperformancewithexistingstate-of-the-artmethods
withonly17.8%-24.8%oftheirmemoryconsumption.Ourqualitativevisualiza-
tions show that GaussianFormer is able to generate a both holistic and realistic
perception of the scene.
2 Related Work
2.1 3D Semantic Occupancy Prediction
3D semantic occupancy prediction has garnered increasing attention in recent
years due to its comprehensive description of the driving scenes, which involves
predictingtheoccupancyandsemanticstatesofallvoxelswithinacertainrange.
Learning an effective representation constitutes the fundamental step for this
challenging task. A straightforward approach discretizes the 3D space into reg-
ular voxels, with each voxel being assigned a feature vector [59]. The capacity
torepresentintricate3Dstructuresrendersvoxel-basedrepresentationsadvanta-
geousfor3Dsemanticoccupancyprediction[4,6,18,21,35,43,50,51,57].However,
duetothesparsityofdrivingscenesandthehighresolutionofvanillavoxelrep-
resentation, these approaches suffer from considerable computation and storage
overhead.Toimproveefficiency,severalmethodsproposetoreducethenumberof
voxelqueriesbythecoarse-to-fineupsamplingstrategy[49],orthedepth-guided
voxel filtering [23]. Nonetheless, the upsampling process might not adequately
compensatefortheinformationlossinthecoarsestage.Andthefilteringstrategy
ignores the occluded area and depends on the quality of depth estimation. Al-
though OctreeOcc [33] introduces voxel queries of multi-granularity to enhance
the efficiency, it still conforms to a predefined regular partition pattern. Our
3D Gaussian representation resembles the voxel counterpart, but can flexibly
adapttovaryingobjectscalesandregioncomplexitiesinadeformableway,thus
achieving better resource allocation and efficiency.
Another line of work utilizes the bird’s-eye-view (BEV) representation for
3D perception in autonomous driving, which can be categorized into two types
according to the view transformation paradigm. Approaches based on lift-splat-
shoot (LSS) actively project image features into 3D space with depth guid-
ance [14,24,25,31,39,40], while query-based methods typically use BEV queries
and deformable attention to aggregate information from image features [19,26,4 Y. Huang, W. Zheng et al.
53]. Although BEV-based perception has achieved great success in 3D object
detection, it is less employed for 3D occupancy prediction due to information
lossfromheightcompression.FB-OCC[27]usesdenseBEVfeaturesfromback-
ward projection to optimize the sparse voxel features from forward projection.
FlashOcc [56] applies a complex multi-scale feature fusion module on BEV fea-
tures for finer details. However, existing 3D occupancy prediction methods are
based on grid representations, which inevitably suffer from the computation
redundancy of empty grids. Differently, our GaussianFormer is based on object-
centric representation and can efficiently tend to flexible regions of interest.
2.2 3D Gaussian Splatting
The recent 3D Gaussian splatting (3D-GS) [20] uses multiple 3D Gaussians
for radiance field rendering, demonstrating superior performance in rendering
quality and speed. In contrast to prior explicit scene representations, such as
meshes [41,42,52] and voxels [10,37], 3D-GS is capable of modeling intricate
shapes with fewer parameters. Compared with implicit neural radiance field [1,
36], 3D-GS facilitates fast rendering through splat-based rasterization, which
projects 3D Gaussians to the target 2D view and renders image patches with
local 2D Gaussians. Recent advances in 3D-GS include adaptation to dynamic
scenarios [34,54], online 3D-GS generalizable to novel scenes [5,61], and gener-
ative 3D-GS [7,28,45,55].
Althoughour3DGaussianrepresentationalsoadoptsthephysicalformof3D
Gaussians (i.e. mean and covariance) and the multi-variant Gaussian distribu-
tion,itdiffersfrom3D-GSinsignificantways,whichimposesuniquechallengesin
3Dsemanticoccupancyprediction:1)Our3DGaussianrepresentationislearned
in an online manner as opposed to offline optimization in 3D-GS. 2) We gener-
ate 3D semantic occupancy predictions from the 3D Gaussian representation in
contrast to rendering 2D RGB images in 3D-GS.
3 Proposed Approach
Inthissection,wepresentourmethodof3DGaussianSplattingfor3Dsemantic
occupancy prediction. We first introduce an object-centric 3D scene representa-
tion which adaptively describes the regions of interest with 3D semantic Gaus-
sians (Sec. 3.1). We then explain how to effectively transform information from
the image inputs to 3D Gaussians and elaborate on the model designs including
self-encoding,imagecross-attention,andpropertyrefinement(Sec.3.2).Atlast,
wedetailtheGaussian-to-voxelsplattingmodulewhichgeneratesdense3Doccu-
pancypredictionsbasedonlocalaggregationandcanbeefficientlyimplemented
with CUDA (Sec. 3.2).
3.1 Object-centric 3D Scene Representation
Vision-based3Dsemanticoccupancypredictionaimstopredictdenseoccupancy
statesandsemanticsforeachvoxelgridwithmulti-viewcameraimagesasinput.GaussianFormer 5
Z
O
Y
X
Voxel BEV TPV 3D Gaussian (ours)
Fig.2: Comparisions of the proposed 3D Gaussian representation with ex-
iting grid-based scene representations (figures from TPVFormer [16]). The
voxel representation [23,50] assigns each voxel in the 3D space with a feature and is
redundantduetothesparsitynatureofthe3Dspace.BEV[26]andTPV[16]employ
2Dplanestodescribe3Dspacebutcanonlyalleviatetheredundancyissue.Differently,
the proposed object-centric 3D Gaussian representation can adapt to flexible regions
of interest yet can still describe the fine-grained structure of the 3D scene due to the
strong approximating ability of mixing Gaussians [8,11].
Formally,givenasetofmulti-viewimagesI ={I ∈R3×H×W|i=1,...,N},and
i
corresponding intrinsics K = {K ∈ R3×3|i = 1,...,N} and extrinsics T =
i
{T ∈ R4×4|i = 1,...,N}, the objective is to predict 3D semantic occupancy
i
O∈CX×Y×Z, where N, {H,W}, C and {X,Y,Z} denote the number of views,
theimageresolution,thesetofsemanticclassesandthetargetvolumeresolution.
The autonomous driving scenes contain foreground objects of various scales
(suchasbusesandpedestrians),andbackgroundregionsofdifferentcomplexities
(suchasroadandvegetation).Densevoxelrepresentation[4,23,48]neglectsthis
diversity and processes every 3D location with equal storage and computation
resources, which often leads to intractable overhead because of unreasonable
resource allocation. Planar representations, such as BEV [14,26] and TPV [16],
achieve3Dperceptionbyfirstencoding3Dinformationinto2Dfeaturemapsfor
efficiency and then recovering 3D structures from 2D features. Although planar
representationsareresource-friendly,theycouldcausealossofdetails.Thegrid-
based methods can hardly adapt to regions of interest for different scenes and
thus lead to representation and computation redundancy.
Toaddressthis,weproposeanobject-centric3Drepresentationfor3Dseman-
ticoccupancypredictionwhereeachunitdescribesaregionofinterestinsteadof
fixed grids, as shown in Fig. 2. We represent an autonomous driving scene with
a number of 3D semantic Gaussians, and each of them instantiates a semantic
Gaussian distribution characterized by mean, covariance, and semantic logits.
The occupancy prediction for a 3D location can be computed by summing up
the values of semantic Gaussian distributions evaluated at that location. Specif-
ically,weuseasetofP 3DGaussiansG ={G ∈Rd|i=1,...,P}foreachscene,
i
and each 3D Gaussian is represented by a d-dimensional vector in the form of
(m∈R3,s∈R3,r∈R4,c∈R|C|), where d=10+|C|, and m, s, r, c denote the
mean, scale, rotation vectors and semantic logits, respectively. Therefore, the6 Y. Huang, W. Zheng et al.
value of a semantic Gaussian distribution g evaluated at point p=(x,y,z) is
g(p;m,s,r,c)=exp(cid:0)
−
1 (p−m)TΣ−1(p−m)(cid:1)
c, (1)
2
Σ=RSSTRT, S=diag(s), R=q2r(r), (2)
where Σ, diag(·) and q2r(·) represent the covariance matrix, the function that
constructs a diagonal matrix from a vector, and the function that transforms a
quaternion into a rotation matrix, respectively. Then the occupancy prediction
result at point p can be formulated as the summation of the contribution of
individual Gaussians on the location p:
P P
oˆ(p;G)=(cid:88) g (p;m ,s ,r ,c )=(cid:88) exp(cid:0) − 1 (p−m )TΣ−1(p−m )(cid:1) c . (3)
i i i i i 2 i i i i
i=1 i=1
Compared with voxel representation, the mean and covariance properties
allow the 3D Gaussian representation to adaptively allocate computation and
storage resources according to object scales and region complexities. Therefore,
we need fewer 3D Gaussians to model a scene for better efficiency while still
maintain expressiveness. Meanwhile, the 3D Gaussian representation take 3D
Gaussians as its basic unit, and thus avoids potential loss of details from di-
mension reduction in planar representations. Moreover, every 3D Gaussian has
explicit semantic meaning, making the transformation from the scene represen-
tation to occupancy predictions much easier than those in other representations
whichofteninvolvedecodingper-voxelsemanticsfromhighdimensionalfeatures.
3.2 GaussianFormer: Image to Gaussians
Based on the 3D semantic Gaussian representation of the scene, we further pro-
poseaGaussianFormermodeltolearnmeaningful3DGaussiansfrommulti-view
images. The overall pipeline is shown in Fig. 3. We first initialize the properties
of 3D Gaussians and their corresponding high-dimensional queries as learnable
vectors.ThenweiterativelyrefinetheGaussianpropertieswithintheBblocksof
GaussianFormer.Eachblockconsistsofaself-encodingmoduletoenableinterac-
tionsamong3DGaussians,animagecross-attentionmoduletoaggregatevisual
information, and a refinement module to rectify the properties of 3D Gaussians.
Gaussian Properties and Queries. We introduce two groups of features
in GaussianFormer. The Gaussian properties G = {G ∈ Rd|i = 1,...,P} are
i
the physical attributes as discussed in Section 3.1, and they are de facto the
learning target of the model. On the other hand, the Gaussian queries Q =
{Q ∈ Rm|i = 1,...,P} are the high-dimensional feature vectors that implicitly
i
encode 3D information in the self-encoding and image cross-attention modules,
andprovideguidanceforrectificationintherefinementmodule.Weinitializethe
Gaussian properties as learnable vectors denoted by Initial Properties in Fig. 3.
Self-encoding Module.Methodswithvoxelorplanarrepresentationsusu-
ally implement self-encoding modules with deformable attention for efficiencyGaussianFormer 7
Initial Queries Self-encoding Image Cross-attention Refinement
Voxelization Ref. Points Query Decoding
Sparse Conv. Deformable Attn. Residual Refine
Initial Properties
×B Blocks
Gaussian-
Image
to-Voxel
Encoder
Splatting
ImageInput M.S. Features 3DGaussians 3DOccupancy
Fig.3: Framework of our GaussianFormer for 3D semantic occupancy pre-
diction.Wefirstextractmulti-scale(M.S.)featuresfromimageinputsusinganimage
backbone. We then randomly initialized a set of queries and properties (mean, covari-
ance, and semantics) to represent 3D Gaussians and update them with interleaved
self-encoding, image cross-attention, and property refinement. Having obtained the
updated 3D Gaussians, we employ an efficient Gaussian-to-voxel splatting module to
generate dense 3D occupancy via local aggregation of Gaussians.
considerations, which is not well supported for unstructured 3D Gaussian rep-
resentation. Instead, we leverage 3D sparse convolution to allow interactions
among 3D Gaussians, sharing the same linear computational complexity as de-
formable attention. Specifically, we treat each Gaussian as a point located at its
meanm,voxelizethegeneratedpointcloud(denotedbyVoxelizationinFig.3),
and apply sparse convolution on the voxel grid. Since the number of 3D Gaus-
sians P is much fewer than X×Y ×Z, sparse convolution could effectively take
advantage of the sparsity of Gaussians.
ImageCross-attentionModule.Theimagecross-attentionmodule(ICA)
is designed to extract visual information from images for our vision-based ap-
proach.Toelaborate,fora3DGaussianG,wefirstgenerateasetof3Dreference
points R={m+∆m |i=1,...,R} by permuting the mean m with offsets ∆m.
i
We calculate the offsets according to the covariance of the Gaussian to reflect
theshapeofitsdistribution.Thenweprojectthe3Dreferencepointsontoimage
featuremapswithextrinsicsT andintrinsicsK.Finally,weupdatetheGaussian
query Q with the weighted sum of retrieved image features:
N R
1 (cid:88)(cid:88)
ICA(R,Q,F;T,K)= DA(Q,π(R;T,K),F ), (4)
N n
n=1i=1
where F, DA(·), π(·) denote the image feature maps, the deformable attention
function and the transformation from world to pixel coordinates, respectively.8 Y. Huang, W. Zheng et al.
Refinement Module. We use the refinement module to rectify the Gaus-
sian properties with guidance from corresponding Gaussian queries which have
aggregated sufficient 3D information in the prior self-encoding and image cross-
attention modules. Specifically, we take inspiration from DETR [60] in object
detection. For a 3D Gaussian G = (m,s,r,c), we first decode the intermediate
properties Gˆ = (mˆ,ˆs,ˆr,ˆc) from the Gaussian query Q with a multi-layer per-
ceptron (MLP). When refining the old properties with the intermediate ones,
we treat the intermediate mean mˆ as a residual and add it with the old mean
m,whilewedirectlysubstitutetheotherintermediateproperties(ˆs,ˆr,ˆc)forthe
corresponding old properties:
Gˆ =(mˆ,ˆs,ˆr,ˆc)=MLP(Q), G =(m+mˆ,ˆs,ˆr,ˆc). (5)
new
We refine the mean of Gaussian with residual connections in order to keep their
coherence throughout the B blocks of GaussianFormer. The direct replacement
of the other properties is due to the concern about vanishing gradient from the
sigmoid and softmax activations we apply on covariance and semantic logits.
3.3 Gaussian-to-Voxel Splatting
Due to the universal approximating ability of Gaussian mixtures [8,11], the 3D
semantic Gaussians can efficiently represent the 3D scene and thus can be di-
rectly processed to perform downstream tasks like motion planning and control.
Specifically, to achieve 3D semantic occupancy prediction, we design an efficient
Gaussian-to-voxel splatting module to efficiently transform 3D Gaussian rep-
resentation to 3D semantic occupancy predictions using only local aggregation
operation.
Although(3)demonstratesthemainideaofthetransformationasasumma-
tion over the contributions of 3D Gaussians, it is infeasible to query all Gaus-
sians for every voxel position due to the intractable computation and storage
complexity (O(XYZ×P)). Since the weight
exp(cid:0) −1(p−m)TΣ−1(p−m)(cid:1)
in
2
(1)decaysexponentiallywithrespecttothesquareofthemahalanobisdistance,
it should be negligible when the distance is large enough. Based on the locality
of the semantic Gaussian distribution, we only consider 3D Gaussians within a
neighborhood of each voxel position to improve efficiency.
As illustrated by Fig. 4, we first embed the 3D Gaussians into the target
voxelgridofsizeX×Y ×Z accordingtotheirmeansm.Foreach3DGaussian,
we then calculate the radius of its neighborhood according to its scale property
s. We append both the index of the Gaussian and the index of each voxel inside
the neighborhood as a tuple (g,v) to a list. Then we sort the list according to
theindicesofvoxelstoderivetheindicesof3DGaussiansthateachvoxelshould
attend to:
sort (cid:0) [(g,v ),...,(g,v )]P (cid:1) =[(g ,v),...,(g ,v)]XYZ, (6)
vox g1 gk g=1 v1 vl v=1
where k, l denote the number of neighboring voxels of a certain Gaussian, and
thenumberofGaussiansthatcontributetoacertainvoxel,respectively.Finally,GaussianFormer 9
Fig.4: Illustration of the Gaussian-to-voxel splatting method in 2D. We
first voxelize the 3D Gaussians and record the affected voxels of each 3D Gaussian
by appending their paired indices to a list. Then we sort the list according to the
voxel indices to identify the neighboring Gaussians of each voxel, followed by a local
aggregation to generate the occupancy prediction.
we can approximate (3) efficiently with only neighboring Gaussians:
(cid:88)
oˆ(p;G)= g (p;m ,s ,r ,c ), (7)
i i i i i
i∈N(p)
where N(p) represents the set of neighboring Gaussians of the point at p. Con-
sidering the dynamic neighborhood sizes of 3D Gaussians, the implementation
of Gaussian-to-voxel splatting is nontrivial. To fully exploit the parallel compu-
tation ability of GPU, we realize it with the CUDA programming language to
achieve better acceleration.
TheoverallGaussianFormermodelcanbetrainedefficientlyinanend-to-end
manner.Fortraining,weusethecrossentropylossL andthelovasz-softmax[2]
ce
loss L following TPVFormer [16]. To refine the Gaussian properties in an
lov
iterativemanner,weapplysupervisionontheoutputofeachrefinementmodule.
The overall loss function is L=(cid:80)B Li +Li , where i denote the i-th block.
i=1 ce lov
4 Experiments
In this paper, we propose a 3D semantic Gaussian representation to effectively
and efficiently describe the 3D scene and devise a GaussianFormer model to
perform3Doccupancyprediction.WeconductedexperimentsonthenuScenes[3]
dataset and the KITTI-360 [29] dataset for 3D semantic occupancy prediction
with surrounding and monocular cameras, respectively.
4.1 Datasets
NuScenes [3] consists of 1000 sequences of various driving scenes collected in
Boston and Singapore, which are officially split into 700/150/150 sequences for10 Y. Huang, W. Zheng et al.
training, validation and testing, respectively. Each sequence lasts 20 seconds
with RGB images collected by 6 surrounding cameras, and the keyframes are
annotated at 2Hz. We leverage the dense semantic occupancy annotations from
SurroundOcc[50]forsupervisionandevaluation.Theannotatedvoxelgridspans
[-50m, 50m] along the X and Y axes, and [-5m, 3m] along the Z axis with a
resolution of 200×200×16. Each voxel is labeled with one of the 18 classes (16
semantic, 1 empty and 1 unknown classes).
KITTI-360[29]isalarge-scaledatasetcoveringadrivingdistanceof73.7km
corresponding to over 320k images and 100k laser scans. We use the dense se-
mantic occupancy annotations from SSCBench-KITTI-360 [22] for supervision
and evaluation. It provides ground truth labels for 9 long sequences with a
total of 12865 key frames, which are officially split into 7/1/1 sequences with
8487/1812/2566keyframesfortraining,validationandtesting,respectively.The
voxel grid spans 51.2×51.2×6.4m3 in front of the ego car with a resolution of
256×256×32,andeachvoxelislabeledwithoneof19classes(18semanticand
1 empty). We use the RGB images from the left camera as input to our model.
4.2 Evaluation Metrics
Followingcommonpractice[4],weusemeanIntersection-over-Union(mIoU)and
Intersection-over-Union (IoU) to evaluate the performance of our model:
mIoU= 1 (cid:88) TP i , IoU= TP ̸=c0 , (8)
|C′| TP +FP +FN TP +FP +FN
i∈C′
i i i ̸=c0 ̸=c0 ̸=c0
where C′, c , TP, FP, FN denote the nonempty classes, the empty class, the
0
numberoftruepositive,falsepositiveandfalsenegativepredictions,respectively.
4.3 Implementation Details
We set the resolutions of input images as 900 × 1600 for nuScenes [3] and
376×1408 for KITTI-360 [29]. We employ ResNet101-DCN [12] initialized from
FCOS3D[47]checkpointastheimagebackbonefornuScenesandResNet50[12]
pretrained with ImageNet [9] for KITTI-360. We use the feature pyramid net-
work [30] (FPN) to generate multi-scale image features with downsample rates
of 4, 8, 16 and 32. We set the number of Gaussians to 144000 and 38400 for
nuScenes and KITTI-360, respectively, and use 4 transformer blocks in Gaus-
sianFormertorefinethepropertiesofGaussians.Foroptimization,weutilizethe
AdamW [32] optimizer with a weight decay of 0.01. The learning rate warms up
in the first 500 iterations to a maximum value of 2e-4 and decreases according
to a cosine schedule. We train our models for 20 epochs with a batch size of 8,
and employ random flip and photometric distortion augmentations.
4.4 Results and Analysis
Surrounding-Camera 3D semantic occupancy prediction. In Table 1,
we present a comprehensive quantitative comparison of various methods forGaussianFormer 11
Table 1: 3D semantic occupancy prediction results on nuScenes validation
set. While the original TPVFormer [16] is trained with LiDAR segmentation labels,
TPVFormer*issupervisedbydenseoccupancyannotations.Ourmethodachievescom-
parable performance with state-of-the-art methods.
SC SSC
Method
IoU mIoU
MonoScene[4] 23.96 7.31 4.03 0.35 8.00 8.04 2.90 0.28 1.16 0.67 4.01 4.35 27.72 5.20 15.13 11.29 9.03 14.86
Atlas[38] 28.66 15.00 10.64 5.68 19.66 24.94 8.90 8.84 6.47 3.28 10.42 16.21 34.86 15.46 21.89 20.95 11.21 20.54
BEVFormer[26] 30.50 16.75 14.22 6.58 23.46 28.28 8.66 10.77 6.64 4.05 11.20 17.78 37.28 18.00 22.88 22.1713.8022.21
TPVFormer[16] 11.51 11.66 16.14 7.17 22.63 17.13 8.83 11.39 10.46 8.23 9.43 17.02 8.07 13.64 13.85 10.34 4.90 7.37
TPVFormer*[16]30.8617.10 15.96 5.31 23.86 27.32 9.79 8.74 7.09 5.20 10.97 19.2238.8721.2524.2623.1511.73 20.81
OccFormer[57] 31.3919.0318.6510.4123.9230.2910.3114.1913.5910.1312.4920.7738.7819.79 24.19 22.2113.4821.35
SurroundOcc[50]31.4920.3020.5911.6828.0630.8610.7015.1414.0912.0614.3822.2637.2923.7024.4922.7714.8921.86
Ours 29.8319.1019.5211.2626.1129.7810.4713.8312.588.6712.7421.5739.6323.2824.4622.99 9.59 19.12
Table 2: 3D semantic occupancy prediction results on SSCBench-KITTI-
360 validation set. Our method achieves performance on par with state-of-the-art
methods,excellingatsomesmallerandgeneralcategories(i.e.motorcycle,other-veh.).
SC SSC
Method
IoU mIoU
LMSCNet[43] L 47.53 13.65 20.91 0 0 0.26 0 0 62.95 13.51 33.51 0.2 43.670.3340.01 26.80 0 0 3.63 0
SSCNet[44] L 53.58 16.95 31.95 0 0.1710.29 0.58 0.07 65.7 17.33 41.243.2244.416.7743.72 28.87 0.78 0.75 8.60 0.67
MonoScene[4] C37.87 12.31 19.340.430.58 8.02 2.03 0.8648.35 11.38 28.133.2232.893.5326.1516.75 6.92 5.67 4.20 3.09
Voxformer[23] C38.76 11.91 17.841.160.89 4.56 2.06 1.6347.01 9.67 27.212.8931.184.9728.99 14.69 6.51 6.92 3.79 2.43
TPVFormer[16]C40.2213.6421.561.091.37 8.06 2.57 2.3852.9911.9931.073.7834.834.8030.0817.517.46 5.86 5.48 2.70
OccFormer[57] C40.2713.8122.580.660.26 9.89 3.822.7754.3013.4431.533.5536.424.8031.0019.517.778.516.95 4.60
Symphonies[18]C44.1218.5830.021.855.9025.0712.068.2054.9413.8332.766.9335.118.5838.3311.5214.019.5714.4411.28
Ours C35.38 12.92 18.931.024.6218.077.593.3545.47 10.89 25.035.3228.445.6829.54 8.62 2.99 2.32 9.51 5.14
multi-view 3D semantic occupancy prediction on nuScenes validation set, with
dense annotations from SurroundOcc [50]. Our GaussianFormer achieves no-
tableimprovementsovermethodsbasedonplanarrepresentations,suchasBEV-
Former [26] and TPVFormer [16]. Even compared with dense grid representa-
tions,GaussianFormerperformsonparwithOccFormer[57]andSurroundOcc[50].
These observations prove the valuable application of the 3D Gaussians for se-
mantic occupancy prediction. This is because the 3D Gaussian representation
betterexploitsthesparsenatureofthedrivingscenesandthediversityofobject
scales with flexible properties of position and covariance.
Monocular 3D semantic occupancy prediction. Table 2 compares the
performance of GaussianFormer with other methods for monocular 3D seman-
tic occupancy prediciton on SSCBench-KITTI-360. Notably, GaussianFormer
achievescomparableperformancewithstate-of-the-artmodels,excellingatsome
smaller categories such as motorcycle and general categories such as other-
vehicle. This is due to 3D Gaussians can adaptively change their positions and
covariance to match the boundaries of small objects in images in contrast to
rigid grid projections on images which might be misleading. Furthermore, the
flexibility of 3D Gaussians also benefits the predictions for general objects (i.e.
tupnI
reirrab■
rac■
elcycib■
elcycib■
elcycrotom■
sub■
kcurt■
rac■
.hev-rehto■
.hev.tsnoc■
nosrep■
elcycrotom■
daor■
nairtsedep■
gnikrap■
enoccffiart■
klawedis■ dnrg-rehto■
reliart■
gnidliub■
kcurt■
ecnef■
.fus.evird■
noitategev■
taflrehto■
niarret■
klawedis■
elop■
niarret■
ngis-.fart■
edamnam■
.tcurts-rehto■
noitategev■
tcejbo-rehto■12 Y. Huang, W. Zheng et al.
Table 3: Efficiency comparison of different representations on nuScenes. The
latencyandmemoryconsumptionforGaussianFormeraretestedononeNVIDIA4090
GPU with batch size one, while the results for other methods are reported in Oc-
treeOcc[33]testedononeNVIDIAA100GPU.Ourmethoddemonstratessignificantly
reduced memory usage compared to other representations.
Methods QueryForm QueryResolution Latency↓ Memory↓
BEVFormer[26] 2DBEV 200×200 302ms 25100M
TPVFormer[16] 2Dtri-plane 200×(200+16+16) 341ms 29000M
PanoOcc[49] 3Dvoxel 100×100×16 502ms 35000M
FBOCC[27] 3Dvoxel&2DBEV 200×200×16&200×200 463ms 31000M
OctreeOcc[33] OctreeQuery 91200 386ms 26500M
GaussianFormer 3DGaussian 144000 372ms 6229M
Table 4: Ablation on the components of GaussianFormer. Deep Supervision
representssupervisingtheoutputofeachrefinementmodule.ResidualRefinemeanson
which properties of Gaussian to apply residual refinement as opposed to substitution.
DeepSupervision SparseConv. ResidualRefine mIoU IoU
✓ mean 16.36 29.32
✓ mean 15.93 28.99
✓ ✓ none - -
✓ ✓ allexceptsemantics 16.24 29.30
✓ ✓ mean 16.41 29.37
categories with other- prefix) which often have distinct shapes and appearances
with normal categories.
Efficiency comparisons with existing methods. We provide the effi-
ciency comparisons of different scene representations in Table 3. Notably, Gaus-
sianFormer surpasses all existing competitors with significantly reduced mem-
ory consumption. The memory efficiency of GaussianFormer originates from its
object-centric nature which assigns explicit semantic meaning to each 3D Gaus-
sian, and thus greatly simplifies the transformation from the scene representa-
tiontooccupancypredictions,gettingridoftheexpensivedecodingprocessfrom
high dimensional features. While being slightly slower (∼ 70 ms) than methods
based on planar representations [16,26], GaussianFormer achieves the lowest
latency among dense grid representations. Notably, our method is faster than
OctreeOcc [33] even with more queries.
Analysis of components of GaussianFormer. In Table 4, we provide
comprehensive analysis on the components of GaussianFormer to validate their
effectiveness. We conduct these experiments on nuScenes and set the number of
3D Gaussians to 51200. The strategy for refinement of Gaussian properties has
a notable influence on the performance. The experiment for the all substitution
strategy(denotedbynone)collapseswherewedirectlyreplacetheoldproperties
withnewones,andthuswedonotreporttheresult.Thisisbecausethepositions
of Gaussians are sensitive to noise which quickly converge to a trivial solution
withoutregularizationforcoherenceduringrefinement.Rectifyingallproperties
except semantics in a residual style (denoted by all except semantics) is also
harmfulbecausethesigmoidactivationusedforcovarianceispronetovanishingGaussianFormer 13
Table 5: Ablation on the number of Gaussians. The latency and memory are
testedonanNVIDIA4090GPUwithbatchsizeoneduringinference.Theperformance
improves consistently with more Gaussians while taking up more time and memory.
Number of Gaussians Latency Memory mIoU IoU
25600 227 ms 4850 M 16.00 28.72
38400 249 ms 4856 M 16.04 28.72
51200 259 ms 4866 M 16.41 29.37
91200 293 ms 5380 M 18.31 27.48
144000 372 ms 6229 M 19.10 29.83
gradient. Moreover, the 3D sparse convolution in the self-encoding module is
crucial for the performance because it is responsible for the interactions among
3DGaussians.Ontheotherhand,thedeepsupervisionstrategyalsocontributes
to the overall performance by ensuring that every intermediate refinement step
benefits the 3D perception.
Effect of the number of Gaussians. Table 5 presents the ablation study
on the number of Gaussians, where we analyze its influence on efficiency and
performance. The mIoU increases linearly when the number of 3D Gaussians is
greater than 38400, which is due to the enhanced ability to represent finer de-
tails with more Gaussians. The latency and memory consumption also correlate
linearly with the number of Gaussians, offering flexibility for deployment.
Visualizationresults.WeprovidequalitativevisualizationresultsinFig.5.
OurGaussianFormercangenerateaholisticandrealisticperceptionofthescene.
Specifically, the 3D Gaussians adjust their covariance matrices to capture the
fine details of object shapes such as the flat-shaped Gaussians at the surface of
the road and walls (e.g. in the third row). Additionally, the density is higher in
regionswithvehiclesandpedestrianscomparedwiththeroadsurface(e.g.inthe
firstrow),whichprovesthatthe3DGaussiansclusteraroundforegroundobjects
with iterative refinement for a reasonable allocation of resources. Furthermore,
ourGaussianFormerevensuccessfullypredictsobjectsthatarenotintheground
truth and barely visible in the images, such as the truck in the front left input
image and the upper right corner of the 3D visualizations in the fourth row.
5 Conclusion and Discussions
In this paper, we have proposed an efficient object-centric 3D Gaussian repre-
sentation for 3D semantic occupancy prediction to better exploit the sparsity
of occupancy and the diversity of object scales. We describe the driving scenes
with sparse 3D Gaussians each of which is characterized by its position, co-
variance and semantics and represents a flexible region of interest. Based on
the 3D Gaussian representation, we have designed the GaussianFormer to ef-
fectively learn 3D Gaussians from input images through attention mechanism
anditerativerefinement.Toefficientlygeneratevoxelizedoccupancypredictions
from 3D Gaussians, we have proposed an efficient Gaussian-to-voxel splatting
method, which only aggregates the neighboring Gaussians for each voxel. Gaus-
sianFormerhasachievedcomparableperformancewithstate-of-the-artmethods14 Y. Huang, W. Zheng et al.
FRONT_LEFT FRONT FRONT_RIGHT 3D Gaussians Occupancy Prediction Occupancy Ground Truth
BACK_LEFT BACK BACK_RIGHT
driveablesurface car bus truck terrain vegetation sidewalk other flat pedestrian bicycle manmade
motorcycle barrier construction vehicle trailer traffic cone
Fig.5: Visualization results for 3D semantic occupancy prediction on
nuScenes. We visualize the 3D Gaussians by treating them as ellipsoids centered
at the Gaussian means with semi-axes determined by the Gaussian covariance matri-
ces. Our GussianFormer not only achieves reasonable allocation of resources, but also
captures the fine details of object shapes.
onthenuScenesandKITTI-360datasets,andsignificantlyreducedthememory
consumption by more than 75%. Our ablation study has shown that the perfor-
mance of GaussianFormer scales well to the number of Gaussians. Additionally,
the visualization has proved the abilities of 3D Gaussians to capture the details
of object shapes and to reasonably allocate computation and storage resources.
Limitations.TheperformanceofGaussianFormerisstillinferiortostate-of-
the-artmethodsdespitethemuchlowermemoryconsumption.Thismightresult
fromtheinaccuracyof3DsemanticGaussianrepresentationorsimplythewrong
choiceofhyperparametersincewedidnotperformmuchhyperparametertuning.
GaussianFormeralsorequiresalargenumberofGaussianstoachievesatisfactory
performance.Thisisperhapsbecausethecurrent3DsemanticGaussiansinclude
empty as one category and thus still can be redundant. It is interesting to only
model solid objects to further improve performance and speed.GaussianFormer 15
Fig.6: Visualizations of the proposed GaussianFormer method for 3D se-
mantic occupancy prediction on the nuScenes [3] validation set.Wevisualize
thesixinputsurroundingimagesandthecorrespondingpredictedsemanticoccupancy
intheupperpart.Thelowerrowshowsthepredicted3DGaussians(left),thepredicted
semantic occupancy in the global view (middle) and the bird’s eye view (right).
A Video Demonstration
Fig. 6 shows a sampled image from the video demo1 for 3D semantic occupancy
prediction on the nuScenes [3] validation set. GaussianFormer successfully pre-
dicts both realistic and holistic semantic occupancy results with only surround
imagesasinput.Thesimilaritybetweenthe3DGaussiansandthepredictedoc-
cupancy suggests the expressiveness of the efficient 3D Gaussian representation.
B Additional Visualizations
InFig.7,weprovidethevisualizationsofGaussianFormerfor3Dsemanticoccu-
pancy prediction on the KITTI-360 [29] validation set. Similar to the visualiza-
tions for nuScenes [3] in Fig. 5, we observe that the density of the 3D Gaussians
ishigherwiththepresenceofvehicles(e.g.the4throw)whichdemonstratesthe
object-centric nature of the 3D Gaussian representation and further benefits re-
sourceallocation.Inadditiontooverallstructure,GaussianFormeralsocaptures
intricatedetailssuchaspolesinthescenes(e.g.the1strow).Thediscrepancyof
the density and scale of the 3D Gaussians in Fig. 5 and 7 is because we set the
number of Gaussians to 144000 / 38400 and the max scale of Gaussians to 0.3m
/ 0.5m for nuScenes and KITTI-360, respectively. We also visualize the output
3D semantic Gaussians of each refinement layer in Fig. 8. In Fig. 9, we provide
a qualitative comparison between our GaussianFormer and SurroundOcc [50].
1 https://wzzheng.net/GaussianFormer16 Y. Huang, W. Zheng et al.
Input Image 3D Gaussians Pred. Occupancy Occupancy G.T.
road car bicycle motorcycle truck other-vehicle person parking sidewalk other-ground
building barrier vegetation terrain pole traffic-sign other-structure other-object
Fig.7: Visualizations of the proposed GaussianFormer method for 3D se-
mantic occupancy prediction on the KITTI-360 [29] validation set. Gaus-
sianFormer is able to capture both the overall structures and intricate details of the
driving scenes in a monocular setting.
C Additional Ablation Study
We conduct additional ablation study on the number of Gaussians, photometric
supervision, splitting & pruning strategy, and initialization schemes in Table 6.
We observe consistent improvement as the number of Gaussians increases.
We also experiment with an additional photometric loss to reconstruct the
inputimageinTable6,whichdoesnotdemonstratesasignificantimprovement.
This is because using photometric loss on nuScenes where the surrounding cam-
eras share little overlap view cannot provide further structural information. In
addition, we experiment with the splitting and pruning strategy prevalent in
theoffline3D-GSliteratureandobservethatitimprovesperformancecompared
with the baseline.
Weemploylearnablepropertiesasinitializationforouronlinemodeltolearn
a prior from different scenes in the main paper. We further experimented withGaussianFormer 17
Initialization, mIoU: 0.32 Refine#1, mIoU: 15.67 Refine#2, mIoU: 16.21 Refine#3, mIoU: 16.32 Refine#4, mIoU: 16.41
Fig.8: Visualization of the outputs of the refinement layers and the corre-
sponding mIoU on nuScenes.
Ours Gaussians Ours Occ. SurroundOcc G.T.
Fig.9: Qualitative comparison between our method and SurroundOcc on
nuScenes.
differentinitializationstrategiesincludinguniformdistribution,predictedpseudo
pointcloud,andground-truthpointcloud.Weseethatinitializationisimportant
to the performance and depth information is especially crucial.
References
1. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-
vasan,P.P.:Mip-nerf:Amultiscalerepresentationforanti-aliasingneuralradiance18 Y. Huang, W. Zheng et al.
Table 6: Analysis on different design choices.
GaussianNumber Photometric Split&Prune Initialization mIoU IoU
144000 ✕ ✕ learnable 19.10 29.83
192000 ✕ ✕ learnable 19.65 30.37
256000 ✕ ✕ learnable 19.76 30.51
51200 ✕ ✕ learnable 16.41 29.37
51200 ✓ ✕ learnable 16.24 29.43
51200 ✕ ✓ learnable 16.50 29.41
51200 ✕ ✕ uniform 16.27 29.23
51200 ✕ ✕ pseudopoints 18.99 28.84
51200 ✕ ✕ G.T.points 26.78 41.81
fields. In: ICCV. pp. 5855–5864 (2021) 4
2. Berman, M., Triki, A.R., Blaschko, M.B.: The lovász-softmax loss: A tractable
surrogate for the optimization of the intersection-over-union measure in neural
networks.In:ProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition. pp. 4413–4421 (2018) 9
3. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,
Pan,Y.,Baldan,G.,Beijbom,O.:nuscenes:Amultimodaldatasetforautonomous
driving. In: CVPR (2020) 2, 9, 10, 15
4. Cao,A.Q.,deCharette,R.:Monoscene:Monocular3dsemanticscenecompletion.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 3991–4001 (2022) 2, 3, 5, 10, 11
5. Charatan,D.,Li,S.,Tagliasacchi,A.,Sitzmann,V.:pixelsplat:3dgaussiansplats
from image pairs for scalable generalizable 3d reconstruction. arXiv preprint
arXiv:2312.12337 (2023) 4
6. Chen, X., Lin, K.Y., Qian, C., Zeng, G., Li, H.: 3d sketch-aware semantic scene
completion via semi-supervised structure prior. In: CVPR (2020) 3
7. Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint
arXiv:2309.16585 (2023) 4
8. Dalal,S.,Hall,W.:Approximatingpriorsbymixturesofnaturalconjugatepriors.
Journal of the Royal Statistical Society 45(2), 278–286 (1983) 1, 3, 5, 8
9. Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:Imagenet:Alarge-scale
hierarchical image database. In: CVPR. pp. 248–255. Ieee (2009) 10
10. Fridovich-Keil,S.,Yu,A.,Tancik,M.,Chen,Q.,Recht,B.,Kanazawa,A.:Plenox-
els: Radiance fields without neural networks. In: CVPR. pp. 5501–5510 (2022) 4
11. Goodfellow, I., Bengio, Y., Courville, A.: Deep learning (2016) 1, 3, 5, 8
12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
(2016) 10
13. Hu,Y.,Yang,J.,Chen,L.,Li,K.,Sima,C.,Zhu,X.,Chai,S.,Du,S.,Lin,T.,Wang,
W., et al.: Goal-oriented autonomous driving. arXiv preprint arXiv:2212.10156
(2022) 2
14. Huang, J., Huang, G., Zhu, Z., Du, D.: Bevdet: High-performance multi-camera
3d object detection in bird-eye-view. arXiv preprint arXiv:2112.11790 (2021) 3, 5
15. Huang,Y.,Zheng,W.,Zhang,B.,Zhou,J.,Lu,J.:Selfocc:Self-supervisedvision-
based 3d occupancy prediction. In: CVPR (2024) 2
16. Huang,Y.,Zheng,W.,Zhang,Y.,Zhou,J.,Lu,J.:Tri-perspectiveviewforvision-
based3dsemanticoccupancyprediction.In:CVPR.pp.9223–9232(2023) 2,5,9,
11, 12GaussianFormer 19
17. Jiang, B., Chen, S., Xu, Q., Liao, B., Chen, J., Zhou, H., Zhang, Q., Liu, W.,
Huang,C.,Wang,X.:Vad:Vectorizedscenerepresentationforefficientautonomous
driving. arXiv preprint arXiv:2303.12077 (2023) 2
18. Jiang, H., Cheng, T., Gao, N., Zhang, H., Liu, W., Wang, X.: Symphonize
3d semantic scene completion with contextual instance queries. arXiv preprint
arXiv:2306.15670 (2023) 2, 3, 11
19. Jiang,Y.,Zhang,L.,Miao,Z.,Zhu,X.,Gao,J.,Hu,W.,Jiang,Y.G.:Polarformer:
Multi-camera 3d object detection with polar transformer. In: AAAI. vol. 37, pp.
1042–1050 (2023) 3
20. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-timeradiancefieldrendering.ACMTransactionsonGraphics42(4)(2023) 3,
4
21. Li, J., Han, K., Wang, P., Liu, Y., Yuan, X.: Anisotropic convolutional networks
for 3d semantic scene completion. In: CVPR (2020) 3
22. Li, Y., Li, S., Liu, X., Gong, M., Li, K., Chen, N., Wang, Z., Li, Z., Jiang, T.,
Yu,F.,etal.:Sscbench:Alarge-scale3dsemanticscenecompletionbenchmarkfor
autonomous driving. arXiv preprint arXiv:2306.09001 (2023) 10
23. Li, Y., Yu, Z., Choy, C., Xiao, C., Alvarez, J.M., Fidler, S., Feng, C., Anandku-
mar,A.:Voxformer:Sparsevoxeltransformerforcamera-based3dsemanticscene
completion. In: CVPR. pp. 9087–9098 (2023) 2, 3, 5, 11
24. Li,Y.,Bao,H.,Ge,Z.,Yang,J.,Sun,J.,Li,Z.:Bevstereo:Enhancingdepthesti-
mationinmulti-view3dobjectdetectionwithtemporalstereo.In:AAAI.vol.37,
pp. 1486–1494 (2023) 3
25. Li, Y., Ge, Z., Yu, G., Yang, J., Wang, Z., Shi, Y., Sun, J., Li, Z.: Bevdepth:
Acquisition of reliable depth for multi-view 3d object detection. arXiv preprint
arXiv:2206.10092 (2022) 2, 3
26. Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Yu, Q., Dai, J.: Bevformer:
Learning bird’s-eye-view representation from multi-camera images via spatiotem-
poral transformers. arXiv preprint arXiv:2203.17270 (2022) 2, 3, 5, 11, 12
27. Li, Z., Yu, Z., Austin, D., Fang, M., Lan, S., Kautz, J., Alvarez, J.M.: Fb-occ:
3d occupancy prediction based on forward-backward view transformation. arXiv
preprint arXiv:2307.01492 (2023) 2, 4, 12
28. Liang, Y., Yang, X., Lin, J., Li, H., Xu, X., Chen, Y.: Luciddreamer: Towards
high-fidelity text-to-3d generation via interval score matching. arXiv preprint
arXiv:2311.11284 (2023) 4
29. Liao,Y.,Xie,J.,Geiger,A.:KITTI-360:Anoveldatasetandbenchmarksforurban
scene understanding in 2d and 3d. PAMI (2022) 9, 10, 15, 16
30. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection. In: CVPR (2017) 10
31. Liu, Z., Tang, H., Amini, A., Yang, X., Mao, H., Rus, D.L., Han, S.: Bevfu-
sion: Multi-task multi-sensor fusion with unified bird’s-eye view representation.
In: ICRA. pp. 2774–2781. IEEE (2023) 3
32. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017) 10
33. Lu, Y., Zhu, X., Wang, T., Ma, Y.: Octreeocc: Efficient and multi-granularity
occupancypredictionusingoctreequeries.arXivpreprintarXiv:2312.03774(2023)
2, 3, 12
34. Luiten,J.,Kopanas,G.,Leibe,B.,Ramanan,D.:Dynamic3dgaussians:Tracking
by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713 (2023) 420 Y. Huang, W. Zheng et al.
35. Miao, R., Liu, W., Chen, M., Gong, Z., Xu, W., Hu, C., Zhou, S.: Oc-
cdepth: A depth-aware method for 3d semantic scene completion. arXiv preprint
arXiv:2302.13540 (2023) 2, 3
36. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.Commu-
nications of the ACM 65(1), 99–106 (2021) 4
37. Müller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives
with a multiresolution hash encoding. ToG 41(4), 1–15 (2022) 4
38. Murez, Z., Van As, T., Bartolozzi, J., Sinha, A., Badrinarayanan, V., Rabinovich,
A.: Atlas: End-to-end 3d scene reconstruction from posed images. In: ECCV. pp.
414–431. Springer (2020) 11
39. Park,J.,Xu,C.,Yang,S.,Keutzer,K.,Kitani,K.,Tomizuka,M.,Zhan,W.:Time
willtell:Newoutlooksandabaselinefortemporalmulti-view3dobjectdetection.
arXiv preprint arXiv:2210.02443 (2022) 3
40. Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary camera
rigs by implicitly unprojecting to 3d. In: ECCV (2020) 3
41. Qiao, Y.L., Gao, A., Xu, Y., Feng, Y., Huang, J.B., Lin, M.C.: Dynamic mesh-
aware radiance fields. In: ICCV. pp. 385–396 (2023) 4
42. Rakotosaona, M.J., Manhardt, F., Arroyo, D.M., Niemeyer, M., Kundu, A.,
Tombari, F.: Nerfmeshing: Distilling neural radiance fields into geometrically-
accurate 3d meshes. arXiv preprint arXiv:2303.09431 (2023) 4
43. Roldão,L.,deCharette,R.,Verroust-Blondet,A.:Lmscnet:Lightweightmultiscale
3d semantic completion. In: ThreeDV (2020) 3, 11
44. Song, S., Yu, F., Zeng, A., Chang, A.X., Savva, M., Funkhouser, T.: Semantic
scene completion from a single depth image. In: CVPR. pp. 1746–1754 (2017) 11
45. Tang,J.,Ren,J.,Zhou,H.,Liu,Z.,Zeng,G.:Dreamgaussian:Generativegaussian
splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)
4
46. Tong, W., Sima, C., Wang, T., Chen, L., Wu, S., Deng, H., Gu, Y., Lu, L., Luo,
P., Lin, D., et al.: Scene as occupancy. In: ICCV. pp. 8406–8415 (2023) 2
47. Wang,T.,Zhu,X.,Pang,J.,Lin,D.:Fcos3d:Fullyconvolutionalone-stagemonoc-
ular 3d object detection. In: ICCV. pp. 913–922 (2021) 10
48. Wang, X., Zhu, Z., Xu, W., Zhang, Y., Wei, Y., Chi, X., Ye, Y., Du, D., Lu,
J., Wang, X.: Openoccupancy: A large scale benchmark for surrounding semantic
occupancy perception. arXiv preprint arXiv:2303.03991 (2023) 5
49. Wang, Y., Chen, Y., Liao, X., Fan, L., Zhang, Z.: Panoocc: Unified occu-
pancy representation for camera-based 3d panoptic segmentation. arXiv preprint
arXiv:2306.10013 (2023) 3, 12
50. Wei, Y., Zhao, L., Zheng, W., Zhu, Z., Zhou, J., Lu, J.: Surroundocc: Multi-
camera 3d occupancy prediction for autonomous driving. In: ICCV. pp. 21729–
21740 (2023) 2, 3, 5, 10, 11, 15
51. Yan,X.,Gao,J.,Li,J.,Zhang,R.,Li,Z.,Huang,R.,Cui,S.:Sparsesinglesweep
lidar point cloud segmentation via learning contextual shape priors from scene
completion. In: AAAI. vol. 35, pp. 3101–3109 (2021) 3
52. Yang, B., Bao, C., Zeng, J., Bao, H., Zhang, Y., Cui, Z., Zhang, G.: Neumesh:
Learning disentangled neural mesh-based implicit field for geometry and texture
editing. In: ECCV. pp. 597–614. Springer (2022) 4
53. Yang,C.,Chen,Y.,Tian,H.,Tao,C.,Zhu,X.,Zhang,Z.,Huang,G.,Li,H.,Qiao,
Y., Lu, L., et al.: Bevformer v2: Adapting modern image backbones to bird’s-
eye-view recognition via perspective supervision. arXiv preprint arXiv:2211.10439
(2022) 3GaussianFormer 21
54. Yang, Z., Gao, X., Zhou, W., Jiao, S., Zhang, Y., Jin, X.: Deformable 3d gaus-
sians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint
arXiv:2309.13101 (2023) 4
55. Yi, T., Fang, J., Wu, G., Xie, L., Zhang, X., Liu, W., Tian, Q., Wang, X.: Gaus-
siandreamer: Fast generation from text to 3d gaussian splatting with point cloud
priors. arXiv preprint arXiv:2310.08529 (2023) 4
56. Yu, Z., Shu, C., Deng, J., Lu, K., Liu, Z., Yu, J., Yang, D., Li, H., Chen, Y.:
Flashocc: Fast and memory-efficient occupancy prediction via channel-to-height
plugin. arXiv preprint arXiv:2311.12058 (2023) 2, 4
57. Zhang,Y.,Zhu,Z.,Du,D.:Occformer:Dual-pathtransformerforvision-based3d
semantic occupancy prediction. arXiv preprint arXiv:2304.05316 (2023) 2, 3, 11
58. Zheng, W., Chen, W., Huang, Y., Zhang, B., Duan, Y., Lu, J.: Occworld:
Learning a 3d occupancy world model for autonomous driving. arXiv preprint
arXiv:2311.16038 (2023) 2
59. Zhou,Y.,Tuzel,O.:Voxelnet:End-to-endlearningforpointcloudbased3dobject
detection. In: CVPR. pp. 4490–4499 (2018) 3
60. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable
transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159
(2020) 8
61. Zou, Z.X., Yu, Z., Guo, Y.C., Li, Y., Liang, D., Cao, Y.P., Zhang, S.H.: Triplane
meetsgaussiansplatting:Fastandgeneralizablesingle-view3dreconstructionwith
transformers. arXiv preprint arXiv:2312.09147 (2023) 4