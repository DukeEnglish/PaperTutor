RB-Modulation: Training-Free Personalization of
Diffusion Models using Stochastic Optimal Control
LituRout1,2,∗ YujiaChen1 NatanielRuiz1 AbhishekKumar3
ConstantineCaramanis2 SanjayShakkottai2 Wen-ShengChu1
1Google 2UTAustin 3GoogleDeepMind
{litu.rout,constantine,sanjay.shakkottai}@utexas.edu
{yujiachen,natanielruiz,abhishk,wschu}@google.com
Reference style (a) Stylization (b) Content-style composition Reference content
A B C A
A guitar A piano A butterfly A dog A cat A bear plushie
B
A skyscraper A lighthour A kangaroo
C
A dwarf A dragon An elf
Figure1: Givenasinglereferenceimage(roundedrectangle),ourmethodRB-Modulationoffersa
plug-and-playsolutionfor(a)stylization,and(b)content-stylecompositionwithvariousprompts
whilemaintainingsamplediversityandpromptalignment. Forinstance,givenareferencestyleimage
(e.g.“meltinggolden3drenderingstyle”)andcontentimage(e.g.(A)“dog”),ourmethodadheres
tothedesiredpromptswithoutleakingcontentsfromthereferencestyleimageandwithoutbeing
restrictedtotheposeofthereferencecontentimage.
Abstract
WeproposeReference-BasedModulation(RB-Modulation),anewplug-and-play
solutionfortraining-freepersonalizationofdiffusionmodels. Existingtraining-
freeapproachesexhibitdifficultiesin(a)styleextractionfromreferenceimagesin
theabsenceofadditionalstyleorcontenttextdescriptions,(b)unwantedcontent
leakagefromreferencestyleimages,and(c)effectivecompositionofstyleand
content. RB-Modulationisbuiltonanovelstochasticoptimalcontrollerwherea
styledescriptorencodesthedesiredattributesthroughaterminalcost.Theresulting
driftnotonlyovercomesthedifficultiesabove,butalsoensureshighfidelitytothe
referencestyleandadherestothegiventextprompt. Wealsointroduceacross-
attention-basedfeatureaggregationschemethatallowsRB-Modulationtodecouple
content and style from the reference image. With theoretical justification and
empiricalevidence,ourframeworkdemonstratespreciseextractionandcontrolof
contentandstyleinatraining-freemanner. Further,ourmethodallowsaseamless
compositionofcontentandstyle,whichmarksadeparturefromthedependency
onexternaladaptersorControlNets.
∗ThisworkwasdoneduringaninternshipatGoogle.
Preprint.Underreview.
4202
yaM
72
]GL.sc[
1v10471.5042:viXra1 Introduction
Text-to-image(T2I)generativemodels[1,2,3,4]haveexcelledincraftingvisuallyappealingimages
fromtextprompts. TheseT2Imodelsareincreasinglyemployedincreativeendeavorssuchasvisual
arts[5],gaming[6],personalizedimagesynthesis[7,8,9,10],stylizedrendering[11,12,13,14],
andimageinversionorediting[15, 16, 17, 18]. Contentcreatorsoftenneedprecisecontrolover
boththecontentandthestyleofgeneratedimagestomatchtheirvision. Whilethecontentofan
imagecanbeconveyedthroughtext,articulatinganartist’suniquestyle–characterizedbydistinct
brushstrokes,colorpalette,material,andtexture–issubstantiallymorenuancedandcomplex. This
hasledtoresearchonpersonalizationthroughvisualprompting[11,12,13].
Recentstudieshavefocusedonfinetuningpre-trainedT2Imodelstolearnstylefromasetofreference
images [19, 7, 11, 9]. This involves optimizing the model’s text embeddings, model weights, or
both,usingthedenoisingdiffusionloss. However,thesemethodsdemandsubstantialcomputational
resourcesfortrainingorfinetuninglarge-scalefoundationmodels,thusmakingthemexpensiveto
adapttonew,unseenstyles. Furthermore,thesemethodsoftendependonhuman-curatedimages
ofthesamestyle,whichislesspracticalandcancompromisequalitywhenonlyasinglereference
imageisavailable.
In training-free stylization, recent methods [12, 13, 14] manipulate keys and values within the
attentionlayersusingjustonereferencestyleimage.Thesemethodsfacechallengesinbothextracting
thestylefromthereferencestyleimageandaccuratelytransferringthestyletoatargetcontentimage.
Forinstance,duringtheDDIMinversionstep[20]utilizedbyStyleAligned[12],fine-graineddetails
tend to be compromised. To mitigate this issue, InstantStyle [13] incorporates features from the
referencestyleimageintospecificlayersofapreviouslytrainedIP-Adapter[21].However,identifying
the exact layer for feature injection in a model is complex and not universally applicable across
models. Also,featureinjectioncancausecontentleakagefromthestyleimageintothegenerated
content. Movingontocontent-stylecomposition,InstantStyle[13]employsaControlNet[22](an
additionallytrainednetwork)topreserveimagelayout,whichinadvertentlylimitsitsdiversity.
WeintroduceReference-BasedModulation(RB-Modulation),anovelapproachforcontentandstyle
personalizationthateliminatestheneedfortrainingorfinetuningdiffusionmodels(e.g.Control-
Net[22]oradapters[21,9]). Ourworkrevealsthatthereversedynamicsindiffusionmodelscanbe
formulatedasstochasticoptimalcontrolwithaterminalcost. Byincorporatingstylefeaturesinto
thecontroller’sterminalcost,wemodulatethedriftfieldindiffusionmodels’reversedynamics,en-
ablingtraining-freepersonalization. Unlikeconventionalattentionprocessorsthatoftenleakcontent
fromthereferencestyleimage,weproposetoenhancetheimagefidelityviaanAttentionFeature
Aggregation(AFA)modulethatdecouplescontentfromreferencestyleimage. Wedemonstratethe
effectivenessofourmethodinstylization[12,13,14]andstyle+contentcomposition,asillustratedin
Fig.1(a)and(b),respectively. OurexperimentsshowthatRB-ModulationoutperformscurrentSoTA
methods[12,13]intermsofhumanpreferenceandprompt-alignmentmetrics.
Ourcontributionsaresummarizedasfollows:
• Wepresentreference-basedmodulation(RB-Modulation),anovelstochasticoptimalcontrol
frameworkthatenablestraining-free,personalizedstyleandcontentcontrol,withanew
Attention Feature Aggregation (AFA) module to maintain high fidelity to the reference
imagewhileadheringtothegivenprompt(§4).
• Weprovidetheoreticaljustificationsconnectingoptimalcontrolandreversediffusiondy-
namics. Weleveragethisconnectiontoincorporatedesiredattributes(e.g., style)inour
controller’sterminalcostandpersonalizeT2Imodelsinatraining-freemanner(§5).
• We perform extensive experiments covering stylization and content-style composition,
demonstratingsuperiorperformanceoverSoTAmethodsinhumanpreferencemetrics(§6).
2 RelatedWork
PersonalizationofT2Imodels: T2Igenerativemodels[3,23,24]arepushingtheboundariesin
generatingaestheticallypleasingimagesbypreciselyinterpretingtextprompts.Theirabilitytofollow
a desired text has unlocked new avenues in personalized content creation, including text-guided
image editing [17, 18], solving inverse problems [16, 17], concept-driven generation [7, 25, 26,
27],personalizedoutpainting[28],identity-preservation[29,8,30],andstylizedsynthesis[11,13,
212,10]. TotailorT2Imodelsforaspecificstyle(e.g.,painting)orcontent(e.g.,object),existing
methodsfollowoneoftworecipes: (1)fullfinetuning(FT)[7,31]orparameterefficientfinetuning
(PEFT)[26,21,9,11,10]and(2)training(finetuning)-free[12,13,14],whichwediscussbelow.
FinetuningT2Imodelsforpersonalization: FT [7,31]andPEFT [26,21,9,11,10]methods,
suchasIP-Adapter[21],LoRA[9],ZipLoRA[10],andStyleDrop[11],excelatcapturingstyleor
objectdetailswhentheunderlyingT2Imodelisfinetunedonafew(typically4)referenceimages
forfewthousanditerations. WiththeincreasingsizeofT2Imodels,PEFTispreferredoverFTdue
to fewer trainable parameters. However, the challenge of curating a set of reference images and
resource-intensivefinetuningforeverystyleorcontentremainslargelyunexplored.
Training(finetuning)-freeT2Imodelsforpersonalization: TheneedtoimproveT2Imodelfinetun-
inghassparkedinterestsintraining-freepersonalizationmethods. InStyleAligned[12],areference
styleimageandatextpromptdescribingthestyleareusedtoextractstylefeaturesviaDDIMinversion
[20]. Targetqueriesandkeysarethennormalizedusingadaptiveinstancenormalization[32]based
onreferencecounterparts. Finally,referenceimagekeysandvaluesaremergedwithDDIM-inverted
latentsinself-attentionlayers,whichtendstoleakcontentinformationfromthereferencestyleimage
(Figure3). Moreover,theneedfortextualdescriptionintheDDIMinversionstepcandegradeits
performance. Swapping Self-Attention (SSA) [14] addresses these limitations by replacing the
targetkeysandvaluesinself-attentionlayerswiththosefromareferencestyleimage. Yet,itstill
reliesonDDIMinversiontocachekeysandvaluesofthereferencestyle,whichtendstocompromise
fine-graineddetails[13]. BothStyleAligned[12] andSSA[14]require tworeverseprocessesto
sharetheirattentionlayerfeaturesandthusdemandsignificantmemory. Recently,InstantStyle[13]
injectsreferencestylefeaturesintospecificcross-attentionlayersofIP-Adapter[21],addressingtwo
keylimitations: DDIMinversionandmemory-intensivereverseprocesses. However,pinpointing
theexactlayersforfeatureinjectioniscomplex,andtheymaynotgeneralizetoothermodels. In
addition,whencomposingstyleandcontent,InstantStyle[13]reliesonControlNet[22],whichcan
limitthediversityofgeneratedimagestofixedlayoutsanddeviatefromtheprompt.
Optimal Control: Stochastic optimal control finds wide applications in diverse fields such as
moleculardynamics[33],economics[34],non-convexoptimization[35],robotics[36],andmean-
fieldgames[37]Despiteitsextensiveuse,ithasbeenlessexploredinpersonalizingdiffusionmodels.
Inthispaper,weintroduceanovelframeworkleveragingthemainconceptsfromoptimalcontrolto
achievetraining-freepersonalization. Akeyaspectofoptimalcontrolisdesigningacontrollerto
guideastochasticprocesstowardsadesiredterminalcondition[34]. Thisalignswithourgoalof
training-freepersonalization,aswetargetaspecificstyleorcontentattheendofthereversediffusion
process,whichcanbeincorporatedinthecontroller’sterminalcondition.
RB-ModulationovercomesseveralchallengesencounteredbySoTAmethods[12,14,13]. SinceRB-
ModulationdoesnotrequireDDIMinversion,itretainsfine-graineddetailsunlikeStyleAligned[12].
Usingastochasticcontrollertorefinethetrajectoryofasinglereverseprocess,itovercomesthelimita-
tionofcoupledreverseprocesses[12]. Byincorporatingastyledescriptorinourcontroller’sterminal
cost,iteliminatesthedependencyonAdapters[21,9]orControlNets[22]byInstantStyle[13].
3 Preliminaries
Diffusionmodelsconsistoftwostochasticprocesses: (a)noisingprocess,modeledbyaStochastic
DifferentialEquation(SDE)knownasforward-SDE:dX =f(X ,t)dt+g(X ,t)dW ,X ∼p ,
t t t t 0 0
and (b) denoising process, modeled by the time-reversal of forward-SDE under mild regularity
conditions[38],alsoknownasreverse-SDE:
dX =(cid:2) f(X ,t)−g2(X ,t)∇logp(X ,t)(cid:3) dt+g(X ,t)dW , X ∼N (0,I ). (1)
t t t t t t 1 d
Here,W =(W ) isstandardBrownianmotioninafilteredprobabilityspace,(Ω,F,(F ) ,P),
t t≥0 t t≥0
p(·,t)denotesthemarginaldensityofpattimet,and∇logp (·)thecorrespondingscorefunction.
t
f(X ,t)andg(X ,t)arecalleddriftandvolatility,respectively.Apopularchoiceoff(X ,t)=−X
t √t t t
andg(X ,t)= 2correspondstothewell-knownforwardOrnstein-Uhlenbeck(OU)process.
t
For T2I generation, the reverse-SDE (1) is simulated using a neural network s(x ,t;θ) [39, 40]
t
to approximate ∇ logp(x ,t). Importantly, to accelerate the sampling process in practice [20,
x t
41, 42], thereverse-SDE(1)sharesthesamepathmeasurewithaprobabilityflowODE:dX =
(cid:2) f(X ,t)− 1g2(X ,t)∇logp(X ,t)(cid:3) dt,whereX ∼N (0,I ). t
t 2 t t 1 d
3Personalizeddiffusionmodelseitherfullyfinetuneθ ofs(x ,t;θ)[7, 31], ortrainaparameter-
t
efficientadapter∆θfors(x ,t;θ+∆θ)onreferencestyleimages[9,11,10]. Ourmethoddoesnot
t
finetuneθortrain∆θ. Instead,wederiveanewdriftfieldthroughastochasticoptimalcontrollerthat
modulatesthedriftofthestandardreverse-SDE(1).
4 Method
Personalizationusingoptimalcontrol: NormalizetimetbythetotalnumberofdiffusionstepsT
suchthat0 ≤ t ≤ 1. Letusdenotebyu : Rd×[0,1] → Rd acontrollerfromtheadmissibleset
ofcontrolsU ⊆ Rd,Xu ∈ Rd astatevariable, ℓ : Rd ×Rd ×[0,1] → Rthetransientcost,and
t
h : Rd → R the terminal cost of the reverse process (Xu)0 . We show in §5 that training-free
t t=1
personalizationcanbeformulatedasacontrolproblemwherethedriftofthestandardreverse-SDE(1)
ismodifiedviaRB-modulation:
(cid:90) 0
minE[ ℓ(Xu,u(Xu,t),t)dt+γh(Xu)], where (2)
t t 0
u∈U 1
dXu =(cid:2) f(Xu,t)−g2(Xu,t)∇logp(Xu,t)+u(Xu,t)(cid:3) dt+g(Xu,t)dW ,Xu ∼N (0,I ).
t t t t t t t 1 d
Importantly,theterminalcosth(·),weightedbyγ,capturesthediscrepancyinfeaturespacebetween
thestylesofthereferenceimageandthegeneratedimage. Theresultingcontrolleru(·,t)modulates
thedriftovertimetosatisfythisterminalcost. Wederivethesolutiontothisoptimalcontrolproblem
throughtheHamilton-Jacobi-Bellman(HJB)equation[34]; refertoAppendixAfordetails. Our
proposedRB-ModulationAlgorithm1hastwokeycomponents: (a)stochasticoptimalcontroller
and(b)attentionfeatureaggregation. Below,wediscusseachinturn.
(a)StochasticOptimalController(SOC):Weshowthatthereversedynamicsindiffusionmodels
canbeframedasastochasticoptimalcontrolproblemwithaquadraticterminalcost(theoretical
analysisin§5). ForpersonalizationusingareferencestyleimageXf = z ,weuseaConsistent
0 0
StyleDescriptor(CSD)[43]toextractstylefeaturesΨ(Xf). Sincethescorefunctionss(x ,t;θ)≈
0 t
∇logp(X ,t)areavailablefrompre-traineddiffusionmodels[23,24],ourgoalistoaddacorrection
t
termu(·,t)tomodulatethereverse-SDEandminimizetheoverallcost(2). WeapproximateXu
0
withitsconditionalexpectationusingTweedie’sformula[16,17]. Finally,weincorporatethestyle
featuresintoourcontroller’sterminalcostas: h(Xu)=∥Ψ(Xf)−Ψ(E[Xu|Xu])∥2.
0 0 0 t 2
Our theoretical results (§5) suggest that the optimal controller can be obtained by solving the
HJB equation and letting γ → ∞. In practice, this translates to dropping the transient cost
ℓ(Xu,u(Xu,t),t)andsolving(2)withonlytheterminalconstraint,i.e.,
t t
min∥Ψ(Xf)−Ψ(E[Xu|Xu])∥2. (3)
0 0 t 2
u∈U
Thus,wesolve(3)tofindtheoptimalcontroluandusethiscontrollerinthereversedynamics(2)to
updatethecurrentstatefromXutoXu (recallthattimeflowsbackwardsinthereverse-SDE(1)).
t t−∆t
Ourimplementationof(3)isgiveninAlgorithm1,whichfollowsfromourtheoreticalinsights.
Implementationchallenge: Forsmallergenerativemodels[3],wecandirectlysolveourcontrol
problem (3). However, for larger models [23, 24], optimizing our control objective (3) requires
backpropagationthroughthescorenetworks(x ,t;θ)withtentativelybillionsofparameters. This
t
significantlyincreasestimeandmemorycomplexity[16,17].
We propose a proximal gradient descent approach to address this challenge. Recall that the key
ingredientofourAlgorithm1istofindthepreviousstateX bymodulatingthecurrentstate
t−∆t
X based on an optimal controller u∗. The optimal controller u∗ is obtained by minimizing the
t
discrepancyinstylebetweenX¯u :=E[Xu|Xu =x ],obtainedusingourcontrolledreverse-SDE(3),
0 0 t t
andthereferencestyleimagez . Motivatedbythisinterpretation,analternateAlgorithm2(see
0
Appendix B.2) avoids back propagation through s(x ,t;θ) by introducing a dummy variable x ,
t 0
which serves as a proxy for X¯u in the terminal cost. Instead of forcing x to be decided by the
0 0
dynamicsofthereverse-SDEasinAlgorithm1,weallowittobeonlyapproximatelyfaithfultothe
dynamics. Thisisimplementedbyaddingaproximalpenalty,i.e.x∗ = argmin ∥Ψ(Xf)−
0 x0∈Rd 0
Ψ(x )∥2+λ∥x −E[Xu|Xu]∥2,wherethehyper-parameterλcontrolsthefaithfulnessofthereverse
0 2 0 0 t 2
dynamics. Thispenaltyassumesthatwithasmallstep-sizeinthereverse-SDEdynamics(3),x∗
0
4Content-Style Composition
Algorithm1:Reference-BasedModulation K V
Input:DiffusionstepsT,referencepromptp,referenceimage K V Kp Vp
z0,styledescriptorΨ(·),scorenetworks(·,·,·;θ) K V K V Kp Vp Ks Vs
Tunableparameter:Stepsizeη,optimizationstepsM Kp Vp Ks Vs Ks Vs Kc Vc
Output:PersonalizedlatentXu
0
1 InitializexT ←N(0,Id)
2 fort=Tto1do Q Multi-Head Attention
3 Initializecontrolleru=0
4 form=1toMdo Avg
5 xˆt=xt+u ▷controlledstate
Figure 2: Attention Feature Aggregation
6 X¯ 0u= √xˆ α¯t
t
+ (1√− α¯α¯ tt)s(xˆt,t,p;θ)
(AFA): Within the cross-attention layers, the
987
end
uh( =X¯ 0u u) −= η∥ ∇Ψ u( hz (0 X) ¯− 0u)Ψ(X¯ 0u) ▷∥2 2 upu dsi an teg cE oq n. t( r3 o)
ller
k tee xy ts ea mnd bev da dlu ine gsf (r Kom p,Vth pe ),p rr ee fv ei ro eu ns cela sy te yr ls e( iK m, aV ge),
11 10 Xx ¯∗ t 0u= =x √t x α+ ¯∗ t tu +(1√− α¯α¯ tt)s(x▷ ∗ to ,p tt ,im p;al θly )c ▷o tn et rr mol il ne ad ls st ta at te e ( cK ons c,V ats e) na an td edre af ne dre pn rc oe cc eo sn sete dn st eim paa rg ae te( lK yc to,V dc i) sa er ne -
12 xt−1←DDIM(X¯ 0u,x∗ t)▷onestepreverse-SDE[20] tangletheinformation,whichisfollowedbyan
13 end averaginglayerfortheoutput. K ,V andonly
14 returnX 0u usedforcontent-stylecompositionc
.
c
andE[Xu|Xu =x ]willbeclose. Therefore,Algorithm2enablespersonalizationoflarge-scale
0 t t
foundationmodelswithoutsignificantlyincreasingtimeandmemorycomplexity.
(b)AttentionFeatureAggregation(AFA):Transformer-baseddiffusionmodels[3,23,24]consist
ofself-attentionandcross-attentionlayersoperatingonlatentembeddingx
t
∈Rd×nh. Withinthe
attentionmoduleAttention(Q,K,V),x
t
isprojectedintoqueriesQ ∈ Rd×nq,keysK ∈ Rd×nq,
andvalues V ∈ Rd×nh usinglinearprojections. Through Q, K, and V, attentionlayerscapture
globalcontextandimprovelong-rangedependencieswithinx .
t
Toaccommodateareferenceimage(e.g.,styleorcontent)whileretainingprompt-alignment,we
proposeanAttentionFeatureAggregation(AFA)module, asillustratedinFigure2. Foragiven
promptp,areferencestyleimageI ,andareferencecontentimageI ,wefirstextracttheembeddings
s c
usingCLIP[44]textandimageencoder,respectively. Then,weprojecttheseembeddingsintokeys
andvaluesusinglinearprojectionlayers. WedenotebyK andV thekeysandvaluesfromp,K
p p s
andV fromI ,K andV fromI (usedonlyincontent-stylecomposition). ThequeryQisobtained
s s c c c
fromalinearprojectionofx ,andremainsthesameintheAFAmodule. Byprocessingthekeysand
t
valuesseparately,wedisentangletheirrelativeimportancewithrespecttothestatevariable. This
ensuresthattheattentionmapsfromtextarenotcontaminatedwithattentionmapsfromstyle. To
makethetextconsistentwiththestyle,wealsocomposethekeysandvaluesofbothtextandstylein
ourattentionprocessor. ThefinaloutputofourAFAmoduleisgivenby
AFA=Avg(A ,A ,A ),A =Attention(Q,[K;K ],[V;V ]),
text style text+style text p p
A =Attention(Q,[K;K ],[V;V ]),A =Attention(Q,[K;K ;K ],[V;V ;V ]),
style s s text+style p s p s
where[K;K p]∈R2d×nq indicatesconcatenationofK withK palongthenumberoftokensdimen-
sion. Forstyle-contentcomposition,weprocessthecontentimageI inthesamewayasthereference
c
styleimageI ,andobtainanothersetofattentionoutputs:
s
AFA=Avg(A ,A ,A ,A ),
text style content content+style
A =Attention(Q,[K;K ],[V;V ]),A =Attention(Q,[K;K ;K ],[V;V ;V ]).
content c c content+style s c s c
Importantly,theAFAmoduleiscomputationallytractableasitonlyrequiresthecomputationofa
multi-headattention,whichiswidelyusedinpractice[23].
5 TheoreticalJustifications
Problemsetup: Weoutlineanapproachtoderivetheoptimalcontrollerforaspecialcaseofour
controlproblem(2). Wesubstitutet←1−ttoaccountforthetimereversalinthereverse-SDE(1).
Here,Xu ∼ N (0,I )andXu ∼ p . WeconsiderthedynamicwithouttheBrownianmotion:
0 d 1 data
dXu =v(Xu,u,t)dt, Xu =x ,where0≤t ≤t≤t ≤1andv :Rd×Rd×[t ,t ]→Rd
dent otesthedt
riftfield.
Thet0 optim0 alcontrolleru0
∗
canbeN derivedbysolvingtheHam0 iltoN
n-Jacobi-
Bellman(HJB)equation[34,45],seeAppendixAfordetails.
5Incorporating optimal control in diffusion: Following recent works [46, 47], we consider a
dynamicalsystemwhosedriftfieldminimizesatransienttrajectorycostandaterminalcost(weighted
byγ)toensure“closeness”toreferencecontentx (AppendixA.1). PropositionA.2[47]outlines
1
the optimal control in the limiting setting where γ → ∞. Furthermore, suppose we replace x
1
withitsconditionalexpectation(discussedinRemarkA.3),theresultingdynamicisthestandard
reverse-SDEfortheOrstein-Uhlenbeck(OU)diffusionprocessforaparticularnoiseschedule. This
connectionbetweenclassiclinearquadraticcontrolandthestandardreverse-SDEallowsustostudy
otherdiffusionproblems(e.g.,personalization)throughthelensofstochasticoptimalcontrol. For
instance,wederivetheoptimalcontrollergivenreferencestylefeaturesy attheterminaltime.
1
Proposition5.1. SupposeA∈Rk×dbealinearstyleextractorthatoperatesontheterminalstate
Xu ∈Rd. Givenreferencestylefeaturesy ,considerthecontrolproblem:
1 1
(cid:90) 1 1 γ
min ∥u(Xu,t)∥2dt+ ∥AXu−y ∥2, wheredXu =u(Xu,t)dt, Xu =x .
u∈U t0 2 t 2 1 1 2 t t t0 0
Then,inthelimitwhenγ →∞,theoptimalcontrolleru∗ =
(ATA)−1AT(y1−Axt)
,whichyieldsthe
1−t
followingcontrolleddynamic: dXu =
(ATA)−1AT(y1−Axt)
dt.
t 1−t
Implication. Theoptimalcontrollerdependsonthereferencestylefeaturesy attheterminaltime,
1
instead of the image content encoded in x . To simulate the controlled dynamic in practice, we
1
useCSD[43]asastylefeatureextractorandreplacey withthestylefeaturesextractedfromthe
1
expectedterminalstateE[Xu|Xu],asdiscussedinAppendixA.2.
1 t
Driftmodulationthroughoptimalcontroller: Wethenstudyacontrolproblemwherethevelocity
field is a linear combination of the state and the control variable. This problem is interesting to
studybecausethereverse-SDEdynamicofthestandardOUprocesshasadriftfieldoftheform:
v(X ,t)=−X −2∇logp(X ,t).ForaGaussianpriorX ∼N (0,I),thelawoftheOUprocess
t t t 0
satisfies∇logp(X ,t)=−X ,andthecorrespondingdriftfieldbecomesv(X ,t)=X . Ourgoal
t t t t
istomodulatethisdriftfieldusingacontrolleru(Xu,t). Theresultbelowprovidesthestructureof
t
theoptimalcontrol(againinthesettingwheretheterminalobjectiveisknown;seeAppendixA1).
Proposition5.2. SupposeA∈Rk×dbealinearstyleextractorthatoperatesontheterminalstate
Xu ∈ Rd. Let p denote ∇ V∗(x,t) in HJB equation (A.1). Given reference style features y ,
1 t x 1
considerthecontrolproblem:
(cid:90) 1 1 γ
min ∥u(Xu,t)∥2dt+ ∥AXu−y ∥2, wheredXu =[Xu+u(Xu,t)]dt, Xu =x ,
u∈U t0 2 t 2 1 1 2 t t t t0 0
Then,theoptimalcontrollerbecomesu∗(t)=−p ,wheretheinstantaneousstateXu =x andp
t t t t
satisfythefollowingcoupledtransitions:
(cid:20) x (cid:21) (cid:20) x et− γAT (Ax −y )e1+t+ γAT (Ax −y )e1−t(cid:21)
t = 0 2 1 1 2 1 1 .
p γAT (Ax −y )e1−t
t 1 1
Summary. We build on the connection between optimal control and reverse diffusion (see Ap-
pendicesA.1-A.3fordetails). Thegeneralstrategyistoderivetheoptimalcontrollerwithknown
terminalstate,andthenreplacetheterminalstateinthecontrollerwithitsestimateusingTweedie’s
formula. ForstylizedmodelsandGaussianprior,thecontrollershaveanexplicitform. However
inpractice,thedatadistributionmaynotbeGaussian,andthus,wedonotaimforaclosed-form
expressiontomodulatethedrift. Thislineofanalysis,however,pointstoourmethodRB-Modulation.
Asdiscussedin§4,weincorporateastyledescriptorinourcontroller’sterminalcostandnumerically
evaluatetheresultingdriftateachreversetimestepeitherthroughbackpropagatingthroughthescore
network(Algorithm1),oranapproximationbasedonproximalgradientupdates(Algorithm2).
6 Experiments
Metrics: Evaluatingstylizedsynthesisischallengingduetothesubjectivenatureofstyle,making
simplemetrics inadequate. Wefollowatwo stepapproach: first usingmetricsfrom priorworks
and then conducting human evaluation. To evaluate prompt-image alignment, we use CLIP-T
6Reference style Ours InstantStyle StyleAligned StyleDrop
A crown A crown A crown A crown
A thumbs up A big smiley face An avocado A big smiley face An avocado A big smiley face An avocado A big smiley face An avocado
Snowboard Snowboard Snowboard Snowboard
A piano A rubber duck A villa A rubber duck A villa A rubber duck A villa A rubber duck A villa
A book A book A book A book
A smartphone A glass of wine A dinner table A glass of wine A dinner table A glass of wine A dinner table A glass of wine A dinner table
A dog A dog A dog A dog
A house A lion A temple A lion A temple A lion A temple A lion A temple
Figure 3: Qualitative results for stylization: A comparison with state-of-the-art methods (In-
stantStyle[13],StyleAligned[12],StyleDrop[11])highlightsouradvantagesinpreventinginforma-
tionleakagefromthereferencestyleandadheringmorecloselytodesiredprompts.
score[12,11,13]andImageReward[5],whichalsoconsiderhumanaesthetics,distortions,andobject
completeness. Whenastyledescriptionisprovided,CLIP-TandImageRewardalsocapturestyle
alignment. WeassessstylesimilarityusingDINO[48]andcontentsimilarityusingCLIP-I[44]asin
priorwork[12,7,11],andhighlighttheirlimitationsindisentanglingstyleandcontentperformance
inevaluation.GiventheimportanceofhumanevaluationinT2Ipersonalization[12,11,7,10,14],we
alsoconductauserstudythoughAmazonMechanicalTurktomeasurebothstyleandtextalignment.
Datasetsandbaselines: WeusestyleimagesfromStyleAlignedbenchmark[12]forstylizationand
contentimagesfromDreamBooth[7]forcontent-stylecomposition. WebaseRB-Modulationonthe
recentlyreleasedStableCascade[24]. Wecompareourapproachwiththreetraining-freemethods:
InstantStyle[13](state-of-the-art),IP-Adapter[21],andStyleAligned[12]. Forcompleteness,we
alsocomparewithtraining-basedmethodsStyleDrop[11]andZipLoRA[10].
Implementationdetails: AllexperimentsrunonasingleA100NVIDIAGPU.Weusethesame
hyper-parametersforourmethodacrosstasks,anddefaultsettingsforalternativemethodsaspertheir
originalpapers. DetailsareprovidedinAppendixB.1.
6.1 ImageStylization
Qualitativeanalysis: Thissectiondescribesimagestylizationexperimentsusingatextpromptanda
referencestyleimage. Figure3comparesourmethodwithSoTAtraining-freeInstantStyle[13]and
StyleAligned[12],andtraining-basedStyleDrop[11]. ExceptforStyleDrop,whichrequires∼5
minutesoftrainingperstyle,allmethods,includingours,aretraining-freeandcompleteinference
in<1minute. Whileallmethodsproducereasonableoutputs,alternativemethodsencounterissues
withinformationleakage. Forinstance,inthethirdrowofFigure3,StyleAlignedandStyleDrop
generateawinebottleandbookresemblingthesmartphoneinthereferencestyleimage. Inthelast
row,StyleAlignedleaksthehouseandthebackgroundofthereferenceimage;InstantStyleexhibits
colorleakagefromthehouse,resultinginsimilar-coloredimages. Ourmethodaccuratelyadheresto
thepromptinthedesiredstyle. Asillustratedinthesecondandthethirdrow,ourmethodgenerates
onlyoneglassofwineandahigh-fidelityrubberduck, comparedtobaselineswhereextraitems
appear(winebottlesstyledliketheleftsmartphone)orincorrectstyles(cartoon-stylerubberduck).
Userstudy: Tovalidatethequalitativeanalysis,weconductauserstudyonAmazonMechanical
Turkwith155participantsusing100stylesfromtheStyleAligneddataset[12],collectingatotal
7Human Oursvs.InstantStyle[13] Oursvs.StyleAligned[12] Oursvs.IP-Adapter[21]
Preference(%) OQ↑ SA↑ PA↑ OQ↑ SA↑ PA↑ OQ↑ SA↑ PA↑
Alternative 39.8 38.5 39.5 24.4 27.8 29.4 8.1 20.1 8.3
Tie 9.3 6.4 7.3 8.8 7.1 5.8 6.9 4.8 4.5
RB-Modulation(ours) 51.0 55.1 53.3 66.9 65.1 64.9 85.0 75.1 87.2
Table1: Userstudy: Wereportthe%ofhumanpreferenceonoursvs.alternativesforoverallquality
(OQ),stylealignment(SA),andpromptalignment(PA),includingtieswhereuserscouldn’tdecide.
Ourmethodconsistentlyoutperformsalternatives,achievinghigherscoresinallmetrics.
Reference style Content prompt StableCascade DirectConcat AFA only SOC only AFA + SOC
(cid:147)A cat(cid:148)
(cid:147)A piano(cid:148)
Figure4: Ablationstudy: Ourmethodbuildsonanytransformer-baseddiffusionmodel. Inthis
case, we use StableCascade [24] as the foundation, and sequentially add each module to show
theireffectiveness. DirectConcatinvolvesconcatenatingreferenceimageembeddingswithprompt
embeddings. Styledescriptionsareexcludedinthisablationstudy.
of7,200answers(8responsesforeachquestion). Eachuseranswers3questionscomparingour
methodwithanalternativemethodregarding(1)overallquality,(2)stylealignment,and(3)prompt
alignment(detailsintheAppendixB.7). Table1summarizesthepercentageofhumanpreferences
forourmethod,thealternativemethod,oratie. Ourmethodconsistentlyoutperformsthealternatives,
includingthemostcompetitivemethodInstantStyle[13]instylealignment. Thepreferencerates
overallthreemetricshighlighttheeffectivenessofourmethodRB-Modulation.
Quantitativeanalysis:Table2evaluates300promptsand100stylesontheStyleAligneddataset[12]
usingthreemetrics,withandwithoutstyledescriptionsintheprompts. Ourmethodoutperforms
others notably in the ImageReward metric, closely matching human aesthetics assessment from
theuserstudyinTable1. Inaddition,theCLIP-Tscoreindicatesoureffectivealignmentbetween
generatedimagesandtextprompts. WhileIP-AdapterandStyleAlignedhavehigherDINOscores,
theirlowerratinginImageReward,CLIP-Tanduserpreferenceexposeinformationleakagefrom
the reference style images. Nevertheless, our DINO score remains competitive with the leading
methodInstantStyle. Notably,allmetricsshowimprovementwithstyledescriptions,particularly
in ImageReward, where leveraging style descriptions enhances prompt alignment. Our method
achieveshighImageRewardandCLIP-Tscoreevenwithoutstyledescriptions,suggestingrobustness
inpromptalignmentwithoutexplicitstyleinformationintheprompt.
Ablation Study: Figure 4 shows an ablation study of the AFA and SOC modules adding new
capabilities to StableCascade [24]. We include a baseline, “DirectConcat”, which concatenates
referencestyleembeddingswithtextembeddingsinthecross-attentionmodules. DirectConcatmixes
bothembeddings,makingitlesseffectiveindisentanglingstylefromprompts(e.g.,catvs.lighthouse).
WhileAFAorSOCalonemitigatesthisbymodulatingthereversedriftandattentionmodules(§4),
eachhasdrawbacks. AFAalonefailstocapturethecat’sstyleaccurately,andSOCalonemisplaces
elements,like“alighthousehatonthecat”and“arailroadtrunkonapiano”. Weobserveconsistent
improvements with each module, with the best results when combined. Quantitative analysis is
omittedduetothelackofsuitablemetricsforinformationleakage,asdetailedinAppendixB.5.
6.2 Content-StyleComposition
Qualitativeanalysis: Content-stylecompositionaimstopreservetheessenceofbothcontentand
styledepictedinthereferenceimages,whileensuringtheresultingimagealignswithagiventext
prompt. Figure5comparesourmethodagainsttraining-freeInstantStyle[13], IP-Adapter[21],
8ImageReward↑ CLIP-Tscore↑ DINOscore
Withstyledescription? No Yes No Yes No Yes
IP-Adapter[21] -1.99 -1.51 0.21 0.26 0.89 0.89
StyleAligned[12] -0.68 0.01 0.26 0.31 0.80 0.85
InstantStyle[13] 0.09 0.72 0.29 0.33 0.68 0.72
RB-Modulation(ours) 0.91 1.18 0.30 0.34 0.68 0.73
Table2: Quantitativeresultsforstylization: Wecomparealternativemethodsonthreemetrics:
ImageReward[5]andCLIP-T[44]forpromptalignment,DINO[48]forstylealignment. Notethat
DINOscoredoesnotcaptureinformationleakage,sohigherscoresarenotnecessarilybetter(§B.5).
Ref. content Reference styles Reference styles Reference styles
(a) (cid:147)A dog dancing on a table near the river(cid:148) (b) (cid:147)A sloth walking on the street(cid:148) (c) (cid:147)A cat walking(cid:148)
Figure 5: Qualitative results for content-style composition: Our method shows better prompt
alignmentandgreaterdiversitythantraining-freemethodsIP-Adapter[21]andInstantStyle[13],and
havecompetitiveperformancewithtraining-basedZipLoRA[10].
ImageReward↑ CLIP-Tscore↑ DINOscore CLIP-Iscore
IP-Adapter[21] -0.78 0.22 0.73 0.68
InstantStyle[13] -0.54 0.21 0.71 0.71
RB-Modulation(ours) 0.74 0.26 0.74 0.71
Table3: Quantitativeresultsforcomposition: Inadditiontothemetricsusedforstylization,we
useCLIP-Tscore[44]toevaluatecontentalignmentwiththereferencecontent. SimilartoDINO,
CLIP-Icouldinflatetestscore[11,10]bycapturingcontentleakage,butnotnecessarilypreferredby
users;higherDINOandCLIP-Iscoresdonotmeanbetterhumanpreference.
andtraining-basedZipLoRA[10]. Notably,thetraining-freeInstantStyleandIP-Adapterrelyon
ControlNet[22],whichoftenconstrainstheirabilitytoaccuratelyfollowpromptsforchangingthe
poseofthegeneratedcontent,suchasillustrating“dancing”inFigure5(b),or“walking”in(c). In
contrast, our method avoids the need for ControlNet or adapters, and can effectively capture the
distinctiveattributesofbothstyleandcontentimageswhileadheringtotheprompttogeneratediverse
images. InFigure5(a),ourmethodaccuratelycaptureselementslike“table”and“river”thatare
overlookedinInstantStyleandIP-Adapter. Inaddition,ourmethodmitigatesinformationleakage,
asevidencedinFigure5(b),wherethetrunkofthetreebehindtheslothiserroneouslycapturedby
InstantStyleandIP-Adapterbutnotbyours. ComparedtoZipLoRA[10]thatrequirestrainingof
12LoRAs[9]andadditionalmergelayersforeachcomposition,ourmethodrequiresnotrainingat
allwhileyieldingcompetitiveorbetterresults. Forinstance,ourmethodeffectivelycapturesthe2D
cartoonand3DrenderingstylesasillustratedinFigures5(a)and(b).
Quantitativeanalysis: Table3showsquantitativeevaluationusing50stylesfromStyleAligned
dataset[12]and5contentsfromDreamBoothdataset[7]. Unlikepriorworks[12,11,10,7,14]
reporting either DINO and CLIP-I scores, we present both metrics and demonstrate comparable
9
retpadA-PI
elytStnatsnI
ARoLpiZ
sruOperformanceacrossthem. Additionally,weobtainnotablyhigherImageRewardscore,whichaligns
closelywithhumanaestheticsassessmentasevidencedin§6.1and[5]. Consequently,weomitteda
userstudyinthissection. Formoredetails,pleaserefertoAppendixB.1.
7 Conclusion
WeintroducedReference-Basedmodulation(RB-Modulation),atraining-freemethodforpersonaliz-
ingtransformer-baseddiffusionmodels. RB-Modulationbuildsonconceptsfromstochasticoptimal
controltomodulatethedriftfieldofreversediffusiondynamics,incorporatingdesiredattributes(e.g.,
styleorcontent)viaaterminalcost. OurAttentionFeatureAggregation(AFA)moduledecouples
contentand styleinthe cross-attentionlayersand enablesprecisecontrol over both. In addition,
we derived theoretical connections between linear quadratic control and the denoising diffusion
process,whichledtothecreationofRB-Modulation. Empirically,ourmethodoutperformedcurrent
state-of-the-artmethodsinstylizationandcontent+stylecomposition. Toourbestknowledge,thisis
thefirsttraining-freepersonalizationframeworkusingstochasticoptimalcontrol,whichmarksthe
departurefromexternaladaptersorControlNets.
Limitation: We proposed a framework and demonstrated its efficacy by incorporating a style
descriptor[43]inapre-traineddiffusionmodel[24]. Theinherentlimitationsofthestyledescriptor
ordiffusionmodelmightpropagateintoourframework.Webelievetheselimitationscanbeaddressed
byappropriatereplacementsofthedescriptororgenerativepriorinaplug-and-playmanner.
Acknowledgements: ThisresearchhasbeensupportedbyNSFGrant2019844,aGoogleresearch
collaborationaward,andtheUTAustinMachineLearningLab. LituRouthasbeensupportedby
Ju-NamandPearlChewPresidentialFellowshipandGeorgeJ.HeuerGraduateFellowship.
References
[1] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,Mark
Chen,andIlyaSutskever.“Zero-shottext-to-imagegeneration”.In:InternationalConference
onMachineLearning.PMLR.2021,pp.8821–8831.
[2] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.“Hierarchical
text-conditional image generation with clip latents”. In: arXiv preprint arXiv:2204.06125
(2022).
[3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
“High-resolution image synthesis with latent diffusion models”. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,pp.10684–10695.
[4] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,Seyed
KamyarSeyedGhasemipour,BurcuKaragolAyan,SSaraMahdavi,RaphaGontijoLopes,
etal.“PhotorealisticText-to-ImageDiffusionModelswithDeepLanguageUnderstanding”.
In:arXivpreprintarXiv:2205.11487(2022).
[5] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and
YuxiaoDong.“Imagereward:Learningandevaluatinghumanpreferencesfortext-to-image
generation”.In:AdvancesinNeuralInformationProcessingSystems36(2024).
[6] TimPearce,TabishRashid,AnssiKanervisto,DaveBignell,MingfeiSun,RalucaGeorgescu,
SergioValcarcelMacua,ShanZhengTan,IdaMomennejad,KatjaHofmann,andSamDe-
vlin.“ImitatingHumanBehaviourwithDiffusionModels”.In:TheEleventhInternational
ConferenceonLearningRepresentations.2023.URL:https://openreview.net/forum?
id=Pv1GPQzRrC8.
[7] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAber-
man.“Dreambooth:Finetuningtext-to-imagediffusionmodelsforsubject-drivengeneration”.
In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
2023,pp.22500–22510.
[8] JiehuiHuang,XiaoDong,WenhuiSong,HanhuiLi,JunZhou,YuhaoCheng,ShutaoLiao,
Long Chen, Yiqiang Yan, Shengcai Liao, et al. “ConsistentID: Portrait Generation with
MultimodalFine-GrainedIdentityPreserving”.In:arXivpreprintarXiv:2404.16771(2024).
10[9] EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,Weizhu
Chen,etal.“LoRA:Low-RankAdaptationofLargeLanguageModels”.In:International
ConferenceonLearningRepresentations.2021.
[10] VirajShah,NatanielRuiz,ForresterCole,ErikaLu,SvetlanaLazebnik,YuanzhenLi,and
VarunJampani.“ZipLoRA:Anysubjectinanystylebyeffectivelymergingloras”.In:arXiv
preprintarXiv:2311.13600(2023).
[11] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang,
JarredBarber,LuJiang,GlennEntis,YuanzhenLi,etal.“StyleDrop:Text-to-ImageGeneration
in Any Style”. In: 37th Conference on Neural Information Processing Systems (NeurIPS).
NeuralInformationProcessingSystemsFoundation.2023.
[12] AmirHertz,AndreyVoynov,ShlomiFruchter,andDanielCohen-Or.“Stylealignedimage
generationviasharedattention”.In:arXivpreprintarXiv:2312.02133(2023).
[13] HaofanWang,QixunWang,XuBai,ZekuiQin,andAnthonyChen.“InstantStyle:FreeLunch
towardsStyle-PreservinginText-to-ImageGeneration”.In:arXivpreprintarXiv:2404.02733
(2024).
[14] JaeseokJeong,JunhoKim,YunjeyChoi,GayoungLee,andYoungjungUh.“VisualStyle
PromptingwithSwappingSelf-Attention”.In:arXivpreprintarXiv:2402.12974(2024).
[15] MauricioDelbracioandPeymanMilanfar.“InversionbyDirectIteration:AnAlternativeto
DenoisingDiffusionforImageRestoration”.In:TransactionsonMachineLearningResearch
(2023).
[16] LituRout,NeginRaoof,GiannisDaras,ConstantineCaramanis,AlexandrosGDimakis,and
SanjayShakkottai.“SolvingInverseProblemsProvablyviaPosteriorSamplingwithLatent
DiffusionModels”.In:Thirty-seventhConferenceonNeuralInformationProcessingSystems.
2023.URL:https://openreview.net/forum?id=XKBFdYwfRo.
[17] LituRout,YujiaChen,AbhishekKumar,ConstantineCaramanis,SanjayShakkottai,andWen-
ShengChu.“BeyondFirst-OrderTweedie:SolvingInverseProblemsusingLatentDiffusion”.
In:2024IEEE/CVFConferenceonComputerVisionandPatternRecognition.2024.
[18] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. “Null-text
inversion for editing real images using guided diffusion models”. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2023,pp.6038–6047.
[19] RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHBermano,GalChechik,and
DanielCohen-Or.“Animageisworthoneword:Personalizingtext-to-imagegenerationusing
textualinversion”.In:arXivpreprintarXiv:2208.01618(2022).
[20] JiamingSong,ChenlinMeng,andStefanoErmon.“DenoisingDiffusionImplicitModels”.In:
InternationalConferenceonLearningRepresentations.2021.URL:https://openreview.
net/forum?id=St1giarCHLP.
[21] HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang.“Ip-adapter:Textcompatibleimage
prompt adapter for text-to-image diffusion models”. In: arXiv preprint arXiv:2308.06721
(2023).
[22] LvminZhang,AnyiRao,andManeeshAgrawala.“Addingconditionalcontroltotext-to-image
diffusionmodels”.In:ProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision.2023,pp.3836–3847.
[23] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,Joe
Penna,andRobinRombach.“SDXL:ImprovingLatentDiffusionModelsforHigh-Resolution
Image Synthesis”. In: The Twelfth International Conference on Learning Representations.
2023.
[24] PabloPernias,DominicRampas,MatsLeonRichter,ChristopherPal,andMarcAubreville.
“Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models”.
In:TheTwelfthInternationalConferenceonLearningRepresentations.2024.URL:https:
//openreview.net/forum?id=gU58d5QeGv.
[25] YoadTewel,RinonGal,GalChechik,andYuvalAtzmon.“Key-lockedrankoneeditingfor
text-to-imagepersonalization”.In:ACMSIGGRAPH2023ConferenceProceedings.2023,
pp.1–11.
[26] NupurKumari,BingliangZhang,RichardZhang,EliShechtman,andJun-YanZhu.“Multi-
conceptcustomizationoftext-to-imagediffusion”.In:ProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition.2023,pp.1931–1941.
11[27] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and
WilliamWCohen.“Subject-driventext-to-imagegenerationviaapprenticeshiplearning”.In:
AdvancesinNeuralInformationProcessingSystems36(2024).
[28] Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David E
Jacobs,BharathHariharan,YaelPritch,NealWadhwa,KfirAberman,etal.“Realfill:Reference-
driven generation for authentic image completion”. In: arXiv preprint arXiv:2309.16668
(2023).
[29] NatanielRuiz,YuanzhenLi,VarunJampani,WeiWei,TingboHou,YaelPritch,NealWad-
hwa, Michael Rubinstein, and Kfir Aberman. “Hyperdreambooth: Hypernetworks for fast
personalizationoftext-to-imagemodels”.In:arXivpreprintarXiv:2307.06949(2023).
[30] QixunWang,XuBai,HaofanWang,ZekuiQin,andAnthonyChen.“Instantid:Zero-shot
identity-preservinggenerationinseconds”.In:arXivpreprintarXiv:2401.07519(2024).
[31] MartinNicolasEveraert,MarcoBocchio,SamiArpa,SabineSüsstrunk,andRadhakrishna
Achanta.“Diffusioninstyle”.In:ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision.2023,pp.2251–2261.
[32] XunHuangandSergeBelongie.“Arbitrarystyletransferinreal-timewithadaptiveinstance
normalization”. In: Proceedings of the IEEE international conference on computer vision.
2017,pp.1501–1510.
[33] Lars Holdijk, Yuanqi Du, Ferry Hooft, Priyank Jaini, Berend Ensing, and Max Welling.
“StochasticOptimalControlforCollectiveVariableFreeSamplingofMolecularTransition
Paths”.In:AdvancesinNeuralInformationProcessingSystems36(2024).
[34] WendellHFlemingandRaymondWRishel.Deterministicandstochasticoptimalcontrol.
Vol.1.SpringerScience&BusinessMedia,2012.
[35] Pratik Chaudhari, Adam Oberman, Stanley Osher, Stefano Soatto, and Guillaume Carlier.
“Deep relaxation: partial differential equations for optimizing deep neural networks”. In:
ResearchintheMathematicalSciences5(2018),pp.1–30.
[36] EvangelosTheodorou,FreekStulp,JonasBuchli,andStefanSchaal.“Aniterativepathintegral
stochasticoptimalcontrolapproachforlearningrobotictasks”.In:IFACProceedingsVolumes
44.1(2011),pp.11594–11601.
[37] RenéCarmona,FrançoisDelarue,etal.Probabilistictheoryofmeanfieldgameswithapplica-
tionsI-II.Springer,2018.
[38] BrianD.O.Anderson.“Reverse-timediffusionequationmodels”.In:StochasticProcessesand
theirApplications12.3(1982),pp.313–326.
[39] AapoHyvärinenandPeterDayan.“Estimationofnon-normalizedstatisticalmodelsbyscore
matching.”In:JournalofMachineLearningResearch6.4(2005).
[40] Pascal Vincent. “A connection between score matching and denoising autoencoders”. In:
Neuralcomputation23.7(2011),pp.1661–1674.
[41] TeroKarras,MiikaAittala,TimoAila,andSamuliLaine.“Elucidatingthedesignspaceof
diffusion-basedgenerativemodels”.In:AdvancesinNeuralInformationProcessingSystems
35(2022),pp.26565–26577.
[42] QinshengZhangandYongxinChen.“FastSamplingofDiffusionModelswithExponential
Integrator”.In:TheEleventhInternationalConferenceonLearningRepresentations.2022.
[43] GowthamiSomepalli,AnubhavGupta,KamalGupta,ShramayPalta,MicahGoldblum,Jonas
Geiping,AbhinavShrivastava,andTomGoldstein.“MeasuringStyleSimilarityinDiffusion
Models”.In:arXivpreprintarXiv:2404.01292(2024).
[44] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.“Learningtransferablevisual
modelsfromnaturallanguagesupervision”.In:Internationalconferenceonmachinelearning.
PMLR.2021,pp.8748–8763.
[45] TamerBasar,SeanMeyn,andWilliamRPerkins.“Lecturenotesoncontrolsystemtheoryand
design”.In:arXivpreprintarXiv:2007.01367(2020).
[46] HJKappen.“Stochasticoptimalcontroltheory”.In:ICML,Helsinki,RadboundUniversity,
Nijmegen,Netherlands(2008).
[47] Tianrong Chen, Jiatao Gu, Laurent Dinh, Evangelos Theodorou, Joshua M Susskind, and
ShuangfeiZhai.“GenerativeModelingwithPhaseStochasticBridge”.In:TheTwelfthInterna-
tionalConferenceonLearningRepresentations.2023.
12[48] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,
andArmandJoulin.“Emergingpropertiesinself-supervisedvisiontransformers”.In:Pro-
ceedings of the IEEE/CVF international conference on computer vision. 2021, pp. 9650–
9660.
[49] LituRout,AdvaitParulekar,ConstantineCaramanis,andSanjayShakkottai.“ATheoretical
JustificationforImageInpaintingusingDenoisingDiffusionProbabilisticModels”.In:arXiv
preprintarXiv:2302.01217(2023).
[50] YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMattLe.“Flow
matchingforgenerativemodeling”.In:arXivpreprintarXiv:2210.02747(2022).
[51] Xingchao Liu, Chengyue Gong, et al. “Flow Straight and Fast: Learning to Generate and
TransferDatawithRectifiedFlow”.In:TheEleventhInternationalConferenceonLearning
Representations.2022.
[52] BradleyEfron.“Tweedie’sformulaandselectionbias”.In:JournaloftheAmericanStatistical
Association106.496(2011),pp.1602–1614.
[53] JonathanHo,AjayJain,andPieterAbbeel.“Denoisingdiffusionprobabilisticmodels”.In:
AdvancesinNeuralInformationProcessingSystems33(2020),pp.6840–6851.
[54] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,and
BenPoole.“Score-BasedGenerativeModelingthroughStochasticDifferentialEquations”.In:
InternationalConferenceonLearningRepresentations.2021.URL:https://openreview.
net/forum?id=PxTIG12RRHS.
[55] HuiwenChang,HanZhang,JarredBarber,AJMaschinot,JoséLezama,LuJiang,Ming-Hsuan
Yang,KevinMurphy,WilliamTFreeman,MichaelRubinstein,etal.“Muse:Text-to-image
generationviamaskedgenerativetransformers”.In:Proceedingsofthe40thInternational
ConferenceonMachineLearning.2023,pp.4055–4075.
13A AdditionalTheoreticalResults
Inthissection,werestatethepropositionsmorepreciselyandprovidetheirtechnicalproofs. First,
werecallstandardterminologiesfromoptimalcontrolliterature[34]. For0≤t ≤t≤t ≤1,the
0 N
costfunctionassociatedwiththecontrolleru(·)isdefinedbytheintegral:
V(u;x ,t
)=(cid:90) tN
ℓ(Xu,u,t)dt+h(cid:0) Xu (cid:1) , Xu =x , (4)
0 0 t tN t0 0
t0
whereℓ(···)denotesascalarvaluedfunctionofthestateXu,controlleru(·),andinstantaneoustime
t
t. ThevaluefunctionV∗(x ,t )isdefinedastheminimumvalueofV(u;x ,t )overthesetof
0 0 0 0
admissiblecontrollersU,i.e.,
V∗ =V∗(x ,t )=minV(u;x ,t
)=min(cid:90) tN
ℓ(Xu,u,t)dt+h(cid:0) Xu (cid:1) , Xu =x , (5)
0 0
u∈U
0 0
u∈U t0
t tN t0 0
whichsatisfiesaPartialDifferentialEquation(PDE)givenbelowinTheoremA.1.
TheoremA.1(HJBEquation,[34,45]). IfV∗hascontinuouspartialderivatives,thenitmustsatisfy
thefollowingPDE,alsoknownasHamilton-Jacobi-Bellman(HJB)equation:
∂V∗ (cid:104) (cid:105)
− (x,t)=min H(x,∇ V∗(x,t),u,t):=ℓ(x,u,t)+(∇ V∗(x,t))T v(x,u,t) .
∂t u∈U x x
Also, the Hamiltonian H(x,∇ V∗(x,t),u,t), optimal controller u∗(t) and the state trajectory
x
x∗(t)mustsatisfy
minH(x∗(t),∇ V∗(x∗(t),t),u,t)=H(x∗(t),∇ V∗(x∗(t),t),u∗(t),t).
x x
u∈U
A.1 Interpretingreverse-SDEasasolutiontooptimalcontrol
Forclarity,werestatetheproblemsetuphereanddescribethemainideasfrom§4inmoredetails.
Problemsetup: Wediscussastandardapproachtoderivetheoptimalcontrollerinaspecialcase
ofourcontrolproblem(2). Wesubstitutet←1−ttoaccountforthetimereversalinthereverse-
SDE (1). In this setup, Xu ∼ N (0,I ) and Xu ∼ p . We consider the following dynamic
0 d 1 data
withouttheBrownianmotion:
dXu =v(Xu,u,t)dt, Xu =x , (6)
t t t0 0
where0≤t ≤t≤t ≤1andv :Rd×Rd×[t ,t ]→Rd denotesthedriftfield. Theoptimal
0 N 0 N
controlleru∗ canbederivedbysolvingtheHamilton-Jacobi-Bellman(HJB)equation[34,45],see
AppendixAfordetails. Bycertaintyequivalence,thesameu∗appliestoamoregeneralcasewith
theBrownianmotion[47],where
dXu =v(Xu,u,t)dt+dW , Xu =x . (7)
t t t t0 0
Therefore,withoutlossofgenerality,weanalyzethereversedynamicintheabsenceoftheBrownian
motion,andemploythesamecontrollerinmoregeneralcaseswiththeBrownianmotion.
Below,weconsideradynamicalsystemwhosedriftfieldischosentominimizeatransienttrajec-
tory cost and a terminal cost (weighted by γ) that enforces “closeness” to reference content x .
1
PropositionA.2providesthestructureoftheoptimalcontrolinthelimitingsettingwhereγ →∞.
Furthermore, suppose we replace x with its conditional expectation (discussed in Remark A.3),
1
theresultingdynamic,interestingly,isthestandardreverse-SDEfortheOrstein-Uhlenbeck(OU)
diffusionprocess. Thisconnectionbetweenoptimalcontrol(moreprecisely,classicLinearQuadratic
Control)andthestandardreverse-SDEprovidesusapathtostudyotherdiffusionproblems(e.g.
personalization[7,12,11,13],imageeditingorinversion[18,15,16,17,49])throughthelensof
stochasticoptimalcontrol.
PropositionA.2(Linearoptimalcontrolwithquadraticcost[47]). Considerthecontrolproblem:
(cid:90) 1 1 γ
min ∥u(Xu,t)∥2dt+ ∥Xu−x ∥2,
u∈U t0 2 t 2 1 1 2
wheredXu =u(Xu,t)dt, Xu =x
t t t0 0
Then, in the limit when γ → ∞, the optimal controller is given by u∗ = x1−X tu , which yields
1−t
dX tu = x1 1− −X ttu dtforthedeterministiccaseanddX tu = x1 1− −X ttu dt+dW tforthestochasticcase.
14TheoptimalcontrollerfortheproblempresentedinPropositionA.2canbederivedusingestablished
techniquesfromcontroltheory[34,45,46];thespecificformoftheaboveresultfollowsfrom[47]
(but without their momentum term). The key steps in this derivation include: (1) computing the
Hamiltonian,(2)applyingtheminimumprincipletheoremtoderiveasetofdifferentialequations,
and(3)takingthelimitasγ → ∞. Thesethreestepsarefundamentalinderivingaclosed-form
solution. The final step is critical for satisfying hard terminal constraint and is essential for the
practicalimplementationofAlgorithm1andAlgorithm2,asdetailedin§4.
Forgenerativemodeling,thecontrolleddynamicsdescribedinPropositionA.2cannotbedirectly
applied. This limitation arises because the optimal control u∗ depends on the terminal state x ,
1
makingitnon-causalorreliantonfutureinformation. Inspiredbyrecentadvancementsinflow-based
generativemodels[50,51],wemaketheoptimalcontrollercausalbyreplacingtheterminalstate
with its conditional expectation given the current state, i.e., , i.e. x ← E[Xu|Xu = x ]. This
1 1 t t
modificationresultsinacontrolleddynamicthatcanbesimulatedtoproduceagenerativemodel
incorporatingprinciplesfromoptimalcontrol,aselaboratedinRemarkA.3.
Remark A.3 (Connections between diffusion-based generative modeling and stochastic optimal
control). Followingconditionaldiffusionmodelsandoptimaltransportpaths[50,51],whereXf =
t
tXf+(1−t)ϵ,thestatevariableXuisequalindistributiontoXf =(1−t)Xf+tϵ, ϵ∼N (0,I )
0 t 1−t 0 d
aftertimereversal. Now,weuseTweedie’sformula[52]tocomputetheposteriormean:
Xu t2
E[Xu|Xu]= t + ∇logp(Xu,1−t). (8)
1 t 1−t 1−t t
SubstitutingtheposteriormeaninthecontrolledreversedynamicofPropositionA.2,wearriveat
(E[Xu|Xu]−Xu)
dXu = 1 t t dt+dW
t (1−t) t
(cid:104) t t2 (cid:105)
= Xu+ ∇logp(Xu,1−t) dt+dW .
(1−t)2 t (1−t)2 t t
Weobservethattheaboveequationisstructurallythesameasreverse-SDEassociatedwithaforward
Orstein-Uhlenbeck(OU)diffusionprocess. Thisrelationbetweendiffusion-basedgenerativemodels
andoptimalcontrolisfurtherexploredintheAppendicesbelow.
Indeed,diffusionmodels[53,54,3,23,24]provideaneffectiveapproximationtotheterminalstate
ofadenoisingprocess. Thisapproximationhasbeenusedforavarietyofgenerativemodelingtasks.
Also, the terminal state can be approximated using Tweedie’s formula [52] with a learned score
function[53]1. Byutilizingthesepre-traineddiffusionmodels,wecanemploytheconnectionto
optimal control as discussed above to develop practically implementable generative models that
incorporatesterminalobjectivessuchasstyleandpersonalization. Consequently,thesubsequent
sectionsarededicatedtoderivingtheoptimalcontrollerassumingaknownterminalstate;wewill
approximatethisinpracticeusingTweedie’sformulaasabove.
A.2 Incorporatingpersonalizedstyleconstraintsthroughaterminalcost
Inthissection,wederivetheoptimalcontrollerwhenwehaveaccesstothereferencestylefeatures
y attheterminaltime(insteadofthecontentoftheimageencodedthroughx ).
1 1
PropositionA.4. SupposeA∈Rk×dbealinearstyleextractorthatoperatesontheterminalstate
Xu ∈Rd. Givenreferencestylefeaturesy ,considerthecontrolproblem:
1 1
(cid:90) 1 1 γ
min ∥u(Xu,t)∥2dt+ ∥AXu−y ∥2, (9)
u∈U t0 2 t 2 1 1 2
wheredXu =u(Xu,t)dt, Xu =x , (10)
t t t0 0
Then,inthelimitwhenγ →∞,theoptimalcontrolleru∗ =
(ATA)−1AT(y1−AX tu)
,whichyieldsthe
1−t
followingcontrolleddynamic:
(cid:0) ATA(cid:1)−1 AT (y −AXu)
dXu = 1 t dt. (11)
t 1−t
1Alternatively,whenthereverseprocessisdescribedbyaprobabilityflowODE,atrainedneuralnetwork
candirectlypredicttheterminalstate[20].
15Proof. We derive the closed-form solution of the optimal controller given a fixed terminal state
condition. Thisissimilarto[47],wherethereverseprocessisacceleratedusingmomentum(seealso
[46,45]forfurtherdetailsonthisapproach). Thedistinction,however,liesinthetreatmentofthe
terminalconstraint. Forcompleteness,weprovidefulldetailsoftheproofbelow.
To derive the closed-form solution2, recall from equation (5) that ℓ(x ,u ,t) = 1∥u ∥2 and the
t t 2 t
terminalcosth(x ) = γ ∥Ax −y ∥2. Letp represent∇ V∗(x,t)inTheoremA.1. Then,the
1 2 1 1 t x
Hamiltonianofthecontrolproblem(9)isgivenby
H(x ,p ,u ,t)=ℓ(x ,u ,t)+pTu
t t t t t t t
1
= ∥u ∥2+pTu .
2 t t t
SincetheminimizeroftheHamiltonianisu∗ =−p ,thevaluefunctionbecomes
t t
1
V∗ =minH(u ,p ,u ,t)=H(u ,p ,u∗,t)=− ∥p ∥2. (12)
ut t t t t t t 2 t
Now,weuseminimumprincipletheorem[45]toobtainthefollowingsetofdifferentialequations:
dx
t =∇ H(x ,p ,u∗,t)=−p ; (13)
dt p t t t t
dp
t =−∇ H(x ,p ,u∗,t)=0; (14)
dt x t t t
x =x ; (15)
t0 0
p =∇ h(x ,t )=γAT (Ax −y ). (16)
tN x tN N tN 1
Integratingbothsidesof(13),wehave
(cid:90) 1 (cid:90) 1
dx =− p dt=−p(1−t ), (17)
t t 0
t0 t0
wherethelastequalityisdueto(14),whichstatesthatp isaconstantindependentoftimet. This
t
impliesx =x −p(1−t ). From(16),weknowfort =1that
1 t0 0 N
p =γAT (Ax −y )
1 1 1
=γ(cid:0) ATA(x −p(1−t ))−ATy (cid:1)
0 0 1
=γATAx −γATAp (1−t )−γATy (18)
0 1 0 1
Rearranging(18)andsolvingforp ,weget
1
p =γ(cid:0) I+γATA(1−t )(cid:1)−1(cid:0) ATAx −ATy (cid:1)
1 0 0 1
=(cid:18)
I +ATA(1−t
)(cid:19)−1
(cid:0) ATAx −ATy (cid:1) =p (19)
γ 0 0 1
Passing(19)throughthelimitγ →∞,weget
(cid:0) ATA(cid:1)−1(cid:0) ATAx −ATy (cid:1)
0 1
lim p= . (20)
γ→∞ 1−t 0
Therefore,theoptimalcontrolbecomesu∗ = −p =
−(ATA)−1(ATAxt−ATy1)
,andtheresulting
t 1−t
dynamicalsystemisgivenby
(cid:0) ATA(cid:1)−1 AT (y −Ax )
1 t
dx = dt,
t 1−t
forthedeterministicprocessand
(cid:0) ATA(cid:1)−1 AT (y −Ax )
1 t
dx = dt+dW ,
t 1−t t
forthestochasticprocesswiththeBrownianmotion. Thiscompletesthestatementoftheproof.
2Withslightabuseofnotation,weusex todenoteXuandu todenoteu(Xu,t)inthedeterministiccase.
t t t t
16Implications: Theoptimalcontrollerdependsonthereferencestylefeaturesy attheterminaltime
1
(instead of the image content x as in Appendix A.1). The reverse dynamic can be simulated in
1
practice by using CSD [43] as a style feature extractor and replacing y with the extracted style
1
featuresfromtheexpectedterminalstateE[Xu|Xu],asdiscussedinRemarkA.3. Thismakesthe
1 t
controllerdriftcausalandnon-anticipatingfutureinformation.
A.3 Incorporatingpersonalizedstyleconstraintthroughmodulationandaterminalcost
Inthissection,westudyacontrolproblemwherethevelocityfieldisalinearcombinationofthe
stateandthecontrolvariable. Thisproblemisinterestingtostudybecauseofthefollowingreason.
Thereverse-SDEdynamicofthestandardOUprocesshasadriftfieldoftheform:
v(X ,t)=−X −2∇logp(X ,t).
t t t
ForaGaussianpriorX ∼N (0,I),thelawoftheOUprocesssatisfies∇logp(X ,t)=−X ,and
0 t t
thecorrespondingdriftfieldbecomesv(X ,t)=X . Ourgoalistomodulatethisdriftfieldusing
t t
acontrolleru(Xu,t). Theresultbelowprovidesthestructureoftheoptimalcontrol(againinthe
t
settingwheretheterminalobjectiveisknown;seeAppendixA1).
PropositionA.5. SupposeA∈Rk×dbealinearstyleextractorthatoperatesontheterminalstate
Xu ∈ Rd. Let p denote ∇ V∗(x,t) in HJB equation (A.1). Given reference style features y ,
1 t x 1
considerthecontrolproblem:
(cid:90) 1 1 γ
min ∥u(Xu,t)∥2dt+ ∥AXu−y ∥2, (21)
u∈U t0 2 t 2 1 1 2
wheredXu =[Xu+u(Xu,t)]dt, Xu =x , (22)
t t t t0 0
Then,theoptimalcontrollerbecomesu∗(t)=−p ,wheretheinstantaneousstateXu =x andp
t t t t
satisfythefollowing:
(cid:20) x (cid:21) (cid:20) x et− γAT (Ax −y )e1+t+ γAT (Ax −y )e1−t(cid:21)
t = 0 2 1 1 2 1 1 .
p γAT (Ax −y )e1−t
t 1 1
Proof. The proof of Proposition A.5 is similar to Proposition A.4. One key distinction is the
set of differential equations obtained using minimum principle theorem [45]. We begin with the
Hamiltonian:
H(x ,p ,u ,t)=ℓ(x ,u ,t)+pT (u +x )
t t t t t t t t
1
= ∥u ∥2+pTu +pTx ,
2 t t t t t
which gives us the minimizer of the Hamiltonian u∗ = −p and its value function becomes
t t
V∗ = min H(u ,p ,u ,t) = H(u ,p ,u∗,t) = −1∥p ∥2+pTx . Bytheminimumprinciple
ut t t t t t t 2 t t t
theorem[45],
dx
x˙ := t =∇ H(x ,p ,u∗,t)=−p +x ; (23)
t dt p t t t t t
dp
p˙ := t =−∇ H(x ,p ,u∗,t)=−p ; (24)
t dt x t t t t
x =x ; (25)
t0 0
p =∇ h(x ,t )=γAT (Ax −y ). (26)
tN x tN N tN 1
Thisleadstoacoupledsystemofdifferentialequationswithboundaryconditionsasgivenbelow:
(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21)
x˙ 1 −1 x
t = t ;
p˙ 0 −1 p
t t
x =x ;
t0 0
p =γAT (Ax −y ).
1 1 1
17(cid:20) (cid:21)
x˙
This can be solved numerically using ODE solvers, see [34, 45] for details. Denote q˙ = t
t p˙
t
(cid:20) (cid:21)
1 −1
andM= . Weseekasolutionoftheformq(t)=qeλt. Ifq(t)isasolutionoftheabove
0 −1
problem,thenitmustsatisfythefollowingeigenvalueproblem:
qeλtλ=Mqeλt. (27)
Writingthecharacteristicpolynomialof(27),wegetdet(M−λI)=0,whichgivestheeigenvalues
λ={1,−1}. Substitutingtheseeigenvalues,wehave
(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21)
0 −1 q 2 −1 q
1 =0, 1 =0,
0 −2 q 0 0 q
2 2
whichgivestwofundamentalsolutions. Bycombiningthesetwo,weobtainthefinalsolution
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
x 1 1
t =ω et+ξ e−t,
p 0 2
t
where ω and ξ can be found using the boundary conditions. Since x = x and p =
0 0 1
γAT (Ax −y ), we get ω = x − γAT (Ax −y )e and ξ = γAT (Ax −y )e. Substitut-
1 1 0 2 1 1 2 1 1
ingthevaluesofωandξ,wearriveat
(cid:20) x (cid:21) (cid:20) x et− γAT (Ax −y )e1+t+ γAT (Ax −y )e1−t(cid:21)
t = 0 2 1 1 2 1 1 .
p γAT (Ax −y )e1−t
t 1 1
Thiscompletestheproofoftheproposition.
Summary: Though Appendices A.1-A.3, we have seen the connection between optimal control
anddiffusionbasedgenerationwithapersonalizedterminalconstraint. Thegeneralstrategyhas
beentoderivetheoptimalcontrollerwithknownterminalstate,andthenreplacetheterminalstate
in the controller with its estimate using Tweedie’s formula. While the controllers so far have an
explicitform,inpractice,thedatadistributionisnotGaussian,andthus,wedonothaveaclosed-form
expressionforthedriftofthecontroller.
This line of analysis, however, points to our method RB-Modulation. As discussed in §4, we
incorporateaconsistentstyledescriptorinourcontroller’sterminalcostandnumericallyevaluatethe
driftofthecontrollerateachreversetimestepeitherthroughbackpropagationthroughthescore
network,oranapproximationbasedonproximalgradientupdates.
B AdditionalExperimentalEvaluation
B.1 Implementationdetails
Baselines: WedemonstratetheapplicabilityofourmethodRB-ModulationwithStableCascade[24]
(releasedbeforeApril2024). Toourbestknowledge, RB-Modulationisthefirstframeworkthat
introducesnewcapabilitiestoStableCascadebyincorporatingSOCandAFAmodules.Sincethereare
noexistingtraining-freepersonalizationbaselinesdesignedforStableCascade,weseekalternatives
builtonothercomparablestate-of-the-artmodelssuchasSDXL[23]andMuse[55]3.
Amongalternatetraining-freebaselines,InstantStyle[13]doesnotdirectlyapplytoStableCascade
becauseitrequiresfeatureinjectionintospecificlayersofanIP-Adapter,whichisnotavailablefor
StableCascade. Similarly,StyleAligned[12]reliesonDDIMinversion,whichiscurrentlyapplicable
only to single-stage diffusion models. In contrast, StableCascade utilizes a two-stage diffusion
process, making the application of standard DDIM inversion [20] infeasible. We run the official
sourcecodeforInstantStyle4andStyleAligned5. Intheabsenceofastyledescription,weuse“image
3NotethatStableCascadeandSDXLhavecomparableperformanceinpromptalignmentwhereasStableCas-
cadeismoreefficientduetoahighlycompressedsemanticlatentspace[24].
4https://github.com/InstantStyle/InstantStyle
5https://github.com/google/style-aligned
18Algorithm2:Reference-BasedModulation(RB-Modulation)forlarge-scalegenerativemodels
Input: DiffusiontimestepsT,referencestyleimagez ,
0
styledescriptorΨ(·),scorenetworks(·,·;θ)
Tunableparameters: Stepsizeη,optimizationstepsM,proximalstrengthλ
Output: PersonalizedlatentXu
0
1 Initializex T ←N (0,I d)
2
Initializecontrolleru∈Rd
3 fort=T to1do
4 ComputeposteriormeanE[X 0u|X tu =x t]= √x α¯t
t
+ (1 √− α¯α¯ tt)s(x t,t;θ)
5 Initializeoptimizationvariablex 0 =0
6 form=1toM do
7 Computecontroller’scostL(x 0):=∥Ψ(z 0)−Ψ(x 0)∥2 2+λ∥x 0−E[X 0u|X tu =x t]∥2 2
8 Updateoptimizationvariablex 0 =x 0−η∇ x0L(x 0)
9 end
10 Computepreviousstatex t−1 =DDIM(x 0,x t)[20]
11 end
12 returnX 0u
Reference Style (cid:147)A dog wearing glasses(cid:148)
0 0.001 0.01 0.1 0.2
step size η
(cid:147)A running robot(cid:148)
0 1 2 3 4
Optimization steps M
Figure 6: Qualitative results of different tunable hyperparameters: Improved style-prompt
disentanglementareshownwhenincreasingtoourbestconfigurationsoptimizationstepsizeη =0.1
andoptimizationstepsM =3.
instyle”forDDIMinversioninStyleAligned. FollowingInstantStyle[13],wealsocomparewith
IP-Adapter. WeincludethequantitativecomparisoninTable2,andonlycomparequalitativelywith
strongerbaselinesinFigure3.
Forcompleteness,wealsocomparewithtraining-basedbaselines: StyleDrop[11]andZipLoRA[10].
SincetheofficialcodebaseforStyleDrop6andZipLoRA7arenotpubliclyavailable,weusethethird-
partyimplementationandfollowthetrainingdetailsinthecorrespondingpapers. Ittakes 5minutes
fortrainingStyleDropfor1000stepsand 20minutesfortrainingeachLoRAforZipLoRA.Wetrain
eachLoRAwithonlyonereferenceimageforbothcontentandstylestomakeafaircomparison
withothermethods. Similarly, wetrainStyleDropwithonlyonereferenceimage. Whenastyle
descriptionisnotprovided,wefollowtheoriginalpaper[11]anduse“ina[v*]style”instead.
Tunableparameters.Ourmethodintroducesonlytwohyper-parameters:stepsizeηandoptimization
stepsM inAlgorithm1. WeuseDDIMsamplingwithη =0.1andM =3foralltheexperiments.
6https://github.com/aim-uofa/StyleDrop-PyTorch
7https://github.com/mkshing/ziplora-pytorch
19Reference style StyleAligned InstantStyle Ours StyleAligned InstantStyle Ours
(a) (cid:147)A sofa in a infographic style(cid:148) (b) (cid:147)A sofa in a infographic style(cid:148)
Figure7: Impactofstyledescriptionsintheprompt: (a)Whenstyledescriptionsareprovided,all
methodsyieldbetterresults. (b)Withoutstyledescriptions(e.g.,hardforuserstodescribeintext),
alternativemethodscouldstruggletocapturetheintendedstyleinthereferenceimage. Ourmethod
offersconsistentstylizationevenwithoutexplicitstyledescriptions.
Content-stylecomposition. Theprompt-guidedcontent-stylecompositiontaskintroducesanew
layerofcomplexitybeyondstylization§6.1. Thistasknecessitatesthedisentanglementofthetext
prompt,referencestyleimage,andreferencecontentimagethroughadditionalconditioning. Such
complexityposessignificantchallengesforDDIMinversion[20]andattentioncachingmechanisms
[12]duetotheinherentdependenciesonmultiplereversepaths.
OurAFAmoduleeffectivelyaddressesthesechallenges. Itmanipulatestransformerlayerstoeasily
incorporatetheseadditionalconditions. Thecontentinformationisintegratedinamannersimilarto
thestyleinformation. Specifically,weuseapre-trainedViT-L/14modeltoextractcontentfeaturesin
theSOCframeworkandupdatethelatentembeddingsconcurrentlyviatheAFAmodule,usingan
additionalsetofkeysandvaluesillustratedinFigure2.
Furthermore,tobetterpreservetheidentityoftheforegroundcontent,weextractthedesiredcontent
usingLangSAM8 basedonthecontentprompt. Thisstepisoptionalbutoffersmoreusercontrol
whenmultiplesubjectsarepresentinthereferenceimage.
B.2 Implementationusinglarge-scalediffusionmodels
Theexactimplementationofourcontrolproblem(3)isgiveninAlgorithm1,whichfollowsfrom
ourtheoreticalinsights. Inpractice,ourcontrollerencountersachallengewhenthegenerativemodel
containsbillionsofparametersasinStableCascade[24]duetobackpropagationthroughthescore
network,asdiscussedin§4. Ourstrategytoovercomethispracticalchallengeinvolvesaproximal
gradientupdate,giveninLine7-8ofAlgorithm2. Toacceleratethesamplingprocess,weruna
fewsteps(M = 3)ofgradientdescentafterinitializingx = E[Xu|Xu =x ],resultinginonly
0 0 t t
twohyperparameterstotune: stepsizeηandthenumberofoptimizationstepsM. Further,sincethe
CSDmodelexpectsacleanimagetoextractstylefeatures,weapplythepreviewermodelavailable
in StableCascade on the terminal state before extracting style features. After obtaining the final
personalizedlatentusingourAlgorithm1andAlgorithm2,wefollowthedecodingprocessasper
theinferencepipelineoftheadoptedgenerativemodel.
B.3 Impactofhyperparametersoncontrollingstyleandcontentfeatures
Asdetailedin§4andtheablationstudyin§6.1,SOChelpsdisentanglethestyleandtheprompt
informationbyupdatingthedriftfieldinthestandardreverse-SDE.Westudytheimpactofthetwo
hyperparameters present in Algorithm 1 and Algorithm 2 that enables this disentanglement, as
showninFigure6. Wefoundbetterdisentanglementwhenthestepsizeη = 0.1andthenumber
of optimization steps M = 3. However, increasing the step size further results in style image
informationleakingintotheoutput(toprow). Additionally,addingmoreoptimizationstepsincreases
computationaloverheadwithoutyieldingmuchperformancegain(bottomrow).
B.4 Styledescriptionintextpromptsforbetterassimilationofuniquestyles
In addition to the quantitative analysis in §6.1, Figure 7 demonstrates that our method generates
consistentstylizedresultswithandwithoutthestyledescription. Incontrast,thealternativesfail
to accurately follow the prompt when the style description is absent. Although all results show
8https://github.com/luca-medeiros/lang-segment-anything
20Reference style StableCascade DirectConcat AFA only SOC only AFA + SOC
ImageReward 0.99 -1.63 0.17 -1.20 0.40
CLIP-T 0.27 0.23 0.27 0.22 0.27
DINO score 0.48 0.74 0.66 0.75 0.72
CLIP-I 0.50 0.88 0.70 0.73 0.72
ImageReward 1.49 0.06 1.39 1.11 1.43
CLIP-T 0.28 0.23 0.29 0.28 0.29
DINO score 0.55 0.80 0.68 0.70 0.65
CLIP-I 0.57 0.82 0.66 0.61 0.61
Figure 8: Comparison of different evaluation metrics: The StableCascade output is provided
forreferencebecauseitdoesn’tusethereferencestyleimage. Thehighestscoreforeachmetricis
markedboldwithunderscore. Wecomparefourmetrics: ImageRewardandCLIP-Tscoreforprompt
alignment,DINOandCLIP-Iscoreforstylealignment. Thepromptforthetoprowis“Acat”andfor
thebottomrowis“Apiano”.
noticeableimprovementwhenthestyledescriptionisprovided,itisoftenchallengingforusersto
describestylesinmanyreal-worldscenarios. WebelieveourearlyresultsbyRB-Modulationwill
pavethewayforinterestingfutureresearchalongthisdirection.
Wepresentadditionalqualitativeresultsonstylizationwith(Figure9)andwithout(Figure10)style
descriptionsusingStyleAligneddataset[12]. Ourresultsconsistentlyalignwiththereferencestyle
andtheprompt,whileothermethodsencounterseveralissues: (1)difficultyinfollowingprompt
guidance,(2)informationleakagefromthestylereferenceimage,and(3)failuretoachievereasonable
prompt/stylealignmentintheabsenceofstyledescriptions.
B.5 Challengesofevaluationmetricsinmeasuringstyleandcontentleakage
In§6,wediscussedthelimitationsofmetricsusedinpreviousworks[11,12,10],suchasDINO
[48]andCLIP-Iscore[44]. Toquantifytheselimitations,weuseresultsfromourablationstudy
shown in Figure 4. As illustrated in Figure 8, DINO and CLIP-I scores are not well-suited for
measuring style similarity in the presence of content leakage. This is because images with high
semanticcorrelationstothereferencestyleimageconsistentlyreceivehigherscores. Forinstance,in
thetoprow,althoughthelasttwocolumnsvisuallyalignmorecloselywiththeisometricillustration
stylesofthereferenceimage,theDirectConcatoutputfeaturingalighthousereceiveshigherscores.
ThemarginisparticularlypronouncedforCLIP-Iscore.
Asimilarobservationcanbemadeinthebottomrow,whereimagescontainingtrain-relatedobjects
receivehigherscoresregardlessoftheirstylisticsimilarity. Conversely,imageswithlesscontent
leakage(asseeninthelastcolumn)areassignedlowerscores. ThisindicatesthatDINOandCLIP-I
scores prioritize semantic content over stylistic fidelity, thus failing to accurately measure style
similarityinscenarioswherecontentleakageprevails.
Ontheotherhand,ourfinalmethod(lastcolumn),whichcombinesAFAandSOC,demonstrates
highscoresforbothpromptalignmentmetrics: ImageReward[5]andCLIP-T[44]. Thismethodalso
21showshigheruserpreference,asevidencedinTable1. Incontrast,theDirectConcatresultssuffer
frominformationleakageandpooralignmentwiththeprompt,resultinginsignificantlyloweror
evennegativerewardscores.
Intheablationstudy,ourprimaryfocusisonthedisentanglementofpromptsandreferencestyles.
The conventional metrics fail to accurately reflect true performance due to information leakage.
Consequently,weemphasizequalitativedemonstrationsandplacegreaterimportanceonuserstudy
results,asshowninTable1,similartopreviousapproaches[12,11].
B.6 Morequalitativeresultsonstylizationandcontent-stylecomposition
WealsoshowcaseresultsonconsistentstylegenerationusinguserdefinedpromptsinFigure11. Our
resultswithdifferentpromptsconsistentlyalignwiththestyleswhileintroducingvariousscenarios
followingtheprompts. Theothermethodsfacechallengeslikeinformationleakage(e.g.hikingboots
andthemonocular)andmonotonousscenes(e.g.InstantStyle). NotethattheoriginalStyleDrop
paper[11]hasmentioneditsdifficultywhentrainingwithoneimagewithoutdescription. Wekeep
theresultsforcompletenesseventhoughtheyarelesssatisfying. Besides,wealsodemonstratemore
qualitativeresultsforcontent-stylecompositioninFigure13.
B.7 Humanevaluationtodiscernhighlysubjectivenatureofstyle
Weconductauserstudywith155participantsviaAmazonMechanicalTurkusing100stylesfrom
the StyleAligned dataset [12]. The study requires no personally identifiable information of the
participants. Thereisnoriskincurredandnovulnerablepopulation. Thestandardguidelineshave
beenfollowedwhileconductingtheuserstudy.
Wefirstprovideparticipantswithinstructionstofamiliarizethemwiththerelevantterminologies.
Foreachstyle,werandomlysamplethreeoutputsusingthreedifferentprompts. Participantsseetwo
rowsofmodeloutputsinrandomorder(3imagesperrow)andanswer3questions,asillustratedin
Figure12.
1. Inwhichrowbelow,theimagesalignbetterwiththereferencestyleimage?
2. In which row below, the images align better with the reference text prompt above each
image?
3. Inwhichrowbelow,theimagesoverallalignbetterwiththereferencestyleimageANDthe
textpromptaboveeachimageANDwithhighquality?
Foreachquestion,participantschooseoneofthreeoptions. Wecollect8responsesforeachquestion,
witheachquestioncomparingourmethodagainstoneofthealternatives. Intotal,wegathered7,200
responses.
B.8 Failurecasesoftraining-freestylizationusingRB-Modulation
InFigure14,weillustratestylizationofdifferentlettersusingasinglereferencestyleimage.Although
ourmethodcapturestheintendedstyleandgeneratespromptedletters,wenoticethatthereisan
inherenttendencytogenerateupper-caseletters(Figure14(a)),eventhoughitispromptedtogenerate
lower-caseletters. Uponfurtherinvestigation,weobservedthatthisissuestemsfromtheunderlying
generativemodelStableCascade,asshowninFigure14(b). Thishighlightsacruciallimitationof
ourmethod. Asatraining-freemethod,RB-Modulationsharesaconcernwithothertraining-free
methods[13,12,14]thattheperformanceisinfluencedbytheoriginalgenerativeprior.
C Broaderimpactstatement
Socialimpact:Imagestylizationandcontent-stylecompositionbasedondiffusionmodelspotentially
havebothpositiveandnegativesocialimpact. Thistechnologyprovidesaneasy-to-usetooltothe
generalpublicforimagegenerationwhichcanhelpvisualizetheirartisticideas. Ontheotherhand,
ourworkonstylizationandcontent-stylecompositionposesariskofgeneratingartsthatclosely
mimic or infringe upon existing copyrighted material, leading to legal and ethical issues. More
22broadly,ourmethodinheritstherisksfromT2Imodelswhicharecapableofgeneratingfakecontents
thatcanbemisusedbymalicioususers.
Safeguards: We build on StableCascade [24], which has a mechanism to filter offensive image
generations. SinceourmethodRB-Modulationbuildsonthispre-trainedgenerativemodel,weinherit
thesesafeguards.
23Reference style Ours InstantStyle StyleAligned StyleDrop
(cid:147)An airplane in watercolor painting style(cid:148)
(cid:147)A bowl of cornflakes in 3d rendering style(cid:148)
(cid:147)An elephant in wooden sculpture style(cid:148)
(cid:147)The letter A in abstract rainbow-colored flowing smoke wave design(cid:148)
(cid:147)A vintage camera in retro hipster style(cid:148)
(cid:147)A milkshake in 1950s dinner art style(cid:148)
(cid:147)A train in cafe logo style(cid:148)
Figure9:Additionalqualitativeresultsforstylizationwithstyledescription:Whilethealternative
methodsfacechallengeslikefollowingtheprompts(e.g.,multipleairplanesinsteadofanairplane)
andinformationleakage(e.g.,thecloudsonthecornflakebowlandtheguitarinthemilkshakeimage),
ourmethoddemonstratesstrongperformanceonbothpromptandstylealignment. Styledescription
isinblue.
24Reference style Ours InstantStyle StyleAligned StyleDrop
(cid:147)A cat(cid:148)
(cid:147)A skyscraper(cid:148)
(cid:147)A leopard(cid:148)
(cid:147)A drum(cid:148)
(cid:147)A ladybug(cid:148)
(cid:147)A fireman(cid:148)
(cid:147)A winter evening by the fire(cid:148)
Figure10: Additionalqualitativeresultsforstylizationwithoutstyledescription: StyleAligned
andStyleDropshowsevereperformancedropafterremovingthestyledescriptions(e.g.,seefireman
andcatimages). InstantStyleresultsshowmoreinformationleakage(e.g., thepinkladybugand
leopard),whereasnoobviousperformancedropisobservedinourresults.
25Reference style Reference style
Ours InstantStyle StyleAligned StyleDrop Ours InstantStyle StyleAligned StyleDrop
(cid:147)A man reading a book in the park(cid:148)
(cid:147)A dog running in the park(cid:148)
(cid:147)A woman reading in the park(cid:148)
(cid:147)A soaring dragon(cid:148)
Figure11: Additionalqualitativeresultsforconsistentstylizationforuserdefinedprompts:
With no style description, our results demonstrate more diversity while following the styles and
prompts. InstantStyleresultsshowmonotonousscenesandStyleAlignedresultssufferfromsevere
informationleakage. WereportStyleDropresultsforcompletenessanditisknowntoperformworse
withnostyledescriptionandsingletrainingimage[11].
26Figure12: Userstudyinterface: Threerandomlysampledoutputsareshownforeachmethodgiven
astylereferenceimage,formingtworowsofimages. Theusersareaskedtoanswerthreequestions
on(1)stylealignment(2)promptalignmentand(3)overallalignmentandquality.
27Ref. content Reference styles
IP-Adapter
InstantStyle
Ours
Ref. content Reference styles
IP-Adapter
InstantStyle
Ours
Figure13: Additionalqualitativeresultsforcontent-stylecomposition: Ourresultsshowbetter
promptandstylealignmentwhilepreservingreferencecontentwithoutleakingcontentsfromthe
referencestyleimages(e.g.backgroundofthefirstcolumnandfruitsinthelastcolumn,). Unlike
compared baselines, our method is not restricted to a fixed pose of the reference content image,
illustratingsamplediversity.
28Reference style
(cid:147)r(cid:148) (cid:147)b(cid:148) (cid:147)m(cid:148) (cid:147)o(cid:148) (cid:147)d(cid:148) (cid:147)u(cid:148) (cid:147)l(cid:148) (cid:147)a(cid:148) (cid:147)t(cid:148) (cid:147)i(cid:148) (cid:147)o(cid:148) (cid:147)n(cid:148)
Prompt: (cid:147)A lower-case letter {}(cid:148)
Figure 14: Failure cases for stylization: The top row shows the results of our method, RB-
Modulation, whilethebottomrowdisplaystheresultsofthebackbone, StableCascade. Notably,
thestylizedimagesdonotadheretotheprompt,“lower-caseletter”. Thishighlightsthelimitations
imposed by the pre-trained generative priors on the capabilities of training-free personalization
models(toprow).
29
sruO
edacsaCelbatS