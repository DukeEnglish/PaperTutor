Analysis of Multiscale Reinforcement Q-Learning Algorithms for
Mean Field Control Games
Andrea Angiuli∗ Jean-Pierre Fouque† Mathieu Lauri`ere‡ Mengrui Zhang§
May 28, 2024
Abstract
Mean Field Control Games (MFCG), introduced in [Angiuli et al., 2022a], represent
competitive games between a large number of large collaborative groups of agents in
theinfinitelimitofnumberandsizeofgroups. Inthispaper,weprovetheconvergence
of a three-timescale Reinforcement Q-Learning (RL) algorithm to solve MFCG in a
model-freeapproachfromthepointofviewofrepresentativeagents. Ouranalysisuses
a Q-table for finite state and action spaces updated at each discrete time-step over
an infinite horizon. In [Angiuli et al., 2023], we proved convergence of two-timescale
algorithms for MFG and MFC separately highlighting the need to follow multiple
population distributions in the MFC case. Here, we integrate this feature for MFCG
aswellasthreeratesofupdatedecreasingtozerointheproperratios. Ourtechnique
of proof uses a generalization to three timescales of the two-timescale analysis in
[Borkar, 1997]. We give a simple example satisfying the various hypothesis made in
the proof of convergence and illustrating the performance of the algorithm.
1 Introduction
1.1 Background
Reinforcement learning (RL) is a set of methods to solve complex problems without knowing the full
knowledge of the model. We refer e.g. to [Sutton and Barto, 2018] for more background on this topic.
While RL is traditionally focused on solving discrete time optimal control problems, also called Markov
decision processes (MDP), the topic has recently attracted the interest of the stochastic optimal control
community and several connections with continuous time optimal control methods have been made, see
e.g. [Wang et al., 2020, Wang and Zhou, 2020]. Besides optimal control problem, RL methods are also used
to solve games. However, games with a large number of players are challenging for traditional methods,
including multi-agent reinforcement learning methods. In order to scale up in terms of number of agents,
mean field games (MFGs) have been introduced by [Lasry and Lions, 2007] and [Huang et al., 2006]. Such
games provide approximate Nash equilibria for finite-player games by passing to the limit and studying
the interactions between a representative player and a mean field representing the rest of the population.
RL methods for MFGs have been developed, see e.g. [Subramanian and Mahajan, 2019, Guo et al., 2019,
Elie et al., 2020, Cui and Koeppl, 2021] among many others. While MFGs typically use the notion of
Nash equilibria, which corresponds to non-cooperative players, one can also consider an infinite popula-
tion of cooperative players, which leads to the notion of solution of social optimum or mean field con-
trol (MFC). MFC problems have also been studied from the RL viewpoint, see e.g. [Carmona et al., 2019,
Gu et al., 2021, Motte and Pham, 2019, Carmona et al., 2023, Frikha et al., 2023]. We refer the interested
reader to [Bensoussan et al., 2013] and [Carmona and Delarue, 2018] for more details on the theoretical
background on MFGs and MFC problems, and to [Lauri`ere et al., 2022] for a survey on learning in MFGs.
AlthoughtheclassicalsettingofMFGsandMFCproblemsefficientlyaddressesthequestionofscalability
intermsofpopulationsize,thesetwoclassesofproblemsfocusonextremesituationswherealltheagentsare
either non-cooperative or cooperative. However, many situations involve a combination of cooperation and
competition. For example, in some situations, there is a competition between several large coalitions. Such
problems have been studied in the framework of mean-field type games [Tembine, 2017]. When the number
ofcoalitionsisalsolarge,theproblemcanbeapproximatedusingtheframeworkofmeanfieldcontrolgames
(MFCG) introduced in [Angiuli et al., 2022a]. The main goal of the present paper is to provide a proof of
convergence for the RL algorithm proposed in [Angiuli et al., 2022a] to solve MFCGs.
This algorithm builds on several previous works. In [Angiuli et al., 2022b], the authors proposed a
unified RL algorithm to solve MFG and MFC problems in the context of finite state and action spaces, and
in discrete infinite time horizon. The algorithm depends on the ratio of the two learning rates, one for the
Q-function and the other for the distribution of the population, leading to a two timescale algorithm. In
[Angiuli et al., 2023],weprovedtheconvergenceofthisalgorithmusingageneralizationofthetwo-timescale
∗PrimeMachineLearningTeam,Amazon. 320WestlakeAveN,SEA83,Seattle,WA,98109(E-mail: aangiuli@amazon.com).
Theworkpresentedheredoesnotrelatetothisauthor’spositionatAmazon.
†Department of Statistics and Applied Probability, South Hall, University of California, Santa Barbara, CA 93106, USA
(E-mail: fouque@pstat.ucsb.edu). WorksupportedbyNSFgrantDMS-1953035.
‡Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning; NYU-ECNU Institute of Mathematical
Sciences at NYU Shanghai; NYU Shanghai, 567 West Yangsi Road, Shanghai, 200126, People’s Republic of China (E-mail:
mathieu.lauriere@nyu.edu).
§Department of Statistics and Applied Probability, South Hall, University of California, Santa Barbara, CA 93106, USA
(E-mail: mengrui@umail.ucsb.edu).
1
4202
yaM
72
]CO.htam[
1v71071.5042:viXraapproach of [Borkar, 1997]. This careful mathematical analysis revealed that the MFC case requires the
updating of one population distribution for each state-action point which led us to consider two different
algorithm, one for MFG and one for MFC. On the other hand, [Angiuli et al., 2022a] revealed that a single
algorithm with three timescales could handle MFCGs, which are a mix of MFG and MFC. They naturally
model competitive games between a large number of large collaborative groups of agents. In the limit of the
number of groups and the sizes of groups going to infinity, we showed in [Angiuli et al., 2022a] that MFCG
providesanapproximationofthecorrespondingfiniteplayergame. Theunifiedthree-timescaleRLalgorithm
thatweanalyseinthispaperupdatestheQ-functionofarepresentativeagentlearningtheoptimalpolicyat
equilibrium, the global population distribution, and one local population distribution for each state-action
point. The local populations are updated faster the Q-function which in turns is updated faster than the
global population.
1.2 Structure of the Paper
InSection2,werecalltheclassicalQ-learningsetupandweintroducethenotationsneededfortheMeanField
ControlGame(MFCG)problempresentedinSection2.2. InSection3, wepresentthealgorithmsstudiedin
the paper. We start in Section 3.1 with the full asynchronous algorithm with stochastic approximation and
explainthatitismodel freefromthepointofviewofrepresentativeagents,oneforeachpopulationinvolved
inthesystem. Wealsohighlightthethreetimescalesinvolved,onefastforthelocalpopulationdistributions
(MFCaspect),oneslowfortheglobalpopulation(MFGaspect),andoneinbetweenfortheupdateoftheQ-
tableusedtochoosethepolicy. InSection3.2,wesimplifythealgorithmbyallowingtoupdatesynchronously
all the entries of the Q-table at each time step. We further simplify it in Section 3.3 by assuming that
expectations are known without stochastic approximation leading to a deterministic system of iterations. In
Section 4, we derive the convergence of our algorithms, starting with ideal deterministic iteration scheme,
thengeneralizedtoincludesuccessivelystochasticapproximationandasynchronousfeatures. Theproofsare
basedongeneralizationstothreetimescalestheresultsof[Borkar, 1997]. WeendinSection5withnumerical
illustrations on a very simple example with two states and two actions.
2 Model and MFCG Formulation
2.1 Model and Notations
First, we present classical Q-learning in a nutshell.
ClassicalQ-learning. ClassicalRLaimsatsolvingaMarkovDecisionProcess(MDP)usingthefollowing
setting. Ateachdiscretetimen,theagentobservesherstateX inafinitestatespaceX ={x ,...,x },
n 0 |X|−1
and, based on it, chooses an action A in a finite action space A={a ,...,a }. Then, the environment
n 0 |A|−1
evolves and provides the agent with a new state X and reports a reward r . The goal of the agent
n+1 n+1
is to find the optimal policy π such that it assigns to each state the optimal probability of actions in order
to maximize the expected cumulative reward. The problem can be recast as the problem of learning the
optimal state-action value function, also called Q-function: Qπ(x,a) represents the expected cumulative
discount rewards when starting at state x, using an action a, and then following policy π. Mathematically,
(cid:34) ∞ (cid:35)
(cid:88)
Qπ(x,a)=E γnr |X =x,A =a ,
n+1 0 0
n=0
where r =r(X ,A ) is the instantaneous reward, γ ∈(0,1) is a discounting factor, X is distributed
n+1 n n n+1
according to a transition probability which depends on X and the action drawn from π(X ). The goal is
n n
to compute the optimal Q-function defined as:
Q∗(x,a)=maxQπ(x,a),
π
which in turns gives the optimal control α∗(x):=argmax Q∗(x,a) if unique.
a
To this end, the Q-learning method was introduced by [Watkins, 1989]. The basic idea is to iteratively
sample an action A ∼π(X ) according to a behavior policy π, observe the induced state X and reward
n n n+1
r , and then update the Q-table according to the formula:
n+1
(cid:20) (cid:21)
Q(X ,A )←Q(X ,A )+ρ r +γmaxQ(X ,a′)−Q(X ,A )
n n n n n+1 n+1 n n
a′∈A
where ρ∈(0,1) is a learning rate.
Notethatinourcase,tobeconsistentwithmostoftheMFGandMFCliterature,wewillminimizecosts
instead of maximizing rewards.
Notations. The following notations will be used in the context of MFCG studied in this paper.
AsinthecaseofclassicalRLwedenotebyX ={x ,...,x }andA={a ,...,a }thefinitestate
0 |X|−1 0 |A|−1
andactionspaces. Wedenotebyδtheindicatorfunction,i.e.,foranyx∈X,δ(x)=[1 (x),...,1 (x)],
x0 x|X|−1
whichisavectorfullof0exceptatthecoordinatecorrespondingtox. Let∆|X| bethesimplexofprobability
measures on X. Let p : X ×X ×A×∆|X|×∆|X| →∆|X| be a transition kernel which depends on both a
global distribution µ and local distribution µ′. We also interpret the kernel p as a function
p:X ×X ×A×∆|X|×∆|X| →[0,1], (x,x′,a,µ,µ′)(cid:55)→p(x′|x,a,µ,µ′),
2whichprovidestheprobabilitythattheprocessjumpstothestatesx′ giventhecurrentstateisx,theaction
a is taken, the global distribution is µ and the local distribution is µ′. A policy π ∈ Π is a collection of
probability distributions {π(x) = π(·|x),x ∈ X} on the set of actions A, that is π : X → ∆|A|. We denote
by Pπ,µ,µ′ the transition kernel according to the global distribution µ and local distribution µ′ on X, and
the policy π ∈Π, defined for any distribution µ˜∈∆|X| by:
µ˜Pπ,µ,µ′ (x)= (cid:88) µ˜(x′)(cid:88) π(a|x′)p(x|x′,a,µ,µ′), x∈X. (1)
x′∈X a∈A
We denote by µπ,µ the asymptotic local distribution of the controlled process following the strategy π when
the global distribution is µ. We assume ergodicity of such processes so that µπ,µ exists and is unique.
Let f: X ×A×∆|X|×∆|X| →R be a running cost function. We interpret f(x,a,µ,µ′) as the one-step
cost, at any given time step, incurred to a representative agent who is at state x and uses action a while the
population distributions are µ and µ′. For a policy π we denote
(cid:88)
f(x,π,µ,µ′)= π(a|x)f(x,a,µ,µ′)
a∈A
Based on these notations, we present the notion of Mean Field Control Game (MFCG) introduced in
[Angiuli et al., 2022a].
2.2 MFCG Definition
Usingthenotationsintroducedintheprevioussection,givenarunningcostfunctionf,adiscountrateγ <1,
andaninitialdistributionµ ∈∆|X|,weconsiderthefollowinginfinite horizon asymptotic formulation
0
of a mean field control game problem:
Definition 2.1. An MFCG problem consists in finding a policy πˆ ∈ Π and a distribution µˆ ∈ ∆|X| such
that:
1. (best response) πˆ is the minimizer of the value function
(cid:34) ∞ (cid:35)
(cid:88)
π (cid:55)→Vπ :=E γnf(Xπ,µˆ,π(Xπ,µˆ),µˆ,µπ,µˆ) ,
µˆ n n
n=0
with µπ,µˆ =lim L(Xπ,µˆ) where the process (Xπ,µˆ) follows the dynamics
n→∞ n n n≥0

Xπ,µˆ ∼µ
 0 0
P(Xπ,µˆ =x′|Xπ,µˆ =x,A =a,µ=µˆ,µ′ =µπ,µˆ)=p(x′|x,a,µˆ,µπ,µˆ)
n+1 n n
A ∼π(·|Xπ,µ) independently at each time n≥0.
n n
2. (consistency) µˆ =lim L(Xπˆ,µˆ)=µπˆ,µˆ.
n→∞ n
Inordertomakesenseoftheaboveproblemstatementwehavetorestricttopoliciesπ :X →∆|A| which
are such that for any µ the controlled process Xπ,µ has a limiting distribution, i.e. lim L(Xπ,µ) exists.
n n→∞ n
For a finite state Markov chain this is the case if (Xπ,µ) is irreducible and aperiodic. We therefore assume
n n
that the strategy πˆ is the minimizer over all strategies such that (Xπ,µ) is irreducible and aperiodic for all
n n
µ.
2.2.1 Q-learning for MFCG
We adapt the Q-learning concepts to the MFCG problem at hand. Focusing on the problem faced by a
representative agent, we note that the local distribution is not fixed and depends on the policy itself. Thus,
we have to adapt the classical Q-learning in the spirit of [Angiuli et al., 2022b]. For an admissible policy
π ∈Π and a pair (x,a)∈X ×A, we define the new control π˜(x,a) by
(cid:40)
δ if x′ =x,
π˜(x,a)(x′):= a (2)
π(x′) for x′ ̸=x.
Given a global distribution µ and a policy π, the Q-function for our problem is given by:
Qπ(x,a)=f(x,a,µ,µπ˜(x,a),µ)+E(cid:34) (cid:88)∞ γnf(cid:16) Xπ,µ,π(Xπ,µ),µ,µπ˜(x,a),µ(cid:17)(cid:12)
(cid:12)Xπ,µ =x,A
=a(cid:35)
,
µ n n (cid:12) 0 0
n=1
where µπ˜(x,a),µ isthelimitinglocaldistributionrelativetothepolicyπ˜(x,a) andwhentheglobal distribution
is µ.
The optimal function in the sense of minimizing cost is given by
Q∗(x,a):=minQπ(x,a).
µ µ
π
Note that the minimizing strategy π∗ may depend on the global measure µ.
AsimplegeneralizationofTheorem2inAppendixCin[Angiuli et al., 2022b],givesthefollowingBellman
equation satisfied by Q∗ for a fixed µ:
µ
Q∗(x,a)=f(x,a,µ,µπ(cid:102)∗(x,a),µ)+γ(cid:88) p(cid:16) x′|x,a,µ,µπ(cid:102)∗(x,a),µ(cid:17)
minQ∗(x′,a′), (3)
µ µ
a′
x′
3whereµπ(cid:102)∗(x,a),µisthelimitinglocaldistributionrelativetothepolicyπ(cid:102)∗(x,a)
andwhentheglobaldistribution
is µ. Note that the advantage of introducing one local distribution for each state-action point (x,a) is that
the Q-table remains a function of state and action.
Finally, we introduce the specific Bellman operator B given by
(cid:88)
(B Q)(x,a):=f(x,a,µ,µ′)+γ p(x′|x,a,µ,µ′)minQ(x′,a′), (4)
µ,µ′
a′
x′∈X
for two distributions µ and µ′, so that (3) reads Q∗(x,a) = B Q∗(x,a), or, in a lighter notation,
µ µ,µπ(cid:103)∗(x,a),µ µ
Q∗ =B Q∗ with µ∗ =µπ(cid:102)∗(x,a),µ where we have dropped the (x,a)-dependence.
µ µ,µ∗ µ
3 Algorithms and Multiscale Learning Rates
First, in Section 3.1, we present our full three-timescale algorithm which updates the Q-table and the
population distributions in an asynchronous way (one (x,a) at a time along one path of a multidimensional
controlledprocess)andusesastochasticapproximation. Then,inSection3.2,weconsiderasimplifiedversion
of the full algorithm which updates the Q-table and the population distributions synchronously (all (x,a)
at a time using sampling from the law of the controlled process) and still using stochastic approximation.
Finally, in Section 3.3, we replace the stochastic approximation by a perfect knowledge of the expectations
involved in the time iterations, leading to a three-timescale ideal deterministic algorithm.
Our proof of convergence in Section 4, will proceed the other way around by first proving convergence
of the ideal deterministic algorithm by using a three-timescale generalization of two-timescale results in
[Borkar, 1997], andthensuccessivelyrestorethestochasticapproximationandtheasynchronousfeaturesby
controlling the corresponding errors.
3.1 Full Algorithm
We present an improved version of the three-timescale Algorithm 1 introduced in [Angiuli et al., 2022a].
The difference is that it considers several population distributions and paths of multidimensional control
processes which are essential features in the derivation of convergence results, the focus of this paper.
Algorithm1belowshowsthepseudo-codeforourthree-timescaleMFCGQ-learningalgorithm. Itinvolves
three learning rates that we choose of the following form
1 1 1
ρµ = ρQ = ρµ˜ =
n (1+n)ωµ n,x,a (1+ν(x,a,n))ωQ n,x,a (1+ν(x,a,n))ωµ˜
where ωµ, ωQ and ωµ˜ are parameters such that 1 < ωµ˜ < ωQ < ωµ < 1 , and ν(x,a,n) is the number of
times a controlled process (X,A) visits (x,a) up t2 o time n, that is, ν(x,a,n)=(cid:80)n 1 .
m=0 {(Xm,Am)=(x,a)}
Algorithm 1 Three-timescale Mean Field Control Game Q-learning Algorithm
Require: N : number of steps; ϕ: parameter for soft-min policy; (ρQ ) : learning rates for the
steps n,x,a n,x,a
value function; (ρµ) : learning rate for the global distribution; (ρµ˜ ) : learning rates for the local
n n n,x,a n,x,a
distribution.
1: Initialization: Q 0(x,a)=0 for all (x,a)∈X ×A, µ 0 =µ( 0x,a) =[ |X1 |,..., |X1 |]
2: Observe X 0 ∼µ 0, X 0(x,a) ∼µ( 0x,a)
3: Update distributions: µ 0 =δ(X 0) , µ( 0x,a) =δ(X 0(x,a))
4: for n=0,1,2,...,N steps−1 do
5: Choose action A n ∼ soft-min ϕQ n(X n,·) and observe state X n+1 ∼ p(·|X n,A n,µ n,µ n(Xn,An)) and
cost f =f(X ,A ,µ ,µ(Xn,An)) provided by the environment
n+1 n n n n
6: Choose action Choose A( nx,a) = a if X n(x,a) = x, otherwise A( nx,a) ∼ soft-min ϕQ n(X n(x,a),·) and
observe state X(x,a) ∼p(·|X(x,a),A(x,a),µ ,µ(x,a))
n+1 n n n n
7: Update local distributions: µ( nx +,a 1) =µ( nx,a)+ρµ n˜
,Xn(x,a),A(
nx,a)(δ(X n(x +,a 1))−µ( nx,a))
8: Update global distribution: µ n+1 =µ n+ρµ n(δ(X n+1)−µ n)
9: if X n(Xn,An) =X n then
10: Update value function: Q n+1(x,a)=Q n(x,a) for all (x,a)̸=(X n,A n), and
Q (X ,A )=Q (X ,A )+ρQ [f +γ minQ (X ,a′)−Q (X ,A )]
n+1 n n n n n n,Xn,An n+1
a′∈A
n n+1 n n n
11: end if
12: end for
13: Return (µ Nsteps,µ( Nx s, ta e) ps,Q Nsteps)
Our algorithm uses the transition distribution p and the cost function f (line 5), only through samples
and evaluations respectively. In other words, the algorithm is model-free from the point of view of the
representative agent. Since it only uses samples and not expected values, such updates are sometimes
referred to as sample-based updates. Furthermore, at time-step n in line 10, the value function Q differs
n+1
from Q at only one state-action pair, so the algorithm is asynchronous.
n
Note that the algorithm differentiates the local population distributions µ(x,a) updated on line 7 from
n
the global population distribution µ updated on line 8, by the choice of the learning rate parameters ωµ˜
n
4and ωµ satisfying ωµ˜ <ωQ <ωµ. In other words, the local distribution are updated faster than the Q-table
providing the control at each time (MFC aspect), while the global distribution is update slower than the
Q-table (in the spirit of MFG).
Observethatthechoiceofactionsonlines5and6usesasoft-min regularizedversionofargminneeded
ϕ
in the proof of convergence in Section 4. We recall that for a positive parameter ϕ,
(cid:32) (cid:33)
e−ϕzi
soft-min (z)= for z ∈R|A|.
ϕ (cid:80) e−ϕzj
j
i=1,2,...,|A|
3.2 Synchronous Algorithm with Stochastic Approximation
In this section we modify Algorithm 1 in the a synchronous setting with stochastic approximation. This is
an intermediate step between the practical setting (asynchronous and sample-based updates), and the ideal
setting (synchronous and expectation-based updates). Let us assume that for any (x,a,µ,µ˜), the learner
can know the value f(x,a,µ,µ˜). Furthermore, the learner can sample realizations of the random variables
′
X ∼p(·|x,π,µ,µ˜),
x,π,µ,µ˜
where the action is sampled from a policy π. Typically the policy π will be soft-min Q (X ) or π˜(x,a)
ϕ n n
defined by
(cid:40)
δ if x′ =x,
π˜(x,a)(x′):= a (5)
soft-min Q(X(x,a)) for x′ ̸=x.
ϕ n
q q
Then,thelearnerhasaccesstorealizationsofthefollowingrandomvariablesT (x,a)andP (ν)taking
µ,Q,µ˜ x,π
values respectively in R and ∆|X|:
 Tq µ,Q,µ˜(x,a)=f(x,a,µ,µ˜)+γmin a′Q(X x′ ,a,µ,µ˜,a′)−Q(x,a)
(Pq (ν)(x′)) =(cid:16) 1 −ν(x′)(cid:17) .
 x,π x′∈X {X′ =x′}
x,π,µ,µ˜ x′∈X
Observe that
E[Tq (x,a)]=(cid:88) p(x′′|x,a,µ,µ˜)(cid:104) f(x,a,µ,µ˜)+γminQ(x′′,a′)−Q(x,a)(cid:105)
=:T (µ,Q,µ˜)(x,a), (6)
µ,Q,µ˜ 3
a′
x′′
which defines the operator T . Likewise, we have
3
E[Pq (ν)(x′)]=(cid:88) p(x′′|x,π,µ,µ˜)(1 −ν(x′))=p(x′|x,π,µ,µ˜)−ν(x′),
x,π {x′′=x′}
x′′
so that
E[Pq (µ)(x′)]=(cid:88) µ(x)(cid:88) p(x′′|x,soft-min Q(x),µ,µ˜)(1 −µ(x′))
X,soft-minϕQ(X) ϕ {x′′=x′}
x x′′
=:P (µ,Q,µ˜)(x′), (7)
3
E[Pq (µ(x,a))(y)]= (cid:88) µ(x,a)(x′)(cid:88) p(x′′|x′,soft-min Q(x′),µ,µ(x,a))(1 −µ(x,a)(y))
X,π˜(X) ϕ {x′′=y}
x′̸=x x′′
(cid:88)
+µ(x,a)(x) p(x′′|x,a,µ,µ(x,a))(1 −µ(x,a)(y))
{x′′=y}
x′′
(cid:88)
= µ(x,a)(x′)p(y|x′,soft-min Q(x′),µ,µ(x,a))+µ(x,a)(x)p(y|x,a,µ,µ(x,a))−µ(x,a)(y)
ϕ
x′̸=x
=:P˜(x,a)(µ,Q,µ(x,a))(y), (8)
3
which defines P (µ,Q,µ˜) in (7) and P˜(x,a)(µ,Q,µ(x,a)) in (8).
3 3
Our synchronous algorithm is obtained from Algorithm 1 by replacing:
• line 7 by µ(x,a) =µ(x,a)+ρµ˜Pq (µ(x,a)),
n+1 n n Xn(x,a),π˜(x,a)(Xn(x,a))
• line 8 by µ =µ +ρµPq (µ),
n+1 n n Xn,soft-minϕQn(Xn)
• and line 10 by Q (x,a)=Q (x,a)+ρQTq (x,a)
n+1 n n µn,Qn,µ( nx,a)
using the deterministic updating rates:
1 1 1
ρµ = ≪ρQ = ≪ρµ˜ = , (9)
n (1+n)ωµ n (1+n)ωQ n (1+n)ωµ˜
where ωµ, ωQ and ωµ˜ are the same parameters as before such that 1 < ωµ˜ < ωQ < ωµ < 1. Then our
2
algorithm becomes synchronous as the Q-table is updated at all entries (x,a) at each time n. There is still
a stochastic approximation which will be handled in the proof of convergence in Section 4 by the using the
martingale difference property of the differences
 P P( nx,a) : := =P Pq qXn(x,a),π˜(x,a)(Xn(x,a))( (µ µ( )x −,a) P)− (µP˜ 3( ,x Q,a)( ,µ
µn
(x, ,Q
a)n
), ,µ( nx,a)),
(10)
Tn
n(x,a) :=Tq
µX nn ,Q,s nof ,t µ- n(m x,i an )ϕ (Q xn ,( aX )n −)
T 3(µ
n,3
Q
nn
,µ(
nxn ,a))(n
x,a).
53.3 Idealized Three-timescale Algorithm
Finally, we remove the stochastic approximation step by using expectations of the random terms in the
q
synchronousalgorithm. Moreprecisely,inthefirstiteminSection3.2,P (µ(x,a))isreplaced
Xn(x,a),π˜(x,a)(Xn(x,a))
by P˜(x,a)(µ ,Q ,µ(x,a)), in the second item Pq (µ) is replaced by P (µ ,Q ,µ(x,a)), and in
3 n n n Xn,soft-minQn(Xn) 3 n n n
thethirditem,Tq (x,a)isreplacedbyT (µ ,Q ,µ(x,a))(x,a). Thatleadstothesystemofiterations:
µn,Qn,µ( nx,a) 3 n n n
 µ(x,a) =µ(x,a)+ρµ˜P˜(x,a)(µ ,Q ,µ(x,a))
 n+1 n n 3 n n n
µ =µ +ρµP (µ ,Q ,µ(x,a)) (11)
n+1 n n 3 n n n
Q (x,a)=Q (x,a)+ρQT (µ ,Q ,µ(x,a))
n+1 n n 3 n n n
where
 P˜(x,a)(µ,Q,µ˜)(y) :=(cid:80) µ˜(x′)p(y|x′,soft-min Q(x′),µ,µ˜)+µ˜(x)p(y|x,a,µ,µ˜)−µ˜(y)
 3 x′̸=x ϕ
P (µ,Q,µ˜)(y) :=(cid:80) µ(x′)p(y|x′,soft-min Q(x′),µ,µ˜)−µ(y) (12)
3 x′ ϕ
T (µ,Q,µ˜)(x,a) :=f(x,a,µ,µ˜)+γ(cid:80) p(x′|x,a,µ.µ˜)min Q(x′,a′)−Q(x,a),
3 x′ a′
Note that the first two equations in (12) can be rewritten
(cid:40) P˜(x,a)(µ,Q,µ˜)(y) =µ˜P˜soft-minϕQ,µ,µ˜ (y)−µ˜(y),
3 (x,a) (13)
P 3(µ,Q,µ˜)(y) =µPsoft-minϕQ,µ,µ˜(y)−µ(y),
where Pπ,µ,µ˜ and P˜π,µ,µ˜ are defined by
(x,a)
(cid:40) µ′Pπ,µ,µ˜(y)=(cid:80) µ′(x′)(cid:80) π(a|x′)p(y|x′,a,µ,µ˜), y ∈X
x′∈X a∈A (14)
µ′P˜π,µ,µ˜(y)=(cid:80) µ′(x′)p(y|x′,π,µ,µ˜)+µ′(x)p(y|x,a,µ,µ˜), y ∈X.
(x,a) x′̸=x
Now, following [Borkar, 1997] in the two-timescale case, we consider the following three-timescale ODE
system which tracks the system (11):
 µ˙ =P (µ ,Q ,µ(x,a))
 t 3 t t t
Q˙ (x,a)= 1T (µ ,Q ,µ(x,a)) (15)
t ϵ 3 t t t
µ˙(x,a) = 1P˜(x,a)(µ ,Q ,µ(x,a)),
t ϵϵ˜ 3 t t t
where the small parameters ϵ and ϵ˜represents the ratios ρµ/ρQ and ρQ/ρµ˜ respectively. This is what we
n n n n
call the idealized algorithm, because it cannot be implemented directly in general since the expectations
cannot be computed exactly.
4 Convergence: Three-timescale Approach
As announced in Section 3, we now proceed with the proof of convergence starting with convergence
of the ideal deterministic algorithm by using a three-timescale generalization of two-timescale results in
[Borkar, 1997], and then successively restoring the stochastic approximation and the asynchronous features
by controlling the corresponding errors.
4.1 Convergence of the Idealized Three-timescale Algorithm
In this section, we establish the convergence of the idealized three-timescale algorithm defined in (11). In
order to do so, we will introduce several assumptions, the first of which is the following regarding regularity
of the functions f and p.
Assumption 4.1. The cost function f is bounded and is Lipschitz with respect to µ and µ′, with Lipschitz
constants denoted by Lglob and Lloc respectively, when using the L1 norm. The transition kernel p(·|·,·,µ,µ′)
f f
is also Lipschitz with respect to µ and µ′, with Lipschitz constants denoted by Lglob and Lloc respectively,
p p
when using the L1 norm. In other words, for every x,a,µ,µ′,µ˜,µ˜′, we have:
(cid:88)
|f(x,a,µ,µ˜)−f(x,a,µ′,µ˜)|≤Lglob∥µ−µ′∥ =Lglob |µ(x)−µ′(x)|,
f 1 f
x
(cid:88)
|f(x,a,µ,µ˜)−f(x,a,µ,µ˜′)|≤Lloc∥µ˜−µ˜′∥ =Lloc |µ˜(x)−µ˜′(x)|,
f 1 f
x
(cid:88)
∥p(·|x,a,µ,µ˜)−p(·|x,a,µ′,µ˜)∥ ≤Lglob∥µ−µ′∥ =Lglob |µ(x)−µ′(x)|,
1 p 1 p
x
(cid:88)
∥p(·|x,a,µ,µ˜)−p(·|x,a,µ,µ˜′)∥ ≤Lloc∥µ˜−µ˜′∥ =Lloc |µ˜(x)−µ˜′(x)|.
1 p 1 p
x
We will denote Lmax :=max(Lglob,Lloc).
p p p
Under Assumption 4.1, we derive in the following propositions, the Lipschitz properties with respect to
µ,Q,µ˜ of the functions P ,P˜(x,a), and T defined in (12).
3 3 3
6First, we introduce the notations:
(cid:88)
cϕ
min
:= min Psoft-minϕQ,µ,µ˜(x′,x)= min soft-min ϕQ(x′,a)p(x|x′,a,µ,µ˜), (16)
x,x′,Q,µ,µ˜ x,x′,Q,µ,µ˜
a
(cid:88)
c := min p(x|x′,a,µ,µ˜) soft-min Q(x′,a)= min p(x|x′,a,µ,µ˜). (17)
min ϕ
x,x′,a,µ,µ˜ x,x′,a,µ,µ˜
a
Note that c ≤cϕ .
min min
Proposition 4.2. If Assumption 4.1 holds, then the function (µ,Q,µ˜)(cid:55)→P (µ,Q,µ˜) is Lipschitz and more
3
precisely:
∥P (µ,Q,µ˜)−P (µ,Q,µ˜′)∥ ≤Lloc∥µ˜−µ˜′∥ , (18)
3 3 ∞ p 1
∥P (µ,Q,µ˜)−P (µ,Q′,µ˜)∥ ≤ϕ|A|∥Q−Q′∥ , (19)
3 3 ∞ ∞
and
∥P (µ,Q,µ˜)−P (µ′,Q,µ˜)∥ ≤(Lglob+2−|X|cϕ )∥µ−µ′∥ . (20)
3 3 ∞ p min 1
Proof. We have:
∥P 3(µ,Q,µ˜)−P 3(µ,Q,µ˜′)∥
∞
≤∥µPsoft-minϕQ,µ,µ˜−µPsoft-minϕQ,µ,µ˜′ ∥
∞
≤∥µPsoft-minϕQ,µ,µ˜−µPsoft-minϕQ,µ,µ˜′
)∥
1
≤Lloc∥µ˜−µ˜′∥ .
p 1
Moreover,
∥P (µ,Q,µ˜)−P (µ,Q′,µ˜)∥
3 3 ∞
=∥µPsoft-minϕQ,µ,µ˜−µPsoft-minϕQ′,µ,µ˜∥
∞
(cid:12) (cid:12)
(cid:88)(cid:12)(cid:88) (cid:88) (cid:12)
≤ (cid:12) µ(x′) ((soft-min Q(x′))(a)p(x|x′,a,µ,µ˜)−(soft-min Q′(x′))(a)p(x|x′,a,µ,µ˜))(cid:12)
(cid:12) ϕ ϕ (cid:12)
(cid:12) (cid:12)
x x′ a
(cid:12) (cid:12)
(cid:88) (cid:12)(cid:88)(cid:88) (cid:12)
≤ µ(x′)(cid:12) ((soft-min Q(x′))(a)p(x|x′,a,µ,µ˜)−(soft-min Q′(x′))(a)p(x|x′,a,µ,µ˜))(cid:12)
(cid:12) ϕ ϕ (cid:12)
(cid:12) (cid:12)
x′ a x
(cid:115)
(cid:88) (cid:88)
≤ µ(x′) 12∥soft-min Q(x′)−soft-min Q′(x′)∥
ϕ ϕ 2
x′ a
≤(cid:88) µ(x′)|A|21ϕ∥Q(x′)−Q′(x′)∥
2
x′
≤ϕ|A|∥Q−Q′∥ .
∞
Last,fortheLipschitzcontinuitywithrespecttoµ,wefirstmakethefollowingremark: wecanfindp(i,j)>ϵ,
so that 1−Nϵ>0, and then define q(i,j)=(p(i,j)−ϵ)/(1−Nϵ). Thus, P =(1−Nϵ)Q+ϵJ, where J is
the N ×N matrix with all entries 1. We use this fact to calculate the total variation. Hence we have:
∥P 3(µ,Q,µ˜)−P 3(µ′,Q,µ˜)∥
∞
≤∥µPsoft-minϕQ,µ,µ˜−µ′Psoft-minϕQ,µ′,µ˜∥ ∞+∥µ−µ′∥
∞
≤∥µPsoft-minϕQ,µ,µ˜−µ′Psoft-minϕQ,µ,µ˜)∥
1
+∥µ′Psoft-minϕQ,µ,µ˜−µ′Psoft-minϕQ,µ′,µ˜∥ 1+∥µ−µ′∥
1
≤(1−|X|cϕ )∥µ−µ′∥ +Lglob∥µ−µ′∥ +∥µ−µ′∥
min 1 p 1 1
≤(Lglob+2−|X|cϕ )∥µ−µ′∥ .
p min 1
Proposition 4.3. If Assumption 4.1 holds, then for every (x,a), the function (µ,Q,µ˜)(cid:55)→P˜(x,a)(µ,Q,µ˜) is
3
Lipschitz and more precisely:
∥P˜(x,a)(µ,Q,µ˜)−P˜(x,a)(µ′,Q,µ˜)∥ ≤Lglob∥µ−µ′∥ , (21)
3 3 ∞ p 1
∥P˜(x,a)(µ,Q,µ˜)−P˜(x,a)(µ,Q′,µ˜)∥ ≤ϕ|A|∥Q−Q′∥ , (22)
3 3 ∞ ∞
and
∥P˜(x,a)(µ,Q,µ˜)−P˜(x,a)(µ,Q,µ˜′)∥ ≤(Lloc+2−|X|c )∥µ˜−µ˜′∥ . (23)
3 3 ∞ p min 1
Proof. Let us fix (x,a). First, we have
∥P˜(x,a)(µ,Q,µ˜)−P˜(x,a)(µ′,Q,µ˜)∥ ≤∥µ˜P˜soft-minϕQ,µ,µ˜ −µ˜P˜soft-minϕQ,µ′,µ˜ ∥
3 3 ∞ (x,a) (x,a) ∞
(cid:88)
≤∥ µ˜(x′)p(·|x′,soft-min Q(x′),µ,µ˜)+µ˜(x)p(·|x,a,µ,µ˜)
ϕ
x′̸=x
(cid:88)
− µ˜(x′)p(·|x′,soft-min Q(x′),µ′,µ˜)+µ˜(x)p(·|x,a,µ′,µ˜)∥
ϕ ∞
x′̸=x
(cid:88)
≤ µ(x′)∥maxp(·|x′,a′,µ,µ˜)−p(·|x′,a′,µ′,µ˜)∥
∞
x′,a′
x′
≤Lglob∥µ−µ′∥ .
p 1
7Second, we have:
∥P˜(x,a)(µ,Q,µ˜)−P˜(x,a)(µ,Q′,µ˜)∥ =∥µP˜soft-minϕQ,µ,µ˜ −µP˜soft-minϕQ′,µ,µ˜ ∥
3 3 ∞ (x,a) (x,a) ∞
(cid:88)
≤∥ µ˜(x′)p(·|x′,soft-min Q(x′),µ,µ˜)+µ˜(x)p(·|x,a,µ,µ˜)
ϕ
x′̸=x
(cid:88)
− µ˜(x′)p(·|x′,soft-min Q′(x′),µ,µ˜)+µ˜(x)p(·|x,a,µ,µ˜)∥
ϕ ∞
x′̸=x
≤∥µPsoft-minϕQ,µ,µ˜−µPsoft-minϕQ′,µ,µ˜∥
1
≤ϕ|A|∥Q−Q′∥ .
∞
Last,
∥P˜(x,a)(µ,Q,µ˜)−P˜(x,a)(µ,Q,µ˜′)∥ ≤∥µ˜P˜soft-minϕQ,µ,µ˜ −µ˜′P˜soft-minϕQ,µ,µ˜′ ∥ +∥µ˜−µ˜′∥
3 3 ∞ (x,a) (x,a) ∞ ∞
≤∥µ˜P˜soft-minϕQ,µ,µ˜ −µ˜′P˜soft-minϕQ,µ,µ˜′ ∥ +∥µ˜−µ˜′∥
(x,a) (x,a) 1 1
≤∥µ˜Psoft-minϕQ,µ,µ˜−µ˜′Psoft-minϕQ,µ,µ˜′ ∥ 1+∥µ˜−µ˜′∥
1
≤(Lloc+2−|X|c )∥µ˜−µ˜′∥ .
p min 1
Proposition 4.4. If Assumption 4.1 holds, then the function (µ,Q,µ˜)(cid:55)→T (µ,Q,µ˜) is Lipschitz.
3
Proof. We recall that the Bellman operator B has been introduced in (4). We note that:
µ,µ˜
∥T (µ,Q,µ˜)−T (µ,Q′,µ˜)∥ ≤∥B Q−B Q′∥ +∥Q−Q′∥
3 3 ∞ µ,µ˜ µ,µ˜ ∞ ∞
(cid:88)
≤γ∥ p(x′|·,·,µ,µ˜)(minQ(x′,a′)−minQ′(x′,a′))∥ +∥Q−Q′∥
∞ ∞
a′ a′
x′
≤γ∥Q−Q′∥ +∥Q−Q′∥
∞ ∞
≤(γ+1)∥Q−Q′∥ ,
∞
where we used the fact that
(cid:88) (cid:88)
∥ p(x′|·,·,µ,µ˜)(minQ(x′,a′)−minQ′(x′,a′))∥ ≤sup p(x′|x,a,µ,µ˜)|(minQ(x′,a′)−minQ′(x′,a′))|,
∞
a′ a′ x,a a′ a′
x′ x′
with p(x′|x,a,µ,µ˜)≤1 and |min Q(x′,a′)−min Q′(x′,a′)|≤∥Q−Q′∥ . Moreover,
a′ a′ ∞
(cid:88)
∥T (µ,Q,µ˜)−T (µ,Q,µ˜′)∥ ≤∥f(·,·,µ,µ˜)−f(·,·,µ,µ˜′)∥ +∥γ |p(x′|·,·,µ,µ˜)−p(x′|·,·,µ,µ˜′)||minQ(x′,a′)|∥
3 2 ∞ ∞ ∞
a′
x′
≤(Lglob+γLglob∥Q∥ )∥µ˜−µ˜′∥
f p ∞ 1
≤|X|(Lglob+γLglob∥Q∥ )∥µ˜−µ˜′∥ .
f p ∞ ∞
Likewise, we can show:
∥T (µ,Q,µ˜)−T (µ′,Q,µ˜)∥ ≤|X|(Lloc+γLloc∥Q∥ )∥µ−µ′∥ .
3 2 ∞ f p ∞ ∞
Now that we have these Lipschitz properties, we show that the ODE system (11) has a unique global
asymptotically stable equilibrium (GASE).
Assumption 4.5. Let Lloc < 1|X|c and Lglob < 1|X|c .
p 2 min p 2 min
We first show that the fast scale variable, which corresponds to the local distribution, converges to a
GASE.
Proposition 4.6. Suppose Assumptions 4.1 and 4.5 hold. For any given Q and µ, for every (x,a), µ˜˙ =
t
P˜(x,a)(µ,Q,µ˜ ) has aunique GASE,that wewilldenote by µ˜∗ϕ,(x,a). Moreover, µ˜∗ϕ,(x,a) :R|X|×|A|×∆|X| →
3 t Q,µ
∆|X|, (µ,Q)(cid:55)→µ˜∗ϕ,(x,a) is Lipschitz.
µ,Q
Proof. Let us fix µ and Q. By Proposition 4.3, we have ∥µ˜P˜soft-minϕQ,µ,µ˜ −µ˜′P˜soft-minϕQ,µ,µ˜′ ∥ ≤ (Lloc +
(x,a) (x,a) 1 p
1−|X|c )∥µ˜−µ˜′∥ . Since (Lloc+1−|X|c ) < 1 by Assumption 4.5, µ˜ (cid:55)→ µ˜P˜soft-minϕQ,µ,µ˜ is a strict
min 1 p min (x,a)
contraction. As a result, by the contraction mapping theorem [Sell, 1973], there is a unique GASE and
furthermore, by [Borkar and Soumyanatha, 1997, Theorem 3.1], it is the limit of µ˜ denoted by µ˜∗ϕ,(x,a).
t µ,Q
Then,
∥µ˜∗ϕ,(x,a)−µ˜∗ϕ,(x,a)∥ ≤∥µ˜∗ϕ,(x,a)P˜soft-minϕQ,µ,µ˜ µ∗ϕ ,Q,(x,a) −µ˜∗ϕ,(x,a)P˜soft-minϕQ′,µ′,µ˜ µ∗ϕ ′,, Q(x ′,a) ∥
µ,Q µ′,Q′ 1 µ,Q (x,a) µ′,Q′ (x,a) 1
≤∥µ˜∗ϕ,(x,a)P˜soft-minϕQ,µ,µ˜∗ µϕ ,Q,(x,a) −µ˜∗ϕ,(x,a)P˜soft-minϕQ,µ,µ˜∗ µϕ ′,, Q(x ′,a)
∥
µ,Q (x,a) µ′,Q′ (x,a) 1
+∥µ˜∗ϕ,(x,a)P˜soft-minϕQ,µ,µ˜ µ∗ϕ ′,, Q(x ′,a) −µ˜∗ϕ,(x,a)P˜soft-minϕQ,µ′,µ˜∗ µϕ ′,, Q(x ′,a)
∥
µ′,Q′ (x,a) µ′,Q′ (x,a) 1
+∥µ˜∗ϕ,(x,a)P˜soft-minϕQ,µ′,µ˜ µ∗ϕ ′,, Q(x ′,a) −µ˜∗ϕ,(x,a)P˜soft-minϕQ′,µ′,µ˜ µ∗ϕ ′,, Q(x ′,a)
∥
µ′,Q′ (x,a) µ′,Q′ (x,a) 1
≤(Lloc+1−|X|c )∥µ˜∗ϕ,(x,a)−µ˜∗ϕ,(x,a)∥
p min µ,Q µ′,Q′ 1
+Lglob∥µ−µ′∥ +ϕ|A|∥Q−Q′∥ .
p 1 ∞
8Furthermore, (Lloc +1−|X|c ) < 1 by Assumption 4.5 and as a result, µ˜∗ϕ,(x,a) is uniformly Lipschitz
p min µ,Q
with respect to µ and Q.
Assumption 4.7. For any fixed µ, the second ODE in the system (15) along the GASE of the first ODEs,
that is Q˙ =T (µ,Q ,µ˜∗ϕ,(x,a)), has a unique GASE. We will denote it by Q∗ϕ.
t 3 t Qt,µ µ
Assumption 4.8. The third ODE in system (15) along the GASE of the second ODE, that is µ˙ =
t
P (µ ,Q∗ϕ,µ˜∗ϕ,(x,a)), has a unique GASE. We will denote it by µ∗ϕ.
3 t µt Q∗ µϕ t,µt
In Appendix A, we show how to derive the existence and uniqueness of these two GASEs using a con-
tractionargument. However,thisargumentrequiresanupperboundonthechoiceoftheparameterϕwhich
limitsitsuseinourcontextofderivingtheconvergenceofouralgorithmtoasolutionoftheMFCGproblem.
Therefore, in what follows we work under Assumption 4.7 and Assumption 4.8 which could alternatively be
verified by means of Lyapunov functions for instance, as we do in the example presented in Section 5.
Wecannowintroduceourfirsttheorem,whichguaranteestheconvergenceoftheidealizedthree-timescale
algorithm. It relies on the following assumptions about the update rates:
Assumption 4.9. The learning rates ρµ˜, ρQ and ρµ are sequences of positive real numbers satisfying
n n n
(cid:88) (cid:88) (cid:88) (cid:88)
ρµ˜ = ρQ = ρµ =∞, |ρµ˜|2+|ρQ|2+|ρµ|2 <∞, ρµ/ρQ −−−−−→0, ρQ/ρµ˜ −−−−−→0.
n n n n n n n n n n
n→+∞ n→+∞
n n n n
Theorem 4.10. Suppose Assumptions 4.1, 4.5, 4.7, 4.8 and 4.9 hold. Then, (µ ,Q ,µ˜(x,a)) defined in (11)
n n n
converge to (µ∗ϕ,Q∗ϕ ,µ˜∗ϕ,(x,a)) as n→∞.
µ∗ϕ Q∗ϕ,µ∗ϕ
Proof. With Assumptions 4.1, 4.5, 4.7, 4.8, 4.9, and the results of Propositions 4.2, 4.3, 4.4, and 4.6, the
assumptions of [Borkar, 1997, Theorem 1.1] are satisfied, and therefore guarantees the convergence in the
statement.
Assumption 4.11. In what follows, we assume that there exists a unique solution to our MFCG problem
in the sense of Definition 2.1, and we further assume that this solution is a pure policy.
We will need the following result:
Lemma 4.12. For any given pure policy α, the following system for µ and µ˜(x,a)
(cid:40) µ˜(x,a) =µ˜(x,a)P˜α,µ,µ˜(x,a)
(x,a) (24)
µ=µPα,µ,µ˜(x,α(x)),
where Pπ,µ,µ˜ and P˜π,µ,µ˜ are defined in (14), has a unique solution. Furthermore, µ˜(x,α(x)) = µ, which is
(x,a)
independent of x.
Proof. See Appendix B.
Next, we show that the limit point given by the algorithm, that is (µ∗ϕ,Q∗ϕ ,µ˜∗ϕ,(x,a)), provides an
µ∗ϕ Q∗ϕ,µ∗ϕ
approximation of this MFCG solution.
Definition 4.13. Given (µ∗ϕ,Q∗ϕ ,µ˜∗ϕ,(x,a)), we define the policy α∗, the distributions µ∗ and µ˜∗,(x,a) for
µ∗ϕ Q∗ϕ,µ∗ϕ
every (x,a), and the state-action value function Q∗ as:
1. α∗(x)=argmin Q∗ϕ (x,a′)
a′ µ∗ϕ
2. µ∗ and µ˜∗,(x,a) are the fixed points of the following equations:
µ˜∗,(x,a) =µ˜∗,(x,a)P˜α∗,µ∗,µ˜∗,(x,a) , µ∗ =µ∗Pα∗,µ∗,µ˜∗,(x,α∗(x)) ,
(x,a)
where we use Lemma 4.12 with α=α∗.
3. For these µ∗ and µ˜∗,(x,a), Q∗ is the solution to the Bellman equation (3).
Remark4.14. Bychoosinga=α∗(x)initem2, byLemma4.12, wehaveµ˜∗,(x,α∗(x)) =µ∗. Thisisexpected
as in the limiting equilibrium local and global distributions are indistinguishable.
Thefollowingresultshowsthat(µ∗ϕ,Q∗ϕ ,µ˜∗ϕ,(x,a))givenbyTheorem4.10arecloseto(µ∗,Q∗,µ˜∗,(x,a)).
µ∗ϕ Q∗ϕ,µ∗ϕ
Theorem 4.15. Suppose Assumptions 4.1, 4.5, 4.7, 4.8 and 4.9 hold. Let (µ∗,Q∗,µ˜∗,(x,a)) given by
Definition 4.13. Let δ(ϕ) be the action gap defined as δ(ϕ) = min (min Q∗ϕ (x,a) −
x∈X a∈/argmin Q∗ϕ µ∗ϕ
a µ∗ϕ
min Q∗ϕ (x,a))>0, and δ(ϕ)=∞ if Q∗ϕ (x) is constant with respect to a for each x. Then,
a µ∗ϕ µ∗ϕ

3
∥µ˜∗ Qϕ ∗, ϕ(x ,µ,a ∗ϕ) −µ˜∗,(x,a)∥ 1+∥µ∗ϕ−µ∗∥
1
≤ 4| |A X| |c2 me ix np −(− 2Lϕ
m
pδ( aϕ x))
(25)
∥Q∗ µϕ
∗ϕ
−Q∗∥
∞
≤ (Lf+ 1−γ γ (1L −m p γa )x (|∥ Xf |∥ c∞ m) in4| −A 2| L3 2
m
pex axp )(−ϕδ(ϕ)) .
9Proof. Using a similar argument as in Proposition 4.2 for the first term of the second inequality below, we
have:
∥µ˜∗ϕ,(x,a) −µ˜∗,(x,a)∥
=∥µ˜∗ϕ,(x,a)P˜soft-minϕQ∗ µϕ ∗ϕ,µ∗ϕ,µ˜ Q∗ϕ ∗, ϕ(x ,µ,a ∗)
ϕ −µ˜∗,(x,a)P˜α∗,µ∗,µ˜∗,(x,a) ∥
Q∗ϕ,µ∗ϕ 1 Q∗ϕ,µ∗ϕ (x,a) (x,a) 1
≤∥µ˜∗ϕ,(x,a)P˜soft-minϕQ∗ µϕ ∗ϕ,µ∗ϕ,µ˜ Q∗ϕ ∗, ϕ(x ,µ,a ∗)
ϕ
−µ˜∗ϕ,(x,a)P˜α∗,µ∗ϕ,µ˜ Q∗ϕ ∗, ϕ(x ,µ,a ∗)
ϕ∥
Q∗ϕ,µ∗ϕ (x,a) Q∗ϕ,µ∗ϕ (x,a) 1
+∥µ˜∗ϕ,(x,a)P˜α∗,µ∗ϕ,µ˜ Q∗ϕ ∗, ϕ(x ,µ,a ∗)
ϕ −µ˜∗,(x,a)P˜α∗,µ∗ϕ,µ˜∗,(x,a) ∥
Q∗ϕ,µ∗ϕ (x,a) (x,a) 1
+∥µ˜∗,(x,a)P˜α∗,µ∗ϕ,µ˜∗,(x,a) −µ˜∗,(x,a)P˜α∗,µ∗,µ˜∗,(x,a)
∥
(x,a) (x,a) 1
≤|A|1 2 m x∈a Xx∥soft-min ϕQ∗ µϕ ∗ϕ(x)−argminQ∗ µϕ ∗ϕ(x)∥ 2
+(Lloc+1−|X|c )∥µ˜∗ϕ,(x,a) −µ˜∗,(x,a)∥ +Lglob∥µ∗ϕ−µ∗∥
p min Q∗ϕ,µ∗ϕ 1 p 1
≤2|A|3 2 exp(−ϕδ(ϕ))+(Ll poc+1−|X|c min)∥µ˜ Q∗ϕ ∗, ϕ(x ,µ,a ∗ϕ) −µ˜∗,(x,a)∥ 1+Lg plob∥µ∗ϕ−µ∗∥ 1,
wherethefirstterminthelastinequalitycomesfrom[Guo et al., 2019,Lemma7]. Withasimilarargument,
we also have
∥µ∗ϕ−µ∗∥ 1 ≤2|A|3 2 exp(−ϕδ(ϕ))+(Lg plob+1−|X|c min)∥µ∗ϕ−µ∗∥ 1+Ll poc∥µ˜ Q∗ϕ ∗, ϕ(x ,µ,α ∗ϕ∗(x))−µ˜∗,(x,α∗(x))∥ 1.
Consequently, we obtain
(|X|c min−2Ll poc)∥µ˜∗ Qϕ ∗, ϕ(x ,µ,a ∗ϕ) −µ˜∗,(x,a)∥ 1+(|X|c min−2Lg plob)∥µ∗ϕ−µ∗∥
1
≤4|A|23 exp(−ϕδ(ϕ)),
which implies
∥µ˜∗ϕ,(x,a) −µ˜∗,(x,a)∥ +∥µ∗ϕ−µ∗∥ ≤ 4|A|3 2 exp(−ϕδ(ϕ)) ,
Q∗ϕ,µ∗ϕ 1 1 |X|c −2Lmax
min p
so that we deduce the first inequality in (25). Then,
∥Q∗ϕ −Q∗∥ =∥B Q∗ϕ −B Q∗∥
µ∗ϕ ∞ µ∗ϕ,µ˜∗ϕ µ∗ϕ µ∗,µ˜∗ ∞
Q∗ϕ,µ∗ϕ
≤∥B Q∗ϕ −B Q∗∥ +∥B Q∗−B Q∗∥
µ∗ϕ,µ˜∗ϕ µ∗ϕ µ∗ϕ,µ˜∗ϕ ∞ µ∗ϕ,µ˜∗ϕ µ∗ϕ,µ˜∗ ∞
Q∗ϕ,µ∗ϕ Q∗ϕ,µ∗ϕ Q∗ϕ,µ∗ϕ
+∥B Q∗−B Q∗∥
µ∗ϕ,µ˜∗ µ∗,µ˜∗ ∞
γ
≤γ∥Q∗ϕ −Q∗∥ +(L + Lglob∥f∥ )∥µ∗ϕ−µ∗∥
µ∗ϕ ∞ f 1−γ p ∞ 1
γ
+(L + Lloc∥f∥ )∥µ˜∗ϕ −µ˜∗∥ ,
f 1−γ p ∞ Q∗ϕ,µ∗ϕ 1
and consequently,
∥Q∗ϕ −Q∗∥ ≤ (L f + 1−γ γLm pax∥f∥ ∞)4|A|3 2 exp(−ϕδ(ϕ)) .
µ∗ϕ ∞ (1−γ)(|X|c −2Lmax)
min p
We are now ready for the main result of this section.
Theorem 4.16. Suppose Assumptions 4.1, 4.5, 4.7, 4.8, 4.9, and 4.11 hold. Let δ = liminf δ(ϕ) and
ϕ→∞
assume δ > 0. Then, the pure policy α∗ and the distribution µ∗ = µ˜∗,(x,α∗(x)) introduced in Definition 4.13
form a solution to the MFCG problem as in Definition 2.1.
Proof. From the iterations described in (11) we have that the limit satisfies, for all (x,a),
Q∗ϕ (x,a)=f(x,a,µ∗ϕ,µ˜∗ϕ,(x,a))+γ(cid:88) p(x′|x,a,µ∗ϕ,µ˜∗ϕ,(x,a))minQ∗ϕ (x,a′),
µ∗ϕ Q∗ϕ,µ∗ϕ Q∗ϕ,µ∗ϕ a′ µ∗ϕ
x′
and by definition of α∗,
Q∗ϕ (x,a)>minQ∗ϕ (x,a′) for a̸=α∗(x).
µ∗ϕ a′ µ∗ϕ
By definition of the action gap, the difference between the left-hand side and the right-hand side above is
greaterthanthegapδ(ϕ). Undertheconditionthattheerrorsin(25)aresmallenough(bychoosingϕlarge
enough), we can replace Q∗ϕ by Q∗, µ˜∗ϕ by µ˜∗ and µ∗ϕ by µ∗ and obtain:
µ∗ϕ Q∗ϕ,µ∗ϕ
(cid:88)
Q∗(x,a)=f(x,a,µ∗,µ˜∗,(x,a))+γ p(x′|x,a,µ∗,µ˜∗,(x,a))minQ∗(x,a′)>minQ∗(x,a′) for a̸=α∗(x),
a′ a′
x′
i.e. argminQ∗ = argminQ∗ϕ = α∗, so that (α∗,µ∗) is indeed the MFCG solution. Recall that by Re-
µ∗ϕ
mark4.14,fora=α∗(x),wehaveµ˜∗,(x,α∗(x)) =µ∗andQ∗(x,α∗(x))=V∗(x),theoptimalvaluefunction.
104.2 Synchronous Stochastic Approximation
Recall in Section 3.2, we defined a synchronous algorithm with stochastic approximation. In this chapter,
we will established the convergence of this three-timescale approach with stochastic approximation. Let us
introduce the following notation,
n n n
ψµ(x):= (cid:88) ρµP (x), ψµ(x,a) (y):= (cid:88) ρµ˜P(x,a)(y), ψQ(x,a):= (cid:88) ρQT (x,a),
n m m n m m n m m
m=1 m=1 m=1
where P (x), P(x,a)(y) and T (x,a) are defined in (10).
m m m
Proposition 4.17. Suppose Assumption 4.9 hold. ψµ(x), ψµ(x,a)(y) and ψQ(x,a) defined above, are square
n n n
integrable martingales and hence they converge a.s. as n→+∞.
Proof. By Lemma 4.5 in [Borkar and Meyn, 2000], we directly have ψQ is a square integrable martingale.
n
Also we have E[∥P ∥2] < 1 and E[∥P(x,a)∥2] < 1. Using this and the square summability of (ρµ) and
n n n n
(ρµ˜) assumed in Assumption 4.9, the bound immediately follows, which shows that ψµ and ψµ(x,a) are also
n n n n
square integrable martingales. Then by martingale convergence theorem [Neveu, 1975, p.62], we have the
convergence of the three martingales.
Based on the above result and the results proved in the previous subsection, we have the following
theorem.
Theorem 4.18. Suppose Assumptions 4.1, 4.5, 4.7, 4.8 and 4.9 hold. Then, (µ ,Q ,µ˜(x,a)) obtained by
n n n
the synchronous algorithm described in Section 3.2 converge to (µ∗ϕ,Q∗ϕ ,µ˜∗ϕ,(x,a)) a.s. as n→∞.
µ∗ϕ Q∗ϕ,µ∗ϕ
Proof. WithAssumptions4.1,4.5,4.7,4.8and4.9,andtheresultsofPropositions4.2,4.3,4.4,4.6and4.17,
the assumptions of [Borkar, 1997, Theorem 1.1] are satisfied. This result guarantees the convergence in the
statement.
4.3 StochasticApproximationandAsynchronousSettingfortheThree-timescale
System
In the previous section, we assumed that learner have access to a generative model, i.e., to a simulator
which can provide the samples of transitions drawn according to the hidden dynamic for arbitrary state
x. However, as in our full Algorithm 1 a more realistic situation, the learner is constrained to follow the
trajectory sampled by the environment without the ability to choose arbitrarily its state. Only one state-
action pair is visited at each time step. We call this an asynchronous setting. Let ρQ˜ be a short notation of
n
ρQ . We define the system as follows for MFCG,
n,Xn(Xn,An),A( nXn,An)

µ( nx +,a 1) =µ( nx,a)+ρµ n˜
,Xn(x,a),A(
nx,a)(P˜ 3(x,a)(µ n,Q n,µ( nx,a))+P( nx,a)) (26a)
µ =µ +ρµ(P (µ ,Q ,µ(x,a))+P ) (26b)
n+1 n n 3 n n n n
Q
n+1(X n,A n)=Q n(X n,A n)+ρQ n˜ (T 3(µ n,Q n,µ( nXn,An))+T n(X n,A n))1
{Xn(Xn,An)=Xn}
(26c)
for x ∈ X, a ∈ A,n ≥ 0, where (X(x,a),A(x,a)) and (X ,A ) are the state-action pair at time n defined
n n n n
in Algorithm 1. Comparing to the synchronous environment in the previous section, now we update the
Q-table at most one state-action pair for each time. As a consequence, the state-action pairs are not all
visitedatthesamefrequencyandthelearningrateneedstobeadjustedaccordingly. Wedenotethelearning
rate for each state-action pair (x,a) as ρQ . The martingale difference sequences are defined as those in
n,x,a
the Section 3.2, namely (10).
Next, we will establish the convergence of the three-timescale approach with stochastic approximation
under asynchronous setting defined in (26a)–(26c).
We will use the following assumption.
Assumption 4.19. For any (x,a), the sequence (ρµ n˜ ,x,a) n∈N satisfy the following conditions:
(cid:88) (cid:88)
ρµ˜ =∞ and |ρµ˜ |2 <∞.
k,x,a k,x,a
k k
Assumption 4.20 (Ideal tapering stepsize). For any (x,a), the sequence (ρQ n,x,a) n∈N satisfies the following
conditions:
• (i) (cid:80) ρQ =∞, and (cid:80) |ρQ |2 <∞.
k k,x,a k k,x,a
• (ii) ρQ ≤ρQ from some k onwards.
k+1,x,a k,x,a
• (iii) there exists r ∈(0,1) such that (cid:80) (ρQ )(1+q) <∞,∀q ≥r.
k k,x,a
• (iv) for ξ ∈(0,1), sup
ρQ
[ξk],x,a <∞, where [·] stands for the integer part.
k ρQ
k,x,a
• (v) for ξ ∈(0,1), and D(k):=(cid:80)k ρQ , D([yk]) →1, uniformly in y ∈[ξ,1].
ℓ=0 ℓ,x,a D(k)
11Intheasynchronoussetting,weupdatestep-by-steptheQtablealongthetrajectorygeneratedrandomly.
In order to ensure convergence, every state-action pair needs to be updated repeatedly. We introduce
ν(x,a,n)=(cid:80)n 1 asthenumberoftimestheprocess(X ,A )visitstate(x,a)uptotime
m=0 {(Xm,Am)=(x,a)} n n
n. The following assumptions are related to the (almost sure) good behavior of the number visits along
trajectories.
Assumption 4.21. There exists a deterministic ∆>0 such that for all (x,a),
ν(x,a,n)
liminf ≥∆ a.s.
n→∞ n
(cid:110) (cid:111)
Furthermore, letting N(n,ξ)=min m>n:(cid:80)m ρQ >ξ for ξ >0, the limit
k=n+1 k,x,a
(cid:80)ν((x,a),N(n,ξ))ρQ
k=ν((x,a),n) k,x,a
lim
n→∞(cid:80)ν((x′,a′),N(n,ξ))ρQ
k=ν((x′,a′),n) k,x′,a′
exists a.s. for all pairs (x,a), (x′,a′).
Assumption 4.22. There exists d >0, such that for each i=(x,a) and j =(x′,a′)
ij
(cid:80)n ρQ
lim m=0 m,j =d .
n→∞ (cid:80)n ρQ ij
m=0 m,i
Proposition 4.23. Suppose Assumptions 4.1, 4.5, 4.7 and 4.8 hold. Then for every µ, there exists a strict
Lyapunov function for the ODE Q˙ =T (µ,Q ,µ˜∗ϕ,(x,a)).
t 3 t Qt,µ
Proof. Let Q˜ be defined by T (µ,Q˜,µ˜∗ϕ,(x,a))=0. Let us define V =∥Q−Q˜∥ . By [Borkar, 1998, Case 1,
3 Q˜,µ ∞
Page 844], V is the strict Lyapunov function for the ODE Q˙ =T (µ,Q ,µ˜∗ϕ,(x,a)).
t 3 t Qt,µ
Then, we have the following theorem.
Theorem 4.24. Suppose Assumptions 4.1, 4.5, 4.7, 4.8, 4.9, 4.19, 4.20, 4.21 and 4.22 hold. Then,
(µ ,Q ,µ˜(x,a)) defined in (26a)–(26c) converges to (µ∗ϕ,Q∗ϕ ,µ˜∗ϕ,(x,a)) a.s. as n→∞.
n n n µ∗ϕ Q∗ϕ,µ∗ϕ
Proof. It’s not hard to check our learning rates defined in (26a)–(26c) satisfy 4.9, 4.19, 4.20, 4.21 and 4.22.
WiththestatedassumptionsandtheresultsofPropositions4.2,4.3,4.4,4.6,4.17and4.23,theassumptions
of [Borkar, 1998, Theorem 3.1] and [Konda and Borkar, 1999, Lemma 4.11] are satisfied. These results
guarantee the convergence in the statement.
5 Numerical Illustration
We present a very simple example with an explicit solution to the MFCG problem to illustrate the perfor-
mance of our algorithm.
5.1 The Model
Let us consider the state space X ={x =0,x =1} and the action space A={stay =0,move=1}. The
0 1
Markovian dynamics is given by
(cid:40)
1−p, if x′ =a(x)
p(x′|x,a)=
p, if x′ ̸=a(x),
where 0 < p < 1 represents a small noise parameter, and where as such we impose 0 < p < 0.5. Note that
the dynamics does not depend on the population distributions µ or µ˜, and, therefore, in the notation of
Assumption 4.1, Lglob =Lloc =0.
p p
Let us consider the cost function:
f(x,a,µ,µ˜)=x+c µ +c µ˜ ,
g 0 l 0
where c > 0 and c > 0, and µ (resp. µ˜ ) is the probability to be at x under the global distribution µ
g l 0 0 0
(resp. under the local distribution µ˜). One may think of µ as 1−µ¯, and µ˜ as 1−µ¯˜, that is as functions
0 0
of the means. Note that
|f(x,a,µ,µ˜)−f(x,a,µ′,µ˜)|=c |µ −µ′|≤Lglob∥µ−µ′∥ ,
g 0 0 f 1
|f(x,a,µ,µ˜)−f(x,a,µ,µ˜′)|=c |µ˜ −µ˜′|≤Lloc∥µ˜−µ˜′∥ ,
l 0 0 f 1
with Lglob =c /2 and Lloc =c /2.
f g f l
Recall that c is defined in (17). In this example we have:
min
c = min p(x′|x,a,µ,µ˜)=p>0.
min
x,x′,a,µ,µ˜
Therefore, in this example Assumptions 4.1 and 4.5 are satisfied.
125.2 Theoretical Solutions
There are 4 pure policies α given by: {(s,s),(s,m),(m,s),(m,m)}, associated to the corresponding limiting
distributions µα: {(1,1),(1−p,p),(p,1−p),(1,1)}.
2 2 2 2
As in Section 2.2.1, for a strategy α, we define the strategy α˜(x,a) by:
(cid:40)
a if x′ =x,
α˜(x,a)(x′):=
α(x′) for x′ ̸=x.
For a given distribution µ, the Bellman equation (3) reads:
Q∗(x,a)=f(x,a,µ,µα˜∗(x,a))+γ(cid:2) (1−p)Q∗(a(x),s)∧Q∗(a(x),m)+pQ∗(1−a(x),s)∧Q∗(1−a(x),m)(cid:3)
.
µ µ µ µ µ
When c large enough (c > 2γ), one can check that the optimal strategy is α∗ = (m,s) which corresponds
l l
to the case
Q∗(0,m)<Q∗(0,s), Q∗(1,s)<Q∗(1,m),
and to the distribution µ∗ =µα∗ =(p,1−p). The optimal Q-values are given by
c p c p−γp+γ
Q∗(0,m)= g + l , Q∗(1,s)=Q∗(0,m)+1,
1−γ 1−γ
c p c c p−2γp+γ+p
Q∗(0,s)= g + l +γ l , Q∗(1,m)=Q∗(0,s)+1,
1−γ 2 1−γ
so that Q∗(0,s)−Q∗(0,m)=Q∗(1,m)−Q∗(1,s)= 1(1−2p)(c −2γ)>0 as soon as c >2γ. Finally, note
2 l l
that V∗(0)=Q∗(0,m) and V∗(1)=Q∗(1,s).
Under our model, for fixed µ, the MFCG problem can be considered as a MFC problem. The global dis-
tributionµwillonlyshifttheQ-tablefromthecorrespondingMFCproblemby cgµ0. In[Angiuli et al., 2023,
1−γ
Appendix D], we proved that for such a model there is a unique optimal strategy (m,s) for such MFC prob-
lem, so that we have unique solution to our MFCG problem. So far, we have proved Assumption is 4.11 is
satisfied.
5.3 MFCG Assumptions 4.7, 4.8 and the gap in Theorem 4.16
We use a soft-min policy π defined by π(a|x)=soft-min Q(x)(a). For any given Q, the four limiting distri-
ϕ
butions (denoted µ˜∗ϕ,(x,a) in Proposition 4.6) corresponding to the four state-action paris are characterized
Q,µ
by:
p 1−p
µ˜(0,s) = , µ˜(0,m) = ,
1 1−(1−2p)π(s|1) 1 2−2p−(1−2p)π(s|1)
1−p−(1−2p)π(s|0) 1−p−(1−2p)π(s|0)
µ˜(1,s) = , µ˜(1,m) = .
1 1−(1−2p)π(s|0) 1 2−2p−(1−2p)π(s|0)
Then, we compute the GASE Q∗ϕ in Assumption 4.7 by solving T (µ,Q∗ϕ,µ˜∗ϕ,(x,a))=0. The uniqueness is
µ 3 µ Q∗ µϕ,µ
proved by a similar argument as in [Angiuli et al., 2023] Section 5.2.4.. Their values are given by:
c µ +c µ˜∗ϕ(0,m)+γ(1−p)(1−c µ˜∗ϕ(0,m)+c µ˜∗ϕ(1,s))
Q∗ϕ(0,m)= g 0 l 0 l 0 l 0
µ 1−γ
Q∗ϕ(1,s)=Q∗ϕ(0,m)+1−c µ˜∗ϕ(0,m)+c µ˜∗ϕ(1,s)
µ µ l 0 l 0
c µ +c µ˜∗ϕ(0,m)+γ(1−p)(1−c µ˜∗ϕ(0,m)+c µ˜∗ϕ(1,s))
Q∗ϕ(0,s)=c µ +c µ˜∗ϕ(0,s)+γ g 0 l 0 l 0 l 0
µ g 0 l 0 1−γ
+γp(1−c µ˜∗ϕ(0,m)+c µ˜∗ϕ(1,s))
l 0 l 0
Q∗ϕ(1,m)=Q∗ϕ(0,s)+1−c µ˜∗ϕ(0,s)+c µ˜∗ϕ(1,m)
µ µ l 0 l 0
Finally, we can compute the GASE µ∗ϕ in Assumption 4.8 by solving P = 0. Since we have two states, it
3
is enough to write this equation at one point, say x=1. Since the global distribution µ affects Q only as an
additive shift, the policy π given by Q∗ϕ is independent of µ, and we obtain the equation:
µ
µ∗ϕ =(1−µ∗ϕ)[π(s|0)p(1|0,s)+π(m|0)p(1|0,m)]+µ∗ϕ[π(s|1)p(1|1,s)+π(m|1)p(1|1,m)]
1 1 1
=(1−µ∗ϕ)[pπ(s|0)+(1−p)π(m|0)]+µ∗ϕ[(1−p)π(s|1)+pπ(m|1)]
1 1
=(pπ(s|0)+(1−p)π(m|0))−µ∗ϕ(pπ(s|0)+(1−p)π(m|0))+µ∗ϕ((1−p)π(s|1)+pπ(m|1))
1 1
=(pπ(s|0)+(1−p)π(m|0))−µ∗ϕ(pπ(s|0)+(1−p)π(m|0)−(1−p)π(s|1)−pπ(m|1))
1
This is a linear equation which has a unique solution. One can also prove this is a GASE because L(µ )=
1
(µ −µ∗ϕ)2 is a Lyapunov function for the ODE µ˙ =P (µ ) since L′(µ )g(µ )<0 and L′(µ∗ϕ)g(µ∗ϕ)=0.
1 1 1 3 1 1 1 1 1
Repeating the argument with µ∗ϕ =1−µ∗ϕ, one deduces that µ∗ϕ is the unique GASE in Assumption 4.8.
0 1
Regarding the gap in Theorem 4.16, we have:
(cid:16) (cid:17)
δ(ϕ)=min Q∗ϕ (0,m)−Q∗ϕ (0,s),Q∗ϕ (1,s)−Q∗ϕ (1,m)
µ∗ϕ µ∗ϕ µ∗ϕ µ∗ϕ
and lim δ(ϕ) = Q∗(0,m)−Q∗(0,s) = Q∗(1,s)−Q∗(1,m) = 1(1−2p)(c −2γ) > 0 as soon as c > 2γ
ϕ→∞ 2 l l
and p<0.5.
135.4 Numerical Results
We consider the problem defined above with the choice of parameters: N = 2 states, p = 0.1, c = c = 5,
l g
ϕ=500,γ =0.5. WerunthedeterministicAlgorithm(11)with(ωµ˜,ωQ,ωµ)=(0.55,0.75,0.95). Thischoice
satisfies the assumptions that we used to prove convergence to the MFCG solution. Figure 1 illustrates the
result. We provide the convergence plot of µ (x ), i.e., the value of the distribution at state 0, as a function
n 0
of the step n. The y-axis is the value of µ (x ), µα∗(x ) and x-axis is the number of iterations.
n 0 n 0
local distribution
global distribution
0 50 100 150 200
iteration
(a)MFCG:(ωµ˜,ωQ,ωµ)=(0.55,0.75,0.95)
Figure 1: MFCG setting: Convergence of the distribution. The plot represents the value of µ (x ) and
n 0
µα∗(x ) as a function of n.
n 0
The resulting distribution µ=µα∗ =(µ(x ),µ(x )) which is about (0.1,0.9)=(p,1−p), corresponds to
0 1
the equilibrium distribution in the MFCG scenario with optimal strategy α∗ =(m,s) given by the limiting
Q-table:
Q-table Theoretical Values
States/Actions Action 0 Action 1 Action 0 Action 1
State 0 4.501 2.901 4.5 2.9
State 1 3.901 5.501 3.9 5.5
Wealsorunthealgorithms1forthisexamplewiththesamechoiceoflearningrates. Figure2illustratesthe
result. We provide the convergence plot of µ (x ), i.e., the value of the distribution at state 0, as a function
n 0
of the step n≤1000K. The y-axis is the value of µ (x ), µα∗(x ) and x-axis is the number of iterations.
n 0 n 0
l go lc oa bl a d l i ds it sri tb riu bt uio tin on 1.0 local distribution
global distribution
0.8
0.6
0.4
0.2
0 50 100 150 200
0.0
iteration 0.0 0.2 0.4 0.6 0.8 1.0
iteration 1e6
(a) MFCG: (ωµ˜,ωQ,ωµ) = (0.55,0.75,0.95), first (b) MFCG: (ωµ˜,ωQ,ωµ) = (0.55,0.75,0.95), for
2Kiterations 1000Kiterations
Figure 2: MFCG setting: Convergence of the distribution. The plot represents the value of µ (x ) and
n 0
µα∗(x ) as a function of n.
n 0
Asexpected,figure2ashowsthatlocaldistributionconvergesfasterthantheglobaldistributionbuthave
more fluctuations since local distribution is updated faster than the global distribution. Figure 2b shows
eventually they converge perfectly.
The resulting distribution µ=µα∗ =(µ(x ),µ(x )) which is about (0.1,0.9)=(p,1−p), corresponds to
0 1
the equilibrium distribution in the MFCG scenario with optimal strategy α∗ =(m,s) given by the limiting
Q-table:
Q-table Theoretical Values
States/Actions Action 0 Action 1 Action 0 Action 1
State 0 4.334 2.907 4.5 2.9
State 1 3.913 5.477 3.9 5.5
14
)0x(μ
53.0
03.0
52.0
02.0
51.0
01.0
)0x(μ
53.0
03.0
52.0
02.0
51.0
01.0
)0x(References
[Angiuli et al., 2022a] Angiuli, A., Detering, N., Fouque, J.-P., Lauri`ere, M., andLin, J.(2022a). Reinforce-
ment learning algorithm for mixed mean field control games. arXiv:2205.02330.
[Angiuli et al., 2022b] Angiuli,A.,Fouque,J.-P.,andLauri`ere,M.(2022b). Unifiedreinforcementq-learning
for mean field game and control problems. Mathematics of Control, Signals, and Systems, 34:217–271.
[Angiuli et al., 2023] Angiuli, A., Fouque, J.-P., Lauri`ere, M., and Zhang, M. (2023). Convergence of multi-
scale reinforcement q-learning algorithms for mean field game and control problems. arXiv:2312.06659.
[Bensoussan et al., 2013] Bensoussan, A., Frehse, J., and Yam, S. C. P. (2013). Mean field games and mean
field type control theory. Springer Briefs in Mathematics. Springer, New York.
[Borkar and Soumyanatha, 1997] Borkar, V. and Soumyanatha, K. (1997). An analog scheme for fixed
point computation. i. theory. IEEE Transactions on Circuits and Systems I: Fundamental Theory and
Applications, 44(4):351–355.
[Borkar, 1997] Borkar, V. S. (1997). Stochastic approximation with two time scales. Systems & Control
Letters, 29(5):291–294.
[Borkar, 1998] Borkar, V. S. (1998). Asynchronous stochastic approximations. SIAM Journal on Control
and Optimization, 36(3):840–851.
[Borkar and Meyn, 2000] Borkar,V.S.andMeyn,S.P.(2000).Theo.d.e.methodforconvergenceofstochas-
ticapproximationandreinforcementlearning. SIAMJournalonControlandOptimization,38(2):447–469.
[Carmona and Delarue, 2018] Carmona, R. and Delarue, F. (2018). Probabilistic Theory of Mean Field
Games with Applications I-II. Springer.
[Carmona et al., 2019] Carmona, R., Lauri`ere, M., and Tan, Z. (2019). Linear-quadratic mean-field rein-
forcement learning: Convergence of policy gradient methods. Preprint.
[Carmona et al., 2023] Carmona, R., Lauri`ere, M., and Tan, Z. (2023). Model-free mean-field reinforcement
learning: mean-fieldmdpandmean-fieldq-learning. TheAnnalsofAppliedProbability,33(6B):5334–5381.
[Cui and Koeppl, 2021] Cui,K.andKoeppl,H.(2021).Approximatelysolvingmeanfieldgamesviaentropy-
regularized deep reinforcement learning. In International Conference on Artificial Intelligence and Statis-
tics, pages 1909–1917. PMLR.
[Elie et al., 2020] Elie, R.,Perolat,J., Lauri`ere, M., Geist, M.,andPietquin, O.(2020). Ontheconvergence
of model free learning in mean field games. In in proc. of AAAI.
[Frikha et al., 2023] Frikha, N., Germain, M., Lauri`ere, M., Pham, H., and Song, X. (2023). Actor-critic
learning for mean-field control in continuous time. arXiv preprint arXiv:2303.06993.
[Gu et al., 2021] Gu, H., Guo, X., Wei, X., and Xu, R. (2021). Mean-field controls with q-learning for
cooperative marl: convergence and complexity analysis. SIAM Journal on Mathematics of Data Science,
3(4):1168–1196.
[Guo et al., 2019] Guo, X., Hu, A., Xu, R., and Zhang, J. (2019). Learning mean-field games. In Advances
in Neural Information Processing Systems, pages 4966–4976.
[Huang et al., 2006] Huang, M., Malham´e, R. P., and Caines, P. E. (2006). Large population stochastic dy-
namicgames: closed-loopMcKean-VlasovsystemsandtheNashcertaintyequivalenceprinciple. Commun.
Inf. Syst., 6(3):221–251.
[Konda and Borkar, 1999] Konda, V. R. and Borkar, V. S. (1999). Actor-critic–type learning algorithms for
markov decision processes. SIAM Journal on Control and Optimization, 38(1):94–123.
[Lasry and Lions, 2007] Lasry,J.-M.andLions,P.-L.(2007). Meanfieldgames. Jpn.J.Math.,2(1):229–260.
[Lauri`ere et al., 2022] Lauri`ere, M., Perrin, S., Geist, M., and Pietquin, O. (2022). Learning mean field
games: A survey. arXiv preprint arXiv:2205.12944.
[Motte and Pham, 2019] Motte, M. and Pham, H. (2019). Mean-field markov decision processes with com-
mon noise and open-loop controls. arXiv preprint arXiv:1912.07883.
[Neveu, 1975] Neveu, J. (1975). Discrete-parameter Martingales. North-Holland mathematical library.
North-Holland.
[Sell, 1973] Sell, G. R. (1973). Differential equations without uniqueness and classical topological dynamics.
Journal of Differential Equations, 14(1):42–56.
[Subramanian and Mahajan, 2019] Subramanian, J. and Mahajan, A. (2019). Reinforcement learning in
stationary mean-field games. In Proceedings. 18th International Conference on Autonomous Agents and
Multiagent Systems.
[Sutton and Barto, 2018] Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction.
MIT press.
15[Tembine, 2017] Tembine, H. (2017). Mean-field-type games. AIMS Math, 2(4):706–735.
[Wang et al., 2020] Wang,H.,Zariphopoulou,T.,andZhou,X.Y.(2020). Reinforcementlearningincontin-
uoustimeandspace: Astochasticcontrolapproach. JournalofMachineLearningResearch,21(198):1–34.
[Wang and Zhou, 2020] Wang, H. and Zhou, X. Y. (2020). Continuous-time mean–variance portfolio selec-
tion: A reinforcement learning framework. Mathematical Finance, 30(4):1273–1308.
[Watkins, 1989] Watkins, C. J. C. H. (1989). Learning from delayed rewards. PhD thesis, King’s College,
Cambridge.
A GASE by Contraction Argument
To prove that the GASE exist for the second and third O.D.E. in (11), we need to control the parameter ϕ
properly to have a strict contraction. To that end, we introduce the following assumption.
Assumption A.1. Assume
(cid:32) (cid:33)
0<ϕ<min
|X|c min−Lt potal 1−γ
,
(|X|c min−Lg plob)2 1−γ
,
|A| Lt fotal+ 1−γ γLt potal∥f∥ ∞ ϕ|A|(|X|c min−Lg plob+Ll poc)Lg flob+ 1−γ γLg plob∥f∥ ∞
where Ltotal =Lglob+Lloc and Ltotal =Lglob+Lloc.
p p p f f f
With this assumption, we have
ϕ|A|
γ+(Lloc+γLloc∥Q∥ ) <1,
f p ∞ |X|c −Lloc
min p
and
ϕ|A|(|X|c −Lglob+Lloc)Lglob+γLglob∥Q∗∥
min p p f p µ ∞ +Lglob+1−|X|c <1,
|X|c −Lglob 1−γ p min
min p
which will give the strict contraction property we need in the next propositions.
Proposition A.2. Suppose Assumption 4.1 and A.1 hold. Then for any given µ, Q˙ =T (µ,Q ,µ∗ϕ ) has
t 3 t Qt,µ
a unique GASE, that we will denote by Q∗ϕ.
µ
Proof. We show that Q(cid:55)→B Q is a strict contraction. We have:
µ,µ∗ϕ
Q
∥B Q−B Q′∥ ≤∥B Q−B Q′∥ +∥B Q′−B Q′∥
µ,µ∗ϕ µ,µ∗ϕ ∞ µ,µ∗ϕ µ,µ∗ϕ ∞ µ,µ∗ϕ µ,µ∗ϕ ∞
Q Q′ Q Q Q Q′
≤γ∥Q−Q′∥ +(Lloc+γLloc∥Q∥ )∥µ∗ϕ−µ∗ϕ∥
∞ f p ∞ Q Q′ 1
ϕ|A|
≤γ∥Q−Q′∥ +(Lloc+γLloc∥Q∥ ) ∥Q−Q′∥
∞ f p ∞ |X|c −L ∞
min p
(cid:18) (cid:19)
ϕ|A|
≤ γ+(Lloc+γLloc∥Q∥ ) ∥Q−Q′∥ ,
f p ∞ |X|c −Lloc ∞
min p
whereweusedtheboundderivedintheproofofProposition4.6. Thisshowsthestrictcontractionproperty.
As a result, by the contraction mapping theorem [Sell, 1973], a unique GASE exists and furthermore by
[Borkar and Soumyanatha, 1997, Theorem 3.1], it is the limit of Q .
t
Proposition A.3. Under assumptions 4.1 and A.1, µ˙ =P (µ ,Q∗ϕ,µ˜∗ϕ ) has a unique GASE, that we
t 3 t µt Q∗ µϕ t,µt
will denote by µ∗ϕ.
Proof. We show that
µ(cid:55)→Psoft-minϕQ∗ µ,µ,µ˜∗ Qϕ
∗ µϕ,µµ is a strict contraction. We have, denoting µ˜∗ Qϕ
∗ µϕ,µ
as µ′
µ
for
the sake of brevity:
∥Psoft-minϕQ∗ µ,µ,µ′ µµ−Psoft-minϕQ∗ µ1,µ1,µ′
µ1µ 1∥
1
≤|Psoft-minϕQ∗ µ,µ,µ′ µµ−Psoft-minϕQ∗ µ′,µ,µ′
µ1µ∥
1+∥Psoft-minϕQ∗ µ,µ,µ′ µ1µ−Psoft-minϕQ∗ µ1,µ,µ′
µ1µ∥
1
+∥Psoft-minϕQ∗ µ1,µ,µ′ µ1µ−Psoft-minϕQ∗ µ1,µ1,µ′
µ1µ 1∥
1
≤∥P (µ,Q∗,µ′ )−P (µ,Q∗,µ′ )∥ +∥P (µ,Q∗,µ′ )−P (µ,Q∗ ,µ′ )∥
3 µ µ 3 µ µ1 1 3 µ µ1 3 µ1 µ1 1
+∥P (µ,Q∗,µ′ )−P (µ ,Q∗ ,µ′ )∥
3 µ µ1 3 1 µ1 µ1 1
≤Lloc∥µ′ −µ′ ∥ +∥P (µ,Q∗,µ′ )−P (µ,Q∗ ,µ′ )∥ +(1+Lglob−|X|c )∥µ−µ ∥
p µ µ1 1 3 µ µ1 3 µ1 µ1 1 p min 1 1
Llocϕ|A|
≤ p ∥Q∗ −Q∗ ∥ +ϕ|A|∥Q∗ −Q∗ ∥ +(1+Lglob−|X|c )∥µ−µ ∥
|X|c −Lglob µ µ1 ∞ µ µ1 ∞ p min 1 1
min p
(cid:32) ϕ|A|(|X|c −Lglob+Lloc)Lglob+γLglob∥Q∗∥ (cid:33)
≤ min p p f p µ ∞ +Lglob+1−|X|c ∥µ−µ ∥ ,
|X|c −Lglob 1−γ p min 1 1
min p
which shows the strict contraction property. As a result, by contraction mapping theorem [Sell, 1973], a
unique GASE exists and furthermore by [Borkar and Soumyanatha, 1997, Theorem 3.1], it is the limit of
µ .
t
16B Proof of Lemma 4.12
Proof. Recall system (24):
(cid:40) µ˜(x,a) =µ˜(x,a)P˜α,µ,µ˜(x,a)
(x,a)
µ=µPα,µ,µ˜(x,α(x)).
Firstly, for fixed µ, by a similar argument as in Proposition 4.6, µ˜(x,a) (cid:55)→µ˜(x,a)P˜α,µ,µ˜(x,a) is a strict contrac-
(x,a)
tion, which guarantees that µ˜(x,a) =µ˜(x,a)P˜α,µ,µ˜(x,a) has unique solution. Then, we can deduce that, for all
(x,a)
x, µ˜(x,α(x)) satisfies
µ˜(x,α(x)) =µ˜(x,α(x))P˜α,µ,µ˜(x,α(x)) =(cid:88) µ˜(x,α(x))(x′)p(·|x′,α(x′),µ,µ˜(x,α(x))). (27)
(x,α(x))
x′
Let us denote µ˜(x,α(x)) by νx. We want to show that νx is the same for all x. By definition in (14), we can
show that (27) can be written as νx = νxPα,µ,νx. Since νx (cid:55)→ νxPα,µ,νx is a strict contraction, we have a
unique solution for νx = νxPα,µ,νx and, consequently, νx is independent of x. Now the second equation in
(24) is well-defined.
Next we want to show that the solution to (24) is unique. Let us consider the following system first:
(cid:40)
νx =νxPα,µ,νx
(28)
µ=µPα,µ,νx
Theabovesystem(28)isthesystem(24)witha=α(x),andwecanshowithasauniquesolutionasfollows.
Assuming ν′x and µ′ also satisfy (28), then
∥νx−ν′x∥ 1+∥µ−µ′∥
1
=∥νxPα,µ,νx −ν′xPα,µ′,ν′x ∥ 1+∥µPα,µ,νx −µ′Pα,µ′,ν x′ ∥
1
≤∥νxPα,µ,νx −ν′xPα,µ,ν′x
∥
+∥ν′xPα,µ,ν′x −ν′xPα,µ′,ν′x
∥
1 1
∥µPα,µ,νx −µ′Pα,µ′,νx∥ 1+∥µ′Pα,µ′,νx −µ′Pα,µ′,ν x′ ∥
1
≤(Lloc+1−|X|c )∥νx−ν′x∥ +Lglob∥µ−µ′∥
p min 1 p 1
+(Lglob+1−|X|c )∥µ−µ′∥ +Lloc∥νx−ν′x∥
p min 1 p 1
≤(2Lloc+1−|X|c )∥νx−ν′x∥ +(2Lglob+1−|X|c )∥µ−µ′∥
p min 1 p min 1
which implies that
(|X|c −2Lloc)∥νx−ν′x∥ +(|X|c −2Lglob)∥µ−µ′∥ ≤0.
min p 1 min p 1
By Assumption 4.5, we have (|X|c −2Lloc)>0 and (|X|c −2Lglob)>0, and consequently we deduce
min p min p
∥νx−ν′x∥ =∥µ−µ′∥ =0, so that νx =ν′x and µ=µ′, which proves that (28) has at most one solution.
1 1
Then, wecanshowthatthereexistsauniquesolutionto(24). Assumethatthereexistsanothersolution
(µ˜′′(x,a),µ′′) with µ′′ ̸= µ satisfying (24). Consequently, (µ˜′′(x,α(x)),µ′′) is a solution to (28) which is a
contradiction with the uniqueness for (28). As a result, there is a unique µ solving (24), and then by the
strict contraction property of µ˜(x,a) (cid:55)→ µ˜(x,a)P˜α,µ,µ˜(x,a) , uniqueness of µ˜(x,a) is obtained. As a result, the
(x,a)
system (24) has unique solution (µ˜(x,a),µ).
Finally,wewanttoshowthatµ=νx. Sinceν (cid:55)→νPα,ν,ν isastrictcontraction,theequationν =νPα,ν,ν
has a unique and well-defined solution ν. Then, one can check that µ = νx = ν is a solution of (28) and
conclude by its uniqueness property.
17