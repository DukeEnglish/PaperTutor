From Neurons to Neutrons: A Case Study in Interpretability
OuailKitouni*12 NiklasNolte*3 V´ıctorSamuelPe´rez-D´ıaz1456 SokratisTrifinopoulos12 MikeWilliams12
Abstract learninghasfocusedonunderstandingthelow-dimensional
representationslearnedbythesemodels,withaparticular
MechanisticInterpretability(MI)promisesapath
emphasisondisentangledrepresentationsthatseparatethe
towardfullyunderstandinghowneuralnetworks
underlying factors of variation in the data (Bengio et al.,
make their predictions. Prior work demon-
2013;Higginsetal.,2018;Locatelloetal.,2019). Disen-
strates that even when trained to perform sim-
tanglementaimstolearnrepresentationswhereeachlatent
ple arithmetic, models can implement a vari-
dimensioncorrespondstoasemanticallymeaningfulfactor,
ety of algorithms (sometimes concurrently) de-
suchthatvaryingonedimensionwhilekeepingothersfixed
pending on initialization and hyperparameters.
producesinterpretablechangesintheinputspace(Burgess
Doesthismeanneuron-levelinterpretabilitytech-
etal.,2018;Chenetal.,2018;Kim&Mnih,2018).
niqueshavelimitedapplicability? Wearguethat
high-dimensionalneuralnetworkscanlearnlow- Given the success of deep learning at modeling a wide
dimensionalrepresentationsoftheirtrainingdata varietyofdata,itseemsplausiblethatinterpretabilitycan
thatareusefulbeyondsimplymakinggoodpre- helpuslearnfromthesemodelsthatareeffectivelydomain
dictions. Suchrepresentationscanbeunderstood experts.1 Inthiswork,weinvestigatetheabilityofmachine-
throughthemechanisticinterpretabilitylensand learnedalgorithmstore-deriveinsightsinhuman-developed
provideinsightsthataresurprisinglyfaithfulto understanding, taking nuclear theory as a case study of
human-deriveddomainknowledge.Thisindicates mechanisticinterpretability.
thatsuchapproachestointerpretabilitycanbeuse-
Modern machine learning posits the manifold hypothe-
fulforderivinganewunderstandingofaproblem
sis(Bengioetal.,2013),theideathatmostnaturaldatawe
frommodelstrainedtosolveit. Asacasestudy,
tendtocareaboutlivesinalow-dimensionalmanifoldem-
weextractnuclearphysicsconceptsbystudying
beddedinthehigh-dimensionalmeasurementspace. Thisis
modelstrainedtoreproducenucleardata.
observedacrossmodalitiesand,morerecently,inlanguage
modelingwherelow-rankrepresentationsareubiquitousin
fully-trainedlargelanguagemodels(Huetal.,2021;Agha-
janyanetal.,2021;Lietal.,2018;Dettmersetal.,2023;
1.Introduction
Zhang et al., 2023). Due to the nature of the data or the
variousimplicitbiasesofthemoderndeeplearningtraining
The scientific process involves understanding high-
procedures,neuralnetworkslearncompactrepresentations
dimensionalphenomena,oftenwithlarge-scaledata,andde-
thatliveinasmallsubspaceoftheinputs. Interpretability
rivinglow-dimensionaltheoriesthatcanaccuratelydescribe
indeeplearninghasalwaysbeenanactiveareaofresearch
andpredicttheoutcomeofobservations. Thereismount-
(Kadir&Brady,2001;Zhangetal.,2021)buttheprocess
ing evidence that modern machine learning operates in a
ofunderstandinghowneuralnetworksoperatetomakepar-
similarfashion,takinglarge-scale,high-dimensionaldata
ticularpredictions(macroscopicphenomena)byuncovering
and deriving low-dimensional representations from them.
thealgorithmstheyimplement(microscopicphenomena),
For instance, recent work on the interpretability of deep
isanascentfieldofdeeplearningbuiltaroundtheideathat
*Equalcontribution 1NSFInstituteforArtificialIntelligence neuralnetworks,despitetheirscaleandcomplexity,canbe
and Fundamental Interactions (IAIFI) 2Massachusetts Institute interpretedandunderstood(Elhageetal.,2021;Olah,2022).
ofTechnology3FAIRatMeta4HarvardJohnA.PaulsonSchool
Here,wefurtherpositthatnotonlycantheybeunderstood,
ofEngineeringandAppliedSciences5CenterforAstrophysics
buttheycanalsobeusedtosaysomethingusefulaboutthe
|Harvard&Smithsonian6SchoolofEngineering, Scienceand
Technology,UniversidaddelRosario.Correspondenceto:Ouail natureoftheproblemtheyaimtosolve. Inthefollowing,
Kitouni<kitouni@mit.edu>. we will investigate whether mechanistic approaches can
Proceedings of the 41st International Conference on Machine 1Thereareofcoursesomecaveatsheresuchasthequestionof
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by therobustnessoflearnedrepresentations.
theauthor(s).
1
4202
yaM
72
]GL.sc[
1v52471.5042:viXraFromNeuronstoNeutrons:ACaseStudyinInterpretability
111 414 1 14 94 18 4 4 17 16 4 15 4 4 14 13 42 13 11 3 10 139 318 31 371 631 53 411 3 3311 222 11 021 2 981 21 72 612 512 42311 2211 111 01 1 91 8111 711 6115411 113 112 0 11 1009 0 108 1 17 0 16 0 105 0 14 903 92 9091 98 7 90 96 95 94 93 928 18 08 98 88 78 68 58 4388 2177 07 97 87 7675774377 26160 698 67 6 66 65 64 63 261 5 50 59 58 57 56 55 54352 5144 0948474 64 54444 3243 13 0933 83 73653 34 333 21 20 29 28 27 26 25 24 23 2 2 21 11 011 96 8711 51 41 31 219 10 111 41 41 4 91 4 81 41 741 641 541 441 34121313 103 1398 13 17 36 135 134 133 132 121 120 129 128 127 126 2 15 2 14 3 2 12 2 11 110 11911811711 611 511 411 311 211 101 001 901 801 7011 601 501 0 49 0 39 0 29 9 19 8 09796 95 94 93 92 81 80 89 88 87 86 85 84 83 82 71 70 79 78 77 7675747 37 26 16 0966 86 76 66 566 46 355 215 05 985 75 65554 532 5 410 49 48 47 46 45 4 44 3 42 4 31 30 39 83733 653 43 33 23222 12 0982 7622 52 432212 1 10 9 18 17 16 115 431 11 29
10
Figure1.Projectionsofneutronnumberembeddingsontotheirfirstthreeprincipalcomponents(PCs).Modelsweretrainedonnuclear
data(left)orahuman-derivednucleartheory(right).X-axis:1stPC,Y-axis:2ndPC,color:3rdPC.Numbersindicatetheneutronnumber
(N)ofeachnucleus(seeSetupinSection3).Thehelixstructureencodesinsightsaboutnuclearphysicsdiscussedinsubsequentsections.
uncoverscientificknowledgederivedfromtheprediction the training data. In this work, we explore this shift in
taskthemodelistrainedon. Inotherwords, wepropose perspectiveinahighlyspecializeddomain.
expandingtheviewonMIfrom“Howdoesamodelmake
First,wewillrevisitsomeofthemechanisticinterpretability
predictions?”toinclude“Whatcanthemodeltellusabout
effortsformodelstrainedtoperformmodularaddition. In
thedata?”
Figure2(left),weshowtheprojectionoftheembeddings
InSection2,wediscusspriorworkonMIinmodulararith- ontotheirfirsttwoprincipalcomponents(PCs). Longaf-
meticandshowanintuitiveexampleofhowitcanbeused terfullgeneralizationandcircuitcleanup(seeNandaetal.
to understand the algorithm that a simple MLP can learn (2023)foradefinition),thealgorithmlearnedbythenetwork
toperformmodularaddition. Transitioningfrommodular involvesasimplevectoraverage. Thiscanbevisualizedeas-
arithmetic,Section3introducesthenuclearphysicsproblem ilybyprojectingthefirstlayeractivationsdowntothefirst
we will be tackling, explains the model architecture, and twoprincipalcomponents,uniformlysamplingpointsina
summarizessomekeypropertiesoftheestablishedphysical two-dimensionalgrid,andfeedingthembackintothenet-
modelsusedbyphysicists. Then,inSection4wemotivate workafterareversetransformationtotherightspace. This
and explain the approach we take to interpret the models procedure,whichwewillhenceforthrefertoaslatentspace
trainedonthenuclearphysicsdata. Finally, inSection5, topography (LST), gives what the output of the network
weinterpretandextractubiquitousconceptsfromthemodel wouldhavebeenaswemoveinaparticular2Dsubspace
representationsandshowthatthesearesimilartothemost of the embeddings. As it turns out, this is quite informa-
important human-derived concepts. For example, in Fig- tive. InFigure2(right),weoverlaythe2Dprojectionsof
ure1weshowaspiralpatternthatemergesinthemodel’s theembeddingsforeachintegerontopofourlatentspace
representation when trained on nuclear data is similar to map and find that in order to compute the modular sum
the one that arises when training instead on pseudo data ofnumbers,thenetworkfirstcomputesthevectoraverage
obtainedfromahuman-derivednucleartheory. betweentheembeddingsandreturnstheindexoftheslice
theresultingsumfallsinto. Thisfullyexplainstheneural
networksolutiontotheproblembutalsoshedslightona
2.ModularArithmeticPrimer
newvisualalgorithmformodularaddition. Simplyarrange
Arecentwaveofresearchininterpretabilityhasfocusedon numbersaroundacircle,createslicesbetweeneverytwo
algorithmictaskssuchasarithmeticorcheckingtheparity
of a sequence. This has good reason: These datasets are
extremelyclean,aIrnbitiiatlirzaatrioyn(i0nitesriaztioen,s)andnon-Otvreirvfititianlg(e1n00o0uitegrahtiotnos) RepresentationLearning(20000iterations)
trainacc:0.0—valacc:0.0 trainacc:1.0—valacc:0.1 trainacc:1.0—valacc:1.0 Accuracy-train:1.0,validation:0.9
showavarietyofinterestingphenomena. Modelstrained 10
to perform modular arithmetic have been shown to yield
8
relativelyinterpretablestructuresintheirembeddings(Liu
6
etal.,2022). Priorworkhasshownthatthealgorithmsby
which the trained models perform the task can be recov- 4
eredpreciselybyunderstandingthemodelmechanistically 2
attheactivationandneuronlevel. Furthermore,thisinter-
0
pretationcanbeusedtoprovideprogressmeasuresforthe
Figure2.(left)Principalcomponentprojectionofmodularaddi-
model’sabilitytogeneralize(Nandaetal.,2023). Beyond
tionembeddings. Thecircularstructuremirrorshuman-derived
thesedirections,wecanleverageinterpretabilitynotonly
approachesusedtoteachmodulararithmetic.(right)Modeloutput
tounderstandmodelsbutalsotoextractknowledgefrom inregionsofthephasespace.From(Liuetal.,2022).
2FromNeuronstoNeutrons:ACaseStudyinInterpretability
points,labeltheslicesfollowingtheschemegivenbythe tails). As a form of regularization, we often also predict
networkinFigure2,thenfinallyobtainthesumofanytwo theinputvaluesZ andN thatareobscuredduringembed-
numbersbyfindingthemidpointandreadingoffthelabel ding. Thiscreatesamultivariateregressiontaskacrossup
oftheslice. to10targetobservablesfor3363totalnuclei. Oneofthe
mostimportantnuclearobservablesisthebindingenergy.
In the following sections, we demonstrate the feasibility
Many models have been developed in the literature with
ofknowledgeextractionbeyondmodulararithmetic,using
theliquid-dropmodelbeingtheprototypicaldescriptionof
nuclearphysicsasacasestudy. Researchershaveinvested
thenucleus. Aconsequenceofthemodelistherenowned
significant effort in understanding and modeling this do-
Semi-EmpiricalMassFormula(SEMF)(Weizsa¨cker,1935):
main over several decades. By training models on such
data,weinvestigatewhetherknownphysicsconceptscan (Z2−Z)
E = a A −a A2/3−a (1)
beidentifiedthroughinspectionoftheirrepresentations. B (cid:124)(cid:123)V
(cid:122)(cid:125)
(cid:124)S
(cid:123)(cid:122) (cid:125)
C A1/3
Volume Surface (cid:124) (cid:123)(cid:122) (cid:125)
Coulomb
3.BeyondArithmetic: APhysicsCaseStudy (N −Z)2
−a +δ(N,Z),
A A
Why Nuclear Physics? We choose to explore nuclear (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Pairing
physics as a case study for several compelling reasons. Asymmetry
First,physicistshavestudiedvariousaspectsofthisdatafor whereA=N +Z isthetotalnucleonnumber. Thecoeffi-
decadesandhavedevelopedsimpleyeteffectiveexpressions cientsa aredeterminedempirically. AppendixCcontains
∗
andconceptsthatexplainthedatawell. Thisprovidesause- moredetailedexplanationsofeachterm. Thisformulais
fulframeofreferenceandaplausibleapproximate“ground fairlyaccurateandtheoreticallywellmotivated. Figure3
truth”forcomparison. However,understandingthedatare- showsE forboththedataandtheSEMF.
B
mainsasignificantchallenge,withseveralphenomenastill
unaccountedforbycurrenttheoriesandlong-standingques- Semi-Empirical Mass Formula Data
tionspersisting. Thiscombinationofestablishedknowledge 175
8641
andongoingscientificchallengesmakesnuclearphysicspar-
150 8486
ticularlyinterestingforinterpretabilityresearch. Tofurther
motivateourchoice,considerasimpleprincipalcomponent 8354
125
projectioninFigure1,extractedthesamewayasFigure2 8233
(left),buttrainedonnuclearphysics. Asurprisinglyperi- 100 8124
odicandcontinuoushelicalstructureemerges,suggesting
75 8018
anopportunityforinsightfulinterpretation.
7891
50
Theremainderofthissectionwillbeorganizedasfollows: 7747
First,weprovideadescriptionoftheexperimentalprocess 25
7550
andthe datatoestablish context. We alsobrieflydiscuss
5390
existing human-derived knowledge about the data. Next, 25 50 75 100 25 50 75 100
Z Z
wetakeacloselookattheinputembeddings. Embeddings
havebeenshowntocarrysignificantstructureinmodular Figure3.BindingenergypernucleonasgivenbytheSEMFfor-
arithmetic training (Liu et al., 2022) and are a promising mula(left)andobservedinmeasurements(right).
firststepformodelinterpretation. Finally,westudymodel
featuresextractedfromthepenultimatelayeractivationand
Setup Weareinterestedinmakingpredictionsoftheform
comparethemtoknownphysicstermstogaugesimilarities
T(Z,N)=?,whereT isthetaskorobservablebeingcon-
betweenmodel-derivedandhuman-derivedfeatures.
sidered, andZ andN areintegersuniquelyidentifyinga
nucleusonwhichpredictionswillbemade. Similartothe
DatasetandNuclearTheory Nuclei,thecoresofatoms, algorithmictaskssetup,inputsaretokenizedandstackedin
haveanarrayofinterestingpropertiesthatdependontheir asequence. Eachtokenisembeddedintoad-dimensional
composition. Likeelementsintheperiodictable,theycan space. Thesequenceofembeddings(E ,E ,E )isthen
Z N T
bevisualizedonatwo-dimensionalgridandarecharacter- fedintothemodelwhichistaskedwithcompletingthese-
izedbytwointeger-valuedinputs: thenumberofprotons quenceusinganumericalprediction. Specifically,thelast
(Z)andneutrons(N),rangingfrom1to118and0to178, tokenpredictioniscomparedagainstthetargetnumerical
respectively. Fromtheseinputs,weaimtopredictseveral valueandpenalizedwithamean-squared-errorloss. Similar
continuoustargetpropertiesofnuclei: bindingenergy(E ), toZhongetal.(2023),wefindthatusingattentionprovidesa
B
chargeradius(R ),andvariousseparationenergies(Q , qualitativelydifferentsolutionthaninput-independentatten-
ch A
Q ,Q ,Q ,S ,S ;seeAppendixC.4formorede- tion(Hassidetal.,2022). Forthepurposesofthispaper,we
BM BMN EC N P
3
N
]Vek[
)N+Z(
/
EBFromNeuronstoNeutrons:ACaseStudyinInterpretability
willfocusonfixedattentionwherealltokensareattended theperformancewitharelativelysmallnumberofPCs. Fig-
toequally2(seeAppendixB). ure4showstheerrorasafunctionofprincipalcomponents
at different layers. To get this prediction, we project the
Inallourexperiments,wewillconsideroneorseveralob-
activations(ortheembeddings)ontotheirfirstkprincipal
servablestopredictwithvariousmodels. Theperformance
components(orderedbyvariance)andsethigherordercom-
ofthemodelswillgenerallybemeasuredbyaRoot-Mean-
ponentstozero. Thenweinverttheinitialprojectionand
Squareerror(RMS)onaholdoutset.3 Wewillalsopredict
considertheresultthenewactivationthatissentthrough
someusefulunitlessquantitiessuchastheneutronandpro-
therestofthenetwork.
tonnumbers.
Objectives Ourgoalwillbetounderstandhowthemod- 104
els’generalizingsolutionswork,extractusefulrepresenta-
tions from them, and compare those solutions to what is
well-knowninnucleartheory. Toascertainthesourceofthe Z embed
learnedrepresentations,wecantrainourmodelondifferent N embed
103
tasksandcollectresultsfromthefollowingexperiments: (1) penultimate layer
SEMF
Trainmultiplemodelswithdifferentseedsondifferentdata
full model
splits to understand the properties of generalizing versus
memorizing solutions. (2) Study the internal representa- 100 101 102
Number of principal components kept
tionsofmodelstrainedondifferenttaskstounderstandthe
mechanisticeffectsofmulti-taskingongeneralizationi.e.
Figure4.Bindingenergypredictionerrorasafunctionofnumber
whatarethefeaturesoftherepresentationsthatgeneralize
ofPCsusedatdifferentlayers.
and where do they come from? (3) Compare the neural
network-derivedconceptswithhuman-derivedmodels.
ThebehaviourobservedinFigure4seemstobefairlyuni-
4.ArePrincipalComponentsMeaningful? versal, albeittovaryingdegrees. Forinstance, Ashkboos
etal.(2024)recentlyutilizedPCAtoincreasesparsityin
PrincipalComponentAnalysis(PCA)isawidelyuseddi-
languagemodelsbyprojectingactivationstotheirprincipal
mensionalityreductiontechniqueduetoitssimplicity. How-
componentswithoutlosingsignificantperformance.
ever, it relies on several assumptions that, when violated,
canresultinerroneousconclusions. Thereisextensiveliter-
4.2.Evidence2: RichStructure
aturediscussingvariousPCApitfalls,suchasthecomplex
relationship between oscillations and PCA (Novembre & Phantomoscillationsaresinusoidalpatternsthatcanemerge
Stephens,2008;Antognini&Sohl-Dickstein,2018;Lebe- in PCA even when the underlying data does not contain
dev et al., 2019; Proix et al., 2022). Remarkably, these oscillations (Shinn, 2023). They can arise due to noise,
studiesreportedinstanceswherenon-oscillatorydataexhib- smoothnessacrossacontinuumliketimeorspace,orsmall
itedoscillatoryprincipalcomponents.Ifthisphenomenonis misalignments/shiftsacrossobservations. Phantomoscilla-
prevalentacrossvarioustypesofdata,itiscrucialtoensure tionscharacteristicallyemergeatmultiplefrequencies,with
itdoesnotaffectourresults. each principal component exhibiting a distinct frequency
and lower frequencies explaining more variance. In this
4.1.Evidence1: PCsCaptureMostofthePerformance work, we found that PC features exhibit unique patterns
thatdifferfromthoseexpectedinthecaseofnoise. Asob-
Thereisevidenceintheliteraturethatmodelsoperateon
servedintheprevioussection,highlyinformativestructures
amuchsmallersubspacethantheirfulldimension. Low-
emergeinthefirsttwoPCsofembeddingswhenlearning
Rankadaptation(Huetal.,2021)isanexampleshowing
modulararithmetic. UsingFigure2asareference,Liuetal.
thatmuchoftheperformancegainsfromsupervisedfine-
(2022)andZhongetal.(2023)hypothesizedthecomplete
tuningcanbeobtainedbytrainingalow-rankapproximation
algorithmusedtoperformthemodularadditiontask. Inthe
ofthemodel. IfthePCsextractedweremeaningless, we
contextofnuclearphysics,similarlyrichstructuresemerge
should see large performance gaps between the original
duringtrainingbeyondwhatwouldbeexpectedinthecase
modelandonethatsolelyreliesonasubsetofthePCsin
ofnoise. Figure5displaysthefirsttwoPCsofprotonnum-
makingpredictions.However,wedoindeedrecovermostof
berembeddingsextractedfromageneralizingmodel. This
2Withoutresidualconnections,thismodelcouldbewrittenas clearly showcases features such as an even-odd split and
afeedforwardMLP. periodicity,whichwefurtherexploreinsubsequentsections.
3ErrorisinunitsofkeVforenergiesandfmforlengths.
4
]Vek[
rorrE
BEFromNeuronstoNeutrons:ACaseStudyinInterpretability
PC0 vs PC1 PC0 vs PC2 embeddings,plottedagainsteachother. Theobservedstruc-
54
99 99 98 97 96
5
94
9
3929 1808 988 88 78
68548 382
81 70 79 78 77 76
7 7574 3727
166 096 86 76 666 546 3626 1505
9585 7655
5 53 52 5 1 40 4 9 48
4764444
3424 1033 98 33 7 36 53 343 3233 1
202 98
99 9 998 979 6 5 949 3929 180 8 9888 8786 8548 3828 1707 9787 7767 7 57 4 3727 1660 9686 766 6 654 6 362 6 1 50 5 9585 76 55 5545 3525 1 40 4 9484 7464 54 44 3424 10 33 98 33 7 36 53 343 32 33 1 202 98
t
p m f i
snu
o tr ro
gr
ro
ue
dt h o
c,
o e i
tba
n l g us sh
rh
en t ee
rre
u ra
vl
im
ii
e
snnx
dub pe
.m(
e d 4
ao
r . rbs
Tr
tT, ie
hs
cri
hp
i us s
sei
,
lor aoeca
n ro m
rl
le
d)
l yo p
eop
r h r
pfa
ias
rnt
t oc
st
h
ge
ih
nzer
e i
oin
sm nm
uaga
neo
ls
csts
ts
h o
erto
a e ds
ac
n ,t c
pi
r
sa
il pi i
net
k t
ae
aii o r
td
rn heng n n
ew
s u tf ,t
hi
me
ot
a
ih
a ge nlt i
hrui
dg
in
-r ch
vtc
e a
htr
as e l
ee
rri
oa
in h ah
rs
e
ndui
t
ln
h e ce
ig
r x ee s -
45 primary components of the neutron number embeddings
fromFigure1. Notethatthecolorinthiscaserepresentsthe
Figure5.PCprojectionsofZembeddingsfromamodeltrained thirdPC.
onalltasks.Thecolorhueisamonotonicfunctionoftheproton
numberZ,tobeabletoquicklyassessthepresenceoforder. Notably,E B hasastrongcorrelationwithbothN andZ,as
seeninthefirsttermoftheSEMF.Therefore,itseemsplau-
siblethattheinductivebiasoforderingneutronandproton
5.Experiments
numbersintheembeddingspaceisparticularlybeneficial.
Tounderstandthemodelbetter,considerFigure6,thelatent
5.1.Embeddings
spacetopographyofZ embeddings,constructedsimilarly
Growing evidence, including studies on language model toFigure2formodularaddition. ItshowsthepredictedE
B
analogies(e.g.,the“king−man+woman=queen”analogy) asacoloredbackgroundtothescatterplotofthetwohigh-
(Mikolovetal.,2013)suggeststhepresenceofinterpretable estvarianceprimarycomponentsintheZ embeddingsfor
androbuststructuresintheinitialembeddinglayersofneu- N =100. Thedominatingeffectisthemonotonicincrease
ralnetworks. Wecanreasonablyexpectsimilarphenomena inbindingenergywhenmovingfromrighttoleftinPC0,
tooccurinnuclearphysics,andthuswewillcloselyexam- whichcorrespondstothefactthatE scalesasA=Z+N
B
inetheneutronandprotonnumberembeddingsfortrained to leading order (this is known as the volume term in the
models. SEMFEquation(1)).
PropertiesofModelsThatGeneralizeWell Modifying
Z PCs, fixed N=100 the model architecture and hyperparameters significantly
canresultindifferentgeneralizingalgorithms. Weexplore
2 8 asmallregionofthealgorithmicphasespaceanddiscover
1
88 821
0
555
765 55 54 53
521
0
33
12 02 98 2 27 26
5
19 076
5
0.7 wth ha it cg hen we era el niz ui mng ers ao tl eut hio en res .shareasetofcommonproperties,
0 11111 111 111 11 111 181 11 1 176 11 5 01 04 30 1 20 10 11 00 90 9 896 07 905 904 93 9 9827 91 6 90
54
9932818
0888 88 76543 77 7 78 79 77 6
75
4
3666 25 15 098 4 4 49 8 47 460
5
333 63 43 532 24 2 23
2
11 21 4
3
00 .. 56 1 s
t
dht. ar
a
tuH
t
ac .ie t tu El ri r
e
xc e pi pt ri ey en
rs
iet mh nW e
ets
ne n
a
te sa u ct rot t ee r
m
vom en pp aea lt ln
l
tt
i
ho d
n ig
si ps gr so o
e
tl rta
o
uot
m
ce n
te
ut e th rrme eicbo aee pr xdi pg pd ei lin
a
an
n
rgo sasf
t
w, it oah hnne ed
o
nh ffie ptl n rhi edx
e
-
1 89 7 772 661 606
6
986 766 543 4 44 43 42 431033 987 2201
1198
1615
1413 1
2
0.4
d thic eti hn eg lib xi ,n wd ein pg ae rn amerg ety e. rT izo ee il tu ac nid da pte erh tuow rbt ph ae ram mo ed te el rsut ti oliz ue ns
-
17 derstandtheireffects(adetailedstudywithvisualizationis
2 0.3 showninAppendixA).Wefitahelixtothevisuallymost
2 0 2 4
helix-likeportionof3DPCAprojectionsasillustratedin
PC0
Figure8. Thefitsmaptotheprojectionswellandenable
Figure6.Projectionofprotonnumber(Z)embeddingsontothe
us to isolate the effect of the different parameters of the
firsttwoprincipalcomponents(PCs),superimposedontheneural
helix. Forinstance,wenotethatincreasingthepitch(length
network’sbindingenergypredictions.ThebindingenergyLSTis
ofthecentralaxis)elongatesthehelix,causingaconstant
computedasafunctionofthefirsttwoPCs,whiletheremaining
offsetinpredictions,similartothevolumetermintheSEMF.
componentsarefixedattheirmeanvalues.Blackdotsindicatethe
Reducingthelengthhastheoppositeeffect. Increasingthe
positionsoftheZembeddingsinthisspace,withthecorrespond-
radius“sharpens”thedownwardarcsinpredictions,likely
ingprotonnumbersannotatednexttoeachdot. Thecolorscale
representsthepredictedbindingenergyvalues,withbrighterhues linkedtotheSEMF’sasymmetryterm,withradiuscontrol-
denotinghigherenergies. lingtheprefactor. Thehelixstructureprovidesaninterest-
inggeometricexplanationofhowthemodelrepresentsthe
data. Inparticular,itpresentsacompletedescriptionofthe
Giventhelargedimensionalityoftheembeddings,weana-
lyzethelatentrepresentationsusingalow-dimensionalPCA
4While the number ordering could be expected for models
projection,asmotivatedinSection4. Figure5illustrates whereN andZareamongthepredictiontargets,itpersistseven
thethreehighestvarianceprincipalcomponentsofproton inmodelswherethosetargetsareabsent.
5
1CP
]delacs[
ygrenE
gnidniBFromNeuronstoNeutrons:ACaseStudyinInterpretability
Z PCs for N=20 N=25 N=30 N=35
21 22 23 24 25 21 22 23 24 25 21 22 23 24 25 21 22 23 24 25
26 26 26 26
27 27 27
20 20 20 20
19 27 19 28 19 28 19 28
9 18 18 18 18
17 17 29 17 29 17 3837 29
16 10 16 30 16 30 36 35 30
31 31
14 15 13 12 11 14 15 13 12 11 14 15 13 34 33 32 34 33 32
PC1 PC1 PC1 PC1
Figure7.Zembeddingsprojectedontoprincipalcomponents1and2(countingfrom0)givenmultiplefixedneutronnumbers.Foreach
N,onlyZembeddingsareshownforwhichactualnucleiexist.Thebackgroundshowsthebindingenergypredictionofthemodelasa
functionofPC1andPC2,whereotherprimarycomponentsarefixedtotheirmeanvalue.BrightermeansmoreE .
B
fromthisoptimalvalue,theE /Adecreasessmoothly.This
Z embeddings N embeddings B
Original Data trendcanbeobservedinFigure3,whereforeachslicealong
Fitted Helix the N axis, there is a peak in E /A around a central Z
B
value(andviceversaforslicesalongtheZ axis). Conse-
quently,foreachN,thereshouldbeacontinuousstripof
Z embeddings, with one embedding marking the highest
E /Avalue,correspondingtothemoststablenucleusfor
B
thatparticularN. SinceeachN requiressuchacontinuous
strip,theentiresequenceofZ embeddingsshouldforma
continuousstructure.
Figure8. FittingahelixtothePC-projectedembeddings.
This is where the helix structure, which can be viewed
as stacked circles, offers a compact and efficient way of
SEMF—itselfmotivatedbygeometry(AppendixC.2)and achievingthiscontinuity. ByarrangingtheZ embeddings
basicphysicsprinciples—andyieldsparticularlyaccurate along a helical path, the model ensures that for each N,
fits,asshowninAppendixA. there is a smooth progression of Z values, with the most
stable element located at the optimal position within the
Figure7presentsacomplementaryviewtoFigure6,with
latentspace. Thehelicalstructureallowsforacontinuous
the latent space topology displayed across the next two
representationofthebindingenergylandscape,capturing
principalcomponents(PC1andPC2). Thisperspectiveis
thelocalvariationsandthestabilitypeaksacrossdifferent
obtained by rotating the viewpoint by 90 degrees out-of-
N values.5
the-pagecomparedtoFigure6. Foreachpane,theneutron
number(N)isfixedtoadifferentvalue,increasinginincre-
mentsof5betweenadjacentpanes. Theprotonnumber(Z) 2. Orderedness Wehypothesizethatorderingnumbersin
thefirstfewprincipalcomponentsisindicativeofgeneraliza-
embeddingsdisplayedineachpanearelimitedtothosecor-
respondingtophysicallyexistingnuclei,i.e.,(Z,N)pairs tionandinvestigatetherelationshipbetween“orderedness”
in embedding structures and generalization performance
presentinthedataset. Thebackgroundisproducedbyeval-
(seeAppendixB.1forthetimeevolutionofthisproperty).
uatingthemodelbyvaryingPC1andPC2,keepingallother
Wetrainmodelswithdifferenttrain/validationsplits(10%
primarycomponentsfixedattheirmean. Wealsotriedvary-
to 90% in 10% increments, 3 random seeds each), vary-
ing PC0 but, as anticipated, we observed that changes in
ingbatchsizeforconsistenttotaloptimizationsteps, and
PC0, which aligns with the helix axis, only influence the
keeping other hyperparameters constant. Given the clear
absolutevaluesofthemodel’soutput. Therelativevalues
structureobservedintheprevioussection,weexperiment
within each LST “slice” remain stable. Note that, since
PC0andN arefixed,theoverarchingnear-lineartrendof withasimplemeasurementoforderingalongthefirstPC
bindingenergywithrespecttoincreasingN andZ doesnot dimension. Itrevealsasurprisingcorrelationwithgeneral-
izationperformance,seeFigure9. Wedefinethequantity,
playaleadingrolehere.
To focus on the local variations, we consider the binding
orderedness=
1 M (cid:88)−1
1(E˜i <E˜i+1),
energy relative to the nucleon number A (E B/A) for the M 0 0
followinganalysis. ForeachfixedN,thereexistsaspecific i=1
Z valuethatcorrespondstothehighestE B/A,representing 5SeeAppendixFforanotherexampleofcontinuityinthelatent
the most stable element for that given N. As Z diverges space.
6
2CPFromNeuronstoNeutrons:ACaseStudyinInterpretability
where1istheindicatorfunction,6E˜i
isthePC0projection
0
of the N or Z embedding, and M is the total number of 1.10
embeddings. Wewillgenerallyusethetilde(˜·)todenote
1.08
PC-projectedvectors. It’simportanttonotethatallmodels
1.06
fitthetrainingdataextremelywell,witherrorsontheorder
1.04
oftensofkeV.However,thereisnocorrelationobserved
betweentrainerrorandthedegreeoforder. 1.02
1.00
Z embeddings N embeddings 0.98
1.05 1.05 0.96 Z; mem Z; gen
0.94 N; mem N; gen
1.00 1.00
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
103 104 103 104
epochs 1e5
10 1 10 1
10 2 Figure10.Parity split R as a function of training time for N
10 2 P
10 3 andZembeddingsformemorizingandgeneralizingmodels.The
10 3
103 104 103 104 uncertaintiesarecomputedover3dataandinitializationseeds.
validation error validation error
Figure9.ParitysplitR (toprow)andorderedness(bottomrow) 5.2.HiddenLayerFeatures
P
calculatedonN andZ embeddingsasafunctionofvalidation
Intheprevioussubsection,weexploredprotonandneutron
error. Zerovalueswereclippedto10−3forvisualization. Error
embeddingstoextractvaluableinformationaboutmodels
barsarestandarddeviationsandeachpointgroupsmodelstrained
thatgeneralizewell.Wediscoveredsomepropertiesofthese
withthesametrainingfraction.
modelsandwereabletomapthemtowell-knownphysics
concepts. However, the functional relationship between
3. Parity Inadditiontoorderedness,weexploreanother initialembeddingsandtheoutputisoftenunclear. Nowwe
prominentfeatureintheembeddingspace: numberparity. focusontheactivationsofthepenultimatelayer,whichdoes
This feature is immediately apparent in the projection of nothavethisdrawbacksinceitmapslinearlytotheoutput.
PC0 and PC2 in Figure 5 where even Z embeddings are WecontinuetousePCAprojectionstovisualizeandanalyze
separatedfromoddZ embeddingsalongPC2. Tomeasure thesehigh-dimensionalfeatures. AsseeninFigure4,we
theinfluenceofparityontheembeddings,weintroducethe canrecovermuchofamodel’sperformanceusingjustafew
followingquantity: ofthesefeatures. Weobservethat,similartothoseweseein
theembeddings,theprincipalcomponentsoftheactivations
2·d(even,odd) exhibit a rich structure, including terms that are smooth
R = ,
P d(even,even)+d(odd,odd) andslowlyvarying,othersthathaveahigh-frequencyand
small-scale,andsomethatarehighlystructured. Examples
whered(·,·)istheaveragepairwiseL -distancebetween fromeachcategoryareshowninthetoprowofFigure11,
2
elementsinthesetsofeven/oddN orZ. Thisquantityis andalargercollectionofPCscanbefoundinFigure21of
theratiooftheaveragedistanceofembeddingsofdifferent theAppendix.
paritytothatofembeddingsofthesameparity. Figure9
Weaimtorecoverhuman-deriveddescriptionsoftheprob-
illustrateshowR ,calculatedonprotonembeddings,corre-
P lemintheselatentrepresentations,andwewilldosobased
lateswithvalidationperformance. Thecleartrendobserved
onasimplematchingheuristic. Letx˜ bethei-thvectorof
i
suggeststhatparityisanimportantindicatorofmodelper-
theneuralnetwork’spenultimatelayerfeatures(givenbythe
formanceandpossiblyanimportantfeatureofthedata.
i-thPCdimension)andy bethej-thphysicaltermvector
j
Itturnsoutthatanimportantfeatureofnuclearproperties producedbyevaluatingthetermatallvaluesofN andZ
is the tendency of nuclear constituents (both protons and (seeAppendicesC.2andC.3forallterms). Weusetheco-
neutrons)toformpairs.7 Numerouscharacteristicsdepend sinesimilarity,definedassim(x˜ i,y j)=x˜ i·y j/||x˜ i||||y j||,
ontheparity(even/odd)ofN andZ. Thisisevidentinthe tocomparethetwosetsofvectors. Wefindthatthisheuris-
PairingtermoftheSEMF,whichchangessignbasedonthe ticrecoversvisuallycompellingmatchesandshowafew
parity. examplesinFigure11withthephysicaltermsatthebottom
andtheirmatchesinneuralfeaturesatthetop. Wenotethe
6Thedirectionoftheordermightbereversed.
following:
7Thisisrelatedtotheso-calledPauliExclusionPrinciple(Pauli,
1925). • PC0showsastrongtrendtowardshighervaluesincreas-
7
tilps
ytirap
ssenderedro
-
1
tilps
ytirapFromNeuronstoNeutrons:ACaseStudyinInterpretability
andfoundhintsofmoreadvancedcorrectionsfromtheshell
175 PC 0 PC 6 PC 4
model,simplybystudyingtheweightsandactivationsof
150
aneuralnetworktrainedonnucleardata. Wearecurrently
125
workingonfurtherdecodingwhatthemachinehaslearned
100
intohuman-interpretableknowledge.
75
50
25
Single-Task
0 104
Multi-Task
175 volume pairing shell
103
150
102
125
101
100
75
100
50 10 1
25
binding z n radius qa qbmqbm_nqec sn sp
0
0 50 100 0 50 100 0 50 100
Z Z Z
Figure12.Testperformanceoverdifferentobservablesformodels
trainedonasingletaskversusmultipletasksjointly.
Figure11.(Top)penultimatelayerPCsand(bottom)physicsterms
withhighsimilarity.
WhereDoTheseRepresentationsComeFrom? Learn-
ingfrommorediversedatasetsshouldyieldhigherquality
modelsandleadtoimprovedgeneralization,providedthat
ing Z and N. Since the model predictions are linear the model has enough capacity and nothing goes wrong
combinationsofthosefeatures,wecandeducethatPC0is withthetrainingprocedure. Naturally,thisisexpectedto
primarilyresponsibleforthegeneralupwardstrendinthe reflectalsointhequalityoftherepresentations. Figure12
output.Notethestrikingconsistencyofthattrendwiththe demonstratesthatusingthesamerepresentationstopredict
effectofthePC0ofinputembeddings(seeninFigure6) avarietyofnuclearobservablesimprovestheperformance
andthenumberorderingdescribedintheprevioussection. oneachofthemindividually. Forthisdemonstration,we
The bottom left pane of Figure 11 shows the dominant performtrainingrunswithonefeatureatatime,orallatthe
volumetermoftheSEMF,closelymatchingourfeature sametime,with50%ofthedataheldoutasavalidationset
PC0. ineachsettingtogaugethegeneralizationperformance. We
• UnlikePC0,thecontributionofPC6isofsmallerscale, observeaconsistentimprovementonallobservableswhen
characterizedbyahigh-frequencyperiodicityinbothN tackling the problem with a multi-task solution, utilizing
andZ. Interestingly,wecanalsomatchthisfeaturequite moredata.
distinctlytothepairingtermintheSEMF,observingthat
But where do the prominent features we observed in the
botharepredominantlyafunctionoftheparityofN and
latentrepresentationscomefrom? Wesystematicallycom-
Z. Note again the close connection to the parity split
pare the representations learned on individual tasks and
observedininitialembeddings.
notethatbindingenergyisprimarilyresponsibleforhelicity
• Lastly,wetakealookatPC4. Thisonestandsoutdueto
andisneverobservedelsewhere,parityismostpronounced
itsobviousstructureandthedistinctive,staircasepattern.
when training on separation energies, ordering seems to
NotermintheSEMFpredictsthisstructure. Asitturns
be partially present in many cases, and Z and N do not
out,ahigher-ordercorrectiontotheSEMFcomesfrom
produceparticularlyinterestingstructures(examplesinAp-
thenuclearshelltheorythatpredictsthesignificanceof
pendixD).
theso-calledmagicnumbersinZandN.Thecorrespond-
ingbottom-rightpaneinFigure11showsthepredicted
SymbolicExpressionsforDiscoveringNewTerms We
contributionfromtheshelltheorywithstrikinglysimilar
can also use the latent representations to model what the
structureasourPC4.
neural network learned, and thus, extract a new physics
Notethesignificanceofthisfinding: thereisavastamount model. Weusesymbolicregressiontomaptothefeatures
ofpossiblewaysinwhichaneuralnetworkcoulddecom- ofthepenultimatelayer, andthenapplyatransformation
posetheproblem,andyet,despitethesimpletechniqueswe that aligns to the binding energy. Using this pipeline we
usedtoinspecttheactivations,wewereabletorecovera recoverapredictivesymbolicexpression. Thenewformula
rangeofhuman-derivedconcepts. Withalloftheabove,we achieves a better performance than the SEMF, though is
have(re)discoveredtheliquiddropmodelofnuclearphysics lessinterpretable. Asabaseline, wealsoregressdirectly
8
N
N
]stinu
lacisyhp[
SMRFromNeuronstoNeutrons:ACaseStudyinInterpretability
over the task. However, we were not able to recover a representations. Finally,byemployinglatentspacetopog-
performance as good as the one obtained exploiting the raphy,8 we were able to arrive at a full description of the
neuralnetworkfeatures. Thoughingeneral,resultswould algorithmsusedbythemodeltomakeaccuratebindingen-
depend on the data, the model trained, and the symbolic ergy predictions. Inparticular, wefound thatthelearned
regressoritself,thisresultsuggeststhatthemodellearnsto embeddingsprovideageometricrepresentationofthethe-
decomposetheproblemintofeaturesthatcanmakeiteasier oretically well-motivated SEMF. These findings provide
to find interpretable symbolic expressions. This is inline aproof-of-conceptthatneuralnetworks,whentrainedon
withpriorworkthatderivessymbolicformulaefromneural scientific data, can learn useful representations that align
networkfeaturesforphysicalsystems(Lemosetal.,2023). withhumanknowledge. Thisopensupexcitingpossibilities
SeeAppendixGfordetails. forfutureresearchonricherdataandmorecomplextasks,
whichmayuncovernewscientificinsights.
6.RelatedWork
Acknowledgements
As an emerging field, mechanistic interpretability has re-
centlyfocusedonlargelanguagemodels(LLMs)(Elhage ThisworkissupportedbytheNationalScienceFoundation
etal.,2021),butitisalsostartingtogainrelevanceinsci- underCooperativeAgreementPHY-2019786(TheNSFAI
entific discovery (Cranmer, 2023). Another relevant line InstituteforArtificialIntelligenceandFundamentalInter-
ofworkstudieswhethermodelsbuildinternal“worldmod- actions,http://iaifi.org/). STisalsosupportedbytheSwiss
els”(Lietal.,2022;Benchekrounetal.,2023;Bowman, NationalScienceFoundation-projectn. P500PT203156.
2023). Glimpses of more complex understanding have VSPD acknowledges support from NASA/Chandra AR3-
already emerged. For instance, LLMs have constructed 24002Xgrant.
(tosomeextent)knowledgeinworldgeography(Roberts
etal.,2023),andmeaningfulrepresentationsofspaceand
ImpactStatement
time(Gurnee&Tegmark,2023),bothofwhichhavebeen
studiedsinceWord2Vec(Mikolovetal.,2013). Thissectionpresentsabriefoverviewofourvisionforan
MI-enhancedapproachtothescientificendeavor. Through-
Incomputervision,interpretabilitycantakeamoredirect
outthehistoryofscience,naturallawshavebeendiscovered
approach due to the visual nature of the data (Kadir &
by domain scientists studying high-dimensional data and
Brady, 2001; Simonyan et al., 2013). Here, mechanistic
realizingthat,insomecases,thesedatacanbeexplainedby
interpretabilitywasusedtogaininsightsonandimprove
asimpleinterpretablepicture. Thesepicturesweregener-
theeffectivenessofconvolutionalnetworks(Zeiler&Fer-
atedinthemindsofthedomainscientists,oftenbasedona
gus, 2014). A more microscopic approach to layer level
simplifiedgeometricalmodelofthesystembeingstudied.
interpretabilityonvisionmodelswasexploredinOlahetal.
(2017). Wepresentanewapproachtogeneratinginterpretablemod-
elsfromscientificdata: ratherthanhavingdomainexperts
7.Conclusion study the high-dimensional data directly, we propose to
first determine if a low-rank structure can be found in a
Inthiswork,weexplorethepotentialofusingmechanistic machine-learned model representation. If it can, human
interpretabilitytoextractscientificknowledgefromneural domainscientistscantryanddecodethisstructureintoan
networkstrainedonphysicsdata. Wenotonlyinvestigate interpretablemodel,ratherthancontinuingtoworkdirectly
howmodelsmaketheirpredictions,butalsowhatinsights withthehigh-dimensionaldata.
the model can provide about the data. Our analysis has
Here,wechoseanexamplewhereahuman-derivedinter-
revealedseveralfindings. First,thelearnedembeddingsof
pretablepictureisknowntoexist—nuclearphysicsandits
protonandneutronnumbersexhibitinterpretablestructures
famous Shell Model—and find that representation learn-
suchasthehelixandparitysplits,whichareindicativeof
ing (without any physics input), along with the use of
the models’ generalization capabilities. These structures
PCA, does indeed discover a low-rank geometric struc-
mirrorknownphysicsconceptslikepairingeffects,suggest-
ture. Afterfurtherstudy,usingtheShellModelasaknown
ingthatthemodelsarecapableoflearningandemploying
baselinesolution,weseethatthemachinehaslearnedthe
established scientific knowledge. Second, our inspection
Shell Model—though with corrections that lead to more
ofhiddenlayeractivationshasuncoveredcomponentsthat
precise predictions than the Nobel Prize-winning human-
resembletermsinestablishedtheories: thesemi-empirical
discovered model. Therefore, the known interpretable
massformulaandthenuclearshellmodel. Thissimilarity
inbothmacroscopictrendsandmicroscopicstructuressug- 8Examplecodeisavailablehere:
gests that the models are learning physically meaningful https://github.com/samuelperezdi/nuclr-icml
9FromNeuronstoNeutrons:ACaseStudyinInterpretability
human-discoveredmodelisfoundbythemachineandcom- 7328,2021.
municatedtous,albeitinadifferentformthatstillneeded
Angeli,I.andMarinova,K.P.Tableofexperimentalnuclear
decodingbydomainexperts.
groundstatechargeradii: Anupdate. AtomicDataand
Asinthenuclearphysicscasestudiedhere, mosthuman- Nuclear Data Tables, 99(1):69–95, January 2013. doi:
discoveredinterpretablescientificmodelsareonlyapproxi- 10.1016/j.adt.2011.12.006.
matelytrue. Insuchcases,ourapproachhasthepotential
toderivecorrectionstothehuman-discoveredmodel,repre- Antognini,J.andSohl-Dickstein,J.Pcaofhighdimensional
sentedasdeviationsinthelow-rankstructure. Weseethis randomwalkswithcomparisontoneuralnetworktraining.
with the nuclear data and are working on fully decoding AdvancesinNeuralInformationProcessingSystems,31,
thesedeviationsintointerpretablecorrectiontermstothe 2018.
ShellModel.
Ashkboos,S.,Croci,M.L.,doNascimento,M.G.,Hoefler,
Suchinterpretablecorrectionswillhaveahugeimpacton T.,andHensman,J. Slicegpt: Compresslargelanguage
thefieldofnuclearphysics.Thisisespeciallytrueforexotic modelsbydeletingrowsandcolumns,2024.
nucleifarfromthestabilityregion,whichareimpossibleto
makeandstudyinthelab. Yet,thepropertiesofthesenuclei Benchekroun, Y., Dervishi, M., Ibrahim, M., Gaya, J.-B.,
arecrucialforunderstandingnuclearprocessesinextreme Martinet,X.,Mialon,G.,Scialom,T.,Dupoux,E.,Hup-
environments,suchasneutronstars. Thisunderstanding,in kes,D.,andVincent,P. WorldSense: ASyntheticBench-
turn,enhancesourknowledgeofhowheavyelementswere markforGroundedReasoninginLargeLanguageModels.
produced in our universe. This is an out-of-distribution arXive-prints,art.arXiv:2311.15930,November2023.
(OOD) problem from the ML perspective, hence finding doi: 10.48550/arXiv.2311.15930.
interpretable corrections that can be trusted in the OOD
Bengio,Y.,Courville,A.,andVincent,P. Representation
regioniscrucial.
learning: Areviewandnewperspectives. IEEEtransac-
Mostotherknowninterpretablemodels(inotherscientific tionsonpatternanalysisandmachineintelligence,35(8):
domains) are also only approximate, and similar correc- 1798–1828,2013.
tions could likely be found to improve scientific knowl-
Bethe,H.A.andBacher,R.F.NuclearPhysicsA.Stationary
edge in those areas as well. Furthermore, in many scien-
StatesofNuclei. Rev.Mod.Phys.,8:82–229,1936. doi:
tificdomains,humanshavenotbeencapableofdeveloping
10.1103/RevModPhys.8.82.
any interpretable theories, even approximate ones, when
studying high-dimensional data. Whether our approach
Bowman, S. R. Eight Things to Know about Large Lan-
could lead to discoveries in such fields is impossible to
guage Models. arXiv e-prints, art. arXiv:2304.00612,
predict—interpretablemodelsmaynotexistforsomehighly
April2023. doi: 10.48550/arXiv.2304.00612.
non-linearproblems—butitisadirectionworthpursuing.
Hence, one of our goals is to encourage the ML commu- Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters,
nitytoworkmorecloselywithdomainscientistsonsuch N.,Desjardins,G.,andLerchner,A. Understandingdis-
problems,whichcandriveadisproportionateimpactacross entanglinginβ-vae. InNeurIPSWorkshoponLearning
disciplines. DisentangledRepresentations,2018.
Insummary,ourworkunderscoresthevalueofinterpretabil-
Chen,R.T.,Li,X.,Grosse,R.B.,andDuvenaud,D.K. Iso-
ity in scientific exploration. By elucidating how models
latingsourcesofdisentanglementinvariationalautoen-
represent problems, interpretability becomes a powerful
coders. InAdvancesinNeuralInformationProcessing
tool for scientific discovery. As we continue to develop
Systems,pp.2610–2620,2018.
and refine these techniques, we anticipate that they will
play an increasingly important role in advancing human Cranmer, M. Interpretable machine learning for science
understandinginawiderangeofdomains. with pysr and symbolicregression. jl. arXiv preprint
arXiv:2305.01582,2023.
References
Davis,B.L.andJin,Z. Discoveryofaplanarblackhole
Aghajanyan, A., Gupta, S., and Zettlemoyer, L. Intrin- massscalingrelationforspiralgalaxies. TheAstrophysi-
sicdimensionalityexplainstheeffectivenessoflanguage calJournalLetters,956(1):L22,2023.
model fine-tuning. In Proceedings of the 59th Annual
Dettmers,T.,Pagnoni,A.,Holtzman,A.,andZettlemoyer,
MeetingoftheAssociationforComputationalLinguistics
L. QLoRA: Efficient Finetuning of Quantized LLMs.
andthe11thInternationalJointConferenceonNatural
arXiv e-prints, art. arXiv:2305.14314, May 2023. doi:
LanguageProcessing(Volume1:LongPapers),pp.7319–
10.48550/arXiv.2305.14314.
10FromNeuronstoNeutrons:ACaseStudyinInterpretability
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, Li, K., Hopkins, A. K., Bau, D., Vie´gas, F., Pfister, H.,
N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, and Wattenberg, M. Emergent World Representations:
T., DasSarma, N., Drain, D., Ganguli, D., Hatfield- ExploringaSequenceModelTrainedonaSyntheticTask.
Dodds,Z.,Hernandez,D.,Jones,A.,Kernion,J.,Lovitt, arXive-prints,art.arXiv:2210.13382,October2022. doi:
L., Ndousse, K., Amodei, D., Brown, T., Clark, J., 10.48550/arXiv.2210.13382.
Kaplan, J., McCandlish, S., and Olah, C. A math-
Liu, Z., Kitouni, O., Nolte, N.S., Michaud, E., Tegmark,
ematical framework for transformer circuits. Trans-
M.,andWilliams,M. Towardsunderstandinggrokking:
former Circuits Thread, 2021. https://transformer-
Aneffectivetheoryofrepresentationlearning. Advances
circuits.pub/2021/framework/index.html.
in Neural Information Processing Systems, 35:34651–
Gurnee,W.andTegmark,M. LanguageModelsRepresent 34663,2022.
SpaceandTime. arXive-prints,art.arXiv:2310.02207,
Locatello,F.,Bauer,S.,Lucic,M.,Raetsch,G.,Gelly,S.,
October2023. doi: 10.48550/arXiv.2310.02207.
Sch”olkopf,B.,andBachem,O. Challengingcommon
assumptionsintheunsupervisedlearningofdisentangled
Hassid, M., Peng, H., Rotem, D., Kasai, J., Montero,
representations. InInternationalConferenceonMachine
I., Smith, N. A., and Schwartz, R. How much does
Learning,pp.4114–4124.PMLR,2019.
attention actually attend? questioning the importance
of attention in pretrained transformers. arXiv preprint
Mengel, T., Steffanic, P., Hughes, C., da Silva, A. C. O.,
arXiv:2211.03495,2022.
and Nattrass, C. Interpretable machine learning meth-
odsappliedtojetbackgroundsubtractioninheavyion
Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey,
collisions. arXivpreprintarXiv:2303.08275,2023.
L., Rezende, D., and Lerchner, A. Towards a defi-
nition of disentangled representations. arXiv preprint Mikolov,T.,Chen,K.,Corrado,G.,andDean,J. Efficient
arXiv:1812.02230,2018. estimationofwordrepresentationsinvectorspace. arXiv
preprintarXiv:1301.3781,2013.
Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,
S.,Wang,L.,andChen,W. Lora:Low-rankadaptationof Nanda, N., Chan, L., Lieberum, T., Smith, J., and Stein-
largelanguagemodels. arXivpreprintarXiv:2106.09685, hardt,J. Progressmeasuresforgrokkingviamechanistic
2021. interpretability. arXivpreprintarXiv:2301.05217,2023.
Kadir,T.andBrady,M. Saliency,scaleandimagedescrip- Novembre,J.andStephens,M. Interpretingprincipalcom-
tion. International Journal of Computer Vision, 45(2): ponentanalysesofspatialpopulationgeneticvariation.
83–105,2001. Naturegenetics,40(5):646–649,2008.
Olah, C. Mechanistic interpretability, variables, and
Kim, H. and Mnih, A. Disentangling by factorising. In
the importance of interpretable bases. Trans-
InternationalConferenceonMachineLearning,pp.2649–
former Circuits Thread, 2022. https://transformer-
2658.PMLR,2018.
circuits.pub/2022/mech-interp-essay/index.html.
Kirson,M.W.Mutualinfluenceoftermsinasemi-empirical
Olah, C., Schubert, L., and Mordvintsev, A. Feature vi-
mass formula. Nucl. Phys. A, 798:29–60, 2008. doi:
sualization. Distill,2017. URLhttps://distill.
10.1016/j.nuclphysa.2007.10.011.
pub/2017/feature-visualization/.
Lebedev, M. A., Ossadtchi, A., Mill, N. A., Urp´ı, N. A., Pauli, W. U¨ber den zusammenhang des abschlusses der
Cervera,M.R.,andNicolelis,M.A.Analysisofneuronal
elektronengruppenimatommitderkomplexstrukturder
ensembleactivityrevealsthepitfallsandshortcomingsof
spektren. Zeitschrift fu¨r Physik, 31(1):765–783, Feb
rotationdynamics. ScientificReports,9(1):18978,2019.
1925. ISSN0044-3328. doi:10.1007/BF02980631. URL
https://doi.org/10.1007/BF02980631.
Lemos,P.,Jeffrey,N.,Cranmer,M.,Ho,S.,andBattaglia,P.
Rediscoveringorbitalmechanicswithmachinelearning. Proix, T., Perich, M. G., and Milekovic, T. Interpreting
MachineLearning:ScienceandTechnology,4(4):045002, dynamicsofneuralactivityafterdimensionalityreduction.
2023. bioRxiv,pp.2022–03,2022.
Li, C., Farkhoor, H., Liu, R., and Yosinski, J. Measur- Roberts, J., Lu¨ddecke, T., Das, S., Han, K., and Albanie,
ingtheintrinsicdimensionofobjectivelandscapes. In S. GPT4GEO:HowaLanguageModelSeestheWorld’s
InternationalConferenceonLearningRepresentations, Geography. arXive-prints,art.arXiv:2306.00020,May
2018. 2023. doi: 10.48550/arXiv.2306.00020.
11FromNeuronstoNeutrons:ACaseStudyinInterpretability
Shinn, M. Phantom oscillations in principal component
analysis. bioRxiv,pp.2023–06,2023.
Simonyan, K., Vedaldi, A., and Zisserman, A. Deep in-
side convolutional networks: Visualising image clas-
sification models and saliency maps. arXiv preprint
arXiv:1312.6034,2013.
Wang,M.,Huang,W.J.,Kondev,F.G.,Audi,G.,andNaimi,
S. TheAME2020atomicmassevaluation(II).Tables,
graphs and references. Chin. Phys. C, 45(3):030003,
2021. doi: 10.1088/1674-1137/abddaf.
Weizsa¨cker,C.F.v. Zurtheoriederkernmassen. Zeitschrift
fu¨rPhysik,96(7):431–458,Jul1935. ISSN0044-3328.
doi:10.1007/BF01337700.URLhttps://doi.org/
10.1007/BF01337700.
Zeiler,M.D.andFergus,R. Visualizingandunderstand-
ingconvolutionalnetworks. InComputerVision–ECCV
2014: 13thEuropeanConference,Zurich,Switzerland,
September6-12,2014,Proceedings,PartI13,pp.818–
833.Springer,2014.
Zhang, Q., Chen, M., Bukharin, A., Karampatziakis, N.,
He, P., Cheng, Y., Chen, W., and Zhao, T. AdaLoRA:
AdaptiveBudgetAllocationforParameter-EfficientFine-
Tuning. arXiv e-prints, art. arXiv:2303.10512, March
2023. doi: 10.48550/arXiv.2303.10512.
Zhang,Y.,Tinˇo,P.,Leonardis,A.,andTang,K. Asurvey
on neural network interpretability. IEEE Transactions
onEmergingTopicsinComputationalIntelligence,5(5):
726–742,2021. doi: 10.1109/TETCI.2021.3100641.
Zhong,Z.,Liu,Z.,Tegmark,M.,andAndreas,J. Theclock
andthepizza: Twostoriesinmechanisticexplanationof
neuralnetworks. arXivpreprintarXiv:2306.17844,2023.
12FromNeuronstoNeutrons:ACaseStudyinInterpretability
A.Whydoesthemodellearnahelix?
Thehelixstructureobservedintheembeddingsofbothneutronandprotonembeddingspresentsoneofthemoststriking
featuresinthemodeltrainedonnuclearproperties. Inanefforttogettothebottomofit,weattempttoisolatewhereit
comesfrom. Fromexperiments in themulti-taskvs.single-tasksettings, wenoticethathavingthe binding energyasa
targetisastrongpredictorfortheappearanceofthehelix. Thereforewewillrestrictourselvestothepredictionofbinding
energy. Ourstrategyforsheddinglightonhowthemodelusesthehelixstructuretoitsadvantageisparameterizingandthen
perturbingthehelixparameters. Wehopetobeabletofactorizecontributionsfromdifferentaspectstobreaktheprocess
intounderstandablepieces. Wefitahelixwithtrainableparametersusingthefollowingparametricequation:
⃗r(t)=R[cos(2πft+ϕ)⃗u+sin(2πft+ϕ)⃗v]+P⃗at+⃗r , (2)
0
where⃗uand⃗vareorthonormalunitvectorsperpendiculartothecentralaxispointingtowardsthedirectiongivenbytheunit
vector⃗a. Theshapeparametersare: thelengthofcentralaxisP⃗,thefrequencyf,thephaseϕ,theradiusR,andtheorigin
⃗r . Thedirectionoftheevolutionischosentobetowardsthevisuallymosthelix-likeportionof3DPCAprojectionsofboth
0
neutronandprotonembeddings.
Inanefforttomaximizevisualclarity,weshowexperimentsforamodeltrainedonbindingenergypredictionsfromthe
SEMF,wherewefindacleanerhelixstructurethanwhentrainingonrealdata,seeFigure1(right).Weconstrainourselvesto
N ∈[40,120],Z ∈[25,80]tobeabletofitthehelixwithaconstantradius. TheresultsofthefitcanbefoundinFigure14.
ThefitsmatchthePCprojectionswellandwecannowperturbhelixparameters. Forvisualization,weprovidethreeplots
foreachparameterchange: First,aplotofthehelixwithandwithoutthechangedparameter. Second,themodelprediction
relativetoA=N +Z withandwithoutthechangedparameterasafunctionofN forafixedvalueofZ. Third,thesame
plotwithN andZ rolesreversed. WefindthatplottingrelativetoAgivesvisuallymoreinformativeresults.
First,weincreasethelengthparameterinFigure15a. Thiselongatesthehelixalongitsmaindirection. Similarlyasdepicted
inFigure6,wefindthatmovingalongthemaindirectioncorrespondstoamacroscopictermakintothevolumeterminthe
SEMF.SinceweplotrelativetoA,thattermcauses,infirstorder,aconstantoffsetinthepredictions. Figure15bshowsa
reductionofthelength,resultinginanegativeoffset.
Next,weincreasetheradiusparameter,seeFigure15d. Thiscausesthedownwardsfacingarcsto“sharpen”. Takinga
closerlookattheSEMFformulaandtheN vs. modeloutputplot,wehypothesizethatthedepictedarcsareinfactthe
approximateparaboladescribedbythethirdtermandthattheradiuscontrolstheprefactorofthatparabola,causingthe
“sharpening”,or,incaseofaradiusparameterreduction,theflatteningdepictedinFigure15c.
Lastly,wedoublethefrequencyparameter,seeFigure15e. Thereisnoclearcorrespondencetoanyoneparticularterm
intheSEMF,butitgivesanindicationabouthowthearciscreated. Doublingthefrequencydoublesthefrequencyofa
nowperiodicsequenceofarcs. ThiscanbeunderstoodintuitivelywhenobservingFigure7. Theringstructurewithdouble
frequencygoesaroundtwiceandtwoperiodsappearinthemodeloutput. Figure15fshowsthatthistrendispersistentalso
whenincreasingthefrequencyevenmore.
Whilewehavemadedecentprogresstowardsunderstandinghowtheembeddingsmaptotheoutputofthemodel,thefull
pictureisnotcompletelyclearyet. However,weareconfidentthataniterativeapproachcanhelpusunderstandthestory
completely.
13FromNeuronstoNeutrons:ACaseStudyinInterpretability
N=80 Z=50 N=80 Z=50
Mod: 1.2 x P Mod: 0.8 x P
default default
modified modified
default
modified
default
modified
50 60 70 50 60 70 80 90 50 60 70 50 60 70 80 90
Z N Z N
(a) (b)
N=80 Z=50 N=80 Z=50
Mod: 0.5 x R Mod: 1.5 x R
default default
modified modified
default default
modified modified
50 60 70 50 60 70 80 90 50 60 70 50 60 70 80 90
Z N Z N
(c) (d)
N=80 Z=50 N=80 Z=50
Mod: 2 x F Mod: 3 x F
default default
default modified default modified
modified modified
50 60 70 50 60 70 80 90 50 60 70 50 60 70 80 90
Z N Z N
(e) (f)
Figure13.Variationsinhelixparametersandtheireffectsonpredictionswhen:(a)increasingthelengthby20%,(b)reducingthelength
by20%,(c)reducingtheradiusby50%,(d)increasingtheradiusby50%,(e)multiplyingthefrequencyby2,(f)multiplyingthefrequency
by3.(Modeltrainedondata).
14
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuOFromNeuronstoNeutrons:ACaseStudyinInterpretability
Z embeddings N embeddings
Original Data
Fitted Helix
Figure14.ResultsoffittingthehelixtotheselectedportionsofN andZembeddings.ThismodelwastrainedontheSEMF.
N=80 Z=50 N=80 Z=50
Mod: 1.2 x P Mod: 0.8 x P
default default
modified modified
default default
modified modified
50 60 70 50 60 70 80 90 50 60 70 50 60 70 80 90
Z N Z N
(a) (b)
N=80 Z=50 N=80 Z=50
Mod: 0.5 x R Mod: 1.5 x R
default default
modified modified
default default
modified modified
50 60 70 50 60 70 80 90 50 60 70 50 60 70 80 90
Z N Z N
(c) (d)
N=80 Z=50 N=80 Z=50
Mod: 2 x F Mod: 3 x F
default default
default modified default modified
modified modified
50 60 70 50 60 70 80 90 50 60 70 50 60 70 80 90
Z N Z N
(e) (f)
Figure15. EquivalentofFigure13,butforamodeltrainedontheSEMFdirectly.
15
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuO
A
/ tuptuOFromNeuronstoNeutrons:ACaseStudyinInterpretability
B.Trainingandmodeldetails
WeuseanattentionablatedtransformerwithSiLUactivationsandresidualconnections. Weexperimentedwithdifferent
norms(RMS/Layer/Batch)Normandtheresultsseemedsimilartohavingnonormatall(probablyduetoshallownessof
themodelsused). Attentionseemstomatteralotmoredespitethefactthatmodelandcontextlengtharerelativelysmall.
Fixingattentioninthewaywedocanbeshowntosimplifythemodelquitedrastically(Zhongetal.,2023). Wealsofound
theembeddingstobeeasiertointerpretsowefocusonthissetupthroughoutthepaper. Weusealinearreadoutlayeratthe
topofthemodeltopredictscalarvalueswhichwetrainwithMSEloss. Wealsoexperimentedwithdifferentweighting
schemesforthetasksandsettledona“physics-informed”schemebasedonexpectedmeasurementerrorsforeachtask.
We use AdamW with mostly default parameters and experiment with a range of hyperparameters in our explorations
learningrate∈[10−4,10−3],weightdecay∈[10−8,10−2]. Therunsusedtogeneratetheembeddingsandvisualizations
havethefollowingparameters:
• EPOCHS = 200,000
• HIDDEN DIM = 2048
• LR = 0.0001
• WD = 0.01
• DEPTH = 2
• Seed = 0
MosttrainingrunswereonNvidiaV100GPUswithsomedoneonNvidiaA6000GPUs.
B.1.Structureevolution
Herewevisualizetheprogressofour“strcuturemeasures”asafunctionoftimeformodelsthatgeneralizewellandmodels
thatmemorize.
1.10
1.08
1.06
1.0
1.04
0.9
1.02
0.8 1.00
0.7 0.98
0.6 Z; mem. Z; gen. 0.96 Z; mem Z; gen
N; mem. N; gen. 0.94 N; mem N; gen
0.5
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
epochs 1e5
(a)Ordernessintimeforgeneralizingandmemorizingmodels. (b)Parityintimeforgeneralizingandmemorizingmodels.
Figure16. Progressofstructuremeasuresplottedagainstthenumberofepochs(normalizedby105).
C.Physicsmodelsandobservables
C.1.Data
Thedatasourcesare: forthevariousenergiestheAtomicMassEvaluation(AME)(Wangetal.,2021)andforthecharge
radiitheAtomicDataandNuclearDataTables99(2013)(Angeli&Marinova,2013). WenotethatalltheRMSmetrics
arecalculatedusingthewholedatasets,whichincludebothexperimentalmeasurementsaswellasestimates,e.g. viathe
methodoftrendsfromthemasssurface(TMS).
16
ssenderedro
tilps
ytirapFromNeuronstoNeutrons:ACaseStudyinInterpretability
C.2.Liquid-DropModel(LDM)-thetheorybehindtheSEMF
Whilethepropertiesofthenucleisharethesamemicroscopicorigin,namelythestrongnuclearforceandelectromagnetism,
experimentallywehaveaccessonlytoasetofmacroscopicobservables. Thefirstandhistoricallymostimportantnuclear
modelisthemacroscopicLDM,whichtreatsthenucleusasadropletofhighlydensefluid,boundtogetherbythestrong
nuclearforce.Themodelexplainswhymostnucleihaveasphericalshapewitharadiusproportionalto∼A1/3.Impressively,
thisdependenceyieldsanexcellentfittothechargeradiusdata.
Moreover,theLDMprovidesanestimationofthebindingenergy(Weizsa¨cker,1935;Bethe&Bacher,1936),whichis
thefundamentalobservableinnuclearphysicsasitentersthecalculationsofmostoftheotherquantities. Itrepresentsthe
energyrequiredtobreakapartanucleusintoitsindividualnucleonsanditisdefinedas
E (Z,N)≡Zm +Nm −M(Z,N), (3)
B p n
TheLDMpredictionforE isgivenbytheSEMF(seeequation1).Inthefollowing,webrieflyexplainthephenomenological
B
motivationforthetermsthatappearintheSEMF.
VolumeTerm+a A: Representsthebulkenergycontribution. Thenucleus’soverallenergyisdirectlyproportionaltoits
V
volume.
SurfaceTerm−a A2/3: Accountsfornucleonsonthesurfacehavingfewerneighboringnucleonstobondwith. Itis
S
proportionaltothesurfaceareaofthenucleusanditisnegative,sinceitcorrectstheadditionalcontributionassumedforthe
volumeterm.
CoulombTerm−a Z(Z−1): Reducesthetotalenergyduetheelectrostaticrepulsionbetweenprotons.
C A1/3
AsymmetryTerm−a
(N−Z)2
AccountsforthePauliexclusionprinciple,i.e.increasedenergyisrequiredwhenneutrons
S A
andprotonsarepresentinunequalnumbers,forcingonetypeofparticleintohigherenergystates.
PairingTerm±a A−1/2 : Thistermisnon-zeroonlyforevenAandreflectsthestabilitygainedthroughthepairingof
P
protonsandneutronsduetospincoupling. ThecontributioniseitherpositiveornegativeifN andZ arebothevenorodd,
respectively.
TheSEMFisrefinedupontheinclusionofanumberofadditionalterms: (i)exchangeCoulombterm,(ii)Wignerterm,(iii)
surfacesymmetryterm,(iv)curvatureterm,and(v)shelleffectsterm. Fordetailedexplanationsoftheseterms,aswellas
thefitsofallthecoefficientsa see(Kirson,2008). ThecontributionsoftheseadditionaltermsaredepictedinFigure22
∗
(therefinedSEMFisdenotedasBW2).
C.3.Nuclearshellmodel
ThefailureoftheSEMFatreproducingthemeasuredvaluesofmassesforlightnucleiandnucleiwithcertainnumbersof
nucleons,themagicnumbers9,ledtothedevelopmentofthenuclearshellmodelbyGoeppert-MayerandJensen(Nobel
PrizeinPhysics,1963). Accordingtothismodel,protonsandneutronsareseperatelyarrangedinshells,andmagicnumbers
occur when shells are filled. Nuclei with either Z or N (or both) equal to a magic (or doubly magic) number exhibit
enhancedstability,andthustheE spikes.
B
Thevariousshellpropertiescanbereproducedbyapproximatingthenuclearpotentialwithathree-dimensionalharmonic
oscillatorplusaspin–orbitinteraction. Moreadvancedtreatmentsincludetheusageofmeanfieldpotentials. However,a
simplephenomenologicaltermcanbestillbeaddedtotheSEMFandimproveitsperformance.Thistermis:a P+a P2,
M1 M2
whereP = νν NN +ν νZ
Z
andν N,Z thenumbersofthevalencenucleons(i.e. thedifferencebetweentheactualnucleonnumbers,
N andZ respectively,andthenearestmagicnumbers). ThecontributionofthistermcanbeseeninFigure23.
C.4.Separationenergies
Thestabilityofanuclideisdeterminedbyitsseparationenergies,whichreferstotheenergiesneededtoremoveaspecific
number of nucleons from it. They reflect the changes in structure across the nuclear landscape and play a crucial role
9Themostwidelyrecognizedare[2,8,20,28,50,82,126]andothersarestilldebated.
17FromNeuronstoNeutrons:ACaseStudyinInterpretability
in understanding the energy requirements involved in nuclear reactions. The separation energies of an isotope can be
determinedincasethebindingenergiesofneighboringisotopesontheN −Z planehavebeenmeasured(andvice-versa).
Theone-neutronS ,one-protonS separationenergy,theenergyreleasedinα-decayQ ,β-decayQ ,doubleβ-decay
N P A BM
Q ,andelectron-captureprocessQ are,respectively
BMN EC
S (Z,N)≡M(Z,N −1)+m −M(Z,N),
N n
S (Z,N)≡M(Z−1,N)+m −M(Z,N).
P p
Q (Z,N)≡M(Z,N)−M(Z−1,N +1)−m
A 4He
2
Q (Z,N)≡M(Z,N)−M(Z+1,N −1),
BM
Q (Z,N)≡M(Z,N)−m −M(Z+1,N −2),
BMN n
Q (Z,N)≡M(Z,N)−M(Z−1,N +1). (4)
EC
103 175 100.8
102
101 150 0.3
100
1000 23.4
125 101 35.8
102
0 50 100 100 43.2
Z
103 75 49.2
102 53.8
101 50
100 57.6
1000 25
101 66.6
102 0 106.0
0 50 100 150 0 50 100
N Z
Figure17. Residualbetweendataandthesemi-empiricalmassformula.Dashedlinesaremagicnumbers.
D.Whichrepresentationscomefromwhichtask?
binding-N
101 1117171 17717617 75 174173 121 61161066 196816716 61 56416 31625111501 95151815751651551451 3521411411049 1484 147 146 15 44 1143 4 12 1331 10 133198 313171 635131 4331 312221 11 0221 98212 7612 51242131221 11110111 918117116115114113112 101 10009 1108 17 0 106 10 15 094 03 99029 91 8 970 969594 93928 18 08 9888788 658 4388 27 170797 87 77675747 1372 0661 70698 867 666 65 64 636251 50 59 58 57 56 5554532514404984746454 444 3243 1303 93 8373635 34 333 2 21 20 298 27 26 25 2423 22 21 11 101 96 81 7151 41 31219 18 076 5 4 2 31 42024 0123456789111011121314151617282920212223242526273839303132333435363748494041424344454647585950515253545556576869606162636465666778797071727374757677888980818283848586879899909192939495969718191010101001101201301401501601711811911011111211311411511611721821921021121221321421521621731831931031131231331431531631741841941041141241341441541641751851951051151251351451551651761861961061161261361461561661771871971071171271 374757 678 01 1
4 2 0 2 4 0 50 100 150
PC 0 Index
101 111 717117 71761775174173 12161 1610 66 19 6816716 61564 16 31 6251115 01951 51 81 5751 6515 51451 3521 41 1411 04 91 48414 714 6154411434 12 1331 10 13 3 198 3 13 17 16 35 13 14 3 313 122 211 1022198212761251242131221 111101 11 91811 71 1 61 151 1 411 311 21011000911 081 7010 6101 509 403990 29 918 970 96 95 94 93 92 81 80 89 88 878865843882717 07 97877767 57 471 37 2066 1706 98 86766 66 5646 36 25150 59 58 57 56 55 54 532 51 4 40 49 8 47 46454444324313 03 93 83 73 63 53 433 322 1202 9827262524 23 22 21 1 110 19 681 7151 4131219 18 076542 3 1 101 0 1 2 3456789 1110 11 12 13 14 15 16 17 282 92 02 1222 32 42 52 62 73 839303 13 2333435 36 37 48 49 40 41 42 43 44 4546475859505152535 45 55 65 76 86960616 26 36 46 56 667787970 71727374 75 76 77 88 89 80 81 82 83 84 85 86 87 98 999091929394959 69 7181910101 01 001 101 201 301 401 501 601 71181191101111121 131141 15 116117 2182192 102 112 12 2 13 214 215 2162173 183 19310 311 31231331 431 531 631 741 841 941 041 141 241 34144154164175 18519510 5115125 13514515 5165176 18619610 6116126136 14615616 61771 871 971071171271374757678 01 1
4 2 0 2 4 0 50 100 150
PC 0 Index
101 11171711771761775174 173121611 61 066196 816716615641631625111 50195151 815751651 551451 35 214114110491484 147 146 15 44 11434121331101331983131716351314331312221110221982127612512421 31221111101 11 9181 1 71 1 611 511 4113112101100091 108 17 0 106 10 15 0 94 03 9902 991897096959493928180898887 8865 843882717 07 9787 77 67 57 47 1372 066 1 706 98 867666 6564 63 62 515059585 75 6555453251440498 47 46 4544443 24313 03 938373635 34 3 33 2 21 2029827262 52 42 32 22 111 10196 81 715 14 13 12 1 91 80 76542 31 101 0 12 3456 7 89 111011 1213 14 15 161 72 82 92 02 12 22324252627 38 39 30 31 32 33 34 35 36 37 48 49 40414243444 54 64 75 85 95 05 15 25 35 45 55 6576869 60 61 62 63 64 65 66 67 78 79 70 71 72 73 74 7576 77 88 898081828384858 68 79 89 99 09 19 29 39 49 59 69 71 819 10 10 1010 0 11 0 120 13 014 0 15 0 160 17 1 18 1 19 1 10 1 11 1 12 1 13 114 1 15 1 16117 218 21921021121221321421521 621 731 831 931 031 131 231 331 431 531 631 74184194104 114 124 13 4 14 4 15 4 16 417 518 5195 105 115 12 5 13 514 5 15 516517 61861 96 10611612 61 361461 561661 771 87197107 117 12713 74 757678 01 1
4 2 0 2 4 0 50 100 150
PC 0 Index
Figure18.FirstfewPCprojectionsoftheN embeddingsforamodeltrainedononlybindingenergy.Indexherereferstothetokenindex
orthevalueofN.
18
]Vek[
FMES
- ataD
]Vek[
FMES
- ataD
1 CP
2 CP
3 CP
N
0 CP
1 CP
2 CP
]Vek[
FMES
- ataD
1 CP
2 CP
3 CPFromNeuronstoNeutrons:ACaseStudyinInterpretability
sn-N
5 7.5 2
202 111171711 17751617 111166 71 1311656 7771697 6161563 11 6 014268541591 65 5211171531105 81455 114 164 151 1 4911 41 51411 5 47 141 23 41333 048123 44639 11217 1 211395 1 03 371 331 3811 21621 41 211 10212 812 151111 111 17 161213 21 991 0259 93 001 010411 21 991 51 101 7758 31 96 911 3040 18018 120 8 9868 0940 973 9858 0 92 688 9 4876 2784 0 1779776 805316 819 772767 7 06 7 868565 7435 2656519 0667 585636245 51 508 6544 2479445 44 533 481 0493 464437 23 3035 33 821 3639 4232 07222 5 213
2
821 649 221 271 05 1181 61 319 11 427 108 3 614 2 2025 .... 5050 0134 56 78 91110 1112 1314 1516 1728 2920212223242526 273839303132333435363748494041424344454647585950 515253545556576869606162636465666778797071727374757677888980818283848586879899909192939495969718191010101001101201301401501601711811911011111211311411511611721821921021121221321421521621731831931031131231331431531631741841941041141241341441541641751851951051151251351451551651761861961061161261361461561661771871971071171271 374757 678 012 21
2 0 2 4 6 0 50 100 150
PC 0 Index
3 5
1012 1 111717111 77516171111667 11 31 165677716976 1615 6311601426854159 165521 1171 531 105814 55 114 164 1511491 141 5141 15 47 141 234133 3 04812 34463 91121 7 12 1139 5 103 371 331 38 11 21 621 41 2111 0212 8 121 5 111111 11 71 61 213 2199 10 2599 300 10 10 4112 199 15 11 017 75 83 19 69 113 04018 0181 208 98 6809 40973 9 85 809 26 889487 627 84017797 76 8053 16 819 772767 7067868565 7435 265 65 19 06675 85 63 624 551 50 865 4424 79 4454 45 3348 10 493464 4372330 35 3382 1363942 32 07 2 2252 13 28 21 6 49 221 27 1 051 18 1 61319 1 14 27 105 8 61 4 2 202 01234 67 89 11 1011 1213 1 415 1617 2829 2021 2223 2425 262 7 3 839 3031 3233 3435 3637 4849 4041 4243 4445 4647 5859 5 051 5253 54555 657 6869 6061 62636465 6667 7879 7071 7273 7475 7677 8889 808 18 28384 8586 8798 9990 9192 939495969718 1910 1010 10 01 10 12 01 30 14 01 50 16 01 71 18 11 9110 11 111211 31141151 16117218 2192102 112122 13 2142 15 21 62 17 31 83 19 31 03 11 31 23 13 31 43 15 3163 17 4184 19 41 0411 41 24 1341 44 15 41 6417 5185 1951 0511 5125 135145 15 51 6517 61 86 1961 06 11 61 2613 61 46 1561 66 17 718719 71 07 11712713 74757678 012
1
2 0 2 4 6 0 50 100 150
PC 0 Index
31 3
101 1 1117 1 7111775161 711116671131 1656 777169761615 63116 01426854 159 16552111715311058 14551141 641 5 1 1491 14151 41 154 714 12 3 4133 304 812 34 46 391 121 712 1 139 510337133 13811 2162 141211 102 12 812151 11 1 111 17 16 12132199 1 02 599 3 001 01041 1219 9 15 1 10 17 758 31 969113 04 018018120 89 86809409739 8 5809 2688948 762 78401 779776 80 5316 819 77 276 7 706 786856 5 743 5 265 651 9 06675 85 63 624551 5 086544 247 9445 44 533 4 81 049 3 464437 23 305 33 82
3
639 423 20 72 225 2132 82 164 922 12 71 05 11 816 13 191 14 2 7105 8 3 61
4 2
1012 01 245 67 89 11 1011
1
21 3 141 51 617 28 29 202 12 223 2 42 52 627 38 393 03 1323 33435 3637484940 4142 434445464 7585 95051 5253 5 455 5657 6 869 6061 6263 646 5 666 7 7879 7071 7273 747 5 7677 8889 8081 828 3 8485
8
687 9 89 9 9091 9293 9495 9697 1819 1010 101 00 11 01 20 13 01 40 15 01 60 17 11 81 19 11 01 11 11 21 13 11 41 15 11 61 17 21 82 19 21 02 11 21 22 13 21 42 15 21 62 17 31 83 19 31 03 11 31 2313 31431531631741 841941 04 1141 24 13 41 44 15 4164 17 518519 51 051151251351451551651761861961 061161261361461561661771871971 071 17127137 47576 78 01
1
2 0 2 4 6 0 50 100 150
PC 0 Index
Figure19. FirstfewPCprojectionsoftheN embeddingsforamodeltrainedonthetargetS only.
N
all-N
101 11711171 8717177 176715 174713 126 611106 61 19 866117 66 61 4156613 125151015 159 158 57 15165511451 351 24411 14 0411 98441 71 6441 5144413 123 311 103 139 183 137 136 135 134 133 122 121 120 219 128 2176212511 4231 22111111 01191 811711 611151 411 311 201 011 001 901 80 1 701 60159 09 4039 099 2819 97 06 995 493 928 81 089 88 87 86 85 848 83 2771 07 978777 677 547 372606106968667665646 36 2551 05 95 85 75 655 54 53 5241 40948 447 46 45 4 443 42313 03 93 83 736354 33 3322102928272 6252 4232 2211 019 18 171 615 41131 211 09 87 6543 21 2025 .... 5050 0123456789111011121314151617282920212223242526273839303132333435363748494041424344454647585950515253545556576869606162636465666778797071727374757677888980818283848586879899909192939495969718191010101001101201301401501601711811911011111211311411511611721821921021121221321421521621731831931031131231331431531631741841941041141241341441541641751851951051151251351451551651761861961061161261361461561661771871971071171271374757678 01 1
4 2 0 2 4 6 0 50 100 150
PC 0 Index
101 11 711 171 87171 77 17 6715 17 4713 126 6111 06 61 19 866117 66 61 4156613 1251 510151 59 15 85715 16551 145 13 5 124 41 11 40 41 19 84 417 164 4151 4441 31 2331 11 031 391831 37 13 61 35 13 41 3312 21 2112 0219 12 8217 62 125 1 1423 122 11 11 110 119 18 117 1161 115 14 113 1120 1011 0019 01 801701 60159 09 4039 099 28 19 97 069 95 493 928 81 089 8887 8685 848 83 27 71 079 787 77 67 75 473 7260 61 069 686 67 665 6463 625 51 059 5857 565 5 5453 5241 4094 8447 4645 4443 4231 3039 3837 3635 4 333 3221 0292 8272 6252 4232 2211 0191 8171 6 15 41 131 211 09 87 65 43 21 101 01 23 45 67 89 11 1011 121314151 61 72829 2021 22 23 24 25 2627 38393 03 13 233 34 35 36 37 48494 04 1 424 34 44 54 647585 95 0515 25 354 55 56 57 6869 6061 6263 6465 6667 7879 70 71 72 73 7475 7677 8889808182838 48 58 6879 89 99091929 39 4959697 1819 1010 1010011 0120 130 140 15 016 0 17 1 181 19 110 1 11 1121 13 1141 15 1161 17 2182 19 210211 21 221321 421 521 621 731 831 931 031 131 231 331 431531 631741 84 194 104 11 4 12 4 13 4 14 4 15 4 16 417 518 51951 051151 251 351 45155165 17618619 6106 1161 26 13 6146 15 61 6617 71 871971 07117127 137475 7678 01
1
4 2 0 2 4 6 0 50 100 150
PC 0 Index
2 2 1
101 1 17 111 718 71 717 71 767 151 747 1312 661 110 66 11 986 611 76 66141 566 1312515 10151591 585715165 5114 513512 4411 140411 9844 171644 1514 44 1312 33 1110 313 918 313 7136 13 5134 1331 221211 202 191282 176 2125 114 23122 1111 110 119 18 1 171 16 111 5141 131 12 010 110 01 901801 70 1601 5909 4 0390 9928 199 70699 54 9392 88 10 8988 8786 858 4 88 32 7710 7978 77 76 77 54 7372 60 6 10 696866 766 564 6362 55 10 59585 7565554 535 24 14 0 9484 4746 4544 4342 3130 3938 3736 3 54 3 3332 2102 9282 7262 5242 3222 1101 9181 7161 54 11 3 121 10 98 76 54 3
1
101 0 23 45 67 89 11 1011 1 213 1 415 1617 2829 2021 2223 2425 2627 3839 3031 3 233 343 5 3637 484 9 4 041 424 3 4 445 4647 5859 5051 5253 5455 5657 6869 6061 6263 6465 6667 787 97 071 7273 7475 767 78 8898081 8283 848 5 8687 9899 9091 9293 9495 9697 18 19 1010 10100 11 0120 13 01 4015 01 60 1711 8119 11 011111 21 1311 4115 11 61 1721 82 19 21 02 11 21 22 13 21 42 15 2162 17 31 83 19 31 03 11 31 23 13 3143 15 3163 17 4184 19 41 0411 41 241341 441541 641751 85 1951 05 11 51 25 13 51 45 15 5165 17 61 86 19 61 06 11 61 26 13 61 46 15 61 6617 71 87 1971 07 11 71 27 13 7475 767 8 01
1
4 2 0 2 4 6 0 50 100 150
PC 0 Index
Figure20. FirstfewPCprojectionsoftheN embeddingsforamodeltrainedon“all”datai.e.,inthemulti-tasksetting.
19
1 CP
2 CP
3 CP
1 CP
2 CP
3 CP
0 CP
0 CP
1 CP
2 CP
1 CP
2 CP
1 CP
2 CP
3 CP
1 CP
2 CP
3 CPFromNeuronstoNeutrons:ACaseStudyinInterpretability
E.Penultimatelayerfeatures
PC 0 Cumm. RMS: 12446.51 PC 1 Cumm. RMS: 10064.69 PC 2 Cumm. RMS: 1198.46 PC 3 Cumm. RMS: 1196.41 PC 4 Cumm. RMS: 1139.92
1.0
11 57 05
0.8
11 57 05 00 .. 55 33 02 05 11 57 05 0.530 11 57 05 00 .. 55 22 77 33 25 50 11 57 05 0.5276
125 125 0.5275 125 0.525 125 0.527300 125 0.5274
100 0.6 100 0.5250 100 0.520 100 0.527275 100
75 0.4 75 0.5225 75 0.515 75 0.527250 75 0.5272
50 50 0.5200 50 50 50
25 0.2 25 0.5175 25 0.510 25 0.527225 25 0.5270
0 0 0.5150 0 0.505 0 0.527200 0
0 50 100 0 50 100 0 50 100 0 50 100 0 50 100
Z Z Z Z Z
PC 5 Cumm. RMS: 1108.92 PC 6 Cumm. RMS: 1030.71 PC 7 Cumm. RMS: 977.49 PC 8 Cumm. RMS: 971.07 PC 9 Cumm. RMS: 965.62
11 57 05 0.5275 11 57 05 00 .. 55 22 78 70 50 11 57 05 0.5276 11 57 05 0.5274 11 57 05 0.5275
125 0.5274 125 0.52750 125 0.5274 125 0.5273 125 0.5274
100 0.5273 100 0.52725 100 100 0.5272 100 0.5273
75 75 0.52700 75 0.5272 75 75
50 0.5272 50 0.52675 50 50 0.5271 50 0.5272
0.5270
25 0.5271 25 0.52650 25 25 0.5270 25 0.5271
0 0 0.52625 0 0.5268 0 0 0.5270
0 50 100 0 50 100 0 50 100 0 50 100 0 50 100
Z Z Z Z Z
PC 10 Cumm. RMS: 911.30 PC 11 Cumm. RMS: 766.02 PC 12 Cumm. RMS: 678.48 PC 13 Cumm. RMS: 670.43 PC 14 Cumm. RMS: 654.20
175 0.5278 175 0.5278 175 0.5278 175 0.52750 175
150 150 0.5276 150 150 0.52745 150 0.5274
125 0.5276 125 0.5274 125 0.5276 125 0.52740 125
0.5274 0.5272 0.5274 0.52735 0.5273
100 100 100 100 100
75 0.5272 75 00 .. 55 22 67 80 75 0.5272 75 00 .. 55 22 77 23 50 75 0.5272
50 0.5270 50 0.5266 50 0.5270 50 0.52720 50
25 0.5268 25 0.5264 25 0.5268 25 0.52715 25 0.5271
0 0.5266 0 0.5262 0 0 0.52710 0
0 50 100 0 50 100 0 50 100 0 50 100 0 50 100
Z Z Z Z Z
Figure21.VisualizationofofafewpenultimatelayerPCfeaturesandtheircumulativeeffectontheerrorinbindingenergyprediction
(theerroriscomputeduptoandincludingthePC).
20
N
N
N
N
N
N
N
N
N
N
N
N
N
N
NFromNeuronstoNeutrons:ACaseStudyinInterpretability
pairing volume surface coulomb asymmetry SEMF
150 5.0 150 4000 150 150 1250 150 300 150 8000
2.5 3000 600 1000 6000
10 50
0
0. 520 .. 05 10 50
0
12 00 00 00 10 50
0
24 00 00 10 50
0
257 505 000 10 50
0
12 00 00 10 50
0
24 00 00 00
0 0 50 100 7.5 0 0 50 100 0 0 50 100 0 0 50 100 0 0 0 50 100 0 0 0 50 100 0
Z Z Z Z Z Z
shell rotational exchange wigner strutinsky BW2
0 0
125 8000
150 2 150 80 150 150 150 200 150 100 150 6000
100 100 60 100 100 100 100 75 100 4 400 50 4000
50 50 40 50 50 50 50 25 50 2000
0 0 50 100 6 0 0 50 100 20 0 0 50 100 0 0 50 100 600 0 0 50 100 0 0 0 50 100 0
Z Z Z Z Z Z
Figure22.Physicstermsvisualized.ThetoprowarethetermsfromtheSEMF.Thebottomrowincludesnuclearshellmodelcorrections
(BW2terms).
pairing PC 6 ( =-0.66) PC 10 ( =0.35) PC 0 ( =-0.20)
150
100
50
0
volume PC 0 ( =1.00) PC 1 ( =-0.04) PC 7 ( =0.03)
150
100
50
0
shell PC 0 ( =-0.44) PC 3 ( =0.38) PC 5 ( =0.29)
150
100
50
0
0 50 100 0 50 100 0 50 100 0 50 100
Z Z Z Z
Figure23.Modelpenultimatefeaturesinthemulti-tasksetting. PhysicaltermsderivedfromtheNuclearShellModelandtheirbest
matchingPCs.
21
N
N
N
N
N
N
N
N
N
N
N
N
N
N
NFromNeuronstoNeutrons:ACaseStudyinInterpretability
F.Otherstructures
Wediscussedhowthehelixstructure(essentiallystackedcircles)isidealtomodelthecontinuousspectrumofbinding
energies. However,continuitycanberealizedinotherwaysthaninacircle(orhelixwhenconsideringPC0),forinstance
by a simple line. In fact, we believe that the circular structure is chosen by the model because weight decay favors a
continuousstructureifitrevolvesaround0. Acircularstructurepresentsagoodtradeoffbetweenembeddingweightnorm
andsufficientdistancebetweenelementstoformseparatepredictionsforeachZ orN withoutresortingtohighweight
norminotherlayers. Figure24showsN embeddingprojectionsfromamodeltrainedwithoutweightdecay, butwith
somewhatcomparabletestsetperformance. Ashypothesized,acontinuousstructureemerges,butnohelix. Thisbehaviour
isconceptuallyconsistentoverdifferentrandomseeds.
9
10
11
12
13
14
15
16
17
18
19
14119414 11847
44161544 1 143 4 1142 11 33013918 113713 1336 1135 343 1123211221 90211 782162 125124 23 11 122 111 11 10 1111179 81 11651114 11321 10101110 10908010116 079504 90903992 97081 996995 8493 821 8890 88 878688 45382 71 77097877767574
3772 6601 698666 7666 5463625150 5958576555455
352410449484 74 6544 4324433108393 733 63 543233
31022 98
2276252243222210
Figure24. NeutronembeddingsprojectedintothefirsttwoPCfromamodeltrainedwithoutweightdecay.
G.Symbolicregression
We use symbolic regression to find functions fi (Z,N) that map from Z and N to the i-th feature extracted from the
PC
penultimatelayer. WeusethePySRlibrary(Cranmer,2023),whichemploysanevolutionarytree-basedalgorithm.10,
Subsequently,wemaywritethenewexpressionforthebindingenergyasE =(cid:80)nF a fi (Z,N)+b,wheren isthe
B i=1 i PC F
numberofPCfeaturesthatareused. Thecoefficientsa andtheinterceptbaredeterminedusinglinearregressiononthe
i
bindingenergydatasetwithouttheTMSvalues. WefindthattheusingthefitsofsolelyPC0andPC2,wecanretainthebulk
oftheprediction. Thenewexpressionforbindingenergyreads,
E =a (cid:0) −0.09+10−6Z2(cid:1) [A+2.5sin(0.25−0.13N +0.2Z)]+a 0.97N +b. (5)
B 1 2
wherea =−88062.52,a =−171331.53andb=95815.44. ThisformulaachievesanRMSofaround4600keV.Asa
1 2
comparison,theperformanceoftheSEMFoverthesamedatasetis8000keV.Noteably,anydirectregressiononthedata
leadstoconsiderablyworsepredictionsforthesamenumberoffreeparameters. Weassessthus,thattheanalysisofthe
representationspaceofneuralnetworksmaystreamlinesymbolicregressiontasks.
H.Limitations
The interpretability of the extracted knowledge is not guaranteed. Even if the network finds a low-rank structure, it
may not necessarily correspond to a simple, interpretable theory that provides clear insight to domain experts. The
learnedrepresentationsmightcapturecomplex, nonlinearinteractionsthatarehardtodistillintocompact, explainable
expressions. Moreover, there is currently a lack of quantitative metrics to assess the interpretability of the extracted
knowledge. Developingsuchmetricsiscrucial,asthatwhichismeasuredcanbeimproved. Withoutawaytoquantify
interpretability,itbecomeschallengingtotrackprogressanditerateontechniquestoenhancetheclarityandusefulnessof
10Inthephysicalsciences,thismethodhasprovenusefulforextractingsymbolicformulasthatrevealnewphysicalpatternsorreinterpret
knownphysicallaws(Mengeletal.,2023;Davis&Jin,2023;Lemosetal.,2023).
22FromNeuronstoNeutrons:ACaseStudyinInterpretability
thederivedinsightsfordomainexperts. Asseenintheattemptsatsymbolicregression,theexpressionsrecoveredfrom
theneuralfeaturesdidnotyieldfullyinterpretableimprovementsoverhuman-derivedmodels. Thislimitationhighlights
theneedformorerigorousmetricstoguidethesearchformoreexplainableandmeaningfulrepresentationsofthelearned
knowledge.
Additionally, integrating MI into the scientific discovery workflow requires interdisciplinary collaborations and close
partnershipsbetweenmachinelearningresearchersanddomainexperts. Translatingbetweenthelanguageofneuralnetwork
componentsandthescientificconceptsofagivenfieldisasignificantchallengethatdemandsdedicatedeffortfromboth
sidestohaveareal-worldimpactindrivingscientificprogress.
23