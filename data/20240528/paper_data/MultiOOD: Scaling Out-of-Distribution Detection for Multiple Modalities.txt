MultiOOD: Scaling Out-of-Distribution Detection for
Multiple Modalities
HaoDong1 YueZhao2 EleniChatzi1 OlgaFink3
1ETHZürich 2UniversityofSouthernCalifornia 3EPFL
{hao.dong, chatzi}@ibk.baug.ethz.ch, yzhao010@usc.edu, olga.fink@epfl.ch
Abstract
Detectingout-of-distribution(OOD)samplesisimportantfordeployingmachine
learning models in safety-critical applications such as autonomous driving and
robot-assistedsurgery. Existingresearchhasmainlyfocusedonunimodalscenar-
iosonimagedata. However,real-worldapplicationsareinherentlymultimodal,
whichmakesitessentialtoleverageinformationfrommultiplemodalitiestoen-
hancetheefficacyofOODdetection. Toestablishafoundationformorerealistic
MultimodalOODDetection,weintroducethefirst-of-its-kindbenchmark,Multi-
OOD,characterizedbydiversedatasetsizesandvaryingmodalitycombinations.
We first evaluate existing unimodal OOD detection algorithms on MultiOOD,
observingthatthemereinclusionofadditionalmodalitiesyieldssubstantialim-
provements. Thisunderscorestheimportanceofutilizingmultiplemodalitiesfor
OOD detection. Based on the observation of Modality Prediction Discrepancy
betweenin-distribution(ID)andOODdata,anditsstrongcorrelationwithOOD
performance, weproposetheAgree-to-Disagree(A2D)algorithmtoencourage
such discrepancy during training. Moreover, we introduce a novel outlier syn-
thesismethod,NP-Mix,whichexploresbroaderfeaturespacesbyleveragingthe
informationfromnearestneighborclassesandcomplementsA2Dtostrengthen
OODdetectionperformance. ExtensiveexperimentsonMultiOODdemonstrate
thattrainingwithA2DandNP-MiximprovesexistingOODdetectionalgorithms
byalargemargin. OursourcecodeandMultiOODbenchmarkareavailableat
https://github.com/donghao51/MultiOOD.
1 Introduction
Mostexistingmachinelearning(ML)modelsaretrainedundertheclosed-worldassumption,where
thetestdataisassumedtobedrawni.i.d. fromthesamedistributionasthetrainingdata,referred
toasin-distribution(ID).However,inopen-worldscenarios,testsamplescanbeout-of-distribution
(OOD),thusimpactingmodelrobustnessandsafety[63]. OODdetectionaimstodetectsampleswith
semanticshiftsthatareundesirableforthemodeltogeneralize[63]andiscriticalfordeployingML
modelsinsafety-criticaldomainssuchasautonomousdriving[18],robotics[15,3],anddiagnostics
forcriticalassets[21]. NumerousOODdetectionalgorithmshavebeendeveloped,rangingfrom
classification-basedtodistance-basedmethods[62]. Classification-basedmethodstypicallyderive
confidencedirectlyfromtheclassifier,employingpost-hocprocessingtechniquessuchasMaximum
SoftmaxProbability(MSP)[28]andEnergy[39]ortrainingstrategiessuchaslogitnormalization[61]
andoutliersynthesis[19]. Distance-basedmethodstypicallymeasuredistancesinhigh-dimensional
feature spaces to distinguish between ID and OOD [37, 55]. Additionally, other methodologies
exploredensityestimation[1,46]andreconstructiontechniques[66]forOODdetection.
CurrentresearchinOODdetectionhaspredominantlyfocusedonunimodalsettings,ofteninvolving
images as inputs [62]. While several recent works [43, 59] have investigated vision-language
Preprint.Underreview.
4202
yaM
72
]VC.sc[
1v91471.5042:viXraFigure1: TheFPR95(lowerisbetter)andAUROC(higherisbetter)onHMDB51datasetacross
variousmodalities. MultimodalOODsubstantiallyimprovesunimodalOODw/obellsandwhistles.
models[49]toenhanceOODperformance,theirevaluationsarestilllimitedtobenchmarkscontaining
solely images. Consequently, existing methods fall short in fully leveraging the complementary
informationfromvariousmodalities, suchasLiDARandcamerainautonomousdriving[18], as
wellasvideo,audio,andopticalflowinactionrecognition[52]. Tounderscoretheimportanceof
using multiple modalities in OOD detection, we evaluate representative OOD algorithms across
variousmodalitiesontheHMDB51[35]datasetwithinourMultiOODbenchmark. Thisisanaction
recognitiontaskandallmodelsaretrainedsolelyusingcross-entropylossbetweenaone-hottarget
vectorandthesoftmaxoutput. ResultsinFig.1showthatevenasimplefusionofvideoandoptical
flowmodalitiescansubstantiallyenhanceOODdetectionperformance.
ToestablishafoundationformorerealisticMultimodalOODDetection,weintroduceanovelOOD
benchmarknamedMultiOOD(Fig.2),whichisthefirstbenchmarkforMultimodalOODDetection
andcoversdiversedatasetsizesandmodalities. MultiOODcomprisesfivevideodatasetswithover
85,000videoclipsintotal. Thedatasetsvaryinthenumberofclasses,rangingfrom7to229,andin
size,spanningfrom3kto57k. Video,opticalflow,andaudioareusedasdifferenttypesofmodalities.
While most existing unimodal OOD algorithms designed for images can be directly applied to
MultimodalOODDetection,suchapproachesmayyieldsuboptimalresultswithoutaccountingfor
theinteractionandcomplementarynatureofdiversemodalities. AsdepictedinFig.1,theAUROC
performanceisverycloseforallunimodalbaselines,underscoringtheimportanceofdeveloping
OODdetectionalgorithmstailoredtoeffectivelyexploitinformationfrommultiplemodalities.
Inthiswork,wefirstidentifyandillustratetheModalityPredictionDiscrepancyphenomenononthe
MultiOODbenchmark,wherethediscrepanciesofsoftmaxpredictionsacrossdifferentmodalities
areshowntobenegligibleforIDdatawhilesignificantforOODdata(Fig.3). Wediscoverastrong
correlationbetweensuchdiscrepanciesandtheOODdetectionperformance(Fig.4). Motivatedby
theseobservations,weintroducetheAgree-to-Disagree(A2D)algorithm,whichaimstoenhance
suchdiscrepanciesduringtraining. Thealgorithmisdesignedsothatdifferentmodalitiesshould
Agreeonthepredictionoftheground-truthclass,andDisagreeonotherclassesbymaximizingthe
distance between their predictions. Additionally, we propose a novel outlier synthesis algorithm
namedNP-Mix,designedtousetheinformationfromnearestneighborclassestoexplorebroader
featurespacesandcomplementA2DtostrengthentheOODdetectionperformance.
ExtensiveexperimentsontheMultiOODbenchmarkdemonstratethesuperiorityofA2DandNP-Mix.
The integration of A2D and NP-Mix yields substantial performance enhancements over existing
unimodalOODdetectionalgorithms. Forinstance,ontheUCF101[53]datasetwithinMultiOOD,
our approach reduces the FPR95 from 32.14% to 10.68% for ASH [14] method, representing a
noteworthyabsolute21.46%improvementoverthebaseline. Ourcontributionsinclude:
• We highlight the significance of integrating more modalities for OOD detection and introduce
MultiOOD,thefirstbenchmarkforMultimodalOODDetectionencompassingdiversedatasetsizes
andvariouscombinationsofmodalities.
• We conduct comprehensive evaluations of existing unimodal OOD algorithms on MultiOOD,
revealingtheirlimitationsinmultimodalscenarios.
• WeproposeanovelA2Dtrainingalgorithm,inspiredbytheobservationoftheModalityPrediction
Discrepancy phenomenon, alongside a new outlier synthesis algorithm NP-Mix that explores
broaderfeaturespacesandcomplementsA2DtostrengthentheOODdetectionperformance.
• ExtensiveexperimentsconductedonMultiOODunderscoretheeffectivenessofA2DandNP-Mix.
OursourcecodeandMultiOODbenchmarkwillbemadepubliclyavailable,facilitatingfuture
researchendeavorsinMultimodalOODDetection.
24× Multimodal Near-OOD benchmark 2× Multimodal Far-OOD benchmark
Video Optical Flow Audio
EPIC-Kitchens HMDB51 UCF101 Kinetics-600
HMDB51 Kinetics-600
4/4 25/26 50/51 129/100
In-Distribution In-Distribution Kinetics-600
catch clap dribble fencing
Out-of-Distribution
golf jump kiss laugh
Out-of-Distribution HMDB51 UCF101
pullup punch situp somersault EPIC-Kitchens HAC
Figure2: AnoverviewofMultiOODBenchmark.
2 Preliminaries: MultimodalOut-of-distributionDetection
MultimodalOODDetectionaimstodetectsampleswithsemanticshiftsusingmultiplemodalities.
WeconsideratrainingsetD = (x ,y ) n drawni.i.d. fromthejointdatadistributionP ,
in { i i }i=1 XY
where istheinputspaceand = 1,2,...,C isthediscretelabelspace. Eachtrainingsample
X Y { }
x is comprised of M modalities, denoted as x = xk k = 1, ,M . Let denote the
mi arginaldistributionon andf : RC bei aneu{ rali n| etworktra· i· n· edon} samplP esin inP that
XY
X X (cid:55)→
predictsthelabelofeachinputsample. Thef inMultimodalOODDetectioncomprisesofM feature
extractorsg ()andaclassifierh(). Eachfeatureextractorg ()extractsanembeddingZk forits
k k
· · ·
correspondingmodalityk,andtheclassifierh()takesthecombinedembeddingsfromallmodalities
·
asinputandoutputsapredictionprobabilitypˆ:
pˆ=δ(f(x))=δ(h([g (x1),...,g (xM)]))=δ(h([Z1,...,ZM])), (1)
1 M
whereδ()isthesoftmaxfunction. Wefurtherincludeaseparateclassifierh ()foreachmodality
k
· ·
k to getpredictionsfrom eachmodality separately, with theprediction probabilityfrom the k-th
modalityaspˆk =δ(h (g (xk))).
k k
Whendeployingf intherealworld,itshouldnotonlyaccuratelyclassifyknownsamplesasID,but
alsoidentifyany“unknown”sampleasOOD.AseparatescorefunctionS(x)isoftenusedtodecide
whetherasamplex isfrom (ID)ornot(OOD):
in
∈X P
(cid:26)
ID S(x) η
G η(x)=
OOD
S(x)≥
<η
,
wheresampleswithhigherscoresS(x)areclassifiedasIDandviceversa,ηisthethreshold. Existing
OODdetectionstudiespredominantlyfocusonunimodalscenarios,withadetailedliteraturereview
offeredinAppendixA.ToestablishafoundationformorerealisticMultimodalOODDetection,
weintroduceanovelMultiOODbenchmark(Sec.3)andproposeaneffectivemultimodaltraining
strategy(Sec.4)thatyieldssignificantenhancementsoverexistingunimodalapproaches.
3 MultiOODBenchmark
WecreatetheMultiOODbenchmarktounderstandtheexistinggapinMultimodalOODDetection
research. OODdetectionprimarilyfocusesondetectingsemanticshifts,withtwomainapproaches
usedforconstructingOODbenchmarks. Acommonmethodinvolvesconsideringanentiredatasetas
in-distribution(ID)andfurthercollectsdatasets,whichcomprisesimilartasksbutaredisconnected
fromanyIDcategories,asOODdatasets.Inthisscenario,bothsemanticanddomainshiftsarepresent
between the ID and OOD samples. We term this setup as Far-OOD in our benchmark. Another
approachistopartitionthecategoriesofexistingdatasetsintotwosubsets,referredtoasclosed(ID)
andopenset(OOD).Here,bothIDandOODsamplesoriginatefromthesamedistribution,withonly
semanticshiftsexistingbetweenthem. WedenotethissetupasNear-OODwithinourbenchmark;
3Distance: 0.16 Distance: 1.90
     
(a) Prediction for ID data (b) Prediction for OOD data
Figure3: AnexampleofsoftmaxoutputsforIDandOODdata. Thepredictionsfromvideoand
opticalflowdemonstrateuniformityacrossIDdataandexhibitvariabilityacrossOODdata.
asetupthatposesgreaterchallengescomparedtoFar-OOD.Thissetupisalsoreferredtoasopen
setrecognition(OSR)insomestudies[57,33,8]. Notably,OSRandOODdetectionbothsharethe
samegoalofidentifyingtestsampleswithsemanticshiftswithoutcompromisingtheaccuracyofID
classification[63]. Inourbenchmark,wetreatOSRandOODassynonymousconceptsandadopt
OODasthegeneralterm. MultiOODcomprisesfiveactionrecognitiondatasets(EPIC-Kitchens[44],
HAC[17],HMDB51[35],UCF101[53],andKinetics-600[5])withover85,000videoclipsintotal.
Thedatasetsvaryinthenumberofclasses,rangingfrom7to229,andinsize,spanningfrom3kto
57k. Video,opticalflow,andaudioareusedasdifferenttypesofmodalities. Anoverviewofthe
MultiOODbenchmarkisprovidedinFig.2,withadditionaldetailsavailableinAppendixB.
3.1 MultimodalNear-OODBenchmark
IntheNear-OODsetup,weincludefourdatasets. EPIC-Kitchens4/4isderivedfromtheEPIC-
Kitchens Domain Adaptation dataset [44], where the dataset is partitioned into four classes for
training as ID and four classes for testing as OOD, with a total of 4,871 video clips. Similarly,
HMDB51 25/26 and UCF101 50/51 are constructed based on HMDB51 [35] and UCF101 [53],
withatotalof6,766and13,320videoclipsrespectively. InthecaseofKinetics-600129/100,we
select229actionclassesfromtheKinetics-600dataset[5],witheachclasscomprisingapproximately
250videoclipsandatotalof57,205videoclips. Withinthissetup,129classesaredesignatedfor
trainingasID,whiletheremaining100classesareallocatedfortestingasOOD.
3.2 MultimodalFar-OODBenchmark
IntheFar-OODsetup,weincludeHMDB51andKinetics-600asIDdatasets.
HMDB51 dataset as ID. For the OOD datasets, we utilize UCF101, EPIC-Kitchens, HAC, and
Kinetics-600datasets. AllofthesedatasetsarecarefullycuratedtoremovesamplesthatbelongtoID
classesinHMDB51. GiventherelativelysmallnumberofclassesintheEPIC-KitchensandHAC
datasets,weremove8classesintheHMDB51datasetthatoverlapwithEPIC-KitchensandHAC,
with43classesleftasIDclasses. ForUCF101,weremove31overlappingclasseswithHMDB51,
resultingin70classesdesignatedasOODclassesforevaluation. Forotherdatasets,noclassoverlap
existsandweutilizetheiroriginalclassesasOOD.
Kinetics-600 dataset as ID. Similarly, we adopt UCF101, EPIC-Kitchens, HAC, and HMDB51
datasets as OOD datasets, with careful selection undertaken to exclude samples belonging to ID
classesinKinetics-600. Wecarefullyselectedasubsetof229actionclassesfromKinetics-600inthe
Near-OODsetuptomitigatethepotentialoverlapwithotherdatasets. FortheUCF101dataset,we
remove11overlappingclasseswithKinetics-600,leaving90classesasOODclassesforevaluation.
Forotherdatasets,therearenoclassoverlapissuesandweusetheiroriginalclassesasOOD.
4 Methodology
Inthissection,wefirstidentifytheModalityPredictionDiscrepancyphenomenonontheMultiOOD
benchmarkanddemonstrateitssubstantialcorrelationwiththeOODdetectionperformance(Sec.4.1).
Subsequently,weintroducetheAgree-to-Disagree(A2D)algorithmaimedatenhancingsuchdiscrep-
4Figure 4: Average prediction L distances between ID and OOD data (l l ) before and
1 OOD ID
−
afterA2DtrainingacrossvariousdatasetswithintheMultiOOD,whereEnergy[39]isusedasscore
function. The distances are highly correlated to the ultimate OOD performance. A2D training
amplifiessuchdistances,consequentlyenhancingtheefficacyofOODdetection.
anciesduringtraining(Sec.4.2). Finally,weproposethenoveloutliersynthesisalgorithmnamed
NP-MixthatcomplementsA2DtofurtherstrengthentheOODdetectionperformance(Sec.4.3).
4.1 ModalityPredictionDiscrepancy
WefirstexplorethepredictivebehaviorsdemonstratedbyvariousmodalitieswhenusingbothID
andOODdataasinputonMultiOOD.Wecomputethepredictionprobabilitiesemployingclassifiers
forvideoandopticalflowonthesameIDandOODsamplesandcalculatetheirL distances. As
1
depictedinFig.3,forIDdata,thepredictionprobabilitiesofbothvideopˆ1andopticalflowpˆ2are
generallyexhibitedconsistentwitheachotherontheground-truthlabel,consequentlyyieldinga
small L distance pˆ1 pˆ2 between them. Conversely, for OOD data, each modality tends to
1 1
∥ − ∥
expressvaryingconfidencepreferencestowardsdistinctclasses,resultinginanotableincreasein
the L distance between their predictions. We refer to this phenomenon as Modality Prediction
1
DiscrepancybetweenIDandOODdata. Thisdiscrepancycanbeattributedtotheunavailabilityof
semanticinformationonOODdataduringmodeltraining,stimulatingeachmodalitytogenerate
conjecturesbasedonitsuniquecharacteristicsuponencounteringOODdataduringtesting.
Toverifytheuniversalityofthisphenomenon,wecalculatetheaverageL distancebetweenpredic-
1
tionsofvideopˆ1 andopticalflowpˆ2 onbothIDandOODdatawithintheHMDB51dataset. The
averagepredictionL distanceis0.63forIDdata(l )and1.42forOODdata(l ),revealinga
1 ID OOD
substantialdiscrepancy. Inaddition,weevaluateotherdatasetsintheNear-OODsetupwithinthe
MultiOODbenchmarkandobservesimilardiscrepancies(l l ). Suchdiscrepancieshavea
OOD ID
−
positivecorrelationwiththeOODdetectionperformance,asillustratedinFig.4.
4.2 Agree-to-DisagreeAlgorithm
Motivated by the Modality Prediction Discrepancy and its strong correlation with Multimodal
OOD Detection performance, we introduce the Agree-to-Disagree (A2D) algorithm to foster the
amplificationofsuchdiscrepanciesduringtraining. Theunderlyingideaisthatdifferentmodalities
should Agree on the prediction regarding the ground-truth class, while they should Disagree on
theremainingclassesbymaximizingtheirpredictiondistance. A2Denablesthemodeltodiversify
predictionsacrossmodalities,consequentlyyieldinghighpredictiondiscrepanciesforOODdata
duringtesting.
Given a training sample x with label c, we obtain prediction probabilities pˆfrom the combined
embeddingsofallmodalities,andpˆ1,pˆ2fromindividualmodality,allofwhichareofshape[1,C],
whereC representsthenumberofclasses. Byremovingthec-thvaluefrompˆ1 andpˆ2 (different
modalitiesshouldAgreeonthepredictionregardingtheground-truthclass),wederivenewprediction
probabilitieswithoutground-truthclasses,denotedasp¯1andp¯2withshapes[1,C 1]. Subsequently,
weaimtomaximizethediscrepancybetweenp¯1andp¯2,whichcanbedefinedas−
:
= Discr(p¯1,p¯2), (2)
Discr
L −
5  
Modality 1
A2D
Training
  
Modality 2
Agree-to-Disagree (A2D)
Outlier Synthesis Using NP-Mix
0.5
+
Prototypes 0.5
Nearest Neighbor
Synthesized Outliers
Figure5: AnoverviewoftheproposedframeworkforMultimodalOODDetection. Weintroduce
A2Dalgorithmtoencourageenlargingthepredictiondiscrepancyacrossmodalities. Additionally,
weproposeanoveloutliersynthesisalgorithm,NP-Mix,designedtoexplorebroaderfeaturespaces,
whichcomplementsA2DtostrengthentheOODdetectionperformance.
whereDiscr()isadistancemetricquantifyingthesimilaritybetweentwoprobabilitydistributions.
·
WeutilizetheHellingerdistance[45]andexploretheefficacyofalternativedistancemetricsinour
ablationstudy. TheHellingerdistancebetweentwoprobabilitydistributionsisdefinedas:
(cid:118)
(cid:117) (cid:117)1C (cid:88)−1(cid:18)(cid:113) (cid:113) (cid:19)2
D(p¯1,p¯2)=(cid:116) p¯1 p¯2 . (3)
2 i − i
i=1
Furthermore,toensurethattheground-truthclassespossessthehighestprobabilities,weincorporate
thecross-entropylossCE()foreachprediction,definedas:
·
1
= (CE(pˆ,y)+CE(pˆ1,y)+CE(pˆ2,y)). (4)
Lcls 3
Thefinallossisobtainedastheweightedsumofthepreviouslydefinedlosses:
= +γ , (5)
cls Discr
L L L
wherethehyperparameterγ controlstherelativeimportanceofthediscrepancyterm.
4.3 NearestNeighborPrototype-basedMixupforOutlierSynthesis
Outliersynthesis[19,56]hasdemonstrateditsefficacyinOODdetectionbyimposingregularization
onthemodel’sdecisionboundaryduringtraining. IntroducingthediscrepancylossinA2Dtothe
synthesizedoutlierdataformodelregularizationhasthepotentialtofurtherenhanceOODdetection
performance. However,existingoutliersynthesismethods[19,56]typicallygenerateoutliersnear
theIDdata(Fig.7),neglectingtoexplorethebroaderembeddingspaces,therebypotentiallyleading
tosuboptimalperformance. Inspiredbytherecentapproachintroducedin[16],weintroduceanovel
algorithmtermedNearestNeighborPrototype-basedMixup(NP-Mix),aimedatsynthesizingoutliers
capableofspanningwiderembeddingspacesbyleveragingtheinformationfromnearestneighbor
classes,asshowninFig.5andFig.7. Tosynthesizeoutliers,weconcatenatetheembeddingsofall
modalities(Z=[Z1,Z2])andtreatthemasaunifiedentity. Subsequently,wecomputeaprototype
embeddingZˆ foreachclasscbycalculatingthemeanofallembeddingswithinthatclass. Foreach
c
prototypeembeddingZˆ ,weidentifyitstopN nearestneighborprototypesandrandomlyselectone
c
prototypeZˆ fromthemformixing. Twosamples,Z andZ ,arerandomlychosenfromclasscand
s 1 2
classsrespectively. TheoutlierZ(cid:101) isgeneratedbytheirconvexcombination:
Z(cid:101) =λZ 1+(1 λ)Z 2, (6)
−
6Table1:MultimodalNear-OODDetectionusingvideoandopticalflow. indicateslargervaluesare
↑
betterandviceversa. TrainingwithA2DandNP-Mixyieldsconsiderableperformanceenhancements.
HMDB5125/26 UCF10150/51 EPIC-Kitchens4/4 Kinetics-600129/100
Methods
FPR95 AUROC IDACC FPR95 AUROC IDACC FPR95 AUROC IDACC FPR95 AUROC IDACC
↓ ↑ ↑ ↓ ↑ ↑ ↓ ↑ ↑ ↓ ↑ ↑
WithoutA2DTraining
MSP 44.66 87.74 89.32 22.14 95.73 99.22 76.31 67.59 71.46 64.08 76.16 80.11
Energy 43.36 87.46 89.32 22.52 96.06 99.22 76.68 68.29 71.46 68.75 75.49 80.11
MaxLogit 43.36 87.75 89.32 22.52 96.02 99.22 76.68 68.29 71.46 68.73 75.98 80.11
Mahalanobis 40.31 85.28 89.32 12.14 97.14 99.22 98.69 42.99 71.46 93.51 35.83 80.11
ReAct 42.05 87.79 89.32 25.63 95.85 99.32 83.96 65.89 71.08 72.40 73.80 80.35
ASH 53.59 87.16 89.54 32.14 94.02 99.22 76.87 67.92 70.15 69.24 76.16 79.62
GEN 43.79 87.49 89.32 23.79 95.54 99.22 76.87 68.52 71.46 69.03 75.33 80.11
KNN 42.92 88.46 89.32 15.63 96.93 99.22 75.93 63.60 71.46 68.67 74.64 80.11
VIM 36.82 88.06 89.32 12.52 97.66 99.22 77.05 65.60 71.46 68.77 75.47 80.11
LogitNorm 48.84 87.65 88.89 19.61 95.85 99.51 80.97 63.41 71.83 67.32 75.84 80.23
WithA2DTraining
MSP+ 38.78−5.88 88.37+0.63 90.63 7.09−15.05 98.19+2.46 99.61 66.23−10.08 71.04+3.45 71.46 63.04−1.04 76.47+0.31 79.50
Energy+ 39.22−4.14 88.84+1.38 90.63 9.81−12.71 98.16+2.10 99.61 66.98−9.70 72.45+4.16 71.46 64.59−4.16 76.45+0.96 79.50
MaxLogit+ 39.22−4.14 88.93+1.18 90.63 9.81−12.71 98.15+2.13 99.61 66.98−9.70 72.23+3.94 71.46 64.57−4.16 76.92+0.94 79.50
Mahalanobis+ 44.88+4.57 86.99+1.71 90.63 8.74−3.40 98.07+0.97 99.61 95.52−3.17 44.43+1.44 71.46 92.86−0.65 50.65+14.82 79.50
ReAct+ 37.91−4.14 89.09+1.30 90.63 10.39−15.24 98.12+2.27 99.61 68.66−15.30 72.03+6.14 70.52 71.26−1.14 74.29+0.49 79.64
ASH+ 42.05−11.54 87.72+0.56 90.41 12.52−19.62 97.43+3.41 99.42 63.25−13.62 74.73+6.81 67.72 64.28−4.96 77.28+1.12 79.15
GEN+ 38.56−5.23 88.61+1.12 90.63 8.83−14.96 98.12+2.58 99.61 65.86−11.01 73.05+4.53 71.46 62.28−6.75 77.08+1.75 79.50
KNN+ 33.33−9.59 89.59+1.13 90.63 7.48−8.15 98.39+1.46 99.61 72.39−3.54 67.83+4.23 71.46 65.71−2.96 76.23+1.59 79.50
VIM+ 33.77−3.05 89.37+1.31 90.63 6.80−5.72 98.62+0.96 99.61 66.42−10.63 68.43+2.83 71.46 64.59−4.18 76.45+0.98 79.50
LogitNorm+ 40.52−8.32 89.33+1.68 91.07 12.43−7.18 97.57+1.72 99.71 77.61−3.36 67.59+4.18 73.51 62.63−4.69 77.76+1.92 79.82
WithA2DTrainingandNP-MixOutlierSynthesis
MSP++ 33.99−10.67 88.79+1.05 89.98 7.96−14.18 98.24+2.51 99.71 67.16−9.15 71.52+3.93 71.64 62.91−1.17 76.92+0.76 80.52
Energy++ 36.38−6.98 88.91+1.45 89.98 6.50−16.02 98.48+2.42 99.71 67.91−8.77 73.79+5.50 71.64 63.69−5.06 77.11+1.62 80.52
MaxLogit++ 36.38−6.98 89.06+1.31 89.98 6.50−16.02 98.49+2.47 99.71 66.98−9.70 73.48+5.19 71.64 63.65−5.08 77.55+1.57 80.52
Mahalanobis++ 41.61+1.30 87.69+2.41 89.98 7.86−4.28 97.99+0.85 99.71 94.78−3.91 44.37+1.38 71.64 92.49−1.02 50.05+14.22 80.52
ReAct++ 37.47−4.58 88.63+0.84 90.20 12.43−13.20 97.33+1.48 99.90 66.60−17.36 73.11+7.22 72.39 67.95−4.45 75.55+1.75 80.70
ASH++ 36.17−17.42 89.30+2.14 89.32 10.68−21.46 97.80+3.78 99.81 66.23−10.64 73.06+5.14 66.23 63.77−5.47 77.44+1.28 79.44
GEN++ 35.95−7.84 89.78+2.29 89.98 7.67−16.12 98.28+2.74 99.71 65.30−11.57 75.17+6.65 71.64 62.95−6.08 76.95+1.62 80.52
KNN++ 33.77−9.15 90.05+1.59 89.98 12.04−3.59 97.65+0.72 99.71 71.27−4.66 69.94+6.34 71.64 66.81−1.86 74.19−0.45 80.52
VIM++ 34.64−2.18 88.80+0.74 89.98 4.56−7.96 98.73+1.07 99.71 67.72−9.33 69.09+3.49 71.64 63.67−5.10 77.12+1.65 80.52
LogitNorm++ 36.38−12.46 90.38+2.73 89.54 9.81−9.80 97.14+1.29 99.71 72.01−8.96 72.91+9.50 71.46 63.02−4.30 77.57+1.73 80.33
where λ Beta(α,α), for α (0, ). In our experiments, we opt for α > 1 to ensure the
∼ ∈ ∞
synthesizedoutliersresideintheintermediaryspacebetweentwoprototypes,ratherthannearthe
IDdata. WethenpartitionZ(cid:101) intodifferentmodalitiesasZ(cid:101) =[Z(cid:101)1,Z(cid:101)2],whereZ(cid:101)1andZ(cid:101)2represent
thesynthesizedoutlierembeddingsforeachmodality. Subsequently,thecorrespondingprediction
probabilitiesarecomputedasp (cid:101)1 =δ(h 1(Z(cid:101)1))andp (cid:101)2 =δ(h 2(Z(cid:101)2)).
Forsynthesizedoutliers,weaimtomaximizethepredictiondiscrepanciesbetweendifferentmodali-
ties,similartoEq.(2)forIDtrainingsamples. Inthiscontext,thereisnoneedtoremoveanyvalue
fromtheprediction,astheoutliersareassumednottobelongtoanyIDclass. Thediscrepancyloss
betweenp1andp2canbedefinedas:
(cid:101) (cid:101)
= Discr(p1,p2). (7)
Discr_syn (cid:101) (cid:101)
L −
Moreover,weseektomitigatetheoverconfidenceofpredictionsforsynthesizedoutlierstowardsany
existingIDclass. Therefore,wemaximizetheentropyofpredictions:
1
= (H(p1)+H(p2)), (8)
LEnt −2 (cid:101) (cid:101)
(cid:80)
whereH(p)istheentropyofpredictionpandcanbecalculatedasH(p) = p logp . The
(cid:101) (cid:101) (cid:101)
−
c(cid:101)c (cid:101)c
finallossisobtainedastheweightedsumofthepreviouslydefinedlosses:
= +γ( + + ). (9)
cls Discr Discr_syn Ent
L L L L L
5 Experiments
5.1 ExperimentalSetting
Baselines. Our objective is to enhance existing unimodal OOD algorithms through multimodal
training utilizing the proposed A2D and NP-Mix. To validate the efficacy and versatility of our
proposed approaches, we include representative algorithms that leverage information from the
probabilityspace(MSP[28],GEN[40]),logitspace(Energy[39],MaxLogit[27]),featurespace
(Mahalanobis [37], KNN [55]), both logit and feature spaces (VIM [58]), penultimate activation
manipulations(ReAct[54],ASH[14]),andtraining-timeregularization(LogitNorm[61]).
Evaluationmetrics. Weevaluatetheperformanceviatheuseofthefollowingmetrics: (1)thefalse
positiverate(FPR95)ofOODsampleswhenthetruepositiverateofIDsamplesisat95%,(2)the
7Table2: MultimodalFar-OODDetectionusingvideoandopticalflow,withHMDB51asID.
OODDatasets
Methods Kinetics-600 UCF101 EPIC-Kitchens HAC Average IDACC
FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC ↑
↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑
WithoutA2DTraining
Energy 32.95 92.48 44.93 87.95 8.10 97.70 32.95 92.28 29.73 92.60 87.23
ASH 51.20 87.81 53.93 84.22 19.95 95.92 42.99 90.23 42.02 89.55 86.20
GEN 41.51 90.34 46.18 87.91 8.21 98.26 38.31 91.28 33.55 91.95 87.23
KNN 22.69 95.01 39.34 89.28 9.92 97.92 20.75 96.02 23.18 94.56 87.23
VIM 13.68 97.01 33.87 91.45 5.93 98.15 13.45 97.12 16.73 95.93 87.23
WithA2DTrainingandNP-MixOutlierSynthesis
Energy++ 24.52−8.43 93.96+1.48 36.49−8.44 89.67+1.72 6.96−1.14 97.53−0.17 22.92−10.14 94.41+2.13 22.72−7.01 93.89+1.29 86.89
ASH++ 27.82−23.38 93.17+5.36 38.43−15.50 89.52+5.30 6.84−13.11 98.23+2.31 23.03−19.96 94.45+4.22 24.03−17.99 93.84+4.29 86.20
GEN++ 25.66−15.85 93.50+3.16 37.40−8.78 91.19+3.28 5.25−2.96 98.98+0.72 24.63−13.68 94.28+3.00 23.24−10.31 94.49+2.54 86.89
KNN++ 15.05−7.64 96.96+1.95 33.06−6.28 91.92+2.64 5.47−4.45 98.97+1.05 13.45−7.30 97.25+1.23 16.76−6.42 96.28+1.72 86.89
VIM++ 9.24−4.44 98.04+1.03 26.45−7.42 92.34+0.89 5.36−0.57 98.09−0.06 6.04−7.41 98.56+1.44 11.77−4.96 96.76+0.83 86.89
Table3: MultimodalFar-OODDetectionusingvideoandopticalflow,withKinetics-600asID.
OODDatasets
Methods HMDB51 UCF101 EPIC-Kitchens HAC Average IDACC
FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC ↑
↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑
WithoutA2DTraining
Energy 72.64 71.75 70.12 71.49 43.66 82.05 61.50 74.99 61.98 75.07 73.14
ASH 71.62 76.66 69.36 72.38 34.38 88.05 47.85 83.49 55.80 80.15 72.20
GEN 68.47 78.43 64.80 73.97 36.81 85.11 49.53 83.67 54.90 80.30 73.14
KNN 71.08 78.84 68.62 74.33 41.83 82.32 57.00 82.53 59.63 79.51 73.14
VIM 72.25 71.88 70.72 70.58 43.14 82.69 59.48 75.46 61.40 75.15 73.14
WithA2DTrainingandNP-MixOutlierSynthesis
Energy++ 63.27−9.37 74.17+2.42 67.20−2.92 74.50+3.01 34.07−9.59 87.49+5.44 56.69−4.81 80.20+5.21 55.31−6.67 79.09+4.02 73.67
ASH++ 63.65−7.97 79.14+2.48 62.15−7.21 75.32+2.94 36.09+1.71 88.69+0.64 46.47−1.38 85.21+1.72 52.09−3.71 82.09+1.94 72.53
GEN++ 61.67−6.80 80.63+2.20 57.03−7.77 77.97+4.00 40.09+3.28 85.10−0.01 41.97−7.56 86.98+3.31 50.19−4.71 82.67+2.37 73.67
KNN++ 64.60−6.48 80.28+1.44 67.44−1.18 77.95+3.62 37.65−4.18 83.55+1.23 53.63−3.37 83.34+0.81 55.83−3.80 81.28+1.77 73.67
VIM++ 63.25−9.00 73.96+2.08 66.31−4.41 74.08+3.50 34.45−8.69 87.67+4.98 53.82−5.66 81.06+5.60 54.46−6.94 79.19+4.04 73.67
areaunderthereceiveroperatingcharacteristiccurve(AUROC),and(3)IDclassificationaccuracy
(IDACC).Foreachbaselinealgorithm,wereporttheresultsbothwithandwithoutA2Dtrainingand
NP-Mixoutliersynthesis. AdditionalimplementationdetailsareprovidedinAppendixD.
5.2 MultimodalNear-OODDetection
WecommenceourevaluationbyassessingtheefficacyofA2DtrainingandNP-Mixoutliersynthesis
withintheMultimodalNear-OODDetectionsetup. AspresentedinTab.1,thesimpleincorporation
ofA2DyieldssubstantialenhancementsinOODperformanceacrossnearlyallscenarios.Theaverage
predictionL distancesbetweenIDandOODdata(l l )beforeandafterA2Dtrainingacross
1 OOD ID
−
variousdatasetsaredepictedinFig.4. Notably,acrossalldatasets,A2Dtrainingservestofurther
enlargetheModalityPredictionDiscrepancyandconsequentlyimproveOODdetectionperformance,
verifying the strong correlation between them. Specifically, A2D training reduces the FPR95 by
uptoabsolute19.62%onUCF10150/51datasetforASHandincreasesAUROCupto14.82%for
Mahalanobis on Kinetics-600 129/100. Combining A2D training with NP-Mix yields additional
improvementsinOODdetectionperformance,underscoringtheefficacyofoutliersynthesisinmodel
regularization. Additionally, A2D training and NP-Mix outlier synthesis exhibit notable efficacy
acrossallbaselineOODdetectionalgorithmsdespitetheirdifferentunderlyingprinciples,indicating
theversatilityofourproposedmethod.
5.3 MultimodalFar-OODDetection
Tab.2andTab.3presenttheresultsofMultimodalFar-OODDetectionwithHMDB51andKinetics-
600asIDdatasets,respectively. SimilartotheNear-OODsetup,trainingwithA2DandNP-Mix
yieldsconsiderableenhancementsinOODdetectionperformanceacrossmostcasesforallbaseline
algorithms. Specifically,trainingwithbothA2DandNP-MixachievesreductionsinFPR95ofupto
absolute23.38%ontheHMDB51datasetforASHandupto14.43%forReActontheKinetics-600
dataset. Duetospacelimits,weprovidecomprehensiveresultsinAppendixE(Tab.7andTab.8).
Interestingly, we observe that the performance on the HMDB51 dataset generally surpasses that
oftheKinetics-600dataset. ThisfindingalignswithobservationsfromunimodalOODdetection
benchmarks[62],whereperformanceondatasetswithfewerclasses(e.g.,CIFAR-10[34])tendsto
outperformthoseondatasetswithagreaternumberofclasses(e.g.,ImageNet[12]).
8Table4: MultimodalNear-OODDetectionusingvideo,audio,andopticalflow.
Modality EPIC-Kitchens4/4 Kinetics-600129/100
Methods
Video Audio Flow FPR95 AUROC IDACC FPR95 AUROC IDACC
↓ ↑ ↑ ↓ ↑ ↑
WithoutA2DTraining
Energy ✓ ✓ ✓ 69.22 72.39 73.13 66.42 76.60 80.33
ASH ✓ ✓ ✓ 70.52 69.70 68.10 63.48 78.11 79.54
GEN ✓ ✓ ✓ 70.34 70.99 73.13 64.24 77.54 80.33
KNN ✓ ✓ ✓ 80.22 60.56 73.13 75.03 65.97 80.33
VIM ✓ ✓ ✓ 76.12 59.03 73.13 66.38 76.59 80.33
WithA2DTrainingandNP-MixOutlierSynthesis
Energy++ ✓ ✓ ✓ 62.69−6.53 74.95+2.56 71.46 63.81−2.61 77.89+1.29 80.82
ASH++ ✓ ✓ ✓ 69.78−0.74 69.49−0.21 67.72 61.22−2.26 78.57+0.46 80.05
GEN++ ✓ ✓ ✓ 63.62−6.72 73.94+2.95 71.46 63.55−0.69 77.92+0.38 80.82
KNN++ ✓ ✓ ✓ 68.47−11.75 67.87+7.31 71.46 71.46−3.57 68.87+2.90 80.82
VIM++ ✓ ✓ ✓ 73.51−2.61 59.57+0.54 71.46 63.42−2.96 77.90+1.31 80.82
Table5: AblationonDistanceFunctionsforNear-OODDetectiononHMDB51dataset.
Methods
Baseline L1 L2 Wasserstein Hellinger
FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC
↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑
Energy 43.36 87.46 37.04 88.52 37.69 88.84 36.17 88.61 36.38 88.91
GEN 43.79 87.49 39.87 90.34 42.27 88.84 37.91 88.76 35.95 89.78
KNN 42.92 88.46 35.73 90.24 35.51 89.78 36.60 88.57 33.77 90.05
VIM 36.82 88.06 32.24 89.36 33.99 89.03 33.99 89.24 34.64 88.80
Table6: AblationonOutlierSynthesisMethodsforNear-OODDetectiononHMDB51dataset.
Baseline VOS NPOS Mixup NP-Mix
Methods
FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC
↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑
Energy 43.36 87.46 37.69 87.94 43.14 87.59 42.27 87.77 36.38 88.91
GEN 43.79 87.49 37.69 89.51 44.44 88.11 42.70 88.28 35.95 89.78
KNN 42.92 88.46 38.34 88.55 40.52 88.81 36.17 89.61 33.77 90.05
VIM 36.82 88.06 35.51 88.93 37.69 88.17 37.04 88.44 34.64 88.80
5.4 AblationStudies
MultimodalNear-OODDetectionwithothermodalities. WedemonstratetheefficacyofA2D
trainingandNP-Mixoutliersynthesisacrossvariouscombinationsofmodalities,notlimitedtovideo
andopticalflow,asdetailedinTab.4andTab.9inAppendixE.Notably,theperformanceofmost
algorithmsisconsistentlyimprovedwithA2DandNP-Mix, regardlessofwhichcombinationsof
modalitiesareused.
Effectivenessofotherdistancefunctions. Wesubstitutethedistancemetricformeasuringprobabil-
itydistributionsinthediscrepancylosswithalternativedistancefunctions,includingL ,L ,and
1 2
Wasserstein[2]distances. AsdepictedinTab.5,A2Dtrainingexhibitsrobustnessacrossvarious
distancefunctions. Regardlessofthespecificdistancemetricemployed,substantialimprovements
areconsistentlyobservedcomparedtothebaselineapproachwithoutA2Dtraining.
Comparedwithotheroutliersynthesismethods. Toevaluatetheeffectivenessofotheroutlier
synthesismethods,wereplaceNP-MixwithVOS[19],NPOS[56],andMixup[65]andtrainwith
A2D.AllbaselinemethodsyieldimprovementsinOODperformance,underscoringthesignificance
ofoutliersynthesisformodelregularization. Notably,NP-Mixemergesasthemosteffectiveamong
thevariousbaselinesbysynthesizingoutliersthatspanbroaderembeddingspaces.
6 Conclusion
Inthispaper,weintroducetheMultimodalOut-of-distributionDetectionproblemandpresentanovel
benchmark,MultiOOD,whichincludesdiversedatasetsizesandvariouscombinationsofmodalities.
MotivatedbytheModalityPredictionDiscrepancyphenomenonobservedwithinMultiOOD,we
proposeanovelA2Dtrainingalgorithmtoencouragetheenlargementofsuchdiscrepancyduring
model training, along with a new outlier synthesis algorithm, NP-Mix, that complements A2D.
ExtensiveexperimentsonMultiOODandunderNear-OODandFar-OODsetupsverifytheefficacy
oftheproposedapproaches. WehopeourworkwillinspirefutureresearchendeavorsinMultimodal
OODDetection.
LimitationandFutureWork. TheperformanceonNear-OODbenchmarksandondatasetswitha
largenumberofclassesstillexhibitspotentialforenhancement. Moreover,theexplorationofOutlier
Exposure[29]deservesattentionasapotentialtobetterlearnID/OODdiscrepancy.
9A RelatedWork
Out-of-Distribution(OOD)Detectionaimstodetecttestsampleswithsemanticshiftwithoutlosing
theIDclassificationaccuracy. NumerousOODdetectionalgorithmshavebeendeveloped,withpost
hocmethodsandtraining-timeregularizationasmajorcategories[62].Posthocmethodsaimtodesign
OODscoresbasedontheclassificationoutputofneuralnetworks,offeringtheadvantageofbeing
easytousewithoutmodifyingthetrainingprocedureandobjective.Earlyapproachesincludeutilizing
MaximumSoftmaxProbability(MSP)[28]asOODscore,oftencoupledwithtemperaturescaling
andinputperturbation[38].Insteadofusingsoftmaxprobabilities,MaxLogit[27]employsmaximum
logitasOODscoresratherthansoftmax. Energy-basedalgorithm[39]demonstratedtheefficacyof
energyfunction[36]inquantifyingOOD-ness. OtherapproacheslikeReAct[54]improvedexisting
scoringfunctionsbytruncatingtheactivationswithhighvalues. Similarly,ASH[14]prunesalarge
portionoftheinputactivationsandadjuststheremainingactivationsusingpruning,binarizing,or
scaling. MethodslikeMahalanobis[37]andk-NearestNeighbor(KNN)[55]usedistancemetricsin
featurespaceforOODdetection,whileVirtual-logitMatching(VIM)[58]integratesinformationfrom
bothfeatureandlogitspacestodefinetheOODscore. Recently,GeneralizedEntropy(GEN)[40]
proposedanentropy-basedscorefunctionthatprovestobebothsimpleandeffective.
Training-timeregularizationmethodssuchasLogitNorm[61]addresspredictionoverconfidence
byimposingaconstantvectornormonthelogitsduringtraining. OutlierExposure[29]leverages
externalOODsamplesfromotherdatasetsduringtrainingtofacilitatethelearningofbetterIDand
OODdiscrepancy. Additionally,someapproaches[19,56]proposedsynthesizingvirtualoutliersfor
training-timeregularization. However,allpreviousapproachesweredesignedforunimodalscenarios,
withoutaccountingfortheinteractionandcomplementarynatureofdiversemodalities.
MultimodalOODDetection. Recentendeavors[43,59]haveexploredvision-languagemodels[49]
to enhance OOD performance, which are also referred to as multimodal in some of the studies.
MaximumConceptMatching(MCM)[43]definesOODscorebyaligningvisualfeatureswithtextual
concepts. CLIPN[59]equipsCLIP[49]withthecapabilityofdistinguishingOODandIDsamples
usingpositive-semanticpromptsandnegation-semanticprompts. However,theevaluationsofall
theseworksarestilllimitedtobenchmarksexclusivelycontainingimages. Consequently,existing
methodsfallshortinfullyleveragingthecomplementaryinformationfromvariousmodalities,such
as LiDAR and camera in autonomous driving [18], as well as video, audio, and optical flow in
action recognition [52]. There is also a lack of benchmark datasets that facilitate the evaluation
of Multimodal OOD Detection. Therefore, we aim to develop a more practical and challenging
benchmarkincorporatingmultiplecombinationsofmodalities(i.e.,video,audio,andopticalflow).
ThisenablesthecreationofOODdetectionalgorithmsthatarespecificallydesignedtoleveragethe
complementarynatureofvariousmodalitieseffectively.
AnomalyDetectionandOpenSetRecognitionaretwocloselyrelatedfieldstoOODDetection.
AnomalyDetection(AD)aimstodetectpatternsthatdeviatefromthepredefinednormalityduring
testing[63]andtreatsallin-distributionsamplesasasingleclass. Therefore,ADalgorithmscan
beappliedtoOODdetectionbyignoringallthelabelsforIDdata. TypicalADalgorithmsinclude
unsupervised[50,4],semi-supervised[51,67],andsupervised[23,48],dependingontheavailability
oflabels[25]. OpenSetRecognition(OSR)[57,33,8]focusesonaccuratelyclassifyingtestsamples
from “known known classes” (ID) and detecting test samples from “unknown unknown classes”
(OOD). While OOD detection benchmarks always take one dataset as ID and find several other
datasetswithnon-overlappingcategoriesasOOD,OSRbenchmarksusuallysplitonemulti-class
classificationdatasetintoIDandOODpartsaccordingtoclasses. However,bothOSRandOOD
detectionsharethesamegoalofidentifyingtestsampleswithsemanticshiftswithoutcompromising
the accuracy of ID classification [63]. Therefore, in our benchmark, we treat OSR and OOD as
synonymousconceptsandadoptOODasthegeneralterm. OurNear-OODBenchmarkissimilarto
traditionalOSRsetupandourFar-OODBenchmarkisthesameasgeneralOODsetup.
OODBenchmarks.Earlyworks[28]inOODdetectionprimarilyfocusonsmall-scaleimagedatasets
suchasMNIST[13]andCIFAR-10/100[34]. RecognizingtheneedforevaluatingOODdetectionat
scale,studiessuchas[58]introducenewOODdatasetsbasedonImageNet[12]. Additionally,some
OODbenchmarks[27,7]arespecificallydesignedforsemanticsegmentationtasks. OpenOOD[62]
offersacomprehensiveOODbenchmarkcomprisingdatasetsfrompreviousworksandincorporating
over30commonOODmethods. However,allofthesebenchmarksarelimitedtoimagedata. In
contrast,ourMultiOODisthefirstpublicOODbenchmarkthatencompassesdifferentcombinations
ofmodalities,facilitatingfutureresearchendeavorsinMultimodalOODDetection.
10EPIC-Kitchens HAC HMDB51
UCF101 Kinetics-600
Figure6: VisualizationofactionrecognitiondatasetsusedinourMultiOODbenchmark.
B MoreDetailsontheMultiOODBenchmark
B.1 DatasetsUsedinMultiOODBenchmark
Actionrecognitionisinherentlymultimodalandservesastheprimarytaskwithinourbenchmark,
andweincludefiveactionrecognitiondatasetsaccordinglyofvaryingsizes,asshowninFig.6.
EPIC-Kitchens[11]. TheEPIC-Kitchensdatasetisalarge-scaleegocentricdatasetcollectedby32
participantsintheirnativekitchenenvironments. Theparticipantswereaskedtocapturealltheirdaily
kitchenactivitiesandrecordsequencesregardlessoftheirduration. Thestartandendtimesforeach
actionareannotated. WeuseasubsetoftheEPIC-KitchensdatasetintroducedintheMultimodal
DomainAdaptationpaper[44],whichcomprises4,871videoclipsfrom8largestactionclassesin
sequenceP22. Theseactionsinclude‘put’,‘take’,‘open’,‘close’,‘wash’,‘cut’,‘mix’,and‘pour’,
withprovidedmodalitiesincludingvideo,opticalflow,andaudio.
HAC[17]. TheHACdatasetencompassessevenactions(‘sleeping’,‘watchingtv’,‘eating’,‘drink-
ing’,‘swimming’,‘running’,and‘openingdoor’)performedbyhumans,animals,andcartoonfigures.
Thereare3,381videoclipsintotalfromsevenactions. Modalitiesprovidedinthisdatasetinclude
video,opticalflow,andaudio.
HMDB51[35]. TheHMDB51datasetisavideoactionrecognitiondataset,comprising6,766video
clipsspanning51actioncategories. Thevideoclipsareextractedfromavarietyofsourcesranging
fromdigitizedmoviestoYouTube. Availablemodalitiesinthisdatasetincludevideoandopticalflow.
UCF101[53]. UCF101isanothervideoactionrecognitiondatasetcollectedfromYouTube,com-
prising13,320videoclipsfrom101actions. UCF101offerssubstantialdiversityinactiontypes
andencompassessignificantvariationsincameramotion,objectappearanceandpose,objectscale,
viewpoint,clutteredbackground,illuminationconditions,etc. Modalitiesprovidedinthisdataset
includevideoandopticalflow.
Kinetics-600[5]. Kinetics-600isalarge-scaleactionrecognitiondataset,featuringapproximately
480kvideosspanning600actioncategories. Eachvideointhedatasetisa10-secondclipofaction
moment annotated from YouTube videos. In our benchmark, we carefully selected a subset of
229actionclassesfromKinetics-600tomitigatethepotentialcategoryoverlapwithotherdatasets,
with each class comprising roughly 250 video clips, yielding a total of 57,205 video clips. The
originaldatasetprovidesvideoandaudiomodalities. Tomakeitconsistentwiththeotherdataset,
ourbenchmarkfurtherprovidestheextractedopticalflowforallvideoclips,amountingtoatotalof
114,410opticalflowsamples. Thedenseopticalflowisextractedat24framespersecondusingthe
TV-L1algorithm[64].
B.2 MultimodalNear-OODBenchmark
IntheNear-OODsetup,weincorporatefourdatasets. EPIC-Kitchens4/4isderivedfromtheEPIC-
KitchensDomainAdaptationdataset[44],wherethedatasetisrandomlypartitionedintofourclasses
fortrainingasIDandfourclassesfortestingasOOD.Similarly,HMDB5125/26andUCF10150/51
11(a) VOS (b) NPOS
(c) Mixup (d) NP-Mix (ours)
Figure7: Visualizationofsynthesizedoutlierscomparedagainstothermethods. VOSandNPOS
tendtogenerateoutliersneartheIDdata,neglectingtoexplorethebroaderembeddingspace. Mixup
randomlyselectssamplesfromallclassestomixandinadvertentlyintroducesunwantednoisesamples
withinthedistributionofIDdata. NP-Mixexcelsatgeneratingsynthesizedoutliersbyeffectively
utilizinginformationfromneighborclassesandspanningwiderembeddingspaces.
areconstructedbasedonHMDB51[35]andUCF101[53],respectively. InthecaseofKinetics-600
129/100,wecurate229actionclassesfromtheKinetics-600dataset[5],witheachclasscomprising
approximately250videoclips. Withinthissetup,129classesarerandomlydesignatedfortrainingas
ID,whiletheremaining100classesareallocatedfortestingasOOD.
B.3 MultimodalFar-OODBenchmark
IntheFar-OODsetup,weincorporateHMDB51andKinetics-600asIDdatasets.
HMDB51 dataset as ID. For the OOD datasets, we utilize UCF101, EPIC-Kitchens, HAC, and
Kinetics-600 datasets. All of these datasets are carefully curated to remove samples that should
belongtoIDclasses. GiventherelativelysmallnumberofclassesintheEPIC-KitchensandHAC
datasets,weremove8classesintheHMDB51datasetthatoverlapwithEPIC-KitchensandHAC,
including‘chew’,’climb_stairs’,‘drink’,‘eat’,‘pick’,‘pour’,’ride_horse’,’run’,leaving43classes
asIDclasses. InthecaseofUCF101,weremove31overlappingclasseswithHMDB51,resultingin
70classesdesignatedasOODclassesforevaluation. ForKinetics-600,weusethesamesubsetof
229classesasintheNear-OODsetup,whicharecarefullyselectedtomitigatethepotentialcategory
overlapwithotherdatasets. Forotherdatasets,noclassoverlapexistsandweutilizetheiroriginal
classesasOOD.
Kinetics-600 dataset as ID. Similarly, we adopt UCF101, EPIC-Kitchens, HAC, and HMDB51
datasets as OOD datasets, with careful curation undertaken to exclude samples belonging to ID
classes. Wecarefullyselectedasubsetof229actionclassesfromKinetics-600intheNear-OOD
setuptomitigatethepotentialoverlapwithotherdatasets. FortheUCF101dataset,weremove11
overlappingclasseswithKinetics-600,leaving90classesasOODclassesforevaluation. Forother
datasets,therearenoclassoverlapissues,andweusetheiroriginalclassesasOOD.
12(a) Energy (b) ASH (c) KNN (d) LogitNorm
(e) Energy++ (f) ASH++ (g) KNN++ (h) LogitNorm++
Figure8: ScoredistributionsofdifferentbaselinemethodsontheHMDB5125/26datasetbeforeand
aftertrainingwithA2DandNP-Mix.
(a) Embeddings without A2D and NP-Mix (b) Embeddings with A2D and NP-Mix
Figure9: VisualizationofthelearnedembeddingsonIDandOODdatausingt-SNEontheHMDB51
25/26datasetbeforeandaftertrainingwithA2DandNP-Mix.
C VisualizationofResults
Visualization of Synthesized Outliers. We visualize the outliers generated by different outlier
synthesisalgorithms,includingVOS[19],NPOS[56],Mixup[65],andourproposedNP-Mix. As
showninFig.7,VOSandNPOSgenerateoutliersneartheIDdata,neglectingtoexplorethebroader
embeddingspace. Mixuprandomlyselectssamplesfromallclassestomixandintroducesunwanted
noisesampleswithinthedistributionofIDdata. NP-Mixexcelsatgeneratingsynthesizedoutliersby
effectivelyutilizinginformationfromneighboringclassesandspanningwiderembeddingspaces.
ScoreDistributions. Fig.8illustratesthescoredistributionsgeneratedbyvariousbaselinemethods
ontheHMDB5125/26datasetbeforeandaftertrainingwithA2DandNP-Mix. Scoredistributions
producedbyA2DandNP-MixleadtobetterID/OODseparation,resultinginstrengthenedOOD
detectionperformance.
VisualizationofLearnedEmbeddingsforIDandOODData. Fig.9showsthevisualizationofthe
learnedembeddingsusingt-SNE[42]ontheHMDB5125/26datasetbeforeandaftertrainingwith
A2DandNP-Mix. TheembeddingofIDandOODdataaremoreseparableafterA2Dtrainingand
NP-Mixoutliersynthesis.
13D MoreImplementationDetails
D.1 TrainingDetails
We conduct experiments across three modalities: video, audio, and optical flow. We adopt the
MMAction2[10]toolkitforexperiments. Toencodethevisualinformation, weutilizeSlowFast
network[20],initializedwithpre-trainedweightsfromKinetics-400[30]. Fortheaudioencoder,we
employResNet-18[26],initializingtheweightsfromtheVGGSoundpre-trainedcheckpoint[9].
Similarly, we use the SlowFast network with a slow-only pathway, again leveraging pre-trained
weightsfromKinetics-400[30]fortheopticalflowencoder. WeusetheAdamoptimizer[32]witha
learningrateof0.0001andabatchsizeof16. Additionally,wesetthehyperparametersasfollows:
γ =0.5,mixupα=10.0,nearestneighborN =2. Wetrainthenetworkfor50epochsonanRTX
3090GPUandselectthemodelwiththebestperformanceonthevalidationdataset.
D.2 ExtensiontoMoreModalities
OurframeworkisnotlimitedtotwomodalitiesandcanbeeasilyextendedtoM modalities. Given
atrainingsamplexwithlabelcandM modalities,weobtainpredictionprobabilitiespˆfromthe
combinedembeddingsofallmodalities, andpˆ1, pˆ2, ..., pˆM fromeachmodality, allofwhichare
ofshape[1,C],whereC representsthenumberofclasses. Byremovingthec-thvaluefromeach
prediction,wederivenewpredictionprobabilitieswithoutground-truthclasses,denotedasp¯1,p¯2,...,
p¯M withshapes[1,C 1]. Subsequently,weaimtomaximizethediscrepancybetweenp¯1,p¯2,...,
p¯M,whichcanbedefi−
nedas:
M−1 M
2 (cid:88) (cid:88)
= Discr(p¯i,p¯j). (10)
LDiscr −M(M 1)
− i=1 j=i+1
whereDiscr()isadistancemetricquantifyingthesimilaritybetweentwoprobabilitydistributions.
·
Similarly,for , and ,wecandefineas:
cls Discr_syn Ent
L L L
M
1 (cid:88)
= (CE(pˆ,y)+ CE(pˆi,y)), (11)
Lcls M +1
i=1
M−1 M
2 (cid:88) (cid:88)
= Discr(pi,pj), (12)
LDiscr_syn −M(M 1) (cid:101) (cid:101)
− i=1 j=i+1
M
1 (cid:88)
= H(pi). (13)
LEnt −M (cid:101)
i=1
Thefinallossisobtainedastheweightedsumofthepreviouslydefinedlosses:
= +γ( + + ). (14)
cls Discr Discr_syn Ent
L L L L L
D.3 InferenceDetails
ForalgorithmsthatdefineOODscoreortruncateactivationsleveraginginformationfromthefeature
space (Mahalanobis [37], KNN [55], VIM [58], ReAct [54], ASH [14]), we use the combined
embeddingZ = [Z1,Z2,...,ZM]fromallmodalities. ForalgorithmsthatdefinetheOODscore
leveraginginformationfromtheprobabilityspaceorlogitspace(MSP[28],GEN[40],Energy[39],
MaxLogit[27],VIM[58],LogitNorm[61]),weusethepredictionprobabilitiespˆorpredictionlogits
h(Z)obtainedfromthecombinedembeddingsofallmodalities.
E FurtherExperimentalResults
MultimodalFar-OODDetection. Tab.7andTab.8presentcomprehensiveresultsforMultimodal
Far-OODDetectionontheHMDB51andKinetics-600datasets. TrainingwithA2DandNP-Mix
significantlyimprovesOODperformanceinmostcasesacrossallbaselinealgorithms,underscoring
theversatilityofourproposedmethod.
14Table7: MultimodalFar-OODDetectionusingvideoandopticalflow,withHMDB51asID.
OODDatasets
Methods Kinetics-600 UCF101 EPIC-Kitchens HAC Average IDACC
FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC ↑
↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑
WithoutA2DTraining
MSP 39.11 88.78 46.64 86.40 17.33 95.99 39.91 89.10 35.75 90.07 87.23
Energy 32.95 92.48 44.93 87.95 8.10 97.70 32.95 92.28 29.73 92.60 87.23
MaxLogit 33.07 92.31 44.93 88.02 9.12 97.77 33.06 92.17 30.05 92.57 87.23
Mahalanobis 14.03 96.69 43.22 86.68 18.13 93.30 11.97 97.10 21.84 93.44 87.23
ReAct 27.59 93.54 44.01 88.05 7.53 97.61 31.01 92.86 27.54 93.02 87.00
ASH 51.20 87.81 53.93 84.22 19.95 95.92 42.99 90.23 42.02 89.55 86.20
GEN 41.51 90.34 46.18 87.91 8.21 98.26 38.31 91.28 33.55 91.95 87.23
KNN 22.69 95.01 39.34 89.28 9.92 97.92 20.75 96.02 23.18 94.56 87.23
VIM 13.68 97.01 33.87 91.45 5.93 98.15 13.45 97.12 16.73 95.93 87.23
LogitNorm 46.07 87.41 49.03 85.96 15.96 96.30 47.09 87.64 39.54 89.33 86.09
WithA2DTrainingandNP-MixOutlierSynthesis
MSP++ 29.42−9.69 90.73+1.95 40.02−6.62 88.08+1.68 13.34−3.99 96.43+0.44 28.16−11.75 91.63+2.53 27.74−8.01 91.72+1.65 86.89
Energy++ 24.52−8.43 93.96+1.48 36.49−8.44 89.67+1.72 6.96−1.14 97.53−0.17 22.92−10.14 94.41+2.13 22.72−7.01 93.89+1.29 86.89
MaxLogit++ 24.86−8.21 93.69+1.38 36.60−8.33 89.71+1.69 6.96−2.16 97.67−0.10 22.92−10.14 94.22+2.05 22.84−7.21 93.82+1.25 86.89
Mahalanobis++ 9.01−5.02 97.72+1.03 27.94−15.28 91.09+4.41 12.77−5.36 95.96+2.66 7.64−4.33 98.23+1.13 14.34−7.50 95.75+2.31 86.89
ReAct++ 21.09−6.50 94.72+1.18 37.51−6.50 89.66+1.61 7.30−0.23 97.38−0.23 20.64−10.37 95.01+2.15 21.63−5.91 94.19+1.17 86.66
ASH++ 27.82−23.38 93.17+5.36 38.43−15.50 89.52+5.30 6.84−13.11 98.23+2.31 23.03−19.96 94.45+4.22 24.03−17.99 93.84+4.29 86.20
GEN++ 25.66−15.85 93.50+3.16 37.40−8.78 91.19+3.28 5.25−2.96 98.98+0.72 24.63−13.68 94.28+3.00 23.24−10.31 94.49+2.54 86.89
KNN++ 15.05−7.64 96.96+1.95 33.06−6.28 91.92+2.64 5.47−4.45 98.97+1.05 13.45−7.30 97.25+1.23 16.76−6.42 96.28+1.72 86.89
VIM++ 9.24−4.44 98.04+1.03 26.45−7.42 92.34+0.89 5.36−0.57 98.09−0.06 6.04−7.41 98.56+1.44 11.77−4.96 96.76+0.83 86.89
LogitNorm++ 29.53−16.54 91.44+4.03 30.22−18.81 91.37+5.41 13.23−2.73 96.81+0.51 25.88−21.21 93.16+5.52 24.72−14.82 93.20+3.87 86.89
Table8: MultimodalFar-OODDetectionusingvideoandopticalflow,withKinetics-600asID.
OODDatasets
Methods HMDB51 UCF101 EPIC-Kitchens HAC Average IDACC
FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC ↑
↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑
WithoutA2DTraining
MSP 66.83 75.64 67.32 71.13 43.37 86.66 56.17 79.50 58.42 78.23 73.14
Energy 72.64 71.75 70.12 71.49 43.66 82.05 61.50 74.99 61.98 75.07 73.14
MaxLogit 72.06 73.68 70.47 71.96 39.84 84.76 57.95 77.27 60.08 76.92 73.14
Mahalanobis 89.01 56.24 81.85 64.49 93.12 41.59 95.88 48.85 89.97 52.79 73.14
ReAct 82.17 67.09 78.44 67.64 53.49 78.07 75.11 70.04 72.30 70.71 73.27
ASH 71.62 76.66 69.36 72.38 34.38 88.05 47.85 83.49 55.80 80.15 72.20
GEN 68.47 78.43 64.80 73.97 36.81 85.11 49.53 83.67 54.90 80.30 73.14
KNN 71.08 78.84 68.62 74.33 41.83 82.32 57.00 82.53 59.63 79.51 73.14
VIM 72.25 71.88 70.72 70.58 43.14 82.69 59.48 75.46 61.40 75.15 73.14
LogitNorm 66.48 79.00 63.79 75.10 39.03 85.27 54.22 81.83 55.88 80.30 74.47
WithA2DTrainingandNP-MixOutlierSynthesis
MSP++ 62.76−4.07 77.53+1.89 66.33−0.99 73.13+2.00 52.84+9.47 83.78−2.88 53.37−2.80 80.82+1.32 58.83+0.41 78.82+0.59 73.67
Energy++ 63.27−9.37 74.17+2.42 67.20−2.92 74.50+3.01 34.07−9.59 87.49+5.44 56.69−4.81 80.20+5.21 55.31−6.67 79.09+4.02 73.67
MaxLogit++ 61.70−10.36 75.95+2.27 67.46−3.01 74.98+3.02 32.24−7.60 88.74+3.98 52.52−5.43 81.95+4.68 53.48−6.60 80.41+3.49 73.67
Mahalanobis++ 85.93−3.08 57.32+1.08 81.96+0.11 62.14−2.35 89.08−4.04 50.37+8.78 95.38−0.50 48.55−0.30 88.09−1.87 54.60+1.81 73.67
ReAct++ 73.00−9.17 70.09+3.00 76.09−2.35 71.40+3.76 39.06−14.43 85.42+7.35 70.09−5.02 76.04+6.00 64.56−7.74 75.74+5.03 73.65
ASH++ 63.65−7.97 79.14+2.48 62.15−7.21 75.32+2.94 36.09+1.71 88.69+0.64 46.47−1.38 85.21+1.72 52.09−3.71 82.09+1.94 72.53
GEN++ 61.67−6.80 80.63+2.20 57.03−7.77 77.97+4.00 40.09+3.28 85.10−0.01 41.97−7.56 86.98+3.31 50.19−4.71 82.67+2.37 73.67
KNN++ 64.60−6.48 80.28+1.44 67.44−1.18 77.95+3.62 37.65−4.18 83.55+1.23 53.63−3.37 83.34+0.81 55.83−3.80 81.28+1.77 73.67
VIM++ 63.25−9.00 73.96+2.08 66.31−4.41 74.08+3.50 34.45−8.69 87.67+4.98 53.82−5.66 81.06+5.60 54.46−6.94 79.19+4.04 73.67
LogitNorm++ 67.67+1.19 79.15+0.15 59.14−4.65 77.91+2.81 31.57−7.46 88.76+3.49 57.51+3.29 78.92−2.91 53.97−1.91 81.19+0.89 70.81
Multimodal Near-OOD Detection with Other Combination of Modalities. We demonstrate
the effectiveness of A2D and NP-Mix across various combinations of modalities, not limited to
video and optical flow, as shown in Tab. 9. The performance of different baseline algorithms
improvessignificantlywithA2DtrainingandNP-Mixoutliersynthesis,regardlessofwhetherthe
inputmodalitiesarevideo-audio,flow-audio,orvideo-audio-flowcombinations.
F MoreAblations
ComparedwithOtherMultimodalTasks. WecompareA2DandNP-Mixwithothermultimodal
self-supervisedtrainingtasks,includingContrastiveLoss[31],RelativeNormAlignment(RNA)
Loss[47],Cross-modalDistillation[24], andCross-modalTranslation[17],asshowninTab.10.
While contrastive loss demonstrates effectiveness in enhancing OOD performance, other tasks
significantlydecreasetheperformance. A2DandNP-Mixshowsubstantialsuperiorityoverother
multimodalself-supervisedtasks.
Influences of N and α in NP-Mix. In this section, we investigate the parameter sensitivity of
NP-MixontheHMDB5125/26dataset. FortheNearestNeighborparameterN,wetestvaluesof1,
2,3,and4. RegardingtheMixupparameterα,weevaluatevaluesof2.0. 4.0,and10.0. Asshown
15Table9: MultimodalNear-OODDetectionusingdifferentcombinationofmodalities.
Modality EPIC-Kitchens4/4 Kinetics-600129/100
Methods
Video Audio Flow FPR95 AUROC IDACC FPR95 AUROC IDACC
↓ ↑ ↑ ↓ ↑ ↑
WithoutA2DTraining
Energy ✓ ✓ 69.59 73.58 71.08 65.69 76.68 81.44
ASH ✓ ✓ 75.75 63.89 62.69 63.32 78.04 80.86
GEN ✓ ✓ 69.22 69.04 71.08 66.18 77.65 81.44
KNN ✓ ✓ 97.20 31.97 71.08 77.38 64.36 81.44
VIM ✓ ✓ 92.35 37.10 71.08 65.77 76.69 81.44
WithA2DTrainingandNP-MixOutlierSynthesis
Energy++ ✓ ✓ 67.72−1.87 72.66−0.92 69.40 64.12−1.57 77.15+0.47 81.23
ASH++ ✓ ✓ 75.93+0.18 67.83+3.94 60.44 62.67−0.65 78.03−0.01 79.80
GEN++ ✓ ✓ 67.35−1.87 72.66+3.62 69.40 62.20−3.98 78.01+0.36 81.23
KNN++ ✓ ✓ 79.66−17.54 65.52+33.55 69.40 73.87−3.51 66.09+1.73 81.23
VIM++ ✓ ✓ 77.80−14.55 57.50+20.40 69.40 63.65−2.12 77.23+0.54 81.23
WithoutA2DTraining
Energy ✓ ✓ 73.88 62.59 67.91 77.76 65.07 56.18
ASH ✓ ✓ 78.36 60.74 62.13 77.27 65.43 54.47
GEN ✓ ✓ 74.25 61.48 67.91 77.46 64.73 56.18
KNN ✓ ✓ 89.55 45.84 67.91 92.02 50.70 56.18
VIM ✓ ✓ 84.33 45.99 67.91 77.78 65.09 56.18
WithA2DTrainingandNP-MixOutlierSynthesis
Energy++ ✓ ✓ 70.90−2.98 68.64+6.05 68.47 76.11−1.65 65.55+0.48 55.90
ASH++ ✓ ✓ 72.57−5.79 66.20+5.46 60.45 75.68−1.59 65.86+0.43 54.55
GEN++ ✓ ✓ 73.13−1.12 66.38+4.90 68.47 74.48−2.98 65.33+0.60 55.90
KNN++ ✓ ✓ 82.09−7.46 58.14+12.30 68.47 89.98−2.04 53.94+3.24 55.90
VIM++ ✓ ✓ 83.40−0.93 49.40+3.41 68.47 76.13−1.65 65.57+0.48 55.90
WithoutA2DTraining
Energy ✓ ✓ ✓ 69.22 72.39 73.13 66.42 76.60 80.33
ASH ✓ ✓ ✓ 70.52 69.70 68.10 63.48 78.11 79.54
GEN ✓ ✓ ✓ 70.34 70.99 73.13 64.24 77.54 80.33
KNN ✓ ✓ ✓ 80.22 60.56 73.13 75.03 65.97 80.33
VIM ✓ ✓ ✓ 76.12 59.03 73.13 66.38 76.59 80.33
WithA2DTrainingandNP-MixOutlierSynthesis
Energy++ ✓ ✓ ✓ 62.69−6.53 74.95+2.56 71.46 63.81−2.61 77.89+1.29 80.82
ASH++ ✓ ✓ ✓ 69.78−0.74 69.49−0.21 67.72 61.22−2.26 78.57+0.46 80.05
GEN++ ✓ ✓ ✓ 63.62−6.72 73.94+2.95 71.46 63.55−0.69 77.92+0.38 80.82
KNN++ ✓ ✓ ✓ 68.47−11.75 67.87+7.31 71.46 71.46−3.57 68.87+2.90 80.82
VIM++ ✓ ✓ ✓ 73.51−2.61 59.57+0.54 71.46 63.42−2.96 77.90+1.31 80.82
Table10: AblationonMultimodalTrainingTasksforNear-OODDetectiononHMDB51dataset.
Baseline ContrastiveLoss RNALoss Cross-modalDistillation Cross-modalTranslation A2D+NP-Mix(ours)
Methods
FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC
↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑
Energy 43.36 87.46 39.65 88.72 55.55 84.33 53.81 85.18 44.44 87.98 36.38 88.91
GEN 43.79 87.49 41.61 89.24 58.61 83.18 55.34 85.10 48.58 87.66 35.95 89.78
KNN 42.92 88.46 37.69 89.27 47.49 84.72 43.79 86.84 39.43 88.75 33.77 90.05
VIM 36.82 88.06 36.38 88.01 43.14 87.16 39.87 87.83 39.43 88.32 34.64 88.80
inFig.10andFig.11,NP-Mixdemonstratesrobustnessacrossdifferentparametersettingsandyields
substantialenhancementsinOODperformanceforallcases.
DifferentArchitectures. Inthissection,wedemonstratethatthestrongperformanceoftrainingwith
A2DandNP-Mixismaintainedacrossdifferentnetworkarchitectures. Wereplacedthearchitectureof
thevideobackbonewithInflated3DConvNet(I3D)[6]andtheopticalflowbackbonewithTemporal
SegmentNetwork(TSN)[60]. AsshowninTab.11,A2DandNP-Mixconsistentlydeliversignificant
improvementinOODdetectionperformancewiththesenewarchitectures. Furthermore,wereplaced
thearchitectureofthevideobackbonearchitecturewithVideoSwinTransformer(Swin-T)[41]and
theaudiobackbonewithAudioSpectrogramTransformer(AST)[22]. AsillustratedinTab.12,A2D
andNP-MixalsoconsistentlyachievesignificantimprovementinOODdetectionperformanceusing
Transformer-basedarchitectures.
G StatisticalSignificanceTests
DifferentRandomSeeds. WeruneachexperimentthreetimesusingdifferentseedsforMultimodal
Near-OODDetectionontheHMDB5125/26datasetinourMultiOODbenchmark,andthencalculate
themeanAUROCandFPR95todemonstratethestatisticalsignificanceofourmethods. Asshown
in Fig. 12, training with A2D and NP-Mix is statistically stable and significantly surpasses the
baselinesunderdifferentrandomseeds.
1646 92
Energy Energy
Energy++ Energy++
44
91
42
90
40
38 89
36
88
34
87
32
30 86
2 4 10 2 4 10
Mixupα Mixupα
Figure10: InfluencesofMixupαforOODperformanceinNP-Mix.
46 92
Energy Energy
Energy++ Energy++
44
91
42
90
40
38 89
36
88
34
87
32
30 86
1 2 3 4 1 2 3 4
NearestNeighborN NearestNeighborN
Figure11: InfluencesofNearestNeighborN forOODperformanceinNP-Mix.
Figure 12: Experiments using three random seeds for Multimodal Near-OOD Detection on the
HMDB5125/26. Foregroundpointsinboldshowresultsaveragedacrossthreedifferentseedswhile
backgroundpoints,shownfeint,indicateresultsfromtheunderlyingindividualseeds.
Different Random Splits. We run each experiment five times using different dataset splits for
MultimodalNear-OODDetectionontheHMDB5125/26datasetinourMultiOODbenchmark,and
thencalculatethemeanAUROCandFPR95todemonstratethestatisticalsignificanceofourmethods.
AsshowninFig.13,trainingwithA2DandNP-Mixisstatisticallystableandsurpassesthebaselines
significantlyunderdifferentdatasetsplits.
17
59RPF
59RPF
↓
↓
CORUA
CORUA
↑
↑Table 11: Multimodal Near-OOD Detection Table 12: Multimodal Near-OOD Detection
usingvideoandflowonHMDB5125/26with using video and audio on EPIC-Kitchens 4/4
I3DandTSNbackbones. withSwin-TandASTbackbones.
HMDB5125/26 EPIC-Kitchens4/4
Methods Methods
FPR95 AUROC IDACC FPR95 AUROC IDACC
↓ ↑ ↑ ↓ ↑ ↑
WithoutA2DTraining WithoutA2DTraining
MSP 51.42 85.00 88.02 MSP 87.87 53.67 61.01
Energy 50.76 85.93 88.02 Energy 83.40 56.44 61.01
MaxLogit 50.98 85.95 88.02 MaxLogit 83.40 56.31 61.01
Mahalanobis 60.13 79.00 88.02 Mahalanobis 92.72 54.12 61.01
ReAct 44.88 86.64 87.80 ReAct 83.21 56.45 60.45
ASH 59.26 85.95 88.02 ASH 94.96 54.23 52.61
GEN 53.38 86.13 88.02 GEN 86.38 54.37 61.01
KNN 43.57 88.66 88.02 KNN 91.04 52.03 61.01
VIM 41.61 86.58 88.02 VIM 85.45 56.68 61.01
LogitNorm 46.62 86.03 88.45 LogitNorm 81.34 59.37 58.40
WithA2DTrainingandNP-Mix WithA2DTrainingandNP-Mix
MSP++ 40.09 −11.33 87.51 +2.51 90.63 MSP++ 77.24 −10.63 67.02 +13.35 62.31
Energy++ 36.60 −14.16 87.48 +1.55 90.63 Energy++ 69.78 −13.62 68.89 +12.45 62.31
MaxLogit++ 36.60 −14.38 87.69 +1.74 90.63 MaxLogit++ 69.96 −13.44 68.99 +12.68 62.31
Mahalanobis++ 54.68 −5.45 81.36 +2.36 90.63 Mahalanobis++ 89.93 −2.79 56.58 +2.46 62.31
ReAct++ 39.22 −5.66 87.11 +0.47 90.85 ReAct++ 70.52 −12.69 68.72 +12.27 62.13
ASH++ 45.32 −13.94 87.02 +1.07 87.80 ASH++ 89.74 −5.22 56.65 +2.42 14.93
GEN++ 36.60 −16.78 88.49 +2.36 90.63 GEN++ 73.13 −13.25 69.46 +15.09 62.31
KNN++ 37.25 −6.32 88.51 −0.15 90.63 KNN++ 77.05 −13.99 63.35 +11.32 62.31
VIM++ 39.00 −2.61 85.70 −0.88 90.63 VIM++ 71.83 −13.62 67.87 +11.19 62.31
LogitNorm++ 39.00 −7.62 87.79 +1.76 87.58 LogitNorm++ 82.84 +1.50 57.54 −1.83 58.58
Figure13: ExperimentsusingfiverandomdatasetsplitsforMultimodalNear-OODDetectiononthe
HMDB5125/26. Foregroundpointsinboldshowresultsaveragedacrossfivedifferentsplitswhile
backgroundpoints,shownfeint,indicateresultsfromtheunderlyingindividualsplits.
References
[1] Abati,D.,Porrello,A.,Calderara,S.,Cucchiara,R.: Latentspaceautoregressionfornovelty
detection.In: CVPR(2019)
[2] Arjovsky,M.,Chintala,S.,Bottou,L.: Wassersteingenerativeadversarialnetworks.In: ICML
(2017)
[3] Blum,H.,Sarlin,P.E.,Nieto,J.,Siegwart,R.,Cadena,C.: Thefishyscapesbenchmark: Measur-
ingblindspotsinsemanticsegmentation.InternationalJournalofComputerVision129(11),
3119–3135(2021)
[4] Breunig,M.M.,Kriegel,H.P.,Ng,R.T.,Sander,J.: Lof: identifyingdensity-basedlocaloutliers.
In: Proceedingsofthe2000ACMSIGMODinternationalconferenceonManagementofdata.
pp.93–104(2000)
[5] Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., Zisserman, A.: A short note about
kinetics-600.arXivpreprintarXiv:1808.01340(2018)
18[6] Carreira,J.,Zisserman,A.: Quovadis,actionrecognition? anewmodelandthekineticsdataset.
In: CVPR(2017)
[7] Chan,R.,Lis,K.,Uhlemeyer,S.,Blum,H.,Honari,S.,Siegwart,R.,Fua,P.,Salzmann,M.,
Rottmann,M.: Segmentmeifyoucan: Abenchmarkforanomalysegmentation.arXivpreprint
arXiv:2104.14812(2021)
[8] Chen, G., Peng, P., Wang, X., Tian, Y.: Adversarial reciprocal points learning for open set
recognition.arXivpreprintarXiv:2103.00953(2021)
[9] Chen,H.,Xie,W.,Vedaldi,A.,Zisserman,A.: Vggsound: Alarge-scaleaudio-visualdataset.
In: ICASSP(2020)
[10] Contributors,M.: Openmmlab’snextgenerationvideounderstandingtoolboxandbenchmark.
https://github.com/open-mmlab/mmaction2(2020)
[11] Damen,D.,Doughty,H.,Farinella,G.M.,Fidler,S.,Furnari,A.,Kazakos,E.,Moltisanti,D.,
Munro,J.,Perrett,T.,Price,W.,Wray,M.: Scalingegocentricvision: Theepic-kitchensdataset.
In: ECCV(2018)
[12] Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.: Imagenet: Alarge-scalehierarchical
imagedatabase.In: CVPR(2009)
[13] Deng,L.: Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch[best
oftheweb].IEEEsignalprocessingmagazine29(6),141–142(2012)
[14] Djurisic,A.,Bozanic,N.,Ashok,A.,Liu,R.: Extremelysimpleactivationshapingforout-of-
distributiondetection.arXivpreprintarXiv:2209.09858(2022)
[15] Dong,H.,Chen,X.,Särkkä,S.,Stachniss,C.: Onlinepolesegmentationonrangeimagesfor
long-termlidarlocalizationinurbanenvironments.RoboticsandAutonomousSystems159,
104283(2023)
[16] Dong,H.,Frusque,G.,Zhao,Y.,Chatzi,E.,Fink,O.: Nng-mix: Improvingsemi-supervised
anomalydetectionwithpseudo-anomalygeneration.arXivpreprintarXiv:2311.11961(2023)
[17] Dong, H., Nejjar, I., Sun, H., Chatzi, E., Fink, O.: SimMMDG: A simple and effective
frameworkformulti-modaldomaingeneralization.In: NeurIPS(2023)
[18] Dong,H.,Zhang,X.,Xu,J.,Ai,R.,Gu,W.,Lu,H.,Kannala,J.,Chen,X.: Superfusion: Multi-
levellidar-camerafusionforlong-rangehdmapgeneration.arXivpreprintarXiv:2211.15656
(2022)
[19] Du, X., Wang, Z., Cai, M., Li, Y.: Vos: Learning what you don’t know by virtual outlier
synthesis.In: ICLR(2022)
[20] Feichtenhofer,C.,Fan,H.,Malik,J.,He,K.: Slowfastnetworksforvideorecognition.In:ICCV
(2019)
[21] Fink, O., Wang, Q., Svensén, M., Dersin, P., Lee, W.J., Ducoffe, M.: Potential,
challenges and future directions for deep learning in prognostics and health manage-
ment applications. Engineering Applications of Artificial Intelligence 92, 103678 (2020).
https://doi.org/https://doi.org/10.1016/j.engappai.2020.103678
[22] Gong, Y., Chung, Y.A., Glass, J.: Ast: Audio spectrogram transformer. arXiv preprint
arXiv:2104.01778(2021)
[23] Gorishniy,Y.,Rubachev,I.,Khrulkov,V.,Babenko,A.: Revisitingdeeplearningmodelsfor
tabulardata.NeurIPS(2021)
[24] Gupta,S.,Hoffman,J.,Malik,J.: Crossmodaldistillationforsupervisiontransfer.In: CVPR
(2016)
[25] Han,S.,Hu,X.,Huang,H.,Jiang,M.,Zhao,Y.: Adbench: Anomalydetectionbenchmark.In:
NeuralInformationProcessingSystems(NeurIPS)(2022)
[26] He,K.,Zhang,X.,Ren,S.,Sun,J.: Deepresiduallearningforimagerecognition.In: CVPR
(2016)
[27] Hendrycks,D.,Basart,S.,Mazeika,M.,Zou,A.,Kwon,J.,Mostajabi,M.,Steinhardt,J.,Song,
D.: Scalingout-of-distributiondetectionforreal-worldsettings.ICML(2022)
[28] Hendrycks, D., Gimpel, K.: A baseline for detecting misclassified and out-of-distribution
examplesinneuralnetworks.In: ICLR(2017)
19[29] Hendrycks,D.,Mazeika,M.,Dietterich,T.: Deepanomalydetectionwithoutlierexposure.In:
ICLR(2019)
[30] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F.,
Green,T.,Back,T.,Natsev,P.,etal.: Thekineticshumanactionvideodataset.arXivpreprint
arXiv:1705.06950(2017)
[31] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C.,
Krishnan,D.: Supervisedcontrastivelearning.In: NeurIPS(2020)
[32] Kingma,D.P.,Ba,J.: Adam: Amethodforstochasticoptimization.ICLR(2015)
[33] Kong,S.,Ramanan,D.: Opengan: Open-setrecognitionviaopendatageneration.In: ICCV
(2021)
[34] Krizhevsky,A.,Nair,V.,Hinton,G.: Cifar-10andcifar-100datasets.URl: https://www.cs.
toronto.edu/kriz/cifar.html6(1), 1(2009)
[35] Kuehne,H.,Jhuang,H.,Garrote,E.,Poggio,T.,Serre,T.: Hmdb: alargevideodatabasefor
humanmotionrecognition.In: ICCV(2011)
[36] LeCun,Y.,Chopra,S.,Hadsell,R.,Ranzato,M.,Huang,F.: Atutorialonenergy-basedlearning.
Predictingstructureddata1(0)(2006)
[37] Lee,K.,Lee,K.,Lee,H.,Shin,J.: Asimpleunifiedframeworkfordetectingout-of-distribution
samplesandadversarialattacks.In: NeurIPS(2018)
[38] Liang,S.,Li,Y.,Srikant,R.: Enhancingthereliabilityofout-of-distributionimagedetectionin
neuralnetworks.In: ICLR(2018)
[39] Liu,W.,Wang,X.,Owens,J.D.,Li,Y.: Energy-basedout-of-distributiondetection.In: NeurIPS
(2020)
[40] Liu,X.,Lochman,Y.,Zach,C.: Gen: Pushingthelimitsofsoftmax-basedout-of-distribution
detection.In: CVPR(2023)
[41] Liu,Z.,Ning,J.,Cao,Y.,Wei,Y.,Zhang,Z.,Lin,S.,Hu,H.:Videoswintransformer.In:CVPR
(2022)
[42] Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning
research9(11)(2008)
[43] Ming,Y.,Cai,Z.,Gu,J.,Sun,Y.,Li,W.,Li,Y.: Delvingintoout-of-distributiondetectionwith
vision-languagerepresentations.In: NeurIPS(2022)
[44] Munro,J.,Damen,D.: Multi-modaldomainadaptationforfine-grainedactionrecognition.In:
CVPR(2020)
[45] Pardo,L.: Statisticalinferencebasedondivergencemeasures.ChapmanandHall/CRC(2018)
[46] Pidhorskyi, S., Almohsen, R., Adjeroh, D.A., Doretto, G.: Generativeprobabilisticnovelty
detectionwithadversarialautoencoders.In: NeurIPS(2018)
[47] Planamente,M.,Plizzari,C.,Alberti,E.,Caputo,B.: Domaingeneralizationthroughaudio-
visualrelativenormalignmentinfirstpersonactionrecognition.In: WACV(2022)
[48] Prokhorenkova,L.,Gusev,G.,Vorobev,A.,Dorogush,A.V.,Gulin,A.: Catboost: unbiased
boostingwithcategoricalfeatures.NeurIPS(2018)
[49] Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,Askell,A.,
Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferablevisualmodels from
naturallanguagesupervision.In: ICML(2021)
[50] Ruff,L.,Vandermeulen,R.,Goernitz,N.,Deecke,L.,Siddiqui,S.A.,Binder,A.,Müller,E.,
Kloft,M.: Deepone-classclassification.In: Internationalconferenceonmachinelearning.pp.
4393–4402.PMLR(2018)
[51] Ruff,L.,Vandermeulen,R.A.,Görnitz,N.,Binder,A.,Müller,E.,Müller,K.,Kloft,M.: Deep
semi-supervisedanomalydetection.In: ICLR(2020)
[52] Simonyan,K.,Zisserman,A.: Two-streamconvolutionalnetworksforactionrecognitionin
videos.In: NeurIPS(2014)
[53] Soomro, K., Zamir, A.R., Shah, M.: Ucf101: Adatasetof101humanactionsclassesfrom
videosinthewild.arXivpreprintarXiv:1212.0402(2012)
20[54] Sun, Y., Guo, C., Li, Y.: React: Out-of-distribution detection with rectified activations. In:
NeurIPS(2021)
[55] Sun,Y.,Ming,Y.,Zhu,X.,Li,Y.: Out-of-distributiondetectionwithdeepnearestneighbors.
ICML(2022)
[56] Tao, L., Du, X., Zhu, X., Li, Y.: Non-parametric outlier synthesis. arXiv preprint
arXiv:2303.02966(2023)
[57] Vaze,S.,Han,K.,Vedaldi,A.,Zisserman,A.:Open-setrecognition:Agoodclosed-setclassifier
isallyouneed.In: ICLR(2022)
[58] Wang,H.,Li,Z.,Feng,L.,Zhang,W.: Vim: Out-of-distributionwithvirtual-logitmatching.In:
CVPR(2022)
[59] Wang,H.,Li,Y.,Yao,H.,Li,X.: Clipnforzero-shotooddetection: Teachingcliptosayno.In:
ICCV(2023)
[60] Wang,L.,Xiong,Y.,Wang,Z.,Qiao,Y.,Lin,D.,Tang,X.,VanGool,L.: Temporalsegment
networks: Towardsgoodpracticesfordeepactionrecognition.In: ECCV(2016)
[61] Wei,H.,Xie,R.,Cheng,H.,Feng,L.,An,B.,Li,Y.: Mitigatingneuralnetworkoverconfidence
withlogitnormalization.In: ICML(2022)
[62] Yang,J.,Wang,P.,Zou,D.,Zhou,Z.,Ding,K.,Peng,W.,Wang,H.,Chen,G.,Li,B.,Sun,Y.,
etal.: Openood: Benchmarkinggeneralizedout-of-distributiondetection.In: NeurIPS(2022)
[63] Yang,J.,Zhou,K.,Li,Y.,Liu,Z.: Generalizedout-of-distributiondetection: Asurvey.arXiv
preprintarXiv:2110.11334(2021)
[64] Zach,C.,Pock,T.,Bischof,H.: Adualitybasedapproachforrealtimetv-l1opticalflow.In:
PatternRecognition(2007)
[65] Zhang,H.,Cisse,M.,Dauphin,Y.N.,Lopez-Paz,D.: mixup: Beyondempiricalriskminimiza-
tion.In: ICLR(2018)
[66] Zhou,Y.: Rethinkingreconstructionautoencoder-basedout-of-distributiondetection.In: CVPR
(2022)
[67] Zhou,Y.,Song,X.,Zhang,Y.,Liu,F.,Zhu,C.,Liu,L.: Featureencodingwithautoencodersfor
weaklysupervisedanomalydetection.IEEETransactionsonNeuralNetworksandLearning
Systems33(6),2454–2465(2021)
21