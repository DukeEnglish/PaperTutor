Reason3D: Searching and Reasoning 3D Segmentation
via Large Language Model
Kuan-ChihHuang1 XiangtaiLi2 LuQi1* ShuichengYan2 Ming-HsuanYang1
1UniversityofCalifornia,Merced 2SkyworkAI
Abstract
Recentadvancementsinmultimodallargelanguagemodels(LLMs)haveshown
theirpotentialinvariousdomains,especiallyconceptreasoning. Despitethesede-
velopments,applicationsinunderstanding3Denvironmentsremainlimited. They
primarilyoffertextualornumericaloutputswithoutthecapabilitytogeneratedense,
informativesegmentationmasks. ThispaperintroducesReason3D,anovelLLM
designedforcomprehensive3Dunderstanding. Reason3Dtakespointclouddata
andtextpromptsasinputtoproducetextualresponsesandsegmentationmasks,
facilitatingadvancedtaskslike3Dreasoningsegmentation,hierarchicalsearching,
expressreferring,andquestionansweringwithdetailedmaskoutputs. Specifically,
weproposeahierarchicalmaskdecodertolocatesmallobjectswithinexpansive
scenes. Thisdecoderinitiallygeneratesacoarselocationestimatecoveringthe
object’sgeneralarea. Thisfoundationalestimationfacilitatesadetailed,coarse-
to-finesegmentationstrategythatsignificantlyenhancestheprecisionofobject
identificationandsegmentation. ExperimentsvalidatethatReason3Dachievesre-
markableresultsonlarge-scaleScanNetandMatterport3Ddatasetsfor3Dexpress
referring,3Dquestionanswering,and3Dreasoningsegmentationtasks. Codeand
modelsareavailableat: https://github.com/KuanchihHuang/Reason3D.
1 Introduction
Recently,largelanguagemodels(LLMs)[41,42,20]havesignificantlyenhancedtheircapabilities
in sophisticated reasoning within the realm of natural language processing. Building on these
developments,anewclassofmodels,knownasMultimodalLargeLanguageModels(MLLMs)[14,
26,3,28,54,36,50,52]haveemerged. Thesemodelsaredesignedtoprocessmultiplemodalities,
including 2D images [28, 3], thereby enriching LLMs’ ability to interpret and understand visual
inputs. However,whileMLLMsdemonstrateremarkableproficiencywith2Dimagery,extending
thesecapabilitiestocomplex3Denvironmentsremainsaformidablechallenge,whichiscriticalin
varioustechnologicaldomains,includingrobotnavigationandembodiedAIagents.
Numerousstudieshavemadesignificantstridesinusingpointcloudsasinputtokensfor3Dlarge
languagemodels(LLMs). Someresearches[49,33,32,40]primarilyfocusedon3Dobject-level
understanding. Prior study 3D-LLM [17] aggregates multi-view features to enrich 3D feature
comprehensionandemploysasubsequentLLMfor3Dreasoning. Meanwhile,LL3DA[8]directly
encodes 3D point clouds for scene representation and facilitates human interaction to enhance
understanding.
These methods integrate large language models (LLMs) with point cloud inputs to enhance 3D
reasoningcapabilities;however,theyalsoencounterspecificlimitations. Firstly,theoutputsfrom
*Correspondingauthor
Preprint.Underreview.
4202
yaM
72
]VC.sc[
1v72471.5042:viXra3D Reasoning
In a lounge area, where can someone sit
comfortably to relax and unwind while
enjoying a drink or conversation with
others in the scene? Please output the
segmentation mask.
Sure. It’s [SEG].
3D Hierarchical Searching
After a long day of work, if someone wants
to relax and take a soothing shower, which
bathroom fixture could use? Please first
output the location of the bathroom then
output the object mask.
The bathroom is located at [LOC].
The mask is [SEG].
3D Express Referring
A long cabinet sitting under the window
that has an air conditioner in it there is a
box of tissues on the cabinet. Please
output the segmentation mask.
Sure, the mask is [SEG].
3D QA
How many pairs of shoes is the table
behind? Please output the related mask.
2 pairs. [SEG].
Figure1: WeproposeReason3D,anovelLLM-baseddense3Dpointcloudsearchingandreasoning
frameworkthatcanoutputdensesegmentationmasksbasedontexturalinformation. OurReason3D
canhandletasksinvolving1)3DReasoning,2)3DHierarchicalSearching,3)3Dexpressreferring,
and4)3DQAwithrespondingdensesegmentationmasks.
thesemodelsareconfinedtotextualornumericalforms,whichareinadequateforpredictingdense
datatypes,suchassegmentationmasks,essentialfordetailedspatialanalysis. Secondly,thesemodels
struggletolocateorsearchforobjectsin3Dscenesbasedoncomplexorabstractconceptsrather
thanrelyingonspatialrelationshipswithinthescenes.
Toenable3D-basedLLMmodelstoproducesegmentationmasks,wecandirecttheLLMtogenerate
a [SEG] token. The embedding of this token is then utilized to guide the decoder in learning to
create3Dsegmentationmasks,similartoapproachesusedin2Dreasoningmodels[24]. However,
unlikestructuredimagedata,thisstraightforwardadaptationmayencounterdifficultiesduetopoint
clouds’inherentsparsityandunstructurednature. Thesechallengesareparticularlypronouncedwhen
attemptingtooutputmasksforsmallobjectswithindense,large-scale3Dscenes.
In this paper, we introduce Reason3D, a framework designed to enable reasoning and searching
within3Dscenesusinglargelanguagemodelswithpointcloudinputs. Unlikeothermethodsthat
generateonlytextualandnumericaloutputs,Reason3Dalsoproduces3Dsegmentationmasksfrom
textualinputs. Toefficientlymanagelarge-scalepointclouds,weutilizeasuperpointpoolinglayer
thataggregatesdataintoprecomputedsuperpoints,significantlyreducingcomplexity. Furthermore,
wedevelopahierarchicalmaskdecodertoaddressthechallengesofsparsityinextensivepointclouds,
such as locating a ball in a large house. This decoder employs a coarse-to-fine strategy, initially
2identifyingalikelyobject-containingcoarseregionbyinstructingtheLLMmodeltooutputa[LOC]
embedding, which guides the learning of the region mask. This region then serves as a prior to
generateapreciseobjectmask,facilitatingeffectivelocalizationincomplex3Denvironments.
Figure1illustratesReason3D’scapabilitytohandlediversescenariosrequiringadvancedreasoning,
searching,andquestionansweringbasedonconceptualandspatialinformation. Tovalidatetheeffec-
tivenessofourapproach,wehavecollectedadatasetfor3Dreasoningsegmentation,comprisingover
onethousandpoint-instructionpairs. Toensurethisdatasetcloselymirrorsreal-worldapplications,
weannotatedpointcloudsfromMatterport3D[6]andScanNetv2[13]withimplicittextqueriesthat
demandcomplexreasoning. Themaincontributionsofthisworkare:
•WeintroduceReason3D,acomprehensiveframeworkforreasoningandsearchingwithin3Dscenes
usingextensivelanguageprompts.Thisframeworknotonlyprocesses3Dpointcloudsandlanguage
promptstogeneratetextualoutputsbutalsocreatesdetailed3Dsegmentationmasks. Reason3D
supports a wide range of tasks, including 3D expressive referring segmentation, 3D reasoning
segmentation,3Dhierarchicalsearching,and3Dquestionanswering.
•We establish the novel task of 3D reasoning segmentation, which involves interpreting implicit
humaninstructionswithin3Dscenes,andbuildadatasettoevaluatethistask. Thecapabilityof3D
reasoningisessentialfordevelopingtrulyadvanced3Dperceptionsystems.
•Wedevelopahierarchicalmaskdecodertoaddressthesparsityof3Dpointcloudseffectively. This
innovativeapproachfirstidentifiesacoarseregionlikelycontainingtheobjectandusesthisregion’s
probabilityasapriortoguidetherefinementofthefinalmaskprediction.
•Wedevelopahierarchicalmaskdecodertoaddressthesparsityof3Dpointcloudseffectively. This
innovativeapproachfirstidentifiesacoarseregionlikelycontainingtheobject. Then,thedecoder
usesthisregion’sprobabilityasapriortoguidetherefinementofthefinalmaskprediction.
2 RelatedWork
3DPointCloudSegmentation.Recentadvancementsinpointcloudsegmentation[31,23,38,48,37,
44]havedriventheevolutionofvariousclass-awarepredictiontechniques,predominantlyemploying
UNet-like models that process data as either 3D points or voxels. Point-based methods [9, 55]
refinefeatureextractionthroughaggregationmechanismsortransformerblocks,whilevoxel-based
methods[18,11]transformirregularpointcloudsintoregularvoxelgridsforprocessingwithdense
orsparse3Dconvolutionalnetworks, enhancingsegmentationaccuracyandefficiency. With3D
segmentationtaskshavingachievedasignificantlevelofmaturity,ithasbecomeessentialtodevelop
moreadvancedmethodsforinteractingwith3Dsegmentationsystems.
The3Dexpressreferringsegmentationtask[7,1]enhancesinteractionthroughhumanlanguageby
segmentinga3Dobjectaccordingtoaspecifictextualdescription. TGNN[19]employsatwo-stage
approachthatintegratesinstanceandtextualfeatures,computingamatchingscoreforeachinstanceto
identifythetargetdescribedinthetext. Buildingonthis,X-RefSeg3D[34]leveragesvision-language
cues by combining entity-related linguistic information with visual features to construct a cross-
modalscenegraph,whichfacilitatesinteractionsbasedontextualandspatialrelations. Similarly,
3D-STMN[45]introducesanefficientend-to-endframeworkthatutilizessuperpointsandaligns
themwithtextualmodalities,therebyenhancingtheirroleinmultimodalrepresentation. However,
whilethesestudiesmakesignificantstridesinobjectidentificationusingspatialrelationcues,they
donotfullyexploredeeperreasoningcapabilities. Inthiswork,weintroduceReason3D,anovel
approachthatextendsbeyondtraditionalidentificationtoincorporateadvancedreasoningwith3D
segmentationmodels,addressingcomplexinteractionsnotyettackledbyexistingmethodologies.
LargeLanguageModel.Recentadvancementsinlargelanguagemodels(LLMs)[41,57,42,20,21]
haveshowcasedtheirbroadgeneralizationacrossdiverselanguagetasks,thankstotrainingonex-
tensive textual datasets. Through self-supervised learning techniques such as token prediction
and masked token reconstruction, as well as further refinements via instruction tuning and spe-
cializeddatasets,researchershavesignificantlyenhancedtheadaptabilityofthesemodelstonew
tasks. Building on this, the remarkable reasoning capabilities of LLMs are increasingly applied
in multimodal contexts. Modern models incorporate advanced architectures that integrate visual
data [26, 2, 3, 28, 58, 24, 14, 47, 50, 53, 46], utilizing mechanisms such as cross-attention and
image-textfeaturealignmenttoenablecomprehensivemultimodalunderstanding. Thishaspaved
thewayformodelsthatengageinvisualquestionansweringandperformcomplexreasoningtasks.
3In a game room, what object in the scene could be During a cozy winter evening, what object in the
used for playing a competitive and strategic game scene can provide warmth and create a comforting
involving balls and cues? ambiance in the room?
Figure2: Examplesoftheannotatedsamples. LeftisonedatafromMatterport3Ddatasetwiththe
answerpooltable. RightisonedatafromtheScannetV2datasetwiththeanswerfireplace.
Notably, LISA [24] introduces a specialized segmentation token into its vocabulary, decoded to
generateasegmentationmask,enablingmorepreciseanddetailedreasoningcapabilities.
Recenteffortshaveextendedlargelanguagemodels(LLMs)toinclude3Ddata,therebyimproving
pointcloudunderstanding. Point-LLM[49]interpretsobject-levelpointsusingLLMs,while3D-
LLM[17]enhancesunderstandingbyintegratingmulti-viewimagefeatureswithLLMs. LL3DA[8]
combinestextualinstructionswithvisualinteractionstoimprovefeatureextractionformoreeffective
instruction-following. Unlikeexistingmethods,whicharelimitedtoboundingbox-levelgrounding,
textualresponses,orlackcontextualreasoning,ourapproachenablesfine-grainedsegmentationof
preciselysearchedobjectswithin3Ddata.
3 3DReasoningSegmentation
ProblemDefinition. 3Dreasoningsegmentationtaskinvolvesgeneratinga3DsegmentationmapM
fromagiven3DscenepointcloudPalongsideacomplextextualinstructionX . Thisinstruction
txt
oftendemandssophisticatedlinguisticcomprehension,extendingbeyondmereidentificationtasks,
like3Dreferringsegmentationtask[19]. Forinstance,ratherthanprocessingsimpledirectiveslike
"theredchair,"thetextualqueriesmightinvolveintricatedescriptionsorscenarios,suchas"anobject
usuallysituatedinalivingroomthatcanaccommodatemultiplepeoplesittingtogethercomfortably."
whichrequiresin-depthworldknowledgeandreasoningunderstanding.
DatasetCollection. Giventheabsenceofastandardizeddatasetforevaluating3Dreasoningsegmen-
tation,wehavecollectedthe3Dscansfromindoordatasets,Matterport3D[6]andScanNetv2[13]
andannotatedthemwithcomplextextinstructionsanddetailed3Dsegmentationmasks. Thedataset
consistsof1339samplesfortrainingand1145samplesforvalidation. Twosampledataareshownin
Figure2. MoredetailscanbefoundintheAppendix.
4 Reason3D
WeintroduceReason3D,anovelLLM-basedframeworkforsearchingandreasoningwithin3Dpoint
clouds,asillustratedinFigure3. Givena3Dpointcloudandatextualquerydescribinganobject
ofinterest, ourmethodleveragesanLLMmodeltoalignpointfeaturesandpredictdenseobject
segmentationmasks. Section4.1discussesthealignmentofpointcloudswithLLMsinthefeature
space. Section4.2introducestheproposedhierarchicalmaskdecoderthatemploysacoarse-to-fine
approachforgeneratingdensesegmentationmasks. Section4.3thendetailsthetraininglossofour
Reason3Dframework.
4.1 AlignmentbetweenLLMsandPointCloud
GivenapointcloudP∈RN×6consistingofN points,eachcharacterizedbythreecolorschannels
(r,g,b)andthreecoordinates(x,y,z),weaimtoextractpointfeaturesandalignthemwithexisting
decoder-onlyLLMmodelstofacilitatesceneunderstandingforansweringcomplexquestions.
SceneEncoder. WeemployavoxelizationoperationonthepointcloudandutilizeaU-Netstyle
backbone[16]toextractpoint-wisefeaturesF ∈RN×C,whereC denotesthechanneldimension.
p
Tofurtherreducecomplexity, wefeedthesefeaturesintoasuperpointpoolinglayerthatutilizes
pre-computed superpoints [25]. This layer aggregates superpoint features F ∈ RM×C through
s
4Region Segmentation
Trainable
Frozen
Hierarchical Mask Decoder
Point Region Mask
Encoder Pooling Decoder Decoder
Fp Fs
hloc hseg
3D Scene 3D Mask
...
... Q-Former LLM ...
Q Q′
It’s located at [LOC], and the mask is [SEG].
In bedroom, many people like to relax by watching
their favorite shows or movies. What object would
most likely be used for this purpose? Please find the
room first and then output the mask. Xtxt
Figure3: OverviewofourReason3Dframework. Initially,weutilizeapointencodertoextract
densefeaturesF fromtheinputscene,simplifiedbyasuperpointpoolinglayertoreducecomplexity.
p
AninteractormergessuperpointfeaturesF withalearnablequery,inputintoafrozenLLMalong
s
withinstructionstogenerateanoutputcontainingcriticaltokens,[LOC]and[SEG].Ahierarchical
decoderthenusesthe[LOC]embeddingtoestimateacoarselocationthatlikelycoverstheobject.
Finally,thisestimatedlocationintegrateswiththe[SEG]embedding,enablingthepredictionofthe
finalsegmentationmasks.
averagepoolingofthepoint-wisefeatureswithineachsuperpoint,effectivelyreducingthenumberof
pointsfromN toM,whereM representsthenumberofsuperpoints.
This reduction is crucial for managing large-scale scenes without the need to segment the point
cloudintosmallersegments. Forinstance,asingleMatterport3Dscene[6]containsapproximately
onemillionpoints—asignificantchallengeforexistingalgorithms[31]thattypicallyrequiredata
segmentation. Ourapproachsignificantlyreducescomplexityandenhancesprocessingefficiencyby
handlingextensivedatainasinglepass.
AlignmentwithLLM.ToalignthesuperpointfeaturesF withexistingdecoder-onlyLLMmodels,
s
weemployaninteractorF followingQ-Former[26]tofacilitatedynamicinteractionbetweenthe
pointcloudfeaturesF andthelearnablequeryQ,resultinginanupdatedqueryQ′ =F(Q,F ).
s s
Subsequently,theupdatedqueryQ′andtextualinstructionsX arefedintoafrozendecoder-only
txt
languagemodel(LLM)togeneratetargetedresponses:
Y =LLM(Q′,X ). (1)
txt txt
WefreezethepointcloudencodersandtheLLM,allowingupdatesonlytotheinteractormodule.
Thissetupfocusesonlearninginteractionsbetween3Dandlinguisticdata,enhancingthemodel’s
abilitytoproduceaccurate,contextuallyrelevantresponsestotextualcommandsabout3Ddata.
4.2 HierarchicalMaskDecoder
Existing3Dscene-levelLLM-basedmethods[17,8]primarilyproducetextualornumericaloutputs
andcannotprovidepredictionsfordense3Dmasks. Toovercometheselimitations,weintroduce
aHierarchicalMaskDecoderthatseamlesslyintegrateswiththeoutputsofLLMs. Thisadvanced
integrationallowsoursystemtointerprettextualinstructionsandgeneratecorrespondingdetailed
segmentationmasksdirectly.
TheHierarchicalMaskDecoderpredictsthemasksM,M,usingthesuperpointfeaturesF ofthe
s
inputsceneandprompts⟨P ,P ⟩generatedfromtheLLMoutputbasedontheinstructionX :
loc seg txt
M=Decoder(Q ;⟨P ,P ⟩|⟨F ,X ⟩), (2)
h loc seg s txt
where Q is a learnable object query and P and P denote the location and segmentation
h loc seg
prompts,respectively. InspiredbyLISA[24],whichdirectsanLLMtooutputspecificembedding
tokens,weadoptasimilarstrategy. OurLLMispromptedtoproducea[SEG]token. Thelast-layer
5embeddingh associatedwiththe[SEG]tokenistransformedthroughanMLPprojectionlayerG,
seg
resultinginthesegmentationpromptP =G(h ),whichisderivedfromthecomplextextual
seg seg
instructionstoguidethefinalmaskprediction.
Moreover,foreffective3Dmaskprediction,especiallywhentargetingsmallobjectswithinlarge
scenes. Weproposeacoarse-to-fineapproach. Wefurtherintroduceaspecialtoken,[LOC],tolearn
thecoarselocationthatmayencompasstheobjectmask. Similartothe[SEG]tokenprocess,we
refinetheembeddingh ofthe[LOC]tokenusinganMLPlayerG,resultinginP =G(h ). To
loc loc loc
thisend,wecanusetwopromptstoguidethefinalmaskprediction. Inpractice,weinitiallyexploita
maskdecoder,F ,constructedwithatransformerdecoderarchitecture[37,38]. Tguidingecoder
loc
usesthelocationpromptasthequerytogeneratealocationmaskM :
loc
M =F (P ,F ). (3)
loc loc loc s
Subsequently,thelocationmaskM servesasapriortoguidethefinalmaskgenerationthrough
loc
anotherMLPlayerH :
loc
M=F (P ,F +H (M )), (4)
seg seg s loc loc
whereMisthefinalmask,andF sharesthesamearchitecturalframeworkasF .
seg loc
4.3 Training3DLLM
ThelossfunctionforReason3Dcomprisestwoessentialcomponents: theLLMlossL andthe
llm
segmentationmasklossL . Theoverallcombinationisrepresentedas:
mask
L=L +L . (5)
llm mask
Inparticular,TheLLMloss,L ,embodiesthelinguisticaspectsthroughanauto-regressivecross-
llm
entropylossfortextgeneration,incorporatingcross-entropylossCEforeachtoken:
L =CE(Y ,Yˆ ) (6)
llm txt txt
where Yˆ represents the ground truth word token. In addition, the mask loss L aims at
txt mask
encouragingthemodeltogeneratehigh-qualitysegmentationmasks. Thislossiscomputedusinga
binarycross-entropy(BCE)lossandDICElossforallsuperpoints,whichisrepresentedas:
L =BCE(M ,Mˆ )+DICE(M ,Mˆ ),∗∈[loc,seg]. (7)
mask∗ ∗ ∗ ∗ ∗
whereMˆ meansthegroundtruthsegmentationmaskforregion-levelandobject-levelsuperpoints.
∗
Fortheobject-levelmaskMˆ ,weutilizethemaskoftheobjectthatwetarget. Fortheregion-level
seg
maskMˆ ,wemarkthepointsastheforegroundpointsifthedistancebetweenanypointandthe
loc
objectcenterissmallerthanthresholdτ.
5 Experiments
5.1 ExperimentalSetting
Datasets. Ourtrainingdataincludesthreemaintypesofdatasets: (1)Forthe3Dexpressivereferring
segmentationtask,weuseScanRefer[7]andSr3Ddatasets[1]. (2)Forthe3Dquestionanswering
task,weutilizeScanQAdataset[4].(3)Forthe3Dreasoningsegmentationtask,weusetheReason3D
datasets,whichweannotatedfromboththeScanNetV2andMatterport3Ddatasets. Theresultsof3D
QAandmoredetailsareincludedintheAppendix.
ModelArchitecture. Weuseapre-trainedSparse3DU-Net[38]toextractpoint-wisefeatures. For
thelanguagelearningmodel,weemployFlanT5[12],maintainingmostofitspre-trainedweights
frozen,exceptforadaptingtheweightsforthenewly-addedlocationandsegmentationtokens. Our
QFormerisconstructedbytheBLIP-2[26]architecture,incorporating1408-dimensionalfeatures.
EvaluationMetrics. Forthe3Dexpressivereferringsegmentationand3Dreasoningsegmentation
tasks,theprimaryevaluationmetricsareMeanIntersectionoverUnion(mIoU),whichquantifies
the average overlap between the predicted and true 3D volumes, and Accuracy at k Intersection
overUnion(Acc@kIoU).Thislattermetricmeasurestheproportionofdescriptionsforwhichthe
predictedmaskoverlapsthegroundtruthwithanIoUgreaterthank,wherekissetatthresholds
of0.25and0.5,thusassessingthemodel’sperformanceatvaryinglevelsofprecision. FortheQA
task,theevaluationmetricsincludeBLEU-4[30],ROUGE-L[27],METEOR[5],andCIDEr[43]to
ensurerobustanswermatching. Thesemetricsevaluatetheprecision,fluency,andsemanticaccuracy
oftheresponses.
6ScanNet Matterport3D
Method Venue
0.25 0.50 mIoU 0.25 0.50 mIoU
OpenScene[31] CVPR’23 4.22 0.97 5.03 4.07 0.57 6.36
OpenScene[31]+FlanT5[12] CVPR’23+ArXiv’22 24.68 7.14 15.03 19.98 4.02 13.60
OpenMask3D[39] NeurIPS’23 5.70 3.25 7.14 3.25 0.12 5.96
OpenMask3D[39]+FlanT5[12] NeurIPS’23+ArXiv’22 20.78 6.82 13.38 17.46 0.23 9.07
3D-STMN[45] AAAI’24 25.43 17.78 18.23 20.68 10.81 13.47
Llama2[20]+CLIP[35] ArXiv’23+ArXiv’22 39.26 25.93 27.23 28.51 14.86 17.80
Reason3D(Ours) - 43.21 32.10 31.20 31.22 17.43 19.54
Table 1: 3D Reasoning Segmentation Results on Reason3D dataset. The evaluation metric is
accuracyatIoU0.25,IoU0.5andmIoU.
Unique(∼19%) Multiple(∼81%) Overall
Method Venue
0.25 0.50 mIoU 0.25 0.50 mIoU 0.25 0.50 mIoU
ScanRefer[7]* ECCV’20 67.6 44.4 39.9 31.2 20.9 19.5 38.2 25.5 23.5
3DVG-Transformer[56]* ICCV’21 79.5 58.0 49.9 42.0 30.8 27.0 49.3 36.1 31.4
3D-SPS[29]* CVPR’22 84.8 65.6 54.7 41.7 30.8 26.7 50.1 37.6 32.1
3D-LLM[17]* NeurIPS’23 57.8 30.6 32.5 24.7 12.8 14.0 31.1 16.3 17.6
TGNN[19] AAAI’21 69.3 57.8 50.7 31.2 26.6 23.6 38.6 32.7 28.8
X-RefSeg3D[34] AAAI’24 - - - - - - 40.3 33.8 29.9
3D-STMN[45] AAAI’24 89.3 84.0 74.5 46.2 29.2 31.1 54.6 39.8 39.5
Reason3D(Ours) - 88.4 84.2 74.6 50.5 31.7 34.1 57.9 41.9 42.0
Table2: 3DReferringExpressionSegmentationResultsonScanReferdatasetwiththeaccuracy
evaluatedbyIoU0.25,IoU0.5andmIoU.Forthefirstblockmethods*thatonlyoutput3Dbounding
boxes,wereproducetheresultsbasedontheirofficialcodesbyextractingthepointsinsidetheboxes
asthesegmentationmaskpredictions.
5.2 3DReasoningSegmentationResults
We present the results of our 3D reasoning segmentation in Table 1. Our model significantly
surpasses previous models in tasks that require advanced reasoning and a deep understanding of
worldknowledge,achievinganotableincreaseinmeanIntersectionoverUnion(mIoU).Differing
fromtypical3Dreferringsegmentationtasks,thistaskdemandsnotjustanunderstandingofspatial
relationshipsbutalsorobustreasoningcapabilitiesandabroadcomprehensionofcontext.
Ourmodelexcelsatinterpretinglongsentencequeriesandmanaging3Dreasoningsegmentation
tasks, outperformingopen-vocabularysegmentationmethodssuchasOpenScene[31]andOpen-
Mask3D[39]. Wealsocompareitsperformanceagainstatwo-stagemethod,whereFlanT5generates
a short text output for the query in the first stage, followed by segmentation using either Open-
SceneorOpenMask3D.Ourmethodsurpassesthisapproachbyleveragingmoreexpressivehidden
embeddings,whichprovidearicherrepresentationthansolelyrelyingontextasanintermediary.
Comparedwithleading3Dreferringsegmentationmodels,specifically3D-STMN[45]fine-tunedon
theReason3Ddataset,weobservethatalthough3D-STMNexcelsindirectreferencing,itstruggles
withindirectqueries. Incontrast,ourmodeladeptlynavigatesthesechallengesbyintegratinglarge
languagemodels,therebydemonstratingsuperioradaptabilityandperformance.
Inaddition, wecompareourmodeltoanothertwo-stagemethodthatcombinesLlama2[20]and
CLIP[35],bothfine-tunedontheReason3Ddataset. ThisapproachbeginswithLlama2generating
anoutputvocabulary,whichCLIPthenprocessestoextracttextualfeatures. Thesefeaturesactas
queriesinteractingwithpointcloudfeaturesviaamaskdecoder(weexploitthesamepointfeatures
andmaskdecoderasourmethod),ultimatelyproducingthesegmentationmaskresults. Theresults
demonstratethatourapproachsignificantlyoutperformsthistwo-stagemethod,whichiscompletely
decoupledandreliessolelyonthetextualoutputsfromtheLLMmodel.
7RoomNum=1∼2 RoomNum=3∼4 RoomNum>=5
Method
0.25 0.50 mIoU 0.25 0.50 mIoU 0.25 0.50 mIoU
Reason3D(w/oHMD) 25.23 10.32 15.56 12.84 5.50 8.23 8.26 2.52 5.33
RegionSeg+Reason3D(w/oHMD) 26.98 13.21 17.02 19.21 8.21 11.67 11.96 4.78 7.21
Reason3D(w/HMD) 29.82 16.97 18.81 22.25 11.93 14.12 16.06 7.34 10.35
Table3: 3DReasoningSegmentationResultsonMatterport3Ddatasetwithdifferentroomnumbers
tovalidatetheeffectivenessofthedesignoftheproposedHierarchicalMaskDecoder(HMD).The
evaluationmetricisIoU@0.25,IoU@0.5andmIoU.
5.3 3DReferringExpressionSegmentationResults
Todemonstratetheeffectivenessofourmodelinthe3Dexpressreferringsegmentationtask,we
compareourapproachagainstcurrentstate-of-the-artmethodsonScanRefervalidationsetinTable2.
Itcanbeobservedthatourapproachsignificantlyoutperforms3D-STMN[45]inoverallperformance.
Consideringthelimitedfocusonthe3Dreferringsegmentationtaskintheexistingliterature,we
extendourcomparisontoseveral3Dgroundingapproachesthatonlypredict3Dboundingboxes.
These methods can predict segmentation masks by simply extracting points within the predicted
boxes. Notably, our approach vastly outperforms the LLM-based method, 3D-LLM [17], which
struggleswithaccuratelylocating3Dboxesandeffectivelyextractingsegmentationmasks.
5.4 AblationStudy
Effectiveness of Hierarchical Mask Decoder. We present experimental results in Table 3 to
demonstratetheeffectivenessofourproposedHierarchicalMaskDecoder(HMD)designinsearch-
ing objects across varying numbers of rooms. Since the Reason3D dataset primarily focuses on
single-room scenarios, we extend it to multi-room settings by reusing a subset of the annotated
Matterport3D[6]datatoincludeavaryingnumberofregions. WeoptnottoutilizeScanNet,asit
featuresonlysingle-roomscenes.
WeestablishthreevariantsofourReason3Dmodel: (1)Reason3DwithouttheHierarchicalMask
Decoder,(2)Reason3Dcombinedwitharegionsegmentationmodulethatfirstpredictsandsegment
the target room’s region then applying the Reason3D model for the segmented region, and (3)
Reason3DequippedwiththeHierarchicalMaskDecoder. Wecanvalidatetheeffectivenessofthe
proposed Hierarchical Mask Decoder (HMD) by comparing the first and last models, especially
whenthenumberofroomsexceedsfive. TheeffectivenessoftheHMDislikelyduetothedifficulty
of integrating language embeddings with point cloud features as the number of points increases
(i.e., inmoreroomsscenarios). Incontrast, thehierarchicalstrategyoftheHMD,whichusesan
additional token to learn a coarse region as a prior, effectively manages the output segmentation
masks. Inaddition,thesecondmodelsegmentspointcloudsfromtheregionofinterestbeforeusing
the Reason3D framework without the HMD. This method outperforms the first model but faces
optimizationchallengeswithinthetrainingpipelineofthetwo-stageapproach.
EffectivenessofDifferentDesigns. Table4showstheablationexperimentsofdifferentchoices.
Table4(a)provesthatthesuperpointspoolingoperationisessentialforourpipelinesinceithelpsto
reducethetrainingcomplexityandenablestheeffectivetrainingofthepipeline. Also,theaverage
poolingforthesuperpointscanachievebetterperformance. Table4(b)presentstheperformance
impactofvariouscomponentsofsegmentationloss. UsingeitherBinaryCross-Entropy(BCE)loss
orDicelossaloneleadstosignificantlyreducedperformance. Incontrast,combiningDicelossand
Superpoint Pool Acc@0.25 Time(ms) DICE BCE Acc@0.25 Acc@0.50 Layer Acc@0.25 Acc@0.50
✗ - 37.55 486.1 ✓ 41.73 31.36 1 40.25 27.65
✓ max 42.97 271.3 ✓ 30.86 22.47 3 42.78 30.62
✓ Avg 43.21 268.5 ✓ ✓ 43.21 32.10 6 43.21 32.10
(a)SuperpointPooling. (b)SegmentationLoss. (c)LayerofDecoders.
Table4: AblationexperimentsfordifferentdesignonthescannetV2datasetforthe3Dreasoning
segmentationtask.
8In a kitchen, what appliance in the scene is typically used In a bedroom, where would one typically find the primary piece
to toast bread for breakfast or snacks? of furniture used for sleeping or resting?
What specific element can be found in a staircase that allows In an office, when employees need to duplicate essential documents
individuals to move between different levels of a building? for a meeting, which item would they most likely utilize for this task?
In case of a fire emergency, what safety equipment can be In a dining room, where would guests typically sit during a meal
found in the hallway to help extinguish small fires? in the scene?
In a bedroom, what object in the scene can be adjusted to In a lounge area, what object in the scene is likely to provide
control the amount of natural light entering the room? warmth and comfort to the guests during colder days?
Figure4: VisualizationResultsfor3DReasoningSegmentationTasks. Eachsub-figurepresentsa
textualqueryalongsidetheinputpointcloud.Thepurpleregionshighlightthepredictedsegmentation
masksgeneratedbyourmodel.
BCElossresultsinthemostfavorableoutcomes. Table4(c)presentstheimpactofdifferentnumbers
ofdecoderlayers. Weuse6layersforthedecoderasthedefaultnumber.
5.5 VisualizationResults
Figure4displaysthevisualizationresultsofourReason3Dmodelforthe3Dreasoningsegmentation
task,highlightingourmodel’sproficiencyinaccuratelygeneratingsegmentationmasksbasedonthe
query. AdditionalvisualizationresultsareincludedintheAppendix.
6 Conclusion
This study presents Reason3D, an innovative framework that leverages Large Language Models
(LLMs)forenhancedsceneunderstanding,capableofgeneratingtextualresponsesandsegmentation
predictions. To demonstrate the effectiveness of our approach, we introduce the novel task of
3Dreasoningsegmentation,whichrequiresinterpretingimplicithumaninstructionswithinthree-
dimensionalscenes. Wehavedevelopedahierarchicalmaskdecoderthatimprovestheaccuracy
offinalmaskpredictionsbyinitiallyidentifyingabroadregionlikelytocontainthetargetobject,
whichthenservesasabasisforfurtherrefinement. OurextensiveexperimentsontheScanNetV2and
Matterport3Ddatasetsdemonstrateoutstandingperformanceinvariouscomplextasks,including3D
reasoningsegmentation,3Dreferringsegmentation,andquestionanswering.
9References
[1] PanosAchlioptas,AhmedAbdelreheem,FeiXia,MohamedElhoseiny,andLeonidasGuibas. Referit3d:
Neurallistenersforfine-grained3dobjectidentificationinreal-worldscenes. InECCV,2020. 3,6,14
[2] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatieMillican,MalcolmReynolds,RomanRing,ElizaRutherford,SerkanCabi,Tengda
Han,ZhitaoGong,SinaSamangooei,MarianneMonteiro,JacobMenick,SebastianBorgeaud,AndyBrock,
AidaNematzadeh,SahandSharifzadeh,MikolajBinkowski,RicardoBarreira,OriolVinyals,Andrew
Zisserman,andKarenSimonyan. Flamingo:avisuallanguagemodelforfew-shotlearning. InNeurIPS,
2022. 3
[3] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,KalyaniMarathe,
YonatanBitton,SamirGadre,ShioriSagawa,JeniaJitsev,SimonKornblith,PangWeiKoh,GabrielIlharco,
MitchellWortsman,andLudwigSchmidt. Openflamingo:Anopen-sourceframeworkfortraininglarge
autoregressivevision-languagemodels. arXivpreprintarXiv:2308.01390,2023. 1,3
[4] DaichiAzuma,TaikiMiyanishi,ShuheiKurita,andMotoakiKawanabe. Scanqa:3dquestionanswering
forspatialsceneunderstanding. InCVPR,2022. 6,14,15
[5] SatanjeevBanerjeeandAlonLavie. METEOR:AnautomaticmetricforMTevaluationwithimproved
correlationwithhumanjudgments. InACLWorkshop,2005. 6
[6] AngelChang,AngelaDai,ThomasFunkhouser,MaciejHalber,MatthiasNiessner,ManolisSavva,Shuran
Song,AndyZeng,andYindaZhang. Matterport3D:LearningfromRGB-Ddatainindoorenvironments.
In3DV,2017. 3,4,5,8,13,14
[7] DaveZhenyuChen,AngelXChang,andMatthiasNießner. Scanrefer: 3dobjectlocalizationinrgb-d
scansusingnaturallanguage. InECCV,2020. 3,6,7,14,15
[8] SijinChen,XinChen,ChiZhang,MingshengLi,GangYu,HaoFei,HongyuanZhu,JiayuanFan,andTao
Chen. Ll3da:Visualinteractiveinstructiontuningforomni-3dunderstanding,reasoning,andplanning. In
CVPR,2024. 1,4,5
[9] ShaoyuChen,JieminFang,QianZhang,WenyuLiu,andXinggangWang. Hierarchicalaggregationfor3d
instancesegmentation. InICCV,2021. 3
[10] ShizheChen, MakarandTapaswi, Pierre-LouisGuhur, CordeliaSchmid, andIvanLaptev. Language
conditionedspatialrelationreasoningfor3dobjectgrounding. InNeurIPS,2022. 15
[11] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski
convolutionalneuralnetworks. InCVPR,2019. 3
[12] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,EricLi,XuezhiWang,
MostafaDehghani,SiddharthaBrahma,andetal. Scalinginstruction-finetunedlanguagemodels. arXiv
preprintarXiv:2210.11416,2022. 6,7
[13] AngelaDai,AngelX.Chang,ManolisSavva,MaciejHalber,ThomasFunkhouser,andMatthiasNießner.
Scannet:Richly-annotated3dreconstructionsofindoorscenes. InCVPR,2017. 3,4,13
[14] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleFung,andStevenHoi. Instructblip: Towardsgeneral-purposevision-languagemodelswith
instructiontuning. arXivpreprintarXiv:2305.06500,2023. 1,3
[15] MartinEster,Hans-PeterKriegel,JörgSander,andXiaoweiXu.Adensity-basedalgorithmfordiscovering
clustersinlargespatialdatabaseswithnoise. InKDD,1996. 14
[16] Benjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3d semantic segmentation with
submanifoldsparseconvolutionalnetworks. InCVPR,2018. 4
[17] YiningHong,HaoyuZhen,PeihaoChen,ShuhongZheng,YilunDu,ZhenfangChen,andChuangGan.
3d-llm:Injectingthe3dworldintolargelanguagemodels. InNeurIPS,2023. 1,4,5,7,8,14,15
[18] JiHou,AngelaDai,andMatthiasNießner. 3d-sis:3dsemanticinstancesegmentationofrgb-dscans. In
CVPR,2019. 3
[19] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural
networksforreferring3dinstancesegmentation. InAAAI,2021. 3,4,7,15
[20] LouisMartinHugoTouvron,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,andetal. Llama2:Openfoundationand
fine-tunedchatmodels. arXiv:2307.09288,2023. 1,3,7
[21] SrinivasanIyer,XiVictoriaLin,RamakanthPasunuru,TodorMihaylov,DanielSimig,PingYu,Kurt
Shuster,TianluWang,QingLiu,PunitSinghKoura,XianLi,BrianO’Horo,GabrielPereyra,JeffWang,
ChristopherDewan,AsliCelikyilmaz,LukeZettlemoyer,andVesStoyanov. Opt-iml:Scalinglanguage
modelinstructionmetalearningthroughthelensofgeneralization. arXivpreprintarXiv:2212.12017,2022.
3
[22] ZhaoJin,MunawarHayat,YuweiYang,YulanGuo,andYinjieLei. Context-awarealignmentandmutual
maskingfor3d-languagepre-training. InCVPR,2023. 15
[23] MaximKolodiazhnyi,AnnaVorontsova,AntonKonushin,andDanilaRukhovich. Oneformer3d: One
transformerforunifiedpointcloudsegmentation. arXivpreprintarXiv:2311.14405,2023. 3
10[24] XinLai,ZhuotaoTian,YukangChen,YanweiLi,YuhuiYuan,ShuLiu,andJiayaJia. Lisa:Reasoning
segmentationvialargelanguagemodel. InCVPR,2024. 2,3,4,5
[25] LoicLandrieuandMartinSimonovski. Large-scalepointcloudsemanticsegmentationwithsuperpoint
graphs. InCVPR,2018. 4
[26] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. arXivpreprintarXiv:2301.12597,2023. 1,3,5,6
[27] Chin-YewLin. Rouge:Apackageforautomaticevaluationofsummaries. InTextsummarizationbranches
out,2004. 6
[28] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,2023.
1,3
[29] JunyuLuo,JiahuiFu,XianghaoKong,ChenGao,HaibingRen,HaoShen,HuaxiaXia,andSiLiu. 3d-sps:
Single-stage3dvisualgroundingviareferredpointprogressiveselection.arXivpreprintarXiv:2204.06272,
2022. 7,15
[30] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. Bleu:amethodforautomaticevaluation
ofmachinetranslation. InACL,2002. 6
[31] SongyouPeng,KyleGenova,Chiyu"Max"Jiang,AndreaTagliasacchi,MarcPollefeys,andThomas
Funkhouser. Openscene:3dsceneunderstandingwithopenvocabularies. InCVPR,2023. 3,5,7
[32] ZekunQi,RunpeiDong,ShaochenZhang,HaoranGeng,ChunruiHan,ZhengGe,HeWang,LiYi,and
KaishengMa. Shapellm: Universal3dobjectunderstandingforembodiedinteraction. arXivpreprint
arXiv:2402.17766,2024. 1
[33] ZhangyangQi,YeFang,ZeyiSun,XiaoyangWu,TongWu,JiaqiWang,DahuaLin,andHengshuang
Zhao. Gpt4point:Aunifiedframeworkforpoint-languageunderstandingandgeneration. InCVPR,2024.
1
[34] ZhipengQian, YiweiMa, JiayiJi, andXiaoshuaiSun. X-refseg3d: Enhancingreferring3dinstance
segmentationviastructuredcross-modalgraphneuralnetworks. InAAAI,2024. 3,7
[35] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. arXivpreprintarXiv:2103.00020,2021. 7
[36] HanoonaRasheed,MuhammadMaaz,SahalShaji,AbdelrahmanShaker,SalmanKhan,HishamCholakkal,
Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel grounding large
multimodalmodel. InCVPR,2024. 1
[37] JonasSchult,FrancisEngelmann,AlexanderHermans,OrLitany,SiyuTang,andBastianLeibe. Mask3D:
MaskTransformerfor3DSemanticInstanceSegmentation. InICRA,2023. 3,6
[38] JiahaoSun,ChunmeiQing,JunpengTan,andXiangminXu. Superpointtransformerfor3dsceneinstance
segmentation. arXivpreprintarXiv:2211.15766,2022. 3,6,13
[39] Ayça Takmaz, Elisabetta Fedele, Robert W. Sumner, Marc Pollefeys, Federico Tombari, and Francis
Engelmann. OpenMask3D:Open-Vocabulary3DInstanceSegmentation. InNeurIPS,2023. 7
[40] YuanTang,XuHan,XianzhiLi,QiaoYu,YixueHao,LongHu,andMinChen. Minigpt-3d:Efficiently
aligning3dpointcloudswithlargelanguagemodelsusing2dpriors. arXivpreprintarXiv:2405.01413,
2024. 1
[41] OpenAIteams. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023. 1,3
[42] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample. Llama:Openandefficientfoundationlanguagemodels. arXiv:2302.13971,
2023. 1,3
[43] RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh. Cider:Consensus-basedimagedescription
evaluation. InCVPR,2015. 6
[44] Peng-ShuaiWang. Octformer:Octree-basedtransformersfor3Dpointclouds. InSIGGRAPH,2023. 3
[45] Changli Wu, Yiwei Ma, Qi Chen, Haowei Wang, Gen Luo, Jiayi Ji, and Xiaoshuai Sun. 3d-stmn:
Dependency-drivensuperpoint-textmatchingnetworkforend-to-end3dreferringexpressionsegmentation.
InAAAI,2024. 3,7,8,15
[46] PenghaoWuandSainingXie. V*:Guidedvisualsearchasacoremechanisminmultimodalllms. arXiv
preprintarXiv:2312.14135,2023. 3
[47] Tsung-HanWu, GiscardBiamby, DavidChan, LisaDunlap, RitwikGupta, XudongWang, JosephE.
Gonzalez,andTrevorDarrell. See,say,andsegment:Teachinglmmstoovercomefalsepremises. arXiv
preprintarXiv:2312.08366,2023. 3
[48] XiaoyangWu,YixingLao,LiJiang,XihuiLiu,andHengshuangZhao. Pointtransformerv2:Grouped
vectorattentionandpartition-basedpooling. InNeurIPS,2022. 3
[49] RunsenXu,XiaolongWang,TaiWang,YilunChen,JiangmiaoPang,andDahuaLin. Pointllm: Em-
poweringlargelanguagemodelstounderstandpointclouds. arXivpreprintarXiv:2308.16911,2023. 1,
4
11[50] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,
PengchengShi,YayaShi,ChenliangLi,YuanhongXu,HehongChen,JunfengTian,QiQian,JiZhang,Fei
Huang,andJingrenZhou.mplug-owl:Modularizationempowerslargelanguagemodelswithmultimodality.
arXivpreprintarXiv:2304.14178,2023. 1,3
[51] Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Zhen Li, and Shuguang Cui. Instancerefer:
Cooperativeholisticunderstandingforvisualgroundingonpointcloudsthroughinstancemulti-level
contextualreferring. InICCV,2021. 15
[52] JunZhan,JunqiDai,JiashengYe,YunhuaZhou,DongZhang,ZhigengLiu,XinZhang,RuibinYuan,
GeZhang,LinyangLi,etal. Anygpt: Unifiedmultimodalllmwithdiscretesequencemodeling. arXiv
preprintarXiv:2402.12226,2024. 1
[53] ShilongZhang,PeizeSun,ShoufaChen,MinXiao,WenqiShao,WenweiZhang,KaiChen,andPingLuo.
Gpt4roi:Instructiontuninglargelanguagemodelonregion-of-interest. arXivpreprintarXiv:2307.03601,
2023. 3
[54] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal
chain-of-thoughtreasoninginlanguagemodels. arXivpreprintarXiv:2302.00923,2023. 1
[55] HengshuangZhao,LiJiang,JiayaJia,PhilipHSTorr,andVladlenKoltun. Pointtransformer. InICCV,
2021. 3
[56] LichenZhao,DaigangCai,LuSheng,andDongXu. 3DVG-Transformer:Relationmodelingforvisual
groundingonpointclouds. InICCV,2021. 7
[57] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi
Lin,ZhuohanLi,DachengLi,Eric.PXing,HaoZhang,JosephE.Gonzalez,andIonStoica. Judging
llm-as-a-judgewithmt-benchandchatbotarena. arXivpreprintarXiv:2306.05685,2023. 3
[58] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4:Enhancingvision-
languageunderstandingwithadvancedlargelanguagemodels. arXivpreprintarXiv:2304.10592,2023.
3
12Appendix
A Reason3DDataset
A.1 Datasetannotation
Each 3D scene in the Reason3D dataset consists of a textual query and a binary segmentation
mask to identify the target objects. As mentioned in the paper, we utilize ScanNetV2 [13] and
Matterport3D[6]asourdatasource. Consideringsingleroomspaceasonescene,wefirstextractthe
instanceobjectannotationandtheroomtypeinformationfromthesetwodatasetsasthetagof3D
scenes. Afterthat,weutilizethesetagsaspartsofthetextprompttoincorporatewithGPT-4. The
illustrationofthepromptconstructionprocessisshowninTable5,andsomesamplesutilizedfor
promptingareshowninTable6.
messages=[{"role":"system", "content":YouareanAIvisualassistant,andyouareseeing
a3Dscene. Whatyouseeisprovidedwithseveralwordstorepresentobjectswithtag<objects>,
describingthesceneyouarelookingat,andalsotheroomtype<type>todescribethetypeofthe
scene.Designaquestion<question>thatcanbeansweredconfidentlywiththe<answer>fromone
oftheprovidedobjectsin<objects>. Pleasedonotaskany<question>thatcannotbeanswered
confidently.Eachquestionshouldhaveoneclearanswerthatismostrelevant,withoutambiguityor
multiplepossibleanswersinthelistofdescriptionwords.Pleaseincludecomplexquestionsrelevant
tothescene’scontent,suchasinquiriesintothebackgroundknowledgeofobjectsordiscussions
abouteventsrelatedtotheseobjects.Avoidquestionsaboutuncertainoruncleardetails.Thequestion
shouldbenatural.}
]
for sample in few_shot_samples:
messages.append({"role":"user", "content":sample[‘context’]})
messages.append({"role":"assistant", "content":sample[‘response’]} )
messages.append({"role":"user", "content":‘\n’.join(query)})
Table5: Theillustrationofthepromptconstructionprocessforgenerating3Dreasoningdataset
withChatGPT/GPT-4.
A.2 DatasetStatistics
Asdetailedinthemainpaper,theReason3DdatasetincorporatestheScanNetV2andMatterport3D
datasets. Weadheretotheirofficialtrainingandvalidationsplitsfordataannotation. Specifically,the
Matterport3Ddatasetprovides934trainingsamplesand837validationsamples. Meanwhile,the
ScanNetV2datasetcontributes405trainingsamplesand308validationsamples.
B Experiments
B.1 ImplementationDetails
WeexecutethemodelsontwoNVIDIATeslaA100GPUs,usingabatchsizeof32fortrainingand
abatchsizeof1forinference. WeemploytheAdamWoptimizerwithparametersβ = 0.9and
1
β = 0.999,andaweightdecayof0.05. Additionally,weapplyalinearwarm-upstrategytothe
2
learningrateduringtheinitial1,000steps,increasingitfrom10−8to10−4,followedbythecosine
decayschedule. AllexperimentsareimplementedwiththePytorchframework.
B.2 DataStatistics
Inadditiontothereason3Ddataset,ourmodelutilizesthefollowingdatasetsfortraining:
ScanNet [13], a comprehensive 3D indoor dataset, covers diverse environments including
apartmentsandvariousroomtypes. Thedatasetisstructuredinto1201trainingscenes,312validation
scenes,and100testingscenes. WeadoptthetrainingconfigurationasSPFormer[38]fortrainingour
13‘context’: <room>gameroom<objects>’armchair’,’ceiling’,’door’,’doorframe’,’fireplace’,
’floor’,’pooltable’,’post’,’rail’,’stair’,’stool’,’tv’,’wall’,’window’
‘response’: <question>Inagameroom,whatobjectinthescenecouldbeusedforplayinga
competitiveandstrategicgameinvolvingballsandcues?<answer>pooltable
‘context’:<room>livingroom<objects>’armchair’,’bedroom’,’bookshelf’,’lamp’,’bureau’,
’carpet’,’ceilingfan’,’chair’,’computer’,’computerdesk’,’couch’,’table’,’drawer’,’dresser’
‘response’:<question>It’sveryhotoutside.Aftercomingbackhome,whatappliancewouldyou
turnontohelpcooldownthetemperature?<answer>ceilingfan
‘context’:<room>gameroom<objects>’table’,’door’,’cabinet’,’desk’,’officechair’,’picture’,
’lamp’,’bathtub’,’bag’,’trashcan’,’mirror’,’radiator’
‘response’:<question>Whenstayingatahotel,whatpartoftheroominthescenecanprovide
additionallightingforreadingorworkingwhileinbed?<answer>lamp
‘context’:<room>gameroom<objects>’floor’,’door’,’cabinet’,’shelf’,’desk’,’officechair’,
’window’,’monitor’,’book’,’box’,’keyboard’,’trashcan’,’filecabinet’,’fan’,’telephone’,’cup’,
’papertowelroll’,’windowsill’,’clock’,’headphones’
‘response’:<question>Ifsomeonewantedtocheckthetimeaftergettingreadyinthemorning,
whatobjectinthisscenewouldtheymostlikelyuse?<answer>clock
Table6: ThefewshotsamplesusedforChatGPTprompting.
sceneencoderontheScanNettrainingdataset.
Matterpor3D [6]datasetisalarge-scale,real-worlddatasetcomprising90houses. Eachhouseis
dividedintovariousregions. Wedonotfine-tunethesceneencoderontheMatterport3Ddataset.
ScanRefer [7], a dataset annotated using ScanNet for 3D express referring segmentation tasks,
including36,665naturallanguagedescriptionsrelatedto7,875objectsacross562scenesfortraining,
and9,508descriptionsof2,068objectsfrom141ScanNetscenesforevaluation.
Nr3D [1], another 3D referring segmentation dataset derived from ScanNet, comprises 32,919
language descriptions associated with 4,664 objects from 511 scenes for training purposes. We
furtheremploythisdatasettotrainfor3Dexpressreferringsegmentationtasks.
ScanQA[4]isadatasetfor3DquestionansweringtaskbasedonScanNet, consistingof25,563
question-answerpairson562scenesfortrainingand4,675question-answerpairson71scenesfor
validation.
B.3 3DQuestionAnsweringResults
Inadditiontoexcellingin3Dreasoningandreferringtasks,ourapproachalsoperformswellin3D
questionansweringtasks. WepresentourresultsontheScanQAvalidationsetinTable7,where
weobservedasignificantimprovementinevaluationmetricsoverbothbaselinemethodsandthe
recentLLM-basedmethod,3D-LLM.Ourapproachnotonlyanswersquestionsaccuratelybutalso
visualizestherelatedsegmentationmaskstofurtherdemonstratetheeffectiveness.
B.4 3DVisualGroundingResults.
Althoughourprimaryfocusison3Dsegmentation,ourmethodalsoeffectivelypredicts3Dbounding
boxes as supplementary outputs, facilitating comparison with 3D visual grounding methods. To
generatea3Dboundingboxforareferredobject,wefirstapplyDBSCAN[15]toeliminatenoisy
points, and then calculate the minimum and maximum XYZ coordinates from the points within
the segmentation mask to form the 3D box. As demonstrated in Table 8, our approach not only
outperformsrecent3DvisualgroundingmethodsbutalsosignificantlysurpassesLLM-basedmethods,
suchas3D-LLM[17],whichstruggletointegratetextualandnumericaldatatoaccuratelylocalize
objectsin3Dspace.
14Method Venue Acc@0.25 Acc@0.50
Method Venue B-4 METEORROUHE-LCIDER
ScanRefer[7] ECCV2020 38.97 26.10
VoteNet+MCAN - 6.2 11.4 29.8 54.7 InstanceRefer[51] ICCV2021 40.23 32.93
ScanRefer+MCAN - 7.9 11.5 30 55.4 3D-SPS[29] CVPR2022 47.65 36.43
ScanQA[4] CVPR2022 10.1 13.1 33.3 64.9 ViL3DRel[10] NeurIPS2022 47.94 37.73
3D-VLP[22] CVPR2023 11.2 13.5 34.5 67.0 3D-LLM[17] NeurIPS2023 30.3 -
3D-LLM[17] NeurIPS202312.0 14.5 35.7 69.4
TGNN[19] AAAI2021 37.37 29.70
Reason3D(Ours) - 12.1 15.1 37.4 73.5 3D-STMN[45] AAAI2024 46.8 36.6
Table7: 3DquestionansweringresultsonScanQA Reason3D(Ours) - 49.60 41.10
validationdataset. Thefirsttworesultsarefrom[4].
Table 8: 3D visual grounding results on
B-4 denotes BLEU-4. Our model achieves better
ScanRefervalidationdataset. Ourapproach
resultsthanallbaselinemodels.
doesnotuse3Dboxannotationfortraining.
B.5 MoreVisualizationResults.
3DReasoningSegmentation. Weprovidemorequalitativeexamplesforthe3Dreasoningsegmenta-
tiontaskandthepredictionsbyourReason3DinFigure5.
3DReferringSegmentation. Weshowthevisualizationresultsofthe3Dreferringsegmentation
task compared with 3D-STMN[45] in Figure 6. We observethat our approach can havecorrect
predictionswhenthescenescontainmultiplesimilarobjectsorwhenthequerysentenceislong,
whichprovestheeffectivenessofourapproach.
C FailureCase.
InFigure7,wepresentrepresentativefailurecasesasfollows: (a)Ifthequestioninvolvesqueryinga
smallobjectinthescene,ourmodelmayfailtogeneratethecorrectprediction. (b)Thepresenceof
similarobjectsinthescenemayleadtofalsepositivepredictionsbyourmodel. (c)Similarstructures
inthepointcloud,suchasmirrorsorsensor-inducedfragments,canmisleadourmodel. (d)Complex
worldknowledgerequiredbythequestionmayhinderourmodel’sabilitytogenerateaccuratemask
predictions.
D Limitations
Whileourmodelintroducesanovelapproachtothe3Dreasoningsegmentationtask,itdoeshave
severallimitations. Firstly,weobservethatourmodelcannothandleverylarge-scalescenesfor3D
reasoningsegmentation(e.g.,searchingforanobjectwithinahousewith30roomsintheMatterport
dataset). Furthermore,ourmodellackstheabilitytohandlefalsepremisescases(e.g.,searchingfor
anobjectthatmaynotbeinsidethescenes),whichcouldbeapromisingdirectiontoexplorewithin
theLLM-based3Dreasoningframework.
E BroaderImpact
Reason3D is designed to segment objects in 3D space based on language inputs. Compared to
traditional3Dsegmentationalgorithms,Reason3Dmodelshavealowercustomizationthreshold,
allowing users to identify objects through natural language. However, this accessibility could
potentiallybeabused. Additionally,thedatasetsandpre-trainedmodelsutilizedinReason3Dmay
containinherentbiases,whichcouldbereflectedinthemodel’sperformance.
15After a tiring day at work, if someone arrives home feeling hungry and
If you were to search for items such as milk, cheese, and vegetables
wants to warm up a meal swiftly, which appliance in the kitchen setting
in the scene, where would you most likely locate them stored?
could they utilize for this task?
In a hotel room, guests commonly use the object in the scene to
In a bedroom, which item in the scene can be utilized to block
contact the front desk for room service or to make external calls.
out sunlight and ensure privacy
What item is typically provided in hotel rooms to serve this function?
What item is commonly used for transporting clothes and personal Where would one typically keep important documents, folders,
items while traveling to ensure convenience and organization? and files to ensure they are securely stored and easily accessible?
In a lounge area, what object in the scene is commonly used for Which appliance in the laundry room is commonly used to clean
disposing of waste such as paper, packaging, or other items? clothes by agitating them in water with detergent?
In a family room, what object in the scene might be used to control the Where in an office environment are employees likely to locate
amount of natural light entering the room or to provide privacy when
their incoming mail or packages?
needed?
Identify the object in the scene that functions to prevent water In colder climates, what item in the scene is frequently utilized to
from splashing out during a shower. warm the room by heating the air?
In a spa room, where would a client typically lie down to receive Which item in the scene can be utilized to eliminate wrinkles from
clothing, especially when guests need to prepare their attire for a formal
a relaxing massage treatment?
event or meeting in a hotel room?
Figure 5: Visualization Results for 3D Reasoning Segmentation Tasks. The purple regions
highlightthepredictedsegmentationmasksgeneratedbyourmodel. Bestviewedwithzoomin.
16Description 3D-STMN Reason3D Ground Truth
The pillow is brown it is
on the left side of the
couch.
The table is square there
is a chair to the left of it,
and a chair to the right of
it.
The seat is north of the
brown coffee table and
west of the westernmost
yellow table the seat is
light brown and square.
The black trash can is to
the left of the copier
machine to the left of the
black trash can is a giant
cork board.
Figure6: VisualizationResultsfor3DReferringSegmentationTasks. Thepurpleregionsdenotes
thepredictedsegmentationmasksfromourReason3D.Theredandgreenmeansthepredictionsfrom
3D-STMNandgroundtruth,respectively. Bestviewedwithzoomin.
17(a.) Small queried object
Safety is crucial in public spaces like spas. What
object in the scene is designed to detect and suppress
fires to ensure the well-being of everyone in the spa?
(b.) Similar objects in the scene.
In a public restroom, if someone wishes to dry their
hands after washing, which item in the scene can
they use for this task?
(c.) Similar structures of the point cloud.
What object in the scene would be most useful for
checking one's appearance before leaving the bathroom?
(d.) Requiring complicate world knowledge.
In the bathroom, there may be decorative elements on the walls that
hold symbolic significance. What object in the scene could be a
religious symbol often associated with Christianity?
Figure7: Failurecases. (a.) Smallqueriedobjects. (b.) Similarobjectinthescene. (c.) Similar
structuresofthepointcloud. (d.) Thequestionrequiringcomplicateworldknowledge. Thepurple
regions denotes the predicted segmentation masks from our Reason3D and the green means the
groundtruth. Bestviewedwithzoomin.
18