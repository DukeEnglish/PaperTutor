Leveraging Offline Data in Linear Latent Bandits
Chinmaya Kausik1, Kevin Tan2, Ambuj Tewari1
1University of Michigan, 2University of Pennsylvania
Abstract
Sequential decision-making domains such as recommender systems, healthcare and
education often have unobserved heterogeneity in the population that can be modeled
using latent bandits – a framework where an unobserved latent state determines the
model for a trajectory. While the latent bandit framework is compelling, the extent of
its generality is unclear. We first address this by establishing a de Finetti theorem for
decision processes, and show that every exchangeable and coherent stateless decision
process is a latent bandit. The latent bandit framework lends itself particularly well
to online learning with offline datasets, a problem of growing interest in sequential
decision-making. One can leverage offline latent bandit data to learn a complex model
for each latent state, so that an agent can simply learn the latent state online to
act optimally. We focus on a linear model for a latent bandit with d -dimensional
A
actions, where the latent states lie in an unknown d -dimensional subspace for d ≪
K K
d . We present SOLD, a novel principled method to learn this subspace from short
A
offline trajectories with guarantees. We then provide two methods to leverage this
subspaceonline: LOCAL-UCBandProBALL-UCB.WedemonstratethatLOCAL-UCB
√ √
enjoys O˜(min(d T,d T(1+(cid:112) d T/d N))) regret guarantees, where the effective
A K A K
dimension is lower when the size N of the offline dataset is larger. ProBALL-UCB
enjoys a slightly weaker guarantee, but is more practical and computationally efficient.
Finally, we establish the efficacy of our methods using experiments on both synthetic
data and real-life movie recommendation data from MovieLens.
1 Introduction
Many sequential-decision making problems can be effectively modeled using the bandit
framework. This can span domains as diverse as healthcare [Lu et al., 2021], randomized
clinical trials [Press, 2009], search and recommendation [Li et al., 2010], distributed networks
[Kar et al., 2011], and portfolio design [Brochu et al., 2011]. However, in many applications,
there often exists unobserved heterogenity in the population, or unobserved context, that
influences the distribution of rewards. In Hong et al. [2020], it is shown that this uncertainty
can be modeled by a latent bandit (or mixture of bandits). This is a bandit where an
unobserved latent state determines the reward model for the trajectory. The latent bandit
framework therefore has high practical value.
However, the extent of the generality of this framework is unclear. Given a general decision
process, how strong do the conditions need to be to make it a latent bandit? To answer
1
4202
yaM
72
]GL.sc[
1v42371.5042:viXrathis, we turn to de Finetti’s celebrated theorem, which says that exchangeable sequences of
Bernoulli random variables are mixtures of sequences of i.i.d. Bernoulli random variables.
This has also been extended to larger classes of stochastic processes. For example, Diaconis
and Freedman [1980] show that any random process satisfying recurrence and a weakened
notion of exchangeability is a mixture of Markov chains. Inspired by such work, we define
a general stateless decision process, and consider a weak notion of exchangeability as well
as a natural coherence condition. We show that in the spirit of de Finetti’s theorem, any
coherent and exchangeable stateless decision process is a latent bandit.
In light of this equivalence and the empirical relevance of linear contextual bandits [Li
et al., 2010, Abbasi-Yadkori et al., 2011, Abbasi-yadkori et al., 2011, Abbasi-Yadkori et al.,
2012], we define and explore the setting of linear latent contextual bandits, where a low-
dimensional latent state imposes a linear effect on the reward model parameters. When
interacting with a latent bandit online, an agent receives rewards based on a single latent
state. For example, a patient’s underlying genetic makeup could be the latent parameter
that affects their response to treatment. Often, one also has access to large offline datasets
with unobserved heterogeneity, like data from randomized controlled trials. Using traditional
bandit algorithms in this setting does not leverage the offline data available to the agent.
One also cannot simply treat the offline data as coming from a single bandit, since there
is significant distribution shift between the mean offline parameter and a single online
parameter.
Addressingthis,Hongetal.[2020]considerstheproblemofspeedinguponlinelearningusing
models learnt offline for finitely many latent states. Unlike them, we tackle the problem of
using subspaces learnt offline to accelerate online learning, as learning the "latent subspace"
spanned by possible reward model parameters is equivalent to learning the infinitely many
models in the subspace. We provide a method to learn this subspace from a dataset of
trajectories collected under some behavior policy π , and seek to use it to speed up online
b
learning. Our subspace learning algorithm is inspired by a long line of work in using spectral
methods to learn mixtures of various generative models [Vempala and Wang, 2004, Kong
etal.,2020,ChenandPoor,2022,Kausiketal.,2023]. However, each ofthesemethods deals
with finitely many latent states, and clusters data under separation assumptions. Clustering
is not compatible with continuous latent states, and so, novel ideas are needed to extend
these spectral methods to our setting. Finally, the subspace learnt offline is typically an
uncertain estimate of the true subspace of reward parameters. Online methods using the
subspace should thus account for this uncertainty. We therefore design two such methods
with a tractability-tightness tradeoff. With this in mind, we outline our contributions below:
• We are the first, to our knowledge, to prove a de Finetti theorem for decision processes.
• We present SOLD, an offline method for learning low-dimensional subspaces of reward
parameters with guarantees, generalizing spectral learning methods from discrete to
continuous latent states.
• We present LOCAL-UCB, an online algorithm leveraging subspaces and uncertainty
√ √
sets estimated offline to sharpen optimism for LinUCB, with O˜(min(d T,d T(1+
A K
(cid:112) d T/d N))) regret.
A K
• Finally, we present ProBALL-UCB, a practical and computationally efficient online
algorithm with a slightly looser regret guarantee. This illustrates a general algorithmic
2idea for integrating offline subspace estimation into optimistic algorithms.
• We establish the efficacy of our algorithms outlined above through a simulation study and
a demonstration on a real recommendation problem with the MovieLens-1M [Harper and
Konstan, 2015] dataset.
Other related work. The work of Tennenholtz et al. [2021] considers linear contextual
bandits with partially observed offline data and fully observable online features, albeit in
a very different model of partial information. Additionally, there has been an increasingly
large amount of work in offline-to-online transfer learning for reinforcement learning, also
known as hybrid RL [Song et al., 2023, Nakamoto et al., 2023, Amortila et al., 2024, Li et al.,
2023, Rajeswaran et al., 2017, Hester et al., 2018, Nair et al., 2018, 2020, Ball et al., 2023,
Xie et al., 2022, Ren et al., 2024, Wagenmaker and Pacchiano, 2023, Tan and Xu, 2024].
2 A De Finetti Theorem for Decision Processes
We first define a stateless decision process at a high level of generality.1
Definition 1. A stateless decision process (SDP) with action set A is a probability space
(Ω,G,P) equipped with a family of random maps F :Ω→(AH →RH) for H ∈N∪{∞}.
H
That is, given a sequence of actions (a ,...a ), an SDP generates a random sequence of
1 H
rewards (Y ,...Y ). In the light of this interpretation, we will abuse notation to denote by
1 H
F (a ,...a ) the random variable ω (cid:55)→F (ω)(a ,...a ). Without any coherence between
H 1 H H 1 H
F across H, a stateless process can behave arbitrarily for different horizons H. We present
H
anaturalcoherenceconditionbelow,essentiallyrequiringthatagivenactionsequenceshould
produce consistent rewards.
Definition 2. A stateless decision process is said to be coherent if for any h≤k ≤H,H′ ∈
N∪{∞}andforanytwoactionsequencesτ,τ′ oflengthsH andH′ sharingthesameactions
(a ,...a ) from index h to k, with F (τ) = (Y ,...Y ) and F (τ′) = (Y′,...Y′ ), we
h k H 1 H H′ 1 H′
have (Y ,...Y )=(Y′,...Y′), viewed as functions of Ω.
h k h k
It is natural to require equality in value and not just in distribution, since after taking an
extra action, the values of past rewards stay the same, not just their distribution. We also
give a natural definition for exchangeability of a stateless decision process – namely that
exchanging any two rewards should lead to the distribution obtained by exchanging the
corresponding actions.
Definition 3. A stateless decision process is said to be exchangeable if for any permu-
tation π : [H] → [H] and F (a ,...a ) = (Y ,...Y ), we have F (a ,...a ) ∼
H 1 H 1 H h π(1) π(H)
(Y ,...Y ).
π(1) π(H)
Finally, a latent bandit is an SDP that behaves like a bandit conditioned on a random latent
state F that determines the reward distribution. As F determines a distribution, it is a
random measure-valued function on A.
1Liuetal.[2023]workwithamuchmorerestrictivenotionofageneralizedbanditandusetheoriginalde
Finettitheoreminsomeoftheirlemmas. SeeAppendixB.1foradiscussion.
3Definition 4. A latent bandit is a stateless decision process equipped with a random
measure-valued function F : Ω → (A → P(R)) so that for any H and action sequence
(a ,...a ), the rewards (Y ,...Y ) := F (a ,...a ) are independent conditioned on F.
1 H 1 H H 1 H
Moreover, the conditional distribution L[Y |F]=F(a ) for all h≤H.2
h h
While exchangeability and coherence are reasonable conditions on an SDP and are clearly
satisfiedbylatentbandits,itisa-prioriuncleariftheyaresufficienttoensurethattheSDPis
a latent bandit. In particular, since only exchanging rewards from the same action preserves
the distribution, standard de Finetti proof ideas do not immediately apply. After all, it is
possible that an SDP could be cleverly designed to choose rewards adaptively across time
and satisfy these properties. Reassuringly, no such counterexamples exist and we have the
following theorem.
Theorem 1 (De Finetti Theorem for Stateless Decision Processes). Every exchangeable and
coherent stateless decision process is a latent bandit.
WeshowinLemma2thatcoherenceisnotaconsequenceofexchangeability–itisanecessary
condition for being a latent bandit. Finally, we consider contexts and define contextual
decision processes in a manner agnostic to context transitions. That is, the process only
carries the data of how rewards are generated from given sequences of contexts and actions,
while the context transitions themselves may be generated by a different process.
Definition 5. A transition-agnostic contextual decision process (TACDP) with action set A
and context set X is a probability space (Ω,G,P) equipped with a family of random maps
F :Ω→((X ×A)H →RH) for H ∈N as well as a map F :Ω→((X ×A)N →RN).
H ∞
Itisnaturaltoisolateawaycontextdynamicsforprocesseslikestochasticcontextualbandits,
in which actions do not affect the context transitions – in contrast to transition-aware
definitions in Jiang et al. [2016]. We can analogously define coherence and exchangeability
for TACDPs and define latent contextual bandits by simply replacing A with X ×A in the
definitions above. We then show an analogous de Finetti theorem, as a corollary of our proof
of Theorem 1. See Appendix B.2 for more details.
3 Linear Latent Contextual Bandits
Latent bandits are a powerful and general framework for encoding uncertainty in reward
models. However,thisgeneralityisbothablessingandacurse. Withoutfurtherassumptions
on the structure of the latent state F, one cannot hope to estimate it. We therefore focus on
a special case here. We consider the natural generalization of the linear contextual bandit
to the latent bandit setting, where we impose a linear structure on the effect of latent
states. We further justify this by noting that in most application domains, it is reasonable
to assume that a parsimonious, low-dimensional latent state affects the reward distribution.
This motivates the following definition.
Definition 6. A linear latent contextual bandit is a TACDP equipped with a feature
map for context-action pairs ϕ : X ×A → RdA, a latent random variable θ ∈ RdK with
2We abuse notation twice here. First, we write F(a ) := (ω (cid:55)→ F(ω)(a )). Second, as the regular
h h
conditionaldistributionL[Y |F]isakernelthatmapsfromΩ×B→R,weviewF(a )asitscurriedmap
h h
(ω,B)(cid:55)→F(a )(ω)(B). Adiscussionofissueslikemeasurabilityandwell-definednessisinAppendixB.2.
h
4distributionD
θ
andamapU
⋆
:RdK →RdA suchthatforanyH andcontext-actionsequence
((x ,a ),...(x ,a )), the rewards (Y ,...Y ) are independent conditioned on θ. Moreover,
1 1 H H 1 H
Y | θ ∼ ϕ(x ,a )⊤β+ϵ, where ϵ is subgaussian noise independent of all actions and all
h h h
other observations, and β =U θ.
⋆
In other words, this is a latent contextual bandit where the random measure-valued function
F :Ω→((X ×A)→P(R)) is defined by setting F(ω)(x,a) to be the distribution given by
ϕ(x,a)⊤U θ(ω)+ϵ,foranyω ∈Ωand(x,a)∈(X×A). Further,wenotethatWLOGU is
⋆ ⋆
unitary. This is because for any invertible map A:RdK →RdK, the observation distribution
does not change upon replacing θ with Aθ and replacing U with U A−1. One can see this
⋆ ⋆
as a generalization of the fact that with finitely many latent states, permuting the labels
and rewards together does not change observations.
Let us now assume that we have access to a dataset D of N short trajectories τ =
off n
((x ,a ,r ),..., (x ,a ,r )) of length H, collected by some behavior policy π .
n,1 n,1 n,1 n,H n,H n,1 b
In online deployment, a single latent label θ is chosen and rewards are generated using
⋆
β =U θ . At each timestep t, an agent observes contexts x and uses both the offline data
⋆ ⋆ ⋆ t
and the online data at time t to execute a policy π . Define the optimal action at time t by
t
a⋆ :=max ϕ(x ,a )⊤β We tackle the problem of minimizing the frequentist regret in linear
t a t t ⋆
latent contextual bandits, given by
Reg :=(cid:80)T ϕ(x ,a⋆)⊤β −E [ϕ(x ,a)⊤β ].
T t=1 t t ⋆ a∼πt t ⋆
For example, in medical applications, data from short randomized controlled trials can be
used to help an agent suggest treatment decisions for a new patient online. In this case, we
would like the algorithm to administer the correct treatments for each patient. This means
that the frequentist regret is the relevant performance metric here, and not the Bayesian
regret over some prior. Additionally, any worst-case bound on the frequentist regret is a
bound on the Bayesian regret for arbitrary priors.
Challenges with latent bandits. Despite the linear assumption, and the dimension
reduction obtained in the common case when d ≪d , significant challenges remain. First,
K A
the value of the latent state θ and the map U are both unknown a priori, making it hard to
⋆
leverage the low-dimensional structure of the problem. Second, a good choice of dimension
d is itself unknown a priori, and must be determined from data in a principled manner.
K
In the following section, we will provide a method to estimate latent subspaces from offline
data that allows us to overcome these challenges.
4 Estimating Latent Subspaces from Offline Data
Although we do not have access to the values of the latent states θ or to the map U , we
⋆
can still extract useful information from data. To that effect, recall that we have access to
a dataset D of N trajectories τ =((x ,a ,r ))H of length H, collected by some
off n n,h n,h n,h h=1
behavior policy π .
b
How can offline data help us in online deployment? To minimize the regret, one
must learn β
⋆
online. However, it is much easier to search among all θ ∈ RdK than to
search among all possible β ∈RdA since typically, d
K
≪d A. So, it will help to learn some
5Uˆ⊤ ≈ U⊤
∗
: RdA → RdK offline so that for any estimate βˆ
t
of β ⋆, Uˆ⊤βˆ
t
is an estimate of
θˆ
t
∈RdK. This amounts to learning a subspace of the feature space from logged bandit data.
We therefore provide a method for Subspace estimation from Offline Latent bandit Data
(SOLD), presented as Algorithm 1.
Algorithm 1 Subspace estimation from Offline Latent bandit Data (SOLD)
1: Input: DatasetD off ofcollectedtrajectoriesτ n =((x n,1,a n,1,r n,1),...,(x n,H,a n,H,r n,1))
under a behavior policy π , dimension of latent subspace d .
b K
2: Divide each τ n into odd and even steps, giving trajectory halves τ n,1 and τ n,2.
3: Estimate reward parameters βˆ n,i ← V− n,1 ib n,i, where V n,i = µI +
(cid:80) ϕ(x,a)ϕ(x,a)⊤ and b =(cid:80) ϕ(x,a)r for i=1,2.
(x,a,r)∈τn,i n,i (x,a,r)∈τn,i
4: Compute M n = 21(βˆ n,1βˆ n⊤ ,2+βˆ n,2βˆ n⊤ ,1) and compute M N := N1 (cid:80)N n=1M n.
5: Compute matrices D N,i := N1 (cid:80)N n=1(I−µV− n,1 i) for i=1,2.
6: Obtain the top d K eigenvectors of D− N1 ,1M ND− N1 ,2, denoting them by Uˆ.
7: return Projection matrix UˆUˆ⊤, ∆ off as in Theorem 2
On trajectory splitting and corrections. We split each trajectory to obtain two
independent estimators βˆ ,βˆ of β, compute the outer products βˆ⊤ βˆ for each
n,1 n,2 n,1 n,2
trajectory, and try to obtain the top d eigenvectors of the mean outer product across
K
trajectories. We want the mean outer product to concentrate to E[ββ⊤]=U E[θθ⊤]U⊤,
⋆ ⋆
which has the same d -dimensional span as U .
K ⋆
However,thereisawrinklehere. Wecannotsimplytakethetopd eigenvectorsofthemean
K
outer product M . Note that E[M ]=E[D β β⊤D ]=E[D U θ θ⊤U⊤D ]. To
N N n,1 n n n,2 n,1 ⋆ n n ⋆ n,2
separate E[β β⊤] from this, we need D ,β ,D to be independent. If π does not use
n n n,1 n n,2 b
θ and contexts are generated independently of each other and of θ, then this is satisfied.
In fact, we show in the lemma below that if any of these three conditions is violated, then
it is in fact impossible to determine the latent subspace U⋆ using any method, even with
infinitely many infinitely long trajectories.
Lemma 1 (Contexts, θ, and π cannot be dependent). For each of these conditions:
b
1. Contexts in a trajectory are dependent but do not depend on θ, and π also does not use
b
θ,
2. Contexts are generated independently using θ, while π does not use θ,
b
3. Contexts are generated independently without using θ, while π uses θ,
b
there exists a pair of linear latent contextual bandits with orthogonal latent subspaces sat-
isfying the condition, and a behavior policy π so that the offline data distributions are
b
indistinguishable and cover all (x,a) pairs with probability at least 1/4. Since the latent
subspaces are orthogonal, an action that gives the maximum reward on one latent bandit
gives reward 0 on the other.
To estimate the latent subspace, one is thus forced to make the following assumption.
6Assumption 1 (Unconfounded Offline Actions). The offline behavior policy π does not
b
use θ to choose actions, and contexts x are stochastic and generated independently of
n,h
each other and of θ.
As an example, this assumption is satisfied in practice when the offline data comes from
randomized controlled trials or A/B testing, which are common sources of offline datasets.3
Now, let the covariance matrix of θ be Λ and let its mean be µ . Then we have that
θ
E[M ]=E[D β β⊤D ]=E[D ]U (Λ+µ µ⊤)U⊤E[D ]. So, we still cannot merely
N n,1 n n n,2 n,1 ⋆ θ θ ⋆ n,2
consider the top d eigenvectors of M without accounting for D . Intuitively, D
K N n,1 n,1
capturesthefactthatwecannotestimaterewardsofactionsthatwehavenottakeninashort
trajectory. We therefore construct correction matrices D and use them to "neutralize"
N,i
the effect of unseen actions from the short trajectories. In particular, D−1 M D−1 is an
N,1 N N,2
estimator for U (Λ+µ µ⊤)U⊤. Crucially, this allows us to aggregate information across
⋆ θ θ ⋆
many trajectories to overcome the challenge of learning from short trajectories. We can now
take the top d eigenvectors of D−1 M D−1 to estimate the subspace determined by U .
K N,1 N N,2 ⋆
However, to give guarantees for our method, we must first make a coverage assumption:
Assumption 2 (BoundednessandCoverage). Rewards|r |≤Rforalln,h,∥ϕ(x,a)∥ ≤1
n,h 2
and ∥β∥ ≤R. Also, λ :=min λ (E[D ])>0 and λ := 1 λ (Λ)>0.
2 A i=1,2 min n,i θ R2 min
Intuitively, λ measures coverage along actions, while λ measures coverage along latent
A θ
states θ. Both must be non-zero to expect satisfactory estimation of the subspace. We
can then use confidence bounds for M and D to give a data-dependent confidence
N N,i
bounds ∆ for the projection matrix UˆUˆ⊤, as in Theorem 2 below. In one instantiation,
off
Propositions 1 and 2 in Appendix C derive simple data-dependent bounds for M and D
N N,i
respectively. Under this choice, we can control the growth of ∆ in terms of the unknown
off
problem parameters, shown at the end of Theorem 2.
Theorem 2 (Computing and Bounding ∆ ). Let ∥M −E[M ]∥ ≤ ∆ and ∥D −
off N 1 2 M N,i
E[D ]∥ ≤ ∆ for i = 1,2 with probability 1−δ/3 each. Then, with probability 1−δ,
n,i 2 D
∥UˆUˆ⊤−U U⊤∥ ≤∆ , where for B =∥D−1 ∥ and λˆ :=λ (M )−λ (M ),
⋆ ⋆ 2 off D N 2 dK N dK+1 N
∆ off = 2√ λ2 ˆdK (cid:18)(cid:16) B (D3 1−(2 B− DB ∆D D∆ )D 2)(cid:17) (R2+∆ M)∆ D+(cid:16) 1−BB DD ∆D(cid:17)2 ∆ M(cid:19)
Obtaining ∆
M
and ∆
D
from Propositions 1 and 2, ∆
off
=O(cid:101)( λθ1
λ3
AN−1/2(cid:112) d Kd Alog(d A/δ)).
Estimating d from offline data. Since our estimator D−1 M D−1 is approximately
K N,1 N N,2
rank-d , the number of significant eigenvalues of the estimator is a principled heuristic for
K
determining d .
K
Insufficiency of PCA and PMF for subspace estimation. Naively performing PCA
on the raw rewards or on single reward estimates βˆ can lead to erroneous subspaces – as
n
while the PCA target is linear-algebraically similar to M , it is statistically different. The
N
3Evenwhentheassumptionisnotsatisfiedinpractice,Algorithm1canlearnagoodsubspacewhenever
−1 −1
D N,1MND
N,2
haseigenspaceclosetothespanofU⋆,e.g. whenhigh-rewardactionscontributeheavilyto
Dn,i.
7PCA target (e.g. E[βˆ βˆ⊤]) is typically full rank due to the variance of the per-reward noise ϵ.
n n
On the other hand, PMF [Mnih and Salakhutdinov, 2007b] offers neither confidence bounds
on the estimated subspace, nor a principled method for determining d .
K
5 Using Offline Data to Speed Up Online Learning with
LOCAL-UCB
Here, we motivate and describe LOCAL-UCB, a natural algorithm that accelerates LinUCB
with offline data. The core idea is sharpening optimism by being optimistic over the
intersection of two confidence sets – one obtained using offline data and another purely from
online data.
To geometrically motivate our update rule, say t online interactions have been completed,
yielding a dataset D of (x ,a ,r ) tuples. From Theorem 2, SOLD gives an estimate and a
t s s s
confidence set for the projection matrix U U⊤. Let’s call the set C . For each projection
⋆ ⋆ U,off
UU⊤ ∈C , we have a d -dimensional confidence ellipsoid for β within the corresponding
U,off K
subspace. The union of all such d -dimensional confidence ellipsoids creates one confidence
K
set for β, coming from offline uncertainty. Let’s call it Ct(β,off). On the other hand, we also
have the purely online d -dimensional confidence ellipsoid Ct(β,on). We can thus sharpen
A
our optimism by being optimistic over the smaller intersection Ct(β,off)∩Ct(β,on) instead
of just over Ct(β,on) – thereby performing "more localized" optimism than LinUCB. This is
the idea behind Algorithm 2, which we thus call LOCAL-UCB. Figure 1 illustrates different
ways in which taking this intersection can affect optimism.
We formalize this intuition in Algorithm 2 by formulating the sharpened optimism as
an optimization problem in Line 4. The first constraint respresents the low dimensional
confidence ellipsoids in the subspace spanned by a given U, while the third constraint lets U
range over all of C . The second constraint merely represents the usual high dimensional
U,off
ellipsoid.
We provide the following guarantee for LOCAL-UCB. Notice that our guarantee shows
that for enough offline data with N ≫T, the effective dimension of the problem is d . It
K
increases to d as T gets closer to N. The quality of the offline data is reflected in the
A
coverage constants λ and λ .
D min
√
Theorem 3 (LOCAL-UCB Regret). Under Assumptions 1 and 2, if α = R µ +
1,t
(cid:112) √ (cid:112)
CR d log(2T/δ) and α =R µ+CR d log(2T/δ) for a suitable universal constant
K 2,t A
C, with probability at least 1−δ, LOCAL-UCB has regret
(cid:16) (cid:16) √ √ (cid:16) (cid:113) (cid:17)(cid:17)(cid:17)
Reg T =O min Rd A T,Rd K T 1+ λθ1 λ3
A
dT Kd NA .
However, the subspace constraint ∥Uˆ⊤U∥ ≥(cid:112) d −∆2 /2 is nonconvex. In fact, we lower
F K off
bound a convex function in the constraint. This essentially searches over a complicated
star-shapedsetinthespaceofallU. Assuch,LOCAL-UCBisunfortunatelycomputationally
inefficient.
8Algorithm 2 Latent Offline subspace Constraints for Accelerating Linear UCB (LOCAL-
UCB)
1: Input: Projection matrix UˆUˆ⊤ and confidence bound ∆ off from an offline uncertainty-
aware method, e.g. SOLD.
2: Initialize V 1 ←I dA, b 1 ←0, α t
3: for t=1,...T do
4: Play action a t and receive reward r t according to:
a t,β(cid:101)t,U(cid:101)t ←argmax a,β,Uϕ(x t,a)⊤β such that
βˆ ←U(UTV U)−1U⊤b , ∥U⊤(β−βˆ )∥ ≤α
1,t t t 1,t (U⊤VtU)−1 1,t
βˆ ←V−1b ,∥(β−βˆ )∥ ≤α
2,t t t 2,t V−1 2,t
t
∥β∥ ≤R,U⊤U=I ,UU⊤β =β, ∥Uˆ⊤U∥ ≥(cid:112) d −∆2 /2
2 dK F K off
5: Compute b t+1 ←b t+ϕ(x t,a)r t, V t+1 ←V t+ϕ(x t,a)ϕ(x t,a)⊤, α t+1
6: end for
Figure 1: Left: Geometric interpretations of LOCAL-UCB. Showing Ct(β,on)∩Ct(β,off) in
greenforthreetimepointst=t ,t ,t . Thedottedlinesdelineatethesubspaceconfidenceset.
1 2 3
Right: Geometric interpretation of ProBALL-UCB. Ct1(β,on)̸⊂C(cid:101)t1(β,off), so we continue
to use projections; but by time t 2, Ct2(β,on)⊂C(cid:101)t2(β,off), so we stop using projections.
96 More Practical Optimism with ProBALL-UCB
We note that while LOCAL-UCB is a natural algorithm with promising guarantees, it is not
computationallyefficient. WeaddressthisissuebyintroducingProBALL-UCB(Algorithm3),
a practical and computationally efficient algorithm for accelerating online learning in linear
latent bandits with offline data. We make three simplifications compared to LOCAL-UCB.
Namely, (1) this has cruder confidence sets, (2) we wait for one confidence set to become a
subsetoftheotherinsteadofconsideringcomplexintersections, and(3)weuseacomputable
proxy for the above condition instead of explicitly checking it. Surprisingly, this leads to
only a minor degradation in provable guarantees.
To geometrically motivate our update rule like in Section 5, we consider the d -dimensional
K
confidenceellipsoidCt(β,Uˆ)ofβ inside subspaceUˆ⊤. First,since∥UˆUˆ⊤−U U⊤∥ ≤∆ ,
⋆ ⋆ 2 off
we can show that a crude bound for ϕ(x,a)⊤β is max ϕ(x,a)⊤β +∆ . This
⋆ β∈Ct(β,Uˆ) off
corresponds to optimism over a crude confidence ball C(cid:101)t(β,off) of radius ∆
off
around
Ct(β,Uˆ). Second, instead of being optimistic over the intersection of this ball with our
usual d -dimensional confidence ellipsoid Ct(β,on), we merely wait for Ct(β,on) to be inside
A
C(cid:101)t(β,off). Finally, instead of actually checking whether √Ct(β,on) is in √side C(cid:101)t(β,off), we
use a proxy condition that roughly demands ∆ T > d T. Here, d T represents the
off A A
"cumulative size" of the ellipsoid across timesteps t, while ∆ T represents the "cumulative
off
size" of the ball. This is illustrated in Figure 1.
We note that the idea behind ProBALL-UCB is fairly general and can be used to design
adaptationsofotheralgorithmsaswell. Broadly,weprojecttoasubspace,performoptimism
there, and track the cumulative error for using this subspace. Once the cumulative error
of using the subspace (≈∆ T in our case) exceeds the cumulative error of not using the
√ off
subspace (≈d T), we stop using the subspace. Although ProBALL-UCB instantiates this
A
framework with LinUCB, the same idea can be immediately applied to other bonus-based
algorithms like SupLinUCB.
Asafinalnotebeforetheregretbound,thereisatechnicalchallengewithanalyzingProBALL-
UCB. Since βˆ lies in Uˆ but β might not, the d -dimensional confidence ellipsoid bound
1,t ⋆ K
no longer applies. We therefore prove our own confidence ellipsoid bound in Lemma 6, to
bypass this issue.
Theorem 4 (Regret for ProBALL-UCB). Let α =R√ µ+τ′R∆ κ +CR(cid:112) d log(T/δ)
1,t off t K
√ (cid:112)
and let α =R µ+CR d log(T/δ). Let S be the first timestep when Algorithm 3 does
2,t A
not play Line 6 and let S =T if no such timestep exists. For τ =τ′ =1 we have that
(cid:18) (cid:18) √ √ (cid:18) (cid:18)(cid:113) (cid:113) (cid:19)(cid:19)(cid:19)(cid:19)
Reg T =O(cid:101) min Rd A T,Rd K T 1+ λ3 A1 λθ dd KA NT + Sd NA (cid:80)S t=1κ2 t .
In the worst case, κ =O(t) and so 1 (cid:80)S κ2 =O(T2), but if all features ϕ(x ,a ) lie in
t S t=1 t t t
the span of Uˆ for t≤S, then 1 (cid:80)S κ2 =O(T).
S t=1 t
While the regret bound looks weaker in the worst case, we emphasize that the "good case"
in Theorem 4 is quite common. As an illustrative example, if the feature set F ={ϕ(x ,a)|
t t
a∈A} is an ℓ ball, then the maximization problem in Step 6 will always choose a with
2 t
10Algorithm 3 ProjectionandBonusesforAcceleratingLatentbanditLinearUCB(ProBALL-
UCB)
1: Input: ProjectionmatrixUˆUˆ⊤,confidencebound∆ off. Hyperparametersα 1,t,α 2,t,τ,τ′.
2: Initialize V 1 ←I, b 1 ←0, C t ←0
3: for t=1,...T do
√ (cid:113)
4: if ∆ offτ t+∆ offτ′ d K(cid:80)t s=1κ2 s/t≤d A then
5: Compute βˆ 1,t ←Uˆ(Uˆ⊤V tUˆ)−1Uˆ⊤b t
6: Play a t ←argmax aϕ(x t,a)⊤UˆUˆ⊤βˆ 1,t+α 1,t∥ϕ(x t,a)⊤Uˆ∥ (Uˆ⊤VtUˆ)−1
7: else
8: Compute βˆ 2,t ←V− t 1b t
9: Play a t ←argmax aϕ(x t,a)⊤βˆ 2,t+α 2,t∥ϕ(x t,a)∥ V−1
t
10: end if
11: Observerewardr t andupdateb t+1 ←b t+ϕ(x t,a)r t,V t+1 ←V t+ϕ(x t,a)ϕ(x t,a)⊤
12: Update C t+1 ←C t+Uˆ⊤ϕ(x t,a t)ϕ(x t,a t)⊤, κ t+1 ←∥C t+1∥ (Uˆ⊤Vt+1Uˆ)−1
13: end for
ϕ(x ,a ) in the span of Uˆ. This can also approximately hold if the features are roughly
t t
isotropic or close to the span of Uˆ.
7 Experiments
WenowestablishthepracticalefficacyofSOLD(Algorithm1)andProBALL-UCB(Algorithm
3)forlinearlatentcontextualbanditsthroughaseriesofnumericalexperiments. Weperform
a simulation study and a demonstration using real-life data.4 We obtain confidence bounds
∆ using three different concentration inequalities – (1) Hoeffding as in Proposition 2
off
(H-ProBALL), (2) empirical Bernstein as in Proposition 1 (E-ProBALL), and (3) confidence
intervals generated by the martingale Bernstein concentration inequalities of Waudby-Smith
and Ramdas [2023], Wang and Ramdas [2024] (M-ProBALL). We use a simpler expression
for ∆ , set τ′ =0, and vary the hyperparameter τ to adjust for overly conservative ∆ 5.
off off
Simulation study. We first perform a simulation study on a latent linear bandit with
d =50andd =2. WegenerateU withU i.i.d. Unif(0, 2.5 ). Wesimulatethehidden
A K ∗ ij dKdA
labels θ ∼N(0,d−1I ), generate feature vectors ϕ(x ,a )∼N(0,I ) normalized to
n K dK n,h n,h dA
unit norm, and sample noise ϵ i.i.d. N(0,0.52). We use SOLD to estimate Uˆ from the
n,h
offline dataset D , which consists of 5000 trajectories of length 20 each. In accordance with
off
theconfidencesetdeterminedbyLietal.[2010],wechooseα =0.33(cid:112) d log(1+10T/d )
1,t K K
4AllexperimentswererunonasinglecomputerwithanInteli9-13900kCPU,128GBofRAM,anda
NVIDIARTX3090GPU,innomorethananhourintotal.
5Namely,weset∆D =0in∆ off. Also,Lemma6andsomethoughtrevealthatchoosingτ′=0recovers
the"good"versionofProBALL-UCBguarantees,iffeaturesareisotropicenough.
11and α =0.33(cid:112) d log(1+10T/d ), and share the LinUCB and ProBALL-UCB hyperpa-
2,t A A
rameters by assigning α =α .
t 2,t
Figure 2: Comparison of ProBALL-UCB with LinUCB, for different choices of τ and
confidenceboundconstructions. AllvariantsperformnoworsethanLinUCB,withmartingale
Bernstein performing the best. The shaded area depicts 1-standard error confidence intervals
over 30 trials.
The results are presented in Figure 2. Note that ProBALL-UCB (Algorithm 3) performs no
worse than LinUCB, no matter what we choose for τ and ∆ . However, we see a distinct
off
benefit from using tighter confidence bounds – as ∆ gets smaller, Algorithm 3 chooses to
off
utilize the projected estimate βˆ more often, resulting in better performance. Note that
1,t
the kinks in the regret curves correspond to points where Algorithm 3 switches over to the
higher dimensional optimism in Line 9.
MovieLens dataset. In line with Hong et al. [2020], we assess the performance of our
algorithms on real data using the MovieLens dataset [Harper and Konstan, 2015]. This is a
large-scale movie recommendation dataset comprising 6040 users and 3883 movies, where
eachusermayrateoneormoremovies. LikeHongetal.[2020],wefilterthedatasettoinclude
only movies rated by at least 200 users and vice-versa. We factor the sparse rating matrix
into user parameters β and movie features Φ using the probabilistic matrix factorization
algorithm of Mnih and Salakhutdinov [2007a], using nuclear norm regularization so that the
rank of β is d =18. However, we consider a much higher dimensional problem than Hong
K
et al. [2020] do – we let d =200 so β ∈R1589×200,Φ∈R200×1426. At each round for user i,
A
the agent chooses between 20 movies of different genres with features Φ ,...,Φ , and has
a1 a20
to recommend the best movie presented to it to maximize the user’s rating of the movie. We
generate rewards for recommending movie j to user i by βTΦ +ϵ ,ϵ i.i.d. N(0,0.5).
i j ij ij
Ourhyperparametersarechosenandvariedjustasinthesimulationstudy. Toreproducethe
methods of Hong et al. [2020], we cluster the user features into d clusters using k-means,
K
and provide mUCB and mmUCB with the mean vectors of each cluster as latent models. We
initialize ProBALL-UCB with a subspace estimated with an unregularized variant of SOLD,
that uses pseudo-inverses instead of inverses, because of difficulties in finding an appropriate
regularization parameter for this large, noisy, and high-dimensional dataset. The subspace
was estimated from 5000 trajectories of length 50 simulated from the reward model and the
uniform behavior policy. Note that we assign ∆ for ϵ in mmUCB, as this is their tolerance
off
parameter for model misspecification.
12Figure 3: Comparison of ProBALL-UCB initialized with SOLD against LinUCB, mUCB,
and mmUCB, for different choices of τ and confidence bound constructions. All variants
of ProBALL-UCB perform no worse than LinUCB, and outperform mUCB and mmUCB.
Shaded area depicts 1-standard error confidence intervals over 30 trials with fresh θ. The
confidence intervals on regret thus account for the variation in frequentist regret for changing
θ.
Figure 3 depicts the result of this experiment. Once again, ProBALL-UCB performs no
worse than LinUCB, no matter what we choose for τ and ∆ , and the benefit of using
off
tighter confidence bounds remains. With τ =0.5, ProBALL-UCB with martingale Bernstein
confidence bands stops using the projected estimates at around timestep 70, but still
continues to outperform LinUCB. Although mUCB and mmUCB perform slightly better
than ProBALL-UCB and LinUCB at the beginning, the model misspecification incurred by
discretizing the features into d clusters ensures that it typically suffers linear regret in this
K
scenario. The lower initial performance of ProBALL-UCB and Lin-UCB is a consequence of
their higher initial exploration.
8 Discussion, Limitations and Further Work
In this paper, we have addressed the problem of leveraging offline data to accelerate online
learninginlinearlatentbandits. Ourworkhasafewlimitations. First,whileProBALL-UCB
is practical and computationally efficient, it has a slightly weaker worse-case guarantee than
the less practical LOCAL-UCB. Second, when the size N of the offline dataset is small, it
can be hard to tune the regularization µ. We use a pseudoinverse-based version of SOLD in
such a case, implemented in our code. Third, the offline uncertainty sets computed using
∆ can be overly conservative, and a discount hyperparameter τ must be fine-tuned online.
off
Despite these limitations, our work enjoys strong theoretical guarantees and convincing
empirical performance. We hope that this method opens the door for developing other
efficient and scalable algorithms for sequential decision-making with continuous latent states.
One can use ideas presented in this paper to design similar algorithms for MDPs, linear
MDPs as well as under general function approximation for both MDPs and bandits. It is
also possible that our regret bounds can be sharpened using feature-space partition ideas
from hybrid RL [Li et al., 2023, Tan and Xu, 2024]. Finally, it will be of interest to obtain
13minimax guarantees for leveraging offline data to speed up online learning in latent and
linear latent contextual bandits.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear
stochastic bandits. In Advances in Neural Information Processing Systems, pages 2312–
2320, 2011.
Yasin Abbasi-yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear
stochastic bandits. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q.
Weinberger, editors, Advances in Neural Information Processing Systems, volume 24.
Curran Associates, Inc., 2011. URL https://proceedings.neurips.cc/paper_files/
paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf.
YasinAbbasi-Yadkori,DavidPal,andCsabaSzepesvari. Online-to-confidence-setconversions
and application to sparse stochastic bandits. In Artificial Intelligence and Statistics, pages
1–9, 2012.
Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari, and Tengyang Xie. Harnessing
density ratios for online reinforcement learning, 2024.
Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement
learning with offline data. In International Conference on Machine Learning, pages
1577–1594. PMLR, 2023.
Eric Brochu, Matthew W. Hoffman, and Nando de Freitas. Portfolio allocation for bayesian
optimization, 2011.
Yanxi Chen and H. Vincent Poor. Learning mixtures of linear dynamical systems. CoRR,
abs/2201.11211, 2022. URL https://arxiv.org/abs/2201.11211.
P. Diaconis and D. Freedman. De Finetti’s Theorem for Markov Chains. The Annals of
Probability, 8(1):115 – 130, 1980. doi: 10.1214/aop/1176994828. URL https://doi.org/
10.1214/aop/1176994828.
F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context.
ACM Trans. Interact. Intell. Syst., 5(4), dec 2015. ISSN 2160-6455. doi: 10.1145/2827872.
URL https://doi.org/10.1145/2827872.
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot,
Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from
demonstrations. InProceedingsoftheAAAIconferenceonartificialintelligence,volume32,
2018.
JoeyHong,BranislavKveton,ManzilZaheer,YinlamChow,AmrAhmed,andCraigBoutilier.
Latent bandits revisited, 2020.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire.
Contextual decision processes with low bellman rank are pac-learnable, 2016.
14Soummya Kar, H. Vincent Poor, and Shuguang Cui. Bandit problems in networks: Asymp-
totically efficient distributed allocation rules. In 2011 50th IEEE Conference on De-
cision and Control and European Control Conference, pages 1771–1778, 2011. doi:
10.1109/CDC.2011.6160719.
Chinmaya Kausik, Kevin Tan, and Ambuj Tewari. Learning mixtures of markov chains and
mdps, 2023.
AchimKlenke. Probabilitytheory: Acomprehensivecourse. InUniversitext, Springer Cham.,
2008. URL https://api.semanticscholar.org/CorpusID:117791811.
Weihao Kong, Raghav Somani, Zhao Song, Sham M. Kakade, and Sewoong Oh. Meta-
learning for mixed linear regression. CoRR, abs/2002.08936, 2020. URL https://arxiv.
org/abs/2002.08936.
Gen Li, Wenhao Zhan, Jason D Lee, Yuejie Chi, and Yuxin Chen. Reward-agnostic fine-
tuning: Provable statistical benefits of hybrid reinforcement learning. arXiv preprint
arXiv:2305.10282, 2023.
Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach
to personalized news article recommendation. In Proceedings of the 19th international
conferenceonWorldwideweb,WWW’10.ACM,April2010.doi: 10.1145/1772690.1772758.
URL http://dx.doi.org/10.1145/1772690.1772758.
Yueyang Liu, Xu Kuang, and Benjamin Van Roy. A definition of non-stationary bandits,
2023.
Yangyi Lu, Ziping Xu, and Ambuj Tewari. Bandit algorithms for precision medicine, 2021.
Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. In J. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing
Systems,volume20.CurranAssociates,Inc.,2007a.URLhttps://proceedings.neurips.
cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf.
Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. In J. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing
Systems,volume20.CurranAssociates,Inc.,2007b.URLhttps://proceedings.neurips.
cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf.
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel.
Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE
international conference on robotics and automation (ICRA), pages 6292–6299. IEEE,
2018.
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn,
Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient
online fine-tuning, 2023.
15William H. Press. Bandit solutions provide unified ethical models for randomized clinical
trials and comparative effectiveness research. Proceedings of the National Academy of
Sciences, 106(52):22387–22392, 2009. doi: 10.1073/pnas.0912378106. URL https://www.
pnas.org/doi/abs/10.1073/pnas.0912378106.
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman,
Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with
deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.
Juntao Ren, Gokul Swamy, Zhiwei Steven Wu, J. Andrew Bagnell, and Sanjiban Choudhury.
Hybrid inverse reinforcement learning, 2024.
Yuda Song, Yifei Zhou, Ayush Sekhari, J. Andrew Bagnell, Akshay Krishnamurthy, and
Wen Sun. Hybrid rl: Using both offline and online data can make rl efficient, 2023.
KevinTanandZipingXu. Anaturalextensiontoonlinealgorithmsforhybridrlwithlimited
coverage, 2024.
Guy Tennenholtz, Uri Shalit, Shie Mannor, and Yonathan Efroni. Bandits with partially
observable confounded data. In Cassio de Campos and Marloes H. Maathuis, editors,
Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence,
volume 161 of Proceedings of Machine Learning Research, pages 430–439. PMLR, 27–30
Jul 2021. URL https://proceedings.mlr.press/v161/tennenholtz21a.html.
Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. J.
Comput. Syst. Sci, 68:2004, 2004.
Andrew Wagenmaker and Aldo Pacchiano. Leveraging offline data in online reinforcement
learning, 2023.
HongjianWangandAadityaRamdas. Positivesemidefinitesupermartingalesandrandomized
matrix concentration inequalities, 2024.
Ian Waudby-Smith and Aaditya Ramdas. Estimating means of bounded random variables
by betting. Journal of the Royal Statistical Society Series B: Statistical Methodology, 86
(1):1–27, 02 2023. ISSN 1369-7412. doi: 10.1093/jrsssb/qkad009. URL https://doi.org/
10.1093/jrsssb/qkad009.
YiminWei, YanhuaCao, andHuaXiang. Anoteonthecomponentwiseperturbationbounds
of matrix inverse and linear systems. Applied Mathematics and Computation, 169(2):
1221–1236, 2005. ISSN 0096-3003. doi: https://doi.org/10.1016/j.amc.2004.10.065. URL
https://www.sciencedirect.com/science/article/pii/S0096300304008471.
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning:
Bridging sample-efficient offline and online reinforcement learning, 2022.
Yi Yu, Tengyao Wang, and Richard J. Samworth. A useful variant of the davis–kahan
theorem for statisticians, 2014.
16A Additional Lemmas and Discussion
A.1 Broader Impacts
Often, protected groups with private identities have temporal behavior correlated with their
privateidentities. Latentstateestimationmethodshavethepotentialtoidentifysuchprivate
identities online, and must be used with care. Use of such methods should comply with the
relevant privacy and data protection acts, and corporations with access to a large customer
base as well as governments should be mindful of the impact of the use of latent state
methods on their customers and citizens respectively.
A.2 Coherence with Equality is Necessary
Lemma 2 (Coherence with Equality is Necessary). There exists a stateless decision process
that is exchangeable and not coherent but satisfies the following property: for any two action
sequences τ,τ′ of lengths H and H′ sharing the same actions (a ,...a ) from index h to
h k
k, with F (τ)=(Y ,...Y ) and F (τ′)=(Y′,...Y′ ), we have (Y ,...Y )∼(Y′,...Y′).
H 1 H H′ 1 H′ h k h k
Additionally, since the SDP is not coherent, it is not a latent bandit.
Proof. Consider an SDP with action set A = {0,1} equipped with a random variable
θ ∈{0,1} distributed as Ber(1/2). If the first action is 0, then all rewards are given by θ. If
the first action is 1, then the first reward X =θ while all future rewards are 1−θ.
1
Notice now that for any timestep h, its rewards are Ber(1/2). We note that this process is
clearlyexchangeable. Moreover,thisprocessiscoherentinaweakersense–foranytwoaction
sequences τ,τ′ of lengths H and H′ sharing the same actions (a ,...a ) from index h to
h k
k, with F (τ)=(Y ,...Y ) and F (τ′)=(Y′,...Y′ ), we have (Y ,...Y )∼(Y′,...Y′).
H 1 H H′ 1 H′ h k h k
That is, the reward subsequences are only equal in distribution. This is easy to see, since if
a is not included in the action subsequence or if a =0, the reward subsequence is always
1 1
given by (X,...X) for X ∼Ber(1/2). If a =1 and a is in the action subsequence, then
1 1
the reward subsequence is always given by (−X,X,...X) for X ∼Ber(1/2).
However, the reward subsequences are not equal in value. If the action subsequence is inside
two action sequences τ = (a ,...a ) and τ′ = (a′,...a′ ) with a = 0 and a′ = 1, then
1 H 1 H 1 1
their rewards are negatives of each other. So, this SDP is not coherent. This means that
this SDP is not a latent bandit.
A.3 Contexts, Latent State and Behavior Policy cannot be Depen-
dent
Lemma 1 (Contexts, θ, and π cannot be dependent). For each of these conditions:
b
1. Contexts in a trajectory are dependent but do not depend on θ, and π also does not use
b
θ,
2. Contexts are generated independently using θ, while π does not use θ,
b
3. Contexts are generated independently without using θ, while π uses θ,
b
17there exists a pair of linear latent contextual bandits with orthogonal latent subspaces sat-
isfying the condition, and a behavior policy π so that the offline data distributions are
b
indistinguishable and cover all (x,a) pairs with probability at least 1/4. Since the latent
subspaces are orthogonal, an action that gives the maximum reward on one latent bandit
gives reward 0 on the other.
Proof. Let e ,e be the standard basis of R2. For all these examples, our two latent bandits
1 2
will satisfy the following:
• Latent contextual bandit 1: Letθ takevaluese +e and−e −e withprobability
1 2 1 2
1/2 each.
• Latent contextual bandit 2: Let θ take values e −e and e −e with probability
1 2 2 1
1/2 each.
A.3.1 Contexts cannot be dependent on each other
We consider two actions, so A = {0,1} We have two contexts x,y so that ϕ(x,0) = e ,
1
ϕ(x,1)=2e while ϕ(y,0)=e , ϕ(y,1)=2e . We design them to be dependent so that any
1 2 2
trajectory either only sees x or only sees y. However, x and y are both seen with probability
1/2. Pick π so that it takes each action with probability 1/2. Now note that the mean
b
rewardofx,0ineitherlatentbandittakesvalues±1withprobability1/2. So,theofflinedata
distributions are also indistinguishable and every context-action pair is seen with probability
at least 1/4.
A.3.2 Contexts and latent state cannot be dependent
We consider two actions, so A={0,1} Again, consider two contexts x,y so that ϕ(x,0)=e ,
1
ϕ(x,1) = 2e while ϕ(y,0) = e , ϕ(y,1) = 2e . Let us say that for either latent bandit
1 2 2
and for any θ, the context distribution is a Dirac-δ over the context whose features have a
positive dot product with θ. Then, note that either context is seen in both latent bandits
with probability 1/2. Again, let π take either action with probability 1/2. So, the offline
b
datadistributionsareindistinguishableandeverycontext-actionpairisseenwithprobability
at least 1/4.
A.3.3 Latent state and behavior policy cannot be dependent
We consider four actions, so that A = {0,1,2,3}. Let there be a single context x with
ϕ(x,0) = e ,ϕ(x,1) = e ,ϕ(x,2) = −e ,ϕ(x,3) = −e . For any latent state θ in either
1 2 1 2
bandit, let π be uniform over the actions that have a positive dot product with θ. It is then
b
easy to see that the offline data distributions are indistinguishable and every context-action
pair is seen with probability at least 1/4.
18B A de Finetti Theorem for decision processes
B.1 Discussion on Liu et al. [2023]’s smaller class of generalized
bandits
WeprovethedeFinettitheoremforaverygeneralformulationofdecisionprocesses. However,
pastwork[Liuetal.,2023]hasstudiedasimplergeneralizationofbandits,namelyastochastic
process valued in RA. A sample point in this space is a sequence of functions in A → R,
which rules out the possibility of adaptivity. In contrast, a sample point in our space is
a function of sequences, which subsumes all sample points of their space, but allows for
adaptivity.
Liu et al. [2023] are able to use the original de Finetti theorem on their random process
directly, but work with a much more restrictive kind of decision process. We show that
even when considering much more general stateless decision processes, latent bandits are the
"right" objects produced by a de Finetti theorem for stateless decision processes.
B.2 Proof of the De Finetti Theorem for Stateless Decision Pro-
cesses
A note on F, measurability and well-definedness in the definition of a latent
bandit. Recall that F in the definition of a latent bandit needs to be a measurable map
Ω→(A→P(R)). To define measurability for the output space of functions (A→P(R)),
we endow P(R) with the topology of weak convergence, endow the space P(R)A of maps
A→P(R)withthetopologyofpoint-wiseconvergence,andrequireF tobemeasurablew.r.t.
theinducedBorelσ-algebra. Wealsorecalltwoabusesofnotationinthedefinitionofalatent
bandit. First, we abuse notation to define the random measure F(a) := (ω (cid:55)→ F(ω)(a)).
Second,wealsoabusenotationtoconflateF(a)withthecurriedmapκ (ω,B):=F(a)(ω)(B),
a
which is a map Ω×B →[0,1]. This map κ will turn out to be a kernel by the construction
a
of F. Equating F(a) to a regular conditional distribution in the definition of a latent bandit
requires that κ be a kernel.
a
We recall our de Finetti theorem for stateless decision processes here.
Theorem 1 (De Finetti Theorem for Stateless Decision Processes). Every exchangeable and
coherent stateless decision process is a latent bandit.
Proof. Consider an exchangeable and coherent stateless decision process. We will establish
that there is a latent bandit with the same reward distribution as this process for any
sequence of actions.
For any sequence τ = (a ,...a ), denote by (Y ,...Y ) := F (a ,...a ). We intend
1 H τ,1 τ,H H 1 H
to establish that there is a random measure-valued function F such that (Y ,...Y ) are
τ,1 τ,H
independent given F and L[Y |F]=F(a ) for all h≤H almost surely. Since conditional
τ,h h
independence is a property of finite subsets of a set of random variables, it suffices to show
this for finite H. The version for H =∞ will immediately follow by coherence.
First recall that regular conditional distributions L[Y |F] are almost surely unique under if
the σ-algebra of the output space of Y is countably generated. Since the Borel σ-algebra on
19R is countably generated, this is true for our case. We also recall for the rusty reader that
conditioning on a random variable is the same as conditioning on the induced σ-algebra in
the domain.
B.3 Constructing F
Fix any finite trajectory τ =(a ,...a ) and index h.
1 H
The trick: Consider the infinite sequence τ :=(a ,a ,...). By coherence, Y =Y .
∞ h h τ,h τ∞,h
Now τ is a sequence where exchanging any finite set of rewards preserves the reward
∞
distribution, since all actions are identical. Since R is locally compact, we can apply the
usual de Finetti representation theorem (Theorem 12.26 in Klenke [2008]) to conclude that:
• The random measure Ξ =wlim 1 (cid:80)n δ is well defined. Here wlim is the
weak limit of
measures.ah n→∞ n j=1 Yτ∞,j
• The regular conditional distribution L[Y |Ξ ]=Ξ for all j.
τ∞,j ah ah
Since Y =Y , we conclude that the conditional distribution L[Y |Ξ ]=L[Y |
τ,h τ∞,h τ,h ah τ∞,h
Ξ ]=Ξ .
ah ah
Constructing F: For every action, consider such a random measure Ξ and define a
ah
random measure-valued function F : Ω → (A → P(R)) in the following manner: for any
ω ∈ Ω, define F(ω)(a) := Ξ (ω). We know that for any a, Ξ is measurable w.r.t. the
ah ah
topologyofweakconvergenceonP(R). ItisnowtediousbutstraightforwardtoverifythatF
is measurable w.r.t. the Borel σ-algebra generated by the topology of pointwise convergence
on P(R)A.
B.4 Establishing that L[Y | F] = F(a ) almost surely for all h
τ,h h
Again, fix any finite trajectory τ =(a ,...a ) and index h. Recall that we abuse notation
1 H
to denote by F(a) the random-measure ω (cid:55)→F(ω)(a). In particular, for any measurable set
B ⊂R, we conflate F(a)(ω)(B)=F(ω)(a)(B)=F(a)(ω,B), where the last equality holds
since F is a regular conditional distribution. Note that F(a ) = Ξ by the construction
h ah
of F. Since Ξ =F(a ), we have L[Y |F(a )]=F(a ). Thus, it suffices to show that
ah h τ,h h h
L[Y |F]=L[Y |F(a )].
τ,h τ,h h
Lemma 3. L[Y |F]=L[Y |F(a )] almost surely.
τ,h τ,h h
Proof. First note that F(a ) is measurable w.r.t. F. For showing this, view the set of maps
h
P(R)A as the product set (cid:81) P(R) . Now merely note that F−1(E ×(cid:81) P(R) ) =
a′ a′ a′̸=a a′
(F(a ))−1(E) for any measurable subset E ⊂P(R) . Hence, F(a ) is measurable w.r.t. F.
h a h
LetB betheBorelσ-algebraonR. NowrecallthattheregularconditionaldistributionL[X |
G] for a real-valued random variable X is the almost surely unique kernel κ :Ω×B →R
G,X
such that:
• ω →κ (ω,B) is G-measurable for any set B ∈B
G,X
• B →κ (ω,B) is a probability measure on R for any sample point ω ∈Ω.
G,X
20• For any measurable set B ⊂R and any G-measurable set A,
(cid:90) (cid:90)
E[1 (Y)1 ]= 1 (Y(ω))1 (ω)dP(ω)= κ (ω,B)1 (ω)dP(ω)
B A B A G,X A
We will show our claim using the definition and a.s. uniqueness of the regular conditional
distribution in our case. Consider any F-measurable set A⊂Ω and Borel-measurable set
B ⊂ R, B ∈ B. Denote by κ := L[Y | F] and by κ := L[Y | F(a )] = F(a). Note
F τ,h ah τ,h h
that by the coherence and exchangeability in section B.3,
(cid:90)
κ (ω,B)1 (ω)dP(ω)=E[1 (Y )1 ]=E[1 (Y )1 ]=E[1 (Y )1 ]
F A B τ,h A B τ∞,h A B τ∞,j A
for all j. Averaging all these equations and taking a limit, we get that
(cid:90)
κ (ω,B)1 (ω)dP(ω)=E[1 (Y )1 ]=E[1 (Y )1 ]
F A B τ,h A B τ∞,h A
n
1 (cid:88)
= lim E[1 (Y )1 ]
n→∞n B τ∞,j A
j=1
  
n
1 (cid:88)
=E nl →im
∞n
1 B(Y τ∞,j)1 A
j=1
where the last equality holds by the dominated convergence theorem, if the limit exists.
To establish that the limit exists and compute it, we apply the usual de Finetti theorem –
specificaly point (i) of remark 12.27 in Klenke [2008] with f set to the identity map. This
gives us that lim 1 (cid:80)n 1 (Y )=E[1 (Y )|Ξ ]=Ξ (B), where Ξ (B) is
n→∞ n j=1 B τ∞,j B τ∞,h ah ah ah
the random variable ω (cid:55)→ Ξ (ω)(B). This in turn satisfies Ξ (ω)(B) = F(a )(ω)(B) =
ah ah h
κ (ω,B). This establishes that for any F-measurable set A and Borel set B,
ah
(cid:90) (cid:90)
κ (ω,B)1 (ω)dP(ω)= κ (ω,B)1 (ω)dP(ω)
F A ah A
. In conclusion, κ satisfies:
ah
• F(a )(B) := ω → κ (ω,B) is F-measurable for any set B ∈ B since F(a )(B) is
h ah h
F(a ) measurable by definition of κ and F(a ) is F-measurable from above.
h ah h
• B →κ (ω,B) is a probability measure on R for any sample point ω ∈Ω by definition
ah
of κ
ah
• For any measurable set B ⊂R and any F-measurable set A, by the argument above,
(cid:90) (cid:90)
E[1 (Y )1 ]= κ (ω,B)1 (ω)dP(ω)= κ (ω,B)1 (ω)dP(ω)
B τ,h A F A ah A
By the definition as well as a.s. uniqueness of regular conditional distributions in our case,
this establishes that L[Y |F]=κ =κ =E[E[Y |F(a )] almost surely.
τ,h F ah τ,h h
21B.5 Establishing conditional independence of rewards
This is the trickier to establish. It suffices to show that for any finite length h trajectory τ =
(cid:104) (cid:105)
(a ,...a )andanytuple(f ,...f )ofboundedmeasurablefunctions,E (cid:81)H f (Y )|F =
1 H 1 H h=1 h τ,h
(cid:81)H E[f (Y )|F]. Equivalently, it suffices to show that for any bounded F-measurable
h=1 h τ,h
real-valued random variable U, the following holds.
(cid:34) H (cid:35) (cid:34) H (cid:35)
(cid:89) (cid:89)
E U f (Y ) =E U E[f (Y )|F]
h τ,h h τ,h
h=1 h=1
We will show this by induction on H. This clearly holds for H = 1 by section B.3 above.
Now assume that this holds for H =l−1.
Replacing the last term, f (Y ), by an empirical average: Fix any trajectory
l τ,l
τ = (a ,...a ). First consider the infinite trajectory τ′ = (a ,...a ,a ,a ,a ,...). This
1 l 1 l−1 l l l
creates a random sequence of rewards Y ,Y ,.... Define τ′ by switching indices l and
τ′,1 τ′,2 j
l+j in τ′ and considering the first l actions. In particular, τ′ gives the sequence of rewards
j
(Y ,...Y ,Y ).
τ′,1 τ′,l−1 τ′,l+j
First note that τ′ =τ. By coherence, (Y ,...Y )=(Y ,...Y ). By exchangeability
0 τ,1 τ,l τ′,1 τ′,l
and coherence, (Y ,...Y ,Y )∼(Y ,...Y ,Y ). This means that for any
τ′,1 τ′,l−1 τ′,l τ′,1 τ′,l−1 τ′,l+j
j ≥0,
(cid:34) l (cid:35) (cid:34) (cid:32)l−1 (cid:33) (cid:35)
(cid:89) (cid:89)
E U f (Y ) =E U f (Y ) f (Y )
h τ,h h τ′,h l τ′,l+j
h=1 h=1
We can then consider the average over all these equations for j =0→n−1 and get that for
all n≥1,
(cid:34) l (cid:35)  (cid:32)l−1 (cid:33) n−1 
(cid:89) (cid:89) 1 (cid:88)
E U f h(Y τ,h) =EU f h(Y τ′,h) 
n
f l(Y τ′,l+j)
h=1 h=1 j=0
Taking limits and using the dominated convergence theorem, we get that
(cid:34) l (cid:35)  (cid:32)l−1 (cid:33) n−1 
(cid:89) (cid:89) 1 (cid:88)
E U f h(Y τ,h) =EU f h(Y τ′,h)  nl →im
∞n
f l(Y τ′,l+j) (1)
h=1 h=1 j=0
if the limit on the right side exists.
Showing that the empirical average is the conditional expectation: Again, consider
a different infinite trajectory τ =(a ,a ,...). Again, by coherence, Y =Y for all
l l l τl,l+j τ′,l+j
j ≥0. From the usual de Finetti theorem, specifically point (i) of remark 12.27 in Klenke
[2008],wehavethat 1 (cid:80)m f (Y )→E[f (Y )|F]. Wecanthenobservethatbygeneral
m j=1 l τl,j l τl,l
properties of convergence, the following holds.
m m−1
1 (cid:88) 1 (cid:88)
E[f (Y )|F]= lim f (Y )= lim f (Y )
l τl,l m→∞m l τl,j m→∞m−l l τl,j
j=1 j=l
22m−1 n−1
1 (cid:88) 1 (cid:88)
= lim f (Y )= lim f (Y )
m→∞m−l l τ′,j n→∞n l τ′,l+j
j=l j=0
We can combine this with equation 1 to get that
(cid:34) l (cid:35) (cid:34) (cid:32)l−1 (cid:33) (cid:35)
(cid:89) (cid:89)
E U f (Y ) =E U f (Y ) E[f (Y )|F]
h τ,h h τ′,h l τl,l
h=1 h=1
SincebothU andE[f (Y )|F]arenowF measurable,wecanapplytheinductionhypothesis
l τl,l
to τ′ truncated at l−1 and conclude that
(cid:34) l (cid:35) (cid:34) l−1 (cid:35) (cid:34) (cid:32) l (cid:33)(cid:35)
(cid:89) (cid:89) (cid:89)
E U f (Y ) =E UE[f (Y )|F] f (Y ) =E U E[f (Y )|F]
h τ,h l τl,l h τ,h h τ′,h
h=1 h=1 h=1
Thus, the induction step holds and the claim holds for all finite H. We discussed at the
beginning of the section how this implies conditional independence for H =∞ as well.
B.6 A de Finetti theorem for TACDPs
Definition 7. A TACDP is said to be coherent if for any h ≤ k ≤ H,H′ ∈ N∪{∞}
and for any two context-action sequences τ,τ′ of lengths H and H′ sharing the same
context-action pairs ((x ,a ),...(x ,a )) from index h to k, with F (τ)=(Y ,...Y ) and
h h k k H 1 H
F (τ′)=(Y′,...Y′ ), we have (Y ,...Y )=(Y′,...Y′), viewed as functions of Ω.
H′ 1 H′ h k h k
Definition8. Astatelessdecisionprocessissaidtobeexchangeableifforanypermutationπ :
[H]→[H]andF ((x ,a ),...(x ,a ))=(Y ,...Y ),wehaveF ((x ,a ),...(x ,a ))∼
H 1 1 H H 1 H h π(1) π(1) π(H) π(H)
(Y ,...Y ).
π(1) π(H)
Definition 9. A latent contextual bandit is a stateless decision process equipped with a
random measure-valued function F :Ω→((X ×A)→P(R)) so that for any H and context-
actionsequence((x ,a ),...(x ,a )),therewards(Y ,...Y ):=F ((x ,a ),...(x ,a ))
1 1 H H 1 H H 1 1 H H
are independent conditioned on F. Moreover, the conditional distribution L[Y | F] =
h
F((x ,a )) for all h≤H.6
h h
Theorem 5 (De Finetti Theorem for Stateless Decision Processes). Every exchangeable and
coherent TACDP is a latent contextual bandit.
Proof. The proof is verbatim the same as that for Theorem 1 after merely replacing A with
X ×A and a with (x,a).
6We abuse notation twice here. First, we write F((x ,a )):=(ω (cid:55)→F(ω)((x ,a ))). Second, as the
h h h h
regularconditionaldistributionL[Y |F]isakernelthatmapsfromΩ×B→R,weviewF((x ,a ))asits
h h h
curriedmap(ω,B)(cid:55)→F((x ,a ))(ω)(B). Adiscussionofissueslikemeasurabilityandwell-definednessisin
h h
AppendixB.2.
23C Proofs for SOLD
Recall that µ :=E[θ] and define µ :=E[β]=U µ . Also recall that Λ:=E[θ θ⊤]. For a
θ β ⋆ θ n n
trajectorywithindexn,denotebyβ :=U θ . DenotebyX :=[ϕ(x ,a ),...ϕ(x ,a )]⊤.
n ⋆ n n 1 1 H H
Denote by η := [ϵ ,ϵ ...ϵ ]⊤ the vector of subgaussian noises with subgaussian
n n,1 n,2 n,H
parameter σ2. Since rewards are bounded by R, we know that σ2 ≤R2. Denote by X and
n,i
η theactionmatrixandnoisevectorcorrespondingtothetrajectoryhalvesτ fori=1,2.
n,i n,i
Since rewards are bounded by R, ϵ is subgaussian for all h and so η is σ2-subgaussian
n,h n,i
for i=1,2. Also note that
βˆ =(µI+X⊤ X )−1X⊤ r
n,i n,i n,i n,i n,i
=(µI+X⊤ X )−1X⊤ (X β +η )
n,i n,i n,i n,i n n,i
=(I−µ(µI+X⊤ X )−1)β +(µI+X⊤ X )−1X⊤ η
n,i n,i n n,i n,i n,i n,i
=(I−µ(µI+X⊤ X )−1)β +(µI+X⊤ X )−1X⊤ η
n,i n,i n n,i n,i n,i n,i
Note that βˆ and βˆ are identically distributed. Now recall that
n,1 n,2
1
M = (βˆ βˆ⊤ +βˆ βˆ⊤ )
n 2 n,1 n,2 n,2 n,1
are i.i.d. random matrices. Denote by D := (I −µ(µI +X⊤ X )−1) and denote by
n,i n,i n,i
u :=(µI+X⊤ X )−1X⊤ η .
i n,i n,i n,i n,i
Lemma 4. If the per-reward noise is σ2-subgaussian, then the following inequalities hold
(cid:18) (cid:19)
H
∥M ∥ ≤R2 2+
n 2 2µ
σ4(d +1)
∥E[M2]∥ ≤R4+ A
n 2 8µ2
Proof. We prove various bounds and assemble them.
Bounding ∥(µI+X⊤ X )−1X⊤ ∥ : Consider any X with SVD X =U⊤ΣV. This means
n,i n,i n,i 2
that
∥(µI+X⊤X)−1X⊤∥ =V⊤(Σ2+µ)−1ΣU
2
=∥V⊤(Σ2+µ)−1Σ∥ =∥(Σ2+µ)−1Σ∥
2 2
a
≤max
a a2+µ
1
= √
2 µ
We can now apply this to X =X
n,i
and conclude that ∥(µI+X n⊤ ,iX n,i)−1X n⊤ ,i∥
2
≤ 2√1
µ
√
u
i
are independent, 4σ µ2-subgaussian and σ 2√H µ-bounded: We claim that u
i
are inde-
√
pendent, 4σ µ2-subgaussian and ∥u i∥2
2
≤ σ 2√H µ. Recall that η
n,i
are σ2-subgaussian vectors
24and ∥(µI +X n⊤ ,iX n,i)−1X n⊤ ,i∥
2
≤ 2√1 µ. So, we have that u
i
= (µI +X n⊤ ,iX n,i)−1X n⊤ ,iη
n,i
is σ2-subgaussian. Also recall that u and u are independent since both contexts and
4µ 1 2
reward-noise are generated independently at each timestep. Finally, since |ϵ |≤R, we also
n,h
have that ∥η n,i∥2
2
≤R2H, so ∥u i∥2
2
≤ R 42 µH since ∥(µI+X n⊤ ,iX n,i)−1X n⊤ ,i∥
2
≤ 2√1 µ.
√
Bounding ∥M n∥ 2: Note that ∥D n,i∥
2
≤1, ∥β 2∥
n
≤R and ∥u i∥
2
≤ σ 2√H µ. So, we have that
(cid:32) √ (cid:33)2 (cid:18) (cid:19)
R H H
∥M ∥ ≤ R+ √ ≤R2 2+
n 2 2 µ 2µ
Bounding ∥E[M2]∥ : We compute that
n 2
1(cid:16)
E[M2]= E[D β β⊤D ]E[β⊤D2 β ]+E[D β β⊤D ]E[β⊤D2 β ]
n 4 n,1 n n n,1 n n,2 n n,2 n n n,2 n n,1 n
+E[(β⊤D D β )D β β⊤D ]]+E[(β⊤D D β )D β β⊤D ]]
n n,2 n,1 n n,1 n n n,2 n n,1 n,2 n n,2 n n n,1
(cid:17)
E[∥u ∥2]E[u u⊤]+E[∥u ∥2]E[u u⊤]+E[u u⊤u u⊤]+E[u u⊤u u⊤]
1 2 2 2 2 2 1 1 1 2 1 2 2 1 2 1
Now since ∥D ∥ ≤1 and ∥β ∥ ≤R, the norm of the first four terms is bounded by R4.
n,i 2 n 2
Now note that
σ2
∥E[u u⊤]∥ = max E[v⊤u u⊤v]= max E[(u⊤v)2]≤
i i 2 v,∥v∥2≤1 i i v,∥v∥2≤1 i 2µ
σ2d
∥E[∥u ∥2]∥ =Tr(E[u u⊤])≤∥E[u u⊤]∥ d ≤ A
i 2 2 i i i i 2 A 2µ
∥E[u u⊤u u⊤]∥ =∥E[u u⊤u u⊤]∥ = max E[v⊤u u⊤u u⊤v]
2 1 2 1 2 1 2 1 2 2 1 2 1 2
v,∥v∥2≤1
σ4
max E[v⊤u u⊤u u⊤v]= max Tr(v⊤E[u u⊤]E[u u⊤]v]=∥E[u u⊤]E[u u⊤]∥ ≤
v,∥v∥2≤1 1 2 1 2 v,∥v∥2≤1 1 1 2 2 1 1 2 2 2 4µ2
Combining all of these, we get that
σ4(d +1)
∥E[M2]∥ ≤R4+ A
n 2 8µ2
Proposition 1 (Confidence Bound for M ). With probability at least 1−δ/2, we have that
N
∥M −E[M ]∥ ≤∆ with
N 1 2 M
(cid:118)
∆
:=(cid:117) (cid:117) (cid:116)2(cid:13) (cid:13) (cid:13)(cid:88)N M2(cid:13) (cid:13)
(cid:13)
log(4d A/δ) +2R2(cid:18)
2+
H(cid:19)(cid:18) 2log(4d A/δ)(cid:19)3/4
M (cid:13) n(cid:13) N 2µ N
(cid:13) (cid:13)
n=1 2
(cid:18) (cid:19)
H log(4d /δ)
+4R2 2+ A
2µ 3N
25(cid:16) (cid:17)2
Proof. Now since ∥M2∥ ≤ ∥M ∥2 ≤ R4 2+ H , we have that ∥M2 − E[M2]∥ ≤
n 2 n 2 4µ n 1 2
(cid:16) (cid:17)2
2R4 2+ H by the matrix Hoeffding bound, we have that with probability 1−δ/2,
4µ
(cid:13) (cid:13)
(cid:13)
1 (cid:88)N
M2
−E[M2](cid:13) (cid:13)
(cid:13)
≤4R4(cid:18)
2+
H(cid:19)2(cid:114) log(d A/δ)
(cid:13)N n 1 (cid:13) 2µ N
(cid:13) (cid:13)
n=1 2
Further, by the matrix Bernstein inequality, we have that with probability 1−δ/2,
(cid:13) (cid:13)
(cid:13)
1 (cid:88)N
M −E[M
](cid:13) (cid:13)
(cid:13)
≤(cid:114)
2∥E[M2]∥
log(d A/δ) +4R2(cid:18)
2+
H(cid:19) log(d A/δ)
(cid:13)N n 1 (cid:13) 1 2 N 2µ 3N
(cid:13) (cid:13)
n=1 2
Combining the two results and using a union bound, we get that with probability 1−δ/2
(cid:13) (cid:13)
(cid:13) 1 (cid:88)N (cid:13)
∥M −E[M ]∥ =(cid:13) M −E[M ](cid:13)
N 1 2 (cid:13)N n 1 (cid:13)
(cid:13) (cid:13)
n=1 2
(cid:118)
≤(cid:117) (cid:117) (cid:116)2(cid:13) (cid:13) (cid:13)(cid:88)N M2(cid:13) (cid:13)
(cid:13)
log(2d A/δ) +2R2(cid:18)
2+
H(cid:19)(cid:18) 2log(2d A/δ)(cid:19)3/4
(cid:13) n(cid:13) N 2µ N
(cid:13) (cid:13)
n=1 2
(cid:18) (cid:19)
H log(d /δ)
+4R2 2+ A
2µ 3N
Proposition 2 (Confidence Bound for D ). With probability 1−δ/4, for i=1,2, we have
N,i
(cid:13) (cid:13)
that ∥D −E[D ]∥ =(cid:13)1 (cid:80)N D −E[D ](cid:13) ≤∆ with
N,i n,i 2 (cid:13)N n=1 n,i n,i (cid:13) D
2
(cid:114)
8log(4d /δ)
∆ ≤ A
D N
Proof. Since ∥D ∥ ≤1, this immediately follows by the matrix Hoeffding inequality.
n,i 2
Lemma 5 (Confidence Bound for D−1 M D−1 ). We have that with probability 1−δ
N,1 N N,2
∥D−1 M D−1 −E[D ]−1E[M ]E[D ]−1∥ ≤(cid:18) B D3(2−B D∆ D)(cid:19) (R2+∆ )∆
N,1 N N,2 N,1 N,1 N,2 2 (1−B ∆ )2 M D
D D
(cid:18)
B
(cid:19)2
+ D ∆
1−B ∆ M
D D
−1
where B =max ∥D ∥ .
D i=1,2 N,i 2
26Proof. For brevity, just for this proof, we define D = E[D ] for i = 1,2 and by M :=
i N,i
E[M ] = E[M ]. By a union bound, Propositions 1 and 2 hold with probability at least
1 N
1−δ. The statements in the rest of this proof thus hold with probability at least 1−δ. Now
note that
∥D−1 M D−1 −D−1MD−1∥ ≤∥D−1 ∥ ∥M ∥ ∥D−1 −D−1∥
N,1 N,1 N,2 1 2 2 N,1 2 N,1 2 N,2 2 2
+∥D−1 −D−1∥ ∥M ∥ ∥D−1∥
N,1 1 2 N,1 2 2
+∥D−1∥ ∥M −M∥ ∥D−1∥
1 2 N,1 2 2 2
Now note that by inequality (1.1) from Wei et al. [2005], we have that
∥D−1 −D−1∥ ≤ B D2∆ D
N,i i 2 1−B ∆
D D
This means that
∥D−1∥ ≤∥D−1 ∥ +∥D−1 −D−1∥ ≤ B D
i 2 N,i 2 N,i i 2 1−B ∆
D D
Also, since contexts in both trajectory halves have the same distribution and contexts are
generated independently, M=E[M ]=D E[β β⊤]D . So, ∥M∥ ≤∥D ∥ ∥D ∥ R2 ≤R2.
1 i 1 1 i 2 1 2 2 2
This implies that
∥M ∥ ≤R2+∥M −M∥ ≤R2+∆
N,1 2 N,1 2 M
Combining all these with the bound above, we get that
∥D−1 M D−1 −E[D ]−1E[M ]E[D ]−1∥ ≤(cid:18) B D3(2−B D∆ D)(cid:19) (R2+∆ )∆
N,1 N N,2 N,1 n,1 N,2 2 (1−B ∆ )2 M D
D D
(cid:18)
B
(cid:19)2
+ D ∆
1−B ∆ M
D D
Theorem 2 (Computing and Bounding ∆ ). Let ∥M −E[M ]∥ ≤ ∆ and ∥D −
off N 1 2 M N,i
E[D ]∥ ≤ ∆ for i = 1,2 with probability 1−δ/3 each. Then, with probability 1−δ,
n,i 2 D
∥UˆUˆ⊤−U U⊤∥ ≤∆ , where for B =∥D−1 ∥ and λˆ :=λ (M )−λ (M ),
⋆ ⋆ 2 off D N 2 dK N dK+1 N
∆ off = 2√ λ2 ˆdK (cid:18)(cid:16) B (D3 1−(2 B− DB ∆D D∆ )D 2)(cid:17) (R2+∆ M)∆ D+(cid:16) 1−BB DD ∆D(cid:17)2 ∆ M(cid:19)
Obtaining ∆
M
and ∆
D
from Propositions 1 and 2, ∆
off
=O(cid:101)( λθ1
λ3
AN−1/2(cid:112) d Kd Alog(d A/δ)).
Proof. First note that E[D ]−1E[M ]E[D ]−1 = U (Λ+µ µ⊤)U⊤. Also recall that
N,1 N,1 N,2 ⋆ θ θ ⋆
Uˆ is given by the top-d eigenvectors for E[D−1 M D−1 ]. This means that there is a
K N,1 N,1 N,2
d ×d unitary matrix W such that U W forms the eigenvectors for U (Λ+µ µ⊤)U⊤.
K K ⋆ ⋆ θ θ ⋆
This means that by the Davis-Kahan theorem for statisticians [Yu et al., 2014] and Lemma 5,
we have that with probability 1−δ
√
∥UˆUˆ⊤−U U⊤∥ =∥UˆUˆ⊤−U WW⊤U⊤∥ = 2(d −∥Uˆ⊤U ∥2)
⋆ ⋆ 2 ⋆ ⋆ 2 K ⋆ F
27√
2 2d
≤ K∥E[D ]−1E[M ]E[D ]−1−D−1 M D−1 ∥
λˆ N,1 N,1 N,2 N,1 N,1 N,2 2
2√ 2d (cid:32)(cid:18) B3(2−B ∆ )(cid:19) (cid:18) B (cid:19)2 (cid:33)
≤ K D D D (R2+∆ )∆ + D ∆
λˆ (1−B D∆ D)2 M D 1−B D∆
D
M
We thus set ∆ to be the value above.
off
(cid:113)
Bounding ∆ : Now note that for large enough N, ∥E[D ]−1∥ ∆ = 8log(dA/δ) ≤ 1.
off n,1 2 D λAN 2
In particular, by applying inequality (1.1) from Wei et al. [2005], we have that B ≤
D
2∥E[D ]−1∥ = 2 . This already gives us the much simpler expression
n,1 2 λA
√
(cid:18) (cid:19)
2 2d 64 16
∆ ≤ K (R2+∆ )∆ + ∆
off λˆ λ3 M D λ2 M
A A
Also note that by Lemma 4 and the matrix Hoeffding inequality, we have that
∥
1
(cid:88)N M2∥≤∥E[M2]∥+4R4(cid:18)
2+
H(cid:19)2(cid:114)
log(d A/δ)
N n 1 2µ N
n=1
σ4(d +1) (cid:18) H(cid:19)2(cid:114) log(d /δ)
≤R4+ A +4R4 2+ A
8µ2 2µ N
We combine this with Proposition 1 to get that
(cid:32)(cid:18) σ2√
d
(cid:19)(cid:114)
log(d
/δ)(cid:33)
∆ =O R2+ A A
M µ N
Since σ2 ≤R2, we get that
(cid:32) (cid:114) (cid:33)
d log(d /δ)
∆ =O R2 A A =O(R2)
M N
Also recall from Proposition 2, we get that
(cid:114)
8log(d /δ)
∆ = A
D N
Also recall that λ is the minimum eigenvalue of 1 Λ, and so the minimum eigenvalue of
θ R2
U ⋆(Λ+µ θµ⊤ θ)U⊤ ⋆ is larger than R2λ θ. We then conclude that λˆ ≥R2λ θ−2∆ M ≥ λ 2θ for
large enough N. Combining all these, we get that
∆
=O(cid:32)√ d
K
(cid:18) R2
+
R2(cid:112)
d
(cid:19)(cid:114) log(d A/δ)(cid:33)
off R2λ λ3 λ2 A N
θ A A
(cid:32) (cid:114) (cid:33)
1 d d log(d /δ)
=O K A A
λ λ3 N
θ A
28D Proofs for LOCAL-UCB
√
Theorem 3 (LOCAL-UCB Regret). Under Assumptions 1 and 2, if α = R µ +
1,t
(cid:112) √ (cid:112)
CR d log(2T/δ) and α =R µ+CR d log(2T/δ) for a suitable universal constant
K 2,t A
C, with probability at least 1−δ, LOCAL-UCB has regret
(cid:16) (cid:16) √ √ (cid:16) (cid:113) (cid:17)(cid:17)(cid:17)
Reg T =O min Rd A T,Rd K T 1+ λθ1 λ3
A
dT Kd NA .
Proof. Let the true latent state for the given trajectory be θ , so that the reward parameter
⋆
β =U θ .
⋆ ⋆ ⋆
Showing that U ,β are in our confidence set: We check all our constraints:
⋆ ⋆
• Note that with probability 1−δ/3, U satisfies ∥Uˆ⊤U ∥ ≥(cid:112) d −∆2 /2.
⋆ ⋆ F K off
• From the standard confidence ellipsoid bound for linear models applied to dimension
d , with probability 1−δ/3T, ∥βˆ −β ∥ ≤α for all t.
A 2,t ⋆ V−1 2,t
t
• Note that U U⊤β =β .
⋆ ⋆ ⋆ ⋆
• Finally,weapplythestandardconfidenceellipsoidboundforlinearmodelstodimension
d instead of d with model r = (ϕ(x ,a )⊤U )(U⊤β ) + ϵ . This means that
K A t t t ⋆ ⋆ ⋆ t
∥U⊤β −U⊤βˆ ∥ ≤ α where V = (I +(cid:80)t−1U⊤ϕ(x ,a )ϕ(x ,a )⊤U ).
⋆ ⋆ ⋆ 1,t V− 1,1
t
1,t 1,t dK s=1 ⋆ t t t t ⋆
Note that since U U⊤ = I , we have that V = (U⊤V U ). So, ∥U⊤(β −
⋆ ⋆ dK 1,t ⋆ t ⋆ ⋆
βˆ )∥ ≤α holds with probability at least 1−δ/3T for all t.
1,t (UT ⋆VtU⋆)−1 1,t
So, by a union bound over all events, we get that for all actions a, the tuple (a,β ,U )
⋆ ⋆
satisfies our conditions with probability 1−δ.
Leveraging optimism in low dimension: From above and by the optimistic design of
the algorithm, with probability 1−δ, ϕ(x ,a )⊤β˜ is an upper bound on ϕ(x ,a)⊤β for any
t t t t ⋆
action a. Thus, we have the following regret decomposition with probability at least 1−δ:
T
(cid:88)
Reg = ϕ(x ,a⋆)⊤β −ϕ(x ,a )⊤β
T t t ⋆ t t ⋆
t=1
T
≤(cid:88) ϕ(x ,a )⊤β˜ −ϕ(x ,a )⊤β
t t t t t ⋆
t=1
T
( =i)(cid:88) ϕ(x ,a )⊤U˜ U˜⊤β˜ −ϕ(x ,a )⊤U U⊤β
t t t t t t t ⋆ ⋆ ⋆
t=1
T
≤(cid:88) ϕ(x ,a )⊤˜ (U U˜⊤−U U⊤)β˜ +ϕ(x ,a )⊤U U⊤(β −β )
t t t t ⋆ ⋆ t t t ⋆ ⋆ t ⋆
t=1
T
≤RT∥U U˜⊤−U U⊤∥ +(cid:88) ϕ(x ,a )⊤U U⊤(β −β )
t t ⋆ ⋆ 2 t t ⋆ ⋆ t ⋆
t=1
29T
(cid:88)
≤RT∆ + ϕ(x ,a )⊤U U⊤(β −β )
off t t ⋆ ⋆ t ⋆
t=1
T
(cid:88)
≤RT∆ + ∥ϕ(x ,a )⊤U ∥ ∥U⊤(β −β )∥
off t t ⋆ (U⊤ ⋆VtU⋆)−1 ⋆ t ⋆ (U⊤ ⋆VtU⋆)−1
t=1
T
(cid:88)
≤RT∆ + α ∥ϕ(x ,a )⊤U ∥
off 2,t t t ⋆ (U⊤ ⋆VtU⋆)−1
t=1
T
( =ii) RT∆ +(cid:88) α ∥ϕ(x ,a )⊤U ∥
off 2,t t t ⋆ V−1
1,t
t=1
(iii) (cid:16) (cid:112) (cid:17)
≤ RT∆ +O d T log(T/δ)
off K
(cid:32) (cid:114) (cid:33)
( =iv) O Rd (cid:112) T log(T/δ)+ Rλ3 D T2d Kd Alog(d A/δ)
K λ N
min
  (cid:115) 
=ORd K(cid:112) T log(T/δ)1+ λλ3 D Td A dlog N(d A) 
min K
(cid:32) √ (cid:32) (cid:114) Td (cid:33)(cid:33)
=O˜ Rd T 1+ A
K d N
K
where(i)holdssinceU˜ U˜⊤β˜ =β˜ andU U⊤β =β (ii)holdssinceV−1 =(U⊤V U )−1,
t t t t ⋆ ⋆ ⋆ ⋆ 1,t ⋆ t ⋆
(iii) holds by the usual proof of LinUCB applied to dimension d , and (iv) holds by
K
Theorem 2.
Leveraging optimism in high dimension: This is merely the proof of LinUCB. From
above and by the optimistic design of the algorithm, with probability 1−δ, ϕ(x ,a )⊤β˜
t t t
is an upper bound on ϕ(x ,a)⊤β for any action a. Thus, we have the following regret
t ⋆
decomposition with probability at least 1−δ:
T
Reg =(cid:88) ϕ(x ,a⋆)⊤β −ϕ(x ,a )⊤β˜
T t t ⋆ t t ⋆
t=1
T
≤(cid:88) ϕ(x ,a )⊤β˜ −ϕ(x ,a )⊤β
t t t t t ⋆
t=1
T
=(cid:88) ∥ϕ(x ,a )⊤∥ ∥β˜ −β ∥
t t V− t1 t ⋆ Vt
t=1
(cid:112)
=O(Rd T log(T/δ))
A
√
=O˜(Rd T)
A
where the last line follows from the standard regret bound for LinUCB applied to dimension
d .
A
30Combining the two bounds, we have our result.
   (cid:115) 
Reg T =OminRd A(cid:112) T log(T/δ),Rd K(cid:112) T log(T/δ)1+ λλ3 D Td A dlog N(d A) 
min K
(cid:32) (cid:32) √ √ (cid:32) (cid:114) Td (cid:33)(cid:33)(cid:33)
=O˜ min Rd T,Rd T 1+ A
A K d N
K
31E Proofs for ProBALL-UCB
E.1 Confidence bound for the low-dimensional reward parameter
Lemma 6 (Confidence Bound for βˆ ). If for all timesteps t, ϕ(x ,a ) lies in the span of
1,t t t
Uˆ, we have that for a universal constant C,
(cid:13) (cid:13)Uˆ⊤βˆ −Uˆ⊤β (cid:13) (cid:13) ≤R√ µ+CR(cid:112) d log(t/δ)
(cid:13) 1,t ⋆(cid:13) A
V−1
1,t
Otherwise, we have that
(cid:13) (cid:13)Uˆ⊤βˆ −Uˆ⊤β (cid:13) (cid:13) ≤R√ µ+Rκ +∆ CR(cid:112) d log(t/δ)
(cid:13) 1,t ⋆(cid:13) t off A
V−1
1,t
(cid:113)
where κ =∥Uˆ⊤X⊤X ∥ , with notation ∥A∥ := ∥A⊤CA∥
t t t (Uˆ⊤VtUˆ)−1 C 2
Proof. Forbrevity,inthisproofwewilldenotebyX :=[ϕ(x ,a ),...ϕ(x ,a )]⊤,which
t 1 1 t−1 t−1
is a t×d matrix. Recall that V = µI +X⊤X . We will also denote V = Uˆ⊤V Uˆ.
A t dA t t 1,t t
Note that the vector of rewards is given by X β +η , where η is a random vector of t
t ⋆ t t
independent R2-subgaussian entries. So, b = X⊤(X β +η ). Also define the notation
t t t ⋆ t
∆ :=UˆUˆ⊤−U U⊤.
U ⋆ ⋆
If all actions lie in the span of Uˆ. Note that since all actions taken lie in the span of Uˆ,
X =X UˆUˆ⊤. This is the key observation. Now note that
t t
βˆ =UˆV−1Uˆ⊤X⊤(X β +η )
1,t 1,t t t ⋆ t
=UˆV−1Uˆ⊤X⊤X β +UˆV−1Uˆ⊤X⊤η
1,t t t ⋆ 1,t t t
=UˆV−1Uˆ⊤X⊤X UˆUˆ⊤β +UˆV−1Uˆ⊤X⊤η
1,t t t ⋆ 1,t t t
=UˆV−1(V −µI )Uˆ⊤β +UˆV−1Uˆ⊤X⊤η
1,t 1,t dK ⋆ 1,t t t
=UˆU⊤β −µUˆV−1Uˆ⊤β +UˆV−1Uˆ⊤X⊤η
⋆ 1,t ⋆ 1,t t t
The rest of the proof is similar to Theorem 2 in Abbasi-Yadkori et al. [2011]. We first note
that for any x, we have that
(cid:12) (cid:12)
x⊤(βˆ −UˆUˆ⊤β )≤(cid:12)(x⊤Uˆ)V−1(−µUˆ⊤β +Uˆ⊤X⊤η )(cid:12)
1,t ⋆ (cid:12) 1,t ⋆ t t (cid:12)
(cid:13) (cid:13)
≤∥Uˆ⊤x∥ (cid:13)(−µUˆ⊤β +Uˆ⊤X⊤η )(cid:13)
V− 1,1 t(cid:13) ⋆ t t (cid:13) V−1
1,t
(cid:16) (cid:17)
≤∥Uˆ⊤x∥ µ∥Uˆ⊤β ∥ +∥Uˆ⊤X⊤η ∥
V−1 ⋆ V−1 t t V−1
1,t 1,t 1,t
√ √
Now note that µ∥Uˆ⊤β ∥ ≤∥Uˆ⊤β ∥ µ≤R µ and by the self normalized martingale
⋆ V−1 ⋆ 2
1,t
concentration inequality from Abbasi-Yadkori et al. [2011] applied to d dimensional vectors
K
32Uˆ⊤X , we have that with probability at least 1−δ, ∥Uˆ⊤X⊤η ∥ ≤CR(cid:112) d log(t/δ) for
t t t V−1 A
some universal constant C. So we have with probability at least 11, −t δ that
=(cid:13) (cid:13)Uˆ(βˆ −UˆUˆ⊤β )(cid:13) (cid:13)2 ≤(cid:13) (cid:13)βˆ −UˆUˆ⊤β (cid:13) (cid:13) (cid:16) R√ µ+CR(cid:112) d log(t/δ)(cid:17)
(cid:13) 1,t ⋆ (cid:13) (cid:13) 1,t ⋆(cid:13) A
V−1 V−1
1,t 1,t
This means that
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Uˆ⊤βˆ −Uˆ⊤β (cid:13) =(cid:13)βˆ −UˆUˆ⊤β (cid:13)
(cid:13) 1,t ⋆(cid:13) (cid:13) 1,t ⋆(cid:13)
V−1 V−1
1,t 1,t
√ (cid:112)
≤R µ+CR d log(t/δ)
A
If all actions don’t lie in the span of Uˆ. This time note that
βˆ =UˆV−1Uˆ⊤X⊤(X β +η )
1,t 1,t t t ⋆ t
=UˆV−1Uˆ⊤X⊤X β +UˆV−1Uˆ⊤X⊤η
1,t t t ⋆ 1,t t t
=UˆV−1Uˆ⊤X⊤X Uˆ U⊤β +UˆV−1U⊤X⊤η
1,t t t ⋆ ⋆ ⋆ 1,t t t
=UˆV−1Uˆ⊤X⊤X UˆUˆ⊤β +UˆV−1Uˆ⊤X⊤X ∆ β +UˆV−1Uˆ⊤X⊤η
1,t t t ⋆ 1,t t t U ⋆ 1,t t t
=UˆV−1(V −µI )Uˆ⊤β +UˆV−1Uˆ⊤X⊤X ∆ β +UˆV−1Uˆ⊤X⊤η
1,t 1,t dK ⋆ 1,t t t U ⋆ 1,t t t
=UˆU⊤β −µUˆV−1Uˆ⊤β +UˆV−1Uˆ⊤X⊤X ∆ β +UˆV−1Uˆ⊤X⊤η
⋆ 1,t ⋆ 1,t t t U ⋆ 1,t t t
The rest of the proof is similar to Theorem 2 in Abbasi-Yadkori et al. [2011]. We first note
that for any x, we have that
(cid:12) (cid:12)
x⊤(βˆ −UˆUˆ⊤β )≤(cid:12)(x⊤Uˆ)V−1(−µUˆ⊤β +Uˆ⊤X⊤X ∆ β +Uˆ⊤X⊤η )(cid:12)
1,t ⋆ (cid:12) 1,t ⋆ t t U ⋆ t t (cid:12)
(cid:13) (cid:13)
≤∥Uˆ⊤x∥ (cid:13)(−µUˆ⊤β +Uˆ⊤X⊤X ∆ β +Uˆ⊤X⊤η )(cid:13)
V− 1,1 t(cid:13) ⋆ t t U ⋆ t t (cid:13) V−1
1,t
(cid:16) (cid:17)
≤∥Uˆ⊤x∥ µ∥Uˆ⊤β ∥ +∥Uˆ⊤X⊤X ∆ β ∥ +∥Uˆ⊤X⊤η ∥
V−1 ⋆ V−1 t t U ⋆ V−1 t t V−1
1,t 1,t 1,t 1,t
(i) (cid:16) (cid:17)
≤ ∥Uˆ⊤x∥ µ∥Uˆ⊤β ∥ +∥∆ β ∥ ∥Uˆ⊤X⊤X ∥ +∥Uˆ⊤X⊤η ∥
V−1 ⋆ V−1 U ⋆ 2 t t V−1 t t V−1
1,t 1,t 1,t 1,t
(cid:16) (cid:17)
≤∥Uˆ⊤x∥ µ∥Uˆ⊤β ∥ +R∥∆ ∥ ∥Uˆ⊤X⊤X ∥ +∥Uˆ⊤X⊤η ∥
V−1 ⋆ V−1 U 2 t t V−1 t t V−1
1,t 1,t 1,t 1,t
(cid:16) (cid:17)
≤∥Uˆ⊤x∥ µ∥Uˆ⊤β ∥ +R∆ ∥Uˆ⊤X⊤X ∥ +∥Uˆ⊤X⊤η ∥
V−1 ⋆ V−1 off t t V−1 t t V−1
1,t 1,t 1,t 1,t
Here, (i) holds because for any matrices A,C and any vector v, we have that ∥Av∥ =
C
(cid:112) (cid:113) (cid:113)
v⊤A⊤CAv≤∥v∥ ∥A⊤CA∥ =∥v∥ ∥A∥ ,wherewerecallthat∥A∥ := ∥A⊤CA∥ .
2 2 2 C C 2
√ √
Now note that µ∥Uˆ⊤β ∥ ≤∥Uˆ⊤β ∥ µ≤R µ and by the self normalized martingale
⋆ V−1 ⋆ 2
1,t
concentration inequality from Abbasi-Yadkori et al. [2011] applied to d dimensional vectors
K
Uˆ⊤X , we have that with probability at least 1−δ, ∥Uˆ⊤X⊤η ∥ ≤ CR(cid:112) d log(t/δ)
t t t V−1 A
1,t
33for some universal constant C. Also recall that ∥Uˆ⊤X⊤X ∥ = κ . So we have with
t t V−1 t
probability at least 1−δ that 1,t
(cid:13) (cid:13)Uˆ(βˆ −UˆUˆ⊤β )(cid:13) (cid:13)2 ≤(cid:13) (cid:13)Uˆ⊤(βˆ −UˆUˆ⊤β )(cid:13) (cid:13) (cid:16) R√ µ+R∆ κ +CR(cid:112) d log(t/δ)(cid:17)
(cid:13) 1,t ⋆ (cid:13) (cid:13) 1,t ⋆ (cid:13) off t A
V−1 V−1
1,t 1,t
This means that
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Uˆ⊤βˆ −Uˆ⊤β (cid:13) =(cid:13)Uˆ(βˆ −UˆUˆ⊤β )(cid:13)
(cid:13) 1,t ⋆(cid:13) (cid:13) 1,t ⋆ (cid:13)
V−1 V−1
1,t 1,t
√ (cid:112)
≤R µ+R∆ κ +CR d log(t/δ)
off t A
(cid:13) (cid:13) (cid:13) (cid:13)
Notethatκ
t
=(cid:13) (cid:13)(cid:80)t s− =1 1Uˆ⊤ϕ(x s,a s)ϕ(x s,a s)⊤(cid:13)
(cid:13)
V−1
= √1 µ(cid:13) (cid:13)(cid:80) st− =1 1Uˆ⊤ϕ(x s,a s)ϕ(x s,a s)⊤(cid:13)
(cid:13)
2
=
1,t
O(t), but this is a worst case bound.
We also state a lemma, borrowed from the standard proof of LinUCB regret.
Lemma7. Foranysequenceofactionsandcontextsx ,a andV =µI+(cid:80)t−1ϕ(x ,a )ϕ(x ,a )⊤,
t t t s=1 t t t t
we have that
T
(cid:88) (cid:16) (cid:17) (cid:112)
min ∥ϕ(x ,a )∥2 ,1 =O( d )
t t V−1 A
t
t=1
T
(cid:88) min(cid:16) ∥Uˆ⊤ϕ(x ,a )∥2 ,1(cid:17) =O((cid:112) d )
t t (Uˆ⊤VtUˆ)−1 K
t=1
Proof. This follows immediately from Lemma 11 of Abbasi-Yadkori et al. [2011].
E.2 Proof of the theorem
Theorem 4 (Regret for ProBALL-UCB). Let α =R√ µ+τ′R∆ κ +CR(cid:112) d log(T/δ)
1,t off t K
√ (cid:112)
and let α =R µ+CR d log(T/δ). Let S be the first timestep when Algorithm 3 does
2,t A
not play Line 6 and let S =T if no such timestep exists. For τ =τ′ =1 we have that
(cid:18) (cid:18) √ √ (cid:18) (cid:18)(cid:113) (cid:113) (cid:19)(cid:19)(cid:19)(cid:19)
Reg T =O(cid:101) min Rd A T,Rd K T 1+ λ3 A1 λθ dd KA NT + Sd NA (cid:80)S t=1κ2 t .
In the worst case, κ =O(t) and so 1 (cid:80)S κ2 =O(T2), but if all features ϕ(x ,a ) lie in
t S t=1 t t t
the span of Uˆ for t≤S, then 1 (cid:80)S κ2 =O(T).
S t=1 t
Proof. We will first show that our bonuses give optimistic estimates of the true reward whp
and then leverage the optimism.
√
Showing optimism when τ∆ T ≤d : In this case, we are inside the "if" statement
off A
and are running a projected and modified version of LinUCB. In that case, for all timesteps
34the projected version is run, all features will lie in the span of Uˆ. This is because the
maximization problem is given by
a =arg max ϕ⊤UˆUˆ⊤βˆ +∥Uˆ⊤ϕ(x ,a)∥
t
a,∥ϕ(xt,a)∥2≤1
1,t t (Uˆ⊤V1,tUˆ)−1
If our features are isotropic, then this is only maximized by a feature in the span of Uˆ. So,
the conditions of the first bound in Lemma 6 are fulfilled. We will denote V =Uˆ⊤V Uˆ.
1,t t
We have that with probability 1−δ/2, for all x,a,t, the following holds.
|ϕ(x,a)⊤β −ϕ(x,a)⊤UˆUˆ⊤βˆ |≤|ϕ(x,a)⊤Uˆ(Uˆ⊤β −Uˆ⊤βˆ )|+|ϕ(x,a)⊤(U U⊤−UˆUˆ⊤)β |
⋆ 1,t ⋆ 1,t ⋆ ⋆ ⋆
≤∥Uˆ⊤ϕ(x,a)∥ ∥Uˆ⊤β −Uˆ⊤βˆ ∥ +R∥U U⊤−UˆUˆ⊤∥
V−1 ⋆ 1,t V−1 ⋆ ⋆ 2
1,t 1,t
≤α ∥Uˆ⊤ϕ(x,a)∥ +R∆
1,t V−1 off
1,t
This shows that with probability at least 1−δ and for all x,a,t,
ϕ(x,a)⊤β ≤ϕ(x,a)⊤UˆUˆ⊤βˆ +α ∥Uˆ⊤ϕ(x,a)∥ +R∆
⋆ 1,t 1,t V−1 off
1,t
This implies that with probability at least 1−δ, for any action a,
ϕ(x ,a)⊤β ≤maxϕ(x ,a)⊤UˆUˆ⊤βˆ +α ∥Uˆ⊤ϕ(x ,a)∥ +R∆
t ⋆
a
t 1,t 1,t t V− 1,1
t
off
=ϕ(x ,a )⊤UˆUˆ⊤βˆ +α ∥Uˆ⊤ϕ(x ,a )∥ +R∆
t t 1,t 1,t t t V−1 off
1,t
√
Leveraging optimism when τ∆ T ≤d : Consider the standard regret decomposition,
off A
which holds with probability 1−δ:
T
(cid:88)
Reg = ϕ(x ,a⋆)⊤β −ϕ(x ,a )⊤β
T t t ⋆ t t ⋆
t=1
T
≤(cid:88) ϕ(x ,a )⊤UˆUˆ⊤βˆ −ϕ(x ,a )⊤β +α ∥Uˆ⊤ϕ(x ,a )∥ +R∆
t t 1,t t t ⋆ 1,t t t V−1 off
1,t
t=1
T
≤(cid:88) 2α ∥Uˆ⊤ϕ(x ,a )∥ +2R∆ (2)
1,t t t V−1 off
1,t
t=1
T
( ≤i)(cid:88)
2α′
min(cid:16)
∥Uˆ⊤ϕ(x ,a )∥
,1(cid:17)
+2R∆
1,t t t V−1 off
1,t
t=1
T
≤(cid:88) 2(R√ µ+C(cid:112) d log(T/δ))min(cid:16) ∥Uˆ⊤ϕ(x ,a )∥ ,1(cid:17) +2R∆
K t t V−1 off
1,t
t=1
T
+(cid:88) 2R∆ κ min(cid:16) ∥Uˆ⊤ϕ(x ,a )∥ ,1(cid:17)
off t t t V−1
1,t
t=1
35(cid:118)
(cid:117) T (cid:18) (cid:19)
( ≤ii) 2(R√ µ+C(cid:112) d log(T/δ))(cid:117) (cid:116)(cid:88) min ∥Uˆ⊤ϕ(x ,a )∥2 ,1 +2R∆ T
K t t V−1 off
1,t
t=1
(cid:118) (cid:118)
(cid:117) T (cid:117) T (cid:18) (cid:19)
+2R∆ (cid:117) (cid:116)(cid:88) κ2(cid:117) (cid:116)(cid:88) min ∥Uˆ⊤ϕ(x ,a )∥2 ,1
off t t t V−1
1,t
t=1 t=1
(cid:115)
(i ≤ii)
O(d
(cid:112)
T log(T/δ))+2R∆ T +2R∆
(cid:80)T t=1κ2 t(cid:112)
d T (3)
K off off T K
  √ (cid:118) 
(cid:117) T
(cid:112) T (cid:117) 1 (cid:88)
=ORd K T log(T/δ)1+∆ off d +(cid:116) Td κ2 t
K K
t=1
where (i) holds since ϕ(x ,a⋆)⊤β −ϕ(x ,a )⊤β ≤2R, (ii) holds by the Cauchy Schwarz
t t ⋆ t t ⋆
inequality and (iii) holds by Lemma 7. So, we have that
Reg T =O Rd K(cid:112) T log(T/δ) 1+ λ31 λ (cid:115) d A Nlo dg(d A) √ T +(cid:118) (cid:117) (cid:117) (cid:116)d TK (cid:88)S κ2 t   
A θ K t=1
   (cid:118) 
=O(cid:101)Rd K√ T 1+ λ31
λ
(cid:114) Nd A dT +(cid:117) (cid:117) (cid:116) Td NA (cid:88)T κ2 t
A θ K t=1
   (cid:118) 
=O(cid:101) Rd
K√
T  1+
λ31
λ

(cid:114) Nd
A
dT +(cid:117) (cid:117)
(cid:116)
min(d
SA
,T)N
min (cid:88)(S,T)
κ2 t  

A θ K t=1
(cid:18)√ (cid:113) (cid:19)
wherethelastinequalityholdssinceT <SforthefirsttimestepSsatisfying∆ off S+ d SK (cid:80)S t=1κ2 t ≥
(cid:18)√ (cid:113) (cid:19)
d A. Additionally, since ∆ off T + d TK (cid:80)T t=1κ2 t ≤d A, we have using equation 3 that
√
Reg
T
=O(cid:101)(d
A
T)
√
as well. So, we have that when τ∆ T ≤d ,
off A
    (cid:118) 
Reg
T
=O(cid:101) min d
A√
T,Rd
K√
T  1+
λ31
λ

(cid:114) Nd
A
dT +(cid:117) (cid:117)
(cid:116)
min(d
SA
,T)N
min (cid:88)(S,T)
κ2 t   

A θ K t=1
(cid:18)√ (cid:113) (cid:19)
Bounding regret when τ∆ off T + d TK (cid:80)T t=1κ2 t ≥ d A: In this regime, after the
(cid:18)√ (cid:113) (cid:19)
first timestep S satisfying ∆ off S+ d SK (cid:80)S t=1κ2 t ≥d A, we run standard LinUCB with
√
dimension d
A
and incur O(cid:101)(d
A
T) regret with probability 1−δ. Until timestep S−1, we
36run the projected and modified version of LinUCB and incur regret bounded by
(cid:115)
√ (cid:80)S κ2(cid:112) (cid:16) √ √ (cid:17)
O(cid:101)(d
K
S)+2R∆ offS+2R∆
off
t= S1 t d KS =O(cid:101) d
A
S+d
K
S
√ √
=O(cid:101)(d
A
T +d
K
T)
√
=O(cid:101)(d
A
T)
√
Combining these, we incur O(d T) regret during the whole method.
A
√ (cid:18)√ (cid:113) (cid:19)
Also, since d
A
T ≤∆
off
TS+ dK ST (cid:80)S t=1κ2
t
, we get that our regret is also bounded
by
  (cid:118) 
Reg T =O(cid:101)∆ off√ TS+(cid:117) (cid:117) (cid:116)d K ST (cid:88)S κ2 t
t=1
 (cid:118) 
=O(cid:101)(d K√ T +∆ off√ TS+(cid:117) (cid:117) (cid:116)d K ST (cid:88)S κ2 t)
t=1
   (cid:118) 
=O(cid:101) Rd
K√
T  1+
λ31
λ

(cid:114) Nd
A
dT +(cid:117) (cid:117)
(cid:116)
min(d
SA
,T)N
min (cid:88)(S,T)
κ2 t  

A θ K t=1
So, our regret satisfies
    (cid:118) 
Reg
T
=O(cid:101) min d
A√
T,Rd
K√
T  1+
λ31
λ

(cid:114) Nd
A
dT +(cid:117) (cid:117)
(cid:116)
min(d
SA
,T)N
min (cid:88)(S,T)
κ2 t   

A θ K t=1
E.2.1 Understanding κ
t
Letting X =Uˆ⊤[ϕ(x ,a ),...ϕ(x ,a )]⊤, recall that κ :=∥UˆX⊤X ∥ . In the
t 1 1 t t t t t (µI+X⊤
t
Xt)−1
worst case, κ ≤∥UˆX⊤X ∥ =O(t) since actions have norm 1. In that case, 1 (cid:80)S κ =
t t t 2 S t=1 t
O(S2). However, if X lies in the span of Uˆ⊤, then X Uˆ⊤Uˆ = X . This means that for
t t t
Y :=X Uˆ⊤,
t t
(cid:113)
κ = UˆX⊤X Uˆ⊤Uˆ(µI+X⊤X )−1U⊤UˆX⊤X Uˆ⊤
t t t t t t t
(cid:113)
= Y⊤Y (µI+Y⊤Y )−1Y⊤Y
t t t t t t
(cid:113)
= (I−µ(µI+Y⊤Y ))Y⊤Y
t t t t
√
=O(cid:101)( t)
So, 1 (cid:80)S κ =O(S)=O(T).
S t=1 t
37F Other Experiments
F.1 Determining the Latent Rank from Offline Data
We note that as discussed in 4, we can use the eigenvalues of D−1 M D−1 to determine
N,1 N N,2
the rank of our subspace. We use the version of this arising from pseudo-inverses instead of
regularization, just like in the MovieLens experiments. We demonstrate that we can indeed
determine that the d = 18 by finding the significant eigenvalues of the pseudo-inverse
K
version of D−1 M D−1 estimated from the offline dataset of 5000 samples. We show the
N,1 N N,2
plotsandlogplotsoftheseeigenvalues. Wealsoplottheeigenvaluesofthecompletedratings
matrix for comparison. Notice that they match and both fall after 18 eigenvalues.
Figure 4: Plot of eigenvalues of aforementioned matrix. Notice the drop after 18 eigenvalues.
38Figure 5: Log-plot of eigenvalues of aforementioned matrix. Notice the drop after 18
eigenvalues.
G Additional Figures
Figure 6: Comparison of ProBALL-UCB initialized with ground truth subspaces against
LinUCB,mUCB,andmmUCB,fordifferentchoicesofτ andconfidenceboundconstructions.
All variants of ProBALL-UCB perform no worse than LinUCB, and outperform mUCB and
mmUCB. Shaded area depicts 1-standard error confidence intervals over 30 trials with fresh
θ. The confidence intervals on regret thus account for the variation in frequentist regret for
changing θ.
39Figure 7: Comparison of ProBALL-UCB initialized with ground truth subspaces against
LinUCB,mUCB,andmmUCB,fordifferentchoicesofτ andconfidenceboundconstructions,
in terms of rolling average rating over 25 timesteps. All variants of ProBALL-UCB perform
noworsethanLinUCB,andoutperformmUCBandmmUCB.Shadedareadepicts1-standard
error confidence intervals over 30 trials with fresh θ. The confidence intervals on regret thus
account for the variation in frequentist regret for changing θ.
Figure 8: Comparison of ProBALL-UCB initialized with SOLD against LinUCB, mUCB,
and mmUCB, for different choices of τ and confidence bound constructions, in terms of
rolling average rating over 25 timesteps. All variants of ProBALL-UCB perform no worse
than LinUCB, and outperform mUCB and mmUCB. Shaded area depicts 1-standard error
confidence intervals over 30 trials with fresh θ. The confidence intervals on regret thus
account for the variation in frequentist regret for changing θ.
40