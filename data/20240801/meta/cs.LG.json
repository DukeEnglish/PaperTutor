[
    {
        "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
        "authors": "Atsuyuki MiyaiJingkang YangJingyang ZhangYifei MingYueqian LinQing YuGo IrieShafiq JotyYixuan LiHai LiZiwei LiuToshihiko YamasakiKiyoharu Aizawa",
        "links": "http://arxiv.org/abs/2407.21794v1",
        "entry_id": "http://arxiv.org/abs/2407.21794v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21794v1",
        "summary": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD\ndetection, and OD in the VLM era. Our framework reveals that, with some field\ninactivity and integration, the demanding challenges have become OOD detection\nand AD. In addition, we also highlight the significant shift in the definition,\nproblem settings, and benchmarks; we thus feature a comprehensive review of the\nmethodology for OOD detection, including the discussion over other related\ntasks to clarify their relationship to OOD detection. Finally, we explore the\nadvancements in the emerging Large Vision Language Model (LVLM) era, such as\nGPT-4V. We conclude this survey with open challenges and future directions.",
        "updated": "2024-07-31 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21794v1"
    },
    {
        "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
        "authors": "Richard RenSteven BasartAdam KhojaAlice GattiLong PhanXuwang YinMantas MazeikaAlexander PanGabriel MukobiRyan H. KimStephen FitzDan Hendrycks",
        "links": "http://arxiv.org/abs/2407.21792v1",
        "entry_id": "http://arxiv.org/abs/2407.21792v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21792v1",
        "summary": "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with upstream model capabilities, potentially\nenabling \"safetywashing\" -- where capability improvements are misrepresented as\nsafety advancements. Based on these findings, we propose an empirical\nfoundation for developing more meaningful safety metrics and define AI safety\nin a machine learning research context as a set of clearly delineated research\ngoals that are empirically separable from generic capabilities advancements. In\ndoing so, we aim to provide a more rigorous framework for AI safety research,\nadvancing the science of safety evaluations and clarifying the path towards\nmeasurable progress.",
        "updated": "2024-07-31 17:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21792v1"
    },
    {
        "title": "Deep Learning for Options Trading: An End-To-End Approach",
        "authors": "Wee Ling TanStephen RobertsStefan Zohren",
        "links": "http://arxiv.org/abs/2407.21791v1",
        "entry_id": "http://arxiv.org/abs/2407.21791v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21791v1",
        "summary": "We introduce a novel approach to options trading strategies using a highly\nscalable and data-driven machine learning algorithm. In contrast to traditional\napproaches that often require specifications of underlying market dynamics or\nassumptions on an option pricing model, our models depart fundamentally from\nthe need for these prerequisites, directly learning non-trivial mappings from\nmarket data to optimal trading signals. Backtesting on more than a decade of\noption contracts for equities listed on the S&P 100, we demonstrate that deep\nlearning models trained according to our end-to-end approach exhibit\nsignificant improvements in risk-adjusted performance over existing rules-based\ntrading strategies. We find that incorporating turnover regularization into the\nmodels leads to further performance enhancements at prohibitively high levels\nof transaction costs.",
        "updated": "2024-07-31 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21791v1"
    },
    {
        "title": "Vision-Language Model Based Handwriting Verification",
        "authors": "Mihir ChauhanAbhishek SatbhaiMohammad Abuzar HashemiMir Basheer AliBina RamamurthyMingchen GaoSiwei LyuSargur Srihari",
        "links": "http://arxiv.org/abs/2407.21788v1",
        "entry_id": "http://arxiv.org/abs/2407.21788v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21788v1",
        "summary": "Handwriting Verification is a critical in document forensics. Deep learning\nbased approaches often face skepticism from forensic document examiners due to\ntheir lack of explainability and reliance on extensive training data and\nhandcrafted features. This paper explores using Vision Language Models (VLMs),\nsuch as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By\nleveraging their Visual Question Answering capabilities and 0-shot\nChain-of-Thought (CoT) reasoning, our goal is to provide clear,\nhuman-understandable explanations for model decisions. Our experiments on the\nCEDAR handwriting dataset demonstrate that VLMs offer enhanced\ninterpretability, reduce the need for large training datasets, and adapt better\nto diverse handwriting styles. However, results show that the CNN-based\nResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach\nwith GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:\n71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings\nhighlight the potential of VLMs in generating human-interpretable decisions\nwhile underscoring the need for further advancements to match the performance\nof specialized deep learning models.",
        "updated": "2024-07-31 17:57:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21788v1"
    },
    {
        "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
        "authors": "Bradley BrownJordan JuravskyRyan EhrlichRonald ClarkQuoc V. LeChristopher RéAzalia Mirhoseini",
        "links": "http://arxiv.org/abs/2407.21787v1",
        "entry_id": "http://arxiv.org/abs/2407.21787v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21787v1",
        "summary": "Scaling the amount of compute used to train language models has dramatically\nimproved their capabilities. However, when it comes to inference, we often\nlimit the amount of compute to only one attempt per problem. Here, we explore\ninference compute as another axis for scaling by increasing the number of\ngenerated samples. Across multiple tasks and models, we observe that coverage -\nthe fraction of problems solved by any attempt - scales with the number of\nsamples over four orders of magnitude. In domains like coding and formal\nproofs, where all answers can be automatically verified, these increases in\ncoverage directly translate into improved performance. When we apply repeated\nsampling to SWE-bench Lite, the fraction of issues solved with\nDeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250\nsamples, outperforming the single-attempt state-of-the-art of 43% which uses\nmore capable frontier models. Moreover, using current API pricing, amplifying\nthe cheaper DeepSeek model with five samples is more cost-effective and solves\nmore issues than paying a premium for one sample from GPT-4o or Claude 3.5\nSonnet. Interestingly, the relationship between coverage and the number of\nsamples is often log-linear and can be modelled with an exponentiated power\nlaw, suggesting the existence of inference-time scaling laws. Finally, we find\nthat identifying correct samples out of many generations remains an important\ndirection for future research in domains without automatic verifiers. When\nsolving math word problems from GSM8K and MATH, coverage with Llama-3 models\ngrows to over 95% with 10,000 samples. However, common methods to pick correct\nsolutions from a sample collection, such as majority voting or reward models,\nplateau beyond several hundred samples and fail to fully scale with the sample\nbudget.",
        "updated": "2024-07-31 17:57:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21787v1"
    }
]