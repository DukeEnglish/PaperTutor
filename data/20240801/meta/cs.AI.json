[
    {
        "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
        "authors": "Atsuyuki MiyaiJingkang YangJingyang ZhangYifei MingYueqian LinQing YuGo IrieShafiq JotyYixuan LiHai LiZiwei LiuToshihiko YamasakiKiyoharu Aizawa",
        "links": "http://arxiv.org/abs/2407.21794v1",
        "entry_id": "http://arxiv.org/abs/2407.21794v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21794v1",
        "summary": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD\ndetection, and OD in the VLM era. Our framework reveals that, with some field\ninactivity and integration, the demanding challenges have become OOD detection\nand AD. In addition, we also highlight the significant shift in the definition,\nproblem settings, and benchmarks; we thus feature a comprehensive review of the\nmethodology for OOD detection, including the discussion over other related\ntasks to clarify their relationship to OOD detection. Finally, we explore the\nadvancements in the emerging Large Vision Language Model (LVLM) era, such as\nGPT-4V. We conclude this survey with open challenges and future directions.",
        "updated": "2024-07-31 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21794v1"
    },
    {
        "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
        "authors": "Richard RenSteven BasartAdam KhojaAlice GattiLong PhanXuwang YinMantas MazeikaAlexander PanGabriel MukobiRyan H. KimStephen FitzDan Hendrycks",
        "links": "http://arxiv.org/abs/2407.21792v1",
        "entry_id": "http://arxiv.org/abs/2407.21792v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21792v1",
        "summary": "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with upstream model capabilities, potentially\nenabling \"safetywashing\" -- where capability improvements are misrepresented as\nsafety advancements. Based on these findings, we propose an empirical\nfoundation for developing more meaningful safety metrics and define AI safety\nin a machine learning research context as a set of clearly delineated research\ngoals that are empirically separable from generic capabilities advancements. In\ndoing so, we aim to provide a more rigorous framework for AI safety research,\nadvancing the science of safety evaluations and clarifying the path towards\nmeasurable progress.",
        "updated": "2024-07-31 17:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21792v1"
    },
    {
        "title": "Vision-Language Model Based Handwriting Verification",
        "authors": "Mihir ChauhanAbhishek SatbhaiMohammad Abuzar HashemiMir Basheer AliBina RamamurthyMingchen GaoSiwei LyuSargur Srihari",
        "links": "http://arxiv.org/abs/2407.21788v1",
        "entry_id": "http://arxiv.org/abs/2407.21788v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21788v1",
        "summary": "Handwriting Verification is a critical in document forensics. Deep learning\nbased approaches often face skepticism from forensic document examiners due to\ntheir lack of explainability and reliance on extensive training data and\nhandcrafted features. This paper explores using Vision Language Models (VLMs),\nsuch as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By\nleveraging their Visual Question Answering capabilities and 0-shot\nChain-of-Thought (CoT) reasoning, our goal is to provide clear,\nhuman-understandable explanations for model decisions. Our experiments on the\nCEDAR handwriting dataset demonstrate that VLMs offer enhanced\ninterpretability, reduce the need for large training datasets, and adapt better\nto diverse handwriting styles. However, results show that the CNN-based\nResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach\nwith GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:\n71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings\nhighlight the potential of VLMs in generating human-interpretable decisions\nwhile underscoring the need for further advancements to match the performance\nof specialized deep learning models.",
        "updated": "2024-07-31 17:57:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21788v1"
    },
    {
        "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
        "authors": "Bradley BrownJordan JuravskyRyan EhrlichRonald ClarkQuoc V. LeChristopher RéAzalia Mirhoseini",
        "links": "http://arxiv.org/abs/2407.21787v1",
        "entry_id": "http://arxiv.org/abs/2407.21787v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21787v1",
        "summary": "Scaling the amount of compute used to train language models has dramatically\nimproved their capabilities. However, when it comes to inference, we often\nlimit the amount of compute to only one attempt per problem. Here, we explore\ninference compute as another axis for scaling by increasing the number of\ngenerated samples. Across multiple tasks and models, we observe that coverage -\nthe fraction of problems solved by any attempt - scales with the number of\nsamples over four orders of magnitude. In domains like coding and formal\nproofs, where all answers can be automatically verified, these increases in\ncoverage directly translate into improved performance. When we apply repeated\nsampling to SWE-bench Lite, the fraction of issues solved with\nDeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250\nsamples, outperforming the single-attempt state-of-the-art of 43% which uses\nmore capable frontier models. Moreover, using current API pricing, amplifying\nthe cheaper DeepSeek model with five samples is more cost-effective and solves\nmore issues than paying a premium for one sample from GPT-4o or Claude 3.5\nSonnet. Interestingly, the relationship between coverage and the number of\nsamples is often log-linear and can be modelled with an exponentiated power\nlaw, suggesting the existence of inference-time scaling laws. Finally, we find\nthat identifying correct samples out of many generations remains an important\ndirection for future research in domains without automatic verifiers. When\nsolving math word problems from GSM8K and MATH, coverage with Llama-3 models\ngrows to over 95% with 10,000 samples. However, common methods to pick correct\nsolutions from a sample collection, such as majority voting or reward models,\nplateau beyond several hundred samples and fail to fully scale with the sample\nbudget.",
        "updated": "2024-07-31 17:57:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21787v1"
    },
    {
        "title": "The Llama 3 Herd of Models",
        "authors": "Abhimanyu DubeyAbhinav JauhriAbhinav PandeyAbhishek KadianAhmad Al-DahleAiesha LetmanAkhil MathurAlan ScheltenAmy YangAngela FanAnirudh GoyalAnthony HartshornAobo YangArchi MitraArchie SravankumarArtem KorenevArthur HinsvarkArun RaoAston ZhangAurelien RodriguezAusten GregersonAva SpataruBaptiste RoziereBethany BironBinh TangBobbie ChernCharlotte CaucheteuxChaya NayakChloe BiChris MarraChris McConnellChristian KellerChristophe TouretChunyang WuCorinne WongCristian Canton FerrerCyrus NikolaidisDamien AllonsiusDaniel SongDanielle PintzDanny LivshitsDavid EsiobuDhruv ChoudharyDhruv MahajanDiego Garcia-OlanoDiego PerinoDieuwke HupkesEgor LakomkinEhab AlBadawyElina LobanovaEmily DinanEric Michael SmithFilip RadenovicFrank ZhangGabriel SynnaeveGabrielle LeeGeorgia Lewis AndersonGraeme NailGregoire MialonGuan PangGuillem CucurellHailey NguyenHannah KorevaarHu XuHugo TouvronIliyan ZarovImanol Arrieta IbarraIsabel KloumannIshan MisraIvan EvtimovJade CopetJaewon LeeJan GeffertJana VranesJason ParkJay MahadeokarJeet ShahJelmer van der LindeJennifer BillockJenny HongJenya LeeJeremy FuJianfeng ChiJianyu HuangJiawen LiuJie WangJiecao YuJoanna BittonJoe SpisakJongsoo ParkJoseph RoccaJoshua JohnstunJoshua SaxeJunteng JiaKalyan Vasuden AlwalaKartikeya UpasaniKate PlawiakKe LiKenneth HeafieldKevin StoneKhalid El-AriniKrithika IyerKshitiz MalikKuenley ChiuKunal BhallaLauren Rantala-YearyLaurens van der MaatenLawrence ChenLiang TanLiz JenkinsLouis MartinLovish MadaanLubo MaloLukas BlecherLukas LandzaatLuke de OliveiraMadeline MuzziMahesh PasupuletiMannat SinghManohar PaluriMarcin KardasMathew OldhamMathieu RitaMaya PavlovaMelanie KambadurMike LewisMin SiMitesh Kumar SinghMona HassanNaman GoyalNarjes TorabiNikolay BashlykovNikolay BogoychevNiladri ChatterjiOlivier DuchenneOnur ÇelebiPatrick AlrassyPengchuan ZhangPengwei LiPetar VasicPeter WengPrajjwal BhargavaPratik DubalPraveen KrishnanPunit Singh KouraPuxin XuQing HeQingxiao DongRagavan SrinivasanRaj GanapathyRamon CaldererRicardo Silveira CabralRobert StojnicRoberta RaileanuRohit GirdharRohit PatelRomain SauvestreRonnie PolidoroRoshan SumbalyRoss TaylorRuan SilvaRui HouRui WangSaghar HosseiniSahana ChennabasappaSanjay SinghSean BellSeohyun Sonia KimSergey EdunovShaoliang NieSharan NarangSharath RaparthySheng ShenShengye WanShruti BhosaleShun ZhangSimon VandenhendeSoumya BatraSpencer WhitmanSten SootlaStephane CollotSuchin GururanganSydney BorodinskyTamar HermanTara FowlerTarek SheashaThomas GeorgiouThomas ScialomTobias SpeckbacherTodor MihaylovTong XiaoUjjwal KarnVedanuj GoswamiVibhor GuptaVignesh RamanathanViktor KerkezVincent GonguetVirginie DoVish VogetiVladan PetrovicWeiwei ChuWenhan XiongWenyin FuWhitney MeersXavier MartinetXiaodong WangXiaoqing Ellen TanXinfeng XieXuchao JiaXuewei WangYaelle GoldschlagYashesh GaurYasmine BabaeiYi WenYiwen SongYuchen ZhangYue LiYuning MaoZacharie Delpierre CoudertZheng YanZhengxing ChenZoe PapakiposAaditya SinghAaron GrattafioriAbha JainAdam KelseyAdam ShajnfeldAdithya GangidiAdolfo VictoriaAhuva GoldstandAjay MenonAjay SharmaAlex BoesenbergAlex VaughanAlexei BaevskiAllie FeinsteinAmanda KalletAmit SanganiAnam YunusAndrei LupuAndres AlvaradoAndrew CaplesAndrew GuAndrew HoAndrew PoultonAndrew RyanAnkit RamchandaniAnnie FrancoAparajita SarafArkabandhu ChowdhuryAshley GabrielAshwin BharambeAssaf EisenmanAzadeh YazdanBeau JamesBen MaurerBenjamin LeonhardiBernie HuangBeth LoydBeto De PaolaBhargavi ParanjapeBing LiuBo WuBoyu NiBraden HancockBram WastiBrandon SpenceBrani StojkovicBrian GamidoBritt MontalvoCarl ParkerCarly BurtonCatalina MejiaChanghan WangChangkyu KimChao ZhouChester HuChing-Hsiang ChuChris CaiChris TindalChristoph FeichtenhoferDamon CivinDana BeatyDaniel KreymerDaniel LiDanny WyattDavid AdkinsDavid XuDavide TestuggineDelia DavidDevi ParikhDiana LiskovichDidem FossDingkang WangDuc LeDustin HollandEdward DowlingEissa JamilElaine MontgomeryEleonora PresaniEmily HahnEmily WoodErik BrinkmanEsteban ArcauteEvan DunbarEvan SmothersFei SunFelix KreukFeng TianFirat OzgenelFrancesco CaggioniFrancisco GuzmánFrank KanayetFrank SeideGabriela Medina FlorezGabriella SchwarzGada BadeerGeorgia SweeGil HalpernGovind ThattaiGrant HermanGrigory SizovGuangyiZhangGuna LakshminarayananHamid ShojanazeriHan ZouHannah WangHanwen ZhaHaroun HabeebHarrison RudolphHelen SukHenry AspegrenHunter GoldmanIgor MolybogIgor TufanovIrina-Elena VelicheItai GatJake WeissmanJames GeboskiJames KohliJaphet AsherJean-Baptiste GayaJeff MarcusJeff TangJennifer ChanJenny ZhenJeremy ReizensteinJeremy TeboulJessica ZhongJian JinJingyi YangJoe CummingsJon CarvillJon ShepardJonathan McPhieJonathan TorresJosh GinsburgJunjie WangKai WuKam Hou UKaran SaxenaKarthik PrasadKartikay KhandelwalKatayoun ZandKathy MatosichKaushik VeeraraghavanKelly MichelenaKeqian LiKun HuangKunal ChawlaKushal LakhotiaKyle HuangLailin ChenLakshya GargLavender ALeandro SilvaLee BellLei ZhangLiangpeng GuoLicheng YuLiron MoshkovichLuca WehrstedtMadian KhabsaManav AvalaniManish BhattMaria TsimpoukelliMartynas MankusMatan HassonMatthew LennieMatthias ResoMaxim GroshevMaxim NaumovMaya LathiMeghan KeneallyMichael L. SeltzerMichal ValkoMichelle RestrepoMihir PatelMik VyatskovMikayel SamvelyanMike ClarkMike MaceyMike WangMiquel Jubert HermosoMo MetanatMohammad RastegariMunish BansalNandhini SanthanamNatascha ParksNatasha WhiteNavyata BawaNayan SinghalNick EgeboNicolas UsunierNikolay Pavlovich LaptevNing DongNing ZhangNorman ChengOleg ChernoguzOlivia HartOmkar SalpekarOzlem KalinliParkin KentParth ParekhPaul SaabPavan BalajiPedro RittnerPhilip BontragerPierre RouxPiotr DollarPolina ZvyaginaPrashant RatanchandaniPritish YuvrajQian LiangRachad AlaoRachel RodriguezRafi AyubRaghotham MurthyRaghu NayaniRahul MitraRaymond LiRebekkah HoganRobin BatteyRocky WangRohan MaheswariRuss HowesRuty RinottSai Jayesh BonduSamyak DattaSara ChughSara HuntSargun DhillonSasha SidorovSatadru PanSaurabh VermaSeiji YamamotoSharadh RamaswamyShaun LindsayShaun LindsaySheng FengShenghao LinShengxin Cindy ZhaShiva ShankarShuqiang ZhangShuqiang ZhangSinong WangSneha AgarwalSoji SajuyigbeSoumith ChintalaStephanie MaxStephen ChenSteve KehoeSteve SatterfieldSudarshan GovindaprasadSumit GuptaSungmin ChoSunny VirkSuraj SubramanianSy ChoudhurySydney GoldmanTal RemezTamar GlaserTamara BestThilo KohlerThomas RobinsonTianhe LiTianjun ZhangTim MatthewsTimothy ChouTzook ShakedVarun VontimittaVictoria AjayiVictoria MontanezVijai MohanVinay Satish KumarVishal ManglaVlad IonescuVlad PoenaruVlad Tiberiu MihailescuVladimir IvanovWei LiWenchen WangWenwen JiangWes BouazizWill ConstableXiaocheng TangXiaofang WangXiaojian WuXiaolan WangXide XiaXilun WuXinbo GaoYanjun ChenYe HuYe JiaYe QiYenda LiYilin ZhangYing ZhangYossi AdiYoungjin NamYuWangYuchen HaoYundi QianYuzi HeZach RaitZachary DeVitoZef RosnbrickZhaoduo WenZhenyu YangZhiwei Zhao",
        "links": "http://arxiv.org/abs/2407.21783v1",
        "entry_id": "http://arxiv.org/abs/2407.21783v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21783v1",
        "summary": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "updated": "2024-07-31 17:54:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21783v1"
    }
]