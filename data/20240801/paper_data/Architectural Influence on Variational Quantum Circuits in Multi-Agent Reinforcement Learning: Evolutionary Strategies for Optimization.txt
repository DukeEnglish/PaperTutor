Architectural Influence on Variational Quantum
Circuits in Multi-Agent Reinforcement Learning:
Evolutionary Strategies for Optimization
Michael Kölle1, Karola Schneider1, Sabrina Egger1, Felix Topp1, Thomy
Phan2, Philipp Altmann1, Jonas Nüßlein1, and Claudia Linnhoff-Popien1
1 LMU Munich, Oettingenstraße 67, 80538 Munich, Germany
2 Thomas Lord Department of Computer Science, University of Southern California,
Los Angeles, USA
michael.koelle@ifi.lmu.de
Abstract. Inrecentyears,Multi-AgentReinforcementLearning(MARL)
hasfoundapplicationinnumerousareasofscienceandindustry,suchas
autonomous driving, telecommunications, and global health. Neverthe-
less, MARL suffers from, for instance, an exponential growth of dimen-
sions.Inherentpropertiesofquantummechanicshelptoovercomethese
limitations,e.g.,bysignificantlyreducingthenumberoftrainableparam-
eters. Previous studies have developed an approach that uses gradient-
freequantumReinforcementLearningandevolutionaryoptimizationfor
variationalquantumcircuits(VQCs)toreducethetrainableparameters
and avoid barren plateaus as well as vanishing gradients. This leads to
asignificantlybetterperformanceofVQCscomparedtoclassicalneural
networkswithasimilarnumberoftrainableparametersandareduction
in the number of parameters by more than 97 % compared to similarly
goodneuralnetworks.WeextendanapproachofKölleetal.bypropos-
ing a Gate-Based, a Layer-Based, and a Prototype-Based concept to
mutateandrecombineVQCs.Ourresultsshowthebestperformancefor
mutation-onlystrategiesandtheGate-Basedapproach.Inparticular,we
observeasignificantlybetterscore,highertotalandowncollectedcoins,
as well as a superior own coin rate for the best agent when evaluated in
the Coin Game environment.
Keywords: Quantum Reinforcement Learning · Multi-Agent Systems
· Evolutionary Optimization · Variational Quantum Circuits · Architec-
ture Search
1 Introduction
Artificial intelligence (AI) is currently on everyone’s lips. This is due to the fact
that it is being used to find innovative solutions in many areas. AI is now a
player in industry, healthcare, transportation, and education, for example, as it
contributes to the progress of many current technologies [16].
But not only single-agent settings are needed; Multi-Agent Systems (MAS)
are also central elements in these application areas. Although these agents are
4202
luJ
03
]hp-tnauq[
1v93702.7042:viXra2 M. Kölle et al.
inherently designed to act independently, they can be orchestrated to cooperate
effectively through Multi-Agent Reinforcement Learning (MARL). MARL has
shown significant efficacy, particularly in addressing social dilemmas [19].
Reinforcement Learning (RL) algorithms, which constitute a subdomain of
AI, are already able to outperform humans in domains like video games [1],
[29]. At the same time, quantum computing is an emerging technology that
allows for faster problem-solving and more efficient training in RL [13]. Nev-
ertheless, Quantum Reinforcement Learning faces new challenges, for example
barren plateaus or vanishing gradients [9], [4]. Research faces these problems,
e.g., by using evolutionary optimization methods that provide promising results
[4]. This suggested the combination of MARL and quantum techniques, which
leads to Multi-Agent Quantum Reinforcement Learning (MAQRL).
Inthisstudy,weemployVariationalQuantumCircuits(VQCs)tomodeleach
agent,leveragingtheirversatilityandcapabilitytoencodecomplexinformation.
VQCs are particularly suitable for representing agent behaviors in varied envi-
ronments due to their inherent flexibility. To optimize the performance of these
quantumcircuits,weutilizeanevolutionaryalgorithm,whichiterativelyadjusts
the VQC parameters to enhance decision-making and interactions. While evo-
lutionary algorithms are traditionally used in classical contexts, applying them
to the quantum domain opens up new possibilities for efficiently exploring the
extensive parameter space of VQCs.
TofindoutifandhowevolutionaryoptimizationcanbeusedinMAQRL,we
evaluate multiple generational evolutionary strategies. We compare the results
with each other to find out which approaches work best to leverage superior
performance from the VQCs. For robustness and fairness, all experiments are
conducted in a cooperative Coin Game setting, as it is a regarded Multi-Agent
environment.
Our contribution includes the application of evolutionary optimization in
a quantum Multi-Agent Reinforcement Learning setting. We introduce three
evolutionary strategies for Variational Quantum Circuits and evaluate them in
a Reinforcement Learning setting. Furthermore, we compare the results gained
from these experiments with a static baseline.
This work is structured as follows: in Section 2, we give a short introduction
to Multi-Agent Reinforcement Learning, Evolutionary Optimization, Quantum
Computing, and the structure of Variational Quantum Circuits. Afterwards, we
highlight some work that is related to our study (Section 3). Then, we outline
theapproachbyKölleetal.[17]beforeweintroduceourapproach(Section4).In
Section5,weprovideinformationaboutourexperimentalsetup,andinSection6,
wesharetheresultsofourstudy.WeconcludeinSection7withashortsummary
andsuggestionsforfurtherresearch.Allexperimentdataandcodecanbefound
here 3.
3 https://github.com/karolaschneider/vqc-opt-arch-evoArchitectural Influence on VQC in Evolutionary MARL 3
2 Preliminaries
Inthissection,weprovidefoundationalconceptsnecessaryforunderstandingthe
multi-agent setting and the methodologies employed in our research. We begin
bydiscussingtheframeworkofMarkovgames,whichunderpinstheinteractions
among multiple agents. This is followed by an overview of independent learning
inMARL,highlightingthechallengesanddynamicsinvolved.Next,wedelveinto
thespecificsofevolutionaryoptimizationanditsapplicationinMARL,drawing
inspiration from natural selection to improve agent performance. Finally, we
cover the basics of quantum computing and VQCs, which are integral to our
approach.
2.1 Multi-Agent Setting
We focus on Markov games M = D,S,A,P,R , where D = 1,...,N repre-
sentsasetofagentsi,S isasetofs⟨tatess attim⟩estept,andA{ = A ,.} ..,A
t 1 N
is the set of joint actions a = a . The transition probabilit⟨y is denoted⟩
t t,i i∈D
as P(s s ,a ), and the joint⟨rewa⟩rd is r ,...,r = R(s ,a ) R. The
t+1 t t t,1 t,N t t
action sele|ction probability for agent i is ⟨represented b⟩y the individu∈al policy
π(a s ).
t,i t
Th|epolicyπ istypicallyevaluatedusingavaluefunctionVπ(s )=E [G s ]
for all s S,
wi
here G = (cid:80)∞ γkr represents the
ini divt
idual
aπ ndt, di |is-t
countedt re∈ turn of agentt,i i Dk= w0 ith at+ dk, ii scount factor γ [0,1), and π =
π ,...,π is the joint poli∈ cy of the multi-agent system (M∈AS). The goal for
1 N
⟨ ⟩
agent i is to find the best response π∗ with V∗ =max V⟨πi,π−i⟩ for all s S,
where π denotes the joint policy exi cluding i agent i. πi i t ∈
−i
We define the efficiency of a MAS or utilitarian metric (U) by the sum of
all individual rewards until time step T:
(cid:88)
U = R , (1)
i
i∈D
where R = (cid:80)T−1r is the undiscounted return or the sum of rewards for
i t=0 t,i
agent i starting from the initial state s .
0
2.2 Multi-Agent Reinforcement Learning
In our research, we focus on independent learning, where each agent i optimizes
its individual policy π based on its own information, such as a and r , using
i i,j i,j
reinforcement learning (RL) techniques. For instance, we employ evolutionary
optimization as described in Section 2.3 on Evolutionary Optimization.
Independent learning introduces non-stationarity because the agents adapt
simultaneously,continuouslyalteringtheenvironmentdynamicsfromeachagent’s
perspective [21], [18], [14]. This non-stationarity can lead to the development of
overlygreedyandexploitativepolicies,causingagentstodefectfromcooperative
behavior [19], [8].4 M. Kölle et al.
2.3 Evolutionary Optimization
Inspiredbytheprocessofnaturalselection,evolutionaryoptimizationhasdemon-
stratedefficacyinsolvingcomplexproblemswheretraditionalmethodsmayfall
short[33].Thisapproachutilizesapopulationofindividuals,eachrandomlygen-
erated with a unique set of parameters. These individuals are assessed based on
a fitness function that evaluates their performance on the given problem. The
most fit individuals are then selected for reproduction, with their parameters
recombined and mutated to create a new population for the next generation [7].
Evolutionary optimization techniques, such as genetic algorithms [15], have
been successfully applied in various fields. These include the optimization of
neural networks and interactive recommendation tasks [6], [10]. Moreover, these
methods have been employed to address a broad spectrum of challenges, from
designingquantumcircuitarchitecturestooptimizingcomplexreal-worlddesigns
[22], [2].
2.4 Quantum Computing
Quantum computing is an emerging field in computer science that leverages
the principles of quantum mechanics to process information. Unlike classical
computers, which use bits to store and process data, quantum computers use
quantum bits, or qubits, which can exist in multiple states simultaneously due
to the principle of superposition [34].
A qubit’s state, denoted as Ψ , can be expressed as a linear combination of
the basis states 0 and 1 : | ⟩
| ⟩ | ⟩
Ψ =α 0 +β 1 , (2)
| ⟩ | ⟩ | ⟩
where α and β are complex coefficients that satisfy the normalization condi-
tion:
α2+ β 2 =1. (3)
| | | |
Uponmeasurement,aqubitinthesuperpositionstateα 0 +β 1 collapsesto
oneofthebasisstates, 0 or 1 ,withprobabilitiesdetermine|d⟩byt|he⟩magnitudes
of the coefficients, α2|a⟩nd | β⟩ 2, respectively. This process marks the transition
fromaquantumsup|e|rpositio|n|toadefiniteclassicalstate,wheretheobservable’s
value is precisely known [26].
Moreover, multiple qubits can be entangled, creating strong correlations be-
tween them. Entanglement allows the qubits to be interconnected in such a way
that the state of one qubit directly influences the state of another, regardless of
the distance separating them. This property is pivotal for the enhanced compu-
tational power of quantum systems, enabling them to solve complex problems
more efficiently than classical computers.
2.5 Variational Quantum Circuits
Variational Quantum Circuits (VQCs), also known as parameterized quantum
circuits, are quantum algorithms designed as function approximators that areArchitectural Influence on VQC in Evolutionary MARL 5
trained using classical optimization methods. They are increasingly used as al-
ternativestoneuralnetworksinDeepReinforcementLearning(DeepRL)appli-
cations[4],[30],[5],[31],[3].AVQCtypicallycomprisesthreemainstages:State
Preparation, Variational Layers, and Measurement.
Inthestatepreparationphase,classicalinputdataisencodedintoaquantum
state through superposition. We employ Amplitude Embedding [23] to embed
classical data into the amplitudes of qubits. For instance, to embed a feature
vector x R3 into a 2-qubit quantum state Ψ =α 00 +β 01 +γ 10 +δ 11 ,
where α∈ 2+ β 2+ γ 2+ δ 2 =1, the feature vecto|r is⟩first|pa⟩dded|to⟩matc|h 2⟩ n
feature|s |(with| n| be|in|g th|e|number of qubits). Next, the padded feature vector
y is normalized such that (cid:80)2n−1 yk =1. Finally, we use the state preparation
k=0 ||y||
methodbyMottonenetal.[23]toencodethenormalizedfeaturevectorintothe
qubit state.
Thevariationallayersstageinvolvesrepeatedapplicationofsingle-qubitrota-
tionsandentanglinggates.Drawinginspirationfromthecircuit-centricclassifier
design[30],ourcircuitsincorporatethreesingle-qubitrotationgates(denotedas
θj,whereiisthequbitindexandj 0,1,2 istheindexofthesingle-qubitro-
tai tion gate) and CNOT gates as ent∈an{glers.}The architecture within the dashed
blueareainFig.1isrepeatedLtimes,whereLrepresentsthenumberoflayers.
The target qubit for the CNOT gate in each layer is determined by (i+l)mod
n, with l indicating the current layer.
Variational Layer
R(α ,β ,γ )
1 1 1
R(α ,β ,γ )
2 2 2
R(α ,β ,γ )
3 3 3
|0⟩ U(x)
R(α ,β ,γ )
4 4 4
R(α ,β ,γ )
5 5 5
R(α ,β ,γ )
6 6 6
Fig.1: Variational Quantum Circuit used by Kölle et al. [17]
In the final stage, the expectation value is measured in the computational
basis (z) for the first k qubits, where k corresponds to the dimensions of the6 M. Kölle et al.
agents’ action space. Each measured expectation value is assigned a bias, which
is included in the VQC parameters and updated during training.
These stages enable VQCs to effectively approximate complex functions,
making them suitable for various quantum machine learning and optimization
tasks. The adaptability of VQCs in modifying their parameters and structures
allows them to tackle computationally demanding problems that classical algo-
rithms struggle to solve.
3 Related Work
Our work extends the approach for using evolutionary optimization for quan-
tum Reinforcement Learning using VQCs by Kölle et al. [17]. They, in turn,
were inspired by Chen et al. [4] who showed that it is possible to reduce pa-
rameter requirement in comparison to classical RL when leveraging VQCs for
Q-value function approximation in Deep Q-Learning. Kölle et al. [17] then de-
veloped three gradient-free genetic variations of VQCs with MAQRL applying
evolutionary optimization. They compared the results of their agents with Neu-
ral Networks in the Coin Game [20], finding similar accomplishments with more
than 97 % less parameters within the VQC.
AnotherapproachtoQuantumMulti-AgentReinforcementLearningislever-
aging Quantum Boltzmann Machines (QBM) [25], [24]. This strategy builds on
anexistingmethodologywhereQBMoutperformsclassicalDeepRLintermsof
convergence speed, measured by the number of required time steps while using
Q-value approximation. The results suggest enhanced stability in learning and
the agents’ ability to attain optimal strategies in grid domains. Similarities to
the method employed here lie in the utilization of Q-values and grid domains as
testing environments.
Yun et al. [35] employ Quantum Multi-Agent Reinforcement Learning us-
ing centralized learning and decentralized execution to avoid the challenges of
the NISQ era and the non-stationary properties from classic MARL. This ap-
proach achieves a superior overall reward in the tested environments compared
to classical methodologies using less parameters.
4 Approach
In this section we describe an idea of optimizing a θ parameterized agent using
the evolutionary approach by Kölle et al. [17]. In the next step, we extend this
conceptbyapplyingarchitecturalchangesandparameteradjustmentstoexplore
the impact of circuit architecture on the performance.
Kölle et al. [17] were inspired by Chen et al. [4], but in contrast, they use
a more general Multi-Agent setup to optimize the utilitarian metric U (Eq. 1).
For maximizing the fitness function, they use a population P, which consists of
η randomly initialized agents. To fully match the quantum circuits’ parameter
space, the agents are parameterized by θ [ π,π).
∈ −Architectural Influence on VQC in Evolutionary MARL 7
Contrary to past work, they use a Variational Quantum Circuit (VQC) in-
stead of neural networks for approximating the value of the agent’s action. This
primarilyaimstodemonstratetheimprovedparameterefficiency,evenwhenap-
pliedtocomplexlearningtasks.AVQCconsistsmainlyofthreecomponents:the
input embedding, the repeated variational layers, and the measurement. Fig. 1
illustrates the VQC employed by [17].
First,itisnecessarytotranslatetheclassicaldataintoaquantumstatewhich
can be achieved by using different embedding strategies like Basis Embedding,
AngleEmbeddingorAmplitudeEmbedding.Atthemoment,AmplitudeEmbed-
dings is, considering the high dimensionality of the state space, the only viable
embedding strategy that can embed the whole state information, enabling the
embedding of 2nq states in n
q
qubits.
Variational layers constitute the second part of VQCs and are variably re-
peated. Each iteration increases the number of α ,β ,γ parameters, which are
i i i
definedbyθanddepicteachindividualthatshouldbeoptimized.Inconsequence,
each layer has n = n 3 parameters, where n is the number of qubits. All
θ q q
rotations are performed s∗equentially as R (α ), R (β ), and R (γ ). Before the
Z i Y i Z i
parameterized rotations can be applied, each variational layer includes adjacent
CNOT gates to entangle all qubits.
After n repetitions of the variational layer, the third part follows: we can
l
measure the first n qubits, where n is the number of possible actions, to
a a
determinethepredictedvaluesoftheindividualactions.AZ-axismeasurementis
usedtodeterminetheQ-valueofthecorrespondingaction,andanagentchooses
the action with the greatest expected value.
To optimize the utilitarian metric U (Eq. 1), Kölle et al. [17] train the pa-
rameters θ with the help of an evolutionary algorithm which works as follows:
First, compute the fitness of each individual i by performing κ steps in the en-
vironment for each generation. Based on this fitness, select the τ best agents for
the development of the upcoming generation.
Second, for building a new population in the next generation, join mutation
andrecombinationpossibilities.Throughcrossover,recombinetheτ bestagents
of the current generation to new individuals. The descendants are the result
of a random selection of two parents and then crossing their parameters at a
randomly selected index. Alternatively, with a mutation step, new agents are
gained.Here,theparametersθ oftheτ bestagentsinthecurrentgenerationare
modified as follows:
θ =θ+σ ε (4)
∗
Again, the agents with the best fitness value T of the new generation will be
the parents of the next upcoming generation. For the mutation, the parameters
θ are modified like in Eq. 4 with the mutation power σ and the Gaussian noise
ϵ (, ).Asaconsequence,allθ parametersundergominormutations,and
i
ne∼wNage′n∞ts, called children, are generated.
Thethirdandlaststepineverygenerationistoaddtheunalteredeliteagent
to the child population. Algorithm 1 shows the EA training procedure proposed8 M. Kölle et al.
by Kölle et. al. [17] for finding the utilitarian metric U (Eq. 1) and Fig. 2 gives
a graphical overview of the training procedure.
Inizialization Evaluation Termination
Population of 250 Agents Coin Game Environment
Retain
best Agent
Mutation Selection
θ=θ+σ∗ε Top 5 Agents
Recombination
Layerwise Crossover,
Random Crossover
Fig.2: Training Loop of Kölle et al. [17]
Nowweextendthisapproachbyusingdifferentarchitecturalconceptsforthe
variationallayersandadditionally,applydifferenttechniquesfortheevolutionary
alternation of the circuit’s architecture: a Layer-Based concept, a Gate-Based
concept and a hybrid Prototype-Based concept.
4.1 Layer-Based Concept
The Layer-Based concept is inspired by Strongly Entangling Layers [30,11],
where all qubits are entangled. This is realized by applying a CNOT Gate to
everyqubitq andselectingqubitq modnasthetargetqubit.ThenR ,R
n n−1 X Y
and R rotation gates are performed for every qubit. In contrast to [17], dur-
Z
ing the evolutionary optimization process the number of layers can increase and
decrease due to mutation and recombination. Here, the evolutionary algorithm
createsnewcircuitsbyrecombiningtheinitiallayersofonecircuitwiththefinal
layersofanother.Mutationaltersthecircuit’sdesignbyeitheraddingorremov-
ing entire layers, thereby preserving the overall layer structure. During tests, we
foundthat theevolutionaryoptimization process works best whenstartingwith
circuits that consist of only one layer.
4.2 Gate-Based Concept
When applying the Gate-Based concept, we eliminate the organization of gates
into layers. Instead, the circuit is built by applying a specific number of ran-
domly sampled gates to randomly chosen qubits. These gates are selected fromArchitectural Influence on VQC in Evolutionary MARL 9
Data: Number of Generations µ, Population Size η, Evaluation Steps κ,
Truncation Selection τ, Mutation Power σ, and Number of Agents N
Result: Population P of optimized agents
P ← Initialize population η with random θ;
0
for g∈{0,1,...,µ−1} do
for i∈{0,1,...,η−1} do
Reset testing environment
Score S ←0
t=0,i
for t∈{0,1,...,κ−1} do
Use policy of agent i for all agents in env
Select action a ←argmaxVQC (s )
t θ t
Execute environment step with action a
t
Observe reward r and next state s
t t+1
S ←S +r
t,i t−1,i t
end
end
λ← Select top τ agents based on S
κ−1,i
Keep top agent based on S
κ−1,i
Recombine η−1 new agents out of λ
Mutate η−1 generated agents
P ←η−1 generated agents + top agent
g+1
end
Algorithm 1: Evolutionary Optimization Algorithm by Kölle et al. [17]
a predefined gate set (R , R , R , and CNOT), and are placed on a qubit q
X Y Z
within the range [0,n 1), where n represents the number of qubits in the
q q
VQC. For multi-qubit g−ates like the CNOT gate, an additional distinct qubit is
selectedtoserveasthecontrolqubit.Duringtheevolutionarystep,adjustments
are made at the gate level. This includes deleting, adding, or substituting gates,
as well as creating new circuits by recombining gates from two parent circuits.
4.3 Prototype-Based Concept
ThePrototype-BasedconceptmergestheLayer-BasedandGate-Basedapproaches
bycreatingacircuitwithrepeatedlayers,whilemanipulationsoccuratthegate
level. One layer of this circuit is constructed similarly to a Gate-Based Circuit,
andtheresultinggatearrangement,calledtheprototype,isthenrepeatedacross
alllayersofthecircuit.Theevolutionaryprocessesappliedtotheprototypework
the same as for the Gate-Based approach. This approach ensures, that the gate
composition remains consistent for all layers.
As we change the circuit architecture, we need to adjust some parameters
for the initialization of VQCs: Layer-Based circuits have a specific number of
initiallayersυ,Gate-Basedcircuitshaveaspecificnumberofinitialgatesχ,and
Prototype-Based circuits have a specific number of initial layers υ as well as a
specificnumberofinitialgatesperlayerχ.ThismeanswhenconstructingaGate-
Based or a Prototype-Based circuit, we have to randomly select χ gates from
our predefined gate set (R ,R ,R ,CNOT) and place them on χ randomly
X Y Z10 M. Kölle et al.
selected qubits within the circuit. For Prototype-Based circuits, this procedure
is repeated υ times. When construction a Layer-Based circuit, we need υ layers
in the predefined structure and randomly initialize the parameters of the gates
within the range [ π,π) to cover the whole parameter space of the circuit.
−
4.4 Evolutionary Algorithm
Initialization
Population of 250 Agents Evaluation Termination
Layer-, Gate-, or Prototype-Based Coin Game Environment
Retain
best Agent
Mutation Selection
θ=θ+σ∗ε Best Agent in Tournament
(Recombination)
Single-point crossover
Fig.3: Training loop of approach containing architectural changes
Like in Kölle et al. [17], for all of the three aforementioned concepts, two
different strategies for creating the next generation can be used: (1) recombina-
tion and mutation, where new individuals are formed by recombining selected
agents from the existing population. Following this, they are mutated based on
the mutation rate. (2) mutation only, where new individuals are created by mu-
tating selected agents from the current population. Both approaches lead to a
new generation of population size η, including the elite agent from the previous
generation. Algorithm 2 shows the employed EA, that extends the approach by
Kölleetal.[17]byapplyingarchitecturalchangesandFig.3depictsthetraining
process. In detail, this works as follows:
We use tournament selection to choose individuals from the population.
This process starts by randomly picking τ individuals from the population to
form a tournament. The fitness of each individual in the tournament is then
compared,andtheonewiththehighestfitnessisselected[32].Byincreasingthe
size of the tournament, the likelihood of selecting high-performing agents rises,
thereby boosting the selection pressure [7].
Weimplementelitismbycarryingtheunchangedfittestagentintothenext
generation [4]. This strategy ensures that advantageous traits are preserved,
preventing a decline in performance and promoting continuous improvement
within the population [12].Architectural Influence on VQC in Evolutionary MARL 11
Data: Number of Generations µ, Population Size η, Evaluation Steps κ,
Tournament Size τ, Mutation Rate ϕ, Parameters θ, and Number of
Agents in game N
Result: Population P containing an optimized agent
P ← Initialize population η with random θ;
0
for g∈{0,1,...,µ−1} do
for i∈{0,1,...,η−1} do
Reset testing environment
Score S ←0
t=0,i
for t∈{1,2,...,κ} do
j ←iterate through {1,2,...,N}
Select action a ←argmaxVQC (s )
t θi t
Execute action a for agent j
t
Observe reward r and next state s
t t+1
S ←S +r
t,i t−1,i t
end
end
α←best agent based on S and circuit size
κ−1,i
Recombine η−1 new agents using tournament selection with size τ
Mutate η−1 generated agents according to mutation rate ϕ
P ←η−1 generated agents +α
g+1
end
Algorithm 2:EvolutionaryOptimizationAlgorithmofapproachcontaining
architectural changes
Recombinationgeneratesnewindividuals,alsocalledoffspring,bymerging
the genetic information of existing individuals, called parents [7]. In our study,
this genetic information includes both the architecture and parameters of the
VQCs. We utilize a variant of single-point crossover for this purpose, where the
geneticdatafromtwoparentindividualsissplitataspecificpointandthenrear-
ranged to form new offspring [32]. This process aims to carry forward beneficial
traits from the parents while exploring the solution space [7]. We choose a cut
pointthatisvalidandpositionedbetweenlayers(Layer-Basedconcept)orgates
(Gate-BasedandPrototype-Basedconcepts).Thiscutpointmustfallwithinthe
rangeofbothparentcircuitsandalloweachcircuittobedividedintoafrontand
a back section, preventing splits at the very beginning or end. In cases where a
circuit contains only a single layer or gate, the cut point is positioned just after
thatlayerorgate.Thisprecautionensuresthatrecombinationdoesnotyieldan
empty circuit, even when merging two single-layer or single-gate circuits. Two
offspring are produced by merging the initial segment of one parent circuit with
the final segment of the other parent circuit. For Prototype-Based circuits, the
recombination is applied to the prototype analogously to Gate-Based circuits
and offspring is created by repeating the recombination n times to retain the
layer concept. Furthermore, the parameters of each gate are also recombined in
the same way.12 M. Kölle et al.
The mutation modifies an individual’s genetic information [7]. The degree
of these alterations can be regulated using the mutation power σ [4], while the
mutation rate ϕ controls how frequently mutations are applied [7]. In our study,
boththeparametersandthearchitectureofaVQCaresubjecttomutation.The
process of parameter mutation is consistent across all architectural variations,
includingtheapproachbyKöleetal.[17].Theprocessofarchitecturalmutation
withmutationrateσ differsdependingonthecircuits’concept:inLayer-Based
a
circuits mutations add or delete whole layers, where the addition can happen
at any position before and after an existing layer. However, it is necessary to
meet the rules for layer composition. In Gate-Based circuits mutations are able
to remove or replace an existing gate, or to insert a new gate at any position
withinthecircuit.Again,thisisrestrictedtofollowingtherulesforplacinggates
in the circuit. In Prototype-Based circuits, mutations occur in the same manner
as in Gate-Based circuits, affecting the prototype of the circuit according to the
same rules.
5 Experimental Setup
In this section, we describe details about our experimental setup we use to eval-
uate the evolutionary algorithm introduced in Section 4. We apply the afore-
mentioned strategies for creating different architectural circuits. The following
parts include information about our game environment, baselines, metrics, and
hyperparameters we use for training.
5.1 Coin Game Environment
We use the Coin Game [20] environment in a 3x3 gridworld version as it has a
comparatively small observation space. The use of the short evaluation environ-
ment is due to the fact that we are currently in the Noisy Intermediate-Scale
Quantum(NISQ)era,wherewecanonlysimulatearestrictedamountofqubits,
thus limiting the data we can embed into a quantum circuit [28].
In the Coin Game, both the Red and Blue agents aim for collecting coins,
with a single coin placed on a free cell in the grid. The color of the coin always
corresponds to one of the agents’ colors. An example state of this well-known
sequentialgamethatissuitedforevaluatingRLstrategiesisillustratedinFig.4.
When an agent occupies the same position as a coin, the coin is collected.
Once a coin is collected, a new one appears at a random location that is not
occupied by an agent, and it is again either red or blue. Each game of the Coin
Gamelastsfor50steps,witheachagentgetting25turns.Thegoalistomaximize
the agents’ rewards.
The Coin Game can be played in both competitive and cooperative modes.
Inthecooperativemode,theagentthatcollectsacoinreceivesanrewardof+1.
Conversely, the second agent’s reward decreases by -2 if the first agent collects
a coin of their color. Therefore, considering the total reward, collecting one’s
own coin results in a common reward of +1, while collecting an opponent’s coinArchitectural Influence on VQC in Evolutionary MARL 13
Fig.4: Example State of the Coin Game [27].
leadsto-1.Thissetupencouragesagentstocollecttheirowncoinsandleavethe
opponent’s coins for the other agent to collect. If both agents act randomly, the
expected reward is zero, making the Coin Game a zero-sum game.
Each cell in the 3×3 gridworld can contain agent 1, agent 2, a red coin, or
a blue coin. Empty cells are not included in the observation since agents can
move into them freely. Agents can choose from four possible actions, provided
the move does not take them outside the 3×3 grid. The actions are numerically
coded: 0 for a step north, 1 for a step south, 2 for a step west, and 3 for a
step east. An action is considered legal and thus can be executed by a player
if the player does not exit the board and the field is unoccupied or contains a
coin.ToensurethattheVQCdoesnotselectillegalactions,theexpectedvalues
are normalized to the range [0,1] and are masked according to the rules of the
environment.
5.2 Baselines
VQCs serve as an alternative to classical neural networks in agent design. In
the first part of the evaluation, neural networks are used as agents because
they function as general approximators. The basic neural network consists of 2-
layers for this purpose. The first layer maps the input observations to a variable
number of hidden units x, and the second layer connects these hidden units to
thenumberofpossibleactions.ThisconfigurationyieldstheindividualQ-values
for each action, similar to the VQC approach. To prevent the selection of illegal
actions, the Q-values are adjusted using an action mask. Our focus is on this
specificneuralnetworkwithvaryinghiddenunitcounts,sothatthenetworkcan
be modified in many ways to affect the outcomes.
In the second part of our evaluation we use a static baseline, which consists
of a 8 layers of the Layer-Based approach where we only change the parameters
andnotthearchitecture.Forthebaseline,werunamutation-onlystrategywith
parameter mutation power σ =0.01.
p14 M. Kölle et al.
5.3 Metrics
WeevaluateourexperimentsintheCoinGameenvironmentusingthreemetrics:
Score, Total Collected Coins, and Own Coin Rate. In our study, the agents play
solely against themselves to simplify the evaluation of their performance.
1. Score (S ):Thismetricconsistsoftheundiscountedindividualrewardsr
n t,i
accumulated over all agents until timestep T 0..49 . It is calculated as:
∈{ }
T−1
(cid:88) (cid:88)
S = r (5)
n t,i
i∈{0,1} t=0
Here, i represents the agent and n (ranging from 0 to 199) denotes the
generation, averaged over five seeds. This score provides a comprehensive
indicator of the agents’ performance in the Coin Game environment.
2. Total Coins Collected (TC ): This metric tracks the total number of
n
coins collected c by all agents until timestep T 0..49 :
t,i
∈{ }
T−1
(cid:88) (cid:88)
TC = c (6)
n t,i
i∈{0,1} t=0
Similar to the Score metric, i represents the agent and n 0..199 the
generation, averaged over five seeds. This metric helps in und∈ers{tandin}g the
total collection performance of the agents.
3. Own Coins Collected (OC ):Thismetricisthesumofallcollectedcoins
n
thatmatchtheagent’sowncoloro untiltimestepT 0..49 accumulated
t,i
over all agents: ∈{ }
T−1
(cid:88) (cid:88)
OC = o (7)
n t,i
i∈{0,1} t=0
Again, similar to the previous metrics, i represents the agents and n
0..199 the generation, averaged over five seeds. ∈
4. O{ wn C} oin Rate (OCR ): Comparing the Own Coins Collected metric
n
with the Total Coins Collected allows us to gauge the level of cooperation
achieved, by determining:
T−1
OCR = (cid:88) (cid:88) o t,i (8)
n c
t,i
i∈{0,1} t=0
This approach provides a detailed evaluation of the agents’ behavior and
their ability to cooperate within the Coin Game environment.
5.4 Training and Hyperparameters
ForallourexperimentsintheCoinGameenvironment,wetraintheagentsover
µ=200generationswithapopulationsizeofη =250.MultipleexecutionshaveArchitectural Influence on VQC in Evolutionary MARL 15
demonstrated that 200 generations are adequate for the evolutionary process to
stabilize and produce an optimized agent. Furthermore, we accordingly choose
the number of individuals in a generation to ensure diversity within the pop-
ulation to successfully explore the solution space and simultaneously not loose
computational effort. The agents play against themselves in games of 50 steps,
with each agent taking 25 steps.
For the first part, evaluating a Multi-Agent setting in the Coin Game with-
out changing the circuit architecture, the VQC is configured with 4 Variational
Layers and n = 6 qubits to embed the 36 features of the Coin Game, result-
q
ing in 76 parameters. After a preliminary study, we set the mutation power to
σ =0.01. We select the top τ =5 agents to regenerate the following population.
For the second part, investigating different architectural concepts, we ad-
justed some hyperparameters depending on the used concept to guarantee a
goodperformance.Thevalueswerechosenafterashortstudy.Otherfundamen-
tal parameters are fixed throughout all experiments.
When evolution includes both recombination and mutation, the mutation
rate is set to σ = 0.1. For selection, the tournament size τ is set to 40% of the
population size µ, which means τ =100 for a population size of η =250. Again,
we use n =6 qubits.
q
For the Layer-Based circuits, we set the initial layer count to 1, resulting in
a total of 22 parameters, which includes 3 rotation angles per qubit and 4 bias
values.Prototype-Basedcircuitsbeginwith8layers,eachconsistingof18gates.
Given that each gate in the gate set has an equal probability of being selected
and 3 out of 4 gates are parameterized, the number of parameterized gates in
suchacircuitisapproximately108,resultinginanaverageof112parameters.A
Gate-Based circuit starts with a total of 70 gates. Considering the probabilities
of selecting parameterized gates, a Gate-Based circuit initially has about 52
parameterizedgates,resultinginapproximately56parametersatthebeginning.
ForLayer-BasedandPrototype-Basedcircuits,theparametermutationpower
is set to σ =0.05, while for Gate-Based circuits, it is set to σ =0.01. The ar-
p p
chitecture mutation power σ is set to 10 for Layer-Based and Prototype-Based
a
circuits, and for Gate-Based circuits, it is set to σ =1.
a
Each experiment is conducted using five different seeds (0 through 4) to en-
sure a more accurate performance assessment. Given the current limitations
of quantum hardware, we use the Pennylane DefaultQubit simulator for all
VQC executions. All experiments were run on nodes equipped with an Intel(R)
Core(TM) i5-4570 CPU @ 3.20GHz.
6 Results
Inthissection,wepresenttheresultsofourexperimentsintheCoinGameenvi-
ronment. We tested all approaches with mutation and recombination combined
andmutationonly.Forourfirstexperiments,weadditionallytestedtwoclassical
neuralnetworks.Theyconsistoutoftwohiddenlayers,onewithsize64x64and
the second one with size 3 x 4. We choose this size to gain better comparability16 M. Kölle et al.
of the model-size and performance ratios, because the number is close to the
numberofparametersweuseintheVCQapproaches.Furthermore,weincluded
agents in our tests that act randomly at every step to get a random baseline.
For the second part of our experiments, we also started with a combination of
mutation and recombination, as well as mutation only. A look at the results
of these experiments, again led us to continue with the mutation-only strategy
in the further experiments. We compare the results of the Layer-Based, Gate-
Based, and Prototype-Based circuits with each other, and additionally include
a static baseline, where the circuit architecture remains unchanged throughout
the generations.
6.1 MAQRL in Coin Game
Comparing Generational Evolution Strategies Our goal is to assess the
impact of different generational evolution strategies. First, we compare the per-
formance of a mutation-only strategy (Mu) with two combined strategies in-
volving both mutation and recombination. The first combined approach uses a
random-point crossover recombination strategy (RaReMu), where crossover oc-
curs at a randomly chosen point in the parameter vector. The second combined
strategyemploysalayer-wisecrossover(LaReMu),selectingarandomlayerand
applying the crossover after the last parameter of that layer. For all strategies,
the mutation power σ is fixed at 0.01.
Fig. 5 illustrates that the mutation-only strategy outperforms the combined
strategies.Themutation-onlyapproachbeginswithanaveragerewardof5,dips
slightly below 4 by the 17th generation, and then steadily increases, stabilizing
aroundascoreof7bythe140thgeneration.Incontrast,thelayer-wiserecombi-
nationstrategystartsat3.3,rapidlyascendsuntilthe30thgeneration,andthen
stabilizes, reaching an average reward of 6 by the 123rd generation with fluctu-
ations thereafter. The random crossover strategy starts near the mutation-only
strategy at 4.7, declines to 3 by the 17th generation, then climbs steadily until
the 131st generation, reaching a score of 6 but eventually settling around 5.5,
making it the least effective of the three methods.
We also evaluated the average number of coins collected during the exper-
iments. The mutation-only strategy consistently collects more coins, as shown
in Fig. 6a. While there are periods where all strategies have nearly identical
coin counts, the mutation-only strategy generally leads, with up to a 2-coin gap
at times. This is reflected in the coin rate (Fig. 6c), where the mutation-only
strategy shows a steady increase over generations, indicating enhanced agent
cooperation in the testing environment.
The comparison of both strategies lead to the result, that the mutation-
only strategy consistently achieves the highest coin rate and collects the most
coins, aligning well with our goal of maximizing the reward. The layer-wise
recombination strategy shows an initial surge in performance, correlating with
anincreaseincollectedcoinsandcoinrate,butitsadvantagediminishesafterthe
90th generation. For the random crossover strategy we observe that it collectsArchitectural Influence on VQC in Evolutionary MARL 17
VQC(148): Mu
8
VQC(148): LaReMu
VQC(148): RaReMu
7
6
5
4
3
2
1
0 25 50 75 100 125 150 175 200
Generation
Fig.5: Average Score over the entire population. Each individual has completed
50 steps in the Coin Game environment in each generation [17].
10 VQC(148):Mu 9 VQC(148):Mu 1.00 VQC(148):Mu
9 V VQ QC C( (1 14 48 8) ): :L Ra aR Re eM Mu u 8 V VQ QC C( (1 14 48 8) ): :L Ra aR Re eM Mu u 0.95 V VQ QC C( (1 14 48 8) ): :L Ra aR Re eM Mu u 8 7 0.90
7 6 0.85
6 5 0.80
5 4 0.75
4 3 0.70
3 2 0.65
2 0.60
0 25 50 75 Gen1e0r0ation 125 150 175 200 0 25 50 75 Gen1e0r0ation 125 150 175 200 0 25 50 75 Gen1e0r0ation 125 150 175 200
(a) Total coins collected (b) Own coins collected (c) Own coin rate
Fig.6:Comparisonof(a)averagecoinscollected,(b)averageowncoinscollected
andtheowncoinrate(c)ina50stepCoinGameeachgeneration,averagedover
10 seeds [17].
erocsegarevA
sniocdetcellocegarevA sniocnwodetcellocegarevA
etarniocnwO18 M. Kölle et al.
more coins than the layer-wise approach, but has a significantly lower coin rate,
resulting in lower overall rewards.
All in all, the mutation-only strategy outperforms the combined strategies
in our experiments, achieving the highest rewards and is best aligning with
our objective of maximizing reward. Consequently, subsequent experiments will
exclusively utilize the mutation-only approach for the VQCs.
Evaluating VQC Performance with Different Layer Counts We investi-
gate the performance dynamics of VQCs with varying layer counts: specifically,
4, 6, 8, and 16 layers. The relationship between layer counts and parameters is
governed by the formula 3 n 6+4, where n is the number of layers, There-
fore, VQCs with 4, 6, 8, an∗d 16∗ layers utilize 76, 112, 148, and 292 parameters
respectively.AllVQCsweretrainedusingthemutation-onlyapproach,withthe
mutation strength set to σ =0.01.
Fig. 7 shows that, except for the 4-layer VQC, which starts slightly below
3, all VQCs begin with scores between 5 and 5.5. Each VQC stabilizes around
a reward of 4 by the 25th generation. The 4-layer VQC gradually increases,
maintaining an average reward of 5 from the 175th generation onward. For the
6-layer VQC we observe a steady rise until the 62nd generation, with a more
pronounced increase thereafter, reaching a maximum at 6.7 around the 165th
generationandthenoscillatingaround6.5.The8-layerVQCconsistentlyoutper-
formstheothers,reachingarewardof5.5bythe70thgeneration,thenclimbing
to 7 by the 140th generation after a brief plateau. The 16-layer VQC displays
significant growth between the 25th and 70th generations, stabilizing around 6
before rising again to 6.5 around the 160th generation.
Next, we next examine the average coin collection in Fig. 8a to provide
a comprehensive understanding. The 4-layer VQC consistently leads the coin
collection metric, increasing from just below 7 to 8. The 6-layer VQC starts
at 6.5, dips to 5.2 by the 23rd generation, and then rises to 8 by the 165th
generation.Despiteachievingthehighestaveragereward,the8-layerVQCbegins
at 6 coins and only stabilizes around 8 coins after the 180th generation. The
16-layer VQC, rapidly increases from the 24th to the 103rd generation, briefly
declines, and then fluctuates around 8 coins. By the later generations, VQCs
with more than 4 layers converge to collect approximately 8 coins each.
Fig. 8c reveals for the own coin count that, except for the 4-layer VQC, all
VQCs initially decline before ascending. The 4-layer VQC shows a modest but
steadyrise,endingwithacountof6.5coins.The6-layerVQCisslowesttobegin
itsascent,eventuallystabilizingaround7.2coins.The8-layerVQCstartsrising
earlier, reaching about 7.5 coins by the end. The 16-layer VQC, known for its
rapidearlyrise,maintainsaround7coinsafterthe100thgeneration.The4-layer
VQC lags, collecting more than one own coin fewer than the other VQCs.
When focusing on the own coin rate, the 4-layer VQC performs the worst.
The 6-layer and 16-layer VQCs show similar performance patterns in terms of
own coin rate and overall reward. The 8-layer VQC stands out with the highest
own coin rate and comparable coin count, achieving the highest reward.Architectural Influence on VQC in Evolutionary MARL 19
VQC(76)
8 VQC(112)
VQC(148)
7 VQC(292)
6
5
4
3
2
0 25 50 75 100 125 150 175 200
Generation
Fig.7: Average Score over the entire population. Each individual has completed
50 steps in the Coin Game environment each generation. [17]
Our analysis identifies the 8-layer VQC as the top performer. As a result,
we will use this configuration, combined with the optimal evolutionary strategy
outlined in Section 6.1, for further experiments. This analysis highlights that a
higher layer count does not necessarily lead to better performance, as the 16-
layer VQC did not match the achievements of the 8-layer VQC. However, the
experimentsdonotconclusivelyestablishtheperformancedynamicsbeyond200
generations.
Comparing Quantum and Classical Approaches Comparative Analy-
sis of VQC Approaches vs. Random BaselineWefirstevaluatetheperfor-
10
9
V V
V
VQ Q
Q
QC C
C
C( (
(
(7 1
1
26 1
4
9) 2
8
2)
) )
89 V V
V
VQ Q
Q
QC C
C
C( (
(
(7 1
1
26 1
4
9) 2
8
2)
) )
001 ... 990 050 V V
V
VQ Q
Q
QC C
C
C( (
(
(7 1
1
26 1
4
9) 2
8
2)
) )
8 7 0.85
7 6 0.80
6 5 0.75
5 4 0.70
4 3 0.65
0 25 50 75 Gen1e0r0ation 125 150 175 200 0 25 50 75 Gen1e0r0ation 125 150 175 200 0 25 50 75 Gen1e0r0ation 125 150 175 200
(a) Total coins collected (b) Own coins collected (c) Own coin rate
Fig.8:Comparisonof(a)averagecoinscollected,(b)averageowncoinscollected
andtheowncoinrate(c)ina50stepCoinGameeachgeneration,averagedover
10 seeds. [17]
erocsegarevA
sniocdetcellocegarevA sniocnwodetcellocegarevA
etarniocnwO20 M. Kölle et al.
manceofourVQCmethodscomparedtoarandombaseline.AsshowninFig.9,
the random agents’ score hovers around zero, reflecting the zero-sum nature of
the cooperative sequential coin game. In contrast, VQC agents trained through
evolutionary methods significantly outperform random agents, achieving an av-
erage score around 7. Figure Fig. 10a illustrates that VQC agents effectively
learn to collect coins, unlike random agents. The number of own coins collected,
asshowninFig.10b,alignswiththeoverallcoinscollected,indicatingthatVQC
agents consistently outperform the random baseline. Interestingly, Fig. 10c re-
veals that cooperation does not increase over time for either approach. In sum-
mary, the trained VQC agents exhibit superior performance across all metrics,
confirming the efficacy of the training process.
8 Random
VQC(148): Mu
NN(147): Mu
6 NN(6788): Mu
4
2
0
2
−
0 25 50 75 100 125 150 175 200
Generation
Fig.9: Average Score over the entire population. Each individual has completed
50 steps in the Coin Game environment each generation.
VQC vs. Small Neural NetworkWithVQCmethodsprovingsuperiorto
random agents, we next compare their performance against a small neural net-
workwithacomparableparametercount.FollowingChenetal.[4],werecognize
that VQCs possess greater expressive power than traditional neural networks,
defined by their capacity to represent complex functions with fewer parameters.
Our VQC has 148 parameters (3 * 6 * 8 + 4), while the neural network, com-
posed of two hidden layers with dimensions 3 and 4, has 147 parameters. Both
models are trained using mutation only, with a mutation power σ =0.01.
InFig.9,theneuralnetwork’srewardsfluctuatebetween2.5and3,whilethe
VQC,despiteaslowerinitiallearningcurve,achievesasignificantlyhigherscore.
The small neural network’s limited number of hidden units likely accounts for
erocsegarevAArchitectural Influence on VQC in Evolutionary MARL 21
10 Random Random 1.0 Random
9 V NQ NC (1( 41 74 )8 :) M:M uu 8 V NQ NC (1( 41 74 )8 :) M:M uu V NQ NC (1( 41 74 )8 :) M:M uu
8 NN(6788):Mu NN(6788):Mu 0.8 NN(6788):Mu
7 6
6 0.6
5 4 0.4
4
2
3 0.2
2
0 25 50 75 Gen1e0r0ation 125 150 175 200 0 25 50 75 Gen1e0r0ation 125 150 175 200 0 25 50 75 Gen1e0r0ation 125 150 175 200
(a) Total coins collected (b) Own coins collected (c) Own coin rate
Fig.10:Comparisonof(a)averagecoinscollected,(b)averageowncoinscollected
andtheowncoinrate(c)ina50stepCoinGameeachgeneration,averagedover
10 seeds.
its poorer performance. Fig. 10a demonstrates that the neural network collects
fewercoinsthanrandomagentsuntilgeneration115,afterwhichitonlyslightly
surpassestheirperformance.Incomparison,theVQCconsistentlycollectsabout
twice as many coins. As shown in Fig. 10b, the neural network is able to out-
perform random agents in terms of own coins collected, leading to a better own
coinrate(Fig.10c).However,theVQCstillsignificantlyoutperformstheneural
networkwithnearlythesameparametercount,highlightingtheVQC’ssuperior
capabilities in this environment.
VQC vs. Larger Neural Network Given the superior performance of
VQCs with a similar parameter count, we next compare the VQC to a signifi-
cantly larger neural network. Both models continue using mutation-only evolu-
tion with a mutation power of σ =0.01. The larger neural network features two
hidden layers of size 64, resulting in 6788 parameters.
As depicted in Fig. 9, the VQC and the larger neural network achieve sim-
ilar results over time, with minor differences in average score from generation
50 onwards, both stabilizing around a score of 7. Despite having 46 times more
parameters, the larger neural network only marginally outperforms the VQC
initially but converges to a similar performance level in the long run. Fig. 10a
shows that the VQC starts with a higher initial coin collection, while the neu-
ral network compensates with a steeper learning curve, ultimately achieving a
comparable number of coins. In terms of own coins collected, shown in Fig. 10b,
both models perform similarly. Initially, the neural network achieves a higher
own coin rate around generation 25, but the VQC maintains a slight edge be-
tween generations 80 and 162, with the neural network slightly outperforming
the VQC towards the end (Fig. 10c).
In conclusion, despite the larger neural network having 46 times more pa-
rameters, the performance difference between the two models is minimal. This
demonstrates that VQCs can achieve comparable performance with a 97.88%
reduction in parameters, supporting the findings of Chen et al. [4] regarding the
expressive power of VQCs.
sniocdetcellocegarevA sniocnwodetcellocegarevA
etarniocnwO22 M. Kölle et al.
6.2 Architectural Variations
Inthesecondpartofourresultsweexaminehowdifferentarchitecturestrategies
during the evolution process influence the performance of the agents. Therefore,
weinvestigatetheefficiencyofrecombinationandmutation,andwecomparethe
Gate-Based, Layer-Based and Prototype-Based approaches against each other
lookingatthescore,coinandgatecountmetrics.Weconcludetheresultssection
with a look at the performance of a static baseline (i.e. an unchanged circuit
architecture throughout the whole evolution process) compared to architectural
evolution.
Assessment of Evolutionary Strategies: ReMu vs. Mu Inthissection,we
evaluatetheeffectivenessoftwoevolutionarystrategiesforgeneratingsuccessive
generations:recombinationcombinedwithmutation(ReMu)andmutation-only
(Mu). The goal is to understand the performance of these strategies to optimize
the evolutionary process in future experiments. As the performance trends of
recombination versus mutation-only are consistent across different approaches,
in our analysis we focus on the Gate-Based approach.
Our findings reveal a significant difference in the performance of the ReMu
and Mu strategies. As illustrated in Fig. 11, the recombination approach failed
toyieldgoodsolutioncandidates.ThebestscoreachievedbytheReMustrategy
declined from 4.5 to 1.3 by the 170th generation, with a minor recovery to just
above2,resultinginanoveralldeclineofmorethan2points.Incontrast,thebest
runofthemutation-onlyapproachshowedasteadyandconsistentimprovement,
with scores rising from 4.5 to almost 12 by generation 150.
FortheReMustrategy,thegapbetweentheaveragescoreandthebestscore
diminished over time, indicating a convergence towards lower-quality solutions.
Bythe125thgeneration,thebestscorewasonlyabout0.6pointsabovetheaver-
age. This suggests that the recombination method might introduce complexities
or incompatibilities that hinder the evolutionary process, leading to suboptimal
solutions and reduced diversity.
Incontrast,theconsistentupwardtrajectoryinperformanceofthemutation-
only strategy indicates a more efficient navigation of the solution space, leading
to the discovery of higher-quality solutions.
Basedonthesefindings,weconcludethatthemutation-onlystrategyismore
effectiveforourpurposes.TheReMustrategy’sintroductionofcomplexitiesap-
pearstohindertheevolutionaryprocess,leadingtolessoptimalsolutions.There-
fore,wewillexclusivelyemploythemutation-onlystrategyinfutureexperiments
to maximize the potential for discovering optimal solutions.
Comparative Analysis of Architectural Evolution Strategies Now, we
evaluate the performance of three Variational Quantum Circuit architecture
structuresandtheirrespectiveevolutionarystrategies:Layer-Based,Gate-Based,
andPrototype-Based.Eachapproachpresentsauniquedesignforthevariational
layers and employs distinct architectural changes within the Evolutionary Algo-
rithm (EA).Architectural Influence on VQC in Evolutionary MARL 23
Fig.11: Best and average score for the different evolutionary algorithm methods
averaged over 5 seeds.
Fig.12illustratestheevolutionofthescoreforallthreeapproaches.Initially,
the average scores start at 0. The Layer-Based and Prototype-Based approach
stabilize around 1 by generation 25. Notably, the Gate-Based approach reaches
thisstabilizationpointfaster,achievingitbythe6thgenerationandconsistently
maintaining a higher score than the other two approaches. The average score of
theLayer-Basedapproach,whileconsistentlylower,differsonlyslightlyfromthe
others.
When examining the best scores, the differences become more pronounced.
All approaches begin with a best score around 4. However, the Gate-Based ap-
proach rapidly ascends, reaching a best score of 10 by generation 35, which
already surpasses the final best scores (within 200 generations) of the Layer-
BasedandPrototype-Basedapproaches.TheGate-Basedapproachcontinuesto
improve,reachingnearly12bygeneration155.Incontrast,theLayer-Basedand
Prototype-Based approaches peak around 9 by generation 75 and show little
change afterward.
To gain further insight into the scores, we also analyze the total coins col-
lected and the own coin rate, which represents the percentage of collected coins
belonging to the player. Fig. 13a shows that, when looking at the best perform-
ing agents, the Gate-Based and Prototype-Based approaches perform similarly
in terms of total collected coins until generation 110, both reaching up to 12
coins. After this point, the Gate-Based approach increases to 12.6 coins, while
the Prototype-Based approach declines to 11.7. The Layer-Based approach, on24 M. Kölle et al.
Fig.12:Bestandaveragescoreforthedifferentarchitecturalapproachesaveraged
over 5 seeds.
the other hand, collects fewer coins than the Prototype-Based approach after
the 30th generation, plateauing at 11 coins by generation 90.
The own coin rate of the best agents (Fig. 13c) reveals that the Prototype-
Based approach starts strong at 0.95 but drops to 0.87 by the 10th generation,
stabilizingat0.89bygeneration115.TheLayer-Basedapproachperformsbetter
initially,reaching0.95bythe12thgeneration,butfallsbelow0.9,climbingback
to0.93bythe200thgeneration.TheGate-Basedapproach,however,consistently
outperforms the others starting from generation 17, achieving a rate of 0.97 by
generation 60. The average own coin rate for all approaches increases from 0.53
to 0.7, with the Gate-Based approach showing the steepest initial increase.
(a) Total coins collected (b) Own coins collected (c) Own coin rate
Fig.13: Comparison of best and average (a) total coins collected, (b) own coins
collected, and (c) own coin rate for the different architectural approaches aver-
aged over 5 seeds.Architectural Influence on VQC in Evolutionary MARL 25
These results indicate that the superior own coin rate of the Gate-Based
approach, along with a higher total coin collection, contributes to its higher
overall score. This suggests that the Gate-Based approach promotes a more
cooperative play style, leading to better performance in the Coin Game. The
flexibility of the Gate-Based architecture may enable more effective adaptation
to the task compared to the more restrictive Layer-Based and Prototype-Based
architectures.
To examine how circuit size evolves, Fig. 14 analyzes the total and param-
eterized gate counts of the best circuits for each approach: For the Gate-Based
approach the total gate count decreases from 70 to 53, and parameterized gates
from50to33.Incontrast,thetotalgatecountforthePrototype-Basedapproach
drops significantly from 144 to 53, and parameterized gates from 106 to 40. The
lowest number of initial gates shows the Layer-Based approach: Starting with
22 total gates and 18 parameterized gates, it rapidly increases to about 90 total
and 68 parameterized gates by generation 60, then slightly decreases to 80 and
60 gates by generation 80, before rising again by generation 180.
Fig.14: Evolution of total and parameterized gate count for the different archi-
tectural approaches averaged over 5 seeds.
TheGate-BasedandPrototype-Basedapproaches,whichallowmoreflexibil-
ity in gate composition, both settle around 53 total gates and 35 parameterized
gates. This suggests that a higher gate count does not necessarily correlate with
better performance. The results highlight that a more adaptable architecture,
like the Gate-Based approach, can outperform a larger, less flexible one. In the
current NISQ era, minimizing circuit sizes is crucial to reduce errors and noise26 M. Kölle et al.
and to benefit from shorter computation times. The ability of the algorithm to
maximize scores while reducing the number of parameterized gates accelerates
the evolutionary process, as fewer parameters require less time to evolve.
The rapid improvement and superior performance of the Gate-Based ap-
proach indicate that its flexible architecture provides a significant advantage in
exploringandoptimizingthesolutionspace.Thisapproach’sadaptabilityallows
for more dynamic changes, leading to better solution candidates. In contrast,
theLayer-BasedandPrototype-Basedapproaches,whileshowingimprovements,
are constrained by their less flexible architectures, resulting in lower scores and
slower progress. In summary, the Gate-Based approach, with its less restrictive
architecture, proves to be crucial for achieving higher performance and faster
convergence in VQCs. It outperforms the more constrained Layer-Based and
Prototype-Based approaches in both score and computational efficiency.
Comparison of Gate-Based VQC and Static Baseline Giventhesuperior
results of the Gate-Based approach in previous experiments, we compare it to
a static baseline to evaluate its efficiency and performance. We analyzed two
versions of the Gate-Based approach: one with 70 initial gates and a scaled-
down version with 50 initial gates.
Thestaticbaselineconsistsof8layersoftheLayer-BasedVQC,withonlypa-
rameter mutations (σ =0.01) applied, and generates the next generation from
p
the top 5 agents. The Gate-Based VQC uses a mutation power of σ = 0.01
p
for parameters and σ = 1 for architecture, employing 40% of the population
a
for tournament selection. Evaluations were conducted in the Coin Game envi-
ronment over κ = 50 steps, with a population size of η = 250 and µ = 200
generations.
We start with the comparison of a 70 gate Gate-Based VQC and the static
baseline. Fig. 15 shows the score progression over 200 generations. Despite the
Gate-Basedapproachhavingaloweraveragescorethroughout,ityieldedabetter
scoreforthebestagent.Thestaticbaseline’sbestscorestartsat4.8,risessharply
to8.8bygeneration50,andreaches10bygeneration140,plateauingafterward.
The average score rises from 0 to 5 in the first 75 generations, slowly increasing
to 6 by generation 200. The Gate-Based best score starts at 4.4, quickly reaches
10 by generation 40, and steadily grows to 11.8 by generation 150, remaining
steady afterward. The average score rises to slightly above 1 by generation 6,
staying constant until generation 200.
TheGate-Basedbestagentreachedthestaticbaseline’stopscoreof10afull
100 generations earlier and achieved a best agent score nearly 2 points higher
within 200 generations. This suggests that evolving the architecture allows for
exploring more optimal structures, leading to a more optimized solution candi-
date in fewer generations.
Now, we continue with the comparison of the 50 gate Gate-Based approach
andthestaticbaseline,wheretheresultsaredepictedinFig.16.Wecanidentify
the Gate-Based approach yielding comparable best scores to the static baseline.
The static baseline, which consists of 192 gates distributed among 8 layers onArchitectural Influence on VQC in Evolutionary MARL 27
Fig.15: Best and average score of Gate-Based approach (with 70 initial gates)
and static approach (without architectural changes) averaged over 5 seeds.
6 qubits, reaches a best score of 10, whereas the average score peaks at 6 by
generation 200. The Gate-Based approach starts with 50 gates that are reduced
toapproximately25gatesforthebestagentbygeneration120,duetothearchi-
tectural changes implemented through evolutionary processes. The best agent
scores similarly to the static baseline with a to score of 9.5. The average score
peaks at 1.7 by generation 12 and then stabilizes around 1.
Usingfewergatesreducescomputationalresourcerequirements,whichiscru-
cialintheNISQera.TheGate-Basedapproach’scomputationtimeaveraged8.06
hours per seed compared to 27.13 hours for the fixed architecture, representing
speedup of 70.29 %.
TheGate-BasedVQCwithaflexiblearchitectureachievesscoresforthebest
candidate that are similar to those of the static baseline while operating more
efficientlybyusingfewergatesandreducingcomputationtime.Thisunderscores
the benefits of adopting a flexible architectural approach in VQCs, which not
only improves performance but also reduces resource usage.
7 Conclusion
At the moment, MAQRL suffers from barren-plateaus and vanishing gradients
due to their gradient based training [4], [9]. Kölle et al. [17] extended an evo-
lutionary optimization process by Chen et al. [4] to a Multi-Agent setting and
used different evolutionary strategies. They proposed three approaches: layer-
wise crossover with mutation, random crossover with mutation and mutation28 M. Kölle et al.
Fig.16: Best and average score of Gate-Based approach (with 50 initial gates)
and static approach (without architectural changes) averaged over 5 seeds.
only. The evaluation in a 3x3 grid world Coin Game showed, that their VQC
approachperformedsignificantlybetterthanclassicneuralnetworkswithasim-
ilar amount of trainable parameters. Comparing the VQC with a much bigger
neuralnetworkresultedinasimilaroutcome.Here,itwaspossibletoreducethe
number of parameters by 97.88%, still preserving a similar performance, which
shows the effectiveness of VQC in MAQRL environments.
WeextendedthisapproachusingtheCoinGamebyproposingthreedifferent
evolutionary strategies, namely a Gate-Based, a Layer-Based and a Prototype-
Based approach. After testing recombination and mutation vs mutation only-
strategies, which resulted in a significant better performance of mutation-only
strategies,wecontinuedwithmutation-onlyinthenextexperiments.Whencom-
paring Gate-Based, Layer-Based and Prototype-Based approaches, we found
that on average, all three strategies lead to a similar score, but focusing on
the best agents reveals a better performance of the Gate-Based approach. The
Gate-Basedapproachalsoachievesbettervaluesforthetotalcoinscollected,own
coinscollectedand,owncoinrate.Additionally,thisapproachneededfewestpa-
rameterized gates. When comparing Gate-Based VQCs of different gate sizes
and a static baseline, the baseline meets higher score values on average, but the
best baseline agents’ score is almost equal to (50 gates) or even succumbs (70
gates)thebestGate-Basedagentscore.Insummaryweshowedthatwecansave
computational resources and still achieve good results at the same time using
architectural evolutionary strategies.Architectural Influence on VQC in Evolutionary MARL 29
For future work, the experiments could be run on real quantum hardware to
determine if there are differences in the results. Another interesting application
of the recombination strategies can be Large Language Models.
Acknowledgements
This research is part of the Munich Quantum Valley, which is supported by the
Bavarian state government with funds from the Hightech Agenda Bayern Plus.
References
1. Badia,A.P.,Piot,B.,Kapturowski,S.,Sprechmann,P.,Vitvitskyi,A.,Guo,Z.D.,
Blundell, C.: Agent57: Outperforming the atari human benchmark. In: Interna-
tional conference on machine learning. pp. 507–517. PMLR (2020)
2. Caldas, L.G., Norford, L.K.: A design optimization tool based on a genetic algo-
rithm. Automation in construction 11(2), 173–184 (2002)
3. Chen, S.Y.C.: Quantum deep recurrent reinforcement learning. In: ICASSP 2023-
2023 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). pp. 1–5. IEEE (2023)
4. Chen,S.Y.C.,Huang,C.M.,Hsing,C.W.,Goan,H.S.,Kao,Y.J.:Variationalquan-
tumreinforcementlearningviaevolutionaryoptimization.MachineLearning:Sci-
ence and Technology 3(1), 015025 (2022)
5. Chen, S.Y.C., Yang, C.H.H., Qi, J., Chen, P.Y., Ma, X., Goan, H.S.: Variational
quantum circuits for deep reinforcement learning (2020), https://arxiv.org/abs/
1907.00397
6. Ding, S., Su, C., Yu, J.: An optimizing bp neural network algorithm based on
genetic algorithm. Artificial intelligence review 36, 153–162 (2011)
7. Eiben,A.E.,Smith,J.E.:Introductiontoevolutionarycomputing.Springer(2015)
8. Foerster, J.N., Chen, R.Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., Mordatch,
I.: Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326
(2017)
9. Franz, M., Wolf, L., Periyasamy, M., Ufrecht, C., Scherer, D.D., Plinge, A.,
Mutschler, C., Mauerer, W.: Uncovering instabilities in variational-quantum deep
q-networks. Journal of The Franklin Institute 360(17), 13822–13844 (2023)
10. Gabor, T., Altmann, P.: Benchmarking surrogate-assisted genetic recommender
systems.In:ProceedingsoftheGeneticandEvolutionaryComputationConference
Companion. pp. 1568–1575 (2019)
11. Giovagnoli,A.,Tresp,V.,Ma,Y.,Schubert,M.:Qneat:Naturalevolutionofvaria-
tionalquantumcircuitarchitecture.In:ProceedingsoftheCompanionConference
on Genetic and Evolutionary Computation. pp. 647–650 (2023)
12. Groşan,C.,Oltean,M.,Oltean,M.:Theroleofelitisminmultiobjectiveoptimiza-
tion with evolutionary algorithms. Acta Univ. Apulensis Math. Inform pp. 83–90
(2003)
13. Harrow, A.W., Montanaro, A.: Quantum computational supremacy. Nature
549(7671), 203–209 (2017)
14. Hernandez-Leal, P., Kaisers, M., Baarslag, T., De Cote, E.M.: A survey of learn-
ing in multiagent environments: Dealing with non-stationarity. arXiv preprint
arXiv:1707.09183 (2017)30 M. Kölle et al.
15. Holland, J.H., Miller, J.H.: Artificial adaptive agents in economic theory. The
American economic review 81(2), 365–370 (1991)
16. Jiang, Y., Li, X., Luo, H., Yin, S., Kaynak, O.: Quo vadis artificial intelligence?
Discover Artificial Intelligence 2(1), 4 (2022)
17. Kölle,M.,Topp,F.,Phan,T.,Altmann,P.,Nüßlein,J.,Linnhoff-Popien,C.:Multi-
agent quantum reinforcement learning using evolutionary optimization. arXiv
preprint arXiv:2311.05546 (2023)
18. Laurent,G.J.,Matignon,L.,Fort-Piat,L.,etal.:Theworldofindependentlearn-
ers is not markovian. International Journal of Knowledge-based and Intelligent
Engineering Systems 15(1), 55–64 (2011)
19. Leibo,J.Z.,Zambaldi,V.,Lanctot,M.,Marecki,J.,Graepel,T.:Multi-agentrein-
forcementlearninginsequentialsocialdilemmas.arXivpreprintarXiv:1702.03037
(2017)
20. Lerer, A., Peysakhovich, A.: Maintaining cooperation in complex social dilemmas
using deep reinforcement learning. arXiv preprint arXiv:1707.01068 (2017)
21. Littman,M.L.:Markovgamesasaframeworkformulti-agentreinforcementlearn-
ing. In: Machine learning proceedings 1994, pp. 157–163. Elsevier (1994)
22. Lukac, M., Perkowski, M.: Evolving quantum circuits using genetic algorithm. In:
Proceedings 2002 NASA/DoD Conference on Evolvable Hardware. pp. 177–185.
IEEE (2002)
23. Mottonen, M., Vartiainen, J.J., Bergholm, V., Salomaa, M.M.: Transformation
of quantum states using uniformly controlled rotations. arXiv preprint quant-
ph/0407010 (2004)
24. Müller,T.,Roch,C.,Schmid,K.,Altmann,P.:Towardsmulti-agentreinforcement
learning using quantum boltzmann machines. arXiv preprint arXiv:2109.10900
(2021)
25. Neumann, N.M., de Heer, P.B., Chiscop, I., Phillipson, F.: Multi-agent reinforce-
ment learning using simulated quantum annealing. In: Computational Science–
ICCS 2020: 20th International Conference, Amsterdam, The Netherlands, June
3–5, 2020, Proceedings, Part VI 20. pp. 562–575. Springer (2020)
26. Nielsen, M.A., Chuang, I.L.: Quantum computation and quantum information.
Cambridge university press (2010)
27. Phan, T., Sommer, F., Altmann, P., Ritz, F., Belzner, L., Linnhoff-Popien, C.:
Emergent cooperation from mutual acknowledgment exchange. In: Proceedings of
the21stInternationalConferenceonAutonomousAgentsandMultiagentSystems.
pp. 1047–1055 (2022)
28. Preskill, J.: Quantum computing in the nisq era and beyond. Quantum 2, 79
(2018)
29. Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S.,
Guez,A.,Lockhart,E.,Hassabis,D.,Graepel,T.,etal.:Masteringatari,go,chess
and shogi by planning with a learned model. Nature 588(7839), 604–609 (2020)
30. Schuld, M., Bocharov, A., Svore, K.M., Wiebe, N.: Circuit-centric quantum clas-
sifiers. Physical Review A 101(3), 032308 (2020)
31. Skolik, A., McClean, J.R., Mohseni, M., Van Der Smagt, P., Leib, M.: Layer-
wiselearningforquantumneuralnetworks.QuantumMachineIntelligence3,1–11
(2021)
32. Sünkel,L.,Martyniuk,D.,Mattern,D.,Jung,J.,Paschke,A.:Ga4qco:genetical-
gorithmforquantumcircuitoptimization.arXivpreprintarXiv:2302.01303(2023)
33. Vikhar, P.A.: Evolutionary algorithms: A critical review and its future prospects.
In:2016Internationalconferenceonglobaltrendsinsignalprocessing,information
computing and communication (ICGTSPICC). pp. 261–265. IEEE (2016)Architectural Influence on VQC in Evolutionary MARL 31
34. Yanofsky, N.S., Mannucci, M.A.: Quantum computing for computer scientists.
Cambridge University Press (2008)
35. Yun, W.J., Kwak, Y., Kim, J.P., Cho, H., Jung, S., Park, J., Kim, J.: Quantum
multi-agentreinforcementlearningviavariationalquantumcircuitdesign.In:2022
IEEE42ndInternationalConferenceonDistributedComputingSystems(ICDCS).
pp. 1332–1335. IEEE (2022)