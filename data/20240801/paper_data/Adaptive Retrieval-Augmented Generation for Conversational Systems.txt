Adaptive Retrieval-Augmented Generation for Conversational Systems
XiWang1,ProchetaSen2,RuizheLi3,EmineYilmaz4
1UniversityofSheffield,2UniversityofLiverpool,
3UniversityofAberdeen,4UniversityCollegeLondon
xi.wang@sheffield.ac.uk1,procheta.sen@liverpool.ac.uk2,
ruizhe.li@abdn.ac.uk3,emine.yilmaz@ucl.ac.uk4
Abstract
Can you find me some interesting things to do?
Despite the success of integrating large lan-
Cloud on earth, clouds are formed by guage models into the development of con-
the saturation of air in the homosphere.
versationalsystems,manystudieshaveshown Cloud the Droplets or particles are
suspended in the atmosphere above
theeffectivenessofretrievingandaugmenting
the surface of a planetary body.
externalknowledgeforinformativeresponses.
Use Knowledge
Hence, many existing studies commonly as-
sumethealwaysneedforRetrievalAugmented Sure! Here are a few interesting things you can do:
1. Explore the science of clouds.
Generation(RAG)inaconversationalsystem 2. Virtual museum tours
3. Online Courses
withoutexplicitcontrol. Thisraisesaresearch
4. Read a Book or listen to an audiobook.
questionaboutsuchanecessity. Inthisstudy,
weproposetoinvestigatetheneedforeachturn Not Use Knowledge
ofsystemresponsetobeaugmentedwithexter- Sure! Here are a few suggestions based on different
interests:
nalknowledge. Inparticular,byleveraginghu- 1. Creative activities: painting, writing, DIY crafts
manjudgementsonthebinarychoiceofadap- 2. Physical activities: exercise, outdoor walk and
dancing
tiveaugmentation,wedevelopRAGate,agat- 3. Entertainment: movies, games, books
ingmodel,whichmodelsconversationcontext
andrelevantinputstopredictifaconversational Figure1: Exampleconversationwhengeneratingare-
systemrequiresRAGforimprovedresponses. sponse with or without a knowledge snippet using a
Weconductextensiveexperimentsondevising languagemodel(GPT-4inthisexample).
andapplyingRAGatetoconversationalmodels
andwell-roundedanalysesofdifferentconver-
sational scenarios. Our experimental results
non-factualorhallucinatedcontent(Huangetal.,
andanalysisindicatetheeffectiveapplication
of RAGate in RAG-based conversational sys- 2021), and restricted domain adaptability (Ren
temsinidentifyingsystemresponsesforappro- etal.,2018). Theseissuescanhinderthedevelop-
priateRAGwithhigh-qualityresponsesanda mentofconversationalagentswithsatisfactoryuser
high generation confidence. This study also experience. Toaddresstheseidentifiedchallenges,
identifies the correlation between the genera-
a common approach is to retrieve and augment
tion‚Äôsconfidencelevelandtherelevanceofthe
LLMswithexternalknowledgetoenhancethecon-
augmentedknowledge.
versationalresponse,makingthemmoreaccurate,
reliable,andadaptabletodifferentdomains(Zhao
1 Introduction
etal.,2020;Lianetal.,2019;Yeetal.,2024). For
Recently, the advancement of Large Language example,Shusteretal.(2021)demonstratedthatus-
Models (LLMs) has significantly improved con- ingadenseretrievalmodel(DPR)(Karpukhinetal.,
versationalsystems,enablingthegenerationofnat- 2020)toretrieverelevantknowledgeforaugmenta-
ural and high-quality responses (Ni et al., 2023). tioncansignificantlyreducethehallucinationrate,
Despite these advancements, recent studies have according to a corresponding human evaluation.
identified several limitations on the simple use Similarly,Yangetal.(2020)showedthatleverag-
of LLMs to address conversational tasks (Onoe ingagraph-structuredknowledgebasecanboost
etal.,2022;Huangetal.,2021;Renetal.,2018). thereasoningabilityanddomaingeneralisabilityof
These limitations include the lack of up-to-date task-orientedconversationalagents. Theseachieve-
knowledge (Onoe et al., 2022), the generation of ments of knowledge-augmented techniques high-
4202
luJ
13
]LC.sc[
1v21712.7042:viXra
egdelwonKlightapromisingdirectionforenhancingconversa- et al., 2023) ‚Äì we show that the "always" aug-
tionalagentsandaddressthecurrentlimitations. mentationofexternalknowledgecansignificantly
However,whileimplementingretrievalaugmen- increasegenerationuncertaintyandtheriskofhal-
tationtoaconversationalsystemforimprovedre- lucination. AfterapplyingRAGate,wecaneffec-
sponse, we question the necessity of knowledge tivelycontroltheconversationsystemtomakecon-
augmentation for every turn of system responses. fident and informative responses. In addition, by
To develop effective human-computer conversa- varyingtheuseofknowledgesnippetsindifferent
tions,itisessentialtoprovidefactualandrelevant relevancelevels,wealsoobservethepositivecorre-
responses, offer appropriate amount of informa- lationbetweenthecalculatedconfidencescoreand
tion, and not unnaturally drive and shift the con- therelevanceofaugmentedknowledge,whichcan
versation to non-relevant topics (Kasirzadeh and bevaluableformanyfuturestudies.
Gabriel,2023;Miehlingetal.,2024). Wearguethat
overusingexternalknowledgecouldresultinsys- 2 RelatedWork
temresponsesagainstthesecorecriteria. Figure1
presentsaconversationexamplethatshowshowthe Inthepipelineofknowledge-augmentedgeneration
systemresponsetoagenericuserutteranceabout foraconversationsystem, twomaincomponents
suggesting activities can vary with and without areidentified: theknowledgeretrieverandthere-
augmentedknowledge. Theknowledge-augmented sponsegenerator. Existingstudieshaveimproved
systemresponseisbeinginformationconditioned conversationalresponsestodifferentextentsbyim-
with limited diversity and assuming specific user proving one or both components (Li et al., 2022;
preferences. In contrast, without the addition of Komeilietal.,2022;Wangetal.,2024).
externalknowledge,thesystemresponseismore KnowledgeRetrieval: Severalstudieshaveex-
diverseandnaturalinthisearlystageofaconversa- ploredtheuseofdensepassageretrievaltechniques
tion. Thisindicatesthatmisusingexternalknowl- (Lewis et al., 2020; Karpukhin et al., 2020) and
edgecanleadtoproblematicsystemresponsesand public search service for effective retrievers (Li
anegativeuserexperience. etal.,2022). Forexample,Lietal.(2022)retrieved
To address this, we investigate an adaptive Wikipedia passages through a database interface
retrieval-augmentedgenerationsolutionforeffec- andthenrankedthemaccordingtostatisticalrele-
tive conversational systems. In particular, moti- vance,calculatedbyTF-IDF,orsemanticrelevance
vatedbythegatefunctioninlong-shorttermmem- as per cosine similarity. Similarly, Komeili et al.
ory models (Graves and Graves, 2012), which usedasearchengineAPItoretrieverelevantknowl-
explicitly controls the use of input and memory, edgebutfirsttransformedthedialoguecontextinto
weproposeabinaryknowledgegatemechanism, a natural search query using an encoder-decoder
called RAGate, to manipulate the use of external modelbeforesearching.
knowledgeforaconversationalsystem. Tomodel Joint Optimisation of Retriever and Genera-
the conversation context and accurately estimate tor: On the other hand, another thread of re-
the need for augmentation, we leverage the hu- searchstudieshasexploredjointoptimisationap-
man labels as ground truth and develop RAGate proaches. Forinstance,Shietal.(2023)introduced
byexploringtheuseofrecentadvancedlanguage a retriever-generator architecture that aims to im-
modelsorconstructingattentionneuralgatemod- provetheperformanceofTask-OrientedDialogue
els. To validate the effectiveness of RAGate, we (TOD) systems by using a dual-feedback mecha-
conduct extensive experiments on an annotated nism. Theretrieveridentifiesrelevantknowledge
Task-Oriented Dialogue (TOD) system dataset, from a database, while the generator uses this in-
KETOD, that builds upon the SGD dataset with formationtocreateappropriatesystemresponses.
TOD-spanning16domains,suchasRestaurantand Thefeedbackfromthegeneratorisfurtherusedas
Weather. TheexperimentalresultsshowthatRA- pseudo-labelstotraintheretrievertoselectperti-
Gateenablesconversationalsystemstoefficiently nentinformation.Shenetal.(2023)introduceda
useexternalknowledgeatappropriateconversation trainingmethodbasedonmaximalmarginallike-
turns,producinghigh-qualitysystemresponses. In lihood. Thismethodjointlyoptimiseaperceptive
particular,bymodellingtheuncertaintyandconfi- retriever and the response generation in a feed-
dencelevelofthesystem‚Äìwhichcorrelateswith back loop. The proposed approach incorporates
the likelihood of hallucinated output (Varshney meta-knowledge, which guides the generator toPrediction
Prediction
Softmax
Prompt‚äï LLM Linear
Add&Norm
Contextor(Context + knowledge)
Feed
Forward
Prediction Nx Add&Norm
RAGate + Multi-Head
Attention
External Knowledge
Response LLM ùëü EP no cs oit dio inn g ‚äï
Generator Input
Embedding
Contextor(Context + knowledge) Contextor(Context & knowledge)
Figure2: RAGatevariantsforimplementingthegatingfunction. Thethreevariantsarethepredictionwithpre-
trainedlanguagemodelsafterprompting(1),afterparameter-efficientfine-tuning(2),andwithamulti-headattention
encoder(3).
improve the utilisation of knowledge and, conse- termines when to search for external knowledge
quently, the quality of the generated responses. toensurenatural,relevantandcontextuallyappro-
Kang et al. (2023) further advance the retriever priateresponses. First,wedefinethetaskofuser-
byproposingSUbgraphRetrieval-augmentedGEn- systemconversation. LetD = {d ,d ,...,d }be
1 2 |D|
eration(SURGE),whichemployedagraphneural asetofuser-systemdialogues,andeachdialogued
network(GNN)-basedcontext-relevantsubgraph comprisesasequenceofinteractionsbetweenusers
retriever. SURGE incorporates contrastive learn- andsystems(i.e.,d = {u ,s ,u ,s ,...,u ,s })
0 0 1 1 T T
ingtooptimisethelatentrepresentationspace,en- with varying lengths. Here, u and s denote the
t t
suring that generated texts closely resemble the userutteranceandsystemresponseatthet-thturn,
retrievedsubgraphs. respectively. Theconversationalcontextuptoturn
Despite the richness of existing retrieval- t can be formulated by aggregating the previous
augmented generation techniques for conversa- user-system interactions, i.e., c = u ,s ,..,u .
t 0 0 t
tional systems, they commonly hypothesise that Withthiscontextinformationc ,theconversation
t
everyconversationturnneedsexternalknowledge. systemcanaugmentitwithalistofretrievedexter-
However,thenecessityofaugmentingeveryturn nal knowledge, e , where k represents the rank-
t,k
of the conversation with external knowledge re- ingcutofffortheretrievedknowledge. Hence,the
mainsquestionable. Arelevantthreadofworkthat binarygatemechanismproposedinthisstudy,de-
aimstoanswerthisquestionistheintroductionof cidingtheknowledgeaugmentation,canbeformu-
the knowledge-seeking turn detection task using lated as f(c ) = {0,1} or f(c ,e ) = {0,1} if
t t t,k
the DSTC-9 dataset (Kim et al., 2020), and the the external knowledge e is considered. Then,
t,k
follow-upstudies,suchas(Hongetal.,2023;Jin thefollow-upresponsegenerationfunctiong(¬∑)can
etal.,2021). However,thistaskistoidentifythe beformulatedasfollows:
turnsinconversationsinjectedbyhumanworkers (cid:40)
g(c ,e ) iff(c )orf(c ,e )
t t,k t t t,k
aboutknowledgeenquiryinsteadofidentifyingthe g(¬∑) = (1)
g(c ) otherwise.
systemresponsesthatrequireknowledgeaugmen- t
tation for improvements. This research gap high- Hence,byevaluatingandestimatingthenecessity
lights the value and novelty of this study, which of augmenting with external knowledge, we dy-
investigatestheadaptiveuseofretrieval-augmented namicallyupdatetheconversationalresponsegen-
generationforadvancedconversationalsystems. erationaccordingly.
3 Methodology 3.2 RAGateGateMechanism
To effectively estimate the need to use external
3.1 ProblemFormulation
knowledgeandimplementadaptiveretrievalaug-
This study addresses the challenge of effectively mentedgenerationforaconversationsystem, we
identifyingconversationturnsthatrequireaugmen- introduceourproposedgatemechanism,RAGate,
tationofexternalknowledge. Inparticular,weaim that uses the conversational context and, option-
todevelopagatemechanismthatdynamicallyde- ally,theretrievedexternalknowledgetopredictthebinarychoiceofusingexternalknowledge. Inpar- structions, joined with paired inputs and outputs
ticular,weexplorethreeRAGatevariantsthatare fordevelopingparameter-efficientfine-tunedlarge
implementedbytheuseofLargeLanguageMod- language models. In particular, we provide a set
els(LLMs)withdevisedprompts,withparameter ofinstruction-input-outputtriplesformodeltrain-
efficientfine-tuning(e.g.,QLoRA(Dettmersetal., ing. The input can vary with the provision of a
2024))andtheconstructionofanend-to-endmulti- setofavailablefeatures. Apartfromtheuseofthe
head attention encoder. This exploration is moti- conversationalcontext(contx),wealsoincludethe
vated by the recent advancement of transformer- systemresponse(resp),syntheticresponsesgener-
structuredneuralmodelsinnaturallanguagepro- atedbythelanguagemodel(syn-resp)duetothe
cessing. InFigure2,weillustratetheapplication missingresponsesasinputinthepracticalscenario,
ofRAGateanditsthreevariants. Wedescribeeach the name entities within the incoming responses
ofthesethreevariantstoclarifytheuseofRAGate: (ner),retrievedknowledge(know)andthedescrip-
RAGate-Prompt: As denoted by Arora et al. tion of the knowledge source, e.g., the WikiHow
(2022), a language model can effectively adapt website(source). Byusingvariouscombinationsof
to new tasks by using a natural language prompt inputsandcustomisingthecorrespondinginstruc-
thatexplainstheprocesstoaddressthetaskswith- tions, we explore the effectiveness of the result-
out extra training. Hence, we can formulate a ing learned language models that implement the
gate function f(¬∑) as f(y|c ) = f(y|Œò,c ,p), RAGate-PEFT.
t t
where Œò denotes the used language model with RAGate-MHA: Apart from the use of pre-
its pre-trained weights and p is the devised nat- trainedlanguagemodelsandfurtherfine-tunedlan-
ural language prompt. Alternatively, if the re- guage models, we also explore the introduction
trieved knowledge is also involved in prediction, ofamulti-headattentionneuralencodertomodel
wehavef(y|c ) = f(y|Œò,c ,e ,p). Specifically, the context as input and estimate the augmenta-
t t t,k
we explore two types of prompts: zero-shot and tionnecessity (i.e., RAGate-MHA).Here, wede-
in-context learning. Zero-shot prompts describe scribe the model structure of RAGate-MHA. At
the task that uses the conversational context and, first, as denoted by (Vaswani et al., 2017), the at-
optionally, the retrieved knowledge to generate tentionmechanismisformulatedastheinteraction
a response with binary feedback. As for the in- betweenthreeobjects,queriesQ,keysK,andval-
context learning prompts, we augment the zero- ues V: Attention(Q,K,V) = softmax(Q ‚àöKT )V.
d
shotpromptswithillustrativeexamples. Weshow k
Toestimatethenecessityofaugmentation, wefit
thesetofpromptsinAppendixA.
the context and the retrieved knowledge into the
RAGate-PEFT: Despite the high adaptabil-
roles of these three objects. Specifically, we in-
ity of the language model with devised prompts, cludethesetupsof(1)usingcontextonly(contx)
we further explored the use of instruction tun-
or(2)usingtheconcatenatedcontextandretrieved
ingonlanguagemodelswithaparameter-efficient knowledge (contx ‚äï know) as queries, keys, and
fine-tuningmethod(i.e.,QLoRA(Dettmersetal.,
values, and (3) using the context as queries and
2024)) to meet the goal of an effective gate func-
interactwiththeretrievedknowledgeaskeysand
tion. QLoRA is built upon the known Low-rank values(contx√óknow). Next,following(Vaswani
Adapter(LoRA)(Huetal.,2021),whichkeepsthe
etal.,2017)intheencoderconstructionofatrans-
pre-trainedweightmatrixW frozenandaddresses
0 former model, we encode the inputs via an input
the gradient updates of the weight matrix ‚àÜW
embeddinglayerintolatentvectorsandaposition
throughlow-rankapproximation(i.e.,‚àÜW = BA,
encodinglayertoencodetheorderoftokensinthe
where B and A are the result of lower-rank de-
sequence. After that, we leverage the multi-head
composition on ‚àÜW). Hence, the forward pass
attention to learn attention weights on the inputs
during the model training can be updated from
andthenfollowedbyafeed-forwardnetwork:
h = W x+‚àÜWxtoh = W x+BAx. QLoRA
0 0
(Dettmersetal.,2024),whichisusedinthisstudy, FFN(x) = max(0,xW +b )W +b (2)
1 1 2 2
further quantises the language model into a 4-bit
NormalFloatdatatypeandleveragesthepage-to- where W and W are two learned parameter
1 2
page transfer between the CPU and GPU to fur- matricswithtwobiasterms(b andb ). Bothmulti-
1 2
theravoidmemoryspikes. ToimplementRAGate- headattentionandfeed-forwardneuralmodulesare
PEFT, we format the train data with devised in- followedbyresidualconnection(Heetal.,2016)RetrievalModels Recall@1 Recall@3 mancewiththeclassicRecall@1andRecall@3on
TF-IDF 0.0227 0.0871 thetestcollection. Weuseashallowcutoffbecause
BERT-Ranker 0.2475 0.4714
we only use top-relevant knowledge snippets for
augmentation. Table 1 shows their retrieval per-
Table1: RetrievalPerformanceEvaluationwhenusing
contextasthequery. formance. Accordingtotheleadingperformance
ofBERT-Ranker,weaugmentknowledgewithits
retrieved top 3 relevant knowledge snippets (i.e.,
andlayernormalisation(Baetal.,2016). Unlike
k = 3). Regarding the development of RAGate-
the introduction of another decoder module that
MHA,weexplorethecombinationsof2to8layers,
addressesthesequence-to-sequencegenerationin
2 or 4 heads and the embedding size in [64, 128,
(Vaswanietal.,2017),wefollowedtheencoderout-
256]forthebestclassificationaccuracy. Wereport
putwithalinearprojectionmoduleandasoftmax
theprecision,recall,F1,AreaUnderCurve(AUC)
functionforourbinaryclassificationtask.
and the False Discovery Rate (FDR) as the main
measurestoshowtheclassificationeffectiveness.
4 ModelTrainingandEvaluationSetups
Next,wefurtherdeploythebest-performingRA-
We evaluate the performance of introducing RA- GategatefunctiontoupdatetheKETODdialogue
Gate according to its binary classification perfor- system(Chenetal.,2022),whichusesGPT-2(Rad-
mance and the effectiveness of the resulting re- fordetal.,2019)asthebackbonemodel. Tohigh-
sponse generation. Specifically, we use the KE- light the effect of various augmentation setups,
TOD dataset (Chen et al., 2022), which has fully we use the context with the gold action without
annotated5,324dialoguesand52,063turnsofcon- extra prediction as input to KETOD. Then, we
versations. Inparticular,itisassociatedwith33,761 comparetheresultingperformancetotheKETOD
knowledgesnippetstoberetrievedandaugmented. modelwithoutknowledgeaugmentationandaug-
In addition, KETOD was developed with human menting every system response as baselines. To
labelsonturnsofconversations(around12.1%of report the response generation effectiveness, we
turns)abouttheneedforaugmentingwithretrieved reporthowclosetheresponseistothegroundtruth
knowledgesnippetsforanaturalandinformative via BLEU, ROUGE-1/2/L and BERTScores and
systemresponse. Hence,weusethesehumanlabels the confidence score calculated by the minimum
asnaturalgroundtruthswhenevaluatingRAGate. probabilitiesofindividualtokensthatcomposethe
Itisworthindicatingthatmanycurrentknowledge- response. AsarguedbyVarshneyetal.(2023),this
augmented conversational datasets often ground calculated confidence score can highly correlate
theirconversationsontheknowledgesnippet,such withalanguagemodel‚Äôslikelihoodofgenerating
as Wizard of Wikipedia (Dinan et al., 2018) and hallucinatedresponses. Wetrainedourmodelsand
CMU_DoG(Zhouetal.,2018),whichmakesthem conductedtheevaluationsononemachinewithone
notanaturalfittobeinvestigatedinthisstudy. NVIDIA4090GPU.
Duetothelimitedcomputationalresourceavail-
ability, we explore the use of Llama-v2-7B and 5 ResultsandAnalysis
Llama-v2-13BtoimplementRAGate-promptand
5.1 AugmentationNeedClassification
fine-tuneLlama-v2-7BforRAGate-PEFT.Weim-
plement QLoRA using the PEFT library (Man- First,weevaluatetheclassificationaccuracyofour
grulkaretal.,2022)andsetthelowerrankto16. As developed RAGate gate methods for addressing
discussedinSection3,wehavevariousinputfea- the adaptive RAG to system responses. Table 2
turestobecombinedforperformanceoptimisation. presentstheclassificationperformanceofRAGate
Webeginwiththeuseofcontextonly,thenconcate- baselineswhileevaluatedonthetestcollectionof
natethecontextwiththerealresponse(contx-resp), the KETOD dataset, which includes rich human
with the synthetic response and recognised enti- labelsontheuseofRAGforresponsegeneration.
ties (contx-syn-resp-ner) and further extend with As discussed in Section 3, we explore the devel-
theuseofretrievedknowledge(contx-syn-resp-ner- opment of RAGate with three variants: the use
know)orthesourceofknowledge(contx-syn-resp- ofLLMprompting(RAGate-Prompt),parameter-
ner-source). Specifically,weretrievetherelevant efficientfine-tunedLLMs(RAGate-PEFT),anda
knowledge by exploring the use of TF-IDF and neuralclassifierwithMulti-HeadAttentionstruc-
alearnedBERTranker. Weevaluatetheirperfor- ture(RAGate-MHA).ModelVariants Precision Recall F1 diction,weobserveasignificantperformancedrop
RAGate-Prompt:LLMs‚ÄìZeroShot across all evaluated aspects. This can be caused
Llama-2-7B 0.1323 0.0278 0.0460 bytheadditionalcomplexityintroducedbythein-
Llama-2-13B 0.1422 0.1083 0.1230
cludedretrievedknowledgesnippets. Furthermore,
RAGate-Prompt:LLMs‚ÄìIn-ContextLearning
wealsoexploredtheperformanceimpactofnam-
Llama-2-7B 0.1417 0.0294 0.0487
ing the source of the knowledge snippet. We use
Llama-2-13B 0.0989 0.0851 0.0915
RAGate-PEFT:ParameterEfficientFine-tunedLLMs(Llama2-7B)
wikiHow1inthisstudy,whichcanproviderichtask
[contx‚äïresp] 0.4926 0.3095 0.3802 instructionsforofferinginformativetask-oriented
contx-only 0.5203 0.3359 0.4082 systemresponse(Senetal.,2023). However,the
contx-(syn-resp)-ner 0.6818 0.2321 0.3464 fine-tuned model cannot reasonably connect the
contx-(syn-resp)-ner-know 0.4698 0.0603 0.1069 promisedrichresourcefromtheknowledgesource
contx-(syn-resp)-ner-source 0.4000 0.0185 0.0355
andthepredictionofaugmentationnecessity.
RAGate-MHA:Contextwith/withoutKnowledgeInput
RAGatePerformancebetweenfine-tunedLLM
MHA(contx)-h(4)-l(5)-emb(64) 0.3210 0.5541 0.4065
MHA([contx‚äïknow])-h(4)-l(2)-emb(64) 0.2795 0.5201 0.3636 and MHA classifier. Next, by comparing the ex-
MHA(contx√óknow)-h(4)-l(2)-emb(64) 0.2272 0.5835 0.3271
perimentalresultsofRAGate-MHAandRAGate-
RAGate-MHA:Context-ResponseInput
PEFT in Table 2, we observe a wide-margin re-
MHA([contx‚äïresp])-h(4)-l(4)-emb(64) 0.3500 0.5510 0.4281
call improvement using RAGate-MHA, reaching
Table2: Classificationaccuracyonadaptiveaugmenta- a minimum recall of 0.52, but with significantly
tionforsystemresponse. "contx","resp",and"know" lower precision accuracy. In Table 2, we also in-
refertotheuseofcontext,initialsystemresponse,and clude the use of both the context and the initial
retrievedknowledgesnippetsasinput. "syn-resp"and system responses (i.e., MHA([contx, resp])) for
"ner"aretheadditionalsyntheticresponseandnameen-
additionalinsights. Wecanobservethatahigher
tityrecognitionstepsinthemodelfine-tuningprompts.
precisioncanbeachievedbuttheuseofresponse
h,landembrefertothebest-performedconfiguration
doesnotimprovetherecallperformance. Thesere-
onthenumberofheads,layersandembeddingsize.
sultsareconsistentwiththeobservedperformance
of RAGate-PEFT, which further encourages the
RAGateperformancewithLLMpromptingver- useofasyntheticresponseduetotheunavailability
susfine-tuning. Bycomparingthecorresponding of a system response in a practical scenario. In
performancereportedinTable2,weobservethat, addition, we also observe a similar performance
onaverage,fine-tuningaLlama-2-7BwithQLoRA dropwhenincludingtheretrievedknowledgesnip-
(i.e.,RAGate-PEFT)cansignificantlyoutperform pets for classification. Even though the RAGate-
RAGate-Prompt. For example, by looking at the MHAmodel,usingtheinteractionbetweencontext
RAG-PEFTwithcontext-onlyinput,withoutusing andretrievedknowledgesnippets,canachievethe
extrainputfeaturesandinstructionupdates,itcan highestrecallof0.5835,itcannotoutperformthe
outperformallRAG-Promptapproachesbyabig RAGate-MHAusingcontext-onlyonothermetrics.
margin(e.g.,0.4082versusthehighest0.1230F1 Hence,consideringthesimilarF1andAUCperfor-
scores). This reflects the difficulty of this adap- mancelevelsofRAGate-PEFTandRAGate-MHA
tiveknowledgeaugmentationtask,whichcannot leadstoatrade-offbalancebetweenprecisionand
beproperlyaddressedbypromptingageneralpre- recallforthetwogroupsofapproaches. Tofurther
trained language model. In particular, the use of evaluatetheclassificationeffectivenessofRAGate,
larger language models and the in-context learn- in Appendix B, we provide a detailed discussion
ing setup, which often result in improved perfor- of a conducted user study that explores whether
mance(Aroraetal.,2022),cannotguaranteethe RAGatecanalsoassessthepotentialcontribution
enhancementofmodels‚Äôclassificationaccuracyre- ofretrievedsnippetswhenpredictingthedecision
gardingthisclassificationtask. forretrievalaugmentation.
Regarding the performance of RAGate-PEFT
approaches, by first examining the effect of us- 5.2 AdaptiveAugmentationAnalysis
ingsyntheticresponse andrecognisednameenti-
Inadditiontotheclassificationaccuracy,wealso
ties,weobservesignificantlyimprovedprecision
compare the choice of human workers and RA-
(0.5203to0.6818)butwiththecostoflowerrecall
Gate approaches in augmenting specific turns.
(0.3359to0.2321). Inaddition,whenweaddthe
retrieved knowledge to the input features for pre- 1https://www.wikihow.com300
RAGate-PEFT Alarm
250 RAGate-MHA Travel
Homes
Human Labels
Hotels
200 Flights
Calendar
150 Movies
Trains
Restaurants
100 Music
Payment
50 Services
Weather
Media
0 Buses
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Position of Turns in a Conversation Events
RideSharing RAGate-PEFT
RentalCars RAGate-MHA
Figure3: Frequencyanalysisofadaptiveaugmentations
Messaging Human Labels
aboutthepositionofaconversation.
0 20 40 60 80 100 120
Number of Selected Augmentations
Specifically, we analyse the frequency of aug- Figure4: Frequencyanalysisofadaptiveaugmentations
aboutdialoguedomains.
mentation in different positions of conversations
and different domains covered in the KETOD
dataset. We use the RAGate-PEFT (contx-(syn- Variants #Augs BLEU ROUGE-L BERTScore Confidence
No-Aug 0 9.38‚Ä¢ 0.3780‚Ä¢ 0.8105‚Ä¢ 9.3425‚Ä¢ ‚Äì
resp)-ner)withthehighestprecisionandRAGate-
AugmentBERTRankerRetrievedKnowledge
MHA(MHA(contx))withthebestoverallperfor- RAGate-PEFT 230 10.45 0.3825 0.8144 9.3374 -0.05%
RAGate-MHA 787 12.14 0.3882 0.8192 9.3083 -0.36%
manceintheaboveanalysisasrepresentativesfor Random-Aug 230 9.53 0.3784 0.8110 9.2984 -0.47%
Random-Aug 787 10.01 0.3795 0.8126 9.1877 -1.65%
comparison. Figure 3 presents the frequency in Human-label 631 11.66 0.3856 0.8176 9.2550 -0.93%
Aug-All 4964 16.08 0.3927 0.8258 8.3677 -10.43%
differentpositions. Duetotheunequalnumberof
AugmentRank-1RelevantKnowledge
conversational turns, we use the ratio to indicate
RAGate-Llama 230 10.54 0.3822 0.8142 9.3642 +0.23%
the relative position. According to the reported RAGate-MHA 787 11.99 0.3883 0.8191 9.3774 +0.37%
Random-Aug 230 9.51 0.3784 0.8110 9.3328 -0.10%
resultsinFigure3,mosthumanaugmentationse- Random-Aug 787 10.01 0.3800 0.8127 9.2982 -0.47%
Human-label 631 11.52 0.3846 0.8170 9.3218 -0.22%
lectionshappenatthebeginningofaconversation. Augment-All 4964 16.05 0.3944 0.8259 9.0655 -2.9%
ThistrendisalsoeffectivelycapturedbybothRA-
Table 3: Performance of applying RAGate and com-
Gateapproaches,especiallyRAGate-MHA.This
paredtotheKETODbaselineontheKETODdataset.
can be caused by the reason that a conversation Confidenceiscalculatedbytheaveragevalueoverthe
issemanticallycoherent,andoncesufficientaddi- lowestlogitofeachgeneration.
tional information is provided at the early stage,
thevalueofknowledgeaugmentationtothelater
domainaugmentationfrequency,weconcludethat
turnsisnaturallylower.
RAGate-MHAcanoutperformRAGate-MHAand
Ontheotherhand,Figure4presentstheaugmen-
effectivelycapturethetrendofaugmentationneeds.
tation frequency over different domains. We ob-
servethatsystemresponsesaboutcertaindomains
5.3 RAGateforResponseGeneration
areselectedmoreoftenbyhumansthanotherdo-
mains,suchastravel,hotels,trains,flights,service ToevaluatetheeffectofadaptiveRAGforaconver-
andrentalcars,whichrequireaccesstoadditional sationalsystem,weuseRAGate-PEFT(contx-(syn-
information to assist the suggestion-making, and resp)-ner)withthehighestprecisionandRAGate-
the domains, like movies, music, media, events MHA(MHA(contx))withthebestoverallperfor-
that often include entities require enriched de- manceintheaboveanalysis,tosupporttheadaptive
scription. By looking into the performance of retrievalaugmentedconversationalresponsegen-
RAGate-PEFTandRAGate-MHA,RAGate-MHA eration. Table 3 presents the results of applying
canmakealignedselectionsforhumans. However, RAGAtetotheKETODmodelforadaptiveknowl-
theRAGate-PEFTdoesnotguaranteetheidentifi- edgeaugmentationwhenevaluatedontheKETOD
cationofappropriateaugmentationuseandoften dataset. Weincludefourtypesofadaptiveaugmen-
presentsfeweraugmentations,apartfromthetravel tation,namelytheuseofRAGateandcomparison
domain. Hence,byconsideringbothpositionand totherandomselectionwithequalnumbersofse-
snoitatnemguA
detceleS
fo
rebmuN
sniamoDlections, human choice, and the commonly used averageconfidencescoreby0.36%,insteadofthe
"all"augmentation. Inaddition,toexploretheef- 1.65%whenrandomlyselectinganequalnumber
fect of varied quality of knowledge snippets, we ofturnstoaugment.
alsoextendtheevaluationofusingthetop-3knowl- Inaddition,consideringtheuseofdifferentqual-
edge snippets ranked by different retrievers (i.e., ityandamountofknowledgesnippetsforaugmen-
BERT-ranker and TF-IDF) and the use of knowl- tation, we also include the use of the most rele-
edgesnippetsatthe1stand5thrankaccordingto vantknowledgesnippetaccordingtoBERT-ranker
theBERT-ranker. Duetothespacelimit,wefirst in Table 3. We observe that the use of different
presenttheresultsofusingBERT-rankerretrieved amounts of knowledge snippets in different rele-
andtop-1relevantknowledgeandtop-1relevantin vancelevelshasamarginaleffectonthislearned
Table3andshowthefullresultsintheAppendixC. dialogue system. However, we observe a signif-
Atfirst,withoutadaptiveknowledgeaugmenta- icant difference in the confidence level. We ob-
tion,wecomparethechoiceofresponsegeneration servethatusingonlythemostrelevantknowledge
withoutaugmentationandwith"always"augmenta- snippet enables the Aug-All to suffer less from a
tion(i.e.,No-AugversusAug-All). InTable3,we lowerconfidencelevel. Inparticular, theapplica-
observethatbyaugmentingatotalof4,964system tion of RAGate can even increase the confidence
responsesinthetestcollection,theconversational level of the conversation system in response gen-
model can generate more informative and effec- eration. This indicates that the confidence score
tiveresponsesaccordingtothereportedscoresof canalsocorrelatewiththequalityoftheaugmented
BLEU,ROUGEandBERTscore. Thisalignswith knowledgesnippets. Thisobservationisfurtherval-
thereportedeffectivenessofRAGinmanyexisting idatedusingknowledgesnippetswithfifth-ranking
studies. However, we also identify a significant positionsbyBERT-rankerandtheuseofTF-IDF
drop in the model‚Äôs generation confidence level. ranker. We include the full experimental results
AsdenotedbyVarshneyetal.(2023),alowercon- in Table 4 and attached in the Appendix. These
fidencelevelcancorrelatewithahigherchanceof observationsindicatethevalueofadaptivesystem
generatinghallucinatedresponses,whichcouldbe responseaugmentationviaRAGateingenerating
causedbytheunnecessaryuseofexternalknowl- high-quality outputs, ensuring faithful responses,
edge. Hence, to investigate the effectiveness of andpotentiallysavingretrievalcosts. Wealsoshow
adaptiveknowledgeaugmentation,weexaminethe thevalueofusingconfidencescorestoreflectthe
impactofusingRAGate. Accordingtothereported contributionofRAG.
experimentalresultsinTable3, theadaptiveaug-
6 Conclusions
mentedresponsegenerationwithfewerknowledge
snippets can indeed result in a higher confidence
Our study investigates a core research question
levelthanAug-All.
about whether retrieval-augmented generation is
Moreover,comparingtheperformancebetween
always useful to a conversational system. To an-
RAGateandrandomselectionsshowsthat,consid-
swer this research question, we propose adaptive
eringequalnumbers(230or787accordingtothe
retrieval-augmentedgenerationforconversational
classificationwithRAGate)ofsystemresponsesfor
systems and introduce corresponding gate func-
augmentation,RAGatecanfurtherresultinahigher
tions,RAGate,forexplicitcontrol. Acomprehen-
qualityofgeneratedresponse. RAGate-MHAeven
sive set of experiments and results show the RA-
enables results that are comparable to Aug-All‚Äôs
Gateapproachescaneffectivelyidentifyaugmen-
responsequality,withonly787turnaugmentations
tationneeds. Inaddition,RAGatecancapturehu-
instead of all 4964 turns. Specifically, the use of
manpreferencebyaugmentingthebeginningturns
RAGate-PEFT, which identifies 230 turns of sys-
ofconversations,andRAGatecanfurtheridentify
tem responses for knowledge augmentation, can
knowledgeaugmentationforassistingsuggestion-
evenoutperformtherandombaselinethataugments
makingandenrichingdescription. Whenapplying
787systemresponseturnswithimprovedresponse
RAGatetoconversationalsystems,weobservethat
quality. Apartfromtheimprovedresponsequality,
it can ensure comparable quality of generated re-
RAGatealsoenablestheconversationalmodelto
sponsesandenablethesystemtoincreasegenera-
maintainahighconfidencelevelandensurefaith-
tionconfidenceforfaithfuloutputs,especiallywith
fulresponses. Indeed,usingRAGate-MHA,which
theappropriateuseofrelevantknowledgesnippets.
augments 787 system responses, only lowers theLimitations AlexGravesandAlexGraves.2012. Longshort-term
memory. Supervisedsequencelabellingwithrecur-
There are three limitations of this study. At first, rentneuralnetworks,pages37‚Äì45.
due to the main focus of examining the adaptive
KaimingHe,XiangyuZhang,ShaoqingRen,andJian
retrieval-augmentedgenerationforaconversation Sun.2016. Deepresiduallearningforimagerecog-
system. We only consider a few examples of nition. In Proceedings of the IEEE conference on
retrieval techniques (TF-IDF and BERT-ranker), computervisionandpatternrecognition,pages770‚Äì
778.
which can be further extended to recent retrieval
techniques,suchasdensepassageretrievalforad- TaesukHong,JunheeCho,HaeunYu,YoungjoongKo,
ditionalinsights. Thesecondlimitationisthemiss- andJungyunSeo.2023. Knowledge-groundeddia-
loguemodellingwithdialogue-statetracking,domain
inguseoflargerlanguagemodels,suchasGPT-4,
tracking,andentityextraction. ComputerSpeech&
duetotheshortageofcomputationalresources. In-
Language,78:101460.
cludinglargerlanguagemodelsforconversational
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,
systemscouldintroduceadditionalexperimentalin-
YuanzhiLi, SheanWang, LuWang, WeizhuChen,
sights. Thethirdlimitationistheshortageofappro-
etal.2021. Lora: Low-rankadaptationoflargelan-
priateconversationaldataforextensiveevaluations. guagemodels. InInternationalConferenceonLearn-
Thisismainlycausedbytherecentdevelopmentof ingRepresentations.
theretrievalaugmentedgenerationtechniqueand
XinxianHuang,HuangHe,SiqiBao,FanWang,Hua
its application to conversational systems. Future Wu, and Haifeng Wang. 2021. Plato-kag: Unsu-
researchisencouragedtoaddressthislimitation. pervisedknowledge-groundedconversationviajoint
modeling. InProc.ofNLP4ConvAI.
EthicsStatement
Di Jin, Shuyang Gao, Seokhwan Kim, Yang Liu,
and Dilek Hakkani-Tur. 2021. Towards zero and
All experiments in this study were conducted us- few-shotknowledge-seekingturndetectionintask-
ingpubliclyavailabledatasetsandopen-released orientated dialogue systems. In 3rd Workshop on
NaturalLanguageProcessingforConversationalAI,
languagemodels,whichdonotcontainanyprivate
NLP4ConvAI2021,pages281‚Äì288.
informationthatcouldraiseethicalconcerns.
Minki Kang, Jin Myung Kwak, Jinheon Baek, and
SungJuHwang.2023. Knowledgegraph-augmented
languagemodelsforknowledge-groundeddialogue
References
generation. arXivpreprintarXiv:2305.18846.
SimranArora,AvanikaNarayan,MayeeFChen,Lau-
VladimirKarpukhin,BarlasOguz,SewonMin,Patrick
rel Orr, Neel Guha, Kush Bhatia, Ines Chami, and
Lewis,LedellWu,SergeyEdunov,DanqiChen,and
Christopher Re. 2022. Ask me anything: A sim-
Wen-tauYih.2020. Densepassageretrievalforopen-
plestrategyforpromptinglanguagemodels. InThe
domainquestionanswering. InProc.ofEMNLP.
EleventhInternationalConferenceonLearningRep-
resentations. AtoosaKasirzadehandIasonGabriel.2023. Inconver-
sationwithartificialintelligence: aligninglanguage
JimmyLeiBa,JamieRyanKiros,andGeoffreyEHin- modelswithhumanvalues. Philosophy&Technol-
ton. 2016. Layer normalization. arXiv preprint ogy,36(2):27.
arXiv:1607.06450.
SeokhwanKim,MihailEric,KarthikGopalakrishnan,
ZhiyuChen,BingLiu,SeungwhanMoon,Chinnadhurai BehnamHedayatnia,YangLiu,andDilekHakkani-
Sankar,PaulACrook,andWilliamYangWang.2022. Tur.2020. Beyonddomainapis: Task-orientedcon-
Ketod: Knowledge-enrichedtask-orienteddialogue. versationalmodelingwithunstructuredknowledge
In Findings of the Association for Computational access. InProc.ofSIGDIAL.
Linguistics: NAACL2022,pages2581‚Äì2593.
MojtabaKomeili,KurtShuster,andJasonWeston.2022.
Internet-augmenteddialoguegeneration. InProceed-
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
ingsofthe60thAnnualMeetingoftheAssociationfor
LukeZettlemoyer.2024. Qlora: Efficientfinetuning
ComputationalLinguistics(Volume1: LongPapers),
ofquantizedllms. AdvancesinNeuralInformation
pages8460‚Äì8478.
ProcessingSystems,36.
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Petroni,VladimirKarpukhin,NamanGoyal,Hein-
Fan,MichaelAuli,andJasonWeston.2018. Wizard richK√ºttler, MikeLewis, Wen-tauYih, TimRock-
of wikipedia: Knowledge-powered conversational t√§schel,etal.2020. Retrieval-augmentedgeneration
agents. In International Conference on Learning forknowledge-intensivenlptasks. AdvancesinNeu-
Representations. ralInformationProcessingSystems,33:9459‚Äì9474.Yu Li, Baolin Peng, Yelong Shen, Yi Mao, Lars Li- NeerajVarshney,WenlinYao,HongmingZhang,Jian-
den,ZhouYu,andJianfengGao.2022. Knowledge- shuChen,andDongYu.2023. Astitchintimesaves
groundeddialoguegenerationwithaunifiedknowl- nine: Detecting and mitigating hallucinations of
edge representation. In Proceedings of the 2022 llmsbyvalidatinglow-confidencegeneration. arXiv
Conference of the North American Chapter of the preprintarXiv:2307.03987.
AssociationforComputationalLinguistics: Human
LanguageTechnologies,pages206‚Äì218. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
RongzhongLian,MinXie,FanWang,JinhuaPeng,and Kaiser,andIlliaPolosukhin.2017. Attentionisall
Hua Wu. 2019. Learning to select knowledge for youneed. Advancesinneuralinformationprocessing
responsegenerationindialogsystems. InProc.of systems,30.
IJCAI.
HongruWang,WenyuHuang,YangDeng,RuiWang,
Sourab Mangrulkar, Sylvain Gugger, Lysandre De- Zezhong Wang, Yufei Wang, Fei Mi, Jeff Z Pan,
but, Younes Belkada, Sayak Paul, and Benjamin and Kam-Fai Wong. 2024. Unims-rag: A uni-
Bossan. 2022. Peft: State-of-the-art parameter- fied multi-source retrieval-augmented generation
efficient fine-tuning methods. https://github. for personalized dialogue systems. arXiv preprint
com/huggingface/peft. arXiv:2401.13256.
ErikMiehling,ManishNagireddy,PrasannaSattigeri, Shiquan Yang, Rui Zhang, and Sarah Erfani. 2020.
Elizabeth M Daly, David Piorkowski, and John T Graphdialog: Integratinggraphknowledgeintoend-
Richards.2024. Languagemodelsindialogue: Con- to-end task-oriented dialogue systems. In Proc. of
versationalmaximsforhuman-aiinteractions. arXiv EMNLP.
preprintarXiv:2403.15115.
Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie
JinjieNi,TomYoung,VladPandelea,FuzhaoXue,and Zhou, and Liang He. 2024. Boosting conversa-
ErikCambria.2023. Recentadvancesindeeplearn- tionalquestionansweringwithfine-grainedretrieval-
ing based dialogue systems: A systematic survey. augmentation and self-check. arXiv preprint
Artificialintelligencereview,56(4):3055‚Äì3155. arXiv:2403.18243.
YasumasaOnoe,MichaelZhang,EunsolChoi,andGreg Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong,
Durrett.2022. Entityclozebydate: Whatlmsknow Qi Liu, and Zhaofeng Liu. 2024. Evaluation of
aboutunseenentities. InProc.ofNAACL. retrieval-augmentedgeneration: Asurvey. Preprint,
arXiv:2405.07437.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
DarioAmodei,IlyaSutskever,etal.2019. Language Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao,
modelsareunsupervisedmultitasklearners. OpenAI Dongyan Zhao, and Rui Yan. 2020. Knowledge-
blog,1(8):9. groundeddialoguegenerationwithpre-trainedlan-
guagemodels. InProc.ofEMNLP.
Liliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018.
Towardsuniversaldialoguestatetracking. InProc. Kangyan Zhou, Shrimai Prabhumoye, and Alan W
ofEMNLP. Black.2018. Adatasetfordocumentgroundedcon-
versations. InProceedingsofthe2018Conferenceon
AlirezaSalemiandHamedZamani.2024. Evaluating EmpiricalMethodsinNaturalLanguageProcessing,
retrieval quality in retrieval-augmented generation. pages708‚Äì713.
Preprint,arXiv:2404.13781.
A PromptsforRAGate-Prompt
ProchetaSen,XiWang,RuiqingXu,andEmineYilmaz.
2023. Task2kb: Apublictask-orientedknowledge
In this section, we list the used prompts for the
base. In Proceedings of the AAAI Conference on
RAGate-Promptgatemechanism.
ArtificialIntelligence.
Zero-ShotPrompt:
Weizhou Shen, Yingqi Gao, Canbin Huang, Fanqi Below is an instruction that describes a task.
Wan, Xiaojun Quan, and Wei Bi. 2023. Retrieval-
Please respond with ‚ÄòTrue‚Äô or ‚ÄòFalse‚Äô only that
generation alignment for end-to-end task-oriented
appropriatelycompletestherequest.
dialoguesystem. arXivpreprintarXiv:2310.08877.
###Instruction: Analysetheconversationalcon-
Tianyuan Shi, Liangzhi Li, Zijian Lin, Tao Yang, Xi-
textsofar. Generateanappropriateresponse. Con-
aojunQuan,andQifanWang.2023. Dual-feedback
sidertheinvovledentites. Estimateifaugmenting
knowledgeretrieval for task-oriented dialogue sys-
tems. arXivpreprintarXiv:2310.14528. the response with external knowledge is helpful
withanoutputof‚ÄòTrue‚Äôor‚ÄòFalse‚Äôonly.
KurtShuster,SpencerPoff,MoyaChen,DouweKiela,
###Input: [ConverstionContextInput]
and Jason Weston. 2021. Retrieval augmentation
###Response:
reduces hallucination in conversation. In Proc. of
EMNLP. In-ContextLearningPrompt:AugmentationVariants #Augs BLEU ROUGE-1 ROUGE-2 ROUGE-L BERTScore Confidence
No-Aug 0 9.38 0.4111 0.2246 0.3780 0.8105 9.3425
AugmentBERTRankerRetrievedKnowledge
RAGate-Llama 230 10.45 0.4165 0.2273 0.3825 0.8144 9.3374
RAGate-MHA 787 12.14 0.4227 0.2318 0.3882 0.8192 9.3083
Random-Aug 230 9.53 0.4119 0.2250 0.3784 0.8110 9.2984
Random-Aug 787 10.01 0.4138 0.2265 0.3795 0.8126 9.1877
Human-label 631 11.66 0.4198 0.2297 0.3856 0.8176 9.2550
Augment-All 4964 16.08 0.4301 0.2364 0.3927 0.8258 8.3677
AugmentTF-IDFRankerRetrievedKnowledge
RAGate-Llama 230 10.52 0.4165 0.2273 0.3826 0.8144 9.3418
RAGate-MHA 787 12.11 0.4233 0.2319 0.3889 0.8193 9.3058
Random-Aug 230 9.47 0.4118 0.2251 0.3783 0.8110 9.3006
Random-Aug 787 9.93 0.4136 0.2259 0.3793 0.8125 9.1942
Human-label 631 11.60 0.4198 0.2293 0.3854 0.8175 9.2639
Augment-All 4964 15.76 0.4289 0.2345 0.3914 0.8256 8.4188
AugmentRank-1RelevantKnowledge
RAGate-Llama 230 10.54 0.4162 0.2271 0.3822 0.8142 9.3642
RAGate-MHA 787 11.99 0.4227 0.2316 0.3883 0.8191 9.3774
Random-Aug 230 9.51 0.4117 0.2250 0.3784 0.8110 9.3328
Random-Aug 787 10.01 0.4140 0.2267 0.3800 0.8127 9.2982
Human-label 631 11.52 0.4189 0.2289 0.3846 0.8170 9.3218
Augment-All 4964 16.05 0.4308 0.2365 0.3944 0.8259 9.0655
AugmentRank-5RelevantKnowledge
RAGate-Llama 230 10.47 0.4161 0.2272 0.3823 0.8142 9.3592
RAGate-MHA 787 12.18 0.4224 0.2314 0.3883 0.8192 9.3704
Random-Aug 230 9.52 0.4118 0.2252 0.3785 0.8110 9.3315
Random-Aug 787 10.01 0.4135 0.2263 0.3794 0.8127 9.2961
Human-label 631 11.58 0.4186 0.2287 0.3845 0.8170 9.3210
Augment-All 4964 15.97 0.4290 0.2349 0.3927 0.8256 9.0604
Table4: PerformanceofapplyingRAGateandcomparedtoKETODontheSGDdataset. Confidenceiscalculated
bytheaveragevalueoverthelowestlogitofeachgeneration.
Below is an instruction that describes a task. ###Response: False
Please respond with ‚ÄòTrue‚Äô or ‚ÄòFalse‚Äô only that ###Input: [ConverstionContextInput]
appropriatelycompletestherequest. ###Response:
###Instruction: Analysetheconversationalcon-
textsofar. Generateanappropriateresponse. Con- B ImpactofRetrievalQualityon
sidertheinvovledentites. Estimateifaugmenting AdaptiveRAG
the response with external knowledge is helpful
To have a successful conversation model with a
withanoutputof‚ÄòTrue‚Äôor‚ÄòFalse‚Äôonly.
retrieval-augmentedsystem,twomaincriteriamust
###Example1: USER:I‚Äômplanningatrip,can
bemet. Oneisidentifyinginsufficientcontext,and
you help me look for a flight? SYSTEM: Which
the other is the quality of retrieved information
day are you planning to return and from which
(Salemi and Zamani, 2024; Yu et al., 2024). A
city? USER:IwanttogofromNYCthedayafter
conversational model performs better when both
tomorrow and return on the 13th of this month.
criteria are satisfied. In our proposed approach,
SYSTEM:Wherewouldyouliketogo? USER:I
as shown in Table 2, we have already assessed
wanttogotoVancouver,BC.Canyoulookfora
whether our adaptive retrieval method can detect
PremiumEconomyclassticket. SYSTEM:Ifound
insufficientcontext. Wefurtherexploredtodeter-
1 flight for you. It is a Delta Airlines flight that
mine whether our model can inherently estimate
takesoffat6amandreturnsat2:50am. Theprice
thequalityoftheretrievedsnippetstoaddresssuch
is$505. USER:Whatisthedepartureairport,and
insufficiencyand,basedonthat,decideonthere-
howmanystopsdoestheflighthave?
trieval. Althoughwedonotexplicitlyprovidere-
###Response: True trievedsnippetstoourmodel,retrievalcomeswith
###Example2: USER:Getmebusticketstoa acorpusthatincludespotentiallyrelevantknowl-
ChereventonMarch6th. SYSTEM:Howmanyto edge snippets. Consequently, given a query and
buy? USER:onlyone,please. aretrievalcollection,itcanbeestimatedwhetherusefulinformationforthequeryexistsinthecorpus
toaddresstheinsufficientcontext. Toinvestigate
byfollowingthisdirection,werandomlyselected
50samplesfrominstanceswhereourproposedap-
proach (RAGate-MHA, the best-performing gate
model)predictedusingretrievalaugmentation. We
askeddomainexperts(co-authors)toscorewhether
theythoughttheretrievedsnippetsinthosescenar-
ioscouldbeusefultoresponsegeneration. Users
ratedthesnippetsonascaleof0‚àí4,withscores
of3or4indicating‚Äòuseful‚Äôor‚Äòhighlyuseful‚Äô. We
found that in 54% of cases where the prediction
wasforaugmentation,usersalsofoundthesnippets
useful. Thisindicatesthatourproposedapproach
can implicitly capture the potential for obtaining
high-qualityretrievalsnippets.
C Additionalexperimentalresultsabout
RAGateforResponseGeneration
InTable4,weincludethecompleteexperimental
resultsofapplyingRAGateforadaptiveretrieval-
augmented system response generation. Specif-
ically, explore the use of retrieved knowledge
snippets to different extents of relevance. We in-
cludetop-3knowledgesnippetsretrievedbyBERT-
ranker and TF-IDF. In addition, we also explore
theuseofknowledgesnippetsindifferentranking
positions (rank 1 and 5) according to the BERT-
ranker retriever. The experimental result shows
thatpreciselyusingasuitableamountofrelevant
knowledge can generate a response with higher
confidence (i.e., less is more). In addition, this
observationalsoindicatesthepotentialuseofcon-
fidence levels to evaluate the quality of the aug-
mentedknowledge.