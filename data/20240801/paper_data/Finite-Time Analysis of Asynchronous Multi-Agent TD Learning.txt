Finite-Time Analysis of Asynchronous Multi-Agent TD Learning
Nicolo` Dal Fabbro, Arman Adibi, Aritra Mitra and George J. Pappas
Abstract—Recent research endeavours have theoretically Given the above premise, our main goal in this paper
shown the beneficial effect of cooperation in multi-agent re- is to contribute to the literature on finite-time analyses of
inforcementlearning(MARL).InasettinginvolvingN agents,
MARL under realistic communication models. To that end,
this beneficial effect usually comes in the form of an N-fold
we consider an asynchronous MARL framework in which
linear convergence speedup, i.e., a reduction - proportional
to N - in the number of iterations required to reach a multiple agents cooperate to evaluate a common policy
certain convergence precision. In this paper, we show for the via temporal difference (TD) learning. To collaboratively
first time that this speedup property also holds for a MARL evaluate the common policy, agents transmit their local TD
framework subject to asynchronous delays in the local agents’
update directions to a central server via up-link commu-
updates.Inparticular,weconsiderapolicyevaluationproblem
nication channels subject to asynchronous bounded delays.
in which multiple agents cooperate to evaluate a common
policy by communicating with a central aggregator. In this Asynchronous settings of this kind have been theoretically
setting, we study the finite-time convergence of AsyncMATD, and empirically studied for FL and distributed optimiza-
anasynchronousmulti-agenttemporaldifference(TD)learning tion [5], [6], [9]. On the other hand, although asynchronous
algorithm in which agents’ local TD update directions are
MARL implementations have shown promising empirical
subjecttoasynchronousboundeddelays.Ourmaincontribution
performance, like in the case of parallel actor-learner frame-
is providing a finite-time analysis of AsyncMATD, for which
we establish a linear convergence speedup while highlighting works [10], [11], little to nothing is known regarding their
theeffectoftime-varyingasynchronousdelaysontheresulting non-asymptoticconvergenceguaranteesandmulti-agentcol-
convergence rate. laborative gains. Indeed, the only existing study provid-
ing finite-sample convergence guarantees for asynchronous
I. INTRODUCTION
MARL[12]establishescollaborativeperformancegainsonly
Training reinforcement learning (RL) algorithms is well-
underasimplifyingi.i.d.samplingassumptionontheagents’
knowntobeacriticallytimeconsumingtaskthatcanrequire
observations, i.e., considering observations that are not tem-
severaldatasamplestoachieveadesiredlevelofaccuracy.In
porally correlated. However, even in the non-delayed single-
an effort to reduce the sample-complexity of RL, recent re-
agent case, the major technical hurdle in the finite-time
searchfindingsinmulti-agentRL(MARL)havetheoretically
analysis of RL algorithms (like TD learning) relative to
shown that, despite temporal correlations in each agent’s
optimization/supervised learning, comes precisely from the
observations, multi-agent cooperation provides collaborative
fact that the agent’s observation sequence is generated by a
performancegains[1],[2].InaMARLframeworkwhereN
Markov chain, and, as such, exhibits temporal correlations.
agents communicate via a central entity - a common setup
For such settings, finite-time convergence bounds have only
in the emerging federated RL (FRL) paradigm [3] - these
recentlybeenprovidedin[13],[14]viasomefairlyinvolved
gainsmanifestthemselvesintheformofanN-foldreduction
analysis. Thus, for the MARL setting we consider with
in the sample-complexity relative to when each agent acts
Markovian sampling and asynchronous delays, establishing
alone. To achieve such gains, the agents must communicate.
collaborative performance benefits turns out to be highly
However,inreal-worldmulti-agentsystems,inter-nodecom-
non-trivial. Nonetheless, providing such an analysis is the
munication imposes strong constraints, such as limited link
main contribution of this paper.
capacityandtransmissiondelays[4]–[6].Whiletheeffectsof
Contributions.WeproposeandanalyzeAsyncMATD,an
these constraints have been widely analyzed for distributed
asynchronous multi-agent TD learning algorithm in which
optimization and federated learning (FL), analogous studies
transmission of agents’ local TD update directions is sub-
in the context of MARL are few and far between. Indeed,
ject to asynchronous bounded delays. At each iteration of
collaborative gains under communication constraints have
AsyncMATD, the server updates the value function associ-
onlyveryrecentlybeenestablishedforFRLunderfinite-rate
ated with the policy to be evaluated using linear function
links and wireless noisy channels [7], [8].
approximation.Todoso,itusespotentiallystaleagents’TD
update directions, where the staleness is a consequence of
N. Dal Fabbro is with the Department of Information Engineering,
UniversityofPadova.Email:dalfabbron@dei.unipd.it.A.Adibiiswiththe the delays. Our main contribution is to provide a finite-time
DepartmentofElectricalandComputerEngineering,PrincetonUniversity. analysis of AsyncMATD that clearly reveals the effect of
Email:aadibi@princeton.edu.A.MitraiswiththeElectricalandComputer
the delay sequence on the resulting convergence rate; see
Engineering Department, North Carolina State University. Email: ami-
tra2@ncsu.edu.G.J.PappasiswiththeElectricalandSystemsEngineering Theorem 1. Remarkably, we establish an N-fold linear con-
Department, University of Pennsylvania. Email: pappasg@seas.upenn.edu. vergencespeedupforAsyncMATDthatshowsthebeneficial
ThisworkwassupportedbyNSFAward1837253,ARLgrantDCISTCRA
effect of collaboration even in the presence of asynchronous
W911NF-17-2-0181 and the Italian Ministry of Education, University and
ResearchthroughthePRINprojectno.2017NS9FEY. delays in the agents’ updates, and Markovian sampling.
4202
luJ
92
]AM.sc[
1v14402.7042:viXraII. SYSTEMMODELANDPROBLEMFORMULATION
We consider a setting in which N agents independently
Server
interact with replicas of the same Markov Decision Process
(MDP), which we denote by M = (S,A,P,R,γ). For the
MDP, we consider a finite set S of n states, a finite action
1 i N
spaceA,asetofaction-dependentMarkovtransitionkernels
P, a reward function R, and we denote by γ ∈ (0,1) the
discountfactor.Thegoalistocollectivelyevaluatethevalue
function associated with a policy µ : S → A. To do so,
the agents communicate with a central aggregator (server) MDP
viaup-linktransmissionssubjecttoasynchronousdelays.We
nowreviewthekeyconceptsregardingpolicyevaluationwith Fig.1. SystemModel.Agents1,...,Ncooperativelylearnacommonpol-
value function approximation. Then, we formally describe icyinteractingwithreplicasofthesameMDP.Ateachiterationk,theserver
the asynchronous communication model, outline our key
usestheavailabledelayedupdatedirectionswithdelaysτ 1,k,...,τ N,k.
objectives, and highlight the major technical challenges.
Policy Evaluation with Linear Function Approxima-
based on the received parameter θ and its k-th observation
tion.Thepurposeoftheconsideredpolicyevaluationsetting k
tuple o =(s ,r ,s ), computes a local TD update
is to evaluate, for each state s ∈ S, the value function i,k i,k i,k i,k+1
direction g(θ ,o ). Note that the observation tuple o is
V (s), which is the discounted expected cumulative reward k i,k i,k
µ obtained by playing policy µ at iteration k, which implies
obtainedbyplayingpolicyµstartingfrominitialstates.The
taking an action a = µ(s ), observing the next state
policy µ interacts with the MDP M to generate a Markov i,k i,k
s ∼ P (·|s ), and collecting an instantaneous reward
Reward Process (MRP) characterized by a reward function i,k+1 µ i,k
r =R (s ).Agenti’sTDupdatedirectionisasfollows:
R :S →R,andaMarkovchainwithtransitionprobability i,k µ i,k
µ
matrix P . Formally, we have g(θ ,o )=(r +γ⟨ϕ′ ,θ ⟩−⟨ϕ′ ,θ ⟩)ϕ′ .
µ k i,k i,k si,k+1 k si,k k si,k
(cid:34) (cid:88)∞ (cid:35) Although all agents play the same policy µ, and interact
V (s)=E γkR (s )|s =s , (1)
µ µ k 0 with replicas of the same MDP, they all experience differ-
k=0 ent realizations of the local observation sequences {o }.
i,k
wheres representsthestateoftheMarkovchainassociated We assume that the local observation sequences {o } are
k i,k
withthekernelP ,wheninitializedfroms =s.Thepolicy statistically independent across the agents. For each agent i,
µ 0
evaluation setting that we consider is model-free, i.e., we however,theobservationsovertimearecorrelatedsincethey
assume that the Markov transition kernel P and the reward are all part of a single Markov chain.
µ
function R are unknown. Convergence speedup. Based on the independence prop-
µ
Asiscommon,weconsideralinearvaluefunctionapprox- erty of the observation sequences, one would expect that
imation setting in which the n-dimensional value function exchanging agents’ local TD update directions should help
V is approximated by vectors in a linear subspace of Rn in reducing the variance in the estimate of θ∗. Achieving
µ
spanned by a set of m basis vectors {ϕ }m . Resorting to this variance reduction in the form of an N-fold linear
ℓ ℓ=1
value function approximation is particularly needed when convergence speedup is precisely the aim of multi-agent
the state space S is very large, and indeed usually m≪ n. TD (MATD) learning. The model we described above is
Let Φ ≜ [ϕ ,...,ϕ ] ∈ Rn×m be the feature matrix. As is a synchronous MARL setting where at each time-step k,
1 m
standard [13], we assume that the columns of Φ are inde- the server updates the model vector θ using the average
k
pendent and that the rows are normalized. The parametric of the agents’ local TD update directions from time-step
approximation Vˆ of V is given by V(θ) := Vˆ = Φθ, k, transmitted over the up-link channels, i.e., there are no
θ µ θ
whereθ ∈Rm istheapproximationparameter.Denotingthe delays. Departing from this setting, we consider, for the
s-th row of Φ by ϕ′, the approximation of V (s) is given first time, the crucial case in which agents’ local TD update
s µ
by Vˆ (s)=⟨θ,ϕ′⟩. directionsarecomputedinanasynchronousfashion.Inwhat
θ s
Asynchronous multi-agent TD Learning. In the setting follows, we describe our model of asynchrony.
described above, the collective aim of the agents is to Asynchrony model of AsyncMATD. We now describe the
estimate the best linear approximation parameter for V model for asynchronous MATD learning that we consider
µ
in the span of Φ; we denote this optimal parameter by in this paper, which is analogous to the models studied,
θ∗. To achieve this goal, agents execute a multi-agent TD for example, in FL [6], [9] and asynchronous MARL [12].
(MATD)variantoftheclassical TD(0)algorithm[15],aswe At each time-step k, the server updates the model vector
describe next. We assume that agents initialize the MATD θ using the average of asynchronously delayed agents’
k
algorithmstartingfromthesamestates ∈S andparameter local TD update directions. Specifically, for each agent i, at
0
θ ∈Rm.InasynchronousMATDsetting,ateachtime-step iteration k, the corresponding available TD update direction
0
k ∈ N, a global parameter θ is broadcasted in down-link issubjecttoabounded delayτ .Definet ≜(k−τ ) ,
k i,k i,k i,k +
by the server to all agents. Each agent i ∈ [N], in turn, where, for x ∈ R, (x) = max{0,x}. The server updates
+the model vector θ according to the following rule: Assumption 2. The Markov chain induced by the policy µ
k
is aperiodic and irreducible.
θ =θ +αv , (2)
k+1 k k
Assumption 2 implies the existence of a unique stationary
where α is a constant step-size/learning rate, and
distribution π for the Markov transition matrix P [17]. Let
µ
1 (cid:88)N D∈Rn×n be a diagonal matrix with entries given by the n
v k = N g(θ ti,k,o i,ti,k). (3) elements of π and define Σ≜Φ⊤DΦ. Since Φ is assumed
i=1 to be full column rank, Σ is full rank with a strictly positive
In this work, we assume that the down-link communication smallest eigenvalue ω <1 that shows up in the convergence
fromtheservertotheagentsisnotsubjecttodelays.Suchan analysis.Next,wedefinethesteady-statelocalTDdirection:
assumption is practically motivated by the fact that in most
client-server architectures (e.g., wireless networks [4]), the g¯(θ)≜E si,k∼π,si,k+1∼P µ(·|si,k)[g(θ,o i,k)],∀θ ∈Rm. (4)
main communication bottleneck comes from up-link trans-
The deterministic recursion θ = θ +αg¯(θ ) captures
missions, instead of down-link broadcasting. In the rest of k+1 k k
thelimitingbehavioroftheTD(0)updaterule.In[13],itwas
thepaper,werefertotheupdaterulein(2)asAsyncMATD.
shown that the iterates generated by this recursion converge
Notethattheupdatedirectionv usedbytheserverfeatures
k exponentially fast to θ∗, where θ∗ is the unique solution of
iterates θ and observations o from potentially stale
ti,k i,ti,k the projected Bellman equation Π T (Φθ∗)=Φθ∗. Here,
time-steps. Furthermore, the delays τ ,...,τ can differ D µ
1,k N,k
Π (·) is the projection operator onto the subspace spanned
acrossagents.Wemakethefollowingassumptiononthede- D
by {ϕ } with respect to the inner product ⟨·,·⟩ , and
laysequence,whichiscommoninthestudyofasynchronous ℓ ℓ∈[m] D
T :Rn →Rn is the policy-specific Bellman operator [16].
distributed optimization and FL [5], [9]. µ
Next, we define the notion of mixing time τ that will play
ϵ
Assumption 1. There exists a positive integer τ max > 0 a crucial role in our non-asymptotic analysis.
such that 0≤τ ≤τ , for all i and for all k.
i,k max
Definition 1. Let τ be the minimum time step such that
ϵ
Objective and Challenges. In the rest of the paper, ∥E[g(θ,o )|o ] − g¯(θ)∥ ≤ ϵ(∥θ∥+1),∀k ≥τ ,∀θ ∈
i,k i,0 ϵ
we provide a finite-time analysis of AsyncMATD. This Rm,∀i∈[N],∀o .1
i,0
poses several challenges. In fact, even in the single-agent
setting,providinganon-asymptoticanalysisofTD(0)without Assumption2impliesthattheMarkovchaininducedbyµ
mixesatageometricrate[17],i.e.,thetotalvariationdistance
performing intermediate projection steps is known to be
between P(s =·|s =s) and the stationary distribution
challenging due to the temporal correlation between the i,k i,0
π decays exponentially fast ∀k ≥ 0,∀i ∈ [N],∀s ∈ S.
Markov samples in the iterative learning process. Crucially,
This, in turn, implies the existence of some K ≥ 1 such
thischallengeisabsentinasynchronousstochasticoptimiza-
that τ in Definition 1 satisfies τ ≤ Klog(1) [18]. We
tion where one assumes i.i.d. data, precluding the use of ϵ ϵ ϵ
infer that the mixing time τ scales logarithmically in the
techniques used in this line of work. For the analysis of ϵ
AsyncMATD, we encounter further obstacles: the update precision dictated by ϵ. For our purpose, we will set ϵ=αq,
rule involves the use of multiple correlated iterates θ ,
whereq isanintegersatisfyingq ≥2.Unlikethecentralized
i = 1,...,N, at which the local TD update
directit oi, nk
s
settingwhereq =1suffices[13],[14],toestablishthelinear
speedup property, we will require q ≥ 2. Henceforth, we
are asynchronously computed. Indeed, note that, although
the observation sequences o are statistically independent will drop the subscript of ϵ = αq in τ ϵ and simply refer to
i,k τ as the mixing time. Let δ2 ≜ ∥θ∗−θ ∥2 and define by
across agents, the iterates used to compute the local TD k k
σ ≜ max{1,r¯,∥θ∗∥,δ } the “variance” of the observation
update directions are all correlated. This aspect introduces 0
model for our problem. We can now state our main result.
the need for a much finer analysis when we want to provide
finite-time convergence guarantees. Furthermore, unlike a Theorem 1. Consider the update rule of AsyncMATD in
single-agent setting, we aim to establish an N-fold linear
(2).ThereexistuniversalconstantsC ,C ,C ,C ≥1,such
0 1 2 3
convergence speedup while jointly dealing with the chal- that, for α≤ ω(1−γ) and T ≥τ +2τ ,
lengesoutlinedabove.Thisnecessitatesaverycarefulstudy,
C0(τ+τmax) max
which we illustrate below.
E(cid:2) δ2(cid:3)
≤exp(cid:18) −α(1−γ)ωT(cid:19)
C σ2
T 2(τ +τ ) 1
III. MAINRESULT max
(5)
(τ +τ )σ2 (cid:18) C α (cid:19)
In this section, we state and discuss our main result, + max 2 +C α3 .
which provides a non-asymptotic convergence bound for ω(1−γ) N 3
AsyncMATD. We start by providing the necessary assump-
Discussion: We now remark on the main takeaways from
tions and technical machinery. We make the standard as-
Theorem1.Fromtheboundin(5),wenotethatAsyncMATD
sumption that the rewards are uniformly bounded, i.e., that
guarantees linear convergence (in mean-square sense) to a
R µ(s) ≤ r¯,∀s ∈ S and for some r¯ > 0, which ensures ball around θ∗ whose radius depends on the variance σ2
thatthe valuefunction in(1) iswell-defined. Next,we make
of the noise model. We now comment on the effect of the
a key assumption which is crucial for the non-asymptotic
analysis of TD learning algorithms [1], [7], [13], [14], [16]. 1Unlessotherwisespecified,weuse∥·∥todenotetheEuclideannorm.asynchronous delays on the convergence bound, and on the which, squared, using r¯≤σ, yields
linear convergence speedup established by the theorem.
∥g(θ,o )∥2 ≤8(∥θ∥2+σ2). (11)
Effect of asynchronous delays. From (5), note how both i,k
the exponent of the linear convergence term and the radius We will often use the fact that, from the definition of the
of the noise ball are impacted by the delay sequence via the mixing time in Definition 1, we have, for a given iteration
maximum delay τ max. Indeed, compared to the centralized k ≥τ, defining Θ
i,k
≜{θ k−τ,o i,k−τ},
TD case [14, Theorem 7], and to the synchronous federated
∥E[g(θ ,o )|Θ ]−g¯(θ )∥≤αq(∥θ ∥+1)
TD case [7, Theorem 1], we see that for AsyncMATD, the k−τ i,k i,k k−τ k−τ
(12)
noise ball gets multiplied by the sum of mixing time and
The proof also relies on the following result from [19]:
maximum delay, i.e., τ + τ . In essence, our analysis
max
reveals that τ +τ max plays the role of an effective delay. Lemma 2. Let V k be non-negative real numbers that satisfy
Interestingly, an immediate implication is that if the under-
V ≤pV +q max V +β,
lying Markov chain mixes slowly, i.e., has a larger mixing k+1 k ℓ
(k−d(k))+≤ℓ≤k
time τ, then the effect of the delay is less pronounced. This
for β,p,q >0. Here, k ≥0 and 0≤d(k)≤d for some
appears to be a novel observation. max
d ≥0. If p+q <1, then we have
Linear convergence speedup. Compared to the centralized max
setting [14, Theorem 7], the noise variance term in (5) gets V ≤ρkV +ϵ,
k 0
scaled down by a factor of N up to a higher-order O(α3)
term that, for small enough α, is dominated by (α/N). where ρ=(p+q)1/(1+dmax) and ϵ= β .
1−p−q
To better illustrate the linear speedup effect, consider the
We will also use the fact that, for any a,b∈R,c≥0,
following choice of α and T (define τ¯≜τ +τ ):
max √ b 1(cid:18) b2(cid:19)
τ¯logNT 2C Nτ¯2logNT ab=a c√ ≤ ca2+ , (13)
α= , and T ≥ 0 . (6) c 2 c
ω(1−γ)T ω2(1−γ)2
and also the fact that, for a ∈R,i=1,...,N,
With the above choices, and simple manipulations of the i
bound in (5), it can be explicitly shown that (cid:32) N (cid:33)2 N
(cid:88) (cid:88)
(cid:18) σ2τ¯2log(NT)(cid:19) a i ≤N a2 i. (14)
E[δ T2]≤O
ω2(1−γ)2NT
. (7) i=1 i=1
Equipped with the above basic results, we now provide an
The above bound tells us that AsyncMATD yields a
outline of our proof before illustrating the technical details.
convergence rate of O(1/(NT)), which is a factor of N Outline of the proof.Recallt ≜(k−τ ) .Wewrite
better than the O(1/T) rate in the centralized case [13]. We i,k i,k +
the update rule (2) as
remark that this is the first analysis for asynchronous multi-
agentandfederatedRLthatprovidesfinite-timeconvergence θ =θ +αv =θ +αg¯(θ )−αe , (15)
k+1 k k k k k
guarantees, while jointly establishing an N-fold linear con-
with e ≜g¯(θ )−v . Thus,
vergence speedup under Markovian sampling. k k k
N
IV. PROOFOFTHEMAINRESULT
e =
1 (cid:88)(cid:0)
g¯(θ )−g(θ ,o
)(cid:1)
. (16)
k N k ti,k i,ti,k
In this section, we prove Theorem 1. We start by intro-
i=1
ducing the following definitions to lighten the notation:
We analyze the following recursion:
η k(i ,) τ(θ)≜∥E[g i,k(θ,o i,k)|o i,k−τ]−g¯(θ)∥, k ≥τ, δ k2
+1
=T 1+α2T 2−2αT 3, with
δ k,h ≜∥θ k−θ k−h∥, k ≥h≥0, (8) T =∥θ −θ∗+αg¯(θ )∥2
d ≜ max E(cid:2) δ2(cid:3) , k ≥τ +2τ . 1 k k (17)
k k−2τmax−τ≤j≤k j max T
2
=∥e k∥2
Forouranalysis,wewillneedthefollowingresultfrom[13].
T =⟨θ −θ∗+αg¯(θ ),e ⟩.
3 k k k
Lemma 1. The following holds ∀θ ∈Rm:
The most important part of the proof consists of obtaining a
⟨θ∗−θ,g¯(θ)⟩≥ω(1−γ)∥θ∗−θ∥2. bound of the following form:
E(cid:2) δ2 (cid:3) ≤pE(cid:2) δ2(cid:3) +O(α2(τ +τ ))d +B , (18)
k+1 k max k α,N
We will also use the fact that the random TD update
directionsandtheirsteady-stateversionsare2-Lipschitz[13], where d k is as in (8), p<1 is a contraction factor and
i.e., ∀i∈[N],∀k ∈N, and ∀θ,θ′ ∈Rm, we have: σ2
B =O(α2(τ +τ )) +O(α4)σ2, (19)
max{∥g(θ)−g(θ′)∥,∥g¯(θ)−g¯(θ′)∥}≤2∥θ−θ′∥. (9) α,N max N
which guarantees the linear speedup effect with N. The
From [14], we also have that ∀i∈[N],∀k ∈N,∀θ ∈Rm:
bound in (18) allows us to obtain the desired result, by
∥g(θ,o )∥≤2∥θ∥+2r¯, (10) picking a step size small enough and applying Lemma 2.
i,kGiven this outline, in the following we provide bounds for Usingthefactthatg¯(θ∗)=0,andCauchy-Schwarzinequal-
E[T ], E[T ] and E[T ]. ity followed by Jensen’s inequality, we can write
1 2 3
Bounding E[T ]. Note that
1 N
T =∥θ −θ∗+αg¯(θ )∥2 E[V ]≤ (cid:88) E(cid:104) η(i) (θ∗)(cid:105) ×E(cid:104) η(j) (θ∗)(cid:105)
1 k k
(20)
22 ti,k,τ tj,k,τ
=δ2+2α⟨θ −θ∗,g¯(θ )⟩+α2∥g¯(θ )∥2. i,j=1 (30)
k k k k i̸=j
Note that, using Lemma 1, we get ≤N2α2q(∥θ∗∥+σ)2 ≤4N2α2qσ2.
⟨θ k−θ∗,g¯(θ k)⟩≤−(1−γ)ωδ k2, (21) Plugging the above bounds on E[V 1] and E[V 2] in (25), we
and using (9) we get can conclude the proof of the lemma.
∥g¯(θ k)∥2 =∥g¯(θ k)−g¯(θ∗)∥2 ≤4δ k2. (22) We are now in the position to proceed bounding E[T 2].
Combiningthetwoboundsaboveandtakingtheexpectation, E[T ]=E(cid:2) ∥e ∥2(cid:3) =E(cid:2) ∥g¯(θ )−v ∥2(cid:3)
2 k k k
(31)
E[T 1]≤(1−2α(1−γ)ω)E(cid:2) δ k2(cid:3) +4α2E(cid:2) δ k2(cid:3) . (23) ≤2E(cid:2) ∥g¯(θ k)∥2+∥v k∥2(cid:3) .
Bounding E[T ]. We need the following result. Note that ∥g¯(θ )∥2 = ∥g¯(θ ) − g¯(θ∗)∥2 ≤ 4δ2, and so
2 k k k
using Lemma 3 we get
Lemma 3. For k ≥τ +τ , we have
max
E(cid:2) ∥v k∥2(cid:3) ≤8 k−τmm aa xx ≤j≤kE(cid:2) δ j2(cid:3) +32σ N2 +8σ2α2q (24) E[T 2]≤24 k−τmm aa xx ≤j≤kE(cid:2) δ j2(cid:3) +64σ N2 +16σ2α2q. (32)
Proof. We write Bounding E[T 3]. We now bound E[T 3], which represents
2 the major technical burden of the proof. We need the
∥v k∥2 ≤ N2(V 1+V 2), with following result.
V
=∥(cid:88)N
g(θ ,o )−g(θ∗,o )∥2,
Lemma 4. Let k ≥τ max+h, for some h≥0. Then,
1
i=1
ti,k i,ti,k i,ti,k (25)
E(cid:2) δ2 (cid:3)
≤8α2h2(cid:18)
d
+4σ2 +σ2α2q(cid:19)
(33)
N k,h k N
(cid:88)
V =∥ g(θ∗,o )∥2.
2 i,ti,k Proof. Note that, using Lemma 3,
i=1
We now bound V 1. E(cid:2) δ k2 ,h(cid:3) =E(cid:2) ∥θ k−θ k−h∥2(cid:3)
N k−1
V ≤N(cid:88) ∥g(θ ,o )−g(θ∗,o )∥2 ≤h (cid:88) E(cid:2) ∥θ −θ ∥2(cid:3)
1 ti,k i,ti,k i,ti,k l+1 l
i=1 l=k−h
N k−1
( ≤9) 4N(cid:88) δ2 . Thus, (26) ≤α2h (cid:88) E(cid:2) ∥v ∥2(cid:3)
ti,k l
i=1 l=k−h
(34)
E[V 1]≤4N(cid:88) i=N 1E(cid:104) δ t2 i,k(cid:105) ≤4N2 k−τmm aa xx ≤j≤kE(cid:2) δ j2(cid:3) ≤α2h l=k (cid:88) k− −1 h(8 l−τmm aa xx ≤j≤lE(cid:2) δ j2(cid:3)
We now proceed to bound V
2
=V 21+V 22, with +32σ2
+8σ2α2q)
N N
V =(cid:88) ∥g(θ∗,o )∥2 (cid:18) σ2 (cid:19)
21 i,ti,k ≤8α2h2 d +4 +σ2α2q .
i=1 k N
N (27)
(cid:88)
V = ⟨g(θ∗,o ),g(θ∗,o )⟩.
22 i,ti,k j,tj,k
i,j=1 Now, we can write T =K+T , with
i̸=j 3 32
We see that, using (11), we get K =⟨θ k−θ∗,e k⟩,
(35)
(cid:88)N T 32 =α⟨g¯(θ k),e k⟩.
V ≤8 (∥θ∗∥2+σ2) ≤16Nσ2. (28)
21 Note that, using Cauchy-Schwarz and (13),
i=1
Now, using the fact that the observations o i,k and o j,k′ are T 32 ≤ α 2 (cid:0) ∥g¯(θ k)∥2+∥e k∥2(cid:1)
independent for i̸=j and for any k,k′ ≥0, α (36)
≤2αδ2+ ∥e ∥2.
N k 2 k
E[V ]= (cid:88) ⟨E(cid:2)E(cid:2) g(θ∗,o )|o (cid:3)(cid:3) ,
22
i,j=1
i,ti,k i,ti,k−τ
(29)
Taking the expectation and using the bound on E[T 2],
i̸=j σ2
E(cid:2)E(cid:2) g(θ∗,o j,tj,k)|o j,tj,k−τ(cid:3)(cid:3) ⟩. E[T 32]≤α(14d k+32
N
+8σ2α2q). (37)Now we bound K. Define g¯ ≜ 1 (cid:80)N g¯(θ ). Note that, using the Lipschitz property (see (9)), we get the
N,k N i=1 k−τi,k
Adding and subtracting g¯ , we write exact same bound for ∆ . Now note that
N,k 3,i
K =K 1+K 2, with
K =
1 (cid:88)N
(∆ +∆ )+∆¯, with
K =⟨θ −θ∗,g¯(θ )−g¯ ⟩, (38) 2 N 1,i 3,i
1 k k N,k i=1 (47)
K 2 =⟨θ k−θ∗,g¯ N,k−v k⟩.
∆¯ =
1 (cid:88)N
∆ .
Now note that, taking the sum of the second term outside of N 2,i
i=1
the inner product in K ,
1 To bound ∆¯, we want to use the geometric mixing property
1 (cid:88)N of the Markov chain, that follows from Assumption 2.
K 1 = N K 1,i, with (39) However,duetothecorrelationsexistingbetweentheiterates
i=1 θ , this needs to be done with special care. Defining
K 1,i =⟨θ k−θ∗,g¯(θ k)−g¯(θ ti,k)⟩. k′ti ≜,k k−τ max−τ, we start by adding and subtracting θ k′
from the first term in the inner product of ∆ , getting
We now bound E[K ]. Using (13) and (9), 2,i
1,i
∆¯ =∆¯ +∆¯ , with
1 1 2
K 1,i ≤ατ maxδ k2+ 4ατ max∥g¯(θ k)−g¯(θ ti,k)∥2 (40) ∆¯ =⟨θ −θ , 1 (cid:88)N g¯(θ )−g(θ ,o )⟩,
1 1 k k′ N ti,k−τ ti,k−τ i,ti,k
≤ατ maxδ k2+
ατ
δ k2 ,τi,k. i=1
max N
Note that, using Lemma 4, which requires k ≥τ max+τ i,k, ∆¯ 2 =⟨θ k′ −θ∗, N1 (cid:88) g¯(θ ti,k−τ)−g(θ ti,k−τ,o i,ti,k)⟩.
which holds for k ≥2τ , i=1
max (48)
(cid:104) (cid:105) (cid:18) σ2 (cid:19) Note that we can write, using (13) and (14),
E δ2 ≤8α2τ2 d +4 +σ2α2q . (41)
k,τi,k max k N 1
∆¯ ≤ δ2
1 2α(τ +τ) k,τ+τmax
Taking the expectation and applying (41), we get max
N
(cid:18) σ2 (cid:19) + α(τ max+τ) ∥(cid:88) g¯(θ )−g(θ ,o )∥2
E[K ]≤ατ 9d +32 +8σ2α2q , (42) 2N2 ti,k−τ ti,k−τ i,ti,k
1,i max k N i=1
(cid:124) (cid:123)(cid:122) (cid:125)
and note that E[K 1] is bounded by the same quantity. We G (49)
now proceed to bound E[K 2]. For k ≥τ +2τ max, we can apply Lemma 4 and get
1 (cid:88)N E(cid:2) δ2 (cid:3) ≤8α2(τ +τ )2(d
+4σ2
+σ2α2q).
K 2 = N K 2,i, with k,τ+τmax max k N (50)
i=1
K =⟨θ −θ∗,g¯(θ )−g(θ ,o )⟩ Now note that
2,i k ti,k ti,k i,ti,k
N N
=∆ +∆ +∆ , where (cid:88) (cid:88)
1,i 2,i 3,i G≤2∥ g¯(θ )∥2+2∥ g(θ ,o )∥2,
ti,k−τ ti,k−τ i,ti,k
∆ 1,i =⟨θ k−θ∗,g¯(θ ti,k)−g¯(θ ti,k−τ)⟩, (cid:124)i=1
(cid:123)(cid:122) (cid:125)
(cid:124)i=1
(cid:123)(cid:122) (cid:125)
∆
2,i
=⟨θ k−θ∗,g¯(θ ti,k−τ)−g(θ ti,k−τ,o i,ti,k)⟩ G1 G2
(51)
∆ =⟨θ −θ∗,g(θ ,o )−g(θ ,o )⟩. (cid:104) (cid:105)
3,i k ti,k−τ i,ti,k ti,k i,ti,k
(43)
with E[G 1]≤4N2E δ t2
i,k−τ
≤4N2d k. Also note that
Note that, using Cauchy-Schwarz inequality and (9),
G ≤2G +2G , with
2 21 22
∆ ≤δ ∥g¯(θ )−g¯(θ )∥ N
1,i k ti,k ti,k−τ
G
=∥(cid:88)
g(θ ,o )−g(θ∗,o )∥2,
≤2 (cid:32)δ kδ
ti,k,τ
(cid:33) (44)
21
i=1
ti,k−τ i,ti,k i,k−τi,k
(52)
( ≤13) ατδ k2+ δ t2 αi,k τ,τ . G
22
=∥(cid:88)N g(θ∗,o i,ti,k)∥2.
i=1
For k ≥τ +τ max, we can apply Lemma 4 and get Note that E[G 21] and E[G 22] can be bounded in the same
(cid:104) (cid:105) (cid:18) σ2 (cid:19) way as E[V 1] and E[V 2] in the proof of Lemma 3. We get
E δ t2 i,k,τ ≤α2τ2 8d k+32 N +8σ2α2q . (45) E[G ]≤8(cid:0) N2d +4Nσ2+N2σ2α2q(cid:1) . (53)
2 k
Thus, taking the expectation, we can get Hence, we get
E[∆ 1,i]≤9ατd k+32ατσ N2 +8ατσ2α2q. (46) E(cid:2) ∆¯ 1(cid:3) ≤16α(τ max+τ)(cid:18) d k+3σ N2 +σ2α2q(cid:19) . (54)Defining ∆¯ ≜⟨θ −θ∗,g¯(θ )−g(θ ,o )⟩, We can now write the above bound in the following way:
2,i k′ ti,k−τ ti,k−τ i,ti,k
V ≤pV +q max V +β, with
k+1 k j
E(cid:2) ∆¯ 2(cid:3) = N1 (cid:88)N E(cid:2) ∆¯ 2,i(cid:3) . (55) V
k
=E(cid:2) δ k2(cid:3) , k−τ′≤j≤k
i=1
p=(1−2α(1−γ)ω),
(62)
Now, defining Θ¯
i,k
≜{θ k′,θ ti,k−τ,o ti,k−τ}, q =112α2(τ max+τ),
(cid:18) σ2(cid:19)
β =352α2(τ +τ) +15α2σ2αq.
E(cid:2) ∆¯ (cid:3) =E[⟨θ −θ∗,g¯(θ )−g(θ ,o )⟩] max N
2,i k′ ti,k−τ ti,k−τ i,ti,k
=E[⟨θ
k′
−θ∗,
Imposing α≤ (1−γ)ω , we get p+q ≤1−α(1−γ)ω,
g¯(θ ti,k−τ)−E(cid:2) g(θ ti,k−τ,o i,ti,k)|Θ¯ i,k(cid:3) ⟩] and we can app1 l1 y2( Lτ+ emτm max a) 2 getting
≤E[δ
k′ E(cid:2) δ2(cid:3) ≤ρT−τ′E(cid:2) δ2 (cid:3) +ϵ, (63)
·∥g¯(θ )−E(cid:2) g(θ ,o )|Θ¯ (cid:3) ∥]. T τ′
(cid:124)
ti,k−τ (cid:123)(cid:122)ti,k−τ i,ti,k i,k
(cid:125) with ρ = (1−α(1−γ)ω)1+1 τ′ and ϵ = β . Note that
η¯k,i 1−p−q
(56) we can easily show that ρ−τ′ ≤ 2 for α ≤ 1 .
Now, recall Θ i,k = {θ ti,k−τ,o i,ti,k−τ}. Note that, for the Furthermore, we can show that E(cid:2) δ τ2 ′(cid:3) ≤ 3σ21 ,6 a(τ sm wax e+τ d) o
memoryless property of the Markov chain {o }, we have
i,k next. Note that, using (11),
E(cid:2) g(θ ,o )|Θ¯ (cid:3) =E(cid:2) g(θ ,o )|Θ (cid:3) . 1 (cid:88)N
ti,k−τ i,ti,k i,k ti,k−τ i,ti,k i,k
(57)
∥v k∥2 ≤
N
8(2δ t2
i,k
+3σ2)
(64)
i=1
Indeed, by inspecting the update rule (2) we see that the
≤16 max δ2 +24σ2.
parameter θ k′ is a function of agent i observations only up i=1,...,N ti,k
to time step k′−1, so up to observation o . Hence, all
i,k′−1 Using this bound, note that, for any k ≥0
the statistical information contained in θ has no influence
k′
ontherandomvariableo
i,ti,k
onceweconditionono i,ti,k−τ, δ k2
+1
=δ k2+2α⟨θ t−θ∗,v k⟩+α2∥v k∥2
because t i,k−τ =k−τ i,k−τ >k′−1=k−τ max−τ−1. ( ≤13)
δ2+αδ2+α∥v ∥2+α2∥v ∥2
Therefore, k k k k (65)
≤(1+α)δ2+2α∥v ∥2
k k
η¯ k,i =∥g¯(θ
ti,k−τ)−E(cid:2)
g(θ ti,k−τ,o i,ti,k)|Θ
i,k(cid:3)
∥
(58)
≤(1+α)δ k2+32α i=m 1,a ..x .,Nδ t2
i,k
+48ασ2,
( ≤12) αq(cid:0)
∥θ
∥+σ(cid:1)
.
ti,k−τ where recall that t ≤k. Now define p¯≜1+α, q¯≜32α
i,k
and ν ≜p¯+q¯and β¯≜48ασ2. We now prove by induction
Hence, we get
that, for all k ≥0,
E(cid:2) ∆¯ 2,i(cid:3) ≤E(cid:2) δ k′αq(cid:0) ∥θ ti,k−τ∥+σ(cid:1)(cid:3) δ k2 ≤νkδ 02+ϵ k, (66)
≤E(cid:2)
δ
αq(cid:0)
δ
+2σ(cid:1)(cid:3)
k′ ti,k−τ where ϵ =νϵ +β¯ for k ≥1 and ϵ =0. The base case
(cid:104)α (cid:16) (cid:17)(cid:105) k k−1 0
≤E δ2 +α2q−1 δ2 +4σ2 (59) is trivially satisfied, because δ2 ≤ δ2. As an inductive step,
2 k′ ti,k−τ 0 0
suppose that (66) is true for 0≤s≤k, for some k ≥0, so
≤αd +4ασ2α2(q−1)
k
≤αd k+4ασ2αq δ s2 ≤νsδ 02+ϵ s, 0≤s≤k. (67)
Now,wecheckthepropertyfork+1,using(65),andnoting
whereweusedthefactthatα≤ 1 andthefactthat2(q−1)≥ that ϵ is an increasing sequence:
2 k
q for q ≥2. Putting all the above bounds together, and also
the fact that α≤ 16(τm1 ax+τ) we get δ k2 +1 ≤p¯δ k2+q¯ i=m 1,a ..x .,Nδ t2 i,k +β¯,
(cid:18) (cid:19)
E[T ]≤α(τ
+τ)(cid:18)
44d
+144σ2(cid:19)
+7ασ2αq. (60)
≤p¯(νkδ 02+ϵ k)+q¯ i=m 1,a ..x .,Nνti,kδ 02+ϵ ti,k +β¯
3 max k N ≤p¯(νkδ2+ϵ )+q¯(νkδ2+ϵ )+β¯
0 k 0 k
≤(p¯+q¯)νkδ2+(p¯+q¯)ϵ +β¯
Concluding the proof. Using the bounds on E[T ],E[T ] 0 k
and E[T 3], we obtain, for k ≥τ′ ≜2τ max+τ, 1 2 =νk+1δ 02+ϵ k+1.
(68)
E(cid:2) δ k2 +1(cid:3) ≤(1−2α(1−γ)ω)E(cid:2) δ k2(cid:3) +15α2σ2αq F thr ao tm ϵw =hic β¯h (cid:80)w ke −c 1a νn j,co ann dclu thd ae tt fh oe
r
p 0r ≤oof ko ≤f τ(6 ′,6). Now, note
(cid:18) σ2(cid:19) (61) k j=0
+α2(τ +τ max) 112d k+352 N . νk ≤ντ′ ≤(1+33α)τ′ ≤e33ατ′ ≤e0.25 ≤2, (69)REFERENCES
[1] S.Khodadadian,P.Sharma,G.Joshi,andS.T.Maguluri,“Federated
reinforcement learning: Linear speedup under markovian sampling,”
inICML. PMLR,2022,pp.10997–11057.
[2] H.Wang,A.Mitra,H.Hassani,G.J.Pappas,andJ.Anderson,“Fed-
eratedtemporaldifferencelearningwithlinearfunctionapproximation
underenvironmentalheterogeneity,”arXiv:2302.02212,2023.
[3] J.Qi,Q.Zhou,L.Lei,andK.Zheng,“Federatedreinforcementlearn-
ing: techniques, applications, and open challenges,” arXiv preprint
arXiv:2108.11887,2021.
[4] M.Chen,Z.Yang,W.Saad,C.Yin,H.V.Poor,andS.Cui,“Ajoint
learning and communications framework for federated learning over
wirelessnetworks,”IEEETransactionsonWirelessCommunications,
vol.20,no.1,pp.269–283,2020.
[5] A.Koloskova,S.U.Stich,andM.Jaggi,“Sharperconvergenceguar-
Fig. 2. Comparison between vanilla MATD and AsyncMATD in single- antees for asynchronous sgd for distributed and federated learning,”
agent (N =1) and multi-agent (N =20) settings. For AsyncMATD, we Advances in Neural Information Processing Systems, vol. 35, pp.
setτmax=100. 17202–17215,2022.
[6] S. Dutta, G. Joshi, S. Ghosh, P. Dube, and P. Nagpurkar, “Slow
and stale gradients can win the race: Error-runtime trade-offs in
imposing α≤ 1 . Hence, for 0≤k ≤τ′, distributed sgd,” in International conference on artificial intelligence
132τ′ andstatistics. PMLR,2018,pp.803–812.
[7] N. Dal Fabbro, A. Mitra, and G. J. Pappas, “Federated TD learning
τ′−1 over finite-rate erasure channels: Linear speedup under markovian
δ2 ≤νkδ2+ϵ ≤2δ2+β¯(cid:88) νj ≤2δ2+2β¯τ′ sampling,”IEEEControlSystemsLetters,2023.
k 0 k 0 0 (70) [8] N. Dal Fabbro, A. Mitra, R. Heath, L. Schenato, and G. J. Pap-
j=0
pas, “Over-the-air federated TD learning,” in MLSys23 Workshop on
=2δ2+2(48ασ2)τ′ ≤2δ2+σ2 ≤3σ2, Resource-ConstrainedLearninginWirelessNetworks,2023.
0 0
[9] J.Nguyen,K.Malik,H.Zhan,A.Yousefpour,M.Rabbat,M.Malek,
where we used the fact that α ≤ 1 and that δ2 ≤ σ2. and D. Huba, “Federated learning with buffered asynchronous ag-
100τ′ 0 gregation,” in International Conference on Artificial Intelligence and
We can therefore conclude, writing the bound in (63) as
Statistics. PMLR,2022,pp.3581–3607.
(cid:18) (cid:19) [10] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
E(cid:2) δ2(cid:3) ≤exp −α(1−γ)ωT 6σ2 D.Silver,andK.Kavukcuoglu,“Asynchronousmethodsfordeeprein-
T 2(τ +τ ) forcementlearning,”inInternationalconferenceonmachinelearning.
max
(71) PMLR,2016,pp.1928–1937.
α(τ +τ )σ2 α3σ2
+352 max +15 . [11] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon,
(1−γ)ωN (1−γ)ω A.DeMaria,V.Panneershelvam,M.Suleyman,C.Beattie,S.Petersen
etal.,“Massivelyparallelmethodsfordeepreinforcementlearning,”
V. SIMULATIONS arXivpreprintarXiv:1507.04296,2015.
[12] H.Shen,K.Zhang,M.Hong,andT.Chen,“Towardsunderstanding
In this section, we provide simulation results to validate asynchronousadvantageactor-critic:Convergenceandlinearspeedup,”
IEEE Transactions on Signal Processing, vol. 71, pp. 2579–2594,
ourtheory.WeconsideranMDPwith|S|=100statesanda
2023.
feature space spanned by d=10 orthonormal basis vectors; [13] J. Bhandari, D. Russo, and R. Singal, “A finite time analysis of
wesetthediscountfactortoγ =0.5andthestepsizetoα= temporal difference learning with linear function approximation,” in
Conferenceonlearningtheory. PMLR,2018,pp.1691–1692.
0.05. To simulate the asynchronous delays in the TD update
[14] R.SrikantandL.Ying,“Finite-timeerrorboundsforlinearstochastic
directions,wegeneraterandomdelaysateachiterationk for approximationandTDlearning,”inConferenceonLearningTheory.
each agent i, by generating a uniform random variable τ PMLR,2019,pp.2803–2830.
i,k [15] R. S. Sutton, “Learning to predict by the methods of temporal
in the range [1,τ ]. We set τ = 100. In the multi-
max max differences,”Machinelearning,vol.3,no.1,pp.9–44,1988.
agent setting, we set the number of agents to N = 20. For [16] J.N.TsitsiklisandB.VanRoy,“Ananalysisoftemporal-difference
each configuration, we plot the average of 20 experiments. learning with function approximation,” in IEEE Transactions on
AutomaticControl,1997.
It is apparent how the linear speedup property also holds for
[17] D.A.LevinandY.Peres,Markovchainsandmixingtimes. American
AsyncMATD.Furthermore,wecanseehowthegapbetween MathematicalSoc.,2017,vol.107.
the delayed and non-delayed settings decreases in the multi- [18] Z.Chen,S.Zhang,T.T.Doan,S.T.Maguluri,andJ.-P.Clarke,“Per-
formance of q-learning with linear function approximation: Stability
agent case, compared to the single-agent configuration, by
andfinite-timeanalysis,”arXivpreprintarXiv:1905.11425,p.4,2019.
more than one order of magnitude. [19] H. R. Feyzmahdavian, A. Aytekin, and M. Johansson, “A delayed
proximalgradientmethodwithlinearconvergencerate,”in2014IEEE
international workshop on machine learning for signal processing
VI. CONCLUSIONANDFUTUREWORK
(MLSP). IEEE,2014,pp.1–6.
[20] A. Adibi, N. Dal Fabbro, L. Schenato, S. Kulkarni, H. V. Poor,
We presented and analysed AsyncMATD, providing the
G. J. Pappas, H. Hassani, and A. Mitra, “Stochastic approximation
first finite-time analysis for asynchronous multi-agent TD with delayed updates: Finite-time rates under markovian sampling,”
learningthatjointlyestablishesalinearconvergencespeedup in International Conference on Artificial Intelligence and Statistics.
PMLR,2024,pp.2746–2754.
with the number of agents. Future work includes more
[21] N.DalFabbro,A.Adibi,H.V.Poor,S.R.Kulkarni,A.Mitra,andG.J.
complex delay models, for example considering random and Pappas,“Dasa:Delay-adaptivemulti-agentstochasticapproximation,”
potentially unbounded delays, and the use of delay-adaptive arXivpreprintarXiv:2403.17247,2024.
algorithms like the ones proposed for the single-agent and
multi-agent settings in [20], [21].