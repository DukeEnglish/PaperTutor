JournalofMachineLearningResearch? (2024)1-23 Submitted?;Revised?;Published?
Two Completely Parameter-Free Alternating Gradient
Projection Algorithms for Nonconvex-(strongly) Concave
Minimax Problems∗
Junnan Yang yangjunnan@shu.edu.cn
Department of Mathematics
Shanghai University
Shanghai 200444,People’s Republic of China
Huiling Zhang zhanghl1209@shu.edu.cn
Department of Mathematics
Shanghai University
Shanghai 200444,People’s Republic of China
Zi Xu† xuzi@shu.edu.cn
Department of Mathematics
Shanghai University
Shanghai 200444, People’s Republic of China
and
Newtouch Center for Mathematics of Shanghai University
Shanghai University
Shanghai 200444, People’s Republic of China
Editor: My editor
Abstract
Due to their importance in various emerging applications, efficient algorithms for solving
minimax problems have recently received increasing attention. However, many existing
algorithms require prior knowledge of the problem parameters in order to achieve optimal
iteration complexity. In this paper, we propose a completely parameter-free alternating
gradientprojection(PF-AGP)algorithmtosolvethesmoothnonconvex-(strongly)concave
minimaxproblemsusingabacktrackingstrategy,whichdoesnotrequirepriorknowledgeof
parameters such as the Lipschtiz constant L or the strongly concave constant µ. The PF-
AGPalgorithmutilizesaparameter-freegradientprojectionsteptoalternatelyupdatethe
outerandinnervariablesineachiteration. Weshowthatthetotalnumberofgradientcalls
of the PF-AGP algorithm to obtain an ε-stationary point for nonconvex-strongly concave
minimax problems is upper bounded by
O(cid:0) Lκ3ε−2(cid:1)
where κ is the condition number,
while the total number of gradient calls to obtain an ε-stationary point for nonconvex-
concave minimax problems is upper bounded by
O(cid:0) L4ε−4(cid:1)
. As far as we know, this
is the first completely parameter-free algorithm for solving nonconvex-strongly concave
minimax problems, and it is also the completely parameter-free algorithm which achieves
thebestiterationcomplexityinsingleloopmethodforsolvingnonconvex-concaveminimax
problems. Numerical results validate the efficiency of the proposed PF-AGP algorithm.
∗. This work is supported by National Natural Science Foundation of China under the grants 12071279.
†. Corresponding author.
©2024JunnanYang,HuilingZhang,andZiXu.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/. Attributionrequirementsareprovided
athttp://jmlr.org/papers/v?/?.html.
4202
luJ
13
]CO.htam[
1v27312.7042:viXraTwo PF-AGP Algorithms for Minimax Problems
Keywords: minimax optimization problem; backtracking strategies; parameter-free al-
ternating gradient projection algorithm; iteration complexity; machine learning
1 Introduction
We consider the following minimax optimization problem:
minmaxf(x,y), (P)
x∈X y∈Y
whereX ⊆ Rn andY ⊆ Rm arenonemptyclosedandboundedconvexsets,andf : X×Y →
R is a smooth function. The minimax problem is a critical class of optimization problems,
known for its extensive research history and broad applications in areas such as machine
learning,signalprocessing,andvariousotherfieldsinrecentyears. Manypracticalproblems
can be formulated as in (P), including machine learning, such as generative adversarial
networks (GANs) (Arjovsky et al., 2017; Goodfellow et al., 2014; Sanjabi et al., 2018),
reinforcement learning (Dai et al., 2018), power control and transceiver design in signal
processing (Lu et al., 2020), distributed nonconvex optimization (Giannakis et al., 2017;
Giordano et al., 2018; Mateos et al., 2010), robust optimization (Ben and Tal, 2009; Gao
andKleywegt,2023), statisticallearning(Giordanoetal.,2018;ShafieezadehAbadehetal.,
2015) and several other applications.
There are many research results on convex-concave minimax problems, i.e., f(x,y) is
convex with respect to x and concave with respect to y. Nemirovski (Nemirovski, 2004)
introduced the mirror-prox method, and Nestrov (Nesterov, 2007) proposed a dual extrap-
olation algorithm, both of which achieve an optimal iteration complexity of O(ε−1) for
solving convex-concave minimax problems. For more details, see (Vandenberghe and Boyd,
2004; Chen et al., 2014, 2017; Dang and Lan, 2014; Monteiro and Svaiter, 2010; Nedi´c and
Ozdaglar, 2009; Yang et al., 2020) and references therein.
Recently, various algorithms have been proposed for nonconvex-(strongly) concave min-
imax problems. For nonconvex-strongly concave minimax problems, several recent works
(Jin et al., 2020; Lin et al., 2020a; Rafique et al., 2022; Lu et al., 2020) have proposed
various algorithms that achieve a gradient complexity of O˜(κ2ε−2) for stationary points
y
of Φ(·) = max f(·,y) (when X = Rn), or for stationary points of f, where κ is the
y∈Y y
condition number for f(x,·). (Lin et al., 2020b) proposed an accelerated algorithm that
√
improves the gradient complexity bound to O˜( κ ε−2). Furthermore, Zhang et al. (Zhang
y
et al., 2021) proposed a generic acceleration framework which can improve the iteration
complexity to
O(cid:0)√
κ
ε−2(cid:1)
.
y
Forgeneralnonconvex-concaveminimaxproblems,therearetwotypesofalgorithms,i.e.,
multi-loop algorithms and single-loop algorithms. Various multi-loop algorithms have been
proposed in (Kong and Monteiro, 2021; Lin et al., 2020b; Nouiehed et al., 2019; Ostrovskii
et al., 2021; Rafique et al., 2022; Thekumparampil et al., 2019). The best known iteration
complexity of multi-loop algorithms for solving nonconvex-concave minimax problems is
O˜(ε−2.5), which was achieved by (Lin et al., 2020b). For solving nonconvex-linear minimax
problems, Pan et al. (Pan et al., 2021) proposed a new alternating gradient projection
algorithm with the iteration complexity of O(ε−3). Various single-loop algorithms have
also been proposed, e.g., the gradient descent-ascent (GDA) algorithm (Lin et al., 2020a),
the hybrid block successive approximation (HiBSA) algorithm (Lu et al., 2020), the unified
2Two PF-AGP Algorithms for Minimax Problems
single-loop alternating gradient projection (AGP) algorithm (Xu et al., 2023), and the
smoothed GDA algorithm (Zhang et al., 2020). Both the AGP algorithm and the smoothed
GDAalgorithmachievethebestknowniterationcomplexity,i.e.,O(ε−4),amongsingle-loop
algorithms for solving nonconvex-concave minimax problems.
It should be noted that for most of the existing results mentioned above, achieving
optimalcomplexityofthealgorithmrequirestheassumptionofknowingpreciseinformation
about some parameters of the problem, such as the Lipschitz constant, or the strongly
concave constant µ, etc. Accurately estimating these parameters is often challenging, and
conservative estimates can significantly impact algorithm performance (Lan et al., 2023).
Therefore, designing parameter-free algorithms with complexity guarantees without relying
on inputs of these parameters has attracted considerable attention recently.
For nonconvex-strongly concave minimax problems, there are two main parameter-free
algorithms: one based on AdaGrad stepsizes and the other based on backtracking. In the
framework of AdaGrad stepsizes, Yang et al. (Yang et al., 2022) proposed NeAda, which
can achieve convergence rates of O˜(ε−2) and O˜(ε−4) in deterministic and stochastic set-
tings, respectively. Li et al. (Li et al., 2022) proposed TiAda, achieving convergence rates
of O(ε−2) and O(ε−4) in deterministic and stochastic settings, respectively. In the frame-
work of backtracking, Xu et al. (Xu et al., 2024) proposed a stochastic gradient descent
ascent(GDA)methodwithbacktracking(SGDA-B),achievinganupperboundonthetotal
number of gradient calls of O(Lκ2log(κ)ε−2) with O(Lκ2ε−2) gradient calls required per
backtracking step. Meanwhile, Zhang et al. (Zhang et al., 2024) proposed a proximal alter-
nating gradient descent ascent method (AGDA+), achieving an upper bound on the total
number of gradient calls of O(Lκ4ε−2) with O(1) gradient calls required per backtracking
step. Both methods do not require knowledge of the specific value of Lipschtiz constant
L, but still require information about the strongly concave constant µ in the function. In
the framework of backtracking, Xu et al. (Xu et al., 2024) proposed a stochastic GDA
method with backtracking (SGDA-B) for nonconvex-concave minimax problems, achieving
an upper bound on the total number of gradient calls of O˜(L3ε−4) with O(L3ε−4) gradient
calls required per backtracking step.
1.1 Contributions
Inthispaper,basedontheframeworkofbacktracking,weproposetwocompletelyparameter-
free alternating gradient projection algorithms, i.e., the PF-AGP-NSC algorithm and the
PF-AGP-NCalgorithm,forsolvingnonconvex-(strongly)concaveminimaxproblemsrespec-
tively, which does not require prior knowledge of parameters such as the Lipschtiz constant
L or the strongly concave constant µ.
Moreover, we show that the total number of gradient calls of the PF-AGP-NSC algo-
rithm to obtain an ε-stationary point for nonconvex-strongly concave minimax problems
is upper bounded by
O(cid:0) Lκ3ε−2(cid:1)
where κ is the condition number, while the total num-
ber of gradient calls of the PF-AGP-NC algorithm to obtain an ε-stationary point for
nonconvex-concave minimax problems is upper bounded by
O(cid:0) L4ε−4(cid:1)
. As far as we know,
the PF-AGP-NSC algorithm is the first completely parameter-free algorithm for solving
nonconvex-strongly concave minimax problems, and the PF-AGP-NC algorithm is the first
3Two PF-AGP Algorithms for Minimax Problems
completely parameter-free algorithm which achieves the best iteration complexity in single
loop algorithms for solving nonconvex-concave minimax problems.
1.2 Organization
InSect. 2,weproposeaparameter-freealternatinggradientprojection(PF-AGP)algorithm
fornonconvex-(strongly)concaveminimaxproblems,andwethenanalyzethecorresponding
gradient complexity for two different settings. We report some numerical results in Sect. 3
and make some concluding remarks in the last section.
Notation. Forvectors,weuse∥·∥todenotethel -norm. Forafunctionf(x,y) : Rn×Rm →
2
R, we use ∇ f(x,y) (or ∇ f(x,y)) to denote the partial gradient of f with respect to the
x y
first variable (or the second variable) at point (x,y). Let P and P denote projections
X Y
onto the sets X and Y. Finally, we use the notation O(·) to hide only absolute constants
which do not depend on any problem parameter, and O˜(·) notation to hide only absolute
constants and log factors. A continuously differentiable function f(·) is called θ-strongly
convex if there exists a constant θ > 0 such that for any x,y ∈ X,
θ
f(y) ≥ f(x)+⟨∇f(x),y−x⟩+ ∥y−x∥2. (1)
2
If −f satisfies (1), f(·) is called θ-strongly concave. Denote the sign function as sgn(x), i.e.,
(cid:40)
1 if x > 0,
sgn(x) = (2)
−1 if x ≤ 0.
2 Two completely parameter-free alternating gradient projection
algorithms
In this section, we propose two completely parameter-free alternating gradient projec-
tion (PF-AGP) algorithms for solving the nonconvex-strongly concave and the nonconvex-
concaveminimaxproblems(P)respectively. Bothalgorithmsdonotrequirepriorknowledge
ofLipschitzconstants,andthealgorithmundernonconvex-strongconcavesettingsalsodoes
not require prior knowledge of strong concave constants. The two proposed algorithms are
based on the AGP algorithm (Xu et al., 2023), which uses the gradient of a regularized
version of the original function, i.e.,
c
f (x,y) = f(x,y)− k ∥y ∥2,
k k
2
where c ≥ 0 is a regularization parameters. At the k-th iteration, AGP algorithm consists
k
of the following two gradient projection steps for updating both x and y:
(cid:16) (cid:17)
x = P x − 1 ∇ f(x ,y ) ,
k+1 X k β x k k
k
(cid:18) (cid:19)
1
y = P y + 1 ∇ f(x ,y )− c y ,
k+1 Y k γ k y k k γ k k k
where P and P is the projection operator onto X and Y, respectively, and β > 0, γ > 0
X Y k k
are stepsize parameters.
4Two PF-AGP Algorithms for Minimax Problems
Although the AGP algorithm achieves the optimal iteration complexity among single
loop algorithms for nonconvex-(strongly) concave minimax problems, it requires the knowl-
edgeoftheLipschtizconstantLandthestronglyconcavecoefficientµ,oratleastknowledge
oftheLipschtizconstantLtocalculatethestepsizesβ andγ underthenonconvex-strongly
k k
concave setting and the nonconvex-concave setting, respectively, which limits its applicabil-
ity. ToeliminatetheneedforthepriorknowledgeoftheLipschtizconstantLorthestrongly
concave coefficient µ, we propose two parameter-free AGP algorithms by employing back-
tracking strategies for solving the nonconvex-strongly concave and the nonconvex-concave
minimax problems, respectively. Similar backtracking strategies have been used in (Lan
et al., 2023; Liu and Luo, 2022; Nesterov and Polyak, 2006; Nesterov, 2015).
2.1 Nonconvex-strongly Concave Setting
Wefirstproposeacompletelyparameter-freeAGPalgorithmforsolvingnonconvex-strongly
concave minimax problems. More specifically, in order to estimate the Lipschitz constants
for∇ f(·,y),∇ f(·,y),∇ f(x,·)denotedasL ,L andL ,respectively,andthestrongly
x y y 11 12 22
concave constant µ, at each iteration of the proposed algorithm, we aim to find a tuple
(lk,i,lk,i,lk,i,µ ) by backtracking such that the following conditions for (x ,y ) are sat-
11 12 22 k,i k,i k,i
isfied:
lk,i
Ck,i = f(x ,y )−f(x ,y )−⟨∇ f(x ,y ),x −x ⟩− 11 ∥x −x ∥2 ≤ 0, (C1)
1 k,i k k k x k k k,i k 2 k,i k
Ck,i = ∥∇ f(x ,y )−∇ f(x ,y )∥−lk,i∥x −x ∥ ≤ 0, (C2)
2 y k,i k y k k 12 k,i k
Ck,i = ∥∇ f(x ,y )−∇ f(x ,y )∥−lk,i∥y −y ∥ ≤ 0, (C3)
3 y k,i k,i y k,i k 22 k,i k
Ck,i = ⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩+µ ∥y −y ∥2 ≤ 0. (C4)
4 y k,i k,i y k,i k k,i k k,i k,i k
Otherwise, we enlarge lk,i,lk,i,lk,i by a factor of 2 or reduce µ by half according to the
11 12 22 k,i
sign of Ck,i, Ck,i, Ck,i and Ck,i, respectively. We then update β and γ by the new
1 2 3 4 k k
estimates, i.e., lk,i,lk,i,lk,i,µ . The proposed algorithm for solving nonconvex-strongly
11 12 22 k,i
concave problems, denoted as PF-AGP-NSC, is formally presented in Algorithm 1.
Itisworthnotingthattheparameter-freeNeAda(Yangetal.,2022)andTiAda(Lietal.,
2022) algorithms with adaptive step size are only used to solve the minimax problems when
x is unconstrained, while the proposed PF-AGP-NSC algorithm can solve the constrained
case. Compared with the SGDA-B (Xu et al., 2023) algorithm and AGDA+ (Zhang et al.,
2024) algorithm using the backtracking framework under the nonconvex-strongly concave
setting, AGDA+andSGDA-Bstillrequireknowledgeofthestronglyconcaveconstant mu,
while the proposed PF-AGP-NSC algorithm is a completely parameter-free algorithm that
does not require any prior knowledge of the parameters.
2.1.1 Complexity analysis
Inthissubsection,weanalyzetheiterationcomplexityofAlgorithm1forsolvingnonconvex-
strongly concave minimax optimization problems, i.e., f(x,y) is nonconvex with respect to
x for any fixed y ∈ Y, and µ-strongly concave with respect to y for any given x ∈ X.
BeforeweprovetheiterationcomplexityofthePF-AGP-NSCalgorithmforsolving(P),
we need to make the following assumption about the smoothness of f(x,y).
5Two PF-AGP Algorithms for Minimax Problems
Algorithm 1 A parameter-free alternating gradient projection (PF-AGP-NSC) algorithm
for nonconvex-strongly concave minimax problems
Step 1: Input x ,y , β , γ , l1 , l1 , l1 , µ ; Set k = 1.
1 1 1 1 11 12 22 1
Step 2: Updata x and y :
k k
(a): Set i = 1, lk,i = lk , lk,i = lk , lk,i = lk , µ = µ , β = β , γ = γ .
11 11 12 12 22 22 k,i k k,i k k,i k
(b): Update x and y :
k,i k,i
(cid:18) (cid:19)
1
x = P x − ∇ f(x ,y ) , (3)
k,i X k x k k
β
k,i
(cid:18) (cid:19)
1
y = P y + ∇ f(x ,y ) . (4)
k,i Y k y k,i k
γ
k,i
(c): Compute Ck,i, Ck,i, Ck,i, Ck,i as in (C1), (C2), (C3), (C4), respectively;
1 2 3 4
(d): Update lk,i, lk,i, lk,i, µ :
11 12 22 k,i
sgn(Ck,i)+3 sgn(Ck,i)+3
lk,i+1 = 1 lk,i, lk,i+1 = 2 lk,i, (5)
11 2 11 12 2 12
sgn(Ck,i)+3 2
lk,i+1 = 3 lk,i, µ = µ . (6)
22 2 22 k,i+1 sgn(Ck,i)+3 k,i
4
(e): If Ck,i ≤ 0, Ck,i ≤ 0, Ck,i ≤ 0 and Ck,i ≤ 0, then
1 2 3 4
x = x , y = y , lk = lk,i+1, lk = lk,i+1, lk = lk,i+1, µ = µ ,
k+1 k,i k+1 k,i 11 11 12 12 22 22 k k,i+1
β = β , γ = γ , go to Step 3;
k k,i k k,i
Otherwise, i = i+1,
lk,iµ 32(lk,i)2(lk−1,i)2 8(lk,i)2
β = lk,i+ 12 k−1,i + 12 22 , γ = 22 ,
k,i 11 8(l 2k 2−1,i)2 µ k,iµ2 k−1,i k,i µ k,i
go to Step 2(b).
Step 3: If some stationary condition is satisfied, stop; Otherwise, set k = k+1, go to
Step 2.
Assumption 1 f(x,y) has Lipschitz continuous gradients, i.e., there exist positive scalars
L ,L ,L such that for any x,x˜ ∈ X,y,y˜∈ Y,
11 12 22
∥∇ f(x,y)−∇ f(x˜,y)∥ ≤ L ∥x−x˜∥,
x x 11
∥∇ f(x,y)−∇ f(x˜,y)∥ ≤ L ∥x−x˜∥,
y y 12
∥∇ f(x,y˜)−∇ f(x,y)∥ ≤ L ∥y˜−y∥.
y y 22
We denote L = max{L ,L ,L }. To analyze the convergence of Algorithm 1, we define
11 12 22
the stationarity gap as the termination criterion as follows.
6Two PF-AGP Algorithms for Minimax Problems
Definition 1 At each iteration of Algorithm 1, the stationarity gap for problem (P) with
respect to f(x,y) is defined as:
 (cid:16) (cid:16) (cid:17)(cid:17)
β x −P x − 1 ∇ f(x ,y )
k k X k β x k k
∇G(x k,y k) =  (cid:16) (cid:16) k (cid:17)(cid:17). (7)
γ y −P y + 1 ∇ f(x ,y )
k k Y k γ y k k
k
We denote ∇G = ∇G(x ,y ), (∇G ) = β (x −P (x − 1 ∇ f(x ,y ))), and (∇G ) =
k k k k x k k X k β x k k k y
k
γ (y −P (y + 1 ∇ f(x ,y ))).
k k Y k γ y k k
k
Definition 2 At each iteration of Algorithm 1, the stationarity gap for problem (P) with
respect to f (x,y) is defined as:
k
 (cid:16) (cid:16) (cid:17)(cid:17)
β x −P x − 1 ∇ f (x ,y )
∇G˜(x k,y k) =  k (cid:16) k x (cid:16) k β k x k k k (cid:17)(cid:17).
γ y −P y + 1 ∇ f (x ,y )
k k y k γ y k k k
k
Under Assumption 1, we first establish the following lemma, which provides bounds on the
changes in the function value when x is updated at each iteration of Algorithm 1. This
k
proof is similar to that of Lemma 2.1 in (Xu et al., 2023), for the sake of completeness, we
give its proof.
Lemma 3 Suppose that Assumption 1 holds. Let {(x ,y )} be a sequence generated by
k k
Algorithm 1, then we have
(cid:18) lk (cid:19)
f(x ,y )−f(x ,y ) ≤ − β − 11 ∥x −x ∥2. (8)
k+1 k k k k k+1 k
2
Proof By the optimality condition for (3), we have
⟨∇ f(x ,y )+β (x −x ),x −x ⟩ ≥ 0. (9)
x k k k k+1 k k k+1
The backtracking strategy (C1) implies that
f(x ,y )−f(x ,y ) ≤ ⟨∇ f(x ,y ),x −x ⟩+ l 1k 1∥x −x ∥2. (10)
k+1 k k k x k k k+1 k 2 k+1 k
By adding (9) and (10), we obtain
f(x ,y )−f(x ,y ) ≤ −(cid:16) β − l 1k 1(cid:17) ∥x −x ∥2,
k+1 k k k k 2 k+1 k
which completes the proof.
Next, we provide an estimate of upper bound of the difference between f(x ,y )
k+1 k+1
and f(x ,y ). First, we need to make the following assumption on the parameter γ .
k k k
7Two PF-AGP Algorithms for Minimax Problems
Assumption 2 {γ } is a nonnegative monotonically increasing sequence.
k
Lemma 4 Suppose that Assumptions 1 and 2 hold. Let {(x ,y )} be a sequence generated
k k
by Algorithm 1, then we have
f(x ,y )−f(x ,y )
k+1 k+1 k k
(cid:18) lk lk (cid:19)
≤ − β − 11 − 12 ∥x −x ∥2+γ ∥y −y ∥2
k k+1 k k−1 k+1 k
2 2γ
k−1
(cid:32) (cid:33)
γ (lk−1)2
− µ − k−1 − 22 ∥y −y ∥2. (11)
k−1 k k−1
2 2γ
k−1
Proof The optimality condition for y in (4) implies that
k
⟨∇ f(x ,y )−γ (y −y ),y −y ⟩ ≤ 0. (12)
y k k−1 k−1 k k−1 k+1 k
By the concavity of f(x,y) with respect to y, and combining (12), we have
f(x ,y )−f(x ,y )
k+1 k+1 k+1 k
≤⟨∇ f(x ,y ),y −y ⟩
y k+1 k k+1 k
≤⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩+γ ⟨y −y ,y −y ⟩. (13)
y k+1 k y k k−1 k+1 k k−1 k k−1 k+1 k
Denoting v = (y −y )−(y −y ), we can write the first inner product term in the
k+1 k+1 k k k−1
r.h.s. of (13) as
⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩
y k+1 k y k k−1 k+1 k
=⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩+⟨∇ f(x ,y )−∇ f(x ,y ),v ⟩
y k+1 k y k k k+1 k y k k y k k−1 k+1
+⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩. (14)
y k k y k k−1 k k−1
Next, we estimate the three terms on the right-hand side of (14) respectively. By the
backtracking strategy (C2), (C3) and the Cauchy-Schwarz inequality, we have
lk γ
⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩ ≤ 12 ∥x −x ∥2+ k ∥y −y ∥2 (15)
y k+1 k y k k k+1 k k+1 k k+1 k
2γ 2
k
and
(lk−1)2 γ
⟨∇ f(x ,y )−∇ f(x ,y ),v ⟩ ≤ 22 ∥y −y ∥2+ k−1 ∥v ∥2. (16)
y k k y k k−1 k+1 k k−1 k+1
2γ 2
k−1
By the backtracking strategy (C4) of µ , we obtain
k−1
⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩ ≤ −µ ∥y −y ∥2. (17)
y k k y k k−1 k k−1 k−1 k k−1
Moreover, it can be easily checked that
1 1 1
⟨y −y ,y −y ⟩ = ∥y −y ∥2+ ∥y −y ∥2− ∥v ∥2. (18)
k k−1 k+1 k k k−1 k+1 k k+1
2 2 2
8Two PF-AGP Algorithms for Minimax Problems
Plugging (14)-(18) into (13) and rearranging the terms, we conclude that
(cid:32) (cid:33)
lk γ (lk−1)2
f(x ,y )−f(x ,y ) ≤ 12 ∥x −x ∥2− µ − k−1 − 22 ∥y −y ∥2
k+1 k+1 k+1 k k+1 k k−1 k k−1
2γ 2 2γ
k k−1
(cid:16)γ γ (cid:17)
+ k + k−1 ∥y −y ∥2. (19)
k+1 k
2 2
By Lemma 3, we have
lk
f(x ,y )−f(x ,y ) ≤ −(β − 11)∥x −x ∥2. (20)
k+1 k k k k k+1 k
2
The proof is then completed by combining (19) with (20).
We now establish an important recursion for Algorithm 1.
Lemma 5 Suppose that Assumptions 1 and 2 hold. Let {(x ,y )} be a sequence generated
k k
by Algorithm 1, and denoting
2γ2
f = f(x ,y ), S = k∥y −y ∥2,
k+1 k+1 k+1 k+1 k+1 k
µ
k
(cid:18) 7γ (lk )2 2(lk )2(cid:19) 2γ2
F = f +S − µ + k − 22 − 22 ∥y −y ∥2− kD2, (21)
k+1 k+1 k+1 k 2 2γ µ k+1 k µ y
k k k
then we have
(cid:18) 2µ γ −(lk )2 µ γ −4(lk )2(cid:19)
F −F ≤− k k 22 + k k 22 ∥y −y ∥2
k+1 k k+1 k
2γ 2µ
k k
(cid:18) lk lk 2(lk )2γ (cid:19)
− β − 11 − 12 − 12 k−1 ∥x −x ∥2, (22)
k k+1 k
2 2γ µ µ
k−1 k−1 k
where D = max{∥y −y ∥ | y ,y ∈ Y}.
y 1 2 1 2
Proof By the optimality condition for y in (4) we obtain
k
⟨∇ f(x ,y )−γ (y −y ),y −y ⟩ ≤ 0. (23)
y k+1 k k k+1 k k k+1
Combining (23) and (12), we have
⟨∇ f(x ,y )−∇ f(x ,y )+γ (y −y )−γ (y −y ),y −y ⟩ ≤ 0, (24)
y k k−1 y k+1 k k k+1 k k−1 k k−1 k+1 k
which implies that
γ ∥y −y ∥2−γ ⟨y −y ,y −y ⟩ ≤ ⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩.
k k+1 k k−1 k k−1 k+1 k y k+1 k y k k−1 k+1 k
(25)
Denoting v = (y − y ) − (y − y ), and using the identity 1⟨v ,y − y ⟩ =
k+1 k+1 k k k−1 2 k+1 k+1 k
1∥y −y ∥2+ 1∥v ∥2− 1∥y −y ∥2, we conclude that
2 k+1 k 2 k+1 2 k k−1
(cid:18) (cid:19)
1 1 1
γ ∥y −y ∥2−γ ∥y −y ∥2+ ∥y −y ∥2− ∥v ∥2
k k+1 k k−1 k+1 k k k−1 k+1
2 2 2
≤ ⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩. (26)
y k+1 k y k k−1 k+1 k
9Two PF-AGP Algorithms for Minimax Problems
By rearranging the terms of the above inequality, we then have
(cid:16) γ (cid:17) γ γ
γ − k−1 ∥y −y ∥2+ k−1 ∥v ∥2− k−1 ∥y −y ∥2
k k+1 k k+1 k k−1
2 2 2
≤⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩
y k+1 k y k k−1 k+1 k
=⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩+⟨∇ f(x ,y )−∇ f(x ,y ),v ⟩
y k+1 k y k k k+1 k y k k y k k−1 k+1
+⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩. (27)
y k k y k k−1 k k−1
By backtracking strategy (C2) and the Cauchy-Schwarz inequality, we can easily get that
(lk )2 µ
⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩ ≤ 12 ∥x −x ∥2+ k ∥y −y ∥2. (28)
y k+1 k y k k k+1 k k+1 k k+1 k
2µ 2
k
Combining (15), (16), (27) and (28), we have
γ γ (lk )2 (cid:16)µ γ γ (cid:17)
k ∥y −y ∥2− k−1 ∥y −y ∥2 ≤ 12 ∥x −x ∥2+ k + k−1 − k ∥y −y ∥2
k+1 k k k−1 k+1 k k+1 k
2 2 2µ 2 2 2
k
(cid:32) (cid:33)
(lk−1)2
− µ − 22 ∥y −y ∥2. (29)
k−1 k k−1
2γ
k−1
4γ
By multiplying k−1 on both sides of (29), using the definition of S and Assumption 2,
µ k+1
k−1
we obtain
2(lk )2γ
S −S ≤ 12 k−1 ∥x −x ∥2+2γ ∥y −y ∥2
k+1 k k+1 k k−1 k+1 k
µ µ
k−1 k
(cid:32) (cid:33) (cid:32) (cid:33)
2(lk−1)2 2γ2 2γ2
− 4γ − 22 ∥y −y ∥2+ k − k−1 D2.
k−1 µ k k−1 µ µ y
k−1 k k−1
The proof is then completed by Lemma 4 and the definition.
We are now ready to establish the iteration complexity for the PF-AGP-NSC algorithm.
In particular, letting ∇G(x ,y ) be defined as in Definition 1 and ε > 0 be a given target
k k
accuracy, we provide a bound on T(ε), the first iteration index to achieve an ε-stationary
point, i.e., ∥∇G(x ,y )∥ ≤ ε, which is equivalent to
k k
T(ε) = min{k | ∥∇G(x ,y )∥ ≤ ε}. (30)
k k
Theorem 6 Suppose that Assumptions 1 and 2 hold. Let {(x ,y )} be a sequence generated
k k
by Algorithm 1, β = lk + l 1k 2µ k−1 + 32(l 1k 2)2(l 2k 2−1)2 , γ = 8(l 2k 2)2 , then it holds that
k 11 8(l 2k 2−1)2 µ kµ2 k−1 k µ k
F −F
2
T(ε) ≤ +1, (31)
d ε2
1
(cid:18) (cid:19)
where F is defined in (21), F = f− 224L2 22 +µ − 2(l 21 2)2 − (l 21 2µ)2 + 213L4 22 D2, with f =
2 µ 1 µ1 128L2
22
µ3 y
min f(x,y), D = max{∥y −y ∥ | y ,y ∈ Y}, d = min{ βmin , 15µ3+27l 21 2µ },
(x,y)∈X×Y y 1 2 1 2 1 2β2 +4L2 215L4
max 12 22
β = l1 + l 11 2µ + 32(l 11 2)2(l 21 2)2 and β = 2L + µ1L12 + 4096(L12)2(L22)2 .
min 11 64(L22)2 µ3
1
max 11 4(l 21 2)2 µ3
10Two PF-AGP Algorithms for Minimax Problems
Proof It follows immediately from Definition 1 and (3) that
∥(∇G ) ∥ = β ∥x −x ∥. (32)
k x k k+1 k
On the other hand, by the triangle inequality, we obtain that
(cid:13) (cid:18) (cid:19) (cid:18) (cid:19)(cid:13)
(cid:13) 1 1 (cid:13)
∥(∇G k) y∥ ≤ γ k(cid:13)P y y k + ∇ yf(x k+1,y k) −P y y k + ∇ yf(x k,y k) (cid:13)
(cid:13) γ γ (cid:13)
k k
+γ ∥y −y ∥
k k+1 k
≤ L ∥x −x ∥+γ ∥y −y ∥. (33)
12 k+1 k k k+1 k
By combining (32) and (33), and using the Cauchy-Schwarz inequality, we obtain
∥∇G ∥2 ≤ (β2+2L2 )∥x −x ∥2+2γ2∥y −y ∥2. (34)
k k 12 k+1 k k k+1 k
Let
d(k)
=
min 2µkγk 2− γk(lk 22)2 +µkγk− 2µ4 k(lk 22)2
,
β k−lk 1 21− 2γl kk 1 −2 1−2(l µk 1 k2 −)2 1γ µk k−1 
> 0. Multiplying both
1 2γ2 β2+2L2
 k k 12 
(k)
sides of (34) by d , and using (22) in Lemma 5, we have
1
d(k) ∥∇G ∥2 ≤ F −F . (35)
1 k k k+1
(k)
By using the definitions of d , β and γ , we have d < d , then we get
1 k k 1 1
d ∥∇G ∥2 ≤ F −F . (36)
1 k k k+1
Summing up the above inequalities from k = 2 to k = T(ε), we obtain
T(ε)
(cid:88)
d ∥∇G ∥2 ≤ F −F . (37)
1 k 2 T(ε)+1
k=2
Note that by the definition of F in Lemma 5, we have
k+1
2γ2
F = f +S − T(ε) D2
T(ε)+1 T(ε)+1 T(ε)+1 µ y
T(ε)
(cid:32) (cid:33)
7 2(lT(ε)+1 )2 (lT(ε)+1 )2
− γ +µ − 22 − 22 ||y −y ||2
2 T(ε)+1 T(ε)+1 µ 2γ T(ε)+1 T(ε)
T(ε)+1 T(ε)+1
(cid:32) 224L2 2(l1 )2 (cid:0) l1 µ(cid:1)2 213L4 (cid:33)
≥ f − 22 +µ − 22 − 22 + 22 D2
µ 1 µ 128L2 µ3 y
1 22
= F, (38)
where the inequality follows from the definitions of f and D , and the fact that S ≥ 0
y k
(∀k ≥ 1). We then conclude from (37) that
T(ε)
(cid:88)
d ∥∇G ∥2 ≤ F −F ≤ F −F, (39)
1 k 2 T(ε)+1 2
k=2
11Two PF-AGP Algorithms for Minimax Problems
which, in view of the definition of T(ε), implies that
d (T(ε)−1)ε2 ≤ F −F, (40)
1 2
or equivalently,
F −F
2
T(ε) ≤ +1. (41)
d ε2
1
We complete the proof.
Remark 7 At the k-th iteration of Algorithm 1, the number of backtracking step is upper
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
bounded by log 2L11 +log 2L12 +log 2L22 +log 2µ1 = O(1). By Theorem 6, the
2 l1 2 l1 2 l1 2 µ
11 12 22
total number of gradient calls of Algorithm 1 to obtain an ε-stationary point that satisfies
∥∇G(x ,y )∥ ≤ ε is upper bounded by O(Lκ3ε−2) under the nonconvex-strongly concave
k k
setting, where κ is the condition number.
2.2 Nonconvex-concave Setting
In this subsection, we propose a completely parameter-free AGP algorithm for solving
nonconvex-concave minimax problems. Similar to the idea of the PF-AGP-NSC Algorithm,
we estimate the Lipschitz constants L , L and L iteratively, however we do not need
11 12 22
to estimate the strongly concave constant µ. At each iteration of the proposed algorithm,
we aim to find a tuple (lk,i,lk,i,lk,i) by backtracking such that conditions (C1), (C2), (C3)
11 12 22
and the following condition (C5) are satisfied at (x ,y ):
k,i k,i
c2 +c lk,i
Ck,i = ⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩+ k,i k,i 22 ∥y −y ∥2
5 y k,i k,i k,i y k,i k,i k k,i k lk,i+2c k,i k
22 k,i
1
+ ∥∇ f (x ,y )−∇ f (x ,y )∥2 ≤ 0. (C5)
lk,i+2c y k,i k,i k,i y k,i k,i k
22 k,i
The proposed algorithm, denoted as PF-AGP-NC, is formally presented in Algorithm 2.
PleasenotethattheexistingSGDA-Balgorithm(Xuetal.,2023)forsolvingnonconvex-
concave minimax problems requires the realization of the diameter of the feasible set Y in
order to construct the key auxiliary problem, while the PF-AGP-NC algorithm we propose
is a completely parameter-free algorithm that does not require any prior parameters.
2.2.1 Complexity Analysis
In this subsection, we analyze the iteration complexity of Algorithm 2 for solving (P) under
the general nonconvex-concave setting. The backtracking strategy (C5) is a key inequality
that we will use to establish some important recursions for the PF-AGP-NC algorithm
under the nonconvex-concave setting in the following two lemmas. This is also one of the
main differences between proofs in the nonconvex- strongly concave setting and proofs in
the nonconvex-concave setting.
12Two PF-AGP Algorithms for Minimax Problems
Algorithm 2 A parameter-free alternating gradient projection (PF-AGP-NC) algorithm
for nonconvex-concave minimax problems
Step 1: Input x ,y , β , γ , c , l1 , l1 , l1 ; Set k = 1.
1 1 1 1 1 11 12 22
Step 2: Updata x and y :
k k
(a): Set i = 1, lk,i = lk , lk,i = lk , lk,i = lk , β = β , γ = γ , c = c .
11 11 12 12 22 22 k,i k k,i k k,i k
(b): Update x and y :
k,i k,i
(cid:18) (cid:19)
1
x = P x − ∇ f(x ,y ) , (42)
k,i X k x k k
β
k,i
(cid:18) (cid:19)
1 1
y = P y + ∇ f(x ,y )− c y . (43)
k,i Y k y k,i k k,i k
γ γ
k,i k,i
(c): Compute Ck,i, Ck,i, Ck,i, Ck,i as in (C1), (C2), (C3), (C5);
1 2 3 5
(d): Update lk,i, lk,i, lk,i:
11 12 22
sgn(Ck,i)+3 sgn(Ck,i)+3
lk,i+1 = 1 lk,i, lk,i+1 = 2 lk,i, (44)
11 2 11 12 2 12
max{sgn(Ck,i),sgn(Ck,i)}+3
lk,i+1 = 3 5 lk,i. (45)
22 2 22
(e): If Ck,i ≤ 0, Ck,i ≤ 0, Ck,i ≤ 0 and Ck,i ≤ 0, then
1 2 3 5
x = x , y = y , lk = lk,i+1, lk = lk,i+1, lk = lk,i+1, β = β , γ = γ ,
k+1 k,i k+1 k,i 11 11 12 12 22 22 k k,i k k,i
c = c , go to Step 3;
k k,i
Otherwise, i = i+1,
√
lk,i 1280(lk,i)2 k 19lk,i
β = 12 + 12 , γ = 20lk,i, c = 22 , (46)
k,i 20lk−1,i 192lk−1,i k,i 22 k,i k1/4
22 12
go to Step 2(b).
Step 3: If some stationary condition is satisfied, stop; Otherwise, set k = k+1, go to
Step 2.
Lemma 8 Suppose that Assumptions 1 and 2 hold. Let {(x ,y )} be a sequence generated
k k
by Algorithm 2. If ∀k, β > lk and γ ≥ l 2k 2+2c k, then
k 11 k 2
f(x ,y )−f(x ,y )
k+1 k+1 k k
(cid:32) β (cid:0) lk (cid:1)2(cid:33) γ
≤− k − 12 ∥x −x ∥2+γ ∥y −y ∥2+ k−1 ∥y −y ∥2
k+1 k k−1 k+1 k k k−1
2 2γ 2
k−1
+ c k−1 (cid:0) ∥y ∥2−∥y ∥2(cid:1) + c k ∥y −y ∥2. (47)
k+1 k k+1 k
2 2
Proof The optimality condition for y in (43) implies that
k
⟨∇ f (x ,y )−γ (y −y ),y −y ⟩ ≤ 0. (48)
y k−1 k k k−1 k k−1 k k+1
13Two PF-AGP Algorithms for Minimax Problems
The concavity of f (x ,y) with respect to y together with (48) then imply that
k k+1
f (x ,y )−f (x ,y ) ≤ ⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩
k k+1 k+1 k k+1 k y k k+1 k y k−1 k k k+1 k
+⟨∇ f (x ,y ),y −y ⟩
y k−1 k k−1 k+1 k
= ⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩
y k k+1 k y k−1 k k k+1 k
+⟨∇ f (x ,y )−∇ f (x ,y ),v ⟩
y k−1 k k y k−1 k k−1 k+1
+⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩
y k−1 k k y k−1 k k−1 k k−1
+γ ⟨y −y ,y −y ⟩, (49)
k−1 k k−1 k+1 k
where v = y −y −(y −y ). We now provide upper bounds for the inner prod-
k+1 k+1 k k k−1
uct terms on the right hand side of (49). Firstly, by the definition of f (x ,y ) and
k k+1 k
f (x ,y ), the backtracking strategy (C2) and the Cauchy-Schwarz inequality, we have
k−1 k k
⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩
y k k+1 k y k−1 k k k+1 k
= ⟨∇ f(x ,y )−∇ f(x ,y ),y −y ⟩−(c −c )⟨y ,y −y ⟩
y k+1 k y k k k+1 k k k−1 k k+1 k
(lk )2 γ c −c
≤ 12 ∥x −x ∥2+ k−1 ∥y −y ∥2− k k−1 (∥y ∥2−∥y ∥2)
k+1 k k+1 k k+1 k
2γ 2 2
k−1
c −c
+ k k−1 ∥y −y ∥2
k+1 k
2
(lk )2 γ c −c
≤ 12 ∥x −x ∥2+ k−1 ∥y −y ∥2− k k−1 (∥y ∥2−∥y ∥2)
k+1 k k+1 k k+1 k
2γ 2 2
k−1
c
+ k ∥y −y ∥2. (50)
k+1 k
2
Secondly, by the Cauchy-Schwarz inequality, we get
⟨∇ f (x ,y )−∇ f (x ,y ),v ⟩
y k−1 k k y k−1 k k−1 k+1
1 γ
≤ ∥∇ f (x ,y )−∇ f (x ,y )∥2+ k−1 ∥v ∥2. (51)
y k−1 k k y k−1 k k−1 k+1
2γ 2
k−1
Thirdly, by (C4) we have that
⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩
y k−1 k k y k−1 k k−1 k k−1
1
≤− ∥∇ f (x ,y )−∇ f (x ,y )∥2
lk−1+2c y k−1 k k y k−1 k k−1
22 k−1
c lk−1+c2
− k−1 22 k−1∥y −y ∥2
lk−1+2c k k−1
22 k−1
1
≤− ∥∇ f (x ,y )−∇ f (x ,y )∥2. (52)
lk−1+2c y k k k y k k k−1
22 k−1
We also have the following equation that
γ γ γ
γ ⟨y −y ,y −y ⟩ = k−1 ∥y −y ∥2+ k−1 ∥y −y ∥2− k−1 ∥v ∥2. (53)
k−1 k k−1 k+1 k k+1 k k k−1 k+1
2 2 2
14Two PF-AGP Algorithms for Minimax Problems
lk +2c
By γ ≥ 22 k, plugging (50)-(53) into (49), and using the definition of f (x ,y ) and
k 2 k k+1 k+1
f (x ,y ), we obtain
k k+1 k
f(x ,y )−f(x ,y )
k+1 k+1 k+1 k
(lk )2 γ
≤ 12 ∥x −x ∥2+γ ∥y −y ∥2+ k−1 ∥y −y ∥2
k+1 k k−1 k+1 k k k−1
2γ 2
k−1
+ c k−1 (cid:0) ∥y ∥2−∥y ∥2(cid:1) + c k ∥y −y ∥2. (54)
k+1 k k+1 k
2 2
By β > lk , we have
k 11
β
f(x ,y )−f(x ,y ) ≤ − k ∥x −x ∥2. (55)
k+1 k k k k+1 k
2
By adding (54) and (55), we complete the proof.
Lemma 9 Suppose that Assumptions 1 and 2 hold. Let {(x ,y )} be a sequence generated
k k
by Algorithm 2. Denote
8γ2 (cid:18) c (cid:19)
S = k∥y −y ∥2+8γ k+1 −1 ∥y ∥2, (56)
k+1 k+1 k k k+1
c c
k k
15γ c
F = f(x ,y )+S − k ∥y −y ∥2− k ∥y ∥2. (57)
k+1 k+1 k+1 k+1 k+1 k k+1
2 2
If
γ γ 1 lk +2c
β > lk , k+1 − k ≤ , γ ≥ 22 k , (58)
k 11 c c 5 k 2
k+1 k
then ∀k ≥ 2,
(cid:18) (cid:19)
9γ c
F −F =− k − k ∥y −y ∥2
k+1 k k+1 k
10 2
(cid:32) (cid:33)
β (lk )2 16(lk )2γ
− + k − 12 − 12 k−1 ∥x −x ∥2
2 2γ c2 k+1 k
k−1 k−1
(cid:18) (cid:19)
c c c −c
+8 γ k+1 −γ k ∥y ∥2+ k−1 k ∥y ∥2. (59)
k k−1 k+1 k+1
c c 2
k k−1
Proof The optimality condition for y in (43) implies that
k
⟨∇ f (x ,y )−γ (y −y ),y −y ⟩ ≤ 0, (60)
y k k+1 k k k+1 k k k+1
By (48) and (60), we have
⟨∇ f (x ,y )−∇ f (x ,y )+γ (y −y )−γ (y −y ),y −y ⟩ ≤ 0,
y k−1 k k−1 y k k+1 k k k+1 k k−1 k k−1 k+1 k
and
γ ∥y −y ∥2−γ ⟨y −y ,y −y ⟩
k k+1 k k−1 k+1 k k k−1
≤⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩, (61)
y k k+1 k y k−1 k k−1 k+1 k
15Two PF-AGP Algorithms for Minimax Problems
which can be rewritten as
(cid:16) γ (cid:17) γ γ
γ − k−1 ∥y −y ∥2+ k−1 ∥v ∥2− k−1 ∥y −y ∥2
k k+1 k k+1 k k−1
2 2 2
≤⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩
y k k+1 k y k−1 k k−1 k+1 k
=⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩+⟨∇ f (x ,y )−∇ f (x ,y ),v ⟩
y k k+1 k y k−1 k k k+1 k y k−1 k k y k−1 k k−1 k+1
+⟨∇ f (x ,y )−∇ f (x ,y ),y −y ⟩. (62)
y k k−1 k−1 y k−1 k k−1 k k−1
Using an argument similar to the proof of (50)-(53), by (C5) and the Cauchy-Schwarz
inequality, we conclude from the above inequality that
(cid:16) γ (cid:17) γ γ
γ − k−1 ∥y −y ∥2+ k−1 ∥v ∥2− k−1 ∥y −y ∥2
k k+1 k k+1 k k−1
2 2 2
(lk )2 a c −c
≤ 12 ∥x −x ∥2+ k ∥y −y ∥2− k k−1 (∥y ∥2−∥y ∥2)
k+1 k k+1 k k+1 k
2a 2 2
k
c −c 1
+ k k−1 ∥y −y ∥2+ ∥∇ f (x ,y )−∇ f (x ,y )∥2
k+1 k y k−1 k k y k−1 k k−1
2 2γ
k−1
γ 1
+ k−1 ∥v ∥− ∥∇ f (x ,y )−∇ f (x ,y )∥2
2 k+1 lk−1+2c y k k k y k k k−1
22 k−1
c lk−1+c2
− k−1 22 k−1∥y −y ∥2, (63)
lk−1+2c k k−1
22 k−1
for any a > 0. Obviously, we have
−c k−1l 2k 2−1+c2
k−1 ≤
−c k−1l 2k 2−1+c2
k−1 =
−c
k−1. Combining
k 2c +lk−1 2(lk−1+c ) 2
k−1 22 22 k−1
lk +2c
γ ≥ 22 k and rearranging the terms in (63), we obtain
k 2
γ c −c
k ∥y −y ∥2+ k k−1 ∥y ∥2
k+1 k k+1
2 2
γ (cid:16)α γ γ (cid:17) c −c
≤ k−1 ∥y −y ∥2+ k + k−1 − k ∥y −y ∥2+ k k−1 ∥y ∥2
k k−1 k+1 k k
2 2 2 2 2
(lk )2 c
+ 12 ∥x −x ∥2− k−1 ∥y −y ∥2. (64)
k+1 k k k−1
2a 2
k
By multiplying 16γ k−1 on both sides of the above inequality, setting a = c k, and using the
c k 2
k−1
definition of S , (58) and {γ } is nodecreasing, we then obtain
k+1 k
(cid:18) c c (cid:19) 16(lk )2γ
S −S ≤ 8 γ k+1 −γ k ∥y ∥2+ 12 k−1 ∥x −x ∥2
k+1 k k c k−1 c k+1 c2 k+1 k
k k−1 k−1
28γ
+ k ∥y −y ∥2−8γ ∥y −y ∥2. (65)
k+1 k k−1 k k−1
5
Combining (47) and (65), and using the definition of F , we complete the proof.
k+1
We are now ready to establish the iteration complexity for the PF-AGP-NC algorithm
to achieve an ε-stationary point under the general nonconvex–concave setting.
16Two PF-AGP Algorithms for Minimax Problems
Theorem 10 Suppose that Assumptions 1 and 2 hold. Let {(x ,y )} be a sequence gen-
√ k k
erated by Algorithm 2. If β = l 1k 2 + 1280(l 1k 2)2 k , γ = 20lk , c = 19l 2k 2, then for any
k 20lk−1 192lk−1 k 22 k k1/4
22 12
given ε > 0, k ≥ 2, we have
 
(cid:32) 5120(L )2d d (cid:33)2 764L4 Dˆ4
12 2 3 22 y
T(ε) ≤ max +1 , ,
192l1 ε2 ε4
22
where d = F − F + 339L Dˆ2, d = max{d¯ , 20·192L2 22}, Dˆ = max{∥y∥|y ∈ Y} with
2 2 22 y 3 1 17(l1 )2 y
12
F = f −639L Dˆ2, f = min f(x,y), d¯ = 32+ 1+1600L2 22.
22 y (x,y)∈X×Y 1 128(l1 )2
12
Proof We can easily see that the relations in (58) are satisfied by the settings of c , γ
k k
16(lk )2γ
and β . Denote α = 12 k−1. Then it follows from the selection of β and α that
k k c2 k k
k−1
β (lk )2 16(lk )2γ
k − 12 − 12 k−1 = α . (66)
2 2γ c2 k
k−1 k−1
This observation, in view of Lemma 8, c k ≤ 19γ k ≤ 19γ k implies that
2 40k1/4 40
17γ
α ∥x −x ∥2+ k ∥y −y ∥2
k k+1 k k+1 k
40
(cid:18) (cid:19) (67)
c c c −c
≤F −F +8 γ k+1 −γ k ∥y ∥2+ k−1 k ∥y ∥2.
k k+1 k k−1 k+1 k+1
c c 2
k k−1
We can easily check from the definition of f (x,y) that
k
∥∇G ∥−∥∇˜G ∥ ≤ c ∥y ∥. (68)
k k k k
By replacing f with f , similar to (32) and (33), we immediately obtain that
k
∥(∇˜G ) ∥ = β ∥x −x ∥, (69)
k x k k+1 k
and
∥(∇˜G ) ∥ ≤ γ ∥y −y ∥+lk ∥x −x ∥. (70)
k y k k+1 k 12 k+1 k
Combining (69) and (70), and using the Cauchy-Schwarz inequality, we have
∥∇˜G ∥2 ≤ (β2+2(lk )2)∥x −x ∥2+2γ 2∥y −y ∥2. (71)
k k 12 k+1 k k k+1 k
Since both α and β are in the same order when k becomes large enough, it then follows
k k
from the definition of d¯ that ∀k ≥ 1,
1
(cid:18) (cid:19)2
l 1k 2 + 64(l 1k 2)2γ k−1 +2(lk )2
(β k)2+2(l 1k 2)2
=
γ k−1 c2 k−1 12
α2 α2
k k
1+1600L2
≤ 32+ 22 = d¯ . (72)
128(l1 )2 1
12
17Two PF-AGP Algorithms for Minimax Problems
Combining the previous two inequalities in (71) and (72), we obtain
∥∇G˜ ∥2 ≤ d¯ (α )2∥x −x ∥2+2γ2∥y −y ∥2. (73)
k 1 k k+1 k k k+1 k
Denote d(2) = 1 , c˜ = 19 , by multiplying d(2) on the both sides of (73), and
k max{d¯ 1α k,1 87 0γ k} k 20k1/4 k
using (67), we have
d(2) ∥∇G˜ ∥2
k k
(cid:18) (cid:19)
c c c −c
≤F −F +8 γ k+1 −γ k ∥y ∥2+ k−1 k ∥y ∥2
k k+1 k k−1 k+1 k+1
c c 2
k k−1
(cid:18) (cid:19)
c c c˜ −c˜
≤F −F +8 γ k+1 −γ k ∥y ∥2+40L k−1 k ∥y ∥2. (74)
k k+1 k k−1 k+1 22 k+1
c c 2
k k−1
Denoting
(cid:110) (cid:12)(cid:13) (cid:13) ε (cid:111)
T˜(ε) = min k(cid:12)(cid:13)∇G˜(x ,y )(cid:13) ≤ ,k ≥ 2 , (75)
(cid:12)(cid:13) k k (cid:13) 2
summing both sides of (74) from k = 2 to k = T˜(ε), we then obtain
T (cid:88)˜(ε) d(2)(cid:13) (cid:13)∇G˜ (cid:13) (cid:13)2 ≤ F −F +8(cid:32) γ T˜(ε)−1c T˜(ε) − γ 1c 2(cid:33) Dˆ2+40L (cid:18) c˜ 1 − c˜ T˜(ε)(cid:19) Dˆ2
k (cid:13) k(cid:13) 2 T˜(ε) c c y 22 2 2 y
k=2
T˜(ε)−1 1
≤ F −F +339L Dˆ2. (76)
2 T˜(ε) 22 y
Note that by the definition of F in Lemma 9, we have
k+1
F ≥ f −639L Dˆ2 = F, (77)
T˜(ε) 22 y
where f = min f(x,y). We then conclude from (76) that
(x,y)∈X×Y
∞
(cid:88) d(2)(cid:13)
(cid:13)∇G˜
(cid:13) (cid:13)2
≤ F −F +339L Dˆ2 = d . (78)
k (cid:13) k(cid:13) 2 22 y 2
k=2
We can see from the selection of d that d ≥ max{d¯ , 80γ k}, which implies d(2) ≥ 1 ,
3 3 1 17α
k
k d3α
k
by multiplying d on the both sides of (78), and combining the definition of d , we have
3 2
(cid:13) (cid:13)2
(cid:80)∞ 1 (cid:13)∇G˜ (cid:13) ≤ d d , which, by the definition of T˜(ε), implies that
k=2 α (cid:13) k(cid:13) 2 3
k
ε2 d d d d
2 3 2 3
≤ ≤ . (79)
4 (cid:80)T˜(ε) 1 (cid:80)T˜(ε) 1
k=2 α k=2 α¯
k
(cid:113)
By using the fact (cid:80)T k˜ =(ε 2) √1
k
≥ T˜(ε)−1 and (79), we conclude ε 42 ≤ 191 22 l8 10· (cid:16)(L√12 T˜) (2 εd )2 −d 13
(cid:17)
or
22
equivalently,
(cid:32) (cid:33)2
5120(L )2d d
T˜(ε) ≤ 12 2 3 +1 . (80)
192l1 ε2
22
18Two PF-AGP Algorithms for Minimax Problems
On the other hand, if k ≥ 764L4 22Dˆ y4 , then c = 19γ k ≤ ε . This inequality together with
ε4 k 20k1/4 2Dˆ
y
the definition of Dˆ , then imply that c ∥y ∥ ≤ ε. Therefore, there exists a
y k k 2
(cid:32)
764L4
Dˆ4(cid:33)
T(ε) ≤ max T˜(ε), 22 y
ε4
 
(cid:32) 5120(L )2d d (cid:33)2 764L4 Dˆ4
12 2 3 22 y
≤ max +1 , , (81)
192l1 ε2 ε4
22
(cid:13) (cid:13)
such that ∥∇G ∥ ≤ (cid:13)∇G˜ (cid:13)+c ∥y ∥ ≤ ε + ε = ε.
k (cid:13) k(cid:13) k k 2 2
Remark 11 At the k-th iteration of Algorithm 2, the number of backtracking step is upper
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
bounded by log 2L11 +log 2L12 +log 2L22 = O(1). According to Theorem 10, the
2 l1 2 l1 2 l1
11 12 22
total number of gradient calls of Algorithm 2 to obtain an ε-stationary point that satisfies
∥∇G(x ,y )∥ ≤ ε is O(L4ε−4) under the nonconvex-concave setting.
k k
3 Numerical results
In this section, we compare the numerical performance of the proposed PF-AGP-NC algo-
rithm with the AGP algorithm.
3.1 Dirac-GAN problem
The Dirac-GAN problem (Mescheder et al., 2018) can be formulated as the following non-
convex–concave minimax problem :
minmaxL(x,y) = −log(1+exp(−xy))+log2, (82)
x y
where (0,0) is the unique stationary point.
Experimental setup. Weset the initialpointoftheexperimentat (x,y) = (1,1). The
termination condition is defined as the norm of the gradient ∥∇f(x ,y )∥ being less than or
i i
equal to 10−5, where i represents the number of gradient calls.
For the AGP algorithm, let α and β be the stepsize of x and y, respectively. We set
x y
α
x
= √0.8 k, β
y
= 0.3 and c
k
= k0 1. /5 4. For the PF-AGP-NC algorithm, we do not need to preset
specific parameters.
In the experiments, we counted the number of gradient calls of the PF-AGP-NC algo-
rithm. The results of alternating gradient projection in the k th iteration are represented
by nodes in the graph and the number of backtracking step computations is represented by
the connecting lines.
Results Figure 1 shows the convergence curves of the AGP and PF-AGP-NC algo-
rithms. It can be seen that although the PF-AGP-NC algorithm involves a double-layer
structure, requiring the determination of points in the outer layer through computations
in the inner layer, even when considering the number of iterations in the inner layer, the
overall convergence speed is still faster than that of the AGP algorithm.
19Two PF-AGP Algorithms for Minimax Problems
Figure 1: number of gradient calls i vs log(||∇f(x ,y )||)
i i
4 Conclusions and discussion
Inthispaper,basedontheframeworkofbacktracking,weproposetwocompletelyparameter-
free alternating gradient projection algorithms, i.e., the PF-AGP-NSC algorithm and the
PF-AGP-NC algorithm, for solving nonconvex-(strongly) concave minimax problems re-
spectively, which does not require prior knowledge of parameters such as the Lipschtiz
constant L or the strongly concave constant µ. Moreover, we show that the total num-
ber of gradient calls of the PF-AGP-NSC algorithm to obtain an ε-stationary point for
nonconvex-strongly concave minimax problems is upper bounded by
O(cid:0) Lκ3ε−2(cid:1)
where κ is
theconditionnumber,whilethetotalnumberofgradientcallsofthePF-AGP-NCalgorithm
to obtain an ε-stationary point for nonconvex-concave minimax problems is upper bounded
by
O(cid:0) L4ε−4(cid:1)
. As far as we know, the PF-AGP-NSC algorithm and the PF-AGP-NC al-
gorithm are the first completely parameter-free algorithms for solving nonconvex-strongly
concaveminimaxproblemsandnonconvex-concaveminimaxproblemsrespectively. Numer-
ical results demonstrate the efficiency of the two proposed algorithms.
References
Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial
networks. In International conference on machine learning, pages 214–223. PMLR, 2017.
A Ben and LE Tal. Ghaoui and a. nemirovski, robust optimization, 2009.
20Two PF-AGP Algorithms for Minimax Problems
Yunmei Chen, Guanghui Lan, and Yuyuan Ouyang. Optimal primal-dual methods for a
class of saddle point problems. SIAM Journal on Optimization, 24(4):1779–1814, 2014.
Yunmei Chen, Guanghui Lan, and Yuyuan Ouyang. Accelerated schemes for a class of
variational inequalities. Mathematical Programming, 165:113–149, 2017.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song.
Sbeed: Convergent reinforcement learning with nonlinear function approximation. In
International conference on machine learning, pages 1125–1134. PMLR, 2018.
Cong Dang and Guanghui Lan. Randomized first-order methods for saddle point optimiza-
tion. arXiv preprint arXiv:1409.8625, 2014.
Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with wasser-
stein distance. Mathematics of Operations Research, 48(2):603–655, 2023.
Georgios B Giannakis, Qing Ling, Gonzalo Mateos, Ioannis D Schizas, and Hao Zhu. De-
centralized learning for wireless communications and networking. In Splitting Methods in
Communication, Imaging, Science, and Engineering, pages 461–497. Springer, 2017.
Ryan Giordano, Tamara Broderick, and Michael I Jordan. Covariances, robustness, and
variational bayes. Journal of machine learning research, 19(51):1–49, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems, 27, 2014.
Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-
nonconcave minimax optimization? In International conference on machine learning,
pages 4880–4889. PMLR, 2020.
Weiwei Kong and Renato DC Monteiro. An accelerated inexact proximal point method for
solving nonconvex-concave min-max problems. SIAM Journal on Optimization, 31(4):
2558–2585, 2021.
Guanghui Lan, Yuyuan Ouyang, and Zhe Zhang. Optimal and parameter-free gradient
minimization methods for smooth optimization. arXiv preprint arXiv:2310.12139, 2023.
XiangLi, JunchiYang, andNiaoHe. Tiada: Atime-scaleadaptivealgorithmfornonconvex
minimax optimization. arXiv preprint arXiv:2210.17478, 2022.
TianyiLin, ChiJin, andMichaelJordan. Ongradientdescentascentfornonconvex-concave
minimax problems. In International Conference on Machine Learning, pages 6083–6093.
PMLR, 2020a.
Tianyi Lin, Chi Jin, and Michael I Jordan. Near-optimal algorithms for minimax optimiza-
tion. In Conference on Learning Theory, pages 2738–2779. PMLR, 2020b.
Chengchang Liu and Luo Luo. Regularized newton methods for monotone variational in-
equalities with h\” older continuous jacobians. arXiv preprint arXiv:2212.07824, 2022.
21Two PF-AGP Algorithms for Minimax Problems
Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen. Hybrid block successive
approximationforone-sidednon-convexmin-maxproblems: algorithmsandapplications.
IEEE Transactions on Signal Processing, 68:3676–3691, 2020.
Gonzalo Mateos, Juan Andr´es Bazerque, and Georgios B Giannakis. Distributed sparse
linear regression. IEEE Transactions on Signal Processing, 58(10):5262–5276, 2010.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans
doactuallyconverge? InInternational conference on machine learning, pages3481–3490.
PMLR, 2018.
Renato DC Monteiro and Benar Fux Svaiter. On the complexity of the hybrid proximal
extragradient method for the iterates and the ergodic mean. SIAM Journal on Optimiza-
tion, 20(6):2755–2787, 2010.
Angelia Nedi´c and Asuman Ozdaglar. Subgradient methods for saddle-point problems.
Journal of optimization theory and applications, 142:205–228, 2009.
ArkadiNemirovski. Prox-methodwithrateofconvergenceo(1/t)forvariationalinequalities
with lipschitz continuous monotone operators and smooth convex-concave saddle point
problems. SIAM Journal on Optimization, 15(1):229–251, 2004.
Yu Nesterov. Universal gradient methods for convex optimization problems. Mathematical
Programming, 152(1):381–404, 2015.
Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities
and related problems. Mathematical Programming, 109(2):319–344, 2007.
Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global
performance. Mathematical programming, 108(1):177–205, 2006.
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn.
Solvingaclassofnon-convexmin-maxgamesusingiterativefirstordermethods.Advances
in Neural Information Processing Systems, 32, 2019.
Dmitrii M Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Efficient search of first-
order nash equilibria in nonconvex-concave smooth min-max problems. SIAM Journal
on Optimization, 31(4):2508–2538, 2021.
WeiweiPan, JingjingShen, andZiXu. Anefficientalgorithmfornonconvex-linearminimax
optimizationproblemanditsapplicationinsolvingweightedmaximindispersionproblem.
Computational Optimization and Applications, 78:287–306, 2021.
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Weakly-convex–concave
min–max optimization: provable algorithms and applications in machine learning. Opti-
mization Methods and Software, 37(3):1087–1121, 2022.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence
and robustness of training gans with regularized optimal transport. Advances in Neural
Information Processing Systems, 31, 2018.
22Two PF-AGP Algorithms for Minimax Problems
Soroosh Shafieezadeh Abadeh, Peyman M Mohajerin Esfahani, and Daniel Kuhn. Distri-
butionally robust logistic regression. Advances in neural information processing systems,
28, 2015.
Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient
algorithmsforsmoothminimaxoptimization. Advances in Neural Information Processing
Systems, 32, 2019.
Lieven Vandenberghe and Stephen Boyd. Convex optimization, volume 1. Cambridge
university press Cambridge, 2004.
QiushuiXu,XuanZhang,NecdetSerhatAybat,andMertGu¨rbu¨zbalaban. Astochasticgda
method with backtracking for solving nonconvex (strongly) concave minimax problems.
arXiv preprint arXiv:2403.07806, 2024.
Zi Xu, Huiling Zhang, Yang Xu, and Guanghui Lan. A unified single-loop alternating
gradient projection algorithm for nonconvex–concave and convex–nonconcave minimax
problems. Mathematical Programming, 201(1):635–706, 2023.
Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance-reduced
optimization for a class of nonconvex-nonconcave minimax problems. arXiv preprint
arXiv:2002.09621, 2020.
Junchi Yang, Xiang Li, and Niao He. Nest your adaptive algorithm for parameter-agnostic
nonconvex minimax optimization. Advances in Neural Information Processing Systems,
35:11202–11216, 2022.
Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhiquan Luo. A single-loop smoothed gradient
descent-ascent algorithm for nonconvex-concave min-max problems. Advances in neural
information processing systems, 33:7377–7389, 2020.
Siqi Zhang, Junchi Yang, Crist´obal Guzm´an, Negar Kiyavash, and Niao He. The complex-
ity of nonconvex-strongly-concave minimax optimization. In Uncertainty in Artificial
Intelligence, pages 482–492. PMLR, 2021.
Xuan Zhang, Qiushui Xu, and Necdet Serhat Aybat. Agda+: Proximal alternating gradi-
ent descent ascent method with a nonmonotone adaptive step-size search for nonconvex
minimax problems. arXiv preprint arXiv:2406.14371, 2024.
23