Transient anisotropic kernel for probabilistic learning on manifolds
ChristianSoizea,∗,RogerGhanemb
aUniversite´GustaveEiffel,MSMEUMR8208,5bdDescartes,77454Marne-la-Valle´e,France
bUniversityofSouthernCalifornia,210KAPHall,LosAngeles,CA90089,UnitedStates
Abstract
PLoM(ProbabilisticLearningonManifolds)isamethodintroducedin2016forhandlingsmalltrainingdatasetsby
projectinganItoˆequationfromastochasticdissipativeHamiltoniandynamicalsystem,actingastheMCMCgenerator,
forwhichtheKDE-estimatedprobabilitymeasurewiththetrainingdatasetistheinvariantmeasure. PLoMperforms
aprojectiononareduced-ordervectorbasisrelatedtothetrainingdataset,usingthediffusionmaps(DMAPS)basis
constructedwithatime-independentisotropickernel. Inthispaper, weproposeanewISDEprojectionvectorbasis
builtfromatransientanisotropickernel,providinganalternativetotheDMAPSbasistoimprovestatisticalsurrogates
for stochastic manifolds with heterogeneous data. The construction ensures that for times near the initial time, the
DMAPS basis coincides with the transient basis. For larger times, the differences between the two bases are char-
acterizedbytheangleoftheirspannedvectorsubspaces. Theoptimalinstantyieldingtheoptimaltransientbasisis
determinedusinganestimationofmutualinformationfromInformationTheory,whichisnormalizedbytheentropy
estimation to account for the effects of the number of realizations used in the estimations. Consequently, this new
vectorbasisbetterrepresentsstatisticaldependenciesinthelearnedprobabilitymeasureforanydimension. Threeap-
plicationswithvaryinglevelsofstatisticalcomplexityanddataheterogeneityvalidatetheproposedtheory,showing
thatthetransientanisotropickernelimprovesthelearnedprobabilitymeasure.
Keywords: Transientkernel,probabilisticlearning,PLoM,diffusionmaps,Fokker-Planckoperator,spectrum
1. Introduction
1.1. Objectivesofthepaper
PLoM (Probabilistic Learning on Manifolds), introduced in 2016 [1], is a method and algorithm specifically
developed for cases where the training dataset consists of a small number of data points. This method is based on
projectinganItoˆ equationassociatedwithastochasticdissipativeHamiltoniandynamicalsystem, whichactsasthe
MCMCgeneratorfromtheprobabilitymeasureestimatedusingtheKDEmethodappliedtothepointsofthetraining
dataset. The projection basis is the diffusion maps (DMAPS) basis associated with a time-independent isotropic
kernel,introducedin[2,3].
Since2016, allextensionsandapplicationsofPLoM(seeSection1.2)havebeencarriedoutusingtheisotropic
kernel.Throughtheseapplications,wehaveseenthattheisotropickernelallowsforobtainingqualityresults,evenfor
heterogeneousdataandsystemsofgreatstatisticalcomplexityinsmallandlargedimensions.However,improvingthe
constructionofstatisticalsurrogatesforstochasticmanifoldsinvolvingconditionalstatisticsandveryheterogeneous
data using PLoM based on a transient anisotropic kernel (time-dependent) was an analysis project. In this paper,
weaddressthisproblem. WeproposeanewconstructionoftheISDEprojectionvectorbasis,builtfromatransient
anisotropickernel, whichimprovestherepresentationofthestatisticaldependenciesofthelearnedjointprobability
measureinanydimension.
∗Correspondingauthor:C.Soize,christian.soize@univ-eiffel.fr
Emailaddresses:christian.soize@univ-eiffel.fr(ChristianSoize),ghanem@usc.edu(RogerGhanem)
PreprintsubmittedtoArXiv August1,2024
4202
luJ
13
]LM.tats[
1v53412.7042:viXra1.2. Frameworkoftheconsideredproblem
Machine learning tools and artificial intelligence [4, 5, 6, 7], such as probabilistic and statistical learning [8,
9, 10, 11, 12], are used in UQ for problems that would require computer resources not available with the most
usual approaches. Thus, methods have emerged in the field of engineering sciences, such as learning on manifolds
[13,1,14,15,16]andphysics-informedprobabilisticlearning[17,18,19].
Probabilistic learning is a very active domain of research for constructing surrogate models (see for instance,
[20,13,21,22,23,15,17]). ProbabilisticLearningonManifolds(PLoM)isatoolincomputationalstatistics,intro-
ducedin2016[1],whichcanbeviewedasatoolforscientificmachinelearning. ThePLoMapproachhasspecifically
beendevelopedfor smalldatasetcases[1,24, 25,26,27]. Themethodavoidsthe scatteringoflearnedrealizations
associated with the probability distribution to preserve its concentration in the neighborhood of the random mani-
folddefinedbytheparameterizedcomputationalmodel. hismethodallowsforsolvingunsupervisedandsupervised
problemsunderuncertaintywhenthetrainingdatasetsaresmall. Thissituationisencounteredinmanyproblemsin
physicsandengineeringsciencewithexpensivefunctionevaluations.Theexplorationoftheadmissiblesolutionspace
inthesesituationsisthushamperedbyavailablecomputationalresources.
Severalextensionshavebeenproposedtoaccountforimplicitconstraintsinducedbyphysics,computationalmod-
els,andmeasurements[18,19,28],toreducethestochasticdimensionusingastatisticalpartitionapproach[29],and
toupdatethepriorprobabilitydistributionwithatargetdataset,whosepointsare,forinstance,experimentalrealiza-
tions of the system observations [30]. Consequently, PLoM, constrained by a stochastic computational model and
statistical moments or samples/realizations, allows for performing probabilistic learning inference and constructing
predictivestatisticalsurrogatemodelsforlargeparameterizedstochasticcomputationalmodels.
ThislastcapabilityofPLoMcanalsobeviewedasanalternativemethodtoBayesianinferenceforhighdimen-
sions[31,32,33,34,35,36,37,38,39,40]andisacomplementaryapproachtoexistingmethodsinmachinelearning
forsamplingdistributionsonmanifoldsunderconstraints. AlthoughaBayesianinferencemethodologyhasalsobeen
developedusingprobabilisticlearningonmanifoldsforhighdimensions[26].
PLoMhassuccessfullybeenadaptedtotacklethesechallengesforseveralrelatedproblems,includingnonconvex
optimizationunderuncertainty[41,42,43,44,45,46,47,48],fracturepathsinrandomcomposites[49],concurrent
multiscale simulations in random media [50], stochastic homogenization in random elastic media [51], ultrasonic
transmissiontechniquesincorticalbonemicrostructures[26],updatingdigitaltwinsunderuncertainties[52],updating
under-observeddynamicalsystems[53,54],calculationoftheSobolindices[55],dynamicmonitoring[56],surrogate
modeling of structural seismic response [57], probabilistic-learning-based stochastic surrogate models from small
incomplete datasets [58, 59], and polynomial-chaos-based conditional statistics for probabilistic learning of atomic
collisions[60],aswellasforaeroacousticlinerimpedancemetamodelsfromsimulationandexperimentaldata[61].
1.3. Methodologyproposedandorganizationofthepaper
Starting with a training dataset of n realizations of ν random variables, we consider the probability flow from
d
eachofthesen realizationstowardstheν-dimensionalsamplingprobabilitydistributionofthetrainingdataset. We
d
construct the associated coupled Fokker-Planck (FKP) equations with each of the realizations as initial condition.
Thesedescribetheevolutionofthetransitionprobabilitymeasuresoverthegraphdescribedbythetrainingdataset,that
transporteachindependentrealization,viewedasaconcentratedmeasureattheinitialtime,intothejointprobability
measure,consistingofthecommonstationaryprobabilitymeasure(alsocalledthesteady-statesolution)oftheFKP
equations. This evolution describes a trajectory along which transition probabilities are consistent with both the
training dataset and its postulated joint probability density function (that we approximate using a Gaussian Kernel
Density Estimate (KDE)). Each of these probabilities provides a distinct geometric characterization of the training
dataset,withitsownplausiblemodelofstatisticaldependence,resultinginatransientanisotropickernelfromwhich
weconstructatimeevolvingPLoM.WethensetcriteriaforselectingamongthesePLoMmodels,whichistantamount
toidentifyingthemostappropriatestatisticaldependencestructureforthelearneddataset,alongtheflowcharacterized
by the FKP equations. The following description of the paper’s organization provides a coherent summary of the
proposedmethodology.
InSection2,wedefinethetrainingdatasetconstitutedofgivenrealizationsofanon-Gaussiannormalizedvector-
valued random variable H (centered and with an identity covariance matrix) and define the associated probability
measureP ,whosedensityisestimatedusingtheGaussianKernelDensityEstimation(GKDE)method.
H
2Section3dealswithashortsummaryofformalresults,introducinganItoˆ stochasticdifferentialequation(ISDE)
andthederivedFokker-Planck(FPK)equationforwhichP istheinvariantmeasure,whichisthesteady-statesolu-
H
tion. We then introduce a formal formulation of the eigenvalue problem of the FPK operator and the nonstationary
solutionoftheFokker-Planckequationwithadeterministicinitialcondition.
Section 4 is devoted to the time-dependent kernel k derived from the time-dependent solution of the Fokker-
t
Planckequationwithadeterministicinitialcondition. Wethendefinethetime-dependentoperatorK associatedwith
t
the transient kernel k, which is a Hilbert-Schmidt operator. We propose a construction of its finite approximation,
t
represented by a matrix [K(cid:98)(t)], using a sampling of P of the bilinear form associated with K, and introduce the
H t
correspondingfiniteapproximationoftheeigenvalueproblem.
InSection5,wepresentthedirectconstructionofmatrix[K(cid:98)(t)]. Weintroducethetransitionprobabilitydensity
function as the solution of the ISDE with a deterministic initial condition and study its existence, uniqueness, and
properties. WethenrewritetheISDEinmatrixformandproposeatime-discreteapproximationofthismatrix-valued
ISDE based on an Euler scheme. We introduce convergence criteria to verify convergence. Finally, we construct
anexplicitrepresentationofthematrix[K(cid:98)(t)]basedonthenonstationarysolutionofthematrix-valuedISDEwitha
deterministicinitialcondition.
AnumericalillustrationoftheproposedformulationisgiveninSection6,forwhichanexplicitsolutionisknown.
Section7dealswiththeconstructionandstudyofthevectorbasisforPLoMderivedfromthetransientanisotropic
kernel,connectedtotheDMAPSbasisconstructedwiththeisotropickernel. Weconstructatime-dependentmatrix
[K˜(n∆t)]ofthetransientanisotropickernelderivedfromthematrix[Kˆ(n∆t)],whosefundamentalpropertyisitscon-
vergencetothematrix[K ]oftheDMAPSisotropickernelastimeapproacheszero. Thetime-dependentreduced-
DM
ordertransientbasisisthentheeigenvectorsof[K˜(n∆t)]associatedwithitsdominantpositiveeigenvalues. Inorderto
qualifyandquantifythegainoftheconstructedreduced-ordertransientbasis,ROTB(n∆t)ataninstantsampledn∆t,
withrespecttothereduced-orderDMAPSbasis, RODB,weintroducetheanglebetweenthesubspacesspannedby
ROTB(n∆t)andRODB.Weproposeamethodologyforidentifyingtheoptimalinstantsampled,whichmaximizesa
selectioncriterion. ThiscriterionisbasedontheestimationofmutualinformationfromInformationTheory,normal-
izedusingentropyestimationtoaccountfortheeffectsofthenumberofrealizationsusedinthestatisticalestimators.
Suchacriterionallowsforselectingthebestjointlearnedprobabilitymeasurewithrespecttostatisticaldependencies.
InSection8,wepresentthreeapplications,eachwithaspecificlevelofcomplexityanddataheterogeneityinthe
trainingdataset. Thefirstapplicationiscreatedsuchthattheprobabilitymeasureof HinR9,definedbythepointsof
thetrainingdataset,isconcentratedinamulticonnecteddomainofR9. Theconstituentconnectedpartsaremanifolds
ofdimensionsmuchlowerthan9,eachwithdifferentdimensions. Thesepartsmayormaynotbeconnectedtoeach
other. The training dataset of the second application consists of realizations of the random vector H with values in
R8,generatedusingapolynomialchaosexpansionofdegree6ofareal-valuedrandomvariable,whoserandomgerm
has a dimension of 2, with each of the two random germs being a uniform random variable with different support.
Therearetherefore28termsinthisexpansion,andthe8componentsof Haredefinedastherandomtermsofgiven
rank,definingarelativelycomplexrandommanifoldinR8.Thethirdapplicationresultsfromastatisticaltreatmentof
anexperimentaldatabasecontainingphotonmeasurementsintheATLASdetectoratCERN.ThePCAstepofPLoM
hasbeenperformed,and45componentshavebeenextractedtoobtainthetrainingdatasetfortheR45-valuedrandom
variableH. Thisapplicationisinhigherdimensionthanthefirsttwobuthaslessstatisticalcomplexity.
The paper is completed by two appendices. Appendix A presents an overview of the probabilistic learning
on manifolds (PLoM) algorithm and its parameterization, using either the DMAPS basis (RODB) or the transient
basis (ROTB(n∆t)) as the projection basis. Appendix B provides the formulas for estimating the Kullback-Leibler
divergence,mutualinformation,andentropyfromasetofrealizations.
It should be noted that there are some very brief repetitions, which have been deliberately made to facilitate
reading.
1.4. Conventionforthevariables,vectors,andmatrices
x,η: lower-caseLatinorGreeklettersaredeterministicrealvariables.
x,η: boldfacelower-caseLatinorGreeklettersaredeterministicvectors.
X: upper-caseLatinlettersarereal-valuedrandomvariables.
X: boldfaceupper-caseLatinlettersarevector-valuedrandomvariables.
3[x]: lower-caseLatinlettersbetweenbracketsaredeterministicmatrices.
[X]: boldfaceupper-caselettersbetweenbracketsarematrix-valuedrandomvariables.
1.5. Algebraicnotations
N,N∗: setofnaturalnumbersincluding0,excluding0.
R,R+,R+∗: setofrealnumbers,subset[0,+∞[,subset]0,+∞[.
Rν: Euclideanvectorspaceofdimensionν.
M ,M : setofthe(n×m),(n×n),realmatrices.
n,m n
M+,M+0: setofthepositive-definite,positive,(n×n)realmatrices.
n n
[I ]: identitymatrixinM .
n n
∥[x]∥ : Frobeniusnormofmatrix[x].
F
∇,div: gradientanddivergenceoperatorsinRν.
δ : DiracmeasureonRν atpoint0 .
0 ν
1.6. Conventionusedforrandomvariables
In this paper, for any finite integer m ≥ 1, the Euclidean space Rm is equipped with the σ-algebra B . If Y
Rm
is a Rm-valued random variable defined on the probability space (Θ,T,P), Y is a mapping θ (cid:55)→ Y(θ) from Θ into
Rm, measurable from (Θ,T) into (Rm,B ), and Y(θ) is a realization (sample) of Y for θ ∈ Θ. The probability
Rm
distributionofY istheprobabilitymeasureP (dy)onthemeasurableset(Rm,B )(wewillsimplysayonRm). The
Y Rm
LebesguemeasureonRm isnoteddyandwhen P (dy)iswrittenas p (y)dy, p istheprobabilitydensityfunction
Y Y Y
(pdf)onRm of P (dy)withrespecttody. Finally, E denotesthemathematicalexpectationoperatorthatissuchthat
(cid:82) Y
E{Y}= yP (dy).
Rm Y
2. Definingtheprobabilitymeasure P ofrandomvectorH
H
Let D (η) = {ηj, j = 1,...,n } be the set of n > 1 independent realizations ηj ∈ Rν, with ν ≥ 1, of a
train d d
second-orderRν-valuedrandomvariabledefinedonaprobabilityspace(Θ,T,P). Letη ∈Rν and[C ]∈M bethe
d
d nd
associatedempiricalestimatesofthemeanvalueandthecovariancematrixconstructedwiththepointsofD (η),
train
1
(cid:88)nd
1
(cid:88)nd
η = ηj , [C ]= (ηj−η )⊗(ηj−η ). (2.1)
d n d n −1 d d
d j=1 d j=1
ItisassumedthatD (η)issuchthat
train
η =0 , [C ]=[I ]. (2.2)
ν d ν
d
Letη(cid:55)→ p (η)betheprobabilitydensityfunctiononRν,withrespectdotheLebesguemeasuredη,definedby
H
1
(cid:88)nd
1
(cid:32)
1 sˆ
(cid:33)
p (η)= √ exp − ∥η− ηj∥2 , ∀η∈Rν, (2.3)
H n d j=1 ( 2πsˆ)ν 2sˆ2 s
wheresˆandsaredefinedby
(cid:32)
4
(cid:33)1/(ν+4)
s
s= n d(2+ν) , sˆ= (cid:112) s2+(n d−1)/n
d
. (2.4)
Eqs.(2.3)and(2.4)correspondtotheGaussiankernel-densityestimation(KDE)constructedusingthen independent
d
realizations of D (η) involving the modification [62] of the usual formulation [63, 33, 64], in which s is the Sil-
train
vermanbandwidth. Let Hbethesecond-orderRν-valuedrandomvariable,definedonaprobabilityspace(Θ,T,P),
whose probability measure P (dη) = p (η)dη on Rν is defined by the probability density function p given by
H H H
Eq.(2.3). Itcanbeseenthat,foranyfixedn >1,wehave
d
(cid:90)
1
E{H}= ηP (dη)= η =0 , (2.5)
H 2sˆ2 d ν
Rν
4(cid:90)
sˆ2 (n −1)
E{H⊗H}= η⊗ηP (dη)= sˆ2[I ]+ d [C ]=[I ]. (2.6)
H ν s2 n d ν
Rν d
Eqs. (2.5) and (2.6) show that H is a normalized Rν-valued random variable. The probability density function p
H
definedbyEq.(2.3)isrewritten,forallηinRν,as
p (η)=c ξ(η) , ξ(η)=e−Φ(η), (2.7)
H ν
√
inwhichc =( 2πsˆ)−ν andwhereΦ(η)=−log(ξ(η))issuchthat
ν
Φ(η)=−log n1 (cid:88)nd exp(cid:32)
−
21
sˆ2∥η−
s sˆ ηj∥2(cid:33) 
, ∀η∈Rν. (2.8)
d j=1
3. Shortsummaryofformalresults
Thissectionislimitedtoasummaryofessentialresults,whichwillbeusedinSection4andwhichareformally
presented.
3.1. Itoˆ stochasticdifferentialequationrelatedtoP
H
WeintroduceanItoˆ stochasticdifferentialequation(ISDE)onRν,withinitialcondition,forwhichP (dη)isthe
H
invariantmeasure. AclassicalcandidatetosuchanISDEiswrittenas
dY(t)=b(Y(t))dt+dW(t) , t>0, (3.1)
Y(0)=x∈Rν, a.s., (3.2)
wherethedriftvectoristhefunctiony(cid:55)→ b(y)fromRν intoRν definedby
1
b(y)=− ∇Φ(y) , ∀y∈Rν. (3.3)
2
In Eq. (3.1), W(t) = (W (t),...,W (t)) is the normalized Wiener stochastic process [65] on R+, with values in Rν,
1 ν
which is a stochastic process with independent increments, such thatW(0) = 0 a.s, and for 0 ≤ τ < t < +∞, the
ν
increment∆W =W(t)−W(τ)isaGaussianRν-valuedsecond-orderrandomvariable,centeredandwithacovariance
τt
matrixthatiswrittenas
[C∆Wτt]= E{∆W τt⊗∆W τt}=(t−τ)[I ν]. (3.4)
ItshouldbenotedthatEqs.(3.1)and(3.2)isequivalentto
(cid:90) t (cid:90) t
Y(t)= x+ b(Y(τ))dτ+ dW(τ) , t≥0. (3.5)
0 0
In Section 5, we will see that {Y(t),t ∈ R+} is a homogeneous diffusion stochastic process, which is asymptotically
stationaryfort→+∞. AssumingthatthetransitionprobabilitymeasureofY(t)givenY(0)= xadmitsadensitywith
respecttody,suchthat,forallt>0,forallxandyinRν,andforanyBorelianBinRν,wehave
(cid:90)
P{Y(t)∈B|Y(0)= x}= ρ(y,t|x,0)dy, (3.6)
B
lim ρ(y,t|x,0)dy=δ (y−x), (3.7)
0
t→0+
(cid:90)
ρ(y,t|x,0)dy=1. (3.8)
Rν
53.2. FKPequationassociatedwiththeISDE
Forall xinRν, thetransitionprobabilitydensityfunction(y,t) (cid:55)→ ρ(y,t|x,0)fromRν ×R+ intoR+ verifiesthe
followingFokker-Planck(FKP)equation(seeforinstance[66,67,68]),
∂ρ
+L (ρ)=0 , t>0, (3.9)
∂t FKP
withtheinitialconditionfort=0definedbyEq.(3.7). TheFokker-PlanckoperatorL canbewritten,afterasmall
FKP
algebraicmanipulationandforanysufficientlydifferentiablefunctiony(cid:55)→v(y)fromRν intoR,as
(cid:40) (cid:32) (cid:33)(cid:41)
1 v(y)
{L FKP(v)}(y)=− 2div p H(y)∇
p (y)
. (3.10)
H
Thedetailedbalance(theprobabilitycurrentvanishes)issatisfiedandthesteadystatesolutionofEq.(3.9)isthepdf
p definedbyEq.(2.3)[68,69,70]. Wethenhave,forv= p ,
H H
L (p )=0. (3.11)
FKP H
3.3. ReturntotheinvariantmeasureP
H
Theinvariantmeasure(seeProposition5)P (dy)= p (y)dyissuchthat,forallyinRν andforallt≥0,
H H
(cid:90)
p (y)= ρ(y,t|x,0)p (x)dx. (3.12)
H H
Rν
TheISDEdefinedbyEqs.(3.1)and(3.2)admitsanasymptotic(t →+∞)stationarysolutionwhosemarginalproba-
bilitydensityfunctionoforderoneis p . Consequently,forallxandyinRν,wehave
H
lim ρ(y,t|x,0)= p (y). (3.13)
H
t→+∞
3.4. FormalformulationoftheeigenvalueproblemoftheFKPoperator
Theeigenvalueproblem,posedinanadaptedfunctionalspace,iswrittenas
L (v)=λv, (3.14)
FKP
forwhichthecurrentmustvanishatinfinity,yieldingthecondition,
lim p (y)∥∇(p (y)−1v(y))∥=0. (3.15)
H H
∥y∥→+∞
Continuingthedevelopmentwithinaformalframework,suchasthatusedin[70],weintroducethechangeoffunction,
v(y)= p (y)1/2q(y) , y∈Rν , q:Rν →R. (3.16)
H
LetLˆ bethelinearoperatordefined,forqbelongingtoanadmissiblesetoffunctions,
FKP
{Lˆ (q)}(y)= p (y)−1/2L (p (y)1/2q(y)) , y∈Rν. (3.17)
FKP H FKP H
Therefore,theeigenvalueproblemdefinedbyEqs.(3.14)and(3.15)canberewritteninqas
Lˆ (q)=λq, (3.18)
FKP
withtheconditionatinfinity,
lim p (y)1/2∥∇(p (y)−1/2q(y))∥=0. (3.19)
H H
∥y∥→+∞
Remark1(AnotheralgebraicrepresentationofoperatorLˆ ). UsingEq.(2.7),whichshowsthatp (y)−1∇p (y)=
FKP H H
−∇Φ,andusingEqs.(3.10)and(3.17),itcanbeseenthat
1
{Lˆ (q)}(y)=V(y)q(y)− ∇2q(y) , y∈Rν, (3.20)
FKP 2
inwhich∇2istheLaplacianoperatorinRν andwherey(cid:55)→V(y)isthefunctionfromRν intoR,whichisdefined,for
allyinRν,as
1 1
V(y)= ∥∇Φ(y)∥2− ∇2Φ(y). (3.21)
8 4
63.5. PropertiesofoperatorLˆ
FKP
LetδqbeafunctionfromRν intoR,belongingtotheadmissiblesetthatallowstheevaluationofthebracket
(cid:90)
⟨Lˆ (q),δq⟩= {Lˆ (q)}(y) δq(y)dy.
FKP FKP
Rν
RemovingyandusingEq.(3.17)withEq.(3.10)yields
(cid:90)
1
⟨Lˆ FKP(q),δq⟩=−
2
(p− H1/2δq)div{p H∇(p− H1/2q)}dy. (3.22)
Rν
Usingtheconditionatinfinity,definedbyEq.(3.19),Eq.(3.22)canberewrittenas,
(cid:90)
1
⟨Lˆ (q),δq⟩= p ⟨∇(p−1/2q),∇(p−1/2δq)⟩ dy. (3.23)
FKP 2 H H H Rν
Rν
(a)Eq.(3.23)showsthatLˆ isasymmetricandpositiveoperator.
FKP
(b)Eqs.(3.11)and(3.17)showthat
Lˆ (q )=0 for q = p1/2. (3.24)
FKP 0 0 H
InProposition5,itwillbeproventhattheISDEdefinedbyEq.(3.1),withtheinitialconditiondefinedby(3.2),hasa
uniquesolutionandauniqueinvariantmeasure p (y)dy. Consequentlythedimensionofthenullspaceofoperator
H
L is1. Sincep (y)dyisaboundedpositivemeasure(probabilitymeasure),theright-handsideofEq.(3.23)shows
FKP H
thatthenullspaceofLˆ ,whichisalsoofdimension1,isconstitutedofthefunctionq = p1/2. Forq=δq̸=q ,and
FKP 0 H 0
∥q ∥≠ 0,wehave⟨Lˆ (q),q⟩ >0. Therefore,Lˆ isapositiveoperator(inthequotientspacebythenullspace).
0 FKP FKP
Hypothesis1(OnthespectrumofoperatorLˆ ). Itisassumedthat p definedbyEq.(2.3),whichisconstructed
FKP H
withthen points{ηj, j=1,...,n }ofthetrainingdataset,issuchthatthespectrumofLˆ iscountable. Dueto(a)
d d FKP
and(b),wethendeducethattheeigenvaluesofLˆ (definedbyEqs.(3.18)and(3.19))arepositiveexceptonethatis
FKP
zero. Wewillalsoassumethatthemultiplicityofeacheigenvalueisfinite.
3.6. EigenvalueproblemforoperatorLˆ
FKP
Under Hypothesis 1, the eigenvalue problem Lˆ (q ) = λ q for operator Lˆ , with the condition defined by
FKP α α α FKP
Eq.(3.19),issuchthat
0=λ <λ ≤λ ≤... , (3.25)
0 1 2
themultiplicityofeacheigenvaluebeingfinite. Wewilladmitthatthefamily{q ,α ∈ N}oftheeigenfunctionsisa
α
HilbertbasisofL2(Rν). Wethenhave
(cid:90)
⟨q ,q ⟩ = q (y)q (y)dy=δ . (3.26)
α β L2 α β αβ
Rν
Theeigenfunctionq associatedwithλ =0,issuchthat(seeEq.(3.24)),
0 0
q = p1/2 , ∥q ∥ =1, (3.27)
0 H 0 L2
andwehave
(cid:88)
q (y)q (x)dy=δ (y−x). (3.28)
α α 0
α∈N
FromEqs.(3.26)and(3.27),itcanbededucedthat
(cid:90)
∀α≥1 , p (y)1/2q (y)dy=0. (3.29)
H α
Rν
73.7. NonstationarysolutionoftheFokker-Planckequationwithinitialcondition
Thetransitionprobabilitydensityfunctionρ(y,t|x,0)introducedinSection3.1andsatisfyingEq.(3.9)withthe
initialconditiondefinedbyEq.(3.7),canbewritten,usingtheHilbertbasis{q ,α∈N}definedinSection3.6,as
α
(cid:88)
ρ(y,t|x,0)= p H(y)1/2p H(x)−1/2 e−λαtq α(y)q α(x) , t>0. (3.30)
α∈N
Thisrepresentationofρ(y,t|x,0),definedbyEq.(3.30),actuallysatisfiedalltherequiredproperties:
(cid:82)
Eqs.(3.25),(3.27),and(3.29)yield ρ(y,t|x,0)dy=1.
Rν
Eqs.(3.25)and(3.27)yieldlim t→+∞ρ(y,t|x,0)dy= p H(y).
Eq.(3.28)yieldslim ρ(y,t|x,0)dy=δ (y−x).
t→0+ (cid:82) 0
Eqs.(3.27)and(3.29)yield ρ(y,t|x,0)p (x)dx= p (y)thatisEq.(3.12).
Rν H H
4. Time-dependentkernel,itsassociatedoperator,andfiniteapproximation
Inthissection, wedefinethekernelk(y,x)andgiveitsbasicpropertiesdirectlydeducedfromthepropertiesof
t
ρ(y,t|x,0)andp (y),withoutusingthespectralrepresentationdefinedbyEq.(3.30).Fromthespectralrepresentation
H
ofk(y,x),wededuceitsspectralrepresentationusingthespectralrepresentationofρ(y,t|x,0),definedbyEq.(3.30).
t
Finally,wedefinethelinearoperatorK associatedwithkernelk andwegivethespectralrepresentationofoperator
t t
K.
t
4.1. Definitionofthekernelk anditsbasicprobabilisticproperties
t
Thekernelk(y,x)associatedwiththetransitionprobabilitydensityfunctionρ(y,t|x,0)isdefinedasfollows.
t
Definition1(Kernelk onRν×Rν). Forevery fixedt > 0, thekernel function(y,x) (cid:55)→ k(y,x), from Rν ×Rν into
t t
R+,isdefinedby
ρ(y,t|x,0)
k(y,x)= . (4.1)
t p (y)
H
ThefollowingLemmagivesbasicpropertiesofkernelk.
t
Lemma1(Propertiesofkernelk). Foreveryfixedt>0,andforallyandxinRν,wehavethefollowingproperties:
t
(cid:90) (cid:90)
(a) k(y,x)p (y)dy=1 , k(y,x)p (x)dx=1, (4.2)
t H t H
Rν Rν
(cid:90) (cid:90)
(b) k(y,x)p (y)p (x)dydx=1, (4.3)
t H H
Rν Rν
(c) limk(y,x)p (y)dy=δ (y−x) , lim k(y,x)=1, (4.4)
t→0+ t H 0 t→+∞ t
(d) (y,x)(cid:55)→ρ(y,t|x,0)∈C0(Rν×Rν)⇒(y,x)(cid:55)→k(y,x)∈C0(Rν×Rν), (4.5)
t
(e) k(y,x)=k(x,y). (4.6)
t t
PROOF. (Lemma1). Definition1isused.
(a)ThefirstequationinEq.(4.2)isduetoEq.(3.8)andthesecondoneisduetoEq.(3.12).
(b)Eq.(4.3)isdirectlydeducedfromEq.(4.2).
(c)ThefirstequationinEq.(4.4)isduetoEq.(3.7)andthesecondoneisduetoEq.(3.13).
(d)ForallyinRν, p (y)>0and p ∈C0(Rν). Thehypothesisρ(·,t|·,0)∈C0(Rν×Rν)yieldsEq.(4.5).
H H
(e)let p (y,t;x,0)bethejointpdfofY(t)withHinwhichY(t)isthesolutionofEq.(3.1)forfixedt>0,withthe
Y(t),H
randominitialconditionY(0) = H. Wehavetheclassicalpropertyrelatedtothedefinitionoftheinvariantmeasure,
(cid:82) (cid:82)
p (y,t)= p (y,t;x,0)dx= ρ(y,t|x,0)p (x)dx= p (y). Forall yand xinRν,andfort >0,wehave,
Y(t) Rν Y(t),H Rν H H
ρ(y,t|x,0)p (x) = p (y,t;x,0) = p (x,0;y,t) = ρ(x,0|y,t)p (y,t) = ρ(x,0|y,t)p (y). Consequently,
H Y(t),H H,Y(t) Y(t) H
k(y,x)=ρ(y,t|x,0)p (x)/(p (y)p (x))=ρ(x,0|y,t)p (y)/(p (y)p (x))=k(x,y).
t H H H H H H t
84.2. Hypothesisandpropertiesofkernelk fromitsrepresentation
t
InSection3,weintroducedanhypothesisofexistenceofadiscrete(countable)spectrum{λ ,α ∈ N}oftheFKP
α
operator Lˆ . Inthissection, westudythespectralrepresentationofkernelk, foreveryfixedt > 0, deducedfrom
FKP t
thetime-dependentspectralrepresentationofρ(y,t|x,0),definedbyEq.(3.30).
Definition2(HilbertspaceH= L2(Rν;p )). LetH= L2(Rν;p )betheHilbertspaceofthesquare-integrablereal-
H H
valuedfunctionsonRν,withrespecttotheprobabilitymeasure p (y)dyonRν,equippedwiththeinnerproductand
H
theassociatednorm,
(cid:90)
⟨u,v⟩ = u(y)v(y)p (y)dy , ∥u∥ =⟨u,u⟩1/2. (4.7)
H H H H
Rν
Lemma2(HilbertbasisinH). Let {q ,α ∈ N} be the Hilbert basis of L2(Rν) introduced in Section 3.6. For all
α
α∈N,wedefinedthereal-valuedfunctionψ onRν suchthat
α
ψ (y)=q (y)p (y)−1/2 , ∀y∈Rν. (4.8)
α α H
Then,{ψ ,α∈N}isaHilbertbasisofHandwehave,
α
(a) ψ ∈H , ∥ψ ∥ =∥q ∥ =1 , ∀α∈N, (4.9)
α α H α L2
(cid:90)
(b) ⟨ψ ,ψ ⟩ = ψ (y)ψ (y)p (y)dy=δ , ∀(α,β)∈N×N, (4.10)
α β H α β H αβ
Rν
(c) ψ (y)=1, ∀y∈Rν , ∥ψ ∥ =1, (4.11)
0 0 H
(cid:90)
(d) ψ (y)p (y)dy=0 , ∀α∈N∗, (4.12)
α H
Rν
(cid:88)
(e) ψ (y)ψ (x)p (y)dy=δ (y−x) , ∀(y,x)∈Rν×Rν. (4.13)
α β H 0
α∈N
PROOF. (Lemma2).
(cid:82)
(a) Since p−1/2 ∈ C0(Rν), Eqs. (4.8) yields Eq. (4.9); combined with Eq. (3.29), this yields ∥ψ ∥ = q (y)2dy =
H α H Rν α
∥q ∥ =1.
α L2 (cid:82)
(b) Using Eq. (3.26) yields ⟨ϕ ,ψ ⟩ = q (y)q (y)dy = ⟨q ,q ⟩ = δ . Thus {ψ ,α ∈ N} is an orthonormal
α β H Rν α β α β L2 αβ α
familyinH. ForalluinL2(Rν),thelinearmappingu(cid:55)→v=up−1/2isacontinuousinjectionfromL2(Rν)intoHwith
H
∥v∥ =∥u∥ . Therefore,{ψ ,α∈N}isaHilbertbasisofH.
H L2 α
(c)ThetwoequationsinEq.(4.11)aredirectlydeducedfromEqs.(3.27)and(4.8).
(d)Since⟨ψ ,ψ ⟩ =0forallα∈N∗,weobtainEq.(4.12).
0 α H
(e)Since{ψ ,α∈N}isaHilbertbasisofH,Eq.(4.13)holds.
α
Proposition1(Spectralrepresentationofkernelk). Let{ψ ,α∈N}betheHilbertbasisofHdefinedinLemma2.
t α
(a)Foreveryfixedt>0,thesymmetrickernelk canbewritten,forallyandxinRν,as
t
(cid:88)
k(y,x)= b (t)ψ (y)ψ (x), (4.14)
t α α α
α∈N
inwhichthefamilyofpositiverealnumbers{b (t)=exp(−λ t),α∈N},issuchthat
α α
1=b (t)>b (t)≥b (t)≥... . (4.15)
0 1 2
(b)Ifforeveryfixedt>0,kernelk satisfies
t
(cid:90) (cid:90)
k(y,x)2p (y)p (x)dydx=c2 <+∞, (4.16)
t H H t
Rν Rν
wherec >1isapositiveconstantdependingont,then,
t
(cid:88)
b (t)2 =c2 <+∞. (4.17)
α t
α∈N
9PROOF. (Proposition1).
(a)SubstitutingEq.(3.30)withq (y)=ψ (y)p (y)1/2(seeEq.(4.8))intoEq.(4.1)yieldsEq.(4.14).FromEq.(3.25)
α α H
andsinceb (t)=exp(−λ t),weobtainEq.(4.15).
α α
(b)AssumingEq.(4.16),substitutingEq.(4.14)intoEq.(4.16),andusingEq.(4.10)yieldsEq.(4.17).Sinceb (t)=1,
0
itcanbededucedthatc2 >1andthus,c >1.
t t
4.3. Hilbert-SchmidtoperatorK associatedwithkernelk
t t
WenowintroducethelinearoperatorinH,definedbykernelk,andwestudyitspropertiesandspectrum.
t
Definition3(OperatorK associatedwithkernelk). Foreveryfixedt > 0,wedefinedthelinearoperatorK from
t t t
HintoHsuchthat,foralluandvinH,
(cid:90) (cid:90)
⟨K u,v⟩ = k(y,x)u(x)v(y)p (y)p (x)dydx, (4.18)
t H t H H
Rν Rν
wherethesymmetrickernelk verifiestheconditiondefinedbyEq.(4.16).
t
Proposition2(K asaHilbert-SchmidtoperatorinH). Foreveryfixedt >0,letK bethecontinuouslinearoper-
t t
atordefinedbyEq.(4.18),inwhichkernelk issymmetriconRν×Rν,andverifiesEq.(4.16).
t
(a)ForalluandvinH,operatorK issuchthat
t
(cid:88)
⟨K u,v⟩ = b (t)⟨u,ψ ⟩ ⟨v,ψ ⟩ , (4.19)
t H α α H α H
α∈N
andisapositivesymmetricoperatorinH. ForalluinH,
(cid:88)
K u= b (t)⟨u,ψ ⟩ ψ . (4.20)
t α α H α
α∈N
(b)ForallαinN,ψ ∈Histheeigenfunctionindependentoft,associatedwiththepositiveeigenvalueb (t),satisfying
α α
Eq.(4.15),ofoperatorK,
t
K ψ =b (t)ψ , b (t)=exp(−λ t) , α∈N, (4.21)
t α α α α α
whichshowsthat,forallαandβinN,
⟨K ψ ,ψ ⟩ =b (t)δ . (4.22)
t α β H α αβ
(c)ForalluinH,wehave
(cid:88)
∥K u∥2= b (t)2⟨u,ψ ⟩2 , (4.23)
t H α α H
α∈N
andforHilbertbasis{ψ ,α∈N}ofH,
α (cid:88)
∥K ψ ∥2=c2 <+∞, (4.24)
t α H t
α∈N
wherec2,definedbyEq.(4.16),issuchthatc2
=(cid:80)
b (t)2,andtherefore,isaHilbert-SchmidtoperatorinH.
t t α∈N α
PROOF. (Proposition2). UndertheconditiondefinedbyEq.(4.16), itiswellknownthatoperator K
t
iscontinuous
fromHintoH.
(a) Substituting Eq. (4.14) into Eq. (4.18) and using Eq. (4.7) yield Eq. (4.19). This equation shows that K is a
t
symmetricandpositiveoperatorbecause⟨K u,u⟩ > 0foralluinHwith∥u∥ ̸= 0. NotethatEq.(4.20)isdirectly
t H H
deducedfromEq.(4.19).
(b) From Eqs. (4.8) and (4.9), we have ψ ∈ H. Taking u = ψ in Eq. (4.20) and using Eq. (4.10) yield K ψ =
α β t β
(cid:80) b (t)⟨ψ ,ψ ⟩ ψ = b (t)ψ , Eq. (4.22) is obtained from Eq. (4.19) by taking u = ψ and v = ψ . The
α∈N α β α H α β β α β
relationshipbetweenb (t)andλ comesfromProposition1.
α α
(c) Using Eqs. (4.20) and (4.10) yields Eq. (4.23). Taking u = ψ in Eq. (4.23) yields ∥K ψ ∥2= b (t)2. From
Eq.(4.17)and(cid:80)
∥K ψ
∥2=(cid:80)
b (t)2,weobtainEq.(4.24).
Itα canbededuced(seeforint stα anH ce[71α
])thatK is
α∈N t α H α∈N α t
aHilbert-SchmidtoperatorinH.
104.4. FiniteapproximationofoperatorK andofitseigenvalueproblem
t
TheHilbert-SchmidtoperatorK definedbyEq.(4.18),operatesininfinitedimension. TheHilbertbasis{ψ ,α∈
t α
N} (which relates to the Hilbert basis {q ,α ∈ N}, see Lemma 2 and Eq. (3.18)) is not explicitly known, thereby
α
preventing the use of the representation defined by Eq. (4.20). We must thus construct a finite approximation of
K. SinceRν isanunboundedsetandν canbeverylarge, classicaldiscretizationsuchasfinite-differenceorfinite-
t
elementmethods(see[72,73,74,75]forFokker-Planckequationand[76]forfractionalFokker-Planckequation)or
such methods based on shape-morphing modes for solving the Fokker-Planck equation as proposed in [77], are not
directlyadaptedforsolvingtheeigenvalueproblemofoperatorLˆ .Anotherclassicalmethodconsistsinintroducing
FKP
afinitefamilyoffunctionsinH,generatingafinitedimensionsubspaceofH,andinperformingtheprojectionofK on
t
thisfinitesubspace. SuchanapproachisnotreallyadaptedtooperatorLˆ forwhichalargenumberofeigenvalues
FKP
andassociatedeigenfunctionshavetobecomputed.Itshouldbenotedthatarelatedproblem,butdistinctfromtheone
addressed,isthatofthenumericalmethodforSchro¨dingeroperatorandtheassociatedequation(seeforinstance[78]
forsolvingtheSchro¨dingerEquation,[79]forthesolutionoftheSchro¨dingerequationbyspectralmethods,[80]for
numericallysolvingthetime-dependentSchro¨dingerequation,and[81]forthenumericalsolutionoftheSchro¨dinger
equation using finite-difference method). Nevertheless, such approaches are not well adapted to the objective of
the actual developments, which has been detailed in Section 1. We then propose to use a statistical sampling of
Rν equipped with the probability measure p (η)dη, which will be well adapted to our objective of performing a
H
constructionconnectedtotheDMAPSapproach.
Proposition3(Probabilisticinterpretationofthebilinearform⟨Ku,v⟩ ). Letusassumethat,foreveryfixedt>
t H
0,wehave(y,x)(cid:55)→ρ(y,t|x,0)∈C0(Rν×Rν). FromEq.(4.5),itcanbededucedthat(y,x)(cid:55)→k(y,x)∈C0(Rν×Rν).
t
Foreveryfixedt>0,foralluandvinH∩C0(Rν),therestrictionto(H∩C0(Rν))×(H∩C0(Rν))ofthebilinearform
(u,v)(cid:55)→⟨Ku,v⟩ ,definedonH×HbyEq.(4.18)withthecontinuoussymmetricfunctionk verifyingEq.(4.16),can
t H t
bewrittenas
⟨Ku,v⟩ = E{k(H,H)u(H)v(H)}, (4.25)
t H t
(cid:101) (cid:101)
in which H is an independent copy of H. The joint probability measure P (dy,dx) of H with H is p (y) ×
H,H H
p (x)dyd(cid:101)x. Thereal-valuedrandomvariablesu(H), v(H), andthepositive-va(cid:101)luedrandomvariable(cid:101)k(H,H), de-
H t
finedon(Θ,T,P),aresecond-orderrandomvariab(cid:101)les, (cid:101)
E{u(H)2}<+∞ , E{v(H)2}<+∞ , E{k(H,H)2}<+∞. (4.26)
t
(cid:101) (cid:101)
LetZ bethereal-valuedrandomvariable,definedon(Θ,T,P),suchthat
t,u,v
Z =k(H,H)u(H)v(H). (4.27)
t,u,v t
(cid:101) (cid:101)
Then,Z issuchthat,
t,u,v
E{Z }<+∞. (4.28)
t,u,v
PROOF. (Proposition3).
(a)SinceuandvarecontinuousfunctionsinH,u(H)andv(H)arereal-valuedrandomvariablesdefinedon(Θ,T,P)
andaresecond-orderbecause,
(cid:90) (cid:90)
E{u(H)2}= u(x)2p (x)dx=∥u∥2<+∞ , E{v(H)2}= v(y)2p (y)dy=∥v∥2<+∞.
H H H H
(cid:101) Rν Rν
Since k ∈ C0(Rν × Rν) (see Eq. (4.5)) and due to Eq. (4.16), k(H,H) is a second-order positive-valued random
t t
variabledefinedon(Θ,T,P), (cid:101)
(cid:90) (cid:90)
E{k(H,H)2}= k(y,x)2p (y)p (x)dydx=c2 <+∞.
t t H H t
(cid:101) Rν Rν
(cid:82)
(b)ForalluinHandfort > 0,fromEq.(4.18),itcanbeseenthat(Ku)(y) = k(y,x)p (x)1/2u(x)p (x)1/2dx.
t Rν t H H
UsingtheCauchy-SchwarzinequalityandEq.(4.16)yield
(cid:90) (cid:90)
∥Ku∥2 ≤ ∥u∥2 k(y,x)2p (y)p (x)dydx=c2∥u∥2 ,
t H H t H H t H
Rν Rν
11(which,inpassing,showsthecontinuityoftheoperatorK inHasstatedatthebeginningoftheproofofProposition2).
t
Consequently,wehave⟨Ku,v⟩2 ≤∥Ku∥2 ∥v∥2,whichshowsthat
t H t H H
⟨Ku,v⟩2 ≤c2∥u∥2 ∥v∥2<+∞, (4.29)
t H t H H
andtherefore,Eq.(4.28)holds.
Definition4(Estimatorconstructedwithastatisticalsamplingandassociatedestimation). Let{Hi,i=1,...,n }
d
and{Hj, j = 1,...,n }ben independentcopiesof Hand H,respectively. Foreveryfixedt > 0,andforalluandv
d d
inH∩(cid:101) C0(Rν),letZij betherealvaluedrandomvariableo(cid:101) n(Θ,T,P),suchthat,foralliand jin{1,...,n },
t,u,v d
Zij =k(Hi,Hj)u(Hj)v(Hi). (4.30)
t,u,v t
(cid:101) (cid:101)
LetZ(cid:98)(nd)bethereal-valuedrandomvariableon(Θ,T,P),definedby
t,u,v
1
(cid:88)nd (cid:88)nd
Z(cid:98)(nd) = Zij . (4.31)
t,u,v n2 t,u,v
d i=1 j=1
Then,Z(cid:98)(nd)isanestimatorof
t,u,v
z = E{Z }=⟨Ku,v⟩ , (4.32)
t,u,v t,u,v t H
where Z is given by Eq. (4.27). Let {ηj, j = 1,...,n } be the n independent realizations of H introduced in
t,u,v d d
Section2. Since Hi and Hj areindependentcopiesof H(because Hisanindependentcopyof H),anestimationof
z isarealizationz(nd) (cid:101) oftheestimatorZ(cid:98)(nd),whichiswrittenas (cid:101)
t,u,v t,u,v t,u,v
1
(cid:88)nd (cid:88)nd
z( tn ,ud ,)
v
=
n2
k t(ηi,ηj)u(ηj)v(ηi). (4.33)
d i=1 j=1
Lemma3(Convergenceofthesequenceofestimators{Z(cid:98)(nd)} ). Under the hypotheses and notations of Defini-
t,u,v nd
tion4,thesequenceofreal-valuedrandomvariables{Z(cid:98)(nd)} on(Θ,T,P)isconvergentinprobabilitytoz ,
t,u,v nd t,u,v
∀ϵ >0 , lim P{|Z(cid:98)(nd)−z | ≥ ϵ}=0. (4.34)
nd→+∞ t,u,v t,u,v
Wehavealsothealmostsureconvergence,thankstothestronglawoflargenumbers,
P{ lim Z(cid:98)(nd) =z }=1. (4.35)
nd→+∞ t,u,v t,u,v
PROOF. (Lemma3). Theproofusestheusualresultsfrommathematicalstatistics(seeforinstance[82]).
(cid:113)
Remark2. (a)Asiswellknown,thespeedofconvergenceisproportionalto1/ n2 = 1/n andisindependentof
d d
dimension ν. The quantification of the approximation error could traditionally be estimated using the central limit
theorem, which involves the variance of the estimator (see, for instance, [82, 64, 83]). In the numerical illustration
providedinSection6,wewillshowthenumericalcalculationofthefirsteigenvaluesoftheFokker-Planckoperator
forwhichareferenceisknown.
(b) Lemma 3 with Definition 4 and Proposition 3 allow a finite approximation of ⟨Ku,v⟩ to be constructed and
t H
consequently,todeducethecorrespondingfiniteapproximationoftheeigenvalueproblemdefinedbyEq.(4.21).
Proposition4(Finiteapproximationoftheeigenvalueproblem). For every fixed t > 0, for all u and v in H ∩
C0(Rν),andforn sufficientlylarge,wehave(inthesenseoftheconvergencedescribedinLemma3),
d
⟨K tu,v⟩ H ≃⟨[K(cid:98)(t)]uˆ,vˆ⟩ Rnd , (4.36)
12in which ⟨·,·⟩ Rnd is the usual Euclidean inner product in Rnd and where [K(cid:98)(t)] ∈ M+ nd0 is such that, for all i and j in
{1,...,n },
d
1
[K(cid:98)(t)] = k(ηi,ηj), (4.37)
ij n t
d
andwherethevectorsuˆ andvˆ inRν aresuchthat
(cid:32) (cid:33) (cid:32) (cid:33)
u(η1) u(ηnd) v(η1) v(ηnd)
uˆ = √ , ... , √ , vˆ = √ , ... , √ . (4.38)
n n n n
d d d d
ThecorrespondingfiniteapproximationoftheeigenvalueproblemdefinedbyEq.(4.21)iswrittenas
[K(cid:98)(t)]ψˆ α(t)=bˆ α(t)ψˆ α(t) , ψˆ α(t)∈Rnd, (4.39)
inwhich
bˆ (t) ≥ bˆ (t) ≥ ... ≥ bˆ (t) ≥0, (4.40)
0 1 nd−1
andwherethenormalizationoftheeigenvectorsischosensothatforαandβin{0,1,...,n −1},
d
⟨ψˆ α(t),ψˆ β(t)⟩ Rnd =δ αβ. (4.41)
PROOF. (Proposition 4). From Eqs. (4.32), (4.33), and (4.37), it can be deduced that, for n
d
sufficiently large,
⟨Ku,v⟩ ≃ n−2(cid:80)nd (cid:80)nd k(ηi,ηj)u(ηj)v(ηi). Since k(ηi,ηj) = k(ηj,ηi) by symmetry of k (see Eq. (4.6)), the
t H d i=1 j=1 t t t t
right-handsidemembercanbewrittenas⟨[K(cid:98)(t)]uˆ,vˆ⟩ Rnd where[K(cid:98)(t)],uˆ,andvˆ aredefinedbyEqs.(4.37)and(4.38).
From Proposition 2, K
t
is a positive operator in H, and as Hilbert-Schmidt operator, b α(t) → 0+ as α → +∞. For
thefiniteapproximation,matrix[K(cid:98)(t)]isthensymmetricandpositive. UsingEq.(4.36),thefiniteapproximationof
Eq. (4.21) is then written as Eq. (4.39). Since [K(cid:98)(t)] is real, symmetric, and positive, we have Eqs. (4.40) and we
choosethenormalizationof{ψˆ (t)} sothat(4.41)holds.
α α
Remark3(Aboutthefiniteapproximationoftheeigenvalueproblem). (a)Itshouldbenotedthatwehavechosen
the construction of {ψˆ α(t)}
α
as an orthonormal basis in Rnd. Consequently, ψˆ α(t) is not related to ψ
α
by a simple
sampling,similartotheonedescribedinEq.(4.38).
(b)Inaddition,ψˆ (t)depends,apriori,ont,whileψ isindependentoft. Onlyforn →+∞,ψˆ (t)goestoavector
α α d α
independentoft. Similarly,althoughb (t)=exp(−λ t)=1becauseλ =0(seeEq.(4.15)),wedonothave, apriori,
0 0 0
bˆ (t) = 1,butthisequalityholdsforn → +∞. Nevertheless,forn finite,wewillexploitthisexistingdependence
0 d d
ontfortheconstructionofthereducedtransientbasisattimet,whichwillbeconnectedtothereducedDMAPSbasis
fort→0.
(c)Notethatasn → +∞,bˆ (t)tendstob (t). Sinceb (t)=exp(−λ t),thatistosayλ =−1 log b (t),wechoose
d α α α α α t α
todefineλˆ (t)byasimilarformula,suchthatfort>0andαforwhichbˆ (t)>0,
α α
1
λˆ (t)=− log bˆ (t). (4.42)
α t α
(d)Foreveryfixedt > 0, tosolvetheeigenvalueproblemdefinedbyEq.(4.39), wehavetoconstructmatrix[K(cid:98)(t)]
withanadaptedmethodology. ThiswillbetheobjectofSection5.
5. Constructionofthematrixofthefiniteapproximation
Inthissection,wepresentthemethodologytoconstructmatrix[K(cid:98)(t)]asdefinedinProposition4. Thisconstruc-
tion requires the numerical evaluation of kernel k, because the entry [K(cid:98)(t)] of [K(cid:98)(t)] is given by k(ηi,ηj)/n (see
t ij t d
Eq.(4.37)). AccordingtoDefinition1(seeEq.(4.1)),foriand jin{1,...,n },
d
1 ρ(ηi,t|ηj,0)
[K(cid:98)(t)] = . (5.1)
ij n p (ηi)
d H
13InEq.(5.1), p isexplicitlydefinedbyEq.(2.3),andρ(y,t|x,0)isthetransientprobabilitydensityfunctionofthe
H
stochastic process {Y(t),t ≥ 0}, starting from Y(0) = x in Rν. This function is the solution of the ISDE defined by
Eq. (3.1) for t > 0 and with the initial condition Y(0) = x (see Eq. (3.2)). We will begin by using a a classical
mathematical result concerning the solution of the ISDE, which must be validated for the specific case where the
invariant measure is defined in Section 2. Additionally, we will obtain a proof of the properties introduced in Sec-
tion 3.2. Then, we will present the numerical method for constructing [K(cid:98)(t)] by numerically solving the ISDE and
usingnonparametricstatisticstoestimateρ(ηi,t|ηj,0). Subsequently,wewillderiveanexplicitalgebraicformulafor
[K(cid:98)(t)] .
ij
5.1. ExistenceanduniquenessofthesolutionofISDEandpropertiesofthetransitionprobability
LetY ={Y(t),t≥0}betheRν-valuedstochasticprocesssatisfying(seeEqs.(3.1)and(3.2)),
dY(t)=b(Y(t))dt+dW(t) , t>0, (5.2)
Y(0)=x∈Rν, a.s., (5.3)
with x∈Rν,wherethedrift y(cid:55)→ b(y):Rν (cid:55)→Rν isdefinedbyEq.(3.3)withEq.(2.7),andwhere{W(t),t ≥0}isthe
normalizedWienerprocessdefinedon(Θ,T,P).
Proposition5(Existenceanduniqueness). Eqs. (5.2) and (5.3) define a unique homogeneous diffusion Rν-valued
stochastic process Y defined on (Θ,T,P), whose transition probability measure is homogeneous (that is to say, it
dependsonlyont−0=t),
P (B,t|x,0)=P{Y(t)∈B|Y(0)= x} , t>0, (5.4)
Y(t)|Y(0)
where B is any Borel set in Rν. For all t > 0 and for all x in Rν, P (dy,t|x,0) admits a density function
Y(t)|Y(0)
y(cid:55)→ρ(y,t|x,0):Rν (cid:55)→]0,+∞[withrespecttotheLebesguemeasuredyonRν,suchthat
P (dy,t|x,0)=ρ(y,t|x,0)dy , t>0, (5.5)
Y(t)|Y(0)
limρ(y,t|x,0)dy=δ (y−x). (5.6)
0
t→0+
Stochastic process Y has almost-surely continuous trajectories and for all t > 0, Y(t) is a second-order random
variable,
∀t≥0 , E{∥Y(t)∥2}<+∞. (5.7)
Fort→+∞,Yisasymptotictoastationarystochasticprocesswhosefirst-ordermarginalprobabilitymeasureisthe
invariantmeasure p (y)dy,
H
lim ρ(y,t|x,0)dy= p (y)dy. (5.8)
H
t→+∞
Thefunction(y,x)(cid:55)→ρ(y,t|x,0)iscontinuousfromRν×Rν intoR+∗,
ρ(·,t|·,0)∈C0(Rν×Rν,R+∗).
(5.9)
PROOF. (Proposition5). Theproofispresentedinfivesteps.
(a)ItiseasytoprovethatdriftfunctionbiscontinuousonRν.
(b)SincethediffusionmatrixistheidentitymatrixandbbelongstoC0(Rν,Rν), wecanestablishtheexistenceofa
uniquediffusionstochasticprocess(see[66]Ch. VIII,Sec. 2;[84]Ch. IV,Secs. 2,3,and5;or[67]Ch. V),provided
thatforallyandy′inRν,wehave
∥b(y)−b(y′)∥≤ c∥y−y′∥ , ∥b(y)∥≤ C(1+∥y∥). (5.10)
UsingEqs.(2.3),(2.7),and(3.3),b(y)canberewritten,forallyinRν,as
1 1
(cid:88)nd (cid:40)
1 sˆ
(cid:41)
b(y)= ξ(y)−1∇ξ(y) , ξ(y)= exp − ∥ ηj−y∥2 > 0. (5.11)
2 n 2sˆ2 s
d j=1
14Calculating∇ξ(y),itcanbeseenthat
1
(cid:88)nd
sˆ
(cid:40)
1 sˆ
(cid:41)
1
2∥b(y)∥≤ ∥ηj∥exp − ∥ ηj−y∥2 + ∥y∥.
ξ(y)n sˆ2 s 2sˆ2 s sˆ2
d j=1
Sincen isfiniteandsinceE{∥H∥2}= 1 (cid:80)nd ∥ηj∥2=νisalsofinite,wehavesup ∥ηj∥=c <+∞andtherefore,
d nd j=1 j=1,...,nd η
(cid:32) (cid:33)
1 1 sˆ 1
2∥b(y)∥≤ c + ∥y∥ ≤ c(1+∥y∥) , 0<c<+∞.
2 sˆ2 s η sˆ2
ThesecondinequalityinEq.(5.10)isthenproven. Itcannowbeverifiedthat
1
∥b(y)−b(y′)∥ ≤ (∥f(y)− f(y′)∥+∥y−y′∥),
2sˆ2
inwhich f(y)= a(y)/β(y)wherea(y)andβ(y)arewrittenas
sˆ(cid:88)nd (cid:40)
1 sˆ
(cid:41) (cid:88)nd (cid:40)
1 sˆ
(cid:41)
a(y)= ηj exp − ∥ ηj−y∥2 , β(y)= exp − ∥ ηj−y∥2 .
s 2sˆ2 s 2sˆ2 s
j=1 j=1
Wemusthave∥f(y)−f(y′)∥≤c∥y−y′∥,thatistrueif∥[∇f(y)]∥ ≤ c˜ <+∞,inwhichthe(ν×ν)-realmatrix[∇f(y)]
F
is written as [∇f(y)] = β(y)−1[∇a(y)]−β(y)−2a(y)⊗∇β(y), and where ∥·∥ is the Frobenius norm. Introducing,
(cid:110) (cid:111) F
temporary,thenotatione =exp − 1 ∥sˆηj−y∥2 ,wehave
j 2sˆ2 s
[∇f(y)]= sˆ1
2
s sˆ2 2(cid:80)n j=d 1(cid:80)n j′d = (cid:80)1e
n
j=dje 1j (cid:80)′(η
n
j′dj =⊗ 1eη jej j−
′
ηj⊗ηj′) .
Itcanthenbededucedthat,
1 sˆ2
∥[∇f(y)]∥ ≤ sup∥ηj⊗ηj−ηj⊗ηj′∥ .
F sˆ2 s2 F
j,j′
Aspreviously,sincesup ∥ηj∥=c <+∞,wehave∥[∇f(y)]∥ ≤ c˜ withc˜ afinitepositiveconstantindependentof
j η F F F
y,thatyieldstheprooffortheLipchitzcontinuityofb.
(c)Wewilladmitthat,forallt>0andforallxinRν,thetransitionprobabilitymeasurehasadensityy(cid:55)→ρ(y,t|x,0)
onRν withrespecttody,andthat(y,x) (cid:55)→ ρ(y,t|x,0)iscontinuousonRν×Rν (forallt > 0)(seeforinstance[85]
ChIII,Secs. 6to9,or[86]Ch. 10).
(d) As a diffusion stochastic process, the trajectories are almost surely continuous functions. Since b is continuous
and∥b(y)∥≤ c(1+∥y∥)implies∥b(y)∥2≤ 2c2(1+∥y∥2),andsinceE{∥Y(0)∥2}=∥x∥2<+∞forallfixed xinRν,we
haveEq.(5.7)(see[84]Ch. IV,Sec. 2).
(e) The existence of the asymptotic stationary solution with Eq. (5.8) can be found in [85] Ch. III, or [68] Ch. VI,
Secs. 5and6.
5.2. RewritingtheItoˆ equationinamatrixform
Let{Yj(t),t ≥ 0}bethesolutionofEqs.(5.2)and(5.3)withtheinitialconditionYj(0) = ηj ∈ Rν,inwhichηj is
definedinSection2. Wethenhave
ρ(y,t|ηj,0)dy= P (dy,t|ηj,0). (5.12)
Yj(t)|Yj(0)
Let{[Y(t)],t≥0}betheM -valuedstochasticprocessand[η ]bethematrixinM suchthat
ν,nd d ν,nd
[Y(t)]=[Y1(t)...Ynd(t)] , [η d]=[η1...ηnd]. (5.13)
15Therefore,{[Y(t)],t≥0}issolutionofthematrix-valuedISDE,
1
d[Y(t)]= [L([Y(t)])]dt+d[W(t)] , t>0, (5.14)
2
[Y(0)]=[η ], a.s., (5.15)
d
where[y](cid:55)→[L([y])]isthefunctionfromM intoM ,defined,fork∈{1,...,ν}and j∈{1,...,n },by
ν,nd ν,nd d
1 ∂ξ(yj)
[L([y])] = . (5.16)
kj ξ(yj) ∂yj
k
In Eq. (5.16), yj is the j-th column of [y] = [y1...ynd], where yj = (y 1j,...,y νj). Additionally, ξ(y) is defined by
Eq.(5.11),and{[W(t)],t≥0}isthenormalizedM -valuedWienerstochasticprocess.
ν,nd
5.3. Time-discreteapproximationoftheISDEandconvergenceanalysis
To estimate [K(cid:98)(n∆t)] , defined by Eq. (5.1), using nonparametric statistics, we need to generate realizations of
ij
Eqs. (5.14) and (5.15). Consequently, a first stage involves introducing a time-discrete approximation of Eq. (5.14)
andanalyzingtheconvergence.
(i) Time sampling ∆t and δt. We first define a time sampling n∆t,n≥1 to be used for estimating [K(cid:98)(n∆t)]. The
timestep∆twillbedefinedinSection7. However,∆tmaynotbesufficientlysmalltoachieveasatisfactoryrateof
convergence. Wethenintroduceδt ≤∆tsuchthat∆t =n ×δt,withn ≥1. TodiscretizeEq.(5.14),weusethetime
s s
sampling{µδt,µ≥1}.
(ii)ISDEdiscretization. Assumingthatδtissufficientlysmallrelativeto1,employingtheEulerscheme(asseen,for
example,in[87])todiscretizethesolution{[Y(t)],t≥0}ofEqs.(5.14)and(5.15)yields
δt √
[Y ]=[Y ]+ [L([Y ])]+ δt[Γ ] , µ≥1, (5.17)
µ µ−1
2
µ−1 µ
[Y ]=[η ], a.s., (5.18)
0 d
where [Y ] is the approximation of [Y(t)] at t = µδt, and where {[Γ ] ;k = 1,...,ν; j = 1,...,n ;µ ≥ 1} is an
µ µ kj d
infinitefamilyofindependentnormalizedGaussianreal-valuedrandomvariables.
(iii) Convergence of the time-discrete approximation. For each j in 1,...,n , Proposition 5 can be applied to the
d
stochastic process {Yj(t),t ≥ 0}, where Yj(t) represents the j-th column of [Y(t)]. Let {[Yδ(t)],t ≥ 0} be the time-
discreteapproximationof{[Y(t)],t≥0}. TakeanyfixedpositiverealnumberT,letµ =int(T/δt)denotesthenearest
T
integertoT/δt. Itcanbeobservedthatasδtapproaches0,µ tendsto+∞. Thefollowingclassicalresultholds(see,
T
forinstance,[87],Page323).
Lemma4(Strongconvergence). UnderProposition5,thetime-discreteapproximation{[Yδ(t)],t≥0}of{[Y(t)],t≥
0}convergesstronglyattimeT if
lim E{∥[Y ]−[Y(T)]∥2}=0, (5.19)
δt→0
µT F
where[Y ]isdeterminedbyEqs.(5.17)and(5.18)uptoµ=µ .
µT T
(iv) Criterion to determine if δt is small enough. We can employ a criterion based on weak convergence, which is
relatedtothecovariancematrix[C ]∈M+ oftheRν-valuedrandomvariableYj(t),fortfixedintheinterval]0,T].
Yj(t) ν
Letσ2(t)bedefinedby
Y
(cid:88)nd (cid:88)ν
σ2(t)= E{∥[Y(t)]−E{[Y(t)]}∥2}= E{(Yj(t)−E{Yj(t)})2}. (5.20)
Y F k k
j=1 k=1
16Itcaneasilybeseenthat
(cid:88)nd
σ2(t)= tr[C ]. (5.21)
Y Yj(t)
j=1
Letσ2 bethecorrespondingquantityforrandommatrix[Y ],
YµT µT
(cid:88)nd
σ2 = E{∥[Y ]−E{[Y ]}∥2}= tr[C ]. (5.22)
YµT µT µT F
j=1
Yµj
T
Thecriterioncanthenbebasedonthefollowingproperties,
lim |σ −σ (T)|= 0. (5.23)
δt→0
YµT Y
Wewilldetailedthiscriterioninparagraph(vi).
(v) Generation of independent realizations of the time-discrete approximation. Let n ≥ 1 and N > 1 be two fixed
s
integers. Let∆tbefixedandletδt =∆t/n . Notethatµ introducedinSection5.3-(iv)issuchthatµ =n ×N. We
s T T s
defineT,N,andN by,
µ
T = N×∆t=n ×N×δt , N ={1,...,N} , N ={1,...,n ×N}. (5.24)
s µ s
For all µ in N , let {[γℓ] ∈ M ,ℓ = 1,...,n } be n independent realizations of the random matrix [Γ ]. As
µ µ ν,nd MC MC µ
discussed in Section 5.3-(ii), it can be seen that the family {[γℓ],ℓ = 1,...,n ;µ ∈ N } consists of n ×n ×N
µ MC µ MC s
independentrealizations. Foreachℓin{1,...,n },therealization{[y˜ℓ],µ ∈ N }ofthetime-discreteapproximation
MC µ µ
{[Y ],µ∈N }iscomputedusingthefollowingrecurrence(refertoEqs.(5.17)and(5.18)),
µ µ
δt √
[y˜ℓ]=[y˜ℓ ]+ [L([y˜ℓ ])]+ δt[γℓ] , µ∈N , (5.25)
µ µ−1 2 µ−1 µ µ
[y˜ℓ]=[η ], (5.26)
0 d
In the following, for performing the statistical estimations, we will use the subsequence {[yℓ],n ∈ N} of {[y˜ℓ],µ ∈
n µ
N },suchthat
µ
∀n∈N , [yℓ]=[y˜ℓ] , µ=n ×n. (5.27)
n µ s
(vi)Practicalcriteriaforcontrollingtheconvergenceparameters. Let∆tbefixed,aswellasn ,meaningδtisfixed,
s
toensuresatisfactionoftheconvergencecriteriaintroducedinSection5.3-(iv). Anadditionalpracticalcriterioncan
beappliedtoverifytheadequacyofalltheconvergenceparameters,asprovidedinLemma5.
Lemma5(Practicalcriteriaforcontrollingtheconvergence). For k ∈ {1,...,ν}, j ∈ {1,...,n }, and n ∈ N, let
d
[y ]and[σ ]bethematricesinM definedby,
n
n ν,nd
1
(cid:88)n
MC 1
(cid:88)n
MC
[y ] = [yℓ] , ([σ ] )2 = ([yℓ] −[y ] )2. (5.28)
n kj n n kj n kj n n kj n kj
MC ℓ=1 MC ℓ=1
Letn(cid:55)→y(n)andn(cid:55)→σ(n)bethepositive-valuedfunctionsdefined,forn∈N,by
1 1
y(n)= √ ∥[y ]∥ , σ(n)= √ ∥[σ ]∥ . (5.29)
F n F
ν×n n ν×n
d d
Then,forδt→0andn →+∞,wehave
MC
lim y(n)=0 , lim σ(n)=1. (5.30)
N→+∞ N→+∞
17PROOF. (Lemma 5). It can be seen that [y ]
kj
and ([σ n] kj)2 are the empirical estimates of the mean value and the
n
varianceofthetime-discreteapproximation[Y ] forµ = n ×nofYj(n∆t). FromProposition5,weknowthat,for
µ kj s k
t → +∞, {Y(t),t ≥ 0}, andconsequently, {Yj(t),t ≥ 0}, isasymptoticallystationaryandthatEq.(5.8)holds. Since
E{H}=0 andE{H⊗H}=[I ](seeEqs.(2.5)and(2.6)),ifδt→0andn →+∞,wehaveforN →+∞,[y ] →
E{H } = 0ν and([σ ] )2 → [Cν ] = 1. Since∥[y ]∥2= (cid:80)ν (cid:80)nd ([y ] )M 2C and∥[σ ]∥2= (cid:80)ν (cid:80)nd ([σ ] )2,n uk sj ing
k n kj √ d kk n F k=1 j=1 n kj n F k=1 j=1 n kj
thesamenormalization1/ ν×n fory(n)andσ(n),weobtainEq.(5.30).
d
5.4. Estimationofthematrixofthefiniteapproximation
Wenowcanestimate[K(cid:98)(n∆t)] definedbyEq.(5.1)usingtherealizations{[yℓ],ℓ=1,...,n }forn∈N,defined
ij n MC
byEq.(5.27).
Proposition6(Estimationofmatrix[K(cid:98)(n∆t)]). Under Propositions 4 and 5, results and notations of Section 5.3-
(v),forn∈N,anestimateofmatrix[K(cid:98)(n∆t)],whoseentriesaredefinedbyEq.(4.37),iswrittenas
[K(cid:98)(n∆t)]=[(cid:98)B]−1[K(cid:98)(n∆t)], (5.31)
inwhich[(cid:98)B]isthediagonalmatrixinM forwhichitsentriesare
nd
(cid:88)nd (cid:40)
1 sˆ
(cid:41)
[(cid:98)B] =δ exp − ∥ηi− ηj′∥2 . (5.32)
ij ij 2sˆ2 s
j′=1
Theentriesofmatrix[K(cid:98)(n∆t)]inM arewrittenas,
nd
[K(cid:98)(n∆t)] ij = n1
MC
(cid:88) ℓn =MC 1(cid:32) sν SB(cid:81)ν ks =ˆ 1ν [σ n] kj(cid:33) exp  −1 2(cid:88) k=ν 1 η si k SB− [σ[y nℓ n ]] kk jj 2   , (5.33)
inwhichsandsˆaredefinedbyEq.(2.4),wheres iswrittenas
SB
(cid:40)
4
(cid:41)1/(ν+4)
s = , (5.34)
SB n (2+ν)
MC
where[yℓ]∈M isdefinedbyEq.(5.27),andwhere[σ ]∈M isdefinedbyEq.(5.28).
n ν,nd n ν,nd
PROOF. (Proposition 6). Eq. (5.31) is deduced from Eq. (5.1) with t = n∆t and with p H(ηi) given by Eq. (2.3) in
whichη = ηi,andwiththeestimateofρ(ηi,n∆t|ηj,0)givenbytheGaussiankernel-densityestimationmethod(see
[63,88])withtheSilvermanbandwidths definedbyEq.(5.34),whichiswrittenas
SB
ρ(ηi,n∆t|ηj,0)= n1
MC
(cid:88) ℓn =MC
1
(√
2πs
SB)ν1
(cid:81)ν
k=1[σ n]
kj
exp  −1 2(cid:88) k=ν 1 η si k SB− [σ[y nℓ n ]] kk jj 2 

.
Remark4. (a) In Section 6, we will provide an illustration by numerically solving the eigenvalue problem defined
by Eq. (4.39) for t = n∆t, using Eq. (5.31). Specifically, we consider the Gaussian case with dimension ν = 1. In
thisscenario, HrepresentsanormalizedGaussianreal-valuedrandomvariable. Forthiscase,areferencesolutionis
available.
(b) Nevertheless, it would be challenging to employ such a formulation with reasonable convergence in very high
dimensions,whereνequalsseveraltensorevenhundreds. Thiswouldnecessitatealargevalueofn .
d
(c)Infact,Proposition6willbeemployedinSection7toderiveanexpressionthatconnectstothekernel[K ] =
(cid:110) (cid:111) DM ij
exp − 1 ∥ηi−ηj∥2 ,whichisusedtocalculatetheDMAPSbasisandprovesefficientforlargevaluesofν.
4ε
DM
186. Numericalillustrationoftheproposedformulation
As explained in Remark 4-(a), this section presents a numerical illustration of the formulation introduced in
Section5tosolvetheapproximatedeigenvalueproblemdefinedbyEq.(4.39),whichisderivedfromtheeigenvalue
probleminEq.(4.21). Tovalidatetheformulation,weselectareferencecasewheretheeigenvalueproblemdefined
byEq.(4.21)canbeexactlysolved. Thisisfeasiblewhenthedimensionνof His1and HisanormalizedGaussian
real-valuedrandomvariable.
6.1. Referencecasedefinitionandexplicitsolution
Thequantitiesrelatedtothereferencecasewillbeindexedbyletterr. TheprobabilitydensityfunctionofH on
r
Ris p (y) = (2π)−1/2exp(−y2/2). ThepotentialfunctionΦ (seeEq.(2.7))isΦ (y) = y2/2andthedriftb defined
Hr r r r
byEq.(3.3)isb (y)=−y/2. TheISDEdefinedbyEqs.(5.2)and(5.3)arerewrittenas
r
1
dY (t)=− Y (t)dt+dW (t) , t>0, (6.1)
r r r
2
Y (0)= x∈R. (6.2)
r
Consequently,{Y (t),t≥0}isasecond-orderGaussianstochasticprocess,whichisexplicitelydefinedby
r
(cid:90) t
Y (t)= xe−t/2+ e−(t−τ)/2dW (τ). (6.3)
r r
0
A simple calculation shows that the mean value m (t) = E{Y (t)} and the standard deviation σ (t) = (E{(Y (t) −
r r r r
m (t))2})1/2oftherandomvariableY (t)forfixedt>0,arewrittenas
r r
√
m (t)= xe−t/2 , σ (t)= 1−e−t. (6.4)
r r
Forallt>0,as{Y (t)|Y (0)= x}isaGaussianrandomvariable,thetransitionprobabilitydensityfunctionis
r r
(cid:40) (cid:41)
1 1
ρ (y,t|x,0)= √ exp − (y−m (t))2 . (6.5)
r 2πσ r(t) 2σ r(t)2 r
Note that Eqs. (6.4) and (6.5) show that, we effectively have lim ρ (y,t|x,0)dy = δ (y− x) (see Eq. (5.6)) and
t→0+ r 0
lim t→+∞ρ r(y,t|x,0)= p Hr(y)(seeEq.(5.8)). Itcanbededucedthatthekernelk r,t(y,x)onR×R,definedbyEq.(4.1)
iswritten,fort>0,as
(cid:40) (cid:41)
1 1 e−t
k (y,x)= √ exp − (y2+x2−2et/2yx)2 . (6.6)
r,t 1−e−t 2(1−e−t)
√
Let{h (y),α∈N}betheHermitepolynomialsand{h (y)/ α!,α∈N}theHilbertbasisinL2(R;p ),
α α Hr
(cid:90)
h (y) h (y) d
R
√α
α!
(cid:112)β
β!
p Hr(y)dy=δ
αβ
, h 0(y)=1 , h 1(y)=y , h α+1(y)=yh α(y)− dyh α(y).
Let{h (y),α∈N}bethepolynomialsdefinedby
α
1 y √ √
h (y)= √ h (√ ) , h (y)= 2αh ( 2y).
α α α α
2α 2
Wehavetheformula[89],
(cid:40) (cid:41)
1 4a (cid:88)aα
√ exp (xˆyˆ−axˆ2−ayˆ2) = h (xˆ)h (yˆ).
1−4a2 1−4a2 α! α α
α∈N
√ √
Takingx= 2xˆ,y= 2yˆ,anda= 1e−t/2 <1/2fort>0,Eq.(6.6)canberewrittenas
2
(cid:88)
k r,t(y,x)= e−λr,αtψ r,α(y)ψ r,α(x), (6.7)
α∈N
19α 1 y 1
λ = , ψ (y)= √ h (√ )= √ h (y), (6.8)
r,α r,α α α
2 2αα! 2 α!
andthus,forαandβinN,
(cid:90)
ψ (y)ψ (y)p (y)dy=δ . (6.9)
r,α r,β Hr α,β
R
ComparingEqs.(6.7)and(6.9)withEqs.(4.14)and(4.10)showsthat,forthisreferencecase,theeigenvaluesofthe
Fokker-PlanckoperatorL are
FKP α
λ = , α∈N. (6.10)
r,α
2
6.2. Estimatingtheeigenvalueswiththeproposednumericalformulation
(i)Fortheconvergenceanalysis,weconsider10valuesofn constitutingthesetN ={100,300,400,800,1000,1200,
d d
1500,1800,2000,2200}. For each n
d
∈ N d, the matrix [η d] = [η1...ηnd] ∈ M
1,nd
is generated with an adapted
generator(instructionrandn(1,n )forMatlabGaussiangenerator).
d
(ii) For each value of n in N , the generation of n = n independent realizations {[y˜ℓ],µ ∈ N } with N =
d d MC d µ µ µ
{1,...,n ×N}isperformedusingEqs.(5.25)and(5.26)withn = 1,δt = ∆t = 0.061796,N = 150. Forn = 1200
s s d
andn = 1200,Figs.1aand1bdisplaythegraphsoffunctionsn (cid:55)→ y(n)andn (cid:55)→ σ(n)computedusingEq.(5.29)
MC
withEq.(5.28). ItcanbeseenthatthecriteriondefinedbyEq.(5.30)issatisfied.
1
1
0.8
0.8
0.6
0.4 0.6
0.2 0.4
0 0.2
0 50 100 150 0 50 100
(a)Graphoffunctionn(cid:55)→y(n). (b)Graphoffunctionn(cid:55)→σ(n).
Figure1: CriteriadefinedinLemma5forcontrollingtheconvergenceoftheGaussian-casereferencewithν = 1, ns = 1, nd = 1200, and
nMC=1200.
(iii) For every n ∈ N , matrix [K(cid:98)(2∆t)] ∈ M+0 is calculated using Eq. (5.31) with sˆ = 0.2486, sˆ/s = 0.9690, and
d d nd
s = 0.2565. The first 6 largest eigenvalues, bˆ (2∆t) of [K(cid:98)(2∆t)] andλˆ (2∆t), simply denoted byλˆ , are obtained
SB α α α
fromEq.(4.42). Fig.2bshowsthegraphoffunctionn
d
(cid:55)→errλ(n d)thatquantifiestherelativeerrorbetweenα(cid:55)→λ
r,α
andα (cid:55)→λˆ α,writtenaserrλ(n d) = (cid:80)5 α=0(λ r,α−λˆ α)2/(cid:80)5 α=0λ2 r,α. Itcanbeseenthattheerrordecreasesasn
d
increases.
For n = n = 1200, Fig. 2a compares the reference eigenvalues λ = α/2 (see Eq. (6.10)) with the computed
d MC r,α
eigenvaluesλˆ forα=0,1,...,5. Thecomparisonisgoodenough.
α
4
0.3
3
0.25
2 0.2
1 0.15
0 0.1
0.05
-1
0 2 4 6 0
0 500 1000 1500 2000 2500
(a)graphsα(cid:55)→λr,α(diamond)andα(cid:55)→
λˆ α(circle). (b)Graphnd(cid:55)→err λ(nd).
Figure2:(a)Comparisonofthereferenceeigenvaluesλr,α(diamond)withthecomputedeigenvaluesλˆ α(circle)forα=0,1,...,5. (b)Graphof
nd(cid:55)→errλ(nd)quantifyingtherelativeerrorbetweenα(cid:55)→λr,αandα(cid:55)→λˆ α.
207. VectorbasisforPLoMderivedfromthetransientanisotropickernel,connectedtotheDMAPSbasis
WerevisittheobjectivepresentedinSection1. Thediffusion-maps(DMAPS)basis,usedbyPLoM[1,27,29],
isassociatedwiththeisotropickerneldetailedin[2,3]. Thissectionintroducestheconstructionofatransientvector
basis,basedonthetransientanisotropickerneldescribedinSection5(seeProposition6). Thisapproachincorporates
therequirementthatas∆t → 0,thetransientkernelatthefirsttime∆t coincideswiththeDMAPSisotropickernel.
Consequently,thetwovectorbaseswillbelinkedasymptoticallyast → 0. Todevelopsucha”connected”transient
anisotropickerneltotheDMAPSisotropickernel,weundertakeareparameterizationofEq.(5.31),whichdefinesthe
matrix[K(cid:98)(n∆t)]∈M+0forn∈N. Thisreformulationenablestheconstructionofthevectorbasisforhighdimensions
nd
(large values of ν) and relatively small n , as implemented in PLoM, designed for probabilistic learning with small
d
trainingdatasets.
7.1. TransientanisotropickernelconnectedtotheDMAPSisotropickernel
ThefollowingdefinitionprovidestheconstructionofatransientanisotropickernelthatislinkedtotheDMAPS
isotropickernel. Thisconstructiondrawsinspirationfromtheexpressionof[K(cid:98)(∆t)]definedbyEq.(5.31).
Definition5(TransientanisotropickernelconnectedtoDMAPS). ForeachninN,for∆tdefinedinSection5.3-
(i),forn and[yℓ]definedinSection5.3-(v),for[σ ]definedbyEq.(5.28),andforagivenrealε > 0,wedefine
MC n n DM
thematrixK˜(n∆t)]∈M+0,suchthat
nd
[K˜(n∆t)]=[B]−1[K(n∆t)], (7.1)
suchthat,foralliand jin{1,...,n },thematrix[K(n∆t)]∈M hasentries,
d nd
[K(n∆t)]
ij
= n1
MC
(cid:88) ℓn =MC 1(cid:16) Πν k=1{[σ n] kj/∆t}(cid:17)−1
exp
 − 4ε1
DM
(cid:88) k=ν
1
 [η σi k n−
]
kj[ /yℓ n √] k ∆j
t 2


, (7.2)
and[B]∈M isadiagonalmatrixwhoseentriesare,
nd
(cid:88)nd (cid:40)
1
(cid:41)
[B] =δ exp − ∥ηi−ηj′∥2 . (7.3)
ij ij 4ε
j′=1 DM
Remark5(Aboutthechoiceofthesmoothingparameterε ). Anoptimalvalue,ε ,forthesmoothingparame-
DM opt
terε ,isproposedforthePLoMalgorithmin[29]andisdetailedinAppendix A.2-(i). Thisoptimalvalueenables
DM
the analysis of high-dimensional problems (large value of ν). It should be noted that Proposition 7 will explain the
definitionof[K˜(n∆t)]asdefinedbyEqs.(7.1)to(7.3),showingtheconnectionwithDMAPS.
Proposition7(Limitof[K˜(∆t)]for∆t→0). WeusethenotationintroducedinDefinition5. Let[K ]∈M+0bethe
DM nd
matrixoftheDMAPSisotropickernel,definedforalli, j∈{1,...,n },by
d
(cid:40) (cid:41)
1
[K ] =exp − ∥ηi−ηj∥2 . (7.4)
DM ij 4ε
DM
Let[K ]∈M bethematrixdefinedby
DM nd
[K ]=[B]−1[K ], (7.5)
DM DM
where[B]isthediagonalmatrixdefinedbyEq.(7.3). Let∆tbedefinedby
∆t= sˆ2/κ , κ≥1, (7.6)
wheresˆisdefinedbyEq.(2.4). Then,asκ→+∞,andconsequently∆t→0,wehave
[K˜(∆t)]→[K ], (7.7)
DM
whichmeansthat,forn = 1,thematrix[K˜(n∆t)]ofthetransientanisotropickernel,definedbyEq.(7.1),converges
tothematrix[K ]oftheDMAPSisotropickernelas∆t→0.
DM
PROOF. (Proposition 7). For n = 1, for κ → +∞, that is to say, for ∆t → 0, Eqs. (5.2) a√nd (5.3) show that, for
all ℓ ∈ {1,...,n }, k ∈ {1,...,ν}, and j ∈ {1,...,n }, we have [yℓ] → ηj and [σ ] → ∆t, and consequently,
MC d 1 kj k n kj
[K(∆t)]→[K ]. FromEqs.(7.1)and(7.5),itcanbededucedEq.(7.7).
DM
217.2. Constructionofthereduced-ordertransientbasisanditscounterpartfortheDMAPSbasis
Inthissection,we: (i)reviewtheconstructionofthereduced-orderdiffusion-maps(DMAPS)basis(RODB)rep-
resented by a matrix [g ] ∈ M with m < n , and (ii) construct, for each n ∈ N, the reduced-order transient
DM nd,mopt opt d
basis(ROTB(n∆t))representedbyamatrix[g(n∆t)]∈M .
nd,mopt
(i)ReminderoftheconstructionoftheRODB.Thisconstruction,dueto[2],istheoneusedbythePLoMalgorithm
[1,27]. Thematrix[K ],definedbyEq.(7.5),haspositiveentriesandrepresentsthetransitionmatrixofaMarkov
DM
chain. Theeigenvaluesb ,...,b andtheassociatedeigenvectors g1 ,...,gnd aresuchthat
DM,1 DM,nd DM DM
[K ]gβ =b gβ , 1=b >b ≥...≥b . (7.8)
DM DM DM,β DM DM,1 DM,2 DM,nd
UsingEq.(7.5),thiseigenvalueproblemisrewrittenasthesymmetriceigenvalueproblem,
PS ϕβ =b ϕβ , ⟨ϕβ,ϕβ′⟩=δ , gβ =[B]−1/2ϕβ, (7.9)
DM DM,β ββ′ DM
inwhichPS = [B]−1/2[K ][B]−1/2 isasymmetricmatrix. Thediffusion-mapsbasis{g1 ,...,gnd}formsavector
DM DM DM DM
basisofRnd. Asexplainedin[1,27],PLoMusestheRODBoforderm,whichisdefinedby{g1 ...gm}. Thisbasis
DM DM
dependsonε andm. Theoptimalvaluem ofmisdefined(see[29])by
DM opt
m =ν+1. (7.10)
opt
Theoptimalvalueε ofε isestimatedtoobtain
opt DM
1=b DM,1 >b DM,2 ≃...≃b DM,mopt ≫b DM,mopt+1 ≥...≥b DM,nd >0, (7.11)
inwhichthejumpamplitude
J
DM
=b DM,mopt+1/b DM,mopt, (7.12)
mustbeequalto J = 0.1(following[27]), butwhichcanalsobechosenintheinterval[0.1,0.5]whenνislarge.
DM
Therefore,theRODBisdefinedform=m andisrepresentedbythematrix,
opt
[g ]=[g1 ...gmopt]∈M . (7.13)
DM DM DM nd,mopt
(ii) Construction of the ROTB(n∆t). For each n in N, the ROTB(n∆t) represented by [g(n∆t)] is constructed as
the eigenvectors of matrix [K˜(n∆t)] defined by Eq. (7.1). Taking into account Eq. (7.7), we want that for n = 1,
[g(∆t)] ∈ M goes to [g ] ∈ M as ∆t → 0. For n > 1, matrix [K˜(n∆t)] is a priori symmetric, but matrix
nd,mopt DM nd,mopt
[K(n∆t)]isnotsymmetric. ForapplyingasimilarapproachtotheonedefinedbyEq.(7.9),wethensymmetrizethe
matrix [P(n∆t)] = [B]−1/2[K(n∆t)][B]−1/2, introducing [PS(n∆t)] = ([P(n∆t)]+[P(n∆t)]T)/2. The transient basis
{g1(n∆t),...,gnd(n∆t)} associated with the eigenvalues b˜ 1(n∆t) ≥ b˜ 2(n∆t) ≥ ... ≥ b˜ nd(n∆t) are then computed by
solvingthesymmetriceigenvalueproblem
[PS(n∆t)]φβ(n∆t)=b˜ (n∆t)φβ(n∆t) , ⟨φβ(n∆t),φβ′ (n∆t)⟩=δ . (7.14)
β ββ′
gβ(n∆t)=[B]−1/2φβ(n∆t). (7.15)
TheROTB(n∆t)isthendefinedbythematrix
[g(n∆t)]=[g1(n∆t)...gmopt(n∆t)]∈M nd,mopt, (7.16)
inwhichm isdefinedasexplainedinSection7.2-(i).
opt
227.3. Criteriaforcomparingthereduced-ordertransientbasiswiththereduced-orderDMAPSbasis
The PLoM algorithm is based on the use of the RODB (see Appendix A). With such an RODB based on the
isotropickernel,PLoMhasprovengenerallyefficient,eveninextremelydifficultcases,asdemonstratedinnumerous
publicationssince2016. ThisefficacywillalsobeevidentthroughtheapplicationspresentedinSection8. However,
for cases involving very heterogeneous data in the training dataset, the learned statistical dependence between the
componentsoftherandomvector H(throughthelearnedprobabilitymeasurefor H)canaprioribeimprovedusing
ROTB(n∆t) for given n. To assess a possible improvement with this reduced-order transient basis compared to the
reduced-orderDMAPSbasis,quantitativecriteriaarenecessary. Inthissection,weintroducecriteriaforcomparing
thetwovectorbases, andthen, inSection7.4, wewillpresentamethodologyforidentifyingtheinstance, n∆t, that
maximizestheselectioncriteriaoftheROTB(n∆t).
(i) The first criterion will be the angle between the two vector subspaces generated by the two vector bases,
ROTB(n∆t) and RODB. If this angle is close to zero then the two bases coincide. This must be the case when we
choosetheinstantn=1fortheROTB(n∆t)(SeeProposition7).
(ii)Thesecondcriterionistheconcentrationofthelearnedprobabilitymeasureinrelationtotheconcentrationof
theprobabilitymeasureofthetrainingdataset. IthasbeenproventhatthePLoMalgorithm,whichusestheRODB,
wasdesignedtopreservetheconcentrationofthelearnedprobabilitymeasurefromasmalltrainingdataset. Forthis,
in[27,29],weintroducedtheindicatord2,linkedtothemean-squareconvergence,whichwewillrecallanduse. We
alsointroduceasecondconcentrationcriterion,KL,basedontheKullback-Leiblerdivergence[90,91]betweenthe
learnedprobabilitymeasureandtheprobabilitymeasureofthetrainingdataset.
(iii) The third criterion is mutual information [92, 93], which is defined as the relative entropy introduced by
KullbackandLeibler[91]. Thiswillbeusedtoquantifythelevelofstatisticaldependenciesamongthecomponents
of the centered random vector H, whose covariance matrix is the identity matrix. Such mutual information will be
estimated for the learned probability measure generated by PLoM, comparing the RODB and the ROTB(n∆t) as a
functionofn.
(iv) The last criterion is the Entropy from Information Theory [94, 93, 95], introduced by Shannon [96]. The
estimationofthisentropyisafunctionofthenumberofrealizationsusedforestimatingtheprobabilitymeasure. This
propertywillbeusedinSection7.4tonormalizetheestimationofthemutualinformation.
7.3.1. AnglebetweenthesubspacesspannedbyROTB(n∆t)andRODB
LetnbefixedinN. Itisassumedthatrank([g ]) = rank([g(n∆t)]) = m inwhichthetwomatricesaredefined
DM opt
by Eqs. (7.13) and (7.16). In addition, it is assumed that the null space of the matrix [g(n∆t)]T[g ] is {0}. Let
DM
V(n∆t) = span{g1(n∆t),...,gmopt(n∆t)} be the m -dimension subspace of Rnd (spanned by the ROTB(n∆t)). Let
opt
V = span{g1 ,...,gmopt} be the m -dimension subspace of Rnd (spanned by the RODB). For β ∈ {1,...m }, let
DM DM DM opt opt
gˆβ =gβ /∥gβ ∥bethenormalizedvectorgβ andletgˆβ(n∆t)=gβ(n∆t)/∥gβ(n∆t)∥bethenormalizedvectorgβ(n∆t).
LD eM t[gˆ DMD ]M =[gˆDM 1 DM...gˆm DMopt]and[gˆ(n∆t)]=[gˆ1D (M n∆t)...gˆ(n∆t)mopt]bematricesinM nd,mopt. Theangleγ(n∆t)betweenthe
subspacesV(n∆t)andV isdefinedby
DM
γ(n∆t)=arccos(σ ([gˆ(n∆t)]T[gˆ ])), (7.17)
min DM
in which σ denotes the smallest singular value. If the angle is close to 0, the two subspaces are nearly linearly
min
dependent.
7.3.2. Indicatorsrelatedtothelearnedprobabilitymeasure
In this section, we detail the indicators used for comparison: concentration of the learned probability measure,
mutual information, and entropy. These will be employed to compare the probability measures associated with the
trainingdatasetandthelearneddatasetgeneratedbythePLoMalgorithm,assummarizedinAppendix A.Specifically,
weusethereduced-orderDMAPSbasis(RODB)and,alternatively,thereduced-ordertransientbasis(ROTB(n∆t)).To
facilitatethecomparisonspresentedinthenumericalillustrations,weintroducethenecessarynotationsforclarifying
thediversequantitiesandtheirnumericalcalculations.
-Trainingdataset. InSection2,theindependentrealizationsoftheRν-valuedrandomvariable Hareη1,...,ηnd,
and the matrix [η d] = [η1...ηnd] ∈ M
ν,nd
represents one realization of the random matrix [H], used by PLoM, and
definedinAppendix A.1. TheprobabilitydensityfunctionofHis p definedbyEq.(2.3).
H
23- Learned dataset. In Appendix A, the learned realizations generated by PLoM are those of the M -valued
ν,nd
random variable [H ] and are written as [ηℓ],ℓ=1,...,n (see Appendix A.3). The corresponding realizations
ar ar MCH
ηℓ′,ℓ=1,...,n ,withn = n ×n ,oftheRν-valuedrandomvariable H areobtainedbyreshaping. Thelearned
ar ar ar d MCH ar
probabilitydensityfunctionofH isdenotedby p . WhenPLoMisusedwithRODB,[H ],[ηℓ],ηℓ′,and p will
berewrittenas[H ],[ηℓ
],ηℓ′,aa nr
d p
,respectiveH lyar
.
WhenPLoMisusedwithROTB(n∆t)a ,r
[H
a ]r ,[ηℓar ],ηℓ′,anH dar
p
willberewrittenaD sB
[H
D (B n∆t)D ]B
,[ηℓ
(n∆DB
t)],ηℓ′(n∆t),and p (.;n∆t),respectively.
ar ar ar Har
TB TB TB TB
(i)ConcentrationofthelearnedprobabilitymeasureforRODBandROTB(n∆t). Basedonthemean-squarenorm,for
PLoMwithRODBandROTB(n∆t),thelearnedprobabilitymeasureconcentrationiswritten(seeEq.(A.6))as
d2(m )= E{∥[H ]−[η ]∥2}/∥[η ]∥2, (7.18)
DB opt DB d d
d2(m ;n∆t)= E{∥[H (n∆t)]−[η ]∥2}/∥[η ]∥2. (7.19)
TB opt TB d d
Usingtherealizations,thesequantitiesareestimatedby
(cid:88)n
MCH
dˆ2(m )=(1/n ) {∥[ηℓ ]−[η ]∥2}/∥[η ]∥2. (7.20)
DB opt MCH DB d d
ℓ=1
(cid:88)n
MCH
dˆ2(m ;n∆t)=(1/n ) {∥[ηℓ (n∆t)]−[η ]∥2}/∥[η ]∥2. (7.21)
DB opt MCH TB d d
ℓ=1
TheconcentrationofthelearnedprobabilitymeasurecanalsobeestimatedusingtheKullback-Leiberdivergenceand
its estimation from a set of realizations as presented in Appendix B.1. For the PLoM formulated with RODB and
ROTB(n∆t),wethenhave,
(cid:90) (cid:32) (cid:33)
p (η)
D(p ∥p )= p (η) log DB dη, (7.22)
DB H DB p (η)
Rν H
(cid:90) (cid:32)
p
(η;n∆t)(cid:33)
D(p (.;n∆t)∥p )= p (η;n∆t) log TB dη. (7.23)
TB H TB p (η)
Rν H
(ii)Mutual-information-basedcomponentsstatisticaldependenciesofthelearnedprobabilitymeasureforRODBand
ROTB(n∆t). ThemutualinformationanditsestimationfromasetofrealizationsarepresentedinAppendix B.2. For
thetrainingdatasetandforPLoMformulatedwithRODBandROTB(n∆t),wehave,forRν-valuedrandomvariables
H,H ,andH (n∆t),
DB TB (cid:90) (cid:32) (cid:33)
p (η)
I(H)= D(p ∥⊗ν p )= p (η) log H dη, (7.24)
H k=1 Hk H Πν p (η )
Rν k=1 Hk k
(cid:90) (cid:32) (cid:33)
p (η)
I(H )= D(p ∥⊗ν p )= p (η) log DB dη, (7.25)
DB DB k=1 DB,k DB Πν p (η )
Rν k=1 DB,k k
(cid:90) (cid:32) p (η;n∆t) (cid:33)
I(H ;n∆t)= D(p (·;n∆t)∥⊗ν p (·;n∆t))= p (η;n∆t) log TB dη, (7.26)
TB TB k=1 TB,k TB Πν p (η ;n∆t)
Rν k=1 TB,k k
in which p is the pdf of component H of H and where p (·;n∆t) is the pdf of component H (n∆t) of
DB,k DB,k DB TB,k TB,k
H (n∆t).
TB
(iii) Entropy for RODB and ROTB(n∆t). The entropy and its estimation from a set of realizations are presented in
Appendix B.3.ForthetrainingdatasetandforPLoMformulatedwithRODBandROTB(n∆t),wehaveforRν-valued
randomvariablesH,H ,andH (n∆t),
DB TB
(cid:90) (cid:90)
S =− p (η) logp (η)dη , S =− p (η) logp (η)dη, (7.27)
H H H DB DB DB
Rν Rν
(cid:90)
S (n∆t)=− p (η;n∆t) logp (η;n∆t)dη. (7.28)
TB TB TB
Rν
247.4. IdentificationmethodologyfortheinstantmaximizingtheselectioncriterionoftheROTB(n∆t)
Inthissection,allquantitiesdenotedwithahat(suchasIˆ(H))representtheestimatedvaluesofthecorresponding
quantitieswithoutahat(suchas I(H)),usingtherealizationsasexplainedinAppendix B. Letn bethenumberof
d
pointsinthetrainingdatasetandn =n ×n ≫n bethenumberoflearnedrealizationswithPLoM(seeAppendix
ar d MCH d
A.3),eitherwiththeRODBorwiththeROTB(n∆t).
(i)DefiningthesubsetC ofN containingtheadmissiblevaluesofn. Let N bethelargestvalueofnforwhichthe
N
ROTB(n∆t), representedbymatrix[g(n∆t)], iscomputed. Thisvalue N beingfixed, thesetN = {1,...,N}defined
byEq.(5.24)isfixed. ThesubsetC oftheadmissiblevaluesofnisthendefinedby
N
1
C ={n∈N, dˆ2(m ;n∆t)≤τ ≪1}, (7.29)
N ν TB opt c
wheredˆ2(m ;n∆t)isdefinedbyEq.(7.21)andwhere0<τ ≪1isfixedsufficientlysmallwithrespectto1inorder
TB opt c
topreservetheconcentrationofthelearnedprobabilitymeasure(seeSection7.3-(i)andAppendix A.4). Itshouldbe
notedthatthemaximumvalueofd2(n )is2(correspondingtoameasureconcentrationcompletelylost,whichcan
DB d
beobtainedwithclassicalMCMCalgorithms). ThePLoMalgorithmallowsforpreservingtheconcentrationofthe
learnedprobabilitymeasure[27]yieldingvaluesoftheorder0.01to0.1throughalltheperformedapplications. For
definingtheconcentrationcriterioninEq.(7.29),wehavenormalizedwithrespecttodimensionν.
(ii)CharacterizingabetterROTB(n∆t)comparedtoRODB.FromProposition7, inparticularfromEq.(7.7), itcan
bededucedthat,forn = 1,wehaveIˆ(H ;1×∆t) ≃ Iˆ(H )for∆tsufficientlysmall(whichistheconsideredcase).
TB DB
LetusassumethatIˆ(H)< Iˆ(H ). FornfixedinC ,wewillsaythattheROTB(n∆t),representedbymatrix[g(n∆t)],
DM N
isbetterthantheRODB,representedbymatrix[g ],if
DM
Iˆ(H)≤ Iˆ(H ;n∆t)< Iˆ(H ). (7.30)
TB DM
(iii)Determiningtheoptimalvaluen ofninC . Theunderlyingideaistoselectareduced-ordertransientbasisthat
opt N
givesalearnedprobabilitymeasurewhosemutualinformationisascloseaspossibletothemutualinformationofthe
probabilitymeasure p (η),dη. Basedonparagraph(ii)above, theoptimaltransientbasis[g(n )]amongthesetof
H opt
possibletransientbases{[g(n∆t)],n∈N}isobtainedforn=n suchthat
opt
n =argmin Iˆ(H ;n∆t). (7.31)
opt n∈C
N
TB
(iv)Definingthenormalizedestimateofthemutualinformation.AsexplainedinAppendix B.3,ifn isthenumber
samp
ofrealizations(eithern orn ),theentropyestimationasymptoticallydecreasesas−log(n )whenn increases.
d ar samp samp
For comparing the estimate of the mutual information for H, which uses n realizations, with the one of H or
d DB
H (n∆t),whichusesn ≫n learnedrealizations,wemustnormalizetheestimatedmutualinformationwithrespect
TB ar d
tothenumberofrealizations. Takingintoaccounttherelationshipbetweenthemutualinformationandtheentropy
estimates(seeAppendix B.3),wechosetonormalizetheestimatedmutualinformationbydividingbythefunction
χ+log(n )(withn = n orn = n ), whereχisarealnumberthatmustbeidentifiedandwhichmustbe
samp samp d samp ar
suchthatχ+log(n )>0. Sincen >n thisconditionimpliesthatχ+log(n )>0isautomaticallyverified. Forthe
d ar d ar
Rν-valuedrandomvariablesH,H ,andH (n∆t),thenormalizedestimatedmutualinformationarethendefinedby
DB TB
Iˆ(H) Iˆ(H ) Iˆ(H ;n∆t)
Iˆ (H)= , Iˆ (H )= DB , Iˆ (H ;n∆t)= TB . (7.32)
norm χ+log(n ) norm DB χ+log(n ) norm TB χ+log(n )
d ar ar
inwhichIˆ(H), Iˆ(H ),andIˆ(H ;n∆t)aretheestimates(computedwithEq.(B.6))ofthemutualinformationI(H),
DB TB
I(H ), and I(H ;n∆t) defined by Eqs. (7.24) to (7.26). Based on paragraph (iii) above, constant χ is defined as
DB TB
follows. Letχ bethesolutioninχoftheequation,
opt
Iˆ(H) Iˆ(H ;n ∆t)
= TB opt . (7.33)
χ+log(n ) χ+log(n )
d ar
25If χ +log(n ) > 0, then χ is the desired value of χ. It should be noted that the optimal value n is calculated
opt d opt opt
withtheestimationofthenon-normalizedmutualinformation(seeEq.(7.31). Normalizationisonlyintroducedfor
thepurposeofcomparingthevalueofthiscriterionforHandH .
TB
(v)IdentificationoftheinstantmaximizingtheselectioncriterionoftheROTB(n∆t). Letn bedefinedbyEq.(7.31),
opt
andletχ beidentifiedbysolvingEq.(7.33). Then,wehave
opt
Iˆ (H)= Iˆ (H ;n ∆t)≤ Iˆ (H ;n∆t) , ∀n∈C . (7.34)
norm norm TB opt norm TB N
WecanthenconcludethattheROTB(n ∆t),representedbymatrix[g(n ∆t)],isbetterthantheRODB,represented
opt opt
bymatrix[g ]. Theproofisstraightforward.
DM
8. Numericalapplications
8.1. Preamble
We will present three applications, each with its own specificities. However, as the generation of the training
datasetrelatedtotherandomvector Xisrelativelycomplextodescribe,wewillstartwiththetrainingdatasetrelated
tothenormalizedrandomvectorHwithvaluesinRν. Thenumberofrealizationsinthetrainingdatasetisn ,andthe
d
trainingdatasetisrepresentedbythematrix[η ] ∈ M . ThismeansthatthePCAstepofPLoM,whichtransforms
d ν,nd
Xinto H(seeAppendix A.1),isnotdetailedhere. Readersinterestedinthetrainingdatasetfor Hcanrequest,from
the”correspondingauthor”ofthearticle,thetransferofthematrix[η ]foreachapplicationpresented. Wewillstill
d
brieflygivethemainspecificitiesofthese3applications.
Application1. Thisapplication(Appli1)wascreatedsothattheprobabilitymeasureof H inR9 (ν = 9), which
isdefinedbythepointsofthetrainingdataset,isconcentratedinamulticonnecteddomainofR9,withtheconstituent
connected parts being manifolds of dimensions much lower than 9, each having different dimensions. These parts
mayormaynotbeconnectedtoeachother.
Application2. Forthesecondapplication(Appli2),then realizationsoftherandomvector HwithvaluesinR8
d
are generated using a polynomial chaos expansion of degree 6 of a real random variable, whose random germ is of
dimension 2, with each of the two random germs being a uniform random variable of different support. There are
therefore28termsinthisexpansion,andthe8componentsof H aredefinedastherandomtermsofrank2,3,6,8,
12,13,17,and19. WethusdefinearelativelycomplexrandommanifoldinR8.
Application 3. The third application (Appli 3) results from a statistical treatment of an experimental database
containing photon measurements in the ATLAS detector at CERN. This dataset was obtained by loading the file
’pid22˙E262144˙eta˙20˙25˙voxalisation.csv’fromthefreeaccessCERNOpenDataPortal. ThePCAstepofPLoM
has been performed, and an extraction of 45 components has been done to obtain the training dataset for the R45-
valuedrandomvariable H. Thisapplicationisinhigherdimensionthanthefirsttwo,butthestatisticalcomplexityis
less.
8.2. Additionalconvergenceanalysisconductedforthethreeapplications
Foreachofthethreeapplications,wewillprovidedetailedanalysesofthecalculationoftheoptimalvalueofthe
instantn ∆t, whichallowsfortheselectionofthebestreduced-ordertransientbasis(ROTB).Wewillalsopresent
opt
theconvergenceresultsofthePLoMalgorithmsunderthenormalizationconstraints(seeAppendix A.6). However,
wecannotshowalltheconvergenceresultsfortheotherparameters.Belowarethedifferentanalysesthatwerecarried
outwithrespecttotheparametersthatcontroltheconstructionofthereduced-ordertransientbasis:
(i)Convergencewithrespecttothevalueofκ. Thispointisimportantbecause∆tmustbesufficientlysmalltoapply
Proposition7. Forthethreeapplications,wefoundthatκ=30isanappropriatevalue.
(ii) Convergence with respect to the size of δt, that is to say, the value of n . For the three applications, we found
s
thatonce∆tisfixedbythevalueofκ,thecriterionassociatedwithEq.(5.30)wassatisfiedforn = 1,aswellasthe
s
evolutionoftheangleγ(n ∆t)attheoptimalinstantwasveryinsensitivetothevaluesofn greaterthan1. Thisis
opt s
26duetothefactthat∆tisalreadysmallenoughtoachievegoodaccuracywiththeEulerschemeusedtointegratethe
ISDE.Wefoundthatn =1,thusδt=∆tisagoodvalue.
s
(iii)Convergencewithrespectton . Thisanalysiswasperformedbyexaminingtheconvergenceoftheangleγ(n∆t)
MC
for n ∈ N. We observed that a large value of n was necessary to achieve convergence. This point is particularly
MC
importantandmustbecarefullycheckedfortheapplications.
8.3. ParametersdefiningthetrainingdatasetandcontrollingtheconstructionoftheRODB
Foreachapplication,Table1providesthevaluesoftheparametersthatdefinethetrainingdatasetandtheprob-
ability measure P (dη) = p (η)dη on Rν, as well as the parameters defined in Section 7.2-(i), which control the
H H
constructionofthereduced-orderDMAPSbasis(RODB).
Table1:Foreachapplication,valuesoftheparametersdefiningthetrainingdatasetandparameterscontrollingtheconstructionoftheRODB
Training RODB
ν n s sˆ sˆ/s J m ε
d DM opt DM
Appli1 9 400 0.5835 0.5044 0.8645 0.2 10 56
Appli2 8 400 0.5623 0.4946 0.8725 0.1 10 53
Appli3 46 560 0.8357 0.6416 0.7677 0.5 46 74
8.4. ParameterscontrollingtheconstructionoftheROTB(n∆t)
Foreachapplication, Table2providesthevaluesoftheparametersthatcontroltheconstructionofthereduced-
order transient basis ROTB(n∆t). In particular, the optimal time n ∆t is estimated using the indicators defined in
opt
Section7.3.2. TheevolutionoftheseindicatorsasafunctionofnwillbepresentedindetailinSections8.6to8.8for
thethreeapplications.
Table2:Foreachapplication,valuesoftheparameterscontrollingtheconstructionoftheROTB(n∆t)
ROTB(n∆t)
N τ κ ∆t n δt n n
c s MC opt
Appli1 9 0.002 30 0.00848 1 0.00848 400000 9
Appli2 10 0.002 30 0.00802 1 0.00802 400000 5
Appli3 9 0.002 30 0.01370 1 0.01370 448000 9
8.5. ParametersofPLoMwithRODBandROTB
For each of the three applications, Table 3 provides the values of the parameters defined in Appendix A.5 and
Appendix A.6 used by the PLoM algorithm with the reduced-order DMAPS basis (RODM) or the reduced-order
transientbasisROTB(n∆t). Takingintoaccounttheconvergenceanalysiscarriedoutonn andtheexpertiseonthe
MC
statistical convergence of the quantities considered, we choose n = n , a sufficiently large value (see Appendix
MCH MC
A.5). ForthePLoMalgorithm,theconstraintsrelatedtonormalization,definedbyEq.(A.12),arealwaysappliedin
thecomputation(seeAppendix A.6)
27Table3:ValuesoftheparametersofPLoMwithRODBandROTB(n∆t)foreachapplication
PLoMwithRODBandwithROTB(n ∆t)
opt
f ∆t M n n β β i i err(i )
0 SV 0 MCH ar 1 2 2 last last
Appli1withRODB 4 0.1585 30 1000 400000 0.001 0.05 20 2563 0.000998
Appli1withROTB 4 0.1585 30 1000 400000 0.001 0.05 20 3438 0.000994
Appli2withRODB 4 0.1541 30 1000 400000 0.001 0.05 20 1882 0.000999
Appli2withROTB 4 0.1541 30 1000 400000 0.001 0.05 20 2589 0.000998
Appli3withRODB 4 0.2016 30 800 448000 0.001 0.05 20 6000 0.00261
Appli3withROTB 4 0.2016 30 800 448000 0.001 0.05 20 6000 0.00383
8.6. ResultsforApplication1
(i)Figure3displaysthegraphsoftheprobabilitydensityfunction(pdf)ofcomponents1,2,4,and5forHestimated
withthen realizationsofthetrainingdataset,andthepdfestimatedwithn learnedrealizations,forH usingMCMC
d ar ar
withoutPLoM(a,d,g,j),for H usingPLoMwithRODB(b,e,h,k),andfor H usingPLoMwithROTB(n ∆t)(c,
DB TB opt
f,i,l).
(ii)Figure4showsthejointprobabilitydensityfunctionofcomponents4and5ofHestimatedwiththen realizations
d
ofthetrainingdataset(a)andestimatedwithn learnedrealizationsforH usingMCMCwithoutPLoM(b),forH
ar ar DB
usingPLoMwithRODB(c),andforH usingPLoMwithROTB(n ∆t)(d).
TB opt
(iii) In Fig. 5, the clouds of n points corresponding to n learned realizations can be seen for components 1, 2, 3
ar ar
(a,b,c) and components 3, 4, 5 (d,e,f). These are shown for H using MCMC without PLoM (a,d), for H using
ar DB
PLoMwithRODB(b,e),andforH usingPLoMwithROTB(n ∆t)(c,f).
TB opt
(iv)Figure6plotsthefunctionsthatcharacterizethereduced-ordertransientbasisROTB(n∆t)asafunctionoftime
n∆t:
• Theeigenvaluesofmatrix[K ]andthoseoftheofsymmetrizedmatrix[K˜(n∆t)]areshowninFig.6a.
DM
• The probability-measure concentration using the d2/ν-criterion is shown in Fig. 6b. For the learning without
PLoM,thed2-concentrationis0.6465,whichshowsthattheconcentrationislost,andforthePLoMwiththe
RODM,theconcentrationis0.0116,whichshowsthattheconcentrationispreserved.
• Theothercriterionoftheprobability-measureconcentrationisgivenbyKullbackmeasure, showninFig.6c.
For the learning without PLoM, Kullbach is 0.3486, and for the PLoM with the RODM, Kullback is 3.9435.
ComparingFigs.6band6cshowsthatthetwocriteriaareconsistentandgivethesameanalysisoftheconcen-
tration.
• The angle between the subspaces spanned by RODB and ROTB(n∆t) is displayed in Fig. 6d. It can be seen
that,fortheoptimaltime9∆t,theangleis53.03◦,whichisasignificantangleshowingthatthetwobasesare
differentwhilethed2-concentrationremainssmallat0.0174.
• Theentropyestimationofpdf p (·;n∆t)isgiveninFig.6e.
TB
• Thenormalizedmutualinformation(MI)ofthepdfs p and p (·;n∆t)isshowninFig.6f. Thisfigureshows
H TB
thattheoptimalvalueofnisn = 9. Forthenon-normalizedestimationofthemutualinformation,wehave
opt
Iˆ(H) = 4.4668, Iˆ(H ;n ∆t) = 6.9996, and Iˆ(H ) = 7.0945. For the normalized one, we have Iˆ (H) =
TB opt DM norm
Iˆ (H ;n ∆t)=0.3666andIˆ (H )=0.3716.
norm TB opt norm DM
(v)Finally,examinationofthesefiguresshowsthattraditionallearningwithoutPLoMgivespoorresultscomparedto
PLoM,whichallowstheconcentrationtobepreservedandproperlylearnsthegeometryoftheprobabilitymeasure
support. We also see that PLoM with the optimal ROTB provides an improvement in learning compared to PLoM
withRODMand,therefore,shouldimprovetheestimatesofconditionalstatisticsthankstobetterlearningofthejoint
probabilitymeasure.
281.5 1.5 1.5
1 1 1
0.5 0.5 0.5
0 0 0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
(a)pdfofH1andHar,1. (b)pdfofH1andHDB,1. (c)pdfofH1andHTB,1attimenopt∆t.
2 2 2
1.5 1.5 1.5
1 1 1
0.5 0.5 0.5
0 0 0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
(d)pdfofH2andHar,2. (e)pdfofH2andHDB,2. (f)pdfofH2andHTB,2attimenopt∆t.
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0 0 0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
(g)pdfofH4andHar,4. (h)pdfofH4andHDB,4. (i)pdfofH4andHTB,4attimenopt∆t.
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0 0 0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
(j)pdfofH5andHar,5. (k)pdfofH5andHDB,5. (l)pdfofH5andHTB,5attimenopt∆t.
Figure3: Application1. Probabilitydensityfunction(pdf)ofcomponents1,2,4,and5forHestimatedwiththend realizationsofthetraining
dataset(thinblackline)andpdfestimatedwithnarlearnedrealizations(thickblueline),forHarusingMCMCwithoutPLoM(a,d,g,j),forH
DB
usingPLoMwithRODB(b,e,h,k),andforH TBusingPLoMwithROTB(nopt∆t)(c,f,i,l).
8.7. ResultsforApplication2
(i)Figure7displaysthegraphsoftheprobabilitydensityfunction(pdf)ofcomponents3,5,6,and8forHestimated
withthen realizationsofthetrainingdataset,andthepdfestimatedwithn learnedrealizations,forH usingMCMC
d ar ar
withoutPLoM(a,d,g,j),for H usingPLoMwithRODB(b,e,h,k),andfor H usingPLoMwithROTB(n ∆t)(c,
DB TB opt
f,i,l).
(ii)Figure8showsthejointprobabilitydensityfunctionofcomponents6and8ofHestimatedwiththen realizations
d
ofthetrainingdataset(a)andestimatedwithn learnedrealizationsforH usingMCMCwithoutPLoM(b),forH
ar ar DB
usingPLoMwithRODB(c),andforH usingPLoMwithROTB(n ∆t)(d).
TB opt
(iii) In Fig. 9, the clouds of n points corresponding to n learned realizations can be seen for components 2, 5, 6
ar ar
(a,b,c) and components 3, 6, 8 (d,e,f). These are shown for H using MCMC without PLoM (a,d), for H using
ar DB
29(a)jointpdfofH4withH5. (b)jointpdfofHar,4withHar,5. (c)jointpdfofHDB,4withHDB,5. (d)jointpdfofHTB,4withHTB,5attime
nopt∆t.
Figure4:Application1. Jointprobabilitydensityfunctionofcomponents4with5ofHestimatedwiththendrealizationsofthetrainingdataset
(a)andestimatedwithnarlearnedrealizations,forHarusingMCMCwithoutPLoM(b),forH DBusingPLoMwithRODB(c),andforH TBusing
PLoMwithROTB(nopt∆t)(d).
PLoMwithRODB(b,e),andforH usingPLoMwithROTB(n ∆t)(c,f).
TB opt
(iv)Figure10plotsthefunctionsthatcharacterizethereduced-ordertransientbasisROTB(n∆t)asafunctionoftime
n∆t:
• Theeigenvaluesofmatrix[K ]andthoseoftheofsymmetrizedmatrix[K˜(n∆t)]areshowninFig.10a.
DM
• Theprobability-measureconcentrationusingthed2/ν-criterionisshowninFig.10b. Forthelearningwithout
PLoM, the d2-concentration is 0.951, which shows that the concentration is lost, and for the PLoM with the
RODM,theconcentrationis0.0091,whichshowsthattheconcentrationispreserved.
• Theothercriterionoftheprobability-measureconcentrationisgivenbyKullbackmeasure,showninFig.10c.
For the learning without PLoM, Kullbach is 0.4594, and for the PLoM with the RODM, Kullback is 3.4473.
ComparingFigs.10band10cshows,similarlytoApplication1thatthetwocriteriaareconsistentandgivethe
sameanalysisoftheconcentration.
• TheanglebetweenthesubspacesspannedbyRODBandROTB(n∆t)isdisplayedinFig.10d. Itcanbeseen
that,fortheoptimaltime5∆t,theangleis17.1◦,whichissignificant,althoughlessthantheoptimalangleof
Application1. Thisshowsthatthetwobasesaredifferentwhilethed2-concentrationremainssmallat0.0153.
• Theentropyofpdf p (·;n∆t)isgiveninFig.6e.
TB
• Thenormalizedmutualinformation(MI)ofthepdfs p and p (·;n∆t)isshowninFig.10f. Thisfigureshows
H TB
that the optimal value of n is n = 5. Unlike Application 1, the normalized mutual information presents a
opt
localminimum,whichisalsoaglobalminimumovertheadmissiblesetC . Forthenon-normalizedestimation
N
of the mutual information, we have Iˆ(H) = 3.4994, Iˆ(H ;n ∆t) = 5.8763, and Iˆ(H ) = 5.9382. For the
TB opt DM
normalizedone,wehaveIˆ (H)= Iˆ (H ;n ∆t)=0.3441,andIˆ (H )=0.3477.
norm norm TB opt norm DM
(v)AsforApplication1,examinationofthesefiguresshowsthattraditionallearningwithoutPLoMgivespoorresults
comparedtoPLoM,whichallowstheconcentrationtobepreservedandproperlylearnsthegeometryoftheprobability
measuresupport. WealsoseethatPLoMwiththeoptimalROTBprovidesanimprovementinlearningcomparedto
PLoMwithRODM.However,thisimprovementislessthaninthecaseofApplication1forwhichthedataaremuch
moreheterogeneous(incorrelationwiththegeometriccomplexityoftheprobability-measuresupport). Nevertheless,
PLoM with the optimal ROTB is an improvement over PLoM with RODB and consequently, should improve the
estimatesofconditionalstatisticsthankstobetterlearningofthejointprobabilitymeasure.
8.8. ResultsforApplication3
(i) Figure 11 displays the graphs of the probability density function (pdf) of components 1, 6, 25, and 40 for H
estimatedwiththen realizationsofthetrainingdataset, andthepdfestimatedwithn learnedrealizations, for H
d ar ar
using MCMC without PLoM (a,d,g,j), for H using PLoM with RODB (b, e, h, k), and for H using PLoM with
DB TB
ROTB(n ∆t)(c,f,i,l).
opt
30(a)cloudsfor(Har,1,Har,2,Har,3). (b)cloudsfor(HDB,1,HDB,2,HDB,3). (c)cloudsfor(HTB,1,HTB,2,HTB,3)attime
nopt∆t.
(d)cloudsfor(Har,3,Har,4,Har,5). (e)cloudsfor(HDB,3,HDB,4,HDB,5). (f)cloudsfor(HTB,3,HTB,4,HTB,5)attime
nopt∆t.
Figure5:Application1.Cloudsofnarpointscorrespondingtonarlearnedrealizations,forcomponents1,2,3(a,b,c)andcomponents3,4,5(d,e,f),
forHarusingMCMCwithoutPLoM(a,d),forH DBusingPLoMwithRODB(b,e),andforH TBusingPLoMwithROTB(nopt∆t)(c,f).
(ii) Figure 12 shows the joint probability density function of components 25 and 40 of H estimated with the n
d
realizationsofthetrainingdataset(a)andestimatedwithn learnedrealizationsforH usingMCMCwithoutPLoM
ar ar
(b),forH usingPLoMwithRODB(c),andforH usingPLoMwithROTB(n ∆t)(d).
DB TB opt
(iii)InFig.13,thecloudsofn pointscorrespondington learnedrealizationscanbeseenforcomponents1,6,12
ar ar
(a,b,c)andcomponents12,25,40(d,e,f). Theseareshownfor H usingMCMCwithoutPLoM(a,d),for H using
ar DB
PLoMwithRODB(b,e),andforH usingPLoMwithROTB(n ∆t)(c,f).
TB opt
(iv)Figure14plotsthefunctionsthatcharacterizethereduced-ordertransientbasisROTB(n∆t)asafunctionoftime
n∆t:
• Theeigenvaluesofmatrix[K ]andthoseoftheofsymmetrizedmatrix[K˜(n∆t)]areshowninFig.14a.
DM
• Theprobability-measureconcentrationusingthed2/ν-criterionisshowninFig.14b. Forthelearningwithout
PLoM, the d2-concentration is 0.574, which shows that the concentration is lost, and for the PLoM with the
RODM,theconcentrationis0.067,whichshowsthattheconcentrationispreserved.
• Theothercriterionoftheprobability-measureconcentrationisgivenbyKullbackmeasure,showninFig.14c.
For the learning without PLoM, Kullbach is 14.35, and for the PLoM with the RODM, Kullback is 5.72.
ComparingFigs.14band14cshows,similarlytoApplications1and 2thatthetwocriteriaareconsistentand
givethesameanalysisoftheconcentration.
• TheanglebetweenthesubspacesspannedbyRODBandROTB(n∆t)isdisplayedinFig.14d. Itcanbeseen
that, for the optimal time 9∆t, the angle is 9.7◦, which is significant, although less than the optimal angle of
Applications1and 2. Thisshowsthatthetwobasesaredifferentwhilethed 2-concentrationremainssmallat
0.078.
• Theentropyofpdf p (·;n∆t)isgiveninFig.14e.
TB
• Thenormalizedmutualinformation(MI)ofthepdfs p and p (·;n∆t)isshowninFig.14f. Thisfigureshows
H TB
thattheoptimalvalueofnisn = 9. Thebehaviorofthenormalizedmutualinformationissimilartothatof
opt
Application 1 and does not present a local minimum as in Application 2. For the non-normalized estimation
31100 10-3 4
2
3.99
1.8
3.98
10-5 1.6 3.97
3.96
1.4
3.95
10-10 0 2 4 6 8 10 12 1.2 0 2 4 6 8 3.94 0 2 4 6 8 10
(a)eigenvaluesβ (cid:55)→ bˆ DM,β(square)and (b)n(cid:55)→dˆ2(mopt;n∆t)/ν. (c)n(cid:55)→Dˆ(pTB(·,n∆t)∥pH)(Kullback).
β(cid:55)→b˜ β(noptδt)(circle).
60 2.77 0.372
50 2.765 0.371
40 2.76 0.37
30 2.755 0.369
20 2.75 0.368
10 2.745 0.367
0 0 2 4 6 8 10 2.74 0 2 4 6 8 10 0.366 0 2 4 6 8 10
(d)n(cid:55)→γ(n∆t)(angleindegree). (e)n(cid:55)→Sˆ TB(n∆t)(entropy). (f) n (cid:55)→ Iˆ norm(H) (square) and n (cid:55)→
Iˆ norm(HTB;n∆t)(circle)(normalizedMI).
Figure6: Application1. Functionscharacterizingthereduced-ordertransientbasisROTB(n∆t)asafunctionoftimen∆t: eigenvaluesof[K ]
DM
andofsymmetrized[K˜(n∆t)](a);probability-measureconcentrationwithd2/ν-criterion(b)andwithKullbackcriterion(c);anglebetweenthe
subspacesspannedbyRODBandROTB(n∆t)(d);entropyofpdfp TB(·;n∆t)(e);normalizedmutualinformation(MI)ofpdfpHandp TB(·;n∆t)
(f).
of the mutual information, we have Iˆ(H) = 24.167, Iˆ(H ;n ∆t) = 25.115, and Iˆ(H ) = 25.673. For the
TB opt DM
normalizedone,wehaveIˆ (H)= Iˆ (H ;n ∆t)=0.1418andIˆ (H )=0.1450.
norm norm TB opt norm DM
(v)AsforApplications1and2,examinationofthesefiguresshowsthattraditionallearningwithoutPLoMgivespoor
resultscomparedtoPLoM,whichallowstheconcentrationtobepreservedandproperlylearnsthegeometryofthe
probability measure support. We also see that PLoM with the optimal ROTB provides an improvement in learning
compared to PLoM with RODM. However, this improvement is less than in the case of Applications 1 and 2. For
thisapplication,relativetoarelativelyhighdimensionofν = 45,thedataaremorehomogeneousthanfortheother
applications(incorrelationwiththegeometriccomplexityoftheprobability-measuresupport). Nevertheless,PLoM
withtheoptimalROTBisanimprovementoverPLoMwithRODBand,consequently,shouldimprovetheestimates
ofconditionalstatisticsthankstobetterlearningofthejointprobabilitymeasure.
9. Conclusion
In this paper, we have presented the theoretical elements of constructing a time-dependent anisotropic kernel,
whichallowsustocreateadataprojectionbasisforPLoM.ThisbasisservesasanalternativetotheDMAPSbasis
built using a time-independent isotropic kernel used by PLoM. We have demonstrated that an optimal time can be
determinedtoobtainanoptimaltransientbasis,bestrespectingthestatisticaldependencebetweenthecomponentsfor
thelearnedjointprobabilitymeasure.
TheproposedtheoryhasbeendevelopedtoimprovePLoMincasesofhighlyheterogeneousdata. Theimprove-
mentofthelearnedjointprobabilitymeasureisquantifiedbyestimatinganobjectivecriterionfrominformationtheory,
namelythemutualinformation,whichwehavenormalizedrelativetothenumberofrealizationsusingentropy.
Thistheoryisconsistentinthesensethat,foratimeclosetotheinitialtime,theDMAPSbasisconstructedwiththe
time-independentisotropickernelcoincideswiththetransientbasisconstructedwiththetime-dependentanisotropic
kernel. Thus, we can characterize the difference between the two bases by the angle of the vector subspaces they
generate.
321.5 1.5 1.5
1 1 1
0.5 0.5 0.5
0 0 0
-2 0 2 4 -2 0 2 4 -2 0 2 4
(a)pdfofH3andHar,3. (b)pdfofH3andHDB,3. (c)pdfofH3andHTB,3attimenopt∆t.
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0 0 0
-2 0 2 4 -2 0 2 4 -2 0 2 4
(d)pdfofH5andHar,5. (e)pdfofH5andHDB,5. (f)pdfofH5andHTB,5attimenopt∆t.
0.6 0.6 0.6
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
0 0 0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
(g)pdfofH6andHar,6. (h)pdfofH6andHDB,6. (i)pdfofH6andHTB,6attimenopt∆t.
0.6 0.6 0.6
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
0 0 0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
(j)pdfofH8andHar,8. (k)pdfofH8andHDB,8. (l)pdfofH8andHTB,8attimenopt∆t.
Figure7: Application2. Probabilitydensityfunction(pdf)ofcomponents3,5,6,and8forHestimatedwiththend realizationsofthetraining
dataset(thinblackline)andpdfestimatedwithnarlearnedrealizations(thickblueline),forHarusingMCMCwithoutPLoM(a,d,g,j),forH
DB
usingPLoMwithRODB(b,e,h,k),andforH TBusingPLoMwithROTB(nopt∆t)(c,f,i,l).
The theory is illustrated through three applications with decreasing levels of data heterogeneity. The three ap-
plications confirmthat PLoMwith the DMAPSbasis (time-independentisotropic kernel)always results inlearning
thatpreservestheconcentrationofthemeasure,unliketheclassicMCMCapproach. Theapplicationsshowthatitis
possibletoimprovethelearnedjointprobabilitymeasurewiththetransientanisotropickernel,whichaprioriallows
forbetterestimatesofconditionalstatistics.
Acknowledgments
TheauthorsacknowledgepartialfundingfromDOESciDACFASTMathInstitute,andanONRMURIonMod-
elingTurbulenceandChemistryinHighSpeedReactiveFlows.
33(a)jointpdfofH6withH8. (b)jointpdfofHar,6withHar,8. (c)jointpdfofHDB,6withHDB,8. (d)jointpdfofHTB,6withHTB,8attime
nopt∆t.
Figure8:Application2. Jointprobabilitydensityfunctionofcomponents6with8ofHestimatedwiththendrealizationsofthetrainingdataset
(a)andestimatedwithnarlearnedrealizations,forHarusingMCMCwithoutPLoM(b),forH DBusingPLoMwithRODB(c),andforH TBusing
PLoMwithROTB(nopt∆t)(d).
Appendix A. Overviewoftheprobabilisticlearningonmanifolds(PLoM)algorithmanditsparameterization
The PLoM approach [1, 27, 29], which has specifically been developed for small data (as opposed to big data)
startsfromatrainingdatasetD madeupofarelativelysmallnumbern ofpoints. ItisassumedthatD isgenerated
d d d
withanunderlyingstochasticmanifoldrelatedtoaRnx-valuedrandomvariable X = (Q,W),definedonaprobability
space(Θ,T,P),inwhichQisthequantityofinterestthatisaRnq-randomvariable,whereWisthecontrolparameter
thatisaRnw-randomvariable,andwheren
x
= n q+n w. AnotherRnu-valuedrandomvariableUdefinedon(Θ,T,P)
isalsobeconsidered,whichisanuncontrolledparameterand/oranoise. RandomvariableQisassumedtobewritten
as Q = f(U,W) in which the measurable mapping f is not explicitly known. The joint probability distribution
P (dw,du)ofW andU isassumedtobegiven. Thenon-Gaussianprobabilitymeasure P (x) = P (dq,dw)of
W,U X Q,W
X = (Q,W)isconcentratedinaregionofRnx forwhichtheonlyavailableinformationisthecloudofthepointsof
trainingdatasetD . ThePLoMmethodmakesitpossibletogeneratethelearneddatasetD for X whosen ≫ n
d ar ar d
points(learnedrealizations)aregeneratedbythenon-Gaussianprobabilitymeasurethatisestimatedusingthetraining
dataset. The concentration of the probability measure is preserved thanks to the use of a diffusion-maps basis that
allows to enrich the available information from the training dataset. The training dataset D is made up of the n
d d
independent realizations x dj = (q dj,w dj) in Rnx = Rnq × Rnw for j ∈ {1,...,n d} of random variable X = (Q,W).
The PLoM method allows for generating the learned dataset D made up of n ≫ n learned realizations {xℓ,ℓ =
ar ar d ar
1,...,n }ofrandomvectorX. Assoonasthelearneddatasethasbeenconstructed,thelearnedrealizationsforQand
ar
W canbeextractedas(qℓ,wℓ) = xℓ forℓ = 1,...,n . UsingthelearneddatasetD ,PLoMallowsforcarryingout
ar ar ar ar ar
any conditional statistics such as w (cid:55)→ E{ξ(Q)|W = w} from C
w
in Rnξ, in which ξ is a given measurable mapping
fromRnq intoRnξ,thatistosaytoconstructstatisticalsurrogatemodels(metamodels)inaprobabilisticframework.
Appendix A.1. Reducedrepresentation
Then independentrealizations{xj, j = 1,...,n }arerepresentedbythematrix[x ] = [x1...xnd]inM . Let
d d d d d d nx,nd
[X]=[X1,...,Xnd]betherandommatrixwithvaluesinM nx,nd,whosecolumnsaren
d
independentcopiesofrandom
vectorX. UsingthePCAofX,randommatrix[X]iswrittenas,
[X]=[x]+[φ][ζ]1/2[H], (A.1)
inwhich[H] = [H1,..., Hnd]isaM ν,nd-valuedrandommatrix,whereν ≤ n x,andwhere[ζ]isthe(ν×ν)diagonal
matrixoftheνpositiveeigenvaluesoftheempiricalestimateofthecovariancematrixof X. The(n ×ν)matrix[φ]
x
ismadeupoftheassociatedeigenvectorssuch[φ]T[φ] = [I ]. Thematrix[x]inM hasidenticalcolumns, each
ν nx,nd
one being equal to the empirical estimate x ∈ Rnx of the mean value of random vector X. The columns of [H] are
n
d
independentcopiesofarandomvector H withvaluesinRν. Therealization[η d] = [η1...ηnd] ∈ M
ν,nd
of[H]is
computedby[η ] = [ζ]−1/2[φ]T([x ]−[x]). Thevalueνisclassicallycalculatedinorderthatthe L2-errorfunction
d d
ν(cid:55)→err (ν)definedby
X (cid:80)ν
ζ
err (ν)=1− α=1 α , (A.2)
X E{∥X∥2}
besmallerthanε . Ifν<n ,thenthereisastatisticalreduction.
PCA x
34(a)cloudsfor(Har,2,Har,5,Har,6). (b)cloudsfor(HDB,2,HDB,5,HDB,6). (c)cloudsfor(HTB,2,HTB,5,HTB,6)attime
nopt∆t.
(d)cloudsfor(Har,3,Har,6,Har,8). (e)cloudsfor(HDB,3,HDB,6,HDB,8). (f)cloudsfor(HTB,3,HTB,6,HTB,8)attime
nopt∆t.
Figure9:Application2.Cloudsofnarpointscorrespondingtonarlearnedrealizations,forcomponents2,5,6(a,b,c)andcomponents3,6,8(d,e,f),
forHarusingMCMCwithoutPLoM(a,d),forH DBusingPLoMwithRODB(b,e),andforH TBusingPLoMwithROTB(nopt∆t)(c,f).
Appendix A.2. Construction of a reduced-order diffusion-maps basis (RODB) and reduced-order transient basis
(ROTB(n∆t))
Inthissection,webeginwiththeconstructionoftheRODBthatisthebasisinitiallyusedinthePLoMalgorithm
(see[1]). ConcerningtheconstructionoftheROTB(n∆t),wereferthereadto7.2-(ii).
(i) Construction of RODB. This construction corresponds to the one initailly proposed in the PLoM algorithm. For
preserving the concentration of the learned realizations in the region in which the points of the training dataset are
concentrated, the PLoM relies on the diffusion-maps method [3, 97]. This is an algebraic basis of vector space
Rnd, which is constructed using the diffusion maps. Let [K ] and [B] be the matrices such that, for all i and j
DM
in {1,...,n }, [K ] = exp{−(4ε )−1∥ηi −ηj∥2} and [B] = δ exp{−(4ε )−1∥ηi −ηj∥2}, in which ε > 0 is
d DM ij DM ij ij DM DM
a smoothing parameter. The eigenvalues b ,...,b and the associated eigenvectors g1 ,...,gnd of the right-
DM,1 DM,nd DM DM
eigenvalue problem [B]−1[K ]gβ = b gβ are such that 1 = b > b ≥ ... ≥ b and are computed
by solving the eigenvalue prD oM blemDM [B]−1D /M 2, [β KDM ][B]−1/2ϕβ = b ϕD βM w,1 ith thD eM,2 normalizatioD nM, ⟨n ϕd β,ϕβ′⟩ = δ , and
DM DM,β ββ′
gβ = [B]−1/2ϕβ. The eigenvector g1 associated with b = 1 is a constant vector. The diffusion-maps basis
DM DM DM,1
{g1 DM,...,gα DM,...,gn Dd M}isavectorbasisofRnd. Foragivenintegerm<n d,thereduced-orderdiffusion-mapsbasisof
ordermisdefinedasthefamily{g1 ,...,gm }. Thisbasisdependsontwoparameters,ε andm,whichhavetobe
DM DM DM
identified. Asexplainedin[29],theoptimalvaluem ofm ischosenasm = ν+1,andtheoptimalvalueε of
opt DM opt opt
ε issuchthat
DM
1=b DM,1 >b DM,2 ≃...≃b DM,mopt ≫b DM,mopt+1 ≥...≥b DM,nd >0, (A.3)
withthejumpamplitude J
DM
= b DM,mopt+1/b DM,mopt,whichis J
DM
= 0.1(following[27]),butwhichcanalsobechosen
intheinterval[0.1,0.5]. Consequently,theRODBisdefinedform=m andisrepresentedbythematrix
opt
[g ]=[g1 ...gmopt]∈M . (A.4)
DM DM DM nd,mopt
(ii)ConstructionofROTB(n∆t).BecausePLoMwillalsousethereduced-ordertransientbasistoquantifyitsefficiency
relative to the reduced-order DMAPS basis, we introduce this basis in this Appendix. For n fixed in {1,...,N}, the
reduced-ordertransientbasis,ROTB(n∆t),isrepresentedbythematrix
[g(n∆t)]=[g1(n∆t)...gmopt(n∆t)]∈M nd,mopt, (A.5)
35100 10-3 3.6
3
2.5 3.55
10-5 2 3.5
1.5 3.45
10-10 0 2 4 6 8 10 12 1 0 2 4 6 8 10 3.4 0 2 4 6 8 10
(a)eigenvaluesβ (cid:55)→ bˆ DM,β(square)and (b)n(cid:55)→dˆ2(mopt;n∆t)/ν. (c)n(cid:55)→Dˆ(pTB(·,n∆t)∥pH)(Kullback).
β(cid:55)→b˜ β(noptδt)(circle).
60 3.74 0.348
50 3.72 0.347
40
0.346
3.7
30
0.345
20 3.68
10 0.344
3.66
0 0.343
0 2 4 6 8 10 0 2 4 6 8 10 0 2 4 6 8 10
(d)n(cid:55)→γ(n∆t)(angleindegree). (e)n(cid:55)→Sˆ TB(n∆t)(entropy). (f) n (cid:55)→ Iˆ norm(H) (square) and n (cid:55)→
Iˆ norm(HTB;n∆t)(circle)(normalizedMI).
Figure10:Application2.Functionscharacterizingthereduced-ordertransientbasisROTB(n∆t)asafunctionoftimen∆t:eigenvaluesof[K ]and
DM
ofsymmetrized[K˜(n∆t)](a);measureconcentrationwithd2/ν-criterion(b)andwithKullbackcriteria(c);anglebetweenthesubspacesspanned
byRODBandROTB(n∆t)(d);entropyofpdfp TB(·;n∆t)(e);normalizedmutualinformation(MI)ofpdfpHandp TB(·;n∆t)(f).
inwhichm istheoptimalvalueidentifiedinAppendix A.2-(i),andwhere[g(n∆t)]isconstructedinSection7.2-(ii)
opt
(seeEq.(7.16).
(iii) Reduced-order basis for PLoM. In this Appendix, the PLoM reduced-order basis will be represented by the
matrix[g ]∈M . Dependingonthecontextofitsuse,thismatrixwilleitherbe[g ],representingthereduced-
opt nd,mopt DM
orderDMAPSbasis(RODB)asusedintheinitialconstructionofthePLoM,or[g(n∆t)]forafixedn, representing
the reduced-order transient basis (ROTB(n∆t)) as proposed in this paper. The latter is introduced with the goal of
comparingtheefficiencyofthetworeduced-ordervectorbases.
Appendix A.3. Reduced-orderrepresentationoftherandommatrices
Thereduced-orderbasisrepresentedbymatrix[g opt] ∈ M
nd,mopt
spansasubspaceofRnd thatcharacterizes,forthe
optimalvaluesm andε ,thelocalgeometrystructureofdataset{ηj, j=1,...,n }.SothePLoMmethodintroduces
opt opt d
the M -valued random matrix [H ] = [Z][g ]T with m < n , corresponding to a data-reduction representation
ν,nd ar opt opt d
of random matrix [H], in which [Z] is a M -valued random matrix. The MCMC generator of random matrix
ν,mopt
[Z] belongs to the class of Hamiltonian Monte Carlo methods, is explicitly described in [1], and is mathematically
detailedinTheorem6.3of[27]. Thisgeneratorallowsforcomputingn realizations{[zℓ],ℓ = 1,...,n }of[Z]
MCH ar MCH
andtherefore,fordeducingthen realizations{[ηℓ],ℓ =1,...,n }of[H ]. Thereshapingofmatrix[ηℓ]∈M
allowsforobtainingn = n
×M nCH learnedrealizata ir
ons{ηℓ′,ℓ′ =
1M ,CH
...,n
}a or
f H .
Theselearnedrealizatia or nsalloν, wnd
ar MCH d ar ar ar
forestimatingconvergedstatisticson H andthenon X = x+[φ][ζ]1/2H ,suchaspdf,moments,orconditional
ar ar ar
expectationofthetypeE{ξ(Q)|W =w}forwgiveninRnw andforanygivenvector-valuedfunctionξdefinedonRnq.
Appendix A.4. Criterionforquantifyingtheconcentrationoftheprobabilitymeasureofrandommatrix[H ]
ar
Theconcentrationoftheprobabilitymeasureofrandommatrix[H ]isdefined(see[27])by
ar
d2(m )= E{∥[H ]−[η ]∥2}/∥[η ]∥2. (A.6)
opt ar d d
LetM={m ,m +1,...,n }inwhichm istheoptimalvalueofm.Theorem7.8of[27]showsthatmin d2(m)≤
opt opt d opt m∈M
1+m /(n −1) < d2(n ), whichmeansthatthePLoMmethod, form = m and[g ]isabettermethodthanthe
opt d d opt opt
362 2 2
1.5 1.5 1.5
1 1 1
0.5 0.5 0.5
0 0 0
-3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3
(a)pdfofH1andHar,1. (b)pdfofH1andHDB,1. (c)pdfofH1andHTB,1attimenopt∆t.
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0 0 0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
(d)pdfofH6andHar,6. (e)pdfofH6andHDB,6. (f)pdfofH6andHTB,6attimenopt∆t.
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
0 0 0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
(g)pdfofH25andHar,25. (h)pdfofH25andHDB,25. (i)pdfofH25andHTB,25attimenopt∆t.
0.6 0.6 0.6
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
0 0 0
-4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4
(j)pdfofH40andHar,40. (k)pdfofH40andHDB,40. (l)pdfofH40andHTB,40attimenopt∆t.
Figure11:Application3.Probabilitydensityfunction(pdf)ofcomponents1,6,25,and40forHestimatedwiththendrealizationsofthetraining
dataset(thinblackline)andpdfestimatedwithnarlearnedrealizations(thickblueline),forHarusingMCMCwithoutPLoM(a,d,g,j),forH
DB
usingPLoMwithRODB(b,e,h,k),andforH TBusingPLoMwithROTB(nopt∆t)(c,f,i,l).
usualonecorrespondingtod2(n ) = 1+n /(n −1) ≃ 2. Usingthen realizations{[ηℓ],ℓ = 1,...,n }of[H ],
d d d MCH ar MCH ar
wehavetheestimate,
(cid:88)n
MCH
dˆ2(m )=(1/n ) {∥[ηℓ]−[η ]∥2}/∥[η ]∥2. (A.7)
opt MCH ar d d
ℓ=1
Appendix A.5. Generationoflearnedrealizations{ηℓ′,ℓ′ =1,...,n }ofrandomvectorH
ar ar ar
Let {([Z(t)], [Y(t)]), t ∈ R+} be the unique asymptotic (for t → +∞) stationary diffusion stochastic process
with values in M ×M , of the following reduced-order ISDE (stochastic nonlinear second-order dissipative
ν,mopt ν,mopt
37(a)jointpdfofH25withH40. (b)jointpdfofHar,25withHar,40. (c)jointpdfofHDB,25withHDB,40. (d) joint pdf of HTB,25 with HTB,40 at
timenopt∆t.
Figure12:Application3.Jointprobabilitydensityfunctionofcomponents24with40ofHestimatedwiththendrealizationsofthetrainingdataset
(a)andestimatedwithnarlearnedrealizations,forHarusingMCMCwithoutPLoM(b),forH DBusingPLoMwithRODB(c),andforH TBusing
PLoMwithROTB(nopt∆t)(d).
(a)cloudsfor(Har,1,Har,6,Har,12). (b)cloudsfor(HDB,1,HDB,6,HDB,12). (c) clouds for (HTB,1,HTB,6,HTB,12) at
timenopt∆t.
(d)cloudsfor(Har,12,Har,25,Har,40). (e)cloudsfor(HDB,12,HDB,25,HDB,40). (f) clouds for (HTB,12,HTB,25,HTB,40) at
timenopt∆t.
Figure13:Application3.Cloudsofnarpointscorrespondingtonarlearnedrealizations,forcomponents1,6,12(a,b,c)andcomponents12,25,40
(d,e,f),forHarusingMCMCwithoutPLoM(a,d),forH DBusingPLoMwithRODB(b,e),andforH TBusingPLoMwithROTB(nopt∆t)(c,f).
Hamiltoniandynamicsystem),fort>0,
d[Z(t)]=[Y(t)]dt,
1 (cid:112)
d[Y(t)]=[L([Z(t)])]dt− f [Y(t)]dt+ f [dWwien(t)],
2 0 0
with[Z(0)]=[η ][a]and[Y(0)]=[N][a],inwhich
d
[a]=[g ]([g ]T[g ])−1 ∈M .
opt opt opt nd,mopt
(1) [L([Z(t)])] = [L([Z(t)][g opt]T)][a] is a random matrix with values in M ν,mopt. For all [u] = [u1...und] in M
ν,nd
38100 10-3 6.3
1.75
6.2
1.7
10-10 1.65 6.1
1.6 6
10-20 1.55 5.9
1.5 5.8
10-30 5.7
0 10 20 30 40 50 0 2 4 6 8 10 0 2 4 6 8 10
(a)eigenvaluesβ (cid:55)→ bˆ DM,β(square)and (b)n(cid:55)→dˆ2(mopt;n∆t)/ν. (c)n(cid:55)→Dˆ(pTB(·,n∆t)∥pH)(Kullback).
β(cid:55)→b˜ β(noptδt)(circle).
12 36.05 0.146
10 0.145
36
8
0.144
6 35.95
0.143
4
35.9
2 0.142
0 35.85 0.141
0 2 4 6 8 10 0 2 4 6 8 10 0 2 4 6 8 10
(d)n(cid:55)→γ(n∆t)(angleindegree). (e)n(cid:55)→Sˆ TB(n∆t)(entropy). (f) n (cid:55)→ Iˆ norm(H) (square) and n (cid:55)→
Iˆ norm(HTB;n∆t)(circle)(normalizedMI).
Figure14:Application3.Functionscharacterizingthereduced-ordertransientbasisROTB(n∆t)asafunctionoftimen∆t:eigenvaluesof[K ]and
DM
ofsymmetrized[K˜(n∆t)](a);measureconcentrationwithd2/ν-criterion(b)andwithKullbackcriteria(c);anglebetweenthesubspacesspanned
byRODBandROTB(n∆t)(d);entropyofpdfp TB(·;n∆t)(e);normalizedmutualinformation(MI)ofpdfpHandp TB(·;n∆t)(f).
withuj =(uj,...,uj)inRν,thematrix[L([u])]inM isdefined,forallk=1,...,νandforall j=1,...,n ,by
1 ν ν,nd d
1
[L([u])] = {∇ p(uj)} , (A.8)
kj p(uj) uj k
1
(cid:88)nd
1 sˆ
p(uj)= exp{− ∥ ηj′ −uj∥2},
n
d j′=1
2sˆ2 s
1
(cid:88)nd
sˆ 1 sˆ
∇ p(uj)= ( ηj′−uj) exp{− ∥ ηj′−uj∥2},
uj sˆ2n s 2sˆ2 s
d j′=1
inwhichsˆisthemodifiedSilvermanbandwidths,whichhasbeenintroducedin[62],
s
(cid:40)
4
(cid:41)1/(ν+4)
sˆ= (cid:114)
s2+
n d−1 , s= n d(2+ν) .
n
d
(2)[Wwien(t)]=[Wwien(t)][a]where{[Wwien(t)],t∈R+}istheM -valuednormalizedWienerprocess.
ν,nd
(3)[N]istheM -valuednormalizedGaussianrandommatrixthatisindependentofprocess[Wwien].
ν,nd
(4)Thefreeparameter f ,suchthat0 < f < 4/sˆ,allowsthedissipationtermofthenonlinearsecond-orderdynamic
0 0
system (dissipative Hamiltonian system) to be controlled in order to kill the transient part induced by the initial
conditions. Acommonvalueis f =4(notethatsˆ<1).
0
(5)Wethenhave[Z]=lim t→+∞[Z(t)]inprobabilitydistribution. TheSto¨rmer-Verletschemeisused[1]forsolving
thereduced-orderISDE,whichallowsforgeneratingthelearnedrealizations,[z1],...,[zn MCH],andthengeneratingthe
ar ar
learnedrealizations[η1 ar],...,[ηn arMCH]suchthat[ηℓ ar] = [zℓ ar][g opt]T. TheimplementationoftheSto¨rmer-Verletscheme
isdetailed,forinstance,intheAppendixof[19]forparallelcomputation,introducingthefollowingparameters: the
integrationtimestep∆t ,theinitialtimet =0,andthefinalintegrationtimet = M ×∆t ,atwhichthestationary
SV i f 0 SV
solutionisreached.
39(6) The learned realizations {xℓ′,ℓ′ = 1,...,n } of random vector X are then calculated (see Eq. (A.1)) by xℓ′ =
ar ar ar
x+[φ][µ]1/2ηℓ′.
ar
Appendix A.6. Constraintsonthesecond-ordermomentsofthecomponentsofH
ar
Ingeneral, themeanvalueof H estimatedusingthen learnedrealizations{ηℓ′,ℓ′ = 1,...,n }, issufficiently
ar ar ar ar
closetozero. Likewise,theestimateofthecovariancematrixofH ,whichmustbetheidentitymatrix,issufficiently
ar
closetoadiagonalmatrix. However,sometimesthediagonalentriesoftheestimatedcovariancematrixcanbelower
than1. Normalizationcanberecoveredbyimposingconstraints
E{(H )2}=1 , k=1,...,ν, (A.9)
ar,k
inthealgorithmpresentedinAppendix A.5. Forthat,weusethemethodandtheiterativealgorithmpresentedin[29]
(thatisbasedonSections5.5and5.6of[18]). TheconstraintsareimposedbyusingtheKullback-Leiblerminimum
cross-entropyprinciple.TheresultingoptimizationproblemisformulatedusingaLagrangemultiplierλ=(λ ,...,λ )
1 ν
associatedwiththeconstraints.TheoptimalsolutionoftheLagrangemultiplieriscomputedusinganefficientiterative
algorithm. Ateachiteration,theMCMCgeneratordetailedinAppendix A.5isused. Theconstraintsarerewrittenas
E{h(H )}= b, (A.10)
ar
inwhichthefunction h=(h ,...,h )andthevector b=(b ,...,b )aresuchthath (H )=(H )2 andb =1fork
1 ν 1 ν k ar ar,k k
in{1,...,ν}. TotakeintoaccounttheconstraintsinthealgorithmpresentedinAppendix A.5,Eq.(A.8)isreplaced
bythefollowingone,
1
[L ([u])] = {∇ p(uj)} −2λ uj. (A.11)
λ kj p(uj) uj k k k
It should be noted that Eqs. (A.9) to (A.11) can be straightforwardly extended to the case in which the constraint
defined by Eq. (A.9) is replaced by the full second-order moment constraints E{H } = 0 and E{H ⊗ H } = [I ],
ar ν ar ar ν
thatistosay,
E{H }=0, E{(H )2}=1 , k=1,...,ν , E{H H }=0 , 1≤k<k′ ≤ν. (A.12)
ar,k ar,k ar,k ar,k′
Theiterationalgorithmforcomputingλi+1asafunctionofλiisthefollowing,
λi+1 =λi−α[Γ′′(λi)]−1Γ′(λi) , i≥0,
i
λ0 =0 ,
ν
inwhichΓ′(λi) = b−E{h(H )}and[Γ′′(λi)] = [cov{h(H )}](thecovariancematrix), andwhereα isarelaxation
λi λi i
function(lessthan1)thatisintroducedforcontrollingtheconvergenceasafunctionofiterationnumberi. Forgiven
i ≥2,forgivenβ andβ suchthat0<β <β ≤1,α canbedefinedby:
2 1 2 1 2 i
-fori≤i ,α =β +(β −β )(i−1)/(i −1);
2 i 1 2 1 2
-fori>i ,α =β .
2 i 2
Theconvergenceoftheiterationalgorithmiscontrolledbytheerrorfunctioni(cid:55)→err(i)definedby
err(i)=∥b−E{h(H )}∥/∥b∥. (A.13)
λi
Ateachiterationi,E{h(H )}and[cov{h(H )}]areestimatedbyusingthen learnedrealizationsofH obtainedby
λi λi ar λi
reshapingthelearnedrealizations. Ifi isthelastiterationcorrespondingtoconvergence,wehave H = H with
last ar λi
i=i .
last
Appendix B. EstimationoftheKullback-Leiblerdivergence,themutualinformation,andtheentropyfroma
setofrealizations
ThedefinitionKullback-Leiblerdivergence,themutualinformation,andtheentropycanbefoundin[93,94,96].
The estimation of these quantities from a set of independent realizations is carried out using the Gaussian kernel
densityestimation(GKDE)method[63,33,64].
40For ν > 1, let X = (X ,...,X ) and Y = (Y ,...,Y ) be Rν-valued random variables defined on the probability
1 ν 1 ν
space(Θ,T,P),whoseprobabilitymeasuresareP (dx)= p (x)dxandP (dy)= p (y)dy,inwhichtheprobability
X X Y Y
densityfunctions p and p areassumedtobestrictlypositive. Let{xℓ,ℓ=1,...,N }beN independentrealizations
X Y x x
of Xandlet{yj, j = 1,...,N }beN independentrealizationsofY. Fork = 1,...,ν,letσ andσ bethestandard
y y Xk Yk
deviationofX andY thatareestimated(empiricalestimator)withtheindependentrealizations.Finally,weintroduce
k k
theSilvermanbandwidthfortheGaussianKDEestimationof p and p ,
X Y
(cid:40)
4
(cid:41)1/(ν+4) (cid:40)
4
(cid:41)1/(ν+4)
s = , s = . (B.1)
x N (2+ν) y N (2+ν)
x y
Appendix B.1. EstimationoftheKullback-Leiblerdivergencefromasetofrealizations
TheKullback-Leiblerdivergence(ortherelativeentropy)between p and p isdefinedby
X Y
(cid:90) (cid:32) (cid:33) (cid:40) (cid:32) (cid:33)(cid:41)
p (x) p (X)
D(p ∥p )= p (x) log X dx= E log X . (B.2)
X Y X p (x) p (X)
Rν Y Y
TheGKDE,Dˆ(p ∥p ),ofD(p ∥p )yieldstheformula,
X Y X Y
(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33)
s N σ ×...×σ
Dˆ(p ∥p )=νlog y +log y +log Y1 Yν
X Y s N σ ×...×σ
x x X1 Xν
+ N1
x
(cid:88) ℓN ′=x 1log  (cid:80) (cid:80)ℓN
N
j= =yx 1 1e ex xp p(cid:18) (cid:18)− −2 21 1s s2
2
yx (cid:80) (cid:80)ν k
ν
k= =1 1( (x xkℓ kℓσ σ′ ′− −X Ykk yx kkℓ
j
)) 22 (cid:19)(cid:19) 

. (B.3)
Appendix B.2. Estimationofthemutualinformationfromasetofrealizations
The mutual information I(X) of X allows to quantify the level of statistical dependencies of the components
X ,...,X ofX =(X ,...,X ). Let p bethepdfofreal-valuedrandomvariableX ,
1 ν 1 ν Xk k
(cid:90)
p Xk(x k)= p X(x 1,...,x k−1,x k,x k+1,...,x ν)dx 1...dx k−1dx k+1... dx ν. (B.4)
Rν−1
ThemutualinformationI(X)isdefinedby
(cid:40) (cid:32) (cid:33)(cid:41)
p (X)
I(X)= D(p ∥⊗ν p )= E log X . (B.5)
X k=1 Xk p (X )×...×p (X )
X1 1 Xν ν
Eq.(B.5)showsthat,ifthecomponents X ,...,X arestatisticallyindependent,then I(X) = 0. TheGKDE, Iˆ(X)of
1 ν
I(X)yieldstheformula,
Iˆ(X)= N1 x (cid:88) ℓN ′=x 1log  (cid:81)ν kN1 =x 1(cid:80) (cid:18) NℓN 1= xx 1 (cid:80)ex ℓN ′p ′x =(cid:18) 1− ex21 s p2 x (cid:18)(cid:80) −ν k 2= 1 s1
2
x( (x xkℓ σ kℓ′ ′ σ− X − Xkx x kkℓ kℓ′) ′2 )(cid:19) 2(cid:19)(cid:19)   . (B.6)
Appendix B.3. Estimationoftheentropyfromasetofrealizations
Theentropyrelatedto p isdefinedby
X
(cid:90)
S =− p (x) logp (x)dx=−E(cid:8) logp (X)(cid:9) . (B.7)
X X X X
Rν
TheGKDE,Sˆ ,ofS yieldstheformula,
X X
√
Sˆ =νlog(s 2π)+log(σ ×...×σ )
X x X1 Xν
−
N1
x
(cid:88) ℓN ′=x 1log  N1
x
(cid:88) ℓN =x 1exp − 21
s2
x
(cid:88) k=ν 1(x kℓ σ′ − Xkx kℓ )2  

. (B.8)
√
SinceJ (N )=νlog(s 2π)isasymptoticallyforN →+∞in−log(N ),theentropydecreaseswhenN increases.
S x x x x x
41Conflictofinterest
Theauthordeclaresthathehasnoconflictofinterest.
References
[1] C.Soize,R.Ghanem,Data-drivenprobabilityconcentrationandsamplingonmanifold,JournalofComputationalPhysics321(2016)242–
258.doi:10.1016/j.jcp.2016.05.044.
[2] R.Coifman, S.Lafon, A.Lee, M.Maggioni, B.Nadler, F.Warner, S.Zucker, Geometricdiffusionsasatoolforharmonicanalysisand
structuredefinitionofdata:Diffusionmaps,PNAS102(21)(2005)7426–7431.doi:10.1073/pnas.0500334102.
[3] R.Coifman,S.Lafon,Diffusionmaps,AppliedandComputationalHarmonicAnalysis21(1)(2006)5–30.doi:10.1016/j.acha.2006.04.006.
[4] K.B.Korb,A.E.Nicholson,Bayesianartificialintelligence,CRCpress,BocaRaton,2010.
[5] K.P.Murphy,MachineLearning:AProbabilisticPerspective,MITpress,2012.
[6] Z.Ghahramani,Probabilisticmachinelearningandartificialintelligence,Nature521(7553)(2015)452–459.doi:10.1038/nature14541.
[7] S.Russel,P.Norvig,ArtificalIntelligence,AModernApproach,ThirdEdition,Pearson,Harlow,2016.
[8] V.Vapnik,TheNatureofStatisticalLearningTheory,Springer,NewYork,2000.doi:10.1007/978-1-4757-3264-1.
[9] T.Hastie,R.Tibshirani,J.Friedman,TheElementsofStatisticalLearning,SecondEdition,Springer,2009.doi:10.1007/b94608.
[10] G.James,D.Witten,T.Hastie,R.Tibshirani,AnIntroductiontoStatisticalLearning,Vol.112,Springer,2013.
[11] J.Taylor,R.J.Tibshirani,Statisticallearningandselectiveinference,ProceedingsoftheNationalAcademyofSciences112(25)(2015)
7629–7634.doi:10.1073/pnas.1507583112.
[12] R.Swischuk,L.Mainini,B.Peherstorfer,K.Willcox,Projection-basedmodelreduction:Formulationsforphysics-basedmachinelearning,
Computers&Fluids179(2019)704–717.doi:10.1016/j.compfluid.2018.07.021.
[13] A. C. O¨ztireli, M. Alexa, M. Gross, Spectral sampling of manifolds, ACM Transactions on Graphics (TOG) 29 (6) (2010) 1–8.
doi:10.1145/1882261.1866190.
[14] G.Perrin,C.Soize,S.Marque-Pucheu,J.Garnier,NestedpolynomialtrendsfortheimprovementofGaussianprocess-basedpredictors,
JournalofComputationalPhysics346(2017)389–402.doi:10.1016/j.jcp.2017.05.051.
[15] Y. Kevrekidis, Manifold learning for parameter reduction, Bulletin of the American Physical Society 65 (2020).
doi:10.1016/j.jcp.2019.04.015.
[16] K. Kontolati, D. Loukrezis, K. R. dos Santos, D. G. Giovanis, M. D. Shields, Manifold learning-based polynomial
chaos expansions for high-dimensional surrogate models, International Journal for Uncertainty Quantification 12 (4) (2022).
doi:10.1615/Int.J.UncertaintyQuantification.2022039936.
[17] S.Pan,K.Duraisamy,Physics-informedprobabilisticlearningoflinearembeddingsofnonlineardynamicswithguaranteedstability,SIAM
JournalonAppliedDynamicalSystems19(1)(2020)480–509.doi:10.1137/19M1267246.
[18] C.Soize,R.Ghanem,Physics-constrainednon-Gaussianprobabilisticlearningonmanifolds,InternationalJournalforNumericalMethodsin
Engineering121(1)(2020)110–145.doi:10.1002/nme.6202.
[19] C.Soize,R.Ghanem,Probabilisticlearningonmanifoldsconstrainedbynonlinearpartialdifferentialequationsforsmalldatasets,Computer
MethodsinAppliedMechanicsandEngineering380(2021)113777.doi:10.1016/j.cma.2021.113777.
[20] A.Talwalkar,S.Kumar,H.Rowley,Large-scalemanifoldlearning,in:2008IEEEConferenceonComputerVisionandPatternRecognition,
IEEE,2008,pp.1–8.doi:10.1109/CVPR.2008.4587670.
[21] Y.Marzouk,T.Moselhy,M.Parno,A.Spantini,Samplingviameasuretransport: Anintroduction,Handbookofuncertaintyquantification
(2016)1–41doi:10.1007/978-3-319-11259-623-1.
[22] M.D.Parno,Y.M.Marzouk,TransportmapacceleratedmarkovchainMonteCarlo,SIAM/ASAJournalonUncertaintyQuantification6(2)
(2018)645–682.doi:10.1137/17M1134640.
[23] G.Perrin,C.Soize,N.Ouhbi,Data-drivenkernelrepresentationsforsamplingwithanunknownblockdependencestructureundercorrelation
constraints,ComputationalStatistics&DataAnalysis119(2018)139–154.doi:10.1016/j.csda.2017.10.005.
[24] C.Soize,R.Ghanem,Polynomialchaosrepresentationofdatabasesonmanifolds,JournalofComputationalPhysics335(2017)201–221.
doi:10.1016/j.jcp.2017.01.031.
[25] C.Soize,R.Ghanem,C.Safta,X.Huan,Z.P.Vane,J.C.Oefelein,G.Lacaze,H.N.Najm,Q.Tang,X.Chen,Entropy-basedclosurefor
probabilisticlearningonmanifolds,JournalofComputationalPhysics388(2019)528–533.doi:10.1016/j.jcp.2018.12.029.
[26] C.Soize,R.Ghanem,C.Desceliers,SamplingofBayesianposteriorswithanon-Gaussianprobabilisticlearningonmanifoldsfromasmall
dataset,StatisticsandComputing30(5)(2020)1433–1457.doi:10.1007/s11222-020-09954-6.
[27] C.Soize,R.Ghanem,Probabilisticlearningonmanifolds,FoundationsofDataScience2(3)(2020)279–307.doi:10.3934/fods.2020013.
[28] C.Soize,ProbabilisticlearninginferenceofboundaryvalueproblemwithuncertaintiesbasedonKullback-Leiblerdivergenceunderimplicit
constraints,ComputerMethodsinAppliedMechanicsandEngineering395(2022)115078.doi:10.1016/j.cma.2022.115078.
[29] C.Soize,R.Ghanem,Probabilisticlearningonmanifolds(PLoM)withpartition,InternationalJournalforNumericalMethodsinEngineering
123(1)(2022)268–290.doi:10.1002/nme.6856.
[30] C.Soize,Probabilisticlearningconstrainedbyrealizationsusingaweakformulationoffouriertransformofprobabilitymeasures,Computa-
tionalStatistics38(4)(2023)1879–1925.doi:10.1007/s00180-022-01300-w.
[31] M.C.Kennedy,A.O’Hagan,Bayesiancalibrationofcomputermodels,JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethod-
ology)63(3)(2001)425–464.doi:10.1111/1467-9868.00294.
[32] Y.M.Marzouk,H.N.Najm,L.A.Rahn,StochasticspectralmethodsforefficientBayesiansolutionofinverseproblems,JournalofCompu-
tationalPhysics224(2)(2007)560–586.doi:10.1016/j.jcp.2006.10.010.
[33] J.E.Gentle,Computationalstatistics,Springer,NewYork,2009.doi:10.1007/978-0-387-98144-4.
[34] A.M.Stuart,Inverseproblems:aBayesianperspective,ActaNumerica19(2010)451–559.doi:10.1017/S0962492910000061.
42[35] H.Owhadi,C.Scovel,T.Sullivan,OnthebrittlenessofBayesianinference,SIAMReview57(4)(2015)566–582.doi:10.1137/130938633.
[36] H.G.Matthies,E.Zander,B.V.Rosic´,A.Litvinenko,O.Pajonk,InverseproblemsinaBayesiansetting,in: ComputationalMethodsfor
SolidsandFluids,Vol.41,Springer,2016,pp.245–286.doi:10.1007/978-3-319-27996-110.
[37] M.Dashti,A.M.Stuart,TheBayesianapproachtoinverseproblems,in:R.Ghanem,D.Higdon,O.Houman(Eds.),HandbookofUncertainty
Quantification,Springer,Cham,Switzerland,2017,Ch.10,pp.311–428.doi:10.1007/978-3-319-12385-17.
[38] R. Ghanem, D. Higdon, H. Owhadi, Handbook of Uncertainty Quantification, Vol. 1 to 3, Springer, Cham, Switzerland, 2017.
doi:10.1007/978-3-319-12385-1.
[39] A.Spantini,T.Cui,K.Willcox,L.Tenorio,Y.Marzouk,Goal-orientedoptimalapproximationsofBayesianlinearinverseproblems,SIAM
JournalonScientificComputing39(5)(2017)S167–S196.doi:10.1137/16M1082123.
[40] G. Perrin, C. Soize, Adaptive method for indirect identification of the statistical properties of random fields in a Bayesian framework,
ComputationalStatistics35(1)(2020)111–133.doi:10.1007/s00180-019-00936-5.
[41] R.Ghanem,C.Soize,Probabilisticnonconvexconstrainedoptimizationwithfixednumberoffunctionevaluations,InternationalJournalfor
NumericalMethodsinEngineering113(4)(2018)719–741.doi:10.1002/nme.5632.
[42] C. Soize, Design optimization under uncertainties of a mesoscale implant in biological tissues using a probabilistic learning algorithm,
ComputationalMechanics62(3)(2018)477–497.doi:10.1007/s00466-017-1509-x.
[43] R.Ghanem,C.Soize,C.Thimmisetty,Optimalwell-placementusingprobabilisticlearning,Data-EnabledDiscoveryandApplications2(1)
(2018)1–16.doi:10.1007/s41688-017-0014-x.
[44] C.Farhat,R.Tezaur,T.Chapman,P.Avery,C.Soize,Feasibleprobabilisticlearningmethodformodel-formuncertaintyquantificationin
vibrationanalysis,AIAAJournal57(11)(2019)4978–4991.doi:10.2514/1.J057797.
[45] R.Ghanem,C.Soize,C.Safta,X.Huan,G.Lacaze,J.C.Oefelein,H.N.Najm,Designoptimizationofascramjetunderuncertaintyusing
probabilisticlearningonmanifolds,JournalofComputationalPhysics399(2019)108930.doi:10.1016/j.jcp.2019.108930.
[46] J.O.Almeida,F.A.Rochinha,Aprobabilisticlearningapproachappliedtotheoptimizationofwakesteeringinwindfarms,Journalof
ComputingandInformationScienceinEngineering23(1)(2022)011003.doi:10.1115/1.4054501.
[47] E. Capiez-Lernout, C. Soize, Nonlinear stochastic dynamics of detuned bladed disks with uncertain mistuning and detuning
optimization using a probabilistic machine learning tool, International Journal of Non-Linear Mechanics 143 (2022) 104023.
doi:10.1016/j.ijnonlinmec.2022.104023.
[48] J.O.Almeida,F.A.Rochinha,Aprobabilisticlearningapproachappliedtotheoptimizationofwakesteeringinwindfarms,Journalof
ComputingandInformationScienceinEngineering23(1)(2023)011003.doi:10.1115/1.4054501.
[49] J.Guilleminot,J.E.Dolbow,Data-drivenenhancementoffracturepathsinrandomcomposites,MechanicsResearchCommunications103
(2020)103443.doi:10.1016/j.mechrescom.2019.103443.
[50] P.Chen,J.Guilleminot,C.Soize,Concurrentmultiscalesimulationsofnonlinearrandommaterialsusingprobabilisticlearning,Computer
MethodsinAppliedMechanicsandEngineering422(2024)116837.doi:10.1016/j.cma.2024.116837.
[51] C.Soize,Anoverviewonuncertaintyquantificationandprobabilisticlearningonmanifoldsinmultiscalemechanicsofmaterials,Mathemat-
icsandMechanicsofComplexSystems11(1)(2023)87–174.doi:10.2140/memocs.2023.11.87.
[52] R.Ghanem,C.Soize,L.Mehrez,V.Aitharaju,Probabilisticlearningandupdatingofadigitaltwinforcompositematerialsystems,Interna-
tionalJournalforNumericalMethodsinEngineering123(13)(2022)3004–3020.doi:10.1002/nme.6430.
[53] O.Ezvan,C.Soize,C.Desceliers,R.Ghanem,Updatinganuncertainandexpensivecomputationalmodelinstructuraldynamicsbasedon
onesingletargetfrfusingaprobabilisticlearningtool,ComputationalMechanics71(2023)1161–1177.doi:10.1007/s00466-023-02301-2.
[54] E.Capiez-Lernout,C.Ezvan,OlivierSoize,Updatingnonlinearstochasticdynamicsofanuncertainnozzlemodelusingprobabilisticlearn-
ingwithpartialobservabilityandincompletedataset,ASMEJournalofComputingandInformationScienceinEngineering24(6)(2024)
061006,1–17.doi:10.1115/1.4065312.
[55] M.Arnst,C.Soize,K.Bulthies,Computationofsobolindicesinglobalsensitivityanalysisfromsmalldatasetsbyprobabilisticlearningon
manifolds,InternationalJournalforUncertaintyQuantification11(2)(2021)1–23.doi:10.1615/Int.J.UncertaintyQuantification.2020032674.
[56] C.Soize,A.Orcesi,Machinelearningfordetectingstructuralchangesfromdynamicmonitoringusingtheprobabilisticlearningonmanifolds,
StructureandInfrastructureEngineeringJournal17(10)(2021)1418–1430.doi:10.1080/15732479.2020.1811991.
[57] K.Zhong,J.G.Navarro,S.Govindjee,G.G.Deierlein,SurrogatemodelingofstructuralseismicresponseusingProbabilisticLearningon
Manifolds,EarthquakeEngineeringandStructuralDynamics52(8)(2023)2407–2428.doi:10.1002/eqe.3839.
[58] C.Soize,R.Ghanem,Probabilistic-learning-basedstochasticsurrogatemodelfromsmallincompletedatasets,ComputerMethodsinApplied
MechanicsandEngineering(2023).
[59] C.Soize,R.Ghanem,Probabilistic-learning-basedstochasticsurrogatemodelfromsmallincompletedatasetsfornonlineardynamicalsys-
tems,ComputerMethodsinAppliedMechanicsandEngineering418(2023)116498.doi:10.1016/j.cma.2023.116498.
[60] C.Soize,Q.-D.To,Polynomial-chaos-basedconditionalstatisticsforprobabilisticlearningwithheterogeneousdataappliedtoatomiccolli-
sionsofheliumongraphitesubstrate,JournalofComputationalPhysics(2023).
[61] A.Sinha,C.Soize,C.Desceliers,G.Cunha,Aeroacousticlinerimpedancemetamodelfromsimulationandexperimentaldatausingproba-
bilisticlearning,AIAAJournal61(11)(2023)4926–4934.doi:10.2514/1.J062991.
[62] C.Soize,Polynomialchaosexpansionofamultimodalrandomvector,SIAM-ASAJournalonUncertaintyQuantification3(1)(2015)34–60.
doi:10.1137/140968495.
[63] A.Bowman,A.Azzalini,AppliedSmoothingTechniquesforDataAnalysis:TheKernelApproachWithS-PlusIllustrations,Vol.18,Oxford
UniversityPress,Oxford:ClarendonPress,NewYork,1997.doi:10.1007/s001800000033.
[64] G.Givens,J.Hoeting,ComputationalStatistics,2ndEdition,JohnWileyandSons,Hoboken,NewJersey,2013.
[65] J.L.Doob,Stochasticprocesses,JohnWiley&Sons,NewYork,1953.
[66] I.I.Guikhman,A.Skorokhod,Introductiona`laThe´oriedesProcessusAle´atoires,EditionMir,1980.
[67] A.Friedman,StochasticDifferentialEquationsandApplications,DoverPublications,Inc.,Mineola,NewYork,2006.
[68] C.Soize,TheFokker-PlanckEquationforStochasticDynamicalSystemsanditsExplicitSteadyStateSolutions,Vol.SeriesonAdvancesin
MathematicsforAppliedSciences:Vol17,WorldScientific,Singapore,1994.doi:10.1142/2347.
43[69] C.W.Gardiner,HandbookofStochasticMethods,SecondEdition,SpringerVerlag,Berlin,Heidelberg,1985.
[70] H.Risken,TheFokker-PlanckEquation,SecondEdition,SpringerVerlag,Berlin,Heidelberg,1989.
[71] I. M. Gelfand, N. I. Vilenkin, Les Distributions. Tome 4. Application de l’Analyse Harmonique, Dunod, 1967. doi:10.1016/0375-
9474(67)90547-7.
[72] B.Spencer,L.Bergman,OnthenumericalsolutionoftheFokker-Planckequationfornonlinearstochasticsystems,NonlinearDynamics4
(1993)357–372.doi:10.1007/BF00120671.
[73] A. Masud, L. A. Bergman, Application of multi-scale finite element methods to the solution of the Fokker-Planck equation, Computer
methodsinappliedmechanicsandengineering194(12-16)(2005)1513–1526.doi:10.1016/j.cma.2004.06.041.
[74] P.Kumar,S.Narayanan,SolutionofFokker-Planckequationbyfiniteelementandfinitedifferencemethodsfornonlinearsystems,Sadhana
31(2006)445–461.doi:10.1007/BF02716786.
[75] L.Pichler,A.Masud,L.A.Bergman,NumericalsolutionoftheFokker-Planckequationbyfinitedifferenceandfiniteelementmethods-a
comparativestudy,ComputationalMethodsinStochasticDynamics:Volume2(2013)69–85doi:10.1007/978-94-007-5134-7˙5.
[76] W.Deng,FiniteelementmethodforthespaceandtimefractionalFokker-Planckequation,SIAMjournalonnumericalanalysis47(1)(2009)
204–226.doi:10.1137/080714130.
[77] W.Anderson,M.Farazmand,Fisherinformationandshape-morphingmodesforsolvingtheFokker-Planckequationinhigherdimensions,
AppliedMathematicsandComputation467(2024)128489.doi:10.1016/j.amc.2023.128489.
[78] P.L.Popelier,SolvingtheSchro¨dingerEquation:HasEverythingBeenTried?,ImperialCollegePress,2011.doi:10.1142/p780.
[79] M.Feit,J.FleckJr,A.Steiger,SolutionoftheSchro¨dingerequationbyaspectralmethod,JournalofComputationalPhysics47(3)(1982)
412–433.doi:10.1016/0021-9991(82)90091-2.
[80] T. Iitaka, Solving the time-dependent Schro¨dinger equation numerically, Physical Review E 49 (5) (1994) 4684.
doi:10.1103/PhysRevE.49.4684.
[81] T.Simos, P.Williams, Afinite-differencemethodforthenumericalsolutionoftheSchro¨dingerequation, JournalofComputationaland
AppliedMathematics79(2)(1997)189–205.doi:10.1016/S0377-0427(96)00156-2.
[82] R.J.Serfling,Approximationtheoremsofmathematicalstatistics,Vol.162,JohnWiley&Sons,1980.
[83] C.Soize, UncertaintyQuantification.AnAcceleratedCoursewithAdvancedApplicationsinComputationalEngineering, Springer, New
York,2017.doi:10.1007/978-3-319-54339-0.
[84] N.Ikeda,S.Watanabe,StochasticDifferentialEquationsandDiffusionProcesses,North-Holland,Amsterdam,1981.
[85] R.Has’minski,StochasticStabilityofDifferentialEquations,Sijthoff&Noordhoff,AlphenaandenRijn,TheNetherlands,1980,firstEnglish
editionKhasminskii,2021,Spinger.
[86] D.W.Stroock,S.S.Varadhan,MultidimensionalDiffusionProcesses,Vol.233,Springer-Verlag,Berlin,Heidelberg,1997.
[87] P.Kloeden,E.Platen,NumericalSolutionofStochasticDifferentialsEquations,Springer-Verlag,Heidelberg,1992.
[88] T.Duong,A.Cowling,I.Koch,M.Wand,Featuresignificanceformultivariatekerneldensityestimation,ComputationalStatistics&Data
Analysis52(9)(2008)4225–4242.doi:10.1016/j.csda.2008.02.035.
[89] E.R.Hansen,ATableofSeriesandProducts,Prentice-Hall,NewYork,1975.
[90] A.Bhattacharyya,Onthemeasuresofdivergencebetweentwostatisticalpopulationsdefinedbytheirprobabilitydistributions,Bulletinof
theCalculttaMathematicalSociety35(1943)99–109.
[91] S. Kullback, R. A. Leibler, On information and sufficiency, The Annals of Mathematical Statistics 22 (1) (1951) 79–86.
doi:10.1214/aoms/1177729694.
[92] A.Kolmogorov, Ontheshannontheoryofinformationtransmissioninthecaseofcontinuoussignals, IRETransactionsonInformation
Theory2(4)(1956)102–108.
[93] T.M.Cover,J.A.Thomas,ElementsofInformationTheory,SecondEdition,JohnWiley&Sons,Hoboken,2006.
[94] J.N.Kapur,H.K.Kesavan,EntropyOptimizationPrincipleswithApplications,AcademicPress,SanDiego,1992.
[95] R.M.Gray,EntropyandInformationTheory,2ndEdition,Springer,NewYork,2011.doi:10.1007/978-1-4419-7970-4.
[96] C. E. Shannon, A mathematical theory of communication, Bell system technical journal 27 (3) (1948) 379–423 & 623–659.
doi:10.1002/j.1538-7305.1948.tb01338.x.
[97] S.Lafon,A.B.Lee,Diffusionmapsandcoarse-graining:Aunifiedframeworkfordimensionalityreduction,graphpartitioning,anddataset
parameterization,IEEEtransactionsonpatternanalysisandmachineintelligence28(9)(2006)1393–1403.doi:10.1109/TPAMI.2006.184.
44