Vision-Language Model Based Handwriting Verification
MihirChauhan†† AbhishekSatbhai§ MohammadAbuzarHashemi¶ MirBasheerAli¶¶
BinaRamamurthy‡ MingchenGao|| SiweiLyu† SargurSrihari‡‡
DepartmentofComputerScienceandEngineering
TheStateUniversityofNewYork,Buffalo,NY,USA
{mihirhem††, bina‡, mgao8||, siweilyu†, srihari‡‡}@buffalo.edu
{ma.hashemi.786¶, abhishek07satbhai§, alimirbasheer¶¶}@gmail.com
Abstract
HandwritingVerificationisacriticalindocumentforensics. Deeplearningbasedapproachesoftenface
skepticismfromforensicdocumentexaminersduetotheirlackofexplainabilityandrelianceonextensive
trainingdataandhandcraftedfeatures. ThispaperexploresusingVisionLanguageModels(VLMs),suchas
OpenAI’sGPT-4oandGoogle’sPaliGemma,toaddressthesechallenges. ByleveragingtheirVisualQues-
tion Answering capabilities and 0-shot Chain-of-Thought (CoT) reasoning, our goal is to provide clear,
human-understandable explanations for model decisions. Our experiments on the CEDAR handwriting
datasetdemonstratethatVLMsofferenhancedinterpretability, reducetheneedforlargetrainingdatasets,
and adapt better to diverse handwriting styles. However, results show that the CNN-based ResNet-18 ar-
chitecture outperforms the 0-shot CoT prompt engineering approach with GPT-4o (Accuracy: 70%) and
supervised fine-tuned PaliGemma (Accuracy: 71%), achieving an accuracy of 84% on the CEDAR AND
dataset. ThesefindingshighlightthepotentialofVLMsingeneratinghuman-interpretabledecisionswhile
underscoringtheneedforfurtheradvancementstomatchtheperformanceofspecializeddeeplearningmod-
els. Ourcodeispubliclyavailableat: https://github.com/Abhishek0057/vlm-hv
Keywords: VisionLanguageModel,MachineVision,HandwritingVerification,Forensics
1 Introduction
Handwritingverificationiscrucialinpatternrecognitionandbiometrics, focusingonauthenticatingandiden-
tifyingindividualsbytheirhandwriting. Itisvitalinforensics, whereexpertsanalyzesamplestoverifydocu-
ments,identifyforgeries,andprovidecourtevidence.
Historical Perspective: Handwriting Verification methods [Shaikhetal.,2018] [Chauhanetal.,2019]
[Chauhanetal.,2024] have evolved from handcrafted features like GSC [FavataandSrikantan,1996] to deep
CNNs such as ResNet-18 [Heetal.,2015], and now Vision Transformers [Dosovitskiyetal.,2021], enabling
comparisonofbothinter-andintra-writervariationsinhandwritingstyles.
Challenges: Despitetheadvancements,ForensicDocumentExaminers(FDE)remainskepticalduetothe
lack of interpretability in the model’s decision making processes. Morever, these methods heavily depend on
largedatasetsoflabeledhandwrittenimages,makingdatasetcollection(X(x ,x ,y)withknownx ,questioned
q k k
x handwrittensamplesandcorrespondingwriterlabels y)iscostlyandtime-intensive.
q
Why use VLM? Vision-Language Models (VLMs) integrate image and textual data to encode complex
relationships between visual and linguistic information, enabling Forensic Document Examiners (FDEs) to
interpretmodeldecisionswithclear,naturallanguageexplanationsthatenhancetrustandreliabilityinforensic
applications. They can adapt to the forensics domain in zero-shot scenarios without training examples and in
few-shotscenarioswithminimalexamplesbyleveragingtransferlearningcapabilities,havingbeenpre-trained
on multiple tasks related to handwritten images and natural language understanding. Fine-tuning them with
forensic-specificdatasetsfurtherenhancestheirperformanceandapplicabilityinreal-worldforensicuse-cases.
4202
luJ
13
]VC.sc[
1v88712.7042:viXraBaselines
GSC / ResNet-18 / ViT MLP with 2 hidden layers Accuracy / Precision / Recall / F1-Score
Feature Extractor Classifier Model Evaluation Metrics
Your task is to provide an Known Sample has
explanationif the two continuity in staff of “a” co-
handwritten samples are OR ordinates (36,45) whereas
“Known” Sample “Questioned” Sample written by the same writer questioned sample shows “Known” Sample “Questioned” Sample
0001a_num1.png 0002a_num1.png or by different . . . GPT-4o PaliGemma no continuity (40,35) . . . 0001a_num1.png 0002a_num1.png
“and”image pair TextPrompt Vision-Language Model TextExplanation VisualEntity Detection
Figure1: OverallprocessofevaluatingVisionLanguageModelforHandwritingVerificationagainstBaselines.
VLMs applied to Forensics: In [Scanlonetal.,2023], the authors explore the capabilities of multi-
modal Large Language Models (LLMs) like GPT-4 in various digital forensics use-cases, including arti-
fact understanding, evidence searching, anomaly detection, and education. More specific studies, such as
[Jiaetal.,2024],demonstratetheeffectivenessofGPT-4specificallyforthemediaforensicstaskofDeepFake
imagedetection. OurresearchappliesVLMstohandwritingverification. Toourknowledge,wearethefirstto
exploreVLMsfortheforensictaskofhandwritingcomparison.
2 Methods
OurstudyaimstouseVLMinhandwritingverification. WehavechosenOpenAI’sGPT-4oVLMforitsstrong
Visual Question Answering (VQA) capabilities. Using their API, we prompt GPT-4o [gpt,2024] with 0-shot
Chain-of-Thought(CoT)reasoning. Thisapproachallowsustofirstgeneratehuman-interpretableexplanations
and then determine whether the given questioned and known handwritten samples were written by the same
person or by different writers, as shown in Figure 1. Since fine-tuning GPT-4o is not generally available, we
compareditsperformancetoarecentopen-sourceVLM,PaliGemma[Beyeretal.,2024],using0-shotprompt
engineeringandparameter-efficientsupervisedfine-tuning(PEFT)onatrainingdatasetwith100examples.
Data: The experiments were conducted using 1,000 sample pairs of known and questioned images from
CEDAR Letter [Sriharietal.,2001] and CEDAR AND [Shaikhetal.,2018] dataset. CEDAR Letter dataset
contains a letter manuscript written by 1568 writers three times. CEDAR AND dataset is a subset of CEDAR
Letter data which only contains the lowercase handwritten word “and” extracted from full letter manuscript.
The evaluation dataset includes 1,000 sample pairs of known and questioned images written by 368 writers
withwriteridsgreaterthan1200(writeridsbelow1200wereusedtotrainthebaselinemodels). Ofthesepairs,
500arefromthesamewriterand500arefromdifferentwriters.
3 Experiments & Results
CEDARANDBaselines: AsshowninFigure1weusehandcraftedfeaturesGSCfeatures,CNNbasedResNet-
18[Heetal.,2015]andMaskedCausalVisionTransformer[Dosovitskiyetal.,2021](ViT)asourbaselinefea-
ture extractors. GSC are 512-dimensional features extracted for the binarized “AND” images. All the three
baselines were trained on 10% and 100% of known and questioned training pairs writers with writer ids less
than 1200 resulting in 13,232 and 129,602 number of train pairs as shown in Table 1. The output of these
feature extractors is fed into 2 fully-connected (FC) layers with 256 and 128 hidden neurons with ReLU acti-
vations. Thefinallayerhas2outputneuronswhosesoftmaxactivationsrepresentsimilarityofsampleswitha
onehotvectorrepresentation. Weusecategoricalcrossentropylossgivenone-hotencodedlogitscomparedto
thetargetwhichisbinary.
PromptEngineeringwithGPT-4oandPaliGemma: ToeffectivelyutilizetheGPT-4oVLMforhandwrit-
ingverification,weexperimentedwithvariouspromptstooptimizeboththegenerationofhuman-interpretable
explanations and the accuracy of verification decisions. Initially, we crafted prompts that directed the model
to compare specific features of the handwriting samples, such as stroke width, slant, and letter spacing. For
example, prompts like "Describe the similarities and differences in the stroke patterns between the two sam-Data Model Approach #TrainPairs Accuracy Precision Recall F1-Score
CEDARAND GSC SupervisedTraining 13,232 0.71 0.69 0.72 0.69
CEDARAND ResNet-18 SupervisedFine-Tuning 13,232 0.72 0.70 0.73 0.72
CEDARAND ViT SupervisedFine-Tuning 13,232 0.65 0.68 0.64 0.66
CEDARAND GSC SupervisedTraining 129,602 0.78 0.81 0.77 0.79
CEDARAND ResNet-18 SupervisedFine-Tuning 129,602 0.84 0.86 0.82 0.84
CEDARAND ViT SupervisedFine-Tuning 129,602 0.79 0.80 0.78 0.79
CEDARAND GPT-4o 0-ShotCoTPromptEngineering 0 0.7 0.68 0.7 0.69
CEDARAND PaliGemma 0-ShotCoTPromptEngineering 0 0.65 0.66 0.65 0.65
CEDARAND PaliGemma SupervisedFine-Tuning 100 0.71 0.72 0.71 0.72
CEDARLetter GPT-4o 0-ShotCoTPromptEngineering 0 0.65 0.67 0.65 0.66
CEDARLetter PaliGemma 0-ShotCoTPromptEngineering 0 0.58 0.6 0.59 0.59
CEDARLetter PaliGemma SupervisedFine-Tuning 100 0.64 0.63 0.66 0.64
Table 1: Performance metrics on 1000 sampled pairs of known and questioned pairs from evaluation dataset.
Baseline performance for GSC, ResNet-18 and ViT is shown with 10% & 100% of train writers on CEDAR
ANDdataset. 0-shotperformanceofGPT-4oandPaliGemmaisevaluatedforCEDARANDandLetterdataset.
Also,performanceoffine-tunedPaliGemmaonCEDARANDdatasetisobserved.
ples"and"Identifyanymatchinguniquecharacteristicsinthehandwriting"wereused. However,weobserved
significant variance in the model’s responses, leading to inconsistent results. To address this, we adopted a
Chain-of-Thought (CoT) reasoning approach. This method guides the model through a structured reasoning
process,allowingittogeneratemoreconsistentandreliableexplanations. Sincenoexampleswereprovidedto
VLMinthepromptengineeringphase,thenumberoftrainingpairsisobservedas0inTable1.
Additionally, we utilized prompt engineered both VLMs to identify and mark coordinates of similarities
anddissimilaritieswithinthetwoimages. Bypromptingthemodeltohighlightspecificareasofinterest,such
as matching or differing stroke patterns, we were able to generate detailed coordinates that pinpointed these
features as shown in Figure 1. This approach provided clear visual indicators of the reasoning behind each
verification decision. By incorporating CoT reasoning, presenting known and questioned samples as different
images, and using VLM to identify key coordinates, we achieved a balance where the model could generate
detailed,interpretableexplanationswhileaccuratelydeterminingtheauthenticityofthehandwritingsamples.
Supervised Fine-Tuning with PaliGemma: We fine-tuned the pre-trained PaliGemma model on 100 cu-
ratedexamplesof0-shotCoTpromptengineeredPaliGemmaresults. ThePaliGemmaProcessorwithinTrans-
formers library was loaded to process the inputs which involved the preparation of prompt templates and
batching text inputs with images. The image tokens and pad tokens were set to -100 to be ignored by the
model, and these preprocessed inputs were used as labels for the model to learn from. The model, PaliGem-
maForConditionalGeneration,wasloadedandsettofine-tuneonlythetextdecoder,leavingthevisionencoder
and multimodal projector frozen. We used BitsAndBytes config options with 4bit quantization and LoRA
[Huetal.,2021] to train only 11M parameters out of the 3B parameters of the network. The results of super-
visedfine-tunedPaliGemmaonbothCEDARANDandLetterdatasetsareshowninTable1.
Results: The results of using VLMs for handwriting verification are presented in Table 1 on the CEDAR
AND and CEDAR Letter datasets. For the CEDAR AND dataset, CNN based ResNet-18 architecture out-
performs 0-shot CoT prompt engineering approach with GPT-4o as well as supervised-finetuned PaliGemma
with100%trainingpairswhichsuggestthatalthoughVLMshasalotofpotentialforgeneratinghumaninter-
pretabledecisionmakingforthetaskofhandwritingverification,stilllagsbehindCNNswhichaarefine-tuned
on specific task of handwriting verification. This gap highlights the need for further improvements in fine-
tuning regime of VLMs to enhance their effectiveness and reliability for specialized tasks like handwriting
verification. FortheCEDARLetterdataset,the0-shotCoTpromptengineeringwithGPT-4oalsoshowsrela-
tivelyhigherperformancethanthesupervisedfine-tuningofPaliGemmabutweobservehighvariabilityinthe
performancemetricsbecauseoflowsamplesize.4 Conclusion
OurstudyexplorestheapplicationofVLMs,specificallyGPT-4oandPaliGemma,inthedomainofhandwriting
verification. Byleveraging the robust VQA capabilitiesof these models and employing 0-shotCoT reasoning
throughpromptengineering,weaimedtogeneratehuman-interpretableexplanationsformodeldecisions. Our
experimentsdemonstratedthatwhileVLMsoffersignificantimprovementsininterpretabilityandadaptability
todiversehandwritingstyles,theycurrentlylagbehindCNN-basedarchitecturessuchasResNet-18intermsof
performance. Specifically,ResNet-18achievedanaccuracyof84%ontheCEDARANDdataset,outperform-
ing GPT-4o’s 70% accuracy and PaliGemma’s 71% accuracy. These findings suggest that while VLMs hold
greatpromiseforenhancingtransparencyandtrustworthinessinforensichandwritingverification,thereisstill
a need for further advancements in their fine-tuning regimes to improve their effectiveness and reliability for
specialized tasks. Moving forward, we aim to work with forensic document examiners to create a fine-tuning
dataset of explanation reports using text and visual information to ensure the applicability of our approach in
practicalforensicsettings.
References
[gpt,2024] (2024). Gpt-4o. https://openai.com/index/hello-gpt-4o/.
[Beyeretal.,2024] Beyer,L.,Steiner,A.,andet.al(2024). Paligemma: Aversatile3bvlmfortransfer.
[Chauhanetal.,2024] Chauhan,M.,Shaikh,M.A.,Ramamurthy,B.,Gao,M.,Lyu,S.,andSrihari,S.(2024).
Self-supervisedlearningbasedhandwritingverification.
[Chauhanetal.,2019] Chauhan,M.,Shaikh,M.A.,andSrihari,S.N.(2019). Explanationbasedhandwriting
verification.
[Dosovitskiyetal.,2021] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T.,Dehghani,M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,andHoulsby,N.(2021). Animageis
worth16x16words: Transformersforimagerecognitionatscale.
[FavataandSrikantan,1996] Favata, J. T. and Srikantan, G. (1996). A multiple feature/resolution approach
to handprinted digit and character recognition. International Journal of Imaging Systems and Technology,
7(4):304–311.
[Heetal.,2015] He,K.,Zhang,X.,Ren,S.,andSun,J.(2015). Deepresiduallearningforimagerecognition.
CoRR,abs/1512.03385.
[Huetal.,2021] Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,S.,Wang,L.,andChen,W.(2021).
Lora: Low-rankadaptationoflargelanguagemodels.
[Jiaetal.,2024] Jia,S.,Lyu,R.,Zhao,K.,Chen,Y.,Yan,Z.,Ju,Y.,Hu,C.,Li,X.,Wu,B.,andLyu,S.(2024).
Canchatgptdetectdeepfakes? astudyofusingmultimodallargelanguagemodelsformediaforensics.
[Scanlonetal.,2023] Scanlon,M.,Breitinger,F.,Hargreaves,C.,Hilgert,J.-N.,andSheppard,J.(2023).Chat-
GPTfordigitalforensicinvestigation: Thegood,thebad,andtheunknown. ForensicScienceInternational:
DigitalInvestigation,46:301609.
[Shaikhetal.,2018] Shaikh,M.A.,Chauhan, M.,Chu,J.,andSrihari,S.(2018). Hybridfeaturelearningfor
handwriting verification. In 2018 16th International Conference on Frontiers in Handwriting Recognition
(ICFHR),pages187–192.IEEE.
[Sriharietal.,2001] Srihari, S., Cha, S.-H., Arora, H., and Lee, S. (2001). Individuality of handwriting: a
validationstudy. InProceedingsofSixthInternationalConferenceonDocumentAnalysisandRecognition,
pages106–109.