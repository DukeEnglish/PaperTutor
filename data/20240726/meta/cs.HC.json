[
    {
        "title": "Tool-Assisted Learning of Computational Reductions",
        "authors": "Tristan KneiselElias RadtkeMarko SchmellenkampFabian VehlkenThomas Zeume",
        "links": "http://arxiv.org/abs/2407.18215v1",
        "entry_id": "http://arxiv.org/abs/2407.18215v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18215v1",
        "summary": "Computational reductions are an important and powerful concept in computer\nscience. However, they are difficult for many students to grasp. In this paper,\nwe outline a concept for how the learning of reductions can be supported by\neducational support systems. We present an implementation of the concept within\nsuch a system, concrete web-based and interactive learning material for\nreductions, and report on our experiences using the material in a large\nintroductory course on theoretical computer science.",
        "updated": "2024-07-25 17:28:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18215v1"
    },
    {
        "title": "IRIS: Wireless Ring for Vision-based Smart Home Interaction",
        "authors": "Maruchi KimAntonio GlennBandhav VeluriYunseo LeeEyoel GebreAditya BagariaShwetak PatelShyamnath Gollakota",
        "links": "http://dx.doi.org/10.1145/3654777.3676327",
        "entry_id": "http://arxiv.org/abs/2407.18141v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18141v1",
        "summary": "Integrating cameras into wireless smart rings has been challenging due to\nsize and power constraints. We introduce IRIS, the first wireless\nvision-enabled smart ring system for smart home interactions. Equipped with a\ncamera, Bluetooth radio, inertial measurement unit (IMU), and an onboard\nbattery, IRIS meets the small size, weight, and power (SWaP) requirements for\nring devices. IRIS is context-aware, adapting its gesture set to the detected\ndevice, and can last for 16-24 hours on a single charge. IRIS leverages the\nscene semantics to achieve instance-level device recognition. In a study\ninvolving 23 participants, IRIS consistently outpaced voice commands, with a\nhigher proportion of participants expressing a preference for IRIS over voice\ncommands regarding toggling a device's state, granular control, and social\nacceptability. Our work pushes the boundary of what is possible with ring\nform-factor devices, addressing system challenges and opening up novel\ninteraction capabilities.",
        "updated": "2024-07-25 15:45:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18141v1"
    },
    {
        "title": "ComPeer: A Generative Conversational Agent for Proactive Peer Support",
        "authors": "Tianjian LiuHongzheng ZhaoYuheng LiuXingbo WangZhenhui Peng",
        "links": "http://arxiv.org/abs/2407.18064v1",
        "entry_id": "http://arxiv.org/abs/2407.18064v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18064v1",
        "summary": "Conversational Agents (CAs) acting as peer supporters have been widely\nstudied and demonstrated beneficial for people's mental health. However,\nprevious peer support CAs either are user-initiated or follow predefined rules\nto initiate the conversations, which may discourage users to engage and build\nrelationships with the CAs for long-term benefits. In this paper, we develop\nComPeer, a generative CA that can proactively offer adaptive peer support to\nusers. ComPeer leverages large language models to detect and reflect\nsignificant events in the dialogue, enabling it to strategically plan the\ntiming and content of proactive care. In addition, ComPeer incorporates peer\nsupport strategies, conversation history, and its persona into the generative\nmessages. Our one-week between-subjects study (N=24) demonstrates ComPeer's\nstrength in providing peer support over time and boosting users' engagement\ncompared to a baseline user-initiated CA.",
        "updated": "2024-07-25 14:19:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18064v1"
    },
    {
        "title": "iNNspector: Visual, Interactive Deep Model Debugging",
        "authors": "Thilo SpinnerDaniel FürstMennatallah El-Assady",
        "links": "http://arxiv.org/abs/2407.17998v1",
        "entry_id": "http://arxiv.org/abs/2407.17998v1",
        "pdf_url": "http://arxiv.org/pdf/2407.17998v1",
        "summary": "Deep learning model design, development, and debugging is a process driven by\nbest practices, guidelines, trial-and-error, and the personal experiences of\nmodel developers. At multiple stages of this process, performance and internal\nmodel data can be logged and made available. However, due to the sheer\ncomplexity and scale of this data and process, model developers often resort to\nevaluating their model performance based on abstract metrics like accuracy and\nloss. We argue that a structured analysis of data along the model's\narchitecture and at multiple abstraction levels can considerably streamline the\ndebugging process. Such a systematic analysis can further connect the\ndeveloper's design choices to their impacts on the model behavior, facilitating\nthe understanding, diagnosis, and refinement of deep learning models. Hence, in\nthis paper, we (1) contribute a conceptual framework structuring the data space\nof deep learning experiments. Our framework, grounded in literature analysis\nand requirements interviews, captures design dimensions and proposes mechanisms\nto make this data explorable and tractable. To operationalize our framework in\na ready-to-use application, we (2) present the iNNspector system. iNNspector\nenables tracking of deep learning experiments and provides interactive\nvisualizations of the data on all levels of abstraction from multiple models to\nindividual neurons. Finally, we (3) evaluate our approach with three real-world\nuse-cases and a user study with deep learning developers and data analysts,\nproving its effectiveness and usability.",
        "updated": "2024-07-25 12:48:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.17998v1"
    },
    {
        "title": "Discursive Patinas: Anchoring Discussions in Data Visualizations",
        "authors": "Tobias KauerDerya AkbabaMarian DörkBenjamin Bach",
        "links": "http://arxiv.org/abs/2407.17994v1",
        "entry_id": "http://arxiv.org/abs/2407.17994v1",
        "pdf_url": "http://arxiv.org/pdf/2407.17994v1",
        "summary": "This paper presents discursive patinas, a technique to visualize discussions\nonto data visualizations, inspired by how people leave traces in the physical\nworld. While data visualizations are widely discussed in online communities and\nsocial media, comments tend to be displayed separately from the visualization\nand we lack ways to relate these discussions back to the content of the\nvisualization, e.g., to situate comments, explain visual patterns, or question\nassumptions. In our visualization annotation interface, users can designate\nareas within the visualization. Discursive patinas are made of overlaid visual\nmarks (anchors), attached to textual comments with category labels, likes, and\nreplies. By coloring and styling the anchors, a meta visualization emerges,\nshowing what and where people comment and annotate the visualization. These\npatinas show regions of heavy discussions, recent commenting activity, and the\ndistribution of questions, suggestions, or personal stories. We ran workshops\nwith 90 students, domain experts, and visualization researchers to study how\npeople use anchors to discuss visualizations and how patinas influence people's\nunderstanding of the discussion. Our results show that discursive patinas\nimprove the ability to navigate discussions and guide people to comments that\nhelp understand, contextualize, or scrutinize the visualization. We discuss the\npotential of anchors and patinas to support discursive engagements, including\ncritical readings of visualizations, design feedback, and feminist approaches\nto data visualization.",
        "updated": "2024-07-25 12:40:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.17994v1"
    }
]