[
    {
        "title": "Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal Models: An Empirical Analysis",
        "authors": "Cristian-Alexandru BotocanRaphael MeierLjiljana Dolamic",
        "links": "http://arxiv.org/abs/2407.18251v1",
        "entry_id": "http://arxiv.org/abs/2407.18251v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18251v1",
        "summary": "Assessing the robustness of multimodal models against adversarial examples is\nan important aspect for the safety of its users. We craft L0-norm perturbation\nattacks on the preprocessed input images. We launch them in a black-box setup\nagainst four multimodal models and two unimodal DNNs, considering both targeted\nand untargeted misclassification. Our attacks target less than 0.04% of\nperturbed image area and integrate different spatial positioning of perturbed\npixels: sparse positioning and pixels arranged in different contiguous shapes\n(row, column, diagonal, and patch). To the best of our knowledge, we are the\nfirst to assess the robustness of three state-of-the-art multimodal models\n(ALIGN, AltCLIP, GroupViT) against different sparse and contiguous pixel\ndistribution perturbations. The obtained results indicate that unimodal DNNs\nare more robust than multimodal models. Furthermore, models using CNN-based\nImage Encoder are more vulnerable than models with ViT - for untargeted\nattacks, we obtain a 99% success rate by perturbing less than 0.02% of the\nimage area.",
        "updated": "2024-07-25 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18251v1"
    },
    {
        "title": "Trajectory-aligned Space-time Tokens for Few-shot Action Recognition",
        "authors": "Pulkit KumarNamitha PadmanabhanLuke LuoSai Saketh RambhatlaAbhinav Shrivastava",
        "links": "http://arxiv.org/abs/2407.18249v1",
        "entry_id": "http://arxiv.org/abs/2407.18249v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18249v1",
        "summary": "We propose a simple yet effective approach for few-shot action recognition,\nemphasizing the disentanglement of motion and appearance representations. By\nharnessing recent progress in tracking, specifically point trajectories and\nself-supervised representation learning, we build trajectory-aligned tokens\n(TATs) that capture motion and appearance information. This approach\nsignificantly reduces the data requirements while retaining essential\ninformation. To process these representations, we use a Masked Space-time\nTransformer that effectively learns to aggregate information to facilitate\nfew-shot action recognition. We demonstrate state-of-the-art results on\nfew-shot action recognition across multiple datasets. Our project page is\navailable at https://www.cs.umd.edu/~pulkit/tats",
        "updated": "2024-07-25 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18249v1"
    },
    {
        "title": "RegionDrag: Fast Region-Based Image Editing with Diffusion Models",
        "authors": "Jingyi LuXinghui LiKai Han",
        "links": "http://arxiv.org/abs/2407.18247v1",
        "entry_id": "http://arxiv.org/abs/2407.18247v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18247v1",
        "summary": "Point-drag-based image editing methods, like DragDiffusion, have attracted\nsignificant attention. However, point-drag-based approaches suffer from\ncomputational overhead and misinterpretation of user intentions due to the\nsparsity of point-based editing instructions. In this paper, we propose a\nregion-based copy-and-paste dragging method, RegionDrag, to overcome these\nlimitations. RegionDrag allows users to express their editing instructions in\nthe form of handle and target regions, enabling more precise control and\nalleviating ambiguity. In addition, region-based operations complete editing in\none iteration and are much faster than point-drag-based methods. We also\nincorporate the attention-swapping technique for enhanced stability during\nediting. To validate our approach, we extend existing point-drag-based datasets\nwith region-based dragging instructions. Experimental results demonstrate that\nRegionDrag outperforms existing point-drag-based approaches in terms of speed,\naccuracy, and alignment with user intentions. Remarkably, RegionDrag completes\nthe edit on an image with a resolution of 512x512 in less than 2 seconds, which\nis more than 100x faster than DragDiffusion, while achieving better\nperformance. Project page: https://visual-ai.github.io/regiondrag.",
        "updated": "2024-07-25 17:59:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18247v1"
    },
    {
        "title": "VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads",
        "authors": "Orest KupynEugene KhvedcheniaChristian Rupprecht",
        "links": "http://arxiv.org/abs/2407.18245v1",
        "entry_id": "http://arxiv.org/abs/2407.18245v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18245v1",
        "summary": "Human head detection, keypoint estimation, and 3D head model fitting are\nimportant tasks with many applications. However, traditional real-world\ndatasets often suffer from bias, privacy, and ethical concerns, and they have\nbeen recorded in laboratory environments, which makes it difficult for trained\nmodels to generalize. Here, we introduce VGGHeads -- a large scale synthetic\ndataset generated with diffusion models for human head detection and 3D mesh\nestimation. Our dataset comprises over 1 million high-resolution images, each\nannotated with detailed 3D head meshes, facial landmarks, and bounding boxes.\nUsing this dataset we introduce a new model architecture capable of\nsimultaneous heads detection and head meshes reconstruction from a single image\nin a single step. Through extensive experimental evaluations, we demonstrate\nthat models trained on our synthetic data achieve strong performance on real\nimages. Furthermore, the versatility of our dataset makes it applicable across\na broad spectrum of tasks, offering a general and comprehensive representation\nof human heads. Additionally, we provide detailed information about the\nsynthetic data generation pipeline, enabling it to be re-used for other tasks\nand domains.",
        "updated": "2024-07-25 17:58:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18245v1"
    },
    {
        "title": "RefMask3D: Language-Guided Transformer for 3D Referring Segmentation",
        "authors": "Shuting HeHenghui Ding",
        "links": "http://arxiv.org/abs/2407.18244v1",
        "entry_id": "http://arxiv.org/abs/2407.18244v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18244v1",
        "summary": "3D referring segmentation is an emerging and challenging vision-language task\nthat aims to segment the object described by a natural language expression in a\npoint cloud scene. The key challenge behind this task is vision-language\nfeature fusion and alignment. In this work, we propose RefMask3D to explore the\ncomprehensive multi-modal feature interaction and understanding. First, we\npropose a Geometry-Enhanced Group-Word Attention to integrate language with\ngeometrically coherent sub-clouds through cross-modal group-word attention,\nwhich effectively addresses the challenges posed by the sparse and irregular\nnature of point clouds. Then, we introduce a Linguistic Primitives Construction\nto produce semantic primitives representing distinct semantic attributes, which\ngreatly enhance the vision-language understanding at the decoding stage.\nFurthermore, we introduce an Object Cluster Module that analyzes the\ninterrelationships among linguistic primitives to consolidate their insights\nand pinpoint common characteristics, helping to capture holistic information\nand enhance the precision of target identification. The proposed RefMask3D\nachieves new state-of-the-art performance on 3D referring segmentation, 3D\nvisual grounding, and also 2D referring image segmentation. Especially,\nRefMask3D outperforms previous state-of-the-art method by a large margin of\n3.16% mIoU} on the challenging ScanRefer dataset. Code is available at\nhttps://github.com/heshuting555/RefMask3D.",
        "updated": "2024-07-25 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18244v1"
    }
]