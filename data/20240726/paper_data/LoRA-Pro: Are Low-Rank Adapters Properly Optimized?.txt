LoRA-Pro: Are Low-Rank Adapters Properly Optimized?
Zhengbo Wang1,2, Jian Liang2,3 ∗
1 University of Science and Technology of China
2 NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences
3 School of Artificial Intelligence, University of Chinese Academy of Sciences
zhengbowang@mail.ustc.edu.cn, liangjian92@gmail.com
Abstract
Low-RankAdaptation,alsoknownasLoRA,hasemergedasaprominentmethodforparameter-efficient
fine-tuningfoundationmodelsbyre-parameterizingtheoriginalmatrixintotheproductoftwolow-rank
matrices.Despiteitsefficiency,LoRAoftenyieldsinferiorperformancecomparedtofullfine-tuning.In
thispaper,weproposeLoRA-Protobridgethisperformancegap.
Firstly,wedelveintotheoptimizationprocessesinLoRAandfullfine-tuning.WerevealthatwhileLoRA
employslow-rankapproximation,itneglectstoapproximatetheoptimizationprocessoffullfine-tuning.
Toaddressthis,weintroduceanovelconceptcalledthe"equivalentgradient."Thisvirtualgradientmakes
theoptimizationprocessonthere-parameterizedmatrixequivalenttoLoRA,whichcanbeusedtoquantify
thedifferencesbetweenLoRAandfullfine-tuning.Theequivalentgradientisderivedfromthegradientsof
matricesAandB.Tonarrowtheperformancegap,ourapproachminimizesthedifferencesbetweenthe
equivalentgradientandthegradientobtainedfromfullfine-tuningduringtheoptimizationprocess.By
solvingthisobjective,wederiveoptimalclosed-formsolutionsforupdatingmatricesAandB.Ourmethod
constrainstheoptimizationprocess,shrinkingtheperformancegapbetweenLoRAandfullfine-tuning.
Extensiveexperimentsonnaturallanguageprocessingtasksvalidatetheeffectivenessofourmethod.
1 Introduction
Foundationalmodels[Radfordetal.,2021,Brownetal.,2020,Achiametal.,2023,Kirillovetal.,2023,
Rombachetal.,2022]havebecomethecornerstoneofmoderndeeplearning. Byundergoingpre-trainingon
massivedatasets,thesemodelstypicallyexhibitexcellentgeneralizationandversatility. Remarkably,some
foundationmodelsevendemonstrateemergentproperties[Hoffmannetal.,2022,Kaplanetal.,2020]. Asa
result,foundationmodelshavebeenwidelyappliedtovariousdownstreamapplications.
Despitetheseadvantages,thehugenumberofparametersinfoundationalmodelshinderstheirbroader
application. Thesubstantialparametercountresultsinhighfine-tuningcostsforthesetasks. Toaddress
thisissue,recentresearchhasfocusedonparameter-efficientfine-tuning(PEFT)methods[Huetal.,2022,
Houlsbyetal.,2019,Lesteretal.,2021,Zhouetal.,2022]. PEFTmethodsreducethefine-tuningcostby
keepingthefoundationmodelsfrozenandonlyfine-tuningsmall,additionallightweightadapters. Withthe
majorityofparametersfrozen,PEFTenablesfasterfine-tuningandrequiresfewercomputationalresources.
Low-rankadaptation[Huetal.,2022],alsoknownasLoRA,isoneofthemostfamousPEFTmethods,
whichhasbeenwidelyadoptedacrossvariousdomains. Inspiredbypreviousworks[Aghajanyanetal.,2021,
Lietal.,2018],LoRAhypothesizesthatthechangesinweightsduringmodeladaptationexhibitalow-rank
structure. Tocapturethis,LoRAre-parameterizesthesechangesbyexpressingthemastheproductoftwo
low-rankmatrices: W =W +∆W ≈W +sBA,wheres isascalingfactor,andA∈Rr×n andB∈Rm×r are
0 0
low-rankmatriceswithrankr≪min(m,n). LoRAreducesthenumberoftrainableparametersfromm×nto
∗
Correspondenceto:JianLiang(liangjian92@gmail.com)
1
4202
luJ
52
]GL.sc[
1v24281.7042:viXrar×(m+n),therebydecreasingthecostoffine-tuning. However,despiteitsefficiency,LoRA’sfine-tuning
performance often falls short compared to full fine-tuning [Hu et al., 2022, Liu et al., 2024, Ding et al.,
2023].
Inthispaper,weproposeanovelPEFTmethod,LoRA-Pro,aimedatbridgingthegapbetweenLoRA
andfullfine-tuning. WhileLoRAemployslow-rankapproximationbyre-parametrizingweightchangesas
theproductoftwolow-rankmatrices,itfallsshortinapproximatingtheoptimizationprocessoffullfine-
tuning. Tomeasuretheirdiscrepancyintheoptimizationprocess,weproposeanovelconcept,“Equivalent
Gradient",forLoRAoptimization. Equivalentgradientcharacterizesthegradientoftheoriginalmatrixafter
low-rankapproximation(despiteitnotbeingdirectlytrainable),iscomposedofgradientsfrommatrices
A and B. Thus, during LoRA fine-tuning, our goal is not only to approximate the matrix with low-rank
matricesbutalsotominimizethedifferencebetweentheequivalentgradientandthegradientfromfull
fine-tuningduringthegradientdescentprocess. Thisisachievedbyselectingappropriategradientsfor
matricesAandB,ensuringamoreaccurateandeffectivefine-tuningprocess. Toachievethis,weformulate
itasanoptimizationproblem. Wethenderivetheoreticalsolutionsfortheproblem,presentingoptimal
gradientsforupdatingmatricesAandB.Thesesolutionsensurethattheequivalentgradientcloselymatch
theoptimizationdynamicsoffullfine-tuning. Bydoingso,weenhancetheeffectivenessLoRA,bridgingthe
gapbetweenLoRAandfullfine-tuning.
Ourmaincontributionsaresummarizedasfollows:
• WeidentifythatLoRAapproximateslow-rankmatricesbutneglectstoapproximatetheoptimization
processoffullparameterfine-tuning. Thisshortcomingisoneofthereasonsfortheperformancegap
betweenLoRAandfullfine-tuning.
• WeintroducetheconceptofEquivalentGradient,whichallowsustoquantifythediscrepancyinthe
optimizationprocessbetweenLoRAandfullfine-tuning. Byminimizingthisdiscrepancy,wederive
theoptimalclosed-formupdatedsolutionsforLoRA.
• Extensiveexperimentsonnaturallanguageprocessingtasksvalidatetheeffectivenessofourmethod.
2 Related Work
Parameter-EfficientFine-Tuning. Giventhehugesizeoffoundationmodels,recentresearchhasfocused
ondevelopingparameter-efficientfine-tuningmethods[Huetal.,2022,Liuetal.,2024,Dingetal.,2023,
Houlsbyetal.,2019,Liuetal.,2023,Lesteretal.,2021]. Thesemethodsaimtoreducethecostoffine-tuning
byadjustingonlyasmallportionofthemodel’sparameters. Generally,thesemethodsfallintotwomain
categories. Thefirstcategoryisadaptertuning[Houlsbyetal.,2019,Sungetal.,2022,Heetal.,2021,Zhang
etal.,2024,BapnaandFirat,2019,Huetal.,2022],whichinvolvesinsertingsmallneuralnetworkmodules,
calledadapters,intospecificlayersofthemodel. Duringfine-tuning,wekeepthemodelfrozenandonly
fine-tunethelightweightadaptermodules,significantlyreducingthememoryfootprintforfine-tuning.
Thesecondcategoryisprompttuning[Lesteretal.,2021,Zhouetal.,2022,LiandLiang,2021,Liuetal.,
2022]. Prompttuningadaptsthemodelstospecifictasksbyaddingspeciallydesignedpromptsorlearnable
tokenstotheinputdata,ratherthandirectlymodifyingtheinternalparametersoffoundationmodels. In
thispaper,wefocusonLoRA[Huetal.,2022],aprominentmethodwithintherealmofadaptertuning.
LowRankAdaptation. Low-rankadaptation,initiallyreferredtoasLoRA[Huetal.,2022],hasevolved
intoabroadcategoryencompassingparameter-efficientfine-tuningmethodsbasedonlow-rankapproxima-
tions[Huetal.,2022,Liuetal.,2024,Hayouetal.,2024,Kalajdzievski,2023,Zhangetal.,2023,Kopiczko
etal.,2024,Hyeon-Wooetal.,2022,ZhangandPilanci,2024,Wangetal.,2024,Zhaoetal.,2024]. LoRA[Hu
etal.,2022]assumesthatthechangesintheweightsofpre-trainedmodelsexhibitalow-rankstructure.
Consequently,itre-parameterizesthesechangesastheproductoflow-rankmatrices,therebyreducingthe
costassociatedwithfine-tuning.
SeveralvariantsofLoRAhavebeenproposedtoaddressdifferentaspectsofthisapproach. Forexample,
DoRA[Liuetal.,2024]improvesLoRA[Huetal.,2022]byincorporatingalearnablemagnitudevector
2tore-scalethenormalizedproductoflow-rankmatrices. Anothervariant,rsLoRAKalajdzievski[2023],
introduces a new scaling factor to stabilize training in high-rank scenarios. LoRA+[Hayou et al., 2024]
improves upon LoRA by applying different learning rates to the two low-rank matrices. Additionally,
Galore[Zhaoetal.,2024]employsSVDtoprojectthegradientsoffullparametertrainingintoalow-rank
space,therebyreducingthememoryfootprintduringpre-trainingandfine-tuning.
3 Method
Inthissection,webeginbyrevisitingLoRA[Huetal.,2022]inSection3.1. Followingthis,weconducta
comparisonbetweenLoRAandfullfine-tuningfromanoptimizationperspectiveinSection3.2. Finally,in
Section3.3,wepointoutthatLoRAfallsshortinapproximatingfullfine-tuningduringtheoptimization
process,andweintroduceLoRA-Proasasolutiontobridgethisperformancegap.
3.1 RevisitLowRankAdaptation
Firstofall,let’sdivebackintoLow-RankAdaptation(LoRA)[Huetal.,2022]. LoRA’scoreidearevolves
aroundrecognizingthelow-rankstructureofthechangematrix∆W inthestandardfine-tuningprocess.
ThisinsightallowsLoRA[Huetal.,2022]tore-parameterizethechangematrixintotheproductoftwo
low-rankmatrices,
W =W +∆W =W +sBA. (1)
0 0
Here,W
∈Rm×n representsthepre-trainedweightmatrix,B∈Rm×r andA∈Rr×n
arethelow-rankmatrices,
0
andsisascalingfactor. ForLoRA[Huetal.,2022],s= α,whileforrsLoRA[Kalajdzievski,2023],s= √α .
r r
Here, α is the hyper-parameter and r ≪ min(m,n) denotes the rank. Consequently, LoRA significantly
reducesthenumberoffine-tuningparametersfromm×ntor×(m+n).
3.2 LoRAv.s. FullFine-tuning
Despitewidespreadapplicationsacrossvariousdomains,LoRA’sperformancestillfallsshortwhencompared
to full fine-tuning. In this part, we review and compare LoRA and full fine-tuning in the optimization
process. Infullfine-tuning,weutilizedifferentialtoanalyzetherelationshipbetweenchangesintheloss
andchangesintheweights:
∂L
dL=⟨ ,dW⟩ , (2)
F
∂W
wheredLanddW denotesthechangesoftheparameterW andthelossL,and∥·∥ istheFrobeniusnorm.
F
Tominimizethelossfunction,wetypicallysetdW =− ∂L ≜−g (omittingthelearningrateforsimplicity),
∂W
whichresultsindL=−∥ ∂L ∥2 ≤0.
∂W F
InLoRAoptimization,giventhatW =W +sBA,wecomputethedifferentialusingthechainrule:
0
∂L
dL=⟨ ,dW⟩
F
∂W
∂L ∂WT ∂WT
=⟨ , dA+ dB⟩
F
∂W ∂A ∂B
(3)
∂L ∂W ∂L ∂W
=⟨ ,dA⟩ +⟨ ,dB⟩
F F
∂W ∂A ∂W ∂B
∂L ∂L
=⟨ ,dA⟩ +⟨ ,dB⟩ .
F F
∂A ∂B
Similarly,LoRAsetsdA=−∂L ≜−gA anddB=−∂L ≜−gB ,andthusdL=−∥∂L∥2−∥∂L∥2 ≤0. Moreover,
∂A lora ∂B lora ∂A F ∂B F
employingthechainrule,wederive:
∂L ∂W ∂L ∂W
gA = =sBTg, gB = =sgAT. (4)
lora ∂W ∂A lora ∂W ∂B
33.3 Low-RankAdaptationwithEquivalentGradient
Definition3.1(EquivalentGradient)
InthecontextofLoRAoptimization,wedefinetheequivalentgradientas,
∂WT ∂WT
g˜≜ gA+ gB=sBgA+sgBA, (5)
∂A ∂B
wheresisthescalingfactor,andgA andgB aregradientswithrespecttoAandB,respectively.
Inthissection,EquivalentGradient. FromEquation(3),wecanseethatchangesinmatricesAandBare
inherentlylinkedtochangesinmatrixW throughthechainrule:
∂WT ∂WT
dW = dA+ dB=−(sBgA +sgB A). (6)
∂A ∂B lora lora
Incomparisontofullfine-tuning,thisisequivalenttoupdatingW usingthegradientg˜=sBgA +sgB A.
lora lora
This critical relationship has been neglected in the LoRA optimization process. Hence, we hypothesize
that by carefully adjusting the gradients of matrices A and B in such a way that g˜ under LoRA closely
approximatesthegradientg fromfullfine-tuning,wecaneffectivelybridgethegapbetweenLoRAandfull
fine-tuning.
Based on this relationship, we define the concept of equivalent gradient in Definition 1. Equivalent
gradient describes the gradient of the matrix W following low-rank adaptation, despite W not being a
trainableparameter. Tonarrowtheperformancegap,ourgoalistocarefullyselectsuitablegA andgB to
minimizethedistancebetweentheequivalentgradientg˜andthegradientunderfullfine-tuningg. Hence,
ourobjectiveis:
min∥g˜−g∥2
F
gA,gB
s.t. g˜=sBgA+sgBA, (7)
dL≤0.
Theorem3.1
Assume matrices B ∈ Rm×r,A ∈ Rr×n are both full rank. For the objective min ∥g˜−g∥2, the
gA,gB F
solutionsaregivenby:
gA= 1 (BTB)−1BTg+XA= 1 (BTB)−1gA +XA (8)
s s2 lora
gB= 1 [I−B(BTB)−1BT]gAT(AAT)−1−BX= 1 [I−B(BTB)−1BT]gB (AAT)−1−BX. (9)
s s2 lora
Here,X∈Rr×r
representsanarbitrarymatrix.
Closed-formSolution. Fortunately,Equation(7)admitsaclosed-formsolution. AccordingtoTheorem3.1,
weobtaintheoptimalgradientsformatricesAandB,ensuringthattheequivalentgradientachievesthe
bestapproximationtothefullfine-tuninggradient. Moreover,weobservethatgA andgB canbeexpressed
as gA and gB , respectively, indicating that we do not explicitly possess the full fine-tuning gradient
lora lora
g. Therefore, ourapproachinvolvesback-propagatinginstandardLoRAandadjustingthegradientsof
matricesAandBusingtheclosed-formsolutionoutlinedinTheorem3.1.
4Theorem3.2
WhenupdatingmatricesAandBusingtheclosed-formsolutionfromTheorem3.1,weproceedas
follows:
A←A−γgA (10)
B←B−γgB, (11)
whereγ ≥0denotesthelearningrate. Ourmethodensuresadecreaseintheloss,akintothestandard
gradientdescentalgorithm,expressedby:
dL=−γ{⟨gA , 1 (BTB)−1gA ⟩ +⟨gB , 1 [I−B(BTB)−1BT]gB (AAT)−1⟩ }≤0 (12)
lora s2 lora F lora s2 lora F
AlthoughTheorem3.1providesaclosed-formsolutiontotheoptimizationproblemmin ∥g˜−g∥2,
gA,gB F
this does not necessarily mean that updating matrices A and B with this solution will decrease the loss.
To address this, we have Theorem 3.2, which guarantees a decrease in the loss during the optimization
process. Thistheoremindicatesthatthechangeinloss,dL,canbeexpressedasanegativescalarmultiplied
bythesumoftwopositivedefinitequadraticforms. ThisrelationshipensuresthatdL≤0duringtheupdate
process,thusconsistentlydrivingtheoptimizationprocesstowardsalowerloss.
Theorem3.3
Considertheoptimizationproblem,
min∥gA−gA ∥2+∥gB−gB ∥2, (13)
lora F lora F
X
wheregAandgBaretheoptimalsolutionsasstatedinTheorem3.1. TheoptimalXcanbedetermined
bysolvingtheSylvesterequation:
BTBX+XAAT =−1 (BTB)−1gA AT, (14)
s2 lora
whichhasauniquesolutionX providedthatBTBand−AAT donothaveanysharedeigenvalues.
SelectionofX.AlthoughtheequivalentgradientitselfisnotdirectlyrelatedtothematrixX,thepresence
of X plays a significant role in the updates of matrices A and B. We select an appropriate X such that
gA andgB remainclosetogA andgB respectively. Consequently,weminimizetheirFrobeniusnorm,
lora lora
as demonstrated in Equation (41). In practical terms, BTB and AAT do not share common eigenvalues.
Therefore,accordingtoTheorem3.3,wecandetermineauniqueoptimalX forupdatingmatricesAandB.
4 Experimental Results
Inthissection,weevaluateourLoRA-Promethodacrossvariousnaturallanguageunderstandingdatasets.
Toprovideacomprehensivecomparison,weincludeseveralbaselinemethods: 1)fullfine-tuningandthe
standardLoRA[Huetal.,2022]. 2)LoRAvariantsmaintainingtheoriginalstructure,suchasrsLoRA[Kala-
jdzievski, 2023], LoRA+ [Hayou et al., 2024], PiSSA [Meng et al., 2024], 3) oRA variants with modified
structures,includingDoRA[Liuetal.,2024]andAdaLoRA[Zhangetal.,2023].
TheresultsareshowninTable1. Wefine-tunetheT5-basemodel[Raffeletal.,2020]withthebaseline
methodsonasubsetofGLUEdatasets. FromTable1,weobservethatLoRA-Proachievesthehighestscores
on 3 out of 5 datasets and the highest average score across all 5 datasets. Moreover, on average over 5
5datasets,LoRA-ProsuppassstandardLoRA[Huetal.,2022]withamarginof6.72. Theseresultsvalidate
theeffectivenessofourmethods.
Table 1: Results on fine-tuning T5-base with Full Fine-tuning and LoRA variants on a subset of GLUE
datasets.
Method MNLI SST2 CoLA QNLI MRPC Avg.
FullFT 86.33±0.00 94.75±0.21 80.70±0.24 93.19±0.22 84.56±0.73 87.91
LoRA 85.30±0.04 94.04±0.11 69.35±0.05 92.96±0.09 68.38±0.01 82.08
PiSSA 85.75±0.07 94.07±0.06 74.27±0.39 93.15±0.14 76.31±0.51 84.71
rsLoRA 85.73±0.10 94.19±0.23 72.32±1.12 93.12±0.09 52.86±2.27 79.64
LoRA+ 85.81±0.09 93.85±0.24 77.53±0.20 93.14±0.03 74.43±1.39 84.95
DoRA 85.67±0.09 94.04±0.53 72.04±0.94 93.04±0.06 68.08±0.51 82.57
AdaLoRA 85.45±0.11 93.69±0.20 69.16±0.24 91.66±0.05 68.14±0.28 81.62
LoRA-GA 85.70±0.09 94.11±0.18 80.57±0.20 93.18±0.06 85.29±0.24 87.77
LoRA-Pro 86.92±0.08 94.46±0.24 82.25±1.01 92.89±0.12 87.50±0.65 88.80
5 Conclusion
Inthispaper,weintroduceLoRA-Pro,anovelapproachdesignedtobridgetheperformancegapbetween
LoRAandfullfine-tuning. Tobridgetheperformancegap,weintroducetheconceptofEquivalentGradient,
whichallowsustoquantifythedifferenceintheoptimizationprocessbetweenLoRAandfullfine-tuning.
Byminimizingthisdiscrepancy,wederivetheoptimalclosed-formupdatedsolutionsforLoRA.Moreover,
weprovethatthesolutionsguaranteethelossdeceaseduringoptimization. Thesesolutionsnotonlyapplya
low-rankapproximationtothefine-tuningmatrixbutalsomaintainconsistencywiththeoptimizationof
fullfine-tuning,enablingmoreeffectivefine-tuning. Finally,wevalidatetheeffectivenessofourmethod
throughextensiveexperimentsonnaturallanguageprocessingtasks.
References
J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman,D.Almeida,J.Altenschmidt,S.Altman,
S.Anadkat,etal. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.
A.Aghajanyan,S.Gupta,andL.Zettlemoyer. Intrinsicdimensionalityexplainstheeffectivenessoflanguage
modelfine-tuning. InACL-IJCNLP,2021.
A.BapnaandO.Firat. Simple,scalableadaptationforneuralmachinetranslation. InEMNLP-IJCNLP,2019.
T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,G.Sastry,
A.Askell,etal. Languagemodelsarefew-shotlearners. InNeurIPS,2020.
N.Ding,Y.Qin,G.Yang,F.Wei,Z.Yang,Y.Su,S.Hu,Y.Chen,C.-M.Chan,W.Chen,etal.Parameter-efficient
fine-tuningoflarge-scalepre-trainedlanguagemodels. NatureMachineIntelligence,5(3):220–235,2023.
S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models. arXiv preprint
arXiv:2402.12354,2024.
R.He,L.Liu,H.Ye,Q.Tan,B.Ding,L.Cheng,J.Low,L.Bing,andL.Si.Ontheeffectivenessofadapter-based
tuningforpretrainedlanguagemodeladaptation. InACL-IJCNLP,2021.
6J.Hoffmann,S.Borgeaud,A.Mensch,E.Buchatskaya,T.Cai,E.Rutherford,D.deLasCasas,L.A.Hendricks,
J.Welbl,A.Clark,etal. Trainingcompute-optimallargelanguagemodels. InNeurIPS,2022.
N.Houlsby, A.Giurgiu, S.Jastrzebski, B.Morrone, Q.DeLaroussilhe, A.Gesmundo, M.Attariyan, and
S.Gelly. Parameter-efficienttransferlearningfornlp. InICML,2019.
E.J.Hu,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,W.Chen,etal. Lora: Low-rankadaptationoflarge
languagemodels. InICLR,2022.
N.Hyeon-Woo,M.Ye-Bin,andT.-H.Oh. Fedpara: Low-rankhadamardproductforcommunication-efficient
federatedlearning. InICLR,2022.
D.Kalajdzievski.Arankstabilizationscalingfactorforfine-tuningwithlora.arXivpreprintarXiv:2312.03732,
2023.
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and
D.Amodei. Scalinglawsforneurallanguagemodels. arXivpreprintarXiv:2001.08361,2020.
A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,A.C.Berg,W.-Y.Lo,
etal. Segmentanything. InICCV,2023.
D.J.Kopiczko,T.Blankevoort,andY.M.Asano. Vera: Vector-basedrandommatrixadaptation. InICLR,
2024.
B.Lester,R.Al-Rfou,andN.Constant. Thepowerofscaleforparameter-efficientprompttuning. InEMNLP,
2021.
C.Li,H.Farkhoor,R.Liu,andJ.Yosinski. Measuringtheintrinsicdimensionofobjectivelandscapes. In
ICLR,2018.
X.L.LiandP.Liang. Prefix-tuning: Optimizingcontinuouspromptsforgeneration. InACL-IJCNLP,2021.
P.Liu,W.Yuan,J.Fu,Z.Jiang,H.Hayashi,andG.Neubig. Pre-train,prompt,andpredict: Asystematic
surveyofpromptingmethodsinnaturallanguageprocessing. ACMComputingSurveys,55(9):1–35,2023.
S.-y. Liu, C.-Y. Wang, H. Yin, P. Molchanov, Y.-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-
decomposedlow-rankadaptation. InICML,2024.
X.Liu,K.Ji,Y.Fu,W.Tam,Z.Du,Z.Yang,andJ.Tang. P-tuning: Prompttuningcanbecomparableto
fine-tuningacrossscalesandtasks. InACL,2022.
I.LoshchilovandF.Hutter. Decoupledweightdecayregularization. InICLR,2019.
F.Meng,Z.Wang,andM.Zhang. Pissa: Principalsingularvaluesandsingularvectorsadaptationoflarge
languagemodels. arXivpreprintarXiv:2404.02948,2024.
A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,P.Mishkin,J.Clark,
etal. Learningtransferablevisualmodelsfromnaturallanguagesupervision. InICML,2021.
C.Raffel,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.Liu. Exploringthe
limitsoftransferlearningwithaunifiedtext-to-texttransformer. Journalofmachinelearningresearch,21
(140):1–67,2020.
R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer. High-resolutionimagesynthesiswithlatent
diffusionmodels. InCVPR,2022.
Y.-L.Sung,J.Cho,andM.Bansal. Vl-adapter: Parameter-efficienttransferlearningforvision-and-language
tasks. InCVPR,2022.
7I.Sutskever,J.Martens,G.Dahl,andG.Hinton. Ontheimportanceofinitializationandmomentumindeep
learning. InICML,2013.
S. Wang, L. Yu, and J. Li. Lora-ga: Low-rank adaptation with gradient approximation. arXiv preprint
arXiv:2407.05000,2024.
F.ZhangandM.Pilanci. Riemannianpreconditionedloraforfine-tuningfoundationmodels. InICML,2024.
Q.Zhang,M.Chen,A.Bukharin,P.He,Y.Cheng,W.Chen,andT.Zhao. Adaptivebudgetallocationfor
parameter-efficientfine-tuning. InICLR,2023.
R.Zhang,J.Han,C.Liu,A.Zhou,P.Lu,Y.Qiao,H.Li,andP.Gao. Llama-adapter: Efficientfine-tuningof
largelanguagemodelswithzero-initializedattention. InICLR,2024.
J.Zhao,Z.Zhang,B.Chen,Z.Wang,A.Anandkumar,andY.Tian. Galore: Memory-efficientllmtrainingby
gradientlow-rankprojection. InICML,2024.
K.Zhou,J.Yang,C.C.Loy,andZ.Liu. Learningtopromptforvision-languagemodels. InternationalJournal
ofComputerVision,130(9):2337–2348,2022.
8LoRA-Pro: Are Low-Rank Adapters Properly Optimized?
————Appendix————
ThestructureofAppendixisasfollows,
• AppendixAcontainsthenotationusageinourpaper.
• AppendixBcontainstheproofsofthetheoremsinthemainmanuscript.
• AppendixCdetailstheoptimizationalgorithmoftheproposedmethods.
A Notations
InTable2,wedetailthenotationsutilizedinourpaper.
Table2: Descriptionofnotationsusedinthepaper.
Notation Description
s scalingfactorinlora
B∈Rm×r,A∈Rr×n
lowrankmatricesinLoRA
g= ∂L ∈Rm×n gradientsoffullrankfine-tuning
∂W
gA = ∂L =sBTg∈Rr×n gradientsofmatrixAinlora
lora ∂A
gB = ∂L =sgAT ∈Rm×r gradientsofmatrixBinlora
lora ∂B
dL differentialofthelossfunction
dA differentialofthematrixA
dB differentialofthematrixB
∥·∥ FrobeniusNorm
F
⟨·,·⟩ Frobeniusinnerproduct
F
B Proof of Theoretical Results
B.1 ProofofTheorem3.1
TheoremB.1
Assume matrices B ∈ Rm×r,A ∈ Rr×n are both full rank. For the objective min ∥g˜−g∥2, the
gA,gB F
solutionsaregivenby:
gA= 1 (BTB)−1BTg+XA= 1 (BTB)−1gA +XA (15)
s s2 lora
gB= 1 [I−B(BTB)−1BT]gAT(AAT)−1−BX= 1 [I−B(BTB)−1BT]gB (AAT)−1−BX. (16)
s s2 lora
Here,X∈Rr×r
representsanarbitrarymatrix.
9Proof
For simplicity, we denote L=∥sBgA+sgBA−g∥2. To solve the optimization problem, we need to
F
satisfythefollowingconditions:
∂L
=2sBT(sBgA+sgBA−g)=0 (17)
∂A
∂L
=2(sBgA+sgBA−g)sAT =0 (18)
∂B
GiventhatmatricesAandBarefull-rank,AAT andBTBareinvertible. AndfromEquation(18),we
derive:
gB= 1 gAT(AAT)−1−BgAAT(AAT)−1. (19)
s
SubstitutingthisintoEquation(17),weobtainthefollowinglinearequation:
gA[I−AT(AAT)−1A]= 1 (BTB)−1BTg.
(20)
s
Here, we notice that the matrix P = I −AT(AAT)−1A is a projection matrix with rank n−r. The
solutiontothelinearequation(20)is:
gA= 1 (BTB)−1BTg+XA, (21)
s
where X
∈Rr×r
represents an arbitrary matrix. We take the solution (24) into Equation (19), we
derive:
gB= 1 [I−B(BTB)−1BT]gAT(AAT)−1−BX (22)
s
Whilewehaveobtainedclosed-formsolutionsforgAandgB,thesesolutionsexplicitlydependonthe
gradientofthematrixW,i.e.,g,whichisundesirablesinceg isunknownduringLoRAoptimization.
Fortunately, the solutions can be transformed into the forms of the gradients of standard LoRA,
wherethegradientsare:
gA =sBTg, gB =sgAT. (23)
lora lora
Therefore,thesolutionstotheoptimizationproblemcanbewrittenas:
gA= 1 (BTB)−1gA +XA, (24)
s2 lora
gB= 1 [I−B(BTB)−1BT]gB (AAT)−1−BX. (25)
s2 lora
Inourmethod, weperformthestandardforward andbackwardpassesofLoRA, thenadjustthe
gradientsofAandBusingSolutions(24)and(25),andsubsequentlyupdatethem.
10B.2 ProofofTheorem3.2
TheoremB.2
WhenupdatingmatricesAandBusingtheclosed-formsolutionfromTheorem3.1,weproceedas
follows:
A←A−γgA, (26)
B←B−γgB, (27)
whereγ ≥0denotesthelearningrate. Ourmethodensuresadecreaseintheloss,akintothe
standardgradientdescentalgorithm,expressedby:
dL=−γ{⟨gA , 1 (BTB)−1gA ⟩ +⟨gB , 1 [I−B(BTB)−1BT]gB (AAT)−1⟩ }≤0 (28)
lora s2 lora F lora s2 lora F
11Proof(Part1)
Insummary,theproofofTheorem3.2isdividedintotwodistinctparts. Tobeginwith,wedemon-
stratethatdLcanbeexpressedinthefollowingform:
dL=−γ{⟨gA , 1 (BTB)−1gA ⟩ +⟨gB , 1 [I−B(BTB)−1BT]gB (AAT)−1⟩ }. (29)
lora s2 lora F lora s2 lora F
Inthesecondpart,weprovethatthisexpressionfordLisalwayslessthanorequaltozero: dL≤0.
Therefore,inthispart,wefirstproveEquation(29). Duringtheoptimizationprocess,thedifferential
changeinthelossfunction,dL,canbeexpressedintermsofthedifferentialsdAanddBasfollows:
∂L ∂L
dL=⟨ ,dA⟩ +⟨ ,dB⟩ . (30)
F F
∂A ∂B
FromEquation(26)and(27),wecanderivethat:
dA=−γgA, dB=γgB. (31)
Giventhat ∂L =gA and ∂L =gB ,itfollowsthat:
∂A lora ∂B lora
dL=−γ(⟨gA ,gA⟩ +⟨gB ,gB⟩ )
lora F lora F
=−γ(⟨gA , 1 (BTB)−1gA ⟩ +⟨gB , 1 [I−B(BTB)−1BT]gB (AAT)−1⟩ (32)
lora s2 lora F lora s2 lora F
+⟨gA ,XA⟩ −⟨gB ,BX⟩ ).
lora F lora F
Andwehavethefollowingequation:
⟨gA ,XA⟩ −⟨gB ,BX⟩
lora F lora F
=⟨gA AT,X⟩ −⟨BTgB ,X⟩
lora F lora F
=⟨gA AT −BTgB ,X⟩ (33)
lora lora F
=⟨(sBTg)AT −BT(sgAT),X⟩
F
=0.
Therefore,wehave:
dL=−γ{⟨gA , 1 (BTB)−1gA ⟩ +⟨gB , 1 [I−B(BTB)−1BT]gB (AAT)−1⟩ }. (34)
lora s2 lora F lora s2 lora F
12Proof(Part2)
In this part, we aim to prove dL ≤ 0. Given that the learning rate γ > 0, it suffices to show the
followinginequalities:
⟨gA , 1 (BTB)−1gA ⟩ ≥0, (35)
lora s2 lora F
⟨gB , 1 [I−B(BTB)−1BT]gB (AAT)−1⟩ ≥0. (36)
lora s2 lora F
Byprovingtheseinequalities,wecanestablishthatdL≤0asderivedfromEquation(29).
①Proofof⟨gA , 1(BTB)−1gA ⟩ ≥0.
lora s2 lora F
Tobeginwith,weneedtoshowthat(BTB)−1 ispositivedefinite. Toestablishthis,itissufficientto
showthatBTBispositivedefinite,astheinverseofapositivedefinitematrixisalsopositivedefinite.
Toachievethis,consideranynon-zerovectorx,andnotingthatBisfull-rank,wehave,
⟨x,BTBx⟩=⟨Bx,Bx⟩=∥Bx∥2>0. (37)
ThisshowsthatBTBispositivedefinite. Consequently,(BTB)−1 ispositivedefiniteaswell. Since
(BTB)−1 ispositivedefinite,andthuswecanapplyCholeskydecomposition,and(BTB)−1 =UUT.
Withthis,wehave,
⟨gA , 1 (BTB)−1gA ⟩ = 1 ⟨gA ,UUTgA ⟩
lora s2 lora F s2 lora lora F
1
= ⟨UTgA ,UTgA ⟩ (38)
s2 lora lora F
1
= ∥UTgA ∥2 ≥0
s2 lora F
②Proofof⟨gB , 1[I−B(BTB)−1BT]gB (AAT)−1⟩ ≥0.
lora s2 lora F
Similarly, we can prove that matrix
(AAT)−1
is positive-definite. By employing Cholesky decom-
position,weexpress(AAT)−1=UUT,whereU
isalower-trianglematrix. Subsequently,wedefine
P =I−B(BTB)−1BT. ItcanbeshownthatP2 =P, indicatingthatP isaprojectionmatrix. Conse-
quently,theeigenvaluesofP areeither0or1,whichimpliesthatP ispositivesemi-definite. Utilizing
theCholeskydecomposition,wederivethatP =VVT,whereV isalower-trianglematrix. Finally,we
have:
⟨gB , 1 [I−B(BTB)−1BT]gB (AAT)−1⟩ = 1 ⟨gB ,VVTgB UUt⟩
lora s2 lora F s2 lora lora F
1
= ⟨VTgB U,VTgB U⟩ (39)
s2 lora lora F
1
= ∥VTgB U∥2 ≥0
s2 lora F
Insummary,basedontheaboveproofs,wehavedemonstratedthat:
dL=−γ{⟨gA , 1 (BTB)−1gA ⟩ +⟨gB , 1 [I−B(BTB)−1BT]gB (AAT)−1⟩ }≤0 (40)
lora s2 lora F lora s2 lora F
13B.3 ProofofTheorem3.3
TheoremB.3
Considertheoptimizationproblem,
min∥gA−gA ∥2+∥gB−gB ∥2, (41)
lora F lora F
X
wheregAandgBaretheoptimalsolutionsasstatedinTheorem3.1. TheoptimalXcanbedetermined
bysolvingtheSylvesterequation:
BTBX+XAAT =−1 (BTB)−1gA AT, (42)
s2 lora
whichhasauniquesolutionX providedthatBTBand−AAT donothaveanysharedeigenvalues.
Proof
Forsimplicity,wedenoteL=∥gA−gA ∥2+∥gB−gB ∥2. Tosolvetheoptimizationproblem,weneed
lora F lora F
tosatisfythefollowingconditions:
∂L
=0. (43)
∂X
SincegA andgB aresolutionsinTheorem3.1andgA =sBTg andgB =sgAT,weobtainthat:
lora lora
2(gA−gA )AT −2BT(gB−gB )=0,
lora lora
⇒ gAAT −BTgB=gA AT −BTgB ,
lora lora (44)
⇒ BTBX+XAAT =−1 (BTB)−1gA AT,
s2 lora
whichisaSylvesterequation. ThisequationhasauniquesolutionforX ifandonlyifBTBand−AAT
havenosharedeigenvalues.
C Optimization Algorithms
Inthissection,wepresentthepseudo-codesforimplementingourLoRA-PromethodusingtheSGD[Sutskever
etal.,2013]andAdamW[LoshchilovandHutter,2019]optimizers. ThesearedetailedinAlgorithm1and
Algorithm2,respectively.
InthestandardSGDalgorithm,asillustratedinAlgorithm1,allweneedtodoisadjustingthegradients
ofmatricesAandBwiththesolutionsinTheorem3.1.
InAdamWoptimizer,theimplementationbecomesmorecomplex. Severalmodificationsarenecessary.
Firstly, inordertomimicfullfine-tuning, afteradjustingthegradientsofmatricesAandB, weneedto
computetheequivalentgradient,
g˜=sgBA+sBgA. (45)
Subsequently,wecalculatethefirstandsecondmomentsofthisequivalentgradienttoderivethecorre-
spondingAdamWgradient,g˜AdamW. Secondly,wedeterminethegradientswithrespecttomatricesAandB
asfollows:
g˜A=sBTg˜AdamW, g˜B=sg˜AdamWAT. (46)
Thirdly,theweightdecayprocessmustbeadjusted. Inlinewithfullfine-tuning,theweightdecayisgiven
by:
W ←(1−γλ)(W +sBA). (47)
0
14Thiscanbedecomposedinto:
(cid:112) (cid:112)
W ←(1−γλ)W , B← 1−γλB, A← 1−γλA (48)
0 0
Algorithm1LoRA-ProwithSGDoptimizer
Require: Giveninitiallearningrateγ,scalingfactors.
1: Initializetimestept←0,low-rankmatricesA 0∈Rr×n andB 0∈Rm×r
2: repeat
3:
t←t+1
4: g lA ora,g lB ora←SelectBatch(A t−1,B t−1) ▷Selectbatchandreturnthecorrespondinggradients
5: A,B←A t−1,B t−1 ▷Obtainthelow-rankmatricesAandB
6:
X←SolveSylvester(BTBX+XAAT =−1(BTB)−1gA AT)▷ComputeXbysolvingthesylvesterequation
s2 lora
7: gA= 1(BTB)−1gA +XA ▷AdjustthegradientsofLoRAwithTheorem3.1
s2 lora
8:
gB= 1[I−B(BTB)−1BT]gB (AAT)−1−BX
s2 lora
9: A t ←A t−1−γgA
10: B t ←B t−1−γgB
11: untilstoppingcriterionismet
12: return optimizedparametersA t andB t
15Algorithm2LoRA-ProwithAdamWoptimizer
Require: Giveninitiallearningrateγ,scalingfactors,originalweightmatrixW
∈Rm×n,andβ
=0.9,β =
0 1 2
0.999,ϵ=10−8,λ∈R
1: Initialize time step t ← 0, low-rank matrices A 0 ∈ Rr×n and B 0 ∈ Rm×r, first momentum m 0 ∈ Rm×n,
secondmomentumv
∈Rm×n
t
2: repeat
3:
t←t+1
4: g lA ora,g lB ora←SelectBatch(A t−1,B t−1) ▷Selectbatchandreturnthecorrespondinggradients
5: A,B←A t−1,B t−1 ▷Obtainthelow-rankmatricesAandB
6:
X←SolveSylvester(BTBX+XAAT =−1(BTB)−1gA AT)▷ComputeXbysolvingthesylvesterequation
s2 lora
7: gA= 1(BTB)−1gA +XA ▷AdjustthegradientsofLoRAwithTheorem3.1
s2 lora
8:
gB= 1[I−B(BTB)−1BT]gB (AAT)−1−BX
s2 lora
9:
g˜←sgBA+sBgA ▷Computeequivalentgradient
10: m t ←β 1m t−1+(1−β 1)g˜
11: v t ←β 2v t−1+(1−β 2)g˜2
12: mˆ t ← 1m −βt t
1
13: vˆ t ← 1−v βt t
2
14:
g˜AdamW ← √mˆt
vˆt+ϵ
15:
g˜A ←sBTg˜AdamW
lora
16:
g˜B ←sg˜AdamWAT
lora
17:
X←SolveSylvester(BTBX+XAAT =−1(BTB)−1g˜A AT)
s2 lora
18: g˜A= 1(BTB)−1g˜A +XA ▷AdjustthegradientsofLoRAwithTheorem3.1
s2 lora
19:
g˜B= 1[I−B(BTB)−1BT]g˜B (AAT)−1−BX
(cid:112)s2 lora
20:
A← 1−γλA ▷WeightDecay
(cid:112)
21:
B← 1−γλB
22: W 0←(1−γλ)W 0
23: A t ←A t−1−γg˜A
24: B t ←B t−1−γg˜B
25: untilstoppingcriterionismet
26: return optimizedparametersA t andB t
16