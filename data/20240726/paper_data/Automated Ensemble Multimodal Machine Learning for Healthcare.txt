Automated Ensemble Multimodal Machine
Learning for Healthcare
Fergus Imrie1*†, Stefan Denner2,3†, Lucas S. Brunschwig4, Klaus
Maier-Hein2,5,6 and Mihaela van der Schaar7
1DepartmentofStatistics,UniversityofOxford,UnitedKingdom.
2DivisionofMedicalImageComputing,GermanCancerResearchCenter
(DKFZ),Germany.
3MedicalFacultyHeidelberg,HeidelbergUniversity,Germany.
4E´colePolytechniqueF´ed´eraledeLausanne,Switzerland.
5PatternAnalysisandLearningGroup,DepartmentofRadiationOncology,
HeidelbergUniversityHospital,Germany.
6NationalCenterforTumorDiseases(NCT)Heidelberg,Germany.
7DepartmentofAppliedMathematicsandTheoreticalPhysics,Universityof
Cambridge,UnitedKingdom.
*Correspondingauthor(s).E-mail(s):fergus.imrie@stats.ox.ac.uk;
†Theseauthorscontributedequallytothiswork.
Abstract
The application of machine learning in medicine and healthcare has led
tothecreationofnumerousdiagnosticandprognosticmodels.However,
despitetheirsuccess,currentapproachesgenerallyissuepredictionsusing
data from a single modality. This stands in stark contrast with clini-
cian decision-making which employs diverse information from multiple
sources. While several multimodal machine learning approaches exist,
significantchallengesindevelopingmultimodalsystemsremainthatare
hindering clinical adoption. In this paper, we introduce a multimodal
framework,AutoPrognosis-M,thatenablestheintegrationofstructured
clinical (tabular) data and medical imaging using automated machine
learning. AutoPrognosis-M incorporates 17 imaging models, including
convolutional neural networks and vision transformers, and three dis-
tinct multimodal fusion strategies. In an illustrative application using
a multimodal skin lesion dataset, we highlight the importance of mul-
timodal machine learning and the power of combining multiple fusion
strategiesusingensemblelearning.Wehaveopen-sourcedourframework
as a tool for the community and hope it will accelerate the uptake of
multimodalmachinelearninginhealthcareandspurfurtherinnovation.
Preprint
1
4202
luJ
52
]GL.sc[
1v72281.7042:viXra2 AutoPrognosis-Multimodal
Introduction
Medical and healthcare data is increasingly diverse in origin and nature,
encompassingpatientrecordsandimagingtogeneticinformationandreal-time
biometrics. Machine learning (ML) can learn complex relationships from data
to construct powerful predictive models. As a result, ML is increasingly being
proposed in medicine and healthcare, particularly for diagnostic and prog-
nostic modeling [1, 2]. However, such approaches typically make predictions
based on only one type of data [3] and thus cannot incorporate all available
information or consider the broader clinical context.
In contrast, clinicians make decisions based on the synthesis of informa-
tionfrommultiplesources,includingimaging,structuredclinicalorlaboratory
data,andclinicalnotes[4].Thiscanbecriticalforaccuratediagnosesandprog-
noses, and the absence of such information has been shown to result in lower
performance and decreased clinical utility in numerous studies [5, 6]. While
true across healthcare, this is perhaps particularly the case in medical imag-
ing. For example, almost 90% of radiologists reported that additional clinical
informationwasimportantandcouldchangediagnosescomparedtousingthe
imagingalone[7].Numerousotherexamplesoftheimportanceofclinicalcon-
text for medical image analysis exist across specialties such as ophthalmology
[8], pathology [9], and dermatology [10].
Multimodalmachinelearningintegratesmultipletypesandsourcesofdata,
offering a more holistic approach to model development that mirrors clinical
decision-making processes. As a result, while multimodal ML remains in its
infancy,modelsthatincorporatemultipledatamodalitieshavebeendeveloped
forseveralmedicaldomainsincludingcardiology[11],dermatology[12],oncol-
ogy [13, 14], and radiology [15]. However, technical challenges in developing,
understanding, and deploying multimodal ML systems are currently prevent-
ing broad adoption in medicine beyond bespoke individual examples. Thus,
techniques that reduce these obstacles could have substantial benefits across
numerous applications and clinical use cases.
In this paper, we propose a general-purpose approach that addresses these
challenges using automated machine learning (AutoML) and ensemble learn-
ing. AutoML can help design powerful ML pipelines by determining the most
appropriate modeling and hyperparameter choices, while requiring minimal
technical expertise from the user. To bridge the gap between clinicians and
cutting-edge ML, we previously proposed an AutoML approach, AutoProgno-
sis[16],forconstructingensembleML-baseddiagnosticandprognosticmodels
using structured data. AutoPrognosis has been used to develop clinical mod-
els for a number of outcomes, including cardiovascular disease [17, 18], cystic
fibrosis[19],breastcancer[20],andlungcancer[21].WhileAutoPrognosishas
been shown to yield promising results across a range of medical areas, it is
constrained to only handling tabular features. Several other frameworks for
automated pipeline optimization, such as Auto-sklearn [22], Auto-Weka [23],
and TPOT [24], suffer from the same limitation.AutoPrognosis-Multimodal 3
(a) How predictive is each modality? (c) When is additional information required?
Modality 1 Unimodal Model Predictions Modality 1 Predictions
Multimodal
Model
Modality n Unimodal Model Predictions Modality n Uncertainty
(b) What is the value of a new modality? (d) How do modalities interact?
Modality 1
Multimodal
Model Predictions
Modality n-1
Modality 1
Multimodal
Explanations Model
Modality 1 Modality n
Multimodal
Modality n-1 Model Predictions
Modality n
(e) What is the most predictive multimodal model?
Multimodal
Modality 1 Approach 1
Predictions
Modality n Multimodal
Approach n
Fig. 1: Overview of the types of questions that can be asked with
multimodal machine learning. In addition to developing powerful mul-
timodal models (e), multimodal ML can help understand the value of each
modality (a), the impact of adding a new modality (b), when an additional
modalityisrequired(c),andhowtheinformationindifferentmodalitiesinter-
acts (d).
Consequently, in this work, we developed AutoPrognosis-Multimodal
(AutoPrognosis-M),anAutoMLframeworkthatincorporatesdatafrommulti-
plemodalities,namelyimagingandtabulardata.Additionally,AutoPrognosis-
M enables such models to be interrogated with explainable AI and provides
uncertainty estimates using conformal prediction, aiding understanding and
helping build model trust [25].
Weappliedourapproachinanillustrativeclinicalscenario:skinlesiondiag-
nosis using both images and clinical features. Our experiments demonstrate
the benefit of incorporating information from multiple modalities and high-
light the impact of the multimodal learning strategy on model performance.
We show different strategies can be effectively combined to form ensemble
models that substantially outperform any individual approach. Additionally,
we quantify the value of information from each modality and show how our
framework can be used to determine whether additional data is necessary on
an individual patient basis.
While our experiments focus on skin lesion diagnosis, we emphasize that
AutoPrognosis-Misageneral-purposeapproachthatcanbeusedtotrainmul-
timodal diagnostic and prognostic models for any disease or clinical outcome,
without requiring substantial ML expertise, and can help answer a range of
clinical questions (Fig. 1). We have open-sourced AutoPrognosis-M as a tool
for the community to aid the clinical adoption of multimodal ML models.
…
…
…
+
… …
…
…
+4 AutoPrognosis-Multimodal
AutoPrognosis-Multimodal
Convolutional Neural Nets Vision Transformers
Transformer
1 2 3 4
Projection
Explanations
Image Modeling
Fusion Strategies
Medical Images Late Fusion
Multimodal
Early Fusion Ensemble Predictions
Tabular Data Joint Fusion 71%
Tabular Pipelines
Uncertainty
Imputation Preprocessing Tabular Models
Fig. 2: Overview of AutoPrognosis-M. AutoPrognosis-M leverages auto-
mated machine learning to produce multimodal ensembles by optimizing
state-of-the-art image and tabular modeling approaches across three fusion
strategies. AutoPrognosis-M also enables such models to be interrogated with
explainableAIandprovidesuncertaintyestimatesusingconformalprediction.
Methods: AutoPrognosis-M
AutoPrognosis-M enables clinicians and other users to develop diagnostic and
prognostic models for a diverse range of applications using state-of-the-art
multimodalML(Fig.2).Perhapsthemostsignificantchallengeisthecomplex
design space of model architectures and associated hyperparameters, which
mustbesetappropriatelyforthespecifictaskanddatabeingconsidered.Fail-
uretodosocansignificantlydegradeperformance;however,thisoftenrequires
significant ML knowledge and expertise to do so effectively. This challenge is
further compounded in the multimodal setting by the different possible ways
of integrating data from multiple sources.
To address this, our framework employs AutoML [23] to efficiently and
effectivelysearchthemodelandhyperparameterspace.AutoPrognosis-Mcon-
structs powerful ensemble multimodal ML models designed for the specific
problem and data under consideration. While incorporating multiple modal-
ities can improve predictive power, this may not always be the case or a
modality might be particularly expensive to acquire. Thus, it is important to
understandtheimportanceofeachmodalityindividually,andtheaddedvalue
of including an additional modality (Fig. 1a-b). AutoPrognosis-M can opti-
mizebothunimodalmodelsandmultimodalmodels,allowingthevalueofeach
modalitytobeassessed,bothseparatelyandincombinationwithothersources
of information, as well as enabling models to be debugged and understood
using techniques from explainable AI (Fig. 2).AutoPrognosis-Multimodal 5
ByautomatingtheoptimizationofMLpipelinesacrossmultiplemodalities
anddeterminingthemostsuitablewayofcombiningtheinformationfromdis-
tinctsources,wereducethebarrierfornon-MLexperts,suchascliniciansand
healthcareprofessionals,tobuildpowerfulmultimodalMLmodelsforproblems
in the healthcare and medicine domains. We believe that AutoPrognosis-M
significantly simplifies the process of training and validating multimodal ML
models without compromising on the expressiveness or quality of the ML
models considered.
Automated Machine Learning
AutoML aims to simplify and automate the process of designing and train-
ingMLmodels,therebyreducingthetechnicalcapabilitiesrequiredtodevelop
effectivemodels.Humanpractitionershavebiasesaboutwhatmodelarchitec-
turesorhyperparameterswillprovidethebestresultsforaspecifictask.While
thismightbehelpfulinsomecases,oftenitwillnotandcancauseinconsistency
inthequalityofthefinalpredictivesystem[26].AutoMLhelpsminimizethese
biases by automatically searching over a more general set of models, hyper-
parameters, and other design choices to optimize a given objective function,
returning the best configurations found. Beyond simply minimizing human
biases, AutoML reduces the demand for human experts and has been shown
to typically match or exceed the skilled human performance [27].
Unimodal approaches
Tabular
We implement the tabular component of our multimodal framework using
AutoPrognosis2.0[16].Incontrasttomanyotherapproachesforlearningfrom
tabulardata,weconsiderfullMLpipelines,ratherthanjustpredictivemodels,
consistingofmissingdataimputation,featureprocessing,modelselection,and
hyperparameter optimization. AutoPrognosis includes implementations for 22
classificationalgorithms,nine imputationalgorithms,includingmeanimputa-
tion, MICE [28], and MissForest [29], five dimensionality reduction, such as
PCA, and six feature scaling algorithms. The best-performing pipelines are
then combined into an ensemble via either a learned weighting or stacking,
where a meta-model is trained on the output of the underlying pipelines. For
further detail, we refer the reader to Imrie et al. [16]
Imaging
Forimagingtasks,weemployaseveraldistinctmodelarchitecturestocaterto
awiderangeofdiagnosticandprognosticapplications.Specifically,weutilized
Convolutional Neural Networks (CNNs), including ResNet [30], EfficientNet
[31], and MobileNetV2 [32], as well as Vision Transformers (ViTs) [33]. Each
model architecture is available in several sizes to be able to handle different6 AutoPrognosis-Multimodal
Medical Image Tabular Data Medical Image Tabular Data Medical Image Tabular Data
- - - - - - - - -
- - - - - - - - -
- - - - - - - - -
- - - - - - - - -
- - - - - - - - -
Unimodal Unimodal Feature Feature Feature Feature
Model Model Extractor Extractor Extractor Extractor
Loss Loss
Prediction Prediction Extracted Features Extracted Features Extracted Features Extracted Features
Combine Combine
Loss Loss
Combine Predictor Predictor
Loss Loss Loss
Final Final Final
Prediction Prediction Prediction
(a) Late Fusion (b) Early Fusion (c) Joint Fusion
Fig. 3: Illustration of the three types of multimodal fusion. (a) Late
fusioncombinesthepredictionsofseparateunimodalmodels.(b)Earlyfusion
trains a predictive model on the combination of fixed extracted features. (c)
Joint fusion flexibly integrates multiple modalities, learning to extract repre-
sentations and make predictions simultaneously in an end-to-end manner.
taskcomplexities.Afulllistofthe17imagingmodelsprovided,togetherwith
additional details, can be found in Table S.1.
While general-purpose models for tabular data do not exist, the transfer-
ability of imaging models has been shown in a diverse range of disciplines
[34, 35], and can be particularly effective when there is limited available data
for a particular problem. Pretrained models are especially useful when the
availabledataforaspecifictaskisscarceorwhencomputationalresourcesare
limited. They allow one to leverage learned features and patterns from vast
datasets, potentially improving performance on related tasks.
While most models are pretrained in a supervised manner, self-supervised
pretraining has been shown to improve performance on many classification
tasks.Thus,inadditiontosupervisedViT[33],wealsoconsiderDINOv2[36].
Oneapproachtousingpretrainedmodelsfornewpredictiontasksistoextract
afixedrepresentationandtrainanewclassificationheadontheavailabletask-
specific data. However, often these representations are not well adapted for
the specific data under consideration, especially when transferring from the
natural image to the medical image domain. In this case, we can train these
models further by fine-tuning the entire model on the available data. Fine-
tuning is most important when the target task or domain is related but not
identical to the one on which the model was originally trained by adapting
the generalized capabilities of the pretrained model to specific needs without
the necessity of training a model from scratch. The optimal training strategy
dependsonthespecifictask,availabilityofdata,andcomputationalresources.
AutoPrognosis-M can be used to train vision models from scratch, use their
existing representations, or finetune on the available data.AutoPrognosis-Multimodal 7
Multimodal data integration
Multimodal ML seeks to train a model that effectively integrates multiple
types of data to enable more accurate predictions than can be obtained using
any single modality. Modalities can exhibit different relationships, including
redundancy,dependingontheinteractionsbetweentheinformationcontained
in each source [37], which can additionally vary in complexity. Thus a key
challenge is discerning the relation between modalities and learning how to
integrate modalities most effectively.
We can typically decompose multimodal architectures into two compo-
nents:modality-specificrepresentationsandajointprediction[37].Multimodal
learning strategies differ primarily in the nature of these components and
whethertheyarejointlyorseparatelylearned.Threemainmultimodalstrate-
giesexistandareincorporatedinAutoPrognosis-M:LateFusion,EarlyFusion,
and Joint Fusion (Fig. 3).
Late Fusion
Late fusion is an ensemble-based approach that combines predictions from
multiple unimodal models, and thus is sometimes referred to as decision-
level fusion [38] (Fig. 3a). Each modality is processed independently using a
modality-specific model before the predictions are combined. This allows the
user to find the best classifier for each modality independently and evaluate
whethereachmodalityhaspredictivepowerfortheoriginaltask.However,this
hasthedrawbackofnotpermittinginteractionsbetweenmodalitiesbeforethe
finaloutput,whichcouldresultinsuboptimalperformancewhentherelation-
shipbetweenmodalitiesiscrucialformakingaccuratepredictionsordecisions.
Onebenefitoflatefusionistheabilitytoincorporatenewmodalitiesbyadding
an additional unimodal model and retraining only the ensembling step. We
implement late fusion using a weighted combination of unimodal tabular and
imaging models.
Early fusion
Earlyfusionnecessitatesthetranslationofeachmodalityintoarepresentation
that can be combined using a fusion module, such as concatenation, into a
single, unified representation (Fig. 3b). The combined representation is then
used as input to train a separate predictive model.
Compared to late fusion, early fusion allows interactions between different
modalities to be captured by the predictive model. However, the represen-
tations are fixed and translating different data types into effective (latent)
representations to form a unified representation can be challenging, especially
whendealingwithheterogeneousdatasourceswithdifferingscales,dimensions,
or types of information.
For image data, a common strategy is to use an intermediate (or the last)
layer of a vision model that was either trained to solve the relevant prediction
task or a general-purpose imaging model. For tabular data, especially if the8 AutoPrognosis-Multimodal
numberoffeaturesisrelativelymodest,therepresentationstepcanbeskipped
andtheoriginalfeaturesaredirectlycombinedwiththelatentrepresentations
extracted from the other modalities. We used concatenation to combine the
imaging and tabular features and trained a fully connected neural network on
this representation.
Joint Fusion
The fixed, independent representations used in early fusion may not capture
relevant factors for the joint prediction task. Joint fusion [4] (or intermediate
fusion [39]) aims to improve these representations using joint optimization
to enable both cross-modal relationships and modality-specific features to be
learnedusingend-to-endtraining(Fig.3c).Theaddedflexibilityofjointfusion
can come at the cost of potentially overfitting, especially in the limited data
setting.
The most popular approaches for joint fusion use differentiable unimodal
modelstoproducelatentrepresentationsthatarecombinedviaconcatenation
and passed to a prediction head. The system is then trained in an end-to-
end manner, either from scratch or using pre-trained unimodal models. We
implement joint fusion similarly to early fusion, except we train end-to-end.
Fusion Ensembles
Onemajorchallengeisdeterminingwhichfusionapproachisbest.Further,no
individualfusionapproachmaybeuniversallybestforallpatients.Ensembling
hasrepeatedlybeenshowntoleadtoimprovedmodelperformance,evenwhen
multiple copies of the same model are trained, but can be particularly bene-
ficial when different approaches are combined due to the increased diversity
of predictions [40]. The three fusion approaches learn to combine the informa-
tion from multiple modalities in distinct ways. Thus combining the strengths
of these different strategies via an ensembling approach could improve both
the absolute performance and the robustness of the final model. We combine
thebest-performingunimodalandmultimodalfusionapproachesinaweighted
ensemble.AllensemblesweredeterminedusingBayesianoptimization[41].We
refer to this ensemble as AutoPrognosis-M in our experiments.
Explainability
Models must be thoroughly understood and debugged to validate the under-
lying logic of the model, engender model trust from both clinical users and
patients[25],andsatisfyregulatoryrequirementspriortoclinicaluse[42].This
is particularly true in the multimodal setting, where we wish to understand
whatinformationisbeingusedfromeachmodalityandforwhichpatientseach
modalityiscontributingtothemodeloutput.Consequently,AutoPrognosis-M
contains multiple classes of explainability techniques to enable ML mod-
els to be better understood. We have included feature-based interpretability
methods, such as SHAP [43] and Integrated Gradients [44], that allow us toAutoPrognosis-Multimodal 9
understandtheimportanceofindividualfeatures,aswellasanexample-based
interpretability method, SimplEx [45], that explains the model output for a
particular sample with examples of similar instances, similar to case-based
reasoning.
Uncertainty estimation
Quantifying the uncertainty of predictions is another critical component
in engendering model trust with both clinicians and patients, and can be
used both to protect against likely inaccurate predictions and inform clinical
decision-making [46, 47]. We adopted the conformal prediction framework for
uncertainty quantification. Conformal prediction produces statistically valid
prediction intervals or sets for any underlying predictor while making mini-
malassumptions[48].Weusedinductiveconformalprediction[49],whichuses
a calibration set to determine the width of prediction intervals or the neces-
sary size of the prediction sets, with local adaptivity to adjust the interval or
set to the specific example [50–52]. For our experiments, we used regularized
adaptive prediction sets [53].
Experiments
WedemonstratetheapplicationofAutoPrognosis-Mtomultimodalhealthcare
data with the example of skin lesion diagnosis. This task is inherently a mul-
timodal process: primary care physicians, dermatologists, or other clinicians
use multiple factors to determine a diagnosis. Visual inspection has formed a
crucial element in lesion diagnosis, for example the “ABCD” rule or the ELM
7-point checklist [54]. These approaches have been refined to include other
characteristics beyond the appearance of the lesion at a single point in time,
such as evolution [55]. Beyond visual examination, clinicians also consider
medical history and other factors, such as itching and bleeding [10].
Data
Experiments were conducted using PAD-UFES-20 [56]. The dataset contains
2,298 skin lesion images from 1,373 patients in Brazil. Images of lesions were
captured from smartphones and each image is associated with 21 tabular fea-
tures, including the patient’s age, the anatomical region where the lesion is
located,demographicinformation,andothercharacteristicsofthelegion,such
as whether it itched, bled, or had grown. An overview of the clinical features
can be found in Table S.2. Further details can be found in the publication
describing the dataset [56].
Skinlesionsareclassifiedasoneofsixdifferentdiagnoses,threeofwhichare
cancerous (Basal Cell Carcinoma, Squamous Cell Carcinoma, and Melanoma)
and three are non-cancerous (Actinic Keratosis, Melanocytic Nevus, and Seb-
orrheicKeratosis).Asiscommoninstudiesofskinlesions,therearesubstantial10 AutoPrognosis-Multimodal
Table 1: Unimodal skin legion classification performance. The best
result for each modality is in bold. The best non-ensemble approach for each
modality is underlined.
LesionCategorization(6-way) CancerDiagnosis(Binary)
Method Acc. Bal.Acc. AUROC F1 Acc. AUROC F1 MCC
Tabular
Log.Reg. 63.6% 63.3% 0.890 0.559 83.0% 0.904 0.814 0.657
RandomForest 65.2% 54.0% 0.865 0.535 83.0% 0.903 0.810 0.662
XGBoost 66.5% 54.4% 0.875 0.545 81.3% 0.885 0.797 0.623
CatBoost 64.3% 57.2% 0.877 0.545 83.4% 0.902 0.822 0.667
MLP 69.7% 52.8% 0.878 0.526 83.1% 0.902 0.819 0.663
AutoPrognosis 69.4% 61.9% 0.891 0.580 83.9% 0.909 0.825 0.676
Imaging
ResNet18 59.8% 57.8% 0.885 0.547 81.9% 0.897 0.808 0.637
ResNet34 57.4% 54.5% 0.873 0.517 80.3% 0.881 0.790 0.603
ResNet50 60.7% 60.0% 0.888 0.562 82.0% 0.888 0.811 0.637
ResNet101 60.3% 56.6% 0.886 0.543 81.6% 0.883 0.802 0.628
ResNet152 63.6% 59.6% 0.895 0.578 82.1% 0.892 0.810 0.640
EfficientNetB0 64.3% 60.8% 0.899 0.577 82.4% 0.900 0.807 0.645
EfficientNetB1 65.5% 63.7% 0.901 0.602 82.6% 0.899 0.811 0.648
EfficientNetB2 65.1% 59.7% 0.899 0.578 81.5% 0.888 0.801 0.628
EfficientNetB3 64.2% 62.6% 0.902 0.598 81.9% 0.898 0.805 0.635
EfficientNetB4 66.7% 62.1% 0.899 0.602 81.9% 0.897 0.807 0.635
EfficientNetB5 66.7% 62.8% 0.904 0.609 82.6% 0.903 0.810 0.649
MobileNetV2 58.4% 54.5% 0.868 0.512 79.6% 0.877 0.779 0.590
ViTBase 67.1% 65.0% 0.917 0.618 82.9% 0.913 0.816 0.657
ViTLarge 68.1% 65.2% 0.916 0.631 84.1% 0.916 0.831 0.682
DinoV2Small 68.1% 65.0% 0.913 0.630 84.2% 0.912 0.834 0.683
DinoV2Base 68.5% 65.9% 0.914 0.639 84.0% 0.913 0.833 0.679
DinoV2Large 69.0% 65.8% 0.916 0.640 84.4% 0.913 0.834 0.686
Imagingensemble 71.6% 69.4% 0.927 0.672 85.3% 0.927 0.845 0.706
differencesinthenumberoflesionswitheachdiagnosis,resultinginclassimbal-
ance(TableS.2).Aggregatingthediagnosesintocancerousandnon-cancerous
almost eliminates this imbalance (47% cancerous, 53% non-cancerous). To
demonstrate the suitability of our framework for balanced and imbalanced
classificationscenarios,weexploreboththetaskofpredictingthespecificdiag-
nosesandthebinarydeterminationofwhetheragivenlesioniscancerous.We
assessedlesioncategorizationusinaccuracy,balancedaccuracy,areaunderthe
receiver operating curve (AUROC), and macro F1 score, and assessed can-
cer diagnosis using accuracy, AUROC, F1 score and Matthew’s correlation
coefficient (MCC).
To account for the presence of multiple images for some patients and
the imbalance in incidence of different diagnoses, we conducted 5-fold cross-
validation with stratified sampling, ensuring all images from the same patient
were contained in the same fold. Additionally, we used 20% of the training
setofeachfoldtooptimizehyperparametersandensembleweights.13clinical
variables had substantial levels of missingness (c. 35%) and this missingness
was often strongly associated with diagnosis. Consequently, we retained only
features without this missingness. Some other features had entries recordedAutoPrognosis-Multimodal 11
as “unknown,” corresponding to the patient being asked the question but not
knowing the answer. This was the case for six features, with an occurrence
between0.1%and17%.Thisresultedineighttabularvariablesbeingretained.
Categorical variables were one-hot encoded, yielding 27 clinical features and
one image for each sample.
How predictive is each modality in isolation?
Collectingadditionalinformationisneverwithoutexpense.Thiscanbefinan-
cial, time, or even adverse effects of collecting the information. Thus it is
critical to understand whether a modality is necessary and brings additional
predictive power. Therefore, we first used AutoPrognosis-M to optimize ML
pipelinesforeachmodalityseparatelytoquantifythevalueofimagingandclin-
ical features individually. Both the clinical variables and lesion images exhibit
some predictive power for lesion categorization and cancer diagnosis (Table
1), with the individual imaging models outperforming the tabular models for
lesion categorization while achieving similar results for cancer diagnosis.
Tabular. We tested several classification models in isolation, as well as
the performance of AutoPrognosis-M on just the tabular modality, which is
equivalent to AutoPrognosis [16]. Overall, AutoPrognosis outperformed any
individual tabular model, in particular for cancer diagnosis, demonstrating
the importance of AutoML and ensembling (Table 1). However, the relative
outperformanceoverlogisticregressionforlesioncategorizationandCatBoost
for cancer diagnosis was relatively minor, perhaps reflecting the nature of the
structured information available.
Imaging. The different imaging architectures displayed significant vari-
ability in performance across the lesion categorization and cancer diagnosis
prediction tasks, however several trends could be observed. The vision trans-
formerarchitectures(ViTandDINOv2)outperformedtheCNN-basedmodels
(ResNet, EfficientNet, MobileNet) almost universally across both tasks. One
explanationbeyondthemodelarchitecturecouldbethepre-trainingset,which
differed between the transformer and CNN models (see Table S.1). Increas-
ing the size of models typically led to improvements in performance for the
transformer models, although the largest models did not necessarily improve
performance (e.g. DINOv2Large vs. DINOv2Base), while the trend was less
clear for the CNN-based models. All model architectures consistently under-
performed when trained from initialization, thus all results shown are from
fine-tuning pre-trained models.
Ensembling the best-performing image models resulted in a substantial
increase in performance across all metrics for both prediction tasks. While
the transformers outperformed the CNNs individually, many of the ensembles
contained CNN-based approaches, demonstrating the importance of diversity
in ensemble learning.12 AutoPrognosis-Multimodal
Table 2: Skin legion classification performance. All multimodal
approachesoutperformtheunimodalbaselines.AutoPrognosis-Machievesthe
best results.
LesionCategorization(6-way) CancerDiagnosis(Binary)
Method Acc. Bal.Acc. AUROC F1 Acc. AUROC F1 MCC
Tabularensemble 69.4% 61.9% 0.891 0.580 83.9% 0.909 0.825 0.676
Bestimagemodel 68.5% 65.9% 0.914 0.639 84.4% 0.913 0.834 0.686
Imagingensemble 71.6% 69.4% 0.927 0.672 85.3% 0.927 0.845 0.706
Bestlatefusion 77.0% 72.8% 0.935 0.706 89.2% 0.950 0.883 0.782
Latefusionensemble 79.0% 74.7% 0.939 0.730 89.1% 0.954 0.882 0.781
Bestearlyfusion 70.9% 68.1% 0.918 0.657 85.7% 0.922 0.847 0.713
Earlyfusionensemble 74.7% 70.2% 0.930 0.701 86.7% 0.936 0.856 0.731
Bestjointfusion 73.8% 71.4% 0.930 0.698 87.5% 0.940 0.866 0.748
Jointfusionensemble 75.6% 71.9% 0.937 0.716 88.8% 0.951 0.880 0.775
AutoPrognosis-M 80.6% 75.8% 0.945 0.758 89.8% 0.956 0.889 0.794
What benefit does multimodal ML provide?
Now that we have shown that each modality has a potential predictive capa-
bility for skin lesion diagnoses, we sought to quantify what, if any, benefit
incorporating both modalities when issuing predictions provides. To do this,
we assessed each of the multimodal strategies included in AutoPrognosis-M.
Allthreefusionapproachesdemonstratesignificantimprovementsoverthe
unimodal classifiers, demonstrating the importance of integrating data from
multiple sources. In Table 2, we report the best single model for each fusion
strategy,togetherwiththeimpactofensemblingthebest-performingmodelsas
measuredontheheld-outportionofthetrainingset.Theimpactofcombining
the modalities varied across the various model architectures, with the results
also differing for each of the fusion strategies, late (Table S.3), early (Table
S.4), and joint (Table S.5).
Perhaps surprisingly, late fusion outperformed both early and joint fusion,
with early fusion the worst-performing fusion approach. This is likely a conse-
quenceoftherelativelystrongpredictivepowerofthetabularfeaturesandthe
numberofsamplesavailable,butcouldalsorevealthenatureoftherelationship
betweenthetwomodalities.Again,ensemblingthebest-performingmodelsfor
each fusion strategy provides a relatively small but consistent improvement,
except for the cancer diagnosis task for late fusion, where the best individual
model performed similarly to the ensemble.
AutoPrognosis-M leverages the power of each fusion strategy and the uni-
modal models by combining them in an ensemble. This approach performed
best across both tasks as measured by any metric, improving the performance
overanyonefusionapproachalone.Despitelatefusionoutperformingtheother
multimodal and unimodal approaches, it was not always selected as the most
important component in the ensemble and the largest weight assigned to any
ofthefivestrategies(twounimodal,threefusion)was39%,furtherreinforcing
the importance of diversity.AutoPrognosis-Multimodal 13
7%
16% Selective Acquisition Selective Acquisition
14% Multimodal 6% Multimodal
12% 5%
10% 4%
8%
3%
6%
2%
4%
1%
2%
0% Tabular only 0% Tabular only
0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100%
Proportion Selected Proportion Selected
(a) Lesion categorization (b) Cancer diagnosis
Fig. 4: Selective acquisition of images based on conformal prediction. By
acquiring images for around 20% of samples with the highest predicted uncer-
tainty based on the tabular features, we capture c. 55% and 65% of the
improvement of the multimodal classifier for (a) lesion categorization and (b)
cancerdiagnosis,respectively.Weapproachtheperformanceofthemultimodal
classifier by acquiring images for around half of all patients.
When are additional modalities most helpful?
While we have shown multimodal systems significantly outperform unimodal
approachesforlesioncategorizationandcancerdiagnosis,wemightnotrequire
all modalities for all patients. As mentioned previously, there might be down-
sides to collecting additional information, thus identifying when we would
benefitisbothimportantandoftenakeyclinicaldecision,sincemodalitiesare
typically acquired sequentially.
WedemonstratehowAutoPrognosis-Mcanbeusedtoanswerthisquestion.
We assumed access initially to the clinical features and wanted to identify for
whichpatientstoacquireanimageofthelesion.Weusedconformalprediction
to estimate the uncertainty of each prediction and chose to acquire images for
the patients with the highest uncertainty.
By acquiring images for around 20% of samples with the highest predicted
uncertainty based on the tabular features, we can capture around two-thirds
of the total improvement of the multimodal ensemble classifier for the cancer
diagnosis task (Fig. 4b) and over half for the lesion categorization task (Fig.
4a).Acquiringlesionimagesforaround50%ofpatientsresultsincomingclose
to matching the performance of the multimodal classifier, thereby halving the
number of images needed to be collected.
Understanding the information provided by each
modality
Understandingwhypredictionswereissuedisincrediblyimportantacrossmed-
ical contexts. We demonstrate how the interpretability techniques included in
.ccA
.laB
ni
niag
%
etulosbA
ycaruccA
ni
niag
%
etulosbA14 AutoPrognosis-Multimodal
Image Attributions - Image only Attributions - Joint fusion Tabular Attributions - Joint fusion
0.15
0.1
0.05
0
−0.05
−0.1
Label: BCC P r e d : { B SC EC K: : 0 0. .1 18 1, , A SC CK C: : 0 0. .0 00 0, , N ME EV L: : 0 0. .7 00 0, } P r e d : { B SC EC K: : 0 0. .5 05 0, , A SC CK C: : 0 0. .0 01 1, , N ME EV L: : 0 0. .4 03 0, } bleed_Th ruu ert_Trua egeitch_True elevatior ne _g Tio run e_Nch Oa Sn Egedg _re Fw al_ sT erue
Fig. 5: Comparison of explanations for unimodal and multimodal models
usingintegratedgradients.Theoriginalimage(left,img id:PAT 521 984 412)
together with attributions for the unimodal (center left) and joint fusion Effi-
cientNetB4 models (center right and right).
AutoPrognosis-M can be used to analyze the rationale for predictions across
multiple modalities. We used integrated gradients [44] to analyze the pre-
dictions from the image-only and joint fusion variants of EfficientNetB4. An
example is shown in Fig. 5.
Theimage-onlymodelincorrectlyclassifiesthelesionasMelanocyticNevus
(NEV, non-cancerous), while the joint fusion model correctly identifies the
lesionasBasalCellCarcinoma(BCC,cancerous).Theimageattributions(Fig.
5center)aresomewhatsimilar,bothplacingthemostimportanceonthelesion,
although there are minor differences in several areas. Importantly, the clinical
variables allow the multimodal approach to correct the image-only prediction.
NEV is typically asymptomatic [57] and more common in younger individu-
als [58]. The patient reported the lesion had bled, hurt, and itched, which the
multimodal model correctly identified made NEV less likely and increased the
chance of BCC, offset by the relatively young age of the patient (32) which
reduced the magnitude of the BCC prediction. This example clearly demon-
stratestheimportanceofincorporatingbothmodalitiesandtheunderstanding
that explainability methods can provide.
Discussion
Predictive modeling has the potential to support clinical decision-making and
improveoutcomes.However,incorporatingmultipletypesofdataintocompu-
tational approaches is not yet widespread in medicine and healthcare. In this
paper, we demonstrated the utility of AutoPrognosis-M for developing clin-
ical models from multimodal data using AutoML. Our framework simplifies
theapplicationofmultimodalfusionstrategies,automaticallydeterminingthe
best strategy for the available data and clinical application.
We have shown how AutoPrognosis-M can be used to perform unimodal
analysis for tabular and imaging data, enabling clinicians to understand when
multimodalapproacheswillprovidebenefit.Beyondprediction,weuseduncer-
tainty estimation to determine for which patients additional information is
necessary and explainability techniques to improve model understanding.AutoPrognosis-Multimodal 15
TheuseofAutoMLframeworkssuchasAutoPrognosis-Mcanaidinmodel
development, but does not alter the necessity to ensure models are suitably
validatedtoensuretheyexhibitthedesiredcharacteristics,suchasbeingaccu-
rate, reliable, and fair. As with any learning algorithm, significant care must
be taken by the user to ensure appropriate study design and data curation,
without which an inaccurate or biased model could be developed which could
have adverse effects on patient health.
While there have been bespoke multimodal ML systems developed, few
general-purpose frameworks exist. HAIM [59] is an early fusion approach
using user-defined pre-trained feature-extraction models to extract represen-
tations that are concatenated and passed to an XGBoost model. Wang et
al. proposed a multimodal approach for esophageal variceal bleeding predic-
tion [60]. They first trained an imaging model and then used automated
machine learning to develop a classifier based on structured clinical data and
theoutputoftheimagingmodel.Finally,AutoGluon-Multimodal[61]enables
fine-tuning of pretrained models across multiple modalities, combining their
outputs via late fusion. In contrast, AutoPrognosis-M incorporates multiple
fusion approaches, including both early and late fusion as possible strategies,
while our experiments highlight the limitations of only considering a single
fusion strategy.
While in this paper, we demonstrated the application of AutoPrognosis-M
totheproblemofdiagnosingskinlesionsusingsmartphoneimagesandclinical
variables,ourframeworkisgenerallyapplicableandcannaturallybeextended
to additional modalities and both new unimodal models and fusion strategies.
We believe AutoPrognosis-M represents a powerful tool for clinicians and ML
experts when working with data from multiple modalities and hope our aids
the adoption of multimodal ML methods in healthcare and medicine.
Code and data availability. AutoPrognosis-M is available at https:
//github.com/vanderschaarlab/AutoPrognosis-Multimodal. PAD-UFES-20 is
freely available at https://data.mendeley.com/datasets/zr7vgbcyr2/1 (DOI:
10.17632/zr7vgbcyr2.1) [56]. All preprocessing and the splits used for
our experiments can be found at https://github.com/vanderschaarlab/
AutoPrognosis-Multimodal.
References
[1] Abra`moff, M. D., Lavin, P. T., Birch, M., Shah, N. & Folk, J. C. Pivotal
trial of an autonomous AI-based diagnostic system for detection of diabetic
retinopathy in primary care offices. npj Digit. Med. 1 (1), 39 (2018). https:
//doi.org/10.1038/s41746-018-0040-6 .
[2] Rajpurkar,P.,Chen,E.,Banerjee,O.&Topol,E.J. AIinhealthandmedicine.
Nat. Med. 28 (1), 31–38 (2022). https://doi.org/10.1038/s41591-021-01614-0 .
[3] Kline, A. et al. Multimodal machine learning in precision health: A scop-
ing review. npj Digit. Med. 5 (1), 171 (2022). https://doi.org/10.1038/16 AutoPrognosis-Multimodal
s41746-022-00712-8 .
[4] Huang, S.-C., Pareek, A., Seyyedi, S., Banerjee, I. & Lungren, M. P. Fusion of
medicalimagingandelectronichealthrecordsusingdeeplearning:Asystematic
reviewandimplementationguidelines. npjDigit.Med.3(1),136(2020). https:
//doi.org/10.1038/s41746-020-00341-z .
[5] Leslie,A.&Jones,P.R.,A.J.andGoddard. Theinfluenceofclinicalinforma-
tiononthereportingofCTbyradiologists. Br. J. Radiol.73(874),1052–1055
(2000). https://doi.org/10.1259/bjr.73.874.11271897 .
[6] Castillo,C.,Steffens,T.,Sim,L.&Caffery,L. Theeffectofclinicalinformation
onradiologyreporting:Asystematicreview. J.Med.Radiat.Sci.68(1),60–74
(2021). https://doi.org/10.1002/jmrs.424 .
[7] Boonn, W. W. & Langlotz, C. P. Radiologist use of and perceived need for
patientdataaccess. J. Digit. Imaging 22(4),357–362(2009). https://doi.org/
10.1007/s10278-008-9115-2 .
[8] Wang, M. Y., Asanad, S., Asanad, K., Karanjia, R. & Sadun, A. A. Value of
medical history in ophthalmology: A study of diagnostic accuracy. J. Curr.
Ophthalmol. 30 (4), 359–364 (2018). https://doi.org/https://doi.org/10.1016/
j.joco.2018.09.001 .
[9] Ombrello, M. J., Sikora, K. A. & Kastner, D. L. Genetics, genomics, and their
relevancetopathologyandtherapy. Best Pract. Res.: Clin. Rheumatol.28(2),
175–189 (2014). https://doi.org/https://doi.org/10.1016/j.berh.2014.05.001 .
[10] Bergenmar, M., Hansson, J. & Brandberg, Y. Detection of nodular and super-
ficial spreading melanoma with tumour thickness ≤ 2.0 mm - an interview
study. Eur. J. Cancer Prev. 11 (1), 49–55 (2002). https://doi.org/10.1097/
00008469-200202000-00007 .
[11] Li, P., Hu, Y. & Liu, Z.-P. Prediction of cardiovascular diseases by integrating
multi-modal features with machine learning methods. Biomed. Signal Pro-
cess. Control. 66, 102474 (2021). https://doi.org/https://doi.org/10.1016/j.
bspc.2021.102474 .
[12] Liu, Y. et al. A deep learning system for differential diagnosis of skin dis-
eases. Nat. Med. 26 (6), 900–908 (2020). URL https://doi.org/10.1038/
s41591-020-0842-3. https://doi.org/10.1038/s41591-020-0842-3 .
[13] Yala, A., Lehman, C., Schuster, T., Portnoi, T. & Barzilay, R. A deep learn-
ing mammography-based model for improved breast cancer risk prediction.
Radiology 292 (1), 60–66 (2019). https://doi.org/10.1148/radiol.2019182716 .
[14] Kyono, T., Gilbert, F. J. & van der Schaar, M. Improving workflow efficiency
for mammography using machine learning. J. Am. Coll. Radiol. 17 (1), 56–63
(2020). https://doi.org/10.1016/j.jacr.2019.05.012 .AutoPrognosis-Multimodal 17
[15] Wu, J. et al. Radiological tumour classification across imaging modality and
histology. Nat. Mach. Intell. 3 (9), 787–798 (2021). https://doi.org/10.1038/
s42256-021-00377-0 .
[16] Imrie, F., Cebere, B., McKinney, E. F. & van der Schaar, M. AutoProgno-
sis 2.0: Democratizing diagnostic and prognostic modeling in healthcare with
automated machine learning. PLOS Digit. Health 2 (6), e0000276 (2023).
https://doi.org/10.1371/journal.pdig.0000276 .
[17] Alaa,A.M.,Bolton,T.,DiAngelantonio,E.,Rudd,J.H.F.&vanderSchaar,
M. Cardiovasculardiseaseriskpredictionusingautomatedmachinelearning:A
prospectivestudyof423,604UKBiobankparticipants. PLoS One 14(5),1–17
(2019). https://doi.org/10.1371/journal.pone.0213653 .
[18] Imrie, F., Rauba, P. & van der Schaar, M. Redefining digital health interfaces
with large language models. arXiv preprint arXiv:2310.03560 (2023) .
[19] Alaa, A. M. & van der Schaar, M. Prognostication and risk factors for cystic
fibrosis via automated machine learning. Sci. Rep. 8 (1), 11242 (2018). https:
//doi.org/10.1038/s41598-018-29523-2 .
[20] Alaa, A. M., Gurdasani, D., Harris, A. L., Rashbass, J. & van der Schaar,
M. Machine learning to guide the use of adjuvant therapies for breast can-
cer. Nat. Mach. Intell. 3 (8), 716–726 (2021). https://doi.org/10.1038/
s42256-021-00353-8 .
[21] Callender,T.etal. Assessingeligibilityforlungcancerscreeningusingparsimo-
nious ensemble machine learning models: A development and validation study.
PLOS Med. 20 (10), e1004287 (2023). https://doi.org/10.1371/journal.pmed.
1004287 .
[22] Feurer,M.etal. Efficientandrobustautomatedmachinelearning. Adv.Neural
Inf. Process. Syst. 28, 2755–2763 (2015) .
[23] Thornton,C.,Hutter,F.,Hoos,H.H.&Leyton-Brown,K. Auto-WEKA:Com-
bined selection and hyperparameter optimization of classification algorithms.
Proceedings of the 19th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining 847–855 (2013). https://doi.org/10.1145/2487575.
2487629 .
[24] Olson, R. S. & Moore, J. H. TPOT: A tree-based pipeline optimization tool
for automating machine learning. In: International Conference on Machine
Learning - AutoML Workshop 66–74 (2016) .
[25] Imrie, F., Davis, R. & van der Schaar, M. Multiple stakeholders drive diverse
interpretability requirements for machine learning in healthcare. Nat. Mach.
Intell. 5 (8), 824–829 (2023). https://doi.org/10.1038/s42256-023-00698-2 .
[26] Callender, T. & van der Schaar, M. Automated machine learning as a partner
in predictive modelling. Lancet Digit. Health 5 (5), e254–e256 (2023). https:
//doi.org/10.1016/S2589-7500(23)00054-7 .18 AutoPrognosis-Multimodal
[27] Waring, J., Lindvall, C. & Umeton, R. Automated machine learning: Review
ofthestate-of-the-artandopportunitiesforhealthcare. Artif.Intell.Med.104,
101822 (2020) .
[28] van Buuren, S. & Groothuis-Oudshoorn, K. mice: Multivariate imputation by
chained equations in R. J. Stat. Softw. 45 (3), 1–67 (2011). https://doi.org/
10.18637/jss.v045.i03 .
[29] Stekhoven, D. J. & Bu¨hlmann, P. MissForest—non-parametric missing value
imputationformixed-typedata. Bioinformatics 28(1),112–118(2011). https:
//doi.org/10.1093/bioinformatics/btr597 .
[30] He,K.,Zhang,X.,Ren,S.&Sun,J. Deepresiduallearningforimagerecogni-
tion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition 770–778 (2016) .
[31] Tan, M. & Le, Q. EfficientNet: Rethinking model scaling for convolutional
neuralnetworks. In: International Conference on Machine Learning 6105–6114
(2019) .
[32] Sandler,M.,Howard,A.,Zhu,M.,Zhmoginov,A.&Chen,L.-C. MobileNetV2:
Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition 4510–4520 (2018) .
[33] Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image
recognition at scale. In: International Conference on Learning Representations
(2021) .
[34] Weiss, K., Khoshgoftaar, T. M. & Wang, D. A survey of transfer learning. J.
Big Data 3, 1–40 (2016) .
[35] Zhuang, F. et al. A comprehensive survey on transfer learning. Proc. IEEE
109 (1), 43–76 (2020) .
[36] Oquab,M.etal. DINOv2:Learningrobustvisualfeatureswithoutsupervision.
Trans. Mach. Learn. Res. (2024) .
[37] Baltruˇsaitis, T., Ahuja, C. & Morency, L.-P. Multimodal machine learning: A
surveyandtaxonomy. IEEETrans.PatternAnal.Mach.Intell.41(2),423–443
(2019). https://doi.org/10.1109/TPAMI.2018.2798607 .
[38] Sharma, R., Pavlovic, V. I. & Huang, T. S. Toward multimodal human-
computer interface. Proc. IEEE 86 (5), 853–869 (1998) .
[39] Stahlschmidt,S.R.,Ulfenborg,B.&Synnergren,J. Multimodaldeeplearning
forbiomedicaldatafusion:Areview.Brief.Bioinform.23(2),bbab569(2022).
[40] Krogh,A.&Vedelsby,J.Neuralnetworkensembles,crossvalidation,andactive
learning. Adv. Neural Inf. Process. Syst. 7 (1994) .AutoPrognosis-Multimodal 19
[41] Akiba, T., Sano, S., Yanase, T., Ohta, T. & Koyama, M. Optuna: A next-
generation hyperparameter optimization framework. In: Proceedings of the
25th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining 2623–2631 (2019) .
[42] Medicines & Healthcare products Regulatory Agency. Soft-
ware and ai as a medical device change programme -
roadmap (2023). URL https://www.gov.uk/government/
publications/software-and-ai-as-a-medical-device-change-programme/
software-and-ai-as-a-medical-device-change-programme-roadmap. Updated
14 June 2023 .
[43] Lundberg, S. M. & Lee, S.-I. A unified approach to interpreting model
predictions. Adv. Neural Inf. Process. Syst. 30 (2017) .
[44] Sundararajan,M.,Taly,A.&Yan,Q. Axiomaticattributionfordeepnetworks.
In: International Conference on Machine Learning 3319–3328 (2017) .
[45] Crabbe, J., Qian, Z., Imrie, F. & van der Schaar, M. Explaining latent rep-
resentations with a corpus of examples. Adv. Neural Inf. Process. Syst. 34,
12154–12166 (2021) .
[46] Kompa, B., Snoek, J. & Beam, A. L. Second opinion needed: Communicating
uncertaintyinmedicalmachinelearning. npjDigit.Med.4(1),4(2021). https:
//doi.org/10.1038/s41746-020-00367-3 .
[47] Helou, M. A., DiazGranados, D., Ryan, M. S. & Cyrus, J. W. Uncertainty
in decision making in medicine: A scoping review and thematic analysis of
conceptual models. Acad. Med. 95 (1) (2020) .
[48] Vovk,V.,Gammerman,A.&Shafer,G.Algorithmiclearninginarandomworld
(Springer Science & Business Media, 2005).
[49] Vovk, V. Conditional validity of inductive conformal predictors. In: Asian
Conference on Machine Learning 475–490 (2012) .
[50] Papadopoulos,H.,Vovk,V.&Gammerman,A.Regressionconformalprediction
with nearest neighbours. J. Artif. Intell. Res. 40, 815–840 (2011) .
[51] Johansson,U.,S¨onstr¨od,C.&Linusson,H. Efficientconformalregressorsusing
baggedneuralnets.In:2015InternationalJointConferenceonNeuralNetworks
1–8 (2015) .
[52] Seedat, N., Jeffares, A., Imrie, F. & van der Schaar, M. Improving adaptive
conformalpredictionusingself-supervisedlearning. In:ProceedingsofThe26th
International Conference on Artificial Intelligence and Statistics (2023) .
[53] Angelopoulos, A., Bates, S., Malik, J. & Jordan, M. I. Uncertainty sets for
image classifiers using conformal prediction. In: International Conference on
Learning Representations (2021) .20 AutoPrognosis-Multimodal
[54] Argenziano,G.et al. Epiluminescencemicroscopyforthediagnosisofdoubtful
melanocytic skin lesions: Comparison of the ABCD rule of dermatoscopy and
a new 7-point checklist based on pattern analysis. Arch. Dermatol. 134 (12),
1563–1570 (1998). https://doi.org/10.1001/archderm.134.12.1563 .
[55] Abbasi, N. R. et al. Early diagnosis of cutaneous melanoma: Revisiting the
ABCD criteria. JAMA 292 (22), 2771–2776 (2004) .
[56] Pacheco,A.G.etal. PAD-UFES-20:Askinlesiondatasetcomposedofpatient
data and clinical images collected from smartphones. Data Brief 32, 106221
(2020). https://doi.org/10.1016/j.dib.2020.106221 .
[57] Macneal,P.&Patel,B.C. Congenitalmelanocyticnevi. StatPearlsPublishing,
Treasure Island (FL) (2020) .
[58] Zalaudek, I. et al. Frequency of dermoscopic nevus subtypes by age and body
site: A cross-sectional study. Arch. Dermatol. 147 (6), 663–670 (2011). https:
//doi.org/10.1001/archdermatol.2011.149 .
[59] Soenksen, L. R. et al. Integrated multimodal artificial intelligence framework
for healthcare applications. npj Digit. Med. 5 (1), 149 (2022). https://doi.org/
10.1038/s41746-022-00689-4 .
[60] Wang,Y.etal.Automatedmultimodalmachinelearningforesophagealvariceal
bleedingpredictionbasedonendoscopyandstructureddata. J. Digit. Imaging
36 (1), 326–338 (2023). https://doi.org/10.1007/s10278-022-00724-6 .
[61] Tang, Z. et al. AutoGluon-Multimodal (AutoMM): Supercharging multimodal
AutoML with foundation models. In: International Conference on Automated
Machine Learning (2024) .AutoPrognosis-Multimodal 21
Table S.1: Imaging models included in AutoPrognosis-M. CNN -
Convolutional Neural Network. ViT - Vision Transformer.
Pretraining Embedding
Model Type # Param. Ref.
Data size
ResNet18 CNN 11.7M ImageNet-1k 512 [30]
ResNet34 CNN 21.8M ImageNet-1k 512 [30]
ResNet50 CNN 25M ImageNet-1k 2048 [30]
ResNet101 CNN 44.5M ImageNet-1k 2048 [30]
ResNet152 CNN 60.2M ImageNet-1k 2048 [30]
EfficientNetB0 CNN 5.3M ImageNet-1k 320 [31]
EfficientNetB1 CNN 7.8M ImageNet-1k 320 [31]
EfficientNetB2 CNN 9.2M ImageNet-1k 352 [31]
EfficientNetB3 CNN 12M ImageNet-1k 384 [31]
EfficientNetB4 CNN 19M ImageNet-1k 448 [31]
EfficientNetB5 CNN 30M ImageNet-1k 512 [31]
MobileNetV2 CNN 3.4M ImageNet-1k 320 [32]
ViTBase ViT-B/16 86M ImageNet-1k 768 [33]
ViTLarge ViT-L/16 307M ImageNet-21k 1024 [33]
DinoV2Small ViT-S/14 22M LVD-142M 384 [36]
DinoV2Base ViT-B/14 86M LVD-142M 768 [36]
DinoV2Large ViT-L/14 307M LVD-142M 1024 [36]22 AutoPrognosis-Multimodal
Table S.2: Clinical variables in the PAD-UFES-20 dataset (n=2,298).
Diagnosis
BasalCellCarcinoma(BCC) 845(36.8%)
SquamousCellCarcinoma(SCC) 192(8.4%)
Melanoma(MEL) 52(2.3%)
ActinicKeratosis(ACK) 730(31.8%)
MelanocyticNevus(NEV) 244(10.6%)
SeborrheicKeratosis(SEK) 235(10.2%)
Age
6-29 92(4.0%)
30-49 386(16.8%)
50-69 1,098(47.8%)
70-94 722(31.4%)
Region
Face 570(24.5%)
Forearm 392(17.1%)
Chest 280(12.2%)
Back 248(10.8%)
Arm 192(8.4%)
Nose 158(6.9%)
Hand 126(5.5%)
Neck 93(4.0%)
Thigh 73(3.2%)
Ear 73(3.2%)
Abdomen 36(1.6%)
Lip 23(1.0%)
Scalp 18(0.8%)
Foot 16(0.7%)
Itch
Yes 1,455(63.3%)
No 837(36.4%)
Unknown 6(0.3%)
Grew
Yes 925(40.2%)
No 971(42.3%)
Unknown 402(17.5%)
Hurt
Yes 397(17.3%)
No 1,891(82.3%)
Unknown 10(0.4%)
Changed
Yes 202(0.9%)
No 1,700(74.0%)
Unknown 396(17.2%)
Bleed
Yes 614(26.7%)
No 1,678(73.0%)
Unknown 6(0.3%)
Elevation
Yes 1,433(62.4%)
No 863(37.6%)
Unknown 2(0.1%)AutoPrognosis-Multimodal 23
Table S.3:Late fusionskin legion classification performance.Thebest
result for each modality is in bold. The best non-ensemble approach for each
modality is underlined.
LesionCategorization(6-way) CancerDiagnosis(Binary)
Method Acc. Bal.Acc. AUROC F1 Acc. AUROC F1 MCC
ResNet18 74.0% 68.0% 0.921 0.656 87.8% 0.946 0.867 0.755
ResNet34 73.2% 67.1% 0.920 0.638 87.7% 0.946 0.867 0.753
ResNet50 73.6% 69.8% 0.925 0.674 87.8% 0.943 0.867 0.754
ResNet101 73.6% 68.4% 0.926 0.665 86.8% 0.942 0.855 0.734
ResNet152 75.5% 70.2% 0.928 0.684 88.1% 0.944 0.870 0.760
EfficientNetB0 75.2% 69.3% 0.925 0.665 87.9% 0.947 0.867 0.756
EfficientNetB1 75.5% 71.3% 0.929 0.693 88.1% 0.949 0.870 0.762
EfficientNetB2 75.5% 69.4% 0.927 0.668 87.6% 0.944 0.864 0.751
EfficientNetB3 74.7% 71.0% 0.928 0.685 88.1% 0.946 0.869 0.760
EfficientNetB4 75.4% 69.9% 0.928 0.672 88.0% 0.946 0.868 0.758
EfficientNetB5 76.9% 72.0% 0.928 0.696 87.9% 0.948 0.867 0.757
MobileNetV2 72.2% 65.6% 0.912 0.627 87.1% 0.939 0.857 0.739
ViTBase 76.6% 71.6% 0.937 0.700 87.3% 0.948 0.861 0.744
ViTLarge 76.3% 71.2% 0.936 0.697 88.0% 0.950 0.869 0.759
DinoV2Small 76.8% 70.9% 0.937 0.695 89.2% 0.950 0.883 0.782
DinoV2Base 77.0% 72.8% 0.935 0.706 88.5% 0.951 0.875 0.767
DinoV2Large 77.7% 72.3% 0.935 0.697 87.5% 0.948 0.864 0.749
Ensemble 79.0% 74.7% 0.939 0.730 89.1% 0.954 0.882 0.781
Table S.4: Early fusion skin legion classification performance. The
best result for each modality is in bold. The best non-ensemble approach for
each modality is underlined.
LesionCategorization(6-way) CancerDiagnosis(Binary)
Method Acc. Bal.Acc. AUROC F1 Acc. AUROC F1 MCC
ResNet18 59.9% 59.2% 0.879 0.558 81.8% 0.899 0.803 0.635
ResNet34 58.1% 56.3% 0.865 0.541 80.9% 0.884 0.795 0.614
ResNet50 62.8% 61.7% 0.891 0.590 84.1% 0.908 0.832 0.679
ResNet101 66.8% 63.4% 0.904 0.617 84.4% 0.921 0.833 0.685
ResNet152 69.1% 64.6% 0.910 0.636 83.3% 0.904 0.821 0.663
EfficientNetB0 66.0% 61.2% 0.884 0.601 83.4% 0.903 0.822 0.665
EfficientNetB1 64.3% 60.8% 0.884 0.586 82.8% 0.903 0.816 0.655
EfficientNetB2 63.8% 58.7% 0.884 0.577 80.3% 0.882 0.786 0.602
EfficientNetB3 64.1% 58.8% 0.882 0.569 81.6% 0.896 0.801 0.629
EfficientNetB4 66.4% 62.2% 0.894 0.607 82.9% 0.902 0.818 0.656
EfficientNetB5 66.3% 61.2% 0.893 0.603 82.0% 0.898 0.807 0.638
MobileNetV2 64.4% 59.4% 0.880 0.576 82.1% 0.898 0.808 0.641
ViTBase 58.1% 55.2% 0.820 0.517 80.7% 0.881 0.792 0.612
ViTLarge 70.1% 66.6% 0.915 0.653 84.1% 0.914 0.829 0.680
DinoV2Small 70.9% 68.1% 0.918 0.657 85.7% 0.922 0.847 0.713
DinoV2Base 69.1% 64.8% 0.904 0.635 83.5% 0.912 0.821 0.669
DinoV2Large 70.4% 62.8% 0.906 0.630 83.8% 0.915 0.829 0.674
Ensemble 74.7% 70.2% 0.930 0.701 86.7% 0.936 0.856 0.73124 AutoPrognosis-Multimodal
Table S.5: Joint fusion skin legion classification performance. The
best result for each modality is in bold. The best non-ensemble approach for
each modality is underlined.
LesionCategorization(6-way) CancerDiagnosis(Binary)
Method Acc. Bal.Acc. AUROC F1 Acc. AUROC F1 MCC
ResNet18 65.8% 61.6% 0.899 0.588 83.4% 0.911 0.823 0.667
ResNet34 62.8% 61.6% 0.889 0.575 82.1% 0.891 0.809 0.639
ResNet50 66.1% 60.1% 0.879 0.568 84.3% 0.909 0.833 0.684
ResNet101 67.5% 61.5% 0.892 0.595 83.9% 0.911 0.827 0.675
ResNet152 68.8% 66.3% 0.913 0.642 85.3% 0.921 0.842 0.704
EfficientNetB0 62.9% 64.0% 0.899 0.605 84.7% 0.921 0.833 0.691
EfficientNetB1 66.1% 65.1% 0.905 0.624 85.4% 0.926 0.843 0.705
EfficientNetB2 68.3% 63.9% 0.912 0.620 84.0% 0.914 0.828 0.677
EfficientNetB3 68.1% 65.3% 0.913 0.634 84.8% 0.920 0.837 0.693
EfficientNetB4 69.0% 66.2% 0.915 0.644 86.3% 0.933 0.853 0.724
EfficientNetB5 71.0% 66.2% 0.922 0.648 87.3% 0.936 0.863 0.744
MobileNetV2 61.9% 60.5% 0.894 0.572 83.3% 0.908 0.821 0.665
ViTBase 64.9% 63.6% 0.898 0.600 83.3% 0.909 0.822 0.665
ViTLarge 72.6% 68.4% 0.930 0.676 87.5% 0.940 0.866 0.748
DinoV2Small 69.6% 66.4% 0.915 0.651 86.2% 0.929 0.856 0.725
DinoV2Base 73.8% 71.4% 0.930 0.698 87.0% 0.935 0.861 0.740
DinoV2Large 73.8% 68.2% 0.933 0.678 86.6% 0.941 0.857 0.733
Ensemble 75.6% 71.9% 0.937 0.716 88.0% 0.951 0.880 0.775