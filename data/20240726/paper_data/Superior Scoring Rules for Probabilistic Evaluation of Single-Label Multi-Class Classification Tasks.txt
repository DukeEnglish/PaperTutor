SUPERIOR SCORING RULES FOR PROBABILISTIC EVALUATION
OF SINGLE-LABEL MULTI-CLASS CLASSIFICATION TASKS
RouhollahAhmadian
DepartmentofMathematicsandComputerScience
AmirkabirUniversityofTechnology(TehranPolytechnic)
Iran
rahmadian@aut.ac.ir
MehdiGhatee∗ JohanWahlstro¨m
DepartmentofMathematicsandComputerScience DepartmentofComputerScience
AmirkabirUniversityofTechnology(TehranPolytechnic) UniversityofExeter
Iran UK
ghatee@aut.ac.ir j.wahlstrom@exeter.ac.uk
July26,2024
ABSTRACT
ThisstudyintroducesnovelsuperiorscoringrulescalledPenalizedBrierScore(PBS)andPenalized
Logarithmic Loss (PLL) to improve model evaluation for probabilistic classification. Traditional
scoringruleslikeBrierScoreandLogarithmicLosssometimesassignbetterscorestomisclassifi-
cationsincomparisonwithcorrectclassifications. Thisdiscrepancyfromtheactualpreferencefor
rewarding correct classifications can lead to suboptimal model selection. By integrating penalties
for misclassifications, PBS and PLL modify traditional proper scoring rules to consistently assign
better scores to correct predictions. Formal proofs demonstrate that PBS and PLL satisfy strictly
proper scoring rule properties while also preferentially rewarding accurate classifications. Experi-
mentsshowcasethebenefitsofusingPBSandPLLformodelselection,modelcheckpointing,and
earlystopping. PBSexhibitsahighernegativecorrelationwiththeF1scorecomparedtotheBrier
Score during training. Thus, PBS more effectively identifies optimal checkpoints and early stop-
pingpoints,leadingtoimprovedF1scores. ComparativeanalysisverifiesmodelsselectedbyPBS
andPLLachievesuperiorF1scores. Therefore,PBSandPLLaddressthegapbetweenuncertainty
quantificationandaccuracymaximization byencapsulatingbothproperscoringprinciplesand ex-
plicit preference for true classifications. The proposed metrics can enhance model evaluation and
selectionforreliableprobabilisticclassification.
Keywords StrictlyProperScoringRules·EvaluationMetric·ProbabilisticEvaluation·ProbabilisticClassification·
ModelSelection
1 Introduction
Evaluationmetricsplayacriticalroleinmodelselection,featureselection,parametertuning,andregularizationwhen
evaluating the performance of a classification model [1]. In model selection, evaluation metrics such as accuracy,
precision,recall,andF-measuresareusedtocomparetheperformanceofdifferentmodelsandselectthebestmodel
for a specific task [2]. Similarly, in feature selection, evaluation metrics are used to identify the most informative
features for a specific task. By comparing the performance of a classification model with different feature subsets,
irrelevant or redundant features can be eliminated, improving the model’s efficiency and effectiveness [3]. Model
checkpointing,alsoknownassnapshotting,involvessavingthestateofamodelduringthetrainingprocessatregular
intervals. Byevaluatingtheperformanceofeachsnapshotonvalidationmetrics,thebest-performingsnapshotcanbe
4202
luJ
52
]GL.sc[
1v79671.7042:viXraAPREPRINT-JULY26,2024
selectedforthe finalmodel. This helpspreventoverfittingby choosing asnapshotthatexhibits goodgeneralization
abilityonthevalidationset,improvingthemodel’sabilitytogeneralizetounseendata[4,5,6,7]. Inregularization,
evaluationmetricsareusedtobalancethemodel’scomplexityanditsabilitytogeneralizetonewdata. Byevaluating
theperformanceofaclassificationmodelwithdifferentregularizationstrengths,overfittingcanbeprevented,ensuring
thatthemodelperformswellonunseendata[8,9].
In classification tasks, the evaluation of the model often relies on simple accuracy measures and related metrics de-
rivedfromtheconfusionmatrixthatonlyconsiderspredictionsspecifyingthetruthclass[10].However,thisapproach
overlooksthecrucialaspectofprobabilisticuncertaintyquantification[11].Ensuringtheaccuracyofpredictiveuncer-
taintyiscriticalforclassificationmodelsusedinsafety-criticalapplications. Tobeconsideredreliableandcalibrated
instatisticalterms,aclassificationmodelmustexhibitanhonestexpressionofitspredictivedistribution[12]. Inother
words, the model should not only make predictions but also convey the level of uncertainty associated with those
predictions[13]. Forexample,considerabinaryclassificationproblemwherethegoalispredictingacertaindisease.
Aclassifierthatoutputsasingleclasspredictionofdiseaseornodiseasemayachieveahighaccuracyrateinterms
of correct predictions. However, it fails to capture and communicate the inherent uncertainty surrounding each pre-
diction. Thisdiscrepancybetweentheuseofsimpleaccuracymeasuresinclassificationandtheneedforprobabilistic
assessmentshasmotivatedresearcherstoexploretheuseofscoringrules[14].
Scoringrulesareusedtoevaluateprobabilisticpredictionsorforecastsindecisiontheory[15]. Whenassessingprob-
abilistic forecasts, it is crucial to use a scoring method that accurately reflects the reported probabilities and their
alignment with actual outcomes [16]. One approach to achieving this is through the use of strictly proper scoring
rules. Theserulesassignoptimalscoresonlywhentheforecastsmatchthetrueprobabilities[17]. TheBrierScore,
initially introduced in meteorology, is an early example of such a rule [18]. It was developed to discourage biased
reportingandpromotehonestassessmentsofuncertainty. Asavariantofthequadraticscoringrule,itremainswidely
utilized [16]. Another pioneering effort resulted in the logarithmic rule, which is notable for its theoretical connec-
tion to entropy [19]. Over time, numerous studies have examined commonly used scoring rules and their desirable
statisticalpropertiesfromdifferentperspectives. Someanalyseshavefocusedonaspectssuchasforecastcalibration
andconsistencyincentives,whileothershaveexploredconnectionstoinformationtheoryconcepts[11,20]. Asprob-
abilisticpredictioncontinuestobeasignificantareaofresearch, ongoinginvestigationsintoexistingruleproperties
and potential novel approaches will contribute to further understanding and advancement of scoring methodologies.
Inthisregard,thispaperproposesanovelattributeforstrictlyproperscoringrules.
1.1 Motivation
Theclassificationdecisioniscommonlypartitionedintocategoriesoftruepositive,truenegative,falsepositive, and
false negative. Let us examine an illustrative scenario involving a true vector of [0,1,0] and two predicted vectors:
A=[0.33,0.34,0.33]andB =[0.51,0.49,0].VectorAcorrespondstoatruenegativeprediction,whilevectorBrep-
resentsafalsenegativeprediction. Itiscrucialtocriticallyevaluatewhichvectordemonstratessuperiorperformance,
taking into account that the argmax of vector A accurately predicts the outcome, while vector B assigns a higher
probability to the correct class. Upon analyzing the performance of vectors A and B in a single-label scenario, it
becomesevidentthatvectorApossessesanadvantageovervectorBintermsofclassificationaccuracy. Thisattribute
holdssignificantimportanceasaccuratedecision-makingishighlydesirableandessentialinnumerousapplications.
Consequently,consideringtheparamountsignificanceofprecisedecisions,itcanbeconcludedthatvectorAsurpasses
vectorB inquality. Therefore,thispaperadvocatesthatthescoringruleoughttoconsistentlyassignbetterscoresto
decisionsabouttruepositivesandtruenegatives. However,ascientificdiscrepancyariseswhenconsideringtheBrier
ScoreandLogarithmicLossmetrics,astheyoccasionallycontradictthisadvocatedprinciple. Surprisingly,vectorA
exhibitshigherBrierScoreandLogarithmicLossvaluescomparedtovectorB contrarytoexpectations. Thisincon-
sistencybetweentheadvocatedprincipleandtheobservedoutcomeshighlightsascientificgaporinconsistencythat
thispaperaimstoaddressandresolve. Therefore,thispaperproposesanovelattributeforstrictlyproperscoringrules
totacklethisissue. Ourcontributionscanbestatedasthefollowing:
• It hypothesizes that evaluation metrics should favor accurate predictions (true positives and true negatives)
overmisclassifiedones(falsepositivesandfalsenegatives)whenevaluatingmulti-classsingle-labelclassifi-
cations.
• WeshowtraditionalstrictlyproperscoreslikeBrierScoreandLogarithmicLosscanrateincorrectpredictions
higherthancorrectones.
• A new property termed superior is defined for scoring rules to formally establish that evaluations should
preferencetruepredictions.
2APREPRINT-JULY26,2024
• Tothisend,thisresearchproposestwonovelscoringrulescalledPenalizedBrierScore(PBS)andPenalized
LogarithmicLoss(PLL).
• ThisresearchmodifiesThescoringrulesBrierScoreandLogarithmicLossbyincorporatingapenaltyterm
to make them superior. This penalizes incorrect predictions more strongly than the original scoring rules,
aligningwiththeinitialhypothesisthatfavorscorrectdecisionsoverwrongs.
• The utility of the proposed evaluation metrics for early stopping and checkpointing is experimentally as-
sessed.
• Experimentalresultsdemonstratethattheproposedevaluationmetricsenhancetheefficiencyoftheclassifier.
Furthermore,thesemetricsexhibitahighcorrelationwiththeF-1scorewhileencompassingthemeasurement
ofpredictionuncertaintyandcalibration.
The paper is structured in the following way: Section 2 provides background information and sets the stage for the
research. Section 3 reviews previous work related to the topic. Section 4 details the motivation for developing new
scoringrulesandintroducestheproposedPenalizedBrierScoreandPenalizedLogarithmicLossmethods. Section5
presentsexperimentalresultsobtainedfromspatio-temporaldata. Finally,section6concludesthepaper.
2 Preliminaries
2.1 ScoringRules
AscoringruleS(·,·)isabivariatefunctionthattakestwoarguments: aprobabilitymeasure,representingaforecast,
and an observed outcome [21]. The function returns a real number that measures the deviation between a forecast
probabilityandreality. Theyprovideasummarymeasurefortheevaluationofpredictionsthatassignprobabilitiesto
events,suchasprobabilisticclassificationofasetofmutuallyexclusiveoutcomesorclasses[22].Scoringrulescanbe
thoughtofascostfunctionorlossfunctionandareevaluatedastheempiricalmeanofagivensample,simplycalled
score[17]. Itisworthnotingthatinneuralnetworks,lossfunctionsaretypicallyutilizedduringthebackpropagation
processtooptimizethenetwork. However,inthisresearch,wearefocusingonevaluationmetricsforassessingmodel
performance.
LetΩrepresentageneralsamplespace,whichcontainsallpossibleoutcomes. LetArepresentaneventspace,which
is a σ-algebra of subsets of Ω. An event refers to a set of observations within the sample space. The extended real
numbersR = [−∞,∞]denotepossiblescores. LetF representaconvexclassofprobabilitymeasuresonthespace
(Ω,A). AprobabilisticforecastisanyprobabilitymeasureF ∈F.
Definition1(ScoringRule). AscoringruleS : F ×Ω → Risafunctionassigningnumericalvaluestoprobability
measurepairs(P,i),whereP ∈F asaprobabilisticforecastisissuedandi∈Ωasanobservationmaterializes.
Let the distributional forecast Q ∈ F represent the forecaster’s best estimate of the probability distribution. In the
contextofmulti-classclassificationtasks,Qrepresentsground-truthvectors. AscoringruleS(P,Q)isdefinedasthe
expectedscoreofS(P,i)wheni∼Q. ItmeansthatS(P,Q):=E [S(P,i)]=(cid:82) S(P,i)dQ(i).
Q
Definition2(StrictlyProperScoringRule). ThescoringruleisproperwithrespecttoF if
(cid:26)
S(P,Q)≥S(Q,Q) S isnegativelyoriented
S(P,Q)≤S(Q,Q) S ispositivelyoriented
anditisstrictlyproperifequalityholdsifandonlyifP =Q.
2.2 ProbabilisticEvaluationofMulti-ClassClassification
Insingle-labelmulti-classclassification, theobjectiveistopredictthespecificclasslabel, denotedasω, foragiven
samplecharacterizedbyafeaturevector, denotedasX. Inprobabilisticclassification, thegoalistolearntheentire
conditional distribution p(ω|X), which represents the probability distribution over classes conditioned on the given
features [23]. This is accomplished through the use of a probabilistic classifier denoted as f : X → F. The set F
of probability measures is typically identified with the probability c-simplex C = {p ∈
[0,1]c|(cid:80)c
p = 1} and
i=1 i
probabilitydistributionsarerepresentedbyvectorsp ∈ C. Therefore,theoverallformofthescoringrulesforsingle-
labelmulti-classclassificationproblemsisS(p,i),wherepisaprobabilityvectorobtainedfromtheclassifierandiis
thetrueclass.
3APREPRINT-JULY26,2024
Definition3. BrierScore(orsquaredlossorquadraticscore),denotedbyS (p,i),isthemostwell-knownstrictly
BS
properscoringruleandisdefinedas[18]
c
(cid:88)
S (p,i)= (p −y )2 (1)
BS j j
j=1
Wherecinthenumberofclassesandy istheground-truthlabelasavectory = (y ,··· ,y )suchthaty = 1ifthe
1 c i
trueclassisi,andy =0otherwise.
j
Definition 4. Logarithmic Loss (or Cross-entropy loss or log-loss or ignorance score), denoted by S (p,i), is a
LL
strictlyproperscoringrulesandisdefinedas[19]
c
(cid:88)
S (p,i)=− y log(p ) (2)
LL j j
j=1
Wherecinthenumberofclassesandy istheground-truthlabelasavectory = (y ,··· ,y )suchthaty = 1ifthe
1 c i
trueclassisi,andy =0otherwise.
j
Inpractice,competingforecastproceduresarerankedbytheaveragescore[17]. Supposethatwewishtofitapara-
metricmodelP basedonrandomsamplesX(1),··· ,X(n). Toestimateθ,wemightmeasurethegoodness-of-fitby
θ
themeanscore[24]
n
1 (cid:88)
S (θ)= S(P (X(i)),ω(i)) (3)
n n θ
i=1
whereSisastrictlyproperscoringrule.Ifθ denotesthetrueparameter,asymptoticargumentsindicatethatargmin
0 θ
S (θ) → θ asn → ∞[17]. Let’ssupposeS isanegativelyorientedscoringruleliketheBrierScore. Itsuggests
n 0
a general approach to estimation: choose a strictly proper scoring rule that is tailored to the problem at hand that
minimizesS (θ)overtheparametersspaceandtakeθˆ = argmin S (θ)astheoptimumscoreestimatorbasedon
n n θ n
thescoringrule[17]. Forpositivelyorientedscoringruleslikeaccuracy,θˆ =argmax S (θ).
n θ n
3 LiteratureReview
Evaluation metrics are crucial for assessing the performance of classification algorithms. They are used to evaluate
theperformanceandeffectivenessofclassifiers,aswellastodiscriminateandselecttheoptimalsolutionduringthe
classificationtrainingprocess[1]. Classificationevaluationmetricscangenerallybedividedintoaccuracy-basedand
confidence-basedmetrics.
Accuracy-basedmetricsonlyconsiderwhetherapredictionmatchesthetruelabel,withoutregardtopredictiveconfi-
denceoruncertainty. Thiscategoryincludesmetricslikeaccuracy,errorrate,precision,recall,F-measures,AUC,and
relatedthreshold-basedmeasures. Accuracyanderrorrateareamongthemostwidespreadevaluationmetrics. How-
ever,theyhavelimitationssuchasprovidinglessdistinctivevaluesandfavoringthemajorityclass[25,10]. Precision
and recall focus on different targets, making them unsuitable for selecting the optimal solution [1]. F-measures and
AUCaimtoaddresssomeoftheshortcomingsofindividualmetrics. F-measuresconsiderbothprecisionandrecall,
whileAUCreflectstheoverallrankingperformanceofaclassifier[25]. However,AUChasahighcomputationalcost
forlargedatasets[1].
Confidence-based metrics assess predictive confidence by rewarding calibrated probability estimates [26]. These
metrics reward probabilistic calibration and include strictly proper scoring rules. In addition to the scoring rules
introducedinthepreliminaries,thereareseveralotherscoringrulesthathavebeenutilizedinforecastverification[27].
Someexamplesincludethesphericalscore,quadraticscore,pseudosphericalscore,andcontinuousrankedprobability
score (CRPS) [28, 29]. Variations of these rules have been used in different fields. This includes electricity price
forecasting to assess volatility over time, and wind/weather modeling to evaluate probabilistic predictions [30, 31].
Financialpredictionandspatialstatisticsalsoapplyscoringrules[29].
A scoring rule is considered proper if the expected score is minimized by the true distribution of the outcome of
interest [32]. This means that a forecaster who uses a proper scoring rule will be incentivized to provide a forecast
that is as close as possible to the true distribution. In terms of elicitation, scoring rules have a significant role in
motivating assessors to provide conscientious and truthful assessments [17]. Nonetheless, because the concept of
strictproprietyaddsanadditionallevelofuniqueness,theutilizationofstrictlyproperscoringrulesoffersenhanced
4APREPRINT-JULY26,2024
theoreticalassurancesregardingtheoptimalityoftruth-tellingasastrategy[17].Thisencouragespredictorstoprovide
morehonestprobabilisticforecasts.
Inaddition,assessingforecastcharacteristicslikecalibrationandsharpnessisimportantforunderstandingindividual
andcombinedforecasts[16]. Calibrationexaminestheconsistencybetweenforecastsandoutcomes,whilesharpness
measures the concentration of the forecast distribution for a variable of interest, regardless of outcomes [17]. The
relationshipbetweenstrictlyproperscoringrulesandmeasuresofcalibrationandsharpnesscanbeestablishedbyde-
composingtherulesintocomponents.Acommondecompositionenablestheexpressionofascoringruleasafunction
ofbothacalibrationmeasureandasharpnessmeasure[16,26]. Byutilizingstrictlyproperscoringrules,theevalu-
ationprocesscancapturetheprobabilisticuncertaintyinherentinclassificationproblemsandpreventoverconfidence
[16].AccuracyandF-measuresaretwoproperscoringrules,whicharepositivelyoriented.Theyarenotstrictlyproper
scoringrulesbecausetheirmaximumarenotunique[17]. Therefore,strictlyproperscoringrulessuchasBrierScore
andLogarithmicLosshavemoreadvantagesthanmetricsderivedfromtheconfusionmatrix. Inconclusion,thispaper
recommendstheevaluationofmulti-classclassificationusingstrictlyproperscoringrulesforthefollowingreasons:
• Elicitation: Strictly proper scoring rules have desirable elicitation properties. They uniquely incentivize
assessorstoprovidehonestprobabilisticforecastsbymaximizingtheexpectedscore[17].
• Reliability: Strictly proper scoring rules enable reliable assessments of uncertainty. Decomposing proper
scoring rules into calibration and sharpness components facilitates the evaluation of forecast uncertainty.
Thispreventsoverconfidencebycapturingtheinherentprobabilisticuncertaintyinclassification[16,26].
While this paper recommends using strictly proper scoring rules, they are not always sufficient to ensure that more
accurateforecastsreceivebetterscores. Previousevaluationmetricsformulti-classclassificationtaskshavetheshort-
comingofnotexplicitlyfavoringaccuratepredictionsovermisclassifiedones. MetricslikeBrierScoreandLogarith-
micLoss,whilebeingstrictlyproperscoringrules,donotconsistentlyassignbetterscorestopredictionsthatresultin
truepositivesandtruenegativescomparedtothosethatarefalsepositivesandfalsenegatives.Thiscanpotentiallylead
tosuboptimalmodelselectionifamodelachievesbetterscoringbymakingincorrectpredictionsratherthancorrect
ones. The paper addresses this shortcoming by proposing novel superior scoring rules called Penalized Brier Score
andPenalizedLogarithmicLoss.
4 ProposedModel
This section will first present the theoretical background, providing theoretical results that motivate the requirement
forproperscoringrulesthatassignhigherscorestocorrectpredictionsratherthanincorrectones. Next,theproposed
methodology will be introduced. It will illustrate how the scoring rules can be modified by incorporating penalty
terms. This serves to penalize incorrect predictions more strongly, thereby ensuring the scoring rules consistently
assignhigherscorestocorrectpredictionsasintended.
4.1 TheoreticalAnalysis
The primary objective of any classifier is to accurately assign observations to their respective classes. Observations
thatarecorrectlyclassifiedareconsideredtobeofgreatervaluethanthosethatareincorrectlyclassified. Aclassifier
that can achieve a high rate of accurate classifications and a low error rate is deemed superior because it indicates
a greater ability to identify patterns and differentiate between classes. Conversely, observations that are incorrectly
classifiedsuggestthattheclassifierhasnotaccuratelymodeledtherelationshipbetweenfeaturesandclasses. These
errors compromise the classifier’s capacity to generalize and make accurate predictions on new samples. Correct
classifications represent successful outcomes for the classifier, while errors indicate failures. Therefore, it is crucial
to maximize correct classifications and minimize errors in order to develop an effective and useful classifier. The
reliabilityofaclassifier’sperformanceisdeterminedbyitsabilitytoconsistentlyreproducethecorrectlabelsinstead
ofsettlingforinferiorresultsthatarepronetomistakes.
Fig. 1showsthebigpictureofthetheoreticalanalysispresentedinthissection. Thesubsequentpartofthissection
demonstrates that both Brier Scoring and Logarithmic Loss are indifferent to this preference. To prove this point, a
novelattributeforscoringrulesisintroduced. Thisattributeguaranteesthatthescoringruleassignsasuperiorscore
totheobservationsthatarecorrectlyclassified.
Definition5(SuperiorScoringRule). Letψandξbetwosubsetsofthesetofallpredictionsbyacertainclassifier:
ψ ={p| argmaxp=argmaxy},
ξ ={p| argmaxp̸=argmaxy}.
5APREPRINT-JULY26,2024
Figure1: Thebigpictureoftheoreticalanalysis.
6APREPRINT-JULY26,2024
Wherey istheground-truthlabelasavectory = (y ,··· ,y
)suchthat(cid:80)c
y = 1andy = 1whentheiisthe
1 c i=1 i i
trueclass. Therefore, ψ containstruepositiveandtruenegativepredictions, andξ containsfalsepositiveandfalse
negativepredictions. LetS(p,ω)representthescorewhenthepredictionp ∈ P isissuedandthetrueclassisω. S
issuperiorifitsscoresforeverymemberbelongingtoψ arealwaysbetterthanthoseofsetξ. Letp(1) ∈ ψ withits
correspondingtrueclassω(1). Letp(2) ∈ξwithitscorrespondingtrueclassω(2). Then,ifthecondition
(cid:26)
S(p(1),ω(1))<S(p(2),ω(2)) S isnegativelyoriented
S(p(1),ω(1))>S(p(2),ω(2)) S ispositivelyoriented
alwayssatisfied,S issuperior.
Therefore,ifascoringrulehasthisproperty,itwillalwaysgivebetterscorestocorrectpredictions. Inthefollowing,
weshowtheoreticallythatLogarithmicLossandBrierScorearenotsuperior.
Theorem1(LLProperty). Inthecontextofsingle-labelmulti-classclassificationtasks,c > 2,LogarithmicLossis
notsuperior.
Proof. SeeAppendix7.
Theorem 2 (BS Property). In the context of single-label multi-class classification tasks, c > 2, Brier Score is not
superior.
Proof. SeeAppendix8.
According to the Monte Carlo simulations provided in Appendix 8, it is shown that Brier Score outperforms Log-
arithmic Loss. However, to ensure that the scoring rule consistently assigns a higher score to accurately classified
observations,anewcriterionisnecessary,whichcanbeattainedbymodifyingthecurrentscoringrules. Specifically,
if the probability vector belongs to set ξ, an extra penalty was integrated into the scoring rule. Since this research
focuses on the Brier Score and Logarithmic Loss, which should be minimized to be optimal, our aim is to assign a
penaltytoincorrectpredictionsthatthepenaltyvalueishigherthanthehighestscorepossibleforacorrectprediction.
Toaccomplishthis,thepenaltyshouldbeestablishedasequivalenttothemaximumvalueofthescoringruleforsetψ.
Theorem3(MaximumS inψ). ThelargestpossiblevalueoftheBrierScoreforthesetψis c−1.
BS c
Proof. SeeAppendix9.
Theorem4(MaximumS inψ). ThelargestpossiblevalueofLogarithmicLossforthesetψis−log(1).
LL c
Proof. SeeAppendix10.
Based on the highest possible scores for the Brier Score and Logarithmic Loss when predictions are incorrect, an
adjustedformofthesescoringrulescanbedefined. ThemodifiedBrierScorewiththepenaltyterm,PenalizedBrier
Score(PBS),canbeexpressedas:
c (cid:26)
(cid:88) c−1 q ∈ξ
S (q,i)= (y −q )2+ c (4)
PBS i i 0 otherwise
i=1
ThemodifiedLogarithmicLosswiththepenaltyterm,PenalizedLogarithmicLoss(PLL),canbeexpressedas:
c (cid:26)
(cid:88) log(1) q ∈ξ
S (q,i)=− y log(p )− c (5)
PLL i i 0 otherwise
i=1
where y is the ground-truth vector, q is the predicted probability vector by the probabilistic classifier, and c is the
number of classes. As shown in the following theorems, the two proposed evaluation metrics are not only strictly
properscoringrulesbutalsosuperior.
Theorem5(PBS&PLLScoringRules). PBSandPLLarestrictlyproperscoringrules.
Proof. SeeAppendix11.
Theorem6(PBS&PLLProperty). PBSandPLLaresuperior.
7APREPRINT-JULY26,2024
Proof. Thepenaltytermrepresentsthemaximumscorethatcanbeassignedtoacorrectprediction. Consequently,the
scoringrulemodifiedbythepenaltytermpassivelyassignshigherscorestocorrectpredictionsoverincorrectonesin
aconsistentmanner.
Penalties in Eq. (4) and Eq. (5) are designed to be applied only to incorrect predictions. Therefore, these penalties
dependonthevaluesofpinEq. (1)andEq. (2). Furthermore,themorenumberofincorrectpredictionsbyθ inEq.
(3),theworsescoreitreceives. Asaresult,themodifiedevaluationmetricscanmorereliablyidentifyoptimalmodel
checkpointsandearlystoppingpointsthatachievebettergeneralizationperformancecomparedtotheoriginalmetrics
likeBrierScoreandLogarithmicLoss.
4.2 Algorithm
Inthissection,weintroducethevectorizationmethodsoftheproposedevaluationmetrics. ThepseudocodeofthePe-
nalizedBrierScoreandthePenalizedLogarithmicLossisdemonstratedinAlgorithm2andAlgorithm3,respectively.
The PBS and PLL algorithms leverage Penalizing Algorithm 1 to incorporate penalties into the Brier Score and the
LogarithmicLoss.
ThePenalizingalgorithmdetectswrongpredictionsandpenalizesthem. Theoutputisintheformofavector,where
0 means that the prediction was correct, otherwise the prediction was wrong and was penalized. This payoff vector
is then used by other scoring functions to calculate performance metrics. It takes as input predicted probabilities
q ∈ [0,1]n×c,ground-truthlabelsy ∈ {0,1}n×c,andapenaltyvalueforwrongpredictions. Itfirstcomputesthehot
valuesforeachsample,whicharethepredictedprobabilitiesforthecorrectclass. Itthensubtractsthehotvaluesfrom
thepredictedprobabilitiestoobtainacontainerofvaluesthatarepositiveforincorrectpredictions.Allnegativevalues
inthecontaineraresettozero,andthesumoftheremainingvaluesiscomputedforeachsample. Thepenaltyvalue
isthenmultipliedbyavectorofonestobecomeavector. Finally,allpositivevaluesinthecontainerarereplacedby
penaltyandthecontainerisreturnedasthepayoff foreachsample.
ThePBSalgorithmtakesasinputpredictedprobabilitiesqandground-truthlabelsy.Itfirstcomputesthepenaltyfactor
as(c−1)/(c2),wherecisthenumberofclasses. ItthencallsthePenalizingalgorithmtocomputethepayoff foreach
sample. Next,theBrierscoresarecomputedasthemeansquareddifferencebetweenthepredictedprobabilitiesand
theground-truthlabels. Finally,thePBSiscomputedasthemeanofthesumoftheBrierscoresandthepayoffs.
ThePLLalgorithmissimilartothePBSalgorithm,butitusestheLogarithmicLossasthebasescoreinsteadofthe
BrierScore.TheLogarithmicLossiscomputedasthemeanofthesumoftheelement-wiseproductoftheground-truth
labelsandthenegativelogarithmofthepredictedprobabilities. Finally,thepayoff issubtractedfromtheLogarithmic
Lossbeforetakingthemean.
Algorithm1Penalizing
Require: • q: Predictionsofsizen×c,
• y: Ground-truthlabelsofsizen×c,wherenisthenumberofsamplesandcisthenumberofclasses.
• penalty: Ascalarforwrongpredictions.
1: Consideroperator==appliedtoanarraymeansapplyingelement-wise==tothearray. Theresultisacondition
arrayconsistingofTrueandFalse.
2: Consider function where(condition,x,y) gives three arrays with the same shape. The result is taken from x
whereconditionisTrueorywhereitisFalse.
3: Consider function sum(x,axis) computes the sum of the x elements over axis. The result is an array with the
sameshapeasx,withthespecifiedaxisremoved.
4: Considerfunctionmean(x,axis)computestheaverageofthexelementsoveraxis. Theresultisanarraywith
thesameshapeasx,withthespecifiedaxisremoved.
5: Considerfunctionzeros(x,y)returnazeroarraywithshapex×y. Andfunctionones(x,y)returnonearraywith
shapex×y. Ifyisnotprovidedtheresultwouldbe1-dimension.
6: hotvalues←sum(where(y ==1,q,y),axis=1)
7: container←q−hotvalues1T
c
8: zeros←zeros(n,c)
9: container←where(container<0,zeros,container)
10: container←sum(container,axis=1)
11: penalty←ones(c)·penalty
12: payoff ←where(container>0,penalty,container)
13: returnpayoff
8APREPRINT-JULY26,2024
Algorithm2PenalizedBrierScore
Require: Probabilitymeasuresqofsizen×c,Ground-truthlabelsyofsizen×c,wherenisthenumberofsamples
andcisthenumberofclasses
1: penalty←(c−1)/(c2)
2: payoff ←Penalizing(q,y,penalty)
3: bs←mean(square(y−q),axis=1) ▷BrierScores
4: returnmean(bs+payoff)
Algorithm3PenalizedLogarithmicLoss
Require: Probabilitymeasuresqofsizen×c,Ground-truthlabelsyofsizen×c,wherenisthenumberofsamples
andcisthenumberofclasses
1: penalty←log(1/c)
2: payoff ←Penalizing(q,y,penalty)
3: ll←mean(sum(y·log(q)))
4: returnmean(ll−payoff)
5 Experimentalresults
5.1 Datasets
Scoringrulesarecommonlyusedinthefieldofspatio-temporalstatisticstocomparedifferentmodels[33]. Toassess
the effectiveness of our proposed method, we have selected several spatio-temporal datasets. Classifying spatio-
temporaldataisgenerallychallenging,asevidencedbythelowaccuracyscoresachievedonthesedatasets. Forthis
reason,thoroughmodelevaluationtakesongreaterimportance. Additionally,itiscrucialtoconsiderincorrectclasses
when the classifier’s ability to generalize is limited. While the probabilities related to incorrect classes may not be
particularly informative if the probability of the true class for all observations is above 0.5, it becomes increasingly
importantwhentheprobabilityofthetrueclassisbelow0.5formanyobservations.Asaresult,anyevaluationmethod
musttakeintoaccountincorrectclasseswhenmakingdecisions.
Theproposedmodelwasevaluatedusingvariousclassificationproblems, asdescribedbelow. Thedatasetsusedfor
evaluationincludedthree-axisaccelerometersignalsobtainedfromparticipants’activitiessuchasrunningorlying,as
describedin[34,35,36,37]. Whilethesedatasetswereintendedforresearchpurposesrelatedtoactivityrecognition,
theyalsopresentedchallengesforidentifyingindividualsbasedontheirmotionpatterns. Also,thedatasetpresented
in [38] was generated for predicting motor failure time using three-axis accelerometer data. The dataset in [39]
provided information about power consumption such as temperature, humidity, wind speed, consumption, general
diffuse flows, and diffuse flows in three zones of Tetouan City, which is suitable for predicting a zone based on
consumption information. The dataset presented in [40] includes hourly air pollutant data such as PM2.5, PM10,
SO2,NO2,CO,O3,temperature,pressure,dewpointtemperature,precipitation,winddirection,windspeedfrom12
air-quality monitoring sites, and the proposed model predicted the sites based on their air-quality information. The
locationofparticipantswascollectedusingtworeceivedsignalstrengthindicator(RSSI)measurements,asdescribed
in[41]. Whiletheprimarytaskwasindoorlocalization,theproposedmodelaimedtoidentifyparticipantsbasedon
theirlocation. Finally,thedatasetpresentedin[42]includedthree-axisaccelerometerandthree-axisgyroscopedata
fromtendrivers,andthegoalwastoidentifydriversbasedontheirdrivingbehaviors.
5.2 EvaluationStrategy
Sincethepurposeofthisevaluationistoanalyzetwoproposedmetrics,theclassificationmodelusedisaConvolutional
NeuralNetwork(CNN).CNNsarewidelyusedforspatialandtemporaldatamodelingtasks[2]. TheFig. 2illustrates
thearchitectureoftheproposedCNNmodel. Asneuralnetworksareoptimizedthroughiterativetrainingprocedures,
the proposed metrics can be systematically tested during this process. Model checkpointing saves model weights
periodically during training. This allows rolling back to previous checkpoints if overfitting or divergence occurs.
Early stopping monitors validation performance and stops training if no improvement is seen for a set number of
epochs,preventingoverfitting. Implementingthesetechniqueswiththeproposedmetricsprovidesinsightsintotheir
behavioratdifferentstagesofneuralnetworkoptimization.
When dealing with spatio-temporal data, it is essential to employ a cross-validation technique that accounts for the
temporal dependencies within the data. This is crucial because the data exhibits interdependence, and employing a
randomsampleselection fortrainingandtestingcould introducebiasesinthe results [43]. h-blockcross-validation
9APREPRINT-JULY26,2024
Figure2: ThearchitectureoftheCNNclassifier.
Algorithm4h-BlockCross-Validation
1: procedureH-BLOCKCROSSVALIDATION(data,h)
2: Dividedataintohnon-overlappingblocks: B ={B 1,B 2,...,B h}
3: ValidationSizenv ←⌊0.2h⌋
4: TestSizent←⌊0.3h⌋
5: fori=1tohdo
6: ValidationblockB v ←B[i:i+nv]
7: TestblockB ts ←B[i+nv :i+nv+nt]
8: TrainblockB tr ←B\(B v∪B ts)
9: Segmentingblocksbyslidingwindow
10: TrainandvalidatemodelonB tr andB v
11: EvaluatemodelonT ts
12: endfor
13: endprocedure
is a type of temporal cross-validation that is often used for temporal data [43]. It helps address the issue of data
leakage that can occur in standard k-fold cross-validation when applied to time series. The Algorithm 4 shows the
pseudocodeofh-blockCV.Thetimeseriesdatawasfirstdividedintohnon-overlappingblocks,eachrepresentinga
continuoustimeinterval. Foreachblocki,avalidationsetB iscreatedbyselectingacontiguoussubsetofnvblocks
v
starting from block i. A test set B is then created by selecting a contiguous subset of nt blocks starting from the
ts
endofthevalidationblock. TheremainingblocksareassignedtothetrainingsetB . Withineachblock,windowing
tr
techniqueswereappliedtosegmentdata. ThemodelistrainedonthesegmentsofthetrainingsetB andvalidated
tr
onthesegmentsofthevalidationsetB . ThetrainedmodelisthenevaluatedonthesegmentsofthetestsetB . For
v ts
eachiteration,50%ofthesubsetsareallocatedfortraining,30%fortesting,and20%forvalidation. Byadoptingthis
approach, themodelistrainedonasufficientlylargedatasetthatenableseffectivegeneralizationtonewdata, while
thetestingphaseutilizesanindependentdataset.
Thevalidationsethelpsguidehyperparametertuning,suchasselectingtheoptimalwindowlengthandoverlapvalues
oftheslidingwindowevaluatedviagridsearch. Table1summarizesthekeyattributesofeachdatasetusedintheex-
periments.Italsoallowsassessingmodelcheckpointingandearlystoppingcriteriabytrackingvalidationperformance
overtrainingepochs. Oncehyperparametersarefixed,thefinalmodelperformanceisreportedontheindependenttest
sets from each fold. This rigorous evaluation process provides robust estimates of the proposed metrics. The CNN
modelisimplementedusingPythonandtheTensorFlowlibrary. Nadamisemployedtooptimizenetworkparameters.
TheresultsoftheexperimentscanbeaccessedviaGitHub.
5.3 DetailsofExperiments
To ensure a thorough evaluation of the proposed evaluation metrics, the model training incorporated model check-
pointing (CP) and early stopping (ES) techniques, considering both traditional evaluation metrics and the proposed
metrics. CPweresavedduringtrainingatepochswhereeitherthetraditionalmetrics(suchasBrierScoreandLoga-
rithmicLoss)ortheproposedmetricsdemonstratedthebestperformanceonthevalidationset. ESwasimplemented
10APREPRINT-JULY26,2024
Table1: Datasetswereusedinevaluatingtheproposedmethod.
Dataset NumberofClasses WindowLength WindowOverlap
[34] 13 00:00:03 75%
[35] 10 00:00:08 75%
[36] 12 00:00:06 75%
[37] 5 00:00:03 75%
[38] 3 00:00:06 75%
[39] 3 00:07:00 75%
[40] 12 00:01:00 75%
[41] 12 00:00:30 75%
[42] 10 00:00:10 75%
aswell,terminatingtrainingiftherewasnoimprovementineitherthetraditionalmetricsortheproposedmetricsfor
aspecifiednumberofepochs,thuspreventingoverfitting. Toenhancetherobustnessoftheresults,thisentireprocess
was repeated 100 times to identify the optimal model hyperparameters. Such an approach enabled a fair compari-
son between the proposed metrics and the sole use of traditional metrics in determining the best model checkpoint.
Subsequently, the checkpoint yielding the highest performance, as indicated by each metric, was evaluated on the
testset,providinganaccurateassessmentoftheproposedmetric’sabilitytoidentifythemodelwiththehighesttrue
performance.
TheF1scoreandaccuracyarewidelyrecognizedasaprominentevaluationmetricforclassificationtasks. Toeffec-
tivelyevaluatemodelperformanceonsuchunbalanceddatasets,accuracyaloneisnotasuitablemetric[44]. Asmost
ofthereal-worlddatasetsareeitherheavilyormoderatelyimbalanced,accuracycanbemisleadingwhenthenumberof
samplesisverydifferentbetweenclasses[45]. Forthisreason,themacro-averagedF1scoreisalsoreported. F1score
considers both precision and recall, providing a better sense of classification effectiveness on unbalanced problems
[1]. However,itisimportanttonotethatthismetricsolelyservesasaproperscoringruleanddoesnotencompassthe
measurement of forecast uncertainty and overconfidence [11]. In order to address this limitation, it is proposed that
strictlyproperscoringrulesbeemployed,astheypossessthecapabilitytogaugeuncertainty. Byexhibitingbehavior
similartotheF1scorewhilealsomeasuringuncertainty,theseproposedscoringrulescaneffectivelyfulfillbothrelia-
bilityandaccuracyrequirements. Therefore,incorporatingsuchscoringrulesintheevaluationprocesswouldprovide
acomprehensiveassessmentofclassificationmodels, encompassingnotonlytheirpredictiveaccuracybutalsotheir
abilitytoquantifyuncertainty.
5.4 Discussion
Fig.3presentsvalidationstatisticsovertrainingepochsforeachdataset,includingthemacro-averagedF1score,Brier
Score, and proposed Penalized Brier Score. This provides an illustrative example of how the different scoring rules
changeduringmodeloptimization. ThePearsoncorrelation(Cor)betweeneachscoringruleandtheF1scoreisalso
shown. When plottingand comparingthe differentvalidationmetric trendstogether, it’simportantto notethat they
are oriented in different directions. Specifically, the F1 score increases positively as the model improves, while the
PLL and PBS decrease negatively as performance increases. Therefore, to enable the results to be interpreted more
easily,thePBSvaluesgraphedhavebeenmultipliedby-1tomatchthepositiveF1scoretrend.
Importantly, the figure also marks the optimal point on each trend with a data point symbol. This optimal point
correspondstotheepochatwhichthemetricreacheditshighestvalidationvalue. Bypinpointingthesepeaks,wecan
seeexactlywhereeachscoringrulereacheditsbestperformancerelativetotheothers.Notably,thePBStrendsexhibit
optimalpointsthatareclosertotheF1scorepointscomparedtothestandardBrierScore. Thisobservationsuggests
that, inthesespecificexamples, thePBS wasmoresuitableformodelcheckpointing. Additionally, theslopesofthe
PBStrendsdisplaygreatersimilaritytotheF1scoretrendsversusthestandardBrierScoreslopes. Additionally,the
negativecorrelationmetricconfirmsthePBSconsistentlymaintainsastrongerrelationshipwiththeF1scorethanthe
standardBrierScore. Therefore,giventheseobservations,implementingearlystoppingbasedonthePBSratherthan
theBrierScorehasthepotentialtoyieldmodelswithimprovedtestperformance.
5.5 Benchmark
Thepreviousdiscussionexaminingsuperiorscoringruletrendsduringtrainingprovidedinitialinsightsintohowthe
proposed metrics may help optimize model performance. First, the optimal points of the PBS trends were closer to
those of the F1 score trends compared to the standard Brier Score. Additionally, the negative correlation between
the PBS and F1 score was consistently higher. Given this observation, it was expected that employing the PBS for
11APREPRINT-JULY26,2024
(a)[42] (b)[37] (c)[36]
(d)[38] (e)[39] (f)[40]
(g)[41] (h)[34] (i)[35]
Figure3: Thisfigurepresentsanillustrativeexampleofvalidationdatastatistics, basedonepochs, foreachdataset.
TheorangelinesshowtheF1scoreateachepoch. TheredlinesshowtheflippedBrierScore(BS×−1). Thegreen
linesaretheflippedPenalizedBrierScore(PBS×−1). Finally,thesymbolCor representsthePearsoncorrelation
betweentheF1scoreandthecorrespondingscoringrule.
12APREPRINT-JULY26,2024
modelselectioncouldleadtoimprovedperformance. Todecisivelyvalidatetheireffectiveness,rigorousquantitative
evaluationwascarriedoutusingmultipleexperiments.
The first stage involved assessing the correlation between the metrics and the F1 score on validation data using k-
foldcross-validation. Tothisend, theperformanceofmodelsselectedwithdifferentscoringrulesusingbothmodel
checkpointing(CP)andearlystopping(ES)areevaluated. AsshowninTable2,Pearsoncorrelationcoefficientswere
calculatedbetweeneachofthescoringrulesandthemacro-averagedF1scoresacrossdatasetsandfolds. Inthistable,
the symbol Cr represents the correlation between the F1 score and the scoring rule x. Strong positive correlation
x
withinthe−1to1rangeindicatescloseagreementbetweentrends. TheresultsdemonstratethatPBSandPLLtrends
exhibitedthehighestdegreeofsimilaritytoF1scorevariations. Withacorrelationexceedingotherscores,PBSand
PLLcanbereliablyusedtotrackchangesinmodelperformance.
Next, the ability of the proposed superior scoring rules to select high-scoring models was evaluated through k-fold
cross-validation. Table 3 compares the macro-averaged F1 scores of the optimized classifier chosen by each metric
ontestdata. TheseF1scoresweredeterminedthroughESorCP,whichreliedontheutilizationofasuperiorscoring
rule. ThesymbolF1 denotestheF1scoreachievedwhenemployingthescoringrulex. Asdepictedinthetable,itis
x
evidentthattheF1scoreconsistentlyperformsbetterwhenthemodelisselectedusingeachoftheproposedsuperior
scoringrules. Thisobservationemphasizestheeffectivenessofthesuperior scoringrulesinidentifyingmodelsthat
yieldhigherF1scores. Furthermore,consistentlybetterscoresemergedwhenPBSguidedselectionratherthanPLL.
Consequently, these findings underscore the importance of employing appropriate superior scoring rules to circum-
vent shortcomings of F1 score alone in model selection, ultimately enabling improved classification capability and
more trustworthy predictions. Collectively, the correlational and benchmark results provide compelling quantitative
evidence that proposed penalties within strictly proper scoring rules augment their capacity to reflect true perfor-
mancechanges,inadditiontofacilitatingtheidentificationofmodelswithstrongerpredictivepowerfornewsamples.
Therefore, by more faithfully reflecting F1 score behavior, the proposed criteria enhance optimal model evaluation,
selection,andclassificationforchallengingspatio-temporalapplications.
Table 2: The table presents a comparison based on the Pearson correlation between F1 scores and the scoring rules
usingvalidationdata. TheabbreviationsESandCPrepresentEarlyStopingandModelCheckpointing,respectively.
ThesymbolCr denotesthecorrelationbetweenF1scoreandthescoringruleofx.
x
Data ES CP CrBS CrPBS ∆ CrLL CrPLL ∆
✓ 0.957(±0.02) 0.969(±0.01) 0.012 0.837(±0.08) 0.900(±0.06) 0.063
[34] ✓ 0.964(±0.01) 0.980(±0.01) 0.016 0.526(±0.47) 0.640(±0.30) 0.113
✓ 0.963(±0.04) 0.983(±0.02) 0.020 0.764(±0.17) 0.845(±0.11) 0.081
[35] ✓ 0.699(±0.46) 0.926(±0.09) 0.228 0.520(±0.53) 0.589(±0.45) 0.069
✓ 0.721(±0.17) 0.740(±0.22) 0.019 0.731(±0.15) 0.745(±0.14) 0.013
[36] ✓ 0.607(±0.49) 0.748(±0.45) 0.141 0.306(±0.53) 0.508(±0.45) 0.202
✓ 0.690(±0.73) 0.748(±0.45) 0.058 0.567(±0.53) 0.619(±0.49) 0.052
[37] ✓ 0.667(±1.00) 0.674(±0.72) 0.007 0.317(±0.65) 0.378(±0.65) 0.061
✓ 0.717(±0.61) 0.728(±0.61) 0.010 0.275(±0.70) 0.292(±0.71) 0.016
[38] ✓ 0.739(±0.63) 0.740(±0.63) 0.001 0.384(±0.60) 0.425(±0.55) 0.041
✓ 0.965(±0.08) 0.987(±0.03) 0.022 0.592(±0.16) 0.664(±0.13) 0.072
[39] ✓ 0.995(±0.01) 0.997(±0.01) 0.002 0.419(±0.65) 0.446(±0.66) 0.026
✓ 0.680(±0.31) 0.899(±0.05) 0.219 0.595(±0.48) 0.777(±0.25) 0.182
[40] ✓ 0.665(±0.30) 0.813(±0.13) 0.148 0.378(±0.71) 0.802(±0.14) 0.424
✓ 0.572(±0.48) 0.597(±0.27) 0.025 0.694(±0.23) 0.724(±0.21) 0.031
[41] ✓ 0.444(±0.48) 0.704(±0.21) 0.260 0.434(±0.61) 0.498(±0.53) 0.064
✓ 0.874(±0.14) 0.924(±0.07) 0.050 0.450(±0.38) 0.553(±0.32) 0.103
[42] ✓ 0.916(±0.12) 0.953(±0.06) 0.037 0.391(±0.52) 0.629(±0.32) 0.239
6 Conclusion
This study introduced novel superior scoring rules called Penalized Brier Score (PBS) and Penalized Logarithmic
Loss (PLL) for evaluating probabilistic classification models. PBS and PLL modify the traditional Brier Score and
Logarithmic Loss by integrating a penalty term for misclassified observations. As demonstrated formally, PBS and
PLLsatisfythepropertiesofstrictlyproperscoringruleswhilealsoconsistentlyassigningsuperiorscorestocorrectly
classifiedobservations. TheexperimentalevaluationhighlightedthebenefitsofusingPBSandPLLformodelcheck-
pointing and early stopping. PBS and PLL demonstrated a higher negative correlation with the F1 score compared
totraditionalBrierScoreandLogarithmicLossduringmodeltraining. Consequently,theproposedscoringfunctions
weremoreeffectiveinidentifyingoptimalmodelcheckpointsanddeterminingearlystoppingpoints, leadingtoim-
provedF1scores. ThetestresultssubstantiatedthatmodelselectionbasedonPBSandPLLyieldedsuperiorF1scores
comparedtotraditionalmetrics. Inconclusion,PBSandPLLenablemoreaccuratemodelevaluationbyencapsulating
bothproperscoringruleprinciplesandpreferentialtreatmentofcorrectclassifications.Theproposedmetricsaddressa
13APREPRINT-JULY26,2024
Table3:ThistableprovidesacomparativeanalysisofscoringrulesusingF1scoresobtainedfromtestdata,whichwere
determined through model selection based on a scoring rule. The abbreviations ES and CP represent Early Stoping
andModelCheckpointing, respectively. Additionally, thesymbolF1 representstheF1scoreachievedthroughthe
x
utilizationofthescoringruleofx.
Data ES CP F1BS F1PBS ∆ F1LL F1PLL ∆
✓ 45.00(±0.05) 51.65(±0.07) 6.65 64.76(±0.09) 65.85(±0.08) 1.10
[34] ✓ 55.47(±0.05) 60.03(±0.07) 4.56 70.00(±0.08) 71.82(±0.07) 1.83
✓ 53.01(±0.06) 57.60(±0.04) 4.59 53.90(±0.08) 55.48(±0.05) 1.59
[35] ✓ 55.37(±0.04) 58.03(±0.05) 2.66 55.74(±0.06) 57.63(±0.07) 1.89
✓ 28.24(±0.07) 32.48(±0.09) 4.24 30.08(±0.04) 30.62(±0.04) 0.53
[36] ✓ 32.30(±0.08) 34.28(±0.09) 1.99 31.65(±0.08) 31.80(±0.05) 0.15
✓ 58.27(±0.20) 59.14(±0.19) 0.87 56.79(±0.14) 59.55(±0.13) 2.76
[37] ✓ 66.51(±0.06) 67.49(±0.05) 0.97 68.21(±0.11) 69.78(±0.08) 1.57
✓ 43.66(±0.34) 50.81(±0.32) 7.14 51.12(±0.32) 59.69(±0.26) 8.57
[38] ✓ 48.00(±0.09) 54.83(±0.16) 6.83 64.12(±0.18) 66.82(±0.22) 2.71
✓ 80.93(±0.08) 83.20(±0.07) 2.27 77.87(±0.06) 80.13(±0.08) 2.26
[39] ✓ 82.08(±0.08) 83.88(±0.07) 1.80 80.22(±0.06) 83.42(±0.06) 3.20
✓ 50.13(±0.09) 53.51(±0.07) 3.38 55.59(±0.08) 56.33(±0.08) 0.74
[40] ✓ 52.04(±0.11) 53.63(±0.10) 1.59 51.73(±0.07) 52.39(±0.07) 0.66
✓ 27.09(±0.02) 27.77(±0.03) 0.69 23.10(±0.04) 23.75(±0.02) 0.65
[41] ✓ 26.51(±0.05) 29.37(±0.04) 2.86 26.77(±0.04) 27.39(±0.05) 0.62
✓ 66.50(±0.03) 66.53(±0.03) 0.03 65.39(±0.06) 65.81(±0.05) 0.43
[42] ✓ 67.85(±0.03) 68.36(±0.02) 0.51 66.64(±0.04) 67.07(±0.05) 0.43
criticalgapbetweenprobabilisticuncertaintyassessmentanddeterministicaccuracymaximization. Byaccountingfor
uncertaintyandthevalueoftrueclassifications,PBSandPLLcanenhancemodelselection,checkpointing,andearly
stopping in classification tasks requiring reliable predictive uncertainty. Further research can explore PBS and PLL
with different model architectures and classification problems. Also, various penalties can be investigated to obtain
betterperformance.
Appendix
7 ProofofTheorem1
Letxbeamemberofthesetψwithx =α,whereiistheindexofthetrueclass,andletybetheground-truthlabel
i
vector. Duetothesingle-labelclassificationproperty,theEq. (2)canbeexpressedas:
c
(cid:88)
S (x,i)=− y log(x )=−log(x ) (6)
LL k k i
k=1
Now,letq ∈ξbeanotherprediction. Therearethreepossiblecasesforthetrueclassprobabilityq :
i
• q =α. ItmeansthatS (x,i)=S (q,i).
i LL LL
• q =α−β,whereβ >0. Sinceα>α−βand−log(q )ismonotonicallydescendingandpositivein[0,1],
i i
itfollowsthatS (q,i)>S (x,i).
LL LL
• q =α+β,whereβ >0. Sinceα+β >αand−log(q )ismonotonicallydescendingandpositivein[0,1],
i i
itfollowsthatS (x,i)>S (q,i).
LL LL
Therefore,whenq ≥x ,theLogarithmicLossdoesnotassignastrictlyhigherscoretothepredictionqcomparedto
i i
thetruepredictionx. Hence,itcannotbeconsideredsuperior.
8 ProofofTheorem2
Letx ∈ ψ suchthatx = α,whereiistheindexofthetrueclass. Lety betheground-truthlabelvector. Duetothe
i
single-label classification property, the Logarithmic Loss can be expressed as: Due to the single-label classification
property,Eq. (1)canbewrittenas:
c
(cid:88)
S (x,i)= x2 +(1−x )2 (7)
BS k i
k=1,k̸=i
Inthefollowing,theterm”hotvalue”isusedtorefertotheelementoftheprobabilityvectorsthatcorrespondtothe
truthclass,whereastheotherelementsarereferredtoas”non-hotvalues”. Now,letq ∈ ξ. Therearethreepossible
casesforthetrueclassprobabilityq :
i
14APREPRINT-JULY26,2024
8.1 q =α
i
Itispossibletodemonstratethatthevarianceofthenon-hotparthasthegreatestimpactonthescorevalueoftheBrier
scorefunction. TheBrierScoreforqisgivenbythefollowingexpression:
c
(cid:88)
S (q,i)= (q )2+(1−q )2 (8)
BS k i
(cid:124) (cid:123)(cid:122) (cid:125)
k=1,k̸=i
hotpart
(cid:124) (cid:123)(cid:122) (cid:125)
non−hotpart
wheretheindexofirepresentsthetrueclassori = argmax y. Thenon-hotpartcanbeexpandedinthefollowing
manner:
c c
(cid:88) (cid:88)
(q )2 = (q −q˜+q˜)2 = (9)
k k
k=1,k̸=i k=1,k̸=i
c
(cid:88) (cid:0) (q −q˜)2+2q˜(q −q˜)+q˜2(cid:1) = (10)
k k
k=1,k̸=i
c
c (cid:88) (cid:0) (q −q˜)2+2q˜(q −q˜)+q˜2(cid:1) = (11)
c k k
k=1,k̸=i
c c
c (cid:88) 2cq˜ (cid:88)
(q −q˜)2+ ((q )−q˜)+cq˜2 = (12)
c k c k
k=1,k̸=i k=1,k̸=i
c c
c (cid:88) 2cq˜ (cid:88)
(q −q˜)2+ (q )−2cq˜2+cq˜2 = (13)
c k c k
k=1,k̸=i k=1,k̸=i
c
c (cid:88)
(q −q˜)2+2cq˜2−2cq˜2+cq˜2 = (14)
c k
k=1,k̸=i
c
c (cid:88)
(q −q˜)2+cq˜2 (15)
c k
k=1,k̸=i
Ifq˜istheaverageofthenon-hatpart,then:
 
 
(cid:88)c  1 (cid:88)c 
(q )2 =c (q −q˜)2 +q˜2 (16)
k  c k 
 
k=1,k̸=i  k=1,k̸=i 
(cid:124) (cid:123)(cid:122) (cid:125)
non−hotpartvariance
Itisevidentthat:
c
(cid:88) q =1−q ⇒q˜= 1−q i (17)
k i c−1
k=1,k̸=i
Consequently,itcanbeobtainedthat:
 
c
1 (cid:88)
S BS(q,i)=c
c
(q k−q˜)2+q˜2 +(1−q i)2 (18)
k=1,k̸=i
wherei = argmax y. Therefore,basedonEq. (16),theBrierScoreinEq. (18)canbeminimizedbyreducingthe
varianceofthenon-hotpartandmaximizingq .
i
15APREPRINT-JULY26,2024
Furthermore,qmayhavetnon-hotvaluesthatexceedα,andthesumofthesetnon-hotvaluesisequaltotα+δ. The
followingexpressionisutilizedtoformulatethesumofc−tnon-hotvaluesthatarelessthanα:
c
(cid:88)
q +α=1 (19)
k
k=1,k̸=i
c
(cid:88)
⇒ q =1−α (20)
k
k=1,k̸=i
c
(cid:88)
⇒ q −(tα+δ)+(tα+δ)=1−α (21)
k
k=1,k̸=i
c
(cid:88)
⇒ q −(tα+δ)=1−(t+1)α−δ (22)
k
k=1,k̸=i
Thisimpliesthatthesumofthenon-hotvaluesisequivalentto(tα+δ)+(1−(t+1)α−δ). Asδ increasesand
approaches1,thedifferencebetweentα+δand1−(t+1)α−δalsoincreases,leadingtoanincreaseinthevariance
ofthenon-hotpartinEq. (18). Therefore,itcanbeinferredthatthevarianceofthenon-hotpartofqisgreaterthanx.
Letx˜representtheaverageofthenon-hatpartofx:
   
c c
1 (cid:88) 1 (cid:88)

c
(x k−x˜)2 <
c
(q k−q˜)2  (23)
k=1,k̸=i k=1,k̸=i
Now,asq =x =αandx˜=q˜= 1−α,wecanproceedasfollows:
i i c−1
S (x,i)−S (q,i)= (24)
BS BS
 
c
(cid:88)
 (x k−x˜)2+cx˜2+(1−x i)2 −
k=1,k̸=i
 
c
(cid:88)
 (q k−q˜)2+cq˜2+(1−q i)2 = (25)
k=1,k̸=i
c c
(cid:88) (cid:88)
(x −x˜)2− (q −q˜)2 = (26)
k k
k=1,k̸=i k=1,k̸=i
 
c c
1 (cid:88) 1 (cid:88)
c
c
(x k−x˜)2−
c
(q k−q˜)2 <0 (27)
k=1,k̸=i k=1,k̸=i
Thus,theconditionS (q,i)>S (x,i)stillholdsevenifanon-hotvalueisonlyslightlygreaterthanα. AndS
BS BS BS
isnotsuperiorinthiscase.
8.2 q =α−β
i
Toverifythevalidityoftheconditionofbeingsuperior,aMonteCarlosimulationcanbeconducted,whichinvolves
comparingS (q,i)toS (x,i)fornumerousrandomlygeneratedvaluesofx∈ψandq ∈ξ. Thevariablesinthis
BS BS
simulationincludex,q,α,β,andc,whereαdenotesthehotvalueofx,β representsthedifferencebetweenthehot
valueofqandα,andcrepresentsthenumberofclasses. Allofthesevariablesarerandomlygeneratedfromanormal
distribution.
Foreachcomparison, a randomx isselectedfrom ψ suchthat x = α and x ∈ Rc, where α representsahot value
i
andcrepresentsthenumberofclasses. Next,arandomqischosenfromξsuchthatq =α−β,whereβisarandom
i
positivevalue. Thepair(x,q)isthenevaluatedtodeterminewhetherS (q,i)>S (x,i)holds.
BS BS
Figure4presentstheresultsoftheMonteCarlosimulationforvaryingnumbersofcomparisons. Thefiguredemon-
stratestheconvergenceoftheMonteCarlomethodandconfirmsthattheconditionS (q,i)>S (x,i)issatisfied
BS BS
inalmost99.996%ofcomparisons. Thefiguredisplaysthecumulativepercentageofcomparisonsinwhichthecon-
ditionS (q,i)>S (x,i)holds. Whenq <α,theBrierScoretypicallyassignsahigherscoretotheobservation
BS BS i
q;however,thisisnotalwaysthecase. Therefore,theBrierScorecannotberegardedassuperior.
16APREPRINT-JULY26,2024
Figure4: CumulativepercentageofinstanceswhereconditionS (q,i)>S (x,i)ismet.
BS BS
Figure5: CumulativepercentageofinstanceswhereconditionS (q,i)>S (x,i)ismet.
BS BS
17APREPRINT-JULY26,2024
8.3 q =α+β
i
Tovalidatetheconditionofbeingsuperior,itispossibletoconductanotherMonteCarlosimulation,whichinvolves
comparing S (q,i) > S (x,i) for a large number of randomly generated values of x ∈ ψ and q ∈ ξ. The
BS BS
simulation includes variables such as x, q, α, β, and c, where α represents the hot value of x, β represents the
difference between the hot value of q and α, and c represents the number of classes. All of these variables are
randomlygeneratedfromanormaldistribution.
Ineachcomparison,arandomvalueofxischosenfromψ suchthatx = αandxbelongstoRc,whereαdenotesa
i
hotvalueandcrepresentsthenumberofclasses. Then,arandomvalueofqisselectedfromξ suchthatq = α+β,
i
whereβ isarandompositivevalue. Thepair(x,q)isevaluatedtodetermineifS (q,i)>S (x,i)istrue.
BS BS
Figure 5 depicts the outcomes of the Monte Carlo simulation for different numbers of comparisons, indicating the
convergenceoftheMonteCarlomethod. Thefigureillustratesthecumulativepercentageofcomparisonswherethe
condition S (q,i) > S (x,i) is satisfied. The results confirm that the condition S (q,i) > S (x,i) holds
BS BS BS BS
in roughly 63% of comparisons. When q > α, it is common for the Brier Score to assign a higher score to the
i
observationx;however,thisisnotalwaysthecase. Hence,theBrierScorecannotbeconsideredsuperior.
9 ProofofTheorem3
Letxbeanarbitrarymemberofthesetψandybetheone-hotground-truthlabel. Letx =αwhereiistheindexof
i
thetrueclass. TomaximizeS (x,i),thefollowingequationishelpful:
BS
c c
(cid:88) (cid:88)
x =1⇒ x =1−α (28)
k k
k=1 k=1,k̸=i
c
(cid:88)
⇒ x2 ≤(1−α)2 (29)
k
k=1,k̸=i
Since (1−α)2 is the primary component of the Brier Score, the maximization of (1−α)2 (or the minimization of
α)isessentialtoobtainthehighestpossiblevalueofS (x,i). Theminimumvalueofα is 1 +ϵ, asanyvalueof
BS c
αbelowthisthresholdwouldmakexunsuitableforinclusioninthesetψ. Asaresult,αisequivalentto 1 +ϵ. On
c
theotherhand,ifx = 1 +ϵ,theremainingelementsofthevectorxcannotexceed 1 +ϵ. Tosimplifytheanalysis,
i c c
assumingϵ=0,theelementsofxwouldbex = 1, ∀k ∈[1,··· ,c]. Therefore:
k c
c
(cid:88)
maxS (x,i)= x2 +(1−x )2 (30)
BS k i
k=1,k̸=i
c
(cid:88) 1 1
= +(1− )2 (31)
c2 c
k=1,k̸=i
1 1
=(c−1) +(1− )2 (32)
c2 c
1 1 2 1
= − +1− + (33)
c c2 c c2
1 c−1
=1− = (34)
c c
10 ProofofTheorem4
Letxbeanarbitrarymemberofthesetψ,andy betheone-hotground-truthlabelvectorsuchthatx = α,wherei
i
istheindexofthetrueclass. AccordingtoEq. (6)andsince−log isadecreasingandpositivefunctionintherange
[0,1],minimizingαisnecessarytomaximizeS (x,i). Theminimumvalueofαis 1 +ϵ,asanyvalueofαbelow
LL c
thisthresholdwouldrenderxunsuitableforinclusioninthesetψ. Tosimplifytheanalysis,weassumeϵ=0,which
yields:
1
maxS (x,i)=−log( ) (35)
LL c
18APREPRINT-JULY26,2024
11 ProofofTheorem5
AsS andS arestrictlyproper,so:
BS LL
S (P,Q)>S (Q,Q) (36)
BS BS
S (P,Q)>S (Q,Q) (37)
LL LL
forQ̸=P. Furthermore,itisclearthat:
S (Q,Q)=S (Q,Q) (38)
BS PBS
S (Q,Q)=S (Q,Q) (39)
LL PLL
andalso:
S (P,Q)≥S (P,Q) (40)
PBS BS
S (P,Q)≥S (P,Q) (41)
PLL LL
Therefore:
S (P,Q)≥S (P,Q)>S (P,Q) (42)
PBS BS BS
S (P,Q)≥S (P,Q)>S (P,Q) (43)
PLL BS LL
Asaresult,S andS arestrictlyproper.
PBS PLL
References
[1] MohammadHossinandMdNasirSulaiman. Areviewonevaluationmetricsfordataclassificationevaluations.
Internationaljournalofdatamining&knowledgemanagementprocess,5(2):1,2015.
[2] SenzhangWang,JiannongCao,andPhilipYu. Deeplearningforspatio-temporaldatamining: Asurvey. IEEE
TransactionsonKnowledgeandDataEngineering,2020.
[3] NicholasPudjihartono,TayazaFadason,AndreasWKempa-Liehr,andJustinMO’Sullivan. Areviewoffeature
selection methods for machine learning-based disease risk prediction. Frontiers in Bioinformatics, 2:927312,
2022.
[4] Wentao Zhang, Jiawei Jiang, Yingxia Shao, and Bin Cui. Snapshot boosting: a fast ensemble framework for
deepneuralnetworks. ScienceChinaInformationSciences,63:1–12,2020.
[5] ChandraSekharaRaoAnnavarapuetal. Deeplearning-basedimprovedsnapshotensembletechniqueforcovid-
19chestx-rayclassification. AppliedIntelligence,51(5):3104–3120,2021.
[6] Muhammad Ibraheem Siddiqui, Khurram Khan, Adnan Fazil, and Muhammad Zakwan. Snapshot ensemble-
based residual network (snapensemresnet) for remote sensing image scene classification. GeoInformatica,
27(2):341–372,2023.
[7] Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of checkpointing for the
reverseoradjointmodeofcomputationaldifferentiation. ACMTransactionsonMathematicalSoftware(TOMS),
26(1):19–45,2000.
[8] Gu¨rol Canbek, Tugba Taskaya Temizel, and Seref Sagiroglu. Ptopi: A comprehensive review, analysis, and
knowledgerepresentationofbinaryclassificationperformancemeasures/metrics.SNComputerScience,4(1):13,
2022.
[9] LutzPrechelt. Earlystopping-butwhen? InNeuralNetworks: Tricksofthetrade,pages55–69.Springer,2002.
[10] AlaaTharwat. Classificationassessmentmethods. Appliedcomputingandinformatics,17(1):168–192,2020.
[11] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In
InternationalConferenceonMachineLearning,pages1321–1330.PMLR,2017.
[12] JuozasVaicenavicius,DavidWidmann,CarlAndersson,FredrikLindsten,JacobRoll,andThomasScho¨n. Eval-
uating model calibration in classification. In The 22nd International Conference on Artificial Intelligence and
Statistics,pages3459–3467.PMLR,2019.
[13] Shaoxun Xu, Yufei Chen, Chao Ma, and Xiaodong Yue. Deep evidential fusion network for medical image
classification. InternationalJournalofApproximateReasoning,150:188–198,2022.
19APREPRINT-JULY26,2024
[14] TilmannGneiting,AdrianERaftery,AntonHWestveld,andTomGoldman. Calibratedprobabilisticforecasting
usingensemblemodeloutputstatisticsandminimumcrpsestimation. MonthlyWeatherReview, 133(5):1098–
1118,2005.
[15] TeddySeidenfeld,MarkJSchervish,andJosephBKadane. Forecastingwithimpreciseprobabilities. Interna-
tionalJournalofApproximateReasoning,53(8):1248–1261,2012.
[16] RobertLWinkler,YaelGrushka-Cockayne,KennethCLichtendahlJr,andVictorRichmondRJose. Probability
forecastsandtheircombination: Aresearchperspective. DecisionAnalysis,16(4):239–260,2019.
[17] TilmannGneitingandAdrianERaftery. Strictlyproperscoringrules,prediction,andestimation. Journalofthe
AmericanstatisticalAssociation,102(477):359–378,2007.
[18] GlennWBrier. Verificationofforecastsexpressedintermsofprobability. Monthlyweatherreview,78(1):1–3,
1950.
[19] Irving John Good. Rational decisions. Journal of the Royal Statistical Society: Series B (Methodological),
14(1):107–114,1952.
[20] ArthurCarvalho. Anoverviewofapplicationsofproperscoringrules. DecisionAnalysis,13(4):223–242,2016.
[21] DavidBolinandJonasWallin. Localscaleinvarianceandrobustnessofproperscoringrules. StatisticalScience,
38(1):140–159,2023.
[22] Ju¨rgenLandes. Probabilism, entropiesandstrictlyproperscoringrules. InternationalJournalofApproximate
Reasoning,63:1–21,2015.
[23] WenbinQian,JintaoHuang,YinglongWang,andYonghongXie. Labeldistributionfeatureselectionformulti-
labelclassificationwithroughset. Internationaljournalofapproximatereasoning,128:32–55,2021.
[24] AlexanderPhilipDawidandMonicaMusio.Theoryandapplicationsofproperscoringrules.Metron,72(2):169–
183,2014.
[25] Ebrahim Mortaz. Imbalance accuracy metric for model selection in multi-class imbalance classification prob-
lems. Knowledge-BasedSystems,210:106490,2020.
[26] MeelisKullandPeterFlach.Noveldecompositionsofproperscoringrulesforclassification:Scoreadjustmentas
precursortocalibration. InMachineLearningandKnowledgeDiscoveryinDatabases: EuropeanConference,
ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15, pages 68–85. Springer,
2015.
[27] DanielSWilks. Forecastverification. InInternationalgeophysics,volume100,pages301–394.Elsevier,2011.
[28] Hans Hersbach. Decomposition of the continuous ranked probability score for ensemble prediction systems.
WeatherandForecasting,15(5):559–570,2000.
[29] Hristos Tyralis and Georgia Papacharalampous. A review of predictive uncertainty estimation with machine
learning. ArtificialIntelligenceReview,57(4):94,2024.
[30] JakubNowotarskiandRafałWeron. Recentadvancesinelectricitypriceforecasting: Areviewofprobabilistic
forecasting. RenewableandSustainableEnergyReviews,81:1548–1568,2018.
[31] Carlo Gaetan, Federica Giummole`, and Valentina Mameli. Calibrated emos: applications to temperature and
windspeedforecasting. EnvironmentalandEcologicalStatistics,pages1–25,2024.
[32] JohannesResin. Fromclassificationaccuracytoproperscoringrules: Elicitabilityofprobabilistictoplistpre-
dictions. JournalofMachineLearningResearch,24(173):1–21,2023.
[33] Matthew JHeaton, AbhirupDatta, Andrew OFinley, ReinhardFurrer, Joseph Guinness, Rajarshi Guhaniyogi,
Florian Gerber, Robert B Gramacy, Dorit Hammerling, Matthias Katzfuss, et al. A case study competition
amongmethodsforanalyzinglargespatialdata.JournalofAgricultural,BiologicalandEnvironmentalStatistics,
24:398–425,2019.
[34] PierluigiCasale,OriolPujol,andPetiaRadeva. Personalizationanduserverificationinwearablesystemsusing
biometricwalkingpatterns. PersonalandUbiquitousComputing,16(5):563–580,2012.
[35] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. Smartphone and smartwatch-based biometrics using
activitiesofdailyliving. IEEEAccess,7:133190–133202,2019.
[36] RobertoLShinmotoTorres,DamithCRanasinghe,QinfengShi,andAlansonPSample. Sensorenabledwear-
ablerfidtechnologyformitigatingtheriskoffallsnearbeds. In2013IEEEinternationalconferenceonRFID
(RFID),pages191–198.IEEE,2013.
20APREPRINT-JULY26,2024
[37] BosˇtjanKaluzˇa,VioletaMirchevska,ErikDovgan,MitjaLusˇtrek,andMatjazˇ Gams. Anagent-basedapproach
tocareinindependentliving.InInternationaljointconferenceonambientintelligence,pages177–186.Springer,
2010.
[38] Gustavo Scalabrini Sampaio, Arnaldo Rabello de Aguiar Vallim Filho, Leilton Santos da Silva, and Leandro
Augusto da Silva. Prediction of motor failure time using an artificial neural network. Sensors, 19(19):4342,
2019.
[39] Abdulwahed Salam and Abdelaaziz El Hibaoui. Comparison of machine learning algorithms for the power
consumption prediction:-case study of tetouan city–. In 2018 6th International Renewable and Sustainable
EnergyConference(IRSEC),pages1–5.IEEE,2018.
[40] Shuyi Zhang, Bin Guo, Anlan Dong, Jing He, Ziping Xu, and Song Xi Chen. Cautionary tales on air-quality
improvementinbeijing. ProceedingsoftheRoyalSocietyA:Mathematical,PhysicalandEngineeringSciences,
473(2205):20170457,2017.
[41] DheeruDuaandCaseyGraff. UCImachinelearningrepository,2017.
[42] Hamid Reza Eftekhari and Mehdi Ghatee. Hybrid of discrete wavelet transform and adaptive neuro fuzzy in-
ferencesystemforoveralldrivingbehaviorrecognition. TransportationresearchpartF:trafficpsychologyand
behaviour,58:782–796,2018.
[43] Jun Shao. Linear model selection by cross-validation. Journal of the American statistical Association,
88(422):486–494,1993.
[44] FaresGrina,ZiedElouedi,andEricLefevre. Re-samplingofmulti-classimbalanceddatausingbelieffunction
theoryandensemblelearning. InternationalJournalofApproximateReasoning,156:1–15,2023.
[45] Seraf´ınMoral-Garc´ıa, CarlosJMantas, JavierGCastellano, andJoaqu´ınAbella´n. Usingcredalc4.5forcali-
bratedlabelrankinginmulti-labelclassification. InternationalJournalofApproximateReasoning,147:60–77,
2022.
21