Differentiable Quantum Architecture Search in
Asynchronous Quantum Reinforcement Learning
Samuel Yen-Chi Chen
Wells Fargo
New York, NY, USA
yen-chi.chen@wellsfargo.com
Abstract—The emergence of quantum reinforcement learning [14],naturallanguageprocessing[15]–[19],andreinforcement
(QRL) is propelled by advancements in quantum computing learning [20]–[26]. Despite the achievements of various QML
(QC)andmachinelearning(ML),particularlythroughquantum
models,asignificantchallengehinderingwidespreadadoption
neural networks (QNN) built on variational quantum circuits
is the necessity for extensive expertise in designing effective
(VQC).Theseadvancementshaveprovensuccessfulinaddressing
sequentialdecision-makingtasks.However,constructingeffective quantum circuit architectures. For instance, crafting the struc-
QRL models demands significant expertise due to challenges in ture of encoding and parameterized circuits within the VQC
designingquantumcircuitarchitectures,includingdataencoding demands specialized design considerations, including appro-
and parameterized circuits, which profoundly influence model
priate entanglement to showcase quantum advantages. There
performance. In this paper, we propose addressing this chal-
exists a pressing demand for an automated procedure capable
lengewithdifferentiablequantumarchitecturesearch(DiffQAS),
enabling trainable circuit parameters and structure weights ofstreamliningthesearchforhigh-performingquantumcircuit
using gradient-based optimization. Furthermore, we enhance architectures. In this manuscript, we address this challenge
training efficiency through asynchronous reinforcement learning throughtheimplementationofdifferentiablequantumarchitec-
(RL) methods facilitating parallel training. Through numerical
ture search (DiffQAS). Our objective is to integrate DiffQAS
simulations, we demonstrate that our proposed DiffQAS-QRL
into quantum reinforcement learning (QRL), as reinforcement
approach achieves performance comparable to manually-crafted
circuitarchitecturesacrossconsideredenvironments,showcasing learning (RL) stands out in the realm of ML for its han-
stability across diverse scenarios. This methodology offers a dling of sequential decision-making problems and potential
pathway for designing QRL models without extensive quantum toexhibithigh-levelproblem-solvingcapabilities.Specifically,
knowledge, ensuring robust performance and fostering broader
we examine a selection of VQC block candidates and assign
application of QRL.
trainable structural weights to these blocks. Through gradient
Index Terms—Quantum machine learning, Quantum neural
networks, Reinforcement learning, Variational quantum circuits descent optimization, we concurrently learn these structural
weightsalongsidetheconventionalquantumcircuitparameters
(rotation angles). Furthermore, departing from prior works
I. INTRODUCTION in QAS for QRL, we train the agents using asynchronous
training rather than single-process policy updates, thereby
Quantum computing (QC) theoretically possesses the po-
leveraging the computational resources of multi-core CPUs or
tential to fundamentally transform computational tasks, pre-
potentially multiple quantum processing units (QPUs) in the
senting clear advantages over classical computers [1]. The
future. Numerical simulations are employed to illustrate the
advancementinQChardware,coupledwithclassicalMLtech-
efficacy of the proposed DiffQAS-QRL framework in identi-
niques, enables the progression of quantum machine learning
fying VQC architectures capable of achieving high scores in
(QML). This is realized through the hybrid quantum-classical
diverse testing environments. Specifically, we demonstrate the
computingparadigm[2],[3],whereinbothclassicalandquan-
stability of the proposed method across various environments,
tumcomputersareharnessed.Specifically,computationaltasks
contrasting with the inconsistent performance of manually-
suitedforQCcapabilitiesareexecutedonquantumcomputers,
designed architectures across different scenarios. This obser-
whiletaskssuchasgradientcalculations,well-handledbyclas-
vationunderscoresthenecessityforatask-agnosticautomated
sical computers, remain within their domain. The variational
procedure in designing QRL circuits and underscores the
quantum circuit (VQC) serves as the fundamental component
effectiveness of the proposed DiffQAS-QRL framework.
in existing QML methodologies. A plethora of QML models,
This manuscript is structured as follows: SectionII presents
based on VQC, have been developed to address various
a concise overview of the current advancements in QAS and
machine learning (ML) tasks including classification [4]–[9],
QRL.InSectionIII,thefundamentalconceptsofquantumRL
time-series prediction [10]–[12], generative modeling [13],
andVQCareelucidated,formingthefoundationalcomponents
The views expressed in this article are those of the authors and do not ofextantQMLandQRLmodels,whichrepresentthefocusof
representtheviewsofWellsFargo.Thisarticleisforinformationalpurposes inquiry for the proposed framework. The formulation of the
only. Nothing contained in this article should be construed as investment
differentiable QAS problem is delineated in SectionIV, while
advice. Wells Fargo makes no express or implied warranties and expressly
disclaimsalllegal,tax,andaccountingimplicationsrelatedtothisarticle. the intricacies of the proposed DiffQAS-QRL framework are
4202
luJ
52
]hp-tnauq[
1v20281.7042:viXraexpounded upon in SectionV, along with the methodologies quantum architecture search (QAS). The objectives of QAS
fornumericalsimulationandthecorrespondingoutcomesdis- may include generating desired quantum states [33]–[43], dis-
cussed in SectionVI. Ultimately, the findings are summarized covering efficient circuits for solving chemical ground states
in SectionVII. [43]–[47], addressing optimization tasks [43], [45], [48]–[52],
optimizingquantumcircuitsforspecifichardwarearchitectures
[53], compiling circuits [54]–[56], or conducting machine
learning tasks [49], [50], [57]–[63]. Various methodologies
are employed to discover the optimal circuit for specific
tasks. For instance, reinforcement learning-based approaches
areexploredinworkssuchas[33]–[36],[38],[40],[41],[44],
[48], [53], [56], while different variants of evolutionary algo-
rithms are utilized in works like [39], [57]–[59] to search for
circuits. Additionally, differentiable QAS methods have been
developed to leverage gradient-based techniques effectively
[50]–[52],[61].Variousapproachestoencodequantumcircuit
architecture have been proposed, with some utilizing graph-
based methods as seen in works such as [46], [49], while
others, like [53], consider convolutional neural network-based
methods.Asforcircuitperformancemetrics,theymayinvolve
directevaluationofcircuitperformanceonspecifictasks[44],
[45], [57], or assessing the proximity of the generated circuit
Fig.1. DifferentiableQASforQRLframework.
to the actual circuit [33], [34], [49]. To alleviate the computa-
tional resources needed for direct evaluation, predictor-based
methods have been proposed, employing neural networks to
II. RELATEDWORK
predict quantum model performance without direct circuit
Quantum reinforcement learning (QRL) has been a sub- evaluation [47], [62].
ject of exploration since the groundbreaking work by Dong The proposed framework in this paper extends the asyn-
et al. in 2008 [27]. Initially, its practicality was hindered chronous training of QRL described in the work [21], [31],
by the requirement to construct environments entirely in a [32] to include the capability of differentiable QAS to search
quantum fashion, thus limiting its real-world applicability. for the best performing circuit. Our work also differentiates
However, subsequent advancements in QRL, leveraging vari- from the previous work [61] as our work considers the
ational quantum circuits (VQCs), have broadened its horizons asynchronous training which can leverage parallel computing
to encompass classical environments with both discrete [23] resource or in the future, multiple-QPU environments. Our
andcontinuousobservationspaces[24],[25].Theevolutionof work is also different from the previous work such as [57]
QRL has witnessed performance improvements through the since our work leverages the differentiable method, unlike the
adoption of policy-based learning methodologies, including evolutionarymethodsrequiringalargeamountofperformance
Proximal Policy Optimization (PPO) [28], Soft Actor-Critic evaluation.
(SAC)[29],REINFORCE[26],AdvantageActor-Critic(A2C)
III. QUANTUMREINFORCEMENTLEARNING
[30], and Asynchronous Advantage Actor-Critic (A3C) [31].
Moreover, in addressing the challenges presented by partially A. Variational Quantum Circuits
observable environments, researchers have explored the uti- Quantummachinelearninglargelydependsonthetrainable
lizationofquantumrecurrentneuralnetworkssuchasquantum quantum circuits: variational quantum circuit (VQC), also
LSTM as RL policies [20], [21]. Recent advancements also known as parameterized quantum circuits (PQC). The VQC,
encompass hybrid models, wherein a classical neural network as shown in Figure 2, usually has three basic components:
istrainedtodynamicallyadjusttheparametersofthequantum encoding circuit U(⃗x), variational circuit V(θ⃗) and the final
circuit. This enables the model to address intricate sequential measurement part. The purpose of encoding circuit U(⃗x)
decision-making tasks without relying on quantum recurrence is to transform the input vector ⃗x into a quantum state
[32]. U(⃗x)|0⟩⊗n, where |0⟩⊗n is the ground state of the quantum
Nevertheless, the accomplishments mentioned in QRL ne- systemandnrepresentsthenumberofthequbit.Theencoded
cessitate profound expertise in designing high-performing state then go through the variational circuit and becomes
quantum circuit architectures to leverage potential quantum V(θ⃗)U(⃗x)|0⟩⊗n. To retrieve the information from the VQC,
advantages.Consequently,thereisanimminentrequirementto measurements can be carried out with pre-defined observ-
develop automated procedures for designing quantum circuit ables Bˆ . The VQC operation can be seen as as quantum
architectures to cater to the demands of various application functionk− f− (− ⃗x− ;→ θ⃗) = (cid:16)(cid:68) Bˆ (cid:69) ,··· ,(cid:68) Bˆ (cid:69)(cid:17) , where (cid:68) Bˆ (cid:69) =
domains. Machine learning techniques have been employed 1 n k
(cid:68) (cid:12) (cid:12) (cid:69) (cid:68) (cid:69)
to address various challenges in quantum computing, such as 0(cid:12)U†(⃗x)V†(θ⃗)Bˆ V(θ⃗)U(⃗x)(cid:12)0 . Expectation values Bˆ
(cid:12) k (cid:12) kcan be obtained by performing multiple samplings (shots) on is an estimation of the value function Vπ(s ). Employing this
t
actual quantum hardware or through direct computation when baseline choice often yields a policy gradient estimate with
utilizing simulation software. reduced variance [64]. The difference R −b = Q(s ,a )−
t t t t
V(s ) can be interpreted as the advantage A(s ,a ) of action
t t t
a at state s . Conceptually, the advantage represents the
t t
relative ”goodness or badness” of action a concerning the
t
average value at state s . This approach is known as the
t
advantage actor-critic (A2C) method. The quantum version
of REINFORCE algorithm with value function baselines is
describedinthework[26].Thework[26]furtherdemonstrates
the advantage of hybrid quantum-classical RL over classical
models on discrete logarithm problem.
The asynchronous advantage actor-critic (A3C) algorithm
Fig.2. GenericstructureofaVQC. [66] extends the A2C approach by employing multiple con-
currentactorstolearnthepolicythroughparallelization.Asyn-
chronoustrainingofRLagentsentailsrunningmultipleagents
B. Quantum RL concurrentlyonvariousinstancesoftheenvironment,enabling
Reinforcement learning (RL) constitutes a ML paradigm them to encounter diverse states at each time step. This
wherein an agent endeavors to achieve a predefined objective reduced correlation between states or observations enhances
or goal through interactions with an environment E within the numerical stability of on-policy RL algorithms like actor-
discrete time intervals [64]. At each time step t, the agent critic[66].Moreover,asynchronoustrainingobviatestheneed
perceivesastates andsubsequentlyselectsanactiona from formaintaininganextensivereplaymemory,therebyreducing
t t
theactionspaceAbasedonitsprevailingpolicyπ.Thepolicy memory requirements [66]. Recent investigations [31] have
signifiesamappingfromaspecificstates totheprobabilities indicated that the quantum version of A3C can yield superior
t
associated with selecting an action from A. Upon executing performance in specific benchmark tasks compared to their
actiona ,theagentreceivesascalarrewardr andtheupdated classical counterparts under certain conditions. Asynchronous
t t
subsequent state s of the environment. For episodic tasks, QRL can facilitate efficient training, leveraging multiple CPU
t+1
this sequence of actions and new states recurs over multiple cores or QPU cores, contingent on whether a simulation
time steps until the agent either reaches a terminal state or backend or a real quantum device is employed.
exhausts the allowable number of steps.
A category of RL training algorithms referred to as policy
gradient methods centers on optimizing the policy function,
represented as π(a|s;θ), which is parameterized by θ. These
parameters, denoted as θ, undergo updates via a gradient
ascent process on the expected total return, E[R ]. In classical
t
RL, the function π(a|s;θ) is realized using a deep neural
network (DNN), with θ representing the DNN’s weights
and biases. In quantum RL, the policy function π(a|s;θ)
can be implemented through VQCs or hybrid models that
integrate both VQCs and conventional DNNs. The integration
of VQCs and DNNs forms a directed acyclic graph (DAG)
and the whole model can be optimized in an end-to-end
manner [20], [22]–[26]. An exemplary instance of a policy
gradient algorithm is the REINFORCE algorithm, initially Fig.3. AnsatzesofVQCconsideredinthiswork.
proposed in [65]. Within the conventional REINFORCE al-
gorithm, the parameters θ undergo updates in the direction
of ∇ logπ(a |s ;θ)R , constituting an unbiased estimate of
IV. DIFFERENTIABLEQUANTUMARCHITECTURESEARCH
θ t t t
∇ E[R ]. Drawing inspiration from classical neural architecture
θ t
Nonetheless,thepolicygradientestimatefrequentlyencoun- search (NAS) [67] and foundational research on differen-
ters high variance, presenting challenges during training. To tiable QAS [51], our DiffQAS approach commences with a
mitigate this variance while preserving its unbiased nature, collection of candidate subcircuits. Suppose we aim to con-
practitioners often subtract a term referred to as the baseline struct a quantum circuit C requiring several sub-components
from the return. This baseline, denoted as b (s ), constitutes S ,S ,··· ,S . Each S is associated with a corresponding
t t 1 2 n i
a learned function of the state s . Consequently, the updated set of allowable circuit choices B , where |B | denotes the
t i i
expression becomes ∇ logπ(a |s ;θ)(R −b (s )). In pol- numberofpermissiblecircuitchoicesforeachsub-component
θ t t t t t
icy gradient RL, a prevalent selection for the baseline b (s ) i.Thus,thetotalnumberofpotentialoutcomesforcircuitC is
t tgiven by N =|B |×|B |×···×|B |. Structural weights w ,
1 2 n j
where j ∈ {1,··· ,N}, are assigned to each possible circuit
realizationC .Additionally,weassumethateachC possesses
j j
its own trainable parameters θ .
j
Consider a ML task for which certain circuit realizations
C , when trained with their corresponding parameters θ , may
j j
offer viable approaches. However, each specific circuit C
j
does not guarantee an optimal solution; it may yield optimal,
suboptimal, or even ineffective outcomes. We introduce the
ensemble function f , defined as the weighted sum of all po-
C
tential circuit realizations, denoted as f
=(cid:80)N
w f . For
C j=1 j Cj
clarity, we omit the notation for quantum circuit parameters
(rotation angles) θ and input vector⃗x. Subsequently, the out-
j
put from the ensemble function f is subjected to processing
C
by the loss function L(f ). Utilizing automatic differentiation
C
algorithms, the gradient with respect to the structural weights
w can be computed as ∇ L(f ). Conventional gradient-
j wj C
basedoptimizerscanthenbeemployedtooptimizetheweights
w j. Fig.4. Quantumfunctionwithdifferentiablestructureweights.
In the proposed DiffQAS framework, VQCs can be as-
sembled from a set of ansatzes, as illustrated in Figure 3.
Specifically, for the encoding circuit U(⃗x), it may incorporate
The value function can then be expressed as Q(⃗s;Θ)=G ◦
η
or exclude the Hadamard gate, followed by the application
F ◦H (⃗s) where η,θ,δ ∈Θ are trainable parameters and G
θ δ
of one of the rotation gates (R , R , and R ). Regarding the
x y z and H are classical functions and F is the quantum function.
variationalcircuitV(θ⃗),twooptionsexistfortheentanglement
ThequantumfunctionF(⃗x;θ)maybecomposedofanarrayof
and three for the parameterized rotation, resulting in six candidate functions f (⃗x;θ ) with trainable structural weights
i i
candidates for both the encoding and variational components. w . It can be expressed as F(⃗x;θ) = (cid:80) w f (⃗x;θ ). The
i i i i
Consequently, for a single VQC, there are 6 × 6 = 36 asynchronous training of the quantum model is presented in
potential configurations. Enumerating all N = 36 feasible Figure5. In conventional quantum A3C [31], the architectures
circuit realizations, we assign structural weights w j to each, ofVQCmodelsremainfixed,withonlythegradientsofquan-
as depicted in Figure4. To meet the requirements of the given tum circuit parameters transmitted to the central storage. In
task,wecanstackmultipleensemblefunctionsf C toconstruct the proposed DiffQAS with A3C, however, both the gradients
deep quantum circuits, as demonstrated in Figure5. It should of quantum circuit parameters and the gradients of structural
benotedthatconstructingdeepquantumcircuitsincreasesthe weights are uploaded.
number of structural weights w . For instance, if we build
j
deep quantum circuits with M circuit blocks, there will be
N×M structural weights, where N represents the number of
possible circuit realizations. The structural weights across all
layers can be optimized using gradient-based optimizers.
V. METHODS
A. Differentiable QAS for QRL
InQRL,certainfunctionssuchasvaluefunctionandpolicy
function would be implemented by hybrid quantum-classical
models.Insidethehybridmodels,thequantumcomponentsare
realizedviaVQCsasdescribedinSectionIII-A.Traditionally,
thearchitectureofVQCisdesignedbeforethemodeltraining
process. While this method has shown several successful
QRL applications [23]–[26], it is with certain limitations. For
example, the designed architecture may be suitable for only
Fig.5. AsynchronousquantumRLwithdifferentiableQAS.
a small set of tasks. And the design of new architecture may
require domain expertise. In this paper, we relax some of the
constraints via defining the hybrid models with the trainable
VI. EXPERIMENTS
structuralweights.ConsiderthevaluefunctionQ(⃗s;Θ),where
⃗s is the state or observation from the environment and Θ is In this study, we utilize the following open-source tools for
thewholeparametersetincludingclassicalandquantumones. simulationpurposes.WeemployPennyLane[68]forquantumcircuit construction and PyTorch for developing the overar- represents the number of valid crossings across walls from
ching hybrid quantum-classical model. The hyperparameters the starting position to the goal.
for the proposed DiffQAS in RL with QA3C training [21],
[31] are set as follows: Adam optimizer with a learning rate TABLEI
of 1 × 10−4, beta = 0.92, beta = 0.999, L = 5 for VQCBASELINES.
1 2
modellookupsteps,andadiscountfactorγ =0.9.Duringthe (cid:97)
(cid:97)
asynchronoustrainingprocess,localagentsormodelscompute (cid:97) (cid:97) VQCconfig
(cid:97)
(cid:97) 1 2 3 4 5 6
their gradients every L steps, corresponding to the trajectory (cid:97)
Component (cid:97) (cid:97)(cid:97)
length used during model updates. The number of parallel
Encoding Ry Rz Rz Ry Rx Rx
processes(numberoflocalagents)is80.Toillustrateboththe
TrainableRotationGate Ry Ry Rz Rz Rz Ry
trend and stability, we present results with the average score
alongside its standard deviation over the past 5,000 episodes.
The standard deviation is shown as the shaded area in the
result plots. We summarize the VQC baselines in the TableI.
Each VQC configuration consists of an 8-qubit system and 2
variational layers, resulting in a total of 16 trainable quantum
parameters (2×8=16).
A. Environment
The MiniGrid environment [69] presents a more complex
scenario, featuring a significantly larger observation input for
the quantum RL agent. In this environment, the RL agent
receives a 7×7×3=147-dimensional vector as observation
input and must select an action from the action space A,
which comprises six options. Notably, the 147-dimensional
vector serves as a compact and efficient representation of
the environment, as opposed to directly representing the real
pixels. In this work, we consider 8-qubit systems, the 147- Fig.6. MiniGridEnvironments.
dimensional vector is transformed into 8-dimensional vectors
using a simple classical linear layer, represented by the H
δ
B. Results
functioninSectionV-A.These8-dimensionalvectorsarethen
processedbytheVQC.Theclassicallinearlayerandtheentire In the environment MiniGrid-Empty-5x5 (shown
DiffQASunits(depictedinFigure4)aretrainedtogetherinan in Figure 7), we can see that the proposed DiffQAS can
end-to-end manner. The action space A comprises six actions reach performance similar to the manually designed models
0,···,5 available for the agent to select. These actions include (Config-1, Config-2 and Config-4). We can also observe that
turnleft,turnright,moveforward,pickupanobject,dropthe our DiffQAS model training is more stable regarding the
object being carried, and toggle. However, it is noteworthy average scores. In addition, our method can outperform the
that only the first three actions have tangible effects in the results from other manually designed models such as Config-
scenarios explored in this study. The agent is tasked with 3, Config-5 and Config-6 with a significant margin. In the
learning this distinction. Within this environment, the agent environment MiniGrid-Empty-6x6 (shown in Figure8),
garners a reward of 1 upon successfully attaining the goal. we can see that the proposed DiffQAS can reach performance
However, a penalty is deducted from this reward following similar to the manually designed models (Config-1, Config-2
the formula 1−0.9×(number of steps/max steps allowed), and Config-4). The proposed DiffQAS model converges
where the maximum permissible step count is delineated as a little bit slower than the manually-crafted Config-1 and
4 × n × n, with n representing the grid size [69]. This Config-2, however, the DiffQAS model has no issue to reach
reward mechanism poses a challenge due to its sparse nature, the optimal score. The slower convergence may due to the
wherein rewards are only dispensed upon goal achievement. larger search for the structural weights such that the program
AsdepictedinFigure6,theagent,denotedbytheredtriangle, requires some additional time to learn these parameters.
is tasked with determining the most direct route from the In addition, our method can outperform the results from
initial position to the designated goal, depicted in green. We other manually designed models such as Config-3, Config-5
consider six cases in this environment: MiniGrid-Empty-5x5- and Config-6 with a significant margin. In the environment
v0(Figure6a),MiniGrid-Empty-6x6-v0(Figure6b),MiniGrid- MiniGrid-Empty-8x8 (shown in Figure 9), we can see
Empty-8x8-v0 (Figure6c), MiniGrid-SimpleCrossingS9N1-v0 that the proposed DiffQAS can reach performance similar
(Figure6d),MiniGrid-SimpleCrossingS9N2-v0(Figure6e)and to the manually designed models (Config-1, Config-2 and
MiniGrid-SimpleCrossingS9N3-v0 (Figure 6f). Here the N Config-4). The proposed DiffQAS model converges a littleFig.9. Results:MiniGrid-Empty-8x8.
Fig.7. Results:MiniGrid-Empty-5x5.
bit slower than the three manually-crafted, however, the
DiffQAS model has no issue to reach the optimal score and
maintain the stability. The slower convergence may due to the
larger search for the structural weights such that the program
requires some additional time to learn these parameters. In
addition, we can observe that the other three manually-crafted
models Config-3, Config-5 and Config-6 fail to learn the
policy at all. The environment MiniGrid-Empty-8x8
is considered to be more difficult than the previous two,
thus it it not surprising that certain models which perform
poorly in previous case fail to learn the policy in this case.
In the environment MiniGrid-SimpleCrossing-S9N1
(shown in Figure10), we can see that the proposed DiffQAS
can reach performance close to the manually designed
models (Config-1, Config-2 and Config-4). The proposed
DiffQAS model converges a bit slower than the three
manually-crafted before the end of 100,000 training episodes.
The slower convergence may due to the larger search for
the structural weights such that the program requires some
additional time to learn these parameters. In addition, we
can observe that the other three manually-crafted models
Config-3, Config-5 and Config-6 fail to learn the policy at all.
The environment MiniGrid-SimpleCrossing-S9N1
is considered to be more difficult than the previous
MiniGrid-Empty environments, thus it it not surprising
that certain models which perform poorly in previous cases
fail to learn the policy in this case. In the environment
Fig.8. Results:MiniGrid-Empty-6x6.
MiniGrid-SimpleCrossing-S9N2 (shown in
Figure 11), we can see that the proposed DiffQAS can
reach performance similar to the best manually designedmodels (Config-2) and beat all other manually-designed
models. The proposed DiffQAS learns a bit slower than the
best manually-crafted in early training episodes but finally
surpass it with higher scores. The slower convergence may
due to the larger search for the structural weights such that
the program requires some additional time to learn these
parameters before reaching optimal architecture weights.
One of the previously good-preforming model (Config-4)
now fails to learn the optimal policy in this more difficult
environment. In addition, we can observe that the other
three manually-crafted models Config-3, Config-5 and
Config-6 fail to learn the policy at all. The environment
MiniGrid-SimpleCrossing-S9N2 is considered to be
more difficult than the previous MiniGrid-Empty and
MiniGrid-SimpleCrossing-S9N1 environments, thus
it it not surprising that certain models which perform poorly
in previous cases fail to learn the policy in this case. In
Fig.10. Results:MiniGrid-SimpleCrossing-S9N1.
Fig.12. Results:MiniGrid-SimpleCrossing-S9N3.
the environment MiniGrid-SimpleCrossing-S9N3
(shown in Figure12), we can see that the proposed DiffQAS
can reach performance similar to the best manually designed
models (Config-4) and beat all other manually-designed
models at the end of training. The proposed DiffQAS
learns a bit slower than the best manually-crafted in early
training episodes but finally reaches the optimal scores. The
observed slower convergence during early training episodes
may due to the larger search for the structural weights
such that the program requires some additional time to
learn these parameters before reaching optimal architecture
Fig.11. Results:MiniGrid-SimpleCrossing-S9N2.
weights. One of the previously good-preforming model in
(Config-2) now fails to learn the optimal policy in this more
difficult environment. In addition, we can observe that theother three manually-crafted models Config-3, Config-5 and [11] S. Y.-C. Chen, D. Fry, A. Deshmukh, V. Rastunkov, and C. Stefanski,
Config-6 fail to learn the policy at all. The environment “Reservoir computing via quantum recurrent neural networks,” arXiv
preprintarXiv:2211.02612,2022.
MiniGrid-SimpleCrossing-S9N3 is considered to be
[12] J. Bausch, “Recurrent quantum neural networks,” Advances in neural
more difficult than the previous MiniGrid-Empty, informationprocessingsystems,vol.33,pp.1368–1379,2020.
MiniGrid-SimpleCrossing-S9N1 and [13] C. Chu, G. Skipper, M. Swany, and F. Chen, “Iqgan: Robust quantum
MiniGrid-SimpleCrossing-S9N2 environments, generativeadversarialnetworkforimagesynthesisonnisqdevices,”in
ICASSP2023-2023IEEEInternationalConferenceonAcoustics,Speech
thus it it not surprising that certain models which perform andSignalProcessing(ICASSP),pp.1–5,IEEE,2023.
poorly in previous cases fail to learn the policy in this case. [14] S. A. Stein, B. Baheri, D. Chen, Y. Mao, Q. Guan, A. Li, B. Fang,
Note that certain manually-designed architectures such as and S. Xu, “Qugan: A quantum state fidelity based generative adver-
sarial network,” in 2021 IEEE International Conference on Quantum
Config-2 and Config-4 cannot perform well consistently
ComputingandEngineering(QCE),pp.71–81,IEEE,2021.
across different environments. This phenomenon further [15] C.-H. H. Yang, J. Qi, S. Y.-C. Chen, P.-Y. Chen, S. M. Siniscalchi,
confirms the requirement to have a systemic and automatic X.Ma,andC.-H.Lee,“Decentralizingfeatureextractionwithquantum
convolutional neural network for automatic speech recognition,” in
way to build VQC architectures.
ICASSP2021-2021IEEEInternationalConferenceonAcoustics,Speech
andSignalProcessing(ICASSP),pp.6523–6527,IEEE,2021.
VII. CONCLUSION [16] S.S.Li,X.Zhang,S.Zhou,H.Shu,R.Liang,H.Liu,andL.P.Garcia,
“Pqlm-multilingualdecentralizedportablequantumlanguagemodel,”in
This paper introduces the DiffQAS-QRL framework, amal- ICASSP2023-2023IEEEInternationalConferenceonAcoustics,Speech
gamating differentiable quantum architecture search (Diff- andSignalProcessing(ICASSP),pp.1–5,IEEE,2023.
[17] C.-H.H.Yang,J.Qi,S.Y.-C.Chen,Y.Tsao,andP.-Y.Chen,“Whenbert
QAS) with quantum reinforcement learning (QRL). Specif-
meets quantum temporal convolution learning for text classification in
ically, we explore the quantum variant of asynchronous heterogeneous computing,” in ICASSP 2022-2022 IEEE International
advantage actor-critic (QA3C) RL, capitalizing on parallel Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp.8602–8606,IEEE,2022.
computing resources to augment training efficiency. Through
[18] R.DiSipio,J.-H.Huang,S.Y.-C.Chen,S.Mangini,andM.Worring,
numerical simulations conducted within diverse testing envi- “Thedawnofquantumnaturallanguageprocessing,”inICASSP2022-
ronments, our proposed DiffQAS-QRL method demonstrates 2022 IEEE International Conference on Acoustics, Speech and Signal
Processing(ICASSP),pp.8612–8616,IEEE,2022.
its capability to identify well-performing VQC architectures
[19] J.Stein,I.Christ,N.Kraus,M.B.Mansky,R.Mu¨ller,andC.Linnhoff-
across various scenarios, surpassing manually designed mod- Popien,“Applyingqnlptosentimentanalysisinfinance,”in2023IEEE
els in certain environments. These findings underscore the International Conference on Quantum Computing and Engineering
(QCE),vol.2,pp.20–25,IEEE,2023.
potentialofDiffQAS-QRLinautonomouslydiscoveringhigh-
[20] S. Y.-C. Chen, “Quantum deep recurrent reinforcement learning,” in
performingVQCarchitecturesforQRLtasks,therebycharting
ICASSP2023-2023IEEEInternationalConferenceonAcoustics,Speech
a novel course in the broader domain of automatic QML. andSignalProcessing(ICASSP),pp.1–5,IEEE,2023.
[21] S. Y.-C. Chen, “Efficient quantum recurrent reinforcement learning
via quantum reservoir computing,” in ICASSP 2024-2024 IEEE In-
REFERENCES
ternational Conference on Acoustics, Speech and Signal Processing
(ICASSP),pp.13186–13190,IEEE,2024.
[1] M. A. Nielsen and I. L. Chuang, Quantum computation and quantum
[22] S. Y.-C. Chen, C.-M. Huang, C.-W. Hsing, H.-S. Goan, and Y.-
information. Cambridgeuniversitypress,2010.
J. Kao, “Variational quantum reinforcement learning via evolutionary
[2] M.Cerezo,A.Arrasmith,R.Babbush,S.C.Benjamin,S.Endo,K.Fujii,
optimization,”MachineLearning:ScienceandTechnology,vol.3,no.1,
J. R. McClean, K. Mitarai, X. Yuan, L. Cincio, et al., “Variational
p.015025,2022.
quantum algorithms,” Nature Reviews Physics, vol. 3, no. 9, pp. 625–
[23] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma, and H.-S.
644,2021.
Goan, “Variational quantum circuits for deep reinforcement learning,”
[3] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea,
IEEEAccess,vol.8,pp.141007–141024,2020.
A.Anand,M.Degroote,H.Heimonen,J.S.Kottmann,T.Menke,etal.,
“Noisy intermediate-scale quantum algorithms,” Reviews of Modern [24] O. Lockwood and M. Si, “Reinforcement learning with quantum vari-
Physics,vol.94,no.1,p.015004,2022. ational circuit,” in Proceedings of the AAAI Conference on Artificial
[4] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, “Quantum circuit
IntelligenceandInteractiveDigitalEntertainment,vol.16,pp.245–251,
learning,”PhysicalReviewA,vol.98,no.3,p.032309,2018. 2020.
[5] S.Y.-C.Chen,C.-M.Huang,C.-W.Hsing,andY.-J.Kao,“Anend-to- [25] A. Skolik, S. Jerbi, and V. Dunjko, “Quantum agents in the gym: a
end trainable hybrid classical-quantum classifier,” Machine Learning: variational quantum algorithm for deep q-learning,” Quantum, vol. 6,
ScienceandTechnology,vol.2,no.4,p.045021,2021. p.720,2022.
[6] S. Y.-C. Chen, T.-C. Wei, C. Zhang, H. Yu, and S. Yoo, “Quantum [26] S. Jerbi, C. Gyurik, S. Marshall, H. Briegel, and V. Dunjko,
convolutional neural networks for high energy physics data analysis,” “Parametrized quantum policies for reinforcement learning,” Advances
PhysicalReviewResearch,vol.4,no.1,p.013231,2022. in Neural Information Processing Systems, vol. 34, pp. 28362–28375,
[7] S.Oh,J.Choi,andJ.Kim,“Atutorialonquantumconvolutionalneural 2021.
networks(qcnn),”in2020InternationalConferenceonInformationand [27] D. Dong, C. Chen, H. Li, and T.-J. Tarn, “Quantum reinforcement
Communication Technology Convergence (ICTC), pp. 236–239, IEEE, learning,” IEEE Transactions on Systems, Man, and Cybernetics, Part
2020. B(Cybernetics),vol.38,no.5,pp.1207–1220,2008.
[8] J.Qi,C.-H.H.Yang,andP.-Y.Chen,“Qtn-vqc:Anend-to-endlearning [28] J.-Y. Hsiao, Y. Du, W.-Y. Chiang, M.-H. Hsieh, and H.-S. Goan,
frameworkforquantumneuralnetworks,”PhysicaScripta,vol.99,12 “Unentangled quantum reinforcement learning agents in the openai
2023. gym,”arXivpreprintarXiv:2203.14348,2022.
[9] J. Wu and Q. Li, “Poster: Scalable quantum convolutional neural [29] Q. Lan, “Variational quantum soft actor-critic,” arXiv preprint
networks for edge computing,” in 2022 IEEE/ACM 7th Symposium on arXiv:2112.11921,2021.
EdgeComputing(SEC),pp.307–309,IEEE,2022. [30] M. Ko¨lle, M. Hgog, F. Ritz, P. Altmann, M. Zorn, J. Stein, and
[10] S. Y.-C. Chen, S. Yoo, and Y.-L. L. Fang, “Quantum long short-term C.Linnhoff-Popien,“Quantumadvantageactor-criticforreinforcement
memory,” in ICASSP 2022-2022 IEEE International Conference on learning,”arXivpreprintarXiv:2401.07043,2024.
Acoustics, Speech and Signal Processing (ICASSP), pp. 8622–8626, [31] S.Y.-C.Chen,“Asynchronoustrainingofquantumreinforcementlearn-
IEEE,2022. ing,”ProcediaComputerScience,vol.222,pp.321–330,2023.Interna-tionalNeuralNetworkSocietyWorkshoponDeepLearningInnovations [55] Z. He, J. Su, C. Chen, M. Pan, and H. Situ, “Search space pruning
andApplications(INNSDLIA2023). forquantumarchitecturesearch,”TheEuropeanPhysicalJournalPlus,
[32] S.Y.-C.Chen,“Learningtoprogramvariationalquantumcircuitswith vol.137,no.4,p.491,2022.
fastweights,”arXivpreprintarXiv:2402.17760,2024. [56] Q. Chen, Y. Du, Q. Zhao, Y. Jiao, X. Lu, and X. Wu, “Efficient
[33] E.-J. Kuo, Y.-L. L. Fang, and S. Y.-C. Chen, “Quantum ar- andpracticalquantumcompilertowardsmulti-qubitsystemswithdeep
chitecture search via deep reinforcement learning,” arXiv preprint reinforcementlearning,”arXivpreprintarXiv:2204.06904,2022.
arXiv:2104.07715,2021. [57] L. Ding and L. Spector, “Evolutionary quantum architecture search
[34] E. Ye and S. Y.-C. Chen, “Quantum architecture search via continual for parametrized quantum circuits,” in Proceedings of the Genetic
reinforcementlearning,”arXivpreprintarXiv:2112.05779,2021. andEvolutionaryComputationConferenceCompanion,pp.2190–2195,
[35] T.Kimura,K.Shiba,C.-C.Chen,M.Sogabe,K.Sakamoto,andT.So- 2022.
gabe, “Quantum circuit architectures via quantum observable markov [58] A. Zhang and S. Zhao, “Evolutionary-based searching method for
decisionprocessplanning,”JournalofPhysicsCommunications,vol.6, quantumcircuitarchitecture,”QuantumInformationProcessing,vol.22,
no.7,p.075006,2022. no.7,p.283,2023.
[36] T.Sogabe,T.Kimura,C.-C.Chen,K.Shiba,N.Kasahara,M.Sogabe, [59] L.DingandL.Spector,“Multi-objectiveevolutionaryarchitecturesearch
andK.Sakamoto,“Model-freedeeprecurrentq-networkreinforcement forparameterizedquantumcircuits,”Entropy,vol.25,no.1,p.93,2023.
learning for quantum circuit architectures design,” Quantum Reports, [60] O. Subasi, “Toward automated quantum variational machine learning,”
vol.4,no.4,pp.380–389,2022. arXivpreprintarXiv:2312.01567,2023.
[37] X. Lu, K. Pan, G. Yan, J. Shan, W. Wu, and J. Yan, “Qas-bench: re- [61] Y. Sun, Y. Ma, and V. Tresp, “Differentiable quantum architecture
thinkingquantumarchitecturesearchandabenchmark,”inInternational searchforquantumreinforcementlearning,”in2023IEEEInternational
ConferenceonMachineLearning,pp.22880–22898,PMLR,2023. Conference on Quantum Computing and Engineering (QCE), vol. 2,
[38] A.Kundu,P.Bedełek,M.Ostaszewski,O.Danaci,Y.J.Patel,V.Dunjko, pp.15–19,IEEE,2023.
andJ.A.Miszczak,“Enhancingvariationalquantumstatediagonaliza- [62] S.-X. Zhang, C.-Y. Hsieh, S. Zhang, and H. Yao, “Neural predictor
tionusingreinforcementlearningtechniques,”NewJournalofPhysics, based quantum architecture search,” Machine Learning: Science and
vol.26,no.1,p.013034,2024. Technology,vol.2,no.4,p.045027,2021.
[39] L.Su¨nkel,D.Martyniuk,D.Mattern,J.Jung,andA.Paschke,“Ga4qco: [63] Y. Du, T. Huang, S. You, M.-H. Hsieh, and D. Tao, “Quantum circuit
genetic algorithm for quantum circuit optimization,” arXiv preprint architecture search for variational quantum algorithms,” npj Quantum
arXiv:2302.01303,2023. Information,vol.8,no.1,p.62,2022.
[40] X. Zhu and X. Hou, “Quantum architecture search via truly proximal [64] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction.
policyoptimization,”ScientificReports,vol.13,no.1,p.5157,2023. MITpress,2018.
[41] S. Y.-C. Chen, “Quantum reinforcement learning for quantum archi- [65] R. J. Williams, “Simple statistical gradient-following algorithms for
tecturesearch,”inProceedingsofthe2023InternationalWorkshopon connectionistreinforcementlearning,”Machinelearning,vol.8,no.3-4,
QuantumClassicalCooperative,pp.17–20,2023. pp.229–256,1992.
[42] P. Selig, N. Murphy, D. Redmond, and S. Caton, “Deepqprep: Neural [66] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
networkaugmentedsearchforquantumstatepreparation,”IEEEAccess, D.Silver,andK.Kavukcuoglu,“Asynchronousmethodsfordeeprein-
2023. forcement learning,” in International conference on machine learning,
[43] Y.Sun,Z.Wu,Y.Ma,andV.Tresp,“Quantumarchitecturesearchwith pp.1928–1937,PMLR,2016.
unsupervisedrepresentationlearning,”arXivpreprintarXiv:2401.11576, [67] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture
2024. search,”arXivpreprintarXiv:1806.09055,2018.
[44] M. Ostaszewski, L. M. Trenkwalder, W. Masarczyk, E. Scerri, and [68] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, C. Blank, K. McK-
V.Dunjko,“Reinforcementlearningforoptimizationofvariationalquan- iernan, and N. Killoran, “Pennylane: Automatic differentiation of hy-
tum circuit architectures,” Advances in Neural Information Processing bridquantum-classicalcomputations,”arXivpreprintarXiv:1811.04968,
Systems,vol.34,pp.18182–18194,2021. 2018.
[45] P.Wang,M.Usman,U.Parampalli,L.C.Hollenberg,andC.R.Myers, [69] M. Chevalier-Boisvert, L. Willems, and S. Pal, “Minimalistic grid-
“Automatedquantumcircuitdesignwithnestedmontecarlotreesearch,” world environment for openai gym.” https://github.com/maximecb/
IEEETransactionsonQuantumEngineering,2023. gym-minigrid,2018.
[46] Z. He, X. Zhang, C. Chen, Z. Huang, Y. Zhou, and H. Situ, “A gnn-
basedpredictorforquantumarchitecturesearch,”QuantumInformation
Processing,vol.22,no.2,p.128,2023.
[47] M. Deng, Z. He, S. Zheng, Y. Zhou, F. Zhang, and H. Situ, “A
progressive predictor-based quantum architecture search with active
learning,”TheEuropeanPhysicalJournalPlus,vol.138,no.10,p.905,
2023.
[48] J. Yao, H. Li, M. Bukov, L. Lin, and L. Ying, “Monte carlo tree
search based hybrid optimization of variational quantum circuits,” in
MathematicalandScientificMachineLearning,pp.49–64,PMLR,2022.
[49] T.Duong,S.T.Truong,M.Pham,B.Bach,andJ.-K.Rhee,“Quantum
neural architecture search with quantum circuits metric and bayesian
optimization,”inICML20222ndAIforScienceWorkshop,2022.
[50] W. Wu, G. Yan, X. Lu, K. Pan, and J. Yan, “Quantumdarts: differen-
tiablequantumarchitecturesearchforvariationalquantumalgorithms,”
in International Conference on Machine Learning, pp. 37745–37764,
PMLR,2023.
[51] S.-X. Zhang, C.-Y. Hsieh, S. Zhang, and H. Yao, “Differentiable
quantumarchitecturesearch,”QuantumScienceandTechnology,vol.7,
no.4,p.045023,2022.
[52] Y. Sun, J. Liu, Y. Ma, and V. Tresp, “Differentiable quantum ar-
chitecture search for job shop scheduling problem,” arXiv preprint
arXiv:2401.01158,2024.
[53] T. Fo¨sel, M. Y. Niu, F. Marquardt, and L. Li, “Quantum cir-
cuit optimization with deep reinforcement learning,” arXiv preprint
arXiv:2103.07585,2021.
[54] Z. He, C. Chen, L. Li, S. Zheng, and H. Situ, “Quantum architecture
search with meta-learning,” Advanced Quantum Technologies, vol. 5,
no.8,p.2100134,2022.