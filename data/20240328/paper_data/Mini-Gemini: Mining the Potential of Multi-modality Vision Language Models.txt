Mini-Gemini: Mining the Potential of Multi-modality
Vision Language Models
YanweiLi1∗ YuechenZhang1∗ ChengyaoWang1∗ ZhishengZhong1
YixinChen1 RuihangChu1 ShaotengLiu1 JiayaJia1,2
TheChineseUniversityofHongKong1 SmartMore2
Abstract
Inthiswork,weintroduceMini-Gemini,asimpleandeffectiveframeworkenhanc-
ingmulti-modalityVisionLanguageModels(VLMs). Despitetheadvancements
inVLMsfacilitatingbasicvisualdialogandreasoning,aperformancegappersists
comparedtoadvancedmodelslikeGPT-4andGemini. Wetrytonarrowthegap
byminingthepotentialofVLMsforbetterperformanceandany-to-anyworkflow
fromthreeaspects,i.e.,high-resolutionvisualtokens,high-qualitydata,andVLM-
guidedgeneration. Toenhancevisualtokens,weproposetoutilizeanadditional
visualencoderforhigh-resolutionrefinementwithoutincreasingthevisualtoken
count. We further construct a high-quality dataset that promotes precise image
comprehensionandreasoning-basedgeneration,expandingtheoperationalscope
ofcurrentVLMs.Ingeneral,Mini-GeminifurtherminesthepotentialofVLMsand
empowerscurrentframeworkswithimageunderstanding,reasoning,andgeneration
simultaneously. Mini-GeminisupportsaseriesofdenseandMoELargeLanguage
Models(LLMs)from2Bto34B.Itisdemonstratedtoachieveleadingperformance
inseveralzero-shotbenchmarksandevensurpassesthedevelopedprivatemodels.
Codeandmodelsareavailableathttps://github.com/dvlab-research/MiniGemini.
WhatshouldIdoafterkneadingthe Feature 13-inch MacBook Pro 15-inch MacBook Pro
dough,beforedividingitintopieces?
Processor Intel Core i5 Intel Core i7
Put in warm place. Graphics Intel Iris Graphics 6100 Intel Iris Pro Graphics
!Mining… HowlongshouldIputinwarmplace? Battery Life Up to 10 hours Up to 8 hours
Trackpad Force Touch trackpad Multi-Touch trackpad
1h. Speed 2.7GHz, 2.9GHz 2.2GHz, 2.5GHz
Memory 8GB 16GB
Storage 128GB, 256GB, 512GB 256GB, 512GB
Step 3 4 5 L i omi ns a et gt ehe ind ti wff oer ce on lc ue mo nf st aw no dc co om mp pu at re ers oi nn eth bi ys Price $1,688.00, $1,988 H.0 ig0,
h
$ -2 R,3 e8 s8 o.0 l0 ution$2 U,6 n88 d. e00 r,
s
$ t3 a, n28 d8 i. n0 g0
LLaVA-1.5baseline Give me an advice where to
MMMU travel to China.
(test)
44.9 Chinaisavastanddiverse
44.7 countrywithmanydifferent
regionstoexplore.Somepop-
ulardestinationsfortravelers
includeBeijing,Shanghai,Xi’-
MM 6-V 4e .3t 59.3 57.4 19281933 (16M 2 51 9M 4 /4E 1 82) a j g ti on ln i rmg, yC i p as nsh t e de hn ce ig n ud c t lo tu a u, p C ri ea t ha …n il nd a aG n 'sdu ril o ii cn f hf. eB r hse isi a- - P a grrle t awa ss o se r lak s , n h b do u bw t ai m n c kte h g a e r os uim ni dla .r
75.2
69.5
74.1 79.6
TextVQA MMB
74.6 80.6 Explain why this meme is
LLaVA-NeXT Mini-Gemini Gemini Pro f p wu i ecn t en u ky r e, e a n wn dd h ceg one m n th ie nr e ga .te a S coh uo lw d m me a ko en e w i id thea t ho if s w .hat I
BenchmarkPerformance GenerationwithReasoning
Figure1: Mini-Geminiisadvancedinvariousvision-relatedtasks.
∗equalcontribution
4202
raM
72
]VC.sc[
1v41881.3042:viXra1 Introduction
With the rapid evolution in Large Language Models (LLMs) [1–3], empowering the impressive
capabilities for multi-modality inputs is becoming an essential part of current Vision Language
Models(VLMs)[4,5]. Tobridgethemodalitygap,severalstudiesareconductedtomarryvision
with LLMs from images [6–8] to videos [9, 10]. Despite these advancements, a significant gap
remainsbetweenacademicinitiativesandtheprowessofwell-establishedmodelslikeGPT-4[4]and
Gemini[5],whicharetrainedwithhugeamountsofdataandresources.
Forvisionitself,imageresolutionisacorepartofexplicitlydespitethesurroundingenvironment
with minimal visual hallucination. To this end, more attempts are performed to further improve
thevisualunderstandingincurrentVLMs. Forinstance,LLaVA-Next[11]andOtter-HD[12]are
proposedtoenhancetheabilitybasedonpreviouswork[7,13]byimprovingtheimageresolution.
Increasingthenumberofvisualtokenswithhigherresolutionimagesundeniablyenrichesvisual
embeddingsinLLMs. However,thisimprovementcomeswithescalatedcomputationaldemands
and associated costs, particularly when processing multiple images. Moreover, the existing data
quality,modelcapabilities,andapplicationscopesremaininadequateforacceleratedtrainingand
developmentprocesses. Thisscenariopromptsacriticalinquiry: howtopushforwardtheVLMs
approachingwell-developedmodelswithacceptablecostinanacademicsetting?
Toanswerthisquestion,weexplorethepotentialofVLMsfromthreestrategicaspects,i.e.,efficient
high-resolutionsolution,high-qualitydata,andexpandedapplications. Firstly,weutilizeConvNet
toefficientlygeneratehigher-resolutioncandidates,thusenhancingvisualdetailwhilemaintaining
the visual token count for LLMs. To bolster data quality, we amalgamate high-quality datasets
fromdiversepublicsources,ensuringarichandvarieddatafoundation. Furthermore,ourapproach
integratestheseenhancementswithcutting-edgeLLMsandgenerativemodels,aimingtoelevate
VLMperformanceanduserexperience. Thismultifacetedstrategyenablesustodelvedeeperintothe
capabilitiesofVLMs,achievingsignificantadvancementswithinmanageableresourceconstraints.
Ingeneral,ourmethodemploysanany-to-anyparadigm,whichisadeptathandlingbothimageand
textasinputandoutput. Inparticular,weintroduceanefficientvisualtokenenhancementpipelinefor
inputimages,featuringadual-encodersystem. Itcomprisestwinencoders,oneforhigh-resolution
imagesandtheotherforlow-resolutionvisualembedding,mirroringthecooperativefunctionality
of the Gemini constellation. During inference, they work in an attention mechanism, where the
low-resolutiononegeneratesvisualqueries,andthehigh-resolutioncounterpartprovidescandidate
keysandvaluesforreference. Toaugmentthedataquality,wecollectandproducemoredatabased
onpublicresources,includinghigh-qualityresponses[14,15],task-orientedinstructions[16–19],and
generation-relateddata[20,21]. Theincreasedamountandqualityimprovetheoverallperformance
andextendthecapability ofmodel. Additionally, ourmodel supportsconcurrentimage andtext
generation,facilitatedbytheseamlessintegrationofourVLMwithadvancedgenerativemodels[22].
ItleveragesVLMguidanceforimagegenerationbyprovidingthegeneratedtextfromLLMs.
The Mini-Gemini framework, can be easily instantiated with a range of LLMs from 2B to 34B
parameter scales, as detailed elaborated in Section 3. Extensive empirical studies are conducted
inSection4torevealtheeffectivenessoftheproposedmethod. Remarkably,ourapproachattains
leading performance in various settings and even surpasses the well-developed Gemini Pro [5],
Qwen-VL-Plus[23],andGPT4V[4]inthecomplexMMB[24]andMMU[25]dataset,respectively.
These results underscore Mini-Gemini’s potential to set new benchmarks in the realm of VLMs,
highlightingitsadvancedcapabilitiesinhandlingcomplexmulti-modaltasks.
2 RelatedWork
LargeLanguageModels. RecentprogressinNaturalLanguageProcessing(NLP)hasbeendra-
maticallyacceleratedbyadvancementsinlargelanguagemodels(LLMs). Theseminalintroduction
oftheTransformerframework[26]servedasacornerstone,enablinganewwaveoflanguagemodels
suchasBERT[27]andOPT[2]thatexhibitprofoundlinguisticunderstanding. Theinceptionofthe
GenerativePre-trainedTransformer(GPT)[28]introducedanovelparadigmthroughauto-regressive
languagemodeling,establishingarobustmethodforlanguagepredictionandgeneration. Theemer-
genceofmodelslikeChatGPT[1],GPT-4[4],LLaMA[3],andMixtral[29]furtherexemplified
the field’s rapid evolution, each demonstrating enhanced performance on complex language pro-
2VisionInput UserTextInput
! Mined TextInput This
X"
HRVision
X′" H WR inR de og wion Tokens …Tokens
…
i dm ea pg ice
ts
Encoder …
HR HR ! Region-wise ,$(&) ,% (.) O Tu etp xtut
Downsample D Eu na cl oV dis ei ro sn FeatureMap Flatten LLM
Flatten …
X# LRVision X′# SDXL
LR Encoder LR *(&) !&# (&… ×(!) AtC tero ns tis on GeneratedTokens (optional)
VisualEmbedding PatchInfoMining Auto-regressiveGeneration O Imu atp gu et
HRFlow LRFlow LanguageFlow
Figure2: TheframeworkofMini-Geminiwithany-to-anyworkflow.
cessingtasks,attributabletotheirtrainingonextensivetextualdatasets. Instructiontuning[30,31]
has emerged as a key technique for refining the output of pre-trained LLMs, as evidenced by its
applicationinthedevelopmentofopen-sourcemodelssuchasAlpaca[32]andVicuna[33]. They
iterateontheLLaMA[3]withcustominstructionsets. Additionally,theintegrationofLLMswith
specifictoolsforvisualtasks[34,35]highlightstheiradaptabilityandpotentialforbroadapplication,
underscoringtheutilityofLLMsinextendingbeyondtraditionaltext-basedprocessingtoinclude
multimodalinteractions. Inthiswork,wetakeseveralpre-trainedLLMs[36,3,29]asbenchmarks
andbuildmulti-modalityframeworksuponthemtofurtherextendtheimpressivereasoningability.
VisionLanguageModels. Theconvergenceofcomputervisionandnaturallanguageprocessinghas
givenrisetoVLMs,whichmarryvisualandlinguisticmodelstoachievecross-modalcomprehension
andreasoningcapabilities. Thisintegrationhasbeenpivotalinadvancingtasksthatrequireboth
visualunderstandingandlanguageprocessing,asevidencedbymodelstrainedondiversedatasets
for understanding [37] and reasoning [16, 38, 39]. Groundbreaking models such as CLIP [40]
havefurtherbridgedthegapbetweenlanguagemodelsandvisiontasks,showcasingthefeasibility
ofcross-modalapplications. Recentdevelopmentsunderscoreagrowingtrendtowardleveraging
the robust capabilities of LLMs within the realm of VLMs. Innovations like Flamingo [41] and
BLIP-2 [6] have capitalized on massive collections of image-text pairs to fine-tune cross-modal
alignment,significantlyboostinglearningefficiency. Buildingupontheseadvancements,several
models[42,8]havefocusedongeneratinghigh-qualityinstructionaldatabasedonBLIP-2,leadingto
markedimprovementsinperformance. Furthermore,LLaVA[7,43]adoptsasimplelinearprojector
tofacilitateimage-textspacealignmentwithminimallearnableparameters. Itleveragestailored
instructiondataandexemplifiesanefficientstrategythatdemonstratesthemodel’spotentcapabilities.
Differentfromthem,weaimtoexplorethepotentialforbothcomprehensionandgeneration.
LLMasGenerationAssistant. CombiningLLMswithimageoutputshasemergedasapivotal
area in recent multimodal research. Methods like InternLM-XComposer [44, 45] utilize image
retrievaltoproduceinterleavedtextandimageoutputs, bypassingdirectgeneration. Conversely,
auto-regressive token prediction approaches, exemplified by EMU [46, 47] and SEED [48, 49],
enableLLMstodecodeimagesthroughmassiveimage-textdatadirectly. Thesemethodsrequire
enormoustrainingresources,andtheirauto-regressivenatureleadstoundesirablelatency. Recent
studies [50–52] strive to align with latent diffusion models [22] to streamline image generation.
Theytypicallyrequiredesigningtextembeddingsandadditionaloptimizationtoachievethedesired
generationeffect. ThisjointtrainingcancompromisetheperformanceofVLMsintextgeneration.
Mini-Gemini distinguishes itself by adopting a text-data-driven approach to enable the model to
generatehigh-qualityimages. Weleverageamere13KpuretextdatatoactivatetheLLM’sabilityas
ahigh-qualityre-captioner[53]withoutunderminingthefundamentalperformanceofVLMs.
3 Mini-Gemini
TheframeworkofMini-Geminiisconceptuallysimple: dualvisionencodersareutilizedtoprovide
low-resolutionvisualembeddingandhigh-resolutioncandidates;patchinfominingisproposedto
conductpatch-levelminingbetweenhigh-resolutionregionsandlow-resolutionvisualqueries;LLM
isutilizedtomarrytextwithimagesforbothcomprehensionandgenerationatthesametime.
3
Generation?X′! (%×'")
! V !×##×1
Q K !×##×1 LR(0,0) LR(0,1) HR 2x
HR
LR
FeatureMap
ImageSplit
X′# (%)
Matmul LR(1,0) LR(1,1) 4$
!×##×1
X′"
LR
LR LR $ 5$
VisualEmbedding In-patchsimilarity )$(%) RawImage !!(5$)
(a)Detailsinpatchinfomining. (b)Detailsinvisualtokenextension.
Figure3: Moredetailsinpatchinfominingandvisualtokenextension.
3.1 DualVisionEncoders
In the Mini-Gemini framework, both text and image inputs can be processed, with the option
tohandlethemindividuallyorincombination. Forillustrativeclarity,weconsidertheconcurrent
processingofbothmodalities. AsdepictedinFigure2,theprocessingbeginswithahigh-resolution
image X ∈ RH×W×3, from which a corresponding low-resolution image X ∈ RH′×W′×3 is
H L
generated via bilinear interpolation, ensuring H′ ≤ H. Then, we process them and encode into
multi-gridvisualembeddingsintwoparallelimageflows. Inparticular,forthelow-resolution(LR)
flow,wemaintainthetraditionalpipeline[42,7]andemployaCLIP-pretrainedViT[40]toencode
thevisualembeddingX′ ∈RN×C,whereN denotesthenumberofvisualpatches. Inthisway,the
L
long-rangerelationamongN visualpatchescanbewellpreservedforsubsequentinteractioninLLMs.
Asforthehigh-resolution(HR)flow,weadopttheCNN-basedencoderforadaptiveandefficientHR
imageprocessing. Forinstance,toalignwiththeLRvisualembedding,theLAION-pretrained[54]
ConvNeXt[55]isusedtoserveasanHRvisionencoder. Therefore,wecanobtaintheHRfeature
mapX′ ∈RN′×N′×C byupsamplingandconcatenatingthefeaturesfromdifferentconvolutional
H
stagesto1/4inputscale. Here,N′ =H/4×W/4=N ×M2denotesthenumberofHRfeatures,
whereM reflectsthepixel-wisefeaturecountwithineachHRsegment,asillustratedinFigure2.
3.2 PatchInfoMining
WiththeabovegeneratedLRembeddingX′ andHRfeatureX′ ,weproposepatchinfominingto
L H
extendthepotentialofVLMswithenhancedvisualtokens. Inparticular,tomaintainthenumberof
finalvisualtokensforefficiencyinLLMs,wetakethelow-resolutionvisualembeddingX′ asquery
L
Q∈RN×C,aimingtoretrieverelevantvisualcuesfromHRcandidate. Meanwhile,theHRfeature
mapX′ istakenaskeyK ∈RN×M2×C andvalueV ∈RN×M2×C,asdepictedinFigure2. Here,
H
thelow-resolutionpatchinQcorrelateswithacorrespondinghigh-resolutionsub-regioninK andV,
encompassingM2pixel-wisefeatures. Therefore,thepatchinfominingprocesscanbeformulatedas
T =MLP(Q+Softmax(ϕ(Q)×ϕ(K)T)×ϕ(V)), (1)
V
whereϕandMLPindicateaprojectionlayerandamulti-layerperceptron,respectively. Aspresented
in Figure 3a, this formula encapsulates the process of synthesizing and refining the visual cues,
leadingtogenerationofenhancedvisualtokensT forsubsequentLLMprocessing. Itensuresthat
V
theminingforeachqueryisconfinedtoitscorrespondingsub-regioninX′ withM2features,thus
H
preservingefficiency.ThisdesignallowsfortheextractionofHRdetailswithoutexpandingthevisual
tokencountofT ,maintainingabalancebetweenrichnessofdetailandcomputationalfeasibility.
V
Furthermore,visualtokenextensionisalsosupportedinthedesignedpatchinfomining. Asdepicted
in Figure 3b, we can extend the visual token to 5N to capture more details. This is achieved by
incorporatingtheoriginalimagealongwithits2×upscaledcounterpart,resultinginabatchedinput
X ∈R5×H′×W′×3. AndwecangettheencodedvisualembeddingX′ ∈R5×N×C withtheLR
L L
visionencoder,asdetailedinSection3.1. ThankstotheflexibledesignofCNN-basedHRvision
encoder,itcanadeptlyhandletheaugmentedvisualtokencountduringthepatchinfomining. The
onlydifferenceintheaforementionedprocedureisthesub-regioninX′ shouldbechangedaccording
H
totheexpandedvisualembeddingX′. WecanalsoupsampletheHRinputtobettersupportthe
L
higherresolutionifneeded,asexperimentallyanalyzedinTable2.
4
LRVisionEncoder Flatten
Concat
PatchInfoMiningRealImageCaption(GPT4-V) References: HighQualityChat(LIMA,Oasst) High-QualityT2IPromptIn-ContextExamples
References: InstructionIn-ContextExamples Instruction:I am making mayonnaise, it was starting to thicken but now it has become runny and liquid
High-QualityT2IPromptIn-ContextExamples again, is there any way to salvage it? <GEN>
Assistant:Yes, it's possible to fix runny mayonnaise! The most common
Instruction:Capture a playful child pre- reason for mayonnaise becoming runny is because the oil was added too
tending to talk on the phone. <GEN> quicklyor the egg yolk wasn't emulsified properly. Here are some steps
you can take to fix it:
Assistant: <h> Young boy with a bright 1. Separate another egg yolk and place it in a clean, dry bowl.
smile, mimicking a phone call, dressed in a 2. Slowly add the runny mayonnaise…
grey sweater and colorful tie, white
background, no text, advertisement style, <h> Serene kitchen scene with a broken egg yolk in a bowl, whisk at the
clean and simple. </h> ready, and a jar of golden liquid nearby, symbolizing the process of
GenbyGPT-API emulsification, soft natural lighting </h> GenbyGPT-API
Figure4: Twotypesofourpure-textdataareusedforimagegeneration. Left: Simpleinstruction
re-captionand Right:In-contextpromptgeneration. SDXLgeneratesimageswiththeoutputprompt.
3.3 TextandImageGeneration
With the mined visual tokens T and input text tokens T , we concatenate them as the input
V T
toLLMsforauto-regressivegeneration, aspresentedinFigure2. Distinguishedfromtraditional
VLMs[42,43,11],theproposedMini-Geminisupportsbothtext-onlyandtext-imagegeneration
as input and output, i.e., any-to-any inference. Despite the image comprehension, we anchor
Mini-Gemini’sabilitytogenerateimagesonitsoutstandingimage-textunderstandingandreasoning
capabilities.Unlikerecentworks[50–52,47],whichaddressthedomaingapbetweentextembeddings
ofLLMsandgenerationmodels,wechoosetooptimizethegapinthedomainoflanguageprompts.
Precisely,Mini-Geminitranslatesuserinstructionsintohigh-qualitypromptsthatproducecontext-
relevantimagesinlatentdiffusionmodels[22,56]. Thisapproachisreflectedinsubsequenthigh-
qualityimagegenerationframeworks,suchasDALLE3[53]andSORA[57],whichleveragethe
generation and understanding capabilities of VLMs to obtain higher-quality text conditions for
generationtasks.
Text-imageInstructions. Forbettercross-modalityalignmentandinstructionfinetuning,wecollect
high-qualitydatasetsfrompubliclyavailablesources. Inparticular,forcross-modalityalignment,we
utilize558Kimage-captionpairsfromtheLLaVA-filteredCC3Mdataset[58]and695Ksampled
GPT-4V-respondedcaptionsfromtheALLaVAdataset[15]. Itbringsabout1.2Mimagecaptionsin
totalforprojectorpretraining. Asforinstructionfinetuning,wesample643Ksingle-andmulti-turn
conversations(excluding21KTextCaps[59]data)fromtheLLaVA[43]dataset,100KQApairs
fromShareGPT4V[14],10KLAION-GPT-4V[60]captions,700KGPT-4V-respondedinstruction
pairsfromALLaVAdataset[15],and6Ktext-onlymulti-turnconversationsfromLIMA[20]and
OpenAssistant2 [21]. To bolster the OCR-related abilities, we further collect 28K QA pairs that
comprise10KDocVQA[17],4KChartQA[18],10KDVQA[61],and4KAI2D[19]data.Ingeneral,
thereareabout1.5Minstruction-relatedconversationsforimagecomprehension. Moreover,wealso
collect13Kpairsforimage-relatedgenerationthatwillbeelaboratedonsubsequently.
Generation-related Instructions. To support image generation, we further construct a 13K
instruction-following dataset using GPT-4 Turbo. As depicted in Figure 4, the training data en-
compassestwotasks: (a)Simpleinstructionre-caption: weadopt8Kdescriptiveimagecaptions
fromLAION-GPT-4V[60]andletGPT-4inverselyinferthecorrespondinguser’sshortinputandthe
targetcaptionintheStableDiffusion(SD)domain. (b)In-contextpromptgeneration: basedonafew
high-qualityreal-worldconversationcontextsinLIMA[20]andOpenAssistant2[21],wegenerate
promptsthatproduceimagessuitablefortheconversationcontext,bringing5Kinstructionsintotal.
Forbothkindsofdata,ineachquerytoGPT-4,werandomlysample5high-qualitySDtext-to-image
promptsfromGigaSheet[62]asin-contextexamplestoobtaintargetpromptsforgeneration. We
formatourdatatouse<GEN>asatriggertoinitiatethegenerationprocessandwrapthetargetcaption
within <h>...</h>. Following text generation, Mini-Gemini extracts target captions and utilizes
SDXL[22]togeneratethecorrespondingimage. MoredetailsarediscussedinAppendixA.
4 Experiments
Inthissection,wefirstoutlineourexperimentalframework,commencingwiththeexperimentalsetup.
Subsequently,wecompareMini-Geminiwithleadingmethodsonvariousbenchmarks. Component-
wiseanalysisandqualitativeresultsaregivenattheendofthissection.
5Table1: Comparisonwithleadingmethodsonzero-shotbenchmarks. ∗and†denoteimagesintrain
subsetareincludedandthedataisnotpubliclyavailable,respectively. Ourresultsaremarkedwith■.
Method LLM Res. VQAT MMB MME MM-Vet MMMUv MMMUt MathVista
Normalresolutionsetting
MobileVLM[63] MLLaMA2.7B 336 47.5 59.6 1289 – – – –
InstructBLIP[42] Vicuna-7B 224 50.1 36.0 – 26.2 – – 25.3
InstructBLIP[42] Vicuna-13B 224 50.7 – 1213 25.6 – – –
Qwen-VL†[23] Qwen-7B 448 63.8∗ 38.2 – – – – –
Qwen-VL-Chat†[23] Qwen-7B 448 61.5∗ 60.6 1488 – 35.9 32.9 –
Shikra[64] Vicuna-13B 224 – 58.8 – – – – –
IDEFICS-80B[65] LLaMA-65B 224 30.9 54.5 – – – – –
LLaMA-VID[10] Vicuna-7B 336 – 65.1 1521 – – – –
LLaMA-VID[10] Vicuna-13B 336 – 66.6 1542 – – – –
LLaVA-1.5[43] Vicuna-7B 336 58.2 65.2 1511 31.1 – – –
LLaVA-1.5[43] Vicuna-13B 336 61.3 69.2 1531/295 36.1 36.4 33.6 27.6
Mini-Gemini Gemma-2B 336 56.2 59.8 1341/312 31.1 31.7 29.1 29.4
Mini-Gemini Vicuna-7B 336 65.2 69.3 1523/316 40.8 36.1 32.8 31.4
Mini-Gemini Vicuna-13B 336 65.9 68.5 1565/322 46.0 38.1 33.5 37.0
Mini-Gemini Mixtral-8x7B 336 69.2 75.6 1639/379 45.8 41.8 37.1 41.8
Mini-Gemini Hermes-2-Yi-34B 336 70.1 79.6 1666/439 53.0 48.7 43.6 38.9
Highresolutionsetting
OtterHD[12] Fuyu-8B 1024 – 53.6 1314 – – – –
CogVLM-Chat[66] Vicuna-7B 490 70.4∗ 63.7 – 51.1 41.1 – 34.5
LLaVA-NeXT[11] Vicuna-7B 672 64.9 68.1 1519/332 43.9 35.8 – 34.6
LLaVA-NeXT[11] Vicuna-13B 672 67.1 70.7 1575/326 48.4 36.2 – 35.3
LLaVA-NeXT[11] Hermes-2-Yi-34B 672 69.5 79.6 1631/397 57.4 51.1 44.7 46.5
Mini-Gemini-HD Vicuna-7B 672 68.4 65.8 1546/319 41.3 36.8 32.9 32.2
Mini-Gemini-HD Vicuna-13B 672 70.2 68.6 1597/320 50.5 37.3 35.1 37.0
Mini-Gemini-HD Mixtral-8x7B 672 71.9 74.7 1633/356 53.5 40.0 37.0 43.1
Mini-Gemini-HD Hermes-2-Yi-34B 672 74.1 80.6 1659/482 59.3 48.0 44.9 43.3
Privatemodels
GeminiPro[5] Private – 74.6 75.2 – 64.3 47.9 – 45.2
Qwen-VL-Plus[23] Private – 78.9 66.2 – – 45.2 40.8 43.3
GPT-4V[4] Private – 78.0 75.1 – 67.6 56.8 55.7 49.9
4.1 ExperimentalSetup
ImplementationDetails. Inthisstudy,weinstantiateMini-GeminiwiththeCLIP-pretrainedViT-
L[40]forLRvisionencoderandtheLAION-pretrainedConvNext-L[54]forHRvisionencoder. For
efficienttraining,wekeeptwovisionencodersfixedandoptimizetheprojectorsofpatchinfomining
inallstages. Meanwhile,weoptimizetheLLMduringtheinstructiontuningstageonly. Regarding
thetrainingscheme,weoptimizeallthemodelsfor1epochwiththeAdamWoptimizerandaCosine
learningschedule. Inmostcases,theinitiallearningratesformodalityalignmentandinstruction
tuningarerespectivelysetat1e−3and2e−5,withanadjustedrateof1e−5fortheMixtral-8×7Band
Hermes-2-Yi-34Btoensurestableinstructiontuning. Theframeworkinvolvestrainingon8×A800
GPUsforstandardmachineconfigurations.ForthelargestmodelwithHermes-2-Yi-34B,weleverage
4machinesandcompletetheoptimizationwithin2dayswithDeepSpeedZero3strategy. FortheHD
version,thetotalcostisenlargedtoabout4daysbecauseoftheextendedvisualtokensinLLMs.
Datasets. Formodeloptimization,weconstructhigh-qualitydataforcross-modalityunderstanding
andgeneration. Itmainlyincludes1.2Mcaptionpairsformodalityalignmentand1.5Msingle-or
multi-roundconversationsforinstructiontuning,aselaboratedinSection3.3. Moreover,wereport
resultsonwidely-adoptedzero-shotimage-basedbenchmarks,includingVQAT (TextVQA)[67],
MMB(MMBench)[24],MME[68],MM-Vet[69],MMMU[25],andMathVista[70]datasets.
4.2 MainResults
NormalResolution. InTable1,wecomparewithpreviousleadingapproachesacrossseveralset-
tings,includingnormalandhighresolution,andalsoconsiderprivatemodels. Atnormalresolution,
Mini-GeminiconsistentlyoutperformsexistingmodelsacrossawiderangeofLLMs. Intheefficient
modelcategory,Mini-Gemini,whenconfiguredwithGemma-2B[36],demonstratessuperiorperfor-
6Table 2: Comparison with different info mining settings. The baseline is LLaVA-1.5 [43] with
Vicuna-7Busingthesametrainingdataandstrategy. TokenNumindicatesthenumberofvisual
tokensT inEquation(1). ∗denotesthatimagesinthetrainsubsetareincluded. Resultswithpatch
V
infominingaremarkedin■. WerespectivelysetConvNeXt-L,336,and768forHRVisionEncoder
(VE-HR),LRimageresolution(LR),andHRimageresolution(HR)bydefault.
Method VE-HR LR HR TokenNum. VQAT MME MM-Vet
Baseline – 224 – 256 54.1∗ 1467.1 30.7
+Infomining ConvX-L 224 512 256 58.1∗ +4.0 1485.2 +18.1 31.3 +0.6
+Higherres. ConvX-L 224 768 256 59.8∗ +1.7 1478.3 -6.9 31.9 +0.6
Baseline – 336 – 576 58.2∗ 1510.7 31.1
+Infomining ConvX-B 336 768 576 58.4∗ +0.2 1451.7 -59.0 33.8 +2.7
+LargerVE-HR ConvX-L 336 768 576 61.5∗ +3.1 1517.0 +65.3 34.6 +0.8
+LargerVE-HR ConvX-XXL 336 768 576 62.0∗ +0.5 1505.7 -11.3 33.8 -0.8
Table3: Comparisonwithdifferentmodelsanddatasettings. WetakeLLaVA-1.5[43]withVicuna-
7Basourbaseline. TokenNumindicatesthenumberofvisualtokensT inEquation(1). ∗denotes
V
imagesintrainsubsetareincluded. Ablationstudiesonmodelanddataaremarkedwith■and■.
Method LR HR TokenNum. VQAT MME MM-Vet
Baseline 336 – 576 58.2∗ 1510.7 31.1
+Infomining 336 768 576 61.5∗ +3.3 1517.0 +6.3 34.6 +3.5
+ShareGPT4V 336 768 576 63.2∗ +1.7 1527.6 +10.6 34.2 -0.4
–TextCaps 336 768 576 59.0 -4.2 1465.2 -62.4 35.0 +0.8
+LAION-GPT-4V 336 768 576 58.7 -0.3 1521.8 +56.6 33.4 -1.6
+OCR-related 336 768 576 61.6 +2.9 1523.5 +1.7 33.7 +0.3
+Gen-related 336 768 576 62.2 +0.6 1521.2 -2.3 37.0 +3.3
+ALLaVA 336 768 576 65.2 +3.0 1523.3 +2.1 40.8 +3.8
+Tokenextension 672 1536 2880 68.4 +3.2 1546.2 +22.9 41.3 +0.5
mancecomparedtotheefficientMobileVLM[63]andevensurpassesInstructBLIP[42]equipped
withVicuna-7Bandeven13B.ThescalabilityofMini-GeminiisevidentwhenlargerLLMsare
employed. GiventhesameLLM,theproposedMini-GeminiisvalidatedtosurpassLLaVA-1.5[43]
withalargemarginacrossallbenchmarks. Notably,withtheHermes-2-Yi-34BLLM,Mini-Gemini
achievesexceptionalresults,outpacinghigh-resourceprivatemodelslikeQwen-VL-Plus[23]and
GeminiPro[5]insomechallengingbenchmarkslikeMMMU[25]andMMB[24].
HighResolution. Tovalidatetheframeworkforextendedvisualtokens,weperformexperiments
withaninputsizeof672forLRvisualencoderand1536forHRvisualencoderinTable1. Asdis-
cussedabove,theHRvisualencoderprimarilyservestoofferhigh-resolutioncandidateinformation.
Importantly,despitetheincreasedresolution,theeffectivenumberofvisualtokensprocessedbythe
LLMremainsconsistentwiththeLRinputsizeof672,ensuringcomputationalefficiency. Thebenefits
ofthisapproachareparticularlyevidentindetail-orientedtasks. Forexample,intheTextVQA[67]
benchmark,ourmethodachievedaperformancerateof74.1%withtheHermes-2-Yi-34Bconfigura-
tion,closelymatchingtheperformanceofthewell-establishedGeminiPro[5]. Detailedresultsin
Table1showthatMini-Geminiexcelsinmorechallengingbenchmarksaswell. Forinstance,thepro-
posedmethodisonparwithQwen-VL-Plus[23]ontheMathVista [70]andMMMU[25]benchmark
andevensurpassesGeminiProandGPT-4Vonthewidely-adoptedMMB[24]benchmark.
4.3 Component-wiseAnalysis
PatchInfoMining. WefirstdelveintotheproposedpatchinfominingandreportresultsinTable2.
ItisclearthatthemodelachievessignificantgainswiththeConvNeXt-Lintegratedasthevision
encoderfor HRimages. Forexample, whenthe LRand HRare respectively setto 224and 512,
themodelincreases4.0%and18.1inTextVQAandMMEdatasets. ElevatingtheHRresolution
to768furtherwidenstheperformancemargin,achievinga5.7%upliftinTextVQAcomparedto
the baseline. These results underscore the substantial impact of patch info mining in harnessing
more detailed visual cues. When we further extend the LR resolution to 336, patch info mining
stillcontributesconsistentgains. Forinstance,withthedefaultConvNeXt-Lasvisionencoder,it
surpassesthebaselinewith3.3%,6.3,and3.5%inTextVQA[67],MME[68],andMM-Vet[69]
dataset,respectively. Thisprovesthecapabilityofdesignedmoduleswithinputresolutionscaledup.
7VisionEncoder. Toinvestigatetheeffectbroughtbyminingcandidates,weconductexperiments
withvariousHRvisionencodersinTable2. ComparedwiththedefaultConvNeXt-L,weaddtwo
encodersforcontrasttrials,i.e.,ConvNeXt-B,andConvNeXt-XXL.WiththebasicConvNeXt-B,
themodelperformsbetterinTextVQA[67]andMM-Vet[69]. However,theConvNeXt-Lencoder
consistentlydeliverspeakresults,especiallyintheMMEandMM-Vetdatasets,indicatingasuperior
balanceinhandlingdetailedvisualinformation. Wecanconcludefromthetablethatalargervision
encoderforHRimagescontributesmoretothecandidatequality,butthemodelconvergeswitha
toolargeencoderlikeConvNeXt-XXL.Hence,consideringthebalancebetweeneffectivenessand
computationalefficiency,ConvNeXt-LischosenasthedefaultHRvisionencoder. Thisdecisionis
basedonitsabilitytoprovidehigh-qualityvisualinformationminingwhilemaintainingreasonable
computationaldemands,asevidencedbythecomparativeperformanceacrossthebenchmarks.
High-qualityData. Inthisera,thesignificanceofhigh-qualitydataforenhancingthecapabilities
ofLLMsandVLMscannotbeoverstated. Inourcomprehensiveanalysisofdatacombinationeffects,
presentedinTable3,webeginwithabaselinemodelincorporatingpatchinfomining. Theintegration
ofhigh-qualitycaptionsfromShareGPT4V[14]yieldsimprovedvisualalignmentandperformance
gains. Wevalidatethezero-shotperformanceontheTextVQA[67]benchmark,notablyremoving
TextCaps[59]datafromthetrainingsetinlinewithpreviousstudies[11]. Thismodificationled
to a notable performance decrease, underscoring the value of specific data types in training. To
counteractthisdecline,weincorporateadditionalhigh-qualitycaptionsfromLAION-GPT-4V[60]
andOCR-specificdata,thusenhancingthemodel’sOCRreasoningcapabilities. Moredetailsare
providedintheappendix. AselaboratedinSection3.3,weutilizegeneration-relatedinstructionsto
expandtheapplication. Itisinterestingtofindthatsuchdataalsobenefitstheimageunderstanding
abilityandbrings3.3%gainsinMM-Vetdataset. Moreover,withthehigh-qualityGPT4Vresponses
from ALLaVA [15] dataset, the framework respectively pushes the baseline over 7% and 9% in
TextVQA and MM-Vet datasets. This comprehensive evaluation underscores the pivotal role of
strategichigh-qualitydataintegrationinamplifyingthepotentialoftheMini-Geminiframework.
Visual Token Extension. As depicted in Figure 3b, the proposed patch info mining is adeptly
designedtoaccommodateextendedvisualtokens,therebygeneralizingitsutilityacrossdifferent
inputresolutions. WevalidatetheeffectivenessofthetokenextensioninTable3. Whenincreasing
LR and HR input resolution, the model achieves significant gain in all benchmarks. Notably, in
detail-oriented tasks such as TextVQA, we observe a performance uplift of over 3%, indicating
a significant enhancement in the model’s ability to handle complex visual data. Our empirical
observationssuggestthattheincreaseinresolutionsignificantlydiminishesvisualhallucinations,
leadingtomoreaccurateandreliableimagecomprehension. Generally,withtheincreasedvisual
tokennumber,Mini-Geminicanbescaleduptowardsbettercapability. Wecanalsodrawthesame
conclusionfromhigh-resolutionresultsinTable1.
4.4 QualitativeResults
VisualUnderstanding. ToascertainthevisualcomprehensionprowessofMini-Geminiinreal-
worldsettings,weapplyittoavarietyofunderstandingandreasoningtasksinFigure5. Thanksto
thepatchinfominingandhigh-qualitydata,Mini-Geminicanwellsolveseveralcomplexcases. For
example,itiscapableofrecognizingplottedcurvesingraphicaldataanddirectlytranslatingthem
intoPythoncodeforimmediateapplication. Beyondmererecognition,itexhibitsakeenattention
todetail,accuratelydescribingintricateelementswithincomplexindoorscenes,anddemonstrating
anuancedunderstandingofcharacterassociationsinmemes. Moreover,Mini-Gemini’sanalytical
capabilitiesextendtochartanalysisandpracticalproblem-solving,suchasintelligencetests.
ImageGeneration. InFigure6,weprovideacomprehensiveevaluationofMini-Gemini’sgener-
ationcapabilities. ComparedwithrecentstudiessuchasAnyGPT[52]andChatIllusion[51],our
strongermulti-modalunderstandingabilityallowsustogeneratetext-to-imagecaptionsthatbetter
alignwiththegiveninstructions,resultinginmorecontextuallyappropriateimage-textanswers. A
noteworthypoint,asshowninFigures1and6,isitsproficiencyingeneratinghigh-qualitycontent
basedonmulti-modalhumaninstructions,withtext-onlytrainingdata. Thiscapabilityunderscores
Mini-Gemini’srobustimage-textalignmentandsemanticinterpretationskills,whichcomeintoplay
effectivelyintheinferencestage. ByleveragingthepowerfulreasoningabilityoftheLLM,itcan
producereasonableimage-textoutputsinsingleormulti-roundconversations.
81.Generate a matplotlib code for the resulting plot using subplot
function to reproduce functions and the layout.
importmatplotlib.pyplotasplt
1 importnumpyasnp
# Generate data for the normal distribution
mu,sigma = 87,8
x = np.linspace(70,110,100)
y1 = np.exp(-((x -mu)/ sigma)** 2/ 2)/ sigma / np.sqrt(2* np.pi)
# Generate data for the standard normal distribution
x = np.linspace(-3,3,100)
y2 = (1/ np.sqrt(2* np.pi))* np.exp(-(x ** 2)/ 2)
# Create a figure with two subplots
fig,axs= plt.subplots(nrows=1,ncols=2,figsize=(10,6))
# Plot the normal distribution
axs[0].plot(x,y1,label='Normal Distribution of Toothbrush Prices')
axs[0].set_xlabel('Toothbrush Prices')
axs[0].set_ylabel('Density')
axs[0].set_title('Normal Distribution of Toothbrush Prices')
# Plot the standard normal distribution
axs[1].plot(x,y2,label='Standard Normal Distribution of Toothbrush Prices')
axs[1].set_xlabel('Toothbrush Prices')
axs[1].set_ylabel('Density')
axs[1].set_title('Standard Normal Distribution of Toothbrush Prices')
# Adjust the layout and add a legend
plt.tight_layout()
plt.legend()
plt.show()
Reproducedwithgeneratedcode Mini-Gemini
2 2.a.Describetheimageindetail.
The image is a photograph of a modern kitchen interior. The kitchen features a white
cabinetry with a wooden countertop and a central island with a white tabletop. The
island is surrounded by four green velvet chairs with black legs. On the countertop, there
are various kitchen items including a coffee maker, a toaster, and a bread machine.
Above the island, there are pendant lights with exposed bulbs. The wall behind the
countertop is adorned with wooden panels and a window with white blinds. On the
! window sill, there are potted plants and a small vase with flowers. The floor is covered
with a patterned rug. The overall style of the kitchen is contemporary with a warm and
inviting atmosphere. Mini-Gemini
2.b.What's the text on the bread machine?
Bread. Mini-Gemini
3.Can you explain this meme?
The meme is a play on words and expectations. It starts with a conversation where one
person is asked where they are, and they respond with "at the gym!" which is a common
answer for someone who is health-conscious or trying to impress others with their
dedication to fitness. The person who asked for the location then requests a picture,
presumably to see the gym or the person's workout. However, the image sent is not of a
gym but of a McDonald's restaurant, with the word "GYM" written on it in a way that
mimics the McDonald's logo. This creates a humorous twist, as the person is actually
atMcDonald's, which is known for fast food and is the opposite of a gym in terms of
health and fitness. The meme plays on the irony of the situation and the expectation
3 versus reality theme. Mini-Gemini
4.Spot two data points that stands out in these charts and what that implicates. Then
produce a detailed and well-organiziedmarkdown table to compare all the data.
The two data points that stand out are the "Annoying pain" in battery cages and furnished
cages, both of which are significantly higher than the other types of pain listed. This
implies that the conditions in battery and furnished cages are particularly stressful for
hens, leading to prolonged periods of discomfort.
Here is a detailed and well-organized markdown table comparing all the data:
Type of cage Annoying pain Hurtful pain Disabling pain Excruciating pain
Battery cages 420 days 253 days 27 days 0.003 days
Furnished cages 384 days 197 days 10 days 0.003 days
Cage-free aviary 130 days 109 days 10 days 0.003 days
This table allows for a clear comparison of the average days of pain endured by hens in
different cage systems, with the "Annoying pain" data being particularly noteworthy due
4
to its higher values.
5.Solvethisissue.
To solve the issue of determining the number of quarter circles in the figure, we need to
count each individual quarter circle. A quarter circle is a segment of a circle that is one-
fourth of the circle's circumference. In the provided image, there are two quarter circles
that are clearly visible. However, the central circle is divided into four equal segments,
each of which is a quarter circle. Therefore, we have a total of six quarter circles: two
from the outer shapes and four from the central circle.
5
The correct answer to the question is 6.
Mini-Gemini
Figure5: QualitativeresultsinvisualunderstandingwithMini-Gemini.
9Tell me a story about a fire-spitting dragon killed by a man Tell me a story about a close-up of a fire spitting dragon, cinematic
shot, killed by a man.
• Once upon a time, in a land far, far away, there lived a
fearsome dragon. This dragon was known throughout In a fantasy world, a man is
the land for its fiery breath and its ability to destroy seen standing in front of a
entire villages with a single breath. large, fire-breathing dragon.
• One day, a brave man decided to take on the dragon. The man is holding a sword
He was a skilled warrior… and is ready to strike the
• As he approached the dragon, he could see its fiery dragon, which is preparing to
breath and hear its roaring… attack. The dragon is a
• The dragon let out a mighty roar and breathed a massive creature with a fiery
stream of fire towards the man. But the man was quick breath, and it is surrounded by
and dodged the flames, dodging and weaving to avoid a cloud of smoke. The scene is
the dragon's breath. set in a desert, with a large fire
• Finally, the man got close enough to the dragon and in the background, adding to
struck it with his sword.. Mini-Gemini the dramatic atmosphere. .. ChatIllusion
Can you show me a picture of a Change the main character of this to a
sunny beach? cat.
Mini-Gemini AnyGPT Mini-Gemini ChatIllusion
I want to make a gift for my daughter Mini-Gemini Whatifmydogwearascarf? Mini-Gemini
using this, please give me one suggestion
If you want to let your dog wear a scarf, there
Sure, here‘s a suggestion for a gift using are a few things to consider. First, make sure
this yellow yarn:You could make a cozy the scarf is made of a soft, non-irritating
yellow beanie for your daughter. The material that won't cause any discomfort
bright yellow color of the yarn would to your dog. Avoid using scarves that are too
make a cheerful and warm accessory tight or have small parts that could be a
for cold weather. To make the beanie, choking hazard.
you would need to knit or crochet a few
simple stitches to create a snug fit for her A cozy yellow beanie with a pom-pom on a When putting the scarf on your dog, start by A i rf rl iu tf af ty i nc go r sg ci a rw fe ,a r si in tg t ia n gs o of nt , a n wo on o- den
head… s dn io gw iy t al la n ad rs tc ,a p we i, n tw ea rr m t ha en md e inviting, gently wrapping it around their neck… l ce hd eg re r, y w bi lt oh s sa o mb sl ,u r wr ae rd m ,b a cc ok zg yr ,o u pn ed t- fo rf i endly
Write me a 50 wordanecdote on why Mini-Gemini C figo hu tl id n gy ?ou write a short poem about angels Mini-Gemini
dinosaurs can eat ice cream without
having a brain freeze In the heavens above,
A battle rages on,
Between angels of love,
Dinosaurs don't get brain freeze because And angels of wrong.
they have a different circulatory Their swords clash and their shields shine,
system than humans. Their large body As they fight for their side,
mass helps to regulate their body The angels of love, so divine,
t fle om wp te hr aa tt u care u, s p er se bv re an inti n frg e t eh ze e r inap hi ud m bl ao no sd .. A o d l f i ac s na i t dr c r st e e co s ao c s pn r , e e ,d a s i m u vn r io w r bs i o ra t u au h n nr o d t u e e t d cn oj a b lo n y oy y ri a sn s ,g i p g r da n e i s h gg i ii o s ta f t an o lt r i as c rc toop A B W …n u hd t o it n ah lte wh a e an yeg sn e cdl os , m io t'f es p otr h uid e te oa. nn g te ol ps , of love, A o o b f n t a ac e h c ne e k tl a r d ae n r ss g h o yt e o p ,i l l a d o bl w i f y i n b e g a Ga l rt d a s et i t gl n s a e g h r R i r ub a e y te l kt g d s ow l , k we o y se w s , kn i e i n t da g in a gg s g ie w a tl o i as r n l, d s t aw a ri n a tt d ,h the
Give me a similar example of the Mini-Gemini [Round1]Show me one idea of what I could Mini-Gemini
contradiction depict in this image. make with this. Round2
One idea for what you could make with
The image presents a visual these two skeins of yarn is a simple two-
contradiction between the typical tone knitted or crocheted scarf…
habitat of a cactus and the presence
of ice, which is not a natural occurrence
in desert environments. A similar [Round2]Can you
example of this kind of contradiction give me more
could be a polar bear in a tropical examples? I want to
r t soa u i rcn vof io vld er ,e ii ns c t y a, ea hns o v tp i ,ro hol ua nr mm b ie de na ctr s ls i m aa nr ae d t e a w .d o …a up ldte nd o t A t t b r r lp o e uo p e el i s a c sr a a k l n yb de r a a dr i e n ns f st o ea r n e fd s oi t ln , ig a s ga u em r ,i r d o us u nt n d d ea e r d l au b s y ch l p ea al rm Mini R-G ouem ndin 1i m cua tk ee a s no dm spet eh ci in ag l. A p p i lw n ah k yi fm s us t li r ,c i a p hl e a s na , dm mi s ag i du t er t ,u i m n ci g o zt o yo ny aw i wt oh o db el nu e s ua rn fd a ce,
A spider man riding a motorbike He then encounters a Lizardman. H Spe i dth ee r n s ib lke .at the Lizardman using his F bi rn ea al kly fah se t. back to home and eating the
Mini-Gemini Mini-Gemini Mini-Gemini Mini-Gemini
Figure6:QualitativeresultsinimagegenerationwithMini-Gemini.Inthefirsttworows,wecompare
Mini-GeminiwithChatIllusion[51]andAnyGPT[52]withtheirofficialcases. Inthefollowingrows,
weillustratemorecasestoshowthereasoninggenerationabilitywhilepreservinghigh-qualitytext
generation. Imageinputs(ifhave)areprovidedattheleft-topcorner. Insomecases,weoverlaythe
generatedpromptsoncorrespondingimages.
105 ConclusionandDiscussion
WepresentedMini-Gemini, astreamlinedandpotentframeworkformulti-modalityVLMs. The
essenceofMini-GeminiistoharnessthelatentcapabilitiesofVLMsthroughstrategicframework
design,enricheddataquality,andexpandedfunctionalscope. Atitscore,patchinfominingenables
efficientextractionofdetailedvisualcuesbyengagingwithhigh-resolutioncandidates. Fromthe
dataperspective,ourmeticulouslycompiledhigh-qualitydatasetensuresaccuratevision-language
alignment and bolsters strong instruction-following ability. Furthermore, we support reasoning-
basedgenerationinMini-GeminiandempowercurrentVLMswithany-to-anyworkflow. Extensive
experimentsonseveralzero-shotbenchmarksprovethesuperiorityoftheproposedmethod,which
surpassespreviousleadingapproachesandevenprivatemodels. WehopetheMini-Geminicanserve
asastrongbenchmarkforimageunderstandingandVLM-guidedgeneration.
AlthoughMini-Geminiachievesgoodresults,itstillhasgreatpotentialtobefurtherexplored. For
visualcomprehension,thecountingabilityandcomplexvisualreasoningabilityarestillfarfrom
satisfactory. This could be attributed to the lack of corresponding training data especially in the
pretrainingstage. Meanwhile,forreasoning-basedgeneration,weusetexttobridgetheVLMand
diffusionmodelinthisworkbecausewedonotfindapparentgainwithembedding-basedapproaches.
Wewilltrytofindamoreadvancedmannerforvisualunderstanding,reasoning,andgeneration.
Appendix
A DataCollectionDetails
Image-text Data Collection. In this section, we delve into the specifics of OCR-related data
collection. Naturalimagescanbeeasilyannotatedwithdetailedcaptions,buttext-richfiguresor
diagrams,suchasdocuments[17],charts[18],andscientificdiagrams[19],presentamoreintricate
challengeformodelsincomplexquestionsandanswers. Therefore,tofacilitatetheoptimization
process,wefollowthestrategyinTextVQA[67]andincorporateOCRtokensformodelreference
inthetrainingphase. Inparticular,weutilizethePaddleOCR[71]toinitiallyidentifyandextract
textualelementswithineachimage. Then,weappendthetextcharacterstotheoriginalconversations
inaformatofReference OCR token:Text_1,...,Text_n,whereText_1toText_nindicates
ndetectedtextstrings. Thisapproachensuresthatthemodelhasaccesstotextualrepresentations
fromtheimages,enhancingitsabilitytounderstandtheimagecontent. Itisimportanttonotethatthe
OCRdetectorisutilizedsolelyforgeneratingenricheddataandisnotemployedduringtesting. This
distinctionunderscoresourobjectivetotrainMini-Geminiwithacomprehensiveunderstandingof
bothtextualandvisualelements,therebyimprovingcapabilityincompleximage-textscenarios.
Generation Data Collection. For the data generation collection described in Section 3.3, we
provide specific examples of query prompts and their corresponding reference data sources for
two generation tasks in Figure 7. We commence with a corpus comprising 10K GPT4V caption
dataand6KEnglish-onlyLLMSFTdata. Afterfilteringoutresultsthatdidnotmeettheformat
requirements,weultimatelyobtained13Kdatapoints. Toenhancethecontextualityandqualityofthe
queries,weincorporatetwodistincttypesofin-contextexamples: get_example_captions()and
get_example_queries(). Theformerfunctionrandomlyselects5high-qualityStableDiffusion
(SD)Text-to-Image(T2I)prompts,whilethelatterextracts3instancesfromarepositoryofsimple
instructionaltemplates. Thesein-contextexamplesserveasafoundationalguide,providingdiverse
andrepresentativepromptsthatsignificantlyenrichthegenerationprocess. Thisstrategicapproach
ensurestheproductionofhigh-quality,relevantdata,effectivelysupportingthegenerativecapabilities.
B ExtendedShowcases
In this section, we further provide more cases to validate the generality and capability of Mini-
Geminiinvariousenvironments. AspresentedinFigures8and9,Mini-Geminicanwellanswer
detail-orientedquestionsandsolveOCR-relatedandscientificproblems. Forimagegeneration,we
presentmoreexamplesinFigure10thatincludedirectT2Igeneration,multi-roundconversation,
reasoning-basedgeneration,storytelling,andin-contextgeneration.Theyfurtherprovethesuperiority
ofMini-Geminiinbothvisualcomprehensionandgeneration.
11Simple Instruction Hi ChatGPT, our objective is to create several high-quality captions that suitable for diffusion models based
Re-caption on the original detailed description given.
1. Thoroughly read and interpret the given description and the given caption, query formats.
QuestionFormat 2. Generate a query based on the given description, we will give you some examples of the queries.
Examples 3. Based on the query and the overall description, generate a high-quality caption that is suitable for the
query's instruction, and suitable for diffusion models. (Example in below)
"Depict an astronaut
with Earth in the
background.” Key considerations:
"Generate a neon-lit -Avoid revealing or implying your identity as a chatbot.
futuristic city at -Do not include some professional concepts or texts that cannot be understood by the general public.
night.”
"Portray a lone oak Example Human Query formats:
tree in autumn.”… {get_example_queries()}
Example high-quality caption formats (limited with 30 words each, descriptive and with some other property
tags):
GigaSheet
{get_example_captions()}
High-QualitySDXL
PromptExamples
Detailed caption information for your reference:
<GPT-4V_Image_Caption_Data>
LAIONGPT4VImage Requested format:
descriptions Human Query: The human query for the generation content. bounded with double quotes.
Related Caption: One caption to describe the image of the query instruction, correlated with the description
and the query. The caption should be no more than 30 words long. bounded with double quotes.
Strictly adhere to this format and guidelines.
In-Context Prompt Hi ChatGPT, our objective is to create several high-quality captions for the following conversations used for
Generation generative models.
1. Thoroughly read and interpret the given conversation and the given caption formats.
GiggaSheet 2. For each round of the conversation. Generate a caption that describe the image with the related
High-QualitySDXL informationsthat is suitable with the conversation context. You can generate captions that is associated with
Promptdatabase the conversation context, but do not directly reference the conversation.
3. The caption is better to be descriptive and describe clearly with the main object, do not include any
"Max Headroom in a offensive or sensitive content.
Perfume advertisement, 4. Please make sure the caption is related to the conversation context and the image, and the caption should
magical, science fiction, be at least 30 words long.
symmetrical face, large
eyes, Chanel, Calvin
Key considerations:
Klein, Burberry, Versace,
Gucci, Dior, hyper -Avoid revealing or implying your identity as a chatbot.
realistic, digital art, -Do not include some professional concepts or texts that cannot be understood by the general public.
octane render, trending
on artstation, Example high-quality caption formats (limited with 30 words each, descriptive and with some other property
artstationHD, tags)
artstationHQ, unreal {get_example_captions()}
engine, 4k, 8k”…
Conversation information for your reference:
Conversation:
OpenAssistant2 <Real-World_Multi-Round_Conversation_Data>
LIMA:
High-Quality Requested format:
LLMSFTdata Related Captions: [A list of captions, with each caption correlated with each round of gptresponse, in double
quotes and separated by a comma, if you do not have any captions for some rounds, please use an empty
string ""]
Strictly adhere to this format and guidelines.
Figure7: Adetailedpromptdesignanddatasourceillustrationusedforallocatingimagegeneration
data. ThetotalcostfortheGPT-4APItogetall13Kdataisaround80$.
121.Can you explain this meme?
The meme uses a visual metaphor to represent the relationship between different programming languages and
Chat-GPT, a language model developed by OpenAI. The tree trunk represents the foundational technologies that
support web development, with Java and Python being two of the most popular programming languages. JavaScript
and HTML are also essential technologies for web development, with JavaScript being the scripting language that
adds interactivity to web pages and HTML being the markup language that defines the structure of web pages.
The character labeled "Chat-GPT" is depicted as a lumberjack cutting down the tree, which symbolizes the
disruptive impact that Chat-GPT could have on the traditional programming landscape. The meme humorously
suggests that Chat-GPT might be so powerful or efficient that it could replace the need for these foundational
technologies, much like a lumberjack cuts down a tree.
The meme plays on the idea that Chat-GPT, with its advanced natural language processing capabilities, could
potentially simplify or automate tasks that were previously done by programmers using these languages. It's
a lighthearted way to comment on the evolving nature of technology and the potential for new tools to change the
1
way we work.
Mini-Gemini
2.Write HTML and CSS code <!DOCTYPE html> Mini-Gemini
to create a webpage based on <html>
the image shown. You should <head>
provide one .html file and one <title>Welcome!</title>
.cssfile. <link rel="stylesheet"type="text/css"
href="style.css">
</head>
<body>
body{ <h1>Welcome!</h1> render
font-family:Arial,sans-serif; <p>This is a simple webpage.</p>
margin:0; <ul>
padding:0; <li><a href="#">Home</a></li>
} <li><a href="#">About</a></li> Mini-Gemini
<li><a href="#">Contact</a></li>
h1{ </ul>
3 text-align:center; <button>Learn More</button>
margin-top:50px; <p>Contact us: <a
2 … href=“mailto:email@example.com”>email@example.
…
CSS HTML
3.Describe this table in detail and summarize what you can get from this table.
The chart is a line graph that illustrates the historical and projected number of children under
the age of 5 worldwide. The data is based on the United Nations' World Population Prospects
from 2022. The graph shows a general upward trend in the number of children under 5
from 1950 to around 2020, peaking at approximately 600 million. After 2020, the graph
projects a decline in the number of children under 5, with the line descending towards the
4
400 million mark by 2100.This suggests a projected decrease in the birth rate or an increase
in the age at which women have children, leading to fewer children under 5 in the future. The
graph is a useful tool for understanding demographic trends and can be used by
policymakers, researchers, and organizations to plan for future needs, such as education and
3 healthcare services for children.
Mini-Gemini
4 4.How many small spheres are the same color as the big rubber cube?
There are no small spheres that are the same color as the big rubber cube. The big rubber
cube is blue, and there are no blue spheres present in the image.
Mini-Gemini
5.Please solve this issue. Tell me which option is the best choice.
The question appears to be a visual puzzle, likely from a logic or brain teaser book. The task is
to identify which option (A, B, C, D, or E) is the best choice to replace the question mark in the
sequence. To solve this, we need to look for a pattern or rule that applies to the sequence of
5 shapes.
Upon inspection, the sequence of shapes in the top row seems to be increasing in the
number of sides: a triangle (3 sides), a square (4 sides), a pentagon (5 sides), and a
hexagon (6 sides). Following this pattern, the next shape should logically have seven sides,
which is a heptagon.
Looking at the options provided, we can see that option A is a hexagon, which does not fit the
pattern. Option B is a pentagon, which is a step back in the sequence. Option C is a triangle,
which is too few sides. Option D is a heptagon, which fits the pattern of increasing sides.
Option E is a hexagon, which does not fit the pattern.
Therefore, the best choice to replace the question mark in the sequence is option D, the
heptagon, as it continues the pattern of increasing the number of sides on the shapes.
Mini-Gemini
Figure8: QualitativeresultsofMini-Gemini’shigh-resolutionunderstanding. Wehighlightafailed
caseinimageunderstandinginorange.
131 1.Here's a solution to a Math problem by a student. Try to reason about the question step
by step. Did the student get the correct answer?
The student has attempted to solve the math problem by adding the number of minutes
Justin practiced on Monday and Tuesday.
The student has correctly added 254 minutes (254 is 250 + 4) to 133 minutes, resulting in
387 minutes. However, the student has not accounted for the fact that Justin practiced
more on Monday than on Tuesday.
The correct answer should be the difference between the total minutes Justin
practiced on Monday and Tuesday, which is 254 minutes (Monday) -133 minutes
(Tuesday) = 121 minutes. Therefore, Justin practiced 121 more minutes on Monday than
on Tuesday. The student's answer is incorrect as they have not subtracted the Tuesday
practice time from the total Monday practice time.
Mini-Gemini
2 2.Why does this image look funny?
The image may appear funny due to the unexpected and unusual sight of a dog
! seemingly 'hiding' among the sheep. This is not a common occurrence, as dogs and
sheep are distinct species with different behaviors and appearances. The dog's
presence among the sheep, especially if it is a herding dog, can be seen as humorous
because it contrasts with the typical expectation of a dog's role in a flock of sheep.
Additionally, the dog's relaxed posture and the sheep's calm demeanor suggest a
harmonious and comical coexistence, which can be amusing to observers. The image
captures a moment that is both endearing and whimsical, which can elicit a light-
hearted response from viewers.
Mini-Gemini
3 3.Choose one of the two pictures below to fill in the blank space above to maintain the
pattern.
To maintain the pattern, the blank space above should be filled with the image of the
green parrot. This is because the pattern alternates between the green parrot and the
orange tiger, and since the last image in the sequence is the orange tiger, the next image
should be the green parrot to continue the alternating pattern.
Mini-Gemini
4.a.Illustrate what problem this code in the image is addressing.
The code in the image is addressing the problem of sorting a list of numbers using the quicksort
4 algorithm. Quicksort is a divide-and-conquer algorithm that works by selecting a pivot element from
the list and partitioning the other elements into two sub-lists, according to whether they are less than
or greater than the pivot. The partition function is used to find the index of the pivot element, which is
then used to divide the list into two parts. The quicksort function then recursively sorts the two sub-
lists, and the process continues until the entire list is sorted. This algorithm is efficient for large lists
because it reduces the problem size by half with each recursive call. Mini-Gemini
4.b. Transfer it to Python code here.
defquicksort(nums,left,right): This code defines the quicksort
ifleft < right:
partition_index= partition(nums,left,right) function, which sorts a list of numbers
quicksort(nums,left,partition_index-1) by calling the partition function to find
quicksort(nums,partition_index+ 1,right) the index of the pivot element and then
recursively sorting the two sub-lists. The
defpartition(nums,left,right): partition function is used to partition
pivot = nums[right] the list into two parts, with elements
i= left -1
forj inrange(left,right): less than the pivot on the left and
ifnums[j]< pivot: elements greater than the pivot on the
i+= 1 right. The pivot element is then placed
nums[i],nums[j]= nums[j],nums[i] in its correct position in the sorted list.
nums[i+ 1],nums[right]= nums[right],nums[i+ 1]
returni+1 Mini-Gemini
5. Explain this meme and illustrate what it implicates.
The meme is a humorous take on the common narrative that artificial intelligence (AI) will become so
advanced that it will eventually surpass human capabilities and perhaps even take over the world. The
left panel shows a person reacting with alarm, which is a common portrayal of the public's reaction to
the idea of AI becoming too powerful. The right panel, however, shows a cat labeled as a "Dog" by a
neural network, which is a humorous example of AI making a mistake. This juxtaposition implies
that while AI is indeed becoming more advanced, it is not yet at the level where it can accurately
5 perform tasks such as image recognition without errors. The meme uses this contrast to humorously
downplay the fear of AI taking over, suggesting that the technology is still in its early stages and prone
to mistakes. Mini-Gemini
Figure9: QualitativeresultsofMini-Gemini’shigh-resolutionunderstanding.
14Absinthe Painting - Capture a scene of space
Cafe Scene In Paris by elevator
Henri Gervex.Analyze
thengenerate.
1DirectT2I
Generation Mini-Gemini ChatIllusion Mini-Gemini LLMGA
[Round1]Generateasimilar [Round2]I want to add a husky in this image. [Round2]I want to add a husky in this image.
image
2Multi-Round
Mini-Gemini Mini-Gemini LLMGA LLMGA
Conversation Round1 Round2 Round1 Round2
Showcase an orange-themed
interior design inspiration collage.
Orange mood board for interior design,
plush towels, floral bedding, striped
containers, and scented candle, high Re-caption Re-caption
contrast, minimalistic, vibrant color
palette, digital collage. Mini-Gemini
3Pluggablewith
ALLModels
DALLE 3 + prompt DALLE 3 + Mini-Gemini SDXL + prompt SDXL + Mini-Gemini
Describe the image and change the character to a dog. Generate the scene if the cat is coding.
4Reasoning
Generation
SDXL+Mini-Gemini DALLE3+Mini-Gemini SDXL+Mini-Gemini DALLE3+Mini-Gemini
Generate a mouse that is rich. Makeitricher! Make it much richer, it has a big Now it is rubbed and has nothing. Eventually, it perked up and
treasure opened a cheese shop.
5Reasoning+
StoryTelling
Mini-Gemini Mini-Gemini Mini-Gemini Mini-Gemini Mini-Gemini
A majestic mouse with a golden crown A majestic mouse with a golden crown A humble mouse with a tattered crown A cheerful mouse with a crown and key,
A luxurious mouse with a golden crown, and key, standing on a white pedestal, and key, standing on a white pedestal, in and key, standing on a dirt ground, in standing behind a cheese shop
holding a golden key, on a white digital art, fantasy, whimsical, by Greg front of a treasure chest filled with gold front of a broken treasure chest, digital counter, digital art, fantasy, whimsical,
background, digital art, fantasy, Rutkowski, highly detailed, vibrant coins and jewels, digital art, fantasy, art, fantasy, whimsical, by Greg Rutkowski, by Greg Rutkowski, highly detailed,
whimsical, by Greg Rutkowski. colors whimsical, by Greg Rutkowski, highly highly detailed, muted colors vibrant colors.
detailed, vibrant colors
Give me an example to Imagine you have a big box of toys, and you want to teach a computer to recognize and pick out a specific
understand deep learning to a toy from the box. The computer will use deep learning to learn how to do this.
primary student. First, the computer will look at lots of pictures of toys and learn what different toys look like. It will learn
to recognize patterns and features that are important for identifying toys, like the shape of a toy car or the
color of a toy doll.
Next, the computer will use this information to recognize the toy you want it to pick out from the box. It
will look at the toy and compare it to what it has learned about toys. If the toy matches the patterns and
6Reasoning+ features it has learned, the computer will pick it out.
In-Context This is an example of deep learning because the computer is using a lot of data (lots of pictures of toys)
and complex algorithms to learn how to recognize and pick out the toy. It's like the computer is "learning"
Generation Mini-Gemini how to do the task, just like a student would learn in school. Mini-Gemini
Figure 10: Rows 1-2: Comparison of Mini-Gemini with ChatIllusion [51] and LLMGA [50] on
interactiveimagegenerationtasks. Mini-Geminidemonstratessuperioradaptabilityandperformance,
capturingintricatedetailsandmaintainingcoherencewithoutfurthertuningonthetext-imageoutput
alignment. Rows3-6showcaseMini-Gemini’suniquecapabilitiesingeneratingimageswithitsplug-
and-playcapability,reasoninggeneration,andmulti-roundstorytelling. AllMini-Gemini-generated
imagesadoptSDXLunlessotherwisespecified.
15
Re-captionReferences
[1] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/,2023. 2
[2] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,Christopher
Dewan,MonaDiab,XianLi,XiVictoriaLin,etal. Opt:Openpre-trainedtransformerlanguagemodels.
arXiv:2205.01068,2022. 2
[3] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample. Llama:Openandefficientfoundationlanguagemodels. arXiv:2302.13971,
2023. 2,3
[4] OpenAI. Gpt-4technicalreport. arXiv:2303.08774,2023. 2,6
[5] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXiv:2312.11805,2023. 2,6,7
[6] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. arXiv:2301.12597,2023. 2,3
[7] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeruIPS,2023.
2,3,4
[8] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. arXiv:2304.10592,2023. 2,3
[9] HangZhang,XinLi,andLidongBing. Video-llama:Aninstruction-tunedaudio-visuallanguagemodel
forvideounderstanding. arXiv:2306.02858,2023. 2
[10] YanweiLi,ChengyaoWang,andJiayaJia. Llama-vid: Animageisworth2tokensinlargelanguage
models. arXiv:2311.17043,2023. 2,6
[11] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-next:
Improvedreasoning,ocr,andworldknowledge,2024. URLhttps://llava-vl.github.io/blog/
2024-01-30-llava-next/. 2,5,6,8
[12] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. Otterhd: A high-
resolutionmulti-modalitymodel. arXiv:2311.04219,2023. 2,6
[13] RohanBavishi,ErichElsen,CurtisHawthorne,MaxwellNye,AugustusOdena,ArushiSomani,andSag˘nak
Tas¸ırlar. Introducingourmultimodalmodels,2023. URLhttps://www.adept.ai/blog/fuyu-8b. 2
[14] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
Sharegpt4v:Improvinglargemulti-modalmodelswithbettercaptions. arXiv:2311.12793,2023. 2,5,8
[15] GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,Zhihong
Chen,JianquanLi,XiangWan,andBenyouWang. Allava:Harnessinggpt4v-synthesizeddataforalite
vision-languagemodel. arXiv:2402.11684,2024. 2,5,8
[16] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. Makingthevinvqa
matter:Elevatingtheroleofimageunderstandinginvisualquestionanswering. InCVPR,2017. 2,3
[17] RubènTito,DimosthenisKaratzas,andErnestValveny. Documentcollectionvisualquestionanswering.
InICDAR2021,2021. 5,11
[18] AhmedMasry,DoXuanLong,JiaQingTan,ShafiqJoty,andEnamulHoque. Chartqa:Abenchmarkfor
questionansweringaboutchartswithvisualandlogicalreasoning. arXiv:2203.10244,2022. 5,11
[19] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAliFarhadi. A
diagramisworthadozenimages. InECCV,2016. 2,5,11
[20] ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,JiaoSun,YuningMao,XuezheMa,AviaEfrat,
PingYu,LiliYu,etal. Lima: Lessismoreforalignment. AdvancesinNeuralInformationProcessing
Systems,36,2024. 2,5
[21] AndreasKöpf, YannicKilcher, DimitrivonRütte, SotirisAnagnostidis, ZhiRuiTam, KeithStevens,
AbdullahBarhoum,DucNguyen,OliverStanley,RichárdNagyfi,etal. Openassistantconversations-
democratizinglargelanguagemodelalignment. AdvancesinNeuralInformationProcessingSystems,36,
2024. 2,5
16[22] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,JoePenna,
andRobinRombach. Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis. arXiv
preprintarXiv:2307.01952,2023. 2,3,5
[23] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities. arXiv:2308.12966,
2023. 2,6,7
[24] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,Jiaqi
Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?
arXiv:2307.06281,2023. 2,6,7
[25] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,Dongfu
Jiang,WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,MingYin,Boyuan
Zheng,ZhenzhuYang,YiboLiu,WenhaoHuang,HuanSun,YuSu,andWenhuChen. Mmmu:Amassive
multi-disciplinemultimodalunderstandingandreasoningbenchmarkforexpertagi.InCVPR,2024.2,6,7
[26] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InNeurIPS,2017. 2
[27] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert:Pre-trainingofdeepbidirec-
tionaltransformersforlanguageunderstanding. arXiv:1810.04805,2018. 2
[28] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
InNeurIPS,2020. 2
[29] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBamford,
DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,etal. Mixtralof
experts. arXiv:2401.04088,2024. 2,3
[30] JasonWei,MaartenBosma,VincentYZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,AndrewM
Dai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. arXiv:2109.01652,2021. 3
[31] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollowinstructionswith
humanfeedback. InNeurIPS,2022. 3
[32] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,
andTatsunoriB.Hashimoto. Stanfordalpaca:Aninstruction-followingllamamodel. https://github.
com/tatsu-lab/stanford_alpaca,2023. 3
[33] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:Anopen-sourcechat-
botimpressinggpt-4with90%*chatgptquality. https://lmsys.org/blog/2023-03-30-vicuna/,
2023. 3
[34] ChenfeiWu,ShengmingYin,WeizhenQi,XiaodongWang,ZechengTang,andNanDuan. Visualchatgpt:
Talking,drawingandeditingwithvisualfoundationmodels. arXiv:2303.04671,2023. 3
[35] RuiYang,LinSong,YanweiLi,SijieZhao,YixiaoGe,XiuLi,andYingShan. Gpt4tools:Teachinglarge
languagemodeltousetoolsviaself-instruction. arXiv:2305.18752,2023. 3
[36] Google.Gemma:Introducingnewstate-of-the-artopenmodels.hhttps://blog.google/technology/
developers/gemma-open-models/,2024. 3,6
[37] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
CLawrenceZitnick. Microsoftcococaptions:Datacollectionandevaluationserver. arXiv:1504.00325,
2015. 3
[38] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,Peter
Clark,andAshwinKalyan.Learntoexplain:Multimodalreasoningviathoughtchainsforsciencequestion
answering. InNeurIPS,2022. 3
[39] XinLai,ZhuotaoTian,YukangChen,YanweiLi,YuhuiYuan,ShuLiu,andJiayaJia. Lisa:Reasoning
segmentationvialargelanguagemodel. arXiv:2308.00692,2023. 3
[40] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,2021. 3,4,6
17[41] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelfor
few-shotlearning. InNeurIPS,2022. 3
[42] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleFung,andStevenHoi. Instructblip: Towardsgeneral-purposevision-languagemodelswith
instructiontuning. arXiv:2305.06500,2023. 3,4,5,6,7
[43] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXiv:2310.03744,2023. 3,5,6,7
[44] PanZhang,XiaoyiDong,BinWang,YuhangCao,ChaoXu,LinkeOuyang,ZhiyuanZhao,Shuangrui
Ding,SongyangZhang,HaodongDuan,WenweiZhang,HangYan,XinyueZhang,WeiLi,JingwenLi,
KaiChen,ConghuiHe,XingchengZhang,YuQiao,DahuaLin,andJiaqiWang. Internlm-xcomposer:
Avision-languagelargemodelforadvancedtext-imagecomprehensionandcomposition. arXivpreprint
arXiv:2309.15112,2023. 3
[45] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,XilinWei,Songyang
Zhang,HaodongDuan,MaosongCao,WenweiZhang,YiningLi,HangYan,YangGao,XinyueZhang,
WeiLi,JingwenLi,KaiChen,ConghuiHe,XingchengZhang,YuQiao,DahuaLin,andJiaqiWang.
Internlm-xcomposer2:Masteringfree-formtext-imagecompositionandcomprehensioninvision-language
largemodel. arXivpreprintarXiv:2401.16420,2024. 3
[46] QuanSun,QiyingYu,YufengCui,FanZhang,XiaosongZhang,YuezeWang,HongchengGao,Jingjing
Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint
arXiv:2307.05222,2023. 3
[47] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,ZhengxiongLuo,YuezeWang,Yongming
Rao,JingjingLiu,TiejunHuang,etal. Generativemultimodalmodelsarein-contextlearners. arXiv
preprintarXiv:2312.13286,2023. 3,5
[48] YuyingGe,YixiaoGe,ZiyunZeng,XintaoWang,andYingShan. Plantingaseedofvisioninlarge
languagemodel. arXivpreprintarXiv:2307.08041,2023. 3
[49] YuyingGe,SijieZhao,ZiyunZeng,YixiaoGe,ChenLi,XintaoWang,andYingShan. Makingllamasee
anddrawwithseedtokenizer. arXivpreprintarXiv:2310.01218,2023. 3
[50] BinXia,ShiyinWang,YingfanTao,YitongWang,andJiayaJia.Llmga:Multimodallargelanguagemodel
basedgenerationassistant. arXivpreprintarXiv:2311.16500,2023. 3,5,15
[51] XiaoweiChi,YijiangLiu,ZhengkaiJiang,RongyuZhang,ZiyiLin,RenruiZhang,PengGao,Chaoyou
Fu,ShanghangZhang,QifengLiu,etal. Chatillusion: Efficient-aligninginterleavedgenerationability
withvisualinstructionmodel. arXivpreprintarXiv:2311.17963,2023. 8,10,15
[52] JunZhan,JunqiDai,JiashengYe,YunhuaZhou,DongZhang,ZhigengLiu,XinZhang,RuibinYuan,
GeZhang,LinyangLi,etal. Anygpt: Unifiedmultimodalllmwithdiscretesequencemodeling. arXiv
preprintarXiv:2402.12226,2024. 3,5,8,10
[53] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. ComputerScience.
https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8,2023. 3,5
[54] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeWGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,SrivatsaR
Kundurthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. LAION-5b:An
openlarge-scaledatasetfortrainingnextgenerationimage-textmodels. InNeurIPS,2022. 4,6
[55] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie. A
convnetforthe2020s. InCVPR,2022. 4
[56] PabloPernias,DominicRampas,MatsL.Richter,ChristopherJ.Pal,andMarcAubreville. Wuerstchen:
Anefficientarchitectureforlarge-scaletext-to-imagediffusionmodels,2023. 5
[57] OpenAI. Video generation models as world simulators. URL https://openai.com/research/
video-generation-models-as-world-simulators. 5
[58] PiyushSharma, NanDing, SebastianGoodman, andRaduSoricut. Conceptualcaptions: Acleaned,
hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InACL,2018. 5
18[59] OleksiiSidorov,RonghangHu,MarcusRohrbach,andAmanpreetSingh. Textcaps:adatasetforimage
captioningwithreadingcomprehension. InECCV,2020. 5,8
[60] LAIONeV.Laion/gpt4v-dataset·datasetsathuggingface.URLhttps://huggingface.co/datasets/
laion/gpt4v-dataset. 5,8
[61] KushalKafle,BrianPrice,ScottCohen,andChristopherKanan. Dvqa:Understandingdatavisualizations
viaquestionanswering. InCVPR,2018. 5
[62] stable-diffusion-prompts. URL https://www.gigasheet.com/sample-data/
stable-diffusion-prompts. 5
[63] XiangxiangChu,LimengQiao,XinyangLin,ShuangXu,YangYang,YimingHu,FeiWei,XinyuZhang,
BoZhang,XiaolinWei,etal. Mobilevlm:Afast,reproducibleandstrongvisionlanguageassistantfor
mobiledevices. arXiv:2312.16886,2023. 6,7
[64] KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,FengZhu,andRuiZhao. Shikra: Unleashing
multimodalllm’sreferentialdialoguemagic. arXiv:2306.15195,2023. 6
[65] IDEFICS. Introducingidefics:Anopenreproductionofstate-of-the-artvisuallanguagemodel. https:
//huggingface.co/blog/idefics,2023. 6
[66] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,ZhuoyiYang,Lei
Zhao,XixuanSong,etal.Cogvlm:Visualexpertforpretrainedlanguagemodels.arXiv:2311.03079,2023.
6
[67] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,and
MarcusRohrbach. Towardsvqamodelsthatcanread. InCVPR,2019. 6,7,8,11
[68] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,WeiLin,
JinruiYang,XiawuZheng,etal. Mme: Acomprehensiveevaluationbenchmarkformultimodallarge
languagemodels. arXiv:2306.13394,2023. 6,7
[69] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,and
LijuanWang. Mm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities. arXiv:2308.02490,
2023. 6,7,8
[70] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,HannanehHajishirzi,HaoCheng,Kai-Wei
Chang,MichelGalley,andJianfengGao. Mathvista:Evaluatingmathematicalreasoningoffoundation
modelsinvisualcontexts. InICLR,2024. 6,7
[71] PaddleOCR. Awesomemultilingualocrtoolkitsbasedonpaddlepaddle. URLhttps://github.com/
PaddlePaddle/PaddleOCR. 11
19