ECoDepth: Effective Conditioning of Diffusion Models
for Monocular Depth Estimation
SurajPatni* AradhyeAgarwal* ChetanArora
IndianInstituteofTechnologyDelhi
https://ecodepth-iitd.github.io
Abstract
In the absence of parallax cues, a learning based sin-
(a)Sun-RGBD[42]
gle image depth estimation (SIDE) model relies heavily
on shading and contextual cues in the image. While this
simplicity is attractive, it is necessary to train such mod-
elsonlargeandvarieddatasets,whicharedifficulttocap-
(b)iBims1[19]
ture. It has been shown that using embeddings from pre-
trainedfoundationalmodels,suchasCLIP,improveszero
shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global im- (c)DIODE[49]
agepriorsgeneratedfromapre-trainedViTmodeltopro-
vide more detailed contextual information. We argue that
the embedding vector from a ViT model, pre-trained on
a large dataset, captures greater relevant information for (d)HyperSim[35]
SIDEthantheusualrouteofgeneratingpseudoimagecap-
Figure1.Qualitativeresultsacrossfourdifferentdatasets,demon-
tions, followedbyCLIPbasedtextembeddings. Basedon
strating the zero-shot performance of our model trained only on
thisidea,weproposeanewSIDEmodelusingadiffusion
theNYU Depth v2dataset. Correspondingquantitativeresults
backbone which is conditioned on ViT embeddings. Our arepresentedinTable3. ThefirstcolumndisplaysRGBimages,
proposed design establishes a new state-of-the-art (SOTA) thesecondcolumndepictsgroundtruthdepth, andthethirdcol-
forSIDEonNYU Depth v2dataset, achievingAbsRel umnshowcasesourmodel’spredicteddepths. Additionalimages
errorof0.059(14%improvement)comparedto0.069bythe foreachdatasetareavailableintheSupplementaryMaterial.
currentSOTA(VPD).AndonKITTIdataset,achievingSq
Rel error of 0.139 (2% improvement) compared to 0.142
by the current SOTA (GED). For zero shot transfer with a typicallyformulatedintwoflavors:metricdepthestimation
model trained on NYU Depth v2, we report mean rela- (MDE), and relative depth estimation (RDE). As the names
tiveimprovementof(20%,23%,81%,25%)overNeWCRF suggest, MDE deals with estimation in physical units such
on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, asmeters,whereas,RDEtechniquesfocusonrelativedepth
compared to (16%, 18%, 45%, 9%) by ZoEDepth. The only,andrequireaper-imageaffinetransformationaspost-
codeisavailableatthislink. processingtoconverttoaphysicalunit.
Conventionalgeometrictechniquesfordepthestimation
typically rely on feature correspondence, parallax, and tri-
1.Introduction
angulationfromtwoormoreviews. However,theproblem
becomes ill-posed for estimation from a single view. In-
Single Image Depth Estimation (SIDE) is the task of pre-
tuitively, depth map is a 3D representation of the scene,
dicting per pixel depth using a single RGB image from a
whereas an RGB image is a 2D projection of the scene.
monocularcamera.Itisafundamentalproblemincomputer
Hence,itcannotbeuniquelydeterminedfromasingleRGB
vision with applications in several domains, viz robotics,
image. Therefore,learning-basedSIDEmodelsrelyonvi-
autonomousdriving,andaugmentedreality.Theproblemis
sual cues like ‘shape from shading’ and other contextual
*Equalcontribution. priorsforper-pixeldepthprediction.
4202
raM
72
]VC.sc[
1v70881.3042:viXraA data-driven approach makes the SIDE pipeline sim- toconditionthediffusionbackbone.
pler,butalsomakesthelearntmodeldependentonthequal-
Contributions. Thekeycontributionsofthisworkare:
ity of training data. It has been observed that such mod-
(1) WeproposeanewmodelforMDEinaSIDEtask. The
els overfit on a particular training distribution/domain and
proposed model uses a conditional diffusion architec-
failtogeneralizeonunseendata. Thisisespeciallytruefor
ture with the semantic context being supplied through
MDEmodelswhentherangeofdepthinthetrainingdataset
embeddings generated using a ViT model. Achiev-
islimited. Hence, trainingonmultipledatasets, withwide
ing a new state-of-the-art (SOTA) performance, our
variationsindepthrangeshasbeenproposed[4,34].
methodoutperformsexistingapproachesonbenchmark
On the other hand, development of large foundational datasets, including the NYU Depth v2 indoor and
models(LFMs)inrecentyearshasalteredthepreferredde- KITTI outdoor datasets. Notably, we report a signif-
sign approach for many computer vision problems. These icant improvement of 14% in absolute relative error,
huge models are trained using extensive datasets of unla- achieving 0.059 compared to the current SOTA (VPD)
beled images and learning objectives that are agnostic to performance of 0.069 on NYU Depth v2. And we
specifictasks.Thelearntembeddingsfromsuchpre-trained report an improvement of 2% in square relative error,
models have been shown to help generalization and zero- achieving 0.139 compared to the current SOTA (GED)
shottransferinmanyapplications. Weareawareofatleast performanceof0.142onKITTI.
two works for MDE in SIDE problem that have appeared (2) We show, qualitatively as well as quantitatively, that
in the last few months and make use of such foundational using ViT embeddings to provide semantic context is
models. VPD[54]usesatext-to-imagediffusionmodelpre- a better alternative to generating pseudo captions and
trained on LAION-400M [39] dataset having large-scale then using its CLIP embeddings to condition a SIDE
image-text pairs as the backbone. The model prompts the model. In contrast to TADP [20], which uses pseudo
denoisingUNetwithtextualinputstomakethevisualcon- captions, but only achieves a RMSE of 0.225 on NYU
tentsinteractwiththetextprompts. Sincetheproblemfor- Depth v2,wereportalower,andanewSOTA,error
mulation doesn’t include text description as the input, the of0.218(VPD[54]achieves0.254).
model generates simple descriptions such as ‘‘A photo (3) We show that providing ViT conditioning, helps our
of a {scene name}’’basedonthescenelabelgiven model perform better in a zero shot transfer task.
intheNYU Depth v2dataset. Anotherwork,TADP[20] ZoEDepth [4], the current SOTA for zero-shot trans-
improvesuponVPD.Insteadofsimpleimagedescriptions, fer, reports an improvement of (16%, 18%, 45%,
TADP uses BLIP-2 [23] to generate image captions, and 9%)overNeWCRFon(Sun-RGBD,iBims1,DIODE,
then uses CLIP [31] embeddings of the pseudo-caption to HyperSim) datasets, after training their model on 12
conditionthediffusionmodel. other datasets and NYU Depth v2. In contrast, we
We view the above works as providing robust semantic only train on NYU Depth v2, and report a much
contexttoLFMsfortheactualtaskathand,whichhelpsin largerimprovementof(21%,23%,81%,25%).
visualrecognitioningeneral,aswellasSIDE.Whilewedo
agreewiththebroadmotivationsoftheseworks, theques- 2.RelatedWork
tionthatweaskisifpseudo-captionsarethemosteffective
Traditional Methods. Earlier techniques for SIDE have
way to provide the semantic context. Textual descriptions
used Markov Random Fields [47], non-parametric depth
ofanimagetypicallyfocusonlargesalientobjectsandem-
sampling [18], and structural similarity with prior depth
phasizeontheirrelationships. Ontheotherhand,largevi-
map[13]topredictpixel-wisedepth.
sionmodelsforimageclassification,typicallycontainrep-
resentation for even smaller objects present in the scene. DeepLearningTechniquesforSIDE.Moderntechniques
Even when a single object is present in the scene, the rep- have approached the problem as a dense regression prob-
resentationstypicallycaptureuncertaintiesandambiguities lem. CNNs have been the dominant architecture for the
inherent in the scene. Hence, we posit that using embed- SIDE in the last decade, with global-local network stack
dings from a transformer model, pretrained on a large im- [9], and multi-scale [21], or encoder-decoder architecture
agedatasetunrelatedtotheSIDEtask,capturesmorerele- [11] as some of the popular solution strategies. Recently,
vantinformation,andisabetteralternativetousingpseudo- PixelFormer [1] used a transformer-based encoder-
captions’CLIPembedding. Sincediffusion-basedmodels decoder architecture with skip connections from encoders
haveshowntheirsuperiorityforthedensepredictiontasks to decoders. MIM [51] proposed masked image modeling
in recent works [8, 16, 20, 38, 54]. Hence, we propose a asageneral-purposepre-trainingforgeometricandmotion
diffusionbackboneforourmodel,alongwithanovelCIDE tasks such as SIDE and pose estimation. Similarly, AiT
modulewhichemploysViT[7]toextractsemanticcontext [28] used mask augmentation and proposed soft tokens to
embeddings. These embeddings are subsequently utilized generalize visual prediction tasks. While earlier works ei-Hierarchical Feature
RGB Image Latent Space Maps C fo en ac tua rt ee n ma ate pd fU eap ts ua rm e p mle ad p Depth Metric Depth
Regressor
Image
Encoder Diffusion Backbone Upsampling conv ,
Decoder conv ,
Proposed Module
without text priors CIDE
Legend
Concat across channel dimension
Existing works scalar-vector multiplication
element-wise addition
Image title
-----------or------------ CLIP Freezed while training
Captioner Sigmoid function
(BLIP) Learnable class embeddings
Image caption generator
CIDE: Comprehensive Image Detail Embedder
Figure 2. An overview of our proposed model: The latent representation of the input image undergoes a diffusion process, which is
conditionedbyourproposedCIDEmodule. WithintheCIDEmodule,theinputimageisfedthroughthefrozenViTmodel. Fromthis,a
linearcombinationofthelearntembeddingsiscomputed,whichistransformedtogeneratea768-dimensionalcontextualembedding.This
embeddingisutilizedtoconditionthediffusionbackbone.Subsequently,hierarchicalfeaturemapsareextractedfromtheUNet’sdecoder
whichareconcatenatedandprocessedthroughadepthregressortogeneratethedepthmap.
therfocusedonMDEforaspecificdatasetorRDEforgener- inatedcomputervisionproblemsduetotheirabilitytocap-
alizationonmultipledatasets,ZoEDepth[4]hasproposed turespatialhierarchiesinimagedata. However,sucharchi-
a generalized method for MDE that performs well in zero- tectures were constrained due to their ability to learn spa-
shot transfer. We outperform [4] by a large margin even tiallylocalizedfeaturesonly. Transformerarchitecturehas
whentrainingonasingledataset-NYU Depth v2forin- aweakerinductivebiasandallowsViTtolearnlong-range
door scenes or KITTI for outdoor scenes. Whereas, [4] dependencies, and robust, generalizable features. ViT ar-
uses12datasetsforpre-trainingandthenfine-tunesonNYU chitectures have replaced CNNs for SOTA performance on
Depth v2orKITTIforzeroshottransfer. most computer vision tasks in recent years. We use a pre-
Diffusion-based Methods with Pretraining on Large trained ViT model for providing semantic information to
Datasets. Recently, many techniques for SIDE has used thediffusionbackboneinourmodel.
diffusionarchitectures.Thesetechniques[8,16,20,38,54],
3.ProposedMethodology
exploit prior knowledge acquired by pretraining on large
datasets like LAION-400M [39], which consists of 400
3.1.Preliminaries
millionimage-textpairs.Incontrast,depthdatasets,suchas
NYU Depth v2andKITTI,containaround20-30thou- Problem Formulation. The objective of single image
sand image-depth pairs. DepthGen [38] and DDP [16] depth prediction task is to predict continuous values, de-
workonanoise-to-depthmapparadigmanduseimagesfor noted as y ∈ RH×W, for every pixel present in the input
conditionalguidanceofthediffusionprocess. DepthGen RGBimage,x∈[0,255]3×H×W. HereH andW represent
employs self-supervised pretraining on tasks like coloriza- theheightandwidthrespectivelyoftheinputimage.
tion,inpainting,andJPEGartifactremoval,followedbysu-
Diffusion Model. Diffusion models are a class of gener-
pervisedtrainingonindoorandoutdoordatasets[6,12,26, ative models that progressively inject noise into the input
27, 48]. DDP [16] decouples the image encoder and map data(forwardpass)andthenlearntoreconstructtheoriginal
decoder,allowingtheimageencodertorunjustonce,while datainareversedenoisingprocess(reversepass).Thereare
the lightweight map decoder is run multiple times. VPD threeformulationsofdiffusionmodels: denoisingdiffusion
[54] and TADP [20] use denoising UNet [37] as a back- probabilisticmodels(DDPMs)[14],score-basedgenerative
bone to extract the rich features at multiple scales. Also, models [43, 44], and those based on stochastic differential
theyutilizetextinsteadofimageforconditioningthediffu- equations [45, 46]. DDPMs are of relevance to our paper,
sionbackbone. and are described below. The architecture of DDPMs con-
VisionTransformerforSceneUnderstanding.Thetrans- sistsoftwoMarkovchains: aforwardchainthataddsnoise
former architecture was initially proposed for NLP tasks tothedata, andareversechainthatconvertsnoisebackto
[50], but introduced to the computer vision community as data by learning transition kernels parameterized by deep
VisionTransformer(ViT)in[7]. Priortothis,CNNsdom- neuralnetworks. Formally,theforwardpassismodeledasaMarkovprocess: 𝑥 𝑥
(cid:16) (cid:112) (cid:17)
P(z t |z t−1)=N z t; 1−β tz t−1,β tI . (1) 𝑧 𝑡 𝑧 𝑡 ℰ
Here z denotes the random variable at the tth time step, 𝒞
t
N(z;µ,σ) denotes Gaussian probability distribution, and
𝑧 𝑧
β is the noise schedule. The above equation leads to the 0 0
t
analyticformofP(z |z ),∀t∈{0,1,...,T}:
t 0 𝑦 𝑦
(cid:113) (cid:113)
z = β¯z + 1−β¯ϵ, (2) (a) (b)
t t 0 t
Figure3.(a)ProbabilisticgraphicalmodelcorrespondingtoVPD.
where β¯ = (cid:81)t β and ϵ ∼ N(0,I). The model then
t s=1 s (b)Thesamecorrespondingtoourformulation.Here,Crepresents
gradually removes noise by executing a learnable Markov the semantic embedding derived from our CIDE module. This
chaininthereversetimedirection,parameterizedbyanor- embeddingisinternallygeneratedbypassingxthroughtheViT,
mal prior distribution P(z T) = N (z T;0,I) and a learn- resulting in E. Subsequently, E undergoes further processing to
abletransitionkernelP (z |z )givenby: yieldC,whichisthenutilizedintheconditionaldiffusionmodule
θ t−1 t
implementing P(z | z ,C). The output of the conditional dif-
P (z |z )=N (z ;µ (z ,t),Σ (z ,t)). (3) 0 t
θ t−1 t t−1 θ t θ t fusionmoduleisfedintotheDepthRegressormodulewithinour
architecture,implementingP(y|z ).
Thegoalofthetrainingprocessistoapproximatelymatch 0
thereverseMarkovchainwiththeactualtimereversalofthe
forwardMarkovchain. Mathematically,parameterθ isad-
whereyandxdenoteoutputdepth,andinputimagerespec-
justed so that the joint distribution of the reverse Markov
tively. In our diffusion formulation, we predict z which
chain P (z ,z ,··· ,z ) := P(z )(cid:81)T P (z |z ) 0
θ 0 1 T T t=1 θ t−1 t is then used to predict pixel-wise depth, y. It has been
closely approximates that of the forward Markov chain
shown that noise prediction in a diffusion model, ϵ , can
P(z ,z ,··· ,z ) := P(z )(cid:81)T P(z |z ). This is θ
0 1 T 0 t=1 t t−1 be seen as predicting the gradient of the density function,
achievedbyminimizingthefollowingloss: ∇ logP(z ). Hence, overallarchitectureforSIDEusing
zt t
(cid:104) (cid:105) adiffusionarchitecturecanbeseenasfactorizingthecondi-
E t∼U[1,T],z0∼P(z0),ϵ∼N(0,I) ∥ϵ−ϵ θ(z t,t)∥2 , (4)
tionalprobability,asshownintheprobabilisticgraphmodel
inFig.3a. Toutilizeadditionalsemanticcontextgenerated
wherez iscomputedusingEq.(2),andϵ ispredictedus-
t θ
ing a neural network, typically a UNet architecture [37].
fromaViTmodel,weconditionitontheViTembeddings
asshowninFig.3b. Mathematically,wemodel:
Inaconditionaldiffusionmodelϵ (z ,t)getsreplacedby
θ t
ϵ (z ,t,C),whereC isaconditioningvariable.
θ t
3.2.OurArchitecture P(y|x,E)=P(y|z )P(z |z ,C)P(z |x)P(C |x),
0 0 t t
Image Encoder and Latent Diffusion. Diffusion models where
typically take large number of time steps to train, and are P(C |x)=P(C |E)P(E |x). (5)
difficult to converge. Recently, [36] proposed a new dif-
fusion with improved convergence properties and is called Here, the first term, P(y | z ), is implemented through
0
“stable-diffusion”. The key idea is to perform the diffu- the Depth Regressor module explained earlier. Similarly,
sioninlatentspace,withlatentembeddinglearntseparately P(z | x) is implemented using the VAE’s Encoder as de-
t
through a variational autoencoder (VAE). The Encoder of scribedearlier. Wegenerateconditionalinformation,C us-
VAEfirsttransformstheinputimagexofsize(H,W)tola- ing our Comprehensive Image Detail Embedding module
tentspace,thenwefollowlatentdiffusionformulationand (hereafter CIDE, and explained below). The CIDE mod-
utilizetheUNetusedinStableDiffusion[36]. Utilizingla- ule takes x as input and generates embedding vector C (of
tent diffusion formulation enables our architecture to cap- dimension 768 in our design) as the output, thus, imple-
turemulti-resolutionfeatures. Hence,weaggregatethefea- mentingP(C | x)giveninEq.(5). WeuseE todenotethe
turemapsfromdifferentlayersoftheUNetmodule(imple- embedding vector of a ViT module, and P(E | x) is im-
mentingconditionaldiffusion)bybringingthemallto1/4th plementedthroughtheViT.P(C |E)isimplementedusing
resolutionofthelatentspace,resultinginafeaturemapof downstream modules in CIDE consisting of learnable em-
size8e×H/32×W/32. beddings. The second term, P(z | z ,C), is implemented
0 t
Exploiting Semantic Context with Conditional Diffu- usingconditionaldiffusion,
sion. Recallthatweformulatedthesingleimagedepthesti- Comprehensive Image Detail Embedding (CIDE) Mod-
mationasadenseregressionproblem,predictingP(y | x), ule. Asdescribedearlier,webelieveusingpseudo-captionsTable1. ResultsonIndoorNYU Depth v2[27]Dataset. Resultsthatareboldperformbest. ↑meansthemetricshouldbehigher,↓
indicatelowerisbetter.Theevaluationusesanupperboundof10metersonthegroundtruthdepthmap.Allthenumbersforotherworks
havebeentakenfromthecorrespondingpapers. ForMIM,andZoEDepthwehaveusedSwinV2-L1K,andZoeDepth-M12-Nversions
respectively.WeseeanoverallimprovementagainstSOTAonallthemetricsusedforevaluation.
Method Venue AbsRel↓ RMSE↓ log ↓ SqRel↓ δ ↑ δ ↑ δ ↑
10 1 2 3
Eigenetal.[9] NIPS’14 0.158 0.641 - - 0.769 0.950 0.988
DORN[10] CVPR’18 0.115 0.509 0.051 - 0.828 0.965 0.992
SharpNet[32] ICCV’19 0.139 0.502 0.047 - 0.836 0.966 0.993
Chenetal.[5] IJCAI-19 0.111 0.514 0.048 - 0.878 0.977 0.994
BTS[22] Arxiv’19 0.110 0.392 0.047 0.066 0.885 0.978 0.994
AdaBins[2] CVPR’21 0.103 0.364 0.044 - 0.903 0.984 0.997
DPT[33] ICCV’21 0.110 0.357 0.045 - 0.904 0.988 0.998
P3Depth[30] CVPR’22 0.104 0.356 0.043 - 0.898 0.981 0.996
NeWCRFs[53] CVPR’22 0.095 0.334 0.041 0.045 0.922 0.992 0.998
SwinV2-B[24] CVPR’22 0.133 0.462 0.059 - 0.819 0.975 0.995
SwinV2-L[24] CVPR’22 0.112 0.381 0.051 - 0.886 0.984 0.997
Localbins[3] ECCV’22 0.099 0.357 0.042 - 0.907 0.987 0.998
Junetal.[17] ECCV’22 0.098 0.355 0.042 - 0.913 0.987 0.998
PixelFormer[1] WACV’23 0.090 0.322 0.039 0.043 0.929 0.991 0.998
DDP[16] ICCV’23 0.094 0.329 0.040 - 0.921 0.990 0.998
MIM[51] CVPR’23 0.083 0.287 0.035 - 0.949 0.994 0.999
AiT[28] ICCV’23 0.076 0.275 0.033 - 0.954 0.994 0.999
ZoeDepth[4] Arxiv’23 0.075 0.270 0.032 0.030 0.955 0.995 0.999
VPD[54] ICCV’23 0.069 0.254 0.030 0.027 0.964 0.995 0.999
Ours CVPR’24 0.059 0.218 0.026 0.013 0.978 0.997 0.999
RGB GT ZoeDepth[4] VPD[54] Ours
Figure 4. Visual Comparison on NYU Depth v2 Indoor Dataset. Note, our method’s ability to delineate objects in terms of their
depth,suchasthetablelampinRow5,evenwhensuchinformationisabsentfromthegroundtruthdepthmap.Table2. PerformanceontheOutdoorKITTI[12]Dataset. PleaserefertothecaptionofTab.1fornotationdetails. ForZoEDepth
results,weusetheZoeDepth-M12-Kversionfollowingtheauthors’recommendation.Ininstanceswhereresultsforcertainmethodswere
notreportedintherespectiveworks,denotedby“-”,andthecodeisunavailable,wewereunabletogeneratethemissingnumbers.Despite
thesaturationofresultsontheoutdoorKITTIdataset,ourmethodconsistentlyachievescomparableorsuperiorperformancetothestate-
of-the-art(SOTA)acrossallmetrics.VPD[54]cannotbetrainedonKITTIdatasetasitrequiredaperimagetextlabelwhichisnotpresent
inKITTI.Thesymbol†indicatesthatthemethodutilizesadditionalinformationbeyondRGB.
Method Venue AbsRel↓ SqRel↓ RMSE ↓ RMSE↓ δ ↑ δ ↑ δ ↑
log 1 2 3
Eigenetal.[9] NIPS’14 0.203 1.517 0.282 6.307 0.702 0.898 0.967
DORN[10] CVPR’18 0.072 0.307 0.120 2.727 0.932 0.984 0.994
BTS[22] Arxiv’19 0.059 0.241 0.096 2.756 0.956 0.993 0.998
AdaBins[2] CVPR’21 0.067 0.190 0.088 2.960 0.949 0.992 0.998
DPT[33] ICCV’21 0.060 - 0.092 2.573 0.959 0.995 0.996
P3Depth[30] CVPR’22 0.071 0.270 0.103 2.842 0.953 0.993 0.998
NeWCRFs[53] CVPR’22 0.052 0.155 0.079 2.129 0.974 0.997 0.999
PixelFormer[1] WACV’23 0.051 0.149 0.077 2.081 0.976 0.997 0.999
ZoeDepth[4] Arxiv’23 0.054 0.189 0.083 2.440 0.97 0.996 0.999
DDP[16] ICCV’23 0.050 0.148 0.076 2.072 0.975 0.997 0.999
URCDC[40] ToM’23 0.050 0.142 0.076 2.032 0.977 0.997 0.999
IEBins[41] NeurIPS’23 0.050 0.142 0.075 2.011 0.978 0.998 0.999
MIM[51] CVPR’23 0.050 0.139 0.075 1.966 0.977 0.998 1.000
GEDepth[52]† ICCV’23 0.048 0.142 0.076 2.044 0.976 0.997 0.999
Ours CVPR’24 0.048 0.139 0.074 2.039 0.979 0.998 1.000
RGB NewCRF[53] Pixelformer[1] IEbins[41] URCDCdepth[40] Ours
Figure5.VisualComparisononKITTIOutdoorDataset.
to generate the semantic context has limited utility, as the Thisresultingembeddingundergoesalineartransformation
textual descriptions typically focus on large salient ob- toyieldasemanticcontextvectorofdimension768,which
jects only. Instead we propose our CIDE module which isthenpassedtoconditionaldiffusionmodule.
use embeddings from a pre-trained ViT, and extract de-
Depth Regressor. The output feature map undergoes
tailedsemanticcontextfromtheseembeddings. Forthiswe
through an Upsampling Decoder, comprised of deconvo-
take 1000 dimensional logit vector from ViT, and pass it
lution layers, followed by a Depth Regressor. The Depth
throughatwolayerMLPwhichconvertsittoa100dimen-
Regressor is essentially a two-layer convolutional neural
sionalvector. Subsequently,weemploythisvectortocom-
network (CNN), with the initial layer having dimensions
pute the linear combination of 100 learnable embeddings.
Conv(3×3),192,andthesubsequentlayerConv(3×3),1.Table3.Quantitativeresultsforzero-shottransfertofourunseenindoordatasets.mRI denotesthemeanrelativeimprovementwith
θ
respecttoNeWCRFsacrossallmetrics(δ ,REL,RMSE).Evaluationdepthiscappedat8mforSUNRGB-D,10mforiBimsandDIODE
1
Indoor,and80mforHyperSim.Bestresultsareinbold,secondbestareunderlined.OurmRI outperformsallmethodsacrossalldatasets
θ
byalargemargin.†denotesthatZoeDistrainedon12datasetsandourmethodistrainedonlyonNYUv2.
SUNRGB-D iBims-1Benchmark DIODEIndoor HyperSim
Method δ1↑ REL↓RMSE↓ mRIθ↑ δ1↑ REL↓RMSE↓ mRIθ↑ δ1↑ REL↓RMSE↓ mRIθ↑ δ1↑ REL↓RMSE↓ mRIθ↑
BTS[22] 0.740 0.172 0.515 -14.2% 0.538 0.231 0.919 -6.9% 0.210 0.418 1.905 2.3% 0.225 0.476 6.404 -8.6%
AdaBins[2] 0.771 0.159 0.476 -7.0% 0.555 0.212 0.901 -2.1% 0.174 0.443 1.963 -7.2% 0.221 0.483 6.546 -10.5%
LocalBins[3] 0.777 0.156 0.470 -5.6% 0.558 0.211 0.880 -0.7% 0.229 0.412 1.853 7.1% 0.234 0.468 6.362 -6.6%
NeWCRFs[53] 0.798 0.151 0.424 0.0% 0.548 0.206 0.861 0.0% 0.187 0.404 1.867 0.0% 0.255 0.442 6.017 0.0%
VPD[54] 0.861 0.121 0.355 14.7% 0.627 0.187 0.767 11.5% 0.480 0.392 1.295 63.4% 0.333 0.531 5.111 8.5%
ZoeD-M12-N†[4] 0.864 0.119 0.346 16.0% 0.658 0.169 0.711 18.5% 0.376 0.327 1.588 45.0% 0.292 0.410 5.771 8.6%
Ours 0.885 0.112 0.319 20.5% 0.688 0.163 0.664 23.1% 0.545 0.344 1.164 81.3% 0.394 0.442 4.739 25.2%
Image Ground Truth VPD Ours
40.62%
 23.07%
 9.67%
 5.75%
 5.40%
 2.48%
 2.14%
 1.43%
 1.20%
 0.81%
 0.55%
 0.46%
 0.45%
 0.39%

desk desktop,
 screen, monitor file, computer
 printer notebook
 laptop
 bookcase ring-binder television loudspeaker computer
computer CRT screen file cabinet mouse keyboard
Image Ground Truth VPD Ours
95.84%
 1.75%
 0.35%
 0.23%
 0.16%
 0.11%
 0.11%
 0.10%
 0.09%
 0.08%
 0.07%
 0.06%
 0.05%
 0.04%

dining table, china cabinet,
 home bookcase folding shoji window desk library entertainment sliding restaurant table lamp television
board china closet theatre chair shade center door
Figure6.VisualizationofimprovementsoverVPD[54]inourmodelduetoViTembeddingspasseddownasconditionalvectors(inblue
andgreen). Intheaboveimages,ViTdetectsthedesktopcomputer(firstimage)andtablelamp(secondimage)withhighprobability,
andtheyarethusbetterdetectedbyourmodel.Additionalvisualizationsareprovidedinthesupplementarymaterial.
4.ExperimentsandResults KITTI dataset is a widely used outdoor benchmark for
monocular depth estimation, containing over 24k densely
Datasets and Evaluation. We use NYU Depth v2 [27] labeled pairs of RGB and depth images. The dataset cov-
and KITTI [12] as the primary datasets for training. The ersoutdoordrivingscenariosandincludesvaryinglighting
NYU Depth v2 dataset is a widely used indoor bench- conditions,weather,andocclusions.Thegroundtruthdepth
markformonoculardepthestimation, containingover24k maps are obtained using a Velodyne LiDAR sensor and
denselylabeledpairsofRGBanddepthimagesinthetrain are provided at a resolution of 1242 × 375. To demon-
setand654inthetestset. Thedatasetcoversawiderange strate generalizability, we evaluate zero-shot performance
of indoor scenes and includes challenging scenarios such onthefollowingdatasets: Sun-RGBD[42],iBims1[19],
as reflective surfaces, transparent objects, and occlusions. DIODE[49],andHyperSim[35]. Whereas,themainpa-
Thegroundtruthdepthmapsareobtainedusingastructured percontainsmostlyquantitative,andonlysomerepresenta-
lightsensorandareprovidedataresolutionof640×480.Table4.Effectivenessofdifferentembeddingsforguidingthedif- 4.3.AblationStudy
fusionprocessfordepthestimationonNYU Depth v2dataset.
Inthethirdrow,ratherthanusingpseudocaptionembeddingsgen- EffectofContextualInformation.
eratedfromthescenelabel(asimplementedbyVPD[54]),wepro- As highlighted in the motivation, a key observation of
videtheone-hotvectorrepresentingthescenelabelasacondition thisstudyistherichnessofinformationcontainedwithinthe
to the diffusion model. We observe a slight improvement in the outputprobabilityvectoroftheViT,surpassingthetextual
metrics. embeddingsemployedincurrentstate-of-the-art(e.g.,VPD
[54]). Tocomparewiththeutilizationofscenelabelinfor-
Embeddings RMSE↓ AbsRel↓ δ 1 ↑ mation (as implemented by VPD), we construct the condi-
Scenelabelemb. [54] 0.254 0.069 0.964 tioningembeddingasaone-hotvectorusingthescenelabel,
Textcaptionemb. [20] 0.225 0.062 0.976 andsubsequentlytransformitusinganMLP.Asillustrated
One-hotvectoremb. 0.244 0.067 0.968
inTab.4,weobserveaslightimprovementoverVPD,sug-
Proposedsceneemb. 0.218 0.059 0.978 gesting that pseudo-captions are less effective than explic-
itly providing the class label. Furthermore, our proposed
methodsurpassesbothapproaches.
tivevisualresults,detailedvisualresultsoneachdatasetare Qualitative Results. Perhaps the most important task
includedinthesupplementary. wouldbetoverifythatuseofrichprobabilityvectorinstead
of text embeddings actually results in an improvement in
ImplementationDetails. Ourmodelisimplementedusing
depth as a direct consequence. We do this by considering
PyTorch[29]. Foroptimization,wehaveusedAdamWop-
the top few objects predicted by the ViT and correlating
timizer [25] with β values of 0.9 and 0.999, a batch size
0 thiswiththedepthpredictedattheseobjects. Wehypothe-
of 32, and a weight decay of 0.1. We train our model for
sisewhenaparticularclasssaydogispredictedwithahigh
25epochsforbothKITTIandNYU Depth v2datasets,
probability (hence there must be a dog in the image), then
withaninitiallearningrateof3×10−5.Wefirstlinearlyin-
the corresponding depth must also be more accurate. We
creaselearningrateto5×10−4,andthenlinearlydecrease
showthisinFig.6.
acrosstrainingiterations. Weuse usualdataaugmentation
techniques,includingrandomhueaddition,horizontalflip- 5.Conclusion
ping, changing the image brightness, and Cut Depth [15].
Our model takes approximately 21 minutes per epoch to WepresentedanewarchitectureblockComprehensiveIm-
trainusing8NVIDIAA100GPUs. ageDetailEmbedding(CIDE)moduleforrobustmonocular
depth estimation in this paper. Our key idea is to do away
4.1.ComparisononBenchmarkDatasets withintermediatetextgenerationforusewithfoundational
model. Instead, we aim to use richer class-wise probabil-
ComparisononNYU Depth v2. Tab.1showsthecom- ity generated by a classification model, such as ViT. The
parisonofourproposedmethodwithSOTAmethodsonthe
hypothesisisthat,whiletextualembeddingtypicallyhigh-
indoor NYU Depth v2 dataset [27]. We achieve a new lightsalientobjects,class-wiseprobabilityvectorpreserves
stateoftheartonthisdataset. Ourmethodsperformbetter more details, including smaller objects in the background
thanthepreviousSOTA([54]))byalargemarginof14%in
also. We implement the idea using proposed module cas-
termsofRMSE.Fig.4providesaqualitativecomparisonon
caded with a conditional diffusion pipeline for monocular
thedataset. depthestimation. Wedemonstratedtheeffectivenessofour
ComparisononKITTI.Tab.2showsthecomparisonwith approachonseveralbenchmarkdatasetsandshowedthatit
various methods on the KITTI dataset [12]. Fig. 5 shows outperformsSOTAmethodsbyasignificantmargin.
the qualitative results. Unlike NYU Depth v2 dataset, Acknowledgement. WethankKartikAnandforassistance
KITTI is an outdoor dataset. On this dataset also, we with the experiments. This work has been supported by
achievesimilarorbetterperformancethanallexistingstate- Staqu Technologies Pvt. Ltd., and Department of Science
of-the-arttechniques. andTechnology(DST),GovernmentofIndia.
4.2.GeneralizationandZeroShotTransfer
References
Unlike state of the art on zero short transfer (ZoEDepth
[1] AshutoshAgarwalandChetanArora.Attentionattentionev-
[4]),whichrequiringtrainingonmanydatasets(12intheir
erywhere: Monocular depth prediction with skip attention.
case) for effective zero shot transfer, we show that our InProceedingsoftheIEEE/CVFWinterConferenceonAp-
model generalizes well to other unseen dataset even when plications of Computer Vision (WACV), pages 5861–5870,
trainedonasingleNYU Depth v2dataset. Tab.3shows 2023. 2,5,6
quantitativeresultstobackourfindings. [2] ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka.Adabins:Depthestimationusingadaptivebins. InProceed- [16] YuanfengJi,ZheChen,EnzeXie,LanqingHong,XihuiLiu,
ingsoftheIEEE/CVFConferenceonComputerVisionand ZhaoqiangLiu,TongLu,ZhenguoLi,andPingLuo. Ddp:
PatternRecognition(CVPR),pages4009–4018,2021. 5,6, Diffusionmodelfordensevisualprediction. arXivpreprint
7 arXiv:2303.17559,2023. 2,3,5,6
[3] ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka. [17] JinyoungJun, Jae-HanLee, ChulLee, andChang-SuKim.
Localbins:Improvingdepthestimationbylearninglocaldis- Depthmapdecompositionformonoculardepthestimation.
tributions. In European Conference on Computer Vision, InEuropeanConferenceonComputerVision,pages18–34.
pages480–496.Springer,2022. 5,7 Springer,2022. 5
[4] ShariqFarooqBhat,ReinerBirkl,DianaWofk,PeterWonka, [18] KevinKarsch,CeLiu,andSingBingKang. Depthtransfer:
andMatthiasMu¨ller. Zoedepth: Zero-shottransferbycom- Depthextractionfromvideousingnon-parametricsampling.
biningrelativeandmetricdepth,2023. 2,3,5,6,7,8,13, IEEEtransactionsonpatternanalysisandmachineintelli-
14,15,16,17 gence,36(11):2144–2158,2014. 2
[5] XiaotianChen,XuejinChen,andZheng-JunZha.Structure- [19] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and
awareresidualpyramidnetworkformonoculardepthestima- MarcoKorner. Evaluationofcnn-basedsingle-imagedepth
tion. arXivpreprintarXiv:1907.06023,2019. 5 estimationmethods. InProceedingsofthe EuropeanCon-
[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ferenceonComputerVision(ECCV)Workshops,pages0–0,
ber, Thomas Funkhouser, and Matthias Nießner. Scannet: 2018. 1,7,17
Richly-annotated 3d reconstructions of indoor scenes. In [20] Neehar Kondapaneni, Markus Marks, Manuel Knott,
ProceedingsoftheIEEEconferenceoncomputervisionand Roge´rio Guimara˜es, and Pietro Perona. Text-image
patternrecognition,pages5828–5839,2017. 3 alignment for diffusion-based perception. arXiv preprint
[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, arXiv:2310.00031,2023. 2,3,8
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [21] IroLaina,ChristianRupprecht,VasileiosBelagiannis,Fed-
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- ericoTombari, andNassirNavab. Deeperdepthprediction
vain Gelly, et al. An image is worth 16x16 words: Trans- withfullyconvolutionalresidualnetworks. In2016Fourth
formers for image recognition at scale. arXiv preprint international conference on 3D vision (3DV), pages 239–
arXiv:2010.11929,2020. 2,3 248.IEEE,2016. 2
[8] YiqunDuan,XiandaGuo,andZhengZhu. Diffusiondepth: [22] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and
Diffusion denoising approach for monocular depth estima- Il Hong Suh. From big to small: Multi-scale local planar
tion,2023. 2,3 guidance for monocular depth estimation. arXiv preprint
[9] DavidEigen,ChristianPuhrsch,andRobFergus.Depthmap arXiv:1907.10326,2019. 5,6,7
predictionfromasingleimageusingamulti-scaledeepnet- [23] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
work. Advances in neural information processing systems, Blip: Bootstrapping language-image pre-training for uni-
27,2014. 2,5,6 fied vision-language understanding and generation. In In-
[10] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat- ternationalConferenceonMachineLearning,pages12888–
manghelich,andDachengTao. Deepordinalregressionnet- 12900.PMLR,2022. 2
workformonoculardepthestimation. InProceedingsofthe [24] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
IEEE conference on computer vision and pattern recogni- YixuanWei,JiaNing,YueCao,ZhengZhang,LiDong,etal.
tion,pages2002–2011,2018. 5,6 Swintransformerv2:Scalingupcapacityandresolution. In
[11] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat- Proceedings of the IEEE/CVF conference on computer vi-
manghelich,andDachengTao. Deepordinalregressionnet- sionandpatternrecognition,pages12009–12019,2022. 5
workformonoculardepthestimation. InProceedingsofthe [25] IlyaLoshchilovandFrankHutter. DecoupledWeightDecay
IEEE conference on computer vision and pattern recogni- Regularization. InICML2019. 8,13
tion,pages2002–2011,2018. 2 [26] John McCormac, Ankur Handa, Stefan Leutenegger, and
[12] AndreasGeiger, PhilipLenz, ChristophStiller, andRaquel Andrew J Davison. Scenenet rgb-d: 5m photorealistic im-
Urtasun. Visionmeetsrobotics: Thekittidataset. Interna- agesofsyntheticindoortrajectorieswithgroundtruth.arXiv
tionalJournalof Robotics Research(IJRR),2013. 3, 6, 7, preprintarXiv:1612.05079,2016. 3
8 [27] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob
[13] Jose´ L.Herrera, CarlosR.delBlanco, andNarcisoGarc´ıa. Fergus. Indoor segmentation and support inference from
Automaticdepthextractionfrom2dimagesusingacluster- rgbdimages. InECCV,2012. 3,5,7,8,12
basedlearningframework.IEEETransactionsonImagePro- [28] Jia Ning, Chen Li, Zheng Zhang, Chunyu Wang, Zigang
cessing,27(7):3288–3299,2018. 2 Geng,QiDai,KunHe,andHanHu.Allintokens:Unifying
[14] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif- outputspaceofvisualtasksviasofttoken. InProceedings
fusionprobabilisticmodels. Advancesinneuralinformation oftheIEEE/CVFInternationalConferenceonComputerVi-
processingsystems,33:6840–6851,2020. 3 sion,pages19900–19910,2023. 2,5
[15] Yasunori Ishii and Takayoshi Yamashita. Cutdepth: Edge- [29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
awaredataaugmentationindepthestimation. arXivpreprint James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
arXiv:2107.07684,2021. 8 Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,AndreasKopf,EdwardYang,ZacharyDeVito,MartinRai- cross-distillation with cutflip for monocular depth estima-
son, AlykhanTejani, SasankChilamkurthy, BenoitSteiner, tion. arXivpreprintarXiv:2302.08149,2023. 6
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An [41] Shuwei Shao, Zhongcai Pei, Xingming Wu, Zhong Liu,
imperativestyle,high-performancedeeplearninglibrary. In Weihai Chen, and Zhengguo Li. Iebins: Iterative elas-
AdvancesinNeuralInformationProcessingSystems.Curran tic bins for monocular depth estimation. arXiv preprint
Associates,Inc.,2019. 8 arXiv:2309.14137,2023. 6
[30] Vaishakh Patil, Christos Sakaridis, Alexander Liniger, and [42] Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao.
LucVanGool. P3depth:Monoculardepthestimationwitha Sun rgb-d: A rgb-d scene understanding benchmark suite.
piecewiseplanarityprior. InProceedingsoftheIEEE/CVF In2015IEEEConferenceonComputerVisionandPattern
Conference on Computer Vision and Pattern Recognition, Recognition(CVPR),pages567–576,2015. 1,7,16
pages1610–1621,2022. 5,6
[43] YangSongandStefanoErmon.Generativemodelingbyesti-
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya matinggradientsofthedatadistribution.Advancesinneural
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, informationprocessingsystems,32,2019. 3
AmandaAskell,PamelaMishkin,JackClark,etal.Learning
[44] Yang Song and Stefano Ermon. Improved techniques for
transferable visual models from natural language supervi-
trainingscore-basedgenerativemodels. Advancesinneural
sion.InInternationalconferenceonmachinelearning,pages
informationprocessingsystems,33:12438–12448,2020. 3
8748–8763.PMLR,2021. 2
[45] YangSong,JaschaSohl-Dickstein,DiederikPKingma,Ab-
[32] MichaelRamamonjisoaandVincentLepetit. Sharpnet:Fast
hishekKumar,StefanoErmon,andBenPoole. Score-based
and accurate recovery of occluding contours in monocular
generative modeling through stochastic differential equa-
depth estimation. In Proceedings of the IEEE/CVF Inter-
tions. arXivpreprintarXiv:2011.13456,2020. 3
nationalConferenceonComputerVisionWorkshops,pages
[46] YangSong,ConorDurkan,IainMurray,andStefanoErmon.
0–0,2019. 5
Maximumlikelihoodtrainingofscore-baseddiffusionmod-
[33] Rene´ Ranftl,AlexeyBochkovskiy,andVladlenKoltun. Vi-
els.AdvancesinNeuralInformationProcessingSystems,34:
sion transformers for dense prediction. In Proceedings of
1415–1428,2021. 3
theIEEE/CVFinternationalconferenceoncomputervision,
[47] M. Sun, A. Y. Ng, and A. Saxena. Make3d: Learning 3d
pages12179–12188,2021. 5,6
scenestructurefromasinglestillimage. IEEETransactions
[34] Rene´ Ranftl, Katrin Lasinger, David Hafner, Konrad
onPatternAnalysisandMachineIntelligence, 31(05):824–
Schindler, and Vladlen Koltun. Towards robust monocular
840,2009. 2
depthestimation:Mixingdatasetsforzero-shotcross-dataset
[48] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
transfer. IEEE Transactions on Pattern Analysis and Ma-
Chouard,VijaysaiPatnaik,PaulTsui,JamesGuo,YinZhou,
chineIntelligence,44(3),2022. 2
YuningChai,BenjaminCaine,etal.Scalabilityinperception
[35] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit
forautonomousdriving: Waymoopendataset. InProceed-
Kumar,MiguelAngelBautista,NathanPaczan,RussWebb,
ings of the IEEE/CVF conference on computer vision and
and Joshua M. Susskind. Hypersim: A photorealistic syn-
patternrecognition,pages2446–2454,2020. 3
thetic dataset for holistic indoor scene understanding. In
InternationalConferenceonComputerVision(ICCV)2021, [49] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,
2021. 1,7,14 HaochenWang, FalconZDai, AndreaFDaniele, Moham-
madrezaMostajabi,StevenBasart,MatthewRWalter,etal.
[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Diode: A dense indoor and outdoor depth dataset. arXiv
Patrick Esser, and Bjo¨rn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of preprintarXiv:1908.00463,2019. 1,7,15
the IEEE/CVF conference on computer vision and pattern [50] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
recognition,pages10684–10695,2022. 4,12 reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- Polosukhin. Attentionisallyouneed. Advancesinneural
net: Convolutionalnetworksforbiomedicalimagesegmen- informationprocessingsystems,30,2017. 3
tation.InMedicalImageComputingandComputer-Assisted [51] ZhendaXie,ZigangGeng,JingchengHu,ZhengZhang,Han
Intervention–MICCAI2015:18thInternationalConference, Hu,andYueCao. Revealingthedarksecretsofmaskedim-
Munich,Germany,October5-9,2015,Proceedings,PartIII agemodeling. InProceedingsoftheIEEE/CVFConference
18,pages234–241.Springer,2015. 3,4 onComputerVisionandPatternRecognition,pages14475–
[38] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and 14485,2023. 2,5,6
DavidJFleet. Monoculardepthestimationusingdiffusion [52] Xiaodong Yang, Zhuang Ma, Zhiyu Ji, and Zhe Ren.
models. arXivpreprintarXiv:2302.14816,2023. 2,3 Gedepth: Ground embedding for monocular depth estima-
[39] Christoph Schuhmann, Richard Vencu, Romain Beaumont, tion. InProceedingsoftheIEEE/CVFInternationalConfer-
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo enceonComputerVision,pages12719–12727,2023. 6
Coombes,JeniaJitsev,andAranKomatsuzaki.Laion-400m: [53] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and
Open dataset of clip-filtered 400 million image-text pairs. PingTan. Newcrfs: Neuralwindowfully-connectedcrfsfor
arXivpreprintarXiv:2111.02114,2021. 2,3 monocular depth estimation. In Proceedings of the IEEE
[40] Shuwei Shao, Zhongcai Pei, Weihai Chen, Ran Li, Zhong Conference on Computer Vision and Pattern Recognition,
Liu, and Zhengguo Li. Urcdc-depth: Uncertainty rectified 2022. 5,6,7[54] WenliangZhao,YongmingRao,ZuyanLiu,BenlinLiu,Jie
Zhou, and Jiwen Lu. Unleashing text-to-image diffusion
models for visual perception. ICCV, 2023. 2, 3, 5, 6, 7,
8,13ECoDepth: Effective Conditioning of Diffusion Models
for Monocular Depth Estimation
Supplementary Material
A.AblationStudy B.ArchitecturalDetails
B.1.ImageEncoder
A.1.Effectof ViTArchitecture
Similar to Latent Diffusion [36], we employed the VQ-
Table5investigatestheimpactofvaryingViTsizesonthe
VAE’s encoder to transition from image space to latent
generation of embeddings from RGB images. Our results
space.
fortheNYU Depth v2[27]datasetsuggestthatViT-base
yieldsoptimalperformance. Additionally,ourobservations B.2.UpsamplingDecoder
intheKITTIdatasetalignwithasimilartrend.
Afterobtainingthehierarchicalfeaturemapfromdenoising
UNet,theconcatenatedfeaturemapundergoesupsampling,
Table5. AblationStudyonViTSizes: Performancecomparison
transitioningfromaresolutionof64×64backtoH ×W.
of different ViT variants in terms of parameters and depth error
Refer to Fig. 7 for a detailed view of the upsampling de-
metricsontheNYUv2[27]dataset.Theresultsguidetheselection
coderarchitecture.
ofViT-baseinourfinalarchitecture.Bestresultsareinbold.
Classifier #Parameters RMSE↓ AbsRel↓ δ ↑
1 Upsampling Decoder
ViT-base 86.6M 0.218 0.059 0.978
Deconv Layer((3 3), 8e, 32)
deit-base 86.6M 0.218 0.059 0.978 BatchNorm2D
ViT-large 303.3M 0.218 0.060 0.978
Deconv Layer((2 2), 32, 32)
ViT-huge 630.8M 0.219 0.060 0.978 BatchNorm2D
Deconv Layer((2 2), 32, 32)
BatchNorm2D
Conv ((3 3), 32, e)
BatchNorm2D
Table6.AblationStudyondimensionofLearnableSceneEm-
beddings(N):Thetableshowstheimpactofvaryingthedimen- Bilinear Upsample
sion of learnable scene embeddings on the depth error metrics.
Bilinear Upsample
WeobserveadecreaseinerrorwithincreasingNuntilsaturation
occurs at N=100, prompting usto limit the model parametersto
N=100.Bestresultsarehighlightedinbold.
Figure7.Detailedarchitectureoftheupsamplingdecoder,respon-
N RMSE↓ AbsRel↓ log ↓ δ ↑ sible for upsampling the concatenated feature map to obtain the
10 1
finalfeaturemapataresolutionofH×W,e=192
10 0.219 0.061 0.027 0.978
50 0.219 0.060 0.026 0.978
100 0.218 0.059 0.026 0.978
200 0.218 0.060 0.026 0.978 C.AdditionalExperimentalDetails
C.1.Hyperparameters
Forreproducibilityoftheresultspresentedinthemainpa-
A.2.AdditionalQualitativeAblation perandthesupplementarymaterial, weprovideacompre-
hensivelistofthehyperparametersemployedinourexper-
InFig. 8,wepresentsupplementaryqualitativeablationre- imentsinTable7.
sultsthathighlightthecorrelationbetweenvalueofViTlog-
its and the improvement in the predicted depth. The visu- D. Qualitative Results for Zero-Shot Perfor-
alizationdemonstratesthatelevatedvalueofViTlogitsfor manceAcrossDatasets
specificobjectscontributetoourmodel’sabilitytofocuson
thoseobjects,enhancingtheaccuracyofpredicteddepthin In the main paper, we presented a quantitative comparison
correspondingregions. ofourmethod’szero-shotperformance. Here,weprovideaImage Ground Truth VPD Ours
40.02%
 6.59%
 5.03%
 3.40%
 3.08%
 2.82%
 1.87%
 1.85%
 1.81%
 1.48%
 1.21%
 1.10%
 1.10%
 1.01%

shoe toyshop loudspeaker sweatshirt knee pad tobacco backpack sleeping
 television
 home theaterconfection jean, blue wardrobe jersey, T-
shop shop bag
 ery jean, denim shirt
Image Ground Truth VPD Ours
33.88%
 19.25%
 9.83%
 7.40%
 4.88%
 4.51%
 1.25%
 1.15%
 1.14%
 1.10%
 0.97%
 0.80%
 0.78%
 0.67%

table lamp window shade quilt, lampshade four-posterchiffon desk studio sliding shoji television spotlight radiator wardrobe,
comforter ier couch door closet
Figure8. EnhancedvisualizationsshowcasingimprovementsoverVPD[54]inourmodel,facilitatedbyViTembeddingsemployedas
conditionalvectorsforthedenoisingprocedure.Inthepresentedimages,ourmodeldemonstratesheightenedaccuracyindetectingobjects,
suchasthetelevision(blueinfirstimage)andtablelamp(greeninsecondimage)whenthesearedetectedwithhighprobabilityby
ViT.
Table7.Hyper-parametersettingsforourmodel.
Hyper-parameter Value
Learningrateschedule onecycle
MinlearningRate 3×10−5
MaxlearningRage 5×10−4
BatchSize 32
Optimizer AdamW[25]
β inoptimizer (0.9,0.999)
s
WeightDecay 5×10−2
LayerDecayRate 0.9
EmbeddingDimension 192
No. ofdenoisingstepinUNet 1
VariancefocusinSiLogloss 0.85
ViTSize ViT-base
Numberoflearnableemb. 100
epochs 25
qualitativeassessmentofourmethod’sperformanceincom-
parisontoZoEDepth[4]acrosstheHyperSim,DIODE,
Sun-RGBDandiBims1datasetsinFig. 9,10,11and12.RGB
GT
Zoed
Ours
RGB
GT
Zoed
Ours
Figure9.QualitativeComparisonontheHyperSim[35]Dataset.OurdepthpredictionsarecontrastedwiththoseofZoedepth[4].The
firstrowdisplaysRGBimages,thesecondrowshowsgroundtruthdepth,thethirdrowexhibitsZoedepth[4]’sdepth,andthefourthrow
showcasesourdepthpredictions.Tofacilitatevisualcomparison,thecolormapscaleremainsconsistentacrosscorrespondingdepthmaps.
Ourmodel,trainedonlyonNYU Depth v2,iscomparedwithZoedepth[4],whichistrainedon12datasetsandthenfine-tunedonNYU
Depth v2.RGB
GT
Zoed
Ours
RGB
GT
Zoed
Ours
Figure10. QualitativeComparisonontheDIODE[49]Dataset. OurdepthpredictionsarecontrastedwiththoseofZoedepth[4]. The
firstrowdisplaysRGBimages,thesecondrowshowsgroundtruthdepth,thethirdrowexhibitsZoedepth[4]’sdepth,andthefourthrow
showcasesourdepthpredictions.Tofacilitatevisualcomparison,thecolormapscaleremainsconsistentacrosscorrespondingdepthmaps.
Ourmodel,trainedonlyonNYU Depth v2,iscomparedwithZoedepth[4],whichistrainedon12datasetsandthenfine-tunedonNYU
Depth v2.RGB
GT
Zoed
Ours
RGB
GT
Zoed
Ours
Figure11. QualitativeComparisonontheSun-RGBD[42]Dataset. OurdepthpredictionsarecontrastedwiththoseofZoedepth[4].
ThefirstrowdisplaysRGBimages,thesecondrowshowsgroundtruthdepth,thethirdrowexhibitsZoedepth[4]’sdepth,andthefourth
rowshowcasesourdepthpredictions. Tofacilitatevisualcomparison,thecolormapscaleremainsconsistentacrosscorrespondingdepth
maps. Ourmodel,trainedonlyonNYU Depth v2,iscomparedwithZoedepth[4],whichistrainedon12datasetsandthenfine-tuned
onNYU Depth v2.RGB
GT
Zoed
Ours
RGB
GT
Zoed
Ours
Figure12. QualitativeComparisonontheiBims1[19]Dataset. OurdepthpredictionsarecontrastedwiththoseofZoedepth[4]. The
firstrowdisplaysRGBimages,thesecondrowshowsgroundtruthdepth,thethirdrowexhibitsZoedepth[4]’sdepth,andthefourthrow
showcasesourdepthpredictions.Tofacilitatevisualcomparison,thecolormapscaleremainsconsistentacrosscorrespondingdepthmaps.
Ourmodel,trainedonlyonNYU Depth v2,iscomparedwithZoedepth[4],whichistrainedon12datasetsandthenfine-tunedonNYU
Depth v2.