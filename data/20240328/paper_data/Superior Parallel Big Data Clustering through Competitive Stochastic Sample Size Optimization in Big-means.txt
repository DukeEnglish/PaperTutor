Superior Parallel Big Data Clustering through
Competitive Stochastic Sample Size Optimization
in Big-means
Rustam Mussabayev1,2[0000−0001−7283−5144] and Ravil
Mussabayev1,3[0000−0003−1105−5990]
1 SatbayevUniversity,Satbayevstr. 22, Almaty 050013, Kazakhstan
2 Laboratory for Analysis and Modeling of Information Processes, Instituteof
Information and Computational Technologies, Pushkin str. 125, Almaty 050010,
Kazakhstan
rustam@iict.kz
3 University of Washington, Department of Mathematics, Padelford Hall C-138,
Seattle 98195-4350, WA, USA
ravmus@uw.edu
Abstract. ThispaperintroducesanovelK-meansclusteringalgorithm,
an advancement on the conventional Big-means methodology. The pro-
posed method efficiently integrates parallel processing, stochastic sam-
pling,andcompetitiveoptimizationtocreateascalablevariantdesigned
for big data applications. It addresses scalability and computation time
challengestypicallyfacedwithtraditionaltechniques.Thealgorithmad-
justssamplesizesdynamicallyforeachworkerduringexecution,optimiz-
ingperformance. Datafrom thesesamplesizesarecontinuallyanalyzed,
facilitating the identification of the most efficient configuration. By in-
corporatingacompetitiveelementamongworkersusingdifferentsample
sizes,efficiencywithintheBig-meansalgorithm isfurtherstimulated.In
essence,thealgorithm balancescomputationaltimeandclusteringqual-
itybyemployingastochastic,competitivesamplingstrategyinaparallel
computing setting.
Keywords: Big-means Clustering · Parallel Computing · Data Mining
·StochasticVariation ·SampleSize·CompetitiveEnvironment·Paral-
lelizationStrategy·MachineLearning·BigDataAnalysis·Optimization
· Cluster Analysis· K-means · K-means++· Unsupervised Learning.
1 Introduction
Clusteringisafoundationaltaskinthe fieldofdataanalysisandmachinelearn-
ing, serving as the cornerstone of unsupervised learning techniques [11]. The
ultimate goal of clustering is to partition a set of objects into groups, known
as clusters, in such a way that objects belonging to the same cluster exhibit
higher similarity to each other than to those in different clusters [12]. This task
4202
raM
72
]GL.sc[
1v66781.3042:viXra2 R. Mussabayev and R. Mussabayev
is driven by the inherent human instinct to understand and organize large vol-
umes of information, and it mirrors our natural desire to categorize or group
similar objects together.
The importance and ubiquity of clustering in various domains cannot be
overstated.For instance, in the realm of image processing and computer vision,
clusteringalgorithmsareroutinelyemployedforimagesegmentation,wheresim-
ilarpixelsaregroupedtogethertoidentify regionsofinterest[5].Inthe worldof
business and marketing, customer segmentation is crucial for targeted market-
ing, and it heavily relies on clustering methods to identify groups of customers
with similar buying patterns or behaviors [19].
In bioinformatics, clustering is used in gene expression data analysis where
genes with similar expression patterns are grouped, leading to the identifica-
tion of functionally related groups of genes [8]. In the field of natural language
processing, clustering is used to group similar texts, helping in tasks like topic
modeling and sentiment analysis [6]. Additionally, social network analysis often
uses clustering to identify communities or groups of users with similar interests
or behaviors [10].
Despitethediverseapplications,allclusteringmethodsshareacommongoal
— to maximize the intra-cluster similarity and minimize the inter-cluster sim-
ilarity. Among various clustering methods, one common and widely accepted
criterion for clustering is the minimum-sum-of-squaresclustering (MSSC) crite-
rion.TheMSSCcriterionisalsoknownastheK-meanscriterion,asthepopular
K-means algorithm optimizes this criterion [14].
TheMSSC criterionaimstopartitionthe dataintoKclusters,suchthatthe
sumofsquaredEuclideandistancesbetweenthedatapointsandtheirassociated
cluster centers is minimized [9]. In mathematical terms, if we denote the center
of a cluster by c ∈ C and the points in a cluster by x ∈ X, then we aim to
minimize
m
min f(C,X)=X min kx i−c jk2 (1)
C j=1,...,k
i=1
where k is the number of clusters, and k·k stands for the Euclidean norm.
Inpractice,the MSSC criterionoftenleadsto high-qualityclusterswhenthe
underlying clusters in the data are spherical and evenly sized [7]. However, the
criterion and its associated algorithms are sensitive to the initial configuration
and are susceptible to getting trapped in local minima [1]. This motivates the
search for advanced and robust clustering algorithms that can overcome these
limitations [4].
The purpose of this paper is to introduce a novel parallel clustering algo-
rithmwithcompetitivestochasticsamplesizeoptimizationandevaluateitsper-
formance. Competitive optimization involves multiple solutions or agents (par-
allel processes or algorithms) competing to achieve optimal results, leading to
a dynamic and adaptive optimization process.Stochastic sampling, the random
selection of data subsets, is used by each process to reduce computational load
whilemaintainingarepresentativedataset.Thenewalgorithmisaparallelver-Title Suppressed Dueto Excessive Length 3
sion of the Big-means clustering algorithm [18,17], enhanced with competitive
stochasticsamplesizeoptimization.Eachparallelprocesscompetesusingdiffer-
entsamplesizesforK-meansclustering,strivingforthe mostefficientclustering
outcome. Results of extensive experiments suggest that this approach is more
efficient and scales better with large datasets compared to traditional K-means
and all of its variants.
Therestofthepaperisstructuredasfollows.Section2motivatesthecreation
of the new algorithm. Section 3 provides a comprehensive literature review of
available methods, highlighting their main shortcomings.Section 4 presents the
proposed method in detail. Section 5 describes the experimental setup used to
evaluate the performance of the algorithm. Section 6 discusses the results and
comparesthenew algorithmwithexistingmethods.Finally,Section7concludes
the paper and suggests directions for future work.
2 Problem Statement
Asthevolumeofdatabeinggeneratedandcollectedcontinuestogrowexponen-
tially,the demandfor efficient,scalable,androbustalgorithmsfor dataanalysis
taskslike clusteringhas beensignificantlyheightened.This is especially true for
MSSC algorithms that can handle large datasets, often referred to as big data.
Despite the ubiquity of MSSC clustering in various domains, several challenges
hinder the efficient execution of these algorithms on big datasets.
Firstly,manyoftheexistingMSSCalgorithmsdonotscalewellwiththesize
of the dataset.The computationalcomplexity ofthese algorithmsgrowsrapidly
as the number of data points increases, rendering them impractical for use on
big data. Notably, the popular K-means algorithm, while simple and effective
on smaller datasets, suffers from a high computational cost when dealing with
large-scale data due to the need to compute distances between each data point
and each centroid in each iteration.
Secondly, several current algorithms do not fully harness the potential of
modernhigh-performancecomputingsystems.Withthe advancementsinmulti-
coreandmulti-processorarchitectures,aswellasthe adventofdistributedcom-
putingplatforms,thereexistsagreatopportunitytodevelopparallelalgorithms
for MSSC that can process large datasets efficiently by utilizing all available
computational resources. However, the development of such parallel algorithms
is not straightforward and involves tackling challenges like data partitioning,
load balancing, and communication overheadamong the processing units.
Finally, a persistent challenge in MSSC is the sensitivity of the algorithms
to the initial configuration, which can lead to sub-optimal clustering solutions
trappedin localminima. This problembecomes more pronouncedin the caseof
big data due to the increased complexity and diversity of the data.
Therefore, the need for a new MSSC clustering algorithm that is both ef-
ficient and scalable, can handle big data, takes full advantage of modern high-
performancecomputingsystems,andprovidesrobustclusteringsolutionsregard-
lessoftheinitialconfiguration,isevident.Suchanalgorithmwouldsignificantly4 R. Mussabayev and R. Mussabayev
enhance our ability to extract valuable insights from large datasets in various
fields.
3 Related Works
Clusteringhasbeenanintenselystudiedareainmachinelearning,andnumerous
algorithmshavebeenproposedtosolvetheproblemofminimum-sum-of-squares
clustering. The K-means algorithm, owing to its simplicity, is the most popular
and widely used of these algorithms[15]. However,K-meanshas severalnotable
limitations.Ittendstogetstuckinlocalminima,issensitivetotheinitialplace-
ment of centroids, and its performance degrades with an increase in the size of
the dataset.
To overcome these limitations, several variants of K-means have been pro-
posed. The K-means++ algorithmintroduces a smarter initialization technique
that aims to provide better initial centroids, thereby improving the final clus-
tering result [1]. While this technique reduces the risk of getting stuck in poor
local minima, it does not adequately address scalability issues.
MiniBatchK-meansisavariantdesignedtohandlelargerdatasetsmoreeffi-
ciently by operatingonarandomlyselectedsubsetofthe dataateachstep[21].
While this improvescomputationalefficiency, it comes atthe cost ofpotentially
reduced clustering quality compared to standard K-means.
Scalable K-means++,as the name suggests, is designed to scale better with
big data. This variant selects initial centers in an “oversampling” phase and
then reduces the number of centers in a “reduce phase”, improving scalability.
However,the complexity of the algorithm and the need for multiple passes over
the data limit its practicality [2].
BisectingK-meansadoptsadivide-and-conquerstrategywherethealgorithm
iteratively splits clusters [22]. This can potentially result in a better quality of
clustering but requires more computational resources as the number of clusters
or the size of the data increases.
Fuzzy C-means provides a probabilistic approachwhere each data point can
belong to multiple clusters with varying membership grades [3]. This can offer
more nuanced cluster assignments but increases the computational complexity
significantly.
X-means and G-means algorithms try to determine the appropriate number
of clusters automatically,thereby removing the need for explicit cluster number
specification [20]. However,this automation introduces its complexities and can
result in increased computational cost.
Hierarchical K-means presents a hierarchical approach to clustering, which
can handle larger datasets and provide a more intuitive cluster hierarchy [24].
Yet, it suffers from high computational costs for large datasets.
Finally, Distributed K-means algorithms take advantage of distributed com-
puting resources to scale the K-means clustering process to larger datasets [23].
While this approach effectively utilizes modern computational resources, theTitle Suppressed Dueto Excessive Length 5
communication overhead and the need for data to be partitioned appropriately
can limit its efficiency.
Despite the strengths of these algorithms, they often introduce additional
computationalcomplexityordonotadequatelyscalewithlargerdatasets.Hence,
the needfor a new algorithmthat efficiently handles largedatasets while taking
full advantage of modern high-performance computing resources is evident.
4 Methodology
4.1 Big-means Algorithm
The Big-means algorithm is specifically designed to tackle large-scaleMinimum
Sum-of-Squares Clustering (MSSC) problems. The concept underlying the Big-
means algorithm is straightforward[18]: in each iteration, a new uniformly ran-
dom sample of size s ≪ |X| is drawn from the provided dataset and clustered
using K-means. The K-means++ algorithm initializes clustering for the first
sample. Every subsequent sample is initialized using the best solution found
thus far across previous iterations in terms of minimizing the objective func-
tion(1)onthesamples.Duringintermediateiterations,onlydegenerateclusters
are reinitialized using K-means++. Iterations persist until a “stop condition” is
met. This “stop condition” can be a limit on CPU time or a maximum number
of samples to be processed.The outcome of the algorithmis the set ofcentroids
that achieved the best objective function value (1) across the iterations. Ulti-
mately, all data points can be allocated to clusters based on their proximity to
the resulting centroids.
The shaking procedure is a vital component of the Big-means algorithm. It
refers to the generation of a new sample in each iteration, which perturbs the
current (incumbent) solution and introduces variability into the clustering re-
sults. When the full dataset is viewed as a cloud of points in an n-dimensional
space, each sample represents only a sparse approximation of this cloud. This
procedureinstillsdiversityandadaptabilityintotheclusteringprocess.Itaccom-
plishes this by iterativelyapplying the K-meansalgorithmto randomsubsets of
the data, progressively refining the centroid locations, and effectively managing
degenerate clusters.
Thealgorithm’sscalabilitycanbefine-tunedbyselectingappropriatesample
sizesandcounts.Processingsmallersubsetsofdataineachiterationsignificantly
reduces computational demands. Additionally, this strategy prevents the algo-
rithm from getting trapped in suboptimal solutions. This is achieved by using
random data subsets in each iteration and periodically re-initializing the cen-
troids of degenerate clusters. The Big-means algorithmis a promising approach
for clustering large datasets, offering scalability, efficiency, and robustness.
4.2 Competitive Sample Size Big-means Algorithm
Inourstudy,weproposeanovelstrategytoparallelizetheBig-meansclustering
algorithm, wherein the size s of the clustered sample from big data used at6 R. Mussabayev and R. Mussabayev
Algorithm 1: Detailed pseudocode of Parallel Big-means Clustering
with Competitive Stochastic Sample Size Optimization
1 Initialization:
2 C w ←Mark all k centroids as degenerate for each worker w;
3 fˆ w ←∞ for each worker w;
4 t w ←0 for each worker w;
5 L←Empty list;
6 whilet w <T for any worker w do
7 for each parallel worker w do
8 s w ←Random integer in [s min,s max];
9 Recalculate fˆ w with thenew s w;
10 p
w
←0;
11 whilep w <p do
12 S w ←Random sample of size s w from X;
13 for each c∈C w do
14 if c is the centroid associated with a degenerate cluster then
15 Reinitialize c usingK-means++ on S w;
16 end
17 end
18 Cnew,w ←K-means clustering on S w with initial centroids C w;
19 if f(Cnew,w,S w)<fˆ w then
20 C
w
←Cnew,w;
21 fˆ w ←f(Cnew,w,S w);
22 Adds w tolist L;
23 end
24 p w ←p w+1;
25 end
26 t w ←t w+1;
27 end
28 end
29 Analyzedistribution of s i valuesin list L;
30 s opt←s i valuewith highest probability of improving objective function;
31 S ←Random sample of size s opt from X;
32 for each parallel worker w do
33 Recalculate fˆ w with s opt usingS;
34 end
35 Cbest ←Centroids of the worker with the smallest fˆ w value;
36 Y ←Assign each point in X to nearest centroid in Cbest;
37 return Cbest, Y, s opt;Title Suppressed Dueto Excessive Length 7
eachiterationvaries.Withinthisparallelizationstrategy,duringthealgorithm’s
initialization stage, each of the w workersrandomly selects their sample size s
w
from the permissible range [s ;s ]. Subsequently, each worker operates in
min max
parallelforp iterations,adheringtothe standardBig-meansalgorithm’sscheme
using the allocated sample size s .
w
After p iterations, each i-th worker is assigned a new random sample size
s . Simultaneously, the value of the target criterion for the worker’s current
w
incumbent solution is recalculated to reflect the change in s , as any change in
w
s necessitates such a recalculation. As the algorithm operates, comprehensive
w
statistics relating to improvements in the objective function’s value for a given
s are collected for all workers. Essentially, each worker, upon witnessing an
w
improvement in the value of its objective function, contributes its current s
w
value to a shared list.
By the end of the algorithm’s execution, we obtain a list of all s values
w
that led to improvements in the objective function’s value. By analyzing the
distributionofvaluesinthis list,weselectthes valuefromthelistthatoffers
opt
the highest probability of enhancing the objective function’s value. The desired
result can be achieved by calculating a simple mean of the given list, which
corresponds to the expected value of the improving sample size.
This strategy effectively creates a competitive environment among workers,
allowing for simultaneous variation in the used sample size s and subsequent
w
determination of its optimal value.
4.3 Detailed Algorithm Description
The algorithmpresentedinAlgorithm1details aparallelimplementationofthe
K-means clustering method with competitive stochastic sample size optimiza-
tion. The method aims to determine the best cluster centroids and assign data
points to these clusters efficiently.
The algorithm initializes by marking all k centroids as degenerate for each
worker w. Each worker also has an initial best-so-far objective function value
fˆ and iteration count t set to ∞ and 0, respectively. An empty list L is also
w w
defined to keep track of sample sizes that lead to improvement in the objective
function.
Ineachiterationofthemainloop,everyworkeroperatesinparallel,choosing
a randomsample size s between s and s , and recomputing the fˆ with
w min max w
thenewsamplesize.Then,withinthedefinedmaximumnumberofpassesp,the
workertakes a randomsample of size s from the data set X and processes the
w
centroids. For each centroid c associated with a degenerate cluster, the worker
reinitializes c using K-means++ on the sample S . The worker performs K-
w
meansclusteringonS
w
withinitialcentroidsC
w
to getnew centroidsCnew,w.If
thenew centroidsresultinabetter objectivefunctionvalue,theworkerupdates
its current centroids, best-so-far objective function value, and adds the sample
size to list L. The pass counter p is then incremented.
w
After all workers finish their iterations or reach the maximum number of
iterations T, the algorithm proceeds to analyze the distribution of sample sizes8 R. Mussabayev and R. Mussabayev
inlistL.The algorithmselectsthe sample sizes ,whichhas the highestprob-
opt
ability of improving the objective function. This selection can be achieved by
calculating the simple mean of the values in the list L. Then, a new sample of
sizes istakenfromthedatasetX,andeachworkerrecalculatesitsbest-so-far
opt
objective function value with this new sample size.
Finally, the algorithm chooses the centroids Cbest of the worker with the
smallest fˆ
w
value, assigns each point in X to its nearest centroid in Cbest, and
returns these centroids, the cluster assignments, and the optimal sample size
s .
opt
In this article, we assume that each worker has equal access to the full-sized
dataset and can independently draw samples from it. For the sake of simplicity,
in this study we are not exploring various available opportunities for further
optimization of the algorithm, particularly those concerning distributed data
storageacrossdifferentnodesofthecomputingsystem.Suchoptimizationsmerit
a separate study.
5 Experiment Setup
Our experiments utilized a system with Ubuntu 22.04(64-bit), an AMD EPYC
7663 56-Core Processor, and 1.46 TB RAM. Up to 16 cores were deployed,
running Python 3.10.11, NumPy 1.24.3, and Numba 0.57.0. Numba [16] was
essential for Python code acceleration and parallelism.
Thestudyinvolvedcomparingtheproposedalgorithmtothebest-performing
hybrid-parallelversionofthe Big-meansalgorithm[17],using19public datasets
(details and URLs are available in [18] and [17]), plus four normalized datasets,
totaling 23. These datasets, with 2 to 5,000 attributes and 7,797 to 10,500,000
instances, were used to assess our algorithm’s versatility.
We executed each algorithm on the 23 datasets n times for cluster sizes
exec
of 2, 3, 5, 10, 15, 20, 25. Select datasets underwent additional clustering into 4
clusters,followingKarmitsaetal.[13]. This resultedin 7,366individual cluster-
ing processes. The algorithms’ performances were measured in terms of relative
clustering accuracy ε, CPU time t, and baseline time t.
The relative clustering accuracy ε is defined as
100×(f −f∗ )
ε(%)= ,
f∗
withf∗ =f∗ (X,k)being the besthistoricalobjectivefunctionvalue for dataset
X and cluster number k.
Toobjectivelymeasureclusteringtime,weuseaspecialbaselinetimemetric
t,whichhelpstoavoidbiasfromminorlate-stageimprovements.Foreveryalgo-
rithm,time tis the timeittakesforanalgorithmto achievethe baselinesample
objective value f . For every pair (X,k) and sample size s, f is derived from
s s
comparing the sample objective values achieved by different Big-means paral-
lelization strategies: sequential, inner, competitive, and collective [17]. Specifi-
cally, f is defined as the maximum of the median (median is taken over n
s execTitle Suppressed Dueto Excessive Length 9
executions for the pair (X,k)) best sample objective function values acrossiter-
ations obtained by the considered parallel Big-means versions. Essentially, this
method determines the worst-performing parallel Big-means version and treats
its accuracy on the best processed sample as the baseline. In multi-worker set-
tings, t is the time taken by the fastest worker to reach f .
s
Clustering was limited to 300 iterations or a relative tolerance of
10−4,
and
K-means++ was used with three candidate points for centroid generation. For
the proposed algorithm, s was chosen to be 0.5 of the sample size s used in
min
Big-means,while s waschosento be 2 times s (or m, if this number exceeds
max
mforthe givendataset).Wesettheparametervaluep=10forallexperiments.
Foreachpair(X,k),thechoiceofparameterss,t andn preciselymatched
max exec
the values specified in the original Big-means paper [18].
PreliminaryexperimentsestablishedtheoptimalnumberofCPUs,baselines,
as well as the optimal parameter values for the hybrid parallel version of Big-
means, as described in [17]. The main experiment used these baselines and pa-
rameter values. We calculated the minimum, median, and maximum values of
relative accuracy and CPU time for each (X,k) pair over n runs. The re-
exec
sults are summarized in Tables 1 – 2. These tables highlight top performances,
providing a comprehensive evaluation across all datasets.
6 Experimental Results and Discussion
6.1 Performance Evaluation
A summary of the results of the main experiment are provided in Tables 1 – 2.
Basedon the experimental results,it was observedthat the proposedalgorithm
performedconsistentlybetter thanBig-meansonalldatasets,bothwithrespect
to the accuracy and time.
We attribute the outstanding performance of the proposed algorithm to its
ability to approximate the probability distribution of sample sizes that improve
the sample objective function. In the competitive parallelization strategy, each
worker starts with its own K-means++ initialization, which strengthens the fi-
nalresultvia diversification.Also,due to this kind ofparallelism,the algorithm
is able to accumulate the necessary statistics for the approximation in a very
timely efficient manner. Then, the optimal sample size value s can be ob-
opt
tainedbyevaluatingthesimplemeanoftheaccumulatedsamplesizeoccurrence
distribution.
The value s is the size of a sample that attains the best balance between
opt
sparsifyinghigh-densityclusterswhilestillincludingenoughmassoflow-density
clustersintothesample.Inadditiontotheaccumulationoftheimprovingsample
size distribution, competitive workers are able to dynamically guide the flow of
centroids through unfavorable situations by using various random sample sizes
inthe range[s ,s ].Forinstance,these unfavorablesituationsmightoccur
min max
whenasamplehaslargegapsbetweenhighlydenseclusters(thuspreventingthe
fluidity of centroids between them) or excludes some clusters due to an overly
intense sparsification.10 R. Mussabayev and R. Mussabayev
Table 1. Relative clustering accuracies ǫ (%) for different algorithms. The highest
accuracies for each experiment (algorithm, data pair (X,k)) are displayed in bold.
Success is indicated when an algorithm’s performance matches the best result among
all algorithms for the current experiment.
Proposedalgorithm Big-means
Dataset
#Succ Min MedianMax #Succ MinMedian Max
CORD-19Embeddings 45/49 -0.07 0.01 0.21 4/49 0.01 0.03 0.38
HEPMASS 41/49 -0.07 0.04 0.33 8/49 0.0 0.12 0.44
USCensusData1990 102/140 0.0 1.43 4.74 38/140 0.03 1.97 5.6
Gisette 97/105 -1.86 0.0 0.1 8/105 -1.71 0.01 0.29
MusicAnalysis 135/140 0.01 0.23 1.3 5/140 0.02 0.54 3.57
ProteinHomology 103/105 -0.05 0.05 1.83 2/105 0.11 0.8 2.69
MiniBooNEParticleIdentification 92/105 -0.54 0.0 2.36 13/105 -0.38 0.01 2.36
MiniBooNEParticleIdentification(normalized) 139/140 -0.0 0.14 0.8 1/140 0.01 0.41 3.16
MFCCsforSpeechEmotionRecognition 131/140 0.0 0.05 1.99 9/140 0.02 0.11 2.21
ISOLET 86/105 -0.49 0.01 0.67 19/105 -0.15 0.25 1.66
SensorlessDriveDiagnosis 224/280 -2.42 -0.0 5.58 56/280 -2.41 -0.0 162.06
SensorlessDriveDiagnosis(normalized) 237/280 0.0 1.02 4.45 43/280 0.01 1.28 7.87
OnlineNewsPopularity 130/140 -0.39 0.17 5.02 10/140 0.01 0.88 11.59
GasSensorArrayDrift 166/210 -0.92 0.04 4.06 44/210 -0.77 0.26 8.42
3DRoadNetwork 267/280 0.0 0.04 0.66 13/280 0.0 0.22 2.76
SkinSegmentation 181/210 -1.38 0.1 5.34 29/210 -1.3 0.21 10.01
KEGGMetabolicRelationNetwork(Directed) 114/140 -1.27 0.0 2.93 26/140 -1.24 0.03 27.4
ShuttleControl 99/120 -3.22 0.0 6.93 21/120 -3.12 1.38 16.48
ShuttleControl(normalized) 145/160 0.01 0.23 5.59 15/160 0.05 1.34 10.43
EEGEyeState 143/160 -0.09 0.0 4.25 17/160 -0.06 0.01 29.91
EEGEyeState(normalized) 212/240 -0.38 -0.0 0.7 28/240 -0.33 0.0 598.89
Pla85900 264/280 -0.06 0.04 1.44 16/280 0.0 0.12 1.6
D15112 100/105 -0.02 0.01 0.69 5/105 0.01 0.12 1.33
OverallResults 3253/3683-0.57 0.29 2.69430/3683-0.49 0.66 39.61
Table 2. Resulting clustering times t (sec.) with respect to baseline sample objective
valuesf .Thelowestclusteringtimesforeachexperiment(algorithm,datapair(X,k))
s
are displayed in bold.
Proposedalgorithm Big-means
Dataset
MinMedian Max MinMedian Max
CORD-19Embeddings 1.69 13.2 41.17 2.14 13.19 40.1
HEPMASS 1.5 4.77 24.6 0.75 2.77 21.53
USCensusData1990 0.46 2.07 5.21 0.09 0.74 2.75
Gisette 2.38 14.53 43.52 5.07 26.14 56.92
MusicAnalysis 0.19 3.49 14.33 0.6 3.58 12.87
ProteinHomology 0.08 3.24 13.25 0.37 3.69 10.43
MiniBooNEParticleIdentification 0.59 4.92 15.56 0.62 4.71 15.48
MiniBooNEParticleIdentification(normalized)0.02 0.6 1.82 0.03 0.74 2.04
MFCCsforSpeechEmotionRecognition 0.03 0.67 2.34 0.23 0.81 1.62
ISOLET 0.04 1.81 7.29 0.62 2.57 6.96
SensorlessDriveDiagnosis 0.22 1.72 4.86 0.08 1.63 4.62
SensorlessDriveDiagnosis(normalized) 0.01 0.21 0.9 0.01 0.28 0.56
OnlineNewsPopularity 0.01 0.53 1.47 0.03 0.52 1.56
GasSensorArrayDrift 0.01 0.7 2.23 0.22 1.15 3.13
3DRoadNetwork 0.02 0.63 2.2 0.03 0.69 2.52
SkinSegmentation 0.01 0.06 0.61 0.01 0.17 0.39
KEGGMetabolicRelationNetwork(Directed) 0.02 0.77 1.96 0.02 0.69 2.11
ShuttleControl 0.03 0.54 1.56 0.21 0.62 1.29
ShuttleControl(normalized) 0.0 0.05 0.39 0.01 0.27 0.4
EEGEyeState 0.07 0.48 1.35 0.04 0.58 1.2
EEGEyeState(normalized) 0.01 0.18 0.84 0.01 0.16 0.77
Pla85900 0.01 0.17 0.68 0.01 0.57 1.51
D15112 0.01 0.04 0.39 0.03 0.38 1.04
OverallResults 0.32 2.64 8.2 0.49 3.04 8.34Title Suppressed Dueto Excessive Length 11
7 Conclusion and Future Works
In this work, we proposed a parallel Big-means clustering algorithm equipped
withcompetitivestochasticsamplesizeoptimization,aswellasthoroughlyeval-
uated its performance against the state-of-the-art hybrid-parallel version of the
Big-meansalgorithm[17]usingawidearrayofreal-worlddatasetsfromtheBig-
means’originalpaper[18].Theideaofusinganautomaticprocedureforapprox-
imating the optimalsample size stemmedfrom multiple considerations.First, it
ispracticallyhardanderror-proneto estimatethe samplesizeforreal-worldbig
datasets.Second,using a fixedsample size makesthe iterative improvementna-
ture ofthe Big-meansalgorithmtoo rigid.Indeed,sampling witha fixedsample
size is limited in the flexibility of approximating and sparsifying regions of the
dataset with different densities.
OurimprovedversionofBig-meansexhibitedexceptionalresultsboth inthe
resulting quality and time, pushing much further the state of the art in the
field of big data clustering. We are confident that our work presents a valuable
contributiontothescientificfield,aswellasbringsatoolofconsiderablepractical
value to practitioners in the field of big data.
For future research, we plan to investigate other dimensions for experimen-
tation,includingdifferentwaysto exploretherange[s ,s ] andexploitthe
min max
currently best obtained sample size across iterations.
Acknowledgements
This research was funded by the Science Committee of the Ministry of Science
and Higher Education of the Republic of Kazakhstan (grant no. BR21882268).
References
1. Arthur, D., Vassilvitskii, S.: K-means++: The advantages of careful seeding. In:
Proceedings of the Eighteenth Annual ACM-SIAMSymposium on Discrete Algo-
rithms.p.1027–1035. SODA’07,SocietyforIndustrialandAppliedMathematics,
USA(2007)
2. Bahmani, B., Moseley, B., Vattani, A., Kumar, R., Vassilvitskii, S.: Scalable k-
means++.Proceedings of theVLDBEndowment 5(7), 622–633 (2012)
3. Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms.
Plenum Press, New York (1981)
4. Celebi, M.E.: Comparative Performance of Seeding Methods for k-Means Algo-
rithm.Springer (2013)
5. Comaniciu,D.,Meer,P.:Meanshift:arobustapproachtowardfeaturespaceanal-
ysis.IEEETransactionsonPatternAnalysisandMachineIntelligence24(5),603–
619 (2002)
6. Deerwester,S.,Dumais,S.T.,Furnas,G.W.,Landauer,T.K.,Harshman,R.:Index-
ing by latent semantic analysis. Journal of the American Society for Information
Science41(6) (1990)12 R. Mussabayev and R. Mussabayev
7. Ding, C., He, X.: K-means clustering via principal component analysis. In: Pro-
ceedings of the twenty-firstinternational conference on Machine learning (2004)
8. Eisen, M.B., Spellman, P.T., Brown, P.O., Botstein, D.: Cluster analysis and dis-
playofgenome-wideexpressionpatterns.Proceedings oftheNationalAcademyof
Sciences 95(25), 14863–14868 (1998)
9. Forgy,E.W.: Cluster analysis of multivariate data: efficiency vsinterpretability of
classifications. Tech. Rep.RM-5437-PR, RANDCorporation (1965)
10. Fortunato, S.: Community detection in graphs. Physics Reports 486(3-5), 75–174
(2010)
11. Hastie, T., Tibshirani, R.,Friedman, J.: Theelements of statistical learning: data
mining, inference, and prediction. SpringerScience & Business Media (2009)
12. Jain,A.K.:Dataclustering:50yearsbeyondk-means.PatternRecognitionLetters
31, 651–666 (2010)
13. Karmitsa, N., Bagirov, A.M., Taheri, S.: Clustering in large data sets with the
limited memory bundlemethod. Pattern Recognition (2018)
14. MacQueen,J.: Somemethodsfor classification and analysis of multivariateobser-
vations.In:ProceedingsofthefifthBerkeleysymposiumonmathematicalstatistics
and probability.vol. 1, pp.281–297 (1967)
15. MacQueen, J.B.: Some methods for classification and analysis of multivariate ob-
servations 1(14) (1967)
16. Marowka, A.: Python accelerators for high-performance computing. The Journal
of Supercomputing 74(4), 1449–1460 (2018)
17. Mussabayev, R., Mussabayev, R.: Strategies for parallelizing the big-means algo-
rithm:A comprehensivetutorial for effective bigdata clustering (2023)
18. Mussabayev, R., Mladenovic, N., Jarboui, B., Mussabayev, R.: How to use
k-means for big data clustering? Pattern Recognition 137, 109269 (2023).
https://doi.org/10.1016/j.patcog.2022.109269
19. Ng, A.Y., Jordan, M.I., Weiss, Y.: On spectral clustering: Analysis and an algo-
rithm.In:Advancesin neural information processing systems.pp.849–856 (2002)
20. Pelleg, D., Moore, A.: X-means: Extending k-means with efficient estimation of
thenumberofclusters.InProceedingsofthe17thInternationalConf.onMachine
Learning pp.727–734 (2000)
21. Sculley, D.: Web-scale k-means clustering. Proceedings of the 19th international
conference on World wide web pp.1177–1178 (2010)
22. Steinbach,M.,Karypis,G.,Kumar,V.:Acomparisonofdocumentclusteringtech-
niques.In:KDD Workshop on Text Mining (2000)
23. Zaharia, M., Chowdhury, M., Das, T., Dave, A., Ma, J., McCauly, M., Franklin,
M.J., Shenker, S., Stoica, I.: Resilient distributed datasets: A fault-tolerant ab-
straction for in-memory cluster computing. In: Proceedings of the 9th USENIX
conference on Networked Systems Design and Implementation. USENIX Associa-
tion (2012)
24. Zhang,T.,Ramakrishnan,R.,Livny,M.:Birch:Anefficientdataclusteringmethod
for large databases. ACM SIGMOD Record 25(2), 103–114 (1996)