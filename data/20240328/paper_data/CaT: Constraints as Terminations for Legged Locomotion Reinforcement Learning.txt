CaT: Constraints as Terminations for Legged Locomotion
Reinforcement Learning
Elliot Chane-Sane∗1, Pierre-Alexandre Leziart∗1, Thomas Flayols1,
Olivier Stasse1,2, Philippe Soue`res1, Nicolas Mansard1,2
Fig.1: Theopen-hardwarequadrupedrobotSolo-12trainedwithCaTperformingagilelocomotionoverchallengingterrains
while satisfying safety and style constraints. The robot can walk up stairs, traverse slopes, and climb over high obstacles.
Abstract—Deep Reinforcement Learning (RL) has demon- policy on the physical robot [11], [12], [13] to overcome
stratedimpressiveresultsinsolvingcomplexrobotictaskssuch challenging obstacles. Compared to previous approaches
as quadruped locomotion. Yet, current solvers fail to produce
in robot motion [14], [15], [16], this workflow requires
efficient policies respecting hard constraints. In this work,
minimal design choices, relying on generic algorithms and
we advocate for integrating constraints into robot learning
and present Constraints as Terminations (CaT), a novel con- simulations that allow to generate a wide variety of tasks.
strained RL algorithm. Departing from classical constrained Yet, reward shaping remains a meticulous endeavor as
RLformulations,wereformulateconstraintsthroughstochastic it demands a delicate balance between accomplishing the
terminations during policy learning: any violation of a con-
desiredtask,adheringtophysicallimitations,enablingseam-
straint triggers a probability of terminating potential future
less sim-to-real transfer, and ensuring natural and efficient
rewardstheRLagentcouldattain.Weproposeanalgorithmic
approach to this formulation, by minimally modifying widely motions. Many of these terms could be more effectively
used off-the-shelf RL algorithms in robot learning (such as and intuitively formulated as constraints. For instance, joint
ProximalPolicyOptimization).Ourapproachleadstoexcellent torque and velocity limits have clear physical meanings that
constraintadherencewithoutintroducingunduecomplexityand
should not be considered through a hyperparameter search.
computational overhead, thus mitigating barriers to broader
While incorporating such constraints aligns with common
adoption.Throughempiricalevaluationontherealquadruped
robot Solo crossing challenging obstacles, we demonstrate practicesinmodel-basedcontrol[17],[18],[19],widespread
that CaT provides a compelling solution for incorporating adoption in robot learning has been limited. Although some
constraintsintoRLframeworks.Videosandcodeareavailable recentconstrainedRLmethodshavebeenappliedtolocomo-
at constraints-as-terminations.github.io.
tion[20],[21],theyoftensimplifyrewardengineeringatthe
cost of algorithmic complexity, as additional critic networks
I. INTRODUCTION
andtermsinthepolicylossfunctionhavetobeimplemented.
Deep reinforcement learning (RL) has proven highly ef- In this work, we propose Constraints as Terminations
fectiveincraftingcontrolpoliciesforcomplexrobotictasks. (CaT), a streamlined approach for constrained RL that pri-
Inquadrupedlocomotion,RLapproacheshavedemonstrated oritizes simplicity and flexibility. We introduce constraints
high performances to train policies capable of traversing through stochastic terminations during policy learning: any
challenging terrains [1], [2], [3], [4] and generating natural, violationofaconstraintleadstoaprobabilityofterminating
animal-like motions [5], [6], [7]. In this work, we follow the future rewards the RL agent could have achieved. To
recent successful approaches based on model-free RL [8] to do so, we down-scale all the future rewards based on the
train policies on a curriculum of increasingly difficult set- magnitudeoftheconstraintviolationsduringpolicylearning
tings [9], [10] in simulation and directly transfer the learned through the discount factor. This naturally encourages the
agent towards satisfying the constraints to maximize future
∗Equalcontribution
rewards, while providing an alternative reward signal to
1LAAS-CNRS, Universite´ de Toulouse, Toulouse, 31400, France
first.last@laas.fr recoverfromconstraintviolations.Thisprinciplecanbeseen
2ArtificialandNaturalIntelligenceToulouseInstitute,Toulouse,France. as a refined extension of the common practice of using a
4202
raM
72
]OR.sc[
1v56781.3042:viXrastraightforward termination function, leveraging stochastic Priorworkshaveexploredtheimpositionofconstraintsor
termination to yield a dense feedback to the policy. safetymechanismsinadditiontorewardswithinthelearning
Our approach is simple to implement and seamlessly process to ensure safety guarantees. Recovery policies have
integrates with existing off-the-shelf RL algorithms. In our been learned jointly with the locomotion policy to address
experiments, we instantiate CaT with Proximal Policy Opti- safety violations [42], [43]. [44], [45] proposed to shield the
mization[8](PPO),amodel-freeon-policyalgorithmwidely learning agent by directly substituting policy actions by safe
used in robot learning. We design a set of constraints to actions whenever necessary to prevent constraint violations.
ensure that the learned policy can be safely deployed to the Other approaches incorporate constraint satisfaction directly
real robot, and a set of style constraints to exhibit natural into the policy optimization algorithms by adjusting the
motions. We demonstrate the effectiveness of our approach policyupdaterulestodiscourageviolations.Forinstance,La-
bydeployinglocomotionpoliciesonaSoloquadrupedrobot grangian methods [46], [47] approach constrained problems
with height-scan observations, producing agile locomotion as unconstrained ones by introducing Lagrange multipliers,
skillscapableoftraversingchallengingterrainscomposedof but this often leads to instability due to hyperparameter sen-
stairs, a steep slope and a high platform (see Fig. 1). sitivity[48].Morecloselyrelatedtoourwork,[20]modifies
In summary, our contributions are the following: the Interior-point Policy Optimization algorithm [49] and
1) weintroducestochasticterminationsasawaytoshape demonstrate quadruped locomotion skills on rough-terrain
the behavior of the policy to satisfy constraints in a whereas [21] implements a modified Penalized Proximal
minimalist fashion, Policy Optimization (P3O) [50] algorithm on a wheeled
2) we propose constraint designs to enforce safe behav- quadruped robot, both showcasing enhanced safety in the
iors and make the policy adhere to a specific walking learned policies and facilitating the tuning of reward terms
style on flat terrains, while letting RL adapt the style atthecostofadditionalalgorithmiccomplexity.Bycontrast,
on rougher terrains, our approach is simple to implement, requiring minimal
3) andwevalidateourapproachonarealSoloquadruped changestoexistinglocomotionRLpipelinesandintroducing
robottoovercomediverseobstaclesinaparkourwhile no additional computational overhead.
satisfying safety and style constraints. Terminating the future rewards and resetting the episode
is ubiquitously used in reinforcement learning to avoid
II. RELATEDWORK
certain behaviors. For instance, [11] terminates the episode
Reinforcement learning has emerged as a particularly with a low reward when the robot base or knees touch
effective method for obtaining agile and adaptive policies the ground. [51] further showed that learning policies for
for quadruped robots. While some approaches attempt to early-terminated Markov decision processes (ET-MDP), i.e.
train RL locomotion policies directly on physical quadruped terminating future rewards on constraint violations without
robots by leveraging sample-efficient RL techniques [22], necessarily resetting the environment, is an effective way
[23], a popular approach entails training policies in simu- to learn constraint-satisfying policies. However, our experi-
lation before transferring them to the real world [24], [25], ments highlight that this approach does not readily scale to
[26],[27].Thistransferreliesonaccuratephysicssimulators complex systems such as quadruped robots with dozens of
and domain randomization to ensure policy transferability constraints. We propose in the next section to capitalize on
to the physical robot [28], [29], [30]. Recently, GPU-based this common practice to design a novel approach to enforce
simulators capable of simulating thousands of robots in generic hard constraints in RL. To that end, we first refor-
parallel [31], [32], [33] have streamlined this process [11]. mulate the constraint as a probability of satisfaction. Then
The resulting policies exhibit natural, animal-like motions we introduce stochastic terminations as a way to downscale
andcanadapttochallengingterrainconfigurations[34],[35], the sum of future possible rewards while keeping a dense
[2], [1], [4], [3], [36]. In our experiments, we follow this feedback to the policy, in particular by keeping informative
sim-to-real approach and deploy our policies on the Solo-12 direction from the domain outside constraint satisfaction.
robot [16], [37] for challenging terrain traversal.
Incorporating constraints is a common practice in model- III. METHOD
based control, where their importance to ensure robot safety A. Problem Formulation
is commonly accepted [38], [39], [40]. Yet constraints have
We consider an infinite, discounted Markov Decision
garnered limited attention in the RL community, where the
Process S,A,r,γ,T with state space S, action space A,
main effective solvers do not readily consider them [8], [41]
reward function r, discount factor γ and dynamics T. RL
and achieving policies that comply with constraints is often
aims to find a policy π that maximizes the discounted sum
donethroughintricaterewardshaping.Inleggedlocomotion,
of future rewards:
thisapproachtypicallyresultsinrewardfunctionscomprising
numeroustermsthatarelabor-intensivetotune.Forinstance, (cid:34) (cid:88)∞ (cid:35)
maxE γtr(s ,a ) . (1)
the reward functions used in [11], [26] comprise a dozen of τ∼π,T t t
π
terms. Moreover, the resulting policy, being a compromise t=0
among maximizing each of these terms, is not guaranteed to In the following, we assume positive rewards r ⩾ 0 for
satisfy constraints in all situations [21]. simplicity (without loss of generality w.r.t. any other lowerFig. 3: (Left) The quadruped robot is trained with CaT in simulation using height-map scan. (Right) The learned policy
is directly deployed on the real robot. Knowing the obstacle course on which the robot is placed, we use external motion
capture cameras to reconstruct the height-map of its surroundings based on its position and orientation in the world.
bounded definition). Constrained RL additionally introduces [51] showed that if the minimum value of the rewards is
a set of constraint functions {c : S ×A → R,i ∈ I} and high enough, which can be easily obtained by adding a
i
aimstomaximizerewardswhilelimitingthediscountedsum high enough constant value, the learned policy will satisfy
of constraints over the trajectories generated by the policy: the constraints. However, terminating the episode if any
(cid:34) ∞ (cid:35) constraint is violated might be overly conservative with
(cid:88)
E γtc (s ,a ) ≤ϵ ∀i∈I. (2) respecttotheconstraintsandimpairexplorationandlearning.
τ∼π,T i t t i
Moreover,suchaterminationconditionoffersasparsesignal
t=0
for recovering from constraint violations: once the agent
While standard in the RL literature [48], [20], this formula-
enters a region of constraint violation, the episode always
tion includes a notion of budget for the constraints. We con-
terminates and the agent does not learn anything.
sider instead maximizing rewards while avoiding constraint
violationateachtimestep:E (cid:2)(cid:80)∞ γt1 (cid:3) ≤ 3) Stochastic terminations: We propose that δ t can take
τ∼π,T t=0 ci(st,at)>0
values beyond 0 or 1 depending on the constraint violations
ϵ , where 1 indicates whether the i-th constraint
i γtci(st,at)>0
at time t. As a result, any violation of a constraint leads to
has been violated at time t. This is equivalent to:
a probability of terminating the future rewards the RL agent
P (s,a)∼ρπ γ,T [c i(s,a)>0]≤ϵ˜ i ∀i∈I, (3) could have achieved. If no constraints are violated, then the
episodeterminateswithaprobabilityofzero,whereasifone
where ρπ,T corresponds to the discounted state-action occu-
γ or more constraints are violated, δ may take positive values
pancydistribution ofthepolicy π.While thiscorrespondsto
between0and1.Inthatcase,thesumofallfuturerewardsat
aspecialcaseofthemoregeneralconstrainedRLsetting,this
tandaftertimearere-scaledby(1−δ ).Therefore,inorder
formulation, akin to chance-constrained optimization [52], t
to maximize the sum of future rewards, the agent naturally
[53], encompasses many practical applications of RL for
gravitates towards satisfying the constraints. Allowing δ to
robotic control.
takevaluesin]0,1[enablestheagenttolearntorecoverfrom
B. Constraints as Terminations constraintviolations.Moreover,dependingonthevalueofδ,
1) Reformulation: Instead of directly solving (1) under this allows some exploration inside the region of constraint
the constraints (3), we propose to reformulate it as: violation.
(cid:34) ∞ (cid:32) t (cid:33) (cid:35) By designing δ such that it is increasing with c i, the
max E (cid:88) (cid:89) γ(1−δ(s ,a )) r(s ,a ) , (4) termination probability will provide a dense signal to the
t′ t′ t t
π τ∼π learning algorithm to recover from constraints. Driven by
t=0 t′=0
simplicity, we propose the following termination probability
where we introduce a random variable δ : S ×A → [0,1]
t function:
indicating whether the episode terminates and the future c+
rewards are terminated from time step t. Importantly, we δ =m i∈a Ix pm iaxclip( cmi ax,0,1), (6)
proposetodesignδ asafunctionoftheconstraintviolations i
t
c i.Notethatepisodeterminationsarenotenvironmentresets, where c+ i =max(0,c i(s,a)) is the violation of constraint i,
butmerelyfuturerewardterminationsfromapolicylearning cm iax isanexponentialmovingaverageofthemaximumcon-
perspective. Under the expectation, the Bernoulli variable straint violation over the last batch of experience collected
and its probability are the same. In the rest of the paper, in the environment:
δ t will refer directly to the probability of termination. cmax ←τccmax+(1−τc) max c+(s,a), (7)
2) Naive termination: A naive approach is to terminate i i (s,a)∈batch i
the future rewards if any constraint is violated [51] with the with decay rate τc ∈]0,1[ and pmax a hyperparameter that
following binary function for δ: i
controls the maximum termination probability for the con-
δ =1−(cid:89) 1 . (5) straint i. We found directly using the maximum over the
ci≤0
batch of experience without exponential moving average to
i∈Ibe slightly less stable. Hence, the termination probability
Taskformulation:throughrewards(OptionA)
for each constraint is proportional to the magnitude of the
constraintviolation,whilethedynamicupdateofcm iax makes Rewardfunction r=e−∥vxde ys 0− .2v 5xy∥2 2 + 1 2e−|ωzde 0s− .2ω 5z|2
sure that the termination function always provides a relevant
Taskformulation:throughsoftconstraints(OptionB)
learningsignalthroughouttraining.Wefoundthatthisdesign
wassimpletoimplementwhileachievingeffectiveconstraint Rewardfunction r=1
satisfaction. AL ni gn uea lar rv ve elo loc cit iy tytr ta rac ck kin ing
g
c l cin anv gel ve= l=(cid:13) (cid:13)v (cid:12) (cid:12)x ωde y zds e− s−vx ωy z(cid:13) (cid:13) (cid:12) (cid:12)2 −−
ϵ
tϵ ratr ca kck
Algorithm 1 Implementation of CaT with PPO, with alter- Hardconstraintsforsafety
ations from the original RL algorithm highlighted in red.
Kneeorbasecollision c knee/basecontact=1 knee/basecontact
1: for epoch=1 to N do Footcontactforce c footcontactj =∥ffootj∥2−flim
2: data ← PPO.collect trajectories() Softconstraintsforsafety(∀k∈1..12)
3: compute δ(data.constraints) using (6)
654 7:::
:
endPdd faa
P
ott
O
raa .. .dr ue o pw n da e ar s td e←s p← oδ lid cyat (a d. are taw )ards×(1−δ) JoJ ino Atin cT a tto ic ov cr nq e eu l lo e re arc a til et ti y im o lini l mt is m l ii ti m sts its
c
actionc raj tc o eij knoc tin a =t to cvr cq e eu
(cid:12) (cid:12)
(cid:12)l lo e ∆e rck ai qt ty i td= ok
e
,n
s
kk= −|τ d= ∆k t|| q q|˙ k t− dq
e
−¨| k
s
1τ − | ,l − kim q
(cid:12) (cid:12)
(cid:12)˙l q¨i −m lim
q˙deslim
Softconstraintsforstyle
Our proposed approach, Constraints as Terminations (Activeonflatterrainsonly,∀j∈1..4)
(CaT),caneasilybeincorporatedintoexistingRLalgorithms Baseorientation c ori=∥baseorixy∥ 2−baselim
with minimal changes, by simply computing δ based on the Hiporientation c hipj =|hipori j|−hiplim
constraint violations using (6), multiplying the rewards by δ Footairtime c airtimej =td aie rs time−t airtimej
and rewriting the terminations with δ. These modifications Numberoffootcontacts c nfootcontacts=|n footcontact−nd foe os tcontact|
canbeimplementedwithjustafewlinesofcodestoexisting
Standstillifvdes=0 c still=(∥q−q⋆∥2−ϵ still)×1
vdes=0
RL algorithms. Algorithm 1 highlights the changes needed
TABLE I: Rewards and constraints used in our experiments.
to implement our approach on top of PPO.
b) Safety constraints: Safety constraints are defined to
IV. APPLICATIONTOLEGGEDLOCOMOTION ensure the policy learned in simulation will transfer well
and safely to the physical robot once training is complete.
We train a policy in simulation using CaT and directly
We prohibit collisions to the knee and the base of the robot
transfer the policy to a real Solo-12 robot (see Fig. 3).
to avoid dangerous behaviors that might destroy the robot.
For this quadruped locomotion problem, the state space S
We limit the contact force of each foot n to prevent the
corresponds to the measured positions q and velocities q˙
t t robot from hitting the ground too harshly, and we limit
of all 12 joints of the robot, the previous action a and
t−1 the torque applied to each joint k to prevent damaging
the linear and angular velocity commands vdes and ωdes that
xy z the actuators. To ensure that the generated motions are
the robot must track. For non-blind navigation, the robot
smooth for seamless sim-to-real transfer, we also limit joint
also observes the height-scan h of its surroundings. The
scan velocities, joint accelerations and action rates.
action space A corresponds to desired joint position offsets
c) Styleconstraints: Styleconstraintsareusedtoguide
a = ∆qdes with respect to a default joint configuration q⋆,
t t learning towards natural-looking motions. However, defin-
that are then converted to torques through a proportional-
ing relevant style constraints in any terrain configuration
derivative (PD) controller operating at a higher frequency
is difficult. We propose to enforce style constraints only
than the neural policy. The derivative part of the controller
on flat surfaces while deactivating them (i.e. set them to
aims to bring the joint velocity to zero.
0) otherwise. This allows us to define a precise style to
One might consider that each reward and constraint can
follow on flat terrains while providing room for the RL
serve one of these three purposes:
algorithmtoadaptthelearnedbehavioronmorechallenging
• define the task to be achieved, terrains. In our implementation, the terrain is considered flat
• ensurethatthegeneratedtrajectoriesaresafeandtrans- if the variance of the scan dots is below a certain threshold
ferable to the physical robot, var(h )<varlim . We limit the orientation of the base and
scan scan
• or impose a style to the generated motions. the angle of the hips. When the velocity command is above
The complete list of rewards and constraints used in our athreshold,weadditionallygroundtheflyingphaseduration
experiments is provided in Table I. We detail them below. ofeachfootandlimittotwothenumberoffootcontactswith
a) Task definition: The legged locomotion task is to the ground whereas, if no velocity is provided, we force the
trackthelinearvelocitycommandinhorizontaldirectionvdes robot to go back to its default pose.
xy
and yaw rate ωdes. We consider a velocity tracking reward d) Soft and hard constraints: Our method introduces
z
function widely used in RL for legged locomotion (Option a hyperparameter pmax for each constraint which trades off
i
A)[11],[21].Alternatively,weproposetodefinethevelocity explorationwithconstraintsatisfaction.Ahighvalueofpmax
i
tracking task as a constraint to be satisfied (Option B). will ensure that the constraint is strictly satisfied duringcleparkourcomprisingasetofstairs,aslopeandaplatform
Method Rewards Cstr.
roughly the height of the robot (see Fig. 1). Following
Hardconstraintsonly 0 n.a.
ET-MDP[51] 0 n.a. Table I, we consider two versions of CaT: one with the
N-P3O[50],[20],[21] 593.2(±49.5) 8%(±1%) task defined through rewards (CaT (Tracking Rewards)) and
CaT(TrackingRewards) 682.9(±5.8) 0.5%(±0.3%) onewiththetaskdefinedthroughconstraints(CaT(Tracking
TABLE II: Average sum of rewards (Rewards) and average Constraints)). We compare CaT to the following baselines:
time proportion of torque constraint violation for any joint • ET-MDP: a modification of our method designed to
(Cstr.) achieved by the policies on flat terrain in simulation. resemble [51] by using (5) to compute δ.
Results are averaged over 4 training seeds. • N-P3O: our reproduction of P3O [50] using techniques
from [20], [21].
training but might lead to overly conservative exploration,
whereas lower values of pmax will allow the learning agent • Hard constraints only: an ablation of our approach
i where we use pmax =1.0 for all constraints.
to discover higher reward regions of the behavior space. i
• Style always active: an ablation of our approach where
Motivated by simplicity, we propose to classify constraints
style constraints are always enforced.
into two groups: hard constraints with pmax = 1.0 for
i For N-P3O, we group constraints of the same type together
constraintsthatshouldneverbeviolated,andsoftconstraints,
following [20], [21], use dense constraint functions as in
where pmax increases from 0.05 to 0.25 throughout the
i CaT as opposed to indicator functions used in [20], [21],
course of training, that the RL algorithm might violate
andemployfootphasedurationandnumberoffootcontacts
during exploration and learn to recover from. We found that
as rewards rather than constraints, as N-P3O struggles to
this design allowed the agent to maximally learn complex
converge otherwise. Solo-12 is a light robot with dynamic,
locomotion skills while further enforcing the constraints in
but limited actuators that should avoid applying a torque of
the later stage of training. In our experiments, base or knee
morethan3Nm.Toevaluatethecapabilitiesofourapproach
contactcollisionsandfootcontactforcesaredefinedashard
to enforce constraints, we focus on the torque constraint
constraints and the rest of the constraints as soft ones.
satisfaction and report the proportion of time where this
This set of constraints results in a large constraint vector
constraint is violated for one or more joints.
comprising more than 60 terms. While prior approaches
group constraints together [20], [21], we found that this B. Results and Analysis
additional engineering burden was unnecessary for CaT.
We first compare CaT (Tracking Rewards) to N-P3O, ET-
MDP and Hard constraints only trained on a flat terrain
V. EXPERIMENTS
for blind locomotion in simulation. Table II reports the re-
A. Experimental setup
wardsandthetorqueconstraintssatisfactionachievedbythe
To train our policies, we leverage the PPO algorithm [8] policies. ET-MDP entirely fails to learn locomotion policies
using the implementation from rl-games [54], which we in our high-dimensional constraint problem. This may be
slightlymodifiedtoaccommodatenon-booleanterminations, due to the fact that at the beginning of training, the robot
alongside massively parallel simulation of Isaac Gym [33]. always violates some constraints, preventing any reward or
Hyperparameters are provided in Appendix VI-A. Blind constraintfeedbacktoallowpolicylearning.Similarly,when
policies for flat terrains are trained for 2000 epochs whereas the constraints are enforced too roughly (Hard constraints
policies with height-scan map are trained for 20000 epochs only), learning fails completely, as overly stringent enforce-
for agile terrain traversal. This amounts to respectively 1 mentofconstraintshindersexplorationandlearning.Despite
hour and 10 hours of training on a single V100 GPU. Ex- being simpler, CaT outperforms N-P3O, in both the sum
cept for CaT specific implementations, the resulting training of tracking rewards attained and the satisfaction of torque
procedure is similar to [11]. constraints after 2000 epochs of training. We hypothesize
After training in simulation, the controller is directly that the integration of rewards and constraints into a unified
deployed on a real Solo-12 robot. The policy runs at 50 RL framework allows CaT to learn faster.
Hz on a Raspberry Pi 4 Model B using a custom C++ Next, we deploy CaT with height-scan map on the real
implementation.Targetjointpositionsaresenttotheonboard robot. In Table III, we report the success rate of traversing
PD controller running at 10 kHz. PD gains are kept low to each obstacle in the parkour. CaT with both sets of rewards
obtain a compliant impedance controller that will achieve a and constraints successfully learns agile locomotion skill
behavior close to torque control and will be able to dampen to overcome each obstacle of the parkour. Fig. 1 shows a
andabsorbimpacts[26].Thisisfurthermadepossiblethanks full traversal of the obstacle parkour, demonstrating natural
to the transparent actuation of Solo-12. For more details on motions on flat surfaces while achieving agile skills on
the hardware, please refer to [16], [37]. Instead of directly morechallengingobstacles.Notably,CaTsuccessfullylearns
capturing a height-scan map of the robot’s surrounding to overcome all the obstacles while satisfying the torque
terrain, we use motion capture to track the position of the constraint.Fig.4showsthat,whileclimbingontheplatform
robot and sample the corresponding height map points. almost as high as the robot, the torque remains within
To validate the agility of the learned policies in diverse the limit set during training. Interestingly, CaT (Tracking
scenarios, we evaluate our approach on a challenging obsta- Constraints), where the locomotion task is defined entirelyFrontStairs SidewaysStairs Slope Platform Average
Method
Succ. Cstr. Succ. Cstr. Succ. Cstr. Succ. Cstr. Succ. Cstr.
Stylealwaysactive 50.0% 2.5% 40.0% 4.8% 30.0% 2.0% 17.5% 5.4% 34.4% 3.7%
CaT(TrackingRewards) 100.0% 0.08% 42.5% 0.3% 97.5% 0.3% 77.5% 1.1% 79.4% 0.5%
CaT(TrackingConstraints) 97.5% 0.5% 85.0% 1.8% 95.0% 1.2% 85.0% 3.4% 90.6% 1.7%
TABLE III: Average success rate (Succ.) and average time proportion of torque constraint violation for any joint (Cstr.)
achieved by the policies on the different obstacles of the parkour on the real robot: walking up the stairs from the front
(Front Stairs) and sideways (Sideways Stairs), walking up the slope (Slope) and walking up the platform as high as the
robot’s base (Platform). Results are averaged over 4 random training seeds and 10 attempts per obstacle per seed.
50
40
30
16
8
0 Fig. 5: CaT trained with a constraint that limits the height
of the base learns crouching locomotion skills.
8
−
stacles. This occurs because adhering strictly to certain
16
− style constraints, as defined on flat surfaces, may not be
compatible with other scenarios. For example, imposing the
3.0 constraint that the robot’s base must remain horizontal is
incompatiblewithscenariosinvolvingstairclimbing.Thisis
1.5
particularly striking when attempting to climb the platform,
0.0 which requires tilting the base and lift the shoulders, as
illustrated in Fig. 4 (top).
1.5 In Fig. 5, we illustrate how simply adding a constraint to
−
FrontLeft FrontRight HindLeft HindRight limit the height of the base (c =height −heightmax)
3.0 height base base
− can learn crouching locomotion skills on the quadruped.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
Time[s] Videos of the robot traversing the parkour and crouching
are available in the supplementary video.
Fig. 4: Joint torques and velocities during the climb of a 24
cm platform. For clarity, we only report data for the knee VI. CONCLUSION
joints, which had the highest torque peaks.
In this study, we introduce CaT, a novel and minimalist
through constraints, learns agile locomotion skills. In partic- algorithm addressing constraints in reinforcement learning.
ular,itoutperformsCaT(TrackingRewards)onclimbingthe We formulate the problem so that the probability of con-
stairs sideways, a difficult task where the height-scan map straint violation is bounded and use stochastic termination
provideslessvisibility.Bycontrast,CaT(TrackingRewards) to seamlessly integrate it on top of standard algorithms such
oftenrefusestowalkoverthestairssidewayswhileachieving as PPO. On a Solo-12 quadruped robot, CaT successfully
similarperformancesonotherobstacles.Wehypothesizethat manages to learn agile locomotion skills on challenging
CaT (Tracking Constraints) is more prone to explore unsafe terrain traversals, showcasing its utility in enforcing safety
behaviors to fulfill the task constraints, resulting in better andstylisticconstraintswithinquadrupedlocomotion.Future
success rates at the expense of more constraint violations. work could explore more principled ways to define the
This highlights how stochastic termination functions can be termination conditions based on the constraints.
used to appropriately shape the behavior of the robot policy, From a practical standpoint, constrained RL significantly
either to ensure the controller is safe and adhere to a certain simplifies the reward engineering process. However, unlike
style, but also to fully define the intended task for the robot. previous, more intricate methods, our approach is notably
We then compare CaT to always enforcing style con- simpler to implement, necessitating minimal code adjust-
straints, even on challenging terrains (Style always active). mentsandisdevoidofanycomputationaloverhead.Wehope
While this approach successfully learns walking skills on the effectiveness and simplicity of our approach will foster
flat and rough terrains, it struggles on more difficult ob- the democratization of constrained RL in robotics.
]mc[thgieH
]s/dar[yticolevtnioJ
]mN[euqrottnioJACKNOWLEDGEMENTS [18] F. Risbourg, T. Corbe`res, P.-A. Le´ziart, T. Flayols, N. Mansard, and
S. Tonneau, “Real-time footstep planning and control of the solo
ThisworkwasfundedinpartbytheCOCOPILprojectof quadrupedrobotin3denvironments,”in2022IEEE/RSJInternational
Re´gion Occitanie (France), the AS2 ANR-22-EXOD-0006 ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2022,
pp.12950–12956.
oftheFrenchPEPRO2R,theDynamogradejointlaboratory
[19] P.-A. Le´ziart, T. Corbe`res, T. Flayols, S. Tonneau, N. Mansard,
(grant ANR-21-LCV3-0002) and ROBOTEX 2.0 (Grants and P. Soue`res, “Improved control scheme for the solo quadruped
ROBOTEX ANR-10-EQPX-44-01 and TIRREX-ANR-21- and experimental comparison of model predictive controllers,” IEEE
ESRE-0015). It was granted access to the HPC resources of
RoboticsandAutomationLetters,vol.7,no.4,pp.9945–9952,2022.
[20] Y. Kim, H. Oh, J. Lee, J. Choi, G. Ji, M. Jung, D. Youm, and
IDRIS under the allocations 2021-AD011012947 and 2023-
J.Hwangbo,“Notonlyrewardsbutalsoconstraints:Applicationson
AD011014301 made by GENCI. leggedrobotlocomotion,”arXivpreprintarXiv:2308.12517,2023.
[21] J. Lee, L. Schroth, V. Klemm, M. Bjelonic, A. Reske, and M. Hut-
REFERENCES ter,“Evaluation ofconstrainedreinforcementlearning algorithmsfor
leggedlocomotion,”arXivpreprintarXiv:2309.15430,2023.
[1] A.Agarwal,A.Kumar,J.Malik,andD.Pathak,“Leggedlocomotion [22] L.Smith,I.Kostrikov,andS.Levine,“Awalkinthepark:Learning
in challenging terrains using egocentric vision,” in Conference on towalkin20minuteswithmodel-freereinforcementlearning,”arXiv
RobotLearning. PMLR,2023,pp.403–415. preprintarXiv:2208.07860,2022.
[2] X.Cheng,K.Shi,A.Agarwal,andD.Pathak,“Extremeparkourwith [23] P.Wu,A.Escontrela,D.Hafner,P.Abbeel,andK.Goldberg,“Day-
leggedrobots,”arXivpreprintarXiv:2309.14341,2023. dreamer:Worldmodelsforphysicalrobotlearning,”inConferenceon
[3] D. Hoeller, N. Rudin, D. Sako, and M. Hutter, “Anymal parkour: RobotLearning. PMLR,2023,pp.2226–2240.
Learning agile navigation for quadrupedal robots,” arXiv preprint [24] X.B.Peng,M.Andrychowicz,W.Zaremba,andP.Abbeel,“Sim-to-
arXiv:2306.14874,2023. realtransferofroboticcontrolwithdynamicsrandomization,”in2018
[4] Z.Zhuang,Z.Fu,J.Wang,C.Atkeson,S.Schwertfeger,C.Finn,and IEEE international conference on robotics and automation (ICRA).
H.Zhao,“Robotparkourlearning,”inConferenceonRobotLearning IEEE,2018,pp.3803–3810.
(CoRL),2023. [25] G.Margolis,G.Yang,K.Paigwar,T.Chen,andP.Agrawal,“Rapid
[5] X.B.Peng,E.Coumans,T.Zhang,T.-W.Lee,J.Tan,andS.Levine, locomotion via reinforcement learning,” in Robotics: Science and
“Learningagileroboticlocomotionskillsbyimitatinganimals,”arXiv Systems,2022.
preprintarXiv:2004.00784,2020. [26] M. Aractingi, P.-A. Le´ziart, T. Flayols, J. Perez, T. Silander, and
[6] A.Escontrela,X.B.Peng,W.Yu,T.Zhang,A.Iscen,K.Goldberg, P. Soue`res, “Controlling the solo12 quadruped robot with deep rein-
and P. Abbeel, “Adversarial motion priors make good substitutes for forcementlearning,”scientificReports,vol.13,no.1,p.11945,2023.
complex reward functions,” in 2022 IEEE/RSJ International Confer- [27] ——,“Ahierarchicalschemeforadaptinglearnedquadrupedlocomo-
ence on Intelligent Robots and Systems (IROS). IEEE, 2022, pp. tion,”in2023IEEE-RAS22ndInternationalConferenceonHumanoid
25–32. Robots(Humanoids). IEEE,2023,pp.1–8.
[7] T. Li, Y. Zhang, C. Zhang, Q. Zhu, J. Sheng, W. Chi, C. Zhou, and [28] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bo-
L. Han, “Learning terrain-adaptive locomotion with agile behaviors hez, and V. Vanhoucke, “Sim-to-real: Learning agile locomotion for
byimitatinganimals,”in2023IEEE/RSJInternationalConferenceon quadrupedrobots,”inProceedingsofRobotics:ScienceandSystems,
IntelligentRobotsandSystems(IROS). IEEE,2023,pp.339–345. 2018.
[8] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, [29] A. Kumar, Z. Fu, D. Pathak, and J. Malik, “Rma: Rapid motor
“Proximal policy optimization algorithms,” arXiv preprint adaptationforleggedrobots,”arXivpreprintarXiv:2107.04034,2021.
arXiv:1707.06347,2017. [30] Z.Xie,X.Da,M.VandePanne,B.Babich,andA.Garg,“Dynamics
[9] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum randomization revisited: A case study for quadrupedal locomotion,”
learning,”inProceedingsofthe26thannualinternationalconference in2021IEEEInternationalConferenceonRoboticsandAutomation
onmachinelearning,2009,pp.41–48. (ICRA). IEEE,2021,pp.4955–4961.
[10] P.Soviany,R.T.Ionescu,P.Rota,andN.Sebe,“Curriculumlearning: [31] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for
Asurvey,”InternationalJournalofComputerVision,vol.130,no.6, model-basedcontrol,”in2012IEEE/RSJInternationalConferenceon
pp.1526–1565,2022. IntelligentRobotsandSystems. IEEE,2012,pp.5026–5033.
[11] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, “Learning to walk [32] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch,
in minutes using massively parallel deep reinforcement learning,” in and O. Bachem, “Brax - a differentiable physics engine for
ConferenceonRobotLearning,2022. large scale rigid body simulation,” 2021. [Online]. Available:
[12] S.Chen,B.Zhang,M.W.Mueller,A.Rai,andK.Sreenath,“Learning http://github.com/google/brax
torquecontrolforquadrupedallocomotion,”in2023IEEE-RAS22nd [33] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Mack-
InternationalConferenceonHumanoidRobots(Humanoids). IEEE, lin, D. Hoeller, N. Rudin, A. Allshire, A. Handa, and G. State,
2023,pp.1–8. “Isaacgym:Highperformancegpu-basedphysicssimulationforrobot
[13] G.BellegardaandA.Ijspeert,“Visualcpg-rl:Learningcentralpattern learning,”2021.
generators for visually-guided quadruped navigation,” arXiv preprint [34] Z. Fu, A. Kumar, J. Malik, and D. Pathak, “Minimizing energy
arXiv:2212.14400,2022. consumptionleadstotheemergenceofgaitsinleggedrobots,”arXiv
[14] S.Kajita,F.Kanehiro,K.Kaneko,K.Fujiwara,K.Harada,K.Yokoi, preprintarXiv:2111.01674,2021.
andH.Hirukawa,“Bipedwalkingpatterngenerationbyusingpreview [35] G.Bellegarda,Y.Chen,Z.Liu,andQ.Nguyen,“Robusthigh-speed
controlofzero-momentpoint,”in2003IEEEinternationalconference running for quadruped robots via deep reinforcement learning,” in
on robotics and automation (Cat. No. 03CH37422), vol. 2. IEEE, 2022 IEEE/RSJ International Conference on Intelligent Robots and
2003,pp.1620–1626. Systems(IROS). IEEE,2022,pp.10364–10370.
[15] F. Farshidian, M. Neunert, A. W. Winkler, G. Rey, and J. Buchli, [36] H.Duan,B.Pandit,M.S.Gadde,B.J.vanMarum,J.Dao,C.Kim,
“Anefficientoptimalplanningandcontrolframeworkforquadrupedal andA.Fern,“Learningvision-basedbipedallocomotionforchalleng-
locomotion,”in2017IEEEInternationalConferenceonRoboticsand ingterrain,”arXivpreprintarXiv:2309.14594,2023.
Automation(ICRA). IEEE,2017,pp.93–100. [37] F. Grimminger, A. Meduri, M. Khadiv, J. Viereck, M. Wu¨thrich,
[16] P.-A.Le´ziart,T.Flayols,F.Grimminger,N.Mansard,andP.Soue`res, M. Naveau, V. Berenz, S. Heim, F. Widmaier, T. Flayols et al.,
“Implementation of a reactive walking controller for the new open- “An open torque-controlled modular robot architecture for legged
hardwarequadrupedsolo-12,”in2021IEEEInternationalConference locomotionresearch,”IEEERoboticsandAutomationLetters,vol.5,
onRoboticsandAutomation(ICRA). IEEE,2021,pp.5007–5013. no.2,pp.3650–3657,2020.
[17] E. Dantec, M. Naveau, P. Fernbach, N. Villa, G. Saurel, O. Stasse, [38] W. Jallet, A. Bambade, E. Arlaud, S. El-Kazdadi, N. Mansard, and
M. Taix, and N. Mansard, “Whole-body model predictive control J.Carpentier,“Proxddp:Proximalconstrainedtrajectoryoptimization,”
for biped locomotion on a torque-controlled humanoid robot,” in 2023.
2022 IEEE-RAS 21st International Conference on Humanoid Robots [39] B.Stellato,G.Banjac,P.Goulart,A.Bemporad,andS.Boyd,“OSQP:
(Humanoids). IEEE,2022,pp.638–644. an operator splitting solver for quadratic programs,” MathematicalProgramming Computation, vol. 12, no. 4, pp. 637–672, 2020.
[Online].Available:https://doi.org/10.1007/s12532-020-00179-2 TABLE IV: Environment hyperparameters
[40] S. Tonneau, D. Song, P. Fernbach, N. Mansard, M. Ta¨ıx, and
A. Del Prete, “Sl1m: Sparse l1-norm minimization for contact plan- Numberofenvs. 4096
ning on uneven terrain,” in 2020 IEEE International Conference on Randomvx range [−0.3,1.0]m/s
RoboticsandAutomation(ICRA). IEEE,2020,pp.6604–6610. Randomvy range [−0.7,0.7]m/s
[41] T.Haarnoja,A.Zhou,P.Abbeel,andS.Levine,“Softactor-critic:Off- Randomωz range [−0.78,0.78]rad/s
policymaximumentropydeepreinforcementlearningwithastochastic Proportionalgain 4.0Nm/rad
actor,” in International conference on machine learning. PMLR, Derivativegain 0.2Nm/(rad/s)
2018,pp.1861–1870. Actionscaling 0.5
[42] T.-Y. Yang, T. Zhang, L. Luu, S. Ha, J. Tan, and W. Yu, “Safe Defaultlegangles [0.05,0.4,-0.8]rad
reinforcement learning for legged locomotion,” in 2022 IEEE/RSJ Simulationtimestep 5ms
International Conference on Intelligent Robots and Systems (IROS). Episodelength 10s
IEEE,2022,pp.2454–2461. Heightscangrid 13x11points
[43] T. He, C. Zhang, W. Xiao, G. He, C. Liu, and G. Shi, “Agile but Heightscanstep 8cm
safe:Learningcollision-freehigh-speedleggedlocomotion,”inarXiv,
2024.
[44] M. Alshiekh, R. Bloem, R. Ehlers, B. Ko¨nighofer, S. Niekum, and
U.Topcu,“Safereinforcementlearningviashielding,”inProceedings TABLE V: Learning hyperparameters
oftheAAAIconferenceonartificialintelligence,vol.32,no.1,2018.
[45] K. Fan, Z. Chen, G. Ferrigno, and E. De Momi, “Learn from safe Actornetwork [512,256,128]
experience:Safereinforcementlearningfortaskautomationofsurgical Criticnetwork [512,256,128]
robot,”IEEETransactionsonArtificialIntelligence,2024. Activation Elu
[46] Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone, “Risk- Discountfactor 0.99
constrainedreinforcementlearningwithpercentileriskcriteria,”Jour- GAEcoefficient 0.95
nalofMachineLearningResearch,2018. PPOclipping 0.2
[47] C. Tessler, D. J. Mankowitz, and S. Mannor, “Reward constrained Entropycoefficient 1e-3
policyoptimization,”arXivpreprintarXiv:1805.11074,2018. Learningrate 3e-4
[48] J.Achiam,D.Held,A.Tamar,andP.Abbeel,“Constrainedpolicyop- Learningrateschedule Adaptive
timization,”inInternationalconferenceonmachinelearning. PMLR, KLthresholdforadaptiveschedule 8e-3
2017,pp.22–31. Maximumgradientnorm 1.0
[49] Y. Liu, J. Ding, and X. Liu, “Ipo: Interior-point policy optimization Horizonlength 24
underconstraints,”inProceedingsoftheAAAIconferenceonartificial Minibatchsize 16384
intelligence,vol.34,no.04,2020,pp.4940–4947. Miniepochs 5
[50] L. Zhang, L. Shen, L. Yang, S. Chen, B. Yuan, X. Wang, and Criticcoefficient 2
D.Tao,“Penalizedproximalpolicyoptimizationforsafereinforcement
learning,”arXivpreprintarXiv:2205.11814,2022.
[51] H. Sun, Z. Xu, Z. Peng, M. Fang, T. Wang, B. Dai, and B. Zhou,
“Constrainedmdpscanbesolvedbyeearly-terminationwithrecurrent TABLE VI: Constraints hyperparameters
models,” in NeurIPS 2022 Foundation Models for Decision Making
Workshop,2022.
Torqueτlim 3Nm
[52] A. Charnes and W. W. Cooper, “Chance-constrained programming,”
Jointvelocityq˙lim 16rad/s
Managementscience,vol.6,no.1,pp.73–79,1959.
[53] A. Nemirovski and A. Shapiro, “Convex approximations of chance
Jointaccelerationq¨lim 800rad/s2
constrainedprograms,”SIAMJournalonOptimization,vol.17,no.4, Actionrateq˙des,lim 80rad/s
pp.969–996,2007. Baseorientationbaselim 0.1rad
[54] D.MakoviichukandV.Makoviychuk,“rl-games:Ahigh-performance Contactforceflim 50N
frameworkforreinforcementlearning,”https://github.com/Denys88/rl Hipanglehiplim 0.2rad
games,May2021. Airtimettarget 0.25s
airtime
Numberoffootcontactsntarget 2
APPENDIX
Velocitytrackingϵ
traf co kotcontact
0.2m/sorrad/s
A. Hyperparameters
[54] details the meaning of some hyperparameters.
TABLE VII: Ranges and dimensions of uniform noise for
randomizing the dynamics and observations.
RandomObservations:
θ U3(−0.05,0.05)
body RandomDynamics:
ω U3(−0.001,0.001)
body
q U12(−0.01,0.01) GroundFriction U(0.5,1.25)
q˙ U12(−0.2,0.2)
hscan U143(−0.01,0.01)