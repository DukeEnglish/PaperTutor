ImageNet-D: Benchmarking Neural Network Robustness on
Diffusion Synthetic Object
ChenshuangZhang1 FeiPan2 JunmoKim1* InSoKweon1 ChengzhiMao3,4*
KAIST1,UniversityofMichigan,AnnArbor2,McGillUniversity3,MILA4
Abstract
We establish rigorous benchmarks for visual percep-
tion robustness. Synthetic images such as ImageNet-C,
ImageNet-9, and Stylized ImageNet provide specific type
ofevaluationoversyntheticcorruptions,backgrounds,and
textures, yet those robustness benchmarks are restricted
in specified variations and have low synthetic quality. In Tennis racket Measuring cup Mixing/salad bowl
thiswork, weintroducegenerativemodelasadatasource
Figure1. ToppredictionsfromCLIP(ViT-L/14)onImageNet-D.
for synthesizing hard images that benchmark deep mod-
Wesynthesizetheimagesbychangingtheirbackground, texture
els’ robustness. Leveraging diffusion models, we are able
and material. The groundtruth for the images are plunger, spat-
togenerateimageswithmorediversifiedbackgrounds,tex-
ula,andladleinorder,togetherwiththebackground(badminton
tures, and materials than any prior work, where we term
court),texture(freckled),andmaterial(painted).
thisbenchmarkasImageNet-D.Experimentalresultsshow
that ImageNet-D results in a significant accuracy drop to
common visual corruptions, such as gaussian noise and
a range of vision models, from the standard ResNet vi-
blur,totestmodels’robustness. ImageNet-9[59]usessim-
sualclassifiertothelatestfoundationmodelslikeCLIPand
plecuttingandpastetechniquetocreaterobustnessbench-
MiniGPT-4, significantly reducing their accuracy by up to
markonobjectbackground,yettheimagesarenotrealistic.
60%. Our work suggests that diffusion models can be an
effectivesourcetotestvisionmodels. Thecodeanddataset Stylized-ImageNet [15] generates new images by altering
are available at https://github.com/chenshuang- thetexturesofImageNetimages,whichcannotcontrolthe
zhang/imagenet_d. globalfactorslikebackground.
Inthiswork,weintroduceImageNet-D,asynthetictest
set generated by diffusion models for object recognition
1.Introduction task. Capitalizing on the capability of pioneering Stable
Diffusionmodels[48],weshowthatwecansteerdiffusion
Neuralnetworkshaveachievedremarkableperformancein
models with language to create realistic test images that
tasks ranging from image classification [33, 34, 56] to vi-
causevisionmodelsfail. Figure1showsthreefailurecases
sual question answering [10, 28, 31, 66]. These advances
ofCLIPmodelonoursyntheticImageNet-Ddataset. Since
haveinspiredtheapplicationofneuralnetworksinvarious
werelyonlanguagetocreateimages,wecanvarythehigh-
fields,includingsecurityandsafety-criticalsystemssuchas
level factors in the images in contrast to the local corrup-
self-drivingcars[27,32,42],malwaredetection[9,43,61]
tionsandtextureinpriorwork,introducingadditionfactors
and robots [6, 7, 25]. Due to their wide adaptation, it is
thatonecanevaluaterobustnesson.
becomingincreasinglyimportanttoidentifytherobustness
To enhance sample difficulty of our dataset, we selec-
ofneuralnetworks[30,41]forsafetyreasons.
tively retain images that cause failures in multiple chosen
To evaluate the robustness of neural networks, Object-
visionmodels. Ourresultsshowthatimagestriggeringer-
Net[3]collectsreal-worldobjectimagesoncontrolledfac-
rorsinchosenmodelscanreliablytransfertheirchallenging
tors like background with human workers, which is time-
naturetoother,previouslyuntestedmodels. Thisleadstoa
consumingandlabor-intensive. Toscaleupdatacollection,
notabledecreaseinaccuracy, eveninstate-of-the-artfoun-
synthetic images are proposed as test images [15, 19, 59].
dation models like MiniGPT-4 [66] and LLaVa [31], sug-
Forexample,ImageNet-C[19]introducesasetoflow-level
gestingourdatasetrevealscommonfailuresinvisionmod-
els.
*Corresponding author. Junmo Kim <junmo.kim@kaist.ac.kr>,
ChengzhiMao<chengzhi.mao@mila.quebec>. Visualizations demonstrate that Imagenet-D signifi-
4202
raM
72
]VC.sc[
1v57781.3042:viXraImageNet-9 Stylized-ImageNet ImageNet-C benchmarks model robustness on low-level corruptions.
ImageNet-9[59]generatesnewimagesbycombiningfore-
ground and background from different images, however,
limited by poor image fidelity. Stylized-ImageNet [15] al-
Background Texture Material
tersthetexturesofImageNetimagesbyAdaINstyletrans-
fer [26] or introducing texture-shape cue conflict, which
cannotcontrolotherfactorslikebackgrounds. Inthiswork,
weintroduceanewtestsetImageNet-D,whichisgenerated
Figure 2. Examples from ImageNet-9 [59], Stylized-
bycontrollingdiffusionmodelsandincludesnovelimages
ImageNet [15] and ImageNet-C [19] and our ImageNet-D. For
withdiversebackgrounds,textures,andmaterials.
the second row, we show images from ImageNet-D with differ-
Image generation. Diffusion models have achieved great
ent backgrounds, textures and materials orderly. Take the back-
successinvarioustasksincludingimagegeneration[46,49,
groundforexample(thetwocolumnsontheleft),ImageNet-9[59]
generatesnewimagesbysimplycuttingandpasteforegroundand 52,63]. Asamilestonework,Stablediffusion[48]enables
backgroundfromdifferentimages,leadingtoobjectdeformation high-fidelity image synthesis controlled by language. In-
and dislocation. By contrast, ImageNet-D includes images with structPix2Pix [8] provides a more sophisticated control by
diverse backgrounds by diffusion generation, achieving superior editing a given image according to human instructions. In
visualfidelity. this paper, we build our pipeline with the standard Stable
Diffusionmodel,yetouralgorithmiscompatiblewithother
generativemodelsthatcanbesteeredbylanguage.
cantly enhances image quality compared to previous
Enhancingperceptionwithdiffusionimages. Diffusion-
synthetic robustness benchmarks, as evidenced in Fig-
generated images have been used for vision perception
ure 2. Imagenet-D serves as an effective tool for reduc-
tasks. A branch of studies [1, 2, 54, 60] improves classi-
ing the performance and assessing model robustness, in-
ficationaccuracybyusingsyntheticimagesastrainingdata
cludingResNet101(reducing55.02%),ViT-L/16(reducing
augmentation. DREAM-OOD[14]findstheoutliersbyde-
59.40%),CLIP(reducing46.05%),andtransferwelltoun-
coding sampled latent embeddings to images. However,
foreseenlargevisionlanguagemodelslikeLLaVa[31](re-
theirmethodlacksspecificcontroloverimagespace,which
ducing 29.67%), and MiniGPT-4 [66] (reducing 16.81%).
iscrucialforbenchmarkslikeImageNet-D. [40]identifies
Our approach of utilizing generative models to evaluate
under-represented attribute pairs, while our study focuses
model robustness is general, and shows significant poten-
onhardimageswithasingleattribute. Unlike [29,44,57]
tialforevengreatereffectivenesswithfutureadvancements
that modify existing datasets, our work generates new im-
ingenerativemodels.
ages and mines the most challenging ones as the test set,
achievinggreateraccuracydropthan[29,44,57].
2.Relatedwork
3.ImageNet-D
Robustness of neural networks. Neural networks have
evolved from CNN [16, 24], ViT [33, 56], to large foun-
WefirstpresenthowImageNet-DiscreatedinSection3.1,
dation models [5, 12, 55]. Previous work has investigated
followedbyanoverviewofitsstatisticsinSection3.2.
neural networks robustness from multiple aspects, such as
adversarialexamples[35,36,38,64,65]andout-of-domain
3.1.Datasetdesign
samples [17, 20, 22, 37]. Foundation models have shown
greaterrobustnessonout-of-distributionsamples[45]. Ro- While neural networks excel in various applications, their
bustexplanationhasalsobeeninvestigated[31,39,66]. To robustnessneedsrigorousevaluationforsafety. Traditional
systematicallyevaluatetherobustnessofdeepmodels,test evaluations use existing test sets, including either natural
setsthatcoverdifferentfactorsareurgentlyneeded. [3,23]orsyntheticimages[15,19,59]. Comparedtoman-
Dataset for benchmarking robustness. To evaluate neu- ualimagecollection,collectingasynthetictestsetismore
ral network robustness, a branch of studies source images efficient [15, 59]. However, the diversity of current syn-
online, including ImageNet-A [23], Imagenet-R [22] and thetic test sets is limited due to their reliance on existing
ImageNet-Sketch [58]. However, they are limited to im- imagesforattributeextraction. Thesesyntheticimagesare
ages that exist on the web. ObjectNet [3] manually col- alsonotrealistic, asshowninFigure2. ImageNet-Disin-
lectsimageswiththehelpof5982workers,whichistime- troducedtoevaluatemodelrobustnessacrossvariousobject
consumingandresource-intensive. andnuisancecombinations,addressingtheselimitations.
To overcome the limitations of web images and reduce Image generation by diffusion models. To construct
thecostofmanualcollection,syntheticimagesareproposed ImageNet-D,diffusionmodelsareusedtocreateavastim-
for robustness evaluation [15, 19, 59]. ImageNet-C [19] age pool by combining all possible object and nuisances,
suoiverP
D-teNegamI
steS
tseTBackground Texture Material
Swimming Trunks Hair Dryer Can Opener Batch Towel Speaker Strainer Shopping Basket Strainer Bench Soup Bowl Bench Bench
Swimming Trunks Bath Towel Weight Exercise Tennis Racket Soap Dispenser Salt Shaker Strainer Bottle Cap Coffee Press Pitcher Shopping Basket Dishcloth
Binder (Closed) Soap Dispenser Vacuum Cleaner Swimming Trunks Strainer Soap Bowl Measuring Cup Salt Shaker Alarm Clock Alarm Clock Desk Lamp Alarm Clock
Figure3. TheImageNet-Dtestset. Eachgroupofimagesisgeneratedwiththesameobjectandnuisance,suchasbackground,texture,
andmaterial. Foreachgroupofimages,thegroundtruthlabeliscolorgreen,whilethepredictedcategoriesbyCLIP(ViT-L/14)oneach
imageareinblack. Leveragingdiffusionmodelsforimagegeneration,wecancreateatestsetwithdiversecombinationsofobjectsand
nuisances. Forexample,thetopleftcornershowsabenchintheswimmingpoolbackground. Interestingly,CLIP(ViT-L/14)recognizes
thebenchinthisimageasswimmingtrunks.
Image Generationby diffusion models Hard image mining Quality control Challenge new models
Category
Background
B L Ca ae m nn dc ph ls ehade + Texture Prom pt D Miff ou ds eio lsn Su Mr oro dg ea lste ImHa ar gd es H Cu hm eca kn TeF si tn a Sl e t MN oe dw els
Hammer Material
· · ·
Figure4. ImageNet-Dcreationframework. ImageNet-Discreatedbyfirstcombiningvariousobjectcategoriesandnuisances,including
background, texture, andmaterial. Tomakethetestsetchallenging, weonlykeepthehardimagesfromthelargepoolthatcommonly
makemultiplesurrogatemodelsfailtopredictthecorrectobjectlabel.Thetestsetisthenrefinedthroughhumanverificationtoensurethe
imagesarevalid,single-class,andhigh-quality,makingImageNet-Dsuitableforevaluatingtherobustnessofdifferentneuralnetworks.
enabling high-fidelity image generation based on user- racyonallthetestsets,withanaccuracyofaround94%on
definedtextinputs. WeuseStableDiffusionmodel[48]for syntheticimagepool.Tocreateachallengingtestsetforro-
image generation, while our approach is compatible with bustnessevaluation,weproposeanefficientstrategytofind
other generative models that can be steered by language. thehardtestsamplesfromallgeneratedimagesasfollows.
Theimagegenerationprocessisformulatedasfollows: Hard image mining with shared perception failures.
Beforeintroducinghowtoidentifyhardsamplesfromsyn-
Image(C,N)=StableDiffusion(Prompt(C,N)), (1)
theticimagepool,wefirstdefinetheconceptofsharedper-
ceptionfailureasfollows.
where C and N refer to the object category and nuisance,
Shared failure: Animageisasharedfailureifit
respectively. The nuisance N includes background, mate-
leadsmultiplemodelstopredictobject’slabelincorrectly.
rial,andtextureinthiswork. Table1presentsanoverview
An ideal hard test set should include images that all
of nuisances and prompts to diffusion models. Using the
tested models fail to identify. However, this is impracti-
backpackcategoryasanexample,wefirstgenerateimages
cal due to the inaccessibility of future test models, termed
ofbackpackswithvariousbackgrounds,materials,andtex-
targetmodel. Instead,weconstructthetestsetusingshared
tures(e.g.,abackpackinawheatfield),offeringabroader
failuresofknownsurrogatemodels. Ifthefailuresofsurro-
rangeofcombinationsthanexistingtestsets. Eachimageis
gate models lead to low accuracy in unknown models, the
labeledwithitspromptcategoryC asgroundtruthforclas-
test set is deemed challenging. This is defined as transfer-
sification. Animageisviewedmisclassifiedifthemodel’s
ablefailureasfollows:
predictedlabeldoesnotmatchthegroundtruthC.
Aftercreatingalargeimagepoolwithallobjectcategory Transferable failure: Shared failures of
andnuisancepairs,weevaluateCLIP(ViT-L/14)modelon knownsurrogatemodelsaretransferableiftheyalsoresult
theseimagesinTable2. Experimentaldetailsarereported inlowaccuracyforunknowntargetmodels.
inSection4.1.Table2showsthatCLIPachieveshighaccu- To test if shared failures of surrogate models are trans-
hcneB
wolliP
sevolG
yarT
elttoB
lliP
eldaL
riahC
elttoB
retaW
elacS
thgieWTable1. Overviewofnuisancesandinputpromptstodiffusionmodels. DuringImageNet-Dconstruction,468backgrounds,47textures,
and32materialsfromtheBrodendataset[4]areusedasnuisances. Imagesaregeneratedbypairingeachobjectwithallnuisancesin
diffusionmodelprompts.ThisapproachallowsforefficientscalingofImageNet-Dwithadditionalcategoriesandnuisances.
Nuisance Nuisancenumber Prompttodiffusionmodels Promptexample
Background 468 A[category]inthe[background] Abackpackinthewheat field
Texture 47 A[texture][category] Aknittedbackpack
Material 32 A[category]madeof[material] Abackpackmadeofleather
Figure5.Testaccuracyoftargetmodelsonsharedfailuresofsurrogatemodels.Weadoptknownsurrogatemodelstoidentifytheirshared
failureimagesasthetestset,thenevaluateanewtargetmodelonthistestset. Weshowthatthesharedfailuresofsurrogatemodelscan
alsodeceiveanewtargetmodel,leadingtolowtestaccuracy. Increasingthenumberofsurrogatemodelslowersthetargetmodels’test
accuracy,suggestingamoredifficulttestset.
Table2.TestaccuracyofCLIP(ViT-L/14)onthesyntheticimage we use Amazon Mechanical Turk [11, 22, 47] to evaluate
poolbyexhaustingalltheobjectcategoryandnuisancecombina- labeling quality. We ask the workers to select the images
tions.WeshowthatCLIPachieveshighaccuracyonthesynthetic
that they can either recognize the main object or the main
imagepool.Tocreateachallengingtestsetforrobustnessevalua-
object can be used functionally as the ground truth cate-
tion,wefurtherminethehardsamplesasthefinaltestset.
gory. Moreover,wedesignsentinelstoensurehigh-quality
responses, including positive, negative and consistent sen-
TestSet ImageNet ObjectNet Syntheticimagepool
tinels.Wereportdetailsofthelabelingtaskintheappendix.
Background Texture Material
Atotalof679qualifiedworkersparticipatedin1540label-
Acc(%) 74.64 66.91 95.79 94.02 93.75
ingtasks,achievinganagreementof91.09%. Figure3dis-
playsimagesfromImageNet-D,demonstratinghighfidelity
and diversity in object and nuisance pairs. We summarize
ferablefordiffusion-generatedimages,weevaluatetestsets
theframeworkofcreatingImageNet-DinFigure4.
created with shared failures from 1 to 8 surrogate mod-
els in Figure 5. We evaluate the accuracy of three target 3.2.Datasetstatistics
models that are not used during test set construction, in-
ImageNet-D includes 113 overlapping categories between
cludingCLIP(ViT-B/16),LLaVa,andMiniGPT-4. Figure5
ImageNet and ObjectNet, and 547 nuisances candidates
showsthattargetmodelaccuracydecreasesasmoresurro-
fromtheBrodendataset[4](seeTable1),resultingin4835
gatemodelsareused. Thetestsetsarecreatedwithdiverse
hard images featuring diverse backgrounds (3,764), tex-
backgrounds, while experiments for texture and material
tures (498), and materials (573). Our pipeline to create
show the same trend. This trend demonstrates that failure
ImageNet-Disgeneralandefficient,allowingeasyaddition
images of multiple surrogate models can form a challeng-
of new categories and nuisances. ImageNet-D’s category
ing test set for unseen new models. Notably, the accuracy
distributionexhibitsanaturallong-tailpattern,asshownin
decrease slows when the number of surrogate models ex-
Figure 6. The sparse and non-uniform category-attribute
ceedsfour.
distribution in Figure 7 shows the necessity of exhausting
Qualitycontrolbyhuman-in-the-loop.Theabovepro-
allcategoryandnuisancepairsintestsetcreation.
cess allows us to automatically find a challenging test set
to unseen models. However, generative models can pro-
4.Experiments
duce incorrect images not matching the prompt category.
We resort to human annotation to ensure the ImageNet- We evaluate various models on ImageNet-D benchmark.
D images are simultaneously valid, single-class, and high- We find that ImageNet-D significantly decreases all mod-
quality. After first-round annotation by graduate students, els’ accuracy by up to 60%. We then show whether prior(b)Texture
(a)Background (c)Material
Figure6.Histogramoftheimagefrequencypercategoryinourtestset,followinganaturallong-taildistribution.
(b)Texture (c)Material
(a)Background
Figure7. Frequencyofobjectcategoryandnuisancepairs. Eachdotindicatesapair, whilethex-axisandy-axisindicatethenuisance
andcategory,respectively. AsshowninTable1,weadopt468backgrounds,47textures,and32materialsfromtheBrodendataset[4],
leadingtodifferentwidthsofFigure(a)(b)(c).Adarkercolorindicatesmoresamples.Thesparseandnon-uniformdistributionshowsthe
necessityofexhaustingallcategoryandnuisancepairsinImageNet-Dcreation.
advancementscouldimproveImageNet-Drobustness,such A common prompt that asks VQA models to rec-
asdataaugmentation. Lastly,wediscussImageNet-Dfrom ognize the object is: What is the main object
variousaspects,suchasnearestneighborretrieval. in this image? To make VQA models choose
from pre-defined category list, we ask VQA models as
4.1.Experimentalsetups follows: What is the main object in this
image? Choose from the following list:
Testsetconstructionsetups. WeuseStableDiffusion[48]
[GT category], [failure category]. GT cate-
to create ImageNet-D, and adopt the pretrained weight of
gory refers to the image’s ground truth category. As for
version stable-diffusion-2-1 from Hugging Face. To find
the failure category, we adopt the category that achieves
the hard images, we finalize ImageNet-D with shared fail-
the highest CLIP (ViT-L/14) confidence among all wrong
ures of 4 surrogate models, including CLIP [45] (ViT-
categories. Withthisprompt,wefindthatbothMiniGPT-4
L/14, ViT-L/14-336px and ResNet50), and vision model
andLLaVacanchoosefromprovidedcategorylistintheir
(ResNet50[16]). Thecandidatesetofsurrogatemodelsin
output. If the model chooses ground truth category, this
Figure5alsoincludesCLIP(Resnet101,ViT-B/32)andvi-
imageisviewedtobecorrectlyrecognized. Therefore,we
sionmodel(ViT-L/16[13]andVGG16[53]).
cancomputetheaccuracyofVQAmodels.
Evaluation of classification models. Robustness on
ImageNet-Dismeasuredbytop-1accuracyinobjectrecog-
4.2.Robustnessevaluation
nition, the ratio of correctly classified images to total im-
ages. We evaluate classification models with the open- Quantitativeresults.WeevaluateImageNet-Don25mod-
source pretrained weights. For CLIP [45], we follow the els,andplottestaccuracytrendinFigure8. Thehorizontal
originalpaper[45]toadoptAphotoofa[category]asthe axisandverticalaxisindicatethetestaccuracyonImageNet
texttemplate. Thezero-shotaccuracyofCLIPisreported. and ImageNet-D, respectively. Figure 8 shows that as Im-
Evaluationofvisualquestionanswering(VQA)mod- ageNetaccuracyincreases,ImageNet-Daccuracyalsogets
els. We evaluate the accuracy of the state-of-the-art open- higher.ImageNet-DaccuracyismuchlowerthanImageNet
sourceVQAmodelsonImageNet-D,includingLLaVa[31], accuracyforallmodels,indicatedbythelowerdistribution
and MiniGPT-4 [66]. Given an input image, VQA models below the y = x reference line. We report the accuracy
output answers based on input text prompt. However, the of14modelsondifferenttestsetsinTable3,andallmod-
textual output of VQA models is not limited to a certain els’ accuracy in appendix. Table 3 shows that ImageNet-
template, thus may not include the category name in pre- D achieves the lowest test accuracy for all models, except
definedcategorylistofobjectrecognitiontasks.Thismakes for the comparable result on Stylized-ImageNet for VQA
ithardtoassesstheaccuracybasedondiverseanswers. models. Note that ImageNet-D achieves higher image fi-Figure8. ModelaccuracyonImageNetvs. ImageNet-D.Eachdatapointcorrespondstoonetestedmodel. Theplotsrevealthatthereisa
significantaccuracydropfromImageNettoournewtestsetImageNet-D.Asthemodel’saccuracyonImageNetincreases,theaccuracyon
ImageNet-Disalsohigher. TheseresultsshowtheeffectivenessofImageNet-Dtoevaluatetherobustnessofneuralnetworks. Wereport
theexactnumberof14modelsofthisfigureinTable3,andtheresultsforallmodelscanbefoundintheappendix.
Table3. Testaccuracy ofvision modelsand largefoundation models(%). Weshow thetestaccuracy forthe visionmodels andlarge
foundationmodels(rows)ondifferenttestsets(columns). ThenumbersingreenrefertotheaccuracydropofImageNet-Dcomparedto
ImageNet.ForMiniGPT-4andLLaVa,ImageNet-Dreducestheaccuracyby16.81%and29.67%comparedtotheImageNet,respectively.
OurresultsshowthatImageNet-Diseffectivetoevaluatetherobustnessofneuralnetworks.
Model Architecture ImageNet ObjectNet ImageNet-9 Stylized ImageNet-D ImageNet-D
Background Texture Material Total
Visionmodel(CNN) VGG19 62.77 27.19 74.84 16.25 9.8(-52.97) 11.45(-51.32) 12.39(-50.38) 10.28(-52.49)
ResNet101 67.66 32.34 81.85 22.66 12.38(-55.28) 13.65(-54.01) 13.44(-54.22) 12.64(-55.02)
Densenet161 66.99 31.86 84.91 22.5 11.34(-55.65) 14.06(-52.93) 13.26(-53.73) 11.85(-55.14)
Wideresnet101 69.2 34.37 82.17 21.48 10.55(-58.65) 13.05(-56.15) 12.04(-57.16) 10.98(-58.22)
Visionmodel(ViT) ViT-B/32 65.02 27.59 77.51 42.34 6.64(-58.38) 12.25(-52.77) 13.79(-51.23) 8.07(-56.95)
ViT-B/16 72.14 34.79 82.49 31.02 10.49(-61.65) 16.87(-55.27) 17.63(-54.51) 12.0(-60.14)
ViT-L/16 68.67 32.7 78.91 29.38 7.68(-60.99) 14.06(-54.61) 15.53(-53.14) 9.27(-59.40)
CLIP RN101 62.48 42.89 83.09 22.58 21.47(-41.01) 21.29(-41.19) 25.83(-36.65) 21.96(-40.52)
ViT-B/32 64.06 43.67 79.56 44.22 18.73(-45.33) 33.33(-30.73) 30.37(-33.69) 21.61(-42.45)
ViT-B/16 67.95 54.87 85.16 40.62 20.64(-47.31) 22.89(-45.06) 29.32(-38.63) 21.9(-46.05)
MiniGPT-4 Vicuna13B 88.77 77.57 89.46 69.88 71.81(-16.96) 72.48(-16.29) 72.5(-16.27) 71.96(-16.81)
LLaVa Vicuna13B 79.32 76.02 90.84 61.94 52.89(-26.43) 40.53(-38.79) 36.28(-43.04) 49.65(-29.67)
LLaVa-1.5 Vicuna13B 89.08 78.66 93.88 64.14 73.31(-15.77) 67.27(-21.81) 67.08(-22.00) 71.95(-17.13)
LLaVa-NeXT Hermes-Yi-34B 85.83 77.54 90.52 57.98 68.77(-17.06) 46.67(-39.16) 54.11(-31.72) 64.76(-21.07)
delity than Stylized-ImageNet as shown in Figure 2. Al- MiniGPT-4 and LLaVa-1.5 can also fail to recognize the
though ObjectNet changes multiple attributes for each im- mainobjectfromImageNet-D.
age,itstillresultsinhigheraccuracythanImageNet-Dthat
4.3.Robustnessimprovement
specifies only one attribute per image. Compared to Ima-
geNet,ImageNet-Dyieldsatestaccuracydropofmorethan Data augmentation. Prior studies reveal that data aug-
16% for all models, including LLaVa (reducing 29.67%) mentation is effective to improve model robustness, such
and MiniGPT-4 (reducing 16.81%). Our ImageNet-D can as on ImageNet-C [19]. We evaluate data augmentation
also cause significant accuracy drop of the latest LLaVa- methodsonImageNet-D,includingSIN[15],AugMix[21],
1.5andLLaVa-NeXT.AlthoughLLaVa-NeXToutperforms ANT [50] and DeepAugment [22]. Table 4 shows the re-
LLaVa-1.5 on benchmarks like MMMU [62], it achieves sults with ResNet50 backbone for all methods. We report
loweraccuracyonImageNet-D,demonstratingtheunique- testaccuracyforImageNetandImageNet-D,meanCorrup-
nessofImageNet-D.Forvisionmodels, theaccuracydrop tion Error (mCE) for ImageNet-C following [19, 21, 22,
is even close to 50% to 60%. The results in Figure 8 and 50]. AlthoughthesemethodsimproveImageNet-Crobust-
Table3showtheeffectivenessofImageNet-Dinevaluating ness(lowermCE)thanbaselinemodel,Table4showsthat
therobustnessofneuralnetworks. their ImageNet-D robustness is comparable or even worse
Visualization results. Figure 3 displays image exam- than baseline. This suggests that current benchmarks like
ples from ImageNet-D, demonstrating high quality. Al- ImageNet-C do not accurately represent real robustness of
thoughhumanscaneasilyrecognizethemainobject,CLIP neuralnetworksinoursetting, makingImageNet-Danec-
(ViT-L/14)mistakenlyclassifiestheseimagesintoawrong essarybenchmarkforrobustnessevaluation.
category. Figure 9 shows conversations with MiniGPT- Model architecture. We compare ImageNet-D robust-
4 and LLaVa-1.5 on ImageNet-D images, indicating that ness of different model architectures in Figure 11. WhenGround Truth Category Failure Category Human MiniGPT-4 LLaVa-1.5
Spatula What is the main object in this image? Choose from the following list: spatula, tennis racket.
The main object in this image is a tennis racket. Tennis racket.
(a) Why do you think the main object in this image is a tennis racket?
Because the image shows a tennis court, and the object in the The main object in this image is a tennis racket because it is the
center of the court appears to be a tennis racket. The racket is the central focus of the scene, and it is the only object that is clearly
main tool used in the sport of tennis for hitting the ball over the net visible and described in detail. The tennis racket is described as
and into the opponent’s court. The presence of the racket in the being white and blue, and it is positioned in the foreground,
Tennis Racket center of the court suggests that it is the main object in the image. with the tennis court in the background.
Band aid What is the main object in this image? Choose from the following list: band aid, tray.
The main object in this image is the tray. Tray.
(b) Why do you think the main object in this image is a tray?
Because the image shows four wooden combs arranged in a row, The main object in this image is a tray because it is described as a close-
and a tray is a flat, usually rectangular, container that is often used up of a tray with three combs on it. The presence of the combs on the
for serving food or holding objects. The combs appear to be placed tray suggests that it is a functional object used for organizing and
Tray on the tray, making it the most likely main object in the image. storing the combs. The tray is placed on a table, which further supports
the idea that it is the main object in the image.
Weight Scale What is the main object in this image? Choose from the following list: weight scale, desk lamp.
The main object in the image is the desk lamp. Desk lamp.
(c) Why do you think the main object in this image is a desk lamp?
The main object in the image is a desk lamp because it is clearly The main object in the image is a desk lamp because it is
visible in the foreground of the image, with the other objects in the prominently featured in the foreground, with a close-up view of
background. Additionally, the lamp has a sleek, modern design and the lamp and its base. The lamp is turned on, which further
appears to be made of metal, which suggests that it is a desk lamp emphasizes its presence and function. Additionally, the lamp is
Desk Lamp rather than a weight scale or a plant. placed on a table, which is a common location for desk lamps.
Figure9. FailurecasesofMiniGPT-4andLLaVa-1.5fromImageNet-D.WeaskMiniGPT-4andLLaVa-1.5forthecategoryoftheinput
imageandthereasonfortheirpredictions. Figure(a)to(c)areimageswithdifferentbackground,textureandmaterial,respectively. Our
resultsshowthatimagesfromImageNet-Dcanalsomakethestate-of-the-artfoundationmodelsfail.
Table 4. Robustness of different augmentation methods. De- theBackgroundset. Theseresultsshowthatpretrainingon
spite superior ImageNet-C robustness, these methods fail to im- alargedatasethelpsimproverobustnessonImageNet-D.
prove ImageNet-D robustness, demonstrating the uniqueness of
ImageNet-DfromImageNet-C. 4.4.Furtherdiscussions
CanCLIPfindthecorrectneighborsofImageNet-Dim-
Model ImageNet(↑) ImageNet-C(mCE)(↓) ImageNet-D(↑)
ages? CLIP model [45] shows potential in nearest neigh-
Baseline 65.82 81.74 10.22
SIN 63.42(-2.40) 78.45(-3.29) 8.81(-1.41) bor search tasks. With ImageNet-D images as query, we
Augmix 66.88(+1.06) 74.7(-7.04) 8.75(-1.47)
retrievethemostsimilarimagesfromImageNettoinvesti-
ANT 65.95(+0.13) 76.74(-5.00) 10.09(-0.13)
DeepAugment 66.54(+0.72) 70.31(-11.43) 9.37(-0.85) gatewhetherCLIPcanfindcorrectneighbors,asshownin
Figure10. TakeBackgroundforexample,retrievedimages
may either have a similar background to the query image
we change the model from ViT to Swin Transformer [33] (Figure 10(a)) or include the object that is related to the
andConvNeXt[34],thetestaccuracyonbothImageNet-D query image’s background(Figure 10(b)).Our results show
(Background)andImageNetimprove.However,therobust- that ImageNet-D can find the failure cases of neural net-
nessonTextureandMaterialtestsetevendecreasesslightly. worksinnearestneighborretrieval.
TheseresultsshowthedifficultyofimprovingImageNet-D Can ImageNet-D match natural test sets in failure
robustnessbymodelarchitecture. transferability? Section 3.1 defines transferable
Pretrainingwithmoredata.Pretrainingonalargedata failure and finalize ImageNet-D with shared failures
setiseffectivetoimprovesmodelperformance,suchasIm- of surrogate models. We conduct the same experiment on
ageNetaccuracy [18]. Figure11comparesConvNext,that ImageNet, introducing ImageNet (Failure) with the shared
is trained directly on ImageNet-1K, with ConvNext (Pre- failure images of surrogate models. Table 5 show that
trained)whichisfirstpretrainedonImageNet-22K.Wefind ImageNet-D achieves similar accuracy to ImageNet (Fail-
thatConvNext(Pretrained)achieveshigherrobustnessthan ure),indicatingthatthatsyntheticimagescanachievesim-
ConvNext on all three sets of ImageNet-D, especially for ilar failure transferability as natural images. In contrast to
muisanmyG
devoorG
rorriMQuery Nearest Neighbors Retrieved Query Nearest Neighbors Retrieved
Abath AT-shirt
towel in the inthe
(a) (b)
forest operating
needleleaf room
A
lampshade Ahammer
(c) inthe inthe (d)
skating rink courtroom
Figure10. Visualizationsofnearestneighborimages. WevisualizethenearestneighborimagesfromImageNetwithImageNet-Dimage
as the query image. Instead of following the same object category as the query image, the nearest neighbor images either follow a
similarbackgroundorfollowanotherobjectcategorythatishighlycorrelatedwiththebackgroundofqueryimage. Ourresultsshowthat
ImageNet-Dcanfindthefailurecasesofneuralnetworksinnearestneighborretrieval.
Figure11. Testaccuracyofdifferentarchitecturesandtrainingdata. Eachdatapointcorrespondstoonetestedmodel. Apartfromthe
ConvNeXt(Pretrained),allothermodelsaretrainedonImageNet-1K.Theplotsshowthatpretrainingachieveshigherrobustnessonall
threesubsets,whilechangingmodelarchitecturesonlyimprovesonImageNet-D(Background).
Table 5. Results of failure transferability. We create ImageNet enttrainingsetsinTable6. Table6showsthattrainingon
(Failure) with shared failures of surrogate models from original Synthetic-easy significantly improves ImageNet-D robust-
ImageNet,achievingcomparableaccuracytoImageNet-D.These
nessby19.26%. Remarkably,modelCoutperformsmodel
resultsshowthatthatoursyntheticimagesachievesimilarability
B in ObjectNet accuracy by 1.34%, indicating model C’s
tonaturalimagesinfindingthefailuresofnewmodels.
superiorgeneralization. Theseresultsimplythatdiffusion-
generated images with diverse object and nuisance pairs
Model Architecture ImageNet ImageNet(Failure) ImageNet-D
couldenhancemodelrobustnessastrainingsamples.
CLIP ViT-B/16 67.95 11.09 21.9
LLaVa Vicuna13B 79.32 41.43 49.65
MiniGPT-4 Vicuna13B 88.77 65.22 71.96
5.Conclusion
Table6. Testaccuracyofmodelsfinetunedonsyntheticdata. We In this paper, we introduce a test set ImageNet-D and es-
finetune a pretrained ResNet18 model on ImageNet-1K together tablisharigourousbenchmarkforvisualperceptionrobust-
with different extra training data. Training on synthetic images ness. Capitalizingtheimagegenerationabilityofdiffusion
achieveshighestrobustnessonbothImageNet-DandObjectNet. models, ImageNet-D includes images with diverse factors
including background, texture and material. Experimen-
Model Extratrainingdata ImageNet ObjectNet ImageNet-D tal results show that ImageNet-D significantly decreases
A / 55.68 21.51 8.6
the accuracy of various models, including CLIP (reducing
B ImageNet 59.78(+4.10) 24.78(+3.27) 10.4(+1.80)
C Synthetic-easy 56.56(+0.88) 26.12(+4.61) 27.86(+19.26) 46.05%), LLaVa [31] (reducing 29.67%), and MiniGPT-
4 [66] (reducing 16.81%), demonstrating the effectiveness
inmodelevaluation. Ourworkmakesastepforwardinim-
naturaldatasetslikeImageNet,ImageNet-Denjoysalower provingsynthetictestsets,andwillcreatemorediverseand
costindatacollectionandcanbescaledefficiently. challengingtestimagesasgenerativemodelsimprove.
Training on diffusion-generated images. By contrast Acknowledgments: This work was supported by Insti-
to shared failure images in ImageNet-D, we term gener- tute of Information & communications Technology Plan-
ated images correctly classified by surrogate models as ning & Evaluation (IITP) grant funded by the Korea
Synthetic-easy, and investigate their influence as training government (MSIT) (No.2022-0-00951, Development of
data. Wefinetuneapre-trainedResNet18modelondiffer- Uncertainty-AwareAgentsLearningbyAskingQuestions).References [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
[1] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
hammad Norouzi, and David J. Fleet. Synthetic data from
vain Gelly, et al. An image is worth 16x16 words: Trans-
diffusionmodelsimprovesimagenetclassification.Transac-
formers for image recognition at scale. arxiv 2020. arXiv
tionsonMachineLearningResearch,2023. 2
preprintarXiv:2010.11929,2010. 5
[2] HritikBansalandAdityaGrover. Leavingrealitytoimagi- [14] XuefengDu,YiyouSun,JerryZhu,andYixuanLi. Dream
nation: Robustclassificationviagenerateddatasets. InDe- the impossible: Outlier imagination with diffusion models.
ployableGenerativeAIWorkshopatICML2023,2023. 2 Advances in Neural Information Processing Systems, 36,
[3] Andrei Barbu, David Mayo, Julian Alverio, William Luo, 2023. 2
Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and [15] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,
BorisKatz. Objectnet: Alarge-scalebias-controlleddataset MatthiasBethge,FelixAWichmann,andWielandBrendel.
for pushing the limits of object recognition models. In Imagenet-trainedcnnsarebiasedtowardstexture;increasing
AdvancesinNeuralInformationProcessingSystems,pages shapebiasimprovesaccuracyandrobustness.arXivpreprint
9448–9458,2019. 1,2 arXiv:1811.12231,2018. 1,2,6
[4] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and [16] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
Antonio Torralba. Network dissection: Quantifying inter- Deep residual learning for image recognition. In Proceed-
pretabilityofdeepvisualrepresentations. InProceedingsof ingsoftheIEEEconferenceoncomputervisionandpattern
theIEEEconferenceoncomputervisionandpatternrecog- recognition,pages770–778,2016. 2,5
nition,pages6541–6549,2017. 4,5 [17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
[5] RishiBommasani,DrewAHudson,EhsanAdeli,RussAlt- Dolla´r,andRossGirshick.Maskedautoencodersarescalable
man,SimranArora,SydneyvonArx,MichaelSBernstein, visionlearners. InProceedingsoftheIEEE/CVFconference
Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. on computer vision and pattern recognition, pages 16000–
Ontheopportunitiesandrisksoffoundationmodels. arXiv 16009,2022. 2
preprintarXiv:2108.07258,2021. 2 [18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
[6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
Dolla´r,andRossGirshick.Maskedautoencodersarescalable
Chebotar,JosephDabis,ChelseaFinn,KeerthanaGopalakr- visionlearners. InProceedingsoftheIEEE/CVFconference
ishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. on computer vision and pattern recognition, pages 16000–
Rt-1: Robotics transformer for real-world control at scale. 16009,2022. 7
arXivpreprintarXiv:2212.06817,2022. 1 [19] DanHendrycksandThomasDietterich. Benchmarkingneu-
ralnetworkrobustnesstocommoncorruptionsandperturba-
[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
tions. arXivpreprintarXiv:1903.12261,2019. 1,2,6
Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,
[20] DanHendrycks, NormanMu, EkinDCubuk, BarretZoph,
Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:
Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A
Vision-language-action models transfer web knowledge to
simple data processing method to improve robustness and
roboticcontrol. arXivpreprintarXiv:2307.15818,2023. 1
uncertainty. arXivpreprintarXiv:1912.02781,2019. 2
[8] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-
[21] DanHendrycks, NormanMu, EkinDCubuk, BarretZoph,
structpix2pix:Learningtofollowimageeditinginstructions.
Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A
In Proceedings of the IEEE/CVF Conference on Computer
simple data processing method to improve robustness and
VisionandPatternRecognition,pages18392–18402,2023.
uncertainty. arXivpreprintarXiv:1912.02781,2019. 6
2
[22] DanHendrycks, StevenBasart, NormanMu, SauravKada-
[9] LiChen,CarterYagemann,andEvanDowning. Tobelieve
vath,FrankWang,EvanDorundo,RahulDesai,TylerZhu,
ornottobelieve:Validatingexplanationfidelityfordynamic
SamyakParajuli,MikeGuo,etal.Themanyfacesofrobust-
malwareanalysis. InCVPRWorkshops,pages48–52,2019.
ness:Acriticalanalysisofout-of-distributiongeneralization.
1
In Proceedings of the IEEE/CVF International Conference
[10] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat onComputerVision,pages8340–8349,2021. 2,4,6,1
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale [23] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
Fung, and Steven Hoi. Instructblip: Towards general- hardt, and Dawn Song. Natural adversarial examples. In
purpose vision-language models with instruction tuning, ProceedingsoftheIEEE/CVFConferenceonComputerVi-
2023. 1 sionandPatternRecognition,pages15262–15271,2021. 2
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, [24] GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKil-
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage ian Q Weinberger. Densely connected convolutional net-
database. In2009IEEEconferenceoncomputervisionand works. InProceedingsoftheIEEEconferenceoncomputer
patternrecognition,pages248–255.Ieee,2009. 4,1 visionandpatternrecognition,pages4700–4708,2017. 2
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina [25] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li,
Toutanova. Bert: Pre-training of deep bidirectional JiajunWu,andLiFei-Fei. Voxposer: Composable3dvalue
transformers for language understanding. arXiv preprint mapsforroboticmanipulationwithlanguagemodels. arXiv
arXiv:1810.04805,2018. 2 preprintarXiv:2307.05973,2023. 1[26] XunHuangandSergeBelongie. Arbitrarystyletransferin puter Vision and Pattern Recognition, pages 2722–2732,
real-timewithadaptiveinstancenormalization. InProceed- 2023. 2
ings of the IEEE international conference on computer vi- [40] Jan Hendrik Metzen, Robin Hutmacher, N Grace Hua, Va-
sion,pages1501–1510,2017. 2 lentynBoreiko,andDanZhang. Identificationofsystematic
[27] Markus Ka¨ngsepp and Meelis Kull. Calibrated perception errorsofimageclassifiersonraresubgroups. ICCV,2023. 2
uncertainty across objects and regions in bird’s-eye-view. [41] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li,
arXivpreprintarXiv:2211.04340,2022. 1 and Yixuan Li. Delving into out-of-distribution detection
[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. with vision-language representations. Advances in Neural
Blip-2: Bootstrapping language-image pre-training with InformationProcessingSystems,35:35087–35102,2022. 1
frozen image encoders and large language models. arXiv [42] Tommaso Nesti, Santhosh Boddana, and Burhaneddin Ya-
preprintarXiv:2301.12597,2023. 1 man. Ultra-sonic sensor based object detection for au-
[29] Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong tonomousvehicles. InProceedingsoftheIEEE/CVFCon-
Zhang, and Hui Xue. Imagenet-e: Benchmarking neural ferenceonComputerVisionandPatternRecognition,pages
networkrobustnessviaattributeediting. InProceedingsof 210–218,2023. 1
theIEEE/CVFConferenceonComputerVisionandPattern [43] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana.
Recognition,pages20371–20381,2023. 2 Deepxplore: Automated whitebox testing of deep learning
[30] Xuanlin Li, Yunhao Fang, Minghua Liu, Zhan Ling, systems. Inproceedingsofthe26thSymposiumonOperat-
ZhuowenTu,andHaoSu. Distillinglargevision-language ingSystemsPrinciples,pages1–18,2017. 1
model with out-of-distribution generalizability. arXiv [44] VirajPrabhu,SriramYenamandra,PrithvijitChattopadhyay,
preprintarXiv:2307.03135,2023. 1 and Judy Hoffman. Lance: Stress-testing visual models
[31] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. by generating language-guided counterfactual images. Ad-
Visualinstructiontuning. arXivpreprintarXiv:2304.08485, vancesinNeuralInformationProcessingSystems,36,2023.
2023. 1,2,5,8 2
[32] Yicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, and [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Hang Zhao. Vectormapnet: End-to-end vectorized hd map Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
learning.InInternationalConferenceonMachineLearning, AmandaAskell,PamelaMishkin,JackClark,etal.Learning
pages22352–22369.PMLR,2023. 1 transferable visual models from natural language supervi-
[33] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng sion.InInternationalconferenceonmachinelearning,pages
Zhang, Stephen Lin, and Baining Guo. Swin transformer: 8748–8763.PMLR,2021. 2,5,7
Hierarchical vision transformer using shifted windows. In [46] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
Proceedings of the IEEE/CVF international conference on andMarkChen. Hierarchicaltext-conditionalimagegener-
computervision,pages10012–10022,2021. 1,2,7 ationwithcliplatents. arXivpreprintarXiv:2204.06125,1
[34] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht- (2):3,2022. 2
enhofer,TrevorDarrell,andSainingXie. Aconvnetforthe [47] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
2020s. InProceedingsoftheIEEE/CVFconferenceoncom- VaishaalShankar. Doimagenetclassifiersgeneralizetoim-
puter vision and pattern recognition, pages 11976–11986, agenet? In International conference on machine learning,
2022. 1,7 pages5389–5400.PMLR,2019. 4,1
[35] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn- Patrick Esser, and Bjo¨rn Ommer. High-resolution image
ing models resistant to adversarial attacks. arXiv preprint synthesis with latent diffusion models. In Proceedings of
arXiv:1706.06083,2017. 2 the IEEE/CVF conference on computer vision and pattern
[36] Kaleel Mahmood, Rigel Mahmood, and Marten Van Dijk. recognition,pages10684–10695,2022. 1,2,3,5
On the robustness of vision transformers to adversarial ex- [49] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
amples.InProceedingsoftheIEEE/CVFInternationalCon- MichaelRubinstein,andKfirAberman. Dreambooth: Fine
ferenceonComputerVision,pages7838–7847,2021. 2 tuning text-to-image diffusion models for subject-driven
[37] ChengzhiMao,LuJiang,MostafaDehghani,CarlVondrick, generation. In Proceedings of the IEEE/CVF Conference
RahulSukthankar,andIrfanEssa. Discreterepresentations onComputerVisionandPatternRecognition,pages22500–
strengthen vision transformer robustness. arXiv preprint 22510,2023. 2
arXiv:2111.10493,2021. 2 [50] Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Ju-
[38] Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, lian Bitterwolf, Oliver Bringmann, Matthias Bethge, and
and Carl Vondrick. Understanding zero-shot adversar- WielandBrendel.Asimplewaytomakeneuralnetworksro-
ial robustness for large-scale models. arXiv preprint bustagainstdiverseimagecorruptions.InComputerVision–
arXiv:2212.07016,2022. 2 ECCV2020: 16thEuropeanConference,Glasgow,UK,Au-
[39] Chengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit gust 23–28, 2020, Proceedings, Part III 16, pages 53–69.
Menon,JunfengYang,XinWang,andCarlVondrick. Dou- Springer,2020. 6
blyrightobjectrecognition: Awhypromptforvisualratio- [51] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,San-
nales.InProceedingsoftheIEEE/CVFConferenceonCom- jeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,Aditya Khosla, Michael Bernstein, et al. Imagenet large conferenceonmachinelearning,pages7472–7482.PMLR,
scalevisualrecognitionchallenge. Internationaljournalof 2019. 2
computervision,115:211–252,2015. 1 [65] YunqingZhao,TianyuPang,ChaoDu,XiaoYang,Chongx-
[52] Chitwan Saharia, William Chan, Saurabh Saxena, Lala uanLi,Ngai-ManCheung,andMinLin. Onevaluatingad-
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, versarialrobustnessoflargevision-languagemodels. arXiv
RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans, preprintarXiv:2305.16934,2023. 2
etal.Photorealistictext-to-imagediffusionmodelswithdeep [66] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
language understanding. Advances in Neural Information hamed Elhoseiny. Minigpt-4: Enhancing vision-language
ProcessingSystems,35:36479–36494,2022. 2 understandingwithadvancedlargelanguagemodels. arXiv
[53] KarenSimonyanandAndrewZisserman. Verydeepconvo- preprintarXiv:2304.10592,2023. 1,2,5,8
lutional networks for large-scale image recognition. arXiv
preprintarXiv:1409.1556,2014. 5
[54] YonglongTian,LijieFan,PhillipIsola,HuiwenChang,and
Dilip Krishnan. Stablerep: Synthetic images from text-to-
image models make strong visual representation learners.
Advances in Neural Information Processing Systems, 36,
2023. 2
[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste
Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXivpreprintarXiv:2302.13971,2023. 2
[56] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin. Attentionisallyouneed. Advancesinneural
informationprocessingsystems,30,2017. 1,2
[57] Joshua Vendrow, Saachi Jain, Logan Engstrom, and Alek-
sanderMadry.Datasetinterfaces:Diagnosingmodelfailures
usingcontrollablecounterfactualgeneration. arXivpreprint
arXiv:2302.07865,2023. 2
[58] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P
Xing. Learningrobustglobalrepresentationsbypenalizing
localpredictivepower.AdvancesinNeuralInformationPro-
cessingSystems,32,2019. 2
[59] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander
Madry. Noiseorsignal: Theroleofimagebackgroundsin
objectrecognition. arXivpreprintarXiv:2006.09994,2020.
1,2
[60] Jianhao Yuan, Francesco Pinto, Adam Davies, and Philip
Torr. Not just pretty pictures: Toward interventional data
augmentationusingtext-to-imagegenerators. 2023. 2
[61] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo
Xue.Droid-sec:deeplearninginandroidmalwaredetection.
InProceedingsofthe2014ACMconferenceonSIGCOMM,
pages371–372,2014. 1
[62] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,Ruoqi
Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming
Ren,YuxuanSun,etal. Mmmu:Amassivemulti-discipline
multimodalunderstandingandreasoningbenchmarkforex-
pertagi. arXivpreprintarXiv:2311.16502,2023. 6
[63] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang,
andInSoKweon. Text-to-imagediffusionmodelingener-
ativeai: Asurvey. arXivpreprintarXiv:2303.07909,2023.
2
[64] HongyangZhang,YaodongYu,JiantaoJiao,EricXing,Lau-
rentElGhaoui,andMichaelJordan.Theoreticallyprincipled
trade-offbetweenrobustnessandaccuracy. InInternationalImageNet-D: Benchmarking Neural Network Robustness on
Diffusion Synthetic Object
Supplementary Material
6.LabelingtaskonAmazonMechanicalTurk forthesameimage,theirannotationsarenotconsistentand
willberemoved.
Forreliablebenchmarks,weuseAmazonMechanicalTurk
Foreachlabelingtaskwithupto20images,weinclude
(MTurk) [11, 22, 47] to evaluate the labeling quality of
onepositivesentinel,onenegativesentinelandtwoconsis-
ImageNet-D.
tent sentinels. We discard the responses if the workers do
notpassallthesentinelchecks.
6.1.Labelingtaskdesign
6.3.Results
Labelinginstructions. SinceImageNet-Dincludesimages
with diverse object and nuisance pairs that may be rare in For each image, we collect independent annotations from
therealworld,wetakeboththeappearanceandfunctional- 10workersandfilteroutresponsesfromtheworkersthatdo
ity of the main object as the labeling criteria. Specifically, not pass the quality check. A total of 679 qualified work-
we ask the workers from MTurk to answer the following erssubmitted1540labelingtasks,resultinganagreementof
twoquestions: 91.09%onsampledimagefromImageNet-D.
Question 1: Can you recognize the desired object
([ground truth category]) in the image? It may 7.ExperimentalresultsonImageNet-D
haverarebackgrounds,textures,materials,orstyles.
More results for Section 4. We compare the model ac-
Question 2: Cantheobjectintheimagebeusedas
curacy of Image-D with existing test sets, including Ima-
thedesiredobject([ground truth category])?
geNet[51],ObjectNet[3]s,ImageNet-9[59]andStylized-
Labeling pipeline. To ensure that the workers under-
ImageNet [15]. All the accuracy numbers are reported in
stand these two criteria, we ask the workers to label two
Table7,whichalsoincludesthenumbersofFigure8inthe
exampleimagesforpractice,whichprovidesthecorrectan-
mainmanuscript.
swerfortheabovetwoquestions.Afterthepracticesession,
TrainingsetupsforTable6.Weintroduceexperimental
theworkersarerequiredtolabelupto20imagesinonetask,
detailsofTable6inthemainmanuscript.Wefinetuneapre-
andanswerbothtwoquestionsforeachimage. Theworker
trainedResNet18modelonvarioustrainingsets. Toexam-
selects’yes’or’no’foreachquestion.
inetheeffectofincorporatingsyntheticimagesintothefine-
Labelling UI. The labeling page is designed as in Fig-
tuningtrainingset,wesampleImageNetandSynthetic-easy
ure12. Theworkerscanproceedtothenextimageonlyif
for same data distributions, where Synthetic-easy includes
theyfinishbothquestionsonthecurrentpage.
diffusion-generatedimagescorrectlyclassifiedbysurrogate
6.2.Qualitycontrolofhumanlabelling models. Each set contains 111098 images, and both sets
havesamenumberofimagespercategory. Allmodelsare
We use sentinels to ensure high-quality annotations. For finetunedononapre-trainedResNet18atepoch90for10
each labeling task with multiple images, we design three epochsfurther,usingaSGDoptimizerwithalearningrate
typesofsentinelsasfollows. of 0.0001. Apart from sampled ImageNet and Synthetic-
Positive sentinel: Image that belongs to the desired easy,weincludeoriginalImageNet-1Kastrainingdatafor
category and is correctly classified by multiple models. If smoothtraining.
theworkersdonotselect’yes’forthisimage,theymaynot
understand the concept well and their annotations will be
removed.
Negative sentinel: Image that does not belong to the
desired category. For example, if the desired category is a
chair, the negative sentinel may be a ladle. If the workers
select ’yes’ for the ladle image, they may not answer the
questionsseriouslyandtheirannotationswillberemoved.
Consistentsentinel.Weassumethattheworkersshould
select the same answer for the same image if it appears
multipletimes. Consistentsentinelsareimagesthatappear
twiceinarandomorder. IftheworkersanswerdifferentlyFigure12. UserinterfaceforMTurkstudies. Theworkerscanproceedtothenextimageonlyiftheyfinishbothquestionsonthecurrent
page.
Table7. Testaccuracy ofvision modelsand largefoundation models(%). Weshow thetestaccuracy forthe visionmodels andlarge
foundationmodels(rows)ondifferenttestsets(columns). ThenumbersingreenrefertotheaccuracydropofImageNet-Dcomparedto
ImageNet.ForMiniGPT-4andLLaVa,ImageNet-Dreducestheaccuracyby16.81%and29.67%comparedtotheImageNet,respectively.
OurresultsshowthatImageNet-Diseffectivetoevaluatetherobustnessofneuralnetworks.
Model Architecture ImageNet ObjectNet ImageNet-9 Stylized-ImageNet ImageNet-D ImageNet-D
Background Texture Material Total
Visionmodel(CNN) VGG11 56.85 21.85 68.59 13.12 6.46(-50.39) 9.64(-47.21) 11.87(-44.98) 7.43(-49.42)
VGG13 58.42 23.23 68.96 13.59 7.39(-51.03) 8.63(-49.79) 9.6(-48.82) 7.78(-50.64)
VGG16 60.86 25.96 73.28 13.83 9.94(-50.92) 10.84(-50.02) 13.79(-47.07) 10.49(-50.37)
VGG19 62.77 27.19 74.84 16.25 9.8(-52.97) 11.45(-51.32) 12.39(-50.38) 10.28(-52.49)
ResNet18 57.15 22.62 71.65 21.17 7.41(-49.74) 10.64(-46.51) 12.22(-44.93) 8.31(-48.84)
ResNet34 61.81 26.15 75.31 21.33 8.87(-52.94) 12.25(-49.56) 12.74(-49.07) 9.68(-52.13)
ResNet101 67.66 32.34 81.85 22.66 12.38(-55.28) 13.65(-54.01) 13.44(-54.22) 12.64(-55.02)
ResNet152 69.18 34.41 83.41 23.05 13.79(-55.39) 13.86(-55.32) 18.85(-50.33) 14.4(-54.78)
Densenet121 63.1 28.74 82.05 19.92 9.99(-53.11) 12.65(-50.45) 15.36(-47.74) 10.9(-52.20)
Densenet161 66.99 31.86 84.91 22.5 11.34(-55.65) 14.06(-52.93) 13.26(-53.73) 11.85(-55.14)
Densenet169 64.13 30.13 83.8 22.97 10.73(-53.40) 12.25(-51.88) 12.39(-51.74) 11.09(-53.04)
Densenet201 66.58 31.36 83.43 21.8 9.88(-56.70) 10.84(-55.74) 15.71(-50.87) 10.67(-55.91)
Wideresnet50 69.06 32.65 81.41 19.45 8.69(-60.37) 11.24(-57.82) 11.17(-57.89) 9.25(-59.81)
Wideresnet101 69.2 34.37 82.17 21.48 10.55(-58.65) 13.05(-56.15) 12.04(-57.16) 10.98(-58.22)
Visionmodel(ViT) ViT-B/32 65.02 27.59 77.51 42.34 6.64(-58.38) 12.25(-52.77) 13.79(-51.23) 8.07(-56.95)
ViT-B/16 72.14 34.79 82.49 31.02 10.49(-61.65) 16.87(-55.27) 17.63(-54.51) 12.0(-60.14)
ViT-L/16 68.67 32.7 78.91 29.38 7.68(-60.99) 14.06(-54.61) 15.53(-53.14) 9.27(-59.40)
CLIP RN101 62.48 42.89 83.09 22.58 21.47(-41.01) 21.29(-41.19) 25.83(-36.65) 21.96(-40.52)
ViT-B/32 64.06 43.67 79.56 44.22 18.73(-45.33) 33.33(-30.73) 30.37(-33.69) 21.61(-42.45)
ViT-B/16 67.95 54.87 85.16 40.62 20.64(-47.31) 22.89(-45.06) 29.32(-38.63) 21.9(-46.05)
MiniGPT-4 Vicuna13B 88.77 77.57 89.46 69.88 71.81(-16.96) 72.48(-16.29) 72.5(-16.27) 71.96(-16.81)
LLaVa Vicuna13B 79.32 76.02 90.84 61.94 52.89(-26.43) 40.53(-38.79) 36.28(-43.04) 49.65(-29.67)
LLaVa-1.5 Vicuna13B 89.08 78.66 93.88 64.14 73.31(-15.77) 67.27(-21.81) 67.08(-22.00) 71.95(-17.13)
LLaVa-NeXT Vicuna13B 86.83 79.97 91.47 62.61 75.91(-10.92) 64.56(-22.27) 60.39(-26.44) 72.9(-13.93)
LLaVa-NeXT Hermes-Yi-34B 85.83 77.54 90.52 57.98 68.77(-17.06) 46.67(-39.16) 54.11(-31.72) 64.76(-21.07)