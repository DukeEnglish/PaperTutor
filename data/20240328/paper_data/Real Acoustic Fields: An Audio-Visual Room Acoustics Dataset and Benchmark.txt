Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark
ZiyangChen1,2∗ IsraelD.Gebru2 ChristianRichardt2 AnuragKumar3
WilliamLaney2 AndrewOwens1 AlexanderRichard2
1UniversityofMichigan 2CodecAvatarsLab,Pittsburgh,Meta 3RealityLabsResearch,Meta
https://facebookresearch.github.io/real-acoustic-fields
Abstract volvedwithaninputsound,simulatethesoundthatwould
beperceivedbythelistener,performingchangeslikeadding
WepresentanewdatasetcalledRealAcousticFields(RAF) reverbordampeningcertainfrequencies. EstimatingRIRs
thatcapturesrealacousticroomdatafrommultiplemodali- fornovelemitterandlistenerposesfromsparselysampled
ties. Thedatasetincludeshigh-qualityanddenselycaptured RIRsacquiredfromascenehasbeenamajorfocusofrecent
roomimpulseresponsedatapairedwithmulti-viewimages, work in audio [5, 21, 26, 28, 42] and audio-visual learn-
andprecise6DoFposetrackingdataforsoundemittersand ing [2, 9, 13, 37, 54, 55]. Inspired by novel-view synthe-
listenersintherooms. Weusedthisdatasettoevaluateexist- sis [32, 38, 40], an emerging line of work has proposed
ingmethodsfornovel-viewacousticsynthesisandimpulsere- learning-basedmodelsbasedonneuralfields[34,36,57].
sponsegenerationwhichpreviouslyreliedonsyntheticdata. Despite the recent interest in sound propagation in the
Inourevaluation,wethoroughlyassessedexistingaudioand audio-visual community, existing methods have been de-
audio-visualmodelsagainstmultiplecriteriaandproposed veloped and evaluated on highly simplified datasets with
settingstoenhancetheirperformanceonreal-worlddata. artificiallygeneratedimpulseresponses. Thisisduetothe
Wealsoconductedexperimentstoinvestigatetheimpactof fact that collecting real-world RIRs is a challenging pro-
incorporatingvisualdata(i.e.,imagesanddepth)intoneu- cessthatrequiresbothplayingandrecordingsoundsfrom
ral acoustic field models. Additionally, we demonstrated densely sampled positions throughout a scene. Many dif-
the effectiveness of a simple sim2real approach, where a ferentdatacollectioneffortshaveeachmadedifferentcom-
modelispre-trainedwithsimulateddataandfine-tunedwith promisesbetweentheconflictingfactorsintermsofrealism,
sparsereal-worlddata,resultinginsignificantimprovements groundtruth,andcosts. Existingdatasets[11,29,33]thus
inthefew-shotlearningapproach. RAFisthefirstdatasetto makehighlyrestrictiveassumptions,suchasbyhavingonly
providedenselycapturedroomacousticdata,makingitan a single sound emitter at a fixed pose, by having limited
idealresourceforresearchersworkingonaudioandaudio- (2D-only)spatialcoverageofthescenes,orbyhavingonly
visualneuralacousticfieldmodelingtechniques.Demosand simple planar geometry. Consequently, these datasets do
datasetsareavailableonourprojectpage. not fully capture the complexities of real-world room ge-
ometry,materialvariations,andsourcedirectivity. Thelack
ofareal“goldstandard”benchmarkmakesitchallenging
1.Introduction toeffectivelyanalyzeexistingapproachesunderreal-world
assumptionsandtodriveresearchonaudio-visualinformed
Soundwavesreflectoffobjectsinascenebeforereachinga
soundpropagationtowarditstruepotential.
listener’sears. Thesereflectionschangethesoundwavesin
Inthispaper,weproposeanaudio-visualsoundpropaga-
complexwaysandconveytheobjects’size,shape,andmate-
tiondatasetandbenchmarkthataddressestheshortcomings
rialproperties. Accuratelymodelingthesechangesiscrucial
of previous approaches. Our Real Acoustic Fields (RAF)
forspatialaudiorendering,andplaysakeyroleinadding
datasetisamultimodalrealacousticroomdatasetwithdense
thesenseofimmersioninavarietyofapplicationdomains,
3Daudiocapturesofalargespacefilledwithandwithout
suchas3Dgames,virtualandaugmentedreality[22,70].
furniture.Tocapturedenseandcalibratedaudiointherooms,
The goal of a sound propagation model is typically to
weusedacustom-builtmicrophonetowersystemandrobotic
estimatearoomimpulseresponse(RIR)foragivenemitter
loudspeakerstand. Themicrophonetowercontains36om-
and listener pose. RIRs are linear filters that, when con-
nidirectional microphones placed at different heights and
*WorkdoneduringaninternshipatMeta. positions. Theroboticstandcanrotateandpositiontheloud-
1
4202
raM
72
]DS.sc[
1v12881.3042:viXraAudio capture Visual capture
Speaker Earful tower Eyeful tower
Figure1.Datacapturingsetup.(a)Audiocapture(left):theloudspeakerandmicrophonerecordingsystem(EarfulTower)areplacedat
differentlocationswithintheroomtomeasureandcaptureRIRs.(b)Visualcapture(right):thecamerarig(EyefulTower)movesaround
roomstocapturemulti-viewimagesforvisualreconstructionandnovel-viewsynthesis.
speaker at different heights, enabling us to capture sound mesh from existing 3D scene datasets [6, 46, 56, 65, 67].
sourcedirectivitydata. Weusedamotioncapturesystem However,thesesyntheticdatasetslackthecomplexitiesof
topreciselytracktheposeofthemicrophonesandtheloud- real-worldroomgeometry,materialvariations,andsource
speakerthroughoutthescene. Moreover,wepairourRIR directivity. Toaddressthis,wehavegatheredareal-world
recordingswithcapturedhigh-fidelityimagesandgeome- multimodalacousticroomdatasettofurtherresearchinthe
try[66]toenablemorepotentialresearchintheaudio-visual fieldofneuralacoustics.
direction. Theresultingdatasetcontainshigh-fidelitydense
RIR synthesis. Synthesizing RIR has been a longstand-
RIRs,speechrecordingsfromexistingspeechdatasets,posi-
ingresearchtopic. SimulatedapproachesforRIRsynthesis
tionannotations,andvisualreconstructions.
primarily rely on wave-based [19, 61] or geometric meth-
Usingthisdataset,weconductthefirstsystematicstudy
ods[8,10,52]. Whilethesemethodseffectivelysimulate
ofrecentaudioandaudio-visualsoundpropagationmodels,
sound propagation in space, they often struggle to repro-
including: 1)extensionofcommon2Dapproachesinto3D
duce all wave-based sound effects. Geometric models do
scenes, 2) how perceptual similarity metrics proposed by
not account for interference and diffraction. While wave-
othermodels[34,37]changethegeneratedsounds,and3)
basedmodelsaretheoreticallyapplicabletoallfrequencies,
theroleofvisualinformationinaudio-visualmodels.Finally,
they face difficulty in accurately modeling the frequency-
ourdatasetalsoallowsustoevaluatefew-shottraining. We
dependentdirectionalcharacteristicsofsoundsources,re-
proposeasimple,yethighlyeffective“sim2real”approach
ceivers,androomswithcomplexgeometries. Recentmeth-
thatbeginsbypretrainingonsyntheticdataandthenrefining
odshaveleveragedmachinelearningtechniquestocreate
theresultwithasmallnumberofreal-worldsamples. We
morerealisticRIRs. Ratnarajahetal.[48]useagenerative
willreleasethedatasetandbenchmarkuponacceptance.
adversarialnetwork(GAN)tosynthesizeRIRs. Laterwork
extendedthisapproachbyconditioningonscenemeshes[47]
2.RelatedWork
andvisualsignals[49]. Fewotherworksfocusonlearning
continuousimplicitneuralrepresentationsforaudioscenes,
Novel-viewacousticdatasets. ManyRIRdatasetsarecol-
whichtargetgeneratinghigh-fidelityimpulseresponsesat
lectedforacousticresearch[26,30,31,63]whiletheyare
any arbitrary emitter-listener positions for a single scene,
notapplicablefornovel-viewacousticpropagationmodel-
suchasNAF[36],INRAS[57]andNACF[34]. Neverthe-
ing. MeshRIR[29]recordedreal-worldmonauralimpulse
less,priorstudieshaveprimarilyfocusedonsimulateddata
responses from a three-dimensional cuboidal room, with
owing to the absence of suitable real-world datasets. Our
microphones at a fixed height. The room was empty and
novel dataset presents a path to extend these approaches
lacked visual information about the scene. Two previous
towardreal-worldmodelingofneuralacousticfields.
methodsbyLiangetal.[33]andChenetal.[11]collected
real-worldaudio-visualdatasetsfornovel-viewacousticsyn- Audio-visual acoustic learning. Recent works have ex-
thesis tasks. Nevertheless, Liang et al. [33] only features ploredlearningacousticinformationfrombothaudioand
asinglestationarysoundsourceandChenetal.[11]only vision. Chenetal.[12]andChowdhuryetal.[15]propose
hassparsereceiverpositions,whichmightnotrepresentthe de-reverberatingaudiosignalsusingvisualenvironmenten-
entire acoustic environment for arbitrary speaker-receiver coding. Some researchers investigate the visual acoustic
pairs. SoundSpaces1.0[7]andSoundSpaces2.0[10]gener- matching problem [9, 54, 55], aiming to synthesize audio
atedlarge-scalesyntheticacousticdatasetsbasedonroom that matches target acoustic properties based on images.
2Table1.Datasetcomparison.Wecomparetheattributesofourdatasetwithpreviouslyproposeddatasets.
Dataset Modality Real-world Visualsource Dimension Scenes Density
SoundSpaces1.0[7] A&V ✗ Mesh 2D 103 16samples/m2
SoundSpaces2.0[10] A&V ✗ Mesh 3D 1600+ –
MeshRIR[29] A ✓ – 2.5D 1 18samples/m3
RAF(ours) A&V ✓ NeRF&Mesh 3D 2 372samples/m3
Someworkslearntogeneratesoundsatthearbitraryspeaker attheaveragehumanearheightlevelandusedfewermicro-
andlistenerpositionsviasparseaudio-visualobservations phonesatlowerlevels. Themicrophonesareintegratedwith
ofscenes[13,37]. Othersfocusonthenovel-viewacoustic threeRME12Mic-Dunits,daisy-chainedandphase-locked
synthesistask,synthesizingbinauralsoundfromaudioand torecordsynchronizedmulti-channelaudiosignals.
visualinformationatanewviewpoint[2,11,14,33].Weuse ForgeneratingroomexcitationsignalsduringRIRmea-
ourdense3Daudio-visualdatasettoevaluatethesemethods’ surements, we used a Genelec 8030C speaker mounted
effectivenessandtheroleofvision. on a robotic stand. This stand offers remote control, pro-
grammableheightadjustment,andspeakeraxisrotation.
Visualscenecaptureandviewsynthesis. Thereisarich
literatureoncapturingstaticscenestoreconstructthemin Capturingprocedure. Weuniformlydistributethemicro-
3D and/or to render novel viewpoints; see recent surveys phone tower at walkable positions in the room that might
foracomprehensiveoverview[50,60]. Manyapproaches beoccupiedbyahumanlistener(i.e.,openareas). Weused
thatfocuson3Dscenereconstructionuserepresentations theroboticstandtoautomatetherotationoftheloudspeaker
suchas(truncated)signeddistancefieldstocombinemul- every120°onitsaxisateachposition,toobtaindifferently
tipleobservationsfromRGB-Dsensors [43,44,58,69]or orientedsoundsources. Duringtherecordingprocess,we
standard color videos [20, 25, 41, 59]. These approaches playedlogarithmicsine-sweepsignalsandsimultaneously
tendtosacrificerenderingfidelityinfavorofbetter3Dre- recordedtheresultingreverberatedsignalsusingthemicro-
constructionaccuracy. Ontheotherhand,whenthevisual phonesonthetower. Aftercompletingafull-circlerotation,
quality of novel views is paramount, approaches building the speaker stand would adjust its height, and we would
onimage-basedrendering[4,24,45,53]or,morerecently, repeatthemeasurementsforeach120°turn. Thenwerelo-
neuralradiancefields[3,39,64,66]haveachievedthehigh- catedthemicrophonetowertoanewpositionandrepeated
estvisualfidelity,evenwhilecompromisingthequalityof themeasurements. Weshiftedthespeakertoanewlocation
thereconstructed3Dgeometry. Tomaximizethevisualfi- after the microphone tower had swept through the entire
delityofourdataset,wecaptureandreconstructitusingthe scene. Meanwhile, after each sine-sweep, we played and
VR-NeRFapproach[66]. recorded6-secondlongspeechutterancesrandomlysampled
fromtheVCTKdataset[62].
3.TheRAFDataset Toaccuratelytracktheorientationandpositionsofthe
loudspeakerandmicrophonesintheroom,weusedtheOpti-
WepresentRAF,adatasetofdenselyrecordedreal-world
Trackmotioncapturesystem. Weplacedreflectivemarkers
roomimpulseresponses(RIR)pairedwithdensemulti-view
ontheloudspeakerandthemicrophonetower,allowingusto
imagesofthescenes. Tothebestofourknowledge,thisis
preciselyestimatetheir6degreesoffreedom(6DoF)poses.
thefirstmulti-modal3DRIRdatasetwithdenseaudioand
visualmeasurementspairedwithprecise6DoFtrackingdata. Captureddata. Withthesetupdescribedabove,wecol-
Inthissection,wewillintroducethehardwaresetupused lecteddensedatafromoneroomundertwodifferentconfig-
fordatacollectionandourdatacollectionpipeline. urations. Inthefirstsetting,theroomwasemptyandonly
containedtheessentialequipmentnecessaryforcapturing
3.1.AudioCapturing impulseresponsedata. Inthesecondsetting,wefurnished
the room to resemble a simple studio or living room. We
OurgoalistocollectdenseRIRsamplesthatcovertheentire
collected47KRIRsfortheemptyroomand39KRIRsfor
scenewithpairedtransmitterandreceiverlocations.
thefurnishedroom. ThecollectedRIRsare4secondslong,
Hardware. Tofacilitatetheaudiodatacollectionprocess, whichcomprehensivelycapturestheacousticinformation.
wedevelopedanovelmicrophonetowersystemcalledEarful Our room has two parts: a large room with soft material
Tower,asshowninFigure1. Thetowerfeatures36omni- wallstoabsorbsounds, andasmallerroomwithconcrete
directionalmicrophones. Thesemicrophoneswereplaced walls for increased reverberation. We show our RIR dis-
atdifferentheightlevelsonthetower,arrangedintheshape tribution and room measurements in Figure 2. Please see
ofaninvertedpinecone. Wepositionedmoremicrophones AppendixA.3formoredetails.
33.2.VisualCapturing Furnished Room Unfurnished Room
4.9 m
Toprovideahigh-fidelityvisualreconstructionofthescenes
andsynthesizetheappearancefromanyviewpoint,wefol- 3.9 m
lowtheVR-NeRFapproach[66]tocapturedensemulti-view
imagesforthescenes,usingtheEyefulTowermulti-camera
rig shown in Figure 1. We move the Eyeful Tower rig to
cover the available floor area for a dense capture of our
8.8 m
static scenes, resulting in 3,388 images for the furnished
roomand8,030imagesfortheemptyroom. WeuseAgisoft 6.8 m
Metashape[1]toestimatetheposesofcameraswithinthe
rigusingstructure-from-motionandreconstructatextured
meshofeachscene. Lastly,weusegroundcontrolpoints
Figure2.DatadistributionofRAF.Bluedotsrepresentspeaker
toalignthecamerasandRIRdatatothesamecoordinate
positionsandreddotsrepresentmicrophonepositions.Theroom
system. Theaudioandvisualcapturesareperformedsepa-
dimensionsareshownontheright.
ratelytopreventanyinterferencebetweenaudioandvisual
devices (e.g., speakers and microphones appearing in the
geometryorvisualcuestomodelacousticfields. Moreover,
images)andtoeliminatetheimpactofcameradeviceson
weintroduceasimpleyeteffectivesim2realapproachfor
audiocapture(e.g.,camerascreatingreflections).
synthesizingRIRsinfew-shotscenarios,whichcansignifi-
To generate the views at each microphone or speaker
cantlyenhanceperformance.
position,wetrainNeRFmodelsusingtheInstantNGParchi-
tecture[66]foreachscene. Thisenablesustoexaminethe 4.1.Models
effectivenessofincorporatingvisualsignals,suchasRGB
Weadoptedseveralexistingstate-of-the-art2Dacousticfield
anddepthinformation,intoacousticfieldmodeling.
andaudio-visualmodelstoour3Dsetup,withsomemodifi-
3.3.ComparisontoPriorDatasets cations. Thesemodelsarebrieflydescribedbelow.
NAF. The neural acoustic field [36] models the room
We compare our dataset to several prior acoustic datasets
acoustics using an implicit representation. NAF learns a
collectedfromrealscenesorthroughsimulatorsinTable1.
gridoflocalgeometricfeaturesGtoencodethespatialin-
IncomparisontotherealMeshRIRdataset[29],ourdataset
formation of speakers and receivers at different positions,
offers20timesdenserandmoreextensivecoverageofroom
andqueriedspeakersandreceiversgridfeaturesG(s)and
impulseresponsedatafromdifferentheightlevels. Further-
G(r)willbeprovidedtotheNAF asadditionalcontext.
more,ourdatasetfeaturesmorecomplexroomgeometryand F
NAFrepresentsimpulseresponsehinthetime-frequency
materialswithfurniture,goingbeyondthelimitationsofa
domainH = STFT(h) RF×K usingshort-timeFourier
singlebox-shapedroom. Comparedtosimulateddatasets | |∈
transform(STFT),whereF isthenumbersoffrequencybin
suchasSoundSpaces1.0[8]andSoundSpaces2.0[10],our
andK isthenumberoftimeframes. Giventhefrequency
datasetstandsoutforitshigh-qualityrealimpulseresponses
binf andtimeframek,NAFpredictsthelogmagnitudeof
andhigh-fidelityvisualrenderingfromNeRF.Incontrast,
thespectrogramHˆ(k,f):
thesimulateddatasetsfallbehindintermsofbothaudioand
visualquality,resultinginalessrealisticrepresentationof Hˆ(k,f)= (G(s),G(r),θ,k,f), (2)
real-worldacoustics. F
anditminimizestheL1lossbetweenpredictedandground-
4.Learning3DNeuralAcousticFields
truthimpulseresponseinlogscale:
Modeling acoustic fields can be formulated as: given the = logHˆ(k,f) logH(k,f) . (3)
speaker’sspatialpositions=(x ,y ,z ) R3,thespeaker LNAF ∥ − ∥1
s s s
orientation θ R2, and the receiver spa∈ tial position r = To obtain the time-domain impulse response h, NAF per-
(x r,y r,z r)
R∈
3 inaroom,afunction predictsthecorre- forms inverse STFT on predicted spectrogram magnitude
∈ F
spondingimpulseresponseh: Hˆ withrandomphase.
| |
:(s,r,θ) h RT. (1) INRAS. The implicit neural representation for audio
F (cid:55)→ ∈ scenes[57]isinspiredbyinteractiveacousticradiancetrans-
Previous studies have proposed various methods to learn fer,wheresoundenergyfirstscattersfromtheemittertothe
,buttheyrelyonsyntheticdata. Weinvestigateandim- boundariesofthescene,thenpropagatesthroughthescene
F
provethoseexistingmodelsinreal-worldscenariosusingour by bouncing between the surfaces, and finally gathers at
dataset. Additionally,weevaluatetheeffectivenessofusing thelistenerposition. INRASdefinesasetofbouncepoints
4
weiv
poT
weiv
ediS
m
1.6
m
6.2
m
7.3
m
9.3b N R3,whichareuniformlysampledfromthescene
{ i }i=1⊂ simulator
surface. Itrepresentsthepositionofspeakersorreceivers Stage 1 pretraining
using the relative distance to those bounce points, which real-world
providesmoreinformationaboutthescenegeometry: Listener simulator simulated RIR fine-tuning
ds N = s b N , dr N = r b N . (4) Emitter
{ i}i=1 { − i }i=1 { i}i=1 { − i }i=1 Stage 2
F
INRAS encodes speaker relative distance ds N , re-
{ i}i=1 predicted RIR real RIR
ceiverrelativedistance dr N ,andbouncepointpositions implicit network
{ thb ei f} eN i a= t1 uri ent do imla et ne sn it of ne sa it{ zu er .ei Ts}i hS= e,1 R tim,B ee∈ mbR eN dd× inD g, Mwher Re TD ×Dis F neig twur oe rk3. oS ni sm im2r ue laa tl em de dt ah taod wo itv he dr ev nie sw el. yF si ar mst, pw line gtr ea min itt th ee r–i lm isp teli nc ei rt
∈
isintroducedforthewholetimesequenceandobtainsspatial- positionpairs.Wethenfine-tuneitonsparsereal-worlddata.
timefeaturesviamatrixmultiplication. INRASgenerates
time-domainRIRsdirectlybydecodinggivenfeatures:
resultingintheoveralllosswithmulti-resolutionSTFT:
hˆ = (cid:0) MS⊤,MR⊤,MB⊤,θ(cid:1) . (5)
F = + +λ , (10)
NACF sc mag decay
L L L L
TheINRASmodelminimizestheSTFTlossinthetime-
frequencydomainH = STFT(h),includingspectralcon- whereλistheweightofthedecayloss.
| |
vergenceloss andmagnitudeloss : AV-NeRF. We also consider the very recent AV-NeRF
sc mag
L L
model [33]. Unlike NACF, which uses fixed visual con-
Hˆ H
= + = ∥ − ∥2 + Hˆ H . (6) textsindependentfromspeaker-receiverpositions,AV-NeRF
LINRAS Lsc Lmag H 2 ∥ − ∥1 provideslocalvisualinformationCr ,Cr thatdepends
∥ ∥ rgb depth
onthelistenerposition. Itpredictsimpulseresponses:
We used multi-resolution STFT loss [68], which involves
computinganSTFTlossatmultipletime-frequencyscales. hˆ = (cid:0) s,r,Cr ,Cr ,θ(cid:1) . (11)
NACF. NeuralAcousticContextField(NACF)[34]isa F rgb depth
multimodalextensionofINRASwhichusesadditionalcon- WealsominimizethelossesinEquation10.
text fromothermodalities. Specifically,NACFusesRGB
NAF++ and INRAS++. We observed that the energy
images v and depth images v for each predefined
rgb depth decay loss (Equation 9) from Majumder et al. [37] used
bounce point b to extract local geometric and semantic
i in NACF can also improve the results of the other mod-
information, and material properties. Similar to INRAS,
els. WethereforeintroduceimprovedmodelsNAF++and
RGBanddepthimagesareencodedintolatentcontextem-
INRAS++thathavetheselosses:
beddings C ,C RN×D via nonlinear projection
rgb depth
∈
and converted into space-time features.1 NACF decodes = +λ , (12)
NAF++ mag decay
L L L
provided features with additional context to generate the
= + +λ . (13)
INRAS++ sc mag decay
impulseresponseinthetimedomain: L L L L
hˆ = (cid:0) MS⊤,MR⊤,MB⊤,MC⊤ ,MC⊤ ,θ(cid:1) . (7) 4.2.Sim2realforFew-ShotRIRSynthesis
F rgb depth
NACFminimizesthesamelossasINRAS(Equation6), Real-worldimpulseresponsescanbeexpensivetoacquirein
as well as the energy decay loss proposed by Majumder largequantities,andobtainingadensecaptureofsuchdata
etal.[37],whichencouragestheenergydecaycurvesofthe forscenescanbeparticularlychallenging. Forexample,in
predictedandtargetRIRstobesimilar. Giventhemagnitude comparisontoavisualNeRF,therearecomparativelyfewer
spectrogram H RF×K, we calculate the decay curve geometricconstraints.Toaddressthislimitation,wepropose
∈
(H): atwo-stagetrainingapproachthatleveragessimulatedaudio
D (H)[k]=1+ E k , (8) data to enhance the synthesis of real-world audio with a
D (cid:80)K E limitedamountoftrainingsamples. Ourmethodcomprises
i=k+1 i
twokeystages: pretrainingondensesyntheticdataandfine-
where E = (cid:80) H(f,k)2 is the energy of time frame k.
k f tuningonsparsereal-worldsamples,asshowninFigure3.
We minimize the L1 distance between the predicted and
Pretraining on dense synthetic data. In the first stage,
ground-truthdecaycurvesinlogspace:
wepretrainouraudioneuralfield , e.g., INRAS,onthe
decay = log (Hˆ) log (H) 1, (9) richsyntheticimpulseresponsesgenF eratedfromanacoustic
L ∥ D − D ∥
simulatorwithdiverseemitterandlistenerpositions. Weuse
1Weremovetheacousticcoefficientcontextduetotheunavailability
theroom’sgeometryandacousticproperties(reverberation)
ofmaterialcoefficientannotationsforourdatasetandourobjectiveof
modelingreal-worldcaptureswithoutadditionalannotations. observedfromlimitedrealexamplestocreatethesimulator.
5Table2.EvaluationonRAFwith48kHzhigh-fidelityimpulseresponses.Weevaluateeachmethodwiththequalityofgeneratedimpulse
response,storagerequirements,andinferencespeed.Theresultsareaveragedacrosstwoscenes.Originaldenotesuncompressedaudio.The
bestresultsareinbold.
Method Variation
STFTerror C 50error EDTerror T 60error Parameters Storage Speed
(dB)↓ (dB)↓ (sec)↓ (%)↓ (Million)↓ (MB)↓ (ms)↓
AAC 1.26 2.49 0.085 25.64 2,033.81
Linear Opus 0.92 0.86 0.029 10.19 – 2,033.81 –
original 0.88 0.83 0.027 7.82 9,518.32
AAC 1.04 1.97 0.064 22.83 2,033.81
Nearest Opus 0.49 0.76 0.021 10.03 – 2,033.81 –
original 0.38 0.71 0.020 7.67 9,518.32
vanilla 0.64 0.71 0.021 10.08
NAF[36] 5.51 22.04 11.98
+decayloss 0.64 0.53 0.017 8.19
vanilla 0.36 0.79 0.025 8.01
INRAS[57] 1.33 5.31 3.36
+decayloss 0.39 0.57 0.017 6.17
vanilla 0.39 0.59 0.017 6.62 1.52 6.05 3.17
NACF[34]
+temporal 0.39 0.59 0.018 7.31 1.75 7.00 3.41
AV-NeRF[33] vanilla 0.39 0.73 0.021 8.11 12.99 51.98 6.48
Byexposingthemodeltoadiverserangeofsimulatedaudio methodssuchasAdvancedAudioCoding(AAC)andOpus
data,weenableittolearngeneralimpulseresponsepatterns tocompressaudiowithlowbitrates;seeAppendixA.2for
andspatialinformation,whichserveasastrongfoundation details.
forsubsequentfine-tuning.
Metrics. FollowingSuetal.[57],weuseseveralmetrics
Fine-tuning on sparse real-world samples. We use
toassessthequalityofthepredictedimpulseresponses,in-
sparsereal-worldaudiosamplesforfine-tuningtheneural
cludingClarity(C ),ReverberationTime(T ),andEarly
50 60
field . By fine-tuning it on real-world data, the model
DecayTime(EDT).C quantifiestheclarityofacousticsby
F 50
adaptstothespecificsofreal-worldaudiowhileretaining
measuringtheratioofinitialsoundenergytosubsequentre-
theknowledgegainedfromthesimulatordata. Bycombin-
flectionswithinaroom,withhighervaluesindicatingclearer
ingthestrengthsofthesimulatorandreal-worlddata,our
acoustics. T reflects the overall sound decay within a
60
method achieves high-quality audio synthesis with sparse
room,whileEDTfocusesontheearlyportionofthesound
real-worlddataandstrikesabalancebetweendatacollection
decaycurve. WealsoevaluateSTFTerror,theabsoluteerror
costandsynthesisperformance.
betweenthepredictedandtheground-truthlog-magnitude
spectrograms[16,36]. Additionally,wemeasurethecom-
5.Experiments
putationalefficiencyofeachmethodbyevaluatingstorage
Weuseourreal-worlddatasettoevaluateaudioandaudio- requirementsforsavingaudioscenesandtheinferencetime
visualacousticfieldmodelingmethods. Also,weshowcase neededforrenderinganimpulseresponse.
oursim2realmethodthatbooststheminthefew-shotsetting.
Experimentalsetup. Foreachscene,weuse80%ofthe
5.1.Evaluationof3DNeuralAcousticFields datafortrainingandholdout5%and15%forvalidationand
testing,respectively.Theimpulseresponsesareresampledto
Weevaluatethemethodsonthe3Dacousticfieldmodeling 48kHzor16kHzsamplingrateandarecutto0.32sfortrain-
taskusingourfullreal-worlddataset. ingandevaluationbasedontheaveragereverberatetimeof
Models. Weconsiderbothstate-of-the-artneuralfieldmod- theroom. Forallexperiments,weusetheAdamWoptimizer
els and classical models. To adapt to the 3D domain, we [27,35]withalearningrateof10−3,anexponentialdecay
extendNAF[36],INRAS[57],NACF[34],AV-NeRF[33], learningrateschedulerwitharateof0.98,andabatchsize
andtheirvariants,introducinganextradimensionforneu- of128. WetrainallthemodelsonanNVIDIAA100GPU
ral acoustic field modeling, which was not feasible with for200epochsandevaluatethelastepoch. ForNACF[34]
otherexistingdatasets. Following[36,57],wealsocompare andAV-NeRF[33],weusethevisualNeRFmodeltorender
withtraditionalsignalprocessingmethodsusinglinearand thecorrespondingRGBanddepthimagesfornovelviews.
nearest-neighborinterpolationonthetrainingdata. Toim- WetestinferencetimeonthesameNVIDIAA100GPUfor
provethestorageefficiency,wealsoapplyaudioencoding allthemethodstoensurefaircomparison.
6
lacissalC
larueNOpus-nearest NAF++ NACF INRAS++
Figure4.VisualizationofgeneratedRIRsfromdifferentmethods.Wevisualizetheground-truth(inblue)andpredicted(inred)impulse
responsesofseveralmethodsforqualitativecomparison.
Results. Weshowourquantitativeresultswith48Ksam- examples. Thetraininginvolvedadensepretrainingstage
plingrateRIRsinTable2. WefoundthatINRAS++,the onsimulateddata,followedbyfine-tuningusingreal-world
versionofINRASwithadecayloss,performsbestonmost exampleswithalearningrateof5 10−4.
×
metrics, has a lightweight architecture, and fast inference
Results. WepresentourresultsinFigure5andTable3.
speed. Opusaudioencodingwithnearest-neighborinterpola-
OurSim2Realmodeldemonstratessubstantialperformance
tionisonparwithseverallearning-basedmethods,suggest-
improvementsinfew-shotsetups,specificallywith0.3%,1%,
ingthedensedistributionofourcapturedRIRsthroughout
and5%ofthetotaltrainingsamples(approximately100,300,
thescenes.INRAS++andNAF++outperformtheirvanilla
and1500samples,respectively). Asthenumberoftraining
models by a large margin on the C , EDT, and T met-
50 60 samplesincreases,theadvantagesofusingsimulateddata
ricswhichindicatesthattheenergydecaylosshelpsmodels
fortrainingbecomesmaller. Whileoursim2realmodellags
learntheenergyattenuationsignificantlybetter. Weshow
behindthemodeltrainedwiththecompletedensedataset,
the qualitative result in Figure 4. We see that NACF and
whichexhibitsa55%improvementoverourmodeltrained
INRAS++cangenerateimpulseresponsesclosertoground
with5%ofthedata,it’sworthnotingthatweonlyuseabasic
truthwhileNAF++failsevendespitehavinggoodmetric
shoebox simulator. We believe this performance gap will
results. WealsovisualizeloudnessmapsinFigure6,where
weobtain3DoccupancygridsfromourvisualNeRFmodel
witharesolutionof0.1m. WecanseethatbothINRAS++ C50 Error (dB) EDT error (sec)
3.0 0.12
and NACF learn a continuous acoustic field. When com- NACF
paringwiththeacousticfieldsinfurnishedandunfurnished 2.5 INRAS++ 0.10
Sim2real INRAS++
rooms, wecanseethemodelshavesuccessfullycaptured 2.0 0.08
thephenomenonofsoundocclusion. SeeAppendixA.1for
0.06
1.5
moreresults.
0.04
1.0
5.2.EvaluationofFew-ShotRIRSynthesis 0.02
0.5
0.3 1 5 10 20 50 100 0.3 1 5 10 20 50 100
Wenextconductexperimentstoexplorehowmodelperfor-
T60 error (%) STFT error (dB)
mancevarieswithdifferenttrainingdatascales.Additionally, 30 0.600
0.575
webenchmarkourSim2Realmethodagainstotherbaselines 25
0.550
inchallengingfew-shotscenarios.
20 0.525
Experimentalsetup. Wetrainedeachmodelonthefur- 0.500
15 0.475
nishedroomusingvarioustrainingdatascales,rangingfrom
0.450
0.3%to100%,wheretheformercomprises 100samples 10
∼ 0.425
andthelatterhas31.3Ksamples. Topreventoverfitting,we 0.400
0.3 1 5 10 20 50 100 0.3 1 5 10 20 50 100
reserved10%ofthetrainingsamplesforearlystoppingat # of training data (%) # of training data (%)
eachscale. Foroursim2realmodel,wecreatedageometric-
Figure5. Few-shotRIRsynthesisresults. Weevaluatetheper-
basedPyroomacousticsScheibleretal.[51]shoeboxacous- formancesofmodelswithdifferentnumbersoftrainingdata.The
ticsimulatorforpretraining,usingtheparametersofroom resultsarereportedinthefurnishedroom.OurSim2Realmethod
boundingboxandaverageT calculatedfromreal-world canimprovetheperformanceincasesoflimitedtrainingdata.
60
7NAF++ NACF INRAS++
Furnished Room
Emitter
Unfurnished Room
Figure6. Loudnessmapvisualization. Givenanemitterpositionanditsorientation,wevisualizetheintensityofpredictedimpulse
responsesoverthespacesfromthetopviewandsideview,forthefurnishedandunfurnishedroom.Redmeansloudandbluemeansquiet.
Thearrowdenotesthespeaker’sorientation.
Table3.Few-shotexperimentswith1%trainingdata. Table5.Ablationexperimentsonspeakerorientation.
STFTErr C Err EDTErr T Err STFTErr C Err EDTErr T Err
Method 50 60 Method 50 60
(dB) (dB) (sec) (%) (dB) (dB) (sec) (%)
↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓
Simulator[51] 1.47 5.14 0.170 44.00 INRAS++w/oori. 0.42 0.64 0.018 6.57
NAF++ 0.69 2.38 0.072 21.77 INRAS++ 0.40 0.55 0.016 5.59
INRAS++ 0.53 2.42 0.098 25.15
NACF 0.50 2.30 0.067 22.87
sim2realINRAS++ 0.51 1.86 0.056 17.31
2Dbouncepointsshowsacomparableperformanceagainst
the audio-visual model NACF, suggesting that the audio
Table4. Ablationexperimentsonthebouncepointsampling
modalityalonesufficesforthecurrentsetup.
strategy.Weperformtheexperimentsonthefurnishedroom.
Speakerorientation. Weuseadirectionalspeakerduring
Method Modality Bncpoint STFT C 50 EDT T 60 ourdatacapture, exhibitingdirectivitypatternsthataffect
strategy (dB) (dB) (sec) (%)
↓ ↓ ↓ ↓ theacousticexperienceofreceiversWeexplorehowneural
INRAS++ 2D 0.42 0.57 0.017 6.21
A models use orientation information by removing speaker
NACF & 2D 0.40 0.58 0.017 6.04
INRAS++ A V 3D 0.41 0.53 0.016 5.84 orientationembeddingsfromtheinputs. Asdemonstrated
A
inTable5, thequalityofthegeneratedRIRssignificantly
improveswhenorientationinformationisincluded. Please
befurthernarroweddownifwecanapplymoreadvanced seeAppendixA.3forRIRvisualizationfordifferentorienta-
simulators,suchasChenetal.[7,10]. tions.
Energy decay loss. We study how model performance
5.3.AblationStudy
varieswiththeweightsoftheenergydecayloss. Weconduct
Bouncepointsampling. Weinvestigatetheimpactofour experimentsforINRAS++onthefurnishedroomandset
bouncepointsamplingstrategyonmodelperformance. To λto 0.1,0.2,0.3,0.5 . WepresentourresultsinTable6.
{ }
dothis,wecompareour3Dbouncepointsamplingmethod Itshowsthatincreasingtheweightsofdecaylossimproves
withtheoriginal2Dsamplingmethodatafixedheight. As metrics like C , EDT, and T errors, though it comes
50 60
showninTable4,our3Dbouncepointsamplingenhances withatradeoffintheSTFTerrormetric. Forourprimary
the model’s performance. Additionally, INRAS++ with experiments,wechooseλ=2.0forbalancedperformance.
8
weiv
pot
weiv
edis
weiv
pot
weiv
edisTable6.Ablationexperimentsontheenergydecayloss.
Srinivasan,andPeterHedman. Zip-NeRF:Anti-aliasedgrid-
λ
STFTErr C 50Err EDTErr T 60Err basedneuralradiancefields. InICCV,2023. 3
(dB) (dB) (sec) (%) [4] TobiasBertel,MingzeYuan,ReubenLindroos,andChristian
↓ ↓ ↓ ↓
1.0 0.39 0.58 0.017 5.98 Richardt. OmniPhotos:Casual360°VRphotography. ACM
2.0 0.40 0.55 0.016 5.59 Trans.Graph.,39(6):267:1–12,2020. 3
INRAS++
3.0 0.41 0.49 0.016 5.48 [5] Diego Di Carlo, Pinchas Tandeitnik, Cedric´ Foy, Nancy
5.0 0.43 0.49 0.015 5.34 Bertin,AntoineDeleforge,andSharonGannot. dechorate:
acalibratedroomimpulseresponsedatasetforecho-aware
signalprocessing. EURASIPJournalonAudio,Speech,and
6.Conclusion MusicProcessing,2021:1–15,2021. 1
[6] AngelChang,AngelaDai,ThomasFunkhouser,MaciejHal-
ThispaperintroducesRAF,amultimodalreal-worldacoustic
ber,MatthiasNießner,ManolisSavva,ShuranSong,Andy
roomdatasetcollectedforfacilitatingresearchonnovel-view Zeng,andYindaZhang. Matterport3D:LearningfromRGB-
acousticsynthesisandneuralacousticfieldmodelingtech- Ddatainindoorenvironments.In3DV,pages667–676,2017.
niques. RAFincludesdense3Droomimpulseresponsecap- 2
turesofalargespace,bothwithandwithoutfurniture.Italso [7] Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vi-
includevisualdatacapturedfrommultipleviewpointsand cencAmengualGari,ZiadAl-Halah,VamsiKrishnaIthapu,
precisetrackingofsoundsourcesandreceiversintheroom. PhilipRobinson,andKristenGrauman.SoundSpaces:Audio-
Wesystematicallyevaluatedexistingtechniquesforaudio visualnavigationin3Denvironments. InECCV,2020. 2,3,
8
and audio-visual novel-view acoustic synthesis using this
[8] Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vi-
real-worlddata. Weprovidedinsightsintotheperformance
cencAmengualGari,ZiadAl-Halah,VamsiKrishnaIthapu,
ofindividualmodelsandproposednewimprovements. Fur-
PhilipRobinson,andKristenGrauman.SoundSpaces:Audio-
thermore,weconductedexperimentstoinvestigatetheim-
visualnavigationin3Denvironments.InECCV,pages17–36.
pact of incorporating visual data (i.e., images and depth)
Springer,2020. 2,4
intoneuralacousticfieldmodels. Thisdatasetfillsagapin
[9] ChanganChen,RuohanGao,PaulCalamia,andKristenGrau-
existingresearchbyprovidingreal-worlddataforevaluating
man. Visual acoustic matching. In CVPR, pages 18858–
andbenchmarkingnovel-viewacousticsynthesismodelsand 18868,2022. 1,2
impulseresponsegenerationtechniques. Inthefuture,we [10] ChanganChen,CarlSchissler,SanchitGarg,PhilipKobernik,
plantoexpandthedatasettomoreroomconfigurations. AlexanderClegg,PaulCalamia,DhruvBatra,PhilipRobin-
son,andKristenGrauman. Soundspaces2.0: Asimulation
LimitationsandBroaderImpacts. Collectingreal-world
platform for visual-acoustic learning. In NeurIPS, pages
roomimpulsedataisexpensiveandtime-consuming,which
8896–8911,2022. 2,3,4,8
makesscalingupdatacollectionformultipleroomsorscenes
[11] Changan Chen, Alexander Richard, Roman Shapovalov,
challenging. OurdatasetonlyhaveRIRsdatafromasingle
VamsiKrishnaIthapu,NataliaNeverova,KristenGrauman,
physicalroom,althoughwithtwodifferentconfigurations.
andAndreaVedaldi.Novel-viewacousticsynthesis.InCVPR,
Thus,itsutilityislimitedforresearchaimingtogeneralize pages6409–6419,2023. 1,2,3
across different rooms and scenes. Using RIR data can [12] ChanganChen,WeiSun,DavidHarwath,andKristenGrau-
produceaudiorecordingsthatmimicrealrecordingsfrom man. Learning audio-visual dereverberation. In ICASSP,
a specific room. However, this capability can lead to the 2023. 2
creationofdeceptiveandmisleadingmedia. [13] MingfeiChen,KunSu,andEliShlizerman. Beeverywhere-
heareverything(BEE):Audioscenereconstructionbysparse
Acknowledgements. We would like to thank Jake San-
audio-visualsamples. InICCV,pages7853–7862,2023. 1,3
dakly,StevenKrenn,andToddKeeblerfortheirassistance
[14] Ziyang Chen, Shengyi Qian, and Andrew Owens. Sound
withdatacollection,aswellasLinningXuandVasuAgrawal
localizationfrommotion:Jointlylearningsounddirectionand
fortheirsupportwithVR-NeRFrendering. Ourgratitude camerarotation. InInternationalConferenceonComputer
alsoextendstoKunSun,MingfeiChen,SusanLiang,and Vision(ICCV),2023. 3
ChaoHuangfortheirinsightfuldiscussions. [15] Sanjoy Chowdhury, Sreyan Ghosh, Subhrajyoti Dasgupta,
AntonRatnarajah,UtkarshTyagi,andDineshManocha. Ad-
References Verb:Visuallyguidedaudiodereverberation. InICCV,pages
7884–7896,2023. 2
[1] Agisoft,LLC. Metashape2.0,2023. 4 [16] AlexandreDe´fossez,NeilZeghidour,NicolasUsunier,Le´on
[2] Byeongjoo Ahn, Karren Yang, Brian Hamilton, Jonathan Bottou,andFrancisBach.Sing:Symbol-to-instrumentneural
Sheaffer,AnuragRanjan,MiguelSarabia,OncelTuzel,and generator. InNeurIPS,2018. 6
Jen-HaoRickChang.Novel-viewacousticsynthesisfrom3D [17] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi
reconstructedrooms. arXiv:2310.15130,2023. 1,3 Fei-Fei.ImageNet:Alarge-scalehierarchicalimagedatabase.
[3] JonathanT.Barron, BenMildenhall, DorVerbin, PratulP. InCVPR,pages248–255,2009. 12
9[18] AngeloFarina. Simultaneousmeasurementofimpulsere- Lovegrove,MichaelGoesele,RichardNewcombe,etal. Neu-
sponseanddistortionwithaswept-sinetechnique. InAudio ral3dvideosynthesisfrommulti-viewvideo.InCVPR,pages
engineeringsocietyconvention108.AudioEngineeringSoci- 5521–5531,2022. 1
ety,2000. 12 [33] Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar,
[19] Nail A Gumerov and Ramani Duraiswami. A broadband and Chenliang Xu. AV-NeRF: Learning neural fields for
fastmultipoleacceleratedboundaryelementmethodforthe real-world audio-visual scene synthesis. arXiv preprint
threedimensionalhelmholtzequation. TheJournalofthe arXiv:2302.02088,2023. 1,2,3,5,6,12,13
AcousticalSocietyofAmerica,125(1):191–205,2009. 2 [34] Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar,
[20] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, andChenliangXu. Neuralacousticcontextfield:Rendering
GuofengZhang,HujunBao,andXiaoweiZhou. Neural3D realistic room impulse response with neural fields. arXiv
scenereconstructionwiththeManhattan-worldassumption. preprintarXiv:2309.15977,2023. 1,2,5,6,13
InCVPR,2022. 3 [35] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
[21] EliorHadad,FlorianHeese,PeterVary,andSharonGannot. regularization. arXivpreprintarXiv:1711.05101,2017. 6
Multichannelaudiodatabaseinvariousacousticenvironments. [36] AndrewLuo,YilunDu,MichaelTarr,JoshTenenbaum,An-
InInternationalWorkshoponAcousticSignalEnhancement tonioTorralba,andChuangGan. Learningneuralacoustic
(IWAENC),pages313–317,2014. 1 fields. InNeurIPS,pages3165–3177,2022. 1,2,4,6,12,13
[22] Dorte Hammershøi and Henrik Møller. Binaural tech- [37] SagnikMajumder,ChanganChen,ZiadAl-Halah,andKris-
nique—basic methods for recording, synthesis, and repro- tenGrauman. Few-shotaudio-visuallearningofenvironment
duction. CommunicationAcoustics,pages223–254,2005. acoustics. InNeurIPS,pages2522–2536,2022. 1,2,3,5
1 [38] RicardoMartin-Brualla,NohaRadwan,MehdiSMSajjadi,
[23] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. JonathanTBarron,AlexeyDosovitskiy,andDanielDuck-
Deepresiduallearningforimagerecognition.InCVPR,pages worth. NeRFinthewild: Neuralradiancefieldsforuncon-
770–778,2016. 12 strainedphotocollections. InCVPR,pages7210–7219,2021.
[24] PeterHedman,TobiasRitschel,GeorgeDrettakis,andGabriel 1
Brostow. Scalableinside-outimage-basedrendering. ACM [39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Trans.Graph.,35(6):231:1–11,2016. 3 JonathanT.Barron,RaviRamamoorthi,andRenNg. NeRF:
[25] HyeonjoongJang,Andre´asMeuleman,DahyunKang,Dong- Representingscenesasneuralradiancefieldsforviewsynthe-
gunKim,ChristianRichardt,andMinH.Kim. Egocentric sis. InECCV,2020. 3
scenereconstructionfromanomnidirectionalvideo. ACM [40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Trans.Graph.,41(4):100:1–12,2022. 3 JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
[26] MarcoJeub, MagnusSchafer, andPeterVary. Abinaural Representingscenesasneuralradiancefieldsforviewsyn-
roomimpulseresponsedatabasefortheevaluationofderever- thesis. CommunicationsoftheACM,65(1):99–106, 2021.
berationalgorithms. InInternationalConferenceonDigital 1
SignalProcessing,2009. 1,2 [41] ZakMurez,TarrencevanAs,JamesBartolozzi,AyanSinha,
[27] Diederik Kingma and Jimmy Ba. Adam: A method for VijayBadrinarayanan,andAndrewRabinovich. Atlas:End-
stochastic optimization. In International Conference on to-end3Dscenereconstructionfromposedimages.InECCV,
LearningRepresentation,2015. 6 2020. 3
[28] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L [42] SatoshiNakamura,KazuoHiyane,FutoshiAsano,Takanobu
Seltzer,andSanjeevKhudanpur. Astudyondataaugmenta- Nishiura,andTakeshiYamada. Acousticalsounddatabasein
tionofreverberantspeechforrobustspeechrecognition. In realenvironmentsforsoundsceneunderstandingandhands-
ICASSP,pages5220–5224,2017. 1 free speech recognition. In LREC, pages 965–968, 2000.
[29] ShoichiKoyama,TomoyaNishida,KeisukeKimura,Takumi 1
Abe,NatsukiUeno,andJesperBrunnstro¨m. MeshRIR:A [43] RichardA.Newcombe,AndrewJ.Davison,ShahramIzadi,
dataset of room impulse responses on meshed grid points Pushmeet Kohli, Otmar Hilliges, Jamie Shotton, David
for evaluating sound field analysis and synthesis methods. Molyneaux,SteveHodges,DavidKim,andAndrewFitzgib-
InIEEEWorkshoponApplicationsofSignalProcessingto bon. KinectFusion: Real-timedensesurfacemappingand
AudioandAcoustics(WASPAA),2021. 1,2,3,4 tracking. InISMAR,pages127–136,2011. 3
[30] JonathanLeRouxandEmmanuelVincent. Acategorization [44] Matthias Nießner, Michael Zollho¨fer, Shahram Izadi, and
ofrobustspeechprocessingdatasets. 2014. 2 MarcStamminger.Real-time3Dreconstructionatscaleusing
[31] JonathanLeRoux,EmmanuelVincent,JohnRHershey,and voxelhashing. ACMTrans.Graph.,32(6):169:1–11,2013. 3
DanielPWEllis. Micbots:collectinglargerealisticdatasets [45] RyanStylesOverbeck,DanielErickson,DanielEvangelakos,
forspeechandaudioresearchusingmobilerobots. In2015 MattPharr,andPaulDebevec. Asystemforacquiring,com-
IEEE International Conference on Acoustics, Speech and pressing,andrenderingpanoramiclightfieldstillsforvirtual
SignalProcessing(ICASSP),pages5635–5639.IEEE,2015. reality. ACMTrans.Graph.,37(6):197:1–15,2018. 3
2 [46] SanthoshK.Ramakrishnan,AaronGokaslan,ErikWijmans,
[32] TianyeLi,MiraSlavcheva,MichaelZollhoefer,SimonGreen, OleksandrMaksymets, AlexClegg, JohnTurner, EricUn-
Christoph Lassner, Changil Kim, Tanner Schmidt, Steven dersander, Wojciech Galuba, Andrew Westbury, Angel X.
10Chang,ManolisSavva,YiliZhao,andDhruvBatra. Habitat- [61] Lonny L Thompson. A review of finite-element methods
Matterport3Ddataset(HM3D):1000large-scale3Denviron- fortime-harmonicacoustics. TheJournaloftheAcoustical
mentsforembodiedAI. arXiv:2109.08238,2021. 2 SocietyofAmerica,119(3):1315–1330,2006. 2
[47] AntonRatnarajah,ZhenyuTang,RohithAralikatti,andDi- [62] ChristopheVeaux,JunichiYamagishi,andSimonKing. The
neshManocha. MESH2IR:Neuralacousticimpulseresponse voicebankcorpus:Design,collectionanddataanalysisofa
generator for complex 3D scenes. In Proceedings of the largeregionalaccentspeechdatabase. In2013international
30thACMInternationalConferenceonMultimedia,pages conferenceorientalCOCOSDAheldjointlywith2013con-
924–933,2022. 2 ferenceonAsianspokenlanguageresearchandevaluation
[48] AntonRatnarajah,Shi-XiongZhang,MengYu,ZhenyuTang, (O-COCOSDA/CASLRE),pages1–4.IEEE,2013. 3
DineshManocha,andDongYu. Fast-rir:Fastneuraldiffuse [63] MasonWang,SamuelClarke,Jui-HsienWang,RuohanGao,
roomimpulseresponsegenerator.InICASSP,pages571–575, and Jiajun Wu. Soundcam: A dataset for finding humans
2022. 2 usingroomacoustics.arXivpreprintarXiv:2311.03517,2023.
[49] Anton Ratnarajah, Sreyan Ghosh, Sonal Kumar, Purva 2
Chiniya,andDineshManocha.Av-rir:Audio-visualroomim- [64] Xiuchao Wu, Jiamin Xu, Zihan Zhu, Hujun Bao, Qixing
pulseresponseestimation. arXivpreprintarXiv:2312.00834, Huang, JamesTompkin, andWeiweiXu. Scalableneural
2023. 2 indoorscenerendering. ACMTrans.Graph.,41(4):98:1–16,
[50] ChristianRichardt,JamesTompkin,andGordonWetzstein. 2022. 3
Capture,reconstruction,andrepresentationofthevisualreal [65] FeiXia,AmirRZamir,ZhiyangHe,AlexanderSax,Jitendra
world for virtual reality. In Real VR – Immersive Digital Malik,andSilvioSavarese. Gibsonenv:Real-worldpercep-
Reality:HowtoImporttheRealWorldintoHead-Mounted tionforembodiedagents. InCVPR,pages9068–9079,2018.
ImmersiveDisplays,pages3–32.Springer,2020. 3 2
[51] RobinScheibler,EricBezzam,andIvanDokmanic´.Pyrooma- [66] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia,
coustics: Apythonpackageforaudioroomsimulationand AayushBansal,ChangilKim,SamuelRotaBulo`,Lorenzo
array processing algorithms. In ICASSP, pages 351–355, Porzi,PeterKontschieder,AljazˇBozˇicˇ,DahuaLin,Michael
2018. 7,8 Zollho¨fer,andChristianRichardt. VR-NeRF:High-fidelity
[52] CarlSchisslerandDineshManocha. Interactivesoundprop- virtualizedwalkablespaces. InSIGGRAPHAsiaConference
agationandrenderingforlargemulti-sourcescenes. ACM Proceedings,2023. 2,3,4,12
TransactionsonGraphics(TOG),36(4):1,2016. 2 [67] KarmeshYadav,RamRamrakhya,SanthoshKumarRamakr-
[53] Heung-YeungShum,Shing-ChowChan,andSingBingKang. ishnan, TheoGervet, JohnTurner, AaronGokaslan, Noah
Image-BasedRendering. Springer,2007. 3 Maestre,AngelXuanChang,DhruvBatra,ManolisSavva,
[54] Nikhil Singh, Jeff Mentch, Jerry Ng, Matthew Beveridge, etal. Habitat-Matterport3Dsemanticsdataset. InCVPR,
andIddoDrori. Image2reverb:Cross-modalreverbimpulse pages4927–4936,2023. 2
responsesynthesis. InICCV,pages286–295,2021. 1,2 [68] RyuichiYamamoto,EunwooSong,andJae-MinKim. Par-
[55] Arjun Somayazulu, Changan Chen, and Kristen Grauman. allelwavegan:Afastwaveformgenerationmodelbasedon
Self-supervised visual acoustic matching. arXiv preprint generativeadversarialnetworkswithmulti-resolutionspectro-
arXiv:2307.15064,2023. 1,2 gram. InICASSP2020-2020IEEEInternationalConference
onAcoustics,SpeechandSignalProcessing(ICASSP),pages
[56] JulianStraub,ThomasWhelan,LingniMa,YufanChen,Erik
6199–6203.IEEE,2020. 5
Wijmans,SimonGreen,JakobJEngel,RaulMur-Artal,Carl
Ren, Shobhit Verma, et al. The replica dataset: A digital [69] ShengYang,BeichenLi,Yan-PeiCao,HongboFu,Yu-Kun
replicaofindoorspaces. arXivpreprintarXiv:1906.05797, Lai,LeifKobbelt,andShi-MinHu. Noise-resilientrecon-
2019. 2 structionofpanoramasand3Dscenesusingrobot-mounted
unsynchronizedcommodityRGB-Dcameras. ACMTrans.
[57] KunSu,MingfeiChen,andEliShlizerman. INRAS:Implicit
Graph.,2020. 3
neuralrepresentationforaudioscenes. InNeurIPS,2022. 1,
[70] WenZhang,ParasangaNSamarasinghe,HanchiChen,and
2,4,6,13
ThusharaDAbhayapala. Surroundbysound: Areviewof
[58] EdgarSucar,ShikunLiu,JosephOrtiz,andAndrewJ.Davi-
spatialaudiorecordingandreproduction. AppliedSciences,7
son. iMAP:Implicitmappingandpositioninginreal-time. In
(5):532,2017. 1
ICCV,2021. 3
[59] JiamingSun,YimingXie,LinghaoChen,XiaoweiZhou,and
HujunBao. NeuralRecon:Real-timecoherent3Dreconstruc-
tionfrommonocularvideo. InCVPR,2021. 3
[60] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srini-
vasan, Edgar Tretschk, Yifan Wang, Christoph Lassner,
Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lom-
bardi,TomasSimon,ChristianTheobalt,MatthiasNiessner,
JonathanT.Barron, GordonWetzstein, MichaelZollho¨fer,
andVladislavGolyanik. Advancesinneuralrendering. Com-
put.Graph.Forum,41(2):703–735,2022. 3
11A.1.AdditionalExperimentalResults NAF. We follow the official implementation of NAF 2,
and create 3D grid features based on the bounding boxes
Benchmarkon16KHzimpulseresponses. Wealsoeval-
ofscenes. Forexperimentswith16kHzsamplingrate,we
uateeachmethodonourbenchmarkwithimpulseresponses
useanSTFTwithanFFTsizeof512,awindowlengthof
of 16kHz sampling rate. We show the results in Table 7.
256,andahoplengthof128. For48kHzsamplingrate,we
We can see that INRAS++ performs best overall, which
use an STFT with an FFT size of 1024, a window length
matcheswiththeconclusioninSection5.1.
of512,andahoplengthof256. WeperforminverseSTFT
on the predicted magnitude of the RIR spectrogram with
Morequalitativeresults. WeprovidemorepredictedRIR
randomspectrogramphasetoobtaintime-domainRIR.We
visualizationforqualitativecomparisoninFigure9. Wealso
setλ=1.0fortheweightofenergydecaylosswhentraining
provide more loudness map visualization on the different
NAF++.
scenesforqualitativecomparisoninFigure10.
INRAS. We follow the implementation of INRAS pro-
Empty versus furnished room. One advantage of our videdbytheauthorsintheirsupplementarymaterial3,and
datasetisthatitcontainsasceneintwoconditions—empty
addanextradimensionfortheemitter,listener,andbounce
andfurnished,whichallowsstudyingthedifferenceinacous-
pointposition. Wechangedtheoriginalbouncepointsam-
ticfieldsintroducedbyfurniture. Duetoalackofground-
pling method, which only sampled points with a specific
truth comparison, we visualize the generated impulse re-
height. Instead, we apply Poisson sampling on the scene
sponsesfromINRAS++trainedoneachsceneindividually
meshestoobtain256bouncepointsin3Dtorepresentscene
asanapproximationoftheacousticfield. Weshowourre-
geometry in a better way. To optimize multi-resolution
sultsinFigure7,wherewecanseethatgeneratedimpulse STFT loss, we set FFT size as 128,512,1024,2048 ,
responseswithdifferentacousticproperties. windowlengthas 80,240,600,12{ 00 , andhoplengtha} s
{ }
16,50,120,240 . We set λ = 2.0 for the weight of the
{ }
A.2.ImplementationDetails energydecayloss.
NACF. WeusethesamearchitectureasINRASforNACF.
Inthissection,wewilldemonstratetheimplementationof
Wekeeptheoriginalbouncepointsamplingstrategyinthe
eachbaselineindetail.
paperandrendervisualcontextusingVR-NeRF[66]. We
AAC and Opus. We convert the raw waveform (.wav) render256 256pixelRGB,anddepthimageswithafield
×
intoAAC(.m4a)andOpus(.opus)encodingandreverse ofviewof90°. Weusethesurfacenormalofeachbounce
thecompressionusingFFmpegcommandsasshownbelow: point to determine the look-at view of the virtual camera.
Following the original paper, RGB and depth images are
1 # AAC compression down-sampledto16 16andareencodedwithanMLPas
2 encode_command = f"ffmpeg -i audio.wav -t { visual contexts. We× set λ = 2.0 for the weight of energy
audio_length} -c:a aac -b:a 24k temp.m4a"
3 decode_command = f"ffmpeg -i temp.m4a -c:a decayloss. Weoptimizethemulti-resolutionSTFTlosswith
pcm_f32le -ar {sampling_rate} audio_aac.wav" thesamehyperparametersasINRAS.
4
5 # Opus compression AV-NeRF. Because we have a different setup from AV-
6 encode_command = f"ffmpeg -i audio.wav -t { NeRF[33]wherewehaveomnidirectionalmicrophonesin-
audio_length} -c:a opus -strict -2 -b:a 24k steadoforientatedbinauralreceivers,weadopttheirmethod
temp.opus"
with several changes. We use VR-NeRF [66] to render 4
7 decode_command = f"ffmpeg -i temp.opus -c:a
pcm_f32le -ar {sampling_rate} audio_opus.wav" perspectiveviewsof256 256RGBanddepthmapswitha
×
fieldofviewof90°foreachreceiver’sposition,andencode
Listing1.FFmpegcommandsforaudiocompression
themwithfrozenResNet18[23]trainedonImageNet[17].
We removed the relative angle because it does not fit our
Wecuttheaudiotobethesamelength(0.32s)andcor-
setup. Wesetλ=2.0fortheweightofenergydecayloss.
respondingsamplingrate(16Kor48K)forfairevaluation
comparison. A.3.Dataset
Note that we use a different Opus encoder which can
achievebettercompressionperformancethanNAFused[36]. Impulseresponsedataprocessing. Wefollowedthesine-
Due to the heavy computation of constructing a high- sweepdeconvolutionprocessasdescribedbyFarina[18]to
dimensionalinterpolationengine, wemodifythebaseline extracttheimpulseresponsefromthesignalsrecordedbythe
algorithmbyfirstmatchingthenearestneighboroftheemit- microphones.Foreachextractedimpulseresponse,wesaved
terinthetrainingdistributionandthenperformingthenear- the3Dlocationofthereceiver,aswellasthe3Dlocationand
estneighbororlinearinterpolationtogenerateimpulsere- 2https://github.com/aluo-x/LearningNeuralAcousticFields/
sponsesforgivenlistenerpositions. 3https://openreview.net/forum?id=7KBzV5IL7W
12Table7.Benchmarkwith16kHzsamplingrate.
STFTerror C error EDTerror T error Parameters Storage Speed
Method Variation 50 60
(dB) (dB) (sec) (%) (Million) (MB) (ms)
↓ ↓ ↓ ↓ ↓ ↓ ↓
AAC 1.14 1.09 0.040 8.79 680.45
Linear Opus 1.06 0.80 0.032 7.48 – 680.45 –
original 1.02 0.82 0.032 6.82 3,172.77
AAC 0.72 0.83 0.027 8.08 680.45
Nearest Opus 0.58 0.61 0.020 6.96 – 680.45 –
original 0.48 0.71 0.020 7.68 3,172.77
vanilla 0.77 0.69 0.025 8.15
NAF[36] 5.51 22.04 5.57
+decayloss 0.77 0.63 0.023 7.43
vanilla 0.44 0.65 0.024 6.15
INRAS[57] 1.33 5.31 2.10
+decayloss 0.45 0.54 0.019 5.34
vanilla 0.45 0.58 0.020 5.47 1.52 6.05 2.39
NACF[34]
+temporal 0.48 0.60 0.022 6.59 1.75 7.00 2.78
AV-NeRF[33] vanilla 0.46 0.58 0.021 6.12 12.99 51.98 5.80
Figure7.VisualizationcomparisonofgeneratedRIRsfromdifferentscenes.Wepresentvisualizationsoffourpairsofgeneratedimpulse
responses,eachsharingthesameemitter-receiverpositioninboththeemptyroomandthefurnishedroom.Thesevisualizationshighlight
thevariationsintheacousticfieldsbetweenthetwodistinctscenes.
orientationofthesoundsource. Thelengthoftheimpulse
response is 4 seconds and all audio data was recorded at
asamplingrateof48kHzandstoredataresolutionof32
bits. WeshowtheRT60distributionofourcollectedRIRs
inFigure8
Visualrendering. Weproviderenderingsofroommeshes
asasimpleoverviewinFigure11.
Speakerorientation. InFigure12,weprovidevisualiza-
tionsofimpulseresponsepairsfromourcaptureddataset.
Thesepairssharethesameemitter-listenerpositionbutdif-
ferinemitterorientations. Theorientationsofdirectional
speakersimpacttheresultingimpulseresponses.
Empty room Furnished room
3000
2000
1000
0
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
RT60 (s) RT60 (s)
Figure8.RT60distribution.
13
ytpmE
dehsinruF
rebmun
#
lacissalC
larueNFigure9.VisualizationofgeneratedRIRs.Wevisualizethegroundtruth(inblue)andpredicted(inred)impulseresponsesofseveral
methodsforqualitativecomparison.
NAF++ NACF INRAS++
Emitter
Figure10.Loudnessmapvisualization.Wevisualizetheintensityofpredictedimpulseresponsesoverthespacesfromthetopviewand
sideviewgivenanemitterpositionanditsorientation.Redmeansloudandbluemeansquiet.
14
++FAN
FCAN
FReN-VA
++SARNI
weiv
pot
weiv
edis
weiv
pot
weiv
edisFurnished Room Unfurnished Room
Figure11.SceneoverviewofRAF.
Figure12.Visualizationofground-truthRIRswithdifferentorientations.
15