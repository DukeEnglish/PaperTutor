Benchmarking Object Detectors with COCO:
A New Path Forward
Shweta Singh1⋆ Aayan Yadav1⋆ Jitesh Jain2
Humphrey Shi2 Justin Johnson3 Karan Desai3
1IITRoorkee 2GeorgiaTech 3UniversityofMichigan
Abstract
The Common Objects in Context (COCO) dataset has been instrumental in benchmarking
object detectors over the past decade. Like every dataset, COCO contains subtle errors and
imperfections stemming from its annotation procedure. With the advent of high-performing
models,weaskwhethertheseerrorsofCOCOarehinderingitsutilityinreliablybenchmarking
furtherprogress. Insearchforananswer,weinspectthousandsofmasksfromCOCO(2017ver-
sion)anduncoverdifferenttypesoferrorssuchasimprecisemaskboundaries,non-exhaustively
annotatedinstances,andmislabeledmasks. DuetotheprevalenceofCOCO,wechoosetocor-
rect these errors to maintain continuity with prior research. We develop COCO-ReM (Refined
Masks),acleanersetofannotationswithvisiblybettermaskqualitythanCOCO-2017. Weeval-
uatefiftyobjectdetectorsandfindthatmodelsthatpredictvisuallysharpermasksscorehigher
on COCO-ReM, affirming that they were being incorrectly penalized due to errors in COCO-
2017. Moreover, our models trained using COCO-ReM converge faster and score higher than
their larger variants trained using COCO-2017, highlighting the importance of data quality in
improvingobjectdetectors. Withthesefindings,weadvocateusingCOCO-ReMforfutureobject
detectionresearch. Ourdatasetisavailableathttps://cocorem.xyz
1 Introduction
Rigorousbenchmarkingofcomputervisionmodelsreliesontask-specificdatasetsannotatedbyhu-
mans [9, 10, 24, 27]. Understandably, human-annotated data is prone to errors and imperfections
due to factors such as ambiguous instructions to annotators, differences in perspective of multiple
annotators, etc. Errors in ground-truth training data provide a noisy learning signal for models,
resulting in sub-par capabilities. Even more problematic is the presence of errors in ground-truth
evaluationdata,whichcancauseamisalignmentbetweenevaluationmetricsandhumanjudgment
when comparing model performances. In this work, we comprehensively study the extent of this
issue for the canonical computer vision task of object detection and segmentation.
The Common Objects in Context (COCO) dataset [27] has been the standard benchmark for ob-
ject detection and segmentation since its inception in 2014. COCO fostered the empirical study
of modern object detectors by providing a large set of 123K images annotated with 896K instance
⋆Denotesequalcontribution. Correspondenceto: KaranDesai(kdexd@umich.edu)
1
4202
raM
72
]VC.sc[
1v91881.3042:viXraCOCO-2017 COCO-ReM (ours)
Dining Table Dining Table Banana Dining Table Dining Table Banana
Apple Teddy Bear Clock Couch Vase
Scissors Dining Table Dog
Figure 1: COCO with Refined Masks (COCO-ReM). Top: COCO-ReM improves upon the quality
of COCO-2017 masks by handling occlusions consistently (dining table) and annotating instances
exhaustively (banana). Bottom: Random examples from COCO-ReM showing the mask quality
(single category per image, for clarity).
masks. TheMaskAP(averageprecision)ofCOCOdetectorshasimproveddramatically,with≈70%
relative gain between the pioneering Mask R-CNN [18, 19] from 2017, and recent models pub-
lished in 2022–23 such as ViTDet [3, 25] and other Transformer-based models [4, 11, 35] like
Mask2Former [8] and OneFormer [21].
Despite its popularity, COCO is not without limitations. COCO masks are known to have coarse
boundaries[1,16]. WemanuallyinspectthousandsofmasksinCOCO(2017version)andobserve
otherimperfections. AsshowninFigure1(top),COCO-2017masksdonothandleholesandocclu-
sionsproperly(diningtable), andsometimeshavenon-exhaustivelyannotatedinstances(banana).
We elaborate on these flaws in Section 2.
From an evaluation perspective, imperfections in ground-truth data make the metrics less reli-
able. COCO-2017 AP would wrongfully penalize models that predict more precise masks than the
imperfect ground-truth masks. Moreover, models trained with such imperfect masks may exploit
unwanted biases from the training data, e.g., learning to never predict masks with holes. Recently,
Kirillov et al. [23] observed that human evaluators consistently rated the masks predicted by their
SegmentAnythingModel(SAM)tobeofhigherqualitythanaViTDetmodel[25]trainedonCOCO,
whereas the latter scored higher on COCO AP.
2Theseobservationsraiseanimportantquestion–canCOCOreliablybenchmarkthefutureprogressin
object detection research? COCO is the de-facto benchmark for object detection – countless research
papers report COCO results every year; libraries like Torchvision [31] and Detectron2 [38] readily
supportobjectdetectordevelopmentwithCOCO.Hence,tomaintaincontinuitywithpriorresearch
andtapintothisrichopen-sourceecosystem,wechoosetorectifytheerrorsofCOCOandreinforce
its utility for future research.
Contributions. We develop COCO-ReM (Refined Masks), a cleaner set of high-quality instance
annotations for COCO images. The improved quality over COCO-2017 masks is visually palpable,
as seen in Figure 1.
Ourannotationpipeline(Section3)systematicallyrectifiestheimperfectionsofCOCO-2017masks.
We refine the mask boundaries using SAM [23], and exhaustively annotate instances by importing
from the LVIS dataset [16] and an ensemble of LVIS-trained models. We (the authors) manually
verify the entire validation set to provide a strong quality guarantee for evaluation.
In Section 4, we use COCO-ReM for benchmarking object detectors. We evaluate fifty object de-
tectorsandobservethattheirCOCO-ReMAPproducesadifferentmodelrankingthanCOCO-2017
AP. Surprisingly, we observe that query-based models (Mask2Former and OneFormer) score much
higher on COCO-ReM than region-based models (ViTDet). Query-based models predict visually
sharper masks – AP trends on COCO-ReM accurately reflect this, unlike COCO-2017. We observe
that models trained using COCO-ReM converge faster and perform better than those trained using
COCO-2017, highlighting the importance of mask quality in improving object detectors.
2 Background: Revisiting COCO Masks
The annotation pipeline of a dataset determines the quality of annotations obtained. To design an
effectiveannotationpipelineforCOCO-ReM,wereviewtheCOCOannotationpipelinebyLinetal.
[27] and understand its shortcomings.
COCO annotation pipeline. Lin et al. [27] employed a three-stage pipeline to collect instance
mask annotations for COCO images:
1. Category labeling: Annotators assigned one or more labels to images depending on the visible
content, choosing from a fixed set of categories.
2. Instance spotting: Annotators locate up to ten instances per label (from the previous stage) in
the corresponding image, by providing clicks (points).
3. Instance segmentation: Finally, annotators clicked around the boundaries of each located
instance (from the previous stage), rendering a polygon mask.
Lin et al. [27] also took numerous measures to improve annotation quality. The first two stages –
categorylabelingandinstancespotting–wereperformedbyeightannotatorsperimagetomaximize
recall. The instance segmentation stage required annotators to complete a training task before
annotating actual instances in the dataset. Next, a different set of crowd workers verified the
correctness and quality of annotated masks. Finally, the annotators used a paintbrush tool to mark
remaining objects as crowd regions. These extensive measures avoided many scenarios that could
have led to low-quality masks.
3Scissors Cup TV TV
(a) Holes (b) Inconsistency in occlusion
Apple Donut Banana Dog
(c) Missing annotations and labeling multiple instances as one
Figure 2: Imperfections in COCO-2017 masks. All masks in general have coarse boundaries.
Moreover, they lack holes when required (scissors) and do not handle occlusions consistently (TV
with or without figurines). Annotations are sometimes non-exhaustive, either masks are missing
(apples and donuts), or multiple instances are grouped into a single mask (banana and dog).
Imperfections in COCO-2017 masks. To further understand the flaws in COCO annotations,
we visualized thousands of COCO-2017 masks and inspected their quality and label correctness.
Through this exercise, we observed recurring types of errors in annotations, as shown in Figure 2.
1. COCO-2017maskshavecoarseboundaries. Thisisanaturalconsequenceofcollectingmasks
aspolygoncontours,whichonlycoarselyapproximatethepixel-precisemaskboundariesthrough
piece-wisestraightedgesconnectingpoints(clicks)suppliedbyannotators. Thisshortcomingis
well-known and identified by several follow-up works [1, 16, 23].
2. COCO-2017 masks lack holes when required. The polygon-drawing interface of COCO did
not allow annotators to erase the interior regions of polygons. Consequently, certain objects
that should naturally contain holes, lack them. Figure 2a shows two examples – the handles of
scissors and cup. While there should be holes in the masks, COCO-2017 does not include them.
3. COCO-2017 masks do not handle occlusions consistently. Masks of occluded objects are
either drawn over the other occluding objects (amodal masks) or around them (modal masks).
For example, observe the TV masks in Figure 2b. While both, amodal and modal masks are
valid,eitheroneshouldbeconsistentinthedatasetaccordingtothetaskdefinition. Webelieve
this inconsistency is a consequence of inadequate task instructions to annotators as well as the
lack of holes in masks.
4. COCO-2017 masks are often non-exhaustive. This issue occurs in two modes, as shown in
Figure 2c – either few instance masks of a category are missing1 (apple, donut), or multiple
instances are grouped in a single mask (banana, dog). This is a recurring issue throughout
COCO–webelievethatcertainannotatorsmaynothavedeemedmaskswithgroupedinstances
to have a wrong object contour, due to oversight.
1Theseimageshavealltheirmissingmasksannotatedasnon-crowdregions.
45. COCO-2017 contains few near-duplicate masks. We found 410 pairs of masks that overlap
with intersection-over-union (IoU) greater than 0.8 in the COCO-2017 validation set (≈2.3%
of all instances). These pairs often segment the same underlying object with different labels.
Examplesincludearmchairs(chairandcouch),calves(cowandsheep),andduffelbags(suitcase
and backpack). We attribute this issue to the lack of a cross-category de-duplication step in the
COCO annotation pipeline.
All these imperfections hurt the reliability of the benchmarking process. They provide noisy super-
visionduringtrainingandwrongfullypenalizecorrectmodelpredictionsduringevaluation. Having
characterized these imperfections, we make targeted design choices in our COCO-ReM annotation
pipeline to rectify them.
3 COCO-ReM: COCO with Refined Masks
In this section, we outline our annotation process for COCO-ReM. Our goal is to rectify the im-
perfections in COCO-2017 annotations as discussed in Section 2 to maintain continuity with prior
research. We hope that higher quality ground-truth masks would improve the reliability of the
benchmarking process.
3.1 Annotation Pipeline
We develop a semi-automatic pipeline – first, we refine the masks using off-the-shelf models and
LVIS dataset [16], followed by manual verification of the updated masks. We refine both, the
trainingandvalidationsetofCOCO-2017,comprising≈860Kand≈36Kinstancesrespectively. We
thoroughly verify the validation set to ensure consistency and pixel-precise mask quality for model
evaluation. As the training set is much larger, it is only processed through the automatic steps of
our pipeline. Our pipeline comprises three stages (Figure 3).
Stage 1. Mask boundary refinement. In this stage, we aim to rectify the coarse boundary qual-
ity of COCO-2017 masks. A straightforward solution to achieve this is to re-annotate all masks
through a web interface with paintbrush and eraser tools instead of drawing polygon contours.
Moreover,thissolutionnaturallyallowsincludingholesinmaskswherenecessary. Whileaccurate,
this solution would sharply increase the mask annotation time and cost.
Along a different trend, prior works have explored model-assisted interactive segmentation as a
feasible solution for collecting mask annotations with smooth boundaries [1, 16]. The key idea is
toletanobjectdetectorpredictacoarseinitialmaskandhavetheannotatorsrefineitbyprompting
corrective mouse clicks. Inspired by the accuracy and practical feasibility of this approach, we use
the Segment Anything Model (SAM [23]) to refine the COCO-2017 masks.
– RefiningmasksusingSAM.SAMisapromptablesegmentationmodelthatpredictshigh-quality
masks conditioned on user-provided point and box prompts. This promptable functionality
makesSAMsuitableforourusecase. WepromptedSAMusingvariouscombinationsofprompts
sampledusingCOCO-2017masksandobtaineditspredictionsastherefinedmasks. Refertothe
appendix for details about this prompting methodology.
– Manualverificationofmaskquality. Wevisualizedandmanuallyinspectedeverymaskcreated
in the previous process. Through this inspection, we identified nearly 900 masks that have low
5Stage 1: Mask Boundary Refinement Stage 2: Exhaustive Instance Annotation Stage 3: Correction of Labeling Errors
Refining masks using SAM (automatic) Importing masks from LVIS (automatic) Removing duplicate masks and marking
and verifying mask quality (manual) and LVIS-trained detectors (manual) grouped instances as crowd (manual)
COCO-2017 COCO-ReM COCO-2017 COCO-ReM COCO-2017 COCO-ReM
Sink Banana Truck
Mask
removed
Dining Table Donut Car
Duplicate
masks
Figure 3: COCO-ReM annotation pipeline. We develop a semi-automatic annotation pipeline
comprising three stages to rectify the imperfections in COCO-2017 masks.
quality or do not accurately cover the underlying object. These masks represent only 2.4% of
36KinstancesintheCOCO-2017validationset,greatlyreducingourmanualeffortandshowing
theefficacyofourautomatedprocess. Tocorrectthesemasks,wedeployedawebbrowserdemo
of SAM and provided point prompts iteratively until obtaining a precise and accurate mask.
Through this stage, we obtain masks with more precise boundaries and better handling of occlu-
sions, as illustrated in Figure 3 (left).
Stage 2. Exhaustive instance annotation. This stage is aimed at alleviating the issue of non-
exhaustivelyannotatedinstancesinCOCO-2017(Figure2c). Wenotethatmanuallyre-annotating
every image is the most reliable solution to ensure exhaustive instance coverage. However, the
manual effort would not only require extensive instructions and verification but would also be
highly cost-ineffective and may lead to redundant annotations. To avoid redundancy and annota-
tion costs, we automatically import instances from the LVIS [16] dataset as a strong starting point.
– Importing instances from LVIS. The LVIS dataset comprises 1.2M high-quality instance anno-
tations across 1203 categories for images in COCO, including all 80 COCO categories. Gupta
et al. [16] implemented specific instructions and verification steps to firmly ensure exhaustive
annotations. EveryimageislabeledwithasetofpositiveLVIScategorieswhoseinstancesareex-
haustively labeled, and a set of negative LVIS categories that are verifiably absent. The presence
(or absence) of the remaining LVIS categories is unverified for the image.
For the 80 COCO categories, LVIS contains ≈20K instances for images in the COCO-2017 val-
idation set. To import these instances, we checked whether LVIS contained more instances of
an (image, category) pair than COCO-2017. If so, we replaced all COCO-2017 instances of this
pair with those from LVIS. Nearly 9% of (image, category) pairs in LVIS are flagged to have
non-exhaustive instance annotations [16] – we ignored these while importing.
– Discovering instances using LVIS-trained models. Instances of a given (image, category) pair
of COCO-2017 may be missing in LVIS if the category is unverified for the corresponding image.
Instances of this pair may be non-exhaustive in COCO-2017, and hence remain unresolved after
the previous automatic step. To resolve this, we use publicly available object detectors trained
usingLVIS.Onthevalidationset,weensemblemaskpredictionsusingtheofficialcheckpointsof
6Instance Source
Dataset Split COCO-2017 LVIS LVIS models Total
COCO-2017 val 36,781 - - 36,781
COCO-ReM val 33,498 6,135 1056 40,689
COCO-2017 train 860,001 - - 860,001
COCO-ReM train 738,353 354,674 - 1,093,027
Table 1: Instance statistics in COCO-ReM. We list the exact number of annotations obtained
from different sources for both train and val splits of COCO-ReM. All masks undergo refinement
through SAM before being included in COCO-ReM.
three best-performing ViTDet models [25]. As model predictions are not as accurate as human-
annotated data, we manually inspect ≈2500 masks predicted with high confidence and select
1056 masks. We refine these masks (Stage 1) and add them to COCO-ReM.
ThisstageaugmentsCOCO-ReMwithadditionalinstancesfromexternalsourcesandincludesmore
exhaustive annotations, see example in Figure 3 (middle).
Stage3. Correctionoflabelingerrors. Thisisthefinalstageofourpipeline,performedmanually,
only for the validation set. Recall from Section 2 that we found 410 near-duplicate mask pairs in
the COCO-2017 validation set. We manually inspected these and retained one mask per pair with
the most accurate label, such as chair for armchairs and car for SUVs (Figure 3 (right)). We also
observed that ≈100 grouped instances were not resolved through Stage 2. We annotated these
masks as crowd to accurately reflect their characteristics.
3.2 Mask Characteristics
Instancestatistics. Ourannotationpipelineimportsinstancesfromexternalsources–LVISdataset
andLVIS-trainedmodels–toannotateinstancesexhaustively. Asaresult,everyinstanceinCOCO-
ReM can be traced back to either one of these sources or COCO-2017. As shown in Table 1, our
COCO-ReM validation set extends COCO-2017 by including ≈6K masks from LVIS [16] and ≈1K
masks from LVIS-trained models [3, 25]. Since these masks replace a subset of COCO-2017 masks
(see Stage 2 description), we retain ≈33.5K masks from COCO-2017 (out of 36K masks). Overall,
COCO-ReM validation set has ≈4K more instances than COCO-2017.
On the other hand, the training set of COCO-ReM comprises a total of 1.09M high-quality masks,
as compared to 860K masks in the COCO-2017 training set. The difference in instance counts
between COCO-ReM and COCO-2017 sharply highlights the extent of non-exhaustively annotated
instances in COCO-2017.
Mask IoU. Figure 4 (left) shows the distribution of IoU between the (refined) masks of COCO-
ReM (validation set) and their corresponding source masks in COCO-2017 and LVIS datasets. The
median IoU is slightly less than one, indicating that most masks have a proper coarse shape, but
lack precise boundaries. This median estimate matches the findings of Gupta et al. [16], albeit
observed from a smaller sample of 100 masks collected from human annotators.
730%
COCO-2017 LVIS-v1 103
Median IOU=0.84 Median IOU=0.89
20%
102
10%
101
0%
0 0.5 1.0 0 0.5 1.0 1 2 3 4 5 5+
Mask IOU Number of holes
Figure 4: Left: Mask IoU distribution. Distribution of IoU between masks in COCO-ReM valida-
tion set and their source dataset. Right: Masks with holes. Nearly 2000 masks in COCO-ReM
validation set have holes, that were missing in COCO-2017.
Holes in masks. Figure 4 (right) shows that ≈2000 masks in COCO-ReM validation set have at
leastonehole. Bydesign,COCO-2017lackstheseholes. Seetheappendixforqualitativeexamples
and additional analysis per category.
4 Experiments
In our experiments, we aim to demonstrate the utility of COCO-ReM for reliable benchmarking
of object detection research. Recall Section 1, we hypothesize that the imperfections in COCO-
2017 may cause two issues: (1) During evaluation, models may be undesirably penalized despite
predictingcorrectmasks,yieldingcounter-intuitivemodelingobservations,and(2)Duringtraining,
models receive noisy supervision and/or may learn unwanted biases (e.g., never predict masks
with holes), resulting in sub-par capabilities. We claim that high-quality masks of COCO-ReM can
alleviate these issues. To this end, we use COCO-ReM for evaluating and training object detectors
and compare the resulting trends with those obtained using COCO-2017.
4.1 Evaluation using COCO-ReM
Inthissection,weevaluatefiftypubliclyavailableCOCOobjectdetectorsusingtheCOCO-ReMval-
idationset. Wecoverawiderangeofmodelsfromexistingliterature. Allmodelscanbecategorized
as either region-based or query-based:
– Region-based models, following Mask R-CNN [19]. These models first predict labeled boxes,
followedbybinarymasksinsidethoseboxes. Wealsoincludecascademodels,followingCascade
R-CNN [3]. These models extend the region-based design by iteratively refining their predicted
bounding boxes before making the final mask prediction.
– Query-based models, inspired by the Detection Transformer design [4]. Instead of predicting
masks within image regions, these models start with a set of image-independent query vectors,
thendecodelabeledmasksthroughaTransformer[35]byiterativelyattendingtoimageembed-
dings. Among query-based models, we consider Mask2Former [8] and OneFormer [21].
We ensure diverse image backbones in our model collection. The architecture and capacity of the
backbone plays a major role in the empirical performance of the object detector. To this end, we
8
sksam
fo
egatnecreP
seloh
htiw
sksam
fo
rebmuN70
COCO-ReM AP Mask2Former Cascade
Swin-L ViTDet-L OneFormer
COCO-2017 AP
InternImg-H
60
(+7.7)
50
QQuueerryy--bbaasseedd MMooddeellss
40 RReeggiioonn--bbaasseedd MMooddeellss
50 40 30 20 10 1
Model rank, ordered by their COCO-2017 AP
Figure 5: Evaluating fifty object detectors using COCO-ReM. All models score higher on COCO-
ReM than COCO-2017. Query-based models (⋆) score much higher than region-based models on
COCO-ReM, yielding opposite trends than COCO-2017.
include convolutional backbones (ResNet [18], RegNet [33], ConvNeXt [29], InternImage [37]),
hierarchicalvisiontransformers(Swin[28],MViTv2[26],DiNAT[17]),andplainvisiontransform-
ers [11]. We include backbones of different sizes for a single model, e.g., for Cascade ViTDet [25],
we include ViT-B (141M parameters), ViT-L (361M parameters), ViT-H (692M parameters).
We believe that our model collection is sufficiently diverse to draw meaningful conclusions from
comparing trends between COCO-ReM and COCO-2017
Testing protocol. We compute the mask Average Precision (AP) of all models using the official
COCO evaluation API in Detectron2 [38]. For simplicity, we adopt a common testing protocol for
allmodels–weresizethelongestedgeofinputimagesto1024pixels(withoutchangingtheaspect
ratio), and limit the number of detections per image to 100.
Result 1. All models score higher on COCO-ReM.InFigure5,weplotthemaskAPtrendsforall
fifty object detectors sorted by their COCO-2017 AP. All fifty models achieve higher AP on COCO-
ReM despite being trained using only COCO-2017 masks. This result validates our hypothesis that
the imperfections of COCO-2017 may have wrongfully penalized all these models.
The difference between COCO-ReM AP and COCO-2017 AP is larger with recent high-performing
models–thecurrentstate-of-the-artmodel(OneFormerInternImage-H)scores+7.7pointshigher
AP on COCO-ReM than COCO-2017. This increasing gap in AP suggests that the imperfections in
ground-truth have worsened the issue of over-penalization and made COCO-2017 AP less adept at
fine-grained assessment and ranking of recent models.
Result 2. Query-based detectors score higher than region-based detectors on COCO-ReM.
Interestingly, we observe that the trend in AP values calculated using COCO-ReM does not align
perfectly with COCO-2017. We observe frequent flips in ranks of several model pairs on COCO-
ReM. While the model rankings do not transfer between benchmarks perfectly, we observe a con-
sistent pattern in model pairs with flipped rankings. Notice the star markers (⋆) in Figure 5 –
the flips are attributed to significantly higher scores of query-based models (Mask2Former [8] and
OneFormer [21]) than region-based models on COCO-ReM.
9Cascade ViTDet-L Mask2Former Swin-L
Figure 6: Mask predictions from models with opposite rank order on COCO-ReM and COCO-
2017. QualitativeexamplesofViTDet(region-basedmodel)andMask2Former(query-basedmodel)
show that the latter predicts visually sharper masks. COCO-ReM AP correctly ranks Mask2Former
higher, unlike COCO-2017 AP.
To further understand the cause of this performance trend, we observe the predicted masks of two
models having opposite rank order according to COCO-ReM and COCO-2017 AP: Cascade ViTDet-
L [25] and Mask2Former Swin-L [8]. The latter is a query-based model. Figure 6 shows some pre-
dictedmasksbybothofthesemodels. Throughacursoryglance,wecanobservethatMask2Former
predictsvisuallysharpermaskswhereasmasksofViTDethavecoarseboundaries. Thishumanjudg-
ment aligns better with COCO-ReM AP, which correctly rates Mask2Former higher. On the other
hand, COCO-2017 AP rates ViTDet higher, yielding a counter-intuitive modeling observation.
Guided by these trends, we reaffirm our claim that benchmarking newer models using the COCO-
2017maybemisleadingfromaresearchperspective,andthecommunityshouldadoptourCOCO-
ReM annotations moving forward. We include numerical results of Figure 5 in the appendix.
4.2 Understanding the Difference in AP
Recall Section 4.1 (Result 1) – all models score higher average precision (AP) on COCO-ReM, as
compared to COCO-2017. What explains the difference in AP? To understand the reasons under-
lying this difference, we observe the components of COCO AP for a few high-performing models
from Section 4.1. For this analysis, we consider two Cascade ViTDet models [25] using ViT-L/H
backbones [11], and two Mask2Former models [8] using Swin-B/L backbones [28].
AP per IoU threshold. The calculation of COCO AP involves an average of per-category AP scores
of80COCOcategories,eachcomputedattendifferentIoUthresholds,t ∈ {0.5,0.55,...,0.9,0.95}.
In Figure 7, we show components of AP at different IoU thresholds, averaged for the four models
weuse. Onaverage,modelsscoresimilarlyatIoU=0.5forCOCO-ReMandCOCO-2017. However,
COCO-ReMAPismuchlargerforIoU=0.75andbeyond. Hence,higherscoresonCOCO-ReMcan
be attributed to AP at higher IoU thresholds.
Intuitively, AP at IoU = t rewards model predictions that overlap with a ground-truth mask of the
same category with IoU ≥ t. AP at IoU = 0.5 requires a very coarse overlap between the predicted
and ground-truth masks, whereas AP at high IoU thresholds have stricter overlap requirements.
1080
COCO-ReM AP
COCO-2017 AP
60
+5.8
+6.0
+8.9
40 +13.2
+15.9
20
+9.9
0
IoU = 0.5:0.95 IoU = 0.5 IoU = 0.75 IoU = 0.8 IoU = 0.85 IoU = 0.9 IoU = 0.95
Figure 7: AP per IoU threshold. While models score similarly at IoU = 0.5 on COCO-ReM and
COCO-2017, they score much higher on COCO-ReM at higher IoU thresholds (0.75 and beyond),
contributingtotheoveralldifferenceinAP.DuetotheimprovedmaskqualityofCOCO-ReM,theAP
athighIoUthresholdsbecomesmoresensitiveandenablesbetterassessmentofmodelcapabilities.
(COCO-ReM AP COCO-2017 AP) per COCO category
0
COCO-ReM
15 AP is higher
10
10
20 COCO-2017
AP is higher
5
dib nio nw gl ta sbl cie ssors b be ad nan ca ouch train sk ris ed fo rin gu et rator apple b wo itt nl see kgl atas es bo ha ar nd sd ub ra fg boar pd ers ao sn ebc ala lr glove hors te oob tir hd brush sin bk api sz ez ba allb hat otd fo rig sb re ee m ot ore ange for ck arrot
b
Figure8: APpercategory. ModelsscorehigherAPonCOCO-ReMfor69outof80categories. The
masks of the remaining 11 categories (left) in COCO-2017 do not handle occlusions consistently
(dining table, bed), lack holes (scissors, donut), or have grouped instances (banana, apple). Models
learn these biases from COCO-2017 training masks and score lower COCO-ReM AP.
SinceIoUissymmetric,impreciseboundariesofground-truthmasks(COCO-2017)canwrongfully
penalize valid predictions and lead to lower AP scores. COCO-ReM fixes this issue, as evident with
higher AP at high IoU thresholds. This analysis hints that precise masks of COCO-ReM improve the
sensitivity of AP and enable fine-grained assessment of high-performing object detectors.
AP per category. Next, we calculate AP for every COCO category by averaging across all IoU
thresholds. Figure8showsthedifferenceinCOCO-ReMAPandCOCO-2017APpercategory,aver-
aged across the same four models. Models score higher on COCO-ReM for 69 out of 80 categories,
which is naturally reflected in the higher overall COCO-ReM AP. We observe large improvements
in categories having a generally consistent appearance (carrot and orange), as well as fine-grained
details(fork). Intuitivelyspeaking,thelargerdifferenceinper-categoryAPcanbeattributedtothe
better quality of ground-truth masks in COCO-ReM.
11COCO evaluation set =⇒ 2017 ReM-S1 ReM-S2a ReM (final)
AP 49.6 57.3 55.6 55.5
Number of instances 36,781 36,781 41,168 40,689
Table 2: COCO-ReM ablations. AP of four high-performing models (Cascade ViTDet-B/L and
Mask2Former Swin-B/L) using intermediate versions from our COCO-ReM annotation pipeline.
Trends suggest that the coarse boundary quality of COCO-2017 masks was a major source of noise
in the benchmarking process.
For 11 categories, models score lower on COCO-ReM than COCO-2017. These categories either
have holes in their natural appearance or are often heavily occluded (bowl, dining table, scissors).
Masks in COCO-ReM include holes and handle occlusions more consistently than in COCO-2017.
Hence,lowerper-categoryAPwithCOCO-ReMimpliesthattheimperfectionsofCOCO-2017train-
ingmasksimparttheseundesirablebiasestotrainedmodels–uponcheckingthemodelpredictions,
we observed that they often include occluding objects for dining table and lack holes for scissors.
Such predictions are not rewarded by COCO-ReM, hence incur a drop in the per-category AP.
4.3 Ablations
In this section, we conduct a simple ablation study to observe the contribution of different stages
in our annotation pipeline. For the sake of simplicity, we select the same four models as chosen in
Section 4.2. We evaluate these models using two versions of COCO-ReM validation set:
1. ReM-S1: This version is obtained after the Stage 1 (mask boundary refinement) in our annota-
tionpipeline. ItcomprisesthesamenumberofinstancesasCOCO-2017,eachwithanimproved
boundary quality.
2. ReM-S2a: This version is obtained after importing instances from the LVIS dataset – the auto-
matic part of Stage 2 (exhaustive instance annotation). We exclude the manual part of Stage 2
from this ablation for simplicity.
Results are shown in Table 2. The large difference between COCO-ReM AP and COCO-2017 AP
appears immediately after Stage 1 – this finding indicates that the coarse mask boundaries are a
majorcauseofnoisethathurtsthereliabilityofCOCO-2017AP.Subsequentstagesincuranegligi-
ble drop in AP – we attribute this to the increased size of the validation set, as COCO AP is known
to favor smaller evaluation sets [16].
4.4 Training with COCO-ReM
Our evaluation study shows that COCO-ReM can serve as a useful benchmark to draw accurate
modeling conclusions. In this section, we train strong baseline models using COCO-ReM.
Implementation details. We train ViTDet models [25] using their official open-source implemen-
tation[38],leavingallhyperparametersunchanged. Wetraintwomodels–MaskR-CNNViTDet-B
(111Mparameters)andCascadeViTDet-B(141Mparameters). Lietal.[25]trainedthismodelfor
100COCOepochsacross64×A100GPUs(batchsizeof64). Tomanagethesecosts,weimplement
gradient accumulation for every 8 iterations to emulate a batch size of 64 using our 8×A40 GPUs,
12Mask R-CNN ViTDet Cascade R-CNN ViTDet
56 TTrraaiinneedd uussiinngg CCOOCCOO--RReeMM 55.2 56 VViiTTDDeett--BBaassee 55 56 .. 43
TTrraaiinneedd uussiinngg CCOOCCOO--22001177 VViiTTDDeett--LLaarrggee
54.5
54 54
52 52 51.4
50.5
50 50
0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100
COCO epochs COCO epochs
Figure 9: Training with COCO-ReM. A ViTDet model trained using COCO-ReM training set con-
verges faster and performs better than its counterpart trained using COCO-2017, and closely
matches a 3× larger ViTDet-L trained using COCO-2017.
resultinginan11day-longtrainingschedule. Wecouldonlyaffordtotrainthebase(ViT-B)models
due to limited resources.
Results. Figure 9 shows that our ViTDet-B models trained using COCO-ReM significantly outper-
formthe sameViTDet-B modelstrained usingCOCO-2017. Higherquality oftraining dataenables
faster convergence. Moreover, our ViTDet-B models are competitive with 3× larger ViTDet-L mod-
els trained using COCO-2017, indicating better parameter efficiency. While numerous prior works
improve object detectors through modeling innovations, our results highlight the critical role of
data quality in improving object detectors.
5 Related Work
Object detection and image segmentation. Visual perception is fundamental for any vision sys-
temtounderstandtheworldaroundit. Atthecoreofavisualperceptionsystemisthetaskofimage
segmentation that aims at recognizing the entities (objects) in a given scene and densely labeling
everypixel constitutingthe correspondingobject. Imagesegmentation canbroadlybe dividedinto
three sub-tasks: semantic, instance, and panoptic segmentation. On the one hand, semantic seg-
mentation [5, 6, 30] targets obtaining a single amorphous binary mask covering all pixels for the
correspondingcategory. Ontheotherhand,instancesegmentation[3,20]involvesdetectingpixels
for only foreground “thing” regions with clear boundaries and differentiating among separate in-
stances for a category. Panoptic segmentation [7, 22, 36] is a combination of the former two tasks
targeted at predicting distinct binary masks for “thing” regions and a single amorphous mask for
the “stuff” regions.
In this work, we particularly look at the task of instance segmentation and contribute high-quality
mask annotations to benchmark further progress.
Image segmentation benchmarks. In the last decade, considerable advancements have been
observed in the classical vision task of image segmentation. A part of this progress is owed to
the introduction of numerous datasets for benchmarking image segmentation models. In one of
the earliest efforts, Everingham et al. [12] introduced the PASCAL-VOC-2012 challenge to recog-
nize 20 classes in about 10K images. More challenging datasets were introduced in the following
years, with ADE20K [10] covering 150 classes. Lin et al. [27] introduced the concept of “stuff”
13
)PA(
noisicerP
egarevAand “thing” classes in their COCO dataset with a total of 171 classes: 80 “thing” and 91 “stuff”.
TheCityscapes[9],Mapillary-Vistas[32],andKITTI[14]datasetsintroducedimagesegmentation
datasets focused on autonomous driving applications.
Guptaetal.[16]introducedtheLVISdatasetwithexhaustiveannotationscovering1203categories
for COCO images. OpenImages [1] introduced 2.8M mask annotations covering 350 classes. More
recently,theSA-1Bdataset[23]introducedover1.1Bfine-grained,class-agnosticmaskannotations
for over 11M images. Despite various datasets, COCO has remained the de-facto benchmark for
objectdetectorsontheinstancesegmentationtask. Inthiswork,wetookacloserlookattheCOCO
instance annotations and developed a new benchmark with much higher mask quality.
Re-assesing datasets for benchmarking. Human-annotated datasets are always prone to errors
and imperfections, as no human is perfect. Some prior works have shared our motivation and
conductedin-depthre-assessmentstudies,theclosesttooursareworksthatre-assesstheImageNet
dataset[2,34]. Concurrenttoourwork,Zimmermannetal.[39]alsore-assessCOCOannotations,
however they opt for recollecting polygon masks from human annotators.
6 Conclusion
In this work, we revisit the COCO-2017 mask annotations and find major issues involving, but not
limited to their boundary quality and exhaustiveness. These issues raise significant concerns about
the dependability of COCO-2017 to benchmark recent and future object detectors accurately. Con-
sequently, we introduce our COCO-ReM benchmark consisting of high-quality mask annotations
obtained using an effective semi-automatic annotation pipeline.
OncomparingthemaskAPtrendsforexistingobjectdetectorsonCOCO-ReMandCOCO-2017,we
noticesignificantchangesinmodelrankings,notablycontradictingtheconclusionsdrawnthrough
COCO-2017. Moreover, training using COCO-ReM shows that paying attention to mask quality
is crucial to advancing the capabilities of object detectors. We hope that COCO-ReM will aid the
future research in object detection.
Acknowledgments
We thank Mohamed El Banani, Agrim Gupta, and Nilesh Kulkarni for helpful discussions and feed-
back. We thank Palak Jagtap for her help during manual verification of masks. We thank all the
authors of the prior works for publicly releasing the pre-trained model checkpoints, which helped
us maintain empirical comprehensiveness in our evaluation study.
14Appendix
This appendix consists of three parts: (1) Appendix A includes implementation details of the mask
boundary refinement stage in the COCO-ReM annotation pipeline, as discussed in Section 3.1 (2)
Appendix B includes an in-depth analysis and qualitative examples from our COCO-ReM dataset,
asdiscussedinSection3.2,and(3)Intheend,Table3liststhenumericresultsofallthefiftyobject
detectors we evaluated in Section 4 and Figure 5.
A Mask Boundary Refinement: Implementation Details
The first stage in our annotation pipeline – mask boundary refinement – aims to refine the coarse
boundaries of COCO-2017 masks. To achieve this, we use the Segment Anything Model (SAM)
for its high-quality mask predictions. We performed an initial pass through the COCO-2017 masks
using the publicly available SAM checkpoint using the ViT-H [11] image encoder.
Prompting strategy for SAM. We mimicked the typical interactive segmentation procedure with
SAM, following Kirillov et al. [23]. We start by providing SAM with the bounding box for every
COCO mask, randomly expanding by up to 10 pixels on all sides. After the initial bounding box
prompt, we input two additional point prompts randomly sampled from the error region (deter-
mined by bitwise XOR) between the predicted mask and the original COCO mask for SAM, labeling
themforegroundorbackgroundaccordingtothecorrespondingpixelintheCOCOmask. Following
Kirillov et al. [23], we provide all prior prompts and un-thresholded mask logits from the previous
prompting iteration for the model to use maximal information in making a mask prediction
Since COCO masks have imprecise boundaries, randomly sampled points may be mislabeled, lead-
ing to incorrect prompt labels for SAM. However, we observed that SAM is sufficiently robust if
initially prompted with a box. To enhance robustness, we repeat the prompting scheme ten times
for every COCO mask and take a majority vote for every pixel label to obtain the final mask.
Manual verification. To further strengthen the high mask quality guarantee, we manually review
every single mask obtained from the initial SAM-assisted refinement stage. Despite a robust au-
tomated procedure, we discovered that our prompting strategy is not a one size fits all solution.
COCO masks do not handle occlusions consistently. Hence, SAM likely receives foreground point
prompts from other occluding objects, resulting in lower-quality masks. We found an overwhelm-
ingnumberoflow-qualitymasksforsixcategories–bed,bicycle,bowl,diningtable,motorcycle,and
scissors. These usually have holes or are commonly occluded by other objects (e.g., silverware on
dining table, people and pets on beds). To accommodate these edge cases, we regenerated masks
with only box (and no point) prompts to obtain refined masks for these object categories.
B COCO-ReM: Additional Analysis
Per-categoryMaskIoU.InFigure10,weplottheper-categorymaskIoUbetweenmasksinCOCO-
ReM and the corresponding source masks (from either COCO-2017 or LVIS) for the validation
set. We observe that the median of the mask IoU distribution lies close to 1 for most categories,
suggesting the change in boundary quality (enhanced) for COCO-ReM annotations.
1540% airplane apple backpack banana baseball baseball bear bed
bat glove
20%
0%
40% bench bicycle bird boat book bottle bowl broccoli
20%
0%
40% bus cake car carrot cat cell chair clock
phone
20%
0%
40% couch cow cup dining dog donut elephant fire
table hydrant
20%
0%
40% fork frisbee giraffe hair handbag horse hot keyboard
drier dog
20%
0%
40% kite knife laptop microwave motorcycle mouse orange oven
20%
0%
40% parking person pizza potted refrigerator remote sandwich scissors
meter plant
20%
0%
40% sheep sink skateboard skis snowboard spoon sports stop
ball sign
20%
0%
40% suitcase surfboard teddy tennis tie toaster toilet toothbrush
bear racket
20%
0%
40% traffic train truck tv umbrella vase wine zebra
light glass
20%
0%
0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1
Mask IOU
Figure 10: Mask IoU per category. We display the intersection-over-union (IoU) distribution
between masks before and after SAM-assisted refinement and manual verification. Distributions
with median IoU ≤ 0.8 are highlighted in gray.
Fortheremainingclasses,wenoticearelativelyspread-outmaskIoUdistributioneitherduetothe
presenceofholesandocclusionsincategorieslikescissors,diningtableormoreintricateshapesfor
categorieslikeknife,fork. NotethatformasksthatdidnotexistintheCOCO-2017orLVISdataset,
such as those sourced from LVIS-trained models, IoU is taken to be 1.
Per-categoryinstancestatistics. Weanalyzeper-categorystatisticsabouttheinstances’sourceand
additionalmasksinourCOCO-ReMevaluationbenchmarkascomparedtoCOCO-2017(validation
set). AsshowninFigure11(left),COCO-ReMconsiderablyleveragestheexhaustivenessqualityof
the LVIS [16] dataset for most object categories. Additionally, we notice that among the instances
sourced from the LVIS [16] dataset, a major component is for food categories like banana, apple,
orange, donut, etc. along with categories frequently arranged collectively in an image like book,
chair, etc., as shown in Figure 11 (right). We attribute these findings to the ambiguity in the
annotator instructions for COCO, resulting in non-exhaustive masks for multiple instances like a
series books in a shelf, a bunch of bananas in a basket, or chairs in a stadium.
Similarly,mostmaskssourcedfromViTDet[3,25]modelstrainedonLVISarealsoincategorieslike
banana, apple, and orange, which may have masks grouping multiple instances as they frequently
occur in groups. Therefore, unlike COCO-2017, integrating instances from LVIS and LVIS-trained
models helps us maintain exhaustive instance annotations in our COCO-ReM benchmark.
16
sksam
fo
egatnecrePInstances per Category Number of extra
in COCO-ReM instances in COCO-ReM
person 11129 125
car
chair 88
book 245
bottle 127
banana 648
cup
dining table
bowl 69
traffic light 56
0 500 1000 1500 2000 2500 101 102 103
apple 411
handbag
donut 220
bird 99
umbrella 91
orange 213
carrot 119
boat
bench
suitcase 120
cow
kite 70
skis 157
sheep
motorcycle
truck
backpack
cake 68
vase 99
broccoli 49
wine glass
knife
potted plant
bicycle
pizza
remote
zebra
elephant
clock
horse
bus
tv
surfboard
tie
cell phone
sports ball
spoon
giraffe
sink
couch
laptop
fork
teddy bear
tennis racket
keyboard 77
dog
cat
train
baseball bat
skateboard
toilet
sandwich
bed
baseball glove
airplane
oven
refrigerator
mouse
hot dog
frisbee
fire hydrant
stop sign
bear Instance Source
parking meter
snowboard COCO-2017
toothbrush
microwave LVIS-v1
scissors
toaster LVIS-Models
hair drier
0 100 200 300 400 500 600 101 102 103
Figure 11: Instance statistics in COCO-ReM. Left: Instances per category, sourced from COCO-
2017 and LVIS. Right: Number of additional masks in COCO-ReM affirm that COCO-2017 has
non-exhaustive annotations.
17Detector Pre-train Training COCO-2017 COCO-ReM
Framework Backbone Dataset Epochs AP AP50 AP75 AP90 AP95 AP AP50 AP75 AP90 AP95
Source:Detectron2[38]ModelZoo(InitialandLSJ[15]baselines)
Region ResNet-50 IN-1K 36.0 37.1 58.5 39.8 11.2 1.7 39.1 56.8 42.3 18.9 4.7
Cascade ResNet-50 IN-1K 36.0 38.2 59.3 41.0 12.3 1.9 40.6 57.9 43.8 20.6 5.5
Region ResNet-50 - 108.0 39.3 60.8 42.2 12.7 2.0 42.1 59.7 45.5 21.6 5.9
Region ResNet-50 - 108.0 39.4 61.0 42.6 13.0 2.1 42.0 59.4 45.1 22.1 6.5
Region ResNet-50 - 100.0 40.0 61.7 43.2 13.5 2.1 42.6 60.1 45.6 22.7 7.0
Region ResNet-50 - 200.0 41.6 63.9 44.9 14.1 2.2 44.7 62.4 48.1 24.2 7.8
Region ResNet-50 - 400.0 42.3 64.8 45.6 15.1 2.5 45.6 63.4 48.9 25.9 8.4
Region ResNet-101 - 100.0 41.4 63.6 44.9 13.9 2.5 44.4 61.9 48.1 24.3 7.5
Region ResNet-101 - 200.0 42.8 65.5 46.5 14.8 2.5 46.2 64.0 49.7 25.9 8.4
Region ResNet-101 - 400.0 43.4 66.1 47.1 15.3 2.6 47.0 64.6 50.7 26.8 8.8
Region RegNetX-4GF IN-1K 100.0 41.3 63.4 44.7 13.6 2.5 44.3 61.9 47.8 24.1 7.8
Region RegNetX-4GF IN-1K 200.0 42.7 65.5 46.2 14.7 2.5 46.0 63.8 49.3 25.9 8.8
Region RegNetX-4GF IN-1K 400.0 43.5 66.4 47.1 15.3 2.7 46.9 64.8 50.4 26.4 8.6
Region RegNetY-4GF IN-1K 100.0 41.2 63.2 44.8 13.7 2.4 44.2 61.6 47.7 23.9 7.5
Region RegNetY-4GF IN-1K 200.0 42.7 65.3 46.1 15.3 2.4 46.0 63.6 49.7 25.4 8.1
Region RegNetY-4GF IN-1K 400.0 43.0 65.5 46.7 15.3 2.5 46.3 63.8 50.0 25.9 8.7
Source:ConvNeXt[29]ModelZoo
Region ConvNeXt-T IN-1K 36.0 41.3 64.3 44.4 13.1 2.1 44.1 63.2 47.8 21.5 5.3
Cascade ConvNeXt-T IN-1K 36.0 43.3 66.2 46.9 15.5 2.8 46.7 65.0 50.5 25.3 7.8
Cascade ConvNeXt-S IN-1K 36.0 44.8 68.2 48.5 16.0 2.9 48.4 67.2 52.3 26.1 8.2
Cascade ConvNeXt-B IN-1K 36.0 45.4 68.7 49.2 16.6 3.2 49.2 67.9 53.2 27.2 8.6
Cascade ConvNeXt-B IN-21K 36.0 46.9 70.3 51.2 17.4 3.3 50.8 69.4 55.3 28.7 8.9
Cascade ConvNeXt-L IN-21K 36.0 47.4 71.2 51.6 17.6 3.6 51.7 70.6 56.4 29.2 9.5
Cascade ConvNeXt-XL IN-21K 36.0 47.7 71.6 51.9 18.0 3.4 51.8 70.7 56.4 29.2 9.0
Source:MViTv2[26]ModelZoo
Region MViTv2-T IN-1K 36.0 43.4 67.2 47.1 14.4 2.2 47.1 66.4 51.1 24.8 7.0
Cascade MViTv2-T IN-1K 36.0 44.8 67.9 48.4 16.0 3.1 48.4 66.8 52.5 26.9 8.4
Cascade MViTv2-S IN-1K 36.0 45.3 68.7 49.2 16.3 3.2 49.5 68.2 53.6 28.1 9.5
Cascade MViTv2-B IN-1K 36.0 46.5 70.0 50.6 17.2 3.4 50.7 68.9 55.2 29.2 10.0
Cascade MViTv2-B IN-21K 36.0 47.1 71.1 51.4 17.5 3.3 51.5 70.0 55.9 29.9 9.8
Cascade MViTv2-H IN-21K 36.0 48.1 71.8 52.8 18.5 3.7 52.7 70.5 57.1 32.2 11.3
Source:Mask2Former[8]ModelZoo
Query ResNet-50 IN-1K 50.0 43.1 65.5 46.2 16.3 3.8 47.9 65.2 51.1 28.7 11.8
Query ResNet-101 IN-1K 50.0 43.7 66.1 47.0 16.7 3.9 48.5 65.7 51.8 29.4 12.4
Query Swin-T IN-1K 50.0 44.6 67.3 47.8 17.0 3.8 50.1 67.4 53.6 30.9 12.8
Query Swin-S IN-1K 50.0 46.0 69.2 49.8 17.5 3.9 51.7 69.2 55.2 32.6 13.0
Query Swin-B IN-1K 50.0 46.4 69.7 50.0 18.1 4.3 52.2 69.7 55.8 32.9 14.2
Query Swin-B IN-21K 50.0 47.6 71.4 51.5 18.5 4.4 53.7 71.5 57.6 33.9 14.1
Query Swin-L IN-21K 100.0 49.7 73.8 54.1 20.1 4.7 56.4 73.8 60.5 36.6 16.1
Source:ViTDet[25]ModelZoo
Cascade Swin-B IN-21K 50.0 46.2 70.1 50.3 16.8 3.3 50.5 69.3 54.8 28.5 9.0
Cascade Swin-L IN-21K 50.0 47.2 71.2 51.6 17.5 3.2 51.8 70.4 56.3 29.7 9.7
Cascade MViTv2-B IN-21K 100.0 48.1 71.9 52.8 18.0 3.2 52.7 70.9 57.2 31.4 10.7
Cascade MViTv2-L IN-21K 50.0 48.3 72.1 53.0 18.7 3.5 53.0 70.9 57.7 32.1 10.9
Cascade MViTv2-H IN-21K 36.0 48.3 72.0 53.1 18.5 3.7 53.0 70.8 57.4 32.5 11.1
Region ViT-B IN-1K(MAE) 100.0 45.9 69.9 49.8 16.5 3.0 50.5 68.6 54.5 29.4 9.5
Region ViT-L IN-1K(MAE) 100.0 49.2 73.5 53.9 18.7 3.7 54.5 72.6 59.2 33.5 12.0
Region ViT-H IN-1K(MAE) 75.0 50.2 74.5 55.0 19.3 3.8 55.9 74.0 60.5 35.1 12.7
Cascade ViT-B IN-1K(MAE) 100.0 46.7 69.8 50.9 17.9 3.7 51.4 68.7 55.8 31.0 11.0
Cascade ViT-L IN-1K(MAE) 100.0 50.0 73.7 55.0 19.7 3.9 55.4 72.6 60.1 35.1 13.0
Cascade ViT-H IN-1K(MAE) 75.0 51.0 74.6 56.2 20.3 4.1 56.7 73.9 61.7 36.8 13.7
Source:OneFormer[21]ModelZoo
Query Swin-L IN-21k 100.0 48.9 73.5 53.0 18.6 4.1 55.9 73.8 60.1 35.6 15.1
Query DiNAT-L IN-21k 100.0 49.2 73.8 53.6 18.6 4.2 56.5 74.3 60.7 36.8 15.8
Query InternImage-H IN-21k 100.0 52.0 76.8 57.2 20.8 4.6 59.7 77.0 64.1 40.1 17.5
Table3: PerformancecomparisonoffiftyobjectdetectorsonCOCO-ReMandCOCO-2017. We
present the numeric results of all models (trained using COCO-2017) from our evaluation study.
18C Datasheet for COCO-ReM dataset
Gebru et al. [13] introduced datasheets for datasets so that creators can carefully reflect on the
dataset curation process and consumers can have the necessary information to make the best use
ofthedataset. Itincludesaseriesofquestionsandanswersthatrelayimportantinformationabout
the motivation behind the dataset, its composition, collection process, maintenance, etc. This
section contains the datasheet for COCO-ReM.
Motivation
1. For what purpose was the dataset created? Was there a specific task in mind? Was there a specific
gap that needed to be filled? Please provide a description. This dataset was created to overcome
the shortcomings of COCO-2017 (Section 2), and offer a set of high-quality annotations to the
research community for accurate and reliable benchmarking.
2. Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,
company, institution, organization)? The annotations were created by the authors. Images of
this dataset are sourced from COCO (https://cocodataset.org).
3. Who funded the creation of the dataset? If there is an associated grant, please provide the name
of the grantor and the grant name and number. Creation of this dataset was not funded by any
grant or person other than the authors.
4. Any other comments? No.
Composition
5. What do the instances that comprise the dataset represent (e.g., documents, photos, people, coun-
tries)? Please provide a description. Each instance in COCO-ReM denotes a labeled mask over a
single image (from COCO). This mask represents a grouping of pixels in an image that can be
associated with a common semantic category/label; COCO has 80 such categories.
6. How many instances are there in total (of each type, if appropriate)? The validation and train
setshave40,689and1,093,027labeledinstances,respectively. Formoredetails,refertoTable1
and Figure 11.
7. Doesthedatasetcontainallpossibleinstancesorisitasampleofinstancesfromalargerset? Ifthe
dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g.,
geographic coverage)? If so, please describe how this representativeness was validated/verified. If it
isnotrepresentativeofthelargerset,pleasedescribewhynot(e.g.,tocoveramorediverserangeof
instances, because instances were withheld or unavailable). COCO-ReM offers a near-exhaustive
set of instance annotations for the 80 COCO categories across all the COCO images. We tried
to ensure exhaustiveness as much as possible (refer Section 3.1, Stage 2). However, there there
might be a tiny fraction of instances which we might have missed. Note that COCO categories
maybeviewedasasubsetofallpossiblevisualconcepts,howeverthisdistinctionisoutofscope
for COCO-ReM.
8. Whatdatadoeseachinstanceconsistof? “Raw”data(e.g.,unprocessedtextorimages)orfeatures?
In either case, please provide a description. Each instance comprises a segmentation mask
coveringanobjectinasingleCOCOimage,pairedwithalabeloutof80COCOcategorylabels.
199. Is there a label or target associated with each instance? If so, please provide a description. Each
annotationincludesacategory idsubfieldthatcorrespondstoacategoryinCOCO-2017. Each
instance is also associated with a source (COCO-2017, LVIS dataset, or LVIS-trained models),
source id, id, image id and iscrowd field.
10. Is any information missing from individual instances? If so, please provide a description, ex-
plaining why this information is missing (e.g., because it was unavailable). This does not include
intentionallyremovedinformation,butmightinclude,e.g.,redactedtext. No,allthesubfieldsfor
each instance annotation are filled with valid values.
11. Are relationships between individual instances made explicit (e.g., users’ movie ratings, social
networklinks)? Ifso,pleasedescribehowtheserelationshipsaremadeexplicit. Thereexistssome
relationship between instances as reflected by same category id.
12. Are there recommended data splits (e.g., training, development/validation, testing)? If so, please
provide a description of these splits, explaining the rationale behind them. Yes, we have clearly
defined training and validation sets (Table 1). COCO test set is held privately – we do not
provide instances for the test set.
13. Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a
description. We use the Segment Anything Model (SAM) for generating the annotations in
COCO-ReM. SAM sometimes hallucinates small disconnected components, as reported by Kir-
illov et al. [23]. This might be a source of noise in some instances.
14. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., web-
sites, tweets, other datasets)? If it links to or relies on external resources, (a) are there guarantees
that they will exist, and remain constant, over time; (b) are there official archival versions of the
complete dataset (i.e., including the external resources as they existed at the time the dataset was
created); (c) are there any restrictions (e.g., licenses, fees) associated with any of the external re-
sourcesthatmightapplytoadatasetconsumer? Pleaseprovidedescriptionsofallexternalresources
and any restrictions associated with them, as well as links or other access points, as appropriate.
Images of the dataset are taken from https://cocodataset.org. COCO is a well-established
dataset in the community, and its distribution has been stable over the years. This assures us
that its availability would not be compromised.
15. Does the dataset contain data that might be considered confidential (e.g., data that is protected
by legal privilege or by doctor–patient confidentiality, data that includes the content of individuals’
non-public communications)? If so, please provide a description. No, the dataset does not cover
any information that may be considered confidential. The images have been publicly available
for nearly 10 years at the time of undertaking of this project.
16. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,
or might otherwise cause anxiety? If so, please describe why. Images of our dataset are sourced
from COCO-2017 (we do not redistribute the images). During the manual verification of our
annotations,wediscovereightimagesinthevalidationset(outof5000)tobedisplayingnudity.
Suchimagesmightbeconsideredsensitiveinsomecontext–apartfromthis,thereisnoknown
offensive data to the best of our knowledge and perspective.
17. Does the dataset relate to people? If not, you may skip the remaining questions in this section.
Yes,fewinstancesinCOCO-ReMaslabeledaspersonandtheysegmentpeopleinCOCOimages.
2018. Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how
thesesubpopulationsareidentifiedandprovideadescriptionoftheirrespectivedistributionswithin
the dataset. There is no known explicit focus on any subpopulation in the dataset. Any such
bias, if discovered later, is unintentional.
19. Isitpossibletoidentifyindividuals(i.e.,oneormorenaturalpersons),eitherdirectlyorindirectly
(i.e., in combination with other data) from the dataset? If so, please describe how. COCO
images contain faces of humans which are easily identifiable by naked eye. However we do
not redistribute COCO images with our dataset – any implications of this data in COCO are not
exacerbated by COCO-ReM.
20. Doesthedatasetcontaindatathatmightbeconsideredsensitiveinanyway(e.g.,datathatreveals
raceorethnicorigins,sexualorientations,religiousbeliefs,politicalopinionsorunionmemberships,
orlocations;financialorhealthdata;biometricorgeneticdata;formsofgovernmentidentification,
such as social security numbers; criminal history)? If so, please provide a description. COCO
images are known to contain human faces (Q19) and some NSFW content (Q16) that might
be considered sensitive in some context. The images are hosted publicly and being used by the
research community for nearly a decade at the time of writing.
21. Any other comments? No.
Collection Process
NOTE: The authors did not collect additional data (images) beyond that available through COCO,
for developing COCO-ReM. COCO images are available at https://cocodataset.org. Annotations
from COCO-2017 [27] and LVIS [16] were used. Hence, questions in this section of the datasheet
are not applicable for COCO-ReM. We answer the questions pertaining the annotation procedure
in the next section.
Preprocessing/Cleaning/Labeling
22. Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tok-
enization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of miss-
ing values)? If so, please provide a description. If not, you may skip the remaining questions in
this section. We manually corrected some labeling errors in the validation set as part of our
annotation pipeline (refer Figure 3).
23. Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support
unanticipatedfutureuses)? Ifso,pleaseprovidealinkorotheraccesspointtothe“raw”data. Raw
data for COCO-ReM was essentially COCO (2017 version), available at https://cocodataset.
org
24. Is the software that was used to preprocess/clean/label the data available? If so, please provide a
link or other access point. The code for the entire pipeline is available at https://github.com/
kdexd/coco-rem
25. Any other comments? No
21Uses
26. Has the dataset been used for any tasks already? If so, please provide a description. The COCO-
2017datasethasbeenusedextensivelybytheresearchcommunitytobenchmarkvisionmodels
over the past years. COCO-ReM is a refined version of COCO-2017. We used COCO-ReM to
evaluate fifty existing models for their object detection and segmentation capabilities. We also
used COCO-ReM to train object detectors as strong baselines for future work.
27. Is there a repository that links to any or all papers or systems that use the dataset? If so, please
provide a link or other access point. We do not maintain such a repository. However, citation
trackers like Google Scholar and Semantic Scholar would list all future works that cite our
dataset.
28. What (other) tasks could the dataset be used for? The dataset could be used for any machine
learning task for which COCO-2017 was traditionally used.
29. Is there anything about the composition of the dataset or the way it was collected and prepro-
cessed/cleaned/labeledthatmightimpactfutureuses? Forexample,isthereanythingthatadataset
consumer might need to know to avoid uses that could result in unfair treatment of individuals or
groups (e.g., stereotyping, quality of serviceissues) orother risks orharms (e.g., legal risks, finan-
cial harms)? If so, please provide a description. Is there anything a dataset consumer could do to
mitigate these risks or harms? No, any such fact about the dataset is not evident to the authors.
30. Are there tasks for which the dataset should not be used? If so, please provide a description. The
authors do not anticipate any task for which the use of the dataset might be harmful.
31. Any other comments? No.
Distribution
32. Will the dataset be distributed to third parties outside of the entity (e.g., company, institution,
organization) on behalf of which the dataset was created? If so, please provide a description. Yes,
our dataset will be available on our dataset website, https://cocorem.xyz.
33. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset
have a digital object identifier (DOI)? We distribute our dataset as a ZIP file containing all the
annotations (JSON files) at our dataset website, https://cocorem.xyz. Users must download
the images separately from the COCO website, https://cocodataset.org
34. When will the dataset be distributed? The dataset will be publicly available from March 2024
onwards.
35. Willthedatasetbedistributedunderacopyrightorotherintellectualproperty(IP)license,and/or
underapplicabletermsofuse(ToU)?Ifso,pleasedescribethislicenseand/orToU,andprovidealink
orotheraccesspointto,orotherwisereproduce,anyrelevantlicensingtermsorToU,aswellasany
fees associated with these restrictions. Images of COCO-2017 are sourced from Flickr and hence
should abide by Flickr terms of use as outlined in https://www.flickr.com/creativecommons/.
Thedataannotationfilesweprovideatourdatasetwebsite,https://cocorem.xyzandcodewe
provide at https://github.com/kdexd/coco-rem is open sourced by us.
36. Have any third parties imposed IP-based or other restrictions on the data associated with the
instances? If so, please describe these restrictions, and provide a link or other access point to,
22or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these
restrictions. AsstatedinQ35,theusershouldadheretotheFlickrtermsofuse,whileusingthe
images from our dataset.
37. Do any export controls or other regulatory restrictions apply to the dataset or to individual in-
stances? If so, please describe these restrictions, and provide a link or other access point to, or
otherwise reproduce, any supporting documentation. No.
38. Any other comments? No.
Maintenance
39. Who will be supporting/hosting/maintaining the dataset? The dataset is hosted at https:
//cocorem.xyz, and the site is maintained by the authors.
40. How can the owner/curator/manager of the dataset be contacted (e.g., email address)? The
contact details of authors are available on the dataset website, https://cocorem.xyz.
41. Isthereanerratum? Ifso,pleaseprovidealinkorotheraccesspoint. Thereisnoerratumforour
initial release. We will version all errata as future releases and document them on the dataset
website.
42. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?
If so, please describe how often, by whom, and how updates will be communicated to dataset con-
sumers (e.g., mailing list, GitHub)? We do not have any plans to update the dataset as of now.
However, we may consider changes if deemed appropriate in the future. Any changes will be
released and communicated through our dataset website, https://cocorem.xyz.
43. If the dataset relates to people, are there applicable limits on the retention of the data associated
with the instances (e.g., were the individuals in question told that their data would be retained for
a fixed period of time and then deleted)? If so, please describe these limits and explain how they
will be enforced. Yes, COCO images in the dataset contain human faces. However, we do not
control the distribution of these images (Q20), hence the limits to the retention of the images,
if any, are the same as COCO.
44. Will older versions of the dataset continue to be supported/hosted/maintained? If so, please
describe how. If not, please describe how its obsolescence will be communicated to dataset con-
sumers. Yes, we will continue hosting all versions of COCO-ReM on our dataset website,
https://cocorem.xyz, in the event of a release of newer versions.
45. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for
them to do so? If so, please provide a description. Will these contributions be validated/verified? If
so, please describe how. If not, why not? Is there a process for communicating/distributing these
contributions to dataset consumers? If so, please provide a description. Anyone who wishes to
extend or contribute to the dataset in any way is welcome to contact the authors through the
provided contacts on the dataset website, https://cocorem.xyz. All contributions and updates
will be highlighted on the dataset website.
46. Any other comments? No.
23References
[1] Rodrigo Benenson, Stefan Popov, and Vittorio Ferrari. Large-scale interactive object segmen-
tation with human annotators. In Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019. 2, 4, 5, 14
[2] Lucas Beyer, Olivier J H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and Aa¨ron van den Oord.
Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020. 14
[3] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into high quality object detec-
tion. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2018. 2, 7, 8, 13, 16
[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
SergeyZagoruyko. End-to-endobjectdetectionwithtransformers. InProceedingsofEuropean
Conference on Computer Vision (ECCV), 2020. 2, 8
[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille.
Semanticimagesegmentationwithdeepconvolutionalnetsandfullyconnectedcrfs. InICLR,
2015. 13
[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs. In TPAMI, 2017. 13
[7] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam,
and Liang-Chieh Chen. Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up
panoptic segmentation. In Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2020. 13
[8] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar.
Masked-attention mask transformer for universal image segmentation. InProceedings of IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 8, 9, 10, 18
[9] MariusCordts,MohamedOmran,SebastianRamos,TimoRehfeld,MarkusEnzweiler,Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic
urbansceneunderstanding. InProceedingsofIEEEConferenceonComputerVisionandPattern
Recognition (CVPR), 2016. 1, 14
[10] MariusCordts,MohamedOmran,SebastianRamos,TimoRehfeld,MarkusEnzweiler,Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. Semantic understanding of scenes
throughtheade20kdataset. InProceedingsofIEEEConferenceonComputerVisionandPattern
Recognition (CVPR), 2017. 1, 13
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
JakobUszkoreit,andNeilHoulsby. AnImageisWorth16x16Words: TransformersforImage
RecognitionatScale.InProceedingsoftheInternationalConferenceonLearningRepresentations
(ICLR), 2021. 2, 9, 10, 15
[12] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The
24PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html, 2012. 13
[13] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna
Wallach, Hal Daum´e III, and Kate Crawford. Datasheets for datasets. arXiv preprint
arXiv:1803.09010, 2018. 19
[14] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the
kittivisionbenchmarksuite. InProceedingsofIEEEConferenceonComputerVisionandPattern
Recognition (CVPR), 2012. 14
[15] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V
Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance
segmentation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2021. 18
[16] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance
segmentation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2019. 2, 3, 4, 5, 6, 7, 12, 14, 16, 21
[17] Ali Hassani and Humphrey Shi. Dilated neighborhood attention transformer.
arXiv:2209.15001, 2022. 9
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2016. 2, 9
[19] Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross Girshick. Mask R-CNN. In Proceedings
of IEEE International Conference on Computer Vision (ICCV), 2017. 2, 8
[20] Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross Girshick. Mask r-cnn. In Proceedings of
IEEE International Conference on Computer Vision (ICCV), 2017. 13
[21] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. One-
Former: One Transformer to Rule Universal Image Segmentation. In Proceedings of IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 8, 9, 18
[22] AlexanderKirillov,RossGirshick,KaimingHe,andPiotrDolla´r. Panopticfeaturepyramidnet-
works. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2019. 13
[23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,
TeteXiao,SpencerWhitehead,AlexanderC.Berg,Wan-YenLo,PiotrDolla´r,andRossGirshick.
SegmentAnything. InProceedingsofIEEEInternationalConferenceonComputerVision(ICCV),
2023. 2, 3, 4, 5, 14, 15, 20
[24] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeepcon-
volutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS),
2012. 1
[25] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer
25backbones for object detection. In Proceedings of European Conference on Computer Vision
(ECCV), 2022. 2, 7, 9, 10, 12, 16, 18
[26] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and
Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification
and detection. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2022. 9, 18
[27] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Dolla´r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In Proceedings
of European Conference on Computer Vision (ECCV), 2014. 1, 3, 13, 21
[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedings
of IEEE International Conference on Computer Vision (ICCV), 2021. 9, 10
[29] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSaining
Xie. Aconvnetforthe2020s. InProceedingsofIEEEConferenceonComputerVisionandPattern
Recognition (CVPR), 2022. 9, 18
[30] JonathanLong,EvanShelhamer,andTrevorDarrell. Fullyconvolutionalnetworksforseman-
ticsegmentation. InProceedingsofIEEEConferenceonComputerVisionandPatternRecognition
(CVPR), 2015. 13
[31] TorchVision maintainers and contributors. TorchVision: PyTorch’s Computer Vision library.
https://github.com/pytorch/vision, 2016. 3
[32] GerhardNeuhold,TobiasOllmann,SamuelRotaBulo,andPeterKontschieder. Themapillary
vistasdatasetforsemanticunderstandingofstreetscenes. InProceedingsofIEEEInternational
Conference on Computer Vision (ICCV), 2017. 14
[33] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dolla´r. Design-
ing network design spaces. In Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2020. 9
[34] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet clas-
sifiers generalize to imagenet? In Proceedings of the International Conference on Machine
Learning (ICML), 2019. 14
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-
tion Processing Systems (NeurIPS), 2017. 2, 8
[36] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. MaX-DeepLab:
End-to-endpanopticsegmentationwithmasktransformers. InProceedingsofIEEEConference
on Computer Vision and Pattern Recognition (CVPR), 2021. 13
[37] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu,
TongLu,LeweiLu,HongshengLi,etal. Internimage: Exploringlarge-scalevisionfoundation
models with deformable convolutions. In Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2023. 9
26[38] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2.
https://github.com/facebookresearch/detectron2, 2019. 3, 9, 12, 18
[39] EricZimmermann,JustinSzeto,JeromePasquero,andFredericRatle.Benchmarkingabench-
mark: How reliable is ms-coco? arXiv preprint arXiv:2311.02709, 2023. 14
27