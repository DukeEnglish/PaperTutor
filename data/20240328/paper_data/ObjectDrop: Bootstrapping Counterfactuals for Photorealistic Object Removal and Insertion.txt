ObjectDrop: Bootstrapping Counterfactuals
for Photorealistic Object Removal and Insertion
DanielWinter1,2, MatanCohen1, ShlomiFruchter1, YaelPritch1, AlexRav-Acha1, YedidHoshen1,2
1GoogleResearch, 2TheHebrewUniversityofJerusalem
https://ObjectDrop.github.io
ABSTRACT
Diffusionmodelshaverevolutionizedimageeditingbutoftengenerateimagesthatviolatephysical
laws,particularlytheeffectsofobjectsonthescene,e.g.,occlusions,shadows,andreflections. By
analyzingthelimitationsofself-supervisedapproaches,weproposeapracticalsolutioncenteredona
“counterfactual”dataset. Ourmethodinvolvescapturingascenebeforeandafterremovingasingle
object,whileminimizingotherchanges. Byfine-tuningadiffusionmodelonthisdataset,weareable
tonotonlyremoveobjectsbutalsotheireffectsonthescene. However,wefindthatapplyingthis
approachforphotorealisticobjectinsertionrequiresanimpracticallylargedataset. Totacklethis
challenge,weproposebootstrapsupervision;leveragingourobjectremovalmodeltrainedonasmall
counterfactualdataset,wesyntheticallyexpandthisdatasetconsiderably. Ourapproachsignificantly
outperformspriormethodsinphotorealisticobjectremovalandinsertion,particularlyatmodelingthe
effectsofobjectsonthescene.
Input Ours Baselines
Figure1: Objectremovalandinsertion. Ourmethodmodelstheeffectofanobjectonthesceneincludingocclusions,
reflections,andshadows,enablingphotorealisticobjectremovalandinsertion. Itsignificantlyoutperformsstate-of-the-
artbaselines.
{daniel.winter, yedid.hoshen}@mail.huji.ac.il
4202
raM
72
]VC.sc[
1v81881.3042:viXra
lavomeR
tcejbO
noitresnI
tcejbOObjectDrop: PhotorealisticObjectRemovalandInsertion
1 Introduction
Photorealisticimageeditingrequiresbothvisualappealandphysicalplausibility. Whilediffusion-basededitingmodels
havesignificantlyenhancedaestheticquality,theyoftenfailtogeneratephysicallyrealisticimages. Forinstance,object
removalmethodsmustnotonlyreplacepixelsoccludedbytheobjectbutalsomodelhowtheobjectaffectedthescene
e.g.,removingshadowsandreflections. Currentdiffusionmethodsfrequentlystrugglewiththis,highlightingtheneed
forbettermodelingoftheeffectsofobjectsontheirscene.
Objectremovalandinsertionisalong-standingbutchallengingtask. Classicalimageeditingmethodswereunable
totacklethefulltaskandinsteadtargetedspecificaspectse.g.,removinghardshadows. Theadventoftext-to-image
diffusionmodelsenabledanewclassofimageeditingtechniquesthataimtoperformmoregeneraledits.
We analyze the limitations of self-supervised editing approaches through the lens of counterfactual inference. A
counterfactualstatement[27]takestheform"iftheobjectdidnotexist,thisreflectionwouldnotoccur". Accurately
addingorremovingtheeffectofanobjectonitsscenerequiresunderstandingwhatthescenewouldlooklikewith
andwithouttheobject. Self-supervisedapproachesrelysolelyonobservationsofexistingimages,lackingaccessto
counterfactualimages. Disentanglementresearch[17,22,33]highlightsthatitisdifficulttoidentifyandlearnthe
underlyingphysicalprocessesfromthistypeofdataalone,leadingtoincorrectedits. Thisoftenmanifestsaseither
incompleteobjectremovalorphysicallyimplausiblechangestothescene.
Here,weproposeapracticalapproachthattrainsadiffusionmodelonameticulouslycurated"counterfactual"dataset.
Eachsampleincludes: i)afactualimagedepictingthescene,andii)acounterfactualimagedepictingthesceneafter
anobjectchange(e.g.,adding/removingit). Wecreatethisdatasetbyphysicallyalteringthescene;aphotographer
capturesthefactualimage,altersthescene(e.g.,removesanobject),andthencapturesthecounterfactualimage. This
approachensuresthateachexamplereflectsonlythescenechangesrelatedtothepresenceoftheobjectinsteadofother
nuisancefactorsofvariation.
We find this technique highly effective for object removal, surprisingly even for large or inaccessible objects that
werenotseenintraining. However,givenitslimitedsize,thesamedatasetprovedinsufficientfortrainingthereverse
taskofmodelinghowanewlyinsertedobjectaffectsthescene. Wehypothesizethatobjectinsertion,whichrequires
synthesizingshadowsandreflectionsratherthanmerelyremovingthem,isinherentlymorecomplex. Weexpectthisto
requireadatasettoolargeforustocollect.
Toaddressthis,weproposeatwo-stepapproach. First,wetrainanobjectremovalmodelusingasmallercounterfactual
dataset. Second,weapplytheremovalmodelonalargeunlabeledimagedatasettocreateavastsyntheticdataset. We
finetuneadiffusionmodelonthislargedatasettoaddrealisticshadowsandreflectionsaroundnewlyinsertedobjects.
Wetermthisapproachbootstrapsupervision.
Ourapproach,ObjectDrop,achievesunprecedentedresultsforbothaddingandremovingtheeffectsofobjects.Weshow
thatitcomparesfavorablytorecentapproachessuchasEmuEdit,AnyDoor,andPaint-by-Example. Ourcontributions
are:
1. Ananalysisofthelimitationsofself-supervisedtrainingforeditingtheeffectsofobjectsonscenes,suchas
shadowsandreflections.
2. Aneffectivecounterfactualsupervisedtrainingmethodforphotorealisticobjectremoval.
3. Abootstrapsupervisionapproachtomitigatethelabelingburdenforobjectinsertion.
Input Image
and Mask
Our Object
Removal
Figure2: Generalization. Ourcounterfactualdatasetisrelativelysmallandwascapturedincontrolledsettings,yetthe
modelgeneralizesexceptionallywelltoout-of-distributionscenariossuchasremovingbuildingsandlargeobjects.
2ObjectDrop: PhotorealisticObjectRemovalandInsertion
Our Small Training Object Removal Synthesizing Large Dataset Training to Reconstruct Object Effects
Counterfactual
Dataset
Counterfactual Ground Truth
Noised Counterfactual D Loe sn soising Noised Ground Truth D Loe sn soising
UNET
UNET
Pasted Object On
Factual (With Object) Prediction Predicted Background Prediction
F (Wac it tu ha Ol I bm jea cg te ) C (o wu /n ot e Orf ba jc et cu t)al Object Mask + R Oe ba jl e cIm t Mag ae s k UNET BP ar ce kd gi rc ote ud n d Object Mask
Figure3: Overviewofourmethod. Wecollectacounterfactualdatasetconsistingofphotosofscenesbeforeand
afterremovinganobject,whilekeepingeverythingelsefixed. Weusedthisdatasettofine-tuneadiffusionmodelto
removeanobjectandallitseffectsfromthescene. Forthetaskofobjectinsertion,webootstrapbiggerdatasetby
removingselectedobjectsfromalargeunsupervisedimagedataset,resultinginavast,syntheticcounterfactualdataset.
Trainingonthissyntheticdatasetandthenfinetuningonasmaller,original,superviseddatasetyieldsahighquality
objectinsertionmodel.
1.1 RelatedWork
Imageinpainting. Thetaskofinpaintingmissingimagepixelshasbeenwidelyexploredintheliterature. Forseveral
years,deeplearningmethodsusedgenerativeadversarialnetwork[11]e.g. [16,30,38,40,43,58]. Severalworks
useend-to-endlearningmethods[18,29,50,56]. Morerecently,theimpressiveadvancementsofdiffusionmodels
[42,44,47,48],havehelpedspursignificantprogressininpainting[1,35,37,45,54]. Weshow(Sec. 3)thatdespitethe
greatprogressinthefieldandusingverypowerfuldiffusionmodels,thesemethodsarenotsufficientforphotorealistic
objectremoval.
ShadowremovalmethodsAnotherlineofworkfocusesonthesub-taskofshadowremoval.Inthistask,themodelaims
toremovetheshadowfromanimagegiventheshadowmask. Variousmethods[6–8,15,20,25,26,32,52,53,62,63]
havebeenproposedforshadowremoval. Morerecentmethods[12,36]usedlatentdiffusionmodels. Unlikethese
methods that remove only shadows, our method aims to remove all effects of the object on the scene including:
occlusionsandreflections. Also,thesemethodsrequireashadowsegmentationmap[6,55],whileourmethodonly
requiresanobjectsegmentationmap,whichiseasytoextractautomaticallye.g.,[23]. OmniMatte[34]aimedtorecover
bothshadowsandreflections,howeveritrequiresvideowhereasthispaperdealswithimages.
Generalimageeditingmodel. Analternativeapproachforremovingobjectsfromphotosistouseageneralpurpose
text-based image editing model [2, 3, 9, 46, 61]. For example, Emu Edit [46] trains a diffusion model on a large
syntheticdatasettoperformdifferenteditingtasksgivenataskembedding. MGIE[9]utilizesadiffusionmodelcoupled
withMultimodalLargeLanguageModel(MLLM)[31,51]toenhancethemodel’scross-modalunderstanding. While
thebreadthofthecapabilitiesofthesemethodsisimpressive,ourmethodoutperformedthemconvincinglyonobject
removal.
ObjectInsertion. Earliermethodsforinsertinganobjectintoanewimageusedend-to-endGenerativeAdversarial
Network(GAN)suchasPix2Pix[19],ShadowGAN[60],ARShadowGAN[28]andSGRNet[14]. Recentstudies
useddiffusionmodels. Paint-by-Example[57]andObjectStitch[49]insertareferenceobjectintoanimageusingthe
guidanceofaimage-textencoder[41],butonlypreservesemanticresemblancetotheinsertedobject. AnyDoor[4]
usedaself-supervisedrepresentation[39]ofthereferenceobjectalongsideitshigh-frequencymapasconditionsto
enhanceobjectidentitypreservation. WhilethefidelityofgeneratedimagesproducedbyAnyDoorimprovedover
formermethods,itsometimeschangesobjectidentityentirelywhilewekeepitunchangedbydesign. Furthermore,
previousmethodsoftendonotmodelobjectreflectionsandshadowsaccurately,leadingtounrealisticoutcomes.
3
…ObjectDrop: PhotorealisticObjectRemovalandInsertion
Input Image Extended Mask SD-XL Inpainting Tight Mask SD-XL Inpainting Ours
Figure4: Objectremoval-comparisonwithinpainting. Ourmodelsuccessfullyremovesthemaskedobject,while
thebaselineinpaintingmodelreplacesitwithadifferentone. Usingamaskthatcoversthereflections(extendedmask)
mayobscureimportantdetailsfromthemodel.
2 TaskDefinition
WeconsidertheinputimageX depictingaphysical3DsceneS. Wewantourmodeltogeneratehowthescenewould
havelooked,hadanobjectObeenaddedtoorremovedfromit. Wedenotethis,thecounterfactualoutcome. Inother
words,thetaskisdefinedasre-renderingthecounterfactualimageXcf,giventhephysicalchange. Thechallengeisto
modeltheeffectsoftheobjectchangeonthescenesuchasocclusions,shadowsandreflections. Formally,assumethe
physicalrenderingmechanismisdenotedasG ,theinputimageX isgivenby:
physics
X =G (O,S) (1)
physics
ThedesiredoutputisthecounterfactualimageXcf s.t.,
Xcf =G (Ocf,S) (2)
physics
Forobjectremoval,anobjectoisoriginallypresentO =oandwewishtoremovetheobjectsothatOcf =ϕ(ϕisthe
emptyobject). Forobjectinsertion,theobjectisinitiallyabsentO =ϕandwewishtoadditOcf =o.
WhilethephysicalrenderingmechanismG isrelativelywellunderstood,theformulationinEq.1cannotbe
physics
useddirectlyforeditingasitrequiresperfectknowledgeofthephysicalobjectandscene,whichisrarelyavailable.
3 Self-SupervisionisNotEnough
Asphysicalsimulationsarenotafeasiblewayforphotorealisticobjectremovalandinsertioninexistingimages,recent
approachesuseddiffusionmodelsinstead. Diffusionmodelsprovidehigh-qualitygenerativemodelsofimagesi.e.,they
provideaneffectivewaytoestimateandsamplefromthedistributionP(X)whereX isanaturalimage. Generative
models are not a natural fit for adding or removing objects from an image as they do not provide a direct way to
accessormodifyitshiddencausalvariables: thephysicalobjectO andthepropertiesofthesceneS. Thetaskof
inferringthehiddenvariables(e.g. OandS)andthegenerativemechanism(e.g.,G )iscalleddisentanglement.
physics
Severalinfluentialworks[17,22,33]establishedthatunsuperviseddisentanglementfromobservationaldataisgenerally
impossiblewithoutstrongpriors. Self-supervisedmethodsattempttoperformdisentanglementusingheuristicschemes.
One common heuristic for object removal is to use diffusion-based inpainting. Such methods rely on an earlier
segmentationstepthatsplitstheimagepixelsintotwonon-overlappingsubsets,asubsetofpixelsthatcontaintheobject
4ObjectDrop: PhotorealisticObjectRemovalandInsertion
Instruction Input Image Text-based Mask Emu Edit MGIE Ours
Remove the
tennis ball from
the photo
Remove the
shoes in front of
the cat.
Remove the table
from the middle
of the room
Figure5: Objectremoval-comparisonwithgeneraleditingmethods. Wecomparetogeneraleditingmethods: Emu
EditandMGIE.Thesemethodsoftenreplacetheobjectwithanewoneandintroduceunintendedchangestotheinput
image. Forthiscomparisonweusedatext-basedsegmentationmodeltomasktheobjectaccordingtotheinstruction
andpassedthemaskasinputtoourmodel.
X andasubsetofthosethatdonotX s.t. X =X ∪X . Inpaintingthenusesthegenerativemodeltoresamplethe
o s o s
valuesoftheobjectpixelsgiventhescene:
x ∼P(X |X =x ) (3)
o o s s
Themainlimitationofthisapproachisitsdependenceonthesegmentationmask. Ifthemaskischosentootightly
aroundtheobject,thenX includestheshadowsandreflectionsoftheobjectandthushasinformationabouttheobject.
s
ThemostlikelyvaluesofP(X |x )willcontainanobjectthatrenderssimilarshadowsandreflectionsastheoriginal,
o s
whichislikelyasimilarobjecttotheoriginal. Ifthemaskischosensoconservativelyastoremoveallscenepixelsthat
areaffectedbytheobject,itwillnotpreservetheoriginalscene. WeshowbothfailuremodesinFig.4.
Attention-basedmethods,suchasprompt-to-prompt[13],useasophisticatedheuristicbasedoncross-attentionwhich
sometimesovercomesthefailuremodesofinpainting. However,astheybiasthegenerativemodelP(X |X ),theycan
o e
resultinunrealisticedits,sometimesremovingtheobjectbutnotitsshadows. Also,theattentionmasksoftenfailto
captureallscenepixelsaffectedbytheobject,resultinginsimilarfailuresasinpainting. NotethatEmuEdit[46]uses
syntheticdatacreatedbyanattention-basedmethodforobjectremovalandcanthereforesufferfromsimilarfailure
modes.
Thecorelimitationoftheaboveself-supervisedapproachesistheinabilitytodirectlyinferthetruegenerativemechanism
andthecausalhiddenvariables,theobjectOandsceneS. Whileheuristicmethodsfordoingsomadeprogress,thecore
limitationsarehardtoovercome. Class-guideddisentanglementmethods[5,10]attempttosolvethedisentanglement
taskfromobservationaldatabyassumingperfectknowledgeofoneofthehiddenvariables(here,thephysicalobject
O),andassumingthattheobjectandsceneareindependent. Bothassumptionsarenotsoundinthissetting,asthe
propertiesofthephysicalobjectandscenearenotknownperfectly,andonlysomeobjectsarelikelyinaparticular
scene. Notethatthegenerativemechanismisnotperfectlyidentifiableevenwhentheassumptionsaresatisfied[21,22].
Thismotivatesoursearchforamoregroundedapproachaswillbedescribedinthefollowingsections.
4 ObjectRemoval
InthissectionweproposeObjectDrop,anewapproachbasedoncounterfactualsupervisionforobjectremoval. As
mentionedinSec. 3,itisinsufficienttomerelymodeltheobservedimages,butwemustalsotakeintoaccounttheir
5ObjectDrop: PhotorealisticObjectRemovalandInsertion
Input Paint-by-Example AnyDoor Ours
Figure6: Intra-imageobjectinsertion-baselinecomparison. Wepreservetheobjectidentitybetterandachieve
morephotorealisticshadowsandreflectionsthanbaselinesPaint-by-ExampleandAnyDoor.
causalhiddenvariablesi.e.,theobjectandthescene. Aswecannotlearnthesepurelyfromobservingimages,we
proposetodirectlyactinthephysicalworldtoestimatethecounterfactualeffectofremovingobjects.
4.1 Collectingacounterfactualdataset.
Thekeyforunlockingsuchmodelsisbycreatingacounterfactualdataset. Theprocedureconsistsofthreesteps:
1. CaptureanimageX ("factual")containingtheobjectOinsceneS.
2. PhysicallyremovetheobjectOwhileavoidingcameramovement,lightingchangesormotionofotherobjects.
3. CaptureanotherimageXcf ("counterfactual")ofthesamescenebutwithouttheobjectO.
Weuseanoff-the-shelfsegmentationmodel[23]tocreateasegmentationmapM fortheobjectOremovedfromthe
o
factualimageX. Thefinaldatasetcontaininputpairsoffactualimageandbinaryobjectmask(X,M (X)),andthe
o
outputcounterfactualimageXcf.
In practice, we collected 2,500 such counterfactual pairs. This number is relatively small due to the high cost of
datacollection. Theimageswerecollectedbyprofessionalphotographerswithatripodmountedcamera,tokeepthe
cameraposeasstableaspossible. Asthecounterfactualpairshave(almost)exactlythesamecamerapose,lightingand
backgroundobjects,theonlydifferencebetweenthefactualandcounterfactualimagesistheremovaloftheobject.
4.2 Counterfactualdistributionestimation.
Givenour high-qualitycounterfactual dataset, our goalis toestimatethe distribution ofthe counterfactual images
P(Xcf|X = x,M (x)), given the factual image x and segmentation mask. We do it by fine-tuning a large-scale
o
diffusion model on our counterfactual dataset. We investigate the impact of using different foundational diffusion
modelsinSec.6. Theestimationisdonebyminimizing:
6ObjectDrop: PhotorealisticObjectRemovalandInsertion
Reference Object Target Image Paint-by-Example AnyDoor Ours
Figure7: Cross-imageobjectinsertion. Similarlytotheresultsofintra-imageobjectinsertion,ourmethodpreserves
objectidentitybetterandsynthesizesmorephotorealisticshadowsandreflectionsthanthebaselines.
(cid:34) N (cid:35)
(cid:88)
L(θ)= E ∥D (α xcf +σ ϵ,x ,M (x ),t,p)−ϵ∥2 (4)
θ t i t i o i
t∼U([0,T]),ϵ∼N(0,I)
i=1
whereD (x˜,x ,m,t,p)isadenoisernetworkwithfollowinginputs: noisedlatentrepresentationofthecounterfac-
θ t cond
tualimagex˜,latentrepresentationoftheimagecontainingtheobjectwewanttoremovex ,maskmindicatingthe
t cond
object’slocation,timestamptandencodingofanemptystring(textprompt)p. Here,x iscalculatedbasedonthe
t
forwardprocessequation:
x˜ =α ·x+σ ·ϵ (5)
t t t
Wherexrepresentstheimagewithouttheobject(thecounterfactual),α andσ aredeterminedbythenoisingschedule,
t t
andϵ∼N(0,I).
Importantly,unliketraditionalinpaintingmethods,weavoidreplacingthepixelsoftheobjectwithuniformgrayor
blackpixels. Thisapproachallowsourmodeltoleverageinformationpreservedwithinthemask,whichisparticularly
beneficialinscenariosinvolvingpartiallytransparentobjectsorimperfectmasks.
4.3 Advantagesovervideosupervision.
Ourprocedurerequiresmakingphysicalworldchanges,whichbothlimitsthepossiblesizeofthedatasetandthescope
ofremovedobjectsastheycannotbeverybig,veryheavyorveryfar.Itistemptingtoreplacethisexpensivesupervision
byusingthecheapersupervisionobtainedfromvideos,asdonebyseveralpreviousmethodsincludingAnyDoor[4]and
[24]. Attrainingtime,theobjectiveofthesemethodsistoreconstructamaskedobjectinonevideoframe,byobserving
theobjectinanotherframe. Whilethisprocedureischeaper,ithasseriouslimitations: i)Inacounterfactualdataset,the
7ObjectDrop: PhotorealisticObjectRemovalandInsertion
Input Composition w/o Bootstrapping Ours Input Composition w/o Bootstrapping Ours
Figure8: Bootstrappingablation. Bootstrapsupervisionimprovesmodelquality.
Table1: Objectinsertion-reconstructionmetrics. Acomparisonwithbaselines: Paint-by-ExampleandAnyDoor,
ontheheld-outtestset. Furthermore,weablatethecontributionofthebootstrapsupervision.
Model PSNR↑ DINO↑ CLIP↑ LPIPS↓
Paint-by-Example 17.523 0.755 0.862 0.138
AnyDoor 19.500 0.889 0.890 0.095
Oursw/oBootstrap 20.178 0.929 0.945 0.066
Ours 21.625 0.939 0.950 0.057
onlychangebetweentheimageswithinapairshouldbetheremovaloftheobject. Conversely,invideomanyother
attributesalsochange,suchascameraviewpoint. Thisleadstospuriouscorrelationsbetweenobjectremovaland
otherattributes. ii)Thisprocedureonlyworksfordynamicobjects(cars,animals,etc.) andcannotcollectsamplesfor
inanimateobjects. WeshowinSec.6thatourmethodworksexceptionallywell,andparticularlyoutperformsmethods
thatusevideosupervision. Furthermore,ourmethodgeneralizessurprisinglywelltoobjectsthatweretoochallenging
tomoveinourcounterfactualdatasetincludingveryheavy,largeorimmobileobjects.
5 ObjectInsertion
WeextendObjectDroptoobjectinsertion. Inthistask,wearegivenanimageofanobject,adesiredposition,and
atargetimage. Theobjectiveistopredicthowthetargetimagewouldlooklike,haditbeenphotographedwiththe
givenobject. Whilecollectingarelativelysmall-scale(2,500samples)counterfactualdatasetwassuccessfulforobject
removal,weobservedthatthisdatasetisinsufficientfortraininganobjectinsertionmodel(seeFig. 8). Wehypothesize
thatthisrequiresmoreexamples,assynthesizingtheshadowsandreflectionsoftheobjectmaybemorechallenging
thanremovingthem.
5.1 Bootstrappingcounterfactualdataset.
Weproposetoleverageoursmallcounterfactualdatasettowardscreatingalarge-scalecounterfactualobjectinsertion
dataset. Wetakealargeexternaldatasetofimages,andfirstdetectrelevantobjectsusingaforegrounddetector. Let
x ,x ,...,x denotetheoriginalimagesandM (x ),M (x ),...,M (x )denotethecorrespondingobjectmasks. We
1 2 n o 1 o 2 o n
useourobjectremovalmodelP(Xcf|X,M (X))toremovetheobjectanditseffectsonthescene,denotingtheresults
o
asz ,z ,...,z where,
1 2 n
z ∼P(Xcf|x ,M (x )) (6)
i i o i
Finally,wepasteeachobjectintotheobject-lessscenesz ,resultinginimageswithoutshadowsandreflections,
i
y =M (x )⊙x +(1−M (x ))⊙z . (7)
i o i i o i i
Thesyntheticdatasetconsistsofasetofinputpairs(y ,M (x )). Thecorrespondingtargetsaretheoriginalimagesx .
i o i i
Toclarify,boththeinputandoutputimagescontaintheobjecto ,buttheinputimagesdonotcontaintheeffectsofthe
i
objectonthescene,whiletheoutputimagesdo. ThetaskofthemodelistogeneratetheeffectsasillustratedinFig.3.
8ObjectDrop: PhotorealisticObjectRemovalandInsertion
Input Image Input Masks 0 Samples 250 Samples 500 Samples 1000 Samples 2500 Samples
Figure9: Counterfactualdatasetsize. Increasingthesizeofthetrainingdatasetimprovesobjectremovalperformance.
Theresultsareofhighqualitywith2500examples,butmayimprovefurtherwithmoreimages.
Inpractice,westartwithadatasetconsistingof14Mimages,weselect700kimageswithsuitableobjects. Werun
objectremovaloneachimage,andfurtherfilteredapproximatelyhalfofthemthatdidnothavesignificantobjecteffects
onthescene. Thefinalbootstrappeddatasetconsistedof350Kimage,around140timesbiggerthanthemanually
labeleddataset. Pleaseseemoredetailsaboutthefilteringprocessinthesupplementary.
5.2 Diffusionmodeltraining.
Weusethebootstrappedcounterfactualdatasettotrainanobjectinsertionmodelwiththediffusionobjectivepresented
inEq. 4.2. Incontrasttotheobjectremovalprocess,weuseapre-trainedtext-to-imagemodelD (x,t,p)thatdidnot
θ
undergoinpaintingpre-training. Astheinputmaskincreasesinputdimension,weaddnewchannelstotheinputofthe
pre-trainedtext-to-imagemodel,initializingtheirweightswith0.
5.3 Fine-tuningonthegroundtruthcounterfactualdataset.
Thesyntheticdatasetisnotrealisticenoughfortrainingthefinalmodel,andisonlyusedforpre-training. Inthelast
stage,wefine-tunethemodelontheoriginalgroundtruthcounterfactualdatasetthatwasmanuallycollected. While
thisdatasetisnotlarge,pre-trainingonthebootstrappeddatasetispowerfulenoughtoenableeffectivefine-tuning
usingthissmallgroundtruthdataset.
6 Experiments
6.1 ImplementationDetails
Counterfactualdataset. Wecreatedacounterfactualdatasetof2,500pairsofphotosusingtheproceduredetailedin
Sec.4. Eachpaircontainsa"factual"imageofasceneandasecond"counterfactual"imageofexactlythesamescene
exceptthatitwasphotographedafterremovingoneobject. Wealsoheldout100counterfactualtestexamples,captured
afterthecompletionoftheresearch,depictingnewobjectsandscenes.
Model architecture. We train a latent diffusion model (LDM) for the object removal task. We initialize using a
pre-trainedinpaintingmodel,whichtakesasinputafactualimage,anobjectmask,andanoisycounterfactualimage.
Weperforminferenceusingdefaultsettings. WeuseainternalmodelwithasimilararchitecturetoStable-Diffusion-XL.
Unlikeotherinpaintingmodels,wedonotreplacetheremovedobjectpixelswithgraypixels.
Quantitativemetrics. Wecomparedtheresultsofourmethodandthebaselinesontheheld-outcounterfactualtestset.
Asthisdatasethasgroundtruth(seesupplementary),weusedstandardreconstructionmetrics: bothclassical(PSNR)
anddeepperceptualsimilaritymetricsusing: DINO[39],CLIP[41],andLPIPS[59](AlexNet)features.
6.2 ObjectRemoval
Qualitativeresults.WeevaluatedourresultonthebenchmarkpublishedbyEmuEdit[46].AsseeninFig.5,ourmodel
removesobjectsandtheireffectsinaphotorealisticmanner. Thebaselinesfailedtoremoveshadowsandreflectionsand
sometimesadverselyaffectedtheimageinotherways.
9ObjectDrop: PhotorealisticObjectRemovalandInsertion
Table2: Objectremoval-reconstructionmetrics. Acomparisonwiththeinpaintingbaselineontheheld-outtestset.
Model PSNR↑ DINO↑ CLIP↑ LPIPS↓
Inpainting 21.192 0.876 0.897 0.056
Ours 23.153 0.948 0.959 0.048
Table3: Objectremoval-userstudy. AcomparisontoEmuEditandMGIEontheEmuEditdataset[46]
Whichmodeldidbetterjobatfollowingtheobjectremovaleditinginstruction?
PreferredEmuEdit 35.9% PreferredMGIE 13.5%
Preferredours 64.1% Preferredours 86.5%
Quantitativeresults. Tab.2comparesourmethodtotheinpaintingpre-trainedmodelontheheld-outtestsetusing
quantitativereconstructionmetrics. Ourmethodoutperformedthebaselinesubstantially.
Userstudy. WeconductedauserstudyonthebenchmarkbyEmuEdit[46]betweenourmethodandbaselines: Emu
EditandMGIE.Asthebenchmarkdoesnothavegroundtruth,userstudyisthemostviablecomparison. Weused
theCloudResearchplatformtogatheruserpreferencesfrom50randomlyselectedparticipants. Eachparticipantwas
presentedwith30examplesofanoriginalimage,removaltextinstructions,andresultsgeneratedbyourmethodand
thebaseline. Tab.3displaystheresults. Notably,ourmethodsurpassedbothbaselinemethodsinuserpreference.
6.3 ObjectInsertion
Qualitative results. We compare our object insertion model with state-of-the-art image reference-based editing
techniques,Paint-by-Example[57]andAnyDoor[4]. Fig.6showsintra-imageinsertions,i.e.,whentheobjectsare
re-positionedwithinsameimage. Forachievingintra-imageinsertionswefirstuseourobjectremovalmodeltoremove
theobjectfromitsoriginalposition,obtainingthebackgroundimage. Forequitablecomparisons,weusedthesame
backgroundimage,obtainedbyourmodel,whencomparingtothebaselines. Fig.7showsinter-imageinsertions,i.e.,
whentheobjectscomefromdifferentimages. Inbothcasesourmethodsynthesizestheshadowsandreflectionsofthe
objectbetterthanthebaselines. Italsopreservestheidentityoftheobject,whileothermethodsmodifyitfreelyandin
manycaseslosetheoriginalidentityentirely.
Quantitativeresults. WecomparetoPaint-by-ExampleandAnyDoorontheheld-outcounterfactualtestdataset. The
resultsarepresentedinTab.1. Ourmethodoutperformsthebaselinesbyasignificantmarginonallmetrics.
Userstudy. Wealsoconductedauser-studyon2intra-imageinsertiondatasets. Thefirstistheheld-outtestset. The
secondisasetof50out-of-distributionimagesdepictingmoregeneralscenes,someareverydifferentfromthoseseen
intraininge.g.,insertingboatsandbuilding. Tab.4showsthatusersoverwhelminglypreferredourmethodoverthe
baselines.
6.4 AblationStudy
Bootstrapping. Weablatedthecontributionofourbootstrappingmethodforobjectinsertion. Here,webootstrapped
2,500realimagesinto350K syntheticimages. Intheablation,wecompareourfullmethodtofinetuningtheoriginal
backboneontheoriginalcounterfactualdatasetwithoutbootstrapping.Bothmodelsusedthesamepretrainingbackbone.
BoththequalitativeresultsinFig.8andthequantitativeresultsinTab.1clearlysupportbootstrapping.
Datasetsize. Collectinglargecounterfactualdatasetsisexpensive. Weevaluatetheinfluenceofdatasetsizeonthe
performanceofourobjectremovalmethod.Wefinetunethebasemodelonsubsetsofthefullcounterfactualdatasetwith
varyingsizes. Fig.9showsthatusingthepre-trainedinpaintingmodelwithoutfinetuning("0samples")merelyreplaces
thetargetobjectbyanothersimilarone. Also,itseffectsonthesceneremain. Theresultsstartlookingattractivearound
1000counterfactualexamples,moreexamplesfurtherimproveperformance.
Text-to-imagevs. inpaintingpretrainedmodel. Fig.10demonstratesthatusingatext-to-image(T2I)insteadof
aninpaintingmodelforpre-trainingobtainscomparablequalityforremovingshadowsandreflections. Thisshows
that inpainting models do not have better inductive bias for modeling object effects on scenes than T2I models.
Unsurprisingly, the inpainting model is better at inpainting the pixels occluded by the objects. Furthermore, we
comparedthepre-trainedmodels(notshown)onobjectinsertion. Consequently,weusedtheinpaintingbackbonefor
theobjectremovalexperimentsandtheT2Ibackbonefortheobjectinsertionexperimentsinthepaper.
10ObjectDrop: PhotorealisticObjectRemovalandInsertion
Input Image and Mask Text-to-Image Pre-train Inpainting Pre-train Input Image and Mask Text-to-Image Pre-train Inpainting Pre-train
Figure10: Inpaintingvs. Text-to-Imagepre-training. Theinpaintingmodelhasbetterresultsonpixelsoccludedby
theobjectsbutresultsincomparablequalityforremovingoraddingphotorealisticreflectionsandshadows.
Table4:Objectinsertion-userstudy. Acomparisononin-distribution(ID)andout-of-distribution(OOD)intra-image
objectinsertiondatasets.
Held-OutSet(ID) In-the-Wild(OOD)
AnyDoor 11.1% Paint-by-Example 3.3% AnyDoor 5.0% Paint-by-Example 2.8%
Ours 88.9% Ours 96.7% Ours 95.0% Ours 97.2%
Table5: Stable-Diffusionresults. OurmethodworkswellonthepublicStable-Diffusion-Inpaintingv1model.
Model PSNR↑ DINO↑ CLIP↑ LPIPS↓
SDInpainting 19.198 0.775 0.884 0.083
OursSD 21.363 0.876 0.930 0.076
Publicmodels. Weverifiedthatourmethodworksonpubliclyavailablemodels. Here,wetrainedourmodelusing
Stable-Diffusion-Inpainting v1 as the pre-trained backbone. We then computed the quantitative metrics for object
removalasinSec.6.2. OurresultsinTab.5showthatourmethodimprovesthispretrainedmodelsignificantly.
7 Limitations
Thisworkfocusesonsimulatingtheeffectthatanobjecthasonthescene,butnottheeffectofthesceneontheobject.
Asaresult,ourmethodmayyieldunrealisticresultsinscenarioswheretheorientationandlightingoftheobjectare
incompatiblewiththescene. Thiscanbesolvedindependentlyusingexistingharmonizationmethods,butthiswas
notexploredinthecontextofthiswork. Additionally,asourmodeldoesnotknowthephysical3Dsceneandlighting
perfectly,itmayresultinrealistic-lookingbutincorrectshadowdirections.
8 Conclusion
WeintroducedObjectDrop, asupervisedapproachforobjectremovalandinsertiontoovercomethelimitationsof
previous self-supervised approaches. We collected a counterfactual dataset consisting of pairs of images before
andafterthephysicalmanipulationoftheobject. Duetothehighcostofobtainingsuchadataset, weproposeda
bootstrapsupervisionmethod. Finally,weshowedthroughcomprehensiveevaluationthatourapproachoutperformsthe
state-of-the-art.
9 Acknowledgement
WewouldliketothanktoGitarthaGoswami,SoumyadipGhosh,ReggieBallesteros,SrimonChatterjee,MichaelMilne
andJamesAdamsonforprovidingthephotographsthatmadethisprojectpossible. WethankYaronBrodsky,Dana
Berman,AmirHertz,MoabArar,andOrenKatzirfortheirinvaluablefeedbackanddiscussions. Wealsoappreciatethe
insightsprovidedbyDaniLischinskiandDanielCohen-Or,whichhelpedimprovethiswork.
11ObjectDrop: PhotorealisticObjectRemovalandInsertion
References
[1] OmriAvrahami,DaniLischinski,andOhadFried. Blendeddiffusionfortext-driveneditingofnaturalimages. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages18208–18218,
2022. 3
[2] OmerBar-Tal,DolevOfri-Amar,RafailFridman,YoniKasten,andTaliDekel. Text2live: Text-drivenlayered
imageandvideoediting. InEuropeanconferenceoncomputervision,pages707–723.Springer,2022. 3
[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing
instructions. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
18392–18402,2023. 3
[4] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot
object-levelimagecustomization. arXivpreprintarXiv:2307.09481,2023. 3,7,10
[5] YunjeyChoi,YoungjungUh,JaejunYoo,andJung-WooHa. Starganv2: Diverseimagesynthesisformultiple
domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
8188–8197,2020. 5
[6] XiaodongCun,Chi-ManPun,andChengShi. Towardsghost-freeshadowremovalviadualhierarchicalaggrega-
tionnetworkandshadowmattinggan. InProceedingsoftheAAAIConferenceonArtificialIntelligence,pages
10680–10687,2020. 3
[7] BinDing,ChengjiangLong,LingZhang,andChunxiaXiao. Argan: Attentiverecurrentgenerativeadversarial
networkforshadowdetectionandremoval.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,pages10213–10222,2019.
[8] LanFu,ChangqingZhou,QingGuo,FelixJuefei-Xu,HongkaiYu,WeiFeng,YangLiu,andSongWang. Auto-
exposurefusionforsingle-imageshadowremoval. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10571–10580,2021. 3
[9] Tsu-JuiFu,WenzeHu,XianzhiDu,WilliamYangWang,YinfeiYang,andZheGan. Guidinginstruction-based
imageeditingviamultimodallargelanguagemodels. arXivpreprintarXiv:2309.17102,2023. 3
[10] Aviv Gabbay, Niv Cohen, and Yedid Hoshen. An image is worth more than a thousand words: Towards
disentanglementinthewild. AdvancesinNeuralInformationProcessingSystems,34:9216–9228,2021. 5
[11] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,AaronCourville,
andYoshuaBengio. Generativeadversarialnets. Advancesinneuralinformationprocessingsystems,27,2014. 3
[12] LanqingGuo,ChongWang,WenhanYang,SiyuHuang,YufeiWang,HanspeterPfister,andBihanWen. Shad-
owdiffusion:Whendegradationpriormeetsdiffusionmodelforshadowremoval. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages14049–14058,2023. 3
[13] AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDanielCohen-Or. Prompt-to-prompt
imageeditingwithcrossattentioncontrol. arXivpreprintarXiv:2208.01626,2022. 5
[14] YanHong,LiNiu,andJianfuZhang.Shadowgenerationforcompositeimageinreal-worldscenes.InProceedings
oftheAAAIconferenceonartificialintelligence,pages914–922,2022. 3
[15] XiaoweiHu,YitongJiang,Chi-WingFu,andPheng-AnnHeng. Mask-shadowgan: Learningtoremoveshadows
from unpaired data. In Proceedings of the IEEE/CVF international conference on computer vision, pages
2472–2481,2019. 3
[16] ZhengHui,JieLi,XiumeiWang,andXinboGao.Imagefine-grainedinpainting.arXivpreprintarXiv:2002.02609,
2020. 3
[17] AapoHyvärinenandPetteriPajunen. Nonlinearindependentcomponentanalysis: Existenceanduniqueness
results. Neuralnetworks,12(3):429–439,1999. 2,4
[18] SatoshiIizuka,EdgarSimo-Serra,andHiroshiIshikawa. Globallyandlocallyconsistentimagecompletion. ACM
TransactionsonGraphics(ToG),36(4):1–14,2017. 3
[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarialnetworks. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
1125–1134,2017. 3
[20] YeyingJin,AashishSharma,andRobbyTTan. Dc-shadownet: Single-imagehardandsoftshadowremovalusing
unsuperviseddomain-classifierguidednetwork. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages5027–5036,2021. 3
12ObjectDrop: PhotorealisticObjectRemovalandInsertion
[21] Jonathan Kahana and Yedid Hoshen. A contrastive objective for learning disentangled representations. In
EuropeanConferenceonComputerVision,pages579–595.Springer,2022. 5
[22] IlyesKhemakhem,DiederikKingma,RicardoMonti,andAapoHyvarinen.Variationalautoencodersandnonlinear
ica: Aunifyingframework. InInternationalConferenceonArtificialIntelligenceandStatistics,pages2207–2217.
PMLR,2020. 2,4,5
[23] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,Spencer
Whitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. arXivpreprintarXiv:2304.02643,2023. 3,
6,16
[24] SumithKulal,TimBrooks,AlexAiken,JiajunWu,JimeiYang,JingwanLu,AlexeiAEfros,andKrishnaKumar
Singh. Putting people in their place: Affordance-aware human insertion into scenes. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages17089–17099,2023. 7
[25] Hieu Le and Dimitris Samaras. Shadow removal via shadow image decomposition. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pages8578–8587,2019. 3
[26] HieuLeandDimitrisSamaras. Fromshadowsegmentationtoshadowremoval. InComputerVision–ECCV2020:
16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartXI16,pages264–281.Springer,
2020. 3
[27] DavidK.Lewis. Counterfactuals. Blackwell,Malden,Mass.,1973. 2
[28] DaquanLiu,ChengjiangLong,HongpanZhang,HanningYu,XinzhiDong,andChunxiaXiao. Arshadowgan:
Shadow generative adversarial network for augmented reality in single light scenes. In Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages8139–8148,2020. 3
[29] GuilinLiu,FitsumAReda,KevinJShih,Ting-ChunWang,AndrewTao,andBryanCatanzaro. Imageinpainting
forirregularholesusingpartialconvolutions. InProceedingsoftheEuropeanconferenceoncomputervision
(ECCV),pages85–100,2018. 3
[30] HongyuLiu,BinJiang,YibingSong,WeiHuang,andChaoYang. Rethinkingimageinpaintingviaamutual
encoder-decoder with feature equalizations. In Computer Vision–ECCV 2020: 16th European Conference,
Glasgow,UK,August23–28,2020,Proceedings,PartII16,pages725–741.Springer,2020. 3
[31] HaotianLiu, ChunyuanLi, QingyangWu, andYongJaeLee. Visualinstructiontuning. Advancesinneural
informationprocessingsystems,36,2024. 3
[32] ZhihaoLiu,HuiYin,XinyiWu,ZhenyaoWu,YangMi,andSongWang. Fromshadowgenerationtoshadow
removal. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
4927–4936,2021. 3
[33] FrancescoLocatello,StefanBauer,MarioLucic,GunnarRaetsch,SylvainGelly,BernhardSchölkopf,andOlivier
Bachem. Challengingcommonassumptionsintheunsupervisedlearningofdisentangledrepresentations. In
internationalconferenceonmachinelearning,pages4114–4124.PMLR,2019. 2,4
[34] ErikaLu,ForresterCole,TaliDekel,AndrewZisserman,WilliamTFreeman,andMichaelRubinstein.Omnimatte:
Associatingobjectsandtheireffectsinvideo. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages4507–4515,2021. 3
[35] AndreasLugmayr,MartinDanelljan,AndresRomero,FisherYu,RaduTimofte,andLucVanGool. Repaint:
Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pages11461–11471,2022. 3
[36] KangfuMei,LuisFigueroa,ZheLin,ZhihongDing,ScottCohen,andVishalMPatel. Latentfeature-guided
diffusionmodelsforshadowremoval. InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsof
ComputerVision,pages4313–4322,2024. 3
[37] ChenlinMeng, YutongHe, YangSong, JiamingSong, JiajunWu, Jun-YanZhu, andStefanoErmon. Sdedit:
Guidedimagesynthesisandeditingwithstochasticdifferentialequations. arXivpreprintarXiv:2108.01073,2021.
3
[38] Evangelos Ntavelis, AndrésRomero, SiavashBigdeli, RaduTimofte, ZhengHui, XiumeiWang, Xinbo Gao,
ChajinShin,TaeohKim,HanbinSon,etal. Aim2020challengeonimageextremeinpainting. InComputer
Vision–ECCV2020Workshops: Glasgow,UK,August23–28,2020,Proceedings,PartIII16,pages716–741.
Springer,2020. 3
[39] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,PierreFernandez,
DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Learningrobustvisualfeatureswithout
supervision. arXivpreprintarXiv:2304.07193,2023. 3,9
13ObjectDrop: PhotorealisticObjectRemovalandInsertion
[40] DeepakPathak,PhilippKrahenbuhl,JeffDonahue,TrevorDarrell,andAlexeiAEfros. Contextencoders: Feature
learningbyinpainting. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
2536–2544,2016. 3
[41] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,
AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfromnaturallanguage
supervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,2021. 3,9
[42] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchicaltext-conditionalimage
generationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022. 3
[43] YuruiRen,XiaomingYu,RuonanZhang,ThomasHLi,ShanLiu,andGeLi. Structureflow: Imageinpaintingvia
structure-awareappearanceflow. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,
pages181–190,2019. 3
[44] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolutionimage
synthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pages10684–10695,2022. 3
[45] ChitwanSaharia,WilliamChan,HuiwenChang,ChrisLee,JonathanHo,TimSalimans,DavidFleet,andMo-
hammadNorouzi. Palette: Image-to-imagediffusionmodels. InACMSIGGRAPH2022ConferenceProceedings,
pages1–10,2022. 3
[46] ShellySheynin,AdamPolyak,UrielSinger,YuvalKirstain,AmitZohar,OronAshual,DeviParikh,andYaniv
Taigman. Emuedit: Preciseimageeditingviarecognitionandgenerationtasks. arXivpreprintarXiv:2311.10089,
2023. 3,5,9,10
[47] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsupervisedlearningusing
nonequilibriumthermodynamics. InInternationalconferenceonmachinelearning,pages2256–2265.PMLR,
2015. 3
[48] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBenPoole. Score-
basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprintarXiv:2011.13456, 2020.
3
[49] YizhiSong,ZhifeiZhang,ZheLin,ScottCohen,BrianPrice,JianmingZhang,SooYeKim,andDanielAliaga.
Objectstitch:Objectcompositingwithdiffusionmodel. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages18310–18319,2023. 3
[50] RomanSuvorov,ElizavetaLogacheva,AntonMashikhin,AnastasiaRemizova,ArseniiAshukha,AlekseiSilve-
strov,NaejinKong,HarshithGoka,KiwoongPark,andVictorLempitsky. Resolution-robustlargemaskinpainting
withfourierconvolutions. InProceedingsoftheIEEE/CVFwinterconferenceonapplicationsofcomputervision,
pages2149–2159,2022. 3
[51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundationlanguage
models. arXivpreprintarXiv:2302.13971,2023. 3
[52] JinWan, HuiYin, ZhenyaoWu, XinyiWu, YantingLiu, andSongWang. Style-guidedshadowremoval. In
EuropeanConferenceonComputerVision,pages361–378.Springer,2022. 3
[53] JifengWang,XiangLi,andJianYang. Stackedconditionalgenerativeadversarialnetworksforjointlylearning
shadowdetectionandshadowremoval. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages1788–1797,2018. 3
[54] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa
Onoe,SarahLaszlo,DavidJFleet,RaduSoricut,etal. Imageneditorandeditbench: Advancingandevaluating
text-guidedimageinpainting. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages18359–18369,2023. 3
[55] TianyuWang,XiaoweiHu,QiongWang,Pheng-AnnHeng,andChi-WingFu. Instanceshadowdetection. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages1880–1889,2020.
3
[56] ChenfeiWu,JianLiang,XiaoweiHu,ZheGan,JianfengWang,LijuanWang,ZichengLiu,YuejianFang,and
NanDuan. Nuwa-infinity: Autoregressiveoverautoregressivegenerationforinfinitevisualsynthesis. arXiv
preprintarXiv:2207.09814,2022. 3
[57] BinxinYang,ShuyangGu,BoZhang,TingZhang,XuejinChen,XiaoyanSun,DongChen,andFangWen. Paint
byexample: Exemplar-basedimageeditingwithdiffusionmodels. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages18381–18391,2023. 3,10
14ObjectDrop: PhotorealisticObjectRemovalandInsertion
[58] YanhongZeng, JianlongFu,HongyangChao,andBainingGuo. Learningpyramid-contextencodernetwork
forhigh-qualityimageinpainting. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages1486–1494,2019. 3
[59] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonableeffectiveness
ofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages586–595,2018. 9
[60] ShuyangZhang,RunzeLiang,andMiaoWang. Shadowgan: Shadowsynthesisforvirtualobjectswithconditional
adversarialnetworks. ComputationalVisualMedia,5:105–115,2019. 3
[61] ShuZhang,XinyiYang,YihaoFeng,CanQin,Chia-ChihChen,NingYu,ZeyuanChen,HuanWang,Silvio
Savarese,StefanoErmon,etal. Hive: Harnessinghumanfeedbackforinstructionalvisualediting. arXivpreprint
arXiv:2303.09618,2023. 3
[62] YuruiZhu,JieHuang,XueyangFu,FengZhao,QibinSun,andZheng-JunZha. Bijectivemappingnetworkfor
shadowremoval. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages5627–5636,2022. 3
[63] YuruiZhu,ZeyuXiao,YanchiFang,XueyangFu,ZhiweiXiong,andZheng-JunZha. Efficientmodel-driven
networkforshadowremoval. InProceedingsoftheAAAIconferenceonartificialintelligence,pages3635–3643,
2022. 3
A TrainingandInference
A.0.1 ObjectRemoval
Inourobjectremovaltrainingprocess,weutilizeapre-trainedtext-to-imagelatentdiffusionmodel(LDM)thatwas
furthertrainedforinpainting. Givenanimageofanobject("factual")anditsmask,wefinetunetheLDMtodenoisean
imageofthesamescenewithouttheobject(thecounterfactualimage). Weperformed50,000optimizationstepswith
batchsizeof128imagesandlearningrateof1e–5.
A.0.2 ObjectInsertion
Totrainourobjectinsertionmodel,wefirstfinetunethemodelusingasyntheticdatasetasdescribedinSection5. This
initialtrainingphaseconsistsof100,000optimizationsteps,employingabatchsizeof512imagesandalearningrate
of5e–5. Subsequently,wefine-tunethemodelonouroriginalcounterfactualdatasetforanadditional40,000steps,
withbatchsizeof128anddecayinglearningrates.
ThedenoiserfunctionD (x ,x ,m,t,p)receivesthefollowinginputs:
θ t cond
• x : Noisedlatentrepresentationoftheimagecontainingtheobject.
t
• x : Latentrepresentationoftheobjectpastedontoabackgroundimageasis,withoutitseffectsonthe
cond
scene.
• m: Maskindicatingtheobject’slocation.
• t: Timestamp.
• p: Encodingofanemptystring(textprompt).
A.1 Inference
Allimagesinthispaperweregeneratedatresolutionof512×512,with50denoisingsteps.
B Bootstrapping
Thebootstrappingprocedureforcreatingtheobjectinsertiontrainingset,asoutlinedinSection5,followsthesesteps:
Webeginwithanexternaldatasetof14millionimagesandextractforegroundsegmentationforeach. Wefilterout
imageswheretheforegroundmaskcoverslessthan5%ormorethan50%ofthetotalimagearea,aimingtoexclude
objectsthatareeithertoosmallortoolarge. Additionally,weeliminateimageswheretheforegroundobjectextends
acrossmorethan20%ofthelowerimageboundary,astheshadoworreflectionoftheseobjectsisoftennotvisible
withintheimage. Thisfilteringprocessresultsin700,000imagespotentiallycontainingsuitableobjectsforremoval.
15ObjectDrop: PhotorealisticObjectRemovalandInsertion
Usingourobjectremovalmodel,wegeneratepredictedbackgroundimages. However,inmanyoftheoriginalimages,
theobjectdoesnothaveasignificantshadoworreflection,sothatthedifferencebetweenthesynthesizedinputand
outputpairsconsistsofnoise. Toaddressthis,wefurtherdiscardimageswheretheareashowingsignificantdifferences
betweentheobjectimageandthepredictedbackgroundistoosmall. Thisyieldsourfinalbootstrappeddatasetof
350,000examples.
C EvaluationDatasets
Toassessourobjectinsertionmodel,weemployedtwodatasets. Thefirst,referredtoastheheld-outdataset,comprises
51tripletsofphotostakenafterthecompletionoftheproject. Eachtripletconsistsof: (1)ascenewithouttheobject,(2)
thesamescenewithanaddedobject,and(3)anotherimageofthesamesceneandthesameobjectplacedelsewhere. We
automaticallysegmented[23]theaddedobjectandrelocateditwithintheimagebynaivelypastingitonthebackground
sceneimage. Themodel’sinputsconsistoftheimagewiththepastedobjectanditscorrespondingmask. Thisdataset,
alongwithourresults,arepresentedinFig. 13. Withgroundtruthimagesillustratinghowobjectmovementshould
appear,weconductedquantitativemetricassessmentsanduserstudies. Additionally,weusedthisdatasetforevaluating
theobjectremovalmodel. Inthistest,weremovedtheobjectandcomparedthegeneratedimagetothegroundtruth
backgroundimage.
Thesecondtestset,utilizedforobjectinsertion,comprises50examples,includingsomeout-of-distributionimages
intendedformovinglargeobjects,asshowninFig. 12. Asthisdatasetislackinggroundtruthimages,weusedthis
datasetsolelyforuserstudy.
D UserStudy
Toassesstheeffectivenessofourobjectremovalmodel,weconductedauserstudyusingthetestsetprovidedbyEmu
Editof264examples,asshowninFig. 11. WecomparedourresultsseparatelywiththoseofEmuEditandMGIE.
UtilizingtheCloudResearchplatform,wecollecteduserpreferencesfrom50randomlyselectedparticipants. Each
participantreviewed30examplesconsistingofanoriginalimage,removalinstructions,andtheoutcomesproducedby
bothourmethodandthebaseline. Werandomizedboththeorderoftheexamplesshownandtheorderofeachmodelin
eachexample. Toimprovethereliabilityoftheresponses,weduplicatedafewquestions,andremovedquestionnaires
thatshowedinconsistencyforthoserepeatedquestions. Asimilaruserstudywascarriedouttocompareourobject
insertionmodelwithAnyDoorandPaint-by-Example,usingthedatasetsdescribedinSectionC.Differentparticipants
wereusedforeachdatasetandcomparisonwithbaselines. ThemajorityofparticipantswerelocatedintheUnited
States. Participantwerecompensatedabovetheminimumwage.
16ObjectDrop: PhotorealisticObjectRemovalandInsertion
Instruction Input Image Text-based Mask Emu Edit MGIE Ours
Remove the cat
from the photo
Remove the
bag from the
grass.
Remove the
laptop from the
desk
Remove the
baseball from
inside the
glove.
Delete the vent
hood on top of
the oven.
Get rid of the
bulls at the
bottom of the
picture.
Remove the
bowl from on
top of the plate
at the right side
of the image.
Remove the
brown goat
from the
image.
Figure11: Additionalexamplesforcomparisonwithgeneraleditingmethods,EmuEditandMGIE.Inthiscomparison,
weutilizedatext-basedsegmentationmodeltogenerateamaskfortheobjectbasedongiveninstructions,whichwas
thenusedasinputforourmodel.
17ObjectDrop: PhotorealisticObjectRemovalandInsertion
Input Paint-by-Example AnyDoor Ours
Figure12: Additionalexamplesofinta-imageobjectinsertion.
18ObjectDrop: PhotorealisticObjectRemovalandInsertion
Model Conditions
Background Original Location Model Output Ground Truth
Mask New Location
Figure13: Ourheld-outtestset. Theobjectinsertionmodelusestwoconditions: (1)Animagewheretheobjectwas
pastednaivelyonthebackgroundand(2)amaskofthatobject.
19ObjectDrop: PhotorealisticObjectRemovalandInsertion
Input Image Input Mask Inpainting Ours
Figure14: Additionalexamplesshowcasingtheperformanceofourobjectremovalmodelcomparedtotheinpainting
modelweinitializedourmodelwith.
20ObjectDrop: PhotorealisticObjectRemovalandInsertion
Input Image Input Mask Inpainting Ours
Figure15: Additionalexamplesshowcasingtheperformanceofourobjectremovalmodelcomparedtotheinpainting
modelweinitializedourmodelwith.
21ObjectDrop: PhotorealisticObjectRemovalandInsertion
Input Image Input Mask Inpainting Ours
Figure16: Additionalexamplesshowcasingtheperformanceofourobjectremovalmodelcomparedtotheinpainting
modelweinitializedourmodelwith.
22