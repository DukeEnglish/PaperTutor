TheoreticalGuaranteesfortheSubspace-Constrained Tyler’sEstimator
GiladLerman FengYu TengZhang
UniversityofMinnesota UniversityofMinnesota UniversityofCentralFlorida
lerman@umn.edu fyu@umn.edu teng.zhang@ucf.edu
Abstract
Thisworkanalyzesthesubspace-constrainedTyler’sestimator(STE)[2]designedforrecoveringalow-dimensionalsub-
spacewithinadatasetthatmaybehighlycorruptedwithoutliers.Itassumesaweakinlier-outliermodelandallowsthefraction
ofinlierstobesmallerthanafractionthatleadstocomputationalhardnessoftherobustsubspacerecoveryproblem.Itshows
thatinthissetting,iftheinitializationofSTE,whichisaniterativealgorithm,satisfiesacertaincondition,thenSTEcanef-
fectivelyrecovertheunderlyingsubspace. Itfurthershowsthatunderthegeneralizedhaystackmodel,STEinitializedbythe
Tyler’sM-estimator(TME),canrecoverthesubspacewhenthefractionofiniliersistoosmallforTMEtohandle.
1.Introduction
Thesubspace-constrainedTyler’sestimator(STE)isarecentmethodforrobustsubspacerecovery(RSR),whichwasrecently
suggestedin[2]. Inparticular,[2]explainsstatetwotheoremsforSTE.While[2]addressesthecomputervisioncommunity,
webelievethattheproofsofthesetheoremsareofindependentinteresttosomemathematiciansandthusincludetheminthis
preprint,whichwilleventuallyevolveintoacompletemanuscript.
TherestofthisintroductionreviewsSTEanditsstatedtheory: §1.1providesverybasicnotations,§1.2reviewstheSTE
algorithmof[2]and§1.3reviewsthestatementofitstheory,withtheirrequireddefinitionsandnotation.Sincethisisareview
of[2]werepeatsomeofitstextatthiscurrentversion,whichsupplements[2]. Intherestofthepaper,weclarifyatechnical
assumptionofthetheoryin§2andprovethestatedtheoremsin§3.
1.1.Notation
We use bold upperandlower case letters formatricesand columnvectors, respectively. LetI denotethe identity matrix
k
inRk×k, whereif k isobviouswejustwriteI. ForamatrixA, wedenotebytr(A)andIm(A) thetraceandimage(i.e.,
columnspace)ofA. We denotebyS (D) andS (D)the sets ofpositivesemidefiniteandpositivedefinite matricesin
+ ++
RD×D,respectively. WedenotebyO(D,d)thesetofsemi-orthogonalRD×d,i.e.,U O(D,d)ifandonlyifU RD×dand
∈ ∈
UTU=I .Forad-subspaceL,wedenotebyP theD DmatrixrepresentingtheorthogonalprojectorontoL.Thismatrix
d L
×
issymmetricandsatisfiesP2 =P andIm(P )=L. WealsoarbitrarilyfixU inO(D,d)suchthatU UT =P (such
L L L L L L L
U isdetermineduptorightmultiplicationbyanorthogonalmatrixinO(d,d)). Throughoutthepaper, = x N RDis
L X { i }i=1⊂
assumedtobeagivencentereddataset,thatis, N x =0.Werefertolineard-dimensionalsubspacesasd-subspaces.
i=1 i
P
1.2.ReviewoftheSTEAlgorithm
The STE algorithm [2] aims to use more cleverly the d-subspace informationwithin the TME framework, instead of first
estimatingthefullcovariance.AtiterationkitfollowsasimilarsteptothatofTME:
N
Z(k):= x x⊤/(x⊤(Σ(k−1))−1x ).
i i i i
i=1
X
4202
raM
72
]TS.htam[
1v85681.3042:viXraItcomputestheeigenvalues σ D ofZ(k)andreplaceseachofthebottom(D d)ofthemwithγσ ,where
{ i }i=1 − d+1,D
D
1
σ := σ . (1)
d+1,D i
D d
− i=d+1
X
It also computes the eigenvectorsof Z(k) and formsthe matrix Σ(k) with the same eigenvectorsas those of Z(k) and the
modifiedeigenvalues,scaledtohavetrace1.Ititerativelyrepeatsthisprocedureuntilthetwoestimatorsaresufficientlyclose.
Algorithm1summarizesthisprocedure.ByobservingthisalgorithmonecannotethatSTEisinvarianttoscalingofthedata.
Algorithm1STE:Subspace-ConstrainedTyler’sEstimator[2]
1: Input: X=[x 1,...,x N] RD×N:centereddatamatrix,d: subspacedimension,K:maximumnumberofiterations,τ,γ:
∈
parameters.
2:
Output:L:d-subspaceinRD
3: Σ(0)=I D/D
4: fork=1,2,...do
5: Z(k)
←
N i=1x ix⊤
i
/(x⊤
i
(Σ(k−1))−1x i)
6: [U(k),S(k),U(k)] EVD(Z(k))
7: σ i ←[SP (k)] iiandσ← d+1,D
←
D i=d+1σ i/(D −d)
8: S(k) ←diag(σ 1,...,σ d,γσ d+P1,D,...,γσ d+1,D),
9: Σ(k) U(k)S(k)(U(k))⊤/tr U(k)S(k)(U(k))⊤
←
10: Setopifk ≥Kor kΣ(k) −Σ(k− (cid:0)1) kF<τ
(cid:1)
11: endfor e e
12: L=firstdcolumnofU(k)
1.3.ReviewoftheStatedTheory
Generic Model: The generic STE theory assumes a noiseless inliers-outliers RSR model. Let L denote the underlying
∗
d-subspacein RD, = L and = be theset ofinliersandoutliers, respectively, andn = and
in ∗ out in 1 in
X X ∩ X X \X |X |
n = bethenumberofinliersandoutliers.ThefirstassumptionofthegenericSTEtheoryis
0 out
|X |
Assumption1:Anyk-subspaceofL ,1 k d,containsatmostn k/dpoints.
∗ 1
≤ ≤
Thenextassumptionusesthefollowingnotionofdimension-scaledsignal-to-noiseratio. InRSR,theratioofinliersper
outliers,n /n ,isoftenreferredtoastheSNR(signal-to-noiseratio)[6,8,9]. Thedimension-scaledSNR(DS-SNR)[2]is
1 0
obtainedwhenscalingn andn bytheirrespectivedimensions(ofL andL⊥):
1 0 ∗ ∗
n /d
DS-SNR:= 1 . (2)
n /(D d)
0
−
Zhang[13]showedthatexactrecoverybyTMEisguaranteedwheneverDS-SNR>1(assuminggeneralpositionassumptions
ontheinliersandoutliers). Infact,ourtheoreticalestimatesshowthatitisokaytoassumeinsteadDS-SNR 1. Furthermore,
≥
Hardt and Moitra [3] showed that when consideringgeneral datasets with generalposition assumptionson the inliers and
outliers,theRSRproblemisSSEhardiftheDS-SNRislowerthan1.ThesecondassumptionofthegenericSTEtheoryis
Assumption2:DS-SNR>γ,whereγ<1istheSTEparameter.
ThelastassumptionforthegenerictheoryofSTErequiresa sufficientlygoodinitializationforSTE,butalso implicitly
involvesadditionalhiddenassumptionsontheinliersandoutliers.Toformulatethenewassumptionwedefinebelowsomesome
basicconditionnumbersforgoodinitialization(whicharemorecomplicatedthantheoneforinitializationbyPCAsuggested
by[9]and[8])andalsoquantitiessimilartotheonesusedtoguaranteelandscapestabilityinthetheoryofRSR[6,7,9,14].
DefinitionsrequiredforAssumption3:RecallthatΣ(0)denotestheinitialvalueinAlgorithm1,anddenote
Σ(0) =U⊤ Σ(0)U .
L1,L2 L1 L2
Wedefinethefollowingconditionnumber
σ Σ(0) Σ(0) Σ(0)−1 Σ(0)
κ = d L∗,L∗− L∗,L⊥ ∗ L⊥ ∗,L⊥ ∗ L⊥ ∗,L∗ .
1 (cid:16) (0) (cid:17)
σ Σ
1 L⊥ ∗,L⊥
∗
(cid:16) (cid:17)TogetabetterintuitiontothisprimaryquantityofAssumption3,wefirstexpresstheinitialestimatorΣ(0),usingbasisvectors
forL andL⊥,asa2 2blockmatrix
∗ ∗ ×
Σ(0) Σ(0)
L∗,L∗ L∗,L⊥ ∗ .
(0) (0)
Σ L⊥ ∗,L∗ Σ L⊥ ∗,L⊥ ∗!
DefiningΣ′=Σ(0) Σ(0)−1 Σ(0) ,wedecomposethisblockmatrixas
L∗,L⊥ ∗ L⊥ ∗,L⊥ ∗ L⊥ ∗,L∗
Σ′ Σ( L0 ∗) ,L⊥
∗ +
Σ( L0 ∗) ,L∗−Σ′ 0
.
Σ( L0
⊥
∗)
,L∗
Σ( L0
⊥
∗)
,L⊥
∗,! 0 0!
Wenotethatthenumeratorofκ isthed-theigenvalueofthesecondmatrixintheabovesum.WeshowinLemma1(a)thatthis
1
eigenvalueispositiveifΣ(0)ispositivedefinite,whichcanbeeasilyenforced.Theconditionnumberisthustheratiobetween
thesmallestpositiveeigenvalueofthesecondmatrixofthesumandthelargesteigenvalueofthecomponentofthefirstmatrix
associatedwithL⊥. Therefore,κ expressesaratiobetweenaquantifierofad-dimensionalcomponentofΣ(0),associated
∗ 1
withL ,andaquantifieroftheprojectionontoL⊥ofafullrankcomponentofΣ(0).
∗ ∗
WealsodefineΣ in,∗astheTMEsolutiontothesetoftheprojectedinliers U L∗x x
in
Rdandthefollowingtwo
{ | ∈X }⊂
conditionnumbers
(0)
σ Σ
κ =
1 L⊥ ∗,L⊥
∗ and κ
=σ 1(Σ in,∗)
,
2 σ(cid:16) (Σ(0))(cid:17) in σ (Σ )
D d in,∗
wherewenotethatκ isanalogoustotheonein(25)of[8],wherewereplacethesamplecovariancebytheTMEestimator.
in
Ananalogtothealignmentofoutliersstatistic[7,9]forSTEis
xx⊤
= .
A
(cid:13)x∈
XXoutkU L⊥ ∗x k2
(cid:13)
(cid:13) (cid:13)
Ananalogtothestabilitystatistic[7,9]forSTEis (cid:13) (cid:13)
xx⊤
=σ ,
S d+1,D x 2
(cid:16)x X∈Xk k (cid:17)
whereσ (X)wasdefinedin(1).
d+1,D
Assumption3:ThereexistsC=C(γ,DS-SNR)>0suchthat
dκ κ
κ C in A κ + A + 2 A(1+κ ) . (3)
1 ≥ n 1 in n d1 −γ Dn −0 d γ S in !
TheexacttechnicalrequirementonC isspecifiedinourproof. Ingeneral,thelargertheRHSof(3),themorerestrictedthe
choiceofΣ(0) is. Inparticular,whenκ = , thedefinitionofκ impliesthatIm(Σ(0))=L ,sothesubspaceisalready
1 1 ∗
∞
recoveredby the initial estimate. Therefore, reducing the lower bound of κ may allow some flexibility, so a marginally
1
suboptimalinitializationcouldstillworkout.In§2weshowthatundertheasymptoticgeneralizedhaystakmodel,Assumption
3canbeinterpretedasanupperboundonthelargestprincipalanglebetweentheinitialandgroundtruthsubspaces.
GenericTheory: Thenexttheoremsuggeststhatunderassumptions1and2,STEnicelyconvergestoanestimatorthat
recoversL . ThemainsignificanceofthistheoryisthatitsassumptionscanallowDS-SNRlowerthan1forspecialinstances
∗
ofdatasets(forwhichtheassumptionshold),unlikethegeneralrecoverytheoriesof[13]and[3].
Theorem1. Underassumptions1and2,thesequenceΣ(k)generatedbySTEconvergestoU Σ U⊤ ,theTMEsolution
L∗ in,∗ L∗
forthesetofinliers .Inaddition,letL(k)bethesubspacespannedbythetopdeigenvectorsofΣ(k),thentheanglebetween
in
L(k)andL
,∠(L(kX
),L )=cos−1( UT U ),convergesr-linearlytozero.
∗ ∗ k L(k) L∗k
WediscussinsightsofthistheoryonchoicesofthealgorithmsandfurtherverifytheabovestatedadvantageofSTEover
TMEassumingacommonprobabilisticmodel.
PossiblewaysofInitialization:IfoneexpectsaninitialestimatedsubspaceLˆtohaveasufficientlysmallangleθwithL ,
∗
whereθ=∠(Lˆ,L ),thenforΣ(0):=Π +ǫIitcanbeshownthatκ >O(1/(ǫ+θ))andκ <O(1+θ). Thusonemayuse
∗ Lˆ 1 2 ǫatrustedRSRmethod,e.g.,FMS.ThechoiceΣ(0)=I(orascaledversionofit)correspondstoLˆ beingthePCAsubspace
(obtainatiteration1).Also,usingtheTMEsolutionforΣ(0)correspondstousingtheTMEsubspaceasLˆ.
Theoryunderaprobabilisticmodel: Weshowthatunderacommonprobabilisticmodel,theassumptionsofTheorem
1,whereΣ(0)isobtainedbyTME,hold. Moreover,weshowthatSTE(initializedbyTME)canrecoverthecorrectsubspace
in situations with DS-SNR< 1, whereas TME cannot recover the underlying subspace in such cases. We follow [9] and
study the Generalized Haystack Model, though for simplicity, we assume Gaussian instead of sub-Gaussian distributions.
Weassumen inliersi.i.d.sampledfromaGaussiandistributionN(0,Σ(in)/d),whereΣ(in) S (D)andL =Im(Σ(in))
1 + ∗
∈
(soΣ(in) hasdnonzeroeigenvalues),andn outliersarei.i.d.sampledfromaGaussiandistributionN(0,Σ(out)/D),where
0
Σ(out)/D S (D).Wedefinethefollowingconditionnumbersofinliers(inL )andoutliers:
++ ∗
∈
σ (Σ(in)) σ (Σ(out))
κ = 1 and κ = 1 .
in σ (Σ(in)) out σ (Σ(out))
d D
Clearly,Assumption1holdsunderthismodel.OurnexttheoremshowsthatAssumption3holdsunderthismodelwhenthe
initialestimateΣ(out)forSTEisobtainedbyTME.ItalsoshowsthatinthiscaseSTEcansolvetheRSRproblemevenwhen
DS-SNR<1,unlikeTME.Forsimplicity,weformulatethetheoryfortheasymptoticcase,whereN andthetheorem
→∞
holdsalmostsurely. ItispossibletoformulateitforaverylargeN withhighprobability,butitrequiresstatingcomplicated
constantsdependingonvariousparameters.
Theorem2. Assumedatageneratedfromtheabovegeneralizedhaystackmodel. Assumefurtherthatfor0<µ<1,which
canbearbitrarily small, d< (1 µ)D 2. Then, foranychosen0 < c < 1, which isa lowerboundforγ, there exists
0
− −
η:=η(κ ,κ ,c ,µ)<1suchthatifDS-SNR ηandΣ(0)isobtainedbyTME,thenAssumption3forΣ(0)issatisfiedwith
in out 0
≥
c <γ <η c almostsurelyasN . Consequently,theoutputoftheSTEalgorithm,initializedbyTMEandwiththe
0 0
− →∞
choiceofc <γ<η c ,recoversL . Ontheotherhand,ifΣ(out) =0andDS-SNR<1,thenthetopdeigenvectorsofTME
0
−
0 ∗ L∗,L⊥
∗ 6
donotrecoverL .
∗
Therearethreedifferentregimesthatthetheoremcovers. WhenDS-SNR 1, bothTME+STE(i.e., STEinitializedby
≥
TME)andTMEsolvetheRSRproblem.Whenη DS-SNR<1,TME+STEsolvestheRSRproblemandTMEgenerallyfails.
≤
Whenγ DS-SNR<η,TME+STEmightalsofail,butSTEwithextremelygoodinitialization(thatsatisfiesAssumption3)
≤
canstillsolvetheproblem.
Togetabasicideaofthedependenceofηonitsparameters,weremarkthatη 1ifeitherc 0,κ ,κ
0 in out
→ → →∞ →∞
orµ 0, where the parameterµ is somewhatartificialand mightbe removedwith a tighterproof. Therefore,successful
→
performanceofTME+STErequiresaDS-SNRthatiscloseto1whenγ isclosetoeither0orη(sothatc isverysmall)or
0
wheneithertheinlieroroutlierdistributionishighlynon-symmetric,thatis,eitherκ orκ islarge.
in out
2.ClarificationofAssumption3undertheAsymptoticGeneralizedHaystackModel
We try to comprehend the technical expression in the RHS of (3), which appear in Assumption 3, under the Asymptotic
GeneralizedHaystack.
We follow [9] and study the Generalized Haystack Model, though for simplicity, we assume Gaussian instead of sub-
Gaussian distributions and we also assume an asymptotic setting with sufficiently large numbers of inliers and outliers,
thoughtheoverallfractionofinlierscanbesmall. Weassumethatn inliersarei.i.d.sampledfromaGaussiandistribution
1
N(0,Σ(in)/d),whereΣ(in) S (D)andL =Im(Σ(in))(soΣ(in) hasdnonzeroeigenvalues),andn percentageofthe
+ ∗ 0
∈
outliersarei.i.d.sampledfromaGaussiandistributionN(0,Σ(out)/D),whereΣ(out) S (D).
++
∈
Letµ denotetheprojectionofN(0,Σ(out)/D)ontheunitsphere,thatis,itisthepushforwardmeasureofN(0,Σ(out)/D)
out
inducedbytheprojectiontotheunitsphereinRD.Wenotethat
xx⊤ xx⊤
n0li →m ∞nA 0= (cid:13)E x∼N(0,Σ(out)/D)
kU L⊥ ∗x k2
(cid:13)= (cid:13)E x∼µout
kU L⊥ ∗x k2
(cid:13). (4)
(cid:13) (cid:13) (cid:13) (cid:13)
Indeed,thefirstequalityfollowsfromth(cid:13)edefinitionof andthelaw(cid:13)ofla(cid:13)rgenumbersandthe(cid:13)secondonefollowsfromthe
A
invariancetoscalingofxx⊤/ kU L⊥ ∗x k2.
Letµ denotetheuniformdistributionontheunitsphereinRD. Letf andf denotetheprobabilitydensityfunctions
0 µout µ0
ofµ andµ .Furthermore,letC ,C >0denotetheoptimalconstantssuchthat
out 0 µout,u µout,l
C f f C f . (5)
µout,l µ0≤ µout≤ µout,u µ0Sinceµ istheprojectionofN(0,Σ(out)/D)ontheunitsphere,itsdensityismaximalatthetopeigenvectorofΣ(out)/D
out
anditisminimalatthesmallesteigenvectorofΣ(out)/D.Moreover,
∞ 1 exp x2 dx
C
µout,u
=
f µout(v 1(Σ(out)/D))
=
0 (2π)D/2√|Σ(out)/D| −2σ1(Σ(out)/D)
=
σ 1(Σ(out)/D))
=√κ .
C
µout,l
f µout(v D(Σ(out)/D)) R 0∞ (2π)D/2√1 |Σ(out)/D|exp(cid:0) −2σD(Σx (o2 ut)/D)(cid:1)dx sσ D(Σ(out)/D)) out
R (cid:0) (cid:1)
ThentheRHSof(4)isboundedasfollows,whilebeingexplainedbelow:
xx⊤ xx⊤ xx⊤
(cid:13)E x∼µout kU L⊥ ∗x k2 (cid:13)≤C µout,u (cid:13)E x∼µ0 kU L⊥ ∗x k2 (cid:13)=C µout,u (cid:13)E x∼N(0,I) kU L⊥ ∗x k2
(cid:13)
(6)
=C(cid:13) (cid:13) µout,umax (cid:16)(cid:13)E x∼N(cid:13) (cid:13) (0,I)U k⊤ L U∗x Lx(cid:13) (cid:13)
⊥
∗⊤ xU k2L∗ (cid:13), (cid:13)E x∼N(0(cid:13) (cid:13) ,I)U⊤ L k⊥ ∗ Ux Lx
⊥
∗⊤(cid:13) (cid:13) xU k2L⊥ ∗ (cid:13)(cid:17). (cid:13) (cid:13)
(cid:13) (cid:13)(cid:13) (cid:13)
Thefirstinequalityapplies(5).Th(cid:13)enextequalityreplacesµ (cid:13)w(cid:13)ithN(0,I)astheexpression(cid:13)isscaleinvariant.Thelastequality
0
usesthefactthat[E x∼N(0,I)kUx Lx
⊥
∗⊤ xk2] L∗,L⊥
∗
=[E x∼N(0,I)kUx Lx
⊥
∗⊤ xk2] L⊥ ∗,L∗=0,whichweclarifynext.Foranyx ∈RD,denote
x˜=P L∗x −P L⊥ ∗xandnotethatforx ∼N(0,I),x˜ ∼N(0,I).Asaresult,
xx⊤ xx⊤ x˜x˜⊤ xx⊤+x˜x˜⊤
2E x∼N(0,I)
kU L⊥ ∗x
k2=E x∼N(0,I)
(cid:16)kU L⊥ ∗x
k2+
kU L⊥ ∗x˜ k2
(cid:17)=E x∼N(0,I)
kU L⊥ ∗x k2
.
Notethatxx⊤+x˜x˜⊤=2(U L∗x)(U L∗x)⊤+2(U L⊥ ∗x)(U L⊥ ∗x)⊤,so[2(U L∗x)(U L∗x)⊤+2(U L⊥ ∗x)(U L⊥ ∗x)⊤] L∗,L∗=0
and[2(U L∗x)(U L∗x)⊤+2(U L⊥ ∗x)(U L⊥ ∗x)⊤] L⊥ ∗,L⊥
∗
=0and(6)isconcluded.
Next,weboundthetwotermsontheRHSof(6). WeclaimthatE x∼N(0,I)U⊤ L k⊥ ∗ Ux Lx
⊥
∗⊤ xU k2L⊥ ∗ isascalarmatrixofsizeD −dby
sD ph− erd e. inW Re Dfi −rs dt .n Wot ee at lh sa ot nth oi ts em tha at tr aix nyis ot rh the os ga om ne ala msE atx ri∼ xµ VD−d Rxx D⊤ −, d×w Dhe −r deµ D−d denotestheuniformdistributionontheunit
∈
E x∼µD−dxx⊤=E x∼µD−d(Vx)(Vx)⊤=V(E x∼µD−dxx⊤)V⊤.
Consequently,E x∼µD−dxx⊤ isascalarmatrix. Inotherwords,E x∼N(0,I)U⊤ L k⊥ ∗ Ux Lx
⊥
∗⊤ xU k2L⊥ ∗ isascalarmatrixofsizeD −dby
D d,andthus
−
(cid:13)E x∼N(0,I)U⊤ L k⊥ ∗ Ux Lx
⊥
∗⊤ xU k2L⊥ ∗ (cid:13)= D1 −dE x∼N(0,I)tr (cid:16)U⊤ L k⊥ ∗ Ux Lx
⊥
∗⊤ xU k2L⊥ ∗ (cid:17)= D1 −d. (7)
(cid:13) (cid:13)
SinceU⊤ xandU⊤ x(cid:13)areindependentlysampled(cid:13)fromtheGaussiandistributionsN(0,I )andN(0,I ),
L∗ L⊥ ∗ d D−d
(cid:13)E x∼N(0,I)U k⊤ L U∗x Lx
⊥
∗⊤ xU k2L∗ (cid:13)=E x∼N(0,I)
kU
L1
⊥ ∗x
k2kE x∼N(0,I)U⊤ L∗xx⊤U L∗k=
D
−1
d
−2·1=
D
−1
d
−2. (8)
(cid:13) (cid:13)
Thelastin(cid:13)equalityusesthefactthatt(cid:13)hedistributionof 1 isinverse-chi-squaredwithparameterD danditsexpectation
kU L⊥ ∗xk2 −
isthus1/(D d 2).
− −
Combining(4)and(6)-(8),weobtainthefollowingestimateof :
A
1
lim A C . (9)
n0→∞n 0≤
µout,u
·D d 2
− −
Next,weestimate asfollowsandexplainthestepsbelow:
S
xx⊤ xx⊤ xx⊤ C
n0li →m ∞nS 0≥σ d+1,D E x∼N(0,Σ(out)/D) x 2 ≥σ D E x∼µout x 2 ≥C µout,lσ D E x∼µ0 x 2 = µ Dout,l . (10)
(cid:16) k k (cid:17) (cid:16) k k (cid:17) (cid:16) k k (cid:17)Thefirstinequalityfollowsfromthedefinitionof andthelawoflargenumbers,wheretheinequalityisneededas isdefined
S S
foralldatapointsandtheRHSisfortheoutliersonly. Thesecondinequalityjustusesthedefinitionofµ andσ . The
out d+1,D
thirdinequalityapplies(5). ThelastequalityusesthefactthatE x∼µ0kx xx k⊤
2
=I/D. Toseethis,wefirstnotebysymmetrythat
E x∼µ0kx xx k⊤
2
isascalarmatrixandthenobservethattr(E x∼µ0kx xx k⊤ 2)=1.
Using these estimates we can clarify Assumption 3. We recall that (3) in Assumption 3 is κ
1
≥
C dκinA κ + A +κ2A(1+κ ) , combining the estimates of and in (9) and (10), and the lower bound
n1 in n d1−γ Dn −0
d
γS in A S
onCin(6(cid:16)2),weobtainthatasn
0
,the(cid:17)RHSof(3)hasanasymptoticupperboundof
→∞
dκ κ n
n0
κ κ D
C in out 0 κ +κ D−d−2 + 2 out (1+κ )
n 1(D −d −2) in outn d1 −γ Dn −0 d γ(D −d −2) in !
22 DS-SNR 10 γ 1 κ κ (D d) D−d κ κ D
=max 58, · − · in out − κ +κ D−d−2 + 2 out (1+κ ) . (11)
DS-SNR γ DS-SNR (D d 2) in outDS-SNR γ γ(D d 2) in
!
(cid:16) − (cid:17) − − − − −
Assumingforsimplicitythatκ ,κ =O(1),whichmeansthattheinliersandoutliersaresufficientlyspreadoutintheir
in out
respectivespaces,andD d 3,then(11)canbesimplifiedtothefollowingorder:
− ≥
22 DS-SNR 10 γ 1 1 1
max 58, · − · 1+ + , (12)
DS-SNR γ DS-SNR DS-SNR γ γ
(cid:16) − (cid:17) (cid:18) − (cid:19)
whichisintheorderof1/DS-SNR2whenγ=DS-SNR/2.
As discussed after Theorem 1, using the initialization Σ(0) := Π +ǫI with 1/∠(Lˆ,L ) bounded by (12) guarantees
Lˆ ∗
the convergenceof the STE algorithmto L . Assuming the abovesimplificationswith γ = DS-SNR/2, this requiresthat
∗
∠(Lˆ,L ) O(γ2).
∗
≤
3.ProofofMainResults
We prove the main stated theorems and provide novel theoretical foundations for understanding the STE framework. In
particular,ourtheoryclarifiessomestatementsmadeinSection1.3. WhilethetheoryofTMEmainlyreliesonitsgeodesic
convexity,STEisnotgeodesicallyconvexandwearerequiredtodevelopcompletelynoveltheoreticaltools,whichrelyon
variousgeometricinsightsinthetheoryofpositivedefinitematrices.
3.1.ProofofTheorem1
The proof is organized as follows: §3.1.1 lists definitions and notation; §3.1.2 presents several technical estimates used
throughouttheproof.
3.1.1 NotationandDefinitions
WeusethecommonnotationX X′ forX,X′ RD×D,whichmeansthatX′ Xispositivesemidefinite. Weuseσ ()
i
(cid:22) ∈ − ·
todenotethei-theigenvalueofacorrespondingmatrix. Wealsouseu ()todenotethei-theigenvalueofacorresponding
i
·
matrix.Whenthematrixisclearfromthecontext,wemayjustwriteσ andu .Whenthematrixispositivesemidefinite,then
i i
σ isalsothei-thsingularvalueofthismatrix,butingeneralthisisnottrue.
i
LetΠ (Σ)denotetheprojectorofΣontothenearestrank-dmatrix,thatis,
d
d
Π (Σ)= u (Σ)σ (Σ)u (Σ)⊤
d i i i
i=1
X
andP denotetheprojectorontothespanofthesmallestD deigenvectorsofΣ,thatis,
d+1,D
−
D
P (Σ)= u (Σ)u (Σ)⊤.
d+1,D i i
i=d+1
XWerecallournotationσ (Σ)= 1 D σ (Σ)fortheaverageofthesmallestD deigenvaluesofΣ. Wefurther
d+1,D D−d i=d+1 i −
denote
P D
Π (Σ)= u (Σ)σ (Σ)u (Σ)⊤.
d+1,D i i i
i=d+1
X
WeremarkthatweuseboldfacenotationfortheoperatorP,sinceitislinearandweidentifyitwithitsmatrixrepresentation.
WedonotusetheboldfacenotationforΠsinceitisnotalinearoperator,eventhoughitsimageisamatrix.
WedefinethefollowingoperatorsfromS toS :
+ +
xx⊤
T (Σ)= ,
1 x⊤Σ−1x
x∈X
X
T (Σ;γ)=Π (Σ)+γσ (Σ)P (Σ),
2 d d+1,D d+1,D
T(Σ)=T (T (Σ),γ).
2 1
TheproceduredescribedinAlgorithm1isthefixed-pointiterationofT uptoascalingfactor. Wewillthusexploreherethe
equivalentsubspacerecoverybythefixed-pointiterationofT.
Wefurtherdefine
xx⊤
Σ = ,
+,in x⊤Σ−1x
x X∈Xin
xx⊤
Σ = ,
+,out x⊤Σ−1x
x∈ XXout
and
Σ =Σ +Σ .
+ +,in +,out
Wenotethat
Σ =T (Σ) and T(Σ)=T (Σ ,γ). (13)
+ 1 2 +
wedefineg :S S andg :S S by
2 + + 1 + +
→ →
g 2(Σ)= Σ L∗,L⊥ ∗Σ Σ− L L⊥ ∗1 ⊥ ∗, ,L L⊥ ∗ ∗Σ L⊥ ∗,L∗ ΣΣ LL ⊥ ∗∗, ,L L⊥ ∗ ⊥ ∗!≡ (cid:18)ΣΣ LL ⊥ ∗∗, ,L L⊥ ∗ ⊥ ∗(cid:19)(cid:16)Σ− L⊥ ∗1 ,L⊥ ∗Σ L⊥ ∗,L∗,I (cid:17), (14)
g 1(Σ)=Σ g 2(Σ)= Σ L∗,L∗−Σ L∗,L⊥ ∗Σ− L⊥ ∗1 ,L⊥ ∗Σ L⊥ ∗,L∗ 0 . (15)
− 0 0
(cid:18) (cid:19)
Wedefineg 1andg 2forgeneralmatricesinS +,whereΣ L⊥ ∗,L⊥ ∗ maynotbeinvertible,byreplacingΣ L∗,L⊥ ∗Σ− L⊥ ∗1 ,L⊥ ∗Σ L⊥ ∗,L∗
withlim ǫ→0Σ L∗,L⊥ ∗(Σ L⊥ ∗,L⊥
∗
+ǫI)−1Σ L⊥ ∗,L∗.Forexample,
Σ 0
if Σ= L∗,L∗ , then g (Σ)=Σ and g (Σ)=0.
0 0 1 2
(cid:18) (cid:19)
WerecallthatΣ in,∗denotestheTMEsolutiontothesetoftheprojectedinliers U L∗x x
in
R.Wedefine
{ | ∈X }⊂
h(Σ)=σ (Σ−0.5[g (Σ)] Σ−0.5),
d in,∗ 1 L∗,L∗ in,∗
whichcorrespondstothed-theigenvalueofthepartofΣexpressedaccordingtothebasisofL withappropriatelyscaled
∗
eigenvalues.Forsimplicity,wemaydenote
Σ =g (Σ) andΣ =g (Σ).
1 1 2 2
ForanyΣ S ,wedefine
+
∈
κˆ (Σ)=
h(Σ 1)
and κˆ
(Σ)=σ 1(Σ L⊥ ∗,L⊥ ∗)
,
1 2
σ 1(Σ L⊥ ∗,L⊥ ∗) σ D(Σ)whereκˆ andκˆ canobtaininfinitevalues.Wenotethatκ =κˆ (Σ(0))andκ issimilartoκˆ (Σ(0))asfollows:
1 2 2 2 1 1
σ g (Σ(0)) σ Σ−0.5g (Σ(0))Σ−0.5
d 1 d in,∗ 1 in,∗
κ = , whereas κˆ (Σ(0))= .
1 σ (cid:16) Σ(0) (cid:17) 1 (cid:16) σ Σ(0) (cid:17)
1 L⊥ ∗,L⊥
∗
1 L⊥ ∗,L⊥
∗
(cid:16) (cid:17) (cid:16) (cid:17)
For the matrix Σ Rd×d, for example, we denote a special embedding into RD×D which agrees on its (L ,L )
in,∗ ∗ ∗
∈
component(usingourblockdecomposition)andisfullrankasfollows:
M(Σ in,∗)=U L∗Σ′U⊤ L∗+P L⊥ ∗.
3.1.2 Preliminaries:SomeUsefulResults
We formulateandproveseveralresultsthatwe will use in the mainproof. Lemma2 suggestsseveralboundsonmatrices
thatwe use. Lemma3 summarizestwo basic andwell-knownpropertiesof matrices. Lemma4 is a sharperalternativeto
Davis-Kahaninoursetting. Lemma5providesaboundonthelowesteigenvalueofablockmatrixusinginformationfrom
otherblocks. Lemma6establishesthattheoperatornormdistancesbetweenT(Σ)(orΠ (Σ ))andΣ isboundedbya
d + +,in
constantfactorof kΣ +,out k. Lastly,Lemma7establishestheexplicitformulaofΣ L⊥ ∗,L⊥ ∗ intermsofΣ L⊥ ∗,L∗,Σ L∗,L∗ and
Σ L∗,L⊥ ∗,undertheconditionthatrank(Σ)=d.Wewritetheselemmataandsubsequentproofsinorder.
Lemma1. Thefollowingpropertieshold:
(a)IfΣ S ,theng (Σ) S andg (Σ) S .IfΣ S ,thenσ (g (Σ))>0andconsequentlyκ >0ifΣ(0) S .
+ 1 + 2 + ++ d 1 1 ++
∈ ∈ ∈ ∈ ∈
(b)IfX,Σ S andtherangeanddomainofΣ XarecontainedinL ,theng (Σ) Σ X.
+ ∗ 1
∈ − (cid:23) −
ProofofLemma1. (a)Adirectapplicationof[1,Theorem1.3.3]impliesthatg (Σ) S wheneverΣ S . Inaddition,by
1 + +
∈ ∈
thedefinitionin(14),
g 2(Σ)= Σ− L⊥ ∗1 ,L⊥ ∗ IΣ L⊥ ∗,L∗ Σ L⊥ ∗,L⊥ ∗ Σ− L⊥ ∗1 ,L⊥ ∗Σ L⊥ ∗,L∗,I = Σ− L⊥ ∗1 ,L⊥ ∗ IΣ L⊥ ∗,L∗ Σ L0. ⊥ ∗5 ,L⊥ ∗ Σ− L⊥ ∗1 ,L⊥ ∗ IΣ L⊥ ∗,L∗ Σ0 L. ⊥ ∗5 ,L⊥ ∗ T
(cid:18) (cid:19) (cid:16) (cid:17) (cid:18)(cid:18) (cid:19) (cid:19)(cid:18)(cid:18) (cid:19) (cid:19)
isalsopositivesemidefinite.
IfΣ S ,then(byobservingtheRHSof(15)),
++
∈
rank(g (Σ))=D dand rank(g (Σ))=d.
2 1
−
Combiningthiswithg (Σ) S impliesthatσ (g (Σ))>0.
1 + d 1
∈
IfΣ(0) ∈S ++,thenΣ( L0 ⊥ ∗) ,L⊥ ∗ =U⊤ L⊥ ∗Σ(0)U L⊥ ∗ ∈S ++,andthusκ 1 ≡σ d(g 1(Σ(0)))/σ 1(Σ( L0 ⊥ ∗) ,L⊥ ∗)>0.
(b)SincethedomainandrangeofΣ XisinL ,wecanexpressXasfollows:
∗
−
X= X L∗,L∗ Σ L∗,L⊥ ∗ .
(cid:18)Σ L⊥ ∗,L∗ Σ L⊥ ∗,L⊥ ∗(cid:19)
SinceX ∈S +,[1,Theorem1.3.3]impliesthatX L∗,L∗(cid:23)Σ L∗,L⊥ ∗Σ− L⊥ ∗1 ,L⊥ ∗Σ L⊥ ∗,L∗,andasaresult,
[g 1(Σ) −(Σ −X)] L∗,L∗=(Σ L∗,L∗−Σ L∗,L⊥ ∗Σ− L⊥ ∗1 ,L⊥ ∗Σ L⊥ ∗,L∗) −(Σ L∗,L∗−X L∗,L∗)=X L∗,L∗−Σ L∗,L⊥ ∗Σ− L⊥ ∗1 ,L⊥ ∗Σ L⊥ ∗,L∗(cid:23)0.
Thisobservation,thefactthatg (Σ) S andtheaboveexpressionforXimplythatg (Σ) Σ X.
1 + 1
∈ (cid:23) −
Lemma2. Thefollowingboundshold:
(a)
n
h(Σ ) 1 h(Σ). (16)
+,in
≥ d(b)
xx⊤
Σ +,out (cid:22)σ 1(Σ L⊥ ∗,L⊥ ∗)
x∈
XXoutkU L⊥ ∗x k2 (17)
andconsequently,
kΣ +,out k≤Aσ 1(Σ L⊥ ∗,L⊥ ∗) and tr([Σ +,out] L⊥ ∗,L⊥ ∗) ≤n 0σ 1(Σ L⊥ ∗,L⊥ ∗). (18)
(c)
xx⊤ xx⊤
Σ σ (Σ) , Σ σ (Σ) . (19)
+,out (cid:23) D x 2 + (cid:23) D x 2
x∈ XXoutk k x X∈Xk k
ProofofLemma2. (a)Wenotethath(Σ )=σ (Σ−0.5[g (Σ )] Σ−0.5)=σ (Σ−0.5[Σ ] Σ−0.5),where
+,in d in,∗ 1 +,in L∗,L∗ in,∗ d in,∗ +,in L∗,L∗ in,∗
weusedthefactthatg (Σ )=Σ ,whichfollowsbythedefinitionofg . Toconcludethisproperty,wewillprovethat
1 +,in +,in 1
Σ−0.5[Σ ] Σ−0.5 h(Σ )n1I.
in,∗ +,in L∗,L∗ in,∗ (cid:23) 1 d
RecallthatΣ +,in = x∈Xinx⊤x Σx −⊤ 1x anddenoteM= M(Σ i− n0 ,∗.5) ≡U L∗Σ i− n0 ,∗.5U⊤ L∗+P L⊥ ∗ andX˜ in = {U L∗x:x ∈
RdbetherepresentationofthesetofinlierswithinL ,notethat
Xin }∈ P ∗
Σ−0.5[xx⊤] Σ−0.5
Σ−0.5[Σ ] Σ−0.5= in,∗ L∗,L∗ in,∗
in,∗ +,in L∗,L∗ in,∗ x⊤M(MΣM)−1Mx
x X∈Xin
Σ−0.5[xx⊤] Σ−0.5 Σ−0.5x˜x˜⊤Σ−0.5
in,∗ L∗,L∗ in,∗ = in,∗ in,∗
(cid:23)
x
X∈Xinx⊤M(Mg 1(Σ)M)−1Mx
x˜X∈X˜
inx˜⊤Σ i− n0 ,∗.5(Σ i− n0 ,∗.5[g 1(Σ)] L∗,L∗Σ i− n0 ,∗.5)−1x˜
Σ−0.5x˜x˜⊤Σ−0.5
n
h(Σ) in,∗ in,∗ h(Σ) 1 I.
(cid:23) x˜⊤Σ−1 x˜ d
x˜X∈X˜
in
in,∗
The first matrix inequality follows from the fact that g (Σ) Σ (which is obviousby the definition of g ). Indeed, this
1 1
inequalityimpliesthatΣ−0.5g (Σ)Σ−0.5 Σ−0.5ΣΣ−0.5,w(cid:22) hichleadsto(Σ−0.5g (Σ)Σ−0.5)−1 (Σ−0.5ΣΣ−0.5)−1and
in,∗ 1 in,∗ (cid:22) in,∗ in,∗ in,∗ 1 in,∗ (cid:23) in,∗ in,∗
consequentlyx⊤(Σ−0.5g (Σ)Σ−0.5)−1x x⊤(Σ−0.5ΣΣ−0.5)−1x.
in,∗ 1 in,∗ ≥ in,∗ in,∗
Thesecondequalityfollowsfromthefactthatg (Σ)isasingularmatrixwithrangeL ,andx lieinL .
1 ∗ in ∗
∈X
ThesecondinequalitysimilarlyfollowsfromthefactΣ−0.5[g (Σ)] Σ−0.5 h(Σ)I,whichisadirectconsequenceof
in,∗ 1 L∗,L∗ in,∗ (cid:23)
thedefinitionh(Σ):=σ (Σ−0.5[g (Σ)] Σ−0.5).
d in,∗ 1 L∗,L∗ in,∗
ThelastequalityappliestheassumptionthatΣ iswell-definedasaTMEsolutionto ˜(whichfollowsfromAssumption
in,∗
1andSection4of[5]),andthefollowingpropertyofTME:ifΣTMEisaTMEsolutiontoaX set Rp,then
X⊂
x⊤x
=|X|ΣTME, (20)
x⊤(ΣTME)−1x p
x∈X
X
whereforourcasep=dand ˜ =n .
in 1
|X |
(b)Equation(17)isprovedasfollows(whereweexplainthedetailsofproofbelow):
xx⊤ xx⊤ xx⊤
Σ = lim = (21)
+,out x∈ XXoutx⊤Σ−1x(cid:22)t→∞ x∈ XXoutx⊤(tP L∗+Σ)−1x x∈ XXoutxTU L⊥ ∗Σ− L⊥ ∗1 ,L⊥ ∗UT L⊥ ∗x
xx⊤ xx⊤
(cid:22) x∈ XXoutxTU L⊥ ∗ σ1(Σ LI ⊥ ∗,L⊥ ∗)UT L⊥ ∗x=σ 1(Σ L⊥ ∗,L⊥ ∗) x∈ XXoutkU L⊥ ∗x k2.
Thefirstequalityfollowsfromthe definitionofΣ . ThefirstinequalityfollowsfromthefactthattP +Σ Σ for
+,out L∗
(cid:23)
t>0andthusxTΣ−1x xT(tP +Σ)−1x. Thesecondequalityfollowsfromobservingthatlim (tP +Σ)−1=
≥
L∗ t→∞ L∗U L⊥ ∗Σ− L⊥ ∗1 ,L⊥ ∗UT L⊥ ∗. Indeed,bylettingt →∞,the3otherblocks,associatedwith(L ∗,L ∗),(L ∗,L⊥ ∗)and(L⊥ ∗,L ∗)zerooutin
theinverseandtheremainingone,thatistheoneassociatedwith(L⊥,L⊥),remainsthesameintheinverse.
∗ ∗
Lastly,(18)isconcludedbyapplyingthenormandtracetobothsidesof(17).
(c)Theproofof(19)is straightforwardandusesthe factthatΣ σ (Σ)I. We exemplifyitforboundingfrombelow
D
(cid:23)
Σ ,wheretheproofforΣ isthesame:
+,out +
xx⊤ xx⊤ xx⊤
Σ = =σ (Σ) .
+,out x⊤Σ−1x(cid:23) x⊤(σ (Σ)I)−1x D x 2
x∈ XXout x∈ XXout D x∈ XXoutk k
Lemma3. Thefollowingtwopropertieshold:
(a)Foranysemi-orthogonalmatrixU O(D,d)andsymmetricmatrixΣ RD×D,
∈ ∈
σ (Σ) σ (UTΣU) σ (Σ), σ (Σ) σ (UTΣU) σ (Σ).
1 1 D−d+1 d d D
≥ ≥ ≥ ≥
(b)ForanytwosymmetricmatricesA,B, σ (A) σ (B) A B .
i i
| − |≤k − k
The proofof part(a) followsfromthe Courant-Fischermin-maxtheoremfor the eigenvalues[10, Theorem1.3.2]. For
completenessweverifythisasfollows:
σ (Σ)= min max vTΣv= min max uTVTΣVu= min σ (VTΣV) σ (UTΣU)
D−d+1 1 1
dim(V)=dv∈V:kvk=1 V∈O(D,d)u∈Rd:kuk=1 V∈O(D,d) ≤
andthepartσ 1(Σ) σ 1(UTΣU)followsfromσ 1(Σ)=maxv:kvk=1vTΣv.
≥
Ontheotherhand,
σ (Σ)= max min vTΣv= max min uTVTΣVu= max σ (VTΣV) σ (UTΣU)
d d d
dim(V)=dv∈V:kvk=1 V∈O(D,d)u∈Rd:kuk=1 V∈O(D,d) ≥
andthepartσ d(UTΣU) σ D(Σ)followsfromσ D(Σ)=minv:kvk=1vTΣv.
≥
Forpart(b)see(1.63)of[10]anditsproofinthere.
Lemma4. IfΣ S (D)andLˆisthespanofthetopdeigenvectorsofΣ,then
+
∈
sin∠(Lˆ,L ∗) ≤2 σ 1(Σ L⊥ ∗,L⊥ ∗)/σ d(Σ L∗,L∗).
q
ProofofLemma4. ForΣ ∈S +,wedenoteκ 0=σ d(Σ L∗,L∗)/σ 1(Σ L⊥ ∗,L⊥ ∗).Usingthisnotation,weneedtoprovethat
sin∠(Lˆ,L ) 2/√κ .
∗ 0
≤
Assumeonthecontrarythatthisisnottrueandthussin∠(Lˆ⊥,L⊥ ∗)=sin∠(Lˆ,L ∗)>2/√κ 0. Thatis,thereexistsaunitvector
v ∈Lˆ⊥suchthatsin∠(v,L⊥ ∗)>2/√κ 0.Weexpressthisvectorv ∈Lˆ⊥asfollows:
v=sinθv +cosθv ,
1 2
wherev L andv L⊥. Ourassumptionthenimpliesthatsinθ>2/√κ . Forsimplicity,weassumewithoutlossof
1 ∈ ∗ 2 ∈ ∗ 0
generalitythatσ 1(Σ L⊥ ∗,L⊥ ∗)=1andthusσ d(Σ L∗,L∗)=κ 0. Wethusobtainthatv 1TΣv 1 ≥κ 0 andv 2TΣv 2 ≤1. SinceΣis
p.s.d., vTΣv vTΣv vTΣv .Consequently,
| 1 2 |≤ 1 1 2 2
p p vTΣv
=sin2θvTΣv +cos2θvTΣv 2sinθcosθvTΣv
1 1 2 2 − 1 2
2
sinθ vTΣv cosθ vTΣv
≥ 1 1 − 2 2
(cid:18) q q (cid:19)
(sinθ√κ cosθ)2
0
≥ −
>(2 1)2=1.
−We show on the otherhand that vTΣv 1 and thus obtain a contradiction, which concludesthe proof. Indeed, since
v ∈Lˆ⊥,itisspannedbythesmallestD −de≤ igenvectorsofΣ,andconsequentlyvTΣv ≤σ d+1(Σ) ≤σ 1(U⊤ L⊥ ∗ΣU L⊥ ∗)=1,
wherethesecondinequalityfollowsfromLemma3(a)).
Lemma5. AssumethatΣispositivesemidefiniteanddenotex=σ D−d(Σ L⊥ ∗,L⊥ ∗),y=σ d(Σ L∗,L∗)andz= kΣ L∗,L⊥ ∗k.Then,
(x+y) (x y)2+4z2
σ (Σ) − − (22)
D
≥ 2
p
and
if z √xy/2, then σ (Σ) min(x,y)/3. (23)
D
≤ ≥
(x+y)−√(x−y)2+4z2
ProofofLemma5. Leta= andnotethat
2
(x+y) (x y)2+4z2 (x+y) (x y)2 (x+y) (y x)
a= − − − − − − =x.
2 ≤ 2 ≤ 2
p p
Itthusfollowsfromthedefinitionofxandthisinequalitythat
(Σ aI) S .
L∗,L∗−
∈
+
Wealsonotethat
z2 2z2 (y x)+ (x y)2+4z2
x =x =x − − − =a. (24)
−y a −(y x)+ (x y)2+4z2 − 2
− − − p
Let p
Σ =
Σ L∗,L∗−aI Σ L∗,L⊥
∗
0
(cid:18)
Σ L⊥ ∗,L∗ Σ L⊥ ∗,L∗(Σ L∗,L∗−aI)−1ΣT L⊥ ∗,L∗,
(cid:19)
sothat
aI 0
Σ Σ = .
− 0 (cid:18)0 Σ L⊥ ∗,L⊥ ∗ −Σ L⊥ ∗,L∗(Σ L∗,L∗−aI)−1ΣT L⊥ ∗,L∗. (cid:19)
WenotethattheeigenvaluesofΣ Σ consistofa(withmultiplicityd)andtheeigenvaluesof
0
−
Σ L⊥ ∗,L⊥
∗
−Σ L⊥ ∗,L∗(Σ L∗,L∗−aI)−1ΣT L⊥ ∗,L∗,
whichareboundedbelowasfollows
σ D−d(Σ L⊥ ∗,L⊥
∗
−Σ L⊥ ∗,L∗(Σ L∗,L∗−aI)−1ΣT L⊥ ∗,L∗) ≥σ D−d(Σ L⊥ ∗,L⊥ ∗) −σ 1(Σ L⊥ ∗,L∗(Σ L∗,L∗−aI)−1ΣT L⊥ ∗,L∗))
x
kΣ L⊥ ∗,L∗k2
x
z2
=a.
≥ −σ (Σ ) a≥ −y a
d L∗,L∗
− −
Theabovefirstinequalityusespart(b)ofLemma3andthefactsthatΣ L∗,L∗−aI ∈S +,thusΣ L⊥ ∗,L∗(Σ L∗,L∗−aI)−1ΣT L⊥ ∗,L∗∈
S ,andconsequentlythelargesteigenvalueofthelattermatrixcoincideswithitsnorm. Thesecondinequalityfollowsfrom
+
basicobservations,inparticular,Σ L∗,L∗−aI (cid:22)σ 1(Σ L⊥ ∗,L∗(Σ L∗,L∗−aI)−1ΣT L⊥ ∗,L∗)I.Thelastequalityis(24).
WehavethusshowedthatalleigenvalueofΣ Σ areboundedbelowbya.WealsonotethatΣ S asfollows:
0 0 +
− ∈
Σ aI
Σ 0=
(cid:18)
L Σ∗ L,L
⊥
∗∗ ,L−
∗
(cid:19)(Σ L∗,L∗−aI)−1 Σ L∗,L∗−aI Σ L∗,L⊥ ∗.
(cid:0) (cid:1) T
Σ aI Σ aI
=
(cid:18)
L Σ∗ L,L
⊥
∗∗ ,L−
∗
(cid:19)(Σ L∗,L∗−aI)−0.5
! (cid:18)
L Σ∗ L,L
⊥
∗∗ ,L−
∗
(cid:19)(Σ L∗,L∗−aI)−0.5
!
.
Consequently,alleigenvaluesofΣareboundedbelowbya,thatis,σ (Σ) aandthusweproved(22).
D
≥Ifz √xy/2 max(x,y)/2,then
≤ ≤
2xy 2z2 2xy 2(√xy/2)2 xy xy min(x,y)
a= − − = ,
(x+y)+ (x y)2+4z2≥(x+y)+ x y +2z≥2max(x,y)+2z≥2max(x,y)+max(x,y) 3
− | − |
p
wherethefirstinequalityfollowsfrom a2+a2 a +a .
1 2≤ 1 2
p
Lemma6. ForanyΣ S ,
+
∈
T(Σ) Σ 2 Σ . (25)
+,in +,out
k − k≤ k k
and
Π (Σ ) Σ 2 Σ . (26)
d + +,in +,out
k − k≤ k k
ProofofLemma6. Wefirstnotethat
T(Σ) Σ = T (Σ ,γ) Σ = Π (Σ )+γσ (Σ )P (Σ ) Σ
+ 2 + + d + d+1,D + d+1,D + +
k − k k − k k − k
= γσ (Σ )P (Σ ) Π (Σ ) σ (Σ ),
d+1,D + d+1,D + d+1,D + d+1 +
k − k≤
wherethelastinequalityfollowsfromthefactthatthenonzeroeigenvaluesofγσ (Σ )P (Σ ) Π (Σ )are
d+1,D + d+1,D + d+1,D +
−
γσ (Σ ) σ (Σ ) andtheysatisfy:
d+1,D + i + d+1≤i≤D
{ − }
γσ (Σ ) σ (Σ ) γσ (Σ ) σ (Σ ) and γσ (Σ ) σ (Σ ) σ (Σ ) σ (Σ ).
d+1,D + i + d+1,D + d+1 + d+1,D + i + i + d+1 +
− ≤ ≤ − ≥− ≥−
WealsoapplyLemma3(a)andnotethat
σ d+1(Σ +) ≤σ 1([Σ +] L⊥ ∗,L⊥ ∗)=σ 1([Σ +,out] L⊥ ∗,L⊥ ∗)) ≤σ 1(Σ +,out).
Applyingtwicethetriangleinequalityandtheabovetwoequations,weobtainthedesiredestimateasfollows:
T(Σ) Σ T(Σ) Σ + Σ Σ σ (Σ )+ Σ 2 Σ ,
+,in + + +,in d+1 + +,out +,out
k − k≤k − k k − k≤ k k≤ k k
Equation26issimilarlyobtained:
Π (Σ ) Σ σ (Σ )+ Σ Σ 2 Σ . (27)
d + +,in d+1 + + +,in +,out
k − k≤ k − k≤ k k
Lemma7. IfΣ S ,rank(Σ)=dandΣ isinvertible,then
∈
+ L∗,L∗
Σ L⊥ ∗,L⊥
∗
=Σ L⊥ ∗,L∗Σ− L∗1 ,L∗Σ L∗,L⊥ ∗.
ProofofLemma7. SinceΣ S andrank(Σ)=d,Σ=UU⊤withU RD×d.Withoutlossofgenerality,weassumethatL
+ ∗
spansthefirstdcolumnvect∈ orsofU.WewriteU=[U ;U ]withU ∈Rd×dandU R(D−d)×d,sothisassumptionmeans
1 2 1 2
thatU 1U⊤ 1 =Σ L∗,L∗ andconsequentlyU 1 =Σ L0. ⊥ ∗5 ,L∗. SinceU 2U∈ ⊤ 1 =Σ L⊥ ∗,L∗, w∈ e deducethatU 2 =Σ L⊥ ∗,L∗Σ L− ⊥ ∗0. ,5 L∗.
NotingfurtherthatΣ L⊥ ∗,L⊥
∗
=U 2U⊤ 2,weconcludethelemma.
Lemma8. IfA,B S (d),then
+
∈
σ (ABA) σ2(A)σ (B) and σ (ABA) σ2(A)σ (B).
d ≥ d d d ≤ 1 dProofofLemma8. Thefirstinequalityfollowsfromthefactthat
for A S (d),σ (A)=min Ax / x . (28)
+ d
∈ x∈Rdk k k k
IfAissingular,σ (ABA)=0andtheinequalityistrivial.Otherwise,weformthevectoru¯:=A−1u (B)andnotethat
d d
1
u¯⊤(ABA)u¯=σ (B) and u¯ ,
d
k k≥σ (A)
1
wheretheinequalityabovefollowsfrom(28)appliedtoA−1,whereσ (A−1)=σ (A)−1.Consequently,
d 1
u¯⊤ABAu¯
σ ABA σ2(A)σ (B).
d ≤ u¯ 2 ≤ 1 d
k k
(cid:0) (cid:1)
3.1.3 Reductionofthetheorem
Wereducetheproofofthetheoremtoprovinganotherstatement. WerecallthatAssumption3requiresasufficientlylarge
constantC,so(3)issatisfied.Usingthisconstantwedefine
d κ
2
κ˜ =C A κ + A + A(1+κ ) ,
1 n 1σ d(Σ in,∗) in n d1 −γ Dn −0 d γ S in !
Wenotethat(3)ofAssumption3requiresthat
κ >σ (Σ ) κ˜ .
1 1 in,∗ 1
·
Wealsodefine
n1
C =2 d (29)
0 n1+γ n0
d D−d
andnotethatAssumption2impliesthatC >1.Usingtheseconstantsandnotationanddefinitionsfrom§3.1.1,weformulate
0
thealternativestatementasfollows:
Ifκˆ (Σ) κ˜ andκˆ (Σ) max(κ ,7),thenκˆ (T(Σ)) C κˆ (Σ)andκˆ (T(Σ)) max(κ ,7). (30)
1 1 2 2 1 0 1 2 2
≥ ≤ ≥ ≤
Wewillshownextthat(30)impliestheconclusionofthetheorem.
Wefirstnotethatκˆ (Σ(0))>κ˜ .Indeed,directapplicationofLemma8yields
1 1
σ (Σ(0))
σ (Σ−0.5Σ(0)Σ−0.5) σ (Σ−0.5)σ (Σ(0))σ (Σ−0.5)= d
d in,∗ in,∗ ≥ d in,∗ d d in,∗ σ (Σ )
1 in,∗
andconsequently
σ Σ−0.5g (Σ(0))Σ−0.5 σ g (Σ(0))
κˆ (Σ(0))= d in,∗ 1 in,∗ d 1 = κ 1 >κ˜ . (31)
1 (cid:16) σ Σ(0) (cid:17)≥ σ (Σ (cid:16) )σ Σ(0(cid:17)) σ 1(Σ in,∗) 1
1 L⊥ ∗,L⊥
∗
1 in,∗ 1 L⊥ ∗,L⊥
∗
(cid:16) (cid:17) (cid:16) (cid:17)
Furthermore, since κˆ (Σ(0)) = κ , both conditions of (30) are satisfied and we can assume the conclusion of (30) for
2 2
Σ(k+1)=T(Σ(k)),k 0.
≥
Wenotethat(30)and(31)implythatκˆ (Σ(k)) andtheconvergencetoinfinityislinear.Wefurthernotethat
1
→∞
Σ L∗,L⊥ ∗Σ− L⊥ ∗1 ,L⊥ ∗Σ L⊥ ∗,L∗=(Σ L∗,L⊥ ∗Σ L− ⊥ ∗0. ,5 L⊥ ∗)(Σ L∗,L⊥ ∗Σ L− ⊥ ∗0. ,5 L⊥ ∗)T ∈S +
andthus
Σ L∗,L∗(cid:23)g 1(Σ)
≡
Σ L∗,L∗−Σ L∗,L⊥ ∗ 0Σ− L⊥ ∗1 ,L⊥ ∗Σ L⊥ ∗,L∗ 00
(cid:18) (cid:19)andconsequently,
σ d(Σ L∗,L∗)/σ 1(Σ L⊥ ∗,L⊥ ∗) ≥σ d(g 1(Σ))/σ 1(Σ L⊥ ∗,L⊥ ∗)=κˆ 1(Σ). (32)
Combining (32) with Lemma 4, we conclude that sin∠(L(k),L ) 2/ κˆ (Σ(k)). This observation and the fact
∗ 1
κˆ (T(Σ)) C κˆ (Σ)(accordingto(30))implythat∠(L(k),L )converg≤ esr-linearlytozero.
1 ≥ 0 1 ∗ p
Next,weshowthatΣ(k)convergestoasingularmatrix.Wefirstobservethat
σ 1([Σ(k)] L⊥ ∗,L⊥ ∗)=σ 1([ tΣ r((k Σ)] (L k)⊥ ∗ ),L⊥ ∗) = tr([Σ(k)]σ L1 ∗( ,[ LΣ ∗)(k +)] tL r⊥ ∗ ([, ΣL⊥ ∗ (k)
)] L⊥ ∗,L⊥ ∗)≤σ
d(Σσ
(
Lk1 ∗)( ,[ LΣ ∗)(k +)] σL 1⊥ ∗ (, ΣL⊥ ∗
(
Lk)
⊥
∗)
,L⊥
∗)≤κˆ
1(Σ(1 k))+1,
wherethefirstequalityusesthefactthatSTEalgorithmscalesΣ(k)byitstracesotr(Σ(k))=1andthelastinequalityfollows
from(32).ApplyingfirstLemma3(a)andthentheaboveestimate,weconcludethat
1
σ d+1(Σ(k)) ≤σ 1([Σ(k)] L⊥ ∗,L⊥ ∗)
≤κˆ
(Σ(k))+1→0 as k →∞.
1
Thatis,Σ(k)convergestoasingularmatrix.
SinceΣ(k) convergestoasingularmatrix,σ (T (Σ(k)))convergestozero. Applyingthisobservationandthenthe
d+1,D 1
factthat∠(L(k),L ) 0,weobtainthatforsufficientlylargek
∗
→
Σ(k+1)=T (T (Σ(k));γ) Π (T (Σ(k)))+γσ (T (Σ(k)))P (T (Σ(k))) Π (T (Σ(k))) P T (Σ(k))P .
2 1
≡
d 1 d+1,D 1 d+1,D 1
→≈
d 1
≈
L∗ 1 L∗
RecallthatT isthestandardTMEprocedureandthusnotethattheaboveargumentimpliesthattheSTEalgorithmconverges
1
totheTMEalgorithmrestrictedtobefromL toL . SinceT (Σ(k))isaweightedsumofxxT withweight1/(xTΣ(k)−1x),
∗ ∗ 1
andforx thisweightgoesto0(sincethedomainandrangeofΣ(k)convergetoL ),
out ∗
∈X
xxT
Σ(k+1) P T (Σ(k))P P P .
≈ L∗ 1 L∗≈ L∗ xTΣ(k)−1x L∗
x X∈Xin
We also note that the iterative update formula of TME restricted to the projection of in L , that is, to
in ∗
˜ = U x:x Rdis X
Xin
{
L∗ ∈Xin
}∈
x˜x˜T
Σ(k+1)= .
X˜ in x˜TΣ(k)−1x˜
x˜X∈X˜
in
X˜
in
Wethusprovedthat[Σ(k) ]convergestheTMEsolutionoftheinliersΣ .Wealsoprovedabovether-linearconvergence
L∗,L∗ ∗,in
to0of∠(L(k),L ),soweverifiedTheorem1assuming(30)holds.Itremainstoprove(30).
∗
3.1.4 Proofof(30)
Weseparatelyprovethetwoimplicationsof(30).
Proofthatκˆ1(T(Σ))≥C 0κˆ1(Σ): Recallthat
h(T(Σ))
κˆ (T(Σ))= , where h(T(Σ))=σ (Σ−0.5[g (T(Σ))] Σ−0.5). (33)
1 σ 1([T(Σ)] L⊥ ∗,L⊥ ∗) d in,∗ 1 L∗,L∗ in,∗
We estimate thedenominatorof κˆ (T(Σ))undera specialcondition, thenits numerator,andatlast use these estimatesto
1
concludethatκˆ (T(Σ)) C κˆ (Σ).
1 0 1
Estimateofthedeno≥ minatorofκˆ1 underaspecialcondition: Wefirstobtainlowerandupperboundsforh(Σ +,in).
Sinceg (Σ )=Σ ,
1 +,in +,in
h(Σ )=σ Σ−0.5[Σ ] Σ−0.5 σ2 Σ−0.5 σ [Σ ] )σ [Σ ] /σ (Σ ), (34)
+,in d in,∗ +,in L∗,L∗ in,∗ ≥ d in,∗ d +,in L∗,L∗ d +,in L∗,L∗ 1 in,∗
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:0) (cid:1)
wheretheinequalityintheaboveequationfollowsfromLemma8andthefactthatthematricesΣ−0.5and[Σ ] are
in,∗ +,in L∗,L∗
inS (d).
+Toboundh(Σ )fromabove,weagainapplyLemma8andobtainthefollowingbounds:
+,in
σ [Σ ] σ Σ
h(Σ )=σ Σ−0.5[Σ ] Σ−0.5 d +,in L∗,L∗ = d +,in . (35)
+,in d in,∗ +,in L∗,L∗ in,∗ ≤ σ (Σ ) σ (Σ )
(cid:0) d in,∗ (cid:1) d(cid:0) in,∗(cid:1)
(cid:0) (cid:1)
Wedenote
ν=σ (Σ )/ Σ (36)
d +,in +,out
k k
andboundνfrombelowasfollows(whileexplainingthedetailsbelow):
h(Σ )σ (Σ ) n1h(Σ)σ (Σ ) n σ (Σ )
ν +,in d in,∗ d d in,∗ =κˆ (Σ) 1 d in,∗ . (37)
1
≥ kΣ +,out
k
≥ Aσ 1(Σ L⊥ ∗,L⊥ ∗) d
A
Thefirstinequalityapplies(35).ThesecondoneappliesLemma2(a)forthenumeratorandLemma2(b)forthedenominator.
Weusetheaboveestimatestoboundthedenominatorofκˆ1 fromaboveundertheconditionν 2. Wenotethat(37)and
≥
theassumptionκˆ (Σ) κ (see(30))implythatthisconditionissatisfiedwhen
1 1
≥
d
κ˜ 2 A . (38)
1
≥ n σ (Σ )
1 d in,∗
Underthisassumption,weobtainthefollowingbound,whichweclarifybelow:
σ 1([T(Σ)] L⊥ ∗,L⊥ ∗) ≤σ 1([Π d(Σ +)] L⊥ ∗,L⊥ ∗)+γσ d+1,D(Σ +)σ 1([P d+1,D(Σ +)] L⊥ ∗,L⊥ ∗) (39)
=σ 1([Π d(Σ +)] L⊥ ∗,L⊥ ∗)+γσ d+1,D(Σ +) ≤σk[Π ([d Π(Σ (Σ+)] L )⊥ ∗
]
,L∗k2 )+γσ d+1,D(Σ +)
d d + L∗,L∗
4 Σ 2 4
+,out
k k +γσ (Σ )= Σ +γσ (Σ ).
d+1,D + +,out d+1,D +
≤σ (Σ ) 2 Σ ν 2k k
d +,in +,out
− k k −
ThefirstinequalityfollowsfromthedefinitionofT andthefactthatforA S ,σ (A)isthespectralnormofA. Thefirst
+ 1
∈
equalityfollowsfromthebasicobservationthatifσ d+1,D(Σ +)>0,thenσ 1([P d+1,D(Σ +)] L⊥ ∗,L⊥ ∗)=1.Thesecondinequality
followsfromLemma7andthefactthatΠ (Σ )isofrankdasfollows:
d +
σ 1([Π d(Σ +)] L⊥ ∗,L⊥ ∗)=σ 1 [Π d(Σ +)] L⊥ ∗,L∗([Π d(Σ +)] L∗,L∗)−1[Π d(Σ +)] L∗,L⊥ ∗
≤ [Π d(Σ +)] L⊥ ∗,L∗ · ([Π d((cid:16) Σ +)] L∗,L∗)−1 · [Π d(Σ +)] L∗,L⊥ ∗ = σk[Π ([d Π(Σ (Σ+)] L )⊥ ∗ ] ,(cid:17) L∗k2 ). (40)
d d + L∗,L∗
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
Thethirdinequality(cid:13)in(39)isverified(cid:13)as(cid:13)follows.Thenumerat(cid:13)or(cid:13)usesthebound (cid:13)
k[Π d(Σ +)] L⊥ ∗,L∗k= k[Π d(Σ +) −Σ +,in] L⊥ ∗,L∗k≤k[Π d(Σ +) −Σ +,in] k≤2 kΣ +,out k,
wherethelastinequalityisobtainedby(26).Theboundforthedenominatorapplies(26)withLemma3(b)asfollows:
σ ([Π (Σ )] ) σ (Σ ) [Π (Σ )] Σ 2 Σ .
|
d d + L∗,L∗
−
d +,in
|≤k
d + L∗,L∗− +,in
k≤ k
+,out
k
Thelastequalityof(39)followsfromthedefinitionofν.
ThesecondtermintheRHSof(39)canbefurtherboundedasfollows:
D
1
σ (Σ )= σ (Σ )= min tr(VTΣ V)
d+1,D + i + +
D d V∈O(D,D−d)
− i=d+1
X
1 1 n
≤D
dtr(U⊤ L⊥ ∗Σ +U L⊥ ∗)=
D
dtr(U⊤ L⊥ ∗Σ +,outU L⊥ ∗)
≤D
0 dσ 1(Σ L⊥ ∗,L⊥ ∗). (41)
− − −
Thesecondequalityisthesecondextremalpartialtraceformulain[10,Proposition1.3.4]andthesecondinequalityapplies
thetraceboundin(18).ThethirdequalityisbecausetheinlierslieonL ∗andthusU⊤ L⊥ ∗Σ +,inU L⊥ ∗ =0.CombiningtheboundonΣ in(18),(39)and(41),weconcludetheupperboundonthedenominatorin(33):
+,out
4 n
σ 1([T(Σ)] L⊥ ∗,L⊥ ∗)
≤ν
2Aσ 1(Σ L⊥ ∗,L⊥ ∗)+γ
D
0 dσ 1(Σ L⊥ ∗,L⊥ ∗). (42)
− −
Estimateofthenumeratorofκˆ1:Wenotethat
k[T(Σ)] L∗,L∗−[g 1(T(Σ))] L∗,L∗) k= k[T(Σ)] L∗,L⊥ ∗([T(Σ)] L⊥ ∗,L⊥ ∗)−1[T(Σ)] L⊥ ∗,L∗k≤σ Dk −[T d(( [Σ T) (] ΣL∗ ), ]L L⊥ ∗
⊥
∗k ,L2
⊥
∗). (43)
Indeed, the first equality followsby the definition of g and the first inequalityuses the sub-multiplicativepropertyof the
1
spectralnormandbasicestimates,similartoverifyingthesecondlineof(40).
Inaddition,wenotethat
[T(Σ)] [Σ ] = [T(Σ) Σ ] T(Σ) Σ 2 Σ , (44)
k
L∗,L∗− +,in L∗,L∗k
k −
+,inL∗,L∗k≤k
−
+,in
k≤ k
+,out
k
wherethelastinequalityapplies(25).Next,weboundh(T(Σ))usingargumentsdetailedbelow:
h(T(Σ))=σ (Σ−0.5[g (T(Σ))] Σ−0.5)
d in,∗ 1 L∗,L∗ in,∗
σ (Σ−0.5[Σ ] Σ−0.5) Σ−0.5([Σ ] [g (T(Σ))] )Σ−0.5
≥ d in,∗ +,in L∗,L∗ in,∗ − in,∗ +,in L∗,L∗− 1 L∗,L∗ in,∗
h(Σ ) k[Σ +,in] L∗,L∗−[g 1((cid:13) (cid:13)T(Σ))] L∗,L∗k h(Σ ) 2 kΣ +,out k+ σDk −[T d(((cid:13) (cid:13) [Σ T) (] ΣL∗ )], LL ⊥ ∗⊥ ∗ ,k L2 ⊥ ∗) . (45)
+,in +,in
≥ − σ (Σ ) ≥ − σ (Σ )
d in,∗ d in,∗
ThefirstinequalityfollowsfromLemma3(b). Thesecondinequalityfirstappliesthedefinitionofhandthentothesecond
termthefollowingargument,whichissimilartotheonein(35):
X
Σ−0.5XΣ−0.5 Σ−0.5 X Σ−0.5 = k k .
k in,∗ in,∗k≤k in,∗k·k k·k in,∗k σ (Σ )
d in,∗
Thelastinequalityappliesthetriangleinequalitywith(43)and(44).
Thecomponentσ D−d([T(Σ)] L⊥ ∗,L⊥ ∗)intheRHSof(45)asfollows:
σ D−d [T(Σ)] L⊥ ∗,L⊥ ∗ ≥σ D T(Σ) =σ D T 2(Σ +,γ) =γσ d+1,D(Σ +) ≥γσ D(Σ) S, (46)
wherethefirstinequalityf(cid:0)ollowsfromLe(cid:1)mma3(cid:0)(a)and(cid:1)thesec(cid:0)ondinequal(cid:1)ityfollowsfromLemma2(c).Inaddition,webound
anothercomponentoftheRHSof(45)usingthetriangleinequalityand(25)asfollows:
k[T(Σ)] L⊥ ∗,L⊥ ∗k≤2 kΣ +,out k+ k[Σ +,in] L⊥ ∗,L⊥ ∗k=2 kΣ +,out k.
Combiningtheabovetwoequationsandtheboundof Σ in(18)yields
+,out
k k
k[T(Σ)] L∗,L⊥ ∗k2 (2 kΣ +,out k)2 =4 kΣ +,out kA σ 1([Σ] L⊥ ∗,L⊥ ∗) =4 kΣ +,out kAκˆ (Σ). (47)
2
σ D−d([T(Σ)] L⊥ ∗,L⊥ ∗)≤ γσ D(Σ)
S
γ
S
· σ D(Σ) γ
S
Pluggingtheaboveboundin(45)resultsinthefollowingboundonthenumeratorof(33):
h(T(Σ)) h(Σ ) 2 kΣ +,out
k+4kΣ
+,ou γt
SkAκˆ2(Σ)
=h(Σ ) 2+4 Aκˆ 2(Σ) kΣ +,out k. (48)
+,in +,in
≥ − σ (Σ ) − γ σ (Σ )
d in,∗ d in,∗
(cid:16) S (cid:17)
Conclusionoftheproofthatκˆ1(T(Σ))≥C 0κˆ1(Σ): Combining(42)and(48),weobtainthefollowingboundwhen
ν>2:
h(Σ ) 2+4Aκˆ2(Σ) kΣ +,outk
κˆ (T(Σ))
+,in − γS σd(Σ in,∗)
1 ≥ ν−4 2Aσ 1(Σ L⊥ ∗(cid:16) ,L⊥ ∗)+γ Dn −0 d(cid:17)σ 1(Σ L⊥ ∗,L⊥ ∗)
=
h(Σ +,in)
−
2+4Aκˆ γ2 S(Σ) νσ σd d( (Σ Σ+ i, ni ,n ∗)
)
h(Σ +,in) −(2+4Aκˆ γ2 S(Σ))κinh(Σ ν+,in)
ν−4 2Aσ 1(Σ L⊥ ∗(cid:16) ,L⊥ ∗)+γ Dn −0 d(cid:17)σ 1(Σ L⊥ ∗,L⊥ ∗)≥ ν−4 2Aσ 1(Σ L⊥ ∗,L⊥ ∗)+γ Dn −0 dσ 1(Σ L⊥ ∗,L⊥ ∗)
1
(2+4Aκˆ2(Σ))κin
h(Σ ) 1
(2+4Aκˆ2(Σ))κin
n
= − γS ν +,in − γS ν 1 κˆ (Σ). (49)
ν−4 2A+γ Dn −0
d
σ 1(Σ L⊥ ∗,L⊥ ∗)≥ ν−4 2A+γ Dn −0
d
d 1Thefirstequalityusesthedefinitionofν inthelastmultiplicativetermofthenumerator. Thesecondinequalityapplies(34),
thatis,σ (Σ ) h(Σ )σ (Σ ),andthedefinitionofκ ,thatis,κ =σ (Σ )/σ (Σ ). Thelastinequality
d +,in +,in 1 in,∗ in in 1 in,∗ d in,∗
≤
followsfromLemma2(a)andthedefinitionκˆ 1(Σ)=h(Σ)/σ 1(Σ L⊥ ∗,L⊥ ∗). Whenκ 1satisfiesthelowerboundin(3)withthe
constantC,therequirementonνin(37)becomes
κ
ν C κ + A + 2 A(1+κ ) . (50)
≥ in n d1 −γ Dn −0 d γ S in !
SinceCcanbechosensufficientlylargeandsinceκ 1,ν canbechosensufficientlylargeandsatisfytheassumptionused
in
≥
earlierthatν>2.Weevenassumeherethatν 4(whichholdswhenC 4)andobtaintheestimate
≥ ≥
4 8 8 n n 4 κˆ (Σ) κ 2 4 6
1 γ 0 and 2+ A 2 in + = .
ν 2A≤νA≤C d − D d γ ν ≤C C C
− (cid:16) − (cid:17) (cid:16) S (cid:17)
Theestimatesintheaboveequationcombinedwith(49)leadtothebound
1 (6) n
κˆ (T(Σ)) − C 1 κˆ (Σ). (51)
1 1
≥ 8 n1 γ n0 +γ n0 d
C d − D−d D−d
(cid:16) (cid:17)
Toconcludetheproof,weshowthatwhenCissufficientlylarge,
1 6 n
−C 1 >C . (52)
0
8 n1 γ n0 +γ n0 d
C d − D−d D−d
(cid:16) (cid:17)
Toshowthis,wesimplify(52)asfollows.ItsLHSbecomes
1 6 n 1 6 DS-SNR
−C 1 = −C
C8 n d1 −γ Dn −0
d
d C8(DS- γSNR −1)+1 γ
(cid:16) (cid:17)
andinviewof(29)itsRHSis2 DS-SNR .Wethusnotethat(52)holdswhen
·DS-SNR+γ
1 6 DS-SNR 2 DS-SNR
−C > · ,
8(DS-SNR 1)+1 γ DS-SNR+γ
C γ −
thatis,
6 DS-SNR 16 DS-SNR DS-SNR
+1 + 1 < 1,
C γ C γ − γ −
(cid:18) (cid:19) (cid:18) (cid:19)
orinotherwords
22 DS-SNR 10 γ
C> · − · . (53)
DS-SNR γ
−
Therefore,thereexistsCsufficientlylarge(specifiedin(53))suchthatκˆ (T(Σ)) C κˆ (Σ).
1 0 1
Proofthatκˆ2(T(Σ))≥max(κˆ2(Σ),7): Recallthat ≥
κˆ
(T(Σ))=σ 1([T(Σ)] L⊥ ∗,L⊥ ∗)
.
2
σ (T(Σ))
D
Wenotethat(39)boundsthenumeratorofκˆ (T(Σ)). Next,weboundthedenominatorofκˆ (T(Σ)),wherethemaintoolis
2 2
Lemma5andatlastweusetheseestimatestoconcludethatκˆ (T(Σ)) max(κˆ (Σ),7).
2 2
Estimateofthedenominatorofκˆ2(T(Σ)): Wederive(57)below≤ andthenapplyLemma5.Wefirstnotethatif
C>58, (54)
then
ν 2 ν 2 C 2 2(C 2)
(ν 2)γ = − νγ − 2Cκ − 2Cκ =2(C 2) κ − max(κ ,7) 16 κˆ (Σ). (55)
2 2 2 2 2
− S ν S≥ ν A≥ C · A − A ≥ 7 A ≥ AThefirstinequalityfollowsfrom(50)(usingonlythethirdterminthesumasalowerboundofν)andthefactκ 1. The
in
≥
secondinequalityfollowsfromthefactthatν C,whichisaconsequenceof(50)(usingonlythefirstterminthesumasa
≥
lowerboundforν)andthefactκ 1.ThelastinequalityfollowsfromC 58andtheconditionκˆ (Σ) max(κ ,7)in(30).
in 2 2
≥ ≥ ≤
Next,weverifythefollowingbound:
(2 k[T(Σ)] L∗,L⊥ ∗k)2 ≤16( kΣ +,out k2)=16 kΣ +,out k·kΣ +,out k≤8 Aσ 1(Σ L⊥ ∗,L⊥ ∗) ·kΣ +,out
k
(ν 2)γ
≤
κˆ− 2(Σ)Sσ 1(Σ L⊥ +,L⊥ +) ·kΣ +,out k=(ν −2)γσ D(Σ) SkΣ +,out k= (ν −2) kΣ +,out
k
γσ D(Σ)
S
.
(cid:16) (cid:17)(cid:16) (5(cid:17)6)
Thefirstinequalityfollowsfrom(25)andthefactthat[Σ +,in] L∗,L⊥
∗
isazeromatrix.Thesecondinequalityapplies(18). The
thirdinequalityapplies(55).Thenextequalityappliesthedefinitionofκˆ (Σ).
2
WeboundthefirstmultiplicativetermintheRHSof(56)asfollows:
(ν 2) Σ =σ ([Σ ] 2 Σ σ ([T(Σ)] ).
− k
+,out
k
d +,in L∗,L∗−
k
+,out
k≤
d L∗,L∗
ThesecondmultiplicativetermintheRHSof(56)wasalreadyboundedin(46)asfollows:
γσ D(Σ) S≤σ D−d [T(Σ)] L⊥ ∗,L⊥ ∗ .
Theabovethreeequationsthusresultinthefollowingbound: (cid:0) (cid:1)
k[T(Σ)] L∗,L⊥
∗k≤
σ D−d([T(Σ)] L⊥ ∗,L⊥ ∗)σ d([T(Σ)] L∗,L∗)/2. (57)
q
Theaboveequationistheconditionstatedin(23)ofLemma5forT(Σ).Thislemmathusimpliesthat
1
σ D(T(Σ)) ≥3min σ D−d([T(Σ)] L⊥ ∗,L⊥ ∗),σ d([T(Σ)] L∗,L∗) . (58)
(cid:16) (cid:17)
ConclusionoftheProofthatκˆ2(T(Σ))≥max(κˆ2(Σ),7): Wefirstnotethat(58)implies
κˆ
(T(Σ))=k[T(Σ)] L⊥ ∗,L⊥
∗k 3max
k[T(Σ)] L⊥ ∗,L⊥
∗k ,
k[T(Σ)] L⊥ ∗,L⊥
∗k . (59)
2
σ D(T(Σ)) ≤ σ D−d([T(Σ)] L⊥ ∗,L⊥ ∗) σ d([T(Σ)] L∗,L∗) !
Weboundthesecondargumentoftheabovemaximumasfollows:
k[T(Σ)] L⊥ ∗,L⊥ ∗k k[Σ +] L⊥ ∗,L⊥ ∗k+2 kΣ +,out k 3 kΣ +,out k = 3 . (60)
σ ([T(Σ)] )≤σ ([Σ ] ) 2 Σ ≤σ ([Σ ] ) 2 Σ ν 2
d L∗,L∗ d +,in L∗,L∗
− k
+,out
k
d +,in L∗,L∗
− k
+,out
k −
Forthe numeratorin the first inequalitywe used the triangleinequalityand Lemma6. For the denominatorin the second
inequality we used Lemma 3(b) and also the triangle inequality and Lemma 6. We can divide the two boundsas long as
thedenominatorispositive,whichholdswhenν >2(recallthedefinitionofν in(36)). Thesecondinequalityusesthefact
[Σ +,in] L⊥ ∗,L⊥
∗
=0andthelastequalityusesthedefinitionofν.
Toboundthefirstargumentofthemaximumin(59),wefirstnotethat
σ D−d([T(Σ)] L⊥ ∗,L⊥ ∗) ≥σ D−d([T(Σ) −Π d(Σ +)] L⊥ ∗,L⊥ ∗) −σ 1([Π d(Σ +)] L⊥ ∗,L⊥ ∗)=γσ d+1,D(Σ +) −σ 1([Π d(Σ +)] L⊥ ∗,L⊥ ∗)
γσ (Σ )
k[Π d(Σ +)] L⊥ ∗,L∗k2
γσ (Σ )
4 kΣ +,out k2
d+1,D + d+1,D +
≥ −σ ([Π (Σ )] )≥ −σ (Σ ) 2 Σ
d d + L∗,L∗ d +,in
− k
+,out
k
4
=γσ (Σ ) Σ .
d+1,D + +,out
−ν 2k k
−
ThefirstinequalityfollowsfromLemma3(b). Therestof thestepsfollowargumentssimilarto deriving(39). Using this
inequalityandtheboundon k[T(Σ)] L⊥ ∗,L⊥ ∗k≡σ 1([T(Σ)] L⊥ ∗,L⊥ ∗)in(39),weboundthefirstargumentofthemaximumabove:
k[T(Σ)] L⊥ ∗,L⊥
∗k
ν−4 2kΣ +,out k+γσ d+1,D(Σ +)
.
σ D−d([T(Σ)] L⊥ ∗,L⊥ ∗)≤ −ν−4 2kΣ +,out k+γσ d+1,D(Σ +)TofurtherboundtheRHSoftheaboveinequality,wenotethatfora= 4 Σ andb=γσ (Σ ),(a+b)/( a+b)
ν−2k +,out k d+1,D + − ≡
1+2((b/a) 1)−1canbeboundedusingalowerboundofbandanupperboundofaandverifyingthattheratio,whilemaking
−
surethatthelowerboundforbislargerthantheupperboundofa. Inordertoboundbfrombelow,weapply(19)andthe
definitionof :
S
xx⊤ xx⊤
σ (Σ ) σ σ (Σ) =σ (Σ)σ =σ (Σ) .
d+1,D + ≥ d+1,D D x 2 D d+1,D x 2 D S
(cid:16) x X∈Xk k (cid:17) (cid:16)x X∈Xk k (cid:17)
Toupperbounda,weapplythefollowingboundofLemma2(b):
kΣ +,out k≤kΣ L⊥ ∗,L⊥ ∗kA.
Finally,tomakesurethatourboundisvalidsoAtlastwemakesurethattheexpression(a+b)/( a+b)ispositiveafterusing
−
thesebounds,soourargumentisvalid. WeassumethatC>28andusetheboundsν>Candν>2Cκ2A (alreadyexplained
γS
whenbounding(55))andtheassumptionκˆ (Σ) max(κ ,7)in(30)toobtain
2 2
≤
4 8 8 28
κˆ (Σ) κˆ (Σ) max(κ ,7) γ .
2 2 2
ν 2 A≤ν A≤ν A≤ C S
−
Applyingthisboundandtheaboveestimates,weconcludethelastrequirementonaandb:
abovelowerboundofγσ (Σ ) γσ (Σ) γ C
d+1,D + D
= S = S 1.
upperboundof ν−4 2kΣ +,out k ν−4 2kΣ L⊥ ∗,L⊥ ∗kA ν−4 2κˆ 2(Σ) A≥28≥
Allaboveestimates(staringat(59))yield
κˆ (T(Σ)) 3max
ν−4 2kΣ L⊥ ∗,L⊥ ∗kA+γσ D(Σ)
S ,
3
=3max
ν−4 2κˆ 2(Σ) A+γ
S ,
3
, (61)
2 ≤ −ν−4 2kΣ L⊥ ∗,L⊥ ∗kA+γσ D(Σ)
S
ν −2 ! −ν−4 2κˆ 2(Σ) A+γ
S
ν −2 !
andtheRHSof(61)isboundedby
28/C+1 3 28/C+1 3
3max , 3max ,
28/C+1 ν 2 ≤ 28/C+1 C 2
(cid:16)− − (cid:17) (cid:16)− − (cid:17)
whichisboundedby7forasufficientlylargeC. Infact,combiningtheboundsofCin(53)and(54),itissufficienttochoose
thefinalvalueofCsuchthat
22 DS-SNR 10 γ
C max 58, · − · . (62)
≥ DS-SNR γ
(cid:16) − (cid:17)
Asaresult,κˆ (T(Σ)) 7 max(7,κ ).
2 2
≤ ≤
3.2.ProofofTheorem2
WedenotebyTME+STEtheSTEalgorithminitializedbyTME.Wewillprovethefollowingthreeclaimsthatwillimplythe
statementofTheorem2.
• TME+STErecoversL underageneralmodelwheretheinlierslieonthed-subspaceL ,both and UT x:x
∗ ∗ Xin
{
L⊥
∗
∈Xout
}
lieingeneralposition,thatis,anyk-dimensionalsubspacecontainsatmostkpoints,andeither(N DandDS-SNR>1)
≥
or(N>DandDS-SNR=1).
• TMEdoesnotrecoverL fordatageneratedbyageneralizedhaystackmodel,whereΣ(out) =0andDS-SNR<1.
∗ L∗,L⊥
∗ 6
• TME+STErecoversL whenη DS-SNR<1fordatageneratedbyageneralizedhaystackmodel.
∗
≤
Note that the first statement is rather general and clearly holds for the special case of data sampled by the generalized
haystackmodel. Wealsoremarkthatourgeneralizedhaystackisnotasgeneralastheoneof[9]. Nonetheless,westillcallit
generalizedsincethecovarianceoftheoutliersisnotsymmetric. Thecaseofsymmetriccovarianceforoutliersiscommonly
assumedinthehaystackmodel,whichmakesitverylimited. Forthissimplercommonmodel,theconditioninthesecond
statementabovewillnotholdandTMEwillstillbeabletoasymptoticallyrecoverthesubspace.
Belowweverifyinorderthesethreeclaims.TME+STE recovers L when DS-SNR 1: If DS-SNR> 1, [13] showed that the range of the TME solution,
∗
≥
Σ(TME), is L and thus TME recovers L . We note that T (Σ(TME)) = Σ(TME) and since rank(Σ(TME)) = d then
∗ ∗ 1
also T (Σ(TME),γ) = Σ(TME). Consequently, T(Σ(TME)) = T (T (Σ(TME)),γ) = Σ(TME). That is, if for STE
2 2 1
Σ(0)=Σ(TME),thenΣ(k)=Σ(TME)forallk 1andthusTME+STEalsorecoversL .
∗
≥
IfDS-SNR=1,thefollowingargumentshowsthattherangeoftheTMEsolutionisalsoL ,andthusimpliesthatTMEand
∗
TME+STErecoverL .Forsimplicity,wedenotetheTMEsolutionbyΣonlyandnotethatasstatedin(20)itsatisfies:
∗
D xxT
Σ= .
N xTΣ−1x
x∈X
X
We assume first that Σ L⊥ ∗,L⊥
∗
is nonsingular. We derive the following matrix inequality using the above formula, but
replacingthesumoverx ∈X tox ∈XoutsinceUT L⊥ ∗xxTU L⊥ ∗ =0forx ∈Xin,andthesameargumentsusedtoobtainthetop
lineof(21):
Σ L⊥ ∗,L⊥
∗
= ND
x∈
XXoutU xT L T⊥ ∗ (x Σx )T −U 1xL⊥ ∗ (cid:22)ND tl →im
∞ x∈
XXoutxTU (ΣT L⊥ ∗ +x tx PT LU ∗)L −⊥ ∗ 1x= ND
x∈ XXout(U L⊥
∗xU )T L T⊥ ∗ Σx
−
Lx
⊥
∗1T ,LU
⊥
∗L (⊥ ∗
U L⊥
∗x). (63)
Sinceγ=1,D/N=(D d)/n and(63)implies
0
−
T
I
D Σ− L⊥ ∗0. ,5 L⊥ ∗U L⊥ ∗xxTU L⊥ ∗Σ L− ⊥ ∗0. ,5 L⊥
∗
=D −d Σ L− ⊥ ∗0. ,5 L⊥ ∗U L⊥ ∗x Σ L− ⊥ ∗0. ,5 L⊥ ∗U L⊥ ∗x
. (64)
(D−d)×(D−d) (cid:22)N x∈ XXout (U L⊥ ∗x)TΣ− L⊥ ∗1 ,L⊥ ∗(U L⊥ ∗x) n 0 x∈ XXout(cid:16) Σ L− ⊥ ∗0. ,5 L⊥ ∗U L⊥ ∗x(cid:17)(cid:16)T Σ L− ⊥ ∗0. ,5 L⊥ ∗U L⊥ ∗(cid:17) x
(cid:16) (cid:17) (cid:16) (cid:17)
SincethetracesofboththeLHSandRHSin(64)areD d,equalitymustholdin(64)andconsequentlyalsoin(63).Notethat
−
equalityin(63)suggeststhatlim xT Σ−1 (Σ+tP )−1 x=0forallx . SinceΣ−1 lim (Σ+tP )−1,
t→∞
−
L∗ ∈Xout
(cid:23)
t→∞ L∗
theequalityholdsonlywheneveryx liesinthekerneloflim Σ−1 (Σ+tP )−1 ,whichisaviolationofthe
generalpositionassumptionthat
∈ isX n(cid:0) oou tt containedinasubsp(cid:1) acewt→ ith∞ inRD.− L∗
out
X (cid:0) (cid:1)
WethusconcludethatΣ L⊥ ∗,L⊥
∗
issingularandsoisΣ.Wenextprovethefollowingproperty.
Lemma9. LetL denotetherangeofΣ.Then,L L .
0 0 ∗
⊆
ProofofLemma9. WenotethattheTMEiterationatstepk,Σ(k),TME,satisfies
D xxT D xxT
g (Σ(k+1),TME)=g
1 1 N xT(Σ(k),TME)−1x (cid:23)N xT(Σ(k),TME)−1x
(cid:16) x X∈X (cid:17) x X∈Xin
D xxT d xxT
= . (65)
(cid:23)N xT(g (Σ(k),TME))−1x n xT(g (Σ(k),TME))−1x
x X∈Xin 1 1x X∈Xin 1
wherethefirstinequalityfollowsfromLemma1(b)andthefactthat
xxT xxT xxT
= S .
xT(Σ(k),TME)−1x− xT(Σ(k),TME)−1x xT(Σ(k),TME)−1x∈ +
x X∈X x X∈Xin x∈ XXout
ThesecondinequalityfollowsfromΣ(k),TME g (Σ(k),TME)(sinceΣ g (Σ) g (Σ) 0byLemma1).
1 1 2
Tosimplifyourargumentswe recallthefol(cid:23) lowingpropertyoftheTM− Esolut≡ ion[11]:(cid:23) IfΣ˜(TME) andΣ(TME) arethe
TMEsolutionstothesetsX˜= Ax:x and ,then
{ ∈X} X
Σ˜(TME)=AΣ(TME)AT. (66)
WLOG we may assume that the TME solution for {U L⊥ ∗x : x
∈
Xout
}
is the identity; otherwise we can apply the linear
transformationA:=Σ(TME)−0.5 suchthatthenewsolutionsatisfiesΣ˜(TME)=AΣ(TME)AT=I.Then(20)andtheabove
assumptionimply
xTx n
= 1 P . (67)
x 2 d L∗
x X∈Xink kWenotethat
d xxT d xxT
σ (g (Σ(k+1),TME)) σ σ
d 1 ≥ d (cid:16)n
1x
X∈XinxT(g 1(Σ(k),TME))−1x (cid:17)≥ d (cid:16)n
1x
X∈XinxT(σ d−1(g 1(Σ(k),TME)))x
(cid:17)
d xTx
=σ (g (Σ(k),TME))σ =σ (g (Σ(k),TME)),
d 1 d n x 2 d 1
(cid:16)
1x
X∈Xink k (cid:17)
wherethefirstinequalityuses(65)andthelastequalityuses(67).
Therefore σ (g (Σ(k),TME)) is a nondecreasingsequence and thus g (Σ) = lim g (Σ(k),TME) is positive
d 1 k≥0 1 k→∞ 1
{ }
definite. Moreover,anynonzerovectoruthatisnotinL⊥satisfiesuTg (Σ)u>0. WerecallthatLemma1impliesthatfor
∗ 1
Σ S ,Σ g (Σ)=g (Σ) 0. CombiningthisobservationwiththelatteroneyieldsuTΣu>0. Consequently,thekernel
+ 1 2
∈ − (cid:23)
ofΣmustlieinL⊥anditsrangeL containsL .
∗ 0 ∗
ThenTMEover isequivalenttoTMEovertheset L , aspointsoutsideL shouldhaveweight1/xTΣ−1x=0.
0 0
X X ∩
Inparticular,wehaveD=dim(L ). Bythegeneralpositionassumption, L n +dim(L ) d,andasaresult,the
0 0 1 0
|X∩ |≤ −
DS-SNRforL islargerthan1and[13]showedinthiscasethattherangeoftheTMEsolutionisL .
∗ ∗
Lastly,weremarkthatinoursettingwithN>DthereisnoothersubspacethanL withmorethandpointsonit(thiswill
∗
contradicttheassumptionsofgeneralpositionandN>D).However,ifN=DandDS SNR=1,wemayhaveatleasttwo
−
d-subspaceswithmorethandpoints.Ourstatementthusavoidsthiscase.
TMEdoesnotrecoverL ifΣ(out) =0andDS-SNR<1
∗ L∗,L⊥ ∗ 6
Wefirststatealemma,whichclarifiespropertiesoftheTMEsolutionunderthegeneralizedhaystackmodel.
Lemma10. FordatageneratedbythegeneralizedhaystackmodelandN ,whereasd,D,n /N andn /N arefixed,the
1 0
→∞
followingtwopropertieshold:
(a)IfΣ(out) =0,theTMEsolutionΣ(TME)satisfies
L∗,L⊥
∗
Σ(TME)=0 and Σ(TME)=0.
L∗,L⊥ ∗ L⊥ ∗,L∗
(b)TheTMEsolution,withaparticularchoiceofscalefactor,satisfiesthefollowingconditions:
σ (Σ(TME))
Σ(TME)=Σ(out) , 1 L∗,L∗ <C and σ (ΣTME)/σ (ΣTME )>C /(1 DS-SNR),
L⊥ ∗,L⊥ ∗ L⊥ ∗,L⊥ ∗ σ (Σ(TME)) 1 d L∗,L∗ 1 L⊥ ∗,L⊥ ∗ 2 −
d L∗,L∗
whereC andC areconstantsdependingontheconditionnumbersofΣ(out)andΣ(in).
1 2
Weprovethislemmain§3.2.1.Hereweusethislemmatoconcludetheabovestatement.For
I Σ(out) (Σ(out) )−1
A= − L∗,L⊥ ∗ L⊥ ∗,L⊥ ∗ ,
0 I !
theset ˜ = Ax: x hasan“outlyingcovariance”AΣ(out)AT, whichhasablockdiagonalstructure, and“inlying
covarianX ce”A{ Σ(in)A∈ T=X Σ} (in).ApplyingLemma10(a),theA-transformedTMEsolution,Σ˜TME,satisfiesΣ˜TME =0.
L∗,L⊥
∗
By(66),ΣTME=AΣ˜TMEAT,anditdoesnothavethesameblockdiagonalstructurewhenΣ(out) =0. Asaresult,its
L⊥ ∗,L∗6
topdeigenvectorsdonotspanL .
∗
TME+STErecoversL whenη DS-SNR<1.NotethattheoperatorAsatisfies
∗
≤
A 2+ Σ(out) (Σ(out) )−1 2+κ
k k≤ k L∗,L⊥ ∗ L⊥ ∗,L⊥ ∗ k≤ out
and
I Σ(out) (Σ(out) )−1
A−1 = L∗,L⊥ ∗ L⊥ ∗,L⊥ ∗ 2+κ out.
k k (cid:13) 0 I !(cid:13)≤
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)ApplyingLemma10(b)totheA-transformedsolution,Σ˜TME,resultsin
σ d(Σ˜T L∗M ,LE ∗)
O
1
. (68)
σ (Σ˜TME )≥ 1 DS-SNR
1 L⊥ ∗,L⊥ ∗ (cid:18) − (cid:19)
TheapplicationofΣTME=A−1Σ˜TME(A−1)T,A =Σ(out) (Σ(out) )−1andthedefinitionofg yields
0 L∗,L⊥
∗
L⊥ ∗,L⊥
∗
1
g (ΣTME)=g A−1
Σ˜T L∗M ,LE
∗
0
A−1 =g
Σ˜T L∗M ,LE ∗+A 0Σ˜T
L⊥
∗M ,LE
⊥
∗AT
0
Σ˜T
L⊥
∗M ,LE
⊥
∗AT
0 =Σ˜TME.
1 1 0 Σ˜TME 1 A Σ˜TME Σ˜TME L∗,L∗
L⊥ ∗,L⊥ ∗! ! 0 L⊥ ∗,L⊥
∗
L⊥ ∗,L⊥
∗
!
Combiningthisformulaandthebound A−1x x / A resultsin
k k≥k k k k
κ
=σ d(g 1(ΣTME)) 1 σ d(Σ˜T L∗M ,LE ∗)
.
1 σ 1(ΣT
L⊥
∗M ,LE
⊥
∗) ≥ kA−1 k2 kA k2σ 1(Σ˜T
L⊥
∗M ,LE
⊥
∗)
Similarly,
σ (ΣTME ) σ (Σ˜TME ) σ (Σ˜TME )
κ =
1 L⊥ ∗,L⊥
∗ A−1 2 A 2
1 L⊥ ∗,L⊥
∗ = A−1 2 A 2
1 L⊥ ∗,L⊥
∗ . (69)
2 σ D(ΣTME) ≤k k k k σ D(Σ˜TME) k k k k σ d(Σ˜T
L⊥
∗M ,LE
⊥
∗)
Wenotethattheaboveformulasand(68)implythatκ convergesto asDS-SNR 1,whereasκ isbounded.
1 2
∞ →
ItremainstoinvestigatetheRHSof(3),thatis,
dA
dκ κ d κ
in A κ + A + 2 A(1+κ ) =κ A κ + n1 +A 2 (1+κ ) .
n 1 in n d1 −γ Dn −0 d γ S in ! in n 1 in 1 −γDS-SNR S γ in !
BytheanalysisoftheGeneralizedHaystackModelinSection2, A and dA haveupperboundsthatonlydependonκ ,κ ,
S n1 in out
µandDS-SNRasfollows:
C D D √κ
A
µout,u
√κ
out
,
out
≤ C D d 2≤ D d 2≤ µ
S
µout,l
− − − −
wheretheestimationof follows(9),theestimationof follows(10),andC andC aredefinedin(5).Inaddition,
A S
µout,l µout,u
d
A
dn 0C µout,uD−1
d−2
DS-SNRC µout,u(D −d) DS-SNRκ out(D −d)
3DS-SNRκ ,
out
n ≤ n ≤ (D d 2) ≤ (D d 2) ≤
1 1
− − − −
wherethelaststepfollowsfromtheassumptionD d>2,soD d 3and D−d 3.
− − ≥ D−d−2≤
Inaddition,κ isboundedusing(69). So,theRHSof(3)hasanupperboundthatdependsonκ ,κ ,µ,DS-SNRand
2 in out
remainsboundedasDS-SNR 1.
→
Recall that the LHS of (3) converges to as DS-SNR 1, and as a result, if DS-SNR > η, there exists
∞ →
η:=η(κ ,κ ,c ,µ)<1suchthat(3)holds,andthetheoremisproved.
in out 0
3.2.1 ProofofLemma10
(a)RecallthattheTMEsolutionistheminimizeroftheobjectivefunction
1
F(Σ)=E xlog(xTΣ−1x)+ logdet(Σ),
D
RecallthatusingthebasisvectorsforL andL⊥,wemaywriteΣas
∗ ∗
Σ= Σ L∗,L∗ Σ L∗,L⊥ ∗ .
(cid:18)Σ L⊥ ∗,L∗ Σ L⊥ ∗,L⊥ ∗(cid:19)
Wedefine
Σ⋆= Σ L∗,L∗ −Σ L∗,L⊥ ∗
(cid:18)−Σ L⊥ ∗,L∗ Σ L⊥ ∗,L⊥ ∗ (cid:19)andnotethatF(Σ)=F(Σ⋆)and,bysymmetry,foreachx=[x L∗,x L⊥ ∗],thereisanequalchanceofx⋆=[x L∗, −x L⊥ ∗]inthe
dataset,andxTΣ−1x=x⋆T(Σ⋆)−1x⋆andxT(Σ⋆)−1x=x⋆TΣ−1x⋆.
BythegeodesicconvexityofF [12,13],thegeometricmeanofΣ(TME)andΣ(TME)⋆shouldalsobetheminimizerofF.
However,foranyΣ,thegeodesicmeanofΣandΣ⋆,denotedbyM,shouldbe“blockdiagonal”inthesensethatM L∗,L⊥
∗
=0
andM L⊥ ∗,L∗=0. Onecanobtainthispropertybyapplyingthegeometricmeanalgorithmin[4,(4)],whichisblockdiagonal
ineveryiteration.SincetheTMEsolutionisunique(uptoscaling),theTMEsolutionisblockdiagonal.
(b)WLOGwemayassumethatΣ L(o ⊥ ∗u ,t L) ⊥
∗
=I. Ifnot,wemayapplyT(x)=P L∗x+U L⊥ ∗(Σ L(o ⊥ ∗u ,t L) ⊥ ∗)−0.5UT L⊥ ∗xsothatthis
conditionissatisfied.
Firstofall, wewillshowthatΣTME isascalarmultipleofΣ(out) . Applyingtherotationalinvarianceofthedataset
L⊥ ∗,L⊥
∗
L⊥ ∗,L⊥
∗
withinL⊥,wheneverΣTME isaminimizerofF,foranysemi-orthogonalmatrixU R(D−d)×(D−d),
∗ ∈
ΣTME 0
Σ˜TME= L∗,L∗
0 UTΣTME U
(cid:18)
L⊥ ∗,L⊥
∗ (cid:19)
isalsoaminimizer.Consideringtheuniquenessofthesolution,ΣTME isascalarmultipleofI=Σ(out) .
L⊥ ∗,L⊥
∗
L⊥ ∗,L⊥
∗
Second,wewillshowthatΣTME hasaboundedconditionnumber. We letx˜ =(ΣTME)−0.5UT xandnotethat(20)
L∗,L∗ L∗,L∗ L∗
implies
n x˜x˜T n x˜x˜T 1
N1E x∈Xin
x˜
2+ N0E x∈Xout
x˜ 2+ (ΣTME )−0.5UT x
2= DI. (70)
k k k k k L⊥ ∗,L⊥ ∗ L⊥ ∗ k
Since for x , x˜ is sampled from a Gaussian distribution with covariance (ΣTME)−0.5[Σ(in)] (ΣTME)−0.5,
∈ Xin L∗,L∗ L∗,L∗ L∗,L∗
E x∈Xinkx˜ x˜x˜ kT
2
hasthesameeigenvectorsas(ΣT L∗M ,LE ∗)−0.5[Σ(in)] L∗,L∗(ΣT L∗M ,LE ∗)−0.5,andE
x∈Xoutkx˜k2+k(ΣT
L⊥
∗Mx ,˜
E
Lx˜
⊥
∗T
)−0.5UT
L⊥
∗xk2
has the same eigenvectors as (ΣTME)−0.5[Σ(out)] (ΣTME)−0.5. In addition, by (70), the largest eigenvector of
L∗,L∗ L∗,L∗ L∗,L∗
(ΣTME)−0.5[Σ(in)] (ΣTME)−0.5 should also the smallest eigenvector of (ΣTME)−0.5[Σ(out)] (ΣTME)−0.5.
L∗,L∗ L∗,L∗ L∗,L∗ L∗,L∗ L∗,L∗ L∗,L∗
Let u be the largest eigenvector of (ΣTME)−0.5[Σ(in)] (ΣTME)−0.5, then its eigenvalue is in the order of
L∗,L∗ L∗,L∗ L∗,L∗
O(σ (ΣTME)−1),and
d L∗,L∗
uT (ΣTME)−0.5[Σ(out)] (ΣTME)−0.5 u=O(σ (ΣTME)−1).
L∗,L∗ L∗,L∗ L∗,L∗ d L∗,L∗
(cid:16) (cid:17)
However,ushouldalsobethesmallesteigenvectorof(ΣTME)−0.5[Σ(out)] (ΣTME)−0.5,whichshouldbeintheorder
L∗,L∗ L∗,L∗ L∗,L∗
ofO(σ (ΣTME)−1),soO(σ (ΣTME)−1)=O(σ (ΣTME)−1)andtheconditionnumberofΣTME isbounded.
1 L∗,L∗ 1 L∗,L∗ d L∗,L∗ L∗,L∗
Lastly, let us bound σ (ΣTME)/σ (ΣTME ). Assuming that ΣTME = I and σ (ΣTME) = 1/η , then the TME
d L∗,L∗ 1 L⊥ ∗,L⊥ ∗ L⊥ ∗,L⊥ ∗ 1 L∗,L∗ 0
conditionimplies
E x∈Xout
kx L⊥
∗x k2L +⊥ ∗x
η
0T L k⊥ ∗
x
L∗k2(cid:23)E x∈XoutxT[x (ΣxT T] ML⊥ ∗ E, )L −⊥ ∗ 1x= D1 (ΣT L⊥ ∗M ,LE ⊥ ∗)= D1 I.
Takingtracesfrombothsides,wehave
E x∈Xout
x
x 2T L +∗x ηL⊥ ∗
x
2≥(D D−d) .
k
L∗k 0
k
L∗k
Asaresult,η shouldbechosensuchthatforX χ2 andX χ2
0 1 ∼ D−d 2 ∼ d
X N(D d) d
E 1 − =1 (1 DS-SNR) ,
X +κ η X ≥ n D − − D
1 out 0 2 0
orequivalently,
η X d
E 0 2 (1 DS-SNR) ,
1 X +η X ≤ − D
κout 1 0 2whichimplies
η X d
E 0 2 κ (1 DS-SNR) .
out
X +η X ≤ − D
1 0 2
Combiningitwith
η X η X η X η d/2 η d/2
E 0 2 E 0 2 =E 0 2 0 Pr(X >d/2) 0.5 0 ,
X +η X ≥
X2E
X +η X
X2D
d+η X ≥(D d)+η d/2
2
≥ (D d)+η d/2
1 0 2 X1 1 0 2
−
0 2
−
0
−
0
yields
2κ (1 DS-SNR)d 2(D d) 4κ (1 DS-SNR)(D d)
out out
η − − = − − .
0 ≤D 2κ (1 DS-SNR)d d D 2κ (1 DS-SNR)d
out out
− − − −
SinceΣTME =I,σ (ΣTME)=1/η ,andΣTME hasaboundedconditionnumber,part(b)isproved.
L⊥ ∗,L⊥ ∗ 1 L∗,L∗ 0 L∗,L∗
Acknowledgement. FY and GL were supported by NSF award DMS-2124913and TZ was supported by NSF award
DMS-2318926.
References
[1] R.Bhatia.PositiveDefiniteMatrices.PrincetonUniversityPress,2009.8
[2] G.LermanF.Yu,T.Zhang. Asubspace-constrainedtyler’sestimatoranditsapplicationstostructurefrommotion. 2024. Acceptedto
CVPR2024.1,2
[3] MoritzHardtandAnkurMoitra. Algorithmsandhardnessforrobustsubspacerecovery. InConferenceonLearningTheory,pages
354–375.PMLR,2013.2,3
[4] BrunoIannazzo. Thegeometricmeanoftwomatricesfromacomputationalviewpoint. NumericalLinearAlgebrawithApplications,
23(2):208–229,2016. 23
[5] JohnT.KentandDavidE.Tyler. Maximumlikelihoodestimationforthewrappedcauchydistribution. JournalofAppliedStatistics,
15(2):247–254,1988. 9
[6] GiladLermanandTylerMaunu.Anoverviewofrobustsubspacerecovery.ProceedingsoftheIEEE,106(8):1380–1410,2018.2
[7] GiladLerman,MichaelB.McCoy,JoelA.Tropp,andTengZhang.Robustcomputationoflinearmodelsbyconvexrelaxation.Found.
Comput.Math.,15(2):363–410,2015.2,3
[8] TylerMaunuandGiladLerman.Robustsubspacerecoverywithadversarialoutliers,2019.2,3
[9] TylerMaunu,TengZhang,andGiladLerman. Awell-temperedlandscapefornon-convexrobustsubspacerecovery. J.Mach.Learn.
Res.,20(1):1348–1406,2019. 2,3,4,19
[10] T.Tao.TopicsinRandomMatrixTheory.AmericanMathematicalSociety,2012.10,15
[11] DavidETyler.Adistribution-freem-estimatorofmultivariatescatter.TheAnnalsofStatistics,pages234–251,1987. 20
[12] Ami Wiesel, Teng Zhang, et al. Structured robust covariance estimation. Foundations and Trends® in Signal Processing, 8(3):
127–216,2015.23
[13] TengZhang. Robustsubspacerecoverybytyler’sm-estimator. InformationandInference:AJournaloftheIMA,5(1):1–21,2016. 2,
3,20,21,23
[14] TengZhangandGiladLerman.Anovelm-estimatorforrobustPCA.J.Mach.Learn.Res.,15(1):749–808,2014.2