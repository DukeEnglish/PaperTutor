MetaCap: Meta-learning Priors from Multi-View
Imagery for Sparse-view Human Performance
Capture and Rendering
Guoxing Sun1, Rishabh Dabral1, Pascal Fua2, Christian Theobalt1,
Marc Habermann1
1 Max Planck Institute for Informatics, Saarland Informatics Campus
2 EPFL
{gsun,rdabral,theobalt,mhaberma}@mpi-inf.mpg.de, pascal.fua@epfl.ch
https://vcai.mpi-inf.mpg.de/projects/MetaCap/
Meta-learning on multi-view imagery Meta-learned weights Fine-tuning on sparse imagery
SDF field & Color field
One input view Two input view Four input view
Fine-tuning
ùùì‚àó
ùüé
Hash table weights
Our novel view renderings and geometry reconstructions
Fig.1: WeproposeMetaCap,anewapproachforcapturing3Dhumansfromsparse-
vieworevenmonocularimages,whichenableshighfidelity3Dgeometryrecoveryand
photoreal free-view synthesis within a range of seconds to minutes.
Abstract. Faithful human performance capture and free-view render-
ing from sparse RGB observations is a long-standing problem in Vision
and Graphics. The main challenges are the lack of observations and the
inherentambiguitiesofthesetting,e.g.occlusionsanddepthambiguity.
As a result, radiance fields, which have shown great promise in cap-
turing high-frequency appearance and geometry details in dense setups,
perform poorly when na√Øvely supervising them on sparse camera views,
as the field simply overfits to the sparse-view inputs. To address this,
we propose MetaCap, a method for efficient and high-quality geome-
try recovery and novel view synthesis given very sparse or even a single
view of the human. Our key idea is to meta-learn the radiance field
weightssolelyfrompotentiallysparsemulti-viewvideos,whichcanserve
as a prior when fine-tuning them on sparse imagery depicting the hu-
man. This prior provides a good network weight initialization, thereby
effectivelyaddressingambiguitiesinsparse-viewcapture.Duetothear-
ticulated structure of the human body and motion-induced surface de-
formations,learningsuchapriorisnon-trivial.Therefore,weproposeto
meta-learnthefieldweightsinapose-canonicalizedspace,whichreduces
thespatialfeaturerangeandmakesfeaturelearningmoreeffective.Con-
sequently,onecanfine-tuneourfieldparameterstoquicklygeneralizeto
unseen poses, novel illumination conditions as well as novel and sparse
4202
raM
72
]VC.sc[
1v02881.3042:viXra2 Sun et al.
(evenmonocular)cameraviews.Forevaluatingourmethodunderdiffer-
entscenarios,wecollectanewdataset,WildDynaCap,whichcontains
subjectscapturedin,both,adensecameradomeandin-the-wildsparse
camerarigs,anddemonstratesuperiorresultscomparedtorecentstate-
of-the-art methods on both public and WildDynaCap dataset.
Keywords: Human performance capture ¬∑ Meta learning
1 Introduction
Human performance capture has made enormous strides in recent years. Typ-
ically, the quality of the reconstruction improves with the number of cameras
being used. The highest-quality results are obtained using many‚Äî8 or more‚Äî
calibratedcamerasinadome-likeconfiguration[11,20,65,68,93].However,such
capture domes are expensive and difficult to set up, which restricts their appli-
cability. A truly practical solution ought to be easy to deploy while supporting
arbitrary camera configurations, including sparse ones, i.e. four or less cameras.
Unfortunately, sparsity comes with its own challenges, such as self-occlusions
and depth ambiguities, which often result in lower quality outcomes.
How to mitigate the effects of these ambiguities and missing information is
the core problem of sparse-view reconstructions and researchers have explored
different priors to compensate for the ambiguities of such an ill-posed problem.
Earlier works introduce priors on template meshes [23,24,30,85] to solve the
above ambiguities. However, meshes suffer from limited resolution and are not
easy to incorporate into learning frameworks. Implicit surface representations
offer a promising alternative and some methods [60,95] can learn implicit field
priors from large-scale human scan datasets. However, because the amount of
data is limited and model capacity is small, they suffer from blurry appearance
and poor generalization capability. While many works have recently focused on
animatable implicit radiance fields for humans [21,38,55,78,80,94], only few
[19,39,48,87]haveexploredpriorsforoptimizingorfittinganeuralimplicitfield
given sparse imagery. They usually only show results for simple objects, but not
for complex structures like articulated humans.
Recently, [69,72] show meta-learning can help learn powerful priors in neu-
ral field methods for few-shot learning. To learn the field prior for personalized
high fidelity human capture more effectively, we argue that we should decom-
pose the human model into a coarse-level geometry represented by mesh-based
templates [22,40] and a fine-level neural implicit field [76,79,93] accounting for
details such as clothing wrinkles. The coarse-level prior from multi-view images
is easier to obtain and can serve as a crucial component in fine-level prior learn-
ing. At the fine level, we, for the first time in literature, explore learning a meta
prior from images for an implicit human representation, which at test time can
beoptimizedusingsparseimagecues,therebyallowingfastadaptationtounseen
poses, novel views, different camera setups, and novel illumination conditions.
Our method, dubbed MetaCap, represents the fine-level human body as a
signed distance field (SDF) and an appearance field parameterized with an effi-MetaCap 3
cient hashgrid encoding [45], which can be volume rendered [76] into an image.
To learn an effective prior for such a representation, we propose to meta-learn
the hashgrid parameters solely from multi-view imagery during training in an
end-to-end manner, i.e. learning the optimal weights, which, at test time under
the sparse settings, yields improved convergence speed and accuracy. However,
na√Øvely meta-learning these implicit parameters, as done in [69,72] for static
scenes, leads to poor performance on complicated settings such as human per-
formancecapture(Fig.5).Thisisbecausethehumansurfaceishighlyarticulated
andalwaysdeforms,thus,learninghashgridparameters,whichliveonaspatially
fixed grid, is not effective, e.g. the limbs can be mapped onto entirely different
hashtableparametersdependingontheirarticulation.Therefore,wefurtherpro-
pose a space canonicalization step, i.e. we transform points from global space to
a canonical pose space. To obtain the transform, we query the nearest transfor-
mation of coarse-level human template. We highlight that this canonicalization
isnottightlyboundtoaspecifictemplate,butsupportsdeformablehumanmod-
els [22] as well as parametric (piece-wise rigid) body models [40]. Additionally,
we introduce occlusion handling, where we use a visibility map to guide the ray
sampling, enabling reconstruction of heavily occluded regions at sparse or even
monocular inputs at inference time.
We evaluate our method on the DynaCap [22] benchmark, which provides
multi-viewrecordingsofhumansinacontrolledstudiosetup.However,DynaCap
lacks in-the-wild scenes for evaluating the robustness to various environment
conditions such as the difference in lighting and camera parameters. Thus, we
provide a new dataset, WildDynaCap, comprising of paired dense multi-view
studio capturesandsparse multi-view in-the-wild recordingsforeachsubject.In
summary, our main contributions are:
‚Äì A meta-learning method for high-quality reconstruction of human geometry
and appearance from sparse imagery.
‚Äì At the technical core, we propose a meta-learning strategy to learn the op-
timal weights of an implicit human representation solely from multi-view
images, which effectively serves as a prior when deployed to the sparse re-
construction task.
‚Äì We further demonstrate the importance of space canonicalization for the
human-specificmeta-learningtask,andintroduceadedicatedocclusionhan-
dling strategy.
OurquantitativeandqualitativeexperimentsdemonstratethatMetaCapachieves
state-of-the-art geometry recovery and novel view synthesis compared to prior
works. Our evaluations also demonstrate that MetaCap generalizes to novel
posesandinducedsurfacedeformations,changeoflightingconditions,andcam-
era parameters. Moreover, we highlight the versatility of our approach as our
meta-learning strategy can be supervised on an arbitrary number of views also
includingmonocularvideos.Similarly,duringfine-tuning,ourapproachsupports
reconstruction from sparse in-the-wild multi-view images as well as monocular
imagery(seeFig.1).Last,ourmethodisagnostictothechoiceofthedeformable
human model, thus, even supporting loose types of apparel.4 Sun et al.
2 Related Work
In the following, we discuss prior works on (1) scene-agnostic implicit represen-
tations for novel view synthesis and reconstruction, (2) human-specific sparse-
view reconstruction and rendering methods, (3) personalized performance cap-
ture and (4) 3D reconstruction using meta-learning. Animatable avatar meth-
ods[21,22,36,38,41,62,66,96],whichonlytaketheskeletalposeasinputduring
inference, are not in our scope, since we focus on the reconstruction task, i.e.
recovering geometry and appearance from 2D imagery during inference.
Scene-agnostic Implicit Representations. The emergence of implicit rep-
resentations for 3D scene reconstruction [42,51,60,76] and novel-view synthe-
sis [5‚Äì7,44] paved the way towards compact and high-fidelity scene representa-
tions.Subsequentmethodshaveattemptedtoimproveuponavarietyofaspects,
such as faster training [9,16,45], faster inference [32,88], sparse [10,19,39] and
monocular-view [18,89] reconstruction. In our work, we build upon the scene
representation introduced in NeuS2 [79] and Instant-NSR [93]. They leverage
multi-grid hash encoding [45] for parameterizing the implicit surface [76] to en-
ablefastandhighqualityreconstruction.However,theyrequiresufficientlydense
multi-viewimagestoensureaccurategeometryandappearancerecovery.Dueto
occlusions and the depth ambiguity inherently present in the sparse setup, sim-
plyoptimizingsuchmethodswithsparseRGBimageryleadstopoorreconstruc-
tions.Morerecently,somemethodsexploreddifferentgeometrypriors[19,39]or
regularizations [48,87] to enable few-shot novel view synthesis. However, none
of them proves to be effective under large camera baselines or sparse 360‚ó¶ sce-
narios, where the image observations are very limited and occlusions happen
morefrequently.Weaddressthesechallengesbymeta-learninganoptimalsetof
initial hashgrid parameters, which effectively serves as a prior during inference
on sparse observations, and a human-specific space canonicalization.
Sparse-View Performance Capture and Rendering. Beyond occlusions
andthedepthambiguity,sparseperformancecapturecomeswiththeadditional
challenges of modeling and capturing the body articulations as well as complex
surface deformations of the clothes. To compensate for the absence of dense
supervision, sparse human reconstruction methods typically employ additional
priors. They range from using a parametric template for a coarse initialization
of the geometry [54‚Äì56,63,95,97], to using depth supervision [49,71,86,90,99]
and data-driven priors [12,27,60,61,65,73,77,83,84,95,98]. Several volumetric
rendering-basedmethodsfocusonlearningthehumangeometryandappearance
in the canonical space [29,50,54,56,58,64,74,75], which learns shared features
acrossdifferentposes.Whilesuchmethodsexcelinnovelviewsynthesistasks,the
recovered geometry is often coarse. Some works [8,34,43,50] learn radiance field
priors from large scale datasets. However, their conventional learning strategy
limitstheirfine-tuningability.Anotherkeychallengeinhumanreconstructionis
associatedwiththemodelingoflooseclothing.Previousmethodshaveattempted
to address this by optimizing the vertex deformations on top of a template
model [22‚Äì24,41,82] or by modeling clothes as a separate layer [13,75]. In this
work, we utilize a hybrid representation where we leverage an explicit humanMetaCap 5
model to facilitate meta-learning an optimal set of weights parameterizing an
implicit field in a pose-canonicalized space.
Personalized Performance Capture. In contrast to generalizable perfor-
mancecapturemethods[50,60,78,95],personalizedhumancapturefirstbuildsa
person-specific prior on (potentially dense) multi-view data [12,24,37,67,74,81]
whileduringinferencetheavatarcanbereconstructedfromsparsersignals,e.g.a
monocularimage.Therefore,thegoalofsuchmethodsislessaboutcross-identity
generalization, but rather highest-fidelity surface and appearance recovery from
sparse sensory data while ensuring generalization to different human motions,
lighting condition, camera poses, camera numbers, and surface dynamics. Our
settingismostcloselyrelatedtothesemethods.However,weforthefirsttimein
literatureproposemeta-learningasapriorontheimplicithumanrepresentation
solely from images while experimentally showing that we outperform previous
methods operating under the same setting.
Reconstruction with Meta-Learning. Meta-learning is intended to learn
from multi tasks or single task [26]. When new observations are presented, do-
mainadaptationorimprovedperformancecanbeachievedwithminimaltraining
iterations. Here, we focus on optimization-based methods [4,15,35,47,57], espe-
ciallyMAML[15]andReptile[47].MetaSDF[69]learnsSDFpriorsandachieves
faster inference than auto-decoder methods [51]. Tancik et al. [72] apply a simi-
lar strategy to different coordinate-based neural representations, such as image
fields and radiance fields [44]. MetaAvatar [77] extends this idea from static to
dynamic human representations. After meta-learning on 3D human scans [41],
the learned neural SDF weights can be fine-tuned using a few depth maps.
ARAH [78] is methodically the most closely related to our work. However,
therearesignificantdifferences:1)Theymeta-learnhumanpriorsfrom3Dscans
while only support image-only supervision during fine-tuning. In stark contrast,
our formulation allows meta-learning from potentially sparse images directly
as we deeply entangle the volume rendering and hash-grid parameterization
with the meta-learning routine. Our end-to-end prior learning and fine-tuning,
thus,enablesbetterrecoveryofhigh-frequencygeometryandappearance.2)Our
whole design, i.e. space canonicalization and scene parameterization, is geared
towards efficiency, significantly reducing fine-tuning time, reducing from hours
(ARAH) to minutes (MetaCap). 3) We experimentally show that our canoni-
calization is agnostic to specific template choices, thus, we can also account for
loose clothing while ARAH solely shows tightly clothed people.
3 Method
Our goal is to recover high-quality human models from few, or even only one,
RGBimage.Letusconsideranimplicitfunctionf parameterizedbyanetwork
œï
with weights œï that encodes the geometry and the appearance of the clothed
human. Our sparse-view reconstruction method comprises two phases, multi-
view meta-learning followed by sparse-view fine-tuning, as shown in Fig. 2.
Duringthemeta-learningphase(Sec.3.1and3.2),wearegivenamulti-view
videousingK cameras,whichcapturestheperformer.Wedenotetheimagesof
d6 Sun et al.
Multi-view RGB loss
Mocap Reptile
‚Ä¶ gni n atr ea Mel elbam rofeDetalpm eT ecapSnoitazilacinonaC nam uH ticilpm InoitatneserpeR esrevnInoitazilacinonaC cirtem uloVgniredneR
Weight
Initialization
gn ein
niu F-t
elbam rofeDetalpm
eT
ecapSnoitazilacinonaC nam uH
ticilpm
InoitatneserpeR esrevnInoitazilacinonaC cirtem uloVgniredneR
Mocap Gradient Decent
Sparse-view RGB loss
Fig.2: Overview. MetaCap is a novel performance capture method, which meta-
learns the pose-canonical and optimal network weights of an implicit human repre-
sentation solely from multi-view images. At inference, with only sparse in-the-wild
images, those weights effectively act as a prior and resolve the inherent ambiguities,
i.e. occlusion and depth, while maintaining high geometric and visual quality.
framef asI ={Ik|k ‚àà[1;K ]}.Wealsoassumetheavailabilityofpairedmasks
f f d
O andthecorrespondingskeletalmotionM ={Œ∏ ,Œ± ,z ,...,Œ∏ ,Œ± ,z },
f f f‚àíW f‚àíW f‚àíW f f f
definedoverawindowofW frames,withŒ∏,Œ±,z representingthejointrotations,
the root rotations, and the root translations, respectively. Given this, we aim at
meta-learning a personalized and optimal set of weights œï‚àó for the implicit hu-
man representation, effectively serving as a data-driven prior.
Duringthefine-tuningphase(Sec.3.3),wefine-tunethemeta-learnedweights
œï‚àó to sparse in-the-wild images IÀÜ = {IÀÜk|k ‚àà [1;K ]} paired with masks OÀÜ =
f f s f
{OÀÜk|k ‚àà[1;K ]}withK <<K .Notably,themotionMÀÜ andsurfacedynamics
f s s d f
of the performer as well as the lighting conditions and camera configuration
duringfine-tuningcansignificantlydifferfromtheonesobservedduringtraining.
Welatershowthatfine-tuningthemeta-learnedweightsœï‚àó onIÀÜ leadstofaster
f
and better convergence compared to random weights œï .
0
3.1 Meta-Learning a Reconstruction Prior
Meta-Learning [15,47] trains a neural model in a way that makes it easier to
adapt to novel tasks. In our case, the novel tasks are sparse-view reconstruction
and novel-view synthesis under unseen motions as well as lighting conditions.
For meta-learning the 3D geometry and appearance of the human, we represent
thehumanasanimplicitSDFandradiancefield.Givenarayr(o,v)emanating
from an image I with origin o, and viewing direction v, we sample n points
f
{p = o + t v|i ‚àà [1;n]} on depths t , and the implicit network f predicts
i i i œï
the color c and the SDF s for each sample. We then utilize unbiased volume
i i
rendering[76]alongeachrayrtoobtainRGBCÀÜ(r)andmaskvalueMÀÜ(r):
CÀÜ(r)=(cid:88)n T Œ± c , MÀÜ(r)=(cid:88)n T Œ± , Œ± =max(cid:0)Œ®(s i)‚àíŒ®(s i+1) ,0(cid:1) , (1)
i i i i i i Œ®(s )
i
i=1 i=1
where T is the accumulated transmittance, Œ± is the unbiased weight function,
i i
and Œ®(¬∑) represents the derivative of a sigmoid function. These rendered rays
can be used to supervise the implicit network f (¬∑) using the following loss:
œïMetaCap 7
L(f ,R)=Œª L +Œª L +Œª L +Œª L , (2)
œï c color e eik m mask s sparse
whereR={r |k ‚àà[1;m]}isacollectionofmrays,L = 1 (cid:80) H(CÀÜ(r ),C(r ))
k color m k k k
is the Huber loss [28], L
eik
= m1 n(cid:80) k,i(cid:12) (cid:12)1‚àí‚ü®n(p k,i),‚àá pk,if œï(p k,i)(cid:11)(cid:12) (cid:12)2
2
is the
Eikonal loss [17] on the normal n(p ) of the sample point p . L =
k,i k,i mask
1 (cid:80) BCE(MÀÜ(r ),O(r )) is the binary cross entropy loss. s(p ) is the SDF
m k k k k,i
value of point p k,i, L
sparse
= m1 n(cid:80) k,iexp‚àí|s(pk,i)| is a sparseness regularization
term[39].C(r )andO(r )arethegroundtruthcolorandmaskvaluesofrayr .
k k k
To accelerate the rendering process, we parameterize the implicit field using a
multi-resolution hashgrid [45]. Thus, the overall learnable parameters œï include
the MLP weights of the implicit function as well as the hashgrid parameters.
Now, the initial weights œï for our coordinate-based implicit function f can
0 œï
belearnedthroughoptimization-basedmeta-learningalgorithms,likeMAML[15]
orReptile[47].Letusassumewehaveadatasetoftasksfollowingthedistribution
T, where a task T ‚àº T is defined as a set {L,{R ‚àº I ,O },f } comprising
f f f œï f
thelossfunction,theinputraysandtheimplicitfunctionf .Ourmeta-learning
œï
optimization is performed in two nested loops. For the jth iteration of the in-
ner loop with sampled rays Rj, it follows a gradient-descent optimization of the
model parameters œï‚Ä≤ using a learning rate l :
j in
œï‚Ä≤ ‚Üêœï‚Ä≤ ‚àíl ‚àá L(f ,Rj)| . (3)
j j‚àí1 in œï œï œï=œï‚Ä≤
j‚àí1
We leverage the model-agnostic meta-learning algorithm, Reptile [47], to opti-
mize the initial weights by nesting an outer loop on top of the inner loop. The
outer loop then updates the meta-learned parameters œï through the first-order
i
update equation:
œï ‚Üêœï +l (œï‚Ä≤ (œï ,T )‚àíœï ), (4)
i i‚àí1 out M i‚àí1 i i‚àí1
whereT arethetaskssampledattheithouteriteration,l denotesthelearning
i out
rateofouterloop.œï‚Ä≤ (œï ,T )representsthenetworkweightsinitializedbythe
M i‚àí1 i
inner-loopwithœï ,whicharethenoptimizedforM stepsonT .Theresulting
i‚àí1 i
weights, œï , at the end of N outer loop iterations serve as the optimal initial
N
weights œï‚àó for the fine-tuning stage. To keep the meta-learning computation
tractable, we choose Reptile [47] in Eq. 4 as it uses a first order approximation
instead of MAML‚Äôs second order gradient update.
Notethattheaboveformulationdirectlymeta-learnsavolumetricrendering-
based implicit network in the world-space. This proves to be suboptimal (as
showninFig.5andTab.3)ashumansarehighlyarticulated;achallengefurther
exacerbatedbylooseclothing.InthetasksetT consistingofhumansindifferent
body poses, the meta-learned hash encoding can map the same body point into
different hash-table parameters. To address this issue, we modify the volume
rendering formulation in Eq. 1 by proposing our human template guidance.
3.2 Template-guided Meta-Learning
Human Template. Here, we introduce a generic motion-driven human tem-
plate, defined as a deformable mesh XÀÜ ‚àà RV√ó3, of a character in the canonical8 Sun et al.
pose (as shown in Fig. 2). The template mesh XÀÜ can be deformed according to
a given skeletal motion M . Each vertex in the human template first under-
f
goesaper-vertexmotion-dependentdeformationinthecanonicalspacethrough
a transform T (M ) ‚àà R4√ó4. After being deformed locally with T , one
def f def
canperformforwardkinematicsT (M )totransformthedeformedcanonical
FK f
spaceverticesT XÀÜ toworldspaceverticesX.Thetotalinvertibletransforma-
def
tion from the template‚Äôs canonical space to world space is: T = T T and
FK def
the template vertices are transformed as X=TXÀÜ.
Template-guided Ray Warping. The template mesh defined above provides
us a coarse geometry information, which effectively guides our meta-learning
since we are now able to learn the meta-weights in a pose-canonical space. The
sampled points p can be warped from the world space into the canonical space
i
through our proposed template guided ray warping. Concretely, we first project
each sampled point onto the nearest face (triangle) of the posed template mesh
X. To compute the transformation matrix T , we barycentricaly interpolate the
i
transformation of the vertices defining the nearest triangle. Next, we apply the
inversetransformationT‚àí1 ofthesampledpointtoobtainitscanonicalposition
i
pÀÜ = T‚àí1p . The canonical points, pÀÜ, can now be encoded with the learnable
i i i i
hash encoding and rendered using the volumetric rendering approach described
in Eq. 1. Moreover the template allows us to perform empty skipping [25] for
point samples whose distance to the template exceeds a threshold Œ∑, thereby
significantly accelerating the convergence. In this way, our final meta-learning
approach(seealsoAlg.1)ismoreefficientandeffectivecomparedtothebaseline,
i.e. learning in global space.
Algorithm 1 MetaCap‚Äôs Meta-Learning Procedure
Initialize: Weights œï, outer/inner learning rate lout and lin
for i=1,...,N do
Sample frames {f(j)}M , views {k(j)}M and generate rays {R(j)}M
j=1 j=1 j=1
Set œï‚Ä≤ =œï
0
for j =1,...,M do
Find closest triangle of points p and compute the point to face distance d
Filter points with d larger than threshold Œ∑
Compute inverse transformation T
Transform points into canonical space pÀÜ =T‚àí1p
œï‚Ä≤ j =œï‚Ä≤ j‚àí1‚àílin‚àá œïL(f œï(RÀÜ(j))| œï=œï‚Ä≤ )
j‚àí1
end for
œï‚Üêœï+lout(œï‚Ä≤
‚àíœï)
M
end for
Result: Optimal weights œï‚àó ‚Üêœï
3.3 Occlusion Handling and Fine-tuning
Atinference,whenonlythecalibratedmonocularorsparse-viewRGBimagesIÀÜ ,
f
masksOÀÜ ,andskeletalmotionMÀÜ areavailable,weinitializeourimplicithuman
f f
field with the meta-learned weights œï‚àó, conduct the space canonicalization withMetaCap 9
(a) (b) (c) (d)
Fig.3:OcclusionHandling.(a)MonocularRGBinput.(b)Naivefine-tunedresults.
(c) Visibility map and proxy image in canonical space. (d) Fine-tuned results with
occlusion handling. Our occlusion handling helps regularize largely occluded regions.
thefittedtemplateX(Sec.3.2),andfine-tuneusingthelossfromEq.2.Thiscan
also be denoted as the task {L,{R ‚àºIÀÜ ,OÀÜ },f |œï =œï‚àó}.
f f f œï 0
Occlusion Handling. For extreme occlusion cases as it is common in the
monocular setting (see also Fig. 3), we further propose a dedicated occlusion
handling strategy by leveraging the posed human template. First, we pre-build
an implicit human field on one frame of dense images I or multiple frames
f
of (in-the-wild) sparse images IÀÜ , and then render virtual views of the canoni-
f
cal space as proxy RGB images Iproxy. Next, during the fine-tuning stage, we
determine per-vertex visibility by checking whether the vertex of the template
is visible in any of the sparse input views. Finally, in addition to sampling rays
fromtheinputimages,wesampleadditionalvirtualraysfortheoccludedregions
from Iproxy. In practice, we found that the occlusion handling is only required
in the case of monocular reconstruction (also see ablations).
4 Results
Implementation Details. For meta-learning, we perform M = 24 inner-loop
gradient steps with lout = 1.0 and lin = 1e ‚àí 4. The inner loop optimizer
is Adam [33] while the outer one uses stochastic gradient descent. Following
NeuS [76], we perform a warmup in the inner and outer loop at the beginning
of training. If not stated otherwise, we use DDC [22] for space canonicalization,
‚àº80 cameras for meta learning, 4 cameras for fine-tuning in comparisons and
ablations. For more details, we refer to the supplemental material.
Datasets. We evaluate our approach on the publicly available dense-view
dome dataset DynaCap [22] and our WildDynaCap dataset. DynaCap pro-
vides around 100-view videos, foreground masks, and motions. We choose two
subjects wearing loose (S5) and tight clothing (S3 in the supplemental). To
compensate for the lack of in-the-wild sequences in DynaCap, we collect a new
datasetWildDynaCap.Itcontains110camerastocapturetheperformerinside
a dome, and a movable 5-camera setup to record videos in different in-the-wild
scenarios.Wecapturedtwosubjectsintwocamerasetups.Werecovertheskele-
tal motion using markerless motion capture [70] using 34 views for the dome
setups and 5 views for the in-the-wild sequences. For all experiments and meth-
ods, we use this motion (if not stated otherwise) as this work focuses on surface
recovery rather than motion capture. We also provide ablation on motions only
from sparse cameras in the supplemental.10 Sun et al.
S2
S27
S5
GT Ours DeepMultiCap DiffuStereo DVA TransHuman ARAH
Fig.4: Qualitative Comparison. Compared to previous works, our method better
captureshigh-frequencygeometryonclothwrinklesandfaithfullyrecoversappearance
details such as facial details and clothing textures.
Root SMPL DDC
m
odnaR
GT
niarterP
Not Converged Not Converged
ateM
Input Images
Fig.5: Qualitative Ablation. Here, we show qualitative results for different weight
initialization strategies and space canonicalization types. Note that our meta-learning
paradigm together with the proposed space canonicalization achieves the best result.
Moreover, we highlight that this applies irrespective of the choice of the character
model, i.e. SMPL [40] or DDC [22].
Metrics and Ground-Truth. To obtain the GT geometry for the dome cap-
tures, we leverage the recent state-of-the-art implicit reconstruction method In-
stantNSR [93]trained on the dense RGB views.To evaluate therenderingqual-
ity, we report the peak signal-to-noise ratio (PSNR), structural similarity index
(SSIM), and learned perceptual image patch similarity (LPIPS) [91]. To evalu-
ate geometry results, we report Chamfer distance (CD), point-to-mesh distance
(P2S), intersection over union (IOU), cosine normal consistency (NC-Cos), and
L2 normal consistency (NC-L2). For all metrics, we sample every 100th frame
from the test set, where the actor is performing unseen motions, and report the
average. We provide an illustration of input cameras and evaluation cameras
distribution in the supplementary material.
4.1 Comparisons
Wecompareourmethodwithrecentreconstructionandnovelsynthesismethods
providing each of them wide-baseline four camera views as input during infer-
ence.Specially,wecompareourmethodwithgeneralizablereconstruction[65,95]MetaCap 11
Table 1: Quantitative Comparison. Our method achieves state-of-the-art results
for novel-view synthesis and geometry reconstruction. *Note, that ARAH requires 4D
scans for meta learning and videos for the fine-tuning whereas other methods solely
require static images.
Appearance Geometry
Method Subject
PSNR‚ÜëSSIM‚ÜëLPIPS‚ÜìNC-Cos‚ÜìNC-L2‚ÜìChamfer‚ÜìP2S‚ÜìIOU‚Üë
S2 - - - 0.143 0.445 1.305 1.303 0.764
DeepMultiCap [95] S27 - - - 0.149 0.454 1.466 1.831 0.797
S5 - - - 0.183 0.504 2.238 2.224 0.794
S2 - - - 0.150 0.450 1.150 1.261 0.813
DiffStereo [65] S27 - - - 0.161 0.471 1.093 1.185 0.818
S5 - - - 0.173 0.491 1.714 1.787 0.805
S2 25.469 0.817 0.307 0.123 0.402 1.647 2.245 0.426
DVA [58] S27 24.709 0.826 0.295 0.137 0.429 1.624 2.145 0.399
S5 23.093 0.750 0.344 0.167 0.482 2.308 2.805 0.247
S2 25.770 0.810 0.305 0.150 0.445 1.618 2.238 0.767
TransHuman [50] S27 23.876 0.800 0.304 0.146 0.440 1.487 1.970 0.791
S5 23.072 0.736 0.349 0.183 0.495 1.898 2.039 0.763
S2 26.279 0.833 0.302 0.079 0.315 0.839 0.913 0.859
ARAH* [78] S27 24.666 0.841 0.282 0.080 0.316 0.776 0.815 0.850
S5 23.532 0.775 0.332 0.139 0.419 1.620 1.915 0.842
S2 26.529 0.841 0.249 0.096 0.351 0.679 0.814 0.887
Ours S27 25.284 0.849 0.247 0.104 0.370 0.614 0.734 0.891
S5 23.996 0.777 0.302 0.147 0.448 1.133 1.277 0.877
Table2:QuantitativeAblation.Here,weevaluatedifferentinitializationstrategies
and canonicalization types on rendering quality. Networks initialized with random or
pre-trained weights tend to overfit the inputs, while our meta weights generalize well
to novel views.
InputView NovelView
InitType CanoType
PSNR‚ÜëSSIM‚ÜëLPIPS‚ÜìNC-Cos‚ÜìNC-L2‚ÜìPSNR‚ÜëSSIM‚ÜëLPIPS‚ÜìNC-Cos‚ÜìNC-L2‚Üì
Root 31.678 0.955 0.113 0.173 0.455 23.802 0.782 0.276 0.171 0.454
Random SMPL 31.180 0.953 0.120 0.225 0.535 23.691 0.770 0.292 0.214 0.523
DDC 31.429 0.954 0.120 0.223 0.530 23.566 0.772 0.289 0.216 0.523
Root - - - - - - - - - -
Pretrain SMPL - - - - - - - - - -
DDC 32.423 0.959 0.114 0.110 0.386 25.829 0.820 0.239 0.116 0.395
Root 24.477 0.870 0.253 0.170 0.480 23.094 0.764 0.326 0.168 0.475
Meta SMPL 28.224 0.903 0.215 0.092 0.345 26.120 0.833 0.258 0.096 0.349
DDC 29.176 0.912 0.209 0.089 0.341 26.529 0.841 0.249 0.096 0.351
and rendering [50] methods, a subject-specific rendering method [58], and a
subject-specificoptimizationmethodwithgeometrypriorinitialization[78].For
more details, please refer to supplementary materials.
(1) Generalizable Methods. Given large-scale 3D human datasets [3,95],
DeepMultiCap [95] learns to predict occupancy fields from multi-view RGBs by
fusingRGBfeatures,normalfeaturesandvoxelfeatureswithanattentionlayer,
while DiffuStereo [65] trained a diffusion model to refine coarse stereo disparity.
TransHuman [50] captures global relationships of human parts in the canonical
space and learns to predict radiance fields from multi-view videos.
(2)Subject-specificMethods.Givendense-viewRGBvideos,DVA[58]learns
articulatedvolumetricprimitivesattachedtoaparametricbodymodelcombined
with texel-aligned features. ARAH [78] extends a meta-learned SDF prior [77]
with a root-finding method to reconstruct clothed avatars from a sparse set of
multi-view RGB videos.
WedemonstratequalitativeandquantitativecomparisonsinFig.4andTab.1.12 Sun et al.
Fig.6:QualitativeAblation.Here,westudytheinfluenceofocclusionhandlingand
view numbers during meta-learning (top) and fine-tuning (bottom).
Table 3: Quantitative Ablation. Here, DeepMultiCap [95] directly regresses
we study initialization strategies and occupancy in world space, due to the
canonicalizationtypesintermsofgeometry articulated structure of humans and
results.Again,ourmeta-learningparadigm the limited scale of 3D scan datasets,
incombinationwithspacecanonicalization
their generalization ability is rather
outperforms the baselines.
limited. DiffuStereo [65] struggles to
find correct image correspondences
InitTypeCanoTypeChamfer‚Üì P2S‚Üì IOU‚Üë
under our challenging wide-baseline
Root 1.610 2.253 0.823
Random SMPL 1.291 1.596 0.806 camera setup, leading to noisy recon-
DDC 1.184 1.498 0.815 struction. DVA [58] learns rotation,
Root - - -
Pretrain SMPL - - - translation, and scale of primitives
DDC 0.726 0.855 0.882 to recover rough geometry. However,
Root 1.692 2.001 0.817 theirtextureun-projectionstepissen-
Meta SMPL 0.799 0.897 0.885
sitive to the template, and tracking
DDC 0.679 0.814 0.887
errors of the template lead to blurred
results. TransHuman [50] integrates pixel-aligned features to improve texture
details. However, they do not model geometry explicitly, which result in noisy
geometryandtextureboundaries.ARAH‚Äôsgeometrynetwork[77]learnsameta
prior taking 3D position as input and outputs the SDF value. However, due to
their purely MLP-based architecture and non-end-to-end design, we found their
results suffer from blurred appearance and less detailed geometry. Besides, the
root-finding is not efficient taking over ten hours with 4 GPUs to finetune. In
contrast, we use explicit hash encodings for spatial features, which is efficient
and effective for meta feature learning. All the rendering methods above use
parametric models [40,53] as template, so they can not model loose clothing
correctly (see S5 in Fig. 4). In contrast, our method recovers high-fidelity ap-
pearance and geometry even for loose types of apparel in minutes under this
challenging setup, i.e. 4 wide-baseline cameras.
4.2 Ablation Studies
We perform ablation studies on the test set of the ‚ÄúS2‚Äù subject to evaluate the
effectiveness of our design choices.MetaCap 13
Fig.7:ConvergenceandQuality.Notethatourmeta-learnedweightsachievefaster
convergence and higher accuracy.
Weight Initialization and Space Canonicalization. Tab. 2/3 and Fig. 5
showtheablationresultsforweightinitializationandspacecanonicalization.We
choosethefollowingbaselinesforweightinitialization:Random:Networkweights
are initialized randomly. Pretrain: Network weights are pre-trained on all train-
ingframes.Meta:Networkweightsarelearnedthroughmeta-learning.Forspace
canonicalization,wechoosethefollowingbaselines:Root:Thespacetransforma-
tion is defined by the skeletal root rotation and translation. SMPL: The space
transformation is defined by the nearest surface point on the SMPL [40] tem-
plate. DDC: The space transformation is defined by the nearest point on the
deformable DDC [22] template.
Randominitializationtypicallyoverfitstheinputviews,however,onnovelviews
itperformspoorly.Meta-learninginroot-normalizedspacealsoperformspoorly
as the articulated nature of the humans poses a significant challenge for meta-
learning the optimal network weights. In contrast, the combination of our key
technical components, i.e. meta-learning weights and space canonicalization, ef-
fectively addresses those issues and achieves the best result for, both, geometry
reconstruction and novel view synthesis. Moreover, we highlight that this holds
trueirrespectiveofthespecificchoiceofthehumantemplate,i.e.SMPLorDDC.
Number of Camera Views and Occlusion Handling. Since our method
solely takes videos for meta-learning and images for fine-tuning, it naturally
supports arbitrary camera setups in both stages. As illustrated in Fig. 6, when
increasing the number of cameras, our method obtains better rendering and re-
construction results. While this is not too surprising, we highlight that even
under sparser settings our method performs reasonably well. Most interestingly,
meta-learningcanevenbeperformedonamonocularvideo,whichisnotpossible
for ARAH [78] and MetaAvatar [77], since they rely on scans for meta-learning.
Besides, at inference, our method allows fine-tuning using just a single view.
Moreover, when performing monocular fine-tuning, our occlusion handling ap-
proach (1+OH) outperforms the baseline, which is not using this component.
Convergence Speed and Quality. In Fig. 7, we evaluate the convergence
of our method compared to random and pre-trained weight initializations. Our
proposeddesignoutperformsthebaselinesin,both,convergencespeedandaccu-
racy. Since one fine-tuning step only takes 50ms, our meta-learned initialization
already converges after 40s.14 Sun et al.
(a) (b) (c) (d) (e) (f)
Fig.8:QualitativeIn-the-wildComparison.Toevaluatetherobustnesstoin-the-
wild conditions, we compare our work and ARAH on WildDynaCap. Our approach
achieves significantly better results in terms of appearance and geometric details. (a,
d) donate ground truth images at novel view. (b, e) are the results of our method. (c,
f) are the results of ARAH.
4.3 Evaluation on In-the-wild Sequences
Fig. 8 and Tab. 4 show results on our
Table 4: Quantitative In-the-Wild
Comparison. We outperform ARAH WildDynaCap dataset. Our method re-
on in-the-wild sequences. covers high-fidelity geometry and appear-
ance, consistently outperforming ARAH.
Method Subject PSNR‚ÜëSSIM‚ÜëLPIPS‚Üì
ARAH* [78] S2 19.027 0.608 0.412 These results show the robustness of our
S27 22.075 0.702 0.343
method to complex background, lighting
S2 19.156 0.661 0.351
Ours S27 22.080 0.750 0.260 conditions, and camera types.
5 Limitations
Although our method achieves high-fidelity personalized human reconstruction
and rendering from sparse observation, it still has a few limitations. First, our
method can be sensitive to template fitting or motion capture results. Second,
we do not take the temporal information into consideration. Integrating reliable
observations or constraints from adjacent frames like Newcombe et al. [46] may
lead to more robust geometry and rendering results. Third, our method can not
handle hands very well. Modeling hand with a more fine-grained template and
motion capture could be a potential solution.
6 Conclusion
ThispaperintroducedMetaCap,anovelapproachtowardshigh-fidelityperfor-
mance capture and photo-realistic rendering of humans from very sparse multi-
vieworevenmonocularRGBimages.Weintroducedahybridrepresentationthat
benefitsfrombothexplicitandimplicithumanrepresentation.Throughcompre-
hensiveevaluations,we demonstratedthatperformingmeta-learningwithspace
canonicalization proves to be a crucial design factor, thereby providing a strong
data-driven prior on the human capture. Our results demonstrate high-fidelity
human reconstruction and free-view rendering as well as the versatility of ap-
proachintermsofcamerasetups,clothingtypes,andcanonicalizationstrategies.
In the future, we plan to explore even faster, potentially real-time, fine-tuning
strategies as well as learning priors across individuals.
Acknowledgements: This research was supported by the ERC Consolidator
Grant 4DRepLy (770784).MetaCap 15
References
1. https://github.com/ayushtewari/GVV-Differentiable-CUDA-Renderer
2. https://github.com/zju3dv/EasyMocap
3. https://web.twindom.com/
4. Antoniou, A., Edwards, H., Storkey, A.: How to train your maml. arXiv preprint
arXiv:1810.09502 (2018)
5. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-
vasan,P.P.:Mip-nerf:Amultiscalerepresentationforanti-aliasingneuralradiance
fields. ICCV (2021)
6. Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-nerf
360: Unbounded anti-aliased neural radiance fields. CVPR (2022)
7. Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Zip-nerf:
Anti-aliased grid-based neural radiance fields. ICCV (2023)
8. B√ºhler,M.C.,Sarkar,K.,Shah,T.,Li,G.,Wang,D.,Helminger,L.,Orts-Escolano,
S.,Lagun,D.,Hilliges,O.,Beeler,T.,etal.:Preface:Adata-drivenvolumetricprior
forfew-shotultrahigh-resolutionfacesynthesis.In:ProceedingsoftheIEEE/CVF
International Conference on Computer Vision. pp. 3402‚Äì3413 (2023)
9. Chen,A., Xu,Z., Geiger, A.,Yu,J., Su,H.: Tensorf: Tensorial radiancefields. In:
European Conference on Computer Vision (ECCV) (2022)
10. Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: Mvsnerf: Fast
generalizableradiancefieldreconstructionfrommulti-viewstereo.In:Proceedings
oftheIEEE/CVFInternationalConferenceonComputerVision.pp.14124‚Äì14133
(2021)
11. Collet,A.,Chuang,M.,Sweeney,P.,Gillett,D.,Evseev,D.,Calabrese,D.,Hoppe,
H., Kirk, A., Sullivan, S.: High-quality streamable free-viewpoint video. ACM
Transactions on Graphics (ToG) 34(4), 1‚Äì13 (2015)
12. Davydov, A., Remizova, A., Constantin, V., Honari, S., Salzmann, M., Fua, P.:
AdversarialParametricPosePrior.In:IEEE/CVFConferenceonComputerVision
and Pattern Recognition (CVPR) (2022)
13. DeLuigi,L.,Li,R.,Guillard,B.,Salzmann,M.,Fua,P.:DrapeNet:GarmentGen-
erationandSelf-SupervisedDraping.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition (2023)
14. Fabbri,R.,Costa,L.D.F.,Torelli,J.C.,Bruno,O.M.:2deuclideandistancetrans-
formalgorithms:Acomparativesurvey.ACMComputingSurveys(CSUR)40(1),
1‚Äì44 (2008)
15. Finn,C.,Abbeel,P.,Levine,S.:Model-agnosticmeta-learningforfastadaptation
ofdeepnetworks.In:Internationalconferenceonmachinelearning.pp.1126‚Äì1135.
PMLR (2017)
16. Fridovich-Keil,S.,Yu,A.,Tancik,M.,Chen,Q.,Recht,B.,Kanazawa,A.:Plenox-
els: Radiance fields without neural networks. In: CVPR (2022)
17. Gropp, A., Yariv, L., Haim, N., Atzmon, M., Lipman, Y.: Implicit geometric reg-
ularization for learning shapes. In: Proceedings of Machine Learning and Systems
2020, pp. 3569‚Äì3579 (2020)
18. Gu,J.,Trevithick,A.,Lin,K.E.,Susskind,J.,Theobalt,C.,Liu,L.,Ramamoorthi,
R.:Nerfdiff:Single-imageviewsynthesiswithnerf-guideddistillationfrom3d-aware
diffusion. In: International Conference on Machine Learning (2023)
19. Guangcong, Chen, Z., Loy, C.C., Liu, Z.: Sparsenerf: Distilling depth ranking for
few-shot novel view synthesis. Technical Report (2023)16 Sun et al.
20. Guo, K., Lincoln, P., Davidson, P., Busch, J., Yu, X., Whalen, M., Harvey, G.,
Orts-Escolano, S.,Pandey, R., Dourgarian, J.,et al.: Therelightables: Volumetric
performance capture of humans with realistic relighting. ACM Transactions on
Graphics (ToG) 38(6), 1‚Äì19 (2019)
21. Habermann,M.,Liu,L.,Xu,W.,Pons-Moll,G.,Zollhoefer,M.,Theobalt,C.:Hd-
humans:Ahybridapproachforhigh-fidelitydigitalhumans.Proc.ACMComput.
Graph. Interact. Tech. (2023)
22. Habermann, M., Liu, L., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.:
Real-time deep dynamic characters. ACM Transactions on Graphics 40(4) (aug
2021)
23. Habermann, M., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.: Livecap:
Real-time human performance capture from monocular video (2019)
24. Habermann, M., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.: Deepcap:
Monocularhumanperformancecaptureusingweaksupervision.In:IEEEConfer-
ence on Computer Vision and Pattern Recognition (CVPR). IEEE (jun 2020)
25. Hadwiger, M., Al-Awami, A.K., Beyer, J., Agus, M., Pfister, H.: Sparseleap: Effi-
cientemptyspaceskippingforlarge-scalevolumerendering.IEEEtransactionson
visualization and computer graphics 24(1), 974‚Äì983 (2017)
26. Hospedales, T., Antoniou, A., Micaelli, P., Storkey, A.: Meta-learning in neural
networks:Asurvey.IEEEtransactionsonpatternanalysisandmachineintelligence
44(9), 5149‚Äì5169 (2021)
27. Huang, Y., Yi, H., Xiu, Y., Liao, T., Tang, J., Cai, D., Thies, J.: TeCH: Text-
guided Reconstruction of Lifelike Clothed Humans. In: International Conference
on 3D Vision (3DV) (2024)
28. Huber,P.J.:Robustestimationofalocationparameter.In:Breakthroughsinstatis-
tics: Methodology and distribution, pp. 492‚Äì518. Springer (1992)
29. Jiang, W., Yi, K.M., Samei, G., Tuzel, O., Ranjan, A.: Neuman: Neural human
radiance field from a single video. In: Proceedings of the European conference on
computer vision (ECCV) (2022)
30. Jiang, Y., Habermann, M., Golyanik, V., Theobalt, C.: Hifecap: Monocular high-
fidelity and expressive capture of human performances. In: BMVC (2022)
31. Kazhdan,M.,Bolitho,M.,Hoppe,H.:Poissonsurfacereconstruction.In:Proceed-
ings of the fourth Eurographics symposium on Geometry processing. vol. 7, p. 0
(2006)
32. Kerbl, B., Kopanas, G., Leimk√ºhler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics 42(4) (July
2023), https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/
33. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017)
34. Kwon, Y., Kim, D., Ceylan, D., Fuchs, H.: Neural human performer: Learning
generalizableradiancefieldsforhumanperformancerendering.AdvancesinNeural
Information Processing Systems 34, 24741‚Äì24752 (2021)
35. Li, K., Malik, J.: Learning to optimize. arXiv preprint arXiv:1606.01885 (2016)
36. Li,R.,Tanke,J.,Vo,M.,Zollh√∂fer,M.,Gall,J.,Kanazawa,A.,Lassner,C.:Tava:
Template-free animatable volumetric actors. In: European Conference on Com-
puter Vision. pp. 419‚Äì436. Springer (2022)
37. Li, Z., Zheng, Z., Zhang, H., Ji, C., Liu, Y.: Avatarcap: Animatable avatar condi-
tioned monocular human volumetric capture. In: European Conference on Com-
puter Vision (ECCV) (October 2022)
38. Liu, L., Habermann, M., Rudnev, V., Sarkar, K., Gu, J., Theobalt, C.: Neural
actor: Neural free-view synthesis of human actors with pose control. ACM Trans.
Graph.(ACM SIGGRAPH Asia) (2021)MetaCap 17
39. Long,X.,Lin,C.,Wang,P.,Komura,T.,Wang,W.:Sparseneus:Fastgeneralizable
neural surface reconstruction from sparse views. ECCV (2022)
40. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A
skinnedmulti-personlinearmodel.ACMTrans.Graphics(Proc.SIGGRAPHAsia)
34(6), 248:1‚Äì248:16 (Oct 2015)
41. Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., Black, M.J.:
Learning to Dress 3D People in Generative Clothing. In: Computer Vision and
Pattern Recognition (CVPR) (2020)
42. Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy
networks: Learning 3d reconstruction in function space. In: Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.4460‚Äì4470
(2019)
43. Mihajlovic, M., Bansal, A., Zollhoefer, M., Tang, S., Saito, S.: KeypointNeRF:
Generalizingimage-basedvolumetricavatarsusingrelativespatialencodingofkey-
points. In: European conference on computer vision (2022)
44. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.In:ECCV
(2020)
45. M√ºller,T.,Evans,A.,Schied,C.,Keller,A.:Instantneuralgraphicsprimitiveswith
amultiresolutionhashencoding.ACMTransactionsonGraphics(ToG)41(4),1‚Äì
15 (2022)
46. Newcombe, R.A., Fox, D., Seitz, S.M.: Dynamicfusion: Reconstruction and track-
ing of non-rigid scenes in real-time. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 343‚Äì352 (2015)
47. Nichol, A., Achiam, J., Schulman, J.: On first-order meta-learning algorithms.
arXiv preprint arXiv:1803.02999 (2018)
48. Niemeyer,M.,Barron,J.T.,Mildenhall,B.,Sajjadi,M.S.,Geiger,A.,Radwan,N.:
Regnerf:Regularizingneuralradiancefieldsforviewsynthesisfromsparseinputs.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 5480‚Äì5490 (2022)
49. Palafox,P.,Sarafianos,N.,Tung,T.,Dai,A.:Spams:Structuredimplicitparamet-
ric models. CVPR (2022)
50. Pan, X., Yang, Z., Ma, J., Zhou, C., Yang, Y.: Transhuman: A transformer-based
humanrepresentationforgeneralizableneuralhumanrendering.In:Proceedingsof
the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 3544‚Äì
3555 (October 2023)
51. Park,J.J.,Florence,P.,Straub,J.,Newcombe,R.,Lovegrove,S.:Deepsdf:Learning
continuous signed distance functions for shape representation. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition. pp. 165‚Äì
174 (2019)
52. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in pytorch (2017)
53. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas,
D.,Black,M.J.:Expressivebodycapture:3Dhands,face,andbodyfromasingle
image. In: Proceedings IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR). pp. 10975‚Äì10985 (2019)
54. Peng,S.,Dong,J.,Wang,Q.,Zhang,S.,Shuai,Q.,Zhou,X.,Bao,H.:Animatable
neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)
55. Peng, S., Zhang, S., Xu, Z., Geng, C., Jiang, B., Bao, H., Zhou, X.: Animat-
able neural implicit surfaces for creating avatars from videos. arXiv preprint
arXiv:2203.08133 (2022)18 Sun et al.
56. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural
body: Implicit neural representations with structured latent codes for novel view
synthesis of dynamic humans. In: CVPR (2021)
57. Rajeswaran, A., Finn, C., Kakade, S.M., Levine, S.: Meta-learning with implicit
gradients. Advances in neural information processing systems 32 (2019)
58. Remelli,E.,Bagautdinov,T.,Saito,S.,Wu,C.,Simon,T.,Wei,S.E.,Guo,K.,Cao,
Z., Prada, F., Saragih, J., et al.: Drivable volumetric avatars using texel-aligned
features. In: ACM SIGGRAPH 2022 Conference Proceedings (2022)
59. Ruder,S.:Anoverviewofgradientdescentoptimizationalgorithms.arXivpreprint
arXiv:1609.04747 (2016)
60. Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., Li, H.: Pifu:
Pixel-aligned implicit function for high-resolution clothed human digitization. In:
Proceedings of the IEEE/CVF international conference on computer vision. pp.
2304‚Äì2314 (2019)
61. Saito, S., Simon, T., Saragih, J., Joo, H.: Pifuhd: Multi-level pixel-aligned im-
plicit function for high-resolution 3d human digitization. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 84‚Äì93
(2020)
62. Saito,S.,Yang,J.,Ma,Q.,Black,M.J.:SCANimate:Weaklysupervisedlearningof
skinned clothed avatar networks. In: Proceedings IEEE/CVF Conf. on Computer
Vision and Pattern Recognition (CVPR) (Jun 2021)
63. Shao,R.,Chen,L.,Zheng,Z.,Zhang,H.,Zhang,Y.,Huang,H.,Guo,Y.,Liu,Y.:
Floren: Real-time high-quality human performance rendering via appearance flow
usingsparsergbcameras.In:SIGGRAPHAsia2022ConferencePapers.pp.1‚Äì10
(2022)
64. Shao, R., Zhang, H., Zhang, H., Chen, M., Cao, Y., Yu, T., Liu, Y.: Doublefield:
Bridgingtheneuralsurfaceandradiancefieldsforhigh-fidelityhumanreconstruc-
tion and rendering. In: CVPR (2022)
65. Shao, R., Zheng, Z., Zhang, H., Sun, J., Liu, Y.: Diffustereo: High quality hu-
man reconstruction via diffusion-based stereo using sparse cameras. In: European
Conference on Computer Vision. pp. 702‚Äì720. Springer (2022)
66. Shen, K., Guo, C., Kaufmann, M., Zarate, J., Valentin, J., Song, J., Hilliges, O.:
X-avatar: Expressive human avatars. Computer Vision and Pattern Recognition
(CVPR) (2023)
67. Shetty, A., Habermann, M., Sun, G., Luvizon, D., Golyanik, V., Theobalt, C.:
Holoported characters: Real-time free-viewpoint rendering of humans from sparse
rgb cameras (2023)
68. Shuai, Q., Geng, C., Fang, Q., Peng, S., Shen, W., Zhou, X., Bao, H.: Novel view
synthesis of human interactions from sparse multi-view videos. In: SIGGRAPH
Conference Proceedings (2022)
69. Sitzmann, V., Chan, E., Tucker, R., Snavely, N., Wetzstein, G.: Metasdf: Meta-
learningsigneddistancefunctions.AdvancesinNeuralInformationProcessingSys-
tems 33, 10136‚Äì10147 (2020)
70. Stoll, C., Hasler, N., Gall, J., Seidel, H.P., Theobalt, C.: Fast articulated motion
trackingusingasumsofgaussiansbodymodel.In:2011InternationalConference
on Computer Vision. pp. 951‚Äì958. IEEE (2011)
71. Su, Z., Xu, L., Zheng, Z., Yu, T., Liu, Y., Fang, L.: Robustfusion: Human volu-
metric capture with data-driven visual cues using a rgbd camera. In: Computer
Vision‚ÄìECCV2020:16thEuropeanConference,Glasgow,UK,August23‚Äì28,2020,
Proceedings, Part IV 16. pp. 246‚Äì264. Springer (2020)MetaCap 19
72. Tancik,M.,Mildenhall,B.,Wang,T.,Schmidt,D.,Srinivasan,P.P.,Barron,J.T.,
Ng, R.: Learned initializations for optimizing coordinate-based neural representa-
tions. In: CVPR (2021)
73. Tretschk,E.,Kairanda,N.,BR,M.,Dabral,R.,Kortylewski,A.,Egger,B.,Haber-
mann,M.,Fua,P.,Theobalt,C.,Golyanik,V.:Stateoftheartindensemonocular
non-rigid3dreconstruction.ComputerGraphicsForum(EurographicsStateofthe
Art Reports) (2023)
74. Wang, K., Peng, S., Zhou, X., Yang, J., Zhang, G.: Nerfcap: Human performance
capture with dynamic neural radiance fields. IEEE Transactions on Visualization
and Computer Graphics (2022)
75. Wang,K.,Zhang,G.,Cong,S.,Yang,J.:Clothedhumanperformancecapturewith
adouble-layerneuralradiancefields.In:ComputerVisionandPatternRecognition
(CVPR) (2023)
76. Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learn-
ing neural implicit surfaces by volume rendering for multi-view reconstruction.
NeurIPS (2021)
77. Wang, S., Mihajlovic, M., Ma, Q., Geiger, A., Tang, S.: Metaavatar: Learning
animatableclothedhumanmodelsfromfewdepthimages.In:AdvancesinNeural
Information Processing Systems (2021)
78. Wang, S., Schwarz, K., Geiger, A., Tang, S.: Arah: Animatable volume rendering
of articulated human sdfs. In: European Conference on Computer Vision (2022)
79. Wang, Y., Han, Q., Habermann, M., Daniilidis, K., Theobalt, C., Liu, L.: Neus2:
Fast learning of neural implicit surfaces for multi-view reconstruction. arXiv
preprint arXiv:2212.05231 (2022)
80. Weng,C.Y.,Curless,B.,Srinivasan,P.P.,Barron,J.T.,Kemelmacher-Shlizerman,
I.:HumanNeRF:Free-viewpointrenderingofmovingpeoplefrommonocularvideo.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 16210‚Äì16220 (June 2022)
81. Xiang, D., Prada, F., Cao, Z., Guo, K., Wu, C., Hodgins, J., Bagautdinov, T.:
Drivable avatar clothing: Faithful full-body telepresence with dynamic clothing
driven by sparse rgb-d input. In: SIGGRAPH Asia 2023 Conference Papers. pp.
1‚Äì11 (2023)
82. Xiang, D., Prada, F., Wu, C., Hodgins, J.: Monoclothcap: Towards temporally
coherent clothing capture from monocular rgb video. In: 2020 International Con-
ference on 3D Vision (3DV). pp. 322‚Äì332. IEEE (2020)
83. Xiu,Y.,Yang,J.,Cao,X.,Tzionas,D.,Black,M.J.:Econ:Explicitclothedhumans
optimized via normal integration. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 512‚Äì523 (2023)
84. Xiu, Y., Yang, J., Tzionas, D., Black, M.J.: ICON: Implicit Clothed humans Ob-
tainedfromNormals.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition (CVPR). pp. 13296‚Äì13306 (June 2022)
85. Xu, W., Chatterjee, A., Zollh√∂fer, M., Rhodin, H., Mehta, D., Seidel, H.P.,
Theobalt, C.: Monoperfcap: Human performance capture from monocular video.
ACM Trans. Graph. 37(2), 27:1‚Äì27:15 (May 2018). https://doi.org/10.1145/
3181973, http://doi.acm.org/10.1145/3181973
86. Xue, Y., Bhatnagar, B.L., Marin, R., Sarafianos, N., Xu, Y., Pons-Moll, G.,
Tung,T.:NSF:NeuralSurfaceFieldforHumanModelingfromMonocularDepth.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) (October 2023)
87. Yang, J., Pavone, M., Wang, Y.: Freenerf: Improving few-shot neural rendering
with free frequency regularization (2023)20 Sun et al.
88. Yu,A.,Li,R.,Tancik,M.,Li,H.,Ng,R.,Kanazawa,A.:PlenOctreesforreal-time
rendering of neural radiance fields. In: ICCV (2021)
89. Yu,A.,Ye,V.,Tancik,M.,Kanazawa,A.:pixelNeRF:Neuralradiancefieldsfrom
one or few images. In: CVPR (2021)
90. Yu, T., Zheng, Z., Guo, K., Zhao, J., Dai, Q., Li, H., Pons-Moll, G., Liu, Y.:
Doublefusion: Real-time capture of human performances with inner body shapes
from a single depth sensor. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 7287‚Äì7296 (2018)
91. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: Conference on Computer
Vision and Pattern Recognition (CVPR). pp. 586‚Äì595. IEEE Computer Society,
Los Alamitos, CA, USA (2018). https://doi.org/10.1109/CVPR.2018.00068,
https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00068
92. Zhang, S., Tong, H., Xu, J., Maciejewski, R.: Graph convolutional networks: a
comprehensive review. Computational Social Networks 6(1), 1‚Äì23 (2019)
93. Zhao, F., Jiang, Y., Yao, K., Zhang, J., Wang, L., Dai, H., Zhong, Y., Zhang, Y.,
Wu, M., Xu, L., et al.: Human performance modeling and rendering via neural
animated mesh. ACM Transactions on Graphics (TOG) 41(6), 1‚Äì17 (2022)
94. Zhao, F., Yang, W., Zhang, J., Lin, P., Zhang, Y., Yu, J., Xu, L.: Humannerf:
Efficiently generated human radiance field from sparse inputs. In: Proceedings of
theIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).
pp. 7743‚Äì7753 (June 2022)
95. Zheng,Y.,Shao,R.,Zhang,Y.,Yu,T.,Zheng,Z.,Dai,Q.,Liu,Y.:Deepmulticap:
Performance capture of multiple characters using sparse multiview cameras. In:
Proceedingsof the IEEE/CVFInternationalConference on ComputerVision. pp.
6239‚Äì6249 (2021)
96. Zheng,Z.,Huang,H.,Yu,T.,Zhang,H.,Guo,Y.,Liu,Y.:Structuredlocalradiance
fields for human avatar modeling. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) (June 2022)
97. Zheng,Z.,Yu,T.,Liu,Y.,Dai,Q.:Pamir:Parametricmodel-conditionedimplicit
representation for image-based human reconstruction (2021)
98. Zheng,Z.,Yu,T.,Wei,Y.,Dai,Q.,Liu,Y.:Deephuman:3dhumanreconstruction
from a single image. In: The IEEE International Conference on Computer Vision
(ICCV) (October 2019)
99. Zuo,X.,Wang,S.,Zheng,J.,Yu,W.,Gong,M.,Yang,R.,Cheng,L.:Sparsefusion:
Dynamichumanavatarmodelingfromsparsergbdimages.IEEETransactionson
Multimedia 23, 1617‚Äì1629 (2020)MetaCap 21
7 Overview of Supplementary Material
To facilitate a more comprehensive analysis and understanding of MetaCap
and experiment configurations, we offer additional results (Sec. 8), method de-
tails (Sec. 9, 10), implementation details (Sec. 11), comparisons (Sec. 12, 13),
ablations (Sec. 14), and applications (Sec. 15).
Fig.9: Qualitative Results. Here, we showcase additional qualitative results of
MetaCap utilizing four-view images as inputs, demonstrating its robustness across
diverse poses and different subjects.
8 More Results on Different Poses and Subjects
Fig. 9 presents additional qualitative results showcasing the performance of our
method across diverse motions and subjects. Since our method learns the meta
prior in the canonical pose space, it is robust to various testing poses.22 Sun et al.
9 Template Model
We revisit two types of human template, SMPL [40] and DDC [22] and demon-
stratehowtocomputethetransformationmatrixanddeformedpositionforeach
vertex, which are crucial for the canonicaliztion step. Here, each template has
vertices XÀÜ ‚ààRV√ó3 in canonical pose.
9.1 Parametric Template‚ÄìSMPL
SMPL[40]isaparametrichumanbodymodel.ItcharacterizesV verticesandJ
joint positions of the human mesh using shape parameters, Œ≤, and pose param-
eters, Œ∏. The overall mesh deformation, T (Œ≤,Œ∏), is determined by the sum of
def
shapedependentdisplacementsandposedependentdisplacements.Linearblend
skinning (LBS) is employed for animating the deformed mesh:
K
(cid:88)
T (Œ∏)= w T (Œ∏) (5)
FK,i j,i j
j=1
T=T (Œ∏)T (Œ≤,Œ∏) (6)
FK def
X=TXÀÜ (7)
where w is the blend weight from joint j to vertex i, T (Œ∏) denotes the joint
j,i j
j‚Äôs local transformation, T (Œ∏) represents the global transformation of the
FK,i
deformed vertex i.
9.2 Deformable Template‚ÄìDDC
DDC[22]isapersonalizeddeformablebodymodel.Itmodelsmotion-dependent
body deformation with embedded graph deformation and vertex displacements.
Given skeletal motion M ={Œ∏ ,Œ± ,z ,...,Œ∏ ,Œ± ,z } at time f, it
f f‚àíW f‚àíW f‚àíW f f f
employs Graph Convolutional Networks (GCN) [92] to estimate the embedded-
graph‚Äôsdeformationparameters A,T‚ààRK√ó3 andtheper-vertexdisplacements
d. The final geometry is obtained through Dual Quaternion Skinning (DQS):
(cid:88) (cid:20)R(a k) d i+(I‚àíR(a k))g k+t k(cid:21)
T (M )= w ‚Üí‚àí (8)
def,i f k,i
0 1
k
T=T (M )T (M ) (9)
FK f def f
X=TXÀÜ (10)
where w represents the weight from graph node k to vertex i, R(¬∑) transforms
k,i
Euler representations into matrix representations, a and t are node k‚Äôs local
k k
rotation and translation, g denotes the position of node k.
kMetaCap 23
(a) (b) (c) (d)
Fig.10: Illustrations of image proxy. (a) Visualization of the camera distribution for
rendering the proxy images. (b) Monocular frames used to generate in-the-wild image
proxy. (c) Proxy images in the dome. (d) Proxy images in the wild. Best viewed with
zoom.
10 Proxy Image Generation for Occlusion Handling
We propose the occlusion handling to address missing information when occlu-
sion happens. To offer additional information for the occluded areas, we render
proxy images of the human in canonical pose space. Depending on the light-
ing condition and camera setup, we have two configurations: in-the-dome and
in-the-wild. Here, we demonstrate the details of proxy image generation (see
Fig. 10).
10.1 Proxy Image Generation in the Dome
We have dense-view cameras for the in-the-dome case. Consequently, we select
one frame to reconstruct its geometry and texture with space canonicalization.
This enables us to render novel-view in-the-dome proxy images in the canonical
space (see Fig. 10 (c)).
10.2 Proxy Image Generation in the Wild
Under the in-the-wild scenarios, where sparse-view or monocular cameras are
predominant,occlusionhandlingbecomesparticularlycrucial,especiallyinmonoc-
ular scenarios. Consider the most challenging scenario, namely the monocular
camerasetup.Insuchsituations,itisnotfeasibletodirectlyreconstructgeome-
tryandappearancelikein-the-domescenario.Instead,leveragingthecapabilities
of the meta prior, we fine-tune multiple frames (see Fig. 10 (b)) into a unified
canonical space simultaneously to construct a complete human representation.
Wethenrenderitintonovel-viewin-the-wildproxyimagesinthecanonicalspace
(see Fig. 10 (d)).
11 Implementation Details
In this section, we present implementation details of MetaCap (Sec. 11.1), im-
plementation details of methods that we compare with (Sec. 11.2), ablations24 Sun et al.
Fig.11:Visualizationofthecameradistributionforpriorlearning,inferenceandevalu-
ationinthecomparisonwithstate-of-the-artmethods.Dense-viewcameraswithpurple
colorarethetrainingviewsusedforpriorlearning.Four-viewcameraswithgreencolor
aretheinputviewsduringinference.Six-viewcameraswithredcoloraretheevaluation
views.
(Sec. 11.3), and comparison on in-the-wild sequences (Sec. 11.4). Fig. 11 illus-
trates the camera distribution when conducting the prior learning, fine-tuning
and evaluation in the comparisons. These three sets of cameras do not overlap.
Additionally, the motions used for prior learning and fine-tuning are distinct.
11.1 MetaCap
Space Canonicalization. During comparison, we utilize the deformable tem-
plate DDC [22] as the default template for space canonicalization. To acquire
the template and the deformation parameters, we first create a character with
smoothedtemplate,embeddedgraph,skeletonanddefaultmotion.Subsequently,
we follow the methodology outlined in [22] to implement multi-view silhouette
supervision using a differentiable renderer [1] and distance transformation [14].
For the loose-cloth subject ‚ÄôS5‚Äô, we apply additional Chamfer loss supervision.
For parametric models SMPL [40] and SMPL-X [53] used in the ablations and
comparisonmethods,wefirstobtain3Dmarkerpositionsbyanimatingthechar-
acter‚Äôsskeletonwiththesamemotionsusedinthedeformabletemplate.Wethen
utilize EasyMocap [2] to estimate the shape and pose parameters.
Meta-learning and Fine-tuning. Duringthemeta-learningstage,weuseap-
proximately100-frameimagescapturedby100-viewcameras,pairedwithhuman
template at each frame. We apply SGD [59] with lout = 1.0 to the outer loopMetaCap 25
and Adam [33] with lin =1e‚àí4 to the inner loop. In each outer loop sampling
step, we randomly sample M =24 camera views and rays on each image simul-
taneously. After M = 24 unrolled gradient steps in the inner loop, we follow
Reptile [47] to update the outer loop weights. Both loops consist of a learning
ratewarm-upschedule.Theinnerloopiswarmedupbylinearlyupdatinglearn-
ing rate from 1% to 100% for 50 steps, while the outer loop has 100 warm-up
steps with the learning rate linearly updated from 10% to 100%. The template
threshold Œ∑ is set to 0.05 for ‚ÄòS2‚Äô and ‚ÄòS5‚Äô, and to 0.01 for ‚ÄòS3‚Äô and ‚ÄòS27‚Äô, with
a threshold decay to 50% applied after 300 outer loop steps. The total number
of meta-learning outer loop steps is 2000. The input images are resized to 50%
and applied Gaussian blur with a 5‚àó5 kernel. The weights of loss functions are
set as Œª =10.0,Œª =0.1,Œª =0.1,Œª =0.01.
c e m s
During the fine-tuning stage, we load the meta-learned weights and apply
the Adam optimizer [33] with learning rate lr = 1e‚àí4, Œ≤ = 0.9, Œ≤ = 0.99,
1 1
œµ = 1e ‚àí 15 to fine-tune weights for 3000 steps. In each step, we randomly
sample 8192 rays from all input observations. The template threshold Œ∑ is set
0.05. There‚Äôs no warm-up in this stage. The weights of loss functions are set as
Œª =10.0,Œª =0.1,Œª =0.1,Œª =0.01.
c e m s
11.2 Comparison Methods
DeepMultiCap. As DeepMultiCap [95] is trained on a large scale human scan
datasetandexhibitsgeneralizationability,weutilizetheofficialcheckpointwith-
outadditionalfine-tuning.ItreliesonSMPL-Xasthetemplatemodeltoprovide
geometry and global normal maps. Following the template procedure outlined
earlier, we fit SMPL-X and subsequently render it to produce normal maps.
DiffuStereo. Official DiffuStereo [65] utilizes geometry results from Double-
Field [64] for initializing the disparity maps. Since DoubleField [64] is not open
source, we employ the deformable template [22] as a substitute for initializing
thedisparitymaps,asitcontainsroughgeometryinformation.Subsequently,we
use the official checkpoint trained with 20-degree angle images to refine the dis-
parity maps. Due to the large camera baseline from 4-view cameras, the output
point-cloudsareoftenincomplete.Therefore,weincorporateadditionaltemplate
point-cloudstocompletethemeshwhenapplyingthePossionsurfacereconstruc-
tion [31].
Drivable Volumetric Avatars (DVA). The dataset division of DVA [58] is
the same as ours, including human template, training multi(dense)-view images
and testing sparse-view images. We utilize the official code with our estimated
SMPL-X parameters. We train the personalized DVA model using images from
dense-view training set. At the testing stage, we adhere to the original paper‚Äôs
mannerthatnofine-tuningadded,andemploysparse-viewimagesandtemplate
to render novel view images. Given that DVA does not focus on geometry re-
construction, we extract their estimated primitive parameters, convert them to26 Sun et al.
box meshes, and use Possion surface reconstruction [31] to reconstruct the final
watertight mesh.
TransHuman. TransHuman [50] is trained on multi-view videos with multi-
ple subjects. However, We found that directly applying the official pre-trained
checkpointonourdatayieldslow-qualityresults.Therefore,foreachsubject,we
fine-tune them individually on our training set, and generate testing set results
without additional fine-tuning.
ARAH. ARAH [78] incorporates a meta prior [77] trained from a large scale
scan dataset. In our implementation, we use the official checkpoint as initializa-
tion and further fine-tune it with all the frames in the testing set. It‚Äôs worth
notingthatothermethodsonlyutilize1-framesparse-viewimagesasinputdur-
ing inference.
11.3 Ablations
Weight Initialization and Space Canonicalization. During this ablation
study, we maintain the camera setup consistent with the comparison section.
Specifically, we utilize dense-view cameras for prior learning and four-view cam-
eras for fine-tuning.
Wehavethreetypesofnetworkinitializationconsistingoftwobaselinemeth-
ods random weights, pre-trained weights and our meta weights. Random weight
initialization utilizes the default weight initialization from PyTorch [52]. Pre-
trained weights are trained on the same views and frames as meta-learning.
It‚Äôs implemented by setting M = 1 in meta-learning process and training for
1000steps.Whenperformingthefine-tuningwithrandominitializationandpre-
trained initialization, we reserve 500 warm-up steps. Meta weights are obtained
following the procedure outlined in Sec. 11.1.
Intermsofspacecanonicalization,weemploythreetypes:rootcanonicaliza-
tion, SMPL canonicalization, and DDC canonicalization. Root canonicalization
is implemented by transforming world-space points to canonical space with the
transformation of root joint of human from motions. With SMPL template and
SMPL motion parameters, we perform a more fine-grained canonicalization by
determining the transformation of each world-space point to the nearest points.
When using DDC as the template, the canonicalization procedure is similar to
SMPL template, but the transformation computation is adjusted for DDC.
Number of Camera Views and Occlusion Handling. In this ablation
study, we investigate the impact of different camera views and occlusion han-
dling.Specifically,weutilizeDDCasthetemplateforspacecanonicalizationand
meta weights for weight initialization.
We first evaluate the effect of varying camera numbers in the meta-learning
phase. We utilize 4-view cameras for fine-tuning, while we experiment with dif-
ferent camera numbers in meta prior learning: 1, 2, 4, 8, and dense.MetaCap 27
S3
GT Ours DeepMultiCap DiffuStereo DVA TransHuman ARAH
Fig.12: Qualitative Comparison.Weadditionallycompareourmethodwithother
approachesonS3ofDynaCapdataset.Ourmethoddemonstratessuperiorperformance
in geometry capturing and rendering quality.
Table5:QuantitativeComparison.FortheS3fromDynaCapdataset,ourmethod
stillachievesstate-of-the-artresultsfornovel-viewsynthesisandgeometryreconstruc-
tion. *Note, that ARAH requires 4D scans for meta learning and videos for the fine-
tuning whereas other methods solely require static images.
Appearance Geometry
Method Subject
PSNR‚ÜëSSIM‚ÜëLPIPS‚ÜìNC-Cos‚ÜìNC-L2‚ÜìChamfer‚ÜìP2S‚ÜìIOU‚Üë
DeepMultiCap [95] S3 - - - 0.131 0.425 1.137 1.158 0.717
DiffStereo [65] S3 - - - 0.143 0.441 1.169 1.269 0.818
DVA [58] S3 24.862 0.824 0.284 0.109 0.378 1.593 2.119 0.465
TransHuman [50] S3 25.136 0.826 0.277 0.118 0.393 1.477 2.006 0.797
ARAH* [78] S3 25.093 0.842 0.278 0.069 0.294 0.780 0.836 0.866
Ours S3 25.528 0.839 0.251 0.106 0.382 0.671 0.792 0.908
Next, we evaluate the influence of camera numbers in the fine-tuning phase.
Here,weutilizedense-viewcamerasforprior-learningbutdifferentcameranum-
bers in the fine-tuning: 1, 2, 4, 8. Additionally, we examine the influence of oc-
clusionhandling(OH)byemployingthisstrategyduringmonocularfine-tuning.
Convergence Speed and Quality Weaimtoinvestigatethetheperformance
of convergence when using different weight initializations. The camera setup
remains consistent with the comparison section and we utilize DDC as the tem-
plate for space canonicalization. For dynamic evaluation during fine-tuning, we
select a single frame from testing data. The corresponding qualitative results of
the curves are presented in the supplementary video.
11.4 Comparison on In-the-wild Sequences
During the comparison on the in-the-wild sequences, we have four views to pro-
vide inputs and an additional view to offer ground truth images. We follow the
Sec. 11.1 and 11.2 to implement our method and ARAH.
12 Additional Comparisons on S3
Fig.12andTab.5presentadditionalqualitativeandquantitativeresultsonthe
‚ÄòS3‚Äô from DynaCap Dataset [22]. The implementations of the methods are con-28 Sun et al.
Monocular Input Image GT Ours (w OH) PiFUHD ICON ECON
Fig.13: Qualitative Comparison. In this comparison, we compare our method,
whichinvolvesmonocularfine-tuningwithocclusionhandling,againstothermonocular
reconstruction approaches, namely PiFUHD [61], ICON [84], and ECON [83]. Our
methodexhibitsrobustnesstothehumanposeandcamerapose,andproducessuperior
geometry and appearance capture.
Table 6: Quantitative Ablation. Here, we study the influence of motion tracking
quality on our method. Comparing to dense mocap, our method with sparse mocap
exhibits a slight decrease in performance.
Appearance Geometry
Method Motion Subject
PSNR‚ÜëSSIM‚ÜëLPIPS‚ÜìNC-Cos‚ÜìNC-L2‚ÜìChamfer‚ÜìP2S‚ÜìIOU‚Üë
ARAH* Dense S2 26.279 0.833 0.302 0.079 0.315 0.839 0.913 0.859
Ours Sparse S2 26.240 0.836 0.253 0.102 0.362 0.712 0.840 0.883
Ours Dense S2 26.529 0.841 0.249 0.096 0.351 0.679 0.814 0.887
sistentwiththoseinthe‚ÄòResults‚Äôsection.Ourapproachcontinuestooutperform
other methods in both rendering and reconstruction.
13 Additional Comparisons on Monocular Methods
Fig.13showsadditionalqualitativecomparisonsbetweenourmethodandmonoc-
ular reconstruction methods. Here, we initialize our network with the meta
prior and fine-tune it using monocular input images, with occlusion handling
applied. Our approach employs perspective camera projection, enabling human
reconstruction in real-world scale and coordinates. In contrast, PiFUHD [61],
ICON [84], and ECON [83] utilize orthogonal camera projection. PiFUHD [61]
exhibits sensitivity to both human and camera poses. ICON [84] demonstrates
limited generalization ability. ECON [83] predicts normal maps for the front
and back sides, and integrates them onto SMPL template. The predicted nor-
mal maps may lack accuracy or fail easily. Our method yields reasonable results
byfine-tuningthecanonicalspacehumanfields.Inthe‚ÄòComparison‚Äôsection,our
method outperforms the multi-view method DeepMultiCap [95], which presents
superior results to multi-view PiFUHD.
14 Ablation on Motion Capture Quality
To evaluate the influence of motion capture quality to our method, we replace
motions from dense mocap with sparse mocap and generate rendering and re-MetaCap 29
Fine-tuned result 1 Interpolated results Fine-tuned result 2
Fig.14: Appearance and geometry interpolation on two fine-tuned results. The red
andgreenboxesrepresenttheappearanceandgeometryofdifferentframes‚Äôfine-tuned
results displayed in both world space and canonical space.
Fig.15: Animatingourfour-viewfine-tunedresultsovertime.Ourhybridrepresenta-
tion can be easily animated with motion and corresponding template.
construction results. The sparse motions come from the same four-view camera
setup used for fine-tuning, while the dense motions are estimated from 34 cam-
eras in the dome. Tab. 6 demonstrates that, though the perfomance drops a bit,
our method with sparse mocap still produces comparable rendering quality and
better geometry compared to ARAH [78] with dense mocap.
15 Applications
15.1 Interpolation in Weight Space
Thanks to the space canonicalization and meta initialization, we are able to
linearly interpolate results from different frames in the weight (hyper) space
and produce meaningful novel interpolated appearance and geometry results, as
shown in Fig. 14. This experiment further validates our hypothesis that space
canonicalization narrows the range of spatial features and facilitates meta prior
learning.
15.2 Animating the Fine-tuned Results
Afterfine-tuningourmetapriorwithfour-viewimages,weobtainacanonicalized
hybridhumanavatar.Thisavatarcanbeeasilyanimatedwithnovelmotionsand
corresponding deformable template, like Fig. 15. The animated results maintain
photorealistic appearance and high-quality geometry.