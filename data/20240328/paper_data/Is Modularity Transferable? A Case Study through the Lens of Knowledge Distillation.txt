Is Modularity Transferable?
A Case Study through the Lens of Knowledge Distillation
Mateusz Klimaszewski1, Piotr Andruszkiewicz1, Alexandra Birch2
InstituteofComputerScience,WarsawUniversityofTechnology1
SchoolofInformatics,UniversityofEdinburgh2
mateusz.klimaszewski.dokt@pw.edu.pl
Abstract
TheriseofModularDeepLearningshowcasesitspotentialinvariousNaturalLanguageProcessingapplications.
Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain
adaptationtomultilingualsetups. However,allthisworkcoversthecasewherethemodularcomponentsaretrained
and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial
limitationontheverymodularitythatmodulararchitecturesaretryingtoachieve. Weaskwhethercurrentmodular
approachesaretransferablebetweenmodelsandwhetherwecantransferthemodulesfrommorerobustandlarger
PLMstosmallerones. Inthiswork,weaimtofillthisgapviaalensofKnowledgeDistillation,commonlyusedfor
modelcompression,andpresentanextremelystraightforwardapproachtotransferringpre-trained,task-specific
PEFTmodulesbetweensame-familyPLMs. Moreover,weproposeamethodthatallowsthetransferofmodules
betweenincompatiblePLMswithoutanychangeintheinferencecomplexity. TheexperimentsonNamedEntity
Recognition,NaturalLanguageInference,andParaphraseIdentificationtasksovermultiplelanguagesandPEFT
methodsshowcasetheinitialpotentialoftransferablemodularity.
Keywords:ModularDeepLearning,Parameter-EfficientFine-tuning,Pre-trainedLanguageModels
1. Introduction
Task-specfic Adapter
ModularDeepLearninghasrecentlygarneredin-
Feed forward
terestasaparadigmthatbuildsupontheideathat Module
Self-Attention transfer
amodelisacombinationofmoduleswithcontrol L=12
of the information flow. This paradigm allows for
thetransferoflearningfromonetaskorlanguage Task-specfic Adapter Module Task-specfic Adapter
Transfer
to another, compositionality of the modules and
parameterefficiency(Pfeifferetal.,2023). Forin- Feed forward Feed forward
stance,modulesallowforefficient(parameter-wise) Self-Attention Self-Attention
L=11 L=6
fine-tuning of Large Language Models (Hu et al.,
2022), enhance task-level generalisation (Ponti
etal.,2023),improvemultilingualmodels(Bapna
Module
andFirat,2019),offerzero-shotcapabilities(Philip Task-specfic Adapter Task-specfic Adapter
Transfer
etal.,2020)andenablecross-lingual(Anselletal.,
2022)orcross-domain(Klimaszewskietal.,2023) Feed forward Feed forward
knowledgetransfer. Furthermore,repositoriesthat Self-Attention Self-Attention
L=0 L=0
storepre-trainedmoduleslikeAdapterHub(Pfeiffer
Parameter-efficient
Student PLM
etal.,2020a)promotethere-usabilityofpreviously fine-tuned
modules initialisation
Teacher PLM
trainedcomponentstonewusecases.
Thecurrentmodularapproachesprimarilyfocus Figure1: Themoststraightforwardcaseoftransfer-
ontransferringknowledgetonewlanguages,do- ablemodularity. Theteachermodelisfirsttrained
mains,ortasks. However,priorresearchassumes onataskusingPEFT,e.g. Adapters,andthenthe
that the base model remains constant and over- studentPEFTmodules,priortofine-tuning,areini-
lookstheconceptoftransferablemodularity,which tialisedwiththeteacherweights.
entailsthepotentialtotransfermodulesbetween
differentmodels. Fromapracticalperspective,the
effective utilisation of the transferable modularity
property canreducethecomputationalburden,es- transferringmodulesfromlargertosmallermodels
pecially given the ongoing scaling of Large Lan- cansignificantlyenhanceknowledgetransfer. And
guageModels(Brownetal.,2020;Touvronetal., finally,eventheterm“modularity”inherentlyimplies
2023),allowingforbroaderre-usability. Moreover, thetransferproperty,suggestingthatmodularap-
4202
raM
72
]LC.sc[
1v40881.3042:viXraTeacher 2. PEFT Fine-tuned 1. Sampling 2. Correlation matrix
Teacher Student Teacher
1
1. Task-agnostic 3. Modularity 2
distillation Transfer
3
1 2 3 1 2 3 4 1 2 3 4
Fine-tuned
Student 4. PEFT 3. LSA 4. Pruning & alignment
Student
Binary matrix Change down/up projection
weights based on
Figure2: Theschemaoftransferablemodularityex- 1
periment. Weinvestigatesetupswheretheteacher-
2 Prune & align
studentpairresultfromtask-agnosticdistillationor
areindependentlytrainedmodels.
3
1 2 3 4
proachesshouldnotbelimitedtoaspecificbase
Figure3: ToyexampleofadaptingthePEFTmod-
model.
ules in the case of mismatched dimensionality.
In this work, we aim to initialise the research
Basedonthesampledembeddings(1.),correlation
objectiveoftransferablemodularity. Wefocusona
matrix C is calculated (2.) and reduced via LSA
setupsimilartoKnowledgeDistillation(KD)(Hinton
toabinarymatrixZ (3.). Inthelaststep(4.),the
et al., 2015), i.e. where we have two differently
pruningandalignmentmappingfunction(derived
sizedPLMs(throughthepaper,weadopttheKD
fromZ)isappliedtodown/upprojectionmatrices
nomenclature, where the bigger model is called
ofLoRA/Adaptermodulesandmatchdimensions.
a teacher and the smaller - student). Unlike KD,
wedonotwanttousetheteachermodel’soutput
directly to train a student but use exclusively its
weuseaPEFTtechniquetotraintheteacherand
fine-tunedPEFTmodules.
its PE modules. Then, we “move” the modules
WeshowthatgivenmatchingPLMs(e.g. BERT
fromtheteacherandinsertthemintothestudent,
(Devlin et al., 2019) and DistilBERT (Sanh et al.,
followed by PEFT of the student. This approach
2019)),itispossibletousepre-trainedmoduleslike
meansthatPEmodulesofthestudenthavenon-
Adapters(Houlsbyetal.,2019;Pfeifferetal.,2021)
randompriorinitialisationduringtraining.
orLoRA(Huetal.,2022)asabetterstartingpoint
Weconsidertwosetups: (1)matchingPLMsand
forparameter-efficient(PE)fine-tuningofasmaller
(2)incompatiblePLMs. Theformerusesashallow
studentPLM(seeFigure1). Moreover,weinvesti-
versionofateacherwithtask-agnosticdistillation
gateamorechallengingsetupwherethemodels
asastudent(KimandHassan,2020). Thiscase
areincompatible,i.e.,havedifferentinternaldimen-
meansthatthemodelsrepresentthesameknowl-
sionality,andadaptmodulesviatheproposedprun-
edge, have the same hidden dimensionality, and
ingandalignmentmethod(withoutinference-time
theonlydifferenceisthedepthofthemodel. The
overhead).
latterrepresentsageneralisedversion,wherethe
Tosummarise,ourcontributionsareasfollows1:
modelsaredifferentlyparameterised(intermsofla-
• Wedefinethepropertyoftransferablemodu- tentspacesize)andtheyareindependentlytrained.
larity. Weproposeaparameter-free,sample-basedprun-
ingandalignmentmethodtoanswerdimensionality
• We investigate transferable modularity in mismatch.
matchingandincompatiblePLMs,proposing
apruningandalignmentmethodforthelatter.
2.1. Pruning and Alignment
2. Transferable Modularity InthecaseofincompatiblePLMs,adimensional-
itymismatchproblemcausestwomainissuesfor
The high-level idea of our study is presented in transferablemodularity. First,themoduleexpects
Figure 2. Given a pair of PLMs, a teacher and a different(higher)dimensionality. Additionally,there
student,weaimtotransfertheparameter-efficient existsanalignmentdiscrepancybetweenthelatent
(PE)modulesfromtheteachertothestudent. First, spacesofthetwomodels,i.e. ifthemodelshave
learned the same features, we do not have any
1Code available at https://github.com/ guaranteeoftheirplacementinthelatentspace-
mklimasz/transferable-modularity theirindices.AcrucialelementofasuccessfulKnowledgeDis- Model Params Layers Hiddendim
tillationframeworkisthecomputationaloverhead;
therefore,weproposeanoffline,parameter-freeso- D’mBERT 135M 6 768
lutionthatdoesnotchangethefinalstudentmodel. mBERT 178M 12 768
ThemethodpresentedinFigure3consistsoffour XLM-R BASE 278M 12 768
phrases: XLM-R LARGE 560M 24 1024
• sampling Table 1: Parameters, layer count and hidden di-
mensionsizeoftheevaluatedmodels.
• calculatingcorrelation
3.2. Training Setup
• solvinglinearsumassignment(LSA)problem
We fine-tune multilingual models for each lan-
• pruning&alignment guage/taskpairusingtwoPEFTmethods: Adapter
(architectureofPfeifferetal.(2021),bottlenecksize
At first, we sample matching embeddings that of96)andLoRA(rank8). Weprovidethetraining
wouldbeaninputtoaPEFTmodule(wedenotethe setupdetailsforeachdatasetinAppendixA.
setofembeddingsX forstudentandX forteacher Forteacher-studentpairs,wedefinetwoconfig-
s t
withx ∈X andx ∈X ). Westoreembeddings urations:
s s t t
per layer l (for clarity, we omit the notation of the • matching: multilingual BERT (mBERT3,
layer).
teacher)–multilingualDistilBERT(D’mBERT4,
Inthenextstep,weestablishacorrelationmatrix
student)
between latent spaces. We calculate Pearson’s
correlationcoefficientmatrixC. C isacorrelation • incompatible: XLM-RoBERTa Large (XLM-
ij
betweentheidimensionofax andthej dimen- R LARGE5, teacher) – XLM-RoBERTa Base
s
sionofax embedding. (XLM-R BASE6,student)(Conneauetal.,2020)
t
Giventhecorrelationmatrix,weattempttofind We report the relevant hyper-parameters of the
thebestpossiblealignment. Wedefinetheproblem modelsinTable1. Asthemodelshavemismatched
asalinearsumassignment(LSA)(Crouse,2016)to layer counts, we test two approaches: skip mod-
establishtheoptimalmapping. AsLSAcalculates ules (denoted SKIP, e.g., transfer every second
theminimumcostassignment,weuse−C asan module)oraveragethem(denotedAVG,e.g.,av-
inputtotheLSAalgorithm. Thealgorithmproduces eragethefirstandsecondlayer’steachermodule
abinarymatrixZ whereZ =1meansthatthei andtransfertothefirstmoduleofastudent).
ij
indexofX ismappedtoj ofX .
s t
3.3. Baselines and Metrics
(cid:88)(cid:88)
min (−C )Z
ij ij Forbothmatchingandincompatibleexperiments,
i j
we define the following structure. As an upper
Finally,usingthecalculatedassignmentindices, bound of our evaluation, we provide the teacher
weremovenot-mappedweightsfrombothdown/up resultsafterPEFT(Step2inFigure2). Thebase-
projectionweightsW ofPEFTmodules. lineisaparameter-efficientfine-tunedstudentwith
defaultmodulesinitialisation(i.e. omittingStep3
inFigure2).
3. Experiments WereportF1forNERandAccuracyforPIand
NLItaskswithanaveragescoreoveralllanguages
3.1. Datasets inSection4. Thedetailedper-languageresultsare
providedinAppendixB.
Toevaluateourmethod,webenchmarkitonthree
tasks: Named Entity Recognition (NER), Para-
phraseIdentification(PI)andNaturalLanguageIn- 4. Results and Discussion
ference(NLI)usingmultilingualdatasets: WikiNeu-
ral (Tedeschi et al., 2021), PAWS-X (Yang et al., 4.1. Matching Models
2019) and XNLI (Conneau et al., 2018) covering
Table2presentstheresultsofthematching exper-
jointlyasetofover20languages2.
iments. TheprefixTMdenotesthetransfermodu-
2Arabic,Bulgarian,Chinese,Dutch,English,French, 3bert-base-multilingual-cased
German,Hindi,Italian,Japanese,Korean,Greek,Polish, 4distilbert-base-multilingual-cased
Portuguese, Russian, Spanish, Swahili, Thai, Turkish, 5xlm-roberta-large
Urdu,Vietnamese 6xlm-roberta-baseNER(F1) PI(Acc) NLI(Acc)
AVG REL AVG REL AVG REL
Adapter
Teacher 95,35 82,60 67,98
Student 92,94 −2,41 71,32 −11,28 62,12 −5,86
TM-Student AVG 93,02 −2,32 72,96 −9,64 62,33 −5,65
TM-Student SKIP 93,45 −1,90 75,11 −7,49 63,01 −4,97
LoRA
Teacher 93,27 74,68 63,00
Student 90,09 −3,18 65,80 −8,88 60,56 −2,43
TM-Student AVG 90,63 −2,64 68,52 −6,16 60,53 −2,47
TM-Student SKIP 90,80 −2,47 70,69 −3,99 60,52 −2,47
Table2: ResultsofthematchingPLMsexperiment. Wereportanaveragescore(F1orAccuracy)overall
thedatasets’languagesandarelativeperformancegaptotheteachermodel.
NER(F1) PI(Acc)
AVG REL AVG REL
Adapter
Teacher 95,34 88,81
Student 93,30 −2,04 84,12 −4,69
TM-Student SKIP 93,34 −2,00 84,27 −4,54
LoRA
Teacher 93,64 87,03
Student 90,83 −2,82 78,72 −8,31
TM-Student SKIP 90,84 −2,80 78,64 −8,39
Table3: ResultsoftheincompatiblePLMsexperiment.
larityexperiments. Theinitialisationofthemodules 4.2. Incompatible Models
transferred from the teacher PLM improved over
a default initialisation on average in all the evalu-
atedtasks. Moreover,theSKIPmethodpresents WepresenttheresultsoftheevaluationinTable3.
consistency;thedifferencecomparedtothebase- Inthecaseofnon-distilledPLMs,theTMmethod
linewaspositiveacrossmosttasksandlanguages doesnotsignificantlyoutperformthebaseline. The
(88,7% cases). While at times the improvement changesareuneven;whilethetransfershowsim-
was marginal (+0.02 gain in Swahili in NLI task), provementuptoalmost+2pointsinKoreanPAWS-
inmostcases,asaveragesindicate,ourapproach X, it can also decrease the performance as in
significantly closes the gap to the teacher model FrenchPAWS-X,losing−1.05.
(e.g. +4 point improvement in Korean on PAWS-
X datasets using Adapter or over +2 in Spanish
Thedisparitybetweenmatchingandincompat-
LoRAonXNLI).SKIPstrugglestooutperformthe ible experiments can be attributed to alignment
challenges. Modelssubjectedtodistillationexhibit
baseline exclusively on XNLI when using LoRA.
reliablealignment,thankstotheinclusionofanaux-
Theresultsareonpar;however,eventheteacher
iliarylosstermsuchasthecosineembeddingloss
modelsstrugglewiththetask,andtheknowledge
(Sanhetal.,2019)inthetask-agnosticdistillation
thatcanbetransferredisrelativelylimited.
process. Incontrast,thecorrelation-basedmethod
The SKIP outperforms AVG across all the ex- encounters difficulties when dealing with models
periments. Considering the results and the find- ofgreaterdepth. Notably,theLSAalgorithmyields
ings of van Aken et al. (2019) indicating that the lowerscoresfordeeperlayers. Consideringthedif-
Transformer-basedmodelshaveinternalmodular- ferentrepresentationsrequiredforeachlanguage
ity and each layer has its own defined task, we and task pair, this outcome implies that indepen-
hypothesise that the averaging might not reflect dently trained models require more robust align-
thesephenomena. Therefore,intheincompatible ment techniques to ensure consistent modularity
experiment,weevaluatedjusttheSKIPmethod. transferacrossallencodedfeatures.5. Conclusions TomBrown,BenjaminMann,NickRyder,Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Inthiswork,wepresentacasestudyoftransferable ArvindNeelakantan,PranavShyam,GirishSas-
modularityproperty. Weevaluatecurrentmodular try, Amanda Askell, Sandhini Agarwal, Ariel
techniquesintwoscenarios: (1)matching,where Herbert-Voss,GretchenKrueger,TomHenighan,
astudentisashallow,task-agnosticdistillationof RewonChild,AdityaRamesh,DanielZiegler,Jef-
theteacherand(2)incompatible,whereastudent frey Wu, Clemens Winter, Chris Hesse, Mark
is independently trained, a shallower model with Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
mismatchedinternaldimensionality. BenjaminChess,JackClark,ChristopherBerner,
The results show that the current modular ap- SamMcCandlish,AlecRadford,IlyaSutskever,
proachcanbetransferableasthemodulesfroma andDarioAmodei.2020. Languagemodelsare
matchingteacherimprovethePEFTofastudent few-shotlearners. InAdvancesinNeuralInfor-
model. However, when a student is not distilled mationProcessingSystems,volume33,pages
fromtheteacher,theevaluatedtechniquesarein- 1877–1901.CurranAssociates,Inc.
consistent under the transfer condition, showing
thelimitationofthecurrentmodularmethods. We Alexis Conneau, Kartikay Khandelwal, Naman
hopethisstudywillinspirefutureworkonmodular Goyal,VishravChaudhary,GuillaumeWenzek,
techniquestoconsiderthetransferablemodularity Francisco Guzmán, Edouard Grave, Myle Ott,
property under a more challenging incompatible LukeZettlemoyer,andVeselinStoyanov.2020.
modelsscenario. Unsupervisedcross-lingualrepresentationlearn-
ingatscale. InProceedingsofthe58thAnnual
MeetingoftheAssociationforComputationalLin-
6. Acknowledgements guistics,pages8440–8451,Online.Association
forComputationalLinguistics.
Part of this work was funded from the European
Union’sHorizonEuropeResearchandInnovation Alexis Conneau, Ruty Rinott, Guillaume Lam-
program under Grant Agreement No 101070631 ple, Adina Williams, Samuel Bowman, Holger
andfromtheUKResearchandInnovation(UKRI) Schwenk, and Veselin Stoyanov. 2018. XNLI:
undertheUKgovernment’sHorizonEuropefunding Evaluating cross-lingual sentence representa-
guarantee(GrantNo10039436). tions. In Proceedings of the 2018 Conference
Thecomputationsinthisworkwereperformed
onEmpiricalMethodsinNaturalLanguagePro-
onPoznańSupercomputingandNetworkingCen- cessing,pages2475–2485,Brussels,Belgium.
ter and Baskerville. The Baskerville Tier 2 HPC AssociationforComputationalLinguistics.
was funded by the EPSRC and UKRI through
the World Class Labs scheme (EP/T022221/1) DavidF.Crouse.2016. Onimplementing2drect-
andtheDigitalResearchInfrastructureprogramme angular assignment algorithms. IEEE Trans-
(EP/W032244/1)andisoperatedbyAdvancedRe- actions on Aerospace and Electronic Systems,
searchComputingattheUniversityofBirmingham. 52(4):1679–1696.
JacobDevlin,Ming-WeiChang,KentonLee,and
7. Bibliographical References Kristina Toutanova. 2019. BERT: Pre-training
ofdeepbidirectionaltransformersforlanguage
understanding. InProceedingsofthe2019Con-
ferenceoftheNorthAmericanChapteroftheAs-
AlanAnsell,EdoardoPonti,AnnaKorhonen,and
sociationforComputationalLinguistics: Human
IvanVulić.2022. Composablesparsefine-tuning
Language Technologies, Volume 1 (Long and
forcross-lingualtransfer. InProceedingsofthe ShortPapers),pages4171–4186,Minneapolis,
60thAnnualMeetingoftheAssociationforCom- Minnesota. Association for Computational Lin-
putationalLinguistics(Volume1: LongPapers), guistics.
pages1778–1796,Dublin,Ireland.Association
forComputationalLinguistics. GeoffreyE.Hinton,OriolVinyals,andJeffreyDean.
2015. Distilling the knowledge in a neural net-
AnkurBapnaandOrhanFirat.2019. Simple,scal- work. ArXiv,abs/1503.02531.
ableadaptationforneuralmachinetranslation.In
Proceedingsofthe2019ConferenceonEmpiri- NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,
calMethodsinNaturalLanguageProcessingand BrunaMorrone,QuentinDeLaroussilhe,Andrea
the9thInternationalJointConferenceonNatural Gesmundo, MonaAttariyan, andSylvainGelly.
LanguageProcessing(EMNLP-IJCNLP),pages 2019. Parameter-efficient transfer learning for
1538–1548,HongKong,China.Associationfor NLP. In Proceedings of the 36th International
ComputationalLinguistics. Conference on Machine Learning, volume 97ofProceedingsofMachineLearningResearch, for zero-shot neural machine translation. In
pages2790–2799.PMLR. Proceedingsofthe2020ConferenceonEmpir-
ical Methods in Natural Language Processing
EdwardJHu,YelongShen,PhillipWallis,Zeyuan
(EMNLP), pages 4465–4470, Online. Associa-
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
tionforComputationalLinguistics.
andWeizhuChen.2022. LoRA:Low-rankadap-
tationoflargelanguagemodels. InInternational EdoardoMariaPonti,AlessandroSordoni,Yoshua
ConferenceonLearningRepresentations. Bengio, and Siva Reddy. 2023. Combining
parameter-efficient modules for task-level gen-
YoungJinKimandHanyHassan.2020. FastForm-
eralisation. In Proceedings of the 17th Confer-
ers: Highlyefficienttransformermodelsfornatu-
ence of the European Chapter of the Associa-
rallanguageunderstanding. InProceedingsof
tion for Computational Linguistics, pages 687–
SustaiNLP: Workshop on Simple and Efficient 702,Dubrovnik,Croatia.AssociationforCompu-
NaturalLanguageProcessing,pages149–158,
tationalLinguistics.
Online. Association for Computational Linguis-
tics. Victor Sanh, Lysandre Debut, Julien Chaumond,
and Thomas Wolf. 2019. Distilbert, a distilled
Mateusz Klimaszewski, Zeno Belligoli, Satendra
version of BERT: smaller, faster, cheaper and
Kumar,andEmmanouilStergiadis.2023. Gated
lighter. CoRR,abs/1910.01108.
adaptersformulti-domainneuralmachinetrans-
lation.InECAI2023-26thEuropeanConference SimoneTedeschi,ValentinoMaiorca,NiccolòCam-
onArtificialIntelligence,volume372ofFrontiers polungo,FrancescoCecconi,andRobertoNav-
inArtificialIntelligenceandApplications,pages igli. 2021. WikiNEuRal: Combined neural and
1264–1271.IOSPress. knowledge-basedsilverdatacreationformulti-
lingualNER. InFindingsoftheAssociationfor
JonasPfeiffer,AishwaryaKamath,AndreasRücklé,
ComputationalLinguistics: EMNLP2021,pages
Kyunghyun Cho, and Iryna Gurevych. 2021.
2521–2533, Punta Cana, Dominican Republic.
AdapterFusion: Non-destructive task composi-
AssociationforComputationalLinguistics.
tionfortransferlearning. InProceedingsofthe
16thConferenceoftheEuropeanChapterofthe HugoTouvron, LouisMartin, KevinR.Stone, Pe-
AssociationforComputationalLinguistics: Main ter Albert, Amjad Almahairi, Yasmine Babaei,
Volume,pages487–503,Online.Associationfor NikolayBashlykov,SoumyaBatra,PrajjwalBhar-
ComputationalLinguistics. gava, Shruti Bhosale, Daniel M. Bikel, Lukas
Blecher, Cristian Cantón Ferrer, Moya Chen,
JonasPfeiffer,AndreasRücklé,CliftonPoth,Aish-
Guillem Cucurull, David Esiobu, Jude Fernan-
warya Kamath, Ivan Vulić, Sebastian Ruder,
des, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-
Kyunghyun Cho, and Iryna Gurevych. 2020a.
thiaGao,VedanujGoswami,NamanGoyal,An-
Adapterhub: Aframeworkforadaptingtransform-
thonyS.Hartshorn,SagharHosseini,RuiHou,
ers. InProceedingsofthe2020Conferenceon
HakanInan,MarcinKardas,ViktorKerkez,Ma-
EmpiricalMethodsinNaturalLanguageProcess- dian Khabsa, Isabel M. Kloumann, A. V. Ko-
ing (EMNLP 2020): Systems Demonstrations, renev,PunitSinghKoura,Marie-AnneLachaux,
pages46–54,Online.AssociationforComputa-
ThibautLavril,JenyaLee,DianaLiskovich,Ying-
tionalLinguistics.
haiLu,YuningMao,XavierMartinet, TodorMi-
haylov, Pushkar Mishra, Igor Molybog, Yixin
JonasPfeiffer,AndreasRücklé,CliftonPoth,Aish-
Nie,AndrewPoulton,JeremyReizenstein,Rashi
warya Kamath, Ivan Vulić, Sebastian Ruder,
Rungta, Kalyan Saladi, Alan Schelten, Ruan
Kyunghyun Cho, and Iryna Gurevych. 2020b.
Silva,EricMichaelSmith,R.Subramanian,Xia
AdapterHub: A framework for adapting trans-
Tan, Binh Tang, Ross Taylor, Adina Williams,
formers. InProceedingsofthe2020Conference
JianXiangKuan,PuxinXu,ZhengxuYan,Iliyan
onEmpiricalMethodsinNaturalLanguagePro-
Zarov, Yuchen Zhang, Angela Fan, Melanie
cessing: SystemDemonstrations,pages46–54,
Kambadur,SharanNarang,AurelienRodriguez,
Online. Association for Computational Linguis-
Robert Stojnic, Sergey Edunov, and Thomas
tics.
Scialom.2023. Llama2: Openfoundationand
Jonas Pfeiffer, Sebastian Ruder, Ivan Vulić, and fine-tunedchatmodels. ArXiv,abs/2307.09288.
Edoardo Ponti. 2023. Modular deep learning.
BettyvanAken,BenjaminWinter,AlexanderLöser,
Transactions on Machine Learning Research.
andFelixA.Gers.2019. Howdoesbertanswer
SurveyCertification.
questions? alayer-wiseanalysisoftransformer
JerinPhilip,AlexandreBerard,MatthiasGallé,and representations. In Proceedings of the 28th
LaurentBesacier.2020. Monolingualadapters ACM International Conference on InformationandKnowledgeManagement, CIKM’19, page
1823–1832,NewYork,NY,USA.Associationfor
ComputingMachinery.
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason
Baldridge. 2019. PAWS-X: A cross-lingual ad-
versarialdatasetforparaphraseidentification. In
Proceedingsofthe2019ConferenceonEmpiri-
calMethodsinNaturalLanguageProcessingand
the9thInternationalJointConferenceonNatural
LanguageProcessing(EMNLP-IJCNLP),pages
3687–3692,HongKong,China.Associationfor
ComputationalLinguistics.
A. Experimental Setup
We use the AdapterHub library (Pfeiffer et al.,
2020b) for all our experiments. We train all our
models using a single GPU with a batch size of
64 and a learning rate of 1e–5 for 10 epochs for
NER & NLI tasks and 30 epochs for the PI task.
Wechoosethefinalcheckpointbasedonvalidation
datasetperformance.
For PEFT hyper-parameters, we set the bottle-
necksizeto96forAdaptermodulesandarankof
8forLoRA.WeapplyLoRAtothequeryandvalue
self-attentionmodules.
B. Per Language Results
InTables4,5and6,weexpandtheresultsreported
inTables2and3andprovidethescoresforeach
evaluatedlanguage.Model de en es fr it nl pl pt ru
MatchingPLMs
Adapter
Teacher 97,57 92,79 98,08 95,49 94,85 97,74 95,54 95,91 90,15
Student 95,34 90,23 95,81 93,23 92,67 95,27 93,73 94,21 85,97
TM-Student AVG 95,53 90,21 95,87 93,26 92,74 95,39 93,88 94,40 85,92
TM-Student SKIP 95,88 90,65 96,24 93,77 93,13 95,87 94,20 94,72 86,57
Lora
Teacher 95,78 90,49 96,45 93,26 93,13 95,94 93,97 94,41 85,97
Student 92,45 87,25 93,55 90,06 89,95 92,85 91,55 92,15 80,96
TM-Student AVG 92,92 87,74 93,94 90,57 90,75 93,24 91,97 92,52 82,03
TM-Student SKIP 93,03 87,99 93,87 90,88 91,04 93,44 92,04 92,61 82,27
IncompatiblePLMs
Adapter
Teacher 97,36 92,30 97,95 95,61 94,99 97,79 96,15 96,12 89,74
Student 95,20 89,92 96,19 93,34 93,06 96,29 94,14 94,56 86,99
TM-Student SKIP 95,30 90,03 96,21 93,40 93,00 96,09 94,19 94,78 87,01
Lora
Teacher 94,68 89,94 96,19 92,35 92,85 95,67 93,82 94,11 85,09
Student 92,21 86,65 92,74 89,40 90,05 93,14 91,61 92,25 81,62
TM-Student SKIP 92,10 86,65 93,00 89,61 90,01 93,01 91,51 92,32 81,69
Table4: NamedEntityRecognitionresultsperlanguage.
Model de en es fr ja ko zh
MatchingPLMs
Adapter
Teacher 83,60 91,60 85,20 86,90 76,05 75,95 78,90
Student 73,30 75,85 72,90 74,65 67,25 65,25 70,05
TM-Student AVG 74,15 82,35 73,35 75,10 67,10 67,40 71,25
TM-Student SKIP 74,50 85,05 77,85 78,15 69,10 69,25 71,85
Lora
Teacher 75,10 83,30 78,70 77,75 67,85 67,95 72,10
Student 70,20 63,45 67,30 70,10 62,80 61,85 64,90
TM-Student AVG 71,95 69,35 69,95 71,85 64,75 64,05 67,75
TM-Student SKIP 72,25 74,50 72,30 74,85 66,70 64,90 69,30
IncompatiblePLMs
Adapter
Teacher 90,45 94,70 91,20 92,15 82,35 85,00 85,80
Student 85,75 92,55 87,25 89,25 77,10 75,65 81,30
TM-Student SKIP 86,15 92,05 88,50 88,20 76,80 77,45 80,75
Lora
Teacher 89,40 93,80 89,90 89,65 80,95 81,45 84,05
Student 80,00 88,05 82,95 83,55 72,55 68,60 75,35
TM-Student SKIP 80,95 88,05 82,10 82,70 71,65 69,80 75,20
Table5: ParaphraseIdentificationresultsperlanguage.hz
iv
ru
rt
ht
ws
ur
ih
rf
se
ne
le
ed
gb
ra
ledoM
retpadA
01,17
82,17
04,06
94,76
04,06
21,06
85,96
93,36
00,27
10,37
30,77
80,86
21,07
81,07
35,56
rehcaeT
10,46
76,36
17,55
68,16
34,65
99,65
41,26
33,75
98,56
11,66
26,96
53,36
70,56
74,36
21,06
tnedutS
95,46
33,36
95,65
00,26
93,65
71,65
35,26
45,75
31,66
96,66
83,07
72,36
91,56
75,36
45,06
GVAtnedutS-MT
37,56
98,36
74,75
16,26
50,75
10,75
54,36
84,85
35,66
61,86
85,07
94,36
72,56
32,46
61,16
PIKStnedutS-MT
aroL
77,66
10,56
74,65
06,16
73,55
75,45
74,46
09,95
99,66
48,76
21,07
77,36
38,66
97,36
64,16
rehcaeT
80,26
47,16
79,55
48,95
76,35
78,45
26,06
65,75
78,36
90,46
51,76
69,16
34,36
04,26
02,95
tnedutS
88,16
47,16
77,55
27,95
76,35
78,45
26,06
65,75
78,36
90,46
51,76
69,16
34,36
04,26
02,95
GVAtnedutS-MT
62,26
03,16
11,65
67,95
91,35
73,45
49,06
65,75
91,46
11,46
91,76
66,16
57,36
63,26
01,95
PIKStnedutS-MT
.tnemirepxesMLP
gnihctamehtrofegaugnalrepstluserecnerefnIegaugnaLlarutaN
:6elbaT