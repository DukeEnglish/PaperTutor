[
    {
        "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
        "authors": "Yanwei LiYuechen ZhangChengyao WangZhisheng ZhongYixin ChenRuihang ChuShaoteng LiuJiaya Jia",
        "links": "http://arxiv.org/abs/2403.18814v1",
        "entry_id": "http://arxiv.org/abs/2403.18814v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18814v1",
        "summary": "In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.",
        "updated": "2024-03-27 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18814v1"
    },
    {
        "title": "Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation",
        "authors": "Mateusz KlimaszewskiPiotr AndruszkiewiczAlexandra Birch",
        "links": "http://arxiv.org/abs/2403.18804v1",
        "entry_id": "http://arxiv.org/abs/2403.18804v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18804v1",
        "summary": "The rise of Modular Deep Learning showcases its potential in various Natural\nLanguage Processing applications. Parameter-efficient fine-tuning (PEFT)\nmodularity has been shown to work for various use cases, from domain adaptation\nto multilingual setups. However, all this work covers the case where the\nmodular components are trained and deployed within one single Pre-trained\nLanguage Model (PLM). This model-specific setup is a substantial limitation on\nthe very modularity that modular architectures are trying to achieve. We ask\nwhether current modular approaches are transferable between models and whether\nwe can transfer the modules from more robust and larger PLMs to smaller ones.\nIn this work, we aim to fill this gap via a lens of Knowledge Distillation,\ncommonly used for model compression, and present an extremely straightforward\napproach to transferring pre-trained, task-specific PEFT modules between\nsame-family PLMs. Moreover, we propose a method that allows the transfer of\nmodules between incompatible PLMs without any change in the inference\ncomplexity. The experiments on Named Entity Recognition, Natural Language\nInference, and Paraphrase Identification tasks over multiple languages and PEFT\nmethods showcase the initial potential of transferable modularity.",
        "updated": "2024-03-27 17:50:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18804v1"
    },
    {
        "title": "Projective Methods for Mitigating Gender Bias in Pre-trained Language Models",
        "authors": "Hillary DawkinsIsar NejadgholiDaniel GillisJudi McCuaig",
        "links": "http://arxiv.org/abs/2403.18803v1",
        "entry_id": "http://arxiv.org/abs/2403.18803v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18803v1",
        "summary": "Mitigation of gender bias in NLP has a long history tied to debiasing static\nword embeddings. More recently, attention has shifted to debiasing pre-trained\nlanguage models. We study to what extent the simplest projective debiasing\nmethods, developed for word embeddings, can help when applied to BERT's\ninternal representations. Projective methods are fast to implement, use a small\nnumber of saved parameters, and make no updates to the existing model\nparameters. We evaluate the efficacy of the methods in reducing both intrinsic\nbias, as measured by BERT's next sentence prediction task, and in mitigating\nobserved bias in a downstream setting when fine-tuned. To this end, we also\nprovide a critical analysis of a popular gender-bias assessment test for\nquantifying intrinsic bias, resulting in an enhanced test set and new bias\nmeasures. We find that projective methods can be effective at both intrinsic\nbias and downstream bias mitigation, but that the two outcomes are not\nnecessarily correlated. This finding serves as a warning that intrinsic bias\ntest sets, based either on language modeling tasks or next sentence prediction,\nshould not be the only benchmark in developing a debiased language model.",
        "updated": "2024-03-27 17:49:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18803v1"
    },
    {
        "title": "Long-form factuality in large language models",
        "authors": "Jerry WeiChengrun YangXinying SongYifeng LuNathan HuDustin TranDaiyi PengRuibo LiuDa HuangCosmo DuQuoc V. Le",
        "links": "http://arxiv.org/abs/2403.18802v1",
        "entry_id": "http://arxiv.org/abs/2403.18802v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18802v1",
        "summary": "Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.",
        "updated": "2024-03-27 17:48:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18802v1"
    },
    {
        "title": "Towards a World-English Language Model for On-Device Virtual Assistants",
        "authors": "Rricha JalotaLyan VerwimpMarkus Nussbaum-ThomAmr MousaArturo ArguetaYoussef Oualil",
        "links": "http://dx.doi.org/10.1109/ICASSP48485.2024.10448018",
        "entry_id": "http://arxiv.org/abs/2403.18783v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18783v1",
        "summary": "Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are\ngenerally language-, region-, and in some cases, device-dependent, which\nincreases the effort to scale and maintain them. Combining NNLMs for one or\nmore of the categories is one way to improve scalability. In this work, we\ncombine regional variants of English to build a ``World English'' NNLM for\non-device VAs. In particular, we investigate the application of adapter\nbottlenecks to model dialect-specific characteristics in our existing\nproduction NNLMs {and enhance the multi-dialect baselines}. We find that\nadapter modules are more effective in modeling dialects than specializing\nentire sub-networks. Based on this insight and leveraging the design of our\nproduction models, we introduce a new architecture for World English NNLM that\nmeets the accuracy, latency, and memory constraints of our single-dialect\nmodels.",
        "updated": "2024-03-27 17:31:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18783v1"
    }
]