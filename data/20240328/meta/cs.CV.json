[
    {
        "title": "Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark",
        "authors": "Ziyang ChenIsrael D. GebruChristian RichardtAnurag KumarWilliam LaneyAndrew OwensAlexander Richard",
        "links": "http://arxiv.org/abs/2403.18821v1",
        "entry_id": "http://arxiv.org/abs/2403.18821v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18821v1",
        "summary": "We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/",
        "updated": "2024-03-27 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18821v1"
    },
    {
        "title": "MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering",
        "authors": "Guoxing SunRishabh DabralPascal FuaChristian TheobaltMarc Habermann",
        "links": "http://arxiv.org/abs/2403.18820v1",
        "entry_id": "http://arxiv.org/abs/2403.18820v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18820v1",
        "summary": "Faithful human performance capture and free-view rendering from sparse RGB\nobservations is a long-standing problem in Vision and Graphics. The main\nchallenges are the lack of observations and the inherent ambiguities of the\nsetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,\nwhich have shown great promise in capturing high-frequency appearance and\ngeometry details in dense setups, perform poorly when na\\\"ively supervising\nthem on sparse camera views, as the field simply overfits to the sparse-view\ninputs. To address this, we propose MetaCap, a method for efficient and\nhigh-quality geometry recovery and novel view synthesis given very sparse or\neven a single view of the human. Our key idea is to meta-learn the radiance\nfield weights solely from potentially sparse multi-view videos, which can serve\nas a prior when fine-tuning them on sparse imagery depicting the human. This\nprior provides a good network weight initialization, thereby effectively\naddressing ambiguities in sparse-view capture. Due to the articulated structure\nof the human body and motion-induced surface deformations, learning such a\nprior is non-trivial. Therefore, we propose to meta-learn the field weights in\na pose-canonicalized space, which reduces the spatial feature range and makes\nfeature learning more effective. Consequently, one can fine-tune our field\nparameters to quickly generalize to unseen poses, novel illumination conditions\nas well as novel and sparse (even monocular) camera views. For evaluating our\nmethod under different scenarios, we collect a new dataset, WildDynaCap, which\ncontains subjects captured in, both, a dense camera dome and in-the-wild sparse\ncamera rigs, and demonstrate superior results compared to recent\nstate-of-the-art methods on both public and WildDynaCap dataset.",
        "updated": "2024-03-27 17:59:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18820v1"
    },
    {
        "title": "Benchmarking Object Detectors with COCO: A New Path Forward",
        "authors": "Shweta SinghAayan YadavJitesh JainHumphrey ShiJustin JohnsonKaran Desai",
        "links": "http://arxiv.org/abs/2403.18819v1",
        "entry_id": "http://arxiv.org/abs/2403.18819v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18819v1",
        "summary": "The Common Objects in Context (COCO) dataset has been instrumental in\nbenchmarking object detectors over the past decade. Like every dataset, COCO\ncontains subtle errors and imperfections stemming from its annotation\nprocedure. With the advent of high-performing models, we ask whether these\nerrors of COCO are hindering its utility in reliably benchmarking further\nprogress. In search for an answer, we inspect thousands of masks from COCO\n(2017 version) and uncover different types of errors such as imprecise mask\nboundaries, non-exhaustively annotated instances, and mislabeled masks. Due to\nthe prevalence of COCO, we choose to correct these errors to maintain\ncontinuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner\nset of annotations with visibly better mask quality than COCO-2017. We evaluate\nfifty object detectors and find that models that predict visually sharper masks\nscore higher on COCO-ReM, affirming that they were being incorrectly penalized\ndue to errors in COCO-2017. Moreover, our models trained using COCO-ReM\nconverge faster and score higher than their larger variants trained using\nCOCO-2017, highlighting the importance of data quality in improving object\ndetectors. With these findings, we advocate using COCO-ReM for future object\ndetection research. Our dataset is available at https://cocorem.xyz",
        "updated": "2024-03-27 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18819v1"
    },
    {
        "title": "ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion",
        "authors": "Daniel WinterMatan CohenShlomi FruchterYael PritchAlex Rav-AchaYedid Hoshen",
        "links": "http://arxiv.org/abs/2403.18818v1",
        "entry_id": "http://arxiv.org/abs/2403.18818v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18818v1",
        "summary": "Diffusion models have revolutionized image editing but often generate images\nthat violate physical laws, particularly the effects of objects on the scene,\ne.g., occlusions, shadows, and reflections. By analyzing the limitations of\nself-supervised approaches, we propose a practical solution centered on a\n\\q{counterfactual} dataset. Our method involves capturing a scene before and\nafter removing a single object, while minimizing other changes. By fine-tuning\na diffusion model on this dataset, we are able to not only remove objects but\nalso their effects on the scene. However, we find that applying this approach\nfor photorealistic object insertion requires an impractically large dataset. To\ntackle this challenge, we propose bootstrap supervision; leveraging our object\nremoval model trained on a small counterfactual dataset, we synthetically\nexpand this dataset considerably. Our approach significantly outperforms prior\nmethods in photorealistic object removal and insertion, particularly at\nmodeling the effects of objects on the scene.",
        "updated": "2024-03-27 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18818v1"
    },
    {
        "title": "Garment3DGen: 3D Garment Stylization and Texture Generation",
        "authors": "Nikolaos SarafianosTuur StuyckXiaoyu XiangYilei LiJovan PopovicRakesh Ranjan",
        "links": "http://arxiv.org/abs/2403.18816v1",
        "entry_id": "http://arxiv.org/abs/2403.18816v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18816v1",
        "summary": "We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. First, we leverage the recent progress of\nimage to 3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nSecond, we introduce carefully designed losses that allow the input base mesh\nto freely deform towards the desired target, yet preserve mesh quality and\ntopology such that they can be simulated. Finally, a texture estimation module\ngenerates high-fidelity texture maps that are globally and locally consistent\nand faithfully capture the input guidance, allowing us to render the generated\n3D assets. With Garment3DGen users can generate the textured 3D garment of\ntheir choice without the need of artist intervention. One can provide a textual\nprompt describing the garment they desire to generate a simulation-ready 3D\nasset. We present a plethora of quantitative and qualitative comparisons on\nvarious assets both real and generated and provide use-cases of how one can\ngenerate simulation-ready 3D garments.",
        "updated": "2024-03-27 17:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18816v1"
    }
]