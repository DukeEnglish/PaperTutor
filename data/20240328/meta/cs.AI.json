[
    {
        "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
        "authors": "Yanwei LiYuechen ZhangChengyao WangZhisheng ZhongYixin ChenRuihang ChuShaoteng LiuJiaya Jia",
        "links": "http://arxiv.org/abs/2403.18814v1",
        "entry_id": "http://arxiv.org/abs/2403.18814v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18814v1",
        "summary": "In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.",
        "updated": "2024-03-27 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18814v1"
    },
    {
        "title": "ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation",
        "authors": "Suraj PatniAradhye AgarwalChetan Arora",
        "links": "http://arxiv.org/abs/2403.18807v1",
        "entry_id": "http://arxiv.org/abs/2403.18807v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18807v1",
        "summary": "In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.",
        "updated": "2024-03-27 17:53:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18807v1"
    },
    {
        "title": "Long-form factuality in large language models",
        "authors": "Jerry WeiChengrun YangXinying SongYifeng LuNathan HuDustin TranDaiyi PengRuibo LiuDa HuangCosmo DuQuoc V. Le",
        "links": "http://arxiv.org/abs/2403.18802v1",
        "entry_id": "http://arxiv.org/abs/2403.18802v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18802v1",
        "summary": "Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.",
        "updated": "2024-03-27 17:48:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18802v1"
    },
    {
        "title": "Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction",
        "authors": "Qiuhong ShenXuanyu YiZike WuPan ZhouHanwang ZhangShuicheng YanXinchao Wang",
        "links": "http://arxiv.org/abs/2403.18795v1",
        "entry_id": "http://arxiv.org/abs/2403.18795v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18795v1",
        "summary": "We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.",
        "updated": "2024-03-27 17:40:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18795v1"
    },
    {
        "title": "ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object",
        "authors": "Chenshuang ZhangFei PanJunmo KimIn So KweonChengzhi Mao",
        "links": "http://arxiv.org/abs/2403.18775v1",
        "entry_id": "http://arxiv.org/abs/2403.18775v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18775v1",
        "summary": "We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.",
        "updated": "2024-03-27 17:23:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18775v1"
    }
]