[
    {
        "title": "Usage-Specific Survival Modeling Based on Operational Data and Neural Networks",
        "authors": "Olov HolmerMattias KrysanderErik Frisk",
        "links": "http://arxiv.org/abs/2403.18739v1",
        "entry_id": "http://arxiv.org/abs/2403.18739v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18739v1",
        "summary": "Accurate predictions of when a component will fail are crucial when planning\nmaintenance, and by modeling the distribution of these failure times, survival\nmodels have shown to be particularly useful in this context. The presented\nmethodology is based on conventional neural network-based survival models that\nare trained using data that is continuously gathered and stored at specific\ntimes, called snapshots. An important property of this type of training data is\nthat it can contain more than one snapshot from a specific individual which\nresults in that standard maximum likelihood training can not be directly\napplied since the data is not independent. However, the papers show that if the\ndata is in a specific format where all snapshot times are the same for all\nindividuals, called homogeneously sampled, maximum likelihood training can be\napplied and produce desirable results. In many cases, the data is not\nhomogeneously sampled and in this case, it is proposed to resample the data to\nmake it homogeneously sampled. How densely the dataset is sampled turns out to\nbe an important parameter; it should be chosen large enough to produce good\nresults, but this also increases the size of the dataset which makes training\nslow. To reduce the number of samples needed during training, the paper also\nproposes a technique to, instead of resampling the dataset once before the\ntraining starts, randomly resample the dataset at the start of each epoch\nduring the training. The proposed methodology is evaluated on both a simulated\ndataset and an experimental dataset of starter battery failures. The results\nshow that if the data is homogeneously sampled the methodology works as\nintended and produces accurate survival models. The results also show that\nrandomly resampling the dataset on each epoch is an effective way to reduce the\nsize of the training data.",
        "updated": "2024-03-27 16:32:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18739v1"
    },
    {
        "title": "Semi-Supervised Learning for Deep Causal Generative Models",
        "authors": "Yasin IbrahimHermione WarrKonstantinos Kamnitsas",
        "links": "http://arxiv.org/abs/2403.18717v1",
        "entry_id": "http://arxiv.org/abs/2403.18717v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18717v1",
        "summary": "Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.",
        "updated": "2024-03-27 16:06:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18717v1"
    },
    {
        "title": "Aiming for Relevance",
        "authors": "Bar Eini PoratDanny EytanUri Shalit",
        "links": "http://arxiv.org/abs/2403.18668v1",
        "entry_id": "http://arxiv.org/abs/2403.18668v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18668v1",
        "summary": "Vital signs are crucial in intensive care units (ICUs). They are used to\ntrack the patient's state and to identify clinically significant changes.\nPredicting vital sign trajectories is valuable for early detection of adverse\nevents. However, conventional machine learning metrics like RMSE often fail to\ncapture the true clinical relevance of such predictions. We introduce novel\nvital sign prediction performance metrics that align with clinical contexts,\nfocusing on deviations from clinical norms, overall trends, and trend\ndeviations. These metrics are derived from empirical utility curves obtained in\na previous study through interviews with ICU clinicians. We validate the\nmetrics' usefulness using simulated and real clinical datasets (MIMIC and\neICU). Furthermore, we employ these metrics as loss functions for neural\nnetworks, resulting in models that excel in predicting clinically significant\nevents. This research paves the way for clinically relevant machine learning\nmodel evaluation and optimization, promising to improve ICU patient care. 10\npages, 9 figures.",
        "updated": "2024-03-27 15:11:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18668v1"
    },
    {
        "title": "Neural Network-Based Piecewise Survival Models",
        "authors": "Olov HolmerErik FriskMattias Krysander",
        "links": "http://arxiv.org/abs/2403.18664v1",
        "entry_id": "http://arxiv.org/abs/2403.18664v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18664v1",
        "summary": "In this paper, a family of neural network-based survival models is presented.\nThe models are specified based on piecewise definitions of the hazard function\nand the density function on a partitioning of the time; both constant and\nlinear piecewise definitions are presented, resulting in a family of four\nmodels. The models can be seen as an extension of the commonly used\ndiscrete-time and piecewise exponential models and thereby add flexibility to\nthis set of standard models. Using a simulated dataset the models are shown to\nperform well compared to the highly expressive, state-of-the-art energy-based\nmodel, while only requiring a fraction of the computation time.",
        "updated": "2024-03-27 15:08:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18664v1"
    },
    {
        "title": "Theoretical Guarantees for the Subspace-Constrained Tyler's Estimator",
        "authors": "Gilad LermanFeng YuTeng Zhang",
        "links": "http://arxiv.org/abs/2403.18658v1",
        "entry_id": "http://arxiv.org/abs/2403.18658v1",
        "pdf_url": "http://arxiv.org/pdf/2403.18658v1",
        "summary": "This work analyzes the subspace-constrained Tyler's estimator (STE) designed\nfor recovering a low-dimensional subspace within a dataset that may be highly\ncorrupted with outliers. It assumes a weak inlier-outlier model and allows the\nfraction of inliers to be smaller than a fraction that leads to computational\nhardness of the robust subspace recovery problem. It shows that in this\nsetting, if the initialization of STE, which is an iterative algorithm,\nsatisfies a certain condition, then STE can effectively recover the underlying\nsubspace. It further shows that under the generalized haystack model, STE\ninitialized by the Tyler's M-estimator (TME), can recover the subspace when the\nfraction of iniliers is too small for TME to handle.",
        "updated": "2024-03-27 15:03:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.18658v1"
    }
]