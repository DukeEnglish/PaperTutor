INFORMATIONAL PUTS
Andrew Koh* Sivakorn Sanguanmoo† Kei Uzui‡
MIT MIT MIT
November 15, 2024
Abstract
We fully characterize how dynamic information should be provided to
uniquely implement the largest equilibrium in dynamic binary-action super-
modular games. The designer offers an informational put: she stays silent
in good times, but injects asymmetric and inconclusive public information if
playerslosefaith. Thereis(i)nomultiplicitygap: thelargest(partially)imple-
mentableequilibriumcanbeimplementeduniquely;and(ii)nointertemporal
commitment gap: the policy is sequentially optimal. Our results have sharp
implicationsforthedesignofpolicyincoordinationenvironments.
*MITDepartmentofEconomics;email: ajkoh@mit.edu
†MITDepartmentofEconomics;email: sanguanm@mit.edu
‡MITDepartmentofEconomics;email: kuzui@mit.edu
First version: December 2023. We are especially grateful to Drew Fudenberg and Stephen Morris
for guidance, support, and many illuminating conversations. We also thank Daron Acemoglu,
Matt Elliott, Nobuhiro Kiyotaki, Daniel Luo, Daisuke Oyama, Satoru Takahashi, Iva´n Werning,
Alex Wolitzky, Muhamet Yildiz, as well as audiences at Cambridge University, the 25th ACM
Conference on Economics and Computation (EC’24), the Econometric Society North American
and Asian Meetings, Nuffield College Oxford, and MIT Finance, Macro, and Theory Lunches for
helpfulcomments.
4202
voN
41
]HT.noce[
1v19190.1142:viXra1 Introduction
Many economic environments feature (i) uncertainty about a payoff-relevant fun-
damental state, (ii) coordination motives, and (iii) stochastic opportunities to revise
actions. These elements are present across all aspects of social and economic life
e.g.,macroeconomics,finance,industrialorganization,andpoliticaleconomy.1
Equilibria of such games are sensitive to dynamic information. Consider a player
who, at any history of the game, finds herself with the opportunity to re-optimize
her action. The fundamental state matters for her flow payoffs, so her decision
must depend on her current beliefs. Moreover, since she plays the same action
untilshecannextre-optimize,herdecisionalsodependsonherbeliefsaboutwhat
futureagentswilldo. Butthosebeliefsdepend,inturn,onwhatsheexpectsfuture
playerstolearn,aswellastheirbeliefsabouttheplayofagentsyetfurtheroutinto
the future. Thus, the stochastic evolution of future beliefs—even those arbitrarily
distant—shapeincentivesinthepresent.
Weareinterestedindynamicinformationpolicieswhichfullyimplementthelargest
time path of aggregate play i.e., as a unique subgame equilibrium of the induced
stochastic game. Our main result (Theorem 1) fully characterizes the form, value,
andsequentialoptimalityofdesigner-optimalpolicies:
1. Form. The form of optimal dynamic information relies on the delivery of care-
fully chosen off-path information. If players take the designer’s preferred ac-
tion, the designer stays silent. If, however, agents deviate from a target path of
play specified by the policy, the designer injects an asymmetric and inconclusive
publicsignal—thisistheinformationalput.2
The signal is asymmetric such that the probability that agents become a little
moreconfidentisfarhigherthantheprobabilitythatagentsbecomemuchmore
pessimistic. Thesesmallbuthigh-probabilitymovementsinthedirectionofthe
1In macroeconomics, firms are uncertain about economic conditions, face complementarities
(NakamuraandSteinsson,2010),andchangetheirpricesatticksofaPoissonclock(Calvo,1983).In
finance,creditorsareuncertainaboutthedebtor’sprofitability/solvency(GoldsteinandPauzner,
2005),haveincentivetorunifothers’run(DiamondandDybvig,1983),butmightonlybeableto
withdraw their debt at staggered intervals (He and Xiong, 2012). In industrial organization, con-
sumersareuncertainaboutaproduct’squality,haveincentivetoadoptthesameproductasothers
(FarrellandSaloner,1985;EllisonandFudenberg,2000),andfacestochasticadoptionopportunities
(Biglaiser,Cre´mer,andVeiga,2022).
2This is analogous to the “Fed put” in which the Fed’s history of intervening to halt market
downturnshasarguablycreatedthebeliefthattheyareinsuredagainstdownsiderisk(Milleretal.,
2002). Thisisasif theFedhasofferedthemarketaputoptionasinsuranceagainstdownturns. In
our setting, the designer steps in to inject information when players start switching to action 0
which, as we will show, with high probability induces aggregate play to correct. This is as if the
designerhasofferedplayersaputoptionasinsuranceagainststrategicuncertaintyabouttheplay
offutureplayers.
1dominance region—at which playing the designer’s preferred action is strictly
dominant—arechainedtogethersuchthattheuniqueequilibriumofthesubgame
is for future players to play the designer-preferred action.3 The signal is incon-
clusive such that, even if agents turn pessimistic, they do not become exces-
sivelyso—thiswillbeimportantforsequentialoptimality.
2. Value. The sequentially optimal policy uniquely implements the upper-bound
onthetimepathofaggregateplay. Thus,thereisnomultiplicitygap: whatever
can be implemented partially (i.e., as an equilibrium) can also be implemented
fully (i.e., as the unique equilibrium). This is in sharp contrast to recent work
onstaticimplementationviainformationdesigninsupermodulargameswhich
findstheregenericallyexistsagapevenwithprivateinformationandtheability
to manipulate higher-order beliefs (Morris, Oyama, and Takahashi, 2024), or
withbothprivateinformationandtransfers(Halac,Lipnowski,andRappoport,
2021).
3. Sequential optimality. Our dynamic information policy is constructed such
that at every history, the designer has no incentive to deviate.4 Thus, there is
nointertemporalcommitmentgap: whatevercanbeimplementedwithex-ante
commitment to the dynamic information structure can also be implemented
when the sender can continually re-optimize her dynamic information.5 Se-
quentiallyoptimalityarisesthroughthedelicateinteractionbetweenproperties
of our policy: asymmetry, chaining, and inconclusiveness. Asymmetric off-path
information are chained together to obtain full implementation at all states in
which the designer-preferred action is not strictly dominated. Then, inconclu-
sive off-path information ensures that, even if agents turn pessimistic, full im-
plementationisstillguaranteed.
Conceptually,ourcontributionhighlightshowoff-pathinformationshouldbeop-
timally deployed to shape on-path incentives. Of course, it is well-known from
implementation theory (Moore and Repullo, 1988; Abreu and Matsushima, 1992)
thatoff-paththreatsarepowerful,albeitnotsequentiallyoptimal—ifthedeviation
actually occurs, there is no incentive to follow-through with the policy.6 Informa-
tionisdifferentintwosubstantiveways. Itislesspowerful: beliefsaremartingales,
3This is done via a ”contagion argument” which can be viewed as the dynamic analog of the
interimdeletionofstrictlydominatedstrategiesinstaticgamesofincompleteinformation.
4With the caveat that for a small set of histories, deviation incentives can be made arbitrarily
small. Forhistorieswherethisisso,thisissimplybecauseoptimalinformationpoliciescontinuing
from those histories do not exist. Nonetheless, this can be approached via a sequence of policies
so that the gap vanishes along this sequence. This openness property is also typical of static full
implementationenvironmentsashighlightedbyMorris,Oyama,andTakahashi(2024).
5We further emphasize that sequential optimality is not given—we offer examples of policies
whichareoptimalbutnotsequentiallyoptimal.
6Withthecaveatthatinimplementationtheory,thedesigner’sobjectivefunctionistypicallynot
2which imposes severe constraints on what payoffs can be delivered off-path. But
it is also more flexible: the designer has the freedom to design any distribution of
off-pathbeliefs. Whatshouldwemakeofthesedifferences?
First, we will show that off-path information, through less powerful on its own,
can be chained together to close the gap between full and partial implementation.
Second,theflexibilityofoff-pathinformationcanbeexploitedtoshapethecontin-
uation incentives of the designer. This ensures that the designer’s counterfactual
selvesatzeroprobabilityhistoriesarewillingtofollowthroughwiththepromised
information. Together, these insights offer a novel and unified treatment of dy-
namicinformationdesigninsupermodulargames.
Economically,ourresultshavesharpimplicationsforarangeofphenomenawhere
coordinationandmultipleequilbiriafeatureprominentlye.g.,infinance(debtruns,
currency crises), macroeconomics (price setting), trade and industrial policy (big
pushes), industrial organization (network goods), and political economy (revolu-
tions). Webrieflydiscussthisafterstatingourmainresult.
Related Literature Our results relate most closely to recent work on full imple-
mentation in supermodular games via information design (Morris, Oyama, and
Takahashi, 2024; Inostroza and Pavan, 2023; Li, Song, and Zhao, 2023). In this
literature, information design induces non-degenerate higher-order beliefs, and
this is important to obtain uniqueness via a ”contagion argument” over the type
space. Bycontrast,ourdynamicinformationispublicandhigher-orderbeliefsare
degenerate but we leverage a distinct kind of ”intertemporal contagion”. A key
takeawayfromthisliteratureisthatthereistypicallyagapbetweenthedesigner’s
value under adversarial equilibrium selection, and under designer-favorable se-
lection (what we call a “multiplicity gap”); by contrast, we show that for dynamic
binary-actionsupermodulargamesthereisnosuchgap.
Also related is the elegant and complementary work of Basak and Zhou (2020)
and Basak and Zhou (2022). We highlight several substantive differences. First,
we study different dynamic games: in Basak and Zhou (2020, 2022) players make
a once-and-for-all decision on whether to play the risky action, and they focus on
regime change games—both features play a key role in their analysis;7 in ours,
specified: we have in mind an environment in which the designer is a player in the game, and
punishingplayersiscostly. Seealsoworkonmechanismdesignwithlimitedcommitment(Laffont
andTirole,1988;BesterandStrausz,2001;Skreta,2015;Liuetal.,2019;DovalandSkreta,2022)and
macroeconomicswheretime-inconsistencyplaysacrucialrole(HalacandYared,2014).
7Basak and Zhou (2020) study a regime change game with private information where the de-
signer can choose the frequency at which she discloses whether or not the regime has survived.
BasakandZhou(2022)studyanoptimalstoppinggamewitharegimechangepayoffstructurein
whichagentschooseswhentoundertakeanirreversibleriskyaction.
3agents can continually re-optimize at the ticks of their Poisson clocks and play
a general binary-action supermodular game where the designer’s payoff is any
increasing functional from the path of aggregate play. Importantly, our optimal
dynamic information policies—and the reasons they work—are entirely distinct;
wediscussthismorethoroughlyafterstatingourmainresult.
Our paper also relates to work on the equilibria of dynamic coordination games.
An important paper of Gale (1995) studies a complete information investment
game where players can decide when, if ever, to make an irreversible investment
and investing is payoff dominant.8 The main result is that investment succeeds
across all subgame perfect equilibria. Our environment and results differ in sev-
eral substantive ways. For instance, our policy allows the designer to implement
thelargestequilibria—irrespectiveofwhetheritispayoffdominant.9
Ourresultsarealsoconnectedtotheliteratureondynamicimplementation. Moore
andRepullo(1988)showthatarbitrarysocialchoicefunctionscanbeachievedwith
large off-path transfers.10 Glazer and Perry (1996) show that virtual implementa-
tionofsocialchoicefunctionscanbeachievedbyappealingtoextensive-formver-
sions of Abreu and Matsushima (1992) mechanisms.11 Chen et al. (2023) weaken
backward induction to initial rationalizability.12 Different from these papers, our
designer is substantially more constrained: (i) there is no freedom to design the
extensive-formgamewhichwetakeasgiven;(ii)thedesigneronlyoffersdynamic
information;and(iii)ourpolicyissequentiallyoptimal.
Our game is one where players have stochastic switching opportunities. Variants
of these models have been studied in macroeconomics (Diamond, 1982; Calvo,
1983; Diamond and Fudenberg, 1989; Frankel and Pauzner, 2000), industrial pol-
icy(Murphy,Shleifer,andVishny,1989;Matsuyama,1991),finance(HeandXiong,
8See also Chamley (1999); Dasgupta (2007); Angeletos, Hellwig, and Pavan (2007); Mathevet
andSteiner(2013);Koh,Li,andUzui(2024a)allofwhichstudytheequilibriaofdifferentdynamic
coordinationgames.
9Moreover, actions in our environment are reversible, so sans any information (and assuming
beliefsarenotinthedominanceregions)therewillexistsubgameperfectequilbiriainwhichplay-
ers “cycle” between actions; this is ruled out in the environment of Gale (1995) because of irre-
versibility. More subtly, our dynamic information works with—but does not rely on—atomless
players i.e., we obtain full implementation even if each player believes that they will not change
theaggregatestate. Bycontrast,atomicplayersisanessentialfeatureofGale(1995).
10SeealsoAghion,Fudenberg,Holden,Kunimoto,andTercieux(2012)foradiscussionofthelack
ofrobustnesstosmallamountsofimperfectinformation,andPenta(2015)whotakesabelief-free
approachtodynamicimplementation.
11SeeworkbyChenandSun(2015)whoexploitthefreedomtodesigntheextensive-form. Sato
(2023) designs both the extensive-form and information structure a la Doval and Ely (2020) and
furtherutilizesthefactthedesignercandesigninformationaboutplayers’pastmoves;bycontrast,
wefixthedynamicgameandpastplayisobserved.
12Thatis,onlyimposingsequentialrationalityandcommonknowledgeofsequentialrationality
atthebeginningofthegame,but”anythinggoes”off-path;seeBen-Porath(1997).
42012), industrial organization (Biglaiser, Cre´mer, and Veiga, 2022), and game the-
ory (Burdzy, Frankel, and Pauzner, 2001; Matsui and Matsuyama, 1995; Oyama,
2002; Kamada and Kandori, 2020).13 A common insight from this literature is that
switching frictions can generate uniqueness, and the risk-dominant profile is se-
lected via a process of backward induction. Our contribution is to show how the
largest equilibrium can be uniquely implemented by carefully choosing the dy-
namicinformationpolicy.
Sequentialoptimalityisanimportantpropertyofourinformationpolicyandthus
our work relates to recent work studying the role of (intertemporal) commitment
in dynamic information design. Koh and Sanguanmoo (2022); Koh, Sanguanmoo,
and Zhong (2024b) show by construction that sequential optimality is generally
achievableinsingle-agentstoppingproblems. Itwillturnoutthatsequentialopti-
malpoliciesalsoexistinourenvironment,butforquitedistinctreasons;wediscuss
thismorethoroughlyinSection3.
2 Model
Environment There is a finite set of states Θ = {θ ,θ ...,θ }. We use ∆(Θ) to
1 2 n
denote the set of probability measures and endow it with the Euclidian metric.
Thereis aninteriorcommonprior µ ∈ ∆(Θ)\∂∆(Θ) anda unitmeasureofplay-
0
ers indexed i ∈ I := [0,1]. Time is continuous and indexed T := [0,+∞). The
action space is binary: a ∈ A := {0,1} where a is i’s action at time t. Write
it it
(cid:82)
A := a di to denote the proportion of players playing action 1 at time t. Work-
t it
ing with a continuum of agents makes our analysis cleaner because randomness
from individual switching frictions vanish in the aggregate.14 An analog of our
resultholdsforafiniteplayers;wedevelopthisinOnlineAppendixI.
Payoffs The flow payoff for each player is u : {0,1}×[0,1]×Θ → R . We write
∆ u(A,θ) := u(1,A,θ) − u(0,A,θ) to denote the payoff difference from action 1
relativeto0andassumethroughout:
(i) Supermodularity. ∆ u(A,θ) is continuously differentiable and strictly in-
creasingin A.
(ii) Dominantstate. Thereexists θ∗ ∈ Θ suchthat ∆ u(0,θ∗) > 0.
13See also more recent work by Guimaraes and Machado (2018); Guimaraes, Machado, and
Pereira(2020). AngeletosandLian(2016)offeranexcellentsurvey.
14By an appropriate continuum law of large numbers (Sun, 2006) where we endow the player
space[0,1]withtheappropriateLebesgueextension. Workingwithacontinuumalsoclarifiesthat
atomicplayersarenotrequiredfortheuseofoff-pathinformation;wediscussthisinSection4.
5Condition (i) states that the game is one of strategic complements. Condition (ii)
isastandardrichnessassumptiononthespaceofpossiblepayoffstructures: there
existssomestate θ∗ underwhichplayingaction1isstrictlydominant.15
(cid:82)
The payoff of player i ∈ I is e−rtu(a ,A ,θ)dt where r > 0 is an arbitrary dis-
it t
count rate. Each player is endowed with a personal Poisson clock which ticks at
anindependentrate λ > 0. Playerscanonlyre-optimizeattheticksoftheirclocks
(Calvo, 1983; Matsui and Matsuyama, 1995; Frankel and Pauzner, 2000; Frankel,
Morris, and Pauzner, 2003; Kamada and Kandori, 2020). Our dynamic supermod-
ular game is quite general with the caveat that players are homogeneous.16 We
discusstheheterogeneouscaseinSection4.
(cid:0) (cid:1)
Dynamic information policies A history H t := (µ s) s≤t,(A s) s≤t specifies be-
liefs and aggregate play up to time t. Let H be the set of all histories and H :=
t
(cid:83) H . WriteF asthenaturalfiltrationgeneratedbyhistories. Adynamicinfor-
t≥0 t t
mationpolicyisa (F ) -martingale. Let
t t
(cid:110) (cid:111)
M := µ′ : µ′ isa (F ) -martingale, µ = µ′ a.s. .
t t 0 0
bethesetofalldynamicinformationpolicies,whereweemphasizethatthelawof
µ ∈ M candependonpastplay.
Strategies and Equilibria A strategy σ : H → ∆{0,1} is a map from histories to
i
a distribution over actions so that if i’s clock ticks at time t, her choice of action is
given by history H t− := lim t′↑t H t.17 Given µ, this induces a stochastic game;18 let
Σ(µ,A ) denote the set of subgame perfect equilibria of the stochastic game. We
0
focus on subgame perfection because there is no private information so the game
continuingfromeachhistorycorrespondstoapropersubgame.19
Figure1illustratestheconnectionbetweendynamicinformationpolicies(topleft),
15ThisassumptionisidenticaltothatinMorris,Oyama,andTakahashi(2024).
16AsimilarassumptionhasbeenmadeinstaticenvironmentsbyInostrozaandPavan(2023);Li
et al. (2023) and was weakened by Morris, Oyama, and Takahashi (2024) who characterize opti-
malprivateinformationforfullimplementationbyfocusingonpotentialgameswithaconvexity
requirement,whichamountstotherenotbeing“toomuchheterogeneity”acrossplayers.
17Thisiswell-definedsince(A ) isa.s. continuousand(µ ) hasleft-limits. Sincethemeasureof
t t t t
agentswhoactattimetisalmostsurelyzero,ourgameisineffectequivalenttooneinwhichplay
attimetdependsonhistoryH .
t
18Notethatinformationispublicsoallagentssharethesamebeliefs;inAppendixBwerelaxthis
toshowthatprivateinformationoftencannotdobetter.
19Hence, subgame perfection in our setting coincides trivially with Perfect-Bayesian Equilibria
(Fudenberg and Tirole, 1991); since we are varying the dynamic information structure, this also
corresponds to dynamic Bayes Correlated Equilibria (Makris and Renou, 2023)—but only in the
trivialsensesincehigher-orderbeliefsaredegenerate.
6Figure1: Relationshipbetweenbeliefs,equilibria,andactionpaths
equilibria(topright),andthepathofaggregateactions(bottom). Eachinformation
policy (µ ) specifies a Cadlag martingale which depends on both its past realiza-
t t
tions, as well as past aggregate play. Given this information policy, this induces
a set of equilibria Σ(µ,A ). Both the realizations of beliefs (µ ) as well as the
0 t t
selected equilibrium σ ∈ Σ(µ,A ) induce a path of aggregate play (A ) . The de-
0 t t
signer’sproblemistochooseitsdynamicinformationpolicytoinfluencethesetof
equilbriaandthus (A ) .
t t
Designer’sproblemunderadversarialequilibriumselection Thedesigner’sprob-
lemundercommitmentwhennatureischoosingthebestequilibriumis
(cid:104) (cid:105)
sup
Eσ ϕ(cid:0) A(cid:1)
(OPT)
µ∈M
σ∈Σ(µ,A )
0
Conversely,whennatureischoosingtheworstequilibrium,theproblemis
(cid:104) (cid:105)
sup inf
Eσ ϕ(cid:0) A(cid:1)
(ADV)
µ∈Mσ∈Σ(µ,A 0)
where ϕ : A → R is an increasing and bounded functional from the path-space
(cid:82)
of aggregate play A e.g., the discounted measure of play ϕ(A) = e−rtA dt with
t
r > 0.
7Sequential Optimality If the designer cannot commit to future information, off-
path delivery of information might have no bite in the present. To this end, we
can define the payoff gap at history H as the value of the best deviation from the
t
originalpolicy µ:
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
inf Eσ ϕ(cid:0) A(cid:1) (cid:12)F − sup inf Eσ ϕ(A)(cid:12)F ≥ 0
(cid:12) t (cid:12) t
σ∈Σ(µ,A 0) µ′∈Mσ∈Σ(µ′,A 0)
where F is the filtration corresponding to H . µ is sequentially optimal if the gap
t t
is zero for all histories H ∈ H. Sequential optimality is demanding and states
t
that at every history—including off-path ones—the designer still finds it optimal
tofollowthroughwithherdynamicinformationpolicy.
3 Optimaldynamicinformation
Webeginwithanintuitivedescriptionofasequentially-optimaldynamicinforma-
tion policy for binary states before constructing it formally. With binary states, we
set Θ = {0,1} where 1 is the dominant state. Beliefs are one-dimensional and we
willdirectlyassociateµ := P(θ = 1|F ). Letµbetheupper-dominanceregion: µ(A)
t t
is the lowest belief such that if the current aggregate play is A, playing action 1 is
strictlydominantnomatterthefutureplayofothers.
I. State is near the upper-dominance region. First suppose that at time t, the
public belief µ and aggregate play A is close to the upper-dominance region as
t t
illustrated by the blue dot labelled (µ ,A ) in Figure 2 (a). If players switch to
t t
action 1, the designer stays silent. Thus, aggregate action progressively increases
asillustratedbytheupwardarrowsinFigure2(a).
Figure2: Policynearupper-dominanceregion
(a)Silenceon-path (b)Injectionintodominanceregion
8Butsuppose,instead,thatplayersstartplayingaction0asdepictedinFigure2(b)
I. Then, the designer injects asymmetric information: it is very likely that agents
become slightly more optimistic i.e., public beliefs move up a little and into the
upper-dominance region, but there is a small chance agents become much more
pessimistic (Fig. 2 (b) II). Suppose that this deviation happened and so this infor-
mationisinjectedand,furthermore,thatithasmadeagentsalittlemoreconfident.
Then,onthisevent,futurebeliefsareintheupper-dominanceregionsoitisstrictly
dominantforfutureagentstotakeaction1. Correspondingly,thedesignerdelivers
no further information (Fig. 2 (b) III) and aggregate play begins to increase there-
after. But,knowingthatthissequenceofeventsislikelytotakeplace,andbecause
agents have coordination motives, deviating to action 0 in the first place is strictly
dominated.
II. State is far from upper-dominance region. Next consider Figure 3 (a) where
(µ ,A ) is further away from the dominance region. Our previous argument now
t t
breaks down: there is no way for off-path information—no matter how cleverly
designed—toensurebeliefsreachthedominanceregionwithahighenoughprob-
ability as to deter the initial deviation to action 0. This is the key weakness of
off-pathinformationvis-a-visoff-pathtransfers. Whatthendoesthedesignerdo?
Figure3: Chainingoff-pathinformation
(a)Chaining (b)Contagiontolower-dominance
If players start switching to 1, the designer delivers asymmetric information so
that, with high probability, agents become a little more confident—but not confi-
dent enough that action 1 is strictly dominant. This is depicted in Figure 3 (a) II.
Uponthisrealization,iffutureagentscontinuedeviatingto0,thepolicyinjectsyet
another bout of asymmetric information which, with high probability, pushes be-
liefs into the upper-dominance region. This is depicted in Figure 3 (a) IIIB. Know-
ing this, we have already seen that those future agents strictly prefer to switch to
91. But knowing that, agents in the present state (µ ,A ), anticipating that upon
t t
deviation the injection will, with high probability, induce future agents to play 1,
alsostrictlyprefertoplay1inthepresent.
What are the limits of this line of reasoning? It turns out that by choosing our dy-
namic information policy carefully, we can chain together these off-path informa-
tioninsuch awayasto obtainfullimplementationatallbelief-aggregatepairsfor
whichaction1isnotstrictlydominated. ThisisdepictedbyFigure(b)where,asbe-
fore,theblueandpinkshadedregionsrepresenttheupper-andlower-dominance
regions respectively. The logic is related to the “contagion arguments” of Frankel
and Pauzner (2000); Burdzy, Frankel, and Pauzner (2001); Frankel, Morris, and
Pauzner (2003). These papers show that the risk-dominant action is typically se-
lected as the limit of some iterated deletion procedure in which the blue and pink
regions expand with each iteration and meet in the middle which pins down the
uniqueequilibrium.20 Bycontrast,weshowhowdynamicinformationcanbeem-
ployed to generate asymmetric contagion such that only the upper-dominance re-
gion expands to engulf the space of all belief-aggregate play pairs where action 1
isnotstrictlydominated.
III. Designer-preferred action strictly dominated. Now suppose beliefs are so
pessimistic that 1 is strictly dominated i.e., µ ≤ µ(A ) where µ(A ) is the highest
t t t
beliefunderwhich,given A ,action1isstrictlydominated.
t
Figure4: Escapingthelower-dominanceregion
(a)Immediateinjection (b)Delayedinjection (c)‘Smooth’injection
20In Frankel and Pauzner (2000); Burdzy, Frankel, and Pauzner (2001) this is also obtained via
backwardinduction,whereasymmetricrandomprocessgovernsaggregateincentives.Mappedto
ourmodel, thiscorrespondstopublicinformationsothatthebeliefmartingaleisatime-changed
Brownianmotion. InFrankel,Morris,andPauzner(2003),thethisisobtainedviainterimdeletion
ofstrictlydominatedstrategiesinmany-actionglobalgames,thoughthelogicissimilar.
10Then,theabovepolicynolongerworks: evenifplayersexpectallfutureplayersto
switchto1,theyaresopessimisticaboutthestatethatswitchingto0isstrictlybet-
ter. Now, the designer has to offer non-trivial information on-path to push beliefs
outofthelower-dominanceregion. Howisthisoptimallydone?
Figure4(a)illustratestheoptimalpolicywhichconsistsofanimmediateandprecise
injectionofinformationsuchthatbeliefsjumptoeither0or(just)outofthelower-
dominanceregion. Theoptimalityofsuchapolicyisbuiltontheobservationthatif
thedesignerdoesnotinterveneearlytocurtailplayersfromprogressivelyswitch-
ing to 0, it simply becomes more difficult to escape the lower-dominance region
down the line. Consider, for instance, the policy in Figure 4 (b) which also injects
precise information to maximize the chance of escaping the lower-dominance re-
gion, but with a delay. Before this injection, players switch to action 0 and since
µ(A) is strictly decreasing, the probability of escaping the dominance region is
strictly smaller. For similar reasons, the policy illustrated in Figure 4 (c) which
inducescontinuoussamplebeliefpathsisalsosub-optimal.
IV. Sequential optimality. Our previous discussion specified off-path injections
of policies upon deviation away from the action 1. Of course, if such deviations
actuallyoccur,thedesignermaynothaveanyincentivetofollow-throughwithits
policy. For instance, consider Figure 5 (a) which employs the strategy of injecting
conclusivebadnewsthatthestateis0sothat,withhighprobabilitybeliefsincrease
a little, and with low probability agents learn conclusively that θ = 0. Indeed,
information of this form maximizes the chance that beliefs increase21 and, as we
havedescribed,thesecanbebechainedtogethertoachievefullimplementation.
Figure5: Sequentialoptimality
(a)Notsequentiallyoptimal (b)Sequentiallyoptimal
21AsinKamenicaandGentzkow(2011)andsubsequentwork.
11However, this policy is not sequentially optimal: if agents do deviate and play
action 0, injecting such information is suboptimal because it poses an extra risk: if
conclusive bad news does arrive, beliefs become absorbing at µ = 0 and further
t
information is powerless to influence beliefs—it is then strictly dominant for all
agentstoplay0thereafter. How,then,issequentialoptimalityobtained?
Considerinconclusiveoff-pathinformationasillustratedinFigure5(b)whereeach
blue dot represents a potential injection of off-path information upon players’ de-
viating to action 0. Each injection induces two kinds of beliefs: upon arrival of a
‘good’ signal, agents become a little more optimistic (right arrow); upon arrival
of a ‘bad’ signal, agents become relatively more pessimistic, but not so much that
action 1 becomes strictly dominated (left arrow). Figure 5 illustrates a particular
policyinwhich,uponrealizationofthebadsignalatstate(µ t−,A t),agents’beliefs
move halfway toward the lower-dominance region i.e., to [µ t− +µ(A t)]/2. Con-
versely, if the good signal arrives, believes move up a little, so that the probability
oftheformerismuchhigherthanthelatter.
Bychoosingthisdistributioncarefullyforeachbelief-aggregateactionpair,wecan
achieve full implementation via the chaining argument outlined above, which re-
quiresthat(i)probabilityofthegoodsignalarrivingissufficientlyhighastodeter
deviations;and(ii)movementinbeliefsgeneratedbythegoodsignalissufficiently
large that, when chained together, we obtain full implementation over the whole
region. Atthesametime,thisissequentiallyoptimalsince,wheneverthedesigner
is faced with the prospect of injecting off-path information, she is willing to do
so: with probability 1 agents’ posterior beliefs are such that full implementation
remainspossible.22
Sequential optimality of dynamic information has been recently studied in single-
agentoptimalstoppingproblems(KohandSanguanmoo,2022;Koh,Sanguanmoo,
and Zhong, 2024b) who show that optimal dynamic information can always be
modified to be sequentially optimal.23 In such environments, sequential optimal-
ity is obtained via an entirely distinct mechanism: the designer progressively de-
livers more interim information to raise the agent’s outside option at future histories
which, in turn, ties the designer’s hands in the future. By contrast, in the present
environmentourdesignerchainstogetheroff-pathinformationtogethertoraiseher
owncontinuationvaluebyguaranteeingthat,evenonrealizationsoftheasymmetric
signal,herfutureselfcanalwaysfullyimplementthelargestpathofplay.
22Weemphasizethatthereisnothingcircularaboutthisargument: weiterativelydeleteswitch-
ingtoaction0undertheworst-caseconjecturethat,uponthebadsignalarriving,allfutureagents
play1. Thisissufficienttoobtainfullimplementationaslongasaction1isnotstrictlydominated.
23SeealsoBall(2023)whofindsinadifferentsingle-agentcontractingenvironmentthattheopti-
maldynamicinformationpolicyhappenstobesequentiallyoptimal.
12Construction of sequentially-optimal policy. We now make our previous dis-
cussionpreciseandgeneral.
We will construct a particular martingale µ∗ ∈ M which is ‘Markovian’ in the
sense that the ‘instantaneous’ information at time t depends only on the belief-
aggregate play pair (µ ,A ), as well as an auxiliary (F ) -predictable process (Z )
t t t t t t
wewilldefineaspartofthepolicy. Webeginwithseveralkeydefinitions:
Definition 1 (Lower dominance region). Let Ψ : [0,1] ⇒ ∆(Θ) denote the set of
LD
beliefsunderwhichplayerspreferaction0evenifallfutureplayerschoosetoplay
action1:
(cid:110) (cid:104)(cid:90) τ (cid:105) (cid:111)
Ψ (A ) := µ ∈ ∆(Θ) : E e−rs∆ u(A¯ ,θ)ds ≤ 0 ,
LD t θ∼µ s
t
where A¯ solves dA¯ = λ(1 − A )ds for s ≥ t with boundary A¯ = A and τ
s s s t t
is independently distributed according to an exponential distribution with rate λ
re-normalizedtostartat t.
ObservethatsupermodularityimpliesΨ isdecreasingin A : Ψ (A ) ⊂ Ψ (A′)
LD t LD t LD t
if A > A′ . Ψ is illustrated by the pink region of Figure 6 for the cases where
t t LD
|Θ| = 2(panel(a)),and |Θ| = 3(panel(b)).
Ψ
Figure6: Illustrationof LD,Bd θ∗,and D
(a)|Θ| =2 (b)|Θ| =3
Definition2. Foreach µ ∈/ Ψ (A ) and A ∈ [0,1],define
t LD t t
(cid:26) (cid:27)
D(µ t,A t) := inf α ∈ [0,1] : µ t −α· 1δ −θ∗ µ− (µ θ∗t ) ∈ Ψ LD(A t)∪Bd θ∗
t
13this gives the ‘distance’ from current beliefs µ as it moves along a linear path
t
startingfromδ θ∗ toeither(i)thelowerdominanceregionΨ LD(A t);or(ii)thesetof
beliefsthatassign zeroprobabilityonstate θ∗ whichwedenote withBd θ∗ := {µ ∈
∆(Θ) : µ(θ∗) = 0}. This is depicted in Figure 6 where each blue dot represents a
belief.
Definition 3 (Tolerance, upward/downward jump sizes, belief direction). To de-
scribe the policy when action 1 is not strictly dominated, we specify the following
variables:
(i) Tolerance. TOR(D) specifies the
magnitude of deviation of off-path
play vis-a-vis a target Z . If this is
t
exceeded,thepolicybeginstoinject
additionalinformation.
(ii) Upward jump size. M · TOR(D)
scales the tolerance by a factor of
M > 0, and specifies the upward
movement in beliefs if the injected
informationispositive.
(iii) Downward jump size. DOWN(D)
specifies the downward movement Figure 7: Illustration of up/downward
inbeliefsiftheinjectedinformation jumpsizesandbeliefdirections
isnegative.
(iv) Belief direction. dˆ(µ) ∈ Rn spec-
ifies the direction of belief move-
ments. We set it as the directional
vectorof µ towards δ θ∗.
Definition3specifiesobjectsrequiredtodefineourinformationpolicywhenbeliefs
lie outside of the lower-dominance region µ ∈/ Ψ . We now develop objects to
LD
define our information policy when beliefs lie inside the lower dominace region
µ ∈ Ψ .
LD
14Definition4(Maximalescapeprobabilityandbeliefs). Todescribethepolicywhen
action1isstrictlydominated,afewmoredefinitionsareinorder:
(i) The set of beliefs which are attain-
ablewithprobability p from µ is
F(p,µ) := {µ′ ∈ ∆(Θ) : pµ′ ≤ µ}
which follows from the martingale
propertyofbeliefs.
(ii) Maximal escape probability.
p∗(µ,A) := max{p ∈ [0,1] :
F(p,µ) ⊂ Ψ (A)} is a tight
LD
upper-bound on the probability
Ψ
thatbeliefsescape .
LD
Figure 8: Illustration of F and maximal
(iii) Themaximalescapebeliefsare
escapeprobabilityandbeliefs
∂(η,µ) := F(p∗(µ,A)−η,µ)∩Ψc (A)
LD
where Ψc (A) = ∆(Θ)\Ψ (A).
LD LD
We are (finally!) ready to define our dynamic information policy µ∗ ∈ M. Recall
that µ∗ isCadlag sohasleft-limitswhich wedenotewith µ t− := lim t′↑tµ t. Wewill
∗
simultaneously specify the law of µ as well as construct the stochastic process
(Z ) which is (F ) -predictable24 and initializing Z = A . (Z ) is interpreted as
t t t t 0 0 t t
thetargetedaggregateplayateachhistory.
Given the tuple (µ∗ t−,Z t−,A t), define the time-t information structure and law of
motionof Z asfollows:
t
1. Silence on-path. If action 1 is not strictly dominated i.e, µ t− ∈/ Ψ LD(A t) and
playiswithinthetoleranceleveli.e., |A
t
−Z t−| < TOR(D) then
µ t = µ t− almostsurely,
i.e.,noinformation,and dZ
t
= λ(1−Z t−).
2. Asymmetric and inconclusive off-path injection. If action 1 is not strictly
dominated i.e., µ t− ∈/ Ψ LD(A t) and play is outside the tolerance level i.e.,
24Thatis,Z
t
ismeasurablewithrespecttotheleftfiltrationlim s↑tF s.
15|A
t
−Z t−| ≥ TOR(D) then

µ = µ t− +(M·TOR(D))·dˆ(µ t−) w.p. DOWND (DO )W +N M(D ·T) OR(D)
t µ t− −DOWN(D)·dˆ(µ t−) w.p. DOWNM (D·T )+OR M( ·D T) OR(D),
where dˆ(µ) := 1δ −θ∗ µ− (θµ
∗)
is the (normalized) directional vector of µ toward δ θ∗,
andreset Z = A .
t t
3. Jump. If action 1 is strictly dominated i.e., µ t− ∈ Ψ LD(A t) then beliefs jump
toamaximalescapepoint: pickany µ+ ∈ ∂(µ,η)
(cid:40)
µ+ w.p. p∗(µ t−,A t)−η
µ =
t
µ− w.p. 1−(p∗(µ t−,A t)−η),
where µ− =
µt−(p∗(µt−,At)−η)µ+
.
1−(p∗(µt−,At)−η)
WehavedefinedafamilyofinformationstructureswhichdependonTOR(D)(tol-
erance), M·TOR(D) (upward jump size), DOWN(D) (downward jump size), and
Ψ
η (distance outside the lower-dominance region ). There is some flexibility to
LD
choose them: we will set DOWN(D) = 1D, TOR(D) = m· D2 where m > 0 is a
2
smallconstant, M > 0isalargeconstant.
We choose m small so that TOR(D), the upward jump size, is much smaller than
the downward jump size—this ensures that the probability of becoming (a little)
more optimistic is much larger. M is the ratio between the upward jump size and
the tolerance—it is large to guarantee that off-path information can push future
beliefsintotheupper-dominanceregion. Theexactchoiceofmand Mwilldepend
on the primitives of the game, but are independent of η; a detailed construction is
inAppendixA.Hence,weparameterizethisfamilyofpoliciesby (µη) .
η>0
Theorem1.
(i) Formandvalue.
(cid:104) (cid:105)
lim inf Eσ ϕ(A) = (ADV) = (OPT).
η↓0 σ∈Σ(µη,A )
0
(ii) Sequentialoptimality.
(cid:12) (cid:12)
(cid:12) (cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)(cid:12)
lim sup (cid:12) inf Eσ ϕ(cid:0) A(cid:1) (cid:12)F − sup inf Eσ ϕ(A)(cid:12)F (cid:12) = 0.
(cid:12) (cid:12) t (cid:12) t (cid:12)
η↓0 Ht∈H(cid:12)σ∈Σ(µη,A 0) µ′∈Mσ∈Σ(µ′,A 0) (cid:12)
Proof. SeeAppendixA.
164 Robustnessandgeneralizations
Our dynamic game is quite general in some regards but more specific in others.
Wenowdiscusswhichaspectsarecrucial,andwhichcanberelaxed.
Continuum vs finite players. We worked with a continuum of players so there is
no aggregate randomness in the time path of agents who can re-optimize their ac-
tion.25 Thisdeliversacleaneranalysissincetheonlysourceofrandomnessisfluc-
tuations in beliefs driven by policy. In Online Appendix I we show that Theorem
1 holds, mutatis mutandis, in a model with large but finite number of players.26
There,weshowthatinafiniteversionofthemodel,thesamepolicythatwasopti-
malcontinuumcaseremainscontinuestosolveproblem(ADV)forlargebutfinite
number of players N. In particular, our policy closes the multiplicity gap at rate
O(N−1/9).27
Mathematically,thisrequiresmoreinvolvedargumentstohandlethe
extrarandomnessfromswitchingtimes.
Conceptually, however, finiteness is simpler. Notice that in our continuum model,
players are atomless but nonetheless off-path information is still effective. That is,
players do not need to believe that they can individually influence the state for
fullimplementationtowork. Thisisinsharpcontrasttoworkondynamiccoordi-
nation, durable goods monopolist, or public-good provision games which rely on
thefactthateachagent’sactionmakesasmallbutnon-negligibledifference.28 The
chiefdifferenceisthatinformationaboutdeviationsarelostinthecontinuumcase
(Levine and Pesendorfer, 1995) which precludes the designer from detecting and
respondingtoindividualdeviations.
Our key insight that this is not required: a dynamic policy with a moving target—
such that asymmetric and inconclusive information is injected if aggregate play
fallstoofarfromthetarget—candeliverstrictincentives,evenifindividualplayers
cannot influence aggregate play. That is, our policy credibly insures players against
pathsoffutureplaybyprecludingthepossibilitythat‘toomany’(asprescribedby
the tolerance level) future agents might switch to action 0. In this regard working
withatomlessagentsdeliversanarguablystrongerresult.
Publicvsprivateinformation. Whentheinitialconditionissuchthatplaying1is
25Byasuitablecontinuumlawoflargenumbers(Sun,2006).
26See Aumann (1966) and Fudenberg and Levine (1986); Levine and Pesendorfer (1995) for a
discussionofthesubtletiesbetweencontinuumandfiniteplayers.
27Thatis,|(ADV)−(OPT)| =O(N−1/9);seeOnlineAppendixI.
28For instance Gale (1995) highlights a gap between a continuum and finite number of players
indynamiccoordinationgames. Asimilargapemergesindurablegoodsmonopolist(Fudenberg,
Levine, andTirole,1985;Gul, Sonnenschein, andWilson,1986;Bagnoli, Salant, andSwierzbinski,
1989). SeealsorecentworkbyBattagliniandPalfrey(2024)inpublicgoodscontextwherethefact
thateachagentcaninfluencethestate(byalittle)isimportant.
17not strictly dominated, our policy fully implements the upper-bound on the time
pathofaggregateplay. Thus,privateinformationcannotdobetter. Ifinitialbeliefs
are such that action 1 is strictly dominated, however, this is more subtle.29 We
discussthiscaseinAppendixBwhereweconstructanupperboundonthepayoff
differenceunderpublicandprivateinformationpolicies.
Homogeneous vs heterogeneous players. Payoffs in our dynamic game are quite
general, with the caveat that they were identical across players. It is well-known
that introducing heterogeneity typically aids equilibrium uniqueness in coordina-
tion games.30 Thus, we expect that this can only make full implementation easier.
Since we have already closed the multiplicity gap under homogeneous payoffs,
qualitativefeaturesofourmainresultshouldcontinuetohold.31
Switching frictions. Switching frictions are commonly used to model switching
costs, inattention, or settings with some staggered structure. They are important
in our environment because dynamic information policy can then inject informa-
tion as soon as players begin deviating from the designer’s preferred action. This
allows off-path information to be chained together by correcting incipient devia-
tions. By contrast, if players could continually re-optimize their actions, then off-
pathinformationispowerlesstoruleoutequilibriaoftheform“allsimultaneously
switch to 0”.32 We note, however, that it would suffice for some frictions to exist,
buttheexactformisnotparticularlyimportant: theswitchingratecouldvarywith
aggregateplay,changeovertime,andcanbetakentobearbitrarilyquickorslow.
5 Discussion
We have shown that dynamic information is a powerful tool for full implementa-
tion in general binary-action supermodular games. In doing so, we highlighted
key properties of off-path information: asymmetric and inconclusive signals are
chained together to obtain full implementation while preserving sequential opti-
mality. Weconcludebybrieflydiscussingimplications.
Implicationsforimplementationviainformation. Arecentliteratureoninforma-
tion and mechanism design finds that in static environments, there is generically
a multiplicity gap—the designer can do strictly better under partial rather than
29Itisstillanopenquestionastohowtocharacterizefeasiblejointpathsofhigher-orderbeliefs
whenplayersalsoobservepastplay.
30SeeMorrisandShin(2006)foranarticulationandsurveyofthisidea.
31Atleastwhenthebelief-aggregateplaypairissothataction1isnotstrictlydominated.
32Indeed, prior work which obtained equilibrium uniqueness (of risk-dominant selection)
(Frankel and Pauzner, 2000; Burdzy, Frankel, and Pauzner, 2001) do so via switching frictions.
Switchingfrictionsareprevalentinmacroeconomicsbut,asAngeletosandLian(2016)note,“Itis
thensomewhatsurprisingthatthisapproach[combiningaggregateshockswithswitchingfrictions
togenerateuniqueness]hasnotattractedmoreattentioninappliedresearch.”
18full implementation (Morris, Oyama, and Takahashi, 2024; Halac, Lipnowski, and
Rappoport, 2021).33 We show that the careful design of dynamic public informa-
tioncanquitegenerallyclosethisgapindynamiccoordinationenvironments.34
But do our results demand more of players’ rationality and common knowledge
ofrationality? Yesandno. Ontheonehand,itiswell-knownthatinenvironments
likeours,thereisatightconnectionbetweentheiterateddeletionofinterimstrictly
dominatedstrategies(asinFrankel,Morris,andPauzner(2003))andbackwardin-
duction,whichcanbeviewedastheiterateddeletionofintermporallystrictlydom-
inated strategies (as in Frankel and Pauzner (2000); Burdzy, Frankel, and Pauzner
(2001)). Inthisregard,wedonotthinkourresultsrequire”moresophistication”of
agents than in static environments. On the other hand, it is also known that com-
mon knowledge of rationality is delicate in dynamic games and must continue to
hold at off-path histories.35 In this regard, our stronger results are obtained at the
priceofarguablystrongerassumptionsoncommonknowledgeofrationality.
Implications forcoordination policy. Our results have simple and sharp implica-
tionsforcoordinationproblems. Itisoftenheldthattopreventagentsfromplaying
undesirableequilbiria,policymakersmustdeliversubstantialon-pathinformation
inordertouniquelyimplementthedesigner’spreferredaction.36 Ourresultsoffer
amorenuancedview.
When public beliefs are so pessimistic that the designer-preferred action is strictly
dominated,anearlyandpreciseinjectionofon-pathinformationisindeedrequired;
waiting only makes implementation harder in the future. But as long as beliefs
are not so pessimistic that the designer-preferred action is strictly dominated, no
additional on-path information is required: silence backed by the credible promise
ofoff-pathinformationsuffices.
References
ABREU, D. AND H. MATSUSHIMA (1992): “Virtualimplementationiniterativelyundomi-
natedstrategies: completeinformation,” Econometrica: JournaloftheEconometricSociety,
993–1008.
33SeealsoInostrozaandPavan(2023);Li,Song,andZhao(2023);Morris,Oyama,andTakahashi
(2022);Halac,Lipnowski,andRappoport(2024).
34Moreover,informationinourenvironmentispublicsohigher-orderbeliefsaredegenerate;by
contrast,optimalstaticimplementationviainformationtypicallyrequiresinducingnon-degenerate
higher-orderbeliefs.
35SeeAumann(1995). Samet(2005)offersanentertainingdiscussion. Thismotivatesimplemen-
tation in ”initial rationalizability” when the designer has freedom to design the extensive form
game(Chen,Holden,Kunimoto,Sun,andWilkening,2023).
36See Morris and Yildiz (2019) for a recent articulation of this idea in static games, and Basak
and Zhou (2020, 2022) in a dynamic regime change game where the planner uses either frequent
warnings(theformer),orearlywarnings(thelatter)toimplementtheirpreferredequilibrium.
19AGHION, P., D. FUDENBERG, R. HOLDEN, T. KUNIMOTO, AND O. TERCIEUX (2012):
“Subgame-perfect implementation under information perturbations,” The Quarterly
JournalofEconomics,127,1843–1881.
ANGELETOS, G.-M., C. HELLWIG, AND A. PAVAN (2007): “Dynamic global games of
regimechange: Learning,multiplicity,andthetimingofattacks,”Econometrica,75,711–
756.
ANGELETOS, G.-M. AND C. LIAN (2016): “Incomplete information in macroeconomics:
Accommodating frictions in coordination,” in Handbook of macroeconomics, Elsevier,
vol.2,1065–1240.
ARIELI, I., Y. BABICHENKO, F. SANDOMIRSKIY, AND O. TAMUZ (2021): “Feasible joint
posteriorbeliefs,”JournalofPoliticalEconomy,129,2546–2594.
AUMANN, R. (1966): “Existence of Competitive Equilibria in Markets with a Continuum
ofTraders,”Econometrica,34,1–17.
AUMANN, R. J. (1995): “Backward induction and common knowledge of rationality,”
GamesandeconomicBehavior,8,6–19.
BAGNOLI, M., S. W. SALANT, AND J. E. SWIERZBINSKI(1989): “Durable-goodsmonopoly
withdiscretedemand,”JournalofPoliticalEconomy,97,1459–1478.
BALL, I. (2023): “Dynamic information provision: Rewarding the past and guiding the
future,”Econometrica,91,1363–1391.
BASAK, D.ANDZ.ZHOU(2020): “Diffusingcoordinationrisk,”AmericanEconomicReview,
110,271–297.
———(2022): “Panicsandearlywarnings,”PBCSF-NIFRResearchPaper.
BATTAGLINI, M. AND T. R. PALFREY (2024): “Dynamic Collective Action and the Power
ofLargeNumbers,”Tech.rep.,NationalBureauofEconomicResearch.
BEN-PORATH, E. (1997): “Rationality, Nash equilibrium and backwards induction in
perfect-informationgames,”TheReviewofEconomicStudies,64,23–46.
BESTER, H. AND R. STRAUSZ (2001): “Contracting with imperfect commitment and the
revelationprinciple: thesingleagentcase,”Econometrica,69,1077–1098.
BIGLAISER,G.,J.CRE´MER,ANDA.VEIGA(2022): “ShouldIstayorshouldIgo? Migrating
awayfromanincumbentplatform,”TheRANDJournalofEconomics,53,453–483.
BURDZY, K., D. M. FRANKEL, AND A. PAUZNER (2001): “Fast equilibrium selection by
rationalplayerslivinginachangingworld,”Econometrica,69,163–189.
CALVO, G. A. (1983): “Staggered prices in a utility-maximizing framework,” Journal of
monetaryEconomics,12,383–398.
CHAMLEY, C. (1999): “Coordinating regime switches,” The Quarterly Journal of Economics,
114,869–905.
CHEN, Y.-C., R. HOLDEN, T. KUNIMOTO, Y. SUN, AND T. WILKENING (2023): “Getting
dynamicimplementationtowork,”JournalofPoliticalEconomy,131,285–387.
CHEN, Y.-C. AND Y. SUN (2015): “Fullimplementationinbackwardinduction,”Journalof
MathematicalEconomics,59,71–76.
DASGUPTA, A.(2007): “Coordinationanddelayinglobalgames,”JournalofEconomicThe-
ory,134,195–225.
DIAMOND,D.W.ANDP.H.DYBVIG(1983): “Bankruns,depositinsurance,andliquidity,”
Journalofpoliticaleconomy,91,401–419.
DIAMOND,P.ANDD.FUDENBERG(1989): “Rationalexpectationsbusinesscyclesinsearch
equilibrium,”JournalofpoliticalEconomy,97,606–619.
DIAMOND, P. A.(1982): “Aggregatedemandmanagementinsearchequilibrium,”Journal
ofpoliticalEconomy,90,881–894.
DOVAL, L. AND J. C. ELY (2020): “Sequentialinformationdesign,”Econometrica,88,2575–
2608.
DOVAL, L. AND V. SKRETA(2022): “Mechanismdesignwithlimitedcommitment,”Econo-
metrica,90,1463–1500.
ELLISON, G. AND D. FUDENBERG(2000): “Theneo-Luddite’slament: Excessiveupgrades
inthesoftwareindustry,”TheRANDJournalofEconomics,253–272.
FARRELL, J. AND G. SALONER (1985): “Standardization, compatibility, and innovation,”
20theRANDJournalofEconomics,70–83.
FRANKEL, D. AND A. PAUZNER (2000): “Resolving indeterminacy in dynamic settings:
theroleofshocks,”TheQuarterlyJournalofEconomics,115,285–304.
FRANKEL, D. M., S. MORRIS, AND A. PAUZNER (2003): “Equilibrium selection in global
gameswithstrategiccomplementarities,”JournalofEconomicTheory,108,1–44.
FUDENBERG, D. AND D. LEVINE (1986): “Limit games and limit equilibria,” Journal of
economicTheory,38,261–279.
FUDENBERG, D., D. LEVINE, AND J. TIROLE (1985): “Infinite-horizon models of bargain-
ingwithone-sidedincompleteinformation,”Game-theoreticmodelsofbargaining,73–98.
FUDENBERG,D.ANDJ.TIROLE(1991): “PerfectBayesianequilibriumandsequentialequi-
librium,”journalofEconomicTheory,53,236–260.
GALE, D. (1995): “Dynamiccoordinationgames,”Economictheory,5,1–18.
GLAZER, J. AND M. PERRY (1996): “Virtual implementation in backwards induction,”
GamesandEconomicBehavior,15,27–32.
GOLDSTEIN, I. AND A. PAUZNER (2005): “Demand–deposit contracts and the probability
ofbankruns,”theJournalofFinance,60,1293–1327.
GUIMARAES, B. AND C. MACHADO(2018): “Dynamiccoordinationandtheoptimalstim-
uluspolicies,”TheEconomicJournal,128,2785–2811.
GUIMARAES, B., C. MACHADO, AND A. E. PEREIRA (2020): “Dynamiccoordinationwith
timingfrictions: Theoryandapplications,”JournalofPublicEconomicTheory,22,656–697.
GUL,F.,H.SONNENSCHEIN,ANDR.WILSON(1986): “Foundationsofdynamicmonopoly
andtheCoaseconjecture,”JournalofeconomicTheory,39,155–190.
HALAC, M., E. LIPNOWSKI, AND D. RAPPOPORT (2021): “Rank uncertainty in organiza-
tions,”AmericanEconomicReview,111,757–786.
———(2024): “PricingforCoordination,”.
HALAC, M. AND P. YARED (2014): “Fiscal rules and discretion under persistent shocks,”
Econometrica,82,1557–1614.
HE, Z. AND W. XIONG (2012): “Dynamic debt runs,” The Review of Financial Studies, 25,
1799–1843.
INOSTROZA, N. AND A. PAVAN(2023): “Adversarialcoordinationandpublicinformation
design,”AvailableatSSRN4531654.
KAMADA, Y. AND M. KANDORI(2020): “Revisiongames,”Econometrica,88,1599–1630.
KAMENICA, E. AND M. GENTZKOW (2011): “Bayesian persuasion,” American Economic
Review,101,2590–2615.
KOH, A., R. LI, AND K. UZUI (2024a): “Inertial Coordination Games,” arXiv preprint
arXiv:2409.08145.
KOH, A. AND S. SANGUANMOO (2022): “Attention Capture,” arXiv preprint
arXiv:2209.05570.
KOH, A., S. SANGUANMOO, AND W. ZHONG (2024b): “Persuasion and Optimal Stop-
ping,”arXivpreprintarXiv:2406.12278.
LAFFONT, J.-J. AND J. TIROLE(1988): “Thedynamicsofincentivecontracts,”Econometrica:
JournaloftheEconometricSociety,1153–1175.
LEVINE, D. K. AND W. PESENDORFER(1995): “Whenareagentsnegligible?” TheAmerican
EconomicReview,1160–1170.
LI,F.,Y.SONG,ANDM.ZHAO(2023): “Globalmanipulationbylocalobfuscation,”Journal
ofEconomicTheory,207,105575.
LIU, Q., K. MIERENDORFF, X. SHI, AND W. ZHONG (2019): “Auctions with limited com-
mitment,”AmericanEconomicReview,109,876–910.
MAKRIS, M. AND L. RENOU(2023): “Informationdesigninmultistagegames,”Theoretical
Economics,18,1475–1509.
MATHEVET, L. AND J. STEINER (2013): “Tractable dynamic global games and applica-
tions,”JournalofEconomicTheory,148,2583–2619.
MATSUI, A. AND K. MATSUYAMA (1995): “Anapproachtoequilibriumselection,”Journal
ofEconomicTheory,65,415–434.
MATSUYAMA,K.(1991): “Increasingreturns,industrialization,andindeterminacyofequi-
21librium,”TheQuarterlyJournalofEconomics,106,617–650.
MILLER,M.,P.WELLER,ANDL.ZHANG(2002): “MoralHazardandTheUSStockMarket:
Analysingthe‘GreenspanPut’,”TheEconomicJournal,112,C171–C186.
MOORE, J. AND R. REPULLO (1988): “Subgame perfect implementation,” Econometrica:
JournaloftheEconometricSociety,1191–1220.
MORRIS, S. (2020): “Notradeandfeasiblejointposteriorbeliefs,”.
MORRIS, S., D. OYAMA, AND S. TAKAHASHI (2022): “On the joint design of information
andtransfers,”AvailableatSSRN4156831.
——— (2024): “Implementation via Information Design in Binary-Action Supermodular
Games,”Econometrica,92,775–813.
MORRIS,S.ANDH.S.SHIN(2006): “Heterogeneityanduniquenessininteractiongames,”
TheEconomyasanEvolvingComplexSystem,3,207–42.
MORRIS,S.ANDM.YILDIZ(2019): “Crises: Equilibriumshiftsandlargeshocks,”American
EconomicReview,109,2823–2854.
MURPHY, K. M., A. SHLEIFER, AND R. W. VISHNY (1989): “Industrialization and the big
push,”Journalofpoliticaleconomy,97,1003–1026.
NAKAMURA, E. AND J. STEINSSON (2010): “Monetary non-neutrality in a multisector
menucostmodel,”TheQuarterlyjournalofeconomics,125,961–1013.
OYAMA, D. (2002): “p-Dominance and equilibrium selection under perfect foresight dy-
namics,”JournalofEconomicTheory,107,288–310.
PENTA,A.(2015): “Robustdynamicimplementation,”JournalofEconomicTheory,160,280–
316.
SAMET, D. (2005): “Counterfactuals in wonderland,” Games and Economic Behavior, 51,
2005.
SATO, H. (2023): “Robust implementation in sequential information design under super-
modularpayoffsandobjective,”ReviewofEconomicDesign,27,269–285.
SKRETA, V. (2015): “Optimalauctiondesignundernon-commitment,” JournalofEconomic
Theory,159,854–890.
SUN, Y.(2006): “TheexactlawoflargenumbersviaFubiniextensionandcharacterization
ofinsurablerisks,”JournalofEconomicTheory,126,31–69.
22APPENDIX TO INFORMATIONAL PUTS
Appendix A proves Theorem 1. Appendix B analyzes the case in which the de-
signercanuseprivateinformation.
APPENDIX A: PROOFS
Preliminaries. We use the following notation for the time-path of aggregate ac-
tionsfollowingfrom A : for s ≥ t, A¯ solves
t s
dA¯ = λ(1− A¯ )·ds withboundary A¯ = A .
s s t t
Similarly,for s ≥ t, A solves
s
dA = −λA ·ds withboundary A¯ = A .
s s t t
In words, A¯ and A denote future paths of aggregate actions when everyone in
s s
thefutureswitchestoactions1and0asquicklyaspossible,respectively.
Finally,itwillbehelpfultodefinetheoperatorS : H → ∆(Θ)×[0,1]mappinghis-
tories to the most recent pair of belief and aggregate action, i.e., S((µ s,A s) s≤t) :=
(µ ,A ).
t t
Outlineofproof. TheproofofTheorem1consistsofthefollowingsteps:
Step1: We first show the result for binary states Θ = {0,1} with θ∗ = 1. With
slight abuse of notation, we associate beliefs directly with the probability that the
state is 1: µ = µ (θ∗). Then, our lower-dominance region is one-dimensional and
t t
summarizedbyathresholdbeliefforeach A:
µ(A) := max µ(θ∗)
µ∈Ψ (A)
LD
We show that µ > µ(A ) implies switching to 1 is the unique subgame perfect
t t
∗
equilibriumundertheinformationpolicy µ . Weshowthisinseveralsub-steps.
• Step1A: Thereexistsabeliefthreshold,whichisa‘rightward’translationofthe
lower-dominanceregionµ(A )suchthatagentsfinditstrictlydominanttoplay
t
action 1 regardless of others’ actions if the current belief is above this threshold
(Lemma2). Wecallthisthreshold ψ (A ).
0 t
• Step1B: For n ∈ N ,supposethatagentsconjecturethatallagentswillswitchto
action 1 at all future histories H such that S(H) = (µ ,A ) fulfills µ > ψ (A ).
s s s n s
Under this assumption, we can compute a lower bound (LB) on the expected
23payoff difference for agents between playing actions 1 and 0 for any given cur-
rentbelief µ ∈ (µ(A ),ψ (A )].
t t n t
To do so, we will separately consider the future periods before and after the
aggregate action deviates from the tolerated distance from the target, at which
∗
pointnewinformationisprovided. Callthistime T .
∗
– Before T , we construct the lower bound using the fact that aggregate ac-
tionscannotbetoofarfromthetargetevenintheworst-casescenario.
∗
– At T ,thedesignerinjectsinformationwithbinarysupport. Wechoosethe
upwardjumpsize M·TOR(D)tobesufficientlylargesothat,wheneverthe
‘good signal’ realizes beliefs exceed ψ n(A T∗). Whenever the ‘bad signal’
realizes,weconjecturetheworst-casethatallagentsswitchtoaction0.
• Step1C: We show that by carefully choosing the information policy, the thresh-
oldunderwhichswitchingto1isstrictlydominant,ψ (A ),isstrictlysmaller
n+1 t
than ψ (A ). Thepolicyhasseveralkeyfeatures:
n t
– Large M: when the aggregate action A T∗ falls below the tolerated distance
TOR(D(µ t,A T∗)) fromthetarget,thehighbeliefaftertheinjectionexceeds
ψ n(A T∗), which ensures the argument in Step 1B. In particular, we choose
M tobelargerelativetotheLipschitzconstantof ψ .
n
– Small TOR(D(µ t,A T∗)): we should maintain a low tolerance level for de-
viations from the target. If the designer allowed a large deviation, the ag-
gregate action could drop so low by the time information is injected that
agents’incentivestoplayaction1wouldbetooweaktorecover.
– Large DOWN(D(µ t,A T∗)): the downward jump size should be large rela-
tive to the upward jump size M ·TOR(D(µ t,A T∗)), but not so large that
beliefs fall into the lower-dominance region. This ensures that the proba-
bilityofthebeliefbeinghighaftertheinjectionissufficientlylarge.
These three features guarantee that the lower bound (LB) is sufficiently large
and remains positive even when the current belief µ is slightly below ψ (A ).
t n t
Henceψ (A )isstrictlysmallerthanψ (A ),allowingustoexpandtherange
n+1 t n t
ofbeliefsunderwhichaction1isuniquelyoptimal(Lemma3).
• Step1D: By iterating Step 1C for n ∈ N , we show that ψ (A ) converges to
n t
µ(A ). Then, if µ > µ(A ), agents who can switch in period t find it uniquely
t t t
optimaltochooseaction1.
Step2: WeextendtheargumentsinStep1frombinarystatestofinitestates: ifµ∗ ∈/
t
Ψ LD(A t)∪Bd θ∗, then playing action 1 is the unique subgame perfect equilibrium
∗
undertheinformationpolicy µ .
24As described in the main text, our policy is such that beliefs move either in the
direction
dˆ(µ)
toward δ θ∗, or in the direction
−dˆ(µ)
away from δ θ∗. The key obser-
vationisthatwecanapplyamodificationofStep1toeachdirection.
Step3: Weestablishsequentialoptimality:
• Step3A: for any ϵ > 0, µ∗ is ϵ-sequentially optimal when µ∗ ∈ Ψ (A )∪
t LD t
Bd θ∗
• Step3B: µ∗ issequentiallyoptimalwhen µ∗
t
∈/ Ψ LD(A t)∪Bd θ∗.
ProofofTheorem1.
Step1. Suppose that Θ = {0,1} and θ∗ = 1. With slight abuse of notation, we
associate beliefs µ with the probability that θ = 1. As in the main text, we let
t
µ(A ) denote the boundary of the lower-dominance region. We will show that as
t
long as action 1 is not strictly dominated i.e., µ∗ > µ(A ), then action 1 is played
t t
underanysubgameperfectequilibrium.
Definition 5. For n ∈ N , we will construct a sequence (ψ ) where ψ ⊂ ∆(Θ)×
n n n
[0,1] is a subset of the round-n dominance region. ψ will satisfy the following
n
conditions:
(i) Contagion. Action1isstrictlypreferredundereveryhistoryHwhereS(H) ∈
′
ψ under the conjecture that action 1 is played under every history H such
n
that S(H′) ∈ ψ .
n−1
(ii) Translation. Thereexistsaconstantc > 0suchthatψ = {(µ,A) : D(µ,A) ≥
n n
c },where D(µ,A) = µ−µ(A).
n
Weinitialize ψ astheupper-dominanceregionwhereby1isstrictlydominant.
0
Observe also that since ∆ u(·,θ) is continuous and strictly increasing on a compact
domain, it is also Lipschitz and we let the constant be L > 0. This also implies
the lower-dominance region (as a function of A) is Lipschitz continuous, and we
denotetheconstantwith L .
µ
Lemma1. µ(·) isLipschitzcontinuous.
ProofofLemma1. Fix any t. The expected payoff difference betweenplaying 1 and
0wheneveryoneinthefutureswitchestoaction1isgivenby
(cid:20)(cid:90)
τ (cid:110) (cid:111)
(cid:21)
∆ U(µ ,A ) := µ E e−r(s−t) ∆ u(A¯ ,1)−∆ u(A¯ ,0) ds
t t t τ s s
s=t
(cid:20)(cid:90)
τ
(cid:21)
+E e−r(s−t)∆ u(A¯ ,0)ds .
τ s
s=t
25∆
Note that U is continuously differentiable and strictly increasing in both µ and
t
∆
A . Sincethedomainof U iscompact,thefollowingvaluesarewell-defined:
t
∆ ∆
∂ U ∂ U
L := max > 0, l := min > 0.
A,µ ∂A A,µ ∂µ
Then,forany A < A′ and µ > µ′ ,wehave
t t t t
∆ U(µ ,A )−∆ U(µ′ ,A′) ≥ −L(A′ − A )+l(µ −µ′)
t t t t t t t t
becausethemeanvaluetheoremimplies
∆ U(µ ,A )−∆ U(µ′ ,A ) ≥ l(µ −µ′)
t t t t t t
∆ U(µ′ ,A′)−∆ U(µ′ ,A ) ≤ L(A′ − A ).
t t t t t t
Substituting µ = µ(A ) and µ′ = µ(A′) intotheaboveinequalityyields
t t t t
0 = ∆ U(µ(A ),A )−∆ U(µ(A′),A′) ≥ −L(A′ − A )+l(µ(A )−µ(A′)),
t t t t t t t t
where the equality follows from the definition of µ, i.e., ∆ U(µ(A ),A ) = 0 for
t t
every A . Hence,wehave
t
L
µ(A )−µ(A′) ≤ (A′ − A ).
t t t t
l
(cid:124)(cid:123)(cid:122)(cid:125)
=:Lµ
Step1A.Construct ψ .
0
Define ψ as
0
(cid:110) (cid:111)
ψ = (µ,A) ∈ ∆(Θ)×[0,1] : D(µ,A) ≥ c ,
0 0
with c := max µ¯(A)−µ(A),where µ¯(A) isdefinedas
0 A
(cid:110) (cid:104)(cid:90) (cid:105) (cid:104)(cid:90) (cid:105)(cid:111)
µ¯(A) := min µ ∈ ∆(Θ) : E u(1,A ,θ)ds ≥ E u(0,A ,θ)ds .
s s
t t
µ¯(A) is the minimum belief under which players prefer action 1 even if all future
playerschoosetoplayaction0.
Lemma2. Action1isstrictlypreferredundereveryhistory H where S(H) ∈ ψ .
0
ProofofLemma2. Fix any history H such that S(H) ∈ ψ . Then, by the definition
0
26of ψ ,thecurrent (µ,A) satisfies
0
(cid:110) (cid:111)
µ ≥ µ(A)+max µ¯(A′)−µ(A′) ≥ µ¯(A).
A′
Hence,action1isstrictlypreferredregardlessofothers’futureplay.
Step1B.Constructalowerboundfortheexpectedpayoffdifferencegiven ψ .
n
Supposethateveryoneplaysaction1foranyhistories H′ suchthatS(H′) = (µ,A)
is in the round-n dominance region ψ . To obtain ψ in Step 1C, we derive the
n n+1
lowerboundontheexpectedpayoffdifferenceofplaying0and1given ψ .
n
To this end, fix any history H with the current target aggregate action Z such that
t
S(H) = (µ ,A ) ∈/ ψ but µ > µ(A ). From our construction of Z , we must
t t n t t t
have Z
t
− A
t
< TOR(D(µ t,A t)).37 Foranycontinuouspath (A s) s≥t,wedefinethe
hittingtime T∗((A s) s≥t) asfollows:
(cid:110) (cid:111)
T∗ = inf s ≥ t : Z − A ≥ TOR(D(µ ,A )) or (µ ,A ) ∈ ψ .
s s t s t s n
∗
T represents the first time at which either the designer injects new information,
or the pair (µ ,A ) enters the round-n dominance region. We will calculate the
s s
agent’s expected payoff before time T∗ given the continuous path (A ) and
s s∈[t,T∗]
findalowerboundforthispayoffbyusingthelowerboundof A for s ≥ t.
s
∗ ∗
Before time T . First, we calculate the agent’s payoff before time T . Given
(A s) s≥t, we have µ
s
= µ
t
and (µ t,A s) ∈/ ψ
n
for every s ∈ [t,T∗) because no in-
formation is injected when Z − A < TOR(D(µ ,A )). Define ψ (A ) = sup{µ ∈
s s t s n t
∆(Θ) : (µ,A ) ∈/ ψ }. Thisimplies
t n
TOR(D(µ ,A )) = TOR(µ −µ(A )) ≤ TOR(ψ (A )−µ(A ))
t s t s n s s
= TOR(ψ (A )−µ(A )), (1)
n t t
wheretheinequalityfollowsfrom δ beingincreasing,andthelastequalityfollows
from the property that ψ (A ) is a translation of µ(A ). Let A¯ = A¯(A ,s − t),
n t t s t
which is the aggregate play at s ≥ t when everyone will switch to action 1 as fast
as possible. By the definition of Z, we must have Z = A¯ for every s ∈ [t,T∗)
s s
because Z − A < TOR(D(µ ,A )). Then we can write down the lower bound of
s s s s
A when s ∈ [t,T∗] asfollows:
s
A ≥ A¯ −TOR(D(µ ,A )) ≥ A¯ −TOR(ψ (A )−µ(A )),
s s t s s n t t
37Byconstruction,if Z t−−A
t
≥ TOR(D(µ t,A t)), Z
t
= A
t
musthold,whichimplies Z t−A
t
=
0 < TOR(D(µ t,A t)). If Z t− − A t < TOR(D(µ t,A t)), Z t − A t < TOR(D(µ t,A t)) is immediate
becauseZ doesnotjump.
t
27where the second inequality follows from (1). By Lipschitz continuity of ∆ u(·,θ),
wemusthave
∆ u(A ,θ) ≥ ∆ u(A¯ ,θ)−TOR(ψ (A )−µ(A ))L (2)
s s n t t
withtheLipschitzconstant L. Thus,theexpectedpayoffdifferenceoftakingaction
1and0attime (µ ,A ) givenacontinuouspath (A ) beforetime T∗ is:
t t s s∈[t,T∗]
(cid:20)(cid:90) τ∧T∗ (cid:12) (cid:21)
E e−r(s−t)∆ u(A ,θ)ds(cid:12)(A )
τ s (cid:12) s s∈[t,T∗]
s=t
(cid:20)(cid:90) τ∧T∗ (cid:21)
= E e−r(s−t)∆ u(A¯ ,θ)ds
τ s
s=t
(cid:20)(cid:90) τ∧T∗ (cid:12) (cid:21)
+E e−r(s−t)(cid:0)∆ u(A ,θ)−∆ u(A¯ ,θ)(cid:1) ds(cid:12)(A )
τ s s (cid:12) s s∈[t,T∗]
s=t
(cid:20)(cid:90) τ∧T∗ (cid:21)
≥ E e−r(s−t)∆ u(A¯ ,θ)ds
τ s
s=t
(cid:20)(cid:90) τ∧T∗ (cid:12) (cid:21)
−TOR(ψ (A )−µ(A ))L·E e−r(s−t) ds(cid:12)(A ) (From(2))
n t t τ (cid:12) s s∈[t,T∗]
s=t
≥ E
(cid:20)(cid:90) τ∧T∗
e−r(s−t)∆ u(A¯
,θ)ds(cid:21)
−
TOR(ψ n(A t)−µ(A t))L
, (3)
τ s
s=t λ
wherethelastinequalityfollowsfrom
(cid:20)(cid:90) τ∧T∗ (cid:12) (cid:21) (cid:20)(cid:90) τ (cid:21) 1
E e−r(s−t) ds(cid:12)(A ) ≤ E ds = .
τ (cid:12) s s∈[t,T∗] τ
s=t s=0 λ
∗
After time T . We calculate the lower bound of the expected payoff difference
after time T∗ . We know that µ T∗− = µ t. From the definition of T∗ , we consider the
followingtwocasesdependingonwhether Z T∗ − A T∗ < TOR(D(µ T∗,A T∗)) holds
ornot.
Case1: Z T∗ − A T∗ < TOR(D(µ T∗,A T∗)). Thismeansµ T∗ = µ t becausenoinforma-
tion has been injected until T∗ . Then the definition of T∗ implies (µ T∗,A T∗) ∈ ψ n,
whereψ istheround-ndominanceregion. Thismeanseveryagentstrictlyprefers
n
∗
totakeaction1atT . Thisincreases A T∗,inducingeveryagenttakingaction1after
time T∗ .38 Thus,for s ≥ T∗ ,wehave
A s = A¯(A T∗,s−T∗) ≥ A¯(A¯ T∗ −TOR(ψ n(A t)−µ(A t)),s−T∗)
≥ A¯ −TOR(ψ (A )−µ(A )), (4)
s n t t
38If(µ,A) ∈ ψ ,then(µ,A′) ∈ ψ holdsforany A′ ≥ A.
n n
28wherethefirstinequalityfollowsfrom
A T∗ ≥ A¯ T∗ −TOR(D(µ t,A T∗)) ≥ A¯ T∗ −TOR(ψ n(A t)−µ(A t)),
andthesecondinequalityfollowsfrom
A¯(A¯
T∗
−TOR(ψ n(A t)−µ(A t)),s−T∗)
(cid:16) (cid:17)
= 1− 1− A¯
T∗
+TOR(ψ n(A t)−µ(A t)) exp(−λ(s−T∗))
= A¯ −TOR(ψ (A )−µ(A ))exp(−λ(s−T∗))
s n t t
≥ A¯ −TOR(ψ (A )−µ(A )).
s n t t
Hence,bytheLipschitzcontinuityof ∆ u(·,θ),if (µ T∗,A T∗) ∈ ψ n,then
∆ u(A ,θ) ≥ ∆ u(A¯ ,θ)−TOR(ψ (A )−µ(A ))L. (5)
s s n t t
Theexpectedpayoffdifferenceoftakingaction1and0attime(µ ,A )givenapath
t t
(A ) aftertime T∗ is
s s∈[t,T∗]
(cid:20)(cid:90)
τ (cid:12)
(cid:21)
E e−r(s−t)∆ u(A ,θ)ds(cid:12)(A )
τ s (cid:12) s s∈[t,T∗]
s=τ∧T∗
(cid:20)(cid:90)
τ
(cid:21)
= E e−r(s−t)∆ u(A¯ ,θ)ds
τ s
s=τ∧T∗
(cid:20)(cid:90)
τ (cid:12)
(cid:21)
+E e−r(s−t)(cid:0)∆ u(A ,θ)−∆ u(A¯ ,θ)(cid:1) ds(cid:12)(A )
τ s s (cid:12) s s∈[t,T∗]
s=τ∧T∗
(cid:20)(cid:90)
τ
(cid:21)
≥ E e−r(s−t)∆ u(A¯ ,θ)ds
τ s
s=τ∧T∗
(cid:20)(cid:90)
τ (cid:12)
(cid:21)
−TOR(ψ (A )−µ(A ))L·E e−r(s−t) ds(cid:12)(A ) (From(5))
n t t τ (cid:12) s s∈[t,T∗]
s=τ∧T∗
≥ E
(cid:20)(cid:90) τ
e−r(s−t)∆ u(A¯
,θ)ds(cid:21)
−
TOR(ψ n(A t)−µ(A t))L
. (6)
τ s
s=τ∧T∗ λ
Case 2: Z T∗ − A T∗ ≥ TOR(D(µ T∗,A s)). By the definition of T∗ , information is in-
∗ ∗
jectedat T ,andthusthebeliefat T mustbe
(cid:40)
µ
t
+ M·TOR(D(µ t,A T∗)) w.p. p +(µ t,A T∗)
µ T∗ =
µ
t
−DOWN(D(µ t,A T∗)) w.p. p −(µ t,A T∗).
Note that, if (µ T∗,A T∗) ∈ ψ n, then everyone strictly prefers to take action 1 at T∗ .
∗
This increases A T∗ and induces every agent to take action 1 after time T because
(µ ,A ) stays in ψ for all s ≥ T∗ . Hence, we can write down the lower bound of
s s n
29A when s > T∗ asfollows:
s
A
s
≥ 1{(µ T∗,A T∗) ∈ ψ n}A¯(A T∗,s−T∗)+1{(µ T∗,A T∗) ∈/ ψ n}A(A T∗,s−T∗)
≥ 1{(µ T∗,A T∗) ∈ ψ n}{A¯
s
−TOR(ψ n(A t)−µ(A t))}
+1{(µ T∗,A T∗) ∈/ ψ n}A(A T∗,s−T∗),
where the first inequality follows from the fact that everyone in the future will
switch to action 0 in the worst-case scenario if (µ
T∗,AT∗
) ∈/ ψ n, and the second
inequality follows from (4). By Lipschitz continuity of ∆ u(·,θ), we must have, if
(µ T∗,A T∗) ∈ ψ n,then
∆ u(A ,θ) ≥ ∆ u(A¯ ,θ)−TOR(ψ (A )−µ(A ))L, (7)
s s n t t
andif (µ T∗,A T∗) ∈/ ψ n,then
∆ u(A ,θ) ≥ ∆ u(A¯ ,θ)−L(A¯ − A ) ≥ ∆ u(A¯ ,θ)−L. (8)
s s s s s
Define p
n
:= P((µ T∗,A T∗) ∈ ψ
n
| (A s) s∈[t,T∗]). The expected payoff difference of
takingaction1and0attime (µ ,A ) givenapath (A ) aftertime T∗ is
t t s s∈[t,T∗]
(cid:20)(cid:90)
τ (cid:12)
(cid:21)
E e−r(s−t)∆ u(A ,θ)ds(cid:12)(A )
τ s (cid:12) s s∈[t,T∗]
s=τ∧T∗
(cid:20)(cid:90)
τ
(cid:21)
= E e−r(s−t)∆ u(A¯ ,θ)ds
τ s
s=τ∧T∗
(cid:20)(cid:90)
τ (cid:12)
(cid:21)
+E e−r(s−t)(cid:0)∆ u(A ,θ)−∆ u(A¯ ,θ)(cid:1) ds(cid:12)(A )
τ s s (cid:12) s s∈[t,T∗]
s=τ∧T∗
(cid:20)(cid:90)
τ
(cid:21)
≥ E e−r(s−t)∆ u(A¯ ,θ)ds
τ s
s=τ∧T∗
(cid:20)(cid:90)
τ (cid:12)
(cid:21)
−(cid:0) TOR(ψ (A )−µ(A ))Lp +L(1− p )(cid:1) ·E e−r(s−t) ds(cid:12)(A )
n t t n n τ (cid:12) s s∈[t,T∗]
s=τ∧T∗
(From(7)and(8))
≥ E (cid:20)(cid:90) τ e−r(s−t)∆ u(A¯ ,θ)ds(cid:21) − TOR(ψ n(A t)−µ(A t))Lp n +L(1− p n) .
τ s
s=τ∧T∗ λ
(9)
∗
Combining before and after time T . We are ready to construct a lower bound of
the expected discounted payoff difference. To evaluate s ≥ T∗ , it is sufficient to
focusonthecaseinwhichinformationisinjected(Case2)since(9)issmallerthan
(6) because TOR(ψ (A )− µ(A )) < 1. By taking the sum of the payoffs before
n t t
∗
andaftertimeT ,thatis(3)and(9),theexpectedpayoffdifferenceoftakingaction
301and0at (µ ,A ) givenapath (A ) islower-boundedasfollows:
t t s s∈[t,T∗]
(cid:104) (cid:12) (cid:105)
E U 1(µ t,(A s) s≥t)−U 0(µ t,(A s) s≥t)(cid:12) (cid:12)(A s)
s∈[t,T∗]
≥ E
(cid:20)(cid:90) τ
e−r(s−t)∆ u(A¯
,θ)ds(cid:21)
−
TOR(ψ n(A t)−µ(A t))L(1+ p n)+L(1− p n)
.
τ s
s=t λ
(LB)
Intuitively, the expected payoff cannot be too low compared to the case where
everyone switches to action 1 in the future because (i) aggregate actions are close
tothetargetbeforenewinformationisinjected;and(ii)ifthebeliefjumpsupward
uponinjection,everyonewillsubsequentlyswitchtoaction1.
Step1C. Finally, we characterize ψ . The following lemma establishes that un-
n+1
∗
der µ , ψ isstrictlyincreasinginthesetorder.
n
Lemma3. Forall n ∈ N , ψ ⊂ ψ (strictinclusion).
n n+1
ProofofLemma3. Tocharacterizeψ ,wefirstshowthatthereexisttolerancelevel
n+1
δ, upward jump magnitude M, and downward jump size ϵ such that if µ ≥
t
ψ(A t)− M·TOR(D(µ t,A t))/2,thenU 1(µ t,(A s) s≥t)−U 0(µ t,(A s) s≥t) > 0.
Suppose µ ≥ ψ(A )− M·TOR(D(µ ,A ))/2. First, we evaluate the first term of
t t t t
(LB). Weknowfromthedefinitionofthelower-dominanceregion µ(A ) that
t
(cid:20)(cid:90)
τ
(cid:21) (cid:20)(cid:90)
τ
(cid:21)
µ(A )E e−r(s−t)∆ u(A¯ ,1)ds +(1−µ(A ))E e−r(s−t)∆ u(A¯ ,0)ds ≥ 0
t τ s t τ s
s=t s=t
(cid:124) (cid:123)(cid:122) (cid:125)
≥0
withequalitywhenµ(A ) > 0. Ifµ(A ) > 0,wemusthaveE (cid:2)(cid:82)τ e−r(s−t)∆ u(A¯ ,1)ds(cid:3) ≤
t t τ s=t s
0,whichimplies
(cid:20)(cid:90)
τ
(cid:21)
E e−r(s−t)∆ u(A¯ ,θ)ds
τ,θ∼µt s
s=t
(cid:20)(cid:90)
τ
(cid:21)
= (µ −µ(A ))E e−r(s−t)(∆ u(A¯ ,1)−∆ u(A¯ ,0))ds
t t τ s s
s=t
(cid:20)(cid:90)
τ
(cid:21)
≥ (µ −µ(A ))E e−r(s−t)∆ u(A¯ ,1)ds
t t τ s
s=t
> C(µ −µ(A )) (10)
t t
31forsome C > 0. Thisconstant C existsbecause
(cid:20)(cid:90)
τ
(cid:21)
min E e−r(s−t)∆ u(A¯ ,1)ds > 0
τ s
At∈[0,1] s=t
since∆ u(A,1) > 0forany A ∈ [0,1]. Ifµ(A ) = 0,wehaveE (cid:2)(cid:82)τ e−r(s−t)∆ u(A¯ ,1)ds(cid:3) ≥
t τ s=t s
0,whichimplies
(cid:20)(cid:90)
τ
(cid:21) (cid:20)(cid:90)
τ
(cid:21)
E e−r(s−t)∆ u(A¯ ,θ)ds ≥ µ E e−r(s−t)∆ u(A¯ ,1)ds > C(µ −µ(A )).
τ,θ∼µt s t τ s t t
s=t s=t
Additionally, note that if δ satisfies M·TOR(D(µ ,A ))/2 ≤ µ for every (µ ,A ),
s s s s s
then µ −µ(A ) ≥ 1(ψ (A )−µ(A )). Thisfollowsfrom
t t 2 n t t
1 1 1
(ψ (A )−µ(A )) ≤ (µ −µ(A ))+ (M·TOR(D(µ ,A ))/2−µ(A ))
n t t t t t t t
2 2 2
< µ −µ(A ),
t t
where the first inequality follows from µ ≥ ψ(A )− M·TOR(D(µ ,A ))/2, and
t t t t
the second inequality follows from M · TOR(D(µ ,A ))/2 ≤ µ . Thus, if µ ≥
t t t t
ψ (A )− M·TOR(D(µ ,A ))/2,then
n t t t
(cid:20)(cid:90)
τ
(cid:21)
C
E e−r(s−t)∆ u(A¯ ,θ)ds > (ψ (A )−µ(A )). (11)
τ,θ∼µt
s=t
s
2
n t t
Next,weevaluatethesecondtermof(LB). Noticethat,if (µ ,A ) ∈/ ψ ,then
t t n
p
n
= P((µ T∗,A T∗) ∈ ψ n) = p +(µ t,A T∗)1{(µ
t
+ M·TOR(D(µ t,A T∗)),A T∗) ∈ ψ n}.
We will show that (µ
t
+ M ·TOR(D(µ t,A T∗)),A T∗) ∈ ψ
n
if µ
t
≥ ψ n(A t) − M ·
TOR(D(µ ,A ))/2. Observethatwhen (µ ,A ) ∈/ ψ ,wemusthave
t t t t n
A T∗ > A t −TOR(D(µ t,A t)).
To see this, suppose for a contradiction that A T∗ ≤ A t −TOR(D(µ t,A t)), which
implies A T∗ < A t. However, since the definition of T∗ implies A T∗ = Z T∗ −
TOR(D(µ t,A T∗)),wehave
A T∗ = Z T∗ −TOR(D(µ t,A T∗))
> A −TOR(D(µ ,A )),
t t t
wheretheinequalityfollowsfromZ T∗ = A¯ T∗ > A tandthefactthatTOR(D(µ t,A))
isincreasingin A. Thisisacontradiction.
32Lemma 1 shows that µ is a Lipschitz function. Since ψ (A ) is a translation of
n t
µ(A ),ψ (A )hasthesameLipschitzconstantL asµ(A ). Hence,ifµ ≥ ψ (A )−
t n t µ t t n t
M·TOR(D(µ ,A ))/2,wemusthave
t t
µ
t
+ M·TOR(D(µ t,A T∗)) ≥ ψ n(A t)+ M·TOR(D(µ t,A t))/2
(cid:0) (cid:1)
> ψ n(A T∗)−L µTOR(D(µ t,A t)) + M·TOR(D(µ t,A t))/2
= ψ n(A T∗),
bysetting M = 2L µ. Thus, (µ
t
+ M·TOR(D(µ t,A T∗)),A T∗) ∈ ψ
n
holds,implying
p
n
= p +(µ t,A T∗) = DOWN(D(µD t,O AW T∗N ))(D +( Mµ t, ·A TT O∗ R))
(D(µ t,A
T∗)).
Weset
µ −µ(A ) λC(µ −µ(A ))
t t t t
DOWN(D(µ ,A )) = & TOR(D(µ ,A )) = δ¯· ,
t t 2 t t 4L+4LM(µ −µ(A ))−1
t t
forafixedsmallnumberδ¯ < 1sothatTOR(D(µ,A)) < 1andM·TOR(D(µ ,A ))/2 ≤
s s
µ forevery µ and A (e.g., δ¯ = min{1, 4L, 4LM}). Thus,
s λC λC
1− p =
M·TOR(D(µ t,A T∗))
n DOWN(D(µ t,A T∗))+ M·TOR(D(µ t,A T∗))
≤
M·TOR(D(µ t,A T∗))
DOWN(D(µ t,A T∗))
δ¯λMC
=
2L+2LM(µ
t
−µ(A T∗))−1
δ¯λMC
≤ , (12)
2L+2LM(ψ (A )−µ(A ))−1
n t t
wherethelastinequalityfollowsfromthecontinuityof A andwhatwearguedin
s
(1)that µ −µ(A ) ≤ ψ (A )−µ(A ) forevery s < T∗ .
t s n t t
Thus,if µ ≥ ψ (A )− M·TOR(D(µ ,A ))/2,wehave
t n t t t
E[U 1(µ t,(A s) s≥t)−U 0(µ t,(A s) s≥t) | (A s) s∈[t,T∗]]
C 1(cid:16) (cid:17)
> (ψ (A )−µ(A ))− TOR(ψ (A )−µ(A ))L(1+ p )+L(1− p )
n t t n t t n n
2 λ
(From(LB)and(11))
C L(cid:16) (cid:17)
> (ψ (A )−µ(A ))− 2TOR(ψ (A )−µ(A ))+(1− p )
n t t n t t n
2 λ
33C δ¯L (cid:18) λC(ψ n(A t)−µ(A t))+λMC (cid:19)
≥ (ψ (A )−µ(A ))− · (From(12))
2 n t t λ 2L+2LM(ψ (A )−µ(A ))−1
n t t
C(1−δ¯)
= (ψ (A )−µ(A ))
n t t
2
> 0,
foreverygivenpath (A ) .
s s∈[t,T∗]
Inconclusion,wefoundδ, M,andϵsuchthatifµ ≥ ψ (A )−M·TOR(D(µ ,A ))/2,
t n t t t
then the agent must choose action 1. Note that δ is increasing in µ and increasing
t
in A . Thus, µ + M ·TOR(D(µ ,A ))/2 is increasing in µ and continuous in µ
t t t t t t
when µ > µ(A ). Therefore,foreach A ,thereexists µ′(A ) < ψ (A ) suchthat
t t t t n t
M·TOR(D(µ′(A ),A ))
µ′(A )+ t t = ψ (A ).
t n t
2
Thenwedefine
ψ = {(µ ,A ) : µ ≥ µ′(A )},
n+1 t t t t
which also implies ψ (A ) := sup{µ ∈ ∆(Θ) : (µ,A ) ∈/ ψ } = µ′(A ). From
n+1 t t n+1 t
the argument above, we must have an agent always choosing action 1 whenever
(µ ,A ) ∈ ψ (Contagion in Definition 5). Moreover, we can rewrite the above
t t n+1
equationasfollows:
M·TOR(ψ (A )−µ(A ))
n+1 t t
(ψ (A )−µ(A ))+ = ψ (A )−µ(A ),
n+1 t t n t t
2
wheretheRHSisconstantin A bythetranslationpropertyofψ . Thus,ψ (A )−
t n n+1 t
µ(A ) must be also constant in A (Translation in Definition 5). This concludes
t t
that round-(n + 1) dominance region ψ satisfies ψ ⊂ ψ because c =
n+1 n n+1 n
ψ (A )−µ(A ) > ψ (A )−µ(A ) =: c .
n t t n+1 t t n+1
Step1D.Inthelimit,thesequence(ψ ) coversthe(µ,A)regionwhereaction1is
n n
notstrictlydominated.
Lemma4.
(cid:110) (cid:111)
(cid:91) ψ = (µ,A) ∈ ∆(Θ)×[0,1] : µ > µ(A) .
n
n∈N
ProofofLemma4. Recall ψ (A ) = sup{µ ∈ ∆(Θ) : (µ,A ) ∈/ ψ }. By Lemma 3,
n t t n
ψ n(A t)isdecreasinginn. Defineψ∗(A t) = lim n→∞ψ n(A t). Inlimit,wemusthave
ψ∗(A )+ M·TOR(D(ψ∗(A ),A ))/2 = ψ∗(A )
t t t t
⇒ TOR(D(ψ∗(A ),A )) = 0 ⇒ ψ∗(A ) = µ(A ),
t t t t
34whichimplies
(cid:110) (cid:111)
(cid:91)
ψ = (µ ,A ) : µ > µ(A )
n t t t t
n≥0
asrequired.
Step2.Wehaveconstructedaninformationpolicywhichuniquelyimplementsan
equilibrium achieving (OPT) for |Θ| = 2. we now lift this to the case with finite
states Θ = {θ ,...θ } as set out in the main text, where recall we set θ∗ as the
1 n
dominantstate.
In particular, we show that if µ∗
t
∈/ Ψ LD(A t)∪Bd θ∗, then playing action 1 is the
∗
unique subgame perfect equilibrium under the information policy µ . To apply
Step 1, we will construct an auxiliary binary-state environment for each direction
from δ θ∗.
Tothisend,wecallavectordˆ= (dˆ θ) θ∈Θ ∈ Rnafeasibledirectionalvectorif∑ θ dˆ θ = 0
and dˆ θ∗ = 1 but dˆ θ < 0 if θ∗ ̸= θ. For each feasible directional vector dˆ , define a
function α¯ : [0,1] → [0,1] suchthat,forevery A ∈ [0,1],
dˆ
(cid:110) (cid:111)
α¯ dˆ(A) = inf α ∈ [0,1] : δ θ∗ −(1−α)dˆ∈/ Ψ LD(A)∪Bd θ∗ .
Notethat δ θ∗ −(1−α)dˆ∈ Bd θ∗ ifandonlyif α = 0because dˆ θ∗ = 1. Observethat
(cid:0)Ψ LD(A t)∪Bd θ∗(cid:1)c = (cid:91) (cid:8) δ θ∗ −(1−α)dˆ : α ∈ (α¯ dˆ(A t)),1](cid:9) ,
dˆ∈D
whereDisthesetofallfeasibledirectionalvectors.
Thisistruebecause1)(cid:0)Ψ
(A
)(cid:1)c
LD t
isapolygonsincetheexpectationoperatorislinear;and2)Ψ LD(A t)∪Bd θ∗∆(Θ)is
closed. Thus, it is equivalent to show that, for every feasible directional vector
dˆ
,
if α ∈ (α¯ (A ),1],thenplayingaction1istheuniquesubgameperfectequilibrium
dˆ t
∗
undertheinformationpolicy µ .
Fixafeasibledirectionalvector
dˆ
.Define
∆(Θ) dˆ = (cid:8) δ θ∗ −(1−α)dˆ : α ∈ [0,1](cid:9)
as the set of beliefs whose direction from δ θ∗ is dˆ . Consider an auxiliary environ-
ment with binary state Θ˜ = {0,1}. Construct a bijection ψ : ∆(Θ) → ∆(Θ˜ )
dˆ dˆ
such that ψ dˆ(µ) = α if µ = δ θ∗ −(1−α)dˆ . Denote µ˜ := ψ dˆ(µ) ∈ ∆(Θ˜ ) for every
µ ∈ ∆(Θ) dˆ. Notethat ψ dˆ(δ θ∗) = 1.
We define a flow payoff for each player under the new environment u˜ : {0,1}×
35[0,1]×Θ˜ → R asfollows:
(cid:16) (cid:17)
u˜(a,A,θ˜) = u a,A,ψ−1(θ˜) .
dˆ
Define ∆ u˜(A,θ˜) := u(1,A,θ˜) − u(0,A,θ˜). Since ψ is a linear map, ∆ u˜(A,θ)
dˆ
is still continuously differentiable and strictly increasing in A. Also, given that
∆ u˜(0,1) = ∆ u(0,θ∗) > 0, we still have an action-1-dominance region under this
newenvironment.
Then we can similarly define the maximum belief under which players prefer ac-
tion0evenifallfutureplayerschoosetoplayaction1 :
(cid:110) (cid:104)(cid:90) (cid:105) (cid:104)(cid:90) (cid:105)(cid:111)
µ (A ) := max µ˜ ∈ ∆(Θ˜ ) : E u˜(0,A¯ ,θ˜)ds ≥ E u˜(1,A¯ ,θ˜)ds .
dˆ t s s
t t
We define D˜(µ˜,A) = µ˜ −µ (A). Then it is easy to see that D˜(µ,A) = D(µ,A) for
dˆ
every µ ∈ ∆(Θ˜ ) and A ∈ [0,1]
A key observation is that if µ t− ∈/ Ψ LD(A t) and µ t− ∈ ∆(Θ) dˆ, then every future
beliefmuststayin∆(Θ)
almostsurelywithrespecttoanystrategy. Wecanrewrite
dˆ
thetime-t informationstruturecorrespondingtothenewenvironmentasfollows:
1. Silenceon-path. If µ˜ t− > µ dˆ(A t) and |A t −Z t−| < TOR(D)
µ t = µ t− almostsurely,
i.e.,noinformation,and dZ
t
= λ(1−Z t−).
2. Noisyandasymmetricoff-path. If µ˜ t− > µ dˆ(A t) and Z t− − A t ≥ TOR(D),

µ˜ = µ˜ t− + M·TOR(D) w.p.
DOWND (DO )W +N M(D ·T)
OR(D)
t µ˜ t− −DOWN(D) w.p. DOWNM (D·T )+OR M( ·D T) OR(D),
andreset Z = A .
t t
By applying Step 1, we conclude that if µ˜ t− > µ dˆ(A t), then action 1 is played
under any subgame perfect equilibrium. The only subtlety is to verify that as in
(10),thereexistsaconstant C > 0suchthat
(cid:20)(cid:90)
τ
(cid:21)
min E e−r(s−t)∆ u˜(A¯ ,1)dt ≥ C
τ s
At∈[0,1] s=t
for any feasible directional vector dˆ . This is clear because ∆ u˜(A,1) = ∆ u(A,θ∗) >
∆ u(0,θ∗) > 0forevery A bythedefinitionof θ∗ .
36Since [0,µ (A )] = ψ (cid:0)∆(Θ) ∩ Ψ (A )(cid:1) , we have (µ (A ),1] = (α¯ (A ),1].
dˆ t dˆ dˆ LD t dˆ t dˆ t
Hence, if α ∈ (α¯ (A ),1], then playing action 1 is the unique subgame perfect
dˆ t
∗
equilibriumundertheinformationpolicy µ ,asdesired.
Step3.Wenowshowsequentialoptimality. Step3Ahandlesthecasewhenbeliefs
aresuchthat1isstrictlydominated,while3Bhandlesthecasewhen1isnotstrictly
dominated.
Step3A. µ∗ is ϵ-sequentiallyoptimalwhen µ∗ ∈ Ψ (A ).
t LD t
Fix any µ ∈ Ψ (A ). Define τ∗ := inf{t : µ ∈/ Ψ (A )} and τ¯ := inf{t : µ ∈/
0 LD 0 t LD 0 t
Ψ (A )},i.e.,τ∗ andτ¯ arethefirsttimestatwhichthebeliefµ isnotinΨ (A )
LD t t LD 0
and Ψ (A ),respectively. Thismeans,at s < τ¯,allagentswhocanswitchchoose
LD t
action 0. This pins down an aggregate action A = A(A ,t) for every t ≤ τ¯.
t 0
Therefore, A < A for every t ≤ τ¯, implying Ψ (A ) ⊂ Ψ (A ). Thus, τ∗ ≥ τ¯,
t 0 LD 0 LD t
andso A = A(A ,t) forevery t ≤ τ∗ .
t 0
Moreover, we know that A ≤ A¯(A ,s−t) for any s ≥ t, so we can find an upper
s t
boundofthedesigner’spayoffasfollows:
(cid:104) (cid:105)
Eσ ϕ(A)
= E
τ∗
[ϕ(A)1(τ∗ = ∞)+ϕ(A)1(τ∗ < ∞)]
≤ E
τ∗
[ϕ(A)1(τ∗ = ∞)+ϕ(A¯)1(τ∗ < ∞)]
= ϕ(A)+{ϕ(A¯)−ϕ(A)}P(τ∗ < ∞),
where A satisfies A = A(A ,t),and A¯ satisfies A¯ = A¯(A ,t).
t 0 t 0
Forevery t ∈ [0,∞),theoptionalstoppingtheoremimplies
µ
0
=
E(cid:2)
µ
τ∗∧t(cid:3)
= E[µ
τ∗
| τ∗ < t]P(τ∗ < t)+E[µ
t
| τ∗ ≥ t]P(τ∗ ≥ t)
≥ E[µ
τ∗
| τ∗ < t]P(τ∗ < t).
(cid:124) (cid:123)(cid:122) (cid:125)
=:µˆt
This implies µˆ ∈ F(P(τ∗ < t),µ ) for every t. By the definition of τ∗ and µ
t 0 t
is right-continuous, µ τ∗ ∈ Ψc LD(A 0) under the event {τ∗ < ∞}. Since Ψc LD(A 0)
is convex, we also have µˆ ∈ Ψc (A ). This means µˆ ∈/ Int Ψ (A ), but µˆ ∈
t LD 0 t LD 0 t
F(P(τ∗ < t),µ ). The definition of p∗(µ ,A ) implies p∗(µ ,A ) ≥ P(τ∗ < t) for
0 0 0 0 0
every t.Thus,
(cid:104) (cid:105)
Eσ ϕ(A) ≤ ϕ(A)+{ϕ(A¯)−ϕ(A)}p∗(µ ,A )
0 0
= (1− p∗(µ ,A ))ϕ(A)+ p∗(µ ,A )ϕ(A¯).
0 0 0 0
37Thisimplies
(cid:104) (cid:105)
(OPT) = sup Eσ ϕ(A) ≤ (1− p∗(µ ,A ))ϕ(A)+ p∗(µ )ϕ(A¯).
0 0 0
µ∈M
σ∈Σ(µ,A )
0
Under µ∗ , if µ ∈ Ψc (A ), then everyone takes action 1 under any equilibrium
0+ LD 0
outcomefromwearguedearlier. Thus,
(cid:104) (cid:105)
inf Eσ ϕ(A) ≥ (1− p∗(µ ,A )+η)ϕ(A)+(p∗(µ ,A )−η)ϕ(A¯).
0 0 0 0
σ∈Σ(µ∗,A )
0
Takinglimit η → 0,weobtain
(cid:104) (cid:105)
(ADV) = sup inf Eσ ϕ(A) ≥ (1− p∗(µ ,A ))ϕ(A)+ p∗(µ ,A )ϕ(A¯) ≥ (OPT).
0 0 0 0
µ∈Mσ∈Σ(µ,A 0)
Since(OPT) ≥ (ADV),weobtain(OPT) = (ADV).
Step3B.Wefinallyshowµ∗ issequentiallyoptimalwhenµ∗
t
∈/ Ψ LD(A t)∪Bd θ∗∆(Θ).
Weproceedcasewise:
• Case 1: If µ t− ∈/ Ψ LD(A t) and |A t − Z t−| < TOR(D(µ t−,A t)). In this case,
there is no information arriving, and everyone takes action 1. This will in-
crease A , and every agent always takes action 1 from time t onwards. This
t
isthebestoutcomeforthedesigner,implyingsequentialoptimality.
• Case 2: If µ t− ∈/ Ψ LD(A t) and |A t − Z t−| ≥ TOR(D(µ t−,A t)). In this case,
the belief moves to either µ t− +(M·TOR(D))·dˆ(µ t−) or µ t− −DOWN(D)·
dˆ(µ t−). Note that µ t− − DOWN(D) · dˆ(µ t−) ∈/ Ψ LD(A t) because ψ dˆ(µ t −
DOWN(D)·dˆ(µ t−)) = (1+α¯ dˆ(A t))/2 > α¯ dˆ(A t). So no matter what infor-
mation arrives, every agent takes action 1. This will increase A , and every
t
agent always takes action 1 after time t. Again, this is the best outcome for
thedesigner,implyingsequentialoptimality.
38APPENDIX B: DESIGNING PRIVATE INFORMATION
In this appendix we discuss whether the designer can do better by designing pri-
vateinformation.
Relaxedfeasibilityforjointbeliefprocesses.
We consider the relaxed problem under which each agent’s belief can be ‘sepa-
rately controlled’ i.e., any joint distribution over agents’ beliefs under which the
marginal distribution is a martingale is feasible under the relaxed problem. There
is a common prior µ and a private belief process µ := (µ ) , where µ := P(θ =
0 i it t it
1|F it) with F
it
beingagent i’stime-t filtrationgeneratedby (A s,µ is) s≤t.
The belief process for agent i ∈ [0,1], µ := (µ ) is R-feasible if it is an (F ) -
i it t it t
martingale. ThesetofjointR-feasiblebeliefprocessis
(cid:110) (cid:111)
MP := (µ ) : i ∈ [0,1], (µ ) isR-feasible .
it t it t
We emphasize that this is a necessary condition on beliefs, but is not sufficient
(see, e.g., Arieli, Babichenko, Sandomirskiy, and Tamuz (2021); Morris (2020) for a
discussion of the static case). Let the set of feasible joint belief processes be MF.
Althoughitisstillanopenquestionofhowtocharacterizethisset,weknowMF ⊆
MP.
Theproblemunderprivateinformation.
(cid:104) (cid:105)
sup inf
Eσ ϕ(cid:0) A(cid:1)
. (ADV-P)
µ∈MFσ∈PBE(µ,A 0)
noting that we have moved from subgame perfection to Perfect-Bayesian Equi-
libria (Fudenberg and Tirole, 1991) since there is now private information among
players. However, observe that M ⊆ MF and, furthermore, that BNE coincides
withSPEunderpublicinformationso(ADV-P) ≥ (ADV).
Theorem1B. Supposethat µ
0
∈/ Ψ LD(A 0)∪Bd θ∗,then
(ADV-P) = (ADV).
If µ ∈ Ψ (A ) andfurthersupposing ϕ isaconvexfunctional,then
0 LD 0
(cid:16) (cid:17)(cid:16) (cid:17) (cid:17)
(ADV-P)−(ADV) ≤ p∗(µ ,A )− p∗(µ ,1) ϕ(cid:0) Aλ −ϕ(cid:0) Aλ(cid:1) .
0 0 0
Proof. The case in which µ 0 ∈/ Ψ LD(A 0) ∪Bd θ∗ follows directly from Theorem 1
since it already attains the upper bound on the time-path of aggregate play. We
39provethesecondpartinseveralsteps.
Step1A.Constructingarelaxedproblem.
Somecareisrequiredinconstructingtherelaxedproblem: bymovingfromMF to
MP,equilibriaoftheresultantgamemightnotbewell-defined. Wewilldealwith
thisintwoways. First,wewillweakenPBEtowhatwecallnon-dominancewhich
requires that players play action 1 whenever it is not strictly dominated. Notice
thatthisisnotanequilibriumconceptandiswell-definedevenwithhetrogeneous
beliefs. Second,wewillreplacetheinnerinfwithsuptoobtaintherelaxedproblem
(cid:104) (cid:105)
sup sup
Eσ ϕ(cid:0) A(cid:1)
. (ADV-P-R)
µ∈MPσ∈ND(µ,A 0)
It is easy to see that this is indeed a relaxed problem i.e., (ADV-P-R) ≥ (ADV-P)
since(i) MP ⊇ MF andfurthermore,foreach µ ∈ MF, PBE(µ,A ) ⊆ ND(µ,A ).
0 0
Step1B.Solvingtherelaxedproblem.
Firstobservethatforeachplayeri ∈ [0,1],anecessaryconditionforaction1tonot
bestrictlydominatedis
µ > µ(A = 1)
it
Hence,considerthestrategyσinwhicheachplayeriplays1ifµ > µ(A = 1)and
it
0otherwise. Clearly,
(cid:104) (cid:105)
sup Eσ ϕ(cid:0) A(cid:1) ≥ (ADV-P-R).
µ∈MP
Let (µ ) be any Cadlag martingale and let τ := inf{t ∈ T : µ ∈/ Ψ (1)}.
it t i it LD
ClearlythisCadlagmartingaleisimprovableifitcontinuestodeliverinformation
after τ, so it is without loss to consider (µ ) which are constant a.s. after τ. But
i it t i
observethatsince(µ ) isamartingale,theprobabilityofexitingtheregionΨ (1)
it t LD
isupper-boundedwiththesamecalculation:
P(τ < +∞) ≤ p∗(µ ,1).
i 0
We define the (random) number of agents whose beliefs eventually cross µ(1) as
follows:
(cid:90)
F = 1{τ < ∞}di.
i
i∈I
Considerthat
(cid:20)(cid:90) (cid:21) (cid:90)
E [F] = E 1{τ < ∞}di = P (τ < ∞)di ≤ p∗(µ ,1).
µ µ i µ i 0
i∈I i∈I
40Now we will derive the upper bound of A for each realization of (µ ) . Agent
t it i,t
i ∈ I takesaction1attime t onlyifeither
(I) agent i’s Poisson clock ticked before t, and his belief eventually crosses µ(1),
or
(II) agent i tookaction1initially,andhisPoissonclockhasnottickedyet.
The measures of agents in (I) and (II) are F(1−exp(−λt)) and A exp(−λt), re-
0
spectively. Thus,
A ≤ F(1−exp(−λt))+ A exp(−λt)
t 0
λ λ λ λ
almost surely. Define A := (A ) as the solution to the ODE dA = λ(1− A )dt
t t t t
with boundary Aλ = A , and Aλ := (Aλ) as the solution to the ODE dAλ =
0 0 t t t
−λAλdt withboundary Aλ = A . Wehave
t 0 0
Aλ = 1−(1− A )exp(−λt), Aλ = A exp(−λt),
t 0 t 0
sowecanrewritetheupperboundof A asfollows:
t
A ≤ FAλ +(1−F)Aλ ∀t ⇒ A ≤ FAλ +(1−F)Aλ
t t t
almostsurely. Since ϕ isaconvexandincreasingfunctional,wemusthave
ϕ(A) ≤ Fϕ(Aλ )+(1−F)ϕ(Aλ)
almostsurely. Thisimplies
(cid:104) (cid:105)
E ϕ(A) ≤ E [F]ϕ(cid:0) Aλ(cid:1) +(1−E [F])ϕ(cid:0) Aλ(cid:1)
µ µ µ
≤ p∗(µ ,1)ϕ(cid:0) Aλ(cid:1) +(cid:0) 1− p∗(µ ,1)(cid:1) ϕ(cid:0) Aλ(cid:1)
0 0
forevery µ.Thus,
(ADV-P) ≤ (ADV-P-R) ≤ p∗(µ ,1)ϕ(cid:0) Aλ(cid:1) +(cid:0) 1− p∗(µ ,1)(cid:1) ϕ(cid:0) Aλ(cid:1) .
0 0
Thisimplies
(ADV-P)−(ADV) ≤ (p∗(µ ,A )− p∗(µ ,1))(cid:0) ϕ(cid:0) Aλ(cid:1) −ϕ(cid:0) Aλ(cid:1)(cid:1) ,
0 0 0
asdesired.
41ONLINE APPENDIX TO ‘INFORMATIONAL PUTS’
ANDREWKOH SIVAKORNSANGUANMOO KEIUZUI
OnlineAppendixIdevelopsTheorem1forfiniteplayers.
ONLINE APPENDIX I: FINITE PLAYERS
I.1 Preliminaries. Let A = A¯ = N−n, where n is the number of agents who
0 0 N
initially play action 0. For each i ∈ {1,...,n}, define τ ∼ Exp(λ) as an iid expo-
i
nentialdistributionwithrateλ,i.e.,τ isagenti’srandomwaitingtimeforthefirst
i
switchingopportunity. Wedefinerandomvariables A and A¯ asfollows:
t t
1 ∑n
A = A + 1{τ ≤ t}
t 0 i
N
i=1
A¯ = 1−(1− A¯ )e−λt,
t 0
where A is the proportion of agents playing action 1 at time t when everyone
t
switches to action 1 as quickly as possible, while A¯ is the auxiliary proportion of
t
agentsplayingaction1attimetwhen1−e−λt
oftheagentsinitiallyplayingaction
0havehadopportunitiestoswitchbytime t.
If the number of agents is finite, the proportion of agents playing action 1 can
deviatefromthetolerateddistancefromthetargetevenwhennoonehasswitched
to action 0. The following lemma provides an upper bound on the probability of
such“unlucky”events.
Lemma5. P(∀t,A +δ ≥ A¯ ) > 1−12δ−4N−1.
t t
Proof. Fix α such that δ = 2N−α. We rearrange (τ) as τ < τ < ··· <
i 1,...,n (1) (2)
τ . For each k ∈ {0,...,⌈nNα−1⌉−1}, define T := [τ ,τ ), where
(n) k (kN1−α) ((k+1)N1−α)
τ = τ , τ = 0, and τ = ∞ . If t ∈ T , We must have A ∈ [A +
(i) (⌊i⌋) (0) (n+1) k t 0
⌊kN1−α⌋
,A
+(k+1)N−α].Therefore,
N 0
P(∀t ≤ T,A ≥ A¯ −δ)
t t
(cid:18)⌈nNα−1⌉−1 (cid:19)
= P (cid:92) {ω : ∀t ∈ T ,A ≥ A¯ −δ}
k t t
k=0
≥
P(cid:18)⌈nNα (cid:92)−1⌉−1(cid:26)
ω : ∀t ∈ T ,A +
⌊kN1−α⌋
≥ A¯
−δ(cid:27)(cid:19)
k 0 t
N
k=0
≥
P(cid:18)⌈nNα (cid:92)−1⌉−1(cid:26)
ω : A +
kN1−α −1
≥ A¯
−δ(cid:27)(cid:19)
0 N τ ((k+1)N1−α)
k=0
1=
P(cid:18)⌈nN1 (cid:92)−α⌉−2(cid:26)
ω : A +
kN1−α −1
≥ A¯
−δ(cid:27)(cid:19)
,
0 N τ ((k+1)N1−α)
k=0
wherethelastequalityfollowsfromthatif k = ⌈nNα−1⌉−1then
kN1−α −1 N−n n− N1−α −1
A + > +
0
N N N
N1−α +1
= 1−
N
> 1−δ
> A¯ −δ.
τ ((k+1)N1−α)
Ω
Wedefinetheevent asfollows:
relax
Ω =
⌈nNα (cid:92)−1⌉−2(cid:26)
τ −τ ≤
λ−1(1+δ/3)log(cid:18) n−kN1−α (cid:19)(cid:27)
.
relax ((k+1)N1−α) (kN1−α) n−(k+1)N1−α
k=0
(cid:124) (cid:123)(cid:122) (cid:125)
Ω
k
Undertheevent Ω ,forevery k ≤ ⌈nNα−1⌉−2,wehave
relax
n
A¯ = 1− exp(−λτ )
τ ((k+1)N1−α) N ((k+1)N1−α)
n (cid:18) ∑k (cid:19)
= 1− exp −λ (τ −τ )
N
((i+1)N1−α) (iN1−α)
i=0
(cid:18) (cid:19)
≤ 1−
n
exp
(1+δ/3)(logn−log(cid:0) n−(k+1)N1−α(cid:1)
N
n
(cid:18) (k+1)N1−α(cid:19)1+δ/3
= 1− 1−
N n
n
(cid:18) (1+δ/3)(1+k)N1−α(cid:19)
≤ 1− 1−
N n
= A +(1+δ/3)(1+k)N−α
0
Notethat k < Nα. Thus,
A¯ −δ ≤ A +(1+δ)(1+k)N−α −δ
τ ((k+1)N1−α) 0
≤ A +kN−α +(1+δ/3+δk/3)N−α −δ
0
≤ A +kN−α +(1+δ/3)N−α −2δ/3
0
≤ A +kN−α −1/N,
0
2wherethelastinequalityholdsif N islargeand δ = 2N−α. Thisimplies
Ω ⊂
⌈nN1 (cid:92)−α⌉−2(cid:26)
ω : A +
kN1−α −1
≥ A¯
−δ(cid:27)
.
relax 0 N τ ((k+1)N1−α)
k=0
NowwecomputeP(Ω ). Notethat τ −τ has
relax ((k+1)N1−α) (kN1−α)
⌊(k+1) ∑N1−α⌋−1
1 1
(cid:18) ⌊n−kN1−α⌋ (cid:19)
mean = ≤ log
λ(n−i) λ ⌊n−(k+1)N1−α⌋
i=⌊kN1−α⌋
⌊(k+1)N1−α⌋ (cid:18) (cid:19)
∑ 1 1 1 1
variance = ≤ − .
λ2(n−i)2 λ2 ⌊n−kN1−α⌋ ⌊n−(k+1)N1−α⌋
i=⌊kN1−α⌋
Let a = ⌊n − kN1−α⌋. Thus, by Chebyshev inequality, the probability of Ωc is
k k
boundedaboveby
1/a −1/(a ) (cid:18) 1/a −1/a (cid:19)2 1/δ2
k k+1 ≤ k k+1 ·
δ2(loga −loga )2 loga −loga 1/a −1/a
k+1 k k+1 k k k+1
1 1
≤ ·
δ2a2
k
1/a
k
−1/a
k+1
(cid:18) (cid:19)
1 1 1
= +
δ2 a a −a
k k+1 k
1
< ·3Nα−1
δ2
for every k ≤ ⌈nNα−1⌉−2. Thus, the probability that the information triggers is
boundedaboveby
3Nα−1
4
Nα · = 3N2α−1δ−2 = 3· N−1δ−2 = 12δ−4N−1
δ2 δ2
since δ = 2N−α,asdesired.
Anothersubtletywithafinitenumberofagentsisthatagenti’sactiontodayaffects
her future decision problem, and thus she needs to account for this effect when
choosing her action today. The following lemma shows that when µ > µ(A¯ ), it is
0
optimalforhertotakeaction1regardlessofherfutureactions.
Lemma 6. For every agent i, suppose (τ ) be a increasing sequence of Poisson
in n
clocks of agent i. Suppose a ∈ {0,1} be a (random) action agent i takes at τ . If
in in
3µ > µ(A¯ ),then
0
E (cid:104) ∑∞ (cid:90) τ i,n+1 e−rsu(a ,A¯ ,θ)ds(cid:105) ≤ E (cid:104)(cid:90) ∞ e−rsu(1,A¯ ,θ)ds(cid:105)
µ in s µ s
n=0 τ in 0
Proof. Forevery n ∈ N ,considerthat
E µ(cid:104)(cid:90) τ i,n+1 e−rs∆ u(A¯ s,θ)ds(cid:105) = E µ(cid:20) e−rτ inE µ(cid:104)(cid:90) τ i,n+1−τ i,n e−rs∆ u(A¯ s+τ in,θ)ds(cid:12) (cid:12)τ in(cid:105)(cid:21)
τ 0
in
≥ E
µ(cid:20)
e−rτ inE
µ(cid:104)(cid:90) τ i,n+1−τ
i,n e−rs∆ u(A¯ s,θ)ds(cid:12) (cid:12)τ
in(cid:105)(cid:21)
0
≥ 0,
wherethelastinequalityfollowsfrom µ > µ(A¯ ). Thisimplies
0
E (cid:104)(cid:90) τ i,n+1 e−rsu(a ,A¯ ,θ)ds(cid:105) ≤ E (cid:104)(cid:90) τ i,n+1 e−rsu(1,A¯ ,θ)ds(cid:105)
µ in s µ s
τ τ
in in
forevery n ∈ N ,asdesired.
I.2 Maintheorem. Forsimplicity,weconsiderbinarystatesΘ = {0,1}withθ∗ =
1. Supposethatthereare N agentsintheeconomy. LetΣN(µ,A )denotethesetof
0
subgame perfect equilibria of the stochastic game induced by a belief martingale
µ under the economy consisting of N agents whenever A can be written as k
0 N
for some k ∈ {0,...,N}. We define the designer’s problem under adversarial
equilibriumselectionwithafinitenumberofagentsasfollows:
(cid:104) (cid:105)
sup inf
Eσ ϕ(cid:0) A(cid:1)
(ADV-N)
µ∈Mσ∈ΣN(µ,A 0)
Theorem 2. Suppose that u(a,·,θ) is Lipschitz continuous for all a ∈ {0,1} and
θ ∈ {0,1} and there exists a constant L
ϕ
such that |ϕ(A)−ϕ(A′)| ≤ L ϕ∥A− A′∥∞
forevery A,A′ ∈ [0,1]∞ . Thenthefollowingshold.
1. There exists a constant d such that, under any subgame perfect equilibrium
σ ∈ ΣN(µη,A ) ( µη defined in Theorem 1), an agent takes action 1 if µ >
0 t
µ(A )+(dN)−1/9 foreveryhistory H ,aggregateaction A ,andbelief µ ,
t t t t
2. Thereexistsaconstant C¯ suchthat,forany (µ ,A )39,wehave
0 0
|(OPT)−(ADV-N)| ≤ C¯N−1/9,
39Weimplicitlyassume A ∈QandN·A isaninteger.
0 0
4forsufficientlylarge N (dependingon (µ ,A )).
0 0
3. Sequentialoptimality:
(cid:12) (cid:12)
(cid:12) (cid:12)
lim sup lim (cid:12)
(cid:12)
inf
Eσ(cid:2) ϕ(cid:0) A(cid:1)(cid:12)
(cid:12)F
t(cid:3)
− sup inf
Eσ(cid:2) ϕ(A)(cid:12)
(cid:12)F
t(cid:3)
(cid:12)
(cid:12)
= 0.
η↓0 Ht∈HN→∞ (cid:12)σ∈ΣN(µη,A 0) µ′∈MσN∈Σ(µ′,A 0) (cid:12)
Proof of Part 1. We follow a similar method as we did in Theorem 1. We restate
Lemma3asfollows:
Lemma7. Thereexists d > 0suchthatif µ > µ(A )+(dN)−1/9, ψ ⊂ ψ holds
t t n n+1
forall n ∈ N .
ProofofLemma7. WefollowasimilarmethodaswedidinLemma2. Supposethat
everyone plays action 1 for any histories H′ such that S(H′) = (µ,A) is in the
round-n dominance region ψ . To obtain ψ , we derive the lower bound on the
n n+1
expectedpayoffdifferenceofplaying0and1given ψ .
n
Fix any history H with the current target aggregate action Z such that S(H) =
t
(µ t,A t) ∈/ ψ n. FromourconstructionofZ t,wehaveZ t−−A t− < TOR(D(µ t−,A t−)).
Foranypath (A s) s≥t withanincrementatmost N1,wedefinea(deterministic)hit-
tingtime T∗((A s)) s≥t asfollows:
T∗ = inf{s ≥ t : Z − A ≥ TOR(D(µ ,A )) or (µ ,A ) ∈ ψ }.
s s t s t s n
Fix T∗ ,andwefirstdetermineabehaviorofthepath (A s) s≥t given T∗ .
Before time T∗ . For any s ∈ [t,T∗), we showed in (1) that TOR(D(µ ,A )) ≤
t s
TOR(ψ (A ) − µ(A )). By the definition of Z, we must have Z = A¯ for every
n t t s s
s ∈ [t,T∗)because Z − A < TOR(D(µ ,A ))foreverys < T∗ . Thenwecanwrite
s s t s
downthelowerboundof A when s ∈ [t,T∗) asfollows:
s
A ≥ A¯ −TOR(D(µ ,A )) ≥ A¯ −TOR(ψ (A )−µ(A )), (13)
s s t s s n t t
∗
almostsurelygiven T .
Aftertime T∗ . Fixany s > T∗ . Weconsiderthefollowingtwocases.
Case 1: Z T∗ − A T∗ < TOR(D(µ t,A T∗)). This means µ T∗ = µ t because no informa-
∗ ∗
tion has been injected until T . Then the definition of T and the right continuity
of Z
s
and A
s
imply(µ T∗,A T∗) ∈ ψ n.Thismeanseveryagentstrictlypreferstotake
∗
action 1 at T . This increases A , inducing every agent taking action 1 after time
s
T∗ untiltimes′ atwhichinformationisinjected(i.e., Z s′ − A s′ > TOR(D(µ t,A s′))).
∗
The event that no information is injected again after time T is equivalent to the
5eventthat Z − A ≤ TOR(D(µ ,A )) forevery s > T∗ . Observethat
s s t s
(cid:16) (cid:17)
P ∀s > T∗ ,Z
s
− A
s
≤ TOR(D(µ t,A s))(cid:12) (cid:12)T∗
(cid:16) (cid:17)
≥ P ∀s > T∗ ,Z
s
− A
s
≤ TOR(D(µ t,A t))(cid:12) (cid:12)T∗
(cid:16) (cid:17)
≥ P ∀s > T∗ ,A¯
s
− A
s
≤ TOR(D(µ t,A t))(cid:12) (cid:12)T∗ ,
where the first inequality follows from A ≥ A , and A¯ is defined as A¯ = 1−
s t s s
(1− A T∗)e−λ(s−T∗) for every s > T∗ . By the definition of Z s, A¯
s
≥ Z
s
holds, which
impliesthesecondinequality. FromLemma5,weknowthat
(cid:16) (cid:17)
P ∀s > T∗ ,A¯
s
− A
s
≤ TOR(D(µ t,A t))(cid:12) (cid:12)T∗ ≥ 1−12N−1TOR(D(µ t,A t))−4.
Therefore,wecanwritedownthelowerboundof A when s > T∗ underthiscase
s
asfollows:
∀s > T∗ ,A ≥ A¯ −TOR(D(µ ,A )) ≥ A¯ −TOR(ψ (A )−µ(A ))
s s t s s n t t
withprobabilityatleast1−12N−1TOR(D(µ
,A
))−4.
t t
Case 2: Z T∗ − A T∗ ≥ TOR(D(µ t,A T∗)). In this case, information is injected at T∗ .
Note that, if (µ T∗,A T∗) ∈ ψ n, then everyone prefers to take action 1 at T∗ . This
∗ ′
increases A , inducing every agent taking action 1 after time T until time s at
s
which information is injected again. Thus, the probability that (µ T∗,A T∗) ∈ ψ
n
andnoinformationisinjectedagainisatleast
(cid:110) (cid:111)
P((µ T∗,A T∗) ∈ ψ
n
| T∗)· 1−12N−1TOR(D(µ t,A t))−4 .
Bydefinition,wehave
P((µ T∗,A T∗) ∈ ψ
n
| T∗) = p +(µ t,A T∗)1{(µ
t
+ M·TOR(D(µ t,A T∗)),A T∗) ∈ ψ n}
Nowweclaimthat
1
A T∗ > A t −TOR(D(µ t,A t))− .
N
To see this, suppose for a contradiction that A T∗ ≤ A t − TOR(D(µ t,A t)) − N1,
which implies A T∗ < A t. However, since the definition of T∗ implies A T∗− ≥
Z T∗− −TOR(D(µ t,A T∗−)),wehave
1
A T∗ ≥ A T∗− −
N
61
≥ Z T∗− −TOR(D(µ t,A T∗−))−
N
1
> A −TOR(D(µ ,A ))− ,
t t t
N
wherethefirstinequalityfollowsfromtheincrementsizeof A beingatmost1/N
t
byassumption. Thisisacontradiction.
Hence,if µ ≥ ψ (A )− M·TOR(D(µ ,A ))/2+ 1 L ,40 wemusthave
t n t t t N µ
1
µ + M·TOR(D(µ ,A )) ≥ ψ (A )+ M·TOR(D(µ ,A ))/2+ L
t t t n t t t µ
N
> (ψ n(A T∗)−L µTOR(D(µ t,A t)))+ M·TOR(D(µ t,A t))/2
= ψ n(A T∗),
bysetting M = 2L ,wherethesecondinequalityfollowsfromLipschitzcontinuity
µ
of ψ n. Thus, (µ
t
+ M·TOR(D(µ t,A t)),A T∗) ∈ ψ
n
holds,implying
P((µ T∗,A T∗) ∈ ψ
n
| T∗) = p +(µ t,A T∗)
= 1−
M·TOR(D(µ t,A T∗))
DOWN(D(µ t,A T∗))+ M·TOR(D(µ t,A T∗))
≥ 1−
M·TOR(D(µ t,A T∗))
DOWN(D(µ t,A T∗))
δ¯λMC
= 1−
2L+2LM(µ
t
−µ(A T∗))−1
δ¯λMC
≥ 1−
2L+2LM(ψ (A )−µ(A ))−1
n t t
≥ 1−δ¯c(ψ (A )−µ(A ))
n t t
for absolute constant c := λC. Then we can write down the lower bound of A for
2L s
every s > T∗ underthiscaseasfollows:
∀s > T∗ ,A ≥ A¯ −TOR(D(µ ,A )) ≥ A¯ −TOR(ψ (A )−µ(A ))
s s t s s n t t
withprobabilityatleast
(1−δ¯c(ψ (A )−µ(A )))·(1−12N−1TOR(D(µ ,A ))−4)
n t t t t
≥ 1−δ¯c(ψ (A )−µ(A ))−12N−1TOR(D(µ ,A ))−4.
n t t t t
40Weusethisconditionwhenweconstructψ n+1.
7CombiningCase1andCase2,wemusthave
∀s > T∗ ,A ≥ A¯ −TOR(ψ (A )−µ(A )) (14)
s s n t t
withprobabilityatleast1−δ¯c(ψ (A )−µ(A ))−12N−1TOR(D(µ ,A ))−4.
n t t t t
Obtaining the lower bound. Our next step is to compute the lower bound when
agent i takes action 1 at time t and the upper bound when agent i takes action 0
at time t. One difference from the case with a continuum of agents is that agent
i’s action affects the entire future path of aggregate actions. Therefore, we need to
accountfortheseeffectswhencomputingthebounds. Finally,usingthesebounds,
we show that there exists d such that if µ > µ(A ) + (dN)−1/9, agent i strictly
t t
prefers action 1 when µ ≥ ψ (A )− M·TOR(D(µ ,A ))/2+ 1 L , given that all
t n t t t N µ
agentstakeaction1forall (µ ,A ) ∈ ψ .
s s n
Suppose that agent i takes action a ∈ {0,1} at (µ t−,A t−) and takes a (random)
action a after each tick of her Poisson clock (τ ) . We call this strategy σ and
in n n i
assumethatitinduces A (σ).41 Herpayofffromstrategy σ isgivenby
s i i
U (σ) = E
(cid:20) ∑∞ (cid:90) τ n+1
e−r(s−t) u(a ,A
(σ),θ)ds(cid:21)
.
i i µ in s i
n=0 τn
In (13), we showed that A (σ) ≥ A¯ −TOR(ψ (A )−µ(A )) for every s < T∗ .42
s i s n t t
∗
AftertimeT ,ifnoinformationisinjectedagain,everyone(includingagenti)takes
action1,implying a = 1if τ > T∗ . In(14),weshowed
in n
∀s > T∗ ,A (σ) ≥ A¯ −TOR(ψ (A )−µ(A ))
s i s n t t
withprobabilityatleast1−δ¯c(ψ (A )−µ(A ))−12N−1TOR(D(µ ,A ))−4.Com-
n t t t t
∗
biningbeforeandafter T ,wehave
∀s ̸= T∗ ,A (σ) ≥ A¯ −TOR(ψ (A )−µ(A ))
s i s n t t
with probability at least 1 − δ¯c(ψ (A ) − µ(A )) − 12N−1TOR(D(µ ,A ))−4. By
n t t t t
Lipschitzcontinuityof u(a,·,θ) and ∆ u(·,θ),wemusthave43
∀s ̸= T∗ ,∀a ∈ {0,1},|u(a,A (σ),θ)−u(a,A¯ ,θ)| ≤ TOR(ψ (A )−µ(A ))L
s i s n t t
41Again, A dependsonagenti’sstrategyσ becauseoffiniteness.
s i
42Theincrementof(A (σ)) isatmost1/N becausetheprobabilitythatPoissonclocksofmore
s i s
thanoneagentstickatthesametimeiszero. Hencewecanapplytheearlierarguments.
43Let L and L be Lipschitz constants of u(0,·,θ) and u(1,·,θ), respectively. Then, ∆u(·,θ) is
0 1
LipschitzcontinuouswithconstantL := L +L .
0 1
8withprobabilityatleastP (µ ,A ) := 1−δ¯c(ψ (A )−µ(A ))−12N−1TOR(D(µ ,A ))−4.
N t t n t t t t
Thus,foreverystrategy σ underconjecture ψ ,thisimplies
i n
(cid:12) (cid:12) (cid:12)U i(σ i)−E µ(cid:20) ∑∞ (cid:90) τ n+1 e−r(s−t) u(a in,A¯ s,θ)ds(cid:21)(cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
n=0 τn
(cid:124) (cid:123)(cid:122) (cid:125)
=:U∗(σ)
i i
(cid:12) (cid:12)
= (cid:12) (cid:12)E (cid:20) ∑∞ (cid:90) τ n+1 e−r(s−t)(cid:8) u(a ,A (σ),θ)−u(a ,A¯ ,θ)(cid:9) ds(cid:12) (cid:12)
(cid:12) µ in s i in s (cid:12)
(cid:12) n=0 τn (cid:12)
≤
E(cid:20) ∑∞ (cid:90) τ n+1
e−r(s−t)(P (µ ,A )TOR(ψ (A )−µ(A ))L+(1−P (µ ,A
))L(cid:17)(cid:21)
ds
N t t n t t N t t
n=0 τn
(cid:110) (cid:111) (cid:90) ∞
= P (µ ,A )TOR(ψ (A )−µ(A ))L+(1−P (µ ,A ))L · e−r(s−t) ds
N t t n t t N t t
t
P (µ ,A )TOR(ψ (A )−µ(A ))L+(1−P (µ ,A ))L
N t t n t t N t t
= .
r
Now define σ1 to be a strategy that agent i always takes action 1. Suppose that
i
agent i takes action 0 at the beginning for σ. Consider that, since µ > µ(A ), if
i t
µ ≥ ψ (A )− M·TOR(D(µ ,A ))/2+ 1 L ,
t n t t t N µ
U∗(σ) = E
(cid:20) ∑∞ (cid:90) τ n+1
e−r(s−t) u(a ,A¯
,θ)ds(cid:21)
i i µ in s
n=0 τn
≤ E (cid:20)(cid:90) τ 1 e−r(s−t) u(0,A¯ ,θ)ds(cid:21) +E (cid:20) ∑∞ (cid:90) τ n+1 e−r(s−t) u(1,A¯ ,θ)ds(cid:21)
µ s µ s
t n=1 τn
(cid:20)(cid:90)
τ
(cid:21)
= U∗(σ1)−E 1 e−r(s−t)∆ u(A¯ ,θ)ds
i i µ s
t
C
≤ U∗(σ1)− (ψ (A )−µ(A )),
i i 2 n t t
wherethefirstinequalityfollowsfromLemma6,andthesecondinequalityfollows
from(LB).
Therefore,wehave
U (σ1)−U (σ)
i i i i
= (U (σ1)−U∗(σ1))+(U∗(σ1)−U∗(σ))+(U∗(σ)−U (σ))
i i i i i i i i i i i i
C P N(µ t,A t)TOR(ψ n(A t)−µ(A t))L+(1−P N(µ t,A t))L
≥ (ψ (A )−µ(A ))−2·
n t t
2 r
C TOR(ψ n(A t)−µ(A t))L+(1−P N(µ t,A t))L
≥ (ψ (A )−µ(A ))−2·
n t t
2 r
9Recallthat
P (µ ,A ) = 1−δ¯c(ψ (A )−µ(A ))−12N−1TOR(D(µ ,A ))−4.
N t t n t t t t
Since D(µ ,A ) = µ −µ(A ) > (dN)−1/9 holdsbyassumption,wemusthave
t t t t
P (µ ,A ) > 1−δ¯c(ψ (A )−µ(A ))−12(eδ¯)−4d8/9N−1/9
N t t n t t
for some constant e¯ and e such that e¯δ¯D2 ≥ TOR(D) ≥ eδ¯D2.44 Let ψ (A ) −
n t
µ(A ) = ϕ . Since µ ∈/ ψ (A ),wehave ϕ ≥ D(µ ,A ) > (dN)−1/9.Thus,
t n t n t n t t
U (σ1)−U (σ)
i i i i
Cϕ e¯δ¯ϕ2L+(δ¯cϕ +12(eδ¯)−4d8/9N−1/9)
≥ n −2· n n
2 r
(cid:18)
C
2e¯δ¯+δ¯c(cid:19) 24(eδ¯)−4d8/9N−1/9
≥ − ϕ −
n
2 r r
(cid:18)
C
2e¯δ¯+δ¯c(cid:19) 24(eδ¯)−4d8/9N−1/9
≥ − (dN)−1/9−
2 r r
> 0,
wheretheseinequalitiesaretrueif
Cr
δ¯ <
2(2e¯+c)
r(eδ¯)4(cid:18)
C
2e¯δ¯+δ¯c(cid:19)
d < − .
24 2 r
In conclusion, we have shown that there exists a constant d such that if µ >
t
µ(A )+(dN)−1/9,agentistrictlyprefersaction1whenµ ≥ ψ (A )−M·TOR(D(µ ,A ))/2+
t t n t t t
1 L ,giventhatallagentstakeaction1forall (µ ,A ) ∈ ψ .
N µ s s n
Characterizingψ . Notethatδisincreasingandincreasingin A . Thus,µ +M·
n+1 t t
TOR(D(µ ,A ))/2 is increasing in µ and continuous in µ . Therefore, for each A ,
t t t t t
thereexists µ′(A ) < ψ (A ) suchthat
t n t
µ′(A )+ M·TOR(D(µ′(A t),A t)) = ψ (A )+ L µ
t n t
2 N
if
M·TOR(D(ψ n(A t),A t))
>
L µ
.
2 N
44Bythedefinitionofδ,anye¯≥ λC/4Lande ≤ λC/{4L(1+M)}works.
10Asufficientconditionforthisis
(cid:32) (cid:33)9
M δ¯e(dN)−2/9
>
L µ
⇔ d <
Mδ¯e 2 N7
2.
2 N 2L
µ
Hence,taking d suchthat
 
(cid:32) (cid:33)9
 Mδ¯e 2 r(eδ¯)4(cid:18) C 2e¯δ¯+δ¯c(cid:19) 
d < min , − (15)
2L 24 2 r
 µ 
issufficient. Thenwedefine
ψ = {(µ ,A ) : µ ≥ µ′(A )}
n+1 t t t t
Fromtheargumentabove,wemusthaveanagentalwayschoosingaction1when-
ever (µ ,A ) ∈ ψ .Moreover,wecanrewritetheaboveequationasfollows:
t t n+1
M·TOR(µ′(A )−µ(A )) L
(µ′(A )−µ(A ))+ t t = ψ (A )−µ(A )+ µ ,
t t n t t
2 N
where the RHS is constant in A by the property of ψ. Thus, µ′(A )−µ(A ) must
t t t
be also constant in A . This concludes that round-(n+1) dominance region ψ
t n+1
satisfies ψ ⊂ ψ because c = ψ (A )−µ(A ) > µ′(A )−µ(A ) =: c when
n n+1 n n t t t t n+1
(15)issatisfied.
Toconcludetheproofofpart1ofTheorem2,weshowthefollowinglemma.
Lemma8.
(cid:110) (cid:111)
(cid:91) ψ ⊇ (µ,A) ∈ ∆(Θ)×[0,1] : µ > µ(A)+(dN)−1/9 .
n
n∈N
ProofofLemma8. Recall ψ (A ) = sup{µ ∈ ∆(Θ) : (µ,A ) ∈/ ψ }. By Lemma 7,
n t t n
ψ n(A t)isdecreasinginn. Defineψ∗(A t) = lim n→∞ψ n(A t). Inlimit,wemusthave
ψ∗(A )+ M·TOR(D(ψ∗(A ),A ))/2 = ψ∗(A )+L /N
t t t t µ
⇒ TOR(D(ψ∗(A ),A )) = 2L /(MN).
t t µ
Sinceourchoiceof d by(15)ensures
2L µ
≤
δ(cid:16) (dN)−1/9(cid:17)
,
MN
11wehave
D(ψ∗(A ),A ) ≤ µ −µ(A ) ⇔ ψ∗(A ) ≤ µ
t t t t t t
forany µ > µ(A )+(dN)−1/9,asdesired.
t t
ProofofPart2. Considerthefollowingtwocases:
Case 1: µ > µ(A ). Consider N large enough so that µ > µ(A )+(dN)−1/9.
0 0 0 0
Under µη andtheenvironmentof N agents,Part1implieseveryonetakesaction1
underanyequilibriumoutcomeuntilnewinformationisinjected.
Withoutlossofgenerality,weassume ϕ ≥ 0.Let τ := (τ)N . Wehave
i i=1
(cid:104) (cid:105)
inf Eσ ϕ(A)
σ∈ΣN(µη,A )
0
(cid:104) (cid:110) (cid:111) (cid:105)
≥ E 1 ∀t,A¯ − A¯N ≤ TOR(D(µ ,A )) ϕ(A¯N)
τ t t 0 t
(cid:104) (cid:110) (cid:111) (cid:105)
≥ E 1 ∀t,A¯ − A¯N ≤ TOR(D(µ ,A )) ϕ(A¯N)
τ t t 0 0
(cid:104) (cid:110) (cid:111) (cid:105)
≥ E 1 ∀t,A¯ − A¯N ≤ TOR(D(µ ,A )) ϕ(A¯) −L ·TOR(D(µ ,A ))
τ t t 0 0 ϕ 0 0
(cid:110) (cid:111)
≥ 1−12N−1TOR(D(µ ,A ))−4 ϕ(A¯)−L ·TOR(D(µ ,A )) (Lemma5)
0 0 ϕ 0 0
(cid:110) (cid:111)
≥ 1−12N−1/9(eδ¯)−4d8/9 ϕ(A¯)−L (eδ¯)(dN)−2/9 (TOR(D) ≥ eδ¯D2)
ϕ
≥ ϕ(A¯)−C¯N−1/9
forsomeconstant C¯. TheproofofTheorem1implies(OPT) = ϕ(A¯).Thus,
|(OPT)−(ADV-N)| ≤ C¯N−1/9
when N islargeenough,asdesired.
Case2: µ ≤ µ(A ). TheproofofTheorem1implies
0 0
(cid:104) (cid:105)
(OPT) = sup Eσ ϕ(A) ≤ (1− p∗(µ ))ϕ(A)+ p∗(µ )ϕ(A¯),
0 0
µ∈M
σ∈Σ(µ,A )
0
where p∗(µ ) := µ /µ(A ), A satisfies A = A(A ,t) = A e−λt, and A¯ satisfies
0 0 0 t 0 0
A¯ = A¯(A ,t) = 1−(1− A )e−λt.
t 0 0
Let η > 2(dN)−1/9/(2(dN)−1/9+µ(A )). Inthiscase,wehave
0
µ
µ+ = 0 > µ(A )+2(dN)−1/9,
0 p∗(µ )−η 0
0
12where µ+ is the maximal escaping belief defined in the main text. Under µη and
0
the environment of N agents, if µ > µ(A ) + (dN)−1/9, then everyone takes
0+ 0
action 1 until new information is injected under any equilibrium outcome by Part
1. Thus,wehave
(cid:104) (cid:105)
inf Eσ ϕ(A)
σ∈ΣN(µη,A )
0
(cid:104) (cid:110) (cid:12) (cid:12) (cid:111) (cid:105)
≥ (1− p∗(µ )+η)E 1 ∀t,(cid:12)A − AN(cid:12) ≤ TOR(D(µ ,A )) ϕ(AN)
0 τ (cid:12) t t (cid:12) t t
(cid:104) (cid:110) (cid:111) (cid:105)
+(p∗(µ )−η)E 1 ∀t,A¯ − A¯N ≤ TOR(D(µ ,A )) ϕ(A¯N)
0 τ t t t t
(cid:104) (cid:110) (cid:12) (cid:12) (cid:111) (cid:105)
≥ (1− p∗(µ )+η)E 1 ∀t,(cid:12)A − AN(cid:12) ≤ TOR(D(µ ,A )) ϕ(AN)
0 τ (cid:12) t t (cid:12) t t
(cid:110) (cid:111) (cid:104) (cid:105)
+(p∗(µ )−η) 1−12N−1TOR(D(µ ,A ))−4 E ϕ(A¯N)
0 t t τ
(cid:16) (cid:17)(cid:110) (cid:111)
≥ (1− p∗(µ )+η) 1−O(N−1/9) ϕ(A)−L (eδ¯)(dN)−2/9
0 ϕ
(cid:110) (cid:111)(cid:110) (cid:111)
+(p∗(µ )−η) 1−12N−1/9(eδ¯)−4d8/9 ϕ(A¯)−L (eδ¯)(dN)−2/9 ,
0 ϕ
where A¯N and AN satisfy
A¯N = A +
1 ∑n
1{τ ≤ t}
t 0 i
N
i=1
AN = A −
1 N ∑−n
1{τ ≤ t}
t 0 i
N
i=1
with n being the number of agents playing 0 at time 0. Note that the second in-
equality follows from Lemma 5, and the third inequality follows from TOR(D) ≥
eδ¯D2 ≥ eδ¯(dN)−2/9. NotealsothatwecanapplythesimilarargumenttoLemma5
andshowthat
(cid:16) (cid:12) (cid:12) (cid:17)
P ∀t,(cid:12)A − AN(cid:12) ≤ TOR(D(µ ,A )) > 1−O(N−1/9).
(cid:12) t t (cid:12) t t
Thisimplies
(cid:110) (cid:111)(cid:110) (cid:111)
(ADV-N) ≥ (1− p∗(µ )+η) 1−O(N−1/9) ϕ(A)−L (eδ¯)(dN)−2/9
0 ϕ
(cid:110) (cid:111)(cid:110) (cid:111)
+(p∗(µ )−η) 1−O(N−1/9) ϕ(A¯)−L (eδ¯)(dN)−2/9
0 ϕ
Hence,wehave
|(OPT)−(ADV-N)| ≤ η{ϕ(A¯)−ϕ(A)}+L (eδ¯)(dN)−2/9
ϕ
(cid:110) (cid:111)
+O(N−1/9)(p∗(µ )−η) ϕ(A¯)−L (eδ¯)(dN)−2/9
0 ϕ
13(cid:110) (cid:111)
+O(N−1/9)(1− p∗(µ )+η) ϕ(A)−L (eδ¯)(dN)−2/9
0 ϕ
= η{ϕ(A¯)−ϕ(A)}+O(N−1/9).
Withourchoiceofη,wehavethereexitsaconstantC¯ suchthat|(OPT)−(ADV-N)| ≤
C¯N−1/9,asdesired.
ProofofPart3. If µ t− > µ(A t), consider N large enough such that µ t− > µ(A t)+
2(dN)−1/9.
Weconsiderthefollowingtwocasesfor:
• Case 1: If µ t− > µ(A t)+2(dN)−1/9 and Z t− ≤ A t. In this case, there is no
information arriving, and everyone takes action 1. This will increase A , and
t
everyagentalwaystakesaction1fromtimetonwardsaslongas A¯ − A¯N ≤
s s
TOR(D(µ ,A )) for all s ≥ t. Since Lemma 5 implies that such probability
s s
converges to 1 as N → ∞ , the designer’s payoff converges to the best case,
implyingsequentialoptimalityas N → ∞ .
• Case 2: If µ t− > µ(A t) + 2(dN)−1/9 and Z t− > A t. In this case, the be-
lief moves to either µ t− + M·TOR(D) or µ t −DOWN(D). Note that µ t− −
DOWN(D) = (µ +µ(A ))/2 > µ(A )+(dN)−1/9. So no matter what infor-
t t t
mation arrives, every agent takes action 1. This will increase A , and every
t
agentalwaystakesaction1aftertimetaslongas A¯ −A¯N ≤ TOR(D(µ ,A ))
s s s s
foralls ≥ t. Again,sincesuchprobabilityconvergesto1as N → ∞ ,wehave
sequentialoptimalityas N → ∞ .
14