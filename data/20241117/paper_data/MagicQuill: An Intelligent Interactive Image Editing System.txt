MagicQuill: An Intelligent Interactive Image Editing System
ZichenLiu♡,1,2 YueYu♡,1,2 HaoOuyang2 QiuyuWang2,
KaLeongCheng1,2 WenWang3,2 ZhihengLiu4 QifengChen†,1 YujunShen†,2
1HKUST 2AntGroup 3ZJU 4HKU
Figure1. MagicQuillisanintelligentandinteractiveimageeditingsystembuiltupondiffusionmodels. Usersseamlesslyeditimages
usingthreeintuitivebrushstrokes:add,subtract,andcolor(A).AMLLMdynamicallypredictsuserintentionsfromtheirbrushstrokesand
suggestscontextualprompts(B1-B4). Theexamplesdemonstratediverseeditingoperations: togenerateajacketfromclothingcontour
(B1),addaflowercrownfromheadsketches(B2),removebackground(B3),andapplycolorchangestothehairandflowers(B4).
Abstract 1.Introduction
Performing precise and efficient edits on digital pho-
Asahighlypracticalapplication,imageeditingencoun-
tographs remains a significant challenge, especially when
tersavarietyofuserdemandsandthusprioritizesexcellent
aiming for nuanced modifications. As shown in Fig. 1,
ease of use. In this paper, we unveil MagicQuill, an
consider the task of editing a portrait of a lady where
integratedimageeditingsystemdesignedtosupportusersin
specific alterations are desired: converting a shirt to a
swiftlyactualizingtheircreativity. Oursystemstartswitha
custom-designedjacket, addingaflowercrownatanexact
streamlinedyetfunctionallyrobustinterface,enablingusers
position with a well-designed shape, dyeing portions of
to articulate their ideas (e.g., inserting elements, erasing
her hair in particular colors, and removing certain parts
objects, alteringcolor, etc.) withjusta fewstrokes. These
of the background to refine her appearance. Despite the
interactions are then monitored by a multimodal large
rapidadvancementsindiffusionmodels[6,10,14,19,35–
language model (MLLM) to anticipate user intentions in
38,47,62,68]andrecentattemptstoenhancecontrol[20,
realtime,bypassingtheneedforpromptentry. Finally,we
23, 48, 69], achieving such fine-grained and precise edits
applythepowerfuldiffusionprior,enhancedbyacarefully
continues to pose difficulties, typically due to a lack of
learned two-branch plug-in module, to process the editing
intuitiveinterfacesandmodelsforfine-grainedcontrol.
request with precise control. Please visit https://magic-
Thechallengeshighlightthecriticalneedforinteractive
quill.github.iototryoutoursystem.
editing systems that facilitate precise and efficient modifi-
cations. Anidealsolutionwouldempoweruserstospecify
♡Equalcontribution.†Correspondingauthor. whattheywanttoedit,wheretoapplythechanges,andhow
1
4202
voN
41
]VC.sc[
1v30790.1142:viXrathe modifications should appear, all within a user-friendly current image editing tools and providing an innovative
interfacethatstreamlinestheeditingprocess. solution that enhances both precision and efficiency, our
We aim to develop the first robust, open-source, in- work advances the field of digital image manipulation.
teractive precise image editing system to make image Our framework opens possibilities for users to engage
editingeasyandefficient. Oursystemseamlesslyintegrates creatively with image editing, achieving their goals easily
three core modules: the Editing Processor, the Painting andeffectively.
Assistor, and the Idea Collector. The Editing Processor
ensures a high-quality, controllable generation of edits, 2.RelatedWorks
accurately reflecting users’ editing intentions in color and
2.1.ImageEditing
edge adjustments. The Painting Assistor enhances the
ability of the system to predict and interpret the users’ Image editing involves modifying the visual appearance,
editing intent. The Idea Collector serves as an intuitive structure, or elements of an existing image [19]. Recent
interface, allowing users to input their ideas quickly and breakthroughs in diffusion models [17, 44, 49] have sig-
effortlessly,significantlyboostingtheeditingefficiency. nificantly advanced visual generation tasks, outperforming
The Editing Processor implements two kinds of GAN-based models [15] in terms of image editing capa-
brushstroke-based guidance mechanisms: scribble guid- bilities. To enable control and guidance in image editing,
anceforstructuralmodifications(e.g.,adding,detailing,or avarietyofapproacheshaveemerged, leveragingdifferent
removingelements)andcolorguidanceformodificationof modalities such as textual instructions [6, 11, 14, 32, 47,
color attributes. Inspired by ControlNet [66] and Brush- 65], masks [20, 23, 48, 69], layouts [10, 33, 68], segmen-
Net[23],ourcontrolarchitectureensurespreciseadherence tation maps [35, 62], and point-dragging interfaces [36–
touserguidancewhilepreservingunmodifiedregions. Our 38]. Despite these advances, these methods often fall
Painting Assistor reduces the repetitive process of typing short when precise modifications at the regional level are
text prompts, which disrupts the editing workflow and required,suchasalterationstoobjectshape,color,andother
createsacumbersometransitionbetweenpromptinputand details. Among the various methods, sketch-based editing
image manipulation. It employs an MLLM to interpret approaches [22, 25, 34, 42, 59, 61, 64] offer users a more
brushstrokes and automatically predicts prompts based on intuitive and precise means of interaction. However, the
image context. We call this novel task Draw&Guess. We currentmethodsremainlimitedbytheaccuracyofthetext
construct a dataset simulating real editing scenarios for signalsinputalongsidethesketches,makingitchallenging
fine-tuning to ensure the effectiveness of the MLLM in to precisely control the information of the editing areas,
understanding user intentions. This enables a continuous suchascolor. Toachieveprecisecontrol,weintroducetwo
editing workflow, allowing users to iteratively edit images typesoflocalguidancebasedonbrushstrokes: scribbleand
withoutmanualpromptinput. TheIdeaCollectorprovides color,therebyenablingfine-grainedcontrolovershapeand
an intuitive interface compatible with various platforms colorattheregionallevel.
including Gradio and ComfyUI, allowing users to draw
2.2.MLLMsforImageEditing
with different brushes, manipulate strokes, and perform
continuouseditingwithease.
Multi-modal large language models (MLLMs) extend
Wepresentacomprehensiveevaluationofourinteractive LLMs to process both text and image content [16], en-
editing framework. Through qualitative and quantitative ablingtext-to-imagegeneration[9,28,52,53,58],prompt-
analyses, we demonstrate that our system significantly refinement[60,63],andimagequalityevaluation[51].
improves both the precision and efficiency of performing Intheareaofimageediting,MLLMshavedemonstrated
detailed image edits compared to existing methods. Our significant potential. MGIE [13] enhances instruction-
Editing Processor achieves superior edge alignment and based image editing by using MLLMs to generate more
colorfidelitycomparedtobaselineslikeSmartEdit[20]and expressive, detailed instructions. SmartEdit [20] leverages
BrushNet[23]. ThePaintingAssistorexhibitssuperioruser MLLM for better understanding and reasoning towards
intent interpretation capabilities compared to state-of-the- complex instruction. FlexEdit [55] integrates MLLM to
artMLLMs,includingLLaVA-1.5[31],LLaVA-Next[30], understand image content, masks, and textual instructions.
and GPT-4o [21]. User studies indicate that the Idea GenArtist[57]usesanMLLMagenttodecomposecomplex
Collectorsignificantlyoutperformsbaselineinterfacesinall tasks, guide tool selection, and enable systematic image
aspectsofsystemusability. generation, editing, and self-correction with step-by-step
By leveraging advanced generative models and a user- verification. Our system extends this line of research by
centric design, our interactive editing framework signifi- introducing a more intuitive approach, utilizing MLLM
cantly reduces the time and expertise required to perform to simplify the editing process. Specifically, it directly
detailed image edits. By addressing the limitations of integrates the image context with user-input strokes to
2Instant prediction
Precise control
“... a ‘draw and guess’ game ... what am I > cake
drawing with these strokes in the image?” MLLM > cake
> red vase
> red vase
“... a ‘draw and guess’ game ... identify
what is inside the red contours?” Text prompt Raw Mask Text Edge Color
❷
❸
A
B
User-friendly interface
Consecutive editing
❶ ❹
C D E
Raw image
F G
Edited image
Figure 2. System framework consisting of three integrated components: an Editing Processor with dual-branch architecture for
controllableimageinpainting, aPaintingAssistorforreal-timeintentprediction, andanIdeaCollectorofferingversatilebrushtools.
Thisdesignenablesintuitiveandpreciseimageeditingthroughbrushstroke-basedinteractions.
infer and translate the editing intentions, thereby automat- 3.SystemDesign
ically generating the necessary prompts without requiring
Oursystemisstructuredaroundthreekeyaspects: Editing
repeated user input. This innovative task, which we term
Processor with strong generative prior, Painting Assistor
Draw&Guess, facilitates a continuous editing workflow,
with instant intent prediction, and Idea Collector with a
enabling users to iteratively refine images with minimal
user-friendly interface. An overview of our system design
manualintervention.
ispresentedinFig.2.
Oursystemintroducesbrushstroke-basedcontrolsignals
2.3.InteractiveSupportforImageGeneration to give intuitive and precise control. These signals allow
userstoexpresstheireditingintentionsbysimplydrawing
Interactivesupportenhancestheperformanceandusability what they envision. We designed two types of brushes,
ofgenerativemodelsthroughhuman-in-the-loopcollabora- scribble and color, to accurately manipulate the edited
tion [27]. Recent works have focused on making prompt image. The scribble brushes, add brush and subtract
engineering more user-friendly through techniques like brush, aim to provide precise structural control by oper-
imageclustering[4,12]andattentionvisualization[56]. ating on the edge map of the original image. The color
Despiteadvancementsininteractivesupport,akeychal- brush works with downsampled color blocks to enable
lenge remains in bridging the gap between verbal prompts fine-grained color manipulation of specific regions. Fig. 3
and visual output. While systems like PromptCharm [56] illustrates the workflow to convert the user hand-drawn
andDesignPrompt[39]useinpaintingforinteractiveimage input signal into control condition for faithfully inpainting
editing, these tools typically offer only coarse-grained the target editing area. Inspired by Ju et al. [23], Zhang
controloverelementadditionandremoval,requiringusers etal.[66],weemploytwoadditionalbranchestothelatent
to brush over areas before generating objects within those diffusionframework[44],withtheinpaintingbranchgiving
regions.Furthermore,usersmustmanuallyinputpromptsto content-aware per-pixel guidance for the re-generation of
specifytheobjectstheywishtogenerate. Ourapproachad- theeditingarea,andthecontrolbranchprovidingstructural
dressestheselimitationsbyintroducingfine-grainedimage guidance. The model architecture is illustrated in Fig. 4.
editing through the use of brushstrokes. Additionally, we FurtherdetailswillbediscussedinSec.3.1.
incorporate a multimodal large language model (MLLM) Toreducethecognitiveloadforuserstoinputappropri-
that provides on-the-fly assistance by interpreting user atepromptsateverystageofediting,oursystemintegrates
intentions and suggesting prompts in real-time, thereby a MLLM [29] as the Painting Assistor. This component
reducingcognitiveloadandenhancingoverallusability. analyzes user brushstrokes to deduce the editing intention
3
teNU
gnitniapnI
teNU
noisuffiD
tenlortnoCbasedontheimagecontext,therebyautomaticallysuggest-
ing contextually relevant prompts for editing. We have
named this innovative task Draw&Guess. To effectively
Raw image Editing signals
preparetheMLLMforDraw&Guess,wedesignedadataset
Editing mask𝐌
construction method to simulate user hand-drawn editing
scenarios and acquire ground truth for Draw&Guess. We
𝐹!""
fine-tunedadedicatedLLaVA[31]model,achievinginstant
promptguessingwithsatisfactoryaccuracy. Morespecifics Raw edge Add brush Subtract brush Edge cond. 𝐄𝒄𝒐𝒏𝒅
willbecoveredinSec.3.2. downscale
Additionally, to provide users with a streamlined, intu-
itiveinterfacethatempowersthemtoexpresstheirideasfor Color blocks Color brsuh Color cond. 𝐂%&’(
compleximageeditingtaskswithease,wedesignedanIdea
Collectorwithauser-friendlyinterface. Thekeyfeaturesof Figure 3. Data processing pipeline. The input image undergoes
edgeextractionviaCNNandcolorsimplificationthroughdown-
theinterfacewillbeoutlinedinSec.3.3.
scaling. Three editing conditions are then generated based on
brushsignals: editingmask,edgecondition,andcolorcondition,
3.1.EditingProcessor
whichtogetherprovidecontrolforimageediting.
ControlConditionfromBrushstrokeSignal. LetM
add
and M denote the binary masks corresponding to add obtainedbydilatingtheunionofbrushregionsbyppixels.
sub
andsubtractbrushrespectively.Thesemaskssharethesame ThemaskedimageI canthenbeformulatedas
masked
dimensions as the original image I, where values are set
to 1 in regions corresponding to user brush strokes and 0 M=Grow p(M add∪M sub∪M color),
(3)
elsewhere. Thesubtractbrushmasksouttheedgesfromthe I =I⊙(1−M).
masked
edge map E, which is initially extracted from the original
imageusingapre-trainedCNNf . Conversely,theadd This expansion accounts for the fact that editing can
CNN
brush introduces new edges by setting designated regions affectareassurroundingthemask,suchasshadowsorother
to white in the edge map. The resulting modified edge adjacent details. By growing the mask, we ensure that
mapE condservesasthecontrolconditionformanipulating theseperipheralregionsareproperlygenerated,resultingin
geometric structure in the editing processor. This can be amoreseamlessandrealisticedit.
formallyexpressedas Controllable Image Inpainting. The inpainting branch
adopts the UNet [23, 45] architecture, incorporating the
E=f CNN(I), masked image feature into the pre-trained diffusion net-
E =E⊙(1−M ), (1) work. This branch inputs the concatenated noisy latent
sub sub
at t-th step z , masked image latent z extracted
E =E +M ⊙(1−E ). t masked
cond sub add sub
using VAE [26] from I , and downsampled mask m
masked
by cubic interpolation from M. The inpainting branch
For precise region-specific colorization, we represent
processes these features, utilizing a trainable clone of
each color brush stroke as a tuple (M ,c,α), where
color
the diffusion model, stripped of cross-attention layers to
M denotes a binary mask indicating the user-defined
color
focus solely on the image feature. The extracted features
stroke region, c specifies the stroke color, and α ∈ [0,1]
carryingpixel-levelinformationareinsertedintoeachlayer
represents the stroke opacity. The colorization operation
of the frozen diffusion model through zero-convolution
canbeformallyexpressedas
layers Z [66]. Given text condition τ, timestep t, let
F(z ,t,c;Θ) represents the feature of the i-th layer in
I =(1−α·M )⊙I+α·M ·c, (2) t i
c color color
the total n layers of the diffusion UNet with parameter
Θ.Similarly,letFI([z ,z ,m],t;ΘI) representsthe
wherethecolorcwithanalphablendingfactorαisapplied t masked i
output of the i-th layer in the inpainting UNet, where [·]
overaspecificregionoftheimageIdefinedbythebinary
denotestheconcatenationoperation. Thisfeatureinsertion
maskM .
color canberepresentedby
To generate the color condition C , we first down-
cond
scale the image I c by a factor of 16 using cubic interpola- F(z t,τ,t;Θ) i+= w I ·Z(FI([z t,z masked,m],t;ΘI) i), (4)
tion,followedbyupscalingtotheoriginalresolutionusing
nearest-neighbor interpolation. This process generated a where w is an adjustable hyperparameter that determines
I
color block preserving the global color structure while the inpainting strength. Equipped with the inpainting
simplifyinglocaldetails. branch, the diffusion UNet can fill the masked area in a
The edge condition E and color condition C content-awaremannerbasedonthetextprompt.
cond cond
jointly guide the inpainting process for precise editing The control branch aims to introduce conditional gen-
control. The editing region, represented by mask M, is eration ability to the diffusion UNet based on condition
4bypass the Q&A process, as the results demonstrate that
ⓒ > cake prompt-freegenerationachievessatisfactoryresults.
> red vase For the color brush, the Q&A setup is similar: “The
userwilluploadanimagecontainingsomecontoursinred
Masked image Mask𝐌 Noisy latents Text Edge 𝐄𝒄𝒐𝒏𝒅 Color 𝐂%&’(
color.Tohelpyoulocatethecontour,...Youneedtoidentify
whatisinsidethecontoursusingasinglewordorphrase.”,
(therepetitivepartisomitted). Thesystemextractscontour
informationfromthecolorbrushstrokeboundaries. Thefi-
nalpredictedpromptisgeneratedbycombiningthestroke’s
colorinformationwithQ&Aoutputs. Tooptimizeresponse
time,weconstrainQ&Aresponsestoconcise,single-word
orshort-phraseformats.
Edited image
For the color brush Q&A task, accurate object recogni-
tion within contours is essential. LLaVA [31] inherently
Figure 4. Overview of our Editing Processor. The proposed
excelsinobjectrecognitiontasks, makingitadeptatiden-
architecture extends the latent diffusion UNet with two special-
ized branches: an inpainting branch for content-aware per-pixel tifying the content within color brush stroke boundaries.
inpaintingguidanceandacontrolbranchforstructuralguidance, However, the interpretation of add brush strokes poses
enablingprecisebrush-basedimageediting. a significant challenge due to the inherent abstraction of
human hand-drawn strokes or sketches. To address this,
C = {E cond,C cond}. We adopt ControlNet [66] to insert we find it necessary to construct a specialized dataset to
conditional control into the middle and decoder blocks of
fine-tune LLaVA to better understand and interpret human
the diffusion UNet. Let FC(z ,C,t;ΘC) represent the
t i hand-drawnbrushstrokes.
outputofthei-thlayerintheControlNet,thecontrolfeature
DatasetConstruction. WeselectedtheDenselyCaptioned
insertioncanbeformulatedas
Images (DCI) dataset [54] as our primary source. Each
F(z t,τ,t;Θ) ⌊n⌋+i+= w C·Z(FC(z t,C,t;ΘC) i), (5) image within the DCI dataset has detailed, multi-granular
2 masks, accompanied by open-vocabulary labels and rich
wherew isanadjustablehyperparameterthatdetermines descriptions. This rich annotation structure enables the
C
the control strength. Both the inpainting and control captureofdiversevisualfeaturesandsemanticcontexts.
branchesdon’taltertheweightsofthepre-traineddiffusion Step 1: Answer Generation for Q&A. The initial stage
models,enablingittobeaplug-and-playcomponentappli- involves generating edge maps using PiDiNet [50] from
cable to any community fine-tuned diffusion models. The imagesintheDCIdataset,asshowninFig.5b.Wecalculate
controlbranchistrainedusingthedenoisingscorematching the edge density within the masked regions and select the
objective,whichcanbewrittenas top5maskswiththehighestedgedensities,asillustratedin
Fig. 5c. The labels corresponding to these selected masks
L=E zt,t,ϵ∼N(0,I)(cid:104)(cid:13) (cid:13)ϵ−ϵc(cid:0) z t,C,t;{Θ,ΘC}(cid:1)(cid:13) (cid:13)2(cid:105) , (6) s me orv de ela fs oct uh se esgr oo nun gd uet sr su it nh gs uf so er rt ih ne tenQ t& raA th. eT ro the an nsu pr ae rsit nh ge
whereϵcisthecombinationofthedenoisingU-Netandthe irrelevant details, we clean the label to keep only noun
components,streamliningtoemphasizeessentialelements.
ControlNetmodel.
Step2: SimulatingBrushstrokewithEdgeOverlay. In
3.2.PaintingAssistor thesecondpartofthedatasetconstruction,wefocusonthe
fivemasksidentifiedinthefirststep. Eachmaskundergoes
Prompt formatting. In our system, we implement two
randomshapeexpansiontointroducevariability.Weusethe
types of question answering (Q&A) [3] tasks to facilitate
BrushNet [23] model based on the SDXL [41] to perform
the Draw&Guess. For the add brush, we utilize a prompt
inpainting on these augmented masks with empty prompt,
structured as follows: “This is a ‘draw and guess’ game.
asshowninFig.5d.Subsequently,theedgemapsgenerated
I will upload an image containing some strokes. To help
earlier are overlaid onto the inpainted areas as in Fig. 5e.
you locate the strokes, I will give you the normalized
These overlay images simulate practical examples of how
boundingboxcoordinatesofthestokeswheretheiroriginal
userhand-drawnstrokesmightalteranimage.
coordinates are divided by the padded image width and
MLLM Fine-Tuning. Our dataset construction method
height. The top-left corner of the bounding box is at
effectively prepares the model to understand and predict
(x 1,y 1), and the bottom-right corner is at (x 2,y 2). Now user edits, which contains a total of 24,315 images, cat-
tellmeinasinglewordaphrase,whatamItryingtodraw egorized under 4,412 different labels, ensuring a broad
withthesestrokesintheimage?” TheQ&Aoutputdirectly spectrumofdatafortraining. Tooptimizetheperformance
serves as the predicted prompt. For the subtract brush, we oftheMLLMoverDraw&Guess,wefine-tunedtheLLaVA
5
teNU
gnitniapnI
teNU
noisuffiD
gniddebmE
txeT
tenlortnoC(a)OriginalImage (b)EdgeMap (c)ChosenMask (d)InpaintingResult (e)EdgeOverlay
Figure5. Illustrationofdatasetconstructionprocess. (a)OriginalimagesfromtheDCIdataset; (b)Edgemapsextractedfromoriginal
images;(c)Selectedmasks(highlightedinpurple)withhighestedgedensity;(d)ResultsafterBrushNetinpaintingonaugmentedmasked
regions;(e)Finalresultswithedgemapoverlayonselectedareas. Byoverlayingedgemapsoninpaintedresults,wesimulatescenarios
whereuserseditimageswithbrushstrokes,astheedgemapsresemblehand-drawnsketches. Theboundingboxcoordinatesofthemask
andlabelsareinheritedfromtheDCIdataset.
model, leveraging the Low-Rank Adaptation (LoRA) [18] 4.Experiment
technique,allowingtheefficientfine-tuningwithoutexten-
sively large dataset. Consistent with the original LLaVA In evaluating our system, we focused on three primary
training objectives, our approach aims to maximize the modules: the Editing Processor, the Painting Assistor,
likelihood of the correct labels given the input corpora u, and the Idea Collector. First, we assessed the quality of
whichisdefinedas controllable generation provided by the Editing Processor,
with particular attention to edge alignment and color fi-
|u| delity. This evaluation involved analyzing how effectively
max(cid:88) logP(cid:16)
u i |u 1,...,u
i−1;{Θpt,Θlora}(cid:17)
, (7) userscouldmanipulateandachievedesiredvisualoutputs,
Θlora
i=1 which ensures the system responds accurately to user’s
control signal, detailed in Sec. 4.1. Second, We evaluated
where Θpt and Θlora are parameters in the pre-trained the Painting Assistor’s semantic prediction accuracy using
simulatedhand-drawninputs. Thisassessmentwascritical
MLLMandtheLoRArespectively.
for validating the capability of the MLLM in interpreting
user intentions, ensuring contextually appropriate sugges-
3.3.IdeaCollector
tionsthatalignwiththeimagesemantics. Additionally,we
conducted user studies to gather feedback on the system’s
Interface Design. The user interface of MagicQuill
efficiency improvements and prediction accuracy in real-
is designed for an intuitive and streamlined image editing
world scenario, presented in Sec. 4.2. Third, we assessed
experience,asdepictedinFigure2.Theinterfaceisdivided
the usability of the user interfaces across all modules. We
into several interactive sections, emphasizing ease of use
decomposes the assessment into four distinct dimensions
while providing flexible control over the editing process.
spanning from operational efficiency to user satisfaction.
The interface comprises several key areas: a Prompt Area
Thismulti-dimensionalassessmentframeworkenabledsys-
(A) displaying MLLM-suggested prompts, a Toolbar (B)
tematic comparison with baseline systems while ensuring
with essential editing tools, Layer Management (C) for
thoroughevaluationoftheinterface,asshowninSec.4.3.
organizingbrushstrokes,themainCanvas(D)forediting,a
GeneratedImagesarea(E)forpreviewingresults,Execute
4.1.ControllableGeneration
Button(F),andParameterAdjustment(G).
Cross-Platform Support. We implement the Idea Col- To thoroughly evaluate the controllable generation capa-
lector as a modular ReactJS component library, designed bilities of our editing processor, we compared it with
for cross-platform compatibility with various generative four representative baselines from different categories: (1)
AI frameworks, such as Gradio [1] and ComfyUI [7]. SmartEdit [20], an instruction-based editing method. We
Thearchitectureseparatesclient-sideuserinteractionsfrom utilize LLaVA-Next [30] to generate the editing instruc-
server-side model computations through HTTP protocols, tion;(2)SketchEdit[64],aGAN-basedsketch-conditioned
enabling platform-independent deployment via standard method; (3) BrushNet [23], the mask and prompt-guided
HTMLrendering. inpaintingmethod;and(4)acompositebaselinecombining
6Figure6. Visualresultcomparison. Thefirsttwocolumnspresenttheedgeandcolorconditionsforediting,whilethelastcolumnshows
the ground truth image that the models aim to recreate. SmartEdit [20] utilizes natural language for guidance, but lacks precision in
controllingshapeandcolor,oftenaffectingnon-targetregions.SketchEdit[64],aGAN-basedapproach[15],struggleswithopen-domain
image generation, falling short compared to models with diffusion-based generative priors. Although BrushNet [23] delivers seamless
imageinpainting,itstrugglestoalignedgesandcolorssimultaneously,evenwithControlNet[66]enhancement. Incontrast,ourEditing
Processorstrictlyadherestobothedgeandcolorconditions,achievinghigh-fidelityconditionalimageediting.
BrushNet [23] and ControNet [66]. As illustrated in We further conducted a quantitative analysis of our
Fig. 6, the instruction-based method, SmartEdit, tends to constructed test dataset in Sec. 3.2, which contains 490
produceoutputsthataretoorandom, lackingtheprecision images. Our model outperformed the baselines across
required for accurate editing purposes. Similarly, while all key metrics as in Tab. 1. These results demonstrate
BrushNetenablesregion-specificmodifications,itstruggles significantimprovementsincontrollablegeneration.
with maintaining predictable detail generation even with
ControlNet enhancement, making precise manipulation 4.2.PredictionAccuracy
challenging. Incontrast,ourmodelachievesmoreaccurate
To evaluate the prediction accuracy of the Painting Assis-
edgealignmentandcolorfidelity,whichweattributetoour
tor, we compared it with three state-of-the-art MLLMs:
specializeddesignoftheinpaintingandcontrolbranchthat
LLaVA-1.5 [31], LLaVA-Next [30], and GPT-4o [21] on
emphasizestheseaspects.
our test dataset of 490 images from Sec. 3.2. Each model
waspromptedwithimagescontainingsketchesandbound-
Table 1. Quantitative results and input condition comparisons
ing box coordinates to generate semantic interpretations.
betweenthebaselinesandours. OurEditingProcessorperforms
The semantic outputs were assessed using three metrics:
betterthanthebaselinesacrossallmetrics,indicatingitssuperior-
BERT [8], CLIP [43], and GPT-4 [2] similarity scores,
ityincontrollablegenerationoveredgeandcolor.
which measure the closeness of the generated descriptions
InputCondition to the ground truth. For GPT-4 similarity, we ask GPT-
Method LPIPS[67] PSNR SSIM
Text Edge Color 4 to rate the semantic and visual similarity between the
SmartEdit ✓ ✗ ✗ 0.339 16.695 0.561 predictedresponseandthegroundtruthona5-pointscale,
SketchEdit ✗ ✓ ✗ 0.138 23.288 0.835 where1means“completelydifferent”,3means“somewhat
BrushNet ✓ ✗ ✗ 0.0817 25.455 0.893 related”,and5means“exactlysame”.
Brush.+Cont. ✓ ✓ ✓ 0.0748 25.770 0.894 TheevaluationresultsarepresentedinTab.2,illustrating
Ours ✓ ✓ ✓ 0.0667 27.282 0.902
that our model achieves the highest prediction accuracy
7Table2. PerformancecomparisonbetweenourPaintingAssistor
and other MLLMs, demonstrating superior visual and semantic
consistencyinpredictions.
GPT-4[2] BERT[8] CLIP[43]
Method
Similarity Similarity Similarity
LLaVA-1.5 1.894 0.721 0.795
LLaVA-Next 1.941 0.716 0.794
GPT-4o 1.976 0.684 0.790
Ours 2.712 0.749 0.824
Figure 8. Comparative user ratings between our system and the
baseline in four dimensions, with standard deviation shown as
errorbars.
Thissetupenablesthefocusonthevalueprovidedwithour
IdeaCollectorbycontrollingothervariables.
Figure 7. User ratings for the Painting Assistor, focusing on its
Procedure. The study lasted approximately 30 minutes
predictionaccuracyandefficiencyenhancementcapabilities.
for each participant with two systems (our system and the
among all tested MLLMs. This superior performance in- baseline). Each session began with a brief introduction to
dicatesthatourPaintingAssistormoreaccuratelycaptures thesystemusingthecaseillustratedinFig.1. Participants
andpredictsthesemanticmeaningsofuserdrawings. thenhad5minutestofreelyexploreandeditimages. After
To further qualitatively evaluate the Painting Assistor, usingbothsystems,participantscompletedaquestionnaire
weconductedauserstudywith30participantswhofreely with22questions(10questionspersystemcoveringallfour
edited images using our system. Participants rated the
dimensionsand2questionsregardingthePaintingAssistor
PaintingAssistorona5-pointscaleforpredictionaccuracy detailed in Sec. 4.2). We employed the System Usability
(1: very poor, 5: excellent) and efficiency facilitation (1: Scale (SUS) [5] for scoring, using a Likert scale from 1
significantlyreduced,5: significantlyenhanced). Asshown (stronglydisagree)to5(stronglyagree),tocaptureaglobal
inFig.7,86.67%ofusersratedpredictionaccuracyatleast viewofsubjectiveusabilityforeachsystem.
4,validatingtheabilityofourfine-tunedMLLMtointerpret As shown in Fig. 8, our system demonstrated signifi-
user intentions. Similarly, 90% rated efficiency facilita- cantlyhigherscoresacrossalldimensionscomparedtothe
tion 4 or above, confirming that Draw&Guess effectively baseline. IndicatingtheeffectivenessofourIdeaCollector.
streamlinestheeditingprocessbyreducingmanualprompt Furtherdetailscanbefoundinthesupplementary.
inputs. Theaveragescoresforaccuracyandefficiencywere
5.Conclusion
4.07and4.37.
4.3.IdeaCollectionEffectivenessandEfficiency In conclusion, our interactive image editing system
MagicQuilleffectivelyaddressesthechallengesofper-
Collectinguserideaseffectivelyandefficientlyiscriticalfor forming precise and efficient edits by combining the
theusabilityandadoptionofinteractivesystems,especially strengths of the Editing Processor, Painting Assistor, and
in creative applications where user engagement is crucial. IdeaCollector.Ourcomprehensiveevaluationsdemonstrate
To evaluate the Idea Collector, we conducted a user study significant improvements over existing methods in terms
with30participants,comparingoursystemagainstabase- ofcontrollablegenerationquality, editingintentprediction
linesystemonthefollowingdimensions: accuracy, and user interface efficiency. For future work,
• ComplexityandEfficiencymeasureshowstreamlinedand we aim to expand the capabilities of our system by incor-
intuitivetheuserfindsthesystemforcreativeediting. porating additional editing types, such as reference-based
• ConsistencyandIntegrationassesseswhetherthesystem editing, which would allow users to guide modifications
maintainsacohesiveinterfaceandinteractiondesign. using external images. We also plan to implement layered
• Ease of Use captures the learnability of the system, image generation to provide better editing flexibility and
especiallyforuserswithvaryingbackgrounds. support for complex compositions. Moreover, enhancing
• Overall Satisfaction reflects users’ general satisfaction typography support will enable more robust manipulation
withthedesign,features,andusabilityofthesystem. of textual elements within images. These developments
Baseline. The baseline system was implemented as a cus- will further enrich our framework, offering users a more
tomized ComfyUI workflow, replacing our Idea Collector versatileandpowerfultoolforcreativeexpressionindigital
interface with an open-source canvas, Painter Node [40]. imageediting.
8References [14] ZigangGeng,BinxinYang,TiankaiHang,ChenLi,Shuyang
Gu,TingZhang,JianminBao,ZhengZhang,HouqiangLi,
[1] Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan,
Han Hu, et al. Instructdiffusion: A generalist modeling
AbdulrahmanAlfozan,andJamesZou. Gradio: Hassle-free
interfaceforvisiontasks. InProceedingsoftheIEEE/CVF
sharingandtestingofmlmodelsinthewild. arXivpreprint
Conference on Computer Vision and Pattern Recognition,
arXiv:1906.02569,2019. 6
pages12709–12720,2024. 1,2
[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-
[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
mad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and
JankoAltenschmidt, SamAltman, ShyamalAnadkat, etal.
YoshuaBengio. Generativeadversarialnetworks. Commu-
Gpt-4 technical report. arXiv preprint arXiv:2303.08774,
nicationsoftheACM,63(11):139–144,2020. 2,7
2023. 7,8
[16] Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian,
[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
HongyuLiu,XiaoweiChi,RuntaoLiu,RuibinYuan,Yazhou
Mitchell,DhruvBatra,CLawrenceZitnick,andDeviParikh.
Xing,WenhaiWang,etal.Llmsmeetmultimodalgeneration
Vqa:Visualquestionanswering.InProceedingsoftheIEEE
and editing: A survey. arXiv preprint arXiv:2405.19334,
international conference on computer vision, pages 2425–
2024. 2
2433,2015. 5
[17] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif-
[4] StephenBrade,BryanWang,MauricioSousa,SageevOore,
fusionprobabilisticmodels. Advancesinneuralinformation
and Tovi Grossman. Promptify: Text-to-image generation
processingsystems,33:6840–6851,2020. 2
through interactive prompt exploration with large language
[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
models.InProceedingsofthe36thAnnualACMSymposium
Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.
on User Interface Software and Technology, pages 1–14,
Lora: Low-rankadaptationoflargelanguagemodels. arXiv
2023. 3
preprintarXiv:2106.09685,2021. 6,1,2
[5] John Brooke et al. Sus-a quick and dirty usability scale.
[19] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi
Usabilityevaluationinindustry,189(194):4–7,1996. 8,3
Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen,
[6] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-
andLiangliangCao. Diffusionmodel-basedimageediting:
structpix2pix:Learningtofollowimageeditinginstructions.
Asurvey. arXivpreprintarXiv:2402.17525,2024. 1,2
In Proceedings of the IEEE/CVF Conference on Computer
[20] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan,
VisionandPatternRecognition,pages18392–18402,2023.
Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui
1,2
Huang,RuimaoZhang,etal. Smartedit:Exploringcomplex
[7] ComfyUI. Themostpowerfulandmodulardiffusionmodel
instruction-based image editing with multimodal large lan-
gui,apiandbackendwithagraph/nodesinterface. https:
guagemodels. InProceedingsoftheIEEE/CVFConference
//github.com/comfyanonymous/ComfyUI, 2024.
on Computer Vision and Pattern Recognition, pages 8362–
6
8371,2024. 1,2,6,7
[8] Jacob Devlin. Bert: Pre-training of deep bidirectional
[21] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perel-
transformers for language understanding. arXiv preprint
man,AdityaRamesh,AidanClark,AJOstrow,AkilaWeli-
arXiv:1810.04805,2018. 7,8
hinda,AlanHayes,AlecRadford,etal. Gpt-4osystemcard.
[9] RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,Zheng
arXivpreprintarXiv:2410.21276,2024. 2,7
Ge,JinrongYang,LiangZhao,JianjianSun,HongyuZhou,
[22] Youngjoo Jo and Jongyoul Park. Sc-fegan: Face editing
HaoranWei,etal. Dreamllm: Synergisticmultimodalcom-
prehensionandcreation. arXivpreprintarXiv:2309.11499, generativeadversarialnetworkwithuser’ssketchandcolor.
InProceedingsoftheIEEE/CVFinternationalconferenceon
2023. 2
computervision,pages1745–1753,2019. 2
[10] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and
Aleksander Holynski. Diffusion self-guidance for control- [23] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying
lable image generation. Advances in Neural Information Shan, and Qiang Xu. Brushnet: A plug-and-play image
ProcessingSystems,36:16222–16239,2023. 1,2 inpainting model with decomposed dual-branch diffusion.
arXivpreprintarXiv:2403.06976,2024. 1,2,3,4,5,6,7
[11] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi,
Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: [24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Diffusion transformer for image editing. arXiv preprint Elucidating the design space of diffusion-based generative
arXiv:2411.03286,2024. 2 models.Advancesinneuralinformationprocessingsystems,
[12] Yingchaojie Feng, Xingbo Wang, Kam Kwai Wong, Sijia 35:26565–26577,2022. 1
Wang,YuhongLu,MinfengZhu,BaichengWang,andWei [25] Kangyeol Kim, Sunghyun Park, Junsoo Lee, and Jaegul
Chen. Promptmagician: Interactivepromptengineeringfor Choo. Reference-based image composition with sketch
text-to-imagecreation. IEEETransactionsonVisualization via structure-aware diffusion model. arXiv preprint
andComputerGraphics,30(1):295–305,2024. 3 arXiv:2304.09748,2023. 2
[13] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, [26] DiederikPKingma.Auto-encodingvariationalbayes.arXiv
Yinfei Yang, and Zhe Gan. Guiding instruction-based preprintarXiv:1312.6114,2013. 4
imageeditingviamultimodallargelanguagemodels. arXiv [27] Hyung-Kwon Ko, Gwanmo Park, Hyeon Jeon, Jaemin Jo,
preprintarXiv:2309.17102,2023. 2 Juho Kim, and Jinwook Seo. Large-scale text-to-image
9generationmodelsforvisualartists’creativeworks. InPro- [41] Dustin Podell, Zion English, Kyle Lacey, Andreas
ceedingsofthe28thInternationalConferenceonIntelligent Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and
UserInterfaces,page919–933,NewYork,NY,USA,2023. Robin Rombach. Sdxl: Improving latent diffusion mod-
AssociationforComputingMachinery. 3 els for high-resolution image synthesis. arXiv preprint
[28] JingYuKoh,DanielFried,andRussRSalakhutdinov. Gen- arXiv:2307.01952,2023. 5
eratingimageswithmultimodallanguagemodels. Advances [42] Tiziano Portenier, Qiyang Hu, Attila Szabo, Siavash Ar-
inNeuralInformationProcessingSystems,36,2024. 2 jomand Bigdeli, Paolo Favaro, and Matthias Zwicker.
[29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Faceshop: Deep sketch-based face image editing. arXiv
Improved baselines with visual instruction tuning. In Pro- preprintarXiv:1804.08972,2018. 2
ceedingsoftheIEEE/CVFConferenceonComputerVision [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
andPatternRecognition,pages26296–26306,2024. 3 Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[30] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan AmandaAskell,PamelaMishkin,JackClark,etal.Learning
Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im- transferable visual models from natural language supervi-
proved reasoning, ocr, and world knowledge, 2024. 2, 6, sion.InInternationalconferenceonmachinelearning,pages
7 8748–8763.PMLR,2021. 7,8
[31] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Visual instruction tuning. Advances in neural information Patrick Esser, and Bjo¨rn Ommer. High-resolution image
processingsystems,36,2024. 2,4,5,7 synthesis with latent diffusion models. In Proceedings of
[32] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng the IEEE/CVF conference on computer vision and pattern
Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. recognition,pages10684–10695,2022. 2,3,1
Cones:Conceptneuronsindiffusionmodelsforcustomized [45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
generation. arXivpreprintarXiv:2303.05125,2023. 2 net: Convolutionalnetworksforbiomedicalimagesegmen-
[33] ZhihengLiu,YifeiZhang,YujunShen,KechengZheng,Kai tation. InMedicalimagecomputingandcomputer-assisted
Zhu,RuiliFeng,YuLiu,DeliZhao,JingrenZhou,andYang intervention–MICCAI2015: 18thinternationalconference,
Cao. Cones2: Customizableimagesynthesiswithmultiple Munich,Germany,October5-9,2015,proceedings,partIII
subjects. In Proceedings of the 37th International Con- 18,pages234–241.Springer,2015. 4
ference on Neural Information Processing Systems, pages [46] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
57500–57519,2023. 2 Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
[34] Weihang Mao, Bo Han, and Zihao Wang. Sketchffusion: Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
Sketch-guidedimageeditingwithdiffusionmodel. In2023 man,etal.Laion-5b:Anopenlarge-scaledatasetfortraining
IEEEInternationalConferenceonImageProcessing(ICIP), next generation image-text models. Advances in Neural
pages790–794.IEEE,2023. 2 InformationProcessingSystems,35:25278–25294,2022. 1
[35] Naoki Matsunaga, Masato Ishii, Akio Hayakawa, Kenji [47] ShellySheynin,AdamPolyak,UrielSinger,YuvalKirstain,
Suzuki,andTakuyaNarihira. Fine-grainedimageeditingby AmitZohar,OronAshual,DeviParikh,andYanivTaigman.
pixel-wiseguidanceusingdiffusionmodels. arXivpreprint Emu edit: Precise image editing via recognition and gen-
arXiv:2212.02024,2022. 1,2 erationtasks. InProceedingsoftheIEEE/CVFConference
[36] ChongMou,XintaoWang,JiechongSong,YingShan,and on Computer Vision and Pattern Recognition, pages 8871–
JianZhang.Dragondiffusion:Enablingdrag-stylemanipula- 8879,2024. 1,2
tionondiffusionmodels. arXivpreprintarXiv:2307.02421, [48] JaskiratSingh,JianmingZhang,QingLiu,CameronSmith,
2023. 2 ZheLin,andLiangZheng.Smartmask:Contextawarehigh-
[37] Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, fidelitymaskgenerationforfine-grainedobjectinsertionand
ChenyuZheng,andChongxuanLi.Theblessingofrandom- layoutcontrol. InProceedingsoftheIEEE/CVFConference
ness:Sdebeatsodeingeneraldiffusion-basedimageediting. on Computer Vision and Pattern Recognition, pages 6497–
arXivpreprintarXiv:2311.01410,2023. 6506,2024. 1,2
[38] Xingang Pan, Ayush Tewari, Thomas Leimku¨hler, Lingjie [49] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Liu, Abhimitra Meka, and Christian Theobalt. Drag your Denoising diffusion implicit models. arXiv preprint
gan: Interactivepoint-basedmanipulationonthegenerative arXiv:2010.02502,2020. 2
image manifold. In ACM SIGGRAPH 2023 Conference [50] Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao,
Proceedings,pages1–11,2023. 1,2 Qi Tian, Matti Pietika¨inen, and Li Liu. Pixel difference
[39] XiaohanPeng,JaninKoch,andWendyE.Mackay. Design- networks for efficient edge detection. In Proceedings of
prompt:Usingmultimodalinteractionfordesignexploration theIEEE/CVFinternationalconferenceoncomputervision,
with generative ai. In Proceedings of the 2024 ACM pages5117–5127,2021. 5
Designing Interactive Systems Conference, page 804–818, [51] Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin,
New York, NY, USA, 2024. Association for Computing Da-ChengJuan,DanaAlon,CharlesHerrmann,Sjoerdvan
Machinery. 3 Steenkiste,RanjayKrishna,etal.Dreamsync:Aligningtext-
[40] AlekseyPetrov.Comfyuicustomnodesalekpet.https:// to-image generation with image understanding feedback.
github.com/AlekPet/ComfyUI_Custom_Nodes_ In Synthetic Data for Computer Vision Workshop@ CVPR
AlekPet,2024. 8 2024,2023. 2
10[52] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong ceedings of the IEEE/CVF conference on computer vision
Zhang, YuezeWang, HongchengGao, JingjingLiu, Tiejun andpatternrecognition,pages5951–5961,2022. 2,6,7
Huang, and Xinlong Wang. Generative pretraining in [65] KaiZhang,LingboMo,WenhuChen,HuanSun,andYuSu.
multimodality. arXivpreprintarXiv:2307.05222,2023. 2 Magicbrush: A manually annotated dataset for instruction-
[53] QuanSun,YufengCui,XiaosongZhang,FanZhang,Qiying guided image editing. Advances in Neural Information
Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun ProcessingSystems,36,2024. 2
Huang,andXinlongWang. Generativemultimodalmodels [66] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
are in-context learners. In Proceedings of the IEEE/CVF conditional control to text-to-image diffusion models. In
Conference on Computer Vision and Pattern Recognition, ProceedingsoftheIEEE/CVFInternationalConferenceon
pages14398–14409,2024. 2 ComputerVision,pages3836–3847,2023. 2,3,4,5,7
[54] Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary [67] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
Williamson, Vasu Sharma, and Adriana Romero-Soriano. and Oliver Wang. The unreasonable effectiveness of deep
A picture is worth more than 77 text tokens: Evaluating featuresasaperceptualmetric. InProceedingsoftheIEEE
clip-style models on dense captions. In Proceedings of conference on computer vision and pattern recognition,
theIEEE/CVFConferenceonComputerVisionandPattern pages586–595,2018. 7
Recognition,pages26700–26709,2024. 5
[68] Xin Zhang, Jiaxian Guo, Paul Yoo, Yutaka Matsuo, and
[55] Jue Wang, Yuxiang Lin, Tianshuo Yuan, Zhi-Qi Cheng, YusukeIwasawa. Paste, inpaintandharmonizeviadenois-
Xiaolong Wang, Jiao GH, Wei Chen, and Xiaojiang Peng. ing: Subject-drivenimageeditingwithpre-traineddiffusion
Flexedit: Marrying free-shape masks to vllm for flexible model. arXivpreprintarXiv:2306.07596,2023. 1,2
imageediting. arXivpreprintarXiv:2408.12429,2024. 2
[69] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan,
[56] ZhijieWang,YuhengHuang,DaSong,LeiMa,andTianyi
andKaiChen. Ataskisworthoneword:Learningwithtask
Zhang. Promptcharm: Text-to-image generation through
prompts for high-quality versatile image inpainting. arXiv
multi-modal prompting and refinement. In Proceedings
preprintarXiv:2312.03594,2023. 1,2
of the CHI Conference on Human Factors in Computing
Systems,NewYork,NY,USA,2024. 3
[57] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu.
Genartist: Multimodal llm as an agent for unified image
generation and editing. arXiv preprint arXiv:2407.05600,
2024. 2
[58] ShengqiongWu,HaoFei,LeigangQu,WeiJi,andTat-Seng
Chua. Next-gpt:Any-to-anymultimodalllm. arXivpreprint
arXiv:2309.05519,2023. 2
[59] Chufeng Xiao and Hongbo Fu. Customsketching: Sketch
concept extraction for sketch-based image synthesis and
editing. arXivpreprintarXiv:2402.17624,2024. 2
[60] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu,
Stefano Ermon, and CUI Bin. Mastering text-to-image
diffusion: Recaptioning, planning, and generating with
multimodalllms. InForty-firstInternationalConferenceon
MachineLearning,2024. 2
[61] ShuaiYang, ZhangyangWang, JiayingLiu, andZongming
Guo. Deepplasticsurgery: Robustandcontrollableimage
editing with human-drawn sketches. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK,
August23–28, 2020, Proceedings, PartXV16, pages601–
617.Springer,2020. 2
[62] Yifan Yang, Houwen Peng, Yifei Shen, Yuqing Yang, Han
Hu, Lili Qiu, Hideki Koike, et al. Imagebrush: Learn-
ingvisualin-contextinstructionsforexemplar-basedimage
manipulation. Advances in Neural Information Processing
Systems,36,2024. 1,2
[63] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin,
Chung-ChingLin,ZichengLiu,andLijuanWang.Idea2img:
Iterative self-refinement with gpt-4v (ision) for auto-
matic image design and generation. arXiv preprint
arXiv:2310.08541,2023. 2
[64] YuZeng, ZheLin, andVishalMPatel. Sketchedit: Mask-
freelocalimagemanipulationwithpartialsketches. InPro-
11MagicQuill: An Intelligent Interactive Image Editing System
Supplementary Material
A.ImplementationDetails cutting a slice out of it, as shown in Fig 2. The user
begins by uploading the image through the toolbar, which
A.1.EditingProcessor
providesaccesstoarangeoftools(Fig.2-B).Usingtheadd
Our Editing Processor is built upon Stable Diffusion brush, the user outlines the slice to be cut directly on the
v1.5[44]andiscompatiblewithallcustomizedfine-tuned canvas (Fig. 2-D). Meanwhile, the Draw & Guess feature
weights. We set the control parameters with inpainting introduced in Sec. 3.2 predicts that the user intends to
strength w = 1.0 and control strength w = 0.5, while manipulate a “cake” and suggests the relevant prompt
I C
expandingthemaskregionby15pixelsduringcontrollable automaticallyinthepromptarea(Fig.2-A).Afterward,the
inpainting. The generation process employs the Euler user switches to the subtract brush to fill in the outlined
ancestral sampler with Karras scheduler [24], requiring 20 slice, visually marking the area to be removed from the
steps per generation. On standard hardware, generating cake. For additional precision, the eraser tool is available
a 512 × 512 resolution image takes approximately 2 sec- to refine the cut. Once the adjustments are made, the user
onds with 15 GB VRAM consumption. For the control generates the image by clicking the Run button (Fig. 2-F),
branch, we conduct fine-tuning on the LAION-Aesthetics whichrunsthemodeldetailedinSec.3.1.
dataset [46], specifically selecting images with aesthetic The resulting image appears in the generated image
scoresabove6.5. Thetrainingprocessspans3epochswith area (Fig. 2-E). Users can confirm changes via the tick
alearningrateof5e−6andbatchsizeof8. icon to update the canvas, or click the cross icon to revert
modifications. This workflow enables iterative refinement
A.2.PaintingAssistor
ofedits,providingflexiblecontrolthroughouttheprocess.
We fine-tune a LLaVA-1.5 model with 7B parameters
B.FailureCase
for Draw&Guess task on our own constructed dataset in
Sec. 3.2, leveraging LoRA [18]. The LoRA rank and
B.1.FailureCaseofEditingProcessor
alpha are 64 and 16 respectively. The model is trained
for 3 epochs with a learning rate of 2e − 5 and batch Scribble-Prompt Trade-Off. We observe quality degra-
size of 8. Under 4-bit quantization, the model achieves dation when user-provided add brush strokes deviate from
real-timepromptinferencewithin0.3secondsusingonly5 the semantic content specified in the prompt, a common
GBVRAM,enablingefficienton-the-flypromptgeneration occurrence among users with limited artistic skills. This
withsatisfactoryaccuracy. creates a fundamental trade-off: strictly following the
scribble structure may compromise the generation quality
A.3.IdeaCollector
with respect to the text prompt. To address this issue, we
Cross-platformSupport. BesidesGradio,MagicQuill proposeadjustingtheedgecontrolstrength.
can also be integrated into ComfyUI as a custom node, as
shown in Fig. 9. It is designed with customizable widgets
forparametersettingsandextensiblearchitectureforfuture
platformintegrations.
(a)User’sInput (b)EdgeStrength:0.6(c)EdgeStrength:0.2
Figure10. IllustrationoftheScribble-PromptTrade-Off. Given
user-provided brush strokes (a) with the text prompt “man”, we
showgenerationresultswithdifferentedgecontrolstrengths: (b)
Figure9.MagicQuillasacustomnodeinComfyUI. withstrengthof0.6and(c)withstrengthof0.2.
Usage Scenario. To demonstrate the user-friendly work- As demonstrated in Fig. 10, when presented with an
flowofMagicQuill,wepresentanillustrativescenario: oversimplified sketch that substantially deviates from the
A user wants to modify an image of a complete cake, prompt“man”,ahighedgestrengthof0.6producesresults
1that, while faithful to the sketch, appear inharmonious.
By reducing the edge strength to 0.2, we achieve notably
improvedgenerationquality.
Colorization-Details Trade-Off. We observe a trade-
off between colorization accuracy and detail preservation.
Since our conditional image inpainting pipeline relies on
downsampled color blocks and CNN-extracted edge maps
as input, structrual details in the edited regions may be
(a)User’sInput (b)Prompt:Candy (c)Prompt:Raspberry
compromisedduringthegenerationprocess.
Figure12. Demonstrationofsemanticambiguityinsketchinter-
pretation.(A)User’ssketchintendedtorepresentaraspberry;(B)
OurDraw&Guessmodelincorrectlyinterpretsthesketchascandy,
leadingtoamisalignedgeneration; (C)Theexpectedgeneration
resultwithcorrectraspberryinterpretation.
misaligned generations that deviate from the user’s expec-
tations. Fortunately,ouruserstudyrevealsthatparticipants
were generally understanding of such interpretation errors
and considered the model’s predictions to be reasonable
(a)OriginalImage (b)Colorbrush,α1.0 (c)Resultforα1.0
attemptsatdisambiguatingtheirsketches.
C.GeneralizabilityofEditingProcessor
OurEditingProcessordemonstratesgeneralizationcapabil-
itiesacrossvariousfine-tunedStableDiffusionv1.5models.
Since both the inpainting and control branches preserve
the weights of pre-trained diffusion models, our method
seamlesslyintegrateswithanycommunityfine-tunedmodel
(d)Colorbrush,α0.8 (e)Resultforα0.8 asaplug-and-playcomponent. Wevalidatethisversatility
Figure 11. Illustration of the Colorization-Detail Trade-Off. by testing on several popular fine-tuned models including
Resultsofcolorbrushstrokeswithdifferentalphavalues: (b, c) RealisticVision, GhostMix, and DreamShaper, achieving
usingalphavalue1.0,and(d,e)usingalphavalue0.8,wherethe consistent editing performance while inheriting the unique
latterbetterpreservesmorestructuraldetailsoftheoriginalimage.
stylisticcharacteristicsofeachmodel,asshowninFig.13.
This compatibility highlights the practical value of our
As illustrated in Fig. 11, this limitation can be partially
EditingProcessor,asuserscanleveragetheirpreferredfine-
mitigated by reducing the alpha value of the color brush
tuned models or LoRA [18] weight while maintaining the
trokes,whichpreservesmoreinformationfromtheoriginal
editingcapabilitiesprovidedbyourframework.
image when downsampled to color blocks. Future work
couldexploreusinggrayscaleimagesasthecontrolcondi-
tiontoachievecolorizationwhilemaintainingfine-grained
D.In-ContextEditingIntentInterpretation
structuraldetails.
The MLLM in Painting Assistor, fine-tuned on our own
B.2.FailureCaseofPaintingAssistor
constructed dataset in Sec. 3.2, demonstrates sophisticated
Ambiguity of the Brush Strokes. Our system enables in-contextreasoningcapabilitiesforeditingintentinterpre-
users to express their editing intentions through brush tation. The model effectively leverages contextual visual
strokes,whicharetheninterpretedbythePaintingAssistor information to interpret user brush strokes based on their
via Draw&Guess. However, this approach faces inherent surrounding environment. For instance, a simple vertical
limitations due to the ambiguous nature of user-provided line is interpreted differently based on its context: as a
sketches. For instance, a simple circular sketch could candle on a cake, a column on ruins, or an antenna on
represent various objects like strawberry, raspberry, or a robot, as illustrated in Fig. 14. These context-aware
candy, making it challenging for the model to accurately interpretations validate the effectiveness of our dataset
infertheuser’sintendedmodification,asshowninFig.12. construction approach and highlight the model’s ability to
This ambiguity in sketch interpretation can lead to incorporateenvironmentalcuesinitsreasoningprocess.
2Figure15.ThebaselinesystemimplementedinComfyUI.
editing experience, with varying skill levels, providing a
realisticrangeofuserproficiency.
To control for learning effects, we randomly divided
participantsintotwogroups: GroupAusedMagicQuill
before the baseline (Fig. 15), while Group B followed the
reverseorder. Eachparticipantcompletedacomprehensive
evaluationconsistingof10questionspersystem, modified
from the System Usability Scale (SUS) [5], spanning four
key categories: Complexity and Efficiency, Consistency
and Integration, Ease of Use, and Overall Satisfaction. .
The detailed evaluation results are presented in Fig. 16.
Additionally,participantsrespondedto2specificquestions
addressing the Painting Assistor’s accuracy and efficiency
detailedinSec.4.2.
In the Ease of Use category, all participants rated the
(a)OriginalImage (b)User’sInput (c)EditingResult
easiness (Q1) with a score of 3 or above, and most
Figure 13. Demonstration of our method’s generalization capa-
reported learning our system more quickly (Q3, Q4) and
bilityacrossdifferentfine-tunedStableDiffusionmodels.Results
independently (Q2) compared to the baseline. These
shown using RealisticVision (top row), GhostMix (middle row),
findings indicate a lower barrier to entry for creative tasks
and DreamShaper (bottom row) as base models, all achieving
with our system. For Complexity and Efficiency, 80%
consistenteditingperformance.
of participants found our system’s complexity appropriate
(Q5),contrastingwithperceptionsofexcessivecomplexity
in the baseline. Additionally, 83.3% felt our system was
smooth to use (Q6), suggesting that our design lowered
cognitive load and supported efficient task completion.
In Consistency and Integration, 80% agreed on effective
featureintegration(Q7),and90%ofparticipantsagreedthat
oursystemwasconsistentandcoherent(Q8).Thisfeedback
(a)Guess:Antenna (b)Guess:Candle (c)Guess:Column
suggestsoursystemprovidedacohesiveandintuitiveuser
Figure 14. Examples of context-aware editing intention inter- experience. Lastly,forOverallSatisfaction,93%expressed
pretation. The MLLM interprets the same vertical line sketch willingness to use our system (Q9), and 83% reported
differently based on surrounding context: (a) as an antenna on
confidence in using it (Q10). This high satisfaction rate
a robot’s head, (b) as a candle on a birthday cake, and (c) as a
reflects positive user reception and highlights the system’s
columnamongancientruins.
overalleffectivenessinmeetinguserexpectationsinediting.
E.UserStudyDetailsandQuestionnaires The system’s ability to maintain user engagement was
evidenced by users voluntarily extending their editing ses-
To assess the effectiveness and usability of the Painting sions beyond the allocated time. After minimal training,
Assistor and Idea Collector, we recruited 30 participants users were able to create compelling edits, demonstrating
fromdiversebackgrounds,includingpostgraduatestudents, thesystem’saccessibilityandeaseofuse.Agalleryofuser-
artists, and computer vision researchers. All had image editedimagesispresentedinFig.17.
3Figure16.ThequestionnaireanduserratingscomparingMagicQuilltothebaselinesystem(1=stronglydisagree,5=stronglyagree).
Figure17. AgalleryofcreativeimageeditingachievedbytheparticipantsoftheuserstudyusingMagicQuill. Eachpairshowsthe
originalimageanditseditedversion,demonstratingdiverseuser-drivenmodifications.
4