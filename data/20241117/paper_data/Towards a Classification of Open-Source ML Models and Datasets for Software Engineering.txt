Towards a Classification of Open-Source ML
Models and Datasets for Software Engineering
Alexandra Gonza´lez Xavier Franch
Universitat Polite`cnica de Catalunya Universitat Polite`cnica de Catalunya
Barcelona, Spain Barcelona, Spain
alexandra.gonzalez.alvarez@upc.edu xavier.franch@upc.edu
David Lo Silverio Mart´ınez-Ferna´ndez
Singapore Management University Universitat Polite`cnica de Catalunya
Singapore Barcelona, Spain
silverio.martinez@upc.edu silverio.martinez@upc.edu
Abstract—Background: Open-Source Pre-Trained Models needs, this research aims to make the selection of PTMs and
(PTMs) and datasets provide extensive resources for various datasets more relevant and effective for SE practitioners and
Machine Learning (ML) tasks, yet these resources lack a classi-
researchers,thusaddressingacriticalneedwithinthefield[5].
fication tailored to Software Engineering (SE) needs. Aims: We
The main contributions of this work are: (a) proposing
apply an SE-oriented classification to PTMs and datasets on a
popular open-source ML repository, Hugging Face (HF), and and proving the feasibility of a preliminary classification
analyzetheevolutionofPTMsovertime.Method:Weconducted framework for PTMs and datasets hosted on HF, tailored
a repository mining study. We started with a systematically to SE needs; (b) providing advanced analysis, including the
gathered database of PTMs and datasets from the HF API.
exploration of the relationship between SE activities and ML
Our selection was refined by analyzing model and dataset cards
tasks, as well as the evolution of SE PTMs over time; (c)
and metadata, such as tags, and confirming SE relevance using
Gemini 1.5 Pro. All analyses are replicable, with a publicly presenting a reproducible pipeline that accesses the HF API,
accessible replication package. Results: The most common SE filters, refines, and classifies resources on specific SE tasks.
task among PTMs and datasets is code generation, with a Data availability statement: All research components,
primary focus on software development and limited attention to
including the original and preprocessed data, along with all
software management. Popular PTMs and datasets mainly target
scripts for data collection, preparation, and analysis, are pub-
software development. Among ML tasks, text generation is the
most common in SE PTMs and datasets. There has been a licly available on Zenodo [6]. This ensures transparency and
marked increase in PTMs for SE since 2023 Q2. Conclusions: enablesindependentreplicationofthestudy,whichisessential
This study underscores the need for broader task coverage to for updating the classification as new open-source PTMs and
enhance the integration of ML within SE practices.
datasets are constantly being released.
Index Terms—Pre-trained models for Software engineering,
Software engineering datasets, Hugging Face II. RELATEDWORK
A systematic literature review conducted by Hou et al. [7]
I. INTRODUCTION
analyzed 395 research papers from January 2017 to January
The fast expansion of open-source platforms like Hugging 2024andcategorizedLargeLanguageModels(LLMs)intoSE
Face (HF) [1] has enhanced access to Machine Learning tasks. These tasks were grouped into SE activities according
(ML) models and datasets, driving advancements across var- to the six phases of the Software Development Life Cycle:
ious domains. With a consistent and significant uptrend in requirements engineering, software design, software develop-
development activities on HF [2], it is distinguished by its ment, software quality assurance, software maintenance, and
vast collection of Pre-Trained Models (PTMs), compared to software management. Di Sipio et al. [5] highlighted the lack
other platforms [3] [4]. However, the categorization of these of a SE classification of PTMs on HF, as the existing one is
resources overlooks the specific needs of Software Engineer- specificforML.Toaddressthisgap,theyproposedextracting
ing (SE). SE tasks frequently involve code generation, code information from the model cards [8] and using a semi-
analysis, and bug detection, which differ significantly from automatedmethodtoidentifySEtasksandtheircorresponding
thetaskscommonlyaddressedbygeneral-purposeMLmodels PTMs from the literature. However, they only tested the
suchasobjectdetectionorimagesegmentation.Therefore,the mapping on three PTMs: BERT, RoBERTa, and T5. Yang et
motivation for this work is to address this gap, as the absence al. [9] analyzed the ecosystem of LLMs as of August 2023,
of SE-specific categorization limits the efficient application curating 366 models and 73 datasets from HF for SE. The
of ML in SE tasks, potentially slowing down SE innovation. study also explores the use of LLMs to assist in constructing
By providing a framework that aligns ML tasks with SE and analyzing the ecosystem, which increased the model size
4202
voN
41
]ES.sc[
1v38690.1142:viXraTABLE I: Inclusion criteria.
Data Data Preparation Models
Collection Initial
Refinement for SE Relevance Datasets
Filtering
Hugging Get models & Remove models & Get HF 1) PTMsanddatasetsmustbeavailableonHF.
F 1,a 0c 6e 0 A ,4P 19I tad sa kt sa s inet ts h w eii rt h c aS rE ds dat kas ee yt ws ola rc dk sing i Gns ei tg Sh Ets Merge 2,009 2 3) ) P TT heM cs ara dn sd mda ut sa tse spts ecm ifu yst atco len ata stin onv eali Sd Em tao sd kel [7a ]n .ddatasetcards.
229,767 10,077 1,836 7,395 982 activities 358 4 5) ) T Ah ne Lc Lar Mds (m Geu mst inin ic 1lu .5de Pr“ oc )od me u” stor co“ nso fif rt mwa rr ee l” e. vancetoSE.
Fig. 1: Data collection and preparation pipeline.
1) Data Collection: WeusedtheHFAPI[11]togatherall
by 16.5. They focus on code-based LLMs, which represent a available resources as of October 19, 2024: 1,060,419 PTMs
more specific scope compared to LLMs for broader SE tasks. and 229,767 datasets. During this process, we collected the
Despite the above notable advances, a comprehensive unique identifiers for each model and dataset, and assessed
framework that organizes PTMs and datasets on HF with the availability of their cards.
a strong SE orientation remains absent. In contrast to prior 2) Data Preparation:
works,thispaperadoptsaSEperspectivetoextendtheexisting a) Initial Filtering: We automatically filtered PTMs and
taxonomy in Hou et al. [7], to encompass a broader and more datasets mentioning SE tasks proposed by Hou et al. [7] in
diverse set of PTMs, as well as datasets, categorizing them their cards. This step enabled us to focus on SE-relevant
according to specific SE tasks and activities. By analyzing a resources, resulting in 10,077 PTMs and 1,836 datasets.
substantially larger set of PTMs and SE-relevant datasets on
b) Refinement for SE Relevance: To ensure our focus
HF, we address the unique SE-specific requirements unmet
on SE, we removed entries that did not contain “code” or
by previous studies, offering a novel, exhaustive preliminary
“software” in the model and dataset cards, resulting in 7,395
classification framework for the community.
PTMS and 982 datasets. For this subset, we retrieved all
available information from HF and mapped the SE task to
III. METHODOLOGY thecorrespondingSEactivity.Furthermore,weusedanLLM,
Gemini 1.5 Pro [12], to identify whether each resource was
A. Research Objectives
intended for SE based on an analysis of their cards and
Following the Goal Question Metric (GQM) template [10], metadata. As a validation step, we manually classified a
our goal is to analyze PTMs and datasets for the purpose balanced sample of 30 PTMs and 30 datasets (between SE-
of their classification with respect to their application to relevant and non-SE), and compared our decisions with those
SE tasks and activities from the point of view of software generatedbytheLLMusingCohen’sKappa[13].Thissample
engineers in the context of the HF Hub. size was chosen to ensure reliable initial validation, as higher
This goal is structured around four RQs. First, we need levelsofagreementareanticipated[14].ForPTMs,theKappa
to assess the quality of model and dataset cards, as this was 0.80, which falls within the 0.61 to 0.80 range indicating
information is essential for the subsequent RQs: substantial agreement [15]; for datasets, it was 0.70, also
• RQ1: What is the status of the model and dataset cards? reflecting this level of agreement. We noticed that some SE
tasks, such as logging and verification, could be ambiguous.
Next, we explore how the selected resources tackle SE:
For instance, a model card containing a code snippet like
• RQ2: How do PTMs and datasets address SE? from transformers import logging might be mis-
- RQ2.1: What SE tasks and activities are covered by classified as a logging task when it may not be. To ensure
PTMs and datasets? rigor,weaskedtheLLMtoclassifythembyprovidingexplicit
- RQ2.2: What are the most popular PTMs and datasets definitions of such tasks. Lastly, we applied the prompts
that address SE activities? to all resources, resulting in 2,009 PTMs and 358 datasets,
Followingthis,weexploretheconnectionbetweenMLtasks representing 0.19% and 0.16% of the original sets.
in HF’s classification and SE activities: 3) Data Analysis: InexploringthelandscapeofPTMsand
datasets within HF, we centered our analysis on their cards,
• RQ3: How are ML tasks related to SE activities?
as well as on metadata such as tags, creation dates, and other
Lastly,weexaminethelong-termrelevanceofourfindings:
relevant attributes. Additionally, we define popularity as the
• RQ4: How stable is this information over time? sum of the normalized number of likes and downloads.
B. Data collection and Preparation
IV. RESULTS
A. What is the status of the model and dataset cards? (RQ1)
Figure 1 illustrates the pipeline we followed to collect
and prepare data for analysis. This process is designed for As summarized in Table II, over 33% of the PTMs lack a
reproducibility, allowing anyone with access to the replication model card, highlighting a gap in the documentation. Further-
package [6] to validate and update the results. The inclusion more, more than 65% do not mention any SE tasks, and only
criteria are summarized in Table I. 0.95% mention SE tasks. The analysis for datasets reveals aTABLE II: Summary of model cards.
System design 1
Rapid prototyping 1 SE Activity
Category NumberofPTMs Proportion Requirements classification 2 Software management (0)
Notavailable 350,524 33.05% Co Sd ee n c til mo Cn ee on dtd ee a t n re eac vlt y ii eso win s 333 S Ro ef qt uw ia rere m d ee ns tsig en n ( g2 i) neering (2)
Availablebutempty 3,686 0.35% Program repair 8 Software maintenance (45)
NoSEtasks 696,132 65.65% Fault lD oce ab lu izg ag ti in ong 1 28 Software quality assurance (65)
Decompilation 1 Software development (326)
WithSEtasks 10,077 0.95% Testing automation 1
Test generation 2
Bug localization 4
Defect detection 4
TABLE III: Summary of dataset cards. Static analysis 7
Vulnerability detection 14
Verification 31
Instruction generation 2
Category Numberofdatasets Proportion Type inference 3
Code representation 4
Notavailable 64,210 27.95% Code editing 6
Availablebutempty 1,294 0.56% CodeC o sd ue m t mra an ris zl aa tt ii oo nn 1 11 2
NoSEtasks 162,427 70.69% Co Pd re o gu rn ad me r ss yt na tn hd ei sn ig s 1 15 8
WithSEtasks 1,836 0.80% Data analysis 22
Code search 30
Code completion 39
Code generation 164
0 25 50 75 100 125 150 175
Number of Datasets
lack of documentation, as detailed in Table III. Over 27% of
Fig. 3: Datasets associated with each SE task and SE activity.
datasets do not have a dataset card, indicating a significant
gap in available information. Additionally, 0.56% are empty,
and a staggering 70.69% do not reference any SE task. Only
The leading SE task is code generation, which has over three
0.80% of the datasets allude to SE tasks.
times the number of datasets compared to the next task.
Finding 1:ThecurrentstateofHFshowsthatonly10,077
Interestingly, the focus of secondary SE activity diverges
PTMs and 1,836 datasets mention SE tasks in their cards.
between PTMs and datasets: software maintenance is the sec-
ond SE activity most addressed for PTMs, while for datasets,
B. How do PTMs and datasets address SE? (RQ2)
software quality assurance holds this position.
1) What SE tasks and activities are covered by PTMs and Finding 2.1: Code generation is the most covered SE task
datasets?: Figure 2 shows the distribution of PTMs across among PTMs and datasets.
SE tasks, color-coded by SE activity. The top tasks are code Finding 2.2: Software development dominates, while soft-
generation and code completion, with the first having nearly ware management is absent in PTMs and datasets.
tentimesasmanyPTMsasthethirdmostcommon.Themost
2) What are the most popular PTMs and datasets that
represented activity is software development, while software
address SE activities?: As shown in Figure 4, the three most
design and requirements engineering have limited PTMs.
popular PTMs across SE activities reveal that software devel-
Additionally, the absence of PTMs for software management
opmenthasahigherpopularityduetoitsbroadrepresentation.
highlights a critical gap in the current landscape.
Figure 5 presents the three most popular datasets for soft-
Regarding datasets, Figure 3 shows that software devel-
ware development, software quality assurance, and software
opment dominates, while software design and requirements
maintenance. Other SE activities are omitted, as their dataset
engineering are underrepresented, and software management
popularity is exactly 0. Popularity values for datasets tend
isentirelyabsent,mirroringthepatternsobservedwithPTMs.
to be higher, which may suggest that the community places
more value on datasets than on PTMs, potentially indicating
a higher demand for quality datasets in comparison to PTMs.
Requirements classification 3
System design 17 SE Activity In other words, researchers might perceive a good dataset as
Rapid prototyping 17 Software management (0)
Testing automation1 Requirements engineering (3) more valuable or essential for advancing SE activities than a
Test prediction1
Static analysis2 Software design (34) well-performing model.
Verification 10 Software quality assurance (173)
Decompilation 16 Software maintenance (247)
Vulner Dab ei fl eit cy t dd ee tt ee cc tt ii oo nn 3 45 2 Software development (2035) Finding2.3:ThemostpopularPTMsanddatasetspredom-
Test generation 66
Code revision1 inantly address software development.
Duplicate bug report detection1
Code clone detection 3 Finding 2.4:DatasetstendtobemorepopularthanPTMs,
Program repair 4
Code review 43 suggesting a higher demand for quality datasets in SE.
Debugging 90
Sentiment analysis 105
Type inference1
Code recommendation1
InstructioC n o gd ee n ee rd ait ti in ong2 6 C. How are ML tasks related to SE activities? (RQ3)
Code representation 10
Code comment generation 11
API recommendation 13 The relationship between SE activities and ML tasks for
Code search 54
CodeC o sd ue m t mra an ris zl aa tt ii oo nn 6 73 1 PTMs and datasets is illustrated in Figures 6 and 7, re-
Code understanding 87
Data analysis 101 spectively, with the flow indicating the number of resources.
Program synthesis 101
Code completion 581 Both figures highlight the prominence of the ML task text
Code generation 933
0 200 400 600 800 1000 generation, which is associated with the largest number of
Number of PTMs
PTMs and datasets. This task is most commonly linked to
Fig. 2: PTMs associated with each SE task and SE activity.
software development from the SE perspective, though it also
ksaT
ES
ksaT
ESSoftware development
sentence-transformers/all-mpnet-base-v2 1.18
bigscience/bloom 1.0
microsoft/Phi-3-mini-128k-instruct 0.34
Software maintenance
THUDM/codegeex2-6b 0.05
Falconsai/text_summarization 0.04
aaditya/Llama3-OpenBioLLM-8B 0.03
Software design
arcee-ai/Arcee-Spark 1.80e-02
arcee-ai/Arcee-Spark-GGUF5.70e-03
LeroyDyer/_Spydaz_Web_AI_1.05e-03
Software quality assurance
WisdomShell/CodeShell-7B 0.02 Salesforce/codet5-small 0.01
ibm-granite/granite-3b-code-base-2k0.01
Fig.7:AssociationofSEactivitieswithMLtasksfordatasets.
Requirements engineering
thearod5/bert4re2.12e-04
thearod5/se-bert4.22e-07
thearod5/pl-bert-siamese-encoder0.00e+00
0 1
Popularity
Fig. 4: Top 3 most popular PTMs per SE activity.
Software development
nuprl/MultiPL-E 1.06
THUDM/LongBench 0.51
argilla/FinePersonas-v0.1 0.45
Software quality assurance
BAAI/COIG 0.61
princeton-nlp/SWE-bench 0.44 Fig. 8: Number of SE PTMs created since 2020.
princeton-nlp/SWE-bench_Verified 0.41
Software maintenance
argilla/magpie-ultra-v0.1 0.3
CyberNative/Code_Vulnerability_Security_DPO 0.08
Magpie-Align/Magpie-Reasoning-150K 0.07
0.00 0.25 0.50 0.75 1.00 1.25 included in the figure to provide a closer examination. While
Popularity
the ranking of activities remains relatively stable over time,
Fig. 5: Top 3 most popular datasets per SE activity.
there are some fluctuations, particularly in 2023 Q3 and 2024
Q1, where software quality assurance outperformed software
maintenance, albeit with a less pronounced change in 2024.
supportsotherSEactivities.Incontrast,otherMLtasksexhibit
considerably smaller flows. Finding 4.1: PTMs for SE have grown consistently since
2020, with notable acceleration from 2023 Q2.
Finding 3: The ML task text generation is the most
Finding 4.2: The ranking of SE tasks remains stable over
common among SE-related PTMs and datasets.
time, with minor fluctuations.
D. How stable is this information over time? (RQ4)
Figure 8 shows a growth in the creation of PTMs for SE V. DISCUSSIONANDLIMITATIONS
tasks since 2020, with quarterly data revealing periods of
OurresultsalignwithHouetal.[7],whichshowsasimilar
accelerated development. Notably, the percentage of PTMs
for software development increased from 6.78% in 2023 Q2 distribution of LLMs across SE activities, with both studies
to 20.55% currently. Similarly, software maintenance and identifying an emphasis on code generation tasks. However,
software quality assurance also showed significant growth, we have found a gap in software maintenance within the
current state of HF. Notably, our findings complement Yang
with the former rising from 1.70% to 32.34%, and the latter
et al.’s analysis [9], highlighting a rapid growth in this area.
increasing from 1.19% to 28.57% over the same period. A
zoom-in view of this period, from 2023 Q2 to 2024 Q3, is Potentialthreatsimpactingourstudy’svalidityareoutlined.
First, the quality of the model and dataset cards may compro-
miseinternalvalidity,asincompletedocumentationcouldlead
to misclassification. Our conclusions rely on the assumption
that each PTM and dataset’s card accurately reflects the SE
tasks it addresses. Second, other platforms/repositories (e.g.,
PyTorch Hub [16]) may host additional relevant resources,
and changes on HF could affect the broader applicability of
our conclusions. Third, our classification of SE tasks and
activities is based on a taxonomy from the existing literature
[7]. Although this taxonomy informs our analysis, emerging
standards may further enrich our understanding of SE. To
mitigate these threats, our study is fully replicable, enabling
Fig. 6: Association of SE activities with ML tasks for PTMs.
future researchers to validate and extend our findings.
DI
ledoM
DI
tesataDVI. CONCLUSIONANDFUTUREWORK [3] L. Gong, J. Zhang, M. Wei, H. Zhang, and Z. Huang, “What is the
intendedusagecontextofthismodel?anexploratorystudyofpre-trained
This study examines the availability of PTMs and datasets
modelsonvariousmodelrepositories,”ACMTransactionsonSoftware
for SE hosted on HF, revealing that almost 1% reference EngineeringandMethodology,vol.32,no.3,pp.1–57,2023.
SE tasks. PTMs predominantly focus on code generation [4] W. Jiang, J. Yasmin, J. Jones, N. Synovic, J. Kuo, N. Bielanski,
Y. Tian, G. K. Thiruvathukal, and J. C. Davis, “Peatmoss: A dataset
and code completion, while code generation is the most
and initial analysis of pre-trained models in open-source software,” in
representeddatasettask.SEactivitieslikesoftwaredesignand Proceedings of the 21st International Conference on Mining Software
requirements engineeringare under-represented,with agap in Repositories, ser. MSR ’24. New York, NY, USA: Association
for Computing Machinery, 2024, p. 431–443. [Online]. Available:
software management resources. SE datasets tend to be more
https://doi.org/10.1145/3643991.3644907
popular than PTMs. The most prevalent ML task across both [5] C. Di Sipio, R. Rubei, J. Di Rocco, D. Di Ruscio, and P. T.
PTMs and datasets is text generation. Additionally, there has Nguyen, “Automated categorization of pre-trained models in software
engineering:AcasestudywithaHuggingFacedataset,”inProceedings
been a significant surge in PTMs for SE since 2023 Q2. This
of the 28th International Conference on Evaluation and Assessment
snapshot of the current landscape highlights the existing gaps in Software Engineering, ser. EASE ’24. New York, NY, USA:
in SE resources and provides valuable insight into the field’s Association for Computing Machinery, 2024, p. 351–356. [Online].
Available:https://doi.org/10.1145/3661167.3661215
evolving needs, helping to motivate future research efforts
[6] Anonymous, “Replication package for “classifying open-source pre-
aimed at addressing these underrepresented areas. trained models and datasets for software engineering”,” Nov. 2024.
Forfuturework,weplantoextendthisclassificationtoother [Online].Available:https://zenodo.org/records/14057757
[7] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo,
repositories,suchasPaperswithCode[17],PyTorchHub[16] D. Lo, J. Grundy, and H. Wang, “Large Language Models for
and TensorFlow Hub [18], allowing for a broader analysis Software Engineering: A Systematic Literature Review,” ACM Trans.
Softw. Eng. Methodol., Sep. 2024, just Accepted. [Online]. Available:
of PTMs and datasets in SE. In addition, we aim to refine
https://doi.org/10.1145/3695988
the classification process by addressing current limitations, [8] M.Mitchell,S.Wu,A.Zaldivar,P.Barnes,L.Vasserman,B.Hutchin-
such as handling synonymous in SE tasks, to improve accu- son, E. Spitzer, I. D. Raji, and T. Gebru, “Model cards for model
reporting,”inProceedingsoftheconferenceonfairness,accountability,
racy. To make our classification more actionable, we propose
andtransparency,2019,pp.220–229.
developing a dashboard to assist researchers with proper [9] Z. Yang, J. Shi, P. Devanbu, and D. Lo, “Ecosystem of Large
samplingandsupportpractitionersintheSEcommunitywhen Language Models for Code,” 2024. [Online]. Available: https:
//arxiv.org/abs/2405.16746
selectingPTMsforreal-worldapplications,therebyenhancing
[10] V.R.B.G.CaldieraandH.D.Rombach,“TheGoalQuestionMetric
the relevance and impact of our work. approach,”Encyclopediaofsoftwareengineering,pp.528–532,1994.
[11] Hugging Face Inc., “Hugging Face Hub documentation — hugging-
ACKNOWLEDGMENT face.co,”https://huggingface.co/docs/hub/index,[Accessed28-10-2024].
[12] “Gemini 1.5 Pro — console.cloud.google.com,” https://console.
This work has been funded by the Spanish research project
cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-1.
DOGO4ML (ref. PID202 0-117191RB-I00). 5-pro-preview-0409,[Accessed28-10-2024].
[13] J.Cohen,“Acoefficientofagreementfornominalscales,”Educational
REFERENCES andpsychologicalmeasurement,vol.20,no.1,pp.37–46,1960.
[14] J.SimandC.C.Wright,“Thekappastatisticinreliabilitystudies:use,
[1] Hugging Face Inc., “Hugging Face – The AI community building the interpretation,andsamplesizerequirements,”Physicaltherapy,vol.85,
future. — huggingface.co,” https://huggingface.co, [Accessed: 28-10- no.3,pp.257–268,2005.
2024]. [15] J. Landis, “The Measurement of Observer Agreement for Categorical
[2] J. Castan˜o, S. Mart´ınez-Ferna´ndez, X. Franch, and J. Bogner, Data,”Biometrics,1977.
“Analyzing the evolution and maintenance of ml models on hugging [16] PyTorchFoundation,“PyTorchHub—pytorch.org,”https://pytorch.org/
face,” in Proceedings of the 21st International Conference on hub/,[Accessed05-11-2024].
Mining Software Repositories, ser. MSR ’24. New York, NY, USA: [17] “Papers with Code - The latest in Machine Learning — paperswith-
Association for Computing Machinery, 2024, p. 607–618. [Online]. code.com,”https://paperswithcode.com,[Accessed05-11-2024].
Available:https://doi.org/10.1145/3643991.3644898 [18] “TensorFlow Hub — tensorflow.org,” https://www.tensorflow.org/hub?
hl=es,[Accessed05-11-2024].