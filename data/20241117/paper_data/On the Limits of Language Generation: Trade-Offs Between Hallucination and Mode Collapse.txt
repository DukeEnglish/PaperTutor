On the Limits of Language Generation:
Trade-Offs Between Hallucination and Mode Collapse
AlkisKalavasis AnayMehrotra GrigorisVelegkas
YaleUniversity YaleUniversity YaleUniversity
alkis.kalavasis@yale.edu anaymehrotra1@gmail.com grigoris.velegkas@yale.edu
Abstract
Specifyingalldesirablepropertiesofalanguagemodelischallenging,butcertainrequire-
mentsseemessentialforanygoodmodel. Givensamplesdrawnfromanunknownlanguage,
thetrainedmodelshould(1)producevalidstringsthathavenotbeenseeninthetrainingdata,
and(2)beexpressiveenoughtocapturethefullrichnessofthelanguage. Otherwise,ifthelan-
guagemodeloutputsinvalidstrings,it“hallucinates,”andifitfailstocapturethefullrangeof
thelanguage,itsuffersfrom“modecollapse.” Inthispaper,weaskwhetheritispossiblefora
languagemodeltomeetbothoftheserequirements.
Weinvestigatethisquestionwithinastatisticalsettingoflanguagegeneration,buildingon
theseminalworksofGold[Gol67,Inf.Control],Angluin[Ang79,STOC],andAngluin[Ang88,
Tech.Report]. Inthissetting,thelanguagemodelispresentedwithrandomlysampledstrings
from a distribution supported on an unknown language K, which is only known to belong
to a possibly infinite collection of candidate languages. The goal of the model is to generate
unseen strings from this target language. We say that the language model generates from K
withconsistencyandbreadthif,asthesizeofthetrainingsetincreases,thesetofstringsitcan
outputconvergestothesetofallunseenstringsinK.
KleinbergandMullainathan[KM24, NeurIPS]posedanopenquestionofwhetherconsis-
tency and breadth in language generation are both possible. We answer this question nega-
tively: for a large class of language models – including next-token-prediction-based models
– this is impossible for most collections of candidate languages. This contrasts with the re-
centpositiveresultofKleinbergandMullainathan[KM24,NeurIPS],whichdemonstratedthat
consistent generation, without requiring breadth, is possible for any countable collection of
candidate languages. Our finding highlights that generation with breadth is fundamentally
differentfromgenerationwithoutbreadth.
As a byproduct of our result, we also examine how many samples are required for gen-
eration with or without breadth, establishing near-tight bounds on the “learning curves” for
generationinthestatisticalframeworkofBousquet,Hanneke,Moran,vanHandel,andYehu-
dayoff[BHM+21,STOC].
Finally,ourresultsalsogivesomehopeforconsistentgenerationwithbreadth: itisachiev-
ableforanycountablecollectionoflanguageswhennegativeexamples–intheformofstrings
outside of K – are available in addition to strings inside of K. This suggests that feedback
in post-training, which encodes negative examples, can be crucial in reducing hallucinations
whilealsolimitingmodecollapse.
4202
voN
41
]GL.sc[
1v24690.1142:viXraContents
1 Introduction 1
1.1 InformalResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1.1 SetupandDefinitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.1.2 MainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 TechnicalOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.3 AdditionalResultsWithRelaxationofConsistencyandBreadth . . . . . . . . . . . . 12
1.4 Takeaways,Discussion,andOpenProblems . . . . . . . . . . . . . . . . . . . . . . . 14
1.5 FurtherRelatedWorks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2 ModelandPreliminaries 19
2.1 LanguageIdentificationandGenerationintheLimit . . . . . . . . . . . . . . . . . . . 20
3 OverviewofResults 23
3.1 ResultsforIdentificationandGenerationWithoutBreadth . . . . . . . . . . . . . . . 24
3.1.1 UniversalRates: ModelandPreliminaries . . . . . . . . . . . . . . . . . . . . . 24
3.1.2 UniversalRatesforIdentification . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.1.3 UniversalRatesforConsistentGeneration . . . . . . . . . . . . . . . . . . . . 26
3.2 ResultsforGenerationWithBreadth . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.2.1 MembershipOracleProblem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.2.2 ResultsforGeneratorsforWhichMOP(·)IsDecidable . . . . . . . . . . . . . 28
3.2.3 AFamilyofGeneratorsforWhichMOP(·)IsDecidable . . . . . . . . . . . . . 29
3.2.4 ResultsforGenerationWithBreadthintheLimit . . . . . . . . . . . . . . . . . 30
3.3 ResultsforGenerationWithApproximateConsistencyandBreadth . . . . . . . . . . 30
3.4 FurtherResultsforIdentification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.4.1 ExponentialRatesforIdentificationUsingSubsetOracle . . . . . . . . . . . . 32
3.4.2 ExponentialRatesforIdentificationofFiniteCollections . . . . . . . . . . . . 33
3.4.3 ExponentialRatesforIdentificationofCollectionsofFiniteLanguages . . . . 33
3.4.4 ExponentialRatesforIdentificationfromPositiveandNegativeExamples . . 33
4 OrganizationoftheRestofthePaper 34
5 ProofsfromSection3.1(RatesforIdentificationandGeneration) 35
5.1 ProofofTheorem3.1(RatesforIdentification) . . . . . . . . . . . . . . . . . . . . . . 35
5.2 ProofofTheorem3.2(RatesforGeneration) . . . . . . . . . . . . . . . . . . . . . . . . 46
5.2.1 OptimalRateforNon-TrivialCollectionsforGeneration . . . . . . . . . . . . 46
5.2.2 ASufficientConditionToAchieveExponentialRate . . . . . . . . . . . . . . . 51
5.2.3 AlgorithmWithAccessToSubsetOracle . . . . . . . . . . . . . . . . . . . . . 52
5.2.4 AlgorithmWithAccessToMembershipOracle . . . . . . . . . . . . . . . . . . 53
6 ProofsfromSection3.2(GenerationWithBreadth) 55
6.1 ProofofTheorem3.4(MOP(·)IsDecidableForIterativeGenerators) . . . . . . . . . 55
6.2 ProofofTheorem3.3(ImpossibilityforGenerationWithBreadth) . . . . . . . . . . . 56
6.3 ProofofTheorem3.5(ImpossibilityforGenerationWithBreadthintheLimit) . . . . 597 ProofsfromSection3.3(GenerationWithApproximateConsistencyandBreadth) 60
7.1 ProofofTheorem3.7(ImpossibilityintheLimit) . . . . . . . . . . . . . . . . . . . . . 60
7.2 ProofofTheorem3.6(ImpossibilityintheStatisticalSetting) . . . . . . . . . . . . . . 63
8 ProofsfromSection3.4(FurtherResultsforIdentification) 66
8.1 ProofofProposition3.8(IdentificationUsingSubsetOracle) . . . . . . . . . . . . . . 66
8.2 ProofofProposition3.9(IdentificationofFiniteCollections) . . . . . . . . . . . . . . 67
8.3 ProofofProposition3.10(IdentificationofCollectionsofFiniteLanguages) . . . . . 68
8.4 ProofofTheorem3.11(IdentificationfromPositiveandNegativeExamples) . . . . . 69
A FurtherDiscussiononDecidabilityofMOP(·) 90
B ResultsWithSubsetOracles 90
B.1 IdentificationintheLimitWithoutTell-TaleOracleviaSubsetOracles . . . . . . . . . 90
B.2 Best-Of-BothWorlds: GeneratingWithBreadthWhenPossible . . . . . . . . . . . . . 92
C FurtherResultsforConsistentGenerationWithApproximateBreadth 92
D FurtherComparisonWithOnlineLearning 96
E Borel-CantelliLemmas 981 Introduction
Languageacquisitionisafundamentalmysteryacrossmultiplescientificfields, rangingfromBi-
ology and Neuroscience to Sociology [SAN96; Bre07; Cla14; MIB+24]. Theoretical Computer Sci-
entists have been fascinated by language since the early days of the field: in the 1950s, Turing
[Tur50]introducedhisfamoustestusinglanguageasaninterfacetocognition,Shannon[Sha51a]
studied statistics of printed English aiming at understanding its entropy and the extent to which
itcouldbecompressed, andMandelbrot[Man53]designedastatisticalmodeltocaptureconnec-
tionsbetweenlanguageandthebrain.
Over the years, language modeling has advanced through simple models, such as the word
n-gram model introduced by Shannon [Sha51b] and widely used in natural language processing
[BDd+92]. In the early 2000s, neural networks achieved a significant breakthrough in the field
[BDV00], leading to fascinating deep learning systems [MKB+10; LBH15; Gol16] built using tra-
ditional architectures like Recurrent Neural Networks [RHW86] and Long Short-Term Memory
[HS97]. In 2017, the field of language modeling was revolutionized by the introduction of the
Transformer architecture [Bah14; SVL14; VSP+17], which led to the development of Large Lan-
guageModels(LLMs). TheachievementsofLLMshavebeengroundbreaking;recentmodelscan
perform well on tasks far beyond natural language processing [BCE+23; TLI+23]. Despite their
impressive performance, their extensive use has revealed that LLMs exhibit various bizarre be-
haviorseveninseeminglymundanetasks[Bor23].
Perhaps the most well-known issue with current LLMs is hallucinations: the models generate
false but plausible-sounding text with surprising frequency [JLF+23; ZLC+23].1 Such hallucina-
tions, highlighted by popular media [WM23], could significantly impact safety, reliability, and
user trust as the adoption of these systems extends to new tasks [AOS+16; HCS+22]. The impor-
tance of this problem, among other concerns, led both the US [Bid23] and the EU [Sat23] to issue
calls for safeguards against misleading outputs generated by LLMs. In this direction, designing
LLMsthatgenerateresponsesconsistentwiththegroundtruthisaneffortthathasgainedalotof
attentionfromMachineLearning(ML)practitioners[WWS+22;AP23;GZA+23;HYM+23;JLF+23;
FSW+24;KWT+24],policymakers[Bid23;Sat23;SK23],andtheorists[HKK+18;KV24;KM24].
If the sole goal is to avoid hallucinations, then, of course, one could simply limit the range of
outputs generated by the language model. As an extreme example, consider a language model
thatonlyoutputs“Iamalanguagemodel”and, therefore, neverhallucinates. However, modern
LLMs do not just aim to generate a few valid outputs; their goal is to obtain the ability to express
a wide range of plausible outputs, thus capturing the richness of human language. The key chal-
lengeliesinavoidinghallucinationswhileachievingbreadth. Theproblemofachievingconsistent
generation with breadth is not new in the ML community, dating back at least to the era of Gen-
erative Adversarial Networks (GANs) [GPM+20]. In this line of work, mode collapse [GPM+20] is
theanalogoflackofbreadth;itreferstothephenomenonwheretheGANassignsnon-zeromass
onlytoafewmodesofthetruedatadistribution,thusproducingalimitedvarietyofsamplesand
becoming repetitive [AB17; SSA18; BZW+19]. The starting point of our work is exactly this puzzling
tensionbetweenconsistentgenerationandbreadthinlanguagegeneration.
1WestressthatLLMsoutputtingwrongfactsbasedonerrorsintrainingdata(e.g.,“TheEarthisflat”)ormiscalcu-
lations(e.g.,“1+1=3”)donotconstitutehallucinations.Ahallucinationisaplausiblebutfalsetextwithunclearorigin
(e.g.,“BarackObamawasthepresidentoftheUSandwasbornonJanuary1,1958”).
1Westartwithamathematicalspecificationinspiredbyclassicalworkonlearningtheory,trac-
ing back to the seminal work of Angluin [Ang88], and the recent formulation of Kleinberg and
X
Mullainathan[KM24]: thedomain isacountablecollectionofstrings,andthereisanunknown
target language K which is a subset of this domain. We know that the true language lies within
a collection of possibly infinite but countably many languages L = {L ,L ,...}. There exists an
1 2
unknown distribution P over strings in K ∈ L that satisfies supp(P) = K; any distribution with
this property is said to be valid for K. The algorithm observes i.i.d. samples from P and aims
to learn how to generate unseen strings from the target language K – this, at a high level, is the
languagegenerationproblem. Intuitively,thetargetlanguageK iscapturing“facts”oftheworld;
everythingthatbelongsto K iscorrect,whereaseverythingoutsideof K isunambiguouslyincor-
rectandcanbethoughtofasa“hallucination.” ObservethatK hastobeinfinitefortheproblem
tobewell-definedas,otherwise,atsomepoint,thealgorithmwillseeallpossiblestringsofKand,
fromthenon,wouldhavenounseenstringstogeneratefrom.
Let us explore language generation further, with the immediate aim of quantifying an algo-
rithm’s progress toward becoming a useful generator. Consider a generating algorithm G 2 that
n
is trained on a set S of n i.i.d. examples from P . To quantify the inconsistency of G , we need an
n
objective. As discussed above, this objective should penalize G for outputting strings outside of
n
K and for repeating examples already seen in the training data S.3 For a target language K and a
modelG trainedonS,weconsiderthefollowinggenerationerror
n
gen_er(G ) := Pr [supp(G ) ⊃ K\S]. (1)
n n
S∼Pn
In words, a model errs according to gen_er(·) if it either hallucinates by outputting strings from
X\K or if it outputs something already contained in the training set S. This is inspired by the
notion of generation considered by Kleinberg and Mullainathan [KM24]; they call an algorithm
a consistent generator if its support becomes a subset of K\S after seeing finitely many training
examples S. WerelaxthisdefinitionandcallanalgorithmaconsistentgeneratorforthecollectionL
P
ifitserror,asdefinedinEquation(1),asymptoticallygoestozeroforanyvaliddistribution .
Let us now review how prior work has approached issues with language generation algo-
rithms – foremost, hallucination. Under the above statistical setting, Kalai and Vempala [KV24]
madeimportantprogressshowingthatcalibratedmodelsmusthallucinatebylowerboundingthe
hallucination rate by the model’s calibration. For a detailed comparison with our work, we refer
toSection1.5. Closertoourpaper,theworkofKleinbergandMullainathan[KM24]exploredlan-
guage generators that must not hallucinate, i.e., they must be consistent. They studied language
P
generation in an online setting where the data are not drawn from but are given as a stream to
thelearner,i.e.,asanadversarialenumerationofthestringsofthetruelanguage K. Intheirsetting,
G is said to generate in the limit from K if, after some finite time n in the enumeration of K, G
n 0 n
is able to generate new unseen strings from K for all subsequent times n ≥ n . They showed
0
2Formally,ageneratingalgorithmisasequenceofmappings(Gn) n∈N:foreachn,itisacomputablemappingfrom
atrainingdatasetofsizentoa(computable)distribution(i.e.,asamplingalgorithm)overX.Wewillusethenotation
(Gn) ntorefertothegeneratingalgorithmandthenotationGnorsimplyG fortheinduceddistribution(generator)after
training;hencewhenwewritex∼Gnorsupp(Gn),werefertothedistributionobtainedaftertraining.
3Whenwerequiregeneratingalgorithmtoachievebreadth,itisnotimportanttoenforcethatthesupportdoesnot
containS.WewillelaborateaftertheformalstatementofDefinition4.
2that there exists an algorithm that can generate in the limit from every countable list of candidate
languages.
This result is surprising because it contrasts with strong negative results for the well-studied
problem of language identification in the limit (where one wants to identify K in the limit and not
simplygeneratefromit;4 seealsoDefinition9). Thefamilyoflanguagesidentifiableinthelimitis
verylimited: theresultsofGold[Gol67]andAngluin[Ang79]showedthatlanguageidentification
is a very difficult problem and most collections of languages are non-identifiable (in fact, there is
a tight characterization due to Angluin [Ang80] which we state in Definition 10). Hence, the
algorithm of Kleinberg and Mullainathan [KM24] shows that language generation in the limit is
much more tractable than identification. We note that while their algorithm operates in a non-
statisticalsetting,itwillbeanimportantbuildingblockforourresults.
KleinbergandMullainathan[KM24]observedthattheiralgorithmeventuallybecomesacon-
sistent generator but suffers from mode collapse: initially, it generates with breadth while being in-
consistent with the target language; later on, as a larger part of the stream is seen, it starts sacri-
ficing breadth in order to generate valid outputs. This behavior led them to leave the existence
of a consistent generator that achieves breadth as an interesting open question. In this work, we
willformallyintroduceanotionofbreadthforlanguagegenerationinourstatisticalsetting(Sec-
tion 1.1.1). For now, we mention that our definition roots in the notion of mode collapse from
GenerativeAdversarialNetworks(GANs)[AB17;GPM+20]and,roughlyspeaking,statesthatan
algorithm (G ) generates with breadth from K if the probability that its support contains all the
n
unseen examples from the target language goes to 1, as the training samples from a valid dis-
tribution go to infinity. Now it is a good point to contrast breadth with consistency: consistent
generators aim at avoiding any elements outside of K while generators achieving breadth try to
cover all unseen elements of K. The question of Kleinberg and Mullainathan [KM24] is asking
whether the equilibrium condition that the support of the generator exactly matches the unseen
elementsofKcaneventuallybeachievedbysomealgorithm. Thisisthemainquestionweaimto
addressinthispaper.
Isitpossibletoachieveconsistentlanguagegenerationwithbreadthor
istheresomeinherenttrade-offbetweenconsistencyandbreadth?
1.1 InformalResults
Our main results confirm the tension between consistent generation and breadth for language
models,conjecturedbyKleinbergandMullainathan[KM24],inastrongway: informally,weshow
that
Alanguagemodelthatgenerateswithbreadthmustbeinconsistent,i.e.,itmusthallucinate.
We focus on the probabilistic setting of Angluin [Ang88] which we have already introduced in-
formally. En route to our results in the probabilistic setting, we also obtain results in the online
setting of Gold [Gol67], Angluin [Ang79], and Kleinberg and Mullainathan [KM24], as we will
4Verybriefly, alanguagecollectionL = {L ,L ,...} iscalledidentifiableinthelimitifthereexistsanalgorithm
1 2
(A n: Xn → N) n such that for any K ∈ L and any enumeration x 1,x 2,... of the strings of K appearing as a stream
to (A n), there is a finite time n
0
∈ N after which the algorithm predicts the correct index of the true language, i.e.,
LA n(x1,...,xn) =Kforanyn≥n 0.
3seelater. Tofacilitateaformaldiscussionofourcontributions,weneedtointroducesomefurther
definitions.
1.1.1 SetupandDefinitions
Agenerating(orlearning)algorithmisasequenceofcomputablemappings(G n) = (G n) n∈N from
samplesS ⊆ Xn togenerators,whicharesimplydistributionsoverthedomainX . Moreformally,
ageneratingalgorithmisasequenceofmappingsfromsamplestoTuringmachinesthatgenerate
samplesfroman(explicitlyorimplicitly)defineddistributionoverstrings.
Inthestatisticalsettingweconsider,thelearnerobservessamplesfromanunknowndistribu-
tionwhichisvalidforsomeunknownlanguageK inthecollectionL = {L ,L ,...}.
1 2
Definition 1 (Valid Distribution [Ang88]). A distribution P over a countable domain X is valid with
respecttoacountablelanguagecollectionLifitssupportisthesameassomelanguageK ∈ L .Inthiscase,
whenwewanttobespecificaboutthelanguagethatPdrawssamplesfrom,wesayPisvalidforK.
L P
Ifthecollection isclearfromcontext,wewillsimplysaythat isvalid. Basedonthisdefinition
andbuildingonthemodelstudiedbyKleinbergandMullainathan[KM24],wegivethefollowing
L
adaptationforconsistentgenerationfromacollection inthestatisticalsetting.
Definition2(Consistency). Ageneratingalgorithm (G ) foralanguagecollectionLisconsistentiffor
n
any valid distribution P, it holds that lim n→∞gen_er(G n) = 0. Otherwise, the algorithm is said to be
inconsistent.
Hence,analgorithmissaidtobeconsistentifthegeneratorsitproducesbytrainingonanyvalid
P P
distribution converge to generating examples from the unseen part of . Some of our results
explore when asymptotic consistency is achievable. However, the main focus of our work is on
understanding theratesat whichconsistency (andother desirableproperties) can beattained –if
possible at all. In particular, we want to study the rate at which the generation error gen_er(G )
n
decreasesasthenumberofsamplesngoestoinfinity–thatis,wewanttostudythelearningcurve
of consistent generation (and other tasks that we introduce later in this section). Bousquet, Han-
neke, Moran, van Handel, and Yehudayoff [BHM+21] characterized learning curves for binary
classification, formalizing the universal rates framework, earlier explored by Schuurmans [Sch97]
andAntosandLugosi[AL98]. Tothisend,weborrowtheirdefinitionofuniversalrates.
Definition3(Informal,UniversalRates;[BHM+21],seeDefinition12). Ageneratingalgorithm(G )
n
hasrate R(·),wherelim n→∞R(n) = 0,foralanguagecollectionLif
∀P ∈ Val(L) ∃C,c > 0 suchthat gen_er(G ) ≤ C·R(c·n) ∀n ∈ N ,
n
whereVal(L)istheclassofvalid(realizable)distributionsforL.
Observe that these learning curves are distribution-dependent since the constants c and C are
P
allowed to depend on . This difference turns out to be crucial and can, sometimes, lead to sig-
nificantdifferencesbetweenuniversalratesandthecorrespondingdistribution-independentrates
[BHM+21]. Amongdifferentuniversalrates,exponentialuniversalratesareofspecificinterestas
theyareoftenthebestpossiblerate,aswewillseelater. Wesaythatthealgorithm(G )generates
n
with an exponential universal rate if R(n) = exp(−n) in the above definition. Next, we turn to
languagegenerationwithbreadth.
4Definition4(Breadth). Ageneratingalgorithm(G )foralanguagecollectionLissaidtoachievebreadth
n
if,foranyvaliddistributionP ,itholdsthatlim n→∞Pr[supp(G n) ⊇ K\S n] = 1,whereS
n
isthedataset
usedtotrainG ,i.i.d. fromP .Otherwise,thealgorithmsuffersfrommodecollapse.
n
Definition 4 is inspired by the literature on GANs (see e.g., [AB17; GPM+20]). For instance, con-
P
sidertheworkofArjovskyandBottou[AB17],whichstudiesdistributionsG and inducedbythe
generator and nature, respectively, and says that mode collapse occurs when the KL divergence
KL(P∥G) := (cid:82) log(P(x)/G(x)) dP(x) → ∞ . In particular, mode collapse happens when there is
some string x ∈ supp(P) for which G(x) = 0. In other words, the generator has breadth when
supp(G)∪S ⊇ supp(P), which recovers our definition for breadth by noting that supp(P) = K
n
since P is valid for K and that, to be compatible with the definition of consistency (Definition 2),
webarageneratorfromrepeatingstringsithasalreadyseen. (Itisworthmentioningthatonecan
modify the definition of breadth to require supp(G ) ⊇ K without changing any of our results;
n
see Remark 2.) We also note that the definition of consistency we use can also be derived in an
analogousfashionbyrequiringthereverseKLdivergence(i.e.,KL(G∥P))tobefinite.
Puttingthedefinitionsofconsistencyandbreadthtogetherimpliesthatanalgorithmgenerates
withconsistencyandbreadthif,eventually,itssupportmatchesthesetofunseenstringsin K,i.e.,
K\S atthe n-thstep. Afterpresentingourmainresults,inSection1.3,wediscussrelaxationsof
n
thisnotionofconsistentgenerationwithbreadth.
A last ingredient for our results concerns the decidability of a folklore Theoretical Computer
Science problem, which we call the membership oracle problem, that has motivated extensive work
informallanguagesandcomplexitytheory[Soa99; Sip12]. AgeneratorG, whichistheoutputof
some generating algorithm, corresponds to some Turing machine, as is standard in the language
X
inferenceliterature,thatsamplesaccordingtoadistributionover [BB75;Ang79;AS83;AB91].
Definition 5 (Membership Oracle Problem). Given a generator G, the membership oracle problem for
G, denoted as MOP(G), is defined as follows: given the description of G and a string x, output Yes if
x ∈ supp(G)andoutputNootherwise.
This problem is, in general, undecidable due to a reduction to the halting problem (Section A);
nevertheless,itsdecidabilitydependsonthestructureoftheTuringmachineaswewillseeshortly.
Theabovedefinitionnaturallyextendstogeneratingalgorithms.
Definition 6 (MOP for Generating Algorithms). The membership oracle problem is decidable for a
generating algorithm (G ) if, for any n ∈ N and any S ⊆ Xn, MOP(·) is decidable for the induced
n
generatorG = G (S).
n
WenotethattheabovedefinitionsimplicitlyassumethatthegeneratorG (S)dependsonlyonthe
n
randomnessofS;wecouldextendthisbyallowingG (S)tobeadistributionovergenerators.
n
1.1.2 MainResults
We now have all the ingredients to state our first result, which establishes that, for all generat-
ing algorithms for which MOP(·) is decidable, (consistent) generation with breadth is as hard as
languageidentificationinthestatisticalsetting.
5AsinDefinition3,wewillsaythatthegeneratingalgorithm(G )generateswithbreadthfrom
n
L atsomerate R(·)if,foranyK ∈ L ,validdistributionP ,andn ∈ N ,
E 1{supp(G ) ̸= K\S} ≤ C·R(c·n),
n
S∼Pn
for some distribution-dependent constants C,c > 0. If no rate R(·) satisfying lim n→∞R(n) = 0
exists,wewillsaythat(G )doesnotgeneratewithbreadthatanyrate.
n
Informal Theorem 1 (see Theorem 3.3). For every language collection L that is not identifiable in the
limit, no generating algorithm (G ), for which MOP(·) is decidable, can generate from L with breadth at
n
anyrate.
Recall that the family of languages non-identifiable in the limit is quite broad. Based on the re-
sults of Gold [Gol67] and Angluin [Ang79; Ang80] on the problem of language identification in
thelimit,ourimpossibilityresultholdsformostinterestingcollectionsoflanguages. ForInformal
Theorem 1 to be valuable and meaningful though, we further need to show that there exists an
algorithm that generates without breadth for the collections of languages for which our impossi-
bility result is true. Our next result states that this is indeed possible: there exists an algorithm
thatgenerateswith(almost)exponentialuniversalratesforanycountablelanguagecollectionL
.
Informal Theorem 2 (see Theorem 3.3). For every language collection L that is not identifiable in the
limit, there exists a generating algorithm (G ), for which MOP(·) is decidable, that generates (possibly)
n
without breadth from L at exponential rates. Further, if L is identifiable in the limit, then there exists a
generatingalgorithm (G ),forwhichMOP(·) isdecidable,thatgenerateswithbreadthfromLat(almost)
n
exponentialrates.
Informal Theorem 2 shows that any countable collection of languages not only admits a consis-
tentgeneratorinthelimitunderanadversarialenumerationofthetargetlanguage(asshownby
Kleinberg and Mullainathan [KM24]), but the statistical rate at which consistency (as per Defini-
tion2)isachievedisexponentialinthenumberofsamples. Further,foridentifiablecollectionsof
languages,wegiveanalgorithmthatgenerateswithbreadthatan(almost)exponentialrate.
ThecombinationofInformalTheorem1and2,revealsastrongseparationbetweengeneration
with and without breadth for any generating algorithm for which MOP(·) is decidable. What is
missing is an answer to: how large is the class of generators for which the membership oracle
problem MOP(·) is decidable? It turns out there is a very broad class of language generators for
whichthisisthecaseandwhichalsocapturesmodernLLMs,asweshownext.
AFamilyofGeneratorsforWhichMOP(·)IsDecidable. Motivatedbythestructureofmodern
language models [BJM83; BCP+90; BCE+23; TLI+23; OAA+24], we consider a family of iterative
generators. A generator is said to be iterative if it generates text one alphabet or “token” at a
time (see Definition 14). To generate each token, the generator can perform an arbitrary (but
finite) amount of computation and, possibly, use randomness. For this to make sense, one has to
X Σ
imagine strings of as strings over some finite alphabet . This holds without loss of generality
X X Σ
as iscountablyinfiniteand,hence,thereisaone-to-onemappingfrom tostringsover (due
6towhichX canbethoughtofasasetofstringsoverΣ ).5 Weshowthatforanyiterativegenerator,
themembershiporacleproblemisdecidableandourInformalTheorem1isapplicable.
InformalTheorem3(seeTheorem3.4). ForanyiterativegeneratorG,MOP(G)isdecidable.
Observe that this family of next-token generators is very general. First, it captures existing large
language models: for instance, to simulate an LLM L, we define the next-token predictor as a
Turingmachinethatsimulates Lontheprovidedstringuntil Lgeneratesonenewtoken. Next,it
alsocapturessystemswhereanLLMcaninteractwithanotherGenerativeAImodeloralgorithmic
system (such as a diffusion model or a code interpreter) – as these auxiliary systems can also be
simulated by the generator. Given this, it becomes evident that this class of generators for which
MOP(·)isdecidableisfairlylargeandinteresting.
ImplicationsfortheGold-AngluinModel. Werepeatthatalltheaforementionedresultsholdin
thestatisticalsetting. Enroutetoobtainingourresultsinthissetting(InformalTheorems1and2),
weshowseveralconnectionstotheonlinesettingofGold[Gol67], Angluin[Ang79; Ang80], and
KleinbergandMullainathan[KM24],whichleadtothefollowingresult.
Informal Theorem 4 (see Theorem 3.5). For any language collection L that is not identifiable in the
limit, no generating algorithm (G ), for which MOP(·) is decidable, can generate from L with breadth in
n
thelimit.
To be more concrete, a generating algorithm generates with breadth in the limit if its support is
eventuallyK\S ,whereS isthesetofthefirstnpositiveexamples(i.e.,examplesthatbelongto
n n
K). We emphasize that Informal Theorem 4 is in a similar spirit as our Informal Theorem 1, but
holdsintheonlinemodelinsteadofthestatisticalmodeldiscussedearlier. Inparticular,Informal
Theorem4combinedwiththealgorithmofKleinbergandMullainathan[KM24]giveaseparation
betweenconsistentgenerationwithandwithoutbreadthintheGold-Angluinmodel. Further, as
explained before, this result applies to any iterative generator due to Informal Theorem 3. More-
over,asMOP(·)isdecidableforthegeneratingalgorithmofKleinbergandMullainathan[KM24]
(since its support contains a singleton element x which can be computed by running their algo-
rithm), the above result, in particular, shows that the algorithm of Kleinberg and Mullainathan
[KM24]cannotgeneratewithbreadthinthelimitfromanynon-identifiablecollection.
Organization of Rest of the Introduction. We proceed with an exposition of our techniques
in order to obtain our main results presented above. In Section 1.3, we relax the definitions of
consistency and breadth and give more “robust” trade-offs between hallucination and breadth.
Next, inSection1.4, wegivealistofopenproblemsforfuturework. Finally, Section1.5contains
anextensiveoverviewofrelatedworks.
1.2 TechnicalOverview
Inthissection,wepresentthetechnicaltoolswedeveloptoobtainourmainresults.
5Inabitmoredetail,sinceXandΣ∗arecountablyinfinite,theyhaveenumerationsx ,x ,... ands ,s ,.... There-
1 2 1 2
fore,givenanystrings ∈ Σ∗ generatedbyaniterativegenerator,onecanmapittoastringx ∈ X,therebygettinga
i i
generatorforX.
7A Natural Strategy to Prove Informal Theorem 1. At first glance, there seems to be a natural
strategytoproveInformalTheorem1: assumethatthereexistsaconsistentgeneratingalgorithm
with breadth G = (G ) for some non-identifiable collection L in the statistical setting and then
n
showthatthisimpliesidentificationinthestatisticalsetting,whichwouldcontradictthefactthat
L
isnon-identifiable. ToimplementthisstrategyoneneedsamethodtoutilizeG, alongwiththe
positive samples from the target language K, for identification. This raises the question: what
additionalpowercanG givethatthepositivesamplesdonotalreadyprovide?
Initial Attempts to Implement the Strategy. Indeed, if one uses no additional properties of G,
then its outputs provide no more information than an adversarial enumeration of K. To develop
some intuition, we begin by considering some properties of the generator and explaining why
theyareinsufficienttoenableidentification.
1. G isnon-adaptive. First,onemaywanttoutilizethefactthatthegeneratorG isfixedand,hence,
the samples it outputs cannot adapt to the specific algorithm being used based on the outputs
of the algorithm. Hence, it will probably provide an algorithm-independent enumeration of the
true language. However, this is not helpful in general since there exist simple non-identifiable
languagecollectionsthatremainnon-identifiableformanyenumerationsofthetargetlanguage.
2. G samples from a fixed distribution. Another property one may want to leverage is the stochas-
ticity of the generator: G samples its outputs from a fixed distribution (which is valid for K).
However, even this does not enable the identification of non-identifiable collections due to a re-
sultbyAngluin[Ang88]. Angluinshowsthatevenifthepositiveexamplesarei.i.d. fromavalid
distribution and do not appear as an adversarial enumeration (as in Gold [Gol67]), this does not
L
enableidentificationofanycollection thatisnon-identifiableinthelimit. (Weproveastronger
versionofthisresultinLemma5.5.)
3. G samplesfromasimpledistribution. Moreover,thedifficultyintheabovenegativeresultisnot
thecomplexityoftheencodeddistribution: itholdsevenwhenG samplesfromadistributionthat
iscomputablebyaTuringmachine.
At this point, it is not clear how to utilize access to a generator G which generates with breadth
fromK. Next,wepresentastrongformofaccesstothegeneratorG thatisusefulforidentification.
4. AccesstoSubsetQueries“supp(G) ⊆ L ”and“L ⊆ L ”. Foroneoftheiralgorithms,Kleinberg
i i j
andMullainathan[KM24]utilizeasubsetoraclethatanswersqueriesoftheform“isL ⊆ L ?”. (In
i j
general,thisoracleisnotguaranteedtobecomputable). Onecanimagineanextensionofthisor-
aclethat,givenanindexianddescriptionofthegeneratorG,outputswhethersupp(G) ⊆ L . The
i
existence of this oracle turns out to be sufficient to identify K, as we explain next: After a finite
amount of time, Kleinberg and Mullainathan [KM24]’s algorithm creates a list of “critical” lan-
guagesC ,C ,...,ofthefollowingform(seeTheorem4.1inKleinbergandMullainathan[KM24])
1 2
C ⊇ C ⊇ ··· ⊇ (C := K) ⊇ C ⊇ ... .
1 2 i i+1
In words, this list has two properties (1) K appears in this list, say, at C = K for some i < ∞
i
and(2)eachlanguageC inthelistisasubsetoftheprecedinglanguageC . Giventhislistand
j j−1
theaforementionedsubsetoracle,onecaneasilyidentifytheindexof K asthelargest j forwhich
supp(G) = K ⊆ C . Thisassumptionallowstoidentifyanycollectioninthelimitgivenaccesstoa
j
8consistentgenerationG withbreadth. However,thistypeofaccessisnotverypracticalsinceitis
notclearwhensuchanoracleisimplementable.
Our Approach. Our first idea is that a much weaker form of access to G – membership oracle to
supp(G) – is sufficient for identification. This is where the membership oracle problem MOP(·)
(Definition5)appearsintheproof. Infact,giventhisidea,itisnotdifficulttoshowthatwiththat
type of access, we can go from a generator with breadth in the online setting to an identification
algorithm in the online setting; and, hence, get Informal Theorem 4. However, our focus is the
statisticalsettingwherethereareseveraladditionalchallengesinusingthemembershiporacleto
supp(G).
A. Need for Universal Rates for Generation and Identification. The key issue is the following: In the
statistical setting, if we assume that we have a generator with breadth at rate R(·), then we can
hope to show an implication that we can get an identification algorithm at rate R(·). However,
L
this need not imply a contradiction to the identifiability of as in the online setting. This is
because,eventhoughL isnon-identifiableintheonlinesetting,itmaybecomeidentifiableatsome
rate R′(·) in the statistical setting. Indeed, this is the case in binary classification, where there
aresimplehypothesisclasses(suchasthresholdsoverreals)thatarenotlearnableinLittlestone’s
online setting [Lit88] but become learnable (at a universal – and uniform – linear rate) in the
statisticalsetting;infact,anyhypothesisclassislearnableinthestatisticalsettingunderuniversal
rates,sincethereisaBayesconsistentalgorithm,underbenignassumptions[BHM+21].
Hence, to get a contradiction, we first need to understand the taxonomy of universal rates
for generation and identification. We remark here that both the learning task (e.g., classification,
regression, identification, and generation) and loss function used in the problem are pivotal for
the landscape of rates that one gets; for instance, with the zero-one loss for binary classification
one gets a trichotomy of rates [BHM+21], but with the L -loss for regression, one gets infinitely
1
manyrates[AHK+24].
Toovercometheabovechallenge,weprovidestatisticalratesforidentificationandgeneration.
L
Westartwithidentification. Weshowthatif isidentifiableinthelimitintheadversarialGold-
Angluinsettingwithpositiveexamples[Gol67;Ang80],thenitisidentifiableunderDefinition12
with (almost) exponential (universal) rates. This is the less technical part of the proof so we will
giveahigh-levelapproach.
B.IdentificationintheLimit =⇒ Identificationat(Almost)ExponentialRates. Ourideaisreminiscent
ofBousquet,Hanneke,Moran,vanHandel,andYehudayoff[BHM+21]andrequiressplittingthe
inputdatasetintomultiplebatcheswhosesizeiscarefullyselected,runningtheonlinealgorithm
oneachbatch,andthentakingamajorityvoteovertheoutputsofthealgorithm. Weremarkthat
therearesometechnicalissueswhichrequirefurthercare,comparedtoBousquetetal.[BHM+21].
First,unlikethesettingofBousquetetal.[BHM+21],weonlyseepositiveexamplesandwegetno
feedbackaboutourguesses. Thus,wecannotusetheirapproachto“estimate”atimeafterwhich
the learner will stop making mistakes. Moreover, when we run the learners on multiple batches,
itcanbethecasethatdifferentbatchesoutputdifferentindicesoflanguagesthatcorrespondtoK
L
(sincethetargetlanguagecanappearatmultiplepositionsinthecountablecollection ).
Thus, taking a majority vote over these indices might not work. Nevertheless, we manage to
handletheseissuesandgetalmostexponentialratesforcollectionsthatsatisfyAngluin’scriterion
9for identification in the limit [Ang79]. A bit more concretely, to circumvent the first issue, our
approachisto“guess”therightbatchsize,andthisguessneedstobeanincreasingfunctionofn–
thisiswhywegetalmostexponentialratesinsteadofexactlyexponentialrates(Lemma5.5). The
second issue is more subtle. At a high level, we use a voting scheme where the output of every
batch (cid:98)L
i
gives a “vote” to every language L ∈ L such that (cid:98)L
i
= L, and we predict the lowest-
indexed language that is voted by at least half of the batches. In its current form, this scheme
is not computable, nevertheless, we show that it can be modified so that it becomes computable
(Lemma5.4).
The more interesting half of establishing universal rates for identification is the lower bound
showingthatifacollectionisnotidentifiableinthelimit,itisalsonotidentifiableinthestatistical
settingatanyrate R(·)suchthatlim n→∞R(n) = 0.
C. Impossible to Identify in the Limit =⇒ Impossible to Identify at Any Rate. Recall that the statis-
tical setting was studied by Angluin [Ang88]. Angluin [Ang88] showed that every learner, with
probability at least 1/3, does not converge to outputting a (stable) index of the target language in
an infinite stream of examples drawn from a valid distribution. In other words, with probability
at least 1/3, the algorithm will either stabilize to an index that does not correspond to the target
language or it will not stabilize to any index. Notice that this does not rule out algorithms that
output different indices of the target language, for all but finitely many n ∈ N . The first step to-
wards establishing our desired lower bound is to strengthen Angluin’s result: we show that any
learningalgorithm,withprobabilityatleast1/3,outputsindicesthatdonotcorrespondtothetar-
getlanguageinfinitelyoften. Moreformally,letusconsideranidentificationalgorithm h ,which
n
mapsatrainingsetofnexamplesx ,...,x toanindexh (x ,...,x )sothat L isthen-th
1 n n 1 n hn(x 1,...,xn)
predictionforthetruelanguage. Theaforementionedlowerboundmeansthat6
(cid:20) (cid:21)
(cid:110) (cid:111) 1
Pr limsup L ̸= K ≥ , (2)
{xi:i∈N}∼P∞ n→∞
hn(x 1,...,xn)
3
where X ∼ P∞ corresponds to an infinite i.i.d. draw from P . One may be tempted to conclude
thatthisimpliesthatwithprobability1/3wecannotidentifythetargetlanguage(inthestatistical
setting). However,thequantitywewishtoboundawayfrom0toderivethedesiredlowerbound
is
(cid:104)(cid:110) (cid:111)(cid:105)
limsup Pr L ̸= K .
n→∞ x 1,...,xn∼Pn
hn(x 1,...,xn)
Itiswell-knownthatforanysequenceofevents{E n} n∈N,
(cid:20) (cid:21)
Pr limsupE ≥ limsupPr[E ]. (3)
n n
n→∞ n→∞
This, however, is not sufficient to deduce the result we need; we need the opposite inequality.
Hence, Angluin’s guarantee does not suffice to get our lower bound. In order to show our re-
sult, we use a boosting argument (Lemma 5.8): if there exists a learner h whose probability of
n
6Informally, limsup of a sequence of events captures the events that occur infinitely often. For instance,
Pr[limsup n→∞E n] represents the probability that infinitely many of the events E 1,E 2,... occur. On the other hand,
limsup n→∞Pr[E n], roughly speaking, denotes the largest value that the probabilities Pr[E 1],Pr[E 2],...,... approach
infinitelyoftenasn→∞.
10misidentification
(cid:104) (cid:105)
Pr L ̸= K
x 1,...,xn∼Pn
hn(x 1,...,xn)
converges to a number strictly less that 1/2, then we can convert it to a learner whose error rate
decreases(almost)exponentiallyquickly. This(almost)exponentialrate,inparticular,impliesthat
∞
(cid:104) (cid:105)
∑ Pr L ̸= K < ∞ .
n=1x 1,...,xn∼Pn
hn(x 1,...,xn)
This, crucially, enables us to use the Borel-Cantelli lemma (see Lemma E.1) which gives us that
(cid:104) (cid:110) (cid:111)(cid:105)
Pr limsup L ̸= K = 0 , and, thus, a contradiction to Equation (2). This implies
n→∞ hn(x 1,...,xn)
thedesiredimpossibilityresult.
Asconsequenceoftheaboveresults,wegetadichotomyforuniversalidentificationrates:
Informal Theorem 5 (see Theorem 3.1). For any language collection L that is identifiable in the limit
andforany g(n) = o(n),thereexistsalearnerthatidentifiesLatrateexp(−g(n)). Otherwise,Lisnot
identifiableatanyrate.
L
We remark that if we have access to subset queries for , we can show that there exists an algo-
rithmthatachievesexactlyexponentialrates,forallidentifiablecollections(seeProposition3.8).
Next,wemovetounderstandinguniversalratesforlanguagegeneration.
D. Universal Rates for Generation (Possibly Lacking Breadth) Without Boosting. One might suspect
thatasimilarbatchingargumentwouldgiveusexponentialratesforgeneration: justruntheon-
line algorithm of Kleinberg and Mullainathan [KM24] multiple times and aggregate. The issue
is that aggregation for generation is different than prediction: for prediction, it is clear how to
implement majority vote as a boosting technique; for generation, it is unclear how to aggregate
different generated strings which is, typically, necessary to obtain a boosting algorithm. One im-
mediate attempt is to take majority votes over the strings that each batch outputs; unfortunately,
even if the majority of them are generating from the target language, they might be outputting
different strings, thus, even a few batches outputting the same invalid strings are enough to fool
ouraggregationrule.
Anothertemptingapproachistomimicthestrategyweusedtoaggregatedifferentindicesof
thetargetlanguageintheidentificationsetting: wegoovereveryoutputofthebatchesandwelet
themgiveavotetoeachofthelanguagesinL theybelongto.7 Itisnothardtoseethateverybatch
whose output corresponds to a valid generator will vote for the target language. Unfortunately,
it will also vote for all supersets of the target language. This is exactly the heart of the difficulty
of identification: telling apart supersets of the target language from the target language, which
iscolloquiallycalledovergeneralization. Takingittotheextreme, imaginethatthefirstlanguage
of the collection contains all the strings, i.e., L = X . Then, all the batches will vote for L . This
1 1
is problematic for two reasons: generating a fresh string from the majority-voted language is as
goodasrandomguessing,andchoosingastringamongtheonesthatvotedforthemajority-voted
languageisasgoodaspickingoneoftheoutputsofallbatchesuniformlyatrandom.
7Theastutereadermightrealizethat,asstated,thisstrategyisnotcomputable–asweexplain,evenifonecould
implementit,thisaggregationschemedoesnotwork.
11Perhaps surprisingly, it turns out that a much simpler approach works: we show that the al-
gorithmofKleinbergandMullainathan[KM24]directlyenjoysexponentialratesinthestatistical
setting, without the use of batching and boosting. This observation is based on a sufficient con-
ditionthatallowsonetouseanalgorithmthatworks“inthelimit”toobtainexponentialratesin
thestatisticalsetting,withoutanymodification(seeLemma5.11).
InformalTheorem6(seeTheorem3.2). ForanycountablelanguagecollectionLthereexistsagenerat-
ingalgorithmthatgeneratesfromLatan(optimal)exponentialrate.
This pair of results for identification (Informal Theorem 5) and generation (Informal Theo-
rem6)allowustogetInformalTheorem1and2. TheideaforInformalTheorem1isthatwewill
usethealgorithmG thatgenerateswithbreadthatsomerateR(·)foranarbitrarynon-identifiable
L L
collection and membership oracle access to G in order to get an identification algorithm for
withsomerateR′(·)suchthatlim n→∞R′(n) = 0. ThisisacontradictionsinceInformalTheorem5
L
shows that admits no rate in the universal setting. Finally, Informal Theorem 2 follows almost
immediatelyfromouruniversalratesresultforgeneration.
1.3 AdditionalResultsWithRelaxationofConsistencyandBreadth
Next, we study a relaxation of consistent generation with breadth, which we call unambiguous
generation,andask: isthereageneratorthatunambiguouslygeneratesfromanon-identifiablecollection?
In this section, we will allow the generator to repeat examples in the training data. Like all
of our results with breadth, this choice is not crucial, and all of the results have analogs where
the generator does not repeat training examples (see Remark 2). We make this choice for sim-
plicity. We show that unambiguous generation (which we define later in this section) from non-
identifiablecollectionsisimpossibleforanygeneratorG forwhichMOP(G)isdecidableandthat
satisfiesthenaturalpropertythatG “stabilizes”afterseeingsufficientlymanyexamples:
Definition7(Stability). Ageneratingalgorithm(G )isstableforalanguagecollectionLifforanytarget
n
language K ∈ Landforanyenumerationof K,thereissomefinite n∗ ∈ Nsuchthatforall n,n′ ≥ n∗,it
holdsthatsupp(G n) = supp(G n′).
We make some initial remarks about stable generators. First, any generator G that is consistent
andachievesbreadthisalsostable,sinceaftersomefinitetimeitssupport,unionthetrainingset,
becomes K and remains so. (Here, whether G repeats training examples or not is not crucial –
the two types of generators are interchangeable; see Remark 2.) Second, this notion of stability
can be seen as trying to capture practical heuristics such as learning rate schedules and early
stoppingthatreducetheamountofchangestothegeneratorasmoreandmoresamplesareseen.
Moreover,theoriginalworkofGold[Gol67]alsorequirestheidentifiertostabilizetoaconsistent
guess, and, more recently, the stability property of learning algorithms was explored in the PEC
learningsettingofMalliarisandMoran[MM23].
Having defined stability, we proceed to discuss relaxations of generation with breadth. In-
tuitively, consistent generation with breadth requires the generator to eventually stop making
mistakes – where a mistake is any element x that G incorrectly includes (if x ̸∈ K or x is part of
thetrainingsamples)orexcludes(if x ∈ K)fromitssupport. Wenowrelaxthisandonlyrequire
that, eventually, the generator G makes finitely many mistakes. Observe that this is a non-trivial
12requirement because the languages contain infinitely many strings and, so, at the start, G is ex-
pected to make infinitely many mistakes. A valuable observation is that it is possible for two
languages L and L to only differ in finitely many strings even if each contains infinitely many
1 2
strings. Withthisobservation,itisnottoohardtoseethattheaforementionedrequirementistoo
weak to capture a reasonable notion of generation from the target language K. Indeed, it would
allow generators that, given examples from K, perpetually generate outputs (with breadth) from
alanguage Lthatisnottheactualtargetlanguage–whichisasevereformofhallucination.
Hence, to create a meaningful model, we must impose some further restrictions on the mis-
takes of the generator G. The above example motivates that, at the least, the generator G should
be“closer”togeneratingfromK thansomelanguage L ̸= K with L ∈ L . Wecallsuchagenerator
unambiguous.
Definition8(UnambiguousGenerator). AgeneratingalgorithmG = (G ) isunambiguousfor alan-
n
guagecollectionLif, forany K ∈ Landeveryenumerationof K, itssupporteventuallybecomescloserto
Kthantoanyotherlanguage L ̸= KinLintermsofthesymmetricdifferencemetric,i.e.,thereexistssome
n∗ ∈ Nsuchthatforalln ≥ n∗ itholdsthat
|supp(G )△K| < min |supp(G )△L|,
n n
L∈L: L̸=K
whererecallthatfortwosetsSand T,S△T := (S\T)∪(T\S).
Figure1: AnUnambiguousGeneratorThatneitherHasConsistencynorBreadth. Inthisexample,the
language collection L has two languages L and K, where K denotes the target language. The red
curve denotes L, the dashed green curve denotes K, and the blue curve denotes the support of
supp(G ).ThegeneratorG hallucinatessincesupp(G )\K ̸= ∅ anddoesnotachievebreadthfor
n n n
the target K since B = K\supp(G ) is non-empty. Nevertheless, this generator is unambiguous
n
as|supp(G )\K|+|B| < |supp(G )\L|+|A|.
n n
Here, we pause to observe that this notion of generation is a significant relaxation of generation
with breadth that we considered earlier (Definition 4). Not only does it allow the generator to
hallucinate certain strings not in the target K and omit strings actually in K for arbitrarily long,
the number of hallucinations and omissions can be arbitrarily large, depending on the structure
13L
ofthelanguagecollection . Surprisingly,weshowthateventhisveryweaknotionof“consistent
generationwithbreadth”isnotachievablebyalargeclassofgenerators.
Informal Theorem 7 (see Theorem 3.6). For every language collection L that is not identifiable in the
limit, no stable generating algorithm (G ) for which MOP(·) is decidable, can generate unambiguously
n
fromLatanyrate.
Thus,undermildconditions,nostablealgorithmcangenerateunambiguouslyfromanon-identifiable
collection. Moreover, we also prove an analog of Informal Theorem 7 in the online setting (see
Theorem 3.7), which extends our earlier result for generation with breadth in the online setting
(Informal Theorem 4). This raises several questions regarding unambiguous generation, which
we leave as interesting open problems (see Section 1.4). Note that while this impossibility result
has a benign requirement that the generator is stable, it already considerably extends our main
resultInformalTheorem1, sinceanygeneratorthatachievesbreadthmustbestable–otherwise,
its support cannot settle on the target language K. (Note that while Informal Theorem 1 requires
thegeneratortonotrepeattrainingexamples,anygeneratorthatrepeatstrainingexamplescanbe
convertedintoonethatdoesnotrepeattrainingexamplesandvice-versa;seeRemark2.)
1.4 Takeaways,Discussion,andOpenProblems
We believe that a key takeaway of our results is that the question of Kleinberg and Mullainathan
[KM24]seemstoopenanavenuetowardsaformalmoderntheoryoflanguagegenerationbridg-
ing learning theory and traditional TCS fields, like complexity theory and formal languages. As
we explain in the subsequent technical overview, our tools contribute to this direction by con-
nectingclassicallinesofworkonidentificationofformallanguagestracingbacktoGold[Gol67],
Angluin [Ang79; Ang80; Ang88] and computability theory [Soa99; Sip12], to modern learning
paradigmssuchaslearningcurves[BHM+21]andlanguagegeneration[KV24;KM24].
Next,weemphasizethatourimpossibilityresult(Theorem3.3)isnotadeadendforlanguage
generation. Instead,itillustratestheneedforadditionalhumanfeedbackduringthepost-training
process–whichprovidesadditionalinformationoverpositivesamplesalone–toachieveeffective
language models. Indeed, if both positive and negative examples are available, then generation
withbreadthisachievableforallcountablecollectionsoflanguages.8 Inotherwords, ourresults
can be seen as further theoretical evidence of the benefits of post-training with human feedback,
highlighting its importance in developing language models that achieve both consistency and
breadth,andaddingtopriortheoreticalresultsfromKalaiandVempala[KV24].
Further, we underline that even though we focus on a prompt-less generation setting [KV24;
KM24],mostofourresultsimmediatelyextendtoapromptedsettingusingtheapproachofKlein-
bergandMullainathan[KM24].
RemarksandOpenQuestions. Wenowstateafewremarksregardingourresultsandposesome
interestingopenquestions. First,asabyproductofourresults,weestablishalmosttightratesfor
8This follows from the work of Gold [Gol67], which showed that any countable collection of languages can be
identifiedwithsuchfeedback. Usingappropriatebatchingandboosting, weshowthatthisidentificationalgorithm
(whichworksinthelimit)canbeconvertedtoagenerationalgorithmwithbreadththatachievesanexponentialrate.
Concretely,Theorem3.11showshowtoidentifyatanexponentialrateandProposition6.5showshowtoconvertthis
toagenerationalgorithm.
14identification and generation with positive examples (see Section 3.1 and Section 3.4 for formal
statementsanddiscussion). Obtainingtightratesforthesetasksisaninterestingproblem.
Next,ourimpossibilityresultscapturealargeclassoflanguage-generatingalgorithmsbutdo
not completely forbid consistent generation with breadth. An immediate open question is how
much further we can extend the class of generating algorithms for which the impossibility result
inInformalTheorem1holds.
OpenQuestion1. Isthereaclassofgenerativealgorithmsforwhichtheinducedgeneratorscan
bemodeledasTuringmachinesandwhichachievebreadthandconsistencyforallcountable
collectionsoflanguages?
Further, we also proved a more robust version of our main result (Informal Theorem 1), namely,
InformalTheorem7,whichshowedthatnoalgorithmfromalargeclassofgeneratorscangenerate
while making a “small” number of hallucinations or omissions (also see Section 3.3 for another
robust version of Informal Theorem 1). It is interesting to understand if one can prove a more
robustversionofInformalTheorem1. Tothisend,weproposethefollowingproblem.
OpenQuestion2. What is the Pareto frontier of an approximate notion of breadth and consis-
tency? In other words, if we fix a collection of languages and allow the generator to hallu-
cinateatsomegivenrate,whatistheminimalfractionofthemassfromthetargetlanguage
thatthisgeneratorhastomiss?
Next, tothebestofourknowledge, itisnotpossibletotestifalanguagecollectionisidentifiable
in the limit (without access to a strong oracle); this, for instance, becomes evident by inspecting
Angluin’s criterion for identifiable collections (see Definition 10). Hence, we would like to know
thefollowing:
OpenQuestion3. Is there a best-of-both-worlds algorithm between consistent generation and
generation with breadth, i.e., is there an algorithm that will always generate in the limit
from the target language consistently but, whenever identification is possible, it will also
achievebreadth?
WemakesomeinitialprogressonthisquestionbyshowingthatthealgorithmproposedbyKlein-
berg and Mullainathan [KM24] already achieves this best-of-both worlds guarantee, provided it
hasaccesstoasubsetoracleforL thatanswersqueriesoftheform“is L ⊆ L ?” (seeSectionB.2).
i j
Finally, our algorithm that achieves (almost) exponential rates for identification uses an algo-
rithmforidentificationinthelimitasablackbox. However,ouralgorithmthatachievesexponen-
tialratesforgenerationmakesuseofcertainspecificpropertiesofthealgorithmofKleinbergand
Mullainathan[KM24]. Thus,weaskthefollowingquestion.
OpenQuestion4. Is there a black-box transformation from an algorithm that generates in the
limitintheonlinesettingtoanalgorithmthatgenerateswithexactlyexponentialratesinthe
statisticalsetting?
151.5 FurtherRelatedWorks
OursettingisbasedonthestatisticalformulationofAngluin[Ang88],whostudiedidentification
from stochastic examples in the limit. However, Angluin [Ang88] does not provide any learn-
ing rates which is one of the main aspects of our work. In terms of techniques, our inspiration
for the statistical rates comes from universal learning, initiated by Bousquet, Hanneke, Moran,
van Handel, and Yehudayoff [BHM+21] and studied in Bousquet et al. [BHM+21], Hanneke et
al. [HKM+22], Kalavasis, Velegkas, and Karbasi [KVK22], Bousquet et al. [BHM+23], Hanneke,
Moran,andZhang[HMZ23],andAttiasetal.[AHK+24]. However,aswehavealreadyexplained
therearevariousdifferencesbetweenoursettingandourtechniques(weprovideamoreextensive
andself-containeddiscussioninSectionD).
Ourworkconnectsvariousdisjointstrandsofresearchandwediscusseachoneofthembelow.
Theory on Hallucinations. In terms of rigorous evidence about hallucinations in LLMs, we
have already mentioned the work of Kalai and Vempala [KV24] at the start of Section 1. The
result of Kalai and Vempala [KV24] is that calibrated9 language models must hallucinate. The
fascinatingimplicationofthisresultisthatonecanlowerboundtherateofhallucination,i.e.,the
quantity E S∼Pn,x∼Gn1{x ∈/ K}, by the extent of a model’s calibration. Their intuition is that the
root of hallucinations are rare patterns in the training data. Informally, their main result (under
assumptions on K and P ) is that for any trained model G with n samples, the hallucination rate
n
E S∼Pn,x∼Gn1{x ∈/ K} ≥ R(cid:98)−MisP(G n)−1/√ n, where R(cid:98) is the fraction of facts that only appear
onceinthetrainingdataandMisP(G n)istheamountofmiscalibrationofthemodel. Hence,ifthe
model is calibrated, i.e., MisP(G n) ≈ 0, the hallucination rate is lower bounded by the rare facts’
rate. Comparedtoourwork, theirgoalistoshowaquantitativelowerbound, whichisobtained
P
under assumptions on the training distribution and the fact that the model is calibrated. Our
goalisdifferent: wewanttounderstandwhetheramodelcanachievebreadthwhileavoidinghal-
lucinationsbuildingontherecentworkofKleinbergandMullainathan[KM24]. Wealsoreferthe
readertoKalaiandVempala[KV24]foranextensiveoverviewofappliedworksonhallucinations.
Peng, Narayanan, and Papadimitriou [PNP24] use communication complexity to prove that
thetransformerlayerisincapableofcomposingfunctionsifthedomainsofthefunctionsarelarge
enough. ThisworkcouldalsobeseenasrigorousevidenceaboutthehallucinationsofLLMssince
functioncompositionisafundamentaltaskforreasoning[GLL+24].
TheworkofXu,Jain,andKankanhalli[XJK24]isalsostudyinghallucinationsofLLMs. They
define hallucination as a failure to identify the target function which belongs to an uncountable
collectionoffunctions. Thisissignificantlystrongerthanthedefinitionweandpriorworks[KV24;
KM24] have considered (making their impossibility results significantly easier to prove). Their
main result is that all LLMs must hallucinate. This is easy to see: consider an LLM learning to
predict the next element in a sequence of 0s and 1s, after observing only a finite prefix of the
enumeration, it hasno way ofknowing thenextelement intheorder (sincetheyallow bothcon-
tinuations)and,hence,thetargetsequencecannotbeidentified.
Finally,theworkofAithaletal.[AML+24],whichismainlyempirical,aimstoexplainhalluci-
nations on the other important family of generative models, namely diffusion-based models, via
9Theexactdefinitionofcalibrationisnotimportantforthiswork:alanguagemodeliscalibratedif,roughlyspeak-
ing,thestringsthatthemodelassignsprobabilitymassp,appearinapfractionofthetruedistribution[Daw82].
16mode interpolation which, in theory, relies on difficulties in approximating non-smooth parts of
thescorefunction.
Language Learning. In our results, we make no implicit assumption about the architecture
of our models; this is in accordance with the works of Solomonoff [Sol64], Gold [Gol67], An-
gluin[Ang82],AngluinandSmith[AS83],Angluin[Ang88],Pitt[Pit89],andKleinbergandMul-
lainathan[KM24]. However,therearevariousworksaimingatunderstandinglanguagelearning
capabilitiesofspecificarchitectures, e.g., [Elm90; GS01; Mer19; BAG20; EGZ20; Hah20; HHG+20;
YPP+21; MS23]. For instance, Liu et al. [LAG+23] show that low-depth transformers can repre-
sent the computations of any finite-state automaton, while Sanford, Hsu, and Telgarsky [SHT23]
identify a particular mathematical problem that cannot be computed by single-layer multi-head
transformers. Theaforementionedworkssharesomesimilaritieswithusinthesensethattheyfo-
cusonwhethermodelscanbetrainedtogenerateorrecognizestringsinafixedformallanguage.
Akyürek et al. [AWK+24] study in-context language learning: the language model is prompted
with a finite collection of strings from an unknown regular language (which changes across dif-
ferenttasks),andmustinferthedistributionoverstringscorrespondingtothefulllanguage. Ina
similarspirit,Edelmanetal.[EEG+24]studyin-contextlearningofMarkovchains. Otherrelated
works are those of Xie et al. [XRL+22] and Hahn and Goyal [HG23] that study conditions under
whichin-contextlearningcanariseforlanguagelearning.
Allen-Zhu and Li [AL24] design context-free grammars and empirically study the consistent
generation (accuracy) and breadth (diversity) of GPT models on these synthetic examples. In
comparisontothiswork,weprovideatheoreticaltreatmentofthetrade-offbetweenconsistency
andbreadthunderaveryabstractmodel,studiedbyGold[Gol67],Angluin[Ang79;Ang88],and
Kleinberg and Mullainathan [KM24]. Our results indicate that, even in a very idealized frame-
work, achieving (perfect) consistency and breadth is impossible. We view the empirical findings
of Allen-Zhu and Li [AL24] as an exciting indication that, in the real world (or more concretely
in controlled experiments on “small” models and synthetic datasets), a balance between (imper-
fect)consistencyandbreadthispossibleandmodernLLMscanachieveit. Furtherunderstanding
how much consistency and breadth one can achieve at the same time theoretically is an exciting
direction.
Finally,inaconcurrentandindependentwork,Li,Raman,andTewari[LRT24]alsostudylan-
guagegeneration,interpretingitinalearning-theoreticsettingreminiscentofthePACframework
andtheonlinelearningsettingofLittlestone[Lit88]. Theypropose“non-uniformgeneratability”
–whichrelaxes“uniformgeneratability”[KM24]–andcharacterizethecollectionsforwhichuni-
formandnon-uniformgeneratabilityareachievableintheGold-Angluinmodel;inparticular,un-
L
like Kleinberg and Mullainathan [KM24] they also allow the collection to contain uncountably
manylanguages. ThesedimensionsareanalogstotheLittlestonedimension(anditsextensionto
the non-uniform setting [Lu23]), which only holds for finite collections of languages. Moreover,
theyshowtheproposeddimensionisincomparabletotheVCdimension. Finally,theygiveanal-
ogous characterizations in the “prompted generation” setting, extending some of the results of
Kleinberg and Mullainathan [KM24]. Our work is orthogonal to theirs: first, we study trade-offs
betweengeneratingwithandwithoutbreadth–bothinastatisticalsettingandtheGold-Angluin
model – and, second, we study the “learning curves” for generation and identification in the
frameworkofBousquetetal.[BHM+21].
17Probably Eventually Correct Learning. As we mentioned Gold’s model is a predecessor of the
famousPACmodelofVapnik[Vap13]andValiant[Val84]. Anaturalquestioniswhetherthereis
a conceptual meeting point for the two works. Is there a notion of “PAC learning in the limit?”
Theanswertothisquestionisaffirmativeandcomesfromthefieldofalgorithmicstability(seee.g.,
[ABL+22; BGH+23; CMY23; KKM+23; MSS23] and the references therein), studied in the context
ofbinaryclassification[MM23].
MalliarisandMoran[MM23]introducetheProbablyEventuallyCorrect(PEC)modeloflearn-
ing. HerewefixacollectionL = {L ,L ,...}oflanguagesandadistributionP overpositiveand
1 2
negativelabeledexamples(incontrasttothestandardidentificationsettingofGold). PEClearning
P L
focusesondistributions realizablebythecollection inthesenseofBousquetetal.[BHM+21]
L P
(seeSectionD).AnalgorithmissaidtoPEClearn ifforanyrealizabledistribution ,withprob-
ability1overi.i.d. samples{(x ,y ): i ∈ N}drawnfromP ,thereexiststimet∗ ∈ N suchthatfor
i i
allt ≥ t∗,given{(x ,y ): 1 ≤ i ≤ t},thealgorithmoutputsan L ∈ L suchthat
i i t
Pr [L (x) ̸= y] = 0.
t
(x,y)∼P
Malliaris and Moran give a combinatorial characterization of the collections of languages that
L
are PEC learnable: a collection of languages is PEC learnable if and only if it does not shatter
an infinite Littlestone tree. We stress that, when the learner has access to positive and negative
examples, the absence of an infinite Littlestone tree does not characterize identification in our
setting. This is in stark contrast with binary classification. In particular, in Section D, we show
thatthereexistsasetoflanguagesthathaveaninfiniteLittlestonetree,hencenotlearnableinthe
onlinesettingofBousquetetal.[BHM+21],butitallowsforidentificationinthelimitwithpositive
andnegativeexamples. Infact,thecollectionweuseinExample3isidentifiableinthelimiteven
withjustpositiveexamples. Thisalreadysetsthestageforastarklydifferentlandscapeofoptimal
learning rates between the setting of Bousquet et al. [BHM+21] and Angluin [Ang88], as we will
seeinSection3.1.
As we said before, the online model of Gold [Gol67] and the classical online setting of Little-
stone [Lit88] have various differences. Lu [Lu23] studies non-uniform online learning in order
to bridge the gaps between the inductive inference model of Gold [Gol67] and classical online
learning. In this setting, the adversary is oblivious and fixes the true language K in advance (as
in Gold’s model). At each round, an example from K is revealed, the learner makes a prediction
but then she observes feedback. The model is non-uniform in the sense that the mistake bound
dependsonK.
Learning from Positive Examples. Learning from positive examples occurs very frequently in
real-world applications and has been extensively studied. A lot of work has been done on learn-
ingfrompositiveexamplesinGold’smodeloflearninginthelimit[Gol67;Ang80;Ber86;Ang88;
Shi90;ZL95]. Apartfromthat,anextensionofValiant’sPACmodelhasbeenalsostudied[Nat87;
Den98]. Natarajan [Nat87] considered the setting where the learner only has access to positive
examples and showed that even very simple classes such as halfspaces in two dimensions are
not learnable from positive examples alone. Denis [Den98] relaxed this requirement: they study
a setting where the learner has access to both positively labeled examples but also to unlabeled
examples[DGL05]. Attheheartofvirtuallyalloftheresultsinthislineofworkistheuseofunla-
beledsamplesinordertogeneratenegativeexamples. Whentheoriginaldistributionisuniform,
18better algorithms are known: De, Diakonikolas, and Servedio [DDS15] gave efficient learning
algorithms for DNFs and LTFs, Frieze, Jerrum, and Kannan [FJK96] and Anderson, Goyal, and
Rademacher[AGR13]gaveefficientlearningalgorithmsforlearningd-dimensionalsimplices. On
the other side, Rademacher and Goyal [RG09] and Eldan [Eld11] give lower bounds for learning
withpositiveexamples.
Recently, interest in learning from positive examples has sparked from work on truncated
statistics (e.g., [DGT+18; DGT+19; KTZ19; FKT20; DKT+21; Ple21; DNS23; DLN+24; DKP+24;
LMZ24]). Kontonis, Tzamos, and Zampetakis [KTZ19] show how to learn concept classes of
bounded Gaussian surface area from positive Gaussian examples and Lee, Mehrotra, and Zam-
petakis[LMZ24]generalizethistoshowhowtolearnconceptclassesapproximablebypolynomi-
als in the L -norm from positive examples. However, all these works focus on computationally
2
efficientlearning/testingwhilewefocusonstatisticalconsistencyofidentificationandgeneration
withoutanyrestrictionsoncomputationtime.
2 Model and Preliminaries
Inthissection,weintroducenotationandpreliminariesthatareusefulinsubsequentsections.
CountableDomainsandEnumerations. Wealwaysassumethatlanguagesaresubsetsofsome
fixed infinite and countable domain X . Since X is infinite and countable, after a suitable bijective
X N X
mapping, one can think of as . In some cases, one may also like to think of as the set of
(arbitrarily long) strings over a finite alphabet Σ , i.e., Σ∗. This is again without loss of generality
since N is bijective to {0,1}∗ (e.g., using the standard binary encoding). Depending upon the
context, we use one interpretation (X = N ) or the other (X = Σ∗), whichever is more intuitive.
The notion of enumeration is important in our work; fix a set L ⊆ X . We refer to L as a language.
An enumeration of L is a complete and ordered listing of all the elements in L that allows for,
potentially, repetitionsofelements. Inparticular, anenumeration x ,x ,... of L hastheproperty
1 2
that for any element w ∈ L there is a finite index i such that x = w. For example, 1,2,3,... is
i
N
a valid enumeration of but 2,4,6,... is not (since, in the latter sequence, odd numbers do not
appearatanyfiniteposition).
AdditionalNotation. WeuseA,I,andG todenotealgorithms,andoftenreserveG foragenera-
tor, i.e., analgorithmthatgivenexamples x ,...,x ∈ X , outputsanewexamplefromX . Weuse
1 n
P andD todenotedistributionsovertheelementsofsomelanguage L ⊆ X . Weusestandardno-
tationrelatedtodistributions: FixadistributionP overlanguage L. Givenanelementx ∈ X ,P(x)
denotestheprobabilitymassP assignstox. ThesupportofdistributionP isdenotedbysupp(P),
i.e.,supp(P) := {x ∈ L: P(x) > 0}. Asashorthand,givenasequencex ,x ,...,x ,foreachindex
1 2 n
1 ≤ i ≤ n, we use x to denote the prefix {x ,x ,...,x }. Finally, we use standard notation for
≤i 1 2 i
indicatorfunctionsandlimits: Givenanexpression E(suchash ̸= K ors ∈ K),1{E}denotesthe
indicatorthat Eistrue. Forafunction R: N → R ≥0, R ↓ 0denotesthatlim n→∞R(n) = 0.
Language Collections and Membership Oracle to Languages. We always consider a countable
collection of languages L = {L ,L ,...} and assume we have access to a membership oracle that,
1 2
givenanindexiandastrings,outputs1{s ∈ L },asisstandardinallpriorworks[Gol67;Ang80;
i
19KM24]. Thisismotivatedbythefactthatiftheselanguagesare“reasonable,”e.g.,theyaregener-
ated by context-free grammars or decided by Turing machines [Sip12], then (1) there can be only
countablymanyofthemand(2)eachofthemadmitsamembershiporacle. Finally,wereservethe
letterK todenotetheunknowntargetlanguageK ∈ L . Wewillsaythatanexample x isapositive
exampleforK if x ∈ K;otherwise xwillbeanegativeexampleforK.
2.1 LanguageIdentificationandGenerationintheLimit
In this section, we first present the Gold-Angluin model for identification in the limit and, then,
KleinbergandMullainathan’smodelforgenerationinthelimit.
LanguageIdentificationintheLimit
The problem of language identification in the limit from positive examples was introduced by
Gold [Gol67] and further studied by Angluin [Ang79; Ang80]. The setting is specified by a col-
lection of languages L = {L ,L ,...}. For a fixed collection L , an adversary and an identifier
1 2
playthefollowinggame: Theadversarychoosesalanguage K fromL withoutrevealingittothe
identifier, and it begins enumerating the strings of K (potentially with repetitions) x ,x ,... over
1 2
a sequence of time steps t = 1,2,3,.... The adversary can repeat strings in its enumeration, but
the crucial point is that for every string x ∈ K, there must be at least one time step t at which it
appears.
At each time t, the identification algorithm I, given the previous examples x ,x ,...,x , out-
1 2 t
putsanindexi thatcorrespondstoitsguessforthetruelanguageK.
t
Definition 9 (Language Identification in the Limit [Gol67]). Fix some language K from collection L.
TheidentificationalgorithmI identifiesKinthelimitifthereissomet∗ ∈ Nsuchthatforallstepst > t∗,
theidentifier’sguessi
t
satisfiesi
t
= i
t−1
and L
it
= K.ThelanguagecollectionLisidentifiableinthelimit
ifthereisanidentifierthatidentifiesinthelimitanyK ∈ L ,foranyenumerationofK.
Gold [Gol67] showed that collections of finite cardinality languages, i.e., each language in the
L
collection isfinite,canbeidentifiedinthelimitfrompositiveexamples. Thisistruesinceinthe
limit,onewillseealltheelementsofthetarget(finite)language,atwhichpointitcanbeidentified.
The identification algorithm is the following: at time t, guess L to consist solely of the elements
that have occurred in the sequence. Since L is finite, there will be a finite time after which all
elements of L will have been revealed, so after that the algorithm will have identified the target.
Interestingly,allfinitecollectionsoflanguagesarealsoidentifiableinthelimit[Gol67].
A super-finite collection of languages denotes any collection which contains all languages of
finitecardinalityandatleastoneofinfinitecardinality. Goldshowedthatsuper-finitecollections
of languages cannot be identified in the limit from positive examples. Further, he showed that
negative examples help: any super-finite collection can be identified in the limit using positive
and negative examples10 (the idea is simple: keep guessing the infinite language until seeing a
negativeexample;thenitreducestothefinitecase).
10ThismeansthattheadversarypresentsanenumerationofthewholedomainX,withalabelindicatingwhetherthe
exampleisinthetargetlanguage.
20Theorem 2.1 ([Gol67]). Let L = {L∞,L 1,L 2,...} be the language collection with L
1
⊂ L
2
⊂ ··· ⊂
L∞ = ∪ i≥1L
i
and for each i, |L i| < ∞. Then, there is no algorithm that identifies L in the limit from
positive examples. Moreover, this collection can be identified in the limit when the algorithm has access to
bothpositiveandnegativeexamples.
Theaboveresultalreadyshowsaseparationintermsofidentificationbetweenobservingonlypos-
itiveexamplesandobservingpositiveandnegativeexamplesinGold’smodel. Moreover,itraises
the question of whether there exist non-trivial collections of languages identifiable in the limit
from positive examples. In that direction, Angluin [Ang79] studied pattern languages (whose
definition is not important for our work) and showed that for that collection identification in the
limitispossibleonlywithpositiveexamples.
Thenextquestioniswhetheronecangetacharacterizationofthelanguagecollectionsthatcan
beidentifiedfrompositiveexamples. Angluin[Ang80]resolvedthisproblem.
Definition 10 (Angluin’s Condition [Ang80]). Fix a language collection L = {L ,L ,...}. Suppose
1 2
there is a membership oracle which, given a string x and index i, answers 1{x ∈ L }. The collection L is
i
saidtosatisfyAngluin’sconditionifthereisanoraclethatgivenanindexienumeratesasetoffinitestrings
T suchthat
i
T ⊆ L andforall j ≥ 1,if T ⊆ L then L isnotapropersubsetof L .
i i i j j i
The difficulty in trying to identify a language from positive examples is the problem of over-
generalization. Ifwhileseeingpositiveexamplesthealgorithmspecifiesalanguagethatisaproper
supersetofthetrueanswerK,thenbyonlyseeingpositiveexamplesitwillneverseeacounterex-
ample to that language. This would be avoided with positive and negative examples. Angluin’s
condition essentially ensures this over-generalization problem can be avoided by from just posi-
tiveexamples(withoutthehelpofnegativeexamples).
Before proceeding to Angluin’s result, we stress one important point: inspecting Angluin’s
definition, we can see that it requires access to a procedure that finds this set of strings T. This
i
oracleiscalledatell-taleoracleandisquitecrucialforAngluin’salgorithmtowork.
Definition10ledtothefollowingcharacterization.
Theorem 2.2 ([Ang80]). A countable language collection L is identifiable in the limit if and only if it
satisfiesAngluin’scriterion.
Finally, let us consider the case of language identification with both positive and negative exam-
ples,i.e.,whentheadversaryprovidesanenumerationofthewholedomainX
andeveryexample
hasalabelindicatingwhetheritisinthetruelanguageK.Wementionthatfocusingonalgorithms
equippedwithmembershiporacle,thefollowingresultappearsinGold[Gol67].
Theorem 2.3 ([Gol67]). Any countable language collection is identifiable in the limit from positive and
negativeexamples.
To see how the algorithm works, let L = {L ,L ,...} and denote by L the smallest indexed
1 2 z
language in L for which L = K. The algorithm observes an enumeration of the form (x ,y ) ∈
z t t
X×{0,1} for t ≥ 1. Recall this means that 1{x ∈ K} = y . The algorithm works as follows: in
t t
everytimestept ∈ N ,itpredictsthelowestindexofaconsistentlanguage,i.e.,thesmallest j ∈ N
21for which 1{x ∈ L } = y for all τ ≤ t. Consider two cases: if z = 1, then the algorithm will
τ j τ
neverpredictanylanguage L z′,z′ ≥ 2,soitwillbecorrectfromthefirststep. Ifz > 1,thenforall
L z′,z′ < z,thatcomebefore L z intheenumerationofL ,thereisafinitetimet z′ whentheexample
(cid:0) (cid:1)
x
t
z′,y
t z′
contradictsthelanguage L z′.
LanguageGenerationintheLimit
We now move to language generation in the limit from positive examples, introduced by Klein-
bergandMullainathan[KM24]. ThesetupisexactlythesameasintheGold-Angluinmodel(the
adversary provides an enumeration of K), but now the goal of the learner is to generate unseen
examplesfromK insteadofidentifyingtheindexofK. Theirformaldefinitionisthefollowing.
Definition 11 (Language Generation in the Limit [KM24]). Fix some language K from the collection
L = {L ,L ,...} and a generating algorithm G. At each step t, let S ⊆ K be the set of all strings that
1 2 t
the algorithm G has seen so far. G must output a string x ∈/ S (its guess for an unseen string in K).
t t
The algorithm G consistently generates from K in the limit if, for all enumerations of K, there is some
t∗ ∈ N such that for all steps t ≥ t∗, the algorithm’s guess a belongs to K\S . The collection L allows
t t
for consistent generation in the limit if there is an algorithm G that, for any choice of the target language
K ∈ L ,itconsistentlygeneratesfromK inthelimit.
Definition11straightforwardlygeneralizestorandomizedalgorithms;considerthesamesetupas
before except that now the output string a may be randomized. The definition of generation is
t
also the same except that instead of requiring a ∈ K\S one requires that the support A of the
t t t
distributionfromwhich a issampledisnon-emptyandsatisfies A ⊆ K\S .
t t t
Observe that language generation requires that the algorithm’s outputs are consistent with K
(in the limit), but allows the algorithm to not generate certain strings from K. For instance, if
K is the set of all strings, then the algorithm that always outputs even length strings (not in S ),
t
generates from K in the limit but also misses infinitely many strings in K (namely, all strings of
oddlength). Consistencyisclearlyadesirablenotion: withoutconsistency, algorithmsmaykeep
outputting strings outside the target language K which, when K is the set of all meaningful and
truestrings,inevitablyleadstohallucinations.
Atriviallyconsistentgeneratorisonethatoutputsdataalreadyseeninthetrainingset. Aswe
already mentioned, we count such outputs as mistakes. This form of predicting unseen positive
examples makes the task of generation interesting. At first sight, it seems that there is an easy
strategy that achieves generation in the limit: given an enumeration of all hypotheses L ,L ,...,
1 2
we sequentially generate from L (i = 1,2,...) until it becomes inconsistent with the sample S ;
i n
then we move to L . This strategy seems natural for generation because we know that there is
i+1
some index k such that the true language K = L . This idea has a fundamental issue, already
k
reported by Kleinberg and Mullainathan [KM24]: if there exists an index i such that i < k and
⊊
L L ,thenthegeneratorwillgetstuckat L andneverupdate.
k i i
Anon-trivialsolutiontothisproblemwasgivenbyKleinbergandMullainathan[KM24]. They
show that all countable sets of languages in countable domains allow for generation in the limit
from positive examples; this is in stark contrast with identification in the limit from positive ex-
amples.
22Theorem 2.4 (Theorem 1 in Kleinberg and Mullainathan [KM24]). There is an algorithm with the
property that for any countable collection of languages L = {L ,L ,...}, any target language K ∈ L ,
1 2
andanyenumerationofoneoftheselanguagesK,thealgorithmgeneratesfromK inthelimitwithpositive
examples.
We now provide some intuition on how this algorithm works. Let L ,L ,... be an enumeration
1 2
of the collection of languages and K be the true language. Let z be an index such that L = K.
z
We say that a language L is consistent with the sample S at time t if S is contained in L . Now
i t t i
assume that we have two languages L and L with L ⊆ L which are both consistent with S .
i j i j t
Then,itisclearthatthegeneratingalgorithmshouldprefertogeneratefromL ratherthanL : any
i j
w ∈ L \S satisfies w ∈ L \S . This property inspired Kleinberg and Mullainathan [KM24] to
i t j t
definethenotionofacriticallanguage. LetC = {L ,L ,...,L }. Alanguage L iscriticalatstept
n 1 2 n n
if L isconsistentwithS andforevery L ∈ C thatisconsistentwithS ,itmustbe L ⊆ L .There
n t i n t n i
aresomekeypropertiesuponwhichthegeneratingalgorithmisbuilt:
• Atanytime,thereisatleastonelanguageconsistentwithS ,thetrueoneL = K.Also,there
t z
is at least one critical language at any step t: for any t, the consistent language L with the
i
lowestindeximustbecriticalatstept,asitistheonlyconsistentlanguageinC.
i
• There exists times t for which L (which is K) is not critical. But eventually, L will become
z z
critical at some step and then remain critical forever after that. Also, any critical language
comingafter L mustbeasubsetof L ,thusitissafetogeneratefromit.
z z
• Hence the algorithm, roughly speaking, keeps track of a list of critical languages and gen-
erates from the last one in the list; this is because, after some finite index, all the critical
languagesaresubsetsof L and,hence,itissafetogeneratefromanyofthem.
z
Moredetailsaboutthisalgorithmwillappearlateronwhenwedesignourgenerationalgorithms
fortheprobabilisticsetting(seeSection5.2).
3 Overview of Results
Inthissection,wepresenttheformalstatementsofourmainresults. Webeginwithstatisticalrates
foridentificationandforconsistentgeneration(withouttherequirementofbreadth)inSection3.1.
Next,inSection3.2,wepresentourresultsforgenerationwithbreadth–showingthatnogenera-
torfromalargefamilyofgenerators(thatincludespresent-dayLLMs)cangeneratewithbreadth
from any language collection that is non-identifiable. Contrasting Kleinberg and Mullainathan
[KM24]’s result for generation without breadth, these results show that generation with breadth
is significantly harder – as hard as identification, for a large and natural class of generators. Sec-
tion 3.3 extends this impossibility result to a relaxation of generation with breadth, showing that
eventhisrelaxeddefinitionofgenerationandbreadthcannotbeachievedbythesamelargeclass
ofgenerators. Finally,inSection3.4,wepresentadditionalresultsforidentificationwhenonehas
someadditionalstructure(e.g.,accesstoastrongeroracle)orinformation(e.g.,negativeexamples).
233.1 ResultsforIdentificationandGenerationWithoutBreadth
PriorworkofGold[Gol67]andKleinbergandMullainathan[KM24]studieslanguageidentifica-
tionandgenerationinanonline,i.e.,adversarialsetting. Inthiswork,westudythedistributional
versionsoftheseproblems. Theidentificationproblemwestudyisnotnewand,infact,goesback
toAngluin’sworkin1988[Ang88]. However,Angluin[Ang88]doesnotprovideanyrateatwhich
language identification can be achieved as the number of samples observed increases (when it is
achievable).
SummaryofResultsinThisSection. Inthissection,wegivelearningratesforbothidentification
and generation (see Theorems 3.1 and 3.2 respectively). For both tasks, we study the learning
curves – that is how the identification or generation error decays as the sample size increases.
As a result, we extend the results of Gold [Gol67] and Kleinberg and Mullainathan [KM24] to
the statistical setting. Our results in this section achieve a near-optimal rate for identification
(Theorem3.1)andanoptimalrateforgeneration(Theorem3.2).
3.1.1 UniversalRates: ModelandPreliminaries
We work under the universal rates framework, introduced by Bousquet, Hanneke, Moran, van
Handel,andYehudayoff[BHM+21],inordertocapturethenotionofalearningcurveforlanguage
identificationandgeneration. Followingthenotationweusedbefore,recallthatwehaveacount-
able set of languages L = {L 1,L 2,...}, where each L ∈ L is also countable and ∪ L∈LL ⊆ X , for
X
somecountabledomain .RecallthenotionofavaliddistributionproposedbyAngluin[Ang88]
in this setting (Definition 1). Intuitively, this condition can be thought of as the equivalent of
realizabilityintheclassificationsetting.
The learning algorithm is a sequence of (universally measurable and computable) functions
{h n} n∈N, where n captures the size of the training set. We are interested in understanding the
behavior of the error of the algorithm, which is defined appropriately based on the downstream
task – either identification or generation for this paper. Given some rate function R: N → [0,1]
wesaythatwecanachieverate R(n) forthesetoflanguageL andthelossfunctioner(·) ifthere
existsalearningalgorithm{h n} n∈N whoseerrorsatisfies
(∀validP)(∃C,c) suchthat E[er(h ))] ≤ C·R(cn), ∀n ∈ N .
n
Crucially, these learning curves are distribution-specific; the constants c,C depend on P but the
rate R holds universally for all valid distributions. Such learning curves are a well-studied topic
inlearningtheory[Sch97;AL98;BHM+21;VL23]. Theabovegivesrisetothefollowingdefinition.
Definition12(LearningRates[BHM+21]). GivenalanguagecollectionL,anerrorfunctioner(·),and
aratefunction R: N → [0,1]satisfyinglim n→∞R(n) → 0,wesay:
• Rate R isachievableforLifthereisanalgorithm {h n} n∈N suchthatforeveryvaliddistributionP,
thereexistc,CforwhichE[er(h )] ≤ C·R(c·n),∀n ∈ N .
n
• Noratefasterthan R(n) isachievableforLifforallalgorithms {h n} n∈N thereexistsavaliddistri-
butionPandc,CforwhichE[er(h )] ≥ C·R(c·n),forinfinitelymanyn ∈ N .
n
Further,wehavethefollowing.
24• (OptimalRate)Rate RisoptimalforLifitisachievableandnoratefasterthan Risachievable.
• (NoRate)WesaythatLadmitsnorateifforeveryalgorithm{h n} n∈N thereexistsavaliddistribu-
tionPsuchthatlimsup E[er(h )] > 0.
n→∞ n
L
In the case of identification, to avoid trivial cases, we consider collections that contain at least
twodistinctlanguagesthatcontainonecommonelement.
Definition 13 (Non-Trivial Collections of Languages for Identification). A language collection L is
non-trivialforidentificationifthereexisttwolanguages L ,L ∈ Lsuchthat L ̸= L and|L ∩L | > 0.
1 2 1 2 1 2
L
Notice that if the collection does not satisfy Definition 13, then one can identify the target lan-
guageK immediatelyafterobservingasingleelementfromK.
In the case of generation, the “non-triviality” condition turns out to be more nuanced, e.g.,
comparedtothecaseofidentificationaboveorbinaryclassification[BHM+21]. Wegiveaninfor-
mal definition below, and we refer to Definition 17 for the formal one and a discussion about its
necessity.
InformalDefinition1(Non-TrivialCollectionsofLanguagesforGeneration,seeDefinition17). A
languagecollectionLisnon-trivialforgenerationifanyalgorithmneedstoseeatleasttwoexamplesfrom
thetargetlanguagetobeabletogeneratefromit.
3.1.2 UniversalRatesforIdentification
For any language collection L = {L ,L ,...} and n ∈ N , with true language K ∈ L , and set of
1 2
examples x ,...,x ∈ Xn, an identification algorithm I gets as input x ,...,x and outputs an
1 n n 1 n
indexI n(x 1,...,x n). Wedefinetheidentificationerrorofthelearner{I n: Xn → N} n∈N as
er(I (x ,...,x )) = 1{L ̸= K}. (4)
n 1 n In(x 1,...,xn)
Under this definition, E
x
1,...,xn∼P[er(I n)] = Pr
x
1,...,xn∼P[L
In(x 1,...,xn)
̸= K], i.e., the probability that it
failstoidentifythecorrectlanguageafteritseesnexamplesfromP .11
Our main result for identification is a fundamental dichotomy: every non-trivial collection of
languages is identifiable with positive examples at either an (almost) exponential rate or it is not
identifiableatanyrate.
Theorem3.1(DichotomyofRatesforIdentificationwithPositiveExamples). Foreverycollectionof
countablymanylanguagesLthatisnon-trivialforidentificationexactlyoneofthefollowingholds:
• Foreveryg(n) = o(n)thereexistsalearnerthatidentifiesLatratee−g(n).Moreover,nolearnercan
achievearatefasterthane−n.
•
Lisnotidentifiableatanyrate.
Concretely,thefirstconditionholdsforLifandonlyifitsatisfiesAngluin’scondition(Definition10).
11Onesubtlepointisthatthisdefinitionallowsthelearnertooutputanyindexj∈NsuchthatL =Kandtheremay
j
bemanysuchindicessincewedonotassumealllanguagesinLaredistinct. Ouridentificationalgorithmswillhave
thepropertythattheyoutputthesmallestindexatwhichKappearsinL={L ,L ,...}.
1 2
25Thisdichotomyofratesdiffersfromprioruniversalratesforclassificationwheretheusualtheme
is a trichotomy of rates [BHM+21; KVK22; HMZ23]. Moreover, while in the universal setting for
binary classification, any measurable class of functions is learnable at arbitrarily slow rates, in
identification, this is not the case: there exist collections of languages that do not admit a Bayes
consistentlearnerandtheseareexactlythecollectionsthatdonotsatisfyAngluin’scondition. For
thefullproof,wereferthereadertoSection5.1.
3.1.3 UniversalRatesforConsistentGeneration
Themaindifferencebetweenthissettingandthesettingoflanguageidentificationisthedefinition
P
oftheerrorrate. Thereexistsavalidtext-generatingdistribution ,meaningonethatissupported
on some target language K ∈ L , and the learning (or rather, generating) algorithm is a sequence
of (universally measurable and computable) functions {G n: Xn → X} n∈N, where each G
n
takes
as input n samples generated i.i.d. from P and outputs a new word, with the goal that this word
belongsto thetarget language(see Remark1). As inthe onlinesetting, toavoid trivialsolutions,
wewanttogenerateexamplesthatdonotappearinthetrainingset.
Remark 1 (Notation for Generating Algorithms). More formally, a generating algorithm is a col-
lection of mappings {G n} n∈N, where for each n, G
n
is a mapping from the domain of n training
samplesXn tothesetof“generators”or(randomized)TuringmachinesG
that,oneachexecution,
X
outputasamplefrom . Forthissection,itissufficienttoimaginegeneratorsasbeingdetermin-
istic(i.e.,generatingsamplesfromapointmass)and,hence,wesimplifywritingG asamapping
n
from
Xn
to
X
. In the next section, where we study generation with breadth, to have any hope of
achievingbreadth,weneedtoconsiderG
initsfullgeneralityasamappingfromXn toG
.
n
Now,wearereadytodefinethegenerationerror: foranyn ∈ N andsetofexamplesx ,...,x ∈ Xn
1 n
wedefinethegenerationerrorofthelearner{G n: Xn → X} n∈N forthistaskas
er(G (x ,...,x )) = 1{G (x ,...,x ) ∈/ K\{x ,...,x }}. (5)
n 1 n n 1 n 1 n
Noticethat,underthisdefinition,
E [er(G (x ,...,x ))] = Pr [G (x ,...,x ) ∈/ K\{x ,...,x }],
n 1 n n 1 n 1 n
x 1,...,xn∼Pn x 1,...,xn∼Pn
i.e., the probability that the learner fails to generate a new word from the target language after
observing n examples from it. Our main result in this section is that we can achieve consistent
generationwithexponentialrates.
Theorem 3.2 (Rates for Generation). For every countable collection of languages L there exists a gen-
erating algorithm that generates from L at rate e−n. Conversely, for every collection of languages that is
non-trivialforgeneration(Definition17),nogeneratingalgorithmcanachieveratefasterthane−n.
Surprisingly, thisshowsthatconsistentgenerationcanbeachievedatanexponentialrateforany
countable collection of languages. We mention that the result we prove is slightly stronger: we
show that, for any L , with probability at least 1−C·e−c·n, we can generate infinitely many new
stringsfromK,aftertrainingthealgorithmonnexamples–notjustasingleword. Together,Theo-
rems3.1and3.2showthatthestarkseparationbetweenlanguageidentificationandgenerationin
theonlinesetting,obtainedbyKleinbergandMullainathan[KM24],alsoextendstothestatistical
setting of Angluin [Ang88] and Bousquet et al. [BHM+21]. The proof of Theorem 3.2 appears in
Section5.2;seeFigure3foranoutlineoftheproof.
263.2 ResultsforGenerationWithBreadth
Next, we present our results for language generation with breadth. Clearly, generation with
breadthisastrongerrequirementthangeneration. But,atleastintuitively,itisweakerthanidenti-
fication: itonlyrequiresonetogeneratesamplesfromtheentiresupportofKandnotidentifythe
indexofK. Contrarytothisintuition,ourresultsshowthat,foralargeclassofgenerators,genera-
tionwithbreadthisashardasidentification. Ourresultsshowthat,whilethisclassofgenerators
ispowerfulenoughtogeneratewithoutbreadth,nogeneratorinthisclasscanachievegeneration
withbreathfornon-identifiablecollectionsoflanguages.
3.2.1 MembershipOracleProblem
The family of generators we consider is implicitly determined by the decidability of a certain
problemassociatedwiththegenerator.
Definition 5 (Membership Oracle Problem). Given a generator G, the membership oracle problem for
G, denoted as MOP(G), is defined as follows: given the description of G and a string x, output Yes if
x ∈ supp(G)andoutputNootherwise.
Asmentionedbeforethedecidabilityofproblemsisextensivelystudiedinformallanguagesand
complexitytheory[Sip12]. Ourmainresult(InformalTheorem1whoseformalstatementappears
as Theorem 3.3) applies to any generator G for which MOP(G) is decidable. Note that our result
only needs a decider of MOP(G) to exist – this is purely a property of the generation algorithm
used – and it does not, for instance, require the individuals training the generator or the users to
haveaccesstothedeciderinanyfashion.
Togainsomeintuitionaboutthemembershiporacleproblem,letusconsiderasimpleexample.
Example1(StandardNext-TokenPredictor). LetG beatextgeneratororlanguagemodel
next-token
that generates text token-by-token: at each step t, it generates certain scores {p (σ): σ ∈ Σ} and
t
outputstokenσwithprobability∝ p (σ). Itisnotimportanthowthesescoresaregenerated. They
t
canbegeneratedinvariousways. Forinstance,theycanbethelogit-scoresoftransformer-based
models. They could also, be generated by thresholding logit-scores in any complicated but com-
putableway–suchas,byusingbeamsearch,top-K,ortop-psampling[HBD+20]. MOP(G )
next-token
is decidable and, in fact, there is a simple decider: given a string w of length n, it computes the
scores for the first n iterations; where in the t-th iteration (t > 1), it conditions on the event that
G next-token has generated the string w 1w 2...w t−1 so far. Then it computes the following function
andoutputstheresult
(cid:40)
Yes if ∏n p (w ) > 0,
t=1 t t
No otherwise.
Westressthatourmainresultonlyneedstheexistenceofsuchadecider,anddoesnotrequirethe
individualstrainingthegeneratorortheuserstohaveanyaccesstoit.
273.2.2 ResultsforGeneratorsforWhichMOP(·)IsDecidable
Before stating our result about the rate at which generation with breadth can be achieved, we
G
need to define the corresponding error function. For the error to make sense, let be the set of
X
(randomized) Turing machines that do not take any input and output one element from (on
each execution). Given a target language K and examples x ,...,x ∈ X , we define the error for
1 n
generationwithbreadthforthelearner{G n: Xn → G} n∈N as
er(G (x ,...,x )) = 1{supp(G (x ,...,x )) ̸= K\{x ,...,x }},
n 1 n n 1 n 1 n
wheresupp(G (x ,...,x ))isthesetofstringsG (x ,...,x )canoutputwithpositiveprobability,
n 1 n n 1 n
i.e.,itisthesupportofthedistributionofoutputsofG (x ,...,x ). Theabovemeansthatwecount
n 1 n
eachsteptasamistakeifthegeneratingalgorithmhasapositiveprobabilityofoutputtingastring
outsideofK(i.e.,hallucination),azeroprobabilityofoutputtinganunseenelementofK(i.e.,mode
collapse),orapositiveprobabilityofrepeatingaseentrainingexample.
Remark 2 (Generating Examples From the Training Set). For generation without breadth, it is im-
portant to restrict the generator from outputting elements it has already seen. Otherwise, the fu-
tilegenerator,whichalwaysoutputsthefirsttrainingsampleitsees,achievesgenerationwithout
breadth. Thisrequirement,however,isnotimportantforgenerationwithbreadth: anygenerator
G that generates with breadth without repeating training examples can be converted to one G′
thatgenerateswithbreadthandrepeatsthetrainingexamplesandviceversa.12 Hence,allofour
resultsholdwitheithernotionofgenerationwithbreadth.
Our main result shows a separation between the rates achievable for generation with and
withoutbreadthbyanygeneratingalgorithmforwhichMOP(·)isdecidable.
Theorem 3.3. Let G be the set of all generating algorithms (G ) for which MOP(·) is decidable (Defi-
n
nitions 5 and 6). For every collection of countably many languages L that is non-trivial for generation
(Definition17)andnotidentifiableinthelimit:
•
NogeneratingalgorithminGgenerateswithbreadthfromLatanyrate;and
•
ThereisageneratingalgorithminGthatgeneratesconsistentlywithoutbreadthfromLatratee−n.
Conversely,nogeneratingalgorithm(evenoutsideofG)cangenerateataratefasterthane−n.
Further,foranycollectionofcountablymanylanguagesLthatisnon-trivialforgeneration(Definition17)
andidentifiableinthelimit,andforany g(n) = o(n),thereisageneratingalgorithminGthatgenerates
withbreadthfromLatratee−g(n). Conversely,nogenerationalgorithmcangenerateconsistentlyatarate
fasterthane−n,evenwithoutthebreadthrequirement.
Thus, while generation without breadth is achievable for any countable collection of languages
(whether it is identifiable or non-identifiable), generators in G can only generate with breadth
from identifiable collections – which are a very restricted subset of all languages [Gol67; Ang80;
KM24]. ItremainstodiscusswhichtypesofgeneratorsMOP(·)isdecidablefor,andwepresenta
12For instance, G′ can run G with probability 1/2 and with the remaining 1/2 probability output a training sample
selecteduniformlyatrandom. GivenG′,G canbeimplementedbyrejectionsamplingasfollows: repeatedlyexecute
G′untilitgeneratesanunseenelementxandoutputx.
28largefamilyinthenextsection. Meanwhile,duetoExample1,itisalreadyclearthatTheorem3.4
applies to present-day LLMs. The proof of this result appears in Section 6.2; see Figure 5 for an
outlineoftheproof.
Our negative result leaves several interesting questions open which we already discussed in
Section1.4.
3.2.3 AFamilyofGeneratorsforWhichMOP(·)IsDecidable
Example1alreadyshowsthatMOP(·)isdecidableformanyexistinglanguagemodels. Next,we
showthatMOP(·) isdecidableunderevenfewerrestrictionsonthegeneratorG –informally, we
willallowforanygeneratorwhichgeneratestexttoken-by-token.
Definition 14 (Token-by-Token Generators). Token-by-token generators G are parameterized by ran-
domized Turing machines M. M can be randomized and halts on all inputs. Given M, the corresponding
token-by-tokengeneratorG generatesoutputsasfollows: foreacht ∈ N,
M
1. Letw 1w 2...w
t−1
bethetokensgeneratedsofar.
2. Let A beanyauxiliaryinformationgeneratedsofar,where A istheemptystring.
t 1
3. Generate(s t,A t+1)byrunning Mwithinputw 1w 2...w
t−1
and A t.
4. Ifs = EOS(i.e.,endofstring),thenoutputs ...s andhalt;otherwiseproceedtoiterationt+1.
t 1 t
Notethattoken-by-tokengeneratorsareaverypowerfulclass: forinstance,anydistributionover
Σ∗
for some finite alphabet
Σ
admits a token-by-token generator by the Bayes rule. That said, of
course,onecanalsoconstructnon-token-by-tokengenerators.
WeshowthatMOP(G)isdecidableforalltoken-by-tokengenerators.
Theorem3.4. Foranytoken-by-tokengeneratorG,MOP(G)isdecidable.
Next, we demonstrate that token-by-token generators capture several interesting language mod-
els. First,thefamilyoftoken-by-tokengeneratorscapturesexistinglargelanguagemodels(LLMs):
forinstance,tosimulateanLLML,wedefinethenexttokenpredictor MasaTuringmachinethat
simulatesLontheprovidedstringuntilLgeneratesonenewtoken. Further,sincewedonotplace
computationalrestrictionson M, McanalsosimulateinteractionsbetweenLLMsorauxiliarysys-
tems that select a suitable LLM to respond depending on the request–a strategy that has led to
recent advances in text generation [SDD+23; JSR+24; Kni24; Tea24]. Finally, due to a reduction
to the halting problem, there are some generators for which MOP(·) is undecidable and give an
explicitexampleinSectionA.
Remark3(NoisyMembershipOracle). Asupposedlyweakerrequirementthanthedecidabilityof
MOP(·)istheexistenceofanoisyoraclethat,givenastring x,correctly(andinfinitetime)decides
the membership of x into supp(G) with a probability at least 2/3. However, due to the folklore
resultthatBPP ⊆ EXP[AB09],anoisyoracleisequivalenttothedecidabilityofMOP(·).
293.2.4 ResultsforGenerationWithBreadthintheLimit
In this section, we state the implications of our techniques for generation with breadth in the
adversarialoronlinesettingofGold[Gol67]andAngluin[Ang79;Ang80].
Theorem 3.5. For every non-identifiable collection of countably many languages L, no generating algo-
rithm,forwhichMOP(·)(Definitions5and6)isdecidable,cangeneratewithbreadthfromLinthelimit.
IfLisidentifiable,thenthereisageneratorG (forwhichMOP(G)isdecidable)thatgenerateswithbreadth
fromL.
This result makes important progress on a question left open by Kleinberg and Mullainathan
[KM24]forafairlylargefamilyofgenerators, whichincludesalliterativegeneratorsduetoThe-
orem 3.4. In particular, MOP(·) is decidable for the generation algorithm of Kleinberg and Mul-
lainathan [KM24] (since it is deterministic and the unique element it outputs can be computed
byexecutingthealgorithm)and, hence, theaboveresultshowsthatKleinbergandMullainathan
[KM24]’salgorithmcannotgeneratewithbreadthinthelimitfromanynon-identifiablecollection.
Further, inSection3.3westrengthenthisresultbyshowingthatevenarelaxednotionofgenera-
tionwithbreadthremainsunreachableforalargeclassofgenerators. Theproofofthisresultcan
befoundinSection6.3.
3.3 ResultsforGenerationWithApproximateConsistencyandBreadth
In this section, we study a relaxation of generation with breadth, which we call unambiguous
generation,andask: Isthereageneratorthatunambiguouslygeneratesfromanon-identifiablecollection?
We recall that, in this section, we will allow the generator to repeat examples in the training
data. Like all of our results with breadth, this choice is not crucial, and all of the results have
analogs where the generator does not repeat training examples (Remark 2). We make this choice
tosimplifythenotation.
We refer the reader to Section 1.3 for a discussion and motivation of the definition for unam-
biguousgeneration,whichwerestatebelow.
Definition8(UnambiguousGenerator). AgeneratingalgorithmG = (G ) isunambiguousfor alan-
n
guagecollectionLif, forany K ∈ Landeveryenumerationof K, itssupporteventuallybecomescloserto
Kthantoanyotherlanguage L ̸= KinLintermsofthesymmetricdifferencemetric,i.e.,thereexistssome
n∗ ∈ Nsuchthatforalln ≥ n∗ itholdsthat
|supp(G )△K| < min |supp(G )△L|,
n n
L∈L: L̸=K
whererecallthatfortwosetsSand T,S△T := (S\T)∪(T\S).
This notion is a significant relaxation of generation with breadth that we considered so far (see
Section3.2): Notonlydoesitallowthegeneratortohallucinatecertainstringsnotinthetarget K
andomitstringsactuallyinKforarbitrarilylong,thenumberofhallucinationsandomissionscan
L
beverylargeand,dependingonthestructureofthelanguagecollection ,evenarbitrarilylarge.
Surprisingly, even this very weak notion of “generation with breadth” turns out to be un-
achievable by a very large family of generators. Concretely, it is unachievable by any generator
forwhichMOP(·)isdecidableandthatsatisfiesthenaturalpropertythatitstabilizesafterafinite
time. Westatetheformalnotionofstabilitybelow.
30Definition7(Stability). Ageneratingalgorithm(G )isstableforalanguagecollectionLifforanytarget
n
language K ∈ Landforanyenumerationof K,thereissomefinite n∗ ∈ Nsuchthatforall n,n′ ≥ n∗,it
holdsthatsupp(G n) = supp(G n′).
Beforeturningtoourformalresult,weneedtoconstructtheerrorfunctionthatdefinesunambigu-
ousgeneration,andweusethenaturalchoice: foralanguageK andexamples x ,...,x ∈ X ,we
i
1
in
denoteandwedefinetheerrorforunambiguousgenerationforthegeneratingalgorithm{G : Xn →
n
G} n∈N oninputS n = {x i 1,...,x in}as13
(cid:26) (cid:27)
er(G (S )) = 1 |supp(G (S ))△K| < min |supp(G (S ))△L| ,
n n n n n n
L∈L: L̸=L
wheresupp(G (S ))isthesetofstringsG (S )canoutputwithpositiveprobability. Similartothe
n n n n
caseofidentificationandgeneration,wesaythatanalgorithmachievesunambiguousgeneration
for a collection L at some rate R, where R: N → R ≥0,R ↓ 0, if for any valid distribution P
with respect to L there are c,C > 0 so that E xi1,...,xin∼Pn [er(G n(x i 1,...,x in))] ≤ C·R(c·n). The
following result shows that this notion of generation is not achievable, for a large and natural
classofgeneratingalgorithms.
Theorem 3.6 (Impossibility of Unambiguous Generation). For every non-identifiable collection of
countably many languages L, no stable generating algorithm, for which MOP(·) (Definitions 5 and 6)
isdecidable,canunambiguouslygeneratefromLatanyrate.
Note that while this result has a benign requirement that the generator is stable, it already con-
siderablyextendsourmainresultTheorem3.3,sinceanygeneratorthatachievesbreadthmustbe
stable–otherwise,itssupportcannotsettleonthetargetlanguage K. (Tobeprecise,Theorem3.3
required generators to not repeat their training examples, but this requirement is not crucial and
anygeneratorthatdoesrepeatitstrainingexamplescanbeconvertedintoonethatdoesnotrepeat
itstrainingexamples,andvice-versa;seeRemark2.)
InadditiontoTheorem3.6,wealsoproveitsanalogintheonlinesetting–significantlyextend-
ingourearlierimpossibilityresultintheonlinesetting(Theorem3.5). Beforestatingtheresultin
the online, we introduce unambiguity in the limit, which is a natural counterpart to its statistical
definition:
• AgeneratingalgorithmG = (G )issaidtobeunambiguousforacollectionL = {L ,L ,...}
n 1 2
if,foranyK ∈ L andenumerationx ,x ,... ofK,thereisann ≥ 1,suchafterseeingn ≥ n
i
1
i2 0 0
elementsS = x ,...,x ,
n i
1
in
|supp(G (S ))△K| < min |supp(G (S ))△L|.
n n n n
L∈L: L̸=K
Theorem 3.7 (Impossibility of Unambiguous Generation in the Limit). For every non-identifiable
collection of countably many languages L, no generating algorithm stable in the limit for which MOP(·)
(Definitions5and6)isdecidablecanunambiguouslygeneratefromLinthelimit.
13RecallthatGisthesetof(randomized)TuringmachinesthatdonottakeanyinputandoutputoneelementfromX
(oneachexecution).
31The proofs of Theorems 3.6 and 3.7 appear in Section 7.1 and 7.2, respectively. To develop some
intuition,werecommendreadingtheproofofTheorem3.7beforetheproofofTheorem3.6.
Remark 4. In Section C, we study another notion of generation with approximate breadth which,
informally,requiresthatthegeneratingalgorithmisconsistentandputszeromassonlyonfinitely
many points of the target language K. This is also a weakening of generation with breadth and
turnsouttobeincomparabletothenotionofunambiguousgenerationstudiedinthissection.
3.4 FurtherResultsforIdentification
In this section, we present identification algorithms that achieve exact exponential rate when one
L
hassomeadditionalstructure –accesstoastrongeroracle, orafinitecollection , ora countable
L
collection offinitelanguages–oradditionalinformation–negativeexamples.
In Section 3.4.1, we allow the identifier to make queries of the form “is L ⊆ L ?” Next,
i j
L
in Section 3.4.2, we consider generation from collections containing finitely many languages
L = {L ,L ,...,L }. (Note that each language in L can still be infinite.) Finally, in Section 3.4.4,
1 2 k
in addition to positive examples, we also give the identifier access to negative examples (i.e., ele-
ments x ∈ X notinthetargetlanguageK).
3.4.1 ExponentialRatesforIdentificationUsingSubsetOracle
L
Our first result shows that when satisfies Angluin’s condition and the learning algorithm has
access to a subset oracle for L (which answers queries of the form “L ⊆ L ?”) then it is possible
i j
toachieveexactexponentialrates.
Proposition 3.8. For every countable language collection L that satisfies Angluin’s condition (Defini-
tion 10), there exists a learning algorithm that has access to a subset oracle for L and identifies L at a
rate e−n. Formally, a subset oracle is a primitive that, given two indices i and j, outputs Yes if L ⊆ L ;
i j
otherwise,itoutputsNo.
Recallthatouralgorithmthatachievesalmostexponentialratesrequiresmerelyblack-boxaccess
L
toanalgorithmthatidentifies inthelimit. Inotherwords,itdoesnotmakeuseoftheparticular
structure of the online identification algorithm. To achieve exact exponential rates, we make use
of a particular algorithm: the one proposed by Kleinberg and Mullainathan [KM24]. At a high
level,theproofconsistsofthefollowingsteps:
C1 First,weshowthatKleinbergandMullainathan[KM24]’salgorithmwithaccesstoasubset
oracleforL can,infact,identifythetargetlanguage(seeSectionB.1).
C2 Next, we identify a sufficient condition that allows one to use any identification algorithm
L
that identifies in the limit to obtain exponential rates (see Lemma 8.1). Interestingly, this
conversiondoesnotneedanychangestotheidentificationalgorithm.
C3 Finally, we show that the algorithm of Kleinberg and Mullainathan [KM24] satisfies this
condition.
ThefullproofofProposition3.8appearsinSection8.1.
323.4.2 ExponentialRatesforIdentificationofFiniteCollections
We now shift our attention to finite collections of languages. Gold [Gol67] and Angluin [Ang80]
showed that all finite collections are identifiable in the limit. We show that for such collections
we can get exact exponential rates, without the need of the subset oracle we used in the previous
result(Proposition3.8).
Proposition3.9. ForeveryfinitelanguagecollectionL,thereexistsalearningalgorithmwhichidentifies
Lataratee−n.
The proof of this result builds on the proof of Proposition 3.8. In particular, we show that the
L
algorithmofpriorworksatisfiesthesufficientconditionthatallowsanalgorithmthatidentifies
in the limit to obtain exponential rates (Condition C2). The full proof of Proposition 3.9 appears
inSection8.2.
3.4.3 ExponentialRatesforIdentificationofCollectionsofFiniteLanguages
Wenowmoveontoaresultaboutidentifyingcountablecollectionsoffinitelanguageswithexactly
exponentialrates. Gold[Gol67]showedsuchcollectionsareidentifiableinthelimitthroughavery
simple algorithm: predict the first language that contains the set of all examples seen so far. We
showthatforsuchcollectionswecangetexactexponentialrates.
Proposition 3.10. For every countable language collection L that only contains languages of finite size,
thereexistsalearningalgorithmwhichidentifiesLataratee−n.
Theideaoftheproofissimple. Sinceanyvaliddistributionhasfinitesupport,forlargeenoughn,
the sample will contain all the elements of the support with probability 1−C·e−c·n. The formal
proofofProposition3.10appearsinSection8.3.
3.4.4 ExponentialRatesforIdentificationfromPositiveandNegativeExamples
We now shift our attention to a setting – introduced by Gold [Gol67] – where, in addition to an
enumerationofthetargetlanguageK,onealsoreceivesanenumerationofX\K.
Letusfirstrecallthedifferencebetweenthedifferenttypesofinformationinthetwosettings.
In the case of just positive examples (considered so far), the adversary picks a target language K
L
from along with an enumeration of this language, and presents the examples from this enu-
merationsequentiallytothelearner. Inthecaseofpositiveandnegativeexamples,theadversary
againpicksatargetlanguageKfromL ,butnowitchoosesalabeledenumerationofthewholedo-
X
main , where now the label of each element indicates whether it belongs to the target language
K or not. It is known that every countable collection of languages is identifiable in the limit with
positiveandnegativeexamples[Gol67].
Naturally,weneedadifferentnotionofavaliddistributioninthissetting. Weadoptadefinition
thatwasproposedbyAngluin[Ang88].
Definition15(ValidDistributionsUnderPositiveandNegativeExamples[Ang88]). Adistribution
PoverX×{0,1}isvalidwithrespecttoacollectionoflanguagesLifandonlyifsupp(P) = Xandthere
existssomeK ∈ Lsuchthatforall x ∈ XitholdsthatPr (X,Y)∼P[Y = 1 | X = x] = 1{x ∈ K}.
33Our main result in this setting is that every countable collection of languages is identifiable with
positiveandnegativeexamplesatanoptimalexponentialrate.
Theorem 3.11 (Identification with Positive and Negative Examples). For every countable collection
oflanguagesL,thereexistsalearnerthatidentifiesLatratee−n.Conversely,foreverycountablecollection
oflanguagesLthatisnon-trivialforidentification,nolearnercanidentifyLatratefasterthane−n.
The proof of Theorem 3.11 appears in Section 8.4. Our proof of this result is inspired by the
approach of Bousquet et al. [BHM+21]. First, we show the exponential rates lower bound by
directly using a result of Bousquet et al. [BHM+21]. In order to get the upper bound, we use a
L
black-boxtransformationfromanylearnerthatidentifies inthelimit,toalearnerthatachieves
exponentialratesinthestatisticalsetting.
The approach shares similarities to the one with just positive examples (see Section 1.2, Para-
graph B). The crucial reason why we can obtain exactly exponential rates here, instead of almost
exponentialratesasintheprevioussetting,isthatwecanusethenegativeexamplestoaccurately
estimate the correct sizes of the batches we use, instead of having to use “guesses” of increasing
sizeaswedidinthesettingofjustpositiveexamples.
To give a more concrete comparison to the binary classification setting of Bousquet et al.
[BHM+21], let us first explain some results from this work. Bousquet et al. [BHM+21] define
the following infinite game sequential, which is appropriately rephrased using the terminology
fromourworklooksasfollows:
• Ineveryround,theadversarypresentsaword x ∈ X tothelearner.
t
• Subsequently,thelearnerpredictsalabelfrom{0,1}forthisword,denotedbyy .
(cid:98)t
• Then,theadversaryrevealsthetruelabely tothelearner.
t
The only constraint on the adversary is that at any given point t ∈ N , there has to be some
language K ∈ L suchthat y t′ = 1{x t′ ∈ K},forall t′ ≤ t.Inotherwords,thechoicesofthelabels
havetobeconsistentwithsomelanguageK ∈ L .Crucially,theconsistentlanguagedoesnotneed
to be fixed in advance and it can keep changing throughout the interaction. In their setting, the
learner “wins” the game if it makes only finitely many mistakes. They provide a necessary and
L
sufficientconditiononthestructureof whichdeterminestheexistenceofawinningstrategyfor
L
the learner: the learner can win this game if and only if does not have an infinite Littlestone
tree (see Definition 22). Interestingly, this condition does not capture the existence of a winning
L
strategy for the learner in Gold’s setting: we have constructed a language family which has
an infinite Littlestone tree, but it is identifiable in the limit from positive and negative examples.
Perhaps more surprisingly, this language is identifiable even with just positive examples. The
constructionappearsinSectionD.
4 Organization of the Rest of the Paper
Wenextdescribetheorganizationoftherestofthepaper.
• The proofs of Section 3.1 (statistical rates for identification and generation) can be found
in Section 5. The proof for the identification universal rates appears in Section 5.1 and for
generationinSection5.2.
34• TheproofsofSection3.2canbefoundinSection6. InSection6.1,wediscussthedecidability
of MOP(·). In Section 6.2 we provide our main result that generation with breadth is not
possibleforgeneratingalgorithmsforwhichMOP(·)isdecidable. Finally,inSection6.3,we
seetheimplicationsofthisresultforgenerationinthelimit.
• TheproofsofSection3.3appearinSection7. InSection7.1wegivetheproofoftheresultin
theonlinesettingandinSection7.2theproofoftheresultinthestatisticalsetting.
• TheproofsofSection3.4appearinSection8. InSection8.1wegivetheproofofexponential
ratesforidentificationusingasubsetoracle,inSection8.2theproofofexponentialratesfor
identificationoffinitecollectionsusingamembershiporacle,andinSection8.3theproofof
exponentialratesforidentificationofcountablecollectionsoffinitelanguages. Theprooffor
theidentificationrateswithpositiveandnegativeexamplesappearsinSection8.4.
5 Proofs from Section 3.1 (Rates for Identification and Generation)
5.1 ProofofTheorem3.1(RatesforIdentification)
Inthissection,wegivethefullproofofTheorem3.1;seeFigure2foranoutline. Aswealludedto
before,thefirststepintheproofistoshowthatallnon-trivialcollectionsarenotlearnableatrate
fasterthane−n.
Theorem3.1
For non-trivial collections Identification in the limit L is not identifiable in the limit =⇒
e−n is the best possible rate =⇒ Almost exponential rate L cannot be identified at any rate
Lemma5.1 Lemma5.5 Lemma5.8
Proposition5.2 Proposition5.3 Lemma 5.4 Lemma5.7
Theorem5.6
Figure2: OutlineofProofofTheorem3.1
Lemma5.1(ExponentialRateIsBestPossibleforIdentifyingAnyNon-trivialCollection). LetLbe
anon-trivialcollectionofcountablymanylanguages. Then,foranyidentificationalgorithmA = {h n} n∈N
thereexistsavaliddistributionPsuchthatE[er(h )] ≥ e−2n,forinfinitelymanyn ∈ N .
n
35Proof. Since L is non-trivial, there exist two distinct languages L ,L ∈ L and x ∈ X such that
i j
x ∈ L i,x ∈ L j. Let P Li,P
Li
be valid distributions for L i,L
j
that place at least 1/2 on x and if the
languages have more elements, they spread the remaining mass on the rest of the elements ar-
bitrarily; otherwise they put the remaining mass on x. Notice that since L ̸= L at least one of
i j
them has at least one more element other than x. For any n ∈ N , under both distributions, with
probabilityatleast2−n thealgorithmwillonlyseetheelement xappearinginthesamples. LetE
n
bethateventandconditiononit. Noticethat
(cid:104) (cid:105) (cid:104) (cid:105)
Pr L = L | E +Pr L = L | E ≤ 1,
hn(x,...,x) i n hn(x,...,x) j n
wheretheprobabilityiswithrespecttotherandomnessoftheidentificationalgorithm. Thus,we
(cid:104) (cid:105) (cid:104) (cid:105)
have that either Pr L
hn(x,...,x)
̸= L
i
| E
n
≥ 1/2 or Pr L
hn(x,...,x)
̸= L
j
| E
n
≥ 1/2 for each n ∈ N .
Hence, by the pigeonhole principle, for at least one of L ,L , the previous inequality holds for
i j
infinitely many n ∈ N . Assume, without loss of generality, that it holds for L
i
and let N(cid:98) denote
thesetofn ∈ N forwhichitholds. Then,foreachn ∈ N(cid:98),wehavethat
E [er(h (X ,...,X ))] = Pr [L ̸= L ]
X 1,...,Xn∼Pn
Li
n 1 n
X 1,...,Xn∼Pn
Li
hn(X 1,...,Xn) i
≥ Pr [L ̸= L | E ]· Pr [E ]
X 1,...,Xn∼Pn
Li
hn(X 1,...,Xn) i n
X 1,...,Xn∼Pn
Li
n
1
≥ ·Pr[L ̸= L | E ] (bythedefinitionofE )
2n hn(x,...,x) i n n
1
≥ , (duetotheassumptionon L )
2n+1 i
whichconcludestheproof.
We now move on to the (almost) exponential rates upper bound for identification. This will be
done via a transformation from learners that achieve identification in the limit in Gold’s model
[Gol67] to learners that achieve (almost) exponential rates in our setting. The first step in this
P
resultistoshowthatwhenwedrawcountablymanysamplesfrom alltheelementsofthetarget
languagewillappearinthesample.
Proposition5.2(InfiniteDrawsAreEnumerations). LetPbeaprobabilitydistributionsupportedona
countabledomainand{X i} i∈N,whereevery X
i
isi.i.d. fromP .Then,
Pr [supp(P) = ∪ i∈N{X i}] = 1.
{Xi} i∈N∼P∞
Proof. ForthedirectionPr
{Xi}
i∈N∼P∞[supp(P) ⊇ ∪ i∈N{X i}]noticethatforanyelementx ∈/ supp(P)
∑
Pr [x ∈ ∪ i∈N{X i}] ≤ Pr [x = X i] = 0. (6)
{Xi} i∈N∼P∞ i∈NXi∼P
Hence,
Pr [supp(P) ⊇ ∪ i∈N{X i}] = 1− Pr [∃x ∈/ supp(P),x ∈ ∪ i∈N{X i}]
{Xi} i∈N∼P∞ {Xi} i∈N∼P∞
∑
≥ 1− Pr [x ∈ ∪ i∈N{X i}]
x∈/supp(P){Xi} i∈N∼P∞
(6)
= 1.
36For the other direction, i.e., Pr
{Xi}
i∈N∼P∞[supp(P) ⊆ ∪ i∈N{X i}], notice that for any element x ∈
supp(P), Pr X∼P[X = x] is a positive constant p
x
> 0 and let (E
n
:= {X
n
= x})
n∈N
be a sequence
ofevents. Noticethattheseeventsareindependentandthat
∑ Pr[E ] = ∑ p = ∞ .
n x
n∈N n∈N
Hence,wecanapplythesecondBorel-Cantellilemma(seeLemmaE.2)andgetthat
(cid:20) (cid:21)
Pr limsupE = 1. (7)
n
n→∞
In other words, the element x will appear infinitely often in the stream X ,..., with probability
1
one. Therefore,
Pr [supp(P) ⊆ ∪ i∈N{X i}] = 1− Pr [∃x ∈ supp(P): x ∈/ ∪ i∈N{X i}]
{Xi} i∈N∼P∞ {Xi} i∈N∼P∞
∑
≥ 1− Pr [x ∈/ ∪ i∈N{X i}]
x∈supp(P){Xi} i∈N∼P∞
(7)
= 1.
A
Next, we show that for any algorithm that identifies the target language in the limit in the
adversarial(online)settingandforanyvaliddistributionP thereissomenumbert∗ := t∗(A ,P) ∈
N suchthat,whenwedrawt∗manyi.i.d. samplesfromP andusethemtosimulatetheadversarial
A
gamewith ,itwillidentifythetargetlanguagewithprobabilityatleast6/7.Wedenotethetime
ofthelastmistakeofthealgorithmA = {h n} n∈N onasequence x 1,x 2,...by TA(x 1,x 2,...),i.e.,
(cid:110) (cid:111)
TA(x 1,x 2,...) = inf n
0
∈ N : L
hn(x 1,...,xn)
̸= K, ∀n ≥ n
0
.
Proposition 5.3 (Tail Bound on the Distribution of Last Mistake). Fix any countable collection of
languagesLandletK ∈ Lbethetruelanguage. ForanyalgorithmA = {h n} n∈N thatidentifiesLinthe
limit in the online setting from positive examples and any valid distribution P for K (Definition 1), there
existsanumbert∗ ∈ Nsuchthat
6
{Xi}
iP ∈Nr ∼P∞[TA(X 1,X 2,...) ≤ t∗] ≥
7
.
Proof. Let X ,X ,..., be a countable i.i.d. sample from P . From Proposition 5.2 we get that this
1 2
sample is a valid input to A since, with probability one, it consists only of elements of K and
eventually every element of K appears in this sequence. Consider the execution of A on prefixes
ofthesequenceanddenotebyTA := TA(X 1,X 2,...)thetimeitmadeitslastmistake. Wehavethat
Pr
{Xi}
i∈N∼P∞[TA ∈ N] = 1.Thus,
tl →im
∞{Xi}
iP ∈Nr ∼P∞[TA(X 1,X 2,...) ≥ t] = 0.
Thus,asrequired,thereexistssomet∗ ∈ N suchthat
1
Pr [TA(X 1,X 2,...) ≥ t∗] ≤ .
{Xi} i∈N∼P∞ 7
37Thus far we have shown that for every valid distribution P there exists some number t∗ ∈ N so
that if we simulate the online learning process with t∗ samples i.i.d. from P , then the algorithm
identifies the true language K correctly with probability at least 6/7. However, the number t∗ de-
P
pends on the distribution , and hence we cannot immediately devise a learning strategy based
on it. To make the exposition easier to follow, let us first assume that we do know t∗; we will
shortlyrelaxthisassumption. Fornsufficientlylargeconsiderthefollowingalgorithm:
• Wesplittheinputsequenceinton/t∗ non-overlappingbatches,wherethei-thbatchconsists
oftheelements X (i−1)·t∗+1,...,X i·t∗.
• We use each of these sequences as an input to a copy of A and we get n/t∗ many predictors
(cid:110) (cid:16) (cid:17)(cid:111)
hi n X (i−1)·t∗+1,...,X i·t∗ .
i∈[n/t∗]
• Since these predictors might be outputting different indices (descriptions) of the same lan-
guage, we find the smallest indexed language the output of each classifier can be mapped
to. In other words, if j is the index outputted by the i-th batch, we find the smallest num-
i
ber j′ ∈ N such that L
ji
= L j′, and we set j
i
:= j′. Since we only have query access to the
languages,wecanonlyapproximatethisstep. Inparticular,forevery n ∈ N ,weset j := j′
i
if j′ ∈ N is the smallest number for which 1(cid:8) x
ℓ
∈ L ji(cid:9) = 1(cid:8) x
ℓ
∈ L j′(cid:9) , for all ℓ ∈ [n]. The
detailsarehandledinLemma5.4.
• We predict the index that at least (5/7)·(n/t∗) of the predictors agree upon; if no such lan-
guageexistsweoutputonearbitrarily.
Beforemovingtothegeneralcasewheret∗ isunknown,itisinstructivetoexplainwhytheprevi-
ous approach achieves exponential rates. Using standard concentration bounds, it is not hard to
seethatwithprobabilityatleast1−c·e−C·n,where c andC areP -dependentconstants,atleasta
5/7 fraction of the predictors will output an index that describes the true language. Conditioned
onthatevent,itisimmediatethata5/7-majorityiswell-definedandpredictingbasedonityields
thecorrectanswer.
Let us now explain how to handle the actual problem setting, in which as we mentioned, we
do not have knowledge of t∗. Let f: N → N be some (very slowly) increasing function of the
input size n, which we will specify shortly. Given that function, we use the following modified
approach,wheret∗ isreplacedby f(n).
• Wesplittheinputsequenceinton/f(n)non-overlappingbatches,wherethei-thbatchconsists
oftheelements X ,...,X .
(i−1)·f(n)+1 i·f(n)
• WeuseeachofthesesequencesasaninputtoacopyofA andwegetn/f(n)manypredictors
(cid:110) (cid:16) (cid:17)(cid:111)
hi X ,...,X .
n (i−1)·f(n)+1 i·f(n)
i∈[n/f(n)]
• Weusethepost-processingapproachfromLemma5.4,thatwealsoexplainedabove.
• We predict the index that at least (5/7)·(n/f(n)) of the predictors agree upon; if no such
languageexistsweoutputonearbitrarily.
38Since f(·) is increasing, there is some n ∈ N such that f(n ) = t∗. Thus, for n ≥ n we can
0 0 0
repeat the previous argument; with probability at least 1−c·e−C·n/f(n), at least (5/7)·(n/f(n))
of the predictors will be outputting the correct target language, so taking the majority vote over
them yields the desired result. Notice that now we do not achieve exactly exponential rates, but
foreverysublinearfunction g(·)wecanachieveratese−g(n).
Wefirststateandprovethepost-processinglemmatomaptheoutputsofpredictorsthatcor-
respondtodifferentindicesofthetargetlanguageK tothesameindex. Forthisresult,itisuseful
X
todefinethenotionof“projection”ofalanguageontoasubsetof .
Definition16(m-ProjectionofaLanguage). LetX = {x ,x ,...}beacountabledomainandletL ⊆ X
1 2
bealanguage. Forany m ∈ Nwedenoteby L[m] := L∩{x ,x ,...,x } theprojectionofthelanguage
1 2 m
ontothefirstmelementsofthedomain.
L
The point of the next lemma is the following: in the enumeration of the language collection ,
we allow repetitions of K (as in Gold’s model). Hence, when running multiple copies of our
identificationalgorithms, themajorityofthemwillidentify K; yetwecannotguaranteethatthey
will identify the same index for K (due to multiple appearances of K in the enumeration). This
X
lemmaguaranteesthatthereexistsasufficientlylargeprefixoftheenumerationofthedomain
sothattheprojectionofpredictedlanguageswillbemappedtothesmallestindexversionofK in
L ,whichwedenoteby L below.
z
Lemma5.4(Post-processingtoMaptoLowest-IndexOccurrenceofK). LetL = {L ,L ,...,}bea
1 2
countablecollectionoflanguagesoverX = {x ,x ...}andK ∈ L. Letz := min{j ∈ N : L = K}bethe
1 2 j
first index at which the target language appears in L. Let I = (i ,...,i ) be a multiset of indices and for
1 m
all1 ≤ ℓ ≤ max i ,n ∈ N,let
j∈[m] j
(cid:98)in
j
= min{1 ≤ ℓ ≤ i j: L ℓ[n] = L ij[n]},
be the index of the first language that has the same projection as L . Then, there exists a number n :=
ij 0
n (K,L ,X)thatdependsonK,L ,X,butnotI,suchthatforalln ≥ n
0 0
L = K =⇒ (cid:98)in = z,∀j ∈ [m].
ij j
Before we give the formal proof, let us explain the main idea and the implication of this result.
For every language L that precedes L , there exists some element x ∈ X such that 1{x ∈ L} ̸=
z
1{x ∈ L }. This will enable us to detect and remove all languages different from K preceding
z
L . Then, by taking projections onto large enough prefixes we indeed map any L = K,j > z, to
z j
L .Thisresultwillbeusefulforourconstructionsthatrequireaggregatingoutputsfromdifferent
z
executionsofthealgorithmwhich,withoutthispost-processingstep,canoutputdifferentindices.
ProofofLemma5.4. Assumewithoutlossofgeneralitythatforsome j ∈ [m]wehavethat L = K,
ij
otherwisethestatementholdsvacuously. Wewillhandlethecasesz = 1,z > 1separately.
Case A (z = 1): For any j ∈ [m] for which L = K and any n ∈ N we have that L [n] = L [n],
ij ij z
and since z = 1 this is the first index for which the equality holds. Hence, in this case, the claim
holdswithn = 1.
0
39CaseB(z > 1): SinceL isthefirstoccurrenceofKinL ,foralllanguagesL with1 ≤ ℓ < z,there
z ℓ
exists some x ∈ X such that 1{x ∈ L } ̸= 1{x ∈ L }. Let x be the smallest indexed element of
j z zℓ
X forwhichthepreviousholds. Moreover,letn = max z .Noticethatforalln ≥ n andall
0 1≤ℓ<z ℓ 0
1 ≤ j < z,holdsthat L [n] ̸= L [n].Furthermore,foralln ∈ N andall j ∈ [m]suchthat L = K it
j z ij
holdsthatL [n] = L [n].Combiningthesetwoclaims,wecandeducethatforall j ∈ [m]suchthat
ij z
L = K andforall n ≥ n thefirstindexi ∈ N suchthat L [n] = L [n] isindeed z.Noticethat n
ij (cid:98)0 ij i 0
dependsonlyontheenumerationofL ,X ,andthetargetlanguageK.
Wearenowreadytostateandprovetheformalresultregardingtheidentificationratesofcollec-
tionsthatareidentifiableinthelimit.
Lemma 5.5 (Reduction From Identification at Almost-Exponential Rate to Online Identification).
LetL = {L ,L ,...} beacountablecollectionoflanguagesand g: N → Nbeasublinearfunction. For
1 2
anyalgorithmA = {h n} n∈N thatidentifiesLinthelimitintheonlinesettingwithpositiveexamplesand
anyvaliddistributionPthereexistsanalgorithmA′ = {h′ n} n∈N suchthatforalln ∈ N
E [er(h′(X ,...,X ))] ≤ c·e−C·g(n) .
X 1,...,Xn∼Pn n 1 n
Proof. Let K := {L ,L ,...} ⊆ L be the set of all languages in L that correspond to represen-
i
1
i2
tations of K, i.e., for all L ∈ K it holds that L = K. First, notice that since g(n) = o(n) we can
constructsomenon-decreasingfunction f: N → N withlim n→∞ f(n) = ∞ andn/f(n) ≥ g(n).Let
t∗ ∈ N beanumbersuchthat
6
Pr [L ∈ K] ≥ .
X 1,...,X
t∗∼Pt∗
h t∗(X 1,...,X t∗)
7
From Proposition 5.3 such a number t∗ is guaranteed to exist. Hence, there is some n such that
0
foralln ≥ n , f(n) ≥ t∗.Thus,foralln ≥ n
0 0
(cid:104) (cid:105) 6
Pr L ∈ K ≥ .
X 1,X2,...,X f(n)∼Pf(n) h f(n)(X 1,...,X f(n)) 7
Recall the error of the classifier h (·) as defined in Equation (4). We have that for all n ≥ n , it
f(n) 0
(cid:16) (cid:17)
holdsthater h
f(n)
isaBernoullirandomvariablewith p ≤ 1/7.Thus, ifwehaveacollectionof
(cid:98)t
n
:= n/f(n)suchi.i.d. randomvariables,usingHoeffding’sbound[DP09]wegetthat
(cid:34) (cid:35)
Pr
1 ∑(cid:98)tn er(cid:16)
h
(cid:16)
X ,...,X
(cid:17)(cid:17)
≥
2
≤ e−2(cid:98)tn/49 ≤ e−2g(n)/49.
X 1,...,Xn (cid:98)t n i=1 f(n) (i−1)·(cid:98)tn+1 i·(cid:98)tn 7
Thus, for n ≥ n 0, atleasta5/7-fractionofthepredictorsoutputsanindexthatcorrespondstothe
targetlanguage.
WeconditiononthateventEn
fortherestoftheproof.
0
LetIn = (i ,...,i )bethemultisetoftheindicesofthelanguagesoutputtedbythepredictors
1 (cid:98)tn
in the previous step and z = min{ℓ ∈ N : L = K}. Then, using Lemma 5.4 we know that there
ℓ
existssomen := n (K,L ,X)suchthatforalln ≥ n
(cid:98)0 (cid:98)0 (cid:98)0
L
ij
= K =⇒ (cid:98)in
j
= z,∀j ∈ [(cid:98)t n],
40(cid:16) (cid:17)
where (cid:98)in is defined in Lemma 5.4. Thus, letting (cid:98)In = (cid:98)in,...,(cid:98)in we have that for all n ≥
j 1 (cid:98)tn
max{n 0,n (cid:98)0},atleasta5/7-fractionoftheindicesin(cid:98)In areexactlytheindexz.
Thus, forall n ≥ max{n 0,n (cid:98)0} andconditionedontheeventE 0n,wehavethatthe5/7-majority
vote over the indices in(cid:98)In corresponds to the the first occurrence of K in L . Since En occurs with
0
probabilityatleast1−e−2g(n)/49,thisconcludestheproof.
We now move on to the final ingredient we require for the proof of Theorem 3.1. It remains
toshowthatAngluin’sconditioncharacterizesthecollectionsoflanguagesthatcanbeidentified
at an (almost) exponential rate. First, we discuss a result from Angluin [Ang88] which our proof
builds upon. Let us first briefly describe the convergence criterion in Angluin’s paper. There is a
valid distribution P that is supported over some K ∈ L and the algorithm is presented with an
P
infinitesequenceofi.i.d. drawsfrom .Afterseeingeachexample,thelearnermustoutputsome
i ∈ N with the goal being that L = K. In that setting, an algorithm learns the target language if
i
for all but finitely many n ∈ N it outputs the same index i ∈ N for which L = K. Notice that
i
thelearningrequirementistwo-fold: i)thelearnerneedstostabilize,andii)theindexitpredicts
needstocorrespondtothetargetlanguage. Inthatsetting,Angluin[Ang88]showedthefollowing
result.
Theorem 5.6 (Corollary 10 [Ang88]). Let L be a countable collection of languages over a countable
domain X that does not satisfy Angluin’s condition (Definition 10). Then, for every learning algorithm
A = {h n} n∈N there exists a valid distribution P supported on some K ∈ L such that, with probability at
least1/3overthei.i.d. drawof{X n} n∈N,thelearnerdoesnotidentifyK.
In particular, Angluin’s result shows that, with probability at least 1/3, the learner will either not
stabilizetoanynumber i ∈ N oritwillstabilizetoanumber j ∈ N with L ̸= K.Ournextresult
j
provides a strengthening of Angluin’s result since it shows that with probability at least 1/3, the
learnerwill,infact,predictinfinitelymanytimesindicesthatdonotcorrespondtoK.
Lemma 5.7. Let L be a countable collection of languages over a countable domain X that does not satisfy
Angluin’scondition(Definition10). Then,foreverylearningalgorithmA = {h n} n∈N thereexistsavalid
distributionPsupportedonsomeK ∈ Lsuchthat
(cid:104) (cid:105) 1
Pr ∃i < i < i < ... : L ̸= K,∀j ∈ N ≥ .
{Xi} i∈N∼P∞
1 2 3 hij(X 1,...,Xij)
3
Proof. LetL beacountablecollectionoflanguagesthatdoesnotsatisfyAngluin’scondition. As-
sume towards contradiction that there is learner {h n} n∈N that, for any valid distribution P , with
probabilitycP < 1/3misidentifiesK infinitelyoften,i.e.,foranyvaliddistributionP
(cid:104) (cid:105) 1
{Xi}
iP ∈Nr
∼P∞
∃i
1
< i
2
< i
3
< ... : L
hij(X 1,...,Xij)
̸= K,∀j ∈ N = cP <
3
. (8)
We will construct a different learner {h′} which, for all valid distributions P , learns the corre-
n
sponding target language K in Angluin’s setting [Ang88] with probability at least 2/3. This will
createthedesiredcontradictionwithTheorem5.6. Leth′ bealearnerthatworksasfollows.
n
41Learnerh′
n
Input: Accesstoanyinfinitedraw X ,X ,... fromP∞ andoracleaccesstolearnerh(·)
1 2
Description:
1. Foreachn ∈ Ndo:
(a) Computei∗ = h (X ,...,X )
n 1 n
(b) Compute h′ n(X 1,...,X n) as the smallest index j∗ such that L j∗ classifies the first n
elements x ,x ,...,x ofthedomainX inthesamewayas L ,i.e.,
1 2 n i
h′ n(X 1,...,X n) := min(cid:8) j ∈ [i∗]: 1(cid:8) x
i
∈ L j(cid:9) = 1{x
i
∈ L i∗},∀i ∈ [n](cid:9) .
# NoticethatthiscanbedonewithO(n·i∗)manymembershipqueries;wherewesendn
queriestoeachofthefirsti∗ languagesinL .
Let z ∈ N be the smallest number for which L = K. We will handle the cases z = 1 and z > 1
z
separately.
(cid:110) (cid:111)
Case A (z = 1): If L = K, we have that 1 x ∈ L = 1{x ∈ L } for all x ∈ X .
h(X 1,...,Xn) h(X 1,...,Xn) 1
Hence,since1isthesmallestindex,wehavethatforalln ∈ N
L = K =⇒ h′(X ,...,X ) = z.
hn(X 1,...,Xn) n 1 n
Inthiscase,wedefinen := 1.
0
Case B (z > 1): Let 1 ≤ z′ < z. Then, since L is the first language for which L = K it must
z z
be the case that L z′ ̸= L z. Hence, the set S z′ := {x ∈ X : 1{x ∈ L z′} ̸= 1{x ∈ L z}} is non-empty.
We let ℓ z′ := min{i ∈ N : x i ∈ S z′}, i.e., let ℓ z′ be the smallest number that x ℓ z′ certifies that L z′ is
differentfrom L z. Noticethatℓ z′ < ∞ .Wealsodefinen 0 := max{ℓ 1,...,ℓ z−1},Noticethatforall
n ≥ n wehavethat
0
L = K =⇒ h′(X ,...,X ) = z.
hn(X 1,...,Xn) n 1 n
E
Let denotetheeventthat
|{n ∈ N : h (X ,...,X ) ̸= K}| < ∞ .
n 1 n
Noticethat,bydefinitionofcP,
Pr [E] = 1−cP.
{Xi} i∈N∼P∞
In other words, conditioned on the event E , the learner {h n} n∈N makes finitely many mistakes.
Thus, conditionedonE , foreverydraw D := {X i} i∈N ∼ P∞ thereissomenumber n D ∈ N such
that h (X ,...,X ) = K,∀n ≥ n . Let n′ = max{n ,n }. Then, conditioned on E , for every
n 1 n D D 0
n ≥ n′ wehavethat
h′(X ,...,X ) = z.
n 1 n
Since cP < 1/3 (see Equation (8)), 1−cP > 2/3 which implies that the learner {h′ n}
n∈N
converges
in Angluin’s model with probability greater than 2/3, for any choice of a valid data-generating
P
distribution .ThiscontradictsTheorem5.6andconcludestheproof.
42Let us now explain why Lemma 5.7 does not immediately imply a lower bound in our setting.
First, notice that the previous result says that any learner {h n} n∈N must make infinitely many
P
errors with probability at least 1/3, under some valid data-generating distribution . Expressing
thisusingalimsup(·)impliesthat14
(cid:20) (cid:21)
1
Pr limsup{L ̸= K} ≥ .
{Xi} i∈N∼P∞ n→∞
hn(X 1,...,Xn)
3
Since R: N → R ≥0 is a rate function, it satisfies lim n→∞R(n) = 0. Thus, in order to show that L
isnotlearnableatanyrate,itisenoughtoshowthatforanylearner {h n} n∈N,thereexistsavalid
(cid:104) (cid:105)
distribution P such that Pr X 1,...,Xn∼Pn L hn(X 1,...,Xn) ̸= K does not converge as n → ∞ , or if it does
convergeitholdsthat
(cid:104) (cid:105)
lim Pr L ̸= K ̸= 0.
n→∞X 1,...,Xn∼Pn hn(X 1,...,Xn)
Forthis,itsufficestoshowthat
(cid:104) (cid:105)
limsup Pr L ̸= K > 0, (9)
n→∞ X 1,...,Xn∼Pn
hn(X 1,...,Xn)
P
forsomevaliddistribution .ItfollowsfromthereverseofFatou’slemmathatforeverysequence
ofevents{E n} n∈N
(cid:20) (cid:21)
Pr limsupE ≥ limsupPr[E ],
n n
n→∞ n→∞
whichisnotsufficienttodeducetheresultweneed. Infact,itisnothardtoconstructafamilyof
eventssuchthat {E } suchthatPr[limsup E ] = 1,butlimsup Pr[E ] = 0 : consider
n n∈N n→∞ n n→∞ n
an infinite stream of independent coin flips, where the probability of success of the n-th try is 1/n.
ThesecondBorel-Cantellilemma(seeLemmaE.2)impliestheresult. Hence,weneedtostudythe
particularstructureofourproblemtoshowthatlimsup Pr[E ] > 0..
n→∞ n
In fact, to deduce that limsup Pr[E ] > 0, we show a stronger result: the limsup of the
n→∞ n
probability of error of the learner is not merely bounded away from zero, but, it is at least 1/2
(Lemma5.8). Tothatend,wefollowastrategywhichconsistsofthefollowingtwomainsteps:
• First,weassumethatthereexistsalearner {h n} n∈N suchthatforeveryvaliddistributionP
thereissomec > 0suchthat
(cid:104)(cid:110) (cid:111)(cid:105) 1
limsup Pr L ̸= K ≤ −c.
n→∞ X 1,...,Xn∼Pn
hn(X 1,...,Xn)
2
Then,weshowthatusingthelearner{h n} n∈N wecanconstructalearner{h′ n} n∈N suchthat
for all valid distributions E X 1,...,Xn∼Pn[1{L h′ n(X 1,...,Xn) ̸= K}] ≤ C·e−c·n/logn, where c,C are
distribution-dependent constants. This can be viewed as a boosting argument for iden-
tification. To make this argument work, we also need to use our post-processing result
(Lemma5.4)tomapdifferentoutputsthatcorrespondtoK tothesameindex.
14Informally, limsup of a sequence of events captures the events that occur infinitely often. For instance,
Pr[limsup n→∞E n] represents the probability that infinitely many of the events E n occur. On the other hand,
limsup n→∞Pr[E n] roughly speaking denotes the largest value that the probabilities Pr[E 1],Pr[E 2],...,... approach
infinitelyoftenasn→∞.
43• Subsequently,usingtheBorel-Cantellilemma(seeLemmaE.1)weshowthatfor{h′ n} n∈N it
P
holdsthatforanyvaliddistribution
(cid:20) (cid:21)
(cid:110) (cid:111)
{Xi}
iP ∈Nr
∼P∞
lim n→s ∞up L h′ n(X 1,...,Xn) ̸= K = 0,
which,combinedwithLemma5.7,leadstoacontradiction.
Theformalstatementandtheproofoftheresultfollow.
Lemma 5.8. For every countable collection of languages L that does not satisfy Angluin’s condition, and
everylearningalgorithmA = {h n} n∈N thereexistsavaliddistributionPsupportedonK ∈ Lsuchthat
(cid:104) (cid:105) 1
limsup Pr L ̸= K ≥ .
n→∞ X 1,...,Xn∼Pn
hn(X 1,...,Xn)
2
Proof. Assume towards contradiction that there exists a countable collection of languages L that
doesnotsatisfyAngluin’sconditionandalearningalgorithmA = {h n} n∈Nsuchthatforalltarget
languagesK ∈ L andforallvaliddistributionsP supportedoverKthereexistssomec < 1/2such
that
(cid:104) (cid:105)
limsup Pr L ̸= K = c.
n→∞ X 1,...,Xn∼Pn
hn(X 1,...,Xn)
Letalsoc := 1/2−c > 0Bydefinitionofthelimitsuperior,itholdsthat
(cid:101)
(cid:12)(cid:26) (cid:27)(cid:12)
(cid:12) (cid:12)
(cid:12)
n ∈ N :
X
1,...P ,Xr n∼Pn[L hn(X 1,...,Xn) ̸= K] > c+ 2c (cid:101) (cid:12) (cid:12)
(cid:12)
< ∞ .
Fortherestoftheproof,letusfixsomevaliddistributionP .Letn bethelargestnumbersuchthat
0
(cid:104) (cid:105)
Pr X 1,...,Xn∼Pn L hn(X 1,...,Xn) ̸= K > c+(c(cid:101)/2). Notice that n 0 depends on P . The previous argument
showsthatn < ∞ .Foralln > n wehavethat
0 0
(cid:104) (cid:105) c 1 c
Pr L ̸= K ≤ c+ (cid:101) = − (cid:101) .
X 1,...,Xn∼Pn
hn(X 1,...,Xn)
2 2 2
ConsiderthealgorithmA′ = {h′ n} n∈N thatworksasfollows: forevery n,itsplitsthedatasetinto
(cid:98)t
n
:= n/logn consecutive and non-overlapping batches, each of size logn. Then, it runs algorithm
h oneachofthebatches. Leti denotetheoutputofthe j-thbatch,In = (i ,...,i )denotethe
logn j 1 (cid:98)tn
multisetofalltheseindicesand z = min{ℓ ∈ N : L = K}.Then,usingLemma5.4weknowthat
ℓ
thereexistssomen := n (K,L ,X)suchthatforalln ≥ n
(cid:98)0 (cid:98)0 (cid:98)0
L
ij
= K =⇒ (cid:98)in
j
= z, ∀j ∈ [(cid:98)t n],
(cid:16) (cid:17)
where (cid:98)in is defined in Lemma 5.4. Thus, letting (cid:98)In = (cid:98)in,...,(cid:98)in we have that for all n ≥
j 1 (cid:98)tn
max{n ,n },alltheindicesofIn thatcorrespondtosomeindexofKaremappedtozinthecollec-
0 (cid:98)0
tion(cid:98)In.
Finally,thealgorithmoutputsthemajorityvoteovertheindicesin(cid:98)In.UsingastandardCher-
noffbound,weseethat
 (cid:110) (cid:111) 
Pr ∑
1 (cid:98)in
j
̸= z
≥ 1 − c (cid:101)  ≤ e−c(cid:101)22n/log(n) , ∀n ≥ max{n 0,n (cid:98)0}.
n/logn 2 4
j∈[(cid:98)tn]
44Thisimpliesthat
Pr (cid:2) h′(X ,...,X ) ̸= z(cid:3) ≤ e−c(cid:101)22n/log(n) , ∀n ≥ max{n ,n }.
n 1 n 0 (cid:98)0
X 1,...,Xn
Thus,wehavethat
∑ ∑
n∈NX
1,...P ,Xr n∼Pn[L h′ n(X 1,...,Xn) ̸= K] =
n≤max{n0,n(cid:98)0}X
1,...P ,Xr n∼Pn[L h′ n(X 1,...,Xn) ̸= K]
∑
+
n>max{n0,n(cid:98)0}X
1,...P ,Xr n∼Pn[L h′ n(X 1,...,Xn) ̸= K]
∑
≤ max{n 0,n (cid:98)0}+
n>max{n0,n(cid:98)0}X
1,...P ,Xr n∼Pn[L h′ n(X 1,...,Xn) ̸= K]
≤ max{n ,n }+ ∑ Pr [h′(X ,...,X ) ̸= z]
0 (cid:98)0 n>max{n0,n(cid:98)0}X 1,...,Xn∼Pn n 1 n
≤ max{n ,n }+ ∑ e−c(cid:101)22n/log(n)
0 (cid:98)0
n>max{n0,n(cid:98)0}
< ∞ .
UsingtheBorel-Cantellilemma(seeLemmaE.1),wegetthat
(cid:20) (cid:21)
(cid:110) (cid:111)
{Xi}
iP ∈Nr
∼P∞
lim n→s ∞up L h′ n(X 1,...,Xn) ̸= K = 0.
P
Sincethisholdsforallvaliddistributions ,itcontradictsLemma5.7,whichstatesthat,forsome
validP′ (whichdependson{h′ n} n∈N),itholdsthat
(cid:20) (cid:21)
(cid:110) (cid:111) 1
{Xi} i∈P Nr ∼P′∞ lim n→s ∞up L h′ n(X 1,...,Xn) ̸= K ≥ 3 .
Thisconcludestheproof.
WenowhaveallthecomponentstoproveTheorem3.1byfollowingtheoutlineinFigure2.
ProofofTheorem3.1. Let L be any non-trivial collection of languages. Then, Lemma 5.1 implies
thatnolearnercanlearnL ataratefasterthane−n.
L L
Letusfirstconsiderthecasethat satisfiesAngluin’scondition. Thisimpliesthat isidenti-
fiableinthelimit(seeTheorem2.2). Let g: N → R ,besomesublinearfunction,i.e., g(n) = o(n).
Then,Lemma5.5showsthatthereexistsalearnerthatachievesratese−g(n) forL .
L
Lastly, we consider the case where does not satisfy Angluin’s condition. Then, Lemma 5.8
showsthatforeverylearner{h n} n∈N,thereexistsavaliddistributionP forwhich
(cid:104) (cid:105) 1
limsup Pr L ̸= K ≥ .
n→∞ X 1,...,Xn∼Pn
hn(X 1,...,Xn)
2
L
Hence, isnotlearnableatanyrate. Thisconcludestheproof.
455.2 ProofofTheorem3.2(RatesforGeneration)
Inthissection,weproveTheorem3.2followingtheoutlineinFigure3.
First,inSection5.2.1weformallydefinethefamilyofcollectionsthatarenon-trivialforgenera-
tion(Definition17)andshowthat(1)foranytrivialcollection,itispossibletogenerateafterseeing
aconstantnumberofexamples(Lemma5.9)and(2)anexponentialrateisthebest-possibleforany
non-trivialcollection(Lemma5.10). Next,inSection5.2.2,wepresentasufficientconditionunder
which a generation algorithm that works “in-the-limit” achieves exponential rate (without any
modifications). Finally,inSections5.2.3and5.2.4,wepresentalgorithmsthatachieveexponential
L
ratesgivenaccesstoasubsetoracleandmembershiporacleforthecollection respectively.
Theorem3.2
Fornon-trivialcollectionse−n isthebestpossiblerate Consistentgenerationatexponentialrate
Lemma5.10 Lemma5.9 Lemma5.11
Lemma5.12 Lemma5.13
Figure3: OutlineofProofofTheorem3.2
5.2.1 OptimalRateforNon-TrivialCollectionsforGeneration
For the case of generation, we need a different notion of non-trivial languages from the one we
did for identification (see Definition 13). Indeed, assume that L = {L ,L }, and L ̸= L ,L ∩
1 2 1 2 1
L = ∞ .ThiscollectionsatisfiesDefinition13,thusnoalgorithmcanidentifyataratefasterthan
2
exponential. However,itisnothardtoseethereisaconsistentgenerationalgorithmthatdoesnot
need any samples: just generate a string from L ∩L . Instead, the following condition turns out
1 2
tocharacterizecollectionsthatarenon-trivialforgeneration.
Definition17(Non-trivialforGeneration). AlanguagecollectionLisnon-trivialforgenerationifthere
existssome x ∈ XandafinitesetoflanguagesL′ ⊆ Lsuchthat:
• each L ∈ L′ contains x;and
• theintersectionofalllanguagesinL′ isfinite,i.e.,|∩ L∈L′L| < ∞ .
To verify that the definition of non-trivial collections is meaningful, we show in the following
resultthatforalltrivialcollections, thereexistsanalgorithmthatgeneratescorrectlywithproba-
bility1,forallvaliddistributions,whennissufficientlylargeevenifthetrainingdatasetcontains
46(a) TrivialforGeneration (b) TrivialforGeneration (c) Non-TrivialForGeneration
Figure 4: Illustrations of language collections that are (a,b) trivial for generation and (c) non-
trivial for generation. In cases (a) and (c), the collection L has three languages – L ,L , and L –
1 2 3
L
denoted by different colors. Case (b) illustrates the example in Example 2; here, the collection
hasinfinitelymanylanguageswhichfollowanestedstructure L ⊋ L ⊋ ··· ⊋ {0}.
1 2
onlyonedistinctelement. Interestingly,ourresultusesanalgorithmthatgeneratesinthelimitin
asettingthatisslightlydifferentfromtheoneconsideredbyKleinbergandMullainathan[KM24]:
we fix some target language K ∈ L , we give a single input x ∈ K to the algorithm and then we
runitforinfinitelymanysteps,withoutgivinganyfurtherinputs. Weshowthatthereexistssome
n K,L ∈ N which depends only on K and the enumeration of L , but, crucially, not on x, such that
thealgorithmgeneratescorrectlyforeveryn ≥ n K,L.Thealgorithmisdescribedbelow.
GeneratinginthelimitfromatrivialcollectionL = {L ,L ,...}
1 2
Input: Asetofnelements{X ,...,X }fromX ,potentiallycontainingrepetitions
1 n
Description:
1. Selectanyarbitraryelement xfrom{X ,...,X }
1 n
2. Initializetheindex j = 1
3. while x ∈/ L do: increment jby1
j
# When X ,...,X aredrawnfromavaliddistributionthisstepterminateswithprobability1
1 n
4. ComputeVn(x) := {L ∈ L : x ∈ L , 1 ≤ i ≤ n}∪(cid:8) L (cid:9)
i i j
5. Letk := 1
6. while x ∈/ (cid:84) L\{X ,...,X } do: incrementkby1
k L∈Vn(x) 1 n
# WhenListrivialforgeneration(seeDefinition17)thisstepterminateswithprobability1
7. return x
k
Noticethatthepreviousalgorithmisindeedcomputablegivenaccesstoamembershiporaclefor
L
eachlanguagein .
Lemma5.9(AlgorithmForGenerationfromTrivialCollections). ForeverycollectionoflanguagesL
that is trivial for generation, there exists a generation algorithm (G n) n∈N such that for every valid distri-
butionPwithrespecttoLitterminateswithprobability1foralln ∈ N,andthereexistssomeconstantC
thatdependsonP ,L ,suchthatforalln ≥ C,itholdsthat
E [1{G (X ,...,X ) ̸∈ supp(P)\{X ,...,X }}] = 0.
n 1 n 1 n
X 1,...,Xn∼Pn
47Proof. LetL beatrivialcollectionforgeneration(seeDefinition17). FixsomevaliddistributionP
supportedovertargetlanguage K.Let z ∈ N bethesmallestnumbersuchthat L = K.Then,the
z
triviality condition states that for every x ∈ X and every finite set of languages L′ ⊆ L it holds
that either x ∈/ L, for some L ∈ L′, or |∩ L∈L′L| = ∞ . We have that, with probability 1, every i.i.d.
drawofnsamplesfromP satisfies X ,...,X ∈ L .Chooseanarbitrary x ∈ {X ,...,X }.Notice
1 n z 1 n
again that, with probability 1, x ∈ K. Consider the execution of the algorithm described above
by fixing this x. Let us now verify that this algorithm generates some x′ ∈ K\{X ,...,X } for
1 n
all n ≥ z, and terminates with probability 1 for all n ∈ N . First, notice that by definition of z,
when n ≥ z it holds that L ∈ Vn(x). Notice that, since for all L ∈ Vn(x) it holds that x ∈ L and
z
|Vn(x)| ≤ n < ∞ ,thetrivialitydefinitionimpliesthatforalln ≥ z,
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:92) L(cid:12) (cid:12) = ∞ .
(cid:12) (cid:12)
(cid:12)L∈Vn(x) (cid:12)
Hence,foralln ≥ zwehavethat
• K ∈ Vn(x),thus(cid:84) L ⊆ K.
L∈Vn(x)
(cid:12) (cid:12)
• (cid:12)(cid:84) L(cid:12) = ∞ .
(cid:12) L∈Vn(x) (cid:12)
Thus,itholdsthat
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:92) L\{X 1,...,X n}(cid:12) (cid:12) = ∞ .
(cid:12) (cid:12)
(cid:12)L∈Vn(x) (cid:12)
Hence, the algorithm can generate unseen strings from the target language K for all n ≥ z, with
probability1.
NoticethatzindeedonlydependsonK,L
.
Wenowprovetheterminationpropertyofouralgorithm. Tothatend,itsufficestoshowthat
both while loops terminate with probability 1. As we argued before, x ∈ L with probability 1,
z
hence, the first while loop terminates after at most z steps. We now consider the termination of
(cid:12) (cid:12)
the second while loop. As we argued above, (cid:12)(cid:84) L\{X ,...,X }(cid:12) = ∞ with probability 1,
(cid:12) L∈Vn(x) 1 n (cid:12)
hencetheloopwillterminateafterafinitenumberofsteps. Thisconcludestheproof.
WeremarkthattherequirementthatthesetL′
inDefinition17isfiniteiscrucial. Thenextexam-
ple gives a collection of languages that is trivial for generation, yet it satisfies a modification of
Definition17thatallowsL′
tobeinfinite.
Example2(ATrivialCollectionforGenerationThat“Almost”SatisfiesDefinition17). Definethe
X L
domain andthelanguagecollection asfollows
(cid:26)(cid:20) (cid:21) (cid:27)
1
X = [0,1]∩Q and L = 0, ∩Q : n ∈ N ,
n
whereQ isthesetofrationalnumbers. NoticethatbothX andL arecountable,andeach L ∈ L is
alsocountable. Considertheelementx = 0andthesetL′ = L .First,noticethatx ∈ L,∀L ∈ L′(by
definitionofeveryL ∈ L ). Moreover,itisnothardtoseethat∩ L∈L′L = {0},hence|∩ L∈L′L| = 1 <
∞ .Itisalsonothardtoseethateveryfinitesub-collectionL′′ ⊆ L ,satisfies|∩ L∈L′′L| = ∞ .Hence,
theconditionsofDefinition17canonlybesatisfiedbyinfinitesub-collections. Wecanshowthat
48L
there is an algorithm that generates from , without seeing any example. Indeed, consider the
algorithm that in every round n ∈ N outputs the element 1/n. Let K be any target language. By
definition, there is some n
K
∈ N such that 1/n ∈ K, for all n ≥ n K. Hence, this algorithm can
generatefromK.
We now move on the proof the main result in this section. Similar to the identification setting
before,weshowthemainresultintwoparts. First,weshowthatforanynon-trivialcollectionof
languages,noalgorithmcangenerateataratefasterthanexponential. Theapproachsharessome
high-levelideaswiththeidentificationsetting,butthemorecomplicatedconditionthatcharacter-
izesnon-trivialgenerationmakesthetechnicaldetailsmorenuanced. Inparticular,leveragingthe
non-trivialitycondition,wecandeducethatthereexistsafinitesetofelements {x ,...,x } and
ℓ ℓ
1 B
afinitecollectionoflanguagesL′ sothat∩ L∈L′L = {x
ℓ
1,...,x
ℓ
B}.Then,wheneverthetrainingset
consistsexactlyoftheelements{x ,...,x }(containingalsoduplicatesofthem),nomatterwhat
ℓ ℓ
1 B
element x ∈ X\{x ,...,x } the algorithm generates, there exists some L ∈ L′ so that x ∈/ L′.
ℓ ℓ
1 B
Thisallowsustofindsome“hard”distribution,whichdependsonthegeneratingalgorithm,and
for which this event happens with exponentially small probability. The formal statement of the
resultandthetechnicaldetailsoftheprooffollow.
Lemma5.10(ExponentialRateIsOptimalforGeneratingFromAnyNon-trivialCollection). LetL
be a non-trivial collection of languages for generation. Then, for any generating algorithm (G n) n∈N there
existsavaliddistributionPsuchthatE[er(G )] ≥ C·e−c·n,forinfinitelymanyn ∈ N .
n
Proof. Since L is non-trivial for generation, there exists some x ∈ X and a finite L′ ⊆ L such that
x ∈ ∩ L∈L′L and |∩ L∈L′L| = B < ∞ . Let {x
ℓ
1,...,x
ℓ
B} := ∩ L∈L′L be the distinct elements that
appear in the intersection of the sub-collection L′. Define a collection of distributions {P }
L L∈L′
thathastwoproperties:
• Forevery L ∈ L′ itholdsthatP isvalidfor L.
L
• Allthedistributions{P } putexactlythesamemassoneveryelementoftheset{x ,...,x }.
L L∈L′ ℓ
1
ℓ
B
Notice that, by definition of {x ,...,x }, there are collections of distributions that satisfy these
ℓ ℓ
1 B
twoconstraints.
Forany n ≥ B,letE betheeventthatthetrainingsetis (x ,...,x ,x ,...,x ).Noticethat
n ℓ ℓ ℓ ℓ
1 B 1 1
underanyP ∈ {P } ,
L L∈L′
Pr [E ] ≥ C·e−c·n,
n
X 1,...,Xn∼Pn
for the same constants C,c for all P ∈ {P } . Recall that for any valid distribution supported
L L∈L′
onatargetlanguageK
er(G (X ,...,X )) = 1{G (X ,...,X ) ∈/ K\{X ,...,X }} .
n 1 n n 1 n 1 n
Since|L| = ∞ ,∀L ∈ L and|∩ L∈L′L| < ∞ ,itfollowsthat|L′| ≥ 2.Letk := |L′|. BydefinitionofL ,
k < ∞ , and by the previous argument k ≥ 2. Moreover, notice that, for all x ∈ X\{x ,...,x }
ℓ ℓ
1 B
thereexists L ∈ L′ suchthat x ∈/ L.Indeed,if x ∈ L,∀L ∈ L′ then x ∈ ∩ L∈L′L\{x
ℓ
1,...,x
ℓ
B},but
∩ L∈L′L\{x
ℓ
1,...,x
ℓ
B} = ∅ .Foreverydistribution poverX itholdsthat
Pr (cid:2) X ∈ {x ,...,x } or∃L ∈ L′ suchthat X ∈/ L(cid:3) = 1.
ℓ ℓ
X∼p 1 B
49Foreveryn ∈ N ,n ≥ B,conditionedontheeventE ,sincethealgorithmisarandomizedmapping
n
fromthetrainingsettoX ,wehavethatG n(x
ℓ
,...,x
ℓ
,x
ℓ
,...,x
ℓ
) = p n.Weconsidertwocases:
1 B 1 1
• Forinfinitelymanyn ∈ N ,n ≥ Bitholdsthat
1
Pr [X ∈ {x ,...,x }] ≥ .
ℓ ℓ
X∼pn 1 B 2
Let N(cid:98) betheinfinitesetforwhichthepreviousholds. Then,forallP ∈ {P L}
L∈L′
andforall
n ∈ N(cid:98) itholdsthat
E [er(G (X ,...,X ))] = Pr [G (X ,...,X ) ∈/ K\{X ,...,X }]
n 1 n n 1 n 1 n
X 1,...,Xn∼Pn X 1,...,Xn∼Pn
≥ Pr [G (X ,...,X ) ∈/ K\{X ,...,X } | E ]· Pr [E ]
n 1 n 1 n n n
X 1,...,Xn∼Pn X 1,...,Xn∼Pn
≥ C·e−c·n·Pr[G n(x
ℓ
,...,x
ℓ
,x
ℓ
,...,x
ℓ
) ∈/ K\{x
ℓ
,...,x
ℓ
}]
1 B 1 1 1 B
E
(bythedefinitionof )
n
≥ C·e−c·n· Pr [X ∈ {x ,...,x }]
ℓ ℓ
X∼pn 1 B
(bythedefinitionof p )
n
1
≥ C·e−c·n· . (bytheassumptionon N(cid:98))
2
Thus, taking as the target distribution any P ∈ {P L} L∈L′ we see that the algorithm indeed
hasanexponentialrateslowerbound.
• Forinfinitelymanyn ∈ N ,n ≥ B,itholdsthat
Pr (cid:2) ∃L ∈ L′ suchthat X ∈/ L(cid:3) ≥ 1 .
X∼pn 2
Then, due to the pigeonhole principle, there is some L ∈ L′ such that for infinitely many
n ∈ N itholdsthat
1
Pr [X ∈/ L] ≥ .
X∼pn 2k
Let N(cid:98) be the infinite set for which the previous holds. Then, for the data-generating distri-
P
bution wehavethat
L
E [er(G (X ,...,X ))] = Pr [G (X ,...,X ) ∈/ L\{X ,...,X }]
n 1 n n 1 n 1 n
X 1,...,Xn∼Pn
L
X 1,...,Xn∼Pn
L
≥ Pr [G (X ,...,X ) ∈/ L\{X ,...,X } | E ]· Pr [E ]
n 1 n 1 n n n
X 1,...,Xn∼Pn
L
X 1,...,Xn∼Pn
≥ C·e−c·n·Pr[G n(x
ℓ
,...,x
ℓ
,x
ℓ
,...,x
ℓ
) ∈/ K\{x
ℓ
,...,x
ℓ
}]
1 B 1 1 1 B
E
(bythedefinitionof )
n
≥ C·e−c·n· Pr [X ∈/ L] (bythedefinitionof p )
n
X∼pn
1
≥ C·e−c·n· . (bytheassumptionon N(cid:98))
2k
P
Then,wecanpickthetargetdistributiontobe ,andtheexponentiallowerboundfollows.
L
Theproofisconcludedbynoticingthat,fromthepigeonholeprinciple,atleastoneoftheprevious
twocasesholdsforanysequenceof{p n} n∈N.
505.2.2 ASufficientConditionToAchieveExponentialRate
Let us now shift our attention to the upper bound. Following the approach of Kleinberg and
Mullainathan [KM24], we consider two settings: first, we assume access to a subset oracle which
can answer questions L ⊆ L , for all i,j ∈ N . Then, we consider the setting where we only have
i j
L
accesstoamembershiporacleforeachlanguagein .
BeforedescribingourapproachletusexplainwhyadirectadaptationoftheapproachofBous-
quetetal.[BHM+21]doesnotseemtoworkinthissetting. RecallthatBousquetetal.[BHM+21]
transforminablack-boxmanneralearnerwhichiseventually“correct”intheadversarialsetting,
to a learner that achieves exponential rates in the statistical setting, by running multiple copies
of it on independent samples of the dataset, and then aggregating their results through a major-
ity vote. A crucial property of the learner of Bousquet et al. [BHM+21] is that the majority vote
is taken over objects that have binary values, namely the predicted label of the test point. One
immediate obstacle to applying this approach here, is that the eventually correct generators will
beoutputtingdifferentvalidstringsineveryiteration. Further, thesevalidstringsmightbeeven
comingfromadifferentsubsetsofthetruelanguage. Thus,itisnotclearatallwhichaggregation
strategycouldleadtothedesiredresult. Onepotentialapproachtocircumventthisobstacleisto
L
haveallthegeneratedstringsgive“votes”tothedifferentlanguagesof (potentiallyuptoacap
n) that they belong to. It is clear that after some finite n , with probability at least 1−C·e−c·n,
0
the target language K would be collecting votes from the majority of the strings. Unfortunately,
it is not hard to see that for infinitely many n ∈ N there must be another L′ ∈ L ,L′ ̸= K, that is
0
accumulating more votes than K; if this was not the case we would have been able to identify K,
L
for all countable , which contradicts our established lower bounds. Thus, it is not clear how to
makethisaggregationstrategyworkeither.
Nevertheless, we show that, perhaps surprisingly, a much simpler strategy works: we only
need to run one copy of the algorithm proposed by Kleinberg and Mullainathan [KM24] on the
entire dataset to get exponential rates. In fact, we identify a sufficient condition that allows us to
useanyalgorithmthatworks“in-the-limit”inthestatisticalsettingwithoutmakinganymodifica-
tionstoit. Webelievethatthisideamightfindotherapplicationsintheuniversalratesliterature.
The following elementary result will be crucial for the analysis of both settings, i.e., the one
withthesubsetoracleandtheonewithjustthemembershiporacle.
Lemma 5.11. Let L be a countable collection of languages. Let A = {h n} n∈N be an algorithm that
generatesfromLinthelimitwithpositiveexampleswiththefollowingadditionalproperty:
• foreverytargetlanguageK ∈ Lthereexistsafinitesetofexamples{x ,...,x } ⊆ K thatdepends
i
1
iℓ
onlyonK andtheenumerationofL ,X ,and
• afinitenumbern ∈ NthatdependsonK andtheenumerationofL ,X ,
0
suchthatAalwaysgeneratescorrectlyifitsinputhassizeatleast n anditcontains x ,...,x .Then, A
0 i
1
iℓ
generatesfromK withexponentialratesinthestatisticalsetting.
Proof. Let P be a valid data-generating distribution. Then, by definition, supp(P) = K, for some
K ∈ L . Let x ,...,x ⊆ K be a set of points such that after A takes as input this set it starts
i
1
iℓ
generatingcorrectly,i.e.,foranySsuchthatx ,...,x ⊆ Sand|S| ≥ n itholdsthath (S) ∈ K\
i
1
iℓ 0 |S|
51S.SinceP isavaliddata-generatingdistributionitholdsthatx ,...,x ⊆ supp(P).Let p ,...,p
i
1
iℓ ii iℓ
be the mass of points x ,...,x under P . Suppose we draw n samples i.i.d. from P . Then, the
i
1
iℓ
probabilitythatwedonotobserveall x ,...,x inthesampleisboundedas
i
1
iℓ
∑
Pr [∃j ∈ [ℓ]: x ∈/ {X ,...,X }] ≤ Pr [x ∈/ {X ,...,X }] (byaunionbound)
X 1,...,Xn∼Pn
ij 1 n
j∈[ℓ]X 1,...,Xn∼Pn
ij 1 n
∑
(cid:16) (cid:17)n
= 1−p (sincewehavei.i.d. draws)
ij
j∈[ℓ]
≤ ∑ e−pij·n (as1−z ≤ e−z forallz ∈ R )
j∈[ℓ]
≤
ℓ·e−min j∈[ℓ]pij·n
.
Thus, the algorithm generates correctly in the statistical setting after taking as input n ≥ n ∈ N
0
examples,withaprobabilityatleast1−C·e−c·n,forsomedistributiondependentconstants C,c.
Thisconcludestheproof.
In the next two sections, we will show that the algorithms proposed by Kleinberg and Mul-
lainathan[KM24]inthesettingwithaccesstoasubsetoracleormembershiporaclealreadysatisfy
thisproperty. Forcompleteness,wepresenttheiralgorithmsandtherelateddefinitions.
5.2.3 AlgorithmWithAccessToSubsetOracle
We start with the algorithm of Kleinberg and Mullainathan [KM24] which requires access to a
subsetoracleforL ,i.e., anoraclethatforanytwolanguages L ,L ∈ L answerswhether L ⊆ L .
i j i j
Tothatend,wefirstdefinethenotionofcriticallanguage[KM24].
Definition 18 (Critical Language [KM24]). Let L = {L ,L ,...,} be a countable collection of lan-
1 2
guages. Let S = {x ,...,x } ⊆ X . For any j ∈ N, we say that L is critical with respect to S if
n in in j n
S ⊆ L andforalli < jifS ⊆ L then L ⊆ L .
n j n i j i
The intuition behind this definition is that if two languages L ,L are both critical and i < j, then
i j
it is “safer” to generate from L . This is exactly the way the algorithm from Kleinberg and Mul-
j
lainathan[KM24]operates. Tobemoreprecise,ineveryiterationn ∈ N itperformsthefollowing
steps:
• Let L = {L ,L ,...,L } be the first n languages of L and S = {x ,...,x } be the set of
n 1 2 n n i 1 in
examplesobservedsofar.
• LetC ⊆ L bethesetofthecriticallanguageswithrespectto S withinL (Definition18).
n n n n
If C = ∅ output an arbitrary x ∈ X and proceed to getting the (n+1)-th input. This step
n
makesuseofthesubsetoracle.15
• Let L ∈ C bethecriticallanguagewiththehighestindex.
k n
15Observethatitmakessensetooutputsomethingarbitrarysincethefirstconsistent(inthesensethatitcontainsthe
observedtrainingexamples)languageinLiscriticalbydefinitionandhenceifC
n
=∅,wehavenotyetencountereda
consistentlanguage.
52• Outputthefirstunseenexamplefrom L ,i.e., x ∈ X suchthat j = min{i ∈ N : x ∈ L ,x ∈/
k j i k i
S }.
n
ItisimplicitintheanalysisofKleinbergandMullainathan[KM24]thatforeverytargetlanguage
K ∈ L , there exists a set x ,...,x ⊆ K and n ∈ N that depend only on K and the enumera-
i
1
iℓ 0
tion of X ,L , such that after n steps if the above algorithm takes as input any set S that contains
0
{x ,...,x }, then it always generates a new example correctly. We make this explicit in the fol-
i
1
iℓ
lowinglemmaandprovideaproofforcompleteness.
Lemma5.12(Adaptationof(4.3)fromKleinbergandMullainathan[KM24]). LetL = {L ,L ,...}
1 2
beacountablecollectionoflanguages,let K ∈ Landlet z ∈ Nbethesmallestnumbersuchthat L = K.
z
Then,thereexistx ,...,x ∈ KthatdependonlyonKandtheenumerationofL ,suchthatifthealgorithm
i
1
iℓ
of Kleinberg and Mullainathan [KM24] takes as input any set S for which x ,...,x ∈ S and |S| ≥ z,
i
1
iℓ
wherezdependsonlyonK andtheenumerationofL ,thenitgeneratescorrectlyfromK.
Proof. Let L ,...,L ⊆ L with i ,...,i < z be the set of all languages that precede L in L for
i 1 iℓ 1 ℓ z
which L ̸⊆ L ,j ∈ [ℓ]. Then, for each such L there exists some x ∈ L so that x ∈/ L . Let x be
z ij ij z ij ij
thesmallestindexedelementinX forwhichthepreviousholds. Noticethatwhenever x ,...,x
i
1
iℓ
is part of the input sample S, then L is critical; this follows immediately from the definition of
z
criticality and the fact that the set S contradicts all the languages L ,i < z, such that L ̸⊆ L .
i z i
Moreover, when |S| ≥ z,thealgorithmoutputsanunseenwordfromacriticallanguage L z′ with
z′ ≥ z. By definition of the critical language, this means that L z′ ⊆ L z. Hence, the algorithm
generatescorrectly.
An immediate consequence of Lemma 5.11 is that the algorithm of Kleinberg and Mullainathan
[KM24]withaccesstoasubsetqueryoraclegenerateswithexponentialuniversalrates.
5.2.4 AlgorithmWithAccessToMembershipOracle
We now move on to the more involved version of the algorithm of Kleinberg and Mullainathan
[KM24] that only requires membership access to every L ∈ L . Recall this means that for every
x ∈ X ,L ∈ L thealgorithmcanaskwhether x ∈ L.
Before we describe the algorithm, we provide the definition of a modified notion of a critical
language [KM24], which is based on a notion of a projection of a language, which we defined in
Definition 16.16 Recall that, given some language L, we denote L[m] = L∩{x ,...,x } (Defini-
1 m
tion16).
Definition 19 (m-Critical Language [KM24]). Let L = {L ,L ,...,} be a countable collection of lan-
1 2
guages. Let S = {x ,...,x } ⊆ X . For any j ∈ N, we say that L is m-critical with respect to S if
n in in j n
S ⊆ L and,foralli < j,ifS ⊆ L ,then L [m] ⊆ L [m].
n j n i j i
Wefirstgiveanintuitivedescriptionofthekeymodificationsofthealgorithmfromtheprevious
section that are required to make it work only with access to a membership oracle. First, notice
that even though the algorithm cannot ask queries of the form L ⊆ L , it can ask queries of the
i j
16KleinbergandMullainathan[KM24]donotexplicitlydefinethisterm;weuseittosimplifyourdiscussion.
53form L [m] ⊆ L [m],foranyfinitem ∈ N ,byjustasking2mmembershipqueries. Thus,thehigh-
i j
level idea is to replace subset queries with queries of the form L [m] ⊆ L [m], for a sufficiently
i j
largem ∈ N .Theexactdetailsareprovidedbelow.
• Let S = {x ,...,x } be the set of elements that have been presented to the learner up to
n i
1
in
stepn.Atthebeginningofstepn,setm
n
= max{m n−1,i n}.17
• LetV ⊆ {L ,L ,...,L }bethesetoflanguageswhoseindexisatmostnandareconsistent
n 1 2 n
withtheinputS ,i.e.,S ⊆ L,∀L ∈ V .Ifnosuchlanguagesexist,outputanarbitraryx ∈ X
n n n
and proceed to reading the (n+1)-th input example. Notice that this can be done with n2
membershipqueries.
• Let m = m +1 and Cm be the set of the m-critical languages within V . Notice that since
n n n
V ̸= ∅ , for all m ∈ N there exists at least one m-critical language (the lowest indexed
n
languagewithinC ism-criticalforallm ∈ N ).
n
• Letcm n ∈ N bethelargestindexofalanguageinCm n.Ifforsomei ≤ m,itholdsthat x i ∈ L cm n
andx ∈/ S ,outputx 18andletm = m.Otherwise,letm = m +1andrepeattheprevious
i n i n n n
bulletpoint.
Kleinberg and Mullainathan [KM24] showed that the previous algorithm terminates in finitely
many steps for every n ∈ N (Result (5.5) from Kleinberg and Mullainathan [KM24]). Moreover,
theyprovedthatforanyenumerationofanytargetlanguage K ∈ L ,thereexistssome n ∈ N so
0
that the algorithm generates correctly for all steps n ≥ n (Result (5.7) from Kleinberg and Mul-
0
lainathan[KM24]). Infact,itisimplicitintheiranalysisthatforallK ∈ L thereexistx ,...,x ∈ K
i
1
iℓ
thatdependonlyonKandtheenumerationofL ,X ,aswellasafiniten ∈ L thatdependsonlyon
0
K and the enumeration of L ,X , such that if an input sample S satisfies that i) x ,...,x ∈ S and
i
1
iℓ
ii)|S| ≥ n ,thenthealgorithmgeneratescorrectly. Wemakethisfactexplicitinthenextresult.
0
Lemma5.13(Adaptationof(5.7)fromKleinbergandMullainathan[KM24]). LetL = {L ,L ,...}
1 2
be a countable collection of languages, let K ∈ L be the target language, and let z ∈ N be the smallest
numbersuchthat L = K.Then,thereexist x ,...,x ∈ KthatdependonlyonKandtheenumerationof
z i
1
iℓ
L ,X ,suchthatifthealgorithmofKleinbergandMullainathan[KM24]takesasinputanyset S forwhich
x ,...,x ∈ S and |S| ≥ z, where z depends only on K and the enumeration of L , then it generates
i
1
iℓ
correctlyfromK.
Proof. Let z ∈ N be the smallest number for which L = K. By definition, z has to be finite. Let
z
L ,...,L bethesetofalllanguagesthatprecedeL inL forwhichL ̸⊆ L ,j ∈ [ℓ].Then,forany
k
1
iℓ z z kj
suchlanguage L thereexistssome x ∈ K suchthat x ∈/ L .Define x tobethesmallestindexed
kj kj ij
elementforwhichthepreviousholds. Hence,whentheinputsampleScontainsx ,...,x noneof
i
1
iℓ
thelanguages L ,j ∈ [ℓ],areconsistentwithS.Consideranyiterationn ∈ N ,where x ,...,x ⊆
ij i
1
iℓ
S ,andn ≥ z.Itfollowsimmediatelythat L ism-criticalforallm ∈ N ,andhenceitiscontained
n z
inthesetCm.Thus,forallm ∈ N ,forthelargestindexcm ofalanguageinCm itholdsthatcm ≥ z.
n n n n
Recallthatsincethealgorithmterminates(Result(5.5)fromKleinbergandMullainathan[KM24]),
17Setm =0.
0
18Iftherearemultiplesuchelements,outputtheonewiththesmallestindex.
54itwilloutputsomex
m
∈ X suchthatx
m
∈/ S n,x
m
∈ L z′,z′ ≥ z,and L z′[m] ⊆ L z[m].Thisisbecause
for all m ∈ N , the largest index of a language in Cm cannot drop below z. Thus, it follows that
n
x ∈ K\S .Hence,thealgorithmgeneratescorrectly.
m n
WearenowreadytoproveTheorem3.2.
ProofofTheorem3.2. Let L be some non-trivial collection for generation. An immediate corollary
ofLemma5.11andLemma5.13isthatthealgorithmofKleinbergandMullainathan[KM24]with
accesstoamembershipqueryoraclegenerateswithexponentialuniversalrates.
TheexponentialrateslowerboundforgenerationfollowsimmediatelyfromLemma5.10.
6 Proofs from Section 3.2 (Generation With Breadth)
6.1 ProofofTheorem3.4(MOP(·) IsDecidableForIterativeGenerators)
Inthissection,weproveTheorem3.4whichwerestatebelow.
Theorem3.4. Foranytoken-by-tokengeneratorG,MOP(G)isdecidable.
Recall that a token-by-token generator G is parameterized by a randomized Turing machine M,
where M has the property that it halts on all inputs. G generates as follows: in each iteration t,
it queries M to get the next token s and iterates until M outputs EOS (i.e., end of string). The
t
algorithm to decide MOP(G) is also simple: given a string s of length n, check token-by-token
whetherG canoutput s
i
conditionedonaprefix s 1,s 2,...,s
i−1
generatedsofar. Ifatanypoint, s
i
isnotinthesupportofG (orrather M)then, outputNo. Otherwise, outputYes. Ateachstep, we
cancheckifs canbegeneratedbyMusingthefolklorefactthatmembershiporaclesaredecidable
i
for Turing machines that always hold (Lemma 6.1). Note that we cannot use this folklore result
directly for the generator G, since even though M halts in each iteration, G may not halt as the
numberofiterationsisnotbounded.
Lemma6.1. Considera(randomized)TuringMachine M thathaltsonallinputs. Thefollowingproblem
isdecidable: givenstringssand pandadescriptionof M,outputYesif Mcanoutputsgiveninput pand
outputNootherwise.
TheproofofLemma6.1usesthefollowingstraightforwardbutsubtlefolklorelemmas.
Lemma 6.2. Consider a (randomized) Turing Machine M that halts on all inputs. M has the following
property: for each input string p, M performs at most a finite number of steps between any consecutive
readsoftheir(internal)tapecontainingrandombits.
Lemma 6.3. Consider a (randomized) Turing Machine M that halts on all inputs. For each input string
p, there is a finite number n ≥ 1 such that M reads at most n random bits always (regardless of the
p p
realizationoftherandombits).
TheseenableustoproveLemma6.1.
55ProofofLemma6.1. Consider a string s ∈ Σ∗ of length m. We will check if M generates s with
positive probability by iteratively checking if, for each 1 ≤ t ≤ m, M generates token s with
t
positiveprobabilityconditionedonhavinggenerateds 1...s t−1 sofar.
Fixany1 ≤ t ≤ m. Suppose M haspassedallearlierchecksand,hence,itgenerates s 1...s t−1
with positive probability. Now, to complete the check for step t, it suffices to check that M gen-
erates s t with positive probability having generated s 1...s t−1 so far. Since M halts on all inputs,
Lemma6.3impliesthatthereisafiniten suchthat Mreadsatmostn bitsofitsinternalrandom
t t
tapewhengiventhecorrespondinginput. Moreover,Lemma6.2impliesthat Mperformsfinitely
manyoperationsbetweeneachofthen consecutivereadsoftheinternalrandomtape. Hence,one
t
can simulate the execution of M in finite time by checking all 2nt possible values of the random
bits of M. If, for any of these 2nt values, M outputs s
t
then we know that M passes the test and,
otherwise,weknowthat Mnevergeneratess whenprovidedthecorrespondinginput.
t
Onesubtletyisthatwedonotknown . Thisiseasytoovercome: sincen isknowntobefinite,
t t
we can iterate over n ∈ N until we reach a value k where for each of the 2k values of the first k
t
randombits, Mhaltsbeforereadingthe(k+1)-thrandombit.
ProofofLemma6.2
ProofofLemma6.2. The statement is vacuously true for M and p if M reads its internal tape at
most once on input p always. Suppose with positive probability (over the randomness on M’s
internal random tape), M reads its internal random tape at least twice given input p. Fix a value
r = v of the first random bit such that M will (eventually) read the second bit r on the random
1 2
tape. Consider the step after M has read r . If M performs a non-finite amount of computation
1
beforereadingr , thenwehaveacontradictiontothefactthat M istotalsincewehavefoundan
2
assignmentvofthefirstrandombitonwhich Mperformsaninfinitenumberofsteps. Hence,the
resultfollowsbycontradiction.
ProofofLemma6.3
ProofofLemma6.3. Fixanyinput ptoM. Towardacontradictionsupposethatforanyfiniten ≥ 1,
there is (at least) one assignment v ,v ...,v of the first n bits on M’s internal random tape on
1 2 n
which Mwillreadthe(n+1)-thrandombitbeforehalting. Therefore,foranyn ≥ 1,wehavean
assignment of the random bits for which M rates at least n+1 random bits and, hence, perform
atleast n+1stepsbeforehalting. Thisisacontradictiontothefactthat M haltsalways, foreach
valueoftherandombitsonitsinternaltape.
6.2 ProofofTheorem3.3(ImpossibilityforGenerationWithBreadth)
Inthissection,wepresenttheproofofTheorem3.3intwomainparts;seeFigure5foranoutline.
First, we prove that if L is not identifiable in the limit, then no algorithm in G generates with
breadth from L at any rate. Recall that G is the class of generating algorithms for which MOP(·)
isdecidable.
56Theorem3.3
Results for non-identifiable collections L Results for identifiable collections L
Algorithms in G cannot Algorithms in G generate Algorithms in G Generation faster than
generate with breadth (without breadth) at (the generate with breadth at exponential is impossible
from L at any rate optimal) exponential rate almost exponential rate for non-trivial L
Lemma6.4 Theorem3.2 MOP(·) is Theorem3.1 Proposition6.5 Lemma5.10
decidable for
Kleinberg and
Mullainathan
[KM24]’s
algorithm
Figure5: OutlineofProofofTheorem3.3
Lemma6.4. LetLbeacountablecollectionoflanguagesthatisnotidentifiableinthelimit. Then,forevery
rateR,thereisnogeneratingalgorithminGthatcangeneratefromLwithconsistencyandbreadthatrate
R.
Proof. Let L be a countable collection of languages that is not identifiable in the limit. Assume
towards a contradiction that there exists some generating algorithm (G ) ∈ G that achieves con-
n
sistencyandbreadthatsomerateR(n).FixalsosomevaliddistributionP
supportedoveratarget
languageK. Thismeansthatthereexistc,C,thatdependonP ,suchthat
E [1{supp(G ) ̸= K\{X ,...,X }}] ≤ C·R(c·n).
n 1 n
X 1,...,Xn∼Pn
Thiscanbeequivalentlywrittenas
Pr [supp(G ) ̸= K\{X ,...,X }] ≤ C·R(c·n).
n 1 n
X 1,...,Xn∼Pn
Forevery n ∈ N ,wedenotebyE theeventthatsupp(G ) = K\{X ,...,X }.Let z ∈ N bethe
n n 1 n
smallest number such that L = K. Recall that the elements of the universe are X = {x ,x ,...}.
z 1 2
Considerthefollowingalgorithm(I n) n∈N foridentification:
• For every n ∈ N , denote by {X } the sample i.i.d. from P . Output the smallest index
i i∈[n]
j ∈ [n]suchthat
1(cid:8) x ∈ L (cid:9) = 1{x ∈ supp(G )∪{X ,...,X }},∀i ∈ [n].
i j i n 1 n
57(Since G ∈ G, MOP(G ) is decidable and, hence, the above j can be computed.) If no such
n n
indexexists,outputanindexarbitrarily.
Weconsidertwocases.
CaseA(z = 1): Inthiscase,noticethatifsupp(G ) = K\{X ,...,X },then, I (X ,...,X ) = z.
n 1 n n 1 n
This is because supp(G )∪{X ,...,X } = K and L = K, so for all x ∈ X it holds 1{x ∈ L } =
n 1 n z z
1{x ∈ supp(G )∪{X ,...,X }}.Thus,
n 1 n
(cid:104) (cid:105)
Pr L ̸= K ≤ Pr [I (X ,...,X ) ̸= z]
X 1,...,Xn∼Pn
In(X 1,...,Xn)
X 1,...,Xn∼Pn
n 1 n
≤ Pr [I (X ,...,X ) ̸= z | Ec]· Pr [Ec]
X 1,...,Xn∼Pn n 1 n n X 1,...,Xn∼Pn n
E
(sinceunder thealgorithmidentifies)
n
≤ 1· Pr [Ec]
X 1,...,Xn∼Pn n
≤ C·R(c·n).
Case B (z > 1): For every language L ,j ∈ [z−1], let i ∈ N be the smallest number such that
j j
(cid:110) (cid:111) (cid:110) (cid:111)
1 x ∈ L ̸= 1 x ∈ L . By definition of L , we have that i is well-defined. Moreover, let
ij j ij z z j
n∗ := max i .Noticethatforalln ≥ n∗,undertheeventE ,wehavethat I (X ,...,X ) = z.
j∈[z−1] j n n 1 n
Toseewhythisisthecase,noticethat
1. UndertheeventE itholdsthat1{x ∈ L } = 1{x ∈ supp(G )∪{X ,...,X }}forallx ∈ X .
n z n 1 n
2. Sincen ≥ n∗,forall j ∈ [z−1]wehavethati ≤ n.Thus,undertheeventE itcannotbethe
j n
casethat:
1(cid:8) x ∈ L (cid:9) = 1{x ∈ supp(G )∪{X ,...,X }},∀i ∈ [n].
i j i n 1 n
Hence,usinganidenticalargumentasinthecasez = 1wehavethat
(cid:104) (cid:105)
Pr L ̸= K ≤ C·R(c·n),∀n ≥ n∗ .
X 1,...,Xn∼Pn
In(X 1,...,Xn)
P P
Sincethisholdsforanyvaliddistribution ,usingdifferent -dependentconstants,weseethatthe
algorithm(I n) n∈NcanidentifyL atarateR.SinceL isnotidentifiableinthelimit,thiscontradicts
Theorem3.1,and,hence,concludestheproof.
The last ingredient we need to prove Theorem 3.3 is an algorithm that given the index of a lan-
guage,samplesfromitwithbreadth.
Proposition 6.5. There exists a randomized computable algorithm A for which MOP(·) is decidable and
that, given as input a number z ∈ N and access to a collection of languages L = {L ,L ,...,}, satisfies
1 2
supp(A(z)) = L .
z
Proof. Thealgorithmworksasfollows. Giventheindexzofthetargetlanguage:
58• Sampleanaturalnaturalnumbern ∈ N fromsomedistributionsupportedoverN .
(cid:98)
• If x ∈ L , (this can be checked by querying the membership oracle) return x . Otherwise,
n(cid:98) z n(cid:98)
repeatthepreviousbullet.
It follows immediately that this algorithm is computable and satisfies the requirements of the
statement,sinceitisimplementedviarejectionsamplingusingamembershiporacleto L (which
z
isdecidable),itisindeedinG.
WearenowreadytoproveTheorem3.3.
ProofofTheorem3.3. First, consider the case that L is not identifiable in the limit. Then, for every
rate R, Lemma 6.4 shows that no generating algorithm from G can generate from L with consis-
tencyandbreadthatrate R.
L
We now show that there is a consistent generation algorithm for at an optimal exponential
L
rate. Noticethatsince isnon-trivialforgeneration,byLemma5.10,itholdsthatnoalgorithmcan
achieveratefasterthanexponential. Moreover,byTheorem3.2thereexistsanalgorithm(namely
theonebyKleinbergandMullainathan[KM24])thatachievesexponentialrates. Moreover,since
thisalgorithmsamplesfromadistributionthatisapointmass,itisindeedinG.
L
Letusnowconsiderthecasethat isidentifiableinthelimit. Then,byTheorem3.1,forevery
g(n) = o(n), there exists an algorithm that identifies L at rate e−g(n). It is not hard to turn this
identification algorithm into an algorithm that is in G and generates with breadth via rejection
sampling. This happens as described in Proposition 6.5. Conditioned on the event that L = K,
z
L
thepreviousalgorithmindeedgenerateswithbreadth. Finally,since isnon-trivial,noalgorithm
(evenoutsideofG)canachieveafasterthanexponentialrateforconsistentgeneration(evenwith-
outbreadth),byLemma5.10.
Remark 5. One subtlety in the above proof is that to use Kleinberg and Mullainathan [KM24]’s
algorithm for generation, we require it to output an arbitrarily large number of samples at each
step. While the vanilla version of Kleinberg and Mullainathan [KM24]’s algorithm only outputs
onesampleatatime,itcaneasilybeextendedsothat,givenanumberm ≥ 1,itoutputsmsamples
ateachstep. Moreover,theresultingalgorithmisinG.
6.3 ProofofTheorem3.5(ImpossibilityforGenerationWithBreadthintheLimit)
Inthissection,weproveTheorem3.5,whichwerestatebelow.
Theorem 3.5. For every non-identifiable collection of countably many languages L, no generating algo-
rithm,forwhichMOP(·)(Definitions5and6)isdecidable,cangeneratewithbreadthfromLinthelimit.
IfLisidentifiable,thenthereisageneratorG (forwhichMOP(G)isdecidable)thatgenerateswithbreadth
fromL.
ProofofTheorem3.5. The proof of Theorem 3.5 is by a contradiction: we will show that if such
L
a generator exists, it can be used to build an identification algorithm I for contradicting the
L
fact that is non-identifiable. In addition to the generator G, this identification algorithm uses
59another sub-routine: an algorithm I that, given a positive and negative enumeration of the
PN
target, identifies it in the limit. Such an identification algorithm always exists due to a result by
Gold[Gol67]. TheidentifierI,whichweconstruct,isasfollows:
Input: AccesstoageneratorG forL that(1)achievesconsistencyandbreadthinthelimitand
(2) for which MOP(G) is decidable, and access to the algorithm I that identifies L in the
PN
limitfromapositiveandnegativeenumerationofthetargetlanguage.
Description:
1. Foreach t ∈ N do:
(a) Observethet-thsamples andletS bethesetofsamplesseensofar
t t
(b) TrainthegeneratorG fromscratchoverthetsamplesinS
t
(c) Labelthefirsttstrings x ,...,x ofthedomainasMOP(G)(x ),...,MOP(G)(x )a
1 t 1 t
(d) TrainI fromscratchonsamplesx ,...,x withlabelsMOP(G)(x ),...,MOP(G)(x )
PN 1 t 1 t
(e) outputtheindexguessedbyI andgotothenextiteration
PN
aHere,MOP(G)(x)istheanswertothemembershiporacleproblemforG giveninputx.
SinceMOP(G)isdecidable,theabovealgorithmcanbeimplementedusingaTuringmachine. We
claimthattheabovealgorithmidentifiesthetargetlanguage K afterafinitenumberofiterations.
LetzbethefirstindexatwhichK appears. Toformalizethis,fixanyenumerations ,s ,... ofthe
1 2
target language K. Since G generates with breadth in the limit, there is a finite iteration t after
G
which K generates with breadth from K and, hence, supp(G) = K. Hence, after iteration t , for
G
anystringx,MOP(G)(x) = 1{x ∈ K}. Inotherwords,I isprovidedwithaccuratepositiveand
PN
negativelabelsinallsubsequentiterationst ≥ t . SinceI identifiesinthelimit,thereisafinite
G PN
t such that I identifies K once it is given labels for the first t ≥ t examples in the domain.
PN PN PN
ItfollowsthatI and,hence,ouralgorithmidentifies K aftermax(cid:8) t ,t (cid:9) < ∞ iterations. This
PN G PN
givesthedesiredcontradiction,provingTheorem3.5. Notethattheaboveidentificationalgorithm
doesnotneedtoknoweithert ort . (Ofcourse,asaconsequence,ouralgorithmdoesnotknow
G PN
whenithasidentifiedK.)
7 Proofs from Section 3.3 (Generation With Approximate Consistency
and Breadth)
7.1 ProofofTheorem3.7(ImpossibilityintheLimit)
Inthissection,weproveTheorem3.7,whichwerestatebelow.
Theorem 3.7 (Impossibility of Unambiguous Generation in the Limit). For every non-identifiable
collection of countably many languages L, no generating algorithm stable in the limit for which MOP(·)
(Definitions5and6)isdecidablecanunambiguouslygeneratefromLinthelimit.
60ProofofTheorem3.7. Bythewayofcontradiction,supposethatthereisanalgorithmG = (G )for
n
whichMOP(·)isdecidable(ateachn)andwhichisanunambiguousgeneratorforL
. Wewilluse
L
G toconstructanalgorithmthatidentifies inthelimit,hence,contradictingthenon-identifiablity
L
of .
Fixanyenumeration x ,x ,... ofthedomainX . Foreachlanguage L ∈ L andnumber t ≥ 1,
1 2
definethet-prefixof Lasthesubset L[t]ofthefirstt-elementsofthedomain{x ,...,x }inL ,i.e.,
1 t
L[t] := {x ,...,x }∩L.
1 t
L
Tocompletetheaboveoutline,considerthefollowingalgorithm,whichweclaimidentifies .
Input: AccesstoageneratorG forL that(1)thatisunambiguousinthelimitand(2)forwhich
MOP(·)isdecidableateachstep
Description:
1. Foreach t ∈ N do:
(a) Observethet-thsamples andletS bethesetofsamplesseensofar
t t
(b) TrainthegeneratorG t−1 ons t togetG t
(c) Create a set of languages consistent with observed samples C (t) ⊆ {L ,...,L }
S 1 t
thatincludeseach L ∈ {L ,...,L }thatisconsistentwithS (i.e., L ⊇ S )
1 t t t
(d) Construct a set of languages consistent with the generator C (t) ⊆ {L ,...,L }
G 1 t
that languages L from {L ,...,L } except if L[t] ̸⊆ supp(G )∪S , which can be
1 t t t
checkedinfinitetimeusingadeciderforMOP(G )
t
(e) outputtheindexofthesmallest-indexedlanguageinC (t)∩C (t)(oranarbitrary
S G
indexifC (t)∩C (t)isempty)
S G
SincethealgorithmoutputsthesmallestindexinC (t)∩C (t),itidentifiesK ifitisthesmallest-
S G
indexedlanguageinC (t)∩C (t).Thefollowingconditionsensurethis:
S G
(A) L ∈ C (t)and L ∈ C (t),wherezisthesmallestindexatwhichK appearsinL ;and
z S z G
(B) Foranyi < z,either L ̸∈ C (t)or L ̸∈ C (t)
i S i G
We claim that there are finite times t and t where, for any t ≥ t , Condition (A) holds and, for
a b a
any t ≥ t , Condition (B) holds. This claim implies that the above algorithm identifies K in the
b
limit,leadingtothedesiredcontradiction.
Condition A holds after a finite time. Since S only contains samples from K, K ∈ C (t) for all
t S
t ≥ 1. Further,sinceG = (G ) isanunambiguousgeneratorforL inthelimit,thereexistsafinite
t
t ≥ 0,suchthatforallt ≥ t ,
0 0
|supp(G )△K| < min |supp(G )△L| . (10)
n n
L∈L: L̸=K
Hence,inparticular,forallt ≥ t
0
|supp(G )\K| , |K\supp(G )| < ∞ .
n n
61Figure6: Figureillustratingthedecompositionofsupp(G )△K andsupp(G )△L.
t t
Furthermore, as G is stable, after some time t , supp(G ) stops changing. Consider any t ≥
1 n
max{t 0,t 1}. Since K\supp(G t) hasfinitelymanyelementsand K\supp(G t) = K\supp(G t′) for
any t′ ≥ max{t ,t }, there is a finite time t after which all elements of K\supp(G ) have been
0 1 2 t
observed. Therefore, for any t ≥ {t ,t ,t }, it must holds that K ⊆ (supp(G )∪S ) and, hence,
0 1 2 t t
thatK ∈ C (t). Thus,itsufficestofixt = max{t ,t ,t }.
G a 0 1 2
ConditionBholdsafterafinitetime. Sincethereareonlyfinitelymanyi < z,itsufficestoshow
that for each i < z, there is a finite time t after which either L ̸∈ C (t) or L ̸∈ C (t). Fix any
i i S i G
i < z. Considertwocases:
• CaseA(K\L ̸= ∅): Inthiscase,thereexistsanx ∈ K\L and,hence,aftersomefinitetime
i i
t when xhasbeenobserved L ̸⊇ S and,hence, L ̸∈ C (t).
i i t i S
• CaseB(K\L = ∅): Inthiscase,L ⊋ K,andourproofisbasedonthefollowingobservation.
i i
Lemma7.1. Foranyt ≥ t and L ⊋ K,itholdsthat L\supp(G ) ̸= ∅.
a t
Proof. Sincet ≥ t ,Equation(10)holds. FromFigure6,observethat
0
|supp(G )△K| ≥ |supp(G )\L|+|K\supp(G )|
t t t
|supp(G )△L| = |supp(G )\L|+|K\supp(G )|+|L\(K∪supp(G ))| .
t t t t
ChainingtheabovewithEquation(10)andcancelingliketermsimplies
|L\(K∪supp(G ))| > 0.
t
Hence,inparticular,|L\supp(G )| > 0,whichisthedesiredresult.
t
62Hence, in this case, L \supp(G ) ̸= ∅ . Let j(i) be the smallest natural number such
i t
that x ∈ L but x ̸∈ supp(G ). (Note that the value j(i) does not depend on t, since
j(i) j(i) t
as discussed in the proof of Condition A after t = t , the supp(G ) becomes stable and not
a t
changeinsubsequentiterations.) Therefore,itfollowsthat L [j(i)] ̸⊆ supp(G )∪S fort ≥ t
i t t a
and, hence, for t ≥ max{i(j),t } by construction, L ̸∈ C (t). This completes the proof of
a i G
Case Bandbyearlierdiscussion,alsotheproofofTheorem3.7.
7.2 ProofofTheorem3.6(ImpossibilityintheStatisticalSetting)
Inthissection,weprovetheimpossibilityresultforunambiguousgenerationinthestatisticalset-
ting. Ourapproachistoestablishaconnectiontotheonlinesettingandleveragetheimpossibility
resultwehavealreadyshownthere(Theorem3.7). Namely,wewillshowthatgivensuchanun-
ambiguousgeneratorthatworksinthestatisticalsetting,wecanconstructageneratorthatworks
in the online setting, with high probability. Using the construction from Section 7.1, we can turn
thisgeneratortoonethatidentifies. Thedetailsofourapproachfollow.
First, we describe some constructions due to Angluin [Ang88] that will be useful for our
derivation. ThefollowingcanbefoundinExample3fromAngluin[Ang88].
Definition20(DistributionInducedbySequence). Letσ = (x ,x ,...,)besomecountablesequence
i
1
i2
ofelementsinX,andσ = x ,j ∈ N. DefineP tobeadistributionsuchthatitsmassP (x)onanypoint
j ij σ σ
x ∈ Xis
P (x) := ∑ 1 ,
σ 2j+1
j∈N:σj=x
withasumoveranemptysetofindicesinterpretedas0.
P
Angluin [Ang88] describes a way to draw i.i.d. samples from , given only access to finite pre-
σ
fixesofσ andtoanoraclethatsimulatesafaircoin.19 Theideaisnatural: flipthefaircoinuntila
headisobserved,let I betherandomvariabledenotingthenumberoftrialsitneeded,andoutput
thestring x I+1.Thisprocessgivesi.i.d. drawsfromP σ.
Proposition 7.2 (Example 4 from Angluin [Ang88]). Let σ = (x ,x ,...,) be some countable se-
i
1
i2
quence of elements in X. Given access to an oracle that simulates fair coin flips and an oracle which given
inputany j ∈ Nreturnsσ,thereexistsacomputablealgorithmthatsamplesfromP (Definition20)and
j σ
terminateswithprobability1.
The next result shows that a stable generating algorithm for which MOP(·) is decidable and
achieves unambiguous generation at some rate R(·), “stabilizes” to an unambiguous generator
whenexecutedonaninfinitei.i.d. streamofdatadrawnfromavaliddistribution.
Lemma 7.3. Let R: N → R ≥0 be a rate function, i.e., lim n→∞R(n) = 0, let L be a countable language
collection, and (G n: Xn → G)
n∈N
be a generating algorithm for which MOP(·) is decidable and which
satisfiesthefollowingtwoproperties:
19Infact,Angluin’sconstructiongeneralizestooraclesthatsimulateany(non-deterministic)coininastraightforward
way.
63• (G n)
n∈N
isastablegenerator(Definition7),and
• for its unambiguous generation error er(·), it holds that, for every valid distribution P with respect
toLthereexistc,C > 0suchthatE X 1,...,Xn∼Pn [er(G n(X 1,...,X n))] ≤ C·R(c·n).
Then,foreveryvaliddistributionPwithrespecttoLitholdsthat
Pr [∃n∗ ∈ N : ∀n ≥ n∗ itholdsthater(G (X ,...,X )) = 0] = 1.
n 1 n
{Xi} i∈N∼P∞
In other words, the generating algorithm G = (G ) stabilizes to an unambiguous generation in
n
the online sense with probability 1. Roughly speaking, given the above result, Theorem 3.6 will
followfromacontradictiontotheimpossibilityresultintheonlinesettingTheorem3.7.
ProofofLemma7.3. AssumetowardscontradictionthatthereexistssomevalidP withrespecttoL
sothat
Pr [∃n∗ ∈ N : ∀n ≥ n∗ itholdsthat {er(G (X ,...,X )) = 0}] = c′ < 1.
n 1 n
{Xi} i∈N∼P∞
Letusalsodenotec′′ := 1−c′.Noticethatc′′ > 0.SinceP isavaliddistributionwithrespecttoL ,
itissupportedoversomeK ∈ L ,sowehavethat,withprobability1,aninfinitei.i.d. drawfromP
isanenumerationofK (seeProposition5.2). LetuscallthiseventE .
1
Moreover,sinceG isastablegenerator(inanonlinesense),undertheeventE (i.e.,whenthe
n 1
samplesfromP formanenumerationof K),thereexistssomesmallestnumbert∗ := t∗(X ,...) ∈
1
N suchthatforalln ≥ t∗
supp(G n(X 1,...,X n)) = supp(G n+1(X 1,...,X n+1)) .
Now, t∗ dependsonthespecificenumerationdrawnand,hence,thedistributionP inducesadis-
tributionovert∗.Further,notethatwithprobability1,t∗ < ∞ .Hence,Pr {Xi} i∈N∼P∞ [t∗(X 1,...) > n]
approaches0asn → ∞ . Inparticular,thereissomenumbern ∈ N suchthatforalln ≥ n
1 1
c′′
Pr [t∗(X ,...) > n] ≤ .
1
{Xi} i∈N∼P∞ 3
Moreover,sincethegeneratorachievesrate R(·)andlim n→∞R(n) = 0,itholdsthat
lim Pr [er(G (X ,...,X )) ̸= 0] = 0.
n→∞X 1,...,Xn∼Pn n 1 n
Thus,thereissomen ∈ N suchthat,foralln ≥ n
2 2
c′′
Pr [er(G (X ,...,X )) ̸= 0] ≤ .
n 1 n
X 1,...,Xn∼Pn 3
Letn
3
:= max{n 1,n 2}. Hence,takingaunionbound,weseethatwithprobabilityatleast1−2c′′/3
overthedrawof{X } itholdsthat
i i∈N
• er(G (X ,...,X )) = 0,and
n3 1 n3
• supp(G (X ,...,X )) = supp(G (X ,...,X )),foralln ≥ n .
n 1 n n3 1 n3 3
64By the definition of er(·) (Equation (6),) for any n,n′ ∈ N , samples x ,...,x and x ,...,x it
i 1 in j 1 j n′
holdsthat
(cid:0) (cid:0) (cid:1)(cid:1)
supp(G n(x i 1,...,x in)) = supp G n′ x j 1,...,x j n′ =⇒
(cid:0) (cid:0) (cid:1)(cid:1)
er(G (x ,...,x )) = er G x ,...,x
n i 1 in n j 1 j n′
Thesetwoconditionsimmediatelyimplythat,withprobabilityatleast1−2c′′/3 > c′,foralln ≥ n
3
itholdsthat
• er(G (X ,...,X )) = 0,and
n 1 n
• supp(G n+1(X 1,...,X n+1)) = supp(G n(X 1,...,X n)).
Hence,
Pr [∃n∗ ∈ N : ∀n ≥ n∗ itholdsthat {er(G (X ,...,X )) = 0}] > c′ ,
n 1 n
{Xi} i∈N∼P∞
whichgivesthedesiredcontradiction. Thisconcludestheproof.
Having established the previous result, we are ready to show how to use such a generator that
works in the statistical setting to get a generator in the online setting. The idea of the proof is to
P
use the enumeration σ provided from the adversary to define a valid distribution (see Propo-
σ
sition7.2)andthenruntheaforementionedgeneratoronthisdistribution.
Lemma7.4. Let R: N → R
≥0
bearatefunction,i.e.,lim n→∞R(n) = 0,letLbealanguagecollection,
and(G n: Xn → G) n∈NbegeneratingalgorithmforwhichMOPisdecidableandsatisfiesthefollowingtwo
properties:
• (G n)
n∈N
isastablegenerator(Definition7),and
• for its unambiguous generation error, it holds that, for every valid distribution P with respect to L
thereexistc,C > 0suchthat E X 1,...,Xn∼Pn [er(G n(X 1,...,X n))] ≤ C·R(c·n).
(cid:16) (cid:17)
Then, there is a randomized generating algorithm G′ : Xn →r G for which, for any target language
n n∈N
K ∈ LandeveryenumerationσofK,itholdsthat
• (G n′)
n∈N
isastablegenerator(Definition7),and
•
Pr(cid:2) ∃n∗ ∈ N : ∀n ≥ n∗ itholdsthater(cid:0) G′ (σ ,...,σ )(cid:1) = 0(cid:3) = 1,
n 1 n
wheretheprobabilityiswithrespecttotheinternalrandomnessofthealgorithm.
Proof. LetK ∈ L beanytargetlanguageandσbeanyenumerationofK.LetP bethedistribution
σ
P L
defined in Definition 20. We know that, by definition, is valid with respect to , since it is
σ
supported on K. Let (G n′) n∈N be a generating algorithm which, for every n ∈ N , runs G n on P σ.
In order to draw samples from P the generator G′ uses its internal randomness and the process
σ n
65P L
describedinProposition7.2. Since isavaliddistributionwithrespectto ,Lemma7.3givesus
σ
that
Pr [∃n∗ ∈ N : ∀n ≥ n∗ itholdsthat {er(G (X ,...,X )) = 0}] = 1.
n 1 n
{Xi} i∈N∼P∞
σ
Hence,thisimpliesthat
Pr(cid:2) ∃n∗ ∈ N : ∀n ≥ n∗ itholdsthat (cid:8) er(cid:0) G′ (σ ,...,σ )(cid:1) = 0(cid:9)(cid:3) = 1,
n 1 n
wheretheprobabilityistakenwithrespecttotheinternalrandomnessofthealgorithm. Moreover,
since(G n) n∈N isastablegeneratoritalsoholdsthat(G n′) n∈N isastablegenerator. Thisconcludes
theproof.
WearenowreadytoproveTheorem3.6,whichfollowsascorollaryofLemma7.4andtheimpos-
sibilityresultfromtheonlinesetting(Theorem3.7).
ProofofTheorem3.6. LetL beacountablecollectionoflanguages. Assumethatsuchastablegener-
atingalgorithmexists. Then,usingtheconstructionfromLemma7.4wegetastablegeneratorthat
generatesunambiguouslyinthelimit, foreverytargetlanguage K ∈ L andeveryenumeration σ
ofK,withprobability1. ThiscontradictstheimpossibilityresultfromTheorem3.7.
8 Proofs from Section 3.4 (Further Results for Identification)
8.1 ProofofProposition3.8(IdentificationUsingSubsetOracle)
We first give a sufficient condition on the algorithm that identifies in the limit that allows one to
directlyuseitinthestatisticalsettingandgetexponentialrates.
Lemma8.1. LetLbeacountablecollectionoflanguages. LetA = {h n} n∈Nbeanalgorithmthatidentifies
Linthelimitwithpositiveexampleswiththefollowingadditionalproperty:
• foreverytargetlanguageK ∈ Lthereexistsafinitesetofexamples{x ,...,x } ⊆ K thatdepends
i
1
iℓ
onlyonK,andtheenumerationofL ,X
,
• andafinitenumbern ∈ NthatdependsonK,theenumerationofL ,X ,
0
such that A always identifies correctly if its input has size at least n and it contains x ,...,x . Then, A
0 i
1
iℓ
identifiesK withexponentialratesinthestatisticalsetting.
Proof. Let P be a valid data-generating distribution. Then, by definition, supp(P) = K, for some
K ∈ L . Let x ,...,x ⊆ K be a set of points such that after A takes as input this set it starts
i
1
iℓ
identifyingcorrectly,i.e.,foranySsuchthat x ,...,x ⊆ Sand|S| ≥ n itholdsthat L = K.
i
1
iℓ 0 h|S|(S)
Since P is a valid data-generating distribution it holds that x ,...,x ⊆ supp(P). Let p ,...,p
i
1
iℓ ii iℓ
66be the mass of points x ,...,x under P . Suppose we draw n samples i.i.d. from P . Then, the
i
1
iℓ
probabilitythatwedonotobserveall x ,...,x inthesampleisboundedas
i
1
iℓ
∑
Pr [∃j ∈ [ℓ]: x ∈/ {X ,...,X }] ≤ Pr [x ∈/ {X ,...,X }] (byaunionbound)
X 1,...,Xn∼Pn
ij 1 n
j∈[ℓ]X 1,...,Xn∼Pn
ij 1 n
∑
(cid:16) (cid:17)n
= 1−p (sincewehavei.i.d. draws)
ij
j∈[ℓ]
≤ ∑ e−pij·n (usingthat1−z ≤ e−z forallz ∈ R )
j∈[ℓ]
≤
ℓ·e−min j∈[ℓ]pij·n
.
Thus, the algorithm identifies correctly in the statistical setting after taking as input n ≥ n ∈ N
0
examples, with probability at least 1−C·e−c·n, for some distribution dependent constants C,c.
Thisconcludestheproof.
WearenowreadytoproveProposition3.8.
ProofofProposition3.8. Let L be a collection that is identifiable in the limit and assume access to
a subset oracle. From Section B.1 we know that the algorithm of Kleinberg and Mullainathan
L
[KM24] given access to a subset oracle identifies any identifiable collection in the limit . More-
over, by Lemma 5.12 we get that this algorithm satisfies the condition of Lemma 8.1, thus this
resultimmediatelygivesusthatthealgorithmofKleinbergandMullainathan[KM24]obtainsex-
ponentialratesforidentificationinthestatisticalsetting(assumingaccesstoasubsetoracle).
8.2 ProofofProposition3.9(IdentificationofFiniteCollections)
Similartotheprevioussection,wewillshowthatthereexistsanalgorithmthatsatisfiesLemma8.1,
i.e.,foranyfinitecollectionof(potentiallyinfinite)languages,itidentifieswithexponentialrates.
Recall the domain X has an enumeration X = {x ,...,}. Consider the following algorithm for
1
identification.
AlgorithmAlgorithm8.2 - IdentifyingafinitecollectionL = {L ,...,L }inthelimit
1 k
Description:
1. foreacht ∈ Ndo:
(a) LetS = {x ,...,x },where x istheelementthealgorithmseesinroundℓ
t i
1
it iℓ
(b) ConstructaversionspaceV containingalllanguages L ⊇ S
t t
(c) LetV′ = (cid:8) L ∈ V : ∀j ∈ [t],∀L′ ∈ V itholdsthat x ∈ L =⇒ x ∈ L′(cid:9)
t t t j j
(d) if V′ ̸= ∅ then: outputthesmallestindex jsuchthat L isinV′
t j t
(e) else: outputanarbitraryindex j
ProofofProposition3.9. Let us first show that the previous algorithm identifies in the limit. Let
k := |L|denotethesizeofL .ConsideranytargetlanguageK ∈ L .NoticethatL canbepartitioned
into three sets: the languages L ∈ L such that L = K, the languages L ∈ L such that K ⊊ L and
67the languages L ∈ L such that K ̸⊆ L. Then, for every language L ∈ L such that K ̸⊆ L there
j j
existssome x ∈ K suchthat x ∈/ L .Leti ∈ N bethesmallestnumberforwhich x ∈ K,x ∈/ L .
j j ij ij j
Let L′ ⊆ L be the set of all such languages and X′ the set of all such smallest indexed x ∈ X .
Notice that, since we consider a fixed enumeration of
X
throughout, the set
X′
depends only on
the target language K and the enumerations of L ,X . Since the collection L is finite we have that
|X′| < k < ∞ .
NowconsideranylanguageL ∈ L suchthatK ⊊ L.LetL′′ ⊆ L bethesetofallsuchlanguages.
Then, for every L ∈ L′′ there is some x ∈ L such that x ∈/ K. Let i′ be the smallest such index.
j j j
Define
(cid:110) (cid:111)
n := max i′ : L ∈ L′′ ,
0 j∈N j j
i.e., the largest index among these elements. Since the collection L is finite it holds that n <
0
∞ . Consider any execution of the algorithm in any round t ∈ N for which the input sample S
satisfies i) X′ ⊆ S, and ii) |S| ≥ n . The first condition on S implies that for this round V =
0 t
{L ∈ L : K ⊆ L}. Moreover, the second condition implies that V′ = {L ∈ L : L = K}. Thus, the
t
smallest j ∈ N such that L ∈ V′ is the smallest index z ∈ N such that L = K. Thus, there is
j t z
somelargeenought∗ sothatthealgorithmoutputstheindexzforallt ≥ t∗.Moreover,thesetX′
and the number n satisfy the conditions of Lemma 8.1, thus the algorithm identifies with exact
0
exponentialratesinthestatisticalsetting. Thisconcludestheproof.
8.3 ProofofProposition3.10(IdentificationofCollectionsofFiniteLanguages)
Inthissection,wegivetheproofofProposition3.10.
ProofofProposition3.10. RecallGold’salgorithm[Gol67]thatidentifiesinthelimitforsuchcollec-
tionsL : atanystepn ∈ N letS bethesetofelementstheadversaryhaspresentedsofar. Output
n
min(cid:8) j ∈ N : S ⊆ L (cid:9) .ConsideranyvaliddistributionP withrespecttoL .Then,P issupported
n j
on some language K ∈ L . Let {x ,...,x } := K and p be the mass of element x ,j ∈ [k]. For
i
1
i
k
ij ij
everyn ∈ N ,letE betheeventthattheni.i.d. drawsfromP containtheset{x ,...,x },i.e.,
n i i
1 k
{x ,...,x } = {X ,...,X }.
i 1 i k 1 n
E
Notice that under , the algorithm identifies correctly. Then, for the complement of this event,
n
wehavethat
∑
Pr [∃j ∈ [k]: x ∈/ {X ,...,X }] ≤ Pr [x ∈/ {X ,...,X }] (byaunionbound)
X 1,...,Xn∼Pn
ij 1 n
j∈[ℓ]X 1,...,Xn∼Pn
ij 1 n
∑
(cid:16) (cid:17)n
= 1−p (sincewehavei.i.d. draws)
ij
j∈[k]
≤ ∑ e−pij·n (usingthat1−z ≤ e−z forallz ∈ R )
j∈[k]
≤
k·e−min j∈[k]pij·n
.
Thisconcludestheproof.
688.4 ProofofTheorem3.11(IdentificationfromPositiveandNegativeExamples)
Wenowmoveontothetaskoflanguageidentificationwithbothpositiveandnegativeexamples.
The main difference between this setting and binary classification is that the objective function
is different. In particular, in our setting, the learner is required to identify the target language,
whereas in the classification setting the learner is required to output a function that labels most
of the elements of the domain according to some target labeling function. Thus, it is clear that
theidentificationtaskismorechallengingthantheclassificationtask. Stickingtothenotationwe
used before, we have a countable set of languages L = {L ,L ,...}, where each L ∈ L is also
1 2
countableand∪ L∈LL ⊆ X ,forsomecountabledomainX .Recallthenotionofvaliddistributionin
thissetting[Ang88]: adistributionP isvalidwithrespecttoL ifandonlyifsupp(P) ⊆ X×{0,1}
andthereexistssomeK ∈ L suchthatforall x ∈ K wehaveP[(x,1)] > 0,P[(x,0)] = 0andforall
x ∈/ K wehaveP[(x,0)] > 0,P[(x,1)] = 0(seeDefinition15).
Next, recall that for any n ∈ N and and set of labeled examples S = (x ,y ),...,(x ,y ) ∈
n 1 1 n n
(X×{0,1})n theerrorofthelearner{h n: (X×{0,1})n → N} n∈N forthistaskis
(cid:110) (cid:111)
er(h (S )) = 1 L ̸= K . (11)
n n hn(Sn)
Notice that, under this definition, E[er(h )] = Pr[L ̸= K], i.e., the probability that h fails to
n hn n
identifythecorrectlanguageafteritseesnexamplesfromthedata-generatingdistribution.
Ourproofproceedsintwoparts. First,weshowthatforallcountablecollectionsoflanguages
thatarenon-trivialforidentification(Definition13)exponentialrateisthebestpossibleforidenti-
fication withpositive and negativeexamples. The approach isessentially identical toLemma 5.1
andthelowerboundfromBousquetetal.[BHM+21]. Then,weshowthatallcountablecollections
oflanguagesarelearnableatexponentialrateswithpositiveandnegativeexamples.
Theformalstatementregardingtheexponentialrateslowerboundfollows.
Lemma8.2. LetLbeanon-trivialcollectionoflanguages. Then,foranylearningalgorithmA = {h n} n∈N
thereexistsavaliddistributionPsuchthatE[er(h )] ≥ C·e−c·n,forinfinitelymanyn ∈ N .
n
Proof. Since L is non-trivial, there exist L,L′ ∈ L and x ∈ X such that L ̸= L′ and x ∈ L,x ∈ L′.
Let P L,P L′ be valid distributions for L,L′ that place at least 1/2 mass on (x,1) and they spread
P P
the remaining mass arbitrarily as follows: half of the remaining mass of
L
(respectively L′) is
spread arbitrarily on all the elements of L (respectively L′) with label 1, and the other half on all
the elements of K\ L (respectively K\ L′) with label 0. Notice that since L ̸= L′ at least one of
them has at least one more element other than x. For any n ∈ N , under both distributions, with
probabilityatleast2−n,thealgorithmwillonlyseetheelement(x,1)appearinginthesample. Let
E
bethateventandconditiononit. Noticethat
n
(cid:104) (cid:105) (cid:104) (cid:105)
Pr L = L | E +Pr L = L′ | E ≤ 1,
hn((x,1),...,(x,1)) n hn(x,...,x) n
wheretheprobabilityiswithrespecttotherandomnessofthelearningalgorithm. Thus,wehave
(cid:104) (cid:105) (cid:104) (cid:105)
that Pr L
hn((x,1),...,(x,1))
̸= L | E
n
≥ 1/2 or Pr L
hn((x,1),...,(x,1))
̸= L′ | E
n
≥ 1/2. By the pigeonhole
principle,foratleastoneofL,L′,thepreviousinequalityholdsforinfinitelymanyn ∈ N .Assume
69withoutlossofgeneralityitholdsforLanddenotebyN(cid:98) thesetofn ∈ N forwhichitholds. Then,
foralln ∈ N(cid:98) wehavethat
(cid:104) (cid:105)
E [er(h ((X ,Y ),...,(X ,Y )))] = Pr L ̸= L
(X 1,Y 1),...,(Xn,Yn)∼Pn
L
n 1 1 n n
(X 1,Y 1),...,(Xn,Yn)∼Pn
L
hn((X 1,Y 1),...,(Xn,Yn))
(cid:104) (cid:105)
≥ Pr L ̸= L | E ·
(X 1,Y 1),...,(Xn,Yn)∼Pn
L
hn((X 1,Y 1),...,(Xn,Yn)) n
Pr [E ]
n
(X 1,Y 1),...,(Xn,Yn)∼Pn
L
1 (cid:104) (cid:105)
≥ ·Pr L ̸= L | E
2n hn((x,1),...,(x,1)) n
E
(bythedefinitionof )
n
1
≥ , (duetotheassumptionon L)
2n+1
whichconcludestheproof.
Wenowmoveontoestablishingtheupperbound. Followingtheapproachfromthesettingwith
onlypositiveexamples,weshowthataninfinitedrawofi.i.d. samplesfromanyvaliddistribution
isa“completepresentation”ofthetargetlanguageK,i.e.,alltheelementsofKappearwithlabel1
andallelementsoutsideofK appearwithlabel0. ThisfollowsimmediatelyfromProposition5.2.
A
ThenextsteptowardsprovingTheorem3.11istoshowif isanalgorithmthatidentifiesthe
target language in the limit in the adversarial (online) setting of Gold [Gol67] with positive and
A
negativeexamples,then isaconsistentalgorithminthestatisticalsetting. Thisimpliesthatfor
any valid distribution P , there is some number t∗ := t∗(A ,P) ∈ N such that, when we draw t∗
P
many i.i.d. samples from , it will identify the target language with probability at least 6/7. We
denotethetimeofthelastmistakeofalgorithmA = {h n} n∈N onalabeledsequenceofexamples
(x 1,y 1),(x 2,y 2),...by TA(x 1,y 1,x 2,y 2,...),i.e.,
(cid:110) (cid:111)
TA(x 1,y 1,x 2,y 2,...) = inf n
0
∈ N : L
hn(x 1,y 1,...,xn,yn)
= K,∀n > n
0
.
Thenextresultformalizestheclaim. ItsproofisalmostidenticaltoProposition5.3,butwepresent
itforcompleteness.
Proposition 8.3. Fix any family of languages L over a countable domain. For any algorithm A =
{h n} n∈N that identifies L in the limit with positive and negative examples in the online setting and any
validdistributionP(Definition15),thereexistsanumbert∗ suchthat
6
{(Xi,Yi)P
}
ir ∈N∼P∞[TA(X 1,Y 1,X 2,Y 2,...) ≤ t∗] ≥
7
.
Proof. Let (X ,Y ),(X ,Y ),..., be a countable i.i.d. sample from P . From Proposition 5.2 we get
1 1 2 2
that this sample is a valid input to A since, with probability one, all elements of K appear with
label 1 and all elements of
X\K
appear with label 0. Consider the execution of
A
on prefixes of
thesequenceanddenotebyTA := TA(X 1,Y 1,X 2,Y 2,...)thetimeitmadeitslastmistake. Wehave
thatPr
{(Xi,Yi)}
i∈N∼P∞[TA ∈ N] = 1.Thus,
tl →im ∞{(Xi,Yi)P }r i∈N∼P∞[TA(X 1,Y 1,X 2,Y 2...) ≥ t] = 0.
70Thus,asrequired,thereexistssomet∗ ∈ N suchthat
1
Pr [TA(X 1,Y 1,X 2,Y 2,...) ≥ t∗] ≤ .
{(Xi,Yi)} i∈N∼P∞ 7
The problem is that this time t∗ depends on the algorithm A and the unknown distribution P .
Supposeweknewanumbert∗ sothatforallt ≥ t∗
(cid:104) (cid:105) 1
E [er(h (S)) > 0] = Pr L ̸= K < .
S∼Pt
t
S∼Pt
ht(S)
4
Then, we could design an identification algorithm {h n} n∈N with exponential rates as follows.
First, we break up the data into batches, each of length t∗. Second, we run the identification
algorithmfromtheonlinesettingseparatelyforeachbatch. Finally,weaggregatethesealgorithms
bytakingamajorityvote. Now,bythedefinitionoft∗ andHoeffding’sinequality,theprobability
thatmorethanone-thirdoftheclassifiershavenotidentifiedthelanguageisexponentiallysmall.
Therearetwoissueswiththisapproach:
• In our language setting, it can be the case that two identification algorithms are correct but
outputdifferentindicesforthetruelanguage. Thiswasalsoanissueforthepositiveexam-
plescaseandwecanresolveitinasimilarwayusingLemma5.4.
• Thesecondissueisthatt∗ dependsonthedistributionP .Inthepreviouscaseofpositiveex-
amples,thesolutionwastoguessthenumbert∗ usingsomeveryslowlyincreasingfunction
g(n). Thiswasthereasonwhywedidnotmanagetogetexponentialrates. Interestingly,in
thesettingwherewehaveaccesstopositiveandnegativeexamples,wecanestimatet∗ from
samples,asweseebelow.
The main lemma behind this result is the following and corresponds to an adaptation of Lemma
4.4fromBousquetetal.[BHM+21]withadifferentlossfunction. Thislemmawillallowustoget
exactlyexponentialrates,comparedtotheratesobtainedinthepositiveexamplesregime.
Lemma 8.4 (Estimation of Stopping Time). Let S be n i.i.d. examples observed from P which is valid
n
with respect to some language K ∈ L . There exists universally measurable t = t (S ), whose definition
n n n
doesnotdependonP,sothatthefollowingholds. Givent∗ suchthat
(cid:104) (cid:105) 1
Pr L ̸= K < ,
S t∗
h t∗(S t∗)
8
thereexistC,c > 0independentofn(butdependingonP ,t∗)sothat
Pr[t ∈ T] ≥ 1−Ce−cn
n
where
(cid:26) (cid:27)
(cid:104) (cid:105) 3
T = 1 ≤ t ≤ t∗ : Pr L ̸= K < .
St
ht(St)
8
Giventheabovelemma,wearenowreadytocompletetheproofofTheorem3.11.
71ProofofTheorem3.11. First,theexponentialratelowerboundfollowsimmediatelyfromLemma8.2.
Theoutputofouridentificationalgorithmh nisthemajorityofthehi
tn
fori ∈ I
n
= {1,2,...,⌊n/(2tn)⌋},
afterweapplytothemthepost-processingcomputationdescribedinLemma5.4tomapthemto
thesameindexofK.Letz ∈ N bethesmallestnumberforwhich L = K. Itremainstoshowthat
z
(cid:104) (cid:105)
E [er(h )] = Pr L ̸= K ≤ Ce−cn,
Sn∼Pn
n
Sn∼Pn
hn(Sn)
forsomeconstantsC,c > 0dependingonP .
Let us consider our predictors {hi } that are obtained by running the identification algo-
tn i∈In
rithm on independent parts of the dataset of size t . Since these predictors might be outputting
n
different indices (descriptions) of the same language, we find the smallest indexed language the
output of each classifier can be mapped to. By Lemma 5.4, for n sufficiently large, all the indices
fromtheoutputsoftheclassifiershi ,i ∈ I ,thatcorrespondtoK willbemappedtoz.
tn n
Let us fix t ∈ T , where T is defined in Lemma 8.4, and consider the predictors {hi} for i ∈
t
I = {1,2,...,⌊n/(2t)⌋}. We also denote by {(cid:98)hi t} the output of predictor {hi t} after applying the
post-processing result from Lemma 8.4. For n sufficiently large, standard concentration bounds
givethat
(cid:34) (cid:35)
Pr 1 ∑ 1{(cid:98)hi ̸= z} > 7 < e−⌊n/2t∗⌋/128.
|I | t 16
n i∈In
This means that, except on an event of exponentially small probability, we have that (cid:98)hi outputs
t
indexz. Recallthat L = K.
z
Nowweemployourestimationt intheabovecalculation. Inparticular,
n
(cid:104) (cid:105)
Pr (cid:98)hi
tn
̸= z foratleasthalfofi ∈ I
n
≤ p 1+p 2,
where
p := Pr[t ∈/ T] < Ce−cn,
1 n
and
(cid:104) (cid:105)
p := Pr ∃t ∈ T : hi doesnotpredictK foratleasthalfindices ≤ t∗ e−⌊n/2t∗⌋/128.
2 t
Thisgivesthedesiredresult.
WeconcludethissectionwiththeproofofLemma8.4.
ProofofLemma8.4. We split the training set S into two sets and we further split the sets into
n
batches. Theideaistousethefirstsettotrainmultipleindependentinstancesoftheonlinelearn-
ingalgorithmandthesecondsettoestimateitsidentificationerror.
More concretely, let A the algorithm that identifies in the limit. For each batch size 1 ≤ t ≤
⌊n/2⌋andbatchindex1 ≤ i ≤ ⌊n/(2t)⌋,welet
(cid:16) (cid:17)
Ii = h X ,Y ,X ,Y
t t (i−1)t+1 (i−1)t+1 it it
be the index of the output of the learning algorithm that is trained on batch i of a subset of the
dataset that has size t. This is a mapping from the training samples to indices of the predicted
language.
72(cid:8) (cid:9)
For every fixed t, the outputs Ii are trained on different parts of the first half of the
t i≤⌊n/2t⌋
trainingsetandtheyareindependentofthewholesecondhalfofthetrainingset. Thismeansthat
(cid:8) (cid:9)
we can view every Ii as an independent draw of the distribution of h (·). To estimate
t i≤⌊n/2t⌋ t
theidentificationerrorofh (·),wewillmakeuseofthesecondhalfofthetrainingset. Wedefine
t
e =
1
⌊n ∑/2t⌋
1(cid:110) 1(cid:110)
X ∈ L
(cid:111)
̸= Y forsome
n
≤ s ≤
n(cid:111)
.
(cid:98)t ⌊n/2t⌋ s I ti s 2
i=1
We underline that this can be computed using just membership calls to the predicted languages.
Nowobservethat,almostsurely,
e ≤ e =
1
⌊n ∑/2t⌋ 1(cid:26)
Pr
(cid:104) 1(cid:110)
X ∈ L
(cid:111)
̸=
Y(cid:105)
>
0(cid:27)
=
1
⌊n ∑/2t⌋
1(cid:110)
L ̸=
K(cid:111)
.
(cid:98)t t ⌊n/2t⌋
i=1
(X,Y)∼P I ti ⌊n/2t⌋
i=1
I ti
Wedefine(cid:98)t
n
= inf{t ≤ ⌊n/2⌋: (cid:98)e
t
< 1/4},whereweassumethatinf∅ = ∞ .
Wenowwanttoboundtheprobabilitythat(cid:98)t
n
> t⋆. UsingHoeffding’sinequalitywegetthat
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
Pr(cid:2) (cid:98)t n > t⋆(cid:3) ≤ Pr (cid:98)e t⋆ ≥ 1 ≤ Pr e t⋆ ≥ 1 = Pr e t⋆ − 1 ≥ 1 = Pr e t⋆ −E[e t⋆] ≥ 1 ≤ e−⌊n/2t⋆⌋/32.
4 4 8 8 8
Thisimpliesthat(cid:98)t
n
≤ t⋆ exceptforaneventwithexponentiallysmallprobability.
Moreover,thereissomeε > 0suchthatforall1 ≤ t ≤ t⋆ with
(cid:20) (cid:21)
(cid:104) (cid:110) (cid:111) (cid:105) 3
Pr Pr 1 X ∈ L ̸= Y > 0 > ,
St∼Pt (X,Y)∼P
ht(St)
8
(cid:104) (cid:104) (cid:110) (cid:111) (cid:105) (cid:105)
wehavethatPr St∼Pt Pr (X,Y)∼P 1 X ∈ L ht(St) ̸= Y > ε > 1/4+1/16(thisholdsbycontinuity).
Nowfixsome1 ≤ t ≤ t⋆ suchthat
(cid:20) (cid:21)
(cid:104) (cid:110) (cid:111) (cid:105) 3
Pr Pr 1 X ∈ L ̸= Y > 0 >
St∼Pt (X,Y)∼P
ht(St)
8
(ifitexists). Then,usingHoeffding’sinequalityagainwegetthat
(cid:34) (cid:35)
Pr
1
⌊n ∑/2t⌋ 1(cid:26)
Pr
(cid:104) 1(cid:110)
X ∈ L
(cid:111)
̸=
Y(cid:105)
>
ε(cid:27)
<
1
≤
e⌊n/2t⋆⌋/128.
⌊n/2t⌋
i=1
(X,Y)∼P I ti 4
Foranylanguage LsuchthatPr (X,Y)∼P[1{X ∈ L} ̸= Y] > ε,then
Pr[1{X
s
∈ L} ̸= Y
s
forsomen/2 ≤ s ≤ n] ≥ 1−(1−ε)n/2 .
As we mentioned before, {Ii} are independent of (X ,Y ) . Thus, applying a union
t i≤⌊n/2t⌋ s s s>n/2
(cid:104) (cid:110) (cid:111) (cid:105)
bound we get that the probability that all I ti that have Pr (X,Y)∼P 1 X ∈ L Ii ̸= Y > ε make at
t
leastoneerroronthesecondhalfofthetrainingsetis
(cid:20) (cid:26) (cid:27) (cid:21)
(cid:104) (cid:110) (cid:111) (cid:105) (cid:110) (cid:110) (cid:111) (cid:111)
Pr 1 Pr 1 X ∈ L
It
̸= Y > ε ≤ 1 1 X
s
∈ L
It
̸= Y
s
forsomen/2 < s ≤ n foralli ∈ [⌊n/2t⌋]
(X,Y)∼P i i
(cid:106)n(cid:107)
≥ 1− (1−ε)n/2 .
2t
73Thus,wegetthat
(cid:20) (cid:21)
1 (cid:106)n(cid:107)
Pr[(cid:98)t
n
= t] ≤ Pr (cid:98)e
t
< ≤ (1−ε)n/2+e−⌊ 2n t⋆⌋/128.
4 2
Usingthepreviousestimatesandapplyingaunionbound,wegetthat
(cid:106)n(cid:107)
Pr[(cid:98)t
n
∈/ T] ≤ e−⌊n/2t⋆⌋/32+t⋆ (1−ε)n/2+t⋆ e−⌊n/2t⋆⌋/128 ≤ Ce−cn,
2
forsomeconstantsC,c > 0. NotethatC = C(P ,t∗)andc = c(P ,t∗).
Acknowledgments
We thank Ahmad Beirami and Manolis Zampetakis for helpful discussions and references after
theoriginaldraft. WethankDylanMcKayfordiscussionsandreferencesaboutthetheoryofcom-
putationduringthepreparationofthispaper,andspecificallyforinformingusaboutLemmas6.2
and 6.3. We thank Kyriakos Lotidis for useful discussions about the lower bound of Angluin
[Ang88]. We also thank Yuan Deng, Sid Mitra, Ansong Ni, Argyris Oikonomou, Xizhi Tan, and
ManolisZampetakisfortheirfeedbackonadraftofthispaper. AlkisKalavasiswassupportedby
the Institute for Foundations of Data Science at Yale. Grigoris Velegkas was supported by the AI
InstituteforLearning-EnabledOptimizationatScale(TILOS).
74References
[AB09] S.AroraandB.Barak.ComputationalComplexity:AModernApproach.CambridgeUni-
versity Press, 2009. ISBN: 9781139477369. URL: https://books.google.com/books?
id=nGvI7cOuOOQC(cit.onp.29).
[AB17] MartinArjovskyandLeonBottou.“TowardsPrincipledMethodsforTrainingGener-
ative Adversarial Networks”. In: International Conference on Learning Representations.
2017.URL:https://openreview.net/forum?id=Hk4_qw5xe(cit.onpp.1,3,5).
[AB91] LeonardMAdlemanandManuelBlum.“InductiveInferenceandUnsolvability”.In:
The Journal of Symbolic Logic 56.3 (1991), pp. 891–900. DOI: 10.2307/2275058 (cit. on
p.5).
[ABL+22] Noga Alon, Mark Bun, Roi Livni, Maryanthe Malliaris, and Shay Moran. “Private
and Online Learnability Are Equivalent”. In: J. ACM 69.4 (Aug. 2022). ISSN: 0004-
5411. DOI: 10.1145/3526074. URL: https://doi.org/10.1145/3526074 (cit. on
p.18).
[AGR13] Joseph Anderson, Navin Goyal, and Luis Rademacher. “Efficient Learning of Sim-
plices”. In: Proceedings of the 26th Annual Conference on Learning Theory. Ed. by Shai
Shalev-Shwartz and Ingo Steinwart. Vol. 30. Proceedings of Machine Learning Re-
search.Princeton,NJ,USA:PMLR,June2013,pp.1020–1045.URL:https://proceedings.
mlr.press/v30/Anderson13.html(cit.onp.19).
[AHK+24] Idan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, and Grigoris Velegkas.
“Universal Rates for Regression: Separations between Cut-Off and Absolute Loss”.
In:ProceedingsofThirtySeventhConferenceonLearningTheory.Ed.byShipraAgrawal
and Aaron Roth. Vol. 247. Proceedings of Machine Learning Research. PMLR, June
2024, pp. 359–405. URL: https://proceedings.mlr.press/v247/attias24a.html
(cit.onpp.9,16).
[AL24] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 1, Learning Hier-
archicalLanguageStructures.2024.arXiv:2305.13673[cs.CL]. URL:https://arxiv.
org/abs/2305.13673(cit.onp.17).
[AL98] András Antos and Gábor Lugosi. “Strong Minimax Lower Bounds for Learning”.
In: Machine Learning 30.1 (1998), pp. 31–56. DOI: 10.1023/A:1007454427662. URL:
https://doi.org/10.1023/A:1007454427662(cit.onpp.4,24).
[AML+24] SumukhKAithal,PratyushMaini,ZacharyC.Lipton,andJ.ZicoKolter.Understand-
ing Hallucinations in Diffusion Models through Mode Interpolation. 2024. arXiv: 2406.
09358[cs.LG].URL:https://arxiv.org/abs/2406.09358(cit.onp.16).
[Ang79] DanaAngluin.“FindingPatternsCommontoaSetofStrings(ExtendedAbstract)”.
In: Proceedings of the Eleventh Annual ACM Symposium on Theory of Computing. STOC
’79. Atlanta, Georgia, USA: Association for Computing Machinery, 1979, pp. 130–
141.ISBN:9781450374385.DOI:10.1145/800135.804406.URL:https://doi.org/10.
1145/800135.804406(cit.onpp.1,3,5–7,10,14,17,20,21,30,96).
75[Ang80] Dana Angluin. “Inductive Inference of Formal Languages From Positive Data”. In:
Information andControl 45.2 (1980), pp. 117–135. ISSN: 0019-9958. DOI:https://doi.
org/10.1016/S0019-9958(80)90285-5. URL: https://www.sciencedirect.com/
science/article/pii/S0019995880902855(cit.onpp.3,6,7,9,14,18–21,28,30,33,
90).
[Ang82] Dana Angluin. “Inference of Reversible Languages”. In: J. ACM 29.3 (July 1982),
pp. 741–765. ISSN: 0004-5411. DOI: 10.1145/322326.322334. URL: https://doi.
org/10.1145/322326.322334(cit.onp.17).
[Ang88] Dana Angluin. Identifying Languages From Stochastic Examples. Yale University. De-
partmentofComputerScience,1988.URL:http://www.cs.yale.edu/publications/
techreports/tr614.pdf(cit.onpp.1–4,8,10,14,16–18,24,26,33,41,63,69,74).
[AOS+16] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
Dan Mané. Concrete Problems in AI Safety. 2016. arXiv: 1606.06565 [cs.AI]. URL:
https://arxiv.org/abs/1606.06565(cit.onp.1).
[AP23] KonstantinosAndriopoulosandJohanPouwelse.AugmentingLLMswithKnowledge:
A Survey on Hallucination Prevention. 2023. arXiv: 2309.16459 [cs.CL]. URL: https:
//arxiv.org/abs/2309.16459(cit.onp.1).
[AS83] Dana Angluin and Carl H. Smith. “Inductive Inference: Theory and Methods”. In:
ACM Comput. Surv. 15.3 (Sept. 1983), pp. 237–269. ISSN: 0360-0300. DOI: 10.1145/
356914.356918.URL:https://doi.org/10.1145/356914.356918(cit.onpp.5,17).
[AWK+24] Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas. “In-Context Language
Learning: Architectures and Algorithms”. In: Forty-first International Conference on
Machine Learning. 2024. URL: https://openreview.net/forum?id=3Z9CRr5srL (cit.
onp.17).
[BAG20] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. “On the Ability and Limita-
tions of Transformers to Recognize Formal Languages”. In: Proceedings of the 2020
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).Ed.byBon-
nie Webber, Trevor Cohn, Yulan He, and Yang Liu. Online: Association for Com-
putational Linguistics, Nov. 2020, pp. 7096–7116. DOI: 10.18653/v1/2020.emnlp-
main.576.URL:https://aclanthology.org/2020.emnlp-main.576(cit.onp.17).
[Bah14] Dzmitry Bahdanau. “Neural machine translation by jointly learning to align and
translate”.In:arXivpreprintarXiv:1409.0473(2014)(cit.onp.1).
[BB75] Lenore Blum and Manuel Blum. “Toward a Mathematical Theory of Inductive In-
ference”. In: Information and Control 28.2 (1975), pp. 125–155. ISSN: 0019-9958. DOI:
https://doi.org/10.1016/S0019-9958(75)90261-2. URL: https://www.
sciencedirect.com/science/article/pii/S0019995875902612(cit.onp.5).
[BCE+23] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,
EceKamar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,HarshaNori,Hamid
Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of Artificial General Intelligence:
EarlyexperimentswithGPT-4.2023.arXiv:2303.12712[cs.CL].URL:https://arxiv.
org/abs/2303.12712(cit.onpp.1,6).
76[BCP+90] PeterF.Brown,JohnCocke,StephenA.DellaPietra,VincentJ.DellaPietra,Fredrick
Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. “A statistical ap-
proachtomachinetranslation”.In:Comput.Linguist.16.2(June1990),pp.79–85.ISSN:
0891-2017(cit.onp.6).
[BDd+92] PeterF.Brown,VincentJ.DellaPietra,PeterV.deSouza,JeniferC.Lai,andRobertL.
Mercer. “Class-Based n-gram Models of Natural Language”. In: Computational Lin-
guistics18.4(1992),pp.467–480. URL:https://aclanthology.org/J92-4003(cit.on
p.1).
[BDV00] YoshuaBengio,RéjeanDucharme,andPascalVincent.“ANeuralProbabilisticLan-
guage Model”. In: Advances in Neural Information Processing Systems. Ed. by T. Leen,
T. Dietterich, and V. Tresp. Vol. 13. MIT Press, 2000. URL: https://proceedings.
neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-
Paper.pdf(cit.onp.1).
[Ber86] Robert C Berwick. “Learning From Positive-Only Examples: The Subset Principle
andThreeCaseStudies”.In:Machinelearning:Anartificialintelligenceapproach2(1986),
pp.625–645(cit.onp.18).
[BGH+23] Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann
Pitassi, Satchit Sivakumar, and Jessica Sorrell. “Stability Is Stable: Connections be-
tweenReplicability,Privacy,andAdaptiveGeneralization”.In:Proceedingsofthe55th
Annual ACM Symposium on Theory of Computing. STOC 2023. Orlando, FL, USA: As-
sociation for Computing Machinery, 2023, pp. 520–527. ISBN: 9781450399135. DOI:
10.1145/3564246.3585246. URL: https://doi.org/10.1145/3564246.3585246
(cit.onp.18).
[BHM+21] OlivierBousquet,SteveHanneke,ShayMoran,RamonvanHandel,andAmirYehu-
dayoff. “A Theory of Universal Learning”. In: Proceedings of the 53rd Annual ACM
SIGACT Symposium on Theory of Computing. STOC 2021. Virtual, Italy: Association
for Computing Machinery, 2021, pp. 532–541. ISBN: 9781450380539. DOI: 10.1145/
3406325.3451087. URL:https://doi.org/10.1145/3406325.3451087(cit.onpp.1,
4,9,14,16–18,24–26,34,51,69,71,96–98).
[BHM+23] OlivierBousquet,SteveHanneke,ShayMoran,JonathanShafer,andIlyaTolstikhin.
“Fine-Grained Distribution-Dependent Learning Curves”. In: Proceedings of Thirty
SixthConferenceonLearningTheory.Ed.byGergelyNeuandLorenzoRosasco.Vol.195.
Proceedings of Machine Learning Research. PMLR, July 2023, pp. 5890–5924. URL:
https://proceedings.mlr.press/v195/bousquet23a.html(cit.onp.16).
[Bid23] JosephRBiden.“ExecutiveOrderontheSafe,Secure,andTrustworthyDevelopment
And Use of Artificial Intelligence””. In: (2023). URL: https://www.whitehouse.
gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-
the-safe-secure-and-trustworthy-development-and-use-of-artificial-
intelligence/(cit.onp.1).
77[BJM83] LalitR.Bahl,FrederickJelinek,andRobertL.Mercer.“AMaximumLikelihoodAp-
proachtoContinuousSpeechRecognition”.In:IEEETransactionsonPatternAnalysis
and Machine Intelligence PAMI-5.2 (1983), pp. 179–190. DOI: 10.1109/TPAMI.1983.
4767370(cit.onp.6).
[Bor23] AliBorji.ACategoricalArchiveofChatGPTFailures.2023.arXiv:2302.03494[cs.CL].
URL:https://arxiv.org/abs/2302.03494(cit.onp.1).
[Bre07] Joan Bresnan. “Is Syntactic Knowledge Probabilistic? Experiments With the English
DativeAlternation”.In:LinguisticsinSearchofitsEvidentialBase.Ed.bySamFeather-
ston and Wolfgang Sternefeld. Berlin, New York: De Gruyter Mouton, 2007, pp. 75–
96. ISBN:9783110198621. DOI:doi:10.1515/9783110198621.75. URL:https://doi.
org/10.1515/9783110198621.75(cit.onp.1).
[BZW+19] DavidBau,Jun-YanZhu,JonasWulff,WilliamPeebles,HendrikStrobelt,BoleiZhou,
andAntonioTorralba.“SeeingWhataGANCannotGenerate”.In:Proceedingsofthe
InternationalConferenceComputerVision(ICCV).2019(cit.onp.1).
[Cla14] AlexanderClark.“DistributionalLearningasaTheoryofLanguageAcquisition”.In:
Proceedings of the 5th Workshop on Cognitive Aspects of Computational Language Learn-
ing(CogACLL).Ed.byAlessandroLenci,MuntsaPadró,ThierryPoibeau,andAline
Villavicencio.Gothenburg,Sweden:AssociationforComputationalLinguistics,Apr.
2014,p.29. DOI:10.3115/v1/W14-0506. URL:https://aclanthology.org/W14-0506
(cit.onp.1).
[CMY23] Zachary Chase, Shay Moran, and Amir Yehudayoff. “Stability and Replicability in
Learning”. In: 2023 IEEE 64th Annual Symposium on Foundations of Computer Science
(FOCS).2023,pp.2430–2439. DOI:10.1109/FOCS57990.2023.00148(cit.onp.18).
[Daw82] A Philip Dawid. “The Well-Calibrated Bayesian”. In: Journal of the American Statisti-
cal Association 77.379 (1982), pp. 605–610. DOI: 10.1080/01621459.1982.10477856.
eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1982.
10477856. URL: https://www.tandfonline.com/doi/abs/10.1080/01621459.
1982.10477856(cit.onp.16).
[DDS15] Anindya De, Ilias Diakonikolas, and Rocco A. Servedio. “Learning From Satisfying
Assignments”. In: Proceedings of the 2015 Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA). 2015, pp. 478–497. DOI: 10.1137/1.9781611973730.33. eprint:
https://epubs.siam.org/doi/pdf/10.1137/1.9781611973730.33. URL: https:
//epubs.siam.org/doi/abs/10.1137/1.9781611973730.33(cit.onp.19).
[Den98] François Denis. “PAC Learning from Positive Statistical Queries”. In: Algorithmic
LearningTheory.Ed.byMichaelM.Richter,CarlH.Smith,RolfWiehagen,andThomas
Zeugmann. Berlin, Heidelberg: Springer Berlin Heidelberg, 1998, pp. 112–126. ISBN:
978-3-540-49730-1(cit.onp.18).
[DGL05] François Denis, Rémi Gilleron, and Fabien Letouzey. “Learning From Positive and
UnlabeledExamples”.In:TheoreticalComputerScience348.1(2005).AlgorithmicLearn-
ingTheory(ALT2000),pp.70–83. ISSN:0304-3975. DOI:https://doi.org/10.1016/
j.tcs.2005.09.007. URL: https://www.sciencedirect.com/science/article/
pii/S0304397505005256(cit.onp.18).
78[DGT+18] ConstantinosDaskalakis,ThemisGouleakis,ChistosTzamos,andManolisZampetakis.
“Efficient Statistics, in High Dimensions, from Truncated Samples”. In: 2018 IEEE
59thAnnualSymposiumonFoundationsofComputerScience(FOCS).2018,pp.639–649.
DOI:10.1109/FOCS.2018.00067(cit.onp.19).
[DGT+19] Constantinos Daskalakis, Themis Gouleakis, Christos Tzamos, and Manolis Zam-
petakis. “Computationally and Statistically Efficient Truncated Regression”. In: Pro-
ceedings of the Thirty-Second Conference on Learning Theory. Ed. by Alina Beygelzimer
and Daniel Hsu. Vol. 99. Proceedings of Machine Learning Research. PMLR, June
2019, pp. 955–960. URL: https://proceedings.mlr.press/v99/daskalakis19a.
html(cit.onp.19).
[DKP+24] Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, and Nikos Zarifis. “Statistical
Query Lower Bounds for Learning Truncated Gaussians”. In: Proceedings of Thirty
SeventhConferenceonLearningTheory.Ed.byShipraAgrawalandAaronRoth.Vol.247.
Proceedings of Machine Learning Research. PMLR, 30 Jun–03 Jul 2024, pp. 1336–
1363. URL: https://proceedings.mlr.press/v247/diakonikolas24b.html (cit. on
p.19).
[DKT+21] Constantinos Daskalakis, Vasilis Kontonis, Christos Tzamos, and Emmanouil Zam-
petakis.“AStatisticalTaylorTheoremandExtrapolationofTruncatedDensities”.In:
Proceedings of Thirty Fourth Conference on Learning Theory. Ed. by Mikhail Belkin and
Samory Kpotufe. Vol. 134. Proceedings of Machine Learning Research. PMLR, Aug.
2021,pp.1395–1398. URL:https://proceedings.mlr.press/v134/daskalakis21a.
html(cit.onp.19).
[DLN+24] AnindyaDe,HuanLi,ShivamNadimpalli,andRoccoA.Servedio.“DetectingLow-
Degree Truncation”. In: Proceedings of the 56th Annual ACM Symposium on Theory of
Computing.STOC2024.Vancouver,BC,Canada:AssociationforComputingMachin-
ery,2024,pp.1027–1038.ISBN:9798400703836.DOI:10.1145/3618260.3649633.URL:
https://doi.org/10.1145/3618260.3649633(cit.onp.19).
[DNS23] Anindya De, Shivam Nadimpalli, and Rocco A. Servedio. “Testing Convex Trunca-
tion”.In:Proceedingsofthe2023AnnualACM-SIAMSymposiumonDiscreteAlgorithms
(SODA).2023,pp.4050–4082.DOI:10.1137/1.9781611977554.ch155.eprint:https:
//epubs.siam.org/doi/pdf/10.1137/1.9781611977554.ch155. URL: https:
//epubs.siam.org/doi/abs/10.1137/1.9781611977554.ch155(cit.onp.19).
[DP09] D.P.DubhashiandA.Panconesi.ConcentrationofMeasurefortheAnalysisofRandom-
izedAlgorithms.CambridgeUniversityPress,2009.ISBN:9781139480994.URL:https:
//books.google.com/books?id=UUohAwAAQBAJ(cit.onp.40).
[EEG+24] BenjaminL.Edelman,EzraEdelman,SurbhiGoel,EranMalach,andNikolaosTsilivis.
The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains. 2024.
arXiv: 2402.11004 [cs.LG]. URL: https://arxiv.org/abs/2402.11004 (cit. on
p.17).
79[EGZ20] Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. “How Can Self-Attention Networks
Recognize Dyck-n Languages?” In: Findings of the Association for Computational Lin-
guistics:EMNLP2020.Ed.byTrevorCohn,YulanHe,andYangLiu.Online:Associ-
ation for Computational Linguistics, Nov. 2020, pp. 4301–4306. DOI: 10.18653/v1/
2020.findings-emnlp.384. URL: https://aclanthology.org/2020.findings-
emnlp.384(cit.onp.17).
[Eld11] Ronen Eldan. “A Polynomial Number of Random Points Does Not Determine the
VolumeofaConvexBody”.In:Discrete&ComputationalGeometry46.1(2011),pp.29–
47. DOI: 10.1007/s00454-011-9328-x. URL: https://doi.org/10.1007/s00454-
011-9328-x(cit.onp.19).
[Elm90] JeffreyL.Elman.“FindingStructureinTime”.In:CognitiveScience14.2(1990),pp.179–
211. DOI: https://doi.org/10.1207/s15516709cog1402\_1. eprint: https://
onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1. URL: https:
//onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1(cit.onp.17).
[FJK96] A. Frieze, M. Jerrum, and R. Kannan. “Learning Linear Transformations”. In: Pro-
ceedingsof37thConferenceonFoundationsofComputerScience.1996,pp.359–368. DOI:
10.1109/SFCS.1996.548495(cit.onp.19).
[FKT20] Dimitris Fotakis, Alkis Kalavasis, and Christos Tzamos. “Efficient parameter esti-
mationoftruncatedbooleanproductdistributions”.In:Conferenceonlearningtheory.
PMLR.2020,pp.1586–1600. URL:https://doi.org/10.1007/s00453-022-00961-9
(cit.onp.19).
[FSW+24] Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and
Yulia Tsvetkov. “Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via
Multi-LLMCollaboration”.In:Proceedingsofthe62ndAnnualMeetingoftheAssociation
forComputationalLinguistics(Volume1:LongPapers).Ed.byLun-WeiKu,AndreMar-
tins, and Vivek Srikumar. Bangkok, Thailand: Association for Computational Lin-
guistics, Aug. 2024, pp. 14664–14690. URL: https://aclanthology.org/2024.acl-
long.786(cit.onp.1).
[GLL+24] Xinyan Guan, Yanjiang Liu, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le
Sun. “Mitigating Large Language Model Hallucinations via Autonomous Knowl-
edge Graph-Based Retrofitting”. In: Proceedings of the AAAI Conference on Artificial
Intelligence. Vol. 38. 16. 2024, pp. 18126–18134. URL: https://doi.org/10.1609/
aaai.v38i16.29770(cit.onp.16).
[Gol16] Yoav Goldberg. “A Primer on Neural Network Models for Natural Language Pro-
cessing”.In:JournalofArtificialIntelligenceResearch57(2016),pp.345–420.DOI:https:
//doi.org/10.1613/jair.4992(cit.onp.1).
[Gol67] E.MarkGold.“LanguageIdentificationintheLimit”.In:InformationandControl10.5
(1967), pp. 447–474. ISSN: 0019-9958. DOI: https://doi.org/10.1016/S0019-
9958(67)91165-5. URL: https://www.sciencedirect.com/science/article/
pii/S0019995867911655 (cit. on pp. 1, 3, 6–9, 12, 14, 17–21, 24, 28, 30, 33, 36, 60, 68,
70,93,96,97).
80[GPM+20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. “Generative Adversarial net-
works”. In: Commun. ACM 63.11 (Oct. 2020), pp. 139–144. ISSN: 0001-0782. DOI: 10.
1145/3422622.URL:https://doi.org/10.1145/3422622(cit.onpp.1,3,5).
[GS01] F.A. Gers and E. Schmidhuber. “LSTM Recurrent Networks Learn Simple Context-
FreeandContext-SensitiveLanguages”.In:IEEETransactionsonNeuralNetworks12.6
(2001),pp.1333–1340.DOI:10.1109/72.963769(cit.onp.17).
[GZA+23] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del
Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli
Saarikivi,AdilSalim,ShitalShah,HarkiratSinghBehl,XinWang,SébastienBubeck,
RonenEldan,AdamTaumanKalai,YinTatLee,andYuanzhiLi.TextbooksAreAllYou
Need. 2023. arXiv: 2306.11644 [cs.CL]. URL: https://arxiv.org/abs/2306.11644
(cit.onp.1).
[Hah20] Michael Hahn. “Theoretical Limitations of Self-Attention in Neural Sequence Mod-
els”.In:TransactionsoftheAssociationforComputationalLinguistics8(Jan.2020),pp.156–
171. ISSN: 2307-387X. DOI: 10.1162/tacl_a_00306. eprint: https://direct.mit.
edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00306/1923102/tacl\_a\_00306.
pdf. URL:https://doi.org/10.1162/tacl%5C_a%5C_00306(cit.onp.17).
[HBD+20] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi.“TheCuriousCase
ofNeuralTextDegeneration”.In:InternationalConferenceonLearningRepresentations.
2020.URL:https://openreview.net/forum?id=rygGQyrFvH(cit.onp.27).
[HCS+22] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved
Problems in ML Safety. 2022. arXiv: 2109.13916 [cs.LG]. URL: https://arxiv.org/
abs/2109.13916(cit.onp.1).
[HG23] Michael Hahn and Navin Goyal. A Theory of Emergent In-Context Learning as Implicit
StructureInduction.2023.arXiv:2303.07971[cs.CL].URL:https://arxiv.org/abs/
2303.07971(cit.onp.17).
[HHG+20] John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher D. Man-
ning. “RNNs Can Generate Bounded Hierarchical Languages With Optimal Mem-
ory”. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP). Ed. by Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu.
Online: Association for Computational Linguistics, Nov. 2020, pp. 1978–2010. DOI:
10.18653/v1/2020.emnlp-main.156. URL: https://aclanthology.org/2020.
emnlp-main.156(cit.onp.17).
[HKK+18] Steve Hanneke, Adam Tauman Kalai, Gautam Kamath, and Christos Tzamos. “Ac-
tivelyAvoidingNonsenseinGenerativeModels”.In:Proceedingsofthe31stConference
OnLearningTheory.Ed.bySébastienBubeck,VianneyPerchet,andPhilippeRigollet.
Vol. 75. Proceedings of Machine Learning Research. PMLR, June 2018, pp. 209–227.
URL:https://proceedings.mlr.press/v75/hanneke18a.html(cit.onp.1).
81[HKM+22] SteveHanneke,AminKarbasi,ShayMoran,andGrigorisVelegkas.“UniversalRates
for Interactive Learning”. In: Advances in Neural Information Processing Systems. Ed.
byAliceH.Oh,AlekhAgarwal,DanielleBelgrave,andKyunghyunCho.2022.URL:
https://openreview.net/forum?id=dTTKMy00PTJ(cit.onp.16).
[HMZ23] SteveHanneke,ShayMoran,andQianZhang.“UniversalRatesforMulticlassLearn-
ing”.In:ProceedingsofThirtySixthConferenceonLearningTheory.Ed.byGergelyNeu
and Lorenzo Rosasco. Vol. 195. Proceedings of Machine Learning Research. PMLR,
July2023,pp.5615–5681.URL:https://proceedings.mlr.press/v195/hanneke23a.
html(cit.onpp.16,26).
[HS97] Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In: Neural
Computation 9.8 (Nov. 1997), pp. 1735–1780. ISSN: 0899-7667. DOI: 10.1162/neco.
1997.9.8.1735. eprint: https://direct.mit.edu/neco/article-pdf/9/8/1735/
813796/neco.1997.9.8.1735.pdf. URL:https://doi.org/10.1162/neco.1997.9.
8.1735(cit.onp.1).
[HYM+23] LeiHuang,WeijiangYu,WeitaoMa,WeihongZhong,ZhangyinFeng,HaotianWang,
Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A Survey
on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open
Questions. 2023. arXiv: 2311.05232 [cs.CL]. URL: https://arxiv.org/abs/2311.
05232(cit.onp.1).
[JLF+23] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye
Jin Bang, Andrea Madotto, and Pascale Fung. “Survey of Hallucination in Natural
Language Generation”. In: ACM Comput. Surv. 55.12 (Mar. 2023). ISSN: 0360-0300.
DOI:10.1145/3571730.URL:https://doi.org/10.1145/3571730(cit.onp.1).
[JSR+24] AlbertQ.Jiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,
ChrisBamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,Flo-
rian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard
Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril,
Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of Experts. 2024.
arXiv: 2401.04088 [cs.LG]. URL: https://arxiv.org/abs/2401.04088 (cit. on
p.29).
[KKM+23] Alkis Kalavasis, Amin Karbasi, Shay Moran, and Grigoris Velegkas. “Statistical In-
distinguishability of Learning Algorithms”. In: Proceedings of the 40th International
ConferenceonMachineLearning.Ed.byAndreasKrause,EmmaBrunskill,Kyunghyun
Cho,BarbaraEngelhardt,SivanSabato,andJonathanScarlett.Vol.202.Proceedings
of Machine Learning Research. PMLR, July 2023, pp. 15586–15622. URL: https://
proceedings.mlr.press/v202/kalavasis23a.html(cit.onp.18).
[KM24] Jon Kleinberg and Sendhil Mullainathan. “Language Generation in the Limit”. In:
AdvancesinNeuralInformationProcessingSystems.Vol.37.2024.URL:https://arxiv.
org/abs/2404.06757 (cit. on pp. 1–4, 6–8, 11, 12, 14–17, 20, 22–24, 26, 28, 30, 32, 47,
51–55,57,59,67,91,92).
82[Kni24] Will Knight. Inside the Creation of DBRX, the World’s Most Powerful Open Source AI
Model | WIRED. Mar. 2024. URL: https://www.wired.com/story/dbrx-inside-
the-creation-of-the-worlds-most-powerful-open-source-ai-model/ (cit. on
p.29).
[KTZ19] Vasilis Kontonis, Christos Tzamos, and Manolis Zampetakis. “Efficient Truncated
StatisticswithUnknownTruncation”.In:2019IEEE60thAnnualSymposiumonFoun-
dationsofComputerScience(FOCS)(2019),pp.1578–1595.URL:https://api.semanticscholar.
org/CorpusID:199442432(cit.onp.19).
[KV24] Adam Tauman Kalai and Santosh S. Vempala. “Calibrated Language Models Must
Hallucinate”. In: Proceedings of the 56th Annual ACM Symposium on Theory of Com-
puting. STOC 2024. Vancouver, BC, Canada: Association for Computing Machin-
ery, 2024, pp. 160–171. ISBN: 9798400703836. DOI: 10.1145/3618260.3649777. URL:
https://doi.org/10.1145/3618260.3649777(cit.onpp.1,2,14,16).
[KVK22] Alkis Kalavasis, Grigoris Velegkas, and Amin Karbasi. “Multiclass Learnability Be-
yond the PAC Framework: Universal Rates and Partial Concept Classes”. In: Ad-
vances in Neural Information Processing Systems. Ed. by S. Koyejo, S. Mohamed, A.
Agarwal, D. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., 2022,
pp. 20809–20822. URL: https://proceedings.neurips.cc/paper_files/paper/
2022/file/82f0dae85424eb743017c90380e7ab9b-Paper-Conference.pdf (cit. on
pp.16,26).
[KWT+24] Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamil-
iar Finetuning Examples Control How Language Models Hallucinate. 2024. arXiv: 2403.
05612[cs.LG].URL:https://arxiv.org/abs/2403.05612(cit.onp.1).
[LAG+23] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
“Transformers Learn Shortcuts to Automata”. In: The Eleventh International Confer-
ence on Learning Representations. 2023. URL: https://openreview.net/forum?id=
De4FYqjFueZ(cit.onp.17).
[LBH15] YannLeCun,YoshuaBengio,andGeoffreyHinton.“DeepLearning”.In:Nature521.7553
(2015), pp. 436–444. DOI: 10.1038/nature14539. URL: https://doi.org/10.1038/
nature14539(cit.onp.1).
[Lit88] Nick Littlestone. “Learning Quickly When Irrelevant Attributes Abound: A New
Linear-Threshold Algorithm”. In: Machine Learning 2.4 (1988), pp. 285–318. DOI: 10.
1007/BF00116827. URL:https://doi.org/10.1007/BF00116827(cit.onpp.9,17,18,
96,97).
[LMZ24] JaneH.Lee,AnayMehrotra,andManolisZampetakis.“EfficientStatisticsWithUn-
knownTruncation,PolynomialTimeAlgorithms,BeyondGaussians”.In:2024IEEE
65thAnnualSymposiumonFoundationsofComputerScience(FOCS).2024.URL:https:
//arxiv.org/abs/2410.01656(cit.onp.19).
[LRT24] Jiaxun Li, Vinod Raman, and Ambuj Tewari. Generation Through the Lens of Learning
Theory.2024.arXiv:2410.13714[cs.LG].URL:https://arxiv.org/abs/2410.13714
(cit.onp.17).
83[Lu23] ZhouLu.Non-uniformOnlineLearning:TowardsUnderstandingInduction.2023.arXiv:
2312.00170[cs.LG].URL:https://arxiv.org/abs/2312.00170(cit.onpp.17,18).
[Man53] Benoit Mandelbrot. “An Informational Theory of the Statistical Structure of Lan-
guage”. In: Communication theory 84 (1953), pp. 486–502. URL: http://pdodds.w3.
uvm.edu/research/papers/others/1953/mandelbrot1953a.pdf(cit.onp.1).
[Mer19] William Merrill. “Sequential Neural Networks as Automata”. In: Proceedings of the
Workshop on Deep Learning and Formal Languages: Building Bridges. Ed. by Jason Eis-
ner, Matthias Gallé, Jeffrey Heinz, Ariadna Quattoni, and Guillaume Rabusseau.
Florence: Association for Computational Linguistics, Aug. 2019, pp. 1–13. DOI: 10.
18653/v1/W19-3901.URL:https://aclanthology.org/W19-3901(cit.onp.17).
[MIB+24] KyleMahowald,AnnaA.Ivanova,IdanA.Blank,NancyKanwisher,JoshuaB.Tenen-
baum, and Evelina Fedorenko. “Dissociating Language and Thought in Large Lan-
guage Models”. In: Trends in Cognitive Sciences 28.6 (2024), pp. 517–540. ISSN: 1364-
6613. DOI: https://doi.org/10.1016/j.tics.2024.01.011. URL: https://www.
sciencedirect.com/science/article/pii/S1364661324000275(cit.onp.1).
[MKB+10] TomasMikolov,MartinKarafiát,LukasBurget,JanCernocky`,andSanjeevKhudan-
pur. “Recurrent neural network based language model”. In: Interspeech. Vol. 2. 3.
Makuhari. 2010, pp. 1045–1048. URL: https://www.fit.vut.cz/research/group/
speech/public/publi/2010/mikolov_interspeech2010_IS100722.pdf(cit.onp.1).
[MM23] MaryantheMalliarisandShayMoran.TheUnstableFormulaTheoremRevisitedviaAl-
gorithms. 2023. arXiv: 2212.05050 [math.LO]. URL: https://arxiv.org/abs/2212.
05050(cit.onpp.12,18).
[MS23] William Merrill and Ashish Sabharwal. “The Parallelism Tradeoff: Limitations of
Log-Precision Transformers”. In: Transactions of the Association for Computational Lin-
guistics11(2023),pp.531–545.DOI:10.1162/tacl_a_00562.URL:https://aclanthology.
org/2023.tacl-1.31(cit.onp.17).
[MSS23] Shay Moran, Hilla Schefler, and Jonathan Shafer. “The Bayesian Stability Zoo”. In:
Advances in Neural Information Processing Systems. Ed. by A. Oh, T. Naumann, A.
Globerson,K.Saenko,M.Hardt,andS.Levine.Vol.36.CurranAssociates,Inc.,2023,
pp. 61725–61746. URL: https://proceedings.neurips.cc/paper_files/paper/
2023/file/c2586b71fd150fb56952e253a9c551cc-Paper-Conference.pdf (cit. on
p.18).
[Nat87] Balaubramaniam Kausik Natarajan. “On Learning Boolean Functions”. In: Proceed-
ingsoftheNineteenthAnnualACMSymposiumonTheoryofComputing.STOC’87.New
York, New York, USA: Association for Computing Machinery, 1987, pp. 296–304.
ISBN: 0897912217. DOI: 10.1145/28395.28427. URL: https://doi.org/10.1145/
28395.28427(cit.onp.18).
[OAA+24] OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,
FlorenciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,Shyamal
Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu,
HaimingBao,MohammadBavarian,JeffBelgum,IrwanBello,JakeBerdine,Gabriel
84Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine
Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin
Button,TrevorCai,RosieCampbell,AndrewCann,BrittanyCarey,ChelseaCarlson,
RoryCarmichael,BrookeChan,CheChang,FotisChantzis,DerekChen,SullyChen,
Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung
WonChung,DaveCummings,JeremiahCurrier,YunxingDai,CoryDecareaux,Thomas
Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fe-
dus,NikoFelix,SimónPosadaFishman,JustonForte,IsabellaFulford,LeoGao,Elie
Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-
Lopes,JonathanGordon,MorganGrafstein,ScottGray,RyanGreene,JoshuaGross,
Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He,
Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter
Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga,
Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin,
Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,
Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kil-
patrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie
Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris
Konstantinidis,KyleKosic,GretchenKrueger,VishalKuo,MichaelLampe,IkaiLan,
Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly
Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna
Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca
Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Chris-
tine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan
Morikawa,DanielMossing,TongMu,MiraMurati,OlegMurk,DavidMély,Ashvin
Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-
woo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo,
AshleyPantuliano,GiambattistaParascandolo,JoelParish,EmyParparita,AlexPas-
sos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
MichaelPetrov,HenriquePondedeOliveiraPinto,Michael,Pokorny,MichellePokrass,
Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
Puri,AlecRadford,JackRae,AdityaRamesh,CameronRaymond,FrancisReal,Kendra
Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted
Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman,DanielSelsam,KylaSheppard,TokiSherbakov,JessicaShieh,SarahShoker,
Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina
Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Pet-
roski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B.
Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick
Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
ChelseaVoss,CarrollWainwright,JustinJayWang,AlvinWang,BenWang,Jonathan
Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian
85Weng,MattWiethoff,DaveWillner,ClemensWinter,SamuelWolrich,HannahWong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo,
Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin
Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret
Zoph.GPT-4TechnicalReport.2024.arXiv:2303.08774[cs.CL].URL:https://arxiv.
org/abs/2303.08774(cit.onp.6).
[Pit89] LeonardPitt.“ProbabilisticInductiveInference”.In:J.ACM36.2(Apr.1989),pp.383–
433. ISSN: 0004-5411. DOI:10.1145/62044.62053. URL:https://doi.org/10.1145/
62044.62053(cit.onp.17).
[Ple21] Orestis Plevrakis. “Learning from Censored and Dependent Data: The case of Lin-
ear Dynamics”. In: Proceedings of Thirty Fourth Conference on Learning Theory. Ed. by
MikhailBelkinandSamoryKpotufe.Vol.134.ProceedingsofMachineLearningRe-
search. PMLR, Aug. 2021, pp. 3771–3787. URL: https://proceedings.mlr.press/
v134/plevrakis21a.html(cit.onp.19).
[PNP24] Binghui Peng, Srini Narayanan, and Christos Papadimitriou. On Limitations of the
Transformer Architecture. 2024. arXiv: 2402.08164 [stat.ML]. URL: https://arxiv.
org/abs/2402.08164(cit.onp.16).
[RG09] Luis Rademacher and Navin Goyal. “Learning Convex Bodies is Hard”. In: COLT.
2009. URL:http://www.cs.mcgill.ca/~colt2009/papers/030.pdf#page=1(cit.on
p.19).
[RHW86] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. “Learning Inter-
nal Representations by Error Propagation”. In: Parallel Distributed Processing: Explo-
rations in the Microstructure of Cognition, Vol. 1: Foundations. Cambridge, MA, USA:
MIT Press, 1986, pp. 318–362. ISBN: 026268053X. DOI: https://ieeexplore.ieee.
org/document/6302929(cit.onp.1).
[SAN96] Jenny R Saffran, Richard N Aslin, and Elissa L Newport. “Statistical Learning by 8-
Month-Old Infants”. In: Science 274.5294 (1996), pp. 1926–1928. DOI: https://doi.
org/10.1126/science.274.5294.1926(cit.onp.1).
[Sat23] Adam Satariano. “E.U. Agrees on Landmark Artificial Intelligence Rules”. In: The
NewYorkTimes8(2023). URL:https://www.nytimes.com/2023/12/08/technology/
eu-ai-act-regulation.html(cit.onp.1).
[Sch97] Dale Schuurmans. “Characterizing Rational versus Exponential Learning Curves”.
In: Journal of Computer and System Sciences 55.1 (1997), pp. 140–160. ISSN: 0022-0000.
DOI:https://doi.org/10.1006/jcss.1997.1505.URL:https://www.sciencedirect.
com/science/article/pii/S0022000097915051(cit.onpp.4,24).
[SDD+23] TimoSchick,JaneDwivedi-Yu,RobertoDessi,RobertaRaileanu,MariaLomeli,Eric
Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. “Toolformer:
Language Models Can Teach Themselves to Use Tools”. In: Advances in Neural In-
formation Processing Systems. Ed. by A. Oh, T. Naumann, A. Globerson, K. Saenko,
M.Hardt,andS.Levine.Vol.36.CurranAssociates,Inc.,2023,pp.68539–68551.URL:
https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-
Paper-Conference.pdf(cit.onp.29).
86[Sha51a] ClaudeE.Shannon.“PredictionandEntropyofPrintedEnglish”.In:TheBellSystem
TechnicalJournal30.1(1951),pp.50–64. DOI:10.1002/j.1538-7305.1951.tb01366.x
(cit.onp.1).
[Sha51b] Claude E Shannon. “The Redundancy of English”. In: Cybernetics; Transactions of the
7th Conference, New York: Josiah Macy, Jr. Foundation. 1951, pp. 248–272. URL: https:
//jontalle.web.engr.illinois.edu/uploads/537.F18/Papers/Shannon50b.pdf
(cit.onp.1).
[Shi90] Takeshi Shinohara. “Inductive Inference From Positive Data Is Powerful”. In: Pro-
ceedings of the Third Annual Workshop on Computational Learning Theory. COLT ’90.
Rochester, New York, USA: Morgan Kaufmann Publishers Inc., 1990, pp. 97–110.
ISBN: 1558601465. URL: https://api.lib.kyushu-u.ac.jp/opac_download_
md/3127/rifis-tr-20.pdf(cit.onp.18).
[SHT23] Clayton Sanford, Daniel J Hsu, and Matus Telgarsky. “Representational Strengths
and Limitations of Transformers”. In: Advances in Neural Information Processing Sys-
tems. Ed. by A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine.
Vol.36.CurranAssociates,Inc.,2023,pp.36677–36707. URL:https://proceedings.
neurips.cc/paper_files/paper/2023/file/73bf692447f174984f30499ec9b20e04-
Paper-Conference.pdf(cit.onp.17).
[Sip12] MichaelSipser.IntroductiontotheTheoryofComputation.IntroductiontotheTheoryof
Computation. Cengage Learning, 2012. ISBN: 9781133187813. URL: https://books.
google.com/books?id=4J1ZMAEACAAJ(cit.onpp.5,14,20,27).
[SK23] AdamSatarianoandCeciliaKang.“HowNationsAreLosingaGlobalRacetoTackle
A.I.’s Harms.” In: The New York Times (Digital Edition) (2023), NA–NA. URL: https:
//www.nytimes.com/2023/12/06/technology/ai-regulation-policies.html
(cit.onp.1).
[Soa99] R.I. Soare. Recursively Enumerable Sets and Degrees: A Study of Computable Functions
and Computably Generated Sets. Perspectives in Mathematical Logic. Springer Berlin
Heidelberg, 1999. ISBN: 9783540152996. URL: https://books.google.com/books?
id=9I7Pl00LU5gC(cit.onpp.5,14).
[Sol64] R.J.Solomonoff.“AFormalTheoryofInductiveInference.PartI”.In:Informationand
Control7.1(1964),pp.1–22.ISSN:0019-9958.DOI:https://doi.org/10.1016/S0019-
9958(64)90223-2. URL: https://www.sciencedirect.com/science/article/pii/
S0019995864902232(cit.onp.17).
[SSA18] Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. “How Good Is My
GAN?” In: Proceedings of the European conference on computer vision (ECCV). 2018,
pp. 213–229. URL: https://openaccess.thecvf.com/content_ECCV_2018/html/
Konstantin_Shmelkov_How_good_is_ECCV_2018_paper.html(cit.onp.1).
[SVL14] IlyaSutskever,OriolVinyals,andQuocV.Le.SequencetoSequenceLearningwithNeu-
ral Networks. 2014. arXiv: 1409.3215 [cs.CL]. URL: https://arxiv.org/abs/1409.
3215(cit.onp.1).
87[Tea24] The Mosaic Research Team. Introducing DBRX: A New State-of-the-Art Open LLM |
Databricks Blog. https://www.databricks.com/blog/introducing-dbrx-new-
state-art-open-llm. Mar. 2024. URL: https://www.databricks.com/blog/
introducing-dbrx-new-state-art-open-llm(cit.onp.29).
[TLI+23] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,
Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Au-
relien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA:
OpenandEfficientFoundationLanguageModels.2023.arXiv:2302.13971[cs.CL].URL:
https://arxiv.org/abs/2302.13971(cit.onpp.1,6).
[Tur50] Alan M. Turing. “Computing Machinery and Intelligence”. In: Mind LIX.236 (1950),
pp. 433–460. DOI: 10.1093/MIND/LIX.236.433. URL: https://doi.org/10.1093/
mind/LIX.236.433(cit.onp.1).
[Val84] Leslie G Valiant. “A Theory of the Learnable”. In: Commun. ACM 27.11 (Nov. 1984),
pp. 1134–1142. ISSN: 0001-0782. DOI: 10.1145/1968.1972. URL: https://doi.org/
10.1145/1968.1972(cit.onp.18).
[Vap13] VladimirVapnik.TheNatureofStatisticalLearningTheory.Springerscience&business
media,2013.URL:https://doi.org/10.1007/978-1-4757-3264-1(cit.onp.18).
[VL23] TomVieringandMarcoLoog.“TheShapeofLearningCurves:AReview”.In:IEEE
Transactions on Pattern Analysis and Machine Intelligence 45.6 (2023), pp. 7799–7819.
DOI:10.1109/TPAMI.2022.3220744(cit.onp.24).
[VSP+17] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN
Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention is All you Need”. In: Ad-
vances in Neural Information Processing Systems. Ed. by I. Guyon, U. Von Luxburg, S.
Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett.Vol.30.CurranAs-
sociates, Inc., 2017. URL: https://proceedings.neurips.cc/paper_files/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf(cit.onp.1).
[WM23] Karen Weise and Cade Metz. “When A.I. Chatbots Hallucinate”. In: The New York
Times9(2023),pp.610–23. URL:https://www.nytimes.com/2023/05/01/business/
ai-chatbots-hallucination.html(cit.onp.1).
[WWS+22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter brian, Fei
Xia,EdChi,QuocVLe,andDennyZhou.“Chain-of-ThoughtPromptingElicitsRea-
soninginLargeLanguageModels”.In:AdvancesinNeuralInformationProcessingSys-
tems. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh.
Vol.35.CurranAssociates,Inc.,2022,pp.24824–24837. URL:https://proceedings.
neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-
Paper-Conference.pdf(cit.onp.1).
[XJK24] Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is Inevitable: An Innate
Limitation of Large Language Models. 2024. arXiv: 2401.11817 [cs.CL]. URL: https:
//arxiv.org/abs/2401.11817(cit.onp.16).
88[XRL+22] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. “An Explana-
tion of In-context Learning as Implicit Bayesian Inference”. In: International Confer-
ence on Learning Representations. 2022. URL: https://openreview.net/forum?id=
RdJVFCHjUMI(cit.onp.17).
[YPP+21] ShunyuYao,BinghuiPeng,ChristosPapadimitriou,andKarthikNarasimhan.“Self-
Attention Networks Can Process Bounded Hierarchical Languages”. In: Proceedings
of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
Ed.byChengqingZong,FeiXia,WenjieLi,andRobertoNavigli.Online:Association
for Computational Linguistics, Aug. 2021, pp. 3770–3785. DOI: 10.18653/v1/2021.
acl-long.292.URL:https://aclanthology.org/2021.acl-long.292(cit.onp.17).
[ZL95] Thomas Zeugmann and Steffen Lange. “A Guided Tour Across the Boundaries of
Learning Recursive Languages”. In: Algorithmic Learning for Knowledge-Based Sys-
tems: GOSLER Final Report. Ed. by Klaus P. Jantke and Steffen Lange. Berlin, Hei-
delberg:SpringerBerlinHeidelberg,1995,pp.190–258.ISBN:978-3-540-44737-5.DOI:
10.1007/3-540-60217-8_12. URL: https://doi.org/10.1007/3-540-60217-8_12
(cit.onp.18).
[ZLC+23] YueZhang,YafuLi,LeyangCui,DengCai,LemaoLiu,TingchenFu,XintingHuang,
Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda
Shi,andShumingShi.Siren’sSongintheAIOcean:ASurveyonHallucinationinLarge
Language Models. 2023. arXiv: 2309.01219 [cs.CL]. URL: https://arxiv.org/abs/
2309.01219(cit.onp.1).
89A Further Discussion on Decidability of MOP(·)
In this section, we present a generating algorithm G = (G ) for which MOP is undecidable. The
n
correspondinggeneratingalgorithmisstaticinthesensethatG = G = .... Foreach t, G isthe
1 2 t
followingrandomizedTuringmachine.
Input: None
Setup: Theinternalrandomtapecontainssymbolsfrom{0,1,2}
Description:
1. Readbitsr = r r ... fromtherandomtapeuntilthefirst2isfoundonthetape
1 2
2. Let b = 1 if r is of the form ⟨M⟩w where ⟨M⟩ is a valid representation of a Turing
machineandwisany(possibly)emptystring
3. If r = 1 then: Execute Moninputwandreturn⟨M⟩w1if Mhalts
4. Else: return0
PropositionA.1. MOP(·)isundecidablefortheaboveTuringmachine.
Proof. Theproofisasimplereductiontothehaltingproblem,whichiswell-knowntobeundecid-
able. Toseethis,observethattodecidewhether M haltsoninputw,itsufficestocheckif⟨M⟩w1
isinthesupportoftheabovemachine.
B Results With Subset Oracles
Inthissection,wedesignalgorithmsthathaveaccesstoasubsetoraclethat,givenindicesiand j,
answers whether “L ⊆ L ?” We give two algorithms (1) an algorithm that identifies in the limit
i j
without requiring tell-tale oracles and (2) a best-of-both-words algorithm that generates consis-
tentlyandachievesbreadthwheneverpossible.
B.1 IdentificationintheLimitWithoutTell-TaleOracleviaSubsetOracles
L
Angluin [Ang80] showed that a collection of (recursive) languages is identifiable in the limit if
andonlyifeachL ∈ L hasafinite“tell-tale”setT that,roughlyspeaking,enablesonetoeliminate
i i
L if it is not the target. If one has access to an oracle that, given an index i, outputs the tell-tale
i
T,thenonecanidentifyL
usinganalgorithmbyAngluin[Ang80]. Ournextresultshowsthat,if
i
one has access to queries of the form “L ⊆ L ?”, then one can identify any identifiable language
i j
collectionwithoutaccesstoatell-taleoracle.
Theorem B.1. Let S be an oracle that, given indices i and j, outputs Yes if L ⊆ L and outputs No
i j
otherwise. Fix any identifiable collection of languages L = {L ,L ,...}. There is an algorithm A that
1 2
given,anenumerationofatargetlanguageK ∈ L(foranyK)andaccesstoS,identifiesK inthelimit.
Importantly,A doesnotneedtobeprovidedthetell-taleofK oranyotherlanguageinL.
90Interestingly, the algorithm in Theorem B.1 is the same as an algorithm proposed by Kleinberg
andMullainathan[KM24]: thealgorithmdefinesacertainnotionofcriticallanguagesandselects
the last critical language, say L . Naturally, since we want to identify the language, instead of
j
outputtinganelementof L thealgorithmwillguesstheindex. Thisalgorithmidentifies K inthe
j
sense that after some finite time t∗, it outputs an index z such that L = K. The specific index
z
z outputted, however, may change infinitely often. This can also be avoided by using the post-
processingroutineinLemma5.4.
Proof. ThealgorithmtoidentifyK issimple:
Fort ∈ {1,2,...}do:
1. Observeelement x andletS bethesetofallelementsobservedsofar
t t
2. ConstructaversionspaceV t consistingofalllanguagesin L ≤t consistentwithS t,i.e.,
(cid:8) (cid:9)
V : L : 1 ≤ j ≤ t, L ⊇ S .
t j j t
# Define an language L ∈ V to be critical if L is the smallest-index language in V or L is a
i t i t i
subsetofalllanguagesprecedingitinV,i.e.,if L ⊆ L forall1 ≤ j < i
t i j
3. ConstructthesetC ⊆ V ofallcriticallanguages
t t
4. returniwhere L isthelargest-indexedlanguageinthesetofcriticallanguagesC
i t
Let i be the first index such that K = L . The above algorithm outputs an index z with L = K
i z
when K is the last element of the set of critical languages C . This condition is implied by the
t
followingtwoconditions.
(A) K isinthesetofcriticallanguagesC .
t
(B) Allthelanguages L with j > ithatareincludedinC satisfy L = K.
j t j
Result(4.3)ofKleinbergandMullainathan[KM24]showsthatthereisafinitetimet afterwhich
a
Condition (A) holds. We will show that there is also a finite time t after which Condition (B)
b
holds. This shows that, for any t ≥ max{t ,t }, A outputs an index z(t) such L = K. As
a b z(t)
mentioned before one can convert this into an algorithm that outputs a fixed index (after some
finitetime)usingthepost-processingroutineinLemma5.4.
Condition(B)holdsafterafinitetime. SinceL isidentifiable,itmustsatisfyAngluin’stell-tale
criteria(Definition10)and,hence,K = L hasafinitetell-talesetT. (RecallthatT isnotknownto
i i i
us;ourproofwillnotneedthis.) Fixany j > iandanytimet ≥ t (afterwhichK isguaranteedto
a
be a critical language). If L is a critical language, then by the definition of critical languages and
j
thefactthatKiscritical(P1) L ⊆ Kand(P2) L ∈ V. Further,if L ⊊ K,then L cannotcontainthe
j j t j j
tell-tale T of K (by the properties of tell-tales; Definition 10). Therefore, if S (the set of samples
i t
seenuntilstept)contains T,then L cannotbeintheversionspaceasotherwiseitwouldneedto
i j
containT. Itfollowsthat,ifS ⊇ T andt ≥ t ,theneitherP2willbeviolatedorP1willimplythat
i t i a
L = K. Finally, since T is finite and, hence, there is a finite time t′ when all elements of T have
j i b i
beenobservedand,theresultfollowsbylettingt :=
max(cid:8)
t
,t′(cid:9)
.
b a b
91B.2 Best-Of-BothWorlds: GeneratingWithBreadthWhenPossible
Considerthevariantofthealgorithmintheprevioussectionwhichinsteadofoutputtingtheindex
of the last critical language outputs an unseen sample from the language. This is precisely the
algorithmofKleinbergandMullainathan[KM24].20 KleinbergandMullainathan[KM24]showed
thatthisalgorithmconsistentlygeneratesinthelimit. Animmediatecorollaryoftheidentification
resultinthelastsectionisthatthisalgorithmalsoachievesbreadthforanyidentifiablecollection
L
.
Corollary B.2. Let S be an oracle that, given indices i and j, outputs Yes if L ⊆ L and outputs No
i j
otherwise. FixanycollectionofcountablymanylanguagesL = {L ,L ,...}.
1 2
There is a generating algorithm G that given, an enumeration of a target language K ∈ L (for any
K) and access to S, consistently generates from K in the limit. Moreover, whenever L is identifiable, this
algorithmgeneratesconsistentlywithbreadthinthelimit.
Importantly,G doesnotneedtobeprovidedthetell-taleofK oranyotherlanguageinL.
L
Finally,werecallthatforanynon-identifiablecollection ,generationwithbreadthisimpossible
wheneveraMOP(G)canbeimplemented.
C FurtherResultsforConsistentGenerationWithApproximateBreadth
ResultintheLimit
In this section, we study another notion of generation with approximate breadth. Informally,
requiresthatthegeneratingalgorithmisconsistentandputszeromassonlyonfinitelymanypoints
ofthetargetlanguageK. Theformaldefinitionisasfollows.
Definition 21 (Generation with Approximate Breadth). A generating algorithm G = (G ) is said to
n
generate with approximate breadth for a collection L = {L ,L ,...} if, for any K ∈ L and enumeration
1 2
x ,x ,... of K, there is an n ≥ 1, such after seeing n ≥ n elements x ,...,x , supp(G ) ⊆ K and
1 2 0 0 1 n n
|K\supp(G )|isfinite.
n
Observethattheaboveisaweakeningofgenerationwithbreadth–sincethegeneratingalgorithm
G isallowedtomisselementsinthetargetlanguageinfinitelyoften. Thisweakeningofgeneration
with breadth turns out to be incomparable to the notion of unambiguous generation studied in
Section3.3. Toseethis, observethat(1)ontheonehand, G cansatisfytheabovedefinitionwhile
generating with breadth from a language L that is a strict subset of K and (2) on the other hand,
unambiguous generation allows the generator to generate samples outside of K infinitely often,
whichisbarredbytheabovedefinition.
Ourmainresultinthissectionisasfollows.
20Note that since we only output an element from the last critical language and not its index, we do not need to
performthepost-processingusedintheprevioussection,sothisisKleinbergandMullainathan[KM24]’salgorithm.
92Theorem C.1 (Impossibility of Approximate Generation in the Limit). For every non-identifiable
collection of countably many languages L, no generating algorithm stable in the limit, for which MOP(·)
(Definitions 5 and 6) is decidable, can generate from L in the limit with approximate breadth according to
Definition21.
The proof of Theorem C.1, like the proof of Theorem 3.5 is also by a contradiction to the non-
L
identifiability of . The difference is that in this proof we also need to identify the finitely many
elementsofK “missed”byG.
Proof. RecallthatGold[Gol67]showedthatforanycollectionofcountablymanylanguages,there
isalwaysanalgorithmI that,givenapositiveandnegativeenumerationofthetarget,identifies
PN
itinthelimit. Now,towardacontradiction,supposethereisastablegeneratorforwhichMOP(G)
is decidable and it has the property described in Definition 21. We claim that using G and I ,
PN
L L
wecanconstructanidentifierfor (frompositiveexamples), whichcontradictsthefactthat is
non-identifiable.
FixanytargetlanguageK,itsenumerations ,s ,...,andthe(unknown)constantt∗,suchthat
1 2
after t∗ many iterations the algorithm achieves generation according to Definition 21. We claim
L
thatthefollowingalgorithmidentifies .
Input: AccesstoageneratorG forL that(1)that,inthelimit,becomesconsistentandsatisfies
|K\supp(G)| < ∞ and (2) for which MOP(G) is decidable, and access to the algorithm I
PN
L
thatidentifies inthelimitfromapositiveandnegativeenumerationofthetargetlanguage.
Description:
1. Foreach t ∈ N do:
(a) Observethet-thsamples andletS bethesetofsamplesseensofar
t t
(b) TrainthegeneratorG fromscratchoverthetsamplesinS
t
(c) Foreach1 ≤ i ≤ t,labelthei-thstring x inthedomainasy = MOP(G)(x )a
i i i
(d) Foreach1 ≤ i ≤ t,if x ∈ S andy = 0,sety = 1#tocorrectelementsmissedbyG
i t i i
(e) TrainI fromscratchonsamples x ,...,x withlabelsy ,...,y
PN 1 t 1 t
(f) outputtheindexguessedbyI andgotothenextiteration
PN
aHere,MOP(G)(x)istheanswertothemembershiporacleproblemforG giveninputx.
SinceMOP(G)isdecidable,theabovealgorithmcanbeimplementedusingaTuringmachine. We
claimthattheabovealgorithmidentifiesthetargetlanguage K afterafinitenumberofiterations.
Toformalizethis,fixanyenumerations ,s ,... ofthetargetlanguage. Sinceafterafinitetimet∗,G
1 2
becomesconsistent,stabilizes,andsatisfies|K\supp(G)| < ∞ ,afteriterationt∗,foranystring x,
MOP(G)(x)matches1{x ∈ K}exceptforthefinitelymanyelementsof M = K\supp(G ). Note
t
thatsinceG’ssupportstabilizes,Misindependentoftheiterationt ≥ t∗. Lett′bethetimewhenall
elementsof M appearintheenumerations ,s ,.... Observethatinalliterationst ≥ max{t∗,t′},
1 2
thelabelsinStep2(d)correctallthemismatchesbetweenMOP(G)(x)matches1{x ∈ K}Finally,
since I identifies in the limit, there is a finite t such that I identifies K once it is given
PN PN PN
labelsforthefirstt ≥ t examplesinthedomain. Combiningthiswiththepreviousinformation
PN
93implies that that I and, hence, our algorithm identifies K after max{t∗,t′,t } < ∞ iterations.
PN PN
This gives the desired contradiction, proving Theorem C.1. Note that the above identification
algorithmdoesnotneedtoknowanyoft∗,t′,andt .
PN
ResultintheStatisticalSetting
Ourapproachtogetthisresultfollowsthesamehigh-levelideawithTheorem3.6. Theerrorofa
generatingalgorithminthissetting,basedonDefinition21is
er(G ) = 1{supp(G ) ̸⊆ K or |K\supp(G )| = ∞} (12)
n n n
Theformalstatementisbelow.
Theorem C.2 (Impossibility of Approximate Generation). For every non-identifiable collection of
countably many languages L, no stable generating algorithm, for which MOP(·) (Definitions 5 and 6)
isdecidable,cangeneratefromLwithapproximatebreadthaccordingtoDefinition21,atanyrate.
Usingthetoolswedevelopedforthesettingofunambiguousgeneration,wecanshowthefollow-
ingresultwhichtransformsalearnerthatworksinthestatisticalsettingintoalearnerthatworks
intheonlinesetting. Westartwiththefollowinglemma.
LemmaC.3. Let R: N → R
≥0
bearatefunction,i.e.,lim n→∞R(n) = 0,letLbealanguagecollection,
and (G n: Xn → G)
n∈N
be generating algorithm for which MOP(·) is decidable and which satisfies the
followingtwoproperties:
• (G n)
n∈N
isastablegenerator(Definition7),and
•
foritsapproximategenerationerrorer(·)(Equation(12))itholdsthat,foreveryvaliddistributionP
withrespecttoLthereexistc,C > 0suchthatE X 1,...,Xn∼Pn [er(G n(X 1,...,X n))] ≤ C·R(c·n).
Then,foreveryvaliddistributionPwithrespecttoLitholdsthat
Pr [∃n∗ ∈ N : ∀n ≥ n∗ itholdsthater(G (X ,...,X )) = 0] = 1.
n 1 n
{Xi} i∈N∼P∞
ProofofLemmaC.3. Assume towards contradiction that there exists some valid P with respect to
L
sothat
Pr [∃n∗ ∈ N : ∀n ≥ n∗ itholdsthat {er(G (X ,...,X )) = 0}] = c′ < 1.
n 1 n
{Xi} i∈N∼P∞
Letusalsodenotec′′ := 1−c′.Noticethatc′′ > 0.SinceP isavaliddistributionwithrespecttoL ,
itissupportedoversomeK ∈ L ,sowehavethat,withprobability1,aninfinitei.i.d. drawfromP
isanenumerationofK (seeProposition5.2). LetuscallthiseventE .
1
Moreover,sinceG isastablegenerator(inanonlinesense),undertheeventE (i.e.,whenthe
n 1
samplesfromP formanenumerationof K),thereexistssomesmallestnumbert∗ := t∗(X ,...) ∈
1
N suchthatforalln ≥ t∗
supp(G n(X 1,...,X n)) = supp(G n+1(X 1,...,X n+1)) .
94Now, t∗ dependsonthespecificenumerationdrawnand,hence,thedistributionP inducesadis-
tributionovert∗.Further,notethatwithprobability1,t∗ < ∞ .Hence,Pr {Xi} i∈N∼P∞ [t∗(X 1,...) > n]
approaches0asn → ∞ . Inparticular,thereissomenumbern ∈ N suchthatforalln ≥ n
1 1
c′′
Pr [t∗(X ,...) > n] ≤ .
1
{Xi} i∈N∼P∞ 3
Moreover,sincethegeneratorachievesrate R(·)andlim n→∞R(n) = 0,itholdsthat
lim Pr [er(G (X ,...,X )) ̸= 0] = 0.
n→∞X 1,...,Xn∼Pn n 1 n
Thus,thereissomen ∈ N suchthat,foralln ≥ n
2 2
c′′
Pr [er(G (X ,...,X )) ̸= 0] ≤ .
n 1 n
X 1,...,Xn∼Pn 3
Letn
3
:= max{n 1,n 2}. Hence,takingaunionbound,weseethatwithprobabilityatleast1−2c′′/3
overthedrawof{X } itholdsthat
i i∈N
• er(G (X ,...,X )) = 0,and
n3 1 n3
• supp(G (X ,...,X )) = supp(G (X ,...,X )),foralln ≥ n .
n 1 n n3 1 n3 3
Bythedefinitionofer(·),foranyn,n′ ∈ N ,samples x ,...,x and x ,...,x itholdsthat
i 1 in j 1 j n′
(cid:0) (cid:0) (cid:1)(cid:1)
supp(G n(x i 1,...,x in)) = supp G n′ x j 1,...,x j n′ =⇒
(cid:0) (cid:0) (cid:1)(cid:1)
er(G (x ,...,x )) = er G x ,...,x
n i 1 in n j 1 j n′
Thesetwoconditionsimmediatelyimplythat,withprobabilityatleast1−2c′′/3 > c′,foralln ≥ n
3
itholdsthat
• er(G (X ,...,X )) = 0,and
n 1 n
• supp(G n+1(X 1,...,X n+1)) = supp(G n(X 1,...,X n)).
Hence,
Pr [∃n∗ ∈ N : ∀n ≥ n∗ itholdsthat {er(G (X ,...,X )) = 0}] > c′ ,
n 1 n
{Xi} i∈N∼P∞
whichgivesthedesiredcontradiction. Thisconcludestheproof.
Using the previous result, we derive the next statement regarding the conversion of a statistical
learnertoanonlineone.
LemmaC.4. Let R: N → R
≥0
bearatefunction,i.e.,lim n→∞R(n) = 0,letLbealanguagecollection,
and(G n: Xn → G) n∈NbegeneratingalgorithmforwhichMOPisdecidableandsatisfiesthefollowingtwo
properties:
• (G n)
n∈N
isastablegenerator(Definition7),and
•
foritsapproximategenerationerror(Equation(12))itholdsthat,foreveryvaliddistributionPwith
respecttoLthereexistc,C > 0suchthat E X 1,...,Xn∼Pn [er(G n(X 1,...,X n))] ≤ C·R(c·n).
95(cid:16) (cid:17)
Then, there is a randomized generating algorithm G′ : Xn →r G for which, for any target language
n n∈N
K ∈ LandeveryenumerationσofK,itholdsthat
• (G n′)
n∈N
isastablegenerator(Definition7),and
•
Pr(cid:2) ∃n∗ ∈ N : ∀n ≥ n∗ itholdsthater(cid:0) G′ (σ ,...,σ )(cid:1) = 0(cid:3) = 1,
n 1 n
wheretheprobabilityiswithrespecttotherandomnessofthealgorithm.
TheproofofLemmaC.4isidenticaltotheproofofLemma7.4,sincetheonlypropertyoftheerror
function that is needed is that once the algorithm has stabilized then its error also stabilizes. For
completeness,wegivethedetailsbelow.
ProofofLemmaC.4. Let K ∈ L be any target language and σ be any enumeration of K. Let P be
σ
P
the distribution defined in Definition 20. We know that, by definition, is valid with respect to
σ
L , since it is supported on K. Let (G n′) n∈N be a generator which, for every n ∈ N , runs G n on P σ.
In order to draw samples from P the generator G′ uses its internal randomness and the process
σ n
P L
described in Proposition 7.2. Since is a valid distribution with respect to , Lemma C.4 gives
σ
usthat
Pr [∃n∗ ∈ N : ∀n ≥ n∗ itholdsthat {er(G (X ,...,X )) = 0}] = 1.
n 1 n
{Xi} i∈N∼P∞
σ
Hence,thisimpliesthat
Pr(cid:2) ∃n∗ ∈ N : ∀n ≥ n∗ itholdsthat (cid:8) er(cid:0) G′ (σ ,...,σ )(cid:1) = 0(cid:9)(cid:3) = 1,
n 1 n
wheretheprobabilityistakenwithrespecttotheinternalrandomnessofthealgorithm. Moreover,
since(G n) n∈N isastablegeneratoritalsoholdsthat(G n′) n∈N isastablegenerator. Thisconcludes
theproof.
The proof of Theorem C.2 follows as a corollary of the previous result (Lemma C.4) and Theo-
remC.1.
ProofofTheoremC.2. LetL beacountablecollectionoflanguages. Assumethatsuchastablegen-
erating algorithm exists. Then, using the construction from Lemma C.4 to get a stable generator
that generates missing only finitely many elements in the limit, for every target language K ∈ L
and every enumeration σ of K, with probability 1. This contradicts the impossibility result from
TheoremC.1.
D Further Comparison With Online Learning
In this section, we provide some comparisons between the Gold-Angluin (GA) model [Gol67;
Ang79]andtheonlinegameofBousquetetal.[BHM+21](thatextendsthestandardonlinelearn-
ingmodelofLittlestone[Lit88]),whichatfirstsightsharealotofsimilarities. However,thereare
importantdifferencesbetweenthetwomodels,whichwebelieveareworthhighlighting.
96LetusfirstrecallthesettingofBousquetetal.[BHM+21],appropriatelyrephrasedtothecon-
text of our work. There is a domain X , a collection of languages L ⊆ {0,1}X , and two players,
the learner andthe adversary, play a game over aninfinite sequence of discrete rounds. In every
round t ∈ N , the adversary presents an example x ∈ X to the learner and the learner guesses
t
its label y . Subsequently, the adversary reveals the true label y to the learner. We say that the
(cid:98)t t
learner makes a mistake if y ̸= y . This can be thought of as the learner trying to guess whether
t (cid:98)t
theexamplebelongstothetargetlanguage. Theadversaryhastosatisfythefollowingconstraint.
For every round t ∈ N there must be some L ∈ L such that 1{x ∈ L } = y ,∀τ ≤ t. The goal
t τ t τ
of the learner is to make finitely many mispredictions and the goal of the adversary is to force
infinitelymanysuchmispredictions.
• IntheGAmodel,thegoalofthelearneristoidentifythetruelanguageinafinitenumberof
steps. In the online game of Bousquet et al. [BHM+21], the goal of the learner is to make a
finitenumberofmistakesinitspredictions. Moreover,intheGAmodel,thelearnerobserves
onlypositiveexampleswhileinthestandardonlinesetting,theadversarycanprovideboth
positiveandnegativeexamples.
• The set of languages identifiable in the limit in Gold’s model is characterized by Angluin’s
criterion (Definition 10). The collections that are online learnable with a finite number of
mistakes is characterized by the absence of infinite Littlestone trees [BHM+21]. If there is a
uniformboundonthenumberofmistakes,thenthiscorrespondstothestandardfiniteness
oftheLittlestonedimension[Lit88].
• In the GA model, the adversary fixes the true language in advance. In (realizable) online
learning, the only thing that matters is consistency of the hypothesis class on the given ex-
amples,i.e.,foreverysequenceofexamples,alongwiththeirlabels,thereshouldexistsome
hypothesisinthehypothesisclassthatperfectlylabelsthegivensequence(whichcanchange
as the online game progresses, see also Section 3 of Bousquet et al. [BHM+21]). This subtle
differenceleadstostarklydifferentlearninglandscapes.
• In the GA model, the adversary must include all elements of K in the enumeration (the
domainiscountable). InLittlestone’sonlinesetting, thereisnosuchrestriction(thefeature
spacecanevenbeuncountable).
• Another crucial difference is that the algorithm does not receive feedback about its guess
in the GA model. This is in contrast to the standard online setting where, at each round,
the learner gets feedback about its prediction. However, it is important to stress that the
incentivesoftheadversariesinGold’smodelandinthemodelofBousquetetal.[BHM+21]
aredifferent. IntheGAmodel,thegoalisnottomaximizethenumberofmistakesthatthe
learnerdoes,buttoessentiallynotallowthelearnertoidentifythetruetargetlanguage.
To further show separations between the online setting of Gold [Gol67] and the online game of
Bousquetetal.[BHM+21],wewillneedtwoimportantdefinitionscomingfromtheworkofBous-
quetetal.[BHM+21].
97Definition22(LittlestoneTree[BHM+21]). ALittlestonetreeforLisacompletebinarytreeofdepth
d ≤ ∞ whose internal nodes are labeled by X, and whose two edges connecting a node to its children are
labeled0and1,suchthateveryfinitepathemanatingfromtherootisconsistentwithalanguage L ∈ L.
Moreprecisely,aLittlestonetreeisacollection
{x : 0 ≤ k < d,u ∈ {0,1}k} ⊆ X
u
suchthatforeveryy ∈ {0,1}d andn < d,thereexistsL ∈
Lsothat1(cid:8)
x ∈
L(cid:9)
= y for0 ≤ k ≤ n.
y≤k k+1
WesaythatLhasaninfiniteLittlestonetreeifthereisaLittlestonetreeforLofdepthd = ∞.
N
Forinstance,thresholdsover donothaveaninfiniteLittlestonetree(yet,theyhaveLittlestone
trees of arbitrary length). On the other side, thresholds over the reals have an infinite Littlestone
tree. Giventhisdefinition,wecanshowaseparationbetweentheonlinesettingofBousquetetal.
[BHM+21]andtheGAmodel.
First,wenotethatalanguagecollectionisnotonlinelearnableintheonlinesettingofBousquet
et al. [BHM+21] if and only if it has an infinite Littlestone tree. We will show that there exists a
L
countable language collection over a countable domain that has an infinite Littlestone tree but
itisidentifiableinthelimitwithpositiveexamples.
Example 3 (Infinite Littlestone Tree but Identifiable with Positive Examples). Consider a count-
able domain X and fix an enumeration x∅,x 0,x 1,x 00,x 01,x 10,x 11,... of its elements. We use this
enumerationtocreatethenodesofaLittlestonetree,i.e. therootconsistsof x∅,itsleft,rightchild
isx ,x ,respectivelyetc. Then,foreachleveld ∈ N andanypathy ∈ {0,1}d wecreateafinitelan-
0 1
guage L withindexy,whoseelementsaretheelementsofX thatappearonthepathwithlabel1.
y
WeconsiderthelanguagecollectionL = {L : y ∈ {0,1}d,d ∈ N}.Thismeansthatforanyfinite
y
level d < ∞ , we add all the 2d languages in the collection (where for any path y, L contains the
y
elementsxthatarelabeledwith1inthepath). Hence,L containsallthefiniteprefixesofthepaths
L
of the infinite Littlestone tree. The language collection is countably infinite since the collection
ofallfinitepathsofthebinarytreeadmitsanenumeration. Byconstruction,thisclassinducesan
infiniteLittlestonetreeandhenceisnotonlinelearnableinthegameofBousquetetal.[BHM+21].
However, since it is a countable collection of finite languages, it is identifiable in the limit with
positiveexamplesintheGAmodel(seealsoSection3.4.3and8.3).
E Borel-Cantelli Lemmas
Inthissection, wepresenttwowell-knownresultsduetoBorelandCantelliwhichareusefulfor
ourderivations.
LemmaE.1(FirstBorel-CantelliLemma). Let{E } beasequenceofevents. If
n n∈N
∑ Pr[E ] < ∞ ,
n
n∈N
thentheprobabilitythatinfinitelymanyofthemoccuris0,thatis
(cid:20) (cid:21)
Pr limsupE = 0.
n
n→∞
98Thepreviousresulthasapartialconverse,whichwestatebelow.
LemmaE.2(SecondBorel-CantelliLemma). Let{E } beasequenceofindependentevents. If
n n∈N
∑ Pr[E ] = ∞ ,
n
n∈N
thentheprobabilitythatinfinitelymanyofthemoccuris1,thatis
(cid:20) (cid:21)
Pr limsupE = 1.
n
n→∞
Notice that, unlike the first Borel-Cantelli lemma, the second one requires that the events are
independent.
99