On the Surprising Effectiveness of Attention Transfer
for Vision Transformers
AlexanderC.Li‚àó YuandongTian BeidiChen
CarnegieMellonUniversity FAIR CarnegieMellonUniversity
DeepakPathak XinleiChen
CarnegieMellonUniversity FAIR
Abstract
Conventional wisdom suggests that pre-training Vision Transformers (ViT) im-
provesdownstreamperformancebylearningusefulrepresentations. Isthisactually
true? Weinvestigatethisquestionandfindthatthefeaturesandrepresentations
learnedduringpre-trainingarenotessential. Surprisingly,usingonlytheattention
patternsfrompre-training(i.e.,guidinghowinformationflowsbetweentokens)is
sufficientformodelstolearnhighqualityfeaturesfromscratchandachievecom-
parabledownstreamperformance. Weshowthisbyintroducingasimplemethod
calledattentiontransfer,whereonlytheattentionpatternsfromapre-trainedteacher
ViTaretransferredtoastudent,eitherbycopyingordistillingtheattentionmaps.
Sinceattentiontransferletsthestudentlearnitsownfeatures,ensemblingitwith
afine-tunedteacheralsofurtherimprovesaccuracyonImageNet. Wesystemat-
icallystudyvariousaspectsofourfindingsonthesufficiencyofattentionmaps,
includingdistributionshiftsettingswheretheyunderperformfine-tuning. Wehope
ourexplorationprovidesabetterunderstandingofwhatpre-trainingaccomplishes
andleadstoausefulalternativetothestandardpracticeoffine-tuning. Codeto
reproduceourresultsisathttps://github.com/alexlioralexli/attention-transfer.
1 Introduction
Pre-traininghasemergedasadominantparadigminmachinelearningandhassignificantlyimproved
performanceonavarietyoftasks[27,11,2,22]. Incomputervisioninparticular,self-supervised
representationlearningmethods[21,6,4,22]andweaklysupervisedmethods[40,45]haveenabled
learningfromlargeamountsofimages. Itiswidelyacceptedthatthesemethodsworkbecausethey
teachmodelsusefulfeaturesthatarerelevantfordownstreamtasks. Butisthisstoryactuallytrue?
Perhapsthereisanothercapabilitylearnedduringpre-trainingthatissufficienttoexplainitsbenefits.
Inthispaper,wepresentanalternativeexplanation: pre-trainingteachesthemodelhowinformation
shouldberoutedbetweentokens. WespecificallyfocusonVisionTransformers(ViT)[12],notonly
becausetheyarethemostpopulararchitectureforscaling,butalsobecauseTransformersexplicitly
decouplethisinformationflow. Inter-tokencommunicationissolelyfulfilledbyattention,whilethe
remainingbulkofcomputationareintra-tokenoperationsthatareappliedtoeachtokenindependently.
Incontrast,otherarchitecturessuchasConvNets[33,20]simultaneouslyexpandthereceptivefields
andextractthefeatures,makingitdifficulttoisolatetheeffectofinformationflow. Wehypothesize
thatthefeaturescomputedbytheintra-tokenoperationsarenotessentialtoexplainthebenefitsof
pre-training,andthatthepre-trainedattentionmapsaretypicallysufficientfordownstreamtasks.
Wetestourhypothesisbyintroducinganewsetofmethodscalledattentiontransfer. Concretely,
we treat a pre-trained ViT as the teacher and train a student model for downstream tasks while
‚àóWorkdoneduringaninternshipatFAIR.
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
voN
41
]GL.sc[
1v20790.1142:viXratransferring only the attention patterns from the teacher. In contrast to the common fine-tuning
paradigmoftransferringalltheweights(whichmixestheeffectoffeaturesandattentionmaps),only
theinter-tokenflowistransferred. Inthisway,thestudentmustlearnfeaturesfromscratch,while
isolatingthebenefitsoftheattentionmapslearnedduringpre-training.
Westudytwotypesofattentiontransfer. The
no transfer full transfer
firstisAttentionCopy,whichdirectly‚Äúcopy- (scratch) (fine-tune)
77.8%
and-pastes‚Äùtheattentionmaps. Thelearning
isfullydecoupled,asinter-tokencomputation Attention Copy 85.1
is entirely from the teacher, and the student 100.0%
onlylearnsintra-tokenpatternsroutedbythe Attention Distillation 85.7
teacher‚Äôsattentionmaps. Thisiswell-suited
83.0 85.7
asascientificprobe,butislesspracticalsince
bothnetworksneedtobeforwardedduringthe Figure1: Usingonlyattentionissufficientforfull
inference.ThesecondisAttentionDistillation, performance. Bycopyingtheattentionmaps(top)
wherethestudentsimplydistillsattentionpat- fromaMAE[22]pre-trainedViT-L[12],aViT-Lcan
ternsfromtheteacher,whoseattentionmaps reachatop-1accuracyof85.1onImageNet-1K[10]
arenolongerusedaftertraining. Thisisprac- ‚Äìrecovering77.8%ofthegapbetweennotransfer
tical,butalsohelpsidentifytheimportanceof (trainingfromscratch,83.0)andfulltransfer(fine-
theteacher‚Äôsinter-tokeninformationflow. tuning all the weights, 85.7). Distilling attention
maps (bottom) can even fully match MAE weight
While both attention transfer variants are
tuningwhileonlytransferringtheinter-tokenflow.
straightforward,wefindthemhighlyeffective.
Figure1illustratesthiswithaViT-L[12]pre-
trainedusingMaskedAutoencoding(MAE)[22]. Comparedtonotransfer(trainingfromscratch)
andfulltransfer(fine-tuningalltheMAEweights), AttentionCopycanclosemostofthegapin
performance,whereasAttentionDistillationcanmatchthefine-tuningaccuracyonImageNet-1K
classification[10]. Thisisachievedbyonlytransferringtheinter-tokenflowfromthesamemodel.
Furthermore,sinceattentiontransferrequiresthestudenttolearnfeaturesfromscratch,thosefeatures
aresignificantlydifferentfromtheteachers‚Äô(Figure5)andimproveImageNet-1Kaccuracyscoreto
86.3(+0.6)whenensembledwiththeteacher(Figure6).
Tosummarize,wemakethefollowingcontributions:
‚Ä¢ Detailedanalysisonthesufficiencyofattentionmaps. Wefindthatsolelyusingthepre-
trainedattentionpatternsistypicallysufficienttoachievethesamedownstreamaccuracyas
fine-tuningonImageNet-1K.Furthermore,weobservepracticalbenefits,asensemblingwith
attentiontransfersignificantlyimprovesImageNetperformance. Thiscallsintoquestion
thecommonly-believedstorythatpre-trainingisonlyaboutfeaturelearning. Whileour
main observation is robust w.r.t. different models and pre-training methods, we do find
settings where pre-trained features are indeed necessary to realize the full gains from
pre-training. Our bare-minimum solution for attention transfer is more affected by data
distribution shifts compared to weight tuning. Section 4 presents extensive analyses to
better understand the behaviors of attention transfer. They are i) partial transfer with a
subset of layers or heads; ii) variants of our method that transfer other attention-related
activations;andimportantly,iii)variouswaystoverifythatthestudentisnotjustre-learning
the teacher model. Section 5 systematically tests how well our findings apply across a
varietyofpre-trainingandfine-tuningdatasets,pre-trainingmethods,modelsizes,andtasks.
‚Ä¢ Attention transfer methods. We introduce Attention Copy and Attention Distillation,
whicharemethodstotrainaViTonadownstreamtaskwhileutilizingonlytheattention
mapsofapre-trainedteacherViT.Thesemethodshelpusunderstandtheroleofthefeatures
versustheattentionpatternslearnedduringpre-training. Withfurtherresearch,attention
transfercouldofferapotentialalternativetothedecade-longpracticeoffine-tuningpre-
trainedvisionmodels[16,12,22]. Nearlyallaspectsofthefine-tuningpipelinehavebeen
thoroughlyexamined,suggestingaprobablesaturationofrecipes. Weightsharingcanalso
face security risks (e.g., white-box attacks [17]). We hope our systematic examination
of attention transfer sheds new light on how to leverage pre-trained ViTs, and will help
establishthisapproachasaneffectivealternativewhenweighttransferislessapplicable.
2student teacher student teacher
MLP block MLP block MLP block MLP block
MSA block MSA block MSA block MSA block
copy distill
attention map √ó ùêª attention map √ó ùêª attention map √ó ùêª attention map √ó ùêª
√ó ùêø √ó ùêø √ó ùêø √ó ùêø
inputs inputs inputs inputs
Attention Copy Attention Distillation
Figure2: TwotypesofAttentiontransferforVisionTransformers. AttentionCopy(left): We
simply‚Äúcopy-and-paste‚Äùtheattentionmapsfromapre-trainedteachermodeltoarandomlyinitialized
studentone.Otherweightsofthestudentarethentrainedviasupervisedlearning.Thisfullydecouples
inter-tokenlearning(fromtheteacher)andintra-tokenlearning(inthestudent);butislesspractical.
Attention Distillation (right): The student computes its own attention maps, with an additional
cross-entropylosstodistillpatternsfromtheteacherduringtraining. Theteacherisnolongerused
duringinference. H: numberofheads;L: numberofTransformerlayers.
2 AttentionTransfer
2.1 AttentionPreliminaries
ToworkwithaVisionTransformer(ViT)[12],animageisfirst‚Äúpatchified‚ÄùintoN tokens. Their
intermediateactivationsarerepresentedasasequenceX=[x ,x ,¬∑¬∑¬∑ ,x ]‚ä§,x ‚ààRC,whereC is
1 2 N i
the embedding dimension. The self-attention [57] mechanism mainly introduces three learnable
parametersW ,W ,W ‚ààRC√óC/H (H isthenumberofheads). Q=XW isoftenreferredtoasthe
q k v q
queries,K=XW asthekeys,andV=XW asthevalues. Thentheattentionfunctionisdefinedas:
k v
f
=softmax(cid:0) QK‚ä§(cid:1)
V, (1)
attn
(cid:124) (cid:123)(cid:122) (cid:125)
attentionmap
wherethesoftmaxfunctioniscomputedperqueryfortheattentionmap. Attentionmapsdetermine
howthevaluesfromothertokensareaggregated,andwithmultipleheads,eachtokenusesmultiple
attentiondistributionswithinthesameMulti-headedSelf-Attention(MSA)block.
ForanL-layerTransformer,MSAblocksareinterleavedwithMLPblocks,andeachTransformer
layer contains one of each block type. Most operations are intra-token computations, which are
appliedindependentlytoeachtoken: valueandprojectionmatrices,normalizationlayers[1],and
MLPs. Theonlyinter-tokencomputationisapplyingtheattentionmapsoftmax(QK‚ä§),whichisthe
onlywayforinformationtoflowbetweentokens. Transformersareuniquebecausetheirinter-and
intra-tokencomputationsaredecoupled;however,therelativeimportanceofeachtypeofoperationis
notwellunderstood,andTransformersaretypicallytrainedbyjointlyfine-tuningalltheweights.
Deviating from the common practice of joint weight tuning, we propose two attention transfer
methodswiththegoalofexploringdecoupledtrainingforViTs,describednext.
2.2 AttentionCopy
Inthissetup,weutilizetwoseparatenetworks: apre-trainedteachernetworkthatonlydoesaforward
passtocomputeitsattentionmaps,andastudentnetworkthatdirectlycopiestheattentionmapsfrom
theteacherbutcomputesalloftheotheractivations. Thestudent‚Äôsweightsarerandomlyinitialized
and trained via back-propagation, while the teacher‚Äôs weights are kept frozen. This setting fully
isolatestheattentionmapsfromthefeaturesthattheyareappliedto,andthusisidealformeasuring
theutilityofpre-trainedattentionpatternswhenthestudentnetworklearnstoperformothertasks
(e.g.,imageclassification).
We call this method Attention Copy, as we ‚Äúcopy-and-paste‚Äù the attention maps from teacher to
student. Figure 2 (left) shows a diagram of this approach. Note that it requires forward passes
3throughboththeteacherandstudentnetworksduringtheinferencetime.Giventheextracomputation,
AttentionCopyisnotmeanttobeanentirelypracticalmethod. Wemitigatethisissuenext.
2.3 AttentionDistillation
InAttentionDistillation,theteachernetworkisonlyutilizedatthetrainingtime. Giveneachtraining
example,weforwardbothnetworksinparallel,withthestudentalsocomputingitsownattention
maps. Butbesidesthetask-drivenloss,wealsoenforceadistillationlossbetweenstudent‚Äôsattention
mapsandtheteacher‚Äôscounterpartsas(soft)targets. Formally,usingQ K ‚ä§ forthestudentand
s s
Q K ‚ä§fortheteacher,thelossisthendefinedas:
t t
(cid:104) (cid:105)
L =H softmax(Q K ‚ä§),softmax(Q K ‚ä§) , (2)
dist s s t t
whereHcomputesthecrossentropy. AstherecanbemultipleheadsandlayersinaTransformer,
wesimplysumupallthelossesfromwhereverattentiondistillationisapplied. Again,thestudentis
trainedviaback-propagation. Figure2(right)showsthediagramofAttentionDistillation.
Compared to Attention Copy, Attention Distillation is much more practical. After training, the
teacherisnolongerneeded,andthestudentcanbeusedasastandalonemodel. Comparedtotraining
ViTsfromscratch,theonlyadditionisthedistillationloss,meaningmostoftheoptimization(e.g.,
learningrate,momentum)andregularization(e.g.,weightdecay,dropoutrate[50])hyperparameters
canfollowthescratchrecipewithminimaladjustments. ItdoesintroduceanewhyperparameterŒª,
whichweightsthedistillationlossandbalancesitwiththetaskloss.
AttentionDistillationcanbeviewedasaformofgeneralizedknowledgedistillation,butithasseveral
key differences from the design proposed by Hinton et al. [26]. Attention Distillation trains the
studenttomatchtheteacher‚Äôsintermediateattentionmaps,notthefinalteacheroutput. Thisgivesthe
flexibilityofdistillingfrommodelstrainedonanytask,notjustmodelstrainedonthesamefinaltask.
Thispropertyiswell-suitedfortoday‚Äôs‚Äúpre-trainandtransfer‚Äùparadigm,wherethepre-trainingtask
(e.g.,reconstruction)andthedownstreamtask(e.g.,classification)areusuallydifferent. However,
AttentionDistillationdoesaddtheconstraintthatthearchitectureneedstocomputeattentionmaps.
Weleaveexperimentingonthisideaforotherarchitecturesasfuturework.
Overall,whilefancierdesignscanbeusedforbothAttentionCopyandAttentionDistillation,we
choosetokeepthemsimpleforcleanerassessmentsoftheireffectiveness.
2.4 ConnectiontoTransformerTrainingDynamics
Ourinvestigationisalsolinkedtorecentattemptstotheoreticallyunderstandthetrainingdynamics
ofTransformers. Specifically,theinter-tokenflowencodedinthepre-trainedattentionmapscan
beregardedasadiscoveredlatenthierarchyfromthedataset. Self-attentioncanquicklycapture
frequently co-occurring token pairs [31, 52]. However, more occasional co-occurrences need to
beexplainedbythetop-levelhierarchy,ratherthandirectlylearnedinthelowerlevels[53]. This
is due to many potential spurious correlations [30], especially in the over-parameterized setting.
Transferringattentionmapsfromatrainedteacherreducesthesespuriousinter-tokencorrelations,so
thestudentcanfocusonintra-tokenlearning(i.e.,computingusefulfeatures).
3 MainResults
AsfeaturedinFigure1,attentiontransferishighlyeffectivedespiteitssimplicity. Specifically,we
demonstratethiswithaViT-L[12]pre-trainedwithMaskedAutoencoding(MAE)[22]forImageNet-
1Kclassification[10]. NotethatthisisthesignatureresultthatestablishedMAE‚Äôseffectivenessfor
pre-training: comparedtoaViT-Ltrainedfromscratch(withanaccuracyscoreof83.0),fine-tuning
theMAEpre-trainedonthesamedatasetresultsinasignificantimprovementto85.7.2
Forattentiontransfer, weusethesamepre-trainedMAEmodelasourteacher, andsincescratch
trainingcanbeviewedasnotransfer,andfine-tuningweightstransfersalltheweights,theabove
tworesultsserveasnaturallowerandupperboundsfortheeffectivenessofourattentiontransfer
methods. Wemaketwoobservations(fromTable1andFigure1).
2Ifnototherwisespecified,ourresultsarebasedonourfaithfulreimplementationoftheofficialcodeinJAX.
4method acc.
scratch 83.0 transfertarget acc.
fine-tune 85.7 Q 85.6
attn.copy 85.1 K 84.4
attn.copyfromfine-tuned 85.6 V 84.4
attn.distill 85.7 Q,K 85.1
ensembleattn.distill+fine-tune 86.3 Table2: Transferotherattentionactivations.
Table 1: Main results. We show that the pre- Wetestcopyingalternativeattentionactivations
trainedattentionpatternsaresufficienttomatch otherthantheattentionmap‚Äìsoftmax(QK‚ä§).
fine-tuning accuracy on ImageNet. Attention All alternatives do better than training from
Copyclosesmostofthegap,andAttentionDis- scratch,andtransferringqueriesQactuallydoes
tillationachievesthesameaccuracy. betterthantransferringtheattentionmap.
AttentionCopycanlargelyclosethegapbetweenscratchtrainingandfullweightfine-tuning
Wereportanaccuracyof85.1withAttentionCopy. Thishaslargelyclosedthegapbetweenscratch
andfullweighttuning(tobeprecise,77.8%ofthe2.7percentagepointgap). Thisissurprising,since
theteacher‚Äôsattentionmapsarefrozenafterpre-trainingforadifferenttask(imagereconstruction),
andthestudentmustlearneverythingelse(theintra-tokenoperations)completelyfromscratch.
Asanotherupper-bound,wealsoexperimentedwithAttentionCopyfromthefine-tunedmodel. This
reachesanaccuracyscoreof85.6‚Äìalmostmatchingtheteacher‚Äôsperformance(85.7),suggesting
thatadaptingattentionmapstothespecifictaskisstillhelpful,butnotcrucial,especiallyasMAE
pre-trainingisperformedonthesamedatadistribution.
AttentionDistillationcanmatchfine-tuningperformance
Evenmoresurprisingly,wefindAttentionDistillationcanachieve85.7‚Äìonparwithfine-tuningthe
ViT-LweightsfromMAE.SinceAttentionDistillationandweighttuningbothresultinthesame-
sizedmodel,whichrequiresthesamecomputebudgetforinference,thisresultsuggestsAttention
Distillationcanbeaneffectivedrop-inreplacementforweighttuningwhenthelatterislessapplicable
(e.g.,ifweightsharingposessecurityrisks,wecaninsteadsendtheteacher‚Äôsattentionmaps).
Wehypothesizethatdistillationisbetterthancopybecausethestudentcanchoosehowcloselyit
matchestheteacherattentionmaps,tobettersuitthetaskobjective. Thisisalsosupportedbythe
85.6accuracyofcopyingfromfine-tunedMAE,whichhasthecorrecttask-specificattentionmaps.
4 Analysis
Next, we provide extensive analyses to better understand the effectiveness of attention transfer.
Broadlyspeaking,theexplorationsaredrivenbythefollowingtwoquestions:
(i) Howimportantaredifferentactivations,layers,andheads? (Section4.1)
(ii) Isattentiontransferre-learningeverythingfromtheteacher? (Section4.2)
4.1 VariantsofAttentionTransfer
Westudyfourvariantsofattentiontransfer. WeuseAttentionCopywithinthissection,sinceitisa
fully-decoupledsettingwell-suitedforscientificanalysis.
TransferasubsetofQ,K andV. Anaturalalternativetotransferringattentionmapsistotransfer
differentactivationsthatcomewithself-attention(Eq.1),namelyqueriesQ,keysK,orvaluesV.
Withoutlossofgenerality,ifwetransfertheteacher‚ÄôsQ,thestudentwillcomputeitsownK andV
andusethemnormally. NotethattransferringbothQandK isequivalenttotransferringthemap
softmax(QK‚ä§). Table2showsthattransferringQworkssurprisinglywell,andisactuallybetter
thantransferringtheattentionmap.
WesuggestthatcopyingQgivesthemodeltheflexibilitytodeviatefromtheteacherattentionmaps
anduseattentionpatternsthatarebettersuitedforthedownstreamtask. Thisissupportedbythefact
585.0 83.6 84.4 84.8 84.9 8 85 4. .1 9 8 85 5. .1 0 85.1 85.0 84.8 85.1
84.4
82.5 81.9 83.4 82.5 81.2
82.1
80.0 80.5 81.2 80.0
from bottom
77.5 76.1 from top 77.5 76.1 77.2
0 3 6 9 12 15 18 21 24 0 4 8 12 16
number of layers transferred number of heads transferred
Figure3: Copyasubsetoflayers. Bydefault, Figure 4: Copy a subset of heads. The pre-
all24ViT-Llayersaretransferred. Hereweonly trainedViT-Lhas16headsineachMSAblock.
transferasubset,andfind: morelayersalways Bydefault,allofthemaretransferred. Herewe
helps; and attention maps from top layers are onlytransferasubset,andfindmoreheadshelps
morebeneficialthanthosefrombottomlayers. ingeneral,butperformancesaturatesat12heads.
thatcopyingQandAttentionCopyfromthefine-tunedmodelbothachievethesameaccuracyof
85.6. AppendixB.3divesdeeperintothishypothesisandfindsthattheattentionmapsforcopying
Qaresimilartotheteacher‚Äôsbutlessconstrainedthantheyareinothertransfermethods. While
moreinvestigationcouldbedoneinfuturework,ourfindingssuggeststhatthequeriesQaremore
importantthanthekeys,whichisconsistentwithpreviousfindingsintextsequencemodelingwhere
thenumberofkeysandvaluesperlayercanbesignificantlyreduced[49].
Finally,wetestwhetherdistillingQcouldoutperformfullAttention method acc.
Distillation. However,Table3showsthatQdistillationdoessignif- attn.distill 85.7
icantlyworse. Wehypothesizethatthisisbecauseitisharderfor Qdistill 81.3
thestudenttolearnusefulkeysK whileQisstillbeinglearned,and
becauseAttentionDistillationalreadyhastheflexibilitytoadjustits Table3: AttentionDistillation
attentionmapstosuitthedownstreamtask. outperformsQdistillation.
Partialtransfer: layers. WenextchangethenumberofTransformerlayerstransferred,aimingto
identifywhichlayersaremorevaluablefromtheteacher. Thebaselinetransfersallthelayers. In
Figure3,wetrytransferringattentionmapsonlyfromthefirstorlastlayers. Fortheremaininglayers,
thestudentlearnstocomputeitsownattentionmaps.
Wemakeseveralobservations: i)Wefindtransferringmorelayersisalwaysmorehelpful. Thisisa
bitsurprising,asonemayexpectattentionpatternsoptimizedforMAE‚Äôspatch-wisereconstruction
tasktobesub-optimalforahigh-levelrecognitiontasklikeclassification. ii)Wefindtransferring
thelaterattentionmapsisgenerallybetter. Infact,performanceroughlysaturateswhentransferring
thelast15attentionmapsoutofatotalof24. ThisindicatesthatViTsarecapableoflearninggood
low-levelrepresentations,aslongastheyaretoldhowtocombinethesefeaturesintohigher-level
ones;butnotviceversa. ThisreinforcesthetheoryfromTianetal.[53]thatguidanceontop-level
hierarchyismoreimportant,astherearemanymorepossibilities,andattentiontransfercanreduce
possiblespuriouscorrelationsinthelowerlevels.
Partialtransfer: heads. Finally,weswitchbacktotransferringallthelayers,butchangethenumber
ofheadscopiedfromeachMSAblock. Specifically,insteadofcopyingtheattentionmapfromevery
head,wecanselectivelychoosetouseasubsetoftheteacher‚Äôsheads. Thestudentcanthencompute
its own attention patterns for the unused heads. Figure 4 shows the effect of transferring fewer
headsforeachlayer. Performanceimprovesaswedoattentiontransferwithmoreheads,though
performancealmostsaturatesat12outof16heads. Notethatwesimplyfollowana√Øveselection
strategy and use the first set of heads; more advanced strategies based on diversity or activation
magnitudecanpotentiallyimprovetherobustnessaswereducethenumberofheads.
4.2 AreWeRe-LearningtheTeacher?
Sinceattentiontransferprovidesasignificantamountofinformationfromtheteacher(ViT-Lattention
mapshaveabout10Mactivationstotalperimage,seeAppendixA.1fordetailedcalculations),anatu-
ralquestioniswhetherthestudentperformswellbecauseitsimplyrelearnsthesamerepresentations
astheteacher. Wethoroughlytestthishypothesisondifferentaspectsofthestudentmodel.
61.0
pre-train attn. copy
0.8 scratch attn. distill. Figure5:CKArepresentationsimilaritytothe
fine-tunedmodel. WeuseCKA[32]tomeasure
0.6 thelayer-wisesimilaritybetweenrepresentations
learnedindifferentmodelsagainstthefine-tuned
0.4
MAEmodel. Highermeansmoresimilar. We
0.2 findthatattentiontransfermethodsarequitedis-
simlartothefine-tunedmodel,withroughlythe
0.0
sameCKAasanindependentscratchmodel.
0 5 10 15 20
layer index
fine-tune 85.7 Figure 6: Ensemble accuracy with the fine-
before ensemble
tunedmodel. Weplottheaccuracyofensem-
ensemble with fine-tuned
attn. copy 1.1 86.2 bling our attention transfer models and a fine-
tunedMAE.Weusethistomeasuremodelpre-
attn. distill. 0.7 86.3 dictionsimilaritywiththefine-tunedmodel. The
ensembleyieldsnotableaccuracygainsoverthe
scratch 2.7 85.7 basemodel,reachingasmuchas86.3,indicating
thattheattentiontransferbasedmodelsarenot
83 84 85 86
accuracy particularlycorrelatedwiththefine-tunedmodel.
Representation similarity. One way that the student can reproduce the teacher is by learning
thesameintermediatefeatures. WemeasurethisusingCenteredKernelAlignment(CKA)[32],a
similaritymeasureforrepresentationsthathasbeenshowntobeeffectiveevenfornetworkstrained
withdifferentinitializationsorarchitectures. CKAisalayer-wisecomparisonthatrangesfrom0
(completely dissimilar) to 1 (identical) and is invariant to orthogonal linear transformations and
isotropicscalingofthefeatures. Figure5showstheCKAbetweenourfine-tunedmodelandour
attentiontransfermethods. Wealsoshowthepre-trainedmodelandaViT-Ltrainedfromscratchfor
reference. WecomputeCKAwithrespecttothefine-tunedmodel,eventhoughwecopyordistill
fromthepre-trainedMAEmodel,sincethefeatureschangesignificantlyduringfine-tuningtobecome
moretask-specific. Overall,AttentionCopyandAttentionDistillationarebothquitedissimilartothe
fine-tunedMAEmodel,followingthesamesimilaritytrendasthescratchmodel. Oursanitycheck
passes,asCKAshowsthatpre-trainedandfine-tunedMAEhaveverysimilarrepresentationsinthe
earlylayers. Thisisexpectedsincefine-tuningwithlayer-wiselearningratedecay[8]meansthe
earliestlayerschangeverylittleduringfine-tuning.
Predictionsimilarityandensembling. OurCKAanalysismaynotcatchsomesimilarityofthe
intermediaterepresentations,asCKAdoesnotdetectallformsofthesameinformation(e.g.,invertible
nonlineartransforms)[32]. Sinceintermediaterepresentationsmaynottellthefullstory,wealso
examinethesimilarityofthenetworkoutputs. Wemeasurethisusingnetworkensembling: given
softmaxpredictionsp fromthefine-tunedmodelandp fromanothermodel,wetesttheaccuracy
ft other
oftheiraverage(p +p )/2. Themoreindependentthemodelpredictionsare,thehighertheir
ft other
ensembleaccuracyis. Figure6comparesaccuracybeforeandafterensemblingwiththefine-tuned
model. Attentiontransferisdissimilarenoughtoachievehighensembleaccuracy,andensembling
AttentionDistillationwithafine-tunedMAEachieves86.3,+0.6overthefine-tunedMAEmodel.
Finally,AppendixB.4visualizestheattentionmapslearnedbyAttentionDistillationandshowsthat
theymatchfordistilledattentionblocksbutaredrasticallydifferentforlayersthatarenotdistilled.
5 GeneralizationandLimitations
Inthissection,wetesthowwellourfindingsonattentiontransferapplyacrossavarietyofpre-training
andfine-tuningdatasets,pre-trainingmethods,modelsizes,andtasks.
5.1 Pre-trainingandfine-tuningdatasets
Sofar,wehavefocusedonaMAEmodelpre-trainedandevaluatedonImageNet-1K.Whathappens
ifwepre-trainorevaluateondifferentdatasets? Wefirsttestthisbypre-trainingMAEViT-Lmodels
7
denut-enif
htiw
AKCpre-trainingdata tune copy eval.data scratch tune copy distill
ImageNet 85.7 85.1 iNat2017 49.7 75.9 69.1 69.3
ImageNet-22K 85.5 84.4 iNat2018 64.3 79.9 71.8 74.1
COCO 85.2 83.1 iNat2019 66.2 83.8 77.9 80.0
Table4: Differentpre-trainingdatasets. We Table5: Long-tailrecognitiononiNaturalist,
pre-trainMAEonmoredatasets,andtheneither with ImageNet-1K pre-trained MAE. We tune
fine-tuneordocopyforImageNet-1Kclassifica- weights or do attention transfer on iNaturalist,
tion.Attentiontransferworkswellwhenthedata andweagainfindattentiontransferisworsethan
distributionstaysstable,butitseffectivenessis tuningweightswhenthepre-trainingdatasetis
morenegativelyaffectedbydistributionshifts. differentfromthedownstreamdataset.
out-of-distributionevaluation scratch tune copy distill
ImageNet-A[24] 32.0 56.5 48.9 54.3
ImageNet-R[23] 51.9 59.6 57.5 56.8
ImageNet-S[60] 38.0 45.2 43.1 42.9
ImageNet-V2[47] 72.4 76.4 75.5 75.9
Table6: Out-of-distributionrobustness. Wetaketwomodelsthatachievethesameaccuracyon
ImageNet-1K(fine-tunedanddistilled),andevaluatethemonasuiteofdistributionshifts. Attention
Distillation does well when the distribution is close (e.g., on ImageNet-V2), but loses the mild
‚Äúeffectiverobustness‚Äùthatfine-tunedMAEhasbeenfoundtohave[14].
ontwonewdatasets: ImageNet-22K[10]andCOCO[37]. Thesehavesubstantiallydifferentdataset
bias[54]fromImageNet,acrossaxeslikeappearance,classbalance,anddiversity. Table4showsthat
theresultingMAEmodelsmaintainrelativelygoodperformancewhenfine-tuningonImageNet-1K,
withamaximumdropofatmost0.5. However,AttentionCopyaccuracydropsmore,losingasmuch
as 2.1. We see a similar phenomenon in Table 5 where we use a MAE pre-trained on ImageNet
andtransfertotheiNaturalistdatasets[56]. Again,whenthepre-trainingdatasetdoesnotmatch
thetransferdataset,AttentionCopyaccuracydropssignificantly. Wehypothesizethatthefrozen
teacher‚Äôsattentionmapsareill-suitedforthefine-tuningdataset,whichlimitstheperformance.
5.2 Out-of-distributionrobustness
Onenotableaspectofastandardfine-tunedMAEmodelisthatitshowsslight‚Äúeffectiverobustness,‚Äù
i.e., it achieves slightly better out-of-distribution (OOD) accuracy than expected based on its in-
distribution(ID)accuracy[14]. WetestwhetherAttentionDistillation, whichachievesthesame
IDaccuracy,hasthesamebenefitsOOD.Table6showsthatAttentionDistillationstilldoesquite
well,buthasloweraccuracythanfine-tunedMAEonall4distributionshiftswetried. Theseresults
indicatethattheattentionmapsdonotaccountforthefullrobustnessbenefits,andthatthefeatures
learnedbyMAEduringpre-trainingarehelpfulOODeveniftheyarenotID.
5.3 Pre-trainingmethods
WehavesofarfocusedonMAE,areconstruction-basedpre-trainingmethod. Wenowcheckwhether
attentiontransferstillworksiftheteacherhasbeenpre-trainedwithadifferentalgorithm.Specifically,
wetestMoCo-v3[7],aself-supervisedcontrastivelearningapproach,andFLIP[36],whichdoes
image-text contrastive learning. Table 7 shows that Attention Copy still achieves most of the
performancebenefitsforeachpre-trainingmethod. Impressively,ViT-Lisevenabletoreach86.6by
justtransferringattentionmapsfromFLIP.Thisconfirmsthatlearningtheproperattentionpatterns
isindeedasignificantbottleneckduringlearning. NotethattheFLIPmodelweusedispre-trainedon
LAION-2B[48],yetitseffectivenessislessaffectedbydistributionshiftstoImageNet-1K.
5.4 Modelsize
Wetestwhetherattentiontransferworksacrossmodelsizes. Forallexperimentssofar,wehaveused
ViT-L;here,wetryAttentionCopyfromasmaller(ViT-B)andlarger(ViT-H)model,bothpre-trained
withMAE.Table8showsthatAttentionCopycontinuestoimprovewithscale,evenreaching86.1%
8pre-trainingmethod tune copy distill
MAE[22] 85.7 85.1 85.7 model scratch tune copy distill
MoCo-v3[7] 84.0 82.5 83.3 ViT-B 82.5 83.6 82.0 83.4
FLIP[36] 87.4 86.6 86.1 ViT-L 83.0 85.7 85.1 85.7
DINO‚Ä†[4] 83.2 82.3 82.8 ViT-H 83.0 86.9 86.1 86.3
none 83.0 72.7 76.3
Table 8: Different model size with MAE pre-
Table7:Differentpre-trainingmethods.Atten- trainedonImageNet-1K.Similartoweighttun-
tiontransferworksforallpre-trainingmethods, ing,theclassificationaccuracyofattentiontrans-
even reaching 86.6 with a FLIP teacher. As a ferscaleswellaswevarythemodelsize,while
sanitycheck,transferringfromarandomlyini- scratchtrainingsaturates.
tializedViTsignificantlyhurts. ‚Ä†DINOisViT-B.
metric scratch tune distill
APbox 39.1 46.3(+7.2) 43.6(+4.5)
APmask 34.6 40.6(+6.0) 38.7(+4.1)
Table9: ObjectdetectionresultsonCOCOwithaMAEViT-Bpre-trainedonCOCO.Attention
transferachievesamajorityofthegainsofpre-traininginthissettingaswell.
accuracywithViT-H.Itcandothiseventhoughscratchmodelperformancealreadysaturatesatthe
ViT-Lsize. Again,thisindicatesthatmodelsneedproperinter-tokenroutinginordertolearngood
featuresthatgeneralize. Otherwise,theycannotproperlyutilizeincreasedmodelcapacity.
5.5 ObjectDetection
Finally,weexaminetheperformanceofattentiontransferinthestandardViTDetpipeline[35]for
COCOobjectdetection. Wecomparetrainingfromscratchagainstfine-tuningandattentiontransfer
fromaMAEViT-Bpre-trainedonCOCO,whichisdonetomitigatetheeffectofdistributionshift.
For fair comparisons, we use a 448√ó448 input to remove the effect from window attention and
positionalembeddinginterpolation,andremovetheeffectofrelativepositionalembeddings. Table9
shows that Attention Distillation recovers a majority of the gains from pre-training in this dense
prediction setting as well. Based on Table 8, we anticipate that the gap between fine-tuning and
attentiontransferwilldecreasewithViT-L,butwearelimitedbycomputationalresources.
6 RelatedWork
Structureinattentionpatterns. Numerouspreviousworkshavestudiedtheattentionpatternsof
pre-trainedvisiontransformers[59,63,43]. Theseworkspresentthesedifferencesonlyasqualitative
observations,whereasweareabletoisolatetheattentionpatternsandshowthattheyarecausally
responsibleformostofthedifferencesinfine-tuningperformance. Othermethods,suchasLora[28]
orPrompt-to-Prompt[25],dorelyontheimportanceofhighqualityattentionpatternswithinpre-
trainednetworks,buttheyalsoutilizepre-trainedfeaturesanddonotprovideourinsightthatthese
features are typically unnecessary for the tasks we examine. Trockman and Kolter [55] observe
diagonalstructurewithintheproductofattentionlayerweightsinatrainedsupervisednetwork. They
showthatinitializingtheweightswiththisstructuremoderatelyimprovesaccuracyforsmallmodels
earlyintraining. TheworkmostsimilartousisZhangetal.[68]inthelanguagedomain,which
findsthatpre-trainedBERTmodelsimprovelengthgeneralizationonafewparticularsynthetictasks.
Theyattributeittotheattentionpatternsofafew,specificheadsandshowthathardcodingthese
patternsintothenetworkachievesthesamebenefit. Ourworkiscomplementaryandemphasizesthe
importanceofattentionmapsoverfeatures.
Decouplinginter-andintra-tokenoperations. GLoMo[64]alsoattemptstodecouplefeatures
fromthewaytheyshouldbecombined. Theyuseunsupervisedpre-trainingtotrainanetworkto
outputagraph,whichislaterusedtocombinetask-specificfeatures. Wefindthatthereisnoneedto
developaspecializedarchitecturetoachievethis‚ÄìVisionTransformersalreadydothisnaturally.
9Knowledge Distillation Knowledge distillation is a popular framework for training small, high-
performingstudentnetworks[26]. Knowledgedistillationmethodstypicallyaddalosstoencourage
the student network to match the teacher network‚Äôs logits, but variants often use other feature
statisticsastargets,suchasthefinalrepresentations[41,13],intermediatefeaturesimilarities[19],
or intermediate feature magnitudes [66, 34]. This last approach has also previously been called
‚Äúattentiontransfer,‚Äùbuttheirmethodisquitedifferentandactuallyreferstodistillingspatialactivation
magnitudesinConvNets. Theseknowledgedistillationapproachesallassumethatstudentsneedto
beexplicitlytaughttherightfeatures. Incontrast,ouranalysiswithattentiontransfershowsthat
attentionmapsaresufficienttorecoverallofthegainsfrompre-training. Somepapershaveused
attention distillation as an auxiliary loss to help a smaller model learn the teacher outputs more
effectively[62,61]. However,theseonlyconsidertransferringthesamefunctionacrossmodelsizes,
insteadoftransferringknowledgefromapre-trainedmodeltoadifferentdownstreamtask.
ConnectiontothelotterytickethypothesisThelotterytickethypothesis[15]suggeststhatlarge,
dense neural networks contain small, sparse subnetworks (‚Äúwinning tickets‚Äù) that, when trained
fromscratch,canmatchorevenoutperformtheperformanceoftheoriginaldensenetwork. This
isparticularlyinterestingbecausethesesparsesubnetworksmaintaintheirperformanceonlywith
theiroriginalinitializationvalues;thestrengthoftheirconnectionsbetweenneuronsisspecialin
someway. Ourfindingsdrawaparallel,indicatingthattheconnectionsbetweenpatches,controlled
solelybytheattentionpatterns, aresimilarlyspecialwithinpretrainedViTs. FrankleandCarbin
[15] further conjecture that overparameterization improves performance because larger models
containexponentiallymoresparsesubnetworksinsuperpositionandarethusmorelikelytocontaina
‚Äúwinningticket‚Äù‚Äìahypothesissupportedbysubsequentempiricalandtheoreticalwork[46,44,42,3].
However,thisphenomenondoesnotariseinoursettingwithViTattentionpatterns,sincethereare
onlyahandfulofattentionmapsperlayer(ratherthanthousandsofneurons). Consequently,good
attentionpatternsareunlikelytoappearbychanceandmustinsteadbelearnedduringpretraining.
Wehopethatanewmodelarchitecturethatefficientlycombinesmanymoreattentionmapsperlayer
canaddressthislimitationandlearnbetterfromscratchthanexistingViTs.
7 Conclusion
EvenasTransformershavesurgedinpopularity,thewayweusethemhasremainedstagnant: pre-
train,thenfine-tunetheweights. Inthiswork,wepresentattentiontransfer,asimplealternativeto
ViTfine-tuningthatdecouplesintra-tokenoperations(howtoextractmoreusablefeaturesforeach
token)frominter-tokenoperations(howthosefeaturesshouldbecombined). Ourkeyfindingisthat
theattentionpatterns(inter-tokenoperations)arethekeyfactorbehindmuchoftheeffectivenessof
pre-training‚ÄìourAttentionDistillationmethodcompletelymatchesfine-tuningonImageNet-1K.We
dofindsomelimitations: attentiontransferdoesnotworkwellifthepre-trainingandtransferdatasets
aredifferent,anditlosesabitofOODrobustness. Nevertheless,ourfindingsprovideinsightsintothe
roleofattentioninpre-trainedViTs,andwehopefutureworkfixesattentiontransfer‚Äôsshortcomings
andexplorestheadvantagesofthisnewtransfermethod.
Somedirectionsforfutureworkareparticularlyinteresting.First,adeeperinvestigationofthequeries
Qcouldhelpusbetterunderstandtheirimportanceandpotentiallyyieldbettertransferstrategies.
Second,attentiontransfereliminatestheneedfortricksthatfine-tuningrequires,suchaslayerwise
learningratedecay. Layerwiselearningratedecayaddsthepriorthatearlylayersshouldchangeless
comparedtolaterlayers. However,thispriormaybeoverlyrestrictivefornext-generationmodels,
sinceitpreventsearlyfeaturesfromchanging,andgettingridofitcouldopenupnewopportunities.
Finally,sinceattentionmapsareL√óL,whereListhesequencelength,attentionmapscouldbe
transferredmoreeasilyacrossmodelsizes. Incontrast,weighttuningismoredifficulttoapplywhen
themodelshavedifferentdimensions. Pre-trainingasmallermodelandtransferringitsattention
patternstoalargerdownstreammodelcouldbemorepracticalthanthecurrentpracticeoffine-tuning.
AcknowledgmentsandDisclosureofFunding
ALissupportedbytheNSFGRFPDGE1745016andDGE2140739andperformedtheworkduring
aninternshipatFAIR.
10References
[1] J.L.Ba,J.R.Kiros,andG.E.Hinton. Layernormalization. arXiv:1607.06450,2016.
[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P.Shyam,G.Sastry,A.Askell,S.Agarwal,A.Herbert-Voss,G.Krueger,T.Henighan,R.Child,
A.Ramesh,D.Ziegler,J.Wu,C.Winter,C.Hesse,M.Chen,E.Sigler,M.Litwin,S.Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.
Languagemodelsarefew-shotlearners. InNeurIPS,2020.
[3] R.Burkholz. Mostactivationfunctionscanwinthelotterywithoutexcessivedepth. Advances
inNeuralInformationProcessingSystems,35:18707‚Äì18720,2022.
[4] M.Caron,H.Touvron,I.Misra,H.J√©gou,J.Mairal,P.Bojanowski,andA.Joulin. Emerging
propertiesinself-supervisedvisiontransformers. InICCV,2021.
[5] M.Chen,A.Radford,R.Child,J.Wu,H.Jun,D.Luan,andI.Sutskever. Generativepretraining
frompixels. InICML,2020.
[6] T.Chen,S.Kornblith,M.Norouzi,andG.Hinton. Asimpleframeworkforcontrastivelearning
ofvisualrepresentations. InICML,2020.
[7] X.Chen,S.Xie,andK.He. Anempiricalstudyoftrainingself-supervisedVisionTransformers.
InICCV,2021.
[8] K.Clark,M.-T.Luong,Q.V.Le,andC.D.Manning. ELECTRA:Pre-trainingtextencodersas
discriminatorsratherthangenerators. InICLR,2020.
[9] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data
augmentationwithareducedsearchspace. InCVPRWorkshops,2020.
[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale
hierarchicalimagedatabase. InCVPR,2009.
[11] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova. BERT:Pre-trainingofdeepbidirectional
transformersforlanguageunderstanding. InNAACL,2019.
[12] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,
M.Minderer,G.Heigold,S.Gelly,J.Uszkoreit,andN.Houlsby. Animageisworth16x16
words: Transformersforimagerecognitionatscale. InICLR,2021.
[13] Q.Duval,I.Misra,andN.Ballas. Asimplerecipeforcompetitivelow-computeselfsupervised
visionmodels. arXivpreprintarXiv:2301.09451,2023.
[14] A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data
determinesdistributionalrobustnessincontrastivelanguageimagepre-training(clip). InICML,
2022.
[15] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXivpreprintarXiv:1803.03635,2018.
[16] R.Girshick,J.Donahue,T.Darrell,andJ.Malik. Richfeaturehierarchiesforaccurateobject
detectionandsemanticsegmentation. InCVPR,2014.
[17] I.J.Goodfellow,J.Shlens,andC.Szegedy. Explainingandharnessingadversarialexamples.
arXivpreprintarXiv:1412.6572,2014.
[18] P.Goyal,P.Doll√°r,R.Girshick,P.Noordhuis,L.Wesolowski,A.Kyrola,A.Tulloch,Y.Jia,and
K.He. Accurate,largeminibatchSGD:TrainingImageNetin1hour. arXiv:1706.02677,2017.
[19] Z.Hao,J.Guo,D.Jia,K.Han,Y.Tang,C.Zhang,H.Hu,andY.Wang. Learningefficient
visiontransformersviafine-grainedmanifolddistillation. InNeurIPS,2022.
[20] K.He,X.Zhang,S.Ren,andJ.Sun. Deepresiduallearningforimagerecognition. InCVPR,
2016.
[21] K.He,H.Fan,Y.Wu,S.Xie,andR.Girshick. Momentumcontrastforunsupervisedvisual
representationlearning. InCVPR,2020.
[22] K.He,X.Chen,S.Xie,Y.Li,P.Doll√°r,andR.Girshick. Maskedautoencodersarescalable
visionlearners. InCVPR,2022.
[23] D.Hendrycks,S.Basart,N.Mu,S.Kadavath,F.Wang,E.Dorundo,R.Desai,T.Zhu,S.Para-
juli, M.Guo, etal. Themanyfacesofrobustness: Acriticalanalysisofout-of-distribution
generalization. InICCV,2021.
11[24] D.Hendrycks,K.Zhao,S.Basart,J.Steinhardt,andD.Song. Naturaladversarialexamples. In
CVPR,2021.
[25] A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-Or. Prompt-to-
promptimageeditingwithcrossattentioncontrol. arXivpreprintarXiv:2208.01626,2022.
[26] G.Hinton,O.Vinyals,andJ.Dean.Distillingtheknowledgeinaneuralnetwork. arXivpreprint
arXiv:1503.02531,2015.
[27] J.HowardandS.Ruder. Universallanguagemodelfine-tuningfortextclassification. arXiv
preprintarXiv:1801.06146,2018.
[28] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:
Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,2021.
[29] G.Huang, Y.Sun, Z.Liu, D.Sedra, andK.Q.Weinberger. Deepnetworkswithstochastic
depth. InECCV,2016.
[30] P.Izmailov,P.Kirichenko,N.Gruver,andA.G.Wilson. Onfeaturelearninginthepresenceof
spuriouscorrelations. AdvancesinNeuralInformationProcessingSystems,35:38516‚Äì38532,
2022.
[31] S.Jelassi,M.Sander,andY.Li. Visiontransformersprovablylearnspatialstructure. Advances
inNeuralInformationProcessingSystems,35:37822‚Äì37836,2022.
[32] S.Kornblith,M.Norouzi,H.Lee,andG.Hinton. Similarityofneuralnetworkrepresentations
revisited. InICML,2019.
[33] Y.LeCun,B.Boser,J.S.Denker,D.Henderson,R.E.Howard,W.Hubbard,andL.D.Jackel.
Backpropagationappliedtohandwrittenzipcoderecognition. Neuralcomputation,1989.
[34] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma. Neural attention distillation: Erasing
backdoortriggersfromdeepneuralnetworks. arXivpreprintarXiv:2101.05930,2021.
[35] Y.Li,H.Mao,R.Girshick,andK.He. Exploringplainvisiontransformerbackbonesforobject
detection. InECCV,2022.
[36] Y.Li,H.Fan,R.Hu,C.Feichtenhofer,andK.He. Scalinglanguage-imagepre-trainingvia
masking. InCVPR,2023.
[37] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Doll√°r,andC.L.Zitnick.
MicrosoftCOCO:Commonobjectsincontext. InECCV,2014.
[38] I.LoshchilovandF.Hutter. SGDR:Stochasticgradientdescentwithwarmrestarts. InICLR,
2017.
[39] I.LoshchilovandF.Hutter. Decoupledweightdecayregularization. InICLR,2019.
[40] D.Mahajan,R.Girshick,V.Ramanathan,K.He,M.Paluri,Y.Li,A.Bharambe,andL.vander
Maaten. Exploringthelimitsofweaklysupervisedpretraining. InECCV,2018.
[41] K. Navaneet, S. A. Koohpayegani, A. Tejankar, and H. Pirsiavash. Simreg: Regression
as a simple yet effective tool for self-supervised knowledge distillation. arXiv preprint
arXiv:2201.05131,2022.
[42] L.Orseau,M.Hutter,andO.Rivasplata. Logarithmicpruningisallyouneed. Advancesin
NeuralInformationProcessingSystems,33:2925‚Äì2934,2020.
[43] N.Park,W.Kim,B.Heo,T.Kim,andS.Yun. Whatdoself-supervisedvisiontransformers
learn? arXivpreprintarXiv:2305.00729,2023.
[44] A.Pensia,S.Rajput,A.Nagle,H.Vishwakarma,andD.Papailiopoulos. Optimallotterytickets
viasubsetsum:Logarithmicover-parameterizationissufficient. Advancesinneuralinformation
processingsystems,33:2599‚Äì2610,2020.
[45] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InICML,2021.
[46] V.Ramanujan,M.Wortsman,A.Kembhavi,A.Farhadi,andM.Rastegari. What‚Äôshiddenina
randomlyweightedneuralnetwork? InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages11893‚Äì11902,2020.
12[47] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to
imagenet? InICML,2019.
[48] C.Schuhmann, R.Beaumont, R.Vencu, C.Gordon, R.Wightman, M.Cherti, T.Coombes,
A.Katta,C.Mullis,M.Wortsman,etal. Laion-5b: Anopenlarge-scaledatasetfortrainingnext
generationimage-textmodels. InNeurIPS,2022.
[49] N. Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint
arXiv:1911.02150,2019.
[50] N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhutdinov. Dropout: Asimple
waytopreventneuralnetworksfromoverfitting. TheJournalofMachineLearningResearch,
pages1929‚Äì1958,2014.
[51] C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna. Rethinkingtheinceptionarchitec-
tureforcomputervision. InCVPR,2016.
[52] Y.Tian,Y.Wang,B.Chen,andS.Du. Scanandsnap: Understandingtrainingdynamicsand
tokencompositionin1-layertransformer. NeurIPS,2023.
[53] Y.Tian,Y.Wang,Z.Zhang,B.Chen,andS.Du. Joma: Demystifyingmultilayertransformers
viajointdynamicsofmlpandattention. ICLR,2024.
[54] A.TorralbaandA.A.Efros. Unbiasedlookatdatasetbias. InCVPR,2011.
[55] A.TrockmanandJ.Z.Kolter. Mimeticinitializationofself-attentionlayers. arXivpreprint
arXiv:2305.09828,2023.
[56] G.VanHorn,O.MacAodha,Y.Song,Y.Cui,C.Sun,A.Shepard,H.Adam,P.Perona,and
S.Belongie. TheiNaturalistspeciesclassificationanddetectiondataset. InCVPR,2018.
[57] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I.Polosukhin. Attentionisallyouneed. InNeurIPS,2017.
[58] S.Venkataramanan, A.Ghodrati, Y.M.Asano, F.Porikli, andA.Habibian. Skip-attention:
Improvingvisiontransformersbypayinglessattention. arXivpreprintarXiv:2301.02240,2023.
[59] M.Walmer,S.Suri,K.Gupta,andA.Shrivastava. Teachingmatters: Investigatingtheroleof
supervisioninvisiontransformers. InCVPR,2023.
[60] H.Wang,S.Ge,Z.Lipton,andE.P.Xing. Learningrobustglobalrepresentationsbypenalizing
localpredictivepower. InNeurIPS,2019.
[61] K.Wang,F.Yang,andJ.vandeWeijer.Attentiondistillation:self-supervisedvisiontransformer
studentsneedmoreguidance. arXivpreprintarXiv:2210.00944,2022.
[62] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou. Minilm: Deep self-attention
distillation for task-agnostic compression of pre-trained transformers. Advances in Neural
InformationProcessingSystems,33:5776‚Äì5788,2020.
[63] Z.Xie,Z.Geng,J.Hu,Z.Zhang,H.Hu,andY.Cao. Revealingthedarksecretsofmasked
imagemodeling. InCVPR,2023.
[64] Z.Yang,J.Zhao,B.Dhingra,K.He,W.W.Cohen,R.Salakhutdinov,andY.LeCun. Glomo:
Unsupervisedly learned relational graphs as transferable representations. arXiv preprint
arXiv:1806.05662v1,2018.
[65] S.Yun,D.Han,S.J.Oh,S.Chun,J.Choe,andY.Yoo. Cutmix: Regularizationstrategytotrain
strongclassifierswithlocalizablefeatures. InICCV,2019.
[66] S.ZagoruykoandN.Komodakis.Payingmoreattentiontoattention:Improvingtheperformance
ofconvolutionalneuralnetworksviaattentiontransfer. InICLR,2017.
[67] H.Zhang,M.Cisse,Y.N.Dauphin,andD.Lopez-Paz. mixup: Beyondempiricalriskmini-
mization. InICLR,2018.
[68] Y.Zhang,A.Backurs,S.Bubeck,R.Eldan,S.Gunasekar,andT.Wagner. Unveilingtransform-
erswithlego: asyntheticreasoningtask. arXivpreprintarXiv:2206.04301,2022.
13Appendix
A KeyNumbers
A.1 InformationinAttentionTransfer
Howmuchinformationistransferredduringattentiontransfer? Table10showstwowaysofdoing
this accounting. If considering the map as 24 layers √ó 16 heads √ó 197 query tokens √ó 197 key
tokens,thereareabout15millionactivationstransferredperexample. However,QK‚ä§islowrank
sinceQandK arevery‚Äútall,‚Äùsotheattentionmapsoftmax(QK‚ä§)canbeconsidered24layers√ó
16heads√ó197tokens√ó64headdim√ó2matrices,whichisabout9.7millionactivations.
AccountingMethod Parameters
CountsizesofQ,K 24√ó16√ó197√ó64√ó2‚âà9.7M
CountsizeofQK‚ä§ 24√ó16√ó197√ó197‚âà15M
Table10: NumberofparametersactivationstransferredperexampleforaViT-Lteacher.
A.2 ComputationalCostofAttentionTransfer
Attentiontransferhasthesamecomputationalandmemorycostasanyotherknowledgedistillation
methodthatdoesaforwardpassthroughateacher. Wecomparefine-tuningvsattentiondistillation
ona16GBNVIDIAGP100withViT-Landabatchsizeof16:
Method Memory(GB) Timeperiteration(s)
WeightFine-tuning 9.4 0.93
KnowledgeDistillation 11.1 1.23
AttentionCopy/Distillation 11.1 1.23
Table11: TrainingcostofAttentionTransfer.
TrainingtheselargemodelsonImageNet-1Kisquitecomputationallyexpensive. 100epochsof
regularfine-tuningisabout2070GPU-hoursper100epochs,and100epochsofattentiontransferis
about2735GPU-hours. Intotal,weestimateabout150kGPU-hoursarerequiredtoreproduceall
experiments.
B AdditionalAnalysis
B.1 AggregatedAttentionTransfer
In Section 4.1, we conducted a thorough analysis of which aspects of attention matter the most.
Another way to identify key properties of the teacher‚Äôs attention maps is to average them across
someaxisduringthetransfer. Forexample,onecanaveragetheattentionmapsoveralllayersofthe
teachernetwork,sothatthestudentusesthesamemapateverylayer. Table12showstheresultswith
aggregationsoverseveralnaturalaxes. Averagingoverexamples(i.e.,usingthesameattentionmap,
independentoftheinput)oraveragingoverquerytokens(i.e.,eachattentiondistributionisthesame,
regardlessofthequerytokengivenanimage)doesquitepoorly. Thisindicates,unsurprisingly,that
thesearekeyelementsofself-attention. Thisalsoshowsthatpriorworkthatfocusesonaggregate
statisticsoftheattentionmaps(e.g.,averagedoverexamples)[59]failtocapturetheper-example
natureoftheattentionmapsthatareactuallyresponsibleforfullfine-tuningperformance. Attention
copyperformanceismorereasonablewhenaveragingoverheadsorlayers.Thispartiallycorroborates
previousfindingsthatattentionmapscanlargelybesharedacrossalllayers[58]. However,whilethe
resultscanbepotentiallyimprovedwithmorerecipesearch,theperformanceisfarshortofthefull
fine-tuningaccuracy(85.7).
14averageover acc.
examples 79.7
layers 82.7
heads 82.2
querytokens 79.9
none 85.1
Table12: Aggregatedattentiontransfer. Thefullsetofattentionmapsfromtheteacherhasshape
(examples,layers,heads,querytokens,keytokens). Herewetrytoaverageovereachoftheseaxes
before the transfer. Performance drops the most when averaging over examples or query tokens,
indicatingthatthesearethemostimportantaspectsoftheattentionmaps.
method acc.
attn.distill 85.7
featuredistill 81.3
fine-tune 85.7
scratch 79.7
Table13: Comparisonwithknowledgedistillation. Wetryknowledgedistillationfromthepre-
trainedteacherbyaddinganauxiliaryMSElossontheresidualstreamoutput. Thisencouragesthe
studentmodeltomatchtherepresentationsoftheteacher(‚Äúfeaturedistill‚Äù). Wefindthatthisdoes
muchworsethanAttentionDistillation.
B.2 ComparisontoKnowledgeDistillation
Ourcentralhypothesishasbeenthatthepre-trainedattentionmapsaresufficient,andthepre-trained
featuresarenotnecessary. Sinceourattentiontransfermethodsarespecialinstancesofknowledge
distillation[26],weadditionallycomparetoabaselineofdistillingtheresidualstreamfeaturesfroma
pre-trainedMAEViT-L.InTable13,weobtainadownstreamaccuracyof81.3onImageNet-1k. This
issignificantlylowerthanthe85.7thatcanbeachievedthroughfine-tuningorattentiondistillation.
Thismakessense: thefeatureslearnedduringself-supervisedpre-trainingarenotdirectlywell-suited
for classification, so trying to match them can hurt performance. CKA analysis of the features
(Figure5)supportsthishypothesis‚Äìthefine-tunedMAEdoeswellbysignificantlychangingthe
featuresinthelatterhalfofthenetwork. Overall,transferringattentionappearstodomuchbetter
thandistillingthefeatures.
B.3 AttentionMapAnalysisforTransferringQ
InSection4.1,wefoundthatcopyingthequeriesQdoessurprisinglywell,almostmatchingAttention
Distillationorfine-tuningthepre-trainedweights. Here,wecomparetheattentionmapslearnedby
thecopyQmodeltothoseofothermodels,inhopesofunderstandingwhycopyQdoessowell.
Eachofthe24layerswithinaViT-Lhas16attentionheads,whicheachcomputeanL√óLattention
mapforanimagewithLpatches. Wewouldliketodeterminethesimilaritybetweentheattention
headsintwomodelsusingsomedivergencemeasure;weusetheJensen-Shannondivergence(JSD)
becauseitissymmetricinitsarguments. However,thereisonecaveat. Becausetheoutputofthe
attentionlayerisinvarianttotheorderingofitsheads,itisinsufficienttocomparetheithheadof
onemodelagainsttheithheadofanother. Weneedtoproperlymatchheadsupacrossmodels. We
exploredfourwaysofdoingso:
1. Directpair: thisisthenaiveapproachofcomputingtheJSDbetweentheithheadofthefirst
modelandtheithheadofthesecondmodel. Thiscanfailsincesimilarheadsmaynotbein
thesameorderacrossmodels.
2. Bipartitematching: foreachlayer,wecomputetheJSDbetweeneachofthe16headsinthe
firstmodelandthe16headsinthesecondmodel. Wethenusebipartitematchingtocreatea
one-to-onepairingbetweentheheadsthatminimizesthecumulativeJSD.Thissolvesthe
previousproblem,butcanstillbethrownoff,suchasifoneofthemodelshasheadsthatit
appliesnoweightto(V =0orW =0orW isorthogonaltothevalues).
proj proj
15Direct pair Bipartite matching Min Averaged maps
0.4
0.3
0.2
0.1
0.0
0 10 20 0 10 20 0 10 20 0 10 20
Layer Layer Layer Layer
Direct pair Bipartite matching Min Averaged maps
0.4 pre-train attn. distill
copy Q fine-tune
0.3 copy K
0.2
0.1
0.0
0 10 20 0 10 20 0 10 20 0 10 20
Layer Layer Layer Layer
Figure7: Attentionmapsimilarityacrossmethods. Eachcolumncorrespondstoadifferentwayof
matchingupattentionheadsbetweentwomodels. ThetoprowshowstheJensen-Shannondivergence
(JSD)withrespecttotheMAEpre-trainedteacher, whereasthebottomrowshowstheJSDwith
respecttothefine-tunedMAEmodel. Theseplotsmatchourintuitionondistillationorfine-tuning
methods,butcopyQisconsistentlydissimilarfromthePTandFTmodels. Notethatthismaybea
limitationofthisparticularanalysis,sincetherearemanysettingsofQ,K,andV thatleadtothe
samelayeroutput.
3. Minimum: instead of creating a one-to-one matching, we allow many-to-one matching
betweenheads. WecallthisMinimumbecauseeachheadinthefirstmodelispairedwith
theheadfromthesecondmodelwiththesmallestJSD.Thisallowsourmetrictopotentially
ignoreextraneousheadsinthesecondmodel,butisstillsusceptibletoextraneousheadsin
thefirstmodel.
4. Averaged maps: we average the attention maps of all heads in a layer and compare the
averagedmapsacrossmodels. Thiscanstillbethrownoffbyextraneousheads.
Figure7showstheresultsofcomparingmodelsagainstthepre-trainedteacher(toprow)orfine-tuned
model(bottomrow)asthesecondmodelintheJSD.Mostofourfindingsalignwithourintuition. In
thetoprow,whencomparingagainstthepre-trainedteacher,attentiondistillationmatchestheteacher
mapscloselyuntillayer18,thelastlayerwhoseattentionmapsitistrainedtoapproximate. The
fine-tunedmodel‚Äôsattentionmapsdivergemoreinlaterlayers,sincelayerwiselearningratedecay
ensuresthattheearlierlayersdon‚Äôtchangemuch. However,copyQisonlysomewhatsimilartothe
pre-trainedteacherorthefine-tunedmodel,acrossallofourwaystomeasureattentionmapsimilarity.
Furthermore, itislesssimilarthancopyK is, eventhoughcopyK hasmuchlowerdownstream
performancethancopyQ.
Notethattheseplotshavemajorlimitationsinwhatkindsofsimilaritytheycapture. Withenough
attentionheadsperlayer,thesameexactattentionmapcanbepartitioneddifferentlyacrosstheheads
betweentwomodels. Hypothetically,let‚Äôssaythatanattentionlayerwantstoattenduniformlyacross
alllocations(i.e.,performaveragepooling),andthatwehave3models,eachwith2attentionheads:
1. Head1attendsuniformlyoveralllocations,head2attendsarbitrarilyoverlocations,and
thesecondhead‚Äôsvaluesaresetto0.
2. Head1attendsuniformlyoverthetophalfoftheimage,head2attendsuniformlyoverthe
bottomhalfoftheimage,andbothusevaluesV/2.
3. Head1attendsuniformlyoverthelefthalfoftheimage,head2attendsuniformlyoverthe
righthalfoftheimage,andbothusevaluesV/2.
16
rehcaet
TP
morf
DSJ
TF
morf
DSJinput init scratch pre-train / copy fine-tune attn. distill.
Figure8: Visualizationofattentionmapsfordifferentmethods. Weshowwhatthe[CLS]token
attends to at various layers within the network. Darker patches indicate more attention weight.
Notably,thepre-trainedMAEmodel‚Äôsattentionmapsprovideasignificantprioroverwhatthemodel
shoulduse,separatingtheobjectfrompotentiallyspuriouscueslikethebranchorthebackground. In
contrast,modelsrightatrandominitialization(‚Äúinit‚Äù)startoffattendinguniformlyovertheimage,
whichleadsthescratchmodeltousemoreofthespuriouspatches. Weshowmoreattentionmap
visualizationsinAppendixD.
All3headscomputethesameexactattentionoperation,yetwouldregisterashighlydissimilarinthe
setupfromFigure7. Overall,thisexperimentshowsthatcopyQ‚Äôsbehaviorishighlycomplex,and
itsstrongdownstreamperformanceisstillnotfullyunderstand.
B.4 AttentionMapVisualizations
Section4.2 provided severalresultsto show thatattention transferdoes notsimply ‚Äúrelearn‚Äùthe
teacher. Here, weexamineonefinalpieceofevidence. Weshowtheattentionmapsofdifferent
networksinFigure8. WefocusonAttentionDistillation,sinceAttentionCopy‚Äôsmapsareidentical
tothoseintheteacher. AttentionDistillation‚Äôsmapsgenerallymatchtheteacher(pre-trainedMAE),
butarenotcompletelyidenticalforlayersthataredistilled(e.g.,layer13). Forlayer24,whichisnot
distilled,AttentionDistillationlooksverydifferentfrompre-trainedmodel,insteadresemblingthe
attentionmapofamodeltrainedfromscratch. Thesevisualizationsalsohighlightthefactthatthese
attentionmapsareaverystrongprioronwhatthemodelshoulduse. Whiletherandomlyinitialized
modelattendscompletelyuniformlyoveralltokens,thepre-trainedteacherattentionmapsalready
separatetherelevantobjectfrompotentiallyspuriouscorrelations(likethebranchorbackgroundin
theexample). WeshowadditionalattentionmapvisualizationsinFigure11andFigure12.
B.5 AttentionDistillationHyperparameterSensitivity
WeshowthesensitivityofAttentionDistillationtoitshyperparameters.
DistillationlossweightWefirstconsiderthedistillationlossweightŒªwhichisusedtocomputethe
overalllossforthestudent:
L=L +ŒªL (3)
task dist
Table9showsthatalargerweight,Œª=3,doesbest. Thismaybebecauseitencouragesthestudent
tolearnusefulattentionmapsmorequickly,lettingitguidefeaturelearningearlierintraining. We
usethisvalueofŒªforourmainresult,wherewematchthe85.7accuracyoffine-tuning. However,
allotherresultsinthispaperuseŒª=1forsimplicity.
Partial distillation: layers Just as we tried for Attention Copy, we also tried distilling various
numbersoflayersfromtheMAEteachernetwork,startingfromthebottomofthenetwork. Table10
showsthatthereisa‚Äúsweetspot‚Äùwhendistillingthefirst21outof24layers. Distillingalllayersmay
17
1
reyaL
31
reyaL
32
reyaLDistillationlossweightŒª Accuracy Layersdistilled Accuracy
0.3 84.7 18 85.3
1.0 85.3 21 85.5
3.0 85.7 24 85.1
Figure9: Weightondistillationloss Figure10: Numberoflayersdistilled
Teacher Student Accuracy
Random Random 72.7
MAE Random 85.1
MoCo-v3 Random 82.5
FLIP Random 86.6
FLIP MAE 84.2
MAE MAE 85.4
MoCo-v3 MAE 82.9
MAE FLIP 83.2
Table14: Decouplingattentionmapsfromfeatures. WetryAttentionDistillationwithvarious
pre-trainedstudents.
hurtperformancebyforcingthestudenttouseattentionmapsthataremoresuitedforreconstruction
thanclassification. Notethatallotherdistillationresultsinthispaperusethefirst18layersbydefault.
B.6 MixandMatch,StudentandTeacher
Inthemainpaper,wefocusedontransferringattentionmapsfromapre-trainedteachertoarandomly
initialized student. However, the fact that Transformers have decoupled inter- and intra-token
computationmeansthatwecanactuallyinitializethestudentwithapre-trainednetworkaswell.
Thisentailstestingwhethertheattentionpatternsfromonenetworkcanimprovethefeaturesofan
already-pre-trainedstudentmodel. WetryAttentionDistillationforvariouscombinationsofMAE,
MoCo-v3, FLIP, and a randomly initialized network. Table 14 shows that this ‚Äúmix-and-match‚Äù
trainingdoesbetterthantrainingfromscratch(83.0)butdoesnotmatchtheperformanceinTable7,
wherethestudentsarerandomlyinitialized. Thesearepreliminaryresults,astheoveralltraining
recipemayneedtobechangedtoaccommodatethedifferentlearningdynamicsofadifferentstudent
model. Furtherhyperparametertuningmaysignificantlyimprovetheseresults.
C ImplementationDetails
WepresentthetrainingrecipeforAttentionCopyinTable15andtherecipeforAttentionDistillation
inTable16. ForourpartiallayertransferexperimentsinFigure3,wesetŒ≤ =0.95asithelpsavoid
2
traininginstabilities.
18Config Value
optimizer AdamW[39]
baselearningrate 1e-3
minimumabsolutelr 2e-3
weightdecay 0.05
optimizermomentum Œ≤ =0.9,Œ≤ =0.999
1 2
layerwiselrdecay 0.75
batchsize 2048
learningrateschedule cosinedecay[38]
warmupepochs[18] 5
trainingepochs 100
augmentation RandAug(9,0.5)[9]
labelsmoothing[51] 0.1
mixup[67] 0.8
cutmix[65] 1.0
droppath[29] 0
exp. movingaverage(EMA) 0.9999
layerscopied 24
Table15: TrainingrecipeforAttentionCopyonViT-L.
Config Value
optimizer AdamW
baselearningrate 1e-4
weightdecay 0.3
optimizermomentum Œ≤ =0.9,Œ≤ =0.95[5]
1 2
batchsize 2048
learningrateschedule cosinedecay
warmupepochs 20
trainingepochs 200
augmentation RandAug(9,0.5)
labelsmoothing 0.1
mixup 0.8
cutmix 1.0
droppath 0.2
exp. movingaverage(EMA) 0.9999
layerscopied 18
distillationweightŒª 3
Table16: TrainingrecipeforAttentionDistillationonViT-L.
19D AdditionalAttentionMapVisualizations
input init scratch pre-train / copy fine-tune attn. distill.
input init scratch pre-train / copy fine-tune attn. distill.
Figure11: Attentionmapvisualizationsonmoreexamples
20
1
reyaL
31
reyaL
32
reyaL
1
reyaL
31
reyaL
32
reyaLinput init scratch pre-train / copy fine-tune attn. distill.
input init scratch pre-train / copy fine-tune attn. distill.
Figure12: Attentionmapvisualizationsonmoreexamples
21
1
reyaL
31
reyaL
32
reyaL
1
reyaL
31
reyaL
32
reyaL