Advancing Fine-Grained Visual Understanding with Multi-Scale Alignment
in Multi-Modal Models
WeiWang*1,2, ZhaoweiLi*2,3, QiXu2, LinfengLi2, YiqingCai2, BotianJiang2,3,
HangSong2, XingcanHu1, PengyuWang3, LiXiao†1
1UniversityofScienceandTechnologyofChina,2ByteDanceInc,3FudanUniversity
wangweiii@mail.ustc.edu.cn, lizhaowei126@gmail.com
Abstract
Multi-modal large language models (MLLMs) have
achieved remarkable success in fine-grained visual under-
standing across a range of tasks. However, they often en-
countersignificantchallengesduetoinadequatealignment
for fine-grained knowledge, which restricts their ability to
accurately capture local details and attain a comprehen-
siveglobalperception.Whilerecentadvancementshavefo-
cusedonaligningobjectexpressionswithgroundinginfor-
mation,theytypicallylackexplicitintegrationofobjectim-
ages,whichcontainaffluentinformationbeyondmeretexts
Figure1.Thecomparisonofalignmentformulti-scaleobjectrep-
or coordinates. To bridge this gap, we introduce a novel
resentations. TheC,T,Idenoteobjectcoordinates,textsandim-
fine-grainedvisualknowledgealignmentmethodthateffec-
ages respectively. The “X-Y” denote MLLMs handle input “X”
tively aligns and integrates multi-scale knowledge of ob-
andoutput“Y”.
jects,includingtexts,coordinates,andimages.Thisinnova-
tivemethodisunderpinnedbyourmulti-scalefine-grained
enhancement data synthesis pipeline, which provides over Shikra[2]andKosmos-2[29]intextuallyformattingposi-
300Kessentialtrainingdatatoenhancealignmentandim- tionalvocabulariesorobjectcoordinates. Subsequentstud-
proveoverallperformance. Furthermore,wepresentTiny- ies aimed at improving model performance primarily fo-
GroundingGPT, a series of compact models optimized for cused on common strategies, including parameter enlarge-
high-levelalignments.Withascaleofapproximately3Bpa- ment[1,2,21,29]anddatasetenrichment[1,3,6,36]. Ad-
rameters, TinyGroundingGPTachievesoutstandingresults ditionally,thereisagrowinginterestindevelopingefficient,
ingroundingtaskswhiledeliveringperformancecompara- smaller fine-grained MLLMs [11, 19, 41, 46, 47] for real-
bletolargerMLLMsincomplexvisualscenarios. Thedata world applications. Regardless of the methods used, the
and code will be released in https://github.com/ core of fine-grained models lies in achieving better align-
wwangweii/TinyGroundingGPT.git. mentbetweenobjecttextsandvisualfeatures, encompass-
ingbothcoordinateandsemanticinformation.
While effective, these methods face a significant chal-
1.Introduction
lenge, i.e., the lack of fine-grained alignments. Visual ob-
jects typically encompass multi-scale representations with
Recent advancements in multi-modal large language mod-
varyinglevelsofinformation, includingcoordinates, texts,
els (MLLMs) have showcased remarkable capabilities in
andimages,asillustratedinFig.1. Inthiscontext,coordi-
multi-modalunderstanding,reasoning,andinteraction,gar-
natesprovidelow-levelobjectgroundinginformation,texts
neringunprecedentedattention[1,2,6,29,35,36]. MLLM
offer primary descriptions that may not capture every de-
researchinfine-grainedvisualunderstandinghasadvanced
tail,andimagesconveyhigh-levelinformationthatextends
significantly, particularly through early contributions from
beyond words. Most fine-grained models [2, 21, 42] pri-
*Equalcontribution.Orderisrandom. marily focus on alignments between object texts and co-
†Correspondingauthor.
1
4202
voN
41
]VC.sc[
1v19690.1142:viXraFigure2.Illustrationoftheproposedmulti-modalfine-grainedvisualknowledgealignmentmethod.Itadoptsathree-stagetrainingstrategy
thatprogressesfromeasytohardandthemulti-scalefine-grainedenhancementdatasynthesispipelineconstructsover300Kfine-grained
alignmentdata.
ordinates (i.e., T-C and C-T), often neglecting direct inter- hancementdatasynthesispipeline(seeFig.2(b))thatcon-
actions with object images. Although recent models like structs alignment data from both local and global perspec-
Qwen2-VL[1]andInternVL2[6]canprocessmultipleim- tives. Leveragingthisframework,weproposeTinyGround-
age inputs and understand relationships between the main ingGPT, which requires less storage for deployment while
imageandobjectimages(T-I),theystillstruggletoestablish outperforming larger models across multiple benchmarks,
explicitalignmentsamongobjecttexts,coordinates,andim- particularly in grounding tasks. Our contributions can be
ages. Thislimitationcanleadtohallucinationsandinsuffi- summarizedasfollows:
cientgroundingcapabilities[4]. • Weintroduceafine-grainedvisualknowledgealignment
To achieve high-level alignments and integrate multi- method that enables the model to progressively enhance
granularity knowledge, as illustrated in Fig. 2(a), we in- itsfine-grainedvisualunderstandingthroughbothglobal
troduceafine-grainedvisualknowledgealignmentmethod andlocalmulti-scaleobjectalignments.
that effectively aligns object texts, coordinates, and im- • Wedevelopamulti-scalefine-grainedenhancementdata
ages across multiple scales. Our method adopts a three- synthesispipelinethatleveragesopen-sourcedatasetsand
stage training strategy that progresses from easy to hard: advancedmodelstogenerateover300Kessentialtraining
1) Object and Relation Perception Pretraining: To develop dataforfine-grainedalignment.
a foundational understanding of object texts and images, • WepresentTinyGroundingGPT,aseriesofcompactmod-
weimplementaprogressivetrainingapproachforMLLMs els (1.5B and 3B parameters) that excel in multi-modal
based on a pretrained LLM. 2) Multi-scale Fine-grained understandingandgroundingcapabilities,achievingper-
Local Knowledge Alignment: To attain fine-grained vi- formancecomparabletothatoflarger7BMLLMs.
sualunderstandingandsharemulti-scaleobjectknowledge,
we conduct data-driven high-level alignments among ob- 2.Relatedworks
ject text descriptions, bounding box coordinates, and im-
Multi-modalLargeLanguageModels Recentadvance-
age features. 3) Detailed Global Knowledge Alignment:
mentsinlargelanguagemodels(LLMs)likeChatGPTand
To enhance the model’s global understanding by integrat-
LLaMA [35] have significantly propelled the development
ingfine-grainedknowledge,weguidetheMLLMstobridge
ofmulti-modallargelanguagemodels. Notableproprietary
different objects with multi-scale representations. To sup-
models, such as GPT-4V [27], have showcased the poten-
portthismethod,weproposeamulti-scalefine-graineden-
2tialofmulti-modalcapabilitiesinvisualtasks. Earlyopen- easy to hard: (a) Object and Relation Perception Pretrain-
sourceinitiativesincludeBLIP-2[17],MiniGPT-4[47],and ing,whichenablesthemodeltounderstandmultimodalin-
LLaVA[22],whichleveragepre-trainedLLMsandexcelin puts, identifying objects in images and their interrelations.
taskslikevisualquestionanswering.Subsequentefforts,in- (b) Multi-scale Fine-grained Local Knowledge Alignment
cludingQwen-VL[1],InternVL[6],andMiniCPM-V[41], by which the model is guided to achieve multi-scale, fine-
havefurtherenhancedmodelcapabilitiesbyintroducingdy- grainedalignments,accommodatingdiverseinputssuchas
namic resolution, expanding training data, and incorporat- object texts, coordinates, and images. (c) Detailed Global
ingreinforcementlearning,achievingimpressiveresultsin Knowledge Alignment which focuses on model training
optical character recognition (OCR) and grounding, while for global alignment and understanding, further integrat-
alsoenhancingthecredibilityofmodelresponses. ingfine-grainedinformationandbridgingdifferentobjects
Despitetheseadvancements,thelargenumberofparam- withmulti-scalerepresentations. Tosupportthishigh-level
etersinMLLMsincursextremelyhighcostsintrainingand alignment, we then propose a multi-scale fine-grained en-
deployment, limiting their widespread application. Many hancementdatasynthesispipeline,asillustratedinFig2(b),
studieshaveexploredhowtobuildmorelightweightLLMs, which generates multi-scale alignment datasets from both
such as Mini-Gemini [19], MobileVLM [7], MiniCPM- globalandlocalperspectives. Buildingonthisframework,
V [41], etc. They are based on lightweight LLMs, com- we propose TinyGroundingGPT, which requires less stor-
bined with optimized structures or training strategies, en- age for deployment while outperforming larger parameter
abling to have performance comparable to that of larger modelsacrossmultiplebenchmarks,particularlyinhalluci-
models. [10, 33, 37] have explored how to distill capa- nationevaluationandgroundingtasks.
bilitiesfromlargermodels,sothatsmallmodelscanobtain
3.1.Fine-grainedVisualKnwoledgeAlignment
complexreasoningabilities.
We elaborate the three training stages in our fine-grained
Fine-grained Multi-modal Models Recent research has visualknowledgealignmentmethodbelow.
increasingly focused on multi-modal language models ca-
pable of fine-grained understanding, which can be applied Object and Relation Perception Pretraining In this
to complex tasks such as grounding and OCR. Methods stage, we aim for the model to comprehend multi-modal
like Shikra [2] and Kosmos-2 [29] enhanced the visual inputs,recognizingtheobjectspresentintheimageandthe
groundingcapabilitiesofmulti-modallargelanguagemod- relationships among them, which forms the foundation for
els by constructing datasets that include coordinate infor- subsequentreasoningandgroundingtasks. Throughoutthe
mation, often by transforming visual task datasets into an training process, we initially keep the LLM and encoder
instruction-following format using templates. Other ap- frozen, training only the projector to connect the text and
proaches integrated additional visual components, such as image semantic spaces. Subsequently, we train both the
GLaMM[32]andLLaVA-Grounding[44],orextractedre- LLM and the projector to enhance the understanding of
gional features as supplementary inputs, as seen in Fer- objectsandtheirrelationships. WeutilizeLLaVA-Pretrain-
ret [42], NExT-Chat [43], and GPT4RoI [45]. Ground- 595k [22] and each sample is accompanied by a sampled
ingGPT [21] extended to support grounding tasks across instruction that requires the model to provide a concise
multiple modalities. Moreover, some initiatives sought to descriptionoftheimage.
broadenthecapabilitiesofMLLMsforvariousvisualtasks,
suchasVisionLLMv2[38]andUnifiedMLLM[20],which Multi-scale Fine-grained Local Knowledge Alignment
facilitatetaskslikeimageeditingandimagesegmentation. After the initial training stage, where the model learns to
Toenhanceperformanceonfine-grainedtasks,modelslike recognize objects and their relationships, it still lacks the
LLaVA-UHD[39]andInternVL[6]haveexploreddynamic grounding capability to accurately locate these objects in
high-resolution techniques, improving results in areas like images and to integrate different representations of a sin-
OCR. However, these models often lack systematic align- gle object. In this stage, we therefore train the model to
mentsamongobjecttexts,coordinates,andimages,limiting achievefine-grainedalignmentsamongobjecttexts,coordi-
thefullintegrationofthesemulti-scalerepresentations. nates,andimages,fullysharingtheirmulti-scaleknowledge
for each representation. We utilize original visual ground-
3.Method ingdatasetssuchasRefCOCO[15],RefCOCO+[15],Re-
fCOCOg [26] and Visual Genomes [16], along with a de-
Inthispaper, wefirstintroduceanovelfine-grainedvisual
velopedmulti-scalefine-grainedenhancementdatasynthe-
knowledge alignment method that harnesses the potential
sispipeline(detailsprovidedinthefollowingsubsection)to
of MLLMs by aligning object texts, coordinates, and im-
constructafine-grainedgroundingdataset.Theinstancesin
ages across multiple scales, as shown in Fig. 2(a). Our
thetrainingdatacanbecategorizedintothreeclasses:
methodconsistsofthreetrainingstagesthatprogressfrom
3Figure3.ThemodelarchitectureofourproposedTinyGroundingGPT.Itutilizesmulti-scalevisualencodersandsupportsqueriesregarding
differentobjectrepresentations.Objectimagesarecroppedandzoomedfromtheinputimageaccordingtotheinputcoordinates.
• Object Texts and Coordinates Alignment: The model different representations of various objects. 2) Grounding
referstocorrespondingcoordinatesforagivenobjecttext Description Data: This dataset prompts the model to pro-
description or describes a region based on input coordi- vide a detailed description of the image to connect multi
nates. objectsinone-roundconversations,wherethegeneratedob-
• Object Images and Coordinates Alignment: Given an jecttextsareenhancedwithcoordinatestoconfirmtheirex-
augmentedobjectimage,themodelidentifiesitslocation istenceandeffectivelyintegrategroundinginformation.
within the image. When provided with coordinates, the This method enables us to leverage the fine-grained
modelselectsthemostrelevantobjectimages. groundingalignmentlearnedinthesecondstagetoenhance
• ObjectTextsandImagesAlignment: Themodelselects the model’s global grounding alignment. Additionally, we
the most relevant augmented object image based on the trainboththeLLMandtheprojectorinthisstage.
input question or answers inquiries about the relation-
3.2. Multi-scale Fine-grained Enhancement Data
shipsinvolvingaugmentedobjectimages.
SynthesisPipeline
Throughoutthetrainingprocess,wetrainboththeLLM
and the projector. Afterwards, the model can effectively AsshowninFig.2(b),wedevelopamulti-scalefine-grained
perform fine-grained image understanding by achieving enhancementdatasynthesispipeline,andconstructamulti-
high-level alignments among object texts, images, and scalefine-grainedgroundingdataset(inStage2)aswellasa
coordinates, while sharing multi-scale knowledge across globalgroundingdataset(inStage3). Specifically,givenan
each representation. Additionally, the forms of input image,weperformthefollowingsteps:
objectstothemodelhavebeenexpanded. ObjectRecognitionWeemployexpertmodelsorMLLMs
forobjectdetectionintheinputimages,generatingalistof
DetailedGlobalKnowledgeAlignmentDespiteachieving identified objects, referred to as L . It is crucial to ensure
1
afine-grainedunderstandingofmulti-modaldatainthepre- thatalllistedobjectsexistintheimagetoavoidintroducing
vious stage, the model lacks systematic training for global hallucinationinthesubsequentdatagenerationprocess. A
image comprehension and the ability to connect different promptexampleforGPT-4VisprovidedinAppendixFig.7.
objectswithvariedrepresentations. Specifically,inthepre- Object Grounding In addition to the object text and co-
viousstage,onlytherepresentationsofindividualobjectsin ordinate pairs in the original datasets such as RefCOCO,
eachtrainingsamplewerealigned. Inthisstage,ourgoalis we utilize the object list L and the corresponding image
1
tofurtheralignandintegratemultipleobjectswithinasin- to apply grounding models for each object in order to ob-
gleimageinputtoenhanceglobalknowledgelearning. To tain bounding box coordinates. In this paper, we employ
achievethis,inadditiontoutilizingcommonimageannota- GroundingDINO [23] to locate the objects in the list and
tiondatasetsforinstructiontuning,includingLLaVA-v1.5- filter out those with low confidence, resulting in an object
mix665k [22] and ShareGPT4V [3], we construct a global boundingboxdictionaryS .
1
grounding dataset with high-level fine-grained alignments Relationship Extraction To explore the relationships
basedonFlickr30KEntities[30]: 1)Multi-roundGround- among objects and provide more material for subsequent
ing Conversation Data: This dataset guides the model to QAgeneration,wepromptGPT-4Vtoextractpotentialre-
achieveaglobalunderstandingoftheimagethroughmulti- lationshipsamongtheobjects. AsseeninAppendixFig.7,
round conversations, requiring it to combine fine-grained given the object list L and image, we obtain a list L
1 2
knowledgeandthoroughlyexploretherelationshipsamong that consisits of triple in the format (< object1 >,<
4relation1>,<object2>). cordingtothecorrespondingcoordinates. Wealsosupport
QA Generation Based on above L , S and L , we use the input and output of object bounding box coordinates
1 1 2
task-specificpromptsforGPTtogeneratedifferentkindsof < loc >, represented in the text format [x1,y1,x2,y2],
datasets(weprovidecaseexamplesinAppendixFigs.8and withvaluesrangingfrom0.000to1.000.
9): (1)256KAdditionalMulti-scaleFine-grainedGround-
ingDataset: comparedtopreviousworks[21]thatfocused 4.Experiments
solely on the alignment between object texts and coordi-
4.1.ExperimentalSetup
nates, we enhance the alignment format by constructing
an additional multi-scale, fine-grained dataset of instances WeemployQwen2.5-3BandQwen2.5-1.5B[34]asthelan-
based on objects and their relationships. This dataset in- guagemodelsforourTinyGroundingGPT.Duringthetrain-
corporates object images, texts, and coordinates, facilitat- ingprocess, allimageswerepaddedtoasquareshapeand
ingmorefine-grainedimageunderstandingandmulti-scale resizedtoaresolutionof336×336.Formoredetailsonthe
alignment. Specifically,inadditiontoQAsthatinvolvede- hyper-parameter settings, training processes and datasets,
scribingobjectsgivencoordinatesorlocatingobjectsbased pleaserefertotheAppendix7.1and 7.2.
on descriptions, we prompt GPT to generate QAs about
4.2.ImageGroundingEvaluation
relationships and mark the objects in the questions. We
then replace these objects with augmented object images ToevaluatetheimagegroundingcapabilityofTinyGround-
in the questions or options. Details can be seen in Ap- ingGPT, we conducted experiments on the Reference Ex-
pendix Fig. 10. (2) 72K Global Grounding Dataset: To pression Understanding (REC) task, which involves locat-
enhance the global alignment and bridge objects with var- ing the bounding box for a given text reference. Our ex-
ious representations, we construct two kinds of datasets: periments utilized three datasets: RefCOCO, RefCOCO+,
1)40KMulti-roundGroundingConversation: Thisdataset andRefCOCOg.WecomparedTinyGroundingGPTagainst
includes multi-turn dialogue formats, focusing on point- variousbaselinemodels,includingend-to-endmulti-modal
to-point questions about local details. 2) 32K Grounding modelslikeUNITER[5],MDETR[14],andUniTAB[40],
Description: Thisdatasetfeaturessingle-turndialoguefor- as well as LLM-based models such as KOSMOS-2,
mats, emphasizing an understanding of overall image de- Shikra, NExTChat, Ferret, and GroundingGPT. Addition-
scriptions with fine-grained grounding information. We ally, smaller models like InternVL2 and Qwen2-VL were
providepromptsinAppendixFig.11andAppendixFig.12. included. We used a unified prompt formatted as “Out-
FilterWefilteroutQAsthatcontainobjectimageswithar- put the coordinate of <exp>”, where “<exp>” repre-
eas that are either too large or too small, as well as those sentsthereferenceexpression. AsshowninTable1,Tiny-
with highIntersection over Union (IoU)among object im- GroundingGPTdemonstratesstrongperformanceacrossall
ages in the options. Additionally, we exclude QAs related datasets, even with smaller LLM sizes (3B and 1.5B),
to objects with low confidence or those with an excessive matchingorexceedingtheperformanceofspecializedfine-
numberofboundingboxes. Thisistoavoidlow-resolution tunedmodelsandlargerMLLMswithadditionalimageper-
noiseorimagereferenceambiguity. ceptionmodules. Notably,the3Bmodelachievedstate-of-
the-artresultsonseveralbenchmarks,attainingthehighest
3.3.TinyGroundingGPT
average accuracy. Furthermore, TinyGroundingGPT-1.5B
Using our proposed alignment method and synthesis data, showedcomparablegroundingresults,outperformingNext-
we train TinyGroundingGPT to demonstrate the effective- Chat-7Bonnearlyalltestsets.
ness of our proposed method. Fig 3 illustrates the overall
4.3.ImageUnderstandingEvaluation
architecture of the TinyGroundingGPT. Images in various
formats are processed through multi-scale vision encoders We evaluated TinyGroundingGPT on seven benchmarks,
to extract features. Specifically, we extract image features providing a comprehensive assessment of its performance
usingthepre-trainedvisualencoderViT-L/14[31]andpre- across various metrics. Experimental results in Table 2
trainedDinov2-L/14[28],concatenatingthemtoobtainim- show that TinyGroundingGPT-3B achieves results compa-
agefeaturesthatincorporateboththeglobalperceptionca- rabletomodelssuchasMiniCPM-V-2,InternVL-2,which
pabilitiesofCLIPandthelocalfine-grainedunderstanding use dynamic high resolution or enriched training data.
of DINOv2 [13]. These features are then mapped to the Compared to models with similar fine-tuning data, includ-
LLMembeddingspaceusinganMLP.Notethatinourpro- ing LLaVA-1.5, GroundingGPT, TinyLLaVA and LLaVA-
posed TinyGroundingGPT, the input supports both global Phi, TinyGroundingGPT-3B demonstrates superior image
imageandobjectimages,eachrepresentedbydifferentspe- understanding capabilities on the VQAv2, GQA, SQA
cial tokens: < image > and < object >. These object and POPE benchmarks, achieving increases of 2.6% on
imagesarecroppedandzoomedfromtheglobalimageac- MMBand1.2%onGQAoverGroundingGPT-7B.Notably,
5RefCOCO RefCOCO+ RefCOCOg
Type Model LLMSize Avg
val testA testB val testA testB val test
UNITER - 81.41 87.04 74.17 75.90 81.45 66.70 74.02 68.67 76.17
Specialist MDETR - 86.75 89.58 81.41 79.52 84.09 70.62 81.64 80.89 81.81
UniTAB - 86.32 88.84 80.61 78.70 83.22 69.48 79.96 79.97 80.89
KOSMOS-2 1.6B 52.32 57.42 47.26 45.48 50.73 42.24 60.57 61.65 52.21
Shikra 7B 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19 82.93
NExT-Chat* 7B 85.50 90.00 77.90 77.20 84.50 68.00 80.10 79.80 80.38
Generalist Ferret* 7B 87.49 91.35 82.45 80.78 87.38 73.14 83.93 84.76 83.91
GroundingGPT 7B 88.02 91.55 82.47 81.61 87.18 73.18 81.67 81.99 83.46
InternVL2+ 2B 82.3 88.2 75.9 73.5 82.8 63.3 77.6 78.3 77.74
Qwen2-VL+ 2B 87.6 90.6 82.3 79.0 84.9 71.0 81.2 80.3 82.11
3B 89.16 92.24 85.38 81.70 87.16 75.09 83.27 84.08 84.76
Generalist TinyGroundingGPT
1.5B 86.76 90.42 81.81 78.86 84.65 70.24 79.88 80.04 81.58
Table1. Performancecomparisononthereferringexpressioncomprehension(REC)task. ”*”indicatesthatthemodelemploysadditional
imageregionperceptionmodulesand”+”indicatesthatthemodelusesdynamichigh-resolution. Thebestresultsarehighlightedinbold,
whilethesecond-bestresultsareunderlined.
Models LLMSize VQAv2 GQA SQAI POPE MMEP MMB LLaVAW
BLIP-2 13B 41.0 41 61 85.3 1293.8 - 38.1
InstructBLIP 7B - 49.2 60.5 - - 36 60.9
InstructBLIP 13B - 49.5 63.1 78.9 1212.8 - 58.2
Shikra 13B 77.4 - - - - 58.8 -
LLaVA-1.5 7B 78.5 62.0 66.8 85.9 1510.7 64.3 63.4
GroundingGPT 7B 78.7 62.1 - 87.4 1454.2 63.8 70.9
Qwen-VL-Chat 7B 78.2 - 68.2 - 1487.5 60.6 -
MiniCPM-V-2+ 2.8B - - - 87.8 - 69.6 69.2
InternVL-2+ 2B - 61.0 - 88.3 1439.6 - 62.5
LLaVA-Phi 2.7B 71.4 - 68.4 86.7 1335.1 59.8 -
TinyLLaVA 2.7B 77.7 61.0 70.1 86.3 1437.3 68.3 67.1
3B 79.3 63.3 70.3 87.9 1423.2 66.4 67.5
TinyGroundingGPT
1.5B 77.9 62.2 63.1 87.6 1392.4 64.2 65.3
Table2.ComparisonofMLLMsonimageunderstandingbenchmarks.Benchmarknamesareabbreviatedduetospacelimits.VQA-v2[9];
GQA[12];SQAI:ScienceQA-IMG[25];POPE[18];MME[8];MMB:MMBench[24];LLaVAW: LLaVA-Bench(In-the-Wild)[22]. ”+”
indicatesthatthemodelusesdynamichigh-resolution.
TinyGroundingGPT-1.5BoutperformsLLaVA-Phi,despite spite a decrease of 27.77% in the ’Yes’ metric. Compared
its larger parameter count, on most benchmarks. Overall, toGroundingGPT-7B,our3BmodelexcelledinthePopu-
TinyGroundingGPT, optimized by our multi-scale visual larandAdversarialsubsetsforbothaccuracyandF1score.
knowledge alignment method, achieved impressive results Similarly, TinyGroundingGPT-1.5B achieved higher accu-
acrossmultipleevaluationsets. racyandF1scorethansomelargermodelslikeShikrawhile
maintainingalower’Yes’score. Thissuperiorperformance
4.4.ObjectHallucinationEvaluation
can be attributed to its fine-grained knowledge alignment
We evaluated MLLMs for object hallucination, as shown frombothglobalandlocalperspectivesduringtraining.
in Table 3. Higher accuracy and F1-score metrics, along
4.5.AblationStudy
withalower’Yes’metric,indicatebetterperformance. Our
TinyGroundingGPT yielded outstanding results across all Ablation Study on Additional Multi-scale Fine-grained
three sampling subsets. Notably, TinyGroundingGPT-3B Grounding Dataset. In Stage 2, compared to the tradi-
outperformed larger models like InstructBLIP-13B in the tional methods that rely solely on alignment datasets for
challenging Adversarial subset, achieving an increase of object texts and coordinates, we utilized our constructed
14.67% in accuracy and a 8.90% increase in F1 score, de- additional multi-scale fine-grained grounding datasets
6Random Popular Adversarial
Models LLMSize
AccuracyF1-Score Yes AccuracyF1-Score Yes AccuracyF1-Score Yes
LLaVA 7B 72.16 78.22 76.29 61.37 71.52 85.63 58.67 70.12 88.33
mPLUG-Owl 7B 53.97 68.39 95.63 50.90 66.94 98.57 50.67 66.82 98.67
MiniGPT-4 13B 79.67 80.17 52.53 69.73 73.02 62.20 65.17 70.42 67.77
InstructBLIP 13B 88.57 89.27 56.57 82.77 84.66 62.37 72.10 77.32 73.03
Shikra 7B 86.90 86.19 43.26 83.97 83.16 45.23 83.10 82.49 46.50
GroundingGPT 7B 89.79 89.22 43.13 88.23 87.38 43.23 86.17 85.50 45.43
3B 89.93 89.47 43.08 88.56 87.90 43.43 86.77 86.22 45.26
TinyGroundingGPT
1.5B 89.59 88.98 42.92 88.67 87.90 42.87 86.74 86.04 44.77
Table3. ResultsonthePOPEbenchmarkforobjecthallucinationevaluation. ”Yes”representstheprobabilityofpositiveanswerstothe
givenquestion.
for TinyGroundingGPT, which enables us to achieve roundgroundingconversationsandgroundingdescriptions.
multi-scale alignment among object texts, images, and
coordinates. The ablation study presented in Table 4
showsthatourproposedmulti-scalefine-grainedalignment Size GlobalAlign GQA VQAv2 SQA POPE MMB
outperformedthetraditionalreferringdatathatonlyaligns 61.7 77.4 65.6 86.6 63.1
3B
object texts with coordinates. Whether for the 3B or 1.5B ✓ 63.3 79.3 70.3 87.9 66.4
TinyGroundingGPT,ourmethodenhancedperformanceon 60.3 77.3 62.1 86.4 63.0
1.5B
the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. ✓ 62.2 77.9 63.1 87.6 64.2
For instance, on the RefCOCO+ benchmark, there was an
increase of 1.67% for the 3B model and an increase of Table 5. Ablation study on our Global Grounding Datasets in
0.87%forthe1.5Bmodel,demonstratingtheeffectiveness Stage 3. If the model is trained without global alignment, it in-
ofourproposedfine-grainedalignmentsanddatasets. dicatesthatwedonotusethesedatasetstofurtheraligndifferent
objectsrepresentedbytexts,coordinates,andimages.
Size Multi-scaleAlign RefCOCO RefCOCO+ RefCOCOg Ablation Study on Models with Larger Parameter.
87.35 78.89 83.25 We applied our method to TinyGroundingGPT with the
3B
✓ 88.50 80.56 83.69 larger language model Qwen2.5-7B, as illustrated in Ap-
85.93 77.05 79.54 pendix7.3. Theresultsshowanaverageincreaseof0.47%
1.5B
✓ 86.33 77.92 79.96 across all grounding task benchmarks, highlighting the ef-
fectivenessandgeneralizabilityofourproposedmethod.
Table 4. Ablation study on our Additional Multi-scale Fine-
grained Grounding Dataset in Stage 2. If the model is trained
withoutmulti-scalealignment,itindicatesthatweareusingonly
traditionalreferringdata(whichalignsonlytextsandcoordinates).
Wereporttheaverageaccuracyforeachbenchmark.
AblationStudyonGlobalGroundingDatasets. InStage
3, we utilized our constructed Global Grounding Datasets
forTinyGroundingGPTtobridgedifferentobjectswithvar-
ied representations and enhance global image comprehen-
Figure4. AcasefortheoutputsofourTinyGroundingGPTwhen
sion. To evaluate the effectiveness of this strategy, we
theinputiseitherenhancedwithcoordinatesornot. Probability
conducted an ablation study, with the results presented in valuesindicatethelikelihoodofgeneratingcorrespondingtokens.
Table 5. Notably, the results on the POPE benchmarks
demonstrated a reduction in hallucinations. Overall, sig-
nificantimprovementsinvisualunderstandingbenchmarks 5.Discussion
underscored the value of detailed global knowledge learn-
5.1.Effectivenessoffine-grainedknowledge
ing and the Global Grounding Datasets, which enhanced
globalobjectalignmentbyconnectingdifferentobjectsrep- In addition to improved grounding ability, our proposed
resented by texts, coordinates, and images through multi- fine-grained visual knowledge alignment method also en-
7Figure5.Visualizationoftheattentionmapforimagepatcheswithdifferentobjectrepresentationoutputs(texts,coordinates,andimages,
underlined). Theredboundingboxdenotesthetargetregion. Theattentionatthefourcornersservesasanchorsforgrounding, while
attentionatspecificobjectshighlightstheirimportance.
hances comprehensive image understanding. We provide inputimagepatches.
examplesofimagedescriptionsgeneratedbyTinyGround- As shown in Fig. 5, the attention maps of our Tiny-
ingGPT in Appendix Fig. 13. Notably, our model, opti- GroundingGPT reveal distinct location attributions, unlike
mized through multi-granularity knowledge alignment, ef- the baseline GroundingGPT-7B. For object coordinates,
fectivelyavoidsincorrectornonexistentobjectdescriptions. highattentionscoresareconcentratedatthefourcornersof
Tofurtherassessthisannotationability,weselected50im- theimage,servingasanchorsforboundingboxcoordinates,
agesfromRefCOCO-testandutilizedGPT-4Vtoscoreim- aswellasatthelocationsoftheintendedobjectsmentioned
agedescriptionsproducedbydifferentmethods.Asdetailed intheprompt.Whenpromptedtodescribeaspecificregion,
inAppendix7.4,TinyGroundingGPTachievesbetterover- the model directs increased attention to the corresponding
all quality and richness compared to GroundingGPT-7B object patches. For the output of an object image, the at-
andQwen2-VL-2B.Additionally, thisalignedfine-grained tention values between image patches and the target ob-
knowledgecanbeusedtoenhanceinputquestionsforTiny- ject highlight relevant regions and reinforce grounding an-
GroundingGPT, resulting in more convincing and certain chors. This indicates that TinyGroundingGPT effectively
responses. As shown in Fig. 4, compared to directly ask- learnsbothalignedfeaturesandgroundinginformationfor
ingquestionsaboutobjectsinanimage,enhancingtheob- objectimages. Insummary,ourfindingsunderscoretheef-
jecttextswithcoordinatesinthequeriesyieldsmoreaccu- fectiveness of the proposed fine-grained visual knowledge
rateandpersuasiveresponsesfromourTinyGroundingGPT. alignment method, achieving high-level alignment among
Thisfindingprovidesinsightsforfurtherunleashingthepo- different object representations. This provides insights for
tentialoffine-grainedMLLMs. furtherexplainingMLLMs,particularlyingroundingtasks.
MorevisualizationscanbefoundinAppendixFig.14.
5.2.Interpretabilityforhigh-levelalignments
6.Conclusion
Grounding MLLMs fundamentally model the maximum
likelihood output based on visual inputs and text prompts.
In this paper, we introduce a novel fine-grained visual
Byconditioningonthereferringprompt,themodelidenti-
knowledge alignment method for MLLMs to address the
fieswhichpartsoftheimagesignificantlyinfluencetheout-
limitations of fine-grained alignments in previous works.
put. Consequently,theattentionmapingroundingMLLMs
Our method progresses from easy to hard, emphasizing
not only enhances interpretability but also illustrates the
multi-scalefine-grainedalignmentsamongobjecttexts,co-
alignment between the model’s output and the input im-
ordinates, and images from both local and global perspec-
age. To demonstrate the effectiveness of our multi-scale
tives. This empowers models to effectively learn fine-
fine-grained grounding capability, we visualize the atten-
grained knowledge and facilitates reasoning and ground-
tion map in the last layer of our TinyGroundingGPT. The
ing tasks. Additionally, we develop a multi-scale fine-
processforobtainingtheheatmapofattentioninvolvessev-
grainedenhancementdatasynthesispipelinethatleverages
eralsteps: (1)weselecttheattentionscoresbetweenimage
open-sourcedatasetsandadvancedmodelstogenerateover
patches and object representations (i.e., texts, coordinates,
300K essential training samples. Building on this founda-
andimages);(2)wesumtheattentionscoresacrossthedi-
tion,wetrainTinyGroundingGPT,aseriesofsmallermod-
mensionsofboththeattentionheadsandobjectrepresenta-
els(1.5Band3Bparameters)optimizedthroughhigh-level
tions;(3)Wemapthenormalizedattentionscoresontothe
alignments,capableofhandlingvariousvisualandground-
8ingtasks,oftensurpassinglargermodels. Experimentalre- WeilinZhao,etal.Minicpm:Unveilingthepotentialofsmall
sultsdemonstratetheeffectivenessofourproposedmethod language models with scalable training strategies. arXiv
andthegenerateddatasets. Ourworkcontributestothead- preprintarXiv:2404.06395,2024. 1
vancementofpracticalapplicationsforMLLMs. [12] DrewAHudsonandChristopherDManning. Gqa: Anew
dataset for real-world visual reasoning and compositional
References questionanswering. InProceedingsoftheIEEE/CVFcon-
ference on computer vision and pattern recognition, pages
[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan 6700–6709,2019. 6
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren [13] Dongsheng Jiang, Yuchen Liu, Songlin Liu, Jin’e Zhao,
Zhou.Qwen-vl:Afrontierlargevision-languagemodelwith Hao Zhang, Zhen Gao, Xiaopeng Zhang, Jin Li, and
versatileabilities.arXivpreprintarXiv:2308.12966,2023.1, Hongkai Xiong. From clip to dino: Visual encoders shout
2,3 in multi-modal large language models. arXiv preprint
[2] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, arXiv:2310.08825,2023. 5
Feng Zhu, and Rui Zhao. Shikra: Unleashing multi- [14] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
modal llm’s referential dialogue magic. arXiv preprint Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-
arXiv:2306.15195,2023. 1,3 modulateddetectionforend-to-endmulti-modalunderstand-
[3] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui ing. InProceedingsoftheIEEE/CVFinternationalconfer-
He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: enceoncomputervision,pages1780–1790,2021. 5
Improving large multi-modal models with better captions. [15] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
arXivpreprintarXiv:2311.12793,2023. 1,4 Tamara Berg. Referitgame: Referring to objects in pho-
[4] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xi- tographsofnaturalscenes. InProceedingsofthe2014con-
aoyan Yang, Qiang Li, Yue Shen, Jinjie Gu, and Huajun ferenceonempiricalmethodsinnaturallanguageprocessing
Chen. Unifiedhallucinationdetectionformultimodallarge (EMNLP),pages787–798,2014. 3
languagemodels.arXivpreprintarXiv:2402.03190,2024.2 [16] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
[5] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, KenjiHata,JoshuaKravitz,StephanieChen,YannisKalan-
FaisalAhmed,ZheGan,YuCheng,andJingjingLiu.Uniter: tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Universal image-text representation learning. In European Connectinglanguageandvisionusingcrowdsourceddense
conference on computer vision, pages 104–120. Springer, imageannotations.Internationaljournalofcomputervision,
2020. 5 123:32–73,2017. 3
[6] ZheChen,WeiyunWang,HaoTian,ShenglongYe,Zhang- [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
wei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Blip-2: Bootstrapping language-image pre-training with
Luo, Zheng Ma, et al. How far are we to gpt-4v? closing frozen image encoders and large language models. In In-
thegaptocommercialmultimodalmodelswithopen-source ternational conference on machine learning, pages 19730–
suites. arXivpreprintarXiv:2404.16821,2024. 1,2,3 19742.PMLR,2023. 3
[7] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, [18] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Zhao, and Ji-Rong Wen. Evaluating object hallucina-
Lin, Bo Zhang, et al. Mobilevlm v2: Faster and tion in large vision-language models. arXiv preprint
strongerbaselineforvisionlanguagemodel. arXivpreprint arXiv:2305.10355,2023. 6
arXiv:2402.03766,2024. 3 [19] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng
[8] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Zhong,YixinChen,RuihangChu,ShaotengLiu,andJiaya
MengdanZhang,XuLin,JinruiYang,XiawuZheng,KeLi, Jia. Mini-gemini: Mining the potential of multi-modality
XingSun,etal. Mme: Acomprehensiveevaluationbench- visionlanguagemodels. arXivpreprintarXiv:2403.18814,
markformultimodallargelanguagemodels. arXivpreprint 2024. 1,3
arXiv:2306.13394,2023. 6 [20] ZhaoweiLi,WeiWang,YiQingCai,XuQi,PengyuWang,
[9] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBa- Dong Zhang, Hang Song, Botian Jiang, Zhida Huang, and
tra,andDeviParikh. Makingthevinvqamatter: Elevating TaoWang.Unifiedmllm:Enablingunifiedrepresentationfor
the role of image understanding in visual question answer- multi-modal multi-tasks with large language model. arXiv
ing. In Proceedings of the IEEE conference on computer preprintarXiv:2408.02503,2024. 3
visionandpatternrecognition,pages6904–6913,2017. 6 [21] ZhaoweiLi,QiXu,DongZhang,HangSong,YiqingCai,Qi
[10] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Qi,RanZhou,JuntingPan,ZefengLi,VuTu,etal.Ground-
Nakhost,YasuhisaFujii,AlexanderRatner,RanjayKrishna, inggpt: Languageenhancedmulti-modalgroundingmodel.
Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! InProceedingsofthe62ndAnnualMeetingoftheAssocia-
outperforminglargerlanguagemodelswithlesstrainingdata tionforComputationalLinguistics(Volume1:LongPapers),
andsmallermodelsizes. arXivpreprintarXiv:2305.02301, pages6657–6678,2024. 1,3,5
2023. 3 [22] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
[11] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Visual instruction tuning. Advances in neural information
Cui,XiangLong,ZhiZheng,YeweiFang,YuxiangHuang, processingsystems,36,2024. 3,4,6
9[23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao [35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste
Zhu, etal. Groundingdino: Marryingdinowithgrounded Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
pre-training for open-set object detection. arXiv preprint Llama: Open and efficient foundation language models.
arXiv:2303.05499,2023. 4 arXivpreprintarXiv:2302.13971,2023. 1,2
[24] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang [36] WeihanWang,QingsongLv,WenmengYu,WenyiHong,Ji
Zhang,WangboZhao,YikeYuan,JiaqiWang,ConghuiHe, Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan
ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelan Song,etal. Cogvlm: Visualexpertforpretrainedlanguage
all-around player? In European Conference on Computer models. arXivpreprintarXiv:2311.03079,2023. 1
Vision,pages216–233.Springer,2025. 6 [37] WeiWang,ZhaoweiLi,QiXu,YiqingCai,HangSong,Qi
[25] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Qi,RanZhou,ZhidaHuang,TaoWang,andLiXiao. Qcrd:
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Quality-guidedcontrastiverationaledistillationforlargelan-
Ashwin Kalyan. Learn to explain: Multimodal reasoning guagemodels. arXivpreprintarXiv:2405.13014,2024. 3
via thought chains for science question answering. In The [38] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai,
36thConferenceonNeuralInformationProcessingSystems ZhaoyangLiu,WenhaiWang,ZheChen,XizhouZhu,Lewei
(NeurIPS),2022. 6 Lu, Tong Lu, et al. Visionllm v2: An end-to-end general-
[26] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana istmultimodallargelanguagemodelforhundredsofvision-
Camburu, Alan L Yuille, and Kevin Murphy. Generation languagetasks. arXivpreprintarXiv:2406.08394,2024. 3
andcomprehensionofunambiguousobjectdescriptions. In [39] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin
ProceedingsoftheIEEEconferenceoncomputervisionand Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong
patternrecognition,pages11–20,2016. 3 Sun, and Gao Huang. Llava-uhd: an lmm perceiving any
[27] OpenAI. Gpt-4 technical report. arXiv preprint aspect ratio and high-resolution images. arXiv preprint
arXiv:2303.08774,2023. 2 arXiv:2403.11703,2024. 3
[28] Maxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy [40] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.
DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Unitab: Unifyingtextandboxoutputsforgroundedvision-
Dinov2:Learningrobustvisualfeatureswithoutsupervision. languagemodeling. InEuropeanConferenceonComputer
arXivpreprintarXiv:2304.07193,2023. 5 Vision,pages521–539.Springer,2022. 5
[29] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan [41] YuanYao,TianyuYu,AoZhang,ChongyiWang,JunboCui,
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground- HongjiZhu,TianchiCai,HaoyuLi,WeilinZhao,ZhihuiHe,
ingmultimodallargelanguagemodelstotheworld. arXiv etal.Minicpm-v:Agpt-4vlevelmllmonyourphone.arXiv
preprintarXiv:2306.14824,2023. 1,3 preprintarXiv:2408.01800,2024. 1,3
[30] Bryan A Plummer, Liwei Wang, Chris M Cervantes, [42] HaoxuanYou,HaotianZhang,ZheGan,XianzhiDu,Bowen
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb- Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and
nik. Flickr30k entities: Collecting region-to-phrase corre- Yinfei Yang. Ferret: Refer and ground anything anywhere
spondences for richer image-to-sentence models. In Pro- atanygranularity. arXivpreprintarXiv:2310.07704,2023.
ceedingsoftheIEEEinternationalconferenceoncomputer 1,3
vision,pages2641–2649,2015. 4 [43] AoZhang,LimingZhao,Chen-WeiXie,YunZheng,WeiJi,
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya andTat-SengChua. Next-chat: Anlmmforchat,detection
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, andsegmentation. arXivpreprintarXiv:2311.04498,2023.
AmandaAskell,PamelaMishkin,JackClark,etal.Learning 3
transferable visual models from natural language supervi- [44] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan
sion.InInternationalconferenceonmachinelearning,pages Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang,
8748–8763.PMLR,2021. 5 Chunyuan Li, et al. Llava-grounding: Grounded vi-
[32] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdel- sual chat with large multimodal models. arXiv preprint
rahman Shaker, Salman Khan, Hisham Cholakkal, Rao M arXiv:2312.02949,2023. 3
Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. [45] ShilongZhang,PeizeSun,ShoufaChen, MinXiao,Wenqi
Glamm: Pixelgroundinglargemultimodalmodel. InPro- Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo.
ceedingsoftheIEEE/CVFConferenceonComputerVision Gpt4roi:Instructiontuninglargelanguagemodelonregion-
andPatternRecognition,pages13009–13018,2024. 3 of-interest. arXivpreprintarXiv:2307.03601,2023. 3
[33] FangxunShu,YueLiao,LeZhuo,ChenningXu,Guanghao [46] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo,
Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, XienLiu,JiWu,andLeiHuang. Tinyllava:Aframeworkof
Siming Fu, et al. Llava-mod: Making llava tiny via moe small-scalelargemultimodalmodels,2024. 1
knowledge distillation. arXiv preprint arXiv:2408.15881, [47] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
2024. 3 hamed Elhoseiny. Minigpt-4: Enhancing vision-language
[34] QwenTeam.Qwen2.5:Apartyoffoundationmodels,2024. understandingwithadvancedlargelanguagemodels. arXiv
5 preprintarXiv:2304.10592,2023. 1,3
107.Appendix 7.3.Groundingabilityforlargermodel
7.1.Implementationdetails We further apply our proposed fine-grained visual knowl-
edge alignment method to TinyGroundingGPT, utilizing
We present additional details about our experimental con-
Qwen2.5-7B as the language model with larger parame-
figuration to facilitate the reproduction of our model. The
ters,toevaluateitsimagegroundingcapability. Theresults,
hyperparametersforallstagesaresummarizedinTable6.
summarized in Table 8, demonstrate an average increase
of 0.47% across all grounding task benchmarks. Specif-
Stage1
Size Stage2 Stage3 ically, on RefCOCO-testB, accuracy improves by 1.01%,
Pretrain Finetune highlightingtheeffectivenessofourproposedmethod.
Batchsize 32 32 32 16
7.4.Assessmentforimageannotation
Learningrate 1e-3 2e-5 2e-5 2e-5
Epochs 1 1 1 2 As illustrated in Section 5.1, we provided examples of
Learningschedule Cosinedecay image descriptions generated by TinyGroundingGPT in
Fig.13. Moreover,weselected50imagesfromRefCOCO-
Warm-upratio 0.03 0.03 0.03 0.03
testandutilizedGPT-4Vtoevaluateimagedescriptionspro-
Weightdecay 0 0 0 0
ducedbyvariousmethods. Weassessedtheimagedescrip-
BF16 ✓ ✓ ✓ ✓
tionsusingscoresrangingfrom1to5acrossthreeperspec-
TF32 ✓ ✓ ✓ ✓
tives: ”Quality,”whichreflectsoverallquality;”Richness,”
DeepSpeedstage ZeRO2
which measures the diversity of object descriptions; and
GPUs 8xA100 ”Accuracy,” which pertains to precision. The prompt used
forGPT-4VandthescoringdetailsarepresentedinTable9.
Table6.Thehyperparametersformodeltraining.
AstheresultssummarizedinTable10,TinyGroundingGPT
achieved better overall quality and richness compared to
7.2.Datasetdetails
GroundingGPT-7BandQwen2-VL-2B.
We provide additional details about the datasets we uti-
lized, as summarized in Table 7. Moreover, the distri- Model Quality Richness Accuracy
bution of fine-grained data and image description length
GPT-4V 4.24 4.10 4.88
are shown in Fig. 6. We also include additional exam-
GroundingGPT-7B 3.68 3.20 3.38
ples of the generated datasets in Fig. 8 for Stage 2 and in
Qwen2-VL-2B 3.90 3.64 4.18
Fig. 9 for Stage 3. As described in Section 3.2, we devel-
opedamulti-scalefine-grainedenhancementdatasynthesis TinyGroundingGPT-3B 4.04 3.90 3.66
pipeline, which includes the construction of a multi-scale
fine-grained grounding dataset (in Stage 2) and a global Table 10. The assessment for image annotation by GPT-4V in-
grounding dataset (in Stage 3). In Fig. 7, we present the cludes”Quality”foroverallquality, ”Richness”forthediversity
prompt messages used for object recognition and relation ofobjectdescriptions, and”Accuracy”forprecision. Scoresare
basedontheaverageratings(1-5)from50samples.”Coordinates”
extractiontoprepareadditionaldatamaterial. Fig.10illus-
denoteswhetheroutputtingobjecttextswithcoordinates.
trates the detailed processing steps involved in construct-
ingthemulti-scalefine-grainedgroundingdataset. Further-
more,Figs.11and12outlinetheprocessingstepsforcon- 7.5.Morevisualizations
structingtheglobalgroundingdataset.
AsillustratedinSection5.2,wevisualizedthelast-layerat-
tention maps of both the GroundingGPT-7B baseline and
Stage Dataset Samples ourTinyGroundingGPT-1.5B.Additionalvisualizationsare
displayed in Fig. 14. As shown, TinyGroundingGPT re-
Stage1 LLaVA-Pretrain-595k 595K
veals more distinct location attributions, indicating that it
Text-coordinatepairs 4.1M
effectivelylearnedmulti-scalefine-grainedknowledgeand
Stage2 Fine-graineddata Image-coordinatepairs 210K achievedhigh-levelalignmentsamongobjecttexts,coordi-
Text-imagepairs 46K nates, and images. This provides insights for further ex-
LLaVA-v1.5,ShareGPT4V 665K plainingMLLMs,particularlyingroundingtasks. Wealso
Stage3 SFTdata Grounding-conv 40K provideademoforutilizingTinyGroundingGPTinFig.15.
Grounding-description 32K
Table7.Thedatasetdetailsusedformodeltraining.
11Figure6.Thedistributionoffine-graineddatainStage2andgeneratedgrounding-descriptionlengthinStage3.
RefCOCO RefCOCO+ RefCOCOg
Model Multi-scaleAlign Avg
val testA testB val testA testB val test
90.28 92.62 86.47 83.98 88.08 77.90 85.27 85.43 86.25
TinyGroundingGPT-7B
90.72 92.31 87.48 84.56 88.76 78.71 85.36 85.86 86.72(+0.47)
Table8.Performancecomparisononthereferringexpressioncomprehension(REC)taskforwhetherconductingourproposedMulti-scale
Fine-grainedLocalKnowledgeAlignment.
Evaluatetheimagedescriptionbasedonthefollowingcriteria:
Quality(1-5): Richness(1-5): Accuracy(1-5):
1-Thedescriptionisincoherent,lacks 1-Thedescriptiononlymentionsafew 1 - The description contains multi-
flow, and does not effectively convey basicobjectsorelementsintheimage, plesignificantinaccuraciesorerrorsin
thecontentsoftheimage. without any contextual details or rela- identifying objects, elements, or their
tionships. characteristics.
2-Thedescriptionhassomecoherence 2 - The description includes some ad- 2-Thedescriptionhassomeinaccura-
butisstilldisjointed,withlimitedflow ditionaldetailsabouttheobjectsorele- ciesorerrorsinidentifyingobjects,el-
andincompletecoverageoftheimage. mentsbutlacksdepthintermsoftheir ements,ortheircharacteristics.
relationshipsorbroadercontext.
3 - The description is generally coher- 3 - The description provides a reason- 3 - The description is generally accu-
ent, with reasonable flow, and covers ablelevelofdetailabouttheobjectsand rateinidentifyingtheobjects,elements,
mostofthekeyelementsintheimage. elements,aswellassomeoftheirrela- andtheircharacteristics, withonlymi-
tionshipsorbroadercontext. norinaccuracies.
4 - The description is coherent, with 4 - The description is rich in detail, 4 - The description is highly accurate
goodflow,andcomprehensivelycovers covering a diverse range of objects, inidentifyingtheobjects,elements,and
theimportantaspectsoftheimage. elements, their relationships, and the theircharacteristics,withminimaltono
broadercontextofthescene. inaccuracies.
5 - The description is highly coher- 5 - The description is exceptionally 5-Thedescriptioniscompletelyaccu-
ent,withexcellentflow,andarticulately rich, providing abundant details about rate in identifying all the objects, ele-
captures the essence of the image in a the diverse array of objects, elements, ments,andtheircharacteristics,withno
compellingmanner. their intricate relationships, and the discernibleerrorsorhallucinations.
comprehensivecontextofthescene.
Table9.ThepromptforGPT-4VtoassessdescriptionsfromtheperspectivesofQuality,Richness,andAccuracy.
12Figure7.Thepromptmessageforobjectrecognitionandrelationextraction.
Figure8. OurgeneratedvariouskindsofdatausedinStage2forachievinghigh-levelalignmentsamongtexts,coordinates,andimages,
wherethe<img>denotesthecorrespondingaugmentedobjectimage.
Figure 9. Our generated various kinds of data used in Stage3 for achieving global object alignment, where the < img > denotes the
correspondingaugmentedobjectimage.
13Figure10.Thepromptmessageanduser’sinputexampleusedforgeneratingourFine-grainedGroundingDatasetinStage2.
14Figure11.Thepromptmessageanduser’sinputexampleusedforgeneratingourMulti-roundGroundingConversationDatainStage3.
15Figure12.Thepromptmessageanduser’sinputexampleusedforgeneratingourGroundingDescriptionDatainStage3.
Figure13.AcomparisonofgeneratedimagedescriptionsbetweenTinyGroundingGPTtrainedwithourmethodandwithoutit.
16Figure14. Thevisualizationoftheattentionmapforimagepatcheswithdifferentobjectrepresentationoutputs(texts,coordinates,and
images,whichareunderlined),wheretheredboundingboxdenotesthetargetregion.
Figure15.AdemofortheuseofourTinygroundingGPT.
17