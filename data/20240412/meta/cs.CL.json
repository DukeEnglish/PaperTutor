[
    {
        "title": "Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding",
        "authors": "Yiwen TangJiaming LiuDong WangZhigang WangShanghang ZhangBin ZhaoXuelong Li",
        "links": "http://arxiv.org/abs/2404.07989v1",
        "entry_id": "http://arxiv.org/abs/2404.07989v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07989v1",
        "summary": "Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.",
        "updated": "2024-04-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07989v1"
    },
    {
        "title": "Language Imbalance Can Boost Cross-lingual Generalisation",
        "authors": "Anton SchäferShauli RavfogelThomas HofmannTiago PimentelImanol Schlag",
        "links": "http://arxiv.org/abs/2404.07982v1",
        "entry_id": "http://arxiv.org/abs/2404.07982v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07982v1",
        "summary": "Multilinguality is crucial for extending recent advancements in language\nmodelling to diverse linguistic communities. To maintain high performance while\nrepresenting multiple languages, multilingual models ideally align\nrepresentations, allowing what is learned in one language to generalise to\nothers. Prior research has emphasised the importance of parallel data and\nshared vocabulary elements as key factors for such alignment. In this study, we\ninvestigate an unintuitive novel driver of cross-lingual generalisation:\nlanguage imbalance. In controlled experiments on perfectly equivalent cloned\nlanguages, we observe that the existence of a predominant language during\ntraining boosts the performance of less frequent languages and leads to\nstronger alignment of model representations across languages. Furthermore, we\nfind that this trend is amplified with scale: with large enough models or long\nenough training, we observe that bilingual training data with a 90/10 language\nsplit yields better performance on both languages than a balanced 50/50 split.\nBuilding on these insights, we design training schemes that can improve\nperformance in all cloned languages, even without altering the training data.\nAs we extend our analysis to real languages, we find that infrequent languages\nstill benefit from frequent ones, yet whether language imbalance causes\ncross-lingual generalisation there is not conclusive.",
        "updated": "2024-04-11 17:58:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07982v1"
    },
    {
        "title": "Manipulating Large Language Models to Increase Product Visibility",
        "authors": "Aounon KumarHimabindu Lakkaraju",
        "links": "http://arxiv.org/abs/2404.07981v1",
        "entry_id": "http://arxiv.org/abs/2404.07981v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07981v1",
        "summary": "Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer.",
        "updated": "2024-04-11 17:57:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07981v1"
    },
    {
        "title": "LLoCO: Learning Long Contexts Offline",
        "authors": "Sijun TanXiuyu LiShishir PatilZiyang WuTianjun ZhangKurt KeutzerJoseph E. GonzalezRaluca Ada Popa",
        "links": "http://arxiv.org/abs/2404.07979v1",
        "entry_id": "http://arxiv.org/abs/2404.07979v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07979v1",
        "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose a\nnovel approach to address this problem by learning contexts offline through\ncontext compression and in-domain parameter-efficient finetuning. Our method\nenables an LLM to create a concise representation of the original context and\nefficiently retrieve relevant information to answer questions accurately. We\nintroduce LLoCO, a technique that combines context compression, retrieval, and\nparameter-efficient finetuning using LoRA. Our approach extends the effective\ncontext window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We\nevaluate our approach on several long-context question-answering datasets,\ndemonstrating that LLoCO significantly outperforms in-context learning while\nusing $30\\times$ fewer tokens during inference. LLoCO achieves up to\n$7.62\\times$ speed-up and substantially reduces the cost of long document\nquestion answering, making it a promising solution for efficient long context\nprocessing. Our code is publicly available at\nhttps://github.com/jeffreysijuntan/lloco.",
        "updated": "2024-04-11 17:57:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07979v1"
    },
    {
        "title": "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
        "authors": "Tianbao XieDanyang ZhangJixuan ChenXiaochuan LiSiheng ZhaoRuisheng CaoToh Jing HuaZhoujun ChengDongchan ShinFangyu LeiYitao LiuYiheng XuShuyan ZhouSilvio SavareseCaiming XiongVictor ZhongTao Yu",
        "links": "http://arxiv.org/abs/2404.07972v1",
        "entry_id": "http://arxiv.org/abs/2404.07972v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07972v1",
        "summary": "Autonomous agents that accomplish complex computer tasks with minimal human\ninterventions have the potential to transform human-computer interaction,\nsignificantly enhancing accessibility and productivity. However, existing\nbenchmarks either lack an interactive environment or are limited to\nenvironments specific to certain applications or domains, failing to reflect\nthe diverse and complex nature of real-world computer use, thereby limiting the\nscope of tasks and agent scalability. To address this issue, we introduce\nOSWorld, the first-of-its-kind scalable, real computer environment for\nmultimodal agents, supporting task setup, execution-based evaluation, and\ninteractive learning across various operating systems such as Ubuntu, Windows,\nand macOS. OSWorld can serve as a unified, integrated computer environment for\nassessing open-ended computer tasks that involve arbitrary applications.\nBuilding upon OSWorld, we create a benchmark of 369 computer tasks involving\nreal web and desktop apps in open domains, OS file I/O, and workflows spanning\nmultiple applications. Each task example is derived from real-world computer\nuse cases and includes a detailed initial state setup configuration and a\ncustom execution-based evaluation script for reliable, reproducible evaluation.\nExtensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld\nreveals significant deficiencies in their ability to serve as computer\nassistants. While humans can accomplish over 72.36% of the tasks, the best\nmodel achieves only 12.24% success, primarily struggling with GUI grounding and\noperational knowledge. Comprehensive analysis using OSWorld provides valuable\ninsights for developing multimodal generalist agents that were not possible\nwith previous benchmarks. Our code, environment, baseline models, and data are\npublicly available at https://os-world.github.io.",
        "updated": "2024-04-11 17:56:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07972v1"
    }
]