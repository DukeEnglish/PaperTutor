[
    {
        "title": "Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding",
        "authors": "Yiwen TangJiaming LiuDong WangZhigang WangShanghang ZhangBin ZhaoXuelong Li",
        "links": "http://arxiv.org/abs/2404.07989v1",
        "entry_id": "http://arxiv.org/abs/2404.07989v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07989v1",
        "summary": "Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.",
        "updated": "2024-04-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07989v1"
    },
    {
        "title": "ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback",
        "authors": "Ming LiTaojiannan YangHuafeng KuangJie WuZhaoning WangXuefeng XiaoChen Chen",
        "links": "http://arxiv.org/abs/2404.07987v1",
        "entry_id": "http://arxiv.org/abs/2404.07987v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07987v1",
        "summary": "To enhance the controllability of text-to-image diffusion models, existing\nefforts like ControlNet incorporated image-based conditional controls. In this\npaper, we reveal that existing methods still face significant challenges in\ngenerating images that align with the image conditional controls. To this end,\nwe propose ControlNet++, a novel approach that improves controllable generation\nby explicitly optimizing pixel-level cycle consistency between generated images\nand conditional controls. Specifically, for an input conditional control, we\nuse a pre-trained discriminative reward model to extract the corresponding\ncondition of the generated images, and then optimize the consistency loss\nbetween the input conditional control and extracted condition. A\nstraightforward implementation would be generating images from random noises\nand then calculating the consistency loss, but such an approach requires\nstoring gradients for multiple sampling timesteps, leading to considerable time\nand memory costs. To address this, we introduce an efficient reward strategy\nthat deliberately disturbs the input images by adding noise, and then uses the\nsingle-step denoised images for reward fine-tuning. This avoids the extensive\ncosts associated with image sampling, allowing for more efficient reward\nfine-tuning. Extensive experiments show that ControlNet++ significantly\nimproves controllability under various conditional controls. For example, it\nachieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE,\nrespectively, for segmentation mask, line-art edge, and depth conditions.",
        "updated": "2024-04-11 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07987v1"
    },
    {
        "title": "Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning",
        "authors": "Simon SchrodiDavid T. HoffmannMax ArgusVolker FischerThomas Brox",
        "links": "http://arxiv.org/abs/2404.07983v1",
        "entry_id": "http://arxiv.org/abs/2404.07983v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07983v1",
        "summary": "Contrastive vision-language models like CLIP have gained popularity for their\nversatile applicable learned representations in various downstream tasks.\nDespite their successes in some tasks, like zero-shot image recognition, they\nalso perform surprisingly poor on other tasks, like attribute detection.\nPrevious work has attributed these challenges to the modality gap, a separation\nof image and text in the shared representation space, and a bias towards\nobjects over other factors, such as attributes. In this work we investigate\nboth phenomena. We find that only a few embedding dimensions drive the modality\ngap. Further, we propose a measure for object bias and find that object bias\ndoes not lead to worse performance on other concepts, such as attributes. But\nwhat leads to the emergence of the modality gap and object bias? To answer this\nquestion we carefully designed an experimental setting which allows us to\ncontrol the amount of shared information between the modalities. This revealed\nthat the driving factor behind both, the modality gap and the object bias, is\nthe information imbalance between images and captions.",
        "updated": "2024-04-11 17:58:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07983v1"
    },
    {
        "title": "Language Imbalance Can Boost Cross-lingual Generalisation",
        "authors": "Anton SchäferShauli RavfogelThomas HofmannTiago PimentelImanol Schlag",
        "links": "http://arxiv.org/abs/2404.07982v1",
        "entry_id": "http://arxiv.org/abs/2404.07982v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07982v1",
        "summary": "Multilinguality is crucial for extending recent advancements in language\nmodelling to diverse linguistic communities. To maintain high performance while\nrepresenting multiple languages, multilingual models ideally align\nrepresentations, allowing what is learned in one language to generalise to\nothers. Prior research has emphasised the importance of parallel data and\nshared vocabulary elements as key factors for such alignment. In this study, we\ninvestigate an unintuitive novel driver of cross-lingual generalisation:\nlanguage imbalance. In controlled experiments on perfectly equivalent cloned\nlanguages, we observe that the existence of a predominant language during\ntraining boosts the performance of less frequent languages and leads to\nstronger alignment of model representations across languages. Furthermore, we\nfind that this trend is amplified with scale: with large enough models or long\nenough training, we observe that bilingual training data with a 90/10 language\nsplit yields better performance on both languages than a balanced 50/50 split.\nBuilding on these insights, we design training schemes that can improve\nperformance in all cloned languages, even without altering the training data.\nAs we extend our analysis to real languages, we find that infrequent languages\nstill benefit from frequent ones, yet whether language imbalance causes\ncross-lingual generalisation there is not conclusive.",
        "updated": "2024-04-11 17:58:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07982v1"
    },
    {
        "title": "LLoCO: Learning Long Contexts Offline",
        "authors": "Sijun TanXiuyu LiShishir PatilZiyang WuTianjun ZhangKurt KeutzerJoseph E. GonzalezRaluca Ada Popa",
        "links": "http://arxiv.org/abs/2404.07979v1",
        "entry_id": "http://arxiv.org/abs/2404.07979v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07979v1",
        "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose a\nnovel approach to address this problem by learning contexts offline through\ncontext compression and in-domain parameter-efficient finetuning. Our method\nenables an LLM to create a concise representation of the original context and\nefficiently retrieve relevant information to answer questions accurately. We\nintroduce LLoCO, a technique that combines context compression, retrieval, and\nparameter-efficient finetuning using LoRA. Our approach extends the effective\ncontext window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We\nevaluate our approach on several long-context question-answering datasets,\ndemonstrating that LLoCO significantly outperforms in-context learning while\nusing $30\\times$ fewer tokens during inference. LLoCO achieves up to\n$7.62\\times$ speed-up and substantially reduces the cost of long document\nquestion answering, making it a promising solution for efficient long context\nprocessing. Our code is publicly available at\nhttps://github.com/jeffreysijuntan/lloco.",
        "updated": "2024-04-11 17:57:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07979v1"
    }
]