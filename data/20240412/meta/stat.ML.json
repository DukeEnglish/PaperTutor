[
    {
        "title": "Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method",
        "authors": "Charis StamouliIngvar ZiemannGeorge J. Pappas",
        "links": "http://arxiv.org/abs/2404.07937v1",
        "entry_id": "http://arxiv.org/abs/2404.07937v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07937v1",
        "summary": "We study the quadratic prediction error method -- i.e., nonlinear least\nsquares -- for a class of time-varying parametric predictor models satisfying a\ncertain identifiability condition. While this method is known to asymptotically\nachieve the optimal rate for a wide range of problems, there have been no\nnon-asymptotic results matching these optimal rates outside of a select few,\ntypically linear, model classes. By leveraging modern tools from learning with\ndependent data, we provide the first rate-optimal non-asymptotic analysis of\nthis method for our more general setting of nonlinearly parametrized model\nclasses. Moreover, we show that our results can be applied to a particular\nclass of identifiable AutoRegressive Moving Average (ARMA) models, resulting in\nthe first optimal non-asymptotic rates for identification of ARMA models.",
        "updated": "2024-04-11 17:36:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07937v1"
    },
    {
        "title": "Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing",
        "authors": "Gabriel ArpinoXiaoqi LiuRamji Venkataramanan",
        "links": "http://arxiv.org/abs/2404.07864v1",
        "entry_id": "http://arxiv.org/abs/2404.07864v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07864v1",
        "summary": "We consider the problem of localizing change points in high-dimensional\nlinear regression. We propose an Approximate Message Passing (AMP) algorithm\nfor estimating both the signals and the change point locations. Assuming\nGaussian covariates, we give an exact asymptotic characterization of its\nestimation performance in the limit where the number of samples grows\nproportionally to the signal dimension. Our algorithm can be tailored to\nexploit any prior information on the signal, noise, and change points. It also\nenables uncertainty quantification in the form of an efficiently computable\napproximate posterior distribution, whose asymptotic form we characterize\nexactly. We validate our theory via numerical experiments, and demonstrate the\nfavorable performance of our estimators on both synthetic data and images.",
        "updated": "2024-04-11 15:57:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07864v1"
    },
    {
        "title": "Overparameterized Multiple Linear Regression as Hyper-Curve Fitting",
        "authors": "E. AtzaN. Budko",
        "links": "http://arxiv.org/abs/2404.07849v1",
        "entry_id": "http://arxiv.org/abs/2404.07849v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07849v1",
        "summary": "The paper shows that the application of the fixed-effect multiple linear\nregression model to an overparameterized dataset is equivalent to fitting the\ndata with a hyper-curve parameterized by a single scalar parameter. This\nequivalence allows for a predictor-focused approach, where each predictor is\ndescribed by a function of the chosen parameter. It is proven that a linear\nmodel will produce exact predictions even in the presence of nonlinear\ndependencies that violate the model assumptions. Parameterization in terms of\nthe dependent variable and the monomial basis in the predictor function space\nare applied here to both synthetic and experimental data. The hyper-curve\napproach is especially suited for the regularization of problems with noise in\npredictor variables and can be used to remove noisy and \"improper\" predictors\nfrom the model.",
        "updated": "2024-04-11 15:43:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07849v1"
    },
    {
        "title": "Post-Hoc Reversal: Are We Selecting Models Prematurely?",
        "authors": "Rishabh RanjanSaurabh GargMrigank RamanCarlos GuestrinZachary Chase Lipton",
        "links": "http://arxiv.org/abs/2404.07815v1",
        "entry_id": "http://arxiv.org/abs/2404.07815v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07815v1",
        "summary": "Trained models are often composed with post-hoc transforms such as\ntemperature scaling (TS), ensembling and stochastic weight averaging (SWA) to\nimprove performance, robustness, uncertainty estimation, etc. However, such\ntransforms are typically applied only after the base models have already been\nfinalized by standard means. In this paper, we challenge this practice with an\nextensive empirical study. In particular, we demonstrate a phenomenon that we\ncall post-hoc reversal, where performance trends are reversed after applying\nthese post-hoc transforms. This phenomenon is especially prominent in\nhigh-noise settings. For example, while base models overfit badly early in\ntraining, both conventional ensembling and SWA favor base models trained for\nmore epochs. Post-hoc reversal can also suppress the appearance of double\ndescent and mitigate mismatches between test loss and test error seen in base\nmodels. Based on our findings, we propose post-hoc selection, a simple\ntechnique whereby post-hoc metrics inform model development decisions such as\nearly stopping, checkpointing, and broader hyperparameter choices. Our\nexperimental analyses span real-world vision, language, tabular and graph\ndatasets from domains like satellite imaging, language modeling, census\nprediction and social network analysis. On an LLM instruction tuning dataset,\npost-hoc selection results in > 1.5x MMLU improvement compared to naive\nselection. Code is available at\nhttps://github.com/rishabh-ranjan/post-hoc-reversal.",
        "updated": "2024-04-11 14:58:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07815v1"
    },
    {
        "title": "Quality check of a sample partition using multinomial distribution",
        "authors": "Soumita Modak",
        "links": "http://arxiv.org/abs/2404.07778v1",
        "entry_id": "http://arxiv.org/abs/2404.07778v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07778v1",
        "summary": "In this paper, we advocate a novel measure for the purpose of checking the\nquality of a cluster partition for a sample into several distinct classes, and\nthus, determine the unknown value for the true number of clusters prevailing\nthe provided set of data. Our objective leads us to the development of an\napproach through applying the multinomial distribution to the distances of data\nmembers, clustered in a group, from their respective cluster representatives.\nThis procedure is carried out independently for each of the clusters, and the\nconcerned statistics are combined together to design our targeted measure.\nIndividual clusters separately possess the category-wise probabilities which\ncorrespond to different positions of its members in the cluster with respect to\na typical member, in the form of cluster-centroid, medoid or mode, referred to\nas the corresponding cluster representative. Our method is robust in the sense\nthat it is distribution-free, since this is devised irrespective of the parent\ndistribution of the underlying sample. It fulfills one of the rare coveted\nqualities, present in the existing cluster accuracy measures, of having the\ncapability to investigate whether the assigned sample owns any inherent\nclusters other than a single group of all members or not. Our measure's simple\nconcept, easy algorithm, fast runtime, good performance, and wide usefulness,\ndemonstrated through extensive simulation and diverse case-studies, make it\nappealing.",
        "updated": "2024-04-11 14:14:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07778v1"
    }
]