[
    {
        "title": "OpenBias: Open-set Bias Detection in Text-to-Image Generative Models",
        "authors": "Moreno D'IncàElia PeruzzoMassimiliano ManciniDejia XuVidit GoelXingqian XuZhangyang WangHumphrey ShiNicu Sebe",
        "links": "http://arxiv.org/abs/2404.07990v1",
        "entry_id": "http://arxiv.org/abs/2404.07990v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07990v1",
        "summary": "Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.",
        "updated": "2024-04-11 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07990v1"
    },
    {
        "title": "Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding",
        "authors": "Yiwen TangJiaming LiuDong WangZhigang WangShanghang ZhangBin ZhaoXuelong Li",
        "links": "http://arxiv.org/abs/2404.07989v1",
        "entry_id": "http://arxiv.org/abs/2404.07989v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07989v1",
        "summary": "Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.",
        "updated": "2024-04-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07989v1"
    },
    {
        "title": "ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback",
        "authors": "Ming LiTaojiannan YangHuafeng KuangJie WuZhaoning WangXuefeng XiaoChen Chen",
        "links": "http://arxiv.org/abs/2404.07987v1",
        "entry_id": "http://arxiv.org/abs/2404.07987v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07987v1",
        "summary": "To enhance the controllability of text-to-image diffusion models, existing\nefforts like ControlNet incorporated image-based conditional controls. In this\npaper, we reveal that existing methods still face significant challenges in\ngenerating images that align with the image conditional controls. To this end,\nwe propose ControlNet++, a novel approach that improves controllable generation\nby explicitly optimizing pixel-level cycle consistency between generated images\nand conditional controls. Specifically, for an input conditional control, we\nuse a pre-trained discriminative reward model to extract the corresponding\ncondition of the generated images, and then optimize the consistency loss\nbetween the input conditional control and extracted condition. A\nstraightforward implementation would be generating images from random noises\nand then calculating the consistency loss, but such an approach requires\nstoring gradients for multiple sampling timesteps, leading to considerable time\nand memory costs. To address this, we introduce an efficient reward strategy\nthat deliberately disturbs the input images by adding noise, and then uses the\nsingle-step denoised images for reward fine-tuning. This avoids the extensive\ncosts associated with image sampling, allowing for more efficient reward\nfine-tuning. Extensive experiments show that ControlNet++ significantly\nimproves controllability under various conditional controls. For example, it\nachieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE,\nrespectively, for segmentation mask, line-art edge, and depth conditions.",
        "updated": "2024-04-11 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07987v1"
    },
    {
        "title": "Manipulating Large Language Models to Increase Product Visibility",
        "authors": "Aounon KumarHimabindu Lakkaraju",
        "links": "http://arxiv.org/abs/2404.07981v1",
        "entry_id": "http://arxiv.org/abs/2404.07981v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07981v1",
        "summary": "Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer.",
        "updated": "2024-04-11 17:57:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07981v1"
    },
    {
        "title": "LLoCO: Learning Long Contexts Offline",
        "authors": "Sijun TanXiuyu LiShishir PatilZiyang WuTianjun ZhangKurt KeutzerJoseph E. GonzalezRaluca Ada Popa",
        "links": "http://arxiv.org/abs/2404.07979v1",
        "entry_id": "http://arxiv.org/abs/2404.07979v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07979v1",
        "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose a\nnovel approach to address this problem by learning contexts offline through\ncontext compression and in-domain parameter-efficient finetuning. Our method\nenables an LLM to create a concise representation of the original context and\nefficiently retrieve relevant information to answer questions accurately. We\nintroduce LLoCO, a technique that combines context compression, retrieval, and\nparameter-efficient finetuning using LoRA. Our approach extends the effective\ncontext window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We\nevaluate our approach on several long-context question-answering datasets,\ndemonstrating that LLoCO significantly outperforms in-context learning while\nusing $30\\times$ fewer tokens during inference. LLoCO achieves up to\n$7.62\\times$ speed-up and substantially reduces the cost of long document\nquestion answering, making it a promising solution for efficient long context\nprocessing. Our code is publicly available at\nhttps://github.com/jeffreysijuntan/lloco.",
        "updated": "2024-04-11 17:57:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07979v1"
    }
]