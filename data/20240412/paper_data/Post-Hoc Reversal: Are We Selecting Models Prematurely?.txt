Post-Hoc Reversal: Are We Selecting Models
Prematurely?
RishabhRanjan∗ SaurabhGarg MrigankRaman
StanfordUniversity CarnegieMellonUniversity CarnegieMellonUniversity
ranjanr@stanford.edu sgarg2@andrew.cmu.edu mrigankr@cmu.edu
CarlosGuestrin ZacharyChaseLipton
StanfordUniversity&ChanZuckerbergBiohub CarnegieMellonUniversity
guestrin@stanford.edu zlipton@cmu.edu
Abstract
Trainedmodelsareoftencomposedwithpost-hoctransformssuchastemperature
scaling (TS), ensembling and stochastic weight averaging (SWA) to improve
performance,robustness,uncertaintyestimation,etc. However,suchtransformsare
typicallyappliedonlyafterthebasemodelshavealreadybeenfinalizedbystandard
means. Inthispaper,wechallengethispracticewithanextensiveempiricalstudy.
Inparticular,wedemonstrateaphenomenonthatwecallpost-hocreversal,where
performancetrendsarereversedafterapplyingthesepost-hoctransforms. This
phenomenonisespeciallyprominentinhigh-noisesettings. Forexample,while
basemodelsoverfitbadlyearlyintraining,bothconventionalensemblingandSWA
favorbasemodelstrainedformoreepochs. Post-hocreversalcanalsosuppressthe
appearanceofdoubledescentandmitigatemismatchesbetweentestlossandtest
errorseeninbasemodels. Basedonourfindings,weproposepost-hocselection,a
simpletechniquewherebypost-hocmetricsinformmodeldevelopmentdecisions
suchasearlystopping,checkpointing,andbroaderhyperparameterchoices. Our
experimentalanalysesspanreal-worldvision,language,tabularandgraphdatasets
fromdomainslikesatelliteimaging, languagemodeling, censuspredictionand
socialnetworkanalysis. OnanLLMinstructiontuningdataset,post-hocselection
resultsin>1.5×MMLUimprovementcomparedtonaiveselection.2
1 Introduction
Manywidelyusedtechniquesindeeplearningoperateontrainedmodels;werefertotheseaspost-hoc
transforms. Examplesincludetemperaturescaling(TS)[17],stochasticweightaveraging(SWA)[26]
andensembling[36]. Thesetechniqueshaveshownpromiseforimprovingpredictiveperformance,
robustness, uncertainty estimation, out-of-distribution generalization, and few-shot performance
[4,6,36,53,78]. Typically,thepre-trainingandpost-hocstagesareisolated. Theworkflowis: (1)
pick model architecture, training recipe, hyperparameters, etc. to optimize for individual model
performance;(2)trainoneormoremodels;(3)pickbest-performingcheckpoints;(4)applypost-hoc
transforms. Werefertothisprocedureasnaiveselection.
Inthispaper,wedemonstrateinterestingdrawbacksofnaiveselection.Inalarge-scaleempiricalstudy,
weuncoverpost-hocreversal—aphenomenonwherebypost-hoctransformsreverseperformance
trendsbetweenmodels(Fig. 1). Wedemonstratepost-hocreversalwithrespecttotrainingepochs,
Preprint.Underreview.
∗UndertakeninpartasavisitoratCarnegieMellonUniversity.
2Codeisavailableathttps://github.com/rishabh-ranjan/post-hoc-reversal.
4202
rpA
11
]GL.sc[
1v51870.4042:viXrat1 t2 t1 t2
2.5 b2
Base curve
(no transform)
44 b2 Post-hoc curve
43 gain from transform b1 (SWA+Ens+TS)
under current practices
40 additional gain from p1 Naive selection
1 1. .6
5
b p1
1
our methodology ( Pc ou sr tr -e hn ot
c
p sr ea lc et ci tc ie o)
n
1.3 p2 36 p2 (our methodology)
0 2 18 0 3 18
Epochs Epochs
Figure 1: An illustration of the phenomenon of post-hoc reversal on the FMoW dataset: base
performance at epoch t is worse than at epoch t (b > b ), but post-hoc performance is better
2 1 2 1
(p <p ). Thecurrentpracticeofnaiveselectionconsidersbasemetricstopickmodelsatepocht .
2 1 1
Ourproposedmethodologyofpost-hocselectioninsteadusespost-hocmetricstopickmodelsatepoch
t ,resultingin>2×improvementovernaiveselectioninbothtestlossanderror. SWA+Ens+TS
2
referstothepost-hoctransformobtainedbycomposingSWA,ensemble(Ens)andtemperaturescaling
(TS).Basecurvesshowmeanof8runs,modelsfromwhichconstitutetheensembles. Individualruns
areshowninlightercolors. SeeFig. 5formoredetailedcurvesonthisdataset.
modelsizes,andotherhyperparameterslikelearningrateschedules. Wefurtherestablishthatpost-
hocreversalisarobustphenomenonbyexperimentingonreal-worlddatasetsacrossdomainsand
modalities,withdiversemodelclassesandtrainingsetups.
Post-hocreversalismostprominentonnoisydatasets(Fig.2).Otherphenomenaexacerbatedbynoise
includecatastrophicoverfitting[47]3,doubledescent[52],andloss-errormismatch[17]. Whilethese
phenomenaposechallengestomodeldevelopment,post-hocreversalsuggestsapathtoalleviatethem.
Noisecanarisenotonlyfromlabelingerrors,butalsofrominherentuncertaintyintheprediction
task, such as in next token prediction [57]. Indeed, severe performance degradation has limited
multi-epochtrainingoflargelanguagemodels(LLMs)[75]. Heretoo,post-hocreversalrevealsa
promisingpathforsustainedperformanceimprovementsoverlongertraining.
Basedonourfindings,weproposepost-hocselection—asimpletechniquewherebybasemodels
areselectedbasedonpost-transformperformance. Thetechniqueispracticalasthetransformsof
interestcanbecheaplyincorporatedintothevalidationphaseofthetrainingloop. Post-hocselection
significantlyimprovestheperformanceofthetransformedmodels,with>2×improvementsover
naiveselectioninsomecases(Fig.2). Intermsofabsoluteperformance,post-hocselectionleadsto
approx. 4-pointreductionintesterrorovernaiveselectiononasatelliteimagingdataset(Fig.1). On
anLLMinstructiontuningdataset,underourprocedureacomposedtransformofSWA,ensembleand
TSgives>1.5×MMLUimprovementoveranaiveapplicationofthesametransformonprematurely
selectedmodels.
C-10-N C-100-N C-10-N C-100-N
0.3
Naive selection 5
Post-hoc selection 4
0.2
3
0.1 2
1
0.0 0
Clean Aggre Rand1 Worst Clean Noisy Clean Aggre Rand1 Worst Clean Noisy
(0) (9) (17) (40) (0) (40) (0) (9) (17) (40) (0) (40)
Label Set (Approx. Noise %) Label Set (Approx. Noise %)
Figure2: Acomparisonofnaiveandpost-hocselectiononlabelsetsfromCIFAR-10/100-N(abbr.
C-10/100-N)fortheSWA+TStransform. Onnoisylabelsets,post-hocselectionisoften>2×better.
3Mallinar et al. [47] propose a finer-grained classification, but for the purposes of this work, we use
catastrophicoverfittingtoencapsulateallformsofnon-benignoverfitting.
2
ssoL
tseT
ssoL
tseT
ni
noitcudeR
)yportnE-ssorC(
)yportnE-ssorC(
)%(
rorrE
tseT
rorrE
tseT
ni
noitcudeR
)%
etulosbA(2 RelatedWork
Aslewofempiricalworks[10,15,29,52,54,55]haverevealedbothchallengesandopportunities
forimprovingtheunderstandingandpracticeofdeeplearning. Ourworkexpandsthislistwitha
novelphenomenontyingtogethernoisydatalearningandpost-hoctransforms. Orthogonaltoour
work,anumberoftraining-stagestrategiesfornoisydatahavebeenproposed(see[63]forasurvey).
TSbelongstoafamilyofcalibrationtechniques[2,17,77]proposedwiththegoalofproducing
well-calibrated probabilities. Ensembling is a foundational technique in machine learning, with
simplevariantsroutinelyusedindeeplearning[3,36]. SWA[26]istheculminationofalineof
work [16, 23] seeking to cheaply approximate ensembling. Despite their prevalence, a thorough
understandingofbestpracticesforwieldingthesetechniquesislacking,especiallyinthecontextof
noisydata. Ourworkfillsthisgap. Foramoredetaileddiscussiononrelatedwork,seeApp. A.
3 PreliminariesandBackground
Wedescribeourlearningsetupin§3.1,withemphasisonnoisydata,akeyfocusofthiswork. In
§3.2,weintroducethepost-hoctransformswestudy.
3.1 LearningonNoisyData
Setup. We consider multi-class classification with C classes, input x ∈ X and label y ∈ Y =
{1,...,C}. Training, validation and test sets are drawn i.i.d. from the data distribution D. A
classifier f: Θ×X → RC outputsthelogitvectorz = f(x;θ), givenparametervectorθ ∈ Θ.
PredictedprobabilityofclasskisP [y =k |x]=σ(z) ,whereσisthesoftmaxfunction.
f k
Noise. DataDissaidtobecleanifP [y |x]isone-hotforallx,i.e.,P [y |x]=1{y =y∗(x)}
D D
forsomelabelingfunctiony∗: X →Y. Then,foranyexampleinputx(i)inthedataset,theobserved
labelisy(i) =y∗(x(i)). WhenP [y |x]isnotone-hot,Dissaidtobenoisyandtheobservedlabel
D
isonlyastochasticsampley(i) ∼ P [y | x = x(i)]fromtheunderlyingconditionaldistribution.
D
Noisecanarisedueto(1)non-determinisminthepredictiontarget(2)insufficientinformationinthe
inputcontext,and(3)annotationerrors. SeeApp. B.1forillustratedexamples.
Metrics. A metric M: RC × Y → R compares the predicted logits z with the observed la-
bel y. M (θ) = M[f(·;θ)] = E [M(f(x;θ),y)] denotes the metric computed over D
f (x,y)∼D
givenf andθ. Weusetwometrics(1)classificationerror, orsimplyerror, withMerror(z,y) =
1{argmax z ̸= y} and (2) cross-entropy loss, or simply loss, with Mloss(z,y) = −logσ(z) .
k k y
Theexponentiatedloss,alsocalledperplexity,iscommoninlanguagemodeling,whereitiscomputed
onaper-tokenbasis. Astandardresultstatesthatlossisminimizedifandonlyifthegroundtruth
conditionalprobabilityisrecovered[18]. SeeApp. B.1foradditionalbackground.
3.2 Post-HocTransformsinMachineLearning
Weformallydefineapost-hoctransforminDef.1andgivemathematicalformsforTS,ensembling
andSWAinEqns.1,2and3respectively.
Definition1(Post-HocTransform) Apost-hoctransformT mapsaclassifierf: Θ×X →Y to
anotherclassifierT ◦f: ΘK ×X →Y,forsomeK.
TemperatureScaling(TS).TS[17]involvesscalingthelogitswithatemperatureτ ∈Robtained
byoptimizingthecross-entropylossoverthevalidationset,withmodelparametersfixed(Eqn. 1).
Temperaturescalingpreserveserrorasitdoesnotaffectthepredictedclass.
(cid:20) (cid:21)
1 1
(T ◦f)(x;θ)= f(x;θ), withτ =argminMloss f(·;θ) (1)
TS τ τ val τ
Ensembling. Inthismethod, predictionsfromanensembleofclassifiersarecombined. Indeep
learning, simply averaging the temperature-scaled logits is effective (Eqn. 2). θ ,...,θ are
1 K
obtainedfrommultipletrainingrunswiththesamearchitectureanddataset,withstochasticityfrom
mini-batchsamplingandsometimesrandominitialization.
3K (cid:20) (cid:21)
1 (cid:88) 1 1
(T ◦f)(x;θ ,...,θ )= f(x;θ ), withτ =argminMloss f(·;θ ) (2)
Ens 1 K K τ k k k τ val τ k
k=1
StochasticWeightAveraging(SWA).SWA[26]involvesaveragingweightsθ ,...,θ fromthe
1 K
sametrainingrun(Eqn. 3). BatchNormstatisticsarerecomputedafteraveraging,ifrequired. Unlike
Izmailovetal.[26],wedonotskipweightsfrominitialepochsormodifythelearningrateschedule4.
(cid:32) K (cid:33)
1 (cid:88)
(T ◦f)(x;θ ,...,θ )=f x; θ (3)
SWA 1 K K i
i=1
Compositions. TS, ensembling and SWA can be readily composed. In particular, we consider
SWA+TSandSWA+Ens+TS,forsingle-andmulti-modelsettingsrespectively. Wedenotethemwith
T =T ◦T andT =T ◦T ◦T (explicitformsinApp. B.2).
S+T TS SWA S+E+T TS Ens SWA
4 Post-HocReversal: FormalizationandEmpiricalStudy
Tousepost-hoctransforms,onemustfirstselectmodelstoapplythemto. Currentpracticeistoselect
thebest-performingmodelindependentofpost-hoctransforms,rationalizedbyanimplicitmonotonic-
ityassumption–“better-performingmodelsresultinbetterperformanceaftertransformation”. Aswe
shallsee,thisassumptionisoftenviolatedinpractice. Wecallsuchviolationspost-hocreversal. In
§4.1,weformalizepost-hocreversalanddiscusswaystodetectit. In§4.2,weempiricallystudy
variouskindsofpost-hocreversalwithspecialpracticalrelevance.
4.1 Definitions
First,wegiveageneraldefinitionofpost-hocreversal(Def.2). IfDef.2holdswithφ ’swhich
k
areoptimalforthebasemetricM ,thennaiveselectionbecomessuboptimalasitpicksφ ’s,but
f k
θ ’sarebetterunderthepost-hocmetricM . SincetheentirespaceofparametertuplesΘK can
k T◦f
belarge,westudypost-hocreversalrestrictedtoindexedparameters(Def.3). Indicescanbe,for
example,trainingepochs(§4.2.1),modelsizes(§4.2.2)orhyperparameterconfigurations(§4.2.3).
Definition2(Post-hocreversal) Letapost-hoctransformT mapaclassifierf: Θ×X → Y to
T ◦f: ΘK ×X → Y. T applied to f exhibits post-hoc reversal for a metric M if there exist
(θ ,...,θ ),(φ ,...,φ ) ∈ ΘK such that M (θ ) ≥ M (φ ) for all k = 1,...,K but
1 K 1 K f k f k
M (θ ,...,θ )<M (φ ,...,φ ).
T◦f 1 K T◦f 1 K
Definition3(Index-wisepost-hocreversal) LetIbeasetofindicesandP: I →ΘK mapindices
toparametertuples. WhenDef. 2holdswith(θ ,...,θ )=P(s),(φ ,...,φ )=P(t)forsome
1 K 1 K
s,t∈I,wecallitindex-wisepost-hocreversal.
Diagnosis. Toenableavisualdiagnosisofpost-hocreversal,wedefinebaseandpost-hoccurves
(Def.4)andarelaxednotionofpost-hocreversalforthem(Def.5). Post-hocreversalischaracterized
by non-monotonicity between the base and post-hoc curves, i.e., there exist regions where one
improves while the other worsens. This happens, for instance, when one curve exhibits double
descentbuttheotherdoesn’t. Differentoptimalindicesforthetwocurvesisanotherindicatorof
post-hocreversal.
Definition4(Baseandpost-hoccurves) The base and post-hoc curves Mbase,Mpost: I → R
are given by Mbase(t) = 1 (cid:80)K M (θ ) and Mpost(t) = M (θ ,...,θ ), where
K k=1 f k T◦f 1 K
(θ ,...,θ )=P(t).
1 K
Definition5(Post-hocreversalforcurves) Base and post-hoc curves Mbase,Mpost: I → R ex-
hibitpost-hocreversalwhenthereexists,t ∈ I suchthatMbase(s) ≥ Mbase(t)butMpost(s) <
Mpost(t).
4Thus,ourvariantofSWAishyperparameter-free.
4Test Loss (pre-TS) Test Loss (post-TS) Test Error (%)
3.5 60
Individual C-10-N Worst, Base
Ensemble C-10-N Worst, SWA
3.0
C-10-N Rand1, Base 50
C-10-N Rand1, SWA
2.5
C-10-N Clean, Base
40
C-10-N Clean, SWA
2.0
30
1.5
1.0 20
0.5
10
0 25 50 75 0 25 50 75 0 25 50 75
Epochs Epochs Epochs
Figure3: LossanderrorforCIFAR-10-NClean(approx. 0%noise),Rand1(approx. 17%noise)and
Worst(approx. 40%noise). Exceptforensemblecurves,meanof8runsisshown;individualruns
areinlightershades. Ensemblescomprisemodelsfromthese8runs. Forexample,observepost-hoc
reversalforC-10-NWorst: (1)errorplot: fromepoch5to50,solidred(base)curveworsensbut
solidorange(SWA)curveimproves;(2)errorplot: solidred(base)curvehasadoubledescentbut
dashedred(ensemble)curvedoesnot;(3)lossplots: solidred(base)curvehasadoubledescent
pre-TSbutnotpost-TS;(4)errorplot: besterrorisatapprox. epoch5forsolidred(base)curvebut
atapprox. epoch60fordashedorange(SWAensemble)curve.
4.2 Experiments
4.2.1 Epoch-WisePost-HocReversal
WhentheindicesinDef.3aretrainingepochs,wecallitepoch-wisepost-hocreversal. Weuseθ to
t
denotethemodelattheendofepocht. Forensembles,asuperscriptj denotesthej-thtrainingrun
(outofN runs). t ∈ I mapstoparametersP(t) ∈ ΘK (K = 1forTS;N forensemble;andtfor
SWA)asfollows: P (t)=(θ );P (t)=(θ1,...,θN)5.;P (t)=(θ ,...,θ ).
TS t Ens t t SWA 1 t
Experimentalsetup. WefocusontheCIFAR-Ndataset[68]. CIFAR-10-Nusesthesameimagesas
CIFAR-10butprovidesmultiplehuman-annotatedlabelsets,allowingthestudyofrealisticnoise
patternsofvaryinglevelsinacontrolledmanner. Cleanistheoriginallabelset; Rand1,2,3are3
setsofhumanlabels;AggrecombinesRand1,2,3bymajorityvote;andWorstcombinesthemby
pickinganincorrectlabel,ifpossible. SimilarlyCIFAR-100-Nhastwolabelsets,CleanandNoisy,
withthelatterbeinghuman-labeled. WetrainResNet18[19]modelsfor100epochswithacosine
annealedlearningrate. AdditionaldetailsondatasetsandtrainingsetupareinApp. C.Fig. 3shows
testcurvesonCIFAR-10-NClean,Rand1andWorst. OtherlabelsetsandCIFAR-100-NareinApp.
E.Forclarity,weomittheSWAbasecurveMbase(t)=(M (θ )+···+M (θ ))/tintheplots,
SWA f 1 f t
andsimplyre-usethecurveMbase(t)=M (θ )tocomparewiththepost-hocSWAcurve. While
f t
deviatingfromDef.4,thisbetterreflectsthecurrentpracticeofearlystoppingonthelatestepoch’s
basemetric.
Observations. First,wefocusonthebasecurves: (1)Overfitting: Asnoiseincreases,testcurvesgo
fromasingledescenttoadoubledescenttoaU-shapedcurvewithincreasedoverfitting. (2)Double
descent: Noiseamplifiesdoubledescent,andtheseconddescentworsenswithincreasingnoise(as
comparedtothefirst). (3)Loss-errormismatch: Lossoverfitsmoredrasticallythanerror,leadingtoa
mismatchwithhighernoise. Optimalmodelsforlossanderrorcanbedifferent.
5Ensemblingmodelsfrompossiblyunequalepochsiscoveredin§5
5CIFAR-10-N Worst Exponential LR Cosine LR Constant LR
3.0
3.2
2.8 2.5
2.4
2.0 2.0
1.6 1.5
1.8 1.7
1.7 1.6
1.6 1.5
1.4
1.5
1.3
60 48
55 44
50 40
45 36
0 25 50 0 20 40 0 20 40 0 20 40
ResNet Width Epochs Epochs Epochs
Base Individual Base Individual
SWA Ensemble SWA Ensemble
Figure 4: C-10-N Worst test Figure5: FMoWtestcurvesfor3LRschedules. Notethatthepre-
curves against model size. TSlossissignificantlyhigherthanthepost-TSloss. Forexample,
Best width for solid blue observepost-hocreversalw.r.t. cosineandconstantLRsatepoch
curves is approx. 10 but for 50between: (1)solidblue(base)anddashedblue(ensemble)error
dashedorangecurves,itisap- curves;(2)solidblue(base)andsolidorange(SWA)post-TSloss
prox. 50forerrorandapprox. curves;(3)solidblue(base)curvesforpre-TSandpost-TSloss.
25forpost-TSloss.
Next,weconsiderthegeneralimpactofpost-hoctransforms: (4)Performanceimprovements: TS,
SWAandensemblealwaysimproveperformace,bothindividuallyandincompositionwithlarger
gapsfornoisylabelsets. (5)Post-hocreversal: Post-hocreversalmanifestsasnon-monotonicity
betweenthebaseandpost-hoccurves,especiallyfornoisylabelsets. (6)SWAvsEnsemble: SWA
canrecovermuchoftheensemblegain,buttheoptimalepochoftendiffersalotfromthebasecurve.
(7)Smoothercurves:Basecurvesfluctuatewildly,butSWAandensemblecurvesaresmooth,making
themmorereliableforearlystopping.
Finally, we discuss some benefits from post-hoc reversal: (8) Overfitting: All transforms reduce
overfitting, often reverting performance degradation. (9) Double descent: SWA, ensemble and
compositionsflattenthedoubledescentpeak. TS,ontheotherhand,leadstoadoubledescentfor
somecaseswheretherewasnonebefore. (10)Loss-errormismatch: TSalignsthelossanderror
curves,enablingsimultaneouslygoodlossanderror.
4.2.2 Model-WisePost-HocReversal
Here, indices represent model sizes. Models of all sizes are trained for T epochs, large enough
for convergence. Following [52], we avoid early stopping. Notation-wise, we add a subscript to
θ to indicate the model size s. Parameters are indexed as follows: P (s) = (θ ); P (s) =
TS T,s Ens
(θ1 ,...,θN );P (s)=(θ ,...,θ ).
T,s T,s SWA 1,s T,s
Experimentalsetup. WeparameterizeafamilyofResNet18sbyscalingthenumberoffiltersinthe
convolutionallayers. Specifically,weuse[k,2k,4k,8k]filtersforwidthk. ThestandardResNet18
correspondstok = 64. Otherwisethetrainingsetupissameasbefore. Fig. 4showsthecurves.
Concretely,theindexsetI ={2,4,...,64}isthesetofResNetwidthskdescribedabove.
Observations. Post-hoctransformsimproveperformance(upto≈10pointsforerror)andmitigate
doubledescent. Further,weseeyetanotherwayinwhichhigher-capacitymodelsarebetter: they
givebetterresultsunderpost-hoctransformsevenwhenlower-capacitybasemodelsperformbetter.
6
)ST-erp(
ssoL
)ST-tsop(
ssoL
)%(
rorrE
)ST-erp(
ssoL
)ST-tsop(
ssoL
)%(
rorrETable1:Naivevspost-hoc(ours)selectionforSWA+TSandSWA+Ens+TStransforms. Bettervalues
areinbold. Exceptsomecleancases,post-hocselectionisalwaysbetter,oftenmorethandoubling
theimprovementovernotransformascomparedtonaiveselection. SeeTab. 6and8inApp. Efor
standarddeviations.
Metric→ TestLoss TestError(%)
Transform→ None SWA+TS SWA+Ens+TS None SWA+TS SWA+Ens+TS
Dataset↓ Naive Ours Naive Ours Naive Ours Naive Ours
C-10-NClean 0.435 0.269 0.270 0.234 0.233 9.75 9.09 9.10 8.30 8.24
C-10-NAggre 0.722 0.663 0.585 0.608 0.543 19.20 17.08 16.95 15.88 15.74
C-10-NRand1 1.009 0.968 0.907 0.916 0.859 28.63 27.13 24.84 24.80 23.50
C-10-NWorst 1.511 1.483 1.443 1.437 1.399 46.84 46.12 44.14 44.30 42.88
C-100-NClean 1.508 1.215 1.205 1.065 1.063 33.83 32.67 32.69 29.90 29.94
C-100-NNoisy 2.416 2.289 2.136 2.129 1.994 58.68 54.94 53.18 51.34 50.26
FMoW 1.583 1.627 1.554 1.494 1.305 43.20 42.69 39.92 37.95 34.93
4.2.3 Hyperparameter-WisePost-HocReversal
Ingeneral,theindexsetI cancontainanyhyperparameterconfigurations. Here,weconsidertwo
hyperparamters: learningratescheduleandtrainingepochs. ToavoidrepeatingCIFAR-Nepoch-wise
curves,weexperimentonafreshdataset,FMoW.
Experimentalsetup. Weexperimentonlearningrates(LRs)andtrainingepochs,withindexset
I = {const,exp,cos}×{1,...,T}. Here,const,expandcosrefertoconstant,exponentially
decaying and cosine annealed LRs respectively, and T is the total number of epochs. We train
DenseNet121[24]modelsontheFMoWdataset[9]whichconstitutesa62-wayclassificationofland
usefromsatelliteimages. Formoredetails,seeApp. C.Fig. 5showsthecurves.
LR-wiseobservations. Weseesomeinterestinginstancesofpost-hocreversal: (1)constantLRhas
theworstbaseperformancebutthebestpost-hocperformance;(2)underSWAandTS(composed),
thecurvescontinuetoimproveatthelaterepochsforconstantLR,butnotforthedecayingLRs6.
Epoch-wiseobservations. Epoch-wisepost-hocreversaloccursforallLRschedules. SWAand
ensemblingconvertthedoubledescentintoastrongsingledescent,withapprox. 10-pointimprove-
mentinerrorforthelatter. ForconstantLR,thisalsochangestheoptimalepoch. SWAonlyrecovers
abouthalfoftheensemblegain,andperhapssurprisingly,ensemblingSWAmodelsisnotbetterthan
ensemblingalone. Pre-TSlosscurvesshowastrongmismatchwiththeerrorcurves,butTSenables
simultaneouslygoodlossanderrorwiththelastepochmodels. Overall,theseobservationsreinforce
thetrendsgleanedfromtheCIFAR-Nexperiments.
5 Post-HocSelection: LeveragingPost-HocReversalinPractice
Ourfindingsfrom§4motivatetheprincipleofpost-hocselection,wheremodeldevelopmentdecisions
takepost-hoctransformsintoaccount. Forconcreteness,wediscussthechoiceofcheckpointsfrom
trainingrunsundertheSWA+TSandSWA+Ens+TStransforms. Checkpointselectionreducestothe
selectionofthefinalepochT(cid:98),asSWAusesallcheckpointsuptothatepoch. M valdenotesametric
ofchoicecomputedonthevalidationset.
SWA+TS.NaiveselectionpicksepochT(cid:98) = argmin
T
Mv fal(θ T). Incontrast,post-hocselection
picksT(cid:98)=argmin
T
Mv Ta Sl +T◦f((θ t)T t=1).
SWA+Ens+TS.HerewehaveN differenttrainingrunstopickepochsfor. Naiveselectionpicks
T(cid:98)j =argmin
T
Mv fal(θj T)foreachrunindependently. Incontrast,post-hocselectionwouldideally
pickT(cid:98)1,...,T(cid:98)N =argmin
T1,...,TN
Mv Ta Sl +E+T◦f((θ1 t)T t=1 1,...,(θN
t
)T t=N 1)whichjointlyminimizesthe
ensemble performance. This being computationally expensive, we instead minimize under the
constraintT(cid:98)1 =···=T(cid:98)N7
6PossiblyduetohighermodelvariancewithconstantLR,beneficialforbothensemblingandSWA.
7Alternatively,onecanselectT(cid:98)j =argmin
T
Mv Ta Sl +T◦f(θj 1,...,θj T)asahybridbetweenpost-hocselection
(withinruns)andnaiveselection(acrossruns).
7Perplexity CLM Error (%) MMLU (%)
5 33 48 Base
SWA+TS
4 31 46 SWA+Ens+TS
Naive selection
29 Post-hoc selection
3 44
0 1 2 3 4 0 1 2 3 4 0 1 2 3 4
Epochs Epochs Epochs
Figure6: Perplexityandcausallanguagemodeling(CLM)errorontheGuanacotestset,andMMLU
accuracy(higherisbetter)forinstructiontuningLLaMA-2-7B.Shadingindicatespost-hocreversal.
BaseandSWA+TScurvesaremeanof8runs. Individualrunsarenotshownastheywouldlargely
overlap with the mean curve (max range among runs: 0.01 perplexity, 0.07 error, 0.13 MMLU).
SWA+Ens+TScurvesensemblemodelsfromthese8runs.
Results. Tab.1comparesnaiveandpost-hocselectionstrategiesforCIFAR-NandFMoW.Except
forsomecleanlabelsets,post-hocselectionisalwaysbetterthannaiveselection,oftenwith>2×
improvementfrompost-hocselectionascomparedtonaiveselection.
Earlystopping.Weadvocatemonitoringpost-hocmetricsforearlystopping.Onlyarunningaverage
needstobeupdatedforSWA,andTSinvolvesaquicksingle-parameteroptimization. Further,while
thebasecurvescanfluctuatewildlybetweenconsecutiveruns,SWA+TScurvesareconsiderably
smoother(seeFigs.3,9and7),makingthemmorereliableforautomatedearlystopping. Onecan
similarlymonitormetricsforSWA+Ens+TSunderparalleltrainingruns.
6 ExperimentsAcrossDomainsandModalities
In§4and§5,weintroducedpost-hocreversalandselectionwithexperimentsontheCIFAR-Nand
FMoWdatasets.Inthissection,wesupplementourexperimentalanalysiswithadditionalexperiments
acrossdiversedomainsandmodalitiestodemonstratethegeneralityofourfindings.
6.1 LLMInstructionTuning
Language models are pre-trained or fine-tuned with a self-supervised objective of predicting the
nexttokeninatextcorpus. Theremightbemanyacceptabletokensfollowingagivenprefix,albeit
withdifferentprobabilities. Thusnexttokenpredictionisnoisyandonemightreasonablyexpect
toseepost-hocreversal. Inthissection, wetestthishypothesisforthetaskoffine-tuningLLMs
tofollowinstructions(instructiontuning[66]). Instructiontuningdatasetsarenaturallysmalland
amenable to multi-epoch training where catastrophic overfitting becomes an important concern.
Recentworks[50,75]havearguedfordatarepetitionsforLLMpre-trainingaswell,andweexpect
post-hocreversaltooccurtheretoo,withimportantconsequences. However,suchexperimentsare
beyondthescopeofthispaper.
Experimentalsetup. Wefine-tuneLLaMA-2-7B[64]ontheGuanacodataset[12]ofchatcomple-
tions. Weevaluateperplexityandcausallanguagemodeling(CLM)erroronthetestset,andalsothe
MMLUaccuracy[22]tobettercontextualizemodelimprovements. Fig. 6showsthecurves. Tab. 7
inApp. Egivesexactnumbers.
Observations. Weobservepost-hocreversalbetweenepochs1and2forperplexityanderror,and
betweenepochs2and3forMMLU.BothSWA+TSandSWA+Ens+TStransformsshowsignificant
improvements,muchofwhichisonlyrealizedunderpost-hocselection.
6.2 OtherText,TabularandGraphDatasets
Inthissection,wefurtherexpandourexperimentalcoveragetotext,tabularandgraphclassification
datasetsfromreal-worldapplications.
Experimental setup. We consider the following tasks: (1) sentiment classification on the Yelp
reviewsdataset[5](text)withapre-trainedtransformerBERT[13],(2)predictiontasksoncensus
datafromFolktables[14](tabular)withMLPsand(3)communitydetectionontheRedditandCollab
datasets[76](graph)withgraphneuralnetworks(GNNs). Folktableshas5predictiontasks: Income,
8Yelp Income Reddit-12k
3.42 0.71 5.09
2.56 0.60 3.85
1.70 0.49 2.61
0.84 0.38 1.37
1.06 0.47 1.70
Base
0.98 0.44 1.58
SWA
0.90 0.41 1.46 Individual
0.82 0.38 1.34 Ensemble
43.1 20.6 60.6
40.7 19.5 57.0
38.3 18.4 53.4
35.9 17.3 49.8
0 10 20 0 10 20 0 200 400
Epochs Epochs Epochs
Figure7: Testcurvesfor3real-worldnoisydatasets. Notethatthepre-TSlossissignificantlyhigher
thanthepost-TSloss. Examplesofpost-hocreversalbetweenthebasecurvesgivenbythesolidblue
linesandthepost-hoccurvesgivenbythedashedorangelines(SWAensemble): (1)optimalepochis
differentforbaseandpost-hoccurvesforerrorandpost-TSlossonalldatasets;(2)forerroronYelp,
basecurveshowsdoubledescentbutpost-hoccurvedoesnot;(3)forerroronIncome,basecurve
overfitscatastrophicallyatapprox. epoch5butpost-hoccurvecontinuesimprovingtillapprox. epoch
20;(4)forerroronReddit-12k,basecurvedoesnotshowdoubledescentbutpost-hoccurvedoes.
PublicCoverage, Mobility, Employment and TravelTime. Reddit has 2 versions: Reddit-5k and
Reddit-12k. Formoredetails,seeApp. C.Figure7showscurvesforYelp,IncomeandReddit-12k.
Tab. 5inApp. Dcomparesnaiveandpost-hocselectiononalldatasets.
Observations. Post-hocreversalisarecurringfeatureacrossdatasets,transformsandmetrics. The3
datasetsshowdifferentpatternsbetweenthebaseandpost-hoccurves,showingthatpost-hocreversal
cantakeavarietyofforms.
7 Conclusion
Weempiricallystudiedtemperaturescaling(TS),ensembling,stochasticweightaveraging(SWA)and
theircompositions,andfoundthatthesetransformscanreversemodelpeformancetrends(post-hoc
reversal).Basedonourfindings,wepresentedthesimpletechniqueofpost-hocselection,andshowed
thatitoutperformsnaiveselection. Wevalidatedourfindingsandproposalsoverawidevarietyof
experimentalsettings.
Our work has broad implications for the field of deep learning. It shows that current practices
surroundingtheuseofpost-hoctransformsleavemuchroomforimprovement. Thisisespecially
truefornoisydata,whichispervasiveinreal-worldapplications. Morebroadly,post-hocreversal
challenges our understanding of neural network training dynamics and calls for rethinking deep
learningworkflows. Forexample,aninterestingquestionforfutureresearchishowpost-hocreversal
impactsscalinglaws, whicharecruciallyreliedonforcomputeallocationinlarge-scaletraining
runs. Otherfuturedirectionsincludebetterstrategiesforcheckpointselection,understandingthe
theoreticalunderpinningsofpost-hocreversal,andcharacterizingotherinstancesofit.
Summaryofpracticalrecommendations. WeadvocatefortheuseofTS,ensemblingandSWA
across deep learning applications. Further, such transforms should be tightly integrated into the
modeldevelopmentpipeline, followingthemethodologyoutlinedinthepaper. Inparticular: (1)
apply SWA+TS and SWA+Ens+TS transforms for better results in the single- and multi-model
settingsrespectively;(2)tracktemperature-scaledlosstoovercomeloss-errormismatch;(3)monitor
post-hocmetricstoavoidprematureearlystopping;(4)makehyperparameterdecisionsinformedby
post-transformperformance;(5)usepost-hocselectiontopickmodelcheckpoints.
9
)ST-erp(
ssoL
)ST-tsop(
ssoL
)%(
rorrEReferences
[1] TaigaAbe,E.KellyBuchanan,GeoffPleiss,andJohnPCunningham. Pathologiesofpredictive
diversityindeepensembles. ArXiv,abs/2302.00704,2023.
[2] Amr Alexandari, Anshul Kundaje, and Avanti Shrikumar. Maximum likelihood with bias-
correctedcalibrationishard-to-beatatlabelshiftadaptation. InInternationalConferenceon
MachineLearning,pages222–232.PMLR,2020.
[3] ZeyuanAllen-ZhuandYuanzhiLi. Towardsunderstandingensemble,knowledgedistillation
andself-distillationindeeplearning. arXivpreprintarXiv:2012.09816,2020.
[4] DevanshArpit,HuanWang,YingboZhou,andCaimingXiong. Ensembleofaverages: Improv-
ingmodelselectionandboostingperformanceindomaingeneralization. AdvancesinNeural
InformationProcessingSystems,35:8265–8277,2022.
[5] Nabiha Asghar. Yelp dataset challenge: Review rating prediction. arXiv preprint
arXiv:1605.05362,2016.
[6] JunbumCha,SanghyukChun,KyungjaeLee,Han-CheolCho,SeunghyunPark,YunsungLee,
andSungraePark. Swad: Domaingeneralizationbyseekingflatminima. AdvancesinNeural
InformationProcessingSystems,34:22405–22418,2021.
[7] JohnChen,QihanWang,andAnastasiosKyrillidis. Mitigatingdeepdoubledescentbycon-
catenatinginputs. Proceedingsofthe30thACMInternationalConferenceonInformation&
KnowledgeManagement,2021.
[8] LichangChen,ShiyangLi,JunYan,HaiWang,KalpaGunaratna,VikasYadav,ZhengTang,
VijaySrinivasan,TianyiZhou,HengHuang,etal. Alpagasus: Trainingabetteralpacawith
fewerdata. arXivpreprintarXiv:2307.08701,2023.
[9] GordonChristie,NeilFendley,JamesWilson,andRyanMukherjee. Functionalmapofthe
world. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,
pages6172–6180,2018.
[10] JeremyM.Cohen,SimranKaur,YuanzhiLi,J.ZicoKolter,andAmeetTalwalkar. Gradient
descentonneuralnetworkstypicallyoccursattheedgeofstability. ArXiv,abs/2103.00065,
2021.
[11] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin
Gilmer,AndreasPeterSteiner,MathildeCaron,RobertGeirhos,IbrahimAlabdulmohsin,etal.
Scalingvisiontransformersto22billionparameters. InInternationalConferenceonMachine
Learning,pages7480–7512.PMLR,2023.
[12] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient
finetuningofquantizedLLMs. InThirty-seventhConferenceonNeuralInformationProcessing
Systems,2023.
[13] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,
2018.
[14] FrancesDing,MoritzHardt,JohnMiller,andLudwigSchmidt. Retiringadult: Newdatasets
forfairmachinelearning. AdvancesinNeuralInformationProcessingSystems,34,2021.
[15] JonathanFrankleandMichaelCarbin. Thelotterytickethypothesis: Findingsparse,trainable
neuralnetworks. arXiv: Learning,2018.
[16] TimurGaripov,PavelIzmailov,DmitriiPodoprikhin,DmitryPVetrov,andAndrewGWilson.
Losssurfaces,modeconnectivity,andfastensemblingofdnns. Advancesinneuralinformation
processingsystems,31,2018.
[17] ChuanGuo,GeoffPleiss,YuSun,andKilianQWeinberger. Oncalibrationofmodernneural
networks. InInternationalconferenceonmachinelearning,pages1321–1330.PMLR,2017.
10[18] Trevor J. Hastie, Robert Tibshirani, and Jerome H. Friedman. The elements of statistical
learning. 2001.
[19] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages770–778,2016.
[20] TongHe,ZhiZhang,HangZhang,ZhongyueZhang,JunyuanXie,andMuLi. Bagoftricks
forimageclassificationwithconvolutionalneuralnetworks. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages558–567,2019.
[21] ReinhardHeckelandFatihYilmaz. Earlystoppingindeepnetworks: Doubledescentandhow
toeliminateit. ArXiv,abs/2007.10099,2020.
[22] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300,2020.
[23] GaoHuang,YixuanLi,GeoffPleiss,ZhuangLiu,JohnEHopcroft,andKilianQWeinberger.
Snapshotensembles: Train1,getmforfree. arXivpreprintarXiv:1704.00109,2017.
[24] GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKilianQWeinberger.Denselyconnected
convolutionalnetworks. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages4700–4708,2017.
[25] GabrielIlharco,MitchellWortsman,SamirYitzhakGadre,ShuranSong,HannanehHajishirzi,
Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by
interpolatingweights. AdvancesinNeuralInformationProcessingSystems,35:29262–29277,
2022.
[26] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. Averagingweightsleadstowideroptimaandbettergeneralization. arXivpreprint
arXiv:1803.05407,2018.
[27] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language
modelswithpairwiserankingandgenerativefusion. arXivpreprintarXiv:2306.02561,2023.
[28] LuJiang,DiHuang,MasonLiu,andWeilongYang. Beyondsyntheticnoise: Deeplearningon
controllednoisylabels. InInternationalConferenceonMachineLearning,2019.
[29] JaredKaplan,SamMcCandlish,T.J.Henighan,TomB.Brown,BenjaminChess,RewonChild,
ScottGray,AlecRadford,JeffWu,andDarioAmodei. Scalinglawsforneurallanguagemodels.
ArXiv,abs/2001.08361,2020.
[30] ThomasNKipfandMaxWelling. Semi-supervisedclassificationwithgraphconvolutional
networks. arXivpreprintarXiv:1609.02907,2016.
[31] PangWeiKoh,ShioriSagawa,HenrikMarklund,SangMichaelXie,MarvinZhang,Akshay
Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al.
Wilds: Abenchmarkofin-the-wilddistributionshifts. InInternationalConferenceonMachine
Learning,pages5637–5664.PMLR,2021.
[32] AlexanderKolesnikov,LucasBeyer,XiaohuaZhai,JoanPuigcerver,JessicaYung,SylvainGelly,
andNeilHoulsby.Bigtransfer(bit):Generalvisualrepresentationlearning.InComputerVision–
ECCV2020: 16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,Part
V16,pages491–507.Springer,2020.
[33] DanKondratyuk,MingxingTan,MatthewBrown,andBoqingGong. Whenensemblingsmaller
modelsismoreefficientthansinglelargemodels. arXivpreprintarXiv:2005.00570,2020.
[34] AndreasKöpf,YannicKilcher,DimitrivonRütte,SotirisAnagnostidis,Zhi-RuiTam,Keith
Stevens,AbdullahBarhoum,NguyenMinhDuc,OliverStanley,RichárdNagyfi,etal. Ope-
nassistant conversations–democratizing large language model alignment. arXiv preprint
arXiv:2304.07327,2023.
11[35] LudmilaI.KunchevaandChristopherJ.Whitaker. Measuresofdiversityinclassifierensembles
andtheirrelationshipwiththeensembleaccuracy. MachineLearning,51:181–207,2003.
[36] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Neural Information Processing
Systems,2016.
[37] Kuang-HueiLee,XiaodongHe,LeiZhang,andLinjunYang. Cleannet: Transferlearningfor
scalableimageclassifiertrainingwithlabelnoise. 2018IEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages5447–5456,2017.
[38] WeishiLi,YongPeng,MiaoZhang,LiangDing,HanHu,andLiShen. Deepmodelfusion: A
survey. arXivpreprintarXiv:2309.15698,2023.
[39] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-
learningregularizationpreventsmemorizationofnoisylabels. Advancesinneuralinformation
processingsystems,33:20331–20342,2020.
[40] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-
learningregularizationpreventsmemorizationofnoisylabels. ArXiv,abs/2007.00151,2020.
[41] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by
over-parameterization. ArXiv,abs/2202.14026,2022.
[42] ShengLiu,ZhihuiZhu,QingQu,andChongYou. Robusttrainingunderlabelnoisebyover-
parameterization. In International Conference on Machine Learning, pages 14153–14172.
PMLR,2022.
[43] ShikunLiu,LinxiFan,EdwardJohns,ZhidingYu,ChaoweiXiao,andAnimaAnandkumar.Pris-
mer: Avision-languagemodelwithanensembleofexperts. arXivpreprintarXiv:2303.02506,
2023.
[44] RaphaelGontijoLopes,YannDauphin,andEkinDogusCubuk. Noonerepresentationtorule
themall: Overlappingfeaturesoftrainingmethods. ArXiv,abs/2110.12899,2021.
[45] Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren
Zhou. Routingtotheexpert: Efficientreward-guidedensembleoflargelanguagemodels. arXiv
preprintarXiv:2311.08692,2023.
[46] Xiaoding Lu, Adian Liusie, Vyas Raina, Yuwen Zhang, and William Beauchamp. Blend-
ing is all you need: Cheaper, better alternative to trillion-parameters llm. arXiv preprint
arXiv:2401.02994,2024.
[47] NeilRohitMallinar,JamesB.Simon,AmirhesamAbedsoltan,ParthePandit,MikhailBelkin,
andPreetumNakkiran. Benign,tempered,orcatastrophic: Ataxonomyofoverfitting. ArXiv,
abs/2207.06569,2022.
[48] Prem Melville and Raymond J. Mooney. Constructing diverse classifier ensembles using
artificialtrainingexamples. InInternationalJointConferenceonArtificialIntelligence,2003.
[49] ChristopherMorris,NilsMKriege,FrankaBause,KristianKersting,PetraMutzel,andMarion
Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv
preprintarXiv:2007.08663,2020.
[50] Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus,
NouamaneTazi,SampoPyysalo,ThomasWolf,andColinRaffel. Scalingdata-constrained
languagemodels. ArXiv,abs/2305.16264,2023.
[51] PreetumNakkiran,PrayaagVenkat,ShamM.Kakade,andTengyuMa. Optimalregularization
canmitigatedoubledescent. ArXiv,abs/2003.01897,2020.
[52] PreetumNakkiran,GalKaplun,YaminiBansal,TristanYang,BoazBarak,andIlyaSutskever.
Deepdoubledescent:Wherebiggermodelsandmoredatahurt.JournalofStatisticalMechanics:
TheoryandExperiment,2021(12):124003,2021.
12[53] YanivOvadia, EmilyFertig, JieJessieRen, ZacharyNado, D.Sculley, SebastianNowozin,
JoshuaV.Dillon, BalajiLakshminarayanan, andJasperSnoek. Canyoutrustyourmodel’s
uncertainty? evaluating predictive uncertainty under dataset shift. In Neural Information
ProcessingSystems,2019.
[54] VardanPapyan,XuemeiHan,andDavidL.Donoho. Prevalenceofneuralcollapseduringthe
terminalphaseofdeeplearningtraining. ProceedingsoftheNationalAcademyofSciencesof
theUnitedStatesofAmerica,117:24652–24663,2020.
[55] AletheaPower,YuriBurda,HarrisonEdwards,IgorBabuschkin,andVedantMisra. Grokking:
Generalizationbeyondoverfittingonsmallalgorithmicdatasets. ArXiv,abs/2201.02177,2022.
[56] VQu’etuandETartaglione. Canweavoiddoubledescentindeepneuralnetworks. arXiv
preprintarXiv:2302.13259,2023.
[57] Alec Radford and Karthik Narasimhan. Improving language understanding by generative
pre-training. 2018.
[58] AlexandreRame,MatthieuKirchmeyer,ThibaudRahier,AlainRakotomamonjy,PatrickGal-
linari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization.
AdvancesinNeuralInformationProcessingSystems,35:10821–10836,2022.
[59] AlexandreRamé,NinoVieillard,LéonardHussenot,RobertDadashi,GeoffreyCideron,Olivier
Bachem,andJohanFerret. Warm: Onthebenefitsofweightaveragedrewardmodels. arXiv
preprintarXiv:2401.12187,2024.
[60] SunnySanyal,AtulaTejaswiNeerkaje,JeanKaddour,AbhishekKumar,etal. Earlyweight
averagingmeetshighlearningratesforllmpre-training. InWorkshoponAdvancingNeural
NetworkTraining: ComputationalEfficiency,Scalability,andResourceOptimization(WANT@
NeurIPS2023),2023.
[61] RylanSchaeffer,MikailKhona,ZacharyRobertson,AkhilanBoopathy,KaterynaPistunova,
JasonW.Rocks,IlaRaniFiete,andOluwasanmiKoyejo. Doubledescentdemystified: Identi-
fying,interpreting&ablatingthesourcesofadeeplearningpuzzle. ArXiv,abs/2303.14151,
2023.
[62] HwanjunSong,MinseokKim,andJae-GilLee. Selfie: Refurbishinguncleansamplesforrobust
deeplearning. InInternationalConferenceonMachineLearning,2019.
[63] HwanjunSong,MinseokKim,DongminPark,YoojuShin,andJae-GilLee. Learningfrom
noisylabelswithdeepneuralnetworks: Asurvey. IEEETransactionsonNeuralNetworksand
LearningSystems,2022.
[64] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[65] DongdongWang, BoqingGong, andLiqiangWang. Oncalibratingsemanticsegmentation
models: Analysesandanalgorithm. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages23652–23662,2023.
[66] JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,
AndrewM.Dai,andQuocV.Le. Finetunedlanguagemodelsarezero-shotlearners. ArXiv,
abs/2109.01652,2021.
[67] JiahengWei,ZhaoweiZhu,HaoCheng,TongliangLiu,GangNiu,andYangLiu. Learningwith
noisylabelsrevisited: Astudyusingreal-worldhumanannotations. ArXiv,abs/2110.12088,
2021.
[68] JiahengWei,ZhaoweiZhu,HaoCheng,TongliangLiu,GangNiu,andYangLiu. Learning
with noisy labels revisited: A study using real-world human annotations. arXiv preprint
arXiv:2110.12088,2021.
13[69] RossWightman,HugoTouvron,andHervéJégou. Resnetstrikesback: Animprovedtraining
procedureintimm. arXivpreprintarXiv:2110.00476,2021.
[70] AndrewGWilsonandPavelIzmailov. Bayesiandeeplearningandaprobabilisticperspective
ofgeneralization. Advancesinneuralinformationprocessingsystems,33:4697–4708,2020.
[71] MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,
AriSMorcos,HongseokNamkoong,AliFarhadi,YairCarmon,SimonKornblith,etal. Model
soups: averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasing
inferencetime. InInternationalConferenceonMachineLearning,pages23965–23998.PMLR,
2022.
[72] Ruixuan Xiao, Yiwen Dong, Haobo Wang, Lei Feng, Runze Wu, Gang Chen, and Junbo
Zhao. Promix: Combatinglabelnoiseviamaximizingcleansampleutility. InEdithElkind,
editor,ProceedingsoftheThirty-SecondInternationalJointConferenceonArtificialIntelli-
gence,IJCAI-23,pages4442–4450.InternationalJointConferencesonArtificialIntelligence
Organization,82023. doi: 10.24963/ijcai.2023/494. MainTrack.
[73] TongXiao,TianXia,YiYang,ChangHuang,andXiaogangWang. Learningfrommassive
noisylabeleddataforimageclassification. 2015IEEEConferenceonComputerVisionand
PatternRecognition(CVPR),pages2691–2699,2015.
[74] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka. Howpowerfularegraphneural
networks? arXivpreprintarXiv:1810.00826,2018.
[75] FuzhaoXue,YaoFu,WangchunshuZhou,ZangweiZheng,andYangYou. Torepeatornotto
repeat: Insightsfromscalingllmundertoken-crisis. ArXiv,abs/2305.13230,2023.
[76] PinarYanardagandSVNVishwanathan. Deepgraphkernels. InProceedingsofthe21thACM
SIGKDDinternationalconferenceonknowledgediscoveryanddatamining,pages1365–1374,
2015.
[77] MertYuksekgonul, LinjunZhang, JamesY.Zou, andCarlosGuestrin. Beyondconfidence:
Reliablemodelsshouldalsoconsideratypicality. ArXiv,abs/2305.18262,2023.
[78] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use:
Improvingfew-shotperformanceoflanguagemodels. InInternationalConferenceonMachine
Learning,pages12697–12706.PMLR,2021.
[79] ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,JiaoSun,YuningMao,XuezheMa,Avia
Efrat,PingYu,LiliYu,etal.Lima:Lessismoreforalignment.arXivpreprintarXiv:2305.11206,
2023.
14A ExpandedRelatedWork
Phenomena. Empiricalworkslikedoubledescent[52],grokking[55],scalinglaws[29],neural-
collapse[54], edge-of-stability [10], lottery-ticket-hypothesis[15]haverevealedbothchallenges
and oppotunities for improving the understanding and practices of deep neural network training.
Post-hocreversalexpandsthislistasanovelphenomenonregardinglearningdynamicsunderthe
lensofpost-hoctransforms. Itismostintimatelyconnectedwithdoubledescent,offeringawayto
mitigateit. Someworks[7,21,51,56,61,70]showothermitigations,suchasregularizationand
dataaugmentation.
TemperatureScaling(TS).TSbelongstoafamilyofpost-hoccalibrationtechniques[2,17,77],
withtheuniquepropertyofpreservingclassificationerror. Recently,calibrationhasbeenapplied
to large vision and language models [11, 65, 78]. While loss-error mismatch has been reported
before[11,17],tothebestofourknowledge,wearethefirsttoreportpost-hocreversalwithTS.
Ensembling. Ensemblingisafoundationaltechniqueinmachinelearning,encompassingbagging,
boosting,etc. Indeeplearning,auniformensembleismostpopular[3,36],althoughrecentwork
onensemblingLLMshasexploredmoreefficientrouting-basedensembles[27,43,45,46]. Various
workshaveexploredstrategiestoformoptimalensembles[33,44,48,71],generallybasedonmodel
diversity[35],butrecentlyAbeetal.[1]havewarnedagainstthis. Incontrast,ourrecommendation
forformingensemblesreliesdirectlyonthevalidationperformanceoftheensemble,introducingno
proxies,andstillbeingcomputationallycheap.
Stochastic Weight Averaging (SWA). SWA [26] is the culmination of a line of work [16, 23]
which seek to cheaply approximate ensembling. It has inspired numerous works which average
weightsinsomeform[4,6,25,38,58,71]oftenincombinationwithensembling. Recently,weight
averaginghasshownupintheLLMspace[59,60]. WhiletheseworksgenerallyapplySWAwith
afixedtrainingtimedeterminedindependently,wepresentSWAintheroleofearlystoppingand
modelselection. Inpractice,SWAhasoftenbeenfoundtobeunreliable8,andisoftenskippedfrom
trainingrecipesevenwhenconsidered[32,69]. Ourworkshedssomelightonthis,offeringarather
counter-intuitivechoiceofmodelstoincludeintheweightaverageforbestresults.
Labelnoise. Manytrainingstrategieshavebeenintroducedtodealwithnoisydata(see[63]for
asurvey). However,theefficacyofsimplepost-hoctransformshasbeenleftunexplored. Further,
most of these works are motivated by labeling errors, which leaves some of the core practical
considerations for dealing with general noisy data unaddressed. For instance, access to a clean
validationsetisassumedandtestlossisoverlookedasanimportantmetric[40,41]. Wealsoentirely
avoidexperimentsonsyntheticnoise,informedbyrecentworkwhichquestionsthetransferabilityof
findingstorealisticnoisepatterns[28,67]. Somerecentdatasets[28,37,62,67,73]makeitpossible
tostudyrealisticnoisealongwithknownnoiseestimates.
Multi-epoch training of LLMs. Multi-epoch training of LLMs runs into severe catastrophic
overfitting. Xueetal.[75]examinethecontributingfactorsandexplorepossiblesolutions. They
findthatregularizationisnothelpful,exceptfordropout. Muennighoffetal.[50]studyscalinglaws
consideringdatarepetitions. Complementarily,weputforwardpost-hoctransformsasaneffective
solutionwithourpost-hocselectionmethodology. Thisisespeciallyimportantforfine-tuningLLMs,
e.g. ininstructiontuning[66],where[79]and[8]advocateforfine-tuningwithasmalleramountof
higherqualitysamplesformoreepochs.
B ExpandedPreliminariesandBackground
B.1 LearningonNoisyData
Figures8,9and10illustratevarioussourcesofnoise: aleatoricunertainty,epistemicuncertainty
andannotationerrors. BelowweprovidesomebackgroundonBayes-optimalclassifieranduseitto
introducethecleanerrormetricandBayesloss/errorasmeasuresofnoiselevel.
Bayes-optimalclassifier. f ,givenbyf (x) = logP [y = k | x]minimizesbothMerror and
D D k D D
Mloss, and is called the Bayes-optimal classifier for D. The Bayes error Merror[f ] and Bayes
D D D
8See, forexample, discussionathttps://discuss.huggingface.co/t/improvements-with-swa/
858.
15(a)GasStation (b)EducationalInstitution
Figure 8: Data can be noisy due to non- Figure 9: Data can be noisy due to insufficient
determinisminthepredictiontarget(aleatoricun- informationintheinputcontext(epistemicuncer-
certainty). Figureshowsamessagetreefromthe tainty). Figures8aand8bshowsatelliteimages
OpenAssistantConverstations(OASST1)Dataset. fromtheFMoWdataset.Thelabelsarecorrect,as
Achatbotcancontinueaconversationsatisfacto- corroboratedbyexternalmapdata.However,they
rily in many different ways, making next token cannotbedeterminedwithfullcertaintyfromthe
predictionnoisy. imagesalone.
(a)L:Cat (b)L:Horse (c)L:Airplane (d)L:Bee (e)L:Dolphin (f)L:Mountain
C:Frog C:Deer C:Ship A:Sunflower A:Woman A:Cloud
Figure 10: Data can be noisy due to annotation errors. Figures 10a, 10b and 10c are mislabeled
imagesfromCIFAR-10. 10d,10eand10fareambiguousimagesfromCIFAR-100withmultiple
correctlabelsamongthegivenclasses. (L=labelindataset,C=correctlabel,A=alternativelabel)
loss Mloss[f ] are measures of the noise level. y∗(x) = argmax f (x) is sometimes called
D D k D k
t Phe [c yle |a xn ]l =ab 1el {. yU =sin yg ∗(y x∗ ), }o .n Te hm ea cy lead nefi en re rot rhe Mcl ee rra orn isda at ca od mis mtr oib nu mtio etn riD c(cid:101) inw ti hth eP laD b(cid:101) e[x l] no= isP elD it[ ex r] ata un rd
e
D(cid:101) D(cid:101)
butnotafocusofourworkasy∗istypicallyinaccessibleinmoregeneralnoisysettings.
B.2 Post-HocTransformsinMachineLearning
TheexplicitformsofthecomposedtransformsSWA+TSandSWA+Ens+TS(denotedasT and
S+T
T )aregivenbyEquations4and5respectively. ForT ,parametersθl,...,θl areweight-
S+E+T S+E+T 1 Kl
averaged and the L resulting models are ensembled, followed by temperature scaling. τ is the
l
temperatureforweight-averagedmodels,andτ isthetemperaturefortheensemble. Asbefore,
Ens
theyareobtainedbyoptimizingthecross-entropylossoverthevalidationset,withmodelparameters
fixed.
(cid:32) K (cid:33) (cid:34) (cid:32) K (cid:33)(cid:35)
1 1 (cid:88) 1 1 (cid:88)
(T ◦f)(x;θ ,...,θ )= f x; θ , withτ =argminMloss f ·; θ
S+T 1 K τ K i τ val τ K i
i=1 i=1
(4)
(cid:16) (cid:17) 1 1
(cid:88)L
1
(cid:32)
1
(cid:88)Kl (cid:33)
(T ◦f) x;θ1,...,θ1 ,...,θL,...,θL = f x; θl (5)
S+E+T 1 K1 1 KL τ L τ K k
Ens l l
l=1 k=1
16Table2: DatasetDetails.
Modality Dataset TrainSize ValSize TestSize Classes InputSize Units
CIFAR-10 40000 5000 5000 10 3×32×32
CIFAR-100-NCoarse 40000 5000 5000 20 3×32×32
Vision C×W×H
CIFAR-100-NFine 40000 5000 5000 100 3×32×32
FMoW 76863 11483 11327 62 3×224×224
Guanaco 8850 500 500 32000 ≲4000
Text Yelp 25000 5000 5000 5 ≲2000 characters
Income 156533 19566 19566 2 816
PublicCoverage 110844 13855 13855 2 88
Tabular Mobility 64265 8032 8032 2 101 features
Employment 303055 37881 37881 2 98
TravelTime 138008 17250 17250 2 615
Collab 4000 500 500 3 74.49,2457.78
nodes,edges
Graph Reddit-5k 4001 499 499 5 508.52,594.87
(avg.)
Reddit-12k 9545 1192 1192 11 391.41,456.89
Table3: TrainingDetails.
Pre- Weight Batch
Dataset Model Optimizer LR LRSchedule Epochs
train Decay Size
C-10/100-N ResNet18-D[20] Yes SGD 0.1 5e-4 Cosine 100 500
FMoW DenseNet121[24] Yes Adam 1e-4 0 Constant 50 64
Guanaco LLaMA-2-7B[64] Yes Adam 2e-4 0 Constant 6 16
Yelp BERT[13] Yes AdamW 5e-5 1e-2 Linear 25 16
Folktables MLP No Adam 0.01 0 Exponential 50 256
Collab GIN[74] No Adam 0.01 0 Exponential 500 128
Reddit GCN[30] No Adam 0.01 0 Exponential 500 128
C DatasetandTrainingDetails
Tables2and3summarizethedatasetsandtrainingdetailsforourexperiments. Theyaredescribedin
detailbelow.
CIFAR-N [68]. CIFAR-10-N uses the same images as CIFAR-10 but provides multiple human-
annotatedlabelsets. Cleanistheoriginallabelset; Rand1,2,3are3setsofhumanlabels; Aggre
combinesRand1,2,3bymajorityvote;andWorstcombinesthembypickinganincorrectlabel,if
possible.CIFAR-100has2variants,afine-grainedonewith100classesandacoarse-grainedonewith
20classes,obtainedbygroupingthefine-grainedclasses. Correspondingly,thereareCIFAR-100-N
CoarseandCIFAR-100-NFinedatasets. Theyhavetwolabelsetseach: CleanandNoisy,withthe
latterbeinghuman-labeled. Inthemainpaper,CIFAR-100-Nreferstothefine-grainedversion.
Bycross-referencingwiththeoriginallabels,itispossibletoestimatethenoiselevels. Theseare
showninTable4.
CIFAR-Nallowsaccesstocleanlabels. Intheliterature,thevalidationandtestsetsforCIFAR-N
typicallyusethecleanlabels[39,42,72]. However,accesstocleanlabelsisaluxuryonlyavailable
forlabelnoisesettings. Eventhere,obtainingcleanlabelsisexpensive,asitrequirescarefulexpert
annotation. Forothersourcesofnoiseitmightnotevenbefeasibletoobtaincleanlabels. Hence,we
restrictourselvestousingnoisy(i.i.d. totrain)validationandtestsets. SinceCIFAR-Nonlyprovides
humanlabelsfortheoriginal50kCIFAR-10/100trainimages,wesplittheseinto40k/5k/5kimages
fortrain/val/testsets.
FMoW[9,31].ThisistheversionoftheoriginalFMoWdataset[9]asusedintheWILDSbenchmark
[31]. TheinputisanRGBsatelliteimage(rescaledto224x224pixels)andthelabelisoneof62
Table4: NoiselevelsforCIFAR-N(%),reproducedfrom[68].
CIFAR-10-N CIFAR-100-NCoarse CIFAR-100-NFine
Clean Aggre Rand1 Rand2 Rand3 Worst Clean Noisy Clean Noisy
0.00 9.03 17.23 18.12 17.64 40.21 0.00 25.60 0.00 40.20
17Table5: Naivevspost-hoc(ours)selectionforSWA+TSandSWA+Ens+TStransformsonsome
real-worlddatasets. Bettervaluesareinbold.
Metric→ TestLoss TestError(%)
Transform→ None SWA+TS SWA+Ens+TS None SWA+TS SWA+Ens+TS
Dataset↓ Naive Ours Naive Ours Naive Ours Naive Ours
Yelp 0.908 0.890 0.854 0.841 0.824 39.41 38.02 37.33 36.18 36.14
Income 0.393 0.390 0.387 0.388 0.385 17.84 17.69 17.54 17.62 17.40
PublicCoverage 0.544 0.540 0.539 0.538 0.538 27.52 27.31 27.25 27.25 27.02
Mobility 0.474 0.472 0.471 0.471 0.468 21.43 21.38 21.42 21.17 21.24
Employment 0.380 0.379 0.378 0.378 0.377 17.94 17.77 17.80 17.72 17.83
TravelTime 0.597 0.597 0.593 0.596 0.591 35.77 35.46 35.35 35.44 35.23
Collab 0.492 0.475 0.460 0.439 0.404 20.65 21.58 20.27 20.40 18.80
Reddit-5k 1.154 1.112 1.100 1.101 1.085 47.42 48.35 47.04 47.09 45.49
Reddit-12k 1.405 1.381 1.366 1.367 1.346 51.78 51.08 51.11 50.34 51.26
buildingorlandusecategories. Thelabelswereobtainedbyacombinationofhumanannotation
andcross-referencedgeographicalinformation. Theoriginaldatasetprovidesadditionalmetadata
aboutlocation,time,sunangles,physicalsizes,etc. whichisignoredintheWILDSdataset(and
henceinours). Whilethelabelshavelownoisecomparedtotheground-truth,thisdatasetisnoisy
becauseofinsufficientinformation. Itishardtodisambiguatethebuildingorlandusecategorywith
fullcertaintybylookingatthesatelliteimagealone. SeeFigure9. Modelsandtrainingsetupareas
usedin[9,31],exceptfortheLRschedule,whereweexperimentwithmultiplealternatives.
Guanaco[12]. ThisisasubsetoftheOASST1dataset[34]containingonlythehighest-ratedpathsin
theconversationtree.Wefollowthefine-tuningsetupfrom[12],exceptthatweusevanillafine-tuning
withoutanyquantizationorlow-rankadapters.
Yelp[5]. ThisisasubsetoftheYelpDatasetChallenge2015datasetwith25kreviewsinthetrainset
and5kreviewseachinthevalidationandtestsets. Theinputisareviewtextandthelabelisoneof
5classes(1to5stars). Assigningaratingtoareviewisintrinsicallynon-deterministicasdifferent
reviewersmighthavedifferentthresholdsforthestarratings. Thisintroducesnoiseinthedata.
Folktables [14]. Folktables consists of 5 classification tasks based on the US Census: Income,
Employment,Health,TravelTimeandPublicCoverage. Thedataistabular. Theavailablefeature
columnsdonotcontainsufficientinformationtopredictthetargetswithfullcertainty,evenifthe
Censusrecordedtheground-truthlabelswithhighaccuracy. Thisresultsinnoise. SeeFigure??.
CollabandReddit[49,76]. ThesedatasetsarefromTUDataset[49],andwereoriginallyintroduced
byYanardagandVishwanathan[76]. Collabisascientificcollaborationdataset. Theinputisan
ego-networkofaresearcherandthelabelisthefieldoftheresearcher(oneofHighEnergyPhysics,
CondensedMatterPhysicsandAstroPhysics). TheReddit-5kandReddit-12kdatasets(originally
calledREDDIT-MULTI-5KandREDDIT-MULTI-12K)arebalanceddatasetswheretheinputisa
graphwhichcorrespondstoanonlinediscussionthreadfromthesocialnetworksiteReddit. Nodes
correspondtousersandthereisanedgeifoneuserrespondedtoanother’scomment. Thetaskisto
predictwhichsubredditadiscussiongraphbelongsto. Reddit-5kissmallerwith5kexamplesand5
classes. Reddit-12kisbiggerwith12kexamplesand11classes.
D Post-HocSelectionResultsforRemainingDatasets
Table5comparesnaiveandpost-hocselectionfordatasetsnotcoveredinthemainpaper. Post-hoc
selectionismostlybetterthannaiveselection,althoughwithvaryingmargins. Post-hocselectionis
sometimesworse,butonlymarginally9.
9Thismaybeattributedto(1)pickingthesameepochforallrunsinpost-hocselection,and(2)generalization
errorbetweenvalidationandtestsetsfortheselectedepoch.
18Table 6: Detailed results for CIFAR-N datasets. Base denotes no transform and Final denotes
the SWA+Ens+TS transform. Gain shows performance improvement. ∆ shows change from
naive selection to post-hoc selection. Since Base and Gain columns involve multiple individual
runs,wereportmean ofthemetric. C-10-N,C-100-N-CandC-100-N-Fareshorthandsfor
±std.dev.
CIFAR-10-N,CIFAR-100-NCoarseandCIFAR-100-NFinerespectively.
Metric→ TestLoss TestError(%)
Dataset↓ Select↓ Epochs Base Final Gain Epochs Base Final Gain
C-10-N
Naive 90±9 0.435±0.012 0.234 0.201±0.012 92±5 9.75±0.24 8.30 1.45±0.24
Clean
Post-hoc 100 0.433±0.009 0.233 0.200±0.009 96 9.82±0.27 8.24 1.58±0.27
∆ ↑10±9 ↓0.001±0.005 ↓0.001 – ↑4±5 ↑0.07±0.10 ↓0.06 –
C-10-N
Naive 10±3 0.722±0.018 0.608 0.114±0.018 94±6 19.20±0.39 15.88 3.33±0.39
Aggre
Post-hoc 53 0.977±0.030 0.543 0.434±0.030 58 22.21±0.62 15.74 6.47±0.62
∆ ↑43±3 ↑0.255±0.027 ↓0.065 – ↓36±6 ↑3.00±0.73 ↓0.14 –
C-10-N
Naive 8±2 1.009±0.008 0.916 0.093±0.008 22±32 28.63±0.57 24.80 3.83±0.57
Rand1
Post-hoc 31 1.189±0.017 0.859 0.330±0.017 67 31.58±0.51 23.50 8.08±0.51
∆ ↑23±2 ↑0.181±0.018 ↓0.057 – ↑44±32 ↑2.95±0.95 ↓1.30 –
C-10-N
Naive 10±1 1.040±0.008 0.931 0.108±0.008 14±6 29.90±0.42 25.44 4.47±0.42
Rand2
Post-hoc 30 1.189±0.037 0.888 0.301±0.037 74 31.15±0.38 24.12 7.02±0.38
∆ ↑20±1 ↑0.150±0.038 ↓0.043 – ↑60±6 ↑1.24±0.56 ↓1.32 –
C-10-N
Naive 9±2 1.005±0.014 0.910 0.095±0.014 24±30 28.96±0.65 24.86 4.10±0.65
Rand3
Post-hoc 32 1.179±0.027 0.864 0.315±0.027 38 32.39±0.68 23.44 8.95±0.68
∆ ↑23±2 ↑0.174±0.031 ↓0.046 – ↑14±30 ↑3.43±1.03 ↓1.42 –
C-10-N
Naive 8±2 1.511±0.008 1.437 0.073±0.008 10±3 46.84±0.56 44.30 2.54±0.56
Worst
Post-hoc 25 1.643±0.019 1.399 0.245±0.019 24 49.67±0.74 42.88 6.79±0.74
∆ ↑17±2 ↑0.133±0.018 ↓0.039 – ↑14±3 ↑2.83±0.94 ↓1.42 –
C-100-N-C
Naive 33±35 1.011±0.014 0.669 0.342±0.014 91±4 23.12±0.40 19.36 3.76±0.40
Clean
Post-hoc 100 1.040±0.019 0.606 0.435±0.019 72 24.39±0.43 19.52 4.87±0.43
∆ ↑67±35 ↑0.029±0.023 ↓0.063 – ↓19±4 ↑1.27±0.26 ↑0.16 –
C-100-N-C
Naive 8±2 1.431±0.008 1.234 0.198±0.008 40±41 41.42±0.45 34.42 7.00±0.45
Noisy
Post-hoc 32 1.744±0.049 1.150 0.594±0.049 38 45.45±0.98 33.54 11.91±0.98
∆ ↑24±2 ↑0.313±0.048 ↓0.084 – ↓2±41 ↑4.03±1.13 ↓0.88 –
C-100-N-F
Naive 93±5 1.508±0.017 1.065 0.443±0.017 88±5 33.83±0.37 29.90 3.93±0.37
Clean
Post-hoc 75 1.567±0.019 1.063 0.504±0.019 95 33.86±0.53 29.94 3.92±0.53
∆ ↓18±5 ↑0.059±0.014 ↓0.002 – ↑7±5 ↑0.03±0.31 ↑0.04 –
C-100-N-F
Naive 7±2 2.416±0.022 2.129 0.287±0.022 91±7 58.68±0.49 51.34 7.34±0.49
Noisy
Post-hoc 27 3.015±0.079 1.994 1.021±0.079 32 63.53±0.55 50.26 13.27±0.55
∆ ↑20±2 ↑0.598±0.075 ↓0.135 – ↓59±7 ↑4.85±0.60 ↓1.08 –
Table7: DetailedresultsforLLMinstructiontuning. Bettervaluesareinbold.
Transform→ None SWA+TS SWA+Ens+TS
Metric↓ Naive Ours Naive Ours
Perplexity 3.756 3.471 3.461 3.245 3.142
Error 32.84 30.81 29.68 30.16 28.93
MMLU 46.64 46.78 47.03 47.23 47.54
E DetailedResults
Tables6,7,and8providedetailedresultsforCIFAR-N,LLMinstructiontuning,andotherdatasets
respectively.
F Noise-AwareTraining
Whileourexperimentsinthemainpaperusethestandardcross-entropy(CE)loss,hereweconsider
twoleadingtrainingobjectivesfromthelabelnoiseliterature: (1)SOP[42]and(2)ELR[39]. Tables
9,10and11comparenaiveandpost-hocselectionstrategiesforCIFAR-NdatasetsunderCE,SOP
andELRlossesrespectively. Hereagainwefindthatpost-hocselectionissuperiortonaiveselection
ingeneral. WealsonotethatthedifferencesbetweenCE,SOPandELRareminimal. Thisislikely
19Table8: Detailedresultsforotherdatasets. SeeTable6captionforadescription.
Objective→ TestLoss TestError(%)
Dataset↓ Select↓ Epochs Base Final Gain Epochs Base Final Gain
Naive 2±0 1.583±0.014 1.494 0.089±0.014 15±19 43.20±0.46 37.95 5.24±0.46
FMoW Post-hoc 50 2.831±0.053 1.305 1.526±0.053 48 43.18±0.55 34.93 8.24±0.55
∆ ↑48±0 ↑1.248±0.062 ↓0.189 – ↑33±19 ↓0.02±0.80 ↓3.02 –
Naive 2±1 0.908±0.008 0.841 0.067±0.008 9±8 39.41±0.76 36.18 3.23±0.76
Yelp Post-hoc 3 0.990±0.044 0.824 0.166±0.044 3 40.28±1.29 36.14 4.14±1.29
∆ ↑1±1 ↑0.082±0.040 ↓0.017 – ↓6±8 ↑0.87±1.16 ↓0.04 –
Naive 5±1 0.393±0.001 0.388 0.005±0.001 7±2 17.84±0.15 17.62 0.22±0.15
Income Post-hoc 11 0.421±0.007 0.385 0.036±0.007 19 19.21±0.14 17.40 1.81±0.14
∆ ↑6±1 ↑0.028±0.006 ↓0.003 – ↑12±2 ↑1.37±0.22 ↓0.22 –
Public
Naive 10±2 0.544±0.001 0.538 0.006±0.001 12±3 27.52±0.24 27.25 0.28±0.24
Coverage
Post-hoc 18 0.554±0.002 0.538 0.016±0.002 22 27.96±0.21 27.02 0.94±0.21
∆ ↑8±2 ↑0.010±0.002 ↓0.000 – ↑10±3 ↑0.44±0.25 ↓0.22 –
Naive 6±2 0.474±0.002 0.471 0.003±0.002 13±5 21.43±0.18 21.17 0.26±0.18
Mobility Post-hoc 14 0.476±0.003 0.468 0.008±0.003 11 21.40±0.17 21.24 0.16±0.17
∆ ↑8±2 ↑0.002±0.003 ↓0.003 – ↓2±5 ↓0.03±0.22 ↑0.07 –
Naive 8±1 0.380±0.000 0.378 0.003±0.000 14±4 17.94±0.08 17.72 0.22±0.08
Employment Post-hoc 15 0.383±0.001 0.377 0.006±0.001 30 18.27±0.12 17.83 0.43±0.12
∆ ↑7±1 ↑0.003±0.001 ↓0.000 – ↑16±4 ↑0.33±0.16 ↑0.11 –
Travel
Naive 6±2 0.597±0.002 0.596 0.001±0.002 9±1 35.77±0.34 35.44 0.32±0.34
Time
Post-hoc 15 0.626±0.003 0.591 0.035±0.003 16 36.40±0.20 35.23 1.17±0.20
∆ ↑8±2 ↑0.029±0.003 ↓0.005 – ↑7±1 ↑0.64±0.42 ↓0.21 –
Naive 52±18 0.492±0.044 0.439 0.053±0.044 75±28 20.65±1.06 20.40 0.25±1.06
Collab Post-hoc 163 1.075±0.122 0.404 0.671±0.122 152 20.95±1.26 18.80 2.15±1.26
∆ ↑111±18 ↑0.583±0.146 ↓0.035 – ↑77±28 ↑0.30±1.31 ↓1.60 –
Naive 15±4 1.154±0.022 1.101 0.053±0.022 13±5 47.42±0.64 47.09 0.33±0.64
Reddit-5k Post-hoc 44 1.448±0.058 1.085 0.362±0.058 45 50.75±1.83 45.49 5.26±1.83
∆ ↑29±4 ↑0.294±0.059 ↓0.015 – ↑32±5 ↑3.33±2.05 ↓1.60 –
Naive 16±3 1.405±0.011 1.367 0.038±0.011 17±4 51.78±1.05 50.34 1.45±1.05
Reddit-12k Post-hoc 41 1.585±0.023 1.346 0.239±0.023 64 55.85±0.97 51.26 4.59±0.97
∆ ↑25±3 ↑0.180±0.027 ↓0.021 – ↑47±4 ↑4.07±1.59 ↑0.92 –
Table 9: Naive vs post-hoc (ours) selection for CIFAR-N trained with cross-entropy (CE) loss.
Bettervaluesareinbold.
Metric→ TestLoss TestError(%)
Transform→ None SWA+TS SWA+Ens+TS None SWA+TS SWA+Ens+TS
Dataset↓ Naive Ours Naive Ours Naive Ours Naive Ours
C-10-NClean 0.435 0.269 0.270 0.234 0.233 9.75 9.09 9.10 8.30 8.24
C-10-NAggre 0.722 0.663 0.585 0.608 0.543 19.20 17.08 16.95 15.88 15.74
C-10-NRand1 1.009 0.968 0.907 0.916 0.859 28.63 27.13 24.84 24.80 23.50
C-10-NRand2 1.040 0.983 0.935 0.931 0.888 29.91 27.60 25.69 25.44 24.12
C-10-NRand3 1.005 0.963 0.911 0.910 0.864 28.96 26.91 25.09 24.86 23.44
C-10-NWorst 1.511 1.483 1.443 1.437 1.399 46.84 46.12 44.14 44.30 42.88
Clean 1.011 0.786 0.686 0.669 0.606 23.12 21.30 21.38 19.36 19.52
Noisy 1.431 1.330 1.235 1.234 1.150 41.42 38.08 35.87 34.42 33.54
C-100-NClean 1.508 1.215 1.205 1.065 1.063 33.83 32.67 32.69 29.90 29.94
C-100-NNoisy 2.416 2.289 2.136 2.129 1.994 58.68 54.94 53.18 51.34 50.26
becauseweusei.i.d. (andthereforenoisy)validationandtestsets,unliketheoriginalpaperswhich
usecleanvalidationandtestsets.
20Table10: Naivevspost-hoc(ours)selectionforCIFAR-NtrainedwithSOPloss. Bettervaluesarein
bold.
Metric→ TestLoss TestError(%)
Transform→ None SWA+TS SWA+Ens+TS None SWA+TS SWA+Ens+TS
Dataset↓ Naive Ours Naive Ours Naive Ours Naive Ours
C-10-NClean 0.425 0.270 0.269 0.236 0.235 9.65 8.82 8.81 7.96 8.00
C-10-NAggre 0.728 0.693 0.573 0.634 0.541 18.03 16.55 16.56 15.58 15.56
C-10-NRand1 1.025 0.980 0.888 0.925 0.851 26.91 24.53 24.50 23.20 23.14
C-10-NRand2 1.045 1.015 0.920 0.957 0.883 27.39 25.34 25.25 24.12 24.16
C-10-NRand3 1.016 0.975 0.889 0.921 0.851 26.66 24.23 24.23 23.02 22.96
C-10-NWorst 1.514 1.492 1.451 1.447 1.413 46.78 46.26 44.29 44.50 42.78
Clean 1.018 0.742 0.686 0.623 0.608 23.07 21.43 21.47 19.18 19.78
Noisy 1.427 1.347 1.229 1.247 1.145 41.39 38.01 35.85 34.32 33.94
C-100-NClean 1.513 1.213 1.203 1.063 1.061 33.79 32.66 32.68 29.46 29.56
C-100-NNoisy 2.415 2.268 2.137 2.118 1.997 58.34 54.76 53.48 51.06 50.54
Table11: Naivevspost-hoc(ours)selectionforCIFAR-NtrainedwithELRloss. Bettervaluesare
inbold.
Metric→ TestLoss TestError(%)
Transform→ None SWA+TS SWA+Ens+TS None SWA+TS SWA+Ens+TS
Dataset↓ Naive Ours Naive Ours Naive Ours Naive Ours
C-10-NClean 0.421 0.271 0.269 0.233 0.232 9.53 8.92 9.01 7.98 7.92
C-10-NAggre 0.730 0.659 0.584 0.606 0.541 19.02 16.86 16.80 15.34 15.52
C-10-NRand1 1.019 0.975 0.911 0.921 0.864 29.42 26.68 24.86 24.30 23.56
C-10-NRand2 1.042 0.994 0.939 0.941 0.893 29.79 27.98 25.74 26.12 24.50
C-10-NRand3 1.004 0.964 0.913 0.912 0.866 28.80 26.68 24.84 24.52 23.32
C-10-NWorst 1.508 1.492 1.443 1.444 1.397 46.94 46.27 44.03 44.64 42.48
Clean 1.030 0.760 0.686 0.644 0.605 23.07 21.27 21.40 19.28 19.24
Noisy 1.415 1.317 1.236 1.228 1.152 41.55 38.40 35.74 34.72 33.60
C-100-NClean 1.518 1.223 1.210 1.070 1.068 34.05 32.92 32.97 29.68 29.66
C-100-NNoisy 2.432 2.287 2.140 2.130 1.997 58.85 54.84 53.24 50.86 50.50
21