Multi-Agent Soft Actor-Critic with Global Loss for Autonomous
Mobility-on-Demand Fleet Control
Zeno Woywood∗1, Jasper I. Wiltfang∗1, Julius Luy1, Tobias Enders1, and Maximilian Schiffer2
1 School of Management,
Technical University of Munich, 80333 Munich, Germany
zeno.woywood@tum.de;jasper.wiltfang@tum.de;julius.luy@tum.de;tobias.enders@tum.de
2 School of Management & Munich Data Science Institute,
Technical University of Munich, 80333 Munich, Germany
schiffer@tum.de
Abstract
We study a sequential decision-making problem for a profit-maximizing operator of an Au-
tonomous Mobility-on-Demand system. Optimizing a central operator’s vehicle-to-request dis-
patching policy requires efficient and effective fleet control strategies. To this end, we employ
a multi-agent Soft Actor-Critic algorithm combined with weighted bipartite matching. We
propose a novel vehicle-based algorithm architecture and adapt the critic’s loss function to
appropriately consider global actions. Furthermore, we extend our algorithm to incorporate re-
balancingcapabilities. Throughnumericalexperiments,weshowthatourapproachoutperforms
state-of-the-art benchmarks by up to 12.9% for dispatching and up to 38.9% with integrated
rebalancing.
Keywords: hybrid learning and optimization, multi-agent learning, deep reinforcement learning,
global loss, autonomous mobility on demand
∗Both authors contributed equally.
4202
rpA
01
]YS.ssee[
1v57960.4042:viXra2
1. Introduction
Autonomous Mobility-on-Demand (AMoD) systems promise to further transform urban mobility
(Pavone 2015), following the lead of Mobility-on-Demand (MoD) providers like Uber and DiDi.
AMoD systems allow to offer MoD services with fast response times and point-to-point trips at
reduced cost, which changes an operators objective: for MoD, operational costs often depend on
hourlywagespaidtohumandrivers,suchthatoperatorsprioritizerevenuemaximization; forAMoD
only distance-related operational costs remain such that operators focus on optimizing profit (McK-
insey & Co. 2023). Additionally, AMoD systems allow for enhanced central control by leveraging
historical trip information and contextual knowledge to anticipatively control vehicels. In this con-
text, operators make two decisions: first, operators decide which customer requests to accept and
subsequently dispatch these to vehicles. Second, operators can rebalance vehicles to align vehicle
supply with anticipated customer demand. These decisions lead to a stochastic control problem,
which we study through the lense of Deep Reinforcement Learning (DRL). In this context, we con-
tributetothestateoftheartbyintroducinganovelparallelalgorithmarchitecture, whichcombines
multi-agent DRL with combinatorial optimization to ensure scalability and efficiency.
Related Work Control algorithms for (A)MoD systems range from rule-based heuristics (see, e.g.,
Hyland & Mahmassani 2018) to (learning-enriched) optimization approaches (see, e.g., Alonso-
Mora et al. 2017, Jungel et al. 2023), and Model Predictive Control (MPC) (see, e.g., Iglesias et al.
2018), but are only loosely related to this work as we focus on Reinforcement Learning (RL)-based
approaches. RL-based approaches split into two streams: single-agent approaches for dispatching
(Qin et al. 2020, Liu et al. 2022, Zheng et al. 2022) or rebalancing (Fluri et al. 2019, Gammelli et al.
2021,Jiaoetal.2021),andmulti-agentapproachesfordispatching(Wangetal.2018,Xuetal.2018,
Li et al. 2019, Tang et al. 2019, Zhou et al. 2019, Enders et al. 2023) and rebalancing (Holler et al.
2019, Li et al. 2022, Liang et al. 2022). Here, multi-agent approaches promise to increase scalability
to larger action spaces and remain the focus of our work. Accordingly, we limit our discussion to
recent works on multi-agent approaches in the following to ensure conciseness.
For dispatching, Li et al. (2019) proposed a mean field actor-critic algorithm, while Tang et al.
(2019) used a value-based approach with a cerebellar embedding, and Zhou et al. (2019) used Q-
LearningwithKL-divergenceoptimization. Endersetal.(2023)employedaSoftActor-Critic(SAC)
with bipartite matching. Earlier methods (cf. Xu et al. 2018, Wang et al. 2018) were shown to
be less effective than at least one of the aforementioned studies. For combined dispatching and
rebalancing, Li et al. (2022) used a graph neural network with a centralized critic for training, and
a decentralized actor-critic for execution. Holler et al. (2019) used Deep Q-Networks and Proximal
Policy Optimization with attention mechanisms to process global states. Liang et al. (2022) pro-
posed a two-stage algorithm for dispatching and rebalancing using centralized programming. All
methods treated dispatching and rebalancing as sequential decisions. The objectives of these works
vary as some maximize revenue (Holler et al. 2019, Li et al. 2019, Tang et al. 2019, Zhou et al.
2019), while others focus on profitability (Liang et al. 2022, Enders et al. 2023).3
Our work contributes to this stream of research as it is the first approach to take integrated
dispatching and rebalancing decisions in a multi-agent DRL setting. More importantly, we are the
first to show how to accurately derive a global loss to estimate future state values when combining
a multi-agent SAC with a global optimization layer to coordinate decisions, which we achieve via
bipartite matching.
Contribution To close the research gap outlined above, we present a novel multi-agent actor-critic
algorithm for AMoD fleet dispatching and integrated rebalancing under a profit maximization ob-
jective. Specifically, we propose a scalable parallel SAC architecture, wherein each vehicle functions
as an agent providing per-request weights, to address the operators’ need for a comprehensive and
feasible solution to its control problem. We obtain fleet-wide (global) actions by solving a bipartite
matching problem, wherein vehicles and requests represent vertices, and the agents’ per-request
weights the edge weights. To account for the differences between per-agent and global actions in-
duced by the combination of DRL and a global optimization layer, we show how to adapt the critic
loss function to a global level (“global loss”) leading to a more precise estimate of the value of future
states. We show that our algorithm outperforms state-of-the-art dispatching-only algorithms by
up to 12.9% on real-world datasets while maintaining stability and scalability. We further extend
our dispatching algorithm to include the concurrent option of rebalancing by providing artificial
rebalancing requests to the same model. The extended algorithm achieves superior performance, up
to 38.9%, compared to a rebalancing heuristic. To foster future research and ensure reproducibility,
our code is publicly available on .
https://github.com/tumBAIS/HybridMADRL-AMoD-Relocation
2. Control Problem
We consider a stochastic multi-stage control problem introduced in Enders et al. (2023), in which
a profit-maximizing central operator manages a fleet of K vehicles to serve stochastic customer
requests that enter the system over time, see Figure 1. In each time step t, the operator decides on
a batch of requests R that arrived in the system in between the last (t−1) and the current time
t
step t. For each request, the operator decides whether to reject the request or assigns it to a vehicle.
As the operator observes the full fleet, it takes decisions based on complete state information,
including the pick-up and drop-off zones of requests in its operating area. We characterize vehicles
by their current position and a buffer of at most two previously assigned requests that need to be
served within a given maximum waiting time. New requests join the system between two time steps
and rejected requests leave the system immediately. Vehicles take the shortest route to their next
destination to either drop off or pick up new customers. We determine the operator’s reward by the
sum of revenues from assigned requests minus operating costs for vehicle movements. We extend
this basic problem setting with a rebalancing decision, where the operator can redistribute idling
vehicles within the operating area to proactively match vehicle supply and anticipated customer
demand (see Section 3). For an extensive definition of the control problem, we refer to Section A.4
3. Methodology
Figure 2 gives an overview of our algorithm’s architecture. To derive an efficient and scalable
algorithm, we employ multi-agent SAC to keep the action space tractable. In this algorithm’s archi-
tecture, vehicles represent agents, which use the same actor neural network and share parameters
for efficiency. All agents evaluate all available requests and assign them a weight. We mask these
weightsasdescribedinSubsectionC.3toensurefeasibilityconstraints, andcreateaweightedbipar-
tite graph with vehicle agents and requests representing vertices accordingly. We use this weighted
matching, which can be solved by applying the Hungarian algorithm (Kuhn 1955), to obtain an
optimal vehicle-to-request allocation, i.e., a coordinated (global) action for all agents. During train-
ing, we use per-agent rewards to avoid a credit assignment problem (Chang et al. 2003, Agogino
& Tumer 2004). The critic learns from these rewards and provides per-agent values to the actor
to update its policy. To train actor and critic, we use the discrete version of the SAC algorithm,
i.e., Soft Actor-Critic Discrete (SACD) (Haarnoja et al. 2018, Christodoulou 2019). Our algorithm
scaleslinearlywiththenumberofrequestsoneagenthastoevaluate. TheemploymentoftheSACD
and the combination with bipartite matching are analogous to the algorithm proposed in Enders
et al. (2023). Contrarily to Enders et al. (2023), we use a vehicle-based parallel algorithm architec-
ture, a novel masking procedure, an adapted critic loss function, and finally extend our algorithm
to include rebalancing.
Algorithm Architecture We model each vehicle as an agent, see Figure 3. The agents’ neural
network architecture assesses each request based on the following input features: miscellaneous
features m contain general environment information, which is the same for all agents, e.g., the
t
current time in the period; vehicle features vj are agent-specific, e.g., the current position and
t
the request buffer; request features ri which we duplicate for all agents, e.g., the origin and the
t
destination. We combine all features to one input-vector per vehicle and request. For the complete
set of features, we refer to Subsection C.1.
Theactorassessesallcurrentlyavailablerequestsri,i ∈ {1,...,R }foronevehicleinparallel. To
t t
fix the input dimension of our agent’s neural network architecture, we pad the evaluated requests to
have a constant size of R . To facilitate more simultaneous requests and speed-up computation
max
for big instance sizes, one may limit R via a broadcasting range, which gives only the closest
max
Figure 1: Schematic visualization of vehicle dispatching (and rebalancing) over time.5
requests to each vehicle. We add one additional empty request r0 that the vehicle can choose if no
t
request is appealing, which indicates a reject action on the vehicle level.
We use a vehicle-based architecture as it enables the agents to build up reward trajectories corre-
spondingtotheiractions, therebyfosteringthelearningprocessandaccuraterewardallocation. We
Figure 2: Overview of our algorithm’s architecture, highlighting actor components (red), critic com-
ponents (blue) and a combinatorial optimization component (gray).
GlobalState
Actor Shared
Ag .e ..nt1 Request
Masking
Constrained W Bie pi ag rh tt ie td
e Global Environment
Parameters AgentK Weights Weights Matching Action
Per-Agent
Cr .i .t .ic1
Shared
Critic
GlobalState
Q-Values Parameters Per-AgentRewards
CriticK
Figure 3: Schematic visualization of our vehicle-based agent architecture in which we combine input
features to an input-vector (left) and use parallel neural networks (right).
Features Input ParallelDense Flatten Dense Softmax
r1 w1
t t
... ...
mt vtj ... ... ... ...
wRmax
t
r0 w0
t t
considereachinput-vectorseparatelyforthefirstfiveparalleldenselayerswithidenticalparameters,
essentially forming a trainable multi-layer embedding. We stack the two-dimensional output tensor
of the parallel layers before flattening them to a one-dimensional vector, which we further evaluate
in five dense layers. The final softmax activation outputs values ranging from 0 to 1, which we in-
terpret as per-request weights wi. All agents share one neural network architecture and parameters.
t
The critic uses the same algorithm architecture and features as the actor, but additionally receives
the global action as an input. For the detailed specifications of the algorithm as well as the final
hyperparameters, we refer to Section C.
Global Critic Loss SAC-Discrete SAC is an off-policy algorithm that incentivizes exploration
by finding an optimal stochastic policy π∗ via a maximum entropy objective. To this end, we
parameterize the actor and critic network with parameters ϕ and θ. We use two critic networks Q ∈
{Q1,Q2}tomitigateoverestimationalongwithtargetnetworkswithparametersθ¯toensureastable
target. We obtain the target parameters through an exponential moving average of the primary
critic parameters θ. In our multi-agent setting, the policy loss matching our control problem is
(cid:20) (cid:21)
(cid:88) (cid:16) (cid:110) (cid:111)(cid:17)
J (ϕ) = E π (a|s,j)T · αlog(π (a|s,j))− min Qm(a|s,j) . (3.1)
π (s,a¯,r,s′)∼D ϕ ϕ
m∈1,2
θ¯
j6
Here, we denote a transition by (s,a¯,r,s′), with global state s, next global state s′ and global
action a¯. We define r as the reward for agent j and sample transitions from a replay buffer D. We
j
define the policy’s entropy as log(π (a|s,j)), wherein α ∈ R controls the degree of exploration.
ϕ ≥0
We denote by a the agent-specific action, i.e., a reject or accept decision per request for vehicle j,
and by γ the discount factor. Note that π(a|s,j) represents the probability of vehicle j deciding
for a specific request. Hence, it corresponds to the weights w in Figure 3 (see Section 3), e.g.,
π(a = r1|s,j) = w1.
t t
A state-of-the-art critic-loss for our setting is
(cid:20) (cid:18) (cid:19)2(cid:21)
(cid:88) 1
J (θ) = E Q (a¯|s,j)−y , with
Q (s,a¯,r,s′)∼D θ j
2
(3.2)
j
(cid:16) (cid:110) (cid:111) (cid:17)
y = r +γ ·π (a′|s′,j)T · min Qm(a′|s′,j) −αlog(π (a′|s′,j))
j j ϕ
m∈1,2
θ¯ ϕ
as proposed in Enders et al. (2023). As π (a′|s′,j) is the per-agent probability of taking per-agent
ϕ
action a′ in the next state, we refer to Equation (3.2) as a local loss. The local loss does not
accurately reflect the probability of agent j executing a′ given s′. The local loss is only accurate, if
a′ is executed (cf. Subsection B.1). However, we derive the global action a¯ from the deterministic
bipartite matching algorithm, which uses the masked actors’ outputs as edge weights. Each agent
executes an action ab based on a¯ which differs from a′ due to discrete assignments and feasibility
j
constraints. To obtain a critic loss function that accurately reflects the global action, we define a
modified π (a′|s′,j), which we denote by π¯ (a′|s′,j) ∈ {0,1} that gets π¯ (a′|s′,j) = 1 if a′ = ab
ϕ ϕ ϕ j
and zero otherwise. Note that the distribution of π¯ is degenerate as its support reduces to ab. We
j
use the modified π¯ (a′|s′,j) to derive a new target y .
ϕ j
Proposition 1. Let us denote by y¯ the per-agent target based on π¯ (a′|s′,j). Then,
j ϕ
(cid:110) (cid:111)
y¯ = r +γ min Qm(ab|s′,j) . (3.3)
j j
m∈1,2
θ¯ j
For a proof of Proposition 1, we refer to Section B. The adjusted critic loss function now reads
(cid:20) (cid:18) (cid:19)2(cid:21)
(cid:88) 1
J (θ) = E Q (a¯|s,j)−y¯ . (3.4)
Q (s,a¯,r,s′)∼D θ j
2
j
and reflects a global loss, as we compute the critic loss using the agent-specific action which we
derived from the global action a¯. Herein, we obtain y¯ according to Equation (3.3). The actor loss
j
does not require adaptation as it learns from the critic’s predictions, which, in turn, already result
from the correct updates based on Equation (3.4).
IntegratedDispatchingandRebalancing Tointegraterebalancinginouralgorithm’sarchitecture,
weextendtheactortoassessfurtherinput-vectorsrepresentingpossiblerebalancingactions. Hence,
we employ artificial requests to imitate rebalancing as a dispatching decision (see Figure 1). To
achieve this, we model rebalancing movements as requests from the vehicle’s current position to
otherzonesintheoperatingarea. Ineachtimestep, avehicleassessesafixednumberofrebalancing7
requests. The vehicle can only rebalance if the vehicle idles. The integrated approach extends
variable customer requests C with time-persistent (re-)balancing requests B. This leads to a new
t
set of evaluated requests R = C ∪B. To enforce solely reasonable rebalancing movements, two
t t
rebalancing variants impact the generation of vehicle-specific rebalancing requests Bj: (i) each
t
vehiclecanrebalancetoeveryzone, exceptitsown, (ii)eachvehiclecanrebalancetoitsneighboring
zones. To leverage knowledge about the environment and enhance training, we alter the training
rewards of rebalancing actions. The agent receives a fare (positive or negative) depending on the
vehicle population of the current and the target zone. We refer to Subsection C.4 for further details
on the rebalancing requests and their training rewards.
4. Experimental Design
We use the well-established NYC Taxi dataset (TLC 2015) to benchmark our model against state-
of-the-art algorithms. We employ a hexagonal grid for spatial discretization to balance real-world
similarity with computational efficiency and differentiate two settings: a smaller 11 zone (larger 38
zone) setting with 500m (1000m) distance between adjacent zones in lower Manhattan. For the
11 small zones, we use the dataset without modifications. In the case of the 38 large zones, we
apply a downscaling factor of 20 to reduce the number of requests, ensuring a comparable fleet size
assessment in both scenarios. We consider daily one-hour intervals as episodes, spanning from 8:30
to 9:30 a.m. with a time step size of one minute. Throughout the year 2015, excluding holidays and
weekends, weincludeatotalof245dates, using200fortraining, 25forvalidation, and20fortesting
purposes. We configure the waiting time to be 5 (10) minutes. The cost parameter determines the
cost per distance driven and is set to make round trips profitable with a 20% margin.
Weconsidertwoexperiments: puredispatchingaswellasintegrateddispatchingandrebalancing.
For the experiment with pure dispatching, we model instances with supply shortage, as operators
in dispatching scenarios with infinite supply can fulfill any request immediately, making a myopic
policy sufficient. Thus, we set a maximum of 12 (20) requests per time step and deploy 12 (50)
vehicles. Fortheexperimentintegratingdispatchingandrebalancing,wemodelasupplyoverhangas
rebalancing is only valuable if vehicles idle. For the 38 large zones, we generate rebalancing requests
to neighboring zones only (see Subsection D.3). Thus, we set a maximum of 6 (10) requests per
time step and deploy 24 (120) vehicles. We add a constraint that requires vehicles to be in the zone
or en route to the origin zone of the request to foster rebalancing further. Moreover, we study the
sensitivity w.r.t. the number of vehicles by considering additional instances with ±50% vehicles.
Finally, we create an additional dataset by shifting the originally rather homogeneous and well-
balanced NYC Taxi dataset towards an imbalanced request distribution. The modified dataset has
one (two) distinct clusters of departing customers, and we refer to it as “Clustered” hereafter. For
details on the setup, evaluated scenarios, and hyperparameters we refer to Section D.
In the first experiment, we denote by V our algorithm using a local critic loss function (cf.
local
Equation 3.2) and by V our algorithm using a global critic loss function (cf. Equation 3.4).
global8
We benchmark our dispatching performance against two algorithms: an algorithm using request-
vehicle combinations as in Enders et al. (2023) (RVC) and a greedy policy (Greedy). The Greedy
algorithm weighs every request according to its profitability before matching. Second, we show
the capabilities of our integrated dispatching and rebalancing algorithm by extending V with
global
rebalancing(V ). Here, webenchmarkagainstGreedy, ouralgorithmwithoutrebalancingand
extended
a rebalancing heuristic (Heuristic) that distributes the number of vehicles per zone equally. For a
detailed description of all benchmarks, see Subsection D.2.
5. Results and Discussion
Dispatching Figure 4 shows the validation results of all four algorithms for dispatching. RVC
and V exhibit a stable performance throughout training in both settings. RVC converges to
global
the same reward as Greedy, while V converges to a significantly higher reward. The validation
global
results of V show stability issues, especially for 38 zones. For the larger setting, V requires
local global
more training steps until convergence as the complexity rises with more vehicles and requests. The
validation reward across training episodes of V demonstrates that our algorithm exhibits a high
local
variance and a convergence below Greedy, when neglecting the global critic loss function. These
observations show that the work of Enders et al. (2023) improves stability and performance by
employing one agent per request-vehicle combination, which improves performance even when using
a local loss. While a vanilla implementation of our parallel vehicle-based algorithm is inferior to
suchanapproach,utilizingouralgorithmwithaglobalcriticlossallowsforsignificantimprovements
as the combination of a global critic loss and a vehicle-based agent representation better reflects
problem dynamics and consequently eases the learning.
Figure 4: AveragevalidationrewardsofallfouralgorithmsonbothNYCsettings. Theshadowedarea
is the minimum and maximum value across 5 seeds.
11 small zones 38 large zones
400
1500
300 Greedy
1000
200
500 RVC
100
0 0 V
-100 -500 local
-200 -1000 V global
-300 -1500
50k 100k 150k 200k 50k 100k 150k 200k
Training steps Training steps
Table1andFigure5showeachalgorithm’simprovementintestperformancecomparedtoGreedy
corresponding to the validation results in Figure 4. V outperforms Greedy significantly, RVC
global
exhibitsatestresultclosetoGreedy andV performsworsethanGreedy. Thestrongperformance
local
of V highlights the significance of employing a global critic loss. By contrast, the low perfor-
global
mance of V indicates that the error from using a local loss function substantially hinders the
local
search for a stable and performant policy, which increases with a higher number of vehicles. RVC
evaluates one request-vehicle combination at a time effectively splitting the vehicle into multiple
draweR draweR9
agents. Thus,RVC lackstheabilitytoperformanticipatoryplanningforindividualvehicleroutesas
request-vehicle combinations only exist in the current time step. Consequently, RVC inadequately
accounts for the subsequent state s′ and the error resulting from bipartite matching, as it employs
the current state for target retrieval, leading to a biased estimate that does not accurately predict
the future state, yielding near Greedy results.
Figure 5: Test performances for dispatching (0% indicates equal performance to Greedy). Each dot
represents the average of one test date across 5 seeds.
11 small zones 38 large zones
20% 20%
10% 10% Greedy
0% RVC
0%
-10% Vlocal
-10% -20% Vglobal
-20% -30%
Table 1: Performance improvement to Table 2: Structuralcomparisonofdifferentalgorithmson
Greedy for dispatching across 5 the 11 small zones setting (average across 5
seeds. seeds).
Algo. 11 small 38 large Metric Greedy RVC V
global
Greedy 350.9 1529.5 Rejects to empty (%) 18.3 18.2 44.7
RVC +1.9% −1.1% Rejects to occupied (%) 19.2 19.8 49.9
V −4.8% −21.0% Pick-up distance (zones) 2.4 2.4 0.2
local
V +12.9% +12.9% Waiting time (min) 3.8 3.7 2.5
global
Our vehicle-based architecture, on the other hand, is able to form a reward trajectory which is
necessary for long-term planning. As the algorithm evaluates all requests and receives a reward
corresponding to only its actions, it gains a correct estimate of the next state’s value and can
therefore optimize its subsequent dispatching decisions.
To analyze the algorithms’ policies, we compare the metrics presented in Table 2 for profitable
requests, i.e., requests for which the fare is higher than the vehicle-specific distance cost and which
can be fulfilled within the waiting time. The metric “rejects to empty” considers empty zones as
destination and “rejects to occupied” counting zones with more than one vehicle located at the
destination. We divide both metrics by the total number of requests to retrieve their ratio. The
“pick-up distance” presents the distance driven to the pick-up zone from the current destination of
the vehicle. Lastly, the “waiting time” is the average time a customer waits until pick-up. Greedy
andRVC behavesimilarly, butV rejectswith44.7%morethandoubletheamountofprofitable
global
requests, keeps waiting times shorter and reduces pick-up distance by a factor of ten. Hence, V
global
learns a different policy in two ways. First, it better utilizes implicit rebalancing by preferring
requests that allow heading towards empty zones over requests ending in occupied zones. Second, it
rejects more low-profit requests to wait for those with higher profits, particularly favoring requests
starting in the vehicle’s current zone as indicated by the low pick-up distance. This leads to lower
waiting times, as future customers rarely have to wait until the vehicle arrives at their location.
CYN CYN10
Such a policy proves effective in instances with supply shortages, where the opportunity cost of
rejecting requests is low.
Integrated Dispatching and Rebalancing Figure 6 shows the improvement in test performance
compared to Greedy for integrated dispatching and rebalancing. V performs close to Greedy
global
in all settings, whereas Heuristic and V perform significantly better than Greedy especially
extended
on the Clustered dataset. The low performance improvement of V compared to dispatching-
global
only bases on the oversupply of vehicles in the tested instances, for which Greedy is inherently
strong. Note that for the Clustered dataset, Heuristic and V are able to outperform Greedy
extended
by 96.2% (51.3%) and 101.0% (111.5%) respectively. Thus, our algorithm performs 39.8% better
than Heuristic on the larger setting, but only 1.3% on the smaller one. The result of Heuristic for
the smaller setting indicates that the rule-based rebalancing policy matches the distribution and
frequency of requests for this instance size well.
Furthermore, we explore the sensitivity of varying number of vehicles on operator returns, see
Table 3. Notably, our algorithm V demonstrates an overall increasing relative performance
extended
Figure 6: Testperformancesonbothscenariosanddatasetsforintegrateddispatchingandrebalancing
compared to Greedy. Each dot represents the average of one test date across 5 seeds.
11 small zones with 24 Vehicles 38 large zones with 120 vehicles
40% 20%
20% 10%
V
0% 0% global
Heuristic
150% 150%
V
100% 100% extended
50% 50%
0% 0%
Table 3: Performance improvement to Greedy for dispatching and rebalancing to compare the impact
of varying numbers of vehicles K (ceteris paribus) across 5 seeds.
Zones 11 small zones 38 large zones
Setting Algo. | K #12 #24 #36 #60 #120 #180
Greedy 297.3 418.3 479.6 1594.4 2282.7 2674.1
V +2.0% +1.7% +1.6% +1.4% +1.0% +0.7%
NYC global
Heuristic +6.4% +14.2% +10.6% +5.6% +8.3% +8.4%
V +9.9% +20.0% +21.3% +6.4% +12.3% +10.1%
extended
Greedy 137.2 198.8 241.8 429.3 688.5 882.6
V +1.2% +3.1% +0.8% +1.4% +1.0% +0.7%
Clustered global
Heuristic +74.1% +96.2% +70.5% +66.5% +51.3% +48.6%
V +85.6% +101.0% +96.7% +107.7% +111.5% +106.4%
extended
improvementincomparisontoHeuristic asthenumberofvehiclesrises. Thissupportsthescalability
of our algorithm and indicates that V effectively capitalizes on past experiences, anticipating
extended
future requests and incorporating the spatial distribution, especially for the Clustered dataset.
CYN
deretsulC11
Determining the optimal fleet size is imperative for operators aiming to maximize their profits
gained from their rebalancing policy. When the fleet size is small, the degree of freedom to improve
is also small. When the fleet size is excessively large, rebalancing becomes less profitable in relative
terms as Greedy becomes sufficient.
6. Conclusion
This work addresses the fleet control problem of a profit-maximizing AMoD operator and offers a
comprehensive solution, including the implementation code. We solve the dispatching problem by
proposing a novel multi-agent SACD architecture, in which each agent first evaluates requests in
parallelandcombinesthemafterwardsforavehicle-basedoutput. Thus,ouralgorithmensurescom-
putational efficiency while optimizing its reward trajectory through long-term planning. To obtain
global actions from agent actions, we employ weighted bipartite matching. We show an error in the
critic’s loss function and propose an adaptation of the critic’s target to accommodate the bipartite
matching. By adjusting the critic loss function to a global loss, we obtain a more accurate estimate
of the next state’s value fostering the learning process of our agent. Additionally, we extend our
dispatching algorithm by incorporating concurrent rebalancing capabilities. Experimental results
show that our approach surpasses state-of-the-art benchmarks while demonstrating stability across
different instances. For dispatching, we outperform the closest benchmark by up to 12.9% and for
integrated dispatching and rebalancing by up to 38.9%. In future work, we will extend the AMoD
fleet control problem by covering charging and investigate how the adaptation to a global critic
loss function impacts the performance of DRL models with combinatorial optimization in other
competitive multi-agent instances.
References
Agogino, A. K., & Tumer, K. (2004). Unifying temporal and structural credit assignment problems. In Proceedings
of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 2 (p.
980–987).
Alonso-Mora,J.,Samaranayake,S.,Wallar,A.,Frazzoli,E.,&Rus,D.(2017).On-demandhigh-capacityride-sharing
via dynamic trip-vehicle assignment. Proceedings of the National Academy of Sciences, (pp. 462–467).
Bösch,P.M.,Becker,F.,Becker,H.,&Axhausen,K.W.(2018).Cost-basedanalysisofautonomousmobilityservices.
Transport Policy, 64, 76–91.
Chang, Y.-h., Ho, T., & Kaelbling, L. (2003). All learning is local: Multi-agent learning in global reward games. In
Advances in Neural Information Processing Systems (pp. 1–8).
Christodoulou, P. (2019). Soft actor-critic for discrete action settings. CoRR, abs/1910.07207. URL: http://arxiv.
org/abs/1910.07207. arXiv:1910.07207.
Enders, T., Harrison, J., Pavone, M., & Schiffer, M. (2023). Hybrid multi-agent deep reinforcement learning for
autonomous mobility on demand systems. In Proceedings of The 5th Annual Learning for Dynamics and
Control Conference (pp. 1284–1296).
Fluri, C., Ruch, C., Zilly, J., Hakenberg, J., &Frazzoli, E.(2019). Learningtooperateafleetofcars. In2019 IEEE
Intelligent Transportation Systems Conference (ITSC) (pp. 2292–2298).
Gammelli, D., Yang, K., Harrison, J., Rodrigues, F., Pereira, F. C., & Pavone, M. (2021). Graph neural network
reinforcementlearningforautonomousmobility-on-demandsystems.In202160thIEEEConferenceonDecision
and Control (CDC) (pp. 2996–3003).12
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep
reinforcementlearningwithastochasticactor. InProceedingsofthe35thInternationalConferenceonMachine
Learning (pp. 1861–1870).
Holler, J., Vuorio, R., Qin, Z., Tang, X., Jiao, Y., Jin, T., Singh, S., Wang, C., & Ye, J. (2019). Deep reinforcement
learningformulti-drivervehicledispatchingandrepositioningproblem. In2019IEEEInternationalConference
on Data Mining (ICDM) (pp. 1090–1095).
Hyland,M.,&Mahmassani,H.S.(2018). Dynamicautonomousvehiclefleetoperations: Optimization-basedstrate-
gies to assign avs to immediate traveler demand requests. Transportation Research Part C: Emerging Tech-
nologies, (pp. 278–297).
Iglesias, R., Rossi, F., Wang, K., Hallac, D., Leskovec, J., & Pavone, M. (2018). Data-driven model predictive
control of autonomous mobility-on-demand systems. In 2018 IEEE International Conference on Robotics and
Automation (ICRA) (p. 1–7).
Iqbal, S., & Sha, F. (2019). Actor-attention-critic for multi-agent reinforcement learning. In Proceedings of the 36th
International Conference on Machine Learning (pp. 2961–2970).
Jiao, Y., Tang, X., Qin, Z., Li, S., Zhang, F., Zhu, H., & Ye, J. (2021). Real-world ride-hailing vehicle repositioning
using deep reinforcement learning. Transportation Research Part C: Emerging Technologies, (p. 103289).
Jungel, K., Parmentier, A., Schiffer, M., & Vidal, T. (2023). Learning-based online optimization for autonomous
mobility-on-demand fleet control. URL: https://arxiv.org/abs/2302.03963. doi:10.48550/ARXIV.2302.03963.
Kuhn,H.W.(1955). TheHungarianMethodfortheAssignmentProblem. Naval Research Logistics Quarterly,(pp.
83–97).
Kullman,N.D.,Cousineau,M.,Goodson,J.C.,&Mendoza,J.E.(2022). Dynamicride-hailingwithelectricvehicles.
Transportation Science, 56, 775–794.
Li, B., Ammar, N., Tiwari, P., & Peng, H. (2022). Decentralized ride-sharing of shared autonomous vehicles us-
ing graph neural network-based reinforcement learning. In 2022 International Conference on Robotics and
Automation (ICRA) (pp. 912–918).
Li,M.,Qin,Z.,Jiao,Y.,Yang,Y.,Wang,J.,Wang,C.,Wu,G.,&Ye,J.(2019).Efficientridesharingorderdispatching
with mean field multi-agent reinforcement learning. In The World Wide Web Conference (p. 983–994).
Liang, E., Wen, K., Lam, W. H. K., Sumalee, A., & Zhong, R. (2022). An integrated reinforcement learning and
centralized programming approach for online taxi dispatching. IEEE Transactions on Neural Networks and
Learning Systems, (pp. 4742–4756).
Liu, Z., Li, J., & Wu, K. (2022). Context-aware taxi dispatching at city-scale using deep reinforcement learning.
IEEE Transactions on Intelligent Transportation Systems, (pp. 1996–2009).
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., & Mordatch, I. (2017). Multi-agent actor-critic for mixed
cooperative-competitive environments. In Advances in Neural Information Processing Systems (pp. 1–12).
McKinsey & Co. (2023). Where does shared autonomous mobility go next? URL: https://www.mckinsey.com/
industries/automotive-and-assembly/our-insights/where-does-shared-autonomous-mobility-go-next#/.
Pavone,M.(2015). Autonomousmobility-on-demandsystemsforfutureurbanmobility. InM.Maurer,J.C.Gerdes,
B. Lenz, & H. Winner (Eds.), Autonomes Fahren: Technische, rechtliche und gesellschaftliche Aspekte (pp.
399–416). Springer Berlin Heidelberg.
Qin, Z., Tang, X., Jiao, Y., Zhang, F., Xu, Z., Zhu, H., & Ye, J. (2020). Ride-hailing order dispatching at didi via
reinforcement learning. INFORMS Journal on Applied Analytics, (p. 272–286).
Tang, X., Qin, Z., Zhang, F., Wang, Z., Xu, Z., Ma, Y., Zhu, H., & Ye, J. (2019). A deep value-network based
approachformulti-driverorderdispatching.InProceedingsofthe25thACMSIGKDDInternationalConference
on Knowledge Discovery & Data Mining (p. 1780–1790).
TLC, N. (2015). Trip record data. URL: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page.
Wang,Z.,Qin,Z.,Tang,X.,Ye,J.,&Zhu,H.(2018). Deepreinforcementlearningwithknowledgetransferforonline
rides order dispatching. In 2018 IEEE International Conference on Data Mining (ICDM) (pp. 617–626).
Xu,Z.,Li,Z.,Guan,Q.,Zhang,D.,Li,Q.,Nan,J.,Liu,C.,Bian,W.,&Ye,J.(2018). Large-scaleorderdispatchin
on-demandride-hailingplatforms: Alearningandplanningapproach.InProceedingsofthe24thACMSIGKDD
International Conference on Knowledge Discovery & Data Mining (p. 905–913).
Zheng, B., Ming, L., Hu, Q., Lü, Z., Liu, G., & Zhou, X. (2022). Supply-demand-aware deep reinforcement learning
for dynamic fleet management. ACM Trans. Intell. Syst. Technol., (pp. 1–19).
Zhou, M., Jin, J., Zhang, W., Qin, Z., Jiao, Y., Wang, C., Wu, G., Yu, Y., & Ye, J. (2019). Multi-agent reinforce-
ment learning for order-dispatching via order-vehicle distribution matching. In Proceedings of the 28th ACM
International Conference on Information and Knowledge Management (p. 2645–2653).13
Appendix A Control Problem
In the following, we detail our control problem, which we adapted from Enders et al. (2023). We
focus on a profit-maximizing central operator that manages a (possibly autonomous) vehicle fleet
with a fixed number of vehicles to serve stochastic customer requests. The operator needs to make
decisions immediately as customers demand immediate feedback. Hence, shifting rejected requests
into the next time step is not possible. After the operator accepts a request, a customer must be
picked up within a given maximum waiting time ωmax ∈ N , and the operator needs to dispatch a
0
vehicle to fulfill the request accordingly.
We extend this basic dispatching problem setting with a rebalancing decision, where the operator
can redistribute idling vehicles within the operating area. We formalize the control problem as a
Markov Decision Process (MDP), which we describe in the following:
Preliminaries: We consider discrete time steps t within a given time horizon T = {0,1,...,T},
and represent the grid-based operating area as a graph G = (V,E) with weight vectors ew =
(ewd,ewt) ∈ R ×N, denoting the distance (ewd) and the number of time steps (ewt) required to
>0
traverse an edge e ∈ E. Vertices v ∈ V represent the centers of zones in the operating area. The
neighbors of a vertex v are denoted by N (v) = {u | (u,v) ∈ E}.
G
States: We denote the system state at time step t ∈ T by S = (t,(kj ) ,(ri) ),
t t j∈{1,...,K} t i∈{1,...,Rt}
with K representing the number of vehicles kj, j ∈ {1,...,K} and R being the number of new
t t
requests ri, i ∈ {1,...,R }.
t t
We describe vehicles k = (p,τ,1r,2r), with a position p ∈ V, a number of time steps τ ∈ N left
0
to reach this position and a vehicle-specific request buffer 1r,2r. Here, p can either be the current
vertex for an idling vehicle or the next vertex on the vehicle’s route if it is traveling. To account for
realistic trip lengths and maximum waiting times, we restrict the number of requests preassigned
to a vehicle to two. An idling vehicle is characterized by an empty request buffer, i.e., 1r = ∅ as
the request buffer fills up in a first-in-first-out fashion. We label the position of vehicle kj as pj and
t t
label the other attributes of the vehicle in the same notation.
The set R = C ∪B contains all available requests in time step t with R = |R |, incl. variable
t t t t
customer requests C and time-persistent (re-)balancing requests B. If the operator does not allow
t
vehicle rebalancing, no rebalancing requests are available, i.e., B = ∅ and R = C . We describe
t t
requests r = (ω,o,d) with a waiting time ω ∈ N ∪ ∅ tracking the elapsed time from request
0
placement to pick-up, an origin o ∈ V, and a destination d ∈ V. To enable the representation of
customer and rebalancing requests based on the triplet r = (ω,o,d), rebalancing requests obtain an
empty waiting time (ω = ∅) and an origin corresponding to their destination (o = d).
Toallowforrebalancing,onecreatesanartificialsetofrebalancingrequestsB = {(∅,v,v) | v ∀ V}
toconsiderallverticesasrebalancingdestinations. Moreover,theoperatorcannotrebalancemultiple
vehicles to the same zone in one time step, as we create only one rebalancing request for each zone.
However,onecaneasilyrelaxthisconstraintbycreatingmultiplerebalancingrequestsforeachzone.
To reduce the possible action space and enforce solely reasonable rebalancing movements, we define14
a vehicle-specific rebalancing request set. We consider two variants to obtain this set for individual
vehicles that permit equivalent rebalancing behavior:
(i) each vehicle j can rebalance to every zone, except its own, i.e., Bj = {(∅,v,v) | v ∀ V \pj},
t t
(ii) each vehicle j can rebalance to its neighboring zones, i.e., Bj = {(∅,v,v) | v ∀ N (pj )}.
t G t
This leads to a new set of evaluated requests per vehicle C ∪Bj.
t t
Actions: The action space of the central operator reads
(cid:26) (cid:16) (cid:17) (cid:12)
A(S ) = a1,...,aRt (cid:12) ai = 0 ∨
t t t (cid:12) t
(ai = j ∧2rj = ∅∧ri ∈ C ) ∨
t t t t
(ai = j ∧1rj = ∅∧ri ∈ Bj ) (A.1)
t t t t
∀ i ∈ {1,...,R }∧j ∈ {1,...K},
t
(cid:88)Rt (cid:27)
1(ai = j) ≤ 1, ∀ j ∈ {1,...,K}
t
i=1
The operator can take one decision ai per request ri,i ∈ {1,...,R }. The operator can either
t t t
reject it (ai = 0) or assign it to a vehicle (ai = j), which is only possible if the vehicle has a free
t t
place in its request buffer and the request is a customer request, i.e., 2rj = ∅∧ri ∈ C . A vehicle
t t t
can also rebalance (ai = j) under the condition that it idles and the request is a vehicle-specific
t
rebalancing request, i.e., 1rj = ∅∧ri ∈ Bj. Each vehicle can be assigned to at most one request per
t t t
time step.
Transition: First we describe the action-dependent transition from the pre-decision state S to
t
the post-decision state S t′. A reject decision does not alter the state. If a (rebalancing) request is
assigned to a vehicle, the request is appended to that vehicle’s request buffer.
The transition from the post-decision state to the next system state S is independent from
t+1
the action and follows system dynamics. If a vehicle picks up a customer at its origin vertex, the
waiting time of that request is reset. If a vehicle finds itself at a vertex and is serving a request, the
position of the vehicle is updated based on the route taken. If a vehicle drops off a customer before
the next decision is made, the request buffer pops the old request and shifts the second request
if available. The waiting time for accepted but not yet served requests increases by the time step
increment. Rejected requests are dropped and leave the system immediately. New requests appear
according to an unknown distribution.
Rewards: The central operator aims to maximize the fleet profit. As autonomous vehicles have
negligible fixed costs after an initial investment, the operational costs c ∈ R and trip fares base
≤0
on the driven distance. Each request is billed individually, depending on whether a car has picked
up the customer on time. The revenue per request rev(r) ∈ R bases on the operator’s pricing
>0
model and the total revenue per time-step over all vehicles depends on the post-decision state15
K
(cid:88)
Rev(S t′) = 1(1rj
t′
̸= ∅∧τ tj
′
= 0∧pj
t′
= o(1rj t′)∧ω(1rj t′) ≤ ωmax) rev(1rj t′)
j=1
Regardlessofwhethercustomersareonboard,thevehicleincursvariablecostsdenotedbyc ∈ R
≤0
per unit of distance traveled (see, e.g., Bösch et al. 2018, McKinsey & Co. 2023). Based on the
vehicle’s route from the post-decision position pj to the next pre-decision position pj , the total
t′ t+1
cost per time-step results to
K
Cost(S t′) = c (cid:88) 1(1rj
t′
̸= ∅∧τ tj
′
= 0) (pj t′,pj t+1)wd
j=1
The total profit at time t′ is Profit(S t′) = Rev(S t′)−Cost(S t′). The post-decision state S t′ is a
function of the pre-decision state S
t
and the taken action a
t
∈ A(S t). Thus, we write Profit(S t′) =
Profit(S ,a ).
t t
ThecentralizedoperatoroftheAMoDfleetaimstofindapolicyπ(a |S )maximizingtheexpected
t t
total profit over the time horizon T, based on the initialized state S :
0
(cid:34) T (cid:88)−1 (cid:12) (cid:35)
Profit(S ) = maxE Profit(S ,a )(cid:12)S .
0
π
(St,at)∼π t t (cid:12) 0
t=0
Appendix B Proof of Proposition 1
Proposition 1. Let us denote by y¯ the per-agent target using π¯ (a′|s′,j). Then:
j ϕ
(cid:110) (cid:111)
y¯ = r +γ min Qm(ab|s′,j)
j j
m∈1,2
θ¯ j
Proof. First, we express the target y¯ based on π¯ (a′|s′,j) using the SACD target definition in
j ϕ
Equation (3.2)
(cid:16) (cid:110) (cid:111) (cid:17)
y¯ = r +γπ¯ (a′|s′,j)T · min Qm(a′|s′,j) −αlog(π¯ (a′|s′,j)) . (B.1)
j j ϕ
m∈1,2
θ¯ ϕ16
Now, we develop (B.1) and start with the scalar product as the sum over all actions belonging to A
(cid:20) (cid:21)
(cid:88) (cid:16) (cid:110) (cid:111) (cid:17)
y¯ = r +γ π¯ (a′|s′,j) min Qm(a′|s′,j) −αlog(π¯ (a′|s′,j))
j j ϕ
m∈1,2
θ¯ ϕ
a′∈A
( =I)
r +γπ¯
(ab|s′,j)(cid:16) min(cid:110) Qm(ab|s′,j)(cid:111)
−αlog(π¯
(ab|s′,j))(cid:17)
j ϕ j
m∈1,2
θ¯ j ϕ j
(cid:20) (cid:21)
(cid:88) (cid:16) (cid:110) (cid:111) (cid:17)
+γ π¯ (a′|s′,j) min Qm(a′|s′,j) −αlog(π¯ (a′|s′,j))
ϕ
m∈1,2
θ¯ ϕ
a′∈A\ab
j
( =II)
r +γπ¯
(ab|s′,j)(cid:16) min(cid:110) Qm(ab|s′,j)(cid:111)
−αlog(π¯
(ab|s′,j))(cid:17)
j ϕ j m∈1,2 θ¯ j ϕ j (B.2)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=1 =0
(cid:20) (cid:21)
(cid:88) (cid:110) (cid:111)
+γ π¯ (a′|s′,j) min Qm(a′|s′,j) −π¯ (a′|s′,j)αlog(π¯ (a′|s′,j))
ϕ
m∈1,2
θ¯ ϕ ϕ
a′∈A\ab j (cid:124) (cid:123)(cid:122) (cid:125)
=0
(cid:20) (cid:21)
(I =II) r +γ min(cid:110) Qm(ab|s′,j)(cid:111) −γ (cid:88) π¯ (a′|s′,j)αlog(π¯ (a′|s′,j))
j
m∈1,2
θ¯ j (cid:124)ϕ
(cid:123)(cid:122)
ϕ
(cid:125)
a′∈A\ab
j =0
(cid:110) (cid:111)
= r +γ min Qm(ab|s′,j) .
j
m∈1,2
θ¯ j
In the above derivation, we used the following reasoning:
(I) We divide the sum into terms depending on the action ab and terms depending on the set of
j
actions A\ab. Here, ab is the agent-specific action derived from the global action a¯.
j j

1, if a′ = ab
(II) We recall that π¯ (a′|s′,j) = j , as the distribution π¯ (a′|s′,j) is degenerate.
ϕ ϕ
0, if a′ ̸= ab
j
(III) We use the rule of L’Hôpital to compute π¯ (a′|s′,j)Tαlog(π¯ (a′|s′,j)) for π¯ (a′|s′,j) = 0,
ϕ ϕ ϕ
if a′ ̸= ab. Let us denote x = π¯ (a′|s′,j), then the following holds
j ϕ
lim x log(x) = lim logx L′H =pital lim x1 = lim −x2 = lim −x = 0. (B.3)
x→0 x→0 1 x→0 − 1 x→0 x x→0
x x2
Hence, we set π¯ (a′|s′,j)Tαlog(π¯ (a′|s′,j)) = 0.
ϕ ϕ
B.1 Dependency on the Environment
We note here, that the usage of the unadjusted loss y can lead to good results depending on
j
the environment. Using the unadjusted loss y in unconstrained environments, where the agent can
j
executeitsactiondirectly, e.g., cooperativeenvironmentsthatdetermineonlytherewarddepending
on the action of others, can yield good results (Iqbal & Sha 2019). However, in our problem setting
the matching algorithm introduces a competitive component leading to a constrained environment.
In competitive environments, where the executed actions depend on the actions of others, using
the per-agent action leads to an increasingly inaccurate y with a higher number of agents. The
j
increasing mismatch between per-agent actions and the actually executed actions, derived from the17
globalaction,isshownbyLoweetal.(2017)wheretheyproposetheirmulti-agentdeepdeterministic
policy gradient algorithm.
Appendix C Methodology
In the following, we describe methodological details. First, we present our chosen features and
architecture details of the employed neural networks. Second, we outline the hyperparameters used
in our experiments. Third, we explain the masking, which we use to retrieve the weights for our
bipartite matching. Fourth, we describe details regarding the definition of rebalancing requests and
the corresponding reward mechanisms applied during the training process.
C.1 Feature & Neural Network Architectures
In this section, we describe the features used for our new parallel architecture in detail. The input
state for the first parallel dense layer is specific to the respective agent j and considers all available
requests i ∈ {1,...,R }. As we fix the number of requests per agent, we extend R to R through
t t max
padding to have a static input size. We use a spatial discretization to divide the operating area into
a hexagonal grid. Each grid zone represents a central point with horizontal and vertical indices. We
map all possible locations of the operating area to zones. The encoding of a location is based on
the normalized indices of the respective zone.
Wenowlistthespecificenvironmentinformationthatweuseasinputstogeneratefeaturesforthe
neural network. The inputs differ slightly depending on whether rebalancing is enabled (V )
extended
or whether only the critic evaluates the feature (Critic):
• Miscellaneous input features m contain general environment information and are the same
t
for all agents:
– Time step in the episode, normalized to [0,1]
– Aggregated time steps needed for all vehicles to reach their final destination positions, normal-
ized to [0, 1], which indicates the current level of activity within the fleet
– Count of requests placed since start of current episode, divided by the count of requests placed
on average until the current time step, which indicates the observed demand compared to an
average episode
• Vehicle input features vj are agent-specific:
t
– Normalized encoding of vehicle’s end position, which is the current vertex if no request is
assigned, otherwise destination of assigned request that will be served last
– Time steps to reach this position, divided by the maximum time between any two vertices in
the graph representing the operating area
– Number of assigned requests, normalized to [0,1]
• Request input features ri, duplicated for all agents:
t
– Normalized encoding of the request origin zone
– Normalized encoding of the request destination zone18
– Distance from origin to destination on graph G, normalized by maximum distance to [0, 1]
– (V ) Rebalancing flag in [0,1]
extended
– (V ) Number of vehicles with final destination in pick-up zone, normalized to [0,1]
extended
– (V ) Number of vehicles with final destination in drop-off zone, normalized to [0,1]
extended
• Request-Vehicle input features (ri,vj) , specified per request i and vehicle j:
t
– Distance from vehicle position to request origin, normalized by maximum distance to [0, 1]
– (Critic) Waiting time flag in [0,1], which indicates whether a request can be picked up in time
Next, we describe the actor network. The general structure of the input processing resembles
prior work by Enders et al. (2023) using the attention mechanism and embeddings presented in
Holler et al. (2019) and Kullman et al. (2022). The request and vehicle input features serve as
inputs to create the respective embeddings. Each embedding consists of a feedforward dense layer
with 32 units and a rectified linear unit (ReLU) activation. The attention mechanism computes
a context from each embedding. The global context is the concatenation of the request context
and the vehicle context. As the number of requests are time-dependent, we use embeddings and
contexts to represent variable-size inputs in a fixed-size global representation. For each path of the
parallel network (see Figure 3), we combine the request and vehicle embeddings, the global context,
and the specific request-vehicle input features. The parallel networks act as trainable multi-layer
embeddings. We experience better performance when we shuffle the parallel inputs for all customer
requests in order to train all subsequent nodes after the flattening equally. We evaluate these inputs
for five parallel dense layers with unit sizes of 512, 256, 128, 64, 32 and after flattening for six layers
with unit sizes of 1024, 512, 256, 128, 64, 32. We apply L2 regularization with a coefficient of 10−4
to all layers. Generally, we use ReLU activation for the feedforward layers and evaluate the final
output on a softmax activation with |R +1| units.
max
Thecriticnetworkarchitectureissimilartotheactornetwork’sarchitecture. However,weaddthe
informationregardingwhichrequestsweregloballyacceptedtotheinputfeaturesfortheembeddings
and the context calculation. To this end, we add a binary flag 0,1 that denotes rejection and
acceptance for requests. For the vehicle input features, we integrate the information of the origin
and the destination of newly assigned requests. The critic output does not have an activation
function.
C.2 Hyperparameters
We mostly use the same hyperparameters as Enders et al. (2023) for our algorithm. We train for
200,000 steps (300,000 for rebalancing on 38 large zones), update the network parameters every 20
steps, and test the performance of the current policy on the validation data every 2,880 steps (48
episodes). During the first 20,000 steps, we collect experience with a random policy and do not
update the network parameters. For the next 30,000 steps, we add linearly declining noise to the
actor’s weights. For the critic loss, we use the Huber loss with a delta value of 10 instead of the
squared error. We use gradient clipping with a clipping ratio of 10 for actor and critic gradients.
We use the Adam optimizer with a learning rate of 3×10−4. We sample batches of size 128 from19
a replay buffer with maximum size of 100,000 for the first experiment and 50,000 for the second
experiment. We set the discount factor to 0.925. For the update of the target critic parameters we
use an exponential moving average with smoothing factor 5×10−4. We tune the entropy coefficient
individually per instance in the range of 0.2 to 0.6.
We conduct multiple training runs using five different random seeds and select the model with
the highest validation performance across these runs. The selected model undergoes testing on the
dataset, and the results presented in this paper reflect its performance on the test data.
C.3 Masking step in Figure 3
We use deterministic post-processing to mask the edge weights used in matching to adhere to our
constrainsandtodeterminetheper-agentrejectaction. Themaskingstepenablestheagenttoreject
single requests by comparing the weights against a constant threshold. The selected threshold is
the highest value at which the agent is able to accept all requests and also rank them against one
another. Thus, a per-request weight must be greater than 1/(R +1). By ranking we refer to
max
the agent’s ability to signal its wish to take multiple requests, but valuing them differently. During
training, the agent differentiates between an active and a passive reject action. The active reject
action occurs if the agent does not want any request. In this case, we will consider the Q-value and
probability of the reject action when calculating the loss. For the passive reject action, the system
does not use the Q-value and probability when calculating the loss, as the matching decided that
the agent does not receive a request instead of the agent choosing to reject all its requests. Thus,
the passive reject action is not the same as the active reject action and therefore the Q-value for
actively rejecting cannot be used.
In Algorithm 1 we describe how we mask the weights obtained from the actor (cf. Figure 2). We
startwiththeplainweightsfromtheactorasinputdataandchangethemtomatchtheenvironment
constraints and to retrieve the reject action. We first check whether the second place of the request
buffer 2rj is occupied (Line 1). If this is the case, we set all weights of the vehicle wri to 0 and a
t t
passiverejectactiona∅ = 0(Line2). Ifnot, wecheckforeachweightofthevehiclewri ifisithigher
t t
than the previously mentioned threshold which we denote as δ (Line 5). If this is the case, we keep
the weight as is (Line 6) if this is not the case, we set it to 0 (Line 8). Finally, we check for an
active reject action by adding up all weights of one vehicle and check whether the sum corresponds
to 0 (Line 9). If this is the case, we determine an active reject action a∅ = 1 (Line 10) and if not a
t
passive reject action a0 = 0 (Line 12).
t
C.4 Rebalancing Requests
To seamlessly integrate rebalancing with dispatching, we model rebalancing actions as requests. We
constrain rebalancing requests and differentiate them from customer requests as follows:
(a) Vehicles can only accept rebalancing requests when they idle, i.e., when their request buffer
is empty (1rj = ∅∧2rj = ∅). Therefore, only one rebalancing action can be undertaken at a
t t
time.20
Algorithm 1 Masking per vehicle agent
Input Data: weights wri ; threshold δ = 1
t Rmax+1
Output Data: weights wri ; reject action a0
t t
1 if 2rj ̸= ∅ then
t
2
wri = 0 ; a0 = 0
t t
3 else
4 for i in R t do
5 if wri > δ then
t
6
wri
=
wri
t t
7 else
8
wri = 0
t
9 if (cid:80) wri = 0 then
i t
10
a0 = 1
t
11 else
12
a0 = 0
t
(b) The origin of rebalancing requests corresponds to their destination. Thus, a vehicle’s approach
to that destination is considered as rebalancing.
(c) Rebalancingrequestsareonlypossibletozonesnotcorrespondingtothevehicle’scurrentzone,
i.e p(v) ̸= d(r).
(d) Rebalancing requests must be accepted immediately and cannot be deferred, i.e., ω(r) = ∅.
(e) Rebalancing requests generate operational costs corresponding to the distance traveled by the
vehicle that fulfills it.
(f) We incorporate a flag into the rebalancing request features to distinguish them from customer
requests.
Asrebalancingrequestsincuronlycosts, everyrebalancingactionreturnsanegativerewardwhen
executed. The algorithm learns to identify the positive impact of rebalancing in future time steps
via the Bellman recursion. However, we can speed-up learning by incorporating additional reward
signals during training. We do so by calculating an artificial fare for rebalancing, only used during
training, that is added to the operational costs of rebalancing in order to incentivize good and to
penalize bad rebalancing behavior.
Algorithm 2 describes the calculation of the artificial fare for rebalancing. The variable xd(ri) in
t
Line1measuresthenumberofvehiclesinthedestinationzoneoftherebalancingrequestnormalized
to a value between 0 (full zone) and 1 (empty zone). The variable xo(ri) calculates a similar proxy
t
measure for the origin zone of the request. We distinguish between empty and full origin zones
(Line 2), as empty zones result in a negative artificial fare (Line 3) and full zones in a positive
artificial fare (Line 5). We give the positive artificial fare in Line 5 proportional to the emptiness
of the destination as we multiply with xd(ri) and we increase the negative artificial fare faster
t
(indicated by factoring it with -2). Finally, we sum up xd(ri) and xo(ri), wherein we attribute a
t t21
lower weight to xo(ri) (Line 7) to mitigate unnecessary rebalancing. Thus, we have an artificial fare
t
that accounts for the occupancy of the destination zone as well as origin zone, to give the complex
state representation additional reward signals, which rebalancing would otherwise not receive.
Algorithm 2 Calculation of fare for rebalancing during training
Input Data: number of vehicles at origin of the request o (ri) ; number of vehicles at destination
t
of the request d (ri), cost parameter c, minimum distance between two vertices l,
t
average number of vehicles per zone rounded up vu, average number of vehicles per
t
zone rounded down vf
t
Output Data: training fare fj
t
1 xd(ri) = 1−min{1,
dt(ri)}
t vf
t
2 if o t(ri) < v u then
3 xo t(ri) = (v tu−ot(ri v) u+1)×(−2)
t
4 else
5 xo t(ri) = ot(r vi) u−v tu ×xd t(ri)
t
6 x t(ri) = xo t( 2ri) +2×xd t(ri)
7 f tj = c×l×x t(ri)
Appendix D Experiments
In the following, we present additional details about our experiments. First, we provide information
regarding the underlying datasets and the system configuration. Second, we elaborate on the three
benchmark algorithms. Third, we offer insights into the rebalancing behavior exhibited by our
policy.
D.1 Datasets
Forourexperiment,weuseyellowtaxitriprecordsinNYCfromtheyear2015(TLC2015),excluding
weekendsandholidays. Weassumethatrequestplacementtimescoincidewiththereportedpick-up
times in the dataset. We extract pick-up and drop-off longitude/latitude coordinates, restricting
our experiment to trips where both coordinates fall on the main island of Manhattan. We use
spatial discretization by employing hexagon grids. We map each request to a pick-up and a drop-off
zone based on the shortest distance of the requests pick-up and drop-off coordinates to the center
of any zone. We exclude trips starting and ending in the the same zone. The distances between
neighboring zones are set at 459 meters for small zones and 917 meters for large zones. Travel time
between two neighboring zones assumes two and five time steps based on realistic driving speeds.
We construct a graph, wherein each zone represents a vertex and edges connect neighboring zones.
Vehicles traveling between non-neighboring zones follow the shortest route.22
We focus on two different subsets of zones within the NYC dataset, which define our simulated
operating areas, i.e., the part of Manhattan in which our fleet operates. Hence, we only consider
requestshavingbothpick-upanddrop-offlocationswithintheoperatingarea. Forthe38largezones,
we downscale trip data by a factor of 20, using every 20th request for simulation. This adjustment
accommodates hardware limitations and results in an average of 360 requests per episode for the
11 small zones and 828 requests for the 38 large zones. The mean trip distance is larger for the 38
large zones, influencing the number of vehicles required.
Figure 7a illustrates the smaller operating area of the NYC dataset, wherein colors represent
pick-up and drop-off frequency. Dark green represents zones with a majority of pick-ups, while dark
red represents zones with a majority of drop-offs. Light yellow/green reflects zones, wherein the
number of pick-ups and drop-offs is almost equal. Although some of the 11 small zones exhibit a
minor bias towards either drop-off or pick-up, it is not strongly pronounced. Therefore, we obtain
a second dataset which maintains the temporal patterns of the NYC taxi dataset while altering
the drop-off and pick-up locations to achieve more imbalanced request distributions. We do so by
sampling pick-ups from a normal distribution around a cluster and sampling drop-offs towards its
edges from another normal distribution. This modification allows for a more imbalanced spatial
distribution of requests, depicted by dark green and dark red zones in Figure 7b. This simulates
realistic scenarios like, e.g., the end of a major sports event in the middle of the operating area.
Figure 8 displays the operating area of the 38 large zones with its spatial distribution.
Weassumeamaximumwaitingtimeoffiveminutesforthe11smallzonessettingandtenminutes
for the 38 large zones setting, with the increased waiting time due to the longer trip lengths and
greater size of the operating area. To achieve a 20% operating profit margin with empty driving to
the pick-up location, we set the revenue at 5.00 USD per km, and operational costs at 2.00 USD
per km.
D.2 Benchmarks
Greedy: Greedy chooses the highest reward at the current time step. Thus, it assigns every request
a weight that depends on the profitability of the request. The weight is zero for unprofitable
requests, i.e., if the cost is higher than the fare or if the time to pick-up the customer is higher
than the maximum waiting time. The algorithm weighs all other requests proportionally to the
profitability for each vehicle. To this end, Greedy subtracts the cost of the total distance, i.e., the
distance to the pick-up plus the distance driven with the customer, from the fare of the request.
Heuristic: The Rebalancing Heuristic weighs customer requests the same way as the greedy
policy, but has additional rebalancing capabilities. Heuristic weighs rebalancing actions lower than
customer requests and aims for an equal distribution of vehicles per zone. Heuristic attributes a
non-zero weight to rebalancing requests only if the vehicle’s current zone has more vehicles than the
rounded up average number of vehicles per zone vu and vehicle’s destination zone has less vehicles
t
than the rounded down vu average number of vehicles per zone. The weight is proportional to the
t
vehicle’s distance to the destination zone.23
Figure 7: The 11 zone operating area. Zones dominated by pick-ups are marked in green, zones
dominated by drop-offs are marked in red, and zones with a balance between pick-ups and
drop-offs are marked in light green/yellow.
(a) NYC - 11 zones (b) Clustered - 11 zones
Figure 8: The 38 zone operating area. Zones dominated by pick-ups are marked in green, zones
dominated by drop-offs are marked in red, and zones with a balance between pick-ups and
drop-offs are marked in light green/yellow.
(a) NYC - 38 zones (b) Clustered - 38 zones
RVC: Each agent represents a request-vehicle pair. All agents return two probabilities, one
for declining the request and one for accepting it. Both probabilities add up to one. The accept
probabilities serve as weights in the bipartite matching problem if they are higher than the reject
probabilities during validation and testing. If the accept probabilities are lower than the reject
probabilities, they are not considered in the bipartite matching problem. During training, accept24
and reject decisions are sampled according to their respective probabilities and the weights in the
bipartite matching problem then correspond to the probability of the sampled action.
D.3 Rebalancing Behavior
Our experiments show for the 11 small zones setting that V always rebalances vehicles to
extended
a neighboring zone in one time step. This behavior is beneficial as the agent can reevaluate its
rebalancing actions in each time step and check for new customer requests. The larger the setting,
the more rebalancing actions the agent has to evaluate. This increases noise and computational
effort. As the agent exclusively rebalances to neighboring zones in the context of the 11 small
zones setting, we intentionally extend this practice to encompass only neighboring zones as viable
rebalancing options for the larger 38 zones.