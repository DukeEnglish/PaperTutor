Overparameterized Multiple Linear Regression as Hyper-Curve Fitting ∗
Elisa Atza and Neil Budko†
Abstract. The paper shows that the application of the fixed-effect multiple linear regression model to an
overparameterized dataset is equivalent to fitting the data with a hyper-curve parameterized by
a single scalar parameter. This equivalence allows for a predictor-focused approach, where each
predictor is described by a function of the chosen parameter. It is proven that a linear model will
produce exact predictions even in the presence of nonlinear dependencies that violate the model
assumptions. Parameterization in terms of the dependent variable and the monomial basis in the
predictorfunctionspaceareappliedheretobothsyntheticandexperimentaldata. Thehyper-curve
approachisespeciallysuitedfortheregularizationofproblemswithnoiseinpredictorvariablesand
can be used to remove noisy and ‘improper’ predictors from the model.
Key words. overparametrization, linear regression, model selection, variable selection, regularization.
MSC codes. 15A06, 62R07, 68T01, 65F22, 62J20.
1. Introduction. ManymodelsencounteredinpracticalapplicationsofMachineLearning
(ML) are overparameterized. Some are superficially so, e.g., a linear random-effect model [1]
with many predictors that come from the same probability distribution, which by itself is
described by a few unknown parameters only. Others, like a linear fixed-effect model, also
known as the Multiple Linear Regression (MLR) model, with fewer training samples than
unknowns, may happen to be truly overparameterized. While there is no universally accepted
definition of an overparameterized ML model, research in this area has recently uncovered
several interesting phenomena, such as the double-dipping of the prediction error [2], and the
so-called benign overfitting [3], [4].
The problems where the number of predictors is extremely large are common in many
applications. Recent technological developments in chemistry, biology, medicine and agricul-
ture have allowed for high-throughput data acquisition pipelines resulting in relatively large
amounts of predictor variables compared to the feasible number of experiments in which a
target phenotype or a trait is measured. For instance, in chemometric research, where one
seeks to predict a physical or biological property from the infrared spectrum of the substance,
the number of spectral components is in the order of thousands [5] [6]. In metabolomics, the
biological traits are predicted from the relative abundance of small-molecule chemicals in a
biological sample, the number of metabolites can reach tens of thousands [7], [8]. Similar
numbers of predictors are used in the microbiome-based predictions [9]. Finally, in genomics,
the number of Single-Nucleotide Polymorphisms (SNP’s), that may potentially predict the
phenotype of some living organism, can be as high as tens of millions [10], [11]. At the same
time, thedurationandcostsofexperimentsaimedatmeasuringthedependentvariables(phe-
∗Submitted to the editors DATE.
Funding: This work was funded by HZPC Research B.V., Averis Seeds B.V., BO-Akkerbouw, and European
Agricultural Fund for Rural Development
†Numerical Analysis, DIAM, EEMCS, Delft University of Technology, Mekelweg 4, 2628 CD Delft, Netherlands
(e.atza@tudelft.nl, n.v.budko@tudelft.nl).
1
4202
rpA
11
]LM.tats[
1v94870.4042:viXra2 E. ATZA AND N. BUDKO
notype, traits, etc.) are often much higher. Therefore, the number of such experiments, i.e.,
the number of training samples, is typically a fraction of the number of predictors (features)
– hundreds or a few thousands at most [12], [13], [14], thus making the overparameterized
nature of such problems inescapable.
Due to the enormous success of the Artificial Neural Networks (ANN’s) in image classifi-
cation, the current focus in the omics-related ML is on the application of these sophisticated
nonlinear models to the existing and new data. However, truly deep ANN’s are also overpa-
rameterized, which leads to learning problems when the number of training samples is low
and require a large training/validation dataset to tune hyperparameters and avoid overfitting.
Sometimes this problem is circumvented by creating an artificial augmented training dataset
[15]. Therefore, it is not surprising that whenever the training is performed on small datasets,
the quality of predictions obtained with an ANN is only marginally better, if at all, when
compared to the predictions with the simple overparameterized MLR model [16], [17].
One could argue that the good results by the overparameterized MLR are also due to
overfitting or a poor testing procedure (e.g. data leakage). While this certainly may be
the case in some practical studies, generally, the fixed-effect MLR model with fewer training
samples than predictors is well-understood on a theoretical level [18],[19]. The solution to the
resulting underdetermined linear system, if it exists, is not unique. As the next best thing,
one aims at recovering the minimum-norm least-squares (LS) solution, which always exists
and is unique. The minimum-norm LS solution may, however, be sensitive to noise in the
dependent variable. If the level of noise in the dependent variable is known, then there is
a regularized solution, which minimizes the error with respect to the noise-free minimum-
norm LS solution. If, as it often happens in practice, the level of noise in the dependent
variable is not known, then the level of regularization can either be estimated from the data
with the cross-validation method or a similar technique, or has to be chosen subjectively.
The case of the noise in predictor variables is much less understood, but there exist various
errors-in-variables models [20].
The main focus of the present paper is the adequacy, performance and optimization of
overparameterized linear models. Specifically, the authors aimed at understanding the nature
of overparameterized datasets, identifying the circumstances where the MLR model makes
good or bad predictions of such datasets, improving the interpretability of regression results
beyondthetraditionalfeatureweights, andincreasingthepredictionaccuracy, simultaneously
making the model more adequate, i.e., satisfying the linear assumptions. This has lead us to
the column-centered reformulation of the MLR, where the (inverse) relation between each
predictor variable and the dependent variable can to a large extent be analyzed independently
ofotherpredictorvariables. WeshowthatthepredictionsmadebysuchanInverseRegression
(IR) model are identical to the predictions of the MLR model on a class of overparameterized
datasets. Topologically this means that on such datasets the MLR model is not a hyper-plane
as suggested by its mathematical form, but a hyper-curve, parameterizable by a single scalar
parameter, which, for the sake of interpretability can be chosen as the dependent variable.
Due to its column-centered nature, the hyper-curve approach allows to filter out the
predictors (features) that are either too noisy or do not satisfy the topological requirements of
a linear model, which significantly improves the predictive power of the trained linear model,
removes the features that may otherwise introduce the illusion of understanding [21], andOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 3
suggests the subsets of predictors where a non-linear or a higher-dimensional-manifold model
would be more adequate.
The paper is structured as follows. In section 2 we define Fundamentally Overparame-
terized Datasets, MLR, PARametric hyper-CURve (PARCUR) and IR models, prove their
equivalence and identify the condition for the exact prediction of a test dataset. In section 3
we study the behavior of the polynomial IR model applied to a dataset which contains both
the polynomial predictors as well as predictors that have a non-functional relation to the de-
pendent variable. We establish conditions under which such a dataset is a FOP dataset. In
section 4 we consider noisy data and introduce a polynomial degree truncation regularization
scheme that can handle the noise in both the dependent and predictor variables. In section 5
we propose a novel predictor removal algorithm that does not suffer from the ambiguities
common to heuristic feature selection methods. In section 6 we apply the regularized IR
model with predictor removal to the widely available experimental chemometric Yarn dataset
[22], [23], and demonstrate the presence of both curve-like and higher-dimensional manifolds
in this dataset. Finally, we present our conclusions and discuss the possible extensions of the
PARCUR and IR models.
2. Parametric Hyper-Curve and Inverse Regression models. In this and the following
section we focus on the mathematical properties of the model and all data is assumed to
be exact. Data with additive noise will be considered in section 4. The standard multiple
regression model expresses the dependent variable y as a function of p predictor variables x ,
j
j = 1,...,p, i.e.,
(2.1) y = f(x ,...,x ), f : Rp R.
1 p
→
In particular, the Multiple Linear Regression (MLR) model,
p
(cid:88)
(2.2) y = β x ,
j j
j=1
obviously, belongs to this class of models. Topologically, the MLR equation (2.2) describes a
p-dimensional linear object, a hyper-plane, in a (p+1)-dimensional space.
Now, consider the model of the form:
y = x (s),
0
(2.3) x = x (s), j = 1,...,p;
j j
s [a,b] R,
∈ ⊂
which simply states that all data are considered to be the functions of some scalar parameter
s. The equations (2.3), describe a parametric hyper-curve in a (p + 1)-dimensional space,
essentially, a one-dimensional object. We shall call this model the PARametric hyper-CURve
(PARCUR) model.
Under a monotone transformation of variables, the PARCUR model is equivalent to the
Inverse Regression (IR) model:
(2.4) x = x (y), j = 1,...,p.
j j4 E. ATZA AND N. BUDKO
In the inverse relation (2.4) the independent variables x , j = 1,...,p, are considered to be
j
the functions of the dependent variable y. This model naturally emerges in the context of
calibration problems [24]. It is also the most easily interpretable version of the PARCUR
model as the functions x (y) provide an insight into the change of each individual predictor
j
variable as a function of the dependent variable y, if such a functional dependence exists.
Obviously, the general model (2.1) and the IR model (2.4) are completely equivalent only
under very stringent constraints on the function f. Specifically, for a complete equivalence
the function f(x ,...,x ) as well as all individual functions x (y) should be invertible. While
1 p j
an inverse of the MLR model (2.2) in the form (2.4) may exist on certain subsets of Rp, a
general nonlinear function f in (2.1) is not invertible and neither is a general IR or PARCUR
model. Yet, the predictive power of both the MLR and the PARCUR models trained on a
finite training dataset of a certain general type appears to be the same even when they are
not mutually invertible.
Our main results, expressed in Theorem 2.3 and Theorem 2.4, are the conditions on the
exact prediction by the MLR model and the equivalence of the predictions made by the MLR
and PARCUR models for what we call a Fundamentally OverParameterized (FOP) dataset.
This equivalence allows to analyze the different types of column functions x (s), j = 1,...,p,
j
and establish the conditions on the existence of a FOP dataset for different types of predictor
data.
In practice, any regression model is trained on a finite discrete dataset. An overparame-
terizeddatasetariseswheneverthenumbernoftrainingsamplesissmallerthanthenumberp
of parameters or predictors. This can happen simply due to the lack of experimental data and
additional data can transform an overparameterized dataset into a well-defined or even an un-
derparameterizedone. However, inthepresentpaper, weshallfocusonthemorefundamental
case, where the simple addition of the training data does not change the overparameterized
nature of the dataset.
Definition 2.1. The dataset
= (y ,x ,...,x ), y ,x R i = 1,...;j = 1,...,p
i i,1 i,p i i,j
S { ∈ | }
is a fundamentally overparameterized (in the linear sense) dataset of rank q, if, for any m,
the data-matrix S Rm×(p+1) with its rows from , i.e.,
m
∈ S
   
y x ... x
1 1,1 1,p
S m = [y,X], y =   . . .  , X =   . . . ... . . .  ,
y x ... x
m m,1 m,p
has the property:
(2.5) rank(S ) q p.
m
≤ ≤
A training dataset is any fixed-size data matrix S Rn×(p+1) with the rows from the FOP
n
∈
dataset . Atrainingdatasetiscompleteifrank(S ) = q. Thecomplementdataset = ,
n t n
S S S\S
is called the test dataset.OVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 5
Inpracticewearedealingwitharbitrarybutfixed-sizetestingdatasetsaswell. Tosumma-
rize, thedependent-variabledatafromS and willbestoredinthedata-vectorsy Rn and
n t
S ∈
y Rm, and the corresponding predictor-variable data will be stored in the data-matrices
t
∈
X Rn×p and X Rm×p, respectively.
t
∈ ∈
An overparameterized MLR model (2.2) corresponds to an underdetermined linear alge-
braic system Xβ = y, which is usually solved in the minimum-norm least-squares sense as
βˆ = XT(XXT)−1y, where we have assumed that rank(X) = n. Then, such a ‘trained’ MLR
model will make the following predictions for the testing dataset:
yˆ = X XT(XXT)−1y,
t t
(2.6)
Xˆ = X XT(XXT)−1X.
t t
Bothy andX areconsideredtobegiveninthetrainingdatasetS ,whereasonlythepredictor
n
data-matrix X is considered to be given in the testing dataset . Hence, strictly speaking,
t t
S
the prediction Xˆ of the given matrix X is not necessary. However, the fact that it represents
t t
a projection of the rows of X on the row space of the training matrix X will be used in our
t
subsequent analysis. Moreover, the two predictions (2.6) can now be written as the prediction
Sˆ of the testing dataset matrix S :
t t
(2.7) Sˆ = X XT(XXT)−1S .
t t n
The following Definition 2.2 establishes the connection between the continuous world of
predictor variables as described by the predictor functions and the discrete world of datasets,
i.e., the columns of the data matrices.
Definition 2.2. A linear vector space , with dim( ) = n, is called a column function
n n
V V
space of the dataset of rank n if there exist p+1 column functions x (s) , x : R R,
j n j
S ∈ V →
j = 0,...p, such that:
y = x (s ),
i 0 i
(2.8)
x = x (s ), j = 1,...,p,
i,j j i
for any data-point (y ,x ,...,x ) from .
i i,1 i,p
S
Notice, we use the same notation for the predictor data x and the column functions
i,j
x (s), with the relation being x = x (s ). Also, obviously, s = y in the IR model (2.4).
j i,j j i
Let v : R R k = 1,...,n be a basis of the column function space of the dataset
k n
{ → | } V
of rank n. Then, each column function x (s) can be expanded as:
j
S
n
(cid:88)
(2.9) x (s) = a v (s).
j k,j k
k=1
The entries of the basis matrix V Rn×n are the sampled basis functions v (s ), k = 1,...,n,
k i
∈
i = 1,...,n:
 
v (s ) ... v (s )
1 1 n 1
(2.10) V =   . . . ... . . .  .
v (s ) ... v (s )
1 n n n6 E. ATZA AND N. BUDKO
If V is invertible, then the data-vector y and the data-matrix X of the training set S can be
n
decomposed as follows:
(2.11) X = VA, y = Va ,
0
where the elements [A] = a of the matrix A Rn×p and the elements [a ] = a of the
k,j k,j 0 k k,0
∈
vector a Rn are the expansion coefficients from (2.9).
0
∈
Formally, the PARCUR model (2.4) can be trained by computing the matrix A and the
vector a as:
0
(2.12) A = V−1X, a = V−1y.
0
To predict the test dataset , one has to solve the following minimization problem:
t
S
(2.13) Vˆ = arg min X V A 2,
t t t 2
Vt∈Rm×n∥ − ∥
where X is the given predictor test data-matrix. If rank(A) = rank(S ) = n, the solution of
t n
the problem (2.13) is given by:
(2.14) Vˆ = X AT(AAT)−1.
t t
Then, applying the relations (2.11), the predictions of the PARCUR model can be written as
follows:
yˆ = Vˆ a = X AT(AAT)−1a ,
t t 0 t 0
(2.15)
Xˆ = Vˆ A = X AT(AAT)−1A.
t t t
This can also be combined into the prediction Sˆ of the testing dataset matrix S :
t t
(2.16) Sˆ = X AT(AAT)−1A ,
t t n
where A = [a ,A] and, from (2.11), S = VA .
n 0 n n
The following Theorem 2.3 establishes the condition under which the prediction made by
the MLR is going to be exact.
Theorem 2.3. The prediction Sˆ produced by the MLR model is exact for any data matrix
t
S from the fundamentally overparameterized dataset of rank q if and only if the training
t
S
dataset S is complete and rank(X) = q.
q
Proof. LetS = [y,X]beacompletetrainingsetoftheFOPdataset . ByDefinition2.1,
q
S
for any test set with the data matrix S = [y ,X ] there exists the matrix U such that
t t t t
S ⊂ S
S = US and X = UX. Hence, from (2.7),
t q t
(2.17) Sˆ = X XT(XXT)−1S = UXXT(XXT)−1S = US = S .
t t q q q t
If the training set S is not complete, then there exists a data row s that is not a linear
q t
∈ S
combination of the rows of S .
qOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 7
Whether the training dataset is compete or not, the MLR and the PARCUR models are
equivalent in the following sense.
Theorem 2.4. Let S = [y,X], rank(X) = n, and = [y ,X ] be a training and a testing
n t t t
S
datasets of the fundamentally overparameterized dataset of rank q, n q. Let also be the
q
S ≤ V
column function space of . Then, for any basis v in with the invertible basis matrix
j q
S { } V
V Rn×n, the predictions of the testing dataset (2.6) and (2.15) made, respectively, by the
∈
MLR and the PARCUR models are equal.
Proof. The proof follows from substituting the relations (2.12) into (2.15).
If the training dataset is incomplete and the testing data-matrix S is not (yet) in the
t
row-range of the training data-matrix S , there will be an error in the predictions made by
n
the MLR and PARCUR models. Let S be decomposed as
t
(2.18) S = US +S⊥, S⊥ST = y⊥yT +X⊥XT = 0 ,
t n t t n t t k,n
where S⊥ = [y⊥,X⊥], and 0 Rk×n is the matrix of all zeros. The existence of this
t t t k,n
∈
decomposition stems from the fact that rank(S ) = n, i.e., S ST is invertible.
n n n
Then, X = UX +X⊥, and the prediction error will be:
t t
S Sˆ = US +S⊥ (UX +X⊥)XT(XXT)−1S
t t n t t n
− −
(2.19) = S t⊥+X t⊥XT(XXT)−1S n = S t⊥ y t⊥yT(XXT)−1S n
−
(cid:104) (cid:105)
= y⊥ y⊥yT(XXT)−1y, X⊥ y⊥yT(XXT)−1X .
t t t t
− −
Hence, apart from the obvious case y⊥ = 0, the prediction of the dependent variable will also
t
be exact if yT(XXT)−1y = 1.
Technically, the difference between the MLR and PARCUR models can be expressed as a
simple transformation of the columns of the training dataset S by the basis matrix V. The
n
relative advantage of the IR version, s = y, of the PARCUR model over other choices of the
parameter s stems from the direct interpretation of the coefficient matrix A since it provides
an insight into the dependence of each predictor variable on the dependent variable. The
magnitudeandthesignofthecoefficientsinAreflectthetype(e.g.,linear,quadratic,etc)and
the strength of these dependencies. It is also clear why interpreting the β vector of the MLR
modelmightbemoreproblematic,asitsrelationtothecoefficientmatrix,βˆ = AT(AAT)−1a ,
0
is rather convoluted. In the standard MLR formulation (2.2), a component βˆ of the vector
j
βˆ is interpreted as the weight of the additive contribution by the corresponding predictor x .
j
However, it does not further specify the nature of the mathematical relation between y and
x .
j
It is also possible, and sometimes profitable, to train the PARCUR model on an overcom-
plete overparameterized training data set S Rn×(p+1), where rank(S ) = q p, but n > q.
n n
∈ ≤
However, since the basis matrix V Rn×q, rank(V) = q, is now singular, one has to resort to
∈
the Ordinary Least-Squares (OLS) solution of the training problem:
(2.20) Aˆ = (VTV)−1VTS .
n n8 E. ATZA AND N. BUDKO
In that case, Aˆ = (VTV)−1VTX, and the prediction of the test set makes use of these OLS
estimates as Sˆ = X AˆT(AˆAˆT)−1Aˆ . Although, the equivalence to the MLR model is only
t t n
achieved if the chosen basis matrix V coincides with the matrix of the left singular vectors
of X.
3. Datacontainingpolynomialcolumnfunctions. Theorem2.3showsthattheprediction
by the MLR model is exact if the training set is complete. From the Definition 2.1 it is clear
thatacompletedatasetisasubsetofaFOPdataset,suchthatitsdatamatrixhasthemaximal
possible rank q, which can be smaller than the number of predictors p. Given a finite training
dataset of size n it is hard to tell if: a) it is a subset of a FOP dataset with some q < p,
b) it is a complete dataset, i.e. n = q. In this section, working under the assumption that
the data set does not contain statistical noise, we establish the sufficient condition for the
m
S
existence of a FOP dataset. To formulate these conditions it is convenient to make a choice
of the column function space, see Definition 2.2. Here, we consider the polynomial function
space and the monomial basis, which lead to easily interpretable regression results.
We note a well-known fact that the monomial basis v (y) = yk k = 0,...,n 1 for the
k
{ | − }
column function space will produce an invertible Vandermonde basis matrix V, given by
n
V
 1 y y2 ... yn−1
1 1 1
(3.1) V =


1
.
.
y
.
.2 y 22 ..
.
.. y 2n−1 
 ,
. . . 
···
1 y y2 ... yn−1
n n n
if all entries of the dependent variable data-vector y = [y ,...,y ]T are distinct. In this basis,
1 n
theentriesa ofthecoefficientmatrixArepresentthecoefficientsofthepolynomialfunctions
i,j
x (y) that may generate some of the columns of the predictor data-matrix X:
j
(3.2) x (y) = a +a y+ +a yn−1.
j 1,j 2,j n,j
···
This also puts the PARCUR model into the context of polynomial fitting. Whether the
columns of X have or have not been generated by polynomial functions of y, the IR model
with the basis matrix (3.1) will be projecting all columns of X on the monomial basis. From
the computational point of view, Vandermonde matrices, while theoretically invertible, are
hard to work with for sizes above n = 15 and become the source of significant round-off
errors. Luckily, one normally does not need polynomial functions of very high degree to
adequately describe a data column. To minimize the numerical errors, we also normalize the
range of the y-data to fit within the interval [ 1,1].
−
In general, it is difficult to decide whether the training dataset is complete by simply
inspecting the entries of its data matrix S . In theory, one could compute the Singular-
n
Value Decomposition (SVD) of the matrix and see if the zero singular value appears after
adding any new sample (row) to the training dataset. However, here we are interested in
an arbitrary column function space with the square invertible basis matrix V Rn×n,
q
V ∈
and the conclusions about the eventual completeness of S will be based on the shape of the
n
corresponding coefficient matrix A = V−1S .
n n
Each of the p+1 data columns x in a dataset belongs to one of the three classes:
j
SOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 9
y y
x2 x2
x x
1 1
1 5
0 0
−1 −5
1 10
0 0
1 10
− 1.0 0.5 0.0 0.5 1.0 − 1.0 0.5 0.0 0.5 1.0
− − y − − y
Figure 1: Examples of column data produced by non-functional relationships between the
dependent variable y and the predictor variables x and x . Left: data on a curve that cannot
1 2
be parameterized by y. Right: data on a conical surface. Two-dimensional scatter-plots: x
1
and x column data sorted by y and displayed as ‘functions’ of y.
2
1. x (y) is a polynomial in y of degree less or equal to n 1
j
−
2. x (y) is a polynomial in y of degree higher than n 1
j
−
3. there is a non-functional dependence between x and y
j
In particular, the first column x = y, i.e., the dependent variable, obviously, belongs to the
0
first class with n = 2.
Anon-functionaldependencebetweenthepredictorx andthedependentvariabley would
j
emerge if the data was situated on a curve that cannot be parameterized by y, Figure 1 (left).
Choosing a different parameterization could transform these ’nonfunctional’ data to functions
y(s), x (s), j = 1,...,p, as in the general PARCUR formulation. A more severe case of non-
j
functional dependence arises where the data is situated on a higher-dimensional manifold,
such as a hyper-surface, Figure 1 (right), and no alternative parameterization can fix this
problem. The column data that one observes in such ‘nonfunctional’ cases are illustrated in
Figure 1 (bottom, two-dimensional scatter plots).
Theorem 3.1. Let be a dataset with one independent variable y and p predictor variables
S
x , j = 1,...,p. Let also contain k p predictors that are polynomials in y of degrees
j
S ≤
greater than q 1, or have a non-functional relation to y. Then, is a fundamentally over-
− S
parameterized dataset (in linear sense) of rank q, 2 q p, if its remaining p k predictors
≤ ≤ −
are the polynomials in y of degree r, 1 r q k 1.
≤ ≤ − −
Proof. Without the loss of generality we may assume that for any subset of size m of the
dataset , the dependent variable data y contains only the distinct values of y so that the
S
1x
2x10 E. ATZA AND N. BUDKO
square monomial basis matrix V Rm×m is invertible. Then, any data-matrix S = [y,X]
m
∈ ∈
Rm×(p+1) canberepresentedasS = VA . BytheconditionsoftheTheorem,foranym > q,
m m
subject to column reordering, the coefficient matrix A Rm×(p+1) has the structure:
m
∈
(3.3)
(cid:20) (cid:21)
e A A
A = 2 1,1 1,2 , A R(q−k)×(p−k), A R(q−k)×k, A R(m−q+k)×k,
m 0 O A 2,2 1,1 ∈ 1,2 ∈ 2,2 ∈
where V−1y = e Rm is the second standard basis vector and O R(m−q+k)×(p−k) is the
2
∈ ∈
matrix of all zeros. Here we have used the fact that the polynomial fit to non-functional data
may produce a polynomial of degree greater than q 1. It is obvious that rank(A ) (q k)
1,1
− ≤ −
and, for any m (q k), rank(A ) k. Therefore, rank(A) q, and rank(S ) q for any
2,2 m
≥ − ≤ ≤ ≤
m, showing that is a FOP dataset.
S
The above Theorem 3.1 shows that having extremely high-degree polynomials, e.g., with
degrees higher than p 1 and non-functional dependencies among the predictors does not
−
prevent the MLR and the IR models from making exact predictions as long as there are
also polynomial data of sufficiently low degree and the set on which the model is trained is
complete. Moreover, a complete dataset can, in principle, be achieved with n < p samples.
The latter fact may seem surprising as it appears that we are able to recover a polynomial of
degree higher than n 1 or a non-functional dependence by training on just n data points.
−
However, it becomes less surprising if we consider the form of the MLR and IR predictors
given by (2.6) and (2.15), as in both cases the leftmost matrix X contains the X-data from
t
the test dataset which ones is trying to ‘predict’.
Inthelimitingcasewithphigh-degreepolynomialsand/ornon-functionaldependencies, a
complete dataset will only be achieved with n = p samples. It seems to be a waste of time and
resources, though, to collect so much training data knowing that the majority of predictors
do not even satisfy the model assumptions. We come back to this question in section 5. In
the other limiting case, where all predictors are polynomials, the size of the complete training
dataset can be as small as n = 2 if the maximal degree of all polynomials is at most one and
there is at least one predictor which is a linear function of y.
Figure2(top)illustratestheperformanceoftheIRmodelwiththethreeclassesofcolumn
data discussed above. Specifically, we are considering the cases where: all predictors are
polynomial functions in y of degree r 1 q (blue, circles), some of the predictors are high-
− ≤
degree polynomials in y (orange, squares), and some of the predictors have non-functional
relation to the dependent variable y (green, triangles).
In these numerical experiments the data x (y) are generated by randomly sampling the
j
range of y [ 1,1] and evaluating polynomial functions of various degrees at the sampled
∈ −
points. Columns that are not functions of y are generated as non-invertible functions y(x ),
j
similar to those in the examples of Figure 1. The predictions are computed as in Eq.’s (2.15),
where the coefficient matrix A is obtained from the training X-data as in Eq. (2.12).
The ranks of the complete datasets are: q = 11 (p = 200 polynomials of degree r 10),
≤
q = 14 (p = 203, with 200 polynomials of degree r 10 and three polynomials of degree
≤
r = 17), and q = 13 (p = 202, with 200 polynomials of degree r 10 and two non-functional
≤
predictors). In all three cases, we expect the prediction errors to vanish as soon as the rankOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 11
figure1
10−1 10−1
10−3 10−3
10−5 10−5
10−7 10−7
10−9 10−9
10−11 10−11
10−13 10−13
10−15 10−15
2 4 6 8 10 12 14 2 4 6 8 10 12 14
exactLSandCV
n n
ρt ρtr r ≥q, q=14 r ≤q&n.f.c. r ≤q, q=11
10−1
10−3
10−2
10−5 10−4
10−7
10−6
10−9
10−8
10−11
10−10
10−13
10−15
10−12
2 4 6 8 10 12 14 2 4 6 8 10 12 14
r r
r q, q=14 r q&n.f.c. r q, q=11
≥ ≤ ≤
Figure 2: Top: prediction errors of the IR model as functions of the training dataset rank
obtained with invertible basis matrix V and exact data (see text for full explanation). Bot-
tom: test of the regularization algorithm on exact (noiseless) data and over-complete training
dataset. Bright enlarged colored markers correspond to the minima of the validation errors
that indicate the optimal polynomial degree r∗ along the horizontal axis (bottom, right) and
show the values of the test errors attained with these r∗ (bottom, left). Note, the horizontal
axis displays rank(Aˆ) = r+1, rather than the actual polynomial degree r.
of the training dataset reaches q.
The prediction errors in y and X are measured on both the training and the test sets as
)ty(ρ
)ty(ρ,)vy(ρ
)tX(ρ
)tX(ρ,)vX(ρ12 E. ATZA AND N. BUDKO
follows:
y yˆ X Xˆ
v v 2 v v F
ρ(y ) = ∥ − ∥ , ρ(X ) = ∥ − ∥ ,
v v
y X
v 2 v F
(3.4) ∥ ∥ ∥ ∥
y yˆ X Xˆ
t t 2 t t F
ρ(y ) = ∥ − ∥ , ρ(X ) = ∥ − ∥ ,
t t
y X
t 2 t F
∥ ∥ ∥ ∥
where y and X are the training (validation) set data and y and X are the test set data.
v v t t
Figure 2 shows the prediction errors as functions of the training set rank (top row), and
of the polynomial representation degree (bottom row). The bright colored dashed lines give
the training set errors and the dim solid gray lines show the corresponding errors on the
test dataset. Plots on the left show the dependent variable errors and on the right – the
errors for the predictor variables. As expected, the errors drop as soon as the training dataset
becomes complete. With the high-degree polynomial predictors and especially with non-
functional predictors the errors on the training set begin to rise at the end and the errors on
thetestsetdonotdroptomachineprecisionvaluesashappenswiththelow-degreepolynomial
predictors. This is due to the fact that the coefficient matrices in the former two cases become
ill-conditioned, making the predictors more sensitive to numerical round-off errors.
4. Polynomial regularization. All measured data are either random in nature or contain
statistical noise. In the latter case, instead of the exact values y of the dependent variable
i
y, one usually measures the quantity y +ϵ , where the ‘noise’ ϵ is a single realization of a
i i i
random variable ϵ with the distribution from some well-defined class, e.g., ϵ (0,σ2), see
∼ N
e.g. [19]. The parameters of the distribution, such as the variance σ2, are often unknown and
estimated from the data. Being measured quantities, the independent variables x may also
j
be contaminated by noise. In the noisy setting, the PARCUR model becomes:
y = y(s)+ϵ ,
0
(4.1) x = x (s)+ϵ , j = 1,...,p;
j j j
s [a,b] R.
∈ ⊂
For simplicity we assume here the most common statistical hypotheses ϵ (0,σ2), j =
j ∼ N j
0,...,p, which provides an adequate description of the “standard” errors due to the finite
samplesize. WeinvestigatetheeffectoftheadditiveGaussiannoiseϵ (0,σ2),j = 0,...,p
j ∼ N j
applied in the following three ways:
1. noise only in the dependent variable y: σ = σ = 0, σ = 0, j = 1,...,p
0 j
̸
2. independent and identically distributed (i.i.d.) noise in S : σ = σ = 0, j = 1,...,p
n j
̸
3. independent, but not identically distributed noise in S :
n
(cid:40)
σ = 0, if j = 0 and x noisy predictors
j
(4.2) σ = ̸ ∈ { }
j
0, if j = 0 and x exact predictors
j
∈ { }
Obviously, the first ‘classical’ case belongs to the last class if considered over the complete
data matrix S .
n
To avoid overfitting and mitigate the effect of noise on the model predictions, one can
use any of the standard regularization techniques, such as the Tikhonov regularization or theOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 13
n < 150 n 150
≥
Step 1 Sort y+ϵ (if possible by y)
Step 2 – Apply Wiener filter on sorted y+ϵ
Step 3 Sort all columns by sorted y+ϵ Sort all columns by sorted and filtered y+ϵ
Step 4 Choose r∗ subjectively Use CV to find r∗
Table1: RegularizationstrategiesfortheIRmodelwithtruncatedmonomialbasis, depending
on the number of training samples.
truncated SVD. In the case of the IR model, truncating the number of terms of the monomial
basis used to represent the column functions appears to be the most natural regularization
approach.
In the previous section 2, the exact training data y, could simply be sorted by magnitude
and used as the values of the curve parameter to construct the monomial basis matrix V.
While this is still permitted in the noisy case (we can choose the parameter s as we wish), the
interpretation of the coefficient matrix A is no longer straightforward, if instead of y we are
using y+ϵ.
At the risk of losing some of the interpretability of the coefficient matrix Aˆ, we shall
nevertheless be sorting all our data by the magnitude of y+ϵ. Notice, that the vector y+ϵ
sorted by the (inaccessible) exact data y represents a smooth function (irregularly sampled
y) with an additive ‘white’ noise. Sometimes, sorting by the exact y can be achieved if the
predictors contain a ‘clean’ (noiseless) monotonous function of y or even a noisy monotonous
functionthathasbeensortedincorrectorder. Inthatcase, allcolumnsinS shouldbesorted
n
by the column corresponding to this predictor variable.
If y+ϵ is correctly sorted by y, then a multitude of denoising techniques is available for
such smooth noisy signals, e.g., the Wiener filter. However, numerical experiments indicate
that,inourcase,theWienerfilterbecomeseffectivestartingfromapproximatelyn = 150data
samples and is of little use below that threshold. Similar lower bound seems to hold for the
effectiveness of the cross-validation and similar techniques that allow to deduce the optimal
value of the regularization parameter (maximal degree r∗ of the monomial basis functions)
from the training data. Therefore, we propose a regularization approach that depends on the
number of available training samples, see Table 1. The optimally regularized representation
xˆ (r∗)ofanoisydata-columnx +ϵ minimizestheerrorbetweentheexact(noiseless)column
j j j
x and its regularized representation xˆ (r∗), e.g. x xˆ (r∗) , where r∗ is the truncation
j j j j 2
∥ − ∥
index in terms of the monomial basis.
Since the noiseless column is not known, the optimal truncation index r∗ is determined
either subjectively by observing the quality of fit for several columns (n < 150) or by the
Cross Validation (CV) technique (n 150).
≥
We employ a 10-fold CV, where during each fold the model is trained on a randomly
chosen subset of the training dataset and evaluated on a complementary (validation) subset.
As a metric, we consider the following validation errors:
(cid:42) (cid:43)
(cid:28) y yˆ (r) (cid:29) X Xˆ (r)
v v 2 v v F
(4.3) ρ(y ) = ∥ − ∥ , ρ(X ) = ∥ − ∥ ,
v v
y X
v 2 v F
∥ ∥ ∥ ∥14 E. ATZA AND N. BUDKO
where the angular brackets denote the arithmetic averaging over the CV folds. The optimal
truncation index r∗ corresponds to the polynomial degree for which the minimum of ρ(X ) is
v
attained.
There is a curious reason behind the fact that we have to use the error ρ(X ) in the
v
X-data rather than the usual error ρ(y ) in the y-data. In the polynomial IR method, the
v
vector of the dependent variable (either exact or noisy) is the second column of the basis
Vandermonde matrix V, see (3.1). Therefore, in exact arithmetic, the training y-data is
exactly reproduced for any r 2. Hence, strictly speaking, the IR model always over-fits the
≥
training y-data. The regularization of the IR model is validated and tuned on the columns of
the matrix X. This is possible, since apart from the noise in X itself, the y-noise is always
propagated into the Vandermonde matrix V and then into the columns of the coefficient
matrix Aˆ = (VTV)−1VTX. Thus, the prediction Xˆ = X AˆT(AˆAˆT)−1AˆX will be always
t t
affected by noise. Simply put, using the noisy data y + ϵ to construct V is equivalent to
using a wrong parameterization s = y, whereas the column data are generated in the s = y
̸
parameterization.
Finally,itisimportanttorealizethattherearetwowaystoregularizethenoisydata. One,
which is the focus of the present section, is to choose a single optimal (maximal) degree r∗ for
the representation of all data columns, i.e., both y and all x , j = 1,...,p. This, obviously,
j
has its drawbacks, since some of the columns may be noiseless or contain a different level of
noise. A more flexible and precise way is to determine the optimal degree r∗, j = 0,1,...,p,
j
for each column individually. Our preliminary numerical experiments have shown that the
latter ‘flexible’ regularization approach does not necessarily result in a better prediction of
the test y data. Nonetheless, in our opinion, this flexible regularization deserves a separate
t
in-depth investigation in the case of the non-i.i.d noise and for the purposes described in the
next section 5.
Figure2(bottom)illustratesthetestingofthepolynomial-degreetruncationregularization
scheme for the IR model on the three noiseless datasets described in section 3. The meaning
of the lines and symbols is the same as in Figure 2 (top), see section 3. The errors ρ(y ) (left,
v
colored, dashed) and ρ(X ) (right, colored, dashed) on the training dataset are the mean CV
v
errors (4.3). All data are exact up to numerical precision. The only differences with the IR
modelofFigure2(top)aretheover-completenatureofthetrainingdatasets(n = 150)andthe
application of the LS estimates (2.20) of the coefficient matrix Aˆ . Therefore, the horizontal
r
axis in Figure 2 is the number r of columns in the rectangular basis matrix V Rn×r, which
∈
is no longer equal to the number of samples n.
InthetestsofFigure2(bottom-left),theρ(y )errorstartsatthelevelofmachineprecision
v
forr = 2andincreasesthereafterduetotheaccumulationofnumericalerrors. Thattheρ(y )
v
error grows for r > 2 has to do with the fact that the exact representation of y is attained
already at r = 2 in the monomial basis. The enlarged coloured markers in the right plot
indicate the minima of the ρ(X ), the same markers in the left plot show the levels of ρ(y )
v t
errors attained with the corresponding r∗. As can be seen, the optimal r∗ gives the smallest
achievable error ρ(y ). Hence, in this noiseless case the regularization procedure correctly
t
identifies the rank of the complete dataset as the optimal r∗.
In the next numerical experiments, for training, validation and testing, we use a synthetic
dataset with p = 202 predictors. Two of these 202 predictors are generated by randomlyOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 15
sampling a non-functional relationship such as those shown in Figure 1. One of these ‘non-
functional’ columns has the data from a curve that cannot be parameterized by y, another
has the data from a cone. The remaining 200 features are obtained by evaluating polynomials
of degree lower than 12 at the sample points y . While we have investigated all three types
i
of noise listed above Eq. (4.2), we only present the results for the first and the third cases, as
the second, i.i.d. case appeared to be very similar to the third, non-i.i.d case. In all examples
we use n = 150 and the optimal polynomial degree r∗ is found with the CV method at the
minimum of ρ(X ).
v
Figure 3 (top) illustrates the application of the regularization procedure to the problem
where only the dependent-variable data y contains additive Gaussian noise. The columns
of the matrix X are exact up to machine precision. We consider three different standard
deviations σ = 0.05,0.1, and 0.2, corresponding to 5%, 10%, and 20% levels of noise relative
to the y-data magnitude. As can be seen from the test-error curves (top-left plot, dim solid
gray), the errors ρ(y ) attained with these choices of r∗ (top-right plot, enlarged colored
t
squares) are close to the smallest achievable ρ(y ) errors for all considered noise levels. At
t
the same time, the minimal ρ(X ) errors do not correspond to the smallest achievable ρ(X )
v t
errors (top-right plot, dim solid gray curves).
Figure 3 (bottom) illustrates the case of additive noise in the dependent variable and in
one-third of the predictors. While the behavior is generally similar to the previous case, the
attained optimal ρ(y ) errors increase more rapidly with the level of noise. The growth of
t
the ρ(y ) error (bottom-left, colored dashed) is now caused not only by the accumulation
v
of numerical errors and the statistical noise in y (via V), but by the statistical noise in the
X-data as well (via Aˆ).
Finally, we remark that the error ρ(y ) attained with the optimal r∗ on the training-
v
validation dataset is always much smaller (see the range of the right vertical axis) than the
corresponding error ρ(y ) on the test dataset. This is, obviously, one more case of the benign
t
overfitting [3], [4], caused by the choice of the monomial column basis in or IR model rather
than the statistical properties of the noise.
5. Removal of improper predictors. From the analysis and examples of the previous
sectionsitisclearthatalinearmodel,beittheclassicalMLRimplementationorthepresentIR
formulation, will produce reasonably exact predictions even if some of the predictor variables
do not satisfy the model assumptions. In the regularized IR formulation of section 4 the
model assumption is that the predictor is a polynomial in the dependent variable y of degree
at most r∗ 1, where r∗ is the rank of the optimally regularized Aˆ. In the case of exact
−
data discussed in section 3, the training set may become complete way before some of the
predictorfunctionshavebeenproperlysampled. Successofpredictionsinthepresenceofsuch
‘improper’ predictors gives an illusion of understanding of the underlying natural phenomena
[21]. In section 4 we have also investigated the case where the noise is present only in some
of the predictors. Such noisy predictors may be negatively affecting the predictive power of
the model. Thus, there appear to be two good reasons to detect and possibly discard both
the high-degree/non-functional and the noisy predictors from the model. Additionally, in
biological and agricultural applications, the large number of predictors included in the FOP
dataset is often due to a broad (untargeted) experimental search in the absence of any a-16 E. ATZA AND N. BUDKO
Noisyfilteredywith2qexactX
10−11 100
4 ×10−1
10−2
3 ×10−1
10−12
10−4
2 ×10−1
10−13 10−6
10−8
10−14
10−10
10−1 10−15 10−12
10−14
2 4 6 8 nois1y0filtered1y2with2qpartiallyno2isyXlssy4stem 6 8 10 12
r r
r∗, {p }opt σ=0.05 10−7 σ=0.1 σ=0.2 r∗
3 ×10−1
100
10−9
2 ×10−1 10−1
10−11
10−2
10−13
10−1 10−3
10−15
10−4
5 10 15 20 5 10 15 20
r r
r∗, {p }opt σ=0.05 σ=0.1 σ=0.2 r∗
Figure 3: Top: application of the regularization procedure in the case of exact predictor
data X and noisy dependent-variable data y + ϵ, ϵ (0,σ2I), with σ = 0.05 (blue),
∼ N i 1
σ = 0.1 (orange), and σ = 0.2 (green). Bright colored lines: validation errors ρ(y ) (left,
2 3 v
right vertical axis) and ρ(X ) (right). Dim dashed gray lines: errors on the test dataset, ρ(y )
v t
(left, left vertical axis) and ρ(X ) (right). Bright colored square markers: values of the test
t
errors ρ(y ) and ρ(X ) attained at r∗ (left and right respectively). Bright colored diamond
t t
markers: values of the test errors ρ(y ) (left, left vertical axis) and ρ(X ) (right) attained with
t t
r∗ and the optimal set p of predictors. Bottom: same as in the row above, but with a
opt
{ }
third of the columns of X affected by the additive Gaussian noise of the same type as the
noise in the dependent variable y.
priori information. The practical goal of such studies is to identify a preferably small subset
of microbiota, fungi, molecules, metabolites, or genes, allowing for future targeted and less
)ty(ρ
)ty(ρ
)vy(ρ
)vy(ρ
)tX(ρ,)vX(ρ
)tX(ρ,)vX(ρOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 17
hyper-parameter data used
optimal degree r∗ ρ(X ), training X-data
v
which predictor to remove χ , test X -data
j t
optimal threshold τ ρ(y), training y-data
opt
Table 2: Hyper-parameters of the regularized polynomial IR model with predictor removal
and the datasets used to tune these hyper-parameters.
expensive measurements of these predictor variables.
Discarding ‘unnecessary’ predictors is known as feature or variable selection in statistics
and ML. The main idea of feature selection is simple: one can sometimes achieve the same
or even better prediction with just a subset of predictors. However, no clear principle for
the inclusion or removal of any particular predictor has been put forward so far. Therefore,
feature selection methods are either combinatorial or heuristic in the ways they produce the
candidate subsets of predictors. Moreover, the criterion for choosing a particular subset is
the prediction error on the training dataset. Since this error has already been used to find
the optimal regularization parameter, the feature selection methods are often regarded with
suspicion in statistics [19], Chapter 5.
Sofarwehavetunedonehyper-parameter, theoptimaldegreer∗, todefinetheregularized
polynomial IR model. We have used the CV procedure on the training X-data to determine
thishyper-parameter. Thisleavesthetrainingy-dataandthetestX -dataavailablefortuning
t
other hyper-parameters in our model.
Since the goal is to remove the eventual ‘improper’ predictors, it is logical to use the
prediction of the X -data as the ‘usefulness’ criterion for each predictor. We introduce the
t
column-wise test set prediction error:
xˆ x
j j 2
(5.1) χ = ∥ − ∥ , j = 1,...,p,
j
x
j 2
∥ ∥
where x and xˆ are, respectively, the jth columns of the test matrix X and its predictor
j j t
Xˆ . One would, naturally, like to remove the predictors with large χ ’s. For example, all
t j
predictors with χ τ.
j
≥
This introduces another hyper-parameter, the threshold τ, which can be tuned utilizing
the last available portion of our data – the training y-data. Recall that this data could not
be used to tune the regularization parameter r∗, since ρ(y) always grows with r. Yet, for
a fixed r = r∗ the error ρ(y) depends on the set of included predictors, and we call it the
feature removal error. Table 2 summarizes the data usage by the regularized polynomial IR
algorithm with predictor removal for tuning its hyper-parameters.
Wedefinetheoptimalthresholdτ tobetheoneforwhichthefeature-removalerrorρ(y)
opt
withr = r∗ isminimal. Unfortunately,whileρ(y)normallytendstogrowforsmallτ,sinceone
starts to loose the useful predictors, the overall dependence of ρ(y) on τ is neither unimodal
nor smooth. Meaning, that there may be a set or a range of τ that gives approximately the
opt
same ρ(y) at r = r∗. In such cases we suggest to make a subjective choice of r∗ favouring the
smallest number of retained predictors.
We have applied the predictor removal procedure to the case of 200 polynomial columns18 E. ATZA AND N. BUDKO
Featureremovalerror Featureremovalerror
101
ρ(y)
101
ρ(y)
ρ(yt) ρ(yt)
100
100
10−1
10−1
10−2 10−1 10−2 10−1
threshold,τ threshold,τ
Figure 4: Feature removal errors ρ(y) (solid blue) and ρ(y ) (dashed gray), both with r = r∗,
t
forsyntheticdatasetscontainingadditivenoiseinthedependentvariableandone-thirdofpre-
dictors, σ = 0.05 (left) and σ = 0.1 (right). Red squares indicate the minima, corresponding
to the optimal thresholds τ .
opt
with degrees q 1 12 and two non-functional columns in the data-matrix X, where the
− ≤
dependent variable and a third of the columns of X are affected by additive Gaussian noise
with σ = 0.05.
Figure 4 presents two examples of the feature removal error on the training y-data (solid
blue)togetherwiththecorrespondingnon-accessiblefeatureremovalerrorsonthetesty -data
t
(dashed gray). In the left plot, where the level of noise is σ = 0.05, the choice of the optimal
threshold τ from the minimum of the training y-data corresponds to the smallest possible
opt
number of predictors. In the right plot, where σ = 0.1, the detected minimum of the feature
removal error is situated in the second valley of the error function and corresponds to many
unnecessary predictors retained in the model, as the depth of both valleys is approximately
the same. Checking for other local minima in this case and choosing the leftmost along the
τ-axis would be a better strategy to find τ . Regardless, it is comforting to observe that the
opt
featureremovalerrorsonthetrainingandtestdatasetsappearsynchronousinthesenumerical
examples.
In Figure 5 (top) the green triangles show the true ‘degrees’ (lengths of the nonzero parts
of the corresponding columns of the coefficient matrix A, used to generate the predictor data)
for each predictor. The vertical dashed dark-grey lines indicate the predictors containing the
Gaussian noise. The vertical dashed red lines indicate the non-functional predictors. The
horizontal solid orange line depicts the optimal degree r∗.
In Figure 5 (bottom) the orange circles show the value of the column prediction error
ρ(x ) on the test X -data for each predictor obtained with r = r∗. The dashed blue line is the
j t
optimal threshold τ . All predictors with the errors above that line (solid light-gray vertical
opt
lines) are discarded. Notice that all noisy predictors, as well as both non-functional predictors
are successfully removed.
Diamond markers in Figure 3 show by how much the removal of ‘improper’ predictors
improves the prediction error ρ(y ). In fact, it is close to the minimal achievable error with
t
all predictors retained. That level of error was, however, not always obtained with the crude
global regularization procedure. Hence, the removal of predictors may sometimes compensateOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 19
Col.wise-residualssigma0.05PartiallynoisyX
10
8
6
4
2
100
10 1
−
10 2
−
0 25 50 75 100 125 150 175 200
ColumnsofX
χ τ Non-function Noisycol. Discarded Truedeg. r
j opt ∗
Figure 5: Removal of ‘improper’ predictors (vertical solid light-gray lines) from synthetic
data. Top: actual degrees of each predictor (green triangles), the global truncation degree
r∗ (orange horizontal line), predictors containing Gaussian noise (vertical dashed dark-gray
lines),non-functionalpredictors(verticaldottedredlines). Bottom: χ errors(orangecircles),
j
the optimal threshold τ (horizontal dashed blue line).
opt
for the errors in determining the optimal regularization parameter.
6. Application to chemometric data. In this section we present the application of the
regularized IR model with predictor removal to the experimental dataset that is often used to
compare various regularized MLR models, such the Principal Component Regression (PCR)
or the Partial Least Squares (PLS) [19]. This dataset was first published in [23] and is also
included in the R-library pls [22] under the name of yarn. The dataset consists of 28 Near
InfraRed (NIR) spectra of Polyethylene terephthalate (PET) yarns, measured at p = 268
wavelengths, and 28 corresponding yarn densities. Seven of these 28 samples are designated
as the test dataset and the remaining n = 21 constitute the training dataset. The predictor
variables(spectralamplitudes)changecontinuouslywiththewavelength, whichbecomesclear
if one plots the rows of X as curves, see the middle plot of Figure 6a.
The training dataset columns have been sorted by the y-data. Since n = 21 < 150, we use
the visual inspection of the fitted polynomials, see Figure 6b, rather than the CV procedure
to determine the optimal degree r∗ = 5. The corresponding validation error ρ(X ) is shown
v
in Figure 7a (left plot) with its minimum reached at the maximum displayed degree r∗ = 5.
χ j
eergeD20 E. ATZA AND N. BUDKO
The first three rows of the coefficient matrix A˜ obtained with r = r∗ are shown as curves in
the bottom plot of Figure 6a. These curves correspond to the constant, linear and quadratic
components in the polynomial fits x (y) of each predictor. The column-wise prediction errors
j
χ are shown in the top plot of Figure 6b (solid blue), together with the optimal threshold
j
τ (horizontal dashed red). The ρ(y) error with r = r∗ = 5 as a function of τ can be seen in
opt
Figure 7a (right plot, solid blue), with its minimum indicated by the red square marker. This
minimum is close to the minimum of the inaccessible ρ(y ) error (dashed gray). Here, as in
t
the numerical examples of section 5, we also observe the synchronous behavior of the feature
removal errors on the training and test datasets.
The removed ‘improper’ predictors are indicated with vertical solid light-gray lines in
Figure 6b (middle and bottom). We have also extracted a few typical predictor data (colored
vertical lines in Figure 6a) and displayed them in Figure 6b using the same colors and line
styles for the corresponding polynomial fits. The retained predictors are in the left plot of
Figure 6b and the removed predictors are in the right plot. It is easy to recognize the cone-
type data, similar to the data in Figure 1 (right), in one of the discarded predictors (blue).
All predictor data in the discarded gray zone around the dashed blue line in Figure 6a have
this ‘conical’ shape. This means that the NIR data in this frequency band is situated on a
hyper-cone. Even if this frequency band is not required to build a high-quality predictive
IR model, it may become useful for testing other higher-dimensional manifold and nonlinear
models.
Finally, in Figure 7b we display the standard quality-of-prediction scatter plot which
compares the exact values of y against their predictions yˆ. As can be seen, the removal
of the ‘improper’ predictors significantly improves the quality of prediction. In fact, the
prediction error ρ(y ) on the test dataset goes down from 0.28 to 0.05 after the predictor
t
removal procedure.
7. Conclusions. Thehyper-curvePARCURmodelintroducedinthispaperprovidesanal-
ternative, predictor-centered look at the overparameterized multiple linear regression. Within
this framework we have been able to focus on the individual roles of predictors and to show
that a linear model can be trained and will make perfect predictions even in the presence
of predictor variables that violate the linear model assumptions, thus giving an illusion of
understanding of the underlying natural phenomena [21]. The column-centered nature of our
approach allowed us to come up with a rigorous algorithm for detecting such ‘improper’ pre-
dictors. Moreover, we observe that removing these predictors improves not only the adequacy
but also the predictive power of the model.
The polynomial IR version of the PARCUR model has been investigated here in consid-
erable detail. It attempts to build an ‘inverse’ relation between the dependent variable and
each predictor that we believe is much easier to interpret than the weights of the regression
vector in the classical MLR model. The polynomial IR model appears to work well with
chemometric data, but may also be suitable for other ‘smooth’ predictors. The main problem
with applying the IR model is the difficulty of sorting the dependent variable by magnitude in
the presence of noise. While directly using a noisy dependent variable as a parameter in the
PARCUR model is not prohibited, it complicates the interpretation of the regression results.
Certain types of predictor data (microbiome, metabolome, genetic) may exhibit jump-likeOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 21
Acceptedcolumns Rejectedcolumns
11 00 −− 43 χ τoj
pt
3.0
10−5
2.5
2.0 2
1.5
0
2.5 Aˆ[0,:] 1.0
Aˆ[1,:]
0.0 Aˆ[2,:] 0.5
0 50 100 150 200 250 0 1 0 1
y y
(a) (b)
Figure 6: (a): column-wise prediction errors χ and the optimal threshold τ (top); rows of
j opt
X displayed as curves (middle); first three rows of Aˆ displayed as curves (bottom). Removed
predictors are shown a vertical light-gray lines (middle, bottom). (b): predictor data and the
corresponding polynomial fits for the retained (left) and removed (right) predictors.
training
0.6
0.06
hρ(Xv)
i
0.5
ρ ρ( (y yt)
)
100 t te es st t, ,a imll pp rr oe pd ei rct po rr es
dictorsremoved
80
0.05
0.4
60
0.04
0.3
40
0.03
0.2
20
0.02
0.1
0
0.01
2 4 10−5 10−4 10−3 0 25 50 75 100
degree,r threshold,τ measuredy
(a) (b)
Figure 7: (a): validation error ρ(y ) as a function of r (left); feature removal error ρ(y) at
v
r = r∗ as a function of threshold τ (right). (b): yˆ v.s. y for the training dataset (blue circles)
and the test dataset predicted at r = r∗ with all predictors (green diamonds) and after the
removal of ‘improper’ predictors (red squares).
changes with the dependent variable, especially, in its lower and higher ranges. Successful
application of the IR model to these kinds of data will depend on finding a suitable basis for
the column function space, e.g., a piecewise-continuous finite-element basis.
AlthoughthePARCURmodelallowsforaflexibleper-columnregularization,inthispaper
we have failed to illustrate its potential advantage over the traditional global regularization
jχ
ˆA
atad-X )y(jx
ydetciderp22 E. ATZA AND N. BUDKO
approach. A separate investigation of this regularization technique is warranted, since it may
provide with a more rigorous way to detect and remove ‘improper’ predictors.
Finally, as we have seen in the application of the polynomial IR model to chemometric
data, it can detect the subsets of predictors for which a higher-dimensional model is more
appropriate. Nonlinear functional relations between the predictors and the dependent vari-
able produce data that are situated on such higher-dimensional manifolds. A two-parameter
hyper-surface model would be a natural extension of the present single-parameter hyper-curve
model.
REFERENCES
[1] Brady T West, Kathleen B Welch, and Andrzej T Galecki. Linear mixed models: a practical guide using
statistical software. Chapman and Hall/CRC, 2022.
[2] David Holzmu¨ller. On the universality of the double descent peak in ridgeless regression. arXiv preprint
arXiv:2010.01851, 2020.
[3] Peter L. Bartlett, Philip M. Long, Ga´bor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020.
[4] TrevorHastie,AndreaMontanari,SaharonRosset,andRyanJ.Tibshirani. Surprisesinhigh-dimensional
ridgeless least squares interpolation. The Annals of Statistics, 50(2):949 – 986, 2022.
[5] Raphael Linker, Itzhak Shmulevich, Amit Kenny, and Avi Shaviv. Soil identification and chemometrics
for direct determination of nitrate in soils using ftir-atr mid-infrared spectroscopy. Chemosphere,
61(5):652–658, 2005.
[6] Nurul Liyana Rozali, Kamalrul Azlan Azizan, Rajinder Singh, Sharifah Nabihah Syed Jaafar, Abrizah
Othman,WolframWeckwerth,andUmiSalamahRamli.Fouriertransforminfrared(ftir)spectroscopy
approachcombinedwithdiscriminantanalysisandpredictionmodelforcrudepalmoilauthentication
of different geographical and temporal origins. Food Control, 146:109509, 2023.
[7] Matthias Steinfath, Nadine Strehmel, Rolf Peters, Nicolas Schauer, Detlef Groth, Jan Hummel, Mar-
tin Steup, Joachim Selbig, Joachim Kopka, Peter Geigenberger, et al. Discovering plant metabolic
biomarkers for phenotype prediction using an untargeted approach. Plant Biotechnology Journal,
8(8):900–911, 2010.
[8] Julong Wei, Aiguo Wang, Ruidong Li, Han Qu, and Zhenyu Jia. Metabolome-wide association studies
for agronomic traits of rice. Heredity, 120(4):342– 355, 2018.
[9] Min Oh and Liqing Zhang. Deepmicro: deep representation learning for disease prediction based on
microbiome data. Scientific Reports, 10(1), April 2020.
[10] CathyCWesthues,GregorySMahone,SofiadaSilva,PatrickThorwarth,MaltheSchmidt,Jan-Christoph
Richter, Henner Simianer, and Timothy M Beissinger. Prediction of maize phenotypic traits with
genomicandenvironmentalpredictorsusinggradientboostingframeworks.Frontiersinplantscience,
12:699589, 2021.
[11] ShivaniSehrawat,KeyhanNajafian,andLinglingJin. Predictingphenotypesfromnovelgenomicmarkers
using deep learning. Bioinformatics Advances, 3(1):vbad028, 03 2023.
[12] Gustavo de Los Campos, John M Hickey, Ricardo Pong-Wong, Hans D Daetwyler, and Mario PL Calus.
Whole-genome regression and prediction methods applied to plant and animal breeding. Genetics,
193(2):327–345, 2013.
[13] Nickolai Alexandrov, Shuaishuai Tai, Wensheng Wang, Locedie Mansueto, Kevin Palis, Roven Rommel
Fuentes,VictorJunUlat,DmytroChebotarov,GengyunZhang,ZhikangLi,etal. Snp-seekdatabase
of snps derived from 3000 rice genomes. Nucleic acids research, 43(D1):D1023–D1027, 2015.
[14] SeungHwanLeeandJuliusHJVanderWerf. Mtg2: anefficientalgorithmformultivariatelinearmixed
model analysis based on genomic information. Bioinformatics, 32(9):1420–1422, 2016.
[15] LorenzoBrigatoandLucaIocchi.Acloselookatdeeplearningwithsmalldata.In202025thInternational
Conference on Pattern Recognition (ICPR), pages 2490–2497, 2021.
[16] Yao Dong, Shaoze Zhou, Li Xing, Yumeng Chen, Ziyu Ren, Yongfeng Dong, and Xuekui Zhang. DeepOVERPARAMETERIZED MULTIPLE LINEAR REGRESSION AS HYPER-CURVE FITTING 23
learningmethodsmaynotoutperformothermachinelearningmethodsonanalyzinggenomicstudies.
Frontiers in Genetics, 13, September 2022.
[17] TongHe,RuKong,AvramJ.Holmes,MinhNguyen,MertR.Sabuncu,SimonB.Eickhoff,DaniloBzdok,
Jiashi Feng, and B.T. Thomas Yeo. Deep neural networks and kernel regression achieve compara-
ble accuracies for functional connectivity prediction of behavior and demographics. NeuroImage,
206:116276, 2020.
[18] C.R.Vogel. Computationalmethodsforinverseproblems. Frontiersinappliedmathematics.SIAM,2002.
[19] Alan J. Izenman. Modern Multivariate Statistical Techniques. Springer New York, 2008.
[20] Total Least Squares and Errors-in-Variables Modeling: Analysis, Algorithms and Applications. Springer
Netherlands, 2002.
[21] L. Messeri and M.J. Crockett. Artificial inteligence and illusions of understanding in scientific research.
Nature, 4627(8002):49–58, 2024.
[22] Bjo¨rn-Helge Mevik and Ron Wehrens. The pls package: Principal component and partial least squares
regression in r. Journal of Statistical Software, 18(2):1–23, 2007.
[23] H. Swierenga, A.P. de Weijer, R.J. van Wijk, and L.M.C. Buydens. Strategy for constructing robust
multivariatecalibrationmodels. Chemometrics and Intelligent Laboratory Systems,49(1):1–17,1999.
[24] EdwardVThomas. Aprimeronmultivariatecalibration. AnalyticalChemistry,66(15):795A–804A,1994.