Q-ITAGS: Quality-Optimized Spatio-Temporal
Heterogeneous Task Allocation with a Time Budget
Glen Neville∗,†, Jiazhen Liu∗,†, Sonia Chernova†, and Harish Ravichandar†
Abstract—Complex multi-objective missions require the co- Incremental Task Allocation Active Learning
ordination of heterogeneous robots at multiple inter-connected
levels, such as coalition formation, scheduling, and motion plan- Specification TBO NAQ Trait-Quality Model
Heuristic Heuristic Maps Error Low?
ning. The associated challenges are exacerbated when solutions
to these interconnected problems need to both maximize task Feasibility & Current No
Duration Allocation
performance and respect practical constraints on time and
Queryable Query
resources. In this work, we formulate a new class of spatio- Scheduling Scheduling Expert Selector
Constraints (MILP)
temporal heterogeneous task allocation problems that considers
these complexities. We contribute a novel framework, named Feasibility & Current
Duration Schedule
Quality-Optimized Incremental Task Allocation Graph Search
Full Solution
(Q-ITAGS), to solve such problems. Q-ITAGS builds upon our Motion Planning
Motion Constraints
prior work in trait-based coordination and offers a flexible
interleavedframeworkthati)explicitlymodelsandoptimizesthe
effectofcollectivecapabilitiesontaskperformancevialearnable Figure 1: Q-ITAGS performs spatio-temporal task allocation
trait-quality maps, and ii) respects both resource constraints for heterogeneous multi-robot teams by optimizing collective
and spatio-temporal constraints, including a user-specified time task performance while also respecting spatio-temporal and
budget (i.e., maximum makespan). In addition to algorithmic
resource constraints. To this end, it explicitly models, actively
contributions,wederivetheoreticalsuboptimalityboundsinterms
learns, and optimizes trait-quality maps that approximate the
of task performance that varies as a function of a single
hyperparameter.Ourdetailedexperimentsinvolvingasimulated effects of collective capabilities on task performance.
emergency response task and a real-world video game dataset
reveal that i) Q-ITAGS results in superior team performance binary success-or-failure model of task outcomes [8], [13],
compared to a state-of-the-art method, while also respecting [14]. Such a limited model does not apply to several real-
complex spatio-temporal and resource constraints, ii) Q-ITAGS world problems which require maximization of performance
efficiently learns trait-quality maps to enable effective trade-off
metrics, as opposed to satisfaction of arbitrary thresholds
between task performance and resource constraints, and iii) Q-
(e.g., distributed sensing, supply chain optimization, and dis-
ITAGS’ suboptimality bounds consistently hold in practice.
aster response). Third, existing methods often demand that
I. INTRODUCTION users explicitly specify the requirements for successful task
completion [5], [15], [16]. However, it is well known that
Heterogeneous multi-robot systems (MRS) bring together
humans, while adept at making complex decisions, often fail
robots with complementary capabilities to tackle challenges
to precisely articulate how they do so; in fact, we tend to
in domains as diverse as agriculture [1], defense [2], assem-
make things up to justify our decisions [17], [18]. We refer
bly [3], and warehouse automation [4]. Achieving effective
the reader to Sec. II for a detailed discussion of related work.
teaming in such complex domains requires reasoning about
We contribute Quality-optimized Incremental Task Alloca-
coalition formation (who) [5], scheduling (when) [6], motion
tion Graph Search (Q-ITAGS)1, a heuristic-driven interleaved
planning (how) [4], as well as their intersections [7], [8].
search algorithm that sheds critical assumptions of existing
Inthiswork,wetacklethechallengeofspatio-temporaltask
methods and overcomes the corresponding limitations (see
allocation for heterogeneous teams of robots, which requires
Fig. 1). First, improving upon our prior work in trait-based
that the problems of task allocation, scheduling, and motion
task allocation [5], [7], [8], [14], [19], Q-ITAGS models both
planning be solved simultaneously. Specifically, we focus on
agentsandtasksintermsofcapabilitiesandtaskrequirements
relaxing three key limiting assumptions of related methods in
in terms of collective capabilities, leading to a flexible and
the current literature. First, several methods assume that one
robot-agnosticframework.Assuch,Q-ITAGSneitherrequires
can either decompose multi-robot tasks into individual robot
decomposition of multi-robot tasks nor the enumeration of all
subtasks [9], [10] or enumerate all possible coalitions [11],
possiblecoalitions.Second,Q-ITAGSutilizescontinuoustrait-
[12].However,itisoftendifficult,ifnotimpossible,toexplic-
quality maps, a novel and expressive model of performance
itlyspecifytheroleofeachrobotintasksthatinvolvecomplex
that maps collective capabilities to task performance, helping
collaboration, and enumerating every capable coalition might
encode and optimize the quality of allocations. Third, Q-
be intractable. Second, existing methods tend to assume a
ITAGS does not demand explicit specification of task require-
ments. Instead, it employs an active learning module to learn
This work was supported by the Army Research Lab under Grants
W911NF-17-2-0181(DCISTCRA)andW911NF-20-2-0036 how collective capabilities relate to task performance.
∗These authors contributed equally; †The authors are with the Georgia
InstituteofTechnology,Atlanta,GA,USA{gneville, jliu3103,
Chernova, harish.ravichandar}@gatech.edu 1Open-sourceimplementation:https://github.com/GT-STAR-Lab/Q-ITAGS
4202
rpA
11
]AM.sc[
1v20970.4042:viXraWe first formalize a new class of problems, dubbed spatio- constraints [9], [10], [22]. Auctions are highly effective, but
temporal task allocation with quality optimization (STAQ), typicallyeitheri)requiremulti-robottaskstobedecomposable
which requires maximizing the quality of allocations (i.e., into sub-tasks, each solvable by a single robot, or ii) assume
task performance) while simultaneously ensuring that the the ideal distribution of agents for each task is known (e.g.,
makespan of the associated schedule is less than a user spec- Task 1 requires one ground and one aerial robot).
ified threshold (i.e., time budget). To solve STAQ problems, Optimization-basedmethodsformanotherclassofsolutions
we then develop two heuristics that guide the search within that formulate the ST-MR-TA problem as a mixed-integer
Q-ITAGS. Specifically, we develop i) Time Budget Overrun linear program (MILP) to optimize the overall makespan or
(TBO), which captures the suboptimality of a given schedule a utility function [23]–[25]. However, these methods often
anchored against a user-specified threshold on makespan, and require that all tasks be decomposable into single-agent tasks
ii) Normalized Allocation Quality (NAQ), which provides a [12]. In contrast to auction-based and optimization-based
normalized estimate of the current allocation’s quality. approaches,ourapproachdoesnotrequiretaskdecomposition.
We extensively evaluated Q-ITAGS in terms of its ability Our approach to task allocation is most closely related
to i) optimize task allocation quality, ii) respect time budgets, to trait-based methods [5], [7], [13]–[15], [19], [26]–[29],
and iii) learn unknown mappings between collective capabil- whichutilizesaflexiblemodelingframeworkthatencodestask
ities and task performance. We conducted our evaluations on requirements in terms of traits (e.g., Task 1 requires traveling
both a simulated emergency response domain and a publicly- at 10m/s while transporting a 50-lb payload). Each task is
available dataset of a multi-player game. Our results conclu- not limited to a specific set or number of agents. Instead,
sivelydemonstratethatQ-ITAGSoutperformsthestate-of-the- the focus is on finding a coalition of agents that collectively
art in terms of allocation quality while generating schedules possess the required capabilities. As such, these methods
of a similar makespan. Further, Q-ITAGS’ active sampling allow for flexible and automated determination of coalition
approach learns unknown task quality mappings consistently sizes and compositions. However, most existing trait-based
more efficiently than a passive approach that learns from approaches are limited to ST-MR-IA problems that do not
uniformly sampled data. require scheduling [5], [14], [15], [26], [27], [29], with a few
In addition to demonstrating the empirical benefits of Q- notable exceptions that can handle ST-MR-TA problems [7],
ITAGS, we provide theoretical insights into its operation. [13], [19], [28].
We derive a bound on Q-ITAGS’ sub-optimality in terms A key limitation of existing trait-based algorithms, includ-
of allocation quality under mild assumptions. Notably, our ing[7],[13],[19],isthattheyassumethattheuserwillexplic-
analysis illuminates an inherent trade-off between allocation itlyquantifytheminimumamountofcollectivetraitsnecessary
quality and schedule makespan that hinges on a single hy- to perform each task. This presents two issues. First, explicit
perparameter. This insight and analysis will provide users quantification of minimum requirements can be intractable
with intuitive guidance about the trade-off when choosing the even for experts when dealing with complex tasks and multi-
hyperparameter.Wealsodemonstrateviaexperimentsthatour dimensional capabilities [17]. Other sub-fields within robotics
bounds consistently hold in practice. recognizethisconcernandhavedevelopedmethodstoprevent
In summary, we contribute: i) a formalism for a new and handle reward or utility mis-specification [30], [31]. Sec-
class of problems in spatio-temporal task allocation that focus ond, all methods, including those that learn requirements [28]
on optimizing allocation quality while respecting makespan assume that any and all additional traits allocated beyond the
constraints, ii) an interleaved search algorithm and associ- minimumprovidenobenefittotheteamandthatnotsatisfying
ated heuristics to effectively solve such problems, iii) an the specified threshold will lead to complete failure. This
activelearningmethodtolearnunknownrelationshipsbetween effectivelyignoresthenaturalvariationintaskperformanceas
collective robot capabilities and task performance, and iv) afunctionofallocatedcapabilities.Instarkcontrast,Q-ITAGS
theoretical bounds on suboptimality in allocation quality. does not require the user to specify trait requirements and
utilizes a more expressive model to capture the relationship
II. RELATEDWORK
between allocated capabilities and task performance.
While the multi-robot task allocation problem has many
variants [20], [21], we limit our discussion to the variant to
III. PROBLEMFORMULATION
which our problem is closely related: single-task (ST) robots,
We begin with preliminaries from our prior work [7], [8],
multi-robot (MR) tasks, and time-extended (TA) allocation.
[19] for context, and then formalize a new class of spatio-
ST-MR-TA problems require assigning coalitions of agents to
temporal task allocation problems.
tasks while respecting temporal constraints, such as prece-
dence and ordering constraints, spatio-temporal constraints
A. Preliminaries
(e.g., travel time), and deadlines. In addition to these issues,
Consider a team of N heterogeneous robots, with the ith
ourproblemformulationalsoinvolvesspatialconstraints(e.g.,
robot’s capabilities described by a collection of traits q(i) =
task locations and obstacles). (cid:104) (cid:105)⊺
q(i), q(i), ,q(i) RU , where q(i) corresponds to the
Auction-based methods to solve ST-MR-TA problems in- 1 2 ··· U ∈ ≥0 u
volve auctioning tasks to robots through a bidding process uth trait for the ith robot. We assign q(i) = 0 when the ith
u
based on a utility function that combines the robot’s (or the robotdoesnotpossesstheuthtrait(e.g.firetruckshaveawater
coalition’s) ability to perform the task with any temporal capacity, but other robots may not). As such, the capabilitiesof the team can be defined by a team trait matrix: is a description of the world (e.g., a map), and C is the
max
(cid:104) (cid:105)⊺ total time budget.
Q= q(1), , q(N) RN×U Finally, we define a solution to the problem defined by D
··· ∈ ≥0 (cid:68) (cid:69)
using the tuple S = Aˆ, σˆ, Xˆ , where Aˆ is an allocation, σˆ
whose iuth element denotes the uth trait of the ith robot.
is a schedule that respects all temporal constraints, and Xˆ is
We model the set of M tasks that need to be completed
a finite set of collision-free motion plans. The goal of STAQ
as a Task Network T: a directed graph G = ( , ), with
V E istofindasolutionthatmaximizesthetotalallocationquality
vertices representing a set of tasks a M , and edges
representV
ing constraints between
tasks{
.
Fm o} rm a= p1
air of tasks
aE Ξ(A) while respecting the total time budget C max.
i Active Learning for STAQ: As above, ξ (y ) is a function
and a with a = a , two kinds of constraints are encoded m m
j i ̸ j that maps the collective traits assigned to the mth task to
in T: (i) precedence constraint a a requires that Task
i ≺ j a measure of task performance. While accurate knowledge
a should be completed before the Task a can begin (e.g., a
i j of these functions is necessary to make effective trade-offs
fire must be put out before repairs can begin) [32]; (ii) mutex
when allocating limited resources (i.e., robots) to competing
constraint ensures that a and a are not concurrent (e.g. a
i j objectives (i.e., tasks), they are often difficult to explicitly
manipulator cannot pick up two objects simultaneously) [33].
specify.Assuch,wedonotassumeknowledgeofξ (), m.
In addition, we also define C max as the user-defined total Instead,weassumeaccesstoasimulatorfromwhicm ht· ole∀ arn.
time budget which encodes the longest acceptable makespan.
Wearespecificallyinterestedinlimitingthenumberofqueries
Let A A 0,1 M×N denote the binary allocation
∈ ⊆ { } madetofacilitatelearningsincesuchqueriescanbeexpensive
matrix where the entry A =1 if and only if the nth robot
mn or time-consuming. As such, we formulate the problem of
is allocated to the mth task, for m 1, ,M and n
∈ { ··· } ∈ learning ξ m(), m as an active learning problem in which
1, ,N ; A = 0 otherwise. Robots can complete tasks · ∀
{ ··· } mn one must effectively sample the most informative coalitions
individually or collaborate as a coalition, and any robot can
to efficiently learn the trait-quality maps.
be allocated to more than one task as long as the tasks are
scheduled to be carried out during non-overlapping intervals. IV. APPROACH
We use A m to denote the mth row of A and it specifies the To solve the STAQ problems, as defined in Sec. III, we
coalitionofrobotsassignedtothemthtask.Wefurtherdefine introduce our Quality-optimized Iterative Task Allocation
Y = AQ
∈
RM ≥0×U to be the aggregated traits for all tasks Graph Search (Q-ITAGS) algorithm. We begin by providing
according to allocation plan A, with y m denoting its mth row a brief overview of Q-ITAGS before supplying details.
containing the collective traits available for the mth task. Q-ITAGS utilizes an interleaved architecture to simultane-
ously solve task allocation, scheduling, and motion planning
B. Quality-based Spatio-Temporal Task Allocation (see Fig. 1). This interleaved approach is inherited from our
In this work, we extend the problem formulation from prior work [7], [8], [19], in which we provide extensive
our prior work to account for the quality of task allocation. evidence for its benefits over sequential counterparts that are
We formulate a new class of problems, spatio-temporal task commonly found in the literature. For instance, interleaved
allocation with quality optimization (STAQ), which involves architectures are considerably more computationally efficient
optimizing the quality of robots’ assignments to tasks while since they avoid backtracking [8]. However, Q-ITAGS makes
ensuring that the associated makespan is less than a user- crucial improvements over our prior work: i) optimizing for
specified threshold. We then extend STAQ to include active allocation quality instead of makespan, ii) utilizing a more
learning of unknown trait-quality maps, which map the col- expressive model of task performance, and iii) respecting a
lective capabilities of a given coalition to task performance. time budget (see Sec. II).
Let ξ m : RU ≥0 → [0,1] be the normalized trait-quality Q-ITAGS performs an incremental graph-based search that
mapthatcomputesanon-negativequalityscore(withnumbers is guided by a novel heuristic. Our heuristic balances the
closer to 1 indicating higher quality coalitions) associated searchbetweenoptimizingtaskallocationqualityandmeeting
with the mth task given the collective traits allocated to it. thetimebudgetrequirement.Weformulateandsolveamixed-
In essence, ξ m quantifies how a given coalition’s capabilities integer linear program (MILP) to address the scheduling and
translate to performance on the mth task. We define the total motion planning problems as part of the incremental search.
allocation quality to summarize the performance of all tasks: To get around the challenge of explicitly specifying task
requirements, Q-ITAGS employs an active learning module
M M
Ξ(A)= (cid:88) ξ (QTAT)= (cid:88) ξ (y ) (1) to learn the relationship between the collective traits of the
m m m m coalition and task performance.
m=1 m=1
Notethatthemapsξ m, marebetweentraits(notrobots)and A. Task Allocation
∀
performance, yielding more generalizable measures. Thetaskallocationlayerperformsagreedybest-firstsearch
We can now define the problem domain using the tuple through an incremental task allocation graph . In this
N
D = T, Q, Ξ, I , L , W,C , where T is the task graph, each node represents an allocation of robots to tasks.
c T max
⟨ ⟩
network, Q is the team trait matrix, Ξ is the summarized Nodes are connected to other nodes that differ only by the
performance function, I and L are respectively the sets of assignment of a single robot (see Fig. 2). Note that the root
c T
allinitialandterminalconfigurationsassociatedwithtasks,W node represents all robots allocated to each task. Indeed, suchA1 A5 Am
A2 A6 Aj
A0 . . . . .
. . .
. . .
. . .
A4 A8 Ak
Move survivor A to hospital
Move survivor B to hospital
Put out fire
Rebuild building
Figure2:Anexampleincrementaltaskallocationgraph.Startingwithallrobotsassignedtoeachtask,Q-ITAGSincrementally
removes assignments until finding a solution that simultaneously meets the time budget and optimizes task performance.
an allocation will result in the best performance (since all where C denotes the makespan of the schedule σ, σ is the
σ
available robots contribute to each task), but will result in the schedule associated with the A being evaluated, C is the
max
longest possible schedule (sequential execution of tasks). Q- user-specified time budget for makespan, and σ is the
worst
ITAGS’ search starts from this initial node, and incrementally schedule at the root node which prohibits parallelization by
removes assignments to find an allocation that can meet the allocating all robots to each task.
timebudgetwithoutsignificantlysacrificingtaskperformance.
Since TBO only considers the schedule and not the allo-
To guide the search, we developed two heuristics: Normal- cation, it tends to favor nodes deeper in the graph that have
ized Allocation Quality (NAQ), which guides the search based fewer agents and constraints and thus a lower makespan. As
on the quality of the allocation, and Time Budget Overrun such, TBO favors allocations that satisfy the time budget at
(TBO), which guides the search based on the makespan of the the expense of a deeper search and more node expansions.
schedule associated with the allocation.
Time-Extended Task Allocation Metric (TETAM): Q-ITAGS
Normalized Allocation Quality (NAQ): NAQ is a normalized
employs a convex combination of NAQ and TBO in order to
measureofhowagivenallocationimprovestaskperformance.
consider both the quality of the allocation and time budget:
Specifically, NAQ is calculated as
Ξ(A ) Ξ(A)
f NAQ(A)=
Ξ(A
ror oo to )t −
Ξ(A null)
(2) f TETAM(A,C σ)=(1 −α)f NAQ(A)+αf TBO(C σ) (4)
−
where A is the allocation being evaluated, Ξ(A) is the total
where α [0,1] is a user-specified parameter that controls
allocation quality as defined in Eq. (1), A root is the allocation each heur∈ istic’s relative influence. Thus, TETAM considers
attherootnode,andA representsnorobotsbeingassigned
null both allocation quality and the schedule simultaneously when
toanytask.SinceΞ(A )andΞ(A )respectivelydefinethe
root null searching for a solution. See Sec. V for a theoretical analysis
upperandlowerboundsofallocationquality,NAQisbounded
of the trade-offs between allocation quality and makespan.
within [0,1]. Note that the quality functions ξ () M ,
{ m · }m=1
needed to compute Ξ(), would be learned as described in
Sec. IV-C. · B. Scheduling and Motion Planning
NAQ does not consider the scheduling layer and promotes Q-ITAGS’ scheduling layer checks the feasibility of
a broader search of the allocation graph. This is due to the scheduling a given allocation and helps compute its TBO. To
fact that shallower nodes have more robots allocated and as a thisend,weformulateamixed-integerlinearprogram(MILP)
resulttendtoresultinhigherallocationquality.Assuch,NAQ thatconsidersthreetypesoftemporalconstraints:precedence,
favors allocations that maximize allocation quality and task mutex, and travel time. Precedence constraints ensure that
P
performance at the expense of a potentially longer makespan. onetaskhappensbeforeanother (e.g.,thefiremustbedoused
before repairs begin). Mutex constraints ensure that two
Time Budget Overrun (TBO): TBO measures how much the
M
tasks do not happen simultaneously (e.g., a robot cannot pick
schedule associated with the current allocation violates the
uptwoobjectssimultaneously).Traveltimeconstraintsensure
time budget. It is calculated as
that robots have sufficient time to travel between task sites
(cid:18) C C (cid:19) (e.g., traveling to the location of fire before dousing). The
f TBO(C σ)=max
C
σ − m Cax , 0 (3) MILP takes the following form:
|
σworst
−
max
|
tobor
cidemaraP
1
tobor
noitcurtsnoC
2
tobor
noitcurtsnoC
tobor
gnithgiferiFdecisions when allocating limited resources (i.e., robots) to
min C competing tasks. Further, experiments reveal that our active
{si},{pij} learningapproachresultsinbettersampleefficiencyandmore
s.t. C s +d , i=1,..,M consistent learning, compared to a passive approach which
i i
≥ ∀
s s +d +x , (i,j) employs uniform sampling (see Sec. VI-C).
j i i ij
≥ ∀ ∈P
s
i
x i, i=1,..,M V. THEORETICALANALYSES
≥ ∀
s s +d +x βp , (i,j) R
i j j ji ij To better understand Q-ITAGS’ performance, we analyze
≥ − ∀ ∈M
s s +d +x β(1 p ), (i,j) R the effect of α – the user-specified parameter that determines
j i i ij ij
≥ − − ∀ ∈M
the relative importance of our two heuristics – on the opti-
where C is the makespan, s and d are the (relative) starting
i i
mality of the obtained solution defined with respect to the
time and duration of Task a , x is the time required to
i ij
quality of task allocation. We demonstrate that the choice of
translate from the site of a to the site of a , x is the initial
i j i α determines a suboptimality bound, where α values between
time required for the allocated coalition to travel to the site of
α = 0 and α = 0.5 promise increasingly tighter bounds on
Task a , p 0,1 , and p = 1 if and only if a precedes
i ij ∈ { } ij i suboptimality. This analysis and the fact that α values closer
a , β R is a large scalar, and and are sets of integer
j +
∈ P M to 1 provide solutions that prioritize makespan can give the
pairscontainingthelistsofprecedenceandmutexconstraints,
user an intuitive understanding of the relationship between
with R = denoting mutex constraints with
M M−P ∩M allocation quality and makespan.
precedence constraints removed.
Below, we derive strict suboptimality bounds for solutions
Q-ITAGSconstructsaninitialschedulebyestimatingtravel
generated by Q-ITAGS, in terms of total allocation quality.
timesbasedonEuclideandistancesbetweentasksites.During
the search, the scheduling layer iteratively queries the motion Theorem 1. For a given problem domain D, let A∗ be the
planner to account for and accurately estimate travel times. optimal allocation w.r.t. total allocation quality under a time
Thisiterativeprocesscontinuesuntilallmotionplansrequired budget, and Aˆ be the allocation of the solution generated by
by the schedule are instantiated. Q-ITAGS also memoizes Q-ITAGS. If α<0.5 in Eq. (4), then
motion plans to improve computational efficiency.
α
Ξ(A∗) Ξ(Aˆ) (Ξ(A ) Ξ(A )), (5)
− ≤ 1 α root − null
C. Active Learning −
Below, we discuss how Q-ITAGS models the trait-quality whereΞ(A root)andΞ(A null)denotethetotalallocationquality
maps ξ () M ,samplesallocationsduringdatacollection, respectively when all robots are assigned to all tasks, and
and u{ pdm ate· s} tm he=1 trait-quality maps from data. To facilitate when no robot is assigned to any task.
learning, we utilize a simulator that can estimate the task
Proof. Since any expansion of a parent node represents the
performanceofanygivencoalition.Notethatwhenasimulator
subtractionofanassignment,anygivennodeN isguaranteed
is not available, a human expert can be queried for labels.
to have fewer agents assigned than its parent N . This obser-
p
Q-ITAGS’activelearningmodulehastwokeycomponents:
vation,whencombinedwiththefactthatremovingagentscan
Trait-Quality Maps: As discussed above, trait-quality maps never improve the allocation quality (as removing an agent
specify the relationship between collective capabilities and always decreases the traits of the coalition), yields
task performance. Formally, we model the joint distribution
of collective traits assigned to a task and the resulting task f (N) f (N ), (6)
NAQ NAQ p
≥
performanceasaGaussianProcesswitharadialbasisfunction
kernel: P (y ,ξ (y )) (µ ,Σ ), m. As such, we since a higher allocation quality results in a lower NAQ
m m m m m
can predict the quality o∼ f aG gP iven collecti∀ on of traits using heuristicvalueandviceversa.Consequently,wecaninferthat
Gaussian Process Regression. Note that this design choice the NAQ value of all nodes in the unopened set of
U ⊆ N
allows Q-ITAGS to not just estimate quality, but also to the Q-ITAGS graph is greater than or equal to that of their
quantifyitsuncertainty,whichinturnhelpsinactivelearning. respective predecessors in the opened set . As such,
O ⊆ N
the smallest NAQ value in the unopened set must be greater
Query Selector: To ensure efficient learning, Q-ITAGS’ query
than that in the opened set:
selector samples the most-informative data points from a
collection of unlabeled data points (i.e., coalitions that have
minf (N) minf (N) (7)
not been sampled). Q-ITAGS employs a maximum-entropy N∈U NAQ ≥N∈O NAQ
samplingapproachthatcomputestheuncertaintyofthecurrent
Irrespective of whether the optimal allocation A∗ corre-
trait-quality map’s quality estimate for each unlabeled data
sponds to a node in the open or unopened set, the inequality
point, and then selects the data point where the uncertainty is
in Eq. (7) implies that
the highest. Note that we do not use upper confidence bound
(UCB)samplingsince,duringlearning,wearemoreinterested Ξ(A∗) maxΞ(A ) (8)
N
in coverage rather than in optimizing task performance. As
≤N∈O
such, our maximum entropy approach chooses samples in where A denotes the allocation of a given node N. We use
N
regions where the current model is the most ignorant, build- N′ to denote the node from the open set with the maximum
ing a more comprehensive model that will inform trade-off total allocation quality. Namely, N′ =argmax Ξ(A ).
N∈O NSince we require any valid solution of Q-ITAGS to respect
AllocationQuality Makespan
the time budget, the solution node (Nˆ) will have a zero TBO 500 200
Q-ITAGS ITAGS
value. As such, the total TETAM heuristic defined in Eq. (4) 400
150
Q-ITAGS(cid:31) ≡ITAGS
for the solution node is given by
300
f (Nˆ)=(1 α)f (Nˆ) (9) 100
TETAM NAQ
− 200
SinceQ-ITAGSperformsgreedybest-firstsearch,theTETAM
50
100
valueofthesolutionnodeisguaranteedtobelessthanorequal
to all nodes in the open set:
0 0
0 100 200 300 400 500 0 50 100 150 200
f (Nˆ) f (N), N (10) ITAGS ITAGS(s)
TETAM TETAM
≤ ∀ ∈O
Expanding the definition of TETAM and using Eq. (9) yields Figure3:Q-ITAGSconsistentlygeneratessolutionsofsuperior
Ξ(A ) Ξ(Aˆ) quality (left) while simultaneously ensuring that its makespan
(1 α) root − is better than or equal to that of ITAGS (right). Green dots
− Ξ(A ) Ξ(A ) ≤
root − null (11) indicate Q-ITAGS performs better than ITAGS and Grey
Ξ(A ) Ξ(A )
αf (N)+(1 α) root − N , N dots indicate Q-ITAGS and ITAGS perform equally. Larger
TBO − Ξ(A root) Ξ(A null) ∀ ∈O allocation quality and smaller makespan are desirable.
−
Using the bound in Eq. (8), and the fact that f () 1,
TBO · ≤ theoretical bounds on resource allocation suboptimality held
we rewrite the RHS of above equation as
in practice. In the third set of experiments, we evaluated the
Ξ(A ) Ξ(Aˆ) active learning strategy of Q-ITAGS on a real-world dataset
(1 α) root −
− Ξ(A ) Ξ(A ) ≤ involving the American Football video game NFL Madden.
root − null (12)
Ξ(A ) Ξ(A∗)
α+(1 α) root − , N , A. Coordinated Emergency Response
− Ξ(A ) Ξ(A ) ∀ ∈O
root − null WeevaluatedQ-ITAGS’performanceon50simulatedemer-
After rearranging and canceling equivalent terms on both gencyresponseproblems(see[7],[8]fordetails),andcontrast
sides, we can simplify the above equation as it with ITAGS [7], [19]. We chose to compare our approach
(1 α)(Ξ(A ) Ξ(Aˆ)) with ITAGS as i) it has been shown to perform better than
− root − ≤ (13) otherstate-of-the-arttime-extendedtaskallocationalgorithms,
α(Ξ(A ) Ξ(A ))+(1 α)(Ξ(A )) Ξ(A∗))
root null root and ii) ITAGS is also a trait-based approach that shares
− − −
Finally,rearrangingthetermsyieldstheboundinEq.(5). similar assumptions allowing for a more reasonable compar-
ison. A key difference is that ITAGS minimizes makespan
Notethattheabovetheoremprovidessensibleboundsonthe
subject to allocation requirements, while Q-ITAGS optimizes
difference in allocation quality between the optimal solution
allocation performance subject to scheduling requirements.
and Q-ITAGS’ solution only when 0 α < 0.5. When
We first computed a solution to each problem using ITAGS
≤
α 0.5, the bound loses significance as it grows beyond the
and evaluated that solution’s allocation quality (using ground-
≥
maximumdifferenceinqualitybetweenΞ(A )andΞ(A ).
root null truth trait-quality maps) and makespan. We then evaluated Q-
In other words, Eq. 5 holds trivially when α 0.5, since
ITAGS’ ability to improve ITAGS’ allocation quality when
≥
Ξ(A ) has the worst quality metric of all allocations and
null constraining Q-ITAGS’ time budget to be equal to ITAGS’
Ξ(A ) has the best one, meaning a bound greater than
root makespan. As shown in Fig. 3, Q-ITAGS consistently im-
Ξ(A ) Ξ(A ) would not be significant.
root null proves the allocation quality for all problems, while simulta-
−
The bound presented in Eq. (5) can be tightened after the neously ensuring that its makespan is either equal to or lower
execution of the Q-ITAGS to facilitate post-hoc performance than that of ITAGS. These results demonstrate that Q-ITAGS
analyses. Specifically, instead of bounding f TBO(N), N can outperform ITAGS in terms of task performance while
∀ ∈
by 1, we can compute post execution the exact value simulateneously respecting time budget.
O
f (N′) for N′, which is the node yielding the maximum
TBO
allocation quality. Subsequently, following similar algebraic B. Validating Guarantees on Optimality Gaps
manipulations as done in the proof above, a tighter bound on In our second experiment, we empirically examined the
the suboptimality gap can be derived as validity of our theoretical guarantees on allocation quality
α from Sec. V. To this end, we varied α between [0,1] in
Ξ(A∗) Ξ(Aˆ) (Ξ(A ) Ξ(A ))f (N′)
− ≤ 1 α root − null TBO incrementsof0.1,andsolvedeachofthe50probleminstances
− from the previous section for each value of α. For every
VI. EVALUATION
combination of problem and α value, we computed the actual
WeevaluatedQ-ITAGSusingthreesetsofexperiments.The normalized optimality gap and the corresponding normalized
first set of experiments evaluated Q-ITAGS in a simulated theoreticalboundinEq.(5).Wefoundthattheoptimalitygaps
emergency response domain [34]–[38] and compared its per- consistently respect the theoretical bound across all values of
formance against a state-of-the-art trait-based task allocation α (see Fig. 4). As we expected, the extreme values α = 0
method. The second set of experiments examined if the (ignoringTBO)andα=1(ignoringNAQ)respectivelyresult
SGATI-Q
)s(SGATI-QValidationofBoundsonAllocationQuality
ComputationTime(Unit:s)
1200
1.0
Theoreticalbound Q-ITAGS ≺ITAGS
Actualoptimalitygap 1000
0.8
800
0.6 600
400
0.4
200
0.2
0
0 200 400 600 800 1000 1200
ITAGS
0.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Figure 6: Q-ITAGS’ many benefits over ITAGS (see Sec. VI)
αvalue
comes at the cost of slightly worse computation time.
Figure4:Thetheoreticalboundconsistentlyholdsforvarying
learning module’s goal is to efficiently learn the trait-quality
values of α. A value of 0 for a normalized optimality gap
mapsthatpredictagivenplayer’squalityscoreforaparticular
represents an optimal allocation, and a value of 1 represents
position, given the player’s traits.
the worst possible allocation seen.
We compared Q-ITAGS’ active learning module to a base-
line module that uniformly randomly samples training data
Active Learning Versus Uniform Sampling from the dataset. To quantify and compare their effectiveness,
we computed their respective root mean squared prediction
error (RMSE) which is defined as
(cid:34) K (cid:35)
1 (cid:88)
RMSE(K)=sqrt (l l )2 , (14)
K i,true − i,pred
i=1
when K data points are supplied. l and l are the
i,true i,pred
ground truth and predicted quality scores from our learning
module, respectively. We computed the performance of the
uniform baseline over 20 random seeds to ensure a fair
comparison. The results for six different positions are shown
in Fig. 5, with shaded regions denoting the maximum and
minimum prediction errors of the uniform-sampling baseline.
As seen in Fig. 5, Q-ITAGS is consistently more sample
efficientthantheuniform-samplingbaseline,andiscapableof
learning the trait-quality maps with fewer training samples. In
Figure 5: Q-ITAGS’ active learning module consistently out- particular, Q-ITAGS’ performance is highly consistent while
performs a uniform-sampling baseline in terms of sample the performance of the uniform sampler vary greatly (as
efficiencyandaccuracywhenlearningtrait-qualitymapsfrom indicatedbythelargevariations).ThissuggeststhatQ-ITAGS’
real-world data. The plots show prediction root mean squared active learning module can select the most informative data
errors (RMSE) as a function of number of training samples. points consistently, leading to more efficient and effective
Solid orange curve is the average performance of the baseline learning of trait-quality maps. Given its sample efficiency, Q-
across 20 random seeds, and the shaded region denotes its ITAGS can be used in domains in which simulating experi-
maximum and minimum performance. mentsarecomputationallyexpensiveormanuallylabelingdata
is cumbersome.
in the best and worst allocation quality.
VII. CONCLUSIONSANDLIMITATIONS
C. Learning Trait Requirements for American Football We have formulated a new class of spatio-temporal task
Finally,weevaluatedouractivelearningframework’sability allocation problems for heterogeneous robot teams, involving
to efficiently learn trait-quality maps from real-world data. optimization of task performance while respecting a time
Specifically,wetestedtheproposedactivelearningframework budget.WealsodevelopedanovelalgorithmnamedQ-ITAGS
usingaNFLMaddendataset,inwhichplayersarerepresented tosolvesuchproblems.Ourapproachexplicitlymodelscoali-
withanarrayof53traits,suchastoughnessandspeed,andare tions’ performance on tasks as continuous trait-quality maps
also given an effectiveness (i.e., quality) score for a particular that relate collective capabilities and task performance, and
position [39]. The ground-truth quality score is given by a our experiments demonstrate that Q-ITAGS can learn these
weighted sum of the players’ traits and the weights used maps efficiently from real-world data. Detailed experiments
dependonthespecificpositionunderconsideration.Ouractive on an emergency response domain show that Q-ITAGS can
paGytilamitpOnoitacollAdezilamroN
SGATI-Qgenerate solutions of higher quality than a SOTA approach, [18] J.RieskampandP.E.Otto,“SSL:atheoryofhowpeoplelearntoselect
whilestillrespectingthetimebudget.Wealsoderivedaupper strategies.,” Journal of experimental psychology: General, vol. 135,
no.2,p.207,2006.
bound on the suboptimality of Q-ITAGS and demonstrated
[19] G. Neville, S. Chernova, and H. Ravichandar, “D-ITAGS: a dynamic
that it holds in practice. Despite all its benefits, Q-ITAGS interleaved approach to resilient task allocation, scheduling, and mo-
has some limitations. For instance, its improved performance tion planning,” IEEE Robotics and Automation Letters, vol. 8, no. 2,
pp.1037–1044,2023.
over baseline approaches in terms of allocation quality and
[20] B. P. Gerkey and M. J. Mataric´, “A formal analysis and taxonomy of
makespan comes at the cost of slight increase in computation taskallocationinmulti-robotsystems,”InternationalJournalofRobotics
time (see Fig. 6). Further, Q-ITAGS doesn’t exploit the fact Research,vol.23,no.9,2004.
[21] E.Nunes,M.Manner,H.Mitiche,andM.Gini,“Ataxonomyfortask
thatqualityfunctionsarelikelytobesubmodularsinceadding
allocation problems with temporal and ordering constraints,” Robotics
morecapabilitiesisboundtohavediminishingreturnsbeyond andAutonomousSystems,vol.90,2017.
a point [40]. Improving computational efficiency and consid- [22] F.Quinton,C.Grand,andC.Lesire,“Marketapproachestothemulti-
robottaskallocationproblem:asurvey,”JournalofIntelligent&Robotic
ering submodular optimization are subjects of future work.
Systems,vol.107,no.2,p.29,2023.
[23] G. A. Korsah, B. Kannan, B. Browning, A. Stentz, and M. B. Dias,
REFERENCES
“xBots: An approach to generating and executing optimal multi-robot
plans with cross-schedule dependencies,” in International Conference
[1] J. J. Rolda´n, P. Garcia-Aunon, M. Garzo´n, J. de Leo´n, J. del Cerro, onRoboticsandAutomation,pp.115–122,IEEE,2012.
and A. Barrientos, “Heterogeneous multi-robot system for mapping [24] J.Guerrero,G.Oliver,andO.Valero,“Multi-robotcoalitionsformation
environmentalvariablesofgreenhouses,”Sensors,vol.16,no.7,2016. withdeadlines:Complexityanalysisandsolutions,”PLoSONE,vol.12,
[2] C. J. McCook and J. M. Esposito, “Flocking for heterogeneous robot no.1,2017.
swarms: A military convoy scenario,” in Proceedings of the Annual [25] H. Chakraa, F. Gue´rin, E. Leclercq, and D. Lefebvre, “Optimization
SoutheasternSymposiumonSystemTheory,pp.26–31,2007. techniquesformulti-robottaskallocationproblems:Reviewonthestate-
[3] A. W. Stroupe, T. Huntsberger, B. Kennedy, H. Aghazarian, E. T. of-the-art,”RoboticsandAutonomousSystems,p.104492,2023.
Baumgartner,A.Ganino,M.Garrett,A.Okon,M.Robinson,andJ.A. [26] M.Koes,I.Nourbakhsh,andK.Sycara,“Constraintoptimizationcoordi-
Townsend,“Heterogeneousroboticsystemsforassemblyandservicing,” nationarchitectureforsearchandrescuerobotics,”inIEEEInternational
EuropeanSpaceAgency,ESASP,no.603,pp.625–631,2005. ConferenceonRoboticsandAutomation,2006.
[4] N. Baras and M. Dasygenis, “An algorithm for routing heterogeneous [27] G.Neville,H.Ravichandar,K.Shaw,andS.Chernova,“Approximated
vehicles in robotized warehouses,” in 5th Panhellenic Conference on dynamictraitmodelsforheterogeneousmulti-robotteams,”IEEEInter-
ElectronicsandTelecommunications,2019. nationalConferenceonIntelligentRobotsandSystems,2020.
[5] H. Ravichandar, K. Shaw, and S. Chernova, “STRATA: A unified [28] B.Fu,W.Smith,D.Rizzo,M.Castanier,M.Ghaffari,andK.Barton,
frameworkfortaskassignmentsinlargeteamsofheterogeneousagents,” “Learningtaskrequirementsandagentcapabilitiesformulti-agenttask
AutonomousAgentsandMulti-AgentSystems,vol.34,no.38,2020. allocation,”arXivpreprintarXiv:2211.03286,2022.
[6] D.Matos,P.Costa,J.Lima,andA.Valente,“MultipleMobileRobots [29] S.Singh,A.Srikanthan,V.Mallampati,andH.Ravichandar,“Concur-
SchedulingBasedonSimulatedAnnealingAlgorithm,”inInternational rent Constrained Optimization of Unknown Rewards for Multi-Robot
Conference on Optimization, Learning Algorithms and Applications, Task Allocation,” in Proceedings of Robotics: Science and Systems,
2021. (Daegu,RepublicofKorea),July2023.
[7] G. Neville, A. Messing, H. Ravichandar, S. Hutchinson, and S. Cher- [30] D.Hadfield-Menell,S.Milli,P.Abbeel,S.Russell,andA.D.Dragan,
nova, “An Interleaved Approach to Trait-Based Task Allocation and “Inverse Reward Design,” in Conference on Neural Information Pro-
Scheduling,” in International Conference on Intellifent Robots and cessingSystem,2017.
Systems(IROS),2021. [31] P. Mallozzi, R. Pardo, V. Duplessis, P. Pelliccione, and G. Schneider,
“MoVEMo:Astructuredapproachforengineeringrewardfunctions,”in
[8] A.Messing,G.Neville,S.Chernova,S.Hutchinson,andH.Ravichan-
Proceedings-2ndIEEEInternationalConferenceonRoboticComput-
dar,“GRSTAPS:GraphicallyRecursiveSimultaneousTaskAllocation,
ing, IRC 2018, vol. 2018-January, pp. 250–257, Institute of Electrical
Planning,andScheduling,”InternationalJournalofRoboticsResearch,
andElectronicsEngineersInc.,42018.
vol.41,no.2,pp.232–256,2022.
[32] D. S. Weld, “An Introduction to Least Commitment Planning,” AI
[9] S. Giordani, M. Lujak, and F. Martinelli, “A distributed multi-agent
magazine,vol.15,no.4,pp.27–27,1994.
production planning and scheduling framework for mobile robots,”
[33] N. Bhargava and B. Williams, “Multiagent disjunctive temporal net-
ComputersandIndustrialEngineering,vol.64,no.1,2013.
works,” in International Joint Conference on Autonomous Agents and
[10] M. Krizmancic, B. Arbanas, T. Petrovic, F. Petric, and S. Bogdan,
MultiagentSystems,vol.1,pp.458–466,2019.
“Cooperative Aerial-Ground Multi-Robot System for Automated Con-
[34] H.Kitano,S.Tadokoro,I.Noda,H.Matsubara,T.Takahashi,A.Shinjou,
structionTasks,”IEEERoboticsandAutomationLetters,vol.5,no.2,
and S. Shimada, “RoboCup rescue: search and rescue in large-scale
pp.798–805,2020.
disastersasadomainforautonomousagentsresearch,”Proceedingsof
[11] M.Gombolay,R.Wilcox,andJ.Shah,“FastSchedulingofMulti-Robot
the IEEE International Conference on Systems, Man and Cybernetics,
TeamswithTemporospatialConstraints,”2016.
vol.6,pp.739–743,1999.
[12] P. Schillinger, M. Buerger, and D. Dimarogonas, “Improving Multi-
[35] P. Bechon, M. Barbier, G. Infantes, C. Lesire, and V. Vidal, “HiPOP:
Robot Behavior Using Learning-Based Receding Horizon Task Allo- HierarchicalPartial-OrderPlanning,”STAIRS,pp.51–60,2014.
cation,”Robotics:ScienceandSystems,2018.
[36] A.MessingandS.Hutchinson,“ForwardChainingHierarchicalPartial-
[13] B.Fu,W.Smith,D.M.Rizzo,M.Castanier,M.Ghaffari,andK.Barton, Order Planning,” International Workshop on the Algorithmic Founda-
“Robusttaskschedulingforheterogeneousrobotteamsundercapability tionsofRobotics,vol.14,2020.
uncertainty,”IEEETransactionsonRobotics,vol.39,no.2,2022. [37] A. Whitbrook, Q. Meng, and P. W. Chung, “A novel distributed
[14] A. Srikanthan and H. Ravichandar, “Resource-aware adaptation of scheduling algorithm for time-critical multi-agent systems,” in IEEE
heterogeneousstrategiesforcoalitionformation,”inInternationalCon- InternationalConferenceonIntelligentRobotsandSystems,vol.2015-
ference on Autonomous Agents and Multiagent Systems (AAMAS), Decem,pp.6451–6458,2015.
pp.1732–1734,2022. [38] W. Zhao, Q. Meng, and P. W. Chung, “A Heuristic Distributed Task
[15] A. Prorok, M. A. Hsieh, and V. Kumar, “The Impact of Diversity AllocationMethod(PIA),”IEEETransactionsonCybernetics,vol.46,
on Optimal Control Policies for Heterogeneous Robot Swarms,” IEEE pp.902–915,42016.
TransactionsonRobotics,2017. [39] M.Staff,“Madden19playerratings.”https://www.maddenguides.com/
[16] S.Mayya,D.S.D’Antonio,D.Saldana,andV.Kumar,“ResilientTask madden-19-player-ratings/,May2023.
AllocationinHeterogeneousMulti-RobotSystems,”IEEERoboticsand [40] J.LiuandR.K.Williams,“Submodularoptimizationforcoupledtask
AutomationLetters,vol.6,pp.1327–1334,42021. allocation and intermittent deployment problems,” IEEE Robotics and
[17] J. Rieskamp, J. R. Busemeyer, and T. Laine, “How do people learn AutomationLetters,vol.4,no.4,pp.3169–3176,2019.
to allocate resources? comparing two learning theories.,” Journal of
Experimental Psychology: Learning, Memory, and Cognition, vol. 29,
no.6,p.1066,2003.