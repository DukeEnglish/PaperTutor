The RoboDrive Challenge:
Drive Anytime Anywhere in Any Condition
Challenge&WorkshopOrganizers
LingdongKong ShaoyuanXie HanjiangHu YaruNiu WeiTsangOoi
BenoitR.Cottereau LaiXingNg YuexinMa WenweiZhang LiangPan
KaiChen ZiweiLiu
https://robodrive-24.github.io
TechnicalCommittee
WeichaoQiu WeiZhang
ChallengeParticipants
XuCao HaoLu Ying-CongChen CaixinKang XinningZhou ChengyangYing
WentaoShang XingxingWei YinpengDong BoYang ShengyinJiang ZeliangMa
DengyiJi HaiwenLi XingliangHuang YuTian GenghuaKou FanJia YingfeiLiu
TiancaiWang YingLi XiaoshuaiHao YifanYang HuiZhang MengchuanWei
YiZhou HaimeiZhao JingZhang JinkeLi XiaoHe XiaoqiangCheng BingyangZhang
LirongZhao DianleiDing FangshengLiu YixiangYan HongmingWang NanfeiYe
LunLuo YuboTian YiweiZuo ZheCao YiRen YunfanLi WenjieLiu XunWu
YifanMao MingLi JianLiu JiayangLiu ZihanQin CunxiChu JialeiXu
WenboZhao JunjunJiang XianmingLiu ZiyanWang ChiweiLi ShilongLi
ChendongYuan SongyueYang WentaoLiu PengChen BinZhou YuboWang
ChiZhang JianhangSun HaiChen XiaoYang LizhongWang DongyiFu
YongchunLin HuitongYang HaoangLi YadanLuo XianjingCheng YongXu
Abstract culminatedin15top-performingsolutions,whichintroduced
arangeofinnovativeapproachesincludingadvanceddata
Intherealmofautonomousdriving,robustperceptionun- augmentation,multi-sensorfusion,self-supervisedlearning
derout-of-distributionconditionsisparamountforthesafe forerrorcorrection,andnewalgorithmicstrategiestoen-
deploymentofvehicles. Challengessuchasadverseweather, hancesensorrobustness. Thesecontributionssignificantly
sensormalfunctions,andenvironmentalunpredictabilitycan advancedthestateoftheart,particularlyinhandlingsensor
severely impact the performance of autonomous systems. inconsistenciesandenvironmentalvariability. Participants,
The 2024 RoboDrive Challenge was crafted to propel the throughcollaborativeefforts,pushedtheboundariesofcur-
development of driving perception technologies that can renttechnologies,showcasingtheirpotentialinreal-world
withstandandadapttothesereal-worldvariabilities. Focus- scenarios. Extensiveevaluationsandanalysesprovidedin-
ingonfourpivotaltasks–BEVdetection,mapsegmentation, sightsintotheeffectivenessofthesesolutions,highlighting
semanticoccupancyprediction,andmulti-viewdepthesti- key trends and successful strategies for improving the re-
mation–thecompetitionlaiddownagauntlettoinnovate silienceofdrivingperceptionsystems. Thischallengehas
andenhancesystemresilienceagainsttypicalandatypical setanewbenchmarkinthefield,providingarichrepository
disturbances. Thisyear’schallengeconsistedoffivedistinct oftechniquesexpectedtoguidefutureresearchinthisfield.
tracksandattracted140registeredteamsfrom93institutes Allresourceshavebeenpubliclyavailableonthechallenge
across11countries,resultinginnearlyonethousandsub- websitetoencourageongoingcommunityengagementand
missions evaluated through our servers. The competition thedevelopmentofrobustdrivingperception.
1
4202
yaM
41
]VC.sc[
1v61880.5042:viXraDetection Segmentation Occupancy DepthEstimation
Camera Camera LiDAR
Corruption Failure Failure
Figure1.Challengeoverview.The2024RoboDrivechallengeaimstofacilitateandencourageinnovativesolutionsfortacklingmainstream
drivingperceptiontasksunderout-of-distribution(OoD)scenariosthatoccurintherealworld.Weareparticularlyinterestedinenhancingthe
OoDrobustnessof3Dobjectdetection,HDmapsegmentation,semanticoccupancyprediction,andmulti-viewdepthestimationalgorithms
inchallengingandunprecedentedconditions,suchascameracorruptions,camerafailures,andLiDARfailures.
1.Introduction andreacttochangesinthedrivingenvironment[64].Seman-
ticoccupancypredictionforecaststhepotentialoccupancy
Theevolutionofvision-centricautonomousdrivinghinges
around the vehicle with corresponding semantic informa-
critically on robust perception systems that interpret and
tion[74,79]. Thispredictionisvitalfordynamicdriving
respondaccuratelytocomplexanddynamicenvironments
decisions,helpingtoidentifyareasthatmightsoonbeoc-
[3,15,72]. AsshowninFig.1,centraltothesesystemsare
cupiedbyothervehiclesorpedestrians,therebyenhancing
four mainstream perception tasks: bird’s eye view (BEV)
situationalawareness[4]. Monocular,stereo,andmulti-view
detection,HDmapsegmentation,semanticoccupancypre-
depthestimationutilizedatafrommultiplesensorstogen-
diction,andmulti-viewdepthestimation[57]. Masteryof
erateaccuratedepthmapsofthesurroundingenvironment
these tasks ensures that vehicles can navigate safely and
[77, 78]. This task is essential for precise distance mea-
interactintelligentlywiththeirsurroundings[25,45,82].
surementandspatialunderstanding,especiallyincomplex
BEVdetectioncreatesacomprehensiveaerialperspective
scenarioswheresingle-viewdatamightbeinsufficient[16].
of the vehicle’s surroundings, which is crucial for under-
standingtheplacementandmovementofvariouselements Autonomousvehiclesmustoperatereliablyunderavari-
suchasvehicles,pedestrians,andstaticobstacles[48]. This etyofconditions,manyofwhichmaydeviatesignificantly
overviewaidsinrouteplanningandobstacleavoidanceby fromtheenvironmentsseenduringthetrainingoftheirper-
providingaclearpictureoftheenvironmentfromabove[25]. ceptionsystems[80]. Theseout-of-distribution(OoD)sce-
Mapsegmentationdelineatesdrivableareasfrompotential narios,suchasadverseweather,sensormalfunctions,and
hazardsbysegmentingroadelements[44]. Itplaysapivotal unpredictableenvironmentalchanges,posesignificantchal-
roleinnavigationbyclarifyingpathsandidentifyingsafe lenges[19]. Theycanseverelyimpacttheperformanceof
zones,ensuringthatthevehiclecanplanroutesefficiently perceptionsystems,leadingtodegradedsituationalaware-
2nessandpotentiallyunsafedrivingdecisions[2,29,35]. boDriveChallengehighlightthedynamicandcollaborative
Toaddressthesechallenges,the2024RoboDriveChal- natureofthefield. Thesepioneeringeffortsunderscorethe
lengewasconceived. Thiscompetitionaimstopropelthe challenge’s role in pushing the boundaries of technologi-
developmentofdrivingperceptiontechnologiescapableof calinnovationandfosteringacompetitiveyetcooperative
withstanding and adapting to real-world variabilities. By environmentforexploringnewsolutions. Thecompetition
focusingontheaforementionedtasks,thechallengeencour- notonlybroughtforwardnovelmethodologiesbutalsoset
agestheexplorationofinnovativeapproachesthatenhance the stage for future developments in autonomous driving
systemresilienceagainsttypicalandatypicaldisturbances, technologies. Thissynergyofcompetitionandinnovationis
such as camera corruptions, camera failures, and LiDAR vitalforacceleratingprogressandencouragingthepractical
failures(seeFig.1). applicationofresearchfindingsinreal-worldscenarios.
Thisyear’sRoboDriveChallenge,heldatthe41stIEEE The remainder of this paper is organized as follows.
ConferenceonRoboticsandAutomation(ICRA2024),fea- Sec.2reviewsrecentadvancementsinautonomousdriving
turedfivedistincttrackscenteredaroundthesetasks. Itat- perceptionandsummarizesrelevantchallengesandcompe-
tractedsignificantinternationalparticipation,with140teams titions. Sec. 3 details the key statistics, public resources,
from93institutionsacross11countriessubmittingnearly andtermsandconditionsofthischallenge. Sec.4presents
onethousandentries. Thecontributionsfromthischallenge detaileddiscussionsoftheinnovativesolutionsandimple-
havesignificantlyadvancedthestateoftheart,particularly mentationdetailsfromthetop-performingteamsacrossthe
inhandlingsensorinconsistenciesandenvironmentalvari- five tracks. Sec. 5 and Sec. 6 offer discussion of future
ability. Atotalof18cameracorruptionandfailurecasesand research directions and concluding remarks, respectively.
3LiDARfailurescenariosareinvolvedinthiscompetition, Finally, Sec. 7 contains acknowledgments and additional
providing a holistic evaluation for the OoD robustness of informationaboutthischallenge.
differentdrivingperceptionmodels.
Theparticipantsemployedvariousadvancedmethodolo- 2.RelatedWork
giestotacklethechallenge,including:
2.1.BEVDetection
• AdvancedDataAugmentationTechniques: Teamsimple-
mentedsophisticatedaugmentationstrategiessuchasfre- 3Dobjectdetectionisacornerstonetaskinautonomousdriv-
quencydomainmanipulationsandrealisticenvironmental ingsystems. Traditionalmethodsusepointcloudsasthein-
simulations. Thesemethodsweredesignedtotrainrobust putforextractingspatialinformation[40,85,91]. Recently,
modelscapableofhandlingunexpectedvariations,thereby thedomainhaswitnessednotableadvancementsinmonocu-
enhancingtheiradaptabilitytoreal-worldconditions. lar3Dobjectdetection,largelyattributedtopopularbench-
• Multi-Sensor Fusion: Demonstrating the power of inte- marks[3,15,72]. Thisprogresshasbeendrivenbyarange
gratedsystems,severalsolutionseffectivelymergeddata ofresearchstudies. Nevertheless,monocularsystemsface
frommultiplemodalities,includingcamerasandLiDAR. limitations due to their dependence on a single viewpoint
This integration not only enhanced detection reliability andlimiteddata,whichhamperstheircapacitytomanage
butalsomaintainedaccuracy,especiallyunderconditions complex scenarios. To overcome these limitations, exten-
wheresensorfunctionalitymightbecompromised. sivebenchmarkshavebeendeveloped,offeringsurrounding
• Self-SupervisedLearningforSensorErrorCorrection: By view data that enriches the field of multi-view 3D object
employing self-supervised learning techniques such as detectionandpropelstheevolutionofadvanceddetection
masked modeling and contrastive learning, participants methodologies[3,72]. Themulti-view3Dobjectdetection
couldreconstructandrefinedatafromcorruptedsensors, researchcanbeprimarilydividedintodensebird’seyeview
substantiallyboostingtheresilienceoftheirsystems. (BEV)-based [25, 27, 45, 48, 82] and sparse query-based
• InnovativeAlgorithmicApproaches: Throughthedevel- algorithms[49,51,53,76]. DenseBEV-basedalgorithms
opmentofnovelalgorithms,teamswereabletoachieve convertmulti-viewimagefeaturesintoadenseBEVrepre-
robustfeatureextraction,intricatesensordatafusion,and sentationusingtheLift-Splat-Shoot(LSS)module[25,45]
improved predictive accuracy. These innovations mark or directly using attention-based transformation [48]. To
significantstridesinthefieldofrobustperception. mitigateoverfittingfromLSS,BEVDet[25]recommends
• Systematic Robustness against Environmental and Sen- dataaugmentationonBEVfeatures. BEVDepth[45]intro-
sorVariabilities: Addressingawiderangeofadversities– ducesdepthestimationsupervisiontoenhanceLSSpreci-
fromadverseweatherconditionstovariationsinlighting sion. Conversely, sparse query-based algorithms such as
andsensoranomalies–participantsensuredthattheirsys- DETR3D [76] and PETR [51] utilize learnable 3D object
temsdeliveredconsistentperformanceacrossanarrayof queriesandpositionencodingtoengagewithmulti-view2D
challengingscenarios. features. Recentstudieshaveintegratedtemporalinforma-
Thecollectiveeffortsandachievementsofthisyear’sRo- tiontorefinethesemethodsfurther[24,63].
32.2.BEVMapSegmentation provide a comprehensive 360◦ view of the surroundings
[3,15], enhancingthevehicle’sabilitytoperceiveandin-
High-definitionmaps(orHDmaps)arecrucialforproviding
teractwithitsenvironmentmoreeffectively. Notableworks
detailedinformationaboutroadenvironments,whichises-
inthisfieldincludeSurroundDepth[78]andS3Depth[17],
sentialforautonomousvehicles. Traditionally,HDmapsare
whichproposeleveragingconsistencyacrossdifferentviews
manuallyannotated,thoughsomestudiesleverageSLAM
toenhancedepthpredictions.Althoughthesemodelsdemon-
algorithmstocreateHDmapsfromrepeatedLiDARsensor
stratesignificantimprovementsindepthaccuracybyintegrat-
scans[10,41,58,67,83]. Thesemethodsrequireextensive
ingfeaturesfrommultiplecameras,theirrobustnessunder
data collection, prolonged iterative processes, and costly
challengingconditionsremainsunknown[34,35]. Arobust
humanannotations. Consequently,generatinglocalmapsdi-
depth estimation system that can suppress real-world cor-
rectlyfromonboardsensorshasemergedasacost-effective
ruptionsiscrucialfornavigationandobstacleavoidancein
alternative.Moreover,thecapabilitytocreatesemanticmaps
autonomousdriving.
onlineenhancessystemredundancy. HDMapNet[44]was
thefirsttoaddresslocalsemanticmaplearning,focusingon 2.5.OutofDistributionRobustness
constructingsemanticmapsonlineusingdatafromLiDAR
sensorsandcameras. ItintroducedamethodtoderiveBEV Theresilienceofmodelsagainstunexpecteddisturbances,
featuresfromsensorinputsandpredictvectorizedmapele- i.e., out-of-distribution (OoD), is a critical area of focus
ments. BEVSegFormer[64]laterintroducedmulti-camera inmachinelearning,particularlyunderconditionsthatde-
deformableattentiontoconvertingimage-viewfeaturesinto viate from typical training environments. Several bench-
BEVrepresentationsforsemanticmapcreation. Morere- marks such as ImageNet-C [19], ObjectNet [1], and Ima-
cently,BEVerse[88]proposesamulti-tasklearningframe- geNetV2[66]havebeenestablishedtosystematicallyevalu-
worktoenhancetheperformancefurther. atetherobustnessof2Dimageclassifiersagainstadiverse
range of corruptions. For example, ImageNet-C [19] in-
2.3.SemanticOccupancyPrediction
troducescontrolledperturbationstopristineImageNet[8]
Semantic occupancy prediction aims to predict the occu- samples, includingcompressionartifactsandmotionblur,
pancyandsemanticlabelsof3Dscenes,unlikeoccupancy totestclassifierresilience. Conversely,ObjectNet[1]chal-
gridmapping[61,73],whichgeneratesprobabilisticmaps lenges models with a test set characterized by significant
fromrangemeasurementsandassumesstaticscenes. This variations in object rotation, background, and viewpoint,
taskdoesnotrequirerangesensors,makingitmoreversatile. emphasizing the importance of adaptability in real-world
SemanticSceneCompletion(SSC)closelyrelatestoseman- applications. Hendrycksetal.[21]highlightthedirectcor-
ticoccupancyprediction,inferringdense3Doccupancywith relation between a model’s ability to withstand synthetic
semanticlabels. MonoScene[4]employsaU-Netforthis corruptionsanditsperformanceinpracticalsettings. Inthe
frommonocularRGBimages. TPVFormer[26]extendsto realmof3Dperception,recentinitiativeshaveaimedtoex-
multi-camerainputbutlacksdensesupervision. Recently, tendtheseconceptstomorecomplexmodels. RoboDepth
improved pipelines are now able to generate dense occu- [31,35]establishesarobustnessbenchmarkformonocular
pancy ground truth for better training [74, 79]. Methods depthestimationundervariouscorruptions. Robo3D[29]
like VoxFormer [47] and OccDepth [59] use depth-aware aims to probe the robustness of LiDAR-based perception
techniques to further enhance the performance. Another modelsinadverseweatherconditionsandsensormalfunc-
taskthatiscloselyrelatedtoSSCisLiDARsegmentation tions. RoboBEV[80]andMetaBEVproposetounderstand
[32, 52, 54, 60]. Sharing a similar objective, the LiDAR theBEVdetectionmodel’srobustnessunderexternaldistri-
segmentationmodelsaimtosegmenttheholistic3Dscenes butions. Mostrecently,MultiCorrupt[2]combinesseveral
acquiredbytheLiDARsensor[7,28,33,36]. Itisworth 2D and 3D corruptions together to further analyze the ro-
mentioningthattheSSClabelsareoftenobtainedbyconcate- bustnessofmulti-modalperceptionmodels.
natingmulti-framesemanticLiDARpointclouds[5,22,46].
2.6.RelevantChallenges
2.4.Multi-ViewDepthEstimation
Numerouscompetitionshavesignificantlypropelledthefield
Depth estimation from multi-view inputs is an emerging ofdepthestimationandrobustvisualperception.TheRobust
areaofresearchthataddressesthelimitationsofmonocular VisionChallenge(RVC)[87]exploredabroadspectrumof
systems by synthesizing information across multiple sen- sceneunderstandingtasksincludingreconstruction,optical
sory inputs [39, 84]. This approach is vital for achieving flow,semanticsegmentation,anddepthprediction,pushing
precisedepthmeasurementsinenvironmentswheresingle- theboundariesofvisualsystems’capabilities. TheDense
viewpointdataisinsufficienttomakereliablenavigational Depth for Autonomous Driving (DDAD) Challenge [13]
decisions [6]. For example, unlike traditional monocular specializedindepthestimationunderdiverseurbancondi-
depth estimation, modern in-vehicle sensing systems can tions,crucialforautonomousnavigation. TheSeasonDepth
4DepthPredictionChallenge[23]adapteddepthmodelsto • Track4: RobustDepthEstimation
variouslightingandseasonalconditions,whiletheMonocu- • Track5: RobustMulti-ModalBEVDetection
larDepthEstimationChallenge(MDEC)[69,70]focused
3.3.ChallengePhases
on depth perception in complex natural landscapes. The
ArgoverseStereoCompetition[37]targetedreal-timestereo Wehostedtwophasesforthiscompetition,whichare:
depth estimation for driving applications, and the NTIRE
• Phase1:preliminarydevelopment,fromJanuarytoMarch.
2023 Challenge [65] tackled depth estimation from non-
• Phase2: Finaldesignsandsolutions,fromApriltoMay.
Lambertiansurfaces,dealingwithspecularandtransparent
materials. The RoboDepth Challenge [30] introduced ro- 3.4.Datasets
bustOoDdepthestimationtocombatreal-worldcorruptions,
Toensurearigorousevaluationofparticipatingmodels,the
includingadverseweather, sensor failures, andnoisecon-
challengeutilizeddatasetsspecificallydesignedtosimulate
tamination. Our competition shares a similar spirit with
awiderangeofreal-worldperturbationsandsensormalfunc-
RoboDepth[30]andfurtherextendssuchendeavorstothe
tions. Thesedatasetscomprisesyntheticandreal-worlddata
mainstream driving perception tasks, i.e., BEV detection,
thathavebeenmeticulouslyalteredtoincludecommondis-
HDmapsegmentation,semanticoccupancyprediction,and
turbancessuchasadverseweather,sensornoise,andlighting
multi-viewdepthestimation.Wehopethiscompetitioncould
variations. Thisapproachallowsustothoroughlyassessthe
facilitateandencouragefutureresearchondesigningmore
adaptabilityandrobustnessofeachsolutioninconditions
robustdrivingperceptiontechniques.
thatmimicreal-worldunpredictability.
3.RoboDriveChallenge2024
3.4.1 CameraCorruptions
Ourcompetitionpresentsanopportunitytotestthelimits
ofdrivingtechnologiesunderout-of-distributionconditions. Atotalof18corruptiontypesareusedinthiscompetition,
Thisyear,thechallengeemphasizestheresilienceandadapt- including brightness, low-light, fog, frost, snow, contrast,
abilityofdrivingperceptionsystemsacrossvariousunpre- defocus blur, glass blur, motion blur, zoom blur, elastic
dictablescenarios. Thesectionsbelowoutlinethespecifics transform,quantization,Gaussiannoise,impulsenoise,shot
ofdatapreparation,evaluationmetrics,baselinemodels,and noise, ISO noise, pixelate, and JPEG compression. The
highlightthenotableoutcomesofthecompetition. rationaleofthesecorruptionsiselaboratedasfollows.
• BrightnessandLow-LightImages. Variationsinlight-
3.1.Overview
nessordarknesscandramaticallyaffectimagevisibility
The2024RoboDriveChallenge,heldinconjunctionwiththe and depth quality. Brightness levels are influenced by
41stIEEEConferenceonRoboticsandAutomation(ICRA), lightingintensityandobjectreflectivity,whilelow-light
representsasignificantstridetowardadvancingautonomous imagesoftensufferfromlowvisibilityandincreasednoise
driving technologies under challenging conditions. This duetounderexposureordarkconditions.
annualcompetitionisdesignedtotestandenhancethero- • AtmosphericConditions. Fogandfrostintroducescatter-
bustnessofautonomousdrivingsystemsagainstaspectrum ingandcoatingeffectsthatblurimagesanddistortdetails,
of real-world challenges such as adverse weather, sensor crucialforaccuratedepthestimation. Fogreducescontrast
malfunctions,andenvironmentalunpredictability,whichare andclaritybyscatteringlight,whereasfrostformsiceon
criticalforthesafedeploymentofautonomousvehicles.This lenses,leadingtodistortedimages.
year’schallengeemphasizedinnovationindrivingpercep- • Precipitation. Snowcanobscuredetailsandaltertheper-
tion across four essential tasks: BEV detection, map seg- ceiveddepthbycoveringsceneswithalayerofsnowflakes,
mentation,semanticoccupancyprediction,andmulti-view leadingtoerrorsinobjectdetectionanddepthestimation.
depthestimation. Thesetasksarepivotalindevelopingsys- • ContrastandFocusIssues.Contrastdefinestheclarityof
temsthatcanperceiveandreacttodynamicandpotentially animagebasedontheluminancedifference,whichcanbe
hazardousenvironmentsaccurately. Thecompetitionstruc- severelyaffectedundervaryinglightconditions. Defocus
turedfivetrackstoaddressthesetasks,attractingparticipa- and glass blur occur when images are out of focus or
tionfrom140teamsrepresenting93internationalinstitutes, shotthroughanunevenglassmedium,blurringthedetails
whichtogethermadenearlyonethousandsubmissions. neededforprecisedepthestimation.
• MotionandZoomBlur. Thesecorruptionsarisewhen
3.2.ChallengeTracks
thereisrelativemotionbetweenthecameraandthesubject
Thefollowingfivetracksareestablishedinthiscompetition: or due to rapid focal length changes, respectively, both
• Track1: RobustBEVDetection distortingthespatialorientationandclarityofthescene.
• Track2: RobustMapSegmentation • Image Quality Degradations. Elastic transformations,
• Track3: RobustOccupancyPrediction colorquantization,andcompressionartifactsfromformats
5likeJPEGaltertheoriginalfidelityofimages,whichcan NDS= 1 [5mAP+ (cid:88) (1−min(1,mTP))], (2)
misleaddepthestimationprocesses. Noisevariationssuch 10
mTP∈TP
asGaussian,impulse,shot,andISOnoiseintroduceran-
wheremTPdenotesthemeanTruePositivescore. Forade-
dominaccuraciesacrosstheimagedata,complicatingthe
taileddefinitionoftheNDSscore,pleaserefertotheofficial
taskofreliabledepthmeasurement.
nuScenesdataset[3].
• Resolution Issues. Pixelation occurs when images are
MapSegmentation. Fortherobustmapsegmentationtrack,
toolowinresolution,makingindividualpixelsvisibleand
we adopt the mean Intersection of Union, i.e., mIoU, to
degradingtheoverallimagequality.
evaluatethesegmentationmodel’sperformance. Inlinewith
thestandardprotocol,theIoUscorecanbecalculatedas:
3.4.2 LiDARFailures
TP
IoU = i , (3)
Inadditiontothe18malfunctionscenariosoncameras,we i TP i+FP i+FN i
alsoconsiderthefollowing3sensorfailurecasesthathave
whereTP ,FP ,FN aretrue-positives,false-positives,and
i i i
certainlikelihoodtooccurontheLiDARsensors.
false-negativesforclassi,respectively,onHDmaps. mIoU
• PointsDropping,whichreferstothephenomenonwhere
istheaveragevalueofIoUscoresforallsemanticclasses.
certaindatapointsthataLiDARsensorcollectsarelostor
SemanticOccupancyPrediction. SimilartotheHDmap
notrecorded. Thiscanoccurduetovariousfactorssuch
segmentationtask,weusethemIoUmetrictoevaluatethe
assensormalfunctions,adverseenvironmentalconditions,
performancesofsemanticoccupancypredictionmodelsin
orissueswithdatatransmission. Pointsdroppingleadsto
Track4.ThecalculationofIoUscoresonvoxelsisthesame
incompleteorsparsepointclouds,whichcansignificantly
asEq.(3). mIoUistheaveragevalueofIoUscoresforall
affecttheaccuracyandreliabilityof3Drepresentations.
semanticclassesinthistrack.
• AngularRangeRestriction,whichisdefinedasrestrict-
Depth Estimation. For the robust multi-view depth esti-
ingtheLiDARtocapturepointsonlywithinaspecifican-
mationtrack,weadopttheconventionalreportingofAbs
gularrangedegreerelativetotheegovehicle’scoordinate
Rel(errorrate)andδ (accuracy)formeasuringthedepth
1
system. Thisselectivesensingcanbeduetointentional
estimationperformance. Abs Relmeasurestheabsolute
configuration,aimingtofocusonthemostrelevantdata
relative difference between the ground-truth (gt) and the
directlyinfrontofthevehicle. However,thisresultsina
prediction(pred),whichcanbecalculatedasfollows:
significantreductioninsurroundingenvironmentaldata,
potentiallyoverlookingcriticalobstaclesandhazardslo- 1 (cid:88) |gt−pred|
AbsRel= . (4)
cated outside of this narrow view range, thus affecting |D| gt
pred∈D
navigationandsafetysystems.
• BeamDropping,whichoccurswhenoneormoreofthe The δ metric, on the other hand, is the depth estimation
LiDARsensor’sbeamsfailtofunctionproperly. Thisfail- accuracygiventhethreshold:
urecanbeattributedtohardwareissues,suchasdamaged
1 gt pred
emittersordetectors,orsoftwarefaultsthatmismanage δ = |{pred∈D|max( , )<1.25t}|×100%,
t |D| pred gt
beamactivation. Beamdroppingresultsingapswithinthe
(5)
collecteddata,impactingtheuniformityandcompleteness
whereδ =δ <1.25,δ =δ <1.252,δ =δ <1.253 are
ofthescannedarea. 1 2 3
thethreeconventionallyusedaccuracyscoresinpriorworks
3.5.EvaluationMetrics [11,16,77]. TheAbs Relmetricisadoptedasthemain
indicatorduringcomparisons.
In this competition, to maintain fairness in evaluating the
3.6.BaselineModels
performancesacrossdifferentteams,weadoptseveralcon-
ventionalperceptionmetrics. Themetricsusedforeachof
In this section, we provide more details on the baseline
thefivetracksaredefinedasfollows.
modelsusedineachofthefivetracksinthiscompetition.
BEV Detection. Following the convention, we adopt the
nuScenesDetectionScore,i.e.,NDSforevaluatingtheper-
3.6.1 Track1
formanceintheBEVdetectiontracks(Track1andTrack
5)ofthiscompetition. TheNDSscoreiscalculatedoverthe Thistrackchallengesparticipantstoemployadvancedlearn-
mean Average Precision (AP) and the True Positive (TP) ingalgorithmsforBEVdetection. Thetaskencompassesthe
metrics,whichcanbedefinedasfollows: identification and localization of dynamic objects such as
vehiclesandpedestrians,aswellasthehandlingofcomplex
1 (cid:88)(cid:88) environmentalelementslikeroadsignsandstaticobstacles.
mAP= AP , (1)
|C||D| c,d
Accuracyandsensitivitytodetailareparamountinthistrack.
c∈Cd∈D
6Table1.Thechallengeresults(Phase2)ofTrack1:RobustBEVDetectioninthe2024RoboDriveChallenge.Allscoresaregivenin
percentage(%).Thehigherthescores,thebettertheoverallrobustness.Thebestscoreforeachcorruptionscenarioishighlightedinbold.
TeamName
WinningTeams
3DeepVision 52.1 39.5 65.3 36.7 49.8 62.3 51.8 53.5 49.1 41.8 37.1 45.3 67.5 71.2 52.9 59.6 56.7 56.9 40.8
3Ponyville 50.2 43.1 62.7 37.5 46.6 60.9 49.2 58.4 46.5 44.7 18.8 44.8 66.7 70.6 42.4 56.0 50.8 56.8 47.5
3CyberBEV 49.0 42.1 61.4 37.0 46.3 60.5 47.0 57.1 44.9 43.5 17.1 43.8 65.9 69.1 40.9 56.1 49.4 55.3 45.1
OtherTeams
Safedrive-promax 48.1 39.2 59.8 37.4 39.8 62.5 47.6 59.0 43.9 41.4 14.3 45.4 63.3 68.5 42.3 53.3 49.5 56.2 42.2
drivingClass 47.8 39.1 60.0 28.1 48.7 56.3 44.1 52.5 46.9 38.9 34.9 44.3 62.9 63.8 50.9 51.6 52.0 56.3 33.4
BUPTMM 43.5 37.7 55.1 30.8 41.1 51.2 45.2 45.1 41.1 38.6 29.6 40.5 56.1 58.7 41.0 48.5 42.4 46.8 34.0
LDC 40.7 31.7 54.8 29.5 36.1 47.7 37.8 46.6 36.2 33.5 25.2 35.4 55.7 58.6 40.6 46.9 40.4 43.1 33.3
googa 40.1 34.5 46.5 27.5 30.8 54.4 41.3 46.3 40.1 32.5 11.5 44.6 49.3 49.0 35.7 47.0 45.5 50.0 35.4
Maodouu 37.8 27.6 47.7 28.7 34.4 46.4 34.5 37.4 36.1 32.7 25.5 30.1 51.9 56.4 36.3 44.9 38.2 41.1 31.1
Baseline
BEVFormer 22.8 28.5 34.9 21.4 10.5 28.0 15.7 28.7 21.1 19.2 6.4 35.0 26.9 20.6 12.3 24.9 25.4 30.3 21.3
• BaselineModel: BEVFormer[48]. 3.6.3 Track3
• EstimatedTrainingRequirements: Themodelrequires
This track requires participants to use advanced learning
8xNVIDIAGeForceRTX3090GPUsandtypicallycom-
algorithmstopredicttheoccupancyandcorrespondingse-
pletesitstrainingcyclewithin1to2days.
manticinformationofurbanroadsandtheirsurroundings.
• BaselinePerformance: Ourbaselinemodelachievedan
Thetaskinvolvesaccuratelyforecastingtrafficflow,pedes-
NDSof31.24%andamAPof18.82%onourrobustness
triandynamics, andstaticobstacles, whicharecrucialfor
evaluationset.
thereal-timeresponseanddecision-makingcapabilitiesof
• EvaluationServer: Allthesubmissionswereevaluated
autonomousdrivingsystems.
on our Track 1 server at the CodaLab platform, acces-
sibleathttps://codalab.lisn.upsaclay.fr/ • BaselineModel: SurroundOcc[79].
competitions/17135. • EstimatedTrainingRequirements: Themodelrequires
8xNVIDIAGeForceRTX3090GPUsandtypicallycom-
pletesitstrainingcyclewithin2to3days.
• BaselinePerformance: Ourbaselinemodelachieveda
3.6.2 Track2 mIoUof11.30%onourrobustnessevaluationset.
• Evaluation Server: The submissions were evaluated
Thistrackchallengesparticipantstouseadvancedlearning on our Track 3 server at the CodaLab platform, acces-
algorithmsforprecisemapsegmentationonhigh-resolution sibleathttps://codalab.lisn.upsaclay.fr/
BEVimages. Thetaskincludesadetailedanalysisofvari- competitions/17063.
ousurbangeographicalfeaturessuchaslanemarkings,side-
walks, andgreenspaces. Additionally, thistrackteststhe
3.6.4 Track4
participants’ ability to handle image segmentation under
varyinglighting,weatherconditions,andnoiselevels.
Thistrackchallengesparticipantstouseadvancedlearning
• BaselineModel: BEVerse[88]. algorithmsfordepthestimationfromdatacapturedbycam-
• EstimatedTrainingRequirements: Themodelrequires erasatmultipleviewpoints. Unliketraditionalmonocular
8xNVIDIAGeForceRTX3090GPUsandtypicallycom- orbinoculardepthestimation,thistaskfocusesonleverag-
pletesitstrainingcyclewithin1to2days. ingsurroundcameradatatoconstructacomprehensiveand
• BaselinePerformance: Ourbaselinemodelachieveda systematic3Denvironmentaldepthestimationmodel. The
mIoUof17.33%onourrobustnessevaluationset. taskdemandsthatalgorithmseffectivelyhandlevariationsin
• EvaluationServer: Allthesubmissionswereevaluated viewpoint,lightingchanges,andotherenvironmentalfactors
on our Track 2 server at the CodaLab platform, acces- thatimpactdepthestimation.
sibleathttps://codalab.lisn.upsaclay.fr/ • BaselineModel: SurroundDepth[78].
competitions/17062. • EstimatedTrainingRequirements: Themodelrequires
7
SDN•
ssenthgirB◦ thgiL-woL◦
goF◦
tsorF◦ wonS◦
tsartnoC◦
rulBsucofeD◦
rulBssalG◦
rulBnoitoM◦
rulBmooZ◦
mrofsnarTcitsalE◦
noitazitnauQ◦
esioNnaissuaG◦
esioNeslupmI◦
esioNtohS◦ esioNOSI◦
etalexiP◦
noisserpmoCGEPJ◦Table2.Thechallengeresults(Phase2)ofTrack2:RobustMapSegmentationinthe2024RoboDriveChallenge.Allscoresaregivenin
percentage(%).Thehigherthescores,thebettertheoverallrobustness.Thebestscoreforeachcorruptionscenarioishighlightedinbold.
TeamName
WinningTeams
3SafeDriveSSR 48.8 54.6 71.1 28.6 23.1 54.5 54.6 64.8 51.2 44.7 21.8 52.1 40.4 58.5 46.1 37.2 64.2 55.2 54.9
3CrazyFriday 34.5 42.7 40.4 29.8 25.0 41.7 17.1 42.4 34.8 33.2 22.4 48.0 31.5 33.7 23.6 26.0 38.0 45.8 45.6
3Samsung 29.8 42.4 28.7 30.4 9.1 8.6 13.8 21.6 43.1 24.8 19.6 75.2 18.7 23.5 14.3 17.7 31.4 68.2 44.5
OtherTeams
hm.unilab 28.2 29.6 30.2 25.8 11.1 21.1 7.3 23.8 30.0 21.0 17.0 45.7 14.5 40.4 36.2 27.6 47.0 45.4 33.0
hyt1407 26.4 18.6 37.6 23.5 10.4 22.8 16.8 40.1 30.2 30.1 22.8 42.9 20.2 23.2 13.5 15.2 28.8 40.3 38.6
Oliguy 26.4 18.9 36.8 23.6 10.9 23.6 16.7 39.2 30.4 29.6 22.6 42.8 19.4 23.7 13.5 15.6 28.4 40.7 38.4
YuTian1995 25.9 33.1 34.6 30.2 10.8 19.7 14.6 39.2 31.2 23.8 19.6 45.8 19.1 19.1 13.4 11.8 28.6 40.5 30.9
Baseline
BEVerse 15.7 21.4 14.1 19.2 6.8 3.2 3.7 18.9 27.9 9.2 17.6 44.6 11.4 5.2 2.1 2.4 9.6 36.8 28.0
Table3.Thechallengeresults(Phase2)ofTrack3:RobustOccupancyPredictioninthe2024RoboDriveChallenge.Allscoresaregiven
inpercentage(%).Thehigherthescores,thebettertheoverallrobustness.Thebestscoreforeachcorruptionscenarioishighlightedinbold.
TeamName
WinningTeams
3ViewFormer 22.3 21.4 29.3 16.8 15.8 31.0 17.9 27.7 20.7 18.9 6.6 27.3 20.5 25.4 19.8 22.5 25.8 30.0 19.8
3APECBlue 10.3 13.3 12.2 8.7 9.7 7.8 5.2 13.0 14.0 4.3 7.7 15.3 18.2 8.8 9.2 10.8 11.7 15.3 10.8
3hm.unilab 8.9 13.1 12.9 7.9 3.1 7.8 4.5 11.8 7.1 4.3 4.9 14.6 6.6 9.0 7.9 10.0 10.9 15.0 9.5
OtherTeams
LVGroupHFUT 8.8 13.7 13.0 6.9 3.2 6.6 4.0 10.9 7.2 4.4 4.9 14.3 6.6 9.0 8.3 10.2 11.2 14.9 9.2
Baseline
SurroundOcc 8.7 12.1 13.1 7.8 3.7 7.5 4.4 11.3 6.9 4.3 4.9 14.3 9.5 8.1 7.0 9.2 9.8 14.9 9.7
8xNVIDIAGeForceRTX3090GPUsandtypicallycom- robustnessandefficiencyoftheiralgorithms.
pletesitstrainingcyclewithin2to3days. • BaselineModel: BEVFusion[56].
• BaselinePerformance: Ourbaselinemodelachievedan • EstimatedTrainingRequirements: Themodelrequires
AbsRelof0.348,anRMSEof7.102,andana1scoreof 8xNVIDIAGeForceRTX3090GPUsandtypicallycom-
62.3%onourrobustnessevaluationset. pletesitstrainingcyclewithin1to2days.
• Evaluation Server: The submissions were evaluated • BaselinePerformance: Ourbaselinemodelachievedan
on our Track 4 server at the CodaLab platform, acces- NDSof42.86%andamAPof24.50%onourrobustness
sibleathttps://codalab.lisn.upsaclay.fr/ evaluationset.
competitions/17226. • Evaluation Server: The submissions were evaluated
on our Track 5 server at the CodaLab platform, acces-
3.6.5 Track5 sibleathttps://codalab.lisn.upsaclay.fr/
competitions/17137.
This track requires participants to use advanced learning
algorithmstofusedatafromdifferentsensors,suchascam-
3.7.ChallengeResults
eras and LiDAR, to enhance the accuracy of Bird’s Eye
View (BEV) 3D object detection. This track encourages Thiscompetitionshowcasedaremarkableadvancementin
participantstodemonstratetheirinnovativecapabilitiesin therobustnessandeffectivenessofperceptionmodels,partic-
multi-modaldatafusionandprocessing,whileensuringthe ularlywhenbenchmarkedagainstpre-establishedbaselines.
8
UoIm•
UoIm•
ssenthgirB◦
ssenthgirB◦
thgiL-woL◦
thgiL-woL◦
goF◦
goF◦
tsorF◦
tsorF◦
wonS◦
wonS◦
tsartnoC◦
tsartnoC◦
rulBsucofeD◦
rulBsucofeD◦
rulBssalG◦
rulBssalG◦
rulBnoitoM◦
rulBnoitoM◦
rulBmooZ◦
rulBmooZ◦
mrofsnarTcitsalE◦
mrofsnarTcitsalE◦
noitazitnauQ◦
noitazitnauQ◦
esioNnaissuaG◦
esioNnaissuaG◦
esioNeslupmI◦
esioNeslupmI◦
esioNtohS◦
esioNtohS◦
esioNOSI◦
esioNOSI◦
etalexiP◦
etalexiP◦
noisserpmoCGEPJ◦
noisserpmoCGEPJ◦Table4.Thechallengeresults(Phase2)ofTrack4:RobustDepthEstimationinthe2024RoboDriveChallenge.Allscoresaregivenin
percentage(%).Thelowerthescores,thebettertheoverallrobustness.Thebestscoreforeachcorruptionscenarioishighlightedinbold.
TeamName
WinningTeams
3HIT-AIIA 18.7 11.7 12.5 34.3 24.4 16.9 15.2 15.6 16.4 29.6 23.7 20.8 19.3 16.2 19.5 12.9 18.9 14.2 14.9
3BUAA-Trans 20.5 10.5 17.6 33.7 26.8 20.0 25.3 18.8 21.4 26.1 22.7 21.5 18.4 15.3 21.5 13.6 20.1 17.3 17.6
3CUSTZS 26.4 16.4 25.7 37.9 30.1 20.4 23.8 23.1 25.6 36.4 36.1 27.5 24.1 26.1 26.6 18.6 33.6 21.5 22.6
OtherTeams
Highway 29.2 16.8 20.9 43.5 32.9 29.7 32.1 24.9 25.3 38.6 33.4 28.4 30.0 35.7 30.4 22.0 39.8 18.8 21.6
Baseline
SurroundDepth 30.4 16.8 21.0 46.4 33.0 29.7 32.1 25.5 25.3 45.1 43.0 28.4 31.8 35.7 31.0 22.0 39.8 18.8 21.6
Table5.Thechallengeresults(Phase2)ofTrack5:RobustMulti- [79]). TheresultsfromTab.4highlightTeamHIT-AIIA’s
Modal BEV Detection in the 2024 RoboDrive Challenge. All leadingperformanceindepthestimationunderadversecon-
scoresaregiveninpercentage(%).TheNDSandmAPscoresare
ditions, as observed by their low error rates in scenarios
thehigherthebetter,whiletheotherscoresarethelowerthebetter.
likelow-lightandfrost,whichexhibitedanotableimprove-
Thebestscoreforeachcorruptionscenarioishighlightedinbold.
mentoverthebaselineSurroundDepth[78]. InTab.5,Team
Safedriveexcelledintherobustmulti-modalBEVdetection
track,achievingthehighestNDSandmAPscores(10.6NDS
and 17.9 mAP higher than the baseline BEVFusion [56]).
Team
Thisunderscorestheireffectiveintegrationofmulti-modal
WinningTeams datatohandlediverseenvironmentalchallenges. Thesefind-
3safedrive 49.7 39.5 54.0 26.5 39.6 59.4 21.1
ings demonstrate considerable progress in enhancing the
3Ponyville 48.2 36.4 56.7 27.2 38.4 56.2 21.4
robustnessofmodelsagainstabroadspectrumofreal-world
3HITSZ 46.6 32.7 41.4 28.1 60.0 45.8 22.6
corruptions.
OtherTeams
UMIC 42.8 26.4 51.6 27.6 29.4 72.3 23.1 Discussions & Analyses. Throughout the solutions ana-
UESTC 41.7 24.8 43.5 29.3 63.6 47.8 23.0 lyzed,severaltechniqueshaveemergedasparticularlyeffec-
Baseline tiveinaugmentingmodel’srobustnessunderOoDscenarios:
BEVFusion 39.1 21.6 44.0 30.0 53.0 64.6 25.1 • Data Augmentation. Data augmentation represents the
most prevalent strategy employed across all teams. No-
tably, powerful augmentation techniques such as Aug-
Analysis of the solutions submitted revealed several inno- mix [20], AugFFT, and DeepAug, among others, have
vativestrategiesthatsignificantlycontributedtoenhancing been widely implemented. It is important to note that
modelrobustnessandperformance. toensurefaircomparions,theuseofcorruptionssimilar
Leaderboards. Weobserveasubstantialenhancementin tothoseemployedingeneratingthedatasetisexplicitly
the robustness of the different driving perception models prohibitedasameansofdataaugmentationduringmodel
whencomparedtotheestablishedbaselines. Asshownin training. Nevertheless,theintegrationofrobustandvaried
Tab. 1, the performance of the baseline BEVFormer [48] dataaugmentationmethodscontinuestobeaneffective
hasbeenlargelyimprovedfrom22.8NDSto52.1NDSby approachforenhancingmodelresilience.
TeamDeepVision. Thiscorrespondstoa56%relativeim- • TemporalFusion. Inadditiontodataaugmentation,lever-
provement. The other two winning teams, i.e., Ponyville agingtemporalfusionhasalsobecomeacommonpractice
Autonauts Ltd. and CyberBEV, achieved 50.2 NDS and amongtheteams. TeamSafeDriveSSRimplementedboth
49.0NDS,respectively. AsshowninTab.2andTab.3,sim- temporalfusionandtemporal-spatialconsistencychecks,
ilartrendsalsooccurinTrack2andTrack3,whereTeam achievingthehighestresultsintherobustmapsegmenta-
SafeDrive-SSRandTeamViewFormerhaveexhibitedclear tiontrackandsignificantlysurpassingthesecond-ranked
advantagesoverthebaselinemodelsandotherparticipants team. Furthermore, by utilizing temporal fusion, Team
intheleaderboards(witha33.1%mIoUanda13.6%mIoU ViewFormerenhancedtheirmodelwithstreamingtempo-
higher than the baseline BEVerse [88] and SurroundOcc ralattention,whichcontributedtotheirfirst-placevictory
9
SDN•
leRsbA•
ssenthgirB◦
PAm•
thgiL-woL◦
ETAm◦
goF◦
ESAm◦
tsorF◦
EOAm◦
wonS◦
EVAm◦
tsartnoC◦
EAAm◦
rulBsucofeD◦
rulBssalG◦
rulBnoitoM◦
rulBmooZ◦
mrofsnarTcitsalE◦
noitazitnauQ◦
esioNnaissuaG◦
esioNeslupmI◦
esioNtohS◦ esioNOSI◦
etalexiP◦
noisserpmoCGEPJ◦intherobustoccupancypredictiontrack. Thisapproach canariseduetotherandomnessinaugmentationsettings
underscorestheefficacyofincorporatingtemporaldynam- acrossadjacentframes.
ics into model architectures to improve robustness and 2ImplementationDetails:
performanceinscenarioswheredataevolvesovertime. TheTSMA-BEVframeworkintegratesanovelviewtrans-
• RobustBackbone. Empoweringthemodelwithastrong former to convert 2D image features into a 3D space, uti-
backbone architecture is another strategy implemented lizing estimated depth from multi-camera inputs to build
bytop-rankedsolutions. Theadoptionofpowerfulback- accurateBEVrepresentations. Thissetupiscrucialforalign-
bonessuchasEVAViT[12],SwinTransformer[55],and ingandconcatenatingvolumefeaturesfromcurrentandhis-
DINOv2[62]illustratesthistrend. Itisparticularlyinter- toricalframes,whichenhancesthepredictiveaccuracyand
estingtoobservehowparticipantsadapttheseadvanced robustnessofthesystemindynamicenvironments. Fig.2
visionbackbonestospecificdrivingperceptiontasks. The provides an overview of the view transformation process.
integrationofsuchrobustarchitecturesnotonlyenhances TheAugFFTmoduleappliesstochasticfrequencycut-offs
themodel’scapabilitytohandlecomplexvisualdatabut andamplitudescalingtointroduceaspectrumofvisualvari-
alsoimprovesitsgeneralizationacrossdifferentenviron- ationsthatsimulatepotentialreal-worlddisturbances. This
mentalconditionsandscenarios.Thisadaptationispivotal aspectofthemethodologyisparticularlyeffectiveincreat-
foradvancingthestate-of-the-artindrivingperception. ingatrainingdatasetthatrobustlypreparesthemodelfor
Theseinsightsdemonstratesubstantialprogressinenhancing unforeseenenvironmentalchanges. TheSeqMixAugstrat-
the robustness of models against a wide spectrum of real- egy maintains consistent augmentation parameters across
worldconditionsandcorruptions,settinganewbenchmark sequences,ensuringthatthetemporaldatausedbythemodel
inthefieldofrobustautonomousdrivingperception. doesnotintroducediscontinuities,thusaidingintheaccu-
ratepredictionofobjecttrajectoriesandmovements. The
4.ChallengeSolutions trainingregimeniscarefullystructured,startingwithshort-
termfusiontoensurequickconvergenceandfollowedbythe
Inthissection,wesummarizethekeycomponentsofeach
integrationoflong-termdata[63]andusingmixedprecision
ofthefifteensolutionsthatcontributedtothischallenge.
tooptimizecomputationalefficiencyanddepthoftraining.
4.1.Track1: RobustBEVDetection Fig.3highlightsthepracticaleffectsoftheiraugmentation
techniqueswithaugmentedimagesamples.
Thissectionintroducesthekeyinnovationsandimplementa-
tiondetailsofthethreewinningsolutionsinTrack1.
4.1.2 TeamPonyvilleAutonautsLtd.
4.1.1 TeamDeepVision
ThisteamdevelopedaMulti-ViewEnhancer(MVE)method
ThisteamhascraftedtheTSMA-BEV(TemporalSequence tosubstantiallyimprovetherobustnessof3Dobjectdetec-
MixAugmentation-Bird’sEyeView)frameworktosignifi- tion across multiple camera perspectives. Their approach
cantlybolstertherobustnessofcamera-only3Dobjectde- buildsupontheRayDN[50]architecturebutintroducessig-
tectionsystems. Theirinnovativeapproachfocusesonover- nificantenhancementswiththeintegrationofImageNet[66]
comingchallengespresentedbyreal-worldcorruptionsand pre-trainedtheEVAViT-Largebackbone[12].Thismodified
sensorinconsistenciesthroughtheintegrationoftwonovel backboneensuresdeeperandmorerobustfeatureextraction
strategies: AugmentationFastFourierTransform(AugFFT) capabilities. Themodelisfurtherenhancedthroughastrate-
andSequentialMixAugmentation(SeqMixAug). giccombinationofAugmix[20]andDeepAugdataaugmen-
KeyInnovations: tationtechniques,meticulouslytailoredtoavoidoverlapping
• AugFFT: This technique involves manipulating the fre- withthecorruptionsencounteredinthechallenge’stestsets.
quency domain of images through stochastic frequency  KeyInnovations:
cut-offs and amplitude scaling. This process generates • EVAViT-LargeBackbone: Theadoptionofasophisti-
adiversearrayoftrainingimages,eachreflectingdiffer- catedTransformer-basedmodelthathasbeenextensively
entpotentialenvironmentaldisturbances. Theaimisto trained on ImageNet [8] for superior feature extraction.
enhancethemodel’sgeneralizationabilityacrossawide Thisbackboneisintegralforenhancingtheprecisionofob-
range of conditions, making it robust against variations jectdetectionfrom2Dimagesconvertedintoa3Dspace.
thatithasnotbeenexplicitlytrainedon. • AdvancedDataAugmentation: ImplementationofAug-
• SeqMixAug: This method extends beyond traditional mix and DeepAug techniques, which are engineered to
single-frame augmentation by ensuring consistency in significantlybolsterthemodel’sresilienceagainstvariable
augmentation across temporal sequences. It is particu- environmentalconditions. Thesemethodsintroducereal-
larlyvitalformodelsthatutilizetemporaldata,helpingto isticvariationsthatpreparethemodeltoperformreliably
maintainlearningstabilityandpreventdiscrepanciesthat underdiverseandunforeseenoperationalconditions.
10Figure2.TheTSMA-BEVframework:Multi-viewimagesundergosequence-consistentaugmentations,areprocessedthroughanimage
encoderfor2Dfeatureextraction,transformedinto3Dspacebyaviewtransformer,andfinally,detectionisperformedusingconcatenated
temporalfeaturesforenhancedaccuracy.
processisstructuredtoprogressivelyevolvefromclean,unal-
tereddatasetstoincreasinglycomplexscenarios,effectively
buildingamodelthatisremarkablyresilient. Theexamples
ofAugmixandDeepAugdataaugmentationareshownin
Fig.5andFig.6,highlightinghowthesetechnologiescreate
diversetrainingdataaugmentation.
4.1.3 TeamCyberBEV
ThisteamdevelopedtheFocalAngle3Dframework,enhanc-
ingtherobustnessof3Dobjectdetectionthroughatwo-stage
modelthatintegratesadvancedfeatureextractionandangle
encoding techniques. Building on the StreamPETR [75]
paradigm,theirmodelleveragesthe2Dboundingboxaux-
iliary supervision provided by RepDETR3D [75], and in-
corporatesanintensivedepthpredictionnetworktoenhance
featureextraction[45]. Thiscontributestomoreaccurate3D
boundingboxregression. Fororientationestimation,they
Figure 3. Examples of AugFFT-generated images, demonstrat-
employanovelPhase-ShiftingCoder(PSC),whichencodes
ingthevarietyoffrequencydomainadjustmentsusedtosimulate
andpreparefordiverseenvironmentalconditions,enhancingthe theorientationanglewiththreephase-shiftedencodingchan-
model’sgeneralizationcapabilities. nels, which enhances the model’s robustness in capturing
subtleangularvariations.
KeyInnovations:
2ImplementationDetails: • IntensiveDepthPredictionNetwork: Aninnovativead-
The overall pipeline of MVE is illustrated in Fig. 4. It is dition that integrates precise depth information into the
designedtotackletheinherentchallengesof3Dobjectdetec- extracted 2D features, significantly improving the posi-
tionindynamicdrivingenvironmentsbyintegratinganovel tionalaccuracyof3Ddetectionboxes.
pipelineforcamera-onlydetection,asophisticatedfeature • Phase-ShiftingCoder(PSC):Arobustencodingmethod
extractionbackbone,andinnovativedataaugmentationtech- fororientationangleprediction,leveragingadditionalan-
niques. Depth-awarehardnegativesamplingisemployed gleinformationtobettercapturesubtleangularvariations
torefinethedetectioncapabilities,enhancingthemodel’s anddifferences.
adaptabilitytovariedenvironmentalconditions.Thetraining 2ImplementationDetails:
11RayDN
Augmix
EVA02-ViT
Loss function
Clean Image
DeepAug
Loss
Data Augment
Train Model
Figure4. PipelineoftheMulti-ViewEnhancer(MVE)methoddevelopedbyTeamPonyville,illustratingtheintegrationofadvanced
computationaltechniquesfromdataaugmentationthroughtomodeltrainingandobjectdetection.
ing scenarios, such as low visibility and adverse weather
conditions. ThemodelstartswiththeStreamPETRarchitec-
ture,knownforitsefficientspatiotemporalmodelingcapabil-
ities,andintroducesdeformableattentionmechanisms[92]
tooptimizefeatureextractionacrossspatialdomains. The
depthpredictionnetworkemploysadensepredictionstrat-
egythatextendsacrossthedetectionplane,allowingfora
richerandmoredetaileddepth understanding. Thissetup
is complemented by the PSC, which adds dimensionality
totheencodingoforientationangles,surpassingtraditional
sine-cosinemethodsincapturingminuteangularvariations.
The encoding technique of the PSC is depicted in Fig. 8,
Figure 5. Examples of Augmix-enhanced data [20] by Team showcasinghowangleinformationisenhanced. Theoverall
Ponyville. pipeline integrates these components into a cohesive sys-
tem,beginningwithimageinputthroughtheintensivedepth
predictionnetworkandtheYOLOX-based2Ddetectionnet-
work[14], flowingintotheFocalAngle3Ddecoderwhere
temporalandspatialfusiontakesplace.
4.2.Track2: RobustMapSegmentation
Thissectionintroducesthekeyinnovationsandimplementa-
tiondetailsofthethreewinningsolutionsinTrack2.
4.2.1 TeamSafeDriveSSR
ThisteamdevelopedtheSafeMapSSRframeworktoaddress
thechallengeofrobustmapsegmentationthroughacompre-
Figure6.ExamplesofDeepAug-enhanceddatabyTeamPonyville. hensiveapproachthatincludesadvanceddataaugmentation,
modelscaling,andeffectivepost-processingstrategies.Their
frameworkisbuiltupontheBEVerse[88]architecture,with
The FocalAngle3D framework, as illustrated in Fig. 7, is significantenhancementstailoredtothemapsegmentation
specificallytailoredtoaddressrobustdetectioninchalleng- task. Theyoptimizedthemodelforhighrobustnessinenvi-
12Figure7.TheframeworkofFocalAngle3D:Surroundingimagesareinputtotheimageencodertoextractimagefeatures,thenprocessed
throughonestagemono3Dpredictionhead,whichincludesYOLOXandtheintensivedepthnetwork.ThesecondstageistheFocalAngle3D
decoder,incorporatingtemporalandspatialfusionmodulesforenhanceddetectionaccuracy.
oftemporalensemblingandmulti-scaletest-timeaugmen-
tation to refine the map segmentation outputs, ensuring
consistencyandaccuracyevenindynamicscenes.
2ImplementationDetails:
Theframeworkintegratesseveralkeycomponentstomaxi-
mizeeffectiveness,includingfine-tuningthedepthclassifi-
cationmoduleinBEVerse[88]tobetteraccommodatemap
segmentationneedsandintroducingaperspective-viewseg-
Figure8.PhaseShiftCoder(PSC)implementation:Illustrationof
thecodedprincipleforfourdifferentanglesandthecorresponding mentationheadthatprocessesmapelementsdirectlyontothe
codedvaluesusingphase-shiftencoding. cameraplane. Thisenhancementsignificantlyimprovesthe
model’sabilitytoparseandsegmentcomplexmapfeatures
from various camera angles, as illustrated in Fig. 9. Ad-
ronmentsaffectedbyvarioussensorcorruptionssuchasfog, ditionally,arobustdataaugmentationstrategyisproposed
noise,andlowlightconditions. in preparing the model to handle unexpected or extreme
KeyInnovations: variations in input data quality. Techniques such as Pix-
• ModelEnhancementforMapSegmentation: TheBEV- elShuffle,whichrandomlyshufflesaspecifiedpercentage
ersemodelarchitecturewasmodifiedtofocussolelyon of pixels, alongside HSV modifications, Cutout [9], and
mapsegmentation,optimizingthedepthpredictionrange Dropout[71],areemployedtotrainthemodelundersimu-
and employing a perspective-view map loss to improve latedcorruptedconditions,enhancinggeneralizationacross
accuracy. diverse operational scenarios. This strategy is shown in
• DataAugmentationCombo:IntroductionofPixelShuffle Fig. 10. Post-processing techniques further leverage the
augmentation,whichrandomlyshufflesacertainpercent- temporaldimensionofinputdatatorefineandstabilizeseg-
ageofpixels,alongsideHSVaugmentation,Cutout,and mentationoutputs,combiningresultsfrommultipleframes
Dropouttoenhancemodelgeneralizationundercorrupted toensureconsistencyandaccuracyevenindynamicscenes.
conditions. Thismethodisparticularlyeffectiveforstaticmapelements,
• TemporalandSpatialPost-Processing: Implementation ensuringreliablesegmentationoutputsovertime.
13Data Augmentation Perspective-view
Inference
auxiliary supervision
HSV
seg supervision Multi-scale Temporal
PixelShuffle TTA Ensemble
depth supervision
Cutout
Dropout
View
Camera Input AugMix
Image Encoder
Transformation
Map decoder Map supervision
Figure 9. Overview of the SafeMapSSR framework. This diagram illustrates the integration of model modifications, advanced data
augmentation,andpost-processingstrategiestoenhancerobustnessinmapsegmentationtasks.
• AdvancedPost-ProcessingTechniques: Employingtech-
niqueslikepillarpooling[40]andspatio-temporalBEV
encoding, the framework refines the map segmentation
outputs, ensuring high fidelity in the representation of
dynamicandcomplexurbanenvironments.
2ImplementationDetails:
ratio=0.1 ratio=0.2
TheMultiViewRobustframeworkcapitalizesonitsrobustar-
chitecturebyincorporatingadvancedbackbonespre-trained
onextensivedatasetslikeImageNet-21K[8],whichsignif-
icantly enhance feature extraction capabilities. For each
timestamp,theframeworkefficientlyaggregatesmulti-view
featuresusingaviewtransformer,extendingcoverageand
ratio=0.3 ratio=0.4 translating these into a detailed point cloud. This point
cloudisprocessedthroughpillarpoolingtocreateprecise
Figure10.ExamplesofPixelShuffleandotheraugmentationtech-
BEV feature representations, which is essential for accu-
niques.Theseaugmentationsarecrucialforenhancingthemodel’s
rate map segmentation. Following the feature extraction,
robustnessagainstvariousimagecorruptions.
thesystemimplementstemporalalignmenttosynchronize
the BEV features from past timestamps with the current
frame. Thisalignment,facilitatedbyknownegomotions,is
4.2.2 TeamCrazyFriday
crucialformaintainingconsistencyacrossdynamicscenes.
ThisteamdevelopedtheMultiViewRobustframeworktoad- Spatio-temporalencodingfurtherrefinesthetemporalfea-
dressthechallengeofrobustmapsegmentation,leveraging tures, ensuring that the map decoders can effectively con-
multi-viewarchitectureandadvancedtemporalinformation structasemanticmap. Thedecodersemployasimpleyet
integration. TheframeworkisbasedontheBEVerse[88] effectiveMLPsetup,translatingrobustfeaturesintoprecise
architecturebutincludessignificantenhancementsforhan- map details, and optimizing the overall performance and
dling real-world corruptions, such as fog, noise, and low accuracyofthemapsegmentationundervariedconditions.
lightconditionsthattypicallydegradetheperformanceof
mapsegmentationsystems.
4.2.3 Team Samsung Research China-Advanced Re-
KeyInnovations:
searchLab
• Enhanced Backbone Integration: The team utilized
large-scalebackboneslikeSwin-L[55]andEVA-02[12] This team presented the DynamicBEV framework in the
forfeatureextraction,whicharecrucialformaintaining robustmapsegmentationtrack,aimedatenhancingthere-
robustnessundervariouscorruptionscenarios.Theseback- silienceofBEVmapsegmentationunderdiverseandchal-
bonesprovideastrongfoundationforfeaturerichnessand lengingenvironmentalconditions. Theirapproachintegrates
diversity. advancedtemporaldatafusion,arobustbackboneforfea-
• Temporal and Multi-View Fusion: By incorporating tureextraction,andstrategicdataaugmentationmethodsto
image-to-BEVtransformationandmulti-viewfeaturesin- improvesegmentationaccuracysignificantly.
tegration,theframeworkeffectivelyhandlestemporaldis-  KeyInnovations:
crepanciesandleveragesspatialcontext,enhancingmap • Temporal Information Utilization: The team imple-
segmentationaccuracy. mented a temporal fusion module that integrates data
14Camera
BEVFeature
2D
Camera-to-BEV
t -N Encoder
View Transform
Camera Feat.
Spatio-Temporal Map
Multi-view Images FusionModule Map Head
Decoder
t En2 coD der VC ia em we Tra r- at no s-B foE rmV C Io nTn fet or m rib mpu o at r ti io a on l
n
3:
Camera
Camera Feat. BEVFeature
Multi-view Images
Contribution1: Contribution2: Prediction
DataAugmentation Strong Backbone
Figure11.OverviewoftheDynamicBEVframework.Thisdiagramillustratestheintegrationofadvancedbackbone,temporalfusion,and
dataaugmentationstrategiestotacklerobustmapsegmentationchallengeseffectively.
(a) Original Image (b) Random Value 1 (c) Random Value 2 (d) Random Swap Channels
(e) Random Saturation (f) Random Hue (g) GridMask (h) All Augmentations (Ours)
Figure12. ExamplesofaugmentationmethodsusedintheDynamicBEVmethod. TechniqueslikeRandomSwappingChannelsand
GridMaskareshown,highlightingtheireffectivenessintrainingthemodeltohandleavarietyofenvironmentalconditions.
acrossmultipleframes, enhancingtherobustnessofthe moduleplaysacrucialrole,merginginformationfromsuc-
segmentationmodelbystabilizingtheoutputovertime. cessiveframestoensurethatdynamicandstaticelements
• StrongBackboneEmployment: Utilizinghigh-capacity withintheBEVareaccuratelyandconsistentlyrepresented
models like Swin Transformers [55] ensures deep and acrosstime. Thisapproachiscriticalformaintainingmap
effectivefeatureextraction,whichiscriticalformaintain- integrityandreliability. Additionally,thedataaugmentation
ingperformanceunderconditionslikepoorlightingand strategyshowninFig.12employedbytheteamintroduces
adverseweather. arangeofrealisticvariationsduringtraining,whichsignif-
• EffectiveDataAugmentationCombo: Techniquessuch icantlyenhancesthemodel’sresilienceandgeneralization
asRandomSwappingChannels,GridMask,andvariations capabilities. TechniquessuchasRandomSwappingChan-
inhueandsaturationwereappliedtotrainthemodeltobe nels,GridMask,andvariedadjustmentsinhueandsaturation
resilientagainstcorruptionandunexpectedscenarios. ensurethatthemodeliswell-preparedtohandlecommon
2ImplementationDetails: visual corruptions and unexpected scenarios encountered
TheDynamicBEVframework,asdepictedinillustratedin in real-world driving conditions. The integration of these
Fig.11,employstheSwinTransformer[55]asarobustback- advancedtechniquesallowstheDynamicBEVframework
bonewithintheBEVerse[88]architecture. Thisbackboneis todeliversuperiorperformanceinrobustmapsegmentation
instrumentalinprocessinghigh-resolutionimagesfrommul- tasks,showcasingtheireffectivenessinthechallenge.
tiplecameraviewsintoaunifiedBEVmap. Theintegration
oftemporaldatathroughtheirsophisticatedtemporalfusion
15
…
…….4.3.Track3: RobustOccupancyPrediction highlyaccurateandreliableacrossvaryingconditions.
Thissectionintroducesthekeyinnovationsandimplementa-
4.3.2 TeamAPECBlue
tiondetailsofthethreewinningsolutionsinTrack3.
TeamAPECBlueintroducedtheSurroundOccEnhanced
4.3.1 TeamViewFormer frameworkintheRobustOccupancyPredictiontrack,aiming
torefineoccupancypredictionincomplexdrivingenviron-
This team introduced the Enhanced ViewFormer frame-
ments. Theirmethodologyemphasizesrefiningthebaseline
workintheRobustOccupancyPredictiontrack,aimingto
SurroundOccmodel[79],optimizingnetworkstructuresfor
significantlyboosttherobustnessandaccuracyofoccupancy
enhancedperformance,andstrategicallyensemblingmulti-
predictionsinautonomousdrivingsystems. Leveragingso-
plemodelstobolsterrobustnessandaccuracy.
phisticatedspatiotemporalmodelingtechniques,theframe-  KeyInnovations:
workisuniquelydesignedtohandlecomplexdynamicsand
• ModelFine-tuning: Focusedfine-tuningofthebaseline
variousenvironmentalconditionsusingmulti-viewcamera
modelonspecificlossfunctionstodirectlearningmore
inputs. Thisapproachensuresthatthesystemnotonlymain-
effectivelyandimprovepredictionprecision.
tainshighaccuracybutalsoadaptstosuddenchangesinthe
• OptimizingNetworkStructure: Adjustmentstonetwork
environment,whicharecommoninreal-worldscenarios.
architecture,includingexperimentingwithdifferentback-
KeyInnovations:
bonesandvoxelsizes,tooptimizethespatialandfeature
• SpatialInteractionthroughViewAttention:Employing resolutionimpactsonmodelperformance.
view-guidedtransformers,theteam’sapproachallowsfor • AlgorithmicModelEnsembling: Integrationofdiverse
effectiveaggregationofmulti-viewimagefeaturesintoa modelstoimprovetheoverallrobustnessandaccuracyby
unified3Doccupancymap,enhancingspatialaccuracy. leveragingthestrengthsofvariousapproaches.
• Streaming Temporal Attention: The team utilizes a 2ImplementationDetails:
memorymechanismtoincorporateonlinevideodataef- TheSurroundOccEnhancedframeworkbuildsupontheSur-
ficientlyfortemporalinteraction,reducingtrainingtime roundOccarchitecture,incorporatingadvancedbackbones
whilemaintainingconsistencyacrosstemporaldata. likeVoVNet-99[42]andResNet-101[18]tocomparetheir
• ReverseVideoPlaybackMechanism: Asanofflineex- efficacyinfeatureextractionandtheirimpactonmodelper-
tension,thisinnovativeapproachusesfutureframedata formance. Theteamexploreddifferentvoxelsizestounder-
toenhancetheaccuracyofpredictionsbyplayingvideo standtheirinfluenceonthemIoUscores,whichhelpedintun-
sequencesbackward,effectivelyleveragingadditionaltem- ingthemodel’sspatialresolutionforbetteraccuracy. Their
poralinformation. fine-tuning strategy focused on minimizing loss from the
2ImplementationDetails:
finallayer,whichprovedessentialindirectingthemodel’s
TheEnhancedViewFormerframeworkutilizesatransformer- learningtowardmorerelevantfeaturesforoccupancypre-
basedarchitecturedesignedforrobustmulti-view3Doccu- diction. Thegeneralframeworkandapproachtointegrating
pancy perception [48, 51, 76]. It processes features from theseelementsareshowninFig.15. Additionally,theteam
multiple camera views through the attention mechanism, employedacomprehensiveensemblingstrategythatcom-
seamlessly integrating them into a coherent 3D space, ef- binedmodelswithvariedbackbonesandtrainingconfigura-
fectivelyenhancingspatialaccuracyaspresentedinFig.13. tionstocreatearobustsystemcapableofhandlingdiverse
This system incorporates a streaming memory queue for andchallengingscenarios, resultinginanotableimprove-
temporal modeling, which dynamically captures and inte- mentinmIoUscoresacrossarangeoftestconditions.
gratesfeaturesovertime,reducinginconsistenciescaused
byenvironmentalchangesandenhancingsceneunderstand-
4.3.3 Teamhm.unilab
ing[63,75]. Furthermore,theinclusionofareversevideo
playback feature markedly improves model performance Thisteamtestedthestate-of-the-artoccupancyprediction
by using future data in a reverse sequence, allowing the method,SurroundOcc,adaptingitforourchallengetoen-
model to access additional temporal information that en- hanceitsrobustnessagainstout-of-distributiondata. They
hancesoccupancypredictionaccuracy. Thisfeature,detailed focusedonrefiningthemodel’sbackboneandintegrating
inFig.14,leveragesfuturedatatomitigatethelimitations acombinationofdiverselossfunctionstoimproveperfor-
ofreal-timeprocessingandprovidesamorecomprehensive mancesignificantlyundervariedenvironmentalconditions.
sceneanalysis. Thesespatialandtemporalprocessingtech-  KeyInnovations:
niquessignificantlyenhancetheframework’sperformance, • BackboneEnhancement: AdoptingResNet-101[18]as
contributingtoitshighaccuracyinchallengingconditions the backbone to leverage its deep network architecture,
anddemonstratingitsrobustnessinadverseenvironments. allowingformoreeffectiveandnuancedfeatureextraction
ThisintegrationensuresthatViewFormerEnhancedremains crucialforoccupancypredictiontasks.
16Initial Voxel Query
View Attention
Multi-view
Interaction
View Voxel
Backbone Coordinate Query 𝑉 !"
Multi-view
Feat. Ft
Streaming Temporal Attention
Squeeze
BEV BEV
Feat. Bt-1 Query Bt
3D Occupancy Prediction
Input Multi- BEV Unsqueeze
view Images Feat. Bt-2 Multi-frame Voxel
Interaction
Query Vt
Streaming
FIFO
Memory Queue
Figure13.OverviewoftheViewFormerEnhancedframework,showingtheintegrationofmulti-viewfeaturesthroughviewattentionandthe
streamingtemporalattentionmechanismforrobust3Doccupancyprediction.
(a) cross-entropyloss,segmentationscaleloss,andgeoscale
loss,eachcontributingtothemodel’sabilitytofine-tune
predictions more accurately across different scales and
Voxel Query geometricaldimensions.
2ImplementationDetails:Teamhm.unilab’smethodin-
volvedseveralenhancementstotheSurroundOccframework.
1 2 3 4 TheuseoftheResNet-101[18]backbonewaspivotalinpro-
cessingmulti-viewimagestoextracthigh-qualityfeatures
t -3 t -2 t -1 thatformthebasisforaccurateoccupancypredictions. Their
implementationofdiverselossfunctionsaimedtooptimize
Memory Queue themodel’sperformancebyaddressingdifferentaspectsof
thepredictiontask,therebyenhancingtherobustnessofthe
(b)
model to various real-world disturbances such as weather
changesandsensorfailures. Theteamconductedaseriesof
experimentstotestdifferentcombinationsofbackbonesand
Voxel Query lossfunctions,ultimatelyachievingthehighestmIoUscore
withResNet-101[18]andtheirinnovativelossstrategy. This
comprehensiveapproachnotonlyimprovedtherobustness
1 2 3 4 oftheoccupancypredictionmethodbutalsorankedthem
3rdinthecompetition,demonstratingtheeffectivenessof
t -3 t -2 t -1 theirmodificationsinchallengingOoDscenarios.
Memory Queue 4.4.Track4: RobustDepthEstimation
Figure14. Illustrationofthereversevideoplaybackmechanism Thissectionintroducesthekeyinnovationsandimplementa-
employedbyViewFormerEnhanced,demonstratinghowfuture tiondetailsofthethreewinningsolutionsinTrack4.
dataisutilizedtoenhanceoccupancypredictionaccuracy.
4.4.1 TeamHIT-AIIA
• Comprehensive Loss Function Strategy: Implement- TeamHIT-AIIApresentedtheDINO-SD(Dinov2-Surround
ingamulti-facetedlossfunctionapproachthatincludes Depth)modelintheRobustDepthEstimationtrack,aimedat
17
mrofsnarT
ogE
hsuPFigure15.OverviewoftheSurroundOccEnhancedframework,illustratingtheintegrationofadvancedbackbones,fine-tuningstrategies,
anddiversemodelensemblingforrobustoccupancyprediction.
enhancingthereliabilityofdepthestimationinautonomous of the output depth maps. This setup ensures that depth
driving through advanced multi-view image processing. estimationisnotonlyprecisebutalsostableacrossdiffer-
Their approach leverages a combination of robust feature entviewpoints,asdepictedinFig.16. Furtherrefiningthe
extractionandinnovativedepthpredictiontechniquestoim- modelpredictions,adepthheadmeticulouslyprocessesthe
proveaccuracyincorruptedenvironments. fusedfeaturestogeneratefinelydetaileddepthmaps,cap-
KeyInnovations: turingthesubtletiesofthesurroundingenvironment. This
• Robust Feature Extraction: Utilizing the pre-trained enhanced processing capability is critical for applications
DINOv2[62]asanencoder,theteamenhancedthecapa- such as autonomous driving, where accuracy and reliabil-
bilityofthemodeltohandleout-of-distributiondataby ityareparamount. Inadditiontoarobusttrainingregime,
extractingrobustimagefeaturesthatarelesssusceptible theteamimplementedanadvancedtestingpipelinethatin-
tocommoncorruptions. corporatesdenoisingandequalizationtechniquestofurther
• Multi-View Depth Prediction: The team developed a improvetheperformanceofthemodelunderpractical,noisy
multiview-DPT(M-DPT)decodertofuseanddecodefea- conditionstypicallyfoundinreal-worldscenarios. Thetest-
turesacrossmultipleviews,effectivelyimprovingthespa- ing pipeline, which enhances the model’s adaptability to
tialconsistencyandaccuracyofthedepthmapsproduced. real-worldconditions,isillustratedinFig.17.
• EnhancedDataAugmentation: AugMix[20],amethod
thatcombinesdifferentaugmentationstrategies,wasused
4.4.2 TeamBUAATrans
totrainthemodel,increasingitsresiliencetovariousim-
agecorruptions. This team introduced the FFASDepth (Fusing Features
2ImplementationDetails: Across Scales Depth Estimation) framework, designed to
TheDINO-SDmodelarchitectureismeticulouslydesigned enhancetherobustnessofdepthestimationmodelsinout-of-
toprocessmulti-viewimagesandproduceaccurateandcon- distributionscenarios,specificallyforautonomousdriving
sistent depth maps efficiently. Utilizing the powerful DI- applications. Theirinnovativeapproachincorporatesmulti-
NOv2asthebackbone,themodelbenefitsfromhigh-quality branchnetworkarchitecturesandadvanceddataaugmenta-
featureextraction,whichiscrucialforrobustperformance tion techniques to tackle the challenges posed by adverse
in varied and challenging environments. The innovative weatherandlightingconditions.
M-DPT decoder, which includes adjacent-view attention  KeyInnovations:
mechanisms, significantly enhances feature fusion across • Multi-branchNetworkArchitecture: TheFFASDepth
different camera views, optimizing the spatial resolution employsDINOv2[62]andResNet[18]asbackbonesfor
18(a) DINO-SD pipeline (b) M-DPT pipeline
Feature Map Feature Map
Feature Map
DINOv2 Block M-DPT (6 ×   × C i) (6 × C m ×   ×   ) (6 × C o × × )
2 +1  
’  ’
Read Resample s ADJ-Att Residual
Conv
DINOv2 Block M-DPT
Project
Residual Rescale
Conv ⨁
DINOv2 Block DPT
Concatenate
Project
DINOv2 Block DPT
Reassemble s M-Fusion
Depth Head
Figure16. TheDINO-SDpipeline. ThisfigureshowstheoverallworkflowoftheDINO-SDmodel,emphasizingtheintegrationofthe
DINOv2encoder,M-DPTdecoder,andtheadvanceddataprocessingtechniquesusedtoenhancedepthestimationaccuracy.
clean depth silog loss
DINO-SD LiDAR
map
Denoise & Equalize
AugMix augmix loss
dirty depth DINO-SD
DINO-SD
map
(a)Trainingpipeline. (b)Testingpipeline.
Figure17.TrainingandtestingpipelinesoftheDINO-SDmodel.Thetwodiagramsillustrateseveralusefulimageprocessingtechniques,
includingdenoisingandequalization,employedtorefinetheperformanceofthedepthestimationmodelinreal-worldconditions.
multi-scalefeatureextraction,significantlyimprovingthe branch strategy. This approach utilizes DINOv2 for pro-
model’sadaptabilityandaccuracyundervariedconditions. cessinghigh-levelsemanticfeatures,essentialforcapturing
• Channel-AttentionBasedFeatureFusion: Theframe- the contextual nuances of the environment, while ResNet
work utilizes a novel channel-attention mechanism that [18]extractsdetailededgefeaturescriticalforprecisedepth
enableseffectiveredistributionandfusionoffeaturesfrom boundarydelineation. Together,thesebranchescoveracom-
bothbranches,tailoringthefeatureintegrationprocessto prehensiverangeoffeaturescalesvitalfordepthestimation
enhancedepthprediction. in diverse conditions. The integration of these features is
• Semi-supervised Data Augmentations: Incorporating accomplishedthroughasophisticatedSENet-basedchannel-
techniqueslikeCutFlipandAugMix,themodelenhances attentionmechanism[38]. Thissystemdynamicallyadjusts
itsgeneralizationcapabilities,makingitrobustagainsta andmergesfeaturesbyweighingtheirrelevance,guidedby
broaderrangeofimagecorruptions. thecontextprovidedbytheinputimageembeddings. These
embeddingsserveasablueprintforchannelfusion,focus-
2ImplementationDetails: ingthemodel’sattentiononthemostpertinentfeaturesfor
TheFFASDepthframework,depictedinFig.18,enhances depth estimation and enhancing the overall precision and
theSurroundDepth[78]architecturebyimplementingadual-
19Figure18.OverviewoftheFFASDepthframework.Thisfigureshowstheintegrationofdual-branchnetworkarchitecture,channel-attention-
basedfeaturefusion,andsemi-superviseddataaugmentationstoenhancetherobustnessofdepthestimation.
reliabilityoftheoutput. Tofurtherbolsterthemodel’sro- model’s ability to process spatial contexts and complex
bustness,FFASDepthincorporatessemi-supervisedlearning texturesmoreeffectivelythanconventionalCNN-based
techniqueslikeAugMix[20],whichintroducesavarietyof approaches.
controlledimagedistortionsduringtraining. Thisprepares • Use of Test-Time Augmentation (TTA): Implement-
themodeltohandleunexpectedcorruptioneffectively. Ad- ing image restoration models at test time, such as
ditionally,theCutFlip[68]techniquechallengesthemodel’s Restormer [86], to refine input quality and thereby en-
relianceontraditionalverticalimagealignment,promoting hance the accuracy of depth predictions under diverse
amoreadaptableandcomprehensiveunderstandingofdepth conditions.
cues. Thesestrategiesensurethatthemodeldeliverscon- • RobustTrainingPipeline: Focusingontrainingrobust-
sistentandaccuratedepthpredictionsacrossdynamicand ness,theteamemployedacombinationofdepthestimation
unpredictableenvironments. models, with MonoViT emerging as the most effective,
particularlywhenenhancedwithTTAstrategies.
4.4.3 TeamCUSTZS
2ImplementationDetails:
The team’s MonoViT+TTA framework is built on a dual
ThisteampresentedtheMonoViT+TTAmodelintheRo- strategyofadvancedfeatureextractionusingtransformers
bust Depth Estimation track, specifically designed to en- andresilienceenhancementthroughtest-timeaugmentation.
hance the robustness of depth estimation in adverse con- TheMonoViTmodelservesasthebackbone,incorporating
ditions. Theirapproachintegratesinnovativearchitectural a CNN-Transformer hybrid architecture that significantly
modificationsandtestingstrategiestoimprovethemodel’s extendsthecapacityfordetailedandcontext-awarefeature
performanceacrossavarietyofchallengingscenarios. extraction. ThissetupisdepictedinFig.19. Fordepthes-
KeyInnovations: timation,theencoderutilizestheMPVIT[43]structureto
• IntegrationofTransformerArchitecture: Byincorpo- integratetherobustcapabilitiesofCNNswiththeextensive
rating the MonoViT [89] architecture, which leverages contextualunderstandingoftransformers,providingacom-
transformersfordepthestimation,theteamenhancedthe prehensivefeaturesetthatiseffectivelydecodedtopredict
20Figure19.OverviewoftheMonoViT+TTAframeworkshowingtheintegrationoftheMPVITencoderandtransformer-baseddepthdecoding
layers,highlightedbytheuseofadvancedimagerestorationtechniquesattesttimetoenhanceinputquality.
depth. The integration of Restormer at test time plays a 4.5.1 Teamsafedrive-promax
crucialroleinenhancingtheinputimages,reducingnoise,
and correcting artifacts that typically degrade the perfor- This team developed the Against Sensor Failure (ASF)
manceofdepthestimationmodelsunderadverseconditions. model,targetingrobustmulti-modalBEVdetection. This
Thetestingpipelineincorporatesvariousimagerestoration framework aims to enhance the resilience of multi-modal
techniquesbeforefeedingimagesintothedepthestimation 3Dobjectdetectionsystemsinscenarioswherecameraor
model,ensuringthatthedepthpredictionsarebasedonthe LiDARsensorsfailordegrade. Byfocusingonadvanced
clearestpossibleinputs. Thismethodhasdemonstratedcon- featurereconstructionandmulti-modalfeatureenhancement,
siderableimprovementsinmodelperformance,particularly ASF addresses critical challenges that arise during sensor
intermsofabsoluterelativeerrorandprecisionmetricson failures,ensuringreliable3Dobjectdetectionindiverseand
challengingdatasetslikeKITTI[15]. adverseenvironmentalconditions.
KeyInnovations:
• Self-SupervisedFeatureReconstruction: ASFemploys
aself-supervisedpre-trainingprocesstoreconstructimage
4.5.Track5: RobustMulti-ModalBEVDetection
featureswhencamerasfail,usingspatialcuestomaintain
detectioncapabilitieswithoutfullsensorinput.
Thissectionintroducesthekeyinnovationsandimplementa- • ImageFeatureEnhancementforLiDAR(IEL):This
tiondetailsofthethreewinningsolutionsinTrack5. moduleenhancesLiDARdatawithcorrespondingimage
21Camera Random Feature + image PE 3D Boxes
Encoder Mask Reconstruction
self-supervised pretraining
T
Reference Point queries
D
ecod
er
ran
sform F F N
er
LiDAR
+point PE
Encoder
Reference points and LiDAR points
projected to the image
Figure20.TheASFframework.Thisfiguredetailsthedual-modalityintegrationandthestrategicfeaturereconstructionandenhancement
techniquesthatenablerobust3Dobjectdetectionundersensorfailureconditions.
features to compensate for LiDAR sparsity or failures,
significantlybolsteringthesystem’srobustnessinmulti-
modaldetectiontasks.
• RobustFusionandDecodingStrategy: Leveragingad-
vancedcross-attentionmechanisms,ASFintegratesandde-
codesinformationfrombothimagingandLiDARmodali-
tiestoensurepreciseobjectdetectionevenunderpartial
sensorfailure.
Figure21. CorruptionsimulationstrategyinRobuAlign,demon-
stratingrobustnessintheabsenceofLiDARbeams.
2ImplementationDetails:
TheASFframework,asillustratedinFig.20,operatesona
foundationoftheCMT[81]architecture,adaptedtohandle
4.5.2 TeamPonyvilleAutonautsLtd.
dual-modalinputseffectively. Duringitsself-supervisedpre-
trainingphase,themodelmasksrandomsectionsofimage ThisteamdevelopedtheCrossModalTransformer(CMT)
data,whicharethenreconstructedthroughasophisticated forthemulti-modalBEVdetectionunderscenariosthatare
decoder equipped with multiple transformer layers. This critical to the perception robustness. This framework en-
approachisnotmerelyaboutreplacinglostdatabutadapting hancestheresilienceofthebaselinemulti-modal3Dobject
themodel’sresponsetopartialinformation,therebyensur- detectionsystemsundersensorfailureconditions,particu-
ingcontinuousoperationduringsensorfailures. Toaddress larlyfocusingonscenarioswherecameraorLiDARsensors
LiDARsensorshortcomings,ASFsimulatesreducedsensor may be compromised. CMT utilizes a new approach to
capability by thinning the point cloud data, which is then integratedatafrombothcameraandLiDARsensorswith-
enrichedwithstrategicallyprojectedimagefeatures. Thisis outexplicitviewtransformations,thusdirectlyoutputting
achievedbymappingreferencepointsdetectedintheCMT accurate3Dboundingboxes.
onto corresponding image data, selectively enhancing Li-  KeyInnovations:
DAR data with detailed image features. Such integration • UnifiedMulti-ModalTokenization: CMTprocessesim-
ensures that even with sparse or incomplete LiDAR data, agesandpointclouddatabyconvertingthemintoaunified
themodelcanstilldeliveraccurateandreliableobjectde- tokenformat,allowingforseamlessintegrationwithout
tection outputs. These methods collectively improve the theneedforexplicitviewtransformations.
system’sadaptabilityandfaulttolerance,demonstratedby • Self-Supervised Feature Compensation: The model
ASF’s marked performance improvements over standard uses self-supervised learning techniques to compensate
modelsundertypicalandstressedconditions. formissingorcorruptedsensordata,whichhasprovento
22Figure22.OverviewofRobuAlign,highlightingtheintegrationofimageandLiDARdataintoaunifiedBEVrepresentation.
enhancerobustnessundersensorfailures. 4.5.3 TeamHITSZrobodrive
• Position-Guided Query Generator: Inspired by ad-
vances in anchor-based detection systems, this feature Team HITSZrobodrive introduced the RobuAlign model
generates queries based on 3D anchor points projected intheRobustMulti-ModalBEVDetectiontrack. Thisap-
acrossmodalities,improvingtheaccuracyandefficiency proach is crafted to ensure effective 3D object detection
ofboundingboxpredictions. evenundersensorfailures,usingasophisticatedcorruption
simulationstrategytorobustlyprocessmulti-modaldata.
2ImplementationDetails:  KeyInnovations:
TheCMTarchitectureemploysdualbackbonestoextract • CorruptionSimulationStrategy: RobuAlignsimulates
features from ring-view camera images and LiDAR point real-worldsensorfailurestotrainthemodeltomaintain
clouds. Thesefeaturesareencodedintomulti-modaltokens highperformancedespitedatadegradation.
through innovative coordinate encoding, which optimizes • RobustMulti-ModalFusion:Themodelemploysanovel
theintegrationof2Dimagedepthwith3DLiDARdata.This architecture to harmonize image and point cloud data,
integrationoccursinaTransformerdecoderwhereposition- ensuringseamlessintegrationandrobustdetectionacross
guidedqueries,derivedfrompredefinedanchorpoints,in- varyingsensorconditions.
teractwiththetokenstoenhancedetectionaccuracybyfo- • AdaptiveDataAugmentation: Dynamicaugmentation
cusing on probable object locations. The decoder refines techniquesareutilizedtoenhancethemodel’sresilience
theseinteractionstooutputpreciseobjectclassificationsand againstunpredictablesensorfailuresandenvironmental
3D bounding box coordinates, utilizing self-attention and challenges.
cross-attention mechanisms to handle the complexities of 2ImplementationDetails:
datafusion. Thissetupiscrucialformaintaininghighper- RobuAlignutilizesanadvanceddualarchitecture,utilizing
formanceevenwhensensorsfail,asthemodelistrainedand ResNet[18]andSwinTransformer[55]forprocessingim-
testedacrossavarietyofsimulatedsensorfailurescenarios. agedataandVoxelNet[90]forLiDARdataextraction. This
ThesesimulationsensurethatCMTcanadapttopartialor setupallowsforrobustfeatureextractionfrombothsensor
completelossofsensorydata,maintainingreliableobject modalities,whichisessentialforaccurateobjectdetection
detectioninadverseconditions. Therobusttrainingregimen, inadverseconditions. Themodelsimulatessensorfailures
pairedwiththemodel’scapacitytocompensateforsensor duringpreprocessingbymaskingimagesanddownsampling
inadequacies,isdemonstratedinhowiteffectivelyhandles LiDARdata(Fig.21). Thiscorruptionsimulationtrainsthe
real-worlddrivingscenarios,significantlyimprovingitsutil- system to adapt to and compensate for real-world sensor
ityforautonomousdrivingapplicationsunderchallenging failures. Tointegratethesefeatures,RobuAlignusesamulti-
conditions. modalfusionmodulethatmergesimageandLiDARinputs
23applicationsinautonomousdriving,wheresensorreliability
iscrucial(Fig.24).
5.Discussions&FutureDirections
The2024RoboDriveChallengehasnotonlyshowcasedthe
current state-of-the-art in autonomous driving perception
technologies but also highlighted key areas for future re-
(a)NormalImage searchanddevelopment. Thischallengehasrevealedboth
the robustness and the limitations of existing approaches
underreal-worldandout-of-distributionconditions. Thein-
novations presented by participants point toward several
promising directions that could shape the future of au-
tonomousvehicletechnologies.
IntegrationofAdvancedSensorTechnologies: Acritical
insightfromthisyear’schallengeisthepotentialtoenhance
systemperformancethroughtheadeptintegrationofmulti-
(b)CorruptedImage sensor data. This is especially crucial under scenarios of
sensorimpairment. Futureresearchcoulddelvedeeperinto
sophisticatedsensorfusiontechniques,particularlyempha-
sizingtheintegrationofunderexploitedmodalitieslikeradar
and thermal imaging. Enhancing these fusion algorithms
couldsubstantiallyimprovetheoperationalreliabilityand
accuracyofautonomoussystemsinadverseenvironmental
conditions.
Advancements in Machine Learning Techniques: The
challenge demonstrated the utility of self-supervised and
semi-supervised learning approaches in correcting errors
andaugmentingfeatureswithoutextensivelabeleddatasets.
(c)NormalPointCloud (d)CorruptedPointCloud Futureendeavorscouldexpandontheuseofunsupervised
learningtechniquestofurtheralleviatetheconstraintsim-
Figure23.Theeffectsofnormalandcorruptedsensordata,which
posed by the scarcity of annotated data, potentially revo-
areusedtotraintheRobuAlignmodelforenhancedrobustness.
lutionizing the efficiency of model training processes in
autonomousdrivingapplications.
Enhancing Algorithmic Robustness: There is a pro-
intoacohesiveBEVoutput,enhancingdetectionaccuracy nouncedneedfordevelopingalgorithmsthatswiftlyadapt
acrossvariedsensorconditions(Fig.22). Theadaptivedata tosuddenenvironmentalshiftsandsensordegradation. Up-
augmentation methods applied include dynamic masking comingchallengescouldfocusoncraftingandevaluating
ofimagesandstrategicLiDARdownsampling, whichare algorithms tailored for dynamic, real-time adjustments to
essentialfortrainingthemodeltohandleunexpectedsensor fostergreateradaptabilityinautonomousdrivingsystems.
outages and data corruption (Fig. 23). In addition to the Standardization and Benchmarking: The diversity of
robustfusioncapabilities,thesystemalsoemploysanovel methodologiesandoutcomeshighlightedbythechallenge
image-to-BEVconversiontechniqueusingdepthmapsgen- underscoresthenecessityforstandardizedtestingenviron-
eratedfromtheLiDARdata. Thismethodensuresthatthe ments that more closely mirror the complexities encoun-
spatialrelationshipsandtopologicalstructuresareaccurately teredinreal-worlddrivingscenarios. Establishingrigorous
maintainedevenwhentheinputdataqualityiscompromised. benchmarksanduniformtestingprotocolswouldensurethat
Theeffectivehandlingofcorruptedsensordataisdemon- modelsarenotonlytestedundercontrolledconditionsbut
stratedthroughrigoroustestingundersimulatedconditions are also adept at navigating the unpredictable nuances of
thatmimicreal-worldchallenges,showingasignificantim- real-worldoperations.
provementinmodelperformanceandresiliencecompared EthicalandSafetyConsiderations: Asautonomoustech-
toconventionalmethods. Therobusttrainingandvalidation nologies continue to evolve, addressing the ethical impli-
strategy includes extensive testing under simulated condi- cations of their deployment in public spaces becomes im-
tionsofsensorfailures, demonstratingthemodel’sability perative. Futureresearchshouldprioritizethedevelopment
tomaintainhighaccuracyandreliability. Thisiscriticalfor ofcomprehensivesafetyprotocolsandethicalguidelinesto
24Figure24.ComparativeanalysisofRobuAlignwithotherSOTAmethodsundersensorfailurescenarios,highlightingsuperiordetection
accuracy.
governtheinteractionsbetweenautonomousvehiclesand insightsgainedfromthischallengeareexpectedtoinfluence
human-operatedvehicles. Thiswillbecrucialinmanaging ongoing research and development, guiding the future of
mixed-traffic environments safely and ethically, ensuring autonomousdrivingtowardssafer,morereliable,andmore
thattheintegrationofautonomousvehiclesenhancesoverall efficientsystems. Asthefieldcontinuestoevolve,itisclear
roadsafetyandefficiency. thatthejourneytowardfullyautonomousdrivingisasmuch
Theinsightsgainedfromthischallengeareinstrumental abouttechnologicalinnovationasitisaboutcollaboration
indirectingfutureresearchefforts,settingnewbenchmarks andcontinuouslearning. TheRoboDriveChallengestands
in the field, and fostering a competitive yet collaborative asatestamenttothepowerofcompetitiveinnovationindriv-
environmentfortechnologicalinnovation. Aswemovefor- ingprogress,settingnewstandardsforwhatcanbeachieved
ward,itisessentialtocontinuepushingtheboundariesof in the quest for autonomous mobility. By drawing on the
whatispossibleinautonomousdrivingtechnologieswhile successesandlearningsfromthisyear’schallenge, future
ensuringtheseadvancementsalignwithbroadersocietaland iterationswillundoubtedlycontinuetorefineandredefine
ethicalstandards. thecapabilitiesofautonomousdrivingtechnologies,steering
ustowardasaferandmoreconnectedworld.
6.Conclusion
Acknowledgments
The2024RoboDriveChallengehassignificantlycontributed
This work is under the programme DesCartes and is sup-
toadvancingthefieldofautonomousdrivingtechnologies,
portedbytheNationalResearchFoundation,PrimeMinis-
particularlyinthedomainofrobustperceptionunderdiverse
ter’sOffice,Singapore,underitsCampusforResearchExcel-
and challenging conditions. The competition has brought
lenceandTechnologicalEnterprise(CREATE)programme.
togethersomeofthebrightestmindsfromacrosstheglobe,
ThisworkisalsosupportedbytheMinistryofEducation,
fostering a collaborative environment that has spurred in-
Singapore,underitsMOEAcRFTier2(MOET2EP20221-
novationandpushedtheboundariesofwhatispossiblein
0012),NTUNAP,andundertheRIE2020IndustryAlign-
autonomous driving. The solutions presented during the
ment Fund – Industry Collaboration Projects (IAF-ICP)
challenge have highlighted the critical importance of ro-
FundingInitiative,aswellascashandin-kindcontribution
bustness and adaptability in autonomous vehicle systems,
fromtheindustrypartner(s).
particularlytheneedforeffectivesensorfusion,advanced
dataaugmentation,andinnovativelearningtechniques. The
257.Appendix • Affiliations:
1BeihangUniversity
Inthisappendix,wesummarizetheinformationofthechal-
2TsinghuaUniversity
lengeorganizers,technicalcommitteemembers,andchal-
3HefeiUniversityofTechnology
lengeparticipantsfromeachtrack.
7.1.Organizers Team CyberBEV
• TeamMembers:
• TeamMembers:
BoYang1,ShengyinJiang1,ZeliangMa1,DengyiJi2,and
LingdongKong1,2,3,ShaoyuanXie4,HanjiangHu5,Yaru
HaiwenLi1
Niu5,WeiTsangOoi2,BenoitR.Cottereau6,7,LaiXing
• Email:
Ng8, Yuexin Ma9, Wenwei Zhang1, Liang Pan1, Kai
bobyang677@gmail.com
Chen1,andZiweiLiu10
• Affiliations:
• Affiliations:
1BeijingUniversityofPostsandTelecommunications
1ShanghaiAILaboratory
2BeijingUniversityofTechnology
2NationalUniversityofSingapore
3CNRS@CREATE
4UniversityofCalifornia,Irvine 7.3.2 Track2: RobustMapSegmentation
5CarnegieMellonUniversity
Team SafeDrive-SSR
6IPAL,CNRSIRL2955,Singapore
• TeamMembers:
7CerCo,CNRSUMR5549,Universite´ ToulouseIII XingliangHuang1andYuTian2
8InstituteforInfocommResearch,A*STAR
• Email:
9ShanghaiTechUniversity
huangxingliang20@mails.ucas.ac.cn
10NanyangTechnologicalUniversity,Singapore
• Affiliations:
• Website: 1UniversityofChineseAcademyofSciences
https://robodrive-24.github.io 2TsinghuaUniversity
7.2.TechnicalCommittee
Team CrazyFriday
• TeamMembers:
• TeamMembers:
WeichaoQiu1andWeiZhang1
Genghua Kou1, Fan Jia2, Yingfei Liu2, Tiancai Wang2,
• Affiliations: andYingLi1
1HUAWEINoah’sArkLab
• Email:
• Website: koughua@bit.edu.cn
https://www.noahlab.com.hk
• Affiliations:
1BeijingInstituteofTechnology
7.3.Teams&Affiliations
2MegviiTechnology
7.3.1 Track1: RobustBEVDetection
Team SamsungResearchChina-AdvancedResearch
Team DeepVision
• TeamMembers:
• TeamMembers:
XiaoshuaiHao1,YifanYang1,HuiZhang1,Mengchuan
XuCao1,2,HaoLu1,2,andYing-CongChen1,2
Wei1,YiZhou1,HaimeiZhao2,andJingZhang2
• Email:
• Email:
xcao635@connect.hkust-gz.edu.cn
xshuai.hao@samsung.com
• Affiliations:
• Affiliations:
1TheHongKongUniversityofScienceandTechnology
1SamsungR&DInstituteChina–Beijing
(Guangzhou)
2TheUniversityofSydney
2TheHongKongUniversityofScienceandTechnology
7.3.3 Track3: RobustOccupancyPrediction
Team PonyvilleAutonautsLtd.
• TeamMembers: Team ViewFormer
CaixinKang1,XinningZhou2,ChengyangYing2,Wentao • TeamMembers:
Shang3,XingxingWei1,andYinpengDong2 JinkeLi1,XiaoHe1,andXiaoqiangCheng1
• Email: • Email:
timkang666@gmail.com jinke.li@uisee.com
26• Affiliations: 7.3.5 Track5: RobustMulti-ModalBEVDetection
1UISEEFoundationResearch&Development
Team safedrive-promax
• TeamMembers:
Team APECBlue HaiChen1,XiaoYang1,andLizhongWang1
• TeamMembers: • Email:
Bingyang Zhang1, Lirong Zhao2, Dianlei Ding1, Fang- chber_ahu@hotmail.com
shengLiu1,YixiangYan1,andHongmingWang1 • Affiliations:
• Email: 1TsinghuaUniversity
zhangbingyang1@foxmail.com
• Affiliations: Team PonyvilleAutonautsLtd.
1BeijingAPECBlueTechnologyCo.,Ltd
• TeamMembers:
2BeihangUniversity CaixinKang1,XinningZhou2,ChengyangYing2,Wentao
Shang3,XingxingWei1,andYinpengDong2
• Email:
Team hm.unilab
timkang666@gmail.com
• TeamMembers:
• Affiliations:
NanfeiYe1,LunLuo1,YuboTian1,YiweiZuo1,ZheCao1,
1BeihangUniversity
YiRen1,YunfanLi1,WenjieLiu1,andXunWu1
2TsinghuaUniversity
• Email:
3HefeiUniversityofTechnology
hm.unilab@gmail.com
• Affiliations:
1Haomo.ai Team HITSZrobodrive
• TeamMembers:
DongyiFu1,YongchunLin1,HuitongYang2,HaoangLi3,
7.3.4 Track4: RobustDepthEstimation YadanLuo4,XianjingCheng1,andYongXu1
• Email:
Team HIT-AIIA
23s151124@stu.hit.edu.cn
• TeamMembers:
• Affiliations:
Yifan Mao1, Ming Li1, Jian Liu1, Jiayang Liu1, Zihan 1HarbinInstituteofTechnology
Qin1, Cunxi Chu1, Jialei Xu1, Wenbo Zhao1, Junjun 2GuangdongUniversityofTechnology
Jiang1,andXianmingLiu1
3TheHongKongUniversityofScienceandTechnology
• Email:
(Guangzhou)
maoyf1105@163.com
4TheUniversityofQueensland
• Affiliations:
1HarbinInstituteofTechnology
References
[1] Andrei Barbu, David Mayo, Julian Alverio, William Luo,
Team BUAA-Trans Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and
• TeamMembers: BorisKatz. Objectnet:Alarge-scalebias-controlleddataset
ZiyanWang1,ChiweiLi1,ShilongLi1,ChendongYuan1, forpushingthelimitsofobjectrecognitionmodels.Advances
SongyueYang1,WentaoLiu1,PengChen1,andBinZhou1 inNeuralInformationProcessingSystems,32,2019. 4
• Email: [2] TillBeemelmanns,QuanZhang,andLutzEckstein. Multi-
cpeng@buaa.edu.cn corrupt:Amulti-modalrobustnessdatasetandbenchmarkof
lidar-camerafusionfor3dobjectdetection. arXivpreprint
• Affiliations:
arXiv:2402.11677,2024. 3,4
1BeihangUniversity
[3] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,
VeniceErinLiong,QiangXu,AnushKrishnan,YuPan,Gi-
Team CUSTZS ancarloBaldan,andOscarBeijbom. nuscenes:Amultimodal
datasetforautonomousdriving. InIEEE/CVFConference
• TeamMembers:
onComputerVisionandPatternRecognition,pages11621–
YuboWang1,ChiZhang1,andJianhangSun1
11631,2020. 2,3,4,6
• Email:
[4] Anh-QuanCaoandRaoulDeCharette. Monoscene:Monoc-
w1055025523@gmail.com
ular3dsemanticscenecompletion. InIEEE/CVFConference
• Affiliations:
onComputerVisionandPatternRecognition,pages3991–
1ChangchunUniversityofScienceandTechnology
4001,2022. 2,4
27[5] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Conference on Computer Vision and Pattern Recognition,
YuexinMa,YikangLi,YuenanHou,YuQiao,andWenping pages770–778,2016. 16,17,18,19,23
Wang. Clip2scene: Towardslabel-efficient3dsceneunder- [19] DanHendrycksandThomasDietterich.Benchmarkingneural
standing by clip. In IEEE/CVF Conference on Computer networkrobustnesstocommoncorruptionsandperturbations.
VisionandPatternRecognition,pages7020–7030,2023. 4 arXivpreprintarXiv:1903.12261,2019. 2,4
[6] JundaCheng,WeiYin,KaixuanWang,XiaozhiChen,Shijie [20] DanHendrycks,NormanMu,EkinDCubuk,BarretZoph,
Wang, andXinYang. Adaptivefusionofsingle-viewand JustinGilmer, andBalajiLakshminarayanan. Augmix: A
multi-view depth for autonomous driving. arXiv preprint simple data processing method to improve robustness and
arXiv:2403.07535,2024. 4 uncertainty. arXivpreprintarXiv:1912.02781,2019. 9,10,
[7] ChristopherChoy,JunYoungGwak,andSilvioSavarese. 4d 12,18,20
spatio-temporalconvnets: Minkowskiconvolutionalneural [21] DanHendrycks,StevenBasart,NormanMu,SauravKada-
networks. InIEEE/CVFConferenceonComputerVisionand vath,FrankWang,EvanDorundo,RahulDesai,TylerZhu,
PatternRecognition,pages3075–3084,2019. 4 SamyakParajuli,MikeGuo,etal. Themanyfacesofrobust-
[8] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi ness:Acriticalanalysisofout-of-distributiongeneralization.
Fei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase. InIEEE/CVFInternationalConferenceonComputerVision,
InIEEE/CVFConferenceonComputerVisionandPattern pages8340–8349,2021. 4
Recognition,pages248–255,2009. 4,10,14 [22] Fangzhou Hong, Lingdong Kong, Hui Zhou, Xinge Zhu,
[9] TerranceDeVriesandGrahamW.Taylor. Improvedregular- HongshengLi,andZiweiLiu. Unified3dand4dpanoptic
izationofconvolutionalneuralnetworkswithcutout. arXiv segmentationviadynamicshiftingnetworks. IEEETransac-
preprintarXiv:1708.04552,2017. 13 tionsonPatternAnalysisandMachineIntelligence,46(5):
[10] Renaud Dube´, Abel Gawel, Hannes Sommer, Juan Nieto, 3480–3495,2024. 4
RolandSiegwart,andCesarCadena. Anonlinemulti-robot [23] HanjiangHu,BaoquanYang,ZhijianQiao,ShiqiLiu,Ding
slamsystemfor3dlidars. InIEEE/RSJInternationalCon- Zhao,andHeshengWang.Seasondepth:Cross-seasonmonoc-
ferenceonIntelligentRobotsandSystems,pages1004–1011, ulardepthpredictiondatasetandbenchmarkundermultiple
2017. 4 environments.InInternationalConferenceonMachineLearn-
[11] David Eigen, Christian Puhrsch, and Rob Fergus. Depth ingWorkshops,2022. 5
mappredictionfromasingleimageusingamulti-scaledeep [24] JunjieHuangandGuanHuang. Bevdet4d:Exploittemporal
network. In Advances in Neural Information Processing cues in multi-camera 3d object detection. arXiv preprint
Systems,2014. 6 arXiv:2203.17054,2022. 3
[12] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu, [25] JunjieHuang,GuanHuang,ZhengZhu,YunYe,andDalong
XinggangWang,TiejunHuang,XinlongWang,andYueCao. Du. Bevdet: High-performancemulti-camera3dobjectde-
Eva: Exploringthelimitsofmaskedvisualrepresentation tectioninbird-eye-view. arXivpreprintarXiv:2112.11790,
learning at scale. In IEEE/CVF Conference on Computer 2021. 2,3
VisionandPatternRecognition,pages19358–19369,2023. [26] YuanhuiHuang,WenzhaoZheng,YunpengZhang,JieZhou,
10,14 andJiwenLu. Tri-perspectiveviewforvision-based3dse-
[13] AdrienGaidon,GregShakhnarovich,RaresAmbrus,Vitor manticoccupancyprediction. InIEEE/CVFConferenceon
Guizilini, Igor Vasiljevic, Matthew Walter, Sudeep Pillai, ComputerVisionandPatternRecognition,pages9223–9232,
and Nick Kolkin. The dense depth for autonomous driv- 2023. 4
ing(ddad)challenge. https://sites.google.com/ [27] Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin
view/mono3d-workshop,2021. 4 Gao,WeimingHu,andYu-GangJiang. Polarformer:Multi-
[14] ZhengGe, SongtaoLiu, FengWang, ZemingLi, andJian camera3dobjectdetectionwithpolartransformer. InAAAI
Sun. Yolox: Exceedingyoloseriesin2021. arXivpreprint ConferenceonArtificialIntelligence,pages1042–1050,2023.
arXiv:2107.08430,2021. 12 3
[15] AndreasGeiger,PhilipLenz,ChristophStiller,andRaquel [28] Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma,
Urtasun. Visionmeetsrobotics:Thekittidataset. TheInter- XingeZhu,YikangLi,YuenanHou,YuQiao,andZiweiLiu.
nationalJournalofRoboticsResearch,32(11):1231–1237, Rethinkingrangeviewrepresentationforlidarsegmentation.
2013. 2,3,4,21 InIEEE/CVFInternationalConferenceonComputerVision,
[16] Cle´mentGodard, OisinMacAodha, MichaelFirman, and pages228–240,2023. 4
GabrielJ.Brostow. Diggingintoself-supervisedmonocular [29] LingdongKong,YouquanLiu,XinLi,RunnanChen,Wen-
depthprediction. InIEEE/CVFInternationalConferenceon wei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei
ComputerVision,pages3828–3838,2019. 2,6 Liu. Robo3d: Towards robust and reliable 3d perception
[17] XiandaGuo,WenjieYuan,YunpengZhang,TianYang,Chen- againstcorruptions. InIEEE/CVFInternationalConference
mingZhang,ZhengZhu,andLongChen. Asimplebaseline onComputerVision,pages19994–20006,2023. 3,4
forsupervisedsurround-viewdepthestimation.arXivpreprint [30] Lingdong Kong, Yaru Niu, Shaoyuan Xie, Hanjiang Hu,
arXiv:2303.07759,2023. 4 LaiXingNg,BenoitCottereau,DingZhao,LiangjunZhang,
[18] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. HeshengWang,WeiTsangOoi,RuijieZhu,ZiyangSong,
Deepresiduallearningforimagerecognition. InIEEE/CVF Li Liu, Tianzhu Zhang, Jun Yu, Mohan Jing, Pengwei Li,
28XiaohuaQi,ChengJin,YingfengChen,JieHou,JieZhang, [42] Youngwan Lee, Joong-won Hwang, Sangrok Lee, Yuseok
ZhenKan,QiangLin,LiangPeng,MingleiLi,DiXu,Chang- Bae, andJongyoulPark. Anenergyandgpu-computation
pengYang,YuanqiYao,GangWu,JianKuai,XianmingLiu, efficient backbone network for real-time object detection.
JunjunJiang,JiamianHuang,BaojunLi,JialeChen,Shuang InIEEE/CVFConferenceonComputerVisionandPattern
Zhang,SunAo,ZhenyuLi,RunzeChen,HaiyongLuo,Fang RecognitionWorkshops,2019. 16
Zhao, and Jingze Yu. The robodepth challenge: Methods [43] YoungwanLee,JongheeKim,JeffreyWillette,andSungJu
andadvancementstowardsrobustdepthestimation. arXiv Hwang. Mpvit: Multi-path vision transformer for dense
preprintarXiv:2307.15061,2023. 5 prediction. InIEEE/CVFConferenceonComputerVision
[31] Lingdong Kong, Yaru Niu, Shaoyuan Xie, Hanjiang Hu, andPatternRecognition,pages7287–7296,2022. 20
Lai Xing Ng, Benoit R Cottereau, Ding Zhao, Liangjun [44] QiLi,YueWang,YilunWang,andHangZhao. Hdmapnet:
Zhang,HeshengWang,WeiTsangOoi,etal. Therobodepth Anonlinehdmapconstructionandevaluationframework. In
challenge:Methodsandadvancementstowardsrobustdepth InternationalConferenceonRoboticsandAutomation,pages
estimation. arXivpreprintarXiv:2307.15061,2023. 4 4628–4634,2022. 2,4
[32] Lingdong Kong, Niamul Quader, and Venice Erin Liong. [45] Yinhao Li, ZhengGe, Guanyi Yu, Jinrong Yang, Zengran
Conda: Unsuperviseddomainadaptationforlidarsegmen- Wang,YukangShi,JianjianSun,andZemingLi. Bevdepth:
tation via regularized domain concatenation. In IEEE In- Acquisitionofreliabledepthformulti-view3dobjectdetec-
ternationalConferenceonRoboticsandAutomation,pages tion. InAAAIConferenceonArtificialIntelligence, pages
9338–9345,2023. 4 1477–1485,2023. 2,3,11
[33] Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu. [46] YimingLi,SihangLi,XinhaoLiu,MoonjunGong,KenanLi,
Lasermixforsemi-supervisedlidarsemanticsegmentation. NuoChen,ZijunWang,ZhihengLi,TaoJiang,FisherYu,Yue
InIEEE/CVFConferenceonComputerVisionandPattern Wang,HangZhao,ZhidingYu,andChenFeng. Sscbench:
Recognition,pages21705–21715,2023. 4 Monocular3dsemanticscenecompletionbenchmarkinstreet
views. arXivpreprintarXiv:2306.09001,2023. 4
[34] LingdongKong, ShaoyuanXie, HanjiangHu, BenoitCot-
[47] YimingLi, ZhidingYu, ChristopherChoy, ChaoweiXiao,
tereau, Lai Xing Ng, and Wei Tsang Ooi. The ro-
JoseMAlvarez,SanjaFidler,ChenFeng,andAnimaAnand-
bodepthbenchmarkforrobustout-of-distributiondepthes-
timation under corruptions. https://github.com/ kumar. Voxformer: Sparse voxel transformer for camera-
ldkong1205/RoboDepth,2023. 4 based 3d semantic scene completion. In IEEE/CVF Con-
ferenceonComputerVisionandPatternRecognition,pages
[35] LingdongKong,ShaoyuanXie,HanjiangHu,LaiXingNg,
9087–9098,2023. 4
BenoitCottereau,andWeiTsangOoi. Robodepth: Robust
[48] ZhiqiLi,WenhaiWang,HongyangLi,EnzeXie,Chonghao
out-of-distributiondepthestimationundercorruptions. Ad-
Sima,TongLu,YuQiao,andJifengDai. Bevformer:Learn-
vancesinNeuralInformationProcessingSystems,36,2024.
ingbird’s-eye-viewrepresentationfrommulti-cameraimages
3,4
viaspatiotemporaltransformers. InEuropeanConferenceon
[36] Lingdong Kong, Xiang Xu, Jiawei Ren, Wenwei Zhang,
ComputerVision,pages1–18,2022. 2,3,7,9,16
LiangPan,KaiChen,WeiTsangOoi,andZiweiLiu. Multi-
[49] XuewuLin,TianweiLin,ZixiangPei,LichaoHuang,and
modaldata-efficient3dsceneunderstandingforautonomous
Zhizhong Su. Sparse4d: Multi-view 3d object detec-
driving. arXivpreprintarXiv:2405.05258,2024. 4
tion with sparse spatial-temporal fusion. arXiv preprint
[37] Henrik Kretzschmar, Alex Liniger, Jose M. Alvarez, Yan
arXiv:2211.10581,2022. 3
Wang,VincentCasser,FisherYu,MarcoPavone,BoLi,An-
[50] FengLiu,TengtengHuang,QianjingZhang,HaotianYao,
dreasGeiger,PeterOndruska,LiErranLi,DragomirAngelov,
ChiZhang,FangWan,QixiangYe,andYanzhaoZhou. Ray
JohnLeonard,andLucVanGool. Theargoversestereocom-
denoising: Depth-aware hard negative sampling for multi-
petition. CVPR2022.wad.vision,2022. 5
view3dobjectdetection. arXivpreprintarXiv:2402.03634,
[38] SouvikKundu,ShunlinLu,YukeZhang,JacquelineLiu,and 2024. 10
PeterABeerel. Learningtolinearizedeepneuralnetworks
[51] YingfeiLiu, TiancaiWang, XiangyuZhang, andJianSun.
for secure and efficient private inference. arXiv preprint
Petr: Positionembeddingtransformationformulti-view3d
arXiv:2301.09254,2023. 19
objectdetection. InEuropeanConferenceonComputerVi-
[39] SomnathLahiri,JingRen,andXiankeLin. Deeplearning- sion,pages531–548,2022. 3,16
basedstereopsisandmonoculardepthestimationtechniques: [52] YouquanLiu,RunnanChen,XinLi,LingdongKong,Yuchen
Areview. Vehicles,6(1):305–351,2024. 4 Yang, Zhaoyang Xia, Yeqi Bai, Xinge Zhu, Yuexin Ma,
[40] AlexHLang,SourabhVora,HolgerCaesar,LubingZhou, YikangLi, YuQiao, andYuenanHou. Uniseg: Aunified
JiongYang,andOscarBeijbom. Pointpillars:Fastencoders multi-modallidarsegmentationnetworkandtheopenpcseg
forobjectdetectionfrompointclouds. InIEEE/CVFCon- codebase. InIEEE/CVFInternationalConferenceonCom-
ferenceonComputerVisionandPatternRecognition,pages puterVision,pages21662–21673,2023. 4
12697–12705,2019. 3,14 [53] YingfeiLiu,JunjieYan,FanJia,ShuailinLi,AqiGao,Tiancai
[41] GimHeeLee,FriedrichFraundorfer,andMarcPollefeys.Ro- Wang, andXiangyuZhang. Petrv2: Aunifiedframework
bustpose-graphloop-closureswithexpectation-maximization. for3dperceptionfrommulti-cameraimages. InIEEE/CVF
InIEEE/RSJInternationalConferenceonIntelligentRobots InternationalConferenceonComputerVision,pages3262–
andSystems,pages556–563,2013. 4 3272,2023. 3
29[54] YouquanLiu,LingdongKong,JunCen,RunnanChen,Wen- agenet? InInternationalConferenceonMachineLearning,
weiZhang,LiangPan,KaiChen,andZiweiLiu.Segmentany pages5389–5400.PMLR,2019. 4,10
pointcloudsequencesbydistillingvisionfoundationmodels. [67] TixiaoShan,BrendanEnglot,DrewMeyers,WeiWang,Carlo
InAdvancesinNeuralInformationProcessingSystems,2024. Ratti,andDanielaRus. Lio-sam: Tightly-coupledlidarin-
4 ertialodometryviasmoothingandmapping. InIEEE/RSJ
[55] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng internationalconferenceonintelligentrobotsandsystems,
Zhang, StephenLin, andBainingGuo. Swintransformer: pages5135–5142,2020. 4
Hierarchicalvisiontransformerusingshiftedwindows. In [68] ShuweiShao, ZhongcaiPei, WeihaiChen, RanLi, Zhong
IEEE/CVF International Conference on Computer Vision, Liu,andZhengguoLi. Urcdc-depth: Uncertaintyrectified
pages10012–10022,2021. 10,14,15,23 cross-distillationwithcutflipformonoculardepthestimation.
[56] ZhijianLiu,HaotianTang,AlexanderAmini,XinyuYang, IEEETransactionsonMultimedia,2023. 20
HuiziMao,DanielaL.Rus,andSongHan. Bevfusion:Multi-
[69] JaimeSpencer,C.StellaQian,ChrisRussell,SimonHadfield,
taskmulti-sensorfusionwithunifiedbird’s-eyeviewrepre-
ErichGraf,WendyAdams,AndrewJ.Schofield,JamesH.El-
sentation. InIEEEInternationalConferenceonRoboticsand
der,RichardBowden,HengCong,StefanoMattoccia,Matteo
Automation,pages2774–2781,2023. 8,9
Poggi,ZeeshanKhanSuri,YangTang,FabioTosi,HaoWang,
[57] Yuexin Ma, Tai Wang, Xuyang Bai, Huitong Yang, Yue-
YouminZhang,YushengZhang,andChaoqiangZhao. The
nan Hou, Yaming Wang, YuQiao, RuigangYang, Dinesh
monoculardepthestimationchallenge. InIEEE/CVFWinter
Manocha,andXingeZhu. Vision-centricbevperception:A
ConferenceonApplicationsofComputerVisionWorkshops,
survey. arXivpreprintarXiv:2208.02797,2022. 2
pages623–632,2023. 5
[58] EllonMendes,PierrickKoch,andSimonLacroix. Icp-based
[70] JaimeSpencer,C.StellaQian,MichaelaTrescakova,Chris
pose-graphslam.InIEEEInternationalSymposiumonSafety,
Russell, Simon Hadfield, Erich Graf, Wendy Adams, An-
Security,andRescueRobotics,pages195–200,2016. 4
drew J. Schofield, James Elder, Richard Bowden, Ali An-
[59] RuihangMiao,WeizhouLiu,MingruiChen,ZhengGong,
war, Hao Chen, Xiaozhi Chen, Kai Cheng, Yuchao Dai,
Weixin Xu, Chen Hu, and Shuchang Zhou. Occdepth: A
HuynhThaiHoa,SadatHossain,JianmianHuang,Mohan
depth-awaremethodfor3dsemanticscenecompletion.arXiv
Jing,BoLi,ChaoLi,BaojunLi,ZhiwenLiu,StefanoMat-
preprintarXiv:2302.13540,2023. 4
toccia,SiegfriedMercelis,MyungwooNam,MatteoPoggi,
[60] AndresMilioto,IgnacioVizzo,JensBehley,andCyrillStach-
XiaohuaQi,JiahuiRen,YangTang,FabioTosi,LinhTrinh,
niss. Rangenet++:Fastandaccuratelidarsemanticsegmen-
SMNadimUddin,KhanMuhammadUmair,KaixuanWang,
tation. InIEEE/RSJInternationalConferenceonIntelligent
YufeiWang,YixingWang,MochuXiang,GuangkaiXu,Wei
RobotsandSystems,pages4213–4220,2019. 4
Yin,JunYu,QiZhang,andChaoqiangZhao. Thesecond
[61] Hans Moravec and Alberto Elfes. High resolution maps
monoculardepthestimationchallenge. InIEEE/CVFConfer-
fromwideanglesonar. InIEEEInternationalConferenceon
enceonComputerVisionandPatternRecognitionWorkshops,
RoboticsandAutomation,pages116–121,1985. 4
pages3063–3075,2023. 5
[62] MaximeOquab,Timothe´eDarcet,The´oMoutakanni,HuyVo,
[71] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
MarcSzafraniec,VasilKhalidov,PierreFernandez,Daniel
Sutskever,andRuslanSalakhutdinov. Dropout:asimpleway
Haziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2:
topreventneuralnetworksfromoverfitting. TheJournalof
Learningrobustvisualfeatureswithoutsupervision. arXiv
MachineLearningResearch,15(1):1929–1958,2014. 13
preprintarXiv:2304.07193,2023. 10,18
[72] PeiSun, HenrikKretzschmar, XerxesDotiwalla, Aurelien
[63] Jinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer,
Chouard,VijaysaiPatnaik,PaulTsui,JamesGuo,YinZhou,
KrisMKitani,MasayoshiTomizuka,andWeiZhan. Time
YuningChai,BenjaminCaine,etal. Scalabilityinperception
willtell:Newoutlooksandabaselinefortemporalmulti-view
forautonomousdriving:Waymoopendataset. InIEEE/CVF
3dobjectdetection. InInternationalConferenceonLearning
Conference on Computer Vision and Pattern Recognition,
Representations,2022. 3,10,16
pages2446–2454,2020. 2,3
[64] Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang,
[73] SebastianThrun. Probabilisticrobotics. Communicationsof
andErkangCheng. Bevsegformer:Bird’seyeviewsemantic
theACM,45(3):52–57,2002. 4
segmentationfromarbitrarycamerarigs.InIEEE/CVFWinter
ConferenceonApplicationsofComputerVision,pages5935– [74] XiaoyuTian,TaoJiang,LongfeiYun,YuchengMao,Huitong
5943,2023. 2,4 Yang, Yue Wang, Yilun Wang, and Hang Zhao. Occ3d:
[65] PierluigiZamaRamirez,FabioTosi,LuigiDiStefano,Radu A large-scale 3d occupancy prediction benchmark for au-
Timofte,AlexCostanzino,MatteoPoggiandSamueleSalti, tonomousdriving. AdvancesinNeuralInformationProcess-
StefanoMattoccia, JunShi, DafengZhang, YongA,Yixi- ingSystems,36,2024. 2,4
angJin,DingzheLi,ChaoLi,ZhiwenLiu,QiZhang,Yixing [75] ShihaoWang,YingfeiLiu,TiancaiWang,YingLi,andXi-
Wang,andShiYin.Ntire2023challengeonhrdepthfromim- angyuZhang.Exploringobject-centrictemporalmodelingfor
agesofspecularandtransparentsurfaces. InIEEE/CVFCon- efficientmulti-view3dobjectdetection. InIEEE/CVFInter-
ferenceonComputerVisionandPatternRecognitionWork- nationalConferenceonComputerVision,pages3621–3631,
shops,pages1384–1395,2023. 5 2023. 11,16
[66] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and [76] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang,
VaishaalShankar. Doimagenetclassifiersgeneralizetoim- YilunWang,HangZhao,andJustinSolomon. Detr3d: 3d
30objectdetectionfrommulti-viewimagesvia3d-to-2dqueries. perceptionandpredictioninbirds-eye-viewforvision-centric
InConferenceonRobotLearning,pages180–191.PMLR, autonomousdriving. arXivpreprintarXiv:2205.09743,2022.
2022. 3,16 4,7,9,12,13,14,15
[77] JamieWatson,MichaelFirman,GabrielJ.Brostow,andDani- [89] ChaoqiangZhao,YouminZhang,MatteoPoggi,FabioTosi,
yarTurmukhambetov. Self-supervisedmonoculardepthhints. XiandaGuo,ZhengZhu,GuanHuang,YangTang,andSte-
InIEEE/CVFInternationalConferenceonComputerVision, fanoMattoccia. Monovit:Self-supervisedmonoculardepth
pages2162–2171,2019. 2,6 estimationwithavisiontransformer. InInternationalConfer-
[78] YiWei,LinqingZhao,WenzhaoZheng,ZhengZhu,Yong- enceon3DVision,pages668–678,2022. 20
mingRao,GuanHuang,JiwenLu,andJieZhou. Surround- [90] YinZhouandOncelTuzel. Voxelnet: End-to-endlearning
depth: Entangling surrounding views for self-supervised for point cloud based 3d object detection. In IEEE/CVF
multi-camera depth estimation. In Conference on Robot Conference on Computer Vision and Pattern Recognition,
Learning,pages539–549.PMLR,2023. 2,4,7,9,19 pages4490–4499,2018. 23
[79] YiWei,LinqingZhao,WenzhaoZheng,ZhengZhu,JieZhou, [91] BenjinZhu,ZhengkaiJiang,XiangxinZhou,ZemingLi,and
andJiwenLu. Surroundocc:Multi-camera3doccupancypre- GangYu. Class-balancedgroupingandsamplingforpoint
dictionforautonomousdriving. InIEEE/CVFInternational cloud3dobjectdetection. arXivpreprintarXiv:1908.09492,
ConferenceonComputerVision,pages21729–21740,2023. 2019. 3
2,4,7,9,16 [92] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
[80] ShaoyuanXie,LingdongKong,WenweiZhang,JiaweiRen, Wang,andJifengDai. Deformabledetr: Deformabletrans-
Liang Pan, Kai Chen, and Ziwei Liu. Robobev: Towards formers for end-to-end object detection. arXiv preprint
robustbird’seyeviewperceptionundercorruptions. arXiv arXiv:2010.04159,2020. 12
preprintarXiv:2304.06719,2023. 2,4
[81] JunjieYan,YingfeiLiu,JianjianSun,FanJia,ShuailinLi,
TiancaiWang,andXiangyuZhang. Crossmodaltransformer:
Towardsfastandrobust3dobjectdetection. InIEEE/CVF
InternationalConferenceonComputerVision,pages18268–
18278,2023. 22
[82] ChenyuYang,YuntaoChen,HaoTian,ChenxinTao,Xizhou
Zhu,ZhaoxiangZhang,GaoHuang,HongyangLi,YuQiao,
Lewei Lu, et al. Bevformer v2: Adapting modern image
backbonestobird’s-eye-viewrecognitionviaperspectivesu-
pervision. InIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages17830–17839,2023. 2,3
[83] ShengYang,XiaolingZhu,XingNian,LuFeng,XiaozhiQu,
andTengMa. Arobustposegraphapproachforcityscale
lidar mapping. In IEEE/RSJ International Conference on
IntelligentRobotsandSystems,pages1175–1182,2018. 4
[84] YaoYao,ZixinLuo,ShiweiLi,TianFang,andLongQuan.
Mvsnet:Depthinferenceforunstructuredmulti-viewstereo.
InEuropeanConferenceonComputerVision,pages767–783,
2018. 4
[85] TianweiYin,XingyiZhou,andPhilippKrahenbuhl. Center-
based3dobjectdetectionandtracking. InIEEE/CVFCon-
ferenceonComputerVisionandPatternRecognition,pages
11784–11793,2021. 3
[86] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-
nawarHayat,FahadShahbazKhan,andMing-HsuanYang.
Restormer: Efficienttransformerforhigh-resolutionimage
restoration. InIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages5728–5739,2022. 20
[87] Oliver Zendel, Angela Dai, Xavier Puig Fernandez, An-
dreasGeiger,VladenKoltun,PeterKontschieder,AdamKo-
rtylewski,Tsung-YiLin,TorstenSattler,DanielScharstein,
HendrikSchilling,JonasUhrig,andJonasWulff. Therobust
visionchallenge. http://www.robustvision.net,
2022. 4
[88] YunpengZhang,ZhengZhu,WenzhaoZheng,JunjieHuang,
Guan Huang, Jie Zhou, and Jiwen Lu. Beverse: Unified
31