Refinement of an Epilepsy Dictionary through Human Annotation
of Health-related posts on Instagram
Aehong Min1†, Xuan Wang2,4†, Rion Brattig Correia3,4, Jordan Rozum4,
Wendy R. Miller5, & Luis M. Rocha4,3,∗
1 Donald Bren School of Information & Computer Sciences, University of California, Irvine, CA, USA
2 Luddy School of Informatics, Computing & Engineering, Indiana University, Bloomington, IN, USA
3 Instituto Gulbenkian de Ciˆencia, Oeiras, Portugal
4 Dept. of Systems Science & Industrial Engineering, Binghamton University, Binghamton, NY, USA.
5 School of Nursing, Indiana University, Indianapolis, IN, USA
* rocha@bighamton.edu
Abstract
Objective — To (1) identify health-related terms used on social media posts that do
not precisely match the health-related meaning of terms in a biomedical dictionary, (2) decide
whichtermsneedtoberemovedinordertoimprovethequalityofthedictionaryinthescopeof
biomedical text mining tasks, (3) evaluate the effect of removing imprecise terms on such tasks,
and (4) discuss how human-centered annotation complements automated annotation in social
media mining for biomedical purposes.
Materials and Methods — We used a dictionary built from biomedical terminology
extracted from various sources such as DrugBank, MedDRA, MedlinePlus, TCMGeneDIT, to
tagmorethan8millionInstagram postsbyuserswhohavementionedanepilepsy-relevantdrug
at least once, between 2010 and early 2016. A random sample of 1,771 posts with 2,947 term
matches was evaluated by human annotators to identify false-positives. Frequent terms with a
highfalse-positiveratewereremovedfromthedictionary. Tostudytheeffectofremovingthose
terms, we constructed knowledge networks using the refined and the original dictionaries and
performed an eigenvector-centrality analysis on both networks. OpenAI’s GPT series models
were compared against human annotation.
Results —
Analysis of the estimated false-positive rates of the annotated terms revealed 8 ambiguous
terms (plus synonyms) used in Instagram posts, which were removed from the original dictio-
nary. We show that the refined dictionary thus produced leads to a significantly different rank
†These authors contributed equally to this work.
1
4202
yaM
41
]LC.sc[
1v48780.5042:viXraofimportantterms,asmeasuredbytheireigenvector-centralityoftheknowledgenetworks. Fur-
thermore, the most important terms obtained after refinement are of greater medical relevance.
In addition, we show that OpenAI’s GPT series models fare worse than human annotators in
this task.
Discussion — Dictionaries built from traditional clinical terminology are not tailored for
social media language and can bias results when used in biomedical inference pipelines, such as
pharmacological surveillance. However, removing relatively few terms identified by human an-
notation significantly improves inference and recommendation of central terms and associations
when studying social media cohorts.
Conclusion — A human-centered methodology to refine biomedical dictionaries improves
biomedical social media mining.
Keywords: Socialmediamining;SocialNetworkAnalysis;Dictionary;DataCuration;Epilepsy.
1 Introduction
Social media data, such as text, hashtags, or images in posts, allow researchers to gain unprece-
dented access to study human cohorts. It is now possible to quantitatively measure very large
populations for their individual or collective experiences, behaviors, perceptions, and emotions
[1, 2, 3, 4, 5, 6]. Social media is more than a source of information or a valuable communication
toolforitsusers[7]—itisalsoavaluableresourceofinformationabouthumanhealthandwell-being
[1]. It has been used to track and predict various public health issues [8], such as mental health
disorders [9, 10], adverse drug reaction (ADR) [11, 12], drug-drug interactions (DDI) [13, 14, 15],
and substance abuse [16, 17].
Instagram is one of the most popular social network services in the world; in 2021, Instagram
reached more than 1 billion users worldwide [18]. Through an application for smartphones or
tablets, users can share photos and videos, often accompanied by long captions. Although most
research on social media has been focused on data from Twitter (now named X) or Facebook ,
Instagram has great potential for social media research given its increasing number of users. It has
already shown its potential for large-scale social media analysis and monitoring of public health
issues, suchasDDIandADR,uncoveringbehavioralpathologyandassociationsbetweendrugsand
symptoms in depression [15]. It can even be used as a tool for communication and support between
2patients and healthcare providers [19], and other health-related applications [1]. For these reasons,
the work described here is focused on data from Instagram, but our methodology and results are
applicable to other social media from Twitter to Reddit [20].
Despite the benefits of social media analysis for public health, social media research has not
focused much on people with epilepsy (PWE), a chronic noncommunicable brain disorder and
one of the most common neurological diseases [21]. In the U.S., more than three million adults
have epilepsy, and about 470,000 children were diagnosed with active epilepsy in 2015 [21, 22,
23]. Epilepsy-relevant Facebook pages and Twitter accounts play an essential role in providing
informationaboutdrugsorcorrectingmisconceptionsorepilepsystigmaononlineplatforms[24,25].
Furthermore, our team has shown that even small cohorts of epilepsy patients on Facebook can
informexpertsaboutrelevantbehaviorsinvolvedinrareoutcomes,suchassuddendeathinepilepsy
[6]. Therefore, more research on PWE and their caregivers’ online behaviors on social media,
from epilepsy-specific online groups to general-purpose platforms, is needed—especially to better
understandtheircomplexsymptomsandmedicationschedules,includingDDIandADR.Tosupport
such a research agenda, it is essential to develop automated annotation pipelines to mine and
detect biomedical signals from large-scale social media data in general [1, 26]. At the core of such
pipelines is the construction of biomedical dictionaries to tag relevant terminology in social media
posts. Typically these are produced from databases and named entity recognition tools that were
developed for scientific discourse, such as papers with experimental evidence available on PubMed
[15, 1, 27, 28]. However, it is unclear whether biomedical dictionaries built from scientific discourse
and evidence are fit for the informal discourse and particularities of discussions on Instagram and
other social media that are relevant for epilepsy (or other conditions of medical interest).
Toaddress thatquestion, here wepresenta human-centereddictionary refinementmethodology
and analysis tailored to tag clinically relevant terminology for the study of epilepsy cohorts on
Instagram . For comparison, OpenAI’s GPT series models, as representatives of Large Language
Model(LLMs), were alsousedas analternativeannotation process, though leadingtoworse results
thanhumanannotatorsinouranalysis. Inadditiontoproducingafocusedbiomedicaldictionaryfor
socialmedia,ourmanualannotationeffortdemonstratesthatfalsepositiveterms(clinicallyrelevant
terms used in a clinically irrelevant context) with high frequency exist in social media discourse. In
other words, dictionaries built from biomedical terminology appropriate for the scientific literature,
3contain terms that are used in social media with other meanings. Moreover, those terms bias
the knowledge inferences that automated pipelines might produce. Indeed, we demonstrate that
the removal of just a few high frequency false-positive dictionary terms improves the biomedical
knowledge extracted from the epilepsy cohort on Instagram, thus highlighting the importance of
human annotation to improve the quality of cohort-specific social media analysis.
2 Data and Methods
2.1 Dictionary Construction
Our dictionary includes terms related to drugs, allergens, medical terms, and natural products,
including cannabis. We construct this dictionary following [15, 29]. We recall the deails of that
construction below. We obtainedthese terms from a variety of existing medical ontologies and data
sources. Drug, allergen, and food terms are retrieved from Drugbank (v.5.1.0) [30]. Medical terms
including, but not limited to, disease symptoms and drug side effects are obtained from MedDRA
(v.15) [31]. Natural products are retrieved from MedlinePlus [32] and TCMGeneDIT [33]. For
cannabis, we manually added commonly referred terms and slang, such as ‘Mary Jane’ and ‘420’,
to our dictionary, as detailed in [15]. In addition, epilepsy terms commonly used by patients on
Internet forums were manually added using a C-value [34] tokenizer on the Epilepsy.com discussion
forums. These epilepsy-related terms include mentions such as ‘VNS’ (i.e. Vagus Nerve Stimulator)
and were validated by an epilepsy specialist and matched to MedDRA codes.
We distinguish dictionary terms into four categories: Allergens, Drugs, Medical Terms, and
Natural Products. Allergens include food names, ingredients, and animals (e.g., Orange, Duck);
Drugs include medicine and chemical compounds (e.g., Diazepam); Medical Terms include sta-
tus and conditions of putative medical relevance, such as physical, psychological, or physiological
features (e.g., Headache, Feeling hot); Natural Products consist of plants and their extracted
elements (e.g., Rose).
Importantly, synonyms are possible for each term. Therefore, all those are matched—as child
terms—to a unique parent (preferred) term. For instance, Weed, Mary jane, 420 and Cannabis
are all synonyms of the parent term Cannabis. The parent term is also included as a child term
for completeness of synonym lists. Drug names are treated similarly, whereby we keep the chem-
4ical name as parent terms (e.g. Diazepam), and all known commercial names (as extracted from
DrugBank [30]) as child terms (e.g. Valium). Some data sources of our dictionary already have
term hierarchy, and we used it as the base of our parent term mapping (for example, the “preferred
term” in MedDRA is mapped to the parent term in our dictionary). Table 1 lists examples of the
extracted posts, terms, and their parent term.
Drug brand names sometimes have very common names in the English language, which may
increase the number of false-positive hits (e.g., Nighttime is a common word and a synonym for
the drug Benadryl). To account for that, we matched terms in our dictionary to the expected
occurrence of such terms in the Brown Corpus [35]. Terms very commonly used in the daily
Englishlanguagewerethenrankedandremoved. Afterthiscollectionandinitialautomaticcuration
procedure, our dictionary contains 176,278 terms, of which 105,345 are Drugs, 66,961 are Medical
Terms, 2,797 are Allergens, and 1,175 are Natural Products.
Category Frequency Parent Term Examples
Coffee bean new nails, new bag and a coffee catch up makes for a
Allergen 874 (29.7%) very happy girl #smiley #girl #pink [...] #positivity
#goodday #instacollage
Cocoa strawberry chocolate tini i made at work yesterday!! so
adorable lol. [...]
Tea leaf studyingwithacupofteamakesmefeelbritish,bringing
my own teabag just makes me cheap , wishing it was
coffee . lol #tea #cheap #wishfulthinking #cuppatea
Diazepam saturdaynightsurvivalkit#goodbook#warmbed#hot-
Drug 204 (6.9%) tea #valium #davidrakoff
Ethanol anyone got a number for a good rehab centre , think i’ll
be booking lottie in soon ! #alcoholic #alcohol #friend
#crazy #needshelp [...]
Caffeine there’snothingbetterthanacuppatake-awaycoffee. ex-
ceptgettingchocolatewithyourtakeawaycoffee! #lifes-
good! #coffee #caffeine #java #chocolate #living [...]
Feeling hot #day16 : hanging out in #laos! and having this
Medical Term 1647 (55.9%) #icecream #dessert in this hot weather! #wanderlust
#travel #100happydays
Tattoo #tattoolove. #tattoohumour. #tattooquote. #tattoo.
#ilovetattoos. #bodyart. #insta. [...]
Nasopharyngitis i have a cold. any remedies? xx #bed #hottowe-
laroundyourhead #sniff #toiletpapertissues
Cannabis medicating made simple. #vaping #cannabidiol
Natural Product 222 (7.5%) #cannabis#cbd#monday#itsgoingtobeagreatweek[...]
Rose [...] #my#girl#beauty#rose#roses#flower#flowers
[...]
Rosa [...] #primavera. #rose. #instaflowers. #rosa. #abso-
lutelyfabulous. #roses. #verde.
Table1: Examplesofpostsintheannotationsamplewithmatchedchildterms(inred)bycategory,
frequency, and parent term.
52.2 Data Collection and Post Tagging
Since June 2016, the collection of Instagram data via the platform’s API has been limited due to
a company policy change. Here we use publicly available data from Instagram posts ranging from
2011 to early 2016 when they were collected and securely stored according to the platform’s terms
on our servers [15]. The data is comprised of the entire timelines (all time-stamped public posts)
of Instagram users who produced at least one post mentioning a hashtag (i.e., #) with a drug
name (or synonym) known to treat epilepsy. The (parent) drug names we used to retrieve time-
lines include carbamazepine, clobazam, diazepam, lacosamide, lamotrigine, levetiracetam,
oxcarbazepine, as well as all their brand name (child term) synonyms (e.g., Valium). In addition,
we added all user timelines that mentioned the epilepsy-associated hashtag ‘#seizuremeds’ which
is commonly used among PWE on discussion forums such as Epilepsy.com. This resulted in the
collection of the entire timelines of a cohort of 9,890 users, comprising 8,496,124 posts. Duplicate
posts from regrams were removed. In order to protect user privacy, we did not extract demographic
information from the collected accounts.
The caption field of all collected posts was subsequently tagged with the dictionary terms
according to an automatic multi-word lexical matching pipeline, resulting in a total of 979,683
dictionary term matches on the more than 8 million Instagram posts on the dataset.
3 Results
3.1 Human-centered Annotation
Automatic lexical matching with biomedical dictionaries built from scientific discourse and evi-
dence is not necessarily contextually accurate when tagging social media posts. Indeed, dictionary
terms used in social media discourse may refer to alternate meanings without any putative clini-
cal relevance. For instance, the term ‘hot’ has multiple meanings depending on context, ranging
from ‘having a fever’ to ‘sexual appeal’. Naturally, we are only interested in the potential clinical
relevance of dictionary term usage. Therefore, we need to refine the dictionary terms to improve
the accuracy of term matches in biomedical social media analytics. That is, the goal is to reduce
false positive term matches, such as in the example above. To do this we designed and pursued a
6Figure 1: Manual annotation workflow
manual annotation workflow to identify the dictionary terms most likely to be false positives in the
context of Instagram discourse related to epilepsy.
Because it was unfeasible for our team to manually annotate over 8 million posts, a sample was
randomly selected to provide a reasonable amount of posts for human annotators. This resulted in
a set of 1,771 posts containing at least one matched term, for a total of 2,947 matches, associated
with 466 unique parent terms. The number and proportion of matches per dictionary category is:
|Allergens| = 874 (29.7%; 108 parent terms), |Drugs| = 204 (6.9%; 64 parent terms), |Medical
terms| = 1,647 (55.9%; 268 parent terms), and |Natural products| = 222 (7.5%; 26 parent terms).
Table 1 shows examples of posts and respective matched child terms (in red), their parent terms
and categories.
Each human annotator was given our annotation guidelines to understand the goal of the study
and criteria to determine whether a matched term is used in the expected or correct sense (‘true-
positive’) or not (‘false-positive’). Also, they were provided with instructions with examples to
learn how to annotate through our annotation tool. This sample was subsequently used in our
annotation workflow which comprises two rounds (See Figure 1 & SI: Figure S1):
1. Initial annotation & guideline refinement. 292 posts with 499 dictionary matches were
used to test the annotation workflow and to refine our annotation guidelines, establishing
a standard for deciding whether a matched term was used as intended by the biomedical
dictionary (true positive) or not (false positive) in the context of the post. For instance, in
the context of medical terms, if a matched term “A” is expressed as signifying a medical term,
healthcondition,drug,food,or(potential)allergen(‘A’),anannotatorisinstructedtolabelit
as ‘True.’ Conversely, if the term is expressed in a manner other than the intended dictionary
meaning, but rather represents a metaphor, proper noun, or anything else, the annotator
7Type Meaning Criteria Example
Allergen Food Whether a term “A” means I could not drink orangejuice for 2 years. True
• Food “A” [True]
• Something (e.g., an imal) that can be food “A” The city drive south orange, CA… False
(Meaning here: Name of place)
[True]
• Not food (but metaphor, proper noun, or others)
[False]
Drug Drug, Whether a term “B” means The man in the station selling you a red bull . True
Chemical/biological • Drug “B” [True]
component • Chemical/biological component “B” [True] That angry man’s face was getting red, and he looked False
• Not drug or component (but metaphor, proper like a red bull.
(Meaning here: likening one’s angry look to an animal)
noun, or others) [False]
Medical Term Medical, Whether a term “C” means It was so hotoutside! True
Health-related, • Medical term “C” [True] This food is too hotnow. I need to eat this later.
Physical/mental • Physical/mental condition/status “C” [True]
condition or status • Not medical term and health condition/status This movie is so hot! False
(but metaphor, proper noun, or others) [False] #techno #hot#techhouse#house #electronic #edm
(Meaning here: passionately enthusiastic, eager, or
excited.)
Natural Natural product such as Whether a term “D” means #grapevines #grapes #rose#rosebush #love #holiday True
Product plant, fruit, etc. • Natural product “D” [True]
• Not natural product (but metaphor, proper noun, #goldengirls#dorothy#rose#miami#bestfriends False
or others) [False] (Meaning here: a person’s name)
Table 2: Finalized annotation guidelines: category & criteria
marks it as ‘False.’ Each sampled post was assigned to two annotators, with disagreements
being collectively discussed to reach a consensus that triggered a revision to the guidelines.
2. Full annotation review. Using the refined annotation guidelines obtained from the first
step of the workflow (See Table 2), all 1,771 posts in the sample and their 2,947 matches were
reviewed independently by two annotators. Data scientists in training as well as epilepsy
researchers participated in the annotation. They decided whether terms were appropriately
matched (true-positive), inappropriately matched (false-positive), or unclear to determine
(e.g., a term with unclear meaning). Annotations were compared, achieving a good inter-
rater reliability rating (Cohen’s Kappa=0.634) [36].
An example of the interface used by annotators is provided in Figure S2 in SI. In this interface,
each row displays a complete Instagram post with a dictionary term match highlighted in red. An
annotator can review the context of each post and determine whether the meaning of the red-
highlighted word is expressed with the intended use of the biomedical dictionary term, following
the criteria provided in the guidelines. Notice that annotators are not shown the photos that
accompany the post text on Instagram. They are only shown the text, which makes this a difficult
task for human annotators and more so for LLMs used below.
8Category Freq. of Examples
FP
Allergen 166 (19.0%) Orange [...] here our bottoms are shown in orange hibiscus
and neon orange paired with coordinating mahina tops.
#aquabikini2016#summer#bikini#etsy#handmade-
bikini [...]
Drug 35 (17.2%) Diazepam quiz team champions of the world #winners #val-
ium#kimye#getyournotesoutforthelads#mensfashion
#vscocam #vsco. #desree. #life. #noghost. #piece-
oftoast.
Medical Term 571 (34.7%) Feeling hot my jam #tool #adjustor #dj #kay d #smith #crazy
#hot #techhouse #hard #techno #house #electronic
#edm#edmlife#realedm#dance#plur#rave#drums
#kick #synth #snare #epic
Natural Prod- 67 (30.2%) Rose as its my birthday tomorrow, wine not!? #itsmybirth-
uct daytomorrow #twentythree #gallo #zinfandel #rose
#wine #black #vapour #electriccigerette #cherrymen-
thol
Table 3: Examples of false-positive terms, and overall false-positive rate per term category.
3.2 Identifying Ambiguous Terms
After the full annotation review step of the human-centered annotation process, 839 (28.5%) false-
positive term matches were observed—by annotators considering and inferring the context of the
post where the match occurred from text alone. Examples of posts with false-positive matches are
showninTable3. Inonecase(toprow), fromcontext, annotatorsinferthatthedictionarytermfor
Orange (as fruit it is in the allergen category) is actually used to mean the color orange. Similarly,
in another post (bottom row), rose was actually identified as a typo for ros´e (wine), thus assigned
to an incorrect parent dictionary term.
Focusing on each of the four term categories, the human-centered annotation revealed the
following false-positive rates (including unclear cases):
• Allergen: 166 out of 874 (19%). These include Orange, Apple and Ginger which were fre-
quently found to indicate a color, name (brand, pet, place, etc.), or toys, respectively.
• Drug: 35 out of 204 (17%). The match for Valium (brand name for diazepam) had the
highest false positive rate in this category. Since this drug brand name is frequently used
as a metaphor on social media (unlike in the scientific literature), a discourse feature human
annotators can easily infer (unlike most automatic methods).
• Medical term: 571 out of 1,647 (35%). This is the category with the largest observed false-
9positive rate (including unclear cases), due to the possible broad meanings in which these
termswerefrequentlyused. Forinstance,HotresolvestotheFeeling hotparentterm,which
clinically means to be “having or feeling a relatively high temperature” (fever). However, in
the sampled posts, Hot often meant “sexually excited” or “newly made” [37]—another case
of very distinct usage between social media and scientific discourse.
• Natural product: 67 out of 222 (30%).
Examples include Rose, Rosa, and Daphne, where the inferred context often indicated one’s
name or color.
Given the false-positive rates observed for term matches of the post sample, the next step
is to identify which terms are most ambiguous and should be removed from the dictionary in
order to refine and tailor it to epilepsy discourse on social media, our problem domain. Figure 2
charts the false-positive rate against the frequency of all parent terms evaluated by the manual
annotation workflow described above. The most concerning terms are naturally those that occur
very frequently in our data and simultaneously have a high false-positive rate—those on the top
and right of Figure 2. Table 4 lists the top 8 such child terms, as well as their respective parent
terms. For instance, the terms Hot, Cold and Euphoria are frequent matches in posts, but most of
the time without any putative medical significance with false-positive rates above 90%, since they
typically occur in contexts without any relationship to the intended meaning of the term in the
biomedical dictionary. The same can be said for the terms Rose and Orange, though less frequently
(fewer than 30 matches) but still unacceptably high false-positive rates above 80%. In contrast,
while the term Cannabis appears very frequently in our samples (144 total matches for all of its
child terms), it is most often used in relevant contexts (126 matches (88%)). In Figure 2, we show
two of the synonyms of Cannabis: 420 and weed.
To obtain a new refined dictionary we established a criterion to remove terms that maximize
false-positive rate and occur frequently enough in the human-annotated post sample. We first
selected terms with false-positive rates ≥ 0.5 and frequency ≥ 20 (red and blue dashed lines in
Figure 2, respectively), subsequently ranking them by frequency. This resulted in the removal of 8
terms listed in Table 4: Hot, Cold, Euphoria, Valium, Death, Rose, Orange, and Ginger. Among
the 8 terms, we found that most occurrences of Hot were not about feeling elevated temperature
101.0 Euphoria
Hot
Cold
Rose
Orange
0.8
0.6
Valium
Ginger
Death
0.4
Type
0.2
420 Drug
Allergen
Weed
Medical term
0.0 Natural product
0 20 40 60 80 100 120 140
Frequency
Figure 2: False-positive rate & frequency of parent terms in the annotated sample of posts. The
dashed horizontal line depicts a false-positive rate of 50%.
Rank Term Parent Term Category Frequency False-positive Rate
1 Hot Feeling hot Medical term 147 0.96
2 Cold Nasopharyngitis Medical term 67 0.94
3 Euphoria Euphoric mood Medical term 47 1.00
4 Valium Diazepam Drug 36 0.58
5 Death Death Medical term 32 0.50
6 Rose Rose Natural product 28 0.86
7 Orange Orange Allergen 27 0.81
8 Ginger Ginger Allergen 25 0.56
Table 4: Top 8 terms with the largest frequencies and false-positive rates.
11
etaR
evitisoP
eslaFbutaboutfeelingexcitementordescribingsomethingaspopular. Onthecontrary, Coldwasmostly
used as a term about temperature, not about the illness known as the Common Cold. Likewise,
the meaning of Euphoria was often not related to one’s feeling of joy, since it was frequently used
in club music-relevant contexts that may indicate a musical genre, a music album, or a brand. As
for Valium, if the word was mentioned with other terms related to drugs, it was easier to determine
whether it is a true- or false-positive term. However, almost half of the posts that mentioned it did
not provide enough context. In the remaining half, it was often used as a metaphor for something
boring (e.g. example on the second row of table 3). Finally, Rose, Orange, and Ginger in the posts
were frequently used as proper nouns, such as a person, an area, or a pet.
Notice that we could have removed more than these 8 terms with a false-positive rate above
50% (red dashed line in Figure 2). However, such terms have low frequencies of matches in our
sample, which means that the false-positive rate was estimated with few human-annotated matches
and potentially subjected to fluctuations from small sample size. Thus, to be conservative given
that terms originate from established medical dictionaries, we retained them. In any case, as we
show next, the impact of removing these 8 is much larger than removing other terms with similar
frequency.
3.3 Impact of Removing Ambiguous Terms
After dictionary refinement, we evaluate the impact of term removal on the eigenvector-centrality
of networks whose edge weights are obtained by tallying dictionary co-mentions in posts (their
construction, especially how we dealt with parent terms and child terms, is detailed in SI Ap-
pendix B.1). These co-mention networks can be seen as associative knowledge structures that
characterize the discourse of social media cohorts [38, 39, 15, 26].
The co-mention networks is built on the level of parent terms in the sense that each node is a
parent term covering several synonyms in the dictionary.
Insuchassociativeknowledgenetworks, removingtermsfromthedictionarypotentiallyreduces
thesetofnodesthatcomprisethem(eachparenttermisrepresentedasanodeandthetermremoval
is done in the level of child term). More importantly, removing terms also affects the edge weights
between nodes, as co-mention counts are altered. Since the structure of connections is altered,
all inferences one can make from these networks—such as information retrieval, link prediction,
12community structure, shortest paths [39]—can be affected.
We chose eigenvector centrality because it is a popular measurement of node importance that
accounts for indirect influence between nodes on undirected graphs, thus allowing us to assess the
network-level impact of term removal [40]. Another popular node centrality is PageRank centrality.
Although it is powerful on directed graphs, it is highly correlated with (or almost exactly the same
as) node degree when applied to undirected networks [41, 42]. Therefore, since our co-mention
networks are undirected, we choose eigenvector centrality rather than PageRank to assess the effect
of removing terms.
Original Refined
Rank Parent term Centrality Parent term Centrality
1 Feeling hot 0.701 Depression 0.599
2 Euphoric mood 0.685 Anxiety 0.540
3 Cocoa 0.087 Completed suicide 0.291
4 Nasopharyngitis 0.064 Pain 0.230
5 Homosexuality 0.060 Insomnia 0.218
6 Coffee bean 0.059 Decreased appetite 0.165
7 Tattoo 0.052 Bulimia nervosa 0.101
8 Tea leaf 0.040 Schizophrenia 0.087
9 Depression 0.040 Agoraphobia 0.086
10 Anxiety 0.038 Cannabis 0.084
11 Vegan 0.036 Migraine 0.081
12 Cannabis 0.032 Stress 0.078
13 Pain 0.032 Weight 0.069
14 Chicken 0.026 Psychotic disorder 0.068
15 Orange 0.021 Vegan 0.066
16 Death 0.021 Mania 0.064
17 Hyperhidrosis 0.020 Fear 0.061
18 Caffeine 0.019 Fibromyalgia 0.057
19 Banana 0.017 Convulsion 0.056
20 Lemon 0.016 Paranoia 0.054
Table 5: Top 20 highest eigenvector centrality terms of the epilepsy cohort Instagram co-mention
knowledge networks built with the original (left) and the refined (right) dictionary.
The top 20 terms ranked by eigenvector centrality, before and after dictionary refinement,
are shown in table 5. Top eigenvector centrality terms before the refinement include terms not
particularly relevant to epilepsy, such as Cocoa or Tattoo. However, after dictionary refinement,
not onlydo parent terms like Feeling hot disappear from the top, since their main child terms
were no longer present in the dictionary, but we see that all the top 10 terms are associated with
epilepsy, asattestedbyourepilepsyspecialists. Forinstance, Depression, aclinicaldiagnosisoften
co-morbid with epilepsy, jumps from 9th (0.040) to the top ranked centrality score term (0.599),
131.0
0.8
0.6
0.4
0.2
Mean CER After Removing Random 8 Terms
CER After Removing Selected 8 Terms
±1 STD of CER After Removing Random 8 Terms
0.0
0 100 200 300 400 500
k
Figure 3: Impact of removing the selected 8 terms on the top k (10 ≤ k ≤ 500) highest eigenvector
centrality terms by calculating the common elements ratio between the top k term lists before and
after term removal. For example, if after term removal, 5 terms are still in the top 10 terms lists,
the common element ratio will be 0.5 for k = 10. The top k eigenvector centrality terms lists
were calculated on the Instagram co-mention network with or without term removal. For baseline
comparison, we generated 1,000 samples (N = 8) from terms with a False Positive Rate less than
0.5 and a frequency greater than or equal to the minimum frequency of the 8 selected terms in our
Instagram data corpus, 10,230.
closely followed by Anxiety. This suggests that our dictionary refinement improved the quality of
the top eigenvector centrality terms, bringing epilepsy-related terms to a more central role in the
knowledge network.
To investigate whether the observed changes on the top eigenvector centrality terms are simply
due the high frequency of the 8 removed terms, we computed a null model experiment in which 8
terms with similar frequencies but smaller false positive rates (below 50%) are randomly sampled
1,000 times and removed, with subsequent computation of eigenvector centrality of the resulting
networks. Because the top k lists before and after dictionary refinement do not have the same set
14
)REC(
oitaR
tnemelE
nommoCk K K P (K ≥K )
refined random obs random refined
10 108 48.1 ± 16.0 0.138
20 393 162.1 ± 40.5 0.000
50 1734 613.0 ± 84.8 0.000
100 6134 1528.6 ± 381.6 0.000
200 19021 4420.1 ± 1153.6 0.000
500 101160 27797.9 ± 6336.6 0.000
Table 6: Fagin’s generalization of Kendall’s distance K for top k terms ranked by co-mention
eigenvector centrality. Distances in the second column (K ) are computed between term
refined
rankings obtained using the original dictionary and those obtained using the refined dictionary
that removes 8 frequent terms with a high false positive rate. The third column (K ) shows
random
the average distance (plus or minus one standard deviation) between the original ranking and the
one obtained after removing an 8 randomly selected highly frequent terms with a false positive
rate below 50% (1,000 samples). The final column indicates the observed proportion of these
samples that have a larger distance from the original ranking than the ranking obtained using our
refinement.
of terms, common rank difference metrics, such as Spearman’s coefficient and Kendall’s τ cannot
be directly applied. In Figure 3, we compare the top k terms by eigenvector centrality (of the
dictionary term co-mention networks), before and after removing 8 dictionary terms, by counting
the common elements ratio (CER) between the top k lists for each case. It is very clear that the
impact of removing the 8 terms with false positive rate above 50% (via human-annotation) is much
higherthanwhenweremove8randomtermswithsameorhigherfrequency,butlowerfalse-positive
rate. For k = 10 (the top 10 with largest eigenvector centrality), only 20% of the terms remain
after removal of the 8 terms with largest false positive rate per human annotation. Whereas, in
the case of random removal of 8 terms with similar frequency, on average, 65% of the terms remain
unaffected in the top 10. The difference between the selected terms and the null model remains
large for all values of k, even if CER increases. While the impact of removing random terms almost
saturates after k ≥ 100, at 91% CER, the impact of removing the 8 terms with high false positive
rate remains significant for all k, with over 20% different terms even for k = 500.
Additionally, we also use Fagin’s generalized Kendall’s distance K, a metric specially designed
to compare top k ranked lists [43]. We use it to quantify the impact on rank of removing the 8
ambiguous terms by computing the distance between the eigenvector centrality rankings obtained
before and after removal. K is comparable only to lists with the same sizes (the same k), with a
highervaluedenotingahigherrankdifference,withthepenaltyofmissingelementsconsidered. The
value of K obtained using our refined dictionary (denoted K ) for various values of k is given
refined
15in Table 6, alongside the average distance obtained by removing 8 similarly frequent random terms
(denoted K ). Table 6 shows that the 8 ambiguous terms selected through human annotation
random
have statistically significantly larger impact on eigenvector centrality rankings (higher K) beyond
the top 20 nodes than removal of similar random sets of terms. Though removing random high
frequency terms could have a similar impact on the top 10 terms list in relatively rare cases (about
14% of the time), we observed no case (out of 1,000 samples) in which this occurred for the top 20
terms or beyond. Moreover, the difference between K for the 8 ambiguous terms and the average
K for the randomly removed terms increases dramatically as the list size k increases.
The null model comparison, with both CER and Fagin’s generalized Kendall’s distance for rank
comparison, demonstratesthatremovaloftermsrevealedasambiguousbyhumanannotation, have
much higher impact on the knowledge network of biomedical terms than removing random terms
with the same frequency (but lower false positive rate). In other words, the high false-positive rate
terms, estimated per human annotation, are likely confusing the co-mention network with many
spurious edges that are not coherent with the remaining biomedical knowledge captured by the co-
mentionnetworks. Thus,removingthemhasahighimpactonthenetworkbecausetheirco-mention
associations are quite different from those of other terms in dictionary. Indeed, random removals
of terms with similarly high frequency are not as impactful, because many of the associations the
random terms induce are more coherent with the structure of other biomedical term associations.
In summary, the network global impact of removing human annotated high false-positive rate
terms highlights the value of human context-specific annotation.
3.4 Comparing Social Media with Medical and Scientific Discourse
At the onset, the hypothesis behind our study is that while there is much discourse on general-
purpose social media platforms that is of biomedical relevance, it is expressed differently than
scientific discourse and unfolds simultaneously with many other contexts that are not relevant to
health. Thus, human-centered dictionary refinement is needed to remove ambiguous terms in such
platforms.
To emphasize this need, and how different general-purpose social media discourse is, we go
beyond Instagram and deploy both the original and the refined version of our dictionary in other
data sources: epilepsy-related abstracts from PubMed, epilepsy-related clinical trials from clinical-
16trials.gov,Twitter (nownamedX)postsfromuserswhohavementionedepilepsyrelatedterms,and
discussion forums from Epilepsy.com. Detailed descriptions of data harvesting and network con-
struction of additional data sources are available in SI Appendix B under each data source section.
The first two additional data sources pertain to scientific discourse. Both Instagram and Twitter
data sources represent epilepsy related users’ speech on a general-purpose social media. The last
datasourceisaformofsocialmediathatisfocusedonasingletopicofmedicalrelevance—epilepsy,
in this case.
Table 7: The comparison of the impact of removing the 8 high false-positive rate high frequency
term on different data sources, measured by Fagin’s generalization of Kendall’s distance K for top
k terms ranked by eigenvector centrality. In parentheses, we divide the distances in the right four
columns by the distances in the second column to show the reader how great the impact relative
to the impact we observe on Instagram. Distances are computed between term rankings obtained
using the co-mention network constructed from the original dictionary and those obtained from
the co-mention network constructed after dictionary refinement. EFF is short for the discussion
forums from Epilepsy.com. CT is short for clinicaltrials.gov.
k Instagram Twitter EFF PubMed CT
10 108 34(0.31) 0(0.00) 3(0.03) 0(0.00)
20 393 300(0.76) 0(0.00) 82(0.21) 0(0.00)
50 1734 974(0.56) 101(0.06) 300(0.17) 82(0.05)
100 6134 2418(0.39) 344(0.06) 1324(0.22) 137(0.02)
200 19021 8607(0.45) 947(0.05) 3816(0.20) 270(0.01)
500 101160 46050(0.46) 1099(0.01) 19594(0.19) 1715(0.02)
In Table 7 we compare the impact of removing the 8 high false positive terms using Fagin’s
generalized Kendall’s distance as in Table 6. Twitter, being a general purpose social media like
Instagram, received an impact approximately a half of Instagram received after terms removal, as
measured by Fagin’s generalized Kendall’s distance. Other data sources received much less impact.
Networks built from clinicaltrials.gov data and the Epilepsy.com forums data set received zero
impact on top 20 terms list and at most 6% of the impact as Instagram on other lists. PubMed,
being a much larger text corpus, received only at most 22% of the impact as Instagram. This is
also supported by Table S2, Table S4, Table S9, and Table S11 in SI, in which we show that aside
from Twitter, for all other data sources, there are only inconsequential impacts on the top 20 terms
ranked by eigenvector centrality—except for the disappearance of those 8 terms we removed. In
other words, unlike in Instagram and Twitter , those 8 terms are either not ambiguous at all or
their ambiguous usages do not affect the network analysis greatly in scientific database (PubMed
17and Clinical trials) nor in epilepsy-specific social media (Epilepsy Foundation Forums) discourse.
Thissuggeststhatgeneral-purposesocialmediaplatformssuchasInstagram aremuchnoisierfor
biomedicalsurveillance, andforepilepsyresearchinparticular, sinceuserstendtopostaboutmany
distinct aspects of their lives. In contrast, in the Epilepsy Foundation (EF) forums users center
discourse around their condition, which naturally makes it a rich resource for epilepsy research.
The same can be said for the chosen PubMed articles or Clinical Trials, where the scientific context
is focused on epilepsy. Therefore, the impact of removing the same 8 terms from the knowledge
networks of both the EF forums and the PubMed abstracts is much less pronounced in comparison
to Instagram .
3.5 The disagreement between GPT-4 and human annotators is significant
Complementary to our human annotation efforts, we tested the feasibility of using a large language
model instead of human annotators in the labeling process of our proposed dictionary refinement
workflow. WeassignedthesamelabelingtasktoOpenAI’slatestandmostadvancedmodel, GPT-4
(version 1106 in 2023) and its precesssor GPT-3.5 via OpenAI’s API, using the same guidelines we
provided to human annotators [44, 45]. We find significant disagreement between GPT-4 and the
decisions of human annotators.
We use OpenAI’s API to ask OpenAI’s GPT-3.5 (version gpt-3.5-turbo-1106) and GPT-4 (ver-
sion gpt-4-1106-preview) to do the same labeling task as our human annotators. We converted the
human annotation guidelines (detailed in section 3.1 and summarized in fig. S2), initially designed
forhumansandpresentedtotheminslides, intoatext-basedpromptfortheAI,undergoingseveral
rounds of refinement for optimization. For human annotators, the term matches to be annotated
are highlighted in color, while for the LLM’s task, these terms are marked with asterisks (*) on
both sides. This method serves as a text-based alternative to visual highlighting, and is commonly
used in prompting LLMs.
For each term tagged on a post, we query the LLM using two prompts: one system prompt
based on the annotation guidelines, and one user prompt containing the entire text of post with
the highlighted term. The LLM was instructed to provide a response in json format, with both the
decision label and justification for the classification. The model’s response was then collected and
parsed. For term matches that the LLM returned different label than human annotators, we used
18the justification text generated by the model to suggest possible causes for the disagreement.
For each round of our prompt iteration, we used 200 tagged terms to test the performance.
During those tests, GPT-3.5 consistently showed inferior performance compared to GPT-4, so we
did not run a full-scale evaluation for GPT-3.5, choosing to focus on GPT-4 only. For the final
full-scale test, we sent 1,500 term matches to GPT-4 (corresponding to 1,500 rows in the tables for
human annotators), using our final version of the prompt.
Treating the consensus of two human annotators as the ground truth and grouping all “Un-
certain” or “Disagree” (if the two annotators gave different annotations) labels with the negative
label, we find a Matthews correlation coefficient (MCC) of 0.55 for GPT-4’s classification results.
Thisimprovesslightlyto0.58ifwediscardall“Uncertain”or“Disagree”frombothGPT-4andhu-
man annotators’ annotation. In particular, GPT-4 tended to label correctly matched terms (“True
Positive” in the annotation guideline and “match” for short in the following tables) as incorrectly
matched(“mismatch”forshortinthetables). Inotherwords,itismorestrictindesignatingaterm
as being used in a putatively clincally relevant way. As shown in Table 8, for the same set of 1,500
term matches, there are only 29 cases where the master annotator determined it as “mismatch”
while the annotator 2 thought it was a “match”, in comparison, there are 285 cases where GPT-4
thought it was “mismatch” and the annotator 2 thought it was a “match”.
Table 8: This table compares GPT-4 annotations to human annotators on 1,500 term matches,
illustrating potential bias in GPT-4’s classifications. Each term was annotated by GPT-4, the
master annotator, and one additional annotator from a pool of human annotators, all following
the same previously described guidelines. The concensus between the master annotator and the
additional annotator is computed also, with disagreements resulting in an “uncertain” label. To
prevent confusion with standard confusion matrix terminology, “true positive” and “false positive”
labelshavebeenrenamedto“match”and“mismatch”respectivelyinthistable. Thedatahighlights
GPT-4’stendencytoclassifytermsthat‘match’as‘mismatch’comparedtohumanannotators. For
context, the agreement between the annotator pool and the master annotator is also displayed.
AnnotatorPool MasterAnnotator Concensus
match mismatch uncertain match mismatch uncertain match mismatch uncertain
match 843 34 34 869 20 22 818 10 83
GPT-4 mismatch 285 266 32 290 226 67 220 193 170
uncertain 3 0 3 4 0 2 3 0 3
MasterAnnotator
match mismatch uncertain
match 1041 29 61
AnnotatorPool mismatch 75 203 22
uncertain 47 14 8
194 Discussion
In this paper, we show the importance of manual curation in developing a biomedical dictionary to
studyepilepsy-relateddiscussionsonsocialmedia. Despiteitsculturalimportanceandglobalreach,
Instagram , the social media we study has not been a focus of social media research. In general,
most biomedical research on social media focuses on platforms with freely and easily accessible
data, such as Twitter (now named X). In addition, such studies are either interested in a few drugs
or medical-related terms [46, 47, 48], or on conditions with less associated stigma. Very few studies
sofarhaveanalyzedthesocialmediadiscourseofpeoplewithepilepsyandtheircaregivers[6]using
biomedical dictionaries.
The use of dictionary-based methods to study biomedical discourse, however, is not new. In
general, they have been used for knowledge discovery and applied to large corpus of clinician-
written notes over the course of clinical examination, and more recently extracted from electronic
health records [49, 50, 51, 52]. As much as these dictionaries have been shown to work well in
clinical settings, they have not been tailored to the informal and broad discourse of social media.
Therefore, our goal here using human annotation is to identify and evaluate the impact of false-
positive dictionary terms matched to social media data. We were able to identify and remove terms
with high false-positive rates and that were not removed in our initial automated curated process,
thus highlighting the importance of a human-in-the-loop throughout the curation process. Human
annotation has been widely used to analyze and understand sentiments, behaviors, perceptions,
languages, or relations of people in social media [53, 54]. Even though automated annotation
techniques are able to identify and match terms faster than human annotation [55], our results
support prior literature showing human annotation is necessary when extracting term meaning
from social media posts [56, 57]. Therefore, balancing between the speed of automated annotation
andthereliabilityandvalidityofhumanannotationcanbeoneofkeyapproachestooptimalresults
[56, 58, 59].
We identified what kinds of terms tend to mean differently than their biomedical meaning
through human annotation and how often they have been used in social media posts. The false
positive matches were from the different meanings of words. Proper nouns were one of the most
common cases. Besides, words were often used as metaphors, which cannot be learned from the
20English dictionary. These results imply that future research on social media with biomedical
dictionaries should be cautious about terms with multiple meanings to perform a more precise
analysis.
Another noise source in term tagging on social media is lack of context information. There were
caseswhereourannotatorsfounditdifficulttodeterminethetruemeaningofthewordsduetolack
of context. It would be impossible for machine based automated methods to determine whether
those term matches should be tagged or not. If there are abundant cases among them that the
term was actually not used with the biomedical relevant meaning, they would introduce the same
kinds of noise as the ambiguous terms we identified in this study, to the text mining analysis.
Our results of dictionary refinement via human annotation also demonstrate how a dictionary
with ambiguous terms can have a great impact on downstream data analysis. We showed that
removing a few terms with high false-positive rates from the dictionary, guided by human annota-
tion, could significantly change the results of network analysis. We found that the impacts of the
refined dictionary were not limited to the terms we removed. There is no reason to believe that
eigenvector centrality is the only analysis that will benefit from dictionary refinement. Trivially,
network analysis based on node weight generated by node centrality based methods could be af-
fected. In addition, in future studies, we can also test the impact of dictionary refinement on other
network analysis, including, but not limited to community detection and link prediction, all the
way to impact on actual application like information recommendation system.
The great difference in the impact of the same dictionary refinement on different data sources
also suggests potential network-based methods for identifying low-quality terms in the dictionary.
Thoselowqualitytermscouldbeeitherambiguoustermswediscussedinthisstudyortermsshould
notbeinthedictionaryinthefirstplace. Ifourassumptioniscorrect,themajorityoftheknowledge
structure and spurious connections generated by noise shall have distinctively different structure
patterns. Either seperating the noisy pattern from the majority knowledge structure directly, or
simply removing nodes and measuring the impact like we did for the dictionary refinement, could
be potentially feasible approaches to identify the low-quality terms.
We also found that some data corpus benefit more from this dictionary refinement than other
data corpus, suggesting the need for dictionary refinement should tailor to each individual research
topic and data corpus. The four additional data sources besides Instagram may have different sets
21of their own ambiguous terms distinct from the 8 identified for Instagram . Indeed, the manual
annotation focused only on Instagram posts and it is possible that a similar approach could be
useful for refining dictionaries specific to the EF forums, PubMed abstracts, Clinical Trials and
even Twitter . Due to the associated cost of manual annotation, it is beyond the scope of this work
to do so. The variation in ambiguous terms across datasets may be attributable to the inherent
characteristics and the qualitative distinctions of each data corpus. Language usage and richness
levels could be different between social media, online health communities, and medical resources.
For instance, polysemous words in online health communities and medical resources have much
higher chances of being used with their medical-relevant and professional meanings than their
casual meanings than in social media. Identifying these differences may benefit future researchers
in improving the quality of biomedical signal analysis on different types of datasets. Especially, it
would be essential to be cautious about the noisy dataset, such as data from social media. Using
deep learning models, multi-corpus training, and normalization could be other ways to improve the
performance of social media mining regarding the differences between those resources [60].
In Section 3, we have shown that GPT-4, as a representative of the latest LLM, could not
replacehumanannotatorsinthistask. Thereareseveralfactorsthatcontributetothisperformance
difference. The most significant factor we observed was that the GPT-4 often stayed within the
narrow definition of some terms in the guidelines. For example, in the guidelines, we mentioned
that some terms in our dictionary were imported from medical dictionaries like MedDRA. Human
annotators could understand why “marriage” and “tattoo” are in MedDRA in the first place, and
why “orange” as food can be an allergen, then labeled the term matches as “true positive” when
they thought it was appropriate, while GPT-4 believed that the term was not used as a medical
term and therefore should be labeled as “false positive”. A few counterexamples in the prompt can
help, but cannot eliminate such kinds of bias. GPT-3.5 did much worse than GPT-4 in this kind
of bias (see SI Appendix C.3). For both models, it was hard to enumerate all possible term types
for which the model may fail.
In this paper, we discuss the results of using GPT series models as a direct replacement for
human annotators. However, it is not the purpose of this paper to find the optimal workflow to use
LLM on these types of tasks. Still, we gained some valuable experience about prompt engineering
for the labeling task in this paper and we recommend people to spend considerable time on prompt
22engineering if they aim to use LLM for similar tasks. The time spent on prompt engineering should
be included into the time cost in project management. The prompt engineering is not just about
writing one good prompt from scratch, but is more about performing a comprehensive test to
catch the “slips” or the bias of LLM in some types of terms, then trying to correct the bias in
many iterations of the prompt. This could require substantial amounts of human annotated data,
depending on the goal of the research, with more accurate estimation on the term mislabeling
need more data, and coarser estimation requires less. Some sampling methods could be used to
reduce the size of data set needed, but in principle cannot improve the coverage of corner cases. If
sufficient human annotated data are readily available, fine-tuning the model could potentially be
more effective than prompt engineering. Taking into account all those steps, the use of LLM for
term labeling still requires substantial time investment if accuracy is a priority.
Although current LLMs are not perfect replacements for human annotators, the research on
LLM is a fast developing field with better models coming out every year. We should not under-
estimate the potential of LLM in the future application of social media mining. Bearing in mind
the prerequisites of conducting comprehensive validations prior to the deployment of LLMs, several
promising approaches are currently emerging that utilize LLMs to enhance term tagging in the im-
mediate or foreseeable future. One possible hybrid approach is discussed above: first uses human
annotation to align LLM output, then LLM could be used to annotate larger data corpus. LLM’s
annotation will then be used for dictionary refinement. In this way, we accumulate much more
annotated data, thus providing better coverage for low-frequency terms and leading to potentially
better dictionary refinement, with the same amount of human annotation data. In the future, LLM
could also be used to tag each occurrence of the term directly for the entire text corpus without
dictionary refinement, so the “correct” usage of high false positive rate terms can be tagged and
contribute to downstream analysis, in contrast to discarding all matches of high false-positive rate
terms. However, even if current LLMs are sufficiently accurate, their high operational costs still
prohibit large-scale applications. This limitation is likely to be alleviated in the future.
235 Conclusion
This research identified the terms in our dictionaries with higher frequencies and false-positive
rates in an epilepsy cohort on Instagram by using the human annotation method. On comparison,
GPT series models cannot replace human annotation in the annotation process. We also identified
which terms should be removed from our dictionary to obtain more epilepsy-relevant results from
network analysis. Additionally, we identify the fundamental cause of incorrect term matches—
having different meanings that are often used—and specific cases. Finally, through validation of
the impact of the refined dictionary on eigenvector centrality analysis of the co-mention networks,
we demonstrated the impacts and complementary role of the human annotation method to the
automated annotation technique.
Our results demonstrate that human-centered dictionary refinement should be performed when
conducting social media mining of biomedical relevance, and epilepsy in particular. It remains to
be determined if this approach is also necessary for analyzing other types of scientific or patient-
centered textual resources.
OurstudyhasutilizedalargenumberoftermsinmedicaldictionariesandfocusedonInstagram
, which may have significant potentials for large-scale social media analysis. Other social media
studies on medical corpus have more focused on Twitter and Facebook , and most of them have
selectedafewspecificmedicaltermsintheiranalyses. Ourresultsandimplicationsmayhelpfuture
research in our research communities, including the areas of bioinformatics and health informatics,
by improving analysis of medical topics on social media speeches. Ultimately, this research would
contributetodevelopingknowledgegraphsthatmoretruthfullyrepresenttheunderlyingknowledge
structure, which can be used for personalized information systems that can support PWE, PWEC.
And the workflow could also be applied to other diseases and support other disease cohorts.
6 Acknowledgement
This work was partially funded by National Institutes of Health, National Library of Medicine
Program, grant 01LM011945-01 (LMR, AM, XW, RBC, and WRM), a Fulbright Commission
fellowship (LMR), the NSF-NRT grant 1735095 “Interdisciplinary Training in Complex Networks
and Systems” (LMR, AM, XW and RBC), and Funda¸c˜ao para a Ciˆencia e a Tecnologia, grant
24PTDC-MEC-AND-30221-2017 (RBC) The funders had no role in study design, data collection and
analysis, decision to publish, or preparation of the manuscript.
References
1. BrattigCorreiaR,WoodIB,BollenJ,RochaLM.MiningSocialMediaDataforBiomedicalSig-
nals and Health-Related Behavior. Annual Review of Biomedical Data Science. 2020;3(1):433-
458.
2. Guntuku SC, Klinger EV, McCalpin HJ, Ungar LH, Asch DA, Merchant RM. Social media
language of healthcare super-utilizers. npj Digital Medicine. 2021;4(1):1–6.
3. Klein A, Sarker A, Rouhizadeh M, O’Connor K, Gonzalez G. Detecting personal medication
intakeinTwitter: anannotatedcorpusandbaselineclassificationsystem.inBioNLP 2017:136–
142 2017.
4. Fan Y, Zhang Y, Ye Y, li X, Zheng W. Social Media for Opioid Addiction Epidemiology:
Automatic Detection of Opioid Addicts from Twitter and Case Studies. in Proceedings of the
2017 ACM on Conference on Information and Knowledge ManagementCIKM ’17(New York,
NY, USA):1259–1267Association for Computing Machinery 2017.
5. Saha K, Sugar B, Torous J, Abrahao B, Kıcıman E, De Choudhury M. A social media study on
the effects of psychiatric medication use. in Proceedings of the International AAAI Conference
on Web and Social Media;13:440–451 2019.
6. Wood IB, Brattig Correia R, Miller WR, Rocha LM. Small Cohort of Epilepsy Patients
Showed Increased Activity on Facebook before Sudden Unexpected Death. Epilepsy & Be-
havior. 2022;128:108580.
7. Sultan M, Brown EM, Thomas RH. Clinicians embracing social media: Potential and pitfalls.
Epilepsy & Behavior. 2021;115:106462.
8. Kautz H. Data mining social media for public health applications. in 23rd International Joint
Conference on Artificial Intelligence (IJCAI 2013), Beijing 2013.
259. ToulisA,GolabL.SocialMediaMiningtoUnderstandPublicMentalHealth.inData Manage-
ment and Analytics for Medicine and Healthcare (Begoli E, Wang F, Luo G. , eds.)(Cham):55–
70Springer International Publishing 2017.
10. Lopez-Castroman J, Moulahi B, Az´e J, et al. Mining social networks to improve suicide pre-
vention: A scoping review. Journal of Neuroscience Research. 2020;98(4):616-625.
11. Nikfarjam A, Sarker A, O’Connor K, Ginn R, Gonzalez G. Pharmacovigilance from social
media: mining adverse drug reaction mentions using sequence labeling with word embedding
clusterfeatures.Journal of the American Medical Informatics Association. 2015;22(3):671-681.
12. SarkerA,GonzalezG.Portableautomatictextclassificationforadversedrugreactiondetection
via multi-corpus training. Journal of biomedical informatics. 2015;53:196–207.
13. Yang H, Yang CC. Harnessing social media for drug-drug interactions detection. in 2013 IEEE
International conference on healthcare informatics:22–29IEEE 2013.
14. HamedAA,WuX,EricksonR,FandyT.TwitterKHnetworksinaction: Advancingbiomedical
literature for drug search. Journal of Biomedical Informatics. 2015;56:157–168.
15. Correia RB, Li L, Rocha LM. Monitoring potential drug interactions and reactions via network
analysis of instagram user timelines. in Pacific Symposium on Biocomputing:492–503 2016.
16. Sarker A, O’connor K, Ginn R, et al. Social media mining for toxicovigilance: automatic
monitoring of prescription medication abuse from Twitter. Drug safety. 2016;39(3):231–240.
17. Garimella VRK, Alfayad A, Weber I. Social Media Image Analysis for Public Health. in Pro-
ceedings of the 2016 CHI Conference on Human Factors in Computing SystemsCHI ’16(New
York, NY, USA):5543–5547Association for Computing Machinery 2016.
18. Enberg J. Global Instagram Users 2020. 2020.
19. Yakar F, Jacobs R, Agarwal N. The current usage of Instagram in neurosurgery. Interdisci-
plinary Neurosurgery. 2020;19:100553.
2620. GuoZG,FelagJ,RozumJC,BrattigCorreiaR,RochaLM.Selectionofrelevantpatientcohorts
from social media using the metric backbone of knowledge networks. journal of biomedical
informatics. 2023:submitted.
21. Hirtz D, Thurman DJ, Gwinn-Hardy K, Mohamed M, Chaudhuri AR, Zalutsky R. How com-
mon are the ”common” neurologic disorders?. Neurology. 2007;68(5):326–337.
22. World Health Organization . Epilepsy. 2019.
23. Zack MM, Kobau R. National and state estimates of the numbers of adults and children with
activeepilepsy—UnitedStates,2015.Tech.Rep.31CentersforDiseaseControlandPrevention
2017.
24. McNeil K, Brna P, Gordon K. Epilepsy in the Twitter era: A need to re-tweet the way we
think about seizures. Epilepsy & Behavior. 2012;23(2):127-130.
25. Meng Y, Elkaim L, Wang J, et al. Social media in epilepsy: A quantitative and qualitative
analysis. Epilepsy & Behavior. 2017;71:79–84.
26. CorreiaRB,Rozum JC,CrossL,et al.myAURA: Personalizedhealthlibraryforepilepsyman-
agementviaknowledgegraphsparsificationandvisualization.arXivpreprintarXiv:2405.05229.
2024.
27. Louren¸co A, Conover M, Wong A, et al. A linear classifier based on entity recognition tools
and a statistical approach to method extraction in the protein-protein interaction literature.
BMC bioinformatics. 2011;12:1–20.
28. Zhang S, Wu H, Wang L, et al. Translational drug–interaction corpus. Database.
2022;2022:baac031.
29. Brattig Correia R. Prediction of Drug Interaction and Adverse Reactions, with Data from
Electronic Health Records, Clinical Reporting, Scientific Literature, and Social Media, Using
Complexity Science Methods. PhD thesisIndiana UniversityUnited States – Indiana 2019.
30. D.S. W, Y.D. F, A.C. G, et al. DrugBank 5.0: a major update to the DrugBank database for
2018. Nucleic Acids Res. 2017.
2731. Medical Dictionary for Regulatory Activities . MedDRA©is the international medical ter-
minology developed under the auspices of the International Conference on Harmonisation of
Technical Requirements for Registration of Pharmaceuticals for Human Use (ICH). https:
//www.meddra.org 2018. Accessed Oct 22.
32. MedlinePlus . Herbal Medicine. http://1.usa.gov/1IF33ng.
33. FangYC,HuangHC,ChenHH,JuanHF.TCMGeneDIT:adatabaseforassociatedtraditional
Chinese medicine, gene and disease information using text mining. BMC Complementary and
Alternative Medicine. 2008;8(1):58.
34. FrantziK,AnaniadouS,MimaH.Automaticrecognitionofmulti-wordterms:.theC-value/NC-
value method. International Journal on Digital Libraries. 2000;3(2):115–130.
35. Francis WN, Kucera H. Manual of Information to accompany A Standard Corpus of Present-
Day Edited American English, for use with Digital Computers. Department of Linguistics,
Brown UniversityProvidence, Rhode Islandrevised 1971. revised and amplified 1979 ed. 1964.
36. McHughML.Interraterreliability: thekappastatistic.Biochemia medica. 2012;22(3):276–282.
37. Merriam-Webster Inc. . Hot — Definition of Hot by Merriam-Webster. 2020.
38. HamedAA,WuX,EricksonR,FandyT.TwitterKHnetworksinaction: advancingbiomedical
literature for drug search. Journal of biomedical informatics. 2015;56:157–168.
39. Simas T, Rocha LM. Distance closures on complex networks. Network Science. 2015;3(2):227–
268.
40. Bonacich P. Power and Centrality: A Family of Measures. American Journal of Sociology.
1986;92:1170–1182.
41. Perra N, Fortunato S. Spectral Centrality Measures in Complex Networks. Physical Review E.
2008;78(3):036107.
42. Grolmusz V. A Note on the PageRank of Undirected Graphs. Information Processing Letters.
2015;115(6):633–634.
2843. Fagin R, Kumar R, Sivakumar D. Comparing Top k Lists. SIAM Journal on Discrete Mathe-
matics. 2003;17(1):134–160.
44. Brown TB, Mann B, Ryder N, et al. Language Models Are Few-Shot Learners. 2020.
45. OpenAI , Achiam J, Adler S, et al. GPT-4 Technical Report. 2024.
46. Pierce CE, Bouri K, Pamer C, et al. Evaluation of Facebook and Twitter monitoring to de-
tect safety signals for medical products: an analysis of recent FDA safety alerts. Drug safety.
2017;40(4):317–331.
47. GinnR,PimpalkhuteP,NikfarjamA,et al.MiningTwitterforadversedrugreactionmentions:
a corpus and classification benchmark. in Proceedings of the fourth workshop on building and
evaluating resources for health and biomedical text processing:1–8Citeseer 2014.
48. O’Connor K, Pimpalkhute P, Nikfarjam A, Ginn R, Smith KL, Gonzalez G. Pharmacovig-
ilance on twitter? Mining tweets for adverse drug reactions. in AMIA annual symposium
proceedings;2014:924American Medical Informatics Association 2014.
49. Aronson AR, Lang FM. An Overview of MetaMap: Historical Perspective and Recent Ad-
vances. Journal of the American Medical Informatics Association. 2010;17(3):229–236.
50. Huang MS, Lai PT, Lin PY, You YT, Tsai RTH, Hsu WL. Biomedical Named Entity Recogni-
tion and Linking Datasets: Survey and Our Recent Development. Briefings in Bioinformatics.
2020;21(6):2219–2238.
51. Re´ategui R, Ratt´e S. Comparison of MetaMap and cTAKES for Entity Extraction in Clinical
Notes. BMC medical informatics and decision making. 2018;18(Suppl 3):74.
52. Yang Z, Lin H, Li Y. Exploiting the Performance of Dictionary-Based Bio-Entity Name Recog-
nition in Biomedical Literature. Computational Biology and Chemistry. 2008;32(4):287–291.
53. Eryi˘git G, Cetin FS, Yanık M, Temel T, Cic¸ekli I. Turksent: A sentiment annotation tool for
social media. in Proceedings of the 7th Linguistic Annotation Workshop and Interoperability
with Discourse:131–134 2013.
2954. Zubiaga A, Liakata M, Procter R, Bontcheva K, Tolmie P. Crowdsourcing the Annotation of
Rumourous Conversations in Social Media. in Proceedings of the 24th International Confer-
ence on World Wide WebWWW ’15 Companion(New York, NY, USA):347–353Association for
Computing Machinery 2015.
55. Taboada M, Rodr´ıguez H, Mart´ınez D, Pardo M, Sobrido MJ. Automated semantic annotation
of rare disease cases: a case study. Database. 2014;2014.
56. Cardie C, Wilkerson J. Text Annotation for Political Science Research. Journal of Information
Technology & Politics. 2008;5(1):1-6.
57. Aroyo L, Welty C. Truth is a lie: Crowd truth and the seven myths of human annotation. AI
Magazine. 2015;36(1):15–24.
58. Russel SJ, Norvig P. Artificial Intelligence: A Modern Approach. Prentice Hall2nd ed. 2002.
59. Mitchell TM. Machine Learning. McGraw-Hill Education 1997.
60. Magge A, Weissenbacher D, O’Connor K, Scotch M, Gonzalez-Hernandez G. SEED: Symp-
tom Extraction from English Social Media Posts using Deep Learning and Transfer Learning.
medRxiv. 2022.
30Supplemental Materials
A Annotation process & analysis example 2
B Dictionary refinement impact on knowledge networks 4
B.1 Construction of co-mention network........................................................... 4
B.2 Comparison methods............................................................................... 5
B.3 Instagram.............................................................................................. 5
B.4 Twitter................................................................................................. 6
B.5 Epilepsy Foundation forums..................................................................... 8
B.6 PubMed................................................................................................ 9
B.7 Clinical Trials ........................................................................................ 12
B.8 Dictionary refinement impacts on different data sources................................ 12
C Comparison with OpenAI’s GPT series models on term annotation 14
C.1 the prompt we used for GPT-4 ................................................................. 14
C.2 compare GPT-4’s annotation with human annotators................................... 17
C.3 GPT-3.5’s annotation comparison with human annotators............................. 18
1A Annotation process & analysis example
2Figure S1: Screenshot of annotation guidelines
Figure S2: Screenshot of annotation analysis
3B Dictionary refinement impact on knowledge networks
Here we list the tables comparing the top scoring terms for eigenvector centrality on knowledge
networks built from both the original and the refined dictionary. We study the effect of both
dictionaries on four different data sources, including: (i) the epilepsy Instagram cohort (as dis-
cussed in main manuscript), (ii) the Epilepsy Foundation (EF) discussion forums, (iii) a selection
of epilepsy-related abstracts retrieved from PubMed, and (iv) epilepsy-related Clinical Trials (CT)
retrieved from clinicaltrials.gov. Sections below contain additional details of each data source as
well as scoring results.
B.1 Construction of co-mention network
We constructed a simple co-mention counts network for each of the data sources. In this network,
each node is a parent term that occurs in any of the posts in the dataset. The edge weight is the
number of posts containing both term A and term B.
The text unit “post” has a different definition for each data source. For social media like
Instagram, the Epilepsy Foundation forum and Twitter, a post is a post. For PubMed, the text
unit is the title plus the abstract text. For clinical trials, we concatenate all text fields in one
clinical trial record into a single “post”.
The co-mention networks is built on the level of parent terms in the sense that each node is a
parent term covering several synonyms in the dictionary. The term matching is still done on the
level of child terms, in which the dictionary refinement takes place. Multiple occurrences of terms
under the same parent term will be counted only once. Removing a child term does not remove
its sibling terms. So the term’s parent term, representing the cluster of terms, could still be in the
co-mention network. Removing a term that happens to be the parent term of a cluster of terms
also does not exclude its usage as a node label in the co-mention network, but only exclude it from
the term matching process.
Due to the above complications, the nodes that disappeared after dictionary refinement are
not necessarily the terms we removed. Removing a term does not directly remove its parent term
from the co-mention network unless it is the only term in the cluster of terms. Though removing a
child term that counts for the majority of the cluster of terms’ occurrence will greatly reduce the
4co-mention counts and the related node degree. Nevertheless, those terms whose all co-mentions
are with the removed terms before term removal, will be removed.
As described in our previous research, simple co-mention counts could be a bad choice to
study the distance and shortest path between nodes and Jaccard Index is a good normalization
method to obtain the proximity network [39, 15]. However, for the purpose of obtaining top
eigenvector centrality nodes, simple co-mention count is still a good choice while Jaccard Index
normalization could potentially suppress too much high frequency terms’ weight. In principle,
eigenvector centrality on co-mention networks reveal the important nodes by assuming that nodes
can vote for each other and important nodes get more votes from other important nodes. If
normalization should be in place, it better take place at the level of all edges of a node, not
between a pair of nodes.
B.2 Comparison methods
Like what we did in the main text, we show the reader two tables, one is the top 20 terms ranking
by eigenvector centrality, before and after the dictionary refinement. The other is a table for top
10, 20, 50, 100, 200, and 500 terms lists before and after the dictionary refinement. To help the
reader get a better idea of if Fagin’s generalized Kendall’s distance is a big or small distance, we
added another column, normalizing the distance with the maximum possible distance for the given
k and a given common element numbers between the two lists. This is the tricky part of Fagin’s
generalized Kendall’s distance: Kendall’s distance is calculated for every pair of items between two
lists. The total number of unique elements of two top k lists will not be k but 2k−z, in which z is
the number of common elements between two lists. Thus it is not advisable to use the normalized
distance to compare between different data sources. For that purpose, the raw distance should be
used as in Table 7.
B.3 Instagram
This section presents the knowledge network results for the Instagram data source. Data details
are described in the main text, section 2.
5Table S1: Fagin’s generalized Kendall’s distance K, (missing pair penalty parameter, p = 0.5) for
the top k highest eigenvector centrality terms comparing the Instagram co-mention network built
with the original versus the refined dictionary. The third column is the distance normalized by the
maximum possible K for a given k and a given count of common elements, so the range of values is
[0,1]. In the fourth column, we show the average K in the null model described in the main text:
1,000 networks built with 8 random-sampled terms with low FPR and similar frequency. The last
column is the standard deviation of K for the null model.
k selected terms K normalized K random term K¯ σ of random term K
10 108 0.706 48.1 16.0
20 393 0.701 162.1 40.5
50 1734 0.642 613.0 84.8
100 6134 0.668 1528.6 381.6
200 19021 0.611 4420.1 1153.6
500 101160 0.569 27797.9 6336.6
B.4 Twitter
ThissectionpresentstheknowledgenetworkresultsfortheTwitterdatasource. WeparsedTwitter
posts from 2006 to 2012 and selected users who have mentioned epilepsy-related drugs or hashtag
#seizuremeds. The criteria we used to select Twitter users are the same as Instagram users, which
are described in the main text, section 2.
6Original Refined
Rank Parent term Eigen-centrality Parent term Eigen-centrality
156 Pyridoxine 0.575 Pyridoxine 0.577
1701 Pantothenic acid 0.575 Pantothenic acid 0.577
614 Niacin 0.574 Niacin 0.576
8595 Coffee bean 0.057 Coffee bean 0.036
176626 Feeling hot 0.049 Tea leaf 0.014
8976 Cocoa 0.035 Caffeine 0.009
8649 Tea leaf 0.033 Somnolence 0.008
181534 Nasopharyngitis 0.031 Cocoa 0.008
192 Caffeine 0.013 Ethanol 0.005
186129 Somnolence 0.011 Riboflavin 0.004
815 Diazepam 0.010 Acetaminophen 0.004
8592 Chicken 0.009 Cocaine 0.004
8645 Vanilla 0.007 Vanilla 0.004
884 Ethanol 0.007 Cyanocobalamin 0.004
307 Acetaminophen 0.007 Thiamine 0.004
8608 Lemon 0.006 Cherry 0.003
178464 Hyperhidrosis 0.005 Nicotinamide 0.003
8633 Rice 0.005 Cannabis 0.003
177790 Headache 0.005 Scratch 0.003
8575 Apple 0.005 Goose 0.003
Table S2: Top 20 eigenvector centrality terms of the epilepsy cohort Twitter knowledge networks
built with the original (left) and the refined (right) dictionary.
Table S3: Fagin’s generalized Kendall’s distance, (missing pair penalty parameter, p = 0.5) for the
topk highesteigenvectorcentralitytermscomparingtheTwitterco-mentionnetworkbuiltwiththe
original versus the refined dictionary. The last column is the distance normalized by the maximum
possible distance so the value range is [0,1].
k generalized Kendall’s distance normalized distance
10 34 0.515
20 300 0.739
50 974 0.515
100 2418 0.396
200 8607 0.357
500 46050 0.318
7B.5 Epilepsy Foundation forums
This section presents the knowledge network results for the Epilepsy Foundation (EF) data source.
Data were collected from the forum of www.epilepsy.com. Forums contained long textual descrip-
tions of epilepsy-related discussions separated by individual questions and answers categorized by
topics.
Table S4 shows the top eigenvector centrality terms of the knowledge network built from this
data source before and after dictionary refinement. Note how false-positive term matches did not
have a great impact over the top ranking terms.
Original Refined
Rank Parent term Eigen-centrality Parent term Eigen-centrality
1 Convulsion 0.595 Convulsion 0.596
2 Epilepsy 0.492 Epilepsy 0.493
3 Levetiracetam 0.249 Levetiracetam 0.249
4 Electroencephalogram 0.232 Electroencephalogram 0.232
5 Grand mal convulsion 0.198 Grand mal convulsion 0.199
6 Lamotrigine 0.195 Lamotrigine 0.195
7 Surgery 0.140 Surgery 0.140
8 Anxiety 0.133 Anxiety 0.134
9 Valproic Acid 0.124 Valproic Acid 0.124
10 Stress 0.120 Stress 0.120
11 Phenytoin 0.098 Phenytoin 0.099
12 Carbamazepine 0.096 Carbamazepine 0.097
13 Topiramate 0.093 Topiramate 0.093
14 Confusional state 0.092 Confusional state 0.092
15 Depression 0.083 Depression 0.083
16 Partial seizures 0.069 Partial seizures 0.069
17 Pain 0.068 Pain 0.068
18 Pregnancy 0.068 Pregnancy 0.068
19 Aura 0.067 Aura 0.067
20 Amnesia 0.066 Amnesia 0.066
Table S4: Top 20 highest eigenvector centrality terms of the forum of epilepsy.com knowledge
networks built with the original (left) and the refined (right) dictionary.
8Table S5: Fagin’s generalized Kendall’s distance, (missing pair penalty parameter, p = 0.5) for the
top k highest eigenvector centrality terms comparing the epilepsy.com forum co-mention network
built with the original versus the refined dictionary. The last column is the distance normalized by
the maximum possible distance so the value range is [0,1].
k generalized Kendall’s distance normalized distance
10 0 0.000
20 0 0.000
50 101 0.079
100 344 0.067
200 947 0.046
500 1099 0.009
B.6 PubMed
This section presents the knowledge network results for the PubMed (Medline) data source. We
selectedpapersfromPubMediftheysatisfyanyofthefollowingcriteria: (i)theywerepublishedon
a expert-curated list of 18 epilepsy-focused journals (see table S6); (ii) they contained at least one
epilepsy-related MeSH term (see table S7; or (iii) their titles or abstracts contain the name of at
least one drug known to treat epilepsy (as described in section 2 and listed in table S8). Dictionary
terms were matched in both the title and the abstract of all selected papers.
TopeigenvectorcentralitytermscanbeseeninTableS9,beforeandafterdictionaryrefinement.
9Journal name
1 Clinical nursing practice in epilepsy
2 Epilepsia
3 Epilepsia open
4 Epilepsy & behavior case reports
5 Epilepsy & behavior : E&B
6 Epilepsy & behavior reports
7 Epilepsy currents
8 Epilepsy journal
9 Epilepsy research
10 Epilepsy research and treatment
11 Epilepsy research. Supplement
12 Epileptic disorders: international epilepsy journal with videotape
13 Journal of epilepsy
14 Journal of epilepsy research
15 Journal of pediatric epilepsy
16 Molecular & cellular epilepsy
17 Newsletter. American Epilepsy League
18 North African and Middle East epilepsy journal
Table S6: List of epilepsy-focused journals used to build the PubMed dataset. All titles and
abstracts from articles published in these journals were extracted and used to build the PubMed
knowledge network.
MeSH ID MeSH term
1 D012640 Seizures
2 D004827 Epilepsy
3 D000069279 Drug Resistant Epilepsy
4 D004828 Epilepsies, Partial
5 D020936 Epilepsy, Benign Neonatal
6 D004829 Epilepsy, Generalized
7 D004834 Epilepsy, Post-Traumatic
8 D020195 Epilepsy, Reflex
9 D000073376 Epileptic Syndromes
10 D000080485 Sudden Unexpected Death in Epilepsy
11 D000078306 clobazam (onfi)
12 D000077287 levetiracetam (keppra)
13 D000077213 lamotrigine
14 D000078334 lacosamide
15 D002220 carbamazepine
16 D003975 diazepam (valium)
17 D000078330 xcarbazepine
Table S7: List of epilepsy-focused PubMed MeSH terms used to build the PubMed dataset. All
titles and abstracts from articles containing these MeSH terms were extracted and used to build
the PubMed knowledge network.
10Drug name
1 Levetiracetamum
2 Valium
3 Levetiracetam
4 Diazepam
5 Clobazam
6 Keppra
7 Oxcarbazepine
8 SPM927
9 Carbamazepen
10 Erlosamide
11 Harkoseride
12 Lamotriginum
13 Lamotrigina
14 Carbamazepina
15 Lamotrigine
16 Carbamazepinum
17 Carbamazepine
18 Diastat
19 Carbamaz´epine
20 Carbamazepin
21 Vimpat
22 Lamictal
23 Lacosamide
24 Onfi
Table S8: List of drug names used to retrieve PubMed articles to build the PubMed dataset. All
titles and abstracts from articles containing any of these drug names were extracted and used to
build the PubMed knowledge network.
Original Refined
Rank Parent term Eigen-centrality Parent term Eigen-centrality
1 Convulsion 0.595 Convulsion 0.599
2 Epilepsy 0.562 Epilepsy 0.573
3 Electroencephalogram 0.283 Electroencephalogram 0.282
4 Surgery 0.166 Surgery 0.170
5 Partial seizures 0.158 Partial seizures 0.157
6 Valproic Acid 0.116 Status epilepticus 0.112
7 Carbamazepine 0.114 Valproic Acid 0.109
8 Status epilepticus 0.111 Hippocampus 0.103
9 Hippocampus 0.108 Carbamazepine 0.102
10 Childhood 0.103 Childhood 0.099
11 Temporal lobe epilepsy 0.092 Temporal lobe epilepsy 0.093
12 Phenytoin 0.091 Phenytoin 0.080
13 Diazepam 0.081 Agitation 0.078
14 Agitation 0.078 Diazepam 0.070
15 Injection 0.072 Grand mal convulsion 0.067
16 Phenobarbital 0.070 Injection 0.066
17 Grand mal convulsion 0.068 Nervousness 0.065
18 Nervousness 0.067 Levetiracetam 0.064
19 Death 0.065 Depression 0.064
20 Lamotrigine 0.061 Phenobarbital 0.062
Table S9: Top 20 highest eigenvector centrality terms of the epilepsy related PubMed abstracts
knowledge networks built with the original (left) and the refined (right) dictionary.
11Table S10: Fagin’s generalized Kendall’s distance, (missing pair penalty parameter, p = 0.5) for
the top k highest eigenvector centrality terms comparing the PubMed co-mention network built
with the original versus the refined dictionary. The last column is the distance normalized by the
maximum possible distance so the value range is [0,1].
k generalized Kendall’s distance normalized distance
10 3 0.067
20 82 0.355
50 300 0.218
100 1324 0.238
200 3816 0.177
500 19594 0.147
B.7 Clinical Trials
This section presents the knowledge network results for the Clinical Trials (CT) data source. All
clinical trials were downloaded from clinicaltrials.gov/ and then filtered if they met any of the
following criteria: (i) the CT conditions field include the word “epilepsy”; (ii) the CT intervention
fieldincludedoneofthesevendrugsknowntotreatepilepsy: Clobazam,Levetiracetam,Lamotrigine,
Lacosamide, Carbamazepine, Diazepam, and Oxcarbazepine; (iii) the CT keywords field included
thewords“epilepsy”,“Anti-epilepticdrug”,or“Seizure”;(iv)theCTdescription,title,orsummary
fields included any of the drug names listed in table S8.
We matched our dictionary to the full text description of each clinical trial record.
Top eigenvector centrality terms can be seen in Table S11 , before and after dictionary refine-
ment.
B.8 Dictionary refinement impacts on different data sources
By comparing Table 6, Table S3, Table S5 , Table S10 and Table S12, it is clear that the rank
difference introduced by dictionary refinement is mostly significant in Instagram dataset, least
(nearly no difference) in EFF and Clincal Trials dataset. This demonstrates the need for dictionary
refinement for each individual data source. It also shows EFF as a focused social platform for
epilepsy, has a high quality data with much less non-epilepsy-related content.
12Original Refined
Rank Parent term Eigen-centrality Parent term Eigen-centrality
1 Epilepsy 0.578 Epilepsy 0.579
2 Convulsion 0.577 Convulsion 0.577
3 Electroencephalogram 0.251 Electroencephalogram 0.251
4 Partial seizures 0.197 Partial seizures 0.197
5 Surgery 0.163 Surgery 0.163
6 Levetiracetam 0.158 Levetiracetam 0.158
7 Depression 0.126 Depression 0.126
8 Valproic Acid 0.090 Valproic Acid 0.090
9 Lamotrigine 0.089 Lamotrigine 0.089
10 Anxiety 0.087 Anxiety 0.087
11 Carbamazepine 0.086 Carbamazepine 0.086
12 Investigation 0.082 Investigation 0.082
13 Blindness 0.081 Blindness 0.081
14 Agitation 0.079 Agitation 0.079
15 Childhood 0.068 Childhood 0.068
16 Topiramate 0.065 Topiramate 0.065
17 Temporal lobe epilepsy 0.065 Temporal lobe epilepsy 0.065
18 Weight 0.062 Weight 0.062
19 Prophylaxis 0.060 Prophylaxis 0.060
20 Injury 0.058 Injury 0.058
Table S11: Top 20 highest eigenvector centrality terms of the epilepsy clinical trials documents
knowledge networks built with the original (left) and the refined (right) dictionary.
Table S12: Fagin’s generalized Kendall’s distance, (missing pair penalty parameter, p = 0.5) for
the top k highest eigenvector centrality terms comparing the Clinical Trials co-mention network
built with the original versus the refined dictionary. The last column is the distance normalized by
the maximum possible distance so the value range is [0,1].
k generalized Kendall’s distance normalized distance
10 0 0.000
20 0 0.000
50 82 0.064
100 137 0.027
200 270 0.013
500 1715 0.014
13C Comparison with OpenAI’s GPT series models on term anno-
tation
C.1 the prompt we used for GPT-4
As part of a study aimed at identifying potentially mislabeled terms
by an automated term tagging system, you are tasked with
classifying social media text excerpts where a term potentially
relevant to epilepsy population text analysis has been pre-tagged
. We have built a dictionary for this social media text analysis.
We have included many medical terms that come from MedDRA and
other medical dictionaries. Additionally, as this analysis could
potentially uncover hidden correlations between diet, daily
habits, and epilepsy conditions, we have also included many terms
commonly used in daily English, such as common foods.
The automated tagging system does not recognize multiple meanings of
terms. In some texts, a tagged term is used with the meaning for
which we added the term to the dictionary; in others, it is not.
Your task is to determine whether the tagged term is used with
the epilepsy text analysis related meaning for which we included
it in the dictionary. To help you discern which meaning is
relevant, we will provide you with the "parent_term" and "type"
fields in the dictionary.
For each text entry, you will be provided with the following fields:
- ‘post_text‘: The full text of the social media post with the pre-
tagged term surrounded by asterisks (e.g., *term*).
14- ‘matched_token‘: The specific term that has been pre-tagged and
requires classification.
- ‘parent_term‘: A synonym of the tagged term that has the same
meaning for the intended meaning or a term whose meaning is
closely related to the intended meaning.
- ‘type‘: The type of term, such as allergen, drug, medical term, or
natural product.
Here are a few examples to help you understand: the term "Mary Jane"
in our dictionary has a "parent_term" of "Cannabis" and a "type"
of "Natural Product". If "Mary Jane" is used in the post to mean
Cannabis, then the term is used with the intended meaning and
the auto-tagging system has correctly identified this term.
However, if "Mary Jane" is used as a person’s name in the text,
it does not carry the intended meaning of "Cannabis". Another
example, the term "Cold", when used in "I got a bad cold", aligns
with its parent term "Nasopharyngitis", but in "It is a cold day
", it is used with an irrelevant meaning. The term "Orange" has a
type of "Allergen", so whenever it is used to describe the
citrus fruit (regardless of whether the person is allergic to
orange), it is used with the intended meaning. However, the
tagging should be considered incorrect if it is used to describe
a color.
Based on the information provided and the context of use in the ‘
post_text‘, determine whether the ‘matched_token‘ is a "True
Positive" or a "False Positive" using the criteria below:
1. **True Positive**:
15- The ‘matched_token‘ is accurately related to its ‘parent term‘
and ‘type‘ within the context of the ‘post_text‘.
- For "Allergen" type, if the term is a food name, as long as the
term does mean the food that somebody could be allergic to,
no matter whether the post text contains allergic reactions,
it is a "True Positive".
2. **False Positive**:
- The usage of the ‘matched_token‘ does not align with its ‘type‘
or ‘parent_term‘ in the context of ‘post_text‘.
- If the term is used metaphorically or as a proper noun, it is
most likely to be used with an unrelated meaning.
3. **Uncertain Classification**:
- If the context or the information provided is insufficient to
classify the ‘matched_token‘ definitively, classify it as "
Uncertain"
- Explain the reasons for uncertainty and what additional
information might assist in classification.
4. **Translation and Cultural Considerations**:
- If the ‘post_text‘ contains language or cultural references
that require interpretation, use your multilingual
capabilities and cultural knowledge to inform your
classification.
**Classification Input Example**:
16- ‘post_text‘: "I took *Valerian* to help with my epilepsy, and it
calmed my nerves."
- ‘matched_token‘: Valerian
- ‘parent term‘: Valerian
- ‘type‘: Drug
- **Classification**: [Your Analysis and Classification Here]
Respond with JSON format with the following schema:
{"token_class": $your_classification, "reason": $your_justification}
$your_classification should be one of "True Positive", "False
Positive" or "Uncertain".
You can provide a brief justification for your decision in
$your_justification.
C.2 compare GPT-4’s annotation with human annotators
Table S13: The contingency table between GPT-4’s labels and the consensus labels of the two
human annotators on the annotation for 1500 tagged terms. In this table, if any of the two
annotators give “uncertain” answer, or the two annotators did not return the same label, we label
the tagged term as “uncertain”.
human: match human: mismatch human: uncertain
GPT-4: match 818 10 83
GPT-4: mismatch 220 193 170
GPT-4: uncertain 3 0 3
Table S14: The contingency table between GPT-4’s labels and the master annotator’s labels on
the annotation for 1500 tagged terms. The master annotator is the most experienced annotator.
master: match master: mismatch master: uncertain
GPT-4: match 869 20 22
GPT-4: mismatch 290 226 67
GPT-4: uncertain 4 0 2
17Table S15: The contingency table between master annotator’s labels and the other annotator’s
labels of the two human annotators on the annotation for 1500 tagged terms. Each tagged term
was annotated by two annotators and one of them is our master annotator for the data in this
table. The annotator 2 in this table is not a sole annotator but several annotators. Therefore this
tableshowsthedisgreementbetweenthemasterannotatorandagroupofotherhumanannotators.
annotator 2: match annotator 2: mismatch annotator 2: uncertain
master: match 1041 75 47
master: mismatch 29 203 14
master: uncertain 61 22 8
C.3 GPT-3.5’s annotation comparison with human annotators
As mentioned in the main text, we only use 200 term matches to evaluate the performance of
GPT-3.5. As shown in Table S16 and Table S17, GPT-3.5’s performance is significantly worse than
that of GPT-4, whose performance is shown in Table 8 and Table S13.
human : match human : mismatch human : uncertain
GPT-3.5: match 52 1 2
GPT-3.5: mismatch 60 42 32
GPT-3.5: uncertain 7 0 4
Table S16: The contingency table between GPT-3.5’s labels and the consensus of two humane
annotators’ labels on the annotation for 1500 tagged terms.
master: match master: mismatch master: uncertain
GPT-3.5: match 54 1 0
GPT-3.5: mismatch 70 49 15
GPT-3.5: uncertain 9 0 2
Table S17: The contingency table between GPT-3.5’s labels and the master annotator’s labels on
the annotation for 1500 tagged terms.
18