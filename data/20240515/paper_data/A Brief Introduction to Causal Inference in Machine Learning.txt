A Brief Introduction to
Causal Inference in Machine Learning
Kyunghyun Cho
New York University & Genentech
2024
4202
yaM
41
]GL.sc[
1v39780.5042:viXra2
PREFACE
This is a lecture note produced for DS-GA 3001.003 “Special Topics in DS
- Causal Inference in Machine Learning” at the Center for Data Science, New
York University in Spring, 2024. This course was created to target master’s
and PhD level students with basic background in machine learning but who
were not exposed to causal inference or causal reasoning in general previously.
In particular, this course focuses on introducing such students to expand their
view and knowledge of machine learning to incorporate causal reasoning, as
this aspect is at the core of so-called out-of-distribution generalization (or lack
thereof.)
Thislecturenotedoesnotfollowatraditionalcurriculumforteachingcausal
inference.Thislecturenotedoesnotsubscribesolelytoeitherthepotentialout-
come framework or the do-calculus framework, but is rather flexible in taking
concepts and ideas from these two camps (which after all do look more or less
the same) in order to build up the foundation of causal inference from the first
principles.Indoing so,the firsthalfofthis note coversa varietyofbasictopics,
including probabilisticgraphicalmodels,structuralcausalmodels,causalquan-
titiesofinterest,conditionalvs.interventionalprobabilities,regression,random-
izedcontrolledtrials,banditalgorithms,inverseprobabilityweighting,matching
and instrumental variables. I do not go too deep into each of these topics, al-
though the emphasis is given to how these topics are all connected with each
other(andsometimesareequivalent.)Forthisfirsthalfofthecourse,Ireadand
consulted the following books (lightly only, though) and recommend students
go deeper into these books if they are interested in learning more about causal
inference:
1. Pearl. Causality. 2nd eds. 2009. [Pearl, 2009]
2. Imbens & Rubin. Causal inference in statistics, social, and biomedical
sciences. 2015. [Imbens and Rubin, 2015]
3. Cunningham. Causal Inference: the Mixtape. 2021. [Cunningham, 2021]
Based on the foundation built in the first half (or more like two thirds) of
thecourse,thecoursetakesaturntowardgeneralizationinmachinelearning.In
particular,Itrytoarguethattheprobabilisticgraphicalmodelbasedframework
fromcausalinferencecanbeaninvaluabletoolforspecifyingandunderstanding
so-called out-of-distribution generalization. I draw (coarse) connections from
causalinference to the following ideas in machine learning, to demonstrate this
point:
1. Distributional shifts
2. The principle of invariance
3. Preference-based learning for language models
To be very honest, this is a very thin lecture note for a course with a very
thin content. This note should be considered as the very first sign post at the
entrance to a huge forest called causality, and nothing more. If you want to3
expand slightly a bit more, see this short introductory material I have written
together with my PhD student, Jiwoong Daniel Im [Im and Cho, 2023].
Finally, I am infinitely grateful to Daniel Im, Divyam Madaan and Taro
Makino for helping me as amazing teaching assistants in preparing the lecture
noteaswellasgivingthelabsessionsinSpring2024.Thelabmaterialstheyhave
preparedareallavailableathttps://github.com/kyunghyuncho/2024-causal-inference-machine-learning.4Contents
1 Probabilistic Graphical Models 1
1.1 Probababilistic Graphical Models . . . . . . . . . . . . . . . . . . 2
1.2 Structural Causal Models . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Learning and a generative process . . . . . . . . . . . . . . . . . 5
1.4 Latent variable models are not necessarily causal models . . . . . 7
1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2 A Basic Setup 11
2.1 Correlation, Independence and Causation . . . . . . . . . . . . . 11
2.2 Confounders, Colliders and Mediators . . . . . . . . . . . . . . . 12
2.3 Dependence and Causation . . . . . . . . . . . . . . . . . . . . . 16
2.4 Causal Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.5 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3 Active Causal Inference 21
3.1 Causal Quantities of Interest . . . . . . . . . . . . . . . . . . . . 22
3.2 Regression: Causal Inference can be Trivial . . . . . . . . . . . . 23
3.3 Randomized Controlled Trials . . . . . . . . . . . . . . . . . . . . 24
3.3.1 The Basic Foundation . . . . . . . . . . . . . . . . . . . . 24
3.3.2 Important Considerations . . . . . . . . . . . . . . . . . . 26
3.4 Causal Inference vs. Outcome Maximization . . . . . . . . . . . . 28
3.5 When Some Confounders are Observed. . . . . . . . . . . . . . . 32
3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4 Passive Causal Inference 37
4.1 Challenges in Randomized Controlled Trials . . . . . . . . . . . . 37
4.2 When Confounders were also Collected . . . . . . . . . . . . . . . 38
4.2.1 Inverse Probability Weighting . . . . . . . . . . . . . . . . 38
4.2.2 Matching. . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.3 Instrumental Variables: When Confounders were not Collected . 42
4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
56 CONTENTS
5 Causality and Machine Learning 51
5.1 Out-of-Distribution Generalization . . . . . . . . . . . . . . . . . 51
5.1.1 Setup: I.I.D.. . . . . . . . . . . . . . . . . . . . . . . . . . 51
5.1.2 Out-of-Distribution Generalization . . . . . . . . . . . . . 53
5.1.3 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . 55
5.2 Invariance: Stable Correlations are Causal Correlations . . . . . . 58
5.2.1 An Environment as a Collider . . . . . . . . . . . . . . . . 59
5.2.2 The Principle of Invariance . . . . . . . . . . . . . . . . . 60
5.3 Prediction vs. Causal Inference . . . . . . . . . . . . . . . . . . . 64
5.4 A Case Study: Language Modeling with Pairwise Preference . . . 66
5.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6 Remaining Topics 71
6.1 Other Techniques in Causal Inference . . . . . . . . . . . . . . . 71
6.1.1 Difference-in-Difference . . . . . . . . . . . . . . . . . . . 72
6.1.2 Regression Discontinuity . . . . . . . . . . . . . . . . . . . 73
6.1.3 Double Machine Learning . . . . . . . . . . . . . . . . . . 74
6.2 Behaviour Cloning from Multiple Expert Policies Requires a World Model 75
6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79Chapter 1
Probabilistic Graphical
Models
The goal of causal inference is to figure out a causal relationship, or the lack
thereof, between two sets of variables; causes and outcomes. It is thus only
naturaltothink ofhowwe determinewhetheranyparticularvariableis acause
or an outcome. It is often relatively more straightforward to determine what
an outcome variable is, as this determination is done based on our subjective
interest. For instance, a typical outcome variable in medicine is a disease-free
survival rate within some years since diagnosis or treatment. This choice is
natural, since this variable, or its quantity, is what we want to maximize. It is
however much less clear how to determine which variable should be considered
a cause. For instance, in the classical example of ‘smoking causes lung cancer’,
whatmakesus choose‘whethersomeonesmokesciagarettes’asacausevariable
rather than ‘a mutation in a particular gene’? It becomse even more mind-
boggling once we realize that this choice of ‘smoking’ as a cause meant that we
decided to ignore many variables, such as ‘whether a farmer decided to grow
tobacco’.
It is thus an important, if not the most important, job for practitioners
of causal inference to convincingly argue why some variables are included and
others were omitted. They also must argue why some of the included variables
are considered potential causes and why they chose a particular variable as an
outcome. This process can be thought of as defining a small universe in which
causal inference must be performed. There are many different ways to define
and describe such a universe, and in this lecture note, we largely stick to using
a probabilistic graphical model, or a corresponding structural causal model, as
a way to describe each universe, which is the main topic of this chapter.
12 CHAPTER 1. PROBABILISTIC GRAPHICAL MODELS
1.1 Probababilistic Graphical Models
Inthis course,we relyonprobabilisticgraphicalmodels quite extensivelyinor-
dertodescribetheirstatisticalandcausalrelationshipsamongrandomvariables.
A probabilistic graphical model, which is also referred as a Bayesian graphical
model,is adirectedgraphG=(V,E),whereV isasetofvertices/nodesandE
is a set of directed edges. Each node v V corresponds to a random variable,
∈
andeachedgee=(v ,v )representsthedependence ofv onv .Letusfornow
s e e s
assume thatthis graphis acyclic,thatis, there is no cycle within this graph.In
other words, G is a directed acyclic graph, throughout the rest of this course.
For each node v V, we define a probability distribution p (v pa(v)) over
v
∈ |
this variable conditioned on all the parent nodes
pa(v)= v′ V (v′,v) E . (1.1)
{ ∈ | ∈ }
We can then write a joint distribution over all the variables as
p (V)= p (v pa(v)), (1.2)
V v
|
v∈V
Y
followingthechainruleofprobabilities.Whenpa(v)= ,i.e.vdoesnothaveany
∅
incoming edges, we call the associated distribution p (v) a prior distribution.
v
With P a set of all conditional probabilities p ’s, we can denote any proba-
v
bilistic graphical model as a triplet (V,E,P).
From this joint distribution, we can derive all kinds of conditional distri-
butions by marginalizing variables and applying the definition of a conditional
probability.Ifwearenotinterestedinaparticularnodev¯ V,wecanmarginal-
∈
ize out this variable by
p(V v¯ )= p (V). (1.3)
V
\{ }
v¯
X
If v¯is a continuousrandomvariable,we replace with . We canalwaysturn
a joint probability into a conditional probability by
P R
p (V)
V
p(V v˜ v˜)= . (1.4)
\{ }| p (v˜)
v˜
Using the definition of the conditional probability, we can write marginal-
ization in the following way:
p (V)
V
p(V v¯ )= p (v˜)= p(V v¯ )p (v˜). (1.5)
\{ } p (v˜)
v˜
\{ }
v˜
v¯
v˜
v¯
X X
Marginalizationcorresponds to computing the weighted sum of the conditional
probability of the remaining variables according to the marginal probability of
the variable being marginalized.1.1. PROBABABILISTIC GRAPHICAL MODELS 3
Let’s assume we can sample readily from any conditional probability p
v
with v V. We can then draw a sample from the joint distribution readily by
∈
breadth-first sweeping of all the variables. That is,
v˜ p (v p˜a(v)), (1.6)
v
∼ |
wherep˜a(v)= v˜′ p (v′ p˜a(v′))v′ pa(v) .Thisprocedureiscalledancetral
v′
{ ∼ | | ∈ }
sampling and is an exact, unbiased way to sample from this joint distribution.
Ifwesetasideefficiency,ancestralsamplingisanextremelypowerfultool,as
itallowsus to samplefromanymarginaldistributionaswellasany conditional
distribution. In the former case, we simply discard the draws that correspond
to the uninteresting variables (that are being marginalized). In the latter case,
weonly keepsampleswhose values,correspondingto the conditioningvariables
(that are on the right hand side of ), are precisely those that we want the
|
distribution to be conditioned on. Both of these approaches are not efficient,
and it is often much better to use a more sophisticated sampling algorithm,
often based on the idea of Markov Chain Monte Carlos (MCMC).
Any probability distribution can be expressedas a table (though, this table
may have infinitely many rows) that consists of two columns. The first column
takesthe value ofinterestandthe secondcolumnits probability (either density
or mass). The probability function p above works by hashing v into the row
v
index in this table and retrieving the associated probability, i.e. p : V R .
v +
→
This function satisfies the following normalization property:
p (v), if v is discrete.
1=
v∈V v
(1.7)
(Pv∈Vp v(v)dv, if v is continuous.
R
This view allows us to effectively turn a set of samples into the originaldis-
tributionfromwhichthesesamplesweredrawn(ofcourse,withsomevariance.)
Let S = (v˜ ,v˜ ,...,v˜ ) be a multi-set of samples drawn from an unknown
1 2 N
distribution over a discrete variable v, without loss of generality. We can then
recover the original distribution by constructing a table where each row is
N
v, 1(v˜ =v) N , (1.8)
n
, !
n=1
X
with v V. This corresponds to maximum likelihood learning without any
∈
regularization.
In this table, we can think of all these rows’ contents as the parameters
of this model we have built to approximate the underlying distribution from
which S was drawn. This view will come handy later when we discuss using
a deep neural network instead of an explicit table to represent a probability
distribution.
A major downside ofthis explicit table basedapproachis that q (v)=0 for
v
allv S.Suchanextremevalue(the probabilityof0)shouldnotbe usedwhen
6∈
we estimate these probabilities from a small number of data points, since we4 CHAPTER 1. PROBABILISTIC GRAPHICAL MODELS
cannotruleoutthe factthatwesimplydidnotseethatparticularinstance just
because we did not draw enough samples. Furthermore, this prevents us from
properly defining a conditional probability p (v′ v), since
v′
|
p (v′,v)
p (v′ v)= v′,v . (1.9)
v′
| p (v)
v
If v is set such that p(v) = 0, this conditional probability is not well defined.
We thus have to regularize maximum likelihood learning.
Thisprobabilisticgraphicalmodelisagoodwaytoabstractoutsomeofthe
details on how to implement individual probability distributions for studying
causal inference in machine learning, as it frees us from worrying about the
aspectoflearninguntilitisabsolutelynecessary.Learninginthiscontextrefers
toinferringtheunderlyingdistributionfromwhichdatapointsweredrawn,and
thetable-basedapproachaboveisthemostnaiveonethatisnotreallyusefulin
practice.We can howeverfor now assume that this table-basedapproachworks
well and continue studying causal inference with already inferred conditional
distributions.
1.2 Structural Causal Models
Although a directed edge in the probabilistic graphical model looks like it en-
codestheorderinwhichthevariablesaregenerated,itisnotnecessarilysofrom
the perspective of probability. According to the Bayes’ rule,
p (v′ v)p (v)
p (v v′)= v′ | v . (1.10)
v | p (v′)
v′
This implies that we can always flip the arrow of the edge between v and v′
without altering the joint as well as conditional probabilities.1
This lack of direct relevance of the direction of each edge in G to the joint
probabilityraisesalotofconfusion,whenwetrytousetheprobabilisticgraphi-
calmodelinthecontextofcausalinferenceandreasoning.Instead,wemaywant
touseaslightlydifferentwaytoexpressthesamegenerativeprocessunderlying
a given probability graphical model G=(V,E).
We do so by writing the process of sampling a value associated with each
variablev V ratherthanitsdistribution,asthecombinationofadeterministic
∈
function f and external (exogenous) noise ǫ :
v v
v f (pa(v),ǫ ). (1.11)
v v
←
Thissaysthatthevalueofv iscomputedbasedonthevaluesofitsparentnodes
pa(v) and external noise ǫ by the deterministic function f .
v v
This way is much more explicit about the generating process than before,
sincetheuseofthefunctionf clearlysuggeststhatperturbingtheoutputofthe
1Wewillassumefromhereonthatp(x)>0foranyxandp.1.3. LEARNING AND A GENERATIVE PROCESS 5
function would not change the input to the function, although perturbing the
input to the function would change the output. For instance, you can imagine
thatf correspondsto pushing a book onyourdesk using yourhandwith force
v
ofv′andthatvencodesthenewpositionofthebook.ǫ canbeanunknownlevel
v
of friction cause by your earlier (but forgotten) choice of your desk. Changing
force v′ of course affects the new position of the book v together with some
changing level of ǫ , but changing the position of the book would not change
v
the force I have not applied yet to the book.
We call this representation of the generative process a structural causal
model. Similarly to the probabilistic graphical model above, we can represent
any structuralcausal model as a triplet (V,F,U), where V is a set of variables,
F is a set of corresponding functions and U is a set of corresponding noise
variables.
Any structural causal model can be turned into a probabilistic graphical
modelbyusingthechangeofvariables,i.e.,f (pa(v), )andassumingtheknown
v
·
prior distribution over the external noise variable ǫ . Or, more simply, we can
v
do so because we can find a distribution over v f (pa(v),ǫ ).
v v
∼
We can drawsamples fromany givenstructuralcausalmodel, just like with
the probabilistic graphical models above, by ancestral sampling. Once we have
samples from the joint distribution, we can infer various conditional distribu-
tions. Among these, the posterior distribution over the external noise variables
is of a particular interest for the purpose of counterfactual reasoning. Let q(U)
be a distribution that corresponds to all samples of ǫ ’s that led to a particu-
v
lar configuration Vˆ. Then the posterior distribution q(v) can be thought of as
the distribution over the external (uncontrollable) factors that led to the par-
ticular outcome. We then can answer the question what would have happened
to a target variable v had some of the variables were set differently, by fixing
the external factors to follow q(v) and the rest of the variables to the original
values Vˆ. This corresponds to counterfactual reasoning (had I done something
differently, what would have happened?)
1.3 Learning and a generative process
Learning, in machine learning, refers to the process by which we configure a
black box predictive model to capture the predictive relationship between a set
of variables. In the simplest form, let us consider having two variables; input v
and output v′. g is a predictive function that models the relationship between
θ
v and v′ by mapping an instance of v to the corresponding v′, i.e., g (v′) is the
θ
predictionofv′ givenv.θ isacollectionofparametersthatalearningalgorithm
configures to make g as predictive of v given v′ as possible.
Learningstartsfromdatawhichisasetofexamplesdrawnfromanunknown
data generatingprocess . This data generatingprocess canbe describedusing
G
eitheraprobabilisticgraphicalmodelorequivalentlya structuralcausalmodel.
LetD = (v ,v′),...,(v ,v′ ) beourtrainingdataset.Thegoalisthentouse
{ 1 1 N N }
this dataset to infer θ that would make g highly predictive of v given v′.6 CHAPTER 1. PROBABILISTIC GRAPHICAL MODELS
Therearemanywaystomeasurehowpredictiveg isgivenapair(v,v′),but
we use here a log probability:
r(θ;v,v′)=logp(v v′;g (v′)). (1.12)
θ
|
Thismeansthatg (v′)parameterizedaconditionaldistributionoverv givenv′,
θ
orequivalently, g (v′)outputs aconditionaldistributionoverv. Ifthis quantity
θ
is large, it means that v is highly probable under the predictive distribution by
g given v′.
Learning is then to solve the following optimization problem:
N
1
argmax r(θ;v ,v′). (1.13)
θ N n n
n=1
X
Once learning is over, we can readily compute
p(v v′) p(v v′;g (v′))=p(v v′;θ). (1.14)
θ
| ≈ | |
Ifweassumethatp(v v′;θ)isagreatapproximationofp(v v′),wecanusethe
| |
former in place of the latter without too much worry. In other words, learning
corresponds to figuring out a conditional distribution p(v v′) from data. This
|
data was produced from the underlying data generating process which may
G
have more variables in addition to v and v′.
From this description of learning, it is immediately clear how this can be
a replacement of the table-based approach from earlier, and that the table-
basedapproachearlierwasaspecialcaseofthislearning-basedapproach,where
θ corresponded to all the entries within a table. Once we take this learning-
based approach, we can free ourselves from having to explicitly construct a
largetable and canalso use various regularizationtechniques to avoidthe issue
of 0 probability as well as benefit from generalization.
Let =(V,E,P) with v,v′ V and V =V v,v′ . Then,
G ∈ \{ }
p( v,v′ V)
p(v v′)= V { }∪ . (1.15)
| p( v′ V′)
P{v}∪V { }∪
P
Thatis,wemarginalizeoutallvariablesinV andthendivideitbythemarginal
probability of v′ to get the conditional probability of v given v′.
On one hand, this tells us that learning allows us to recover any arbitrary
conditional distribution induced by an underlying data generating process as
long as we have data produced following the same data generating process. On
theotherhand,thisalsotellsusthattherecanbemanydifferentdatagenerating
processes that result in the exactly same conditional distribution given a pair
of variables. In fact, as long as we do not use all variables from the generating
process, there will always be ambiguities that cannot be resolved based on the
learning procedure laid out above.
As an example, consider the following two structural causal models:1.4. LATENTVARIABLEMODELSARENOTNECESSARILYCAUSALMODELS7
Causal Model 1.
v′ ǫ (1.16)
v′
←
vl v′+a+ǫ (1.17)
vl
←
vr v′+b+ǫ (1.18)
vr
←
v vl+vr+ǫ , (1.19)
v
←
where ǫ and ǫ both follow standard Normal distributions ( (0,12). ǫ also
vl vr v
N
follows standard Normal distribution.
Causal Model 2.
v′ ǫ (1.20)
v′
←
vc v′+a+b+ǫ vc (1.21)
←
v vc+ǫ , (1.22)
v
←
where ǫ (0,2).
vc
∼N
Then, p(v v′) = (v;v′ +a+b,3) for both causal models, although these
| N
two models are very different.
Thisambiguityplaysanimportantroleinbothcausalinferenceandso-called
out-of-distributiongeneralization.Wewillstudybothmorecarefullylaterinthe
course.
1.4 Latent variable models are not necessarily
causal models
When we build a predictive model on a pair (v,v′), there are variables that are
leftoutfromtheoriginaldatageneratingprocess.Thoseunusedvariablesmaybe
theonesforwhichwesimplythrewawayobservations,becausetheywerenotin
ourinterest,ortheonesweactuallydonotobserve.Theseunobservedvariables
contribute to the complexity of p(v v′) via the process of marginalization.
|
It is not trivial to build a predictive model, or equivalently in our context a
deep neural network, that outputs a highly complex predictive distribution, as
these complex distributions often do not have easily-parametrizable analytical
forms. In these cases, there are two commonly used approaches; (1) autore-
gressive models and (2) latent variable models. The former relies on the chain
ruleofprobabilitiesanddecomposesthe conditionalprobabilityasaproductof
coordinate-wise conditional probabilities:
d
p(v v′;θ)= p(v v′;θ), (1.23)
i
| |
i=1
Y
wherev =[v ,...,v ].Byassumingthateachcoordinate-wiseconditionalprob-
1 d
ability is of a simpler form, we can still use a simple deep neural network to8 CHAPTER 1. PROBABILISTIC GRAPHICAL MODELS
approximate a highly complex joint conditional probability. Because each con-
ditional probability onthe righthand side is parametrizedwith the same set of
parametersθ,itispossibletobuildsuchanautoregressivemodelforavariable-
sized observation, that is, dim(v) is not fixed a priori. This approach is behind
the recently successful and popular large-scale language models [Brown et al.,
2020].
Unlike autoregressive modeling, latent variable models explicitly introduce
an unobserved variable u that represents the missing portion of the underlying
data generatingprocess.Justlike the missing (unobserved)portion gaverise to
the highly complex predictive distribution via the process of marginalization,
the introduced (anonymous) latent variable u does the same:
p(v v′;θ)= p (u)p(v v′,u;θ). (1.24)
u
| |
u
X
If u is continuous, we replace with .
Becausethismarginalizationisdifficultalmostalways,itisnaturaltoresort
P R
to sampling-based approximation. Because we are often interested in gradient-
based learning, we consider sampling-based gradient approximation:
p (u)p(v v′,u;θ) logp(v v′,u;θ)
logp(v v′;θ)= u | ∇ | (1.25)
∇ | p (u′)p(v v′,u′;θ)
u u′ u |
X
= p(uv,Pv′;θ) logp(v v′,u;θ) (1.26)
| ∇ |
u
X
M
1
logp(v v′,um;θ), (1.27)
≈ M ∇ |
m=1
X
where um is the m-th posterior sample.
Itishoweveraschallengingtocomputetheposteriordistributionp(uv,v′;θ)
|
nortosamplefromit.Itisthusmorecommonthesedaystomaximizethelower
boundtop(v v′;θ)whileamortizingapproximateposteriorinferenceintoasepa-
|
rateneuralnetwork.Thisiscalledavariationalautoencoder[Kingma and Welling,
2013].
Despitetheseeminglysimilarity,theselatentvariablesarenotcloselyrelated
to actual variables in the data generating process. They may or may not. If
they indeed correspond to actual variables in the data generating process that
we simply did not observe nor decided not to use data from, we may be able
to derive conditions under which we can identify these unobserved variables
and their associated distributions from partial data alone. It is however more
commonandrealistictothink oftheselatentvariablesasameansto improving
the expressive power of a deep neural network. In other words, latent variables
are useful even if there are truly two variables, v and v′, in the data generating
process,since true p (v v′) may be complicated onits own.Nevertheless,it is a
v
|
useful tool to model any complex distribution, and thus we have spent a little
bit of time discussing them.1.5. SUMMARY 9
1.5 Summary
In this chapter, we have established the very foundations on which we can
discuss causal inference and its relationships to machine learning in the rest of
the course:
1. Abriefdiscussiononthenecessityofdefiningauniverseoverwhichcausal
inference is performed;
2. Two(equivalent)waystodefineauniverse,probabilisticgraphicalmodels
and structural causal models;
3. What learning is, once the universe is defined.
Basedonthese,webeginourjourneyintocausalinferenceinmachinelearn-
ing.10 CHAPTER 1. PROBABILISTIC GRAPHICAL MODELSChapter 2
A Basic Setup
2.1 Correlation, Independence and Causation
Two random variables, u and v, are independent if and only if
p(u,v)=p(u)p(v). (2.1)
That is, the probability of u taking a certain value is not affected by that of v
taking another value. We say u and v are dependent upon each other when the
condition above does not hold.
In everyday life, we often confuse dependence with correlation, where two
variables are correlated if and only if
cov(u,v)=E[(u µ )(v µ )]>0, (2.2)
u v
− −
where µ = E[u] and µ = E[v]. When this covariance is 0, we say these two
u v
variables are uncorrelated.
Despite our everyday confusion, these two quantities are only related and
notequivalent.Whentwovariablesareindependent,theyarealsouncorrelated,
but when two variables are uncorrelated, they may not be independent. Fur-
thermore, it turned out these two quantities are also only remotely related to
the existence/lack of causation between two variables.
You must have heard of the statement “correlation does not imply causa-
tion.” There are two sides to this statement. First, the existence of correlation
between two variables does not imply that there exists a causal relationship
between these two variables. An extreme example of this is tautology; if the
relationship between u and v is identity, there is no causal relationship but the
correlationbetween these two is maximal.
Second, the lack of correlation does not imply the lack of causation. This
is the more important aspect of this statement. Even if there is no correlation
between two variables, there could be a causal mechanism behind these two
variables. Although it is a degenerate case, consider the following structural
1112 CHAPTER 2. A BASIC SETUP
causal model:
a u+ǫ (2.3)
a
←−
b +u+ǫ (2.4)
b
←
v a+b+ǫ . (2.5)
v
←
The value of v is caused by u via two paths; u a v and u b v, but
→ → → →
these paths cancel each other. If we only observe (u,v) pairs, it is easy to see
that they are uncorrelated, since v is constant. We will have more discussion
later in the semester, but it is good time for you to pause and think whether
these two paths matter, since they cancel each other.
Consider as another example the following structural causal model:
z ǫ (2.6)
z
←
u 0.2z+√1.04ǫ (2.7)
u
←
v 0.1u 0.5z+0.1ǫ , (2.8)
v
← −
where ǫ , ǫ and ǫ are all standard Normal variables. Again, the structural
z u v
causal model clearly indicates that u causally affects v via 0.1u, but the corre-
lationbetweenu andv is 0whenwe considerthose twovariablesalone(thatis,
after marginalizing out z).
This second observation applies equally to independence. That is, the inde-
pendence between two variables does not imply the lack of a causal mechanism
between two variables.The examples above apply here equally, as two uncorre-
lated Normal variables are also independent.
This observation connects to an earlier observation that there are poten-
tially many data generating processes that give rise to the same conditional
distribution between two sets of variables. Here as well, the independence or
correlatednessoftwovariablesmaymapto many differentgeneratingprocesses
thatencodedifferentcausalmechanismsbehindthesevariables.Inotherwords,
wecannotdeterminethecausalrelationshipbetweentwovariables(withoutloss
of generality) without predefining the underlying generating process (in terms
of either the probabilistic graphicalmodel or equivalently the structural causal
model.)1
In other words, we must consider both variables of interest and the associ-
ated data generatingmodel in order to determine whether there exists a causal
relationship between these variables and what that relationship is.
2.2 Confounders, Colliders and Mediators
Letusconsiderasimplescenariowherethereareonlythreevariables;u,vandw.
Weareprimarilyinterestedintherelationshipbetweenthefirsttwovariables;u
1There are algorithms to discover an underlying structural causal model from data, but
these algorithms also require some assumptions such as the definition of the goodness of
a structural causal model. This is necessary, since these algorithms all work by effectively
enumerating all structural causal models that can produce data faithfully and choosing the
bestoneamongthese.2.2. CONFOUNDERS, COLLIDERS AND MEDIATORS 13
andv.Wewillconsidervariouswaysinwhichthesethreevariablesareconnected
with each other and how such wiring affects the relationship between u and v.
Directly connected. Consider the following probabilistic graphical model.
w
u v
w does not affect either u nor v, while u directly causes v. In this case,
the causal relationship between u and v is clear. If we perturb u, it will affect
v, according to the conditional distribution p (v u). This tells us that we can
v
|
ignoreany node in a probabilistic graphicalmodel that is not connected to any
variable of interest.
An observed confounder. Consider the following probabilistic graphical
model, where w is shaded, which indicates that w is observed.
w
u v
In this graph,the value/distributionof u and that ofv are both determined
individually already because we have observedw. This corresponds to the defi-
nition of conditional independence;
p(u,v w)=p(uw)p(v w). (2.9)
| | |
Because the edge is directed from w to u, perturbing u does not change the
observedvalue of w. The same applies to v as well, since perturbing v does not
affect u, since this path between u and v via w is blocked by observing w.
An unobserved confounder. Consider the case where w was not observed.
w
u v14 CHAPTER 2. A BASIC SETUP
We first notice that in general
p(u,v)= p(uw)p(v w)p(w)dw =q(u)q(v), (2.10)
| | 6
Zw
implyingthatuandv arenotindependent, unlikewhenw wasnotobserved.In
other words, u and v are not conditionally independent given w. Perturbing u
howeverstilldoesnotaffectv,sincethevalueofwisnotdeterminedbythevalue
ofuaccordingtothecorrespondingcausalstructuralmodel.Thatis,udoesnot
affect v causally (and vice versa.) This is the case where the independence and
causalitystarttodeviatefromeachother;uandv arenotindependentbuteach
is not the cause of the other. Analogous to the former case of the observed w,
where we say the path u w v was closed, we say that the same path is
← →
open in this latter case.
Because of this effect w has, we call it a confounder. The existence of a
confounder w makes it difficult to tell whether the dependence between two
variables we see is due to a causal relationship between these variables. w con-
founds this analysis.
An observed collider. Consider the following graph where the arrows are
flipped.
w
u v
In general,
p(wu,v)p(u)p(v)
p(u,v w)= | =q(u)q(v), (2.11)
| p(wu,v)p(u)p(v)dudv 6
|
whichmeans that u andvRarRe notindependent conditionedon w. This is some-
times called the explaining-awayeffect, because observing w explains away one
of two potential causes behind w.
Although u and v are not independent in this case, there is no causal rela-
tionship between u and v. w is where the causal effects of u and v collide with
each other (hence, w is a collider) and does not pass along the causal effect
between u and v. Similarly to the case ofan unobservedconfounder above,this
is one of those cases where independence does not imply causation.
We say that the path u w v is open.
→ ←
An unobserved collider. Consider the case where the collider w is not ob-
served.2.2. CONFOUNDERS, COLLIDERS AND MEDIATORS 15
w
u v
By construction, u and v are independent, as
p(u,v)= p(u)p(v)p(wu,v)dw =p(u)p(v) p(wu,v)dw =p(u)p(v). (2.12)
| |
Z Z
Just like before, neither u or v is the cause of the other. This path is closed.
An observed mediator. Consider the case where there is an intermediate
variable between u and v:
u w v
Because
p(u,v w)=p(u)p(wu)p(v w)=q(uw)q(v w) (2.13)
| | | | |
uandvareindependentconditionedonw.However,perturbingudoesnotaffect
v, since the value of w is observed (that is, fixed.) We say that u w v is
→ →
closed in this case, and independence implies the lack of causality.
An unobserved mediator. What if w is not observed, as below?
u w v
It is then clear that u and v are not independent, since
p(u,v)= p(u)p(wu)p(v w)dw =p(u) p(wu)p(v w)dw =p(u)q(v u).
| | | | |
Z Z
(2.14)
Perturbing u will change the distribution/value of w which will consequently
affectthat ofv, meaning thatu causallyaffects v. This effect is mediatedby w,
and hence we call w a mediator.16 CHAPTER 2. A BASIC SETUP
2.3 Dependence and Causation
We can chain the rules that were defined between three variables; u, v and w,
in order to determine the dependence between two nodes, u and v, within any
arbitrary probabilistic graphical model given a set Z of observed nodes. This
procedure is called D-separation, and it tells us two things. First, we can check
whether
u v. (2.15)
Z
⊥⊥
Moreimportantly,however,we getall openpaths betweenu andv. Theseopen
paths areconduits that carrystatistical dependence betweenu andv regardless
of whether there is a causal path between u and v, where we define a causal
path as an open directed path between u and v.2
Dependenciesarisingfromopen,non-causalpathsareoftencasuallyreferred
to as ‘spurious correlation’ or ‘spurious dependency’. When we are performing
causalinference forthepurposeofdesigningacausalinterventioninthe future,
itisimperativetodissectoutthesespuriouscorrelationsandidentifytruecausal
relationshipbetween u and v. It is howeverunclear whether we wantto remove
all spurious dependencies or whether we should only remove spurious depen-
dencies that are unstable, when it comes to prediction in machine learning. We
will discuss more about this contention later in the course.
In this course, we do not go deeper into D-separation. We instead stick to
a simple setting where there are only three or four variables, so that we can
readilyidentifyallopenpathsanddeterminewhicharecausalandwhichothers
are spurious.
2.4 Causal Effects
We have so far avoided defining more carefully what it means for a node u
to effect another node v causally. Instead, we simply said u effects v causally
if there is a directed path from u to v under the underlying data generating
process.This is howeverunsatisfactory,as there are loopholes in this approach.
The mostobviousone is thatsome ofthose directed edgesmay correspondto a
constant function. For instance, an extreme case is where the structural causal
model is
a f (u,ǫ ) (2.16)
a a
←
v f (a,ǫ ), (2.17)
v v
←
where f () = 0. In this case, the edge from u to a is effectively non-existent,
a
·
althoughwewroteitasifitexisted.Rather,wewanttodefine acausaleffectof
uonvbythinkingofhowperturbationonupropagatesoverthedatagenerating
process and arrives at v.
2Unlike a usual path, in a direct path, the directions of all edges must agree with each
other,i.e.,pointingtothesamedirection.2.4. CAUSAL EFFECTS 17
More specifically, we consider forcefully setting the variable u (the cause
variable) to an arbitrary value uˆ. This corresponds to replacing the following
line in the structural causal model G=(V,F,U)
u f (pa(u),ǫ ) (2.18)
u u
←
with
u uˆ. (2.19)
←
uˆ can be a constant or can also be a random variable as long as it is not
dependent on any other variables in the structural causal model.
Oncethis replacementisdone,werunthis modifiedstructuralcausalmodel
G(uˆ)=(V,F,U) in order to compute the following expected outcome:
E [v ], (2.20)
U G(uˆ)
whereU isasetofexogenousfactors(e.g.noise.)Ifudoesnotaffectv causally,
this expected outcome would not change (much) regardless of the choice of uˆ.
As an example, assume u can take either 0 or 1, as in treated or placebo.
We then check the expected treatment effect on the outcome v by
E [v ] E [v ]. (2.21)
U G(uˆ=1) − U G(uˆ=0)
Wewouldwantthisquantitytobepositiveandlargetoknowthatthetreatment
has a positive causal effect on the outcome.
This procedure of forcefully setting a variable to a particular value is called
a do operator. The impact of do is more starkly demonstrated if we con-
sider a probabilistic graphical model rather than a structural causal model.
Let p (upa(u)) be the conditional distribution over u in a probabilistic graph-
u
|
ical model G = (V,E,P). We construct a so-called interventional distribution
as
p(v do(u=uˆ)), (2.22)
|
which states that we are now forcefully setting u to uˆ instead of letting it be a
sampledrawnfromthe conditionaldistributionp (upa(u)). Thatis,insteadof
u
|
u upa(u), (2.23)
∼ |
we do
u uˆ. (2.24)
←
In other words, we replace the conditional probability p (upa(u)) with
u
|
p (upa(u))=δ(u uˆ), (2.25)
u
| −18 CHAPTER 2. A BASIC SETUP
where δ is a Dirac measure, or replace all occurrences of u in the conditional
probabilities of child(u) with a constant uˆ, where
child(u)= u′ V (u,v′) E . (2.26)
{ ∈ | ∈ }
As a consequence,in the new modified graphG, there is no edge coming into u
(or uˆ) anymore, i.e., pa(u) = . u is now independent of all the other nodes a
∅
priori.
Because do modifies the underlying data generating process, p(v u = uˆ;G)
|
and p(v do(u = uˆ);G) = p(v u = uˆ;G) differ from each other. This difference
| |
signifies the separation between statistical and causal quantities. We consider
this separation in some minimal cases.
2.5 Case Studies
An unobserved confounder and a direct connection. Consider the case
where w was not observed.
w
u v
Under this graph G,
p(w)p(uˆw)p(v uˆ,w) p(uˆw)
p (v u=uˆ)= w | | =E | p(v uˆ,w) , (2.27)
G w
| p(uˆ) p(uˆ) |
P (cid:20) (cid:21)
from which we see that there are two open paths between v and u:
1. u v: a direct path;
→
2. u w v: an indirect path via the unobserved confounder.
← →
The statistical dependence between3 u and v flows through both of these
paths, while the causal effect of u and v only flows through the direct path
u v. The application of do(u=uˆ) in this case would severe the edge from w
→
to u, as in
w
uˆ v
3We do not need to specify the direction of statistical dependence, since the Bayes’ rule
allowsustoflipthedirection.2.5. CASE STUDIES 19
Under this modified graph G,
1
p (v do(u=uˆ))=p (v u=uˆ)= p(w)q(uˆ)p(v uˆ,w) (2.28)
G
|
G˜
| q(uˆ) |
w
X
= p(w)p(v uˆ,w)=E [p(v uˆ,w)], (2.29)
w
| |
w
X
where we use q(uˆ) to signify that this is not the same as p(uˆ) above.
Thefirstonep (v u=uˆ)isastatisticalquantity,andwecallitaconditional
G
|
probability.The latterp (v do(u=uˆ))is insteada causalquantity,andwecall
G
|
it an interventional probability. Comparing these two quantities, p (v u = uˆ)
G
|
and p (v do(u = uˆ)), the main difference is the multiplicative factor p(uˆ|w)
G | p(uˆ)
insidetheexpectation.Thenumeratorp(uˆw)tellsushowlikelythistreatmentuˆ
|
wasgivenunderw,whilethedenominatorp(uˆ)tellsushowlikelythetreatment
uˆwasgivenoverall.Inotherwords,weupweightthe impactofuˆandw ifuˆ was
more probable under w than overall. This observation will allow us to convert
between these two quantities later.
An observed collider and a direct connection. Letusfliptheedgesfrom
w sothatthoseedgesaredirectedtowardw.We further assumethatwealways
observe w to be a constant (1).
w =1
u v
The do operator on u does not alter the graph above. This means that the
conditional probability and interventional probability coincide with each other
in this case, conditioned on observing w =1.
Thishoweverdoesnotmeanthattheconditionalprobabilityp(v u=uˆ,w=
|
1), or equivalently the interventional probability p(v do(u = uˆ),w = 1), mea-
|
suresthe causaleffectofuonv alone.As wesawbefore andalsocanseebelow,
there are two open paths, u v and u w v, between u and v through
→ ← →
which the dependence between u and v flows:
✟✟
✟p(u)p(v u)p(w =1u,v)
p(v u,w=1)= ✟✟ | | . (2.30)
| ✟p(u)p(v′ u)p(w =1u,v′)
v′ | |
We must then wonder whethPer we can separate out these two paths from
eachother.Itturnedoutunfortunatelythatthis isnotpossibleinthis scenario,
because we need the cases of w = 1 (e.g. w = 0) for this separation. If you
620 CHAPTER 2. A BASIC SETUP
recall how we can draw samples from a probabilistic graphical model while
conditioning some variables to take particular values, it was all about selecting
asubsetofsamplesdrawnfromthesamegraphwithoutanyobservedvariables.
Thisselectioneffectivelypreventsusfromfiguringouttheeffectofuonv viaw.
We will have more discussion on this topic later in the semester in the context
of invariant prediction.
Becauseofthisinherentchallenge,thatmaynotevenbeaddressableinmany
cases, we will largely stick to the case of having a confounder in this semester.
2.6 Summary
In this chapter, we have learned about the following topics:
1. How to represent a data generating process: a probabilistic graphical
model vs. a structural causal model;
2. How to read out various distributions from a data generating process:
ancestral sampling and Bayes’ rule;
3. The effect of confounders, colliders and mediators on independence;
4. Causal dependency vs. spurious dependency;
5. The do operator.Chapter 3
Active Causal Inference
In this chapter, we assume the following graph G. We use a, y and x, instead
of u, v and w, to denote the action/treatment, the outcome and the covariate,
respectively. The covariate x is a confounder in this case, and it may or may
not observed, depending on the situation.
x
a y
An example case corresponding to this graph is vaccination.
• a: is the individual vaccinated?
• y: has the individual been infected by the target infectious disease with
symptoms, within 6 months of vaccine administration?
• x: the underlying health condition of the individual.
The edge x a is understandable, since we often cannot vaccinate an indi-
→
vidualwithanactiveunderlyinghealthcondition.Theedgex yisalsounder-
→
standable,sincehealthyindividualsmaycontractthediseasewithoutanysymp-
toms, while immunocompromised individuals for instance may show a greater
degreeofsymptomswithahigherchance.Theedgea y isalsoreasonable,as
→
the vaccine must have been developed with the target infectious disease as its
goal.Inotherwords,thisgraphsencodesourstructuralprioraboutvaccination.
Withthisgraphthatencodesthereasonabledatageneratingprocess,causal
inference then refers to figuring out the degree of the causal effect of a on y.
2122 CHAPTER 3. ACTIVE CAUSAL INFERENCE
3.1 Causal Quantities of Interest
In this particular case, we are interested in a number of causal quantities. The
most basic and perhaps most important one is whether the treament is effec-
tive (i.e., results in a positive outcome) generally. This corresponds to checking
whether E[y do(a=1)]>E[y do(a=0)], or equivalently computing
| |
ATE=E[y do(a=1)] E[y do(a=0)], (3.1)
| − |
where
E[y do(a=aˆ)]= yp(y do(a=aˆ)) (3.2)
| |
y
X
= y p(x)p(y aˆ,x)= yE [p(y aˆ,x)]. (3.3)
x∼p(x)
| |
y x y
X X X
In words, we average the effect of aˆ on y over the covariate distribution
but the choice of aˆ should not depend on x. Then, we use this interventional
distribution p(y do(a = aˆ)) to compute the average outcome. We then look at
|
thedifferenceinthe averageoutcomebetweenthe treatmentandnot(placebo),
to which we refer as the average treatment effect (ATE).
It is natural to extend ATE such that we do not marginalize the entire
covariate x, but fix some part to a particular value. For instance, we might
want to compute ATE but only among people in their twenties. Let us rewrite
the covariate x as a concatenation of x and x′, where x′ is what we want to
condition ATE on. That is, instead of p(y do(a = aˆ)), we are interested in
|
p(y do(a=aˆ),x′ =xˆ′). This corresponds to first modifying G into
|
x xˆ′
a y
and then into
x xˆ′
a y
We then get the following conditional average treatment effect (CATE):
CATE=E[y do(a=1),x′ =xˆ′] E[y do(a=0),x′ =xˆ′], (3.4)
| − |3.2. REGRESSION: CAUSAL INFERENCE CAN BE TRIVIAL 23
where
E[y do(a=aˆ),x′ =xˆ′]= yp(y do(a=aˆ),x′ =xˆ′) (3.5)
| |
y
X
= y p(xx′)p(y aˆ,x′ =xˆ′,x) (3.6)
| |
y x
X X
= yE p(y aˆ,x′ =xˆ′,x) . (3.7)
x∼p(x|x′)
|
Xy h i
You can see that this is really nothing but ATE conditioned on x′ =xˆ′.
From these two quantities of interest above, we see that the core question
is whether and how to compute the interventionalprobability of the outcome y
given the intervention on the action a conditioned on the context x′. Once we
cancomputethisquantity,wecancomputervarioustargetquantitiesunderthis
distribution. We thus do not go deeper into other widely used causal quantities
in this course but largely stick to ATE/CATE.
3.2 Regression: Causal Inference can be Trivial
Assume for now that we are given a set of data points drawn from this graph
G:
D = (a ,y ,x ),...,(a ,y ,x ) . (3.8)
1 1 1 N N N
{ }
For every instance, we observe all of the action a, outcome y and covariate x.
Furthermore, we assume all these data points were drawn from the same fixed
distribution
p∗(a,y,x)=p∗(x)p∗(ax)p∗(y a,x) (3.9)
| |
and that N is large.
In this case, we can use a non-parametric estimator, such as tables, deep
neuralnetworks and gradientboosted trees,to reverse-engineereach individual
conditionaldistributionfromthislargedatasetD.Thisisjustlikewhatwehave
discussed earlier in §1.3. Among three conditional distributions above, we are
only interested in learning p∗(x) and p∗(y a,x) from data, resulting p(x;θ) and
|
p(y a,x;θ), where θ refers to the parameters of each deep neural network.1
|
Once learning is over, we can use it to approximate ATE as
ATE yE [p(y a=1,x;θ)] yE [p(y a=0,x;θ)]
x∼p(x;θ) x∼p(x;θ)
≈ | − |
y y
X X
(3.10)
= yE [p(y a=1,x;θ) p(y a=0,x;θ)]. (3.11)
x∼p(x;θ)
| − |
y
X
1Althoughthereisnoreasontopreferdeepneural networks over random forestsorother
non-parametriclearners,wewilllargelysticktodeepneuralnetworks,asIlikethemmore.24 CHAPTER 3. ACTIVE CAUSAL INFERENCE
Therearetwoconditionsthatmakethisregression-basedapproachtocausal
inference work:
1. No unobserved confounder: we observe the covariate x in G;
2. LargeN:wehaveenoughexamplestoinferp∗(y a,x)withalowvariance.
|
Ifthereisanydimensionofxthatisnotobservedinthedataset,itisimpos-
sible for any learner to infer neither p∗(y a,x) nor p∗(x) correctly. “Correctly”
|
here in particular refers to identifying the true p∗(y a,x). This is not really im-
|
portant if the goal is to approximate the conditional probability p∗(y a), since
|
we can simply drop x and use (a,y) pairs. It is however critical if the goal is to
approximatetheinterventionalprobaiblityp∗(y do(a))becausethisnecessitates
|
us to access p∗(y a,x) (approximately).
|
Large N is necessary for two reasons. First, the problem may be ill-posed
when N is small. Consider rewriting p(y a,x) as
|
p(y,a,x)
p(y a,x)= . (3.12)
| p(ax)p(x)
|
This quantity has in the denominator both p(ax) and p(x). If N is small to
|
the point that we do not observe all possible combination of (a,x) for which
p(x) > 0, this conditional probability is not well-defined. This connects to one
of the major assumptions in causal inference, called positivity, which we will
discuss further later in the semester.
The second, perhaps less important, reason is that the variance of the es-
timator is often inversely proportional to N. That is, with more N, we can
approximatep∗(y a,x) with less variance.The variance of this estimate is criti-
|
cal,asitdirectlyleadstothatofATE.IfthevarianceofATEishigh,wecannot
draw a confident conclusion whether the treatment is effective.
This sectiontells us thatcausalinference canbe done triviallyby statistical
regressionwhen the following conditions are satisfied:
1. There are no unobserved confounder: We observe every variable.
2. Positivity: All possible combinations of (a,y,x) are observed in data.
3. We have enough data.
Unfortunately, it is rare that all these conditions are satisfied in real life.
3.3 Randomized Controlled Trials
3.3.1 The Basic Foundation
In this section, we consider the case where there are unobserved confounders.
In such a case, we cannot rely on regression, as these unobserved confounders
prevent a learner from identifying p∗(y a,x) correctly. One may tempted to
|3.3. RANDOMIZED CONTROLLED TRIALS 25
simply fit a predictive model on (a,y) pairs to get p(y a;θ) to approximate
|
p∗(y a) and call it a day. We have however learned earlier that this does not
|
approximate the causal effect, that is, p∗(y do(a)), due to the spurious path,
|
a x y, which is open because we did not observe x.
← →
When there are unobserved confounders, we can actively collect data that
allows us to perform causalinference. This is a departure from a usual practice
in machine learning, where we often assume that data is provided to us and
the goal is for us to use a learning algorithm to build a predictive model. This
is often not enough in causal inference, and we are now presented with the
first such case, where the assumption of ‘no unobserved confounder’ has been
violated.
In order to estimate the causal effect of the action a on the outcome y, we
severed the edge from the confounder x to the action a, as in
x x
a y → a y
G G
ThissuggeststhatifwecollectdataaccordingtothelattergraphG,wemay
beabletoestimatethecausaleffectofaony despitetheunobservedconfounder
x. To do so, we need to decide on the prior distribution over the action, p(a),
suchthata is independent ofthe covariatex.It isa commonpracticeto choose
a uniform distribution over the action as p(a). For instance, if a is binary,
p(a=0)=p(a=1)=0.5. (3.13)
The priordistributionoverthe covariatex, p(x), is notwhatwechoose,but
iswhattheenvironmentis like.Thatis,insteadofspecifyingp(x) norsampling
explicitly from it (which was by assumption impossible), we go out there and
recruit samples from this prior distribution. In the vaccination example above,
this would correspondto recruiting subjects from a generalpopulationwithout
any particular filtering or selection.2
For each recruited subject x, we assign the treatment a, drawn from p(a)
that is independent of x. This process is called ‘randomization’, because this
process assigns an individual drawn from the population, x p(x), randomly
∼
to either a treatment or placebo group according to p(a), where ‘randomly’
refer to ‘without any information’. This process is also ‘controlled’, because we
control the assignment of each individual to a group.
For the randomly assigned pair (a,x), we observe the outcome y by letting
the environment (nature) simulate and sample from p∗(y a,x). This process is
|
a ‘trial’, where we try the action a on the subject x by administering a to
2Ofcourse,wecanfilterthesesubjectstosatisfyacertainsetofcriteria(inclusioncriteria)
inordertoestimateaconditional averagetreatmenteffect.26 CHAPTER 3. ACTIVE CAUSAL INFERENCE
x. Putting all these together, we call this process of collecting data from G a
randomized controlled trial (RCT).
Itisimportanttoemphasizethatxisnotrecorded,fully knownnorneeded,
and we end up with
D = (a ,y ),...,(a ,y ) . (3.14)
1 1 N N
{ }
Using this dataset, we can now approximate the interventional probability as
E [y do(a=aˆ)]=E [y a=aˆ] (3.15)
G | G |
✟✟
p(x)✟p(aˆ)p(y aˆ,x)
= y x ✟✟ | (3.16)
✟p(aˆ)
y P
X
= yp(x)p(y aˆ,x) (3.17)
|
x y
XX
N 1(a =aˆ)y
n=1 n n , (3.18)
≈ N 1(a =aˆ)
Pn′=1 n′
because x p(x), a p(a) and y Pp(y aˆ,x).
n n n
∼ ∼ ∼ |
As evident from the final line, we do not need to know the confounder x,
which means that RCT avoids the issue of unobserved, or even unknown, con-
founders. Furthermore, it does not involve p(a), implying that we can use an
arbitrarymechanism to randomly assigneachsubjectto a treatment option,as
longaswedonotconditionitontheconfounderx.Ifwehavestrongconfidence
in the effectiveness of a newly developed vaccine, for instance, we wouldchoose
p(a) to be skewed toward the treatment (a=1).
3.3.2 Important Considerations
Perhapsthemostimportantconsiderationthatmustbegivenwhenimplement-
ing a randomized controlled trial is to ensure that the action a is independent
of the covariate x. As soon as dependence forms between a and x, the esti-
matedcausaleffectE [y do(a=aˆ)]becomesbiased.Itishoweververyeasyfor
G
|
suchdependency to arisewithout a carefuldesignof anRCT, oftendue to sub-
conscious biases formed by people who implement the randomized assignment
procedure.Forinstance,inthecaseofthe vaccinationtrialabove,adoctormay
subconsciously assign older people less to the vaccination arm3 simply because
sheissubconsciouslyworriedthatvaccinationmayhavemoreseveresideeffects
on an older population. Such a subconscious decision will create a dependency
between the action a and the covariate x (the age of a subject in this case),
whichwillleadto the biasinthe eventualestimate ofthe causaleffect.Inorder
to avoid such a subconscious bias in assignment, it is a common practice to
automate the assignmentprocessso that the trialadministratoris not awareof
to which action group each subject was assigned.
3An‘arm’herereferstoa‘group’.3.3. RANDOMIZED CONTROLLED TRIALS 27
Second, we must ensure that the causal effect of the action a on the out-
comey muststayconstantthroughoutthe trial.Moreprecisely,p∗(y a,x) must
|
not change throughout the trial. This may sound obvious, as it is difficult to
imagine how for instance the effect of vaccination changes rapidly over a single
trial. There are however many ways in which this does not hold true. A ma-
jor way by which p∗(y a,x) drifts during a trial is when a participant changes
|
theirbehavior.Continuingtheexampleofvaccinationabove,letusassumethat
each participant knows that whether they were vaccinated and also that the
pandemic is ongoing.Once the participant knows that they were given placebo
instead of actual vaccine, they may become more careful about their hygiene,
as they are worriedabout potential contracting the rampant infectious disease.
Similarly, if they knew they were given actual vaccine, they may become less
careful and expose themselves to more situations in which they could contract
thedisease.Thatis,thecausaleffectofvaccinationchangesduetothealteration
ofparticipants’behaviours.Itisthusacommonandimportantpracticetoblind
participants from knowing to which treatment groups they were assigned. For
instance, in the case of vaccination above, we would administer saline solution
viainjectiontocontrol(untreated)participantssothattheycannottellwhether
they are being injected actual vaccine or placebo.
Putting these two considerations together, we end up with a double blind
trial design. In a double blind trial design, neither the participant nor the trial
administrator is made aware of their action/treatment assignment. This helps
ensure that the underlying causaleffect is stationarythroughoutthe study and
that there is no bias creeping in due to the undesirable dependency of the ac-
tion/treatmentassignmentonthecovariate(informationabouttheparticipant.)
ThefinalconsiderationisnotaboutdesigninganRCTbutaboutinterpreting
the conclusionfrom an RCT. As we saw above,the causal effect from the RCT
based on G is mathematically expressed as
E [y do(a=aˆ)]= yp(x)p(y aˆ,x). (3.19)
G
| |
x y
XX
The right hand side of this equation includes p(x), meaning that the causal
effect is conditioned on the prior distribution over the covariate.
Wedonothavedirectaccesstop(x),butwehavesamplesfromthisdistribu-
tion,intheformofparticipantsarrivingandbeing includedintothe trial.That
is,we have implicit access to p(x). This implies that the estimated causaleffect
wouldonly be validwhen itis usedfor apopulationthatcloselyfollowsp(x). If
there is any shift in the population distribution itself or there was any filtering
applied to participants in the trialstage that does not apply after the trial, the
estimated causal effect would not be valid anymore. For instance, clinical tri-
als, such as the vaccination trial above, are often run by research-oriented and
financially-stableclinicswhichareoftenlocatedinaffluentneighbourhoods.The
effect of the treatment from such a trial is thus more precise for the population
in such affluent neighbourhoods. This is the reason why inclusion is important
in randomized controlled trials.
Overall, a successful RCT requires the following conditions to be met:28 CHAPTER 3. ACTIVE CAUSAL INFERENCE
1. Randomization: the action distribution must be independent of the co-
variate.
2. Stationarity of the causaleffect: the causaleffect must be stable through-
out the trial.
3. Stationarityofthe popluation:the covariatedistributionmustnotchange
during and after the trial.
Aslongasthesethreeconditionsaremet,RCTprovidesuswithanopportunity
to cope with unobserved confounders.
3.4 Causal Inference vs. Outcome Maximization
Beside curiosity, the goal of causal inference is to use the inferred causal rela-
tionship for better outcomes in the future. Once we estimate E [y do(a)] using
G
|
RCT, we would simply choose the following action for all future subject:
aˆ=argmaxE [y do(a)], (3.20)
G
a∈A |
where isasetofallpossibleactions.Thisapproachhashoweveronedownside
A
that we had to give an incorrect treatment (e.g. placebo) to many trial partic-
ipants who lost their opportunities to have a better outcome (e.g. protection
against the infectious disease.)
Consider an RCT where subjects arrive and are tested serially, that is, one
at a time. If t subjects have participated in the RCT so far, we have
D = (a ,y ),...,(a ,y ) . (3.21)
1 1 t t
{ }
Based on D, we can estimate the outcome of each action by
t 1(a =a)y
yˆ(a)= t′=1 t′ t′ . (3.22)
t N 1(a =a)
P t′′=1 t′′
This estimate would be unbiased P(correct on average), if every a was drawn
t′
fromanactiondistributionthatisindependent ofthe covariatex fromaniden-
tical distribution q(a).4 More generally, the bias (the degree of incorrectness)
would be proportional to
t
1
ǫ = 1(a was drawn independently of x ). (3.23)
≤t−1
t
t′ t′
t′=1
X
If ǫ = 1, the estimate is unbiased, corresponding to causal inference. If
≤t−1
ǫ =0, what we have is not interventional but conditional.
≤t−1
4ThisisanotherconstraintonRCT,thateverysubjectmustbeassignedaccordingtothe
sameassignmentpolicyq(a).3.4. CAUSAL INFERENCE VS. OUTCOME MAXIMIZATION 29
Assuming ǫ 0 and t 1, we have a reasonable causal estimate of the
t
≫ ≫
outcome y given each action a. Then, in order to maximize the outcome of the
next subject (x ), we want to assign them to an action sampled from the
t+1
following Boltzmann distribution:
exp 1 yˆ(a)
q (a)=
βt t
, (3.24)
t (cid:16) (cid:17)
exp 1 yˆ(a′)
a′∈A βt t
(cid:16) (cid:17)
P
where β [0, ) is a temperature parameter.
t
∈ ∞
When β (a high temperature), this is equivalent to sampling the
t
→ ∞
actiona froma uniform distribution,which implies that we do not trust the
t+1
causal estimates of the outcomes, perhaps due to small t. On the other hand,
when β 0 (a low temperature), the best-outcome action would be selected,
t
→
as
1, if yˆ(a)=max yˆ(a′)
a′ t
q (a)= (3.25)
t β→∞
(0, otherwise
assuming there is a unique action that leads to the best outcome. In this case,
we are fully trusting our causal estimates of the outcomes and simply choose
the best action accordingly, which corresponds to outcome maximization.
We now combine these two in order to make a trade off between causal
inference and outcome maximization. At time t, we sample the action a for a
t
new participant from
exp 1 yˆ(a)
q (a)=ǫ
1
+(1 ǫ )
βt t
, (3.26)
t t − t e(cid:16) xp 1 yˆ(cid:17) (a′)
|A| a′∈A βt t
(cid:16) (cid:17)
P
where ǫ [0,1] and is the number of all possible actions.
t
∈ |A|
We can sample from this mixture distribution by
1. Sample e 0,1 from a Bernoulli distribution of mean ǫ .
t t
∈{ }
2. Check e
t
• If e =1, we uniformly choose a at random.
t t
• If e =0, we sample a proportionally to yˆ(a).
t t
As we continue the RCT according to this assignment policy, we assign
participants increasingly more to actions with better outcomes, because our
causal estimate gets better over time. We however ensure that participants are
randomlyassignedto actionsata reasonablerateofǫ ,in orderto estimate the
t
causalquantityratherthanthestatisticalquantity.Itiscommontostartwitha
largerǫ 1 and graduallyannealit toward0, as we wantto ensure we quickly
t
≈
estimate the correct causal effect early on. It is also usual to start with a large30 CHAPTER 3. ACTIVE CAUSAL INFERENCE
β 1 and anneal it toward0,as the early estimate of the causaleffect is often
t
≥
not trustworthy.
Whenthefirstcomponent(theuniformdistribution)isselected,wesaythat
weareexploring,andotherwise,wesayweareexploiting.e isahyperparameter
t
that allows us to compromise between exploration and exploitation, while β is
t
how we express our belief in the current estimate of the causal effect.
ThiswholeprocedureisavariantofEXP-3,whichstandsfortheexponential-
weight algorithm for exploration and exploitation [Allesiardo et al., 2017], that
is used to solve the multi-armed bandit problem. However, with an appropriate
choice of ǫ and β , we obtain as a special case RCT that can estimate the
t t
causaleffectoftheactionontheoutcome.Forinstance,wecanusethefollowing
schedules of these two hyperparameters:
1, if t<T
ǫ =β = (3.27)
t t
(0, if t T
≥
with a larger T 1. We can however choose smoother schedulers for ǫ and β
t t
≫
in order to make a better compromise between causal inference and outcome
maximization,in orderto avoidassigningtoo many subjects to a placebo (that
is, ineffective) group.
Thechoiceofǫ andβ alsoaffectsthebias-variancetrade-off.Althoughthis
t t
is out of the scope of this course, it is easy to guess that higher ǫ and higher
t
β lead to a higher variance but a lower bias, and vice versa.
t
A never-ending trial. A major assumption that must be satisfied for RCT
is the stationarity. Both the causal distribution p∗(y a,x) and the covariate
|
distributionp∗(x)mustbestationaryinthattheydonotchangethroughoutthe
trialaswellasafterthe trial.Especiallywhenthese distributionsdriftafter the
trial, that is, after running the trial with T participants,our causalestimate as
wellasthe decisionbasedonitwillbecomelessaccurate.Weseesuchinstances
often in the real world. For instance, as viruses mutate, the effectiveness of
vaccination wanes over time, although the very same vaccine was found to be
effective by an earlier RCT.
When the underlying conditional distributions are all stationary, we do not
need to keep the entire set of collected data points in order to compute the
approximate causal effect, because
t 1(a =a)y t−1 1(a =a) 1(a =a)
yˆ(a)= t′=1 t′ t′ = t′=1 t′ yˆ (a)+ t y .
t P N t′′=11(a t′′ =a) Pt t′=11(a t′ =a) t−1 t t′=11(a t′ =a) t
(3.28)
P P P
Inotherwords,wecanjustkeepasinglescalaryˆ(a)foreachactiontomaintain
t
the causal effect over time.
We can tweak this recursive formula to cope with slow-drifting underlying
distributions by emphasizing recent data points much more so than older data3.4. CAUSAL INFERENCE VS. OUTCOME MAXIMIZATION 31
points. This can be implemented with exponential moving average,5 as follows:
ηyˆ (a)+(1 η)y , if a =a
yˆ(a)=
t−1
−
t t
(3.29)
t
(yˆ t−1(a), if a
t
=a
6
where η [0,1). As η 0, we consider an increasingly smaller window into
∈ →
the past and do not trust what we have seen happen given a particular action.
On the other hand, when η 1, we do not trust what happens now but rather
→
what we already know about the causal effect of an action a should be.
Bykeepingtrackofthecausaleffectwithexponentialmovingaverage,wecan
continuouslyrunthetrial.Whendoingso,wehavetobecarefulinchoosingthe
schedules of ǫ and β . Unlike before, ǫ should not be monotonically annealed
t t t
toward 0, as earlier exploration may not be useful later when the underlying
distributions drift. β also should not be annealed toward 0, as the estimate of
t
the causal effect we have at any moment cannot be fully trusted due to the
unanticipated drift of underlying distributions. It is thus reasonable to simply
set both β and ǫ to reasonably large constants.
t t
Checking the bias. At time t, we have accumulated
D = (e ,a ,y ),...,(e ,a ,y ) , (3.30)
t 1 1 1 t t t
{ }
where e indicates whether we explored (1) or exploited (0) at time t′.
t′
We canthen getthe unbiasedestimate ofthe causaleffect of a on y by only
using the triplets (e,a,y) for which e=1. That is,
t 1(e =1)1(a =a)y
y˜(a)= t′=1 t′ t′ t′ . (3.31)
t t 1(e =1)1(a =a)
Pt′′=1 t′′ t′′
P
This estimate is unbiased, unlike yˆ(a) from EXP-3 above, since we only used
the action-outcomepairswhenthe actionwasselectedrandomlyfromthe same
uniform distribution.
Assumingalarget(soastominimizetheimpactofahighvariance,)wecan
thencomputethe(noisy)biasofthecausaleffectestimatedandusedbyEXP-3
above as
b =(yˆ(a) y˜(a))2. (3.32)
t t t
−
Of course this estimate of the bias is noisy and especially so when t is small,
since the effective number of data points used to estimate y˜ is on average
t
t
ǫ t. (3.33)
t′
≤
t′=1
X
5‘exponential-weight’ inEXP-3comesfromthischoice.32 CHAPTER 3. ACTIVE CAUSAL INFERENCE
3.5 When Some Confounders are Observed
The assignment of an individual x to a particular treatment option a is called
a policy. In the case of RCT, this policy was a uniform distribution over all
possible actions a, and in the case of EXP-3, it was a mixture of a uniform
policyandaeffect-proportionalpolicyfromEq.(3.26).Inbothcases,thepolicy
was not conditioned on the covariate x, meaning that no information about
the individual was used for assignment. This is how we addressed the issue of
unobserved confounders.
Suchanapproachishoweveroverlyrestrictiveinmanycases,assometreat-
ments may only be effective for a subset of the population that share a certain
trait. For instance, consider the problem of inferring the effect of a monoclonal
antibody therapeutics called Trastuzumab (or Herceptin) for breast cancer on
the disease-free survival of a patient [Nahta and Esteva, 2007]. If we run RCT
without taking into account any covariate information as above, we very likely
would not see any positive effect on the patient’s disease-free survival, because
TrastuzumabwasspecificallydesignedtoworkforHER2-positivebreastcancer.
That is, only breast cancer patients with over-expressed ERBB2 gene (which
encodestheHER2receptor)wouldbenefitfromTrastuzumab.Inthesecases,we
areinterestedinconditionalaveragetreatmenteffect(CATE)fromearlier,that
is, to answer the question of what causal effect Trastuzumab has on patients
given their gene expression profile. CATE given the overly-expressed ERBB2
geneofTrastuzumabwouldbepositive,whileCATEwithoutover-expressionof
ERBB2 gene would be essentially zero.
When we observe some confounders, such as the gene expression profile of
a subject in the example above, and do not observe all the other confounders,
we canmix RCT (§3.3) andregression(§3.2) to estimate the conditionalcausal
effect conditioned on the observed confounders.
The graph G with the partially-observed confounder is depicted as below
with the unobserved x and the observed x′:
x x′
a y
Each subject in RCT then corresponds to the action-outcome-observed-
covariate triplet (a ,y ,x′). Assume we have enrolled and experimented with
t t t
t participants so far, resulting in
D = (a ,y ,x′),...,(a ,y ,x′) . (3.34)
t { 1 1 1 t t t }
Let xˆ′ be the condition of interest, such as the overexpression of ERBB2. we3.5. WHEN SOME CONFOUNDERS ARE OBSERVED 33
can then create a subset D(xˆ′) as
D (xˆ′)= (a,y,x′) D x′ =xˆ′ D . (3.35)
t t t
{ ∈ | }⊆
We can then use this subset D (xˆ′) as if it were the set from the ordinary
t
RCT, in order to estimate the conditional causal effect, as follows.
1(a =a)y
yˆ(ax′) (ai,yi,x′ i)∈Dt(x′) i i . (3.36)
t | ≈ 1(a =a)
P(aj,yj,x′ j)∈Dt(x′) j
Just like what we did earlier, wPe can easily turn this into a recursive version in
order to save memory:
yˆ (ax′), if x′ =x′
yˆ t(a |x′)= yˆt t− −11 (a|x| ′)P
(aj,yj,x′
j)∈Dt−1(x′)1(aj=a)+yt1(at=a)
, if
x′t =6
x′
(3.37)
 P (ak,yk,x′ k)∈Dt(x′)1(ak=a) t
Similarly toearlier, we can instead of exponential moving average in order to
cope with the distribution drift over the sequential RCT, as
yˆ (ax′), if x′ =x′
yˆ(ax′)= t−1 | t 6 (3.38)
t | (η tyˆ t−1(a |x′)+(1 −η t)y t1(a
t
=a), if x′
t
=x′
Wecanthenusethisrunningestimatesofthe causaleffectsinordertobuildan
assignmentpolicyπ (ax′)thatnowdepends onthe observedcovariatex′.Ifwe
t
|
go back to the earlier example of Trastuzumab, this policy would increasingly
more assign participants with over-expressed ERBB2 to the treatment arm,
while it would continue to be largely uniform for the remaining population.
AParametrized CausalEffect. Upuntilthispoint,partially-observedcon-
founders do not look like anything special. It is effectively running multiple
RCT’s in parallel by running an individual RCT for each covariate configura-
tion. There is no benefit of running these RCT’s in parallel relative to running
theseRCT’sinsequence.Wecouldhoweverimagineascenariowheretheformer
is more beneficial than the latter, and we consider one such case here.
Assume thatx′ is amulti-dimensionalvector,i.e., x′ Rd andthatthe true
∈
causal effect of each action a is a linear function of the observed covariate x′:
d
yˆ∗(ax′)=θ∗(a)⊤x′+b∗(a)= θ∗(a)x′ +b(a). (3.39)
| d d
i=1
X
Thismeansthateachdimensionx′ ofthecovariatehasanadditiveeffectonthe
d
expected outcome yˆ(ax) weighted by the associated coefficient θ∗(a), and that
| d
the effect θ∗(a)x′ ofeachdimensiononthe expected outcomeis independent of
d d
the other dimensions’ effects.
As anexample,considerestimatingthe effectofweightlifting onthe overall
health. The action is whether to perform weight lifting each day, and the out-
come is the degree of the subject’s healthiness. Each dimension x′ refers to a
d34 CHAPTER 3. ACTIVE CAUSAL INFERENCE
habit of a person. For instance, it could be a habit of smoking, and the corre-
sponding dimension x′ encodes the number of cigarettes the subject smokes a
d
day. Another habit could be jogging, and the corresponding dimension would
encode the number of minutes the subject runs a day. Smoking is associated
with a negative coefficient regardless of a. On the other hand, jogging is as-
sociated with a negative coefficient when a = 1, because an excessive level of
workout leads to frequent injuries, while it is with a positive coefficient when
a=0.
Of course, some of these habits may have nonlinear effects. Running just
the right duration each day in addition to weight lifting could lead to a better
health outcome. It is however reasonable to assume linearity as the first-order
approximation.
We can estimate the coefficients θ(a) by regressionfrom §3.2 by solving
t
min1 1(a =a) y θ(a)⊤x′ b∗(a) 2 . (3.40)
θ(a) 2 t′ t′ − t′ −
t′=1
X (cid:0) (cid:1)
Instead of keeping the count for each and every possible x′, we now keep only
θ(a) for each action a. This has an obvious advantage of requiring only O(d)
memory rather than O(2d).
More importantly however is that the estimated causal effect generalizes
unseen covariate configuration. Let us continue from the example of having
smoking and jogging as two dimensions of x′. During RCT, we may have seen
participants who either smoke or jog but never both. Because of the linearity,
the estimated causal effect predictor,
θ(a) x′ +θ(a) x′ , (3.41)
smoke smoke run run
generalizes to participants who both smokes and jogs as well as who neither
smokes nor jogs.
This case of a linear causal effect suggests that we can rely on the power
of generalization in machine learning in order to bypass the strong assumption
of positivity. Even if we do not observe a covariate or an associated action, a
parametrized causal effect predictor can generalize to those unseen cases. We
will discuss this potential further later in the semester.
When there are many possible actions. Assume we do not observe any
confounder, that is, there is no x′. Then, at each time, RCT is nothing but
estimating a single scalar for each action. Let be a set of all actions and
A |A|
a cardinality of this action set. Then, at any time t of running an RCT, the
number of data points we can use to estimate the causal effect of a particular
action is
t
N(a)= 1(a =a) tp (a),
t′ a
≈
t′=1
X3.6. SUMMARY 35
where p (a) is the probability of selecting the action a during randomization.
a
Justlikethecaseabovewithpartiallyobservedconfounders,thevarianceofthe
estimate of the causal effect of an individual action decreases dramatically as
the number of possible actions increases.
Wemusthavesomeextrainformation(context)abouttheseactionsinorder
to break out of this issue. Let c(a) Rd be the context of the action a. For
∈
instance, each dimension of c(a) corresponds to the amount of one ingredient
for making the perfect steak seasoning, such as salt, pepper, garlic and others.
Then, each action a corresponds to a unique combination of these ingredients.
In this case, the causal effect of any particular action can be thought of
mappingc(a)totheoutcomeyˆ(a)associatedwitha.Ifweassumethismapping
was linear [Li et al., 2010], we can write it as
yˆ(a)=c(a)⊤θ∗+b∗, (3.42)
where θ Rd and b R.
∈ ∈
Similarly to the case where there was an observed confounder above, with
linearity, we do not need to maintain the causal estimate for each and every
possibleaction,whichamountsto numbers,buttheeffectofeachdimension
|A|
of the action context on the outcome, which amounts of d numbers. When
d , we gain a significant improvement in the variance of our estimates.
≪|A|
Furthermore, just like what we saw above, we benefit from the composi-
tionality,or compositionalgeneralization.For instance,if the effects of saltand
pepperonthefinalqualityofseasoningareindependentandadditive,wecanac-
curatelyestimatethe effectofhavingbothsaltandpepperevenwhenalltested
seasonings had either salt or pepper but never both. Let c(a) = [s ,s ],
salt pepper
andassumes ,s 0,1 andthatallpasttrialsweresuchthats =0
salt pepper salt
∈{ }
or s = 0. We can approximate θ∗ and θ∗ from these past trials,
pepper salt pepper
and due to the linearity assumption, we can now compute the causal effect of
c(a)=[1,1], as
θˆ +θˆ . (3.43)
salt pepper
Thiswouldnothavebeenpossiblewithoutthelinearity,ormoregenerallycom-
positionality, because this particular action of adding both salt and pepper has
never been seen before, i.e., it violates the positivity assumption. This is yet
another example of overcoming the violation of positivity by generalization.
At this point, one sees a clear connection between having some confounders
observedandhavingmanyactionsassociatedwiththeircontexts.Thisisbecause
they are simply two sides of the same coin. I leave this to you to think of why
this is the case.
3.6 Summary
In this chapter, we have learned about the following topics:
1. Average treatment effect;36 CHAPTER 3. ACTIVE CAUSAL INFERENCE
2. Regression for causal inference;
3. Randomized controlled trials;
4. Outcome maximization with a bandit algorithm;
5. A contextual bandit.Chapter 4
Passive Causal Inference
4.1 Challenges in Randomized Controlled Trials
A major issue with randomized controlled trials (RCT) is that we must exper-
iment with subjects. This raises many issues that are not necessarily related
to causal inference itself but are more broadly about ethics and legality. For
instance, the “Tuskegee Study of Untreated Syphilis in the Negro Male” was
the widely-known and widely-condemned study for investigating the effect of
untreated syphilis [for Disease Control et al., 2020]. As RCT requires careful,
double blinding, the trial administrators did not reveal to the study partici-
pants that they were diagnosedwith (latent) syphilis. The study was originally
designed (and the participants were told) to run for six months but lasted for
40 years until the details of the study were leaked to the press. During these
decades, the treatment for syphilis was made available but none of the partici-
pantsweretreatedproperly,resultinginthedeathofmorethan100participants
due to syphilis, out of approximately 400 participants, the syphilis infection of
the wives of fourty participants and the congenital syphilis infection of 19 chil-
dren. It took more than half a century for the US governmentto formally issue
apology.
AsimilarissuepersiststhroughoutmedicinewhenitcomestoRCTwhichis
defactostandardforestablishinganycausaleffectofatreatmentontheoutcome
of a patient. Due to the necessity of randomization, some patient participants
will inevitably receive placebo rather than the actual treatment. Even if the
tested treatment ultimately turns out to be causally effective, by then it may
be already late for those patients who were put on the control arm to receive
and benefit from this new treatment. How ready are you to put patients into
suffering because we wantto (and often need to) establishthe causaleffect ofa
new treatment?
Sometimes, it is impossible to design a placebo that ensures double blind-
nessof atrial.Consider forinstance anRCT onthe effectiveness ofmaskingon
preventingrespiratorydiseases.Participantswillunderstandablyalter their be-
3738 CHAPTER 4. PASSIVE CAUSAL INFERENCE
havioursbased ontheir assignments;treatment (masking)or control(no mask-
ing), as their perception of risk is altered, which violates the stationarity of the
causaleffect p∗(y a,x). In orderto avoidthis, we must ensure that participants
|
cannot tell whether they are in the treatment or control arm, but it is pretty
much impossible to design a placebo mask that looks and feels the same as an
actual mask but does not filter any particle in the air. In other words, RCT is
only possible when placebos can be effectively designed and deployed.
In this example, we run into yet another problem; how do we enforce the
treatmentonsubjects?Inthecaseofvaccination,subjectscomeintoclinicsand
are for instance injected on the spot under the supervision of a clinician, after
whichthesubjectscannotgetridofinjectedvaccine.Inthecaseofmasking,for
instance,wecannotensurethatparticipantswearmasksasthey areinstructed,
as this requires non-stop monitoring throughout the trial period.
Finally, some actions take long to have measurable impact on the outcome.
Forinstance,considerapolicyproposalofintroducinganewcourseonprogram-
ming at elementary schools (1-6 grades) with the goal of improving students’
job prospects and growing the information technology (IT) sector. It will take
anywherebetween12to20yearsforthesestudentstofinishtheireducationand
participate in society,and we will have to waitanother 4 to 15 yearsto see any
measurable economic impact on the IT sector. Such a long duration between
the action and the outcome further complicates RCT, as it is often impossible
to ensure the stationarity of underlying distributions over that duration. RCT
is thus notsuitable for suchactionsthatrequirea significantamountoftime to
have any measurable impact.
In this chapter, we instead consider an alternative approachto RCT, where
we rely on existing data to infer the causal relationship between the action
and outcome. As we use already collected data, we can often avoid the issues
arising from actively experimentation, although we are now faced with another
set of challenges, such as the existence of spurious correlations arising from
various unobserved confounders that affected the choice of actions earlier. We
willdiscusshowwecanavoidtheseissuesinthischapter.Itishoweverimportant
to emphasize that there is no silver bullet in causal inference.
4.2 When Confounders were also Collected
4.2.1 Inverse Probability Weighting
x
a y
Let us come back to the originalgraphG that consists of three variables; a,4.2. WHEN CONFOUNDERS WERE ALSO COLLECTED 39
y and x, with the following joint probability:
p∗(a,y,x)=p∗(x)p∗(ax)p∗(y a,x). (4.1)
| |
We also assume that we have a set D of triplets (a ,y ,x ) drawn from this
n n n
underlyinggraphG.Inother words,we assumethatwe observethe confounder
in this case.
If we have a large such set, i.e. N = D 1, we can use regression, as in
| | ≫
§3.2, to approximate p∗(y a,x) in order to compute the causal effect as
|
E [y do(a=aˆ)] p∗(x) ypˆ(y aˆ,x) (4.2)
G
| ≈ |
x y
X X
N
1
E [y]. (4.3)
≈ N pˆ(y|aˆ,xn)
n=1
X
Although this regression-based approach is straightforward, this approach
has a disadvantage of having to fit a regressor on the concatenation of the
action a and confounder x. The issue is exactly that of RCT with partially
observedconfounders,thatis,wemusthaveenoughdatapointsforeachaction-
covariate combination in order for regression to have a low variance. We can
use regularizationto reduce the variance, which may unfortunately introduce a
bias.
We can reduce the variance by using regressionto estimate a simpler quan-
tity. In particular, we consider approximating p∗(ax). Because this is a map
|
from ,wejustneedenoughdatapointsforeachcovariateconfigurationrather
X
thantheaction-covariatecombination.Approximatingp∗(ax)allowsustoesti-
|
matethecausaleffectusingdatapointsdrawnfromtheoriginalgraphGrather
than the modified graph G from §3.3, because
E [y do(a=aˆ)]= p∗(x) 1(a=aˆ) p∗(y aˆ,x)y (4.4)
G
| |
x a y
X X X
1
= p∗(x)p∗(ax)1(a=aˆ)p∗(y aˆ,x) y (4.5)
| | p∗(ax)
x a y |
XXX
N
1 y
= 1(a =aˆ) n . (4.6)
N n′=11(a
n′
=aˆ)
n=1
n p∗(aˆ |x n)
X
Instead of the truePp∗(aˆx ), we plug in the regression-basedapproximation
n
|
pˆ(aˆ x ) and arrive at
n
|
N 1(a =aˆ) yn
E [y do(a=aˆ)] n=1 n pˆ(aˆ|xn) . (4.7)
G | ≈ P N n′=11(a n′ =aˆ)
In words, we look for all data pointsPwithin the previously-collected set D
that are associated with the action aˆ of interest. The simple average of the
associated outcomes would be a biased estimate of the casual effect, since it40 CHAPTER 4. PASSIVE CAUSAL INFERENCE
combines the effects of a on y via two paths; (causal) a y and (spurious)
→
a x y.Wecorrectthisbiasbyweightingeachdatapoint,ortheassociated
← →
outcome, by the inverse probability of the action given the confounder, 1 .
pˆ(aˆ|xn)
This approachis thus calledinverse probability weighting (IPW), andp∗(ax) is
|
often referred to as a propensity score.
Itisfinetohave missingoutcomes. OneimportantadvantageoftheIPW-
basedapproachis that we can approximatep∗(ax) using all (a,x) pairseven if
|
they are not associated with the outcome y, unlike the earlier regression-based
approach which required having all three variables observed (a,y,x). Imagine
using clinical notes andmeasurements from the electronic health record(EHR)
of a large hospital in order to estimate the causal effect of a particular drug on
a target disease. Of course, the prescription of the drug a is not made blindly
but based on the patient information which includes their underlying health
condition. Since the existing health conditions affect the outcome y of almost
anykindofadisease,suchpatientinformationisaconfounderx.Somepatients
often do notreturn to the same hospitalfor follow-upchecks,meaning that the
EHRdoes not recordthe outcome ofthese patients,leavingus only with (a,x).
Wecanthenusealltheprescriptionstoapproximatethepropensityscorep(ax)
|
and then use a subset of these prescriptions for which the patients’ outcomes
are recorded (i.e. they came back for follow-up visits) to compute the causal
effect.
Mathematically, it means that we solve two separate optimization problems
using twodifferentdatasets (though,one is asupersetofthe other,)asfollows:
pˆ(ax)=argmax logp(a′ x′), (4.8)
| p |
(a′,x X′)∈Dπ
1(a′ =aˆ) yn
yˆ(aˆ)=
(a′,x′,y′)∈Dy¯ pˆ(aˆ|xn)
, (4.9)
1(a′′ =aˆ)
P (a′′,x′′,y′′)∈Dy¯
P
where D D .
y¯ π
⊆
A doubly robust estimator. A major issue with the IPW-based approach
is that the variance of the estimator can be very large, even if the variance of
estimating the propensity score is low, because the propensity scores shows up
in the denominator. On the other hand, the regression-based approach has a
low variance due to the missing division by the propensity score. It is however
likely a biased estimator, as we cannot easily guarantee that the choice of a
non-parametricregressorcanidentifythecorrectconditionalprobability,dueto
a variety of issues, such as the lack of realizability.
If the regression-based approach is correct for an instance (a,y,x), y˜(a,x)
wouldcoincide withy,andwe wouldjust use y˜(a,x) asit is andprefer to avoid
the IPW-based estimate due to the potentially large variance arising from the
denominator.Otherwise,we wantto rely onthe IPW-basedestimate to correct4.2. WHEN CONFOUNDERS WERE ALSO COLLECTED 41
for the incorrectness arising from the regression-based approach. This can be
expressed as
1
1
yˆ(aˆ)= p(x)  y˜(a,x)+p∗(ax) |A| y∗(ax) y˜(a,x) .
|
pˆ(ax)
| −

Xx Xa  |A| |
 (a)


 (b)  
 | {z } (4.10)
| {z }
Ify˜(a,x)isperfect,(a)disappears,asexpected.Ifourestimateofthepropensity
score is perfect, (b) is 1, resulting in using the true y∗(a,x) while ignoring the
regression-basedapproach.1
Since we areprovidedwith data ratherthanthe actualprobabilitydistribu-
tions, we end up with
1 1
yˆ(aˆ)= 1(a′ =aˆ) y˜(aˆ,x )+ (y y˜(aˆ,x )) ,
n n n
Z(aˆ) pˆ(aˆx ) −
(a′,x X′,y′)∈D (cid:18) | n (cid:19)
(4.11)
whereZ(aˆ)= 1(a′′ =aˆ). This estimatoris calleda doubly robust
(a′′,x′′,y′′)∈D
estimator.
P
4.2.2 Matching.
Instead of estimating p∗(ax) and multiplying the observed outcome with its
|
inverse, we can achieve a similar outcome by manipulating data itself. When
p∗(ax) = p∗(a), that is, the action is independent of the covariate, the IPW-
|
basedestimatecoincideswithsimpleaveraging,justlikeinRCTfrom§3.3.This
happens when the ratio of actions associated with each unique x in the data
is the same across the data set. To make it simpler, let us express this ratio of
actions by assigning the minimal number of eachaction in relationto the other
actions. For instance, if our desired ratio between the treatment and placebo is
0.8:0.2, we would express it for instance as 4:1.
Starting from the original data set D = (a ,x ,y ),...,(a ,x ,y ) , we
1 1 1 N N N
{ }
go through each x by collecting as many (a,x ,y) D as n , where n is
n n a a
∈
the targetnumber ofexamples of actiona, for instance 4 above.This collection
can be done randomly or by following some fixed strategy, such as round-robin
scheduling.Sometimes itis necessaryto choose the same triplet multiple times,
which is not ideal but may be necessary. By aggregating these collections, we
arrive at a new dataset D˜.
Under this new dataset D˜, the propensity score is guaranteed to be
pˆ(ax) n , (4.12)
a
| ∝
1y∗(a,x)isthetrueexpectedoutcomegiventheactionaandthecovariatex.Sinceexpec-
tationislinear,wecanpushtheexpectation allthewayinsidetoobtainthisexpression.42 CHAPTER 4. PASSIVE CAUSAL INFERENCE
regardless of x. Furthermore, pˆ(ax) = pˆ(a) as well. Meanwhile, pˆ(x) stays the
|
same as thatunder the originaldata setD. Then, the expected causaloutcome
of any given action a under this dataset is
N
1
yˆ(aˆ)= 1(a′ =aˆ)y′, (4.13)
N n n
n=1
X
where (a′ ,y′,x′ ) D˜. This avoids the issue of the high variance arising from
n n n ∈
theIPW-basedapproach.Thishoweverdoesnotmeanthatthisapproachalways
works.
The mostobvious issue with this approachis that the originaldata set may
nothaveenoughtripletsassociatedwitheachxtoensurethatp∗(ax)isidentical
|
for allx. Furthermore,evenif wehaveenoughassociatedtriplets for eachx, we
may endup with discardingmany triples fromthe originaldata setto formthe
new data set. We never want to discard data points we have. This approach is
thus appropriate when data is already well balanced and the goal is to further
ensure that the propensity score is constant.
A relatively simple variant of this approach, called ‘matching’ because we
match triplets based on x, is to relax the constraint that we only consider the
exact match of the covariate x. The original formulation samples the triplets
according to the target counts with replacement from the following multiset:2
D(x)= (a′,y′,x′) D x′ =x . (4.14)
{ ∈ | }
This condition of x′ = x may be too strict, leaving us with only a very small
D(x).Wecaninsteadrelaxthisconditionusingapredefinednotionofsimilarity
such that
D˜(x)= (a′,y′,x′) D s(x′,x)<ǫ , (4.15)
{ ∈ | }
where s(x′,x) is a predefined distance function and ǫ a distance threshold.
4.3 Instrumental Variables: When Confounders
were not Collected
So far in this section we have considered a case where the confounder x was
available in the observational data. This allowed us to either fit the regressor
directly onp(y a,x),use inverseprobabilityweightingorre-balancethe dataset
|
usingthematchingscheme.Itishoweverunlikelythatwearegivenfullaccessto
theconfounders(oranykindofcovariate)intherealworld.Itisthusimportant
to come up with an approach that works on passively collected data without
covariates.
2Itisamultiset,sincetherecanbeduplicates.4.3. INSTRUMENTALVARIABLES:WHENCONFOUNDERSWERENOTCOLLECTED43
An instrumental variable estimator. Let us rewrite the following graph
G into a corresponding structural causal model:
0
x
a y
The structural causal model is then
x ǫ (4.16)
x
←
a f (x,ǫ ) (4.17)
a a
←
y f (a,x,ǫ ). (4.18)
y y
←
From this structural causal model, we can read out two important points.
First, as we have learned earlier in §2.2, x is a confounder, and when it is not
observed, the path a x y is open, creating a spurious effect of a on y.
← →
Second, the choice of a is not fully determined by x. It is determined by the
combinationofxandǫ ,wherethelatterisindependentofx.Weareparticularly
a
interested in the second aspect here, since it gives us an opportunity to modify
this graph by introducing a new variable that may help us remove the effect of
the confounder x.
We now consider an alternative to the structural causal model above by
assuming that we found another variable z that largely explains the exogenous
factor ǫ . That is, instead of saying that the action a is determined by the
a
combination of the covariate x and an exogenous factor ǫ , we now say that it
a
is determinedby the combinationofthe covariatex, this new variable z andan
exogenous factor ǫ′. Because z explains a part of the exogenous factor rather
a
thanx,zisindependentofxapriori.Thisintroductionofzaltersthestructural
causal model to become
x ǫ (4.19)
x
←
z ǫ (4.20)
z
←
a f′(x,z,ǫ′) (4.21)
← a a
y f (a,x,ǫ ), (4.22)
y y
←
which corresponds to the following graph G :
1
x
z a y
This altered graph G does not help us infer the causal effect of a on y any
1
more than the original graph G did. It however provides us with an opportu-
0
nity to replace the original action a with a proxy based purely on the newly
introduced variable z independent of x.44 CHAPTER 4. PASSIVE CAUSAL INFERENCE
We first notice that x cannot be predicted from z, because z and x are by
construction independent. The best we can do is thus to predict the associated
action.3 Let p (az) be the conditional distribution induced by g (z,ǫ′′). Then,
a˜ | a˜ a
we want to find g that minimizes
a
= E E E [logg (f′(ǫ ,z,ǫ′),z)]. (4.23)
Ea − ǫz ǫx ǫ′ a a˜ a x a
Wealsonoticethatzcannotbepredictedfromaperfectlywithoutx,because
a is an observed collider, creating a dependency between z and x. The best we
candois to thus to predictthe expectedvalue ofz.Thatis,we lookfor g that
z
minimizes
= E E [logp (ǫ f′(ǫ ,ǫ ,ǫ′))], (4.24)
Ez − ǫz ǫx z˜ z | a x z a
wherewehaveusedp (z a)bethe conditionaldistributioninducedbyg (a,ǫ′),
z˜ | z˜ z
similarly to p (az) above.
a˜
|
Oncewefoundreasonablesolutions,gˆ andgˆ ,totheminimizationproblems
a˜ z˜
above, respectively, we can further modify the structural causal model into
a aˆ (4.25)
←
x ǫ (4.26)
x
←
z˜ gˆ (a,ǫ′) (4.27)
← z˜ z
a˜ gˆ (z˜,ǫ′′) (4.28)
← a˜ a
y f (a˜,x,ǫ ). (4.29)
y y
←
Since x is really nothing but an exogenous factor of y without impacting a˜ nor
z in this case, we can simplify this by merging x and ǫ into
y
a aˆ (4.30)
←
z˜ gˆ (a,ǫ′) (4.31)
← z z
a˜ gˆ (z˜,ǫ′′) (4.32)
← a˜ a
y f (a˜,ǫ′). (4.33)
← y y
Becauseweassumethe actionais alwaysgiven,we simplysetitto a particular
action aˆ.
This structural causal model can be depicted as the following graph G :
2
aˆ z˜ a˜ y
Inother words,we startfromthe action,approximatelyinfer the extravariable
z˜, approximately infer back the action and then predict the outcome. During
two stages of inference (z˜a and a˜z˜), we drop the dependence of a on z. This
| |
happens, because z was chosen to be independent of x a priori.
In this graph, we only have two mediators in sequence, z˜ and a˜, from the
action a to the outcome y. We can then simply marginalize out both of these
3Wecanpredictthemarginaldistributionovertheactionaftermarginalizingoutx.4.3. INSTRUMENTALVARIABLES:WHENCONFOUNDERSWERENOTCOLLECTED45
mediatorsinorderto compute the interventionaldistributionovery givena, as
we learned in §2.2. That is,
yˆ (a=aˆ)=yˆ (a=aˆ) yˆ (a=aˆ)=E E E [y]. (4.34)
G0 G1
≈
G2 z˜|aˆ a˜|z˜ y|a˜
We call this estimator an instrumental variable estimator and call the extra
variable z an instrument. Unlike the earlier approaches such as regression and
IPW, this approach is almost guaranteed to give you a biased estimate of the
causal effect.
Assume we are provided with D = (a ,y ,z ),...,(a ,y ,z ) after we
1 1 1 N N N
{ }
are done estimating those functions above. We can then get the approximate
causal effect of the action of interest aˆ by
N 1(a =aˆ)E fˆ(gˆ (z ,ǫ′′),ǫ′)
yˆ (aˆ) n=1 n ǫ′ a′,ǫ′ y y a˜ n a y . (4.35)
IV ≈ P N n′=11(a n′ =aˆ)
Additionally,ifweareprovidedfurtP herwithD = (a′,y′),...,(a′ ,y′ ) ,we
z¯ { 1 1 N′ N′ }
canusegˆ (a,ǫ′)toapproximatethisquantity,togetherwithD.Let(a ,y ,r ,z )
D¯ be z z n n n n ∈
(a ,y ,1,z ), if n N
n n n
(a ,y ,r ,z )= ≤ (4.36)
n n n n ((a′ n−N′+1,y n′ −N′+1,0, −1), if N <n ≤N +N′
Then,
N+N′ 1(a =aˆ)E r fˆ(gˆ (z ,ǫ′′),ǫ′)+(1 r )fˆ(gˆ (gˆ (a ,ǫ′),ǫ′′),ǫ′)
yˆ (aˆ) n ǫ′ a′,ǫ′ y,ǫ′ z n y a˜ n a y − n y a˜ z n z a y .
IV ≈ h N+N′1(a =aˆ) i
n=1 n′=1 n′
X
(4.37)
P
Intheformercase,wemustsolvetworegressionproblems,findingfˆ andgˆ .
y a˜
Whenwedosoby solvinga leastsquaresproblemforeach,we endupwithtwo
least squares problems that must be solved sequentially. Such a case is often
referred as two-stage least squares. In the latter case, we benefit from extra
data by solving three regression problems. Though, in most cases, we choose
an easy-to-obtain instrument so that it is often enough to solve two regression
problems.
There are two criteria that need to be considered when choosing an instru-
mentz.First,theinstrumentmustbeindependentoftheconfounderxa priori.
If this condition does not hold, we end up with the following graph:
x
z a y46 CHAPTER 4. PASSIVE CAUSAL INFERENCE
The undirected edge between z and x indicates that they are not indepen-
dent.Inthiscase,evenifwemanagetoremovetheedgebetweenaandx,there
is still a spurious path a z x y, that prevents us from avoiding this
← ↔ →
bias. This first criterion is therefore the most important consideration behind
choosing an instrument.
Instrumental variables must be predictive of the action. The second
criterion is that z be a cause of a together with x. That is, a part of what-
ever cannot be explained by x in determining a must be captured by z. We
can see why this is important by recalling the IPW-based estimate against the
instrumental variable based estimate. The IPW-based estimate from Eq. (4.9)
is reproduced here as
N 1(a =aˆ) yn
yˆ (aˆ)
n=1 n pˆ(aˆ|xn)
. (4.38)
IPW ≈ P N n′=11(a n′ =aˆ)
If we contrast it with the instrumenPt variable based estimate in Eq. (4.35), we
get
(a)
y
N 1(a =aˆ) n E fˆ(gˆ (z ,ǫ′′),ǫ )
yˆ (aˆ) yˆ (aˆ)=
n=1 n z (cid:18)pˆ(aˆ |x n) − ǫy,ǫ′ a}′ |y a˜ n a y (cid:19){
,
IPW − IV P N 1(a =aˆ)
n′=1 n′
(4.39)
P
where we assume D = (a ,y ,x ,z ),...,(a ,y ,x ,z ) .
1 1 1 1 N N N N
{ }
There aretwoestimates within (a)abovethatresultina bias.Among these
two,fˆ andgˆ ,theformerdoesnotstandachanceofbeinganunbiasedestimate,
y a˜
because it is not given the unbiased estimate of the action nor the covariate x.
Contrast this with the regression-based estimate above where fˆ afforded to
y
rely on the true (sampled) action and the true (sampled) covariate. The latter
is however where we have a clear control over.
Assume that f∗ is linear. That is,
y
f∗(a,x,ǫ )=a⊤α∗+x⊤β∗+ǫ , (4.40)
y y y
where α and β are the coefficients. If E[x]=0 and a is selected independent of
x,
f∗(a,ǫ )=a⊤α∗+ǫ . (4.41)
y y y
Theterm(a),withtheassumptionthat yn =f∗(a ),canthenbeexpressed
pˆ(an|x) y n
as
a E gˆ (z ,ǫ′′) ⊤ α∗ E [gˆ (z ,ǫ′′)]⊤r , (4.42)
n − ǫ′ a′ a˜ n a − ǫ′ a′ a˜ n a α
where r is the e(cid:0)rror in estimating(cid:1)α∗, i.e., αˆ =α∗+r .
α α4.3. INSTRUMENTALVARIABLES:WHENCONFOUNDERSWERENOTCOLLECTED47
For brevity, let gˆ(z )=E gˆ (z ,ǫ′′), which allows us to rewrite it as
n ǫ′ a′ a˜ n a
(a gˆ(z ))⊤α∗ gˆ(z )⊤r . (4.43)
n n n α
− −
Let us now look at the overallsquared error:
M
1 a⊤α∗ gˆ(z )⊤(α∗ r ) 2 , (4.44)
M m − m − α
m=1
X (cid:0) (cid:1)
wherewe use m to referto eachexample witha =aˆ.Ifwe further expandthe
m
squared term,
M
1
α∗⊤a a⊤α∗+(α∗ r )⊤gˆ(z )gˆ⊤(z )(α∗ r ) 2α∗⊤a gˆ⊤(z )(α∗ r )
M m m − α m m − α − m m − α
m=1
X
(4.45)
α∗⊤E[aa⊤]α∗+(α∗ r )⊤E[gˆ(z)gˆ⊤(z)](α∗ r ) 2α∗⊤E[agˆ⊤(z)](α∗ r ).
α α α
≈ − − − −
(4.46)
The first term is constant.It simply tells us that the error would be greater
if the variance of the relevant dimensions of the action on the outcome, where
the relevancy is determined by α∗, is great, the chance of mis-approximatingit
wouldbe simply greatas well.The secondtermtells us that the errorwouldbe
proportional to the variance of the relevant dimensions of the predicted action
onthe outcome, where the relevancy is determined by the predicted coefficient,
α∗ rˆ. That is, if the variance of the predicted outcome is great, the chance of
−
a large error is also great. The third term is where we consider the correlation
between the true action and the predicted action, again along the dimensions
of relevance.
This derivation tells us that the instrument must be selected to be highly
predictive of the action (the third term) but also exhibit a low variance in its
prediction(the secondterm).Herecomesthe classicaldilemma ofbias-variance
trade-off in machine learning.
The linear case. The instrument variable approach is quite confusing. Con-
sider a 1-dimensional fully linear case here in order to build up our intuition.
Assume
x ǫ (4.47)
x
←
a γx+ǫ (4.48)
a
←
y αa+βx+ǫ , (4.49)
y
←
where ǫ and ǫ are both zero-meanNormal variables.If we intervene on a, we
x y
would find that the expected outcome equals
E[y do(a)]=αa, (4.50)
|48 CHAPTER 4. PASSIVE CAUSAL INFERENCE
and thereby the ATE is
ATE=E[y do(a=1)] E[y do(a=0)]=α. (4.51)
| − |
With a properly selected instrument z, that is, z x, we get
⊥⊥
z ǫ (4.52)
z
←
a γx+ψz+ǫ′. (4.53)
← a
Because z x, the best we can do is to estimate ψ to minimize
⊥⊥
N
min (a ψz )2, (4.54)
n n
ψ −
n=1
X
given N (a,z) pairs. The minimum attainable loss is
(γx)2, (4.55)
assuming zero-mean ǫ′, because the contribution from x cannot be explained
a
by the instrument z.
With the estimated ψˆ, we get
aˆ ψˆz+ǫ′′, (4.56)
← a
and know that
aˆ=a γx (4.57)
−
on expectation.
By plugging in aˆ into the original structural causal model, we end up with
x ǫ (4.58)
x
←
aˆ ψˆz+ǫ′′ (4.59)
← a
y α˜aˆ+βx+ǫ′. (4.60)
← y
We can now estimate α˜ by minimizing
N
(y α˜aˆ )2, (4.61)
n n
−
n=1
X
assuming both ǫ and ǫ′ are centered.
x y4.3. INSTRUMENTALVARIABLES:WHENCONFOUNDERSWERENOTCOLLECTED49
If we assume we have x as well, we get
N
(y α˜(a γx ))2 (4.62)
n n n
− −
n=1
X
N
= (y α(a γx )+(α α˜)(a γx ))2 (4.63)
n n n n n
− − − −
n=1
X
N
= ǫ +(α α˜)ǫ′ 2 (4.64)
y,n − a,n
n=1
X(cid:0) (cid:1)
N
= ǫ2 +(α α˜)2(ǫ′ )2+2(α α˜)ǫ ǫ′ (4.65)
y,n − a,n − y,n a,n
n=1
X(cid:0) (cid:1)
= V[ǫ ]+(α α˜)2V[ǫ′]. (4.66)
n→∞ y − a
The first term is irreducible, and therefore we focus on the second term. The
second term is the product of two things. The first one, (α α˜)2, measures
−
the difference between the correct α and the estimated effect. It tells us that
minimizing this loss w.r.t. α˜ is the right way to approximate the true causal
effect α.
We can plug in aˆ from Eq. (4.59) instead:
N
2
y α˜(ψˆz +ǫ′′ ) (4.67)
n − n a,n
n X=1(cid:16) (cid:17)
N
2
= (y α˜ψˆz )+α˜ǫ′′ (4.68)
n − n a,n
n X=1(cid:16) (cid:17)
N
= (y α˜ψˆz )2+α˜2(ǫ′′ )2+2(y α˜ψˆz )α˜ǫ′′ (4.69)
n − n a,n n − n a,n
n X=1(cid:16) (cid:17)
N
= (y α˜ψˆz )2+α˜2V[ǫ′′]. (4.70)
n→∞ n − n a
n=1
X
The first term is about how predictive the instrument z is of y, which is a key
consideration in choosing the instrument. If the instrument is not predictive of
y, the instrument variable approach fails dramatically. The second term corre-
sponds to the variance of the action not explained by the instrument, implying
that the instrument must also be highly correlated with the action.
Inthisprocedure,wehavesolvedleastsquarestwice,(4.54)and(4.61),which
isawidelyusedpracticewithinstrumentvariables.Wealsosawtheimportance
of the choice of the instrument variable.
An example: taxation One of the most typical example of an instrument
is taxation. It is particularly in the United States of America (USA), due to
the existence of different tax laws and rates across fifty states. For instance,50 CHAPTER 4. PASSIVE CAUSAL INFERENCE
imagine an example where the action is cigarette smoking, the outcome is the
contractionof lung cancer and the confounder is an unknown genetic mutation
that both affects the affinity to nicotine addiction and the incidence of a lung
cancer.Becausewedonotknowsuchageneticmutation,wecannoteasilydraw
a conclusion about the causaleffect of cigarette smoking on lung cancer. There
may be a spurious correlation arising from this unknown, and thereby unob-
served, genetic mutation. Furthermore, it is definitely unethical to randomly
force people to smoke cigarettes, which prevents us from running an RCT.
We caninsteadusestate-leveltaxationontobaccoasaninstrument,assum-
ing that lower tax on tobacco products would lead to a higher chance and also
rateof smoking,andvice versa.First,we build a predictorof smokingfromthe
state(orevencounty,ifapplicable)taxrate.Thepredictedamountofcigarettes
smoked by a participant can now work as a proxy to the original action, that
is the actual amount of cigarettes smoked. We then build a predictor of the
incidence of lung cancer as well as the reverse predictor (action-to-instrument
prediction). We can then use one of the two instrument variable estimators
above to approximate the potential outcome of smoking on lung cancer.
4.4 Summary
In this chapter, we have learned the following concepts:
1. Challengesinactivecausalinference:practical,ethicalandlegalchallenges
2. When confounders were observed: Regression, inverse probability weight-
ing and matching
3. When confounders were not observed: instrument variables
There are a few other widely used passive causal inference algorithms, but
they are left for the final section on §6 Remaining Topics,such as difference-in-
difference, regression discontinuity and double machine learning.Chapter 5
Causality and Machine
Learning
In this chapter, we finally delve in to the ‘machine learning’ side of this course,
whichistitled ‘Introductionto CausalInference inMachineLearning’.Inorder
todoso,weneedtostartbyestablishingwhenwedonotneedtothinkofcausal
inference, or more broadly causality, in machine learning. After establishing it,
we will move on to the other extreme, where conventional machine learning
cannotdoanythingonitsown.Wethentrytoincorporatesomeoftheconcepts
we have learned so far, in order to land between these two extreme cases and
solvesomeof the mostchallengingandimportantproblems inmodernmachine
learning.
5.1 Out-of-Distribution Generalization
5.1.1 Setup: I.I.D.
We must start by defining what we mean by ‘prediction’. In this particular
course, we first assume that each and every input-output pair (x,y), input x
or output y is sampled independently of each other. This is a pretty strong
assumption,sincethe worldoftenchangesbasedonwhatwehaveseen,because
those who saw a sample pair may and often do change their behaviors. For
instance, consider building a stock price forecasting model. Once you use a
predictor to predict whether the price of a particular stock goes up or down
and trade based on the outcome, the next input x, that is the stock of your
next interest, is not anymore independently selected but based on your own
success/failure from the previous trade.
This assumption is however also reasonable, because there are many phe-
nomena in which our behaviours do not matter much in a reasonably short
horizon. For instance, consider installing and using a bird classifier at a par-
ticular forest. With a fixed camera, the input to this classifier will be largely
5152 CHAPTER 5. CAUSALITY AND MACHINE LEARNING
independent of which birds (or not) were seen earlier, although spotting of a
particularbirdmayattractpoacherstothisforestwhowoulddramaticallyaffect
the bird population in a longer time frame.
Next, we assume that allthese pairs aredrawnfromthe ‘identical’distribu-
tion. This is similar if not identical to the stationarity assumption from RCT.
In RCT, we often rely on a double blind experiment design, in order to ensure
that the causal effect p∗(y a,x) does not change over the trial. In this section
|
as well as conventional statistical learning theory, we assume all input-output
pairs were drawn from the same distribution.
Combining these two assumptions, we arrive at a so-called training set D
which satisfies
p(D)= p∗(x,y), (5.1)
(x, Yy)∈D
according to the definition of independence. We do not have access to nor have
knowledge of p∗. We use this training set D for both model fitting (training)
and selection (validation).
Once the predictive model pˆis ready, we deploy it to make a prediction on
a novel input x′ drawn from a distribution q∗. That is,
yˆ pˆ(y x′), (5.2)
∼ |
where (x′,y′) q∗. We are often not given y′. After all, y′ is what we want to
∼
use our predictive model to infer.
Wesaythatthepredictivemodelisaccurate,ifthefollowingquantityislow:
R(pˆ)=E [l(y′,pˆ(y x′))], (5.3)
(x′,y′)∼q∗
|
where l(, ) 0 is the loss (misclassification rate).
· · ≥
In traditional statistical learning theory, q∗ is assumed to be p∗, and under
this assumption, the goal of designing a learning algorithm is to minimize a
so-called excess risk:
R (pˆ)=R(pˆ) R(p∗) (5.4)
excess
−
with respect to pˆ. Since we do not have access to p∗, we often use Monte Carlo
approximation to compute R(pˆ), as follows
N
1
R(pˆ) Rˆ (pˆ)= l(y ,p(y x )), (5.5)
N n n
≈ N |
n=1
X
where (x ,y ) p∗.
n n
∼
With a (strong) assumption of uniform convergence,which is defined as
sup R(pˆ) Rˆ (pˆ) 0, (5.6)
N p
− →
pˆ
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)5.1. OUT-OF-DISTRIBUTION GENERALIZATION 53
wecanminimize RusingRˆ withalargeenoughdataset,i.e.,N ,andfind
→∞
a goodpredictive model pˆ. Ofcourse,since N is alwaysfinite inreality,there is
almost always non-zero generalization error.
SinceweneverhaveaccesstoR(pˆ)evenafterlearning,itisausualpracticeto
useaseparate(held-out)setofexamplesagaindrawnfromthesamedistribution
p∗ =q∗ asthetestsettoapproximatethegeneralizationerrorofatrainedmodel
pˆ. Let D′ = (x′,y′),...,(x′ ,y′ ) . Then,
{ 1 1 K K }
K
1
R(pˆ) l(y′,pˆ(y x′)). (5.7)
≈ K k | k
k=1
X
Such a test-set accuracy, or more simply a test accuracy, has been a workhorse
behind rapid advances in machine learning over the past several decades.
With this whole paradigm in your mind, it is important to notice that the
key assumption here is q∗(x,y) = p∗(x,y). In other words, we assume that an
instanceapredictivemodelwouldbe testedinthe deploymentwouldfollowthe
same distribution as that from which the training examples were drawn, i.e.,
q∗(x)=p∗(x). Furthermore,the conditionaldistributionoverthe outcomedoes
not change either, i.e., q∗(y x)=p∗(y x). In this case, there is no reason for us
| |
to consider the underlying generating process behind p∗ nor q∗ separately.
5.1.2 Out-of-Distribution Generalization
Impossibility of Out-of-Distribution (ood) generalization. In reality,
it is rarely that q∗ = p∗, because the world changes. When q∗ = p∗, we must
6
be careful about discussing generalization.We must be careful, because we can
always choose q∗ to be such that minimizing R(pˆ) in Eq. (5.5) would lead to
maximizing
Rq∗ (pˆ)=E [l(y,pˆ(y x))]. (5.8)
(x,y)∼q∗
|
Assume y 0,1 . Consider the following q∗, given p∗(x,y)=p∗(x)p∗(y x),
∈{ } |
q∗(x,y)=p∗(x)q∗(y x), (5.9)
|
where
q∗(y x)=1 p∗(y x). (5.10)
| − |
That is, the mapping fromx to y is reversed.When x was more probable to be
observedtogether with y =1 under p∗, it is now more probable to be observed
together with y =0 now under q∗, and vice versa.
If we take the log loss, which is defined as
l(y,pˆ(y x))= logpˆ(y x), (5.11)
| − |54 CHAPTER 5. CAUSALITY AND MACHINE LEARNING
learningcorrespondstominimizingtheKLdivergencefromthetruedistribution
to the learned, predictive distribution. Mathematically,
N
1
argmin l(y ,pˆ(y x )) argminE KL(p∗( x) pˆ( x)). (5.12)
n n n x
pˆ N | ≈ pˆ ·| k ·|
n=1
X
In other words, learning corresponds to recovering p∗ as much as we can for as
many probable x’s under p∗(x).
It is clear that minimizing this loss function would make our predictive
model worse on a new distribution (5.10). Because the following holds for any
particular example (x,y):
logp∗(y x)=log(1 q∗(y x)). (5.13)
| − |
Since log is a monotonic function, maximizing p∗ is equivalent to minimizing
q∗. As soon as we start minimizing the log loss for learning, out-of-distribution
generalization to q∗ gets worse, and there is no way to avoid it, other than not
learning at all.
This is a simple but clear example showing how out-of-distribution gener-
alization is not possible in general. There will always be a target distribution
that disagrees with the original distribution, such that learning on the latter is
guaranteed to hurt the generalization accuracy on the former. In general, such
a target distribution can be written down as
logq∗(y x) log(1 p∗(y x)). (5.14)
| ∝ − |
We canalsocome upwith a similarformulafor q∗(x), suchthat there is almost
no support overlapbetween p∗(x) and q∗(x).
Out-of-distribution generalization. Wethenmustnarrowdownthe scope
in order to discuss out-of-distribution generalization. There are many different
waysto narrowthe scope, andone wayis to ensure that the targetdistribution
q∗ is not too far from the original distribution p∗. Let D : R be a
+
P ×P →
(asymmetric)divergencebetweentwodistributions,suchthatthelargerD(p,q)
implies the greater difference between these two distributions, p and q. Then,
we can write a so-called distributionally-robust loss as
min sup E [l(y,pˆ(y x))], (5.15)
(x,y)∼q
pˆ q:D(p∗,q)≤δ |
wheresupisthesupremumwhichisthesmallestitemthatisgreaterthanequal
to all the other items in a partially ordered set [Shapiro, 2017].
The distributionally-robust loss above minimizes (min ) the expected loss
pˆ
(E [l(y,pˆ(y x))]) over the worst-casedistribution (sup ) within the diver-
(x,y)∼q | q
gence constraint (q : D(p∗,q) δ). Despite its generality, due to the freedom
≤
in the choice of the divergence D and the universality (the worst case), such
distributionally-robust optimization is challenging to use in practice. The chal-
lenge mainly comes from the fact that we must solve a nested optimization5.1. OUT-OF-DISTRIBUTION GENERALIZATION 55
problem, where for each update of pˆwe must solve another optimization prob-
lem that maximizes the loss w.r.t. the distribution q. This problem can be
cast as a two-player minimax game which is more challenging, both in terms
of convergence and its speed, than a more conventional optimization problem.
Furthermore,itisoftenunclearhowtochooseanappropriatedivergenceD and
the threshold δ, as these choices are not grounded in the problem of interest.
Instead, we are more interested in an alternative to the distributionally ro-
bustoptimizationapproach.Insteadofspecifying adivergence,we candescribe
how the distribution changes in terms of the probabilistic graphical model, or
equivalently the structural causal model underlying p∗ and q∗. Depending on
such a distributional change, we may be able to characterize the degree of gen-
eralization or even to come up with a better learning algorithm.
5.1.3 Case Studies
The label proportion shift. Let us consider a very basic example of a gen-
erative classier which assumes the following generating process:
y
x
Under this generating process, the joint probability is written as
p∗(x,y)=p∗(y)p∗(xy), (5.16)
|
and the posterior distribution over the output y is
p(y)p(xy) p(y)p(xy)
p(y x)= | = | . (5.17)
| p(x) p(y′)p(xy′)
y′∈Y |
P
Given a training set D = (x ,y ),...,(x ,y ) , where each (x ,y ) was
1 1 N N n n
{ }
drawn from the generating process above, that is,
y p∗(y) (5.18)
n
∼
x p∗(xy ). (5.19)
n n
∼ |
We can train a neural network classifier that takes as input x and outputs a
probability for each possible value of y. This neural network can be written as
exp(f (x;θ)+b )
y y
pˆ(y x;θ,b)= , (5.20)
| exp(f (x;θ)+b )
y′∈Y y′ y′
P56 CHAPTER 5. CAUSALITY AND MACHINE LEARNING
wheref (x;θ)isthey-thelementofthe -dimensionaloutputfromtheneural
y
network f, parametrized by θ and the b| iY as| vector b R|Y|.
∈
Inspecting thisneuralnet’s formulation,basedontheso-calledsoftmaxout-
put, we notice the following correspondences:
1. p∗(y) 1 exp(b )
≈ Zy y
2. p∗(xy) 1 exp(f (x;θ)),
| ≈ Zx|y y
where Z ’s and Z ’s are the normalization constants, which are cancelled out
y x|y
in Eq. (5.20).1 In other words, the bias b captures the marginal distribution
y
over the output, and the rest the conditional distribution over the input given
the output.
This view suggests a two-stage learning process. In the first stage, we sim-
ply set b to be logp∗(y) (and thereby set Z = 1 implicitly.) Then, we use
y y
optimization, such as stochastic gradient descent, to estimate the rest of the
parameters, θ. After learning is over, we get
exp(f (x;θˆ))
y
pˆ(y x)=pˆ(y) . (5.21)
| exp(f (x;θˆ))
y′ y′
P
It is important to notice that the second term on the right hand side is not the
estimateofp∗(xy),sincethedenominatormustincludetheextranormalization,
|
i.e. p(x). In other words,
exp(f (x;θˆ)) pˆ(xy)
y
= | . (5.22)
exp(f (x;θˆ)) pˆ(x)
y′ y′
P
This predictive model pˆ(y x) would work well even ona new instance under
|
theiidassumption,thatis,p∗(y x)=q∗(y x).Itishowevernotthecase,because
| |
q∗(y) = p∗(y). For instance, imagine we trained a COVID-19 diagnosis model
6
based on various symptoms, including cough sound, temperature and others,
during the winter of 2021. During this period, COVID-19 was rampant, that
is, p∗(y = 1) was very high. If we use this model however in the winter of
2024, the overall incident rate of COVID-19 is much lower. In other words,
q∗(y = 1) p∗(y = 1). This would lead to the overestimation of p(y = 1x),
≪ |
because the prediction is proportional to pˆ(y = 1) which is an estimate of the
outdated prior p∗(y =1) over the output not of the latest prior q∗(y =1). The
prediction becomes worse as q∗ deviates further away from p∗.
Onesimplewaytoaddressthisistoassumethata prioriitismoreprobable
forthelabelmarginal,i.e.,themarginaldistributionovertheoutput,tobecloser
to the uniform distribution. This is a reasonable assumption in many contexts
when we are not allowed any information about the situation. For instance, it
is perfectly sensible to assume that any givencoin is likely to be fair (that is, it
1exp(a+b)=exp(a)exp(b).5.1. OUT-OF-DISTRIBUTION GENERALIZATION 57
has the equalchance of landing head or tail.) In that case,we would simply set
the bias b to be an all-zero vector so that
exp(f (x;θˆ))
y
pˆ(y x)= . (5.23)
| exp(f (x;θˆ))
y′ y′
SometimeswearegivensomeglP impseintoq∗.InthecaseofCOVID-19,itis
difficulttocollect(x,y)pairsbutitisofteneasytocollecty’sbyvariousmeans,
including the survey and rapid testing in various event venues. Let qˆ(y) be the
estimate of q∗(y) from such a source. We can then replace pˆ(y) with this new
estimate in Eq. (5.21), resulting in
exp(f (x;θˆ))
y
pˆ(y x)=qˆ(y) . (5.24)
| exp(f (x;θˆ))
y′ y′
P
This is equivalently to replacing the bias b with logqˆ(y).
y
In practice, it is often the case that the number of y samples we can collect
is limited, leading to a high-variance estimate of q∗. We do not want to rely
solely on such an estimate. Instead, we can interpolate between pˆ(y) and qˆ(y),
leading to replacing the bias of each output with
b log(αpˆ(y)+(1 α)qˆ(y)), (5.25)
y
← −
with α [0,1]. α describes the degree of our trust in the original estimate of
∈
the label marginal. if α = 1, we end up with the original iid setup, and with
α=0, we fully trust our new estimate of the label marginal.
Data augmentation. Consider an object classification task, where the goal
is to build a classifier that categorizes the object in the center of an image into
oneofK predefinedclasses.Justlikebefore,weassumegenerativeclassification
in which the object label produces the image. We however further assume that
there exists an extra variable z = (i,j) that determines the precise position of
the object.
y z
x
During the training time, z follows a Normal distribution centered at the
centeroftheimage,i.e.,z (µ =[0,0]⊤,I ).Assumingthatthebackground
z 2
∼N
is randomly produced and does not correlate with the identity of the object in
the center, a classifier we train on data produced from this data generating58 CHAPTER 5. CAUSALITY AND MACHINE LEARNING
process should become blind to periphery pixels, since cov(x ,y) 0, where
mn
≈
m 0 and n 0. This can be written down as
| |≫ | |≫
p(x y) p(x ), (5.26)
mn nm
| ≈
meaning that x is independent of y.
mn
If we make the na¨ıve Bayes assumption, that is, all pixels are independent
conditioned on the label, we get the following expression of the posterior over
the label:
p(y x) p(y) p(x y) p(y) p(x y), (5.27)
mn mn
| ∝ | ∝ |
m,n (m,n)∈C
Y Y
whereC isasetofpixelsnearthecenter.Inotherwords,iftheobjectisoutside
the center of the image, the posterior distribution over the label would not
capture the actual identity of the object.
This dependence on the position arises from the existence of the hidden
variablez and its priordistribution p∗(z).If this prior distributionoverz shifts
in the test time, such that q∗(z) = (µ = [100,100]⊤,I ), all objects in the
z 2
N
images would be positioned on the top-right corners. The classifier based on
the training set with p∗(z) will then completely fail to detect and classify these
objects.
Because we assume to know the precise type of shift that is possible, we
can now mitigate this issue by data augmentation [Yaeger et al., 1996]. During
training,werandomlyshiftatrainingimagesuchthatthepositionoftheobject
intheimagevariesmoregreatlythanitusuallydoesintheoriginaltrainingset.
This can be thought of as introducing another random variable u such that
p(l z,u)=p(l), (5.28)
|
where l indicates the position of the object in an image. In other words, u
makes the position of an object independent of z, such that a classifier trained
on the training data with such data augmentation is able to detect objects in
any position, making it invariant to the distributional shift of z.
5.2 Invariance: Stable Correlations are Causal
Correlations
Once we have a probabilistic graphical model, or a structural causal model,
thatdescribesthegeneratingprocessandhaveacrispideaofwhichdistribution
shifts how, we can come up with a learning algorithm that may alleviate the
detrimental effect of such a distribution shift. It is however rare that we can
write down the description of a generating process in detail. It is even rarer to
have a crisp sense of how distributions shift between training and test times.
For instance, how would you describe relationships among millions of pixels of
a photo and unobserved identities of objects within it?5.2. INVARIANCE:STABLECORRELATIONSARECAUSALCORRELATIONS59
We can instead focus on devising an alternative way to determine which
correlationsareconsideredcausalandwhichothercorrelationsarespurious.The
originalway to distinguish causal and spurious correlationswas entirely reliant
on the availability of a full generating process in the form of a probabilistic
graphical model. One alternative is to designate correlation that holds both
during training and test time as causal, and the rest as spurious [Peters et al.,
2016]. In other words, any correlation that is invariant to the distributional
shift is considered causal, while any correlation that varies according to the
distributional shift is considered spurious. The goal is then to find a learning
algorithmthat canignore spurious (unstable) correlationswhile capturing only
(stable) causal correlations,for the purpose of prediction.
5.2.1 An Environment as a Collider
A case study: a bird or a branch? Imagine a picture of a bird takenfrom
a forest. The bird is probably somewhere near the center of the photo, since
the bird is the object of interest. It is extremely difficult to take a good picture
of a flying bird, and hence, it is highly likely that the bird is not flying but is
sitting. Since we are in a forest, it is highly likely that the bird is sitting on a
treebranchwiththebranchplacednearthebottomofthephoto.Comparethis
to a picture with a birdtakenfromthe same forest.the chance ofa tree branch
being solely near the bottom ofthe photo is pretty slim. After all,it is a forest,
and there are many branches all over. I can then create a bird detector using
either oftwo features;one is a feature describing a bird nearthe center and the
other is a feature describing the location of a tree branch. Clearly, we want our
bird detector to use the first feature, that is, to check whether there is a bird
in the picture rather than whether there is a tree branch near the bottom of
the picture, in order to tell whether there is a bird in the picture. Either way,
however,the bird detector would work pretty well in this situation.
A bird detector that relies on the position of a tree branch would not work
wellifsuddenlyallthepicturesarefromindoorsratherthanfromaforest.Most
of the birds indoors would be confined in their cages and would not be sitting
on tree branches. Rather, they would be sitting on an artificial beam or on the
ground.Ontheotherhand,abirddetectorthatreliesonthe actualappearance
features of a bird wouldcontinue to work well.That is, the correlationbetween
the label (‘bird’ or not) and the position of a tree branch (‘bottom’ or not) is
notstable,while the correlationbetweenthe label andthe bird-likeappearance
of a bird is stable. That is, the former is spurious, while the latter is causal.
A desirable bird detector would rely on the causal correlation and discard any
spurious correlation during learning.
An environment indicator is a collider. A precise mechanism by which
these unstable correlations arise can be extremely complex and is often un-
known. In other words, we cannot rely on having a precise structural casual
modelfromwhichwecanreadoutallpathsbetweentheinputandoutput,des-
ignate each as causal or spurious and adjust for those spurious paths. Instead,60 CHAPTER 5. CAUSALITY AND MACHINE LEARNING
we can think of an extremely simplified causal model that includes only three
variables; input x, output y and collider z, as in
z =e
x y
In this causal model, the collider z tell us whether we are in a particular
environment(e.g. a forestabove.)When we collect data fromthis causalmodel
while being conditioned on a particular environment, this conditioning on the
collider opens the path x z y, as we have learned earlier in §2.2.
→ ←
This way of thinking necessitates a bit of mental contortion. Rather than
saying that a particular environment affects the input and output, but we are
saying that a particular combination of the input and output probabilistically
defines an environment. That is, p(z x,y) is the distribution defined over all
|
possible environments z given the combination of x and y. Indeed, if x is a
picture with a tree branchnear the bottom of a picture and y states that there
is a bird, the probability of z being a forest is quite high. The environment
dependencecanthenbethoughtofasdrawingtraininginstancesfromthegraph
abovewheretheenvironmentwtakesaparticulartargetenvironmentvalue(e.g.
‘forest’.)
The most naive solution to this issue is to collect as much extra data as
possiblewhileavoidingsuch‘selectionbias’arisingfromconditioningthecollider
z on any particular value. If we do so, it is as if the collider z did not exist at
all, since marginalizing out z leads to the following simplified graph:
x y
Apredictivemodelfittedonthisgraphpˆ(y x)wouldcapturethecausalrela-
|
tionshipbetweenthe inputandoutput,since the conditionalandinterventional
distributionscoincideinthiscase,thatis,p∗(y x)=p∗(y do(x)).Thisapproach
| |
is however often unrealistic.
5.2.2 The Principle of Invariance
Invariant features. So far, we have considered each variable as an unbreak-
able unit. This is however a very strong assumption, and we should be able to
easily split any variable into two or more pieces. This is in fact precisely what
we often do by representing an object as a d-dimensional vector by embedding5.2. INVARIANCE:STABLECORRELATIONSARECAUSALCORRELATIONS61
it into the d-dimensional Euclidean space. We are splitting a variable x into a
set of d scalars which collectively representing the value the variable takes. We
can then look at a subset of these dimensions and instead of the full variable,
in which case the statistical as well as causal relationships with other variables
may change. This applies even to a 1-dimensional random variable, where we
can apply a nonlinear function to alter its relationship with other variables.
Consider the following structural causal model:
x ǫ , (5.29)
x
←
z 1(x>0)max(0,x+ǫ ), (5.30)
z
←
y 1(x 0)min(0,x+ǫ )+z, (5.31)
y
← ≤
where
ǫ (0,12) (5.32)
x
∼N
ǫ (0,12) (5.33)
z
∼N
ǫ (0,12). (5.34)
y
∼N
this model simplifies to y (0,12+12), where two unit variances come from
∼N
ǫ and either ǫ or ǫ depending on the sign of x. With the following nonlinear
x z y
function applied to x, however, y takes a different form:
g(x)=1(x 0)x. (5.35)
≤
By replacing x with g(x) above,
0, if y >0,
p(y) (5.36)
∝( (y;0,12+12), otherwise
N
This has the effect of removing the correlation flowing through the path x
→
z y, leaving only x y, because z is now a constant function regardless of
→ →
the value x takes. By inspecting the relationship between g(x) and y, we can
measure the direct causal effect of x on y.
This example illustrates that there may be a nonlinear function of x that
mayresultsinavariablethatpreservesenoughinformationtopreparethedirect
causalrelationship between x and the output y but removes any relationship x
has with the other variables in the structural causal model. In the context of
the environmentvariablez,whichisacollider,the goalisthento findafeature
extractor g such that the original graph is modified into
z
x y
g x′62 CHAPTER 5. CAUSALITY AND MACHINE LEARNING
Ideally, we want g such that g(x) explains the whole of x’s direct effect on
y. That is,
z
x y
g x′
Effectively,x′ worksasamediatorbetweenxandy.Becauseg isadetermin-
istic function, the effect of x on y is then perfectly captured by x′. In order to
understand when this would happen, it helps to consider the structural causal
model:2
x ǫ (5.37)
x
←
x′ g(x) (5.38)
←
y f (x,x′,ǫ ) (5.39)
y y
←
z f (x,y,ǫ ). (5.40)
z z
←
What changesbetweenthe lasttwographsis the thirdline in the structural
causal model above. The original one is
y f (x,x′,ǫ ), (5.41)
y y
←
while the new one is
y f′(x′,ǫ ). (5.42)
← y y
Forthis to happen, x′ mustabsorballrelationshipbetweenxandy.Thatis,x′
must be fully predictive of y, leaving only external noise ǫ and nothing more
y
to be captured by x.
Consider a slightly more realistic example of detecting a fox in a picture.
There are two major features of any object within any picture; shape and tex-
ture.Theshapeiswhatweoftenwantourpredictortorelyon,whilethetexture,
which is usually dominated by colour information, should be ignored. For in-
stance, if we have a bunch of pictures taken from any place in the sub-arctic
NorthernHemisphere, mostof the foxes in these pictures will be yellowishwith
white-colouredbreastanddark-colouredfeet andtail.Onthe other hand,foxes
in the pictures taken in the Arctic will largely be white only, implying that the
texture/colour feature of a fox is an environment-dependent feature and is not
2gcouldtakeasinputnoiseinadditiontox,buttostronglyemphasizethatx′isanonlinear
featureofx,weomitithere.5.2. INVARIANCE:STABLECORRELATIONSARECAUSALCORRELATIONS63
stable across the environments. Meanwhile, the shape information, a fox-like
shape, is the invariant feature of a fox across multiple environments. In this
case, x′ would be the shape feature of x.
We now see two criteria a function g must satisfy:
1. Given x and y, x′ =g(x) and z are independent.
2. x′ =g(x) is highly predictive of (correlated with) y.
Once we find such g, the (potentially biased) outcome can be obtained given
a new instance x, by fitting a predictive model pˆ(y x′) [Arjovsky et al., 2019].
|
That is,
yˆ(x)=E [y]. (5.43)
pˆ(y|x′=g(x))
This would be free of the spurious correlation arising from the environment
condition.
Learning. We now demonstrate one way to learn g to satisfy two conditions
aboveas muchaspossible.First,in orderto satisfythe firstcondition,we must
buildapredictorofz givenx′.Thispredictorshouldbenon-parametricinorder
to capture as much (higher-order) correlations that could exist between z and
x′. Let pˆ(z x′) = h(x′) be such a predictor obtained by solving the following
|
optimization problem:
N
1
min logp(zn g(xn)), (5.44)
p −N |
n=1
X
where (xn,yn,zn) is the n-th training example drawn from the original graph
whileensuringthatzn . isasetofenvironmentsinthetrainingset.Inother
∈E E
words, we have a few environments we observe and then condition sampling of
(x,y) on, and we use these examples to build an environment predictor from
g(x), given g.
The goal is then to minimize the following cost function w.r.t. g, where we
assume z is discrete:
C (g)= pˆ(z =z′ x′ =g(x))logpˆ(z =z′ x′ =g(x)). (5.45)
1
| |
z′∈Z
X
In other words, we maximize the entropy of pˆ(z x′), which is maximized when
|
it is uniform. When pˆ(z x′) is uniform, it is equivalent to z x′.
| ⊥⊥
One may ask where the condition on observing y went. This is hidden in
pˆ(z x′), since pˆwas estimated using (g(x),z) pairs derived from a set of triples
|
(x,y,z) drawn from the original graph, as clear from Eq. (5.44).
Of course, this cost function alone is not useful, since it will simply drive g
to be a constant function. The second criterion prevents this, and the second
criterion can be expressed as
N
1
C (g,q)= logq(yn g(xn)). (5.46)
2 −N |
n=1
X64 CHAPTER 5. CAUSALITY AND MACHINE LEARNING
This second criterion must be minimized with respect to both the feature ex-
tractor g and the y predictor q(y x′). This criterion ensures that the feature x′
|
is predictive of (that is, highly correlated with) the output y.
Given pˆ(z x′), the feature extractor is then trained to minimize
|
minC (g)+αC (g,q), (5.47)
1 2
g
where α is a hyperparameter and balances between C and C .
1 2
We then alteratebetween solvingEq.(5.44)to findpˆandsolvingEq.(5.47)
to find g and qˆ[Ganin et al., 2016]. This is a challenging, bi-level optimization
problem and may not even converge both in theory and in practice, although
this approachhas been used successfully in a few application areas.
The most important assumption here is that we have access to training
examplesfrommorethanoneenvironments.Preferably,wewouldhaveexamples
from all possible environments (that is, from all possible values z can take),
even if they do not necessarily follow p∗(z x,y) closely. If so, we would simply
|
ignore z by considering z as marginalized. If we have only a small number of
environmentsduringtraining,itwillbeimpossibleforustoensurethatg(x)does
notencode anyinformationaboutz.Thereis a connectionto generalization,as
better generalizationin pˆ(z x′) wouldimply a fewer environments necessaryfor
|
creatingagoodpˆ(z x′)andinturnforproducingamorestablefeatureextractor
|
g.
5.3 Prediction vs. Causal Inference
The major difference between prediction and causal inference is the goal. The
goal of prediction is to predict which value a particular variable, in our case
often the outcome variable, would take given that we have observed the values
oftheothervariables.Ontheotherhand,thegoalofcausalinferenceistoknow
which value the outcome variable would take had we intervened on the action
variable. This difference implies that causal inference may not be the best way
to predict what would happen based on what we have observed.
In the example of birds vs. branches above, if our goal is good prediction,
we would be certainly open to using the location of the branch as one of the
featuresaswell.Evenifalargeportionofthebirdinapictureisoccludedbye.g.
leaves,wemaybeabletoaccuratelypredictthatthereisabirdinthepictureby
noticing the horizontalbranchnear the bottom ofthe tree.This branchfeature
isclearlynotacausalfeature,butneverthelesshelps usmakebetterprediction.
In short, if I knew that the picture was taken in a forest, I would rely on both
the beak and the branch’s location to determine whether there is a bird in the
picture. This is however a brittle strategy, as it would certainly degrade my
prediction ability had the picture been taken somewhere else.
The invariant predictor q(y g(x)) from above is thus likely sub-optimal in
|
the context of prediction under any environment, although this may be the
right distribution to compute the causal effect of x and y. This is because the5.3. PREDICTION VS. CAUSAL INFERENCE 65
invariantpredictoronly explainsa partofy (markedredbelow),while ignoring
the open path (marked blue below) via the collider:
z
x y
g x′
Givenanenvironmentz =zˆ,wemustcapturebothcorrelationsarisingfrom
g(x) y and x zˆ y, in order to properly predict what value y is likely to
→ → ←
take given x. This can be addressed by introducing an environment-dependent
featureextractorh (x)thatisorthogonaltotheinvariantfeatureextractorg(x).
zˆ
We can impose such orthogonality (or independence) when learning h (x) by
zˆ
N
1
min logq(yn g(xn),h (xn)), (5.48)
h,q −N |
zˆ
n=1
X
with a given g. h would only capture about y that was not already captured
zˆ
by g, leading to the orthogonality. This however assumes that q is constrained
to the point that it cannot simply ignore g(x) entirely.
This view allows us to use a small number of labelled examples from a new
environment in the test time to quickly learn the environment-specific feature
extractor h while having learned the environment-invariant feature extractor
z
g in the training time from a diverse set of environments. One can view such a
schemeasmeta-learningortransferlearning,althoughneitheroftheseconcepts
is well defined.
It is possible to flip the process described here to obtain an environment-
invariant feature extractor g, if we know of an environment-dependent feature
extractor h , by
z
N
1
min logq(yn g(xn),h (xn)), (5.49)
g,q −N |
zˆ
n=1
X
assuming again that q is constrained to the point that it cannot simply ignore
h(x) entirely. This flipped approach has been used to build a predictive model
that is free of a known societal bias, of which the detector can be easily con-
structed [He et al., 2019].66 CHAPTER 5. CAUSALITY AND MACHINE LEARNING
5.4 A Case Study: Language Modeling with Pair-
wise Preference
An autoregressivelanguage model is described as a repeated application of the
next-token conditional probability, as in
T
p(w ,w ,...,w )= p(w w ). (5.50)
1 2 T t <t
|
t=1
Y
A conditional autoregressivelanguage model is exactly the same except that it
is conditioned on another variable X:
T
p(w ,w ,...,w x)= p(w w ,x). (5.51)
1 2 T t <t
| |
t=1
Y
Therearemanydifferentwaystobuildaneuralnetworktoimplementthenext-
token conditional distribution. We do not discuss any of those approaches, as
they are out of the course’s scope.
An interesting property of a language model is that it can be used for two
purposes:
1. Scoring a sequence: we can use p(w ,w ,...,w X) to score an answer
1 2 T
|
sequence w given a query x.
2. Approximately finding the best sequence: we can use approximate decod-
ing to find argmax p(wx).
w
|
This allows us to perform causal inference and outcome maximization simulta-
neously.
Consider the problem of query-based text generation, where the goal is to
produceanopen-endedanswerw toaqueryx.Becauseitisoftenimpossibleto
give an absolute score to the answer w given a query x, it is customary to ask
a human annotator a relative ranking between two (or more) answers w and
+
w given a query x. Without loss of generality, let w be the preferred answer
− +
to w .
−
We assume that there exists a strict total order among all possible answers.
That is,
1. Irreflexive: r(wx) <r(wx) cannot hold.
| |
2. Asymmetric: If r(wx) <r(w′ x), then r(wx) >r(w′ x) cannot hold.
| | | |
3. Transitive: If r(wx) < r(w′ x) and r(w′ x) < r(w′′ x), then r(wx) <
| | | | |
r(w′′ x).
|
4. Connected: If w = w′, then either r(wx) < r(w′ x) or r(wx) > r(w′ x)
6 | | | |
holds.
In other words, we can enumerate all possible answers according to their (un-
observed) ratings on a 1-dimensional line.5.4. ACASESTUDY:LANGUAGEMODELINGWITHPAIRWISEPREFERENCE67
A non-causal approach. It is then relatively trivial to train this language
model, assuming that we have a large amount of triplets
D = (x1,w1,w1),...,(xN,wN,wN) .
+ − + −
For each triplet, we ensu(cid:8)re that the language model puts(cid:9)a higher probability
on w than on w given x by minimizing the following loss function:
+ −
N
1
L (p)= max(0,m logp(wn x)+logp(wn x)), (5.52)
pairwise N − +| −|
n=1
X
where m [0, ) is a margin hyperparameter. For each triplet, the loss inside
∈ ∞
the summation is zero, if the language model puts the log-probability on w
+
more than that on w with the minimum margin of m.
−
Thislossaloneishowevernotenoughtotrainawell-trainedlanguagemodel
from which we can produce a high-quality answer. For we have only pair-wise
preference triplets for reasonable answers only. The language model trained in
this way is not encouraged to put low probabilities on gibberish. We avoid this
issuebyensuringthatthelanguagemodelputsreasonablyhighprobabilitieson
all reasonable answer by minimizing the following extra loss function:
N
1
L (p)= logp(wn x)+logp(wn x) , (5.53)
likelihood −2N +| −|
n=1
X(cid:0) (cid:1)
which corresponds to the so-called negative log-likelihood loss.
A causal consideration. This approach works well under the assumption
that it is only the content that is embedded in the answer w. This is unfortu-
nately not the case. Any answer is a combination of the content and the style,
and the latter should not be the basis on which the answer is rated. For in-
stance,one aspect of style is the verbosity.Often, a longer answeris considered
tobe highlyrated,becauseofthe subconsciousbiasby a humanraterbelieving
a better answer would be able to write a longer answer, although there is no
reason why there should not be a better and more concise answer.
This process canbe describedas the graphbelow, where r is the rating and
s is the style:
s
x w r
Thedirecteffectofw onthe ratingr isbasedonthecontent,butthenthere
is spourious correlation between w and r via the style s. For instance, s could68 CHAPTER 5. CAUSALITY AND MACHINE LEARNING
encode the verbosity which affects both how w is written and how a human
rater perceives the quality and gives the rating r. In the naive approachabove,
the language model, as a scorer, will fail to distinguish between these two and
capture both, which is clearly undesirable; a longer answer is not necessarily a
betteranswer.Inotherwords,alanguagemodelp trainedinapurelysupervised
0
learningwayabovewillscorewhighforbothcausalandspurious(vias)reasons.
Ananswerw sampledfromp canthenbe considereddependent uponnotonly
0
the question x itself but also of an unobserved style variable s.
Direct preference optimization [Rafailov et al., 2024] or unlikelihood
learning [Welleck et al., 2019]. We can resolve this issue by combining
twoideas we havestudiedearlier;randomizedcontrolledtrials (RCT; §3.3)and
inverse probability weighting (IPW; §4.2.1). First, we sample two answers, w
and w′, from the already trained model p , using supervised learning above:
0
w,w′ p (wx). (5.54)
0
∼ |
These two answers (approximately) maximize the estimated outcome (rating)
bycapturingboththecontentandstyle.Oneinterestingside-effectofimperfect
learning and inference (generation) is that both of these answers would largely
share the style. If we use s′ to denote that style, we can think of each answer
as sampled from wx,s′. With a new language model p (potentially initialized
1
|
fromp ),wecancomputetheratingafterremovingthedependenceonthestyle
0
s by IPW:
p (wx)
rˆ(wx)= 1 | . (5.55)
| p (wx)
0
|
This reminds us of do operation, resulting in the following modified graph:
s
x w r
Of course, this score rˆ does not mean anything, since p does not mean
1
anythingyet.Wehavetotrainp byaskinganexperttoprovidetheirpreference
1
betweenwandw′.Withoutlossofgenerality,letwbethepreferredanswerover
w′. That is, w =w and w =w′. We train p by minimizing
+ − 1
1 N p (wn x) p (wn x)
L′ (p )= max 0,m log 1 +| +log 1 −| , (5.56)
pairwise 1 N − p (wn x) p (wn x)
n=1 (cid:18) 0 +| 0 −| (cid:19)
X
whereweassumehaveN pairs.misamarginasbefore.Itispossibletoreplace
the margin loss with another loss function, such as a log loss or linear loss.5.5. SUMMARY 69
Thisprocedureencouragesp tocaptureonlythedirect(causal)effectofthe
1
answer on the rating, dissecting out the indirect (spurious) effect via the style
s. One training is done, we use p to produce a better answer, which dependes
1
lessonthespuriouscorrelationbetweentheanswerandthe ratingviathestyle.
Because this procedure is extremely implicit about the existence of and the
dependence on the style, it can be beneficial to repeat this procedure multiple
rounds in order to further remove the effect of the spurious correlation and
improve the quality of a generated answer [Ouyang et al., 2022].
5.5 Summary
In this chapter, we have learned the following concepts:
1. Out-of-distribution generalization and its impossibility
2. Invariance as a core principle behind out-of-distribution generalization
3. Preference modeling for training a language model, as causal learning
The goal of this chapter has been to introduce students to the concept of
learning beyond independently-and-identically-distribution settings, by relying
on concepts and frameworks from causal inference and more broadly causality.
The topics covered in this chapter are sometimes referred to as causal machine
learning [Kaddour et al., 2022].70 CHAPTER 5. CAUSALITY AND MACHINE LEARNINGChapter 6
Remaining Topics
As the purpose of this course is to be a thin and quick introductory course at
the intersectionof causalinference and machine learning,it is notthe intention
nor desirable to cover all topics in causal inference exhaustively. In this final
chapter,Idiscussafew topicsthatIdidnotfeelnecessarytobe includedinthe
main course but could be useful for students if they could be taught.
6.1 Other Techniques in Causal Inference
In practice the following observational causal inference techniques are widely
used:
• Regression in §3.2
• Inverse probability weighting in §4.2.1 and Matching in §4.2.2
• Instrument variables in §4.3
• Difference-in-difference
• Regression discontinuity design
• Double machine learning
Difference-in-difference and regression discontinuity design are heavily used
in practice, but they work for relatively more specialized cases, which is why
this course has omitted them so far. In this section, we briefly cover these two
approachesfor the sakeof completeness.Furthermore,this sectionwrapsup by
providingahigh-levelintuitionbehindamorerecentlyproposedandpopularized
technique of double machine learning.
7172 CHAPTER 6. REMAINING TOPICS
6.1.1 Difference-in-Difference
The average treatment effect (ATE) from §3.1 measures the difference between
the outcomes of two groups;treated and not treated, or more precisely, it mea-
suresthe differencebetweentheoutcome ofthe treatedgroupandthe expected
outcome over all possible actions.
One way to interpret this is to view ATE as checking what happens to a
treatedindividualhadthe individualwasnottreated,onaverage.First,we can
compute what happens to the individualonce they weretreated,onaverage,as
y1 =E E E [1(a=1)(y y )], (6.1)
diff x a ypre,ypost post − pre
where y and y are the outcomes before and after the treatment (a = 1).
pre post
We can similarly compute what happens to the individual had they not been
treated, on average,as well by
y0 =E E E [1(a=0)(y y )]. (6.2)
diff x a ypre,ypost post − pre
We now check the difference between these two quantities:
y1 y0 =E E [E [1(a=1)y 1(a=0)y ] (6.3)
diff − diff x a ypost post − post
E [1(a=1)y 1(a=0)y ] . (6.4)
−
ypre pre
−
pre
IfweusedRCTfrom§3.3toassigntheactionindependentofth(cid:3)ecovariatex
and also uniformly, the secondterm, that is the difference in the pre-treatment
outcome,shoulddisappear,sincethetreatmenthadnotbeengiventothetreat-
mentgroupyet.Thisleavesonlythefirstterm,whichispreciselyhowwewould
compute the outcome from RCT.
In an observational study, that is passive causal inference, we often do not
have a control over how the participants were split into treatment and placebo
groups. This often leads to the discrepancy in the base outcome between the
treated and placebo groups. In that case, the second term above would not
vanish but will work to remove this baseline effect.
Consider measuring the effect of a vitamin supplement on the height of
school-attending girls of age 10. Let us assume that this particular vitamin
supplement is provided to school children by default in Netherlands from age
10 but is not in North Korea. We may be tempted to simply measure the
averageheightsofschool-attendinggirlsofage10fromthesetwocountries,and
draw a conclusion whether this supplement helps school children grow taller.
This however would not be a reasonable way to draw the conclusion, since the
averages heights of girls of age 9, right before the vitamin supplement begins
to be provided in Netherlands, differ quite significantly between two countries
(146.55cmvs.140.58cm.)Wewouldratherlookathowmuchtallerthesechildren
grew between ages of 9 and 10.
Because we consider the difference ofthe difference in Eq.(6.3),we callthis
estimator difference-in-difference. This approach is widely used and was one of
the most successful cases of passive causal inference, dating back to the 19th
century [Snow, 1856].6.1. OTHER TECHNIQUES IN CAUSAL INFERENCE 73
In the context of what we have learned this course,let us write a structural
causal model that admits this difference-in-different estimator:
x ǫ (6.5)
x
←
a 1(x+ǫ ) (6.6)
a
←
y 1(x>0)y +αa+ǫ . (6.7)
0 y
←
With zero-meanandsymmetric ǫ andǫ , those withpositive x are morelikely
x a
to be assigned to a=1. Due to the first term in y, the outcome has a constant
bias y when x is positive. In other words,those, who are likely to be given the
0
treatment, have y added to the outcome regardless of the treatment (a = 1)
0
itself, since +y does not depend on a. The difference-in-difference estimator
0
removes the effect of y from estimating α which is the direct causal effect of a
0
on y.
This tells us when the difference-in-difference estimator works, and how we
can extend it further. For instance, it is not necessary to assume the linearity
between a and y. I leave it to you as an exercise.
6.1.2 Regression Discontinuity
Another popular technique for passive causal inference is called regression dis-
continuity [see Imbens and Lemieux, 2008, and references therein]. Regression
discontinuity assumes that there exists a simple rule to determine to which
group, either treated or placebo, an individual is assigned based on the covari-
ate x. This rule can be written down as
1, if x c
a= d ≥ 0 (6.8)
(0, otherwise.
If the d-th covariate crosses over the threshold c , the individual is assigned to
0
a=1.
We further assume that the outcome given a particular action is a smooth
function of the covariate. That is, the outcome of a particular action, f(aˆ,x),
changessmoothlyespeciallyaroundthe thresholdc .Inotherwords,haditnot
0
been for the assignment rule above, lim f(aˆ,x) = lim f(aˆ,x). There
xd→c0 c0←xd
is no discontinuity of f(aˆ,x) at x =c ,and we canfit a smoothpredictor that
d 0
extrapolates well to approximate f(aˆ,x) (or E f(aˆ,x x =c ).)
x d′6=d d′ ∪ c 0
Ifweassumethatthethresholdc waschosenarbitrarily,thatisindependent
0
ofthevaluesofx ,itfollowsthatthedistributionsoverx beforeandafterc
6=d 6=d 0
toremainthesameatleastlocally.1Thismeansthattheassignmentofanaction
a and the covariate other than x are independent locally, i.e., x c ǫ,
d d 0
| − | ≤
where ǫ defines the radius of the local neighbourhood centered on c . Thanks
0
to this independence, which is the key difference between the conditional and
1Thisprovidesagoodgroundfortestingthevalidityofregressiondiscontinuity.Ifthedis-
tributionsofxbeforeandafterc0differsignificantlyfromeachother,regressiondiscontinuity
cannotbeused.74 CHAPTER 6. REMAINING TOPICS
interventional distributions, as we have seen repeatedly earlier, we can now
compute the average treatment effect locally (so is often called a local average
treatment effect) as
LATE=E [1(x c ǫ)f(1,x) f(0,x)] (6.9)
x d 0
| − |≤ −
=E [f(1,x)] E [f(0,x)]. (6.10)
x:|xd−c0|≤ǫ
−
x:|xd−c0|≤ǫ
Ofcourse,ourassumptionhereis that wedo notobservedx .Evenworse,
6=d
we never observe f(1,x) when x < c and f(0,x) when x > c . Instead, we
d 0 d 0
canfitanon-parametricregressionmodelfˆ(aˆ,x )toapproximateE f(aˆ,x)
d x6=d|xd
andexpect(orhope?)thatitwouldextrapolateeitherbeforeorafterthethresh-
old c . Then, LATE becomes
0
c0+ǫ
LATE= fˆ(1,x ) fˆ(0,x )dx (6.11)
d d d
−
Zc0−ǫ
= fˆ(1,c ) fˆ(0,c ), (6.12)
ǫ→0 0 0
−
thanks to the smoothness assumption of f.
Thefinallineabovetellsusprettyplainlywhythisapproachiscalledregres-
sion discontinuity design. We literally fit two regression models on the treated
andplacebogroupsandlookattheirdiscrepancyatthedecisionthreshold.The
amountofthediscrepancyimpliesthe changeinthe outcomedue tothechange
in the action, of course under the strong set of assumptions we have discussed
so far.
6.1.3 Double Machine Learning
Recent advances in machine learning have open a door to training large-scale
non-parametric methods on high-dimensional data. This allows us to expand
some of the more conventional approaches. One such example is double ma-
chine learning [Chernozhukov et al., 2018]. We briefly describe one particular
instantiation of double machine learning here.
Recall the instrument variable approach from §4.3. The basic idea was to
noticethattheactionawasdeterminedusingtwoindependentsourcesofinfor-
mation, the confounder x and the external noise ǫ :
a
a f (x,ǫ ), (6.13)
a a
←
with x ǫ . We then introduced an instrument z that is a subset of ǫ , such
a a
⊥⊥
that z is predictive of a but continues to be independent of x. From z, using
regression,wecapture apartofvariationina thatis independent ofx,in order
to severe the edge from the confounder x to the outcome y. Then, we use this
instrument-predicted action a′ to predict the outcome y. We can instead think
offitting a regressionmodel g fromx to a and use the residual a =a g (x)
a ⊥ a
−
as the component of a that is independent of x, because the residual was not
predictable from x.6.2. BEHAVIOURCLONINGFROMMULTIPLEEXPERTPOLICIESREQUIRESAWORLDMODEL75
Thisprocedurecannowbe appliedtotheoutcomewhichiswrittendownas
y f (a ,x,ǫ ). (6.14)
y ⊥ y
←
Because x and a are independent, we can estimate the portion of y that is
⊥
predictable from y by building a predictor g of y given x. The residual y =
y ⊥
y g (x) is then what cannot be predicted by x, directly nor via a. We are
y
−
in fact relying on the fact that such a non-parametric predictor would capture
both causal and spurious correlations indiscriminately.
a isasubsetofathatisindependentoftheconfounderx,andy isasubset
⊥ ⊥
of y that is independent of the confounder x. The relationship between a and
⊥
y must then be the direct causal effect of the action on the outcome. In other
⊥
words,wehaveremovedtheeffectofxonatoclosethebackdoorpath,resulting
ina .Wehaveremovedtheeffectofxonytoreducenon-causalnoise,resulting
⊥
y . What remains is the direct effect of a on the outcome y. We therefore fit
⊥
anotherregressionfroma toy ,inordertocapturethisremainingcorrelation
⊥ ⊥
that is equivalent to the direct cause of a on y.
6.2 Behaviour Cloning from Multiple Expert Poli-
cies Requires a World Model
A Markovdecisionprocess (MDP) is often describedas a tuple of the following
items:
1. : a set of all possible states
S
2. : a set of all possible actions
A
3. τ : : a transition dynamics. s′ =τ(s,a,ǫ).
S×A×E →S
4. ρ: R: a reward function. r=ρ(s,a,s′).
S×A×S →
The transition dynamics τ is a deterministic function but takes as input
noise ǫ , which overall makes it stochastic. We use p (s′ s,a) to denote the
τ
∈ E |
conditional distribution over the next state given the current state and action
by marginalizing out noise ǫ. The reward function r depends on the current
state, the action taken and the next state. It is however often the case that the
rewardfunction only depends on the next (resulting) state.76 CHAPTER 6. REMAINING TOPICS
A major goal is then to find a policy p : R that maximizes
π >0
S×A→
J(π)= p (s ) p (s ,a ) p (s s ,a ) γ0ρ(s ,a ,s )
0 0 π 0 0 τ 1 0 0 0 0 1
|
Xs0 Xa0 Xs1
(cid:0)
+ p (s ,a ) p (s s ,a ) γ1ρ(s ,a ,s )+ (6.15)
π 1 1 τ 2 1 1 1 1 2
| ··· !
Xa1 Xs2
(cid:0) (cid:1)
=E E
s0∼p0(s0) a0,s1∼pπ(a0|s0)pτ(s1|s0,a0)
∞
E γtρ(s ,a ,s ) (6.16)
a1,s2∼pπ(a1|s1)pτ(s2|s1,a1)
···"
t t t+1
#
t=0
X
∞
=E γtρ(s ,a ,s ) , (6.17)
p0,pπ,pτ t t t+1
" #
t=0
X
where p (s ) is the distribution over the initial state.
0 0
γ (0,1]is a discounting factor.The discountingfactor canbe viewed from
∈
twoangles.First,wecanviewit conceptuallyasa wayto expresshowmuchwe
care about the future rewards. With a large γ, our policy can sacrifice earlier
time steps’ rewards in return of higher rewards in the future. The other way
to think of the discounting factor is purely computational. With γ <1, we can
prevent the total return J(π) from diverging to infinity, even when the length
of each episode is not bounded.
As we have learned earlier when we saw the equivalence between the prob-
abilistic graphical model and the structural causal model in §1.1–1.2, we can
guess the form of π as a deterministic function:
a π(s,ǫ ). (6.18)
π
←
Together with the transition dynamics τ and the reward function ρ, we notice
that the Markov decision process can be thought of as defining a structural
causal model for each time step t as follows:
s is given. (6.19)
a π(s,ǫ ) (6.20)
π
←
s′ τ(s,a,ǫ ) (6.21)
s′
←
r ρ(s′,ǫ ), (6.22)
r
←
where we make a simplifying assumption that the reward only depends on the
landing state.
Graphically,6.2. BEHAVIOURCLONINGFROMMULTIPLEEXPERTPOLICIESREQUIRESAWORLDMODEL77
s
s′ r
a
Behaviour cloning. With this in our mind, let us consider the problem of
so-called ‘behavior cloning’. In behaviour cloning, we assume the existence of
anexpertpolicyπ∗ thatresultsinahighreturnJ(π∗)fromEq.(6.15)andthat
we have access to a large amount of data collected from the expert policy. This
dataset consists of tuples of current state s, action by the expert policy a and
the next state s′. We often do not observe the associated rewarddirectly.
D = (s ,a ,s′ ) N , (6.23)
{ n n n }n=1
where a p (as ) and s′ p (s′ s ,a ).
n ∼ π∗ | n n ∼ τ | n n
Behaviorcloningreferstotrainingapolicyπ thatimitatestheexpertpolicy
π∗ using this dataset. We train a new policy π often by maximizing
N
J (π)= logπ(a ,s ). (6.24)
bc n n
n=1
X
In other words, we ensure that the learned policy π puts a high probability on
the action that was taken by the expert policy π∗.
Behaviour cloning with multiple experts. It is however often that it is
not just one expert policy that was used to collect data but a set of expert-
like policies that collected these data points. It is furthermore often that we
do not know which such expert-like policy was used to produce each tuple
(s ,a ,s′ ). This necessitates us to consider the policy used to collect these
n n n
tuples as a random variable that we do not observe, resulting the following
graphical model:2
2Iamonlydrawingtwotimestepsforsimplicity,however, withoutlossofgenerality.78 CHAPTER 6. REMAINING TOPICS
π˜
a t−1 a t
s s s
t−1 t t+1
r t−1 r t r t+1
The inclusion of an unobserved π˜ makes the original behaviour cloning ob-
jective in Eq. (6.24) less than ideal. In the original graph, because we sampled
boths anda withoutconditioning ons′, therewasonly oneopenpathbetween
s and a, that is, s a. We could thereby simply train a policy to capture the
→
correlationbetweensandatolearnthepolicywhichshouldcapturep(ado(s)).
|
With the unobserved variable π, this does not hold anymore.
Consider (s ,a ). There are two open paths between these two variables.
t t
The first one is the original direct path; s a . There is however the second
t t
→
path now; s a π a . If we na¨ıvely train a policy π on this dataset,
t t−1 t
← ← →
thispolicywouldlearntocapturethecorrelationbetweenthecurrentstateand
associated action arising from both of these paths. This is not desirable as the
secondpathis not causal,as we discussedearlierin§2.2. In otherwords,π(as)
|
would not correspond to p(ado(s)).
|
In order to block this backdoor path, we can use the idea of inverse prob-
ability weighting (IPW; §4.2.1). If we assume we have access to the transition
model τ, we can use it to severe two direct connections into s ; s s and
t t−1 t
→
a s , by
t−1 t
→
p (a s )
E [a ]=E π t | t a . (6.25)
at∼pπ(at|do(st)) t st p (s s ,a ) t
(cid:20) τ t | t−1 t−1 (cid:21)
Learned transition:aworld model. Ofcourse,weoftendonothaveaccess
toτ directly,butmustinferthistransitiondynamicsfromdata.Unlikethepolicy
s a, fortunately, the transition (s,a) s′ is however not confounded by π.
→ →
We can therefore learn an approximate transition model, which is sometimes
referred to as a world model [LeCun, 2022, and references therein], from data.
This can be done by
N
τˆ=argmax logp (s′ s ,a ). (6.26)
τ
τ n| n n
n=1
X6.3. SUMMARY 79
Deconfounded behaviour closing. Once training is done, we can use τˆ in
place of the true transition dynamics τ, to train a de-confounded policy by
N p (a′ s′ )
πˆ =argmax log π n| n , (6.27)
π
n=1
p τˆ(s′ n|s n,a n)
X
where a′ is the next action in the dataset. That is, the dataset now consists
n
of (s ,a ,s′ ,a′ ) rather than (s ,a ,s′ ). This effectively makes us lose a few
n n n n n n n
examplesfromtheoriginaldatasetthatcorrespondtothefinalstepsofepisodes,
althoughthisisasmallpricetopaytoavoidtheconfoundingbymultipleexpert
policies.
Causal reinforcement learning. This is an example of how causality can
assist us in identifying a potential issue a priori and design a better learning
algorithm without relying on trials and errors. In the context of reinforcement
learning, which is a sub-field of machine learning focused on learning a policy,
such as like behaviour cloning, this is often referredto as and studied in causal
reinforcement learning [Bareinboim, 2020].
6.3 Summary
In this final chapter, I have touched upon a few topics that were left out from
themainchaptersperhapsfornoparticularstrongreason.Thesetopicsincluded
1. Difference-in-Difference
2. Regression discontinuity
3. Double machine learning
4. A taste of causal reinforcement learning
There are many interesting topics that were not discussed in this lecture
note both due to the lack of time as well as the lack of my own knowledge
and expertise. I find the following two areas to be particular interesting and
recommend you to follow up on.
1. Counterfactualanalysis:Canwebuildanalgorithmthatcanimaginetak-
ing an alternative action and guess the resulting outcome instead of the
actual outcome?
2. (Scalable) causal discovery: How can we infer useful causal relationship
among many variables?
3. Beyondinvariance(§5.2.2): Invarianceis a strongassumption.Can we re-
laxthisassumptiontoidentifyamoreflexiblenotionofcausalprediction?80 CHAPTER 6. REMAINING TOPICSBibliography
R. Allesiardo, R. F´eraud, and O.-A. Maillard. The non-stationary stochastic
multi-armedbandit problem. International Journal of Data Science and An-
alytics, 3:267–283,2017.
M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk mini-
mization. arXiv preprint arXiv:1907.02893, 2019.
E. Bareinboim. Icml tutorial on causal reinforcement learning, 2020.
T.Brown,B.Mann,N.Ryder,M.Subbiah, J.D. Kaplan,P.Dhariwal,A.Nee-
lakantan,P.Shyam,G.Sastry,A.Askell,etal. Languagemodelsarefew-shot
learners. Advances in neural information processing systems, 33:1877–1901,
2020.
V.Chernozhukov,D.Chetverikov,M.Demirer,E.Duflo,C.Hansen,W.Newey,
andJ.Robins. Double/debiased machinelearningfortreatmentandstructural
parameters. Oxford University Press Oxford, UK, 2018.
S. Cunningham. Causal inference: The mixtape. Yale university press, 2021.
C. for Disease Control, Prevention, et al. Tuskegee study of untreated syphilis
inthenegromale. US Public Health Service Syphilis Study at Tuskegee,2020.
Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
M.March,andV.Lempitsky.Domain-adversarialtrainingofneuralnetworks.
Journal of machine learning research, 17(59):1–35,2016.
H.He,S.Zha,andH.Wang. Unlearndatasetbiasinnaturallanguageinference
by fitting the residual. arXiv preprint arXiv:1908.10763, 2019.
D. J. Im and K. Cho. Active and passive causal inference learning. arXiv
preprint arXiv:2308.09248, 2023.
G. W. Imbens and T. Lemieux. Regression discontinuity designs: A guide to
practice. Journal of econometrics, 142(2):615–635,2008.
G. W. Imbens and D. B. Rubin. Causal inference in statistics, social, and
biomedical sciences. Cambridge University Press, 2015.
8182 BIBLIOGRAPHY
J. Kaddour, A. Lynch, Q. Liu, M. J. Kusner, and R. Silva. Causal machine
learning: A survey and open problems. arXiv preprint arXiv:2206.15475,
2022.
D.P.KingmaandM.Welling. Auto-encodingvariationalbayes. arXiv preprint
arXiv:1312.6114, 2013.
Y. LeCun. A path towards autonomous machine intelligence version 0.9. 2,
2022-06-27. Open Review, 62(1), 2022.
L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach
to personalized news article recommendation. In Proceedings of the 19th in-
ternational conference on World wide web, pages 661–670,2010.
R.e.NahtaandF.Esteva. Trastuzumab:triumphsandtribulations. Oncogene,
26(25):3637–3643,2007.
L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,
S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow in-
structions with human feedback. Advances in Neural Information Processing
Systems, 35:27730–27744,2022.
J. Pearl. Causality. Cambridge university press, 2009.
J. Peters,P. Bu¨hlmann, and N. Meinshausen. Causal inference by using invari-
ant prediction: identification and confidence intervals. Journal of the Royal
Statistical Society Series B: Statistical Methodology, 78(5):947–1012,2016.
R.Rafailov,A.Sharma,E.Mitchell,C.D.Manning,S.Ermon,andC.Finn. Di-
rectpreferenceoptimization:Yourlanguagemodelissecretlyarewardmodel.
Advances in Neural Information Processing Systems, 36, 2024.
A. Shapiro. Distributionally robuststochastic programming. SIAM Journal on
Optimization, 27(4):2258–2275,2017.
J.Snow. Onthemodeofcommunicationofcholera. Edinburgh Medical Journal,
1(7):668–670,1856.
S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural
text generationwith unlikelihood training. arXiv preprint arXiv:1908.04319,
2019.
L.Yaeger,R.Lyon,andB.Webb. Effectivetrainingofaneuralnetworkcharac-
ter classifier for wordrecognition. Advances in neural information processing
systems, 9, 1996.