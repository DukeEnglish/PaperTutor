2024IEEEINTERNATIONALWORKSHOPONMACHINELEARNINGFORSIGNALPROCESSING,SEPT.22–25,2024,LONDON,UK
Kolmogorov-Arnold Networks (KANs) for Time Series Analysis
CristianJ.Vaca-Rubio,LuisBlanco,RobertoPereira,andMa`riusCaus
CentreTecnolo`gicdeTelecomunicacionsdeCatalunya(CTTC),Barcelona,Spain.
Email: {cvaca,lblanco,rpereira,mcaus}@cttc.es
ABSTRACT ofthemostwell-knowntechniquesistheAutoRegressiveIn-
tegratedMovingAverage(ARIMA)model, whichcombines
This paper introduces a novel application of Kolmogorov-
auto-regression,integration,andmovingaveragestoforecast ArnoldNetworks(KANs)totimeseriesforecasting,leverag-
data. The authors in [1] detailed this approach, providing a
ingtheiradaptiveactivationfunctionsforenhancedpredictive
comprehensivemethodologyfoundationalforsubsequentsta-
modeling. Inspired by the Kolmogorov-Arnold representa-
tisticalforecastingmethods.ExtensionsofARIMA,likeSea-
tion theorem, KANs replace traditional linear weights with
sonalARIMA(SARIMA),adaptthemodeltohandleseason-
spline-parametrized univariate functions, allowing them to
alityindataseries,particularlyusefulinfieldslikeretailand
learn activation patterns dynamically. We demonstrate that
climatology [2]. Exponential Smoothing techniques consti-
KANs outperforms conventional Multi-Layer Perceptrons
tute another popular set of traditional (non-ML-based) fore-
(MLPs) in a real-world satellite traffic forecasting task, pro-
casting methods. They are characterized by their simplicity
vidingmoreaccurateresultswithconsiderablyfewernumber
andeffectivenessinhandlingdatawithtrendsandseasonality.
oflearnableparameters. Wealsoprovideanablationstudyof
Anexponentofthisfamilyoftechniquesistheso-calledHolt-
KAN-specific parameters impact on performance. The pro-
Wintersseasonaltechnique,whichadjuststhemodelparame-
posed approach opens new avenues for adaptive forecasting
tersinresponsetochangesintrendandseasonalitywithinthe
models, emphasizing the potential of KANs as a powerful
timeseriesdata[3,4]. Thesemodelshavebeenwidelyused toolinpredictiveanalytics.
fortheirefficiency,interpretabilityandimplementation.
Index Terms— Kolmogorov-Arnold Networks, ML, More recently, ML models have significantly impacted
Timeseries the forecasting landscape by handling large datasets and
1. INTRODUCTION capturing complex nonlinear relationships that traditional
methodscannot. Inrecentyears, DeepLearning(DL)-based
Time series forecasting plays a key role in a wide range of forecasting models have gained popularity, motivated by the
fields, driving critical decision-making processes in finance, notable achievements in many fields. For instance, neural
economics,medicine,meteorology,andbiology,amongoth- networks have been extensively studied due to their flexibil-
ers, reflecting the wide applicability and its significance ityandadaptability. SimpleMulti-LayerPerceptron(MLPs)
across many domains. It involves predicting future values were among the first to be applied to forecasting problems,
basedonthepreviouslyobserveddatapoints. Withthisgoal demonstratingsignificantpotentialinnon-lineardatamodel-
inmind,understandingthedynamicsoftime-dependentphe- ing[5].
nomenaisessentialandrequiresunveilingthepatterns,trends Built upon these light models, more complex architec-
and dependencies hidden with the historical data. While tures have progressively expanded the capabilities of neural
conventional approaches have been traditionally centered networks in time series forecasting. Typical examples are
on parametric models grounded in domain-specific knowl- recurrent neural network architectures such as Long Short-
edge,suchasautoregressive(AR),exponentialsmoothing,or TermMemory(LSTM)networksandGatedRecurrentUnits
structuraltimeseriesmodels, contemporaryMachineLearn- (GRUs),whicharedesignedtomaintaininformationinmem-
ing (ML) techniques offered a pathway to discern temporal oryforlongperiodswithouttheriskofvanishinggradients–
patternssolelyfromdata-driveninsights. a common issue in traditional recurrent networks [6]. On a
Non-MLmethodstraditionallytacklethetimeseriesfore- relatednote,ConvolutionalNeuralNetworks(CNNs),which
castingproblemandoftenrelyonstatisticalmethodstopre- arefundamentallyinspiredbyMLPs,arealsoextensivelyem-
dict future values based on previously observed data. One ployed in time series forecasting. These architectures are
particularlyefficientatprocessingtemporalsequencesdueto
ThisworkhasbeensubmittedtoIEEEforpossiblepublication. Copy-
theirstrongspatialpatternrecognitioncapabilities. Thecom-
right may be transferred without notice, after which this version may no
longerbeaccessible. binationofCNNswithLSTMshasresultedinmodelsthatef-
4202
yaM
41
]PS.ssee[
1v09780.5042:viXraficientlyprocessbothspatialandtemporaldependencies,en- tion3willpresenttheexperimentalsetupdescription. Simu-
hancingforecastingaccuracy[7]. Thesemodelshavestarted lation results analyzing the performance of KANs with real-
to outperform established benchmarks in complex forecast- world datasets are shown in Section 4. Finally, concluding
ing tasks, motivating a significant shift towards more com- remarksareprovidedinSection5.
plexnetworkstructures. Unfortunately,asthemajorityofthe 2. PROBLEMSTATEMENT
models mentioned above are inspired by MLP architecture,
theytendtohavepoorscalinglaw,i.e.,thenumberofparam- Weformulatethetrafficforecastingproblemasatimeseries
eters in MLPs networks do not scale linear with the number at time t represented by y . Our objective is to predict the
t
oflayers,andoftenlackinterpretability. futurevaluesoftheseries
A recent study in reference [8], which was presented
y =[y ,y ,...,y ] (1)
just 12 days prior to the submission of this paper, intro- t0:T t0 t0+1 t0+T
ducesKolmogorov-ArnoldNetworks(KANs),anovelneural
basedsolelyonitshistoricalvalues
network architecture designed to potentially replace tradi-
tional multilayer perceptrons. KANs represent a disruptive x =[x ,...,x ,x ] (2)
t0−c:t0−1 t0−c t0−2 t0−1
paradigmshift,andasapotentialgamechangerhaverecently
attracted the interest of the AI community worldwide. They wheret denotesthestartingpointfromwhichfuturevalues
0
are inspired by the Kolmogorov-Arnold representation theo- y ,t = t ,...,T aretobepredicted. Wedifferentiatethehis-
t 0
rem [9], [10]. Unlike MLPs, which are inspired by the uni- toricaltimerange[t −c,t −1]andtheforecastrange[t ,T]
0 0 0
versal approximation theorem, KANs take advantage of this as the context and prediction lengths, respectively. Our ap-
representation theorem to generate a different architecture. proach focuses on generating point forecasts for each time
They innovate by replacing linear weights with spline-based stepinthepredictionlength, aimingtoachieveaccurateand
univariate functions along the edges of the network, which reliableforecasts. Figure1showsanexemplarytimeseries.
are structured as learnable activation functions. This design
not only enhances the accuracy and interpretability of the 2.1. Kolmogorov-Arnoldrepresentationbackground
networks,butalsoenablesthemtoachievecomparableorsu-
periorresultswithsmallernetworksizesacrossvarioustasks, ContrarytoMLPs,whicharebasedonuniversalapproxima-
suchasdatafittingandsolvingpartialdifferentialequations. tion theorem, KANs rely on the Kolmogorov-Arnold repre-
While KANs show promise in improving the efficiency and sentationtheorem,alsoknownastheKolmogorov-Arnoldsu-
interpretability of neural network architectures, the study perposition theorem. A fundamental result in the theory of
acknowledges the necessity for further research into their dynamicalsystemsandergodictheory. Itwasindependently
robustnesswhenappliedtodiversedatasetsandtheircompat- formulated by Andrey Kolmogorov and Vladimir Arnold in
ibilitywithotherdeeplearningarchitectures. Theseareasare themid-20thcentury.
crucialforunderstandingthefullpotentialandlimitationsof Thetheoremstatesthatanymultivariatecontinuousfunc-
KANs. tionf,whichdependsonx=[x 1,x 2,...,x n],onabounded
domain,canberepresentedasthefinitecompositionofsim-
Ourpaperisaprospectivestudythatinvestigatestheap-
pler continuous functions, involving only one variable. For-
plication of KANs to time series forecasting. To the best
mally, a real, smooth, and continuous multivariate function
of authors’ knowledge, not previously explored in the liter-
f(x) : [0,1]n → Rcanberepresentedbythefinitesuperpo-
ature. We aim to evaluate the practicality of KANs in real-
sitionofunivariatefunctions[9]:
world scenarios, analyzing their efficiency in terms of the
number of trainable parameters and discussing how the ad-  
2n+1 n
ditional degrees of freedom might affect forecasting perfor- (cid:88) (cid:88)
f(x)= Φ i ϕ i,j(x j), (3)
mance. Herein, we will assess the performance using real-
i=1 j=1
world satellite traffic data. This exploration seeks to further
validateKANsasaversatiletoolinadvancedneuralnetwork whereΦ :R→Randϕ :[0,1]→Rdenotetheso-called
i i,j
designfortimeseriesforecasting,althoughmorecomprehen- outer and inner functions, respectively. One might initially
sivestudiesarerequiredtooptimizetheiruseacrossbroader perceive this development as highly advantageous for ML.
applications. Finally, we note that due to the early stage of The task of learning a high-dimensional function simplifies
KANs, it is fair to compare it as a potential alternative to to learning a polynomial number of one dimensional func-
MLPs, but further investigation is needed to develop more tions. Nevertheless, these 1-dimensional functions can ex-
complex solutions that can compete with advanced architec- hibit non-smooth characteristics, rendering them potentially
turessuchasLSTMs,GRUsandCNNs. unlearnable in practical contexts. As a result of this prob-
This paper is structured as follows. Section 2 presents lematicbehavior,theKolmogorov-Arnoldrepresentationthe-
theproblemstatement,providingfundamentalbackgroundon orem has been traditionally disregarded in machine learning
Kolmogorov-ArnoldrepresentationtheoremandKANs. Sec- circles, recognized as theoretically solid, but ineffective inpractice. Unexpectedly, the theoretical result in [8] has re-
Prediction
centlyemergedasapotentialgamechanger, pavingtheway Context length
length
for new network architectures, inspired by the Kolmogorov-
Arnoldtheorem.
2.2. Kolmogorov-Arnoldnetworkbackground
Theauthorsin[8]mentionthatequation(3)hastwolayersof
non-linearities,with2n+1termsinthemiddlelayer. Thus,
we only need to find the proper functions inner univariate
functionsϕ andΦ thatapproximatethefunction. Theone
i,j i
dimensional inner functions ϕ can be approximated using
i,j
B-splines. Asplineisasmoothcurvedefinedbyasetofcon- Fig.1:Exampleofnormalizedsatellitetrafficseriesdatawith
trol points or knots. Splines are often used to interpolate or the conditioning and prediction lengths denoted in blue, and
approximatedatapointsinasmoothandcontinuousmanner. red,respectively.
Asplineisdefinedbytheorderk(k =3isacommonvalue),
whichreferstothedegreeofthepolynomialfunctionsusedto weaknesses. Splines stand out for their accuracy on low-
interpolateorapproximatethecurvebetweencontrolpoints. dimensional functions and allow transition between various
Thenumberofintervals,denotedbyG,referstothenumber resolutions. Nevertheless, they suffer from a major dimen-
ofsegmentsorsubintervalsbetweenadjacentcontrolpoints. sionality problem due to their inability to effectively exploit
Insplineinterpolation,thedatapointsareconnectedbythese compositional structures. In contrast, MLPs experience a
segmentstoformasmoothcurve(ofG+1gridpoints). Al- lower dimensionality problem, due to their ability to learn
thoughsplinesotherthanB-splinescouldalsobeconsidered, features, but exhibit lower accuracy than splines in low di-
thisistheapproachproposedin[8]. Equation(3)canberep- mensions due to their inability to optimize univariate func-
resentedasa2-layer(oranalogous2-depth)network,withac- tions effectively. KANs have by their construction 2 levels
tivationfunctionsplacedattheedges(insteadofatthenodes) ofdegreesoffreedom. Consequently, KANspossesstheca-
and nodes performing a simple summation. Such two-layer pability not only to acquire features, owing to their external
networkistoosimplistictoeffectivelyapproximateanyarbi- resemblance to MLPs, but also to optimize these acquired
traryfunctionwithsmoothsplines. Forthisreason,reference features with a high degree of accuracy, facilitated by their
[8]extendstheideasdiscussedabovebyproposingageneral- internalresemblancetosplines. Tolearnfeaturesaccurately,
izedarchitecturewithwideranddeeperKANs. KANs can capture compositional structure (external degrees
A KAN layer is defined by a matrix Φ [8] composed by offreedom),butalsoeffectivelyapproximateunivariatefunc-
univariate functions {ϕ (·)} with i = 1,...,N and j = tions(internaldegreesoffreedomwiththesplines). Itshould
i,j in
1,...,N , where N and N denote the number of in- be noted that by increasing the number of layers L or the
out in out
putsandthenumberofoutputs,respectively,andϕ arethe dimension of the grid G, we are increasing the number of
i,j
trainablesplinefunctionsdescribedabove. Noteaccordingto parametersand,consequently,thecomplexityofthenetwork.
the previous definition, the Kolmogorov-Arnold representa- This approach constitutes an alternative to traditional DL
tion theorem presented in Section 2.1 can be expressed as a models, which are currently relying on MLP architectures
two-layerKAN.TheinnerfunctionsconstituteaKANlayer andmotivatesourextensionofthiswork.
withN = nandN = 2n+1, whiletheexternalfunc-
in out
tions constitute another KAN layer with N in = 2n+1 and 2.3. KANtimeseriesforecastingnetwork
N =1.
out
We frame our traffic forecasting problem as a supervised
LetusdefinetheshapeofaKANby[n ,...,n ],where
1 L+1
learning framework consisting of a training dataset with
LdenotesthenumberoflayersoftheKAN.Itisworthnot-
input-output{x ,y }intheconditionandpredic-
ingtheKolmogorov-ArnoldtheoremisdefinedbyaKANof t0−c:t0−1 t0:T
tionlengths. Wewanttofindf thatapproximatesy ,i.e.,
shape[n,2n+1,1]. AgenericdeeperKANcanbeexpressed t0:T
y ≈f(x ). Foreaseofnotation,wedescribeour
bythecompositionLlayers: t0:T t0−c:t0−1
framework as a two-layer (2-depth) KAN [N , n, N ](note
i o
y=KAN(x)=(Φ ◦Φ ◦...◦Φ )x. (4) that to comply with the original paper notation, the input
L L−1 1
layerisnotaccountedasalayerperse). Theoutputandinput
Notice that all the operations are differentiable. Conse- layerswillbecomprisedofN ,andN nodescorresponding
o i
quently,KANscanbetrainedwithbackpropagation. Despite to the total amount of time steps in (1) and (2), while the
their elegant mathematical foundation, KANs are simply transformation/hiddenlayerofnnodes. Theinnerfunctions
combinations of splines and MLPs, which effectively ex- constituteaKANlayerwithN = N andN = n,while
in i out
ploit each other’s strengths while mitigating their respective the external functions constitute another KAN layer withTable1: Modelconfigurationsforsatellitetrafficforecasting
Model Configuration Timehorizon(h) Splinedetails Activations
MLP(3-depth) [168,300,300,300,24] Context/Prediction: 168/24 N/A ReLU(fixed)
MLP(4-depth) [168,300,300,300,300,24] Context/Prediction: 168/24 N/A ReLU(fixed)
KAN(3-depth) [168,40,40,24] Context/Prediction: 168/24 Type: B-spline,k =3,G=5 learnable
KAN(4-depth) [168,40,40,40,24] Context/Prediction: 168/24 Type: B-spline,k =3,G=5 learnable
canbeusedforAI-drivenpredictiveanalysis,toforecasttraf-
Context length Network structure Prediction length fic conditions, which is essential to avoid congestion and to
make efficient use of satellite resources. Endowing the net-
...
workwithintelligencewillbebeneficialtomeetthedifferent
.....
.
demandsofsatelliteapplications.
...
4. SIMULATIONRESULTS
..... .
... This section investigates the forecasting performance of dif-
ferent KAN and MLP architectures for predicting satellite
traffic over the six beam areas. Concretely, we have a con-
text length of 168 hours (one week) and a prediction length
Fig. 2: Example ofthe flowof informationin theKAN net-
of 24 hours (one day). This translates to T = 24, c = 168,
work architecture for our traffic forecasting task. Learnable
wherey =192in(1)and(2). Ourfocusisonevaluating
activationsarerepresentedinsideasquarebox. t0+T
the efficacy of KAN models compared to traditional MLPs.
We designed our experiments to compare models with simi-
lardepthsbutvaryingarchitecturestoanalyzetheirimpacton
N =nandN =N . OurKANcanbeexpressedbythe
in out o
forecastingaccuracyandparameterefficiency. Table1sum-
compositionof2layers:
marizestheparametersselectedforthisevaluation. Wehave
dataforthesixbeamsoveronemonth. Weusetwoweeks+1
y=KAN(x)=(Φ ◦Φ )x. (5)
2 1
dayfortrainingandoneweek+1dayfortestingforallbeams
where the output functions Φ generates the N outputs thatwerenotseenbythenetwork. Wetrainallthenetworks
2 o
with500epochsandAdamoptimizerwithalearningrateof
valuescorrespondingto(1)bydoingthetransformationfrom
0.001. Theselectedlossfunctionminimizesthemeanabso-
the previous layers. The proposed network can be used to
luteerror(MAE)ofthevaluesaroundthepredictionlength.
forecast future traffic data in the prediction length, based
solelyonthecontextlength. Fig. 2showsagenericrepresen-
tationforanyarbitrarynumberoflayersL. 4.1. Performanceanalysis
We analyze the forecasting performance in the prediction
3. EXPERIMENTALSETUP length. Figures 3a-c depicts the real traffic value used as
input(ingreen)tothenetworks, theexpectedoutputpredic-
ThedatasethasbeengeneratedwithinthecontextoftheEuro- tion length (in blue) and the values predicted values using
peanproject5G-STARDUST.Theinputsareobtainedfroma a KAN (in red) and MLP (in purple) of depth 4 both – see
satelliteoperator(SO),asaresultofprocessingrealinforma- Table 1 for details on model configuration. In general, our
tionfromaGEOsatellitecommunicationsystem,whichpro- results show that the predictions obtained using KANs bet-
visionsbroadbandservices. Thedatasetisalongtimeseries ter approximates the real traffic values than the predictions
capturingaggregatedtrafficdata. Topreserveprivacy,anony- obtainedusingtraditionalMLPs.
mousclientshavebeendefinedwithmorethan500connected ThisisparticularlyevidentinFigure3a. Here, KANac-
users,andthetraffichasbeennormalized. Themeasurements curately matches rapid changes in traffic volume, which the
aremonthlylong,andthetimegranularityis1hour. MLPmodelssometimesmoderatelyover/under-predicted,as
The traffic has been extracted per satellite beam in the last part of the forecast shows. This capability suggests
Megabits per second (Mbps). Although the data has been thatKANsarebettersuitedtoadapttosuddenshiftsintraffic
collected using a GEO satellite communication system, it is conditions,acriticalaspectofeffectivetrafficmanagement.
expected that user needs could be used to address LEO sys- Additionally, the responsiveness of KANs is particularly
tems,aswell. Itisworthemphasizingthatthedatacollected noticeable in Figure 3b during fast changing traffic condi-
... ... ...
...
...
...
...
... ... ...Table2: Resultssummary
0.6 Real traffic (past)
Real traffic (future)
KAN (4-depth) Model MSE(×10−3) RMSE(×10−2) MAE(×10−2) MAPE Parameters
0.4
MLP (4-depth) MLP(3-depth) 6.34 7.96 5.41 0.64 238k
0.2 MLP(4-depth) 6.12 7.82 5.55 1.05 329k
KAN(3-depth) 5.99 7.73 5.51 0.62 93k
0 25 50 75 100 125 150 175 200 KAN(4-depth) 5.08 7.12 5.06 0.52 109k
Time step [hours]
Zoomed-in view of prediction horizon
0.6 Real traffic (future)
KAN (4-depth)
0.4 MLP (4-depth) demonstratedtherobustnessofKANinmaintaininghighper-
formancedespitethecomplexityandhighervolume. Thisro-
0.2
bustnesssuggeststhatKANscanmanagedifferentscalesand
0 5 10 15 20 intensitiesoftrafficdatamoreeffectivelythanMLPs,making
Time step [hours]
themmorereliablefordeploymentinvariedtrafficscenarios.
(a)Forecastoverbeam1. To further quantify the performance and advantages of
usingKANsforthesatellitetrafficforecastingtaskweshow
Real traffic (past)
Table2. ItshowsadetailedcomparisonofMLPsandKANs
0.2 Real traffic (future)
KAN (4-depth) differentarchitecturesusedforevaluationoverallthebeams.
MLP (4-depth)
0.1
The table displays the Mean Squared Error (MSE), Root
0.0 MeanSquaredError(RMSE),MeanAbsoluteError(MAE),
0 25 50 75 100 125 150 175 200 Mean Absolute Percentage Error (MAPE), and the number
Time step [hours]
Zoomed-in view of prediction horizon of trainable parameters for each model. Analyzing the er-
Real traffic (future) ror metrics, it becomes clear that KANs outperform MLPs,
0.10 KAN (4-depth)
where the KAN (4-depth) is the best in performance. Its MLP (4-depth)
0.05 lower values in MSE and RMSE indicates its better ability
to predict traffic volumes with lower deviation. Similarly,
0.00
0 5 10 15 20 itslowervaluesinMAEandMAPEsuggeststhatKANsnot
Time step [hours]
only provides more accurate predictions but also maintains
(b)Forecastoverbeam2. consistencyacrossdifferenttrafficvolumes, whichiscrucial
forpracticaltrafficforecastingscenarios.
0.8 Real traffic (past)
Furthermore,theparametercountrevealsasignificantdif-
Real traffic (future)
0.6
KAN (4-depth) ferenceinmodelcomplexity. KANmodelsarenotablymore
0.4 MLP (4-depth)
parameter-efficient, with KAN (4-depth) utilizing only 109k
0.2
parameterscomparedto329kparametersforMLP(4-depth)
0.0
0 25 50 75 100 125 150 175 200 or 238k for MLP (3-depth). This reduced complexity sug-
Time step [hours]
Zoomed-in view of prediction horizon gests that KANs can achieve higher or comparable fore-
Real traffic (future) castingaccuracywithsimplerandpotentiallyfastermod-
0.6
KAN (4-depth)
els. Suchefficiencyisespeciallyvaluableinscenarioswhere
MLP (4-depth)
0.4
computationalresourcesarelimitedorwhererapidmodelde-
0.2
ploymentisrequired. Theresultsalsoshowthatwithanaug-
0.0 0 5 10 15 20 mentationof16kparametersinKAN,theperformancecanbe
Time step [hours]
significantlyimproved,contrarytoMLPswhichanincrement
(c)Forecastoverbeam3. of 91k parameters does not showcase a significant improve-
ment.
Fig. 3: Satellite traffic over three different beams with their
From a technical perspective, KANs leverage a theoret-
forecastedvaluesusinga4-depthKANanda4-depthMLP.
ical foundation that provides an intrinsic advantage in mod-
eling complex, non-linear patterns typical in traffic systems.
This capability likely contributes to their flexibility and ac-
tions. KAN shows a rapid adjustment to its forecast that is curacyintrafficforecasting. Theconsistencyinperformance
closelyalignedwiththeactualtrafficpattern. Thisispartic- acrossdiverseconditionsalsosuggeststhatKANshavestrong
ularly noticeable in the last 6 hours of the prediction length generalizationcapabilities,whichisessentialformodelsused
whereMLPexhibitsalagfailingtocapturetheseimmediate ingeographicallyvariedlocationsunderdifferenttrafficcon-
fluctuations, which shows its worse performance to capture ditions. Moreover, besides obtaining lower error rates, our
dynamic traffic variations. Further analysis is shown in Fig- results also suggest that KANs can do so with considerably
ure3c,wheretrafficconditionsaremorevariableandintense, smallernumberofparametersthantraditionalMLPnetworks.
ciffart
dezilamroN
ciffart
dezilamroN
ciffart
dezilamroN
ciffart
dezilamroN
ciffart
dezilamroN
ciffart
dezilamroNputational requirements. For practical applications, particu-
KANs: Loss across nodes and grid sizes larlyinreal-timetrafficmanagementwheretimelyresponses
1.0 n=5-G=5 are critical, it is essential to strike a balance. An effective
n=5-G=10
n=5-G=20 approach could involve starting with moderate settings and
n=10-G=5 graduallyadjustingthenodesandgridsizesbasedonperfor-
0.8 n=10-G=10
n=10-G=20 mance assessments and computational constraints. Besides,
n=20-G=5
wewanttohighlightthatforthisstudycontinuallearningwas
0.6 n=20-G=10 n=20-G=20 notassessed,apossibilitymentionedintheoriginalpaper[8].
0.4
5. CONCLUSION
0.2 In this paper, we have performed an analysis of KANs and
MLPsforsatellitetrafficforecasting. Theresultshighlighted
0 25 50 75 100 125 150 175 200 severalbenefitsofKANs,includingsuperiorforecastingper-
Epochs formance and greater parameter efficiency. In our analysis,
we showed that KANs consistently outperformed MLPs in
Fig.4: AblationcomparisonofKAN-specificparameters.
terms of lower error metrics and were able to achieve better
results with lower computational resources. Additionally,
4.2. KANsparameter-specificanalysis
we explored specific KAN parameters impact on perfor-
We provide an insightful analysis of how different config- mance. This study showcases the importance of optimizing
urations of nodes and grid sizes affect the performance of node counts and grid sizes to enhance model performance.
KANs, particularly in the context of traffic forecasting. For Giventheireffectivenessandefficiency,KANsappeartobea
thisanalysis,wedesigned3KANs(2-depth)[168,n,24]with reasonablealternativetotraditionalMLPsintrafficmanage-
n∈{5,10,20}andvaryinggridsG∈{5,10,20}forak =3 ment.
orderB-spline. Theseresultsareshownduringtrainingtime.
Figure4showsacleartrendwhereincreasingthenumber
6. REFERENCES
of nodes generally results in lower loss values. This indi-
catesthathighernodecountsaremoreeffectiveatcapturing [1] GeorgeEPBoxandal., Timeseriesanalysis: forecast-
the complex patterns in traffic data, thus improving the per- ingandcontrol, JohnWiley&Sons,2015.
formance. Forinstance, configurationswithn = 20demon-
stratesignificantlylowerlossesacrossallgridsizescompared [2] RobJHyndmanandGeorgeAthanasopoulos, Forecast-
tothosewithfewernodes. ing: principlesandpractice, OTexts,2018.
Similarly, thegridsizewithinthesplinesofKANshas
anotableimpactonmodelperformance. Largergridsizes, [3] Charles C Holt, “Forecasting seasonals and trends by
whenusedwithasignificantamountofnodes(n∈{10,20}), exponentially weighted moving averages,” Int. journal
consistentlyresultinbetterperformance. However,whenthe offorecasting,vol.20,no.1,pp.5–10,2004.
amount of nodes is low (n = 5) the extra complexity of the
[4] Peter R Winters, “Forecasting sales by exponentially
grid size shows the opposite effect. When having a signifi-
weightedmovingaverages,” Managementscience,vol.
cant amount of nodes larger grids likely provide a more de-
6,no.3,pp.324–342,1960.
tailed basis for the spline functions, allowing the model to
accommodate better variations in the data, which is crucial
[5] G Peter Zhang et al., “Neural networks for time-series
forcapturingcomplextemporaltrafficpatterns.
forecasting.,” Handbook of natural computing, vol. 1,
The best performance is observed in configurations that
pp.4,2012.
combineahighnodecountwithalargegridsize,suchasthe
n = 20, and G = 20 setup. This combination likely offers
[6] SeppHochreiterandJu¨rgenSchmidhuber, “Longshort-
the highest degree of flexibility and learning capacity, mak-
term memory,” Neural computation, vol. 9, no. 8, pp.
ing it particularly effective for modeling the intricate depen-
1735–1780,1997.
dencies found in traffic data. However, this superior perfor-
mancecomesatthecostofpotentiallyhighercomputational [7] Anastasia Borovykh and al., “Conditional time series
demandsandlongertrainingtimes,asmoretrainableparam- forecastingwithconvolutionalneuralnetworks,” arXiv
etersareincluded. preprintarXiv:1703.04691,2017.
Thesefindingsimplythatwhileincreasingnodesandgrid
sizes can significantly enhance the performance of KANs, [8] Ziming Liu and al., “Kan: Kolmogorov-arnold net-
these benefits must be weighed against the increased com- works,” arXivpreprintarXiv:2404.19756,2024.
ssoL[9] Andre˘ıNikolaevichKolmogorov, Ontherepresentation
ofcontinuousfunctionsofseveralvariablesbysuperpo-
sitions of continuous functions of a smaller number of
variables, AmericanMathematicalSociety,1961.
[10] Ju¨rgenBraunandMichaelGriebel, “Onaconstructive
proof of Kolmogorov’s superposition theorem,” Con-
structiveapproximation,vol.30,pp.653–675,2009.