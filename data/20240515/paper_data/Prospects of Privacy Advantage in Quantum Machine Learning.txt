Prospects of Privacy Advantage in Quantum Machine Learning
Jamie Heredge,1,2 Niraj Kumar,1,∗ Dylan Herman,1 Shouvanik Chakrabarti,1
Romina Yalovetzky,1 Shree Hari Sureshbabu,1 Changhao Li,1 and Marco Pistoia1
1Global Technology Applied Research, JPMorgan Chase, New York, NY 10017
2School of Physics, The University of Melbourne, Parkville, VIC 3010, Australia
(Dated: May 15, 2024)
Ensuring data privacy in machine learning models is critical, particularly in distributed settings
where model gradients are typically shared among multiple parties to allow collaborative learning.
Motivatedbytheincreasingsuccessofrecoveringinputdatafromthegradientsofclassicalmodels,
this study addresses a central question: How hard is it to recover the input data from the gradients
of quantum machine learning models? Focusingonvariationalquantumcircuits(VQC)aslearning
models, weuncoverthecrucialroleplayedbythedynamicalLiealgebra(DLA)oftheVQCansatz
in determining privacy vulnerabilities. While the DLA has previously been linked to the classical
simulatabilityandtrainabilityofVQCmodels,thiswork,forthefirsttime,establishesitsconnection
to the privacy of VQC models. In particular, we show that properties conducive to the trainability
ofVQCs,suchasapolynomial-sizedDLA,alsofacilitatetheextractionofdetailedsnapshots ofthe
input. Wetermthisaweakprivacybreach,asthesnapshotsenabletrainingVQCmodelsfordistinct
learning tasks without direct access to the original input. Further, we investigate the conditions
forastrongprivacybreachwheretheoriginalinputdatacanberecoveredfromthesesnapshotsby
classical or quantum-assisted polynomial time methods. We establish conditions on the encoding
mapsuchasclassicalsimulatability,overlapwithDLAbasis,anditsFourierfrequencycharacteristics
thatenablesuchaprivacybreachofVQCmodels. Ourfindingsthusplayacrucialroleindetailing
the prospects of quantum privacy advantage by guiding the requirements for designing quantum
machine learning models that balance trainability with robust privacy protection.
I. INTRODUCTION
In the contemporary technological landscape, data privacy concerns command increasing attention, particularly
within the domain of machine learning (ML) models that are trained on sensitive datasets. Privacy concerns are
widespreadinmanydifferentapplications,includingfinancialrecords[1,2],healthcareinformation[3–5],andlocation
data[6],eachprovidinguniqueconsiderations. Furthermore,themulti-nationaladoptionofstringentlegalframeworks
[7] has further amplified the urgency to improve data privacy.
The introduction of distributed learning frameworks, such as federated learning [8–10], not only promises increased
computationalefficiencybutalsodemonstratesthepotentialforincreasedprivacyinMLtasks. Infederatedlearning,
each user trains a machine learning model, typically a neural network, locally on their device using their confidential
data,meaningthattheyonlyneedtosendtheirmodelgradientstothecentralserver,whichaggregatesgradientsofall
userstocalculatethemodelparametersforthenexttrainingstep. Astheuserdoesnotsendtheirconfidentialdata,but
rathertheirtraininggradients,thiswasproposedasthefirstsolutiontoenablecollaborativelearningwhilepreventing
data leakage. However, subsequent works have shown that neural networks are particularly susceptible to gradient
inversion-based attacks to recover the original input data [11–15]. To mitigate the above issue, classical techniques
have been proposed to enhance the privacy of distributed learning models, ranging from gradient encryption-based
methods [16], the addition of artificial noise in the gradients to leverage differential-privacy type techniques [10], or
strategies involving the use of batch training to perform gradient mixing [17]. These techniques, although mitagative
in nature, are not fully robust since they either still leak some input information, add substantial computational
overhead while training the model in the distributed setting, or result in reduced performance of the model.
A natural question that follows is whether quantum machine learning can help mitigate the privacy concerns that
theirclassicalcounterpartsexhibit. Specifically, oneisinterestedinexploringthefundamentalquestionunderpinning
the privacy of quantum models: Given the gradients of a quantum machine learning model, how difficult is it to
reconstruct the original classical data inputs? In search of quantum privacy advantages, several quantum distributed
learning proposals have been previously introduced [18–26]. Previous methods for improving privacy in a federated
learningcontexthaverangedfromtheuseofblindquantumcomputing[27],high-frequencyencodingcircuits[28],and
hybrid quantum-classical methods that combine pre-trained classical models with quantum neural networks [29]. In
∗ niraj.x7.kumar@jpmchase.com
4202
yaM
41
]hp-tnauq[
1v10880.5042:viXra2
Privacy Breach Description Complexity Requirements
Weak Snapshot recovery (Sec III) Algorithm 2: O(poly(n)) sized DLA + LASA condition
O(poly(dim(g))) (Def 5)+ Slow Pauli Expansion (Def 9)
Strong Snapshot inversion for local Algorithm 4: Snapshot recovery requirement + Separable
Pauli encoding (Sec IVA2) O(poly(n,1/ϵ) state with ρ (x) parameterized by subset x ⊆x
J J
• dim(x )=O(1)
J
• each x is encoded at most R=O(poly(n))
k
times
• Snapshot components with non-zero overlap
w.r.t. ρ (x ) has cardinality at least dim(x ).
J J J
Snapshot inversion for Grid Search : Privacy breach is not possible
Strong
generic encoding (Sec IVB)
O(cid:16)(cid:0)L(cid:1)d(cid:17)
ϵ
TABLE I: SummaryofresultsontheprivacyguaranteesandcomplexityprovidedbystudiedattackmodelsonvariousVQC
models. We consider two privacy breach scenarios involving VQCs : weak privacy breach and strong privacy breach for
classical or quantum-assisted polynomial time methods. Weak privacy breach concerns the recovery of the meaningful
snapshotsoftheinputencodedstate,allowingtrainingVQCmodelsfordistinctlearningtaskswithoutrequiringaccesstothe
input. Strong privacy breach concerns subsequently inverting the snapshots to recover the original input. We consider the
snapshot invertibility for local Pauli encoding map which admits an efficient (polynomial in the number of qubit n) algorithm
if the requirements stated in the table is met. For the case of generic encoding maps where the VQC is considered as a
black-box L-Lipschitz function, snapshot invertibility requires performing the grid search which scales exponentially in the
input dimension d, and thus it rules out efficient privacy breaches.
particular, the work of [28] considered variational quantum circuits (VQC) as quantum machine learning models and
suggested that highly expressive product encoding maps along with an overparameterized hardware efficient ansatz
(HEA) would necessitate an exponential amount of resources (in terms of the number of qubits n) for an attacker
to learn the input from the gradients. Their work, although the first and sole one to date to theoretically analyze
the privacy of a specific VQC model architecture, has certain key drawbacks. The first is that overparameterization
of a HEA leads to an untrainable model, since it mixes very quickly to a 2-design [30] and thus leads to a barren
plateau phenomenon [31]. The authors enforced the requirement of overparameterization to ensure that there are no
spurious localminima in the optimization landscape and that all local minima are exponentially concentrated toward
global minima [32]. However, this requires the HEA to have an exponential depth and thus an exponential number
of parameters, which precludes efficient training due to an exponential memory requirement to store and update the
parameters. Secondly, the difficulty of inverting gradients to recover data primarily stems from the high expressivity,
characterizedinthiscasebyanexponentiallylargenumberofnon-degeneratefrequenciesofthegeneratorHamiltonian
oftheencodingmap. Introducinghigh-frequencytermsintheencodingmapmaynotbeanexclusivequantumeffect,
as classical machine learning models could also be enhanced by initially loading the data with these high-frequency
feature maps [33].
While previous studies have aimed to highlight the advantages of employing VQC models in safeguarding input
privacy,nonehaveconvincinglyaddressedwhatsetsVQCmodelsapartfromclassicalneuralnetworksintheirpotential
to provide a quantum privacy advantage. A critical aspect missing in a comprehensive examination of the privacy
benefitsofferedbyVQCmodelsinaprivacyframeworktailoredforthem. Suchaframeworkshouldavoiddependence
on specific privacy-enhancing procedures or architectures and instead focus on exploring the fundamental properties
of VQC models that result in input privacy.
To address the above concerns, we introduce a framework designed to assess the possibility of retrieving classical
inputs from the gradients observed in VQC models. We consider VQCs that satisfy the Lie algebra supported ansatz
(LASA) property, which has been key in establishing connections with the trainability and classical simulatability
of VQCs [34–36]. Our study systematically differentiates the separate prerequisites for input reconstruction across
both the variational ansatz and encoding map architectures of these VQC models as summarized in Table I. Our
first result concerns the properties of variational ansatz and the measurement operator of the VQC. Specifically, we
show that when the VQC satisfies the LASA condition, i.e., when the measurement operator is within the dynamical
Lie algebra (DLA) of the ansatz, and when the DLA scales polynomially with the number of qubits, it is possible
to efficiently extract meaningful snapshots of the input, enabling training and evaluation of VQC models for other
learning tasks without having direct access to the original input. We call this the weak privacy breach of the model.
Further, we investigate conditions for strong privacy breach, i.e., recoverability of the original input by classical or
quantum-assisted polynomial time methods. Fully reconstructing the input data from these snapshots to perform a
strong privacy breach presents a further challenge which we show is dependent on properties of the encoding map,
suchasthehardnessofclassicallysimulatingtheencoding,overlapoftheDLAbasiswithencodingcircuitgenerators,3
Weak Privacy Breach Strong Privacy Breach
Can attempt to invert the recovered
snapshots to try find the input data
Snapshots can be input
to other models Snapshot Inversion
Snapshot
Recovery
Snapshot
Recoverable
Strong Privacy
Breach
Snapshot Weak Privacy Snapshot
Recoverable Breach Invertible
FIG. 1: Overview of the general framework and definitions. Weak privacy breach corresponds to attacks where snapshots of
the data are retrieved. These can be used as inputs to other models, without explicitly needing the exact data, allowing one
to potentially learn characteristics of the data. If these snapshots can then be further inverted to retrieve the input data x
explicitly, we say the attack has succeeded in a strong privacy breach.
and its Fourier frequency characteristics. The two types of privacy breach we introduce are summarised in Figure 1
while more specific definitions regarding snapshots, recoverability, and invertibility are provided in Section IIC.
This investigation presents a comprehensive picture of strategies to extract the key properties of VQCs to provide
robustprivacyguaranteeswhileensuringthattheyarestilltrainable. Westructureourpaperinthefollowingmanner.
Appendix A provides the notation used in this work. Sec II provides a general framework for studying privacy with
VQC. This includes describing the VQC framework, providing Lie theoretic definitions required for this work, and
the privacy definitions in terms of input recoverability. Next, Sec III followed by IV covers a detailed analysis of
the snapshot recoverability from the gradients, and snapshot inversion to recover the input, respectively. Sec VA
establishes the connections between privacy and the well-studied trainability of VQCs, and Sec VB consequently
highlightsthefuturedirectionsofenablingrobustprivacywithquantummachinelearningmodels. Wefinallyconclude
our results in Sec VI.
II. GENERAL FRAMEWORK
A. Variational Quantum Circuits for Machine Learning
A variational quantum circuit (VQC) is described in the following manner. We consider the d dimensional input
vector x Rd, which is loaded into the quantum encoding circuit V(x) of n qubits to produce a feature map
∈ X ⊂
with the input state mapping,
ρ(x)=V(x) 0 ⊗n 0⊗n V(x)†. (1)
| ⟩ ⟨ |
This operation loads the input vector of dimension d to a Hilbert space = (C2)⊗n of dimension dim(ρ(x)) = 2n.
H
We will explicitly consider the scenario where n=Θ(d), which is a common setting in most existing VQC algorithms
and hence the number of qubits in a given algorithm will be of the same order as the input vector dimension d. The
state ρ(x) is then passed through a variational circuit ansatz U(θ) defined as
D
(cid:89)
U(θ)= e−iθkHk, (2)
k=1
which is parameterized by a vector of variational parameters θ = [θ , ,θ ], where D is the total number of
1 D
···
variational parameters. Here H , ,H are the set of D Hermitian generators of the circuit U. We note that
1 D
{ ··· }
the above structure is quite general since some common ansatz structures such as the hardware efficient ansatz, the
quantum alternating operator ansatz, and Hamiltonian variational ansatz among others, are all encapsulated in this
framework as highlighted in [37].
Theparameterizedstateρ(x)ispassedthroughavariationalcircuitdenotedbyU(θ),followedbythemeasurement
of some observable O . For a given θ, the output of the variational quantum circuit model is expressed as the
∈ H4
expectation value of O with the parameterized state,
y (x)=Tr(U†(θ)OU(θ)ρ(x)). (3)
θ
For the task of optimizing the variational quantum circuits, the model output is fed into the desired cost function
Cost(θ,x) which is subsequently minimized to obtain,
θ∗ =arg minCost(θ,x), (4)
θ
where θ∗ are the final parameter values after optimization. Typical examples of cost functions include cross-entropy
loss, and mean-squared error loss, among others [38].
The typical optimization procedure involves computing the gradient of the cost function with respect to the pa-
rameters θ, which in turn, involves computing the gradient with respect to the model output y (x)
θ
∂y (x)
C = θ , j [D]. (5)
j
∂θ ∈
j
Going forward, we will directly deal with the recoverability of input x given C , instead of working with specific cost
j
functions. Details of how to reconstruct our results when considering gradients with respect to specific cost functions
are covered in Appendix B.
B. Lie Theoretic Framework
We review some introductory as well as recent results on Lie theoretic framework for variational quantum circuits
which are relevant to our work. For a more detailed review of this topic, we refer the reader to [37, 39]. We provide
the Lie theoretic definitions for a periodic ansatz of the form Eq 2.
Definition 1 (Dynamical Lie Algebra). The dynamical Lie algebra (DLA) g for an ansatz U(θ) of the form Eq 2 is
defined as the real span of the Lie closure of the generators of U
g=span R iH 1, ,iH D Lie, (6)
⟨ ··· ⟩
where the closure is defined under taking all possible nested commutators of S = iH , ,iH . In other words, it
1 D
{ ··· }
is the set of elements obtained by taking the commutation between elements of S until no further linearly independent
elements are obtained.
Definition2(DynamicalLieGroup). ThedynamicalLiegroup foranansatzU(θ)oftheformofEq2isdetermined
G
by the DLA g such that,
=eg, (7)
G
where eg := eiH, iH g and is a subgroup of SU(2n). For generators in g, the set of all U(θ) of the form Eq 2
{ ∈ }
generates a dense subgroup of .
G
Definition 3 (Adjointrepresentation). TheLiealgebraadjointrepresentationisthefollowinglinearaction: K,H
∀ ∈
g,
ad K:=[H,K] g, (8)
H
∈
and the Lie group adjoint representation is the following linear action U , H g,
∀ ∈G ∀ ∈
Ad H:=U†HU g. (9)
U
∈
Definition4(DLAbasis). ThebasisoftheDLAisdenotedas iB ,α 1, ,dim(g) ,whereB areHermitian
α α α
{ } ∈{ ··· }
operators and form an orthonormal basis of g with respect to the Frobenius inner product.
Any observable O is said to be entirely supported by the DLA whenever iO g, or in other words
∈
(cid:88)
O= µ B , (10)
α α
α
where µ is coefficient of support of O in the basis B .
α α5
Definition 5 (Lie Algebra Supported Ansatz [34]). A Lie Algebra Supported Ansatz (LASA) is a periodic ansatz of
the form Eq 2 of a VQC where the measurement operator O is completely supported by the DLA g associated with the
generators of U(θ), that is,
iO g. (11)
∈
In addition to its connections to the trainability of a VQC, this condition also implies that θ,U†(θ)iOU(θ) g,
∀ ∈
whichenablesustoexpresstheevolutionoftheobservableO intermsofelementsofg. Thisiskeytosomesimulation
algorithms that are possible for polynomial-sized DLAs [35, 36].
C. Input Recoverability Definitions
In this section, we provide meaningful definitions of what it means to recover the classical input data given access
to the gradients C D of a VQC. Notably, our definitions are motivated in a manner that allows us to consider the
{ j }j=1
encoding and variational portions of a quantum variational model separately.
A useful concept in machine learning is the creation of data snapshots. These snapshots are compact and efficient
representations of the input data’s feature map encoding. Essentially, a snapshot retains enough information to
substitute for the full feature map encoded data, enabling the training of a machine learning model for a distinct
task with the same data but without the need to explicitly know the input data was passed through the feature map.
For example, in methods such as g-sim [36], these snapshots are used as input vectors for classical simulators. The
simulator can then process these vectors efficiently under certain conditions, recreating the operation of a variational
quantum circuit.
It will become useful to classify the process of input data x recovery into two stages; the first concerns recovering
snapshots of the quantum state ρ(x) (Eq 1) from the gradients, which involves only considering the variational part
of the circuit.
Definition 6 (Snapshot Recovery). Given the gradients C , j [D] as defined in Eq 5 as well as the parameters
j
θ = [θ , ,θ ], we consider a VQC to be snapshot recoverable∈ if there exists an efficient (poly(d,1)) classical
1 ··· D O ϵ
polynomial time algorithm to recover the vector e such that,
snap
[e ] Tr(B ρ(x)) ϵ, α [dim(g)], (12)
snap α α
| − |≤ ∀ ∈
for some B forming an Frobenius-orthonormal basis of the DLA g corresponding to U(θ) in Eq 2, and the above
α
{ }
holds for any ϵ>0. We call e the snapshot of x.
snap
In other words, e is the orthogonal projection of the input state ρ(x) onto the DLA of the ansatz, and thus the
snap
elements of e are the only components of the input state that contribute to the generation of the model output
snap
y (x) as defined in Eq 3. Here, we constitute the retrieval of the snapshot e of a quantum state ρ(x) as weak
θ snap
privacy breach, since the snapshot could be used to train the VQC model for other learning tasks involving the same
data x but without the need to use the actual data. As an example, consider an adversary that has access to
{ }
the snapshots corresponding to the data of certain customers. Their task is to train the VQC to learn the distinct
behavioral patterns of the customers. It becomes apparent that the adversary can easily carry out this task without
ever needing the original data input since the entire contribution of the input x in the VQC output decision-making
y (x) is captured by e .
θ snap
Next, we consider the stronger notion of privacy breach in which the input data x must be fully reconstructed.
Assuming that the snapshot has been recovered, the second step we therefore consider is inverting the recovered
snapshot e to find the original data x, a process that is primarily dependent on the encoding part of the circuit.
snap
Within our snapshot inversion definition, we consider two cases that enable different solution strategies, snapshot
inversion utilizing purely classical methods, and snapshot inversion methods that can utilize quantum samples.
Definition 7 (ClassicallySnapshotInvertibleModel). Given the snapshot e as the expectation values of the input
snap
stateρ(x), wesaythatVQCadmitsclassicalsnapshotinvertibility, ifthereexistsanefficient (poly(d,1))polynomial
O ϵ
time classical randomized algorithm to recover
x′ : x′ x ϵ, (13)
2
∥ − ∥ ≤
with probability at least p= 2, for any user defined ϵ>0.
36
FIG. 2: a) Visualization of the difference between the circuit implementation of a variational quantum model and a Lie
algebraicsimulationprocedureofthesamemodel[36]. Intheuppercircuit,aVQCworksbyencodingtheinputdataxintoa
quantum circuit using the encoding step V(x), which is then passed through a variational circuit U(θ). After this some
measurement O is taken of the quantum state in order to calculate the model output y . In the lower circuit, the Lie
θ
Algebraic Simulation framework [36] is shown; similarly, input data x is encoded into a quantum circuit using the encoding
step V(x), however, the measurements are then performed on this encoded state and used to form a vector of snapshot
expectation values. This vector of snapshot expectation values can then be passed as inputs to a classical simulator that uses
the adjoint form of U(θ), which can be performed with resources scaling with the dimension of the DLA formed by the
generators of U(θ). b) In this work, we assess the ability to recover an input x from gradients C . This can be broken into
j
two parts: Firstly, the snapshot e must be recovered from the gradients C , which corresponds to reversing the Lie
snap j
Algebraic simulation step. Secondly, the recovered snapshot e must be inverted to find the original data x, which requires
snap
finding the values of x that when input into V(x) will give the same snapshot values e . If both snapshot recovery and
snap
snapshot inversion can be performed, then it admits efficient input recovery.
Definition 8 (QuantumAssistedSnapshotinversion). Giventhesnapshote astheexpectationvaluesoftheinput
snap
state ρ(x), and the ability to query poly(d,1) number of samples from the encoding circuit V to generate snapshots
ϵ
e′ for any given input x′, we say that VQC admits quantum-assisted snapshot invertibility, if there exists an
snap
efficient (poly(d,1)) polynomial time classical randomised algorithm to recover
O ϵ
x′ : x′ x ϵ, (14)
2
∥ − ∥ ≤
with probability at least p= 2, for any user defined ϵ>0.
3
In this work, we specifically focus on input recoverability by considering the conditions under which VQC would
admit snapshot recovery followed by snapshot invertibility. Considering these two steps individually allows us to
delineate the exact mechanisms that contribute to the overall recovery of the input.
It is important to mention that it may potentially only be possible to recover the inputs of a VQC up to some
periodicity, such that there only exists a classical polynomial time algorithm to recover x˜ =x+kπ up to ϵ-closeness,
where k Z. As the encodings generated by quantum feature maps inherently contain trigonometric terms, in the
∈
most general case it may therefore only be possible to recover x up to some periodicity. However, this can be relaxed
if the quantum feature map is assumed to be injective.
Figure 2. shows a diagram that highlights the Lie algebraic simulation method [36] along with specifications of the
input recovery framework as defined in this work.
III. SNAPSHOT RECOVERY
This section addresses the weak privacy notion of recovering the snapshots of the input as introduced in Def 6. As
the name implies, the goal here is to recover the vector e for some Schmidt orthonormal basis B of
snap α α∈dim(g)
{ }
the DLA corresponding to the VQC ansatz U(θ), given that the attacker is provided the following information,
1. D gradient information updates C = ∂yθ(x),j [D] as defined in Eq 5.
j ∂θj ∈
2. Ansatz architecture U(θ) presented as an ordered sequence of Hermitian generators θ ,H D , where H is
{ k k }k=1 k
expressed as a polynomial (in the number of qubits) linear combination of Pauli strings.
3. Measurement operator O which satisfies the LASA condition according to Def 5 and expressed as a polynomial
(in the number of qubits) linear combination of Pauli strings7
Recovering these snapshots will enable an attacker to train the VQC model for other learning tasks that effectively
extract the same information from the input states ρ(x) but without the need to use the actual data. The main
component of the snapshot recoverability algorithm makes use of the g-sim [35, 36] framework, which we briefly
review in the following subsection while also clarifying some previously implicit assumptions, to construct a system
of linear equations that can be solved to recover e as detailed in Algorithm 2.
snap
A. Review of Lie-Algebraic Simulation Framework
We start by reviewing the g-sim framework [35, 36] for classically computing the cost function and gradients of
VQCs, whentheobservableliesintheDLAofthechosenansatz. Specifically, thisframeworkevolvestheexpectation
values of observables via the adjoint representation. However, a necessary condition for this procedure to be efficient
is that the dimension of the DLA (dim(g)) is only polynomially growing in the number of qubits.
The first step of g-sim consists of building an orthonormal basis for the DLA g given ( θ ,H )D . Algorithm
{ k k } k=1
1 presents a well-known procedure to do this. The procedure simply computes pairwise commutators until no new
linearly independent elements are found. Given that all operators are expressed in the Pauli basis, the required
orthogonalprojectorsandnormcomputationsperformedbyAlgorithm1canbeperformedefficiently. Ifthedimension
ofDLAis (poly(n)), thentheiterationcomplexity, i.e., thenumberofsetsofcommutatorsthatwecompute, ofthis
O
procedure is polynomial in n. However, an important caveat is that potentially the elements forming our estimation
for the DLA basis could have exponential support on the Pauli basis, which is a result of computing new pairwise
commutators at each iteration. Thus, for this overall procedure to be efficient, we effectively require that the nested
commutators of the generators H do not have exponential support on the Pauli basis.
k
Definition 9 (Slow Pauli Expansion). A set of Hermitian generators H ,...,H on n-qubits expressed as lin-
1 D
{ }
ear combinations of (poly(dim(g))) Pauli strings satisfies the slow Pauli expansion condition if r [dimg],
O ∀ ∈
[H ,[ ,[H ,H ]]] can be expressed as a linear combination of (poly(dim(g))) Pauli strings.
r 2 1
··· O
In general, it is unclear how strong of an assumption this is, which means that the attacks that we present may
not be practical for all VQCs that satisfy the polynomial DLA condition, and thus privacy preservation may still
be possible. Also, it does not seem to be possible to apply the g-sim framework without the slow Pauli expansion
condition. Lastly, a trivial example of a set of Hermitian generators that satisfies the slow Pauli expansion are those
for the quantum compound ansatz discussed in [34].
Algorithm1 : Finding DLA basis
Require: Hermitiancircuitgenerators{H ,...,H },allelementsarelinearcombinationsofpolynomially-manyPaulistrings
1 D
Ensure: A′′′ ={B ,...,B } as the basis for the DLA g
1 dim(g)
1. Let A={H ,...,H }, with all elements represented in the Pauli basis.
1 D
2. Repeat until breaks
(a) Compute pairwise commutators of elements of A into A′
(b) Orthogonally project A′ onto orthogonal complement of A in g
(c) Set new A′′ to be A plus new orthogonal elements. If no new elements, break.
3. Perform Gram–Schmidt on A forming A′′′.
4. Return A′′′.
(cid:80)
Given the orthonormal basis B for g, under the LASA condition, we can express O = µ B , and
α α∈[dim(g)] α α
hence we can write the output as
(cid:88) (cid:88)
y (x)=Tr(U†(θ)OU(θ)ρ(x))= Tr(µ U†B Uρ(x))= Tr(µ Ad (B )ρ(x)). (15)
θ α α α U α
α α
In addition, given the form of U, we can express Ad as,
U
D
(cid:89)
Ad
U
= e−θkadiHk. (16)
k=1
We can also compute the structure constants for our basis B , which is the collection of dim(g) dim(g) matrices for
α
×
the operators ad . As a result of linearity, we also have the matrix for each ad for H g in the basis B . Then
iBα iH
∈
α
by performing matrix exponentiation and multiplying dim(g) dim(g) we can compute the matrix for Ad .
U
×8
Using the above, the model output may be written,
(cid:88)
y = µ [Ad ] Tr(B ρ(x))=µTAd e , (17)
θ α U αβ β U snap
α,β
where e is a vector of expectation values of the initial state, i.e. [e ] =Tr[B ρ(x)].
snap snap β β
Similar to the cost function, the circuit gradient can also be computed via g-sim. Let,
∂y ∂Ad
C = θ =µT Ue =:χ(j) e , (18)
j snap snap
∂θ ∂θ ·
j j
where the adjoint term differentiated with respect to θ can be written as,
j
 D  (cid:34) j (cid:35)
∂ ∂A θd U =(cid:89) eθkadiHkad
iHj
(cid:89) eθkadiHk . (19)
j
k=j k=1
The components of χ(j) can be expressed as,
(cid:20) (cid:21)
χ(j) =(cid:88) µ ∂Ad U , (20)
β α ∂θ
α j α,β
allowing C terms to be represented in a simplified manner as
j
dim(g)
C = (cid:88) χ(j)[e ] . (21)
j β snap β
β=1
The key feature of this setup is that the matrices and vectors involved have dimension dim(g), therefore for a
polynomial-sizedDLA,thesimulationtimewillscalepolynomiallyandmodeloutputscanbecalculatedinpolynomial
time [36]. Specifically, the matrices for each ad in the basis B and Ad are polynomial in this case.
iHk
{
l
}
U
This Lie-algebraic simulation technique was introduced in order to show efficient methods of simulating LASA
circuits with polynomially sized DLA. In this work, we utilize the framework in order to investigate the snapshot
recovery of variational quantum algorithms. Based on the above discussion, the proof of the following theorem is
self-evident.
Theorem 1 (Complexityofg-sim). IfansatzfamilyU(θ)withanobservableOsatisfiesboththeLASAconditionand
Slow Pauli Expansion, then the cost function and its gradients can be simulated with complexity (poly(dim(g))) using
O
a procedure that at most queries a quantum device a polynomial number of times to compute the dim(g)-dimensional
snapshot vector e .
snap
B. Snapshot Recovery Algorithm
With the framework for the g-sim [36] established, we focus on how snapshots e of the input data can be
snap
recovered using the VQC model gradients C , with the process detailed in Algorithm 2. In particular, the form of
j
Eq 21 allows a set-up leading to the recovery the snapshot vector e from the gradients C D , but requires the
snap { j }j=1
ability to solve the system of D linear equations given by C with dim(g) unknowns [e ] . The following
j snap β∈dim(g)
{ }
theorem formalizes the complexity of recovering the snapshots from the gradients.
Theorem 2 (Snapshot Recovery). Given the requirements specified in Algorithm 2, along with the assumption that
the number of variational parameters D dim(g), where dim(g) is the dimension of the DLA g, the VQC model
≥
admits snapshot e recovery with complexity scaling as (poly(dim(g))).
snap
O
Proof. Firstly, we note that given the gradients C and parameters θ , the only unknowns are the components of
j j∈[D]
thevectore oflengthdim(g). Therefore,itisnecessarytohavedim(g)equationsintotal;otherwise,thesystemof
snap
equations would be underdetermined and it would be impossible to find a unique solution. The number of equations
depends on the number of gradients and, therefore, the number of variational parameters in the model, hence the
requirement that D dim(g).
≥9
Algorithm2 : Snapshot Recovery
Require: ObservableOsuchthatiO∈g,generators{H }D ,orderedsequence({θ ,H })D ,andgradientsC = ∂yθ(x),j ∈
k k=1 k k k=1 j ∂θj
[D] for some unknown classical input x.
Ensure: Snapshot e for x
snap
1. Run Algorithm 1 to obtain an orthonormal basis for the DLA {B }
β β∈[dim(g)]
2. For β ∈[dim(g)], compute the dim(g)×dim(g) matrix ad
iBβ
3. For k∈[D], compute the coefficients of H in the basis {B } , which gives us ad
k β β∈[dim(g)] iHk
4. For k∈[D], compute the dim(g)×dim(g) matrix exponential eθkadiHk
5. For j ∈[D] compute the dim(g)×dim(g) matrix
 D  (cid:34) j (cid:35)
∂ ∂A θd U =(cid:89) eθkadiHkad
iHj
(cid:89) eθkadiHk . (22)
j
k=j k=1
6. For β ∈[dim(g)], compute the coefficients µ of O in the basis {B }
β β β∈[dim(g)]
7. For j ∈[D],β ∈[dim(g)], compute
(cid:20) (cid:21)
χ(j) =(cid:88) µ ∂Ad U , (23)
β α ∂θ
α j α,β
and construct D×dim(g) matrix A with A =χ(r).
rs s
8. Solve the following linear system,
[C ,...,C ]T =Ay, (24)
1 D
and return y as the snapshot e .
snap
Assuming now that we deal with the case that there are D dim(g) variational parameters of the VQC model,
≥
we can therefore arrive at a determined system of equations. The resulting system of simultaneous equations can be
written in a matrix form as,
 
  χ(1) χ(1) χ(1)  
C 1 1 2 ··· dim(g) [e snap] 1
   C . . .2  

=   

χ( 1 . . .2) χ( 2 . . .2) · ..· .· χ( d2 im) . . .(g)   

  

[e sn . . .ap] 2   

. (25)
 
C D D×1 χ(D) χ(D) χ(D) [e snap] dim(g) dim(g)×1
dim(g) dim(g) ··· dim(g) D×dim(g)
In order to solve the system of equations highlighted in Eq 25 to obtain e , we first need to compute the
snap
coefficients χ(j) . This can done by the g-sim procedure highlighted in the previous section and in
{ β }j∈[D],β∈[dim(g)]
steps 1-7 in Algorithm 2 with complexity (poly(dim(g))). The next step is to solve the system of equations, i.e.,
step8ofAlgorithm2,whichcansolvedusinO gGaussianeliminationprocedureincurringacomplexity (dim(g)3)[40].
O
Thus, the overall complexity of recovering the snapshots from the gradients is (poly(dim(g))). This completes the
O
proof.
InthecasethatthedimensionofDLAisexponentiallylargedim(g)= (exp(n)),thenperformingsnapshotrecovery
O
bysolvingthesystemofequationswouldrequireanexponentialnumberofgradientsandthusanexponentialnumber
of total trainable parameters D = (exp(n)). However, this would require storing an exponential amount of classical
O
data, as even the variational parameter array θ would contain (exp(n)) many elements and hence this model would
O
already breach the privacy definition which only allows for a polynomial (in n = Θ(d)) time attacker. In addition,
the complexity of obtaining the coefficients χ(j) and subsequently solving the system of linear equations would also
β
incur a cost exponential in n. Hence, for the system of simultaneous equations to be determined, it is required that
dim(g)= (poly(n)). Under the above requirement, it will also be possible to solve the system of equations in Eq 25
O
in polynomial time and retrieve the snapshot vector e . Hence, a model is snapshot recoverable if the dimension
snap
of the DLA dimension scales polynomially in d.10
IV. SNAPSHOT INVERTIBILITY
We have shown that in the case that the DLA dimension of the VQC is polynomial in the number of qubits n and
theslowPauliexpansioncondition(Def9)issatisfied,thenitispossibletoreverseengineerthesnapshotvectore
snap
from the gradients. As a result, this breaks the weak-privacy criterion. The next step in terms of privacy analysis is
to see if a strong privacy breach can also occur. This is true when it is possible to recover the original data x that
wasusedintheencodingsteptogeneratethestateρ(x); theexpectationvaluesofthisstatewithrespecttotheDLA
basis elements forms the snapshot e . Hence, even if the DLA is polynomial and snapshot recovery allows the
snap
discovery of e , there is still the possibility of achieving some input privacy if e cannot be efficiently inverted
snap snap
to find x. The overall privacy of the VQC model, therefore, depends on both the data encoding and the variational
ansatz.
One common condition that is necessary for our approaches to snapshot inversion is the ability to compute the
expectation values Tr(ρ(x′)B ), k [dim(g)] forsome guess input x′. This isthe main conditionthat distinguishes
k
∀ ∈
between completely classical snapshot inversion and quantum-assisted snapshot inversion. It is well-known that
computingexpectationvaluesofspecificobservablesisaweakerconditionthanρ(x)beingclassicallysimulatable[41].
Hence, it may be possible to classically perform snapshot inversion even if the state ρ(x) overall is hard to classically
simulate. In the quantum-assisted case it is always possible to calculate Tr(ρ(x′B ) values by taking appropriate
k
measurements of the encoding circuit V(x′) .
Inthefirstsubsection, wepresentinversionattacksthatapplytocommonly-usedfeaturemapsandexplicitlymake
use of knowledge about the locality of encoding circuit. The common theme among these feature maps is that by
restricting to only a subset of the inputs, it is possible to express the ρ(x) or expectations there of in a simpler way.
The second subsection focuses on arbitrary encoding schemes by viewing the problem as black-box optimization. In
general, snapshot inversion can be challenging or intractable even if the snapshots can be efficiently recovered and/or
the feature map can be classically simulated. Our focus will be on presenting sufficient conditions for performing
snapshot inversion, which leads to suggestions for increasing privacy.
A. Snapshot Inversion for Local Encodings
For efficiency reasons, it is common to encode components of the input vector x in local quantum gates, typically
justsingle-qubitrotations. Themajorityofthecircuitcomplexityisusuallyeitherputintothevariationalpartorvia
non-parameterizedentanglinggatesinthefeaturemap. Inthissection,wedemonstrateattackstorecovercomponents
of x, up to periodicity, given snapshot vectors when the feature map encodes each x locally. More specifically, we
j
putboundsontheallowedamountofinteractionbetweenqubitsthatareusedtoencodeeachx . Inaddition,wealso
j
require that the number of times the feature map can encode a single x be sufficiently small. While the conditions
j
will appear strict, we note that they are satisfied for some commonly used encodings, e.g., the Pauli product feature
map or Fourier tower map [28] which was previously used in a VQC model that demonstrated resilience to input
recovery.
For the Pauli product encoding, we show that a completely classical snapshot inversion attack is possible. An
example of a Pauli product encoding is the following:
n n
(cid:79) (cid:79)
ρ (x )= R (x )0 0R ( x ). (26)
j j X j X j
| ⟩⟨ | −
j1 j1
where R is the parameterized Pauli X rotation gate. The Fourier tower map is similar to Equation (26) but utilizes
X
a parallel data reuploading scheme, i.e.
d m
(cid:79)(cid:16)(cid:79) (cid:17)
R (5l−1x ) . (27)
X j
j=1 l=1
where n=dm, with m being the number of qubits used to encode a single dimension of the input.11
1. Pauli Product Encoding
ThefirstattackthatwepresentwillspecificallytargetEquation(26). However,theattackdoesapplytotheFourier
tower map as well. More generally, the procedure applies to any parallel data reuploading schemes of the form:
d m
(cid:79)(cid:16)(cid:79) (cid:17)
R (α x ) . (28)
X l j
j=1 l=1
We explicitly utilize Pauli X rotations, but a similar result holds for Y or Z. For Pauli operator P, let P :=
j
iI⊗(j−1) P I⊗(n−j).
⊗ ⊗
Algorithm3 : Classical Snapshot Inversion for Pauli Product Encoding
Require: Snapshot vector e (x) of dimension dim(g) = O(poly(n)) corresponding to a basis (B )dim(g) of DLA g. Each
snap k k=1
B isexpressedasalinearcombinationofO(poly(n))Paulistrings. SnapshotinversionisbeingperformedforaVQCmodel
k
that utilized a trainable portion of U(θ) with DLA g and Pauli product encoding Equation (26). Index j ∈[d], ϵ<1
Ensure: An ϵ estimate of the j-th component x of the data input x∈Rd up to periodicity, or output FAILURE.
j
if iZ ∈g then
j
α←1,β ←0
W←Y
j
else if iY ∈g then
j
α←1,β ←0
W←Y
j
else
1. Determine set of Pauli strings required to span elements (iB )dim(g) and denote the set P .
k k=1 g
2. P ←P ∪{Z ,Y }, |P |=O(poly(n)) by assumption. Reduce P to a basis.
g g j j g g
3. Let C be a |P |×dim(g) matrix whose k-th column corresponds to the components of iB in the basis P .
g k g
4. LetAbea|P |×2whosefirstcolumncontainsa1intherowcorrespondingtoZ andwhosesecondcolumncontains
g j
a 1 in the row corresponding to Y .
j
5. Perform a singular value decomposition on ATC, and there are at most two nonzero singular values r ,r .
1 2
if r ̸=1 and r ̸=1 then
1 2
return FAILURE
else
1. W← singular vector with singular value 1.
2. Expand iW in basis (iZ ,iY ) record components as α and β respectively.
j j
end if
end if
1. Expand iW in basis (B )dim(g), and record components as γ .
k k=1 k
2. Compute
 
dim(g)
x˜ j =cos−1  (cid:112)2 (cid:88) γ k[e snap] k−tan−1(β/α). (29)
sign(α) α2+β2
k=1
3. return x˜ .
j
Theorem 3. Suppose that the polynomial DLA and slow Pauli expansion (Def 9) conditions are satisfied. Also, sup-
posethatwearegivenasnapshotvectore (x)foraVQCwithtrainableportionU(θ)withDLAgandPauliproduct
snap
feature encoding (Equation (26)) and the corresponding DLA basis elements (B )dim(g). The classical Algorithm 3
k k=1
outputs an ϵ estimate of x , up to periodicity, or outputs FAILURE, with time (poly(n)log(1/ϵ)).
j
O
Proof. Steps 1-5 can be performed in (poly(n)) classical time due to the polynomial DLA and slow Pauli expansion
O
conditions. The purpose of step 5 is to compute the angles between the linear subspaces g and span R iZ j,iY j . This
{ }
is to identify if there is any intersection, i.e. if α,β such that αiZ +βiY g, which is identified by singular values
j j
∃ ∈
equalto1. Thealgorithmcannotproceediftheintersectionistriviallyemptyasthesnapshotvectordoesnotprovide12
FIG. 3: Aproductmapencoding,wherebyeachinputvariablex isencodedintoanindividualqubit,andthesnapshotused
j
by the model corresponds to single qubit measurements of the DLA basis elements. In this setting, the snapshot is trivial to
(cid:16) (cid:17)
invert and find the original data using the relation x =cos−1 2γ(j)·e .
j snap
the required measurement to obtain x efficiently with this scheme. From now on, we suppose that such an element
j
was found.
We can without loss of generality just focus on the one-qubit reduced density matrix for x . In this case, using
j
Bloch sphere representation:
I sin(x )Y+cos(x )Z
ρ (x )=R (x )0 0R ( x )= − j j , (30)
j j X j | ⟩⟨ | X − j 2
so that
α β
Tr([αZ +βY ]ρ (x ))= cos(x ) sin(x ) (31)
j j j j 2 j − 2 j
=
sign(α)(cid:112) α2+β2cos(cid:0)
x
+tan−1(β/α)(cid:1)
. (32)
2 j
However, by assumption, γ R such that
k
∈
dim(g) dim(g)
(cid:88) (cid:88)
αiZ +βiY = γ B = Tr([αZ +βY ]ρ (x ))= γ [e ] . (33)
j j i k j j j j k snap k
⇒
k=1 k=1
So to recover x , we only need to solve:
j
dim(g)
(cid:88)
γ [e ] =
sign(α)(cid:112) α2+β2cos(cid:0)
x
+tan−1(β/α)(cid:1)
, (34)
k snap k 2 j
k=1
which after rearranging allows the recovery of
 
dim(g)
2 (cid:88)
x j =cos−1  (cid:112) γ k[e snap] k tan−1(β/α), (35)
sign(α) α2+β2 −
k=1
up to periodicity. By the polynomial DLA and slow Pauli expansion conditions (i.e. all DLA basis elements are
expressed as linear combinations of Paulis), we can compute γ in (poly(n)log(1/ϵ)) time.
k
O
For illustrative purposes, we show in Figure 3 the snapshot inversion process for the special case where iZ g, i.e.
j
∈
(cid:16) (cid:17)
x =cos−1 2γ(j) e , (36)
j snap
·
for iZ = (cid:80)dim(g) γ(j)B . The general parallel data reuploading case can be handled by applying the procedure to
j k=1 k k
only one of the rotations that encodes at x at a time, checking to find one that does not cause the algorithm to
j
return FAILURE.13
2. General Pauli Encoding
We now present a more general procedure that applies to feature maps that use serial data reuploading and multi-
qubit Paulis. However, we introduce a condition that ensures that each x is locally encoded. More generally, we
j
focus our discussion on encoding states that may be written as a tensor product of Ω subsystems, i.e. multipartite
states.
(cid:79)
ρ(x)= ρ (x), (37)
J
J∈P
where dim(x ) is constant. The procedure is highlighted in Algorithm 4 and requires solving a system of polynomial
J
equations.
In addition, the procedure may not be completely classical as quantum assistance may be required to compute
certainexpectationvaluesofρ (x), specificallywithrespecttotheDLAbasiselements. Forsimplicity, thealgorithm
J
and the theorem characterizing the runtime ignore potential errors in estimating these expectations. If classical
estimationispossible,thenwecanpotentiallyachievea (poly(log(1/ϵ)))scaling. However,ifwemustusequantum,
O
then we will incur a (1/ϵ) (due to amplitude estimation) dependence, which can be significant. Theorem 4 presents
O
the attack complexity ignoring these errors.
Algorithm4 : Snapshot Inversion for General Pauli Encodings
Require: Snapshot vector e (x) of dimension dim(g) = O(poly(n)) corresponding to a basis (B )dim(g)) of DLA g. Each
snap k k=1
B isexpressedasalinearcombinationofO(poly(n))Paulistrings. SnapshotinversionisbeingperformedforaVQCmodel
k
that utilized a trainable portion of U(θ) with DLA g and separable encoding Equation (37) with qubit partition P. Index
j ∈[d], ϵ<1
Ensure: An ϵ estimate of the j-th component x of the data input x∈Rd up to periodicity
j
1. Find a ρ for J ∈ P that depends on x . Let R denote the number of Pauli rotations in the circuit for preparing ρ
J j J
that involve x .
j
2. For each k∈[dim(g)], compute Tr(B kρ J(x)) and Tr(B kρ Jc(x)).
3. Determine the set S
J
={k:Tr(B kρ J(x))̸=0 & Tr(B kρ Jc(x))=0}, Jc :=[n]−J.
if S <dim(x ) then
J J
return FAILURE
else
1. For each k∈S
J
evaluate Tr(B kρ J(x)) at M =2Rdim(xJ)+1 points, x
r
∈{ 22 Rπ +r
1
:r=−R,...R}dim(xJ)
2. For each k, solve linear system
Tr(B ρ (x ))=α + (cid:88) α eir·xr
k J r 0 r
r∈[R]dim(xJ)
for α’s.
3. Consider the polynomial system:
 
(cid:88)
dim (cid:89)(xJ)
[e snap]
k
=Reα 0+ α
r
(T rj(u j)+iv jU rj−1(u j)),k∈S
J
(38)
r∈[R]dim(xJ) j=1
u2+v2 =1,j ∈J, (39)
j j
where u =cos(x ),v =sin(x ) and T ,U relate to Chebyshev polynomials.
j j k j r r
4. Apply Buchberger’s to obtain a Gro¨bner basis for the system.
5. Back substitution and univariable root-finding algorithm [42] (e.g., Jenkins-Traub [43]) to obtain x˜ .
J
6. return x˜
j
end if
Theorem 4. Suppose that the feature encoding state ρ(x) is a multipartite state, specifically there exists a partition
of qubits [n] such that
P
(cid:79)
ρ(x)= ρ (x),
J
J∈P
where we define x x to be components of x on which ρ depends. In addition, we have as input an (poly(n))-
J J
⊆ O
dimensional snapshot vector e with respect to a known basis B for the DLA of the VQC.
snap k14
Suppose that for ρ (x) the following conditions are satisfied:
J
• dim(x )= (1),
J
O
• each x is encoded at most R= (poly(n)) times in, potentially multi-qubit, Pauli rotations.
k
O
• and the set
J
= k : Tr(B kρ J(x)) = 0 & Tr(B kρ Jc(x)) = 0 has cardinality at least dim(x J), where Jc :=
S { ̸ }
[n] J.
−
then the model admits quantum-assisted snapshot inversion for recovering x . Furthermore, a classical snapshot
J
inversion can be performed if k,Tr(B ρ (x)) can be evaluated classically for all x. Overall, ignoring error in
k J
∀
estimating Tr(B ρ (x)), with the chosen parameters, this leads to a (poly(n,log(1/ϵ))) algorithm.
k J
O
Proof. Giventhateachx isencodedwithmultiqubitPaulirotations, i.e. possibleeigenvaluesare1and 1, itiswell
k
−
known [44] that the following holds:
(cid:88)
f (x )=Tr(B ρ (x))=α + α eir·xr, k , (40)
k J k J 0 r
∀ ∈S
r∈[R]dim(xJ)
andTr(B ρ (x))isreal. Theset istoensurethatwecanisolateasubsystemwheredim(x )isconstant. Toensure
k J J J
S
thatthenumberoftermsis (poly(n))itsufficestorestricttodim(x )= (log(n)),R= (log(n)). Theαcoefficients
J
canbecomputedbyevaluatO ingTr(B kρ J(x))at2Rdim(xJ)+1= (poly(nO ))differentpoinO tsx′. Dependingonwhether
O
Tr(B ρ (x)) can be evaluated classically or quantumly implies whether this falls under classical or quantum-assisted
k J
snapshot inversion. This leads to a system of dim(x ) equations in x :
J J
[e ] =f (x ),k =1,...,dim(g). (41)
snap k k J
Using the Chebyshev polynomials T , U of the first and second kind, respectively, we can expressed the system as a
n n
system of polynomial equations with additional constraints:
 
(cid:88)
dim (cid:89)(xJ)
[e snap]
k
=Reα 0+ α
r
(T rj(u j)+iv jU rj−1(u j)),k
∈SJ
(42)
r∈[R]dim(xJ) j=1
u2+v2 =1,j J, (43)
j j ∈
where u = cos(x ),v = sin(x ). In addition, we use the Chebyshev polynomials defined as cos(nθ) = T (cos(θ))
j j k j n
and sin(θ)U (cos(θ))=sin(nθ). By our assumption that the DLA is polynomial, we have (poly(n)) equations in
n−1
O
2dim(x )= (loglog(n)) unknowns.
J
O
Ifallconditionsuntilnowaresatisfied,wewillhavesuccessfullywrittendownasystemofdeterminedsimultaneous
equations. Consideringboundsfromcomputationalgeometrywenotethatintheworst-caseofBuchberger’salgorithm
[45] the degrees of a reduced Gr¨obner basis are bounded by
(cid:16)∆2 (cid:17)2Q−2
M =2 +∆ , (44)
2
where ∆ is the maximum degree of any polynomial and Q is the number of unknown variables [46]. For a system of
linear equations, it was shown that a worst case degree bound grows double exponentially in the number of variables
[47]. The maximum degree of any equation in Eq 42 is ∆=Rdim(xJ), and Q=2dim(x J) so that
M =
(R2dim(xJ)2dim(xJ)
), (45)
O
so for our chosen conditions the maximum degree is bounded by M = (poly(n)).
O
Buchberger’s algorithm provides a Gr¨obner basis in which backsubstituion could be used to solve equations in one
variable. Numerical methods for solving polynomials in one variable generally scale polynomially in the degree. For
solving each univariate polynomial at each step of the back substitution, we can apply a polynomial root-finding
method, such that Jenkins–Traub [43], which can achieve at least quadratic global convergence (converge from any
initialpointandataratethatisatleastloglog(1/ϵ)). Thisleadstoanoverall (poly(n,log(1/ϵ))algorithm,ignoring
O
error in estimating Tr(B ρ ).
j J
In the case a circuit has an encoding structure that leads to a separable state, we have indicated conditions that
guarantee snapshot inversion can be performed. If the model is also snapshot recoverable, by having a polynomially
sized DLA, then this means the initial data input can be fully recovered from the gradients, and hence the attack
constitutes a strong privacy breach.15
B. Snapshot Inversion for Generic Encodings
In the general case but still dim(g)= (poly(n)), where it is unclear how to make efficient use of our knowledge of
O
thecircuit,weattempttofindanxviablack-boxoptimizationmethodsthatproducesthedesiredsnapshotsignature.
More specifically, suppose for simplicity we restrict our search to [ 1,1]d. We start with an initial guess for the input
parameters, denotedasx′, andusethesetocalculateexpectedsna− pshotvaluesTr[B ρ(x′)]. Acostfunctioncanthen
k
be calculated that compares this to the true snapshot, denoted e . As an example, one can use the mean squared
snap
error as the cost function,
f(x′)= e (Tr[B ρ(x′)])dim(g) 2 = (cid:88) ([e ] Tr[B ρ(x′)])2 . (46)
∥ snap − k k=1 ∥2 snap k − k
k∈[dim(g)]
The goal will be to solve the optimization problem min f(x′). For general encoding maps, it appears
x′∈[−1,1]d
that we need to treat this as a black-box optimization problem, where we evaluate the complexity in terms of the
evaluations of f or, potentially, its gradient. However, in our setting, it is unclear what is the significance of finding
approximatelocalminimum, andthusitseemsforprivacybreakage, wemustresorttoanexhaustivegridsearch. For
completeness, we still state results on first-order methods that can produce approximate local minima.
Westartbyreviewingsomeofthewell-knownresultsforblack-boxoptimization. WerecallLipschitzcontinuityby,
Definition 10 (L-Lipschitz Continuous Function). A function f : Rd R is said be be L-Lipschitz continuous if
→
there exists a real positive constant L>0 for which,
f(x) f(y) L x y .
2
| − |≤ ∥ − ∥
If we consider the quantum circuit as a black-box L-Lipschitz function and x′ in some convex, compact set with
diameter P (e.g. [ 1,1]d with diameter 2√d). One can roughly upper bound L by the highest frequency component
−
of multidimensional trig series for f, which can be an exponential in n quantity. In this case the amount of function
evaluations that would be required to find x′ such that x x′ ϵ would scale as
2
∥ − ∥ ≤
(cid:32) (cid:18) (cid:19)d(cid:33)
L
P , (47)
O ϵ
which is the complexity of grid search [48]. Thus if for constant L this is a computationally daunting task, i.e.
exponential in d=Θ(n).
As mentioned earlier, we it is possible to resort to first-order methods to obtain, an effectively dimension-
independent, algorithm for finding an approximate local minima. We recall the definition of β-smoothness as,
Definition 11 (β-Smooth Function). A differentiable function f : Rd R is said be be β-smooth if there exists a
→
real positive constant β >0 for which
f(x) f(y) β x y .
2 2
∥∇ −∇ ∥ ≤ ∥ − ∥
If we have access to gradients of the cost function with respect to each parameter, then using perturbed gradient
descent [49] would roughly require
(cid:16)PLβ(cid:17)
˜ , (48)
O ϵ2
function and gradient evaluations for an L-Lipschitz function that is β-smooth to find an approximate local min.
With regards to first-order optimization, computing the gradient of f can be expressed in terms of computing certain
expectations values of ρ, either via finite-difference approximation or the parameter-shift rule for certain gate sets
[44].
Regardless whether recovering an approximate local min reveals any useful information about x, up to periodicity,
it is still possible to make such a task challenging for an adversary. In general, the encoding circuit will generate
expectationvalueswithtrigonometricterms. Todemonstrate,wecanconsideraunivariatecaseofasingletrigonomial
f(x)=sin(ωx),withfrequencyω. Thisfunctionwillbeω-Lipschitzcontinuouswithω2-Lipchitzcontinuousgradient.
Hence,whenconsideringthescalingofgradient-basedapproachinEq48weseethatthefrequencyofthetrigonometric
terms will directly impact the ability to find a solution. Hence, if selecting a frequency that scales exponentially
ω = (exp(n)), then snapshot inversion appears to be exponentially difficult with this technique.
O
Importantly, if the feature map includes high frequency terms, for example the Fourier Tower map of [28], then
β and L can be (exp(n)). However, as noted in Section IVA it is possible to make use of the circuit structure to
O
obtain more efficient attacks. In addition, a poor local minimum may not leak any information about x.16
1. Direct Input Recovery
Notethatitalsomaybepossibletocompletelyskipthesnapshotrecoveryprocedureandinsteadvariationallyadjust
x′ sothatthemeasuredgradientsofthequantumcircuitC′,matchtheknowngradientsC withrespecttotheactual
j j
input data. This approach requires consideration of the same scaling characteristics explained in Eq 48, particularly
focusing on identifying the highest frequency component in the gradient spectrum. If the highest frequency term in
the gradient C scales exponentially, ω = (exp(n)), then even gradient descent based methods are not expected to
j
O
find an approximate local min in polynomial time.
Further privacy insights can be gained from Eq 21 where a direct relationship between the gradients and the
expectation value snapshot is shown, which in general can be written as
C (x)=χ(j) e (x). (49)
j t · snap
This indicates that the highest frequency terms of any e component will also correspond to the highest frequency
snap
terms in C (x), as long as its respective coefficient is non-zero χ(j) =0.
j t ̸
Thisunderscoresscenarioswheredirectinputrecoverymayprovemorechallengingcomparedtosnapshotinversion,
particularly in a VQC model. Consider a subset e˜ e where each component has the highest frequency that
snap snap
⊆
scales polynomially with n. If there are sufficiently many values in e˜ then recovering approximate local min
snap
to Eq 46 may be feasible for these components. However, for gradient terms C (x) that depend on all values of
j
e ,includingtermsoutsideofe˜ thatexhibitexponentialfrequencyscaling, thengradientdescentmethodsmay
snap snap
take exponentially long when attempting direct input inversion, even if recovering approximate local minima to the
snapshot inversion task can be performed in polynomial time.
Investigations into direct input recovery have been covered in previous work [28] where the findings concluded that
thegradientsgeneratedbyC (x)wouldformalosslandscapedependentonthehighestfrequencyω generatedbythe
j
encodingcircuit, indicatingthat exponentially scalingfrequencies ledto models thattake exponentialtime torecover
the input using quantum-assisted direct input recovery. The Fourier tower map encoding circuit used in [28] was
designed such that ω scales exponentially to provide privacy, this was done by using m qubits in a sub-register per
data input x , with the single qubit rotation gates parameterized by an exponentially scaling amount. The encoding
j
can be defined as
d m
(cid:79)(cid:16)(cid:79) (cid:17)
R (5l−1x ) . (50)
X j
j=1 l=1
Hence,thegradientcontainedexponentiallyscalinghighestfrequencyterms,leadingtoamodelwheregradientdescent
techniques took exponential time. However, if considering the expectation value of the first qubit in a sub-register
of this model, we note this corresponds to a frequency ω = 1 and hence the respective expectation value for the
first qubit would be snapshot invertible. However, in the case of [28] the DLA was exponentially large, meaning the
model was not snapshot recoverable, hence these snapshots could not be found to then be invertible. Hence, from
our new insights, we can conclude that the privacy demonstrated in [28] was dependent on having an exponential
DLA dimension. However, an exponentially large DLA also led to an untrainable model, limiting the real-world
applicability of this previous work. Lastly, recall that Algorithm 3 in the case poly DLA and slow Pauli expansion
is a completely classical snapshot inversion attack for the Fourier tower map. Further, highlighting how snapshot
inversion can be easier than direct inversion.
We show that both direct input recovery and snapshot inversion are dependent on frequencies ω generated by the
encoding circuit, highlighting that this is a key consideration when constructing VQC models. The introduction of
high frequency components can be used to slow down methods that obtain approximate local minimum to Eq 46.
However, for true privacy breakage, it appears that in general we still need to resort to grid search, which becomes
exponentiallyhardwithdimensionregardlessofhigh-frequencyterms. However, forproblemswithasmallamountof
input, introducing high frequency terms can be used to also make grid search harder. The idea of introducing large
frequencies is a proxy for the more general condition that our results hint at for privacy, which is that the feature
map ρ(x′) should be untrainable in terms of varying x′.
Notably,casesexistwherethesamemodelcanhaveanexponentialfrequencygradient,butcanstillcontainacertain
number of expectation snapshot values with polynomial scaling frequencies. Hence, it is also important to note that
merely showing that a model is not direct input recoverable does not guarantee privacy, as one needs to also consider
that if the model is snapshot recoverable and that these snapshots may be invertible if sufficient polynomial scaling
frequency terms can be recovered. This duality highlights the complexity of ensuring privacy in quantum computing
models and stresses the need for a comprehensive analysis of the frequency spectrum in both model construction and
evaluation of privacy safeguards.17
2. Expectation Value Landscape Numerical Results
In this section, we provide a numerical investigation of the impact of high-frequency components in the encoding
circuit on the landscape of Eq 46 for snapshot inversion. The idea is to present examples that move beyond the
Fourier tower map. We present two cases of encodings that would generally be difficult to simulate classically. By
plotting a given expectation value against a univariate x we can numerically investigate the frequencies produced by
both models.
FIG. 4: Encoding circuit diagram showing a single qubit R rotation gate parameterised by the univariate parameter x, but
X
with arbitrary 2n dimensional unitaries applied before and after the x parameterized gate. Despite being hard to simulate
analytically, the expectation value e varies as a simple sinusoidal function in x, regardless of the total number of qubits n.
in
In Figure 4 we demonstrate a circuit in which x parameterizes a single R rotation gate, but on either side of this
X
is an unknown arbitrary unitary matrix acting on n qubits. This would be classically hard to simulate due to the
arbitrary unitary matrices; however, the result effectively corresponds to taking measurements on an unknown basis,
andusingonlyafewsamplesofxitispossibletorecreatethegraphasasinglefrequencysinusoidalrelationship. This
results in the distance between the stationary points being r =π for any value of n. This corresponds to a frequency
ω = r = 1, regardless of the value of n. This circuit therefore exhibits constant frequency scaling independent of n
π
and hence could be easy for gradient-based methods to recovery approximate local min.
FIG. 5: Encoding circuit diagram showing a SU(2n) gate parameterised by a univariate parameter x.
We briefly give an example of a type of circuit that can generate high-frequency expectation values. Figure 5
demonstrates a circuit where x parameterizes an SU(2n) gate. The result when measuring the same expectation
value corresponds to a highest frequency term that is exponentially increasing. This is shown in the plot in Figure 5
in which the distance between stationary points r shrinks exponentially as the number of qubits increases for the
SU(2n) parameterized model, which roughly corresponds to an exponentially increasing highest frequency term. A
comparisonbetweentheexpectationvaluelandscapeofthetwodifferentencodingarchitectures,isshowninFigure6,
demonstrating that the single rotation gate parameterization, as shown in Figure 4, produces a sinusoidal single-
frequency distribution, even as the number of qubits is increased; while the SU(2n) gate parameterization, shown in
Figure 5, contains exponentially increasing frequency terms. A visual representation for the multivariate case is also
demonstratedinFigure8whichshowstheexpectationvaluelandscapewhentwoinputparametersareadjusted,fora
model comprised of two different SU(2n) parameterized gates parameterized by the variables x and x respectively,
1 2
demonstrating that as more qubits are used, the frequencies of the model increase and hence so does the difficulty of
finding a solution using gradient descent techniques.
Thetwoexamplecircuitsdemonstrateencodingcircuitsthatarehardtosimulate,andhencenoanalyticalexpression
for the expectation values can be easily found. These models do not admit classical snapshot inversion; however, by
samplingexpectationvaluesitmaybepossibletovariationallyperformquantum-assistedsnapshotinversion. Whether
numerical snapshot inversion can be performed efficiently will likely be affected by the highest frequency ω inherent
in the encoding, which will itself depend on the architecture of the encoding circuit. This suggests that designing
encoding circuits such that they contain high-frequency components is beneficial in high-privacy designs. We have
shown that SU(2n) parameterized gates can produce high-frequency terms, whereas single-qubit encoding gates will
be severely limited in the frequencies they produce.18
0.4
0.7
0.625
0.3
0.3
0.6 0.600
0.2 0.2
0.575
0.1 0.5 0.1
0.550
0.0 0.0
0.4
0 2 4 6 0 2 4 6
x(radians) x(radians)
(a) Numberofqubitsn=2. (b) Numberofqubitsn=3.
0.25
0.06 0.625
0.20 0.50
0.600
0.15 0.45 0.04
0.575
0.10 0.40
0.02 0.550
0.05 0.35
0.525
0.00 0.30 0.00
0 2 4 6 0 2 4 6
x(radians) x(radians)
(c) Numberofqubitsn=4. (d) Numberofqubitsn=5.
FIG. 6: Comparison of how expectation value of the measurement of Z varies with x for both the model parameterized
1
using a single R rotation gate as detailed in Figure 4 and the model parameterized using an SU(2n) gate as detailed in
X
Figure 5, for varying amounts of qubits.
1 Fit: r=6.91e−0.91n
0
1
−
2
−
3
−
1 2 3 4 5 6
NumberofQubitsn
FIG. 7: Plot showing the relationship between the average minimum distance r between stationary points of the expectation
value Z⊗I⊗n−1 as a function of a univariate x input. The encoding circuit considered is a parameterized SU(2n) gate which
is parameterized by a univariate input x as U =eiHx, where H is a randomly generated Hermitian matrix. The average was
taken over ten repeated experiments where H was regenerated each time.
)x(pansetohspanstiucric)n2(US
)x(pansetohspanstiucric)n2(US
)r(gol
)x(pansetohspanstiucricXR
)x(pansetohspanstiucricXR
)x(pansetohspanstiucric)n2(US
)x(pansetohspanstiucric)n2(US
)x(pansetohspanstiucricXR
)x(pansetohspanstiucricXR19
0.6
0.8 0.4
0.5
0.6 0.4 0.3
0.4 0 0.3 0 0.2 0
1 0.2 1 1
0.2 2 0.1 2 0.1 2
3 3 3
0.0 4 x2 0.0 4 x2 0.0 4 x2
0 1 2 x3 1 4 5 6 65 0 1 2 x3 1 4 5 6 65 0 1 2 x3 1 4 5 6 65
(a) Numberofqubitsn=1. (b) Numberofqubitsn=2. (c) Numberofqubitsn=3.
FIG. 8: Comparison how expectation value of the measurement of Z varies with x and x for a model parameterized by
1 1 2
the circuit v(x)=eiH2x2eiH1x1, where H
1
and H
2
are randomly chosen Hermitian matrices of dimension 2n, where n is the
number of qubits in the circuit.
V. DISCUSSION
We utilize this section to draw the connections between the two key properties of VQC: trainability, i.e., the lack
of barren plateaus [34, 50], and the ability to retain privacy of input. Building upon this connection, we discuss the
prospects and future of achieving robust privacy guarantees with VQC models.
A. Connections between Trainability and Privacy in VQC
Solely requiring a machine learning model to be private is not sufficient to deploy it for a practical use case of
distributed learning such as federated learning. A key requirement in this collaborative learning scenario is also to
ensure that the model remains trainable. A plethora of works have gone into exactly characterizing the trainability
of VQC models by analyzing the presence of barren plateau in the VQC model, starting from the work of [31] and
culminating in the works of [34, 50]. Especially, the work of [34] provides an exact expression of the variance of the
gradient of the model when the VQC is constrained to be in the LASA case, the details of which we also provide in
Appendix C4 for completeness. A key insight into these works suggests that LASA models, with exponentially-sized
DLA, may lead to the presence of barren plateaus (see Theorem 6 under Appendix C4), drastically deteriorating the
trainability of such models.
Within our privacy framework centered around snapshot recoverability (SecIII), we also show via Theorem 2 that
LASA models with an exponential size DLA are not classically snapshot recoverable, although this may lead to
untrainablemodels. Wecanthereforeconcludethatapossibleconditionforprotectionagainstclassicalinputrecovery
using gradients in a VQC model is to choose an ansatz that exhibits an exponentially large dynamical Lie algebra
dimension, as this would render snapshot recovery difficult. Through our framework, we can see that previous works
[28] effectively relied on this property to ensure privacy. Combining the concept of trainability leads to the following
corollary on the privacy of VQC models:
Corollary 5. Any trainable VQC on n qubits that satisfies the LASA condition in Def 5, fulfills the slow Pauli
expansion condition as highlighted in Def 9, and has a DLA g whose dimension scales as (poly(n)), would admit
O
snapshot recoverability with complexity (poly(n)).
O
Hencewecanconcludethat,atleastintheLASAcaseofVQC,theprivacyofthemodelislinkedtotheDLAdimen-
sion and furthermore that there is a direct tradeoff between privacy and trainability of the model. As exponentially
sized DLA models are expected to be untrainable in the LASA case, this means that for realistic applications, it does
not seem feasible to rely on quantum privacy derived from an exponential DLA precluding snapshot recoverability
in the model. This suggests that any privacy enhancement from quantum VQCs should not derive itself from the
variational part of the circuit for LASA type models that are intended to be trainable. In other words, we expect
the majority of trainable VQC models to be vulnerable to weak privacy breaches. The privacy of variational models
beyond the LASA case becomes linked to a larger question within the field, notably, whether there exist quantum
variational models that are not classically simulatable and do not have barren plateaus [51].
esnap(x1,x2) esnap(x1,x2) esnap(x1,x2)20
It is also worth noting that if one attempted to create a model that is not snapshot recoverable by ensuring that
D <dim(g),andhenceanunderparameterizedsystemofequations,itwouldeffectivelyleadtoanunderparameterized
model. A model is underparmeterized when there are not enough variational parameters to fully explore the space
generated by the DLA of the ansatz, which is a property that may not be desirable for machine learning models [52].
B. Future Direction of VQC Quantum Privacy
DuetotheaboveargumentsuggestingthatachievingprivacyviaanexponentiallylargeDLAmaycausetrainability
issues in the underlying model, it appears that future improvements in privacy using VQC may primarily focus on
preventing the snapshot inversion step, as we highlight in Sec IIC. This promotes a focus on the encoding circuit
architectures of the VQC in order to prevent the model admitting snapshot inversion to facilitate input recovery.
We have explicitly shown the necessary condition required to achieve privacy from purely classical attacks. If it is
notpossibletoclassicallysimulatetheexpectationvaluesofthequantumencodedstatewithrespecttotheDLAbasis
elements of the variational circuit, then it will not be possible to attempt classical analytical or numerical inversion
attacks. Any VQC designed where these expectation values cannot be simulated will, therefore, be protected against
any purely classical snapshot inversion attempts. This condition can therefore prevent strong privacy breaches, as
long as the attacking agent only has access to a classical device.
In the case where the attacker can simulate expectation values of the DLA basis or has access to a quantum device
to obtain the expectation values, then numerical classical snapshot inversion or numerical quantum-assisted snapshot
inversion can be attempted, respectively. We have shown that in this case an important factor in preventing these
techniquesisthattheexpectationvalueshaveexponentiallyscalingfrequencytermsresultingintheattacksrequiring
to solve a system of high-degree polynomial equations. The implication of this is that to achieve useful privacy
advantageinVQCitmayrequirethattheencodingcircuitisconstructedinsuchawaythattheexpectationvaluesof
the DLA basis elements of the variational circuit contain frequency terms that scale exponentially. Notably, we find
that having high frequency terms in the gradients, as suggested in the encoding circuit of [28], does not necessarily
protect against numerical snapshot inversion attacks. This is because the gradients inherit the highest frequency
term from all expectation values, but there may be a sufficient number of polynomial frequency expectation values to
perform snapshot inversion, even if direct input inversion is not possible.
Unlike the variational case, where a connection between DLA dimension and trainability has been established, the
effectthatprivacy-enhancingquantumencodingswouldhaveonthetrainabilityofamodelislessclear. Ifthemajority
of expectation values used in model contain exponentially large frequencies, then this potentially restricts the model
to certain datasets. In classical machine learning, there have been positive results using trigonometric feature maps
to classify high frequency data in low dimensions [33]. It remains as a question for future research the types of data
which may be trained appropriately using the privacy-preserving high frequency feature maps proposed. If models of
this form are indeed limited in number, then the prospects for achieving input privacy from VQC models appear to
be limited. More generally the prospect for quantum privacy rests on feature maps that are untrainable with regard
toadjustingx′ torecoverexpectationvaluese ,whileatthesametimeremainingusefulfeaturemapswithrespect
snap
to the underlying dataset and overall model.
VI. CONCLUSION
In this research, we conducted a detailed exploration of the privacy safeguards inherent in VQC models regard-
ing the recovery of original input data from observed gradient information. Our primary objective was to develop
a systematic framework capable of assessing the vulnerability of these quantum models to general class of inver-
sion attacks, specifically through introducing the snapshot recovery and snapshot inversion attack techniques, which
primarily depend on the variational and encoding architectures, respectively.
Ouranalysisbeganbyestablishingthefeasibilityofrecoveringsnapshotexpectationvaluesfromthemodelgradients
under the LASA assumption. We demonstrated that such recovery is viable when the Lie algebra dimension of the
variational circuit exhibits polynomial scaling in the number of qubits. This result underscores the importance of
algebraicstructureindeterminingthepotentialforprivacybreachesinquantumcomputationalmodels. Furthermore,
duetothefactthatapolynomialscalingDLAdimensioniscommonlyrequiredformodelstobetrainable,ourresults
suggest that a trade-off may exist between privacy and trainability of VQC models. Assuming one insists on a
polynomial-sized DLA, our framework suggests that a weak privacy breach will always be possible for the type of
VQC model studied. To ensure privacy of the model overall, one cannot rely on the variational circuit and needs to
instead focus more on the encoding architecture and ensuring snapshot inversion cannot be performed. If snapshot
inversion is not possible, then at least strong privacy breaches can be prevented.21
We then explored snapshot inversion, where the task is to find the original input from the snapshot expectation
values,effectivelyinvertingtheencodingprocedure. Studyingwidelyusedencodingansatz,suchasthelocalmultiqubit
Pauli encoding, we found that under the conditions that a fixed subset of the data paramaterizes a constituent state
which has sufficient overlap with the DLA, and the number of gates used to encode each dimension of the input
x was polynomial, that snapshot inversion was possible in (poly(n,log(1/ϵ)) time. This shows that a potentially
O
wide range of encoding circuits are vulnerable to strong privacy breaches and brings their usage in privacy-focused
models into question. For the most general encoding, which we approached as a black-box optimization problem, we
demonstrated that using perturbed gradient descent to find a solution is constrained by the frequency terms within
the expectation value Fourier spectrum. In general for exactly finding x it appears that a grid search would be
required. Although we cannot provide strict sufficient conditions due to the possibility of unfavorable local minima
with perturbed gradient, we note that gradient descent for snapshot inversion may, in some cases, be easier to
perform for snapshot inversion than for direct input data recovery from the gradients. This simplification arises
because gradients can inherit the highest frequency term from the snapshots, potentially leading to scenarios where
thegradienttermcontainsexponentiallylargefrequencies. However,theremaystillbesufficientpolynomialfrequency
snapshots to permit snapshot inversion. This shifts the focus in attack models away from direct input recovery from
gradients, a common approach in classical privacy analysis, towards performing snapshot inversion as detailed in this
study as a potentially more efficient attack method.
Thedualinvestigationallowedustoconstructarobustevaluativeframeworkthatnotonlyfacilitatestheassessment
of existing VQC models for privacy vulnerabilities but also aids in the conceptualization and development of new
models where privacy is a critical concern. Our reevaluation of previous studies, such as those cited in [28], through
thelensofournewframework,revealsthattheprivacymechanismsemployed,namelytheutilizationofhigh-frequency
componentsandexponentiallylargeDLA,effectivelypreventinputdatarecoveryviaalackofsnapshotrecoverability,
but at the same time contribute to an untrainable model of limited practical use.
Inconclusion,weofferamethodologicalapproachforclassifyingandanalyzingtheprivacyfeaturesofVQCmodels,
presenting conditions for weak and strong privacy breaches for a broad spectrum of possible VQC architectures. Our
findingsnotonlyenhancetheunderstandingofquantumprivacymechanismsbutalsoofferstrategicguidelinesforthe
design of quantum circuits that prioritize security while at the same time maintaining trainability. Looking ahead,
thisresearchpavesthewayformorerobustquantummachinelearningmodeldesigns,whereprivacyandfunctionality
are balanced. This knowledge offers the potential to deliver effective machine learning models that simultaneously
demonstrate a privacy advantage over conventional classical methods.
ACKNOWLEDGMENTS
The authors thank Brandon Augustino, Raymond Rudy Putra and the rest of our colleagues at the Global Tech-
nology Applied Research Center of JPMorgan Chase for helpful comments and discussions.
DISCLAIMER
ThispaperwaspreparedforinformationalpurposesbytheGlobalTechnologyAppliedResearchcenterofJPMorgan
Chase & Co. This paper is not a product of the Research Department of JPMorgan Chase & Co. or its affiliates.
Neither JPMorgan Chase & Co. nor any of its affiliates makes any explicit or implied representation or warranty
and none of them accept any liability in connection with this paper, including, without limitation, with respect to
thecompleteness, accuracy, orreliabilityoftheinformationcontainedhereinandthepotentiallegal, compliance, tax,
or accounting effects thereof. This document is not intended as investment research or investment advice, or as a
recommendation, offer, or solicitation for the purchase or sale of any security, financial instrument, financial product
or service, or to be used in any way for evaluating the merits of participating in any transaction.
[1] T. Liu, Z. Wang, H. He, W. Shi, L. Lin, W. Shi, R. An, and C. Li, “Efficient and secure federated learning for financial
applications,” 2023.
[2] T.Awosika,R.M.Shukla,andB.Pranggono,“Transparencyandprivacy: Theroleofexplainableaiandfederatedlearning
in financial fraud detection,” 2023.
[3] G.Kaissis,M.R.Makowski,D.Ru¨ckert,andR.F.Braren,“Secure,privacy-preservingandfederatedmachinelearningin
medical imaging,” Nature Machine Intelligence, vol. 2, pp. 305 – 311, 2020.22
[4] S. Ahamed, N. Nishant, A. Selvaraj, N. Gandhewar, S. A, and B. K, “Investigating privacy-preserving machine learning
for healthcare data sharing through federated learning,” The Scientific Temper, vol. 14, pp. 1308–1315, 12 2023.
[5] E.Aguiar,C.Jr,andA.Traina,“Securityandprivacyinmachinelearningforhealthsystems: Strategiesandchallenges,”
Yearbook of Medical Informatics, vol. 32, pp. 269–281, 12 2023.
[6] C. Park, T. Choi, T. Kim, M. Cho, J. Hong, M. Choi, and J. Choo, “Fedgeo: Privacy-preserving user next location
prediction with federated learning,” 2023.
[7] J. P. Albrecht, “How the gdpr will change the world,” European Data Protection Law Review, vol. 2, pp. 287–289, 2016.
[8] A.Brauneck,L.Schmalhorst,M.Majdabadi,M.Bakhtiari,U.Vo¨lker,C.Saak,J.Baumbach,L.Baumbach,andG.Buch-
holtz, “Federated machine learning in data-protection-compliant research,” Nature Machine Intelligence, vol. 5, pp. 2–4,
01 2023.
[9] J. Tong, C. Luo, M. Islam, N. Sheils, J. Buresh, M. Edmondson, P. Merkel, E. Lautenbach, R. Duan, and Y. Chen,
“Distributed learning for heterogeneous clinical data with application to integrating covid-19 data across 230 sites,” npj
Digital Medicine, vol. 5, p. 76, 06 2022.
[10] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient learning of deep
networks from decentralized data,” 2023.
[11] L.Zhu,Z.Liu,andS.Han,“Deepleakagefromgradients,”inAdvancesinNeuralInformationProcessingSystems,vol.32,
Curran Associates, Inc., 2019.
[12] Y. Huang, S. Gupta, Z. Song, K. Li, and S. Arora, “Evaluating gradient inversion attacks and defenses in federated
learning,” in Advances in Neural Information Processing Systems (A. Beygelzimer, Y. Dauphin, P. Liang, and J. W.
Vaughan, eds.), 2021.
[13] B. Zhao, K. R. Mopuri, and H. Bilen, “idlg: Improved deep leakage from gradients,” arXiv preprint arXiv:2001.02610,
2020.
[14] J. Geiping, H. Bauermeister, H. Dro¨ge, and M. Moeller, “Inverting Gradients - How easy is it to break privacy in fed-
erated learning?,” in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[15] H.Yin,A.Mallya,A.Vahdat,J.Alvarez,J.Kautz,andP.Molchanov,“SeethroughGradients: ImageBatchRecoveryvia
GradInversion,”in2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),pp.16332–16341,
2021.
[16] L.T.Phong,Y.Aono,T.Hayashi,L.Wang,andS.Moriai,“Privacy-preservingdeeplearningviaadditivelyhomomorphic
encryption,” IEEE Transactions on Information Forensics and Security, vol. 13, no. 5, pp. 1333–1345, 2018.
[17] S.Eloul,F.Silavong,S.Kamthe,A.Georgiadis,andS.J.Moran,“Enhancingprivacyagainstinversionattacksinfederated
learning by using mixing gradients strategies,” arXiv preprint arXiv:2204.12495, 2022.
[18] R. Huang, X. Tan, and Q. Xu, “Quantum federated learning with decentralized data,” IEEE Journal of Selected Topics
in Quantum Electronics, vol. 28, pp. 1–10, 2022.
[19] J. Qi, X. Zhang, and J. Tejedor, “Optimizing quantum federated learning based on federated quantum natural gradient
descent,” ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 1–5, 2023.
[20] M. Chehimi and W. Saad, “Quantum federated learning with quantum data,” ICASSP 2022 - 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 8617–8621, 2021.
[21] C. Li, B. Li, O. Amer, R. Shaydulin, S. Chakrabarti, G. Wang, H. Xu, H. Tang, I. Schoch, N. Kumar, C. Lim, J. Li,
P. Cappellaro, and M. Pistoia, “Blind quantum machine learning with quantum bipartite correlator,” 2023.
[22] D. Gurung, S. R. Pokhrel, and G. Li, “Decentralized quantum federated learning for metaverse: Analysis, design and
implementation,” ArXiv, vol. abs/2306.11297, 2023.
[23] L.Lusnig,A.Sagingalieva,M.Surmach,T.Protasevich,O.Michiu,J.McLoughlin,C.Mansell,G.de’Petris,D.Bonazza,
F.Zanconati,A.Melnikov,andF.Cavalli,“Hybridquantumimageclassificationandfederatedlearningforhepaticsteatosis
diagnosis,” Diagnostics, vol. 14, p. 558, Mar. 2024.
[24] D. Gilboa and J. R. McClean, “Exponential quantum communication advantage in distributed learning,” 2023.
[25] C. Li, N. Kumar, Z. Song, S. Chakrabarti, and M. Pistoia, “Privacy-preserving quantum federated learning via gradient
hiding,” Quantum Science and Technology, vol. 9, no. 3, p. 035028, 2024.
[26] I. Koyasu, R. Raymond, and H. Imai, “Distributed coordinate descent algorithm for variational quantum classification,”
in2023IEEEInternationalConferenceonQuantumComputingandEngineering(QCE),vol.1,pp.457–467,IEEE,2023.
[27] W. Li, S. Lu, and D.-L. Deng, “Quantum federated learning through blind quantum computing,” Science China Physics,
Mechanics & Astronomy, vol. 64, 2021.
[28] N.Kumar,J.Heredge,C.Li,S.Eloul,S.H.Sureshbabu,andM.Pistoia,“Expressivevariationalquantumcircuitsprovide
inherent privacy in federated learning,” arXiv preprint arXiv:2309.13002, 2023.
[29] S. Y.-C. Chen and S. Yoo, “Federated quantum machine learning,” Entropy, vol. 23, no. 4, 2021.
[30] J. Haah, Y. Liu, and X. Tan, “Efficient approximate unitary designs from random pauli rotations,” arXiv preprint
arXiv:2402.05239, 2024.
[31] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven, “Barren plateaus in quantum neural network
training landscapes,” Nature communications, vol. 9, no. 1, p. 4812, 2018.
[32] E.AnschuetzandB.Kiani, “Quantumvariationalalgorithmsareswampedwithtraps,” Nature Communications, vol.13,
p. 7760, 12 2022.
[33] M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. T. Barron,
and R. Ng, “Fourier features let networks learn high frequency functions in low dimensional domains,” 2020.23
[34] E. Fontana, D. Herman, S. Chakrabarti, N. Kumar, R. Yalovetzky, J. Heredge, S. H. Sureshbabu, and M. Pistoia, “The
adjoint is all you need: Characterizing barren plateaus in quantum ans\” atze,” arXiv preprint arXiv:2309.07902, 2023.
[35] R. Somma, G. Ortiz, H. Barnum, E. Knill, and L. Viola, “Nature and measure of entanglement in quantum phase
transitions,” Physical Review A, vol. 70, no. 4, p. 042311, 2004.
[36] M.L.Goh,M.Larocca,L.Cincio,M.Cerezo,andF.Sauvage,“Lie-algebraicclassicalsimulationsforvariationalquantum
computing,” arXiv preprint arXiv:2308.01432, 2023.
[37] M. Larocca, P. Czarnik, K. Sharma, G. Muraleedharan, P. J. Coles, and M. Cerezo, “Diagnosing barren plateaus with
tools from quantum optimal control,” Quantum, vol. 6, p. 824, 2022.
[38] Q. Wang, Y. Ma, K. Zhao, and Y. Tian, “A comprehensive survey of loss functions in machine learning,” Annals of Data
Science, pp. 1–26, 2020.
[39] M. Ragone, P. Braccia, Q. T. Nguyen, L. Schatzki, P. J. Coles, F. Sauvage, M. Larocca, and M. Cerezo, “Representation
theory for geometric quantum machine learning,” arXiv preprint arXiv:2210.07980, 2022.
[40] J. F. Grcar, “Mathematicians of gaussian elimination,” Notices of the AMS, vol. 58, no. 6, pp. 782–792, 2011.
[41] R. Suzuki, K. Mitarai, and K. Fujii, “Computational power of one- and two-dimensional dual-unitary quantum circuits,”
Quantum, vol. 6, p. 631, Jan. 2022.
[42] J. Nocedal and S. J. Wright, Numerical optimization. Springer, 1999.
[43] M. A. Jenkins and J. F. Traub, “A three-stage variable-shift iteration for polynomial zeros and its relation to generalized
rayleigh iteration,” Numerische Mathematik, vol. 14, no. 3, pp. 252–263, 1970.
[44] D.Wierichs,J.Izaac,C.Wang,andC.Y.-Y.Lin,“Generalparameter-shiftrulesforquantumgradients,”Quantum,vol.6,
p. 677, Mar. 2022.
[45] B. Buchberger, Gro¨bner Bases: An Algorithmic Method in Polynomial Ideal Theory, pp. 184–232. Reidel Publishing
Company, 01 1985.
[46] T.W.Dub´e,“Thestructureofpolynomialidealsandgro¨bnerbases,”SIAMJournalonComputing,vol.19,no.4,pp.750–
773, 1990.
[47] E.W.MayrandA.R.Meyer,“Thecomplexityofthewordproblemsforcommutativesemigroupsandpolynomialideals,”
Advances in Mathematics, vol. 46, no. 3, pp. 305–329, 1982.
[48] Y. Nesterov et al., Lectures on convex optimization, vol. 137. Springer, 2018.
[49] C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan, “How to escape saddle points efficiently,” 2017.
[50] M. Ragone, B. N. Bakalov, F. Sauvage, A. F. Kemper, C. O. Marrero, M. Larocca, and M. Cerezo, “A Unified Theory of
Barren Plateaus for Deep Parametrized Quantum Circuits,” arXiv, 9 2023.
[51] M. Cerezo, M. Larocca, D. Garc´ıa-Mart´ın, N. Diaz, P. Braccia, E. Fontana, M. S. Rudolph, P. Bermejo, A. Ijaz,
S. Thanasilp, et al., “Does provable absence of barren plateaus imply classical simulability? or, why we need to rethink
variational quantum computing,” arXiv preprint arXiv:2312.09121, 2023.
[52] N.ShohamandH.Avron,“Experimentaldesignforoverparameterizedlearningwithapplicationtosingleshotdeepactive
learning,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 10, pp. 11766–11777, 2023.
[53] F. J. Schreiber, J. Eisert, and J. J. Meyer, “Classical surrogates for quantum learning models,” Physical Review Letters,
vol. 131, Sept. 2023.
[54] R. Sweke, E. Recio, S. Jerbi, E. Gil-Fuster, B. Fuller, J. Eisert, and J. J. Meyer, “Potential and limitations of random
fourier features for dequantizing quantum machine learning,” 2023.
[55] M. Schuld, R. Sweke, and J. J. Meyer, “Effect of data encoding on the expressive power of variational quantum-machine-
learning models,” Physical Review A, vol. 103, Mar. 2021.
[56] J. Landman, S. Thabet, C. Dalyac, H. Mhiri, and E. Kashefi, “Classically approximating variational quantum machine
learning with random fourier features,” arXiv preprint arXiv:2210.13200, 2022.
[57] M. Larocca, S. Thanasilp, S. Wang, K. Sharma, J. Biamonte, P. J. Coles, L. Cincio, J. R. McClean, Z. Holmes, and
M. Cerezo, “A review of barren plateaus in variational quantum computing,” 2024.
[58] M. Ragone, B. N. Bakalov, F. Sauvage, A. F. Kemper, C. O. Marrero, M. Larocca, and M. Cerezo, “A unified theory of
barren plateaus for deep parametrized quantum circuits,” arXiv preprint arXiv:2309.09342, 2023.
[59] M. Larocca, N. Ju, D. Garc´ıa-Mart´ın, P. J. Coles, and M. Cerezo, “Theory of overparametrization in quantum neural
networks,” Nature Computational Science, vol. 3, no. 6, pp. 542–551, 2023.24
Appendix A: Notation used in the work
Symbol Meaning
x Data Input
θ Variational Parameters
y (x) Model Output
θ
C Gradient of Model Output wrt θ
j j
n Total number of qubits
d Dimension of input vector
D Number of variational parameters
V(x) Encoding Circuit
U(θ) Variational Circuit
O Measurement Operator in VQC Model
µ Coefficients of O when written in terms of supported DLA basis terms
α
g Dynamical Lie algebra
Ω Maximum number of sub-register partitions
B Basis of DLA
k
B Portion of basis DLA basis element that only acts on a given sub-register
k
H Hermitian gate generators of the encoding map circuit
k
m Number of qubits in sub-register
ρ (x ) Constituent state in a separable state
J J
Ω Maximum number of constituent states in a separable state.
A Element of constituent separable state density matrix sum expansion
k
λ Terms when expanding constituent density matrix ρ (x )
J J
P Diameter of sphere of possible x values in Rd
ϵ User defined acceptable error
TABLE II: Table of Notation
Appendix B: General Cost Function
In this section we go into more detail regarding how the choice of cost function would affect the attack procedure.
In practical real-world examples, we would most likely have gradients with respect to some cost function ∂Cost(yθ,y)
∂θ
rather than gradients in the form C = ∂yθ that are used in this paper (which would correspond to a linear cost
j ∂θ
function of the form Cost(y ,y) = y g(y), where g(y) is any function independent of y ). We briefly show here
θ θ θ
−
that with a slight modification our results hold for any differentiable cost function.
Using the chain rule we can see that the θ gradient can be written
j
∂Cost(y ,y) ∂Cost(y ,y) ∂y
C˜ = θ = θ θ . (B1)
j
∂θ ∂y · ∂θ
j θ j
If the value of ∂Cost(yθ,y) can be calculated, which would be possible if we were given the output of the model y ,
∂yθ θ
along with the data labels y, then it would be possible to directly convert between C˜ and C . When this is not the
j j
case, it is still possible to attempt a solution, although it requires one additional gradient than usual. We show this
by considering that the ∂Cost(yθ,y) term, although unknown, will be the same for all θ . Hence we can eliminate this
∂yθ j
term considering ratios of the known gradients defined as
R
C˜
j =
∂ ∂y θθ
j . (B2)
j ≡ C˜ ∂yθ
1 ∂θ1
This allows us to write new equations for all j >1 without this unknown term as
∂y ∂y
θ =R θ , (B3)
j
∂θ · ∂θ
j 1
hence in the notation used throughout this work
C =R C . (B4)
j j 1
·25
In Eq 21 it was previously shown that the C gradients can be written in terms of snapshot components as
j
dim(g)
C = (cid:88) χ(j)(e ) . (B5)
j t snap t
t=1
Hence, given dim(g)+1 gradients C , we can construct dim(g) simultaneous equations
j
dim(g)
C R C = (cid:88) (cid:16) χ(j)(e ) R χ(1)(e ) (cid:17) =0. (B6)
j − j · 1 t snap t − j t snap t
t=1
Hence, wehaveasystemofequationsthatcanbesolvedasbefore, althoughaswecanonlytechnicallyfindtheratios
between gradients we will require dim(g)+1 gradients as opposed to dim(g) gradients.
Appendix C: High Frequency Trigonometric Models
The Fourier series picture was utilized in [28] to analyze the privacy of VQC models. In this section, we examine
this picture with regards to privacy analysis.
QuantummodelsadmitanaturaldecompositionintoaFourierseries,whichmeansthatifthemodelisconstructed
such that it contains high-frequency terms, then it gains certain privacy advantages [28]. This is not a uniquely
quantum effect however, and in this section, we shall show how it can be recreated with a classical linear classifier
equipped with a trigonometric feature map.
1. Quantum Models are Linear Classifiers with Trigonometric Feature Maps
The connection between Quantum models and Fourier series has previously informed the construction of certain
dequantisation techniques [53, 54]. Classical surrogate models are capable of being run on classical machines and are
simultaneouslyabletomatchtheoutputofaquantummodeltowithinsomeerror. Itisknownthatthemodeloutput
of a QML model as a Fourier series [55] in the following form
(cid:88)
y(θ)= A eiωx, (C1)
ω
ω∈Ω
where the A coefficients depend on the entire VQC architecture, but the frequency spectrum Ω depends only on the
ω
encoding architecture. If we define
b (θ):=A (θ)+A (θ), (C2)
ω ω −ω
d (θ):=i(A (θ) A (θ)). (C3)
ω ω −ω
−
Then the model output can be written as
(cid:88)(cid:0) (cid:1)
y(θ)=a + b cos(ωx)+d sin(ωx) . (C4)
0 ω ω
ω∈Ω
We can rewrite the model as a linear regression [56] defined by the inner product between a vector of coefficients A
and a trigonometric feature map ϕ(x) as follows
y(θ)=A ϕ(x), (C5)
·
where
(cid:112)
A= Ω(a ,b ,d ...b ,d ), (C6)
| |
0 ω1 ω1 ωδ ωδ
1 (cid:0) (cid:1)
ϕ(x)= 1,cos(ω x),sin(ω x),...,cos(ω x),sin(ω x) . (C7)
(cid:112) 1 1 δ δ
Ω
| |
Hence, a VQC model can effectively be thought of as a linear regression on data that has been transformed by a
trigonometric feature map ϕ(x). However, a key difference is that the coefficients in this linear regression model are
not directly varied in the quantum case, but the coefficients A have a dependency on the θ values in the variational
circuit and therefore evolve as the θ values are adjusted during training.26
2. Classical High Frequency Trigonometric Models
We consider the case of a fully classical model with a classical attack model. In this regime, we encode the data
using some feature map ϕ(x) and then perform linear regression on the resulting input. This can be considered as a
single-layer neural network with a single output neuron with the identity of an activation. Note that in the quantum
case, the corresponding ϕ(x) is exponentially large. If we maintained this condition here, then the classical model
would not even be able to be evaluated (as even storing ϕ(x) would require exponential resources), and hence in
some sense, we would achieve a trivial form of privacy via considering a model we do not have the resources to run
in the first place. We therefore assume a polynomial number of ϕ(x) values in the classical model. This setup,while
technicallydistinct,isreminiscentofthetechniqueofRandomFourierFeatures[56]whichbeenshowntobesuccessful
in creating classical surrogates of quantum models by utilizing a polynomial number of Fourier features.
The classical model output is defined by
y(θ)=A ϕ(x), (C8)
·
where in the classical case the coefficients A are adjusted variationally directly, we are effectively performing a linear
regression. We can write the gradients as
∂Cost (cid:104) (cid:105)
C = = 2(y A ϕ(x)) ϕ(x) . (C9)
j i
∂A j − − · j
This allows simple relations between all the parameters to be written using the fact that
C [ϕ(x)]
k = k , (C10)
C [ϕ(x)]
l l
which if we substitute into the normalisation condition ϕ(x)2 =1. Then we obtain a closed-form solution
| |
C
[ϕ(x)] i = (cid:80) i C2. (C11)
j j
Hence, it is easy to recover the ϕ(x) values from the gradients. If we have ϕ(x) then we can also simply invert the
equation to find x as if we know that [ϕ(x)] =cos(ω x) then we can find
k k
1
x= cos−1([ϕ(x)] ). (C12)
k
ω
k
Thisdemonstratesthatthefeaturemapintheclassicalcaseisbothretrievableandinvertible(uptoperiodicityunless
we enforce the feature map to be injective) and hence there is no analytical privacy in the feature space.
An extension of this would be to use a neural network with the feature map ϕ(x) for the inputs. As long as the
ϕ(x)valuescanberecoveredbyconsideringtheneuralnetworkequations,thenitwillbepossibletoinverthisfeature
map. Trigonometricfeaturemapsofthisstructurehavefoundsuccessinsomeclassicalmachinelearningapplications,
such as learning high-frequency functions in low-dimensional spaces [33].
3. Quantum High Frequency Trigonometric Models
In the quantum case, if one assumes an exponentially large ϕ(x) generated by the quantum variational model as
was the case in [28]. Then this would not be a problem for a classical model using a trigonometric feature map as the
ratio of specific gradients could be taken such that,
C [ϕ(x)] sin(ω x)
k = k = k =tan(ω x), (C13)
C [ϕ(x)] cos(ω x) k
k+1 k+1 k
which would still allow one to invert and find x as
1 (cid:16) C (cid:17)
x= cos−1 k , (C14)
ω C
k k+1
using only two gradient values. However, in a quantum variational circuit, we do not train the Fourier coefficients A
directly, but we train the gate parameters θ that themselves influence the coefficients A. Hence, we cannot perform
the same technique to recover this ratio of feature vector elements.27
If we knew the coefficients A and ∂Cost, then it would be an easy task. However, we only have access to the θ
∂Aj
gradients that can be written
∂Cost (cid:16)∂A (cid:17)
C = = 2(y A ϕ(x)) ϕ(x) . (C15)
j i
∂θ − − · ∂θ ·
j j
Asolutiontothissystemofequationswouldrequirecalculatingtheinnerproductbetweenexponentiallylargevectors.
Even if one attempts a similar trick to the classical case to eliminate the A ϕ(x) term this would yield
·
(cid:16) (cid:17)
∂A ϕ(x)
C k = (cid:16)∂θk · (cid:17), (C16)
C l ∂A ϕ(x)
∂θl ·
which would still require calculating the inner product of exponentially large vectors (before even considering the
challenge of finding the ∂A terms).
∂θj
Hence, even if the feature map ϕ(x) is easy to invert, we see that the privacy of quantum variational circuits in the
trigonometric feature map space can derive from the fact ϕ(x) is not recoverable. This is analogous to the snapshot
recovery discussed in the main work.
Whilebothclassicalandquantumhighfrequencytrigonometricmodelsprovideprotectionagainstmachinelearning
attacks and analytical Attacks in the input space. The exponential nature of quantum models provides analytical
privacy in the trigonometric feature vector space, while polynomial-sized classical models do not exhibit privacy in
featuremapspaceandarefeaturevectorrecoverable. Thissectionhasshownexplicitlyhowanexponentialnumberof
frequencies provides protection in the quantum case, against analytical attacks, when considering the Fourier space.
However, they may still be vulnerable to the snapshot recovery and inversion techniques specified in the main text.
4. Results on Trainability of VQCs
One of the main trainability problems that plague VQCs is exponentially vanishing gradients, more commonly
known as the barren plateau (BP) problem [31, 37, 57]. The BP problem has been characterized [34, 58] for a
restricted class of VQCs known as Lie algebraic supported ansatz (LASA), which cover a wide variety of commonly
used models. Hence, when looking for trainable quantum models, it is easiest to restrict the LASA setting, which
is what is done in this paper. Furthermore, it can be shown that under a stronger yet potentially more reasonable
definition of vanishing gradients for VQCs, a necessary condition for a LASA to avoid a BP is to have a dynamical
Lie algebra (DLA) with a polynomial dimension. It has been conjectured that this claim is far-reaching, in the sense
that models that avoid BPs evolve within spaces that are polynomially sized, in terms of the number of qubits [51].
The following theorem presents a closed form expression for the variance, over uniform parameter initialization, of
the quantum circuit gradient for input x, denoted GradVar .
x
Theorem 6 (Variance of Gradient Theorem 2.9 [34]). Consider ansatz U(θ) with DLA g admitting a decomposition
into simple ideals g and its center c. Given the input state ρ(x) and if the measurement operator iO g (LASA
α
∈
condition), then the variance of the gradient for the classical input x, denoted by GradVar , is given by,
x
GradVar
=(cid:88) ∥H gα∥2 K∥O gα∥2 F∥ρ gα(x) ∥2
F, (C17)
x dim(g )2
α
α
where the subscript α under the operators H, O and ρ denotes the orthogonal projection (under Frobenius inner
product) onto the ideal g g, Further, H 2 is the Killing norm [35] of the generator of the parameter θ with
respect to the gradient whα ic⊆ h is computed,∥ i.e∥ .K , e−θiH in the ansatz U(θ). The Killing norm is defined to be the
Frobenius norm of the operator ad .
iH
A VQC is defined to be trainable when gradients can be efficiently estimated.
Definition 12 (TrainabilityofVQC). The VQC is considered trainable for input x if GradVar satisfies the condition,
(cid:18) (cid:19)
1
GradVar = , (C18)
x O poly(n)
where n is the number of qubits.28
Fact 6.1 (DLAs are Reductive [34, 58]). The DLA admits the following orthogonal (in Frobenius inner product)
decomposition:
(cid:77)
g= g c, (C19)
α
⊕
α
where each g g is a simple ideal (a minimal Lie subalgebra satisfying H g, K g ,[H,K] g ) and c g is
α α α
⊂ ∀ ∈ ∀ ∈ ∈ ⊂
the center of g.
This expression reveals the explicit dependence that the gradient variance has on the DLA dimension in the LASA
setting,whichisthereasoningbehindtheconjecturednecessitythattheDLAdimensionmustbepolynomialtoavoid
BPs. More specifically, we can immediately see from Theorem 6 that as long as dim(g) scales polynomially in n, and
theFrobeniusnormoftheprojectionofinputstateintheDLAaswellasthemeasurementoperatorisnotvanishingly
small in n, the variance of the gradient does not decay exponentially in n and thus leads to a trainable model.
Another source of untrainability of the model is when the cost function optimization landscape is swamped with
spurious local minima which renders it unamenable to any efficient optimizer to find a good approximation to the
optimal solution. As shown by [32], models in the underparameterized phase characterized by trainable parameters
fewerthanthedegreesoffreedomofthesystemexhibittheabovespuriouslocalminimabehavior. Aphasetransition
occurswhenthenumberofparametersscalewiththesystem’sdegreesoffreedomwherethelocalminimaconcentrate
aroundtheglobalminimumrenderingitmoreefficientfortheoptimizerstoconvergetoagoodapproximatesolution.
Thisisreferredtoastheoverparameterized phase. Theworkby[59]showsthatoverparameterizationcanbeachieved
with the number of model parameters scaling as the size of the DLA dimension. Thus for poly(n) sized DLA,
overparameterization requires polynomial ansatz depth, thus ensuring trainability.