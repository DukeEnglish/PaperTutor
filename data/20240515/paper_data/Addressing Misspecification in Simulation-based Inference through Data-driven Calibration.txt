Addressing Misspecification in Simulation-based
Inference through Data-driven Calibration
AntoineWehenkel∗ JuanL.Gamella∗ OzanSener JensBehrmann
Apple ETHZürich&Apple Apple Apple
GuillermoSapiro MarcoCuturi Jörn-HenrikJacobsen
Apple Apple Apple
Abstract
Drivenbysteadyprogressingenerativemodeling,simulation-basedinference(SBI)
hasenabledinferenceoverstochasticsimulators. However,recentworkhasdemon-
strated that model misspecification canharm SBI’s reliability. This work intro-
duces robust posterior estimation (ROPE), a framework that overcomes model
misspecificationwithasmallreal-worldcalibrationsetofgroundtruthparameter
measurements. Weformalizethemisspecificationgapasthesolutionofanoptimal
transport problem between learned representations of real-world and simulated
observations. Assumingthepriordistributionovertheparametersofinterestis
knownandwell-specified,ourmethodoffersacontrollablebalancebetweencali-
brateduncertaintyandinformativeinferenceunderallpossiblemisspecifications
ofthesimulator. Ourempiricalresultsonfoursynthetictasksandtworeal-world
problemsdemonstratethatROPEoutperformsbaselinesandconsistentlyreturns
informativeandcalibratedcredibleintervals.
1 Introduction
Many fields of science and engineering have shifted in recent years from modeling real-world
phenomena through a few equations to relying instead on highly complex computer simulations.
Whilethisshifthasincreasedmodelversatilityandtheabilitytoexplaincomplexphenomena,ithas
alsonecessitatedthedevelopmentofnewstatisticalinferencemethods. Inparticular,state-of-the-art
simulation-basedinference(SBI,Cranmeretal.,2020)algorithmsleverageneuralnetworkstolearn
surrogatemodelsofthelikelihood(Papamakariosetal.,2019),likelihoodratio(Hermansetal.,2020),
orposteriordistribution(PapamakariosandMurray,2016),fromwhichonecanextractconfidence
orcredibleintervalsovertheparametersofinterestgivenanobservation. WhileSBIhasproven
helpfulwhenthesimulatorisafaithfuldescriptionofthestudiedphenomenon,e.g.,forscientific
applications(Delaunoyetal.,2020;Brehmer,2021;Lückmann,2022;Linhartetal.,2022;Hashemi
etal.,2022;Tolleyetal.,2023;Avecillaetal.,2022),recentworkhashighlightedtheunreliabilityof
SBImethodsundermodelmisspecification(Cannonetal.,2022;Rodriguesetal.,2022).
MisspecificationinSBI. Amodelisasimplifieddescriptionofareal-worldphenomenonthat
allowsreasoningaboutitsproperties. Thus,modelmisspecificationarisesinacontextwherethe
validity of simplifying assumptions cannot be totally verified. In the Bayesian inference litera-
ture (Walker, 2013), a statistical model p(x | θ) that relates a parameter of interest θ ∈ Θ to a
s
conditional distribution oversimulated observations x is said to bemisspecified if the true data
s
generatingprocessp⋆(x )oftherealobservationsx ∼ p⋆(x )doesnotfallwithinthefamilyof
o o o
∗Equalcontribution.CorrespondencetoAntoineWehenkelawehenkel@apple.com
Preprint.Underreview.
4202
yaM
41
]LM.tats[
1v91780.5042:viXraProblem setup ROPE algorithm
1 simulated
Simulated data
Real observations posterior
NSE
Reality NPE
(unknown)
Calibration set
2 fine
Physical Misspecification tuning OT solution
parameters
Test set
NSE
Inference
Simulator pipeline
Simulated observations
Figure1: (left)Problemsetup: weconsiderareal-worldprocesswhichdependsonsomephysical
parametersθ. Givenrealobservationsx oftheprocess,ourgoalistoprovideuncertaintyquan-
o
tificationontheunderlyingparametersθ. Tohelpus,wehaveaccesstoamisspecifiedsimulator
thattakesparametersθasinputandproducessimulatedobservationsx . (right)Avisualizationof
s
ROPE.Thetrainingconsistsoftwosteps: (1)Weapproximatetheposteriorgiventhesimulateddata
usingNPE,resultingintheNSEh . (2)Usingthecalibrationset,wefine-tuneh intog using
ω⋆ ω⋆ φ⋆
theobjective(7). Attesttime,wesolvetheOTproblembetweentherepresentations{h ω⋆(xj s)}N j=s
1
and{g φ⋆(xi o)}N i=o 1,resultinginourestimatedposterior(6): theaverageofsimulations’posteriors
weightedbytheOTsolutionP⋆. SeeAlgorithm1formoredetails.
distributionsdefinedbythemodel,i.e. ∄θ ∈Θ:p⋆(x )=p(x |θ). Basedonthisdefinition,Ward
o o
etal.(2022);Huangetal.(2023)haveinvestigatedsolutionstoimprovetherobustnessofexistingSBI
algorithmstomisspecifiedmodels. Here,wedepartslightlyfromthisdefinitionofmisspecification
andinsteadtailorittoBayesianuncertaintyquantification,inparticular,thepredictionofcredible
regionsontheparameterθgivenanobservationx . Weassumereal-worlddataaresampledi.i.d.
o
fromthedistributionp⋆(x ):=(cid:82) p(θ)p⋆(x |θ)dθ,wherep(θ)isaknownpriorovertheparameter
o o
weaimtoinferandp⋆(x |θ)isanunknownprocessapproximatedbyasimulatorthatgenerates
o
simulations x ∼ p(x | θ) given a parameter θ. We say the simulator is misspecified if there
s s
existsatleastanobservationx forwhichthesimulatorposteriorp(θ | x ) ̸= p⋆(θ | x ),where
o o o
p⋆(θ |x ):= p(θ)p⋆(xo|θ). Notably,ourdefinitioncanbeinterpretedasageneralizeddefinitionof
o p⋆(xo)
miss-calibrationandencompassesthedefinitionofmisspecificationusedintheBayesianinference
literature as a special case. While such miss-calibration often arises in practical settings where
the simulator does not faithfully model some aspects of the true generating process, current SBI
algorithmsandtheirrobustversionmayfaildrastically.
AddressingMisspecificationwithaCalibrationSet Inthiswork,weaddressmodelmisspecifi-
cationthroughacalibrationsetconsistingofonlyafewpairsofreal-worldobservationsandtheir
correspondingground-truthlabels. Atfirstglance,havingaccesstoacalibrationsetmayseemlikean
assumptionthatlimitstheapplicabilityofsuchanapproach. Onthecontrary,wearguethatforany
real-worldtaskwheretheoutputoftheinferenceisitselftobetrusted,e.g.,tomonitoraparameterin
acriticalprocess,wewilllikelyhaveaccesstoacalibrationsetasobtained,forexample,througha
morecostlyprocedureormeasurementdevice. Indeed,unlesswecanconfidentlyplaceadditional
assumptionsontheformofthemisspecification–adifficulttask,sinceitisbydefinitionunknown–the
onlywaytovalidatetheinferencedirectlyisthroughavalidationsetconsistingofreal-worldobser-
vationsandtheirground-truthlabels. Ifsuchasetexists,wecantakeafewobservationstoformour
calibrationset.
OurContributions. Weintroducerobustposteriorestimation(ROPE),analgorithmthatovercomes
modelmisspecificationtoprovideaccurateuncertaintyquantificationfortheparametersofstochastic
and non-differentiable simulators. To achieve this, ROPE relies on (1) a correctly defined prior,
and(2)havingaccesstoasmall,real-worldcalibrationsetofpairedparametersandobservations.
ThealgorithmextendsneuralposteriorestimationPapamakariosandMurray(2016)withoptimal
transport(OT,Peyréetal.,2017;Villanietal.,2009)tomodelthemisspecificationasanOTcoupling
betweensimulatedandreal-worlddata. Weevaluatetheperformanceofthealgorithmonexisting
2benchmarksfromtheSBIliterature,andintroducefournewbenchmarks,ofwhichtwoaresynthetic
andtwocomefromrealphysicalsystemsforwhichbothlabeleddataandsimulatorsareavailable.
Tothebestof ourknowledge, thelatter constitute thefirstreal-world benchmarksforSBIunder
misspecifiedmodels. Wealsoperformexperimentstoexploretheeffectdifferentcalibrationsetsizes
haveontheperformanceofthealgorithm,togetherwithablationstudiestounderstandtheimpactof
eachofitscomponents.
2 Background&Notations
Inthissection,weprovideashortreviewofSBIandOT,asourmethodisattheintersectionofthese
twofields. Westartwithsomemorefundamentaldefinitions. Weconsiderasimulator,implemented
asacomputerprogramS :RK ×[0,1]→RD,thattakesinphysicalparametersθ ∈Θ⊆RK and
arandomseedε ∈ [0,1]togeneratemeasurementsx ∈ X ⊆ RD. Thesimulatorisasimplified
s s
version of a real and unknown generative process p⋆(x ) that produces real-world observations
o
x ∈X ⊆RD. Weassumethisprocessdependsonparameterswiththesamephysicalmeaningas
o o
theonesofthesimulatorandthususethesamenotationθ. Ourgoalistoestimateawell-calibrated
andinformativeposteriordistributionp(θ |xi)foreachobservationinthetestsetxi ∈D,which
o o
reduces uncertainty compared to the prior distribution. As a remark, the most informative and
calibratedposterioristheBayesianposteriorp⋆(θ |x )thatcorrespondstotruegenerativeprocess
o
p⋆(x ):=(cid:82) p⋆(x |θ)p(θ)dθ. Toachieveourgoal,wehaveaccessto(a)themisspecifiedsimulator
o o
S thatembedsdomainknowledgeandapproximatesp⋆(x |θ),(b)asmallcalibrationsetoflabeled
o
real-worldobservationsC :={(θi,xi)}Nc ,whichenablesdata-drivencorrectionofthesimulator’s
o i=1
misspecification,and(c)atestsetD :={xi}No ofreal-worldobservationsarisingfromp⋆(x )for
o i=1 o
whichwewanttoestimatetheposterior.
2.1 Simulation-basedInference(SBI)
Applyingstatisticalinferencetosimulatorsischallengedbytheabsenceofatractablelikelihood
function(Cranmeretal.,2020). Asasolution,SBIalgorithmsleveragemodernmachinelearning
methodstotackleinferenceinthislikelihood-freesetting(Lueckmannetal.,2021;Delaunoyetal.,
2021;Glöckleretal.,2022). AmongSBIalgorithms,neuralposteriorestimationNPE(Papamakarios
andMurray,2016;Lueckmannetal.,2017)isabroadlyapplicablemethodthattrainsaconditional
densityestimatorofp(θ |x )fromadatasetofparameter-simulationpairs. Inthispaper,wefocus
s
onmakingNPErobusttomodelmisspecification.
NPEusuallyparametrizestheposteriorwithaneuralconditionaldensityestimator(NCDE),which
iscomposedof(1)aneuralstatisticestimator(NSE),denotedbyh : X → Rl,thatcompresses
ω s
observationsintol-dimensionalrepresentationsand,(2)anormalizingflow(NF,Papamakariosetal.,
2021;TabakandVanden-Eijnden,2010)thatparameterizestheposteriordensityasp (θ |h (x )).
ϕ ω s
Theparametersϕandω oftheNCDEaretrainedwithstochasticgradientascentontheexpected
log-posteriorprobability,solvingthefollowingoptimizationproblem
ϕ⋆,ω⋆ =argmaxE [logp (θ |h (S(θ,ε)))], (1)
θ∼p(θ) ϕ ω
ϕ,ω
ε∼U[0,1]
wherep(θ)denotesapriordistributionovertheparametersθ.
UndertheassumptionthattheclassoffunctionsrepresentedbytheNCDEcontainsthetrueposterior,
solving(1)leadstoaperfectsurrogatep ϕ⋆(θ |h ω⋆(x s))ofthetrueposteriorp(θ |x s). Inthatcase,
θ ⊥x s |h ω⋆(x s),thatis,theNSEh ω⋆ isasufficientstatisticofx sfortheparameterθ. Inpractice,
weapproachperfecttrainingbygeneratingasufficientlylargenumberofpairs(θ,x )anddoinga
s
searchontheNCDE’sarchitectureandtraininghyperparameters. AswewillseeinSection3,NPE
andthesufficiencyoftheNSEareattherootofouralgorithm. Tosimplifynotation,wedenotethe
NCDElearnedwithNPEasp˜(θ |x )
s
2.2 OptimalTransport(OT)
AsdetailedinSection3,ouralgorithmmodelsthemisspecificationbetweensimulationsandreal-
worldobservationsasanOTcoupling.Inparticular,letp(x )andp(x )betwocontinuousprobability
o s
measuresinX andX ,respectively,andconsideracostfunctionc:X ×X →Rthatassignsa
o s o s
3costtoeachpair(x ,x )∈X ×X . Then,wewishtofindtheOTcoupling
o s o s
π⋆ =argminE [c(x ,x )], (2)
π o s
π∈Π
whereΠisthesetofjointprobabilitymeasuresonX ×X whosemarginalsarep(x )andp(x ).
o s o s
In our setting, we only have access to a limited number N of real-world observations {xi}No ,
which we assume result from an unknown generative p⋆(xo ) = (cid:82) p⋆(x | θ)p(θ)dθ. Thuo s,i= w1 e
o o
solvethediscreteandentropy-regularizedversionof(2). Namely,givenaset{xj}Ns ofsimulated
s j=1
observations,wesearchforthedoublystochastictransportmatrixP⋆,inthepolytopeoftransport
couplingsB:={P ∈RNo×Ns :P1 =1 /N ,PT1 =1 /N },thatsolves
+ Ns No o No Ns s
P⋆
=argminN (cid:88)o×Ns
P (cid:0) c(xi,xj)+γlogP (cid:1) , (3)
ij o s ij
P∈B
ij
where γ > 0 can be seen as a hyperparameter that encourages entropic transport matrices. The
entropy-regularizedtransportproblemcanbesolvedwiththeSinkhornalgorithm(Cuturi,2013),
whichisafixed-pointiterationalgorithmwithanefficientGPUimplementation. Inourexperiments,
werelyontheOTT(Cuturietal.,2022)implementationofSinkhorn,whichreturnsthetransport
couplingP⋆giventhecostmatrixC :C =c(xi,xj)andtheentropicregularizationfactorγ.
ij o s
3 ModelingMisspecificationwithOT
Weapproachtheproblemofmisspecificationasamodelingtaskratherthanseeingitasanissueof
theinferencealgorithm. Asourmainmodelingassumption,weassumethat
x ⊥θ |x , (4)
o s
thatis,giventhesimulatedobservationsx ,therealobservationsx containnoadditionalinformation
s o
abouttheparametersθ.
Whilethisassumptionallowsustoexpressthemisspecificationindependentlyfromθ,itmightbe
violatedinpractice,asisthecaseforthetworealphysicalsystemswestudyinsection4. Duetothe
constructionofouralgorithm,violationsdonotpreventobtainingcalibratedandinformativeposterior
distributions. However,theassumptionmaylimituncertaintyreductionbypreventingourmethod
fromexploitinganysignalintherealdatabeyondwhatisspecifiedbythesimulator. Thiscanbea
limitingfactorforhighlymisspecifiedsimulators,butifthesimulatorencodesphenomenathatthe
practitionerbelievesareinvariantacrossdifferentapplicationenvironments,theassumptioncanalso
preventshortcutlearningfromthecalibrationdataandbenefitthegeneralizationofthemethod. In
AppendixA,weevaluatethemethodonrealout-of-distributiondataanddemonstratethisproperty.
TheOTcouplingin(2)fulfillsthedefinitionofajointdistributioninX ×X ,andtogetherwithour
o s
modellingassumption(4),wecanexpresstheposteriordistributionforreal-worldobservationsas
(cid:90)
p(θ |x )= p(θ |x )π⋆(x |x )dx , (5)
o s s o s
wherethesimulationposteriorp(θ |x )canbeapproximatedarbitrarilywellwithNPE(Papamakar-
s
iosandMurray,2016),asNFsareuniversaldensityestimatorsofcontinuousdistributions(Wehenkel
andLouppe,2019).
Motivatedbythefactorizationin(5),ouralgorithmcomputesatransportmatrixP⋆betweenthetest
setDandaset{xj}Ns ofN simulationsgeneratedbyrunningthesimulatoronparametersfrom
s j=1 s
thepriorθj ∼p(θ). Thus,approximating(5),weestimatetheposteriorforreal-worldobservations
asamixtureofposteriorsp˜obtainedwithNPE,thatis,
(cid:88)Ns
p˜(θ |xi):= α p˜(θ |xj), whereα =N P⋆. (6)
o ij s ij o ij
j
AninterestingpropertyofdefiningROPEasin(6)isthat,bydesign,themarginalposteriordistribution
overthetestset,i.e.,p˜(θ):=(cid:82) p˜(θ |x )p⋆(x )dx ,convergestothepriordistributionasthenumber
o o o
of simulated observations N approaches infinity, as expected from a well-estimated posterior
s
distribution. Aproofandfurtherdiscussionofthisself-calibrationpropertyisgiveninAppendixB.
43.1 DefiningtheOTCostFunction
In our context, an ideal coupling would assign simulations to real-world observations generated
from the same parameter. Hence, we can express the corresponding ideal cost as c(x ,x ) =
o s
c(h (x ),h (x )),whereh andh areanysufficientstatisticsforθgivenx andx ,respectively.
o o s s o s o s
AsdiscussedinAppendixG,wecanlearnanapproximatedminimalsufficientstatistich forthe
ω⋆
simulatedobservationswithNPE.Furthermore,asthesimulatorcarriesinformationaboutthetrue
generativeprocessandthecalibrationsetistoosmalltolearnarepresentationfromreal-worlddata
only,itisreasonabletolearnasufficientstatistich fortherealobservationsbyfine-tuningh .
o ω⋆
Denotingthisnewneuralnetworkasg :X →Rl,thefine-tuningobjectivereads:
φ o
L(C):=(cid:88)Nc
|g φ(xi o)−E ε∼U[0,1][h ω⋆(cid:0) S(θi,ε)(cid:1) ]| 2, (7)
i=1
wheretheexpectationisapproximatedviaaMonte-Carloapproximation. Thetrainingofgstarts
fromtheweightsω⋆andoptimizes(7)withgradientdescent. Optimizing(7)enforces,atleastonthe
calibrationset,thatgandharecloseinL2normwhentheycorrespondtothesameparameter. Thus,
wedefinetheOTcostasc(x o,x s):=|g φ⋆(xi o)−h ω⋆(x s)|
2
,whereg istheNSEobtainedafterfine-tuning(7).Figure1depictsthemaintrainingandinference
φ⋆
stepsofROPE.
3.2 Entropy’sMagic
TheentropicregularizationofOTnotonlyenablesfastcomputationofthetransportcouplingbut
also provides an effective control mechanism to balance the calibration of the posterior with its
informativeness. Indeed,forsmallentropicregularization,theestimatedposteriorshavelowentropy
andmaybeoverconfident,astheyaresparsemixturesofafewsimulationposteriorsp˜(θ |xj). In
s
contrast,forlargevaluesofγ in(3),thecouplingmatrixbecomesuniformandthecorresponding
posteriorstendtotheprior,asp(θ | x ) ≈ 1 (cid:80)Nsp˜(θ | xj)isaMonte-Carloapproximationof
E [p˜(θ |x )]≈p(θ). Thus,theprao ctitioN ns ershj ouldoptims izethehyper-parameterγ tofindthe
p(xs) s
righttrade-offbetweencalibration,favoredbyhigherγ,andinformativeness,favoredbylowerγ,of
theestimatedposteriordistributions.
4 Experiments
Our experiments aim to (1) empirically validate the discussion in Section 3.2, and (2) illustrate
settings in which our algorithm enables uncertainty quantification under model misspecification
andsmallcalibrationdatasets. TheexperimentscomprisetwoexistingbenchmarksfromtheSBI
literature,twosyntheticbenchmarks,andtwonewbenchmarksfromrealphysicalsystemsforwhich
bothlabeleddataandsimulatorsareavailable. Tothebestofourknowledge,thelatterconstitutethe
firstreal-worldbenchmarksforSBIundermisspecifiedmodels. Altogether,thebenchmarksrepresent
varioustypesofmisspecificationandparameterandobservationspace. Webrieflydescribeeach
taskandprovideexamplesofrealvs. simulatedobservationsinFigure2. Furtherdetailsaboutthe
completesetupcanbefoundinAppendixD.
TaskA(synthetic): CS. Wereproducethecancerandstromalcelldevelopmentbenchmarkfrom
Wardetal.(2022). Thesimulatoremulatesthedevelopmentofcancerandstromalcellsina2D
environmentasafunctionofthreePoissonrateparameters(λ ,λ ,λ ). Theobservationsarevectors
c p d
composedofthenumberofcancerandstromalcellsandthemeanandmaximumdistancebetween
stromalcellsandtheirnearestcancercell. Syntheticmisspecificationisintroducedbyremoving
cancercellsthataretooclosetotheirgeneratingparent.
TaskB(synthetic):SIR. WealsousethestochasticepidemicmodelfromWardetal.(2022),which
describesepidemicdynamicsthroughtheinfectionrateβ andrecoveryrateγ. Eachobservationisa
vectorcomposedofthemean,median,andmaximumnumberofinfections,thedayofoccurrenceof
themaximumnumberofinfections,thedayatwhichhalfthetotalnumberofinfectionswasreached,
and the mean auto-correlation (lag 1) of the infections. Misspecification is a delay in weekend
infectioncounts,ofwhich5%areaddedtothecountofthefollowingMonday.
5Task A CS Task B SIR Task C Pendulum
Synthetic data Synthetic data Synthetic data
LPP LPP LPP
ACAUC ACAUC ACAUC
ours baselines ablations comparison
Legend
ROPE Prior SBI NPE J-NPE tuning-only OT-only MLP NNPE HVAE
Task D Hemodynamics Task E Light Tunnel Task F Wind Tunnel
Synthetic data Real data Real data
LPP LPP LPP
ACAUC ACAUC ACAUC
Figure2: Resultsforourmethod(ROPE)andthecompetingbaselinesonsixbenchmarktasks. For
eachtask,weshowanexampleoftherealobservations(x )andtheobservationsproducedbythe
o
misspecifiedsimulator(x ). Weshoweachmethod’sLPPandACAUCmetrics,ascomputedona
s
labeledtestsetofsize2000. Wereporttheaveragemetricsand±1std. deviationoverthreerandom
drawsofthetestsetandadditionalsourcesofrandomness. Insomeinstances,e.g.,tasksAandE,the
likelihoodofJ-NPEcanbe−∞andisnotplotted. ForreadabilityoftheLPPmetric,weusealinear
scalebetweentheSBIandthePriorandalogarithmicscaleforvaluesbelowthat.
6TaskC(synthetic): Pendulum. Thedampedpendulumisacommonbenchmarktoassesshybrid
learning algorithms (Wehenkel et al., 2022), which jointly exploit domain knowledge and real-
worlddata. Thesimulatorgeneratesthehorizontalpositionofafriction-lesspendulumgivenits
fundamentalfrequencyω ∈R+andamplitudeA∈R+. Randomnessentersthesimulatorthrough
0
arandomphaseshiftandwhitemeasurementnoise. Asmisspecified“real-world”data,wesimulate
observationsfromadampedpendulumthattakesfrictionintoaccount.
TaskD(synthetic): Hemodynamics. FollowingWehenkeletal.(2023), wedefinethetaskof
inferring the stroke volume (SV) and the left ventricular ejection time (LVET) from normalized
arterialpressurewaveforms. ThesimulatorisaPDEsolver(Melis,2017)thatproducesan8-second
time-seriesx sampledat125Hz. Assyntheticmisspecification,thesimulatorassumesallarteries
s
haveconstantlength,whereasthisparametervariesinthe“real-world”data.
Task E (real): Light Tunnel. We employ one of the light tunnel datasets from Gamella et al.
(2024). Thetunnelisanelongatedchamberwithacontrollablelightsourceatoneend,twolinear
polarizersmountedonrotatingframes,andacamera. Ourtaskconsistsofpredictingthecolorsetting
ofthelightsource((R,G,B)∈[0,255]3)andthedimmingeffectofthepolarizersα∈[0,1]from
thecapturedimages. Thesimulatortakestheparametersθ :=[R,G,B,α]andproducesanimage
consistingofahexagonroughlythesizeofthelightsource,withacolorequalto[αR,αG,αB].
Task F (real): Wind Tunnel. We employ one of the wind tunnel datasets from Gamella et al.
(2024). Thetunnelisachamberwithtwocontrollablefansthatpushairthroughit,andbarometers
thatmeasureairpressureatdifferentlocations. Ahatchcontrolstheareaofanadditionalopeningto
theoutside. Thedatasetisacollectionofpressurecurvesthatresultfromapplyingashortimpulseto
theintakefanpowerandmeasuringthechangeinairpressureinsidethetunnel. Ourinferencetask
consistsofpredictingthehatchposition,θ := H ∈ [0,45]givenapressurecurve. Asasimulator
model,weadaptthephysicalmodelgiveninGamellaetal.(2024,ModelC3,AppendixD).
Metrics WeconsidertwodifferentmetricstoassesswhetherROPEprovidesreliableanduseful
uncertaintyquantification. First,givenalabeledtestset{(θi,xi)}N ,wecomputethelog-posterior
o i=1
probability(LPP)as
N
1 (cid:88)
LPP:= logp˜(θi |xi)≈E [logp˜(x |θ)]. (8)
N o p(θ) o
i=1 p(xo|θ)
TheLPPisanempiricalestimationoftheexpectedcrossentropybetweenthetrueandestimated
posterior;thus,foraninfinitetestset,itisonlymaximizedbythetrueposterior. LPPcharacterizes
theentropyreductionontheestimationofθ achievedbyaposteriorestimatorp˜whengivenone
observation,onaverage,overthetestset. Second,theaveragecoverageAUC(ACAUC)indicatesthe
averagecalibrationofK 1Dcredibleintervalsextractedfromtheestimatedposteriors,i.e.,
1 (cid:88)K (cid:88)N (cid:90) 1
ACAUC:= α−1[θi ∈Θ (α)]dα, (9)
KN j p˜(θj|xi o)
j=1i=1 0
whereΘ (α)denotesthecredibleintervalforthejthdimensionoftheparameterθatlevelα.Its
p˜(θj|xi o)
valueispositive(negative)if,onaverageoverdifferentcrediblelevels,parameterdimensionality,and
observations,thecorrespondingcredibleintervalsareoverconfident(underconfident). TheACAUC
ofaperfectlyspecifiedpriordistributioniszero. Theintegralcanbeefficientlyapproximated,as
describedinAppendixF. Forallexperiments,wecomputetheLPPandACAUConlabeledtestset
containing2000pairs(θ,x ).
o
Baselines Asasanitycheck,wecomparetheperformanceofROPEagainstfourreferencebaselines:
thepriorp(θ),whichamountstothelowerboundontheLPPforanycalibratedposteriorestimator;
theSBIposterioronthesimulatedexampleswhich,undertheindependenceassumptionx ⊥θ |x ,
o s
isanupperboundontheLPPforROPE;(NPE)aposteriorestimatorfittedtothesimulateddataand
appliedtotherealdata;and(J-NPE)aposteriorestimatortrainedjointlyonthepooledsimulated
andrealobservations. Thelattertwobaselinesrepresentsomefirstapproachesthatapractitioner
mayconsider. Additionally,wecomparetheperformanceofROPEtoMLP,whichtrainsaneural
70 Task E Light Tunnel Task F Wind Tunnel Task E Light Tunnel
Posterior estimates Posterior estimates Entropic regularization
128 LPP
256
0
128
256
ACAUC
1
0.5
ROPE tuning-only
0 J-NPE OT-only
0 128 256 0 128 256 0 128 256
MLP HVAE
Figure3: (left)Credibleintervalsoftheposteriorestimatesatlevels65%and90%,forasingletest
samplefromthelight-tunneltask. Theblackstarsdenotethetruevalueoftheparameter. (center)
Posteriorestimatesforasingletestsamplefromthewind-tunneltask,wherethetrueparameteris
denotedbyaverticalblackline. (right)Effectofγ ontheLPPandACAUCscoresofROPEonthe
light-tunneltaskfordifferentsizesofthecalibrationset. Thevalueofγ isshownbyeachcurve. For
reference,weplotthemetricsachievedbytheSBIposteriorandpriordistributiononsimulateddata.
networkthatpredictsthemeanandlog-varianceofaGaussianposteriordistributionbymaximizing
the calibration set log-likelihood. For the CS and SIR benchmarks, we additionally run Noisy
NPE(NNPE,Wardetal.,2022),whichimprovestherobustnessofNPEbyintroducingaSpikeand
Slaberrormodelonsimulateddatastatistics. WedonotrunNNPEontasksC-F,asthiswouldbe
unfairforNNPEwhosenoisemodelisdesignedforlow-dimensionalstatistics,asopposedtothe
temporalandspatialdataconsideredintasksC-F.ExtendingNNPEtosuchsettingsisoutofthe
scopeofourexperiments. WealsorunthehybridlearningmethodHVAE(TakeishiandKalousis,
2021),whichconstitutesastrongbaselinewhenthesimulatorcanbemadedeterministic(tasksCand
E)butisnotdirectlyapplicablewhenthesimulatorisnotdifferentiable. Moredetailsabouteach
methodandtheexperimentalsetupcanbefoundinAppendixD.
4.1 Results
ROPEachievesrobustposteriorestimationforalltasks. InFigure2,wecomparetheperfor-
manceofROPE(withγ =0.5)againstbaselinesforthe6tasksweconsider. Foralltasks,evenwith
minimalcalibrationbudgets,ROPEistheonlymethodthatconsistentlyreturnswell-calibrated,or
sometimesslightlyunder-confident,posteriorestimationwhilesignificantlyreducinguncertainty
comparedtothepriordistribution. Asthesizeofthecalibrationsetincreases,weseetheadaptability
of J-NPE and MLP as their performance improves and aligns with or outperforms ROPE. This
adaptabilityisanexpectedbehaviorinIIDsettings,wherereal-worlddataeventuallyallowsfinding
theminimizerofempiricalriskamongaclassofpredictors. However,ontaskE,whereposteriorsare
complexconditionaldistributions—whoseentropyincreaseswithdarkerimagesandcontainnon-
trivialdependenciesbetweenparameters—ROPEremainsthebestapproach,evenwithacalibration
setcontainingmorethan1000examples. Asanoutlier,weobservethatNPEtrainedonsimulated
dataachievesthebestresultsfortheSIRbenchmark(TaskB),indicatingthatthemisspecification
ofthisbenchmarkisnotachallengingtestcaseforexistingSBImethods. Finally,asinterpretinga
numericalgapinLPPmetricscanbedifficult,wecomplementthesenumericalresultswithvisual
cornerplotsforthetworeal-worldexperimentsinFigure3andforalltasksinsubsectionE.1.
Ablation study. Our algorithm combines two steps with distinct roles: (1) a fine-tuning step,
which improves the generalization domain of the NSE, and (2) an OT step, aiming to model the
misspecificationasastochasticmappingbetweensimulationsandobservations. Tobetterunderstand
theirrespectivecontributiontotheperformanceofROPE,welookattwoablatedversionsofour
algorithm: tuning-onlywhichappendsthefine-tunedNSEtotheNFtrainedonsimulateddatap
ϕ⋆
and directly applies it to the real observations without an OT step; and OT-only, which directly
8performs OT with L2-norm in the original NSE space c(x o,x s) = |h ω⋆(x o) − h ω⋆(x s)| 2. In
Figure2,weobservethattuning-only’sresultarepoorexceptforTaskB,forwhichmisspecification
isnegligible. Incontrast,fortasksA,D,andF,OT-onlyexhibitsperformanceonparwithROPE.
Nevertheless, ROPE can significantly outperform OT-only, such as in tasks C and E where the
misspecificationissignificant. WeconcludethattheOTstepiscrucialandfine-tuningissometimes
necessary—werecommendthatpractitionersfirstevaluateOT-only’sperformanceandoptimizethe
valueofγ beforeusingasubsetofthereal-worlddataforfine-tuning.
Effectofentropicregularization. InFigure3,westudytheeffectofentropicregularizationby
varyingtheregularizationparameterγ. Forallvaluesofγ,excludingγ ≥5,weobservethatboth
LPPandcalibrationconsistentlyimprovewiththecalibrationsetsize. Forlargevaluesofγ, the
entropicregularizationdominatesandpushestowardauniformmapping,resultinginposteriorsthat
approximatethepriordistribution,ashighlightedinsubsection3.2. Inthesecases,theLPPbarely
changes with the calibration set size. These empirical results are consistent with the theoretical
discussion in subsection 3.2. Furthermore, it is notable that the value of γ does not excessively
influenceROPE’sperformance. Asarecommendationforpractitioners,ourempiricalevaluation
suggeststhatvaluesbetween0.1and1providewell-calibratedandprecisecredibleintervals.
5 Discussion
WhileourexperimentshavedemonstratedthatROPEenablesreliableuncertaintyquantificationfrom
misspecifiedsimulator,ourmethodandsetuphascertainshortcomingsthatwenowdiscuss.
Priormisspecification. Whiletheimportanceofcorrectpriorspecificationvanishesasthenumber
ofobservationsorsharpnessofthelikelihoodfunctionincreasesforwell-specifiedlikelihoodmodels,
theimplicationsofviolatingitmayseverelydamageROPE’sperformancebecauseoftheOTmatching
betweensimulatedandreal-worldobservations. Asapossiblesolution,unbalancedOT(Séjourné
etal.,2019;Fatrasetal.,2021)relaxesthemassconservationconstraintandcouldbeanelegant
solutiontocoupleasubsetofthesimulationstothereal-worlddata.
Curseofdimensionality. ThedimensionalityofθmayimpacttwocriticalpartsofROPE.First,
witheachadditionalparameterθ[K+1],theNSEmustencodeuptoKdependencebetweenθ[K+1]
andotherdimensionsθ[1:K]givenx . Whilegeneratingmoresimulationscanaddressthecurseof
o
dimensionalityinthesimulationspace,fine-tuningonasmallcalibrationmaynotsufficeanymore
tocopewithmisspecification. Second,thedimensionalityofthemanifoldonwhichthesimulated
andreal-worldobservationsareprojectedwiththeNSEgrowsandfindingameaningfulmatching
betweenthetwopopulationsmayrequirelargersamplesizes.Apotentialsolutionistofocusmarginal
or2Dposteriordistributionsandignorehigher-dimensionaldependenciesinp(θ |x ). Nevertheless,
o
extendingROPEtosuchsettingscertainlyopensnewquestions,e.g.,onthedevelopmentofbetter
fine-tuningstrategiesthatcanleveragepartialcalibrationsetswherelabelscanbeincomplete.
Otherextensions. Similarlytoincompletelabels,certainapplicationshaveonlyaccesstonoisy
labels measured with a well-modeled, but noisy, measurement process. Further developing the
fine-tuningstagetoexploitsuchnoisylabelswouldbenecessarytomakeanapproachsimilarto
ROPEapplicable. Asanotherdirection,leveraginginductivebiasembeddedintotheneuralnetwork
architectureofneuralOTandtheabilitytobettercopewithlargetestsetappearsasapromising
directiontoamortizethemappingbetweensimulationandreal-worlddata. Webelievefollowing
ROPE’sstrategyofmodelingmisspecificationinSBIasanOTcouplingopensupseveraldirections
tocopewithmorespecificproblemsetup.
Conclusion Inthispaper, wehaveargued thatmodelmisspecificationinsimulation-based isa
modeling issue that requires real-world labeled data to be solved. Under this premise, we have
introducedROPE,analgorithmthatjointlyexploitsasmallcalibrationsetandoptimaltransport
to extend neural posterior estimation for misspecified simulators. Our experiments on diverse
benchmarks, demonstrate ROPE’s ability to estimate well-calibrated and informative posterior
distributionsforvarioussimulatorsandreal-worldexamples. Asaconclusion,ROPEisasimple
methodthatpractitionerscanusetopredictcalibratedposteriorovertheparameterofmisspecified
simulatoronreal-worlddata.
9BroaderImpact Thispaperpresentsaframeworkandanalgorithmtoaddressmodelmisspecifi-
cationinsimulation-basedinference(SBI).SBIispredominantlyappliedinscientificfieldswhere
complexsimulatorsofphysicalphenomenaareavailable,suchasastronomy,particlephysics,or
climatemodeling. Apriori,thiscircumscribestheapplicationofouralgorithmtohighlyspecialized
scientificdomainsinthenaturalsciences,precludingissuessuchasfairnessorprivacy. However,its
applicationtothescientificdomainisnotexemptfromsocietalorethicalimplications,particularly
whencomputersimulationsmayinformresearchorpolicydecisions. Inthisregard,wefindsome
propertiesofthealgorithmparticularlypromising,suchasuncertaintyquantificationandthelimita-
tionofnotdrawingconclusionsbeyondthegivenexpertmodel. However,muchmoreworkisneeded
todeeplyunderstandthereliabilityofthesepropertiesandhowtheyareaffectedbyviolationsofthe
coreassumptions,suchasawell-specifiedprior. Suchworkshouldprecedeanysortofover-sellingto
practitionersaboutthebenefitsofthealgorithm. Rather,weseeourworkasanacademiccontribution
towardsamorebroadandsuccessfulapplicationofSBItechniques;successinthisendeavor,asfor
theestablishmentofanyscientifictool,willrequireaniterativedialoguebetweenthescientistswho
developthemethodologyandthosewhouseit.
References
Avecilla,G.,Chuong,J.N.,Li,F.,Sherlock,G.,Gresham,D.,andRam,Y.(2022). Neuralnetworks
enableefficientandaccuratesimulation-basedinferenceofevolutionaryparametersfromadaptation
dynamics. PLoSbiology,20(5):e3001633.
Brehmer,J.(2021).Simulation-basedinferenceinparticlephysics.NatureReviewsPhysics,3(5):305–
305.
Cannon,P.,Ward,D.,andSchmon,S.M.(2022). Investigatingtheimpactofmodelmisspecification
inneuralsimulation-basedinference. arXivpreprintarXiv:2209.01845.
Collett,E.(2005). Fieldguidetopolarization. Internationalsocietyforopticsandphotonics.
Cranmer, K., Brehmer, J., and Louppe, G. (2020). The frontier of simulation-based inference.
ProceedingsoftheNationalAcademyofSciences,117(48):30055–30062.
Cuturi,M.(2013). Sinkhorndistances: Lightspeedcomputationofoptimaltransport. Advancesin
neuralinformationprocessingsystems,26.
Cuturi,M.,Meng-Papaxanthos,L.,Tian,Y.,Bunne,C.,Davis,G.,andTeboul,O.(2022). Optimal
transporttools(ott): Ajaxtoolboxforallthingswasserstein. arXivpreprintarXiv:2201.12324.
Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., and Louppe, G. (2021). Towards reliable
simulation-basedinferencewithbalancedneuralratioestimation. InAdvancesinNeuralInforma-
tionProcessingSystems2022.
Delaunoy,A.,Wehenkel,A.,Hinderer,T.,Nissanke,S.,Weniger,C.,Williamson,A.,andLouppe,
G.(2020). Lightning-fastgravitationalwaveparameterinferencethroughneuralamortization.
In Machine Learning and the Physical Sciences. Workshop at the 34th Conference on Neural
InformationProcessingSystems(NeurIPS).
Fatras,K.,Séjourné,T.,Flamary,R.,andCourty,N.(2021). Unbalancedminibatchoptimaltransport;
applications to domain adaptation. In International Conference on Machine Learning, pages
3186–3197.PMLR.
Gamella,J.L.,Bühlmann,P.,andPeters,J.(2024). Thecausalchambers: Realphysicalsystemsasa
testbedforAImethodology. arXivpreprintarXiv:2404.11341.
Glöckler, M., Deistler, M., and Macke, J. H. (2022). Variational methods for simulation-based
inference. InInternationalConferenceonLearningRepresentations2022.
Hashemi,M.,Vattikonda,A.N.,Jha,J.,Sip,V.,Woodman,M.M.,Bartolomei,F.,andJirsa,V.K.
(2022). Simulation-basedinferenceforwhole-brainnetworkmodelingofepilepsyusingdeep
neuraldensityestimators. medRxiv,pages2022–06.
10Hermans,J.,Begy,V.,andLouppe,G.(2020). Likelihood-freemcmcwithamortizedapproximate
ratioestimators. InInternationalconferenceonmachinelearning,pages4239–4248.PMLR.
Huang,D.,Bharti,A.,Souza,A.,Acerbi,L.,andKaski,S.(2023). Learningrobuststatisticsfor
simulation-basedinferenceundermodelmisspecification. arXivpreprintarXiv:2305.15871.
Linhart, J.,Rodrigues, P.L.C.,Moreau, T., Louppe,G., andGramfort, A.(2022). Neuralposte-
riorestimationofhierarchicalmodelsinneuroscience. InGRETSI2022-XXVIIIèmeColloque
FrancophonedeTraitementduSignaletdesImages.
Lückmann,J.-M.(2022). Simulation-BasedInferenceforNeuroscienceandBeyond. PhDthesis,
UniversitätTübingen.
Lueckmann,J.-M.,Boelts,J.,Greenberg,D.,Goncalves,P.,andMacke,J.(2021). Benchmarking
simulation-basedinference. InInternationalConferenceonArtificialIntelligenceandStatistics,
pages343–351.PMLR.
Lueckmann,J.-M.,Goncalves,P.J.,Bassetto,G.,Öcal,K.,Nonnenmacher,M.,andMacke,J.H.
(2017). Flexiblestatisticalinferenceformechanisticmodelsofneuraldynamics. Advancesin
neuralinformationprocessingsystems,30.
Melis,A.(2017). Gaussianprocessemulatorsfor1dvascularmodels.
Papamakarios,G.andMurray,I.(2016). Fastε-freeinferenceofsimulationmodelswithbayesian
conditionaldensityestimation. Advancesinneuralinformationprocessingsystems,29.
Papamakarios,G.,Nalisnick,E.,Rezende,D.J.,Mohamed,S.,andLakshminarayanan,B.(2021).
Normalizingflowsforprobabilisticmodelingandinference. TheJournalofMachineLearning
Research,22(1):2617–2680.
Papamakarios,G.,Sterratt,D.,andMurray,I.(2019). Sequentialneurallikelihood: Fastlikelihood-
free inference with autoregressive flows. In The 22nd International Conference on Artificial
IntelligenceandStatistics,pages837–848.PMLR.
Peyré, G., Cuturi, M., et al. (2017). Computational optimal transport. Center for Research in
EconomicsandStatisticsWorkingPapers.
Rodrigues,P.L.,Statify,I.,Arbel,M.N.,Thoth,I.,Forbes,F.,andMairal,J.(2022). Investigating
modelmisspecificationinsimulation-basedinference.
Séjourné,T.,Feydy,J.,Vialard,F.-X.,Trouvé,A.,andPeyré,G.(2019). Sinkhorndivergencesfor
unbalancedoptimaltransport. arXivpreprintarXiv:1910.12958.
Tabak,E.G.andVanden-Eijnden,E.(2010). Densityestimationbydualascentofthelog-likelihood.
CommunicationsinMathematicalSciences,8(1):217–233.
Takeishi,N.andKalousis,A.(2021). Physics-integratedvariationalautoencodersforrobustand
interpretablegenerativemodeling. AdvancesinNeuralInformationProcessingSystems,34:14809–
14821.
Tolley,N.,Rodrigues,P.L.,Gramfort,A.,andJones,S.R.(2023). Methodsandconsiderationsfor
estimatingparametersinbiophysicallydetailedneuralmodelswithsimulationbasedinference.
bioRxiv,pages2023–04.
Villani,C.etal.(2009). Optimaltransport: oldandnew,volume338. Springer.
Walker,S.G.(2013). Bayesianinferencewithmisspecifiedmodels. Journalofstatisticalplanning
andinference,143(10):1621–1633.
Ward,D.,Cannon,P.,Beaumont,M.,Fasiolo,M.,andSchmon,S.(2022). Robustneuralposterior
estimationandstatisticalmodelcriticism. AdvancesinNeuralInformationProcessingSystems,
35:33845–33859.
Wehenkel,A.,Behrmann,J.,Hsu,H.,Sapiro,G.,Louppe,G.,andJacobsen,J.-H.(2022). Robust
hybridlearningwithexpertaugmentation. TransactiononMachineLearningResearch.
11Wehenkel,A.,Behrmann,J.,Miller,A.C.,Sapiro,G.,Sener,O.,Cuturi,M.,andJacobsen,J.-H.
(2023). Simulation-basedinferenceforcardiovascularmodels. arXivpreprintarXiv:2307.13918.
Wehenkel,A.andLouppe,G.(2019). Unconstrainedmonotonicneuralnetworks. Advancesinneural
informationprocessingsystems,32.
12A Robustnesstodistributionshifts
Task E Light Tunnel
Out-of-distribution performance Training distribution Target distribution
RROOPPEE
Training distribution Target distribution J-NPE
flipped images NPE
tuning-only
OT-only
LPP MLP
ACAUC
Figure4: Out-of-distributionperformanceofROPEandsomebaselines. WetrainROPEandother
baselinesonthesamelight-tunneldataasintaskE(trainingdistribution),butapplyittotestsets
originatingfromatargetdistributionwherethereal-worldimagesareflippedvertically. Wecompare
theperformanceontestsetsfrombothdistributions,showingtheLPPandACAUCscoresforeach
method. Forcomparison,intherightplotweshowagaintheLPPcurve(lightgray,dotted)attained
byROPEunderthetrainingdistribution. TheperformanceofROPEisbarelyaffectedasitcannot
exploitanysignalintherealimages(x )beyondwhatisencodedinthesimulator,andthesimulator
o
output (x ) is invariant to the transformation we consider. Because NPE is not trained on real
s
observations,itsperformance,althoughpoor,alsoremainsvirtuallyunchanged. Ontheotherhand,
theperformanceofMLPandJ-NPEdropsinthetargetdistribution,asthesemethodsarenotlimited
inwhatinformationtheycanexploitfromtherealobservationsonwhichtheyaretrained,potentially
learningshortcutsthatarenotpresentinthetargetdistribution. Thisresultsdemonstratethatifthe
simulatorembedstherightinvariances,ourmodelingassumptionx ⊥θ |x canbefavorableto
o s
out-of-distributiongeneralization.
B Self-calibrationProperty
WesayROPEisself-calibratingbecause, bydesign, theposteriordistributionmarginalizedover
observationstendstothepriorasthenumberofsimulationincreases,thatis,
(cid:90)
p˜(θ |x )p(x )=p(θ). (10)
o o
Thispropertyisalsocalledmarginalcalibrationasitisanecessaryconditionforaposteriorestimation
method to be calibrated. Considering NPE, p˜(θ | x ), is marginally calibrated and observations
s
x aregeneratedfromtheassumedprior, thatissampledfromanunknowndistributionp(x ) =
o o
(cid:82)
p(x |θ)p(θ),wecanshowROPEismarginallycalibrated. Indeed,consideringtheMonte-Carlo
o
13
snoitavresbo
laer
snoitavresbo
.misapproximationofthemarginalizedposteriordistributionoverthetestsetD :={x }No ,wehave,
o o i=1
(cid:90)
1
(cid:88)No
p˜(θ |x )p(x )≈ p˜(θ |x ) (11)
o o o
N
o
i=1
1
(cid:88)No (cid:88)Ns
= N P⋆p˜(θ |xj) (12)
N o ij s
o
i=1j=1
(cid:88)Ns (cid:34) (cid:88)No (cid:35)
= P⋆ p˜(θ |xj) (13)
ij s
j=1 i=1
1
(cid:88)Ns
= p˜(θ |xj) (14)
N s
s
j=1
≈p(θ), (15)
whereweusethedefinitionofthetransportmatrixtoget(cid:80)No P⋆ = 1 . Thelastapproximation
i=1 ij Ns
tendstobeexactasthenumberofsimulationsincreases,iftheNPEismarginallycalibrated.
14C ROPEAlgorithm
Algorithm1PosteriorInferenceusingRobustNeuralPosteriorEstimation(ROPE)
Input: Simulator S(θ,ε), prior distribution p(θ), calibration set C = {(xi,θi)}Nc , test set
o i=1
D ={xi}No
o i=1
Output: p˜(θ |x )∀xi ∈D
o o
Step1: NeuralPosteriorEstimation(NPE)
Trainneuralnetworkh andconditionalnormalizingflowp(θ |·)usingNPE:
ω
p˜,ω⋆ =argmaxE [logp(θ |h (S(θ,ϵ)))]
θ∼π(θ) ω
p,ω
ε∼U[0,1]
Step2: Fine-tunesufficientstatisticsh ontheCalibrationSet
ω⋆
g
ψ
:=COPY(h ω⋆)
C ,C =RandomSplit(C,1)
train val 5
best =∞
val
forN do
iter (cid:104) (cid:105)
ψ ←ψ−α∇
ψ
(cid:80) (θ,xo)∈Ctrain|g ψ(x o)−E ε[h ω⋆(S(θ,ε))]|
2
cur
val
=(cid:80) (θ,xo)∈Cval|g ψ(x o)−E ε[h ω⋆(S(θ,ε))]|
2
ifcur <best then
val val
best =cur
val val
ψ⋆ =ψ
endif
endfor
Step3: GenerateSimulationsforTestSet(N =N )
s o
S ={xj}Ns ,
s j=1
wherexj ∼S(θj,ε) θj ∼π(θ) ε∼U[0,1]
s
Step4: Entropic-regularizedOT
P⋆
=argminN (cid:88)o×Ns
P ij(cid:0) |f ω⋆(xj s)−g ψ⋆(xi o)|+γlogP ij(cid:1)
P∈B
ij
Step5: ComputePosteriorDistributions
p(θ|xi
o):=(cid:88)Ns
P i⋆ jp˜(cid:0) θ |h ω⋆(xj s)(cid:1)
j=1
Returnp˜(θ|xi) ∀xi ∈D
o o
D ExperimentalSetup
Inthissection,weprovidemoredetailsonourexperiments. Forcompleteness,weprovidedetailson
theneuralarchitecturesandtraininghyperparameters. However,weencouragethereaderinterested
inreproducingourexperimentstoexamineourcodedirectly(alinktothecodewillbemadeavailable
inthepublicversionofthepaper).
Forallmethodstrainingoncalibrationsetwekeepalwayskeep20%ofthecalibrationtomonitor
validationperformanceandweselectthebestmodelbasedonthismetric.
For the MLP we use the same architecture as the NSE for all our experiments and optimize its
parametersonthecalibrationsetwithAdamandalearningrateequalto0.0003,weselectthebest
modelbasedontheLPPattributedtothevalidationsubsetofthecalibrationset.
15D.1 TaskA:CS&TaskB:SIR
WereferthereadertoWardetal.(2022)formoredetailsaboutthesimulatorandpriordistribution.
Weusetheexactsamesettingastheirs.
Neuralarchitecture&TrainingHyperparameters
For all methods we use the same backbone MLP as the NSE with ReLU activations and layers
composedof[4K,16K,16K,12K,3K]neurons, whereK isthedimensionalityofθ. TheNFis
a 1-step UMNN-MAF (Wehenkel and Louppe, 2019) with [100,100,100] neurons for both the
autoregressiveconditionerandnormalizer. ForNNPE,wetraintheUMNN-MAFonsimulations
polutedbySpikeandSlaberrors. WetrainmodelswithAdamandalearningrateequalto0.0005
andallotherparameterssettodefault. WeoptimizetheSBImodelfor106gradientstepsandselect
thebestmodelonrandomvalidationsetscontaining105simulations.
D.2 TaskC:Pendulum
Description
Thefirsttaskisinspiredfromthedampedpendulumbenchmarkcommonlyusedtoassesshybrid
learning algorithms. Given a 2D physical parameter θ := [ω ,A], where ω ∈ R+ denotes the
0 0
fundamental frequency and A ∈ R+ the amplitude of a friction-less pendulum, the simulator
generatesthehorizontalpositionofthependulumat200discretetimesduringuniformlysampledin
a10secondsintervalas
x :=[θ(t=0),...,θ(t=10s)]∈R200
s
whereθ(t)=Acos(ω t+φ) φ∼U(−π,π). (16)
0
Therelationshipbetweentheparametersandthesimulationisthusstochasticasφaccountsforan
unknownphaseshiftwhenthemeasurementsstart. Wegeneratereal-worldobservationssynthetically
byreplacingθ(t)from(16)by
θ˜(t)=eαtAcos(ω t+φ) φ∼U[−π,π] α∼U[0,1],
0
whereαrepresentstheeffectoffriction.WealsoaddGaussiannoiseonbothsimulatedandreal-world
datatorepresenttheinaccuracyofasensormeasuringthependulum’sposition. Thepriordistribution
isaproductofuniformdistribution,p(θ :=[ω ,A])=U[0,3]×U[0.5,10].
0
Neuralarchitecture&TrainingHyperparameters
NeuralPosteriorEstimator. TheNSEisa1Dconvolutionalneuralnetwork,withthearchitecture
describedinAlgorithm2.
The NCDE is a one-step discrete normalizing flow with an autoregressive conditioner and a
UMNN(WehenkelandLouppe,2019)asthenormalizer. TheautoregressiveconditionerisaMADE
withReLUactivationand3layersof100neuronsthatoutputa10dimensionalvectortotheUMNN.
TheUMNNhasanintegrandnetwith3layersof100neuronswithReLUactivations. Fortrainingthe
NPE,weuseabatchsizeof100andalearningfactorequalto1e-4. NPEistraineduntilconvergence.
OtherparametersaresettodefaultvaluesandshouldmarginallyimpacttheNPEobtained.
ROPE NSE. We have selected the best NPE based on the validation set with 10000 examples
generatedwiththesimulator. TheNPEisfixedtoonebest-of-allmodel. Wefine-tunetheNCDE
withalearningrateequalto1e-5for5000gradientstepson80%thefullcalibrationset. Weusea
1-sampleMonteCarloestimateoftheexpectationin(7).
J-NPE TotrainJ-NPE,wesimplyrandomlyuseabatchcomposedof50%ofsimulatedpairs(θ,x )
s
andof50%(θ,x )fromthecalibrationset. Weusethesamearchitectureandhyper-parametersas
o
theSBINPE.Thebestmodelisselectedbasedonthebesttrainingsetperformance. Wedo50epochs
with50000simulatedexamplesforeachepoch. Thebatchsizeis100.
16Algorithm2ConvolutionalNeuralNetworkforTasksAandD.
1: Conv1d(1,16,3,1,dilation = 2,padding = 12: ReLU()
1) 13: Conv1d(128,128,3,2,dilation =
2: ReLU() 2,padding=1)
3: Conv1d(16,64,3,2,dilation=2,padding= 14: ReLU()
1) 15: AvgPool1d(3,1)
4: ReLU() 16: Conv1d(128,128,3,1,dilation =
5: AvgPool1d(3,1) 2,padding=1)
6: Conv1d(64,128,3,1,dilation=2,padding= 17: ReLU()
1) 18: Flatten()
7: ReLU() 19: Linear(2048,512)
8: Conv1d(128,128,3,2,dilation = 20: ReLU()
2,padding=1) 21: Linear(512,128)
9: ReLU() 22: ReLU()
10: AvgPool1d(3,1) 23: Linear(128,32)
11: Conv1d(128,128,3,1,dilation = 24: ReLU()
2,padding=1) 25: Linear(32,10)
HVAE FortheHVAE,were-usetheNPEmodelasthephysicsencoderandreplacethedecoder
withadeterministicversionofthesimulator,thusremovingtheGaussiannoiseonarandomphase
shift. Inaddition,wefollowtheapproachofTakeishiandKalousis(2021)andhave1)areal-world
encoderthatmapsx toz ,2)areality-to-physicsencoder,and3)aphysics-to-realitydecoder. The
o a
real-world encoder has the same architecture as the NSE of the NPE and outputs the mean and
log-varianceofa5Dlatentvectorz . Thereality-to-physicsandphysics-to-realityalsohavethe
a
samearchitecturesandaretwoconditional1DU-Netwithneuralnetworkarchitecturedescribedin
Algorithm3.
To train the HVAE, we freeze the parameters of the NPE and optimizes the ELBO as well as a
calibrationlossthatevaluatesthelikelihoodassignedtothetruephysicalparameters. Alldistributions
areparameterizedbyGaussianwithmeanandlog-variancepredictedbytheneuralnetworks. We
donotuseanyadditionallossesasweexpectconstrainingNPEandusingthecalibrationsetshould
alreadyprovidethenecessarysupporttousethephysicsinameaningfulway. TheHVAEistrained
onthe2000testexamplesasitistheonlyreal-worlddata,calibrationsetaside,thatwehaveaccess
to. Weuseabatchsizeequalto100andalearningrateequalto1e-3. Webelieveobtainingabetter
HVAEispossible. However,weemphasizethecomplexityofsettingupagoodHVAEfortheonly
purposeofstatisticalinferenceoverparameters.
Datasets
Forthistask,wecangeneratesamples(θ,x )ontheflytotraintheNPE.Thecalibrationandtestsets
s
arealsogeneratedrandomlybysamplingfromthepriordistributionandusingthedampedpendulum
simulator.
D.3 TaskD:Hemodynamics
Description
InspiredbyWehenkeletal.(2023),wedefinethetaskofinferringimportantcardiovascularparameters
fromnormalizedarterialpressurewaveformsmeasuredattheradialartery. Thesimulatorusesmany
physiological parameters that modulates the heart function, physical properties of the 116 main
arterial segments, and behavior of the vascular beds. Our inference concerns two parameters of
theheartfunction,θ := [SV,LVET],thestrokevolume(SV)istheamountpumpedoutfromthe
leftventricleovertheheartbeatmodeled,andtheleftventricularejectiontime(LVET)isthetime
intervalbetweenopeningandclosureoftheaorticvalve. Otherparameters,suchastheheartrate
orarteries’stiffness,areconsideredasnuisanceeffectsandarerandomlysampledfromarealistic
populationdistribution. Anadditionalsourceofrandomnessisaddedbymodelingmeasurement
17Algorithm3UNet1DArchitecture Algorithm 4 Block1D(in_channels,
out_channels)
1: Unet1D:
2: Encoder1D: 1: Conv1d(in_channels, out_channels,
3: Block(in_channels = 1,out_channels = kernel_size=3,padding=1)
64) 2: ReLU()
4: Block(in_channels = 64,out_channels = 3: Conv1d(out_channels, out_channels,
128) kernel_size=3,padding=1)
5: Block(in_channels = 128,out_channels = 4: ReLU()
256)
6: Block(in_channels = 256,out_channels =
512)
7: Block(in_channels = 512,out_channels =
Algorithm52DConvolutionalNeuralNet-
work
1024)
8: MaxPool1d(2) 1: Conv2d(3,64,3,2,dilation=1),ReLU()
9: Decoder1D: 2: Conv2d(64, 128, 3, 2, dilation=1),
10: ConvTranspose1d(1024 + ReLU()
5,512,2,stride=2) 3: MaxPool2d(3)
11: Block(in_channels = 4: Conv2d(128, 128, 3, 2, dilation=1),
1024,out_channels=512) ReLU()
12: ConvTranspose1d(512,256,2,stride=2) 5: Conv2d(128, 64, 1, 1, dilation=1),
13: Block(in_channels = 512,out_channels = ReLU()
256) 6: Conv2d(64,3,1,1,dilation=1),ReLU()
14: ConvTranspose1d(256,128,2,stride=2) 7: Flatten()
15: Block(in_channels = 256,out_channels = 8: Linear(27,100),ReLU()
128) 9: Linear(100,20)
16: ConvTranspose1d(128,64,2,stride=2)
17: Block(in_channels = 128,out_channels =
64)
18: ConvTranspose1d(64,1,2,stride=2)
19: Block(in_channels = 64,out_channels =
1)
20: Conv1d(64,1,1)
errorswithawhiteGaussiannoiseandrandomizingthestartingrecordingtimewithrespecttothe
cardiac cycle. The simulator produces 8-second timeseries x ∈ R1000 sampled at 125Hz. As
t
syntheticmisspecification,thesimulatorassumesallarterieshavethesamelengthoverthepopulation
considered,whereas"real-world"dataareartificiallygeneratedbyalsovaryingthelengthofarteries
andaccountfortheeffectofhuman’sheight.ThesimulatorisbasedontheopenBFPDEsolver(Melis,
2017)specializedforhemodynamics,whichisnotdifferentiableandtakesapproximatelyoneminute
tosimulateonesampleonastandardCPU.Thissynthetictasksrepresentacommonscenarioin
whichasimulator,althoughfaithfultotheeffectofcertainparameters,missesadditionaldegreesof
freedomthatexistsforthereal-worlddata.
Neuralarchitecture&TrainingHyperparameters
NeuralPosteriorEstimator. TheNSEisthe1DconvolutionalneuralnetworkdescribedinAlgo-
rithm6.TheNCDEisa5-stepdiscretenormalizingflowwithanautoregressiveconditionerandaffine
normalizers.Eachofthe5autoregressiveconditionersisaMADEwithReLUactivationsand4layers
of300neuronsthatoutput4dimensionalvectorsusedtoparameterizetheaffinetransformations.
FortrainingtheNPE,weuseabatchsizeof100andalearningfactorequalto5e-4. NPEistrained
untilconvergence. OtherparametersaresettodefaultvaluesandshouldmarginallyimpacttheNPE
obtained.
ROPE NSE. We have selected the best NPE based on the validation set with 2000 examples
generatedwiththesimulator. TheNPEisfixedtoonebest-of-allmodel. Wefine-tunetheNCDEwith
18Algorithm6CNNArchitectureforTaskC.
1: Conv1d(1,16,3,1,dilation=2,padding=1),ReLU()
2: Conv1d(16,64,3,2,dilation=2,padding=1),ReLU()
3: AvgPool1d(4,2)
4: Conv1d(64,128,3,1,dilation=2,padding=1),ReLU()
5: Conv1d(128,128,3,2,dilation=2,padding=1),ReLU()
6: AvgPool1d(4,2)
7: Conv1d(128,128,3,1,dilation=2,padding=1),ReLU()
8: Conv1d(128,128,3,2,dilation=2,padding=1),ReLU()
9: AvgPool1d(4,1)
10: Conv1d(128,128,3,1,dilation=2,padding=1),ReLU()
11: Flatten()
12: Linear(1024,512),ReLU()
13: Linear(512,128),ReLU()
14: Linear(128,32),ReLU()
15: Linear(32,5)
alearningrateequalto1e-5for2000gradientstepson80%ofcalibrationset. Weusea1-sample
MonteCarloestimateoftheexpectationin(7).
J-NPE TotrainJ-NPE,wesimplyrandomlyuseabatchcomposedof50%ofsimulatedpairs(θ,x )
s
andof50%(θ,x )fromthecalibrationset. Weusethesamearchitectureandhyper-parametersas
o
theSBINPE.Thebestmodelisselectedbasedonthebesttrainingsetperformance. Wedo50epochs
with6000simulatedexamplesforeachepoch. Thebatchsizeis100.
HVAE ThereisnoHVAEforthisexperimentasthesimulatorisnon-differentiable.
Datasets
Forthistask, wecannotgeneratesamples(θ,x )ontheflytotraintheNPE.Forthepurposeof
s
thisexperiment,wehavegenerated10000simulationsandreal-worldobservations. Ourfine-tuning
strategyapproximates(7)byfindingthesimulationswiththeclosestparametervalue.
D.4 TaskE:LightTunnel
Description
We use one of the light-tunnel datasets from the causal chamber project (Gamella et al., 2024,
causalchamber.org). Inparticular,weusethedatafromtheap_1.8_iso_500.0_ss_0.005ex-
perimentinthelt_camera_v0dataset. Thelighttunnelisanelongatedchamberwithacontrollable
lightsourceatoneend,twolinearpolarizersmountedonrotatingframes,andacamerathattakes
imagesofthelightsourcethroughthepolarizers. WereferthereadertoGamellaetal.(2024,Figure
2) for a complete schematic. Our task consists of predicting the color setting of the light source
((R,G,B)∈[0,255]3)andthedimmingeffectofthelinearpolarizersα∈[0,1]fromthecaptured
images. Asamisspecifiedsimulatoroftheimage-generatingprocess,weadoptthesimplemodel
describedinGamellaetal.(2024,ModelF1,AppendixD).APythonimplementationisavailable
throughthecausalchamberpackage(models.model_f1); visitcausalchamber.orgformore
details. As input, the simulator takes the parameters θ := [R,G,B,α] and produces an image
consisting of a hexagon roughly the size of the light source, with an RGB color vector equal to
[αR,αG,αB]. Thefactorα:=cos2(θ −θ ),whereθ ,θ denotetheanglesofthetwopolarizers,
1 2 1 2
correspondstoMalus’law(e.g.,Collett,2005),whichmodelsthedimmingeffectofthepolarizersas
afunctionoftheirrelativeangle. Besidestheobviousmisspecificationwithrespecttoimagerealism
(seeFigure2),themodelignoresotherimportantphysicalaspects,suchasthespectralresponseof
thecamerasensororthenon-uniformeffectofthepolarizersonthedifferentcolors—moredetails
can be found in Gamella et al. (2024, Appendix D.IV.2.2). The prior is uniform over colors and
polarizerangles,whichleadstoanon-uniformprioroverthedimmingeffectα.
19Neuralarchitecture&TrainingHyperparameters
Neural Posterior Estimator. The NSE is the 2D convolutional neural network described by
Algorithm5.
TheNCDEisalsoaone-stepdiscretenormalizingflowwithanautoregressiveconditioneranda
UMNN(WehenkelandLouppe,2019)asthenormalizer. TheautoregressiveconditionerisaMADE
withReLUactivationand3layersof500neuronsthatoutputsa10dimensionalvectortotheUMNN.
TheUMNNhasanintegrandnetwith4layersof150neuronswithReLUactivations. Fortrainingthe
NPE,weuseabatchsizeof100andalearningfactorequalto5e-4. NPEistraineduntilconvergence.
OtherparametersaresettodefaultvaluesandshouldmarginallyimpacttheNPEobtained.
ROPE NSE. We have selected the best NPE based on the validation set with 10000 examples
generatedwiththesimulator. TheNPEisfixedtoonebest-of-allmodel. Wefine-tunetheNCDE
withalearningrateequalto1e-4for2000gradientstepsonon80%ofthecalibrationset. Weusea
1-sampleMonteCarloestimateoftheexpectationin(7).
J-NPE TotrainJ-NPE,wesimplyrandomlyuseabatchcomposedof50%ofsimulatedpairs(θ,x )
s
andof50%(θ,x )fromthecalibrationset. Weusethesamearchitectureandhyper-parametersas
o
theSBINPE.Thebestmodelisselectedbasedonthebesttrainingsetperformance. Wedo50epochs
with1000simulatedexamplesforeachepoch. Simulationsaregeneratedrandomlyforeachbatchby
samplingthepriorandsimulatingforthecorrespondingparameters. Thebatchsizeis100.
HVAE FortheHVAE,were-usetheNPEmodelasthephysicsencoderandusethesimulatorasis
asitisdifferentiablewithoutadditionaleffort. Inaddition,wefollowtheapproachofTakeishiand
Kalousis(2021)andhave1)areal-worldencoderthatmapsx toz ,2)areality-to-physicsencoder,
o a
and3)aphysics-to-realitydecoder. Thereal-worldencoderhasthesamearchitectureastheNSE
oftheNPEandoutputsthemeanandlog-varianceofa5Dlatentvectorz . Thereality-to-physics
a
andphysics-to-realityalsohavethesamearchitecturesandaretwoconditional2DU-Netwiththe
architecturedescribedbyAlgorithm7.
To train the HVAE, we freeze the parameters of the NPE and optimizes the ELBO as well as a
calibrationlossthatevaluatesthelikelihoodassignedtothetruephysicalparameters. Alldistributions
areparameterizedbyGaussianwithmeanandlog-variancepredictedbytheneuralnetworks. We
donotuseanyadditionallossesasweexpectconstrainingNPEandusingthecalibrationsetshould
alreadyprovidethenecessarysupporttousethephysicsinameaningfulway. TheHVAEistrained
onthe2000testexamplesasitistheonlyreal-worlddata,calibrationsetaside,thatwehaveaccess
to. Weuseabatchsizeequalto100andalearningrateequalto1e-3. Webelieveobtainingabetter
HVAEispossible. However,weemphasizethecomplexityofsettingupagoodHVAEfortheonly
purposeofstatisticalinferenceoverparameters.
Datasets
Forthistask,wecangeneratesamples(θ,x )ontheflytotraintheNPE.However,thecalibration
s
andtestsetsarereal-worlddata. Weensurethereisnotoverlapbetweencalibrationandtestset. The
isnorandomizationandthetestsetisconstantforallexperiments,thecalibrationsetarealsofixed
foragivencalibrationsetsize.
D.5 TaskF:WindTunnel
Description
We use one of the wind-tunnel datasets from the causal chamber project (Gamella et al., 2024,
causalchamber.org). Inparticular,weusethedatafromtheload_out_0.5_osr_downwind_4
experimentinthewt_intake_impulse_v1dataset. Thetunnelisachamberwithtwocontrollable
fansthatpushairthroughitandbarometersthatmeasureairpressureatdifferentlocations. Ahatch
preciselycontrolstheareaofanadditionalopeningtotheoutside(seeGamellaetal.,2024,Figure2).
Thedataisacollectionofpressurecurvesthatresultfromapplyingashortimpulsetotheintakefan
loadandmeasuringthechangeinairpressureusingoneofthebarometersinsidethetunnel. Our
inferencetaskconsistsofpredictingthehatchposition,θ := [H] ∈ [0,45]givenapressurecurve
(seeFigure2). Asasimulatormodel,wecombinethemodelsA2andC3describedinGamellaetal.
20Algorithm72DUNet Algorithm 8 Block2D(in_channels,
out_channels)
1: Encoder2D:
2: Block2D(in_channels=3,out_channels=64) 1: Conv2d(in_channels, out_channels,
3: Block2D(in_channels=64,out_channels=128) kernel_size=3,padding=1,bias=False)
4: Block2D(in_channels=128, 2: BatchNorm2d(num_features=out_channels)
out_channels=256) 3: ReLU(inplace=True)
5: Block2D(in_channels=256, 4: Conv2d(out_channels, out_channels,
out_channels=512) kernel_size=3,padding=1,bias=False)
6: Block2D(in_channels=512, 5: BatchNorm2d(num_features=out_channels)
out_channels=1024) 6: ReLU(inplace=True)
7: MaxPool2d(2)
8: Decoder2D:
9: ConvTranspose2d(1024+5,512,2,stride=2)
10: Block2D(in_channels=1024,
out_channels=512)
11: ConvTranspose2d(512,256,2,stride=2)
12: Block2D(in_channels=512,
out_channels=256)
13: ConvTranspose2d(256,128,2,stride=2)
14: Block2D(in_channels=256,
out_channels=128)
15: ConvTranspose2d(128,64,2,stride=2)
16: Block2D(in_channels=128,out_channels=64)
17: ConvTranspose2d(64,1,2,stride=2)
18: Block2D(in_channels=64,out_channels=1)
19: Conv2d(64,1,1)
(2024,AppendixD);wenumericallysolvetheODEinmodelA2,andaddstochasticcomponents
to simulate the sensor noise and the unknown time point at which the impulse is applied. This
resultsinthesimulatorbeingneitherdifferentiablenordeterministic. APythonimplementationof
thecompletesimulatorisavailableinthecausalchamberpackage(models.simulator_a2_c3);
visit causalchamber.org for more details. Misspecification arises from the many simplifying
assumptionsneededtomodelthecomplexdynamicsoftheairflowinsidethetunnel—moredetails
canbefoundinGamellaetal.(2024,AppendixD.IV.1.2).
NeuralPosteriorEstimator. TheNSEandNCDEhavethesame1Dconvolutionalneuralnetwork
asforTaskA.FortrainingtheNPE,weuseabatchsizeof100andalearningfactorequalto5e-4.
NPEistraineduntilconvergence. Otherparametersaresettodefaultvaluesandshouldmarginally
impacttheNPEobtained.
ROPE NSE. We have selected the best NPE based on the validation set with 10000 examples
generatedwiththesimulator. TheNPEisfixedtoonebest-of-allmodel. Wefine-tunetheNCDE
withalearningrateequalto1e-4for20000gradientstepsonon80%ofthecalibrationset. Weusea
1-sampleMonteCarloestimateoftheexpectationin(7).
J-NPE. TotrainJ-NPE,wesimplyrandomlyuseabatchcomposedof50%ofsimulatedpairs(θ,x )
s
andof50%(θ,x )fromthecalibrationset. Weusethesamearchitectureandhyper-parametersas
o
theSBINPE.Thebestmodelisselectedbasedonthebesttrainingsetperformance. Wedo50epochs
with10000simulatedexamplesforeachepoch. Thebatchsizeis100.
HVAE. ThereisnoHVAEforthisexperimentasthesimulatorisnon-differentiable.
21Datasets
Forthistask,althoughslightlyslowerthanTaskAandB,wecangeneratesamples(θ,x )onthefly
s
totraintheNPE.However,thecalibrationandtestsetsarereal-worlddata. Weensurenooverlap
betweenthetwosetsforallcalibrationsetsizes. Allsetsarefixedforallexperiments.
E Additionalresults
E.1 Cornerplots
20 20 20
16 16 16
12 12 12
8 8 8
4 4 4
20.0 20.0 20.0
17.5 17.5 17.5
15.0 15.0 15.0
12.5 12.5 12.5
10.0
300 600 900 1200 1500 4 8 12 16 20 10.0 12.5 15.0 17.5 20.0
10.0
300 600 900 1200 1500 4 8 12 16 20 10.0 12.5 15.0 17.5 20.0
10.0
300 600 900 1200 1500 4 8 12 16 20 10.0 12.5 15.0 17.5 20.0
λc λp λd λc λp λd λc λp λd
Figure5: ThreecornerplotsfortaskAwithacalibrationsetwith50samples.
0.45 0.45 0.45
0.30 0.30 0.30
0.15 0.15 0.15
0.00
0.00 0.15 0.30 0.45 0.00 0.15 0.30 0.45
0.00
0.00 0.15 0.30 0.45 0.00 0.15 0.30 0.45
0.00
0.00 0.15 0.30 0.45 0.00 0.15 0.30 0.45
γ β γ β γ β
Figure6: ThreecornerplotsfortaskBwithacalibrationsetwith50samples.
10.0 10.0 10.0
7.5 7.5 7.5
5.0 5.0 5.0
2.5 2.5 2.5
0.0 0.8 1.6 2.4 2.5 5.0 7.5 10.0 0.0 0.8 1.6 2.4 2.5 5.0 7.5 10.0 0.0 0.8 1.6 2.4 2.5 5.0 7.5 10.0
ω0 A ω0 A ω0 A
Figure7: ThreecornerplotsfortaskCwithacalibrationsetwith50samples.
E.2 Calibrationplots
F ComputingACAUC
22
pλ
dλ
β
A
pλ
dλ
β
A
pλ
dλ
β
A360 360 360
300 300 300
240 240 240
180 180 180
120 50 75 100 125 120 180 240 300 360 120 50 75 100 125 120 180 240 300 360 120 50 75 100 125 120 180 240 300 360
HR SV HR SV HR SV
Figure8: ThreecornerplotsfortaskDwithacalibrationsetwith50samples.
240 240 240
180 180 180
120 120 120
60 60 60
0 0 0
240 240 240
180 180 180
120 120 120
60 60 60
0 0 0
1.00 1.00 1.00
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00
0 60 120 180 240 0 60 120 180 240 0 60 120 180 240 0.00 0.25 0.50 0.75 1.00
0.00
0 60 120 180 240 0 60 120 180 240 0 60 120 180 240 0.00 0.25 0.50 0.75 1.00
0.00
0 60 120 180 240 0 60 120 180 240 0 60 120 180 240 0.00 0.25 0.50 0.75 1.00
R G B α R G B α R G B α
Figure9: ThreecornerplotsfortaskEwithacalibrationsetwith50samples.
240 240 240
180 180 180
120 120 120
60 60 60
0 0 0
240 240 240
180 180 180
120 120 120
60 60 60
0 0 0
1.00 1.00 1.00
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00
0 60 120 180 240 0 60 120 180 240 0 60 120 180 240 0.00 0.25 0.50 0.75 1.00
0.00
0 60 120 180 240 0 60 120 180 240 0 60 120 180 240 0.00 0.25 0.50 0.75 1.00
0.00
0 60 120 180 240 0 60 120 180 240 0 60 120 180 240 0.00 0.25 0.50 0.75 1.00
R G B α R G B α R G B α
Figure10: ThreecornerplotsfortaskEwithdistributionshiftwithacalibrationsetwith50samples.
G LearningMinimalSufficientStatisticswithNeuralPosteriorEstimation
WenowdiscusswhyNPEmaylearnaminimalsufficientstatistic. First,underasufficientlylarge
validationset,NPE’sobjectivefunctionisonlyoptimalonthevalidationsetifNPEmodelsthetrue
posteriorasdefinedimplicitlybythepriorp(θ)andthelikelihoodcorrespondingtothesimulatorS.
Thisconsistencyhasbeenprovenin(PapamakariosandMurray,2016)andisthemotivationtouse
suchanobjectivewhenestimatingdensity. Second,somenormalizingflows,suchasautoregressive
UMNNflows(WehenkelandLouppe,2019),areuniversalapproximatorsofcontinuousdensities.
Inaddition,neuralnetworksarealsouniversalfunctionapproximators. Assuch,wecanclaimthat
itisalwayspossibletoparameterizetheNCDEp (θ | h (x))suchthattheclassoffunctionsits
θ ω
parametersrepresentcontainsthetrueposterior. WedirectlyobservethatxisonlyusedbytheNCDE
throughh ω(x). Thus,underperfecttrainingp θ⋆(θ |h ω⋆(x))=p(θ |x)andh ω⋆(x)isasufficient
statisticforθgivenxunderthesimulator’smodel.
Withoutadditionalconstraint, wecannotclaimanythingabouttheminimalityofh ω⋆(x). Never-
theless,wecanenforcetheneuralnetworkh ω⋆(x)hasaninformationbottleneckandthusreduces
23
VS
G
B
α
G
B
α
VS
G
B
α
G
B
α
VS
G
B
α
G
B
α(a) (b) (c)
(d) (e) (f)
Figure11: Calibrationplotsofthedifferentmethodsonthe6benchmarks,thecoverageateachlevel
istheaverageofthecoverageofthemarginaldistributions. Eachcolorindicatesadifferentalgorithm
andtheopacityisproportionaltothesizeofthecalibrationsetwhichrangesfrom10to1000. We
observethatROPEandOT-onlyareconsistentlywellcalibratedfor.
Algorithm9StatisticalCalibrationofPosteriorDistribution
Input: DatasetofpairsD ={(θi,xi)},Posteriorestimatorp˜(θ |x),NumberofsamplesN.
Output: ACAUC
1: AVG_CALIBRATION=0
2: fork ∈{1,...,K})do
3: InitializeanemptylistCredLevels
4: for(θi,xi)∈Ddo
5: InitializeanemptylistSamples
6: forj =1toM do
7: Sampleθj fromp˜(θ |xi)
8: Appendθj toSamples
9: endfor
10: SortSamples
11: Computetherank(positioninascendingorder)rofθinSamples
12: SetCredLevels= r
N
13: AppendCredLeveltoCredLevels
14: endfor
15: SortCredLevels
16: CALIBRATION=(cid:80)N CredLevels[i]− i
i=1 N
17: AVG_CALIBRATION=AVG_CALIBRATION+ CALIBRATION
K
18: endfor
Return: AVG_CALIBRATION
the information carried. In practice, we ensure the output dimension of h ω⋆(x) and the NCDE
achievesoptimalperformanceonthetestset. Recallingthat,inthecontextofSBI,wecangenerate
asmanysamplesasneededandobtainestimatorsthatcloselyapproachthesimulation’sposteriorand
aminimalsufficientstatistic.
24