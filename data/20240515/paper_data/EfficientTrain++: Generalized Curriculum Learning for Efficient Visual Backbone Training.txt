EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 1
EfficientTrain++: Generalized Curriculum
Learning for Efficient Visual Backbone Training
Yulin Wang, Yang Yue, Rui Lu, Yizeng Han, Shiji Song, Senior Member, IEEE,
and Gao Huang†, Member, IEEE
Abstract—Thesuperiorperformanceofmoderncomputervisionbackbones(e.g.,visionTransformerslearnedonImageNet-1K/22K)
usuallycomeswithacostlytrainingprocedure.Thisstudycontributestothisissuebygeneralizingtheideaofcurriculumlearning
beyonditsoriginalformulation,i.e.,trainingmodelsusingeasier-to-harderdata.Specifically,wereformulatethetrainingcurriculumasa
soft-selectionfunction,whichuncoversprogressivelymoredifficultpatternswithineachexampleduringtraining,insteadofperforming
easier-to-hardersampleselection.Ourworkisinspiredbyanintriguingobservationonthelearningdynamicsofvisualbackbones:
duringtheearlierstagesoftraining,themodelpredominantlylearnstorecognizesome‘easier-to-learn’discriminativepatternsinthe
data.Thesepatterns,whenobservedthroughfrequencyandspatialdomains,incorporatelower-frequencycomponents,andthe
naturalimagecontentswithoutdistortionordataaugmentation.Motivatedbythesefindings,weproposeacurriculumwherethemodel
alwaysleveragesallthetrainingdataateverylearningstage,yettheexposuretothe‘easier-to-learn’patternsofeachexampleis
initiatedfirst,withharderpatternsgraduallyintroducedastrainingprogresses.Toimplementthisideainacomputationallyefficientway,
weintroduceacroppingoperationintheFourierspectrumoftheinputs,enablingthemodeltolearnfromonlythelower-frequency
components.Thenweshowthatexposingthecontentsofnaturalimagescanbereadilyachievedbymodulatingtheintensityofdata
augmentation.Finally,weintegratethesetwoaspectsanddesigncurriculumlearningschedulesbyproposingtailoredsearching
algorithms.Moreover,wepresentusefultechniquesfordeployingourapproachefficientlyinchallengingpracticalscenarios,suchas
large-scaleparalleltraining,andlimitedinput/outputordatapre-processingspeed.Theresultingmethod,EfficientTrain++,issimple,
general,yetsurprisinglyeffective.Asanoff-the-shelfapproach,itreducesthetrainingtimeofvariouspopularmodels(e.g.,ResNet,
ConvNeXt,DeiT,PVT,Swin,CSWin,andCAFormer)by1.5−3.0×onImageNet-1K/22Kwithoutsacrificingaccuracy.Italso
demonstratesefficacyinself-supervisedlearning(e.g.,MAE).Codeisavailableat:https://github.com/LeapLabTHU/EfficientTrain.
IndexTerms—Deepnetworks,visualbackbones,efficienttraining,curriculumlearning.
✦
1 INTRODUCTION
The remarkable success of modern visual backbones is
largely fueled by the interest in exploring big models on
comprehensive, large-scale benchmark datasets [1], [2], [3],
[4]. In particular, the recent introduction of vision Trans-
formers(ViTs)scalesupthenumberofmodelparametersto
more than 1.8 billion, concurrently expanding the training
datato3billionsamples[3],[5].Whilethishasfacilitatedthe
(a) Sample-wise CL (existing works) (b) Generalized CL (ours)
attainmentofstate-of-the-artaccuracy,thishuge-modeland Discrete-selection: ‘selecting Soft-selection: ‘uncovering
high-dataregimeresultsinatime-consumingandexpensive easier-to-harder samples’ progressively more difficult patterns’
training process. For example, it takes 2,500 TPUv3-core- Fig. 1: (a) Sample-wise curriculum learning (CL): making a discrete
decision on whether each example should be leveraged to train the
days to train ViT-H/14 on JFT-300M [3], which may be
model. (b) Generalized CL: we consider a continuous function Tt(·),
unaffordable for practitioners in both academia and in- whichonlyexposesthe‘easier-to-learn’patternswithineachexampleat
dustry. Moreover, the considerable power consumption for thebeginningoftraining(e.g.,lower-frequencycomponents;see:Section
trainingdeeplearningmodelswillleadtosignificantcarbon 4), while gradually introducing relatively more difficult patterns as
learningprogresses.
emissions[6],[7].Duetobotheconomicandenvironmental
concerns, there has been a growing demand for reducing
andincreasingthedifficultylevelgradually.Themajorityof
thetrainingcostofmoderndeepnetworks.
extantresearcheffortsimplementthisapproachbyintroduc-
In this paper, we contribute to this issue by revisiting
ingeasier-to-harderexamplesprogressivelyduringtraining
the concept of curriculum learning [8], which reveals that
[9], [10], [11], [12], [13], [14], [15], [16]. However, obtaining
a model can be trained efficiently by starting with the
a light-weighted and generalizable difficulty measurer is
easier aspects of a given task or certain easier subtasks,
typicallynon-trivial[9],[10].Ingeneral,thesemethodshave
notexhibitedthecapacitytobeauniversalefficienttraining
• Y.Wang,Y.Yue,R.Lu,Y.Han,S.Song,andG.HuangarewiththeDe-
techniqueformodernvisualbackbones.
partmentofAutomation,BNRist,TsinghuaUniversity,Beijing100084,
China.G.HuangisalsowithBeijingAcademyofArtificialIntelligence. Incontrasttopriorworks,thispaperseeksasimpleyet
Email:wang-yl19@mails.tsinghua.edu.cn,gaohuang@tsinghua.edu.cn. broadly applicable efficient learning approach with the po-
• †:correspondingauthor.
tential for widespread implementation. To attain this goal,
©2024 IEEE.Personal use ofthis material ispermitted. Permissionfrom IEEEmust be obtainedfor all otheruses, in anycurrent or futuremedia, including
reprinting/republishingthismaterialforadvertisingorpromotionalpurposes,creatingnewcollectiveworks,forresaleorredistributiontoserversorlists,orreuseof
anycopyrightedcomponentofthisworkinotherworks.
4202
yaM
41
]VC.sc[
1v86780.5042:viXraEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 2
our investigation delves into a generalization of curricu- Parts of the results in this paper were published orig-
lum learning. In specific, we extend the notion of training inally in its conference version [24]. However, this paper
curricula beyond only differentiating between ‘easier’ and extendsourearlierworksinseveralimportantaspects:
‘harder’ examples, and adopt a more flexible hypothesis, • An improved EfficientTrain++ approach is presented. We
which indicates that the discriminative features of each introduce a computational-constrained sequential search-
trainingsamplecompriseboth‘easier-to-learn’and‘harder- ingalgorithmforsolvingforthetrainingcurriculum(Sec-
to-learn’ patterns. Instead of making a discrete decision on tion5.1),whichisnotonlysignificantlymoreefficient,but
whether each example should appear in the training set, alsooutperformsthegreedy-searchalgorithminEfficient-
we argue that it would be more proper to establish a Train in terms of the resulting curriculum. Moreover, we
continuousfunctionthatadaptivelyextractsthesimplerand propose an efficient low-frequency down-sampling oper-
more learnable discriminative patterns within every exam- ation (Section 5.2), which effectively addresses the issue
ple. In other words, a curriculum may always leverage all of high CPU-GPU I/O costs caused by EfficientTrain. We
examplesatanystageoflearning,butitshouldeliminatethe conduct new experiments to investigate the effectiveness
relatively more difficult or complex patterns within inputs ofthesetwocomponents(Tables8,19,20,21).
atearlierlearningstages.OurideaisillustratedinFigure1. • We develop two implementation techniques for Efficient-
Drivenbyourhypothesis,astraightforwardyetsurpris- Train++(Section5.3).Thefirstoneimprovesthescalability
ingly effective algorithm is derived. We first demonstrate ofthepracticaltrainingefficiencyofEfficientTrain++with
that the ‘easier-to-learn’ patterns incorporate the lower- respecttothenumberofGPUsusedfortraining.Thesec-
frequency components of images. We further show that a ond one can significantly reduce the data pre-processing
lossless extraction of these components can be achieved loads for CPUs/memory, while preserving competitive
by introducing a cropping operation in the frequency do- performance. Extensive experimental evidence has been
main. This operation not only retains exactly all the lower- providedtodemonstratetheirutility(Tables13,14).
frequency information, but also yields a smaller input size • Ourexperimentshavebeenimprovedbyextensivelyeval-
for the model to be trained. By triggering this operation uating EfficientTrain++, e.g., via supervised learning on
at earlier training stages, the overall computational/time ImageNet-1K (Tables 7, 9, and Figure 7), ImageNet-22K
costfortrainingcanbeconsiderablyreducedwhilethefinal pre-training(Table12),self-supervisedlearning(Table15),
performance of the model will not be sacrificed. Moreover, transfer learning (Tables 16, 17, 18), and being compared
we show that the original information before heavy data with existing approaches (Table 11). Besides, new analyt-
augmentation is more learnable, and hence starting the icalinvestigationshavebeenadded(Tables22,23).These
trainingwithweakeraugmentationtechniquesisbeneficial. comprehensive results demonstrate that EfficientTrain++
Finally,thesetheoreticalandexperimentalinsightsareinte- outperformsbothvanillaEfficientTrainandothercompet-
grated into a unified ‘EfficientTrain’ learning curriculum by itivebaselines,attainingstate-of-the-arttrainingefficiency.
leveragingagreedy-searchalgorithm.
Built upon our aforementioned exploration, we further
2 RELATED WORK
introduce an enhanced EfficientTrain++ approach that im-
proves EfficientTrain from two aspects. First, we propose Curriculumlearningisatrainingparadigminspiredbythe
an advanced searching algorithm, which not only dramati- organized learning order of examples in human curricula
cally saves the cost for solving for the curriculum learning [8], [25], [26]. This idea has been widely explored in the
schedule, but also contributes to a well-generalizable and contextoftrainingdeepnetworksfromeasierdatatoharder
more effective resulting curriculum. Second, we present an data [9], [10], [11], [12], [16], [27], [28]. Typically, a pre-
efficientlow-frequencydown-samplingoperation,aimingto defined[8],[29],[30],[31]orautomatically-learned[11],[12],
alleviatetheproblemofhighCPU-GPUinput/output(I/O) [14], [16], [27], [32], [33], [34], [35], [36], [37] difficulty mea-
costscausedbyEfficientTrain.Beyondthesemethodological surerisdeployedtodifferentiatebetweeneasierandharder
advancements,wedeveloptwoimplementationtechniques samples, while a scheduler [8], [9], [11], [28] is defined to
forEfficientTrain++.Theyareusefulforachievingmoresig- determinewhenandhowtointroducehardertrainingdata.
nificantpracticaltrainingspeedup,e.g.,through1)facilitat- For example, DIHCL [38] and InfoBatch [39] determine
ingefficientlarge-scaleparalleltrainingusinganincreasing thehardnessofeachsamplebyleveragingitsonlinetraining
number of GPUs (e.g., 32 or 64 GPUs), and 2) reducing the loss or the change in model outputs. MCL [40] adaptively
datapre-processingloadsforCPUs/memory. selectsasequenceoftrainingsubsetsbyrepeatedlysolving
One of the most appealing advantages of our method joint continuous-discrete minimax optimization problems,
may be its simplicity and generalizability. Our method can whose objective combines both a continuous training loss
beconvenientlyappliedtomostdeepnetworkswithoutaddi- thatreflectstrainingsethardnessandadiscretesubmodular
tionalhyper-parametertuning,butsignificantlyimprovestheir promoterofdiversityforthechosensubset.CurriculumNet
training efficiency. Empirically, for the supervised learn- [41] designs a curriculum that measures the difficulty of
ingonImageNet-1K/22K[17],EfficientTrain++reducesthe different data using the distribution density in the feature
training cost of a wide variety of visual backbones (e.g., space obtained by training an initial model with all data.
ResNet [1], ConvNeXt [18], DeiT [19], PVT [20], Swin [4], Ourworkisbasedonasimilar‘startingsmall’spirit[25]to
CSWin [21], and CAFormer [22]) by 1.5−3.0×, while these methods, but we reformulate the training curriculum
achieving competitive or better performance compared to asasoft-selectionfunctionthatuncoversprogressivelymore
the baselines. Importantly, our method is also effective for difficultpatternswithineachexample,ratherthanperform-
self-supervisedlearning(e.g.,MAE[23]). ingeasier-to-harderdataselection.EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 3
Our work is also related to curriculum by smoothing frequency components first is inline with [61], but the dis-
[42], curriculum dropout [43] and label-similarity curricu- cussions in [61] focus on the robustness of ConvNets and
lum [44], which do not perform example selection as well. are mainly based on some small models and tiny datasets.
However, our method is orthogonal to them since we pro- Towards this direction, several existing works also explore
pose to reduce the training cost by modifying the model decomposingtheinputsofmodelsinthefrequencydomain
inputs. In contrast, they focus on regularizing the deep [62],[63],[64]inordertounderstandorimprovetherobust-
features during training (e.g., via anti-aliasing smoothing nessofthenetworks.Incontrast,ouraimistoimprovethe
[42]andfeaturedropout[43]),orprogressivelyconfiguring trainingefficiencyofmoderndeepvisualbackbones.
thelearningobjective(e.g.,throughcomputingthesimilarity
ofthewordembeddingsoflabels[44]).
3 GENERALIZED CURRICULUM LEARNING
Efficient progressive learning algorithms. It has been
observed that smaller networks are typically more efficient As uncovered in previous research, machine learning algo-
totrainatearlierlearningstages.Morespecifically,previous rithmsgenerallybenefitfroma‘startingsmall’strategy,i.e.,
research has revealed that deep networks can be trained tofirstlearncertaineasieraspectsofatask,andincreasethe
efficiently with the increasing model size during training, levelofdifficultyprogressively[8],[25],[26].Thedominant
e.g.,aprogressivelygrowingnumberorlayers[7],[45],[46], implementation of this idea, curriculum learning, proposes
[47], [48], [49], [50], [51], a growing network width [52], or tointroducegraduallymoredifficultexamplesduringtrain-
a dynamically changing topology of network connections ing[9],[10],[12].Inspecific,acurriculumisdefinedontop
[53], [54]. For example, AutoProg [7] and budgeted ViT of the training process to determine whether or not each
[55]demonstratethatthetrainingefficiencyofvisionTrans- sampleshouldbeleveragedatagivenepoch(Figure1(a)).
formers can be effectively improved by increasing network Onthelimitationsofsample-wisecurriculumlearning.
depthorwidthprogressivelyduringtraining.LipGrow[56] Although curriculum learning has been widely explored
conducts theoretical analyses of network growth from an from the lens of the sample-wise regime, its extensive
ordinary differential equation perspective, and proposes a application is usually limited by two major issues. First,
novel performance measure, yielding an adaptive training differentiatingbetween‘easier’and‘harder’trainingdatais
algorithm for residual networks, which increases depth non-trivial. It typically requires deploying additional deep
automatically and accelerates training. Moreover, the bud- networks as a ‘teacher’ or exploiting specialized automatic
getedtrainingalgorithm[57]showsthatproperlyconfigur- learning approaches [11], [12], [14], [16], [27]. The resulting
inglearningrateschedulesisbeneficialforefficienttraining. implementation complexity and the increased overall com-
Ingeneral,ourworkisorthogonaltotheseeffortssincewe putationalcostarebothnoteworthyweaknessesintermsof
donotmodifynetworkarchitecturesorlearningrates. improvingthetrainingefficiency.Second,itischallengingto
Furthermore,someworks[7],[58],[59]proposetodown- attain a principled approach that specifies which examples
sample early training inputs to save training cost. Our shouldbeattendedtoattheearlierstagesoflearning.Asa
work differs from them in several important aspects: 1) matteroffact,the‘easytohard’strategyisnotalwayshelp-
EfficientTrain++ is drawn from a distinctly different moti- ful [9]. The hard-to-learn samples can be more informative
vation of generalized curriculum learning, based on which andmaybebeneficialtobeemphasizedinmanycases[38],
we present a novel frequency-domain-inspired analysis; 2) [65],[66],[67],[68],[69],sometimesevenleadingtoa‘hard
we introduce a cropping operation in the frequency do- toeasy’anti-curriculum[40],[70],[71],[72],[73],[74].
main, which is not only theoretically different from image Our work is inspired by the above two issues. In the
down-sampling (see: Proposition 1), but also outperforms following, we start by proposing a generalized hypothesis
it empirically (see: Table 19 (b)); 3) we propose a novel forcurriculumlearning,aimingtoaddressthesecondissue.
efficient searching algorithm (see: Algorithm 2) and design Then we demonstrate that an implementation of our idea
an EfficientTrain++ curriculum, achieving a significantly naturallyaddressesthefirstissue.
higher training efficiency than existing works on a variety Generalizedcurriculumlearning.Wearguethatsimply
ofstate-of-the-artmodels(see:Table11).Besides,FixRes[60] measuring the easiness of training samples tends to be
showsthatasmallertrainingresolutionimprovesaccuracy ambiguous and may be insufficient to reflect the effects of
byfixingthediscrepancybetweenthescaleoftrainingand a sample on the learning process. As aforementioned, even
test inputs. However, we do not borrow gains from FixRes the difficult examples may provide beneficial information
as we adopt a standard resolution at final learning stages. for guiding the training, and they do not necessarily need
OurmethodisactuallyorthogonaltoFixRes(see:Table11). to be introduced after easier examples. To this end, we
AutoProg [7] proposes a novel and effective algorithm hypothesize that every training sample, either ‘easier’ or
thatfine-tunesthemodelateachtrainingstagetosearchfor ‘harder’, contains both easier-to-learn or more accessible pat-
aproperprogressivelearningstrategy.Ourworkisrelevant terns, as well as certain difficult discriminative information
toAutoProg[7]inthisparadigm.However,weintroducea whichmaybechallengingforthedeepnetworkstocapture.
novel formulation of computational-constrained searching Ideally, a curriculum should be a continuous function on
(i.e., maximizing validation accuracy under a fixed ratio of topofthetrainingprocess,whichstartswithafocusonthe
training cost saving; see: Section 5.1), leading to a simple ‘easiest’ patterns of the inputs, while the ‘harder-to-learn’
searching objective as well as a significantly better training patternsaregraduallyintroducedaslearningprogresses.
curriculum.AnanalysisonthisissueisgiveninTable21. AformalillustrationisshowninFigure1(b).Anyinput
Frequency-based analysis of deep networks. Our ob- dataX willbeprocessedbyatransformationfunction t()
T ·
servation that deep networks tend to capture the low- conditioned on the training epoch t (t T) before being
≤EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 4
fed into the model, where t() is designed to dynamically
filterouttheexcessivelydifT ficu· ltandlesslearnablepatterns Large
r
withinthetrainingdata.Wealwayslet T(X)=X.Notably, <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""JJJJXXXXxxxxddddjjjjwwwwccccMMMMQQQQ00003333hhhhxxxxccccCCCCaaaaAAAAGGGGKKKKkkkkmmmmLLLLMMMMFFFFbbbb++++cccc===="""">>>>AAAAAAAAAAAAIIIIHHHH3333iiiiccccppppVVVVVVVVLLLLTTTT9999RRRRQQQQFFFFDDDD6666AAAAWWWWhhhhxxxxffffooooEEEEssss3333jjjjRRRRMMMMSSSSFFFFzzzzJJJJOOOOiiiiYYYYkkkkssssXXXXJJJJCCCC4444cccceeeeEEEECCCCDDDDAAAAMMMMkkkkSSSSEEEEzzzzbbbbuuuuTTTTMMMM000000005555ffffttttrrrrUUUUggggIIIIOOOO3333dddduuuu9999QQQQffff4444aaaa9999wwwwZZZZtttt////wwwwDDDD////RRRRdddd++++55559999wwwwOOOO8888++++5555AAAAbbbbMMMMNNNNwwwweeeeuuuu55553333vvvvnnnnPPPPPPPP666611114444vvvvDDDDYYYYNNNNccccNNNN5555ssssXXXXCCCC4444ttttLLLLNNNN22227777eeeessssppppZZZZvvvv1111++++7777ccccvvvvXXXXffff////wwwwccccrrrrqqqqwwww777700008888KKKKTTTTJJJJffffttttffffwwwwkkkkTTTTLLLLIIIIDDDDzzzz88881111VVVVGGGGMMMMSSSSqqqqppppQQQQMMMMddddqqqqooooMMMM0000UUUU22227777kkkkhhhhWWWWrrrrffff666677773333mmmm9999ffff1111PPPPKKKKssssuuuuDDDDJJJJNNNN7777VVVVpppp6666kkkk6666iiiittttxxxxuuuuHHHHHHHHQQQQCCCC33339999VVVVQQQQ7777WWWWQQQQffffVVVVuuuurrrrNNNNRRRRllllMMMMeeeeeeee1111JJJJwwwwSSSSqqqqFFFFOOOO5555bbbbOOOOddddrrrrCCCC55559999ooooffffffffUUUUppppooooRRRR8888KKKKiiiiggggiiiiRRRRTTTTFFFFppppyyyyCCCCGGGG5555llllOOOOMMMM9999JJJJIIIIeeeeaaaallllEEEEJJJJ3333RRRRGGGGffffQQQQZZZZZZZZAAAACCCCWWWWVVVVdddd0000TTTTjjjjXXXXYYYYFFFFkkkkAAAAppppIIIIFFFFxxxxooooeeee////jjjjtttt4444uuuuuuuuwwww1111MMMMbbbb4444ZZZZssss5555ccccrrrrHHHH11114444CCCCffffGGGGXXXXwwwwddddKKKKmmmmNNNNddddggggkkkkwwwwGGGGWWWWQQQQ2222ZZZZsssstttt66664444UUUUwwwwssss3333YYYYWWWW99995555llllwwww8888tttt5555OOOO8888dddd8888rrrruuuuSSSSJJJJooooNNNNRRRR1111DDDDOOOO8888++++uuuujjjj7777yyyyqqqqHHHHcccceeeeiiiiqqqqUUUUOOOObbbbEEEEkkkkOOOOAAAAmmmmFFFFLLLLRRRRccccHHHHRRRR++++yyyyVVVVJJJJIIIIVVVVnnnnjjjjnnnn9999llllBBBBUUUUGGGGggggwwwwppppddddCCCCyyyy3333ssssZZZZ5555BBBB9999ssssWWWWyyyynnnn2222ddddbbbbbbbbHHHHKKKKJJJJnnnnXXXXPPPPrrrryyyyvvvvooooffffQQQQbbbbKKKKWWWWvvvv////0000SSSSWWWW9999DDDDffffyyyyuuuugggg8888ssssHHHHJJJJFFFFaaaavvvv8888VVVVCCCCffffssssMMMMppppUUUUoooonnnnkkkkAAAA1111++++HHHHRRRRbbbb2222EEEEHHHH44442222////yyyy66666666hhhhbbbbkkkk7777ssssIIII9999HHHHmmmmMMMM22227777JJJJppppyyyy55555555GGGGkkkkTTTTvvvvBBBB5555YYYYOOOOQQQQttttssssZZZZ11119999aaaaVVVVUUUUVVVV5555jjjjKKKKqqqqbbbbnnnnHHHHKKKKOOOOOOOOkkkkPPPPccccAAAAwwww8888DDDDTTTTCCCCiiii77777777llllUUUUyyyyFFFFooooIIIIbbbb5555xxxxllllllllzzzzMMMMEEEETTTTwwwwddddJJJJUUUUuuuuEEEEXXXXvvvv6666KKKK3333UUUU1111////hhhhhhhhVVVViiii0000ddddEEEEIIIIOOOOzzzzqqqqkkkkIIIIJJJJ3333pppp77770000hhhhQQQQffffEEEEttttLLLL2222nnnn2222EEEE2222CCCCuuuuJJJJRRRR0000eeeeCCCCAAAAZZZZ4444YYYYqqqqssss00000000eeeewwwwuuuuhhhhIIIIRRRR++++7777VVVVLLLLFFFFttttMMMM////88883333pppp++++kkkkNNNNllllppppPPPPjjjj2222ZZZZPPPP7777bbbbiiiiHHHHXXXXIIIIffffttttqqqqHHHHttttTTTT4444ZZZZ9999OOOOWWWWkkkk8888vvvvVVVVWWWW++++YYYYjjjjkkkkZZZZuuuuCCCCqqqqmmmmkkkk6666ffff55554444hhhhmmmmPPPPSSSSkkkkssss9999wwwwWWWW11116666ggggOOOOffffFFFFooooeeeeeeee0000QQQQcccc8888ggggKKKKTTTTmmmmPPPPGGGGhhhhVVVV++++IIII////HHHHLLLLeeeeeeeettttXXXX5555HHHHzzzzCCCC7777zzzzggggmmmmggggWWWW0000mmmm88888888nnnnddddMMMMnnnn++++qqqqrrrrnnnnqqqqmmmmVVVVCCCCFFFFDDDDSSSSBBBBxxxxddddIIIIFFFFiiiizzzzoooo++++ttttZZZZVVVVKKKKEEEEjjjjOOOOTTTTuuuu1111nnnnDDDDMMMM8888vvvvVVVVXXXXYYYYQQQQeeeeyyyy8888hhhh8888++++VVVV2222BBBBxxxx6666DDDD3333UUUUJJJJ8888FFFF3333IIII1111FFFFWWWWhhhhTTTT0000ppppmmmmgggg5555++++ffffuuuueeeevvvvEEEEZZZZeeee6666PPPP++++ZZZZyyyyDDDDmmmm2222EEEE2222MMMMkkkkWWWWPPPPdddd0000TTTTbbbbllllQQQQkkkkCCCCFFFFnnnneeeennnnMMMM33335555TTTTTTTTggggpppp7777GGGGwwww2222nnnn2222XXXXBBBB2222XXXXttttSSSS3333XXXXppppWWWW33336666DDDDIIII9999ppppiiiiffff0000FFFFDDDD33337777kkkkrrrrbbbbooooDDDDWWWW3333jjjjhhhhOOOOAAAA5555++++EEEErrrrffff6666LLLLvvvv1111wwww////pppppppp////bbbbJJJJ++++GGGG++++jjjjiiiiQQQQmmmmnnnnzzzziiiiEEEEYYYYeeee6666++++IIIIffffOOOO////OOOOMMMMttttgggg========<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
T 2D DFT
our approach can be seen as a generalized form of the
sample-wise curriculum learning. It reduces to example-
selectionbysetting t(X) ,X .
T ∈{∅ }
Overview.Intherestofthispaper,wewilldemonstrate
Low-pass Filtering
that an algorithm drawn from our hypothesis dramatically ( r : radius of the filter)
improves the implementation efficiency and generalization <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""JJJJXXXXxxxxddddjjjjwwwwccccMMMMQQQQ00003333hhhhxxxxccccCCCCaaaaAAAAGGGGKKKKkkkkmmmmLLLLMMMMFFFFbbbb++++cccc===="""">>>>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======<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
abilityofcurriculumlearning.Wewillshowthatazero-cost
criterion pre-defined by humans is able to effectively mea- Inverse
2D DFT
surethedifficultylevelofdifferentpatternswithinimages.
Based on such simple criteria, even a surprisingly straight-
forward implementation of introducing ‘easier-to-harder’
Small
patterns yields significant and consistent improvements on r
thetrainingefficiencyofmodernvisualbackbones. (DFT:( da i) s L cro ew te- p Fa os us rF ieil rt e tr ri an ng s form) (b) Effects ofr <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""JJJJXXXXxxxxddddjjjjwwwwccccMMMMQQQQ00003333hhhhxxxxccccCCCCaaaaAAAAGGGGKKKKkkkkmmmmLLLLMMMMFFFFbbbb++++cccc===="""">>>>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======<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""JJJJXXXXxxxxddddjjjjwwwwccccMMMMQQQQ00003333hhhhxxxxccccCCCCaaaaAAAAGGGGKKKKkkkkmmmmLLLLMMMMFFFFbbbb++++cccc===="""">>>>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======<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
Fig.2:Low-passfiltering.Following[61],weadoptacircularfilter.
4 THE EFFICIENTTRAIN APPROACH
80
To obtain a training curriculum following our aforemen- 80 r=40(low-pass) 70 r=60(low-pass)
tionedhypothesis,weneedtosolvetwochallenges:1)iden- r=80(low-pass)
tifying the ‘easier-to-learn’ patterns and designing trans- 60 78 r= 1(baseline)
formation functions to extract them; 2) establishing a cur- 50 r=40(low-pass) 76
riculumlearningscheduletoperformthesetransformations r=60(low-pass) 74
r=80(low-pass)
40
dynamically during training. This section will demonstrate r=90(low-pass)
r= (baseline) 72
that a proper transformation for 1) can be easily found in 30 1
0 50 100 150 200 250 300 70
both the frequency and the spatial domain, while 2) can be TrainingEpoch 100 150 200 250 300
addressedwithagreedysearchalgorithm.Implementation 75 80 r=40(low-pass)
detailsoftheexperimentsinthissection:seeAppendixA. 70 r=60(low-pass)
r=90(low-pass)
65 78 r= 1(baseline)
60 76
4.1 Easier-to-learnPatterns:FrequencyDomain
55
74
Image-based data can naturally be decomposed in the fre- 50
r=40(low-pass) 72
quency domain [75], [76], [77], [78]. In this subsection, we 45 r=60(low-pass)
r= (baseline)
revealthatthepatternsinthelower-frequencycomponents 40 0 25 50 75 101 0 125 70 100 150 200 250 300
of images, which describe the smoothly changing contents,
Train & Val. : Low-pass Filtered Images
arerelativelyeasierforthenetworkstolearntorecognize.
Fig.3:Ablationstudyresultswithlow-passfiltering(r:bandwidthof
Ablationstudieswiththelow-passfilteredinputdata. thefilter,seeFigure2fordetails).Weablatethehigher-frequencycom-
We first consider an ablation study, where the low-pass fil- ponentsoftheinputsforaDeiT-Small[19],andpresentthecurvesof
teringisperformedonthedataweuse.AsshowninFigure validationaccuracyv.s.trainingepochsonImageNet-1K.Wehighlight
theseparationpointsofthecurveswithblackboxes.
2 (a), we map the images to the Fourier spectrum with
the lowest frequency at the centre, set all the components
outside a centred circle (radius: r) to zero, and map the and the training process on low-pass filtered data moves
spectrumbacktothepixelspace.Figure2(b)illustratesthe towards the end of training. To explain these observations,
effects of r. The curves of accuracy v.s. training epochs on we postulate that, in a natural learning process where the
top of the low-pass filtered data are presented in Figure 3. input images contain both lower- and higher-frequency informa-
Here both training and validation data is processed by the tion, a model tends to first learn to capture the lower-frequency
filtertoensurethecompatibilitywiththei.i.d.assumption. components, while the higher-frequency information is gradually
Lower-frequency components are captured first. The exploitedonthebasisofthem.
modelsinFigure3areimposedtoleverageonlythelower- More evidence. Our assumption can be further con-
frequencycomponentsoftheinputs.However,anappealing firmed by a well-controlled experiment. Consider train-
phenomenonarises:theirtrainingprocessisapproximately ing a model using original images, where lower/higher-
identicaltotheoriginalbaselineatthebeginningoftraining. frequencycomponentsaresimultaneouslyprovided.InFig-
Although the baseline finally outperforms, this tendency ure 4, we evaluate all the intermediate checkpoints on
starts midway in the training process, instead of from the low-pass filtered validation sets with varying bandwidths.
very beginning. In other words, the learning behaviors at Obviously, at earlier epochs, only leveraging the low-pass
earlier epochs remain unchanged even though the higher- filteredvalidationdatadoesnotdegradetheaccuracy.This
frequencycomponentsofimagesareeliminated.Moreover, phenomenonsuggeststhatthelearningprocessstartswitha
considerincreasingthefilterbandwidthr,whichpreserves focusonthelower-frequencyinformation,eventhoughthe
progressively more information about the images from the modelisalwaysaccessibletohigher-frequencycomponents
lowestfrequency.Theseparationpointbetweenthebaseline during training. Furthermore, in Table 1, we compare the
)%(ycaruccAnoitadilaVEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 5
80 originalinputs?Asamatteroffact,thisideaisfeasible,and
Fig. 4: Performing low-pass fil-
70 wemayhaveatleasttwoapproaches.
teringonlyonthevalidationin-
60 puts (other setups are the same • 1) Down-sampling. Approximating the low-pass fil-
50 as Figure 3). We train a model tering in Table 2 with image down-sampling may be a
40 using the original images with- straightforward solution. Down-sampling preserves much
30 r=40(low-pass) out any filtering (i.e., containing of the lower-frequency information, while it quadratically
20 r=60(low-pass) bothlower-andhigher-frequency
r=80(low-pass) components),andevaluateallthe saves the computational cost for a model to process the in-
10
r= 1(baseline) intermediatecheckpointsonlow- puts[79],[80],[81].However,itisnotanoperationtailored
0
0 50 100 150 200 250 300 passfilteredvalidationsets with
Train: Original ImaT gr ea si n (i Lng owE +po Hch igh Frequency); varyingbandwidths. forextractinglower-frequencycomponents.Theoretically,it
preservessomeofthehigher-frequencycomponentsaswell
Val. : Low-pass Filtered Images;
(see: Proposition 1). Empirically, we observe that this issue
TrainingEpoch 20th 50th 100th 200th 300th(end)
degradestheperformance(see:Table19(b)).
Low-passFilteredVal.Set 46.9% 58.8% 63.1% 68.6% 71.3% • 2) Low-frequency cropping (see: Figure 5). We pro-
High-passFilteredVal.Set 23.9% 43.5% 53.5% 64.3% 71.3%
pose a more precise approach that extracts exactly all the
TABLE1:Comparisons:evaluatingthemodelinFigure4onlow/high-
lower-frequencyinformation.ConsidermappinganH W
passfilteredvalidationsets.Notethatthemodelistrainedusingthe ×
originalimageswithoutanyfiltering.Thebandwidthsofthelow/high- image X into the frequency domain with the 2D discrete
pass filters are configured to make the finally trained model (300th Fouriertransform(DFT),obtaininganH W Fourierspec-
epoch)havethesameaccuracyonthetwovalidationsets(i.e.,71.3%). trum, where the value in the centre den× otes the strength
of the component with the lowest frequency. The positions
accuracies of the intermediate checkpoints on low/high- distant from the centre correspond to higher-frequency. We
pass filtered validation sets. The accuracy on the low-pass crop a B B patch from the centre of the spectrum, where
×
filtered validation set grows much faster at earlier training B is the window size (B <H,W). Since the patch is still
stages,eventhoughthetwofinalaccuraciesarethesame. centrosymmetric,wecanmapitbacktothepixelspacewith
Frequency-basedcurricula.Returningtoourhypothesis theinverse2DDFT,obtainingaB ×B newimageX c,i.e.,
in Section 3, we have shown that lower-frequency com-
ponents are naturally captured earlier. Hence, it would be X c = F−1 ◦CB,B ◦F(X) ∈RB×B, X ∈RH×W, (1)
straightforward to consider them as a type of the ‘easier- where , −1 and B,B denote 2D DFT, inverse 2D DFT
to-learn’ patterns. This begs a question: can we design a andB2F cenF tre-croppC
ing.Thecomputationalorthetimecost
training curriculum, which starts with providing only the
foraccomplishingEq.(1)isnegligibleonGPUs.
lower-frequencyinformationforthemodel,whilegradually Notably, X achieves a lossless extraction of lower-
c
introducing higher-frequency components? We investigate
frequencycomponents,whilethehigher-frequencypartsare
thisideainTable2,whereweperformlow-passfilteringon strictly eliminated. Hence, feeding X into the model at
c
the training data only in a given number of the beginning
earlier training stages can provide the vast majority of the
epochs.Therestofthetrainingprocessremainsunchanged.
usefulinformation,suchthatthefinalaccuracywillbemini-
mallyaffectedorevennotaffected.Incontrast,importantly,
Curricula(ep:epoch) FinalTop-1Accuracy
Low-passFiltered Original (r:filterbandwidth) duetothereducedinputsizeofX c,thecomputationalcost
TrainingData TrainingData r=40 r=60 r=80 for a model to process X is able to be dramatically saved,
c
1st–300thep – 78.3% 79.6% 80.0% yieldingaconsiderablymoreefficienttrainingprocess.
1st–225thep 226th–300thep 79.4% 80.2% 80.5%
1st–150thep 151th–300thep 80.1% 80.2% 80.6% Proposition 1. Suppose that X c= F−1 ◦CB,B ◦F(X), and
1st– 7 –5th ep 76 1t sh t–– 33 00 00 tt hh ee pp 80.3% 80.3%80 (. b4 a% seline)80.6% it sha rt eaX lizd ed= bD yB a,B co( mX m) o, nwh ine tr ee rpB ol× atiB ond ao lw gon r- is ta hm mpl (i en .gg .,D nB ea,B re( s· t)
,
TABLE2:Resultswiththestraightforwardfrequency-basedtraining bilinearorbicubic).Thenwehavetwoproperties.
curricula(DeiT-Small[19]onImageNet-1K).Observation:onecanelim- • a) X is only determined by the lower-frequency spectrum
inatethehigher-frequencycomponentsoftheinputsin50-75%ofthe c
trainingprocesswithoutsacrificingthefinalaccuracy(see:comparisons of X (i.e., CB,B ◦F(X)). In addition, the mapping to X c is
betweenthebaselineandtheunderlineddata). reversible.Wecanalwaysrecover CB,B ◦F(X)fromX c.
• b) X has a non-zero dependency on the higher-frequency
d
Learning from low-frequency information efficiently. spectrumofX (i.e.,theregionsoutside B,B (X)).
C ◦F
At the first glance, the results in Table 2 may be less
Proof.See:AppendixC.
dramatic, i.e., by processing the images with a properly-
configured low-pass filter at earlier epochs, the accuracy is Our claims can be empirically supported by the results
moderately improved. However, an important observation in Table 3, where we replace the low-pass filtering oper-
isnoteworthy:thefinalaccuracyofthemodelcanbelargely ation in Table 2 with low-frequency cropping. Even such a
preservedevenwithaggressivefiltering(e.g.,r=40,60)per- straightforwardimplementationyieldsfavorableresults:the
formedin50-75%ofthetrainingprocess.Thisphenomenon trainingcostcanbesavedby 20%whileacompetitivefinal
∼
turnsourattentiontotrainingefficiency.Atearlierlearning accuracyispreserved.Thisphenomenoncanbeinterpreted
stages,itisharmlesstotrainthemodelwithonlythelower- via Figure 6: at intermediate training stages, the models
frequencycomponents.Thesecomponentsincorporateonly trained using the inputs with low-frequency cropping can
a selected subset of all the information within the original learn deep representations similar to the baseline with a
input images. Hence, can we enable the model to learn from significantly reduced cost, i.e., the bright parts are clearly
them efficiently with less computational cost than processing the abovethewhitelines.
)%(ycaruccAnoitadilaVEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 6
Curricula(ep:epoch) FinalTop-1Accuracy/RelativeComputationalCostforTraining(comparedtothebaseline)
2D DFT Low-frequency OriginalTraining DeiT-Small[19] Swin-Tiny[4]
Cropping(B2) Data(B=224) B=96 B=128 B=160 B=192 B=96 B=128 B=160 B=192
1st–300thep – 70.5%/0.18 75.3%/0.31 77.9%/0.49 79.1%/0.72 73.3%/0.18 76.8%/0.32 78.9%/0.50 80.5%/0.73
Cropping 1st–225thep 226th–300thep 78.7%/0.38 79.6%/0.48 80.0%/0.62 80.3%/0.79 80.0%/0.38 80.5%/0.49 81.0%/0.63 81.2%/0.80
(window size: <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""EEEEppppKKKKqqqqOOOOuuuukkkk2222YYYY8888uuuuXXXX0000CCCCFFFFUUUUNNNNCCCCBBBBZZZZyyyyggggpppp2222ppppyyyyEEEE===="""">>>>AAAAAAAAAAAAIIIIMMMMHHHHiiiiccccppppVVVVXXXXLLLLbbbbttttNNNNQQQQEEEEJJJJ22222222QQQQEEEEpppp4444ppppbbbbBBBBkkkk4444xxxxJJJJVVVVYYYYkkkkGGGGDDDDEEEEyyyyHHHHRRRRZZZZQQQQUUUUbbbbFFFFiiiiwwwwKKKKaaaattttppppKKKKTTTTVVVVXXXXZZZZzzzzkkkk1111rrrrxxxxSSSS////ssss66665555YYYYqqqqyyyyoooo7777PPPPYYYYAAAAssssffffwwwwNNNNffffAAAACCCCrrrrEEEEDDDDvvvvooooIIIIzzzzYYYY6666dddd5555OOOO66662222wwww1111XXXXQQQQ888899998888yyyyZZZZOOOO6666999977777777cccchhhhzzzzEEEE22222222aaaaPPPP5555aaaaWWWWVVVV22227777ccccvvvvFFFFVVVVaaaavvvvVVVV2222++++cccc////ffffeeee////QQQQeeeeVVVVttttYYYYdddd7777SSSSZZZZjjjjGGGGjjjjmmmmoooo6666ooooRRRRffffGGGGBBBB7777aaaaVVVVKKKKMMMM8888NNNNVVVVFFFFOOOO77772222llllMMMMHHHHUUUUaaaawwwwssss3333////bbbbUUUUvvvvtttt11119999zzzzeeeevvvv7777ZZZZyyyyppppOOOO3333DDDDDDDDYYYY1111RRRReeeeRRRROOOOvvvvKKKKttttkkkk8888DDDDttttuuuuIIII6666llllooooTTTTqqqquuuuVVVVHHHHqqqqvvvv++++qqqq33331111llllnnnnZZZZ9999llllbbbbTTTTWWWW8888XXXXFFFFccccqqqqZZZZoooo1111UUUUxxxx5555jjjjWWWWqqqqjjjjnnnnQQQQppppXXXXyyyyZZZZyyyyddddccccWWWW////llllEEEELLLLWWWWppppTTTTSSSSAAAA6666llll5555JJJJOOOOiiiiggggDDDDRRRRkkkkjjjjyyyyxxxxKKKK8888BBBB5555SSSSnnnnUUUUyyyyKKKKooooDDDDuuuuiiiiHHHHnnnnQQQQxxxxJJJJFFFFffffWWWWFFFFffffWWWWppppDDDDNNNNssssUUUUKKKKAAAAWWWWEEEEBBBBWWWW0000XXXXvvvvyyyyffff4444OOOOssssyyyy1111AAAAbbbb6666ZZZZMMMMxxxxFFFFrrrrBBBB111144448888////MMMMWWWWwwwwNNNNGGGGggggDDDDNNNNiiiiFFFFwwwwMMMMWWWWTTTT2222ZZZZsssshhhh6666KKKKssssyyyyssssnnnnccccffffddddEEEE00007777eeee2222wwwwXXXX++++2222zzzzmmmmXXXXDDDD66662222mmmmUUUU2222ggggXXXX2222QQQQ2222QQQQVVVV7777XXXXjjjjWWWWDDDDRRRR1111aaaaEEEEttttiiiiccccBBBBFFFFTTTTJJJJBBBBqqqqOOOOzzzzssssllllZZZZUUUUsssskkkkKKKK77779999wwwwYYYYiiiiUUUUqqqqDDDDIIIIYYYYKKKKOOOO5555TTTTbbbbWWWWYYYY8888iiiiOOOOWWWWAAAA7777yyyybbbbIIIIhhhhNNNNIIIIrrrrFFFFzzzzbbbbiiii1111ZZZZ////yyyy1111IIII1111vvvvKKKK3333kkkk2222NNNNTTTT++++llllMMMMYYYYnnnnQQQQ1111WWWWrrrrkkkkjjjj5555vvvvyyyyJJJJhhhhnnnn55555555UUUU6666RRRRxxxxyyyyhhhhtttt++++EEEEhhhhTTTTGGGGCCCCnnnn8888++++////iiii22225555hhhh7777gggg7777ssssggggzzzzHHHHmmmm7777NNNN0000QQQQzzzzkkkkTTTTyyyyttttAAAAVVVVeeeeGGGG6666yyyyccccBBBBbbbbYYYYzzzzLLLLqqqq2222KKKKoooojjjjxxxxFFFF1111bbbbOOOOccccccccoooo44446666IIII9999xxxxDDDDDDDD0000OOOOMMMMJJJJ7777vvvvuuuuFFFFjjjjKKKKmmmmggggppppvvvvkkkkGGGGWWWWddddMMMMwwwwOOOOPPPPDDDDMMMMqqqqttttwwwwkkkk99997777TTTTWWWW6666llllvvvv5555ooooddddZZZZttttXXXXRRRRAAAAAAAAMMMM6666iiiiCCCCooooVVVV4444uuuu9999IIIIXXXXNNNNhhhhCCCCzzzz9999hhhh5555hhhhNNNNyyyyHHHHiiiiUUUUttttLLLLhhhhrrrrmmmmSSSSEEEEKKKK7777JJJJJJJJHHHH8888BBBBqqqqSSSSUUUUTTTTssss11118888hhhhZZZZssssvvvv5555ZZZZ1111PPPPPPPPDDDDzzzzMMMM7777yyyyaaaaccccvvvv8888ssssRRRRXXXXvvvvkkkkPPPPuuuuwwwwDDDDeeee1111ggggMMMMoooozzzzLLLLSSSSeeeePPPPppppLLLLffffIIIIVVVVyyyyMMMMnnnnAAAAVVVVcccckkkk6666eeeeZZZZYYYYvvvvnnnnnnnnEEEE////tttt9999RRRRTTTT3333FFFFkkkkPPPP8888LLLLzzzzUUUU6666TTTTkkkk11116666BBBBkkkkkkkkJJJJeeeeddddRRRRrrrrccccCCCCvvvvLLLL333344445555bbbb4444OOOOKKKK9999KKKKffff8888TTTTmmmmJJJJCCCC2222MMMMYYYYyyyynnnn9999wwwwttttiiii6666ffffqqqqqqqqmmmmddddKKKKEEEEddddKKKKDDDDxxxxNNNNGGGG5555ggggssss11112222ddddDDDD2222LLLLIIIIrrrrQQQQvvvvZZZZ6666eeeeWWWWcccc4444aaaannnnttttwwwwgggg7777jjjjJJJJ333333338888LLLLEEEEQQQQmmmm0000BBBBvvvvooooyyyy4444uuuuvvvvllllOOOOZZZZuuuuiiiiLLLL0000eeeecccc6666cccc4444RRRRddddnnnn7777jjjjppppxxxxZZZZffffffffHHHHYYYYssss7777hhhhzzzzTTTTAAAAffffGGGGaaaaHHHHHHHHOOOO6666IIII9999kkkkQQQQkkkkCCCCFFFFnnnnddddnnnnffffffffKKKKmmmmnnnnBBBBbbbb2222GGGGrrrrWWWW6666WWWWaaaauuuu////eeee1111HHHHddddbbbbuuuuSSSS33336666CCCCoooo9999ppppiiiiffff0000FFFFDDDD33337777kkkkrrrrbbbbppppDDDDeeee3333gggghhhhHHHHDDDDoooojjjjDDDD7777TTTTFFFF////ppppaaaa++++llllbbbb6666XXXXvvvvppppZZZZ++++ppppVVVVBBBBllll5555ddddyyyymmmm0000cccc00009999ppppTTTT++++////ggggNNNN++++IIIIZZZZLLLLtttt<<<<////llllaaaatttteeeexxxxiiiitttt>>>>B ⇥ B ) 1st–150thep 151th–300thep 79.2%/0.59 79.8%/0.66 80.3%/0.75 80.4%/0.86 80.9%/0.59 80.9%/0.66 81.2%/0.75 81.3%/0.86
Inverse 2D DFT 1st– 75th ep 76th–300thep 79.6%/0.79 80.2%/0.83 80.4%/0.87 80.3%/0.93 81.2%/0.79 81.2%/0.83 81.3%/0.88 81.3%/0.93
– 1st–300thep 80.3%/1.00(baseline) 81.3%/1.00(baseline)
Fig.5:Low-frequencycroppinginthe TABLE3:ResultsonImageNet-1Kwiththelow-passfilteringinTable2replacedbythelow-frequency
frequencydomain(B2:bandwidth). cropping,whichyieldscompetitiveaccuracywithasignificantlyreducedtrainingcost(see:underlineddata).
Fig. 6: CKA feature similarity heatmaps plain why weaker-to-stronger augmentation is effective.
[82],[83]betweentheDeiT-S[19]trained However, note that this does not mean that our technical
250
usingtheinputswithlow-frequencycrop-
innovationsover[7],[58]arelimited.Ourcontributionsalso ping (B=128,160) and the original in-
200
puts (B=224). The X and Y axes index incorporatemanyotherimportantaspects(see:Section2).
150 training epochs (scaled according to the
computational cost of training). Here we 4.3 AUnifiedTrainingCurriculum
100 feedthesameoriginalimagesintoallthe
models (including the ones trained with Finally,weintegratethetechniquesproposedinSections4.1
50
B=128,160) and take the features from and 4.2, and design a unified efficient training curriculum.
0 the final layer. The 45◦ lines are high- Inspecific,wefirstsetthemagnitudemofRandAugtobea
0 Epo1 ch00 (B2 =00 123 80 )0 0 Ep1 o0 c0 h(B=20 10
60)
300lightedinwhite.
linearfunctionofthetrainingepocht:m=(t/T) m 0,with
×
Curricula(ep:epoch) FinalTop-1Accuracy otherdataaugmentationtechniquesunchanged.Despitethe
Weaker RandAug Low-frequency Original (m:magnitudeofRandAug) simplicity, this setting yields consistent and significant em-
RandAug (m=9) (B=128) (B=224) m=1 m=3 m=5 m=7 m=9
piricalimprovements.Notethatweadoptm 0=9following
1st– 151th– – 1st–300thep 80.4% 80.6% 80.7% 80.5% 80.3%
thecommonpractice[4],[18],[19],[20],[21].
150thep 300thep 1st–150thep 151th–300thep 80.2% 80.2% 80.2% 79.9% 79.8%
Then we leverage a greedy-search algorithm to deter-
TABLE 4: Performance of the data-augmentation-based curricula minethescheduleofB duringtraining(i.e.,thebandwidth
(DeiT-Small[19]onImageNet-1K).Wetestreducingthemagnitudeof
forlow-frequencycropping).AsshowninAlgorithm1,we
RandAugat1st-150thtrainingepochs(m=9referstothebaselines).
dividethetrainingprocessintoseveralstagesandsolvefor
avalueofBforeachstage.Thealgorithmstartsfromthelast
4.2 Easier-to-learnPatterns:SpatialDomain
stage, minimizing B under the constraint of not degrading
Apart from the frequency domain operations, extracting the performance compared to the baseline (trained with a
‘easier-to-learn’patternscanalsobeattainedthroughspatial fixed B = 224). In our implementation, we only execute
domain transformations. For example, modern deep net- Algorithm 1 for a single time. We obtain a schedule on top
works are typically trained with strong and delicate data of Swin-Tiny [4] on ImageNet-1K under the standard 300-
augmentation techniques [4], [18], [19], [21], [84], [85], [86]. epoch training setting [4], and directly adopt this schedule
Wearguethattheaugmentedtrainingdataprovidesacom- forothermodelsorothertrainingsettings.
binationofboththeinformationfromoriginalsamplesand
Algorithm1TheGreedy-searchAlgorithm.
theinformationintroducedbytheaugmentationoperations.
The original patterns may be ‘easier-to-learn’ as they are 1: Input:NumberoftrainingepochsT andtrainingstages
N (i.e.,T/N epochsforeachstage).
drawn from real-world distributions. This assumption can
2: Input:Baselineaccuracya0(with2242images).
be supported by the observation that data augmentation is
3:
Tosolvefor:ThevalueofBforithtrainingstage:Bˆ
i.
mainlyinfluentialatthelaterstagesoftraining[87]. 4: Initialize:Bˆ 1=...=Bˆ N =224
To this end, following our generalized formulation of 5: fori=N 1to1do
curriculum learning in Section 3, a curriculum may adopt 6: Bˆ i= − minimize B,
B1=...=Bi=B,Bj=Bˆ j,i<j≤N
a weaker-to-stronger data augmentation strategy during s.t.ValidationAccuracy(B1,...,BN) a0
≥
training.WeinvestigatethisideabyselectingRandAug[88] 7: endfor
as a representative example, which incorporates a family 8:
Output:Bˆ 1,...,Bˆ
N
ofcommonspatial-wisedataaugmentationtransformations
Epochs Low-frequencyCropping RandAug
(rotate,sharpness,shear,solarize,etc.).InTable4,themag-
1st –180th B=160
nitudeofRandAugisvariedinthefirsthalftrainingprocess. m=0→9
One can observe that this idea improves the accuracy, and
181th–240th B=192
Increaselinearly.
241th–300th B=224
thegainsarecompatiblewithlow-frequencycropping.
TABLE 5: The EfficientTrain curriculum obtained from Algorithm 1.
Notably, the observation from Table 4 is consistent with
We only execute Algorithm 1 once, and directly adopt this resulting
[7], [58]. We do not introduce weaker-to-stronger data curriculumfordifferentmodelsandtrainingsettings.Thestandard300-
augmentation for the first time. In contrast, our contribu- epoch training pipeline [4] is considered when executing Algorithm
tions here lie in demonstrating that ‘easier-to-learn’ pat- 1,andhencethecurriculumpresentedhereadoptsthisconfiguration.
However, EfficientTrain can easily adapt to varying epochs and final
terns can be identified and extracted through the lens of
inputsizesutilizingsimplelinearscaling(see:Table10&AppendixB).
both frequency and spatial domains. This finding not only
enhances the completeness of our generalized curriculum Derived from the aforementioned procedure, our fi-
learning framework, but also offers novel insights to ex- nally proposed learning curriculum is presented in Table
)enilesab,422=B(hcopEEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 7
5. We refer to it as EfficientTrain. Notably, although being Amoreefficientalgorithmtosolveforthecurriculum.
simple,EfficientTrainiswell-generalizableandsurprisingly Driven by the aforementioned analysis, we propose a new
effective. In the context of training or pre-training visual algorithmtodeterminethescheduleofBinthecurriculum,
backbones on large-scale natural image databases (e.g., on as shown in Algorithm 2. This new approach is not only
ImageNet-1K/22K [17]), EfficientTrain can be directly ap- moreefficientthanAlgorithm1,butalsooutperformsAlgo-
plied to state-of-the-art deep networks without additional rithm 1 in terms of its resulting curriculum. To be specific,
hyper-parameter tuning, and improves their training effi- Algorithm2introducestwomajorinnovations:
ciency significantly. The gains are consistent across various • Newformulation:computational-constrainedsearching.
network architectures, different computational budgets for Weconsideradifferentconstrainedoptimizationproblem
training, supervised/self-supervised learning algorithms, from Algorithm 1. The constraint is to save the training
andvaryingamountsoftrainingdata. cost by a pre-defined ratio compared with the baseline
(specified by β in Algorithm 2), while the optimization
5 EFFICIENTTRAIN++ objective is to maximize the final accuracy. Similar to Al-
gorithm1,wesolvethisproblembydividingthetraining
In this section, we propose an enhanced EfficientTrain++
process into N stages, and solving for a B value for
approach. EfficientTrain++ improves the vanilla Efficient-
each stage. However, here we allocate 1/N of the total
Train from two aspects, i.e., 1) saving the non-trivial com-
training cost to each stage, and then simultaneously vary
putational cost for executing Algorithm 1, and 2) reducing
B and the number of training steps at each stage with
theincreasedCPU-GPUinput/output(I/O)costcausedby
this training budget fixed. That is to say, given a pre-
the low-frequency cropping operation. We address these
definedbatchsize,whenB isrelativelysmall,thecostfor
issues by introducing a low-cost algorithm to determine
a model to process a mini-batch of training data is small,
the schedule of B (Section 5.1), as well as an efficient low-
andweupdatethemodelformoretimesusingmoremini-
frequencydown-samplingoperation(Section5.2).
batches of data, yet the overall amount of computation
Inadditiontothesemethodologicalinnovations,wefur-
is unchanged. In contrast, when B is large at a training
ther present two simple but effective implementation tech-
stage, the number of model updating times needs to be
niques for EfficientTrain++ (Section 5.3). Neither technique
relativelylesssincethetrainingbudgetisfixed.Notethat
isanindispensablecomponentofourmethod,butadopting
the learning rate schedule for each training stage always
themenablesEfficientTrain++toachieveamoresignificant
remainsunchanged.
practical training speedup, e.g., by 1) taking full advantage
ofmoreGPUsforlarge-scaleparalleltrainingand2)reduc- • Performing sequential searching. We solve for the B
values from the first training stage to the last one, where
ingthedatapre-processingloadsforCPUs/memory.
the solving of B at any stage is based on the checkpoint
from the previous stage. Specifically, given a training
5.1 Computational-constrainedSequentialSearching
stage, we execute it from the previous checkpoint with
TheunderlyinglogicbehindAlgorithm1isstraightforward. severalcandidateB valuesrespectively,andfine-tunethe
Aconstrainedoptimizationproblemisconsidered.Thecon- obtained models with the original images (B =224) for
straint is not to degrade the final accuracy of the model, a small number of epochs (T in Algorithm 2). We select
ft
whiletheoptimizationobjectiveistominimizethetraining thebestB bycomparingthevalidationaccuracyofthese
cost. We solve this problem in multiple steps (Algorithm 1, fine-tunedmodels,whichwefindisaneffectiveproxyto
lines 5-7), where we always try to reduce as much training reflecttheinfluenceofdifferentB onthefinalaccuracy.
costaspossibleateachstep.
In our implementation, Algorithm 2 is executed on top
EfficiencyofAlgorithm1.However,Algorithm1suffers
of Swin-Tiny [4] on ImageNet-1K, where we set β = 2/3,
from a high computational cost. First, the final accuracy
is considered as the constraint. Hence, given any possible
T 0=300andT ft=10.Theresultingcurriculumispresented
in Table 6, which is named as EfficientTrain++. We directly
combinationof B 1,...,B N ,weneedtotrainthemodelto
{ } adoptthecurriculuminTable6forothermodelsandother
convergence,obtainthefinalvalidationaccuracy,andverify
trainingsettingswithoutexecutingAlgorithm2again.
whether B 1,...,B N satisfiestheconstraint.Second,such
{ }
verification always needs to train the model from scratch, Algorithm 2 The Computational-constrained Sequential Searching
i.e.,sincewesolveforB valuesfromthelasttrainingstage Algorithm.
to the first one, we need to alter B for certain beginning 1: Input: Training epochs T =βT0 (T0: baseline training epochs;
stagesoftrainingwheneverweexecuteline6ofAlgorithm 0<β<1:pre-definedratioofsavingtrainingcost).Numberof
1 ha( vi. ee., Nch ta rn ag inin ing gB st1 a, g. e. s., aB ni d). MIn g ce an ne dr ia dl a, ta ess vu am lue esth fa ot
r
w Be
.
2: Itr na pin ui tn :g Lest aa rg ne ins gN r. aP tero sx cy hefi dn ue l- etu αn li rn ,g we hp eo rc eh αs l tT r 1f :t t. 2(0 ≤t1<t2 ≤1)
denotesacontinuoussegment.
Executing Algorithm 1 will train the model from scratch 3: Input:RandominitializedmodelparameterizedbyΘ.
to convergence for (N+M 1) times, which may in- 4: Tosolvefor:ThevalueofBforithtrainingstage:Bˆ i.
∼ −
troducenon-trivialcomputationalandtimecosts.Although 5: fori=1toN 1do
this drawback does not interfere with directly utilizing the 6: Bˆ i=argmax− ValidationAccuracy(Θ(cid:98)i B),
B
r be rs ou al dti ln yg -apcu pr lir cic au bl lu em effifr co iem ntA trl ag io nr ii nth gm tec1 ha ns iqa un e,o tf hf- eth le im-s ih te el df s.t. ΘΘ(cid:98) ii B == TF rin aie n-t (u Θne( BΘ 2i B
,
T|22 F4 LO2, PsT (2ft 2e 4p 2)o ec ph os, chα sl i ,r /N α: l( ri/N+Tft/T)) ),
B | N· FLOPs(B2) (i−1)/N:i/N
e df efi sc igie nnc ay to aif loA rl eg dor ci uth rrm icu1 lum may foh rin nd ee wr f gu et nu ere raw lizo er dks c, ue. rg r. i, cuto - 7 8: : enΘ df← orΘi Bˆ i
lumlearningapproachesorforotherrelatedmethods. 9:
Output:Bˆ 1,...,Bˆ
NEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 8
ComputationalBudget Low-frequency modelinputsneedtobetransferredtoGPUs.Therefore,the
RandAug
forTraining Cropping CPU-GPUI/Ocostiseffectivelyreduced.
0%–20% B=96
m=0→9
20%–60% B=160 Increaselinearly.
60%–100% B=224 5.3 ImplementationTechniquesforEfficientTrain++
TABLE6:TheEfficientTrain++curriculum.Notably,thecomputation- Facilitatinglarge-scaleparalleltraining:earlylargebatch.
constrainedformulationofAlgorithm2isadoptedhere,i.e.,thevalues InTable6,thebatchsizeisfixedduringthewholetraining
ofB,m,andthelearningrateareconfiguredconditionedonhowmuch
process.Nevertheless,ourmethodalwaysstartsthetraining
ofthetotaltrainingcosthasbeenconsumed.Besides,tomeasurethe
total length of the training process, we convert the required compu- withasmallB (e.g.,B=96),wherethecomputationalcost
tationfortrainingintothenumberofthestandardtrainingepochsin for each mini-batch is dramatically reduced compared to a
baselines(e.g.,withtheoriginal2242inputs),andreportthisequivalent
largerB (e.g.,B=224).Thispropertyisharmlessontypical
numberofepochsfortheeaseofunderstanding.
settings of the training hardware (e.g., training a model on
16GPUs).However,ifweconsideranincreasingnumber
Importantly, different from Table 5, EfficientTrain++ is
≤
ofGPUs(e.g.,32/64GPUsormore),thesmallerB atearlier
based on the computational-constrained formulation intro-
duced by Algorithm 2. In other words, the values of B, m, training stages usually yields a bottleneck that inhibits the
efficientimplementationofEfficientTrain++.
and the learning rate are determined conditioned on how
This issue can be addressed by simultaneously increas-
much of the total computational budget for training has
been consumed, i.e., their schedules are defined on top of ing the batch size and the learning rate when B is small.
For one thing, such a modification has been shown to be
the computational cost. Built upon this characteristic, our
aneffectiveapproximationofthelearningprocesswiththe
EfficientTrain++ curriculum can flexibly adapt to a varying
original smaller batch size [91], [92], and hence the final
number of total training budgets. Notably, to measure the
performance of the model will not be degraded (see: Table
totalamountoftrainingcostofourmethod,weconvertthe
13). For another, the per-batch training cost for small B is
required computation for training into the number of the
standardtrainingepochsinbaselines(e.g.,withtheoriginal increased,suchthatthescalingefficiencyofEfficientTrain++
2242inputs),anddirectlyreportequivalentepochnumbers. with respect to the GPU number is significantly improved.
In our implementation, we use an upper-bounded square
Thissetupisadoptedfortheeaseofunderstanding.
rootlearningratescalingrule:
5.2 EfficientLow-frequencyDown-sampling (cid:32) (cid:115) (cid:33)
LRmax =min LRmax
BSB
, LRmax , (2)
As aforementioned, the low-frequency cropping operation B 224× BS224
in Figure 5 is able to extract all the lower-frequency com-
ponents while eliminating the rest higher-frequency in- where LRmax denotes the maximum value of the learning
B
formation. However, it suffers from a high CPU-GPU in- rate schedule (in this paper, we adopt the cosine annealing
put/output (I/O) cost. This issue is caused by the 2D DFT schedule with a linear warm-up [4], [18]) corresponding to
and inverse 2D DFT in Figure 5, which are unaffordable B, and BSB denotes the batch size corresponding to B.
for CPUs and have to be performed on GPUs. Hence, we We set LRmax to be the upper-bound of LRmax, since an
B
need to transfer the original images (e.g., 2242) from CPUs excessively large learning rate usually leads to an unstable
to GPUs. Consequently, the demand for CPU-to-GPU I/O trainingprocess.
throughput will increase by the same ratio as the training Reducing data pre-processing loads: replay buffer. In
speedup,e.g.,by>5 withB=96.Weempiricallyfindthat our method, the computational cost for learning from each
×
this high I/O cost is an important bottleneck that inhibits trainingsampleisquadraticallysavedwhenB issmall.As
theefficientimplementationofourmethod. a matter of fact, this reduction in learning cost comes with
Low-pass filtering + image down-sampling. Inspired the demand for a higher data pre-processing throughput,
bythisissue,weproposeanefficientapproximationoflow- i.e., the speed of preparing training inputs needs to keep
frequency cropping. Our method is derived by improving up with the model training speed on GPUs. Therefore, Ef-
the image down-sampling operation. As demonstrated by ficientTrain++mayincreasetheloadsforthehardwarelike
Proposition1,thedrawbackofdown-samplingisthatitcan- memory or CPUs, compared with the baseline, which may
not strictly filter out all the higher-frequency information. affect the practical training efficiency of our method. To al-
Toaddressthisproblem,weproposeatwo-stepprocedure. leviatethisproblem,weproposetoleverageareplaybuffer.
First, we perform low-pass filtering on the original images Onceanewlypre-processedmini-batchofdataisproduced,
with a B B square filter (without changing the image we put it into the buffer, while if a pre-defined maximum
×
size). Second, we perform B B down-sampling. Since size is reached, we remove the oldest data. We observe
×
low-pass filtering extracts exactly the same information as that this maximum size can typically be set to reasonably
the B B low-frequency cropping, down-sampling will no large on common hardware. During training, to obtain the
×
longer introduce the undesirable higher-frequency compo- model inputs, we alternately execute the following two
nents,andthusthefinalaccuracyofthemodelwillnotbeaf- steps: 1) sampling mini-batches from the replay buffer for
fected.Moreover,B Blow-passfilteringismathematically n times,and2)producinganewmini-batchoftraining
× buffer
equivalentto2Dsincconvolution(see:AppendixD),which data, feeding it into the model for training, and updating
can be efficiently implemented on CPUs with the Lanczos the replay buffer with it. With this technique, the data pre-
algorithm[89],[90].Asaresult,ourtwo-stepprocedurecan processing loads are reduced by n +1 times, while the
buffer
beaccomplishedusingCPUs,whileonlytheresultingB B generalization performance of the model can be effectively
×EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 9
InputSize Top-1Accuracy TrainingSpeedup
Models #Param. #FLOPs
(inference) Baseline EfficientTrain++ Computation Wall-time
ResNet-50[1] 2242 26M 4.1G 78.8% 79.6% 1.49× 1.45×
ConvNeXt-Tiny[18] 2242 29M 4.5G 82.1% 82.2% 1.49× 1.49×
ConvNets
ConvNeXt-Small[18] 2242 50M 8.7G 83.1% 83.2% 1.49× 1.52×
ConvNeXt-Base[18] 2242 89M 15.4G 83.8% 83.8% 1.49× 1.49×
Isotropic DeiT-Tiny[19] 2242 5M 1.3G 72.5% 73.7% 1.56× 1.64×
ViTs DeiT-Small[19] 2242 22M 4.6G 80.3% 81.0% 1.52× 1.60×
PVT-Tiny[20] 2242 13M 1.9G 75.5% 75.9% 1.51× 1.51×
PVT-Small[20] 2242 25M 3.8G 79.9% 80.4% 1.51× 1.49×
PVT-Medium[20] 2242 44M 6.7G 81.8% 81.9% 1.51× 1.49×
PVT-Large[20] 2242 61M 9.8G 82.3% 82.4% 1.51× 1.48×
Multi-stage Swin-Tiny[4] 2242 28M 4.5G 81.3% 81.6% 1.51× 1.49×
ViTs Swin-Small[4] 2242 50M 8.7G 83.1% 83.2% 1.51× 1.51×
Swin-Base[4] 2242 88M 15.4G 83.4% 83.6% 1.50× 1.47×
CSWin-Tiny[21] 2242 23M 4.3G 82.7% 82.9% 1.52× 1.50×
CSWin-Small[21] 2242 35M 6.9G 83.4% 83.6% 1.52× 1.52×
CSWin-Base[21] 2242 78M 15.0G 84.3% 84.3% 1.51× 1.52×
CAFormer-S18[22] 2242 26M 4.1G 83.5% 83.4% 1.52× 1.59×
ConvNet
+ViT CAFormer-S36[22] 2242 39M 8.0G 84.3% 84.3% 1.52× 1.59×
CAFormer-M36[22] 2242 56M 13.2G 84.9% 85.0% 1.51× 1.58×
TABLE 7: Results on ImageNet-1K (IN-1K). We train the models with or without EfficientTrain++ on the IN-1K training set, and report the
accuracyontheIN-1Kvalidationset.ThenumberofequivalenttrainingepochsforEfficientTrain++issetto200,whichissufficienttoachievea
competitiveorbetterperformancecomparedwiththebaselines.Thewall-timetrainingspeedupisbenchmarkedonNVIDIA3090GPUs.
preserved (see: Table 14). Notably, only the number of pre- EfficientTrain by leveraging the advancements proposed in
processedmini-batchesisreduced.Boththetotalnumberof Sections 5.1 and 5.2. The two implementation techniques
training iterations and the computational cost for training proposedinSection5.3aretreatedasoptionalcomponents,
modelsremainunchanged. and their effectiveness is separately investigated in Section
6.1.4. Moreover, to measure the total length of the training
process, for the baselines and the vanilla EfficientTrain, we
6 EXPERIMENTS
directlyreporttherealtrainingepochs.ForEfficientTrain++,
Overview. Section 6.1 evaluates our proposed method in as we adopt the new computational-constrained formula-
standard supervised learning scenarios. Our experiments tion,weconvertthetotalamountofcomputationconsumed
incorporatetrainingavarietyofvisualbackbones(e.g.,Con- bytrainingmodelsintothenumberofthestandardtraining
vNets,isotropic/multi-stageViTs,andConvNet-ViThybrid epochs in baselines, and report the equivalent epoch num-
models) under diverse training settings (e.g., varying train- bers for the ease of understanding (see: Section 5.1). For
ingbudgets,differenttestinputsizes,andvariousamounts example, the computational cost of M-epoch training for
of training data). We also present a comprehensive com- EfficientTrain++ is equal to the M-epoch trained baselines.
parisonofourmethodandstate-of-the-artefficienttraining Besides,unlessotherwisespecified,wereporttheresultsof
methods. In section 6.2, we demonstrate that our method ourimplementationforbothourmethodandthebaselines.
can be conveniently applied to self-supervised learning MoreimplementationdetailscanbefoundinAppendixA.
(e.g., MAE) and yields notable improvements in training
efficiency. In Section 6.3, we study the transferability of
6.1 SupervisedLearning
the models pre-trained using our method to downstream
tasks, e.g., image classification, object detection, and dense 6.1.1 MainResultsonImageNet-1K
prediction. In Section 6.4, thorough ablation studies and TrainingvariousvisualbackbonesonImageNet-1K.Table
analyticalresultsareprovidedforabetterunderstanding. 7 presents the results of applying EfficientTrain++ to train
Datasets.Ourmainexperimentsarebasedonthelarge- representative deep networks on ImageNet-1K, which is
scale ImageNet-1K/22K [17] datasets, which consist of one of the most widely-used settings for evaluating deep
1.28M/ 14.2Mimagesin1K/ 22Kclasses.Wealsoverify learning algorithms. It is clear that our method achieves a
∼ ∼ ∼
the transferability of our method on ADE20K [93], MS competitive or better validation accuracy compared to the
COCO [94], CIFAR [95], Flowers-102 [96], Stanford Dogs baselines (e.g., 85.0% v.s. 84.9% on CAFormer-M36), while
[97],NABirds[98],andFood-101[99]. saving the computational cost for training by 1.5 1.6 .
− ×
Models & Setups. A wide variety of visual backbone Moreover, an important observation is that the gains of
modelsareconsideredinourexperiments,includingResNet EfficientTrain++areconsistentacrossdifferenttypesofdeep
[1],ConvNeXt[18],DeiT[19],PVT[20],Swin[4]Transform- networks,whichdemonstratesthestronggeneralizabilityof
ers,CSWin[21]Transformers,andCAFormer[22].Weadopt our method. In addition, Table 7 also reports the wall-time
the300-epochtrainingpipelinein[4],[18]asourbaseline,on training speedup on GPU devices. It can be observed that
top of which EfficientTrain and EfficientTrain++ only mod- thepracticalefficiencyofEfficientTrain++isinlinewiththe
ify the terms mentioned in Tables 5 and 6, respectively. In theoretical results. Visualization of the curves of accuracy
particular, the default version of EfficientTrain++ improves duringtrainingv.s.timecanbefoundinAppendixB.EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 10
73.7 81.0
73.3 80.6
2.33×
72.4 2.94× 79.8
80.3
72.5
79.5
71.5
4.1% 3.1%
68.3 76.7
81.4 81.6 2.16 × 82.9 83.1 83.2 2.01 ×
83.1
83.3 83.5 83.6 2.40 ×
80.9 81.3 83.3 83.4
82.6
80.8
1.9% 1.5%
1.9%
79.0 81.0 81.8
Fig.7:Top-1accuracyv.s.totaltrainingcostonImageNet-1K.ForthebaselinesandEfficientTrain++,wevarythetotalnumberoftrainingepochs
andequivalenttrainingepochswithin[100,300]and[100,200],respectively.
BaselineEfficientTrain ET++
EfficientTrain v.s. EfficientTrain++. In Table 8, we
CostforSolvingforthe 425.8
present a comprehensive comparison of EfficientTrain, Ef- Curriculum(inGPU-hours) – 1088.1 (↓2.56×)
ficientTrain++, and the baselines. Several observations can
Wall-time
be obtained from the results. First, both the two versions TrainingSpeedup 1.0× ∼1.5× ∼1.5×
of our methods effectively improve the training efficiency.
ResNet-50[1] 78.8% 79.4% 79.6%
ThevanillaEfficientTrainisabletoreducethetrainingcost
DeiT-Tiny[19] 72.5% 73.3% 73.7%
of a variety of deep networks by 1.5 compared to the Resultson
× ImageNet-1K DeiT-Small[19] 80.3% 80.4% 81.0%
baselines, while effectively enhancing the accuracy at the Top-1
Swin-Tiny[4] 81.3% 81.4% 81.6%
Acc.
same time. Second, the EfficientTrain++ curriculum further
Swin-Small[4] 83.1% 83.2% 83.2%
outperforms EfficientTrain consistently. For example, on CSWin-Tiny[21] 82.7% 82.8% 82.9%
ImageNet-1K,EfficientTrain++booststheaccuracyby0.6% CSWin-Small[21] 83.4% 83.6% 83.6%
for DeiT-small (81.0% v.s. 80.4%). In the context of pre-
Wall-time
1.0× ∼1.5× ∼2.0×
training larger models like CSWin-Base/Large, Efficient- Resultswith Pre-trainingSpeedup
ImageNet-22K
Train++ achieves a significantly higher training speedup Pre-training† Top-1CSWin-Base[21] 86.0% 86.2% 86.3%
than EfficientTrain (2.0 v.s. 1.5 ), and yields a better Acc. CSWin-Large[21] 86.8% 86.9% 87.0%
× ×
generalization performance. Third, one can observe that TABLE8:ComparisonsofEfficientTrainandEfficientTrain++(ET++).
Algorithm2utilizedinEfficientTrain++reducesthesolving †:theseresultsarepresentedhereforacomprehensivecomparison(for
cost for obtaining the curriculum by 2.56 compared to thedetailedresultsonImageNet-22K,pleaserefertoTable12).
×
Algorithm1usedforacquiringEfficientTrain.Overall,these InputSize Top-1Accuracy
Models
aforementioned observations indicate that EfficientTrain++ (inference) Baseline EfficientTrain ET++
improvesEfficientTrainholistically,asweexpect. DeiT-Tiny[19] 2242 72.5% 74.3%(+1.8) 74.4%(+1.9)
DeiT-Small[19] 2242 80.3% 80.9%(+0.6) 81.3%(+1.0)
Adapted to varying training budgets. Our method can
TABLE9:Higheraccuracywiththesametrainingcost(ET++:Efficient-
conveniently adapt to varying computational budgets for
Train++).Hereourmethodisconfiguredtohavethesametrainingcost
training, i.e., by simply modulating the number of total asthebaselines(e.g.,forET++,wesetequivalenttrainingepochs=300).
training epochs on top of Tables 5 and 6. As represen-
tative examples, we report the curves of validation accu- easily obtained on top of EfficientTrain (see: Appendix B).
racy v.s. computational training cost in Figure 7 for both Furthermore, in Table 9, we deploy our proposed training
EfficientTrain++ and the baselines. The advantage of our curriculabyallowingthemtoutilizethesametrainingcost
method is even more significant under the constraint of as the standard 300-epoch training pipeline [4], [18]. The
a relatively smaller training cost, e.g., it outperforms the results show that our method significantly improves the
baseline by 3.1% (79.8% v.s. 76.7%) on top of the 100- accuracy (e.g., by 1.9% for DeiT-Tiny). Interestingly, when
epoch trained DeiT-Small. We attribute this to the greater properly trained using EfficientTrain++, the vanilla DeiT-
importance of efficient training algorithms in the scenarios SmallnetworkperformsonparwiththebaselineSwin-Tiny
of limited training resources. In addition, we observe that, model(81.3%),withoutincreasingthetrainingwall-time.
if we mainly hope to achieve the same performance as the Adapted to any final input size γ. Our method can
baselines, EfficientTrain++ is able to save the training cost flexibly adapt to an arbitrary final input size γ. To at-
by2 3 atmost,whichismoresignificantthantheresults tain this goal, the value of B for the three stages of Ef-
− ×
in Table 7. These aforementioned observations can also be ficientTrain or EfficientTrain++ can be simply adjusted toEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 11
Top-1Accuracy/Wall-timeTrainingSpeedup TrainingAug- Top-1 Training
Models Method Models TrainingApproach
2242 3842 5122 Epochs RegsAccuracySpeedup
Swin-Base Baseline 83.4%/1.00× 84.5%/1.00× 84.7%/1.00× ResNet-18SmoothingCurriculum[42](NeurIPS’20) 90 ✗ 71.0% 1.00×
[4] EfficientTrain 83.6%/1.50× 84.7%/2.91× 85.1%/3.37× [1] EfficientTrain 90 ✗ 71.0% 1.48×
TABLE10:Adaptedtodifferentfinalinputsizes.Swin-Baseisselected Self-pacedLearning[27](NeurIPS’10) 200 ✗ 73.2% 1.15×
asarepresentativeexamplesincelargermodelstypicallybenefitmore MinimaxCurriculum[40](ICLR’18) 200 ✗ 75.1% 1.97×
fromlargerinputsizes. DIHCurriculum[38](NeurIPS’20) 200 ✗ 76.3% 2.45×
EfficientTrain 200 ✗ 77.5% 1.44×
160,(160+γ)/2,γ or 96,(96+γ)/2,γ. As shown in Table CurriculumNet[41](ECCV’18) 90 ✗ 76.1% <2.22×
10,ourmethodoutperformsthebaselinesbylargemargins Res [N 1]et-50 Label-sim.Curriculum[44](ECCV’20) 90 ✗ 76.9% 2.22×
forγ >224intermsoftrainingefficiency. EfficientTrain 90 ✗ 77.0% 3.21×
Orthogonal to 2242 pre-training + γ2 fine-tuning. In ProgressiveLearning[58](ICML’21) 350 ✓ 78.4% 1.21×
particular, in some cases, existing works find it efficient to
InfoBatch†[39](ICLR’24) 300 ✓ 78.6% 1.39×
EfficientTrain 300 ✓ 79.4% 1.44×
fine-tune2242 pre-trainedmodelstoatargettestinputsize EfficientTrain++ 200 ✓ 79.6% 1.45×
γ2 [4],[18],[21].Hereourmethodcanbedirectlyleveraged DeiT- AutoProgressiveLearning[7](CVPR’22) 300 ✓ 72.4% 1.51×
formoreefficientpre-training(e.g.,γ=384inTables12). Tiny EfficientTrain 300 ✓ 73.3% 1.55×
[19] EfficientTrain++ 200 ✓ 73.7% 1.64×
6.1.2 ComparisonswithExistingEfficientTrainingMethods ProgressiveLearning[58](ICML’21) 100 ✓ 72.6% 1.54×
Comparisons with state-of-the-art efficient training algo- AutoProgressiveLearning[7](CVPR’22) 100 ✓ 74.4% 1.41×
rithmsaresummarizedinTable11.Ourmethodiscompre- BudgetedViT[55](ICLR’23) 128 ✓ 74.5% 1.34×
EfficientTrain 100 ✓ 76.4% 1.51×
hensively compared with 1) the recently proposed sample-
wise[27],[38],[39],[40],[41]orregularization-wise[42],[44] DeiT- BudgetedTraining†[57](ICLR’20) 225 ✓ 79.6% 1.33×
Small ProgressiveLearning†[58](ICML’21) 300 ✓ 79.5% 1.49×
curriculumlearningapproaches;2)theprogressivelearning [19] AutoProgressiveLearning[7](CVPR’22) 300 ✓ 79.8% 1.42×
algorithms[7],[58];and3)theefficienttrainingmethodsfor DeiTIII[59](ECCV’22) 300 ✓ 79.9% 1.00×
vision Transformers [55], [57], [59]. Notably, some of these BudgetedViT[55](ICLR’23) 303 ✓ 80.1% 1.34×
methods are not developed on top of the state-of-the-art EfficientTrain 300 ✓ 80.4% 1.51×
trainingpipelineweconsider(i.e.,‘AugRegs’inTable11,see EfficientTrain++ 200 ✓ 81.0% 1.60×
detailsinAppendixA).Whencomparingourmethodwith ProgressiveLearning†[58](ICML’21) 300 ✓ 82.3% 1.51×
them, we also implement our method without AugRegs EfficientTrain 300 ✓ 82.8% 1.55×
CSWin- EfficientTrain++ 200 ✓ 82.9% 1.50×
(‘RandAug’ in our method is removed in this scenario), Tiny
[21] FixRes†[60](NeurIPS’19) 300 ✓ 82.9% 1.00×
and adopt the same training settings as them (e.g., training EfficientTrain+FixRes 300 ✓ 83.1% 1.55×
ResNet-18/50 for 90/200 epochs). The results in Table 11 EfficientTrain+++FixRes 200 ✓ 83.3% 1.50×
indicatethatEfficientTrain/EfficientTrain++outperformsall ProgressiveLearning†[58](ICML’21) 300 ✓ 83.3% 1.48×
the competitive baselines in terms of both accuracy and EfficientTrain 300 ✓ 83.6% 1.51×
training efficiency. Moreover, the simplicity of our method CSWin- EfficientTrain++ 200 ✓ 83.6% 1.52×
Small
enables it to be conveniently applied to different models [21] FixRes†[60](NeurIPS’19) 300 ✓ 83.7% 1.00×
andtrainingsettingsbyonlymodifyingafewlinesofcode, EfficientTrain+FixRes 300 ✓ 83.8% 1.51×
EfficientTrain+++FixRes 200 ✓ 83.9% 1.52×
whichisanimportantadvantageoverbaselines.
TABLE 11: EfficientTrain v.s. state-of-the-art efficient training algo-
Orthogonal to FixRes. FixRes [60] reveals that there
rithms on ImageNet-1K. Here ‘AugRegs’ denotes the widely-used
exists a discrepancy in the scale of images between the holisticcombinationofvariousmodelregularizationanddataaugmen-
trainingandtestinputs,andthustheinferencewithalarger tationtechniques[4],[18],[19],[20],[21].ForEfficientTrain++,wereport
resolution will yield a better test accuracy. However, our
thenumberofequivalenttrainingepochs.†:ourreproducedresults.
methoddoesnotleveragethegainsofFixRes.Weadoptthe
original inputs (e.g., 2242) at the final training stage, and cost. One can observe that, similar to ImageNet-1K, our
hence the finally-trained model resembles the 2242-trained method performs at least on par with the baselines on top
networks, while FixRes is orthogonal to our method. This ofbothConvNetsandvisionTransformers,whileachieving
factcanbeconfirmedbyboththedirectempiricalevidence a significant training speedup of up to 2 3 . A highlight
− ×
in Table 11 (see: FixRes v.s. EfficientTrain/EfficientTrain++ fromtheresultsisthatEfficientTrain++savesaconsiderable
+ FixRes on top of the state-of-the-art CSWin Transformers amountofrealtrainingtime,e.g.,158.5GPU-days(237.8v.s.
[21])andtheresultsinTable12(see:InputSize=3842). 79.3)forCSWin-Large,whichcorrespondto 20daysfora
∼
standardcomputationalnodewith8GPUs.
6.1.3 MainResultsonImageNet-22K
ImageNet-22K pre-training. One of the important advan- 6.1.4 ImplementationTechniquesforEfficientTrain++
tages of modern visual backbones is their excellent scala- Inthefollowing,weinvestigatetheeffectivenessofthetwo
bility with a growing amount of training data [18], [21]. implementationtechniquesforEfficientTrain++proposedin
To this end, we further verify the effectiveness of our Section5.3.Wedemonstratethatthesetechniqueseffectively
method on the larger ImageNet-22K benchmark dataset. lowerthethresholdsofimplementingEfficientTrain++,and
The results are summarized in Table 12, where the mod- enable our method to acquire significant practical training
els are pre-trained on ImageNet-22K, and evaluated by speedupinabroaderrangeofscenarios.
being fine-tuned to ImageNet-1K. We implement Efficient- Large-scale parallel training: early large batch. Since
Train/EfficientTrain++ at the pre-training stage, which ac- this mechanism is designed to enable EfficientTrain++ to
counts for the vast majority of the total computation/time makefulluseofmoreGPUs,weevaluateitinthescenarioofEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 12
Top-1Accuracy
Models #Param. (2# 2F 42L /O 3P 8s 42) P Are p-t pr ra oin acin hg Inp( ufi tne si- ztu e:n 2e 2d 4t 2oIm Ina pg ue tN se izt- e1 :K 3) 842 W Ca oll s- tti (m ine GP Pre U-t -r da ain yi sn )g (forT ai nm 8e -GSa Pv Uin ng ode)
Baseline 85.8% 86.7% 181.9 –
ConvNeXt-Base[18] 89M 15.4G/45.1G EfficientTrain 85.8% 86.8% 122.4(↓1.49×) 22.7→15.3Days
EfficientTrain++ 85.9% 86.9% 122.3(↓1.49×) 22.7→15.3Days
Baseline 86.4% 87.3% 375.8 –
ConvNeXt-Large[18] 198M 34.4G/101.0G EfficientTrain 86.4% 87.3% 243.8(↓1.54×) 47.0→30.5Days
EfficientTrain++ 86.5% 87.3% 249.3(↓1.51×) 47.0→31.2Days
Baseline 86.0% 87.2% 126.9 –
EfficientTrain 86.2% 87.4% 83.7(↓1.52×) 15.9→10.5Days
CSWin-Base[21] 78M 15.0G/47.0G
86.3% 87.4% 62.9(↓2.02×) 15.9→7.9Days
EfficientTrain++
86.1% 87.1% 41.9(↓3.03×) 15.9→5.2Days
Baseline 86.8% 87.9% 237.8 –
EfficientTrain 86.9% 87.9% 155.8(↓1.53×) 29.7→19.5Days
CSWin-Large[21] 173M 31.5G/96.8G
87.0% 87.9% 119.0(↓2.00×) 29.7→14.9Days
EfficientTrain++
86.8% 87.8% 79.3(↓3.00×) 29.7→9.9Days
TABLE12:ResultswithImageNet-22K(IN-22K)pre-training.Themodelsarepre-trainedonIN-22Kwithorwithoutourmethod,fine-tunedon
theImageNet-1K(IN-1K)trainingset,andevaluatedontheIN-1Kvalidationset.Wepresenttheresultsbysettingthetotalnumberofequivalent
pre-trainingepochsofEfficientTrain++to2/3,1/2,and1/3ofthebaselines,withinwhichrangeourmethodgenerallyperformsonparwiththe
baselinesintermsofaccuracy.Thewall-timepre-trainingcostisbenchmarkedonNVIDIA3090GPUs.
Top-1Accuracy Wall-timePre-training
Models Pre-trainingApproach Early (fine-tunedtoImageNet-1K) Speedup
LargeBatch Inputsize:2242 Inputsize:3842 16GPUs 64GPUs
Baseline – 86.0% 87.2% 1.0× 1.0×
✗ 86.3% 87.4% 2.0× 1.4×
EfficientTrain++
CSWin-Base[21] ✓ 86.4% 87.4% 2.0× 2.0×
✗ 86.1% 87.1% 3.0× 2.1×
EfficientTrain++
✓ 86.0% 87.1% 3.0× 2.9×
Baseline – 86.8% 87.9% 1.0× 1.0×
✗ 87.0% 87.9% 2.0× 1.4×
EfficientTrain++
CSWin-Large[21] ✓ 87.1% 88.1% 2.0× 1.9×
✗ 86.8% 87.8% 3.0× 2.1×
EfficientTrain++
✓ 86.8% 87.8% 3.0× 2.9×
TABLE13:Effectsofintroducingtheearlylargebatchmechanism(see: Section5.3)inEfficientTrain++.ThescenarioofImageNet-22Kpre-
training+ImageNet-1Kfine-tuningisconsideredhere.TheexperimentalsettingsarethesameasTable12.
Training Replay Top-1 Computational PeakDataPre-
pre-training CSWin-Base/Large on ImageNet-22K. CSWin Models Approach Buffer Accuracy TrainingSpeedup processingLoads
Transformers are representative examples of state-of-the- DeiT-Tiny ✗ 73.7% 1.56× 12630images/s
ET++
art deep networks. Due to their large model size and the [19] ✓ 73.6% 1.56× 6315images/s
considerable amount of ImageNet-22K training data, there DeiT-Small ✗ 81.0% 1.52× 5329images/s
ET++
isapracticaldemandofpre-trainingthemodelsinparallel [19] ✓ 80.8% 1.52× 2665images/s
using a number of GPUs (e.g., 64). In Table 13, we study CSWin-Tiny ✗ 82.9% 1.52× 2294images/s
ET++
the effects of introducing larger batches at earlier learning [21] ✓ 82.9% 1.52× 1147images/s
stages(namedas‘earlylargebatch’),whereourimplemen- CSWin-Small ✗ 83.6% 1.52× 1517images/s
ET++
tationfollowsfromthestatementsinSection5.3.Ingeneral, [21] ✓ 83.6% 1.52× 759images/s
the vanilla EfficientTrain++ is able to achieve significant TABLE 14: Effects of introducing the replay buffer (see: Section
training speedup when leveraging 16 GPUs. However, its 5.3) in EfficientTrain++ (ET++). The scenario of training relatively
small models on ImageNet-1K is considered here, where high data
practical efficiency notably decreases when the number of
pre-processing loads tend to be a practical bottleneck for efficient
GPUs grows to 64. In contrast, the wall-time pre-training training.TheexperimentalsettingsarethesameasTable7.Thevalueof
speedup of EfficientTrain++ equipped with the early large ‘peakdatapre-processingloads’referstothepeakloadsintroducedby
saturatingallthecomputationalcoresofasingleNVIDIA3090GPU.
batch mechanism on 64 GPUs is approximately identical
to exploiting 16 GPUs. This observation demonstrates that
enlarging batch sizes for small B effectively enhances the
thattheyputhigherdemandsonCPUandmemoryinterms
scalability of EfficientTrain++ with the growing number of
of the throughput of preparing training inputs. Hence, the
GPUs.Besides,itisworthnotingthatthismechanismdoes
datapre-processingloadisanotablepotentialbottleneckfor
notaffecttheperformanceofthemodels.
trainingthemefficiently.Wetrainthesemodelsbyintroduc-
Reducing data pre-processing loads: replay buffer. ing the replay buffer as stated in Section 5.3, and compare
To investigate this mechanism, we take training relatively the results with the original EfficientTrain++ in Table 14.
light-weighted models (e.g., DeiT-T/S and CSWin-T/S) as One can observe that the replay buffer contributes to the
representative examples. Due to the smaller size of these significantly reduced peak data pre-processing loads, and
networks,theirtrainingspeedonGPUsisusuallyfast,such maintainsacompetitiveperformancewiththebaselines.EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 13
Top-1Accuracy Pre-training Top-1Accuracy
Methods Backbone #Param. Pre-training
(fine-tuning) Speedup Backbone (fine-tunedtodownstreamdatasets)
Method Speedup C10 C100 Flowers-102 StanfordDogs
DINO[100](ICCV’21) ViT-B 86M 82.8% –
MoCoV3[101](ICCV’21) ViT-B 86M 83.2% –
DeiT-S
Baseline 1.0× 98.39% 88.65% 96.57% 90.72%
BEiT[102](ICLR’22) ViT-B 86M 83.2%† 0.92× [19] EfficientTrain 1.5× 98.47% 88.93% 96.62% 91.12%
MaskFeat[103](CVPR’22) ViT-B 86M 83.6% 1.50× EfficientTrain++ 1.6× 98.51% 89.55% 97.38% 91.49%
LoMaR[104](arXiv’22) ViT-B+RPE‡ 86M 83.6% 3.52× TABLE 16: Transferability to downstream image recognition tasks.
CAE[105](IJCV’23) ViT-B 86M 83.6%† 2.10× Themodelsarepre-trainedonImageNet-1Kw/orw/oEfficientTrain
or EfficientTrain++, and fine-tuned to the downstream datasets to
LocalMIM[106](CVPR’23)
ViT-B 86M 83.7% 3.14× reporttheaccuracy.C10/C100referstotheCIFAR-10/100datasets.
+HOGtarget[103](CVPR’22)
MAE1600-epoch[23](CVPR’22) ViT-B 86M 83.6% 1.00× Backbone Pre-training APbox APbox APbox APmask APmask APmask
MAE400-epoch[23](CVPR’22) ViT-B 86M 83.0% 4.00× Method Speedup 50 75 50 75
MAE1600-epoch+HOG ViT-B 86M 83.7% 1.00× RetinaNet[111](1×schedule)
MAE400-epoch+HOG ViT-B 86M 83.2% 4.00× Sw [4in ]-T EffiB ca iese nl ti Tn re
ain
11. .0 5×
×
4 41 1. .7
8
6 62 3. .9
3
44 44 .. 65 –– –
–
––
MAE(ET++)400-epoch+HOG ViT-B 86M 83.7% 3.98×
CascadeMask-RCNN[112](1×schedule)
TABLE15:Self-supervisedlearningresultsontopofMAE[23].The
Baseline 1.0× 48.1 67.0 52.1 41.5 64.2 44.9
modelispre-trainedonImageNet-1KwithEfficientTrain++(ET++),and Swin-T
evaluatedbyend-to-endfine-tuning[23].†:usingadditionaldata(the [4] EfficientTrain 1.5× 48.2 67.5 52.3 41.8 64.6 45.0
DALL-Etokenizer[107]).‡:usingrelativepositionencoding(RPE). EfficientTrain++ 1.5× 48.4 67.6 52.6 41.9 64.8 45.2
Swin-S Baseline 1.0× 50.0 69.1 54.4 43.1 66.3 46.3
6.2 Self-supervisedLearning [4] EfficientTrain++ 1.5× 50.7 69.9 55.0 43.7 67.1 47.3
Results on top of Masked Autoencoders (MAE). In addi- Swin-B Baseline 1.0× 50.9 70.2 55.5 44.0 67.4 47.4
[4] EfficientTrain++ 1.5× 51.3 70.5 55.9 44.3 67.6 48.0
tion to supervised learning, our method can also be conve-
TABLE17:ObjectdetectionandinstancesegmentationonCOCO.We
nientlyappliedtoself-supervisedlearningalgorithmssince
implementrepresentativedetection/segmentationalgorithmsontopof
it only modifies the training inputs. Table 15 presents a thebackbonespre-trainedw/orw/oourmethodonImageNet-1K.
representative example, where we deploy EfficientTrain++
Pre-training
on top of MAE [23] equipped with HOG reconstruction Backbone Method Speedup mIoU mIoUMS+Flip†
targets [103]. We also present the results of several recently UperNet[109](80ksteps)
proposed self-supervised learning approaches that use the Baseline 1.0× 43.0 43.8
DeiT-S[19]
same backbone and have comparable pre-training costs EfficientTrain++ 1.6× 43.8 44.9
with us. One can observe that our method reduces the Baseline 1.0× 43.3 44.5
Swin-T[4]
pre-trainingcostofMAEsignificantlywhilepreservingthe EfficientTrain++ 1.5× 44.1 44.9
accuracy,outperformingallthecompetitivebaselines. Swin-S[4] Baseline 1.0× 47.5 48.9
EfficientTrain++ 1.5× 47.8 49.2
6.3 TransferLearning UperNet[109](160ksteps)
Baseline 1.0× 43.1 44.0
Downstream image recognition tasks. We first evaluate DeiT-S[19]
EfficientTrain++ 1.6× 44.2 45.2
the transferability of the models trained with Efficient-
Baseline 1.0× 44.4 45.8
Train/EfficientTrain++byfine-tuningthemondownstream Swin-T[4]
EfficientTrain++ 1.5× 44.8 46.1
classification datasets. The results are reported in Table 16.
Baseline 1.0× 47.7 49.2
Notably,following[19],the32 32imagesinCIFAR-10/100 Swin-S[4]
× EfficientTrain++ 1.5× 48.2 49.7
[95]areresizedto224 224forpre-processing,andthusthe
× Swin-B[4] Baseline 1.0× 48.0 49.6
discriminative patterns are mainly distributed within the EfficientTrain++ 1.5× 48.5 50.0
lower-frequency components. On the contrary, Flowers-102 TABLE 18: Semantic segmentation on ADE20K. We initialize the
[96]andStanfordDogs[97]arefine-grainedvisualrecogni- backbonenetworksinUperNet[109]withthemodelspre-trainedw/or
tiondatasetswherethehigh-frequencycluescontainimpor- w/oourmethodonImageNet-1K.†:‘MS+Flip’indicatestheensemble
resultsofmulti-scaleandflippedinputimages.
tantdiscriminativeinformation(e.g.,thedetailedfacial/skin
characteristics to distinguish between the species of dogs).
Semantic segmentation. Table 18 further evaluates the
One can observe that our method yields competitive or
performance of our method on downstream semantic seg-
better transfer learning performance than the baselines on
mentationtasks.Weimplementseveralrepresentativeback-
bothtypesofdatasets.Inotherwords,althoughourmethod
bonespre-trainedwithEfficientTrain++astheinitialization
learns to exploit the lower/higher-frequency information
of encoders in UperNet [109], and fine-tune UperNet on
via an ordered curriculum, the finally obtained models can
ADE20K using MMSegmentation [110]. The observation
leveragebothtypesofinformationeffectively.
from Table 18 is similar to Tables 16 and 17. Our method
Object detection & instance segmentation. To in-
cansavepre-trainingcostbyatleast1.5×,whileeffectively
vestigate transferring our pre-trained models to more
enhancingdownstreamperformance.Forexample,ontopof
complex computer vision tasks, we initialize the back-
DeiT-Small, EfficientTrain++ improves mIoU/mIoUMS+Flip
bonesofrepresentativedetectionandinstancesegmentation
by1.1/1.2underthe160k-step-trainingsetting.
frameworks with the models pre-trained using Efficient-
Train/EfficientTrain++. Results on MS COCO are reported
6.4 Discussions
inTable17.Whenreducingthepre-trainingcostby∼1.5×,
ourmethodsignificantlyoutperformsthebaselinesinterms 6.4.1 AblationStudy
ofdetection/segmentationperformanceinallscenarios.Our Togetabetterunderstandingofourproposedapproach,we
implementationfollowsfromMMDetection[108]. conduct a series of ablation studies. In specific, followingEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 14
Low-frequency Linear Training Top-1Accuracy(100ep:100-epoch;Others:300-epoch)
Cropping RandAug Speedup DeiT-Tiny DeiT-Small100ep DeiT-Small Swin-Tiny Swin-Small CSWin-Tiny
1.0× 72.5% 75.5% 80.3% 81.3% 83.1% 82.7%
✓ ∼1.5× 72.4% 75.5% 80.0% 81.1% 83.0% 82.6%
✓ ✓ ∼1.5× 73.3% 76.4% 80.4% 81.4% 83.2% 82.8%
(a)Ablatinglow-frequencycroppingandlinearlyincreasedRandAug.
Low-frequencyInformationExtraction Training Top-1Accuracy(100ep:100-epoch;Others:300-epoch)
inEfficientTrain Speedup DeiT-Small100ep DeiT-Small Swin-Tiny Swin-Small CSWin-Large†
StandardImageDown-sampling ∼1.5× 75.9% 80.3% 81.0% 83.0% 86.4%
Low-frequencyCropping ∼1.5× 76.4% 80.4% 81.4% 83.2% 86.6%
EfficientLow-frequencyDown-sampling‡ ∼1.5× 76.5% 80.4% 81.4% 83.1% 86.7%
(b)Designchoicesoftheoperationforextractinglow-frequencyinformation.†:pre-trainedonImageNet-22K
followingtheconfigurationsin[21],[24].‡:mainlyproposedtoalleviatehighCPU-GPUI/OCosts(see:Table20).
Training Top-1Accuracy
ScheduleofBinEfficientTrain
Speedup DeiT-Tiny DeiT-Small Swin-Tiny
LinearIncreasing[58] ∼1.5× 72.8% 79.9% 81.0%
ObtainedfromAlgorithm1 ∼1.5× 73.3% 80.4% 81.4%
(c)ScheduleofB.Forfaircomparisons,thelinearincreasingscheduleisconfigured
tohavethesametrainingcostasthescheduleofEfficientTrain.
TABLE19:AblationstudiesofthetechniquesproposedinEfficientTrain.Unlessotherwisespecified,theresultsarereportedonImageNet-1K.
EfficientLow-frequency CostforSolving PeakCPU-GPU Training Top-1Accuracy
Method Algorithm2 Down-sampling fortheCurriculum I/OCost† Speedup DeiT-Small Swin-Tiny Swin-Small
EfficientTrain 1088.1GPU-hours 1078.1M/s ∼1.5× 80.4% 81.4% 83.2%
EfficientTrain+ ✓ 1088.1GPU-hours 550.1M/s ∼1.5× 80.4% 81.4% 83.1%
EfficientTrain+ ✓ 425.8GPU-hours 3060.1M/s <1.5×‡ 81.0% 81.6% 83.3%
EfficientTrain++ ✓ ✓ 425.8GPU-hours(↓2.6×) 562.1M/s(↓5.4×) ∼1.5× 81.0% 81.6% 83.2%
TABLE20:AblationstudiesoftheimprovementsofEfficientTrain++overEfficientTrain(onImageNet-1K).†:asarepresentativeexample,we
reporttheresultscorrespondingtotrainingDeiT-Small,wherethevalueofI/Oisobtainedbysaturatingallthecomputationalcoresofasingle
NVIDIA3090GPU.‡:thepracticaltrainingefficiencyisusuallylowerthanthetheoreticalresultsduetohighCPU-GPUI/Ocosts.
Variables EffectivenessoftheResultingCurriculum
OptimizationObjective
SearchingConfigurations† Constant Changeable Training Top-1Accuracy
(seecaptionforthemeaningofnotations) Speedup‡ DeiT-Small Swin-Tiny
FollowingAutoProg[7] Ii Bi,Ti minimize Li·T iα (α:hyper-parameter) ∼1.5× 80.2% 81.1%
– Ii Bi,Ti minimize (1−Acci)·T iα (α:hyper-parameter) ∼1.5× 80.3% 81.3%
Algorithm2 Ti Bi,Ii maximize Acci ∼1.5× 81.0% 81.6%
TABLE 21: Ablation studies of the design choices of Algorithm 2. We modify the searching configurations of Algorithm 2 (e.g., configs of
variablesandoptimizationobjectives),andsolveforcurriculabyleveragingthesameprocedureasobtainingEfficientTrain++.Thenwecompare
theresultingcurriculatoinvestigatetheeffectivenessofdifferentsearchingconfigurations.In‘Variables’,Bi,Ii,andTirefertothebandwidthof
efficientlow-frequencydown-sampling,numberoftrainingiterations,andcomputationaltrainingcostcorrespondingtoithtrainingstage,while
i can be an arbitrary index.In ‘Optimization Objective’, Li and Acci denote trainingloss and validationaccuracy, respectively. †: allvariants
consideredherehavesimilarsearchingcost.‡:forclarity,differentcurriculaarecomparedunderthesametrainingcost.
theorganizationofthispaper,wefirstinvestigatetheeffec- training curriculum. Moreover, the models trained using
tivenessofthecomponentsofEfficientTrain,andthenstudy the curriculum found by Algorithm 2 are able to acquire a
whether the new techniques proposed in EfficientTrain++ strongergeneralizationperformancewiththesametraining
improveEfficientTrainasweexpect. cost. On its basis, further introducing the efficient low-
Ablation study results with EfficientTrain are summa- frequencydown-samplingoperationalleviatestheproblem
rizedinTable19.InTable19(a),weshowthatthemajorgain of high CPU-GPU I/O costs, and thus effectively improves
of training efficiency comes from low-frequency cropping, thepracticaltrainingefficiencyofourmethod.
which effectively reduces the training cost at the price of Design of Algorithm 2. We further study whether the
a slight drop of accuracy. On top of this, linear RandAug design of Algorithm 2 is reasonable. Specifically, we select
further improves the accuracy. Moreover, replacing low- the searching configurations of AutoProg [7] as a compet-
frequencycroppingwithimagedown-samplingconsistently itive baseline. AutoProg is a recently proposed efficient
degrades the accuracy (see: Table 19 (b)), since down- training approach for ViTs. It is similar to Algorithm 2 in
sampling cannot strictly filter out all the higher-frequency theparadigmoffine-tuningthemodelateachtrainingstage
information (see: Proposition 1), yielding a sub-optimal tosearchfortheproperprogressivelearningstrategy.How-
implementation of our idea. In addition, as shown in Table ever, both its optimization objectives and its configurations
19(c),thescheduleofB foundbyAlgorithm1outperforms of variables are different from our method. In our exper-
theheuristicdesignchoices(e.g.,thelinearschedulein[58]).
iments, we first implement the searching configurations of
Improvements of EfficientTrain++ over EfficientTrain. AutoProginourproblem.Then,weintroducemodifications
InTable20,westudytheeffectivenessoftheimprovedtech- on top of AutoProg step by step and finally obtain Algo-
niques proposed in EfficientTrain++ (i.e., the components rithm 2, aiming to demonstrate our advantages over the
introduced in Sections 5.1 and 5.2). One can observe that current state-of-the-art design. The results are summarized
Algorithm2dramaticallyreducesthecostforobtainingthe inTable21,wheretwoobservationscanbeobtained:1)val-EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 15
Training IN-1K
Method
SpeedupAccuracy
Training IN-1K Training IN-1K
Method
SpeedupAccuracy
Method
SpeedupAccuracy
Baseline,Swin-SIN-1K 1.0× 83.1%
B E Ea T Ts (+ +e a+ +l )i , ,n MS De w, eD uii Tn le -- ti STT
iI
-I N- N sS --
1
tI 1 KN aK- g1 →→K
eD
VD ee iii TTT
-
s- SS
I
→I NN -- 11 KK iso1
1
t1 r. .. o0 6 6× ×
×
picV88 8 i10 1 T.. . 23 0 s%% %
.
B E Ea T Ts + +e +
+
(li b, ,n )S Ce w, SSC Wi wnS i-W inT n-Ii TNn →- I- 1 NT K -1I KN → a- d1 →K C vS aCW
nSW
cin ei- dnT -I TN m- I1 NK
u-1 lK
ti-s1 11 t. .. a0 5
5
g× ×
×
eViT8 88 2 22 s. .. .9 97 % %% E E
B
ET T
a
Ts+ + +e+ + +li, ,, nS CS ew w
, SC
Wi in n
S
i- -
W
nT S -II N
i
TN n-- I1 -1 NK SK -1IN→ K→
-1
→KSS ww Cii Snn W-- SS iII N nN - -- 11 SKK
IN-1K
1 1 11. . ..05 5 5×× ×
×
8 8
8
83 3
3
3. .
.
.2 2
4
6% %
% %
ET++,CSWin-SIN-1K→CSWin-SIN-1K 1.5× 83.7%
(c)Modelsize:smaller→larger.
Pre-training IN-1K
Method
Speedup Accuracy Training AccuracyonDifferentDataset
Method
Baseline,CSWin-B(IN-22Kpre-training) 1.0× 86.0% SpeedupFood-101NABirdsCIFAR-100StanfordDogs
2.0× 86.3% Baseline,Swin-T(trainedonspecificdataset) 1.0× 87.9% 49.7% 74.4% 54.3%
ET++,Swin-TIN-1K→CSWin-BIN-22K
3.0× 86.1% ET++,Swin-T(IN-1K→specificdataset) 1.5× 89.1% 59.4% 81.8% 61.0%
3.0× 86.3% ET++,Swin-T(specificdataset→specificdataset) 1.5× 89.3% 59.6% 81.9% 61.3%
ET++,CSWin-BIN-22K→CSWin-BIN-22K
3.8× 86.0% (e)ImageNet-1K→differentdatasets.
(d)Smallmodels+IN-1K→largermodels+IN-22K.
TABLE22:InvestigationonthetransferabilityofEfficientTrain++(ET++).Theleftsideof‘→’indicatesthemodelanddatasetwithwhichwe
solveforthelearningcurriculum,whiletherightsideindicatestheconfigurationswithwhichweimplementthecurriculumtotrainthemodel.
AllthecurriculaareobtainedusingAlgorithm2.IN-1K/22KdenotesImageNet-1K/22K.Ourdefaultsettingsaremarkedingray.
Training ImageNet-1K
idationaccuracyisabettersearchingobjectivethantraining Models TrainingApproach Speedup Accuracy
loss in our problem; and 2) our proposed formulation of Baseline 1.0× 78.8%
computational-constrained searching (i.e., maximizing vali-
ResNet-50
InfoBatch[39](ICLR’24) 1.4× 78.6%
dation accuracy under a fixed ratio of training cost saving; EfficientTrain++ 1.5× 79.6%
see Section 5.1 for details) results in a significantly better EfficientTrain+++InfoBatch 1.7× 79.6%
training curriculum, while it can simplify the searching TABLE23:Compatibilitywithsample-selection-basedmethods.
objective by eliminating the hyper-parameter for balancing
to ImageNet-22K (by 10 ), EfficientTrain++ can already
theeffectiveness-efficiencytrade-off. ∼ ×
acquire the majority of gains for efficient training (lossless
6.4.2 TransferabilityofEfficientTrain++ training speedup: 3.0 v.s. 3.8 ). Hence, it is reasonable
× ×
Table 22 systematically discusses the generalization ability to solve for the curriculum on a smaller subset of the full
ofourEfficientTrain++curriculum(i.e.,Table6,obtainedon large-scaledataset(e.g.,with 1/10data),unlessonewants
∼
Swin-Tiny and ImageNet-1K). We execute Algorithm 2 on to maximize training efficacy regardless of cost. Moreover,
top of different visual backbone architectures and diverse as shown in Table 22 (e), on four widely-used datasets
datasets.Then,ineachcorrespondingscenario,wecompare (i.e., Food-101 [99], NABirds [98], CIFAR [95], and Stanford
theresultingspecializedcurriculumwithEfficientTrain++. Dogs [97]), the ImageNet-1K-based curriculum performs
Different networks. Like many other useful training comparably with dataset-specific curriculum optimization.
techniques (e.g., cosine learning rate schedule, AdamW This may be attributed to the fact that accomplishing the
optimizer, and random erasing), the effectiveness of Ef- taskoflearningrepresentationsonachallenginglarge-scale
ficientTrain++ is generally not tied to a specific network natural image database like ImageNet has already covered
architecture. For example, Table 22 (b) & (c) reveal that the needs of many specialized visual scenarios. Therefore,
thecurriculumoptimizedonSwin/CSWin-Tinyisstillclose on many common datasets, one may simply adopt an off-
to the optimal design (found by Algorithm 2) if we adopt the-shelf curriculum obtained on comprehensive natural
advancedarchitectureswithinthefamilyofmulti-stageViTs imagedatasetslikeImageNet-1K(e.g.,EfficientTrain++).
or enlarge model size. Besides, as shown in Table 22 (a),
6.4.3 IncorporatingSample-selection-basedMethods
whenapplyingstandardEfficientTrain++toanotherfamily
ofnetworks(e.g.,isotropicViTs),model-specificcurriculum As a flexible framework, our proposed generalized cur-
searching can yield minor improvements in training effi- riculum learning can easily incorporate previous sample-
ciency(e.g.,81.0% 81.2%forDeiT-Small),whilethiscomes selection curriculum learning methods. In this paper, these
→
atthenotablepriceofexecutingAlgorithm2onceagain.In methodsaremainlyconsideredasthebaselinestocompare
contrast, directly utilizing standard EfficientTrain++ intro- with, aiming to elaborate on our new contributions. How-
ducesnoadditionalcost,anditcanalreadyacquirethevast ever, the idea of data-selection is actually compatible with
majorityofgainsforefficienttraining. us. It is straightforward to consider a curriculum learn-
Different datasets. When applying our method to a ing function that simultaneously uncovers progressively
new dataset, two facts are noteworthy. First, based on the moredifficultpatternswithineachexampleanddetermines
aforementioned robustness across different networks, one whether each example should be leveraged for training. To
can solve for a generalizable curriculum on top of a com- makethisclear,weprovidearepresentativeexampleinTa-
mon, small model (e.g., Swin-Tiny). According to Table 8, ble 23, where sample-selection is attained with the recently
the searching cost is approximately equal to training this proposed InfoBatch algorithm [39]. It can be observed that
small model for 2 3 times. Second, in many cases, it may the combination of EfficientTrain++ and InfoBatch further
∼
not be necessary to conduct searching on the new dataset improves training efficiency compared to either individual
(oronthefullnewdataset).Forexample,Table22(d)shows method,whiletheeffectofEfficientTrain++ismoresignifi-
thatwhenincreasingtrainingdatascalefromImageNet-1K cant,whichisconsistentwiththefindingsofourpaper.EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 16
7 CONCLUSION [6] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy
considerationsfordeeplearninginnlp,”inACL,2019,pp.3645–
Thispaperaimedtodevelopasimpleandbroadlyapplica- 3650.
ble efficient learning algorithm for modern deep networks [7] C. Li, B. Zhuang, G. Wang, X. Liang, X. Chang, and Y. Yang,
with the potential for widespread implementation. Under “Automatedprogressivelearningforefficienttrainingofvision
transformers,”inCVPR,2022.
thisgoal,weproposedEfficientTrain++,anovelgeneralized
[8] Y.Bengio,J.Louradour,R.Collobert,andJ.Weston,“Curriculum
curriculum learning approach that always leverages all the learning,”inICML,2009,pp.41–48.
data at every learning stage, but only exposes the ‘easier- [9] X.Wang,Y.Chen,andW.Zhu,“Asurveyoncurriculumlearn-
to-learn’patternsofeachexampleatthebeginningoftrain- ing,”IEEETPAMI,2021.
[10] P. Soviany, R. T. Ionescu, P. Rota, and N. Sebe, “Curriculum
ing (e.g., lower-frequency components of images and origi-
learning:Asurvey,”InternationalJournalofComputerVision,pp.
nal information before data augmentation), and gradually
1–40,2022.
introduces more difficult patterns as learning progresses. [11] A. Graves, M. G. Bellemare, J. Menick, R. Munos, and
To design a proper curriculum learning schedule, we K. Kavukcuoglu, “Automated curriculum learning for neural
networks,”inICML,2017,pp.1311–1320.
proposed a tailored computational-constrained sequential
[12] G. Hacohen and D. Weinshall, “On the power of curriculum
searching algorithm, yielding a simple, well-generalizable,
learning in training deep networks,” in ICML, 2019, pp. 2535–
yet surprisingly effective training curriculum. The effec- 2544.
tiveness of EfficientTrain++ is extensively validated on the [13] L.Jiang,D.Meng,Q.Zhao,S.Shan,andA.G.Hauptmann,“Self-
pacedcurriculumlearning,”inAAAI,2015.
large-scale ImageNet-1K/22K datasets, on top of various
[14] L.Jiang,Z.Zhou,T.Leung,L.-J.Li,andL.Fei-Fei,“Mentornet:
state-of-the-art deep networks (e.g., ConvNets, ViTs, and
Learningdata-drivencurriculumforverydeepneuralnetworks
ConvNet-ViT hybrid models), under diverse training set- oncorruptedlabels,”inICML,2018,pp.2304–2313.
tings (e.g., different training budgets, varying test input [15] C. Gong, D. Tao, S. J. Maybank, W. Liu, G. Kang, and J. Yang,
“Multi-modal curriculum learning for semi-supervised image
sizes, and supervised/self-supervised learning scenarios).
classification,”TIP,vol.25,no.7,pp.3249–3260,2016.
Withoutsacrificingmodelperformanceorintroducingnew
[16] Y.Fan,F.Tian,T.Qin,X.-Y.Li,andT.-Y.Liu,“Learningtoteach,”
tunablehyper-parameters,EfficientTrain++reducesthewall- inICLR,2018.
timetrainingcostofvisualbackbonesby1.5−3.0×. [17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
“Imagenet:Alarge-scalehierarchicalimagedatabase,”inCVPR,
Potentially, our work may open new avenues for de-
2009,pp.248–255.
veloping efficient curriculum learning approaches for vi-
[18] Z.Liu,H.Mao,C.-Y.Wu,C.Feichtenhofer,T.Darrell,andS.Xie,
sion models. For example, future work may explore more “Aconvnetforthe2020s,”inCVPR,2022.
general forms of the transformation function for extracting [19] H.Touvron,M.Cord,M.Douze,F.Massa,A.Sablayrolles,and
H.Je´gou,“Trainingdata-efficientimagetransformers&distilla-
‘easier-to-learn’patterns,e.g.,formulatingalearnableneural
tionthroughattention,”inICML,2021,pp.10347–10357.
network under principles like meta-learning, or consid-
[20] W.Wang,E.Xie,X.Li,D.-P.Fan,K.Song,D.Liang,T.Lu,P.Luo,
ering differentiating between the patterns within ‘easier- andL.Shao,“Pyramidvisiontransformer:Aversatilebackbone
to-learn’ and ‘harder-to-learn’ spatial-temporal regions of for dense prediction without convolutions,” in ICCV, 2021, pp.
568–578.
visiondata.Moreover,itwouldbeinterestingtoinvestigate
[21] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen,
howtouncoverprogressivelymoredifficultpatternsduring
and B. Guo, “Cswin transformer: A general vision transformer
training for more general data categories, e.g., for tabular backbonewithcross-shapedwindows,”inCVPR,2022.
data,wemayconsidermaskingoutsomefeatures(columns) [22] W. Yu, C. Si, P. Zhou, M. Luo, Y. Zhou, J. Feng, S. Yan,
and X. Wang, “Metaformer baselines for vision,” arXiv preprint
toonlyexposecertain‘easier-to-learn’relationshipsbetween
arXiv:2210.13452,2022.
differentfeaturesatearliertrainingstages.
[23] K.He,X.Chen,S.Xie,Y.Li,P.Dolla´r,andR.Girshick,“Masked
autoencoders are scalable vision learners,” in CVPR, 2022, pp.
16000–16009.
ACKNOWLEDGMENTS [24] Y.Wang,Y.Yue,R.Lu,T.Liu,Z.Zhong,S.Song,andG.Huang,
“Efficienttrain: Exploring generalized curriculum learning for
ThisworkwassupportedinpartbytheNationalKeyR&D trainingvisualbackbones,”inICCV,2023.
Program of China under Grant 2021ZD0140407, in part by [25] J.L.Elman,“Learninganddevelopmentinneuralnetworks:The
importanceofstartingsmall,”Cognition,vol.48,no.1,pp.71–99,
the National Natural Science Foundation of China under
1993.
Grants62022048and62276150,andinpartbytheGuoqiang
[26] K.A.KruegerandP.Dayan,“Flexibleshaping:Howlearningin
InstituteofTsinghuaUniversity. smallstepshelps,”Cognition,vol.110,no.3,pp.380–394,2009.
[27] M. Kumar, B. Packer, and D. Koller, “Self-paced learning for
latentvariablemodels,”inNeurIPS,2010.
REFERENCES [28] E. A. Platanios, O. Stretcu, G. Neubig, B. Po´czos, and T. M.
Mitchell,“Competence-basedcurriculumlearningforneuralma-
[1] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningfor chinetranslation,”inNAACL-HLT(1),2019.
imagerecognition,”inCVPR,2016,pp.770–778. [29] X.ChenandA.Gupta,“Weblysupervisedlearningofconvolu-
[2] G.Huang,Z.Liu,G.Pleiss,L.v.d.Maaten,andK.Q.Weinberger, tionalnetworks,”inICCV,2015,pp.1431–1439.
“Convolutionalnetworkswithdenseconnectivity,”IEEETPAMI, [30] Y.Wei,X.Liang,Y.Chen,X.Shen,M.-M.Cheng,J.Feng,Y.Zhao,
vol.44,no.12,pp.8704–8716,2022. and S. Yan, “Stc: A simple to complex framework for weakly-
[3] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai, supervisedsemanticsegmentation,”IEEETPAMI,vol.39,no.11,
T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly, pp.2314–2320,2016.
J.Uszkoreit,andN.Houlsby,“Animageisworth16x16words: [31] R. Tudor Ionescu, B. Alexe, M. Leordeanu, M. Popescu, D. P.
Transformersforimagerecognitionatscale,”inICLR,2021. Papadopoulos,andV.Ferrari,“Howhardcanitbe?estimating
[4] Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,andB.Guo, thedifficultyofvisualsearchinanimage,”inCVPR,2016,pp.
“Swintransformer:Hierarchicalvisiontransformerusingshifted 2157–2166.
windows,”inICCV,2021,pp.10012–10022. [32] J.G.TullisandA.S.Benjamin,“Ontheeffectivenessofself-paced
[5] X.Zhai,A.Kolesnikov,N.Houlsby,andL.Beyer,“Scalingvision learning,”Journalofmemoryandlanguage,vol.64,no.2,pp.109–
transformers,”inCVPR,2022. 118,2011.EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 17
[33] L. Jiang, D. Meng, T. Mitamura, and A. G. Hauptmann, “Easy [63] R. G. Lopes, D. Yin, B. Poole, J. Gilmer, and E. D. Cubuk,
samplesfirst:Self-pacedrerankingforzero-examplemultimedia “Improving robustness without sacrificing accuracy with patch
search,”inACMMM,2014,pp.547–556. gaussianaugmentation,”arXivpreprintarXiv:1906.02611,2019.
[34] D.Weinshall,G.Cohen,andD.Amir,“Curriculumlearningby [64] S.PaulandP.-Y.Chen,“Visiontransformersarerobustlearners,”
transferlearning:Theoryandexperimentswithdeepnetworks,” inAAAI,2022.
inICML,2018,pp.5238–5246. [65] Y.Freund,R.E.Schapireetal.,“Experimentswithanewboosting
[35] M.Ren,W.Zeng,B.Yang,andR.Urtasun,“Learningtoreweight algorithm,”inICML,1996,pp.148–156.
examples for robust deep learning,” in ICML, 2018, pp. 4334– [66] G. Alain, A. Lamb, C. Sankar, A. Courville, and Y. Bengio,
4343. “Variancereductioninsgdbydistributedimportancesampling,”
[36] D. Zhang, J. Han, L. Zhao, and D. Meng, “Leveraging prior- arXivpreprintarXiv:1511.06481,2015.
knowledgeforweaklysupervisedobjectdetectionunderacol- [67] I. Loshchilov and F. Hutter, “Online batch selection for faster
laborativeself-pacedcurriculumlearningframework,”IJCV,vol. training of neural networks,” arXiv preprint arXiv:1511.06343,
127,no.4,pp.363–380,2019. 2015.
[37] T. Matiisen, A. Oliver, T. Cohen, and J. Schulman, “Teacher– [68] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-
student curriculum learning,” IEEE TNNLS, vol. 31, no. 9, pp. based object detectors with online hard example mining,” in
3732–3740,2019. CVPR,2016,pp.761–769.
[38] T. Zhou, S. Wang, and J. Bilmes, “Curriculum learning by dy- [69] S. Gopal, “Adaptive sampling for sgd by exploiting side infor-
namicinstancehardness,”inNeurIPS,2020,pp.8602–8613. mation,”inICML,2016,pp.364–372.
[39] Z. Qin, K. Wang, Z. Zheng, J. Gu, X. Peng, Z. Xu, D. Zhou, [70] T.Pi,X.Li, Z.Zhang,D.Meng, F.Wu,J.Xiao, andY.Zhuang,
L.Shang,B.Sun,X.Xie,andY.You,“Infobatch:Losslesstraining “Self-pacedboostlearningforclassification.”inIJCAI,2016,pp.
speedupbyunbiaseddynamicdatapruning,”inICLR,2024. 1932–1938.
[40] T.ZhouandJ.Bilmes,“Minimaxcurriculumlearning:Machine [71] S.Braun,D.Neil,andS.-C.Liu,“Acurriculumlearningmethod
teachingwithdesirabledifficultiesandscheduleddiversity,”in forimprovednoiserobustnessinautomaticspeechrecognition,”
ICLR,2018. inEUSIPCO,2017,pp.548–552.
[41] S.Guo,W.Huang,H.Zhang,C.Zhuang,D.Dong,M.R.Scott, [72] X. Zhang, G. Kumar, H. Khayrallah, K. Murray, J. Gwinnup,
and D. Huang, “Curriculumnet: Weakly supervised learning M. J. Martindale, P. McNamee, K. Duh, and M. Carpuat, “An
fromlarge-scalewebimages,”inECCV,2018,pp.135–150. empiricalexplorationofcurriculumlearningforneuralmachine
[42] S. Sinha, A. Garg, and H. Larochelle, “Curriculum by smooth- translation,”arXivpreprintarXiv:1811.00739,2018.
ing,”inNeurIPS,2020,pp.21653–21664. [73] W. Wang, I. Caswell, and C. Chelba, “Dynamically composing
[43] P.Morerio,J.Cavazza,R.Volpi,R.Vidal,andV.Murino,“Cur- domain-dataselectionwithclean-dataselectionby“co-curricular
riculumdropout,”inICCV,2017,pp.3544–3552. learning”forneuralmachinetranslation,”inACL,2019,pp.1282–
[44] U¨.Dogan,A.A.Deshmukh,M.B.Machura,andC.Igel,“Label- 1292.
similaritycurriculumlearning,”inECCV,2020,pp.174–190. [74] F.Liu,Y.Tian,Y.Chen,Y.Liu,V.Belagiannis,andG.Carneiro,
“Acpl: Anti-curriculum pseudo-labelling for semi-supervised
[45] S. Fahlman and C. Lebiere, “The cascade-correlation learning
medicalimageclassification,”inCVPR,2022,pp.20697–20706.
architecture,”inNeurIPS,1989.
[75] F.W.CampbellandJ.G.Robson,“Applicationoffourieranalysis
[46] R. Lengelle´ and T. Denoeux, “Training mlps layer by layer
to the visibility of gratings,” The Journal of physiology, vol. 197,
usinganobjectivefunctionforinternalrepresentations,”Neural
no.3,p.551,1968.
Networks,vol.9,no.1,pp.83–97,1996.
[76] W.Sweldens,“Theliftingscheme:Aconstructionofsecondgen-
[47] Y.Bengio, P. Lamblin, D.Popovici, and H.Larochelle, “Greedy
erationwavelets,”SIAMjournalonmathematicalanalysis,vol.29,
layer-wisetrainingofdeepnetworks,”inNeurIPS,2006.
no.2,pp.511–546,1998.
[48] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning
[77] S.Mallat,Awavelettourofsignalprocessing. Elsevier,1999.
algorithmfordeepbeliefnets,”Neuralcomputation,vol.18,no.7,
[78] Y.Chen,H.Fan,B.Xu,Z.Yan,Y.Kalantidis,M.Rohrbach,S.Yan,
pp.1527–1554,2006.
and J. Feng, “Drop an octave: Reducing spatial redundancy
[49] K. Simonyan and A. Zisserman, “Very deep convolutional net-
in convolutional neural networks with octave convolution,” in
worksforlarge-scaleimagerecognition,”inICLR,2014.
ICCV,2019,pp.3435–3444.
[50] G.Wang,X.Xie,J.Lai,andJ.Zhuo,“Deepgrowinglearning,”in
[79] L.Yang,Y.Han,X.Chen,S.Song,J.Dai,andG.Huang,“Reso-
ICCV,2017,pp.2812–2820.
lutionadaptivenetworksforefficientinference,”inCVPR,2020,
[51] T.Karras,T.Aila,S.Laine,andJ.Lehtinen,“Progressivegrowing
pp.2369–2378.
of gans for improved quality, stability, and variation,” in ICLR,
[80] Y. Wang, K. Lv, R. Huang, S. Song, L. Yang, and G. Huang,
2018.
“Glance and focus: a dynamic approach to reducing spatial
[52] T. Chen, I. Goodfellow, and J. Shlens, “Net2net: Accelerating
redundancyinimageclassification,”inNeurIPS,2020,pp.2432–
learningviaknowledgetransfer,”inICLR,2015.
2444.
[53] T.Wei,C.Wang,Y.Rui,andC.W.Chen,“Networkmorphism,” [81] Y. Wang, R. Huang, S. Song, Z. Huang, and G. Huang, “Not
inICML,2016,pp.564–572.
all images are worth 16x16 words: Dynamic transformers for
[54] T. Wei, C. Wang, and C. W. Chen, “Modularized morphing of efficientimagerecognition,”inNeurIPS,vol.34,2021,pp.11960–
deep convolutional neural networks: A graph approach,” IEEE 11973.
TransactionsonComputers,vol.70,no.2,pp.305–315,2020. [82] C. Cortes, M. Mohri, and A. Rostamizadeh, “Algorithms for
[55] zhuofanxia,X.Pan,X.Jin,Y.He,H.Xue’,S.Song,andG.Huang, learning kernels based on centered alignment,” JMLR, vol. 13,
“Budgetedtrainingforvisiontransformer,”inICLR,2023. pp.795–828,2012.
[56] C.Dong,L.Liu,Z.Li,andJ.Shang,“Towardsadaptiveresidual [83] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton, “Similarity
networktraining:Aneural-odeperspective,”inICML,2020. ofneuralnetworkrepresentationsrevisited,”inICML,2019,pp.
[57] M.Li,E.Yumer,andD.Ramanan,“Budgetedtraining:Rethink- 3519–3529.
ingdeepneuralnetworktrainingunderresourceconstraints,”in [84] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le,
ICLR,2020. “Autoaugment:Learningaugmentationstrategiesfromdata,”in
[58] M. Tan and Q. Le, “Efficientnetv2: Smaller models and faster CVPR,2019,pp.113–123.
training,”inICML,2021,pp.10096–10106. [85] X. Zhang, Q. Wang, J. Zhang, and Z. Zhong, “Adversarial au-
[59] H.Touvron,M.Cord,andH.Jegou,“Deitiii:Revengeofthevit,” toaugment,”inICLR,2019.
inECCV,2022. [86] Y.Wang,G.Huang,S.Song,X.Pan,Y.Xia,andC.Wu,“Regu-
[60] H. Touvron, A. Vedaldi, M. Douze, and H. Je´gou, “Fixing the larizingdeepnetworkswithsemanticdataaugmentation,”IEEE
train-testresolutiondiscrepancy,”inNeurIPS,2019. TPAMI,vol.44,no.7,pp.3733–3748,2021.
[61] H. Wang, X. Wu, Z. Huang, and E. P. Xing, “High-frequency [87] K. Tian, C. Lin, M. Sun, L. Zhou, J. Yan, and W. Ouyang, “Im-
component helps explain the generalization of convolutional proving auto-augment via augmentation-wise weight sharing,”
neuralnetworks,”inCVPR,2020,pp.8684–8694. inNeurIPS,2020,pp.19088–19098.
[62] D.Yin,R.GontijoLopes,J.Shlens,E.D.Cubuk,andJ.Gilmer, [88] E. D. Cubuk, B. Zoph, J. Shlens, and Q. Le, “Randaugment:
“Afourierperspectiveonmodelrobustnessincomputervision,” Practical automated data augmentation with a reduced search
inNeurIPS,2019. space,”inNeurIPS,2020,pp.18613–18624.EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 18
[89] K. Turkowski, “Filters for common resampling tasks,” Graphics [115] S.Yun,D.Han,S.J.Oh,S.Chun,J.Choe,andY.Yoo,“Cutmix:
gems,pp.147–165,1990. Regularizationstrategytotrainstrongclassifierswithlocalizable
[90] W. Burger and M. J. Burge, Principles of digital image processing: features,”inICCV,2019,pp.6023–6032.
corealgorithms. SpringerScience&BusinessMedia,2010. [116] Z.Zhong,L.Zheng,G.Kang,S.Li,andY.Yang,“Randomerasing
[91] P. Goyal, P. Dolla´r, R. Girshick, P. Noordhuis, L. Wesolowski, dataaugmentation,”inAAAI,2020,pp.13001–13008.
A. Kyrola, A. Tulloch, Y. Jia, and K. He, “Accurate, large [117] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,
minibatch sgd: Training imagenet in 1 hour,” arXiv preprint “Rethinking the inception architecture for computer vision,” in
arXiv:1706.02677,2017. CVPR,2016,pp.2818–2826.
[118] G.Huang,Y.Sun,Z.Liu,D.Sedra,andK.Q.Weinberger,“Deep
[92] Y.You,J.Li,S.Reddi,J.Hseu,S.Kumar,S.Bhojanapalli,X.Song,
networkswithstochasticdepth,”inECCV,2016,pp.646–661.
J.Demmel,K.Keutzer,andC.-J.Hsieh,“Largebatchoptimization
fordeeplearning:Trainingbertin76minutes,”inICLR,2019. [119] H.Touvron,M.Cord,A.Sablayrolles,G.Synnaeve,andH.Je´gou,
“Goingdeeperwithimagetransformers,”inICCV,2021,pp.32–
[93] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and
42.
A. Torralba, “Semantic understanding of scenes through the
[120] B. T. Polyak and A. B. Juditsky, “Acceleration of stochastic
ade20kdataset,”InternationalJournalofComputerVision,vol.127,
approximation by averaging,” SIAM journal on control and opti-
pp.302–321,2019.
mization,vol.30,no.4,pp.838–855,1992.
[94] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,
[121] P.Micikevicius,S.Narang,J.Alben,G.Diamos,E.Elsen,D.Gar-
P.Dolla´r,andC.L.Zitnick,“Microsoftcoco:Commonobjectsin
cia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al.,
context,”inECCV,2014,pp.740–755.
“Mixedprecisiontraining,”inICLR,2018.
[95] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of [122] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor,
featuresfromtinyimages,”2009. “Imagenet-21k pretraining for the masses,” arXiv preprint
[96] M.-E. Nilsback and A. Zisserman, “Automated flower classi- arXiv:2104.10972,2021.
fication over a large number of classes,” in 2008 Sixth Indian [123] Z. Xia, X. Pan, S. Song, L. E. Li, and G. Huang, “Vision trans-
ConferenceonComputerVision,Graphics&ImageProcessing,2008, formerwithdeformableattention,”inCVPR,2022.
pp.722–729. [124] M.Raghu,T.Unterthiner,S.Kornblith,C.Zhang,andA.Doso-
[97] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li, “Novel vitskiy, “Do vision transformers see like convolutional neural
datasetforfine-grainedimagecategorization:Stanforddogs,”in networks?”inNeurIPS,2021.
CVPRW,2011.
[98] G.VanHorn,S.Branson,R.Farrell,S.Haber,J.Barry,P.Ipeirotis,
P.Perona,andS.Belongie,“Buildingabirdrecognitionappand
largescaledatasetwithcitizenscientists:Thefineprintinfine-
graineddatasetcollection,”inCVPR,2015,pp.595–604.
[99] L.Bossard,M.Guillaumin,andL.VanGool,“Food-101–mining
discriminativecomponentswithrandomforests,”inECCV,2014,
pp.446–461.
[100] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski,
and A. Joulin, “Emerging properties in self-supervised vision
transformers,”inICCV,2021,pp.9650–9660.
[101] X.Chen,S.Xie,andK.He,“Anempiricalstudyoftrainingself-
supervisedvisiontransformers,”inICCV,2021.
[102] H.Bao,L.Dong,S.Piao,andF.Wei,“BEit:BERTpre-trainingof
imagetransformers,”inICLR,2022.
[103] C. Wei, H. Fan, S. Xie, C.-Y. Wu, A. Yuille, and C. Feichten-
hofer,“Maskedfeaturepredictionforself-supervisedvisualpre-
training,”inCVPR,2022,pp.14668–14678.
[104] J.Chen,M.Hu,B.Li,andM.Elhoseiny,“Efficientself-supervised
vision pretraining with local masked reconstruction,” arXiv
preprintarXiv:2206.00790,2022.
[105] X.Chen,M.Ding,X.Wang,Y.Xin,S.Mo,Y.Wang,S.Han,P.Luo,
G.Zeng,andJ.Wang,“Contextautoencoderforself-supervised
representationlearning,”IJCV,pp.1–16,2023.
[106] H. Wang, Y. Tang, Y. Wang, J. Guo, Z.-H. Deng, and K. Han,
“Maskedimagemodelingwithlocalmulti-scalereconstruction,”
inCVPR,2023,pp.2122–2131.
[107] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford,
M.Chen,andI.Sutskever,“Zero-shottext-to-imagegeneration,”
inInternationalConferenceonMachineLearning. PMLR,2021,pp.
8821–8831.
[108] K.Chen,J.Wang,J.Pang,Y.Cao,Y.Xiong,X.Li,S.Sun,W.Feng,
Z.Liu,J.Xuetal.,“MMDetection:Openmmlabdetectiontoolbox
andbenchmark,”arXivpreprintarXiv:1906.07155,2019.
[109] T.Xiao,Y.Liu,B.Zhou,Y.Jiang,andJ.Sun,“Unifiedperceptual
parsingforsceneunderstanding,”inECCV,2018,pp.418–434.
[110] M. Contributors, “MMSegmentation: Openmmlab semantic
segmentation toolbox and benchmark,” https://github.com/
open-mmlab/mmsegmentation,2020.
[111] T.-Y.Lin,P.Goyal,R.Girshick,K.He,andP.Dolla´r,“Focalloss
fordenseobjectdetection,”inICCV,2017,pp.2980–2988.
[112] Z. Cai and N. Vasconcelos, “Cascade r-cnn: high quality object
detectionandinstancesegmentation,”IEEETPAMI,vol.43,no.5,
pp.1483–1498,2019.
[113] Y.You,J.Li,S.Reddi,J.Hseu,S.Kumar,S.Bhojanapalli,X.Song,
J.Demmel,K.Keutzer,andC.-J.Hsieh,“Largebatchoptimization
fordeeplearning:Trainingbertin76minutes,”inICLR,2020.
[114] H.Zhang,M.Cisse,Y.N.Dauphin,andD.Lopez-Paz,“mixup:
Beyondempiricalriskminimization,”inICLR,2018.EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 19
APPENDIX FOR
“EFFICIENTTRAIN++: GENERALIZED CURRICULUM
LEARNING FOR EFFICIENT VISUAL BACKBONE TRAINING”
APPENDIX A
IMPLEMENTATION DETAILS
A.1 TrainingModelsonImageNet-1K
Dataset. We use the data provided by ILSVRC20121 [17]. The dataset includes 1.2 million images for training and 50,000
imagesforvalidation,bothofwhicharecategorizedin1,000classes.
Training.Ourapproachisdevelopedontopofastate-of-the-arttrainingpipelineofdeepnetworks,whichincorporates
a holistic combination of various model regularization & data augmentation techniques, and is widely applied to train
recently proposed models [4], [18], [19], [20], [21], [22]. Our training settings for the standard case (300-epoch training
forthebaselinesandEfficientTrain;200-epochtrainingforEfficientTrain++)aresummarizedinTable24,whichgenerally
follow from [18], except for the following differences. We modify the configurations of weight decay, stochastic depth
andexponentialmovingaverage(EMA)accordingtotherecommendationintheoriginalpapersofdifferentmodels(i.e.,
ConvNeXt [18], DeiT [19], PVT [20], Swin Transformer [4], CSWin Transformer [21], and CAFormer [22])2. In addition,
following [22], the training of CSWin-Base [21] and CAFormer [22] adopts the LAMB optimizer [113] with an initial
learningrateof8e-3,whichcontributestoastablelearningprocedureforthesemodelswiththebatchsizeof4096.
When we reduce the training cost on the basis of the standard case (300-epoch training for the baselines; 200-epoch
training for EfficientTrain++), we linearly reduce the maximum value of the increasing stochastic depth regularization
simultaneously. This configuration is motivated by that relatively less computational budgets for training require weaker
regularization.Notably,bothbaselinesandourmethodadoptthissetting,whichconsistentlyimprovestheaccuracy.
On top of the baselines, EfficientTrain and EfficientTrain++ only modify the terms mentioned in Tables 5 and 6,
respectively. The only exception is, when training ConvNeXts, we replace B = 96 with B = 160 in EfficientTrain++, as
B =96willmakesomeparametersofthe7 7convolutionkernelswithinthelastnetworkstageofConvNeXtshavezero
×
gradients. We believe that this straightforward rule for adjustment does not degrade the effectiveness of EfficientTrain++
oraffectthemajorcontributionsofourmethod.Inaddition,notethatthelow-frequencycroppingoperationinourmethod
leads to a varying input size during training. On this issue, visual backbones can naturally process different sizes of
inputs with no or minimal modifications. Specifically, once the input size varies, ResNets and ConvNeXts do not need
any change, while vision Transformers (i.e., DeiT, PVT, Swin, CSWin, an CAFormer) only need to resize their position
biascorrespondingly,assuggestedintheirpapers.Ourmethodstartsthetrainingwithsmall-sizeinputsandthereduced
computationalcost.Theinputsizeisswitchedmidwayinthetrainingprocess,whereweresizethepositionbiasforViTs
(donothingforConvNets).Finally,thelearningendsupwithfull-sizeinputs,asusedattesttime.Asaconsequence,the
overallcomputational/timecosttoobtainthefinaltrainedmodelsiseffectivelysaved.
TrainingConfig Values/Setups
Inputsize 2242
Weightinit. Truncatednormal(0.2)
Optimizer AdamW
Optimizerhyper-parameters β1,β2=0.9,0.999
Initiallearningrate 4e-3
Learningrateschedule Cosineannealing
Weightdecay 0.05
Batchsize 4,096
Trainingepochs 300
Warmupepochs 20
Warmupschedule linear
RandAug[88] (9,0.5)
Mixup[114] 0.8
Cutmix[115] 1.0
Randomerasing[116] 0.25
Labelsmoothing[117] 0.1
Stochasticdepth[118] Followingtheconfigurationsinoriginalpapers[4],[18],[19],[20],[21],[22].
Layerscale[119] 1e-6(ConvNeXt[18])/None(others)
Gradientclip 5.0(DeiT[19],PVT[20]andSwin[4])/None(others)
Exp.mov.avg.(EMA)[120] Followingtheconfigurationsinoriginalpapers[4],[18],[19],[20],[21],[22].
Auto.mix.prec.(AMP)[121] Inactivated(ConvNeXt,following[18])/Activated(others)
TABLE24:Basichyper-parametersandconfigurationsfortrainingmodelsonImageNet-1K
Inference.Following[4],[18],[19],[20],[21],weuseacropratioof0.875and1.0fortheinferenceinputsizeof2242and
3842,respectively.
1.https://image-net.org/index.php
2.ThetrainingofResNet[1]followstherecipeprovidedin[18].EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 20
A.2 ImageNet-22KPre-training
Dataset. The officially released ImageNet-22K dataset [17], [122] from https://www.image-net.org/ is used. Our experi-
mentsarebasedonthelatest‘Winter2021release’version.
Pre-training. We pre-train ConvNeXt-Base/Large and CSWin-Base/Large on ImageNet-22K. The pre-training process
basically follows the training configurations of ImageNet-1K (i.e., Table 24), except for the differences presented in the
following. The basic number of training epochs is set to 150 with a 5-epoch learning rate warm-up. The maximum
value of the increasing stochastic depth regularization [118] is set to 0.1/0.1 for ConvNeXt-Base/Large and 0.2/0.5
for CSWin-Base/Large. Following [21], [22], the initial learning rate for CSWin-Base/Large is set to 2e-3, while the
weight-decay coefficient for CSWin-Base/Large is set to 0.05/0.1. Following [18], we do not leverage the exponential
moving average (EMA) mechanism. To ensure a fair comparison, we report the results of our implementation for both
baselinesandourmethod,wheretheyadoptexactlythesametrainingsettings(apartfromtheconfigurationsmodifiedby
EfficientTrain/EfficientTrain++itself).
Fine-tuning.WeevaluatetheImageNet-22Kpre-trainedmodelsbyfine-tuningthemandreportingthecorresponding
accuracyonImageNet-1K.Thefine-tuningprocessfollowsfrom[18].Inspecific,wedirectlyutilizethebasicconfigurations
in [18], and tune the rate of layer-wise lr decay, stochastic depth regularization, and exponential moving average (EMA)
conditioned on each model, as suggested by [18]. It is worth noting that the baselines and our method adopt exactly the
samefine-tuningsettings.
A.3 ObjectDetectionandSegmentationonCOCO
OurimplementationofRetinaNet[111]followsfrom[123].OurimplementationofCascadeMask-RCNN[112]isthesame
as[18].
A.4 ExperimentsinSection4
In particular, the experimental results provided in Section 4 are based on the training settings listed in Table 24 as well,
expectforthespecifiedmodifications(e.g.,withthelow-passedfilteredinputs).ThecomputingofCKAfeaturesimilarity
followsfrom[124].EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 21
APPENDIX B
80
ADDITIONAL RESULTS
75
B.1 TrainingCurves
70
CurvesofaccuracyduringtrainingarepresentedinFigure8.Thehorizontalaxisdenotes 65
the wall-time training cost. The low-frequency cropping in our method is performed 60
on both the training and test inputs. It is clear that our method learns discriminative DeiT-Small(EfficientTrain++)
55
DeiT-Small(baseline)
representationsmoreefficientlyatearlierepochs.
50
0 25 50 75 100 125 150
Wall-timeTrainingCost(GPU-hours)
Fig.8:Val.accuracyduringtraining.
B.2 ResultsofEfficientTrainwithVaryingEpochs
EfficientTrain can conveniently adapt to a varying number of computational training budgets, i.e., by simply scaling the
indices of epochs in Table 5. As shown in Table 25, the advantage of EfficientTrain is even more significant with fewer
training epochs, e.g., it outperforms the baseline by 0.9% (76.4% v.s. 75.5%) for the 100-epoch trained DeiT-Small (the
speedup is the same as 300-epoch). We attribute this to the greater importance of efficient training algorithms in the
scenariosoflimitedtrainingresources.
Notably, these results are originally reported in the conference version of this paper. We do not directly present them
in Figure 7, since the experimental settings here are slightly different from Figure 7: here the configuration of stochastic
depthregularizationremainsunchangedforalltrainingbudgets.Incontrast,inFigure7,wereplacethisstraightforward
setting with a more appropriate one: the maximum value of the increasing stochastic depth regularization is linearly
decreasedtogetherwiththereductionoftrainingbudgets,motivatedbythatfewertrainingbudgetsmayrequireweaker
regularization. Our new setting significantly improves the validation accuracy for both baselines and our method, making
our experimental results more reasonable. However, we believe that Table 25 may be sufficient to support the following
conclusion: EfficientTrain can conveniently adapt to different numbers of training epochs (i.e., varying training budgets)
utilizingsimplelinearscaling,whichisthemajoraimofTable25.
InputSize Top-1Accuracy(baseline/EfficientTrain) Wall-timeTra-
Models
(inference) 100epochs 200epochs 300epochs iningSpeedup
DeiT-Tiny[19] 2242 65.8%/68.1% 70.5%/71.8% 72.5%/73.3% 1.55×
DeiT-Small[19] 2242 75.5%/76.4% 79.0%/79.1% 80.3%/80.4% 1.51×
Swin-Tiny[4] 2242 78.4%/78.5% 80.6%/80.6% 81.3%/81.4% 1.49×
Swin-Small[4] 2242 80.6%/80.7% 82.7%/82.6% 83.1%/83.2% 1.50×
Swin-Base[4] 2242 80.7%/81.1% 83.2%/83.2% 83.4%/83.6% 1.50×
TABLE 25: Comparison of EfficientTrain and the baselines on top of varying training epochs. EfficientTrain can easily adapt to different
trainingepochsutilizingsimplelinearscaling.Comparedtothebaselines,ourmethodreducesthetrainingcosteffectivelywiththesamenumber
ofepochs,whileachievingcompetitiveorbetteraccuracy.
)%(ycaruccA1-poTEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 22
APPENDIX C
PROOF OF PROPOSITION 1
Inthissection,wetheoreticallydemonstratethedifferencebetweentwotransformations,namelylow-frequencycropping
and image down-sampling. In specific, we will show that from the perspective of signal processing, the former perfectly
preserves the lower-frequency signals within a square region in the frequency domain and discards the rest, while the
imageobtainedfrompixel-spacedown-samplingcontainsthesignalsmixedfrombothlower-andhigher-frequencies.
C.1 Preliminaries
An image can be seen as a high-dimensional data point X RC0×H0×W0, where C 0,H 0,W 0 represent the number of
∈
channels, height and width. Since each channel’s signals are regarded as independent, for the sake of simplicity, we can
focus on a single-channel image with even edge length X R2H×2W. Denote the 2D discrete Fourier transform as ().
∈ F ·
Withoutlossofgenerality,weassumethatthecoordinaterangesare H, H+1,...,H 1 and W, W+1,...,W
{− − − } {− − −
1 .Thevalueofthepixelattheposition[u,v]inthefrequencymapF = (X)iscomputedby
} F
H−1 W−1
(cid:88) (cid:88) (cid:16) (cid:16)ux vy (cid:17)(cid:17)
F[u,v]= X[x,y] exp j2π + .
· − 2H 2W
x=−Hy=−W
Similarly,theinverse2DdiscreteFouriertransformX = −1(F)isdefinedby
F
1 H (cid:88)−1 W (cid:88)−1 (cid:16) (cid:16)ux vy (cid:17)(cid:17)
X[x,y]= F[u,v] exp j2π + .
4HW · 2H 2W
u=−Hv=−W
Denote the low-frequency cropping operation parametrized by the output size (2H′,2W′) as H′,W′(), which gives
C ·
outputsbysimplecropping:
H′W′
H′,W′(F)[u,v]= F[u,v].
C HW ·
Notethathereu H, H+1,...,H 1 ,v W, W+1,...,W 1 ,andthisoperationsimplycopiesthecentral
areaofF
witha∈ sc{ al− ingr− atio.Thescalin− gra}
tio
∈ H′{ W−′ isa− naturaltermfr− om}
thechangeoftotalenergyinthepixels,since
HW
thenumberofpixelsshrinksbytheratioof
H′W′
.
HW
Also,denotethedown-samplingoperationparametrizedbytheratior (0,1]as r().Forsimplicity,wefirstconsider
the case where r = 1 for an integer k N+, and then extend our conclu∈ sions to tD he g· eneral cases where r (0,1]. In
k ∈ ∈
real applications, there are many different down-sampling strategies using different interpolation methods, i.e., nearest,
bilinear,bicubic,etc.Whenkisaninteger,theseoperationscanbemodeledasusingaconstantconvolutionkerneltoaggregate
the neighborhood pixels. Denote this kernel’s parameter as w s,t where s,t
∈
{0,1,...,k −1
}
and (cid:80)k s=− 01(cid:80)k t=− 01w s,t = k1 2.
Thenthedown-samplingoperationcanberepresentedas
k−1k−1
(cid:88)(cid:88)
(X)[x′,y′]= w X[kx′+s,ky′+t].
1/k s,t
D ·
s=0t=0
C.2 Propositions
Nowwearereadytodemonstratethedifferencebetweenthetwooperationsandproveourclaims.Westartbyconsidering
shrinking the image size by k and k is an integer. Here the low-frequency region of an image X RH×W refers to the
∈
signalswithinrange[ H/k,H/k 1] [ W/k,W/k 1]in (X),whiletherestisnamedasthehigh-frequencyregion.
− − × − − F
Proposition 1.1.SupposethattheoriginalimageisX,andthattheimagegeneratedfromthelow-frequencycroppingoperation
isX c = −1 H/k,W/k (X),k N+.WehavethatallthesignalsinthespectralmapofX c isonlyfromthelowfrequency
F ◦C ◦F ∈
regionofX,whilewecanalwaysrecover
H/k,W/k
(X)fromX c.
C ◦F
Proof. The proof of this proposition is simple and straightforward. Take Fourier transform on both sides of the above
transformationequation,weget
(X )= (X).
c H/k,W/k
F C ◦F
DenotethespectralofX casF
c
= (X c)andsimilarlyF = (X).Accordingtoourdefinitionofthecroppingoperation,
F F
weknowthat
H/k W/k 1
F [u,v]= · F[u,v]= F[u,v].
c HW · k2 ·
Hence,thespectralinformationofX
c
simplycopiesX’slowfrequencypartsandconductsauniformscalingbydividing
k2.
Proposition 1.2. Suppose that the original image is X, and that the image generated from the down-sampling operation is
X = (X),k N+. We have that the signals in the spectral map of X have a non-zero dependency on the high frequency
d D1/k ∈ d
regionofX.EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 23
Proof.TakingFouriertransformonbothsides,wehave
(X )= ( (X)).
F d F D1/k
Foranyu [ H/k,H/k 1],v [ W/k,W/k 1],accordingtothedefinitionwehave
∈ − − ∈ − −
H/k−1 W/k−1 (cid:18) (cid:18) (cid:19)(cid:19)
(cid:88) (cid:88) kux kvy
(X )[u,v]= (X)[x,y] exp j2π +
F d D1/k · − 2H 2W
x=−H/ky=−W/k
H/k−1 W/k−1 k−1k−1 (cid:18) (cid:18) (cid:19)(cid:19)
(cid:88) (cid:88) (cid:88)(cid:88) kux kvy
= w s,t ·X[kx+s,ky+t] ·exp −j2π 2H + 2W , (*1)
x=−H/ky=−W/ks=0t=0
whileatthesametimewehavetheinverseDFTforX:
1 H (cid:88)−1 W (cid:88)−1 (cid:18) (cid:18) u′x v′y(cid:19)(cid:19)
X[x,y]= F[u′,v′] exp j2π + . (*2)
2H 2W · 2H 2W
· u′=−Hv′=−W
Plugging(*2)into(*1),itiseasytoseethatessentiallyeachF (u,v) = (X )[u,v]isalinearcombinationoftheoriginal
signalsF[u′,v′].Namely,itcanberepresentedas d F d
H−1 W−1
(cid:88) (cid:88)
F (u,v)= α(u,v,u′,v′) F(u′,v′).
d ·
u′=−Hv′=−W
Therefore,wecancomputethedependencyweightforanygiventuple(u,v,u′,v′)as
α(u,v,u′,v′)=
1 H (cid:88)−1 W (cid:88)−1
w
exp(cid:16) j2π(cid:16)ux
p +
vy p(cid:17)(cid:17) exp(cid:18) j2π(cid:18) u′x
+
v′y(cid:19)(cid:19)
4HW xr,yr · − 2H 2W · 2H 2W
x=−Hy=−W
=
1 H (cid:88)−1 W (cid:88)−1
w
exp(cid:18) j2π(cid:18) u′x −ux
p +
v′y −vy p(cid:19)(cid:19)
,
4HW xr,yr · 2H 2W
x=−Hy=−W
wherex r =x mod k,x p =x x r,samefory r,y p.Furtherdeductionshows
−
α(u,v,u′,v′)=
1 H (cid:88)−1 W (cid:88)−1
w
exp(cid:18) j2π(cid:18)(u′ −u)x p+u′x
r +
(v′ −v)y p+v′y r(cid:19)(cid:19)
4HW xr,yr · 2H 2W
x=−Hy=−W
1 H (cid:88)/k−1 W (cid:88)/k−1 k (cid:88)−1k (cid:88)−1 (cid:18) (cid:18)(u′ u)x′ (v′ v)y′(cid:19)(cid:19) (cid:18) (cid:18) u′s v′t (cid:19)(cid:19)
= w exp j2πk − + − exp j2π +
4HW s,t · 2H 2W · 2H 2W
x′=−H/ky′=−W/ks=0t=0
1 H (cid:88)/k−1 W (cid:88)/k−1 (cid:18) (cid:18)(u′ u)x′ (v′ v)y′(cid:19)(cid:19)k (cid:88)−1k (cid:88)−1 (cid:18) (cid:18) u′s v′t (cid:19)(cid:19)
= exp j2πk − + − w exp j2π + .
4HW 2H 2W s,t · 2H 2W
x′=−H/ky′=−W/k s=0t=0
(cid:16) (cid:16) (cid:17)(cid:17)
Denoteβ(u′,v′)=(cid:80)k s=− 01(cid:80)k t=− 01w
s,t
·exp j2π u 2H′s + 2v W′t ,whichisaconstantconditionedon(u′,v′).Thenweknow
β(u′,v′) H (cid:88)/k−1 W (cid:88)/k−1 (cid:18) (cid:18)(u′ u)x′ (v′ v)y′(cid:19)(cid:19)
α(u,v,u′,v′)= exp j2πk − + −
4HW 2H 2W
x′=−H/ky=−W/k
β(u′,v′) H (cid:88)/k−1 (cid:18) (cid:18)(u′ u)x′(cid:19)(cid:19) W (cid:88)/k−1 (cid:18) (cid:18)(v′ v)y′(cid:19)(cid:19)
= exp j2πk − exp j2πk −
4HW 2H 2W
x′=−H/k y′=−W/k
(cid:40) β(u′,v′), u′ u=a 2H,v′ v =b 2W, a,b Z
= k2 − · k − · k ∈ . (*3)
0, otherwise
Ingeneral,β(u′,v′) = 0whenu′ = c 2H,v′ = d 2W,c,d Z,cd = 0.Hence,when 2H (u′ u),2W (v′ v),wehave
α(u,v,u′,v′) = 0 giv̸ en uv = 0, w̸ hile· αk (u,0,̸ u′,0· ) =k 0 give∈ n u = 0̸ , α(0,v,0,v′) = 0 gk iv| en v− = 0. Tk h| erefo− re, the image
̸ ̸ ̸ ̸ ̸ ̸
generated through down-sampling contains mixed information from both low frequency and high frequency, since most
signalshaveanon-zerodependencyontheglobalsignalsoftheoriginalimage.
Proposition1.3.TheconclusionsofProposition1.1andProposition1.2stillholdwhenk Q+ isnotaninteger.
∈EFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 24
Proof.ItisobviousthatProposition1.1canbenaturallyextendedtok Q+.Therefore,herewefocusonProposition
1.2.First,considerup-samplinganimageX bym N+ timeswiththenea∈ restinterpolation,namely
∈
X [mx+s,my+t]=X[x,y], s,t 0,1,...,m 1 .
up ∈{ − }
TakingFouriertransform,wehave
mH−1 mW−1 (cid:18) (cid:18) (cid:19)(cid:19)
(cid:88) (cid:88) ux kvy
(X )[u,v]= X [x,y] exp j2π +
F up up · − 2mH 2mW
x=−mHy=−mW
H (cid:88)−1 W (cid:88)−1 m (cid:88)−1m (cid:88)−1 (cid:18) (cid:18) u(mx+s) v(my+t)(cid:19)(cid:19)
= X[x,y] exp j2π + . (*4)
· − 2mH 2mW
x=−Hy=−W s=0 t=0
SimilartotheproofofProposition1.2,byplugging(*2)into(*4),itiseasytoseethatF (u,v) = (X )[u,v]isalinear
up F up
combinationofthesignalsfromtheoriginalimage.Namely,wehave
H−1 W−1
(cid:88) (cid:88)
F (u,v)= α (u,v,u′,v′) F(u′,v′).
up up ·
u′=−Hv′=−W
Givenany(u,v,u′,v′),α (u,v,u′,v′)canbecomputedas
up
1 H (cid:88)−1 W (cid:88)−1 m (cid:88)−1m (cid:88)−1 (cid:18) (cid:18) u(mx+s) v(my+t)(cid:19)(cid:19) (cid:18) (cid:18) u′x v′y(cid:19)(cid:19)
α (u,v,u′,v′)= exp j2π + exp j2π +
up 4HW − 2mH 2mW · 2H 2W
x=−Hy=−W s=0 t=0
1 H (cid:88)−1 W (cid:88)−1 (cid:18) (cid:18)(u′ u)x (v′ v)y(cid:19)(cid:19)m (cid:88)−1m (cid:88)−1 (cid:18) (cid:18) us vt (cid:19)(cid:19)
= exp j2π − + − exp j2π + .
4HW 2H 2W − 2mH 2mW
x=−Hy=−W s=0 t=0
Denoteβ (u,v)=(cid:80)m−1(cid:80)m−1exp(cid:0) j2π(cid:0) us + vt (cid:1)(cid:1) ,whichisaconstantconditionedon(u,v).Thenweknow
up s=0 t=0 − 2mH 2mW
α
(u,v,u′,v′)=β up(u,v) H (cid:88)−1 W (cid:88)−1 exp(cid:18) j2π(cid:18)(u′ −u)x
+
(v′ −v)y(cid:19)(cid:19)
up 4HW 2H 2W
x=−Hy=−W
=β up(u,v) H (cid:88)−1 exp(cid:18) j2π(cid:18)(u′ −u)x(cid:19)(cid:19) W (cid:88)−1 exp(cid:18) j2π(cid:18)(v′ −v)y(cid:19)(cid:19)
4HW 2H 2W
x=−H y=−W
(cid:40)
β (u,v), u′ u=a 2H,v′ v =b 2W, a,b Z
= up − · − · ∈ . (*5)
0, otherwise
Since H u H 1, W v W 1, we have β (u,v) = 0. Thus, we have α (u,v,u′,v′) = 0 when 2H (u′
u),2W− (v′≤ v).≤ − − ≤ ≤ − up ̸ up ̸ | −
| −
Now we return to Proposition 1.3. Suppose that the original image is X, and that the image obtained through down-
samplingisX d = D1/k(X),wherek ∈ Q+ maynotbeaninteger.Wecanalwaysfindtwointegersm 0 andk 0 suchthat
mk0
0
= k. Consider first up-sampling X by m 0 times with the nearest interpolation and then performing down-sampling
byk 0 times.Bycombining(*3)and(*5),itiseasytoverifythatProposition1.3istrue.
APPENDIX D
RELATIONSHIPS BETWEEN LOW-PASS FILTERING AND 2D SINC CONVOLUTION
Inthissection,wetheoreticallydemonstratethatthelow-passfilteringwithaB Bsquarefilterisequivalenttoperforming
×
a convolution operation on the image with a kernel function called 2D sinc function. This is actually a classical result in
the field of signal processing, indicating that low-pass filtering can be efficiently implemented by the convolution with a
certainkernel.
D.1 Preliminaries
The basic preliminaries (e.g., 2D discrete Fourier transform and the inverse transform) are the same as Appendix C.1). In
particular,herewedefinetheprocedureoflow-passfilteringwithaB B squarefilteras low(),namely,
× TB,B ·
(cid:40)
F[u,v], B u,v B 1
low(F)[u,v]= −2 ≤ ≤ 2 − .
TB,B 0, otherwiseEFFICIENTTRAIN++:GENERALIZEDCURRICULUMLEARNINGFOREFFICIENTVISUALBACKBONETRAINING 25
D.2 Proposition
Proposition2.1.Low-passfilteringwithaB Bsquarefilterisequivalenttoperformingaconstant-kernelconvolutionontheoriginal
×
image.
Proof.Takinginverse2DdiscreteFouriertransformover low(F),wehave
TB,B
B−1 B−1
1 (cid:88)2 (cid:88)2 (cid:16) (cid:16)ux vy (cid:17)(cid:17)
−1( low(F))[x,y]= F[u,v] exp j2π +
F TB,B 4HW · 2H 2W
u=−B v=−B
2 2
1 B (cid:88)2−1 B (cid:88)2−1 (cid:16) (cid:16)ux vy (cid:17)(cid:17) H (cid:88)−1 W (cid:88)−1 (cid:18) (cid:18) us vt (cid:19)(cid:19)
= exp j2π + X[s,t] exp j2π +
4HW 2H 2W · − 2H 2W
u=−B v=−B s=−Ht=−W
2 2
1 B (cid:88)2−1 B (cid:88)2−1 H (cid:88)−1 W (cid:88)−1 (cid:18) (cid:18) u(x s) v(y t)(cid:19)(cid:19)
= exp j2π − + − X[s,t].
4HW 2H 2W
u=−B v=−B s=−Ht=−W
2 2
Herewecansee,everypixelofthenewtransformedimage −1( low(F))isessentiallyalinearcombinationoftheoriginal
F TB,B
pixels,whichcanberepresentedas
H−1 W−1
(cid:88) (cid:88)
−1( low(F))[x,y]= α(x,y,s,t)X[s,t].
F TB,B
s=−Ht=−W
1 B (cid:88)2−1 B (cid:88)2−1 (cid:18) (cid:18) u(x s) v(y t)(cid:19)(cid:19)
α(x,y,s,t)= exp j2π − + − .
4HW 2H 2W
u=−B v=−B
2 2
Itcanbeobservedthatthevalueofα(x,y,s,t)onlydependsontherelativeposition(x s,y t).Sowefurthersimplify
thenotationofαusingtherelativepositionoftwopoints:α(x′,y′),x′ =x s,y′ =y − t,whe− re
− −
1 B (cid:88)2−1 B (cid:88)2−1 (cid:18) (cid:18) ux′ vy′)(cid:19)(cid:19)
α(x′,y′)= exp j2π +
4HW 2H 2W
u=−B v=−B
2 2
1 B (cid:88)2−1 B (cid:88)2−1 (cid:18) (cid:18) ux′ vy′)(cid:19)(cid:19) (cid:18) (cid:18) ux′ vy′)(cid:19)(cid:19)
= exp j2π + +exp j2π +
8HW 2H 2W − 2H 2W
u=−B v=−B
2 2
1 B (cid:88)2−1 B (cid:88)2−1 (cid:18) πux′ πvy′(cid:19)
= cos + .
4HW H W
u=−B v=−B
2 2
Therefore,weprovethat low()correspondstoaconvolutionoperationonX inthespatialdomainwiththekernelα(, ).
TB,B · · ·
D.3 InfiniteLimit
To better understand this kernel function, we can fix the ratio γ = B as a constant, and let H . For the sake of
simplicity,weassumeH =W
=N,thenthefunctionα(x′,y′)willco2 nH
vergetoalimitfunctionas
→ ∞
1 B (cid:88)2−1 B (cid:88)2−1 (cid:18) (cid:18) x′s y′t(cid:19)(cid:19)
α(x′,y′)= lim exp j2π +
N→∞(2N)2 − 2N 2N
s=−B t=−B
2 2
1 (cid:88)γN (cid:88)γN (cid:18) (cid:18) s t (cid:19)(cid:19)
= lim exp j2π x′ +y′
N→∞(2N)2 − · 2N · 2N
s=−γNt=−γN
1 (cid:90) γ (cid:90) γ
= exp( j2π(x′s+y′t))dsdt
(4γ)2 −
−γ −γ
1 (cid:90) γ (cid:90) γ
= exp( j2πx′s)ds exp( j2πy′t)dt
(4γ)2 − −
−γ −γ
sin(2πγx′)sin(2πγy′)
= .
4π2γ2x′y′
Thislimitfunctioniscalled2Dsincfunction.Itisaclassicalfunctioninsignalprocessing,sinceitistheFouriertransform
and inverse Fourier transform of the 2D square wave function. Here, γ controls the width of the kernel. The larger B is,
thelargerγ is,andthe2Dsincfunctionbecomesmorestrike-likeatpoint(0,0),whichmeanseverypixelonlytakesvalue
fromitself.