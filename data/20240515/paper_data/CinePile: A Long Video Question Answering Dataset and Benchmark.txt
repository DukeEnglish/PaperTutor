CinePile: A Long Video Question Answering Dataset
and Benchmark
♦ ♦ ♣ ♦
RuchitRawal KhalidSaifullah RonenBasri DavidJacobs
GowthamiSomepalli⋆♦ TomGoldstein⋆♦
♦ ♣
UniversityofMaryland,CollegePark WeizmannInstituteofScience
https://hf.co/datasets/tomg-group-umd/cinepile
Figure1:Asampleclip(fromhere)andcorrespondingMCQsfromCinePile.
Abstract
Currentdatasetsforlong-formvideounderstandingoftenfallshortofproviding
genuinelong-formcomprehensionchallenges,asmanytasksderivedfromthese
datasetscanbesuccessfullytackledbyanalyzingjustoneorafewrandomframes
fromavideo. Toaddressthisissue,wepresentanoveldatasetandbenchmark,
CinePile,specificallydesignedforauthenticlong-formvideounderstanding. This
paperdetailsourinnovativeapproachforcreatingaquestion-answerdataset,utiliz-
ingadvancedLLMswithhuman-in-the-loopandbuildinguponhuman-generated
rawdata. Ourcomprehensivedatasetcomprises305,000multiple-choicequestions
(MCQs),coveringvariousvisualandmultimodalaspects,includingtemporalcom-
prehension,understandinghuman-objectinteractions,andreasoningaboutevents
oractionswithinascene. Additionally,weevaluaterecentvideo-centricLLMs,
both open-source and proprietary, on the test split of our dataset. The findings
revealthatevenstate-of-the-artvideo-centricLLMssignificantlylagbehindhuman
performanceinthesetasks,highlightingthecomplexityandchallengeinherentin
videounderstanding.
⋆Equalcontribution.Correspondence:{ruchitr,gowthami}@umd.edu
Preprint.Underreview.
4202
yaM
41
]VC.sc[
1v31880.5042:viXra1 Introduction
Large multi-modal models offer the potential to analyze and understand long, complex videos.
However, training and evaluating models on video data offers difficult challenges. Most videos
containdialogueandpixeldataandcompletesceneunderstandingrequiresboth. Furthermore,most
existingvision-languagemodelsarepre-trainedprimarilyonstillframes,whileunderstandinglong
videosrequirestheabilitytoidentifyinteractionsandplotprogressionsinthetemporaldimension.
In this paper, we introduce CinePile, a large-scale dataset consisting of ∼ 305k question-answer
pairs from 9396 videos, split into train and test sets. Our dataset emphasizes question diversity,
andtopicsspantemporalunderstanding,perceptualanalysis,complexreasoning,andmore. Italso
emphasizesquestiondifficulty,withhumansexceedingthebestcommercialvision/omnimodelsby
approximately26%,andexceedingopensourcevideounderstandingmodelsby70%.
Wepresentasceneandafewquestion-answerpairsfromourdatasetinFig.1. Considerthefirst
question, How does Gru’s emotional state transition throughout the scene? For a
model to answer this correctly, it needs to understand both the visual and temporal aspects, and
evenreasonabouttheplotprogressionofthescene. Toanswerthesecondquestion,What are the
objects poking out of the book cover and what is their purpose, the model must
localizeanobjectintimeandspace,anduseitsworldknowledgetoreasonabouttheirpurpose.
CinePileaddressesseveralweaknessesofexistingvideounderstandingdatasets. First,thelargesize
ofCinePileenablesittoserveasbothaninstruction-tuningdatasetandanevaluationbenchmark. We
believetheabilitytodoinstructiontuningforvideoatalargescalecanbridgethegapbetweenthe
open-sourceandcommercialvideounderstandingmodels. Also,thequestiondiversityinCinePile
makesitamorecomprehensivemeasureofmodelperformancethanexistingbenchmarks. Unlike
existingmetrics(seeSection6fordetailedcomparisons),CinePiledoesnotover-emphasizeonpurely
visualquestions(e.g.,What color is the car?),oronclassificationquestions(e.g.,What genre
is the video?) thatdonotrequiretemporalunderstanding. Rather,CinePileiscomprehensive
withdiversequestionsaboutvision,temporal,andnarrativereasoningwithabreakdownofquestion
typestohelpdevelopersidentifyblindspotsintheirmodels.
The large size of CinePile is made possible by our novel pipeline for automated question gener-
ation and verification using large language models. Our method leverages large existing sets of
audiodescriptionsthathavebeencreatedtoassistthevisionimpaired. Wetranscribetheseaudio
descriptionsandalignthemwithpubliclyavailablemovievideoclipsfromYouTube. Usingthis
detailedhumandescriptionofscenes,powerfulLLMsareabletocreatecomplexanddifficultques-
tionsaboutthewholevideowithoutusingexplicitvideoinput. Attesttime,video-centricmodels
must answer these questions from only the dialogue and raw video, and will not have access to
thehand-writtendescriptionsusedtobuildthequestions. Datasetandartifactsareavailableatthis
https://ruchitrawal.github.io/cinepile.
2 Creatingalongvideounderstandingbenchmark
Ourdatasetcurationprocesshasfourprimarycomponents1)Collectionofrawvideoandrelated
data. 2)Generationofquestiontemplates. 3)Automatedconstructionofthequestion-answerdataset
usingvideoandtemplates,and4)Applicationofafilteringpipelinetoremovemalformedquestions.
2.1 Datacollectionandconsolidation
WeobtainclipsfromEnglish-languagefilmsfromtheYouTubechannelMovieClips1. Thischannel
hostsself-containedclips,eachencapsulatingamajorplotpoint,facilitatingthecreationofadataset
focusedonunderstandingandreasoning. Next,wecollectedAudioDescriptionsfromAudioVault2.
Lastly,wecollectmovieinformationforeachscene,suchasgenre,actors,andmainplotfromIMDB3.
1https://www.youtube.com/@MOVIECLIPS
2https://audiovault.net/movies
3https://www.imdb.com/
2Figure2:Questiontemplategenerationpipeline:Webeginbysubstitutingthefirstnamesinhuman-written
sourcequestionsandthenclusterthem. WethenfeedaselectionofquestionsfromeachclusterintoGPT-4
whichinturnoutputs“questiontemplates”whichareusedinthenextstageofdatasetcreation.SeeSection2.2
formoredetails.
Gettingvisualdescriptionsofvideoforfree. Audiodescriptions(ADs)areaudiotracksformovies
thatfeatureanarratorwhoexplainsthevisualelementscrucialtothestoryduringpausesindialogue.
Theyhavebeencreatedformanymoviestoassistthevisionimpaired. Thekeydistinctionbetween
conventional video caption datasets and ADs lies in the contextual nature of the latter. In ADs,
humans emphasize the important visual elements in their narrations, unlike other video caption
datasets,whichtendtobeoverlydescriptive. Weusetheaudiodescriptionsasaproxyforvisual
annotationinthevideosforourdatasetcreation.
ScenelocalizationinAD.Thevideoclipswehavegatheredaretypically2-3minuteslong,whileAu-
dioDescriptions(ADs)coverentiremovies. Toaligndescriptionswithvideo,wetranscribetheaudio
fromboththemovieclipandthewholemovieADfileusinganAutomaticSpeechRecognition(ASR)
system WhisperX [4], an enhanced version of Whisper [22] designed to offer quicker inference
andmorepreciseword-leveltimestamps. Wethenembedthefirst3andlast3linesofthetexttran-
scriptionofaYouTubemovieclipusingasentenceembeddingmodel,WhereIsAI/UAE-Large-V1.
We similarly embed all the sentences in the corresponding movie AD file. We then localize the
YouTubeclipwithintheADfileviatherollingwindowalgorithm. WethenextractallADdatathat
liesbetweenthematchedstartandendofthemovieclipembeddings. Thislocalizedtextcontains
boththevisualelementsandthedialogueforthegivenYouTubeclip. Thisservesasabasetextfor
creatingtheQAdatasetwhichwewilldiscussintherestofthepaper.
For the rest of the paper, we will refer to the human-written description of the scene as “visual
description”andthespeakingordialoguepartofthevideoas“dialogue”. Whencombined,wewill
refertobothdatasourcesas“scene-text-annotation”.
Sentenceclassification. WhenwetranscribeanADfile,thetextcontainsahuman’svisualdescrip-
tionsandthemovie’sdialogue. However,thetranscriptionmodeldoesnotlabelwhetheragiven
sentencebelongstoavisualdescriptionoradialogue. Sinceweplannedtocreateafewquestions
solelyonthevisualcomponentsofthevideo,thedistinctionisimportanttous. Tocategorizeeach
sentenceaseithervisualordialogue,wefine-tunedaBERT-Basemodel[7]usingannotationsfrom
theMADdataset[25],whichcontainslabelsindicatingwhetherasentenceisadialogueoravisual
description. We applied a binary classification head for this task. For training the classification
model, wesplittheMADdatasetannotationsintoan80-20training-evaluationsplit. Themodel
achieves96%accuracyonevalsplitafter3epochtraining. Qualitatively,weobservedthatthemodel
accuratelyclassifiessentencesinthedatawecurated,distinguishingeffectivelybetweendialogue
andvisualdescriptioncontent.
2.2 AutomatedQuestionTemplates
Many prominent video question-answering benchmarks were written by human annotators. The
question-answerpairsaretypicallycuratedinoneoftwoways: 1)Humanannotatorsaregivencom-
pletefreedomtoaskquestionsaboutagivenscene[28]2)Theyareaskedtofocusonspecificaspects
andaretrainedorprovidedwithexamplesofquestions,encouragingthemtowritemorequestionsin
asimilarstyle[11,13,21,37]. Forinstance,inthePerceptionTestBenchmark[21],annotatorsare
3QuestionTemplateAutomation
Generateafewtemplatesbasedonthefollowingquestions
[Question 1]Wheredidthecouplemeet?
[Question 2]Whatholidayiscomingupwhentheymeet?
[Question 3]Whatmadehimbreakwithher?
ResponseBasedonthenatureofthesequestions,Icanidentifyafewunderlyingtemplatesthatencompassmostofthem:
[Template 1]CatalyticActions
[Proto Question 1]Whateventactsasacatalystforthecharacter’snextmajordecision?
[Template 2]SettingandContext
[Proto Question 2]Wheredoesthisinteractiontakeplace,andhowdoesthelocationimpacttheconversation?
...
Figure3:Extractingtemplatesfromhuman-generatedquestions.Weshare10samplesfromeachquestion
cluster,andpromptanLLMtocreateafewtemplatesandaprototypicalquestion.SeeFig.2andSection2.2for
moredetails.
directedtoconcentrateontemporalorspatialaspects,whilefortheNext-QAdataset[37],annotators
mainlyfocusedontemporalandcausalactionreasoningquestions.
Duringearlyexperimentation,wefoundthatprovidingarangeoftemplatesalongwithscene-text-
annotation to an LLM helped it create more detailed, diverse, and well-formed questions, so we
decidedtouseatemplate-basedapproachforquestiongeneration. Ratherthanconfiningquestionsto
afewhand-curatedthemes,weproposeapipelinetocreatequestiontemplatesnaturallyontopof
human-generatedquestions. Theautomatedquestiontemplategenerationpipelineisillustratedin
Fig.2.
Our starting point is approximately 30,000 human-curated questions from the MovieQA [28],
TVQA [11], and Perception Test [21] datasets. We cluster these questions, select a few repre-
sentatives per cluster, and then use GPT-4 to discern the underlying themes and write a prompt.
First,wepreprocessthequestionsbyreplacingfirstnamesandentitieswithpronouns,asBERT[24]
embeddingsover-indexonpropernouns,hencetheresultantclustersendupwithsharednamesrather
thanthemes. Forinstance,‘WhyisRachelhidinginthebedroom?’ isalteredto‘Whyisshehidingin
thebedroom?’. WeusedGPT-3.5todothisreplacement,asithandlednounreplacementbetterthan
manyopen-sourceandcommercialalternatives. Themodifiedquestionsarethenembeddedusing
WhereIsAI/UAE-Large-V1,asemantictextualsimilaritymodelwhichisatopperformeronthe
MTEBleaderboard4. Whenthefirstnameswerereplaced,weobservedsignificantrepetitionamong
questions,whichpromptedustoduplicatethem,ultimatelyresultingin17,575uniquequestions. We
thenperformk-meansclusteringtocategorizethequestionsintodistinctclusters. Weexperimented
withdifferentvaluesofk =10,50,100. Qualitatively,wefoundk =50tobeanoptimalnumberof
clusterswheretheclustersarediverseandatthesametimeclustersarenottoospecific. Forexample,
weseea‘high-schooldance’specificquestionclusterwhenk =100,andthesequestionsaremerged
intoan‘event’clusterwhenwereducethenumberofclustersto50. ThePerceptionTestquestions
arelessdiverseashumanannotatorswererestrictedtocreatingquestionsbasedonasmallnumberof
themes,soweusedk =20forthisset.
Thenumberofquestionsineachclustervaried,withcountsrangingfrom60to450. Weselected10
randomquestionsfromeach,andusedthemtopromptGPT-4tocreaterelevantquestiontemplates,
asillustratedinFig.3. Wealsoablatedthescenariooftakingtheclosest10questionstothecluster
center, we however qualitatively observed that random questions produced more general/ higher
qualitytemplates.
Wegeneratefourtemplatesforeachquestioncluster,resultinginaround300templatesacrossthree
datasets. Wethenmanuallyreviewedall300templates,eliminatingthosethatwereoverlyspecific
and merging similar ones. Overly specific templates and their proto-questions looked like “Pre-
weddingDilemmas: What complicates character Z’s plans to propose marriage to
their partner?” and “Crime and Consequence: What is the consequence of the
character’s criminal actions?”. The authors also added a few templates that are compli-
4https://huggingface.co/spaces/mteb/leaderboard
4Figure4:AutomatedQAGenerationandFilteringOurprocessbeginswithasetofautomatedtemplatesand
scenes.Initially,wefilteroutthetemplatesrelevanttoeachscene.Next,wepassthesetemplatesalongwiththe
annotated-scene-texttoGPT-4,whichisthenusedtocreatemultiple-choicequestions(MCQs).Thegenerated
MCQsarethensubjectedtonumerousfilterstocuratethefinaldataset.Formoredetailedinformation,referto
Section2.3andSection2.4
mentarytotheauto-generatedones. Thisprocessresultedin86uniquetemplates. Followingthat,
wemanuallybinnedtheseintofourhigh-levelcategories: CharacterandRelationshipDynamics,
NarrativeandPlotAnalysis,ThematicExplorationandSetting,andTechnicalAnalysis.Foradetailed
discussiononeachofthesequestiontemplatecategories,pleaserefertotheAppendix. Wepresent
twosamplequestiontemplatespercategoryandtheirprototypicalquestionsinTable1.
Table1:Sampletemplatesandprototypicalquestionsfromeachofthecategories
Category Questiontemplate Prototypicalquestion
Character and Relationship InterpersonalDynamics Whatchangesoccurintherelationship
Dynamics(CRD) betweenpersonAandpersonBfollow-
ingasharedexperienceoractions?
Character and Relationship DecisionJustification Whatreasonsdidthecharactergivefor
Dynamics(CRD) makingtheirdecision?
Narrative and Plot Analysis CrisisEvent Whatmajoreventleadstothecharacter’s
(NPA) drasticaction?
Narrative and Plot Analysis MysteriesUnveiled What secret does character A reveal
(NPA) abouteventB?
SettingandTechnicalAnaly- PhysicalPossessions Whatis[CharacterName]holding?
sis(STA)
SettingandTechnicalAnaly- EnvironmentalDetails What does the [setting/location]
sis(STA) look like [during/at] [specific time/-
place/event]?
ThematicExploration(TH) SymbolismandMotifTrack- Arethereanysymbolsormotifsintro-
ing ducedinSceneAthatreappearorevolve
inSceneB,andwhatdotheysignify?
ThematicExploration(TH) ThematicParallels Whatdoesthechaosinthesceneparallel
intermsofthemovie’sthemes?
2.3 AutomatedQAgenerationwithLLMs
ThepipelineforgeneratingquestionsisshowninFig.4. Whilethequestiontemplatesaregeneral,
theymightnotberelevanttoallthemovieclips. Henceforagivenscene,wechooseafewrelevant
questiontemplates. Todothis,weprovideGeminiwiththescene-text-annotationofthescene,and
askthemodeltoshortlistthe20mostrelevanttemplatestothatscene,outofwhichwerandomly
select5-6templates.
Wethenprovideacommerciallanguagemodelwith(i)thescene-text-annotation,whichincludes
both visual descriptions and dialogue, (ii) the selected question template names (e.g. ‘Physical
Possession’), (iii) the prototypical questions for the templates (e.g. “What is [Character Name]
holding”),and(iv)asystempromptaskingittowritequestionsaboutthescene. Throughrigorous
experimentation,wedevisedasystempromptthatmakesthemodelattentivetotheentiresceneand
5iscapableofgeneratingdeeper,longer-termquestionsasopposedtomeresurface-levelperceptual
queries. WeobservedthatprovidingtheprototypicalexamplepreventsGPT-4fromhallucination,and
alsoleadstomoreplausiblemultiple-choicequestion(MCQ)distractors. Wealsofoundthatasking
themodeltoproviderationaleforitsanswerenhancesthequalityofthequestions. Additionally,
wefoundthatincludingtimestampsfordialoguesandvisualdescriptionsaugmentsthequalityof
generatedtemporalquestions. Throughthismethod,wewereabletogenerate≈32questionsfor
eachvideointhedataset.
Afterexperimentingwiththispipeline,weanalyzedthegeneratedQApairsandnoticedaconsistent
trend: mostquestionsarefocusedonreasoningorunderstanding. Fordiversity,wealsowantedto
includepurelyperceptualquestions. Toachievethis,weintroducedadditionalhand-craftedprompt
templatesforperceptualquestionsandalsotemplatesfortemporalquestions. WhileGPT-4performs
wellacrossallquestiontemplates,wefoundthatGeminiexcelsparticularlywithperceptualtemplates.
Therefore,weutilizedGeminitogenerateasegmentofperceptualquestionsinthedataset,while
usingGPT-4forreasoningtemplates. Ourexperimentswithopen-sourcemodelsindicatedsubpar
questionquality,despiteextensiveprompttuning. WepresentexamplequestionsgeneratedbyGPT-4
andGemini,alongwiththeirtemplates,intheAppendix.
A small proportion of questions produced by this pipeline can be answered without referring to
theclip,suchasWhat’s the color of the blonde woman’s hair?. Weimplementedafew
checkstoeliminatesuchtrivialorpoorlyframedquestions. Wediscussthesechecksandafewaxes
ofevaluatingthequestion-answeringdatasetinthefollowingsection.
2.4 Testingthequalityofthedataset
Whiletheprocessaboveconsistentlyproduceswell-formedandanswerablequestions,weobserved
thatsomequestionsareeithertrivial,withanswersembeddedwithinthequestionitself,orpertaining
tobasicworldconceptsthatdonotrequireviewingtheclip. Toprunethese,weevaluatedourdataset
withthehelpofafewLLMsonthefollowingaxesandweeitherremovedthequestionsfromthe
datasetorcomputedametricthattheuserscanuseinthedownstreamtasks.
Degeneracy. Aquestionisconsidereddegenerateiftheanswerisimplicitinthequestionitself,e.g.,
What is the color of the pink house?. Inourdataset,thesetypesofquestionsconstitute
onlyasmallfraction. Manuallyreviewingallquestionsbeingimpractical,weemployedthreedistinct
languagemodels(LMs)toautomatethisprocess: Gemini[29],GPT-3.5[1],andPhi-1.5[14]. These
modelsvaryintheirunderlyingtrainingdataandsizes. Wepresentedonlythequestionsandchoices
to these models, omitting any context, and calculated the accuracy of each question across the
multiplemodels. Ifallmodelscorrectlyansweraquestion,itislikelytobedegenerate. Weexcluded
degeneratequestionsfromtheevaluationsplitofourdatasetbutretainedtheminthetrainingset.
VisionReliance. Whengeneratingthemultiple-choicequestions(MCQs),weconsideredtheentire
scenewithoutdifferentiatingbetweenvisualtextanddialogue. Consequently,somequestionsinthe
datasetmightbeanswerablesolelybasedondialogue,withoutthenecessityofthevideocomponent.
Forthisanalysis,weutilizedtheGeminimodel. Themodelwasprovidedwithonlythedialogue,
excludinganyvisualdescriptions,toassessitsperformance.Ifthemodelcorrectlyanswersaquestion,
itisassignedascoreof0forthevisualdependencemetric;ifitfails,thescoreissetat1. Inlater
sections,wepresentthedistributionofthevisualdependencescoresacrossdifferentMCQcategories.
Hardness. Wedevelopedametrictogaugethedifficultyofquestionsforthemodels,evenwhen
providedwithfullcontext. Forthispurpose,weselectedtheGeminimodel,givenitsstatusasoneof
thelargerandmorecapablemodels. Thismetricdiffersfromaccuracy;foraccuracycomputation,
themodelsareonlysuppliedwithvideosanddialogueinformation,excludingvisualdescriptions.
However,incalculatingthehardnessmetric,weincludevisualdescriptionsaspartofthecontext
giventothemodel.
Inadditiontothis,authorsregularlyverifiedthequalityofquestionsacrossmultiplescenesandfixed
anysystemicerrorsthatariseinthepipeline. Next,wediscussahumanstudyweconductedonour
datasetandsomeofthefindings.
6Figure5: Samplefailurecasesfromhumanstudy: Weconductedahumanstudytocheckthequalityof
questionsandwefoundafewsystemicissues.Wefixedallsystemicissuesinthefinalversionofthedataset.
ThemovieclipforQ1canbefoundhere;forQ2,here;forQ3,here;andforQ4,here.
Table2: Wecompareourdataset,CinePileagainstthepre-existingvideo-QAdatasets. Ourdatasetisboth
largeanddiverse.Multimodalreferstowhetherboththevideoandaudiodataisusedforquestioncreationand
answering.ForunderstandingdifferentQAtypes,refertoSection2.3
QAType
Dataset Annotation NumQA Avgsec Multimodal
Temporal Attribute Narrative Theme
TGIF-QA[10] Auto 165,165 3 ✗ ✓ ✗ ✗ ✗
MSRVTT-QA[39] Auto 243,690 15 ✗ ✗ ✓ ✗ ✗
How2QA[13] Human 44,007 60 ✗ ✓ ✓ ✗ ✗
NExT-QA[37] Human 52,044 44 ✗ ✓ ✓ ✗ ✗
EgoSchema[19] Auto 5,000 180 ✗ ✓ ✓ ✓ ✗
MovieQA[28] Human 6,462 203 ✓ ✓ ✓ ✓ ✗
TVQA[11] Human 152,545 76 ✓ ✓ ✓ ✓ ✗
PerceptionTest[21] Human 44,000 23 ✓ ✓ ✓ ✗ ✗
MoVQA[44] Human 21,953 992 ✓ ✓ ✓ ✓ ✗
CinePile(Ours) Human+Auto 303,828 160 ✓ ✓ ✓ ✓ ✓
3 HumanStudy
Oncewefinalizedthepipelineandgeneratedareasonablenumberofquestions,weconductedasmall
humanstudytohelpusfindweaknessesorsystematicproblemswiththedataset. Eachparticipant
(graduatestudentvolunteers)answered10questionsabouttwodifferentrandomlychosenvideos. We
randomlysampledthe10questionsfromthelistofallthequestionswegeneratedforthescene. We
had25participantsexcludingauthors. Wealsointerviewedeachparticipantafterthesurveytoaskif
theyfoundanysystematicissuesinanyofthequestionstheywereaskedtoansweraboutthevideo.
Later,apanelofauthorsauditedallquestionswherehumansgottheanswerwrong. Wenoticedthat
mostofthetimewhenahumangotaquestionwrongitwaslikelyduetooneofthefollowingreasons
(i)duetotheirinabilitytoattendovertheentireclipatonce,(ii)duetotheirinabilitytounderstand
thedialogueorunderstandculturalreferences(iii)carelessnessinanswering,asthecorrectanswer
wasindeedpresentinthevideo. Wedidnoticesomeproblematicpatternswithasmallsubsetof
questions. Themainissueisdistractorsimilarity,wherehumansfoundtwoplausibleanswersand
theychoseonerandomly. WepresentafewsuchexamplesinFig.5. Weremovedthequestionsfrom
thetestsetforwhichwefoundambiguousanswers.
Weagainconductedasecondhumanstudyonthetestset’sfinalversion,andthehumanaccuracy
is73%. Theauthorshaveindependentlytakenthesurvey,andthecorrespondingaccuracyis86%.
Onceagain, acarefulinvestigationbyateamofauthorsindicatesthatevenmostofthesewrong
answersareduetohumanerrorandconfusionoverthemanyeventsinascene.Weconcludefromthis
studythatmanyofthequestionsareanswerablebutdifficult. Wepresentthequestioncategory-level
performanceinSection5.
4 Alookatthedataset
Intheinitialphaseofourdatasetcollection,wecollected∼15,000movieclipsfromchannelslike
MovieClips on YouTube. We filtered out clips that did not have corresponding recordings from
750
Ch Ra er la ac tt ioe nr sa hn ipd T Exh pem lore ation 40.22 40
Dynamics 41.8 3.8 40 34.58 34.13
Temporal 32.14 30 27.6
14.0 30
20
20 16.91 14.69 16.7 13.86
10 10.42 10
9.5
Narrativeand 30.9
PlotAnalysis S Te et ct hin ng icaa lnd 0
CRD NPA STA TEMP TH
0
CRD NPA STA TEMP TH
Analysis Questiontype Questiontype
Figure6: Testsplitstatistics. Left: Questioncategorycompositioninthedataset. Middle: Percentageof
vision-reliantquestionsacrosscategories. Right: Percentageofhardquestionsperquestioncategorytype.
TEMP-Temporal,CRD-CharacterandRelationshipDynamics,NPA-NarrativeandPlotAnalysis,STA-
SettingandTechnicalAnalysis,TH-ThematicExploration.Thecolorscorrespondtothesamecategoriesacross
theplots.RefertotheAppendixforcorrespondingplotsoftrainsplit.
Audiovault,asourquestiongenerationmethodologyreliesontheintegrationofvisualandauditory
cues—interleaved dialogues and descriptive audio—to construct meaningful questions. We also
excludedclipswithlowalignmentscoreswhencomparingtheYouTubeclip’stranscriptionwiththe
localizedscene’stranscriptionintheAudioDescription(AD)fileasdiscussedinSection2.1. This
processresultedinarefineddatasetof9396movieclips. Theaveragevideolengthinourdatasetis
∼160sec,significantlylongerthanmanyotherVideoQAdatasetsandbenchmarks.
Wesplit9396videosintotrainandtestsplitsof9248and148videoseach. Wemadesureboththe
splitsandthesamplingpreservedthedataset’sdiversityintermsofmoviegenresandreleaseyears.
Wefollowthequestion-answergenerationandfilteringpipelinewhichwasthoroughlyoutlinedin
Section2. Weendedupwith298,888trainingpointsand4,940test-setpointswitharound32
questionspervideoscene. EachMCQcontainsaquestion,answer,andfourdistractors. Asaposthoc
step,werandomizedthepositionofthecorrectansweramongthedistractorsforeveryquestion,thus
eliminatinganypositionalbias. Wefilteredoutthedegeneratequestionsfromthetestsplit,however,
welefttheminthetrainset,sincethosequestionsareharmlessandmighteventeachsmallermodels
somehelpfulbiasesthelargermultimodalmodelslikeGeminimightinherentlypossess.
Ourdatasetislargeandvariedbecauseweusedawidevarietyofmovieclipsanddifferentprompting
strategiesaboutdiversequestiontypes. Eachstrategyzeroesinonparticularaspectsofthemovie
content. WepresentasceneandexampleMCQsfromdifferentquestiontemplatesinFig.1. Please
refertotheAppendixforotherexamples. InFig.6(Left),weprovideavisualbreakdownofthe
variouscategoriesofquestionsgeneratedinourdataset. Asignificantportionofthequestionsfalls
under“CharacterRelationshipDynamics”. Thisisattributedtothefactthatalargenumberofour
automatedquestiontemplates,whichwerederivedfromhuman-writtenquestionsbelongedtothis
category. This is followed by “Setting and Technical Analysis” questions, which predominantly
requirevisualinterpretation.
Wedisplaythemetricsforvisionrelianceandquestionhardness,asdiscussedinSection2.4,atthe
category level in Fig. 6 (Middle, Right). As anticipated, questions in the “Setting and Technical
Analysis”categoryexhibitthehighestdependencyonvisualelements,followedbythosein“Character
RelationshipDynamics”,and“Temporal”categories. Intermsofthehardnessmetric,the“Temporal”
categorycontainsthemostchallengingquestions,with“ThematicExploration”followingclosely
behind.
Finally, we compare our dataset with other existing datasets in this field in Table 2, showing its
superiorityinboththenumberofquestionsandaveragevideolengthcomparedtoitscounterparts.
Testsplit. Asmentionedpreviously,ourtestsplitcomprises148videoclips,derivedfrommovies
distinctbetweentrainingandtestingtoavoidinformationleakage. Additionally,wehaveeliminated
alldegeneratequestionsfromthissplit,whichconstituted4.5%ofthegeneratedquestions. Following
severalroundsofmanualcleanupandthoroughtesting,ourfinalcountstandsat4940questions. Of
allthetestquestions,33.21%arereliantonvisualinformation.
8
egatnecrepecnailernoisiV egatnecrepssendraHFigure7:HardquestionsaccordingtohumansandGPT-4V:Afterconductingthehumanstudy,welooked
atthequestionswhichhumangotwrongandthequestionswhichGPT-4gotwrong.Someofthesequestionsare
difficultandcanonlybeansweredbypayingcarefulattentiontothevideo.ThemovieclipforQ1canbefound
here;forQ2andQ3,here;andforQ4,here.
5 Modelevaluation
InthissectionwediscusstheevaluationsofvariousclosedandopensourcevideoLLMsonourdataset,
somechallenges,andthemodelperformancetrends. Giventhatourdatasetisoftypemultiple-choice
questionanswers(MCQs),weevaluateagivenmodel’sperformanceonourbenchmarkquestions
bymeasuringitsabilitytoaccuratelyselecttherightanswerfromasetofmultiple-choiceoptions,
containingonlyonecorrectanswerandfourdistractors. Onekeychallengeinthisprocessisreliably
parsingthemodel’sresponsetoextractitschosenanswer,andmappingittooneofthepredefined
answerchoices. Model’sresponsesmayvaryinformat,includingadditionalmarkers,ormayonly
containtheoptionletter,orhaveacombinationoftheoptionletteranditscorrespondingtext,etc.
Suchvariationsnecessitatearobustpost-processingsteptoaccuratelyextractandmatchthemodel’s
responsetothecorrectoption.
Table3:ModelEvaluations.WepresenttheaccuracyofvariousvideoLLMsontheCinePile’stestsplit.We
alsopresentHumanperformanceforcomparison.Wealsoablatetheaccuraciesacrossthequestioncategories
wediscussedearlier.TEMP-Temporal,CRD-CharacterandRelationshipDynamics,NPA-NarrativeandPlot
Analysis,STA-SettingandTechnicalAnalysis,TH-ThematicExploration.
Model Average CRD NPA STA TEMP TH
Human 73.21 82.92 75.00 73.00 75.52 64.93
Human(authors) 86.00 92.00 87.5 71.20 100 75.00
GPT-4o[20] 59.65 66.54 77.22 52.76 42.39 62.33
GPT-4Vision[1] 58.04 65.37 80.97 47.42 42.01 70.37
Gemini1.5Pro[23] 59.08 63.44 63.88 59.52 37.50 68.42
GeminiProVision[29] 50.10 55.52 66.11 45.02 34.05 53.24
Claude3(Opus)[3] 44.72 49.64 61.11 38.86 32.60 44.87
mPLUG-Owl[40] 10.72 10.95 10.98 10.66 10.35 9.12
Video-ChatGPT[17] 15.44 17.31 15.05 15.79 7.14 23.38
MovieChat[26] 4.61 4.95 4.29 5.23 2.48 4.21
Ourevaluationmethodincorporatesatwo-stageprocesstoaddressthesevariations. Inthefirststage,
weemployanormalizationfunctiontoparsethemodel’sresponse,extractingtheoptionletter(A-E)
andtheaccompanyingoptiontextifpresent. Thisnormalizationhandlesvariousresponseformats,
such as direct letter responses (e.g., “A”) or more verbose forms (e.g., “Answer: D, The Eiffel
Tower”),ensuringthatboththeoptionletterandtextareaccuratelyidentified,ifpresent. Following
normalization,theprocessentersitssecondstage,whichentailscomparingthenormalizedmodel
responsewiththecorrectanswerkey. Thiscomparisoninvolvescheckingforboththeoptionletter
andtextinthemodelresponse. Ifbothelementsarepresentandmatchtheanswerkey,ascoreof
oneisawarded. However,ifonlytheoptionletterortextappears,thecomparisonislimitedtothe
relevantpart,andthescoreisassignedaccordingly.
9Weevaluatevariouscommercialandopen-sourceLLMmodelsandwepresenttheirperformancein
Table3. Wealsopresenthumannumbers(authorandnon-author)forcomparison. Thisdistinction
is important because the authors carefully watched the video (go back and rewatch the video if
necessary)whileansweringthequestions. Thisremovesthecarelessnesserrorsfromthehumanstudy.
Onaverage,VLMmodelsbothcommercialandOSS,arebehindhumanperformanceonourdataset.
WhilecommercialVLMsperformreasonablywell,theOSSmodelsperformquitepoorlyshowing
thegapintheircapabilities. WepresentafewQA’swhichhumansgotwrongandGPT-4gotwrong
andtheplausiblereasonforerrorsinFig.7.
AmongthevariouscommercialVLMsanalyzed,GPT-4o,GPT-4VisionandGemini1.5Pro(Video)
emerge as top performers, each achieving around 60% average performance. 5 Gemini 1.5 Pro
particularlyoutperformstheGPT-4modelsinthe“SettingandTechnicalAnalysis”categorythat
is dominated by visually reliant questions focusing on the environmental and surroundings of a
moviescene,anditsimpactonthecharacters. Onthecontrary,wenotethatGPT-4modelsperforms
substantiallybetteronotherquestioncategoriessuchas“NarrativeandPlotAnalysis”thatrevolve
aroundthecorestorylines,andinteractionbetweenthekeycharacters. It’simportanttonotethat
Gemini1.5Proisdesignedtohandlelongmultimodalcontexts,andwhileGPT-4oisalsoanatively
multimodal model, GPT-4V only handles 10 frames per video in our evaluation setup. When
comparedunderthesameconditionsof10frames,GeminiProVision,anothermodelintheGemini
series,performs10%lowercomparedtoGPT-4V.Meanwhile,Claude3(Opus)ranksastheleast
effectivemodelcommercialVLM,trailingGeminiProVisionby≈5%. Additionally,asdiscussed
inSec.2.4,weprovidea“hardsplit”inthetestsetconsistingofparticularlychallengingquestions.
InFig.8,wecomparetheperformanceofvariousmodelsonboththeaverageandthehardsplits
ofourdataset. Wenotethatwhileeachmodelsuffersaperformancedeclineof15%-20%onthe
hardsplit;however,therelativerankingamongthemodelsremainsunchanged,withtheexceptionof
Gemini1.5ProoutperformingtheGPT-4models. Whileanoticeablegappersistsbetweenhuman
performanceandthatofstate-of-the-artVLMs, OSSmodelstrailsignificantlytoboth. Tobetter
understandthesetrends,weconductedaqualitativeanalysisoftherawresponsesgeneratedbythe
OSSmodels. Thisinvestigationaimedtodeterminewhetherthedeficienciesofthesemodelswereas
significantaspresumed,orifthereareotherfactorsthatmightbeinfluencingtheoverallscore.
WhyareOSSmodelssofarbehind? Weobservedthatakeyreasonforthesubparperformance
of OSS models is their inability to adhere to the given instructions strictly. For instance, in our
evaluation prompts, we explicitly instruct the models to only output the option marker in their
responses. However,theOSSmodelsfrequentlydeviatedfromthisinstruction,generatingexcessive
andsometimesirrelevanttexte.g.,includingunnecessarypersonalopinions,rephrasingtheanswer
options,mentioningchoicesnotlistedintheprovidedoptions,andproducingcompletelyunrelated
textsuchasrepeatinglinesfromsubtitlesorattemptingtocompletescenesfrommovies. Wepresent
a few examples inTab.4. Wesolve some ofthese issues byimplementing a substringmatching
modulewithinourevaluationpipeline,whichsearchesforthecorrectanswerkeywithinthemodel’s
response,onlyconsideringthoseresponsesthatmentionasinglechoice.However,thismodificationis
unabletoreliablyevaluatescenarioswherethemodeloutputsarephrasedversionofthetext. Hence,
we also computed traditional video-caption evaluation metrics like BertScore [45], CIDEr [31],
ROUGE-L[15],andSPICE[2]foropen-sourcemodels. WepresenttheseresultsintheAppendix,
andfindthattherelativerankingbetweenthemodelsremainsunchanged. WehopetrainingtheOSS
modelsonourtrainingdatacanhelpbridgethegapintheperformancebetweenOSSmodelsand
commercialonesinthefuture.
6 RelatedWork
6.1 Videounderstandingandquestionanswering.
LVU [36], despite being one of the early datasets proposed for long video understanding, barely
addressestheproblemofvideounderstandingasthemaintasksaddressedinthisdatasetareyear,
genreclassificationorpredictingthelikeratioforthevideo. Asingleframemightsufficetoanswer
5Modelversionsanddates:GPT-4o[gpt-4o-2024-05-13]onMay14th,2024;GPT-4Vision[gpt-4-turbo],
Gemini1.5Pro[gemini-1.5-pro-latest],GeminiProVision[gemini-pro-vision],Claude3(Opus)[claude-3-
opus-20240229]fromApril29thtoMay10th,2024.
1060 59.65 58.04 59.08 Average
50.1 Hard-Split
45.45 44.72
40 38.63 37.69
30.96
27.32
20 15.44
9.82
0
GPT-4o GPT-4Vision Gemini1.5Pro GeminiProVision Claude3(Opus) Video-ChatGPT
Models
Figure8:Models’performanceonCinePiletestsplit,allquestionsvshardquestions.
Table4:AnalyzingrawresponsesgeneratedbyOSSmodels,scoresassignedbyourevaluationpipeline,and
correspondingfailuremodesforthemovieclip:Area51(2015)-SneakingOntotheBaseScene(4/10)
Question Answer Model Assigned Failure
Key Response Score Mode
Howdoestheemotionaltonetransi- Fromanx- “5, 4, 3, 2, 0 The model doesn’t
tionduringthescene? ietytoex- 1.\n All right, chooseanyoftheop-
-A)Fromdespairtohope citement I think I got tions, rather just re-
-B)Fromfeartoacceptance it, guys.\n You peatsallthesubtitles.
-C)Fromconfusiontounderstand- did it?\n .....
ing .....
-D)Fromtensiontopanic .....
-E)Fromanxietytoexcitement This is the
base.”
WhatactiondoesDarrentakeafter Suggests "\n ) Darren 0 While we are able
observingthebrokensensor? nextsteps calls for help.” to reliably extract
-A)Callsforhelp the choice present,
-B)Repairsit itdoesn’tmatchthe
-C)Panics answerkey.
-D)Ignoresit
-E)Suggestsnextsteps
WhereisJelenainrelationtothe Directly Jelena is 0 While we are able
helicopterattheendofthescene? under- standing in to reliably extract
-A)Inside neath front of the the choice present,
-B)Infront helicopter at itdoesn’tmatchthe
-C)Totheside the end of the answerkey.
-D)Directlyunderneath scene.
-E)Onit
HowdoDarrenandReedfirstinter- They In the scene, 1 Weareabletoextract
actwitheachotherinthescene? exchange Darren and Reed the response, and it
-A)Theyargue informa- are seen walking matches the answer
-B)Theyfight tion together, and key.
-C)Theyexchangeinformation they exchange
-D)Theygreeteachother information with
-E)Theyignoreeachother each other.
thequestionsandthesetaskscannotbeconsideredquiteas“understanding”tasks. MovieQA[28]is
oneofthefirstattemptstocreateatrulyunderstandingQAdataset,wherethequestionsarebasedon
entireplotthemoviebutnotlocalizedtoasinglescene. Oncloserexamination,veryfewquestions
are vision focused and most of them can be answered just based on dialogue. EgoSchema [19]
isoneoftherecentbenchmarks,focusedonvideounderstandingwhichrequiresprocessinglong
enoughsegmentsinthevideotobeabletoanswerthequestions. However,thevideosarebased
on egocentric videos and hence the questions mostly require perceptual knowledge, rather than
multimodalreasoning. Anotherrecentbenchmark,PerceptionTest[21],focusesoncoreperception
skills,suchasmemoryandabstraction,acrossvariousreasoningabilities(e.g.,descriptive,predictive,
etc)forshort-formvideos. MAD[25]datasetcontainsdialogueandvisualdescriptionsforfull-length
moviesandistypicallyusedinscenecaptioningtaskratherthanunderstanding. Anotherissueisthis
datasetdoesnotproviderawvisualdata,theyshareonly[CLS]tokenembeddings,whichmakes
ithardtouse. TVQA[11]isQAdatasetbasedonshort1-minclipsfromfamousTVshows. The
annotatorsareinstructedtoaskWhat/How/Whysortofquestionscombiningtwoormoreevents
inthevideo. MoVQA[44]manuallycuratesquestionsacrosslevelsmultiplelevels—singlescene,
11
ycaruccAmultiple scenes, full movie— by guiding annotators to develop queries in predefined categories
likeInformationProcessing,TemporalPerception,etc. TGIF-QA[10]isheuristicbasedquestions,
likehowmanytimescatnoddeditshead,orwhathappenedaftereventA?Itispartiallymanually
annotatedwithsomeautomationforcountingtypesofquestions. Next-QA[37]comprisesmanually
curatedcausal,temporal,anddescriptivequestionsaboutdailylifevideos. Longvideounderstanding
datasets,suchasEpicKitchens[6],tendtoconcentrateheavilyontasksrelatedtothememoryof
visual representations, rather than on reasoning skills. While these benchmarks are valuable for
gaugingtheextentofvisualrepresentationcapturedbyamodel,theyfallshortinprovidinginsights
intovideounderstanding. Thesedatasetsmainlytestamodel’sabilitytorecallandrecognizevisual
elementsbutdonotadequatelyassessitscapabilitytoreasonandinterpretthecontextandnarrative
ofvideos.
CinePilediffersfromalltheabovedatasetsthathavemuchlongervideosandweaskmanyquestions
pervideotocapturetheperceptual, temporal, andreasoningaspectsofthevideo. Anditistruly
multimodal where the person has to watch the video as well as audio/dialogue to answer many
questions. Unlikethepreviousdatasetswithfixedtemplates,weautomatedthisprocessonpreviously
human-generatedquestions,thisletuscapturemanymorequestioncategoriescomparedtoprevious
works. Lastly,ourapproachtodatasetgenerationisscalableandhenceitiseasyforanyonetoextend
ourdataset.
6.2 Syntheticdatawithhumanintheloop.
Trainingmodelsonsyntheticdataisapopularparadigminrecenttimes.Wehaveseenmanyadvances
ingenerationaswellasusageonsyntheticdatainrecenttimes, bothinvision[5,9,30,35]and
language [14, 18, 27, 34, 42]. For instance, Self-Instruct [33] proposes a pipeline to create an
instructiondatasetbasedonafewinstructionexamplesandcategoriesdefinedbyhumans. Wemainly
derivedinspirationandthefactthatmodernLLMsarequitegoodatunderstandinglongtextand
creatingquestion-answerpairs. UltraChat[8]isanothersyntheticlanguagedatasetwhichiscreated
byusingseparateLLMstoiterativelygenerateopeningdialoguelines,simulateuserqueries,and
provideresponses. Thisallowsconstructinglarge-scalemulti-turndialoguedatawithoutdirectly
usingexistinginternetdataasprompts. Additionally,Evol-Instruct[38],automaticallygenerates
adiversecorpusofopen-domaininstructionsofvaryingcomplexitiesbypromptinganLLMand
applyingiterativeevolutionoperationslikein-depthevolving(addingconstraints,deepening,etc.)
andin-breadthevolving(generatingnewinstructions). Toourknowledge,weareamongthefirstto
applyautomatedtemplategenerationandquestionsynthesistechniquestovisionandvideomodalities
usingLLMs.
7 DiscussionandConclusion
Inthispaper,weintroducedCinePile,auniquelongvideounderstandingdatasetandbenchmark,
featuring ∼ 300k questions in the training set and ∼ 5000 in the test split. We detailed a novel
methodforcuratingandfilteringthisdataset,whichisbothscalableandcost-effective. Additionally,
webenchmarkedvariousrecentcommercialvideo-centricLLMsandconductedahumanstudyto
gauge the achievable performance on this dataset. To our knowledge, CinePile is the only large-
scaledatasetthatfocusesonmulti-modalunderstanding,asopposedtothepurelyvisualreasoning
addressed inpreviousdatasets. Thedataset ispublicly availableat https://hf.co/datasets/
tomg-group-umd/cinepile.Additionally,weplantosetupaleaderboardforthetestset,providing
aplatformfornewvideoLLMstoassessandbenchmarktheirperformanceonCinePile.
Despiteitsstrengths,therearestillafewareasforimprovementinourdataset,suchastheincorpo-
rationofcharactergroundingintime. Whilewebelieveourdataset’squalityiscomparabletoor
evenbetterthanthatofaMechanicalTurkannotator,weacknowledgethatamotivatedhuman,given
sufficienttime,cancreatemorechallengingquestionsthanthosecurrentlygeneratedbyanLLM.
OurgoalistonarrowthisgapinfutureiterationsofCinePile.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
12technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic
propositional image caption evaluation. In Computer Vision–ECCV 2016: 14th European
Conference,Amsterdam,TheNetherlands,October11-14,2016,Proceedings,PartV14,pages
382–398.Springer,2016.
[3] Anthropic. Theclaude3modelfamily: Opus,sonnet,haiku. URLhttps://www.anthropic.
com/claude.
[4] MaxBain,JaesungHuh,TengdaHan,andAndrewZisserman. Whisperx:Time-accuratespeech
transcriptionoflong-formaudio. INTERSPEECH2023,2023.
[5] FlorianBordes,ShashankShekhar,MarkIbrahim,DianeBouchacourt,PascalVincent,andAri
Morcos. Pug: Photorealisticandsemanticallycontrollablesyntheticdataforrepresentation
learning. AdvancesinNeuralInformationProcessingSystems,36,2024.
[6] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari,
EvangelosKazakos,DavideMoltisanti,JonathanMunro,TobyPerrett,WillPrice,andMichael
Wray. Scaling egocentric vision: The epic-kitchens dataset. In European Conference on
ComputerVision(ECCV),2018.
[7] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,
2018.
[8] NingDing,YulinChen,BokaiXu,YujiaQin,ZhiZheng,ShengdingHu,ZhiyuanLiu,Maosong
Sun,andBowenZhou. Enhancingchatlanguagemodelsbyscalinghigh-qualityinstructional
conversations. arXivpreprintarXiv:2305.14233,2023.
[9] ReyhaneAskariHemmat,MohammadPezeshki,FlorianBordes,MichalDrozdzal,andAdriana
Romero-Soriano. Feedback-guideddatasynthesisforimbalancedclassification. arXivpreprint
arXiv:2310.00158,2023.
[10] YunseokJang,YaleSong,YoungjaeYu,YoungjinKim,andGunheeKim. Tgif-qa: Toward
spatio-temporalreasoninginvisualquestionanswering. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pages2758–2766,2017.
[11] JieLei,LichengYu,MohitBansal,andTamaraLBerg. Tvqa: Localized,compositionalvideo
questionanswering. arXivpreprintarXiv:1809.01696,2018.
[12] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter: A
multi-modalmodelwithin-contextinstructiontuning. arXivpreprintarXiv:2305.03726,2023.
[13] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hi-
erarchical encoder for video+ language omni-representation pre-training. arXiv preprint
arXiv:2005.00200,2020.
[14] YuanzhiLi,SébastienBubeck,RonenEldan,AllieDelGiorno,SuriyaGunasekar,andYinTat
Lee. Textbooksareallyouneedii: phi-1.5technicalreport. arXivpreprintarXiv:2309.05463,
2023.
[15] Chin-YewLin.Rouge:Apackageforautomaticevaluationofsummaries.InTextsummarization
branchesout,pages74–81,2004.
[16] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning. arXivpreprintarXiv:2310.03744,2023.
[17] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:
Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. arXiv:2306.05424,
2023.
[18] Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly.
Rephrasingtheweb: Arecipeforcomputeanddata-efficientlanguagemodeling. arXivpreprint
arXiv:2401.16380,2024.
13[19] KarttikeyaMangalam,RaiymbekAkshulakov,andJitendraMalik. Egoschema: Adiagnostic
benchmarkforverylong-formvideolanguageunderstanding. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[20] OpenAI. Gpt-4orelease,2024. URLhttps://openai.com/index/hello-gpt-4o/.
[21] VioricaPatraucean,LucasSmaira,AnkushGupta,AdriaRecasens,LarisaMarkeeva,Dylan
Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception
test: Adiagnosticbenchmarkformultimodalvideomodels. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[22] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya
Sutskever. Robust speech recognition via large-scale weak supervision. In International
ConferenceonMachineLearning,pages28492–28518.PMLR,2023.
[23] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-
baptisteAlayrac, RaduSoricut, AngelikiLazaridou, OrhanFirat, JulianSchrittwieser, etal.
Gemini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXiv
preprintarXiv:2403.05530,2024.
[24] NilsReimersandIrynaGurevych. Sentence-bert: Sentenceembeddingsusingsiamesebert-
networks. arXivpreprintarXiv:1908.10084,2019.
[25] MattiaSoldan,AlejandroPardo,JuanLeónAlcázar,FabianCaba,ChenZhao,SilvioGiancola,
andBernardGhanem. Mad: Ascalabledatasetforlanguagegroundinginvideosfrommovie
audio descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pages5026–5035,2022.
[26] EnxinSong,WenhaoChai,GuanhongWang,YuchengZhang,HaoyangZhou,FeiyangWu,
XunGuo,TianYe,YanLu,Jenq-NengHwang,etal. Moviechat: Fromdensetokentosparse
memoryforlongvideounderstanding. arXivpreprintarXiv:2307.16449,2023.
[27] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,Percy
Liang,andTatsunoriB.Hashimoto. Stanfordalpaca: Aninstruction-followingllamamodel.
https://github.com/tatsu-lab/stanford_alpaca,2023.
[28] MakarandTapaswi,YukunZhu,RainerStiefelhagen,AntonioTorralba,RaquelUrtasun,and
Sanja Fidler. Movieqa: Understanding stories in movies through question-answering. In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages4631–
4640,2016.
[29] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[30] Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola.
Learningvisionfrommodelsrivalslearningvisionfromdata. arXivpreprintarXiv:2312.17742,
2023.
[31] RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh. Cider: Consensus-basedimage
descriptionevaluation. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages4566–4575,2015.
[32] YiWang,YinanHe,YizhuoLi,KunchangLi,JiashuoYu,XinMa,XinyuanChen,YaohuiWang,
PingLuo,ZiweiLiu,YaliWang,LiminWang,andYuQiao. Internvid: Alarge-scalevideo-text
datasetformultimodalunderstandingandgeneration. arXivpreprintarXiv:2307.06942,2023.
[33] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahASmith,DanielKhashabi,
andHannanehHajishirzi. Self-instruct: Aligninglanguagemodelwithselfgeneratedinstruc-
tions. arXivpreprintarXiv:2212.10560,2022.
[34] JerryWei,DaHuang,YifengLu,DennyZhou,andQuocVLe. Simplesyntheticdatareduces
sycophancyinlargelanguagemodels. arXivpreprintarXiv:2308.03958,2023.
14[35] ErrollWood,TadasBaltrušaitis,CharlieHewitt,SebastianDziadzio,ThomasJCashman,and
JamieShotton. Fakeittillyoumakeit: faceanalysisinthewildusingsyntheticdataalone. In
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages3681–3691,
2021.
[36] Chao-YuanWuandPhilippKrähenbühl. TowardsLong-FormVideoUnderstanding. InCVPR,
2021.
[37] JunbinXiao,XindiShang,AngelaYao,andTat-SengChua. Next-qa: Nextphaseofquestion-
answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pages9777–9786,2021.
[38] CanXu,QingfengSun,KaiZheng,XiuboGeng,PuZhao,JiazhanFeng,ChongyangTao,and
DaxinJiang. Wizardlm: Empoweringlargelanguagemodelstofollowcomplexinstructions.
arXivpreprintarXiv:2304.12244,2023.
[39] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,XiangnanHe,andYuetingZhuang.
Video question answering via gradually refined attention over appearance and motion. In
Proceedingsofthe25thACMinternationalconferenceonMultimedia,pages1645–1653,2017.
[40] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
AnwenHu, PengchengShi, YayaShi, ChaoyaJiang, ChenliangLi, YuanhongXu, Hehong
Chen,JunfengTian,QianQi,JiZhang,andFeiHuang. mplug-owl: Modularizationempowers
largelanguagemodelswithmultimodality,2023.
[41] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large
languagemodelswithmultimodality. arXivpreprintarXiv:2304.14178,2023.
[42] WeizheYuan,RichardYuanzhePang,KyunghyunCho,SainbayarSukhbaatar,JingXu,and
JasonWeston. Self-rewardinglanguagemodels. arXivpreprintarXiv:2401.10020,2024.
[43] CeZhang, TaixiLu, MdMohaiminulIslam, ZiyangWang, ShoubinYu, MohitBansal, and
GedasBertasius. Asimplellmframeworkforlong-rangevideoquestion-answering. arXiv
preprintarXiv:2312.17235,2023.
[44] Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang,
and Yu Qiao. Movqa: A benchmark of versatile question-answering for long-form movie
understanding. arXivpreprintarXiv:2312.04817,2023.
[45] Tianyi Zhang, VarshaKishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore:
Evaluatingtextgenerationwithbert. arXivpreprintarXiv:1904.09675,2019.
[46] DeyaoZhu, JunChen, XiaoqianShen, XiangLi, andMohamedElhoseiny. Minigpt-4: En-
hancingvision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprint
arXiv:2304.10592,2023.
15CinePile: ALongVideoQuestionAnsweringDatasetandBenchmark
SupplementaryMaterial
A Additionalmovieclip&questionsexamples
WepresentafewexamplesfromourdatasetinFigs.11a,11b,12a,12b,13a,13b,14aand14b.
B AdditionalQAGenerationDetails
Inadditiontothehand-craftedperceptualtemplates,wealsocreatelong-formquestionandanswers
basedonascene’svisualsummary. Toachievethis,wefirstgenerateavisualsummaryofavideo
clip. Then,wepromptthemodeltocreatequestion-answerssolelybasedonthatsummary.
WecreateapurevisualsummaryofthescenebyusingavisionLLM,similartosomeoftherecent
works[32,43]. First,weuseashotdetectionalgorithmtopicktheimportantframes6,thenwe
annotateeachoftheseframeswithGeminivisionAPI(gemini-pro-vision). Weablatedmany
SOTAopen-sourcevisionLLMssuchasLlava1.5-13B[16],OtterHD[12],mPlug-Owl[41]and
MinGPT-4[46],alongwithGeminiandGPT-4V(GPT-4-1106-vision-preview). WhileGPT-4V
hashighfidelityintermsofimagecaptioning,itisquiteexpensive. Mostoftheopen-sourceLLM
captionsareriddledwithhallucinations. Afterqualitativelyevaluatingacrossmanyscenes,wefound
thatGemini’sframedescriptionsarereliableandtheydonotsuffertoomuchfromhallucination.
Oncewehaveframe-leveldescriptions,wethenpasstheconcatenatedtexttoGeminitextmodel
gemini-proandpromptittoproduceashortdescriptivesummaryofthewholescene. Eventhough
Gemini’sscenevisualsummaryislesslikelytohavehallucinatedelements,wehoweverspotteda
fewhallucinatedsentences. HencealltheMCQsgeneratedusingthissummaryareaddedonlytothe
trainingsplitbutnottotheevalsplit.
C QuestionTemplateCategoryDetails
CharacterandRelationshipDynamics: Thiscategorywouldincludetemplatesthatfocusonthe
actions,motivations,andinteractionsofcharacterswithinthemovie. Itwouldalsocoveraspects
suchascharacterroles,reactions,decisions,andrelationships.
NarrativeandPlotAnalysis: Thiscategorywouldencompasstemplatesthatdelveintothe
storyline,plottwists,eventsequences,andtheoverallnarrativestructureofthemovie. Itwouldalso
includetemplatesthatexplorethecause-and-effectdynamicswithintheplot.
ThematicExploration: Thiscategorywouldincludetemplatesthatfocusontheunderlyingthemes,
symbols,motifs,andsubtextwithinthemovie. Itwouldalsocoveraspectssuchasmoraldilemmas,
emotionalresponses,andtheimpactofdiscoveries.
SettingandTechnicalAnalysis: Thiscategorywouldencompasstemplatesthatfocusonthesetting,
environment,andtechnicalaspectsofthemovie. Itwouldincludetemplatesthatanalyzethelocation
ofcharactersandobjects,theuseofprops,theimpactofinteractionsontheenvironment,andthe
descriptionandfunctionofobjects.
Temporal: Thiscategorypertainstoquestionsandanswersthatassessamodel’scomprehensionofa
movieclip’stemporalaspects,suchastheaccuratecountingofspecificactions,theunderstandingof
thesequenceofevents,etc.
D QAGenerationbyDifferentModels
Inthissection,wepresentexamplequestion-answer(QA)pairsgeneratedbyGPT-4andGemini
acrossvariousquestioncategoriesinTable5andTable6. Asalludedtointhemainpaper,wenote
thatGPT-4consistentlyproduceshigh-qualityquestionsinallcategories. Incontrast,Geminiworks
wellonlyforafewselectcategories,namely,CharacterRelationshipsandInterpersonalDynamics
6https://www.scenedetect.com/
16Table5:Comparingquestion-answerpairsgeneratedbyGPT-4withthosegeneratedbyGemini,forthemovie
clip:TheHeartbreakKid(3/9)MovieCLIP-TakingthePlunge(2007)HD.TEMPreferstoTemporal.Please
refertoTable1forotheracronyms.
Category GPT-4GeneratedQA GeminiGeneratedQA
CRD Question:WhatisthesignificanteventthatEddie Question:WhatisEddiedoingatthebeginningof
andLilaarecelebrating? thescene?
-A)Theirwedding✓ -A)DancingwithLila
-B)Theirfirstdateanniversary -B)Givingaspeech
-C)Lila’sbirthday -C)Cuttingtheweddingcake
-D)Theirengagement -D)KissingLila✓
-E)Eddie’spromotionatwork -E)Talkingtohisfriends
NPA Question:Whatincidentleadstothemaincharac- Question:HowdoesEddieresolvehisconflictwith
ter’schangeinattitudetowardsmarriage? hisfriend?
-A)Hisfriend’sadvice✓ -A)Heapologizesforhispastbehavior.
-B)Hismother’sarrival -B)Heconfrontshisfriendabouttheirdifferences.
-C)Hisbride’sbeauty -C)Heignoreshisfriendandmoveson.
-D)Hisfriend’sgift -D)Heseeksrevengeonhisfriend.
-E)Hisbride’sdress -E)Hereconcileswithhisfriend.✓
TEMP Question:Howlongisthecoupleplanningtotake Question:Whatoccursimmediatelyafterthewed-
offfortheirroadtrip? dingceremony?
-A)Oneweek -A)Thecouplekisses.
-B)Fourweeks -B)Theguestscongratulatethecouple.
-C)Fiveweeks -C)Thebride’smotherarrives.✓
-D)Twoweeks -D)Thecoupleleavesfortheirhoneymoon.
-E)Threeweeks✓ -E)Thegroomgivesaspeech.
STA Question:WhereisthegiftEddie’sfriendgives Question:WhatistheprimarycolorofLila’sdress
himsupposedtoendup? inthescene?
-A)WithUncleTito✓ -A)Red
-B)WithLila -B)Blue
-C)WithEddie -C)Yellow
-D)Withtheweddingguests -D)Green
-E)WithEddie’smom -E)White✓
TH Question:Howdoestheemotionaltoneshiftfrom Question: Whatdoesthechaoticatmosphereat
thebeginningtotheendofthescene? thereceptionsymbolizeinrelationtothefilm’s
-A)Fromexcitementtodisappointment themes?
-B)Fromjoytosorrow -A)Theunpredictabilityoflife✓
-C)Fromanticipationtoregret -B)Thechallengesofmarriage
-D)Fromhappinesstosurprise✓ -C)Theimportanceoffamily
-E)Fromnervousnesstorelief -D)Thepoweroflove
-E)Thefragilityofrelationships
Characterand Theme
Relationship Exploration
Dynamics 42.6 4.0
Temporal
13.9
10.8 28.6
Narrativeand Settingand
PlotAnalysis Technical
Analysis
Figure9:Questioncategorycompositioninthetrainsplitofthedataset.
(CDR),andSettingandTechnicalAnalysis(STA).ThegapinqualityoftheQAgeneratedstemsnot
onlyfromtheimplicitlybetteranddiverseconceptscapturedbyGPT-4,butalsofromthe
hallucinationtendenciesofGemini. Forinstance,inTable-5,Geminimistakesthedialogue–
“Thankyoufortalkingsomesenseintome,man”,betweenEddieandhisfriendasasuggestionfor
conflictresolution,andformsanarrativequestionbasedonit–“HowdoesEddieresolvehisconflict
withhisfriend?”. Similarly,inTable6,Geminimisremembersthetemporalsequenceandselectsa
wrongoptionastheanswerchoiceforthetemporalcategory.
E Traindatastatistics
WepresentthequestioncategorystatisticsoftrainsplitinFig.9.
17Table6:Comparingquestion-answerpairsgeneratedbyGPT-4withthosegeneratedbyGemini,forthemovie
clip: Ghostbusters: Afterlife(2021)-MuncherAttackScene(3/7)|Movieclips. TEMPreferstoTemporal.
PleaserefertoTable1forotheracronyms.
Category GPT-4GeneratedQA GeminiGeneratedQA
CRD Question:HowdoesPhoebe’sinteractionwiththe Question: How does the interaction between
ghostchangethroughoutthescene? Phoebeandtheghostchangethroughoutthescene?
-A)Shegoesfromhidingtoattacking.✓ -A)Phoebebecomesmoreaggressive✓
-B)Shegoesfromattackingtohiding. -B)Theghostbecomesmorepassive
-C)Shegoesfromrunningawaytoattacking. -C)Phoebebecomesmorefearful
-D)Shegoesfromcommunicatingtoattacking. -D)Theghostbecomesmoreaggressive
-E)Shegoesfromhidingtorunningaway. -E)Theyremainneutraltowardseachother
NPA Question:WhatmajoreventpromptsPhoebeto Question: WhatobservationpromptsPhoebeto
takedrasticactionagainsttheghost? takeaction?
-A)Theghostscuttlesoffandpipesburstinto -A)Theghost’sfearoftheAztecdeathwhistle
flame. -B)Theghost’svulnerabilitytoprotonblasts
-B)Theghosthidesbehindmachinery. -C)Theghost’ssearchforsomethingontheground.
-C)Theghoststartseatingapieceofpipe. ✓
-D)Theghostbelchesmetalfragmentsthatspark -D)Theghost’sreactiontoPodcast’scameragog-
andricochetaroundthem.✓ gles
-E)Theghoststartssearchingtheground. -E)Theghost’sbelchingofmetalfragments
TEMP Question: Whathappensimmediatelyafterthe Question:BetweenwhichtwoeventsdoesPhoebe
ghostbelchesmetalfragments? duckdown?
-A)Phoebeducksdown. -A)TheghostsearchesthegroundandPhoebe
-B)Theghostscuttlesoffandpipesburstinto pokesherheadup.
flame. -B)TheghostchompsonapipeandPhoebepokes
-C)PodcastblowstheAztecdeathwhistle. herheadup.
-D)Phoebepowersupandfiresasteadystreamof - C) Podcast blows the whistle and the ghost
protons.✓ belchesmetalfragments.
-E)Phoebepokesherheadup. -D)Theghostscuttlesoffandpipesburstinto
flame.✓
-E)Phoebefiresprotonsandtheghostpokesits
headout.
STA Question:WheredoPodcastandPhoebehidedur- Question:Whatistheprimarymaterialoftheob-
ingtheghostencounter? jectthattheghostischewingon?
-A)Insideacar -A)Wood
-B)Inabuilding -B)Metal✓
-C)Behindatree -C)Plastic
-D)Underatable -D)Rubber
-E)Behindmachinery✓ -E)Fabric
TH Howdoestheemotionaltoneshiftthroughoutthis Question:Howdoestheemotionaltoneshiftfrom
scene? thecharacters’initialfeartotheirdetermination?
-A)Fromcalmtochaotic -A)Thepodcast’scalmnessinspiresPhoebetobe-
-B)Fromfeartocourage✓ comemoreassertive.
-C)Fromconfusiontounderstanding -B)Theghost’ssearchforsomethingontheground
-D)Fromexcitementtodisappointment createsasenseofurgency.
-E)Fromsadnesstojoy -C)Thecharacters’realizationthattheyhavea
planinstillsconfidence.✓
-D)Theghost’sbelchingofmetalfragmentsinten-
sifiesthefearandchaos.
-E)Thecharacters’decisiontousethetrapmarks
ashiftfromfeartodetermination.
F EvaluationonCaptioningMetrics
Table7:Performanceofvariousopensourcevideo-LLMsonCinePile’stestsplit,asevaluatedusingvarious
videocaptioningmetrics–BERTSCoRE[7],CIDEr[31],ROUGE-L[15].
Model BERTScore↑ CIDEr↑ ROUGE-L↑
mPLUG-Owl[40] 0.46 1.36 0.31
Video-ChatGPT[17] 0.48 1.14 0.32
MovieChat[26] 0.40 0.66 0.18
AsdiscussedinSec.5ofthemainpaper,weevaluateamodel’sperformanceonCinePile’stest-split
bycomputingitsaccuracyinchoosingthecorrectanswerfromasetofmultiple-choiceoptions. This
involvesextractingthechosenanswerfromthemodel’srawresponseandmappingittooneofthe
predefinedansweroptions. Whileweperformextensiveprompttuningtoensurethemodeloutputs
onlytheoption-letterinitsresponseandrigorouslypost-processresponsestoseparatelyextractthe
chosenoption-letterandthecorrespondingoption-textgenerated(ifgenerated),thereremainsa
possibilityoferrors. Themodelmaynotalwaysfollowtheseinstructionsperfectlyandcould
produceverboseresponseswithunnecessarytextsnippets,suchas"Inmyopinion,""Thecorrect
answeris,"or"... isthecorrectanswer."
18Therefore,inthissection,wecomputetraditionalvideo-captionevaluationmetricsthatemphasize
thesemanticsimilaritybetweentheanswerkeytextandtherawmodelresponse,insteadofexact
stringmatching. Wefocusourevaluationanddiscussiononopen-sourcemodelshere,aswe
qualitativelynotedthatproprietarymodels,suchasGPT-4V,Gemini-Pro,andClaude,strictlyadhere
tothepromptinstructions,producingonlytheoptionletterintheirresponse. Specifically,we
calculatethefollowingvideo-captioningmetrics–BERTScore[45],CIDEr[31],and
ROUGE-L[15]. BERTScorecalculatesthecontextualsimilaritybetweentheanswerkeyandmodel
responseintheembeddingspaceofapretrainedtransformermodellikeBERT-Base. Calculatingthe
similaritybetweenthelatentrepresentations,insteadofdirectstringmatching,providesrobustnessto
paraphrasingdifferencesintheanswerkeyandmodelresponse. Incontrast,CIDErevaluatesthe
degreetowhichthemodelresponsealignswiththeconsensusofasetofreferenceanswerkeys. In
oursetup,eachquestionisassociatedwithonlyonereferenceanswer. Thealignmenthereis
computedbymeasuringthesimilaritybetweenthenon-trivialn-gramspresentinthemodelresponse
andtheanswerkey. Finally,ROUGE-Lcomputesthesimilaritybetweentheanswerkeyandmodel
responsebasedontheirlongestcommonsubsequence.
Weevaluatethreeopensourcemodels,i.e. mPLUG-Owl,Video-ChatGPT,andMovieChatusingthe
aforementionedmetricsandreporttheresultsinTable7. Inlinewiththeaccuracytrendinthemain
paper,weobservethatwhilemPLUG-OwlandVideo-ChatGPTdon’thaveasubstantialdifferencein
theirperformance,theybothsignificantlyoutperformMovieChat. Thesefindingsfurthersupportthe
reliabilityofournormalizationandpost-processingstepsduringaccuracycomputation,and
underscoretheneedforimprovingopen-sourcevideo-LLMstoclosethegapwithproprietarymodels.
G ExampleDegenerateQuestions
Table8: Exampledegeneratequestions. ExamplesofdegeneratequestionsfilteredfromCinePile. These
questionscanbecategorizedasdegenerateforvariousreasons,including:beinganswerablethroughcommon
sense(rowsonetothree)andthemodelspossiblymemorizingthemoviescripts(rowsfourandfive)
MovieClip DegenerateQuestions
Question: Wheredoestheconversationbetweenthecharacterstake
place?
Scream (1996) - Wrong -A)Inarestaurant
Answer Scene (2/12) | -B)Inacar
Movieclips -C)Inaclassroom
-D)Ataparty
-E)Overthephone✓
Question:Whatthematicelementisparalleledinthecharacter’sdia-
logueabouthispastandhisdestiny?
TheGodfather: Part3(8/10) -A)Thethemeofrevenge
MovieCLIP-MichaelApolo- -B)Thethemeoffateandfreewill✓
gizestoKay(1990)HD -C)Thethemeofbetrayal
-D)Thethemeoflostinnocence
-E)Thethemeofloveandsacrifice
Question:WhathappensrightbeforeGrugslipsonabanana?
-A)SandyhelpsGuyhandbananasouttoallthemonkeys.
TheCroods(2013)-TryThis
-B)Thesaber-toothedcatroarsatthemfromthebottomofagorge.
On For Size Scene (6/10) | -C)Grugthrowsabananadownangrily.✓
Movieclips
-D)Grugputsuphisdukesandsodoesthemonkey.
-E)GuygivesGrugabanana.
Question:WhateventpromptsKiraWatanabetocallMr.Pickles?
-A)Therobot’sdestructionofthevillage.
Rugrats in Paris (2000) -
-B)Therobot’spopularityamongthevillagers.
We’reGoingtoFrance!Scene -C)Themalfunctionofthegiantrobot.✓
(1/10)|Movieclips
-D)Thevillagers’protestagainsttherobot.
-E)Therobot’ssuccessfulperformance.
Question:WhathappensimmediatelyafterAnthonyandDignanfinish
eatingtheirsandwichesonthepatio?
Bottle Rocket (3/8) Movie -A)Anthonychewsanut.
CLIP-FutureManandStacy -B)Aguyinabrownshirtapproachesthem.✓
(1996)HD -C)StaceySinclairintroducesherself.
-D)Anthonytellshisstoryaboutthebeachhouse.
-E)Anthonygoestocleanthepool.
19AsdiscussedinSection2.4ofthemainpaper,mostquestion-answersgeneratedarewell-formedand
includechallengingdistractors. However,asmallminorityaredegenerateinthattheycanbe
answereddirectly,i.e.,withoutviewingthemovievideoclip. Toautomaticallyfilteroutsuch
questions,weformulateadegeneracycriterion. Ifaquestioncanbeansweredbyawidevarietyof
modelswithoutanycontext—thatis,allmodelsselectthecorrectanswermerelybyprocessingthe
questionandthefiveoptions—welabelitasadegeneratequestion. Inthissection,wepresentand
discusssomeofthesedegeneratequestionsinTable8. Wenotethataquestioncanbecategorizedas
degenerateduetomultiplepossiblereasons. Forinstance,considerthequestions,“Wheredoesthe
conversationbetweenthecharacterstakeplace?”,and“WhathappensrightbeforeGrugslipsona
banana?”. Theanswerkeyforthesecorrespondstothemostcommon-senseresponse,andthe
modelsareabletoreliablyidentifythecorrectchoices("Overthephone","Grugangrilythrowsa
bananadown")fromamongthedistractions. There’sanothertypeofquestionthatmodelsmight
answercorrectlyifthey’vememorizedthemoviescript. Forexample,thequestion,"Whatevent
promptsKiraWatanabetocallMr. Pickles?"fromthemovieRugratsinParis,isaccuratelyanswered.
Thislikelyhappensbecauseofthememorizationofthescriptandthedistinctcharacternames
mentionedinthequestion.
H HumanStudyDetails
(b) Sample Movie-Clip
(a) Instructions Page
Question-Answering Page
Figure10: (left)(a)InstructionsPage: Theinstructionspageatthebeginningofthesurvey,aspresented
to participants. The participants provide informed consent before viewing any video clip and answering
questions.(right)(b)SampleMovie-ClipQuestion-AnsweringPage:Anexampleofoneofthemovieclips
andcorrespondingquestion,aspresentedtotheparticipants.Theparticipantsarerequiredtowatchtheclipand
answerthequestionsbyselectingthecorrectanswerchoiceoutoffiveoptions.
Theauthorsconductedasmallhumanstudywith25graduatestudentvolunteerstoevaluatethe
qualityoftheCinePiledatasetquestions. Eachparticipantansweredtenrandomlysampled
multiple-choicequestionsabouttwovideoclips. Ourhumanstudysurveywasgrantedanexemption
byourinstitute’sInstitutionalReviewBoard(IRB),andallparticipantsgavetheirinformedconsent
beforeviewingthevideosandrespondingtothequestions. Forfullinstructionsandconsent
questionsgiventoparticipants,pleaserefertoFig.10-(a). Additionally,wedidnotcollectany
personallyidentifiableinformationfromtheparticipants. It’simportanttonotethatourdataset
consistsofEnglishmoviesproducedintheUnitedStates. Thesefilmsarelikelycertifiedbythe
MotionPictureAssociationofAmerica(MPAA),whichmeanstheyadheretostrictcontentstandards
20andclassificationguidelines. Asaresult,they’reexpectedtocontainminimaloffensivecontent. An
exampleofthequestion-answeringpagecanbefoundinFig.10-(b).
(a)
(b)
Figure11: Examplemovieclipandmultiple-choicequestionsfromCinePile. Thefirstandsecondrows
depictaselectionofimageframesextractedfrommovieclipsfrom(a)NowYouSeeMe2,and(b)CatchMeif
YouCan,accompaniedbytheircorrespondingsubtitles.Thenextrowshowcasesexamplequestionsalongwith
thequestiontemplateshownincoloredheaders. TEMPreferstoTemporal. PleaserefertoTable1forother
categoryacronyms.
21(a)
(b)
Figure12: Examplemovieclipandmultiple-choicequestionsfromCinePile. Thefirstandsecondrows
depictaselectionofimageframesextractedfrommovieclipsfrom(a)EscapeFromL.A.,and(b)Ghostbusters:
Afterlife,accompaniedbytheircorrespondingsubtitles.Thenextrowshowcasesexamplequestionsalongwith
thequestiontemplateshownincoloredheaders. TEMPreferstoTemporal. PleaserefertoTable1forother
acronyms.
22(a)
(b)
Figure13: Examplemovieclipandmultiple-choicequestionsfromCinePile. Thefirstandsecondrows
depictaselectionofimageframesextractedfrommovieclipsfrom(a)NeverBackDown,and(b)TheCroods,
accompanied by their corresponding subtitles. The next row showcases example questions along with the
question template shown in colored headers. TEMP refers to Temporal. Please refer to Table 1 for other
acronyms.
23(a)
(b)
Figure14: Examplemovieclipandmultiple-choicequestionsfromCinePile. Thefirstandsecondrows
depictaselectionofimageframesextractedfrommovieclipsfrom(a)Valentine’sDay,and(b)YouCanCount
onMe,accompaniedbytheircorrespondingsubtitles.Thenextrowshowcasesexamplequestionsalongwith
thequestiontemplateshownincoloredheaders. TEMPreferstoTemporal. PleaserefertoTable1forother
acronyms.
24