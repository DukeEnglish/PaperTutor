Efficient Vision-Language Pre-training by Cluster Masking
ZihaoWei* ZixuanPan* AndrewOwens
UniversityofMichigan
Abstract
Weproposeasimplestrategyformaskingimagepatches
during visual-language contrastive learning that improves
the quality of the learned representations and the training
speed.Duringeachiterationoftraining,werandomlymask
clusters of visually similar image patches, as measured by
theirrawpixelintensities. Thisprovidesanextralearning
signal,beyondthecontrastivetrainingitself,sinceitforces
amodeltopredictwordsformaskedvisualstructuressolely
from context. It also speeds up training by reducing the
amountofdatausedineachimage. Weevaluatetheeffec-
tivenessofourmodelbypre-trainingonanumberofbench-
marks,findingthatitoutperformsothermaskingstrategies,
suchasFLIP,onthequalityofthelearnedrepresentation.
1.Introduction
Imagescontainagreatdealofredundantinformation,mak- (a)Aredandgold (b)Aburgundylow- (c)Twosurfboars
paintedfire ridershow-quality, onabeach
ing it challenging to efficiently learn representations from
hydrantonthestreet. hotrodChevytruck. nearthewater.
them at scale. Recent work has addressed this problem
by masking image patches during vision-language con- Figure1.Clustermasking.Wemaskrandomclustersofvisually
trastive learning [15, 33, 36, 70]. One simple approach similar image patches when training contrastive vision-language
models (bottom). This masking strategy distinguishes our ap-
is to drop a large fraction of the patches at random, mak-
proachfrommethodsthatindependentlymaskimagepatchesfor
ing training more efficient by reducing the computational
efficiency[36](middle),whileprovidingasimilarimprovementin
costandmemoryusageineachtrainingiteration[36]. An
trainingspeed. Itprovidesanextralearningsignal,sinceitforces
alternative strategy is to mask sets of semantically related
amodeltopredictwordsformissingscenestructuressolelyfrom
patches[15,33,70], suchasthosethatbelongtothesame
context.
object. Thisforcesthelearnedmodeltopredictwordsthat
describesmissingscenestructuresfromcontext,improving especially when clusters are sampled randomly (Fig. 1).
thelearnedrepresentation. However,thisapproachrequires Ourapproachthusleadstomoreefficienttraining,likeap-
a separate mechanism to group together semantically re- proaches that independently drop patches [36], while im-
lated patches, which adds considerable complexity to the provingthelearnedrepresentationviacontextprediction.
learningprocedureandiscomputationallyexpensive.
We take inspiration from masked region classification,
We propose a simple masking strategy for multimodal
a pre-training task widely used in vision-language models
contrastivelearningthatavoidstheseshortcomings. During
[9,56,57]. Thesemodelsextractobjectfeatures,thenpre-
training,wemaskrandomclustersofpatches(Fig.1). For
dictobjectlabelsfortherandomlymaskedoutregions. Our
thisclustering,weusethepatches’rawRGBvaluesasthe
masking approach provides a similar training signal, since
featurerepresentation. Ourapproachtakesadvantageofthe
meaningfullabelsareincludedintheimagecaption.Forex-
factthatsimplemeasuresofvisualsimilaritycanoftencap-
ample,asshowninFigure1(a),themodelistaskedwithas-
turecoherentvisualstructures,suchasobjectparts[18,53],
sociatingthewords“firehydrant”withanimageevenwith
*Equalcontribution.Authororderwasdeterminedbyacoinflip. thehydrantitselfismostlymaskedout.
4202
yaM
41
]VC.sc[
1v51880.5042:viXra
egamIlanigirO
ksaMmodnaR
gniksaMretsulC
noitpaCWetrainourmodelonConceptual12Mdataset[4]and their features [2, 5, 8, 14, 22, 63–65, 68]. The pioneering
evaluate our learned representation on a number of down- workinBEIT[2]introducedthereconstructionofdiscrete
stream tasks. These tasks include the zero-shot classifica- tokens, akin to VQ-VAE [59], using block-wise masking.
tion and linear probing on ImageNet [11], text and image This method demonstrated results on par with contrastive
retrieval on MS-COCO [38], and the SUGARCREPE lan- learning and self-distillation methods [3, 7] during model
guage composition benchmark [25]. In our experiments, fine-tuning. LaterapproachesincludePeCo’s[14]novelvi-
ourmodeloutperformsFLIP[36]andCLIP[49]ondown- sual codebook learning method and BEIT V2’s [47] inte-
stream performance, while having efficiency comparable grationofself-distillationmethods,usingateacher-student
with FLIP. We also show that the performance can further backbone and feature-level KL divergence loss [58]. Fur-
be improved by using the model’s learned feature embed- ther exploration in this field has led to the use of natural
dingduringclustering. imagesignalsasreconstructiontargets,movingawayfrom
learned features. Examples include SimMIM [68], which
2.RelatedWork
reconstructs pure RGB values, MaskFeat [64], introduc-
ing reconstruction of the Histogram of Oriented Gradients
Contrastive Vision-Language Pre-training. Vision-
(HOG) features, and MAE [22], which reconstructs pixel-
Language Pre-training (VLP) focuses on establishing
normalizedRGBvalues. Ourworkdrawsinspirationfrom
connections between images or their components and
these studies, particularly in using pixel-normalized RGB
human-interpretable language. This field initially evolved
valuestocomputepatchsimilarities,arguingforamoreef-
from transferring supervised learning models, which
fectivedistributionofpatchfeatures.
incorporated object detection modules to generate fine-
grained visual labels [9, 56, 57]. Subsequently, there
MaskingStrategiesinMIM. Parallelinvestigationshave
was a shift towards large-scale learning using noisy web
focusedonmaskingstrategiesinMIMs[19,28,33,37,54,
data, moving away from reliance on fine-grained labels
66, 67]. Early works like BEIT and its successors used
[1,27,34,50,69,72,73]. Asignificantdevelopmentinthis
block masking, while others such as SimMIM, MaskFeat,
domainwasCLIP[50], whichappliedcontrastivelearning
andMAEappliedrandompatch-wisemasking. Attention-
techniques [6, 21] to train models to associate correct
basedmaskingstrategieshavealsobeenexplored,typically
image-text pairs and dissociate incorrect ones. CLIP
using attention maps from vision transformers. MST [37]
scaled contrastive visual-language models significantly
masks less essential parts with low attention scores, using
beyond previous work, enabling strong feature learning
areconstructionlossapproach. Incontrast,AttnMask[28]
and zero-shot performance. However, further scaling
masks highly attentive patches and applies self-distillation
significantly increases the pre-training demands, requiring
loss. These methods involve simultaneous updates of at-
largerdatasetsandbatchsizes.
tention maps and masks during training. A potential lim-
In response to these challenges, recent research has ex-
itation of this approach is that insufficiently trained atten-
ploredincorporatingmaskingintoimagestoreducetraining
tion maps may not capture structured features effectively.
timeandallowformoresamplesperbatch[15,20,36,70].
SemMAE [33], starting with iBot features [75], adopts an
Methods such as MaskClip [15], FLIP [36], and VIOLET
easy-to-hard masking strategy, starting with masking parts
[20] have implemented random masking strategies. Yet, it
within clusters and gradually expanding to entire clusters.
hasbeennotedthatrandommaskingmaynotbeaseffective
Wilf et al. [66] introduce a unique entity-reinforced lan-
onrelativelysmalldatasets[35,70].Toaddressthis,ACLIP
guagemodelformaskingobjectsinvideoframes.However,
[70]introducedamethodofmaskingtokenswithlowcross-
the reliance on pre-trained features or extracting attention
attention scores with text. However, this approach neces-
mapscanbecomputationallyintensive.EvolvedPartMask-
sitates two forward passes to generate the attention map
ing[19]proposesusingEMalgorithmonattentionmapsto
andrequiresadditionalcomputationalmodules[41]. Inour
getaclusteringbeforeperformingSemMAEstylemasking.
work, we aim to avoid these limitations, proposing an ef-
Ourapproachalsoadoptsaclusterbasedmaskingstrategy
fectivemaskingmethodthatisbasedonapatch’srawRGB
invision-languagepre-training,enablingfasterpre-training
values.
withoutrequiringadditionalmodificationstothemodel.
MaskedImageModeling. Inthefieldoflanguagemod-
3.Method
eling, the effectiveness of models that learn to reconstruct
corrupted inputs for generating robust features has been Weproposeaclusterbasedmaskingstrategyforcontrastive
recognized [29, 39]. This approach, known as Mask Lan- vision-languagepre-training, focusingonmaskingrandom
guageModeling(MLM),hasbeenadaptedintherealmof clusters with visually similar semantics. Our method se-
image processing as Mask Image Modeling (MIM). MIM lects random anchor patches as cluster centers and com-
techniques involve reconstructing either image patches or putespairwisepatchdistancestoformclusters. Theseclus-r. The cluster for an exemplar patch x is represented by:
S ={y|d(x,y)≤r}forimagepatches.
x
All patches within a cluster are masked out. The dis-
tance threshold r is automatically searched before training
according to an average masking ratio. We provide a sim-
plifiedpseudocodeofthemaskingstrategyinAlgorithm1.
AnchorPatch DistanceHeatmap MaskedImage
Figure 2. Choosing clusters. The process begins by randomly Clustering Embedding Features. Another variant of
selecting anchor patches from the image. We then calculate the patch feature is the combination of pure RGB values and
pairwise distances among all patches. Clusters formed within a patch embedding layer features from transformers [16].
distancethresholdaremaskedout.Weshowclusterobtainedfrom When computing similarity scores, we integrate these two
asingleanchorpatch. measures into a weighted sum, where the weight of each
measureisdeterminedby:
tersarethenmaskedentirely. Toenhanceaccuracyinclus-
d(x,y)=α·d (x,y)+(1−α)·d (x,y), (2)
rgb emb
ter formation, we introduce an adaptive layer for refining
where x and y represent two patches, d is the cosine
the distance matrix. Additionally, attention masks and a rgb
similarity based on pure RGB values, and d is the co-
hardpatchcutoffareusedtoensureuniforminputsizesare emb
sine similarity based on the transformer’s embedding fea-
consistentinbatchesforautodifferentiation.
tures. Theweightparameterαlinearlyincreasesfrom0to
1duringtraining.
3.1.ContrastiveVision-LanguagePre-training
Theembeddinglayeriscalculatedbeforethepatchesen-
Our approach builds on contrastive vision-language pre- ter the transformer, thus we could reuse the patch embed-
training methods, such as CLIP [49]. We use contrastive dingsinthetransformerwithoutcomputingthemtwice.Us-
learningtoalignembeddingsofmatchingtext-imagepairs
ingtheembeddinglayerisadvantageousbecauseitincorpo-
and separate those of non-matching pairs. This process is
ratespositionalencodings[60]. Thisintegrationpotentially
steeredbytwosymmetricInfoNCElosses[43]: thevision-
introducesspatialconstraints,whichwebelievecanfurther
to-languagelossL anditscounterpart,thelanguage-to-
v→l
enhanceourmaskingstrategy.
visionlossL . Thevision-to-languagelossisdefinedas:
l→v
exp(sim(I,T)/τ)
L =−log , (1)
v→l (cid:80)N exp(sim(I,T )/τ) Handling Batched Inputs. Deep learning libraries, like
j=1 j
PyTorch [46], typically process batched inputs of uniform
whereIandT aretheembeddingsfortheimageandtextre-
size. However, in our method, the mask ratio would vary
spectively,simdenotesthesimilarityfunction(weuseadot
acrossdifferentimages,leadingtofluctuationsinthenum-
product),andτ isatemperatureparameter.Similarly,L
l→v
ber of patches. To accelerate the process, we introduce a
is formulated by normalizing the loss using a batch of N
minimummaskratiothresholdβ foreachimage. Ifthecal-
imageexamples{I }N insteadoftextexamples{T }N .
j i=1 j i=1 culatedmaskratioforanimagedoesn’tmeetthispredefined
3.2.ClusterMasking threshold, we proceed to randomly drop patches until the
desiredratioisachieved. Conversely,forimageswithpatch
We introduce a masking strategy that drops out random counts less than the threshold, we use attention masks to
clusters. While one option would be to use an off-the- avoidmaskedpartsengaginginattentioncalculation[13].
shelfclusteringmethod,suchasK-Means[40],wechoose
instead to use a simple and efficient method that results 4.Experiments
in a random clustering each training iteration (Figure 2).
OurapproachresemblesasingleiterationofK-Means,and Wepresentacomprehensiveevaluationofourproposedal-
works by selecting a set of exemplar patches, which each gorithmtoexhibittheperformance,robustness,scalability,
defineacluster. Inourexperiments,wealsoevaluatemask- andefficiencyofourframework.
ing clusters obtained using K-Means as an alternative ap-
4.1.ImplementationDetails
proach.
We split an input H × W image into patches, follow- Datasets and Training Details. We train our model us-
ing [16]. We then compute the pairwise cosine similarity ing the Conceptual 12M (CC12M) dataset[4], containing
between every pair of normalized patches, which we use 12 million unique image-text pairs, for pre-training our
as a distance function d(x,y). We choose a small subset vision-language models. We use ViT-B/16 as backbone
(less than 5%) of these patches at random to act as cluster for image encoder. The text encoder is a 12-layer trans-
centers. Foreachoftheseselectedanchorpatches,wede- former,equippedwith8multi-headattentionunitsand512-
fineaclusterconsistingofpatchesthatliewithinadistance dimensional embeddings. Input images are processed at aAlgorithm 1 Pseudocode of cluster based masking in a across attention heads in the final transformer block to de-
PyTorch-likestyle. termine attention scores. Patches that receive the highest
attentioninrelationtothe[CLS]tokenareretained.
# img: the image for mask
# mask_ratio: the ratio for chosing anchor patches To ensure a fair comparison among these methods, we
# r: the threshold for computing cluster keep the number of patches consistent the same as ours
def generate_mask(img, mask_ratio, r): withinasinglebatch, whichmeansforFLIPandFLIP ,
Attn
# Make image into shape (N,L,C).
# N: number of samples per batch weapplyabatchsizeof256foroneGPUandforCLIP,we
# L: number of patches use 128 instead. Additionally, we apply a scaling law on
# C: size of feature dimension
img = patchify(img) learningrateacrossdifferentmodels.
# Normalize each patch
img = (img - img.mean(dim=-1)) / img.std(dim=-1)
# Compute pairwise cosine similarity matrix Evaluation Details. Our models are tested across vari-
x = img / img.norm(dim=-1) ousbenchmarksto ensureitsrobustnessand effectiveness.
distance = bmm(x, x.transpose(-2, -1))
We conduct zero-shot image-to-text and text-to-image re-
# Generate a boolean masking matrix of shape (N, L)
init_mask = random_patch_indicies(mask_ratio) trieval tasks on COCO [38] and Flickr [71], assessing its
init_mask = init_mask[:, :, None].float() performance meticulously. Further more, we evaluate the
# Get the cluster-based mask
candidates = (distance * init_mask) ≥ r models’imagerepresentationqualitybyreportingboththe
cluster_mask = candidates.sum(1) ≥ 1
# Mask both the anchor patches and clusters zero-shot classification and linear probing performance on
mask = init_mask or cluster_mask threemainstreamdatasets: ImageNet[11],CIFAR-10,and
return mask CIFAR-100 [30]. Zero-shot results of some other datasets
bmm:batchmatrixmultiplication. likeImageNetvariants[23,24,51,55,62],Caltech101[17],
Flowers [42] and Pets [61], are also reported to verify the
resolutionof224×224, andtextinputsareadjustedto77
method’srobustness.
tokens, either by truncation or padding. A class token is
Forthesetasks,ourapproachadheresstrictlytotheim-
transformedintoa512-dimensionalfeatureembeddingvia
plementationusedintheCLIPbenchmark,ensuringconsis-
a multi-layer perceptron (MLP). For optimization, we use
tencyandreliabilityintheevaluationprocess.Furthermore,
the AdamW optimizer with a learning rate of 5 × 10−4,
weassesstheeffectivenessofourmethodologyonlanguage
β = 0.9, and β = 0.98. We use a batch size of 256 per
1 2 composition tasks using SUGARCREPE [25]. This evalua-
GPU,andtrainusing8NVIDIAA40GPUs.
tionaimstodetermineitsadaptabilityandefficiencyacross
Ourmethodcomesinthreevariants:K-Means,RGBand
various contexts, including object, attribute, and relation
Embedding. TheRGBmodelclustersbasedonrawimage
manipulations.WithintheSUGARCREPEframework,mod-
patches, while the embedding model integrates patch em-
elsaretaskedwithidentifyingthecorrectcaptionthataccu-
beddingfeatureswithRGBforclustering. IntheK-Means
rately describes an image, distinguishing it from a closely
variantof themodel, wemaskout halfoftheclusters ran-
relatedbutincorrecttexthardnegatives. Thehardnegatives
domly. The model constructs 12 clusters and runs for a
ischaracterizedbyminorcompositiondifferencesfromthe
maximum of 10 iterations. For both RGB and embedding
accuratecaption.
models, we set an average masking ratio of 50%, follow-
ing the recommendations of FLIP [36] for optimal mask- 4.2.MainResults
ing ratios. In the RGB approach, we use a 50% cutoff for
VisualizationofClusters. Figure3offersavisualdepic-
Ours-RGB and30%forOurs-RGB ,whereastheOurs-
0.5 0.3 tion of our cluster based masking technique as outlined in
Embeddingmodelusesa30%cutoff.Also,theRGBmodel
themethodologysection. Forthisillustration,werandomly
selectsanchorpatchesata3%ratio,comparedtoa5%ratio
select a number of image-text pairs from the COCO vali-
intheembeddingmodel.
dationsetandapplyourmaskingmethodtothepureRGB
dataoftheimages. Thevisualizationisshowingthemask-
Baselines. Inourstudy,weestablishbaselinesusingthree ing result of the two-stages. In the first stage, a subset of
models: CLIP, FLIP, and FLIP , each trained from patches (5%) is randomly selected as anchor patches from
Attn
scratch on the CC12M dataset. These baseline mod- thepoolofallimagepatches,whichareannotatedwiththe
els are derived from the open-source implementation of red boxes. In the second stage, we visualize the masked
CLIP, known as OpenClip [10, 26, 48, 52]. For both clusters that are calculated based on the similarity matrix,
FLIP and FLIP , we implement a patch dropout ratio of whereeachclusterisrepresentedbyadistinctcolor.
Attn
50%. Specifically, FLIP uses a random dropout approach,
whereas FLIP adopts an attention-based masking strat- Zero-shot Retrieval Results. In our investigation into
Attn
egy inspired by ACLIP [70]. This strategy involves pro- the model’s understanding of the relationship between vi-
cessing the image through the encoder and then averaging sual and linguistic representations, we conduct zero-shotAlittlegirl Thetelephonehas Apersonstanding
Alargewhite Amanflying Abananais
isholdingan abananawhere inshoreof
bowlofmany throughtheair layingona
umbrellaona thereceiver beachwitha
greenapples. whileridingskis. smallplate
wetday. shouldbe. frisbeeinthesky.
Figure3. Visualizationofclustermasks. Differentcolorsrepresentdistinctclustersformedbythesimilaritymatrixcalculatedfromthe
chosenanchorpatches.
retrievaltestsonseveralleadingretrievalbenchmarks. The sification benchmarks. The zero-shot classification results
results, detailed in Table 1, provide insights into the per- are presented in Table 2, while the linear probing results
formanceofourapproachagainstothers,particularlyinthe canbefoundinTable3.Forbetterevaluatingthetimespent
contextofImage2TextandText2Image’srecallprecisionat ontraining,wenormalizeallmethod’strainingtimebythe
top1(R1),top5(R5)andtop10(R10)metrics. CLIP’strainingtime,whichisconsideredas1×.
In the evaluation on the MS-COCO [38], Flickr8k, and Whencomparingourmodel’sperformancetoCLIP(i.e.,
Flickr30k [71] datasets, our model outperforms both the no masking), our model demonstrates superior results on
baselinesinmostparts.Notably,intheImage-to-Texttasks, themajorityoftestcases,showcasinganaverageimprove-
our model performs best in most datasets, with the excep- mentof+2.1%,withabout+36%speedingup. Incompar-
tionofaslightperformancedecreasecomparedtoFLIP isontotheFLIPstrategy,whichhasasimilartrainingdura-
Attn
on the MS-COCO dataset. We attribute this success to tion,ourmodelhasanimprovementof+5.5%. Incompar-
ourtrainingstrategy,whichprioritizesprimaryclustersand isontotheFLIP ,ourmodeldoesnotneedtheattention
Attn
minimizestheinfluenceofnoise. Furthermore,weobserve mapforguidance, whichgivesamuchfasttrainingspeed,
that methods combining RGB information with token em- whilehavingaperformanceof+2.6%onaverage.
beddingsoutperformthoserelyingsolelyonRGB.Wehy- Outof12datasetsonthezero-shotclassificationbench-
pothesize that this is because the embedding layer, which mark,ourRGBandembeddingmodelachievesthetopper-
containsslightlyhigher-levelinformation. formance on 11 of them. In particular, it obtains strong
When comparing FLIP to CLIP, FLIP’s performance is performance on the ImageNet variants: ImageNet-A [24],
noticeablyweaker,evenwithlargebatchsizes. Wesuspect ImageNet-O[55],ImageNet-R[55],andImageNet-S[62],
thatFLIP’ssub-optimalresultsinourexperimentalsettings which often contain challenging and diverse images. The
may not fully exploit its strengths. This aligns with find- RGB version of our method also significantly outperforms
ings from other studies, such as Yang et al.’s research on FLIP and surpasses the CLIP model, especially on Ima-
ACLIP [70], which also noted FLIP’s limitations. We ob- geNetanditsvariants,whichdemonstratestheeffectiveness
serve that using attention scores for masking can improve ofourmethodwitheventhenaturalguidance.
performance compared to purely random masking. How- The linear probing results further suggest the effective-
ever, random masking still falls short of our cluster based ness of our method. Our models achieve +1.8% accuracy
maskingoreventheoriginalCLIPmethodinsomebench- onImageNet,+3.1%onCIFAR-10,and+4.2%onCIFAR-
marks. 100.
ResultsonZero-shotClassificationandLinearProbing. Language Composition. A potential drawback of our
We evaluate our model on several widely recognized clas- method might be understanding compositions of concepts
segamI
srohcnA
sretsulC
noitpaCtext→image image→text
MS-COCO Flickr8k Flickr30k MS-COCO Flickr8k Flickr30k
R1 R5 R10 R1 R5 R10 R1 R5 R10 R1 R5 R10 R1 R5 R10 R1 R5 R10
CLIP[49] 34.60 61.98 72.72 55.70 81.60 89.90 58.50 83.80 89.10 23.49 47.80 59.66 40.54 68.90 80.20 43.18 70.44 80.40
FLIP[36] 32.62 59.14 70.64 55.00 80.90 88.90 53.80 80.80 88.50 22.56 46.08 58.09 40.32 68.10 78.64 41.52 67.90 77.46
FLIPAttn[70] 33.66 60.18 71.02 53.70 80.09 87.99 55.29 81.40 87.60 23.89 48.33 60.04 40.58 68.88 78.86 43.06 70.10 78.82
Ours-Kmeans 33.68 61.14 71.97 55.10 83.30 90.90 55.40 82.00 88.90 22.60 46.93 58.84 40.10 69.86 79.91 41.32 68.50 77.58
Ours-RGB0.5 32.82 60.20 71.40 52.10 81.20 89.50 54.90 81.20 88.00 22.97 47.29 59.21 40.84 68.72 79.14 41.80 70.20 79.42
Ours-RGB0.3 35.87 61.50 72.34 54.70 81.40 90.70 57.60 83.90 90.30 23.65 47.54 59.25 40.90 68.22 79.00 42.92 69.96 79.12
Ours-Embedding 34.26 61.96 73.30 57.00 82.70 90.10 55.80 84.20 89.60 23.77 48.18 59.76 42.00 69.40 79.64 43.30 70.92 80.16
Table1. Zero-shotretrievalresults. WeevaluateonMS-COCO[38],Flickr8kandFlickr30kdatasets[71],wheretheRecall@1(R1),
Recall@5(R5),andRecall@10(R10)arereported.
Method N.T. IN-1K[11] IN-A[24] IN-O[55] IN-R[23] IN-S[62] INv2[51] MNIST[12] Cal101[17] CIFAR-10[31] CIFAR-100[31] Flowers[42] Pets[61] Average
CLIP[49] 1.00× 36.1 8.0 38.4 47.6 24.9 30.7 11.7 73.5 57.7 25.0 26.0 53.9 36.1
FLIP[36] 0.53× 34.4 7.1 39.5 41.4 20.1 29.5 10.4 70.4 52.8 24.5 25.3 46.0 33.5
FLIPAttn[70] 1.06× 35.2 8.1 39.4 45.1 23.7 30.1 9.4 73.5 61.6 27.1 25.7 51.2 35.8
Ours-Kmeans 0.70× 35.5 7.2 38.3 43.0 22.2 30.1 9.7 69.9 61.4 25.3 25.6 52.1 35.0
Ours-RGB0.5 0.54× 36.0 7.7 39.8 45.3 23.8 30.5 11.2 72.2 63.6 27.2 26.1 55.0 36.5
Ours-RGB0.3 0.64× 36.6 8.8 39.9 45.9 24.9 31.8 9.4 72.3 63.1 26.3 25.4 57.3 36.8
Ours-Embedding 0.64× 36.3 8.1 39.6 47.9 25.4 30.7 11.0 73.7 70.7 32.0 28.4 55.4 38.2
Table 2. Zero-shot classification result. We evaluate popular datasets using clip-benchmark [32]. The training time is normalized
accordingtotheCLIP’strainingtime.HereN.T.representsnormalizedtimeagainstandINrepresentsImageNet.
Method CIFAR-10 CIFAR-100 IN-1K understanding.
CLIP 88.0 67.4 62.3
FLIP 85.9 65.5 61.3
Qualitative Comparison of Masking Strategies Our
FLIP 86.4 66.1 62.0
Attn
method outperforms the random masking strategy by pre-
Ours-Kmeans 88.0 69.1 62.2
Ours-RGB 86.7 66.2 62.5 serving more semantic content in the unmasked image
0.5
Ours-RGB 88.6 68.7 63.1 patches, a comparison showcased in Figure 1. The advan-
0.3
Ours-Embedding 89.0 69.7 62.7 tage of our technique is further explored by the caption-
Table3. Linearprobingresult. Allmethodsaretrainedfor10 ing experiment detailed in Figure 7, wherein two sets of
epochsat learningrateof 1e-3. ForCIFAR-10andCIFAR-100, images, each masked differently, are fed into a captioner,
we use a batch size of 64 and for ImageNet-1k the batch size is GPT-4[44,45]. ThecaptionerispromptedtogenerateMS-
1024. COCO-style captions for the unobscured sections. When
comparing these captions to the standard references, it be-
comesclearthatourclusterbasedmaskingnotonlyretains
in language. As we mask out clusters, there is a risk that
key elements but also the interrelations among them. For
themodelmayincreasinglyadoptbag-of-wordstendencies
instance, our approach accurately enables the captioning
[74], which could impede its ability to learn the relation-
system to identify an airplane in the first example and to
ships between objects. For example, if an image is cap-
describe the baseball player’s action in the second, while
tioned with ”dog on grass”, the grass may be masked for
the random masking strategy failed to achieve this clarity.
a large portion in our model as they are highly similar to
These results indicate that our masking method provides a
eachother. Thiswillmakelearningtherelation“on”diffi-
moredetailedcomprehensionoftheimage.
cult. Therefore, weapply SUGARCREPE [25]benchmarks
to test the model’s ability to understand language compo-
4.3.AblationStudy
sitions. SUGARCREPE benchmarks assess this by gener-
ating negative captions through manipulations like adding, AblationonAnchorPatchRatio. Inourstudy,wecon-
swapping, or replacing concepts in sentences, followed by ducted an ablation on finding the optimal proportion of
text retrieval tests to evaluate the model’s accuracy in se- patchestoserveasanchorpatches. Theresultsofthisabla-
lectingthecorrectanswer. Fromtheourtestresults,which tionaresummarizedinFigure5. Weusezero-shotlearning
shown in Table 4, our model yields comparable results in results on the ImageNet-1k dataset as the benchmark for
Relation tests and demonstrates a significant enhancement assessing the quality of the representations learned by our
in Object and Attribution tests, with an average improve- model. Additionally,wecalibratethethresholdforeachex-
mentof+3.9%and+3.0%respectively,comparedtoFLIP. perimenttoensurethattheaveragefinalmaskingratiowas
Thisimprovementmaystemfromthemaskingofentireob- maintainedat50%.
jects,whichsimplifiesthechallengeofcontrastivelearning Ourfindingsindicatethatasmallerproportionofanchor
by reducing ambiguity. This clarity facilitates the model’s patches tends to yield superior performance. We hypoth-
learning of relationships, a crucial factor for composition esize that this improvement is due to the decreased ran-REPLACE SWAP ADD Average
Object Attribute Relation Object Attribute Object Attribute Object Attribute Relation
CLIP 85.77 79.18 64.51 61.78 58.71 74.24 68.35 73.71 68.75 64.51
FLIP 84.07 75.88 66.00 60.16 61.56 71.67 63.15 71.97 66.86 66.00
FLIPAttn-0.5 86.62 75.50 63.22 52.44 63.06 71.65 66.76 71.57 68.39 63.22
FLIPAttn-0.3 86.07 75.00 62.23 60.57 59.16 74.68 68.64 72.82 63.47 62.23
Ours-Kmeans 84.50 76.90 63.09 62.20 61.86 71.77 65.46 73.66 67.60 63.09
Ours-RGB0.5 86.86 73.47 60.24 59.35 63.36 73.33 66.76 73.18 68.42 60.24
Ours-RGB0.3 86.13 75.13 64.65 66.67 63.36 74.92 71.24 75.91 69.91 64.65
Table 4. Language composition test result. This table presents the performance of models on the SUGARCREPE evaluation, which
involves replacing, swapping, or adding atomic concepts such as objects, attributes, and relations in a sentence to create mismatched
captions.
Reference RandomMask ClusterMask as shown in Table 1, 2, and Table 5. For our method, the
cutoff ratio denotes the minimum mask ratio applied and
the true visible patch ratio is shown in visible ratio. For
the FLIP counterpart, it maintains a consistent mask ratio
acrossallimages. Theresultsindicatethatourmethodnot
only matches the speed of FLIP but also surpasses FLIP
Twoplanesflying Aclearsky Jetsflying
inthesky abovean overan in zero-shot ImageNet-1K classification accuracy with a
overabridge. archbridge. archbridge.
+1.6%improvementevenbyseeingfewerpatches. Theat-
tentionbasedmaskingmethodwithlessmaskratioachieves
similarperfromanceasoursbutthespeedismuchslower.
Thesefindingssuggestthatclusterbasedmaskingserves
asaneffectivedenoisingtechniqueforthedataset.Areason
forthisenhancedperformanceisthatwecouldeasilymask
Aboycatchesa Playersona Abaseballplayer
ballasaplayer baseballfield slidingintohome out typically irrelevant areas, such as uniformly colored
slidestothebase. duringagame. plateduringagame.
backgrounds,whicharelessinformativeandoftentimesdo
Figure4. Generatedcaptionfromvisiblepatches. Weprocess not correspond to any word in the caption. This targeted
themaskedimagesthroughGPT-4[44,45]tocreatecaptionsfor approach enables the model to focus on more meaningful
theunmaskedsegments. contentwithintheimages.
domness in the selection of anchor patches, which in turn Additionally, our findings reveal an improved feature
enhances the clustering performance by providing a more learning by the model when a smaller random masking is
stable set of reference points for the model to learn from. applied. By reducing the cutoff ratio from 50% to 30%,
However,themaskingratiocannotbetoosmall,asthesim- we observed a 1% enhancement in classification accuracy.
ilaritythresholdwillbecometoosmall, whichmayreduce Thus, there is some trade-off between the model’s perfor-
theclusteringquality. manceandspeed.Despitethis,ourmodelwithlargermask-
ing cut off still remains significantly faster compared to
attention-basedmaskingortheoriginalCLIPmethod.
36.6 IN-1K
36.6
36.4 Method β VisibleRatio NormalizedTime IN-1K
36.3
FLIP 50% 50% 0.84× 34.4
36.2
36.1 FLIP 30% 70% 1.00× 35.4
36.0
36.0 36.0 FLIPattn 50% 50% 1.73× 35.2
FLIPattn 30% 70% 1.97× 36.6
35.8 Ours-RGB 50% 43% 0.84× 36.0
Ours-RGB 30% 50% 1.00× 36.6
35.6
35.5
Table 5. Ablation on minimum mask ratio β. Comparison of
35.4 0.01 0.03 0.05 0.1 0.3 0.4 variousmethodsagainstdifferentminimummaskingratios. The
AnchorPatchRatio zero-shot ImageNet-1k classification results are used as metric.
Figure 5. Effect of anchor patch ratio. All the final masking ThetimeisnormalizedtooursRGBmodelwithβ=30%.
ratioistunedtobe50%.
Ablation on Pixel Normalization. In our experiments,
AblationonMinimumMaskRatio. Wefurtherdemon- we incorporate pixel normalization (making each patch
stratethecapabilityofourmethodbysettingtheminimum mean zero and unit standard deviation 1) into the process
maskratioβthesameastheaveragemaskingratioofFLIP, ofcomputingthesimilaritymatrixforimages. Thisyieldsaperformanceimprovementof+1.1%, asshowninthere-
sults presented in Table 6a. The underlying rationale for
thisenhancementisattributedtothestandardizationofim-
agepatches. Byusingpixelnormalization,wefocusonthe
relative intensity of pixels, thereby diminishing the impact
oflightingvariationsamongdifferentimages.
This normalization process is particularly beneficial in
scenarios where the dynamic range of pixel values varies
significantly across different patches. By scaling the
patches to a common range, pixel-norm mitigates the risk
of disproportionate influence from patches with higher in-
AnchorPatch RGB-based Embedding-based
tensityvalues. Consequently,thisleadstoamorebalanced
and equitable comparison among patches, enhancing the Figure 6. Representation used in clustering. We illustrate the
distinctionsbetweenusingpureRGBandViTembeddingstocom-
model’sabilitytodiscernandquantifysimilaritiesmoreef-
putethesimilaritymatrixusedinmaskingpatches.
fectively.
4.4.Limitations
Method IN-1K k IN-1K Ourmethodologyusesauniformthresholdforallimages,a
w/oP.N. 35.5 0.5 36.1 strategythat,whileeffective,maynotbethemostoptimal.
w/ P.N. 36.6 1 36.3 Future research could explore the implementation of indi-
2 35.9 vidualizedthresholdsforeachimage,potentiallyleadingto
amoreintelligentandadaptivemaskingprocess.
(a)Ablationonpixelnormaliza- (b)Ablationonpolynomialcoef-
tion(P.N.). ficientk. All of our approaches use the popular backbone archi-
tectureViT-B/16[16]andaretrainedsolelyontheCC12M
Table6. AblationStudy. Table6apresentsanablationstudyon
dataset[4]. Expandingthescopeoftheexperimentscould
theapplicationofpixelnormalizationwhencalculatingthesimi-
laritymatrixforclustering. Table6bexplorestheeffectsofvary- offeradditionalinsights.
ingthepolynomialcoefficientk, whichadjuststheadaptiverate
usedwhencombiningRGBandembeddingfeatures. 5.Conclusion
In our study, we introduce a novel cluster based mask-
ingstrategydesignedforvision-languagepret-raining. Us-
EffectofFeaturesusedinClustering. InTables1and2,
ing either pure RGB values or shallow features from the
embedding-based methods surpass those dependent solely
patchembeddinglayer,ourmethodeffectivelyclustersim-
on RGB data, especially in image-to-text retrieval tasks.
age patches, maintaining essential visual semantics. We
One reason for this may be the fact that the embedding
then randomly mask out these clusters, enabling efficient
model has access to the positional encoding, whereas the
training. Ourapproachdemonstratessuccessacrossvarious
RGB-basedmodelsolelyusestheappearanceofeachpatch.
downstream evaluation tasks, including both pure image-
Figure 6 qualitatively shows this advantage: while the
based tasks such as image classification and multimodal
RGB-onlyapproachmasksextraareas(suchashairorshad-
tasks like image-text retrieval and language composition
ows in the first scenario; a laptop and phone in the sec-
tests. We believe our work marks a considerable progres-
ond)duetocolorsimilarities,theembedding-basedmethod
sioninthisdomainandanticipatethatitwillstimulatefur-
masksmorecompleteobjectparts.
therresearchintooptimizingmaskingstrategiesforsimilar
applications.
AblationonAdaptiveRate. Inourapproach,weinterpo-
AuthorContributions Allauthorscontributedtodesign-
latebetweenusingRGBfeaturesandthepatchembedding
ingprojects,launchingexperiments,andwritingthepaper.
layerfeatureusingacoefficient,α,whichvarieswitheach
Zixuan Pan focused on algorithm optimization; Zihao Wei
epoch. Denoting the current running epoch as E and the
c focused on code design; Andrew Owens supervised this
numberoftotaltrainingepochsasE ,thiscoefficientisde-
t project,offeredfeedback,andassistedinwritingthepaper.
(cid:16) (cid:17)k
finedasα= Ec .Inadditiontothelinearmethodwhere
Et
k = 1, weexploreotherpolynomialcoefficientsk forthis Acknowledgements ThisresearchissupportedbyaSony
combination,assummarizedinTable6b. Ourfindingssug- Research Award. We are grateful to Jeongsoo Park, Chao
gestthatthelinearcombinationismosteffective,likelydue Feng,YimingDou,DanielGeng,ZiyangChen,andAyush
toitssmoothtransition. Shrivastavafortheirvaluablesuggestionsanddiscussion.References [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
[1] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine
formersforlanguageunderstanding. 2018. 3
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
[14] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen,
KatherineMillican, MalcolmReynolds, etal. Flamingo: a
WeimingZhang,LuYuan,DongChen,FangWen,Nenghai
visual language model for few-shot learning. Advances in
Yu, andBainingGuo. Peco: Perceptualcodebookforbert
Neural Information Processing Systems, 35:23716–23736,
pre-training of vision transformers. In Proceedings of the
2022. 2
AAAIConferenceonArtificialIntelligence,pages552–560,
[2] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit:
2023. 2
Bert pre-training of image transformers. arXiv preprint
[15] Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang,
arXiv:2106.08254,2021. 2
Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang,
[3] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu.
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg- Maskclip: Masked self-distillation advances contrastive
ing properties in self-supervised vision transformers. In language-imagepretraining,2023. 1,2
ProceedingsoftheIEEE/CVFInternationalConferenceon
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
ComputerVision,pages9650–9660,2021. 2
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
Soricut.Conceptual12m:Pushingweb-scaleimage-textpre- vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis
trainingtorecognizelong-tailvisualconcepts. InProceed- worth16x16words: Transformersforimagerecognitionat
ingsoftheIEEE/CVFConferenceonComputerVisionand scale. ICLR,2021. 3,8
PatternRecognition(CVPR),pages3558–3568,2021. 2,3,
[17] LiFei-Fei, R.Fergus, andP.Perona. One-shotlearningof
8
object categories. IEEE Transactions on Pattern Analysis
[5] MarkChen,AlecRadford,RewonChild,JeffreyWu,Hee- andMachineIntelligence,28(4):594–611,2006. 4,6
wooJun, DavidLuan, andIlyaSutskever. Generativepre- [18] PedroFFelzenszwalbandDanielPHuttenlocher. Efficient
training from pixels. In International Conference on Ma- graph-based image segmentation. International journal of
chineLearning,pages1691–1703.PMLR,2020. 2 computervision,59:167–181,2004. 1
[6] TingChen,SimonKornblith,MohammadNorouzi,andGe- [19] Zhanzhou Feng and Shiliang Zhang. Evolved part mask-
offreyHinton. Asimpleframeworkforcontrastivelearning ing for self-supervised learning. In Proceedings of the
ofvisualrepresentations.InInternationalconferenceonma- IEEE/CVF Conference on Computer Vision and Pattern
chinelearning,pages1597–1607.PMLR,2020. 2 Recognition,pages10386–10395,2023. 2
[7] X. Chen, S. Xie, and K. He. An empirical study of train- [20] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang
ingself-supervisedvisiontransformers. In2021IEEE/CVF Wang, Lijuan Wang, and ZichengLiu. Violet: End-to-end
InternationalConferenceonComputerVision(ICCV),pages video-languagetransformerswithmaskedvisual-tokenmod-
9620–9629,2021. 2 eling. arXivpreprintarXiv:2111.12681,2021. 2
[8] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, [21] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRoss
Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Girshick. Momentumcontrastforunsupervisedvisualrep-
Gang Zeng, and Jingdong Wang. Context autoencoder resentationlearning. InProceedingsoftheIEEE/CVFcon-
for self-supervised representation learning. arXiv preprint ference on computer vision and pattern recognition, pages
arXiv:2202.03026,2022. 2 9729–9738,2020. 2
[9] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, [22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
FaisalAhmed,ZheGan,YuCheng,andJingjingLiu.Uniter: Dolla´r,andRossGirshick.Maskedautoencodersarescalable
Universal image-text representation learning. In European visionlearners. arXivpreprintarXiv:2111.06377,2021. 2
conference on computer vision, pages 104–120. Springer, [23] DanHendrycks, StevenBasart, NormanMu, SauravKada-
2020. 1,2 vath,FrankWang,EvanDorundo,RahulDesai,TylerZhu,
[10] MehdiCherti,RomainBeaumont,RossWightman,Mitchell SamyakParajuli,MikeGuo,DawnSong,JacobSteinhardt,
Wortsman,GabrielIlharco,CadeGordon,ChristophSchuh- andJustinGilmer. Themanyfacesofrobustness: Acritical
mann,LudwigSchmidt,andJeniaJitsev.Reproduciblescal- analysis of out-of-distribution generalization. ICCV, 2021.
ing laws for contrastive language-image learning. In Pro- 4,6
ceedingsoftheIEEE/CVFConferenceonComputerVision [24] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
andPatternRecognition,pages2818–2829,2023. 4 hardt,andDawnSong.Naturaladversarialexamples.CVPR,
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, 2021. 4,5,6
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage [25] Cheng-YuHsieh,JieyuZhang,ZixianMa,AniruddhaKem-
database.In2009IEEEConferenceonComputerVisionand bhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable
PatternRecognition,pages248–255,2009. 2,4,6 benchmarks for vision-language compositionality. arXiv
[12] LiDeng.Themnistdatabaseofhandwrittendigitimagesfor preprintarXiv:2306.14610,2023. 2,4,6
machinelearningresearch. IEEESignalProcessingMaga- [26] GabrielIlharco,MitchellWortsman,RossWightman,Cade
zine,29(6):141–142,2012. 6 Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,VaishaalShankar,HongseokNamkoong,JohnMiller,Han- [41] Norman Mu, Alexander Kirillov, David Wagner, and Sain-
nanehHajishirzi,AliFarhadi,andLudwigSchmidt. Open- ingXie. Slip: Self-supervisionmeetslanguage-imagepre-
clip,2021. Ifyouusethissoftware,pleaseciteitasbelow. training,2021. 2
4 [42] Maria-Elena Nilsback and Andrew Zisserman. Automated
[27] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh, flowerclassificationoveralargenumberofclasses. InPro-
HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, andTom ceedingsofthe2008SixthIndianConferenceonComputer
Duerig. Scaling up visual and vision-language representa- Vision,Graphics&ImageProcessing,page722–729,USA,
tion learning with noisy text supervision. In International 2008.IEEEComputerSociety. 4,6
conferenceonmachinelearning,pages4904–4916.PMLR, [43] AaronvandenOord, YazheLi, andOriolVinyals. Repre-
2021. 2 sentationlearningwithcontrastivepredictivecoding. arXiv
[28] Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yan- preprintarXiv:1807.03748,2018. 3
nisAvrithis,AndreiBursuc,KonstantinosKarantzalos,and [44] OpenAI. Gpt-4technicalreport,2023. 6,7,12
Nikos Komodakis. What to hide from your students:
[45] LongOuyang, JeffreyWu, XuJiang, DiogoAlmeida, Car-
Attention-guidedmaskedimagemodeling. InComputerVi-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
sion–ECCV2022,pages300–318.SpringerNatureSwitzer-
Agarwal,KatarinaSlama,AlexRay,JohnSchulman,Jacob
land,2022. 2
Hilton,FraserKelton,LukeMiller,MaddieSimens,Amanda
[29] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Askell, Peter Welinder, Paul F Christiano, Jan Leike, and
Toutanova. Bert: Pre-training of deep bidirectional trans- RyanLowe.Traininglanguagemodelstofollowinstructions
formers for language understanding. In Proceedings of with human feedback. In Advances in Neural Information
naacL-HLT,page2,2019. 2 ProcessingSystems,pages27730–27744.CurranAssociates,
[30] AlexKrizhevsky. Learningmultiplelayersoffeaturesfrom Inc.,2022. 6,7,12
tinyimages. Technicalreport,2009. 4 [46] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
[31] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
layersoffeaturesfromtinyimages. 2009. 6 banDesmaison, LucaAntiga, andAdamLerer. Automatic
[32] LAION-AI. Clipbenchmark. https://github.com/ differentiationinpytorch. 2017. 3
LAION-AI/CLIP_benchmark,2023. 6 [47] ZhiliangPeng,LiDong,HangboBao,QixiangYe,andFuru
[33] GangLi,HeliangZheng,DaqingLiu,ChaoyueWang,Bing Wei.Beitv2:Maskedimagemodelingwithvector-quantized
Su,andChangwenZheng.Semmae:Semantic-guidedmask- visualtokenizers.arXivpreprintarXiv:2208.06366,2022.2
ingforlearningmaskedautoencoders. AdvancesinNeural [48] AlecRadford,JongWookKim,ChrisHallacy,A.Ramesh,
InformationProcessingSystems,35:14290–14302,2022. 1, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
2 Askell,PamelaMishkin,JackClark,GretchenKrueger,and
[34] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, IlyaSutskever.Learningtransferablevisualmodelsfromnat-
Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. urallanguagesupervision. InICML,2021. 4
Alignbeforefuse:Visionandlanguagerepresentationlearn- [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
ingwithmomentumdistillation. Advancesinneuralinfor- Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
mationprocessingsystems,34:9694–9705,2021. 2 AmandaAskell,PamelaMishkin,JackClark,etal.Learning
[35] XianhangLi,ZeyuWang,andCihangXie.Clipa-v2:Scaling transferable visual models from natural language supervi-
cliptrainingwith81.12 sion.InInternationalconferenceonmachinelearning,pages
[36] YanghaoLi,HaoqiFan,RonghangHu,ChristophFeichten- 8748–8763.PMLR,2021. 2,3,6
hofer,andKaimingHe.Scalinglanguage-imagepre-training [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
viamasking,2023. 1,2,4,6 Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[37] Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong AmandaAskell,PamelaMishkin,JackClark,etal.Learning
Zhu,ChaoyangZhao,RuiDeng,LiweiWu,RuiZhao,Ming transferable visual models from natural language supervi-
Tang, et al. Mst: Masked self-supervised transformer for sion.InInternationalconferenceonmachinelearning,pages
visualrepresentation. AdvancesinNeuralInformationPro- 8748–8763.PMLR,2021. 2
cessingSystems,34:13165–13176,2021. 2 [51] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
[38] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, VaishaalShankar. Doimagenetclassifiersgeneralizetoim-
Pietro Perona, Deva Ramanan, Piotr Dollar, and Larry Zit- agenet?,2019. 4,6
nick.Microsoftcoco:Commonobjectsincontext.InECCV. [52] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
EuropeanConferenceonComputerVision,2014. 2,4,5,6 Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo
[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle- man,PatrickSchramowski,SrivatsaRKundurthy,Katherine
moyer,andVeselinStoyanov.Roberta:Arobustlyoptimized Crowson,LudwigSchmidt,RobertKaczmarczyk,andJenia
bertpretrainingapproach.arXivpreprintarXiv:1907.11692, Jitsev. LAION-5b: An open large-scale dataset for train-
2019. 2 ingnextgenerationimage-textmodels. InThirty-sixthCon-
[40] StuartLloyd.Leastsquaresquantizationinpcm.IEEEtrans- ferenceonNeuralInformationProcessingSystemsDatasets
actionsoninformationtheory,28(2):129–137,1982. 3 andBenchmarksTrack,2022. 4[53] JianboShiandJitendraMalik. Normalizedcutsandimage [68] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin
segmentation. IEEE Transactions on pattern analysis and Bao,ZhuliangYao,QiDai,andHanHu.Simmim:Asimple
machineintelligence,22(8):888–905,2000. 1 frameworkformaskedimagemodeling. InProceedingsof
[54] YugeShi,NSiddharth,PhilipTorr,andAdamRKosiorek. theIEEE/CVFConferenceonComputerVisionandPattern
Adversarialmaskingforself-supervisedlearning. InInter- Recognition,pages9653–9663,2022. 2
national Conference on Machine Learning, pages 20026– [69] FengyuYang,ChaoFeng,ZiyangChen,HyoungseobPark,
20040.PMLR,2022. 2 DanielWang,YimingDou,ZiyaoZeng,XienChen,RitGan-
[55] AnugyaSrivastava,ShriyaJain,andMugdhaThigle. Outof gopadhyay, AndrewOwens, etal. Bindingtouchtoevery-
distributiondetectiononimagenet-o,2022. 4,5,6 thing: Learning unified multimodal tactile representations.
arXivpreprintarXiv:2401.18084,2024. 2
[56] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
[70] Yifan Yang, Weiquan Huang, Yixuan Wei, Houwen Peng,
Wei,andJifengDai. Vl-bert:Pre-trainingofgenericvisual-
Xinyang Jiang, Huiqiang Jiang, Fangyun Wei, Yin Wang,
linguisticrepresentations. arXivpreprintarXiv:1908.08530,
Han Hu, Lili Qiu, and Yuqing Yang. Attentive mask clip,
2019. 1,2
2022. 1,2,4,5,6
[57] Hao Tan and Mohit Bansal. Lxmert: Learning cross-
[71] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-
modalityencoderrepresentationsfromtransformers. arXiv
maier. Fromimagedescriptionstovisualdenotations: New
preprintarXiv:1908.07490,2019. 1,2
similaritymetricsforsemanticinferenceovereventdescrip-
[58] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
tions. Transactions of the Association for Computational
Massa,AlexandreSablayrolles,andHerve´ Je´gou. Training
Linguistics,2:67–78,2014. 4,5,6
data-efficient image transformers & distillation through at-
[72] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,Mo-
tention. In International conference on machine learning,
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
pages10347–10357.PMLR,2021. 2
captionersareimage-textfoundationmodels. arXivpreprint
[59] AaronVanDenOord, OriolVinyals, etal. Neuraldiscrete
arXiv:2205.01917,2022. 2
representationlearning.Advancesinneuralinformationpro-
[73] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
cessingsystems,30,2017. 2
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
[60] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko- Boxin Li, Chunyuan Li, et al. Florence: A new
reit,LlionJones,AidanN.Gomez,LukaszKaiser,andIllia foundation model for computer vision. arXiv preprint
Polosukhin. Attentionisallyouneed. 2017. 3 arXiv:2111.11432,2021. 2
[61] AndreaVedaldi. Catsanddogs. InProceedingsofthe2012 [74] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,
IEEEConferenceonComputerVisionandPatternRecogni- Dan Jurafsky, and James Zou. When and why vision-
tion(CVPR),page3498–3505,USA,2012.IEEEComputer language models behave like bags-of-words, and what to
Society. 4,6 do about it? In The Eleventh International Conference on
[62] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P LearningRepresentations,2022. 6
Xing. Learningrobustglobalrepresentationsbypenalizing [75] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang
localpredictivepower. InAdvancesinNeuralInformation Xie,AlanYuille,andTaoKong.ibot:Imagebertpre-training
ProcessingSystems,pages10506–10518,2019. 4,5,6 with online tokenizer. arXiv preprint arXiv:2111.07832,
[63] YiqingWang,ZihanLi,JieruMei,ZihaoWei,LiLiu,Chen 2021. 2
Wang,ShengtianSang,AlanYuille,CihangXie,andYuyin
Zhou. Swinmm:Maskedmulti-viewwithswintransformers
for3dmedicalimagesegmentation. InMICCAI,2023. 2
[64] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan
Yuille,andChristophFeichtenhofer. Maskedfeaturepredic-
tionforself-supervisedvisualpre-training.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages14668–14678,2022. 2
[65] Zihao Wei, Chen Wei, Jieru Mei, Zeyu Wang, Xianhang
Li,HuiyuWang,AlanYuille,YuyinZhou,andCihangXie.
MAEaresecretlyefficientlearners,2023. 2
[66] Alex Wilf, Syeda Nahida Akter, Leena Mathur, Paul Pu
Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, and
Louis-Philippe Morency. Difference-masking: Choosing
what to mask in continued pretraining. arXiv preprint
arXiv:2305.14577,2023. 2
[67] Jiahao Xie, Wei Li, Xiaohang Zhan, Ziwei Liu, Yew Soon
Ong, and Chen Change Loy. Masked frequency model-
ing for self-supervised visual pre-training. arXiv preprint
arXiv:2206.07706,2022. 2A. Qualitative Comparison of Masking Strat- Attn.Init. Attn.E16 Attn.E32 Ours
egy
Our method outperforms a random masking strategy by
preserving more semantic content in the unmasked image
patches, a comparison showcased in Figure 1. The advan-
tage of our technique is underscored by the captioning ex-
periment detailed in Figure7, whereintwo setsof images,
each masked differently, were fed into a captioner, GPT-
4 [44, 45]. The model was tasked to generate MSCOCO-
style captions for the unobscured sections. When compar-
ing these captions to the standard references, it becomes
clear that our cluster based masking not only retains key
depicted elements but also the interrelations among them. Figure8. Visualizationofattention-basedmasking. Theimages
Forinstance, ourapproachaccuratelyenabledthecaption- arethesamefromFigure1.
ing system to identify an airplane in the first example and
todescribethebaseballplayer’sactioninthesecond,while
C.ClusteringVisualization
the random masking strategy failed to achieve this clarity.
These results indicate that our masking method provides a We provide more examples of our clustering masking vi-
moredetailedcomprehensionoftheimage. sualization onCOCO and CC3Mdatasets onFigure 9 and
Figure 10 respectively. We mask out at least 50% patches
ineachimage.
Reference RandomMask OurMethod
Twoplanesflying Aclearsky Jetsflying
inthesky abovean overan
overabridge. archbridge. archbridge.
Aboycatches Playersona Abaseballplayer
aballasa baseballfield slidingintohome
playerslides duringagame. plateduringagame.
tothebase.
Figure7. Generatedcaptionfromvisiblepatches. Weprocess
themaskedimagesthroughGPT-4[44,45]tocreatecaptionsfor
theunmaskedsegments.
B.Visualizationofattention-basedmasking
We extend Figure 1 with examples from the attention-
guidedbaseline(Figure8). IncontrasttoourRGBmodel,
the behavior of the attention-based method changes dur-
ing training. In early iterations, it masks randomly, while
laterintrainingitproducesfairlyconsistentclustersthatdo
notvarymuchbetweeniterations,sincetheattentionmaps Figure9.IllustrationofclusterbasedmasksonCOCO.
change less over time, potentially limiting the diversity of
trainingexamples.Figure10.IllustrationofclusterbasedmasksonCC3M.