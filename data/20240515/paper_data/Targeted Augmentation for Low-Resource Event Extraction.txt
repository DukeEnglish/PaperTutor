Targeted Augmentation for Low-Resource Event Extraction
SijiaWang, LifuHuang
VirginiaTech
{sijiawang,lifuh}@vt.edu
Abstract Zou, 2019) or through back translation (Edunov
etal.,2018),contributeminimallytodistribution
Addressingthechallengeoflow-resourceinfor-
enrichment,whiledrasticaugmentationscanlead-
mationextractionremainsanongoingissuedue
ingtomisguidedacquisitions(Caoetal.,2015;Gao totheinherentinformationscarcitywithinlim-
itedtrainingexamples. Existingdataaugmen- etal.,2022). Drasticaugmentationsusuallyunder-
tationmethods,consideredpotentialsolutions, mine existing event structure, resulting in gram-
struggletostrikeabalancebetweenweakaug- matical incorrectness, structure misalignment, or
mentation (e.g., synonym augmentation) and
semanticdrifting(Wangetal.,2023a).
drasticaugmentation(e.g.,conditionalgenera-
In this work, we explore several dimensions
tionwithoutproperguidance).Thispaperintro-
ducesanovelparadigmthatemploystargeted
fordataaugmentation,includingdiversity,polar-
augmentationandbackvalidationtoproduce ity,accuracy,andcoherence. Ourfocusrevolves
augmentedexampleswithenhanceddiversity, around enhancing diversity in the context of tar-
polarity,accuracy,andcoherence. Extensive getedaugmentationforlow-resourceeventextrac-
experimentalresultsdemonstratetheeffective-
tion(TALOR-EE).Thisinvolvesenrichingevent
nessoftheproposedparadigm. Furthermore,
structureswithentitiesdrawnfromatargetedsub-
identifiedlimitationsarediscussed, shedding
set(Gaoetal.,2022). Simultaneously,weaddress
lightonareasforfutureimprovement1.
the issue of polarity by not only generating posi-
1 Introduction tiveeventmentionsbasedonactualoccurrencesbut
alsoincorporatingnegativeeventmentions,e.g.,hy-
Eventextraction(EE) (Grishman,1997;Chinchor
potheticaleventmentions(LinguisticDataConsor-
and Marsh, 1998; Ahn, 2006) is the task of iden-
tium,2005). Thisapproachisparticularlyvaluable
tifyingandcategorizingeventmentionsinnatural
forovercominglimitationsingenerativeeventex-
languagetext. Whilesupervisedmethodsdeliver
tractionmodels(Hsuetal.,2022;Liuetal.,2022).
impressive performance, they depend heavily on
Toensurebothaccuracyandcoherenceinourgen-
extensive manual annotations (Chen et al., 2020;
eratedcontent,weintroduceaback-and-forthvali-
Du and Cardie, 2020; Lin et al., 2020; Liu et al.,
dationmoduleBACK-VALIDATION. Therationale
2020; Li et al., 2020a; Lyu et al., 2021). Gener-
behindthismoduleisthatanaccurategeneration
alizingtheseapproachestolow-resourcelearning
shouldalignwiththegiveneventstructure,while
settingposeschallenges(PasupatandLiang,2014;
coherent generation should seamlessly integrate
Huangetal.,2016;HuangandJi,2020;Laietal.,
withthesamestructure.
2020b;Shenetal.,2021b;Lyuetal.,2021;Zhang
etal.,2021b;Wangetal.,2023b). Our research encompasses a series of compre-
Dataaugmentationisonedirectionforefficiently hensiveexperimentsconductedacrossvariouslow-
addressingthelow-resourceeventextractionprob- resource learning scenarios, including zero-shot
lem. However, it’s remained unexplored what andfew-shotlearningsettings. Theseexperiments
dataaugmentationstrategiesarethebestforlow- span different event extraction models. The out-
resource event extraction given its unique chal- comesoftheseexperimentsconsistentlyhighlight
lenges. Previousstudiesshowthatweakaugmen- theeffectivenessoftargetedaugmentationinlow-
tations,suchassynonymaugmentation(Weiand resourceeventextraction. Notably,amongallthe
dimensionsinvestigated,diversityemergesasthe
1The source code, model checkpoints, and data are
mostcrucialfactor. Additionally,wemeticulously
publicly available at https://github.com/VT-NLP/
TALOR-EE. scrutinize the quality of the generated sentences,
4202
yaM
41
]LC.sc[
1v92780.5042:viXranow it 's up to the appeals court and the board of pardon Original Annotated
and paroles to officially clear their names. Sentence Talor-EE
board of board of
court pardon and court pardon and
(i) The court cleared Paul
paroles paroles
Adj Adj Paul Adj Adj Laxalt, as advised by the board
None Laxalt Generation of pardon and paroles.
Def Place Def Place Agent (ii) The court refused
None Nevada 1988 to clear Paul Laxalt in 1988 ...
clear clear
Generated
Original Event Augmented
Sentences
Structure Structure
Generation
PER Prompts Back-Validation
Paul TIME
Sample 1988
Laxalt
clear GPE
Nevada Retrieved
External Corpus
Context
Generated The court cleared Paul Laxalt, as advised by the board of pardon and paroles.
examples The court refused to clear Paul Laxalt in 1988, as advised by the board of pardon and paroles.
The court would clear Paul Laxalt if he behaved well in the past two years, as advised by
the board of pardon and paroles.
Figure1: TALOR-EEframeworkoverview.
shedding light on the limitations inherent in the 2023)haveexploredin-contextlearningbyprovid-
proposedframework. ing task instructions and a handful of in-context
Thecontributionsofthisworkareasfollows: examples. Nevertheless, their experimental find-
ings reveal a notable performance gap between
• Weexploretheapplicationofdataaugmenta-
in-contextlearningandapproachesbasedonfine-
tiontechniquesforlow-resourceeventextrac-
tuning.
tion.
DataAugmentation createssyntheticdatafrom
• We develop a novel augmentation method
theexistingdata. Traditionaldataaugmentationap-
thatincorporatesenrichedeventstructuresand
proachesfocusonexpandinglexicaldiversity (Wei
contextualentities,retrievedfromexternalcor-
andZou,2019;Fengetal.,2020;Ngetal.,2020)
pus. The generated examples are validated
orsyntaxvariation(Kimetal.,2022;Loemetal.,
through a back-validation module, ensuring
2022; Hussein et al., 2022; Wang et al., 2023a).
accuracyandcoherence.
Postselection(Yangetal.,2020)orrepresentative
selection(Edwardsetal.,2021)helpstopreventa
• Comprehensive experiments are conducted
wasteofresourcesandtimeingeneratingnewdoc-
to assess the effectiveness of the proposed
uments. Yetexistingaugmentationmethodssuffer
paradigmacrossvariousmodelsanddatasets.
from gradual drift problem (Hu et al., 2021a,b).
2 RelatedWork The previous work (Ma et al., 2023) utilizes lan-
guagemodelsfortrainingdatasynthesisbutlacks
Low-resourceEventExtraction Althoughsome
assuranceinthesoundnessandnaturalnessofevent
studieshaveemployedmeta-learning(Kangetal.,
structuresduetotherandomcombinationofsam-
2019;Lietal.,2021;XiaoandMarlet,2020;Yan
pledtriggersandarguments. Additionally,itfalls
et al., 2019; Chowdhury et al., 2021), or metric
short by primarily relying on the self-reflection
learning (Sun et al., 2021; Wang et al., 2020a;
capabilityoflanguagemodels,withoutfullylever-
Zhangetal.,2021a;Agarwaletal.,2021)toalign
aging annotations for existing event annotations.
candidateeventsemanticswithafewexamplesof
Thus,inadditiontothelexicalandsyntacticaldi-
noveleventtypesforfew-shoteventdetection,their
versity,weleveragethelarge-scalepre-trainedau-
performanceisinherentlyconstrainedbythelim-
toregressivemodelstogeneratecontextuallydiver-
ited examples provided (Lai et al., 2020a; Deng
sifiedfreetexts.
et al., 2020; Lai et al., 2020b; Cong et al., 2021;
Chenetal.,2021;Shenetal.,2021b). Recentstud- ControlledTextGeneration approaches(Ghosh
ies (Wei et al., 2023; Han et al., 2023; Li et al., etal.,2021)generatetextwithspecificconstraint.
eveirteR
"raelc"Approaches thatpromote similarity (Guan etal., ouswork,wesetN = 5,10andK = 0,1,5,10in
2021)orcoherence(Shenetal.,2021a;Wangetal., thiswork.
2021a)towardstheoriginalsentenceslackcontex-
tual diversity and might produce over-confident 3.2 TargetedAugmentation[Diversity]
probabilityestimation (Wangetal.,2021a;Gowda
In contrast to previous data augmentation ap-
andMay,2020). Rule-basedconstraintgeneration
proaches (Wei and Zou, 2019; Feng et al., 2020;
might generate meaningless tokens to meet con-
Ngetal.,2020;Kimetal.,2022;Loemetal.,2022;
straints(Wangetal.,2021b),whiletemplate-based
Husseinetal.,2022;Wangetal.,2023a),wehave
constraintgeneration(CaoandWang,2021)isdif-
improved upon the conventional conditional gen-
ficulttogeneralizetonewdomainswithouthuman
erationmethodbytransitioningfromrandomsam-
effort.
plingtoatargetedselectionstrategy. Thetargeted
augmentation module serves as a mechanism to
Learning with noisy labels Many works learn
ensure diversity. Theoretically, it can retrieve an
withnoisylabelsbydetectingcorruptedinstances,
infinite number of entities from the external cor-
e.g.,(Hanetal.,2018;Yuetal.,2019;Huangetal.,
pus, seamlessly incorporating these entities into
2019;Yaoetal.,2020;Weietal.,2020;Jiangetal.,
thegiveneventstructure. Consequently,themod-
2020;Zhangetal.,2021c),andtheirapplicationto
ule can generate an infinite variety of new event
low-resourcelearningsetting(Wangetal.,2020b;
structures. Thus, the targeted augmentation pro-
Li et al., 2020b; Cheng et al., 2021). However,
vides a theoretical framework for sampling and
jointtrainingofthesampleselectionmoduleand
augmentinganextensivearrayofentities,particu-
thetargettaskmodeltakesconsiderableiterations
larlybeneficialwhenworkingwithalimitedsetof
toconverge. Traditionaldata-centricmethods(Zhu
annotatedeventmentions.
et al., 2022) face limitations in low-resource set-
tings due to biased neighbor information. This
DependentContextRetrieval Foragivenevent
studydemonstratesthattrainingwithrelativelyfair-
structure,weretrievecontextcandidatesfromthe
qualitylabelscanbeeffective.
corpusthatsharetokenswiththeeventstructure. In
ourexperiments,wegatheredsentencescontaining
3 Model
thementionoftheeventtrigger. Toextractcontext
information from the sampled sentences, we uti-
3.1 ProblemFormulation
lizedthespaCyNamedEntityRecognition(NER)
Givena sentence, theEvent Extraction (EE)task parser2 toidentifyentitymentions. Consequently,
aimstoextracteventmentions,representedbyan
the extracted entity mentions from each sampled
event trigger and a set of event arguments. For-
sentenceserveascontextcandidatesforthegiven
mally, given a sentence w = {w ,...,w }, and a
1 n event structure. The context corpus employed in
targeteventtypee i,ifthereisaneventoccurrence thisstudyistheNYTAnnotatedCorpus3.
ofe inw,aEEsystemaimstoextractaneventtrig-
i
gertanditsargumentmentionsa = {a ,...,a }. TargetedGeneration Givenaneventstructure
1 g
Inthiswork,wefocusonzero-shotandfew-shot e = {t ,a ,...,a }andasampledcontextcandi-
i i 1 p
learningsettingsofEE.Forfew-shotEE(FSEE), date c = {c ,...,c }, a generator is leveraged to
1 q
trainingdatacontainstwoparts: (1)Alarge-scale generateacorrespondingsentence. Ifthesampled
data set D = {(x ,y )}M that covers the contextentitiescouldpotentiallyserveasargument
base i i i=1
seeneventtypes(namedbasetypes),whereM de- rolesintheoriginaleventstructures,weemployan
notesthenumberofbaseeventtypes;(2)asmaller add-or-replacestrategy,tofurthertailortheevent
data set D = {(x ,y )}N×K that covers N structure. The feasibility of integrating an entity
novel j j j=1
novel event types, with K examples each. Note intotheeventstructuredependsonitsentitytype. If
that the base and novel event types are disjoint theargumentroleisvacantintheoriginalstructure,
exceptfortheOtherclass,indicatingnon-event andtheentitytypeofthesampledentityalignswith
type. In zero-shot event extraction (ZSEE), the the argument role, we add the entity to the event
training data set only contains a large-scale set
D = {(x ,y )}M for the base event types. 2https://spacy.io/usage/
base i i i=1 linguistic-features
Themodelf willbeoptimizedonbaseeventtypes
3https://catalog.ldc.upenn.edu/
andevaluatedonthenoveltypes. Followingprevi- LDC2008T19The court <Adjudicator> in Nevada <Place> clear clear
Paul Laxalt , as advised by the board
<Defendant>
of pardon and paroles <Adjudicator>. NLI Def Adj Adj Place
Paul board of
Place clear Laxalt court pardon and None
Nevada Event Trigger is [clear]. [Paul paroles
Laxalt] received a pardon from
Event Trigger is [clear]. [Paul
[court] and [board of pardon and
Def Adj Adj Laxalt] received a pardon from
paroles] in [Nevada].
Paul board [court] and [board of pardon and
court
Laxalt of pardon and paroles paroles] in [an unspecific place].
NLI The court clear Paul Laxalt, as NLI
Figure2: Eventmentionaccuracyverificationmodule.
advised by the board of pardon
and paroles.
structure. Iftheargumentroleisalreadypopulated, Figure3: Eventmentioncoherenceverificationmodule.
wesubstituteitwiththesampledentity.
For example, given an annotation on the sen-
tence "now it ’s up to the appeals or never, or a negative lexical context such deny,
court and the board of pardon and refuseordisobey,(2)assertedmentions: including
paroles to officially clear their hypotheticalevents,believedevents,orpromised
names.", a Justice:Pardon event is represented events,etc(LinguisticDataConsortium,2005).
bytheeventstructure{Trigger: clear,Adjudicator: Thus in addition to augmenting high-quality
court,Adjudicator: boardofpardonandparoles}. positive training examples, particular attention is
A complete Justice:Pardon structure may also paid to augmenting negative training examples.
includetwoargumentroles,namelyDefendantand In this work, we write negative/asserted expres-
Place. Fromthesampledcontextentities[Paul sion prompts to guide their generation. Prompts
Laxalt, 1988, Nevada], Nevada is andgeneratednegativeaugmentationexamplesare
addedtotheeventstructureasanPlacerole,and listedinTable6andTable7inAppendixB.
Paul Laxalt is added as a Defendant role.
Notethat"Nevada"isaddedbecauseitisaGPE
3.4 Back-Validation
entity and a GPE entity is one of the possible
entity types for a Place role. Similarly, Paul Givennoisytrainingexamples,previousresearch
Laxalt is added as a Defendant because it is a hasutilizedmethodstodetectandrectifycorrupted
PERentity. Herewepresentageneratedsentence data during training (Han et al., 2018; Yu et al.,
withtheenrichedeventstructure: "The court 2019; Huang et al., 2019; Yao et al., 2020; Wei
in Nevada clear Paul Laxalt, as etal.,2020;Jiangetal.,2020;Zhangetal.,2021c),
advised by the board of pardon butsuchapproachesnecessitateextensivetraining.
and paroles." The process is illustrated in Inourcontext,wherethegenerateddataisconsid-
Figure1. eredofreasonablequality,weproposetheincorpo-
rationofaback-and-forthvalidationmodule. This
3.3 NegativeAugmentation[Polarity] moduleaimstoensuretheaccuracyandcoherence
Polarity is maintained through the negative aug- of the generated content, thereby enhancing the
mentationdesign. Thisprocessgeneratesnotonly reliabilityoftheaugmentedexamples.
positiveeventmentionsbutalsonegativementions,
includinghypotheticalmentionsandbelievedevent Event Mention Accuracy Verification [Accu-
mentions. Foreventextraction,wefocusonidenti- racy] Foreachgeneratedexample,itsaccuracy
fyingeventthatoccurs,andalsonegativementions. canbeverifiedthroughanentailmentverification
For example, in the sentence “John Hinkley de- module. AsshowninFigure2,giventhegenerated
niedhisattempttoassassinateRonaldReagan.”, sentenceanditssourceeventstructure,wefirsttex-
amodel,especiallygenerativemodels,mightover- tualizetheeventstructureintoapassagetoexpress
lookthisConflict:Attackmentiontriggeredbythe theeventstructure,byapre-definedtemplate(Hsu
token assassinate, because this is not an actual et al., 2022). Then the two texts will be passed
event that happens. More specifically, negative into an NLI entailment verification module. The
eventmentionsinclude(1)explicitnegativemen- intuition is that, for a valid generation, it should
tions: expressedwithanegativewordsuchasnot entailthetemplatepassagewiththeeventstructure.EventMentionCoherenceVerification[Coher- Algorithm1RobustFine-tuning
ence] In addition to ensuring generation accu- Input: Base data set D ; few shot training set D ;
base novel
synthesizedtrainingsetD .
racy,weaimforthegeneratedsentencetoexhibit gen
Output:ModelM,validatorV
strong coherence with the provided event struc-
fine-tune V with back-validation data constructed from
ture. Specifically, there should be no extraneous D
train
oromittedargumentswhencomparedtothegiven passD genintoV,collectD g′ enthatpassback-validation
foreachepochtdo
event structure. The intuition is that if the gener- SamplemetabatchDt fromD
base base
atedsentencealignscoherentlywiththeprovided SamplenoisybatchDt fromD′
gen gen
eventstructure, a templatepassage incorporating UpdatemodelM withD tt rain,D novel,andD gt en
Discardcorrupteddatabysemanticdistancetothecen-
theeventstructureshouldentailthegeneratedsen-
terinstances
tence,andviceversa. Adistinctivescenarioarises endfor
when the event structure is incomplete. In such
instances, we adapt the missing argument role in
Model Time/Sentence(s) Cost/Sentence($)
thetemplatewiththeexpression"anunspecific[ar-
Vicuna-7B 2.7 0
gumentrole]."IllustratedinFigure3,ifthePlace
LLaMA2-7B 8.7 0
argument role is absent, we want to ensure that
GPT-3.5-turbo 2.4 ∼0.0035
thegeneratedeventmentiondoesnotintroducean
extraneous arbitraryPlace argumentrole. Conse-
Table1: Augmentationcostpersentence.
quently, we substitute "[Place]" with "[an unspe-
cific Place]." This modification ensures that the
generatedsentencefailstheforward-and-backward
servesasaplaceholdercorrespondingtoanargu-
entailmenttestinsuchscenarios.
ment role for a Justice:Fine event. For example,
"somewhere"correspondstothePlacewherethe
3.5 GenerativeEventExtractionModel
eventoccurs. Notethateveryeventtypehasitsown
argumenttemplate. Eventextractiontemplatesand
DEGREE(Hsuetal.,2022)isagenerativeevent
theconstructiondetailscanbefoundin(Hsuetal.,
extractionmodelthatconceptualizeseventextrac-
2022).
tion as a conditional generation problem. Given
asentenceandacraftedprompt,DEGREEgener-
3.6 RobustFine-tuning
ates an output following a specified format. The
predictions for event triggers and argument roles GiventhesynthesizedtrainingsamplesD that
gen
can be then parsed from the generated output us- augmentD forfine-tuningaclassificationM.
train
ingadeterministicalgorithm. Incontrasttoearlier Theprimaryconcernisthepresenceoflabelnoise,
classification-basedmodels,thegenerationframe- where some generated samples may inaccurately
workoffersaversatileapproachtoincorporatesup- align with their corresponding labels, potentially
plementaryinformationandguidance. Throughthe degradingmodelperformancewhenusingstandard
creationofsuitableprompts,DEGREEcanbetter supervisedlearning. Toaddressthischallenge,we
capturethedependenciesbetweenentitiesand,con- employ a noise-robust training procedure to en-
sequentlydiminishtherequisitenumberoftraining hancestability. Wefirstfine-tunetheback-validator
examples. V withthetrainingdataconstructedfromthebase
TheEEtemplatedefinestheanticipatedoutput dataset. Fornegativeexamples,weconstructtwo
formatandisorganizedintotwomainparts. The datasets: (1)sampleunpairedeventstructuresand
initialsegmentisreferredtoasthetriggertemplate, sentences within the corpus and (2) replace argu-
structured as “Event trigger is <Trigger>”, with mentrolesinthetemplatewith"anunspecific[ar-
“<Trigger>”actingasaplaceholderforeventtrig- gument role]". Then we validate the augmented
gerintheoriginalpassage. Thesubsequentsection exampleswiththefine-tunedvalidatorV,andval-
istheargumenttemplate,anditscompositionvaries idatedexamplesarethenusedforfine-tuningthe
basedonthespecificeventtype. Forinstance,the EEmodelM. Finally,weemployarandomsample
argument template for a Conflict:Attack event is selectiononthebasedatasetD andthesynthe-
base
“somepeople or someorganization in somewhere sizedtrainingsetD , alongwiththeentirefew
gen
was ordered by someadjudicator to pay a fine.” shottrainingsetD toupdatetheEEmodelM.
novel
Each underlined string, beginning with "some-," ThealgorithmisshowninAlgorithm1.Common5 Common10
Method K-shot
Tri-I Tri-C Arg-I Arg-C Tri-I Tri-C Arg-I Arg-C
MatchingBaseline full 42.7 42.1 - - 46.3 46.3 - -
LemmatizationBaseline full 51.5 50.2 - - 56.0 56.0 - -
OneIE full 72.7 70.5 52.3 49.9 74.5 73.0 51.2 48.9
DEGREE full 68.4 66.0 51.9 48.7 72.0 69.8 52.5 49.2
1-shot 10.0 1.4 1.3 1.3 8.2 1.6 1.1 1.1
5-shot 14.0 12.6 11.1 10.8 20.8 15.4 14.6 13.9
BERT_QA
10-shot 37.8 11.3 22.9 22.1 32.0 27.8 19.5 18.6
1-shot 4.2 4.2 1.5 1.5 4.1 2.7 2.0 2.0
5-shot 39.3 38.5 24.8 22.8 41.9 41.9 29.7 27.2
OneIE
10-shot 54.8 53.3 36.0 34.9 61.5 57.8 41.4 39.2
0-shot 53.3 46.8 29.6 25.1 60.9 54.5 42.0 31.4
1-shot 60.1 53.3 38.8 31.6 61.2 60.9 41.1 34.7
DEGREE
5-shot 57.8 55.5 40.6 36.1 65.8 64.8 45.3 42.7
10-shot 63.8 61.2 46.0 42.0 72.1 68.8 52.5 48.4
0-shot 66.1 62.3 38.7 32.9 71.6 68.7 40.7 35.9
1-shot 63.5 55.7 37.5 32.0 69.2 64.5 47.8 43.2
TALOR-EE(Vicuna)
5-shot 67.0 65.2 46.6 43.1 72.7 70.0 50.1 44.9
10-shot 70.4 66.2 46.4 42.7 73.9 71.7 49.2 44.9
0-shot 65.0 62.5 41.0 36.5 65.6 64.8 47.5 43.8
1-shot 66.5 61.0 42.3 34.4 71.5 66.7 45.4 42.4
TALOR-EE(LLaMA)
5-shot 70.2 63.9 46.3 42.4 71.7 70.1 50.5 46.7
10-shot 70.0 67.6 46.2 43.3 70.5 70.2 51.2 49.5
0shot 67.9 66.1 46.1 40.0 72.5 70.3 46.9 42.8
1-shot 68.5 64.8 42.1 35.6 72.5 68.1 46.5 42.8
TALOR-EE(GPT)
5-shot 67.9 64.2 44.6 42.6 73.6 70.6 48.5 44.7
10-shot 70.2 67.4 43.0 41.4 74.2 70.5 48.3 47.7
Table2: Low-resourceEEresultsonACE05-E.Boldrepresentsthehighestscoreforthecurrentsetting.
4 Experiments 2022). Theimplementationdetailscanbefoundin
AppendixA.
We perform experiments on three public bench-
mark datasets, including ACE05-E (Automatic Generation Agents Three generation agents
Content Extraction)4 and ERE (Entity Relation are experimented in this work, including
Event) (Song et al., 2015). To showcase the ef- vicuna-7b-v1.3 (Vicuna), Llama-2-7b
fectivenessoftheproposedmethodunderlowre- (LLaMA), and gpt-3.5-turbo (GPT). For
sourcesettings,experimentsareconductedunder eachagent,welisttheaugmentationcostinTable
Nway-Kshotlearningsetting,whereN ∈ {5,10}, 1,wheretwofactorsarelistedincludinggeneration
andK ∈ {0,1,5,10}. timeandcostpersentence.
Comparedbaselines Weconsiderthefollowing
4.1 Mainresults
baselines: (1)Matchingbaseline5,aproposedbase-
linethatmakestriggerpredictionsbyperforming The experimental results for low-resource Event
stringmatchingbetweentheinputpassageandthe Extraction(EE)arepresentedinTable2andFig-
event keywords. (2) Lemmatization baseline, an- ure4forACE05-E,andTable3andFigure5for
otherproposedbaselinethatperformsstringmatch- ERE, respectively. From the experiment results,
ing on lemmatized input passage and the event severalconclusionscanbedrawn: (1)Withtheaug-
keywords. (3) BERT_QA(Du and Cardie, 2020), mentedexamples,theperformanceoflow-resource
(4) OneIE (Lin et al., 2020), (5) DEGREE (Hsu EEgenerallyexhibitsimprovement,evidentinboth
et al., 2022) and (6) QueryExtract (Wang et al., zero-shotlearningandfew-shotlearningsettings.
Thisimprovementisconsistentacrossdifferentgen-
4https://catalog.ldc.upenn.edu/
eration agents (Vicuna, LLaMA, and GPT) and
LDC2006T06
backboneEEmodels. Table8displaysexperimen-
5(1)and(2)arebaselinesforeventdetectiontasks,thus
onlytriggerdetectionresultsarereported. talresultsonACE05-EwithQueryExtractastheFigure4: ExperimentalresultsonACE05-E.(a-b)arevisualizationsforCommon5,and(c-d)forCommon10.
Common5 Common10
Method K-shot
Tri-I Tri-C Arg-I Arg-C Tri-I Tri-C Arg-I Arg-C
DEGREE full 54.7 53.1 45.4 44.7 58.8 58.2 51.3 50.8
0-shot 32.2 26.8 16.1 15.5 47.7 45.4 28.7 28.0
1-shot 34.4 33.8 28.0 26.2 39.4 39.4 30.7 29.9
DEGREE
5-shot 44.8 39.2 28.9 28.7 56.3 55.5 44.5 42.7
10-shot 48.4 45.8 39.3 38.8 59.3 57.8 48.4 47.8
0-shot 41.9 40.2 31.0 28.9 50.6 49.0 37.9 36.6
1-shot 48.5 38.7 31.3 30.4 47.8 41.6 35.9 34.8
TALOR-EE(Vicuna)
5-shot 45.8 43.0 35.8 33.4 56.2 53.7 42.5 41.0
10-shot 55.7 52.0 40.6 37.6 58.2 56.7 47.8 44.9
0-shot 40.8 34.7 26.2 23.8 51.6 45.4 37.8 36.4
1-shot 47.4 39.1 33.4 33.2 47.3 44.4 46.2 44.6
TALOR-EE(LLaMA)
5-shot 48.9 44.5 37.7 34.8 55.3 54.6 48.5 47.8
10-shot 58.1 55.7 45.5 42.5 58.2 57.5 52.2 48.4
0-shot 49.3 41.9 34.0 32.4 57.1 55.8 43.1 40.8
1-shot 50.3 42.0 34.5 32.1 51.6 44.3 43.7 42.1
TALOR-EE(GPT)
5-shot 52.9 48.2 39.1 37.3 57.5 56.0 49.4 45.5
10-shot 56.9 54.6 43.5 43.0 62.4 61.7 53.4 49.6
Table3: Low-resourceEEresultsonERE.Boldrepresentsthehighestscoreforthecurrentsetting.
backbonemodel,highlightingtheeffectivenessof eratedbyLLaMAandGPT.
augmented training examples across various EE
models. (2) The observed improvement is more Additionally,wehaveevaluatedthegeneration
pronounced in extremely low-resource scenarios, qualityandtheeffectivenessoftheproposedmod-
particularlyinzero-shot,1-shot,and5-shotscenar- ules. Notably, for diversity, there is a substantial
ios. Theimpactislesssignificantwhenmoreclean increaseinuniqueargumentrolescomparedtothe
trainingexamplesareavailable,suchasinthe10- few-shotexamples. Forexample,inthecommon
shotsetting. (3)Weobservethattheperformance 10 and 5-shot settings, the count of unique argu-
ofzero-shotaugmentedtrainingcansurpassthatof ment roles surged from 142 to 1184, marking a
1-shottrainingwithcleanexamples. Thisdiscrep- remarkableincreaseof2502percentagepoints,on
ancy arises because some sampled clean training averageacrossthegenerationmodels. Regarding
examplesmaynotstraightforwardlyexpressevent polarity,amongthe30sampledaugmentationsveri-
information. Forinstance,thetoken“open”could fiedthroughhumanevaluation,thegeneratedevent
triggeraStart-Organizationevent,introducingcon- mentionexpressionsconsistentlyalignwiththetar-
fusioninthesemanticsoftheStart-Organization getednegativeexpressiontypes. Intermsofback-
eventtype. (4)Augmentedexamplesgeneratedby validation,theevaluationinvolvedtwoannotators
different generation agents consistently enhance whoeachassessed200randomlysampledgenera-
low-resource EE performance. Notably, greater tions(100forwithback-validationgenerationsand
performancegainsareachievedwithexamplesgen- 100forgenerationswithoutback-validation). On
average,sevengenerationsweredeemednotfluentFigure5: ExperimentalresultsonERE.(a-b)arevisualizationsforCommon5,and(c-d)forCommon10.
Common5 Common10
Method K-shot
Tri-I Tri-C Arg-I Arg-C Tri-I Tri-C Arg-I Arg-C
1-shot 66.5 61.0 42.3 34.4 71.5 66.7 45.4 42.4
TALOR-EE(LLaMA) 5-shot 70.2 63.9 46.3 42.4 71.7 70.1 50.5 46.7
10-shot 70.0 67.6 46.2 43.3 70.5 70.2 51.2 49.5
1-shot 61.2 52.1 35.9 28.3 72.9 64.6 46.2 40.6
-enrichedcontext 5-shot 68.5 64.2 43.5 41.1 73.2 70.0 45.7 44.6
10-shot 67.0 63.4 43.1 39.5 74.7 71.7 46.4 43.2
1-shot 70.5 65.1 41.8 34.4 74.1 67.4 44.4 38.8
-negativeaugmentations 5-shot 69.3 62.6 41.8 39.3 77.4 73.4 48.4 42.8
10-shot 69.1 61.3 40.8 39.6 74.1 70.5 46.6 44.3
1-shot 61.2 52.1 35.9 28.3 72.7 66.0 47.3 42.2
-back-validation 5-shot 68.0 62.8 43.1 38.6 76.1 74.6 48.6 44.4
10-shot 67.2 65.2 42.1 40.2 75.3 71.2 47.3 46.7
Table4: AblationstudyonACE05-E.
whenutilizingtheback-validationmodule,while cial role of augmentation diversity in mitigating
19generationswereidentifiedasnotfluentwithout low-resourceargumentextraction.
theback-validationmodule.
4.3 ErrorAnalysis
4.2 AblationStudies
Table 5 illustrates several challenging examples.
Anablationstudywasconductedtoassesstheeffec- Foreventtriggerdetection,mostoftheerrorsare
tivenessofeachproposedmodule,andtheexperi- from the insufficient understanding of the trig-
mentalresultsarepresentedinTable4. (Omitting ger phrase. For example in example (a) in Ta-
theenrichedcontextinthesettingentailsbypassing ble 5, linking the phrase “crumbling” to the End-
theDependentContextRetrievalmodule,resulting Org(anization)eventischallenginggiventhelim-
intheabsenceofnewlygeneratedeventstructures.) ited trigger training examples from either clean
Onaverage,acrossallsettings,theperformanceof dataoraugmenteddata. Example(b)ischalleng-
triggerclassificationdecreasedby2.5%and1.9%, ingbecausethetoken“combination”entailscloser
andargumentclassificationdecreasedby8.3%and semanticrelationtotheMerge-Orgevent. Example
7.1%,intheabsenceofenrichedcontextorback- (c)illustratesacasewherethecurrentdataaugmen-
validation,respectively. Withoutnegativeaugmen- tationmodelfallsshortingeneratingintricateevent
tations, the argument classification decreases by expressions. Example(d)illustratesascenarioin
7.5%, while trigger classification performance is whichtheuseofaugmenteddatacouldpotentially
onparwithTALOR-EE(LLaMA).Thishighlights causeconfusion. Inthiscase,theactualeventper-
thatthedesignedmoduleshaveamorepronounced tainstoafilmreleaseratherthanajudicialrelease.
impactonargumentationclassificationthanontrig- Despiteinadequatecontextinformation,thereisa
ger detection. The absence of enriched context likelihoodthattheaugmenteddatamighthavegen-
led to the most significant decrease in argument eratedafalsepredictionwithincreasedconfidence.
classification performance, emphasizing the cru- Onepotentialsolutiontothischallengeistheabil-ID Text GTH Predictions
HoonsaidSaddam’sregimewascrum-
crumbling;End-Org;regime:
(a) blingunderthepressureofahugeair None
Org;
assault.
Thecombinationofthebankingopera- combination;
combination,Merge-Org;
tionsofBarclaysSpainandZaragozano Transfer-Ownership;Barclays
(b) businesses,Org
willbringtogethertwocomplementary Spain:Buyer;Zaragozano:
businesses. Artifact;
Marriedforthesecondtime,Haririhas
(c) Married,Marry;Hariri:Person; None
fivechildren.
HoweverthefirmannouncedonFriday
thatithadreachedadealwiththeBritish releases;Release-Parole;firm:
(d) None
armofFrenchdistributorsPathetoshow Entity;
fourreleases.
Table5: Casestudyforchallengingexamples
itytodistinguishbetweenmultiplemeaningsofthe Generationagentsareemployedtocreateadiverse
sameword. trainingdatasetforeventstructuresenrichedwith
In contrast to event trigger detection, argu- domain-invariant entities. The generated exam-
mentextractionpresentsgreaterchallenges,asim- plesundergoathoroughback-and-forthvalidation
provementsinargumentextractionprovelesspro- process to assess accuracy and coherence. Our
nouncedthanthoseintriggerdetection. Ourcon- research encompasses extensive experiments in
clusionstemsfromameticulousanalysisofthegen- diverse low-resource learning scenarios, such as
eratedoutputsandpredictionresults,revealingtwo zero-shot and few-shot learning settings, across
primaryreasons. Thefirstreasonisthelackofclear various event extraction models. The outcomes
andcomprehensiveexplanationsforcertainargu- of these experiments highlight the effectiveness
mentroles,forexample,theargumentrole“agent” oftheproposedframework. Furthermore,ourpro-
intheStart-Orgeventtype. Accordingtothedefini- posedmethodologycaninspireresearchersfromdi-
tion(LinguisticDataConsortium,2005),an“agent” versedomainstoembraceacomparableparadigm
inaStart-Orgeventisa“PER”,“ORG”,or“GPE” or delve into the investigation of data augmenta-
entity responsible for the “START-ORG” Event. tionmethodsasameansofenrichingtheirtraining
However,itrequirestremendousexpertknowledge datasets.
towritepreciseinstructionsforargumentroleslike
this. Thesecondreasonpertainstothelackofclear Limitations
distinctions among argument roles in generation
prompts. We recognize that elucidating the pur- TALOR-EE establishesapowerfulstartingpoint
poseanddifferentiationofeachargumentrolecan foradvancingfew-shotlearningresearch,offering
be intricate. For instance, we observed minimal aflexibleframeworkforframingnewtasksthrough
orevenadverseeffectsofaugmenteddataonthe ourproposedaugmentationmethod. Itencourages
event type “Transfer-Ownership”. This complex- a systematic exploration of general and resilient
ity arises from the potential confusion surround- enhancements for low-resource event extraction
ing three specific argument roles: “Beneficiary”, systems. However, augmenting non-event exam-
“Buyer”,and“Seller”,particularlywhenthetrigger ples takes appropriate attention, as the proposed
involvestermslike“sell”or“acquire”. Notably,al- systemmaytendtopredictadditionaleventmen-
teringthetriggerfrom“sell”to“acquire”inducesa tions. Theabsenceofacleardistinctionbetween
substantialchangeinthesentence’sentiresyntactic anactualeventandanon-eventmention,duetothe
structure. lackofaprecisedefinition, underscorestheneed
for appropriate action. We extend a warm invita-
5 Conclusion tiontofuturelow-resourceresearchendeavorsand
augmentationmethodstodelveintothestructural
Inconclusion,thisstudyproposesanewparadigm aspects of event generation within a contrastive
for tackling low-resource event extraction tasks. setting.Acknowledgements XinCong,ShiyaoCui,BowenYu,TingwenLiu,Yubin
Wang,andBinWang.2021. Few-shoteventdetec-
This research is supported by the award No. tionwithprototypicalamortizedconditionalrandom
2238940fromtheFacultyEarly Career Develop- field. In Findings of the Association for Computa-
tionalLinguistics: ACL-IJCNLP.
mentProgram(CAREER)oftheNationalScience
Foundation(NSF).Theviewsandconclusionscon-
Shumin Deng, Ningyu Zhang, Jiaojian Kang, Yichi
tained herein are those of the authors and should Zhang,WeiZhang,andHuajunChen.2020. Meta-
not be interpreted as necessarily representing the learningwithdynamic-memory-basedprototypical
networkforfew-shoteventdetection. Proceedings
official policies, either expressed or implied, of
ofthe13thInternationalConferenceonWebSearch
the U.S. Government. The U.S. Government is
andDataMining.
authorizedtoreproduceanddistributereprintsfor
governmentalpurposesnotwithstandinganycopy- XinyaDuandClaireCardie.2020. Eventextractionby
rightannotationtherein. answering(almost)naturalquestions. InProceedings
ofthe2020ConferenceonEmpiricalMethodsinNat-
uralLanguageProcessing(EMNLP),pages671–683,
Online.AssociationforComputationalLinguistics.
References
Sergey Edunov, Myle Ott, Michael Auli, and David
AshutoshAgarwal,AnayMajee,AnbumaniSubrama-
Grangier.2018. Understandingback-translationat
nian,andChetanArora.2021. Attentionguidedco-
scale. In Proceedings of the 2018 Conference on
sinemarginforovercomingclass-imbalanceinfew-
EmpiricalMethodsinNaturalLanguageProcessing,
shotroadobjectdetection.
pages489–500,Brussels,Belgium.Associationfor
David Ahn. 2006. The stages of event extraction. In ComputationalLinguistics.
ProceedingsoftheWorkshoponAnnotatingandRea-
soningaboutTimeandEvents,pages1–8. Aleksandra Edwards, Asahi Ushio, Jose Camacho-
Collados,HélènedeRibaupierre,andAlunPreece.
Kai Cao, Xiang Li, Miao Fan, and Ralph Grishman. 2021. Guidinggenerativelanguagemodelsfordata
2015. Improvingeventdetectionwithactivelearn- augmentationinfew-shottextclassification.
ing. In Proceedings of the International Confer-
enceRecentAdvancesinNaturalLanguageProcess- StevenY.Feng,VarunGangal,DongyeopKang,Teruko
ing, pages 72–77, Hissar, Bulgaria. INCOMA Ltd. Mitamura,andEduardHovy.2020. GenAug: Data
Shoumen,BULGARIA. augmentationforfinetuningtextgenerators. InPro-
ceedingsofDeepLearningInsideOut(DeeLIO):The
ShuyangCaoandLuWang.2021. Controllableopen- FirstWorkshoponKnowledgeExtractionandIntegra-
endedquestiongenerationwithanewquestiontype tionforDeepLearningArchitectures,pages29–42,
ontology. Online.AssociationforComputationalLinguistics.
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.
Irena Gao, Shiori Sagawa, Pang Wei Koh, Tat-
2021. Honeyorpoison? solvingthetriggercursein
sunoriHashimoto,andPercyLiang.2022. Out-of-
few-shoteventdetectionviacausalintervention.
distribution robustness via targeted augmentations.
In NeurIPS 2022 Workshop on Distribution Shifts:
YunmoChen,TongfeiChen,SethEbner,AaronSteven
ConnectingMethodsandApplications.
White, and Benjamin Van Durme. 2020. Reading
themanual: Eventextractionasdefinitioncompre-
Sayan Ghosh, Zheng Qi, Snigdha Chaturvedi, and
hension. InProceedingsoftheFourthWorkshopon
ShashankSrivastava.2021. Howhelpfulisinverse
StructuredPredictionforNLP,Online.Association
reinforcementlearningfortable-to-textgeneration?
forComputationalLinguistics.
InProceedingsofthe59thAnnualMeetingoftheAs-
sociationforComputationalLinguisticsandthe11th
HaoCheng,ZhaoweiZhu,XingyuLi,YifeiGong,Xing
InternationalJointConferenceonNaturalLanguage
Sun, and Yang Liu. 2021. Learning with instance-
Processing(Volume2: ShortPapers),pages71–79,
dependentlabelnoise: Asamplesieveapproach. In
Online.AssociationforComputationalLinguistics.
International Conference on Learning Representa-
tions.
ThammeGowdaandJonathanMay.2020. Findingthe
Nancy Chinchor and Elaine Marsh. 1998. Muc-7 in- optimalvocabularysizeforneuralmachinetransla-
formationextractiontaskdefinition. InProceeding tion. In Findings of the Association for Computa-
of the seventh message understanding conference tionalLinguistics: EMNLP2020,pages3955–3964,
(MUC-7),Appendices,pages359–367. Online.AssociationforComputationalLinguistics.
Arkabandhu Chowdhury, Mingchao Jiang, and Chris RalphGrishman.1997. Informationextraction: Tech-
Jermaine.2021. Few-shotimageclassification: Just niques and challenges. In International summer
usealibraryofpre-trainedfeatureextractorsanda school on information extraction, pages 10–27.
simpleclassifier. Springer.JianGuan,Xiao-XiMao,ChangjieFan,ZitaoLiu,Wen- Lu Jiang, Di Huang, Mason Liu, and Weilong Yang.
biaoDing,andMinlieHuang.2021. Longtextgener- 2020. Beyond synthetic noise: Deep learning on
ationbymodelingsentence-levelanddiscourse-level controllednoisylabels. InICML.
coherence. InACL.
Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Ji-
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, ashi Feng, and Trevor Darrell. 2019. Few-shot
MiaoXu,WeihuaHu,IvorW.Tsang,andMasashi object detection via feature reweighting. In 2019
Sugiyama. 2018. Co-teaching: Robust training of IEEE/CVF International Conference on Computer
deepneuralnetworkswithextremelynoisylabels. In Vision(ICCV),pages8419–8428.
Proceedings of the 32nd International Conference
onNeuralInformationProcessingSystems,NIPS’18, HazelKim,DaecheolWoo,SeongJoonOh,Jeong-Won
page8536–8546,RedHook,NY,USA.CurranAsso- Cha,andYo-SubHan.2022. Alp:Dataaugmentation
ciatesInc. usinglexicalizedpcfgsforfew-shottextclassification.
Proceedings of the AAAI Conference on Artificial
RidongHan,TaoPeng,ChaohaoYang,BenyouWang, Intelligence,36:10894–10902.
LuLiu,andXiangWan.2023. Isinformationextrac-
tionsolvedbychatgpt? ananalysisofperformance, Viet Dac Lai, Franck Dernoncourt, and Thien Huu
evaluationcriteria,robustnessanderrors. Nguyen.2020a. Exploitingthematchinginformation
in the support set for few shot event classification.
I-Hung Hsu, Kuan-Hao Huang, Elizabeth Boschee,
Pacific-Asia Conference on Knowledge Discovery
ScottMiller,PremNatarajan,Kai-WeiChang,and
andDataMining,page233–245.
NanyunPeng.2022. Degree: Adata-efficientgen-
erative event extraction model. In Proceedings of VietDacLai,ThienHuuNguyen,andFranckDernon-
the2022ConferenceoftheNorthAmericanChap-
court. 2020b. Extensively matching for few-shot
teroftheAssociationforComputationalLinguistics learningeventdetection. InProceedingsoftheFirst
(NAACL). JointWorkshoponNarrativeUnderstanding,Story-
lines,andEvents,pages38–45,Online.Association
XumingHu,ChenweiZhang,FukunMa,ChenyaoLiu,
forComputationalLinguistics.
LijieWen,andPhilipS.Yu.2021a. Semi-supervised
relationextractionviaincrementalmetaself-training.
BoLi,GexiangFang,YangYang,QuansenWang,Wei
In Findings of the Association for Computational
Ye,WenZhao,andShikunZhang.2023. Evaluating
Linguistics: EMNLP2021,OnlineandPuntaCana,
chatgpt’sinformationextractioncapabilities: Anas-
DominicanRepublic.AssociationforComputational
sessmentofperformance,explainability,calibration,
Linguistics.
andfaithfulness.
XumingHu,ChenweiZhang,YawenYang,XiaoheLi,
BohaoLi,BoyuYang,ChangLiu,FengLiu,Rongrong
LiLin,LijieWen,andPhilipS.Yu.2021b. Gradi-
Ji,andQixiangYe.2021. Beyondmax-margin:Class
entimitationreinforcementlearningforlowresource
margin equilibrium for few-shot object detection.
relationextraction. InProceedingsofthe2021Con-
2021IEEE/CVFConferenceonComputerVisionand
ferenceonEmpiricalMethodsinNaturalLanguage
PatternRecognition(CVPR),pages7359–7368.
Processing,OnlineandPuntaCana,DominicanRe-
public.AssociationforComputationalLinguistics.
FayuanLi,WeihuaPeng,YuguangChen,QuanWang,
LuPan, YajuanLyu, andYongZhu.2020a. Event
Jinchi Huang, Lie Qu, Rongfei Jia, and Binqiang
extractionasmulti-turnquestionanswering. InFind-
Zhao. 2019. O2u-net: A simple noisy label detec-
ingsoftheAssociationforComputationalLinguistics:
tion approach for deep neural networks. In 2019
EMNLP2020,pages829–838,Online.Association
IEEE/CVF International Conference on Computer
forComputationalLinguistics.
Vision(ICCV),pages3325–3333.
Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng JunnanLi,RichardSocher,andStevenC.H.Hoi.2020b.
Ji, Clare Voss, Jiawei Han, and Avirup Sil. 2016. Dividemix: Learning with noisy labels as semi-
Liberaleventextractionandeventschemainduction. supervised learning. In International Conference
In Proceedings of the 54th Annual Meeting of the onLearningRepresentations.
AssociationforComputationalLinguistics(Volume
YingLin,HengJi,FeiHuang,andLingfeiWu.2020.
1: LongPapers),pages258–268.
Ajointneuralmodelforinformationextractionwith
LifuHuangandHengJi.2020. Semi-supervisednew globalfeatures. InProceedingsofthe58thAnnual
eventtypeinductionandeventdetection. InProceed- Meeting of the Association for Computational Lin-
ingsofthe2020ConferenceonEmpiricalMethods guistics,pages7999–8009,Online.Associationfor
in Natural Language Processing (EMNLP), pages ComputationalLinguistics.
718–724.
LinguisticDataConsortium.2005. Englishannotation
Amir Hussein, Shammur Absar Chowdhury, Ahmed guidelines for events. https://www.ldc.
Abdelali,NajimDehak,andAhmedAli.2022. Code- upenn.edu/sites/www.ldc.upenn.edu/
switchingtextaugmentationformultilingualspeech files/english-events-guidelines-v5.
processing. 4.3.pdf.JianLiu,YuboChen,KangLiu,WeiBi,andXiaojiang ZhiyiSong,AnnBies,StephanieStrassel,TomRiese,
Liu.2020. Eventextractionasmachinereadingcom- JustinMott,JoeEllis,JonathanWright,SethKulick,
prehension. InProceedingsofthe2020Conference NevilleRyant,andXiaoyiMa.2015. Fromlightto
onEmpiricalMethodsinNaturalLanguageProcess- richere: annotationofentities,relations,andevents.
ing(EMNLP),pages1641–1651,Online.Association InProceedingsofthethe3rdWorkshoponEVENTS:
forComputationalLinguistics. Definition,Detection,Coreference,andRepresenta-
tion,pages89–98.
XiaoLiu,HeyanHuang,GeShi,andBoWang.2022.
Dynamicprefix-tuningforgenerativetemplate-based BoSun,BanghuaiLi,ShengcaiCai,YeYuan,andChi
eventextraction. InProceedingsofthe60thAnnual Zhang. 2021. Fsce: Few-shot object detection via
Meeting of the Association for Computational Lin- contrastiveproposalencoding. 2021IEEE/CVFCon-
guistics(Volume1: LongPapers),pages5216–5228, ferenceonComputerVisionandPatternRecognition
Dublin,Ireland.AssociationforComputationalLin- (CVPR),pages7348–7358.
guistics.
Bo Wang, Heyan Huang, Xiaochi Wei, Ge Shi, Xiao
Mengsay Loem, Sho Takase, Masahiro Kaneko, and Liu,ChongFeng,TongZhou,ShuaiqiangWang,and
NaoakiOkazaki.2022. Extraphrase: Efficientdata DaweiYin.2023a. Boostingeventextractionwithde-
augmentationforabstractivesummarization. noisedstructure-to-textaugmentation. InFindingsof
theAssociationforComputationalLinguistics: ACL
Qing Lyu, Hongming Zhang, Elior Sulem, and Dan 2023,pages11267–11281,Toronto,Canada.Associ-
Roth.2021. Zero-shotEventExtractionviaTransfer ationforComputationalLinguistics.
Learning: ChallengesandInsights. InProceedings
of the 59th Annual Meeting of the Association for SijiaWang,MoYu,ShiyuChang,LichaoSun,andLifu
ComputationalLinguisticsandthe11thInternational Huang. 2022. Query and extract: Refining event
JointConferenceonNaturalLanguageProcessing extractionastype-orientedbinarydecoding. InFind-
(Volume 2: Short Papers), pages 322–332, Online. ingsoftheAssociationforComputationalLinguistics:
AssociationforComputationalLinguistics. ACL2022,pages169–182,Dublin,Ireland.Associa-
tionforComputationalLinguistics.
Mingyu Derek Ma, Xiaoxuan Wang, Po-Nien Kung,
P.JeffreyBrantingham,NanyunPeng,andWeiWang. SijiaWang, MoYu, andLifuHuang.2023b. Theart
2023. Star: Improving low-resource information ofprompting: Eventdetectionbasedontypespecific
extractionbystructure-to-textdatagenerationwith prompts. In Proceedings of the 61st Annual Meet-
largelanguagemodels. ingoftheAssociationforComputationalLinguistics
(Volume2:ShortPapers),pages1286–1299,Toronto,
NathanNg,KyunghyunCho,andMarzyehGhassemi. Canada.AssociationforComputationalLinguistics.
2020. SSMBA:Self-supervisedmanifoldbaseddata
augmentation for improving out-of-domain robust- XinWang,ThomasE.Huang,TrevorDarrell,JosephE
ness. In Proceedings of the 2020 Conference on Gonzalez,andFisherYu.2020a. Frustratinglysimple
EmpiricalMethodsinNaturalLanguageProcessing few-shotobjectdetection.
(EMNLP),pages1268–1283,Online.Associationfor
ComputationalLinguistics. Yida Wang, Yinhe Zheng, Yong Jiang, and Minlie
Huang. 2021a. Diversifying dialog generation via
PanupongPasupatandPercyLiang.2014. Zero-shot adaptivelabelsmoothing. InProceedingsofthe59th
entity extraction from web pages. In Proceedings AnnualMeetingoftheAssociationforComputational
of the 52nd Annual Meeting of the Association for Linguistics.
ComputationalLinguistics(Volume1: LongPapers),
pages391–401. Yufei Wang, Ian D. Wood, Stephen Wan, Mark Dras,
andMarkJohnson.2021b. Mentionflags(mf): con-
LeiShen,FandongMeng,JinchaoZhang,YangFeng, strainingtransformer-basedtextgenerators. InPro-
and Jie Zhou. 2021a. GTM: A generative triple- ceedings of the 59th Annual Meeting of the Asso-
wisemodelforconversationalquestiongeneration. ciationforComputationalLinguisticsandthe11th
In Proceedings of the 59th Annual Meeting of the InternationalJointConferenceonNaturalLanguage
Association for Computational Linguistics and the Processing(Volume1: LongPapers),ACL-IJCNLP
11thInternationalJointConferenceonNaturalLan- 2021-59thAnnualMeetingoftheAssociationfor
guageProcessing(Volume1: LongPapers),pages ComputationalLinguisticsandthe11thInternational
3495–3506,Online.AssociationforComputational JointConferenceonNaturalLanguageProcessing,
Linguistics. ProceedingsoftheConference,pages103–113.As-
sociationforComputationalLinguistics(ACL).
ShirongShen,TongtongWu,GuilinQi,Yuan-FangLi,
Gholamreza Haffari, and Sheng Bi. 2021b. Adap- ZhuoweiWang,JingJiang,BoHan,LeiFeng,BoAn,
tiveknowledge-enhancedbayesianmeta-learningfor Gang Niu, and Guodong Long. 2020b. Seminll:
few-shoteventdetection. InFindingsoftheAssocia- A framework of noisy-label learning by semi-
tionforComputationalLinguistics,page2417–2429. supervisedlearning. TransactionsonMachineLearn-
AssociationforComputationalLinguistics(ACL). ingResearch,2022.Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Mingyuan Zhang, Jane Lee, and Shivani Agarwal.
2020. Combating noisy labels by agreement: A 2021c. Learningfromnoisylabelswithnochangeto
jointtrainingmethodwithco-regularization. In2020 thetrainingprocess. InInternationalConferenceon
IEEE/CVFConferenceonComputerVisionandPat- MachineLearning.
ternRecognition(CVPR),pages13723–13732.
ZhaoweiZhu, ZihaoDong, andYangLiu.2022. De-
JasonWeiandKaiZou.2019. EDA:Easydataaugmen- tectingcorruptedlabelswithouttrainingamodelto
tationtechniquesforboostingperformanceontext predict. In International Conference on Machine
classificationtasks. InProceedingsofthe2019Con- Learning.
ferenceonEmpiricalMethodsinNaturalLanguage
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP),pages6383–6389,HongKong,China.As-
sociationforComputationalLinguistics.
XiangWei,XingyuCui,NingCheng,XiaobinWang,
Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,
YufengChen,MeishanZhang,YongJiang,andWen-
juanHan.2023. Zero-shotinformationextractionvia
chattingwithchatgpt.
YangXiaoandRenaudMarlet.2020. Few-shotobject
detectionandviewpointestimationforobjectsinthe
wild. InECCV.
XiaopengYan,ZiliangChen,AnniXu,XiaoxiWang,
Xiaodan Liang, and Liang Lin. 2019. Meta r-cnn:
Towards general solver for instance-level low-shot
learning. 2019IEEE/CVFInternationalConference
onComputerVision(ICCV),pages9576–9585.
Yiben Yang, Chaitanya Malaviya, Jared Fernandez,
SwabhaSwayamdipta,RonanLeBras,Ji-PingWang,
ChandraBhagavatula,YejinChoi,andDougDowney.
2020. Generative data augmentation for common-
sensereasoning. InFindingsoftheAssociationfor
Computational Linguistics: EMNLP 2020, pages
1008–1025,Online.AssociationforComputational
Linguistics.
QuanmingYao, HansiYang, BoHan, GangNiu, and
James Tin-Yau Kwok. 2020. Searching to exploit
memorizationeffectinlearningwithnoisylabels. In
InternationalConferenceonMachineLearning.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor
Tsang, and Masashi Sugiyama. 2019. How does
disagreement help generalization against label cor-
ruption? In International Conference on Machine
Learning,pages7164–7173.
Gongjie Zhang, Kaiwen Cui, Rongliang Wu, Shijian
Lu, and Yonghong Tian. 2021a. Pnpdet: Efficient
few-shotdetectionwithoutforgettingviaplug-and-
playsub-networks. 2021IEEEWinterConference
onApplicationsofComputerVision(WACV),pages
3822–3831.
HongmingZhang,HaoyuWang,andDanRoth.2021b.
Zero-shot Label-aware Event Trigger and Argu-
mentClassification. InFindingsoftheAssociation
forComputationalLinguistics: ACL-IJCNLP2021,
pages1331–1340,Online.AssociationforComputa-
tionalLinguistics.A Implementation the given event information is in the generated
sentence. However, the given context informa-
For a fair comparison with baseline approaches,
tion is optional in generation. Generate a sen-
weusethepre-trainedbert-large-uncased
tencewith{event_type_name}event,withoptional
model for fine-tuning and optimizing our model
context information: {list_of_context_entitites}.
with BertAdam. We optimize the parameters
{event_template}.” The {event_template} refers
with grid search: training epoch 10, learn-
tothetextualrepresentationgiventheeventstruc-
ing rate ∈ [3e-6,1e-4], training batch size ∈
ture,aspresentedin(Hsuetal.,2022).
{8,12,16,24,32}, dropout rate ∈ {0.4,0.5,0.6}.
Our experiments run on one Quadro RTX 8000. B NegativeEventMentionsPrompts
For trigger detection, the average runtime is 3.0
Table 6 list generation instructions of negative
hours. For argument detection, the average run-
event mentions for generation agents. Table 7
timeis1.3hours. WeuseSpacytogeneratePOS
showsnegativeaugmentationexamples.
tags. We use three random seed 0, 39, 42 for all
experiments,andreportthemeanscores.
C ExperimentalResultswithQE
Sampling Strategy Note that in the context of
Table8showsExperimentalresultsforACE05-E
few-shotlearningwithanNway-Kshotsetting,the
withQueryExtract(QE)asthebaselinemodel.
variableKdenotesthenumberofeventmentions
rather than training examples. The original cor- D FeaturesContributedbyAugmented
pus contains numerous instances where a single Data
sentenceincludesmultipleeventmentions,present-
The features that are better captured by the pro-
ingachallengeforthefew-shotexamplesampling
posedapproachinclude(1)Themappingbetween
process. Without regularization, the sampled ex-
candidate triggers and event types. The presence
amplesmayprobablyexceedthespecifiedKevent
of a greater variety of event mention expressions
mentions.
within diverse contexts enhances the robustness
Toaddressthisissueandensurethat,forevery
and comprehensiveness of the mapping between
setting, the sampled examples with novel event
candidate triggers and event types. (2) The map-
types do not surpass K, we employ a sorting
pingbetweennegativeexpressionsandeventtypes.
mechanismbasedonthefrequencyofeventtypes
Due to the limited occurrence of negative events
in decreasing order. This involves sorting the
in the training data, their availability as few-shot
event types and then sampling in the sorted or-
examplesisrestricted. Withtheintegrationofthe
der. Forinstance,considertheexampleswith"Jus-
negative augmentation module, the mapping be-
tice:Acquit"mentions,oneofwhichalsoincludes
tween negative expressions and event types be-
a "Justice:Convict" mention. If we were to first
comes clearer. (3) The relation between candi-
sampleexamplesfor"Justice:Convict"andthispar-
date triggers and arguments. The generated sen-
ticularexampleisomitted,wewouldmisstheop-
tences exhibit a comparatively higher prevalence
portunitytoincludethiscrucialinstancefor"Jus-
of straightforward event expressions than those
tice:Acquit."Thisbecomesespeciallysignificant
presentinannotateddata,suchasACE2005. These
in settings such as 5-shot or 10-shot, where "Jus-
less complex expressions contribute to a good fit
tice:Acquit"hasatotaloffourexamples. Without
forfeaturesrelatedtotherelationbetweencandi-
this sampling approach, the mentioned example
date triggers and arguments, in the low-resource
maybeexcludedfromthetrainingprocedure,im-
settings.
pactingthemodel’sperformance.
GenerationInstruction Thefollowinginstruc-
tion are used to prompt generations given the
event structure: “You are a helpful assistant in
generating fluent and reasonable sentences with
event mentions. An Event is a specific occur-
rence involving participants. An Event is some-
thing that happens. An Event can frequently be
described as a change of state. Please be sureEventExpressionType InstructionPrompt
AnEventisNEGATIVEwhenitisexplicitlyindicatedthattheEventdidnotoccur.Negative
example1:Hiswifewassittinginthebackseatandwas’nothurt’.Negativeexample2:Yeltsin
NegativeEvents
ordered Skuratov’s suspension, but parliament repeatedly ’refused to sack’ him. Given the
generatedsentence,“[SENT]”,changeitintoanegativeexpressionthattheEventdidnotoccur.
BelievedEventsareeventmentionsthatsomepeopleororganizationsthinkorbelievewould
happenbutarenotnecessarilyrealortrueeventoccurrences.Example1:Rumorsof’arrests’
BelievedEvents
circulatedinVancouver.Example2:Thecharitywassuspectedof’giving’moneytoalQaeda.
Giventhegeneratedsentenceyouprovide,’[SENT]’,changeitintoabelievedeventsentence:
Hypotheticaleventsareeventmentionsthataresupposedtohappenbutarenotnecessarilyreal
ortrueeventoccurrences. Example1: Shouldhenot’pay’themoney,theywould’kill’him.
HypotheticalEvents
Example2:Ademonstrationofhowhewouldbehaveifhewereto’become’President.Given
thegeneratedsentenceyouprovide,’[SENT]’,changeitintoahypotheticaleventsentence:
PromisedEventsareeventmentionsthatarepromisedtohappenbutarenotnecessarilyrealor
trueeventoccurrences.Example1:Hesaidhewould’leave’town.Example2:Promisesof’aid’
PromisedEvents
madebyArabandEuropeancountries. Giventhegeneratedsentenceyouprovide,’[SENT]’,
changeitintoapromisedeventsentence:
Desiredeventsareeventmentionsthataredesiredtohappenbutnotnecessarilyrealortrueevent
DesiredEvent occurrences. Example: Theywantedto’acquire’thecompanylastyear. Giventhegenerated
sentenceyouprovide,“[SENT]”,changeitintoaDesiredeventsentence:
Table6: Negative/assertedexpressiongenerationtemplate. “[SENT]”isaplaceholderforthegeneratedsentence
withapositiveeventexpression. Theinstructionisadaptedfrom(LinguisticDataConsortium,2005).
id Note Content
EventStructure Trigger:bankruptcy.Org:Hazelhurst&AssociatesInc.
Context 10percent,yesterday,$22.5million
Positivemention Hazelhurst&AssociatesInc.declaredbankruptcyyesterday,with$22.5millionindebts.
Negativemention Hazelhurst&AssociatesInc.didnotdeclarebankruptcyyesterday,with$22.5millionindebts.
1
ItisbelievedthatHazelhurst&AssociatesInc. willdeclarebankruptcytomorrow,with$30
Assertedmention:
millionindebts.
EventStructure Trigger:pardon,Place:Jordan,Adjudicator:AbdullahII,Defendant:Rich
Context Republicans,today,hisdarkesthours
RichreceivedapardonfromAbdullahIIduringhisdarkesthours,asRepublicansgathered
Positivemention
todaytooffertheirsupport.
2
Rich’spardonfromAbdullahIIwascanceledduringhisdarkesthours,asRepublicansdidnot
Negativemention
gather.
RichdesiredtoreceiveapardonfromAbdullahIIduringhisdarkesthours,asRepublicans
Assertedmention
gatheredlastyeartooffertheirsupport.
Table7: NegativeAugmentationExample
Common5 Common10
Method K-shot
Tri-I Tri-C Arg-I Arg-C Tri-I Tri-C Arg-I Arg-C
1-shot 58.6 48.7 33.1 29.3 58.6 51.2 37.5 30.1
QE 5-shot 61.9 57.1 37.6 33.1 66.7 61.1 41.7 36.5
10-shot 64.1 62.2 40.3 38.6 72.0 67.2 45.6 45.2
1-shot 60.6 58.0 41.8 34.2 60.4 58.0 41.4 35.0
TOLAR-QE(Vicuna) 5-shot 65.4 62.1 44.3 35.8 70.8 68.8 47.2 41.6
10-shot 65.7 64.0 43.4 39.6 69.5 68.1 50.8 43.7
1-shot 64.7 57.6 39.3 28.3 57.8 54.9 43.5 33.9
TOLAR-QE(LLaMa) 5-shot 61.6 59.4 42.3 37.1 71.2 65.1 46.2 40.9
10-shot 66.0 64.9 44.1 39.8 68.2 67.4 49.4 44.9
1-shot 64.8 58.7 38.4 31.3 62.8 61.2 43.8 36.1
TOLAR-QE(GPT) 5-shot 67.5 59.6 41.4 36.5 66.1 66.1 47.5 43.6
10-shot 67.4 65.2 42.7 39.1 71.1 70.4 49.2 46.5
Table8: Few-shotEventExtractionresultswithdataaugmentationonACE05-EwithQueryExtract(QE).