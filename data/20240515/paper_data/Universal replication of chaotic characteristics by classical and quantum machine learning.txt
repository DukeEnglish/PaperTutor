Universalreplicationofchaoticcharacteristicsbyclassicalandquantummachinelearning
Sheng-Chen Bai1 and Shi-Ju Ran1,∗
1Center for Quantum Physics and Intelligent Sciences,
Department of Physics, Capital Normal University, Beijing 10048, China
(Dated:May15,2024)
Replicatingchaoticcharacteristicsofnon-lineardynamicsbymachinelearning(ML)hasrecentlydrawnwide
attentions. Inthiswork,weproposethataMLmodel,trainedtopredictthestateone-step-aheadfromseveral
latesthistoricstates,canaccuratelyreplicatethebifurcationdiagramandtheLyapunovexponentsofdiscrete
dynamic systems. The characteristics for different values of the hyper-parameters are captured universally
by a single ML model, while the previous works considered training the ML model independently by fixing
thehyper-parameterstobespecificvalues. Ourbenchmarksontheone-andtwo-dimensionalLogisticmaps
show that variational quantum circuit can reproduce the long-term characteristics with higher accuracy than
thelongshort-termmemory(awell-recognizedclassicalMLmodel). Ourworkrevealsanessentialdifference
betweentheMLforthechaoticcharacteristicsandthatforstandardtasks,fromtheperspectiveoftherelation
betweenperformanceandmodelcomplexity. Ourresultssuggestthatquantumcircuitmodelexhibitspotential
advantagesonmitigatingover-fitting,achievinghigheraccuracyandstability.
Introduction.— To what extent can a machine learning
(ML) model learn from the dynamical data of a chaotic sys-
tem? Chaosreferstoaseeminglydisorderlybutdeterministic
behaviorofdynamicsystems [1,2],whichappearsinawide
rangeofrealisticscenariosthataredeemedhighlynon-linear
(suchaspopulationgrowth[3]andatmosphere[4]).Achaotic
system possesses several exotic properties, including high
sensitivity to the initial conditions and perturbations [1, 2].
Thismakesthetimeseriesprediction(TPS),particularlyafter
along-timeevolution,almostinfeasible.
MLhasbeendemonstratedasapowerfulapproachforTPS.
Forinstance,neuralnetworks,suchasthewidely-recognized
long short-term memory (LSTM) [5, 6] and the transformer
modelsdevelopedmostrecently[7],exhibitexcellentperfor-
mancesin,e.g.,weatherforecasting[10,11]. TheML-based
TSPforchaoticsystemsinvolvetwokeyissues:(i)short-term
prediction of the system’s states [12–14], and (ii) long-term
FIG.1. (Coloronline)Theillustrationonthemainprocessoftime- prediction/replicationofthestatistical/ergodicdynamicalbe-
seriespredictionbymachinelearning. Thetop-leftpanelshowsthe
haviors [15–19]. Focusing on the second issue, the previ-
one-dimensional Logistic map [Eq. (1)] as an example. By imple-
ousworksmainlyattemptedtoreplicatebyfixingthehyper-
menting the µ-tuned pre-processing map [see Eqs. (4) and (5)], a
parametersofthedynamicalsystemtobespecificvalues(see,
sample (the data of several states) is mapped to a series of vectors
e.g.,Refs.[15–17]). Itisofgreaterchallengeandimportance as the input of the ML model such as automatically-differentiable
to use a single ML model for universally capturing the char- quantum circuit (ADQC) illustrated in the bottom-left panel. The
acteristics with the hyper-parameters varying in a non-trivial MLmodelisoptimizedbyminimizingthelossfunctionthatistaken
range,whichremainsanopenissue. astherootmean-squareerroroftheone-step-aheadpredictions[see
Inthiswork,weshowthataMLmodel,whichistrainedto thebottom-rightpanelandEq.(3).
predictthestateone-step-aheadfromM latesthistoricstates,
canreplicatethelong-termchaoticcharacteristicsofdiscrete
dynamical systems. We utilize both classical and quantum parameter of the Logistic map varies from stable to chaotic
MLmodels,namelyLSTM[5,6]andvariationalquantumcir- regions,arewellreplicatedbyasingleMLmodel.
cuit [20, 21] (specifically automatically-differentiable quan- Our results suggest that enhancing the model complex-
tumcircuit[22], ADQCinshort), tolearntheone-andtwo- ity generally shows no beneficial effects on improving the
dimensional Logistic maps [3, 23, 24]. The long-term char- accuracy of replicating the long-term characteristics due to
acteristics, including bifurcation diagram and Lyapunov ex- the high sensitivity to over-fitting. ADQC achieves remark-
ponents, are simulated by iterating the ML map for suffi- ably higher accuracy and stability than LSTM, particularly
cientlymanysteps.Thesecharacteristics,forwhichthehyper- forreplicatingtheLyapunovexponents.Theseresultsdemon-
stratethevalidityandpotentialadvantageofquantumcircuit
models on mitigating over-fitting in replicating chaotic char-
acteristics.
∗Correspondingauthor.Email:sjran@cnu.edu.cn Classical and quantum machine learning for time series
4202
yaM
41
]hp-tnauq[
1v48480.5042:viXra2
prediction.—Adynamicalsystemcanbegenerallywrittenas a scalar) to vector by a µ-tuned trainable map, say x[n] →
t
amapx t+1 =f(x t),withx t calledthestateofsystematthe v[n,t] =(v[n,t],v[n,t],··· ,v[n,t]),withx[n]thet-thfeatureof
1 2 d t
(discrete)timet.Here,wetakethe1DLogisticchaoticmapas
the n-th sample and d = dim(v[n,t]) a preset dimension. In
anexample,whichisafundamentalnonlineardynamicalsys-
this way, a sample is mapped to a set of vectors. Note that
tem (see the top-left panel of Fig. 1), with wide applications
bothLSTMandADQCcantakeasetofvectorsasinput.
in understanding chaotic phenomena and generating random
The pre-processing map also depends on some variational
numbers[3]. Themapsatisfies
parameters that will be optimized in the training stage. We
heredefinethepre-processingmapas
f(x ;µ)=µx (1−x ), (1)
t t t
with0≤µ≤4thehyper-parameter. v[n,t] =(cid:88) ξ (x[n];θ)ξ (µ[n];θ)T . (4)
k i t j ijk
AMLmodelisalsoessentiallyamap.Consideringthepre- i,j
dictionofthestateone-step-aheadfromthelatestM historic
states,theMLmapcanbeformallywrittenas The(d×d×d)-dimensionaltensorT willbeoptimizedwhen
trainingtheMLmodel. Thevectorsξ(x[n];θ)andξ(µ[n];θ),
y˜=F(x ,x ,...,x ;W), (2) t
1 2 M whicharebothd-dimensional, areobtainedbythefollowing
mapthattransformsascalartoanormalizedvector[27]. For
with W denoting all the variational parameters of the ML
agivenscalar(saya),thej-thelementsoftheresultingvector
model. The time series generated by the dynamical system
ξ(a;θ)satisfies
f isusedtotraintheMLmodel. Accordingly, aMLsample
isapieceofthetimeserieswiththedataofM states,andthe
(cid:115)
label of this sample is the ground truth of the state one-step-
(cid:18) d−1(cid:19) (cid:18)
θπ
(cid:19)d−j (cid:18)
θπ
(cid:19)j−1
ξ (a;θ)= cos a sin a . (5)
ahead given by f, i.e., y = f(x M) = x M+1. A state in a j j−1 2 2
sampleisalsocalledafeatureinthelanguageofML.
A ML model can be trained by minimizing the prediction The parameter θ will also be optimized (independently on µ
error on the so-called training samples. We discretize the and samples) in the training stage. Note since the training
hyper-parameter µ into 50 different values in the range of samplesareobtainedfromthedynamicstakingdifferentval-
2 < µ ≤ 4. For eachvalue of µ, wegenerate 2000samples uesofµ,oneshouldtakeµ[n]inEq.(4)tobethecorrespond-
as the training set, where each sample is obtained by itera- ingvalueofµforthen-thsample.
tivelyimplementingthedynamicalmaponarandomstatefor The pre-processing map can be generalized to the dy-
(M −1)times. Thevariationalparametersareoptimizedby namical system that contains multiple variables and hyper-
minimizingtherootmean-squareerror(RMSE) parameters. We can always use Eq. (5) and map these vari-
(cid:115) ables (hyper-parameters) to multiple d-dimensional vectors.
L= 1 (cid:88) (y˜[n]−y[n])2 , (3) MoredetailsaboutthedatasetandMLmodels,includingpre-
N processing, forward and backward propagations, prediction,
n
andsettingsofhyper-parameterscanbefoundintheAppen-
where y˜[n] denotes the prediction of the ML model for the dices.
n-th sample, y[n] denotes the ground truth (the label of this Numerical results and discussions.— Fig. 2 demonstrates
sample), and the summation over n goes through the whole theabilityofLSTMandADQConthepredictionofthestates
training set. The gradient descent method is used to update with different values of µ. In (a), we show the bifurcation
variational parameters as W → W −η ∂L with η a small diagramsobtainedbyADQCandLSTM,whichareconsistent
∂W
positiveconstantknownasthegradientsteporlearningrate. withtheonegivenbytheLogisticmap(seetheinset). Each
We here consider LSTM and ADQC for ML. LSTM is a data point in the diagrams is obtained by iterating the map
recognizedclassicalMLmodelforTSP.Itbelongstothevari- (the Logistic map or those of the ML models) for more than
ants of recursive neural networks [25, 26] and addresses the 200times(whichissufficientlylarge),startingfromarandom
issues of vanishing and exploding gradients by introducing state. We take 500 random states for each value of µ, which
theso-calledgatefunctions. ADQCbelongstothevariational aretakendifferentlyfromthoseofthetrainingsamples. The
quantumcircuits.Itsmainadvantageisauniversalparameter- dense of the data points is indicated by the darkness of the
izationwayofthequantumgates,sothatonedoesnotneedto colors. Notewedefinethesamplesgeneratedbytheseinitial
specifythetypes(rotation,phase-shift,controlled-NOT,etc.) states as the testing set. Since these samples are not used to
of gates but just design the structure of the circuit. We use traintheMLmodel,thetestingsetisusedrevealstheso-called
the brick-wall structure as illustrated in bottom-left panel of generalization ability, which refers to the power of the ML
Fig.1. Thepredictionisobtainedbymeasuringthelastqubit modelondealingwiththeunlearnedsamples.
ofthequantumstateafterimplementingtheADQC. The similarity between two bifurcation diagrams can be
Foruniversallyreplicatingthecharacteristicsofthedynam- characterized by the peak signal-to-noise ratios (PSNR) r ,
P
ical system with varying hyper-parameter [say µ in Eq. (1)] whichiswidelyusedinthefieldofcomputervisiontochar-
usingasingleMLmodel,weproposetointroduceaµ-tuned acterize the similarity between two images [29]. We have
trainablepre-processingonthesamplesbeforeinputtingthem r = 43.40 and 43.58 by comparing the ground-truth dia-
P
to the ML model. Our idea is to map each feature (which is gram by Logistic map with the ones by LSTM and ADQC,3
chaoticregion(withpositiveLE),previousworksuggeststhat
the state prediction is valid in a short duration characterized
usually by the Lyapunov time T ≡ λ−1 [32]. Our results
LE
show that ε grows exponentially with t before it converges
R
toO(1),whichobeys
ε ∼eηt. (7)
R
ThetableinFig.2(b)showstheexponentialindextobeabout
η ≈ 0.4+O(10−1). Note the system is chaotic for 3.57 <
µ<4[28].
The above results show the ability of the ML models on
replicatingthecharacteristicsinsteadofpredictingthestates.
SuchanabilityisfurtherdemonstratedbycomparingtheLE
ofthe1DLogisticmapandtheMLmaps(thetrainedLSTM
andADQC),asshowninFig.3(a).TheLEoftheMLmodels
is obtained by replacing the differential term in Eq. (6) by
dF(x ,...,x;W]/dxatx=x .
t−M+1 t
The LE’s with varying µ given by the LSTM and ADQC
with the µ-tuned pre-processing (green and blue lines with
symbols)areconsistentwiththosegivenbytheLogisticmap
(thegroundtruthshownbyredsolidline). Asacomparison,
much worse consistence is reached by the standard LSTM
withouttheµ-tunedpre-processing(blackdash-dotline).This
is a demonstration of the improvement brought by our µ-
tuned pre-processing. The two horizontal stripes indicate
FIG. 2. (Color online) (a) The bifurcation diagrams obtained by
ADQC,LSTM,andLogisticmap(seetheinset). Thedenseofthe whethertheµ-tunedLSTMandADQCgivetheLEwithcor-
datapointsisindicatedbythedarknessofthecolors. (b)Thesemi- rect(green)orincorrect(red)sign.Generally,theaccuracyon
logarithmicplotoftherelativeε versustimetindifferentregionsof replicatingtheLE’ssignishigh.Theincorrectsignsappealin
R
thebifurcationdiagram.Wetakeµ=2.2,3.2,3.4,and3.92,which thechaoticregion.
givethreenegativeandonepositivevaluesfortheLyapunovexpo- Similar results are obtained for the two-dimensional (2D)
nent.Theexponentialgrowthofε Rforµ=3.92isfittedbyEq.(7)
Logistic map [see Fig. 3 (b)], which represents a more intri-
withtheexponentialindexη = 0.44(seetheblacksolidline). The
catenonlinearsystemwithricherdynamicalbehaviorsbyin-
tablein(b)givestheηfordifferentvaluesofµinthechaoticregion.
corporatingalinearcouplingterm[23,24]. Thetwovariables
[x ,x′]aremappedas
t t
respectively. Such a consistency between the bifurcation di-
x =4µ x (1−x )+βx′
agrams does not require to accurately predict the states, but t+1 1 t t t . (8)
x′ =4µ x′(1−x′)+βx
requiresthevalidityofaccuratelyreplicatingthedistribution t+1 2 t t t
ofthestatesafterlong-termevolutions.
We take µ = µ = µ and fix β = 0.1 for simplicity. Two
1 2
In Fig. 2(b), we show the relative error ε =
R LE’s are defined. Obviously, replicating the LE’s of the 2D
N1 (cid:80)N n=1(cid:12) (cid:12)(y˜[n]−y[n])/y[n](cid:12) (cid:12) by ADQC versus the discrete Logistic map is much more challenging than the 1D case.
time t for different values of µ. Note the ML models are al- Consequently,theLSTMwithouttheµ-tunedpre-processing
waystrainedtopredictthestateone-step-ahead. Thesumma- becomes almost invalid. With the µ-tuned pre-processing,
tion in ε R is over the testing test. Different values of µ are ADQCachieveshigheraccuracythanLSTM.
taken so that the system is in the converged (µ = 2.2), bi- AusualwaytoimprovetheperformanceofaMLmodelis
furcation (µ = 3.2), quadrifurcation (µ = 3.4) and chaotic toincreaseitscomplexity. However,suchawaypossiblyfails
(µ = 3.92) regions. The values of Lyapunov exponent inourcase. Benotedthatthoughthedynamicalsystemmay
(LE)[30,31]aregiveninthelegend,whichiscalculatedas justcontainahandfulofhyper-parameters(sayoneparameter
µinthe1DLogisticmap),leaningitsdynamicsisabigchal-
T (cid:12) (cid:12)
1 (cid:88) (cid:12)df(x;µ)(cid:12) lenge, and the key factor to determine the performance of a
λ= lim ln(cid:12) (cid:12) . (6)
T→∞T (cid:12) dx (cid:12) MLmodelonreplicatingthelong-termcharacteristicsshould
t=1 x=xt
notbeitsparametercomplexity. Weexpecthighsensitivityto
In our numerical simulations, we take T > 200, which is theover-fittingissue.
sufficientlylarge. Fig.4supportssuchanexpectation. Increasingtheparam-
Ourresultssuggestthatforthestatepredictioninthecon- eter complexity generally lowers the error on the short-term
vergedandbifurcationregions(withnegativeLE),ADQCcan prediction (say predicting the state one-step ahead). For the
well predict the system’s state after a long-time evolution, 1Dand2DLogisticmaps,Fig.4(a)showsthattheRMSEL
withtherelativeerrorε ∼O(10−3).Whenthesystemisina [Eq. (3)] obtained by LSTM decreases gradually from about
R4
FIG. 4. (Color online) The errors of predicting the state one-step-
aheadandthoseofreplicatingLEforthe1Dand2DLogisticmaps.
ThesetwoerrorsarecharacterizedbyL[Eq.(3)]andL , respec-
LE
FIG.3. (Coloronline)TheLE’softhe(a)1Dand(b)2DLogistic tively. In(a),weshowtheLandL obtainedbyLSTMversusits
LE
mapsandthatofthecorrespondingMLmodels(LSTMandADQC). hiddendimensiond ,andin(b)weshowthosebyADQCversusits
h
Eachpointistheaverageoffiveindependentsimulations.Atthetop numberoflayersN .Eachdatapointistheaverageonfiveindepen-
L
of(a),thetwohorizontalstripesshowwhethertheLSTMandADQC dentsimulations, withthestandarddeviationsdemonstratedbythe
correctly(green)orincorrectly(red)givethesignofLE.In(b),the errorbarsandcoloredshadows.
firsttwostripesatthetopshowtheaccuracyongivingthesignofthe
twoLE’sbyLSTM,andthelasttwostripesgivethatbyADQC.
image generation [33, 34] and model compression [35, 36].
In comparison, LSTM suffers from severe over-fitting issue,
0.02to0.004whenincreasingthehiddendimensiond h from though it has been widely recognized as a powerful model
2to80. SimilarobservationisobtainedwithADQC,whereL withremarkablegeneralizationabilityandvenialover-fitting
decreases to about 0.002 (for the 1D Logistic map) or 0.004 issuewhendealingwithmanyotherMLtasks.
(forthe2DLogisticmap)byincreasingthenumberofcircuit Summary.—Thisworkrevealstheabilityofmachinelearn-
layerstoN L =8. ing (ML) models, which are trained to predict the state one-
The RMSE of LE (denoted as L ) behaves differently step-aheadfromcertainhistoricdata,onreplicatingthelong-
LE
fromthatofL. L iscalculatedinthesamewayasLusing term characteristics of discrete dynamical systems. In com-
LE
the definition of RMSE [Eq. (3)]. For LSTM, the L fluc- parisonwiththepreviousworks,thecharacteristicswithvary-
LE
tuates approximately in the range of 0.005 < L < 0.025. ing values of hyper-parameters are replicated universally by
LE
Increasingd ofLSTMcannotimprovetheaccuracyonrepli- a single ML model. Our results suggest high sensitivity to
h
catingLE.ForADQC,thoughwefailtoseeobviousdropof the over-fitting issue for the replication of the long-term dy-
L by increasing N , L varies much more smoothly ap- namicalcharacteristics. Takingtheone-andtwo-dimensional
LE L LE
proximatelyintherangeof(0.002<L <0.005),whichis Logistic maps as examples, the variational quantum circuit
LE
muchlowerthanthatofLSTM.NoteallthedatainFig.4are exhibits superior performance on replicating the bifurcation
computedusingthetestingset. diagram and Lyapunov exponents. Our findings add to the
Our results suggest that the key of improving the accu- potentialadvantagesofquantumcircuitmodelsonachieving
racy on the prediction of long-term dynamical characteris- highaccuracyandstabilityfortheMLtasksthataresensitive
tics is to developing ML models that better mitigate over- toover-fitting.
fitting. This is different from the ML tasks such as classifi- Acknowledgment. SCB is grateful to Qing lv, Peng-Fei
cation and short-term prediction, where the accuracy can be Zhou,YongQing,Zhang-XuChen,Guo-DongCheng,KeLi,
generallyimprovedbyenhancingthemodelcomplexity. Our RuiHong,YingLu,Yi-ChengTang,andYu-JiaAnforhelp-
workdemonstratesthesuperiorabilityofvariationalquantum ful discussions. This work was supported in part by NSFC
circuitsonmitigatingover-fitting,whichisconsistentwiththe (Grant No. 12004266), Beijing Natural Science Foundation
previousinvestigationsondifferentbutrelevanttopicssuchas (GrantNo. 1232025),TianjinNaturalScienceFoundationof5
China(GrantNo. 20JCYBJC00500),andAcademyforMul- 3000trainingsamplesand500testingsamples. Sinceitcon-
tidisciplinaryStudies,CapitalNormalUniversity. tainstwovariables(x andx′), wedefinethen-thsampleas
t t
(x[n],x′[n],··· ,x[n],x′[n]), and take M = 4 in our simula-
1 1 M M
tions. The pre-processing map is applied similarly to trans-
AppendixA:Detailsofdatasetandpre-processing formthefeaturesinasampleto2M vectors.
Asanexample,weconsidertheone-dimensional(1D)Lo-
gistic map f(x ;µ) = µx (1 − x ). We take the hyper- AppendixB:Automatically-differentiablequantumcircuitfor
t t t
parameter µ to vary from 2 to 4, and discretize to 50 values timeseriesprediction
with an interval of 0.04. For each value of µ, we randomly
generate3000trainingsamplesand500testingsamples.Each The input of an automatically-differentiable quantum cir-
sample(sayx[n])isgeneratedbyimplementingthedynamical cuit (ADQC) [22] is usually a M-qubit quantum state.
mapf onarandomly-takeninitialstate(sayx[n])for(M−1) With the pre-processing explained above, one maps a sam-
1
times. Therefore, a sample x[n] = (x[ 1n],··· ,x[ Mn]) contains ple to a set of vectors, namely from (x[ 1n],··· ,x[ Mn]) to
M featureswithx[n] =ft−1(x[n]),whereft−1meanstoim- (v[n,1],··· ,v[n,M]). Thesevectorsaresubsequentlymapped
t 1 toaproductstateas
plementf for(t−1)times.
The ML model is trained to predict the value of x[n] = M
M+1 (cid:89)
f(x(n);µ)fromthissample. Thegroundtruthofx[n] (gen- ψ[n] = v[n,t]. (B1)
M M+1
erated by the dynamical system itself) is called the label of ⊗t=1
the n-th sample. For different independent simulations, we
In other words, the elements of ψ[n] satisfies ψ[n] =
randomlytake2000trainingsamplesfromthetrainingsetfor s1,···,sM
each value of µ to train the ML models. If not specified, we (cid:81)M t=1v s[n t,t].
takeM =8,andalldatasuchasthebifurcationdiagramand TheADQCrepresentsaunitarytransformation(denotedas
theLyapunovexponentsinthemaintextarecomputedusing Uˆ) that maps the input state to the final state that is usually
thetestingset. entangled. Formally,wehaveΨ[n] =Uˆψ[n]forthestatecor-
A µ-tuned pre-processing is introduced to map each sam- responding to the sample x[n], where Uˆ can be regarded as
ple to a set of vectors. Specifically, the t-th feature of the a (dM ×dM)-dimensional unitary matrix. For a variational
n-th sample x[n] is mapped to a vector v[n,t] as x[n] → quantum circuit including ADQC, Uˆ is written as the prod-
t t
v[n,t] = (v[n,t],v[n,t],··· ,v[n,t]), where the dimension d = uct of multiple local gates. As illustrated in the bottom-left
1 2 d
dim(v[n,t])isapresethyper-parameter. Consequently,asam- panel of Fig. 1 of the main text, we here choose the gates to
betwo-body,whichare(d2×d2)-dimensionalunitarymatri-
pleismappedtoM vectors.
Wedefinethepre-processingmaptodependonµandsome ces(denotedas{Gˆ[g]}forg = 1,··· ,N G,withN G thetotal
variational parameters that will be optimized in the training numberofgates).
stage. Here, the pre-processing map [also see Eq. (4) of the For the 1D Logistic map, the prediction from ADQC is
maintext]isdefinedas givenbythemeasurementonthefinalstateΨ[n],satisfying
v k[n,t] =(cid:88) ξ i(x[ tn];θ)ξ j(µ[n];θ)T ijk. (A1) y˜[n] =|Ψ[ sn 1] ,···,sM−1,0|2. (B2)
i,j
Inthequantumlanguage,y˜[n] istheprobabilityofprojecting
The (d × d × d)-dimensional tensor T and scalar θ are the last qubit to the spin-up state. For the 1D Logistic map,
the variational parameters that are optimized in the training thepredictionsofthetwovariablesaredefinedas
stage. The vectors ξ(x[n];θ) and ξ(µ[n];θ), which are both
t y˜[n] = |Ψ[n] |2, (B3)
d-dimensional, areobtainedbythefollowingmapthattrans- s1,···,sM−2,sM−1,0
formsascalartoanormalizedvector[alsoseeEq.(5)ofthe y˜[n] = |Ψ[n] |2. (B4)
maintext]. Foragivenscalar(saya),thej-thelementsofthe
s1,···,sM−2,0,sM
resultingvectorξ(a;θ)satisfies These are the probabilities of projecting the penultimate and
lastqubitstothespin-upstate,respectively.
(cid:115)
(cid:18) d−1(cid:19) (cid:18)
θπ
(cid:19)d−j (cid:18)
θπ
(cid:19)j−1 For the ADQC, each unitary gate is parameterized by a
ξ j(a;θ)=
j−1
cos
2
a sin
2
a . (d2×d2)-dimensionalmatrixnamedaslatentgates(denoted
as{G[g]}). Inotherwords,thelatentgatesarethevariational
(A2)
parameters of the ADQC. To satisfy the unitary condition of
Since we train a ML model with different values of µ, one
should take µ[n] to be the corresponding value of µ for the
{G[g]},thelatentgatesaremappedtounitarygatesbysingu-
larvaluedecomposition(SVD)as
n-thsample.
For the 2D logistic map, we take the hyper-parameter µ
G[g] S →VD U[g]Λ[g]V[g]†, (B5)
to be the discrete values from 0.51 to 0.9 with an inter-
val of 0.01. For each value of µ, we randomly generate U[g]V[g]† →Gˆ[g]. (B6)6
A main advantage of ADQC is that any unitary gate can with k = 1 or 2, and the (2 × 2) matrix R(t) obtained by
be parameterized in the same way by a matrix (latent gate) implementingQRdecompositionJ(t) → Q(t)R(t). Westill
of the same dimensions. Therefore, we only need to specify takeT =264inoursimulations,whichissufficientlylarge.
thestructureofthecircuit,suchasthedimensionsofthegates
(e.g.,thenumberofspinsthatonegateactsonandthenumber
of levels for each spin) and how they are connected to each AppendixD:Peaksignal-to-noiseratios
other.Forothervariationalquantumcircuitmodels,onehasto
additionallyspecifythetypesofgates(suchasrotationaland Peaksignal-to-noiseratios(PSNR)r [29]isameasureof
P
phase-shiftgates). Differenttypesofgatesareparameterized
thesimilaritybetweentwoimages(2Ddata),whichisapplied
indifferentways[37].
toassess thequalityon denoising andcompression. Consid-
The latent gates (and all other variational parameters) are eringtwoimagesIandI′,ther withdecibel(dB)astheunit
P
updatedbythegradientdecentmethodinthetrainingstages,
isdefinedas
asG[g] →G[g]−η ∂L withηthelearningrate. Wechoose
∂G[g] (cid:32) (cid:33)
L to be the root mean-square error (see Eq. (3) of the main 2552
r =10×lg , (D1)
text). Asthemapfromlatentgatestounitarygatesisdiffer- P 1 (cid:80)W (cid:80)L (I −I′ )2
entiable,onecanusethestandardbackpropagationtechnique WL w=1 l=1 wl wl
inMLtoobtainthegradients.
whereW andLarewidthandlengthoftheimages,and255
isthemaximumvalueofapixel. Inthispaper,r iscomputed
P
with the grey-scale images read as 8-bit data. Taking image
AppendixC:Lyapunovexponent
compressionasanexample,aPSNRforaboutr ≃40dBor
P
larger usually indicates a well-performed compression [29].
TheLyapunovexponent(LE)[30,31]isanimportantmea- The larger the r is, the closer the two images are to each
P
suretodescribethechaoticnatureofadynamicalsystem. It other.
reflects the exponential growth rate of small perturbations in Inthiswork, eachdatapointinabifurcationdiagram[see
thesystem,quantifyingthesensitivitytoinitialconditionsand Fig. 2(a) of the main text] is obtained by iterating the map
theamplificationofuncertainties. (the Logistic map or those of the ML models) for more than
Consider a discrete dynamical system with one variable 200times(whichissufficientlylarge),startingfromarandom
x t+1 = f(x t) (such as the one-dimensional Logistic map). state. We take 500 random states for each value of µ, which
One can assume an exponential form for the difference be- aretakendifferentlyfromthoseofthetrainingsamples. The
tweenthestateswithandwithoutaperturbationεontheini- dense of the data points is indicated by the darkness of the
tialstate,whereonehas colors. PSNR is used to characterize the similarity between
twobifurcationdiagrams.
εeTλ(x0) =|fT(x +ε)−fT(x )|, (C1)
0 0
where fT means to recursively implement the map f for T
times. LE is defined as the exponential index λ in the limit AppendixE:Additionalresultsandhyper-parameters
of ε → 0 and T → ∞. In practical simulations, LE can be
calculatedas TableA1showtheRMSEofLEL byLSTMandADQC
LE
onthe1Dand2DLogisticmaps. Threeobservationscanbe
T (cid:12) (cid:12)
λ= 1 (cid:88) ln(cid:12) (cid:12)df(x;µ)(cid:12) (cid:12) , (C2) made:
T (cid:12) dx (cid:12)
t=1 x=xt 1. L can be significantly reduced for both LSTM and
LE
by taking a sufficiently large T. In this work, we take T = ADQCbyintroducingtheµ-tunedpre-processingmap;
264.
2. Similar L is achieved with the training and testing
For the 2D case, we employ the QR decomposition LE
sets,indicatingsufficientgeneralizationabilityondeal-
method [38, 39] to compute the LE’s. Consider a map with
two variables (x ,x′). The map f (which has two compo- ingwiththeunlearneddata;
t t
nents) can be generally written as x = f (x ,x′) and
t+1 1 t t 3. Withtheµ-tunedpreprocessing,ADQCachievesmuch
x′ = f (x ,x′). The Jacobi matrix at the discrete time t
t+1 2 t t lowerL thanLSTM,implyingthatADQCbettermit-
LE
isdefinedas
igating the over-fitting issue than LSTM. This is also
 ∂f (x,x′) ∂f (x,x′)  consistent with the results reported in the main text
1 1
(Fig.4).
J(t) = ∂x ∂x′  . (C3)
 ∂f (x,x′) ∂f (x,x′) 
2 2 Ifnotspecified,thesettingsofhyper-parametersofLSTM
∂x ∂x′ x=xt,x′=x′ t andADQCusedinthisworkaregiveninTableA2.
ThetwoLE’scanbecalculatesas
λ =
1 (cid:88)T ln(cid:12) (cid:12)R(t)(cid:12)
(cid:12), (C4)
k T (cid:12) k,k(cid:12)
t=17
TABLEA1. TheRMSEoftheLE(L )onthe1Dand2DLogistic TABLEA2. Thehyper-parametersofADQCandLSTMforthe1D
LE
maps obtained by LSTM, ADQC, and those with the µ-tuned pre- and2DLogisticmaps. ForLSTM,d andd representtheinput
in out
processing [dubbed as LSTM (µ) and ADQC (µ)]. Each value of andoutputdimensions,respectively,L representsthelengthofse-
SQ
L isobtainedbytheaverageoffiveindependentsimulations. The quence,N representsthenumberoflayers,andD representsthe
LE L h
standarddeviations(std)arealsoprovided. depth. ForADQC,thehyper-parametersaretakeninthesameway
withorwithouttheµ-tunedpre-processing.
L LSTM LSTM(µ) ADQC ADQC(µ)
LE
Training 0.8278 0.4845 1.1486 0.2688 LSTM d in =1, d out =1, L SQ =8, N L =1, D h =8
1D std 0.3193 0.1280 0.0471 0.0510 1D LSTM(µ) d in =3, d out =1, L SQ =8, N L =1, D h =8
Testing 0.8273 0.4837 1.1486 0.2686 ADQC d=3, M =8, N L =4
std 0.3190 0.1281 0.0471 0.0512 LSTM d in =2, d out =2, L SQ =4, N L =1, D h =8
Training 0.8050 0.2333 1.2195 0.1588 2D LSTM(µ) d in =3, d out =2, L SQ =8, N L =1, D h =8
2D std 0.2320 0.0610 0.1389 0.0584 ADQC d=3, M =8, N L =4
Testing 0.8048 0.2332 1.2189 0.1582
std 0.2317 0.0606 0.1388 0.0583
[1] H. Kantz and T. Schreiber, Nonlinear time series analysis, predictabilityofchaoticmultiscalesystemsviamachinelearn-
Vol.7(Cambridgeuniversitypress,2004). ing,Phys.Rev.E102,052203(2020).
[2] S.H.Strogatz,Nonlineardynamicsandchaoswithstudentso- [15] J.Pathak,Z.Lu,B.R.Hunt,M.Girvan,andE.Ott,Usingma-
lutions manual: With applications to physics, biology, chem- chinelearningtoreplicatechaoticattractorsandcalculateLya-
istry,andengineering(CRCpress,2018). punovexponentsfromdata,Chaos27,121102(2017).
[3] R.M.May,Simplemathematicalmodelswithverycomplicated [16] Z.Lu,B.R.Hunt,andE.Ott,Attractorreconstructionbyma-
dynamics,Nature261,459(1976). chinelearning,Chaos28,061104(2018).
[4] E.N.Lorenz,Deterministicnonperiodicflow,JournalofAtmo- [17] P. Antonik, M. Gulina, J. Pauwels, and S. Massar, Using a
sphericSciences20,130 (1963). reservoircomputertolearnchaoticattractors,withapplications
[5] S. Hochreiter and J. Schmidhuber, Long short-term memory, to chaos synchronization and cryptography, Phys. Rev. E 98,
NeuralComputation9,1735(1997). 012215(2018).
[6] Y.Yu,X.Si,C.Hu,andJ.Zhang,AReviewofRecurrentNeu- [18] H.Fan,J.Jiang,C.Zhang,X.Wang,andY.-C.Lai,Long-term
ralNetworks: LSTMCellsandNetworkArchitectures,Neural predictionofchaoticsystemswithmachinelearning,Phys.Rev.
Computation31,1235(2019). Res.2,012080(2020).
[7] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N. [19] Y.Sun, L.Zhang,andM.Yao,Chaotictimeseriesprediction
Gomez, L. u. Kaiser, and I. Polosukhin, Attention is all you ofnonlinearsystemsbasedonvariousneuralnetworkmodels,
need,in Advancesin NeuralInformation ProcessingSystems, Chaos,Solitons&Fractals175,113971(2023).
Vol.30,editedbyI.Guyon,U.V.Luxburg,S.Bengio,H.Wal- [20] A.Peruzzo,J.McClean,P.Shadbolt,M.-H.Yung,X.-Q.Zhou,
lach,R.Fergus,S.Vishwanathan,andR.Garnett(CurranAs- P.J.Love, A.Aspuru-Guzik,andJ.L.O’Brien,Avariational
sociates,Inc.,2017). eigenvalue solver on a photonic quantum processor, Nature
[8] T. He and J. Droppo, Exploiting lstm structure in deep neu- Communications5,4213(2014).
ral networks for speech recognition, in 2016 IEEE interna- [21] M.Cerezo,A.Arrasmith,R.Babbush,S.C.Benjamin,S.Endo,
tional conference on acoustics, speech and signal processing K. Fujii, J. R. McClean, K. Mitarai, X. Yuan, L. Cincio, and
(ICASSP)(IEEE,2016)pp.5445–5449. P. J. Coles, Variational quantum algorithms, Nature Reviews
[9] S.BhaskarandT.T.M.,Lstmmodelforvisualspeechrecogni- Physics3,625(2021).
tionthroughfacialexpressions,MultimediaToolsandApplica- [22] P.-F.Zhou,R.Hong,andS.-J.Ran,Automaticallydifferentiable
tions82,5455(2023). quantumcircuitformany-qubitstatepreparation,Phys.Rev.A
[10] K.Bi,L.Xie,H.Zhang,X.Chen,X.Gu,andQ.Tian,Accu- 104,042601(2021).
rate medium-range global weather forecasting with 3d neural [23] J.-M.Yuan,M.Tung,D.H.Feng,andL.M.Narducci,Instabil-
networks,Nature619,533(2023). ityandirregularbehaviorofcoupledlogisticequations,Phys.
[11] L.Chen,X.Zhong,F.Zhang,Y.Cheng,Y.Xu,Y.Qi,andH.Li, Rev.A28,1662(1983).
Fuxi:acascademachinelearningforecastingsystemfor15-day [24] A.FerrettiandN.Rahman,Astudyofcoupledlogisticmapand
globalweatherforecast,npjClimateandAtmosphericScience itsapplicationsinchemicalphysics,ChemicalPhysics119,275
6,190(2023). (1988).
[12] J. Pathak, B. Hunt, M. Girvan, Z. Lu, and E. Ott, Model-free [25] Z. C. Lipton, J. Berkowitz, and C. Elkan, A critical review
predictionoflargespatiotemporallychaoticsystemsfromdata: of recurrent neural networks for sequence learning (2015),
Areservoircomputingapproach,Phys.Rev.Lett.120,024102 arXiv:1506.00019[cs.LG].
(2018). [26] H. Salehinejad, S. Sankar, J. Barfett, E. Colak, and
[13] S. Herzog, F. Wo¨rgo¨tter, and U. Parlitz, Convolutional au- S.Valaee,Recentadvancesinrecurrentneuralnetworks(2018),
toencoderandconditionalrandomfieldshybridforpredicting arXiv:1801.01078[cs.NE].
spatial-temporalchaos,Chaos29,123116(2019). [27] E.StoudenmireandD.J.Schwab,Supervisedlearningwithten-
[14] F. Borra, A. Vulpiani, and M. Cencini, Effective models and sor networks, in Advances in Neural Information Processing8
Systems,Vol.29,editedbyD.Lee,M.Sugiyama,U.Luxburg, [34] S. Cheng, L. Wang, T. Xiang, and P. Zhang, Tree tensor
I.Guyon,andR.Garnett(CurranAssociates,Inc.,2016). networks for generative modeling, Phys. Rev. B 99, 155131
[28] M.Andrecut,Logisticmapasarandomnumbergenerator,In- (2019).
ternationalJournalofModernPhysicsB12,921(1998). [35] Z.-F. Gao, S. Cheng, R.-Q. He, Z. Y. Xie, H.-H. Zhao, Z.-Y.
[29] Z.KotevskiandP.Mitrevski,Experimentalcomparisonofpsnr Lu,andT.Xiang,Compressingdeepneuralnetworksbymatrix
andssimmetricsforvideoqualityestimation,inICTInnova- productoperators,Phys.Rev.Res.2,023300(2020).
tions 2009, edited by D. Davcev and J. M. Go´mez (Springer [36] Y.Qing,P.-F.Zhou,K.Li,andS.-J.Ran,Compressingneural
BerlinHeidelberg,Berlin,Heidelberg,2010)pp.357–366. networkbytensornetworkwithexponentiallyfewervariational
[30] T. S. Parker and L. Chua, Practical numerical algorithms for parameters(2023),arXiv:2305.06058[cs.LG].
chaoticsystems(SpringerScience&BusinessMedia,2012). [37] S.S.Gill,A.Kumar,H.Singh,M.Singh,K.Kaur,M.Usman,
[31] F.C.Moon,Chaoticandfractaldynamics:introductionforap- and R. Buyya, Quantum computing: A taxonomy, systematic
pliedscientistsandengineers(JohnWiley&Sons,2008). reviewandfuturedirections,Software:PracticeandExperience
[32] B. Bezruchko and D. Smirnov, Extracting Knowledge From 52,66(2022).
Time Series: An Introduction to Nonlinear Empirical Model- [38] K.Geist,U.Parlitz,andW.Lauterborn,ComparisonofDiffer-
ing,SpringerSeriesinSynergetics(SpringerBerlinHeidelberg, entMethodsforComputingLyapunovExponents,Progressof
2010)pp.56–57. TheoreticalPhysics83,875(1990).
[33] Z.-Y.Han,J.Wang,H.Fan,L.Wang,andP.Zhang,Unsuper- [39] E. J. McDonald and D. J. Higham, Error analysis of qr algo-
vised generative modeling using matrix product states, Phys. rithmsforcomputinglyapunovexponents,ETNA-Electronic
Rev.X8,031012(2018). TransactionsonNumericalAnalysis12,234(2001).