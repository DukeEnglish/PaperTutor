[
    {
        "title": "Beyond Theorems: A Counterexample to Potential Markov Game Criteria",
        "authors": "Fatemeh FardnoSeyed Majid Zahedi",
        "links": "http://arxiv.org/abs/2405.08206v1",
        "entry_id": "http://arxiv.org/abs/2405.08206v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08206v1",
        "summary": "There are only limited classes of multi-player stochastic games in which\nindependent learning is guaranteed to converge to a Nash equilibrium. Markov\npotential games are a key example of such classes. Prior work has outlined sets\nof sufficient conditions for a stochastic game to qualify as a Markov potential\ngame. However, these conditions often impose strict limitations on the game's\nstructure and tend to be challenging to verify. To address these limitations,\nMguni et al. [12] introduce a relaxed notion of Markov potential games and\noffer an alternative set of necessary conditions for categorizing stochastic\ngames as potential games. Under these conditions, the authors claim that a\ndeterministic Nash equilibrium can be computed efficiently by solving a dual\nMarkov decision process. In this paper, we offer evidence refuting this claim\nby presenting a counterexample.",
        "updated": "2024-05-13 21:49:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08206v1"
    },
    {
        "title": "Random walk model that universally generates inverse square Lévy walk by eliminating search cost minimization constraint",
        "authors": "Shuji ShinoharaDaiki MoritaHayato HiraiRyosuke KuribayashiNobuhito ManomeToru MoriyamaHiroshi OkamotoYoshihiro NakajimaPegio-Yukio GunjiUng-il Chung",
        "links": "http://arxiv.org/abs/2405.07541v2",
        "entry_id": "http://arxiv.org/abs/2405.07541v2",
        "pdf_url": "http://arxiv.org/pdf/2405.07541v2",
        "summary": "The L\\'evy walk, a type of random walk characterized by linear step lengths\nthat follow a power-law distribution, is observed in the migratory behaviors of\nvarious organisms, ranging from bacteria to humans. Notably, L\\'evy walks with\npower exponents close to two are frequently observed, though their underlying\ncauses remain elusive. This study introduces a simplified, abstract random walk\nmodel designed to produce inverse square L\\'evy walks, also known as Cauchy\nwalks and explores the conditions that facilitate these phenomena. In our\nmodel, agents move toward a randomly selected destination in multi-dimensional\nspace, and their movement strategy is parameterized by the extent to which they\npursue the shortest path. When the search cost is proportional to the distance\ntraveled, this parameter effectively reflects the emphasis on minimizing search\ncosts. Our findings reveal that strict adherence to this cost minimization\nconstraint results in a Brownian walk pattern. However, removing this\nconstraint transitions the movement to an inverse square L\\'evy walk.\nTherefore, by modulating the prioritization of search costs, our model can\nseamlessly alternate between Brownian and Cauchy walk dynamics. This model has\nthe potential to be utilized for exploring the parameter space of an\noptimization problem.",
        "updated": "2024-05-14 01:44:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07541v2"
    },
    {
        "title": "MAxPrototyper: A Multi-Agent Generation System for Interactive User Interface Prototyping",
        "authors": "Mingyue YuanJieshan ChenAaron Quigley",
        "links": "http://arxiv.org/abs/2405.07131v1",
        "entry_id": "http://arxiv.org/abs/2405.07131v1",
        "pdf_url": "http://arxiv.org/pdf/2405.07131v1",
        "summary": "In automated user interactive design, designers face key challenges,\nincluding accurate representation of user intent, crafting high-quality\ncomponents, and ensuring both aesthetic and semantic consistency. Addressing\nthese challenges, we introduce MAxPrototyper, our human-centered, multi-agent\nsystem for interactive design generation. The core of MAxPrototyper is a theme\ndesign agent. It coordinates with specialized sub-agents, each responsible for\ngenerating specific parts of the design. Through an intuitive online interface,\nusers can control the design process by providing text descriptions and layout.\nEnhanced by improved language and image generation models, MAxPrototyper\ngenerates each component with careful detail and contextual understanding. Its\nmulti-agent architecture enables a multi-round interaction capability between\nthe system and users, facilitating precise and customized design adjustments\nthroughout the creation process.",
        "updated": "2024-05-12 01:57:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.07131v1"
    },
    {
        "title": "(A Partial Survey of) Decentralized, Cooperative Multi-Agent Reinforcement Learning",
        "authors": "Christopher Amato",
        "links": "http://arxiv.org/abs/2405.06161v1",
        "entry_id": "http://arxiv.org/abs/2405.06161v1",
        "pdf_url": "http://arxiv.org/pdf/2405.06161v1",
        "summary": "Multi-agent reinforcement learning (MARL) has exploded in popularity in\nrecent years. Many approaches have been developed but they can be divided into\nthree main types: centralized training and execution (CTE), centralized\ntraining for decentralized execution (CTDE), and Decentralized training and\nexecution (DTE).\n  Decentralized training and execution methods make the fewest assumptions and\nare often simple to implement. In fact, as I'll discuss, any single-agent RL\nmethod can be used for DTE by just letting each agent learn separately. Of\ncourse, there are pros and cons to such approaches as we discuss below. It is\nworth noting that DTE is required if no offline coordination is available. That\nis, if all agents must learn during online interactions without prior\ncoordination, learning and execution must both be decentralized. DTE methods\ncan be applied in cooperative, competitive, or mixed cases but this text will\nfocus on the cooperative MARL case.\n  In this text, I will first give a brief description of the cooperative MARL\nproblem in the form of the Dec-POMDP. Then, I will discuss value-based DTE\nmethods starting with independent Q-learning and its extensions and then\ndiscuss the extension to the deep case with DQN, the additional complications\nthis causes, and methods that have been developed to (attempt to) address these\nissues. Next, I will discuss policy gradient DTE methods starting with\nindependent REINFORCE (i.e., vanilla policy gradient), and then extending to\nthe actor-critic case and deep variants (such as independent PPO). Finally, I\nwill discuss some general topics related to DTE and future directions.",
        "updated": "2024-05-10 00:50:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.06161v1"
    },
    {
        "title": "Federated Combinatorial Multi-Agent Multi-Armed Bandits",
        "authors": "Fares FouratiMohamed-Slim AlouiniVaneet Aggarwal",
        "links": "http://arxiv.org/abs/2405.05950v1",
        "entry_id": "http://arxiv.org/abs/2405.05950v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05950v1",
        "summary": "This paper introduces a federated learning framework tailored for online\ncombinatorial optimization with bandit feedback. In this setting, agents select\nsubsets of arms, observe noisy rewards for these subsets without accessing\nindividual arm information, and can cooperate and share information at specific\nintervals. Our framework transforms any offline resilient single-agent\n$(\\alpha-\\epsilon)$-approximation algorithm, having a complexity of\n$\\tilde{\\mathcal{O}}(\\frac{\\psi}{\\epsilon^\\beta})$, where the logarithm is\nomitted, for some function $\\psi$ and constant $\\beta$, into an online\nmulti-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of\nno more than $\\tilde{\\mathcal{O}}(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta}\nT^\\frac{2+\\beta}{3+\\beta})$. This approach not only eliminates the $\\epsilon$\napproximation error but also ensures sublinear growth with respect to the time\nhorizon $T$ and demonstrates a linear speedup with an increasing number of\ncommunicating agents. Additionally, the algorithm is notably\ncommunication-efficient, requiring only a sublinear number of communication\nrounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi\nT^\\frac{\\beta}{\\beta+1}\\right)$. Furthermore, the framework has been\nsuccessfully applied to online stochastic submodular maximization using various\noffline algorithms, yielding the first results for both single-agent and\nmulti-agent settings and recovering specialized single-agent theoretical\nguarantees. We empirically validate our approach to a stochastic data\nsummarization problem, illustrating the effectiveness of the proposed\nframework, even in single-agent scenarios.",
        "updated": "2024-05-09 17:40:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05950v1"
    }
]