[
    {
        "title": "Addressing Misspecification in Simulation-based Inference through Data-driven Calibration",
        "authors": "Antoine WehenkelJuan L. GamellaOzan SenerJens BehrmannGuillermo SapiroMarco CuturiJörn-Henrik Jacobsen",
        "links": "http://arxiv.org/abs/2405.08719v1",
        "entry_id": "http://arxiv.org/abs/2405.08719v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08719v1",
        "summary": "Driven by steady progress in generative modeling, simulation-based inference\n(SBI) has enabled inference over stochastic simulators. However, recent work\nhas demonstrated that model misspecification can harm SBI's reliability. This\nwork introduces robust posterior estimation (ROPE), a framework that overcomes\nmodel misspecification with a small real-world calibration set of ground truth\nparameter measurements. We formalize the misspecification gap as the solution\nof an optimal transport problem between learned representations of real-world\nand simulated observations. Assuming the prior distribution over the parameters\nof interest is known and well-specified, our method offers a controllable\nbalance between calibrated uncertainty and informative inference under all\npossible misspecifications of the simulator. Our empirical results on four\nsynthetic tasks and two real-world problems demonstrate that ROPE outperforms\nbaselines and consistently returns informative and calibrated credible\nintervals.",
        "updated": "2024-05-14 16:04:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08719v1"
    },
    {
        "title": "Weakly-supervised causal discovery based on fuzzy knowledge and complex data complementarity",
        "authors": "Wenrui LiWei ZhangQinghao ZhangXuegong ZhangXiaowo Wang",
        "links": "http://arxiv.org/abs/2405.08699v1",
        "entry_id": "http://arxiv.org/abs/2405.08699v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08699v1",
        "summary": "Causal discovery based on observational data is important for deciphering the\ncausal mechanism behind complex systems. However, the effectiveness of existing\ncausal discovery methods is limited due to inferior prior knowledge, domain\ninconsistencies, and the challenges of high-dimensional datasets with small\nsample sizes. To address this gap, we propose a novel weakly-supervised fuzzy\nknowledge and data co-driven causal discovery method named KEEL. KEEL adopts a\nfuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge,\nand forms corresponding weakened constraints. This schema not only lessens the\ndependency on expertise but also allows various types of limited and\nerror-prone fuzzy knowledge to guide causal discovery. It can enhance the\ngeneralization and robustness of causal discovery, especially in\nhigh-dimensional and small-sample scenarios. In addition, we integrate the\nextended linear causal model (ELCM) into KEEL for dealing with the\nmulti-distribution and incomplete data. Extensive experiments with different\ndatasets demonstrate the superiority of KEEL over several state-of-the-art\nmethods in accuracy, robustness and computational efficiency. For causal\ndiscovery in real protein signal transduction processes, KEEL outperforms the\nbenchmark method with limited data. In summary, KEEL is effective to tackle the\ncausal discovery tasks with higher accuracy while alleviating the requirement\nfor extensive domain expertise.",
        "updated": "2024-05-14 15:39:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08699v1"
    },
    {
        "title": "Simplifying Debiased Inference via Automatic Differentiation and Probabilistic Programming",
        "authors": "Alex Luedtke",
        "links": "http://arxiv.org/abs/2405.08675v1",
        "entry_id": "http://arxiv.org/abs/2405.08675v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08675v1",
        "summary": "We introduce an algorithm that simplifies the construction of efficient\nestimators, making them accessible to a broader audience. 'Dimple' takes as\ninput computer code representing a parameter of interest and outputs an\nefficient estimator. Unlike standard approaches, it does not require users to\nderive a functional derivative known as the efficient influence function.\nDimple avoids this task by applying automatic differentiation to the\nstatistical functional of interest. Doing so requires expressing this\nfunctional as a composition of primitives satisfying a novel differentiability\ncondition. Dimple also uses this composition to determine the nuisances it must\nestimate. In software, primitives can be implemented independently of one\nanother and reused across different estimation problems. We provide a\nproof-of-concept Python implementation and showcase through examples how it\nallows users to go from parameter specification to efficient estimation with\njust a few lines of code.",
        "updated": "2024-05-14 14:56:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08675v1"
    },
    {
        "title": "Learning Decision Policies with Instrumental Variables through Double Machine Learning",
        "authors": "Daqian ShaoAshkan SoleymaniFrancesco QuinzanMarta Kwiatkowska",
        "links": "http://arxiv.org/abs/2405.08498v1",
        "entry_id": "http://arxiv.org/abs/2405.08498v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08498v1",
        "summary": "A common issue in learning decision-making policies in data-rich settings is\nspurious correlations in the offline dataset, which can be caused by hidden\nconfounders. Instrumental variable (IV) regression, which utilises a key\nunconfounded variable known as the instrument, is a standard technique for\nlearning causal relationships between confounded action, outcome, and context\nvariables. Most recent IV regression algorithms use a two-stage approach, where\na deep neural network (DNN) estimator learnt in the first stage is directly\nplugged into the second stage, in which another DNN is used to estimate the\ncausal effect. Naively plugging the estimator can cause heavy bias in the\nsecond stage, especially when regularisation bias is present in the first stage\nestimator. We propose DML-IV, a non-linear IV regression method that reduces\nthe bias in two-stage IV regressions and effectively learns high-performing\npolicies. We derive a novel learning objective to reduce bias and design the\nDML-IV algorithm following the double/debiased machine learning (DML)\nframework. The learnt DML-IV estimator has strong convergence rate and\n$O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is\nunconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV\nregression benchmarks and learns high-performing policies in the presence of\ninstruments.",
        "updated": "2024-05-14 10:55:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08498v1"
    },
    {
        "title": "Universal replication of chaotic characteristics by classical and quantum machine learning",
        "authors": "Sheng-Chen BaiShi-Ju Ran",
        "links": "http://arxiv.org/abs/2405.08484v1",
        "entry_id": "http://arxiv.org/abs/2405.08484v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08484v1",
        "summary": "Replicating chaotic characteristics of non-linear dynamics by machine\nlearning (ML) has recently drawn wide attentions. In this work, we propose that\na ML model, trained to predict the state one-step-ahead from several latest\nhistoric states, can accurately replicate the bifurcation diagram and the\nLyapunov exponents of discrete dynamic systems. The characteristics for\ndifferent values of the hyper-parameters are captured universally by a single\nML model, while the previous works considered training the ML model\nindependently by fixing the hyper-parameters to be specific values. Our\nbenchmarks on the one- and two-dimensional Logistic maps show that variational\nquantum circuit can reproduce the long-term characteristics with higher\naccuracy than the long short-term memory (a well-recognized classical ML\nmodel). Our work reveals an essential difference between the ML for the chaotic\ncharacteristics and that for standard tasks, from the perspective of the\nrelation between performance and model complexity. Our results suggest that\nquantum circuit model exhibits potential advantages on mitigating over-fitting,\nachieving higher accuracy and stability.",
        "updated": "2024-05-14 10:12:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08484v1"
    }
]