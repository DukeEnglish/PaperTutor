[
    {
        "title": "The RoboDrive Challenge: Drive Anytime Anywhere in Any Condition",
        "authors": "Lingdong KongShaoyuan XieHanjiang HuYaru NiuWei Tsang OoiBenoit R. CottereauLai Xing NgYuexin MaWenwei ZhangLiang PanKai ChenZiwei LiuWeichao QiuWei ZhangXu CaoHao LuYing-Cong ChenCaixin KangXinning ZhouChengyang YingWentao ShangXingxing WeiYinpeng DongBo YangShengyin JiangZeliang MaDengyi JiHaiwen LiXingliang HuangYu TianGenghua KouFan JiaYingfei LiuTiancai WangYing LiXiaoshuai HaoYifan YangHui ZhangMengchuan WeiYi ZhouHaimei ZhaoJing ZhangJinke LiXiao HeXiaoqiang ChengBingyang ZhangLirong ZhaoDianlei DingFangsheng LiuYixiang YanHongming WangNanfei YeLun LuoYubo TianYiwei ZuoZhe CaoYi RenYunfan LiWenjie LiuXun WuYifan MaoMing LiJian LiuJiayang LiuZihan QinCunxi ChuJialei XuWenbo ZhaoJunjun JiangXianming LiuZiyan WangChiwei LiShilong LiChendong YuanSongyue YangWentao LiuPeng ChenBin ZhouYubo WangChi ZhangJianhang SunHai ChenXiao YangLizhong WangDongyi FuYongchun LinHuitong YangHaoang LiYadan LuoXianjing ChengYong Xu",
        "links": "http://arxiv.org/abs/2405.08816v1",
        "entry_id": "http://arxiv.org/abs/2405.08816v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08816v1",
        "summary": "In the realm of autonomous driving, robust perception under\nout-of-distribution conditions is paramount for the safe deployment of\nvehicles. Challenges such as adverse weather, sensor malfunctions, and\nenvironmental unpredictability can severely impact the performance of\nautonomous systems. The 2024 RoboDrive Challenge was crafted to propel the\ndevelopment of driving perception technologies that can withstand and adapt to\nthese real-world variabilities. Focusing on four pivotal tasks -- BEV\ndetection, map segmentation, semantic occupancy prediction, and multi-view\ndepth estimation -- the competition laid down a gauntlet to innovate and\nenhance system resilience against typical and atypical disturbances. This\nyear's challenge consisted of five distinct tracks and attracted 140 registered\nteams from 93 institutes across 11 countries, resulting in nearly one thousand\nsubmissions evaluated through our servers. The competition culminated in 15\ntop-performing solutions, which introduced a range of innovative approaches\nincluding advanced data augmentation, multi-sensor fusion, self-supervised\nlearning for error correction, and new algorithmic strategies to enhance sensor\nrobustness. These contributions significantly advanced the state of the art,\nparticularly in handling sensor inconsistencies and environmental variability.\nParticipants, through collaborative efforts, pushed the boundaries of current\ntechnologies, showcasing their potential in real-world scenarios. Extensive\nevaluations and analyses provided insights into the effectiveness of these\nsolutions, highlighting key trends and successful strategies for improving the\nresilience of driving perception systems. This challenge has set a new\nbenchmark in the field, providing a rich repository of techniques expected to\nguide future research in this field.",
        "updated": "2024-05-14 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08816v1"
    },
    {
        "title": "Efficient Vision-Language Pre-training by Cluster Masking",
        "authors": "Zihao WeiZixuan PanAndrew Owens",
        "links": "http://arxiv.org/abs/2405.08815v1",
        "entry_id": "http://arxiv.org/abs/2405.08815v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08815v1",
        "summary": "We propose a simple strategy for masking image patches during visual-language\ncontrastive learning that improves the quality of the learned representations\nand the training speed. During each iteration of training, we randomly mask\nclusters of visually similar image patches, as measured by their raw pixel\nintensities. This provides an extra learning signal, beyond the contrastive\ntraining itself, since it forces a model to predict words for masked visual\nstructures solely from context. It also speeds up training by reducing the\namount of data used in each image. We evaluate the effectiveness of our model\nby pre-training on a number of benchmarks, finding that it outperforms other\nmasking strategies, such as FLIP, on the quality of the learned representation.",
        "updated": "2024-05-14 17:59:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08815v1"
    },
    {
        "title": "CinePile: A Long Video Question Answering Dataset and Benchmark",
        "authors": "Ruchit RawalKhalid SaifullahRonen BasriDavid JacobsGowthami SomepalliTom Goldstein",
        "links": "http://arxiv.org/abs/2405.08813v1",
        "entry_id": "http://arxiv.org/abs/2405.08813v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08813v1",
        "summary": "Current datasets for long-form video understanding often fall short of\nproviding genuine long-form comprehension challenges, as many tasks derived\nfrom these datasets can be successfully tackled by analyzing just one or a few\nrandom frames from a video. To address this issue, we present a novel dataset\nand benchmark, CinePile, specifically designed for authentic long-form video\nunderstanding. This paper details our innovative approach for creating a\nquestion-answer dataset, utilizing advanced LLMs with human-in-the-loop and\nbuilding upon human-generated raw data. Our comprehensive dataset comprises\n305,000 multiple-choice questions (MCQs), covering various visual and\nmultimodal aspects, including temporal comprehension, understanding\nhuman-object interactions, and reasoning about events or actions within a\nscene. Additionally, we evaluate recent video-centric LLMs, both open-source\nand proprietary, on the test split of our dataset. The findings reveal that\neven state-of-the-art video-centric LLMs significantly lag behind human\nperformance in these tasks, highlighting the complexity and challenge inherent\nin video understanding. The dataset is available at\nhttps://hf.co/datasets/tomg-group-umd/cinepile",
        "updated": "2024-05-14 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08813v1"
    },
    {
        "title": "SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation",
        "authors": "Jonathan RobertsKai HanNeil HoulsbySamuel Albanie",
        "links": "http://arxiv.org/abs/2405.08807v1",
        "entry_id": "http://arxiv.org/abs/2405.08807v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08807v1",
        "summary": "Large multimodal models (LMMs) have proven flexible and generalisable across\nmany tasks and fields. Although they have strong potential to aid scientific\nresearch, their capabilities in this domain are not well characterised. A key\naspect of scientific research is the ability to understand and interpret\nfigures, which serve as a rich, compressed source of complex information. In\nthis work, we present SciFIBench, a scientific figure interpretation benchmark.\nOur main benchmark consists of a 1000-question gold set of multiple-choice\nquestions split between two tasks across 12 categories. The questions are\ncurated from CS arXiv paper figures and captions, using adversarial filtering\nto find hard negatives and human verification for quality control. We evaluate\n26 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we\ninvestigate the alignment and reasoning faithfulness of the LMMs on augmented\nquestion sets from our benchmark. We release SciFIBench to encourage progress\nin this domain.",
        "updated": "2024-05-14 17:54:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08807v1"
    },
    {
        "title": "Ambiguous Annotations: When is a Pedestrian not a Pedestrian?",
        "authors": "Luisa SchwirtenJannes ScholzDaniel KondermannJanis Keuper",
        "links": "http://arxiv.org/abs/2405.08794v1",
        "entry_id": "http://arxiv.org/abs/2405.08794v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08794v1",
        "summary": "Datasets labelled by human annotators are widely used in the training and\ntesting of machine learning models. In recent years, researchers are\nincreasingly paying attention to label quality. However, it is not always\npossible to objectively determine whether an assigned label is correct or not.\nThe present work investigates this ambiguity in the annotation of autonomous\ndriving datasets as an important dimension of data quality. Our experiments\nshow that excluding highly ambiguous data from the training improves model\nperformance of a state-of-the-art pedestrian detector in terms of LAMR,\nprecision and F1 score, thereby saving training time and annotation costs.\nFurthermore, we demonstrate that, in order to safely remove ambiguous instances\nand ensure the retained representativeness of the training data, an\nunderstanding of the properties of the dataset and class under investigation is\ncrucial.",
        "updated": "2024-05-14 17:44:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08794v1"
    }
]