[
    {
        "title": "Towards Enhanced RAC Accessibility: Leveraging Datasets and LLMs",
        "authors": "Edison Jair Bejarano SepulvedaNicolai Potes HectorSantiago Pineda MontoyaFelipe Ivan RodriguezJaime Enrique OrduyAlec Rosales CabezasDanny Traslaviña NavarreteSergio Madrid Farfan",
        "links": "http://arxiv.org/abs/2405.08792v1",
        "entry_id": "http://arxiv.org/abs/2405.08792v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08792v1",
        "summary": "This paper explores the potential of large language models (LLMs) to make the\nAeronautical Regulations of Colombia (RAC) more accessible. Given the\ncomplexity and extensive technicality of the RAC, this study introduces a novel\napproach to simplifying these regulations for broader understanding. By\ndeveloping the first-ever RAC database, which contains 24,478 expertly labeled\nquestion-and-answer pairs, and fine-tuning LLMs specifically for RAC\napplications, the paper outlines the methodology for dataset assembly,\nexpert-led annotation, and model training. Utilizing the Gemma1.1 2b model\nalong with advanced techniques like Unsloth for efficient VRAM usage and flash\nattention mechanisms, the research aims to expedite training processes. This\ninitiative establishes a foundation to enhance the comprehensibility and\naccessibility of RAC, potentially benefiting novices and reducing dependence on\nexpert consultations for navigating the aviation industry's regulatory\nlandscape.\n  You can visit the dataset\n(https://huggingface.co/somosnlp/gemma-1.1-2b-it_ColombiaRAC_FullyCurated_format_chatML_V1)\nand the model\n(https://huggingface.co/datasets/somosnlp/ColombiaRAC_FullyCurated) here.",
        "updated": "2024-05-14 17:41:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08792v1"
    },
    {
        "title": "Kolmogorov-Arnold Networks (KANs) for Time Series Analysis",
        "authors": "Cristian J. Vaca-RubioLuis BlancoRoberto PereiraMàrius Caus",
        "links": "http://arxiv.org/abs/2405.08790v1",
        "entry_id": "http://arxiv.org/abs/2405.08790v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08790v1",
        "summary": "This paper introduces a novel application of Kolmogorov-Arnold Networks\n(KANs) to time series forecasting, leveraging their adaptive activation\nfunctions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold\nrepresentation theorem, KANs replace traditional linear weights with\nspline-parametrized univariate functions, allowing them to learn activation\npatterns dynamically. We demonstrate that KANs outperforms conventional\nMulti-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting\ntask, providing more accurate results with considerably fewer number of\nlearnable parameters. We also provide an ablation study of KAN-specific\nparameters impact on performance. The proposed approach opens new avenues for\nadaptive forecasting models, emphasizing the potential of KANs as a powerful\ntool in predictive analytics.",
        "updated": "2024-05-14 17:38:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08790v1"
    },
    {
        "title": "Harnessing the power of longitudinal medical imaging for eye disease prognosis using Transformer-based sequence modeling",
        "authors": "Gregory HolsteMingquan LinRuiwen ZhouFei WangLei LiuQi YanSarah H. Van TasselKyle KovacsEmily Y. ChewZhiyong LuZhangyang WangYifan Peng",
        "links": "http://arxiv.org/abs/2405.08780v1",
        "entry_id": "http://arxiv.org/abs/2405.08780v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08780v1",
        "summary": "Deep learning has enabled breakthroughs in automated diagnosis from medical\nimaging, with many successful applications in ophthalmology. However, standard\nmedical image classification approaches only assess disease presence at the\ntime of acquisition, neglecting the common clinical setting of longitudinal\nimaging. For slow, progressive eye diseases like age-related macular\ndegeneration (AMD) and primary open-angle glaucoma (POAG), patients undergo\nrepeated imaging over time to track disease progression and forecasting the\nfuture risk of developing disease is critical to properly plan treatment. Our\nproposed Longitudinal Transformer for Survival Analysis (LTSA) enables dynamic\ndisease prognosis from longitudinal medical imaging, modeling the time to\ndisease from sequences of fundus photography images captured over long,\nirregular time periods. Using longitudinal imaging data from the Age-Related\nEye Disease Study (AREDS) and Ocular Hypertension Treatment Study (OHTS), LTSA\nsignificantly outperformed a single-image baseline in 19/20 head-to-head\ncomparisons on late AMD prognosis and 18/20 comparisons on POAG prognosis. A\ntemporal attention analysis also suggested that, while the most recent image is\ntypically the most influential, prior imaging still provides additional\nprognostic value.",
        "updated": "2024-05-14 17:15:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08780v1"
    },
    {
        "title": "EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training",
        "authors": "Yulin WangYang YueRui LuYizeng HanShiji SongGao Huang",
        "links": "http://arxiv.org/abs/2405.08768v1",
        "entry_id": "http://arxiv.org/abs/2405.08768v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08768v1",
        "summary": "The superior performance of modern visual backbones usually comes with a\ncostly training procedure. We contribute to this issue by generalizing the idea\nof curriculum learning beyond its original formulation, i.e., training models\nusing easier-to-harder data. Specifically, we reformulate the training\ncurriculum as a soft-selection function, which uncovers progressively more\ndifficult patterns within each example during training, instead of performing\neasier-to-harder sample selection. Our work is inspired by an intriguing\nobservation on the learning dynamics of visual backbones: during the earlier\nstages of training, the model predominantly learns to recognize some\n'easier-to-learn' discriminative patterns in the data. These patterns, when\nobserved through frequency and spatial domains, incorporate lower-frequency\ncomponents, and the natural image contents without distortion or data\naugmentation. Motivated by these findings, we propose a curriculum where the\nmodel always leverages all the training data at every learning stage, yet the\nexposure to the 'easier-to-learn' patterns of each example is initiated first,\nwith harder patterns gradually introduced as training progresses. To implement\nthis idea in a computationally efficient way, we introduce a cropping operation\nin the Fourier spectrum of the inputs, enabling the model to learn from only\nthe lower-frequency components. Then we show that exposing the contents of\nnatural images can be readily achieved by modulating the intensity of data\naugmentation. Finally, we integrate these aspects and design curriculum\nschedules with tailored search algorithms. The resulting method,\nEfficientTrain++, is simple, general, yet surprisingly effective. It reduces\nthe training time of a wide variety of popular models by 1.5-3.0x on\nImageNet-1K/22K without sacrificing accuracy. It also demonstrates efficacy in\nself-supervised learning (e.g., MAE).",
        "updated": "2024-05-14 17:00:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08768v1"
    },
    {
        "title": "Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Intent Resolution in LLMs",
        "authors": "Akhila YerukolaSaujas VaduguruDaniel FriedMaarten Sap",
        "links": "http://arxiv.org/abs/2405.08760v1",
        "entry_id": "http://arxiv.org/abs/2405.08760v1",
        "pdf_url": "http://arxiv.org/pdf/2405.08760v1",
        "summary": "Humans often express their communicative intents indirectly or non-literally,\nwhich requires their interlocutors -- human or AI -- to understand beyond the\nliteral meaning of words. While most existing work has focused on\ndiscriminative evaluations, we present a new approach to generatively evaluate\nlarge language models' (LLMs') intention understanding by examining their\nresponses to non-literal utterances. Ideally, an LLM should respond in line\nwith the true intention of a non-literal utterance, not its literal\ninterpretation. Our findings show that LLMs struggle to generate pragmatically\nrelevant responses to non-literal language, achieving only 50-55% accuracy on\naverage. While explicitly providing oracle intentions significantly improves\nperformance (e.g., 75% for Mistral-Instruct), this still indicates challenges\nin leveraging given intentions to produce appropriate responses. Using\nchain-of-thought to make models spell out intentions yields much smaller gains\n(60% for Mistral-Instruct). These findings suggest that LLMs are not yet\neffective pragmatic interlocutors, highlighting the need for better approaches\nfor modeling intentions and utilizing them for pragmatic generation.",
        "updated": "2024-05-14 16:48:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.08760v1"
    }
]