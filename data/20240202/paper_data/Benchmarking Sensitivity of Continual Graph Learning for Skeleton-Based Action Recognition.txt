Benchmarking sensitivity of continual graph learning for skeleton-based
action recognition
WeiWei1 a,TomDeSchepper1 b andKevinMets2 c
1UniversityofAntwerp-imec,IDLab,DepartmentofComputerScience,Sint-Pietersvliet7,2000Antwerp,Belgium
2UniversityofAntwerp-imec,IDLab,FacultyofAppliedEngineering,Sint-Pietersvliet7,2000Antwerp,Belgium
{wei.wei,tom.deschepper,kevin.mets}@uantwerpen.be
Keywords: ContinualGraphLearning,ActionRecognition,Spatio-TemporalGraph,SensitivityAnalysis,Benchmark.
Abstract: Continuallearning(CL)istheresearchfieldthataimstobuildmachinelearningmodelsthatcanaccumulate
knowledgecontinuouslyoverdifferenttaskswithoutretrainingfromscratch. Previousstudieshaveshown
thatpre-traininggraphneuralnetworks(GNN)mayleadtonegativetransfer(Huetal.,2020)afterfine-tuning,
asettingwhichiscloselyrelatedtoCL.Thus,wefocusonstudyingGNNinthecontinualgraphlearning
(CGL)setting.Weproposethefirstcontinualgraphlearningbenchmarkforspatio-temporalgraphsanduseit
tobenchmarkwell-knownCGLmethodsinthisnovelsetting.ThebenchmarkisbasedontheN-UCLAand
NTU-RGB+Ddatasetsforskeleton-basedactionrecognition.Beyondbenchmarkingforstandardperformance
metrics,westudytheclassandtask-ordersensitivityofCGLmethods,i.e.,theimpactoflearningorderon
eachclass/task’sperformance,andthearchitecturalsensitivityofCGLmethodswithbackboneGNNatvarious
widthsanddepths. Werevealthattask-orderrobustmethodscanstillbeclass-ordersensitiveandobserve
resultsthatcontradictpreviousempiricalobservationsonarchitecturalsensitivityinCL.
1 INTRODUCTION theaverageaccuracyandforgettingmetricswhenthe
backbonearchitectureorthelearningorderchanges.
Continual Learning (CL) is a subfield of machine Research on graph neural networks (GNN) in-
learning.Itaimstobuilddeeplearningmodelsthatcan creased lately due to the availability of graph-
continuallyaccumulateknowledgeoverdifferenttasks structureddata(Wuetal.,2020). However,continual
without needing to retrain from scratch (De Lange graphlearning(CGL)isstillunderexplored(Febri-
et al., 2021). Good CL performance can not be nanto et al., 2023). Previous studies show that pre-
achieved easily. The phenomenon of catastrophic trainingGNNsisnotalwaysbeneficialandmayleadto
forgetting(McCloskeyandCohen,1989)causesdeep negativetransferondownstreamtasksafterfine-tuning
learningmodelstoabruptlyforgettheinformationof (Huetal.,2020). Thisiscontrarytotheobservations
past tasks, as they are no longer relevant to current intransferlearningwithCNNs(Girshicketal.,2014).
optimizationobjectives. WenotethattrainingamodelinaCGLsettingwithout
Many previous works propose new CL methods CGLmethodsisequivalenttofine-tuningthemodel.
to alleviate the catastrophic forgetting phenomenon As GNNs show empirically worse performance on
(Kirkpatricketal.,2017)(Aljundietal.,2018)(Liand fine-tuning compared to CNNs, they may also have
Hoiem, 2017)(Lopez-PazandRanzato, 2017)(Rol- uniquepropertiesinCGL.Thismotivatesourresearch
nicketal.,2019)(IseleandCosgun,2018). Incompar- onthesensitivityoftheCL/CGLmethodswithgraph-
ison,thestudyonthesensitivityofCLperformances structureddataandGNNs. CurrentCGLbenchmarks
isrelativelyscarce(DeLangeetal.,2021)(Mirzadeh (Zhangetal.,2022)(Koetal.,2022)covermanytypes
et al., 2022) (Bell and Lawrence, 2022) (Lin et al., ofgraphs,however,temporalgraphshavenotyetbeen
2023). However,fromthesestudies,wealreadysee benchmarked. Ourbenchmarkusesskeleton-basedac-
anoticeabledifferenceinperformance,measuredby tionrecognitiondatasetswithspatio-temporalgraphs,
extendingcurrentCGLbenchmarks.
a https://orcid.org/0000-0001-5651-8712 InCL,themethodmustlearntodiscriminatebe-
b https://orcid.org/0000-0002-2969-3133 tweenagrowingnumberofclasses. Often,asequence
c https://orcid.org/0000-0002-4812-4841 of classification tasks with disjoint classes is used
4202
naJ
13
]VC.sc[
1v45081.1042:viXra(a)Exampleoftaskordershuffling,taskshavesameclasses. (b)Exampleofclassordershuffling,taskshaverandomclasses.
Figure1:Theaccuracyforeachclassfluctuateswhenthetask/classorderforCGLchanges.Classeswithinonetaskcanhave
largeaccuracydifferences(Fig.1a,class2/3).Thisisnotcapturedbytask-ordersensitivity.Imagesfrom(Wangetal.,2014).
(van de Ven et al., 2022). Previous literature often Ourcontributionsareasfollows:
focusesontask-incrementallearning(task-IL),where
• To the best of our knowledge, we are the first
thetaskidentifierisexplicitlyprovidedtothemodel
to benchmark CGL methods for tasks involving
during the train and test phases. We focus on class-
spatio-temporalgraphs,suchasskeleton-basedac-
incrementallearning(class-IL),wherethetaskidenti-
tionrecognition,inclass-ILsetting. Itrequiresthe
fierisnotprovidedtothemethod. Themethodmust
modeltopredictusinginputacrossmultipletimes-
learntodistinguishbetweenallseenclasses,i.e. the
tamps. ThisisnotcoveredinpreviousCGLbench-
methodneedstosolveeachtaskandidentifywhich
marks(Zhangetal.,2022)(Koetal.,2022). We
taskasamplebelongsto. Theclass-ILsettingismore
benchmarkdifferentCGLmethodsontwodatasets,
difficultbutalsomorerealistic,sincewedonotalways
coveringprimitivemovementstodailyactivities.
haveataskidentifierassociatedwiththecollecteddata.
• Weextendtheordersensitivityissueproposedby
Our work focuses on benchmarking the perfor-
(Yoonetal.,2020)totwodifferentsetting: task-
mance,aswellastheorderandarchitecturalsensitivity
ordersensitivityandclass-ordersensitivity. By
oftheCGLmethods. Wemeasuretheordersensitivity
comparingtheresultofbothsettings,wecancap-
bytrainingCGLmethodsonrandomlyshuffledtask
turetheimbalanceofperformanceoftheclasses
orclassorderandcomputethecorrespondingmetrics.
withinthesametaskasinFig.1a.
Fig. 1a is an example of a shuffled task order: We
pre-define a set of tasks with corresponding classes, • Wepresentextensiveempiricalexperimentsonthe
e.g. TaskX willcontainsamplesofclasses0and1, orderandarchitecturalsensitivityofwell-known
and taskY will contain samples of classes 2 and 3. CGLmethods.I.e.,theperformancefluctuationfor
Incontrary,Fig.1bshowsarandomlyshuffledclass eachtask/classwhenthelearningorderchanges,
order. Here,tasksareconstructedwithrandomclasses. and the performance fluctuation when the back-
Task X may consists of classes 0 and 5 in one class bonearchitecturechanges.Wearethefirsttostudy
order experiment, but will contain samples of other theclass-ordersensitivityoftheCGLmethods.
classesinanotherclassorderexperiment. Weobserve We demonstrate that task-order robust methods
thattheaccuracyofeachclassdiffersnoticeablyasthe canstillbeclass-ordersensitive. Thescaleofour
learningorderchanges. Thelargedifferencedenotes experiment on order sensitivity is larger than in
the high class-order sensitivity of the CGL method. any previous works. Our work provides a com-
Incontrasttostandardsensitivitybenchmarks,which prehensiveviewoftheordersensitivityoftheex-
measuresthefluctuationoftheaverageaccuracycom- perimentedCGLmethodsandasetupforbench-
putedacrossthetasks/classes. Wemeasuretheorder markingCGLmethods’sensitivityinclass-ILon
sensitivityproposedby(Yoonetal.,2020)andextend skeleton-basedactionrecognition.
the setting to two differentscenario: task-order sen-
• We study the correlation between the two most
sitivity and class-order sensitivity separately. They
usedevaluationmetricsinCGL:AverageForget-
measure the impact of learning order on the perfor-
ting(AF)andAverageAccuracy(AA).Wepropose
manceeachspecifictaskorclass. Thiscanhelpwith
atheoremthatdefinestheupperboundofAFwhen
assessingwhetherthepasttasks/classeswillhaveequal
AAisgiven. Wevisualizethisinourresults.
performance when a CL/CGL model is deployed in
realworldsetting,wherewedonothavecontrolover • Wecomparetheresultofourbenchmarkwithpre-
theorderofarrivalforthetasks/classes. Itisimportant viousempiricalstudiesinCLanddemonstratethe
fortheapplicationwherefairnessiscrucial. differenceinarchitecturalsensitivitywhengraph-
structureddataareusedinsteadofEuclideandata.2 RELATED WORKS revealinganotherproblemthatreal-worldapplicable
CLmethodsneedtosolve.
CGL benchmarks. (Zhang et al., 2022) created a Class-ordersensitivity. Class-ordersensitivityis
benchmarkforCGLmethodsbasedonpublicdatasets. theproblemwheretheCLperformanceperclassvaries
It includes both node-level and graph-level predic- when the classes within each task change. (Masana
tion tasks in class-IL or task-IL CGL. (Ko et al., etal.,2020)experimentedwithdifferentclassordering.
2022) extends the benchmark to include edge-level Theyconstructedtasksequenceswithincreasingorde-
prediction tasks, as well as tasks that use different creasingdifficultyinclass-IL.Theyfoundthat,fordif-
CGLsetting,suchasdomain-incrementallearningor ferentCLmethods,optimalperformanceisachieved
time-incrementallearning. However,thedatausedin usingdifferentclassordering. (Heetal.,2022)studied
thesebenchmarksdonotcontainthetemporaldimen- withadatasetwhereclassescanbegroupedassuper-
sion. Ourbenchmarkimplementsskeleton-basedac- classes. Theyreportedthatcreatingtaskswithclasses
tionrecognitiondatasetsthatcontainspatio-temporal from different superclasses yields better CL perfor-
graphs. Itrequiresthemodeltoreasonovermultiple mance. Thesetwoworksonlyconsideralimitednum-
timestamps of human skeleton-joint coordinates for berofclassorders. Ourworkgenerates100random
accurateprediction. Wenotethatthetime-incremental classorders,groupclassesadjacentinthegeneratedor-
learning in (Ko et al., 2022) refers to creating tasks derintotasks,andperformclass-ILCGLbasedonthe
basedonthetimestampofthedata,thedataitselfdo modifiedtaskstoapproximatetheclass-ordersensitiv-
notcontainatemporaldimension. Moreover,notall ityoftheCGLmethods. Next,theseworksaimtofind
benchmarksconsiderdifferenttask/classorders. Our thebestclassordertoincreasetheCLperformancebut
benchmarkexperimentswithdifferentordersandwe omittheevaluationofperformancechangesforeach
reporttheordersensitivity. class. Our work considers the performance change
Task-order sensitivity. (Yoon et al., 2020) pro- perclasstoassessthefairnessoftheCGLmethodin
posed the problem of task-order sensitivity, where real-worldapplications.
the CL performance for each task fluctuates when ImpactofnetworkarchitectureonCL.(Good-
theorder oftaskschanges. Theydefinedthe Order- fellowetal.,2013)showedthataddingadropoutlayer
normalizedPerformanceDisparity(OPD)metricto tofeed-forwardnetworksimprovesCLperformance.
evaluate it and reported that the tested CL methods (DeLangeetal.,2021)expandedtheresearchbyaddi-
haveahightask-ordersensitivity. (BellandLawrence, tionallyreportingtheexperimentalresultsfor4mod-
2022)studied thesignificanceof taskorders forCL elswithdifferentwidthsanddepthswithCLmethods
performance. They used synthetic data to find the onVGGandAlexNet-basedarchitectures. Theyob-
besttaskorderbycalculatingthedistancebetweenthe servedthat(too)deepmodelsdonothavehighaccu-
tasksandproposedawaytoestimatetaskdistances racy,whilewidermodelsoftenperformbetter. How-
usingdatasetssuchasMNISTusinggradientcurva- ever,awidermodelhasahigherriskofover-fittingon
ture. Theyfoundthatreorderingthetaskstocreatethe thefirsttask. (Mirzadehetal.,2022)studiedtheim-
largest total task distances yields better results. We pactofdifferentarchitecturesandbuildingblockson
notethatwedonotalwayshavecontrolovertaskorder, CLperformance. Theyusedexistingstate-of-the-art
thus, thestudyoftask-ordersensitivityisnecessary. (SOTA)computervisionarchitecturessuchasResNet
(Linetal.,2023)theorizedthecatastrophicforgetting (He et al., 2016) and Vision Transformers (Dosovit-
effect in task-IL. One of the conclusions is that the skiyetal.,2021).Theyobservedthatwideandshallow
besttaskorderforminimizingforgettingtendstoar- networksoftenoutperformthinanddeepnetworksin
rangedissimilartasksintheearlystageofCL.This CL.Ourworkconductsarchitecturalsensitivityexperi-
correspondswith(BellandLawrence,2022). Further, mentsusingGNNs,whichreactdifferentlyfortransfer
otherworksstudiedtheimpactofdifferenttaskorders learning(Huetal.,2020). Wealsoreporttheevolution
in task-incremental CL setting too (De Lange et al., oftheperformanceswithgrowingdepthandwidth.
2021)(Lietal.,2022). Ourworkdistinguishesfrom ContinualActionRecognition. Tothebestofour
thembyconsideringthemoredifficultclass-ILCGL. knowledge,Else-Net(Lietal.,2021)istheonlyprevi-
Exceptfor(Yoonetal., 2020), theseworksonly ousliteraturewhichstudiedCGLwithactionrecogni-
consider the sensitivity of the average accuracy and tion. Theyusedthedomain-incrementalsetting,where
averageforgetting,calculatedacrossthetasks,butnot thesubject/viewpointisusedtocreatetasks. Thisis
the order sensitivity defined in (Yoon et al., 2020), asimplersettingaseachtaskincludesdatasamples
wheretheperformanceofeachtaskisevaluatedsepa- of all classes. Our work evaluates CGL methods in
rately.Ourworkadditionallybenchmarksthisproblem thetaskandclassincrementalsetting,whereeachtask
extensively,andonthemorefine-grainedclasslevel, onlycontainsunseenhumanactions.3 PRELIMINARIES Wherear denotestheaccuracyoftaskt afterlearn-
B,t
ingthefulltasksequenceoflengthBintherandom
3.1 Class-incrementalLearning taskorderr. ThemaximumOPD(MOPD)isdefined
as MOPD=max(OPD ,...,OPD ) and the average
1 B
Weuseasimilardefinitionofclass-ILlike(Zhouetal., OPD (AOPD) is defined as AOPD = B1∑ tB =1OPD t.
2023).Class-ILaimstolearnfromanevolutivestream Weusethismetrictoreportthetask-ordersensitivity.
withincomingnewclasses(Rebuffietal.,2017). We Weadoptthesameformulaforclass-ordersensitivity,
assume a sequence of B tasks: {D1,D2,D3,...DB}. butinsteadofusingtask-wiseaccuracy,wewilluse
Db={(xb,yb)}nb denotestheb-thincrementaltrain- theaccuracyoftheindividualclass.
i i i=1
ingtaskwithn graph-structureddatainstances. Each
b
task D contains data instances from one or more 3.3 ContinualLearningMethods
classesc∈C. WedenotefunctionClsasafunction
thatmapsthetasktotheclassesofthedatainstances We used popular CL/CGL methods for our experi-
itcontains. TheclassesineachtaskDdonotoverlap ments. We refer to (Febrinanto et al., 2023) for the
withanyothertasksdefinedinthesequence. Thus: taxonomy.
Regularization-basedmethodspreventcatastrophic
Cls(Da)∩Cls(Db)=0/,∀a,b∈{1,...,B},a̸=b (1)
forgetting by preserving weights from the previous
During the b-th incremental training process, the model based on their importance. We have chosen
model only has access to the data instances from EWC (Kirkpatricketal.,2017),MAS(Aljundietal.,
thecurrenttaskDb. Aftereachincrementaltraining 2018)andTWP(Liuetal.,2021). EWCapproximates
process,themodelisevaluatedoverallseenclasses the weight importance using the Fisher information
Cb =Cls(D1)∪...∪Cls(Db). Class-ILaimstofind matrix. MAS uses the sensitivity of the output to a
seen
themodel f(x):X(cid:55)→Cwhichminimizesthelossover change in weight as importance. TWP is designed
allthetasksinthetasksequence. specificallyforCGL,itextendsMASbyadditionally
measuring the sensitivity of neighborhood topology
3.2 Metrics changes when computing importance. Knowledge
distillation-based methods preserve the output from
We define AA and AF as in (Chaudhry et al., 2018) the previous model. We have chosen LwF (Li and
andtheOPDmetricsasin(Yoonetal.,2020),which Hoiem,2017)fortheexperiments.
isusedtomeasureordersensitivity. Rehearsal-based methods preserve past data in-
AverageAccuracy(AA):Theaccuracya ∈[0,1] stancesandreplaythemfrequentlyduringthelearning
k,j
isevaluatedonthetestsetoftask jaftertrainingthe ofnewtasks. WehavechosenGEM(Lopez-Pazand
modelfromtask1tok. Theaverageaccuracyofthe Ranzato, 2017), which is a hybrid of rehearsal and
modelafterincrementallytraininguptotaskkis: regularization-basedmethods. GEM storesapartof
pastdatasamplesandregularizesthegradienttoensure
1 k
AA = ∑a (2) thatthelossofthestoredsamplesdoesnotincrease.
k k,j
k Next, we implement a simple, purely rehearsal-
j=1
basedCGLmethod. Itstoresx%ofthedatainstance
AverageForgetting(AF): Theaverageforgetting
ofeachtaskasamemorybuffer. Thedatainstancesin
metricisdefinedontopoftheaverageaccuracymetric.
thebufferaremixedwiththedatainstancesfromthe
First,wedefineforgettingforasingletask jas:
newtasksandreplayedtothemodel. Thisstrategyis
fk= max a −a , ∀j<k (3) denotedasREPLAY intheresultsofourexperiments.
j l,j k,j
l∈{1,...,k−1} Wereportthehyperparameter,thesearch-strategy,and
fk∈[−1,1]denotestheforgettingoftask jafterlearn- theruntimesofthemethodsinAppendixC.
j
ingtaskk. Next,wedefinetheaverageforgettingfora Architectural-basedmethodsmodifythearchitec-
modelafterlearningtaskkas: AF k= k−1 1∑k j=− 11 f jk. turedynamicallytopreventcatastrophicforgetting.As
Order-normalized Performance Disparity oneofourfocusesisarchitecturalsensitivity,wedid
(OPD): order-normalizedperformancedisparityisde- notincludearchitectural-basedmethodsinourstudy.
finedasthedisparitybetweenthemodelperformance Finally,weimplementtheBAREandJOINT base-
forataskkonRrandomtaskorders. I.e.,itmeasures line. BAREbaselinetrainsadeeplearningmodelwith-
the impact of the task order on the performance of outanyCGLmethod,modelstrainedbythisbaseline
each task. Following (Yoon et al., 2020), we use will suffer catastrophic forgetting. It is used as the
accuracytodenotethemodel’sperformance: lowerbound. JOINT baselinetrainsadeeplearning
model by accumulating all available past data, it is
OPD =max(a1 ,...,aR )−min(a1 ,...,aR ) (4)
t B,t B,t B,t B,tequivalenttoREPLAY whichstores100%ofthedata ourclass-ILCGLvariantofthedatasetsothatboth
asbuffer. JOINT baselineisusedastheupperbound. datasethasthesamenumberofclassesandtasks. We
usethedataprocessingprotocolin(Chenetal.,2021)
forbothdatasets.
4 EXPERIMENT SETUP Wecreatedtasksbasedontheclassesofthedata
samples. Eachtaskcontainstwoclassesandisdivided
intotrain,val,andtestsetwith8:1:1ratio.Thedetailed
Inthissection,weintroduceourexperimentalsetup,
informationofthedatasetsispresentedinTab.1. The
includingthedatasets,ourexperiments,andtheirgoals.
datasetwillbereleasedtofacilitatefutureresearch.
WeprovidetheparametersforeachexperimentinAp-
pendixBandAppendixC.
4.3 BackboneGraphNeuralNetworks
4.1 Visualization
MostoftheCL/CGLmethodsrequireabackboneneu-
ralnetwork. WeuseGCN(KipfandWelling,2017)
To facilitate compact sharing of our experiment re-
and ST-GCN (Yan et al., 2018) as backbone GNNs.
sults,wevisualizethemusingthescatterplotwithAA
GCNisthefoundationalGNNthatisusedformany
on the x-axis and AF on the y-axis. This visualiza-
SOTAgraphlearningtasks. ST-GCNisaspecialized
tionisadvantageouscomparedtostandardmeanand
GNNforthetaskofskeleton-basedactionrecognition.
variance reporting, as we can observe the exact dis-
AppendixBcontainstheimplementationdetails.
tributionacrossdifferentexperiments. E.g,inFig.2,
weobservetwoclustersofredpoints,oneclusterhas Table1:Constructedclass-incrementalCGLdataset
lowerAAastheyendwithadifficulttask. Moreover,
wevisualizethetheoreticalupperboundofAFforany Dataset UCLA-CIL NTU-CIL
givenAAbyproposingTheorem1. Thisshowsthatan N-UCLA NTU-RGB+D
Datasource
improvementofAAwillalsoimproveAFbylowering (Wangetal.,2014) (Shahroudyetal.,2016)
itsupperbound. TheproofisprovidedinAppendixA. #actionseq. 1484 6667
#joints 20 25
Theorem1. LetAA k betheaverageaccuracyofthe #classes 10 10
model after incrementally learning up to task k in #tasks 5 5
class-IL.Then,thefollowinginequationdenotesthe avg.#seq./task 297 1333
upperboundofAF . #frame/seq. 52 300
k
k 1
AF ≤1− AA + a (5)
k k k,k
k−1 k−1
4.4 Experiments
1.0
method We introduce our experiments with their objectives
ewc
0.8 gem andpresenttheresultsinSec.5.
mas
0.6 bare
lwf
twp 4.4.1 OrderSensitivity 0.4 replay
0.2 The goal of the order sensitivity experiments is to
0.0 test the variance of CGL performance for each task
0.0 0.2 0.4 0.6 0.8 1.0
Average Accuracy individuallywhentheorderoftasksorclassesisunde-
Figure2: ExamplescatterplotofAAandAF. Thedotted termined. Theperformanceofeachtaskismeasured
greendiagonallineshowsthetheoreticalupperboundofAF.
byAA,andtheordersensitivityismeasuredbyOPD.
Weexecutetwotypesofexperimentstotesttheor-
dersensitivityempirically. Inthetask-ordersensitivity
4.2 Datasets experiment,werandomlyshuffletheorderinwhich
thetasksarepresented, withoutperturbingtheclass
Weimplementtwodifferentactionrecognitiondatasets thatconstructsthetasks. Intheclass-ordersensitivity
on our benchmark to cover different complexity of experiment,werandomlyshuffletheorderinwhich
classesandtasks. N-UCLA(Wangetal.,2014)cov- the classes are presented to construct the tasks. I.e.,
ers primitive movements, e.g. walk, pick up. NTU- tasksareconstructedwithrandomclasses. Thistests
RGB+D(Shahroudyetal.,2016)coversdailyactivi- boththeimpactoftasklearningorderandtheimpact
ties,e.g. drinkwater,eatmeal. Weonlyusedthefirst of task difficulty. The class-order sensitivity experi-
10classesfromNTU-RGB+Dfortheconstructionof mentismoredifficultandclosertoreal-worldsetting,
gnittegroF
egarevAasreal-worlddataoftenarriveinrandomorder. The 1.0 1.0
method method
ewc ewc
OPD of task-order sensitivity is computed via task- 0.8 gem 0.8 gem
mas mas
levelaverageaccuracy,whiletheOPDofclass-order 0.6 bare 0.6 bare
lwf lwf
sensitivityiscomputedviatheaccuracyofeachclass. twp twp 0.4 replay 0.4 replay
Wenotethatataskordercanbetransformedinto
0.2 0.2
aclassorder. E.g.,ataskorder{1,2,3,4,5}isequiv-
alent with the class order {0,1,2...,8,9}. Thus, we 0.00.0 0. A2 vera0. g4 e Ac0 c.6 uracy0.8 1.0 0.00.0 0. A2 vera0. g4 e Ac0 c.6 uracy0.8 1.0
aggregatedtheresultsofbothexperimentstoobtaina (a)UCLA-CIL (b)NTU-CIL
moreaccurateapproximationofclass-ordersensitivity. Figure3: ScatterplotofAAandAFfortask-orderexperi-
ment withGCN.
4.4.2 ArchitecturalSensitivity
1.0 1.0
method method
Thearchitectural sensitivity experiment aims totest 0.8 e gw emc 0.8 l rw epf lay
mas
the performance stability of the CGL methods with 0.6 bare 0.6
lwf
differentwidthsanddepthsofthebackbonenetwork. replay 0.4 0.4
WemeasurethisbyobservingtheevolutionofAAand
0.2 0.2
AFwithdifferentGNNarchitecture.
Althoughpreviousstudies(DeLangeetal.,2021) 0.00.0 0.2 0.4 0.6 0.8 1.0 0.00.0 0.2 0.4 0.6 0.8 1.0
Average Accuracy Average Accuracy
(Mirzadehetal.,2022)reportthatawideandshallow (a)UCLA-CIL (b)NTU-CIL
architectureoftenoutperformsthinanddeeparchitec- Figure4: ScatterplotofAAandAFfortask-orderexperi-
tures,theevolutionofperformancechangesisnever ment withST-GCN.
reported. We experiment with a gradual change in
ismoresensitivecomparedtoREPLAY,eventhough
modelwidthanddepthandreporttheresultsforeach
theystorethesamefractionofthepastdataasbuffer.
step. Thisinformationisusefultodeterminethetrend
Thiscouldmeanthatregularization-basedmethodsdo
ofperformancechanges.
notpreventcatastrophicforgettingaswellasrehearsal-
basedmethodsinskeleton-basedactionrecognition,
wherethedatafromthedifferentactionclasseslooks
5 RESULTS
vastlydifferent. WehypothesizethatnoneoftheCGL
methods learns features in the first task that can be
5.1 Task-orderSensitivity generalizedforthefulltaskspace. However,REPLAY
canescapethelocalminimumfasterbecauseitdoes
Wefirstdiscussthetask-ordersensitivityexperiment. notrelyontheoutputorweightsofpreviousmodels.
As we only have 5 tasks for our datasets, we have ThisleadstobettergeneralizationandahigherAAof
5!=120differenttaskorders. Weexperimentedwith REPLAY. Thisphenomenoncanalsobeobservedin
all120taskorderswithGCNasbackboneandvisu- Fig.4a. Alltheregularizationbasedmethodshavelow
alized their AA and AF in Fig. 3. The result of ST- AAastheembeddingstheylearnarenotgeneralizable
GCNarevisualizedinFig.4. Werestrainourselves toothertasks.
of extensively experimenting ST-GCN on the NTU- Next,asprovenbyTheorem1,changesinAAlow-
CILdataset,asalltheregularization-basedmethods erstheupperboundofAF. FromFig.3a,weobserve
performedon-parwiththeBAREbaselineinthesim- a linear correlation between the increase in AA and
plerUCLA-CILdataset. Ourresultsaresufficientto thedecreaseinAF,thishintsthecorrelationbetween
supportthehypothesis. Theexplanationgivenbelow AAandAF. However,weobserveinFig.3adifferent
appliestobothbackbonesastheresultssharesimilar levels of AF when the AA of the different methods
trends. Wenotethatordersensitivitydoesnotapplyto are similar. We suspect that it indicates regulariza-
theJOINT baseline,asJOINT willbetrainingonall tionstrengthofthedifferentregularization-basedCGL
availabledataintheend,changingtheclass/taskorder methodsunderthescenariowhentheembeddingsare
willhavelittleeffect. Thus,wedidnottrainJOINT notgeneralizable,i.e.,whenthemodeloverfitstoprevi-
forordersensitivityexperiments. oustasks. MAScomputesthesensitivityoftheoutput
FromFig.3,weobserveclearclustersfordifferent toeachweight. Whenthemodeloverfits,theoutput
methodsinUCLA-CIL.Moreover,differentmethods will likely be very sensitive to a subset of weights.
exhibitdifferentcharacteristics. Regularization-based MASregularizesthemodelmoreontheseweights,hin-
methodsaresensitivefortheperformancemetricAA, dering the learning of new tasks. This reduces the
whileLwFandREPLAYaremorestable.Interestingly, maximumAAofthenewtasks,thusreducingAF. In
weseethatGEM performedsignificantlyworseand NTU-CIL, where overfitting is less likely due to a
gnittegroF
egarevA
gnittegroF
egarevA
gnittegroF
egarevA
gnittegroF
egarevATable2: OrdersensitivitymeasuredbyOPD forexperimentswithaGCNbackbone. BoldentriesarethelowestOPDs,
underlinedentriesarethehighestOPDs,excludingtheBAREbaseline.TheOPDmetricintheclass-ordersensitivityexperiment
isbasedontheaccuracyoftheclasses.Allthemethodsareordersensitive,thereisalargedifferenceintask-ordersensitivity
andclass-ordersensitivity.
task-ordersensitivityexperiment aggregatedclass-ordersensitivityexperiment
method\Setting UCLA-CIL NTU-CIL UCLA-CIL NTU-CIL
AOPD(↓) MOPD(↓) AOPD(↓) MOPD(↓) AOPD(↓) MOPD(↓) AOPD(↓) MOPD(↓)
BARE 96.74%±0.72% 100.0%±0.0% 92.91%±0.38% 99.24%±0.0% 100.0%±0.0% 100.0%±0.0% 99.73%±0.06% 100.0%±0.0%
EWC 93.94%±0.56% 100.0%±0.0% 85.36%±0.88% 94.24%±1.23% 100.0%±0.0% 100.0%±0.0% 98.12%±0.49% 100.0%±0.0%
MAS 89.46%±1.6% 98.7%±1.61% 87.88%±0.51% 95.61%±1.3% 99.73%±0.33% 100.0%±0.0% 97.61%±0.4% 100.0%±0.0%
TWP 85.39%±1.93% 100.0%±0.0% 77.45%±1.4% 89.24%±1.11% 99.3%±0.45% 100.0%±0.0% 95.73%±0.74% 100.0%±0.0%
LwF 63.21%±2.46% 75.04%±6.46% 78.36%±1.1% 93.48%±1.13% 93.44%±1.46% 100.0%±0.0% 94.0%±0.61% 98.79%±0.61%
GEM 83.89%±1.45% 100.0%±0.0% 80.91%±0.25% 94.09%±0.88% 99.59%±0.33% 100.0%±0.0% 95.36%±0.49% 100.0%±0.0%
REPLAY 58.21%±7.54% 80.25%±5.2% 39.52%±0.94% 53.18%±2.41% 82.54%±4.56% 100.0%±0.0% 50.52%±1.16% 71.82%±2.27%
largersamplesize,AFofMASiscomparabletoEWC. ableweights,itisexpectedthatthelastlearnedtasks
TWP has an extra regularization on top of MAS that willhavehigherperformances. Whenthetasksaresuf-
preserves the topology. However, SOTA GNNs on ficientlydifferent,theweightwillshiftfarenoughso
skeleton-basedactionrecognition(Chenetal.,2021) thattheoldertasksareforgotten. Incomparison,LwF
showthatusingdifferenttopologiesisbeneficialtopre- andREPLAY alterthegradientofthemodelbyresp.
dictionaccuracy. Thus,theAFofTWPislowerthan knowledge distillation and rehearsal. By modifying
MASwhentheirAAaresimilar.Finally,GEMprevents the gradient using other means than constraints, the
thelossincrementofthestoredsamples. Whenmodel modelhasmorefreedomandmayfindalossregion
overfitstopasttasks,GEMisunabletounlearnit.This whereitgeneralizestobothpresentandpasttasks.
isreflectedinthelowestAF inbothdatasets. Inthe In NTU-CIL, we see an improvement in task-
end,LwF’sAFisverysensitivetothetaskorder. The orderrobustnessformanymethods,exceptLwF. In-
features learned in the first task influence the learn- creasedtaskcomplexitydecreasesthechanceofover-
ingoffollowingtasksheavily. Whenthedifference fitting,whichfacilitatestheregularizationprocessand
betweenthetasksislarge,thefeatureslearnedbythe REPLAY astheylearngeneralizableembeddingthat
oldmodelmaynotbeimportantforthenewtask. In can be reused by following tasks. We observe two
thatcase,theoutputoftheoldmodelcanbeunstable. distinguishableclustersforLwF andBAREbaseline,
Thus,theregularizationstrengthofLwFdependson whenwecomparethetaskorderswhichformthetwo
thesimilarityofpastandcurrenttasks. Achangein clusters,wefindthemtobeidentical. Moreover,the
taskorderwillinfluencethis,andindirectlyinfluence clusterwithhigherAAalwaysendswithtask0or1.
AFtoo.ThehighestAFforLwFisobtainedbythetask The classes for these tasks are resp. {drink water,
orderthatstartswithtaskswithmanylegmovements, eat meal} and {brush teeth, brush hair}. Based on
andendswithtasksthathaveminimallegmovements, ourearlierhypothesis,largedifferencebetweentasks
whilethetaskorderwiththelowestAFhastoalternate hinders LwF the most. In NTU-CIL, both primitive
betweentaskswithmanyorminimallegmovements. movement and daily actions exists, thus the models
Wenotethat,alowAF canbeachievedbylowering thatlearnedtheprimitivemovementmaynotfocuson
themaximumAAorraisingtheminimumAAofeach thefinermovementsthatidentifiesdailyactions. This
taskduringtheCGLprocess. Thephenomenonhere makestheknowledgedistillationbetweenoldandnew
correspondstotheformercase. Itdoesnotimprove modelsunstableandpotentiallydestructiveduringthe
theusabilityoftheCGLmodelandisundesirable. learningprocessofthenewtask,whichmakesLwF
Then, we study the task-order sensitivity by ob- ordersensitive.
servingtheOPDmetric. OPDiscalculatedover120
randomtask-ordersand220randomclass-orders,we 5.2 Class-orderSensitivity
repeated the experiment 5 times, obtaining 5 OPD
metrics. Wereportthemeanandstandarddeviation Wetest100differentclassordersforeachCGLmethod
in Tab. 2. Regularization-based methods are highly inbothdatasets. Weaggregatetheresultsfromboth
task-order sensitive in UCLA-CIL, while LwF and experimentstoobtainamoreaccurateclass-ordersen-
REPLAY arecomparativelylesssensitive. EWC,MAS sitivityoftheCGLmethodsasmentionedinSec.4.4.1,
andGEMachieved100%MOPD,whichdenotesthat themetricsarevisualizedinFig.5andFig.6. Fig.6b
thereexisttwotaskordersandataskA,wheretaskA containspartialresults,asmentionedabove.
has100%AAinoneorder,and0%inanotherorder. Limitation.100randomclassorderisonlyasmall
This behavior is not desirable for CL methods. As subset of all possible class orders, for our datasets,
regularization-basedmethodsonlyconstrainthelearn- therewillbe10!differentclassorders.Itisnotfeasibletoexperimentonalltheclassorders,thus,ourresult haveclasseswithbadperformances,whichisanissue
onlyapproximatestheclass-ordersensitivity. inreal-lifesettingswherefairnessiscrucial.
1.0 method 1.0 method 5.3 ArchitecturalSensitivity
ewc ewc
0.8 gem 0.8 gem
mas mas
0.6 bare 0.6 bare Wetestthearchitecturalsensitivityfortheperformance
lwf lwf
twp twp 0.4 replay 0.4 replay oftheCGLmodelsbyincreasingmodelwidth/depth
0.2 0.2 usingaGCNasbackbone. Wereportthewidthevolu-
tioninFig.7anddepthevolutioninFig.8.
0.00.0 0.2 0.4 0.6 0.8 1.0 0.00.0 0.2 0.4 0.6 0.8 1.0
Average Accuracy Average Accuracy
(a)UCLA-CIL (b)NTU-CIL 1.0 ewc F exig pu er re im5 e: nS tc sa wtt ie tr hp Glo Ct Nof .AAandAFforclassandtask-order 0.8 g m be aa rm s e 0.8
00 .. 46 l t r jw ow e ipf np l ta ty rain 00 .. 46 e g m b lww e aa frmc s e
1.0 1.0 0.2 twp
0.8 met e gh w eo mcd 0.8 met l rwh eo pfd lay 0.2 0.0 r joe ip nl ta ty rain
0.6 m baa rs e 0.6 100 M20 o0 del 3 w00 idth400 500 100 M20 o0 del 3 w00 idth400 500
lwf
replay (a)UCLA-CIL,AA (b)UCLA-CIL,AF 0.4 0.4
Figure7:EvolutionofthemetricswhenGCNwidthvaries.
0.2 0.2
0.00.0 0.2 0.4 0.6 0.8 1.0 0.00.0 0.2 0.4 0.6 0.8 1.0
Average Accuracy Average Accuracy 1.0
(a)UCLA-CIL (b)NTU-CIL 0.8
0.8 ewc
Figure6:ScatterplotofAAandAFforclassandtask-order gem
mas 0.6 ewc
experimentswithST-GCN. 0.6 bare gem
lwf 0.4 mas
0.4 t rw epp lay b lwa fre
InFig.6,weobservethesamephenomenonwhere jointtrain 0.2 twp
0.2 replay
theregularization-basedmethodshavealowAAfor 0.0 jointtrain
0 2 4 6 8 10 12 14 16 0 2 4 6 8 10 12 14 16
ST-GCN,whichleadstothesamehypothesisthatnon- Model depth Model depth
generalizableembeddingsarelearned. InFig.5,we (a)UCLA-CIL,AA (b)UCLA-CIL,AF
observethat,withrandomizedclassorder,thecluster Figure8:EvolutionofthemetricswhenGCNdepthvaries.
ofresultsisvastlydifferentcomparedtotherandom-
Previous studies in CL (De Lange et al., 2021)
ized task order. However, the trends are consistent
(Mirzadehetal.,2022)establishthatawideandshal-
withthepreviousexperiment: EWCandTWPperform
lownetworkoutperformsathinanddeepnetwork.Our
the worst, GEM, MAS and LwF perform better, and
resultswithGNNscontradicttheirobservations. In-
REPLAY performsthebest. Surprisingly,weseethat
creasingthewidthdoesnotalwaysshowgains,theef-
allmethodsareclass-ordersensitiveinUCLA-CILby
fectofincreasingdepthisonlyconsistentforREPLAY.
observingtheOPDmetricinTab.2.
Wehypothesizethatitiscausedbythecombina-
TheOPDoftask-ordersensitivityiscomputedvia
tion of GNN and skeleton-based action recognition
task-levelaccuracy,whiletheOPDoftheclass-order
tasks. The skeleton-joint graph input is connected
sensitivity is calculated using the accuracy of each
sparsely. There are 10 edges between the left hand
class. Alargedifferencebetweenthetwodenotesthat
andleftfootnode. ThismakesshallowGNNsunable
theaccuracyofclasseswithinthetaskarenotequal.
tocapturehigh-levelfeaturesthatrequireinformation
AnexamplecanbeobservedinFig.1,intaskY,the
from both hand and foot movements, e.g., touching
accuracyofclass2ismuchworsethantheaccuracy
foot. Previouswork(Dwivedietal.,2022)showsthat
of class 3. Previous evaluation setting proposed by
GCNdoesnotcapturelong-rangedinformationwell.
(Yoon et al., 2020) can not detect this issue as the
Ourwidthsensitivityexperimentuses2-layerGNNs,
performanceisevaluatedattasklevel. Ourevaluation
which means that it can not produce the abovemen-
settingisatthemorefine-grainedclasslevel.
tioned features. Increasing the width only worsens
We hypothesize that this is caused by the unbal-
theover-fittingofthetasks. Thishypothesisalsoex-
ancedfeaturesusedtoidentifytheclasses. Themodel
plainswhyincreasingthedepthdoesnotalwayslead
maylearnmanyfeaturestoidentifyoneclassandonly
toworseperformance,deeperGNNscancapturelong-
afewfeaturesfortheother classes. Thelatterclass
rangedinformation,creatingmoregeneralizablefea-
ismorepronetocatastrophicforgetting. Ourresults
tures,whichincreasestheperformance. Thiscanbe
demonstratethatatask-orderrobustmethodcanstill
observedinFig.8b,weseeadownwardtrendofAF
gnittegroF
egarevA
gnittegroF
egarevA
gnittegroF
egarevA
gnittegroF
egarevA
ycarucca
egarevA
ycarucca
egarevA
gnittegrof
egarevA
gnittegrof
egarevAformostregularization-basedmethodsaswellasthe ACKNOWLEDGEMENTS
BARE baseline. However, increasing the depth also
worsenstheoverfittingeffectduetotheincreasedex- ThisresearchreceivedfundingfromtheFlemishGov-
pressiveness. Theover-squashingphenomenonalso ernmentunderthe“OnderzoeksprogrammaArtificie¨le
preventsthemodelfromlearninghigh-levelfeatures Intelligentie(AI)Vlaanderen”programme.
(Alon and Yahav, 2021). This is most noticeable in
REPLAY,wheretheoverfittingcanoccuronboththe
currenttaskandonthebuffersofthepasttasks. Thus, REFERENCES
contrarytoobservationinpreviouswork,forskeleton-
basedactionrecognition,increasingthewidthisnot Aljundi,R.,Babiloni,F.,Elhoseiny,M.,Rohrbach,M.,and
useful for shallow GNNs, and increasing the depth Tuytelaars,T.(2018). Memoryawaresynapses:Learn-
mayhavesomebenefits. Thisobservationmotivates ingwhat(not)toforget. InProceedingsoftheEuro-
futurestudiesindeep-GNNsandtheirapplicationsin peanconferenceoncomputervision(ECCV),pages
139–154. 1,4
thespatio-temporalgraphs.
Alon,U.andYahav,E.(2021). Onthebottleneckofgraph
neuralnetworksanditspracticalimplications. In9th
InternationalConferenceonLearningRepresentations,
6 CONCLUSION ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net. 9
Bell, S. J. and Lawrence, N. D. (2022). The effect of
We constructed the first Continual Graph Learning
task ordering in continual learning. ArXiv preprint,
(CGL)benchmarkforspatio-temporalgraphs. Weare
abs/2205.13323. 1,3
thefirsttobenchmarkskeleton-basedactionrecogni-
Chaudhry,A.,Dokania,P.K.,Ajanthan,T.,andTorr,P.H.
tioninclass-incrementallearning(class-IL)setting.
(2018).Riemannianwalkforincrementallearning:Un-
Weextendedtheordersentivityissueproposedby derstandingforgettingandintransigence. InProceed-
(Yoonetal.,2020)totwodifferentsettings: task-order ingsoftheEuropeanconferenceoncomputervision
sensitivityandclass-ordersensitivity. Bycomparing (ECCV),pages532–547. 4
the order sensitivity at both task and class level, we Chen,Y.,Zhang,Z.,Yuan,C.,Li,B.,Deng,Y.,andHu,W.
capturedtheimbalanceofperformancebetweenthe (2021). Channel-wisetopologyrefinementgraphcon-
volutionforskeleton-basedactionrecognition.In2021
classeswithinthesametask,whichisanunexplored
IEEE/CVFInternationalConferenceonComputerVi-
probleminCGL.Weconductedextensiveexperiments
sion, ICCV 2021, Montreal, QC, Canada, October
onthetaskandclass-ordersensitivityofthepopular
10-17,2021,pages13339–13348.IEEE. 5,7,11,12
CGLmethods. Wediscoveredthatthetask-orderro-
DeLange,M.,Aljundi,R.,Masana,M.,Parisot,S.,Jia,X.,
bustmethodscanstillbeclass-ordersensitive,i.e.,in Leonardis,A.,Slabaugh,G.,andTuytelaars,T.(2021).
sometasks, thereareclasseswhichoutperformsthe Acontinuallearningsurvey:Defyingforgettinginclas-
otherclasses. Next,weshowthatpopularCGLmeth- sificationtasks. IEEEtransactionsonpatternanalysis
odsareallordersensitive,i.e. theperformanceofeach andmachineintelligence,44(7):3366–3385. 1,3,6,8
task/classdependentsonthelearningorder. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,
Zhai,X.,Unterthiner,T.,Dehghani,M.,Minderer,M.,
Wealsostudiedthearchitecturalsensitivity. We
Heigold,G.,Gelly,S.,Uszkoreit,J.,andHoulsby,N.
report the evolution of AA and AF when the depth
(2021).Animageisworth16x16words:Transformers
andwidthofthebackboneGNNvaries. Ourresults forimagerecognitionatscale. In9thInternational
contradictpreviousempiricalobservationsinCL.We ConferenceonLearningRepresentations,ICLR2021,
providedourinsightonthecontradiction. VirtualEvent,Austria,May3-7,2021.OpenReview.net.
Westudiedthecorrelationsbetweenaveragefor- 3
getting(AF)andaverageaccuracy(AA),identifiedthe Dwivedi,V.P.,Rampa´sˇek,L.,Galkin,M.,Parviz,A.,Wolf,
upper-bound of AF, demonstrated that an improve- G., Luu, A. T., and Beaini, D. (2022). Long range
graph benchmark. Advances in Neural Information
mentofAAnaturallylowerstheupper-boundofAF
ProcessingSystems,35:22326–22340. 8
andvisualizeditinourresults.
Febrinanto,F.G.,Xia,F.,Moore,K.,Thapa,C.,andAg-
The studies in order and architectural sensitivity
garwal,C.(2023). Graphlifelonglearning:Asurvey.
arestillunderexploredinclass-ILCGL.Ourpaperis IEEEComputationalIntelligenceMagazine,18(1):32–
anintroductiontothesetwoissues.Futureworksare1) 51. 1,4
Expandthebenchmarktoincludenode-levelandedge- Girshick,R.B.,Donahue,J.,Darrell,T.,andMalik,J.(2014).
level tasks. 2) Investigate the intuitions to propose Richfeaturehierarchiesforaccurateobjectdetection
class-order robust CGL methods. We hope that our andsemanticsegmentation. In2014IEEEConference
onComputerVisionandPatternRecognition,CVPR
paperinitiatesfurtherresearchonthesensitivitiesof
2014,Columbus,OH,USA,June23-28,2014,pages
CGLmethods.
580–587.IEEEComputerSociety. 1Goodfellow,I.J.,Mirza,M.,Xiao,D.,Courville,A.,and SymposiumonEducationalAdvancesinArtificialIntel-
Bengio,Y.(2013). Anempiricalinvestigationofcatas- ligence,EAAI2021,VirtualEvent,February2-9,2021,
trophicforgettingingradient-basedneuralnetworks. pages8653–8661.AAAIPress. 4
arXivpreprintarXiv:1312.6211. 3 Lopez-Paz,D.andRanzato,M.(2017). Gradientepisodic
He,C.,Wang,R.,andChen,X.(2022). Rethinkingclass memory for continual learning. In Guyon, I., von
ordersandtransferabilityinclassincrementallearning. Luxburg,U.,Bengio,S.,Wallach,H.M.,Fergus,R.,
PatternRecognitionLetters,161:67–73. 3 Vishwanathan,S.V.N.,andGarnett,R.,editors,Ad-
He,K.,Zhang,X.,Ren,S.,andSun,J.(2016).Deepresidual vancesinNeuralInformationProcessingSystems30:
learningforimagerecognition. In2016IEEECon- AnnualConferenceonNeuralInformationProcessing
ferenceonComputerVisionandPatternRecognition, Systems2017,December4-9,2017,LongBeach,CA,
CVPR2016,LasVegas,NV,USA,June27-30,2016, USA,pages6467–6476. 1,4
pages770–778.IEEEComputerSociety. 3 Masana,M.,Twardowski,B.,andVandeWeijer,J.(2020).
Hu,W.,Liu,B.,Gomes,J.,Zitnik,M.,Liang,P.,Pande,V.S., On class orderings for incremental learning. ArXiv
andLeskovec, J.(2020). Strategiesforpre-training preprint,abs/2007.02145. 3
graphneuralnetworks.In8thInternationalConference McCloskey,M.andCohen,N.J.(1989). Catastrophicin-
onLearningRepresentations,ICLR2020,AddisAbaba, terferenceinconnectionistnetworks: Thesequential
Ethiopia,April26-30,2020.OpenReview.net. 1,3 learningproblem. InPsychologyoflearningandmoti-
Isele,D.andCosgun,A.(2018).Selectiveexperiencereplay vation,volume24,pages109–165.Elsevier. 1
forlifelonglearning. InMcIlraith, S.A.andWein- Mirzadeh, S. I., Chaudhry, A., Yin, D., Nguyen, T., Pas-
berger,K.Q.,editors,ProceedingsoftheThirty-Second canu,R.,Gorur,D.,andFarajtabar,M.(2022). Archi-
AAAIConferenceonArtificialIntelligence,(AAAI-18), tecturemattersincontinuallearning. ArXivpreprint,
the30thinnovativeApplicationsofArtificialIntelli- abs/2202.00275. 1,3,6,8
gence(IAAI-18),andthe8thAAAISymposiumonEdu- Rebuffi,S.,Kolesnikov,A.,Sperl,G.,andLampert,C.H.
cationalAdvancesinArtificialIntelligence(EAAI-18), (2017). icarl:Incrementalclassifierandrepresentation
New Orleans, Louisiana, USA, February 2-7, 2018, learning. In2017IEEEConferenceonComputerVi-
pages3302–3309.AAAIPress. 1 sionandPatternRecognition,CVPR2017,Honolulu,
Kipf,T.N.andWelling,M.(2017). Semi-supervisedclas- HI,USA,July21-26,2017,pages5533–5542.IEEE
sificationwithgraphconvolutionalnetworks. In5th ComputerSociety. 4
InternationalConferenceonLearningRepresentations, Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., and
ICLR2017,Toulon,France,April24-26,2017,Con- Wayne, G. (2019). Experience replay for continual
ferenceTrackProceedings.OpenReview.net. 5 learning. InWallach,H.M.,Larochelle,H.,Beygelz-
Kirkpatrick,J.,Pascanu,R.,Rabinowitz,N.,Veness,J.,Des- imer,A.,d’Alche´-Buc,F.,Fox,E.B.,andGarnett,R.,
jardins,G.,Rusu,A.A.,Milan,K.,Quan,J.,Ramalho, editors,AdvancesinNeuralInformationProcessing
T.,Grabska-Barwinska,A.,etal.(2017). Overcoming Systems32:AnnualConferenceonNeuralInformation
catastrophicforgettinginneuralnetworks.Proceedings ProcessingSystems2019,NeurIPS2019,December
of the national academy of sciences, 114(13):3521– 8-14,2019,Vancouver,BC,Canada,pages348–358. 1
3526. 1,4 Shahroudy,A.,Liu,J.,Ng,T.,andWang,G.(2016). NTU
Ko, J., Kang, S., and Shin, K. (2022). Begin: Exten- RGB+D:Alargescaledatasetfor3dhumanactivity
sivebenchmarkscenariosandaneasy-to-useframe- analysis. In2016IEEEConferenceonComputerVi-
work for graph continual learning. ArXiv preprint, sionandPatternRecognition,CVPR2016,LasVegas,
abs/2211.14568. 1,2,3 NV,USA,June27-30,2016,pages1010–1019.IEEE
Li,T.,Ke,Q.,Rahmani,H.,Ho,R.E.,Ding,H.,andLiu, ComputerSociety. 5
J.(2021). Else-net:Elasticsemanticnetworkforcon- vandeVen,G.M.,Tuytelaars,T.,andTolias,A.S.(2022).
tinualactionrecognitionfromskeletondata. InPro- Threetypesofincrementallearning. NatureMachine
ceedingsoftheIEEE/CVFInternationalConference Intelligence,pages1–13. 2
onComputerVision,pages13434–13443. 3 Wang, J., Nie, X., Xia, Y., Wu, Y., and Zhu, S. (2014).
Li,Y.,Li,M.,Asif,M.S.,andOymak,S.(2022). Provable Cross-viewactionmodeling,learning,andrecognition.
andefficientcontinualrepresentationlearning. ArXiv In 2014 IEEE Conference on Computer Vision and
preprint,abs/2203.02026. 3 PatternRecognition,CVPR2014,Columbus,OH,USA,
Li,Z.andHoiem,D.(2017). Learningwithoutforgetting. June23-28,2014,pages2649–2656.IEEEComputer
IEEEtransactionsonpatternanalysisandmachine Society. 2,5
intelligence,40(12):2935–2947. 1,4 Wang, M., Zheng, D., Ye, Z., Gan, Q., Li, M., Song, X.,
Lin, S., Ju, P., Liang, Y., and Shroff, N. (2023). Theory Zhou,J.,Ma,C.,Yu,L.,Gai,Y.,etal.(2019). Deep
onforgettingandgeneralizationofcontinuallearning. graph library: A graph-centric, highly-performant
ArXivpreprint,abs/2302.05836. 1,3 packageforgraphneuralnetworks. ArXivpreprint,
abs/1909.01315. 11
Liu,H.,Yang,Y.,andWang,X.(2021). Overcomingcatas-
trophicforgettingingraphneuralnetworks. InThirty- Wu,Z.,Pan,S.,Chen,F.,Long,G.,Zhang,C.,andPhilip,
FifthAAAIConferenceonArtificialIntelligence,AAAI S.Y.(2020). Acomprehensivesurveyongraphneural
2021,Thirty-ThirdConferenceonInnovativeApplica- networks. IEEEtransactionsonneuralnetworksand
tionsofArtificialIntelligence,IAAI2021,TheEleventh learningsystems,32(1):4–24. 1Yan, S., Xiong, Y., andLin, D.(2018). Spatialtemporal The term k−1 1∑k j=− 11a k,j is very closely related to
g tir oa nph rec co on gv no itl iu ot nio .n Ia nl Mne ct Iw lro ar itk hs ,f So .r As .ke al ne dto Wn- eb ia ns be ed rga ec r-
,
AA, which is 1 k∑k j=1a k,j. We can denote the term
K.Q.,editors,ProceedingsoftheThirty-SecondAAAI k−1 1∑k j=− 11a k,j asxandtransformtheinequality:
ConferenceonArtificialIntelligence,(AAAI-18),the
30thinnovativeApplicationsofArtificialIntelligence 1 k
(IAAI-18), and the 8th AAAI Symposium on Educa- AA k= ∑a k,j
k
tionalAdvancesinArtificialIntelligence(EAAI-18), j=1
New Orleans, Louisiana, USA, February 2-7, 2018,
k−1 1 k−1
pages7444–7452.AAAIPress. 5 AA = (∑a +a )
k k,j k,k
Yoon,J.,Kim,S.,Yang,E.,andHwang,S.J.(2020). Scal-
k k−1
j=1
ableandorder-robustcontinuallearningwithadditive
parameterdecomposition. In8thInternationalConfer- k−1 1 k−1 k−1 1
AA = (∑a )+ a
enceonLearningRepresentations,ICLR2020,Addis k k k−1 k,j k k−1 k,k
Ababa,Ethiopia,April26-30,2020.OpenReview.net. j=1
2,3,4,8,9 k−1 1
AA = x+ a
Zhang, X., Song, D., and Tao, D. (2022). Cglb: Bench- k k k k,k
marktasksforcontinualgraphlearning. InKoyejo, k 1
S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,Cho,K., (AA − a )=x
k k,k
k−1 k
andOh,A.,editors,AdvancesinNeuralInformation
ProcessingSystems,volume35,pages13006–13021. Whenwereplacethetermxintheoriginalinequality
CurranAssociates,Inc. 1,2,3,11,12 bythetransformedAA ,wegetthefollowing:
k
Zhou,D.-W.,Wang,Q.-W.,Qi,Z.-H.,Ye,H.-J.,Zhan,D.-C.,
k 1
andLiu,Z.(2023). Deepclass-incrementallearning: AF ≤1−( (AA − a ))
k k k,k
Asurvey. ArXivpreprint,abs/2302.03648. 4 k−1 k
k 1
AF ≤1− AA + a
k k k,k
k−1 k−1
APPENDIX A: THEOREM 1
ThisdenotestheupperboundofAF foragivenAA .
k k
However,tovisualizetheupperboundinthescatter
ToprovethattheupperboundofAF iscorrelatedto plot, itisnotpossibletoincludea ofeachexperi-
k,k
AA,weusethedefinitionfromSec.3.2: ment. Forsimplicity,wecanassumea isalwaysat
k,k
maximum,i.e. 100%accuracy. Wecanthendrawthe
1 k
AA = ∑a (6) dottedgreenlineasvisualizedinFig.2usingEq.(10):
k k,j
k
j=1 k 1
AF ≤1− AA + (10)
k k
fk= max a −a , ∀j<k (7) k−1 k−1
j l,j k,j
l∈{1,...,k−1}
1 k−1
AF = ∑ fk (8) APPENDIX B: IMPLEMENTATION
k k−1 j
j=1
Next,weexpandEq.(8)withEq.(3): OurbenchmarkadaptedthecodefromCGLB(Zhang
et al., 2022). We extended the code to handle extra
experimentoptions,includingreorderingclasses,tasks,
1 k−1 k−1
AF = (∑ max a −∑a ) andoptionsforchangingbackbonearchitectureand
k l,j k,j
k−1 j=1l∈{1,...,k−1} j=1 thecomputationofcorrespondingmetrics.
WeuseGCNimplementedbytheDGLpythonli-
Whena increases,theupperboundofAF will
k,j k brary(Wangetal.,2019)andST-GCNimplemented
besmaller. Whenweassumetheforgettingismaxi-
asinCTR-GCN(Chenetal., 2021)asthebackbone
mum,withoutmodifyingAA ,i.e. max a
k l∈{1,...,k−1} l,j GNNforourbenchmark. ForGCN,weusetwograph
isalways1,wewillhave:
convolutionallayers,followedbyasumandmaxread-
outasfeatureextractionlayers,andamlppredictoras
1 k−1 1 k−1
AF ≤( ∑1)− ∑a (9) classificationlayer. ForST-GCN,twoST-GCNlayers
k k,j
k−1 k−1
j=1 j=1 areused. Alllayershave64hiddenunits. TheGCN
architectureisvisualizedinFig.9a.
AF ≤1−
1 k ∑−1
a
Our benchmark added the data loader for ‘N-
k k,j
k−1 UCLA’ and ‘NTU-RGB+D’ skeleton-based action
j=1
recognition. WeadaptedthebasecodefromthedataTable3:BesthyperparametercandidatesfoundviagridsearchforGCN.frac memoriesdenotesthepercentageofeachtask
thatGEM/REPLAYstoresasbuffer.
algorithm besthyper-parameter
N-UCLA-CIL NTU-CIL
EWC memory strength:1000000 memory strength:1000000
MAS memory strength:100 memory strength:100
TWP lambda l:10000;lambda t:10000;beta:0.01 lambda l:100;lambda t:10000;beta:0.01
LwF lambda dist:1;T:2 lambda dist:0.1;T:2
GEM memory strength:5;frac memories:0.2 memory strength:5;frac memories:0.2
REPLAY frac memories:0.2 frac memories:0.2
averageoftheaccuracyoftheclassinthetask. The
output
mlp predictor reportedmetricsaretheaverageofthe5executions.
sum readout max readout Table4:Parameterforarchitecturalsensitivityexperiment.
gcn-layer2 Experiment parametergrid
gcn-layer1 width width:[32,64,128,256,512]
depth depth:[1,2,4,8,16]
input
(a)Visualizationofbaseline (b) Data pre-processing for
backboneGCNarchitecture. GCNarchitecture.
For the architectural sensitivity experiment, the
Figure9:architectureanddataprocessing
exactwidthsanddepthsusedarepresentedinTab.4.
loadersofCTR-GCN(Chenetal.,2021). Wechanged Inthewidthexperiment, thedepthofGCNissetto
the loader to create a DGL compatible graph. The default, i.e. two GCN layers. We only changed the
features of each node are preprocessed as shown in width of the GCN layers, and not the classification
Fig.9b. AsGCNimplementedbyDGLonlyaccepts layers,asweareinterestedintheimpactofchangesin
1-Dnodefeature,weconcatenatedthespatialcoordi- graphfeatureextractionsontheCGLperformance. In
nateinformationwiththetemporalinformationofeach thedepthexperiment,thewidthoftheGCNlayersis
jointtocreatealongvectorasthefinalnodefeature. settothedefault64hiddenunits.
Forthedataofthe‘N-UCLA’dataset,weusedthe Table5: Hyperparametercandidatesusedforgridsearch.
prepareddatafromCTR-GCN(Chenetal.,2021),for frac memories denotes the percentage of each task that
thedataof‘NTU-RGB+D’,weusedtheprepareddata GEM/REPLAYstoresasbuffer.
fromthe‘PaddleVideo’GitHubrepository1. Weused
algorithm hyperparametergrid
the train data in the x-sub category to construct our
EWC memorystrength:[1,100,10000,1000000]
class-incrementallearningvariantofthedataset.
MAS memorystrength:[1,100,10000,1000000]
lambdal:[100,10000];lambdat:[100,10000];
TWP
beta:[0.01,0.1]
APPENDIX C: EXPERIMENTS LwF lambdadist:[0.1,1,10];T:[0.2,2,20]
memorystrength:[0.05,0.5,5];
GEM
fracmemories:[0.05,0.1,0.2]
Weexecutedallexperiments5timeswiththe‘Tesla REPLAY fracmemories:[0.05,0.1,0.2]
V100-SXM3-32GB‘GPU.Eachtaskislearnedfor100
epochswith0.001learningrateandbatchsize10000,
this is equivalent to full batch training, as our tasks Toensurehyperparameterfairness,weconducted
containlessthan10000datapoints. EachCLprocess agridsearchforeachalgorithmoneachdatasetusing
contains5tasks,wherethemodelsequentiallylearns the default task and class order. The grid is shown
them using the CL method. Each CL process takes inTab.5. ComparedtoCGLB’s(Zhangetal.,2022)
around80minutesforregularization-basedmethods hyperparameter,wereplacedthememorybuffersize
andLwF inNTU-CIL,and10minutesinN-UCLA- from the number of samples to the fraction of task
CIL.ForGEMandREPLAY,ittakesaround100min- samples. Thetwodatasetsthatweimplementedare
utesinNTU-CILand15minutesinN-UCLA-CIL.We of different sizes. It is fairer to assess the result of
computetask-levelaccuracya bytakingthemacro- rehearsal-basedmethodswhenwedefinethefraction
k,k
oftaskdatasamplestobestored. Wereportthebest
1https://github.com/PaddlePaddle/PaddleVideo/blob/ hyperparameterwehavefoundforGCNinTab.3.
develop/docs/en/dataset/ntu-rgbd.md