Binding Touch to Everything:
Learning Unified Multimodal Tactile Representations
FengyuYang1* ChaoFeng2* ZiyangChen2* HyoungseobPark1 DanielWang1 YimingDou2
ZiyaoZeng1 XienChen1 RitGangopadhyay1 AndrewOwens2 AlexWong1
1YaleUniversity 2UniversityofMichigan
Zero-shot Touch Understanding Cross-modal Retrieval Zero-shot Image Synthesis with Touch
Touch Image Audio Text
Input touch Text prompts: Input touch Input touch
”It is
[ CLASS ] touching
Touching the string Touching
denim guitar string Guitar of a guitar.” rock
This Touch-LLM
feels cotton
Generated Image Restyled Image
like Can you tell me materials and hardness
Reference Original Image
Reference nylon of objects in the touch image?
Reference Input touch X-to-Touch Generation
cosine similarity
Input Generated Input Generated
image touch text touch
”The surface
of densely
The materials of the objects in the woven or
touch image are likely to be rocks, or looped
stones, which is a hard and durable. carpet.”
Score
Figure1. Puttingtouch“intouch”withothermodalities. Weshowthatavarietyoftactilesensingtasks,rangingfromtouchimage
understandingtoimagesynthesiswithtouch,canbesolvedzero-shotbyaligningtouchtopretrainedmultimodalmodels,extendingprevious
approachesonworkonothermodalities[35].Ourlearnedmodelcanbeappliedtovariousvision-basedtactilesensorsandsimulators(e.g.,
GelSight,DIGIT,Taxim,andTacto).Forvisualizationpurposes,weshowthecorrespondingvisualsignal(labeled“reference”)foreach
touchsignal,eventhoughitisnotusedbythemodel.
Abstract agequestionanswering. Tothebestofourknowledge,Uni-
Touchisthefirsttodemonstratesuchcapabilities. Project
Theabilitytoassociatetouchwithothermodalitieshas
Page: https://cfeng16.github.io/UniTouch/.
hugeimplicationsforhumansandcomputationalsystems.
However,multimodallearningwithtouchremainschalleng-
ing due to the expensive data collection process and non-
standardizedsensoroutputs. WeintroduceUniTouch,auni- 1.Introduction
fiedtactilemodelforvision-basedtouchsensorsconnected
tomultiplemodalities,includingvision,language,andsound. Amongstourfivemainsenses,touchsensingisperhapsthe
WeachievethisbyaligningourUniTouchembeddingstopre- most crucial to human survival, due to its role in perceiv-
trainedimageembeddingsalreadyassociatedwithavariety
ing physical contact — rivaling even vision in its overall
ofothermodalities. Wefurtherproposelearnablesensor-
importance [46, 73, 79]. Our ability to form cross-modal
specific tokens, allowing the model to learn from a set of
associationsbetweentouchandourothersenses[91]thus
heterogeneoustactilesensors,allatthesametime.UniTouch
underliesagreatdealofourphysicalcapabilities. Forexam-
iscapableofconductingvarioustouchsensingtasksinthe
ple,wepredictfromvisionhowasurfacewillfeelbeforewe
zero-shotsetting,fromrobotgraspingpredictiontotouchim-
touchit,andwepredictfromtouchhowanobjectwillsound
*Indicatesequalcontribution. beforewestrikeit.Thesecross-modalassociationsarealsoa
1
4202
naJ
13
]VC.sc[
1v48081.1042:viXra sessalC
···
···keycomponentofcomputationalsystems,suchasforrobotic GelSightfrom[111] DIGITfrom[94] Taximfrom[32]
manipulation[6,8,65,68,75,83–85,107,114,116], ma-
terialandgeometryestimation[10,38,111,119],assistive
technology[42],andtexturerecognition[50,78,118].
Despite their importance, cross-modal associations be-
tweentouchandothermodalitieshavereceivedconsiderably
GelSlimfrom[33] TACTOfrom[30] DIGITfrom[57]
lessattentionfromthemultimodalresearchcommunitythan
those of other modalities, such as vision, language, and
sound. Touchisexpensivetoacquire[30,32,111]asitre-
quiresactivelyprobingobjectswithtouchsensors,limiting
thescaleofdatacollectedfortrainingtactile“foundation”
models. Moreover,touchsensorsarenotfullystandardized,
Figure2. Tactileimagesofdifferentsensorsanddatasets. In
andthustherearelargedifferencesbetweenoutputsofdif-
contrast to many other modalities, signals from different touch
ferentsensors[31,121]. Evenamongstthecommonlyused sensinghardwareexhibitlargeamountsofvariation.
vision-based sensors, the difference in mechanical design
andelastomericmaterialwillleadtodivergentartifacts,lim-
iting generalization (Fig. 2). As a result, existing tactile thanpreviouslyproposedapproachesonmultipletasks.
representationsaretypicallyconstrainedtoasinglesensor.
Anemerginglineofworkhasaddressedthechallenges 2.RelatedWork
oflearningfromotherlow-resourcemodalities,likesound,
Tactile sensing. Early tactile sensors were chiefly engi-
point clouds, and depth, by aligning examples with pre-
neered to register fundamental, low-dimensional sensory
trainedvision-languageembeddings[35,64,109]. Inthis
outputs such as force, pressure, vibration, and tempera-
paper,weshowthatthisapproachcanbeadaptedtotactile
ture[19,56,61,62]. Lately,therehasbeenagrowingfocus
sensing. Wealigntactilesignalstovisualsignals,thereby
onvision-basedtactilesensors. GelSight [54,117]asone
linkingtouchtoavarietyofothermodalities,suchaslan-
of the representative sensors, features an elastomeric gel
guage and sound. Then we can use the representations
with an embedded camera and illumination system. The
withinoff-the-shelfmodelstrainedonothermodalities(e.g.
geldeformsuponcontactwithanobjectandcreatesahigh-
CLIP [87]), to solve a variety of tactile sensing tasks. To
resolutionheightmapusingphotometricstereo[55],which
dealwiththelargevariationsindifferenttouchsensors,we
provides detailed information about the shape and physi-
trainasinglemodelwithmultipletactilesignalsatonce,and
calpropertiesoftouch[66,97]. Onevariant, DIGIT[59],
introducelearnabletokenstomodelsensor-specificproper-
hasaspeciallydesignedsilicone-basedelastomergelwitha
ties,suchasthecalibrationandintensityprofilesinthetouch
hardersurfaceandadifferentilluminationsystem. Another
signal.
variantGelSlim[97]containsastretchy,loose-weavefabric
Ourtrainedmodel,whichwecallUniTouch,isageneral-
gelsurface. Recentworkalsoturnsintothesimulationof
purposeinterfaceformultiplevision-basedtactilesensors.
tactilesensors[1,17,36,53,90,101]. Taxim[90]simulates
Ourmodelunifiesmanypreviouslystudiedtactilesensing
theopticalresponseofaGelSightsensorandTACTO[101]
tasks“zeroshot”andgreatlyexpandstherangeoftasksthat
calculatesthelocalcontactgeometryandthecorresponding
touchsensingcanbeapplied,asshowninFig.1:(i)Weapply
rendering. Wefocusonthesevision-basedsensorsasthey
ittozero-shottouchunderstandingtaskslikematerialrecog-
arewidelyavailableinvisuo-tactiledatasets[30,32,33,94,
nitionandroboticgraspstabilityprediction. (ii)Weobtain
100,108,111,117,118],arecommonlyusedinvariousap-
strongperformanceincross-modalretrievalwithtouchby
plications[9,11,12,34,41,45,51,60,67,68,95,115,127],
aligningtouchwithothermodalitiesinasharedlatentspace.
andalladoptimageastheoutputformat. Whilethesevision-
(iii)Thelearnedrepresentationcanalsosupportimagesyn-
basedtactilesensorsandsimulatorssharesimilarimaging
thesistasks,includingtouch-to-imagegeneration[71,112]
patterns,thedifferenceindesignandcalibrationresultsin
andtactile-drivenimagestylization[111,112],byusingit
a significant domain gap, as shown in Fig. 2. Hence, re-
withinoff-the-shelftext-to-imagediffusionmodels. (iv)We
searchers typically study each sensor separately. In our
combinetouchwithlargelanguagemodels(LLM),allowing
work,weintroduceanovelapproachtounderstandingmulti-
us to perform tasks such as tactile question answering in
plesensorsthroughourunifiedtouchencoder.
avarietyoftactiledomains,includingcontactlocalization,
graspingstabilityprediction,andetc.(v)Finally,weperform Representationlearningwithtouch. Theinitialefforts
“X-to-touch”generation,producingtouchimagesfromvi- learntactilerepresentationsforspecifictasks[29,63,72,96,
sion,text,andaudio. Ourexperimentssuggestourzero-shot 118]. Leeetal.[63]undertookacollaborativetrainingof
model achieves competitive (or even better) performance ConvolutionalNeuralNetworks(CNN)foranRGBcamera
2and a force sensor to facilitate contact-rich manipulation
Frozen “wood”
tasks. Similarly,Yuanetal.[118]employedacomparable
methodologytoestablishasharedlatentspacebetweenvi- Trainable
sualandtactilemodalitiesusingtheGelsighttouchsensor, Touch Training switch Inference hitting
Encoder wood
aimedatprecisefabricclassification. Recently,researchers
Touch Image Other conditions
havelearnedgeneralrepresentationsoftouchthroughself-
supervision. Yangetal.[111]learnedtactilerepresentations < GelSight > Binding space
Sensor token Contrastive loss
forGelsightsensorswithvisuo-tactilecontrastivemultiview L
coding[98]andKerretal.[57]proposedacontrastivepre-
trainingmethodfortheDIGITsensor. Otherworksadopted
Binding space Stable
BYOLframework[39]orcontrastivepredictivecoding[120] Image Diffusion
Encoder
tolearnrepresentationsfornonvision-basedtactilesensors
likeBioTac. Somework[52]appliesmaskedautoencoders
Touch Generated touch
Image
tolearntactilerepresentationsdirectlyfromtactileinputs.
Unlikemethodsconcentratedsolelyonvisuo-tactilelearning Figure3.Methodoverview.Wealignourtouchembeddingwitha
pre-trainedimageembeddingderivedfromlarge-scalevisionlan-
for a single sensor, our approach aims to learn touch rep-
guagedata,usingsensor-specifictokensformulti-sensortraining.
resentationsthatcanbeappliedacrossvarioussensorsand
interconnectedwithmultiplemodalities.
3.1.Bindingtouchwithimages
Multimodal representation learning. The success of
Welearnamultimodaltactilerepresentationfromtouchand
vision-language pretraining [23, 77, 86, 88, 106] has
visionsolely,withouttheneedforpairedtextandaudiodata
demonstrated the ability to bridge the gap between visual
fortouch. Weachievethatbyaligningourtouchembedding
content, such as images or videos, and textual descrip-
toapretrainedimageembeddingusingcontrastivelearning
tions [48, 49, 69]. Furthermore, some researchers have
asshowninFig.3,wheretheimageembeddingisalready
extendedtheapplicationofCLIPintothe3Ddomain[37,
aligned with modalities like language and audio training
122,123,128]. Someworkslearnsharedaudio-visualrepre-
fromlarge-scaleimage-paireddatasets[35].
sentation[2,13,25,27,44,80,82,93,105]byleveraging
We denote Ω as the visual image domain and Ω as
naturalcorrespondencewithvideos. Someworksalsostudy v t
thetactileimagedomain. Thus,givenB visualandtouch
shared audio-language representation [26, 40, 103]. Ben-
pairsinabatch, (v ,t ) B ,wherev : Ω R2 R3
deretal.[4]craftedanembeddingspacefortheflavorsof
and t : Ω
{R2i i }i R=1
3, we
aligi
n a
tv ac⊂
tile
e→
mbed-
winesbyleveragingbothimageandtextannotations. Chen
ding
i
(t
)t ⊂RC wi→
th the pretrained visual embedding
etal.[15]learnedsharedspatialinformationfrombinaural (vF )T i RC∈
from[35]bymaximizingthecosinesimilar-
soundandvision. Someworkslearnedtheassociationbe- FV i ∈
itybetweencorrespondingvisuo-tactilepairs. Weoptimize
tweenvisionandmetadata[14,102,126]. Imagebind[35]
thisobjectiveusingInfoNCEloss[81]tomatchtouchesto
proposedtolearnajointembeddingforsixdiversemodali-
correctimages:
tiessolelythroughimagealignmentandemergezero-shot
c cr eo ps ts t- om to hd eal sec na sp eab oi fli tt oie us c. hIn ano dur bw ino drk it,w toe oe tx ht ee rn md oth di as lic to ien s-
LT→V
=
−B1 (cid:88)B
log
(cid:80)Bexp e(
xF pT
((t i)
(t·F
)V(v
i
() v/τ ))
/τ),
including text and audio by aligning tactile data with im- i=1 j=1 FT i ·FV j
(1)
ages,encouragingamorecomprehensiveunderstandingof
whereτ isatemperaturehyperparameter[104]andC isfea-
cross-modaltouchinteractionswithoutpaireddata.
turedimension. Analogously,wecanalsomatchfromimage
v totoucht usingtheloss . Thus,weminimizethe
i i V→T
L
3.Method overallloss:
= + . (2)
T→V V→T
Weaimtolearnaunifiedtactilerepresentationfordifferent L L L
touchsensorsthatcapturesrelationshipsbetweentouchand Naturally,minimizingthecontrastiveobjective[27,98,
differentmodalities,e.g. vision,text,andaudio. First,we 110,126]will“pull”avisuo-tactilepairclosetogetherand
presentourcontrastivevisuo-tactilepretraining,inspiredby “push” it away from other pairs, achieving the alignment
[35], that can emerge interconnections of touch and other between touch and visual embedding. As the visual em-
modalities. We then introduce our touch encoder design beddingcomesfromalearnedjointspacethathasalready
and data sampling strategy that can be used for different alignedwithdifferentmodalities,touchthatisboundwith
tactilesensorsatonce. Finally, weshowhowourlearned imageswillbridgeaconnectiontoothermodalities,yielding
representationcanbeappliedtovariousdownstreamtasks. amulti-modalunifiedtactilerepresentation.
33.2.Learningfrommultiplesensorsatonce Material Robot
Dataset Sensor #data
cls. grasp
Wewanttolearnageneralizabletactilerepresentationthat
TouchandGo[111] GelSight 120k ✓
willbesuitablefordifferenttactilesensors. Therefore,we
TheFeelingofSuccess[6] GelSight 9.3k ✓
designedour touch encoder tobridgethe domaingap
FT YCB-Slide[94] DIGIT 183k ✓
among various vision-based tactile sensors caused by the ObjectFolder2.0[32] Taxim 180k ✓ ✓
differenceinsensordesigns.
ObjectFolderReal[33] GelSlim 20k ✓
Specifically, we introduce a set of learnable sensor- ObjectFolder1.0[30] TACTO 20k ✓ ✓
specific tokens {s k }K k=1, where s k
∈
RL×D, to capture SSVTP[57] DIGIT 4.6k ✓
specific details for each senor, e.g., calibration and back-
Table1.Datasetsfortrainingandevaluation.
groundcolorintouchimages,sothattheremainingmodel
capacity can be used to learn common knowledge across
differenttypeoftouchsensors,suchastextureandgeome-
nearestneighborsensor-specifictokensfromthelearnedsen-
try. Here,K representsthenumberofsensorswetrainon,
sorset s N . Specifically,wefirstcomputeaprototype
Listhenumberofsensor-specifictokensforeachsensor, { k }k=1
foreachsensor,a1Dvectorthataveragesalltherawpixels
andD isthetokendimension. Forthegiventouchimage
belongingtothetactileimagescollectedbythissensor,and
t , and its corresponding tactile sensor tokens s , we ap-
i ti storetheseprototypesaftertraining. Then,duringtheinfer-
pendthesesensor-specifictokensasprefixestotouchimage
encestage,wecomputetheL1distanceofbetweenaninput
patchtokensandthenencodethemwithourtouchencoder
tactileimageandallthesensorprototypesandretrievethe
resultinginthefinalembedding (t ,s )(Fig.3). Forour
FT i ti sensorwithminimumdistance.
contrastivevision-touchpretraining,weoptimize:
3.3.Applications
B
=
1 (cid:88)
log
exp( FT(t i,s ti) ·FV(v i)/τ)
,
LT→V −B (cid:80)B exp( (t ,s ) (v )/τ) Byaligningourtouchembeddingtothejointlatentspace,we
i=1 j=1 FT i ti ·FV j establishalinkbetweentouchandothermodalities. These
(3)
alignmentsallowustoperformvariouszero-shotandcross-
aswellas fromtheotherdirection.
V→T
L modalapplicationswithoutanyfurthertraining.
In-batchdatasampling. Wefoundthatbatchsampling
strategy [18] plays an important role when we train with Zero-shottouchunderstanding. Emergentalignmentof
data,acquiredbymultipletouchsensors,usingcontrastive touchandtextenableszero-shottouchunderstanding,e.g.,
learning. The model will under-perform if we randomly materialclassificationandgraspstabilityprediction. Follow-
samplefromeachdatasource[113]whichresultsinasurplus ingCLIP[88],weencodethetouchimagesandtextprompts
ofeasynegativesduetothedomaingapbetweendifferent withtemplatesandclassnames. Wecomputetheirsimilarity
sensors. Therefore,wedesignabatchsamplingstrategyto scoreandrankthemtoachievethezero-shotclassification.
guaranteethatσpercentoftrainingexamplesinabatchare
Touch-LLM. Using an existing vision-language
sampledfromthesamedatasets. Giventhatourdataset
D model [28, 124] with the image embedding [35] that
istheunionoverN datasetscollectedwithdiversetactile
(cid:83) we align our touch embedding with, we can create our
sensors = ,theprobabilityofselecting
D
n∈{1,2,...,N}Dn
touch-language model by switching to our touch encoder.
agivendatasetD tosamplefromisdefinedas:
n
Giventhetouchimageandlanguageinputs,wecanobtaina
morecomprehensiveunderstandingviaquestion-answering.
p = ∥Dn ∥ , (4)
n (cid:80)N
m=1∥Dm ∥ Imagesynthesiswithtouch. Bindingtouchwithtextalso
where denotes cardinality. denotes the selected opensupmorepotentialabilitiesforimagesynthesiswith
σ
dataset∥ fr· o∥ mwhichweperformunD iformrandomsampling touch. Weleveragethepretrainedtext-to-imagediffusion
toyieldσ B examples;therest(1 σ) B examplesare model[89]anduseourtouchfeaturestoconditionthede-
uniformly· sampledfromotherdatase− ts,i.e· ., ,where noisingprocess,achievingzero-shottouch-to-imagegenera-
σ
σ is a hyperparameter range from 0 to 1 reD pr\ esD enting the tion[71,112]andtactile-drivenimagestylization.
portionofthebatch. Thisbatchsamplingstrategysignifi-
X-to-touchgeneration. Wealsoconnectothermodalities
cantlybenefitsourtrainingasitallowsthemodeltomostly
totouchusingthediffusionmodelsothatwecanachievex-
focusonintra-sensorhardnegativesbutstillbeexposedto
to-touchgeneration,whereweimaginethetouchbyseeing,
differentsensorstoenhanceinter-sensordiscrimination.
describing,orlistening.Wetrainanimage-to-touchdiffusion
Inference. Togeneralizeourlearnedrepresentationtoun- model[112]usingthepretrainedjointimageembeddingand
seentypesofsensorsduringtheinference,weretrievethe thenwecangeneratetouchfromtextandaudioaswell.
4
lavE&niarT
.lavEPretrain IndomainDatasets Out-of-domainDatasets
Method
Data
TouchandGo ObjectFolder2.0 YCB-Slide ObjectFolder1.0 ObjectFolderReal SSVTP
Chance – 5.0 14.2 10.0 14.2 14.2 16.6
Supervised ImageNet 47.1 70.3 72.3 37.5 54.8 73.4
VTCMC[111] Single 56.5 74.3 75.2 – – –
SSVTP[57] Single 47.6 69.8 74.8 – – –
LinearProbing
VTCMC[111] All 49.2 70.3 69.5 33.8 48.1 68.5
SSVTP[57] All 43.8 68.9 67.4 35.1 49.7 66.8
Ours All 61.3 85.4 78.1 41.3 61.2 77.4
Zero-Shot Ours All 52.7 43.5 66.4 32.7 33.2 60.9
Table2.Tactilematerialclassification.WecompareourtouchfeatureswithothermethodsandImageNetpretraining.Wealsoreportour
zero-shotclassificationperformance.Themetricisaccuracy(%).
Pretrain Indomain Out-of-domain sor interactions, and the multimodal dataset ObjectFolder
Method
Data 2.0[32]whichcontainssimulatedvisual,tactile,andaudio
Feeling OF2.0 OF1.0
dataofdailyobjectsusingTaximtactilesimulators. Wetrain
Chance - 52.3 52.0 50.7
ourmodelsolelyonthenaturallypairedimageandtouch
Supervised ImageNet 75.9 70.1 68.9
dataviaself-supervision. Totestthegeneralizationability
VTCMC[111] Single 80.1 74.8 -
Linear SSVTP[57] Single 80.3 74.0 - ofourmodel,wealsoevaluateitwiththreeout-of-domain
Probing VTCMC[111] All 66.1 65.8 67.2 datasets with two unseen sensors, including ObjectFolder
SSVTP[57] All 65.8 64.2 65.3
Real[33],ObjectFolder1.0[30]andSSVTP[57].Wespecif-
Ours All 82.3 78.1 75.8
icallyselectobjects101-1000fromObjectFolder2.0toavoid
Zero-Shot Ours All 65.5 64.3 64.7
overlapwithObjectFolder1.0. Also,ObejctFolderRealcon-
Table3.Roboticsgraspingstabilityprediction.Wecompareour tainsobjectsdistinctfromthoseinObjectFolder1.0and2.0.
touchfeatureswithothermethodsandImageNetpretrainingon
PleaseseeAppendixA.1formoredetails.
graspingstabilitypredictiontask.Wereportourzero-shotresults.
Themetricisaccuracy(%). 4.1.UniTouchrepresentation
First,weevaluatethequalityofourlearnedtouchfeatures
4.Experiments fordownstreamtasks: materialclassificationandgrasping
stabilitypredictionvialinearprobing. Wefreezethelearned
Weevaluateourmodelonextensivetasksspanningvarious
touchembeddingsandtrainalinearclassifieronthedown-
applicationdomains,includingzero-shottouchunderstand-
streamtasksforspecificdatasets.
ing,cross-modalretrieval,zero-shotimagesynthesiswith
touch,Touch-LLM,andX-to-touchgeneration. Baselines. Wecompareourmodelwithtworecentvisuo-
tactileself-supervisedmethodsforvision-basedtactilesen-
Implementations. WebaseourmodelonImageBind[35]. sors: VTCMC[111]andSSVTP[57]. Wealsoadoptthem
WeusetheAdamWoptimizer[58,76]withthebaselearning toourmulti-datasetsetup. Weusethesamearchitecturesto
rateof1 10−5 andcosinedecaylearningratescheduler. ensureafaircomparison. Wealsocomparewiththesuper-
×
Wetrainourmodelwithabatchsizeof48oneachofthe visedImageNet[22]features,whicharecommonlyusedto
4NVIDIAA40GPUsfor150epochs. Wesetthetemper- representtactileimages[6,7,119]. Following[6,33,111],
ature parameter τ = 0.07. We adopt Vision Transformer we evaluate models’ performance via accuracy metric for
(ViT)[24]asthebackboneforourtouchencoder,whichcon- bothdownstreamtasks.
tains24multi-headattentionblockswith16headsoneach.
Material classification. We evaluate the touch material
ThefeaturedimensionC is1024. WeuseL=5learnable
classificationtaskonthreein-domaindatasetsTouchandGo,
tokensforeachsensortypeinourpretrainingdatasetswith
ObjectFolder2.0,andYCB-Slide,andthreeout-of-domain
K =3differentsensors. Forthein-batchsampling,weset
datasetsObjectFolder1.0,ObjectFolderReal,andSSVTP.It
σ = 0.75, meaning that 75% of the data comes from the
isworthnotingthatObjectFolderRealandObjectFolder1.0
samedataset,withtheremaindersourcedfromothers.
containsensorsneverseenduringthetraining.
Datasets. Wetrainandevaluateourmodelonfourvisuo- Tab.2showsresultsonlinearprobing. UniTouchoutper-
tactiledatasetscollectedbythreedifferentvision-basedtac- formsallthebaselinesbyalargemargin,implyingthatour
tilesensors(Tab.1). Theseincludethereal-worlddataset tactilerepresentationsbenefitfromthealignmenttoawell-
Touch and Go [111], the robotic dataset Feeling of Suc- structuredembeddingspacetrainedonlarge-scaledatasets.
cess[6],theYCB-Slide[94]datasetfeaturingDIGITsen- Inaddition,theconsistentimprovementsacrossalldatasets
5Touch Reference Vision-from-touch Ours Touch Reference Source Vision-from-touch Ours
Touch-to-Image Generation Tactile-driven Image Stylization
Figure4. Zero-shotimagesynthesiswithtouch. (Left)Wegenerateanimageofascenegivenatactilesignal. (Right)Weperform
tactile-drivenimagestylizationtomanipulateanimagetomatchagiventouchsignal. Wecompareourmethodtothestate-of-the-art
superviseddiffusionmethod[112]trainedonTouchandGo. Wedenote“reference”asvisualimagespairedwiththeinputtouchinthe
dataset,whicharenotseenbythemodelbutonlyshownforthedemonstrationpurpose.SeeAppendixA.4formoreexamples.
RetrievedModality 4.2.Zero-shottouchunderstanding
Method
Touch Vision Touch Audio Touch Text
→ → → WefurtherevaluateUniTouchwithzero-shotclassification
Chance 1.0 1.0 1.0
tasks,enabledbytheemergentalignmentwithtextduring
CCA† 8.50 6.18 -
PLSCA† 6.25 7.11 - pretraining. Weperformmaterialclassificationandgrasping
DSCMR† 4.92 6.15 - predictiontasksbycomputingthecosinesimilaritybetween
Fully DAR† 8.80 7.77 -
the embeddings of touch and corresponding text prompts.
supervised CCA 17.8 15.7 16.8
PLSCA 16.8 15.9 18.2 Classpredictionsarechosenbasedonhighestscores,without
DSCMR 26.5 19.6 22.7 training on labeled data. To the best of our knowledge,
DAR 32.3 27.8 31.9
therearenootherbaselinesthatcanperformzero-shottouch
Zero-shot Ours 41.9 37.9 38.0
understandinginourmanner.
Table 4. Cross-modal retrieval from touch. We evaluate the
performanceusingmeanAveragePrecision(mAP)onObjectFolder Material classification. We conduct zero-shot material
2.0.†denotesresultsfrom[33]. classificationbypromptingthemodelwith“Thisfeelslike
[CLS]”,where[CLS]isthenameofthematerial. Weshow
ourzero-shotperformanceinthelastrowofTab.2.Ourzero-
shotmethodshowsacomparableperformanceagainstsev-
and sensors validate our proposed sensor-specific tokens
eralsupervisedmethods,whichnotonlyindicatesastrong
andin-batchsamplingstrategyduringtraining–resultingin
tactilerepresentationthatiswell-alignedwiththetextbut
insignificantgeneralizationgainsacrossdifferentsensors.
alsoshowsthatoff-the-shelfmodelstrainedforothermodal-
itiescanbeusedtosuccessfullysolvetouchsensingtasks.
Graspingstabilityprediction. Similarly,weperformthe
Graspingstabilityprediction. Wefollowthesettingof[6,
zero-shot grasping stability prediction task by using text
33]topredict,fromtactileinput,whetheraroboticgripper
promptslike“theobjectisliftedintheair”and“”theobject
cansuccessfullygraspandstablyholdanobjectbeforeitis
is falling on the ground”. Tab. 3 shows that we are com-
lifted. Failuresoccurwhenthegraspedobjectslipsbymore
parabletosomeofthesupervisedmethods,demonstrating
than3cm. WeevaluateUniTouchonthreedatasets: Feeling
thecapabilitiesofaligningtouchandtextcanbeextended
ofSuccess,ObjectFolder2.0,andObjectFolder1.0,where
to robotics tasks, which may be out of the training scope
ObjectFolder1.0isanout-of-domaindataset.
of the vision language model like CLIP with appropriate
ThelinearprobingresultsareshowninTab.3. Ourper- prompting. Thismaycomefromthefactthatwelinkthe
formanceconsistentlyoutperformsexistingbaselinesbya touchofthesuccessfulgraspstotherobot’sactionoflifting
largemargin. Thus,wefurtherdemonstratethatourmodel objectswhilefailedgraspsasthosefalling. Wefoundconsis-
designandtrainingparadigmareusefulnotonlyincomputer tentperformanceinbothinandout-of-distributiondatasets,
visionbutalsocanbegeneralizedtoroboticstasks. demonstratingthegeneralizationcapabilityofthislink.
6Can this object be grasped
Can you describe material
into the air? Explain why by What part of the table this
of the object presented in
describing where specifically touch located?
touch image?
is the gripper touching on
the object.
Reference Input touch Reference Input touch
Reference Input touch
The object in the touch
No, the object cannot be image is a green, grassy The touch is located
grasped into the air as the blade. It is a thin, flat piece on the corner of
gripper is touching the of grass that is likely part of the table.
object at the edge. a larger grassy field or lawn.
Figure5.Touch-LLM.OurTouch-LLMcanconductaseriesoftactilequestion-answertaskssuchasrobotgraspingstabilityprediction,
contactlocalization,andtouchimagecaptioning.Wealsoshow“reference”visualimagespairedwiththeinputtouch,forbetterdemonstration.
SeeAppendixA.4formoreexamples.
EvaluationMetrics Eval
Method Method LLM
CVTP(↑) Material(↑) FID(↓) GPT-4Rating(↑)
Pix2Pix[47] 0.09 0.15 136.4 BLIP-2[70] Vicuna[16] 1.01
Supervised VisGel[71] 0.10 0.20 128.3 InstructBLIP[20] Vicuna[16] 1.93
Vision-from-touch [112] 0.35 0.25 81.2 LLaVA-1.5[74] Vicuna[16] 2.33
Zero-shot Ours 0.56 0.31 103.11 Touch-LLM(ours) LLaMA[99] 3.30
Table5.Zero-shottouch-to-imagegenerationonTouchandGo. Table6.Touchimagecaptionevaluation.WeevaluateourTouch-
LLMandthreebaselinesonourtestcasesfromTouchandGo[111].
Eachmodel’sresponseisratedbyGPT-4onascalefrom1to5.
4.3.Cross-modalretrievalwithtouch
Weconductcross-modalretrievaltoevaluatethealignment
margin(Tab.4). Thisdemonstratesourstrongcross-modal
ofourtouchembeddingstothoseofothermodalities. Given
abilitytoaligntouchwithothermodalitieswithouttheneed
atouchimage,weaimtoidentifythecorrespondingvision,
forexplicitpairedtrainingdataoradditionalsupervision.
text,andaudiodescribingthesamepointofcontact.
4.4.Imagesynthesiswithtouch
Experimental setup. We evaluate on ObjectFolder 2.0
cross-sensoryretrievalbenchmark[33]. Following[33],we
Inthispart,wedemonstratethatwecancombineourtouch
treat points from the same object as positive samples and
embeddingwithanoff-the-shelfimagesynthesismodeleas-
evaluateusingmAP.Toevaluatetouch-to-textretrieval,we
ilytoperformtheimagesynthesistasksconditioningtouch
annotatedtextdescriptionsthatdepictthecontactpointofthe
imagesinazero-shotmanner. Weperformtwotasks: touch-
objectfromitsvisualinput,servingaspairedground-truth
to-imagegeneration[71,112]andtactile-drivenimagestyl-
text. Weobtaintheretrievalresultbyrankingthecosinesim-
ization[111,112]. Following[111,112],weusethreeevalu-
ilaritybetweenaninputtouchandothermodalities. Given
ationmetrics: FrechetInceptionDistance(FID),Contrastive
thatourmethodisnottrainedwithpairedaudioortextdata,
Visuo-TactilePre-Training(CVTP),andmaterialclassifica-
we consider its performance in these two modalities as a
tionconsistency. SeeAppendixA.3fordetails.
demonstrationofzero-shotlearning.
Touch-to-imagegeneration. Weaimtogenerateimages
Baselines. We compare our method with several estab-
solely from touch. We use a pretrained text-to-image dif-
lished baselines, including Canonical Correlation Analy-
fusionmodel[89],conditioningonourtouchfeatures,and
sis(CCA)[43],PartialLeastSquares(PLSCA)[21],Deep
guidingthedenoisingprocess. Comparedtothestate-of-the-
AlignedRepresentations(DAR)[3],andDeepSupervised
art visuo-tactile diffusion-based model [112], our method
Cross-ModalRetrieval(DSCMR)[125].
generatesmorerealisticobjectsthathavenotbeenpreviously
Results. UniTouchachievesstate-of-the-artperformance seeninthedataset(see Fig.4(left)).Whiletheimagesgener-
on all three modalities and outperforms those supervised atedby[112]notonlyincludethesensorandthearmholding
methodsthataretrainedwithpairedmodalitiesbyalarge itbutalsocloselyresemblethevisualimagesinthetraining
7Datasets otherVLMsbyalargemargin, indicatingthatourTouch-
Prompt
TouchandGo OF2 LLMhasmuchbetterunderstandingcapabilitiesfortouch
This is an image of [CLS] 40.7 34.3 images even with a less powerful LLM than Vicuna [16]
This is a touch image of [CLS] 43.8 36.8 whichusedbyothermodels. SeeAppendixA.3formore
This looks like [CLS] 49.3 41.7 details.
This feels like [CLS] 52.7 43.5
Image of [CLS] 48.8 40.3 4.6.X-to-touchgeneration
Touch of [CLS] 51.2 40.9
We conduct X-to-touch generation to synthesize realistic
Table 7. Prompt analysis for touch. We evaluate our prompt
tactileimagescorrespondingtotheinputmodalityofvision,
designsforzero-shotmaterialclassificationonTouchandGoand
language,andaudio. Fig.1showsplausibleandconsistent
ObjectFolder2.0datasets.
tactileimagesgeneratedfromboththevisualinputandits
textcaptioning. Quantitatively,weevaluateourmodelon
Touch and Go [111], where we measure material classifi-
set. Tab. 5 shows quantitative results, where we compare
cation consistency between touch images generated from
withVision-from-touch[112],VisGel[71]andPix2Pix[47]
visionanditscorrespondinglanguagecaptions. Ourmodel
onTouchandGo[111]. DespiteaslightlylowerFIDscore
achieves55.3%consistency,illustratingthereliabilityofthe
comparedto[112],ourmethodoutperformsontheCVTP
generated results. See Appendices A.3 and A.4 for more
andmaterialconsistencymetrics. Thissuggeststhatwhile
examplesanddetails.
our generated images are out of the distribution of Touch
andGo,ourapproacheffectivelybridgesvisionandtouch.
4.7.Ablationstudy
Tactile-drivenimagestylization. Wealsomanipulatean Learningfrommultiplesensors. Tab.8ablatestheim-
image to align with a given touch signal [111, 112] zero portanceofeachmoduledesignonthezero-shotmaterial
shot. Weachievethisbymixingtheinputimageembedding classificationtaskwiththeTouchandGodataset. Thebase-
withourconditionedtouchembeddingandfeedingitinto line,avanillatransformermodelaligningtouchembedding
thepretraineddiffusionmodel. Weshowqualitativeresults toafixedvisionencoder, dropsperformancesignificantly
inFig.4(right),wheretheinputimageisoutofthedistri- when applied to multiple sensors and datasets, i.e., from
butionofTouchandGo[111]. Weobservethesupervised 43.1%to21.4%,indicatingthedifficultyofthesensordo-
state-of-the-artmethod[112]failstochangethevisualstyle maingap. Weimprovetheperformanceby17%byadding
accordingtothetouchimageseventhoughtheseareseen thesensor-specifictokenstoit. Similarly,wefounda19%
duringthetrainingstage. SeeAppendixA.4formoredetails. byaddingoursamplingstrategy. Withourproposedbatch
samplingstrategyandsensor-specifictokens,ourmodelcan
4.5.Touch-LLM
achievestrongperformance,surpassingthemodeltrained
on a single dataset, which emphasizes the significance of
Interpretingvision-basedtouchimages,crucialfordelicate
ourproposedmethodsforlearningabettertouchrepresen-
tasksinfieldslikerobotics,ischallengingduetohumanper-
tationfrommultiplesensors. Wearguethatthisisbecause
ceptuallimitations. Toaddressthis,weintegrateUniTouch
sensor-specificembeddingshelpdistinguishhardsamples
embeddingintoalargelanguagemodel(LLM),leveraging
fromdifferentsensorswhilesamplingstrategyhelpsiden-
itsrobustunderstandingandreasoningcapabilitiesfortouch
tify hard negatives within the same sensor in the training.
image interpretation, and name it as Touch-LLM. Touch-
Combiningthese,wecantackleinter-sensorandintra-sensor
LLMiscapableofaseriesoftactiletaskssuchasgrasping
hardsamplesthusobtainingtheperformanceboost.
stabilityprediction,touchimageinterpretation,tactilecon-
tactlocalizationandetc., mostofwhicharenon-trivialto Language prompting for touch. We explore how lan-
humans,demonstratingtheusefulnessofcombiningtouch guagepromptingcanhelpwithunderstandingtouch,thefirst
withLLMs. WeshowsomeexampletasksinFig.5. endeavorinthisdomain. Giventhatvisioncapturesmore
Quantitatively,wecompareourmodelwiththreeopen- globalandsemanticinformation,andtouchfocusesonmate-
source vision-language models (VLMs): BLIP-2 [70], In- rialproperties,texture,andmicrogeometry,directlyadopting
structBLIP [20], and LLaVA-1.5 [74] in the touch image promptsfromvision-languageworksmaynotyieldsatisfac-
captioning task by feeding them the same touch images toryresults. Wedesigntouch-specificprompttemplatesby
and text prompts. We manually create captions for 400 adoptingthecommonpromptsfromvision-languageworks
randomlysampledRGBimagesfromTouchandGo[111] and replacing with words related to haptics, i.e., chang-
as the ground truth. Following [5], we use GPT–4 to per- ing“image”to“touch image”and“look like”to
formautomaticevaluationbyinstructingGPT-4torateeach “feel like” (see Tab. 7). We evaluate them using the
model’sgenerationsonascaleof1to5giventhereference zero-shotmaterialclassificationtaskonTouchandGoand
response. AsshowninTab.6,ourTouch-LLMoutperforms ObjectFolder2.0. Weempiricallyfoundthatourprompts
8Pretrain Eval [2] YukiAsano,MandelaPatrick,ChristianRupprecht,andAn-
Method
Data dreaVedaldi. Labellingunlabelledvideosfromscratchwith
TouchandGo
multi-modalself-supervision. AdvancesinNeuralInforma-
Chance – 16.7 tionProcessingSystems,33:4660–4671,2020. 3
Baseline TouchandGo 43.1
[3] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. See,
Baseline All 21.4
Baseline+sensortoken All 38.1 hear, and read: Deep aligned representations. ArXiv,
Baseline+sample All 40.3 abs/1706.00932,2017. 7
Baseline+sensortoken+sample All 52.7 [4] ThorannaBender,SimonMøeSørensen,AlirezaKashani,
K Eldjarn Hjorleifsson, Grethe Hyldig, Søren Hauberg,
Table8.Ablationstudy.Weablatetheeffectivenessofeachofour
SergeBelongie,andFrederikWarburg. Learningtotaste:A
proposedcontributionsviathezero-shotmaterialclassification.
multimodalwinedataset. arXivpreprintarXiv:2308.16900,
2023. 3
cansignificantlyimprovetheperformance,indicatingthat [5] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao,
languagecanindeedunderstandtouch. Wesuspectthisphe- WanrongZhu,AnasAwadalla,JoshGardner,RohanTaori,
andLudwigSchimdt. Visit-bench:Abenchmarkforvision-
nomenonmaybeduetothedesignofvisuo-tactiledatasets,
languageinstructionfollowinginspiredbyreal-worlduse.
whichfeaturehumanorrobotictouchactions,thusenabling
arXivpreprintarXiv:2308.06595,2023. 8,16
themodeltoassociatetactileimageswiththeseactions.
[6] RobertoCalandra,AndrewOwens,ManuUpadhyaya,Wen-
zhen Yuan, Justin Lin, Edward H Adelson, and Sergey
5.Discussion
Levine. Thefeelingofsuccess: Doestouchsensinghelp
WeintroducedUniTouch,aunifiedmultimodaltactilerepre- predictgraspoutcomes? ConferenceonRobotLearning
(CoRL),2017. 2,4,5,6,15
sentationforvision-basedtactilesensors.Toachievethis,we
[7] Roberto Calandra, Andrew Owens, Dinesh Jayaraman,
alignourtouchembeddingtoasharedmultimodalembed-
JustinLin,WenzhenYuan,JitendraMalik,EdwardH.Adel-
dingspaceusingcontrastivelearning. Wefurtherintroduce
son,andSergeyLevine. Morethanafeeling: Learningto
sensor-specifictokensthatenableslearningfromdifferent
graspandregraspusingvisionandtouch. IEEERobotics
sensorsallatonce. UniTouchunifiesmanyexistingtactile
andAutomationLetters,3:3300–3307,2018. 5
sensingtasksandsignificantlyexpandstherangeoftasksfor
[8] GuanqunCaoandShanLuo. Multimodalperceptionfor
touchsensing. Nonetheless,thefieldofmultimodal(foun-
dexterousmanipulation. ArXiv,abs/2112.14298,2021. 2
dational)modelisadmittedlystillyoung. Agents,likeour-
[9] Guanqun Cao, Yi Zhou, Danushka Bollegala, and Shan
selves,leveragecomplementarystrengthsofmulti-sensory
Luo. Spatio-temporal attention model for tactile texture
observations,incorporatingallfivesensesineverydaytasks. recognition. 2020IEEE/RSJInternationalConferenceon
Withthatgoalinmind,weseeourworkasaconcretestep IntelligentRobotsandSystems(IROS),pages9896–9902,
towardsthatdirection,openingnewavenuesformultimodal 2020. 2
touchexperiencebeyondvisionandtouchandintegrating [10] Guanqun Cao, Jiaqi Jiang, Chen Lu, Daniel Fernandes
tactilesensingintomultimodalfoundationmodels. Gomes,andShanLuo. Touchroller: Arollingopticaltac-
tilesensorforrapidassessmentoflargesurfaces. ArXiv,
Limitations. Asthefullrangeoftactilesensorsexhibitdif-
abs/2103.00595,2021. 2
feringoutputformats(e.g. image,barometricsignals,force),
[11] GuanqunCao,JiaqiJiang,NingtaoMao,DanushkaBolle-
welimitourscopetovision-basedtactilesensors. Scaling
gala,MinLi,andShanLuo. Vis2hap:Vision-basedhaptic
upourtrainingstrategyiskeytofurtherintegrateemerging
rendering by cross-modal generation. 2023 IEEE Inter-
tactile sensors in the future. In addition, like other multi- nationalConferenceonRoboticsandAutomation(ICRA),
modal foundational models, our representation is “black- pages12443–12449,2023. 2
box”,whichdoesnoteasilyforinterpretabilityinthespace, [12] ArkadeepNarayanChaudhury,TimMan,WenzhenYuan,
whereonemaybenefitfromexplainability. andChristopherG.Atkeson. Usingcollocatedvisionand
tactilesensorsforvisualservoingandlocalization. IEEE
Acknowledgements. WethankJiachengZhang,Shaokai
RoboticsandAutomationLetters,7:3427–3434,2022. 2
Wu and Chenyang Ma for the helpful discussions and
feedbacks on our manuscript. This work is supported by [13] JiabenChen,RenruiZhang,DongzeLian,JiaqiYang,Ziyao
NSF 2112562 Athena AI Institute and Sony Research. Zeng, and Jianbo Shi. iquery: Instruments as queries
foraudio-visualsoundseparation. InProceedingsofthe
IEEE/CVF Conference on Computer Vision and Pattern
References Recognition,pages14675–14686,2023. 3
[14] Shixing Chen, Chun-Hao Liu, Xiang Hao, Xiaohan Nie,
[1] ArpitAgarwal,TimMan,andWenzhenYuan. Simulation MaximArap,andRaffayHamid. Movies2scenes: Using
ofvision-basedtactilesensorsusingphysicsbasedrender- moviemetadatatolearnscenerepresentation. InProceed-
ing. 2021IEEEInternationalConferenceonRoboticsand ingsoftheIEEE/CVFConferenceonComputerVisionand
Automation(ICRA),pages1–7,2020. 2 PatternRecognition,pages6535–6544,2023. 3
9[15] ZiyangChen,ShengyiQian,andAndrewOwens. Sound visualinstructionmodel. arXivpreprintarXiv:2304.15010,
localizationfrommotion:Jointlylearningsounddirection 2023. 4,16
andcamerarotation.arXivpreprintarXiv:2303.11329,2023. [29] RuihanGao,TasbolatTaunyazov,ZhipingLin,andY.Wu.
3 Supervised autoencoder joint learning on heterogeneous
[16] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhanghao tactilesensorydata:Improvingmaterialclassificationper-
Wu,HaoZhang,LianminZheng,SiyuanZhuang,Yonghao formance. 2020 IEEE/RSJ International Conference on
Zhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. IntelligentRobotsandSystems(IROS),pages10907–10913,
Vicuna:Anopen-sourcechatbotimpressinggpt-4with90%* 2020. 2
chatgptquality,2023. 7,8 [30] RuohanGao,Yen-YuChang,ShivaniMall,LiFei-Fei,and
[17] AlexChurch,JohnLloyd,RaiaHadsell,andNathanF.Lep- JiajunWu. Objectfolder:Adatasetofobjectswithimplicit
ora. Tactilesim-to-realpolicytransferviareal-to-simimage visual,auditory,andtactilerepresentations. InCoRL,2021.
translation. InConferenceonRobotLearning,2021. 2 2,4,5,15
[18] QuanCui,BoyanZhou,YuGuo,WeidongYin,HaoWu, [31] RuihanGao, TianTian, ZhipingLin, andY.Wu. Onex-
OsamuYoshie,andYuboChen.Contrastivevision-language plainabilityandsensor-adaptabilityofarobottactiletexture
pre-trainingwithlimitedresources.InEuropeanConference representationusingatwo-stagerecurrentnetworks. 2021
onComputerVision,2022. 4 IEEE/RSJInternationalConferenceonIntelligentRobots
[19] Mark R. Cutkosky, Robert D. Howe, and William R. andSystems(IROS),pages1296–1303,2021. 2
Provancher.Forceandtactilesensors.InSpringerHandbook
[32] RuohanGao,ZilinSi,Yen-YuChang,SamuelClarke,Jean-
ofRobotics,2008. 2
netteBohg,LiFei-Fei,WenzhenYuan,andJiajunWu. Ob-
[20] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat
jectfolder2.0: Amultisensoryobjectdatasetforsim2real
Tiong,JunqiZhao,WeishengWang,BoyangAlbertLi,Pas-
transfer. InCVPR,2022. 2,4,5,15,21
cale Fung, and Steven C. H. Hoi. Instructblip: Towards
[33] RuohanGao,YimingDou,HaoLi,TanmayAgarwal,Jean-
general-purpose vision-language models with instruction
nette Bohg, Yunzhu Li, Li Fei-Fei, and Jiajun Wu. The
tuning. ArXiv,abs/2305.06500,2023. 7,8
objectfolderbenchmark:Multisensorylearningwithneural
[21] SijmendeJong,BarryM.Wise,andN.L.Ricker. Canon-
andrealobjects. InProceedingsoftheIEEE/CVFConfer-
icalpartialleastsquaresandcontinuumpowerregression.
enceonComputerVisionandPatternRecognition(CVPR),
JournalofChemometrics,15,2001. 7
pages17276–17286,2023. 2,4,5,6,7,15
[22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
[34] RuihanGao,WenzhenYuan,andJun-YanZhu.Controllable
andLiFei-Fei. Imagenet:Alarge-scalehierarchicalimage
visual-tactilesynthesis. InProceedingsoftheIEEE/CVF
database. In2009IEEEConferenceonComputerVision
InternationalConferenceonComputerVision(ICCV),pages
andPatternRecognition,pages248–255,2009. 5
7040–7052,2023. 2
[23] KaranDesaiandJustinJohnson. Virtex: Learningvisual
[35] RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,Mannat
representationsfromtextualannotations. InProceedingsof
Singh,KalyanVasudevAlwala,ArmandJoulin,andIshan
theIEEE/CVFconferenceoncomputervisionandpattern
Misra. Imagebind: One embedding space to bind them
recognition,pages11162–11173,2021. 3
all. 2023IEEE/CVFConferenceonComputerVisionand
[24] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,
PatternRecognition(CVPR),pages15180–15190,2023. 1,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
2,3,4,5
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
[36] DanielFernandesGomes,PaoloPaoletti,andShanLuo. Be-
vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis
yondflatgelsightsensors:Simulationofopticaltactilesen-
worth16x16words:Transformersforimagerecognitionat
sorsofcomplexmorphologiesforsim2reallearning. ArXiv,
scale. InInternationalConferenceonLearningRepresenta-
abs/2305.12605,2023. 2
tions,2021. 5
[25] YuexiDu,ZiyangChen,JustinSalamon,BryanRussell,and [37] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang,
AndrewOwens.Conditionalgenerationofaudiofromvideo XianzhengMa,JiamingHan,KexinChen,PengGao,Xi-
viafoleyanalogies. InConferenceonComputerVisionand anzhi Li, Hongsheng Li, et al. Point-bind & point-llm:
PatternRecognition2023,2023. 3 Aligningpointcloudwithmulti-modalityfor3dunderstand-
ing,generation,andinstructionfollowing. arXivpreprint
[26] BenjaminElizalde,SohamDeshmukh,MahmoudAlIsmail,
arXiv:2309.00615,2023. 3
and Huaming Wang. Clap learning audio concepts from
naturallanguagesupervision. InICASSP2023-2023IEEE [38] AnupamK.Gupta,LaurenceAitchison,andNathanF.Lep-
InternationalConferenceonAcoustics,SpeechandSignal ora. Tactile image-to-image disentanglement of contact
Processing(ICASSP),pages1–5.IEEE,2023. 3 geometryfrommotion-inducedshear. In5thAnnualConfer-
[27] Chao Feng, Ziyang Chen, and Andrew Owens. Self- enceonRobotLearning,2021. 2
supervisedvideoforensicsbyaudio-visualanomalydetec- [39] Irmak Guzey, Ben Evans, Soumith Chintala, and Lerrel
tion. Computer Vision and Pattern Recognition (CVPR), Pinto. Dexterityfromtouch:Self-supervisedpre-trainingof
2023. 3 tactilerepresentationswithroboticplay,2023. 3
[28] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie [40] AndreyGuzhov, FedericoRaue, Jo¨rnHees, andAndreas
Geng,AojunZhou,WeiZhang,PanLu,ConghuiHe,Xi- Dengel. Audioclip:Extendingcliptoimage,textandaudio.
angyu Yue, et al. Llama-adapter v2: Parameter-efficient InICASSP2022-2022IEEEInternationalConferenceon
10Acoustics,SpeechandSignalProcessing(ICASSP),pages [57] Justin Kerr, Huang Huang, Albert Wilcox, Ryan Hoque,
976–980.IEEE,2022. 3 Jeffrey Ichnowski, Roberto Calandra, and Ken Goldberg.
[41] NeginHeravi, WenzhenYuan, AllisonM.Okamura, and Self-supervisedvisuo-tactilepretrainingtolocateandfollow
JeannetteBohg. Learninganaction-conditionalmodelfor garmentfeatures. InRobotics:ScienceandSystems,2023.
haptictexturegeneration. 2020IEEEInternationalCon- 2,3,4,5,15
ferenceonRoboticsandAutomation(ICRA),pages11088– [58] Diederik Kingma and Jimmy Ba. Adam: A method for
11095,2019. 2 stochastic optimization. In International Conference on
[42] Carolina Higuera, Byron Boots, and Mustafa Mukadam. LearningRepresentation,2015. 5
Learningtoreadbraille:Bridgingthetactilerealitygapwith [59] Mike Lambeta, Po wei Chou, Stephen Tian, Brian Yang,
diffusionmodels. 2023. 2 BenjaminMaloon,VictoriaRoseMost,DaveStroud,Ray-
[43] HaroldHotelling. Relationsbetweentwosetsofvariates. mondSantos,AhmadByagowi,GreggKammerer,Dinesh
Biometrika,28:321–377,1936. 7 Jayaraman,andRobertoCalandra. Digit: Anoveldesign
[44] XixiHu,ZiyangChen,andAndrewOwens. Mixandlocal- foralow-costcompacthigh-resolutiontactilesensorwith
ize:Localizingsoundsourcesinmixtures. ComputerVision applicationtoin-handmanipulation. IEEERoboticsand
andPatternRecognition(CVPR),2022. 3 AutomationLetters,5:3838–3845,2020. 2
[45] Hung-JuiHuang,XiaofengGuo,andWenzhenYuan. Un- [60] Mike Lambeta, Huazhe Xu, Jingwei Xu, Po wei Chou,
derstandingdynamictactilesensingforliquidpropertyesti- Shaoxiong Wang, Trevor Darrell, and Roberto Calandra.
mation. ArXiv,abs/2205.08771,2022. 2 Pytouch:Amachinelearninglibraryfortouchprocessing.
[46] FabianHutmacher. Whyistheresomuchmoreresearch 2021IEEEInternationalConferenceonRoboticsandAu-
onvisionthanonanyothersensorymodality? Frontiersin tomation(ICRA),pages13208–13214,2021. 2
psychology,10:2246,2019. 1
[61] SusanJ.LedermanandRobertaL.Klatzky. Handmove-
[47] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A ments:Awindowintohapticobjectrecognition. Cognitive
Efros. Image-to-imagetranslationwithconditionaladver- Psychology,19:342–368,1987. 2
sarialnetworks. CVPR,2017. 7,8
[62] SusanJ.LedermanandR.L.Klatzky.Tutorialreviewhaptic
[48] WeiJi,LongChen,YinweiWei,YimingWu,andTat-Seng
perception:Atutorial. 2009. 2
Chua. Mrtnet:Multi-resolutiontemporalnetworkforvideo
[63] MichelleA.Lee,YukeZhu,PeterZachares,MatthewTan,
sentencegrounding. ICASSP,2023. 3
KrishnaParasuramSrinivasan,SilvioSavarese,Fei-FeiLi,
[49] WeiJi,XiangyanLiu,AnZhang,YinweiWei,andXiang
AnimeshGarg,andJeannetteBohg. Makingsenseofvision
Wang.Onlinedistillation-enhancedmulti-modaltransformer
andtouch:Learningmultimodalrepresentationsforcontact-
forsequentialrecommendation. InProceedingsofthe31th
rich tasks. IEEE Transactions on Robotics, 36:582–596,
ACMinternationalconferenceonMultimedia,2023. 3
2019. 2
[50] Jiaqi Jiang and Shan Luo. Robotic perception of object
[64] SeungHyunLee,WonseokRoh,WonminByeon,SangHo
properties using tactile sensing. ArXiv, abs/2112.14119,
Yoon, Chanyoung Kim, Jinkyu Kim, and Sangpil Kim.
2021. 2
Sound-guidedsemanticimagemanipulation.InProceedings
[51] JiaqiJiang,GuanqunCao,DanielFernandesGomes,and
oftheIEEE/CVFconferenceoncomputervisionandpattern
ShanLuo. Vision-guidedactivetactileperceptionforcrack
recognition,pages3377–3386,2022. 2
detection and reconstruction. 2021 29th Mediterranean
[65] MarionLepert,ChaoyiPan,ShenliYuan,RikaAntonova,
ConferenceonControlandAutomation(MED),pages930–
and Jeannette Bohg. In-hand manipulation of unknown
936,2021. 2
objects with tactile sensing for insertion. In Embracing
[52] JiaqiJiang,DanushkaBollegala,ShanLuo,etal.Learnfrom
Contacts-WorkshopatICRA2023,2023. 2
incompletetactiledata:Tactilerepresentationlearningwith
maskedautoencoders.InProceedingsofthe2023IEEE/RSJ [66] NathanF.Lepora,YijiongLin,BenMoney-Coomes,and
InternationalConferenceonIntelligentRobotsandSystems John Lloyd. Digitac: A digit-tactip hybrid tactile sensor
(IROS),2023. 3 forcomparinglow-costhigh-resolutionrobottouch. IEEE
RoboticsandAutomationLetters,7:9382–9388,2022. 2
[53] TudorJianu,DanielFernandesGomes,andShanLuo. Re-
ducingtactilesim2realdomaingapsviadeeptexturegenera- [67] Hao Li, Yizhi Zhang, Junzhe Zhu, Shaoxiong Wang,
tionnetworks. 2022InternationalConferenceonRobotics MichelleA.Lee,HuazheXu,EdwardH.Adelson,LiFei-
andAutomation(ICRA),pages8305–8311,2021. 2 Fei,RuohanGao,andJiajunWu. See,hear,andfeel:Smart
[54] MicahKJohnsonandEdwardHAdelson. Retrographic sensoryfusionforroboticmanipulation. InConferenceon
sensingforthemeasurementofsurfacetextureandshape. RobotLearning,2022. 2
In2009IEEEConferenceonComputerVisionandPattern [68] HongyuLi,SnehalDikhale,SoshiIba,andNawidJamali.
Recognition,pages1070–1077.IEEE,2009. 2 Vihope:Visuotactilein-handobject6dposeestimationwith
[55] MicahK.Johnson,ForresterCole,AlvinRaj,andEdwardH. shapecompletion. IEEERoboticsandAutomationLetters,
Adelson. Microgeometrycaptureusinganelastomericsen- 8(11):6963–6970,2023. 2
sor. ACMSIGGRAPH2011papers,2011. 2 [69] Hangfei Li, Yiming Wu, and Fangfang Wang. Dynamic
[56] ZhanatKappasov,JuanAntonioCorrales,andVe´ronique networkforlanguage-basedfashionretrieval. InProceed-
Perdereau. Tactilesensingindexterousrobothands-review. ingsofthe1stInternationalWorkshoponDeepMultimodal
RoboticsAuton.Syst.,74:195–220,2015. 2 LearningforInformationRetrieval,pages49–57,2023. 3
11[70] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip- in-hand object rotation with vision and touch. ArXiv,
2:Bootstrappinglanguage-imagepre-trainingwithfrozen abs/2309.09979,2023. 2
imageencodersandlargelanguagemodels. arXivpreprint [86] LongtianQiu,RenruiZhang,ZiyuGuo,ZiyaoZeng,Yafeng
arXiv:2301.12597,2023. 7,8 Li, and Guangnan Zhang. Vt-clip: Enhancing vision-
[71] YunzhuLi,Jun-YanZhu,RussTedrake,andAntonioTor- languagemodelswithvisual-guidedtexts. arXivpreprint
ralba. Connectingtouchandvisionviacross-modalpredic- arXiv:2112.02399,2021. 3
tion. 2019IEEE/CVFConferenceonComputerVisionand [87] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
PatternRecognition(CVPR),pages10601–10610,2019. 2, Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
4,7,8 Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
[72] JustinLin,RobertoCalandra,andSergeyLevine. Learning Krueger,andIlyaSutskever. Learningtransferablevisual
toidentifyobjectinstancesbytouch:Tactilerecognitionvia modelsfromnaturallanguagesupervision. InInternational
multimodalmatching. 2019InternationalConferenceon ConferenceonMachineLearning,2021. 2
RoboticsandAutomation(ICRA),pages3644–3650,2019. [88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
2 Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[73] DavidJLinden. Touch:Thescienceofthehand,heart,and AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
mind. PenguinBooks,2016. 1 ingtransferablevisualmodelsfromnaturallanguagesuper-
vision. InInternationalconferenceonmachinelearning,
[74] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.
Improvedbaselineswithvisualinstructiontuning. arXiv pages8748–8763.PMLR,2021. 3,4
preprintarXiv:2310.03744,2023. 7,8 [89] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
PatrickEsser, andBjo¨rnOmmer. High-resolutionimage
[75] John Lloyd and Nathan F. Lepora. Goal-driven robotic
synthesiswithlatentdiffusionmodels. InProceedingsof
pushingusingtactileandproprioceptivefeedback. IEEE
theIEEE/CVFconferenceoncomputervisionandpattern
TransactionsonRobotics,38:1201–1212,2020. 2
recognition,pages10684–10695,2022. 4,7,15,17
[76] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
[90] Zilin Si and Wenzhen Yuan. Taxim: An example-based
regularization. arXivpreprintarXiv:1711.05101,2017. 5
simulationmodelforgelsighttactilesensors.IEEERobotics
[77] HuaishaoLuo,LeiJi,MingZhong,YangChen,WenLei,
andAutomationLetters,PP:1–1,2021. 2
NanDuan,andTianruiLi. Clip4clip: Anempiricalstudy
[91] LindaSmithandMichaelGasser. Thedevelopmentofem-
ofclipforendtoendvideoclipretrievalandcaptioning.
bodiedcognition: Sixlessonsfrombabies. Artificiallife,
Neurocomputing,508:293–304,2022. 3
2005. 1
[78] ShanLuo,WenzhenYuan,EdwardH.Adelson,AnthonyG.
[92] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Cohn,andRaulFuentes. Vitac: Featuresharingbetween
Denoising diffusion implicit models. arXiv preprint
visionandtactilesensingforclothtexturerecognition. 2018
arXiv:2010.02502,2020. 15,16
IEEEInternationalConferenceonRoboticsandAutomation
[93] Kim Sung-Bin, Arda Senocak, Hyunwoo Ha, Andrew
(ICRA),pages2722–2727,2018. 2
Owens, andTae-HyunOh. Soundtovisualscenegener-
[79] Paul R Manske. The sense of touch. Journal of Hand
ationbyaudio-to-visuallatentalignment. ComputerVision
Surgery,24(2):213–214,1999. 1
andPatternRecognition(CVPR),2023. 3
[80] PedroMorgado,NunoVasconcelos,andIshanMisra.Audio-
[94] Sudharshan Suresh, Zilin Si, Stuart Anderson, Michael
visualinstancediscriminationwithcross-modalagreement.
Kaess,andMustafaMukadam. MidasTouch:Monte-Carlo
InProceedingsoftheIEEE/CVFConferenceonComputer
inferenceoverdistributionsacrossslidingtouch. InProc.
VisionandPatternRecognition,pages12475–12486,2021.
Conf.onRobotLearning,CoRL,Auckland,NZ,2022. 2,4,
3
5,15
[81] AaronvandenOord,YazheLi,andOriolVinyals. Repre-
[95] S. Suresh, Z. Si, J. Mangelson, W. Yuan, and M. Kaess.
sentationlearningwithcontrastivepredictivecoding. arXiv
ShapeMap 3-D: Efficient shape mapping through dense
preprintarXiv:1807.03748,2018. 3 touch and vision. In Proc. IEEE Intl. Conf. on Robotics
[82] AndrewOwens,JiajunWu,JoshHMcDermott,WilliamT andAutomation,ICRA,Philadelphia,PA,USA,2022. 2
Freeman,andAntonioTorralba. Learningsightfromsound: [96] TasbolatTaunyazov,YansongChua,RuihanGao,Harold
Ambient sound provides supervision for visual learning. Soh, and Y. Wu. Fast texture classification using tactile
2018. 3 neuralcodingandspikingneuralnetwork. 2020IEEE/RSJ
[83] ChaoyiPan,MarionLepert,ShenliYuan,RikaAntonova, InternationalConferenceonIntelligentRobotsandSystems
and Jeannette Bohg. In-hand manipulation of unknown (IROS),pages9890–9895,2020. 2
objectswithtactilesensingforinsertion. 2022. 2 [97] IanTaylor,SiyuanDong,andAlbertoRodriguez. Gelslim
[84] LeszekPecyna,SiyuanDong,andShanLuo. Visual-tactile 3.0:High-resolutionmeasurementofshape,forceandslip
multimodalityforfollowingdeformablelinearobjectsus- inacompacttactile-sensingfinger. 2022InternationalCon-
ingreinforcementlearning. 2022IEEE/RSJInternational ferenceonRoboticsandAutomation(ICRA),pages10781–
ConferenceonIntelligentRobotsandSystems(IROS),pages 10787,2021. 2
3987–3994,2022. [98] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
[85] HaozhiQi, BrentYi, SudharshanSuresh, MikeLambeta, trastivemultiviewcoding. InEuropeanconferenceoncom-
Y. Ma, Roberto Calandra, and Jitendra Malik. General putervision,pages776–794.Springer,2020. 3
12[99] HugoTouvron,ThibautLavril,GautierIzacard,XavierMar- ceedingsoftheIEEE/CVFConferenceonComputerVision
tinet, Marie-Anne Lachaux, Timothe´e Lacroix, Baptiste andPatternRecognition,pages1809–1818,2022. 3
Rozie`re,NamanGoyal,EricHambro,FaisalAzhar,etal. [111] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu,
Llama: Open and efficient foundation language models. WenzhenYuan,andAndrewOwens. Touchandgo:Learn-
arXivpreprintarXiv:2302.13971,2023. 7,16 ing from human-collected vision and touch. Neural In-
[100] Ruoyu Wang, Shiheng Wang, Songyu Du, Erdong Xiao, formation Processing Systems (NeurIPS) - Datasets and
WenzhenYuan,andChenFeng. Real-timesoftbody3dpro- BenchmarksTrack,2022. 2,3,4,5,7,8,15,17,21
prioceptionviadeepvision-basedsensing. IEEERobotics [112] FengyuYang,JiachengZhang,andAndrewOwens. Gener-
andAutomationLetters,5:3382–3389,2019. 2 atingvisualscenesfromtouch. InternationalConferenceon
[101] ShaoxiongWang,MikeLambeta,PoweiChou,andRoberto ComputerVision(ICCV),2023. 2,4,6,7,8,16
Calandra. Tacto: Afast,flexible,andopen-sourcesimula- [113] JianweiYang,ChunyuanLi,PengchuanZhang,BinXiao,
torforhigh-resolutionvision-basedtactilesensors. IEEE CeLiu, LuYuan, andJianfengGao. Unifiedcontrastive
RoboticsandAutomationLetters,7:3930–3937,2020. 2 learninginimage-text-labelspace. 2022IEEE/CVFConfer-
[102] YimingWu,XintianWu,XiLi,andJianTian. Mgh:Meta- enceonComputerVisionandPatternRecognition(CVPR),
dataguidedhypergraphmodelingforunsupervisedperson pages19141–19151,2022. 4
re-identification. InProceedingsofthe29thACMInterna- [114] Zhao-HengYin,BinghaoHuang,YuzheQin,QifengChen,
tionalConferenceonMultimedia,pages1571–1580,2021. and Xiaolong Wang. Rotating without seeing: Towards
3 in-hand dexterity through touch. Robotics: Science and
[103] YusongWu,KeChen,TianyuZhang,YuchenHui,Taylor Systems,2023. 2
Berg-Kirkpatrick, andShlomoDubnov. Large-scalecon- [115] KelinYu,YunhaiHan,MatthewZhu,andYeZhao. Mimic-
trastivelanguage-audiopretrainingwithfeaturefusionand touch:Learninghuman’scontrolstrategywithmulti-modal
keyword-to-captionaugmentation. InICASSP2023-2023 tactilefeedback. ArXiv,abs/2310.16917,2023. 2
IEEEInternationalConferenceonAcoustics,Speechand [116] Xihang Yu, Sangli Teng, Theodor Chakhachiro, Wenzhe
SignalProcessing(ICASSP),pages1–5.IEEE,2023. 3 Tong, TingjunLi, Tzu-YuanLin, SarahKoehler, Manuel
[104] ZhirongWu,YuanjunXiong,StellaXYu,andDahuaLin. Ahumada,JeffreyMWalls,andMaaniGhaffari. Fullypro-
Unsupervisedfeaturelearningvianon-parametricinstance prioceptiveslip-velocity-awarestateestimationformobile
discrimination. InProceedingsoftheIEEEconferenceon robots via invariant kalman filtering and disturbance ob-
computervisionandpatternrecognition,pages3733–3742, server. arXivpreprintarXiv:2209.15140,2022. 2
2018. 3 [117] WenzhenYuan,SiyuanDong,andEdwardH.Adelson. Gel-
[105] EricZhongcongXu,ZeyangSong,SatoshiTsutsui,Chao sight: High-resolutionrobottactilesensorsforestimating
Feng,MangYe,andMikeZhengShou. Ava-avd: Audio- geometryandforce. Sensors(Basel,Switzerland),17,2017.
visualspeakerdiarizationinthewild. InProceedingsofthe 2
30thACMInternationalConferenceonMultimedia,pages [118] WenzhenYuan, ShaoxiongWang, SiyuanDong, andEd-
3838–3847,2022. 3 wardH.Adelson. Connectinglookandfeel: Associating
[106] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, thevisualandtactilepropertiesofphysicalmaterials. 2017
ArmenAghajanyan,FlorianMetze,LukeZettlemoyer,and IEEEConferenceonComputerVisionandPatternRecogni-
ChristophFeichtenhofer.Videoclip:Contrastivepre-training tion(CVPR),pages4494–4502,2017. 2,3
for zero-shot video-text understanding. arXiv preprint [119] Wenzhen Yuan, Chenzhuo Zhu, Andrew Owens, Man-
arXiv:2109.14084,2021. 3 dayam A Srinivasan, and Edward H Adelson. Shape-
[107] HuazheXu,YupingLuo,ShaoxiongWang,TrevorDarrell, independenthardnessestimationusingdeeplearningand
andRobertoCalandra. Towardslearningtoplaypianowith a gelsight tactile sensor. In International Conference on
dexteroushandsandtouch. 2022IEEE/RSJInternational RoboticsandAutomation(ICRA),2017. 2,5
ConferenceonIntelligentRobotsandSystems(IROS),pages [120] MartinaZambelli,YusufAytar,FrancescoVisin,Yuxiang
10410–10416,2021. 2 Zhou,andRaiaHadsell.Learningrichtouchrepresentations
[108] WenqiangXu,ZhenjunYu,HanXue,RuolinYe,Siqiong through cross-modal self-supervision. In Conference on
Yao,andCewuLu. Visual-tactilesensingforin-handobject RobotLearning,2021. 3
reconstruction. 2023IEEE/CVFConferenceonComputer [121] Ben Zandonati, Ruohan Wang, Ruihan Gao, and Y. Wu.
VisionandPatternRecognition(CVPR),pages8803–8812, Investigatingvisionfoundationalmodelsfortactilerepre-
2023. 2 sentationlearning. ArXiv,abs/2305.00596,2023. 2
[109] LeXue,MingfeiGao,ChenXing,RobertoMart’in-Mart’in, [122] RenruiZhang,ZiyuGuo,WeiZhang,KunchangLi,Xupeng
JiajunWu,CaimingXiong,RanXu,JuanCarlosNiebles, Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li.
andSilvioSavarese. Ulip:Learningaunifiedrepresentation Pointclip:Pointcloudunderstandingbyclip. arXivpreprint
oflanguage, images, andpointcloudsfor3dunderstand- arXiv:2112.02413,2021. 3
ing. 2023IEEE/CVFConferenceonComputerVisionand [123] RenruiZhang,ZiyaoZeng,ZiyuGuo,andYafengLi. Can
PatternRecognition(CVPR),pages1179–1189,2022. 2 languageunderstanddepth? InProceedingsofthe30thACM
[110] FengyuYangandChenyangMa.Sparseandcompletelatent InternationalConferenceonMultimedia,pages6868–6874,
organizationforgeospatialsemanticsegmentation. InPro- 2022. 3
13[124] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
ShilinYan,PanLu,HongshengLi,PengGao,andYuQiao.
Llama-adapter: Efficient fine-tuning of language models
withzero-initattention. arXivpreprintarXiv:2303.16199,
2023. 4,16
[125] LiangliZhen,PengHu,XuWang,andDezhongPeng.Deep
supervisedcross-modalretrieval. 2019IEEE/CVFConfer-
enceonComputerVisionandPatternRecognition(CVPR),
pages10386–10395,2019. 7
[126] ChenhaoZheng, AyushShrivastava, andAndrewOwens.
Exif as language: Learning cross-modal associations be-
tweenimagesandcamerametadata. ComputerVisionand
PatternRecognition(CVPR),2023. 3
[127] Shaohong Zhong, Alessandro Albini, Oiwi Parker Jones,
PerlaMaiolino,andIngmarPosner. Touchinganerf:Lever-
agingneuralradiancefieldsfortactilesensorydatagenera-
tion. InConferenceonRobotLearning,pages1618–1628.
PMLR,2023. 2
[128] XiangyangZhu,RenruiZhang,BoweiHe,ZiyuGuo,Ziyao
Zeng,ZipengQin,ShanghangZhang,andPengGao. Point-
clipv2:Promptingclipandgptforpowerful3dopen-world
learning. ICCV2023,2022. 3
14A.1.DatasetsandMetrics iron,ceramic,andglass. Thesecategoriesarealsoapplied
toObjectFolder2.0andObjectFolderRealdatasets.
Weprovidemoredetailsofdatasetsusedinourpaper,allof
whicharepubliclyavailable. ObjectFolder2.0[32]. TheObjectFolder2.0datasetex-
tends [30] to 1000 objects and improves the acoustic and
TouchandGo[111]. TheTouchandGodatasetisarecent, tactilesimulationpipelinestorendermorerealisticmultisen-
real-worldvisuo-tactiledatasetfeaturinghumaninteractions sory data. For the tactile simulation, it utilizes the Taxim
with various objects in both indoor and outdoor environ- simulatorinsteadofTACTO.Similartothepreprocessing
mentsusingaGelSighttactilesensor. Itcomprises13,900 ofObjectFolder1.0,wesample200pointsforeachobject.
instancesoftouchacrossapproximately4,000distinctob- Toavoidoverlappingwith[30],weonlytakethe101-1000
jectinstancesand20typesofmaterials. Sinceitistheonly objects. We apply this dataset to material classification,
real-worldin-the-wilddataset,weapplyittomultipletasks cross-modal retrieval, robot grasping stability prediction,
includingmaterialclassification,imagesynthesiswithtouch, andTouch-LLM.Forcross-modalretrievalandTouch-LLM
TouchLLM,andX-to-touchgeneration. Weusetheofficial tasks,weannotatetextdescriptionsthatdepictthecontact
train/testsplitof[111]wherethedatasetissplitbytouches, pointoftheobjectfromitsvisualinput,e.g.“The corner
notbyframestoavoidsimilartouchimagesbetweenthetrain of a wooden table.”
andtestset. ForTouch-LLMandX-to-touchapplications,
ObjectFolderReal[33]. ObjectFolderRealisanobject-
welabel400visualimagesbyaskingturkerstoprovidetheir
centricmultimodaldatasetcontaining100real-worldhouse-
captioningtodescribetheobject,touchfeeling,andtexture
holdobjects. ThetouchimagesarecapturedbytheGelSlim
fromit.
tactilesensor. Similarly,wesample200pointsforeachob-
jectthuscontainingintotalof20kvisuo-tactilepairs. We
The feeling of success [6]. The Feeling of Success is a
applythisdatasettoamaterialclassificationtask,whichis
robot-collectedvisuo-tactiledatasetofrobotsgraspingob-
consideredanout-of-domaindataset.
jectsonatabletop. Thetactileimagesareallcapturedby
GelSighttactilesensors. Itcontains9.3kpairedvisionand SSVTP[57]. SSVTPdatasetisarecenthuman-collected
touchimages. Weapplythisdatasettoroboticgraspingsta- visuo-tactiledatasetcontaining4.9kpairedvisuo-tactileim-
bilitypredictions. Asthereisnoofficialsplitoftrain/val/test, ages. ThetouchimagesarecollectedviatheDIGITtactile
following [33, 111], we split the dataset by objects in the sensor. Theobjectsinthisdatasetaremainlyfromgarments
ratioof8:1:1. butalsocontainmaterialsofmetal. Weapplythisdataset
to material classification. As the dataset does not contain
YCB-Slide[94]. TheYCB-SlidedatasetcomprisesDIGIT
materiallabels,weannotatemateriallabelsfromthevisual
slidinginteractionsonYCBobjects. Thedatasetisinthe
images. Intotal,weclassifyallimagesinto6materialcate-
video format where we take all 180k frames for our ex-
goriesincludingcotton,metal,denimfabric,plastic,wood,
periments. Thedatasetcontains10YCBobjectsincluding
andnylon.
asugarbox,atomatosoupcan,amustardbottle,ableach
cleanser,amug,apowerdrill,scissors,anadjustablewrench,
A.2.ImplementationDetails
ahammer,andabaseball. Whilethetactileimagesarecol-
lectedviaslidinginteraction,thevisualinputisgeneratedby Weshowmoreimplementationdetailsinthissection.
simulationoftheYCBobjects. Inourexperiment,wetreat
Image synthesis with touch. We used a pretrained sta-
eachoftheobjectsasanindividualmaterialandourgoal
blediffusion-2.1unclip[89]toperformzero-shottouch-to-
istoclassify10classes. Weapplythisdatasettomaterial
imagegenerationbyreplacingthetextconditionwithour
classification.
alignedUniTouchembedding. Specifically,wekeepthesim-
ObjectFolder 1.0 [30]. The ObjectFolder 1.0 dataset is ple text "high quality" as the condition while using
asimulationdatasetcontaining3Dmodelsof100objects our touch embedding as an additional condition. We use
fromonlinerepositories. Thetouchimagesaresimulatedby DDIMsampler[92]withaguidancescaleof9anddenois-
TACTOsimulators.Astherawdatasetisa3Dmodelwithin- ingstepsof50. Additionally,wesetanembeddingstrength
finitepoints,werandomlysample200pointsforeachobject. of 0.75 for our touch embedding condition. Synthesized
Weapplythisdatasettomaterialclassificationandgrasping imagesareattheresolutionof768 768.
×
stabilitypredictionexperiments. Itisworthnotingthatfor Asfortactile-drivenimagestylization,similarly,westill
graspingstabilitypredictionexperiments,weselect6objects keepthesimpletext"high quality"asthecondition.
suitableforgraspingfollowingtheirsettingandachieverela- However,weusebothtouchandimageembeddingsasextra
tivelybalancedsuccessfulandfailureoutcomesforgrasping. conditionstoconductimagestylization. Weperformalinear
Following[30],allmaterialscanbecategorizedinto7mate- combinationoftouchandimageembeddings,theweights
rialcategoriesincludingwood,steel,polycarbonate,plastic, fortouchandimagearesetto0.3and0.7respectively. We
15use DDIM sampler [92] with a guidance scale of 9 and learned for the generated images and conditioned tactile
denoisingstepsof50. Thestrengthforlinearcombination signals,whichusedanoff-the-shelfnetwork.Materialclassi-
embeddingissetto1andeditedimagesareattheresolution ficationconsistency[112]usesamaterialclassifiertocatego-
of768 768. rizethepredictedandgroundtruthimagesandmeasurethe
×
rateatwhichtheyagree,whereweuseCLIPasthezero-shot
Touch-LLM. Weadaptourmodelfrom[28,124],which
material classifier by feeding the prompt of "material
leveragesanadaptertoconnectourtouchencoderandan
of [CLS]".
open-source large language model LLaMA [99]. We re-
place RGB image embedding with our aligned UniTouch Touch-LLM. We feed each vision language model
embedding. Concretely, we denote the global touch fea- (including our Touch-LLM) with a touch image and
tureencodedbyourtouchencoderasF
T
R1×CT,where text prompt: "You will be presented with
∈
C isthedimensionofthetouchembedding. Inspiredby a touch image from an object/surface.
T
priorwork[28,124],weuseaprojectorf,whichencodes Can you describe the touch feeling and
F tohavethesamedimensionasthetokenembeddingin the texture?". Intheend, weuseGPT-4toperform
T
LLaMA[99]: the automatic evaluation for each model following prior
F′ =f(F ). (5) work[5]. Specifically,weprovideGPT-4with: 1)asystem
T T
prompt describing the desired evaluation behavior; 2) the
Then we repeat F′ and add it to all text tokens across all
T question; and 3) a human-crafted reference response; 4)
layersinlanguagemodelLLaMA[99]withazero-initialized
eachmodel’sgenerationresult(moredetailsseesupp.). We
learnablegatefunction:
instructGPT-4torateeachmodel’sgenerationsonascaleof
Tq =h F′ +Tq, (6) 1to5giventhereferenceresponse. Thetemplateisshown
j zero · T j inFig.7.
wherej andqdenotesthelayerandsequenceindexrespec-
X-to-touch. We test the effectiveness of the x-to-touch
tively,Tq isthetexttokenembedding,andh isthezero-
j zero modelontheTouchandGodataset,whichistheonlyreal-
initializedlearnablegatefunction. Inourexperiments,we
worlddatasetthatcontainsobjectsandscenesinthewild.As
usepretrainedh ,andplugourUniTouchembeddingin.
zero theobjectsinthisdatasetarecloselyrelatedtothematerial
X-to-touchgeneration WeconductourX-to-touchgener- properties, we measure the material classification consis-
ationmodelbasedonstablediffusion. Whilemostexisting tency between different touches generated from different
multimodaltactiledatasetsonly containvisionandtouch, modalities. WeuseourUniTouchembeddingastheoff-the-
wefirsttrainanimage-to-touchdiffusionmodelandweare shelfzero-shotmaterialclassifier. Forquantitativeresults
abletoconducttext-to-touchandaudio-to-touchzeroshotby fortext-to-touchgeneration,weusethe400human-labeled
replacingtheimageconditioningastheyarealreadyaligned. text captions as the input. For audio-to-touch generation,
WeusetheAdamoptimizerwithabaselearningrateof1e-6. as there is no impact sound correlated to this dataset, we
Models are all trained with 30 iterations using the above manuallyselectaudiosfromObjectFolder2.0astheinput
learningratepolicy. Wetrainourmodelwithabatchsizeof thathavethesamematerialpropertiesorgeometrywiththe
48on4RTXA40GPUs. Sincewewanttousethealigned visualimageforqualitativeevaluations,asshowninFig.10.
conditionembeddings,theconditionalmodelisfrozendur-
ingtraining. Theconditionembeddingsareintegratedinto A.4.AdditionalExperiments
the model using cross-attention. We use the frozen, pre-
trained VQGAN to obtain our latent representation, with In-batchsamplingmixrateselection. Weevaluatedif-
a spatial dimension of 64×64. During the inference, we ferentchoicesofσforin-batchsampling,whereσdenotes
conductedthedenoisingprocessfor200stepsandsetthe thepercentageofthedatathatcomesfromthesamedataset
guidancescales=7.5. while the rest from others. We set σ to 0,0.5,0.75,1.0
{ }
and evaluate their zero-shot material classification perfor-
A.3.EvaluationDetails manceonallsixdatasets,asshowninFig.6. Weobserve
thatifweselectσ = 0, theabilitytodistinguishbetween
Touch-to-image generation Following [112], we use intra-sensorsamplesissignificantlyunderminedthusleading
threeevaluationmetricsofFrechetInceptionDistance(FID), toinferiorperformance. Astheσ isincreasing,themodel
ContrastiveVisuo-TactilePre-Training(CVTP),andMate- is able to better distinguish between intra-sensor samples.
rialClassificationConsistency. FIDisastandardevaluation Intheextremecasewhenσ =1.0whereallsamplescome
metricinimagesynthesisthatcomparesthedistributionof fromthesamedataset,themodelwillhavenoexposureto
realandgeneratedimageactivationsusingatrainednetwork. theinter-classnegatives. Weobservethattheperformance
CVTP [112] is a metric similar to CLIP but measures the in this case is actually decreasing. This demonstrates the
cosinesimilaritybetweenthevisualandtactileembeddings effectivenessofdesigntobalancebetweeninter-sensorand
1650
48
46
44
42
40
38
36
34
32
0.00 0.25 0.50 0.75 1.00
Figure6. Effectofσ forin-batchsampling. Wecomparethe
averagezero-shotmaterialclassificationaccuracyfromsixdatasets
usingdifferentσof0,0.5,0.75,1.
intra-sensornegatives. Weempiricallyfoundthatselecting
σ =0.75obtainsagoodtrade-offbetweenthesefactors.
Image synthesis with touch. We leverage our aligned
UniTouch embedding and pretrained text-to-image stable
diffusion model [89] to generate more qualitative results
oftouch-to-imagegenerationandtactile-drivenimagestyl-
izationaspresentedinFig.8. ItshowsthatourUniTouch
embeddingcanguideimagesynthesissuccessfullyinazero-
shotmanner.
X-to-touch generation. We show more examples of X-
to-touchgenerationsontheTouchandGo[111]datasetin
Fig.10,wherewegeneratetouchimagesusingimage,text,
andaudio.
Touch-LLM. We show more touch image question an-
sweringexamplesinFig.9.
17
ycarucca
egarevAsystem prompt (human authored)
You are EvaluationGPT, an expert language model at judging whether or not a response adequately answers the question.
Morespecifically,youwillbegiventhefollowing:
1. Aquestion:Thisisaquestionwhichrequiresmachinetogenerateareasonableanswer.
2. Ahigh-qualityreferenceanswer:Thisisahumancraftedanswer,whichshouldaddressthequestion.
3. Acandidateresponse:Thisisaresponsegeneratedbymodeltryingtoanswerthequestion.
Your job is to judge whether the response adequately answers the question given the reference answer. Please output the
numberfrom1to5.
- 1 represents the response is totally irrelevant to the question. 5 means the response fully addresses the question given
referenceanswer.
user(humanauthored)
Iwillprovidequestiontoyou.Then,Iwillprovideareferenceanswerwhichisanexampleofahighqualityoutputforthat
questioninthecontextoftheimage(imageisnotprovided).ThenIwillofferyouacandidateresponsethattriestoaddress
thequestion.Yourjobistoratetheresponseonascaleof1to5.
assistant(humanauthored)
Sure,pleaseprovidethequestion,referenceanswer,andthecandidateresponse.ThenIwillratetheresponseonascaleof1
to5tojudgewhethertheresponseadequatelyaddressthequestion.
user(humanauthored)
OK.Hereisthequestion,thehigh-qualityreferenceanswer,andthecandidateresponse.
Question: You will be presented with an touch image from a object/surface. Can you describe the touch feeling and the
texture?
Reference: The surface appears to be a piece of fabric. The fabric looks soft and textured with a knit or woven pattern.
Touching it would likely feel warm, soft, and slightly bumpy due to the textures. The material would be flexible and
malleable,conformingeasilytopressureandtouch,providingacomfortableandgentletactileexperience.
Response: The touch image shows a soft, fuzzy, and cozy texture, which is characteristic of the material used to make the
sweater. The sweater is likely made from a warm and comfortable material, such as cotton or a blend of cotton and other
fibers.Thetextureofthesweaterissmoothandinviting,makingitanidealchoiceforacomfortableandrelaxedoutfit.
Figure7. GPT-4evaluationtemplate. WeusethistemplatetoinstructGPT-4forautomaticevaluationofourTouch-LLMandother
selectedopen-sourceVLMbaselines.
18Touch Reference Ours Touch Reference Source Ours
Touch-to-Image Generation Tactile-driven Image Stylization
Figure8.Moreexamplesofzero-shotimagesynthesiswithtouch.(Left)Wegenerateanimageofascenegivenatactilesignal.(Right)
Weperformtactile-drivenimagestylizationtomanipulateanimagetomatchagiventouchsignal.Wedenote“reference”asvisualimages
pairedwiththeinputtouchinthedataset,whicharenotseenbythemodelbutonlyshownfordemonstrationpurposes.Thelasttworowsare
failurecases.
19Can you describe material
你能告诉我提供的触觉图里的 How easy or difficult is it to
触觉的感觉和质地纹理吗？ of the object presented in hold or grip the object?
touch image?
Reference Input touch Reference Input touch Reference Input touch
The object in the touch image
触觉图的触觉的感觉和质地 is made of a rough, grainy The object is likely to be a flat
纹理是由植物的叶子组成的， material, which could be a surface, which suggests that it
这些植物的叶子呈现出细腻 mixture of sand, small rocks, might be difficultto hold or
的纹理和柔软的触觉。 or other similar materials. grip it securely.
How does the object respond to
Can you tell me the
pressure? Does it bounce back What part of the medical
reflectivity of the object in
or retain indentations? Please bottle this touch located?
the touch image?
answer based on its material.
Reference Input touch Reference Input touch Reference Input touch
The object in the touch image
The object, seemingly made of
appears to be highly
wood, reboundswhen pressed, reflective, which suggests that The touch located on the
suggesting that it is stiff and it is likely made of a material medical bottle is on the neck
resistantto deformationor with a high reflective index, of the bottle.
damage.
such as glassor a mirror.
Is this object grasped
Will people get hurt when What is the material of the
into the air? Please explain
hitting the point from the object presented in
whether the gripper is in contact
touch image? Why? touch image?
with the object from the touch
image.
Reference Input touch Reference Input touch
Reference Input touch
Yes. This is because the point
is likely to be a sharp edge, The object presented in Yes. Because the gripper is still
which can cause injurieswhen touch image is likely made of in contact with the object.
it comes into contact witha wood.
person's skin.
Can you describe material What is the letter in English Can you describe material
of the object presented in alphabet of this touch of the object presented in
touch image? image of braille? touch image?
Reference Input touch Reference Input touch Reference Input touch
The object in the touch image is The object in the touch image is
made of a material that appears made of a material that is not
The letter in English alphabet
to be a combination of metaland easily identifiable, but it appears
represented by the touch
plastic. It could be a metal watch to be a small, lightweight, and
image of braille is D.
or a metal-encased electronic possibly plasticor rubber-like
device. material.
Figure9. MoreexamplesofTouch-LLM.Weshowmorequestion-and-answeringexamplesfortouchimagesusingourTouch-LLM.
Wedenote“reference”asvisualimagespairedwiththeinputtouchinthedataset,whicharenotseenbythemodelbutonlyshownfor
demonstrationpurposes.Thelastrowisthefailurecase.Incorrectportionishighlightedinred.
20Vision Touch Text Touch Audio Touch
The surface in Ceramic
the image
appears to be a
flatsurface
made of
ceramic.
The surface in Plastic
the image
appears to be a
printed sign or
poster with a
smoothfinish.
The surface is a Wood
woodenplank
or board, which
is textured,
grainy, and
slightly rough.
Edge
The surface in
the image
appears to be
the edgeof a
metal object.
The object in
the image is a
large rockwith
a fairly rough
and irregular
surface.
The surface
appears to be a
piece of fabric,
possibly a
garment.
Figure10. MoreexamplesforX-to-touchgeneration. Weshowmoreexamplesofx-to-touchgenerationsontheTouchandGo[111]
dataset. WemanuallyselectaudiosfromObjectFolder2.0[32]matchingthevisioninput. Sincetheoverlappingmaterialcategories
between[32]and[111]arelimitedand[32]onlycontainsrigidobjects,impactsoundformaterialslikestoneandclothcannotbefound.
21