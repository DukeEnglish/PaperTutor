Do Language Models Exhibit the Same Cognitive Biases in Problem Solving
as Human Learners?
AndreasOpedal*12 AlessandroStolfo*1 HarukiShirakami1 YingJiao3
RyanCotterell1 BernhardSchölkopf12 AbulhairSaparov4 MrinmayaSachan1
Abstract
Thereisincreasinginterestinemployinglargelan-
guagemodels(LLMs)ascognitivemodels. For
suchpurposes,itiscentraltounderstandwhich
cognitivepropertiesarewell-modeledbyLLMs,
and which are not. In this work, we study the
biases of LLMs in relation to those known in
childrenwhensolvingarithmeticwordproblems.
Surveyingthelearningscienceliterature,weposit
thattheproblem-solvingprocesscanbesplitinto
threedistinctsteps: textcomprehension,solution
planning and solution execution. We construct
tests for each one in order to understand which
Figure1.Athree-stepmodelofthecognitiveprocessinvolvedin
parts of this process can be faithfully modeled solvingmathwordproblems. WestudywhetherLLMsexhibit
bycurrentstate-of-the-artLLMs. Wegeneratea similarbiasesashumanchildrenalongeachstepofthisprocess.
novelsetofwordproblemsforeachofthesetests,
usinganeuro-symbolicmethodthatenablesfine- outcomes (VanLehn et al., 1994). Several works have al-
grainedcontrolovertheproblemfeatures.Wefind readyemployedLLMsasmodelsofstudents(Macinaetal.,
evidencethatLLMs,withandwithoutinstruction- 2023;Nguyenetal.,2023). However, forsuchmodeling
tuning,exhibithuman-likebiasesinboththetext- to be meaningful, it is imperative that the student model
comprehensionandthesolution-planningstepsof isconsistentwithactualstudentbehavior. Yet,thatisnot
thesolvingprocess,butnotduringthefinalstep alwaysthecase: Manyexistingstudentmodelsfailtovali-
whichreliesontheproblem’sarithmeticexpres- datefaithfulnesstorealisticclassroomscenarios(Käserand
sions(solutionexecution).1 Alexandron,2023). Importantly,anLLMthatmodelsthe
problem-solvingprocessofchildrenshouldalsomakesimi-
larmistakesaschildren,i.e.,itshouldmimicthecognitive
1.Introduction
biasesthataresalientinchildrenduringproblemsolving.
Thereisactivediscussionaroundlargepretrainedlanguage
In this work we study whether this holds true for current
models(LLMs)asplausiblecognitivemodels(Mahowald
state-of-the-artLLMs,particularly,whensolvingarithmetic
etal.,2023),e.g.,intermsoflanguageacquisition(Warstadt
mathwordproblems.2 Theseproblemsareinterestingbe-
andBowman,2022),decisionmaking(Aheretal.,2023)
causetheyareconceptuallysimple,andyet,requireseveral
andpoliticalorientation(Argyleetal.,2023). Inthecon-
distinct skills to solve (Stern, 1993). A learner needs to
textofeducation,cognitivemodelingenablesthestudyof
understand the situation described, relate it to arithmetic
human learning without the high cost of data collection
equations,andperformtherequiredcomputations,asFig.1
fromhumansubjects,whichcanleadtoabetterunderstand-
illustrates. ByPiaget’s(1936)viewoncognitivedevelop-
ing of human learning and, therefore, improved learning
ment,aproblemmightbedifficultforachildduetoinsuffi-
*Equalcontribution 1ETHZürich2MaxPlanckInstitutefor cientdevelopmentinanyoneoftheseskills.Muchisknown
IntelligentSystems3KULeuven4NewYorkUniversity. Corre- aboutwhatmakesawordproblemdifficultforhumans;we
spondence to: Andreas Opedal <andreas.opedal@inf.ethz.ch>, askwhetherthesamerelativedifficultiesapplytoLLMs.
AlessandroStolfo<alessandro.stolfo@inf.ethz.ch>.
2RelatedstudieshavecomparedLLMbiasestohumanones
onthetaskofsyllogisticreasoning(Andoetal.,2023;Dasgupta
1Codewillbepublishedwiththefinalversion. etal.,2023;Eisapeetal.,2023),discussedin§6.
1
4202
naJ
13
]LC.sc[
1v07081.1042:viXraDoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
Toanswerthisquestion,weconstructteststhataregrounded Ourconceptualization. Weareinterestedinidentifying
intheextensiveliteratureonmathwordproblemsolvingby when LLMs are likely to exhibit human-like biases, and
children3(forarecentreview,seeJaffeandBolger,2023), therefore,requireaholisticanalysisofthehumanproblem-
andperformthemonasetofopen-sourceLLMs. Specifi- solvingprocess. Ourconceptualization,illustratedinFig.1,
cally,eachtestvariesaproblemfeatureforwhichaneffect includesfourrepresentationallevelsofthemathwordprob-
onchildperformancehasbeenestablishedinliterature(like lem,alongwiththeskillsassociatedwithtransitioningfrom
themannerinwhichaparticularmathematicaloperationis oneleveltothenext. Weassumethatachildgoesthrough
expressedintext),whilecontrollingforotherfeatures. We thefollowingprocedurewhenposedwithamathwordprob-
createnewEnglishproblemsspecificallyforthesetests,by lem:First,theyformamentalmodelofthemathematicalre-
developingagenerationpipelinebasedonasemanticfor- lationshipsexpressedinthatproblem(textcomprehension).
malismovermathwordproblems(Opedaletal.,2023). Our Next,theydistillthatmentalmodelintoasequenceofsym-
generationpipelineadmitsafamilyofstandardarithmetic bolicexpressionsrepresentingthestep-by-stepreasoning
wordproblems,whilecontrollingnotonlyfornumericalfea- processtofindthesolution(solutionplanning)and,finally,
tures,butstructural(e.g.,entityrelationships)andlinguistic calculatetheanswerfromthoseexpressions(solutionexecu-
ones(e.g.,sentencestructure)aswell. tion). Therepresentationallevelswillbeformalizedin§4.1.
Wetestthreecognitivebiases,eachoneassociatedwitha
separatestepofthesolvingprocess(whichareillustrated Background. Children tend to experience greater diffi-
byarrowsinFig.1). Thefirsttesttargetsconsistencybias: cultywhenposedwitharithmeticmathwordproblemscom-
A problem text is easier to comprehend if the relational paredtothesameproblemsformulatedsolelyasarithmetic
keywordverballysuggeststheoperationthatisrequiredto expressions (see, e.g., Jaffe and Bolger, 2023). This sug-
solvetheproblem(LewisandMayer,1987).Thesecondtest geststhatarithmeticcomputationskillaloneisnotsufficient
targetswhatwecalltransfervscomparisonbias,thatprob- tosuccessfullysolvemathwordproblems. Inordertodis-
lemswithastaticcomparisonrelationshiparemoredifficult tinguishthedifferentskillsthatareinvolved,pastworkhas
forchildrenthanproblemswithadynamicchangeofstate, representedmathwordproblemsalongsimilarlevelsaswe
even when they involve the same arithmetic expressions doabove(NesherandTeubal,1975): (i)problemtext,(ii)
(Rileyetal.,1983). Thethirdtesttargetsthecarryeffect, underlyingmathematicalrelationships,and(iii)symbolic
i.e.,thatarithmeticcomputationsaremoredifficultifthey expressions. Solvingaproblem,then,involvestransitioning
entailmovingsomedigittoacolumnofmoresignificant fromlevel(i)toafinalanswer,possiblythroughlevels(ii)
digits(Hitch,1978).
and(iii),witheachtransitionrequiringaseparateskill.4
WefindthatLLMsdoindeedexhibitsomebiasesthatreflect Thereismuchresearchonwhichfactorsbestexplainprob-
thoseobservedinchildren. Ourexperimentswithbothbase lemdifficulty. Rileyetal.’s(1983)modeloftheproblem-
andinstruction-tunedmodels—specifically,LLaMA2(Tou- solving process emphasized the degree of complexity at
vronetal.,2023),Mistral(Jiangetal.,2023),andMixtral level(ii)astheleadingfactorunderlyingperformance,mo-
(Jiangetal.,2024)—revealthatthesemodelsdemonstrate tivatedbyempiricalevidencethatsomearithmeticconcepts
significanteffectsofconsistencybiasandtransfervscom- areharderforchildrentounderstandthanothers. However,
parison bias, like child learners. Interestingly, however, theirmodeldoesnotaccountforhowthemathematicalrela-
thistrenddoesnotextendtothecarryeffectbiasfromthe tionshipsarederivedfromtheproblemtext(Cumminsetal.,
solutionexecutionstep. Theseresultscontributetoourun- 1988).Thispartissignificantaswell,asseveralstudieshave
derstandingofthecapabilitiesandlimitationsofLLMsas found that altering the linguistic form of a problem with-
cognitive models, particularly in the context of cognitive outchangingtheunderlyingmathematicalrelationshipscan
developmentresearchandeducationalapplications. havedrasticeffectsonperformance(Hudson,1983;Lewis
andMayer,1987; interalia), inbothchildrenandadults.
Thispartoftheprocessisencompassedbythemodelsof
2.CognitiveModelingoftheSolvingProcess
BriarsandLarkin(1984)andKintschandGreeno(1985).
Thissectiondiscussesthecognitiveprocessthatisinvolved Noneofthesemodelsgiveanexplicitaccountofthecom-
in solving math word problems. We first introduce our plexityofthesymbolicexpression,however,whichalsohas
conceptualizationthatisillustratedinFig.1,whichwethen significantinfluenceonperformance(Daroczyetal.,2015).
motivatebysummarizingrelevantliterature.
4Not everyone uses all three levels in their solving process.
Hegartyetal.(1995)findevidencethatunsuccessfulhumanprob-
3Thisworkpredominantlyreferstostudiesthathavebeenper-
lemsolversoftenoptforshort-cutstrategiesthatrelyonsurface-
formedonchildren,butwenotethatsomeofthebiasesconsidered
levelfeaturesoftheproblemtext(thus,byourconceptualization,
seemtobepresentinadultsaswell(albeitwithweakereffects).We
movingdirectlyfromtexttosymbolicexpressions),whereassuc-
discusssomepotentialimplicationsthismayhaveinconjunction
cessful solvers are more likely to make use of mental models.
withourresults(§5.2-5.4).
2DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
3. HumanBiasesinWordProblems Symbolicexpressions:Thecarryeffect. Beyondthetext
andmentalmodel,itisnaturalthattheparticularnumbers
Inthissectionwediscusstheparticularfactorsthatinfluence
used in a problem will have an effect on a child’s perfor-
performanceofhumanchildren(i.e.,cognitivebiases)which
mance(Daroczyetal.,2015).Considerthesameproblem(s)
westudyinLLMs(§5). Eachbiasisreflectedbyavariation
asabove,butwithadifferentnumbergiveninthepremise:
intherepresentationofaspecificlevelofFig.1. Westudy
onebiasforeachofthethreelevelsthatprecedestheanswer Alicehas22apples.Bobhas3fewerapplesthan
(i.e.,problemtext,mentalmodelandsymbolicexpressions). Alice. HowmanyapplesdoesBobhave?
Theproblemnowhasthesymbolicexpression22−3,which
Problemtext: Consistencybias. Giventhepremise“Al-
involves an arithmetic carry (also called borrow, in the
ice has 5 apples” and a question querying the (smaller)
case of subtraction). A carry is a digit that is transferred
numberofapplesofanotheragent“Bob”,anadditivecom-
from one column of digits to another as the result of an
parisonstatementbetweenthetwoagentscouldtakeeither
arithmeticcomputation. Inthissubtractioncomputation,a
ofthefollowingforms:
unit carry of 1 is transferred from the column of units to
(1)Bobhas3fewerapplesthanAlice. thecolumnoftensinordertomaketheanswer19.6 The
(2)Alicehas3moreapplesthanBob. previousexpression(5−3)didnothaveacarry,whichis
easierforchildren(Hitch,1978;Ashcraftetal.,1992). The
Here, (1) represents a consistent statement because the
presenceofacarryintroducesanadditionalsubroutinefrom
relational keyword (“fewer”) suggests the operation that
thestandardsequenceofoperations, whichplaceshigher
isindeedneededtocomputeBob’squantity(subtraction). demandonworkingmemory(FürstandHitch,2000).7
Conversely,(2)isaninconsistentstatementbecausethere-
lational keyword (“more”) suggests a different operation
4.ProblemGenerationMethod
(“addition”). Note that these two statements express the
samecomparisonrelationship, sothedifferenceliesonly
Our experiments on LLMs wrt the biases just discussed
in the problem text. Problems with an inconsistent state-
(§5)relyondatageneratedforthesolepurposeofourstudy.
mentaremoredifficultforchildrentosolvethanconsistent
Bynotusingproblemsfromexistingdatasetsorprevious
ones (Nesher and Teubal, 1975; Stern, 1993). This has
work it becomes unlikely that our data has been used for
beenhypothesizedtobethecaseduetoinconsistentstate-
trainingofthemodels(whichisanincreasinglycommon
mentsrequiringanadditional,error-prone,deductionstep:
issue;Dodgeetal.,2021;Elazaretal.,2023).
torearrangetherelationalstatementtobeinthepreferred
consistentform(LewisandMayer,1987). Thissectiongivesthedetailsofourdatagenerationpipeline,
which provides control over features across all levels of
Fig.1. In§4.1weoperationalizeFig.1,givingdefinitions
Mentalmodel: Transfervscomparisonbias. Irrespec-
relatedtothementalmodellevelandotheraspectsofthe
tiveofwhichrelationalkeywordisused,comparison-type
process. Usingthesedefinitions,wethenexplainourgener-
problemstendtobemoredifficultforchildrenthanother
ationpipelinein§4.2.
typesofarithmeticconcepts(Rileyetal.,1983). Inpartic-
ular,gradeschoolchildrendisplayasignificantdifference
inperformancebetweencomparisonproblemsandtransfer 4.1. OperationalizingFig.1
problems. Considerthesamepremiseasabovebutwitha
The mental model level is operationalized using the for-
slightlydifferentcontinuation:
malism from Opedal et al. (2023), called MATHWORLD.
MATHWORLD annotates each math word problem with
Alicehas5apples. Alicegave3applestoBob.
alogicalrepresentation,whichcapturesthemathematical
HowmanyapplesdoesAlicehave?
relationshipsbetweentheentitiesdescribedintext.Eachen-
This is a transfer problem,5 concerning a change of state tityhasanon-negativeintegerquantity. Optionally,there
ofsomequantity. Ithasthesamesymbolicexpressionas may be additional information associated with an entity;
thecomparisonproblemsabove(althoughwithadifferent namely, an agent, a unit and some attribute. (The five
mentalmodel),butiseasierforyoungchildrentosolve. In bolditemsinthetwoprecedingsentencesarereferredtoas
analyzing their solution strategies, it has been found that
6Additionisanalogous:19+3involvesaunitcarryof1.
comparisonproblemsrequireanumber-matchingtypestrat- 7Theparticularnumbersgivenintheexamplesherearesmall
egythatappearstobemoresophisticatedthanthecounting- enough for children to likely be relying on memory for these
typestrategiesthatareoftensufficientforsolvingtransfer computations(KoshmiderandAshcraft,1991),whichcoulderase
problems(Rileyetal.,1983;CarpenterandMoser,1984). theeffectofcarry.Weaccountforthisinourexperiments(§5.4)
byconsideringlargernumberranges.
5Transferisoftencalledchangeelsewhere(Nesheretal.,1982).
3DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
properties.) Thearithmeticrelationshipsareclassifiedac- LogicalForm
ExampleSentences
cordingtoconcepts;weusetransfer,comparison(additive Predicate Properties
andmultiplicative),andrateinthiswork. agent=Alice Alicehas5kilogramsofred
quantity=5 apples.
Whendiscussingdatageneration(§4.2)andtheexperiments container entity=apple
Aliceowns5kilogramsofred
attribute=red
the data is used for (§5), we will rely on the following apples.
unit=kg
definitions: Apredicateisasymbolthatrepresentseither
type=+ Bobhas3fewerapplesthan
an arithmetic concept, or ownership of an entity (in that agentA=Bob Alice.
casethepredicateis“container”). Eachpredicatetakes comparison agentB=Alice
quantity=3 Alicehas3moreapplesthanBob.
asetofpropertiesasarguments.8 Apredicateinstantiated entity=apple
withpropertiesiscalledalogicalform,andrepresentsthe receiver_agent=Bob
AlicegaveBob3apples.
semanticsofagivensentenceinaproblem. SeeTable1for transfer sender_agent=Alice
quantity=3
examplesoflogicalforms. Thementalmodelofaproblem entity=apple Bobgot3moreapplesfromAlice.
is a sequence of logical forms for each sentence in that agent=Alice EachofAlice’sbasketsholds4
problem(inthesameorder),representingitssemantics.9 We quantity=4 apples.
rate
entityA=apple EverybasketthatAlicehas
saythataproblemtextisfaithfultoamentalmodeliffthe
entityB=basket contains4apples.
mentalmodelrepresentsthesemanticsofthattextunderthe
MATHWORLDformalism. Thestructureofaproblemisa
Table1.ExamplesofMATHWORLDlogicalforms.Alogicalform
mentalmodelinwhichthepropertyvaluesarereplacedby
consistsofapredicateandasetofproperties. Notethattheat-
uniqueplaceholders. Ontheleft-handsideofFig.2wegive
tributeandunitpropertiesareoptional,andonlythefirstlogical
agraphicalrepresentationofthestructureoftheproblem formhasnon-nullvaluesforthem(“red”and“kg”,respectively).
textthatisshownontheright-handsideofthesamefigure. Eachlogicalformisgivenwithtwooftheviabletemplatedsen-
tencesfromourgenerationpipeline(§4.2),thesemanticsofwhich
Finally,weformalizethesymbolicexpressionlevelofFig.1.
isrepresentedbythelogicalform.
Everyconcept-basedpredicatecorrespondstoanequation
x=y⊙z,with⊙∈{+,−,×,÷}andx,y,z ∈Z ∪V
≥0 theproblemschematafromRileyetal.(1983),specifically,
whereV isasetofvariablesymbols. Werequirethat∃!v ∈
intermsofthebreadthofconceptsandpropertiessupported.
{x,y,z}:v ∈V. Werefertothedeductiveinferencestep
taken to solve that equation as a reasoning step, and its
4.2.GenerationPipeline
output(i.e.,thevalueofthevariable)asanintermediate
result.Thesymbolicexpressionlevelconsistsofasequence We propose a general, but simple, problem generation
ofsuchreasoningsteps,whichisaproofoftheanswerof pipeline,describedinthissectionandlaterappliedin§5. It
theproblem. Anyfactfromthementalmodelisanaxiom proceedsinfoursteps: (i)samplingtheproblemstructure,
thatcanbeusedintheproof. Thereasoningofaproblemis (ii)obtainingamentalmodelbyinstantiatingthestructure
lineariffeveryreasoningstephasatmostonenon-axiom withproperties,(iii)transformingthementalmodelintotem-
premise,andnonlinearotherwise. platednaturallanguage,and,finally,(iv)correctingspelling
errorsandawkwardphrasingsinthetemplatedtextusingan
Noteonplausibility. Amentalmodeltheoryoversome instruction-tunedLLM.Fig.2illustratesthefullpipeline.
reasoningdomainmustbeabletoadequatelyexplainthe
relativedifficultyacrossdifferenttypesofreasoningprob- (i)Problemstructuregeneration. Westartbygenerating
lems(Johnson-Laird,1983). OurMATHWORLD-basedop- theproblemstructure. Thenumberofreasoningstepsand
erationalizationemphasizesarithmeticconceptsandtheir theparticularpredicatesusedarebothsampleduniformlyat
relationalstructureasthekeyfeaturesthatexplainerrorsat random,thelatterfollowingtheformer.Thespecificclassof
thementalmodellevel,whichisinlinewithexistingtheo- structuresfromwhichwesampleistestdependent(see§5).
ries(Rileyetal.,1983;BriarsandLarkin,1984;Kintschand Suchaclasscould, forinstance, betheclassofalllinear
Greeno,1985). Ourschemataforthelogicalformsextend reasoningstructureswithatmost5reasoningstepsthatonly
containtransferpredicates. Byourdefinitionofstructure,
8Weenforceallquantitiesthatareassociatedwithpredicatesto
theitemsgeneratedatthisstephaveuniqueplaceholders
beexplicitnumbers.Notethatthisplacesaconstraintonthefor-
matoftheproblems:Thenumberassociatedwithamathematical inplaceoftheproperties(e.g.,agent2,entity1). We
relationshipisneveranintermediateresult,butisalwaysgivenin onlyintroducenewentityplaceholdersinratelogicalforms
text. Ageneralizationthatallowsvariablesisstraightforwardin (seeTable1,inwhichrateistheonlypredicatethattakes
theory.Itwouldinvolvedefininglogicalformsovermultiplepred-
twoentityproperties). Wedetermineuniformlyatrandom
icateswhich,however,wouldrequirealargernumberoftemplates
whetheranentityispairedwithanattribute,aunit,ornone
forstep(iii)in§4.2.
9Mental models can be illustrated graphically, as in Figs. 1 ofthem. Theinstantiationofagentpropertiesistestspecific.
and2. Finally,theanswerofaproblemisalwayssetastheinter-
4Edward has 2 watchs.
Annie has 2 times the amount of watchs
compared to the amount Edward has.
Annie bought 3 watchs.
Annie sold 4 watchs.
Maggie owns 2 boxs.
Every box that Maggie has contains 4 watchs.
Maggie has 7 more watchs than Annie.
DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
Template
1 Problem Structure Generation 3 Sampling Body
2 Edward has 2 watchs.
n1a g ee nn tit t1 y 1 Problem E 2 d ww aa tcrd h A con mni pe ah ra es d 2 to t i tm hee s a t mh oe ua nm t o Eu dn wt ao rf d w ha at sc .h s
Structure
[n2]x +n3 ?a eg ne tn itt y2 1 Instantiation 2x +3 ?A wn an ti ce h Annie bought 3 watchs.
agent2 -n4 Annie -4 Annie sold 4 watchs.
? entity1 ? watch
agent2 Annie Maggie owns 2 boxs.
? entity1 ? watch
Every box that Maggie has contains 4 watchs.
Δ=n7 Δ=7
Maggie has 7 more watchs than Annie.
agent3 ratio=n6 agent3 Maggie ratio=4 Maggie
n5 entity2 ? entity1 2 box ? watch How many watchs does Annie have?
Question
container ( [agent1], [n1], [entity1] ); Complete MWP 4 LLM Prompting
comparison ( ×, [agent2], [agent1], [n2], [entity1] );
transfer ( None, [agent2], [n3], [entity1] ); Edward has 2 watches. Annie has twice Linguistic Error
the number of watches that Edward has. Correction
transfer ( [agent2], None, [n4] , [entity1] );
Annie bought 3 more watches. Annie sold 🤖
container ( [agent3], [n5], [entity2] ); 4 watches. Maggie owns 2 boxes. Every
rate ( [agent3], [n6], [entity1], [entity2] ); box that Maggie has contains 4 watches.
comparison ( +, [agent2], [agent3], [n7] , [entity1] ); Maggie has 7 more watches than Annie.
How many watches does Annie have?
Figure2.Overviewofourgenerationpipelinewithanonlinearexampleproblem. (1)Westartbygeneratingtheproblemstructure,
whichdeterminesthesymbolicexpressionsandthearithmeticconceptsfromwhichthoseexpressionsareentailed.Thealignmentofthe
graphicalandsequentialformatsofthestructureareillustratedbycolorcoding,andtheblackcontainmentboxesinthegraphrepresent
intermediateresults.(2)Thepropertiesofthosestructuresaretheninstantiatedwithvalues,resultinginamentalmodel.(3)Next,we
sampleatemplatedsentenceforeachofthelogicalformsinthementalmodel,andconcatenatethemintheorderingofthelogical
forms.(4)Finally,wecorrecterrorsandawkwardphrasingsbypromptinganinstruction-tunedLLM.Thisparticularexampleincludesall
predicatetypesthatweusein§5.
mediateresultcorrespondingtothelastlogicalforminthe foreachofthepredicatesweconstructasetoftemplatesthat
ordering(whichisunique). representnaturallanguageadheringtothatpredicate.Forin-
stance,transfer(Alice, Bob, 3, apple)canbe
convertedto“[Bob]gave[3][apple]sto[Alice]”(seeTable1
(ii)Problemstructureinstantiation. Next,theproblem
foradditionalexamples).Thetemplatesarehandcrafted.We
structureisinstantiatedwithproperties,whichgivesamen-
sampleonetemplateuniformlyatrandomforeachpredicate
tal model. We use a handwritten vocabulary for each of
inthementalmodel. Wealsocreateandsampleinterrog-
thelexicalproperties(entity,agent,unitandattribute)and
ativetemplatesforthequestions,whichalwaysquerythe
sampleinstantiationsofthesepropertiesfromthosevocab-
intermediateresultderivedfromthelastpredicate. Finally,
ulariesuniformlyatrandom. Thenumericalquantitiesare
weconcatenatethesentencestoobtainthefullproblemtext.
instantiatedbysamplinguniformlyatrandomasetofnum-
bersfromtherange2-20(onenumberforeachpredicate).10 This step enables control over the linguistic form of the
Then,weenumeratethelogicalformsandaccordinglycom- sentencesintheproblemtext,whichwillbeimportantfor
puteintermediateresultsforeachreasoningstep,making ourtestrelatedtotextcomprehensionin§5.2. Moreover,
surethattheintermediatequantitiesfallintherange0-999; since the faithfulness of the templated text is ensured by
ifnot,anewsetofnumbersissampledandtheprocedure manualdesign,theprocedureuptothispointensuresthat
is repeated from the beginning. This naive procedure is thetextisfaithfultothementalmodel.
sufficientlyfastforourpurposes. Empirically,weobserved
an average runtime of ∼4 ms to generate a numerical in-
stantiation for a problem. This step enables control over
the numerical features of the problem, as well as certain (iv) Linguistic error correction. However, templated
linguisticfeatures(i.e.,thelexicalproperties). textsoccasionallyinducespellingmistakesandawkward
phrasings. For instance, imagine that the entity “watch”
is inserted into the example template from the paragraph
(iii)Templatesampling. Thementalmodelisthencon-
above, to make “watchs”. Inspired by the demonstrated
vertedtonaturallanguageusingtemplatedtext. Specifically,
successofzero-shotgrammaticalerrorcorrection(Kwon
10We omit 0 and 1 from the number range in order to avoid etal.,2023;Loemetal.,2023),weuseaninstruction-tuned
unnaturalphraseslike“Bobhas1timesasmanyapplesasAlice.” languagemodel(Ouyangetal.,2022),GPT-3.5Turbo,to
5DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
correctsucherrors.11 Wewriteashortinstructiveprompt ofX. Usingthisdata,weestimatetheconditionalaverage
andhavethemodelgenerateacorrectedproblemtextcondi- treatmenteffect(CATE;ImbensandRubin,2015)
tionedonthatprompttogetherwiththeparticulartemplated
textwewanttocorrect. Thepromptinstructsthemodelto E[Y(x)−Y(x′)|Z], (1)
beconservative,toonlycorrectlinguisticerrorsandawk-
ward phrasings. We provide the exact prompt used and
where Y is 1 if the model’s prediction is correct and 0
additionalgenerationdetailsinApp.A.
otherwise,xandx′aretwodistinctvaluesofthetreatment
variableX,andZisthesubgroupofdatageneratedthrough
Evaluatingdataquality. Itiscrucialthatthegenerated ourpipelineforaspecifictest. Thetwovaluesxandx′are
problemtextsarefaithfultothementalmodelsfromwhich definedsuchthatpositiveCATEsareconsistentwithhuman
theyweregenerated,soweperformmanualevaluationof biases. WerefertoFederetal.(2022)forfurtherreadingon
thedata. Wefollowagenericprocedureandperformitfor causality-basedmethodsforNLP.
eachofthedatasetsgeneratedfortheexperimentsin§5.2-
Foreachofthetestsdescribedbelowweselectaspecific
5.4. Theprocedureisiterateduntilweachievesatisfactory
featureX whichislocalizedtooneofthelevels,totheex-
quality. Thefinalerror-rateestimateswere0%forallthree
tentpossible.Thatis,varyingXassociatedwithaparticular
datasets. DetailsaregiveninApp.B.
levelshouldhavenoeffectonthelevelsabove,andminimal
effectonthelevelsbelow. Forinstance,in§5.2wevarythe
Relatedapproaches. Ourgenerationpipelinediffersfrom problemtextwithoutaffectingthementalmodel,symbolic
Opedal et al.’s (2023) method in that we generate inter- expressionoranswer.
mediatetemplatedtexts,whiletheygeneratetheproblem
HavingselectedX,weadaptthepipeline(§4.2)togenerate
textsdirectlyconditionedonmentalmodels. Polozovetal.
examplepairs,onewithX =xandonewithX =x′. Next,
(2015)alsogenerateswordproblemsfromlogicalrepresen-
weevaluatethedataqualityusingtheproceduredescribed
tations,buttheirapproachdoesnotallowexplicitcontrol
in App. B. After quality assurance, we generate a larger
overarithmeticconcepts(whichisanimportantfactorun-
sampleof400additionalproblempairs,which(including
derlyingdifficultylevel;§3). Whileourexperimentsrequire
thequalityevaluationset)givesatotalof500pairsforthe
strictcontroloverlinguisticform,ourerrorcorrectionstep
tests. WethengenerateoutcomesY(x)andY(x′)foreach
couldinprinciplebebroadenedtoperformparaphrasingand
ofthepairsforasetofselectedopen-sourceLLMs. Weuse
themerewriting(Koncel-Kedziorskietal.,2016)aswell.
LLaMA2(Touvronetal.,2023)with7Band13Bparame-
ters,aswellasMistral7B(Jiangetal.,2023)andMixtral
5.Experiments
8x7B(Jiangetal.,2024). Forallmodels,weconsiderboth
the pretrained-only and instruction-tuned versions. This
Wegeneratedatatoperformtestsonwhetherlargelanguage
givesatotalofeightmodels. Wecarryoutzero-shotinfer-
modelsexhibitchild-likebiasesusingthepipelinediscussed
encewithbothastandardpromptandachain-of-thought
above. We aim to identify where in the process in Fig. 1
prompt(CoT;Weietal.,2022). Withtheformer,themodels
thosebiasesemerge. Wethereforesplitourtestsaccording
arepromptedtodirectlyprovideanansweraftertheinput,
totheleveltheytarget: problemtext(§5.2),mentalmodel
andthemodelpredictionisretrievedfromtheresponseby
(§5.3)andsymbolicexpressions(§5.4). First,wediscuss
extractingthefirstnumberinthemodel’soutput. Forthe
thegeneralexperimentalsetup(§5.1).
CoTexperimentalprocedure,wefollowtheexactmethod
fromKojimaetal.(2022). First,themodelispromptedto
5.1.ExperimentalSetup
generateareasoningchainbyappending“Let’sthinkstep
Webaseourexperimentsontheproblemfeaturesdiscussed by step” to the input.12 Then, the model is re-prompted
in§3thathavebeenfoundtohaveaneffectonchildperfor- to generate the final answer, which is extracted from the
manceinsolvingwordproblems. Specifically,givensuch outputasinthedirectcase. Responsesaregeneratedwith
afeatureX,wewanttoknowwhetherX hasacausalef- greedydecodingandamaximumlengthof256tokens. Af-
fectontheperformanceofLLMs. Ourgenerationpipeline terobtainingthemodel’spredictions,weestimatetheCATE
enablesexactmatchingofthedata: Wegenerateproblems andperformapairedsamplet-testtodeterminewhetherthe
in pairs, where the two problems differ only in the value CATEestimateissignificantlydifferentfrom0.
11Theerrorcorrectionstepmightintroducebiasfromthespe- 12TakinginspirationfromclaimsthatLLMscanactasagent
cificmodelthatisused,soweavoidevaluatingonGPT-3.5Turbo models(Andreas,2022),wealsoexperimentedwithanadditional,
inourexperiments. Indeed, wefoundthatitwouldsometimes similar,promptinwhichtheLLMisinstructedtoimpersonatea
transforminconsistentformulationsofcomparison-typerelation- grade-schoolchild: “Let’sthinkstepbystepasagrade-school
shipsintosemanticallyequivalentconsistentones.Sucherroneous childwould.”Theresultsfromthissetupweresimilartothoseof
transformationswerefilteredout(seeApp.A). theCoTsetup,andarepresentedinApp.C.1.
6DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
Consistencybias(§5.2) Transfervscomparisonbias(§5.3) Carryeffect(§5.4)
Mode Model Accuracy(%) Accuracy(%) Accuracy(%)
p-val p-val p-val
Co InCo CATE T C CATE NCa Ca CATE
LLaMA27B 9.6 4.8 4.8 <0.001 21.8 13.0 8.8 <0.001 64.8 60.0 4.8 0.009
LLaMA213B 17.2 14.0 3.2 0.006 28.6 20.0 8.6 <0.001 72.2 67.2 5.0 0.030
Mistral7B 17.8 12.0 5.8 <0.001 34.0 20.4 13.6 <0.001 72.4 72.0 0.4 0.835
Mixtral8x7B 23.0 17.0 6.0 <0.001 42.2 30.4 11.8 <0.001 95.4 93.6 1.8 0.117
Direct
LLaMA27BChat 14.2 10.8 3.4 0.009 20.2 15.8 4.4 0.005 61.2 54.2 7.0 0.012
LLaMA213BChat 16.4 11.8 4.6 <0.001 25.4 18.2 7.2 <0.001 65.6 59.6 6.0 0.018
Mistral7BInstr. 17.6 14.2 3.4 0.008 28.0 21.8 6.2 <0.001 78.0 78.6 -0.6 0.802
Mixtral8x7BInstr. 23.4 21.8 1.6 0.195 42.6 28.0 14.6 <0.001 95.8 96.4 -0.6 0.578
LLaMA27B 16.4 6.0 10.4 <0.001 18.8 13.6 5.2 0.009 33.2 38.8 -5.6 0.006
LLaMA213B 30.2 8.6 21.6 <0.001 37.8 13.2 24.6 <0.001 33.8 33.4 0.4 0.833
Mistral7B 36.4 16.8 19.6 <0.001 49.8 58.8 -9.0 0.004 73.2 71.0 2.2 0.283
Mixtral8x7B 62.4 42.2 20.2 <0.001 68.6 65.0 3.6 0.206 79.8 79.8 0.0 1.000
CoT
LLaMA27BChat 66.8 38.6 28.2 <0.001 69.6 40.8 28.8 <0.001 72.4 71.0 1.4 0.514
LLaMA213BChat 67.0 28.6 38.4 <0.001 79.4 48.0 31.4 <0.001 73.8 78.6 -4.8 0.017
Mistral7BInstr. 61.8 33.6 28.2 <0.001 83.4 52.0 31.4 <0.001 78.6 75.6 3.0 0.162
Mixtral8x7BInstr. 85.4 71.6 13.8 <0.001 98.2 83.8 14.4 <0.001 97.0 94.6 2.4 0.014
Table2.Accuracy,conditionalaveragetreatmenteffect(CATE),andstatisticalsignificance(p-value)onmathwordproblemsgenerated
forthethreetestsdetailedin§5.2,§5.3and§5.4.‘Co’denotesconsistent,‘InCo’inconsistent,‘T’transfer,‘C’comparison,‘Ca’carry,
and‘NCa’nocarryconditions. Resultsareseparatedbywhetherthepromptingmethodisdirectorchain-of-thought(CoT).‘Chat’
and‘Inst.’indicatetheinstruction-tunedversionsofthemodels.CATEvaluesareboldedwhenp<0.01.Resultsfromanadditional
promptingstrategy,child-personaprompting,arepresentedinTable4.
5.2.ProblemText: ConsistencyBias tocomparison,onebeingconsistentandtheotherbeing
itsanalogousinconsistentform.
Varyingthelinguisticformofanotherwiseequivalentprob-
lemstructurecanhavealargeeffectonchildperformance
(Cumminsetal.,1988). Herewetestwhethercomparison
problemswithinconsistentstatementsaremoredifficultfor Results and discussion. The results of the consistency
LLMs than the same problems replaced by an analogous biastestrevealstatisticallysignificantCATEs. Asdisplayed
consistentstatement. inTable2(leftmostcolumn),allmodelsexhibitlowerac-
curacy when solving inconsistent problems compared to
theirconsistentcounterparts. Possibly,thisbiasstemsfrom
Method. Following Lewis and Mayer’s (1987) study,
thedistributionofthetrainingdata,whichinturncouldbe
we consider consistent/inconsistent problem pairs where
a product of the same cognitive bias in the humans who
the required operation is either addition, subtraction,
createdit. Thisexplanationseemssensiblegiventhatthe
multiplicationordivision. Thegenerationpipelineistuned
consistencybiasispresentnotonlyinchildren,butinadults
such that the problem structures follow the specification:
aswell(LewisandMayer,1987;Hegartyetal.,1995).
container◦(transfer|rate)◦···◦(transfer|rate)◦ Interestingly, the consistency bias appears to be exacer-
(cid:124) (cid:123)(cid:122) (cid:125) batedbyCoTprompting,whichimprovestheoverallmodel
0−2times
comparison◦(transfer|rate)◦···◦(transfer|rate); performance,butmagnifiesthedifferenceinaccuracybe-
(cid:124) (cid:123)(cid:122) (cid:125) tweenconsistentandinconsistentproblems. Thisfinding
0−2times
aligns with research indicating that CoT prompting may
also amplify other types of biases present in the training
where ◦ is sentence-separating concatenation. The rea-
data(Shaikhetal.,2023). ParticularlynotableCATEsare
soning is linear with 1−5 steps in total (one for every
observedforthebaseversionsofLLaMA27B,LLaMA2
non-container predicate). Apart from the first predicate
13B,andMistral7B,forwhichtheinconsistentformulation
(container),onlythecomparisonpredicateintroduces
oftheproblemsleadstoanaccuracydroplargerthan50%.
anewagent. Thequestionqueriestheagentthatwasintro-
duced by comparison.13 The pairs are generated such
5.3. MentalModel: TransfervsComparisonBias
thattheonlysentencethatvariesisthatwhichcorresponds
Anotherfactorbehindperformanceisthatitmightbeharder
13Thefollowingisa(consistent)examplefromourdatasetthat
toperformsolutionplanningbasedonsomementalmodels
followsthepatterncontainer◦transfer◦comparison:
“Averyhas15desks.Averybought18desks.Nataliehas16fewer compared to others. We test whether LLMs are better at
desksthanAvery.HowmanydesksdoesNataliehave?” solvingtransfer-typeproblemsthancomparisonproblems.
7DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
Method. Theproblemstructurestakethefollowingforms: CATE by Number of Steps (Transf. vs Comp.)
LLaMA2 7B LLaMA2 13B
container◦transfer◦···◦transfer; 0.6 0.6
(cid:124) (cid:123)(cid:122) (cid:125) 0.4 0.4
1−5times TE 0.2 0.2
A
container◦comparison◦···◦comparison; C 0 0
(cid:124) (cid:123)(cid:122) (cid:125) −0.2 −0.2
1−5times
1 2 3 4 5 1 2 3 4 5
Theyaregeneratedtohavelinearreasoning,andhavethe Mistral 7B Mixtral 8x7B
0.6 0.6
samesymbolicexpressions(includingnumbers). Eachcom-
0.4 0.4
parison predicate corresponds to a comparison of a new TE 0.2 0.2
A
agentwiththeagentintroducedintheprecedingsentence. C 0 0
Eachtransferstatementfollowsthesameagent,whowas −0.2 −0.2
1 2 3 4 5 1 2 3 4 5
introducedinthefirstsentenceandwhosestateisupdated
Number of Reasoning Steps Number of Reasoning Steps
through a transfer with some other agent. The problems
resemble each other in linguistic form as much as possi- Base Model Instruction-tuned
ble. Inparticular,wemakesurethatthesameagentnames Figure3.CATE for transfer vs comparison (§5.3) in the CoT-
areintroducedineachsentenceacrossthetwoproblems.14 promptedcasestratifiedbynumberofreasoningsteps.‘Instruction-
Consistentorinconsistentformsofcomparisonaresampled tuned’referstotheChatandInstructversionsofLLaMA2and
uniformlyatrandom. Mistral/Mixtral,respectively.Baseandinstruction-tunedmodels
displayoppositerelationshipsbetweenlengthandbiasstrength.
Resultsanddiscussion. Theexperimentalresults(middle Method. Wegeneratepairsunderthecomparisonspecifi-
columninTable2)showthatmodelsareconsistentlymore cationfrom§5.3,butwithonlyonestep:15
accuratewhensolvingproblemsbasedontransfersrather
than comparisons. With the exception of CoT-prompted container◦comparison;
pretrained-onlyMistralandMixtral,weobservestatistically
Likein§5.3,weuseadditivecomparisons,whichensures
significant positive CATEs, mirroring biases seen in
thatthesymbolicexpressionsonlyhaveadditionand/orsub-
children’sproblem-solving(§3). Asfortheconsistencytest,
tractionoperators. Thetwoproblemsofapairareidentical
this phenomenon might be due to the distribution of the
exceptforthenumbers. LikeFürstandHitch(2000),ween-
trainingdatabeingskewedaccordingtothebiasesofthe
surethatbothoperandsaswellastheansweroftheproblem
humanswhocreatedit. However,unlikefortheconsistency
are three-digit numbers (since children appear to rely on
bias, we are unaware of whether this particular bias is
memoryforproblemswithsmallnumbers;Koshmiderand
presentinadults.
Ashcraft,1991). Oneoftheproblemshasnocarry,theother
InFig.3weshowhowtheCATEvarieswiththenumberof has at least one (i.e., unit and/or tens carry). The correct
reasoningstepsintheproblemsfortheCoTsetting. Inter- answerofthetwoproblemsiscontrolledtobethesame.
estingly,weobservethattheCATEsizesincreasewiththe
numberofreasoningstepsfortheinstruction-tunedmodels, Resultsanddiscussion. Theresults(rightmostcolumnin
whereastheydecreaseforthepretrained-onlybasemodels. Table2)showadeparturefromthefindingsabove,which
We are unaware of literature on the relationship between gaveevidenceforthepresenceofchild-likebiases. Inthis
humantransfervscomparisonbiasandthenumberofsteps, case, modelperformanceissimilarinproblemswithand
sowecannotmakeanyclaimsaboutwhichofthesepatterns without carry operations. The absence of a carry effect
ismorecognitivelyplausible. Theothertestsdonotexhibit might reflect fundamental differences in how LLMs and
suchdivergingtrendsfortheCATEs(seeApp.C). humans approach arithmetic computations. In particular,
the carry effect in humans is partially attributed to work-
5.4.SymbolicExpressions: TheCarryEffect ingmemorylimitations(Hitch,1978),whichLLMsdonot
implementinthesamemanner. Thememoryandcompu-
Whilemuchofachild’sperformanceonwordproblemscan
tationalmechanismsthroughwhichmodelsperformarith-
be explained by properties introduced by the text format,
metic(Nandaetal.,2023;Stolfoetal.,2023a;Quirkeand
alargeportionstilldependsonthenatureofthesymbolic
Barez,2024)arelikelynotaffectedbytheincreasedcog-
expression(Daroczyetal.,2015). WetestwhetherLLMs
aresensitivetothepresenceofarithmeticcarrywhenposed 15Theone-stepcasefollowsthesetupsfromstudiesonhumans
withadditionandsubtractioninmathwordproblems. (§3). We discovered in the previous two tests that the models
frequentlyfailoncomparisonproblemswithonlyonestep(which
14LLMsappeartobesensitivetotheparticularchoiceofin- canbeinferredfromFigs.3and5),soifacarryeffectispresent,
contextnamedentities(Goodarzietal.,2023). itshouldbeobservableinsuchasetting.
8DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
nitive load that the carry operation introduces in humans. otherfactorsthatmightexplainlanguagemodelbehavior,
Moreover, itmaybethatthereisnodifferenceinthefre- sinceonewouldexpectthetrainingsettobeheavilybiased
quency of problems with or without carry in the models’ towards adult (rather than child) thinking. Therefore, it
training data (unlike what we hypothesized for the other mightbeapromisingdirectionforwardinlanguagemodel
biases),whichwouldexplaintheabsenceofaneffect. Per- interpretabilitywork.
hapsthecarryeffectislessprevalentinadultscomparedto
theothertwobiasesand,therefore,haslessofaninfluence
BroaderImpact
onthefeaturesofthetrainingdata(whichis,presumably,
createdbyadults). Cognitivemodelingenableshumansimulationsinplaceof
datacollectionthatmightotherwisebeunethical,harmful
6.RelatedWork or costly. On the other hand, issues could arise if those
simulationsareunfaithfultohumanbehavior. Asabroader
Our work relates most closely to studies that have com- implication of our work, we encourage practitioners to
paredhumanandLLMbiasesonsyllogisms(Andoetal., exercise care when developing and deploying cognitive
2023;Eisapeetal.,2023)andothernon-mathematicalinfer- models of students using LLMs, particularly, in how the
encetasks(Dasguptaetal.,2023). Theirfindingsindicate student model treats numbers and other properties of
thatLLMsaresusceptibletosomeofthesamebiasesashu- arithmeticexpressions. Wedonotseeanynotableethical
mans,likecontenteffects(Andoetal.,2023;Dasguptaetal., issueswithourwork.
2023)andpremiseorderingeffects(Eisapeetal.,2023). We
observesimilarresultsinamathematicalproblem-solving
Limitations
settingforconsistencybiasandtransfervscomparisonbias,
butnotforthecarryeffectwhichrelatestothestepofthe Note that we do not adopt child-performance annotated
cognitiveprocessthatinvolvessolvingarithmeticequations. problems from learning science studies. This is done to
Ourstudyalsodiffersfromthosereferencedaboveinthat avoidevaluatingonexamplesthatmayhavebeenincluded
wesystematicallycomparetheeffectofCoTpromptingto inthemodel’strainingdata. Instead,weuseinsightsfrom
direct prompting, observing amplified effects in the CoT anaggregateofsuchresearchasbackgroundforthetests,
settinginmostcaseswhereeffectsarepresent. forwhichwegeneratenewdata. Thuswecannotdrawany
parallelsontheabsoluteperformanceincomparisonwith
We are unaware of any other work that studies cognitive
children, only on the presence or absence of each effect.
biasesthat,likethecarryeffect,relatedirectlytonumbers.
Moreover,wedonotconsiderthegradeleveloftheprob-
However,thereisevidencethatLLMstosomeextentrely
lems(see,e.g.,Jiaoetal.(2023)foragenerationmethod
onspuriouscorrelationsinnumericalreasoning(Razeghi
thatdoes).
etal.,2022;Stolfoetal.,2023b)andthattheirperformance
decreases with increasing number size on multiplication Inselectingspecificcognitivebiasestostudy,wechosebi-
problems (Dziri et al., 2023; Shen et al., 2023). Beyond asesthatarewell-establishedinliteratureonhumanchildren
numericalreasoning,LLMsappeartohavedifficultieswith andwhoseeffectscouldbeclearlyassociatedwithoneofthe
causalreasoning(BinzandSchulz,2023;Jinetal.,2023; stepsofFig.1. Anotherfactorthatfulfillsthesedesiderata
2024)andproofplanning(SaparovandHe,2023). istheeffectofexplicitverbalcues(Hudson,1983;Vicente
etal.,2007). Morefundamentally,acompletecomparison
7.ConclusionandImplications of the biases between LLMs and humans would need to
studybiasesthathavebeenfoundinLLMsbutarenotnec-
ThisstudyexploredwhetherLLMsexhibitchild-likecogni- essarilypresentinhumans. Wedonottakethatdirection
tivebiasesinarithmeticwordproblem-solving. Wefound intoaccount,butwenotethatthenumberfrequencyeffect
thatLLMsdemonstratebiasesintextcomprehensionandso- reported by Razeghi et al. (2022) appears to be one such
lutionplanningthatmirrorhumantendencies. Specifically, example.
modelsperformedbetteronproblemswheretherelational
WestressthattheconceptualizationinFig.1isasimplified
keyword is consistent with the appropriate arithmetic op-
model of the solving process. For instance, it fails to ac-
eratorcomparedtoproblemswhereitisnot,aswellason
countforshortcutstrategies(seeFootnote4)anditdoesnot
problemswithadynamicchangeofstatecomparedtoprob-
consideranypropositionaltext-baserepresentationwhich
lems with a static comparison. However, at the solution
precedes the mental model representation in some other
execution step, LLMs did not exhibit the child-like carry
models(KintschandGreeno,1985;Hegartyetal.,1995).
effectforarithmeticcomputations. Ingeneral,studyingbi-
asesthatarepresentinchildrenbutnotinadultsmayenable Finallyandimportantly,weonlyconsiderproblemsformu-
thedisentanglementoftheinfluenceoftrainingdatafrom latedinEnglish.Wenotethatsomeeffectscouldvaryacross
9DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
languages.Forinstance,thecarryeffectismorepronounced ThomasP.CarpenterandJamesM.Moser.1984. Theac-
inGermanandotherlanguageswherethespelled-outorder quisitionofadditionandsubtractionconceptsingrades
oftensandunitsisinvertedinrelationtoArabicnumerical onethroughthree. JournalforResearchinMathematics
notation(Göbeletal.,2014). Ourgenerationpipelinecan Education,15(3):179–202.
bestraightforwardlyadaptedtootherlanguages,andfuture
DeniseDellarosaCummins,WalterKintsch,KurtReusser,
workmightconsiderdoingso.
andRhondaWeimer.1988. Theroleofunderstandingin
solvingwordproblems.CognitivePsychology,20(4):405–
Acknowledgements
438.
WethankEmoWelzlandEthanGotliebWilcoxforvalu-
GabriellaDaroczy,MagdalenaWolska,WaltDetmarMeur-
ablediscussions. AndreasOpedalacknowledgesfunding
ers,andHans-ChristophNuerk.2015. Wordproblems: a
from the Max Planck ETH Center for Learning Systems.
reviewoflinguisticandnumericalfactorscontributingto
AlessandroStolfoissupportedbyarmasuisseScienceand
theirdifficulty. FrontiersinPsychology,6.
TechnologythroughaCYDDoctoralFellowship.
Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y.
References Chan,HannahR.Sheahan,AntoniaCreswell,Dharshan
Kumaran, James L. McClelland, and Felix Hill. 2023.
GatiAher,RosaI.Arriaga,andAdamTaumanKalai.2023. Language models show human-like content effects on
Usinglargelanguagemodelstosimulatemultiplehumans reasoningtasks.
andreplicatehumansubjectstudies. InProceedingsof
the40thInternationalConferenceonMachineLearning, JesseDodge,MaartenSap,AnaMarasovic´,WilliamAgnew,
ICML’23.JMLR.org. GabrielIlharco,DirkGroeneveld,MargaretMitchell,and
MattGardner.2021. Documentinglargewebtextcorpora:
RisakoAndo,TakanobuMorishita,HirohikoAbe,KojiMi- Acasestudyonthecolossalcleancrawledcorpus.InPro-
neshima,andMitsuhiroOkada.2023. Evaluatinglarge ceedingsofthe2021ConferenceonEmpiricalMethods
languagemodelswithNeuBAROCO:Syllogisticreason- inNaturalLanguageProcessing,pages1286–1305,On-
ingabilityandhuman-likebiases. InProceedingsofthe lineandPuntaCana,DominicanRepublic.Association
4th Natural Logic Meets Machine Learning Workshop, forComputationalLinguistics.
pages 1–11, Nancy, France. Association for Computa-
NouhaDziri,XimingLu,MelanieSclar,XiangLorraineLi,
tionalLinguistics.
LiweiJiang,BillYuchenLin,SeanWelleck,PeterWest,
JacobAndreas.2022. Languagemodelsasagentmodels. ChandraBhagavatula,RonanLeBras,JenaD.Hwang,
In Findings of the Association for Computational Lin- SoumyaSanyal,XiangRen,AllysonEttinger,ZaidHar-
guistics: EMNLP2022,pages5769–5779,AbuDhabi, chaoui, and Yejin Choi. 2023. Faith and fate: Limits
United Arab Emirates. Association for Computational oftransformersoncompositionality. InThirty-seventh
Linguistics. ConferenceonNeuralInformationProcessingSystems.
Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R. Tiwalayo Eisape, MH Tessler, Ishita Dasgupta, Fei Sha,
Gubler,ChristopherRytting,andDavidWingate.2023. SjoerdvanSteenkiste,andTalLinzen.2023. Asystem-
Outofone, many: Usinglanguagemodelstosimulate aticcomparisonofsyllogisticreasoninginhumansand
humansamples. PoliticalAnalysis,31(3):337–351. languagemodels.
MarkH.Ashcraft,RickD.Donley,MargaretA.Halas,and Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhi-
Mary Vakali. 1992. Chapter 8 working memory, auto- lasha Ravichander, Dustin Schwenk, Alane Suhr, Pete
maticity,andproblemdifficulty. InJamieI.D.Campbell, Walsh,DirkGroeneveld,LucaSoldaini,SameerSingh,
editor, TheNatureandOriginsofMathematicalSkills, et al. 2023. What’s in my big data? arXiv preprint
volume91ofAdvancesinPsychology,pages301–329. arXiv:2310.20707.
North-Holland.
Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid
Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob
MarcelBinzandEricSchulz.2023. Usingcognitivepsy-
chologytounderstandgpt-3. ProceedingsoftheNational Eisenstein, Justin Grimmer, Roi Reichart, Margaret E.
AcademyofSciences,120(6). Roberts, Brandon M. Stewart, Victor Veitch, and Diyi
Yang.2022. Causalinferenceinnaturallanguageprocess-
DianeJ.BriarsandJillH.Larkin.1984.Anintegratedmodel ing: Estimation, prediction, interpretation and beyond.
ofskillinsolvingelementarywordproblems. Cognition TransactionsoftheAssociationforComputationalLin-
andInstruction,1(3):245–296. guistics,10:1138–1158.
10DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
AnsgarJ.FürstandGrahamJ.Hitch.2000. Separateroles ThéophileGervet,ThibautLavril,ThomasWang,Tim-
forexecutiveandphonologicalcomponentsofworking othéeLacroix,andWilliamElSayed.2024. Mixtralof
memory in mental arithmetic. Memory & Cognition, experts.
28(5):774–782.
YingJiao,KumarShridhar,PengCui,WangchunshuZhou,
SaeedGoodarzi,NikhilKagita,DennisMinn,ShufanWang, and Mrinmaya Sachan. 2023. Automatic educational
Roberto Dessi, Shubham Toshniwal, Adina Williams, questiongenerationwithdifficultylevelcontrols. InIn-
JackLanchantin,andKoustuvSinha.2023. Robustness ternationalConferenceonArtificialIntelligenceinEdu-
ofnamed-entityreplacementsforin-contextlearning. In cation,pages476–488.Springer.
FindingsoftheAssociationforComputationalLinguis-
tics: EMNLP2023,pages10914–10931,Singapore.As- ZhijingJin,YuenChen,FelixLeeb,LuigiGresele,Ojasv
sociationforComputationalLinguistics. Kamal,ZhihengLYU,KevinBlin,FernandoGonzalez
Adauto,MaxKleiman-Weiner,MrinmayaSachan,and
Silke M. Göbel, Korbinian Moeller, Silvia Pixner, Liane BernhardSchölkopf.2023. CLadder: Abenchmarkto
Kaufmann,andHans-ChristophNuerk.2014. Language assesscausalreasoningcapabilitiesoflanguagemodels.
affectssymbolicarithmeticinchildren: Thecaseofnum- InThirty-seventhConferenceonNeuralInformationPro-
berwordinversion. JournalofExperimentalChildPsy- cessingSystems.
chology,119:17–25.
ZhijingJin,JiaruiLiu, ZhihengLyu, SpencerPoff, Mrin-
MaryHegarty,RichardE.Mayer,andChristopherA.Monk.
mayaSachan,RadaMihalcea,MonaDiab,andBernhard
1995. Comprehension of arithmetic word problems:
Schölkopf.2024. Canlargelanguagemodelsinfercau-
A comparison of successful and unsuccessful problem
sation from correlation? In The Twelfth International
solvers. JournalofEducationalPsychology,87:18–32.
ConferenceonLearningRepresentations.
Graham J Hitch. 1978. The role of short-term working
PhilipNicholasJohnson-Laird.1983. Mentalmodels: to-
memory in mental arithmetic. Cognitive Psychology,
wards a cognitive science of language, inference and
10(3):302–323.
consciousness, volume 6 of Cognitive Science Series.
HarvardUniversityPress,Cambridge,Massachusetts.
TomHudson.1983. Correspondencesandnumericaldiffer-
encesbetweendisjointsets. ChildDevelopment,54:84–
WalterKintschandJamesG.Greeno.1985. Understanding
90.
and solving word arithmetic problems. Psychological
Guido W. Imbens and Donald B. Rubin. 2015. Causal
Review,92(1):109–129.
InferenceforStatistics,Social,andBiomedicalSciences:
TakeshiKojima,ShixiangShaneGu,MachelReid,Yutaka
AnIntroduction. CambridgeUniversityPress,USA.
Matsuo, and Yusuke Iwasawa. 2022. Large language
Joshua Benjamin Jaffe and Donald Joseph Bolger. 2023. modelsarezero-shotreasoners. InICML2022Workshop
Cognitive processes, linguistic factors, and arithmetic onKnowledgeRetrievalandLanguageModels.
word problem success: a review of behavioral studies.
EducationalPsychologyReview,35(4):105. RikKoncel-Kedziorski,IoannisKonstas,LukeZettlemoyer,
andHannanehHajishirzi.2016. Atheme-rewritingap-
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, proachforgeneratingalgebrawordproblems.InProceed-
ChrisBamford,DevendraSinghChaplot,Diegodelas ings of the 2016 Conference on Empirical Methods in
Casas, Florian Bressand, Gianna Lengyel, Guillaume NaturalLanguageProcessing,pages1617–1628,Austin,
Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie- Texas.AssociationforComputationalLinguistics.
Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut
Lavril,ThomasWang,TimothéeLacroix,andWilliamEl John W. Koshmider and Mark H. Ashcraft. 1991. The
Sayed.2023. Mistral7b. development of children’s mental multiplication skills.
JournalofExperimentalChildPsychology,51(1):53–89.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,
ArthurMensch,BlancheSavary,ChrisBamford,Deven- SangKwon,GaganBhatia,ElMoatezBillahNagoudi,and
draSinghChaplot,DiegodelasCasas,EmmaBouHanna, MuhammadAbdul-Mageed.2023.BeyondEnglish:Eval-
FlorianBressand,GiannaLengyel,GuillaumeBour,Guil- uatingLLMsforArabicgrammaticalerrorcorrection. In
laume Lample, Lélio Renard Lavaud, Lucile Saulnier, ProceedingsofArabicNLP2023,pages101–119,Singa-
Marie-AnneLachaux,PierreStock,SandeepSubrama- pore(Hybrid).AssociationforComputationalLinguis-
nian, Sophia Yang, Szymon Antoniak, Teven Le Scao, tics.
11DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
TanjaKäserandGioraAlexandron.2023. Simulatedlearn- LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,Car-
ers in educational technology: A systematic literature rollWainwright,PamelaMishkin,ChongZhang,Sand-
reviewandaturing-liketest. InternationalJournalOf hini Agarwal, Katarina Slama, Alex Gray, John Schul-
ArtificialIntelligenceInEducation. man,JacobHilton,FraserKelton,LukeMiller,Maddie
Simens,AmandaAskell,PeterWelinder,PaulChristiano,
AnneBovenmyerLewisandRichardE.Mayer.1987. Stu-
Jan Leike, and Ryan Lowe. 2022. Training language
dents’miscomprehensionofrelationalstatementsinarith-
modelstofollowinstructionswithhumanfeedback. In
meticwordproblems.JournalofEducationalPsychology,
AdvancesinNeuralInformationProcessingSystems.
79:363–371.
Jean Piaget. 1936. La naissance de l’intelligence chez
MengsayLoem,MasahiroKaneko,ShoTakase,andNaoaki l’enfant. DelachauxetNiestlé.
Okazaki. 2023. Exploring effectiveness of GPT-3 in
grammaticalerrorcorrection: Astudyonperformance Oleksandr Polozov, Eleanor O’Rourke, Adam M. Smith,
andcontrollabilityinprompt-basedmethods. InProceed- LukeZettlemoyer,SumitGulwani,andZoranPopovic.
ingsofthe18thWorkshoponInnovativeUseofNLPfor 2015. Personalizedmathematicalwordproblemgenera-
Building Educational Applications (BEA 2023), pages tion. InIJCAI.
205–219, Toronto, Canada. Association for Computa-
PhilipQuirkeandFazlBarez.2024.Understandingaddition
tionalLinguistics.
intransformers. InTheTwelfthInternationalConference
Jakub Macina, Nico Daheim, Sankalan Chowdhury, Tan- onLearningRepresentations.
maySinha,ManuKapur,IrynaGurevych,andMrinmaya
YasamanRazeghi,RobertLLoganIV,MattGardner,and
Sachan.2023.MathDial:Adialoguetutoringdatasetwith
SameerSingh.2022. Impactofpretrainingtermfrequen-
richpedagogicalpropertiesgroundedinmathreasoning
cies on few-shot numerical reasoning. In Findings of
problems. In Findings of the Association for Compu-
theAssociationforComputationalLinguistics: EMNLP
tational Linguistics: EMNLP 2023, pages 5602–5621,
2022,pages840–854,AbuDhabi,UnitedArabEmirates.
Singapore.AssociationforComputationalLinguistics.
AssociationforComputationalLinguistics.
Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy
MaryRiley,JamesGreeno,andJoanHeller.1983. Develop-
Kanwisher, Joshua B Tenenbaum, and Evelina Fe-
mentofChildren’sProblem-SolvingAbilityinArithmetic,
dorenko. 2023. Dissociating language and thought in
page153–196.LearningResearchandDevelopmentCen-
largelanguagemodels: acognitiveperspective. arXiv,
ter,UniversityofPittsburgh.
arXiv:2301.06627.
AbulhairSaparovandHeHe.2023. Languagemodelsare
NeelNanda,LawrenceChan,TomLieberum,JessSmith,
greedyreasoners: Asystematicformalanalysisofchain-
and Jacob Steinhardt. 2023. Progress measures for
of-thought. In International Conference on Learning
grokkingviamechanisticinterpretability.InTheEleventh
Representations.
InternationalConferenceonLearningRepresentations.
OmarShaikh,HongxinZhang,WilliamHeld,MichaelBern-
P.Nesher,JamesG.Greeno,andMaryS.Riley.1982. The stein,andDiyiYang.2023. Onsecondthought,let’snot
developmentofsemanticcategoriesforadditionandsub- think step by step! bias and toxicity in zero-shot rea-
traction. EducationalStudiesinMathematics, 13:373– soning. In Proceedings of the 61st Annual Meeting of
394. theAssociationforComputationalLinguistics(Volume
1: Long Papers), pages 4454–4470, Toronto, Canada.
Perla Nesher and Eva Teubal. 1975. Verbal cues as an
AssociationforComputationalLinguistics.
interferingfactorinverbalproblemsolving. Educational
StudiesinMathematics,6(1):41–51. RuoqiShen,SébastienBubeck,RonenEldan,YinTatLee,
YuanzhiLi,andYiZhang.2023. Positionaldescription
Manh Hung Nguyen, Sebastian Tschiatschek, and Adish
mattersfortransformersarithmetic.
Singla.2023. Largelanguagemodelsforin-contextstu-
dentmodeling: Synthesizingstudent’sbehaviorinvisual ElsbethStern.1993. Whatmakescertainarithmeticword
programmingfromone-shotobservation. problemsinvolvingthecomparisonofsetssodifficultfor
children? JournalofEducationalPsychology,85:7–23.
Andreas Opedal, Niklas Stoehr, Abulhair Saparov, and
MrinmayaSachan.2023. Worldmodelsformathstory Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya
problems. InFindingsoftheAssociationforComputa- Sachan. 2023a. A mechanistic interpretation of arith-
tionalLinguistics: ACL2023,pages9088–9115,Toronto, meticreasoninginlanguagemodelsusingcausalmedi-
Canada.AssociationforComputationalLinguistics. ationanalysis. InProceedingsofthe2023Conference
12DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
onEmpiricalMethodsinNaturalLanguageProcessing, Correctallgrammaticalmistakesthatappearinthefol-
pages7035–7052,Singapore.AssociationforComputa- lowingmathwordproblem: [templated text]
tionalLinguistics. Fixanyawkwardorredundantphrasing. Paycloseatten-
tiontoincorrectpluralforms.DoNOTsolvetheproblem.
AlessandroStolfo,ZhijingJin,KumarShridhar,Bernhard
DoNOTcomputeanyintermediatesolutions. DoNOT
Schoelkopf, and Mrinmaya Sachan. 2023b. A causal
make any changes to the numerical values or implied
framework to quantify the robustness of mathematical
mathematicaloperations.Onlyoutputthecorrectedmath
reasoningwithlanguagemodels. InProceedingsofthe
wordproblemandnothingelse.DoNOTrestatetheorig-
61st Annual Meeting of the Association for Computa-
inalproblem. DoNOTinclude"CorrectedVersion:"or
tionalLinguistics(Volume1: LongPapers),pages545– anydescriptionofthetask.
561, Toronto, Canada. Association for Computational
Linguistics.
Table3. Promptusedforthelinguisticerrorcorrectionstepinour
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, generationpipelinefrom§4.2.
AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,Dan CATE by Number of Steps (Carry)
Bikel,LukasBlecher,CristianCantonFerrer,MoyaChen,
LLaMA2 7B LLaMA2 13B
GuillemCucurull,DavidEsiobu,JudeFernandes,Jeremy 0.2 0.2
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj 0.1 0.1
E
Goswami, Naman Goyal, Anthony Hartshorn, Saghar AT 0 0
C
Hosseini,RuiHou,HakanInan,MarcinKardas,Viktor −0.1 −0.1
Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko- −0.2 1 1.5 2 2.5 3 −0.2 1 1.5 2 2.5 3
renev,PunitSinghKoura,Marie-AnneLachaux,Thibaut
Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yun- 0.2 Mistral 7B 0.2 Mixtral 8x7B
ing Mao, Xavier Martinet, Todor Mihaylov, Pushkar 0.1 0.1
E
Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, AT 0 0
C
JeremyReizenstein,RashiRungta,KalyanSaladi,Alan −0.1 −0.1
Schelten,RuanSilva,EricMichaelSmith,RanjanSub- −0.2 −0.2
1 1.5 2 2.5 3 1 1.5 2 2.5 3
ramanian,XiaoqingEllenTan,BinhTang,RossTaylor, Number of Reasoning Steps Number of Reasoning Steps
AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,
Base Model Instruction-tuned
IliyanZarov,YuchenZhang,AngelaFan,MelanieKam-
badur,SharanNarang,AurelienRodriguez,RobertSto-
Figure4.CATEforthecarryeffect(§5.4)inCoT-promptedmodels
jnic,SergeyEdunov,andThomasScialom.2023. Llama
stratifiedbynumberofreasoningsteps.‘Instruction-tuned’refers
2: Openfoundationandfine-tunedchatmodels. totheChatandInstructversionsofLLaMA2andMistral/Mixtral,
respectively.
KurtVanLehn,StellanOhlsson,andRodNason.1994. Ap-
plicationsofsimulatedstudents: Anexploration. Inter-
nationalJournalofArtificialIntelligenceinEducation,
A.DetailsonLinguisticErrorCorrection
5.
WeuseGPT-3.5Turbotocarryoutthelinguisticerrorcor-
Santiago.Vicente,Jose.Orrantia,andLieven.Verschaffel.
rection step detailed in §4.2. In Table 3, we provide the
2007. Influenceofsituationalandconceptualrewording
exactpromptusedforthetask. Thecorrectedproblemis
onwordproblemsolving. BritishJournalofEducational
generatedusinggreedydecoding(temperature=0). We
Psychology,77(4):829–848.
carryoutadditionalintegritychecksofthegeneratedprob-
lem against the original templated text. In particular, we
AlexWarstadtandSamuelRBowman.2022. Whatartifi-
verifythatthesentencecountandrelationalterms(suchas
cialneuralnetworkscantellusabouthumanlanguage
"more")areconsistentpost-errorcorrection. Theproblem
acquisition. InAlgebraicStructuresinNaturalLanguage,
isdiscardediftheseadditionalchecksarenotsatisfied.
pages17–60.CRCPress.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten B.DataQualityEvaluation
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
andDennyZhou.2022. Chainofthoughtpromptingelic- Wedescribethemanualevaluationofthedatasetsgenerated
itsreasoninginlargelanguagemodels. InAdvancesin in§5. Foreachofthedatasets,wedothefollowing: First,
NeuralInformationProcessingSystems. generateacontrolsetof10examples. These10examples
areevaluatedindependentlybythreeofthispaper’sauthors.
13DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
CATE by Number of Steps (Consistency) Thecarryeffect(§5.4). Weevaluateddataqualityaccord-
ingtowhetheroneproblemhadnocarrycomputationsteps,
LLaMA2 7B LLaMA2 13B
theotheronehadatleastone,andtheywereequalotherwise.
0.4 0.4 Thatis,onlythenumbersdifferedandthetwoproblemshad
E
T
A thesameanswer. Ourpipelineachieveda0%errorrateon
C 0.2 0.2
bothcriteriaonthe100evaluatedexampleproblems.
0 0
1 2 3 4 5 1 2 3 4 5
C.AdditionalResults
Mistral 7B Mixtral 8x7B
0.4 0.4 C.1.Child-PersonaPrompting
E
T
A
C 0.2 0.2 Weinvestigatewhethervaryingthepromptingstrategycould
0 0 influencethemodel’sbiases.Specifically,weemployamod-
1 2 3 4 5 1 2 3 4 5
ifiedversionofthezero-shotchain-of-thoughtprompt,tai-
Number of Reasoning Steps Number of Reasoning Steps
loredtosimulateachild’sreasoningprocess.Wepromptthe
Base Model Instruction-tuned modelwiththephrase“Let’sthinkstepbystepasagrade-
schoolchildwould,”replacingthestandardCoTinstruction.
Figure5.CATEfortheconsistencyeffect(§5.2)inCoT-prompted
Followingthis,weapplythesamedecodingmethodusedin
modelsstratifiedbynumberofreasoningsteps.‘Instruction-tuned’
traditionalCoT.Theresultsforthisapproacharereportedin
refers to the Chat and Instruct versions of LLaMA2 and Mis-
Table4. Whilewenoticelargerconsistencyandtransfervs
tral/Mixtral,respectively.
comparisoneffectsforsomemodels,weobservenosubstan-
tialdeparturefromtheresultsachievedwithconventional
CoTprompting.
Ifthereareanyerrorswemakeappropriatemodifications
tothepipelineandrestarttheprocedure. Ifnot,weproceed
C.2.BiasStrengthbyNumberofReasoningSteps
toevaluate90moreexamples,allocating30toeachofthe
threeauthors. Errorrateisestimatedonthissampleof100
Figs. 4 and 5 illustrate how the strength of the measured
examples.
biases(consistencyandcarry,respectively)changeinrela-
We use two binary evaluation criteria, one assessing the tiontothenumberofreasoningstepsinaproblem(inthe
linguisticerrorcorrectionstep(iv)andoneassessingtest- CoT-promptedcase). Unlikeforthetransfervscomparison
specificattributes. Aproblemisdeemedtobegoodaccord- bias (§5.3), we do not observe a consistent difference in
ing to the former if the generated problem only deviates the trend between pre-trained-only and instruction-tuned
fromthetemplatedtextthroughspellingorgrammarcor- models.
rection. The test-specific criterion and the obtained error
estimatesaregivenbelow.
Consistency bias (§5.2). We evaluated data quality ac-
cordingtowhetherthetwocomparisonstatementsactually
wereconsistentandinconsistentformstoexpressthesame
comparisonrelationship. Tobeprecise, thefirstproblem
needstohaveaconsistentrelationalstatementforthecom-
parison predicate, the second problem needs to have an
equivalent inconsistent relational statement for the same
comparison predicate, and the two problems need to be
identicalotherwise. Ourpipelineachieveda0%errorrate
onbothcriteriaonthe100evaluatedexampleproblems.
Transfervscomparisonbias(§5.3). Weevaluateddata
qualityaccordingtowhetherthecomparisonandtransfer
problemshadthesamesymbolicexpressionandwhether
they followed the specified problem structure. We also
ensuredthattheagentnamesandotherpropertiesmatched.
Ourpipelineachieveda0%errorrateonbothcriteriaonthe
100evaluatedexampleproblems.
14DoLanguageModelsExhibittheSameCognitiveBiasesinProblemSolvingasHumanLearners?
Consistencybias(§5.2) Transfervscomparisonbias(§5.3) Carryeffect(§5.4)
Mode Model Accuracy(%) Accuracy(%) Accuracy(%)
p-val p-val p-val
Co InCo CATE T C CATE NCa Ca CATE
LLaMA27B 14.6 5.0 9.6 <0.001 19.8 11.6 8.2 <0.001 40.0 44.4 -4.4 0.048
LLaMA213B 20.0 5.0 15.0 <0.001 35.0 7.0 28.0 <0.001 20.8 21.0 -0.2 0.903
Mistral7B 39.2 17.2 22.0 <0.001 48.8 26.6 22.2 <0.001 58.4 57.2 1.2 0.415
Mixtral8x7B 66.2 34.6 31.6 <0.001 69.8 49.4 20.4 <0.001 70.4 69.4 1.0 0.701
ChildCoT
LLaMA27BChat 55.2 24.2 31.0 <0.001 62.6 33.8 28.8 <0.001 67.8 63.0 4.8 0.069
LLaMA213BChat 65.2 27.0 38.2 <0.001 79.8 48.2 31.6 <0.001 80.0 77.0 3.0 0.108
Mistral7BInstr. 65.0 30.6 34.4 <0.001 75.2 52.8 22.4 <0.001 77.2 74.4 2.8 0.178
Mixtral8x7BInstr. 88.6 72.4 16.2 <0.001 98.8 82.4 16.4 <0.001 97.6 97.4 0.2 0.809
Table4.Accuracy,conditionalaveragetreatmenteffect(CATE),andstatisticalsignificance(p-value)onmathwordproblemsgenerated
forthethreetestsdetailedin§5.2,§5.3and§5.4.‘Co’denotesconsistent,‘InCo’inconsistent,‘T’transfer,‘C’comparison,‘Ca’carry,
and‘NCa’nocarryconditions.Theresultspresentedareforthechild-personapromptingstrategydescribedinFootnote12andApp.C.1.
‘Chat’and‘Inst.’indicatetheinstruction-tunedversionsofthemodels.CATEvaluesareboldedwhenp<0.01.
15