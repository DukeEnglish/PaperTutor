Multipath parsing in the brain
BertaFranzluebbers DonaldDunagan MilošStanojevic´
UniversityofGeorgia UniversityofGeorgia GoogleDeepMind
berta@uga.edu dgd45125@uga.edu stanojevic@google.com
JanBuys JohnT.Hale
UniversityofCapeTown UniversityofGeorgia
jbuys@cs.uct.ac.za jthale@uga.edu
Abstract
Humansunderstandsentencesword-by-word,
intheorderthattheyhearthem. Thisincremen-
talityentailsresolvingtemporaryambiguities Figure1: Asentencefragmentfromourstimulustext
about syntactic relationships. We investigate showingtemporarysyntacticambiguityaboutthecor-
howhumansprocessthesesyntacticambigui- rectrelationshipbetweenTheanddesert,whichisre-
tiesbycorrelatingpredictionsfromincremen- solvedbyhearingthenextword(blue).
tal generative dependency parsers with time-
coursedatafrompeopleundergoingfunctional andPearlmutter,2000)? Ifitis,whereinthebrain
neuroimagingwhilelisteningtoanaudiobook. doessuchmultipathparsinghappen?
Inparticular,wecomparecompetinghypothe-
We take up the call to answer this question
ses regarding the number of developing syn-
in a linguistically-transparent way by combining
tactic analyses in play during word-by-word
broad coverage incremental parsing (e.g. Nivre,
comprehension: one vs more than one. This
comparisoninvolvesevaluatingsyntacticsur- 2004)withinformation-theoreticalcomplexitymet-
prisalfromastate-of-the-artdependencyparser ricstoderivepredictionsaboutneuroimaging. An-
withLLM-adaptedencodingsagainstanexist- alyzing an entire book (Li et al., 2022) makes it
ingfMRIdataset. InbothEnglishandChinese possibletoexamine–forthefirsttime–allofthe
data, we find evidence for multipath parsing.
ambiguitiesthatareimplicatedbyagivenconcep-
Brainregionsassociatedwiththismultipathef-
tion of syntax and a given parsing strategy – not
fectincludebilateralsuperiortemporalgyrus.
justthoseattestedinclassicgardenpathmaterials
(Masonetal.,2003;Hopfetal.,2003;Roddetal.,
1 Introduction
2010).
Amajorunsolvedproblemincomputationalpsy- We extend the state of the art in incremental
cholinguisticsisdeterminingwhetherhumansen- generative dependency parsing by using a large
tence comprehension considers a single analy- languagemodel(BLOOM;LeScaoetal.,2022b)
sispath1 atatime,orwhetheritsometimesenter- togetherwithparameterefficientfine-tuningusing
tainsmultiplelinesofreasoningaboutthestructure adapters (Pfeiffer et al., 2020). We fix the pars-
of a sentence. These lines of reasoning typically ing strategy while varying the number of allow-
correspondtosyntacticambiguities(e.g. between able analysis paths and connect neural data with
SubjectandModifierasshowninFigure1). Mul- parser actions via surprisal (for a review of this
tipathaccountsofambiguityresolution(e.g.Gib- information-theoretical metric see Hale, 2016 or
son,1991;Jurafsky,1996)arequitedifferentfrom neuroimagingstudiessuchasBrennanetal.,2016;
single-pathaccounts(e.g.FrazierandFodor,1978; Hendersonetal.,2016;Shainetal.,2020;Brennan
Marcus, 1980). The debate within cognitive sci- et al., 2020). This sets up a contrast between a
encehasfocusedonthespecialcaseofgardenpath multipathmodelthatconsiders(atmost)fivepaths
sentences leaving open the broader question: Is atatime,versusasingle-pathmodelthatcanonly
humancomprehensionevermultipathinthesense consider one at a time. The results, reported in
ofRankedParallelparsing(Lewis,2000a;Gibson section5,ultimatelysupportthemultipathview,as
thesurprisalsfromthefive-wayparserarebetter-
1The“paths”terminology(vs“serial”or“parallel”)serves
correlatedwiththeneuroimagingdata. Thisobtains
tocategorizethecognitiveissueasoneofprocessratherthan
architecture(seeLewis,2000b,§3.3.3). forbothEnglishandChinese.
4202
naJ
13
]LC.sc[
1v64081.1042:viXraAction Before After Arc Probability
Shift (σ|i,j) (σ|i|j,j +1) - p (sh|h ,h )p (w |h ,h )
tr i j gen j+1 i j
Left-arc (σ|i,j) (σ,j) j → i p (re|h ,h )p (la|h ,h )
tr i j dir i j
Right-arc (σ|l|i,j) (σ|l,j) l → i p (re|h ,h )p (ra|h ,h )
tr i j dir i j
Table 1: The arc-hybrid transition system of Kuhlmann et al. (2011) defines the possible parser actions (shift,
left-arc,andright-arc)astransitionsfromprevioustocurrentparserstates. Statesareindicatedby(stack,current
index)tuples,e.g. w isthetokenontopofthestack(σ),andw isthecurrentindextokenattimestepj. The
i j
probabilitiesassociatedwiththeseactionsaredecomposedintocomplementaryshift(sh)andreduce(re)transitions,
andcomplementaryleft-arc(la)andright-arc(ra)arcdirections. Theshiftactionalsoincludestheprobabilityof
generatingthenexttokenw .
j+1
2 RelatedWork ThisallowsLLMstocapturedistributionalregular-
itiesatavastscale. However,itischallengingto
Thispaperfollowsalineofresearchwhichaimsto explainatanalgorithmiclevelwhatthesemodels
characterizehumanlanguageprocessingbyevalu- aredoing.
atingword-by-worddifficultypredictionsagainst Instead of deriving a processing complexity
neuroimaging data from the brain. Hale et al. metric directly from the output of a large lan-
(2022)reviewsmanystudiesthatfitintothistradi- guagemodel,orcorrelatingnextwordpredictions
tion. Mostofthemconsiderjustasinglegoldstan- with fMRI data, as in Schrimpf et al. (2021) and
dardorsinglesystem-assignedanalysis. Haleetal. CaucheteuxandKing(2022),thisworkusesinitial
(2018)andCrabbéetal.(2019)arenotableexcep- substring encodings from an LLM to inform an
tionstothisgeneraltrendbecausetheyderivepre- incremental parser. Our project also differs from
dictionsfrommultipleanalysesthatwouldbecon- Eisape et al. (2022), who probe LLM represen-
sideredaspartofabeam. HaleandCrabbéwork tations to arrive at incremental unlabeled depen-
withphrasestructure. Bycontrast,thepresentstudy dencyanalyses. Incontrasttotheirgoalofinves-
usesdependencyparsing. Thischoiceismotivated tigatingthesyntacticpropertiesofLLMs,weuse
bypriorworkinneurolinguistics(e.g.Bornkessel- LLMrepresentationsasastartingpointforaparser
SchlesewskyandSchlesewsky,2015). Forinstance, which builds the set of all possible analyses, in-
Li and Hale (2019) relates a dependency-based creasing accuracy. Our scientific goal is to relate
structuraldistancemetrictohemodynamicactivity suchparserstatestohumanbrainactivity.
inleftposteriortemporallobe,amongotherbrain
areas. TheirstudyusedspokenEnglishmaterials. 3 DependencyParsing
Lopopolo et al. (2021) apply a related metric to
Dutch. Oota et al. (2023) use graph neural net- 3.1 ParserArchitecture
workstoembeddependencyanalysesofsentence-
Theconstructionofourdependencyparserrelieson
initialsubstrings. Usingtheencodingapproachof
theworkofBuysandBlunsom(2018),whichem-
ReddyandWehbe(2021)andwrittenstimuli,this
ploysthetransitionbasedparsingsystemdetailed
workidentifiesmanyofthesamebrainareasasthe
inNivre(2008)andexactlyenumeratesallparser
studiesmentionedabove.
paths. FollowingKuhlmannetal.(2011),dynamic
The single-path vs multipath question is itself programmingisusedtosumoverallpaths. Weuse
motivatedbypriorworkwitheyetrackingdata. By thearc-hybridtransitionsystemshowninTable1
varyingthenumberofpathsavailabletoaparser, because it showed good parsing performance in
Bostonetal.(2011)findsupportforRankedParal- BuysandBlunsom(2018). Thegenerativeparser
lelparsing(inthesenseofGibson1991andJuraf- assignsaprobabilitytoeachofthewordsinthesen-
sky1996,furthercharacterizedbelowin§3.4). tenceincrementally,similartoalanguagemodel,
Apart from these cognitive considerations, a incontrasttodiscriminativeparserswhichonlypre-
complementary motivation for this research is to dicttransitionactions. Theshiftactionpredictsthe
investigatetheparsingimprovementsthatcomeby nextwordonthebuffer,whichcorrespondstothe
leveraginglargelanguagemodels(LLMs;seeac- buffer-nextmodelinBuysandBlunsom(2018).
curacyscoresinTable2). Suchmodelsaretrained WeupdateBuysandBlunsombyencodingse-
ondatasetsthatfarexceedclassictreebanksinsize. quencesnotwithanLSTMbutwithrepresentationsfrom the BigScience Large Open-science Open-
access Multilingual Language Model (BLOOM;
Le Scao et al., 2022b), chosen based on its open-
accessstatusandtrainingonbothEnglishandChi-
nesedata. TheseBLOOMrepresentationsencode
an entire initial substring up to and including a
particularword(orsubwordtoken). Asetofclassi-
fiersoverthesesubstringrepresentationsestimates
theprobabilityofeachtransition,wordandarcla-
bel. Theclassifierscorrespondtothesubscripted
probabilitiesintherightmostcolumnofTable1.
We use the pre-trained BLOOM-560 model.
LargerBLOOMmodelsshowednosignificantin-
creaseinaccuracy. Thiscorroboratestheworkof
OhandSchuler(2023)andPasquiouetal.(2023), Figure2: Parserinputintheformof(stack(σ),current
index(β))tuplesispre-processedbyaddingaleading
whocorrelatelargelanguagemodelencodingsto
space to all pretokenized text from the corpus. The
humanreadingtimesandfMRIdata,respectively,
BLOOMmodelispre-trainedandfinetunedusingthe
andfindthatsmallermodelsprovideanequal(or
PfeifferAdapter,andthesystemofclassifiersistrained
better)fittothehumandata. from random weights. Note that the word prediction
Instead of fine-tuning the entire pre-trained classifierpredictsoverthesizeofthetrainingdatavo-
BLOOMmodelondependencyparsing,weapply cabulary + Berkeley unknown tokens instead of the
BLOOMvocabulary.
a Pfeiffer adapter after each layer (Pfeiffer et al.,
2020). Thisbottleneckadapterintroducesanewlin-
earlayerwhichreducesthedimensionfrom1024
on section 23. The Chinese model is trained on
down to 64 and back up to 1024, for input into
ChineseTreebankversion7,withthestandardsplit
the next pre-trained BLOOM layer. The adapter
of files 0-2082 for training, development on files
approach enables parameter-efficient fine-tuning
2083-2242, and testing on files 2243-2447. This
whilepreservingtheoriginallanguagemodelling
resultsin39,832trainingsentencesforEnglishand
representationsasmuchaspossibleinthegenera-
19,457trainingsentencesforChinese.
tiveparser.
The parser’s next word prediction classifier
The goal of these design choices is to deliver predicts over a limited vocabulary, where words
linguistically plausible dependency analyses of seen only once in the training data are replaced
sentence-initial substrings. Some aspects of the withunknownwordtokensaccordingtotherules
overall architecture play the role of auxiliary hy- in the Berkeley Parser. This results in 23,830
potheses that do not map on to the brain. As word types for English and 19,671 for Chinese.
subsection 3.5 lays out in further detail, the cog- The LLM encoder, however, uses the BLOOM
nitive claim is limited to proposals that (a) hu- sub-word tokenizer and the BLOOM vocabulary,
manparsinginvolvesrecognizingdependencyrela- whichisofsize250,680.2
tionsviaaschemalikeTable1and(b)surprising
Token sequences are encoded by the LLM, se-
parseractions,withinthisschema,callforgreater
lecting the encoding of the right-most sub-word
hemodynamicresourcesthandounsurprisingones.
of each word for parser predictions. This allows
Proposal (a) is situated at Marr’s middle level of
sequence encodings to be as detailed as possible
analysis(Marr,1982).
whilestilllimitingthevocabularyduringtraining.
It should be noted, though, that using this encod-
3.2 ParserTraining ing for words that are unknown to the parser’s
nextwordclassifierpreventstheparserfrombeing
The parser is trained on English and Chinese
strictly generative. More information on strictly
treebanksannotatedwithStanfordDependencies
generativemodelscanbefoundintheappendix.
(De Marneffe and Manning, 2008). The English
While training, sentences are shuffled at each
modelistrainedonthePennTreebank(PTB)ver-
sion 3, with the standard split of training on sec-
2Thisvocabularysizemismatchisonereasonwedidnot
tions02-21,developmentonsection22,andtesting directlyassignnext-wordprobabilitiesusingtheLLM.Dev Test
Corpus Model
LAS UAS LAS UAS
PTB-3 BuysandBlunsom(2018) 88.66 91.19 88.54 91.01
PTB-3 EnglishBLOOM(brainanalysis) 90.26 92.71 90.32 92.62
CTB-7 ChineseBLOOM(brainanalysis) 77.07 83.65 74.71 81.66
Table2: Labeledattachmentscore(LAS),Unlabeledattachmentscore(UAS),andLabelaccuracyscoreforthe
EnglishPTBcorpusandChineseCTBcorpus
epoch,andwithinbatches(batchsize=16),which dings)inordertopredictdependencyrelations,for
arecreatedfromsentencesofthesamelength. The the purposes of creating a cognitively plausible
BLOOM-560 model has 24 hidden layers, after model,strictlyincrementalmethodsmustbeused
encoding. withoutincludingusefulinformationfromwords
occurring later in the sentence. We compare the
accuracyoftheincrementalgenerativeparserpro-
posed here to that of Buys and Blunsom (2018),
whichemploysthesametransitionsystemwithen-
codings based on an LSTM with random initial
weights.
Parser accuracy is evaluated on the develop-
mentsetaftereachepoch,andthehighestscoring
epoch is reported in Table 2. However, the cog-
nitivemodelingresultsreportedinSection5rely
onearliertrainingcheckpoints. Theepochischo-
sen by maximizing r2 correlation with the fMRI
data. Thelabeledattachmentscoreandunlabeled
Figure3: LabeledAccuracyScore(LAS),Unlabeled attachmentscore(LAS/UAS)fortheepochscho-
AccuracyScore(UAS),LabelAccuracyforourparser senare89.75/92.37forEnglish,and77.06/83.59
trainedandevaluated(devset)ontheUniversalDepen- forChinese. Wefindthattheobjectiveofobtain-
denciesParTUTcorpus. Eachmodelusesadifferent
ing minimal loss does not correspond to optimal
BLOOMlayerasasequenceencoder.
correlationwithfMRIactivity.
Weselectthe17thlayerastherepresentationto
inputintothemodel,whichwasdecidedbasedon
3.4 Multiplepaths
optimization of development set accuracy on the
ParTUTUniversalDependenciescorpus(DeMarn-
effe et al., 2021). We use this smaller corpus Multipath parsing, as a psycholinguistic claim
(2,090 sentences) in order to conserve resources about human sentence comprehension, is simply
wheninvestigatingtheutilityofBLOOMrepresen- the idea that ambiguity-resolution pursues more
tationsfordependencyparsing. AsseeninFig. 3, than one alternative at the same time. This con-
BLOOMlayers10to17havesimilaraccuracylev- trastswithFrazierandFodor’s(1978)conception,
els. Selectingdifferentlayersbasedonasearchof inwhichasingleparseisdevelopedalongonepath.
theEnglishandChineseTreebankcorporadevel- We consider here a version of the multipath idea
opmentsetmaybeabletoprovideminorimprove- thatKurtzman(1984)calls“strong”parallelism,in
mentsinaccuracybeyondthatreportedinTable2. which paths can persist from word to word. Gor-
Furthertraininghyperparameterscanbefoundin rell (1987) emphasizes that multiple alternatives
theAppendix. are not equally available (page 84). This is natu-
rallyformalizedbyrankingthepaths. Suchranking
3.3 ParserAccuracy
canbeimplementedwithbeamsearch,attheheart
Whilestateoftheartmethodsemployinformation of parsing systems like Roark (2001), which is
about the entire sentence (using global methods, widelyappliedincomputationalpsycholinguistics
orincrementalmethodswithbidirectionalembed- andneurolinguistics.3.5 ComplexityMetric
In order to correlate parser predictions to brain
activity, and in particular to take multiple parser
pathsintoconsideration,itisnecessarytoinclude
a metric which summarizes the parser prediction
information. Onesuchmetricissurprisal,revived
byHale(2001)asamethodofpredictingsentence-
processingdifficultyatawordlevel. Thebasicfor-
mulaisthelogarithmoftheinverseofthemarginal
probability.
(cid:18) 1 (cid:19)
log = −log (p(y)) (1)
2 p(y) 2
Inthecaseofincrementalsentencecomprehen-
sion,thesurprisalofawordatpositionw which
i
follows a string ending in w , is a ratio of the
i−1 Figure4: AsentencefromTheLittlePrince,showing
probability at the current word with the previous
incrementalsurprisalfromjointprobabilitydecomposed
probability: intolexical(red),andsyntacticsurprisal(blue)
(cid:18)p(w ,w ,...,w ))(cid:19)
i i−1 1
−log (2)
2 p(w ,...,w )) AnexamplecalculationisshowninFig. 5.
i−1 1
3.5.1 SyntacticSurprisal
Word probabilities, denoted p in Table 1, are
gen
affectedbynonsyntacticfactors. AsFig. 4shows,
thesenonsyntacticfactorscanovershadowthedif-
ference between syntactic alternatives. For this
reason,wefollowRoarketal.(2009)indecompos-
ingsurprisalintolexicalandsyntacticexpectations.
Discarding lexical expectations, we focus exclu-
sively on differences among competing syntactic Figure5: AsentencefragmentfromTheLittlePrince
analysesbyadoptingtheirsyntacticsurprisalmea- showing shift-reduce parser actions with associated
sure,givenbelowasEq. 3. Syntacticsurprisaldoes probabilities. Theprobabilityofthemostlikelypathat
time=3is0.55,whichcorrespondstosurprisalfork=1:
notconsiderwordprobability.
-log (0.55/0.99)=0.86,andk=2: -log(0.99/0.99)=0.
Inaddition,welimitthesumtothetopk transi- 2
tionsequences. Here, j indexesdiscreteranksof
3.5.2 WhyFivePaths?
a ranked parallelparser, and t refers toall the
a(i)
parsertransitionsinthepaththatendsatgeneration Addressingthesingle-pathvsmultipathquestion
oftokenw . entailschoosingsomenumberofpaths,k torepre-
i
sentmultipathparsingingeneral. Empiricalliter-
ature such as Gibson and Pearlmutter (2000) has
(cid:16) (cid:17)
(cid:88)k p t a(i) | w i−1...w 1
j
beenprimarilyconcernedwiththedistinctionbe-
SynS (w ) = −log
k i 2 (cid:16) (cid:17) tween 1 and 2 paths. By contrast, accurate NLP
p t | w ...w
j=1 a(i−1) i−1 1
j systemssometimesconsidertensorevenhundreds
(3)
of paths.3 This mismatch calls for a setting of k
This limitation to k paths follows Boston et al.
thatissmall,butclearlydifferentfrom1. Thesyn-
(2011). UnlikeBostonetal.,weretainallanalyses
andsimplychosethetopkateachsuccessiveword. 3Manyofthesepathsleadtosimilarsyntacticstructures.
Groupingtheminacognitivelyrealisticwaybringsupfounda-
This allows paths that would have been lost for-
tionalquestionsregardingthementalrepresentationofgram-
ever in true beam search to later rejoin the beam. maticalrelations(seeBresnanandKaplan1982,Sturt1996,
Suchrestorationhasessentiallythesameeffectas and §4.4 of Brasoveanu and Dotlacˇil 2020, among others).
Thesequestions,alongwiththeproperformulationofreanal-
backtracking, in cases where it would lead to a
ysisorrepairoperations(e.g.Lewis,1992;Buch-Kromann,
higher-scoringanalyses. 2004)areimportantdirectionsforfuturework.Figure6: Syntacticsurprisalatk =1(blue)andk =5(orange)fortokensinTheLittlePrince
tacticsurprisalmetricitselfalsomitigatesagainst (36questionsintotal). Thesequestionswereused
settings of k that are too large. With this metric, toconfirmparticipantcomprehensionofthestory.
more ranks quickly eat up the available probabil- The English and Chinese brain imaging data
ity mass, up to 1.0, when considering all possi- were acquired with a 3T MRI GE Discovery
blepaths. Inthislimitingcasesyntacticsurprisal MR750 scanner with a 32-channel head coil.
would trivially equal zero for all tokens (see Fig- Anatomical scans were acquired using a T1-
ure6). Forthesereasons,weselectedfivepathsto weightedvolumetricmagnetizationpreparedrapid
contrastwithsingle-pathparsing.4 gradient-echopulsesequence. Blood-oxygen-level-
dependent(BOLD)functionalscanswereacquired
4 fMRIMethods
usingamulti-echoplanarimagingsequencewith
onlinereconstruction(TR=2000ms;TE’s=12.8,
4.1 Participants
27.5,43ms;FA=77°;matrixsize=72x72;FOV
AsdetailedinLietal.(2022),theEnglishdataset = 240.0 mm x 240.0 mm; 2x image acceleration;
includes 49 participants (30 female, mean age = 33axialslices,voxelsize=3.75x3.75x3.8mm).
21.3, range = 18-37), and the Chinese dataset in-
cludes35participants(15female,meanage=19.9, 4.3 DataPreprocessing
range=18-24).
The English and Chinese fMRI data were pre-
processed using AFNI version 16 (Cox, 1996).
4.2 DataAcquisition
The first 4 volumes in each run were excluded
The English audio stimulus is an English transla- fromanalysestoallowforT1-equilibrationeffects.
tion of The Little Prince, read by Karen Savage. Multi-echoindependentcomponentsanalysis(ME-
The Chinese audio stimulus is a Chinese transla- ICA),wasusedtodenoisedataformotion, phys-
tion of The Little Prince, read by a professional iology,andscannerartifacts(Kunduetal.,2012).
femaleChinesebroadcaster. TheEnglishandChi- Imageswerethenspatiallynormalizedtothestan-
neseaudiobooksare94and99minutesinlength, dardspaceoftheMontrealNeurologicalInstitute
respectively. Thepresentationsweredividedinto (MNI) atlas, yielding a volumetric time series re-
ninesections,eachlastingaroundtenminutes. Par- sampledat2mmcubicvoxels.
ticipantslistenedpassivelytotheninesectionsand
completed four quiz questions after each section 4.4 StatisticalAnalysis
Thegoaloftheanalysisistocomparesurprisalfor
4Thisgoalofthisstudyistoadjudicatebetweensingle-
pathandmultipathparsing.Thisquestionhasremainedopen modelsthatconsiderdifferentnumbersofanalysis
withintheliteratureonhumansentenceprocessingforquite paths(k)againstobservedfMRIsignal. Tothisend,
sometime. Analternativeapproachseekstomaximizecor-
we apply the same methodology as Crabbé et al.
relationswithfMRIdatabye.g. searchingfortheoptimal
numberofpaths.Thissuggestsanaturalfollow-up. (2019),inwhichmodelcomparisonsusingcross-validatedcoefficientofdetermination(r2)mapsare tailed,alloftheresultingsignificantclustersshow
carriedoutinordertoevaluatethegoodnessoffit greaterr2 increaseforsurprisalatk=5thanforsur-
oftwoalternatemetricswithBOLDsignal. prisalatk=1. Ther2 increasemapsforeachmodel
For each subject individually, the BOLD sig- individuallycanbefoundintheAppendix.
nalismodeledbyaGeneralLinearModel(GLM)
at each voxel. The following five regressors are 6 Discussion
includedintheGLM:wordrate,fundamentalfre-
Multipath parsing effort, outside of the 1-best
quency (f0), word frequency, root mean square
analysis, localizes to superior temporal regions
intensity(RMS),andsurprisalatthetopk deriva-
of the brain bilaterally in both English and Chi-
tions. Wordrateisatimingfunctionmarkingthe
nese. This finding converges with Crabbé et al.
offsetofeachspokenword;f0isthefundamental
(2019). Crabbéandcolleaguesparsephrasestruc-
frequency,orpitchoftheaudio;wordfrequencyis
ture,ratherthandependencies,employadifferent
obtainedfromwordsinamoviesubtitlesdatabase
complexitymetric,anduseaverylargebeam(400).
(Brysbaert and New, 2009); and RMS is an indi-
Despiteallthesedifferences,theirresultsimplicate
catorofaudiointensity,takenevery10ms. These
roughlythesamebrainregions.
predictorsareknowntoaffectspeechcomprehen-
Theseresultsalsocoherewithproposalsregard-
sion,andareincludedascontrolvariablestoisolate
ing the large-scale organization of language pro-
the BOLDsignal specificto processing syntactic
cessing in the brain. On the Neuroanatomical
ambiguitycorrespondingtothesurprisalregressor.
Pathwaymodel,forinstance,“mostbasicsyntac-
Surprisalforeachwordwasalsoalignedtothe
tic processes” are handled by a ventral pathway
offsetofeachwordintheaudiobook. Allpredictors
that passes through STG regions identified here
wereconvolvedusingSPM’scanonicalHemody-
(Friederici, 2015). Matchin and Hickok (2020)
namicResponseFunction(Fristonetal.,2007).
similarlypointtotheventralpathwayasessential
Foreachparticipant,wecomputehowmuchthe
for“basic”syntacticprocesses. Thisheuristicno-
inclusion of the surprisal regressor increases the
tionofbasicsyntacticprocessingalignswellwith
cross-validatedr2 overthebaselinemodel,which
StanfordDependencies’avowedgoalofproviding
includesonlythecontrolvariables. Werepeatthis
asimple,surface-orientednotationforlocalgram-
process for surprisal at different levels of k sepa-
maticalrelations.
rately in order to keep the number of parameters
in each GLM model constant. Therefore, the r2 Thepresentresultsdonotidentifyasignificant
differenceinr2 increaseinBroca’sarea(leftIFG).
increasescoresrepresentthevarianceexplainedin
Someincreaseinr2 canbeseen,however,inmid-
eachvoxelbytheadditionofsurprisaltothemodel
dleandIFGintheindividualmodelmapsshownin
asapredictor.
theAppendix. Theabsenceofasignificantdiffer-
Tocomparemodelsacrosstwodifferentlevels
ence,howevermakessenseinlightoftwoconsider-
ofk andanalyzetheirabilitytoexplainfMRIsig-
ations. ThefirstistheideathatleftIFGsubserves
nal,weperformedapairedt-testonindividualr2
reanalysisinthefaceofmisinterpretation(Novick
increasemaps,andobtainedz-mapsshowingthe
etal.,2005). Thesecondistheliterarystyleofthe
brainregionswheresurprisalatonelevelofparal-
TheLittlePrince. Thestimulustextisdissimilarto
lelismexplainsthesignalsignificantlybetterthan
thesortsofgarden-pathmaterialspsycholinguists
surprisalattheother(seeFigures7and8).
haveused(seee.g. Sedivy2019,chapter9orFer-
nándezandCairns2010,chapter7). Itseemslikely
5 Results
thatanygarden-pathingwasmild,remainingbelow
Both English and Chinese paired t-tests on r2 in- thelevelofconsciousawareness. Thisdistinction
creasemapsshowdifferencesinthesuperiortempo- betweenmisinterpretationsthatrisetothelevelof
ralgyrus(STG).TheChineseresultsfurthershow conscious awareness and those that do not could
differencesinmiddletemporalgyrus(MTG),while help reconcile these results with earlier studies
the English results show differences in the pari- showingIFGactivationinresponsetogardenpath
etal lobe, including bilateral angular gyrus. The stimuli(e.g.Masonetal.,2003;Roddetal.,2010).
full list of statistically significant clusters corre- Forinstance,Jägeretal.(2015)suggeststhatChi-
sponding to Figures 7 and 8 can be found in the neserelativeclausesinvolveasmanyasfourtempo-
Appendix. Although the paired t-tests were two- raryambiguities. ToourknowledgenoneoftheseFigure7: EnglishBrainz-mapsshowingthesignificantclusters(p<.001uncorrected;clusterthreshold=15voxels)
forthemodelcomparisonbetweensyntacticsurprisalatk=1vsk=5. Allsignificantclustersshowgreaterr2increase
forsurprisalatk=5thanforsurprisalatk=1.
Figure 8: Chinese Brain z-maps showing the significant clusters (p < .001 uncorrected; cluster threshold = 15
voxels)forthemodelcomparisonbetweensyntacticsurprisalatk=1vsk=5. Allsignificantclustersshowgreaterr2
increaseforsurprisalatk=5thanforsurprisalatk=1.
reachthelevelofconsciousawarenessineveryday Thismultipathinterpretationthatweofferhere
comprehension. Yet in a naturalistic fMRI study can be confirmed or refuted with other linked
Dunaganetal.(2022)observeactivationinanterior linguistic and neuro-cognitive databases – for in-
andmiddleSTGthatisspecifictoChineseobject- stance,indifferentgenresorlanguages. Inaddition,
extracted relative clauses. This contrast was not usingbraindatawithahighertemporalresolution,
reliableinastatisticalcomparisonbetweenEnglish suchasMEG,mayprovideabenefitgiventhetem-
andChinese. Furtherworkisneededtochartthe porarynatureofthesyntacticambiguitiesincluded
spacebetweenunproblematicambiguities(Lewis, inourmodel. Althoughwetaketheprimaryfind-
1993,§2.4.2)andconsciousmisinterpretationsen- ingtobeoneofcommonalityacrossSTGregions
genderedbysyntacticambiguity. Inasimilarvein, in both languages, English listeners did uniquely
thelocalizationtoprimaryauditorycorticescould show an additional effect of multipath parsing in
meanthatbasicsyntacticprocessingismultipath parietal regions. Activation in this area has been
evenatveryearlystagesofperceptualprocessing. correlatedwithmeasuresofhumanmemorysuch
Thispossibilitycouldbeexaminedinafollowup as digit span (Meyer et al., 2012). Individuals’
fMRIstudywithwrittenorsignedmaterials,along memoryspanmaymodulatethenumberofpaths
thelinesofHendersonetal.(2016). thattheypursueduringcomprehension(Vosetal.,
2001;PratandJust,2010). Futureworkshouldad-
7 ConclusionandFutureWork dresstheinteractionbetweendisambiguationand
memory(e.g.Campanellietal.,2018).
Themainconclusionisthathumanparsingismul-
tipath. This follows from observing greater r2 Limitations
increase for multipath surprisal than single-path
surprisalinfMRIdata. Evenwithaverybottom- ThegoalofcorrelatingparserstateswithfMRIdata
up strategy, such as the arc-hybrid system used islimitedbytheparticularitiesoftheparsingsys-
here,itseemsthatmorethanonepathmustbepur- tem–namelythearc-hybridtransitionsystemand
suedinordertobest-alignwithhumans’word-by- the limited size and content of the training data
word effort profile. This conclusion is consistent (whichisdissimilaringenretotheaudiobooktext
with disjunctive representations of choicese such we study). In addition, the English results in par-
asPPattachment(Kitaevetal.,2022,§5.5). ticulararetheresultofcarefulmodelselectiontolimit “training away” the syntactic ambiguity we Jonathan R. Brennan, Edward P. Stabler, Sarah E
wouldliketomeasure. Thismayindicatethatmore Van Wagenen, Wen-Ming Luh, and John T. Hale.
2016. Abstract linguistic structure correlates with
formalizedexternallimitationsshouldbeapplied
temporalactivityduringnaturalisticcomprehension.
tomodernhigh-performingparsersshouldtheybe
BrainLang.,157-158:81–94.
usedtomodelhumansentenceprocessing.
JoanBresnanandRonaldM.Kaplan.1982. Introduc-
tion: Grammars as mental representations of lan-
Ethics
guage. InJoanBresnan,editor,TheMentalRepre-
sentation of Grammatical Relations, pages xvii,lii.
Language models pose risks when used outside
MITPress,Cambridge,MA.
of their intended scope. We use BLOOM, which
MarcBrysbaertandBorisNew.2009. Movingbeyond
ispubliclyavailableundertheResponsibleAILi-
KucˇeraandFrancis: Acriticalevaluationofcurrent
cense(RAIL)(LeScaoetal.,2022a). Ourscientific
wordfrequencynormsandtheintroductionofanew
enquiryfallswithintheintendeduseofpublicre- andimprovedwordfrequencymeasureforAmerican
searchonLLMs. Wealsouseapubliclyavailable English. Behaviorresearchmethods,41(4):977–990.
fMRI dataset (Li et al., 2022), which is available
Matthias Buch-Kromann. 2004. Optimality parsing
underaCCOlicense. Thisdatasetwasanonymized
andlocalcostfunctionsinDiscontinuousGrammar.
toremoveidentifyingfacialfeaturesbeforepubli- Electronic Notes in Theoretical Computer Science,
cation. 53:163–179. Proceedings of the joint meeting of
the6thConferenceonFormalGrammarandthe7th
ConferenceonMathematicsofLanguage.
Acknowledgements
Jan Buys and Phil Blunsom. 2018. Neural syntactic
This material is based upon work supported by generative models with exact marginalization. In
the National Science Foundation under Grant Proceedings of the 2018 Conference of the North
AmericanChapteroftheAssociationforComputa-
Number 1903783. We thank Laura Rimell and
tionalLinguistics: HumanLanguageTechnologies,
MichaelCovingtonforvaluablediscussion,along
Volume1(LongPapers),pages942–952.
with Christophe Pallier for developing the cross-
validated r2 code used here. We are grateful to LucaCampanelli, JulieA.VanDyke, andKlaraMar-
ton.2018. Themodulatoryeffectofexpectationson
many collaborators for assistance with the fMRI
memoryretrievalduringsentencecomprehension. In
data collection. Among them are Shohini Bhat- TimothyT.Rogers, MarinaRau, XiaojinZhu, and
tasali, Jixing Li, Wen-Ming Luh and Nathan CharlesW.Kalish,editors,Proceedingsofthe40th
AnnualConferenceoftheCognitiveScienceSociety,
Spreng.
pages1434–1439.CognitiveScienceSociety,Austin,
Texas.
References Charlotte Caucheteux and Jean-Rémi King. 2022.
Brainsandalgorithmspartiallyconvergeinnatural
InaBornkesselandMatthiasSchlesewsky.2006. The languageprocessing. NatureCommunicationsBiol-
extendedargumentdependencymodel: Aneurocog- ogy,5(1):134.
nitive approach to sentence comprehension across
languages. PsychologicalReview,113(4):787–821. Robert W Cox. 1996. Afni: software for analysis
andvisualizationoffunctionalmagneticresonance
InaBornkessel-SchlesewskyandMatthiasSchlesewsky. neuroimages. ComputersandBiomedicalresearch,
2015. The Argument Dependency Model. In 29(3):162–173.
(Hickoketal.,2015),chapter30.
BenoitCrabbé,MurielleFabre,andChristophePallier.
2019. Variable beam search for generative neural
MarisaFerraraBoston,JohnTHale,ShravanVasishth,
parsing and its relevance for the analysis of neuro-
andReinholdKliegl.2011. Parallelprocessingand
imagingsignal. InProceedingsofthe2019Confer-
sentence comprehension difficulty. Language and
enceonEmpiricalMethodsinNaturalLanguagePro-
CognitiveProcesses,26(3):301–349.
cessingandthe9thInternationalJointConference
onNaturalLanguageProcessing(EMNLP-IJCNLP),
AdrianBrasoveanuandJakubDotlacˇil.2020. Compu-
pages1150–1160,HongKong,China.Association
tationalCognitiveModelingandLinguisticTheory.
forComputationalLinguistics.
Language,Cognition,andMind.Springer.
Marie-CatherineDeMarneffeandChristopherDMan-
JonathanR.Brennan,ChrisDyer,AdhigunaKuncoro, ning.2008. Thestanfordtypeddependenciesrepre-
and John T. Hale. 2020. Localizing syntactic pre- sentation. InColing2008: proceedingsofthework-
dictions using recurrent neural network grammars. shoponcross-frameworkandcross-domainparser
Neuropsychologia,146:1074–1079. evaluation,pages1–8.Marie-CatherineDeMarneffe,ChristopherDManning, JohnTHale.2001. AprobabilisticEarleyparserasa
JoakimNivre,andDanielZeman.2021. Universal psycholinguistic model. In Second meeting of the
dependencies. Computationallinguistics,47(2):255– NorthAmericanchapteroftheassociationforcom-
308. putationallinguistics.
DonaldDunagan,MilošStanojevic´,MaximinCoavoux, JohnTHale,LucaCampanelli,JixingLi,ShohiniBhat-
ShulinZhang,ShohiniBhattasali,JixingLi,Jonathan tasali,ChristophePallier,andJonathanRBrennan.
Brennan, and John Hale. 2022. Neural correlates 2022. Neurocomputationalmodelsoflanguagepro-
ofobject-extractedrelativeclauseprocessingacross cessing. AnnualReviewofLinguistics,8:427–446.
English and Chinese. Neurobiology of Language,
JohnMHenderson,WonilChoi,MatthewWLowder,
pages1–43.
andFernandaFerreira.2016. Languagestructurein
thebrain: Afixation-relatedfMRIstudyofsyntactic
TiwalayoEisape,VineetGangireddy,RogerLevy,and
surprisalinreading. NeuroImage,132:293–300.
Yoon Kim. 2022. Probing for incremental parse
states in autoregressive language models. In Find-
GregoryHickok,StevenL.Small,andStevenL.Small.
ingsoftheAssociationforComputationalLinguistics:
2015. Neurobiology of Language. Elsevier, San
EMNLP2022,pages2801–2813,AbuDhabi,United
Diego.
ArabEmirates.AssociationforComputationalLin-
guistics. Jens-Max Hopf, Markus Bader, Michael Meng, and
JosefBayer.2003. Ishumansentenceparsingserial
EvaM.FernándezandHelenSmithCairns.2010. Fun- orparallel?: Evidencefromevent-relatedbrainpo-
damentalsofPsycholinguistics. Wiley-Blackwell. tentials. CognitiveBrainResearch,15(2):165–177.
Marilyn Ford. 1989. Parsing complexity and a the- LenaJäger,ZhongChen,QiangLi,Chien-JerCharles
oryofparsing. InGregN.CarlsonandMichaelK. Lin, and Shravan Vasishth. 2015. The subject-
Tanenhaus,editors,LinguisticStructureinLanguage relative advantage in Chinese: Evidence for
Processing,pages239–272.Kluwer. expectation-based processing. Journal of Memory
andLanguage,79:97–120.
LynFrazierandJanetDeanFodor.1978. Thesausage
machine: anewtwo-stageparsingmodel. Cognition, DanielJurafsky.1996. Aprobabilisticmodeloflexical
6:291–325. andsyntacticaccessanddisambiguation. Cognitive
science,20(2):137–194.
AngelaD.Friederici.2015. Theneuroanatomicalpath-
Nikita Kitaev, Thomas Lu, and Dan Klein. 2022.
way model of language. In (Hickok et al., 2015),
Learnedincrementalrepresentationsforparsing. In
chapter29.
Proceedings of the60th Annual Meeting of the As-
sociationforComputationalLinguistics(Volume1:
Karl J Friston, John Ashburner, Stefan J Kiebel,
TNichols,andWilliamPenny.2007. StatisticalPara-
LongPapers),pages3086–3095,Dublin,Ireland.As-
metricMapping. AcademicPress. sociationforComputationalLinguistics.
MarcoKuhlmann,CarlosGómez-Rodríguez,andGior-
Edward Gibson. 1991. A Computational Theory of
gioSatta.2011. Dynamicprogrammingalgorithms
HumanLinguisticProcessing: MemoryLimitations
fortransition-baseddependencyparsers. InProceed-
andProcessingBreakdown. Ph.D.thesis,Carnegie
ingsofthe49thAnnualMeetingoftheAssociationfor
MellonUniversity.
ComputationalLinguistics: HumanLanguageTech-
nologies, pages 673–682, Portland, Oregon, USA.
Edward Gibson and Neal J. Pearlmutter. 2000. Dis-
AssociationforComputationalLinguistics.
tinguishing serial and parallel parsing. Journal of
PsycholinguisticResearch,29(2).
PrantikKundu,SouheilJInati,JenniferWEvans,Wen-
MingLuh,andPeterABandettini.2012. Differenti-
PaulG.Gorrell.1987. Studiesofhumansyntacticpro-
atingBOLDandnon-BOLDsignalsinfMRItimese-
cessing: Ranked-parallelversusserialmodels. Ph.D.
riesusingmulti-echoEPI. Neuroimage,60(3):1759–
thesis,UniversityofConnecticut.
1770.
John Hale. 2016. Information-theoretical complex- HowardS.Kurtzman.1984. Ambiguityresolutionin
ity metrics. Language and Linguistics Compass, thehumansyntacticparser: Anexperimentalstudy.
10(9):397–412. In10thInternationalConferenceonComputational
Linguisticsand22ndAnnualMeetingoftheAssocia-
John Hale, Chris Dyer, Adhiguna Kuncoro, and tionforComputationalLinguistics,pages481–485,
JonathanBrennan.2018. Findingsyntaxinhuman Stanford,California,USA.AssociationforComputa-
encephalographywithbeamsearch. InProceedings tionalLinguistics.
of the 56th Annual Meeting of the Association for
ComputationalLinguistics(Volume1: LongPapers), Teven Le Scao, Angela Fan, Christopher Akiki, El-
pages2727–2736,Melbourne,Australia.Association lie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman
forComputationalLinguistics. Castagné,AlexandraSashaLuccioni,FrançoisYvon,Matthias Gallé, et al. 2022a. Bloom: A 176b- RobertAMason,MarcelAdamJust,TimothyAKeller,
parameteropen-accessmultilinguallanguagemodel. and Patricia A Carpenter. 2003. Ambiguity in the
arXivpreprint2211.05100. brain: whatbrainimagingrevealsabouttheprocess-
ingofsyntacticallyambiguoussentences. Journalof
Teven Le Scao, Thomas Wang, Daniel Hesslow, Stas ExperimentalPsychology: Learning,Memory,and
Bekman,MSaifulBari,StellaBiderman,HadyElsa- Cognition,29(6):1319.
har, Niklas Muennighoff, Jason Phang, Ofir Press,
Colin Raffel, Victor Sanh, Sheng Shen, Lintang William Matchin and Gregory Hickok. 2020. The
Sutawika,JaesungTae,ZhengXinYong,JulienLau- cortical organization of syntax. Cerebral Cortex,
nay,andIzBeltagy.2022b. Whatlanguagemodelto 30(3):1481–1498.
trainifyouhaveonemillionGPUhours? InFind-
ingsoftheAssociationforComputationalLinguistics: Lars Meyer, Jonas Obleser, Alfred Anwander, and
EMNLP2022, pages765–782, AbuDhabi, United Angela D Friederici. 2012. Linking ordering in
ArabEmirates.AssociationforComputationalLin- broca’s area to storage in left temporo-parietal re-
guistics. gions: thecaseofsentenceprocessing. Neuroimage,
62(3):1987–1998.
RichardL.Lewis.1992. RecentDevelopmentsinthe
NL-Soar Garden Path Theory. Technical Report Joakim Nivre. 2004. Incrementality in deterministic
CMU-CS-92-141,CarnegieMellonUniversity. dependencyparsing. InProceedingsoftheworkshop
onincrementalparsing: Bringingengineeringand
RichardL.Lewis.1993. AnArchitecturally-basedThe- cognitiontogether,pages50–57.
oryofHumanSentenceComprehension. Ph.D.thesis,
CarnegieMellonUniversity,Pittsburgh,PA. JoakimNivre.2008. Algorithmsfordeterministicincre-
mentaldependencyparsing. ComputationalLinguis-
RichardL.Lewis.2000a. Falsifyingserialandparal- tics,34(4):513–553.
lel parsing models: empirical conundrums and an
overlooked paradigm. Journal of Psycholinguistic Jared M. Novick, John C. Trueswell, and Sharon L.
Research,29(2). Thompson-Schill.2005. Cognitivecontrolandpars-
ing:ReexaminingtheroleofBroca’sareainsentence
RichardL.Lewis.2000b. Specifyingarchitecturesfor comprehension. Cognitive,Affective&Behavioral
languageprocessing: Process,control,andmemory Neuroscience,5(3):263–281.
inparsingandinterpretation. InMatthewW.Crocker,
MartinPickering,andCharlesClifton,Jr.,editors,Ar- Byung-DohOhandWilliamSchuler.2023. Whydoes
chitecturesandmechanismsforlanguageprocessing. surprisal from larger transformer-based language
CambridgeUniversityPress. modelsprovideapoorerfittohumanreadingtimes?
TransactionsoftheAssociationforComputational
Jixing Li, Shohini Bhattasali, Shulin Zhang, Berta Linguistics,11:336–350.
Franzluebbers, Wen-Ming Luh, R Nathan Spreng,
JonathanRBrennan,YimingYang,ChristophePal- SubbaReddyOota,MounikaMarreddy,ManishGupta,
lier, and John Hale. 2022. Le Petit Prince multi- and Raju Bapi. 2023. How does the brain process
lingual naturalistic fMRI corpus. Scientific data, syntacticstructurewhilelistening? InFindingsof
9(1):530. theAssociationforComputationalLinguistics: ACL
2023,pages6624–6647,Toronto,Canada.Associa-
JixingLiandJohnHale.2019. Grammaticalpredictors tionforComputationalLinguistics.
forfMRItimecourses. InRobertC.BerwickandEd-
wardP.Stabler,editors,MinimalistParsing.Oxford Alexandre Pasquiou, Yair Lakretz, Bertrand Thirion,
UniversityPress. andChristophePallier.2023. Information-restricted
neurallanguagemodelsrevealdifferentbrainregions’
Alessandro Lopopolo, Antal van den Bosch, Karl- sensitivitytosemantics,syntaxandcontext. arXiv
MagnusPetersson,andRoelM.Willems.2021. Dis- preprintarXiv:2302.14389.
tinguishingsyntacticoperationsinthebrain: Depen-
dencyandphrase-structureparsing. Neurobiologyof JonasPfeiffer,AndreasRücklé,CliftonPoth,Aishwarya
Language,2(1):152–175. Kamath, Ivan Vulic´, Sebastian Ruder, Kyunghyun
Cho, and Iryna Gurevych. 2020. Adapterhub: A
MitchellP.Marcus.1980. Atheoryofsyntacticrecogni- frameworkforadaptingtransformers. InProceedings
tionfornaturallanguage. MITPress. ofthe2020ConferenceonEmpiricalMethodsinNat-
uralLanguageProcessing(EMNLP2020): Systems
DavidMarr.1982. Vision: Acomputationalinvestiga- Demonstrations,pages46–54,Online.Association
tionintothehumanrepresentationandprocessingof forComputationalLinguistics.
visualinformation. W.H.FreemanandCompany.
ChantelS.PratandMarcelAdamJust.2010. Explor-
William Marslen-Wilson. 1973. Linguistic structure ing the Neural Dynamics Underpinning Individual
andspeechshadowingatveryshortlatencies. Nature, DifferencesinSentenceComprehension. Cerebral
244:522–523. Cortex,21(8):1747–1760.AnikethJanardhanReddyandLeilaWehbe.2021. Can
fmrirevealtherepresentationofsyntacticstructure
in the brain? In Advances in Neural Information
Processing Systems, volume 34, pages 9843–9856.
CurranAssociates,Inc.
Brian Roark. 2001. Probabilistic top-down parsing
andlanguagemodeling. ComputationalLinguistics,
27(2):249–276.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
ChristophePallier.2009. Derivinglexicalandsyntac-
ticexpectation-basedmeasuresforpsycholinguistic
modelingviaincrementaltop-downparsing. InPro-
ceedingsofthe2009ConferenceonEmpiricalMeth-
odsinNaturalLanguageProcessing,pages324–333,
Singapore.AssociationforComputationalLinguis-
tics.
JenniferMRodd, OliviaALonge, BilliRandall, and
LorraineKTyler.2010. Thefunctionalorganisation
of the fronto-temporal language system: evidence
fromsyntacticandsemanticambiguity. Neuropsy-
chologia,48(5):1324–1335.
MartinSchrimpf,IdanAsherBlank,GretaTuckute,Ca-
rina Kauf, Eghbal A. Hosseini, Nancy Kanwisher,
JoshuaB.Tenenbaum,andEvelinaFedorenko.2021.
The neural architecture of language: Integrative
modelingconvergesonpredictiveprocessing. Pro-
ceedings of the National Academy of Sciences,
118(45):e2105646118.
Julie Sedivy. 2019. Language in Mind: An Introduc-
tion to Psycholinguistics, second edition. Oxford
UniversityPress.
Cory Shain, Idan Asher Blank, Marten van Schijn-
del,WilliamSchuler,andEvelinaFedorenko.2020.
fMRIrevealslanguage-specificpredictivecodingdur-
ingnaturalisticsentencecomprehension. Neuropsy-
chologia,138:107307.
PatrickSturt.1996. MonotonicSyntacticProcessing: A
Cross-linguisticStudyofAttachmentandReanalysis.
LanguageandCognitiveProcesses,11(5):449–494.
MichaelTanenhaus,MichaelSpivey-Knowlton,Kath-
leenEberhard,andJulieSedivy.1995. Integrationof
visualandlinguisticinformationinspokenlanguage
comprehension. Science,268:1632–1634.
SandraH.Vos,ThomasC.Gunter,HerbertSchriefers,
and Angela D. Friederici. 2001. Syntactic parsing
andworkingmemory: Theeffectsofsyntacticcom-
plexity,readingspan,andconcurrentload. Language
andCognitiveProcesses,16(1):65–103.A Appendix
A.1 GenerativeModels
AsnotedinSection3.2,weuseBLOOM’stokenizertoencodealltokens. Sincesomeofthesetokensare
notfoundinthetrainingdata,andthusareunknowntotheparser’snextwordpredictionclassifier,this
preventsourparserfrombeingstrictlygenerative.
Asacomparison,wetrainadditionalmodelswhicharetrulygenerativebyreplacingtokensnotseen
inthetrainingdatawithunknowntokensintheencoderinput,inthesamewayasthewordprediction
classesaredefined. Wereporttheaccuracyforthisgenerativeapproachalongsidethepreviouslydefined
fullinputmodelinTable3. ThisdistinctionisalsoexplainedinFig. 9foranexampleprefixstring.
Figure9: Parserinputintheformof(stack(σ),currentindex(β))tuplesispre-processedaccordingtothegiven
method: FullInputindicatesnopre-processingapartfromaddingaleadingspacetoallpretokenizedtextfromthe
corpus. Generativeindicatestokensarepreprocessedbyreplacingtokensnotseenmorethanonceinthetraining
datawithunknowntokensaccordingtotherulesoftheBerkeleyParser. TheBLOOMmodelandthesystemof
classifiersusedtooutputprobabilitiesisthesameforbothmodels. Notethatthewordpredictionclassifierpredicts
overthesizeofthetrainingdatavocabulary+Berkeleyunknowntokensforbothmethods.
Dev Test
Corpus Model
LAS UAS LAS UAS
PTB-3 BuysandBlunsom(2018) 88.66 91.19 88.54 91.01
PTB-3 EnglishBLOOMGenerative 89.27 92.00 90.53 92.68
PTB-3 EnglishBLOOMFullInput(brainanalysis) 90.26 92.71 90.32 92.62
CTB-7 ChineseBLOOMGenerative 73.12 80.60 74.47 81.60
CTB-7 ChineseBLOOMFullInput(brainanalysis) 77.07 83.65 74.71 81.66
Table3: Labeledattachmentscore(LAS),Unlabeledattachmentscore(UAS),andLabelaccuracyscoreforthe
EnglishPTBcorpusandChineseCTBcorpus
A.2 ParserTraining
TheoutputrepresentationfromtheBLOOMmodelhasadropoutof0.5applied,andisthenfedintoa
singlelayerfeedforwardneuralnetwork.
Duringtraining,gradientnormsareclippedto5.0,andtheinitiallearningrateis1.0,withadecayfactor
of1.7appliedeveryepochaftertheinitial6epochs. EnglishandChinesemodelstrainforapproximately
5.5and4.5minutes,respectively,perepochonanA100GPU.A.3 IntermediateResults
Figure10: EnglishBrainz-mapsshowingther2increaseforthemodelincludingsurprisalatk=5comparedtothe
modelincludingonlythefollowingregressors: wordrate,fundamentalfrequency(f0),wordfrequency,androot
meansquareintensity(RMS).
Figure11: EnglishBrainz-mapsshowingther2increaseforthemodelincludingsurprisalatk=1comparedtothe
modelincludingonlythefollowingregressors: wordrate,fundamentalfrequency(f0),wordfrequency,androot
meansquareintensity(RMS).
Figure12: ChineseBrainz-mapsshowingther2increaseforthemodelincludingsurprisalatk=5comparedtothe
modelincludingonlythefollowingregressors: wordrate,fundamentalfrequency(f0),wordfrequency,androot
meansquareintensity(RMS).
Figure13: ChineseBrainz-mapsshowingther2increaseforthemodelincludingsurprisalatk=1comparedtothe
modelincludingonlythefollowingregressors: wordrate,fundamentalfrequency(f0),wordfrequency,androot
meansquareintensity(RMS).A.4 Results
Tables4and5showthefulllistofsignificantclusters(p<.001uncorrected;clusterthreshold=15voxels)
forthemodelcomparisonbetweensyntacticsurprisalatk=1vsk=5.
Region X Y Z PeakStat ClusterSize(mm3)
Left-SecVisual(18),Right/LeftDorsalPCC(31) -12.0 -64.0 24.0 4.78 968
Right-VisMotor(7) 0.0 -64.0 48.0 4.02 856
Right-AngGyrus(39) 38.0 -54.0 38.0 4.42 800
Left-PreMot+SuppMot(6)/Left-PrimAuditory(41) -48.0 -10.0 4.0 4.68 568
Right-VentPostCing(23) 14.0 -54.0 20.0 4.57 488
Right-AngGyrus(39) 36.0 -74.0 40.0 4.27 360
Left-AngGyrus(39) -44.0 -68.0 36.0 3.77 352
Right-DorsalPCC(31) 14.0 -62.0 34.0 4.03 216
Right-DorsalPCC(31) 4.0 -50.0 44.0 3.65 192
Right-PrimAuditory(41) 48.0 -12.0 2.0 3.69 160
Right-PrimAuditory(41) 38.0 -26.0 12.0 3.39 152
Table4: Englishsignificantclusters(p<.001uncorrected;clusterthreshold=15voxels)forthemodelcomparison
betweensyntacticsurprisalatk=1vsk=5
Region X Y Z PeakStat ClusterSize(mm3)
Right-MedTempGyrus(21)/Right-SupTempGyrus(22) 56.0 -36.0 4.0 4.68 1336
Left-SupTempGyrus(22) -64.0 -26.0 6.0 4.32 392
Right-PrimAuditory(41) 52.0 -14.0 6.0 4.59 384
Left-PrimAuditory(41) -56.0 -6.0 4.0 5.04 384
Left-MedTempGyrus(21) -48.0 -42.0 4.0 4.25 320
Left-SupTempGyrus(22) -58.0 -26.0 0.0 4.06 288
Table5: Chinesesignificantclusters(p<.001uncorrected;clusterthreshold=15voxels)forthemodelcomparison
betweensyntacticsurprisalatk=1vsk=5