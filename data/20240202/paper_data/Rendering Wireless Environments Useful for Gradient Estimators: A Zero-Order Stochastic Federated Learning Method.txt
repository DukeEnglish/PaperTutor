Rendering Wireless Environments Useful for
Gradient Estimators: A Zero-Order Stochastic
Federated Learning Method
Elissa Mhanna and Mohamad Assaad
Laboratoire des Signaux et Systèmes
Université Paris-Saclay, CNRS, CentraleSupélec
91190 Gif-sur-Yvette, France
Email: {elissa.mhanna, mohamad.assaad}@centralesupelec.fr
Abstract—Federated learning (FL) is a novel approach to ma- one-point estimates that use only one function value [3]–[5],
chinelearningthatallowsmultipleedgedevicestocollaboratively principally obtained at a random point,
train a model without disclosing their raw data. However, several
challenges hinder the practical implementation of this approach, d
g = f(θ+γΦ,S)Φ,
especiallywhendevicesandtheservercommunicateoverwireless γ
channels, as it suffers from communication and computation
bottlenecks in this case. By utilizing a communication-efficient assume that the settings are continuously changing during
framework, we propose a novel zero-order (ZO) method with optimization. This is an important property as it resonates
a one-point gradient estimator that harnesses the nature of the
with many realistic applications, like when the optimization is
wireless communication channel without requiring the knowledge
performed in wireless environments or is based on previous
ofthechannelstatecoefficient.Itisthefirstmethodthatincludes
the wireless channel in the learning algorithm itself instead simulation results. Recently, an appeal to ZO optimization is
of wasting resources to analyze it and remove its impact. The emerginginthemachine-learningcommunity,whereoptimizers
two main difficulties of this work are that in FL, the objective are based on gradient methods. Examples include reinforce-
function is usually not convex, which makes the extension of FL
ment learning [6], [7], generating contrastive explanations for
to ZO methods challenging, and that including the impact of
black-box classification models [8], and effecting adversarial
wireless channels requires extra attention. However, we overcome
these difficulties and comprehensively analyze the proposed zero- perturbations on such models [9], [10].
order federated learning (ZOFL) framework. We establish its On the other hand, with the massive amounts of data
convergence theoretically, and we prove a convergence rate of generated or accessed by mobile devices, a growing research
O(√31 K) in the nonconvex setting. We further demonstrate the
interestinbothsectorsofacademiaandindustry[11]isfocused
potential of our algorithm with experimental results, taking into
on FL[12], [13], asit’s a practicalsolution for trainingmodels
accountindependentandidenticallydistributed(IID)andnon-IID
on such data without the need to log them to a server. A
device data distributions.
lot of effort has been invested in developing first-order [12],
I. INTRODUCTION [14], [15] and second-order [16], [17] methods to improve the
efficacy of FL. These methods typically require access to the
ZO methods are a subfield of optimization that assume that
gradient or the Hessian of the local objective functions in their
first-order (FO) information or access to function gradients is
implementation to solve the optimization problem. However,
unavailable.ZOoptimizationisbasedonestimatingthegradient
usingandexchangingsuchinformationraisesmanychallenges,
using function values queried at a certain number of points.
suchasexpensivecommunicationandcomputationandprivacy
The number of function queries depends on the assumptions of
concerns [18].
theproblem.Forexample,inmulti-pointgradientestimates[1],
[2], they construct the gradient by performing the difference of
function values obtained at many random or predefined points.
However,theyassumethatthestochasticsettingstaysthesame
duringallthesequeries.Forexample,forfunctionsθ (cid:55)→f(θ,S)
subject to a stochastic variable S, two-point gradient estimates
have the form,
f(θ+γΦ,S)−f(θ−γΦ,S)
g =d Φ, Fig. 1: Federated learning over wireless networks.
2γ
withθ ∈Rd theoptimizationvariable,γ >0asmallvalue,and There is more interest recently in learning over wireless
Φ a random vector with a symmetric distribution. By contrast, environments[19]–[24],withtheincreaseofdevicesconnected
4202
naJ
03
]GL.sc[
1v06471.1042:viXrato servers through cellular networks. In this paper, we are resources since only a scalar is computed instead of a long
interested in this scenario illustrated in Fig. 1. Similarly to the vector of gradient/model as in the standard gradient approach.
aforementioned work, we are examining the case of analog
A. Motivation for Our Work
communications between the devices and the server. However,
it is a challenging problem as when the information is sent Communication Bottleneck. In general, the main idea of
over the wireless channel, it becomes subject to a perturbation FL is that the devices receive the model from the server,
induced by the channel. This perturbation is not limited to make use of their data to update the gradient, and then send
additive noise, as noise is, in fact, due to thermal changes at back their gradients without ever disclosing their data. The
the receiver. The channel acts as a filter for the transmitted server then updatesthe model using the collected and averaged
signal [25], [26], gradients, and the process repeats. Since the gradients have
xˆ=Hx+n (1) the same dimension as the model, in every uplink step, there
are Nd values that need to be uploaded, which forms the
where x and xˆ ∈ Rd are the sent and received signals, fundamental communication bottleneck in FL. To deal with
respectively. H ∈Rd×d is the channel matrix, and n∈Rd is this issue, some propose local multiple gradient descent steps
the additive noise, both of which are stochastic, constantly to be done by the devices before sending their gradients
changing, and unknown. In general, all these entities are back to the server to save communication resources [27],
considered to have complex values. However, as we are or allow partial device participation at every iteration [28],
interested in sending only real scalar values and we are not [29], or both [12]. Others propose lossy compression of the
interested in decoding without error the sent information (but gradient before uploading it to the server. For example, all
rather on using a perturbing on loss function), the phase [30]–[32] suggest stochastic unbiased quantization approaches,
shift introduced by the channel is considered as an additional where gradients are approximated with a finite set of discrete
perturbation in the channel model. Since our model is based values for efficiency. [33]proposes the quantizationof gradient
on perturbing the loss function, considering only real values differences of the current and previous iterations, allowing the
in the equation above is sufficient. We elaborate further on the updatetoincorporatenewinformation,while[34]proposesthe
channelmodelingandwhywecanconsideritrealinAppendix sparsification of this difference. Sparsification means that if a
A for the interested reader. In FL, x may denote the model or vectorcomponentisnotlargeenough,itwillnotbetransmitted.
itsgradientssentoverthechannel.Toremovethisimpact,every Channel Impact. In FL over wireless channels, there is a
channel element must be analyzed and removed to retrieve problem with channel knowledge. When the devices upload
the sent information. This analysis is costly in computation their gradient g ∈Rd to the server, the server receives Hg+n
and time resources. Thus, here our objective is to study FL in as shown in equation (1). In [19] and [35] and all references
wireless environments without wasting resources. within, they assume that they can remove the impact of the
Further, we are interested in exploring the potential of ZO channel.However,asthechannelmatrixH coefficientsfollowa
optimization to deal with some of the difficulties demonstrated stochastic process and there are two unknown received entities,
by FL. We then consider an FL setting where a central server the channel H and the gradient, the knowledge of the gradient
coordinates with N edge devices to solve an optimization requires estimating the channel coefficients at each iteration
problemcollaboratively.Thedataisprivatetoeverydeviceand of the FL. This requires computation resources, and more
the exchanges between the server and the devices is restricted importantly, it requires resources to exchange control/reference
totheoptimizationparameters.Tothatend,letN ={1,...,N} signalsbetweenthedevicesandtheserverateachtime/iteration
be the set of devices and θ ∈ Rd denote the global model. to estimate the channel coefficients H. Alternatively, our work
We define F : Rd → R as the loss function associated with offers a much simpler approach. We do not waste resources
i
the local data stored on device i, ∀i∈N. The objective is to trying to analyze the channel. We use the channel in the
minimize the function F : Rd → R that is composed of the implementationitself.Itispartofthelearning.Weharnessitto
said devices’ loss functions, such that construct our gradient estimate without the need to remove its
impact, saving both computation and communication resources.
N
(cid:88) Computation Demands. Unlike standard methods that rely
minF(θ):= F (θ) with F (θ)=E f (θ,S ). (2)
θ∈Rd i i Si∼Di i i on the computational capabilities of participating devices, our
i=1
approach is less demanding. Devices simply receive the global
S isani.i.d.ergodicstochasticprocessfollowingalocaldis- model, query it with their data, and send back the scalar loss,
i
tributionD .S isusedtomodelvariousstochasticperturbation, eliminating the need for "backward pass" computation. Only
i i
e.g. local data distribution among others. We further consider the "forward pass" is performed.
thecasewherethedevicesdonothaveaccesstotheirgradients Black-Box Optimization in FL. One motivation for em-
forcomputationalandcommunicationrestraints,andtheymust ployingZOmethodsisblack-boxproblems[35]whengradient
estimate this gradient by querying their model only once per information cannot be acquired or is complicated to compute.
update. They obtain a scalar value from this query (rather than For example, in hyperparameter tuning, gradients cannot be
a long vector as in the gradient), that they must send back calculated, as there isn’t an analytic relationship between the
to the server, which saves computation and communication loss function and the hyperparameters [36].B. Challenges and Contribution Algorithm 1 The 1P-ZOFL Algorithm
Addressing nonconvexity in FL is challenging. Our ZO Input: Initial model θ 0 ∈ Rd, the initial step-
method must handle nonconvexity, noise, and stochasticity sizes α 0 and γ 0, and the channels’ standard deviation
efficiently, which can slow down convergence in gradient tech- σ h
niques.Additionally,thechannel’sintroductionaddsuncertainty 1: for k =0,2,4,... do
and constraints on the number of communication exchanges. 2: The server receives (cid:80)N j=1 h σj 2,k +n j,k
W side ern ie ne gd ut no kne on wsu nre prc oo bn as bis ilt ie tn yt da in std ribre ul ti ia ob nl se ap ne drf fo er wm ea rn fc ue n, cc tio on n- 3: The server broadcasts θ k +γh kΦ k(cid:80)N j=1(cid:16) h σj 2,k +n j,k(cid:17)
h
to all devices
evaluations. Unlike convex cases, nonconvex optimization does
4: The server receives
notalloweasyquantificationofoptimizationprogress.Verifying (cid:80)N
h
f˜(cid:16)
θ + γ Φ
(cid:80)N (cid:0)hj,k
+
gradient convergence becomes intricate due to biased gradient i=1 i,k+1 i k k k j=1 σ2
(cid:1) (cid:17) h
estimates.Moreover,unboundedvarianceinone-pointestimates n ,S +n
j,k i,k+1 i,k+1
can lead to significant gradient deviations. These challenges 5: The server multiplies the received scalar sum by Φ k to
involve technical and intuitive complexities we navigate. assemble g given in (3)
k
In this work, we overcome these difficulties and propose 6: The server updates θ k+1 =θ k−α kg k
a new communication-efficient algorithm in the nonconvex 7: end for
setting. This algorithm differs from the standard gradient
method as it entails two reception-update steps instead of
one, and it is not a simple extension of FO to ZO where the
variable with standard deviation σ , ∀i ∈ N,∀k ∈ N+, and
devices still have to upload their full model/gradient, as is h
n an additive noise on the transmitted signal. Assuming that
the case in [35]. By limiting the exchange to scalar-valued i,k
the channel is time-correlated for two consecutive iterations
updates, we counter the communication bottleneck, and we
k and k+1, such that the autocovariance is E[h h ]=
save up to a factor of O(d) per communication round, in i,k i,k+1
K , ∀i ∈ N, ∀k ∈ N+, we present our learning method in
comparison to standard methods, in terms of total exchanges hh
Algorithm 1:
of variables between the devices and the server, saving a lot
The devices must carry out two communication steps. In the
of execution time. We harness the nature of autocorrelated
first, every device sends the value 1 to the server. According
channels for truly "blind" reception of the data. We prove the σ2
h
convergencetheoreticallywithaone-pointestimateandprovide to equation (1), the server receives hj,k +n from every
σ2 j,k
experimental evidence. An important distinction worth noting device j. Hence, it receives the sum hin step 2. Afterward,
is that standard ZO methods establish convergence by focusing the server uses the values received to adjust the model and
on the expected convergence of the exact gradient. In contrast broadcasts it to the devices. When device i receives the model,
to prior research, our approach goes further in the proof. We itreceiveshDL [θ +γ Φ (cid:80)N (hj,k+n )]+nDL ,andto
i,k+1 k k k j=1 σ2 j,k i,k+1
demonstrate the convergence of the exact gradient itself almost simplify notation, we let the stochasth ic vector [hDL ,nDL ]
surely, not solely its expected value. The key element in this i,k+1 i,k+1
be included within the big vector S of stochastic pertur-
i,k+1
proof is employing Doob’s martingale inequality to constrain
bations. Device i then queries this received model to obtain
the stochastic error resulting from the estimated gradient. We
the stochastic loss f . Then the devices send f to the server in
i
finally extend the analysis to non-symmetrical channel models,
the second communication step, and according to equation (1),
i.e., channels without zero-mean, and thus provide a practical
the server receives the quantity indicated in step 4. Finally, the
algorithm for general settings.
server assembles the gradient estimate and is able to update θ
according to step 7. All transmissions are subject to channel
II. ALGORITHM
scaling and additive noise. We designate them by h and n in
This section illustrates our proposed zero-order stochastic
the device-to-server transmission. In the server-to-devices one,
federated learning algorithm with a new gradient estimator wedesignatethembyS.Weletf˜ = fi bethenormalizedloss
(ZOFL). i σ2
function and define α and γ as twoh step-sizes and Φ ∈Rd
k k k
A. The 1P-ZOFL Algorithm as a perturbation vector generated by the server that has the
We consider an intermediary wireless environment between same dimension as that of the model.
the server and each device i for i ∈ N as shown in Fig. 1. Weemphasizeherethatg (instep6)isthegradientestimate
k
Wireless channels introduce a stochastic scaling on the sent in this case, and one can see that the impact of the channel
signal as elaborated in equation (1). As we only send a scalar is included in the gradient estimate and hence in the learning.
valueoverthechannelatatime,ourchannelhasonlyonescalar The major advantage of this algorithm is that each device
coefficient in addition to a scalar noise. Channel coefficients sends only two scalar values. This is stark improvement in
are usually autocorrelated from one timeslot to the next. Let communication efficiency over standard FL algorithms that
h denote the channel scaling affecting the sent signal from require each device to send back the whole model or local
i,k
device i to the server at timeslot k, independent from all other gradient of dimension d. In effect, it is resource draining and
devices’ channels. We assume h to be a zero-mean random can be unrealistic to assume it is possible.
i,kB. The Estimated Gradient Lemma 4: By Assumptions 1, 2, and 4, we can find a scalar
value c >0 such that ∥b ∥≤c γ .
We provide here analysis of our ZO gradient estimate. We 3 k 3 k
propose the one-point estimate: Proof: Refer to Appendix B-C.
N
g =Φ
(cid:88)(cid:104)
h
f˜(cid:16)
θ′,S
(cid:17)
+n
(cid:105)
, (3)
A. 1P-ZOFL convergence
k k i,k+1 i k i,k+1 i,k+1
i=1 In Lemma 1, we see that in expectation, our estimator
where θ′ = θ + γ Φ (cid:80)N (cid:0)hj,k + n (cid:1) . The values deviates from the gradient direction by the bias term. To
k k k k j=1 σ2 j,k providethatthistermdoesnotgrowlargerandpreferablygrows
h , h , and the noise remainh unknown. This saves
i,k i,k+1 smaller as the algorithm evolves, we impose that γ vanishes.
k
computation complexity and is very communication efficient
Additionally, to ensure that the expected norm squared of the
as it transcends the need to send pilot signals to estimate the
estimator, as shown in Lemma 2, does not accumulate residual
channel continuously.
constant terms, we impose that the step size α vanishes. The
We next consider the following assumptions on the additive k
series properties in the following assumption come from the
noise, the perturbation vector, and the local loss functions.
recursive analysis of the algorithm.
Assumption 1: n is assumed to be a zero-mean uncorre-
i,k Assumption5:Boththestepsizesα andγ vanishtozeroas
lated noise with bounded variance, meaning E(n )=0 and k k
i,k k →∞ and the following series composed of them satisfy the
E(n2 i,k) = σ n2 < ∞, ∀i ∈ N, ∀k ∈ N+. For any timeslot k, convergenceassumptions(cid:80)∞ α γ =∞,(cid:80)∞ α γ3 <∞,
E(n i,kn j,k)=0 if i̸=j. For any device i, E(n i,kn i,k′)=0 and (cid:80)∞ α2 <∞. k=0 k k k=0 k k
if k ̸=k′. k=0 k
Example 2: To satisfy Assumption 5, we consider the
Assumption 2: Let Φ = (ϕ1,ϕ2,...,ϕd)T. At each
iteration k, the server
genk
erates
itsk Φk vectork
independently
following form of the step sizes, α
k
= α 0(1+k)−υ1 and
from other iterations. In addition,
thk
e elements of Φ are
γ
k
= γ 0(1+k)−υ2 with υ 1,υ
2
> 0. Then, it is sufficient to
k find υ and υ such that 0<υ +υ ≤1, υ +3υ >1, and
assumedi.i.dwithE(ϕd1ϕd2)=0ford ̸=d andthereexists 1 2 1 2 1 2
k k 1 2 υ >0.5.
α >0 such that E(ϕdj)2 =α , ∀d , ∀k. We further assume 1
2 k 2 j Wenextdefinethestochasticerrore asthedifferencebetween
there exists a constant α >0 where ∥Φ ∥≤α , ∀k. k
3 k 3 the value of a single realization of g and its conditional ex-
Example 1: An example of a perturbation vector satisfy- k
pectation given the history sequence, i.e., e =g −E[g |H ].
ing Assumption 2, is picking every dimension of Φ from k k k k
k The study of this noise and how it evolves is essential
{−√1 d,√1 d} with equal probability. Then, α
2
= d1 and α
3
=1.
for the analysis of the algorithm as it gives access to the
Assumption 3: All loss functions θ (cid:55)→ f (θ,S ) are Lip-
i i exact gradient when examining the algorithm’s convergence
schitz continuous with Lipschitz constant L , |f (θ,S )−
Si i i behaviorandpermitsustoprovethat,infact,theexactgradient
f (θ′,S )|≤L ∥θ−θ′∥,∀i∈N.Inaddition,E f (θ,S )<
i i Si Si i i converges to zero and not just the expectation of the exact
∞,∀i∈N.
gradient. This is a stronger convergence property, and it has
Let H = {θ ,S ,θ ,S ,...,θ ,S } denote the history
k 0 0 1 1 k k notbeendonebeforeinZOnonconvexoptimizationtothebest
sequence, then the following two lemmas characterize our
of our knowledge. The trick is to show that e is a martingale
gradient estimate. k
difference sequence and to apply Doob’s martingale inequality
Lemma 1: Let Assumptions 1 and 2 be satisfied and define
to derive the following lemma.
the scalar value c 1 = α 2K σh 4h, then the gradient estimator is Lemma 5: If all Assumptions 1-5 hold and ∥θ ∥ < ∞
h k
biased w.r.t. the objective function’s exact gradient ∇F(θ).
almost surely, then for any constant ν > 0, we have
Concretely, E[g k|H k]=c 1γ k(∇F(θ k)+b k) ∀k ∈N+, where P(lim sup ∥(cid:80)K′ α e ∥≥ν)=0.
b is the bias term. K→∞ K′≥K k=K k k
k Proof: Refer to Appendix C-A.
Proof: Refer to Appendix B-A.
The smoothness inequality allows for the first main result,
Lemma2:LetAssumptions1-3andtheinequality∥θ ∥<∞
k
leading to the second in the following theorem.
hold almost surely. There exist a bounded constant c > 0,
2
such that E[∥g ∥2|H ]≤c almost surely. Theorem 1: When Assumptions 1-5 hold, we have
k k 2 (cid:80) α γ ∥∇F(θ )∥2 < +∞ and lim ∥∇F(θ )∥ = 0
Proof: Refer to Appendix B-B. k k k k k→∞ k
almost surely, meaning that the algorithm converges.
III. CONVERGENCEANALYSIS Proof: Refer to Appendix C-B.
Webeginbyassumingthataglobalminimizerθ∗ ∈Rdexists Proof sketch: We substitute the algorithm’s updates in the
such that min θ∈RdF(θ)=F(θ∗)>−∞ and ∇F(θ∗)=0. second inequality of Lemma 3 and replace the estimate by
Assumption4:Weassumetheexistenceandthecontinuityof its expectation plus the stochastic error. We then perform a
∇F (θ) and ∇2F (θ), and that there exists a constant α >0 recursive addition over the iterations k >0. With Lemma 5,
i i 1
such that ∥∇2F (θ)∥ ≤α ,∀i∈N. theconditionsonthestepsizes,andtheupperboundestimate’s
i 2 1
Lemma 3: By Assumption 4, we know that the objective squared norm, we are able to find an upper bound on
functionθ (cid:55)−→F(θ)isL-smoothforsomepositiveconstantL, (cid:80) α γ ∥∇F(θ )∥2 when k grows to ∞. The next step is
k k k k
∥∇F(θ)−∇F(θ′)∥≤L∥θ−θ′∥, ∀θ,θ′ ∈Rd,orequivalently, to consider the hypothesis lim sup∥∇F(θ )∥ ≥ ρ, for
k→∞ k
F(θ)≤F(θ′)+⟨∇F(θ′),θ−θ′⟩+ L∥θ−θ′∥2. ρ>0, and prove that it contradicts with the first result.
2Define δ =F(θ )−F(θ∗). We next find an upper bound random variations influencing every simulation in line with
k k
on the convergence rate of Algorithm 1. our theoretical results. Considering non-IID data distribution
Theorem 2: Consider in addition to the assumptions in seems to be better impacted by our ZO algorithm slightly at
Theorem 1, that the step sizes are those of Example 2 with the beginning without a major effect on the final result.
υ 3 =υ 1+υ 2 <1. Then, we can write In Fig. 3, we test our algorithm with different noise variance
(cid:80) kα kγ (cid:80)kE k(cid:2) α∥∇ kγF k(θ k)∥2(cid:3)
≤
(KA +( 21 )− 1−υ
υ3
3)
−1. (4)
wσ ben2 egia cnn hnd
ain
nn
g
go
,
eti ic
t
αe doth
e
=a st nw
0o
.th 1i al fe
afe
nt ch dte
t
γa hl ego
c
=ori nt 0h
v
.m
e 8r
.gi es Wns
c
hl eo e.w nWed
σh
2es nli =g σh n2t 1ly
=
0.a 02t 4.t 82h 95e
,,
0 0 n
with A= c12 αδ 00 γ0 +c2 3γ 02(cid:0) υ1υ +1+ 3υ3 2υ −2 1(cid:1) + L cc 12 γα 00(cid:0) 2υ2 1υ −1 1(cid:1) . α 0 =0.07andγ 0 =0.3.Weprovideallotherdetailsalongside
Proof: Refer to Appendix C-C. a discussion of our algorithm’s performance in relation to the
In Theorem 2, we see that the optimal choice of the exponents noise variance in Appendix E.
for the time-varying component O( 1 ), is υ = 1 and
K1−υ1−υ2 1 2
υ
2
= 1
6
forarateofO(√ 31 K).However,topreventtheconstant
component from growing too large, it is recommended to
chooseslightlylargerexponentsofυ = 1+ϵ andυ = 1+ϵ,
1 2 2 2 6 2
where ϵ is a small strictly positive value. This will result in a
rate of O(cid:0) 1 (cid:1) .
K31−ϵ
B. Non-Symmetrical Channels Case
Assuming a non-symmetrical channel model with E[h ]=
i,k
µ andσ2 =E[h2 ]−µ2,∀i,∀k,weprovidehowourgradient
h h i,k h
estimates and algorithms can be adjusted in Appendix D
to account for this case. In fact, non-symmetrical channel
models (e.g., Rician) offer a simplification of both analysis
and implementation in comparison to symmetrical models
(e.g., Rayleigh), as the non-zero mean no longer cancels out
the gradient, and the design is further independent of the Fig. 2: Accuracy evolution of 1P-ZOFL vs. FedAvg for
autocorrelation of the channels. IID data and non-IID distribution.
IV. EXPERIMENTALRESULTS
For the experimental results, we test our algorithms in
nonconvex binary image classification problems, and we
compare them against the original FL algorithm FedAvg
[12] with exact gradient and one local update per round.
However, we do not consider the effect of the channel or any
noise/stochasticity for the FedAvg algorithm. All experiments
are done for 100 devices and data batches of 10 images per
user per round. Every communication round in the graphs
include all steps 2 through 7 for Algorithm 1.
We classify photos of the two digits “0” and “1” from the
MNIST dataset [37] using a nonconvex logistic regression
model with a regularization parameter of 0.001. All images
are divided equally among the devices and are considered
to be preprocessed by being compressed to have dimension Fig. 3: Accuracy evolution of 1P-ZOFL for σ2 =
n
d = 10 using a lossy autoencoder. We run our code on 50 {0.25,1,2.25,10.0489}.
simulations with different random model initializations testing
the accuracy in every iteration against an independent test set.
The graphs in Fig. 2 are averaged over all these simulations. V. CONCLUSION
For the non-IID data distribution, we first sort the images
according to their labels and then divide them among the This work considers a learning problem over wireless
devices. Φ is generated according to Example 1. All channels channels and proposes a new zero-order federated learning
k
aregeneratedusingthenormaldistributionwithautocovariance method with a one-point gradient estimator. We limit the
K = 1. The noise is Gaussian with 0 mean and variance communication to scalar-valued feedback from the devices and
hh 2
σ2 = 1. The step size for FedAvg is taken as η =0.15. For incorporate the wireless channel in the learning algorithm. We
n 4
1P-ZOFL, α = 0.5(1+k)−0.51 and γ = 2.5(1+k)−0.18. provide theoretical and experimental evidence for convergence
k k
Our algorithm performs consistently well with all the different and find an upper bound on the convergence rate.REFERENCES [18] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” IEEE Signal Processing
[1] J.C.Duchi,M.I.Jordan,M.J.Wainwright,andA.Wibisono,“Optimal
Magazine,vol.37,no.3,pp.50–60,2020.
rates for zero-order convex optimization: The power of two function [19] K.Yang,T.Jiang,Y.Shi,andZ.Ding,“Federatedlearningviaover-
evaluations,”IEEETransactionsonInformationTheory,vol.61,no.5, the-aircomputation,”IEEETransactionsonWirelessCommunications,
pp.2788–2806,2015. vol.19,no.3,pp.2022–2035,2020.
[20] M.M.AmiriandD.Gündüz,“Federatedlearningoverwirelessfading
[2] A. Agarwal, O. Dekel, and L. Xiao, “Optimal algorithms for online
channels,” IEEE Transactions on Wireless Communications, vol. 19,
convexoptimizationwithmulti-pointbanditfeedback,”inCOLT,2010.
no.5,pp.3546–3557,2020.
[3] A. Flaxman, A. T. Kalai, and H. B. McMahan, “Online convex
[21] T.SeryandK.Cohen,“Onanaloggradientdescentlearningovermultiple
optimization in the bandit setting: gradient descent without a
accessfadingchannels,”IEEETransactionsonSignalProcessing,vol.68,
gradient,” CoRR, vol. cs.LG/0408007, 2004. [Online]. Available:
pp.2897–2911,2020.
http://arxiv.org/abs/cs.LG/0408007
[22] H. Guo, A. Liu, and V. K. N. Lau, “Analog gradient aggregation
[4] W.LiandM.Assaad,“Distributedstochasticoptimizationinnetworks
forfederatedlearningoverwirelessnetworks:Customizeddesignand
withlowinformationalexchange,”IEEETransactionsonInformation
convergenceanalysis,”IEEEInternetofThingsJournal,vol.8,no.1,
Theory,vol.67,no.5,pp.2989–3008,2021.
pp.197–210,2021.
[5] E. Mhanna and M. Assaad, “Zero-order one-point estimate with
[23] T. Sery, N. Shlezinger, K. Cohen, and Y. C. Eldar, “Over-the-air
distributed stochastic gradient-tracking technique,” 2022. [Online].
federated learning from heterogeneous data,” IEEE Transactions on
Available:https://arxiv.org/abs/2210.05618
SignalProcessing,vol.69,pp.3796–3811,2021.
[6] A. Vemula, W. Sun, and J. Bagnell, “Contrasting exploration in
[24] Y.Sun,S.Zhou,Z.Niu,andD.Gündüz,“Dynamicschedulingforover-
parameterandactionspace:Azeroth-orderoptimizationperspective,”in
the-airfederatededgelearningwithenergyconstraints,”IEEEJournal
ProceedingsoftheTwenty-SecondInternationalConferenceonArtificial
onSelectedAreasinCommunications,vol.40,no.1,pp.227–242,2022.
IntelligenceandStatistics,vol.89. PMLR,16–18Apr2019,pp.2926–
[25] D. Tse and P. Viswanath, Fundamentals of Wireless Communication.
2935.
CambridgeUniversityPress,2005.
[7] D.Malik,A.Pananjady,K.Bhatia,K.Khamaru,P.Bartlett,andM.Wain- [26] E. Björnson and L. Sanguinetti, “Making cell-free massive mimo
wright, “Derivative-free methods for policy optimization: Guarantees competitivewithmmseprocessingandcentralizedimplementation,”IEEE
for linear quadratic systems,” in Proceedings of the Twenty-Second Transactions on Wireless Communications, vol. 19, no. 1, pp. 77–90,
InternationalConferenceonArtificialIntelligenceandStatistics,vol.89. 2020.
PMLR,16–18Apr2019,pp.2916–2925. [27] A.Khaled,K.Mishchenko,andP.Richtarik,“Tightertheoryforlocal
[8] A.Dhurandhar,T.Pedapati,A.Balakrishnan,P.-Y.Chen,K.Shanmugam, sgdonidenticalandheterogeneousdata,”inProceedingsoftheTwenty
andR.Puri,“Modelagnosticcontrastiveexplanationsforstructureddata,” ThirdInternationalConferenceonArtificialIntelligenceandStatistics,
2019. vol.108. PMLR,26–28Aug2020,pp.4519–4529.
[9] A.Ilyas,L.Engstrom,A.Athalye,andJ.Lin,“Black-boxadversarial [28] T. Chen, G. Giannakis, T. Sun, and W. Yin, “Lag: Lazily aggregated
attackswithlimitedqueriesandinformation,”inProceedingsofthe35th gradientforcommunication-efficientdistributedlearning,”inAdvances
InternationalConferenceonMachineLearning,vol.80. PMLR,10–15 inNeuralInformationProcessingSystems,vol.31. CurranAssociates,
Jul2018,pp.2137–2146. Inc., 2018. [Online]. Available: https://proceedings.neurips.cc/paper/
[10] X.Chen,S.Liu,K.Xu,X.Li,X.Lin,M.Hong,andD.Cox,“Zo-adamm: 2018/file/feecee9f1643651799ede2740927317a-Paper.pdf
Zeroth-orderadaptivemomentummethodforblack-boxoptimization,” [29] M.M.Amiri,D.Gündüz,S.R.Kulkarni,andH.V.Poor,“Convergence
inAdvancesinNeuralInformationProcessingSystems,vol.32. Curran ofupdateawaredeviceschedulingforfederatedlearningatthewireless
Associates,Inc.,2019. edge,”IEEETransactionsonWirelessCommunications,vol.20,no.6,
[11] K.Bonawitz,H.Eichner,W.Grieskamp,D.Huba,A.Ingerman,V.Ivanov, pp.3643–3658,2021.
C.Kiddon,J.Konecˇný,S.Mazzocchi,B.McMahan,T.VanOverveldt, [30] J.Konecˇný,H.B.McMahan,F.X.Yu,P.Richtárik,A.T.Suresh,and
D.Petrou,D.Ramage,andJ.Roselander,“Towardsfederatedlearningat D.Bacon,“Federatedlearning:Strategiesforimprovingcommunication
scale:Systemdesign,”inProceedingsofMachineLearningandSystems, efficiency,”2016.[Online].Available:https://arxiv.org/abs/1610.05492
vol.1,2019,pp.374–388. [31] S. Khirirat, H. R. Feyzmahdavian, and M. Johansson, “Distributed
[12] B.McMahan,E.Moore,D.Ramage,S.Hampson,andB.A.y.Arcas, learning with compressed gradients,” 2018. [Online]. Available:
“Communication-EfficientLearningofDeepNetworksfromDecentralized https://arxiv.org/abs/1806.06573
Data,”inProceedingsofthe20thInternationalConferenceonArtificial [32] A.Elgabli,J.Park,A.S.Bedi,M.Bennis,andV.Aggarwal,“Q-gadmm:
IntelligenceandStatistics,vol.54. PMLR,20–22Apr2017,pp.1273– Quantized group admm for communication efficient decentralized
1282. machinelearning,”inICASSP2020-2020IEEEInternationalConference
[13] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. onAcoustics,SpeechandSignalProcessing(ICASSP),2020,pp.8876–
Bhagoji,K.Bonawit,Z.Charles,G.Cormode,R.Cummings,R.G.L. 8880.
D’Oliveira,H.Eichner,S.ElRouayheb,D.Evans,J.Gardner,Z.Garrett, [33] K.Mishchenko,E.Gorbunov,M.Takácˇ,andP.Richtárik,“Distributed
A. Gascón, B. Ghazi, P. B. Gibbons, M. Gruteser, Z. Harchaoui, learningwithcompressedgradientdifferences,”2019.[Online].Available:
C. He, L. He, Z. Huo, B. Hutchinson, J. Hsu, M. Jaggi, T. Javidi, https://arxiv.org/abs/1901.09269
G.Joshi,M.Khodak,J.Konecný,A.Korolova,F.Koushanfar,S.Koyejo, [34] Y.Chen,R.S.Blum,M.Takácˇ,andB.M.Sadler,“Distributedlearning
T.Lepoint,Y.Liu,P.Mittal,M.Mohri,R.Nock,A.Özgür,R.Pagh, withsparsifiedgradientdifferences,”IEEEJournalofSelectedTopicsin
H. Qi, D. Ramage, R. Raskar, M. Raykova, D. Song, W. Song, S. U. SignalProcessing,vol.16,no.3,pp.585–600,2022.
Stich,Z.Sun,A.TheerthaSuresh,F.Tramèr,P.Vepakomma,J.Wang, [35] W. Fang, Z. Yu, Y. Jiang, Y. Shi, C. N. Jones, and Y. Zhou,
L.Xiong,Z.Xu,Q.Yang,F.X.Yu,H.Yu,andS.Zhao,2021. “Communication-efficientstochasticzeroth-orderoptimizationforfed-
[14] X.Zhang,M.Hong,S.Dhople,W.Yin,andY.Liu,“Fedpd:Afederated eratedlearning,”IEEETransactionsonSignalProcessing,vol.70,pp.
learningframeworkwithadaptivitytonon-iiddata,”IEEETransactions 5058–5073,2022.
onSignalProcessing,vol.69,pp.6055–6070,2021. [36] Z.Dai,B.K.H.Low,andP.Jaillet,“Federatedbayesianoptimization
[15] J.Wang,Q.Liu,H.Liang,G.Joshi,andH.V.Poor,“Anovelframework viathompsonsampling,”inAdvancesinNeuralInformationProcessing
fortheanalysisanddesignofheterogeneousfederatedlearning,”IEEE Systems,vol.33. CurranAssociates,Inc.,2020,pp.9687–9699.
TransactionsonSignalProcessing,vol.69,pp.5234–5249,2021. [37] Y.LeCunandC.Cortes,“TheMNISTdatabaseofhandwrittendigits,”
[16] A.Elgabli,C.B.Issaid,A.S.Bedi,K.Rajawat,M.Bennis,andV.Ag- 2005.
garwal, “FedNew: A communication-efficient and privacy-preserving [38] J.L.Doob,“Stochasticprocesses,”1953.
Newton-typemethodforfederatedlearning,”inProceedingsofthe39th
InternationalConferenceonMachineLearning,vol.162. PMLR,17–23
Jul2022,pp.5861–5877.
[17] T.Li,A.K.Sahu,M.Zaheer,M.Sanjabi,A.Talwalkar,andV.Smithy,
“Feddane: A federated newton-type method,” in 2019 53rd Asilomar
ConferenceonSignals,Systems,andComputers,2019,pp.1227–1231.(cid:88)
=x a cos(φ (t))(1+cos(4πf t))
APPENDIXA i i c
ONTHECHANNELMODEL i
(cid:88)
√ (7)
−x a sin(φ (t))sin(4πf t)+ 2w(t)cos(2πf t)
i i c c
AsdescribedinSection2.2ofChapter2of[25]andconsider-
i
ingadouble-sidebandsuppressedcarrieramplitudemodulation
(DSB-SC) instead of quadrature amplitude modulation (QAM): Afterthelowpassfilter,weobtainthereceivedbasebandsignal
Having a baseband signal x, to send it over the channel, we
√
(cid:88)
modulate (multiply) it by 2cos2πf ct where f c is the carrier y =x a icos(φ i(t))+n(t)
frequency and t is the time index.
i
(8)
When sent over the channel, the transmitted signal x =R[hˆ(t)]x+n(t)
undergoes perturbation and thus the received signal becomes:
=h(t)x+n(t)
√
(cid:88)
z = 2 a xcos(2πf t+φ (t))+w(t), (5)
i c i
where n(t) is the baseband equivalent noise with a zero-mean
i
Gaussian distribution and IID components (Section 2.2.4 of
where a i is the amplitude attenuation of path i and φ i(t) = [25]) and R[hˆ(t)]=I(t) is the real part of the channel.
2πf t+φ is the phase shift incurred by Doppler frequency
l l As we are interested to send real values over the wireless
shift f and/or any time delay φ . w(t) is an additive noise.
l l channel in this paper, one can easily see how equation (8) is
By developing the cosine term in z, we obtain validtousewitharealchannelh=R[hˆ]followingaGaussian
√ (cid:88) distribution with zero mean and variance equal to σ2.
z =x 2 a cos(φ (t)) cos(2πf t)
i i c
i
(cid:124) (cid:123)(cid:122) (cid:125)
APPENDIXB
in-phasecomponent,I(t)
√ (cid:88) (6) ONTHEESTIMATEDGRADIENT
−x 2 a sin(φ (t)) sin(2πf t)+w(t),
i i c
i
(cid:124) (cid:123)(cid:122) (cid:125) A. Proof of Lemma 1: Biased Estimator
quadraturecomponent,Q(t)
Let g have the form in (3), then the conditional expectation
k
From Section 2.4.2 of [25]: According to the central limit
given H can be written as
k
theorem, if there is a large number of channel paths, the in-
phaseandquadraturecomponentsofthereceivedsignal,which
E[g |H ]
k k
are not correlated with each other, will exhibit distributions
that resemble the normal (Gaussian) distribution. Specifically, =E(cid:104) Φ (cid:88)N (cid:16) h f˜(cid:0) θ′,S (cid:1) +n (cid:17)(cid:12) (cid:12)H (cid:105)
each component will have an average value of zero and a k i,k+1 i k i,k+1 i,k+1 (cid:12) k
variance of Σ/2, which is equivalent to σ2. The magnitude i=1
of the perturbation (cid:112) I(t)2+Q(t)2 thus becomes Rayleigh ( =a)E(cid:104) Φ (cid:88)N h F˜(cid:0) θ +γ Φ (cid:88)N (cid:0)h j,k +n (cid:1)(cid:1)(cid:12) (cid:12)H (cid:105)
distributed. This is the Rayleigh fading model. In addition, k i,k+1 i k k k σ2 j,k (cid:12) k
i=1 j=1 h
whentheline-of-sightpathislargeandhasaknownmagnitude,
N
the probabilistic model becomes a Rician fading. ( =b)E(cid:104) Φ (cid:16)(cid:88) h F˜(θ )
k i,k+1 i k
Furthermore, as I(t) and Q(t) are orthogonal, an equivalent
i=1
complex channel model hˆ(t) = I(t)+jQ(t) = a(t)e−jφ(t) N N
can be derived. Since the carrier frequency f is not involved +γ (cid:88) h (cid:88)(cid:16)h j,k +n (cid:17) ΦT∇F˜(θ )
in hˆ(t), this representation is valid at basebc and level. Thus, k i=1 i,k+1 j=1 σ h2 j,k k i k
t rh ece ec ivo em dp sl ie gx nac lh hˆa (n tn )e xl +m nod (te )l ais
t
bu as su ea bl aly ndus we id thto hˆ(r te )p are cs oen mt pt lh exe +γ2(cid:88)N
h
(cid:16)(cid:88)N h
j,k +n
(cid:17)2
ΦT∇2F˜(θ˘ )Φ
(cid:17)(cid:12)
(cid:12)H
(cid:105)
k i,k+1 σ2 j,k k i k k (cid:12) k
entity. i=1 j=1 h
basC eo bn anti dnu rein ceg ivf er dom sign( a6 l) y, ,t zo isd fie rm sto mdu ul la tit pe liez dba ynd √ 2ob cota si 2n πfth ce t ( =c)E(cid:104) Φ k(cid:18) σγ k
2
(cid:88)N h i,k+1h i,kΦT k∇F˜ i(θ k)+
then the result goes through a low pass filter. h i=1
√ +γ
k2(cid:88)N
h
i,k+1(cid:16)(cid:88)N h
σj 2,k +n
j,k(cid:17)2
ΦT k∇2F˜ i(θ˘ k)Φ
k(cid:17)(cid:12)
(cid:12) (cid:12)H
k(cid:105)
z 2cos(2πf ct) i=1 j=1 h
=2x(cid:88) a icos(φ i(t))cos2(2πf ct) =γ
k
(cid:88)N E(cid:104)
h h
(cid:12)
(cid:12)H
(cid:105) E(cid:104)
Φ
ΦT(cid:12)
(cid:12)H
(cid:105)
∇F˜(θ )+
i σ2 i,k+1 i,k(cid:12) k k k(cid:12) k i k
(cid:88) h i=1
−2x a sin(φ (t))sin(2πf t)cos(2πf t)
√ i
i i c c
γ
k2(cid:88)N E(cid:104)
h
i,k+1(cid:16)(cid:88)N h
σj 2,k +n
j,k(cid:17)2
Φ kΦT k∇2F˜ i(θ˘ k)Φ
k(cid:12)
(cid:12) (cid:12)H
k(cid:105)
+ 2w(t)cos(2πf ct) i=1 j=1 h( =d) γ kα 2K σh 4h (cid:88)N ∇F i(θ k)+ ( =f)3N σ2 h2α 32(cid:16) µ S +L S∥θ k∥2(cid:17)
γ
k2(cid:88)N Eh (cid:104) hi i= ,k1 +1(cid:16)(cid:88)N h
σj 2,k +n
j,k(cid:17)2
Φ kΦT k∇2F˜ i(θ˘ k)Φ
k(cid:12)
(cid:12) (cid:12)H
k(cid:105) +3N2α 34L Sγ k2(cid:0)(N −
σ
h81)σ h4 + σ h4 + σ2 h8K h2 h + N σσ h2n2 +0(cid:1)
i=1 j=1 h +N2α 32σ n2
(e) :=c ,
=c γ (∇F(θ )+b ), 2
1 k k k
(10)
(9)
where(a)isbyAssumption2andCauchy-Schwartz((cid:80)N
1·
where (a) is by the definition in (2) and due to Assumption 1, j=1
(b)isbyTaylorexpansionandmean-valuetheoremandconsid- a i)2 ≤ ((cid:80)N j=1a2 i)·((cid:80)N j=112) = N((cid:80)N j=1a2 i), for real a i.
ering θ˘ between θ and θ +γ Φ (cid:80)N (cid:0)hj,k +n (cid:1) . (c) is (b) is due the independence of noise, (c) is by Assumption
sinceEk
[h
]=0k forthek firstk elek menj t= a1 ndEσ h2
[h
j, hk
]=0
3, and (d) is again by Cauchy-Schwartz. In (e), we let µ
S
=
i,k+1 i,k+1 j,k max E[∥f (0,S )∥2|H ] and L = max E[L2 |H ].
when i ̸= j and the independence of noise for the second i i i,k+1 k S i Si,k+1 k
e al ne dm (cid:17)e
t
2hn et.( bd i) asis bd kue =toA γs ks αu (cid:12)2m
σ
Kh2p ht
h
(cid:105)io (cid:80)n2
N
i. =I 1n E( (cid:104)e h), i,w k+e 1le (cid:16)t (cid:80)c 1
N
j= =1α
h
σ2
j
h2,K kσh h4 +h ( 2E hf K[) h h22 ii hs
, ak ,
nd
+
dau
1
ne (cid:80) hdto Ej̸=t [h whie
2
ih
i,
ln k2
j
lo
, +k
ar 1] lm w(cid:80)=a al yl j(y s<N- ld bhi e−s jt i,r nk1i db h)u eσ
l,
pt kh4e e],d niE dc
s
eh [h
e
na
q
t2 in
,
ukn oa+e fll
1
ttr hh oa e2 in
,
zkd oe|o tH
r
hm
o ek
ra]v
s
ta = er
o
ri ma nσb
e
sh4le
o
i+s nf,
n Φ ΦT∇2F˜(θ˘ )Φ (cid:12)H . j,k l,k
j,k k k i k k(cid:12) k the product and has a mean equal to zero.
C. Proof of Lemma 4: Norm of the Bias
B. ProofofLemma2:ExpectedNormSquaredoftheEstimated
Gradient Considering the form of the bias in (9), by Assumptions 1,
2 and 4,
To bound the norm squared of the one-point gradient
estimate,westartbylettingθ k′ =θ k+γ kΦ k(cid:80)N j=1(cid:0)h σj 2,k+n j,k(cid:1) ∥b k∥
h
E[∥g k∥2|H k]
( ≤a)
γ
kασ Kh2 (cid:88)N E(cid:104) 2N(cid:12)
(cid:12) (cid:12)h
i,k+1(cid:88)N (cid:16)h σ2
j 4,k +n2
j,k(cid:17)(cid:12)
(cid:12) (cid:12)×
2 hh i=1 j=1 h
=E(cid:104) ∥Φ ∥2(cid:16)(cid:88)N h f˜(cid:0) θ′,S (cid:1) +n (cid:17)2(cid:12) (cid:12)H (cid:105) ∥Φ ∥∥ΦT∥∥∇2F˜(θ˘ )∥∥Φ ∥(cid:12) (cid:12)H (cid:105)
k i,k+1 i k i,k+1 i,k+1 (cid:12) k k k i k k (cid:12) k
i=1
( ≤a) α2N(cid:88)N E(cid:104)(cid:16) h f˜(cid:0) θ′,S (cid:1) +n (cid:17)2(cid:12) (cid:12)H (cid:105) ≤2γ kN αα 1 Kα 33σ h2 (cid:88)N E(cid:104)(cid:12) (cid:12) (cid:12)h i,k+1(cid:88)N (cid:16)h σ2 j 4,k +n2 j,k(cid:17)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)H k(cid:105)
3 i,k+1 i k i,k+1 i,k+1 (cid:12) k 2 hh i=1 j=1 h
i=1
( =b) α 32N(cid:88)N E(cid:104) h2 i,k+1f˜ i2(cid:0) θ k′,S i,k+1(cid:1) +n2 i,k+1(cid:12) (cid:12) (cid:12)H k(cid:105) ( ≤b) 2γ kN αα 21 Kα h33 hσ h2 (cid:88)N (cid:104) σ h(cid:114) π2(cid:16) 2K hh+(cid:113) σ h4 −K h2 h(cid:17)
i=1
i=1 (cid:114) (cid:114)
2 2 (cid:105)
( ≤c) α2N(cid:88)N E(cid:104)h2 i,k+1(cid:16)
∥f (0,S )∥+L
∥θ′∥(cid:17)2(cid:12)
(cid:12)H
(cid:105) +(N −1)σ h3
π
+N πσ hσ n2
3
i=1
σ h4 i i,k+1 Si,k+1 k (cid:12) k
N2α
α3σ3(cid:114)
2
+N2α 32σ n2 =2γ k α 2K1 h3
h
h π×
≤α 32N(cid:88)N E(cid:104)h2 i σ,k 4+1(cid:16) ∥f i(0,S i,k+1)∥+L Si,k+1∥θ k∥ (cid:104)(cid:16) 2K hh+(cid:113) σ h4 −K h2 h(cid:17) +(N −1)σ h2 +Nσ n2(cid:105)
i=1 h (c)
=c γ ,
+L γ ∥Φ
∥(cid:12) (cid:12)(cid:88)N h
j,k +n
(cid:12) (cid:12)(cid:17)2(cid:12)
(cid:12)H
(cid:105)
+N2α2σ2
3 k
(11)
Si,k+1 k k (cid:12) σ2 j,k(cid:12) (cid:12) k 3 n
j=1 h where (a) is due to Jensen’s inequality and Cauchy-Schwartz,
( ≤d) 3α2N(cid:88)N E(cid:104)h2 i,k+1(cid:16)
∥f (0,S )∥2+L2 ∥θ ∥2
(b) is by using the half-normal distribution for normal random
3 σ4 i i,k+1 Si,k+1 k variablesinabsolutevalueexplainedinthefollowingparagraph,
+αi= 321 L2 Si,k+1γh k2(cid:0)(cid:88)N h σj 2,k +n j,k(cid:1)2(cid:17)(cid:12) (cid:12) (cid:12)H k(cid:105) +N2α 32σ n2 (a Nnd −in 1)( σc h2), +c 3 N= σ n22 (cid:105) .N α2α 2K1α h3 3 hσ h3(cid:113) π2(cid:104)(cid:16) 2K hh +(cid:112) σ h4 −K h2 h(cid:17) +
j=1 h Let X and Y be two variables representing time-correlated
( ≤e)3N2α 32(cid:16)
µ +L ∥θ
∥2(cid:17)
+3Nα4L
γ2(cid:88)N E(cid:104)h2
i,k+1×
channel realizations at times k and k′ respectively. Assume
σ2 S S k 3 S k σ4 theyfollowtheN(0,σ)distributionandtheyhaveacorrelation
h i=1 h coefficientϱ.Then,wecanwriteY =ϱX+(cid:112) 1−ϱ2Z,where
(cid:0)(cid:88)h2
j,k +
h2
i,k +n2
+2(cid:88)h j,kh l,k(cid:1)(cid:12)
(cid:12)H
(cid:105)
+N2α2σ2 Z is independent of X and following the same distribution
σ4 σ4 j,k σ4 (cid:12) k 3 n N(0,σ). Then, E[|YX2|] ≤ E[ϱ|X3| + (cid:112) 1−ϱ2|ZX2|] =
j̸=i h h j<l h(cid:113) (cid:112) (cid:113) (cid:112) (cid:113)
2ϱ 2σ3+ 1−ϱ2 2σ×σ2 =(2ϱ+ 1−ϱ2) 2σ3. If B. Proof of Theorem 1: Convergence Analysis
π π π
we substitute σ = σ h and ϱ = K σh 2h, we obtain the previous By the L-smoothness assumption and the algorithm update
h
inequality (b). step θ =θ −α g , we have
k+1 k k k
APPENDIXC F(θ )
k+1
1P-ZOFLALGORITHMCONVERGENCE α2L
≤F(θ )−α ⟨∇F(θ ),g ⟩+ k ∥g ∥2
A. Stochastic Noise k k k k 2 k
To prove Lemma 5, we begin by demonstrating that the α2L
=F(θ )−α ⟨∇F(θ ),e +E[g |H ]⟩+ k ∥g ∥2
sequence {(cid:80)K′ α e } is a martingale. To do so, we k k k k k k 2 k
k=K k k K′≥K
have to prove that for all K′ ≥ K, X
K′
= (cid:80)K k=′ Kα ke
k
=F(θ k)−α k⟨∇F(θ k),e k⟩−c 1α kγ k∥∇F(θ k)∥2
satisfies the following two conditions: α2L
−c α γ ⟨∇F(θ ),b ⟩+ k ∥g ∥2
(i) E[X K′+1|X K′]=X K′ 1 k k k k 2 k
( Wii e) E[ k∥ nX owK′∥2 th] a<
t
∞
E[e ] = E[g − E[g |H ]] =
( ≤a)
F(θ k)−α k⟨∇F(θ k),e k⟩−c 1α kγ k∥∇F(θ k)∥2
E Hk(cid:104) E(cid:104) g k − E[g k|H k](cid:12) (cid:12) (cid:12)Hk k(cid:105)(cid:105) = 0 bk y th (cid:104)e lak w ok f total + c 1α 2kγ k∥∇F(θ k)∥2+ c 1α 2kγ k∥b k∥2+ α k2 2L ∥g k∥2
expectation. Hence, E[X |X ] = E α e + c α γ
K′+1 K′ K′+1 K′+1 =F(θ )−α ⟨∇F(θ ),e ⟩− 1 k k∥∇F(θ )∥2
(cid:80)K′
α e
(cid:12) (cid:12)(cid:80)K′
α e
(cid:105) =0+(cid:80)K′
α e =X .
k k k k 2 k
k=K k k(cid:12) k=K k k k=K k k K′ c α γ α2L
In addition, e and e are uncorrelated for any k ̸= k′ + 1 k k∥b ∥2+ k ∥g ∥2
s Ei (cid:2)n ece E(a [es Tsu |Hmin ](cid:3)gk =k 0>
.
Thkk u′′ ) s,E(cid:2) eT ke k′(cid:3) = E(cid:2)E[eT ke k′|H k](cid:3) = 2 k 2 k (14)
k′ k k
where (a) is by −⟨a,b⟩≤ 1∥a∥2+ 1∥b∥2.
K′ K′ K′ 2 2
E(∥ (cid:88) α e ∥2)=E((cid:88) (cid:88) α α ⟨e ,e ⟩) By taking the telescoping sum, we get
k k k k′ k k′
k=K
(
=a)E(k= (cid:88)KK
′
k ∥′= αK
e ∥2)
F(θ∗)≤F(θ K+1)≤F(θ 0)−
c
21
(cid:88)K
α kγ k∥∇F(θ k)∥2
k k k=0
k=K K
(cid:88)
∞ − α ⟨∇F(θ ),e ⟩
(cid:88) k k k
≤ E(α2∥g −E[g |H ]∥2)
k k k k k=0
k=K K K
=
(cid:88)∞
α2E(∥g ∥2)−E (∥E[g |H ]∥2)
+ c 21 (cid:88) α kγ k∥b k∥2+ c 2 2L(cid:88) α k2∥g k∥2
k k Hk k k k=0 k=0
k=K (15)
∞ ∞
(cid:88) (b) (cid:88) (c)
≤ α2E(∥g ∥2) ≤ c α2γ2 < ∞, Hence,
k k 2 k k
k=K k=K
K
(12) (cid:88) 2
α γ ∥∇F(θ )∥2 ≤ (F(θ )−F(θ∗))
where (a) is due to the uncorrelatedness E[⟨e ,e ⟩]=0, (b) k k k c 1 0
k k′ k=0
isbyLemma2,and(c)isbyAssumption5.Therefore,both(i) K
and (ii) are satisfied and we can say that {(cid:80)K k=′ Kα ke k}
K′≥K
− c2 (cid:88) α k⟨∇F(θ k),e k⟩
is a martingale. This permits us to use Doob’s martingale 1 k=0
ine Fq ou ral ai nty y[ c3 o8 n] s:
tant ν >0,
+(cid:88)K
α kγ k∥b k∥2+
c c2L(cid:88)K
α k2∥g k∥2
1
k=0 k=0
(cid:88)K′ 1 (cid:88)K′ (16)
P( sup ∥ α e ∥≥ν)≤ E(∥ α e ∥2)
K′≥K k=K
k k ν2
k=K
k k
(13) By Assumption 3, ∥∇F(θ k)∥ is bounded for any θ k ∈Rd and
( ≤a) c
2
(cid:88)∞
α2γ2,
by taking the summation in (13) between 0 and ∞, we have
ν2 k k
K
k=K (cid:88)
lim ∥ α ⟨∇F(θ ),e ⟩∥<∞. (17)
where (a) is following the exact same steps as (12). K→∞ k k k
Sincec isaboundedconstantandlim (cid:80)∞ α2γ2 = k=0
2 K→∞ k=K k k
0 by Assumption 5, we get lim
K→∞
νc2
2
(cid:80)∞ k=Kα k2γ k2 = 0 From Lemma 4, we know that ∥b k∥2 ∼ γ k2. Hence, by
for any bounded constant ν. Hence, the probability that Assumption 5, lim (cid:80)K α γ3 <∞.
∥(cid:80)K′
α e ∥ ≥ ν also vanishes as K → ∞, which From Lemma
K 2→ a∞
nd
bk= y0 lok okk
ing closely at the use
k=K k k
concludes the proof. of the Lipschitz continuity property in (10), we can say(a) c α γ
∥g ∥2 ≤ c for some c > 0. Thus, again by Assumption 5, ≤F(θ )−c α γ ∥∇F(θ )∥2+ 1 k k∥∇F(θ )∥2
limk (cid:80)K α2 <∞. We conclude that k 1 k k k 2 k
K→∞ k=0 k c α γ α2Lc
+ 1 k k∥b ∥2+ k 2
(cid:88)K 2 k 2
lim α kγ k∥∇F(θ k)∥2 <∞. (18) c α γ c α γ α2Lc
K→∞ =F(θ )− 1 k k∥∇F(θ )∥2+ 1 k k∥b ∥2+ k 2
k=0 k 2 k 2 k 2
Moreover, since the series (cid:80) α γ diverges by Assumption (22)
k k k
5, we have lim inf∥∇F(θ )∥=0.
k→∞ k where (a) is by −⟨a,b⟩≤ 1∥a∥2+ 1∥b∥2
To prove that lim ∥∇F(θ )∥ = 0, we consider the 2 2
k→∞ k By considering a large value K > 0 and taking the
hypothesis H) lim sup∥∇F(θ )∥ ≥ ρ for an arbitrary
k→∞ k telescoping sum of (22), we get
ρ>0.
Assume (H) to be true. Then, we can always find an E[F(θ K+1)|H K]
(cid:0) (cid:1)
a thr ab tit ∥ra ∇ry F(s θu kb ls )e ∥qu ≥en ρc −e ε,∥∇ ∀lF , f( oθ rkl ρ)∥ −l ε∈N >o 0f a∥ n∇ dF ε( >θ k 0) .∥, such ≤F(θ 0)− c 21 (cid:88)K α kγ k∥∇F(θ k)∥2+ c 21 (cid:88)K α kγ k∥b k∥2
Then,bytheL-smoothnesspropertyandapplyingthedescent k=0 k=0
step of the algorithm,
+
Lc
2
(cid:88)K
α2.
∥∇F(θ )∥≥∥∇F(θ )∥−∥∇F(θ )−∇F(θ )∥ 2 k
kl+1 kl kl+1 kl
k=0
≥ρ−ε−L∥θ −θ ∥ (23)
kl+1 kl
=ρ−ε−Lα √kl∥g kl∥ Given the assumption that F(θ∗)=min θ∈RdF(θ) exists, we
≥ρ−ε−L cα kl, know that δ k =F(θ k)−F(θ∗)≥0. Then,
(19)
K
Sincek
l
→∞asl→∞,wecanalwaysfindasubsequence 0≤E[δ K+1|H K]≤δ 0− c 21 (cid:88) α kγ k∥∇F(θ k)∥2
o cof n( sk il dp e) rp∈ (N k l)su l∈c Nh sth taa rt tik nl gp+ f1 ro− mk l αp k>
l
<1. LρA − √s ε cα .k Tl hi us sv ,anishing, we
+
c 1c
3
(cid:88)k K=0
α γ3+
Lc
2
(cid:88)K
α2.
(24)
2 k k 2 k
∞
(cid:88) k=0 k=0
α γ ∥∇F(θ )∥2
k+1 k+1 k+1 Finally,
k=0
(cid:88)∞ √ (cid:88)∞ (cid:88) α γ E[∥∇F(θ )∥2]≤ 2 δ +(cid:88) α γ3+ Lc 2 (cid:88) α2
≥(ρ−ε)2 α k+1γ k+1−2(ρ−ε)L c α k+1γ k+1α k k k k c 1c 3 0 k k c 1 k
k k k
k=0 k=0 (25)
∞
(cid:88)
+L2c α γ α2 Letα andγ havetheformsgiveninExample2.Weknow
k+1 k+1 k k k
k=0 that, ∀K >0,
=+∞, K K
(cid:88) (cid:88)
(20) α γ3 =α γ3+ α γ3
k k 0 0 k k
as the first series diverges, and the second and the third k=0 k=1
converge by Assumption 5. This implies that the series
≤α
γ3(cid:16) 1+(cid:90) K (x+1)−υ1−3υ2dx(cid:17)
(cid:80) α γ ∥∇F(θ )∥2 diverges. This is a contradiction as this 0 0 (26)
k k k k 0
series converges almost surely by (18). Therefore, hypothesis (cid:16) 1 (cid:17)
≤α γ3 1+
(H) cannot be true and ∥∇F(θ )∥ converges to zero almost 0 0 υ +3υ −1
k 1 2
surely. (cid:16) υ +3υ (cid:17)
=α γ3 1 2 .
0 0 υ +3υ −1
C. Proof of Theorem 2: Convergence Rate 1 2
Starting again from the L-smoothness in Lemma 3 and the Similarly, (cid:80)K k=0α k2 ≤ α 02(cid:0) 2υ2 1υ −1 1(cid:1) . Next, when further 0 <
algorithm update step θ k+1 =θ k−α kg k, we have υ 1+υ 2 <1,
α2L (cid:88)K (cid:90) K+1
F(θ k+1)≤F(θ k)−α k⟨∇F(θ k),g k⟩+ k
2
∥g k∥2. (21) α kγ
k
≥α 0γ
0
(x+1)−υ1−υ2dx
k=0 0 (27)
Taking the conditional expectation given H k,
=
α 0γ
0
(cid:16)
(K+2)1−υ1−υ2
−1(cid:17)
.
F(θ k+1) (1−υ 1−υ 2)
α2Lc Thus, making use of inequality (25)
≤F(θ )−c α γ ⟨∇F(θ ),∇F(θ )+b ⟩+ k 2
k 1 k k k k k 2 (cid:80) α γ E[∥∇F(θ )∥2] A(1−υ −υ )
=F(θ k)−c 1α kγ k∥∇F(θ k)∥2−c 1α kγ k⟨∇F(θ k),b k⟩ k k (cid:80)k
kα kγ
k
k ≤ (K+2)1−1 υ1−υ22
−1
(28)
α2Lc
+ k
2
2 with A= c1α2 0γ0δ 0+c2 3γ 02(cid:0) υ1υ +1+ 3υ3 2υ −2 1(cid:1) + L cc 12 γα 00(cid:0) 2υ2 1υ −1 1(cid:1)APPENDIXD
≤3α2N(cid:88)N E(cid:20)h2 i,k+1(cid:16)
∥f (0,S )∥2+L2 ∥θ ∥2
NON-SYMMETRICALCHANNELS 3
i=1
σ h4 i i,k+1 Si,k+1 k
(cid:17)(cid:12) (cid:21)
σ2A =ssu Em [i hn 2g n ]o −n- µsy 2m
,
m ∀ie ,t ∀ri kc ,al thch ea on nne el -s pow init th gE r[ ah di i, ek n] t= esµ th ima an td
e
+α 32L2 Si,k+1γ k2 (cid:12) (cid:12)H k +N2α 32σ n2
h i,k h
becomes =3N2α 32(σ h2 +µ2 h)(cid:16) µ +L ∥θ ∥2+α2L γ2(cid:17) +N2α2σ2
σ2 S S k 3 S k 3 n
N (cid:20) (cid:21) h
g =Φ
(cid:88)
h
f˜(cid:0)
θ +γ Φ ,S
(cid:1)
+n . (29)
:=c 2.
k k i,k+1 i k k k i,k+1 i,k+1
(31)
i=1
APPENDIXE
And 1P-ZOFL is updated to Algorithm 2. We then analyze
EXPERIMENTALRESULTSDETAILS
Algorithm 2 The 1P-ZOFL algorithm with non-symmetrical A. Autoencoder
channels We compress images from the MNIST dataset using a lossy
Input: Initial model θ ∈ Rd, the initial step- autoencoder with encoder-decoder architecture. The encoder
0
sizes α and γ , and the channels’ standard deviation reducestheinputsizefrom784to10throughthreelinearlayers
0 0
σ with dimensions 784-512-128-10. The first two layers have
h
1: for k =0,2,4,... do ELU activation functions. The decoder reverses this process,
2: The server broadcasts θ k+γ kΦ k going from 10 to 784 through three linear layers (10-128-512-
3: Theserverreceives(cid:80)N i=1h i,k+1f˜ i(cid:0) θ k+γ kΦ k,S i,k+1(cid:1) + 784), with ELU activation in the first two layers and a sigmoid
n activation in the last layer to ensure output pixel intensities
i,k+1
4: The server multiplies the received scalar sum by Φ k to between0and1.TrainingisdoneontheMNISTdatasetfor10
assemble g in (29) epochs, using mean squared error loss and the Adam optimizer.
k
5: The server updates θ k+1 =θ k−α kg k B. Performance of 1P-ZOFL vs SNR
6: end for
An important remark is that high SNR is generally needed
when we must decode the information in the received signal.
the properties of our modified gradient estimate:
In our case, nothing is decoded; there is no channel estimation
E[g |H ] or gradient extraction from the received signal. Rather, the
k k
received signal is fed directly into the learning (the channel is
=E(cid:104) Φ (cid:88)N (cid:16) h f˜(cid:0) θ +γ Φ ,S (cid:1) +n (cid:17)(cid:12) (cid:12)H (cid:105) part of the learning). Ultimately, the amount of noise present
k i,k+1 i k k k i,k+1 i,k+1 (cid:12) k in the system does not affect our algorithms’ convergence:
i=1
Examining (10) and (11), we see that the noise variance σ2
=E(cid:104) Φ (cid:88)N µ F˜(cid:0) θ +γ Φ (cid:1)(cid:12) (cid:12)H (cid:105) might only increase the norms of the estimate and bias, but in f
k h i k k k (cid:12) k
we refer to (14), we find that both terms are multiplied by step
i=1
sizes, and we can counter the noise effect by decreasing the
=µ hγ
k(cid:88)N E(cid:104)
Φ kΦT k∇F˜ i(θ k)+γ kΦ kΦT k∇2F˜ i(θ˘ k)Φ
k(cid:12)
(cid:12) (cid:12)H
k(cid:105)
step sizes’ constant parts (i.e, in the terms c1α 2kγk∥b k∥2 and
i=1 α2 kL∥g(1P)∥2). For the different plots in Fig. 3, we decrease
2 k
=c 1γ k(∇F(θ k)+b k), α
0
and γ
0
when we increase σ n2. However, this change of step
(30) sizes generally affects the rate of convergence as they enter in
the structure of g and b themselves and as can be seen in
(cid:104) (cid:12) (cid:105) k k
with c 1 = µ σh hα 22 and b k = αγk 2E Φ kΦT k∇2F i(θ˘ k)Φ k(cid:12) (cid:12)H k . ( p4 a) r, ts om fa tl hle er uα p0 pea rnd boγ u0 ni dnc or nea ts he es t ch oe nvfi er rs gt et ner cm
e
ri an teth .e Tc ho un s,st ta hn et
Then, ∥b k∥≤ αγk 2α 33α 1, now c
3
= α α3 3α 21 and ∥b k∥≤c 3γ k.
effect shown on the plots in Fig. 3 is logical given the changes
Let θ′ =θ +γ Φ ,
k k k k of α 0 and γ 0.
E[∥g ∥2|H ]
k k
=E(cid:20)
∥Φ
∥2(cid:16)(cid:88)N
h
f˜(cid:18)
θ′,S
(cid:19)
+n
(cid:17)2(cid:12)
(cid:12)H
(cid:21)
k i,k+1 i k i,k+1 i,k+1 (cid:12) k
i=1
=α2N(cid:88)N E(cid:20)
h2
f˜2(cid:18)
θ′,S
(cid:19)
+n2
(cid:12)
(cid:12)H
(cid:21)
3 i,k+1 i k i,k+1 i,k+1(cid:12) k
i=1
≤α2N(cid:88)N E(cid:20)h2 i,k+1(cid:16)
∥f (0,S )∥+L
∥θ′∥(cid:17)2(cid:12)
(cid:12)H
(cid:21)
3 σ4 i i,k+1 Si,k+1 k (cid:12) k
i=1 h
+N2α2σ2
3 n