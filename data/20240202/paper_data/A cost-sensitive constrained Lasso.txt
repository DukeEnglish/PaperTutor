A cost-sensitive constrained Lasso
Rafael Blanquero1,2, Emilio Carrizosa1,2, Pepa Ram´ırez-Cobo1,3, and M. Remedios
Sillero-Denamiel1,2
1Instituto de Matema´ticas de la Universidad de Sevilla (IMUS), Seville, Spain
2 Departamento de Estad´ıstica e Investigacio´n Operativa, Universidad de Sevilla.
3Departamento de Estad´ıstica e Investigacio´n Operativa, Universidad de Ca´diz.
Abstract
The Lasso has become a benchmark data analysis procedure, and numerous variants have
been proposed in the literature. Although the Lasso formulations are stated so that overall
prediction error is optimized, no full control over the accuracy prediction on certain individ-
uals of interest is allowed.
InthisworkweproposeanovelversionoftheLassoinwhichquadraticperformancecon-
straintsareaddedtoLasso-basedobjectivefunctions,insuchawaythatthresholdvaluesare
settoboundthepredictionerrorsinthedifferentgroupsofinterest(notnecessarilydisjoint).
Asaresult,aconstrainedsparseregressionmodelisdefinedbyanonlinearoptimizationprob-
lem. This cost-sensitive constrained Lasso has a direct application in heterogeneous samples
wheredataarecollectedfromdistinctsources,asitisstandardinmanybiomedicalcontexts.
Both theoretical properties and empirical studies concerning the new method are explored
in this paper. In addition, two illustrations of the method on biomedical and sociological
contexts are considered.
Keywords: Performance Constraints, Cost-Sensitive Learning, Sparse Solutions, Sample
Average Approximation, Heterogeneity, Lasso
1 Introduction
Let (Y,X) be a random vector, where X = (X ,...,X ) is a vector of p predictors and Y
1 p
identifies the response variable. Given the observed response vector y = (y ,...,y )′, n > p,
1 n
1
4202
naJ
13
]EM.tats[
1v32081.1042:viXraand the related observed predictors, x = (x ,...,x )′, j = 1,...,p, the linear regression
j 1j nj
model predicts y by
yˆ = βˆ +βˆ x +...+βˆ x .
0 1 1 p p
Consider the well-known prostate database [31], which consists of the measurements of
p = 8 predictors and one response variable (clinical measures) on n = 97 men who were about
to receive a radical prostatectomy. Further, assume that the dataset is divided into two groups:
Group 1, corresponding to young individuals (aged less than 65) and Group 2, related to the
population older than 65. If the goal is to minimize the overall mean squared error (MSE),
the parameter vector β = (β ,β ,...,β ) can be estimated by a fitting procedure as ordinary
0 1 p
least squares (OLS), yielding βˆols. The results obtained under the OLS are shown in the first
two rows of Table 1, where 3/4 of the total set has been used to fit the model (training set)
and the remaining samples for the assessment of the generalization error of the resulting model
(test set). The overall MSE, the prediction errors over the two groups as well as the number of
coefficients involved in the model are presented.
Method Overall MSE Group 1 MSE Group 2 MSE Non-Zero Coefficients
Training set 0.344 0.355 0.333
OLS 8
Test set 0.373 0.380 0.367
Training set 0.365 0.397 0.335
Lasso 5
Test set 0.408 0.414 0.403
Training set 0.355 0.357 0.352
CSCLasso 6
Test set 0.393 0.399 0.388
Table 1: Results obtained using prostate dataset
Once the model is fitted, there are two fundamental criteria for evaluating its performance:
the accuracy of prediction and the identification of significant predictors, which provides a good
interpretation of the solution. It is well-known that βˆols is not sparse, as can be observed from
Table 1 where the eight predictor variables have been used by the model. This fact has entailed
newpenalizationtechniquesasridgeregression[19],whichisacontinuousshrinkagemethodthat
achieves its better accuracy prediction through a bias-variance trade-off. Nevertheless, ridge re-
2gression is known not to be able to render a parsimonious solution. In contrast, best-subset
selection ([15]) achieves more sparse solutions, but it suffers from high variability and computa-
tional difficulties [11]. To overcome those shortcomings, [33] proposed the Lasso regularization
technique, which achieves both estimation and selection of relevant predictors simultaneously
by construction. Given X = [1 | x | ... | x ] the predictor matrix; then, the Lasso solution can
1 p
be defined as
1
βˆLasso(λ) = argmin ∥y−Xβ∥2+λ∥(β ,...,β )∥ (1)
1 p 1
n
β
where λ ≥ 0 is a tuning parameter and ∥.∥ is the l norm. To visualize the effect of the penalty
1 1
term in the Lasso formulation, consider the third and fourth rows in Table 1, which provide the
results obtained under the Lasso for the prostate dataset. In this case, in comparison with
OLS results, a sparser, and therefore a more interpretable solution, has been obtained at the
expense of slightly worsening MSE values.
One of the advantages of the Lasso is that the entire path of solutions can be found thanks
totheLARSalgorithm[10]. Inaddition, itiswell-knownthat, undersomeconditions, theLasso
enjoysgoodtheoreticalandstatisticalproperties([9],[13],[5]). However,theLassopresentssome
limitations; in particular, the literature related to the Lasso has not undertaken the problem
of fully controlling the accuracy prediction on certain individuals of interest. In the previous
prostate database, assume for instance that we are interested in fitting a sparse regression
model to the dataset where, apart from obtaining a small overall mean squared error, also the
prediction error for the young individuals should not exceed a given threshold. In this paper
we propose a Lasso-based model that allows for such aim, namely the cost-sensitive constrained
Lasso, denoted from now on as CSCLasso. The results obtained for the prostate database
under the CSCLasso, whose definition and main properties shall be discussed in Section 2 and
3, are shown in the last two rows of Table 1. A threshold for the mean squared error over Group
1 is set equal to 0.357, which represents an improvement of 10% over the prediction error of
the Lasso (0.397). Note that in the training set the MSE satisfies the imposed constraint, as
expected. Also note that the improvement in Group 1 is at the expense of slightly increasing
the prediction error over Group 2. In terms of sparsity, the CSCLasso model has needed an
additional predictor variable comparing to Lasso in order to comply with the constraint.
As it will be seen in Section 2, the novel approach is set up by adding convex quadratic
3constraints to the Lasso formulation. Other approaches have considered constrained versions of
the Lasso before, see for example ([21], [14], [36], [20] and references therein). In such works,
equality or/and inequality linear constraints are considered for imposing prior knowledge and
structure onto the coefficient estimates. In our approach instead, quadratic convex constraints
are formulated and thus, our approach and results generalize those previously obtained in the
literature.
Not only constrained versions of the Lasso can be found in the literature. Indeed, many
different variants have been proposed. For example, in [39] adaptive weights for penalizing
different coefficients in the l penalty are included as a way for fitting sparser models under
1
more general conditions. Moreover, in the presence of highly correlated predictor variables (as is
usualinmicroarraystudies)orwhenpredictorsarestructurallygrouped(e.g. dummyvariables),
the Lasso sometimes does not perform well and, as a consequence, the elastic net [40] and the
group lasso ([38], [30]) were proposed. They combine l and l penalties to try to select (or
2 1
remove) the correlated or structured predictor variables together. Other extension is to consider
1
βˆLasso(λ) = argmin ∥y−Xβ∥2+λ∥Aβ∥ . (2)
1
n
β
instead of (1), where A is a fixed matrix (see [35]). If A = (0|Ip×p), then the Lasso objective
function is obtained; however, other forms of A different from the identity can be found in the
literature, see for example [25]. In fact, various choices of A in (2) define problems that are
already well-known in the literature as the fused lasso [34]. See [17] for an extensive review
about Lasso problem and generalizations.
The motivation of this paper (controlling the performance measure on certain groups of
interest) is not novel in the Data Analysis literature, and indeed, it has been exploited in classi-
fication contexts with the term cost-sensitive learning ([26] and [18]). Many realworld problems,
such as those found in medical diagnosis or credit card fraud detection, have asymmetric mis-
classification costs associated, since the consequences of wrong predictions across the classes
may be very different. Therefore, for these problems, it is more important to achieve better
classification rates for the individuals of interest (ill people, defaulting customers). See [6], [32],
[8], [23], [4]and[12]formoredetailsandapplications. Thosemethodsarebasedonadaptingthe
classifier construction or adding parameters, among others. As some examples, consider [8], [6]
and [23], which adapt the support vector machine (SVM) classifier. In [8] the decision boundary
4shift is combined with unequal misclassification penalties. A biobjective problem is introduced
in [6], which simultaneous minimizes the misclassification rates. In [23], the authors propose a
new weight adjustment factor that is applied to a weighted SVM.
This paper is structured as follows. In Section 2, the cost-sensitive constrained Lasso
(CSCLasso) is introduced and some key issues are discussed. Section 3 considers theoretical
properties of the CSCLasso, as the existence and uniqueness of solution, limit behaviour (in
terms of the penalty parameter) and consistency. Section 4 presents a detailed numerical anal-
ysis with both simulated and real datasets, and finally, some conclusions and extensions are
provided in Section 5. Technical proofs are relegated to the Appendix.
2 The cost-sensitive constrained Lasso: definition and key as-
pects
This section presents the cost-sensitive constrained Lasso, which, as will be seen, is defined
through an optimization problem with constraints related to prediction errors for individuals of
interest. In addition, some computational details, as well as different key aspects concerning the
tuning parameters of our proposal, are presented.
2.1 Definition
TheproposedCSCLassoisanovelvariantoftheLassowhereweshalldemandthattheprediction
errors for the groups of interest are below certain threshold values,
1
min ∥y −X β∥2+λ∥Aβ∥
0 0 1
β n 0
1
s.t. ∥y −X β∥2−f ≤ 0,
1 1 1
n
1 (3)
.
.
.
1
∥y −X β∥2−f ≤ 0.
L L L
n
L
In the previous formulation, (y ,X ) is the set of observations used to build the sparse model
0 0
withoverallminimumMSE,whichcanbethecompletedataset(y,X),orasubsetofsmallersize.
Additionally, let (y ,X ), l = 1,...,L, define groups of interest (not necessarily disjoint), where
l l
the MSE predictions are to be controlled. Then, n is the number of instances related to group
l
5l. Finally, f = (f ,...,f ) contains the different threshold values for the MSE on the different
1 L
groups. The solution of optimization problem (3) will be denoted by βˆCSCLasso(λ). From the
formulation (3) it is natural to wonder whether running a Lasso on just the groups of interest
is more advantageous. However, if a single Lasso is run on the groups of interest, dramatically
badpredictionscanbeobtainedwhen theresultingmodelisappliedtonew observationsoutside
thosegroups,whichisnotthecaseforourapproach. ThesameissueariseswhenadifferentLasso
model is built on each group of interest, but, in addition, new observations are not given with
their group of origin. Contrary to what happens with our novel approach (3), the L predictions
obtained through the L different estimated Lasso models may not be suitable to give a final
prediction for such new samples.
The proposed method can be formulated as a Lasso with weighted quadratic penalties in the
objective function associated with the different groups, but finding real meaning to their param-
eters (one per group) to be chosen is not an easy task (see [7] and the references therein) and
the full control over the accuracy prediction on certain individuals of interest would disappear.
However, the parameters f = (f ,...,f ) involved in our model have a clear interpretation and,
1 L
in addition, this formulation enables us to bound the prediction errors in the different groups of
interest.
Asanexample,intheillustrationofthemethodinSection1relatedtotheprostatedataset,
whereas the training set was used in the objective function with A =
(cid:0) 0|I8×8(cid:1)
, the prediction
errorovertheyoung populationofthetrainingset,(y ,X ),iscontrolledthroughaperformance
1 1
constraint (f = 0.357). In a real application, once the L groups of interest are selected by the
1
user,thresholdvaluesf ,...,f havetobefixed. Notethatthesethresholdswilldependdirectly
1 L
on the dataset in question and the considered groups of interest. As a first option, they could
be fixed by the user according to her demand, but therefore unfeasibility problems may appear
when solving the CSCLasso problem (3). For that reason, in Section 2.3 two procedures for
determining such threshold values so that (3) is feasible are given.
Next, some other aspects related to the formulation of the CSCLasso and its resolution will
be discussed.
62.2 Computational details
The CSCLasso problem as defined by (3) is a non-differentiable convex optimization problem
with quadratic and convex constraints. However, if we rewrite the non-differentiable term in (3)
as
Aβ = u+−u−,
where u+ = (u+,...,u+) and u− = (u−,...,u−) are new vectors of positive auxiliary variables,
1 p 1 p
a differentiable version for the CSCLasso problem (3) is obtained in a straightforward manner
as
p p
1 (cid:88) (cid:88)
min ∥y −X β∥2+λ u++λ u−
β,u+,u− n 0 0 0 j j
j=1 j=1
1
s.t. ∥y −X β∥2−f ≤ 0,
1 1 1
n
1
.
.
.
1
∥y −X β∥2−f ≤ 0,
L L L
n
L
Aβ = u+−u−,
u+,u− ≥ 0.
This previous smooth formulation for the CSCLasso eases its resolution notably, since ef-
ficient solvers for quadratically constrained programming problems, such as Gurobi [16], are
available. In particular, the Gurobi R interface will be used in this work to obtain all numerical
results.
AnotherremarkconcerningtheformulationoftheCSCLassoisthat,insteadofusingthesum
of squared deviations, least absolute deviations could have been considered. Then, (3) would be
reduced to a regression problem under linear inequality constraints, as those described in [21],
[14] and [20]. Nevertheless, to cope the non-differentiability of the absolute value function, a
huge number of constraints and new auxiliary variables, which would depend on n, should have
been added. Consequently, these constrained approaches are likely to face severe numerical
difficulties in practice for large datasets.
72.3 The choice of threshold values
As commented in Section 2.1, threshold values f ,...,f could be fixed by the user. If the user
1 L
is too demanding, imposing very low MSE threshold values for (some of) the different groups,
the optimization problem may become unfeasible. Although a try-and-error procedure may be
used, it would be very helpful to have strategies yielding feasible solutions. Here we propose
two procedures for determining f ,...,f in such a way that (3) is feasible.
1 L
First, we propose a choice of the threshold values so that they are close to the OLS results,
f = (1+τ)MSE (βˆols), l = 1,...,L, (4)
l l
1
where MSE (β) = ∥y −X β∥2, l = 1,...,L and τ ≥ 0 is a small parameter whose meaning
l l l
n
l
is the percentage of worsening with respect to the OLS prediction error. For the numerical
example in Section 1, we could have imposed the threshold for the MSE over Group 1 equal
to 0.352, which is a 10% (τ = 0.1) more than MSE (βˆols) = 0.319. The choice (4) deals
1
with the heterogeneity coming from the variability of the different groups (MSE is different
l
across groups). Nevertheless, when heterogeneity related to the importance of each group is also
considered, the parameter τ can be replaced in (4) by τ , l = 1,...,L.
l
Next, we shall compute the minimum value of τ, τ , so as to (3) is feasible. That is, the
min
minimum τ so that there exists β∗ satisfying
(cid:32) (cid:33)
MSE (β∗)
l
max −1 ≤ τ,
l MSE (βˆols)
l
and, therefore, τ will be given as
min
(cid:32) (cid:33)
MSE (β∗)
l
τ = max −1.
min l MSE (βˆols)
l
Such τ can be found as the optimal value of the following linear problem with convex and
min
quadratic constraints
min z
β,z
(5)
MSE (β)
l
s.t. z ≥ −1, ∀l = 1,...,L.
MSE (βˆols)
l
The feasible version of the CSCLasso optimization problem can be formulated as
81
min ∥y −X β∥2+λ∥Aβ∥
0 0 1
β n 0
1
s.t. ∥y −X β∥2−(1+τ)MSE (βˆols) ≤ 0,
1 1 1
n
1 (6)
.
.
.
1
∥y −X β∥2−(1+τ)MSE (βˆols) ≤ 0,
L L L
n
L
where τ ≥ τ .
min
Finally, note that if τ is big enough, then solving (6) is equivalent to solve the unconstrained
problem. Indeed, it is possible to find the value of τ, τ (λ), such that both the constrained
max
and unconstrained problems are equivalent
MSE (βˆLasso(λ))
l
τ (λ) = max −1. (7)
max l∈{1,...,L} MSE (βˆols)
l
A second possible choice for the threshold values follows an analogous approach but, instead
of considering the results of the OLS, we shall consider the mean squared error of the Lasso, as
in the numerical example introduced in Section 1. For each l = 1,...,L,
f = (1−γ)MSE (βˆLasso(λ)), l = 1,...,L, (8)
l l
whereγ ≥ 0isrelatedtothedesiredpercentageofimprovementovertheLassosolution(γ = 0.1
in the numerical example of Section 1). In this case, we will compute the maximum value of γ,
γ , in such a way that (3) is feasible under (8), and the linear problem associated with γ
max max
is
max z
β,z
(9)
MSE (β)
l
s.t. 1− ≥ z, ∀l = 1,...,L.
MSE (βˆLasso(λ))
l
Thus, anotherpossiblefeasibleversionoftheCSCLassooptimizationproblemcanbeformulated
9as
1
min ∥y −X β∥2+λ∥Aβ∥
0 0 1
β n 0
1
s.t. ∥y −X β∥2−(1−γ)MSE (βˆLasso(λ)) ≤ 0,
1 1 1
n
1 (10)
.
.
.
1
∥y −X β∥2−(1−γ)MSE (βˆLasso(λ)) ≤ 0,
L L L
n
L
where γ ≤ γ .
max
Note that the two choices previously described for selecting the threshold values are not
unique. Indeed, instead of using the MSE, another statistical measure as the R-squared can be
considered. Further details about how they perform in numerical applications are described in
Sections 2.4 and 4.
2.4 The role of the tuning parameters
The CSCLasso, as defined by (6) or (10), is stated in terms of two tuning parameters, λ and τ or
λ and γ, respectively. The first one, λ, is related to the sparsity of the solution, and the second
oneislinkedtotheuser’sdemandinglevel, sincethedegreeofrequirementincreasesasτ → τ
min
(or γ → γ ). In this section we investigate how the solution of the CSCLasso changes when
max
λ and τ jointly vary (analogous results are obtained if λ and γ are analyzed instead). With
this purpose, consider again the experimental setting as in the example of Section 1 related to
prostate dataset with A = (0|Ip×p) (Lasso objective function), but in this occasion assume
that the prediction errors of both groups (the young and the elderly people) shall be controlled.
The interval of variation of the parameter λ is set to I = [0,30]. Moreover, according
λ
to (5), the smallest value of τ such that the CSCLasso optimization problem (6) is feasible is
τ = 0.055. On the other hand, following (7), τ = max τ (λ) = 2.355, although
min max max
λ∈[0,30]
we will enlarge the interval of variation of τ to also visualize the unconstrained solution; such
interval will be finally set as I = [τ ,τ +2] = [0.055,4.355]. Figure 1 represents, via a
τ min max
heat map, the solution for βˆCSCLasso(λ) for the different values of (λ,τ) in a grid contained in
1
I ×I .
λ τ
Some conclusions can be drawn from the figure. Consider first the cases where τ and λ are
bigenough. Since, inthiscase, τ ≥ τ , then, ascommentedattheendoftheprevioussection,
max
10solving (6) is equivalent to solving the Lasso. Therefore, βˆCSCLasso(λ) = βˆLasso(λ) = 0 will be
the optimal solution, provided that λ is big enough. Analogously, if τ ≥ τ but λ is small,
max
then βˆCSCLasso(λ) = βˆLasso(λ), which will be equal to zero or not depending on the importance
of the variable. When τ is small, the constraints are demanding and, even for large values of λ,
it might happen that βˆCSCLasso(λ) ̸= βˆLasso(λ) = 0, as it is the case.
1 1
Figure 1: Heat map of βˆCSCLasso(λ) using prostate dataset
1
Figure 7 (see Appendix for further results) represents the analogous heat maps concerning
βˆCSCLasso(λ),...,βˆCSCLasso(λ). A similar discussion as with βˆCSCLasso(λ) is applicable to
2 8 1
these figures. An interesting remark to be made concerns the importance of each variable: while
variable 1 is the only one selected for the Lasso, the CSCLasso returns a less sparse solution in
this case, since predictor variables 1, 2, 4 and 5 turn out to be significant. However, this is not
the rule, since there are examples where the level of sparsity is higher for the CSCLasso, as will
be shown in Section 4.
113 Theoretical properties
In this section we discuss some theoretical results concerning the CSCLasso model. In Section
3.1, the existence of a unique optimal solution to Problem (3) is proven for a fixed value of
λ ≥ 0. Section 3.2 deals with the limit behavior of the solution when λ approaches infinity.
Finally, some consistency properties of the CSCLasso solution are derived in Section 3.3 from
the Sample Average Approximation theory (see [29]).
3.1 Existence and uniqueness of solution
For constrained versions of the Lasso in the literature, as in [21], it is not possible to obtain the
path of solutions and therefore approximations are made with the use of numerical algorithms.
For the CSCLasso problem, a closed form solution of expression βˆCSCLasso(λ) is not available.
However, an implicit characterization of the CSCLasso solution (with only one constraint) can
be found as the following result states.
Proposition 1. Consider the CSCLasso problem with one constraint,
1
min ∥y −X β∥2+λ∥Aβ∥
0 0 1
β n 0
1
s.t. ∥y −X β∥2−(1+τ)MSE (βˆols) ≤ 0, (11)
1 1 1
n
1
where A = (0|Ip×p) and assume that X and X are maximum rank matrices. Then
0 1
(cid:18)
1 1
(cid:19)−1(cid:18)
1 1
(cid:19)
1
(cid:18)
1 1
(cid:19)−1
βˆCSCLasso(λ) = X′X + η(λ)X′X X′y + η(λ)X′y − X′X + η(λ)X′X b(λ)
n 0 0 n 1 1 n 0 0 n 1 1 2 n 0 0 n 1 1
0 1 0 1 0 1
(12)
where η(λ) is the Lagrange multiplier of the constraint and the component s, s = 0,1,...,p, of
the vector b(λ) is given by

 λ, if βˆCSCLasso(λ) > 0,
  s

b s(λ) = −λ, if βˆ sCSCLasso(λ) < 0,



 0 else.
12From the previous proposition, it is clear that a closed form solution is hard to be obtained,
even in the simplest scenario.
Nevertheless, given a fixed value of λ, the CSCLasso problem can be solved using quadratic
programmingviaanyofthestandardsolversavailableintheliterature. Asanexample, Figure2
depictsthepathofsolutionsfortheprostateexampleintroducedinSection1,foranassortment
of values of λ in a grid (see Section 3.2 for details). Each line represents a component of
βˆCSCLasso(λ), βˆCSCLasso(λ) with j = 1,...,8. It can be observed from the figure that, contrary
j
to what happens in the Lasso path of solutions (top panel of Figure 2), the CSCLasso path
of solutions is not piecewise linear (bottom panel of Figure 2). Such non-linearity (due to the
quadratic constraints) hinders the application of an iterative algorithm to obtain the path of
solutions as those given in papers [21] and [14].
Also note from Figure 3 that as a consequence of the performance constraints, the solution is
stabilized when λ increases, but does not shrink to 0, as with Lasso. This is detailed in Section
3.2.
Even without having the expression of the general solution of (3), we next prove that, under
full rank assumptions, the solution is unique. First, in order to simplify the formulation of (3),
henceforthitsfeasiblesetwillbedenotedbyB,whichisconvexandclosed. Thisisalsotrue(and
the results which follow remain valid) if, on top of the performance constraints, one adds linear
constraints modeling other aspects of interest (for example, the sign of a certain coefficient can
be fixed to be positive or negative depending on the known relation between the corresponding
predictors with the response variable). In the same vein, henceforth, (y ,X ) = (y,X) is used
0 0
to minimize the overall MSE. In this way, the CSCLasso problem (3) is rewritten as
1
min ∥y−Xβ∥2+λ∥Aβ∥ . (13)
1
β∈B n
The following result guarantees that the solution of problem (13) is unique.
Theorem 1. Consider Problem (13) where X is assumed to be a maximum rank matrix and its
feasible region B is a convex and closed set in Rp+1. Then, Problem (13) has a unique optimal
solution.
13Figure 2: Path of solutions under Lasso (top) and CSCLasso (bottom) for prostate dataset
3.2 Asymptotic behaviour
One of the key points when dealing with Lasso-type problems is the choice of the regularization
parameter λ. In the case of the Lasso, such choice is straightforward since the entire path of
solutions is known to be piecewise linear, shrinking to 0. In particular, it is known that there
14Figure 3: Path of solutions under Lasso (top) and CSCLasso (bottom) for prostate dataset
when λ increases
exists a value of λ, λ∗, such that the solution βˆLasso(λ) = 0 is optimal for all values λ ≥ λ∗.
The following result provides explicitly the value of such λ∗.
Proposition 2. Consider the Lasso model (2). Define λ∗ as the optimal value of the linear
15programming problem
min z
z,t
2
s.t. X′y = A′λt,
n
−z ≤ λt ≤ z, s = 0,1,...,p.
s
Then, βˆLasso(λ) = 0 for all λ ≥ λ∗. In particular, for A = (0|Ip×p),
(cid:13) (cid:13)
λ∗ = (cid:13) (cid:13)2 X′y(cid:13) (cid:13)
(cid:13)n (cid:13)
∞
Some works dealing with (linear) constrained versions of the Lasso (as [14] and [21]) have
developed efficient algorithms to build the associated solutions path. Consider now the general
problem (13). As commented in Section 3.1, the expression of βˆCSCLasso(λ) is not available in
closed form and, consequently, the entire path cannot be computed. In this case, when λ tends
to +∞, the solution βˆCSCLasso(λ) stabilizes around βˆCSCLasso(+∞) = argmin∥Aβ∥ . This
1
β∈B
idea is also used in [14] and [21], where, in order to find an initialization for the algorithms, the
proposed constrained problems are solved by only considering the penalty term in the objective
function. Such a limit solution is obtained by solving an optimization problem with a linear
objective function and convex quadratic constraints, namely
p
(cid:88)
min (u++u−)
s s
β,u+,u−
s=0
s.t. β ∈ B
Aβ = u+−u−
u+,u− ≥ 0.
A grid search is carried out in the general CSCLasso problem (13) to obtain suitable values
ofλ. Inorder to fixthe grid, we proposethe followingdynamic approach tofind anapproximate
maximum value of λ, λ∗ (see Algorithm 1).
Once the maximum value λ∗ is found, then the grid ranges from 0 to λ∗ with the de-
sired step. Note that the previous algorithm already provides an initial grid of the form
(2−5,2−4,...,20,...,λ∗).
16Algorithm 1 Dynamic approach for selecting λ∗ in the CSCLasso
1. Fix ε > 0 and c = (2−5). Fix i = 1 and compute βˆCSCLasso(c[i]).
2. Compute βˆCSCLasso(+∞) = argmin∥Aβ∥ .
1
β∈B
3. While ∥βˆCSCLasso(λ)−βˆCSCLasso(+∞)∥ > ε, repeat
a) i = i+1
b) c = (c,2c[i−1])
c) compute βˆCSCLasso(c[i])
4. λ∗ = c[i]
3.3 Consistency properties in the CSCLasso
Thepurposeofthissectionistoprovesomeresultsrelatedtotheconsistencyofboththesolution
and the objective value for CSCLasso problem (13). To do that, the theory of Sample Average
Approximation (SAA) [29] will be applied. Consider the following stochastic programming
problem
min f(β) := E[F(β,(Y,X))], (14)
β∈B
where B is a nonempty closed subset of Rp+1, (Y,X) is an absolutely continuous random vector
whose probability distribution P is supported on a set Ξ ⊂ Rp+1 and F : B ×Ξ → R. In [29],
under some conditions, the true problem (14) can be estimated by the SAA:
n
1 (cid:88)
min fˆ (β) := F(β,(y ,x )), (15)
n i i
β∈B n
i=1
where x = (1,x ,...,x )′, and {(y ,x )} is a realization of the n random vectors
i i1 ip i i i=1,...,n
{(Y ,X )} , whichareindependentandidenticallydistributed(i.i.d.) astherandomvector
i i i=1,...,n
(Y,X). Note that the CSCLasso problem as in (13) takes the form of (15) as
n
min 1 (cid:88) (y −x′ β)2+λ∥Aβ∥ (16)
β∈B n i i 1
i=1
and the true CSCLasso problem equivalent to (14) is
minE[(Y −X′β)2+λ∥Aβ∥ ]. (17)
1
β∈B
17BeforeprovingthemainresultontheconsistencyoftheCSCLasso, wefirstshowtheunique-
ness of the solution of such a problem.
Proposition 3. The optimal solution of the true CSCLasso problem (17) is unique.
Denote by νCSCLasso(λ) and βCSCLasso(λ), respectively, the optimal value and the optimal
solution of problem (17). Analogously, let νˆCSCLasso(λ) and βˆCSCLasso(λ) be the optimal value
and the optimal solution, respectively, of the SAA CSCLasso problem (16). The following result
shows the consistency of the SAA values to the true values.
Theorem 2. Assume that E[∥X∥2] < ∞, E[Y2] < ∞, E[∥YX∥] < ∞. Then, νˆCSCLasso(λ)
converges to νCSCLasso(λ) and βˆCSCLasso(λ) converges to βCSCLasso(λ) with probability one
(w.p. 1).
Finally, note that the theoretical results that have been studied in this work are also appli-
cable to other versions of constrained Lasso as long as the feasible set is convex and closed, as
is the case with the above-mentioned works [21], [14] and [20].
4 Numerical experiments
In this section, the behaviour and performance of our approach is illustrated throughout an
extensive empirical study. In particular, using both simulated and real datasets, the aim of
the experiments shall be to improve the prediction errors of the Lasso in one or more groups
of interest. Or in other words, threshold values shall be fixed as in (10). Since our proposal
is a novel extension of the Lasso, we will also show the results under the Lasso, not only for
those groups that are controlled (for which obviously, the Lasso performs worse) but also for
the non-controlled groups. In this way the CSCLasso can be better inspected in comparison
to the Lasso. Other aspects as the overall MSE and the percentage of non-zero coefficients in
the regression model, among others, will be explored. Such measures will be estimated through
median values using a 5-fold cross validation approach. To this end, the dataset will be split
at each fold into three sets: the so-called training, validation and test sets. The training set is
used to fit the model, the validation set is used to estimate prediction error for model selection
and the test set is used for assessment of the generalization error of the final chosen model.
184.1 A simulation study
The generation of the synthetic datasets in this section follows that of [25], where an overparam-
eterized regression model is considered to cope with stratified data. A number of groups K = 20
is set and two different sample sizes per group are considered, n = {150,500}, for k = 1,...,K.
k
The number of predictors p will be chosen from {20,100,500}. The matrix of predictor values
X is generated according to a multivariate normal distribution with zero mean and covariance
matrix Σ being a Toeplitz matrix with element (i,j) equal to 0.5|i−j|. Regarding the response
vector, a set of 20 predictors are randomly selected (with indexes included in a set P ), while
0
the rest of predictors are noise (that is, β = 0 for j ∈/ P ). The coefficients of the significant
j 0
20 predictors are chosen as follows. First, consider 10 random predictors out of the 20 selected.
1
For such predictors, if the group k > 6 then β
j
= 1 and, otherwise, β
j
= 1+K2. For the other
1
10 predictors, β
j
= 1 if k ≤ 6 and β
j
= 1+K2 otherwise. In this way, the predictors behave
differently depending on the group. Finally, the response vector for each group is generated
according to the standard linear regression model with normal error.
Once the synthetic dataset is built and its response and predictor variables have been stan-
dardized, the CSCLasso with A = (0|Ip×p) is run with constraints imposed over the first six
groups. The choice of λ will change at each fold. A grid in λ is built as in Section 3.2, and, the
value of λ which leads to the lower overall MSE in the validation set is selected. Table 2 shows
the median prediction errors per group k (MSE ), k = 1,...,6, obtained by the Lasso (rows in
k
grey color) and the corresponding values obtained under the CSCLasso for different threshold
values f.
In particular, the values of f have been set as improvement percentages over the Lasso
values, where the improvement levels are 3%, 5%, 7%, 10%, 15% and 20% (γ equal to 0.03, 0.05,
0.07, 0.15 and 0.20, respectively). The results are obtained for different combinations (p,n ),
k
where, as commented before, p is chosen from the set {20,100,500} and n from {150,500}. For
k
example, if n = 150 and p = 20, the median of the mean squared error for the Lasso in Group
k
1 was equal to 1.084. If the goal is to achieve an improvement of 3%, f must be chosen equal
to 1.052. The median of the mean squared errors for the CSCLasso is equal to 0.933 in this
case (results obtained on the test sample). It is important to remark that, for some levels of
improvement, the CSCLasso problem is unfeasible due to the fact that γ for such datasets is
max
19nk=150 nk=300
p Group1 Group2 Group3 Group4 Group5 Group6 Group1 Group2 Group3 Group4 Group5 Group6
MSEk(Lasso) 1.084 0.904 0.768 0.925 0.944 0.674 0.801 0.770 0.966 0.937 0.776 0.938
Improv. f 1.052 0.877 0.745 0.898 0.916 0.653 0.777 0.747 0.937 0.909 0.753 0.910
3% MSEk 0.933 0.722 0.695 0.859 0.822 0.574 0.752 0.689 0.810 0.859 0.744 0.870
Improv. f 1.030 0.859 0.730 0.879 0.897 0.640 0.761 0.732 0.918 0.890 0.737 0.891
5% MSEk 0.921 0.708 0.689 0.848 0.804 0.564 0.734 0.682 0.787 0.844 0.736 0.859
Improv. 0.745 0.717 0.899 0.871 0.722 0.873
- - - - - -
20 7% 0.717 0.677 0.768 0.834 0.721 0.847
Improv.
- - - - - - - - - - - -
10%
Improv.
- - - - - - - - - - - -
15%
Improv.
- - - - - - - - - - - -
20%
MSEk(Lasso) 1.491 1.139 0.875 1.496 1.151 0.759 1.104 1.151 0.932 0.981 1.311 1.142
Improv. f 1.447 1.105 0.848 1.451 1.116 0.736 1.070 1.117 0.904 0.951 1.272 1.108
3% MSEk 1.234 0.948 0.781 1.322 0.945 0.708 1.044 1.145 0.911 0.963 1.306 1.139
Improv. f 1.417 1.082 0.831 1.421 1.093 0.721 1.048 1.094 0.885 0.932 1.245 1.085
5% MSEk 1.226 0.941 0.777 1.317 0.937 0.706 1.025 1.123 0.897 0.947 1.311 1.142
Improv. f 1.387 1.059 0.813 1.391 1.070 0.706 1.026 1.071 0.866 0.912 1.219 1.062
100 7% MSEk 1.219 0.933 0.773 1.311 0.929 0.704 1.005 1.111 0.883 0.934 1.309 1.141
Improv. f 1.342 1.025 0.787 1.346 1.035 0.683 0.993 1.036 0.838 0.883 1.180 1.028
10% MSEk 1.207 0.921 0.767 1.303 0.917 0.702 0.978 1.094 0.864 0.915 1.311 1.131
Improv. f 1.268 0.968 0.743 1.271 0.978 0.645 0.938 0.979 0.792 0.834 1.114 0.971
15% MSEk 1.128 0.903 0.765 1.250 0.919 0.695 0.936 1.061 0.833 0.882 1.274 1.087
Improv. f 1.193 0.911 0.700 1.196 0.920 0.607 0.883 0.921 0.745 0.785 1.049 0.914
20% MSEk 1.155 0.909 0.771 1.251 0.900 0.700 0.895 1.036 0.803 0.855 1.227 1.046
MSEk(Lasso) 1.306 1.133 1.318 1.261 1.246 1.473 1.151 1.204 1.171 1.148 1.278 1.068
Improv. f 1.267 1.099 1.279 1.223 1.208 1.429 1.116 1.168 1.136 1.114 1.240 1.035
3% MSEk 1.270 1.133 1.314 1.248 1.246 1.453 1.029 1.077 1.047 1.022 1.186 0.961
Improv. f 1.241 1.076 1.252 1.198 1.183 1.400 1.093 1.144 1.113 1.091 1.214 1.014
5% MSEk 1.257 1.133 1.308 1.245 1.245 1.437 1.025 1.060 1.032 1.004 1.165 0.945
Improv. f 1.215 1.053 1.226 1.173 1.158 1.370 1.070 1.119 1.089 1.068 1.189 0.993
500 7% MSEk 1.246 1.133 1.312 1.241 1.246 1.425 1.018 1.042 1.023 0.987 1.144 0.929
Improv. f 1.176 1.019 1.186 1.135 1.121 1.326 1.036 1.083 1.054 1.033 1.150 0.961
10% MSEk 1.231 1.132 1.313 1.230 1.246 1.401 1.005 1.016 1.006 0.961 1.114 0.905
Improv. f 1.110 0.963 1.120 1.072 1.059 1.252 0.978 1.023 0.995 0.976 1.086 0.907
15% MSEk 1.207 1.114 1.271 1.193 1.241 1.376 0.971 0.977 1.039 0.923 1.060 0.866
Improv. f 1.045 0.906 1.054 1.009 0.996 1.179 0.921 0.963 0.937 0.919 1.022 0.854
20% MSEk 1.184 1.081 1.266 1.150 1.230 1.354 0.952 0.972 0.995 0.908 1.057 0.857
Table 2: Median errors over test sets for synthetic datasets
smaller than the required γ, and, therefore, those cases are represented as empty spaces in the
table. It must be noted that γ will also depend on each fold in the cross-validation because
max
it is associated with the partition of the data, since the MSE in (9) depends on such partition
l
(l = 1,...,L). It is also worth mentioning that the constraints will always be satisfied on the
training set but not necessarily on the test set, see, for example, the case k = 3, n = 150,
k
p = 100 with improvement level equal to 15%. This phenomenon is particularly common as p
increases and n decreases (see for example the values corresponding to p = 500 and n = 150
k k
20in Table 2).
We next investigate how the improvement in the prediction errors of the groups of interest
affects the prediction errors in the rest of the groups, the overall prediction error and the
sparsity level. Figure 4 represents the percentage of non-zero (NZ) coefficients and the overall
prediction error for different sample sizes, different levels of improvement and for p = 100.
Lasso results are also included. For instance, when n = 300 (black squares in Figure 4), the
k
NZ percentage for Lasso is 39.60 with an associated overall MSE of 0.734; whereas running the
CSCLassodemandinga3%ofimprovementoverthefirstsixgroups, weachieveaNZpercentage
of 38.61, and an overall MSE of 0.735. In general terms, it can be seen that the sparsity of the
solutiondecreaseswiththeimprovementlevel: smallersquares, whichrepresentsmallerimposed
improvementpercentages,areontheleftofbiggersquares(whichareassociatedwithdemanding
percentage of improvement). Then, if the user is very demanding in predicting a specific group,
this implies, in the majority of the cases, a less sparse solution. Notwhitstanding, when no
level of improvement is imposed (Lasso problem), the solution can be less sparse than in the
CSCLasso, as in the case of n = 300. This also occurs when p = 500 and n = 150 (see the
k k
bottom graphic of Figure 10 in Appendix: further results). Furthermore, the overall prediction
error slightly worsens with the improvement level, due to the worsening of the predictions in the
uncontrolled groups.
Figure 4: Median overall MSE over the test sets and NZ percentage under the choice p = 100
21Figure5representsthepredictionerrorsoverthegroupsthatarenotcontrolledaswellasthe
overall mean squared error, for different improvement levels, n = 150 (top figure) and n = 300
k k
(bottom) when p = 100. In the figure, Lasso values are also shown. From the figure, it can be
concluded that the Lasso performs better in the uncontrolled groups, since the prediction errors
worsen under our proposal. However, the overall mean squared error remains almost constant
(since the improved errors compensate the more deteriorated ones). Similar conclusions can be
drawn under the choices p = 20 and p = 500 (see Figures 8 and 9 in Appendix: further results,
respectively).
Next, we test how the solution of the CSCLasso behaves with respect to other global per-
formance measures as the l distance, false positive and negative rates, which are defined not
2
in terms of prediction errors, but on the correct fitting of the generator process (see [37]). In
particular, the l distance is defined as ∥βˆ−β∥ , where β is the vector of coefficients that gen-
2 2
erated the datasets (described at the beginning of this section), and βˆ(λ) are the estimators. In
addition, the false positive rate (FPR) and false negative rate (FNR) are calculated as follows:
| j : β = 0 & βˆ (λ) = 0 |
j j
FPR = ,
| j : β = 0 |
j
| j : β ̸= 0 & βˆ (λ) = 0 |
j j
FNR = ,
| j : β ̸= 0 |
j
where j = 1,...,p. The median of these three measures as well as the median of the overall
MSE (already shown in Figures 4-5 and Figures 8, 9 and 10 in Appendix: further results), are
presented in Table 3. For the choices where p = 20, the FPR values are not given since all the
predictors have associated non-zero coefficients when the datasets were created. From this table
it can be deduced that similar or even better results, comparing with those of the Lasso, are
obtained in the majority of the cases across the four different measures.
A final remark concerning the computational cost of the CSCLasso when comparing with
Lasso is as follows. The median user time required to solve the problem with the largest dataset
consideredinthisstudy(n = 300andp = 500)is0.85secondswhentheLassoisrunonIntel(R)
k
Core(TM) i7-7500U CPU at 2.70GHz 2.90GHz with 8.0 GB of RAM; whereas the CSCLasso
requires 6.60 seconds. Nevertheless, to better understand how the computation time behaves
depending on p value, a grid in this parameter has been inspected, while n is set to 300. Figure
k
6 depicts the logarithm of the user times in seconds obtained under Lasso and CSCLasso models
22Figure 5: Median MSE over the test sets for k = 7,...,20 under p = 100 features and,
k
n = 150 (top) and n = 300 (bottom). Each subgraph represents one group and the Y-axis
k k
shows the different percentages of improvement
when n = 300 and p changes. Then, under a reasonable computational cost, the desired results
k
are achieved. Further analyses regarding the computational times are shown in the Appendix
23n =150 n =300
k k
p Overall MSE l distance FPR FNR Overall MSE l distance FPR FNR
2 2
Lasso 0.667 4.149 - 0.050 0.666 4.146 - 0.000
Improv. 3% 0.691 4.108 - 0.000 0.673 4.099 - 0.000
Improv. 5% 0.695 4.104 - 0.000 0.683 4.089 - 0.000
20 Improv. 7% - - - - 0.693 4.077 - 0.000
Improv. 10% - - - - - - - -
Improv. 15% - - - - - - - -
Improv. 20% - - - - - - - -
Lasso 0.732 2.931 0.275 0.250 0.734 2.729 0.250 0.150
Improv. 3% 0.740 2.857 0.388 0.100 0.735 2.721 0.163 0.150
Improv. 5% 0.741 2.855 0.375 0.100 0.734 2.721 0.163 0.100
100 Improv. 7% 0.742 2.853 0.400 0.100 0.734 2.718 0.163 0.100
Improv. 10% 0.745 2.849 0.400 0.100 0.732 2.704 0.275 0.100
Improv. 15% 0.751 2.844 0.425 0.100 0.735 2.678 0.288 0.050
Improv. 20% 0.759 2.835 0.463 0.100 0.738 2.655 0.288 0.100
Lasso 0.772 2.778 0.135 0.300 0.744 2.750 0.065 0.300
Improv. 3% 0.772 2.779 0.040 0.350 0.744 2.658 0.063 0.250
Improv. 5% 0.772 2.775 0.050 0.300 0.745 2.653 0.075 0.250
500 Improv. 7% 0.772 2.769 0.042 0.350 0.746 2.647 0.077 0.250
Improv. 10% 0.772 2.760 0.056 0.350 0.748 2.632 0.104 0.250
Improv. 15% 0.771 2.754 0.063 0.300 0.752 2.640 0.129 0.250
Improv. 20% 0.774 2.735 0.069 0.350 0.760 2.607 0.148 0.200
Table 3: Median performance measures over test sets for synthetic datasets
(Figure 11).
4.2 Leukemia dataset: a gene expression dataset
The real stratified dataset described in [22] is explored here. The data contain information
related to myeloid monocytic leukemia cells undergoing differentiation to macrophages. In par-
ticular, the dataset is formed by expression levels of 45 transcription factors (response and
predictor variables) measured at 8 distinct times (groups) of the differentiation process. As in
[25], the aim is to predict the EGR2 transcription factor in terms of the other p = 44 factors.
24Figure 6: A two-dimensional graph of the logarithm of the user times in seconds for n = 300
k
as p increases
The sample size per group is equal to 120. Similarly as in Section 4.1, the Lasso was run and the
overall prediction errors, individual prediction errors per group and percentage of non-zero coef-
ficients are recorded. The records in Group 1 yield the best MSE using Lasso model. Therefore,
we may be interested in obtaining an even better fitting for such data. The CSCLasso problem
is solved with threshold values smaller than the Lasso error, which turns out to be 0.370. Table
4 shows the obtained median results for an assortment of improvement levels, namely, 5%, 7%,
10% and 15% or, equivalently, γ is equal to 0.05, 0.05, 0.07 and 0.15, respectively. From that
table, it can be seen how the prediction error of interest (corresponding to Group 1) decreases
with the improvement level, as expected. Similarly as in Section 4.1, the overall mean squared
error does not exhibit significant changes, while the prediction errors in the uncontrolled groups
do not exhibit the same behaviour. Some of them slightly improve (Group 6), others slightly
worsen (as the Group 5 and Group 8) and others remain constant (as Group 2). Finally, in
regards to the sparsity of the solution, for this dataset, less sparse solutions are obtained by
CSCLasso in comparison with the Lasso ones.
25f Overall MSE MSE MSE MSE MSE MSE MSE MSE MSE NZ
1 1 2 3 4 5 6 7 8
Lasso - 0.620 0.370 0.417 0.902 0.496 0.480 0.535 0.496 0.685 53.33
Improv. 5% 0.352 0.623 0.357 0.417 0.918 0.500 0.555 0.512 0.497 0.719 73.33
Improv. 7% 0.344 0.626 0.348 0.418 0.915 0.504 0.565 0.514 0.497 0.722 66.67
Improv. 10% 0.333 0.630 0.335 0.418 0.911 0.510 0.574 0.513 0.498 0.728 66.67
Improv. 15% 0.315 0.636 0.331 0.415 0.903 0.523 0.591 0.512 0.502 0.737 73.33
Table 4: Median errors over test set for gene expression dataset. Constraints imposed over
Group 1
4.3 Communities and Crime dataset
Inthissection, arealdatasetfromtheUCIMachineLearningRepository([24])willbeanalyzed.
Inparticular, theso-calledCommunities and Crime Unnormalized Data Set shallbeconsidered.
The dataset is about communities within the United States and has already been inspected in
the literature (see [27]). This dataset combines crime information from the FBI databases [3] as
well as socio-economic and law enforcement data from [1] and [2], respectively. The dataset is
formedbyp = 124predictors, 23ofwhichpresentmissingvalues, andn = 2215instances, where
the response variable measures the number of murders per 100K population. The predictor
variables with missing values are not consider for the next experiments. As such, we finally
consider p = 101 predictors. Additionally, for each instance (community), the region from
which it comes is known. Thus, if we were interested in obtaining a good prediction in a certain
region, say Midwest, we could control these communities by including a performance constraint.
Table 5 shows the median errors over the test set for Group 1, formed by the communities of
Midwest, and over the rest of communities (Group 2). In terms of overall MSE and MSE over
the two groups, similar conclusions as in Section 4.2 are drawn. Whereas different improvement
levels are imposed, the MSE of interest (MSE ) is getting smaller but the overall prediction
1
error is almost not affected by the constraint. Lastly, regarding the sparsity of the solution, an
analogous behaviour as that observed in the case of simulated data is obtained: the solution
becomes less sparse with the improvement level.
As previously commented, the groups of interest may overlap. As an illustration, assume
that the interest is in controlling the prediction error in communities of Midwest or communities
26f Overall MSE MSE MSE NZ
1 1 2
Lasso - 0.488 0.433 0.453 21.57
Improv. 5% 0.411 0.488 0.422 0.453 25.49
Improv. 7% 0.403 0.487 0.420 0.453 28.43
Improv. 10% 0.390 0.488 0.416 0.453 26.47
Improv. 15% 0.368 0.486 0.403 0.459 34.31
Table 5: Median errors over test set for communities and crime dataset. Constraints imposed
over Group 1
with a population density larger than or equal to the 75th percentile. Let Group 1 denote
the communities from Midwest, while Group 2 represents the communities where the density
of population is higher than the 75th percentile. For instance, if we aim to improve in a 7%
the errors obtained by the Lasso model (equal to 0.513 and 0.442), then the CSCLasso results
become 0.475 and 0.441, respectively.
5 Conclusions and extensions
In this paper a new version of the Lasso regression model that strives to control the performance
rates associated with individuals of interest is proposed. The method has a significant appli-
cation in the context of heterogeneous data, where it is common that certain sources are more
reliable than others, or simply the prediction on some groups of data are of higher interest, and
thus a better fit is sought for some data. In order to control the individuals of interest, perfor-
mance constraints are included in the regression model. This approach leads to a novel method
(CSCLasso) which is not reported in the literature previously, up to our knowledge. Theoretical
results concerning this novel methodology have been discussed and, in addition, the CSCLasso
has been tested on six synthetic datasets with different properties, on a well-referenced real
stratified biomedical dataset and on a real social sciences dataset. The numerical section shows
that, with a low computational cost, the accuracy prediction errors for the groups of interest
are controlled. This is done at the expense of reducing sparsity (if the regularization parameter
is kept fixed) or the overall accuracy.
A number of extensions to this work are possible. Regarding the selection of the threshold
27values f ,...,f , as commented in Section 2.4, a more flexible choice would be to consider a
1 L
different fine-tuning parameter τ (or γ) for each group of interest l, say τ (γ ). However, this
l l
generalization implies tuning (many) more parameters, making the model less usable by users.
In addition, for the sake of dealing with strongly correlated predictors, it may be of interest
to change the objective function by that of the elastic net ([40]). Another non-straightforward
extension could be to address classification problems (via the logistic regression) instead of
regression problems. In this case, we would not adress a quadratic problem. Work in these
issues is underway.
Acknowledgements
Research partially supported by research grants and projects MTM2015-65915-R (Ministerio de
Econom´ıa y Competitividad, Spain), FQM-329 and P18-FR-2369 (Junta de Andaluc´ıa, Spain),
Fundaci´on BBVA and EC H2020 MSCA RISE NeEDS Project (Grant agreement ID: 822214).
In addition, we would like to thank the associated editor and two anonymous reviewers for
carefully reading this work, and for their insightful comments, which have helped to improve
the quality of this paper.
Appendix: proofs
Proof of Proposition 1
Given λ ≥ 0, consider problem (11). If β = β+ − β− with β+ ≥ 0 and β− ≥ 0 and λ =
(0,λ,...,λ)′, a vector whose length is p+1, then the differentiable version of that problem turns
out to be
1
min ∥y −X (β+−β−)∥2+λ′β++λ′β−
0 0
β+,β− n 0
1
s.t. ∥y −X (β+−β−)∥2−(1+τ)MSE (βˆols) ≤ 0,
1 1 1
n
1
β+ ≥ 0 ⇔ −β+ ≤ 0,
β− ≥ 0 ⇔ −β− ≤ 0.
28From the Karush-Kuhn-Tucker conditions,
1
L(β+,β−,θ+,θ−,η) = ∥y −X (β+−β−)∥2+λ′β++λ′β−−(θ+)′β+−(θ−)′β−+
0 0
n
0
(cid:18) (cid:19)
1
+η ∥y −X (β+−β−)∥2−(1+τ)MSE (βˆols)
1 1 1
n
1
∂ 2 2
: − X′(y −X (β+−β−))+λ−θ+− ηX′(y −X (β+−β−)) = 0
∂β+ n 0 0 0 n 1 1 1
0 1
∂ 2 2
: X′(y −X (β+−β−))+λ−θ−+ ηX′(y −X (β+−β−)) = 0
∂β− n 0 0 0 n 1 1 1
0 1
θ+,θ−,η ≥ 0
(θ+)′β+ = 0
(θ−)′β− = 0
(cid:18) (cid:19)
1
η ∥y −X (β+−β−)∥2−(1+τ)MSE (βˆols) = 0
1 1 1
n
1
Thus,
2 2
• if β > 0 ⇒ β+ > 0,β− = 0 ⇒ θ+ = 0 ⇒ − X′(y −X β)+λ− ηX′(y −X β) = 0
n 0 0 0 n 1 1 1
0 1
2 2
• if β < 0 ⇒ β+ = 0,β− > 0 ⇒ θ− = 0 ⇒ X′(y −X β)+λ+ ηX′(y −X β) = 0
n 0 0 0 n 1 1 1
0 1
Therefore,
2 2
X′(y −X β)+ η(λ)X′(y −X β) = b(λ), (18)
n 0 0 0 n 1 1 1
0 1
where η(λ) is the Lagrange multiplier associated with the first constraint and b(λ) is a (p+1)-
dimensional vector whose s-th component, s = 0,1,...,p, takes the following value

  λ, if β s > 0,


b s(λ) = −λ, if β
s
< 0,



 0, else.
Then, since X and X are maximum rank matrices, one obtains from (18) the following
0 1
implicit expression for the solution βˆCSCLasso(λ) of Problem (11)
(cid:18)
1 1
(cid:19)−1(cid:18)
1 1
(cid:19)
1
(cid:18)
1 1
(cid:19)−1
βˆCSCLasso(λ) = X′X + η(λ)X′X X′y + η(λ)X′y − X′X + η(λ)X′X b(λ).
n 0 0 n 1 1 n 0 0 n 1 1 2 n 0 0 n 1 1
0 1 0 1 0 1
29Proof of Theorem 1
1 1
Consider the function h : β (cid:55)→ ∥y−Xβ∥2 = (y−Xβ)′(y−Xβ), where X is a maximum
n n
rank matrix by hypothesis. The matrix X is of maximum rank and therefore the Hessian matrix
2
H (β) = X′X is positive definite, from where we conclude that h(β) is strictly convex, and
h
n
hence, h(β)+λ∥Aβ∥ is also a strictly convex function.
1
Wenextprovethath(β)isacoercivefunction. SinceX′Xispositivedefinite, itseigenvalues
are all positive. In particular, the smallest eigenvalue, say γ , will be nonzero. Moreover, using
r
the spectral decomposition of a symmetric matrix,
1 1 1 2 1
∥y−Xβ∥2 = (y−Xβ)′(y−Xβ) = β′X′Xβ− y′Xβ+ y′y =
n n n n n
1 2 1
= β′Q′DQβ− y′Xβ+ y′y ≥
n n n
(cid:12) (cid:12) (cid:13) (cid:13)
≥ 1 β′Q′DQβ−(cid:12) (cid:12)2 y′Xβ(cid:12) (cid:12)+ 1 y′y ≥ γ r ∥Qβ∥2−(cid:13) (cid:13)2 y′X(cid:13) (cid:13)∥β∥+ 1 y′y =
n (cid:12)n (cid:12) n n (cid:13)n (cid:13) n
(cid:13) (cid:13)
= γ r ∥β∥2−(cid:13) (cid:13)2 y′X(cid:13) (cid:13)∥β∥+ 1 y′y,
n (cid:13)n (cid:13) n
where, in the second-to-last step, the Cauchy-Schwarz inequality has been used. As ∥β∥ → +∞,
then h(β) → +∞ too, and thus h(β) is a coercive function.
Now we show that (13) has optimal solution. Let β∗ ∈ B. As h(β) is coercive, then there
exists R > 0 such that
1 1
∥y−Xβ∥2 > ∥y−Xβ∗∥2+λ∥Aβ∗∥ ,
1
n n
for all β such that ∥β∥ > R. For that reason, the problem can be reduced to the feasible
compact region B∩{β : ∥β∥ ≤ R}, which implies that the optimal solution is reached. Finally,
the uniqueness of the solution follows from the fact that h(β)+λ∥Aβ∥ is strictly convex.
1
Proof of Proposition 2
Let us consider the optimization problem (2) and let βˆLasso(λ) denotes its optimal solution.
The necessary and sufficient optimality condition is:
1
∇ ∥y−XβˆLasso(λ)∥2+λ∂∥AβˆLasso(λ)∥ ∋ 0. (19)
1
n
30From the properties of subdifferential (see Theorem 23.9 of [28]) it follows that
∂∥AβˆLasso(λ)∥ = A′∂∥.∥ ,
1 1 |AβˆLasso(λ)
which implies that (19) becomes
2
− X′(y−XβˆLasso(λ))+λA′∂∥.∥ ∋ 0. (20)
n 1 |AβˆLasso(λ)
Consequently, the necessary and sufficient condition (20) in βˆLasso(λ) = 0 is
2
− X′y+λ{A′t : ∥t∥ ≤ 1} ∋ 0,
∞
n
since ∂∥0∥ is the unit ball of the ∥.∥ . Equivalently,
1 ∞
2
X′y ∈ {A′λt : ∥t∥ ≤ 1}.
∞
n
Therefore, the solution of the problem
min λ
λ,t
2
s.t. X′y = A′λt,
n (21)
∥t∥ ≤ 1,
∞
λ ≥ 0,
will provide the minimum λ from which βˆLasso(λ) = 0 is the optimal solution. If q = λt, then
Problem (21) becomes
min λ
λ,q
2
s.t. X′y = A′q,
n
∥q∥ ≤ λ.
∞
The constraint ∥q∥ is equivalent to |q | ≤ λ, s = 0,1,...,p and the result follows.
∞ s
31Proof of Proposition 3
The proof follows very closely that of Theorem 1. First of all, it shall be proven that h : β (cid:55)→
E[(Y − X′β)2] is coercive. It is strictly convex on β since its Hessian matrix, 2E[XX′], is
positive definite due to X is an absolutely continuous p-dimensional random variable:
u′E[XX′]u = E[u′XX′u] = E[(X′u)2] > 0,
since P(X′u = 0) = 0. Moreover, λ∥Aβ∥ is a convex function on β and, therefore, E[(Y −
1
X′β)2+λ∥Aβ∥ ] is also a strictly convex function on β.
1
On the one hand, the eigenvalues of the Hessian matrix are all positive and, in particu-
lar, the smallest eigenvalue, say γ , will be non-zero. On the other hand, using the spectral
r
decomposition of a symmetric matrix,
E[(Y −X′β)2] = β′E[XX′]β−2E[YX]β+E[Y2] = β′Q′DQβ−2E[YX]β+E[Y2] ≥
≥ β′Q′DQβ− | 2E[YX]β | +E[Y2] ≥ γ ∥Qβ∥2−∥E[YX]∥∥β∥+E[Y2] =
r
= γ ∥β∥2−∥E[YX]∥∥β∥+E[Y2],
r
where, inthesecond-to-laststep, theCauchy-Schwarzinequalitywasused. As∥β∥ → +∞, then
E[(Y −X′β)2] → +∞, that is, the quadratic function h(β) = E[(Y −X′β)2] is coercive. The
next step in the proof is to transform the original true problem (17) into an equivalent one with
a feasible compact region B∗. Given β∗ ∈ B, since h(β) = E[(Y −X′β)2] is coercive, there
exists R such that
E[(Y −X′β)2] > E[(Y −X′β∗)2+λ∥Aβ∗∥ ],
1
forallβ with∥β∥ > R. Forthatreason,theproblem(17)canbereducedtothefeasiblecompact
region B∗ = B∩{β : ∥β∥ ≤ R}, which implies that the optimal solution is reached.
Finally, the uniqueness of solution is a consequence of the strict convexity of the objective
function.
Proof of Theorem 2
For the sake of simplicity, βCSCLasso(λ) and βˆCSCLasso(λ) will be denoted henceforth by β and
βˆ, respectively. In addition, let us consider the nonempty compact set C = B∩{β : ∥β∥ ≤ R},
where R is chosen according to the proof of Theorem 3.1.
32Theorem 2 is a direct consequence of Theorem 5.3 in [29] under some technical conditions,
namely:
C1. The expected value function E[(Y −X′β)2 +λ∥Aβ∥ ] is finite valued and continuous on
1
C.
1
C2. (cid:80)n ((y −x′β)2+λ∥Aβ∥ ) converges to E[(Y −X′β)2+λ∥Aβ∥ ] w.p. 1, as n → ∞,
n i=1 i i 1 1
uniformly in β ∈ C.
Let us denote F(β,(Y,X)) = (Y −X′β)2+λ∥Aβ∥ . Then, the previous conditions C1 and
1
C2 are consequences of Theorem 7.48 in [29] provided that
A1. for any β ∈ C, the function F(·,(Y,X)) is continuous at β for almost every (Y,X),
A2. the function F(β,(Y,X)), with β ∈ C, is dominated by an integrable function,
A3. the sample is i.i.d.
Given (Y,X), the function (Y −X′β)2 +λ∥Aβ∥ is continuous at β for any β ∈ C, and
1
therefore A1 is fulfilled. The sample is i.i.d. by hypothesis, and thus A3 holds too. Finally,
in order to prove A2, it is necessary to find a measurable function g(Y,X) > 0 such that
E[g(Y,X)] < ∞ and, for every β ∈ C, | F(β,(Y,X)) |≤ g(Y,X) w.p. 1. Using the Cauchy-
Schwarz inequality, one has,
| F(β,(Y,X)) |=| (Y −Xβ)2+λ∥Aβ∥ |=
1
=| Y2−2YX′β+β′XX′β+λ∥Aβ∥ |≤
1
≤ Y2+(X′β)2+2 | YX′β | +λ∥Aβ∥ =
1
= Y2+ | X′β |2 +2 | YX′β | +λ∥Aβ∥ ≤
1
≤ Y2+∥X∥2∥β∥2+2∥YX∥∥β∥+λ∥Aβ∥ .
1
Let M and M be given by
1 2
M = max∥β∥ M = max|Aβ|
1 2
β∈C β∈C
which are well defined due to the compactness of C. Therefore, g can be chosen as
g(Y,X) = Y2+M2∥X∥2+2M ∥YX∥+λM ,
1 1 2
which is positive and, since E(∥X∥2) < ∞, E(Y2) < ∞, E(∥YX∥) < ∞, its expected value is
finite. In consequence, A2 holds and the proof is concluded.
33Appendix: further results
Figure 7: Heat maps of βˆCSCLasso(λ) = (βˆCSCLasso(λ),...,βˆCSCLasso(λ)) using prostate
1 8
dataset
34Figure 8: Median MSE over the test sets for k = 7,...,20 under p = 20 features and the two
k
n options. Each subgraph represents one group and the Y-axis shows the different percentages
k
of improvement
35Figure 9: Median MSE over the test sets for k = 7,...,20 under p = 500 features and the two
k
n options. Each subgraph represents one group and the Y-axis shows the different percentages
k
of improvement
36Figure 10: Median overall MSE over the test sets and NZ percentage under the choice p = 20
(top) and p = 500 (bottom)
37To fully understand how the computation time behaves depending on n and p values, a grid
k
in both parameters have been inspected. Figure 11 displays the logarithm of the user times in
seconds obtained under Lasso and CSCLasso models when n and p change. The perspective
k
drawn in the top left figure shows that Lasso model (bottom surface) is solved faster and in a
smoother way. Besides, whereas smaller times are obtained for both methods when n and p
k
are small, the biggest times are associated to n = 300 and p = 500.
k
Figure 11: Four perspectives of the logarithm of the user times in seconds for Lasso (bottom
surface in the four graphics) and CSCLasso (top surfaces) models across a grid in n and p
k
References
[1] U.S. Department of Commerce, Bureau of the Census, Census Of Population And Housing
1990 United States: Summary Tape File 1a & 3a (Computer Files), U.S. Department
38Of Commerce, Bureau Of The Census Producer, Washington, DC and Inter-university
Consortium for Political and Social Research Ann Arbor, Michigan (1992).
[2] U.S. Department of Justice, Bureau of Justice Statistics, Law Enforcement Management
And Administrative Statistics (Computer File) U.S. Department Of Commerce, Bureau Of
The Census Producer, Washington, DC and Inter-university Consortium for Political and
Social Research Ann Arbor, Michigan. (1992).
[3] U.S. Department of Justice, Federal Bureau of Investigation, Crime in the United States
(Computer File) (1995).
[4] J. P. Bradford, C. Kunz, R. Kohavi, C. Brunk, and C. E. Brodley. Pruning decision trees
with misclassification costs. In C. N´edellec and C. Rouveirol, editors, Machine Learning:
ECML-98, pages 131–136, Berlin, Heidelberg, 1998. Springer Berlin Heidelberg.
[5] P. Bu¨hlmann and S. Van-De Geer. Statistics for high-dimensional data. Springer, 2011.
[6] E.Carrizosa,B.Mart´ın-Barrag´an,andD.R.Morales. Multi-groupsupportvectormachines
with measurement costs: A biobjective approach. Discrete Applied Mathematics, 156:950–
966, 2008.
[7] E. Carrizosa and D. Romero-Morales. Combining minsum and minmax: A goal program-
ming approach. Operations Research, 49(1):169–174, 2001.
[8] S. Datta and S. Das. Near-Bayesian support vector machines for imbalanced data classifi-
cation with equal or unequal misclassification costs. Neural Networks, 70:39–52, 2015.
[9] D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Wavelet shrinkage:
Asymptopia? Journalof theRoyalStatisticalSociety.Series B(Methodological),57(2):301–
369, 1995.
[10] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. The Annals
of Statistics, 32(2):407–499, 04 2004.
[11] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American Statistical Association, 96(456):1348–1360, 2001.
39[12] A.Freitas,A.Costa-Pereira,andP.Brazdil. Cost-sensitivedecisiontreesappliedtomedical
data. In I. Y. Song, J. Eder, and T. M. Nguyen, editors, Data Warehousing and Knowledge
Discovery, pages 303–312, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.
[13] J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning. Springer-
Verlag, Heidelberg, 2001.
[14] B. R. Gaines, J. Kim, and H. Zhou. Algorithms for Fitting the Constrained Lasso. Journal
of Computational and Graphical Statistics, 27(4):861–871, 2018.
[15] M. J. Garside. The Best Sub-Set in Multiple Regression Analysis. Journal of the Royal
Statistical Society: Series C (Applied Statistics), 14(2-3):196–200, 1965.
[16] L. Gurobi Optimization. Gurobi optimizer reference manual, 2018.
[17] T. Hastie, R. Tibshirani, andM. Wainwright. Statistical Learning with Sparsity. New York:
Chapman and Hall/CRC, 2015.
[18] H. He and Y. Ma. Imbalanced learning: foundations, algorithms, and applications. John
Wiley & Sons, 2013.
[19] A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55–67, 1970.
[20] Q. Hu, P. Zeng, and L. Lin. The dual and degrees of freedom of linearly constrained
generalized lasso. Computational Statistics & Data Analysis, 86:13 – 26, 2015.
[21] G. M. James, C. Paulson, and P. Rusmevichientong. Penalized and Constrained Optimiza-
tion: An Application to High-Dimensional Website Advertising. Journal of the American
Statistical Association, 0(0):1–31, 2019.
[22] T. Kouno, M. de Hoon, J. C. Mar, Y. Tomaru, M. Kawano, P. Carninci, H. Suzuki,
Y. Hayashizaki, and J. W. Shin. Temporal dynamics and transcriptional control using
single-cell gene expression analysis. Genome Biology, 14(10):R118, Dec 2013.
[23] W. Lee, C.-H. Jun, and J.-S. Lee. Instance categorization by support vector machines
to adjust weights in adaboost for imbalanced data classification. Information Sciences,
381(Supplement C):92 – 103, 2017.
40[24] M. Lichman. UCI Machine Learning Repository, 2013.
[25] E.OllierandV.Viallon. Regressionmodellingonstratifieddatawiththelasso. Biometrika,
104(1):83–96, 2017.
[26] R.C.Prati, G.E.A.P.A.Batista, andD.F.Silva. Classimbalancerevisited: anewexper-
imental setup to assess the performance of treatment methods. Knowledge and Information
Systems, 45(1):247–270, Oct 2015.
[27] M. Redmond and A. Baveja. A data-driven software tool for enabling cooperative in-
formation sharing among police departments. European Journal of Operational Research,
141(3):660 – 678, 2002.
[28] R. T. Rockafellar. Convex analysis. Princeton University Press, 1972.
[29] A. Shapiro, D. Dentcheva, and A. Ruszczyn´ski. Lectures on stochastic programming: mod-
eling and theory. SIAM, 2009.
[30] N. Simon, J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for Cox’s
proportionalhazardsmodelviacoordinatedescent. Journal of Statistical Software, 39(5):1–
13, 2011.
[31] T. A. Stamey, J. N. Kabalin, J. E. McNeal, I. M. Johnstone, F. Freiha, E. A. Redwine, and
N. Yang. Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the
prostate: II.Radicalprostatectomytreatedpatients. Journal of Urology, 141(5):1076–1083,
1989.
[32] Y. Sun, A. K. Wong, and M. S. Kamel. Classification of imbalanced data: A review.
International Journal of Pattern Recognition and Artificial Intelligence, 23:687–719, 2009.
[33] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society. Series B (Methodological), 58(1):267–288, 1996.
[34] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via
the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
67(1):91–108, 2005.
41[35] R. J. Tibshirani and J. Taylor. The solution path of the generalized Lasso. The Annals of
Statistics, 39(3):1335–1371, 2011.
[36] A.Torres-Barr´an, C.M.Ala´ız, andJ.R.Dorronsoro. ν-SVMsolutionsofconstrainedLasso
and Elastic net. Neurocomputing, 275:1921 – 1931, 2018.
[37] G. Yu and Y. Liu. Sparse regression incorporating graphical structure among predictors.
Journal of the American Statistical Association, 111(514):707–720, 2016.
[38] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67,
2006.
[39] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical
Association, 101(476):1418–1429, 2006.
[40] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of
the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.
42