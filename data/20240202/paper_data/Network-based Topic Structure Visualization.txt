APPLICATION NOTE
Network-based Topic Structure Visualization
Yeseul Jeon a,b, Jina Park a,b, Ick Hoon Jin a,b and Dongjun Chungc
a Department of Statistics and Data Science, Yonsei University, Seoul, South Korea; bDepartment of
Applied Statistics, Yonsei University, Seoul, South Korea; cDepartment of Biomedical Informatics, The
Ohio State University, Columbus, Ohio, U.S.A
ARTICLE HISTORY
CompiledFebruary1,2024
ABSTRACT
In the real world, many topics are inter-correlated, making it challenging to investigate their
structureandrelationships.Understandingtheinterplaybetweentopicsandtheirrelevancecan
provide valuable insights for researchers, guiding their studies and informing the direction of
research.Inthispaper,weutilizethetopic-wordsdistribution,obtainedfromtopicmodels,as
item-responsedatatomodelthestructureoftopicsusingalatentspaceitemresponsemodel.
By estimating the latent positions of topics based on their distances toward words, we can
capture the underlying topic structure and reveal their relationships. Visualizing the latent
positions of topics in Euclidean space allows for an intuitive understanding of their proximity
andassociations.Weinterpretrelationshipsamongtopicsbycharacterizingeachtopicbasedon
representative words selected using a newly proposed scoring scheme. Additionally, we assess
the maturity of topics by tracking their latent positions using different word sets, providing
insights into the robustness of topics. To demonstrate the effectiveness of our approach, we
analyze the topic composition of COVID-19 studies during the early stage of its emergence
usingbiomedicalliteratureinthePubMeddatabase.Thesoftwareanddatausedinthispaper
arepubliclyavailableathttps://github.com/jeon9677/gViz.
KEYWORDS
LatentSpaceItemResponseModel;TopicEmbedding;TopicStructureVisualization;Text
Mining;NetworkAnalysis
1. Introduction
Mostrealworlddatararelyconsistsofindependenttopicsbutrathermultipletopicsthatareof-
ten inter-correlated. By exploring the relationships and dependencies among topics, researchers
can gain a deeper understanding of the underlying structure and dynamics of their research
domain. This understanding can help identify emerging trends, uncover research gaps, and
highlight areas of high importance or potential impact. It can help researchers be knowledge-
able of research areas that are emerging and/or significant to the field and allow them to put
their efforts on important ones. There have been attempts to model correlated structure among
topicsbymodelingthecorrelatedstructurewithinthetopicmodelorcombiningthetopicmodel
with statistical models. For instance, Blei and Lafferty [3] regarded the topic of correlation as a
structureofheterogeneity.Tocapturetheheterogeneityoftopics,theysuggestedthecorrelated
topic model (CTM), which models topic proportions to exhibit correlation through the logistic
normal distribution within Latent Dirichlet Allocation (LDA). Rusch et al. [11] combined LDA
with decision trees to interpret the topic relationships from Afghanistan war logs. It classified
the topics using tree structures, which helped to understand the different circumstances in the
CONTACTIckHoonJin(ijin@yonsei.c.kr)andDongjunChung(chung.911@osu.edu)
4202
naJ
13
]PA.tats[
1v55871.1042:viXraAfghanistan war. However, these approaches do not directly allow us to investigate the degree
of correlation and closeness among topics. To address this, Sievert and Shirley [13] attempted
toquantifytherelationshipsandproximitybetweentopicsbycalculatingthedistancesbetween
topics.Specifically,theyusedmulti-dimensionalscaling(MDS)toembedthetopicsonametric
space based on a distance matrix calculated from the output of a topic model (LDA). However,
MDS requires specification of similarity and dissimilarity measures to construct the distance
matrixandtheresultscanbesignificantlyaffectedbythechoiceofmeasuresinspiteoftheirad
hocness. In practice, it is often difficult to determine the appropriate measures for calculating
and defining distances between topics. Hence, it is beneficial to have a unified approach to esti-
mate the structure of topics, which allows us to avoid such ad hoc choices and guarantee more
consistent results. One of the statistical methods for handling correlated structure is network
analysis. Specifically, network model estimates the relationships among nodes based on their
dependency structure. Moreover, it can provide global and local representation of nodes at the
same time. In this context, Mei et al. [9] tried to identify topical communities by combining a
topic model with a social network approach. Specifically, it estimated topic relationships given
the known network structure and tried to separate out topics based on connectivity among
them. However, in this approach, the network information needs to be provided to construct
and visualize the topic network, which is not a trivial task in practice.
To address this, here we propose an alternative approach to examine the structure of topics
using the information from the output of topic models. Specifically, our approach leverages a
latentspacenetworkmodel,whichestimatestherelationshipsamongnodesbyestimatingtopics’
latent positions. The latent positions can then be used to quantify the relationships among
nodes, i.e., the distances between topics. In our study, we treat the output of the topic-words
distribution as a topic-words matrix X, which can be seen as an item-response data [5] with
an inter-correlated structure. Using this matrix, we simultaneously estimate the relationship
between topics and words, which yields the latent position of topics. When two topics share
similar word meanings, their latent positions are located close to each other, making it easier
tounderstandrelationshipsbetweentopicsbasedontheirassociatedwords.Furthermore,when
topicssharecommonwords,theirlatentpositionstendtobelocatedneartheorigin.Conversely,
topics that incorporate more distinctive and exclusive words compared to other topics tend to
be positioned farther away from the origin. This behavior elucidates the relationship between
the words associated with topics and their respective positions in the latent space. This also
enables us to visualize the structure of topics using the estimated latent positions.
In this approach, we interpret the relationships among topics by characterizing topic based
on its representative words. If two topics are closely located in the latent positions, they might
deliver similar meanings in documents. To make the process to select the meaningful words
within the topics more efficient and objective, it is desirable to have a score calculated based
on the information provided by our analytical framework. Previously Airoldi and Bischof [1]
evaluatedthetopicsusingthenovelscorecalledFREX,whichquantifieswords’closenesswithin
the topic based on the word frequency and exclusivity. Here, FREX reflects how word j is
i,j
exclusivelyclosetotopicicomparedtoothertopics.However,relyingsolelyonthefrequencyof
wordsindocumentsisnotenoughtoidentifykeywordswithintopicsthatreflecttheassociation
among topics. To address this, here we propose a score that takes into account both (i) the
probability of words belonging to each topic based on the frequency of associated words using
the topic model; and (ii) how closely words are exclusively related to each topic by considering
therelevancebetweentopicsusingthenetworkmodel.Byputtingthesetwopiecesofinformation
together, our analytical framework can pick up information about relativeness and exclusivity.
It is also important to acknowledge the fact that topics also vary in the sense of how mature
it is and whether it is common or specific. We can evaluate this by checking whether latent
positions of topics are robust to different word sizes. This is based on the rationale that mature
topics and/or those consisting of common words might remain similar and stable upon changes
2in word sets, compared to emerging or specific topics. Hence, to understand the changes in
latentpositionsoftopics,wetrackthelatentpositionsoftopicsbydifferingwordsets.Toselect
words sets of different sizes, we used two criteria to select informative words: high coefficient
variation and high maximum probability from topic-words distribution. First, words with large
variationsinprobabilitiesamongtopicsareexpectedtobemoreimportantbecauseifawordhas
lowvariationacrosstopics,itislikelythattheworddoesnotrepresentanytopicspecifically.We
usedcoefficientofvariationtocharacterizewords’dispersionamongtopics.Second,ameaningful
word should have a high probability in at least one topic. Even when a word has high variation
amongtopics,ifthatwordhaveonlylowprobabilitiesacrosstopics,itmightnotbeinformative
to differentiate topics.
The rest of this article is organized as follows. In Section 2, we introduce our dataset and the
Gaussian version of the latent space item response model [8], which estimates latent positions
of topics based on their latent distances between words. In addition, we introduce our scoring
scheme that helps to select representative words from each topic to enhance the understanding
of the topic structure. In Section 3, we apply our approach to the COVID-19 literature to
evaluate and demonstrate the usefulness of our approach. In Section 4, we summarize our topic
structure visualization with conclusion.
2. Methodology
2.1. Data
Since the emergence of the worldwide pandemic of COVID-19, relevant research has been pub-
lishedatadazzlingpace,whichyieldsanabundantamountofbigdatainbiomedicalliterature.
Due to the high volume of relevant literature, it is practically impossible to follow up the re-
search manually. Furthermore, in the early stages of research when a specific topic has not yet
been established, many studies are likely to be connected to each other. Therefore, by exam-
ining which topics are being discussed together, we can more easily understand the research
direction of COVID-19. For instance, if some researchers want to investigate a specific topic,
say ‘COVID-19 symptoms’, we want to answer the questions like the following: (1) Which set
of words are associated with ‘COVID-19 symptoms’? (2) Are there any other topics related
to ‘COVID-19 symptoms’, which can be used for extending and elaborating research? (3) Is
‘COVID-19 symptoms’ a common or specific topic?
TomodeltheCOVID-19literature,wedownloadedtheCOVID-19articlespublishedfromthe
PubMeddatabase(https://pubmed.ncbi.nlm.nih.gov)withatime-framebetweenDecember
1st,2019,andAugust3rd,2020,coincidingwiththedateoftheWHO’sdesignationofCOVID-
19 as a pandemic. We collected articles whose titles contain “coronavirus2”, “covid-19” or
“SARS-CoV-2”, which resulted in a total of 35,585 articles. Since COVID-19 was a worldwide
pandemic that spread at an unprecedented rate, some articles that examine COVID-19 have
been published with only an abstract. After eliminating articles without abstracts (i.e., only
titles or abstract keywords), our final text data contained a total of 15,015 abstracts.
Based on the abstracts, we constructed the corpus using the abstract keywords that con-
cisely captured the messages delivered by the paper. To enrich the corpus, we further used the
word2vec approach [10] to train against relationships between nouns from the abstract and
the abstract keywords. Specifically, word2vec extracted nouns from abstracts, which were em-
bedded near the abstract keywords, and added those selected words to the corpus. Using the
trained word2vec network with 256 dimensions, we selected ten words from the abstract nouns
that were near each abstract keyword. We provide the details of our training strategy for the
word2vec network in the Supplemental Materials (see Appendix A). The corpus construction
resulted in 9,643 words from 15,015 documents. We further filtered out noise words, including
singlealphabets,numbers,andotherwordsthatarenotmeaningful,e.g.,‘p.001’,‘p.05’,‘n1427’,
3‘l.’, and ‘ie’. Finally, to obtain more meaningful topics, we removed common words like ‘data’,
‘analysis’, ‘fact’, and ‘disease’. We provide the full list of filtered keywords in the Supplemental
Materials (see Appendix B).
Toobtainthetopic-wordsmatrixX,weimplementedthetopicmodeling.Sincetheabstracts
of the COVID-19 literature are categorized as short text, conventional topic models including
LDA often suffer from poor performance due to the sparsity problem. Yan et al. [14] made
importantprogressinthemodelingofshorttextdata,theso-calledBitermTopicModel(BTM).
Unlike the LDA, BTM replaced words with bi-terms, where a bi-term is defined as a set of two
words occurring in the same document. This approach attempted to compensate for the lack
of words in a short text by pairing two words, thereby creating more words in the document.
This is based on observation that if two words are mentioned together, they are more likely to
belong to the same topic. Using the BTM, we were able to obtain a topic-words distribution,
which is essentially a matrix of topics and their associated words X. This matrix X was then
used as an input for the latent space item response model.
2.2. Estimating Topic Relationships Using Latent Space Item Response Model
Hoff et al. [6] proposed the latent space model, which expresses a relationship between actors of
anetworkinanunobserved“socialspace”,so-calledlatentspace.InspiredbyHoffetal.[6],Jeon
et al. [8] proposed the latent space item response model (LSIRM) that viewed item response
as a bipartite network and estimated the relationship between respondents and items using the
latent space. In order to achieve our objective of estimating the topic structure and visualizing
their relationships, we utilized the concept of latent positions. These latent positions allow us
to estimate the relevance of topics based on the distances between topics and associated words.
By measuring the closeness between topics in terms of their latent positions, we can intuitively
represent their distances. To estimate the latent positions of topics, we employed LSIRM with
a bipartite network representation using the matrix X. In this network, each item represents a
topic i and respondents correspond to word j. However, the original LSIRM proposed by Jeon
et al. [8] cannot be directly applicable here because it was designed for binary item response
dataset, where each cell in the item response data has a binary value (0 or 1). On the contrary,
here our input data X has continuous probabilities indicating how likely each word belongs to
each topic. Therefore, in order to apply LSIRM to our input data X, we use Gaussian version
of LSIRM, which is described in detail below. The modified Gaussian version of LSIRM can be
written as
x Θ=β +θ v u +ϵ ,
i,j i j i j i,j
| −|| − ||
ϵ N(0,σ2),
i,j
∼
wherex indicatesthelogscaleprobabilitythatwordj belongstotopici,fori=1, ,P and
i,j
···
j = 1, ,N. Because the original LSIRM use logit link function to handle the binary data,
···
hereweusethelinearityassumptionbetweenx andtheattributepartwiththerelevancepart.
i,j
We added an error term ϵ N(0,σ2) to satisfy the normality equation. We use the notation
j,i
∼
Θ = β = β ,θ = θ ,U = u ,V = v , and v u represents the Euclidean
i j j i i j
{ { } { } { } { }} || − ||
distance between latent positions of word j and topic i. We estimate the relevance between
topics by measuring the distances between topics. Note that the distances between topics are
measured based on their relationships with words. Specifically, if two topics share similar word
sets, we consider them to have a close relationship and estimate their relationship accordingly.
Given the model described above, we use Bayesian inference to estimate parameters in the
4Gaussian version of LSIRM. We specify prior distributions for the parameters as follows:
β N(0,1),
i
∼
θ σ2 N(0,σ2), σ2 >0
j | ∼ θ
σ2 Inv-Gamma(a,b), a 0, b>0
>
∼
σ2 Inv-Gamma(a ,b ), a >0, b >0
θ ∼ σ σ σ σ
u MVN (0,I )
j ∼ d d
v MVN (0,I ),
i ∼ d d
where 0 is a length-d vector of zeros and I is the d d identify matrix. We treat β as a fixed
d i
×
effectandθ asarandomeffectinordertoaccountforthevariabilityamongwords.Byincluding
j
a random effect, the model acknowledges that different words may exhibit unique patterns and
behaviors towards specific topics. The posterior distribution of LSIRM is
π(Θ,σ2 |X)
∝
P(x
i,j
|Θ) π(θ
j
|σ θ2)π(σ θ2) π(β i) π(u j) π(v i)π(σ2)
j i j i j i
YY Y Y Y Y
andweuseMarkovChainMontelCarlo(MCMC)toestimatetheparametersofLSIRM.Inthis
way,wecanobtainlatentpositionsofu andv ontheRd.Sinceweareinterestedinconstructing
j i
the topic network, we utilize v ,i = 1, ,P and make it as matrices of A RP d where row
i ×
··· ∈
indicates P number of topics and column indicates d number of dimension of coordinates.
The latent positions of topics are assumed to follow a standard normal distribution. As a
result, there is a tendency for these latent positions to spread out from the origin. Specifically,
when topics share common words, their latent positions tend to be located near the origin,
indicating their similarity. On the other hand, topics with more distinctive and exclusive words
compared to other topics tend to be positioned farther away from the origin, indicating their
uniqueness. This behavior illustrates the relationship between the words associated with topics
and their corresponding positions in the latent space. Therefore, latent positions of topics not
only enable us to visualize the dependency structure of topics, but also help understand the
topic structure through their spreading tendency.
Here we further improve the interpretation of relationships among topics by tracing how
topics’ latent positions change as a function of word sets. In other words, we compare topics’
latentpositionsA fromthevarioussetsofmatricesX ,k =1, ,K toestimateandtracetheir
k k
···
relevanceamongtopicsbasedonwordsetswithdifferentsizes.AfterproceedingwiththeLSIRM
model with various sets of matrices X , we can obtain matrices of A , composed of coordinates
k k
of each topic. Since the distances between pairs of words and topics, which are crucial for
inferringlatentpositions,canbesubjecttovariationsduetorotation,translation,andreflection,
multiple possible realizations of latent positions can exist. These variations in the likelihood
function necessitate careful consideration to ensure accurate estimation and inference [6, 12].
In order to tackle this invariance property for determining latent positions, we implemented
within-matrix Procrustes matching [4] as post-processing of MCMC samples. Specifically, we
implemented a Procrustes matching two times. First, we implement so-called within-matrix
matching within the MCMC samples for each topic’s latent positions generated from LSIRM.
Second,weimplementedso-calledbetween-matrixmatchingfortheestimatedmatricestolocate
topics in the same quadrant. To align the latent positions of topics, we need to set up the
baseline matrix, denoted as A , which maximizes the dependency structure among topics.
max
To measure the degree of dependency structure, we take the average of the distances of topics’
latentpositionsfromtheorigin.Thelongerdistanceoflatentpositionsfromtheoriginimpliesa
stronger dependency on the network. It helps nicely show the change of topics’ latent positions
because those rotated positions A from each matrix X are based on the most stretched-out
k k
5network from the origin. As a result, we can obtain the re-positioned matrices A , which still
∗k
maintain the dependency structure among topics but are located in the same quadrant.
With the oblique rotation, the interpretation of axes can be further improved and topics can
be categorized based on these axes. For this purpose, we applied the oblim rotation [7] to the
estimatedtopicpositionmatrixA ,usingtheRpackageGPAroation[2].Wedenotetherotated
∗k
topic position metric by B . To interpret the trajectory plot showing traces of topics’ latent
k
positions, we extracted the rotation information matrix (R) resulting from an oblique rotation
as the baseline matrix B . Then, we multiplied each matrix (B ) by the rotation matrix (R)
base k
to plot the topics’ latent positions.
2.3. Scoring the Words Relation to Topics
BycombiningtheideafromAiroldiandBischof[1]withtherelevanceinformationamongtopics
in our problem, here we propose the score s which measures the exclusiveness of word j in
i,j
the topic i. We define score s as
i,j
w w
1 2
s = +
i,j
2 ECDF (δ ) 2 ECDF (δ )
(cid:18) − δ .,j i,j (cid:19) (cid:18) − δ i,. i,j (cid:19) (1)
w w
3 4
+ + ,
1+ECDF (γ ) 1+ECDF (γ )
(cid:18) γ .,j i,j (cid:19) (cid:18) γ i,. i,j (cid:19)
wherew ,w ,w ,andw aretheweightsforexclusivity(here,wesetw =w =w =w =0.25)
1 2 3 4 1 2 3 4
andECDFistheempiricalCDFfunction.Here,δ denotestheprobabilityofwordj belonging
i,j
to topic i given by BTM, and γ is the distance between the latent position of word j and
i,j
topic i estimated by LSIRM. The higher value of δ indicates the closer relationship between
i,j
word j and topic i and the smaller value of γ indicates the shorter distance between word j
i,j
and topic i. To make the meaning of the shorter distance and the higher probability consistent
as both contribute to the higher scores, first, we subtracted ECDF (δ ) and ECDF (δ )
δ i,j δ i,j
.,j i,.
from 2. Note that here we use 2 to avoid the zero in the denominator. For example, if δ has
i,j
a higher probability within the topic i and between the other topics and word j, then both
2 ECDF and 2 ECDF will have smaller values. This means that the word j is
δ (δ ) δ (δ )
− .,j i,j − i,. i,j
distinctive enough to represent the meaning of topic i. Second, if the latent distance between
word j and topic i has the shorter distance within topic i than between the other topics and
word j, then both 1+ECDF and 1+ECDF have the smaller values contributing
γ (γ ) γ (γ )
.,j i,j i,. i,j
to a higher score s . Here we added 1 to each denominator term, preventing the denominator
i,j
from becoming zero. Based on s , we can determine whether the word j and the topic i are
i,j
close enough to be mentioned in the same document. By collecting the high-score words, we
can characterize the topics.
3. Application
3.1. Topic-words matrix of COVID-19 Biomedical literature using BTM
To implement BTM, we set the topic number to 20. For the hyper-parameters, we assigned
α = 3 and β = 0.01. Since our main goal is to estimate the topic structure, we empirically
searchedanddeterminedthehyper-parametersofBTM.Weexplainthedetailsofthemodeling
the topic model in the Supplemental Materials (see Appendix C). The posterior distribution of
thetopic-wordswasestimatedusingtheGibbssampler.Specifically,wegeneratedsampleswith
50,000 iterations after the 20,000 burn-in iterations and then implemented thinning for every
100th iteration.
6Ineachtopic-wordsdistributionobtainedfromBTM,wordswithhighprobabilitiescharacter-
izethetopic.Sincethetopic-wordsdistributioncontainsallthewordsetsfromthecorpus,using
all word sets can lead to redundancy. To confirm this, we draw histogram of log-transformed
probabilities and it shows bimodal topic-words distributions. This pattern indicates that there
are some words that had low probabilities of belonging to a specific topic, whereas the mode
in the center corresponds to the words that have probabilities high enough to characterize the
meaning of the topic. Therefore, it might be more desirable to estimate topic structure using
only the words corresponding to the mode in the center rather than all the words. On the other
hand, our exploratory analysis indicated that more than 1,000 words are needed to represent
topics properly. Based on this rationale, we decided to use at least 1,000 words to estimate
the positions of topics in the latent space based on the positions of words. Given the selected
minimum number of words, we extracted meaningful words that can characterize topics based
on the coefficient of variation and maximum probability.
Given the coefficient of variation and maximum probability, we obtained multiple matrices
corresponding to the top 60% to 40% of words determined based on the two criteria. The num-
bers of words corresponding to the 60th and 40th percentiles were 2,648 and 1,095, respectively.
We used 21 sets of matrices, X (k = 40%, ,60%), as the LSIRM input data, where their
k
···
dimensions were ranged from 2,648 20 to 1,095 20. In this way, we obtained the 21 sets
× ×
of matrices X and we considered 20 topics for all the matrix sets. We applied a log transform
k
to the values in these matrices, which originally ranged between 0 and 1, to convert them into
continuous values ranging from to . MCMC was implemented to estimate topics’ latent
−∞ ∞
positions V = v where i = 1, ,20. The MCMC ran 55,000 iterations, and the first 5,000
i
{ } ···
iterations were discarded as burned-in processes. Then, from the remaining 50,000 iterations,
we collected 10,000 samples using a thinning of 5. To visualize relationships among topics, we
used two-dimensional Euclidean space. Additionally, we set 0.28 for β jumping rule, 1 for θ
jumping rule, and 0.06 for w and z jumping rules. Here, we fixed prior β follow N(0,1). We
j i
set a =b =0.001.
σ σ
LSIRM takes each matrix X as input and provides the A matrix as output after the
k k
Procrustes-matchingwithinthemodel.Sincewecalculatedtopics’distanceonthe2-dimensional
Euclidean space, A is of dimension 20 2. We visualized the topic structure using the baseline
k
×
matrix A so that we can compare topics’ latent positions without having identifiable issues
max
from the invariance property. From A , we calculated the distance between the origin and
k
each topic’s coordinates. The closer distance of a topic position from the origin indicates the
weakerdependencywithothertopics.InourinterrogationofA ,wefoundthatthedependency
k
structure among topics starts to be built up from A . There are two possibilities that can
47%
lead to low dependency. First, it is possible that a small number of words could distinguish the
characteristicsoftheirtopicsfromtheothertopics.Second,itispossiblethatmostofthewords
werecommonlysharedwithothertopics.WeprovidethedistanceplotofA intheSupplemental
k
Materials(seeAppendixD).Basedonthisrationale,wechoseA asthebaselinematrixA .
47% max
WiththisbaselinematrixA ,weimplementedtheProcrustesmatchingtoalignthedirection
47%
of the topic’s latent positions from each matrix A . Using this process, we could obtain the
k
A matrix matched to the baseline matrix A . We named the identified topics based on top
∗k 47%
rankingwordsusingtheA matrixbecausethebaselinematrixA hasthemostsubstantial
47% 47%
dependencystructure.WeappliedobliminrotationtotheestimatedtopicpositionmatrixA
∗k%
using the R package GPArotation[2], and obtained matrix B k = 40%, ,60% with the
k
···
rotation matrix R. By rotating the original latent space, we could obtain more interpretable
axes for the identified latent space (e.g., determining the meaning of a topic’s transition based
on the X-axis or Y-axis).
73
0.9
2.5
0.9
12
0.0 11
18 131149202
id 0.8 id 7 0 0
15 6 1 1
0.8
−2.5 4 817
9 0.7
1
−5.0 16 5 0.7
10
−3 0x 3 0.25 0.A50ffinity 0.75 1.00 0.00 0.02 Affinity 0.04 0.06
(a)Topics’latentpositionsofB (b) The score and affinity plot of (c) The score and affinity plot of
47%
Topic19inB Topic3inB
47% 47%
Figure 1. Iftopicsarelocatedinthecenterinlatentposition(a),twodistinctsubgroupsareobservedinthescoreand
affinity plot (b). Otherwise, such subgroup patterns are not observed in the score and affinity plot (c). In (b) and (c),
orange color (id of 1) indicates top 20% of words of high score words while green color (id of 0) indicates the remaining
words.
3.2. Words selection for each topic based on the score
Wecalculatedthescores foreachwordj withtopicibasedoneachB fork =40%, ,60%.
i,j k
···
Figure 1 shows the two different scenarios of the plot of s and affinity (exp( γ )) in B ;
i,j i,j 47%
−
onewithahighscoreandhighaffinity,andtheotherwithalowscorebuthighaffinity.Figure1a
shows the estimated latent positions of topics obtained from B . We can see that topics 2, 7,
47%
13, 14, 19, and 20 are located around the origin, indicating their general nature and similarity
in terms of their word composition. These common topics share many words in common with
other nearby topics, making it challenging to discern their unique characteristics solely based
on word probabilities (Figure 1b). On the other hand, the remaining topics are positioned away
from the origin, indicating that they have distinctive characteristics compared to other topics
(Figure 1c). For these topics, it is important to understand their direction and identify which
topics are closely related. We provide score and affinity plot of each topic from B in the
47%
Supplemental Materials (see Appendix E). We extracted the top 20% of words with high values
in score to interpret each topic and we collected the meaningful words that commonly appear
on the top of the list among every portion of words set B ,k = 40%, ,60%. Table 1 shows
k
···
the interpretation of each topic made based on the selected words with its top score words. We
provide more detailed interpretation of the topics in the Supplemental Materials (see Appendix
F).
3.3. COVID-19 Topic Structure Visualization
We interrogated what kinds of topics have been extensively studied in the biomedical literature
on COVID-19. In addition, we also studied how those topics were related to each other, based
ontheirclosenessinthesenseoflatentpositions.Wealsopartiallyclusteredthetopicsbasedon
their relationships using the quadrants. This allows us to check which studies about COVID-19
are relevant to each other and could be integrated. Figure 2 displays the trajectory plot and it
shows how topics were positioned on the latent space and how these topics make the transition
across word sets of different sizes. Here the direction of arrows refers to how topics’ coordinates
changed as a function of the numbers of words, where each arrow moved from B to B as
60% 40%
the number of words decreased.
According to Figure 2, we observed two distinct groups. The first group consists of topics
that have common words shared among them, while the second group consists of topics that
8
y erocS erocSFigure2. TopicarchitecturevisualizationofCOVID-19literaturewiththreetopicclusters(A)-(C).Thearrowindicates
the direction of latent positions by increasing the words size. (A) COVID-19 and its impacts on different areas. (B)
SymptomsandtreatmentofCOVID-19andrelevantstudies.(C)MolecularstudiesassociatedwiththeCOVID-19infection.
9Table 1. Interpretation of the topic based on A∗47% matrix.Theabbreviationismarkedbyanasteriskandthe
moredetailedinterpretationcanbefoundintheSupplementalMaterials(seeAppendixG).
Topic Name Topscorewords
1 LungScan subpleural crazypaving bronchogram
(0.963) (0.957) (0.950)
2 CompoundandDrug ProteinDataBank papain-like intermolecular
(0.938) (0.936) (0.934)
3 Bacteria,Nano,andDiet DHA∗ biofilm vancomycin-resistant
(0.968) (0.968) (0.967)
4 TreatmentofOtherDiseases sarcoma arthroplasty neoadjuvant
(0.948) (0.944) (0.943)
5 Symptoms(Cardiovascular) vein antithrombotic VTE∗
(0.906) (0.899) (0.898)
6 Molecular-levelResponsetoInfection NLRP3∗ metalloproteinase upregulation
(0.929) (0.927) (0.926)
7 COVID-19RiskPredictionMarkers alanin BUN∗ prealbumin
(0.976) (0.972) (0.969)
8 LiteratureReview ProsperoDB WanfangDB meta-analys
(0.967) (0.966) (0.944)
9 SymptomandComorbidity petechiae guillain-barresyndrome ageusia
(0.958) (0.955) (0.952)
10 Cardiovascular infarction thromboprophylaxis VTE∗
(0.912) (0.909) (0.909)
11 SocialImpact classroom pedagogy zoom
(0.925) (0.911) (0.906)
12 FinancialandEconomicalImpact macroeconomics monetary profit
(0.929) (0.928) (0.928)
13 StatisticalModeling Weibull least-square MAE∗
(0.984) (0.975) (0.966)
14 COVID-19Test LFIAs∗ transcription-PCR∗ Wantai
(0.979) (0.974) (0.972)
15 Psychological/MentalIssues anxious PTSD∗ post-traumatic
(0.942) (0.940) (0.935)
16 LungScan procalcitonin ground-glass opacity
(0.924) (0.922) (0.916)
17 Treatment reintubation multi-centric Hydroxychloroquine
(0.943) (0.943) (0.937)
18 Air,Mask,Breathing laryngoscope facepiece supraglottic
(0.978) (0.978) (0.975)
19 PreventionofCOVID-19 alcohol-based decontamination rub
(0.944) (0.941) (0.919)
20 ImmunologyandVirus MHC∗ multi-epitope immunodominant
(0.976) (0.975) (0.973)
have their own distinct words. First, the topics ‘Statistical Modeling’, ‘COVID-19 Test’, and
‘Preventing COVID-19’ (Topics 13, 14, and 19, respectively) were located in the center of the
plot (the former group). This indicates that no matter how many words were used to estimate
thetopics’latentpositions,thosetopicsremainedasgeneraltopicsandsharedmanywordswith
other topics. This makes sense given the fact that the outbreak of COVID-19 and the testing of
COVID-19 were mentioned in a large proportion of literature, and statistical models were key
tools to study the COVID-19 pandemic. For example, among the 137K publications mention-
ing “COVID-19” in the PubMed database, more than 76,000, 64,782 ,and 44,602 publications
also mentioned “outbreak”, “testing”, and “prevention” respectively. Second, the topics ‘So-
cialImpact(financialandeconomical)’,‘SocialImpact’,‘Cardiovascular’,and‘CytokineStorm’
(Topics 12, 11, 10, and 6, respectively) were located away from the center in the plot (the latter
group), which implies their dependency structures with other topics. These topics usually stay
on the boundary of the plot regardless of the number of words because they mainly consist of
unique words. Finally, the topics like ‘Psychological Mental Issues’, ‘Relevance of Treatment’,
and ‘Compound and Drug’ (Topics 15, 4, and 2, respectively) start from the origin, move away
10fromtheoriginforawhile,andthenreturntotheorigin.Thisimpliesthatitcouldnotmaintain
the nature of the topic when fewer words were considered, and it is likely that those topics are
either ongoing research or burgeoning topics that have not been studied enough yet.
Next, we interpreted the topics’ meaning based on their latent positions, specifically using
subsets of topics divided by directions. Since we implemented the oblique rotation that maxi-
mizeseachaxis’distinctmeaning,wecanrendermeaningtoadirection.Figure2indicatesthat
there are three topic clusters. First, the center cluster denoted as (A) in Figure 2 are about the
outbreakofCOVID-19anditsimpactsondifferentareas,includingtheoutbreakofCOVID-19,
diagnosis or testing of COVID-19 using statistical models, the impacts on general social, finan-
cial, or economic problems, and the mental pain of facing the pandemic shock of COVID-19.
Second, the topics located at the bottom of the plot (cluster (B) in Figure 2) are related to
the symptoms of COVID-19 and their treatment. For instance, there are studies of COVID-19
related to their symptoms, such as cardiovascular diseases and lung scan images. On the other
side, there are topics related to treatment and risk prediction markers for COVID-19. These
subjects pertain to ‘Literature Review (8)’, ‘Lung Scan Imaging (1)’, ‘Symptoms Comorbidity
(9),’ ‘Lung Scan (16)’, ‘Treatment (17)’, ‘Relevance of Treatment (4)’, ‘COVID-19 Risk Predic-
tion Markers (7)’, and ‘Cardiovascular (10)’. Finally, the cluster (C) is related to what happens
inside our body in response to COVID-19, e.g., cytokine storm, immune system response to the
COVID-19 infection, compounds of drugs, and mechanisms of SARS-CoV-2.
In summary, we identified three main groups of the COVID-19 literature; the outbreak of
COVID-19 and its effects on the society, the studies of symptoms and treatment of COVID-19,
and the impacts of COVID-19 on our body, including molecular changes caused by COVID-19
infection.Wecanderiveanotherinsightfromthelocationsofclusters.Specifically,fromCluster
(A) to Cluster (C), we can observe a counter-clockwise transition from macro perspectives to
micro perspectives. Specifically, this flow starts with the center Cluster (A) related to the oc-
currenceofCOVID-19andthesocialimpactofCOVID-19,followedbystudiesofthesymptoms
andtreatmentofCOVID-19(Cluster(B))andthenendswithCluster(C),whicharerelatedto
micro-levelevents,e.g.,howSARS-CoV-2bindingoccursandhowtheimmunesystemresponds
to upon the COVID-19 infection.
4. Conclusion
We utilize the latent space item response model to estimate the topic structure in this
manuscript. First, byestimating thelatentpositions oftopics, we areable tovisualize thetopic
structure in Euclidean space. Specifically, we apply this approach to the COVID-19 biomedical
literature, which helps improve our understanding of COVID-19 knowledge in the biomedical
literaturebyevaluatingthenetworksoftopicsviatheirlatentpositionsderivedfromtopicshar-
ing patterns with words. Additionally, visualization of the topic structure provides an intuitive
depiction of the relationships among topics. This embedded derivation of topic relationships
will reduce the burden on data analysts because it does not require prior knowledge about
relationships between topics, e.g., a connectivity matrix or distance matrix.
Second, we introduce a score, denoted by s , that aids in interpreting the topics without
i,j
requiring expert knowledge. Without such scoring scheme, topics can be misinterpreted due
to the inter-correlated structure of some topics, which can make estimated probabilities from
topic models inaccurately reflect the exclusivity of words associated with each topic. Moreover,
withoutsuchscoringscheme,identifyingmeaningfulwordsforeachtopicandcharacterizingthe
topic using those words often requires significant expert knowledge. Our approach overcomes
these challenges by providing a means to determine the exclusiveness of word j with topic i
without relying on subjective interpretation.
Third, we interpret the topic structure by tracing their latent positions as a function of
11differentlevelsofwordrichness.Thisfeaturehastwoimportantproperties:(1)itcouldmeasure
themainlocationofthetopic,whichissteadilypositionedinasimilarplaceinspiteofdiffering
network structures; and (2) we could distinguish popular topics mentioned across articles from
recentlyemergingtopicsbyscanningthelatentpositionofeachtopic.Specifically,ifaparticular
topic shares most of the words with other topics, it is more likely to be located in the center. In
contrast,ifaspecifictopicconsistsmostlyofwordsuniquetothattopic(e.g.,araretopicoran
independent topic containing its own referring words), it is more likely to be located away from
thecenter.Forexample,inthecontextofCOVID-19,itismorelikelythatcommonsubjectslike
‘outbreak’ and ‘diagnosis’ are located in the center, while more specific subjects like ‘Cytokine
Storm’ are located more outside.
Theproposedframeworkcanstillbefurtherimprovedinseveralways.Althoughthedistance
between each topic and relevant words is taken into account in our model when estimating
topics’ latent positions, simultaneous representation and visualization of words are still not
embedded in the current framework. We believe that adopting a variable selection procedure
to determine key words can potentially address this issue and this will be an interesting future
research avenue.
Acknowledgements
This work was supported by the National Institutes of Health [grant numbers R21-HG012482,
R21-CA209848, U01-DA045300, R01-GM122078, U54-AG075931 awarded to DC], Yonsei Uni-
versity Research Fund [grant number 2019-22-0210 awarded to IHJ] and the National Research
Foundation of Korea [grant number NRF 2020R1A2C1A01009881; Basic Science Research Pro-
gram awarded to IHJ]. The funders had no role in study design, data collection and analysis,
decision to publish, or preparation of the manuscript.
Disclosure statement
No potential conflict of interest was reported by the authors.
References
[1] E. M. Airoldi and J. M. Bischof. Improving and evaluating topic models and other models of text.
Journal of the American Statistical Association, 111(516):1381–1403, 2016.
[2] C. A. Bernaards and R. I. Jennrich. Gradient projection algorithms and software for arbitrary
rotation criteria in factor analysis. Educational and Psychological Measurement, 65(5):676–696,
2005.
[3] D. M. Blei and J. D. Lafferty. A correlated topic model of science. The annals of applied statistics,
1(1):17–35, 2007.
[4] I. Borg and P. J. Groenen. Modern multidimensional scaling: Theory and applications. Springer
Science & Business Media, 2005.
[5] S. E. Embretson and S. P. Reise. Item response theory. Psychology Press, 2013.
[6] P.D.Hoff,A.E.Raftery,andM.S.Handcock. Latentspaceapproachestosocialnetworkanalysis.
Journal of the american Statistical association, 97(460):1090–1098, 2002.
[7] R. I. Jennrich. A simple general method for oblique rotation. Psychometrika, 67(1):7–19, 2002.
[8] M.Jeon,I.Jin,M.Schweinberger,andS.Baugh.Mappingunobserveditem–respondentinteractions:
A latent space item response model with interaction map. Psychometrika, 2021. .
[9] Q.Mei,D.Cai,D.Zhang,andC.Zhai. Topicmodelingwithnetworkregularization. InProceedings
of the 17th international conference on World Wide Web, pages 101–110, 2008.
12[10] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in
vector space. arXiv preprint arXiv:1301.3781, 2013.
[11] T.Rusch,P.Hofmarcher,R.Hatzinger,andK.Hornik. Modeltreeswithtopicmodelpreprocessing:
An approach for data journalism illustrated with the wikileaks afghanistan war logs. The Annals
of Applied Statistics, 7(2):613–639, 2013.
[12] S. Shortreed, M. S. Handcock, and P. Hoff. Positional estimation within a latent space model for
networks. Methodology, 2(1):24–33, 2006.
[13] C. Sievert and K. Shirley. Ldavis: A method for visualizing and interpreting topics. In Proceedings
of the workshop on interactive language learning, visualization, and interfaces, pages 63–70, 2014.
[14] X.Yan,J.Guo,Y.Lan,andX.Cheng. Abitermtopicmodelforshorttexts. InProceedings of the
22nd international conference on World Wide Web, pages 1445–1456, 2013.
13Network-based Topic Structure Visualization
Supplemental Materials
Yeseul Jeon, Jina Park, Ick Hoon Jin, and Dongjun Chung
Appendix A. Training word2vec Network
To train the words’ network, ? ] suggested the negative-sampling approach, which fits
the network of words by training the near words and unrelated words, and this ap-
proachwasreportedtobeefficientinvectorizingthewords’relationships.?]expanded
the negative-sampling strategy to model the words and the contexts through the joint
modeling of words and contexts, which makes the problem non-convex.
Before implementing the negative sampling, we need to assign the window size that
defines how many neighbors of words to consider. For example, let’s assume that we
want to select four neighboring words of ‘princess’. According to the word embedding
network, there are only a few words located close in the latent Euclidean space, e.g.,
‘horse’, ‘money’, ‘king’, ‘queen’, ‘princess’, ‘prince’, ‘palace’, ‘flowers’, and ‘...’. Since
we want to select four neighboring words, we set the window size to 2 so that two
wordsfromeachsidecentering‘princess’canbeconsidered.Therefore,wedefine‘king’,
‘queen’,‘prince’,and‘palace’asnearwordsfortheword‘princess’.Ontheotherhand,
we can sample 20 negative words that are not included in these near word sets. By
repeating this process, it trains the words’ network that reflects the context. In this
way, we could obtain the word embedding network with 256 dimensions.
Appendix B. Filtered Keywords
Table S1.: Filtered keywords
p.001 find z9.738 n102 p0.0001 p.05 measure
p0.03 n1427 sd coronaviru case number studi
date result hope refer major b n
besid ie l. fact e.g. h p
half virus viru disease coronavirus data rate
factor method test model analysis health death
1
4202
naJ
13
]PA.tats[
1v55871.1042:viXraAppendix C. Estimating Latent Topics using Biterm Topic Model
The abstract is an excellent source for understanding the overall text. Since most
abstracts are limited to 200 words, they could be regarded as short texts. Therefore,
weuseBitermTopicModel(BTM)forliteraturemining.ToimplementBTM,wefirst
need a word corpus, and, then, we extract biterms from each paper, which is input
for the BTM. We extract information from the text using morphology analysis, one
type of natural language processing technique. Specifically, it splits each word with a
suffix to identify the base element unit of a term. Among the basic units of words, we
first extract nouns in their most basic forms. This set of nouns is called a corpus. We
further expand the corpus by adding relevant words using word2vec, which vectorizes
distances among words in the Euclidean latent space according to their similarity in
meaning.Weextractneighboringwordsbasedonthesedistancesandenlargetheword
sets by collecting these words. Following this, it is necessary to understand the overall
semantic structure of a given text. It summarizes the latent structures, called topics,
by identifying topicsand estimating theircorresponding clusters ofwords. That is, we
can estimate topics and their distributions of words using topic modeling.
TheBTMisbasedonthefollowingassumptionsdescribinghowbi-termsandtopics
jointly related to each other:
Two words are assumed to belong to some hidden clustered set of words.
•
It is assumed that there are hidden topics, each of which represents similar
•
meaning of words.
It is assumed that those hidden clustered sets of words are hidden topics.
•
The likelihood of BTM consists of both topic-word distribution and topic distribu-
tion. Therefore, we need two sets of parameters to estimate the topic distribution θ
and the topic-word distribution ϕ . The whole likelihood of BTM is constructed as
z
follows.First,thepriordistributionforwords(ϕ )issettoDirichletdistributionwith
z
hyper-parameter β while the prior distribution for topics is set to Dirichlet distribu-
tion with hyper-parameter α. Next, we represent the topic with the latent variable z,
which follows Multinomial distribution with parameter θ. Likewise, each word follows
Multinomial distribution with parameter ϕ so that each word can be generated from
z
a specific topic. Therefore, there are three parameters to estimate, including ϕ , θ,
z
and z. Using the collapsed Gibbs sampling [? ], we can construct distribution of ϕ
wz
and θ with estimated statistics n and n , given as follows: |
z wz z
|
n n+α
wz
ϕ w |z = wn
w
|z| +Mβ, and θ z = |B |+Kα (A)
P
TheBTMisbasedonthefollowingassumptionsdescribinghowbi-termsandtopics
jointly related to each other:
Two words are assumed to belong to some hidden clustered set of words.
•
It is assumed that there are hidden topics, each of which represents similar
•
meaning of words.
It is assumed that those hidden clustered sets of words are hidden topics.
•
Based on these assumptions, the likelihood of BTM consists of both topic-word distri-
bution and topic distribution. Therefore, we need two sets of parameters to estimate
the topic distribution θ and the topic-word distribution ϕ . The whole likelihood of
z
BTM is constructed as follows. First, the prior distribution for words (ϕ ) is set to
z
2Dirichletdistributionwithhyper-parameterβ whilethepriordistributionfortopicsis
settoDirichletdistributionwithhyper-parameterα.Next,werepresentthetopicwith
the latent variable z, which follows Multinomial distribution with parameter θ. Like-
wise,eachwordfollowsMultinomialdistributionwithparameterϕ sothateachword
z
can be generated from a specific topic. Therefore, there are three parameters to esti-
mate, including ϕ , θ, and z. The likelihood construction process can be summarized
z
as follows:
(1) Draw a whole topic distribution from θ Dirichlet(α).
∼
(2) For each biterm b from biterm set B, calculate to which among latent topics z
those two words belong: z Multinomial(θ)
∼
(3) Draw a topic-word distribution of topic z from ϕ Dirichlet(β).
z
∼
(4) Drawprobabilitiesfortwowordsfromthetopic-worddistributioncorresponding
to the selected topic: w ,w Multinomial(ϕ )
i j z
∼
After the above steps, the joint likelihood of all biterm set B is calculated as:
p(B)= θ ϕ ϕ (B)
z iz jz
| |
i,j z
YX
The conditional posterior for a latent topic z, p(z z ,B,α,β), is given as:
b
| −
(n +β)(n +β)
w z w z
p(z |z −b,B,α,β) ∝(n
z
+α)
(
i|
wn
wz
+Mj|
β)2
(C)
|
P
where n is the number of times that the biterm b is assigned to topic z, z refers to
z b
−
the topic without biterm b, and n is the number of times of the word w assigned
wz
to the topic z. Because a direct appl|ication of the Gibbs sampling for p(z z ,B,α,β)
b
| −
sometimes can result in lack of convergence due to dependency between variables, we
use the collapsed Gibbs sampling to constrain unnecessary parameters by integrating
out [? ]. In particular, because the prior distributions are Dirichlet distributions, ϕ
z
and θ can be integrated out. After some iterations, we can construct distribution of
ϕ and θ with estimated statistics n and n , given as follows:
wz z wz z
| |
n n+α
wz
ϕ w |z = wn
w
|z| +Mβ, and θ z = |B |+Kα (D)
P
Algorithm 1 Gibbs sampler for BTM
Input: the number of topics; K, hyper-parameters; α ,β, biterm set; B
Output: distribution ; ϕ and θ
wz z
|
1: initialize topic assignments randomly for all the biterms
2: for iteration=1,2,...,N do
3: for b=1,2,...,B do
4: sample z b from p(z z b,B,α,β)
| −
5: update statistics n wz and n z
6: compute the parame|ter ϕ wz and θ z
7: end for |
8: end for
3Appendix D. Distance Plot of A
k
57%
49%
47%
50% 54%55%
48%
51% 53% 56%
3 59%
52%
58%
60%
2
43%
40%
42%
45%
41%
44%
46%
1
0 5 10 15 20
Proportion of selected words
Figure S1.: Distance plot of A
k
4
ecnatsid
egarevAAppendix E. Score and Affinity Plots of Topics
Topic1 Topic2 1.0
to l lpw 0 1ords 000000 ...... 677889 505050 lllll l ll ll lll l llll ll lllll ll llll lll ll llllll ll lll ll ll l lllll lll llll llll llll l lll lllll ll l llll lll l llll lllll ll l llllllll lll lll lll lllll l llll llll ll lllll l llllll ll l llll lllll lllllll ll ll l ll l lll llll llll ll llll llll lll ll l ll l llll lll llll ll l ll ll ll llll lll lllllll llll l lll lllll ll lll ll lllllll lllllll l lll llllllll ll llll lll lll lllll ll lllll llll ll ll ll llll lll lll ll llll lll ll lllll l lllll ll lllll lllllllll lllll l ll llllllllll lll ll ll lll lllll llll ll ll lll lll ll l lll lll ll lll lll ll ll ll l lll lll lll l ll ll l lll lll l l lllllllll lll llll llll lll llll lll ll lllllll lllll l ll ll l lll l lllllll llll ll ll llll llll ll llll ll llll lll lllll ll llll l lll llll ll l lll lll lllll ll lll ll lllllll l ll lllll l l lll llllllll lllll llll ll ll lllllll llll llll l llll llll ll ll l lll llllll llll ll lllll lll lllll lll llllll llllll lll ll ll l lll l lll lll llllll ll lll lll lllll l llll llllllllll l lll llllll ll l ll lll ll ll ll l llll llll l ll llll ll lll l ll lll ll lll l lllll lll lllll llll llll llll lll ll llll l ll l ll ll ll lll ll ll ll ll ll ll ll l llll l llll ll ll l ll lll ll l ll ll llll lll llllll llllll llll llll llllll lll llll lll l lll l ll llll ll ll llll ll l ll ll lllll lllll ll lllll llll lll ll llll llllllll ll ll lll llllll llllllll l lll llll ll ll l ll l ll lll lll llll ll ll l llllllll ll lllll llllll ll ll lll l lllll ll ll llll ll l ll l ll ll l ll ll lllll ll lllll lll lll l lllllll lllll lll lll lll lllll lll lll ll lll l l llll l ll ll llllllll ll l lll llll ll llll llll llll ll lll lll ll ll ll l lllll ll lllll ll l lllll ll l ll lll lllll ll lll ll l llll l lll lll ll l llllll lll ll ll ll llllll llll ll l ll ll lllllll llllllll lllllllll lll lll lll ll l l ll lll lllll ll ll lll l lll llll ll lllll lll lll l ll l l ll ll l lll ll lll l ll llll l ll l ll lll lllll ll llllll ll l ll l lll l ll ll l llllllll l ll ll lll lll llll ll ll lll ll l lll llll l lll l ll ll ll llll l lll ll llll lll llllll lll lll ll l ll lll lllll lll lll lll lll l lll l to l lpw 0 1ords 000 ... 789 ll lll lll lll lllll ll llll l llll ll lll ll lll lllll llll llll l ll l lll ll ll ll lllll ll lll l ll ll l l lllll llll lll ll ll ll ll ll lll lll ll ll ll l ll lll lllll l lll l lll l l llll lll ll l llll ll ll l lll ll ll lllllll l l lll ll lllll l lllll l ll ll lll lllllll lll ll l ll lll l llll lll ll ll ll ll llll lll llllll ll ll llll ll llll lll ll lll llll ll lll ll lllllll ll ll ll lll ll llll lll ll llll lllllll llll ll l llll l l l ll ll lll lllll ll l l ll ll l llll lll ll l ll l l llll l lll llll llllll llll ll ll lll ll ll l lll l l ll ll lllll lll lll ll ll ll lll l lll ll lll lll ll l lll llll lll lll lllll ll ll lll ll llll lll ll ll llll lll ll llll ll lll ll ll ll ll ll ll lllll l ll ll ll lllll l lllll lllll llll ll l l l ll lllll l lll llll ll ll llll ll l llll lll lll llll llll ll lll ll lll l ll lll l ll ll llll l lll l lll ll ll ll ll llll ll llllll l ll llll ll lll ll l ll ll ll ll ll ll llll lll l l l lll lll l ll llll l ll l l ll ll lll lll l ll lll lll ll ll l lll ll ll l lll lll llll l lll l llll llll ll l lll l lll llll ll l ll llll ll l ll l ll ll ll lllll l l ll l ll lll lll l llll ll ll l ll l l ll llll lll lll l lll lll lll lll ll ll llll lll lll l lll ll lll ll llll ll ll ll llll llll lll l lll ll ll ll lll l l lllll lll l ll ll l llll ll l llll ll lll ll lll lll ll ll l ll ll ll lll ll l ll lllll l lllll ll ll l ll ll lll l l llll lll l ll ll ll lll l ll lll llll ll ll l lll ll llll lll lllll lllllll ll ll llll ll ll ll lll lll ll l lll ll llll lll l ll ll l ll l ll llll ll lll ll llll l lll ll lll l lll lll lll ll ll lll ll l ll ll l ll lll lll lll ll ll lll ll ll ll ll lll llll lllll llllll lll llll llll l lll l lll ll lllll ll lll lll lllll l llllll lllll ll lll ll lllllll ll llll ll lll llllllll ll l ll l ll lllll l lll ll ll ll lll ll lll llll llll l ll ll ll ll ll lll ll l ll l lll ll ll l lll lll ll lllll ll l ll l lllll lll lllll l lll ll ll llll llll lll ll llll lllll lll ll l llll ll llllllllllllllll llllllll ll lll lll lllll ll lll l ll ll l l ll lll ll ll llll ll llllllll lll lll l llll ll l llll llll lll ll l llllllllllllll llllllll lllllllllllll lll lllll lll l ll lll ll llllll llll
0.00 0.03 0.06 0.09 0.25 0.50 0.75
Affinity Affinity
Topic3 Topic4
to l lpw 0 1ords 000 ... 789 ll lll lll l ll lll lll lll lllll ll lll ll lllllll lll ll l lll lllll l ll ll l lllll llll l ll l l llll ll ll llll l lll ll lll lll ll lll ll lll lllll ll lllll l llll llllllllll l lll lll ll llll lllllll llll ll ll lll llllllll l lllllll lll llllllll llll ll lllllllll lll ll ll lllll lll ll lllll ll ll llll ll ll lll ll lllll llllll ll ll l l lllll ll llll ll lll ll ll lll llllllll ll l llll lllll llll l ll ll l lll l ll llllll l llll lllll llllll llll lll ll ll ll lll ll llllll ll lllll ll l llll l lllll ll lll llllll lll ll lllll llllll lll ll l llll lllll ll lll lll lll lll ll ll ll llllll l llllll ll ll l ll lllll lll ll lllll lllllll lll ll lll l llll llllll lll ll ll ll lllll lll ll lll lllll l lllll lllllll ll llll llll lll ll ll llllll l lll lll ll llll l l lll lll llll ll l ll ll lllll l lll lll ll llll l lll ll ll l ll ll llll l ll lll llll ll ll lll ll ll lll lll ll ll l llllll lll lll llllll lll lll llllll llll lll lll ll ll lllllll lll lll ll llll lllll llllll llll l l llll ll lll llll ll lll llll lllll ll ll lllll lll lllllllll l lllll lllll l llllll ll llll lllll llll lll ll lll ll ll ll llll l lll l llllll ll lll ll ll l ll ll l llll ll lll lll llllllllllllllllll llllllll lllllll lll lll llll llllll ll l lll ll llll l llllll ll llllllll lll lll lll lll l llll ll lll ll lllllll lll ll llllll ll ll l llll lll l ll lll llllll lll ll ll ll l ll lll lllll l llll lll llll ll ll l lllllllllllllllllllllllllll lll llllll llllllllllll l llllll llll llll lllllllll llllll l lll llllll ll lllll llll llll lll llll llllllll lll ll ll lllll ll lll llllllllll l lll ll l llllllll l lll llllllllll lll lllllll lllll llllll ll ll llllll lllllllllll lll lll lllllll llllll lll ll lllll lll llll lll llll ll lll ll ll l ll l ll l ll l lll lllllll l llll lll ll lll ll ll lll lll l lllll lll ll ll l lll l llll llll lll lll l l ll ll lllllll llllll lllll l lllll llll l ll ll ll llll ll llll l lll lllll llll lll lll ll lll l lll l llll ll ll l llllll ll llll lll lllll lllllllllllllll llll lll llllllllll lllll lllll ll l l l to l lpw 0 1ords 0000 .... 6789 lllllllllllllllllllllllllllllllll lll lllllll llll lllllllllllllllllllllllllllllllllllllllllllllllllll lllll llllllllllllllllllllllllllllllllllllllllllllll llll llll llllll l llllllllllllllllllllllllllllllll lllll llllll llllll lllllllllll lllllll ll llll lllllll ll llllllllllll llll lllllll llllll ll lllll ll llll llllll llllllll lllll llll llll lllllllll ll lllllllll llllllllll lllllllllll lllll llll llllll lllllllll llllllllllllll llllllllllllllllllllllllllllllllllll lllllllllllllll ll lllllll lllllllllllllllllllll lll lllll l llll lllllllllllllllllllllllllll l lll lllllll lllll ll lll l lllllll llllll llllllllllllll llllllllllllllllll llll llll llllllll ll lllll llll lllllllllll llllll lllllll llll lllllll llll lllll lllll ll llllll lllllll l lll ll l llll lllllll lllllllllllll l lllll llllllllllllllllllllllllllllllllllllll llllll lllllllllllll lll lll ll ll llll lll llll lll l lllll ll llllll lll l ll llll lll ll llll lllllll llllllllll lllllllll llllllllll llllll lllllllll ll lllllllllllllll lll ll llll ll lllll lllll ll llll lll lllll lll lllll lll lllllll lllll l ll lllll ll lll lllllll ll lll llll llll llllll ll llll ll ll ll lllll llll lllllll llll llll llll ll llllll l llll lll llllllllllllll lll llllll ll lllll lllll lllll ll lll llllll llll ll llllllllll lll ll l ll lllllllll l ll llll lllllllll lll lll llll llllllllll lll lllllllllllllllllllllll ll lll lll lll llllllll lllll llllllllll lll ll llll lll lll ll l lll lllll ll ll lll lll lllll lllll llllll ll ll lllllllll lll lllllll llll lllll llllllll lll llll lll lll lll llll ll l llll ll llll ll llllllll lll lll llll lllll ll lll ll lllll lll llll llll l llll l lllll llllll ll ll lll llllll lll lllll l lll l ll ll ll l ll l lll llll lll lll l llll l lll ll l lll l llll llll l l ll ll lll lll llllllll l ll l lllll llllll l l llllll ll lll ll l l ll l l llll ll lll lll lll lll ll l lll l l
0.00 0.02 0.04 0.06 0.00 0.01 0.02 0.03
Affinity Affinity
Figure S2.: Score and affinity plots of Topics 1 - 4.
5
erocS
erocS
erocS
erocSTopic5 0.9 Topic6
to l lpw 0 1ords 000 ... 678 llllllllllllll lll llllll llll llll llllll llllllllllllllllllllllllllll llllllllllll lllll lllllllllllllllll ll lllllllll lll llllllllllll lll lllll ll llllllllll llll l llllll llllllllll lllll llllll ll llll llllll lllll llll ll ll ll ll llllll ll ll l l llll lll ll ll lllllllllllll llllllll lllllllllllllllllllllllllllllllll ll llll lllllllllllll llllllllllllllllllllllllllllllll lllllllllll llllll lllllllllllllllllllllllllllllllllllllllllll lllllllll llllllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllll lllllllllllllllllllllllllllllllllll lllll ll lllllll ll llllllllllll ll llllll llllllllllllllll llllll l llllll llll lll ll llllll lllllllll lll lllll lllll lll lll llll ll llllllllll llllll ll llll llll lll llll lllllll llllllll lllllll llllllllllllllllllllll lll lllll lllll llllllllllllllllll ll lllllll llllllllll lllllll lll l ll llllllll llll lllll llll l lll lll ll lll llll lllll llllllll llll lllllll llllllllllllllllllllllllllllll ll lllll lllllll llll lll l llllll lll lllllllllll ll lll ll llll ll llllll llll lllllll lllllll lllll lll lllllll ll lllll lll ll llllll lll lllll llllllll llll llllll ll l llllll llllllllllllll lllllllllllllllllll lll lll llllll llllllll lllll lllll lll ll llll lllll ll llll ll lllllll lllll lllll l lll llllll lllllllll lll ll l ll llll l lllllllllll llll ll l ll llllll llll lllllll lll l l lllll lll ll llll lll lll ll ll ll llllll ll lll ll llll ll l lllllllll l llllll ll l ll l lllllllll l llll l lll lll llll ll llll l ll lll ll lll lll ll ll lllllll llll llll l ll llll ll lllll ll llll llll ll l ll l ll ll llllll lll lll l ll lll lll l ll ll lll llllll l llllll lll lll l lll l lllllll llll ll lll llll lll lll l ll lll lll lll ll l lll lllll lllll ll ll ll ll ll llll ll lll lllll lllll lllll ll ll llll l ll ll llllll l l ll l ll ll llllllll lll lll l ll lll llllll ll lll l ll l lll ll ll ll lll lll l l ll ll ll l l to l lpw 0 1ords 0000 .... 6789 lllllllllllllllllll ll lllllllllllllllll llllllllllllllllllllll ll llllllllllllllllllllllllllllllllll llllllllllllllllllllllllllll lllllllllll llll llllllllllll lll llllllllll l lllll lll lllllllllllllll ll lllllllllllllll llll llllllllll llllllllllllllll lllll lll llll ll l lllllllllllll lll lll ll llllll lllllll llll ll ll lll llllll llllllllll llllllllllll ll lll lllllll lllll lllll ll lll lll llllllll ll llllll lll l lllll llll lll lllllll ll llll lllllll lll lll lll lllll ll ll lllllll lll lll llll llllllllllll ll ll lll lll lll lllll ll lll lllllll lll llll lllllll ll llll lll ll l lllll llllll llll lllll lll llll llllllllllllll lll llll llllllllllllll llll ll l lllllll ll ll l ll lll lllll lllll l lllllllllll l llll llllll lll ll l llllll lll l l lll lll llllllll lll lll llllllllllllllllll llllllll ll llll llll llll llllllll lll lll ll ll llllllll lll llll ll llll lll lll ll ll llllll lllll ll lll ll l lll llll llllll lllll l llll lll llllll ll ll llll lll llllllll lllll ll ll l llllll lllllll l ll lll ll lllll lll lll lll llll lllllllll llll llllll lllllll lllllll llllllll ll lll llllll lll ll lllll ll llll ll l ll llll l lllllllll llll lllllll lll lll lllll ll lllllllll lll ll lllll ll lll llll lll lll llll ll lll llll llll lllllll lll llllllll lllll lllll ll lllll llll lll lll ll ll lllll llll lllllllllll l lll llll ll ll ll lllll llll ll ll l ll ll llllllll lllll lll llllllllllllllllllllllllll ll llllllllllllllllll l ll lllll llll lllllllllll llllllllll llllll llllllllllll lllllllllllllllllllll ll lllllll lllll lll ll ll lllllll ll lllll llll lll lllll ll lll lll llll l ll ll l lll ll llll ll ll ll lll l ll ll llll ll llll l ll lll l lllllllllllll ll l llll ll l ll ll lllll ll ll l lll ll llll lllllll lllllll ll ll lll llll llllll lll llllll ll ll ll l lll ll lllll lll ll l lll l llllll ll ll ll ll l ll l lllllll lll lll ll llllll llll l l lllll ll llll l l ll lll l llll llll lll l lll lll l
0.00 0.01 0.02 0.03 0.04 0.00 0.01 0.02
Affinity Affinity
Topic7 Topic8
to l lpw 0 1ords 00 .. 89 lll
l
ll ll lll llll ll ll lll ll lll ll llll ll llll lllll lll lll ll lllll lll lll lll ll lllll l l lll l ll ll lll l lll ll lll llll lll lll ll lll llll ll l ll lll ll lllll l lllllll ll llll ll llll llll llll ll lll l ll ll ll llll llll l lllll lll llll ll ll ll llll lll lllll llllllllllllllll lll ll l lllllllll ll lll llllllll ll l ll lll llll l l lll llll llllll ll l ll ll llll l lllll lll l lll lll lll ll l llll llllll l ll lll lll llll llll ll ll lll ll ll ll lll ll ll llllllll lll lllll l lll llll llllll l l ll ll l lll ll lll lllll lllll llll lll ll ll llll ll lll llll lll lllll lllll l lll llll l lll lllllll ll ll lll lll l ll lll llll lllllll ll lll l l llll lll ll lllll l ll ll l llll ll l lllll ll llll l ll llll lll lll ll lll ll llll l ll lllllll llllll llll lllll llllll llllll lll l ll l lll ll lllllll ll ll lll lll ll ll llll l llll lll llll l l l ll l ll lll ll lll lll l ll lll lllllll l llll ll lllll ll ll lll ll ll lllll llll ll ll lll lll llllllll llll llll llll ll llllll lll l llllllll llllllll ll lll ll ll lll lll ll l llll lll lll ll lll ll l ll l lll l lllll l l ll lll lll lll llll lllll ll llll ll ll lll l l ll lllll llll llll ll l lll ll llll ll l lllll lll l llllllllll lllll llllllll llllll ll lllllll l l lllllllll l ll lll lll ll l lll llll ll ll ll lll ll lll ll l llll llllll lll lll lllll llll ll lll llll l llllll lllllllll lll lll llllllll l l lll ll lll lll lll ll ll lllll ll llll lll l llll l lll ll ll lll ll ll l l lll l lllll lll lll ll ll llll ll l ll l llll ll lll ll l ll ll lll l llll ll l ll ll lll l ll ll ll lll ll l l ll lll ll ll ll llll lll ll ll ll ll ll ll ll ll lll lll l lll ll lll lll l ll lll ll llllll llllll lllllll ll ll ll ll ll lllll ll l llll llll l ll ll lll lll l lll llll ll l l llll ll ll ll ll ll ll lll lll lll l ll ll llll l ll ll lllll lllllll ll lll lll lll llllll ll llll l llllll ll ll llll llll lllllll ll l lll ll llll llll ll l lll ll llll llll l llll lll ll lll ll l ll ll lll l llllll lll l llll lll llll lllllll ll l lll lll ll ll lllllll l llllllll ll ll ll lll ll l l lll ll llll llll llll llll lll ll ll lllll lll to l lpw 0 1ords 00000 ..... 77889 05050
l
llllll ll lllll l llll lll l llllll lll ll lll ll ll ll lllll ll ll ll lll lll l ll lll lll ll llll ll l lll lllllll ll lll l llll ll ll lll ll ll llll lll l ll ll lll l lll ll ll llll lll l ll ll ll l ll lll ll ll l lll llll llll l ll ll lll lll lll l lll l llll ll ll ll ll l ll llll l llll ll llll ll ll ll lll lll lll l ll lll lll ll lllllll ll lll ll l lllll lll ll lllllll lll ll ll l llll ll lll lll lllll lll ll ll lll llll ll l lll ll ll llll ll l ll ll l llllll ll l lllll ll ll ll lll llll l lllllll llllll l ll lll llllll ll llll lll lllll lllll lll l llll ll lll l l ll ll ll llll l ll ll l ll ll l ll ll ll lll llllll ll lll ll ll l llll lllll lllll lll lll ll llll l ll lllll lll lll ll llllllll l lll ll l lll llll lll lll ll lll ll ll ll lllll ll lll ll lll ll ll ll l ll ll lllllllll lll ll lll lll ll lll l llll lll llll lll lll ll l ll ll lll llll l lll lll ll ll l lllll ll ll ll llll ll l ll llll ll ll lll l llll ll ll ll ll ll lll lllll l ll lllll l ll ll ll lllll l ll lll l ll lll l ll l ll lll ll ll ll l ll lllll lll llll l l ll ll ll lll ll lll l lll lllll lll ll lll l ll ll ll ll ll l ll lllll lll ll ll l lllll ll lll lllll lll l lllllll lll ll l llll ll l ll lll ll ll lll ll lll ll l l l ll ll ll llll ll l ll ll ll lll ll lll l lll l ll ll lll lllll llll l lllll lll ll llll l llll l ll l l llllll ll ll l lll l llll ll ll ll lllll lll ll lll ll lllllll llll lll lll ll lll ll ll lllll ll lll lllll lll ll ll lll llll l ll ll lll l lll lll lll ll lll llll llllll lll l lll ll l ll l lll lll llll lllll l ll llllll ll lll ll ll llll llll lll l l ll l ll ll l l llll lll ll lll l lll ll ll l ll ll ll ll l llll ll lll ll llll l lll ll lll lll ll l l ll ll l l ll ll l ll ll ll ll ll lll lll lll lll l ll ll l llll llll lllll l l lllll l lll lll l l lll lll ll ll lll ll ll llllllll llll llllll ll ll ll l llll lll l l ll lll llll lll llll lll l l ll ll ll ll lll ll l l lll llll lll ll lll llll ll lll lll lll lll lll ll l ll ll ll lll lll ll lll lll ll l lll lll lll ll lllll ll lll lll lll llll l lll l llll ll llll ll l llll ll lll lllll ll l l llll llll ll l ll l llll l ll ll ll ll llll lllll ll ll lll ll l llll lllll ll ll ll l l lll lll ll ll l
0.25 0.50 0.75 1.00 0.00 0.05 0.10 0.15 0.20
Affinity Affinity
Figure S3.: Score and affinity plots of Topics 5 - 8.
6
erocS
erocS
erocS
erocSTopic9 Topic10
l l
to l lpw 0 1ords 000000 ...... 677889 505050 lllllll l ll lllll ll lll lllll lll lllllll lll llll lll llll lllll l lll llll lllll ll ll llll llll lll lllll ll ll ll lll ll llll lll l llllll lllll l llll lll l llll llll llll llllll lll lllll l lll ll ll ll lll lllll l lll lll lll llll lllll ll llll l lll lll ll ll ll lllll ll lll lllll l lll lll l lll llllll ll llll l lll ll llll lllll l ll ll ll ll ll ll lllll ll lll llll l lll l ll ll ll llll l llll lll llll lll llll l llllll llll l ll lll l ll lll llllll lll ll llll lll lllll ll lll l llllllll lllll lll llllll ll llllll llll ll ll llll l ll llll ll llll lll ll l lll lll lllllll lll llll lll ll lll lll lllllllll llll l llll lll llllll ll l lllll l ll ll l ll ll ll llll lll l lll llll l lllll lll lll l ll lll l llll lll lll lllll lll ll ll ll ll l ll lll ll llll ll lll llll lllll ll l lllll lll ll llllll llll llll llllll lll l ll lllll llll lll ll ll lll lllll ll llll lllllllllll ll l ll lllll lll ll llll ll ll ll ll ll lll ll ll ll l lll l ll l ll ll llll ll lll lll ll lllll lll l l lllll lll lllll llll l lll ll ll ll ll ll ll llll llll lll lll ll lll ll lll lll lll ll ll ll ll ll llll llll lll lllllllll llllllllllll llll llllll l l ll ll llll ll llllll l ll ll lll lll lll ll ll llllll lll ll ll ll ll ll lll ll l lll lll ll llll ll ll llll lllll ll llllll ll ll lll l ll lll lll llll l llll l ll ll ll lll lll llll llll l l lll ll ll ll llll lll llllllll l lll lllll lll llll ll lll lll l l lllll llll lllll ll llll lllll ll lll llllll ll lll lll ll l lll lll lll ll l lll lll l lll ll lll ll l llll ll ll ll ll llll lll l ll llll ll lll lll ll ll llll ll l lllll llll l ll ll ll ll lll llllll ll llllllll lllllllll lll ll l llll l llll ll l lllllll llllll lllll l ll ll lllllllll ll ll ll lll l ll ll lllll llll llll l ll llll lll llllll lll lllllll ll llll llll lll l ll lll lll llllll lll ll l ll lll lll ll lll ll lll lll ll ll llll ll l llll ll ll lll l ll lllllll lll ll llllll lllllll lll ll ll lll ll lllllll llllll llll ll lll ll ll ll l ll l llll lll lll lllll l llll ll ll l ll llll lll ll lll ll llllll lllll l lll ll llll ll l lll ll lllllll ll l ll l l l lll l to l lpw 0 1ords 000 ... 789 llllll llll lll llllll llll lllllll llll lllllllllllllllllll lllllll lllllllll lllllllll lll l lll llllll ll lllllllllllll lllllllllllllll lllllll l ll lllll lll lllll lll l ll llll lllll lllll lllllll lllll lllll lll llll ll lll lll llllllllll lllllll llllllllll lll lllll llllll llll ll lll lll lll ll lll lll ll lllll l lllll lllll l ll ll lll lll lll lll ll lll ll llllll ll llll lll lll ll ll lllll l lllll lllll ll ll l llllll lll ll llll lll l lll lllllllllllllll llllll lllllllllll lllllll lll lllllllll l lll llllllllllll lllll llll lll llll ll llllllll lllllll ll llll llllllllll lllllll llll ll lllll ll llll lll llll lllllll lllll ll lll llllllll llll ll lll lll llll lllll lllllll ll lll llllllllllllll llll ll lll lll lll llllll llllll llllllll lllll l lllllllll lll lllll lll lllllllllll lllllllllllllllll llll llll llllllll lll lll lllllllllll lllllll ll ll ll lllllll llllllll llll llll llll llll lll llll ll llllll llllll lllllllll llllllllll llll ll lll llll lllllllllllllllllllllllllllllllll ll lll lllllllll llll l lllllll ll ll l ll llll lll lllllllll llllll llll l llllll lllll lll llllll llll llll lllllll llll llllllll lllll llll ll llll lll lllllll ll llllllllllll ll lll llllllll ll l ll lll lllll lll lllllllllllll llll llllllllllllll lllllll llllllllll llllllllllll llllllll lll lll llll ll lll lllll lll lll ll lllll ll lllll lll l lll lllllllll lllll l lll l llll llll lll lll l lll lllll lllll lll llll lll ll ll llll llllll lll ll ll lll llll ll ll llll ll lll lll ll ll l lll l lll l lll llll ll l lllll llllllll lll ll ll lll lllll lll lll lll lll llllll ll ll lll ll llll l llll ll l llllll ll lll llll llll l lll l ll ll llll lll l llll l ll lll l ll lllll llll llll lll llll lll ll lllll l ll lll lll lll l lllll ll l lll lll ll lll l ll l ll lll ll l ll ll l llll lll lll lll lll ll lll llll lll llllll l lll lll l ll ll ll llllll ll llll lllllll lll ll ll lllll l ll llllll ll l ll ll llll ll llll l lll llll lll l l l ll l
0.00 0.05 0.10 0.15 0.20 0.00 0.01 0.02 0.03
Affinity Affinity
Topic11 Topic12
to l lpw 0 1ords 0000 .... 6789 lllll lllll lllllllllllllll llllll l ll lllllllllllll lll llllll lllllllllllllllllllllllll lllll lllllllllllllllllllllll ll llllll llllllllllll llll lllllllllllllllllll lllllllllllllllllllllllll lllllllllllllllllllllllll lll ll lll llllllllllll lllllll lllllllllll ll l lll lll lll ll ll llllll l lllllll lllll llll lll lll lll lll ll lll lllllllll lllll llll ll lll lll lllllll lllll ll l llllll lll llllllllllll llllll lll lllll lllllll lll ll lllllllll llllllllllllllllllllllllllll lllllll llll lllllll ll llllllll ll lllllllllllllllll lll llll ll lllllll llllll ll lll ll lll llll lllllll llll lll llllllllllllll ll llllll ll ll lll lllll llll llllll lll llllllllll lllllllll ll l llllllll ll lll l lll ll llll ll ll ll ll lll lll lll ll lllll llllllll llll lllll ll llll llll l llllllll ll ll ll ll lll ll ll lll lllllll ll ll llllll lll lll llllll l lllllllllll lllll ll llllllll lllllll lllll lllll llll lll l lll lllll lll lllllllll lllllllllllllllll llllll l llll lllll llllllllll lllllllllllllllll llllllll lll lll llllllll ll lllllll llllllllll llll ll llllllllll llllllllllllll llllllll ll lllll lllllllllllllllllll llllllllll llll llll llll llllll llllllll ll lll llll ll llllllllll l l lll llll ll lll ll ll ll ll l llll ll lll lllllll lllllll ll lll l ll lllllll ll lllllllll lll ll lllll llll llllll l llll ll lllll llll llllll llll lllllll lll lll ll lllll lll l lllll lllll l llll ll ll lll lllll ll ll lll lll ll lllllll lllllll l llll llll llll lllllllllll llllllll lllll llll lllll lllllllllll ll llllll ll lll l llll l llll llllllll ll lllllllllllllllllllll llll ll lll ll llll lll llll llllll lllll ll ll lll lll llllll lll lll ll l ll ll lll ll ll l lll ll lll ll llll lll lllll ll ll lll l llll l lll l llll l lll lll ll ll ll ll l ll l ll lll ll ll ll ll l l lll lll ll ll ll l ll lllll lll ll ll ll ll llll ll ll ll lll l ll lll l ll ll lll ll l ll lll lll lll lll lll ll l lll lll l ll l ll llll ll l ll l l ll to l lpw 0 1ords 0000 .... 6789 l lllllllll lll lll lll lll ll lllllllll ll llll ll llllllllllll llll ll ll llllllllll llll llllll llllllllllllll ll llll lllll l llllllllllll ll ll ll lll llllll ll lllll lllllllllll lllllll ll lllll llll ll ll lllll l lllll lllll lll lll ll l ll llll ll ll ll lllllll lll lllllllllll ll lll llllll llll llll llll ll l lll lllll llllllllll lll lll ll llll ll lll lllll llll lllll ll llll ll llll ll llll l lll lll llllllll ll llll llll lll lll llll ll lll lll lllll lll lll ll l ll lll lll lll ll ll ll ll lll lll ll lllll lllllll ll lll llll llll llllllll llllllll lllll llllll ll llll ll llllll lllllll lllll lll llll lll llllll lllllllllllllll ll ll ll lll lll lll ll ll lllll llllll lll lllllll lllll llll ll ll lllll l lll llll l lllll ll llll llllll lll l llll llllllll llll llll lllll lll ll ll lllllllllllll l lllllllllll lll llll llll llllll lllllll lllll llllllll ll llllllll lll ll ll lll lllllll lll llllllllllll lll lllll ll lll llll lllll lllll ll llll llll l ll ll llll lllll ll l lll llll llllll lll lll ll lll lllll lll lllll ll ll l llll ll lll ll llll lllll ll lll lllll l llll ll lll lll lll llllll l ll l ll ll ll llll llll lll lllll lllllll lll lll llll lllll llll l llllllllllllllllll llllllllllllllll ll l lllllll ll ll ll lll lll lllll llll ll llll ll ll ll lll llll ll l lll lll ll l l l lllll ll l llll ll ll ll ll llllllll l llll ll ll ll l lllll lll lll lll lllll l llll l lll ll llll ll l lll lll ll ll llll lll l lll llll llll lll lll lll l ll l ll ll lllll lll lll lllllll ll ll llllllll llll lll llllllll ll l llll l llll llll ll llllll ll ll l lllll llll ll l llll lll ll lll lll lll llllllll lll ll lllll lll ll llll ll l lllllllll lll lll llll ll lll ll ll lllll lll ll llll llll l lll ll lllllllllllll lll lll llllll l l ll lll ll lll lll lll ll lll ll l lll ll lllll llll llll l ll ll ll lllll lll ll ll ll llll l ll llll l l l ll ll l l lll ll ll ll llll ll ll lll lll l ll llll ll lll l llll l llll lll lll lll l ll l l ll llll ll l llll ll l llll lllll lll ll lll l ll lll l l lll lll lll l
0.00 0.01 0.02 0.03 0.04 0.05 0.00 0.02 0.04 0.06
Affinity Affinity
Figure S4.: Score and affinity plots of Topics 9 - 12.
7
erocS
erocS
erocS
erocSTopic13 Topic14 1.0
to l lpw 0 1ords 000 ... 789
l
ll l ll lll ll ll lll lllllllll llllll ll ll l ll ll lll lllll lll ll lllll lllllll ll llllll lllllllll ll lllll llllll llllllll llllll lll lllllll ll llllllllll lll lllllllllllllllllll lllllll ll llllllllll ll ll llll lllll lll llllllllll lllllll lllllllll ll llllllll llllllllllll lll lllllllllll llllll lllllll lll ll l lll lll ll llll ll ll lllll lll lll l llll lllll ll lll l lll ll llllllll lll llll ll lll l lllllll lllll l lll ll lll l l lll l lllll lllllll llll llll lll l ll l lll llll lll ll l ll ll ll ll l lll ll l llll ll ll ll lll llll lll lll llll ll llll lll ll ll lll l llll llll lll l ll lll llll ll l llll lll ll l llll lll ll lllll llll ll lll llll ll llll llll l llll lll l ll llll l lll llll l lll ll ll llll ll lllllllll llllllll ll lllll lllll llll ll lllllllllllll lllllll lll lll llll lll lllllllllll lll lllllll llll lllll lll lllll ll l llll lllll llllll lllll llll l lllllllllllll ll ll ll llll ll llll ll lllll l lll lll lll lllll ll lll lllllll ll ll llll llllll ll lllll ll l ll ll ll llllll lll lll lll llll ll ll l lll l lll ll ll lll ll ll ll ll llll ll lllll l lll lll l ll lllllll lll lll ll ll lllllllllll lll ll lll lll lllll ll ll llll ll l lll ll llll ll lllllll llll l ll ll llllll lllllll ll lll llll lll llll ll lll ll l llllll l ll lllll lllll ll lllllll l lllll llll llll ll lllllll llll ll ll llll ll llll lll lll ll ll l ll l ll llll llllll ll lll lll llllll lllll ll lll ll lllll lll ll ll ll llll l ll l lllllllll llll l ll l llll lll llll llll
ll
l ll llll l lll lll ll ll ll ll ll ll l l ll ll ll ll lll ll lll llllll lllll lllllll llll lllllll lllll lll lll llll lll lll
lll
llll
lll
lll lll
llllll
ll lll ll ll llll
lll ll
lll ll
ll
l
llll lllll ll
l llll ll
ll
lll
lllllll
l lll lllllllllllllllllll
llllll llll
l ll
ll
ll ll
ll ll
ll
ll lll
l l
lll lll lll
lll
lll ll ll l llll
l
ll lll ll ll
l
llll ll
l
ll lllll ll lll lll lll ll lllll lll
l
llll
l
lll lll lllll lll ll
ll ll
lll
l
ll llllllll lll ll ll lll lll lllllllllll l llll ll llllll lllll lll llllllll
lll
llllllllllll
lll to l lpw 0 1ords 000 ... 789 llll llll ll l lllll lll ll l lll lll l llllll llll ll llll lll ll lll l l lll ll llll ll ll lll llll l llllll ll ll lllll ll lllll l llll ll ll lll ll llll lllll lll ll llll ll ll l l ll lll llll lll ll ll ll ll l l ll l lll ll ll ll lll l llllll l ll lll lll ll lll ll ll lll ll l llll l llll ll llllll ll llll l ll lll lll lll lll ll ll lllllll l lll l lll ll l l lll lll l llll l lllll lll llll ll lll lll l llll lll ll l ll l l ll ll lllll l lll lllll l ll ll lll l llll l lllll ll l lll lllll l ll ll ll ll l ll ll ll l lllll ll llll llll l lllll l ll lll lllll l l ll ll ll ll lll ll l llllll lll ll l llll llll ll lllll l llllll llllll llll ll l ll ll lll lll lll lll ll l ll ll l ll llll llll llll lll lll ll llll ll l ll lll lll ll ll l ll l ll l lllll ll ll lllll lllllllll lll ll ll llll lll llllll l ll ll lll lllllll l ll llll l l llll ll llll l ll l ll l lll ll l ll l l llllll lll ll lll ll ll ll llll llll l lll llll ll ll l lll l ll ll lll lllll lll lll ll l ll l lll llll lll ll lll l llll llll ll ll l lllll lll lllll ll lll ll l llllll llll llllll lll lll llll llll lll lll lll l llll llll l lll lll llll ll lll l llll ll ll ll lll ll lll ll l lllllll ll ll ll lll ll ll lll lll lll lllll lll llll lll llllll llll l llll lllll lll lll llllll ll llll lllll l lll llll lll llll lll lllll llll ll ll l l l lll ll lll lll lllll ll lllllll lll l llllll ll ll lll l ll lll ll ll llll ll lllllll lllll lll ll ll l llll l lllll llll ll lll lll ll l l llll l llllll ll lll llllllll ll ll l llll ll l l llll l lll ll ll llll l l llll ll ll l llll ll lll ll lllll ll l ll lll lll llll ll llll llllll lllll lllllll ll lllllll llllllll l llll l ll lll llll lll l lllll llll ll lll l ll ll ll l llllllll l ll l ll ll l lllll llllll ll lllll ll l ll l lll l lll l ll llll ll lll ll ll lllll l ll ll ll ll lllll ll ll l lll ll llll lll ll llll ll
ll
ll llll ll lll ll lll
lll
ll ll lll
lll llll
l l ll
ll
ll ll lll lllll ll
llllll
l l
ll
ll l ll lll
lllllll
ll ll
lllll llllllllll ll ll lll ll lll ll l llllllll l llllll lllllll ll
l
llll ll
l lllll
lllll llll ll ll ll
lll
ll llll
llll
lll lllll l l
l
ll
lll l
llll l lll ll l
lll
lll llll
0.25 0.50 0.75 1.00 0.25 0.50 0.75
Affinity Affinity
Topic15 Topic16
to l lpw 0 1ords 000 ... 789 ll llllllllll llllllll llll llllllll llllll llll ll llllllll lllll llll ll ll lllllllllllllllllll l lll ll lll l llll lll lllllllll lll lll l ll l llllllll llllll llll lllllllllll l ll ll l lll ll ll lll lll lllll lllll ll lll llll llll l ll llll ll l ll ll ll llll ll ll l ll llll llll l lll lll ll l lllll l lll lll lll ll ll lll ll l ll ll llll lll lll ll ll lll l ll llllll llll lllllll lll l llllll lllll lllllllllll llll lllll ll lll ll ll lll ll l lllll ll l ll ll ll lllll ll lllll llll lllll lll lll ll llll lllllllllll llllllllllllll llll llllllllllllllllllllll lll ll llllllllll lll llll llllll llll lllll llllllllllll lll lll lllllll ll lll lll llll ll lllll lll llllll ll ll llll lllllll lllllll ll llllllll lllllllllll l ll lll llllll lll lllllllllllllll ll llllll llllll lll llllllll ll llllllll lllllllll ll lll lllll lll ll llll ll llllll l lllll lllllllllllllllllllll lll llllllllll lllll llll lllll lllllllllll lllll llllllllllllllllll l ll llll llllll lllllllll ll ll lllll lll lllllll lll llll llll llllllll llll lllllllllllllllll lllllll lll lllll ll ll lll l llllllll l llll ll ll lllllllll ll lllll llll ll llll llllllllllll lll lll ll llllll ll ll l l l lll lllll ll llll ll lllllll llllll ll ll ll lllll llll lll ll llllll ll llll llll lll l ll lllllll lllllll lll ll llll l lllll llll ll lll ll ll ll ll lll l llll lll l lll lll l llll l lllll ll ll ll ll ll llll ll lll l l lll lll ll l ll lll lll ll l l lll lll ll lllllll ll ll llll l lllll ll ll llllll llll lll lll lll lll lll l ll l llll l l ll lllll lll lll ll lll lll ll lll lll lllll l llll l ll ll l llll l ll l lll lll l llll l llllll lllll lllll ll l lll ll ll lll lll llll lll ll ll ll lll ll l ll l llll llll llll l ll ll l ll ll lll llll ll l ll lllll llllll l ll ll ll ll lll ll lll l llll ll l ll ll llll llll l ll llll ll l ll ll llll l ll l ll llll l llll l llll llll lll llll ll llll l l ll ll llll l lll ll ll ll llll lll ll lll l llll lll l l llll lll ll ll llll lll ll ll lllll ll llll ll lll lll lll ll lll l lll ll ll ll ll ll l to l lpw 0 1ords 000 ... 789 lllllll llllll lllllllllllllllllllll llllllllllllllllllll llllllllllllllll lllllllllllllllllllllllll lllll llllllllllllllllll lllllllllll llllll l l llllllllll lll lllll llllllllllllllll llllllllllllllllllllllll llllllll llllll llll llll ll lll ll llll lll llll llll ll ll lll l ll lllll llll ll lllllll l lll llll ll llll ll lllll ll llll llll l llll ll l ll lll lll lll lll llllll lllll lllllll lllllllllllllllll lllllllllll lllll ll llllllllllllllllll lll llllllll lllllll llll lllllllllllll lllll ll ll llll lllllllllllllllllll lllllllllll lllll ll ll lll lllllll ll ll ll lll lll ll lll llll lll lll ll lll ll lll ll lllll llll lll lllll llllll ll l ll llllll llllllll lllll ll llll lll ll lll llllllll l ll llll lllll ll lllll lllll ll lllll lll lllllllllll ll lllllll lllllllllllllll lllllllll llllllllll l llll ll l ll lllll ll ll ll lll llllllll llllll llllllllllll llllllllll lll ll llllll l ll llll llllll ll ll lll lllll ll lllllll llll l lllllll lllll lllllll lll llll lllllllll lllll lll lllllllll llllll lllllllllll ll lllllllll llllll lll llll llll lllllll ll llll ll lll ll lll ll l llll ll lllllllll lll lllllll lllllllll lllll l ll lllllll lllll ll llllll lll ll l ll ll lll lll ll llll ll lllllllll ll lll llll lllll llllll lll lll llllll l lllll ll ll ll lll l lll lll lll l ll lllllllllll l l lll llllllll lll ll lll ll llll lll ll l ll l ll l ll lll l lll l llllllllllllllll l llllll llll ll ll lllll lllll llll l l ll lll ll lll ll l ll lll llll ll lll l l lll lllll ll ll lllll llll ll lllllll l ll ll ll llll l llll lll lllll lll llllll ll l llllllll llll ll lll ll lllllllll ll l l lll ll lll lll lll ll lllll l lll ll ll ll ll ll ll lll lll l llll llll llll llllllllllll lll llll lllll lll ll ll ll lll lllllll ll lll ll llll l ll ll llll l llllll lll ll ll ll lll ll lll ll ll llll ll l llll l l llllll l ll ll l l lll l ll lllll l llll llllllll lll ll lll lllllll lllllll ll llll llllll l ll lll lll llllll ll ll llll l l llll ll l l
0.6
0.00 0.05 0.10 0.15 0.20 0.25 0.00 0.02 0.04
Affinity Affinity
Figure S5.: Score and affinity plots of Topics 13 - 16.
8
erocS
erocS
erocS
erocSTopic17 Topic18
to l lpw 0 1ords 000 ... 789 l lllll lll lllllll l lll ll ll ll llll lll llllll l lll ll l lll l l lll l llll ll l l lll llll ll ll ll llll l llll ll lll ll lll ll ll llllll lll ll ll llllll ll llll ll ll lll ll l ll ll lllll ll ll ll llll l ll l llll llll lll ll lll l ll lllll ll lllll lll l ll llll lll l ll llll lll lll ll lllll lllll lll ll llll ll ll llll ll lll ll lll llllll lll llll lll llllllll llll llllll lll llll lll l llll ll llll l ll llll ll llll lll ll ll ll ll lll lllllll lllll l lllll lll lllll lllllll ll ll llllllllllll ll lll lll ll llll lll lll lllll ll ll ll lll l llll llll ll lll l ll l l lll l ll l lllll l lllll ll l lll l lll lll l ll llll ll ll ll lll lll ll l ll ll lll lllll llll ll ll ll ll lll l l ll lll lll l lllll llllllll l ll lll ll lll ll l lll ll lllll ll ll l lll l lll ll lll ll lll lll ll ll l lll l ll ll ll ll ll llll ll ll l llllll ll lllllll llll ll lll lll lllll l lll ll ll ll ll l lll l ll ll l ll lll l l ll ll l lll llll llll l lllll lll ll l ll ll lll lll lllll l l ll lll l lll ll lll ll llll l llll llll lll ll l ll ll lll lll ll lll ll ll ll ll l lll lll llll ll l lll ll lllll l lll lll l ll ll ll l ll lll lll ll l lll l lllll ll l ll lll llll llll lllllll lllllll ll ll l l ll l llllll lll lll ll lll lll ll l ll lll llll lll ll lll lll lll ll lll l llll ll l ll ll lll lll lllllll ll l llll llll ll lll ll ll ll ll ll ll lll ll l ll lll lll lll lllllll ll llll lll ll lllllll llllll lll lllllllll ll lll l lll lll llll l ll l ll l llll lllll lllll lll ll l lll l l llll l llll l llll ll ll l ll ll ll llll l llll ll l ll ll llll l ll ll lll llllll ll lll ll l ll lllll llll ll ll lll ll ll ll ll llll lllll ll ll llll ll ll lllll llll ll l lll ll ll l ll ll l lll l ll llll llll l ll lll lll lll ll ll l ll l ll ll l ll llll lll lll lll lll lll ll l lll ll l ll ll lll ll lllll ll l lll ll lllll ll lll l lll l ll ll lll lllllll ll lll llll lll ll l ll lllll l llll llll lll lll l llll ll ll ll lll ll lll ll ll lll ll ll l lll lllll llll lll llll ll l ll lllll l ll ll lll lll lll lll llll lllllllll l ll lll l ll ll lll lll lll ll lllll l lllll llllll l lll lll ll
ll
l l llll ll l ll ll llll llll lll l llllll lll ll ll l ll l to l lpw 0 1ords 000 ... 789 ll llllll llllllllll llll l lllll llllll ll l lll lll llllll l llll ll lllll lll llll lllllll ll ll lll lll lllllllllllll ll lllll l ll lll llllllllll lll l lll ll lllll lllllllll l llll lll ll llll ll ll ll lllll lllll ll lllll ll ll llll lll l llll ll lll l lllll llll ll l ll ll ll ll l ll llllllll lll l llll lll ll ll ll ll lll llllllll l llll lll llll lll ll lllllll l ll l llllll ll llllllll llll l ll llll lll ll ll llll llllllll ll llllllllllll lll llllllllllll lllll lllllll llll lll lll lllll llllllllll lllllll ll lll lll lllllll llllllllllllllll lllllllll llllllllllll llllllll llll lll lllllllllll ll llll lll llllllll ll llll l llllllll ll ll ll lll llll lll llll lll lll lllll lll lll llll l ll ll lllll llllll ll ll lll ll l ll lll ll lll lll ll l lll lllll llllllll llll llll llllllll llll ll lllll ll llll llll ll lllll llllll llll llllllll lllllllll lllllll llll llllllllllll l ll l ll ll llll lll lllll lll ll llll lll llllllllllll lll lllll ll llllllllll ll llll ll lll l lll llllll lllll ll lll lll lll llll l lll lllll lll lllll lll ll lll ll llllll llllllllllll lll l ll ll llll lll ll lll lll ll ll ll ll lllllll llll llll ll lll lllll llll llllllll llll l llllll l llllllllllll lllll ll llllllll llllllllllll lllll llll ll ll lllll ll lll lll l l llll ll ll lll ll l l lllll ll ll ll llll lllll l lllllll l ll lll l ll ll llll lll llll ll l lll ll ll lll ll ll ll llllllll lll lllll lll llll ll ll llll llll ll lllll l l lll l lll ll llll l ll lllll ll lll lll lllllll ll ll ll lll l lllllllll lll llll lllll llllllll lll llllll lll lll ll ll ll lllll ll l llllll llllll lllllllll ll ll l ll ll lllll lllll llll llll llll l ll ll l l lll ll ll llllllll lll llll lllll ll ll llll llll ll lllllll ll ll ll l l lllll lllll llll lll llll ll ll lll ll l lll ll llll lllll llll lll llll l llll l llll l l ll lllll lllll llllll l ll lllll llllll l lllllll ll ll lllllllll ll ll lll lll llll llll ll llllll ll llll l ll ll lllll ll ll lllll ll llll l ll ll lllllll lll lllll ll
lll
l ll
lll
l ll
0.0 0.1 0.2 0.3 0.000 0.025 0.050 0.075 0.100
Affinity Affinity
Topic19 Topic20
to l lpw 0 1ords 000 ... 789 ll lll l llll l lll l ll llllll lllll lll llll lllllll llll ll llll lll llll lllll lll llll lll l llll lllllll lllll ll l ll lll lll l ll lllll lllllllllll llll lll llll lllllllll llll lllllllll lll lllll ll lll llll llllll lllllllll lllllll lllll llll ll lllll l llllllllllll lllllll lllll lllllllllll lllll ll ll ll llllll ll l lllll lllll llll l ll ll lllllll llllll ll llll ll llllllll ll ll lll ll llll lll llllll llllll l lllll ll lll ll ll llll lll ll llll lll llll llllll lll llll l lll ll l lll l ll lll l llllll llllll llllll lll lllll llllll ll ll ll ll llllll lll ll llllll llll llllll lllll llll l ll llll lllll l lll llll ll ll l lllll lll l llllll lll lll llllllll ll lll l ll lll ll l ll lll lll ll ll lll ll l llll ll lll lll llllll l llllll ll lll llll llll llll lll lll l lll l lllll lll lllll llll llllllllllll lllll l lll lll lllllllll llllll lllllllllllll ll ll lllllllll lllll lllllll ll l lllll lll lll ll llllllll ll ll llllllllll l l llll lll llllllll lll lllll l llllll lllll lll lllll lll lllllll llllll llll lllll llllll lll llllllll l lll lllll lll ll lll ll ll lll ll llll llll ll ll llll ll llll ll lll lllll lll l ll ll ll l ll lll lll llll ll ll l l llll ll l lll ll llll l l llll l lll l ll llll llll ll llll ll ll l llll llll ll lll l lll l llll lllllllll lll ll ll ll ll l ll ll lll llllll lllll ll ll lll lll ll llll l llll ll lll lllll ll lll lllll ll lllllllll llllllllll llllllll ll ll l lll ll lll lllll lll lll llll lll llll ll lll lll llll llll lll lll lllll ll l ll l ll ll ll l lll l llllllllll ll ll ll llll l lll lll l llll l lll lll ll ll lll lll llllll llll llllll lllll l lll lll lllll lll lll lllll ll llllll ll ll ll l ll lll ll lll l lllll llllllllllllllllll llllllllllllllllllllllllll lllll lll llllll l ll llll llll lll ll ll lll lllll ll llll l ll lllllll llll lllll llll llllll llllllll llllllll ll llllll ll lll llll l ll ll lll llll l ll lll ll llll ll llll l ll lllll ll ll lllll llll ll lllllll lll lllllll ll lll ll ll ll lll ll ll lllllll ll l llllll ll l ll to l lpw 0 1ords 000 ... 789 lll llll ll lll lll lll ll l lll ll lllllll l ll ll llll llll ll l lll llll ll l ll l lll llll lll l llll l lll l lll ll llll ll ll lllll l lll l llllll lll llll lll ll lll llll ll llll lll l ll ll lll lll ll lllll ll ll lll ll l ll lll l ll llll llll lll l llll llll ll l ll lllll ll llll l ll l lll l l lll llll ll l ll ll llll lll ll ll llll l lllll ll ll llll ll lll l llll ll l ll l lll llll llll lll llll ll ll ll llll l ll ll l ll l lllll l lll ll ll ll ll lllll lllll l llll ll lll l llll l llll ll lll lll lll ll ll l l llllll l l lll ll ll ll ll lll ll ll l l lll ll ll ll l ll lll lllll lll l ll ll lllll l llll lll ll lll l l lll ll lllll ll l l lllll l lll l ll l lll l l ll llll lll ll l lll ll lll ll lll lll l lll l llllll ll ll l l lll ll l llllllll lll llll l l ll llll lllllll l ll lll l lll ll ll ll l llll lll ll llll ll ll lll llll ll lll llll ll lllll l ll lll l lll l lllll l lllll lll l l lll llll ll l lll l ll llll llll ll ll lll ll lll lll l lll l ll llllll l lll lll llllll lllll ll lll lll ll l lll lllll lll ll lll ll lll ll llll ll ll lll ll ll ll lll ll ll ll l lll ll lllll l l lll ll llll l ll lll ll ll l ll ll l ll ll lll lll l llll ll l ll lllll llll l ll l ll llll l lll ll l l ll l l lllll lll l l lll llll ll lll lll ll llll ll lll l ll llll lll ll lll ll lll ll lll lll lll ll lll l ll l lll lll lll l ll l lll ll lll ll ll l lll llll llll lll lll lll ll ll ll ll llll lll ll l ll ll ll l ll llll llll lll ll lll l lll lll lll l ll lll ll ll lllll l ll l lll ll ll lll ll ll llll l ll llll l llll ll l lll l lll lll lll lll ll llll ll lll ll lll llll ll ll lll ll ll lll llll l lll l ll ll lllllll ll ll ll lllll lll lll ll llll llll lll l lll llll ll ll lllllllll ll ll lll ll ll lll ll l ll llll ll llll ll lll l ll l lll l ll lllll ll ll ll lll ll l ll ll l llll llll ll lll ll ll ll ll llllll lll llll ll ll ll ll ll llllll l llll lllll llll ll ll ll ll lll llllll lll lll ll lll ll lll llll llll ll lllllllll lll ll llll llllllll ll l lll ll llll ll lll lllll l lll l llll lll lllll ll lll ll llll ll l lllll ll l ll ll ll ll l l ll ll ll lllll ll l lllllllll lll l ll llll lllllll ll ll lll lll l ll l lllll lll lllllll ll ll llllll l
0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
Affinity Affinity
Figure S6.: Score and affinity plots of Topics 17 - 20.
9
erocS
erocS
erocS
erocSAppendix F. Topic Interpretation
Table S2.: Interpretation of the topic based on A matrix. The abbreviation
∗47%
is marked by an asterisk and the full form can be found in the Web Appendix G.
Topic Interpretation Topscorewords(score)
1 LungScan subpleural crazypaving bronchogram
(0.963) (0.957) (0.950)
2 CompoundandDrug ProteinDataBank papain-like intermolecular
(0.938) (0.936) (0.934)
3 Bacteria,Nano,andDiet DHA∗ biofilm vancomycin-resistant
(0.968) (0.968) (0.967)
4 TreatmentofOtherDiseases sarcoma arthroplasty neoadjuvant
(0.948) (0.944) (0.943)
5 Symptoms(Cardiovascular) vein antithrombotic VTE∗
(0.906) (0.899) (0.898)
6 Molecular-levelResponsetoInfection NLRP3∗ metalloproteinase upregulation
(0.929) (0.927) (0.926)
7 COVID-19RiskPredictionMarkers alanin BUN∗ prealbumin
(0.976) (0.972) (0.969)
8 LiteratureReview ProsperoDB WanfangDB meta-analys
(0.967) (0.966) (0.944)
9 SymptomandComorbidity petechiae guillain-barresyndrome ageusia
(0.958) (0.955) (0.952)
10 Cardiovascular infarction thromboprophylaxis VTE∗
(0.912) (0.909) (0.909)
11 SocialImpact classroom pedagogy zoom
(0.925) (0.911) (0.906)
12 FinancialandEconomicalImpact macroeconomics monetary profit
(0.929) (0.928) (0.928)
13 StatisticalModeling Weibull least-square MAE∗
(0.984) (0.975) (0.966)
14 COVID-19Test LFIAs∗ transcription-PCR∗ Wantai
(0.979) (0.974) (0.972)
15 Psychological/MentalIssues anxious PTSD∗ post-traumatic
(0.942) (0.940) (0.935)
16 LungScan procalcitonin ground-glass opacity
(0.924) (0.922) (0.916)
17 Treatment reintubation multi-centric Hydroxychloroquine
(0.943) (0.943) (0.937)
18 Air,Mask,Breathing laryngoscope facepiece supraglottic
(0.978) (0.978) (0.975)
19 PreventionofCOVID-19 alcohol-based decontamination rub
(0.944) (0.941) (0.919)
20 ImmunologyandVirus MHC∗ multi-epitope immunodominant
(0.976) (0.975) (0.973)
10Appendix G. Full Form of Abbreviation
Table S3.: Abbreviation in Table S2.
Abbreviation Full name
DHA Docosahexaenoic Acid
∗
VTE Venous Thromboembolism
∗
BUN Blood Urea Nitrogen
∗
MAE Mean Bbsolute Error
∗
LFIAs Lateral Flow Immunoassays
∗
PCR Polymerase Chain Reaction
∗
PTSD Post-traumatic Stress Disorder
∗
MHC Major Histocompatibility Complex
∗
11