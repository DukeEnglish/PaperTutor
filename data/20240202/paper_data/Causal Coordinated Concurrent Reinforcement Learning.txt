Causal Coordinated Concurrent Reinforcement Learning
TimTse1 IsaacChan1 ZhitangChen1
Abstract the slight modification in which each environment has a
constantstrengthwind. Thewindexertsahorizontalforce
In this work, we propose a novel algorithmic
onthependulumwhichmayormaynotbethesameacross
framework for data sharing and coordinated ex- the environments. Intuitively, one can imagine that it is
ploration for the purpose of learning more data-
possible,forexample,tosharedataamongsttheagentsand
efficient and better performing policies under
to coordinate their actions for the purpose of accelerating
a concurrent reinforcement learning (CRL) set-
the learning of each controller. However, due to the pres-
ting. In contrast to other work which make
ence of non-identical environments, it is not immediately
the assumption that all agents act under identi-
clearhowonemaygoaboutthis.
cal environments, we relax this restriction and
instead consider the formulation where each Thisworkseekstoaddresstheseresearchquestions;specif-
agentactswithinanenvironmentwhichsharesa ically,weaimtoinvestigateanalgorithmto1)extractand
globalstructurebutalsoexhibitsindividualvari- separatethevariationsacrossasetofMDPs,2)leveragethe
ations. Our algorithm leverages a causal infer- knowledge of the similarities of environments to provide
ence algorithm in the form of Additive Noise a better informed approach to policy and/or state-action
Model-MixtureModel(ANM-MM)inextract- value function learning and 3) coordinate the selection of
ing model parameters governing individual dif- actions for a set of agents so as to accelerate the explo-
ferentials via independence enforcement. We rationoftheirstatespace. Tothisend,weleverageANM-
propose a new data sharing scheme based on a MMs (Hu et al., 2018) which is a recently proposed al-
similaritymeasureoftheextractedmodelparam- gorithmincausalinferencethatperformslatentmodelpa-
eters and demonstrate superior learning speeds rameterextractionviaindependenceenforcement. Mecha-
on a set of autoregressive, pendulum and cart- nismclusteringisshowntobeequivalenttoclusteringover
poleswing-uptasksandfinally,weshowtheef- theextractedlatentparametersandinthiswork, weapply
fectiveness of diverse action selection between expectation–maximization(EM)(Dempsteretal.,1977)to
commonagentsunderasparserewardsetting.To determine a soft similarity metric that is used to inform a
thebestofourknowledge,thisisthefirstworkin data-sharingschemeforCRL.Finally,weintroduceasim-
considering non-identical environments in CRL ple yet effective sampling-based coordinated action selec-
and one of the few works which seek to inte- tionheuristicwhichweshowtobeveryeffectiveunderthe
grate causal inference with reinforcement learn- sparse reward setting when coupled with our proposed al-
ing(RL). gorithm.
In contrast to other causal inference methods based
on functional models such as ANM (Hoyer et al.,
1.Introduction 2009), LiNGAM (Shimizu et al., 2006), PNL (Zhang &
Hyva¨rinen, 2009) and IGCI (Janzing & Scho¨lkopf, 2010)
Weaddresstheproblemofconcurrentreinforcementlearn-
which assume a single causal model for all observations,
ing(CRL)whereineachagentactswithinanenvironment
we opt for the ANM-MM in this work due to its general-
sharingaglobalstructurebutalsoexhibitsindividualvari-
izing assumption that the observed data is generated from
abilitiesspecifictoanagent. Concretely, considertheset-
multiple sources with varying causal models in alignment
tingwheremultiplereinforcementlearning(RL)agentsare
with our goal of modelling concurrent MDPs which may
performingtheclassicalpendulumswing-uptaskbutwith
beperturbedbymultiplesources. Therehasbeensurpris-
1Huawei Noah’s Ark Lab. Correspondence ingly little work in the area of CRL despite their applica-
to: Tim Tse <tim.tse@huawei.com>, Isaac Chan bility to important domains such as web services (Silver
<isaac.chan@huawei.com>, Zhitang Chen <chenzhi- et al., 2013) and robots concurrently learning how to per-
tang2@huawei.com>.
form a task (Gu et al., 2016). The work of (Silver et al.,
2013) and (Gu et al., 2016) only considers data-gathering
4202
naJ
13
]LM.tats[
1v21081.1042:viXraCausalCoordinatedConcurrentReinforcementLearning
in parallel and does not address coordinated action selec- ilarities. Moreconcretely,wemaketheassumptionthatthe
tion. Whiletheworkof(Dimakopoulou&Roy,2018)es- state transition models of all agents have the same func-
tablishes essential conditions for efficient coordinated ex- tional form which is parameterized by ξ and their reward
ploration and their follow-up work (Dimakopoulou et al., functionssharethesamefunctionalformparameterizedby
2018) demonstrates the scalability of their algorithm to ω,i.e.,
a continuous control problem, their frameworks assume
identicalMDPsacrossallconcurrentinstances. Inthispa- P n(s t+1|s t,a t)=P(s t+1|s t,a t;ξ n), (1)
per, we address data-gathering, coordinated action selec- R (s ,a )=R(s ,a ;ω ) (2)
n t t t t n
tion,andnon-identicalMDPsforCRL.
for each nth agent. We stress that ξ and ω are unknown,
The remainder of the paper is organized as follows: first
rendering the task of CRL with heterogeneous environ-
we providesome preliminaries, thenwe describeour pro-
mentsdifficult.
posedmodel. Next,wepresentexperimentsonautoregres-
sive,sparseautoregressive,pendulumandcart-poleswing-
2.3.AdditiveNoiseModel-MixtureModel
uptasksandfinally,wewillconclude.
ANM-MM Additive Noise Model - Mixture Models (Hu
2.BackgroundandRelatedWork et al., 2018) is defined as a mixture of causal models of
thesamecausaldirectionbetweentworandomvariablesX
Inthissection,weprovidesomebackgroundtoRL,concur- andY whereallthemodelssharethesameformgivenby
rentRL,andANM-MMandalsodiscusstherelatedwork thefollowingAdditiveNoiseModel(ANM)(Hoyeretal.,
seedsampling. 2009):
Y =f(X;θ)+ϵ (3)
2.1.ReinforcementLearning
where X denotes the cause, Y denotes the effect, f is
Reinforcementlearning(RL)enablesanagentwithagoal a nonlinear function parameterized by θ and ϵ ⊥⊥ X is
to learn by continuously interacting with an environment. the noise. The goal of the ANM-MM model is to ex-
At each time step, the agent exists in a given state within tract the model parameter θ and this is done by mapping
theenvironmentandtheagentmakesanactionontheen- X and Y to a latent space using Gaussian process par-
vironment where the environment then provides feedback tially observable model (GPPOM) and then incorporating
to the agent with the next state and reward. The above theHilbert-Schmidtindependencecriterion(HSIC)(Gret-
interaction can be modeled by a Markov decision process tonetal.,2005)onthelatentspaceofGPPOMtoestimate
(MDP) consisting of s ∈ S the set of states, a ∈ A the themodelparameterθ. ByassumingaGaussianpriorover
t t
setofactions,P(r |s ,a )therewardfunctionwhichmaps the model parameters, the method proposes minimizing a
t t t
the reward that the agent receives in state s taking action log-likelihood loss function subject to a HSIC regulariza-
t
a ,P(s |s ,a )thetransitionfunctionwhichdefinesthe tiontermforextractingthelatentparametersofthemodel.
t t+1 t t
probabilityoftransitioningtostates whiletakingaction Toidentifytheunderlyingdatageneratingmechanisms,the
t+1
a instates andγ ∈ [0,1]thediscountfactorwhichsets authorsproposeamechanismclusteringtechniqueamount-
t t
thepriorityofshortversuslongtermrewards. Thegoalof ing to conducting k-means clustering directly on the ex-
the agent is to find a policy π : S → A such that the ex- tractedlatentparameters.
pected discounted cumulative rewards E[(cid:80)T γtr |a =
t=0 t t
π(s t)]inthelifetimeT oftheagentismaximized. 2.4.SeedSampling
In this section, we review seed sampling (Dimakopoulou
2.2.ConcurrentReinforcementLearning
et al., 2018) which is a recently proposed algorithm for
One extension to the RL framework is the concurrent RL coordinatedexplorationforCRL.Westress,however,that
formulation where instead of having one agent, there ex- theproblemformulationofseedsamplingisdifferentfrom
istsN RLagentsthatareinteractingintheirownenviron- ours in that the former assumes each concurrent MDP is
ments. There are benefits to be exploited, as agents may identical. Notwithstanding, our problem formulation is
potentiallysharedataand/orcoordinatetheirexplorationto niche to the extent where, to the best of our knowledge,
learnmoredata-efficientand/orbetterperformingpolicies. there are no current works which exactly address our for-
MuchoftheworkinconcurrentRLconsidersthescenario mulation and thus we settle with seed sampling which, in
whereeachagentactsinidenticalenvironments(i.e.,iden- ouropinion,istheclosestalternativebaseline.
ticalMDPs). Inthiswork,werelaxthisrestrictionandin-
Seed sampling is an extension of posterior sampling (aka
steadassumethatthestatetransitionmodelandthereward
ThompsonSampling(Thompson,1933))forreinforcement
functionofdifferentagentsarenotidenticalbutsharesim-
learning (PSRL) (Strens, 2000) to the parallel agent set-CausalCoordinatedConcurrentReinforcementLearning
ting satisfying properties of efficient coordinated explo-
ration (Dimakopoulou & Roy, 2018). The main idea is
that each agent k possesses a sampled seed ω which is X
k
intrinsic to the agent and differentiates how agent k per- X
ceivestheshareddatabufferB. Theseedω k remainsfixed GP Y
throughoutthecourseoflearning. Oneformofseedsam-
plinginvolveseachagentindependentlyandrandomlyper-
Y Θ
turbing the observations in B via different noise term z
k
determined by seed ω which induces diversity by creat-
k
ingmodifiedtrainingsetsfromthesamehistoryamongthe
agents. Thepresenceofindependentrandomseedsencour- Figure1: Anautoencoderinterpretationofourmodel.
age diverse exploration while a consistent seed ensures a
certain degree of commitment from each agent. We refer
the reader to the original paper for further details on seed nextstatesY ≜[s′,...,s′ ]⊤ orthesetofrewardsY ≜
s 1 N r
sampling. [r ,...,r ]⊤ as the effect depending on if one wishes
1 N
to extract, respectively, the model parameters ξ or ω, the
matrixcollectingthemodelparametersofeachdata-point
3.AlgorithmicFramework
byΘ = [θ ,...,θ ]⊤ andtherandomvariablethatcon-
1 N
Ouralgorithmcannaturallybedividedintotwopartswhere tributes to the effect by X˜ = [X,Θ]. Emulating the for-
inthefirstpart,weapplytheindependenceenforcementof mulationofANM-MMs,weapplyGaussianprocesslatent
ANM-MMstoextractthemodelparameters.Next,weper- variable model (GP-LVM) (Lawrence, 2005) for mapping
formsoftclusteringonsaidmodelparametersintheform observationtolatent,butmorespecifically,weuseaback-
ofclusteringwithGaussianMixtureModel(GMM)where constrained version of GP-LVM (Lawrence & Quin˜onero
then the similarity features given by the clustering model Candela, 2006) that introduced a multilayer perceptron
is used to dictate the sharing of data and the coordination (MLP) mapping which preserves local distances between
ofactions. Wedetaileachconstituentbelow. Algorithm1 observation space and latent space. Denote Z = [X,Y],
providesapseudocodesummaryoftheoverallalgorithm. thenletΘbetheoutputoftheconstrainingMLPEwithin-
putsZ,parameterizedbytheweightsw,i.e.Θ≜E (Z).
GivenN RLagents,ourworkseekstodeterminethemodel w
Following(Huetal.,2018),welearnthemodelparameters
parameters θ of each agent given a cause X and a result-
bymappingΘbacktoobservationspaceusingaGaussian
ing effect Y. We make note that in a significant bulk of process (GP) and therefore, denoting X˜ = [X,Θ], the
literatureincausalinference,theproposedalgorithmsseek
log-likelihoodlossoftheobservationsisgivenby
to determine a causal direction X −→ Y using sampled
observations x and y . Perhaps an intuitive extension of
this model to
Ri
L
datai
would involve finding model pa-
L(Θ|X,Y,β)=−DN
ln(2π)−
D ln(cid:0) |K˜|(cid:1)
2 2
rameters of each agent using individual transition tuples
(s ,a ,r ,s ) for each nth agent. However, the infor- −
1 Tr(cid:0) K˜−1YY−1(cid:1)
,
t t t t+1 n 2
mationprovidedbyasingletransitiontupleconcerningthe
probabilitydistributionsP(r |s ,a )andP(s |s ,a )is whereK˜ denotesahigherdimensionalkernelmappingof
t t t t+1 t t
too sparse and instead, we propose to learn a causal map- the inputs X˜ and in this paper, we adopt the radial basis
ping for each nth agent using P samples of the state i.e., function (RBF) kernel. In summary, our architecture may
P = {(s ,a ,r ,s ) |p = 1,...,P} wheredata-setP be interpreted as an autoencoder consisting of a MLP en-
t t t t+1 p n
ought to provide sufficient information to the reward and coderandaGPdecoderasillustratedinFigure1.
state transition functions (i.e., the density of P does not
The model parameters cannot be found by maximizing
resembleadeltafunction).
just the log likelihood alone since ANM-MMs addition-
ally require the independence between X and θ. To this
3.1.ModelParameterExtractionviaIndependence
end,weapplyindependenceenforcementviaHSIC,whose
Enforcement
empirical estimator based on a sampled dataset D ≜
Moreconcretely,denotes
n
= [s 1,...,s P]⊤
n
astheaggre- {(x n,y n)}N n=1isgivenby
gationofthesampledstatesforthenthagentandsimilarly,
1
r = [r ,...,r ]⊤ and s′ = [s′,...,s′ ]⊤ as the ag- HSIC (D)= Tr(KHLH)
n 1 P n n 1 P n b N2
gregation of the sampled next states and rewards for the
nth agent, respectively. We denote the collection of states whereH =I−1⃗1⃗1⊤,⃗1isaN×1vectorofonesandboth
X ≜ [s 1,...,s N]⊤ as the observed cause and the set of K and L are keN rnel mappings. Incorporating the HSICCausalCoordinatedConcurrentReinforcementLearning
term,wearriveatthefinallossobjectivegivenby 3.3.CoordinatedExplorationHeuristic
Under the CRL setting, there are benefits in coordinating
argminJ(Θ)=argmin[−L(Θ|X,Y,Ω)
theselectionofactionsofagroupofagents. Forexample,
Θ,Ω Θ,Ω
intuition tells us that two similar agents should take dif-
+λlogHSIC (X,Θ)] (4)
b ferentactionsiftheyalsohappentobeinsimilarstatesas
thiswouldhavethedesirableeffectofencouragingdiverse
where λ controls the importance of the HSIC term and Ω
exploration across the group of agents. In this work, we
isthesetofallhyperparametersincludingallofthekernel
proposeasimpleactionselectionheuristicwhichachieves
hyperparameters. J(Θ)canbeminimizedbyanystochas-
justthat. Theheuristicworksasfollows: atthebeginning
tic gradient-based optimizer and in this work, we use the
of each episode, each agent n samples a µ ∼ N(0,σ2)
scaledconjugategradientalgorithm(Møller,1993). n
which acts as the mean to an Ornstein-Uhlenbeck pro-
cess whose noise is used to perturb the actions of agent
3.2.SoftMechanismClusteringforSimilarity-Based
n. At each time-step t, the noise scale σ gets anneal in
DataSharing
anϵ-greedy-likefashionsothatitsaverageperturbanceap-
For each observation pair (x ,y ), there is an associated proaches zero as time approaches infinity. Informed data-
n n
model parameter θ which characterizes its underlying sharingthenallowssimilaragentstobecognizantofstate
n
data generating mechanism. Furthermore, common data spaceswhichitmayotherwisebeobliviousto. Despiteits
generatingmechanismwouldhavesimilarθandhenceare simplicity, we demonstrate in the experiments its signifi-
identifiable with respect to θ. In the windy pendulum ex- canceinthesparserewardsetting.
ample, the model parameters may correspond to various
wind strengths experienced by the agents and under our 3.4.AConnectiontoTransportability
framework, similar wind strengths may be grouped with
Inthissectionweprovideanintuitionbydrawingconnec-
respect to similar model parameters. To identify common
tionstothecausalitymodelof(Rosenbaum&Rubin,1983)
data generating mechanism, we employ a soft clustering
and transportability (Bareinboim & Pearl, 2016) in causal
mechanismintheformofGaussianmixturemodel(GMM)
inference. Firstwedefineournotation: LetQ bequeryat
clusteringoverΘ. GivenC componentsofGaussianmix- t
time t where query is next state prediction, b(E) the bal-
tures, we apply the EM algorithm to assign each θ its
n ancingscoreforacovariateE whereinourcontext,e∈E
probabilityofbeingineachoftheC centroids.
isaindividualrealizationinthesetofMDPs,π thepolicy
AtconvergenceoftheEMalgorithmeachmodelparameter andg(E)=P(π|b(E))thepropensityscore.
isassociatedwithavectorv ∈RC lyingintheprobability
n According to (Rosenbaum & Rubin, 1983), a score b(E)
simplex(i.e.,v ≽0,∥v ∥ =1)wherethecthentryofv
n n 1 n can act as a balancing score for covariates E if it satisfies
representstheprobabilitythatv belongstothecthcompo-
n thefollowingconditions:
nentforallc∈{1,...,C}. Eachv istreatedasafeature
n
vectorandtogetherareusedtodefineapairwisesimilarity
measuregivenbyaRBFkernel,i.e., 1. abalancingscoremustbe“finer”thanthepropensity
scoreand
K≜K(v ,v )
m n
2. our“treatmentassignment”,orpropensityscore,must
(cid:18)
∥v −v
∥2(cid:19)
=exp − m n 2 ∀m,n∈{1,...,N}. bestronglyignorablegiventhebalancingscore.
2
Assumption1 All realizations of an environment e ∈ E
Thedegreeinwhichdataissharedforthenth agentacross
varyonlybyparameterθandareotherwisethesame.
alltheotheragentsisthendictatedbyhowsimilarthenth
agent’s environment is compared to the environments of
Thisfollowsfromourproblemdefinitionandisillustrated
all other agents. For this, we first calculate Kˆ which is
as well in our windy pendulum example where environ-
the result of row normalizing K such that each row sums
ment only varies by wind represented by θ. Therefore,
to unity, i.e., Kˆ = (cid:104) K 1⊤ ,..., K N⊤ (cid:105)⊤ . Next, thedifferencebetweenenvironments(i.e.,inputcovariates)
(cid:80)N q=1K1,q (cid:80)N q=1KN,q
can be represented entirely by θ. We show θ satisfies the
upontrainingthenth agentwithbatchsizeB,aminibatch
conditionsabove(i.e.,isabalancingscore).
isconstructedbyaggregatingthedataobtainedviarandom
sampling from all other agents’ replay buffers where the
3.4.1.θ ISFINERTHANTHEPROPENSITYSCOREg(E)
amountofdatasampledfromtheqth agent’sreplaybuffer
isgivenbyK¯ whereK¯ =round(BKˆ )andround(·)is becausethereexistsanf suchthatg(E)=f(θ)(e.g.,f is
n,q n n
theelement-wiseroundingoperator. theexogeneity-injectingwindinourcontext)andtherefore,CausalCoordinatedConcurrentReinforcementLearning
Algorithm 1 Causal Coordinated Concurrent Reinforce- derivationfrom(Bareinboim&Pearl,2016)toshow
mentLearning
(cid:88)
Q = P(s |do(π(s |e∗)),E =e∗,s ,θ)
1: Initialize environments E n, DDPGs D n, and replay t t+1 t t
θ
buffersB forn=1,...,N
n P(θ|E =e∗,do(π(s|e∗)))
2: Initializescalefactorsσ 1,σ 2
(cid:88)
3: Generate cause X by uniformly sampling from = P(s |do(π(s |e∗)),s ,θ)
t+1 t t
[−s ,s ]
min max θ
4: Generateuniformlyrandompolicyπ P(θ|E =e∗,do(π(s|e∗)))
5: foreachnthagentdo (cid:88)
= P(s |do(π(s |e∗)),s ,θ)P(θ|E =e∗)
6: Generate effect Y n by evaluating X n with π {i.e., t+1 t t
rewardsornextstatesdependingonextractingξ or θ
(cid:88)
θ} = P(s |do(π(s |θ)),s ,θ)P(θ|e∗)
t+1 t t
7: endfor θ
8: Determine latent model parameters Θ by optimizing
where π(s|e∗) is an alternative notation for the propen-
Loss(4)
sityscore. Thesecondlinefollowsfrom“S-admissibility”,
9: Determine Kˆ by fitting GMM clustering then row-
the third line shows independence between θ and the pol-
normalizing
icy/propensityscore(i.e.,thirdruleofdo-calculus)andthe
10: forenumberofepochsdo
lastlineillustratesthatθ isasufficientbalancingscorefor
11: Initialize OU(µ n,σ 2), µ n ∼ N(0,σ 12) for n = e∗. π(s |θ),θ) can be modelled by any function approxi-
1,...,N t
matewhileP(θ|e∗)inourwork,isestimatedusingANM-
12: whileepochisnotoverdo
MM.Westressthatitisnotthecasethatanylatentvariable
13: foreachnthagentdo
modelmayextractagoodbalancingscoreastheextracted
14: Takeactiona+ϵwithaaccordingtoD n and θ may be dependent on the policy as well. As a result,
ϵ∼OU(µ ,σ )inE
n 2 n we therefore select to use the ANM-MM which employs
15: Cacheexperiencetuple(s,a,r,s′)toB n HSICtoenforceindependencebetweentheextractedθand
16: endfor
theembeddedpolicyinformation.
17: Annealσ 1,σ 2
18: foreachnthagentdo
4.Experiments
19: Create mini-batch B M by sampling from all
otheragents’BaccordingtoK¯
n Wedescribeourexperimentalsetupsandtheresultsonan
20: UpdatecriticandactornetworkofD nwithB M autoregressive(AR)task,anARtaskwithsparserewards,
21: endfor
pendulumandcart-poleswing-uptask.Inallofourexperi-
22: endwhile
ments,weassumecontinuousstatesandactionsandthere-
23: endfor
fore, opt for the actor-critic algorithm, deep deterministic
policy gradient (DDPG) (Lillicrap et al., 2015) for policy
learning. Ineveryexperiment,eachinstantiationofDDPG
had the following settings: a critic network consisting of
twohiddenlayersof256unitswhichreceivesthestatesand
byTheorem2of(Rosenbaum&Rubin,1983)b(E)=θis
actions as inputs, another hidden layer of 128 units and a
abalancingscore.
singleoutputunitparameterizingthestate-actionvalue.An
actornetworkconsistingofahiddenlayerof256units,an-
3.4.2.THEPROPENSITYSCOREISSTRONGLY
otherhiddenlayerof128unitsandfinally,anoutputlayer
IGNORABLE
with action dimension number of units. All units use the
givenEandthereforebyTheorem3of(Rosenbaum&Ru- ReLUactivationfunctionexceptfortheoutputofthecritic
bin,1983),the“treatmentassignment”,inotherwords,the networkwhichuseslinearactivationandtheoutputofthe
policydeterminationmustalsobestronglyignorablegiven actor network which uses tanh(·) activation to bound the
balancingscoreθ. actionspace.
Therefore, having established that θ satisfies the con-
4.1.AutoregressiveTask
ditions above, θ is a fully expressive representation of
the covariates. In the case of transportability, we ob- WeconductsomepreliminaryexperimentsonARtasksas
serve a set of next-state predictions on environment in- aproofofconceptoftheparameterextractioncapabilities
stances e∗ ∈ E and we seek to answer the query Q = and the faster learning speeds of our algorithm. The AR
t
P(s |π(s |e∗),E = e∗,s )P(θ,E). We draw on the model used in this experiment is defined by the state evo-
t+1 t tCausalCoordinatedConcurrentReinforcementLearning
100 100 O Glu or bs al 100
Individual
80 80 Seed sampling 80
60 60 60 Ours
Global
40 40 40 I Sn ed eiv di d sau mal pling
20 20 20
Ours
0 G Inl do ib va idl ual 0 0
Seed sampling
20
0 50 100 150 200 250 300 0 50 100 150 200 250 300 0 50 100 150 200 250 300
Episode Episode Episode
(a)N(−4,0.12) (b)N(−1,0.12) (c)N(4,0.12)
Figure 2: A comparison between our model vs. three baselines for s sampled from a trimodal GMM on the AR task.
∗
ShadedregionrepresentsoneSDofuncertaintyfrom30sampledenvironments.
17.5 4.0 8 17.5
15.0 3.5 7 15.0
12.5 3.0 6 12.5
10.0 2.5 5 10.0
2.0 4
7.5 7.5
1.5 3
5.0 1.0 2 5.0
2.5 0.5 1 2.5
0.0−0.4 −0.3 −0.2 −0.1 0.0 0.1 0.2 0.3 0.4 0.0 −0.6 −0.4 −0.2 0.0 0.2 0.4 0.6 0 −0.1 0.0 0.1 0.2 0.3 0.0 −0.20 −0.15 −0.10 −0.05 0.00 0.05 0.10
Latent Latent Latent
(a) (b) (c) (d)
Figure3: HistogramoftheextractedmodelparametersfittedwithGMMclusteringforthe(a)ARtask,(b)ARtaskwith
sparserewards,(c)windypendulumtaskand(d)cart-poleswing-uptask. Barsofthehistogramarecolor-codedaccording
totheirtruedatageneratingmechanism.
lutionequations =ϕs +a +ϵwhereϕisaconstant histogram bars (i.e., hidden model parameters) are color-
t+1 t t
setto0.95andϵ ∼ N(0,0.12)isthenoiseparameter. The codedingreen,blueandredwhereeachcolorcorresponds
rewardisdefinedasr =exp(−|s −s |),inotherwords, toadifferentdatageneratingmechanism(i.e.,meanoftar-
t t ∗
the goal of the agent is to drive and maintain its state as getlocation),whichasillustrated,ouralgorithmisableto
close as possible to s . Initial state s is sampled from a cleanlyextractandseparate.
∗ 0
standard normal distribution. We examine the model pa-
Next,wecomparethelearningcurvesofourproposeddata
rameter extraction capabilities of our algorithm based on
sharing scheme against a baseline where data is naively
variationsintherewardfunctionandtothisend,wecreate
shared across all agents, a second baseline where concur-
three groups of 20 MDPs where the first group is instan-
rencyisdisregardedandeachagentlearnsfromtheirown
tiated with a reward function s ∼ N(−4,0.12), the sec-
∗ experience and a third baseline where the coordinated ex-
ondgroupwiths ∼ N(−1,0.12)andthelastgroupwith
∗ ploration seed sampling algorithm is used. A batch size
s ∼ N(4,0.12). Thesetofstatesrepresentingthecause,
∗ of192,192,64and192isusedforouralgorithm,thefirst
that is, X is generated by uniformly sampling 100 state
baseline,thesecondbaselineandthethirdbaseline,respec-
samples in the interval [−10,10] and each agent shares
tively.Thedecisionforusingalargerbatchsizestemsfrom
a common cause that is perturbed slightly with Gaussian
the rationale that algorithms utilizing data sharing have a
noise (so as to prevent singular matrices during the opti-
largeramountofdataavailabletothemandalargerbatch
mizationprocedure). Togeneratethecorrespondingeffect
size allows models to leverage this. The resulting train-
Y, we first generate a data set D = {(s,a,r,s′) |m =
n m ing curves are depicted in Figure 2(a)-(c). We see that
1,...,M}oftransitiontuplesforeachagentnusingran-
acrossthethreegroupings,ourreplaybuffersharingstrat-
dom policies. Then for each s ∈ X, the correspond-
egyresultedinfasterpolicylearningaswellsuperiorpoli-
ing cause for agent n is defined by y (s) ≜ D (m )[r]
n n ∗ ciescomparedtothebaselines. Theresultscorroboratethe
where m = argmin ∥s−D (m)[s]∥2 and D (m)[x]
∗ m n 2 n ideathatnaivedatasharingposesasadetrimenttopolicy
denotes element x ∈ {s,a,r,s′} of the mth tuple of D .
n learningduetothefactthattargetlocationsaredifferentbut
Figure 3(a) depicts a histogram of the resulting extracted
benefits can clearly be reaped with judicious data sharing
modelparametersalongwithafittedGMMclustering.The
whencontrastedwithpolicieslearnedwithnodatasharing.
ytisneD
dna
ycneuqerF
sdraweR
)detnuocsidnU(
latoT
ytisneD
dna
ycneuqerF
sdraweR
)detnuocsidnU(
latoT
sdraweR
)detnuocsidnU(
latoT
ytisneD
dna
ycneuqerFCausalCoordinatedConcurrentReinforcementLearning
8000 O Bau sr es line 1 70 .. 50 1 70 50
6000 5.0 50
2.5 25
4000 0.0 0
−2.5 −25
2000 −5.0 −50
0 −7.5 −75
−100
0 50 100 150 200 250 300 −10.0 0 20 40 60 80 100 0 20 40 60 80 100
Episode Timestep Timestep
(a)N(−20,0.32) (c)Baselineatepoch1 (e)Oursatepoch1
10.0 30
8000 Ours
Baseline 7.5 20
6000 5.0
2.5 10
4000 0.0 0
2000 −2.5 −10
−5.0
−7.5 −20
0
0 50 100 150 200 250 300 −10.0 0 20 40 60 80 100 −30 0 20 40 60 80 100
Episode Timestep Timestep
(b)N(20,0.32) (d)Baselineatepoch300 (f)Oursatepoch300
Figure4: (a)and(b): Acomparisonbetweenourcoordinatedactionselectionheuritisticvs. aCRLagnosticbaselinefor
s sampledfromabimodalGMMonthesparserewardARtask. ShadedregionrepresentsoneSDofuncertaintyfrom18
∗
sampledenvironments. (c)-(f): StatetrajectoriesoftheARprocessesforvariousinstances.
Similartonaivedatasharing,seedsamplingalsoperforms We compare our algorithm to a baseline algorithm which
poorlywiththemostlikelyculpritbeingthebaseline’sas- onlysharesdatabasedonenvironmentsimilaritybutdoes
sumption that all agents interact with underlying identical notcoordinatetheirexplorationefforts(i.e.,eachagentexe-
MDPs. cutestheirownexplorationstrategy).Theresultingtraining
curvesaredepictedinFigure4(a)andFigure4(b). Wesee
4.2.AutoregressiveTaskwithSparseRewards thatforbothtargetlocations,ouralgorithmisabletolearn
a good policy whereas the baseline algorithm is unable to
UnderaCRLsettingitisintuitivetocoordinatetheactions
learnanythingatall.Toinvestigatewhythisisthecase,we
ofeachagentinordertoefficientlyexplorethestatespace
lookintothestatetrajectoriesoftheARprocessesofbase-
as quickly as possible. The benefits of this are most pro-
lineatepoch1,baselineatepoch300,oursatepoch1and
nouncedundersparserewardsettingsandwedemonstrate
oursatepoch300whicharedepictedinFigure4(c),Figure
the superior performance of our action selection heuris-
4(d), Figure 4(e) and Figure 4(f), respectively. Each line
tic coupled with our data sharing strategy under this set-
represents the state trajectory of a single agent. The fig-
ting. We use the same AR state evolution equation in the
uresshowthatforthebaselinealgorithm,despiteexploring
previous experiments but in order to make the task ex-
the environment during the initial episodes, each individ-
hibit sparse rewards, we modify the reward function so
ual’s exploration strategy is insufficient in discovering the
that there is a penalty incurred for making an action pro-
locationofthelargerewardpayoffandinsteadsettledwith
portionate to the magnitude squared of the action and the
a sub-optimal policy which involves simply not moving.
agentreceivesalargerewardformovingtothetarget,i.e.,
Ontheotherhand,ouractionselectionheuristicisableto
r t =100e−|s−s∗|−a 102 t. Wecreatetwogroupsof18MDPs maketheagents“fanout”,allowingthemtovisitadiverse
where the first group is instantiated with a reward func- set of states during the initial episodes, ultimately leading
tionwheres ∗ ∼ N(−20,0.32)andthesecondgroupwith tomoresuccessfulpolicies.
s ∼N(20,0.32). Thesetofstatesrepresentingthecause
∗
is generated by uniformly sampling 100 states in the in- 4.3.PendulumSwing-Up
terval [−25,25] and the corresponding effect is generated
usingthesameprocedureasthepreviousexperiment. Fig- Inthisnextexperimentweextendouralgorithmtotheclas-
ure 3(b) depicts the extracted model parameters and a fit- sicalpendulumswing-uptasksubjectedtotheslightaddi-
tedGMMclustering,showingclearseparationbetweenthe tionofaconstantwindstrength. Wecreatefourgroupsof
twoclusters. 20MDPswherethefirstgroupisinstantiatedwithawind
strength sampled from N(−4,0.12), the second group
sdraweR
)detnuocsidnU(
latoT
sdraweR
)detnuocsidnU(
latoT
etatS
etatS
etatS
etatSCausalCoordinatedConcurrentReinforcementLearning
0 O G Inlu dor ibs va idl ual 0 0 0 O G Inlu dor ibs va idl ual
250 Seed sampling 250 250 250 Seed sampling
500 500 500 500
750 750 750 750
1000 1000 1000 1000
1250 1250 Ours 1250 Ours 1250
1500 1500 G I Sn el do eib v da i d sl au mal pling 1500 G I Sn el do eib v da i d sl au mal pling 1500
0 20 40 60 80 100 120 140 1750 0 20 40 60 80 100 120 140 1750 0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140
Episode Episode Episode Episode
(a)N(−4,0.12) (b)N(−1.5,0.12) (c)N(1.5,0.12) (d)N(4,0.12)
Figure5: Acomparisonbetweenourmodelvs. threebaselinesforwindstrengthssampledfromamultimodalGMMon
thewindypendulumtask. ShadedregionrepresentsoneSDofuncertaintyfrom20sampledenvironments.
175 Ours 140 Ours
Global 140 Global
150 Individual 120 Individual
Seed sampling 120 Seed sampling
125 100
100
100 80 80
75 60 60
50 40 40
Ours
25 20 Global 20
Individual
0 0 Seed sampling 0
0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140
Episode Episode Episode
(a)N(7.82,0.12) (b)N(11.82,0.12) (c)N(15.82,0.12)
Figure6: Acomparisonbetweenourmodelvs. threebaselinesforgravitystrengthssampledfromatrimodalGMMonthe
cart-poleswing-uptask. ShadedregionrepresentsoneSDofuncertaintyfrom20sampledenvironments.
from N(−1.5,0.12), the third group from N(1.5,0.12) thepositionandvelocityofthecartandθandθ˙are,respec-
andfinally,thelastgroupfromN(4,0.12). Thestatespace tively, the angle and angular velocity of the pole. The re-
ofthetaskisgivenbys∈[cos(θ),sin(θ),θ˙)]whereθ ∈R mainderoftheexperimentalsetupissimilartotheprevious
and θ˙ ∈ [−8,8] are the angle and angular velocity of the experiment.Figure3(d)isahistogramoftheextractedhid-
pendulum, respectively. 100states areuniformly sampled denmodelparametersalongwithafittedGMMclustering
fromthestatespaceandtheircorrespondingnextstateare andtheresultinglearningcurvesaregiveninFigure6.
evaluatedusingarandompolicyandtheangularvelocities
of states and next states are taken to be the cause and ef-
5.Conclusion
fect,respectively.Figure3(c)depictsahistogramoftheex-
tractedhiddenmodelparametersalongwithafittedGMM In this paper, we address 1) model identification for non-
clustering and suggests that our algorithm is able to ex- identical MDPs 2) data-sharing for policy learning and
tractandseparateinlatentspace,thevariousdatagenerat- 3) action selection for coordinated exploration, under het-
ingmechanismscorrespondingtodifferentwindstrengths. erogeneous environments for CRL. We propose an algo-
Nextweexaminetheperformanceofourdata-sharingstrat- rithmic framework inspired by the recent work on ANM-
egy against a global sharing baseline, a no sharing base- MMs where model identification is performed via inde-
lineandaseedsamplingbaselineandtheresultinglearning pendence enforcement on latent space. We apply GMM
curvesaresummarizedinFigure5. clusteringtoidentifythesourcesofdatageneratingmech-
anismsaswellastoproduceasoftsimilaritymeasurethat
4.4.Cart-PoleSwing-Up is used to inform a data-sharing strategy. Finally we pro-
pound a sampling-based coordinated exploration heuristic
Finally, wetestouralgorithmoncart-poleswing-uptasks
that achieves diverse state visitations for a group of con-
with varying gravity strengths. We create three groups
currentagents. ComparisonsandresultsonanARtask, a
of 20 MDPs where the first group is instantiated with
sparse AR task and two classical swing-up tasks demon-
a gravity strength sampled from N(7.82,0.12), the sec-
strate the efficacy of our method. For future work, we
ond group from N(11.82,0.12) and the last group from
planonextendingourcausal-basedmodelusingvariational
N(15.82,0.12). The state space of the task is given by
Bayesmethods(Kingma&Welling,2014).
s∈[x,x˙,cos(θ),sin(θ),θ˙]wherexandx˙ are,respectively,
sdraweR
)detnuocsidnU(
latoT
sdraweR
)detnuocsidnU(
latoT
sdraweR
)detnuocsidnU(
latoT
sdraweR
)detnuocsidnU(
latoT
sdraweR
)detnuocsidnU(
latoT
sdraweR
)detnuocsidnU(
latoT
sdraweR
)detnuocsidnU(
latoTCausalCoordinatedConcurrentReinforcementLearning
References Theor., 56(10):5168–5194, October 2010. ISSN 0018-
9448. doi: 10.1109/TIT.2010.2060095. URL http:
Bareinboim,E.andPearl,J. Causalinferenceandthedata-
//dx.doi.org/10.1109/TIT.2010.2060095.
fusionproblem.ProceedingsoftheNationalAcademyof
Sciences, 113(27):7345–7352, 2016. ISSN 0027-8424. Kingma,D.P.andWelling,M. Auto-encodingvariational
doi: 10.1073/pnas.1510507113. URLhttps://www. bayes. InICLR,2014.
pnas.org/content/113/27/7345.
Lawrence, N. Probabilistic non-linear principal compo-
Dempster, A. P., Laird, N. M., and Rubin, D. B. Maxi- nentanalysiswithgaussianprocesslatentvariablemod-
mum likelihood from incomplete data via the em algo- els. J. Mach. Learn. Res., 6:1783–1816, December
rithm. JOURNAL OF THE ROYAL STATISTICAL SO- 2005. ISSN 1532-4435. URL http://dl.acm.
CIETY,SERIESB,39(1):1–38,1977. org/citation.cfm?id=1046920.1194904.
Dimakopoulou, M. and Roy, B. V. Coordinated explo- Lawrence,N.D.andQuin˜oneroCandela,J. Localdistance
ration in concurrent reinforcement learning. CoRR, preservationinthegp-lvmthroughbackconstraints. In
abs/1802.01282, 2018. URL http://arxiv.org/ Proceedings of the 23rd International Conference on
abs/1802.01282. MachineLearning,ICML’06,pp.513–520,NewYork,
NY, USA, 2006. ACM. ISBN 1-59593-383-2. doi:
Dimakopoulou, M., Osband, I., and Roy, B. V. Scal-
10.1145/1143844.1143909.URLhttp://doi.acm.
able coordinated exploration in concurrent reinforce-
org/10.1145/1143844.1143909.
ment learning. CoRR, abs/1805.08948, 2018. URL
http://arxiv.org/abs/1805.08948. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continu-
Gretton, A., Bousquet, O., Smola, A., and Scho¨lkopf,
ous control with deep reinforcement learning. CoRR,
B. Measuring statistical dependence with hilbert-
abs/1509.02971, 2015. URL http://arxiv.org/
schmidtnorms. InProceedingsofthe16thInternational
abs/1509.02971.
Conference on Algorithmic Learning Theory, ALT’05,
pp. 63–77, Berlin, Heidelberg, 2005. Springer-Verlag. Møller, M. Moller, m.f.: A scaled conjugate gradient al-
ISBN 3-540-29242-X, 978-3-540-29242-5. doi: 10. gorithmforfastsupervisedlearning.neuralnetworks6,
1007/11564089 7.URLhttp://dx.doi.org/10. 525-533. Neural Networks, 6:525–533, 12 1993. doi:
1007/11564089_7. 10.1016/S0893-6080(05)80056-5.
Gu, S., Holly, E., Lillicrap, T. P., and Levine, S. Deep Rosenbaum, P. R. and Rubin, D. B. The central role of
reinforcementlearningforroboticmanipulation. CoRR, the propensity score in observational studies for causal
abs/1610.00633, 2016. URL http://arxiv.org/ effects. Biometrika,70(1):41–55,041983. ISSN0006-
abs/1610.00633. 3444. doi: 10.1093/biomet/70.1.41. URL https://
doi.org/10.1093/biomet/70.1.41.
Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J.,
and Scho¨lkopf, B. Nonlinear causal discovery Shimizu, S., Hoyer, P. O., Hyva¨rinen, A., and Kerminen,
with additive noise models. In Koller, D., Schu- A. A linear non-gaussian acyclic model for causal dis-
urmans, D., Bengio, Y., and Bottou, L. (eds.), covery. J. Mach. Learn. Res., 7:2003–2030, Decem-
Advances in Neural Information Processing Sys- ber2006. ISSN1532-4435. URLhttp://dl.acm.
tems 21, pp. 689–696. Curran Associates, Inc., org/citation.cfm?id=1248547.1248619.
2009. URL http://papers.nips.cc/paper/
3548-nonlinear-causal-discovery-with-addiStilvievr,eD-n.,oNieswen-hmamod,eLl.,sB.arker, D., Weller, S., and Mc-
pdf. Fall, J. Concurrent reinforcement learning from cus-
tomer interactions. In Dasgupta, S. and McAllester,
Hu, S., Chen, Z., Nia, V. P., Chan, L., and Geng, Y. D. (eds.), Proceedings of the 30th International Con-
Causalinferenceandmechanismclusteringofamixture ference on Machine Learning, volume 28 of Pro-
of additive noise models. In Proceedings of the 32Nd ceedings of Machine Learning Research, pp. 924–
International Conference on Neural Information Pro- 932, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.
cessing Systems, NIPS’18, pp. 5212–5222, USA, 2018. URL http://proceedings.mlr.press/v28/
CurranAssociatesInc. URLhttp://dl.acm.org/ silver13.html.
citation.cfm?id=3327345.3327427.
Strens, M. J. A. A bayesian framework for reinforce-
Janzing, D. and Scho¨lkopf, B. Causal inference using ment learning. In Proceedings of the Seventeenth In-
the algorithmic markov condition. IEEE Trans. Inf. ternational Conference on Machine Learning, ICMLCausalCoordinatedConcurrentReinforcementLearning
’00,pp.943–950,SanFrancisco,CA,USA,2000.Mor-
gan Kaufmann Publishers Inc. ISBN 1-55860-707-
2. URL http://dl.acm.org/citation.cfm?
id=645529.658114.
Thompson, W. R. On the likelihood that one un-
known probability exceeds another in view of the ev-
idence of two samples. Biometrika, 25(3-4):285–294,
12 1933. ISSN 0006-3444. doi: 10.1093/biomet/
25.3-4.285. URL https://doi.org/10.1093/
biomet/25.3-4.285.
Zhang, K. and Hyva¨rinen, A. On the identifiability of
the post-nonlinear causal model. In Proceedings of
the Twenty-Fifth Conference on Uncertainty in Arti-
ficial Intelligence, UAI ’09, pp. 647–655, Arlington,
Virginia, United States, 2009. AUAI Press. ISBN
978-0-9749039-5-8. URL http://dl.acm.org/
citation.cfm?id=1795114.1795190.