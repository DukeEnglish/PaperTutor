SpeechComposer:
Unifying Multiple Speech Tasks with Prompt Composition
YihanWu2,1∗, SoumiMaiti1, YifanPeng1, WangyouZhang3,1, ChendaLi3,1, Yuyue
Wang2, XihuaWang2, ShinjiWatanabe1, RuihuaSong2
1CarnegieMellonUniversity,USA
2RenminUniversityofChina,China
3ShanghaiJiaoTongUniversity,China
Abstract Output data Output data
Recentadvancementsinlanguagemodelshavesig- Speech language model SpeechComposer
nificantlyenhancedperformanceinmultiplespeech-
Voice conversion
relatedtasks. Existingspeechlanguagemodelstypi- SpeechLM TTS ASR TextLM SpeechLM TTS ASR TextLM
…
callyutilizetask-dependentprompttokenstounify Speech Speech Voice Speech Speech editing Speech translation
various speech tasks in a single model. However, enhancement editing conversiontranslation Speech enhancement
thisdesignomitstheintrinsicconnectionsbetween Input datafor different tasks. (Represented Input datafor different tasks. (Represented
differentspeechtasks,whichcanpotentiallyboost by N task-dependent prompt tokens) by five uniform primary prompt tokens)
(a) Previous unified speech language models (b) SpeechComposer
theperformanceofeachtask. Inthiswork,wepro-
poseanoveldecoder-onlyspeechlanguagemodel, Figure1:ThecomparisonofSpeechComposerwithothermulti-task
SpeechComposer, that can unify common speech speech models when performing N tasks. (a) shows a cascaded
tasks by composing a fixed set of prompt tokens. modelthatrequiresdifferentsub-modelsforeachtask.(b)identifies
Builtuponfourprimarytasks—speechsynthesis, previous works using N task-dependent prompt tokens for each
speechrecognition,speechlanguagemodeling,and task. (c)showsthatourproposedSpeechComposerusesaunified
architectureandcomposesprimarytaskstomoretaskswithonlyfive
text language modeling — SpeechComposer can
uniformprompttokens.
easilyextendtomorespeechtasksviacompositions
ofwell-designedprompttokens,likevoiceconver-
sion and speech enhancement. The unification of
ingthespeechsignalintoadiscreterepresentation[Baevski
prompttokensalsomakesitpossibleforknowledge et al., 2020; Hsu et al., 2021] and modeling it with lan-
sharingamongdifferentspeechtasksinamorestruc- guage models [Wang et al., 2023a; Wang et al., 2023c;
turedmanner.Experimentalresultsdemonstratethat Huang et al., 2023; Borsos et al., 2023; Lakhotia et al.,
ourproposedSpeechComposercanimprovetheper- 2021]. Taking advantage of large-scale speech data, some
formanceofbothprimarytasksandcompositetasks, recent studies [Wang et al., 2023c; Zhang et al., 2023b;
showingtheeffectivenessofthesharedpromptto-
Dongetal.,2023]extendtheapplicationofspeechlanguage
kens. Remarkably,theunifieddecoder-onlymodel modelstomultipletasks,integratingmulti-tasklearningintoa
achievesacomparableandevenbetterperformance unifiedmodel(asshowninFigure1(a)). Comparedwiththe
thanthebaselineswhichareexpertmodelsdesigned cascadedmodelthatrequiresseparatemodelsforeverydiffer-
forsingletasks. enttask,languagemodelbasedapproachesintegratediverse
speechtasksintoagenerativelanguagemodel,andpotentially
tackledifferentspeechtaskswithasinglemodel,including
1 Introduction speech synthesis, voice conversion, noise suppression, and
speechediting.However,thesespeechlanguagemodelsutilize
Recently, large language models show remarkable perfor-
task-dependentprompttokensforvariousspeechtasks. This
mance across diverse domains, including natural language
processing[Zhangetal.,2022;Brownetal.,2020;Touvron designignorestheinherentinterrelationshipsamongdiverse
et al., 2023], vision [Singh et al., 2022], and multi-modal speechtasks,whichhavethepotentialtosignificantlyenhance
tasks[Lietal.,2023;Alayracetal.,2022].Predominantlyem- theperformanceofeachindividualtask.Takethevoiceconver-
siontaskasanexample,thegoalistoretainthelinguisticinfor-
ployinganautoregressivemannerandthedecoder-onlyarchi-
mationinspeechwhiletransformingnon-linguisticelements.
tecture,thesemodelstransformvarioustasksintogenerative
ThisrequiresthemodeltohavecapabilitieslikeASRmodels,
ones. Despitethesimplicityoftheirtrainingmethodologies,
wherelinguisticinformationisextracted,aswellasabilities
theyexhibitexceptionalcapabilities.
similartoTTSmodels,wherespeechisgeneratedforatarget
Thecurrentworksonspeechlanguagemodelsmainlytreat
speaker. Someofthepreviousvoiceconversionmodelsare
speechtasksasconditionalgenerationtasks.Itinvolvesencod-
builtusingcascadedASRandTTSmodel[Huangetal.,2020;
∗WorkdoneduringthevisitatCMU Mertes et al., 2021; Liao et al., 2022]. These previous ap-
4202
naJ
13
]LC.sc[
1v54081.1042:viXraTask Taskcomposition Dataformat Evaluationmetrics
TextLM — 〈generate-text〉,Y PPL
SpeechLM — 〈generate-speech〉,D PPL
Primarytasks
ASR — 〈start-speech〉,D,〈generate-text〉,Y WER
〈start-text〉,Y,〈enroll-speech〉,Denroll,
TTS — CER,MOSNet
〈generate-speech〉,Dtgt
〈start-speech〉,Dsrc,〈generate-text〉,Y, MOSNet,CER,
VC ASR+TTS
Compositetasks 〈enroll-speech〉,Denroll,〈generate-speech〉,Dtgt Sim-speech,TER
〈start-speech〉,Dnoise,〈generate-text〉,Y,
SE ASR+TTS DNSMOS,WER
〈enroll-speech〉,Denroll,〈generate-speech〉,Dclean
Table1:SpeechComposer’sdataformatfordifferenttasks(primarytasksandcompositetasks)inthetrainingstageandinferencestage.Here,
Y andDrefertotheinputtextsequenceanddiscretespeechtokensseparately.Theinputformatforspeechenhancementandvoiceconversion
isthesame,butmanipulatingdifferentenrollmentspeechDenrolltocontrolthegeneratedspeech.Forthespeechenhancementtask,Denrollisa
cleanspeechsamplefromthesamespeakerduringtraining,whileinvoiceconversion,Denrollisaspeechutterancefromthetargetspeaker.
proaches,withtheirseparatehandlingoftasks,donotcombine ference. TheexperimentalresultsshowthatSpeechCom-
theseoverlappingskills,limitingtheoverallperformanceand poserperformswellbothinprimarytasksandcomposite
efficiencyofthemodelintaskslikevoiceconversion. Consid- tasks. Also, SpeechComposer is flexible enough to be
eringthislimitation,someworkssuchasVoxtLM[Maitietal., extended to new tasks in zero-shot scenarios. Audio
2023]takesadvantageofsharedprompttokensandachieves samplesareavailableatdemopage1.
goodperformanceinlimitedtasks.
In this work, we further exploit the capabilities of large 2 RelatedWork
language models across a wider range of speech tasks, and
2.1 SpeechLanguageModels
facilitate the straightforward integration of more tasks. As
showninFigure1(b),weintroduceSpeechComposer,aspeech Transformer-based autoregressive language models (LM)
language model that unifies common speech tasks by com- show remarkable capacity in speech processing tasks. Au-
posingafixedsetofprimarytasks. Weconsiderthatspeech dioLM[Borsosetal.,2023]explorestheaudiogenerationtask
taskscanbeseenascompositesoffourprimaryspeechtasks: asalanguagemodelingtask. Otherworks[Agostinellietal.,
speechlanguagemodeling(SpeechLM),textlanguagemodel- 2023;Wangetal.,2023a]furtherusesimilarautoregressive
ing(TextLM),speechsynthesis(TTS),andspeechrecognition architectureinotheraudiorelatedtasks,includingmusicgen-
(ASR).Bycomposingthefixedsetofuniformprompttokens, erationandzero-shotTTS.Furthermore,someworksexplore
themodelcanshareknowledgebetweenbothprimarytasks modelperformanceinmulti-taskscenarios[Huangetal.,2023;
andcompositetasks. Guidedbydifferentenrollmentspeech, Dongetal.,2023;Wangetal.,2023c;Zhangetal.,2023b].
SpeechComposercanbeextendedtomanyspeechtasks,such Make-a-Voice[Huangetal.,2023]exploresbothspeechsyn-
asvoiceconversion(VC),speechenhancement(SE),speech thesisandvoiceconversiontasksinspeechlanguagemodel-
editing,speechtranslation,andexpressivespeechsynthesis. ing. SpeechX[Wangetal.,2023c]exploresspeechsynthesis,
Inthiswork,weparticularlyfocusonVCandSEtodemon- noisesuppression,andspeecheditingtasksbydesigningtask-
stratetheeffectivenessandscalabilityofSpeechComposerin dependentprompts. VioLA[Wangetal.,2023b]andVALLE-
handlingprimarytasksandcompositetasks. VCisdefined X [Zhang et al., 2023b] integrate task IDs or language IDs
asacompositionofASRandTTS.Guidedbyanenrollment tofurtherperformspeechtranslationtasks. Thesemulti-task
speechofthetargetspeaker,SpeechComposerconvertsspeech speechlanguagemodelsemploytask-specificprompttokens
ofthesourcespeakerintothoseofthetargetspeakerthrough orpatternsfordifferenttasks,whichignorestheinherentcon-
ASRandTTStasks. SEisdefinedasthesamecomposition, nectionsbetweenthesetrainingtasks.Furthermore,forcertain
butwithcleanspeechastheenrollment. Themaincontribu- extended speech tasks like voice conversion, where paired
tionsofthisworkaresummarizedasfollows: training data is scarce, training a language model becomes
• WeproposeSpeechComposer,aunifiedlanguagemodel evenmorechallenging.Inourwork,insteadofregardingthese
thatunifiescommonspeechtasksandcanbeeasilyex- tasksasisolatedcomponents,weviewspeechtasksascombi-
tendedtomoretasksthroughtaskcompositions. Speech- nationsofprimarytasks. Thisapproachallowsustoleverage
Composercanhandlecompositetaskswithoutchanging theknowledgefromprimarytaskswithabundanttrainingdata,
themodelstructureandaddingadditionalprompttokens facilitatingeasiertasktransferandachievingbetterresults.
forspecifictasks.
• Whenexpandingtonewtasks,SpeechComposerlever- 2.2 CompositeSpeechTaskFrameworks
agesthecapabilitiesofprimarytasksthroughthecom- Manypreviousstudiesemploycascadedmodelstoaccomplish
positionofprompts,allowingformutualenhancement speechrelatedtasks. Forexample,someworkslinkspeech
betweentheprimaryandcompositetasks. recognitionandtext-to-speechforspeechenhancementand
• To ensure reproducibility, we use publicly available
datasetsandanopen-sourcetoolkitfortrainingandin- 1https://speechcomposer.github.io/samples/Generated speech
Task Prompts:
<start-speech>
Speech token
<generate-speech> decoder
<generate-text>
<enroll-speech>
SpeechComposer
Source speech tokens Text tokens Enrollment speech tokens
ASR +
Voice conversion/speech enhancement
Figure2:TheoverallarchitectureofSpeechComposer. Inthispicture,weusethetaskofvoiceconversionandspeechenhancementasan
example.Itdemonstrateshowcompositetaskscanbeaccomplishedthroughthecompositionofprimarytasksandtheuseofprompttokens.
Speechenhancementcanalsobecomposedinasimilarmannerwithdifferentenrollmentspeechtokens.
voice conversion tasks [Huang et al., 2020; Mertes et al., sequencefromavocabularyV . Wemergetextandspeechin
dst
2021; Liao et al., 2022; Chang et al., 2019]. For speech aunifiedvocabularyV =V ∪V ,andmodelthemin
Comp txt dst
translation,previousworks[Matusovetal.,2005]combine thesamemanner. Then,wecanmodeltheprobabilityofany
ASRwithmachinetranslation. Additionally,integratingASR, sequenceofspeechortexttokensZ = (z ∈ V|i = 1,...,t)
i
machine translation, and TTS has been used for advanced as p(Z) = (cid:81)t p(z |z ,...z ), where Z can be discrete
i=1 i 1 i−1
speechtranslationtasks[Agüeroetal.,2006]. However,these speechtokensD,texttokensY oracombinationofY andD.
approachesusuallyrelyonseparatemodelsforvarioussub- Tolearntheinterrelationshipbetweendifferenttasks,we
tasksandoptimizetheminstagesindependently. Compared incorporate new tasks through the method of task composi-
withpreviousworks,ourmethodemploysaunifiedframework tion.Beingdifferentfrompreviousworks[Wangetal.,2023b;
toaccomplishvarioustasks,allowingformutualenhancement Wang et al., 2023c; Zhang et al., 2023b], we replace task-
amongcascadedtasksduringthetrainingprocess. specificprompttokenswithcompositeonesbasedonafixed
Also,somepriorstudiesfocusontrainingmodelsfordual setofpromptsfordifferenttasks. First,asshownintheupper
tasksorutilizingauxiliarytaskstoenhancetheperformance partofTable1,weidentifyfourprimarytasks: textlanguage
ofprimarytasks. Chenetal.[2015]exploretherelationship modeling(TextLM),speech-languagemodeling(SpeechLM),
between speech enhancement and automatic speech recog- speechrecognition(ASR),andspeechsynthesis(TTS).Cor-
nition. Ren et al. [2019] and Hori et al. [2019] investigate respondingtotheseprimarytasks,thefollowingfiveprimary
the interplay between ASR and TTS. Zhang et al. [2019a] prompttokensareaddedtotheunifiedvocabularyV to
Comp
revealthatvoiceconversionperformancecouldbeenhanced distinguishvarioustypesofinformation.
throughtextsupervision. Furthermore,otherworkslinkvoice
• 〈start-text〉and〈start-speech〉representthestart
conversionwithTTS[Zhangetal.,2019b]. However,these
ofatextordiscretespeechsequence.
studiesgenerallydesignmodelsforspecificspeechprocessing
tasks,limitingtheirapplicabilitytoabroaderrangeoftasks. • 〈generate-text〉 and 〈generate-speech〉 indicate
Comparedwiththeseworks, ourproposedmodelaltersthe thedatamodalitytobegeneratedbythemodel.
compositionoftasksbychangingtheformatoftheinputdata,
• 〈enroll-speech〉 indicates enrollment speech to pro-
withoutredesigningthemodelforspecifictasks.
vide context information for future token generation.
Here, the goal of the enrollment speech is to provide
3 ProposedMethod
additionalcontextinformation,guidingittogeneratethe
3.1 DataFormat desiredtokensfordifferenttasks. Morespecifically,we
firstemploythespeechtokenizertoencodecontinuous
In the proposed SpeechComposer, various speech tasks are
enrollmentspeechintodiscretetokens. Thesetokensare
formulatedasaunifiedlanguagemodelingtask. Tounifyall
thenprefixedwiththe〈enroll-speech〉tosignifythe
tasksandcorrespondingdata,weconvertallspeechsignals
typeoftheinformationthatfollows.
intodiscretetokens,allowingthemtobemodeledinasimilar
mannertothetokensintextprocessing. SupposeY =(y ∈ Basedontheseprimarytasksandprompttokens,wedefine
i
V |i = 1,...,t )isatextsequencefromavocabularyV , compositetasksbycomposingthem,asshowninthelower
txt txt txt
andD =(d ∈V |i=1,...,t )isadiscretespeechtoken partofTable1. Inthiswork,weexpandfourprimarytasksto
i dst dstModel Pretrained Layers Hiddensize Heads Params Section 3.1, we train our model on both primary tasks and
SpeechComposer-Base (cid:37) 12 768 12 125M compositetasks. Specifically,foracompositetaskcomposed
SpeechComposer-Pretrained (cid:33) 12 768 12 125M ofN primarytasks, itsoutputisdeterminedbytheoutputs
SpeechComposer-Large (cid:33) 24 1024 16 350M
of these N subtasks autoregressively. Therefore, it can be
formulatedas:
Table2:DetailsofSpeechComposermodelvariants.
p(Zcomp|Zcond,Zenroll)
voiceconversionandspeechenhancement. However,itshould (cid:89)N (1)
be emphasized that SpeechComposer is flexible enough to = p(Ztaskn|Ztask1,··· ,Ztaskn−1;Zcond;Zenroll),
be expanded to other speech tasks like speech translation, n=1
speechediting,andspeechstyletransfer. Table1showsthat whereZcomp referstothefinalpredictionforthecomposite
voice conversion is defined as a composition of ASR and task,Ztask1,...,Ztaskn−1 aretheoutputsforpreviousn−1pri-
TTS.Guidedbytheenrollmentspeechofthetargetspeaker marytasks. Besides,Zcondreferstotheconditioninformation,
with〈enroll-speech〉,SpeechComposerconvertsdiscrete
whichistheinitialinput,andZenroll referstotheenrollment
speechtokensofthesourcespeakerintothoseofthetarget
sequence, which can be both speech and text. Specifically,
speakerthroughASRandTTStasks. Similarly, thetaskof forvoiceconversion,Zenrollisanygivenspeechofthetarget
speechenhancementoperatesonthesameprinciple. Using
speakerrepresentedasdiscretetokens. Forspeechenhance-
clean audio as the enrollment speech for guidance, Speech- menttasks,Zenrollisthediscretetokensofthecorresponding
ComposerutilizesASRandTTStogenerateclean,enhanced
speaker’scleanspeech.
speech. It is important to note that for different tasks, the
Inthetrainingstage,weintegrateN primarytasksintoa
enrollmentspeechhasaspecificfunctiondependingonthe
unifiedsequence. Thiscompositemethodtraditionallycom-
task,butwealwaysusethesame〈enroll-speech〉tokento
putes losses for all predicted components, which has been
maintainaconsistentdataformat.WefurthercompareSpeech-
observedtoinducetraininginstability. Tocounteractthis,we
Composerwithotherbaselinemodelstoshowtheefficacyof
introducearandomizedsamplingstrategyforlosscompu-
ourdesigneddataformatinSection6relatedtoRQ1.
tation. TheSpeechComposergeneratespredictionsfortheN
primarytasks,representedasZtask1,...,ZtaskN,andthepredic-
3.2 Modelarchitectures pred pred
tionforthecompositetaskisdenotedasZcomp. Inessence,
Aunifiedframeworkisdesignedtoprovidearchitecturecom- pred
weallocateaseriesofhyper-parametersq ,q ,··· ,q ,q
patibilityacrossdifferenttaskswithauniformdataformat. As 1 2 N global
torandomlyselecttasksforlosscomputation. Eachq corre-
showninFigure2,SpeechComposerconsistsofthreemain j
spondstothelikelihoodthatthelossforthej-thpredictedpart
components: speechtokenizer,decoder-onlylanguagemodel
(j ∈{1,··· ,N})ortheglobalsequence(j =global)willbe
andspeechtokendecoder. Underthisarchitecture,LLMcan
computedinagiventrainingstep,withtheconstraintthatthe
perceive both text and speech inputs and generates desired
sumofallq valuesequals1. Thisensuresaprobabilistically
outputsformultipletasks. j
balancedlosscalculationacrossalltaskpredictionsandthe
SpeechComposer. SpeechComposerconsistsofanembed-
overallsequence,allowingforamorestabletrainingprocess
dinglayerandaseriesoftransformer[Vaswanietal.,2017]
bypreventingdisproportionateweightingonanysingletask’s
decoder layers. First, the embedding layer E maps the dis-
loss. Wefurtherdesignexperimentstoverifytheeffectiveness
cretetokenindexsequenceZ (asshowninSection3.1)into
oftherandomizedsamplingstrategyinSection6.5.
anF-dimensionalfeaturespace. AsshowninTable2,weset
Intheinferencestage,weusebeamsearchtopredictdis-
differentLtransformerdecoderlayersandH attentionheads
crete tokens in an autoregressive manner. Prediction of the
dependingonthevariantsofSpeechComposer.
desiredtaskcanbeexpressedas:
Speechtokenizer. Weusethespeechtokenizertoconvert
continuousspeechtodiscretetokens, sothatwecanmodel Z pc ro em dp ←p(·|Zcond,Z pta rs ek d1,...,Z pta rs ek dN−1,Zenroll) (2)
them in a same manner with text tokens. Therefore, it is
feasible to perform speech tasks using the discrete speech where Z pc ro em dp is the prediction of the desired task and Z pta rs ek dj
tokensasinputorwiththecorrespondingtext. Wewilldiscuss refers to the prediction of the j-th primary task that forms
theeffectivenessofextratextinputinSection6.5. partofthecompositeoveralltask. Forexample,asshownin
Table1,forASR,theconditionisspeechtokensD,whilethe
Speechtokendecoder. AsshowninFigure2,weemploy
predictionistherecognizedtextutteranceY. ForTTS,the
a pre-trained discrete token based vocoder to convert pre-
conditionisthetextutteranceY andspeechtokensfromthe
dicted discrete tokens to continuous speech. Here, we use
targetspeakerasenrollmentDenroll, andthediscretetokens
theHiFiGAN[Kongetal.,2020]asthearchitectureanduse
Dtgtcorrespondingtothetextutterancearepredicted. Asfor
x-vector[Snyderetal.,2018]asthespeakerinformation.
compositetaskslikespeechenhancement,itiscomposedof
3.3 TrainingandInference twoprimarytasks,i.e. N = 2. Themodelfirstpredictstext
Y = Ztask1 basedonthediscretetokensofthenoisyspeech
Themodelpredictsaprobabilitydistributionoverthetokens pred
inV byusingalinearlayerfollowedbysoftmax. Over- Dnoise and then uses the original condition information, the
Comp
all, SpeechComposer is trained as a language model in an predictedtextZ p1 red,alongwiththeenrollmentspeechDenroll
autoregressivemanner. Basedonthedataformatdefinedin asconditionstogeneratecleanspeechDclean =Zcomp.WetrainSpeechComposerusingfourprimarytasksandtwo Voice conversion. For voice conversion, we use parallel
compositetasks,takingadvantagesoftheinterrelationships datasets which have pairs of audio recordings where the
between primary tasks and composite tasks. In Section 6, samelinguisticcontentisspokenbydifferentspeakers. We
correspondingtoRQ2,wedesignexperimentswithavariety useVCTK[Veauxetal.,2017],CMU-Arctic[Kominekand
of training task numbers in Section 6.3. This allows us to Black,2004],VCC2018[Lorenzo-Truebaetal.,2018],VCC
comparehowdifferentcombinationsofprimaryandcompos- 2020 [Zhao et al., 2020]. The VCTK dataset contains 44
itetasksduringtrainingimpactthetaskperformanceatthe hoursofstudiorecordeddatafrom109speakers. Thesespeak-
inferencestage. Additionally,weinvestigatetheperformance ersreadthesameRainbowPassageandelicitationparagraph.
ofSpeechComposerinazero-shotscenario. Inthiscontext, CMU-Arctic contains about 3 hours of speech from three
zero-shotisdefinedastrainingSpeechComposeronprimary speakers. VCC2018andVCC2020comefromVoiceconver-
tasksandasubsetofcompositetasks,andthentestingiton sionchallenges2018and2020respectively. Weobtainabout
othercompositetasksthatareunseenduringtraining. Since 50hoursofparallelspeechdataintotal.
no extra prompt tokens are added when expanding to new
Speechenhancement Forthespeechenhancementtask,we
composites,wefindSpeechComposerissufficientlyflexible useVoiceBank+DEMANDdataset[Valentini-Botinhaoetal.,
toextendtounseencompositetasks. Wewillfurtherdiscuss 2016], which is a typical SE database with clean and noisy
experimentalresultsinSection6.4correspondingtoRQ3.
parallel speech. It contains about 8.8 hours of speech data
from28speakers. Therearearound400sentencesavailable
4 ExperimentSetup
fromeachspeaker.
4.1 Modelconfigurations
5 Evaluationmetrics
To train the sub-word model, we use all paired text-speech
datasets. For the speech tokenizer, we use the pretrained Toensureconsistencyandreproducibility,weemployobjec-
English HuBERT-Base model [Hsu et al., 2021] and use tivemetricsforeachtask.
k-means clustering to discretize the 6th-layer embeddings.
Word Error Rate (WER)/Character Error Rate (CER)
Here, we set k = 1000 for k-means. Also, we convert the
WeuseWERandCERtoassesstheperformanceoftheASR
samplingrateofallspeechdatato16kHz. AsshowninTa-
model and the intelligibility of the generated speech to the
ble2,wetrainthreevariantsofSpeechComposerwithdistinct
giventranscription. Fortheevaluationofgeneratedspeech,
configurations: SpeechComposer-base, SpeechComposer- we use Whisper Large v22 [Radford et al., 2023] model to
pretrained,andSpeechComposer-large. Priorresearch[Has-
transcribethegeneratedspeechandcalculatetheWERscore.
sidetal.,2023]indicatesthatinitializingaSpeechLMwith
Speakersimilarityscore. Thespeakersimilarityscoreis
a pretrained TextLM leads to enhanced performance and
usedtoevaluatehowwellthegeneratedspeechmatchedthe
quickerconvergence. Inspiredbythismethodology,forboth
specificcharacteristicsofthespeaker. Thisscoreisderived
SpeechComposer-pretrainedandSpeechComposer-large,we
fromthecosinesimilaritybetweenthespeakerembeddings
initializetheSpeechComposerweightsusingthepretrained
TextLMOPT[Zhangetal.,2022]andconstructtheembedding of both the generated and the target speech signals. Here,
we calculate the x-vector as the speaker embedding. The
tablefromthegroundup. Weuse4A100GPUsfortraining
small/mediummodelswiththeAdamoptimizer[Kingmaand speakersimilarityscore(Sim-speech)metricisappliedintasks
Ba,2015]andawarmuplearningratescheduler. includingmulti-speakerTTSandvoiceconversion. Besides,
weusetheTER(tokenerrorrate)toevaluatethesimilarity
4.2 Datasets betweenthepredicteddiscretespeechtokensandtheactual
discretespeechtokens.
Weuseacombinationofspeech-only,text-only,pairedspeech-
text,andpairedspeech-textdatasetsforSpeechComposer’s MOSNet. Following [Maitietal.,2023],weuseautomatic
trainingandinference.Alldatasetsweusedarepubliccorpora. meanopinionscoreMOSNet[Loetal.,2019;Cooperetal.,
SpeechLM. Weusespeech-onlydatasetLibriLight[Kahn
2022]toevaluatethequalityofgeneratedspeech.
etal.,]forspeechlanguagemodelingtask. Librilightcontains DNSMOS. Following previous works, we use DNSMOS
morethan60Khoursofspeechdatafrom7,439speakers. (OVRL) [Reddy et al., 2022] to evaluate the model’s per-
TextLM. WeusethetextportionofLibriSpeech[Panayotov formance in the speech enhancement task. DNSMOS is a
etal.,]datasetasthetrainingdataforatextlanguagemodeling non-intrusive perceptual objective metric, which is used to
simulatethehumansubjectiveevaluationontheDNSblind
task. Itcontainsabout40Mtextutterances.
testset. Morespecifically,weleveragetheOVRLscorefrom
Speechrecognition. Weusepairedspeech-textdatasetLib- theDNSMOSP.835model3.
rispeech[Panayotovetal.,]foraspeechrecognitiontask. It
Perplexity. Perplexityisawidelyusedmetricinnaturallan-
contains960hoursofspeechdatawith281Kutterances.
guageprocessing,especiallyforevaluatinglanguagemodels.
Multi-speakerTTS. Forspeechsynthesis,weusetwomul-
Itmeasureshowwellalanguagemodelpredictsasample. We
tispeakerdatasets,LibriTTS[Zenetal.,2019]andHi-FiTTS.
useperplexitytoevaluatetheperformanceofspeechlanguage
LibriTTSisamulti-speakerEnglishcorpusofapproximately
modelingandtextlanguagemodeling.
585hoursofreadEnglishspeechfrom2456speakers. Hi-Fi
TTS contains about 292 hours of speech from 10 speakers 2https://huggingface.co/openai/whisper-large-v2
withatleast17hoursperspeaker. 3https://github.com/microsoft/DNS-Challenge/TextLM SpeechLM ASR TTS Voiceconversion Speechenhancement
Setting #params
PPL(↓) PPL(↓) WER(↓) CER(↓) MOSNet(↑) CER(↓) MOSNet(↑) WER(↓) DNSMOS(↑)
dst-ASR-Hubert VITS PPG-VC USES
Expertmodels — — —
4.2/10.8 7.7 4.20 6.9 4.29 4.2 3.10
VoxtLM-Base⋆ 125M 18.3 36.7 4.7/11.9 3.9 4.28 — — — —
VoxtLM-Large⋆ 350M 16.4 32.3 3.6/8.6 6.1 4.27 — — — —
SpeechComposer-Base 125M 18.7 35.6 7.4/15.7 3.2† 4.23 7.0 4.40† 8.3 3.08
SpeechComposer-Pretrained⋆ 125M 17.6† 34.7† 5.1/13.0 3.1† 4.24 7.8 4.36† 7.6 3.07
SpeechComposer-Large⋆ 350M 16.8 34.0 6.4/16.9 3.4† 4.26 6.7† 4.35† 7.6 3.08
Table 3: Experimental results comparing SpeechComposer with language model based baselines for different tasks (primary tasks and
compositetasks).ForASR,wereporttest-clean/test-otherresults.Here,⋆denotesinitializationwithOPT,and†signifiesthatSpeechComposer
achievesbetterresultscomparedwiththeVoxtLMbaselinesthathavethesamemodelsizesortheexpertmodels.
5.1 Evaluationmetrics RQ3: IsthetaskcompositiondesigninSpeechComposer
Toensureconsistencyandreproducibility,weemployobjec- robustenoughtogeneralizetonewtasksinazero-shotman-
tivemetricsforeachtask,asshowninTable1. ner?
Word Error Rate (WER) /Character Error Rate (CER)
6.1 Performanceonprimarytasks(RQ1)
WeuseWERandCERtoassesstheperformanceoftheASR
model and the intelligibility of the generated speech to the Table3showstheperformanceofSpeechComposerinvarious
giventranscription. Fortheevaluationofgeneratedspeech, taskscomparedtothebaselinemodels,includingbothspeech
we use Whisper Large v24 [Radford et al., 2023] model to languagemodels6andindividualexpertmodels. SpeechCom-
transcribethegeneratedspeech. poserachievescomparableorevenbetterperformancethan
speechlanguagemodelbaselinesthathavethesamemodel
Speaker similarity score. Speaker similarity scores mea-
sizesortheexpertmodels,especiallyintheTTStask. Based
surethematchofgeneratedspeechtothespeaker’scharacter-
on the performance of different models in primary tasks as
istics. Thisscoreisderivedfromthecosinesimilaritybetween
presentedinTable3,wehavethefollowingobservations:
thespeakerembeddingsofboththegeneratedandthetarget
speech. Here,wecalculatethex-vectorasthespeakerembed- Comparisonwithexpertmodels. InTTStasks,wecom-
ding. Thespeakersimilarityscore(Sim-speech)isappliedin pareSpeechComposerwithVITS[Kimetal.,2021],aparallel
multi-speakerTTSandVC.Besides,weusetheTER(token end-to-end TTS model that can generate high quality natu-
error rate) to evaluate the similarity between the predicted ral speech. we create a test set of 270 utterances from two
discretespeechtokensandtheactualdiscretespeechtokens. speakersfromtheLibriTTStest-clean7. ComparedtoVITS,
MOSNet. Following [Maitietal.,2023],weuseautomatic SpeechComposergetsasuperiorMOSNetscorethatshows
meanopinionscoreMOSNet[Loetal.,2019]toevaluatethe higherspeechquality. ItisnoteworthythatSpeechComposer-
Pretrainedsignificantlyoutperformsintermsofintelligibility,
qualityofthegeneratedspeechinTTSandVCtasks.
evidenced by a substantial reduction in the Character Error
DNSMOS. Following previous works, we use DNSMOS
Rate(CER)from7.7%to3.1%. ThoughSpeechComposeris
(OVRL)[Reddyetal.,2022]toevaluatethemodel’sperfor-
trainedwithalargerdatasetcomparedtoVITSandYourTTS,
manceinSEtasks. DNSMOSisanon-intrusiveperceptual
itshouldbementionedthatforTTSmodels,havingdiverse
objectivemetric,whichisusedtosimulatethehumansubjec-
training data with more noise and more speakers often de-
tiveevaluationonDNSblindtestsets. Morespecifically,we
grades rather than improves the performance [Maiti et al.,
leveragetheOVRLscorefromtheDNSMOSP.835model5.
2023]. AsfortheASRtasks,weusethesameexpertmodel
Perplexity. Perplexityisawidelyusedmetricinnaturallan- with VoxtLM, dst-ASR-Hubert [Chang et al., 2023]. Com-
guageprocessing,especiallyforevaluatinglanguagemodels. paringtheresultsofSpeechComposerwithdst-ASR-Hubert,
Itmeasureshowwellalanguagemodelpredictsasample. We SpeechComposerdoesnotachievebetterperformanceinASR
useperplexitytoevaluatetheperformanceofspeechlanguage tasks. Weassumethismightberelatedtothetrainingmethod
modelingandtextlanguagemodeling. ofthelanguagemodel,makingitmoresuitedforgenerative
tasksinspeech. Frompreviousworks[Maitietal.,2023],we
6 Experimentresults suggestthattheperformanceinASRtaskscouldpotentially
beimprovedbyincreasingtheASRtrainingdata.
Wefocusonthefollowingresearchquestions:
RQ1: Withtheunifieddataformat,canSpeechComposer Comparisonwithlanguagebasedmodels. Wefurthercom-
achievebetterperformanceforbothprimarytasksandcom- pareSpeechComposerwiththeunifiedspeechlanguagemodel
positetasks?
RQ2:Doesthenumberoftaskstrainedinthemodelandthe 6Due to the lack of open-source availability for related works
quantityoftrainingdatahaveanimpactontheperformance suchasSpeechX,VioLA,andMake-a-Voice,weareunabletocon-
ofprimaryandcompositetasks? duct a fair comparison with them. Therefore, in this context, we
mainly compare our work with the open-source project VoxtLM
4https://huggingface.co/openai/whisper-large-v2 (https://github.com/espnet/espnet/pull/5472).
5https://github.com/microsoft/DNS-Challenge/ 7Specifically,speakeridsare1089and1284.VoxtLM [Maiti et al., 2023]. Under similar parameter con- results. Wehavethefollowingobservations. (1)Increasing
ditions, SpeechComposer demonstrates much better perfor- thenumberoftrainingtasksandthecorrespondingvolume
mance in the TTS tasks. For example, SpeechComposer- oftrainingdatacanenhancetheperformanceofbothprimary
PretraineddemonstratesbetterintelligibilityintheTTStask, tasks(TTS)andcompositetasks(VCandSE).Forinstance,as
intermsofCERfrom3.9%to3.1%. Also,SpeechComposer- thenumberoftrainingtasksincreases,theCERmetricforthe
Largeachievessignificantimprovementinintelligibility, in TTStaskimprovesfrom3.9%to3.1%. ForVCandSEtasks,
terms of CER from 6.1% to 3.4% and comparable speech weobservethattrainingonlyontheVCorSEtaskwithrelated
quality. Besides,SpeechComposer-Pretrainedachievesbetter datasetsresultsinthemodel’sinabilitytoconverge. Thisis
performanceinTextLMandSpeechLMtasks. Thisdemon- attributedtotheinsufficienttrainingdatacomparedtothelarge
stratesthatforprimarytasks,SpeechComposerbenefitsfrom modelsize.Thisfurtheremphasizestheimportanceoftraining
aunifiedtrainingapproach,achievingbetterresultscompared datavolumeinlanguagemodeltraining. (2)Addingprimary
tolanguagemodelbasedbaselinesthathavethesamemodel tasks can improve the performance of composite tasks. As
sizes,particularlyingenerativetasks. showninTable4,introducingtheTTStaskaddsmoretraining
data, which enhances the speech quality of the VC and SE
Comparison of model size Furthermore, we compare
tasks. Additionally,theASRtasksignificantlyimprovesthe
SpeechComposer with different parameters and train-
intelligibilityofVCandSEintermsofCERandWERmet-
ing strategies. Comparing SpeechComposer-Base and
rics. Italsohighlightsthenecessityofutilizingprimarytasks
SpeechComposer-Pretrained, we observe that initializa-
whenexpandingtonewtasks. (3)Addingcompositetaskscan
tion with OPT can improve the performance of TextLM,
alsoimprovetheperformanceofprimarytasks. FromTable4,
SpeechLM, ASR, and TTS. What’s more, by comparing
wecanobservethatintroducingthespeechenhancementtask
SpeechComposer-LargewithSpeechComposer-Pretrained,we
significantlyenhancesthequalityandintelligibilityofthegen-
observethatalargermodelsizecanimprovetheperformance
eratedspeech,forbothTTSandVCtasks. Forexample,when
ofTextLM,SpeechLM,andASR.
trainedonlywiththeTTStask,theCERis3.9%. However,
6.2 PerformanceonCompositeTasks(RQ1) aftertrainingwithTTSandspeechenhancementtaskscom-
bined,theCERimprovedto3.4%. Voiceconversationtasks
ToanswerRQ1,wefurthercompareSpeechComposerwith
havesimilarresultstotheformer.
languagemodelbasedmodelsandexpertmodelsoncomposite
tasks,i.e.,voiceconversionandspeechenhancement.
6.4 ThePerformanceofZero-shotTransfer(RQ3)
Comparisonwithexpertmodels. IntheVCtask,wecom-
To answer RQ3, we experiment with zero-shot generation
pare SpeechComposer with an expert model PPC-VG [Liu
on tasks for which the model was not specifically trained.
et al., 2021], a voice conversion model based on phonetic
Here, we conduct evaluations on two composite tasks, SE
posterior-gram(PPG).WesplitthetestsetfromtheVCTK
andVC,comparingtheirperformancewithscenarioswhere
corpus,whichincludesconversionsfromseenspeakerstoseen
trainingdataisavailable. Asindicatedinthelasttworowsof
speakers. Eachtestsetcontains350pairedspeechsamples.
Table4, SpeechComposer-BaseisnottrainedonSEorVC
TheexperimentalresultsareshowninTable3. Comparedto
tasks respectively, but it is tested on these two tasks. The
theexpertmodel,theSpeechComposer-basemodelachieves
results reveal that the model is capable of performing zero-
better speech quality, while SpeechComposer-Large shows
shotSEandVCtasksduetoourdesignofuniformprompt
betterintelligibilityintermsofCERfrom6.9%to6.7%. Es-
tokens,evenwithoutexplicittrainingonsuchtasks. Forboth
peciallyinSim-speech,SpeechComposer-baseoutperforms
tasks, zero-shot generation results in a noticeable increase
PPG-VC from 0.63 to 0.74. For speech enhancement, we
inWERorCER,whereasthedegradationinDNSMOSand
comparewithUSES[Zhangetal.,2023a],anunconstrained
MOSNetscoresismodest.ItisnoteworthythatintheVCtask,
speechenhancementandseparationnetworkthatachieveshigh
the models not trained on the VC task outperformed those
performanceacrossdifferentconditions. Weusethetestset
trainedsolelyonVCorVC+TTStasks. Thisunderscoresthe
ofVoiceBank+DEMAND,whichhas824utterancesoftwo
importanceofdataquantityforspeechlanguagemodels. Also,
speakersfromtheVoiceBankcorpusmixedwithunseenDE-
followingthiszero-shotmanner,ourmodelisflexibleenough
MANDnoises. ComparedtotheUSES,SpeechComposer’s
to extend to more composite tasks, even in the absence of
effectiveness is slightly inferior to that of an expert model
task-specifictrainingdata.
specificallydesignedforspeechenhancementtasks.
6.5 AblationStudies
Comparison of model size. We further explore if a
larger model size can help composite tasks by comparing Effectivenessofadditionaltextinput. Withthedataformat
SpeechComposer-PretrainedandSpeechComposer-Large. For design of SpeechComposer described in Section 3.1, it is
bothVCandSE,weobservethatthelargermodelachieves feasibletoperformVCandSEusingthediscretespeechtokens
betterintelligibilityandcomparablespeechquality. asinputorwiththecorrespondingtext. Toverifytheefficacy
ofincorporatingadditionaltextinputintheSpeechComposer,
6.3 TheEffectsofTrainingTasksNumbers. (RQ2)
weconductexperimentsonVCandSEtasks. Weconsider
As mentioned in Section 3.3, we also conduct experiments twoscenarios–usingonlyspeechinputandusingcombined
where we use different numbers of tasks during training to speechandgroundtruthtextinput. Theexperimentalresults
explorethepotentialinteractionsbetweendifferentprimary arepresentedinTable5.Forbothtasks,omittingthetextinput
tasksandcompositetasks. Table4showstheexperimental resultedinanoticeableincreaseinCERorWER,whereastheTTS Voiceconversion Speechenhancement
Trainingtasks
MOSNet(↑) CER(↓) MOSNet(↑) CER(↓) Sim-speech(↑) DNSMOS(↑) WER(↓)
TTS 4.20 3.9 — — — — —
VC — — ——Cannotconverge—– — —
SE — — — — — ——Cannotconverge—–
TTS+VC 4.22 5.1 4.36 72.6 0.69 — —
TTS+SE 4.25 3.4 — — — 3.07 32.6
TTS+ASR+VC 4.23 5.3 4.36 9.0 0.71 — —
TTS+ASR+SE 4.24 3.8 — — — 3.04 28.2
TTS+ASR+VC+SE 4.21 3.1 4.38 7.8 0.70 3.07 7.9
SpeechComposer-Basew/oVC — — 4.37 12.1 0.73 — —
SpeechComposer-Basew/oSE — — — — — 3.03 29.4
Table4:ExperimentalresultsofSpeechComposerwithdifferenttrainingtasks.Theboldednumbershighlightthebestperformanceachieved
foreachmetric.Generally,themoretasksamodelistrainedon,thebetteritsperformance.
Voiceconversion Speechenhancement Settings Trainingtasks MOSNet(↑) CER(↓) Sim-speech(↑) TER(↓)
Prompt
MOSNet(↑) CER(↓) Sim-speech(↑) DNSMOS(↑) WER(↓)
Base alltasks 4.35/4.32 1.1/7.0 0.72/0.71 0.63/0.70
w/text 4.35 1.1 0.72 3.11 3.4 w/ors alltasks 4.33/4.32 1.7/7.3 0.71/0.70 0.65/0.67
w/otext 4.32 7.0 0.71 3.09 8.1 Base VC 4.38/4.37 25.3/− 0.72/− 0.89/−
w/ors VC 4.35/4.35 62.7/− 0.70/− 0.99/−
Table5:Resultsonvoiceconversionandspeechenhancementwith
Table 7: Effects of randomized sampling strategy with different
orwithouttextinput.Itshowsthatleveragingthetextinputispartic-
amountoftrainingdata. Here,“Base”referstoSpeechComposer-
ularlybeneficialforenhancingtheintelligibilityoftheoutputspeech.
Base,and“rs”istheabbreviationofrandomizedsamplingstrategy.
Settings MOSNet(↑) CER(↓) Sim-speech(↑) TER(↓)
7 Conclusion
seenspeakers 4.35/4.32 1.1/7.0 0.72/0.71 0.63/0.70
unseenspeakers 4.36/4.28 1.5/9.0 0.68/0.65 0.69/0.72 Inthispaper,wedescribeSpeechComposer,adecoder-only
languagemodelthatunifiescommonspeechtasksbycompos-
Table6:Resultsofvoiceconversionbetweenseenspeakerandunseen ing a fixed set of prompt tokens. By defining four primary
speakers,includingbothspeech-text/speech-onlyresults.
tasks,speechLM,textLM,TTS,andASR,SpeechComposer
caneasilyextendtomorespeechtasks,includingVCandSE.
Thesetaskscansharetheknowledgeofeachother,andachieve
degradationinMOSNetorDNSMOSscoresismodest. These
betterperformance. WedemonstrateSpeechComposer’sef-
findingssuggestthatleveragingthetextinputisparticularly
ficacy through comprehensive experiments and analyze the
beneficialforenhancingtheintelligibilityoftheoutputspeech.
impactofvarioustrainingtasksonmodelperformance. Also,
PerformanceofVCforunseenspeakers. Tovalidatethe weverifythebehaviorofcompositetasksinzero-shotscenar-
SpeechComposer’sperformanceonVCtasksbetweenunseen ios,whichshowstheflexibilityofSpeechComposertoextend
speakers,wesplit350pairedspeechsamplesbetweenunseen tomorenewtasks. Wewillfurtherexplorethisworkbyex-
speakersfromtheVCTKcorpusasthetestset. Asshownin pandingsupportedtasks,takingadvantageoftheknowledge
Table6,weobservethatSpeechComposerperformswellin ofpretrainedfoundationmodelsandenhancingrobustness.
VCbetweenunseenspeakers. ComparedwithVCbetween
seen speakers, VC between unseen speakers is more diffi- 8 Acknowledgements
cult,leadingtoaslightlylowerSim-speechandhigherTER
ExperimnetsofthisworkusedtheBridges2systematPSC
metrics.
and Delta system at NCSA through allocations CIS210014
andIRI120008PfromtheAdvancedCyberinfrastructureCoor-
Effectivenessofrandomizedsamplingstrategy. Wecon-
dinationEcosystem: Services&Support(ACCESS)program.
ductexperimentstoverifytheeffectivenessofourproposed
randomized sampling loss on the VC task. For the hyper-
parameters q ,··· ,q and q of randomized sampling
1 N global
strategy mentioned in Section 3.3, we set q = q = 0.3,
1 2
q =0.4respectively. AsshowninTable7,weobserve
global
thatremovingtherandomizedsamplinglosswillleadtoworse
intelligibility,especiallywhenweusefewtrainingdata. For
example,removingtherandomizedsamplinglosstrainedon
theVCtaskleadstoanoticeableincreaseintheCERscore
(from25.3%to62.7%). Thisdemonstratesthatthequantity
oftrainingdatacontributestotrainingstabilityandvalidates
theeffectivenessoftherandomizedsampling.References memoryrecurrentneuralnetworks. InProc.Interspeech,
pages3274–3278.ISCA,2015.
[Agostinellietal.,2023] AndreaAgostinelli,TimoI.Denk,
ZalánBorsos,JesseEngel,MauroVerzetti,AntoineCail- [Cooperetal.,2022] Erica Cooper, Wen-Chin Huang,
lon,QingqingHuang,ArenJansen,AdamRoberts,Marco Tomoki Toda, and Junichi Yamagishi. Generalization
Tagliasacchi,MattSharifi,NeilZeghidour,andChristian ability of MOS prediction networks. In Proc. IEEE
Frank. MusicLM:Generatingmusicfromtext,2023. ICASSP,pages8442–8446,2022.
[Agüeroetal.,2006] PabloDanielAgüero,JordiAdell,and [Dongetal.,2023] Qianqian Dong, Zhiying Huang, Qiao
Antonio Bonafonte. Prosody generation for speech-to- Tian,ChenXu,TomKo,YunlongZhao,SiyuanFeng,Tang
speechtranslation. InProc.IEEEICASSP,pages557–560, Li, Kexin Wang, Xuxin Cheng, Fengpeng Yue, Ye Bai,
2006. XiChen,LuLu,ZejunMa,YupingWang,MingxuanWang,
[Alayracetal.,2022] Jean-Baptiste Alayrac, Jeff Donahue, andYuxuanWang.Polyvoice:Languagemodelsforspeech
tospeechtranslation. CoRR,abs/2306.02982,2023.
Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
KarelLenc,ArthurMensch,KatherineMillican,Malcolm [Hassidetal.,2023] Michael Hassid, Tal Remez, Tu Anh
Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade
Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Copet,AlexandreDéfossez,GabrielSynnaeve,Emmanuel
Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Dupoux,RoySchwartz,andYossiAdi.Textuallypretrained
Brock, AidaNematzadeh, SahandSharifzadeh, Mikołaj speechlanguagemodels. CoRR,abs/2305.13009,2023.
Bin´kowski,RicardoBarreira,OriolVinyals,AndrewZis-
[Horietal.,2019] TakaakiHori,RamónFernandezAstudillo,
serman,andKarénSimonyan. Flamingo: avisuallanguage
Tomoki Hayashi, Yu Zhang, Shinji Watanabe, and
modelforfew-shotlearning. AdvancesinNeuralInforma-
Jonathan Le Roux. Cycle-consistency training for end-
tionProcessingSystems,35:23716–23736,2022.
to-endspeechrecognition. InProc.IEEEICASSP,pages
[Baevskietal.,2020] Alexei Baevski, Yuhao Zhou, Abdel- 6271–6275,2019.
rahman Mohamed, and Michael Auli. wav2vec 2.0: A
[Hsuetal.,2021] Wei-Ning Hsu, Benjamin Bolte, Yao-
frameworkforself-supervisedlearningofspeechrepresen-
Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdi-
tations. In Advances in Neural Information Processing
nov,andAbdelrahmanMohamed. Hubert: Self-supervised
Systems,volume33,pages12449–12460,2020.
speechrepresentationlearningbymaskedpredictionofhid-
[Borsosetal.,2023] Zalán Borsos, Raphaël Marinier, denunits. IEEE/ACMTransactionsonAudio,Speech,and
Damien Vincent, Eugene Kharitonov, Olivier Pietquin, LanguageProcessing,pages3451–3460,2021.
MatthewSharifi,DominikRoblek,OlivierTeboul,David
[Huangetal.,2020] Wen-Chin Huang, Tomoki Hayashi,
Grangier,MarcoTagliasacchi,andNeilZeghidour. Audi-
Shinji Watanabe, and Tomoki Toda. The sequence-to-
olm: Alanguagemodelingapproachtoaudiogeneration.
sequencebaselineforthevoiceconversionchallenge2020:
IEEE ACM Trans. Audio Speech Lang. Process., pages
CascadingASRandTTS. CoRR,abs/2010.02434,2020.
2523–2533,2023.
[Huangetal.,2023] RongjieHuang,ChunleiZhang,Yongqi
[Brownetal.,2020] TomBrown,BenjaminMann,NickRy-
Wang, Dongchao Yang, Luping Liu, Zhenhui Ye, Ziyue
der,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
Jiang, Chao Weng, Zhou Zhao, and Dong Yu. Make-A-
ArvindNeelakantan,PranavShyam,GirishSastry,Amanda
Voice: Unifiedvoicesynthesiswithdiscreterepresentation.
Askell, SandhiniAgarwal, ArielHerbert-Voss, Gretchen
CoRR,abs/2305.19269,2023.
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse, [Kahnetal.,] JacobKahn, MorganeRivière, WeiyiZheng,
MarkChen,EricSigler,MateuszLitwin,ScottGray,Ben- Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel
jaminChess,JackClark,ChristopherBerner,SamMcCan- Mazaré,JulienKaradayi,VitaliyLiptchinsky,RonanCol-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. lobert,ChristianFuegen,TatianaLikhomanenko,Gabriel
Language models are few-shot learners. In Advances in Synnaeve, ArmandJoulin, AbdelrahmanMohamed, and
NeuralInformationProcessingSystems,volume33,pages Emmanuel Dupoux. Libri-light: A benchmark for ASR
1877–1901,2020. with limited or no supervision. In Proc. IEEE ICASSP,
pages7669–7673.
[Changetal.,2019] XuankaiChang,WangyouZhang,Yan-
minQian,JonathanLeRoux,andShinjiWatanabe. MIMO- [Kimetal.,2021] Jaehyeon Kim, Jungil Kong, and Juhee
Speech: End-to-end multi-channel multi-speaker speech Son. Conditionalvariationalautoencoderwithadversarial
recognition. InProc.IEEEASRU,pages237–244,2019. learning for end-to-end text-to-speech. In Proc. ICML,
2021.
[Changetal.,2023] XuankaiChang,BrianYan,YuyaFujita,
Takashi Maekaku, and Shinji Watanabe. Exploration of [KingmaandBa,2015] DiederikP.KingmaandJimmyBa.
efficientend-to-endASRusingdiscretizedinputfromself- Adam: A method for stochastic optimization. In Proc.
supervisedlearning. CoRR,abs/2305.18108,2023. ICLR,2015.
[Chenetal.,2015] ZhuoChen,ShinjiWatanabe,HakanEr- [KominekandBlack,2004] John Kominek and Alan W
dogan, and John R. Hershey. Speech enhancement and Black. TheCMUArcticspeechdatabases. InFifthISCA
recognition using multi-task learning of long short-term workshoponspeechsynthesis,2004.[Kongetal.,2020] JungilKong, JaehyeonKim, andJaeky- [Panayotovetal.,] VassilPanayotov,GuoguoChen,Daniel
oungBae. HiFi-GAN:Generativeadversarialnetworksfor Povey, and Sanjeev Khudanpur. Librispeech: An ASR
efficientandhighfidelityspeechsynthesis. Advancesin corpusbasedonpublicdomainaudiobooks. InProc.IEEE
NeuralInformationProcessingSystems,33:17022–17033, ICASSP,pages5206–5210.
2020.
[Radfordetal.,2023] Alec Radford, Jong Wook Kim, Tao
[Lakhotiaetal.,2021] KushalLakhotia,EugeneKharitonov, Xu, Greg Brockman, Christine McLeavey, and Ilya
Wei-NingHsu,YossiAdi,AdamPolyak,BenjaminBolte, Sutskever. Robustspeechrecognitionvialarge-scaleweak
Tu-AnhNguyen,JadeCopet,AlexeiBaevski,Abdelrahman supervision.InProc.ICML,ProceedingsofMachineLearn-
Mohamed,andEmmanuelDupoux. Ongenerativespoken ingResearch,pages28492–28518,2023.
language modeling from raw audio. Transactions of the [Reddyetal.,2022] Chandan K. A. Reddy, Vishak Gopal,
Association for Computational Linguistics, pages 1336–
and Ross Cutler. DNSMOS P.835: A non-intrusive per-
1354,2021.
ceptualobjectivespeechqualitymetrictoevaluatenoise
[Lietal.,2023] JunnanLi,DongxuLi,SilvioSavarese,and suppressors. InProc.IEEEICASSP,pages886–890,2022.
StevenC.H.Hoi. BLIP-2: bootstrappinglanguage-image [Renetal.,2019] Yi Ren, Xu Tan, Tao Qin, Sheng Zhao,
pre-trainingwithfrozenimageencodersandlargelanguage ZhouZhao,andTie-YanLiu. Almostunsupervisedtextto
models. InProc.ICML,ProceedingsofMachineLearning speechandautomaticspeechrecognition. InProc.ICML,
Research,pages19730–19742,2023. pages5410–5419,2019.
[Liaoetal.,2022] Yuan–Fu Liao, Wen–Han Hsu, [Singhetal.,2022] Amanpreet Singh, Ronghang Hu,
Chen–MingPan,Wern–JunWang,MatusPleva,andDaniel Vedanuj Goswami, Guillaume Couairon, Wojciech
Hladek. Personalized taiwanese speech synthesis using Galuba, Marcus Rohrbach, and Douwe Kiela. FLAVA:
cascadedASRandTTSframework. In32ndInternational Afoundationallanguageandvisionalignmentmodel. In
ConferenceRadioelektronika,pages01–05,2022. Proceedings of the IEEE/CVF Conference on Computer
[Liuetal.,2021] SongxiangLiu,YuewenCao,DisongWang,
VisionandPatternRecognition,pages15638–15650,2022.
Xixin Wu, Xunying Liu, and Helen Meng. Any-to- [Snyderetal.,2018] DavidSnyder, DanielGarcia-Romero,
manyvoiceconversionwithlocation-relativesequence-to- GregorySell,DanielPovey,andSanjeevKhudanpur. X-
sequencemodeling. IEEEACMTrans.AudioSpeechLang. vectors: RobustDNNembeddingsforspeakerrecognition.
Process.,pages1717–1728,2021. InProc.IEEEICASSP,pages5329–5333,2018.
[Loetal.,2019] Chen-Chou Lo, Szu-Wei Fu, Wen-Chin [Touvronetal.,2023] HugoTouvron,ThibautLavril,Gautier
Huang,XinWang,JunichiYamagishi,YuTsao,andHsin- Izacard,XavierMartinet,Marie-AnneLachaux,Timothée
MinWang. MOSNet: Deeplearning-basedobjectiveas- Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
sessmentforvoiceconversion. InProc.Interspeech,pages FaisalAzhar,AurélienRodriguez,ArmandJoulin,Edouard
1541–1545.ISCA,2019. Grave,andGuillaumeLample.LLaMA:Openandefficient
foundationlanguagemodels. CoRR,abs/2302.13971,2023.
[Lorenzo-Truebaetal.,2018] Jaime Lorenzo-Trueba, Ju-
nichiYamagishi,TomokiToda,DaisukeSaito,Fernando [Valentini-Botinhaoetal.,2016] Cassia Valentini-Botinhao,
Villavicencio,TomiKinnunen,andZhen-HuaLing. The XinWang,ShinjiTakaki,andJunichiYamagishi. Speech
voiceconversionchallenge2018: Promotingdevelopment enhancement for a noise-robust text-to-speech synthesis
ofparallelandnonparallelmethods. InOdyssey2018: The system using deep recurrent neural networks. In Proc.
SpeakerandLanguageRecognitionWorkshop,26-29June Interspeech,pages352–356,2016.
2018,LesSablesd’Olonne,France,pages195–202.ISCA, [Vaswanietal.,2017] AshishVaswani,NoamShazeer,Niki
2018. Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
[Maitietal.,2023] SoumiMaiti,YifanPeng,ShukjaeChoi, LukaszKaiser,andIlliaPolosukhin. Attentionisallyou
Jee-weon Jung, Xuankai Chang, and Shinji Watanabe. need. InAdvancesinNeuralInformationProcessingSys-
VoxtLM: unified decoder-only models for consolidating tems,2017.
speechrecognition/synthesisandspeech/textcontinuation [Veauxetal.,2017] Christophe Veaux, Junichi Yamagishi,
tasks. CoRR,abs/2309.07937,2023. KirstenMacDonald,etal. CSTRVCTKcorpus: English
[Matusovetal.,2005] Evgeny Matusov, Stephan Kanthak, multi-speakercorpusforCSTRvoicecloningtoolkit. Uni-
versity of Edinburgh. The Centre for Speech Technology
andHermannNey.Ontheintegrationofspeechrecognition
Research(CSTR),page15,2017.
andstatisticalmachinetranslation. InProc.Interspeech,
pages3177–3180,2005. [Wangetal.,2023a] ChengyiWang,SanyuanChen,YuWu,
ZiqiangZhang,LongZhou,ShujieLiu,ZhuoChen,Yan-
[Mertesetal.,2021] SilvanMertes,ThomasKiderle,Ruben
qingLiu,HuamingWang,JinyuLi,LeiHe,ShengZhao,
Schlagowski, Florian Lingenfelser, and Elisabeth Andre.
andFuruWei. Neuralcodeclanguagemodelsarezero-shot
On the potential of modular voice conversion for virtual
texttospeechsynthesizers. CoRR,abs/2301.02111,2023.
agents. In9thInternationalConferenceonAffectiveCom-
putingandIntelligentInteractionWorkshopsandDemos [Wangetal.,2023b] Tianrui Wang, Long Zhou, Ziqiang
(ACIIW),pages1–7,2021. Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen,JinyuLi,andFuruWei. VioLA:Unifiedcodeclanguage
modelsforspeechrecognition,synthesis,andtranslation.
CoRR,abs/2305.16107,2023.
[Wangetal.,2023c] XiaofeiWang,ManthanThakker,Zhuo
Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan
Chen,MinTang,ShujieLiu,JinyuLi,andTakuyaYosh-
ioka. SpeechX:Neuralcodeclanguagemodelasaversatile
speechtransformer. CoRR,abs/2308.06873,2023.
[Zenetal.,2019] Heiga Zen, Viet Dang, Rob Clark,
YuZhang,RonJ.Weiss,YeJia,ZhifengChen,andYonghui
Wu. LibriTTS:Acorpusderivedfromlibrispeechfortext-
to-speech. InProc.Interspeech,pages1526–1530.ISCA,
2019.
[Zhangetal.,2019a] Jing-Xuan Zhang, Zhen-Hua Ling,
YuanJiang,Li-JuanLiu,ChenLiang,andLi-RongDai. Im-
provingsequence-to-sequencevoiceconversionbyadding
text-supervision. InProc.IEEEICASSP,pages6785–6789,
2019.
[Zhangetal.,2019b] MingyangZhang,XinWang,Fuming
Fang,HaizhouLi,andJunichiYamagishi. Jointtraining
frameworkfortext-to-speechandvoiceconversionusing
multi-sourcetacotronandwavenet. InProc.Interspeech,
pages1298–1302.ISCA,2019.
[Zhangetal.,2022] Susan Zhang, Stephen Roller, Naman
Goyal,MikelArtetxe,MoyaChen,ShuohuiChen,Christo-
pherDewan,MonaT.Diab,XianLi,XiVictoriaLin,Todor
Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,
andLukeZettlemoyer. OPT:Openpre-trainedtransformer
languagemodels. arXivpreprintarXiv:2205.01068,2022.
[Zhangetal.,2023a] WangyouZhang,KoheiSaijo,Zhong-
Qiu Wang, Shinji Watanabe, and Yanmin Qian. Toward
universalspeechenhancementfordiverseinputconditions.
InProc.IEEEASRU,2023.
[Zhangetal.,2023b] Ziqiang Zhang, Long Zhou, Chengyi
Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen,
Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng
Zhao,andFuruWei. Speakforeignlanguageswithyour
ownvoice: Cross-lingualneuralcodeclanguagemodeling.
CoRR,abs/2303.03926,2023.
[Zhaoetal.,2020] YiZhao,Wen-ChinHuang,XiaohaiTian,
Junichi Yamagishi, Rohan Kumar Das, Tomi Kinnunen,
Zhen-HuaLing,andTomokiToda. Voiceconversionchal-
lenge2020: Intra-lingualsemi-parallelandcross-lingual
voiceconversion. CoRR,abs/2008.12527,2020.