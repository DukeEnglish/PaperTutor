CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting
Jiezhi“Stephen”Yang1 KhushiDesai2 CharlesPacker3
HarshilBhatia4 NicholasRhinehart3 RowanMcAllister5 JosephGonzalez3
1HarvardUniversity 2ColumbiaUniversity 3UCBerkeley 4Avataar.ai 5ToyotaResearchInstitute
Abstract
CARFF 3D Driving Planning Application
We propose CARFF: Conditional Auto-encoded Radi- STOP
ance Field for 3D Scene Forecasting, a method for pre-
CARFF 3D
dicting future 3D scenes given past observations, such as Encoder Planning
2D ego-centric images. Our method maps an image to
a distribution over plausible 3D latent scene configura- Input Image
tions using a probabilistic encoder, and predicts the evo- 3D State Beliefs
3D Planning Results
lution of the hypothesized scenes through time. Our la-
Figure1. CARFF3Dplanningapplicationfordriving. Anin- tentscenerepresentationconditionsaglobalNeuralRadi-
put image containing a partially observable view of an intersec-
ance Field (NeRF) to represent a 3D scene model, which
tion is processed by CARFF’s encoder to establish 3D environ-
enablesexplainablepredictionsandstraightforwarddown-
ment state beliefs, i.e. the predicted possible state of the world:
stream applications. This approach extends beyond pre-
whetherornottherecouldbeanothervehicleapproachingthein-
vious neural rendering work by considering complex sce-
tersection. Thesebeliefsareusedtoforecastthefuturein3Dfor
nariosofuncertaintyinenvironmentalstatesanddynamics. planning,generatingoneamongtwopossibleactionsforthevehi-
We employ a two-stage training of Pose-Conditional-VAE cletomergeintotheotherlane.
and NeRF to learn 3D representations. Additionally, we
overnotjusttheexistenceandpositionofindividualobjects
auto-regressively predict latent scene representations as a
(e.g.,whetherornotthereisanoncomingcar),butalsothe
partially observable Markov decision process, utilizing a
shapes,colors,andtexturescomposingtheentireoccluded
mixturedensitynetwork. Wedemonstratetheutilityofour
portionofthescene.
methodinrealisticscenariosusingtheCARLAdrivingsim-
Traditionally, autonomous systems with high-
ulator, where CARFF can be used to enable efficient tra-
dimensional sensor observations such as video or LiDAR
jectory and contingency planning in complex multi-agent
process the data into low-dimensional state information,
autonomous driving scenarios involving visual occlusions.
suchasthepositionandvelocityoftrackedobjects, which
Our website containing video demonstrations and code is
are then used for downstream prediction and planning.
availableat: www.carff.website.
This object-centric framework can be extended to reason
aboutpartiallyobservedsettingsbymodelingtheexistence
1.Introduction
and state of potentially dangerous unobserved objects in
Humans often imagine what they cannot see given partial addition to tracked fully observed objects. Such systems
visualcontext. Considerascenariowherereasoningabout often plan with respect to worst-case hypotheses, e.g., by
the unobserved is critical to safe decision-making: for ex- placing a “ghost car” traveling at speed on the edge of the
ample, a driver navigating a blind intersection. An expert visiblefieldofview[41].
driverwillplanaccordingtowhattheybelievemayormay Recent advances in neural rendering have seen tremen-
not exist in occluded regions of their vision. The driver’s dous success in learning 3D scene representations directly
belief – defined as the understanding of the world mod- fromposedmulti-viewimagesusingdeepneuralnetworks.
eled with consideration for inherent uncertainties of real- Neural Radiance Fields (NeRF) allow for synthesizing
worldenvironments–isinformedbytheirpartialobserva- novelimagesina3Dscenefromarbitraryviewingangles,
tions (i.e., the presence of other vehicles on the road), as making seeing behind an occlusion as simple as rendering
well as their prior knowledge (e.g., past experience nav- fromanunoccludedcameraangle. BecauseNeRFcangen-
igating this intersection). When reasoning about the un- erate RGB images from novel views, it decouples the de-
observed, humans are capable of holding complex beliefs pendency of the scene representation on the object detec-
1
4202
naJ
13
]VC.sc[
1v57081.1042:viXration and tracking pipeline. For example, images rendered large-scaleunboundedscenes[2,40,46],scenefromsparse
from a NeRF may contain visual information that would views[7,35,45]andmultiplescenes[17,48]. Tewarietal.
be lost by an object detector, but may still be relevant for [42] presents an in-depth survey on neural representation
safe decision-making. Additionally, because NeRF repre- learninganditsapplications.
sents explicit geometry via an implicit density map, it can Generalizablenovelviewsynthesismodelssuchaspix-
beuseddirectlyformotionplanningwithoutrequiringany elNeRF and pixelSplat [5, 51] learn a scene prior to ren-
rendering [1]. NeRF’s ability to represent both visual and dernovelviewsconditionedonsparseexistingviews. Dy-
geometric information makes them a more general and in- namicNeRF,ontheotherhand,modelssceneswithmoving
tuitive3Drepresentationforautonomoussystems. objectsorobjectsundergoingdeformation. Awidelyused
DespiteNeRF’sadvantages,achievingprobabilisticpre- approachistoconstructacanonicalspaceandpredictade-
dictions in 3D based on reasoning from the occluded is formationfield[19,29,30,32]. Thecanonicalspaceisusu-
challenging. Forexample,discriminativemodelsthatyield allyastaticscene,andthemodellearnsanimplicitlyrepre-
categorical predictions are unable to capture the underly- sentedflowfield[29,32].Arecentlineofworkalsomodels
ing 3D structure, impeding their ability to model uncer- dynamicscenesviadifferentrepresentationsanddecompo-
tainty. While prior work on 3D representation captures sition [3, 37]. These approaches tend to perform better
view-invariantstructures,theirapplicationisprimarilycon- forspatiallyboundedandpredictablesceneswithrelatively
finedto simplescenarios[17]. WepresentCARFF, which small variations [3, 20, 29, 51]. Moreover, these methods
to our knowledge, is the first forecasting approach in sce- onlysolveforchangesintheenvironmentbutarelimitedin
narios with partial observations that uniquely facilitates incorporatingstochasticityintheenvironment.
stochastic predictions within a 3D representation, effec-
Multi-scene NeRF: Our approach builds on multi-scene
tivelyintegratingvisualperceptionandexplicitgeometry.
NeRFapproaches[17,44,48,49]thatlearnagloballatent
CARFF addresses the aforementioned difficulties by
scenerepresentation,whichconditionstheNeRF,allowing
proposingPC-VAE:Pose-ConditionedVariationalAutoen-
a single NeRF to effectively represent various scenes. A
coder, a framework that trains a convolution and Vision
similar method, NeRF-VAE, was introduced by Kosiorek
Transformer (ViT) [9] based image encoder. The encoder
et al. [17] to create a geometrically consistent 3D genera-
mapsapotentiallypartiallyobservableego-centricimageto
tive model with generalization to out-of-distribution cam-
latent scene representations, which hold state beliefs with
eras. However, NeRF-VAE[17]ispronetomodecollapse
implicit probabilities. The latents later condition a neural
whenevaluatedoncomplex,real-worlddatasets.
radiancefieldthatfunctionsasa3Ddecodertorecover3D
scenesfromarbitraryviewpoints. ThisistrainedafterPC-
2.2.SceneForecasting
VAEinourtwo-stagetrainingpipeline(seeSec.3.1). Ad-
ditionally,wedesignamixturedensitymodeltopredictthe Planning in 2Dspace: In general, planning inlarge and
evolution of 3D scenes over time stochastically in the en- continuousstate-actionspacesisdifficultduetotheresult-
coderbeliefspace(seeSec.3.2). Apotentialapplicationof ing exponentially large search space [28]. Consequently,
CARFF is illustrated in Fig. 1. Using the CARLA driving several approximation methods have been proposed for
simulator,wedemonstratehowCARFFcanbeusedtoen- tractability [22, 31]. Various model-free [12, 27, 43]
able contingency planning in real-world driving scenarios and model-based [4] reinforcement learning frameworks
thatrequirereasoningintovisualocclusions. emerge as viable approaches, along with other learning-
based methods [6, 25]. Several other approaches forecast
2.Relatedwork fordownstreamcontrol[15],learnbehaviormodelsforcon-
tingency planning [34], or predict potential existence and
2.1.DynamicNeuralRadianceFields
intentionsofpossiblyunobservedagents[26]. Whilethese
Neural radiance fields: Neural Radiance Fields methodsarein2D,wesimilarlyreasonunderpartialobser-
(NeRF) [2, 23, 39] for 3D representations have gar- vations,andaccountforthesefactorsin3D.
nered significant attention due to their ability to generate
high-resolution, photorealistic scenes. Instant Neural NeRF in robotics: Several recent works have explored
Graphics Primitive (Instant-NGP) [24] speeds up training theapplicationofNeRFsinrobotics,likelocalization[50],
and rendering time by introducing a multi-resolution navigation[1,21],dynamics-modeling[10,19]androbotic
hash encoding. Other works like Plenoxels [11] and grasping [14, 16]. Adamkiewicz et al. [1] proposes a
DirectVoxGo(DVGO) [38]alsoprovidesimilarspeedups. method for quadcopter motion planning in NeRF models
GiventhewideadoptionandspeedupsofInstant-NGP,we via sampling the learned density function, which is a de-
use it to model 3D radiance fields in our work. NeRFs sirable characteristic of NeRF that can be leveraged for
havealsobeenextendedforseveraltaskssuchasmodeling forecasting and planning purposes. Additionally, Driess
2planningamidstperceptualuncertainty(seeSec.3.2).
3.1.NeRFPose-ConditionalVAE(PC-VAE)
Architecture: GivenasceneS attimestampt,werender
t
anego-centricobservationimageIt capturedfromcamera
c
pose c. The objective is to formulate a 3D representation
Ego- centric Lane View Predicted 3D View
oftheimagewherewecanperformaforecastingstepthat
Figure2. Novelviewplanningapplication. CARFFallowsrea- evolvesthesceneforward.Toachievethis,weutilizearadi-
soningbehindoccludedviewsfromtheegocarassimpleasmov- ancefieldconditionedonlatentvariablezsampledfromthe
ingthecameratoseethesampledbeliefpredictions,allowingsim- posteriordistributionq (z|It). Now,tolearntheposterior,
ϕ c
ple downstream planning using, for example, density probing or
weutilizePC-VAE.Weconstructanencoderusingconvolu-
2Dsegmentationmodelsfromarbitraryangles.
tionallayersandapre-trainedViTonImageNet[9].Theen-
et al. [10] utilize a graph neural network to learn a dy- coderlearnsamappingfromtheimagespacetoaGaussian
namics model in a multi-object scene represented through distributedlatentspaceq (z|It) = N(µ,σ2)parametrized
ϕ c
NeRF. Li et al. [18] primarily perform pushing tasks in a by mean µ and variance σ2. The decoder, p(I|z,c), con-
scene with basic shapes, and approach grasping and plan- ditioned on camera pose c, maps the latent z ∼ N(µ,σ2)
ning with NeRF and a separately learned latent dynamics intotheimagespaceI. Thishelpstheencodertogenerate
model. Priorworkeitheronlyperformswellonsimpleand latentsthatareinvarianttothecameraposec.
staticscenes[1]orhasadeterministicdynamicsmodel[18]. To enable 3D scene modeling, we employ Instant-
CARFF focuses on complicated realistic environments in- NGP [24], which incorporates a hash grid and an occu-
volving both state uncertainty and dynamics uncertainty, pancy grid to enhance computation efficiency. Addition-
which account for the potential existence of an object and ally, a smaller multilayer perceptron (MLP), F (z) can be
θ
unknownobjectmovementsrespectively. utilizedtomodelthedensityandappearance,givenby:
3.Method F (z):(x,d,z)→((r,g,b),σ) (1)
θ
3Dscenerepresentationhaswitnessedsignificantadvance- Here,x ∈ R3 andd ∈ (θ,ϕ)representthelocationvector
mentsinrecentyears,allowingformodelingenvironments and the viewing direction respectively. The MLP is also
inacontextuallyrichandinteractive3Dspace. Thisoffers conditionedonthesampledscenelatentsz ∼q (z|It)(see
ϕ c
manyanalyticalbenefits,suchasproducingsoftoccupancy AppendixC).
grids for spatial analysis and novel view synthesis for ob- Training methodology: The architecture alone does not
jectdetection. Giventheadvantages,ourprimaryobjective
enable us to model complex real-world scenarios, as seen
is to develop a model for probabilistic 3D scene forecast- through a similar example in NeRF-VAE [17]. A crucial
ing in dynamic environments. However, direct integration
contribution of our work is our two-stage training frame-
of3DscenerepresentationviaNeRFandprobabilisticmod-
work which stabilizes the training. First, we optimize the
elslikeVAEofteninvolvesnon-convexandinter-dependent
convolutionalViTbasedencoderandpose-conditionalcon-
optimization,whichcausesunstabletraining. Forinstance,
volutional decoder in the pixel space for reconstruction.
NeRF’s optimization may rely on the VAE’s latent space
Thisenablesourmethodtodealwithmorecomplexandre-
beingstructuredtoprovideinformativegradients.
alistic scenes as the encoding is learned in a semantically
To navigate these complexities, our method bifurcates rich 2D space. By conditioning the decoder on camera
the training process into two stages (see Fig. 3). First, poses, we achieve disentanglement between camera view
we train the PC-VAE to learn view-invariant scene repre- angles and scene context, making the representation view-
sentations. Next, we replace the decoder with a NeRF to invariantandtheencoder3D-aware. Next,oncerichlatent
learn a 3D scene from the latent representations. The la- representations are learned, we replace the decoder with a
tent scene representations capture the environmental states latent-conditionedNeRFoverthelatentspaceofthefrozen
anddynamicsoverpossibleunderlyingscenes,whileNeRF encoder. The NeRF reconstructs encoder beliefs in 3D for
synthesizes novel views within the belief space, giving us novelviewsynthesis.
the ability to see the unobserved (see Fig. 2 and Sec. 3.1).
Loss: Our PC-VAE is trained using standard VAE loss,
During prediction, uncertainties can be modeled by sam-
withmeansquareerror(MSE)andaKullback–Leibler(KL)
pling latents auto-regressively from a predicted Gaussian
divergencegivenbyevidencelowerbound(ELBO):
mixture, allowing for effective decision-making. To this
L =L +L =
extent,weapproachsceneforecastingasapartiallyobserv- PC-VAE MSE,PC-VAE KLD,PC-VAE
ableMarkovdecisionprocess(POMDP)overlatentdistri- ||p(I|z,c′′)−I ct ′′∥2+E q(z|I ct)[logp(I|z)] (2)
butions,whichenablesustocapturemulti-modalbeliefsfor −w D (q (z|It)||p(I|z))
KL KL ϕ c
3Decoder Decoded images for
camera pose
Posed Images
Conditioning on
camera pose
Gaussian latent
distribution
Stage 1: VAE Encoder and Decoder training
Ground truths for
NeRF camera pose
Encoder
* Images are shown as batched for training only
Stage 2: Frozen encoder, NeRF Decoder
Figure3.VisualizingCARFF’stwostagetrainingprocess.Left:TheconvolutionalVITbasedencoderencodeseachoftheimagesIat
timestampst,t′andcameraposesc,c′intoGaussianlatentdistributions.Assumingonlytwotimestampsandanoverparameterizedlatent,
oneoftheGaussiandistributionswillhaveasmallerσ2,anddifferentµacrosstimestamps. UpperRight: Thepose-conditionaldecoder
stochasticallydecodesthesampledlatentz usingthecameraposec′′ intoimagesIt andIt′ . Thedecodedreconstructionandground
c′′ c′′
truthimagesareusedtotakethelossL .LowerRight:ANeRFistrainedbyconditioningonthelatentvariablessampledfrom
MSE,PC-VAE
theoptimizedGaussianparameters.TheseparameterscharacterizethedistincttimestampdistributionsderivedfromthePC-VAE.AnMSE
lossiscalculatedforNeRFasL .
MSE,NeRF
where w denotes the KL divergence loss weight and During inference, we observe stochastic behaviors un-
KL
z ∼ q (z|It). Tomakeourrepresentation3D-aware,our derocclusion,whichmotivatesustolearnamixtureofsev-
ϕ c
posterior is encoded using camera c while the decoder is eralGaussiandistributionsthatpotentiallydenotedifferent
conditionedonarandomlysampledposec′′. scene possibilities. Therefore, we model the POMDP us-
KL divergence regularizes the latent space to balance ingaMixtureDensityNetwork(MDN),withmulti-headed
conditioned reconstruction and stochasticity under occlu- MLPs, that predicts a mixture of K Gaussians. At any
sion. An elevated KL divergence loss weight w pushes timestamptthedistributionisgivenas:
KL
thelatentsclosertoastandardnormaldistribution,N(0,1), q′(z |It−1)=MDN(q (z |It−1)) (4)
ϕ t c ϕ t−1 c
thereby ensuring probabilistic sampling in scenarios un-
The model is conditioned on the posterior distribu-
der partial observation. However, excessive regularization
tion q (z ) to learn a predicted posterior distribution
causesthelatentstobelessseparable,leadingtomodecol- ϕ t−1
q′(z |It−1)ateachtimestamp. Thepredictedposteriordis-
lapse. To mitigate this, we adopt delayed linear KL diver- ϕ t c
tributionisgivenbythemixtureofGaussian:
gencelossweightschedulingtostrikeabalancedw .
KL
K
Next, we learn a NeRF-based decoder on the posterior (cid:88)
q′(z )= π N(µ ,σ2) (5)
oftheVAEtomodelscenes. Atanytimestamptweusea ϕ t i i i
i=1
standardpixel-wiseMSElossfortrainingtheNeRF,given
bythefollowingequation: here, π i, µ i, andσ i2 denotethemixtureweight, mean, and
L =∥It−render(F (·|q (z|It)))∥2 (3) variance of the ith Gaussian distribution within the pos-
MSE,NeRF c θ ϕ c
terior distribution. Here, K is the total number of Gaus-
We use a standard rendering algorithm as proposed by sians. Forbrevityweremovetheirconditioningonthepos-
Mu¨lleretal.[24]. Next,webuildaforecastingmoduleover teriorq (z )andsampledlatentz .Wesamplez from
ϕ t−1 t−1 t
thelearnedlatentspaceofourpose-conditionalencoder. themixtureofGaussiansq′(z ),wherez mostlikelyfalls
ϕ t t
within one of the Gaussian modes. The scene configura-
3.2.SceneForecasting
tioncorrespondingtothemodeisreflectedinthe3Dscene
Formulation: Thecurrentformulationallowsustomodel renderedbyNeRF.
scenes with different configurations across timestamps. In
Loss: TooptimizetheMDN,weminimizeanegativelog-
order to forecast future configurations of a scene given an
likelihoodfunction,givenby:
ego-centric view, we need to predict future latent distribu-
tions.Weformulatetheforecastingasapartiallyobservable  
N K
(cid:88) (cid:88)
Markovdecisionprocess(POMDP)overtheposteriordis- L
MDN
=− log π jN(y i;µ j,σ j2) (6)
tributionq (z|It)inthePC-VAE’slatentspace.
ϕ c i=1 j=1
4Scene 1: Ego car with Scene 2: Ego car only Scene 1: Ego car with Scene 2: Ego car with mented several scenarios in the CARLA driving simula-
actor ambulance slow- moving ambulance fast- moving ambulance
tor [8] (see Fig. 4). A single NVIDIA RTX 3090 GPU is
used to train PC-VAE, NeRF, and the MDN. All models,
trained sequentially, tend to converge within a combined
timeframeof24hours. Adetailedexperimentalsetupcan
befoundinAppendixC.Weshowthat,givenpartiallyob-
Multi- Scene Approaching Intersection Multi- Scene Two Lane Merge servable2Dinputs,CARFFperformswellinpredictingla-
tentdistributionsthatrepresentcomplete3Dscenes. Using
Figure4. Multi-sceneCARLAdatasets. Imagesillustratingthe
these predictions we design a CARFF-based controller for
varying car configurations and scenes for the Multi-Scene Two
performingdownstreamplanningtasks.
Lane Merge dataset (left) and the Multi-Scene Approaching In-
tersectiondataset(right).
4.1.DataGeneration
We generate datasets containing an ego object and vary-
ing actor objects in different configurations to test the ro-
bustness of our method. We conduct experiments on (a)
synthetic blender dataset for simple, controllable simula-
tionand(b)CARLA-baseddrivingdatasetsforcomplicated
real-worldscenarios[8].
Blender synthetic dataset: This comprises of a station-
ary blue cube (ego) accompanied by a red cylinder (actor)
that may or may not be present (see Fig. 5). If the ac-
tor is present, it exhibits lateral movement as depicted in
Fig. 5. This simplistic setting provides us with an inter-
Figure5.Blenderdataset.SimpleBlenderdatasetwithastation-
pretableframeworktoevaluateourmodel.
ary blue cube, accompanied by a potential red cylinder exhibit-
ingprobabilistictemporalmovement. Thedifferentcameraposes
demonstratehowmovementneedstobemodeledprobabilistically CARLAdataset: EachdatasetissimulatedforN times-
basedonpossibleocclusionsfromdifferentcameraangles. tamps and uses C = 100 predefined camera poses to cap-
tureimagesoftheenvironmentunderfullobservation,par-
wherey ∼q (z )issampledfromthedistributionoflatent tialobservation,andnovisibility. Thesedatasetsaremod-
i ϕ t
z ,learnedbytheencoder,andN denotesthetotalnumber eledaftercommondrivingscenariosinvolvingstateuncer-
t
ofsamples. taintythathavebeenproposedinrelatedworkssuchasAc-
tiveVisualPlanning[25].
Inference: Weconsideranunseenego-centricimageand
a) Single-Scene Approaching Intersection: The ego ve-
retrieve itsposterior q (z ) through theencoder. Next, we
ϕ t
hicleispositionedataT-intersectionwithanactorvehicle
predict the possible future posterior distribution q′(z ).
ϕ t+1 traversing the crossing along an evenly spaced, predefined
Fromthepredictedposterior,wesampleascenelatentand
trajectory. We simulate this for N = 10 timestamps. We
performlocalization. Weachievethisvia(a)densityprob-
mainly use this dataset to predict the evolution of times-
ing the NeRF or (b) segmenting the rendered novel views
tampsunderfullobservation.
usingoff-the-shelfmethodssuchasYOLO[33](seeFig.2).
b)Multi-SceneApproachingIntersection:Weextendthe
These allow us to retrieve a corresponding Gaussian dis-
aforementionedscenariotoamorecomplicatedsettingwith
tribution q (z ) in encoder latent space. This is auto-
ϕ t+1
stateuncertainty,bymakingtheexistenceoftheactorvehi-
regressively fed back into the MDN to predict the next
cle probabilistic. A similar intersection crossing is simu-
timestamp. SeeFig.6foranoverviewofthepipeline.
latedforN = 3timestampsforbothpossibilities. Theego
vehicle’sviewoftheactormaybeoccludedasitapproaches
4.Results
theT-intersectionovertheN timestamps. Theegovehicle
Decision-making under perceptual uncertainty is a perva- caneithermoveforwardorhaltatthejunction(seeFig.4).
sive challenge faced in robotics and autonomous driving, c) Multi-Scene Multi-actor Two Lane Merge: To add
especiallyinpartiallyobservableenvironmentsencountered more environment dynamics uncertainty, we consider a
in driving tasks. In these scenarios, accurate inference re- multi-actorsettingatanintersectionoftwomerginglanes.
garding the presence ofpotentially obscured agents is piv- Wesimulatethescenarioatanintersectionwithpartialoc-
otal. We evaluate the effectiveness of CARFF on similar clusions,withthesecondapproachingactorhavingvariable
real-world situations with partial observability. We imple- speed. Here the ego vehicle can either merge into the left
5Repeated Sampling
Mixture Density Gaussian Mixture NeRF
Pretrained Network
Encoder
Sampled Beliefs
Probing For Autoregressive Prediction
Figure6. Auto-regressiveinferenceinsceneprediction. Theinputimageattimestampt,It,isencodedusingthepre-trainedencoder
c
fromPC-VAE.ThecorrespondinglatentdistributionisfedintotheMixtureDensityNetwork,whichpredictsamixtureofGaussians.Each
oftheK Gaussiansisalatentdistributionthatmaycorrespondtodifferentbeliefsatthenexttimestamp. ThemixtureofGaussiansis
sampledrepeatedlyforthepredictedlatentbeliefs,visualizedasIt+1 ,representingpotentiallytheithpossibleoutcome.Thisisusedto
c′,scni
conditiontheNeRFtogenerate3Dviewsofthescene. Toaccomplishautoregressivepredictions,weprobetheNeRFforthelocationof
thecarandfeedthisinformationbacktothepre-trainedencodertopredictthesceneatthenexttimestamp.
lanebeforethesecondactororafteralltheactorspass,(see tially partially observable inputs (see Fig. 9). We calcu-
Fig.4). EachbranchissimulatedforN =3timestamps. lated an average Peak Signal-to-Noise Ratio (PSNR) over
the training data, as well as novel view encoder inputs.
4.2.CARFFEvaluation
To evaluate the quality of the latent space generated by
Adesirablebehaviorfromourmodelisthatitshouldpredict the encoder, we utilize t-SNE [47] plots to visualize the
acompletesetofpossiblescenesconsistentwiththegiven distribution of latent samples for each image in a given
ego-centric image, which could be partially observable. dataset (see Appendix E). We introduce a Support Vector
Thisiscrucialforautonomousdrivinginunpredictableen- Machine (SVM) [13] based metric to measure the visual-
vironmentsasitensuresstrategicdecision-makingbasedon ized clustering quantitatively, where a higher value indi-
potential hazards. To achieve this we require a rich PC- cates better clustering based on timestamps. Most latent
VAE latent space, high-quality novel view synthesis, and scenesamplesareseparablebytimestamps,whichindicates
auto-regressiveprobabilisticpredictionsoflatentsatfuture that the latents are view-invariant. Samples that are mis-
timestamps. We evaluate CARFF on a simple synthetic classifiedorlieontheboundaryusuallyrepresentpartially
blender-baseddatasetandeachCARLA-baseddataset. or fully occluded regions. This is desirable for forecast-
ing, as it enables us to model probabilistic behavior over
Evaluationonblenderdataset: InFig.5,forbothScene
these samples. In this process, balancing KL divergence
1a and 1b, our model correctly forecasts the lateral move-
weight scheduling maintains the quality of the PC-VAE’s
mentofthecylindertobeineitherpositionapproximately
latentspaceandreconstructions(seeAppendixC).There-
50%ofthetime,consideringaleftviewingangle. InScene
sultspresentedinTab.2substantiatethebenefitsofourPC-
2, with the absence of the red cylinder in the input cam-
VAEencoderarchitecturecomparedtootherformulations.
eraangle, themodelpredictsthepotentialexistenceofthe
Specifically,anon-conditionalVAEfailsinSVMaccuracy
red cylinder approximately 50% of the time, and predicts
asitonlyreconstructsimagesanddoesnotcapturetheun-
lateralmovementswithroughlyequalprobability. Thisval-
derlying3Dstructures. VanillaPC-VAEandPC-VAEwith-
idates PC-VAE’s ability to predict and infer from the oc-
out freezing weights require careful fine-tuning of several
cludedinthelatentspace,consistentwithhumanintuitions.
hyper-parametersanddon’tgeneralizewelltodrasticcam-
Similarintuitions,demonstratedwithinthesimplescenesof
era movements. Our experiments show that our proposed
theBlenderdataset,canbetransferredtodrivingscenarios
modeliscapableofsustainingstochasticcharacteristicsvia
simulatedinourCARLAdatasets.
latentrepresentationsinthepresenceofocclusion,whilesi-
PC-VAE performance and ablations: We evaluate the multaneouslyensuringprecisereconstructions.
performanceofPC-VAEonCARLAdatasetswithmultiple
encoderarchitectures.WeshowthatPC-VAEeffectivelyre- 3D novel view synthesis: Given an unseen ego-centric
constructscomplexenvironmentsinvolvingvariablescenes, view with potentially partial observations, our method
actorconfigurations,andenvironmentalnoisegivenpoten- maintainsallpossiblecurrentstatebeliefsin3D,andfaith-
6Pose Inputs PC- VAE Decoded Images From Set of New Pose
Figure7.PC-VAEreconstructions.Theencoderinput,It,amongtheothergroundtruthimagesI viewedfromcameraposecatdifferent
c c
timestamps,isreconstructedacrossanewsetofposesc′′respectingtimestampt,generatingIt .AcompletegridisinAppendixE.
c′′
GroundTruth Avg. PSNR Avg. PSNR EncoderArchitectures Train SVM NV
PredictionPair (Scene1) (Scene2) PSNR Accuracy PSNR
Single-SceneApproachingIntersection PC-VAE 26.30 75.20 25.24
PC-VAEw/oCL 26.24 70.60 24.80
MatchingPairs 29.06 N.A
VanillaPC-VAE 26.02 25.70 24.65
Un-matchingPairs 24.01 N.A
PC-VAEw/oFreezing 24.57 5.80 24.60
Multi-SceneApproachingIntersection PC-VAEw/MobileNet 17.14 19.70 17.16
MatchingPairs 28.00 28.26 VanillaVAE 24.15 10.60 11.43
Un-matchingPairs 23.27 24.56
Multi-SceneTwoLaneMerge Table2. PC-VAEmetricsandablations. CARFF’sPC-VAEen-
coder outperforms other encoder architectures in both image re-
MatchingPairs 28.14 28.17 construction and pose-conditioning. We evaluated the following
Un-matchingPairs 22.74 23.32 ablations: PC-VAE without Conv Layer, PC-VAE with a vanilla
encoder, PC-VAE without freezing weights in ViT, PC-VAE re-
Table1. AveragedPSNRforfullyobservable3Dpredictions. placingViTwithpre-trainedMobileNet,andnonpose-conditional
CARFF correctly predicts scene evolution across all timestamps VanillaVAE.ThetabledisplaystheaveragetrainingPSNR,novel
foreachdataset.TheaveragePSNRissignificantlyhigherforeach view(NV)inputPSNR,andSVMaccuracyforlatenttimestamp
predictionIˆ andcorrespondinggroundtruth,I . PSNRvalues prediction.
ti ti
forincorrectcorrespondences,Iˆ,I ,isaresultofmatchingsur-
ti tj accuracies. Inscenarioswithpartialobservableego-centric
roundings.ThecompletetableofpredictionsisinAppendixE.
images where several plausible scenarios exist, we utilize
fullyreconstructsnovelviewsfromarbitrarycameraangles recallinsteadofaccuracyusingasimilarsetup. Thisletsus
foreachbelief. Fig.2illustratesoneofthepossible3Dbe- evaluatetheencoder’sabilitytoavoidfalsenegativepredic-
liefsthatCARFFholds.Thisdemonstratesourmethod’sca- tionsofpotentialdanger.
pacityforgenerating3Dbeliefsthatcouldbeusedfornovel Fig. 8 shows that our model achieves high accuracy
view synthesis in a view-consistent manner. Our model’s and recall in both datasets, demonstrating the ability to
abilitytoachieveaccurateandcomplete3Denvironmental modelstateuncertainty(ApproachingIntersection)anddy-
understandingisimportantforapplicationslikeprediction- namicuncertainty(TwoLaneMerge). Theresultsindicate
basedplanning. CARFF’sresilienceagainstrandomnessinresampling,and
completenessinprobabilisticmodelingofthebeliefspace.
Inference under full and partial observations: Under
Giventheseobservations,wenowbuildareliablecontroller
fullobservation,weuseMDNtopredictthesubsequentcar
toplanandnavigatethroughcomplexscenarios.
positions in all three datasets. PSNR values are calculated
based on bird-eye view NeRF renderings and ground truth
4.3.Planning
bird-eye view images of the scene across different times-
tamps. In Tab. 1 we report the PSNR values for rendered Inallourexperiments,theegovehiclemustmakedecisions
images over the predicted posterior with the ground truth to advance under certain observability. The scenarios are
images at each timestamp. We also evaluate the efficacy designed such that the ego views contain partial occlusion
of our prediction model using the accuracy curve given in and the state of the actor(s) is uncertain in some scenar-
Fig. 8. This represents CARFF’s ability to generate stable ios.Inordertofacilitatedecision-makingusingCARFF,we
beliefs, without producing incorrect predictions, based on designacontroller thattakesego-centricinputimages and
actor(s) localization results. For each number of samples outputsanaction.Decisionsaremadeincorporatingsample
between n = 0 to n = 50, we choose a random subset of consistencyfromthemixturedensitynetwork.Forinstance,
3 fully observable ego images and take an average of the thecontrollerinfersocclusionandpromotestheegocarto
7Multi-SceneApproachingIntersection 1 1
ControllerType ActorExists NoActor
0.8 0.8
Underconfident 30/30 0/30
Overconfident 0/30 30/30
0.6 0.6
CARFF(n=2) 17/30 30/30 Accuracy Accuracy
CARFF(n=10) 30/30 30/30 Recall Recall
0.4 0.4
CARFF(n=35) 30/30 19/30 0 10 20 30 40 50 0 10 20 30 40 50
NumSamples NumSamples
Multi-SceneTwoLaneMerge (a)ApproachingIntersection (b)TwoLaneMerge
ControllerType FastActor SlowActor
Figure8.Multi-Scenedatasetaccuracyandrecallcurvesfrom
Underconfident 30/30 0/30 predicted beliefs. We test our framework across n = 1 and
Overconfident 0/30 30/30 n = 50samplesfromMDN’spredictedlatentdistributionsfrom
CARFF(n=2) 21/30 30/30 ego-centric image input. Across the number of samples n, we
CARFF(n=10) 30/30 30/30 achieveanidealmarginofbeliefstatecoveragegeneratedunder
CARFF(n=35) 30/30 22/30 partial observation (recall), and the proportion of correct beliefs
sampledunderfullobservation(accuracy).Aswesignificantlyin-
creasethenumberofsamples,theaccuracystartstodecreasedue
Table 3. Planning in 3D with controllers with varying sam-
torandomnessinlatentdistributionresampling.
pling numbers n. CARFF-based controllers outperform base-
linesinsuccessrateover30trials. Forn=10,theCARFF-based unpredictableroadenvironments, therebyfosteringamore
controllerconsistentlychoosestheoptimalactioninpotentialcol- reliableandhuman-likeresponseinautonomousvehicles.
lision scenarios. For actor exists and fast-actor scenes, we con-
5.Discussion
sideroccludedego-centricinputstotestCARFF’sabilitytoavoid
collisions. Forno-actorandslow-actorscenes,weconsiderstate
Limitations: Like other NeRF-based methods, CARFF
observabilityandtestthecontrollers’abilitytorecognizetheopti-
currentlyreliesonposedimagesofspecificscenessuchas
malactiontoadvance.Tomaintainconsistency,weuseonesingle
roadintersections,limitingitsdirectapplicabilitytounseen
imageinputacross30trials.
environments. However,weanticipateenhancedgeneraliz-
pausewhenscenesalternatebetweenactorpresenceandab- ability with the increasing deployment of cameras around
senceinthesamples. Weusethetwomulti-scenedatasets populated areas, such as traffic cameras at intersections.
toassesstheperformanceoftheCARFF-basedcontrolleras Additionally,handlingverycomplexdynamicswithanex-
theycontainactorswithpotentiallyunknownbehaviors. tremelylargenumberofactorsstillposesachallengeforour
Todesignaneffectivecontroller, weneedtofindabal- method, requiring careful fine-tuning to balance compre-
ance between accuracy and recall (see Fig. 8). A lowered hensive dynamics modeling against accuracy. Potentially
accuracy from excessive sampling means unwanted ran- stronger models in the near future may offer a promising
domnessinthepredictedstate.However,takinginsufficient avenueforfurtherenhancementsinthisregard.
samples would generate low recall i.e., not recovering all
Conclusion: We presented CARFF, a novel method for
plausiblestates. Thiswouldleadtoincorrectpredictionsas
probabilistic 3D scene forecasting from partial observa-
wewouldbeunabletoaccountfortheplausibleuncertainty
tions. By employing a Pose-Conditional VAE, a NeRF
presentintheenvironment. Tofindabalance,wedesignan
conditionedonthelearnedposterior,andamixturedensity
open-loop planning controller opting for a sampling strat-
network that forecasts future scenes, we effectively model
egythat involvesgenerating n = 2,10,35 samples, where
complex real-world environments with state and dynam-
nisahyperparametertobetunedforpeakperformance.
ics uncertainty in occluded regions critical for planning.
Forsamplingvaluesthatlieonthebordersoftheaccu-
Wedemonstratedthecapabilitiesofourmethodinrealistic
racy and recall margin, for example, n = 2 and 35, we
autonomous driving scenarios, where, under full observa-
seethattheCARFF-basedcontrollerobtainslowersuccess
tions,wecanforecastintothefutureprovidinghigh-fidelity
rates, whereas n = 10 produces the best result. Across
3D reconstructions of the environment, while we maintain
thetwodatasetsinTab.3,theoverconfidentcontrollerwill
completerecallofpotentialhazardsgivenincompletescene
inevitablyexperiencecollisionsincaseofatruckapproach-
information.Overall,CARFFoffersanintuitiveandunified
ing, since it does not cautiously account for occlusions.
approach to perceiving, forecasting, and acting under un-
On the other hand, an overly cautious approach results in
certainty that could prove invaluable for vision algorithms
stasis, inhibiting the controller’s ability to advance in the
inunstructuredenvironments.
scene. Thisnuanceddecision-makingusingCARFF-based
controllerisespeciallycrucialindrivingscenarios,asiten-
hances safety and efficiency by adapting to complex and
8
% %References [15] Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien
Gaidon, andMarcoPavone. Mats: Aninterpretabletrajec-
[1] Michal Adamkiewicz, Timothy Chen, Adam Caccavale,
toryforecastingrepresentationforplanningandcontrol. In
Rachel Gardner, Preston Culbertson, Jeannette Bohg, and
Conf.onRobolLearning,2021. 2
MacSchwager. Vision-onlyrobotnavigationinaneuralra-
[16] Justin Kerr, Letian Fu, Huang Huang, Yahav Avigal,
dianceworld. IEEERoboticsandAutomationLetters,7(2):
MatthewTancik,JeffreyIchnowski,AngjooKanazawa,and
4606–4613,2022. 2,3,12
KenGoldberg. Evo-nerf: Evolvingnerfforsequentialrobot
[2] JonathanTBarron,BenMildenhall,MatthewTancik,Peter
graspingoftransparentobjects. InConf.onRobolLearning,
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.
2022. 2
Mip-nerf: Amultiscalerepresentationforanti-aliasingneu-
[17] Adam R Kosiorek, Heiko Strathmann, Daniel Zo-
ralradiancefields. InInt.Conf.Comput.Vis.,pages5855–
ran, Pol Moreno, Rosalia Schneider, Sona Mokra´, and
5864,2021. 2
Danilo Jimenez Rezende. NeRF-VAE: A geometry aware
[3] AngCaoandJustinJohnson. Hexplane: Afastrepresenta-
3dscenegenerativemodel.pages5742–5752,2021.2,3,12
tionfordynamicscenes.InIEEEConf.Comput.Vis.Pattern
[18] Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal,
Recog.,pages130–141,2023. 2
and Antonio Torralba. 3d neural scene representations for
[4] Jinkun Cao, Xin Wang, Trevor Darrell, and Fisher Yu. visuomotorcontrol.InConf.onRobolLearning,pages112–
Instance-awarepredictivenavigationinmulti-agentenviron-
123,2022. 3,12
ments.InIEEEInt.Conf.onRoboticsandAutomation,pages
[19] Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang,
5096–5102.IEEE,2021. 2
DavidJunhaoZhang,JussiKeppo,YingShan,XiaohuQie,
[5] DavidCharatan,SizheLi,AndreaTagliasacchi,andVincent and Mike Zheng Shou. Devrf: Fast deformable voxel ra-
Sitzmann. pixelsplat: 3d gaussian splats from image pairs diance fields for dynamic scenes. In Adv. Neural Inform.
forscalablegeneralizable3dreconstruction,2023. 2 Process.Syst.,pages36762–36775,2022. 2
[6] Felipe Codevilla, Eder Santana, Antonio M Lo´pez, and [20] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and
Adrien Gaidon. Exploring the limitations of behavior Deva Ramanan. Dynamic 3d gaussians: Tracking by per-
cloningforautonomousdriving. InInt.Conf.Comput.Vis., sistentdynamicviewsynthesis,2023. 2
pages9329–9338,2019. 2
[21] Pierre Marza, Laetitia Matignon, Olivier Simonin, and
[7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra- Christian Wolf. Multi-object navigation with dynamically
manan.Depth-supervisednerf:Fewerviewsandfastertrain- learned neural implicit representations. In Int. Conf. Com-
ing for free. In IEEE Conf. Comput. Vis. Pattern Recog., put.Vis.,pages11004–11015,2023. 2
pages12882–12891,2022. 2
[22] Rowan McAllister and Carl Edward Rasmussen. Data-
[8] AlexeyDosovitskiy,GermanRos,FelipeCodevilla,Antonio efficient reinforcement learning in continuous state-action
Lopez,andVladlenKoltun.CARLA:Anopenurbandriving gaussian-pomdps. In Adv. Neural Inform. Process. Syst.,
simulator.InConf.onRobolLearning,pages1–16,2017.5, 2017. 2
11
[23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Representingscenesasneuralradiancefieldsforviewsyn-
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- thesis. InEur.Conf.Comput.Vis.,2020. 2
vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis [24] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexan-
worth16x16words: Transformersforimagerecognitionat der Keller. Instant neural graphics primitives with a mul-
scale. InInt.Conf.Learn.Represent.,2021. 2,3,11 tiresolutionhashencoding.ACMTrans.Graph.,41(4):1–15,
[10] Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, 2022. 2,3,4,11
and Marc Toussaint. Learning multi-object dynamics [25] CharlesPacker,NicholasRhinehart,RowanThomasMcAl-
with compositional neural radiance fields. arXiv preprint lister, Matthew A. Wright, Xin Wang, Jeff He, Sergey
arXiv:2202.11855,2022. 2,3 Levine, and Joseph E. Gonzalez. Is anyone there? learn-
[11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong ingaplannercontingentonperceptualuncertainty. InConf.
Chen, BenjaminRecht, andAngjooKanazawa. Plenoxels: onRobolLearning,2022. 2,5,11,12
Radiance fields without neural networks. In IEEE Conf. [26] CharlesPacker,NicholasRhinehart,RowanThomasMcAl-
Comput.Vis.PatternRecog.,pages5501–5510,2022. 2 lister, Matthew A. Wright, Xin Wang, Jeff He, Sergey
[12] Matthew Hausknecht and Peter Stone. Deep recurrent q- Levine, and Joseph E. Gonzalez. Is anyone there? learn-
learningforpartiallyobservablemdps. InAAAI,2015. 2 ingaplannercontingentonperceptualuncertainty. InConf.
[13] MartiA.Hearst,SusanTDumais,EdgarOsuna,JohnPlatt, onRobolLearning,pages1607–1617,2023. 2
andBernhardScholkopf.Supportvectormachines.IEEEIn- [27] XinleiPan,YurongYou,ZiyanWang,andCewuLu. Virtual
telligentSystemsandtheirapplications,13(4):18–28,1998. torealreinforcementlearningforautonomousdriving.arXiv
6 preprintarXiv:1704.03952,2017. 2
[14] JeffreyIchnowski,YahavAvigal,JustinKerr,andKenGold- [28] ChristosHPapadimitriouandJohnNTsitsiklis. Thecom-
berg. Dex-nerf:Usinganeuralradiancefieldtograsptrans- plexityofmarkovdecisionprocesses. Mathematicsofoper-
parentobjects. arXivpreprintarXiv:2110.14217,2021. 2 ationsresearch,12(3):441–450,1987. 2
9[29] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien uncertaintyawaremotionplanningforautomateddriving.In
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo IEEEIntelligentVehiclesSymposium(IV),2018. 1
Martin-Brualla. Nerfies: Deformableneuralradiancefields. [42] A.Tewari,J.Thies,B.Mildenhall,P.Srinivasan,E.Tretschk,
InInt.Conf.Comput.Vis.,pages5865–5874,2021. 2 W. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S.
[30] KeunhongPark, UtkarshSinha, PeterHedman, JonathanT Lombardi,T.Simon,C.Theobalt,M.Nießner,J.T.Barron,
Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin- G.Wetzstein, M.Zollho¨fer, andV.Golyanik. Advancesin
Brualla, and Steven M Seitz. Hypernerf: A higher- NeuralRendering. Comput.Graph.Forum,2022. 2
dimensionalrepresentationfortopologicallyvaryingneural [43] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde.
radiancefields. ACMTrans.Graph.,2021. 2 End-to-end model-free reinforcement learning for urban
[31] JoellePineau,GeoffGordon,SebastianThrun,etal. Point- drivingusingimplicitaffordances. InIEEEConf.Comput.
basedvalueiteration: Ananytimealgorithmforpomdps. In Vis.PatternRecog.,pages7153–7162,2020. 2
IJCAI,pages1025–1032,2003. 2 [44] Edith Tretschk, Vladislav Golyanik, Michael Zollhoefer,
[32] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Aljaz Bozic, Christoph Lassner, and Christian Theobalt.
FrancescMoreno-Noguer.D-NeRF:NeuralRadianceFields Scenerflow: Time-consistent reconstruction of general dy-
for Dynamic Scenes. In IEEE Conf. Comput. Vis. Pattern namic scenes. In International Conference on 3D Vision
Recog.,2020. 2 (3DV),2023. 2
[33] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali [45] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,
Farhadi. Youonlylookonce: Unified,real-timeobjectde- and Federico Tombari. Sparf: Neural radiance fields from
tection. InIEEEConf.Comput.Vis.PatternRecog., 2016. sparseandnoisyposes. InIEEEConf.Comput.Vis.Pattern
5 Recog.,pages4190–4200,2023. 2
[34] Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A [46] Haithem Turki, Deva Ramanan, and Mahadev Satya-
Wright,RowanMcAllister,JosephEGonzalez,andSergey narayanan. Mega-nerf: Scalableconstructionoflarge-scale
Levine. Contingencies from observations: Tractable con- nerfs for virtual fly-throughs. In IEEE Conf. Comput. Vis.
tingency planning with learned behavior models. In IEEE PatternRecog.,pages12922–12931,2022. 2
Int.Conf.onRoboticsandAutomation,pages13663–13669, [47] LaurensvanderMaatenandGeoffreyHinton. Visualizing
2021. 2 datausingt-SNE. JournalofMachineLearningResearch,
[35] Barbara Roessle, Jonathan T Barron, Ben Mildenhall, 9:2579–2605,2008. 6
Pratul P Srinivasan, and Matthias Nießner. Dense depth [48] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P
priors for neural radiance fields from sparse input views. Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo
In IEEE Conf. Comput. Vis. Pattern Recog., pages 12892– Martin-Brualla,NoahSnavely,andThomasFunkhouser.Ibr-
12901,2022. 2 net: Learning multi-view image-based rendering. In IEEE
[36] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,San- Conf.Comput.Vis.PatternRecog.,pages4690–4699,2021.
jeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy, 2
AdityaKhosla,MichaelBernstein, AlexanderC.Berg, and [49] QiangengXu,ZexiangXu,JulienPhilip,SaiBi,ZhixinShu,
LiFei-Fei.Imagenetlargescalevisualrecognitionchallenge, KalyanSunkavalli,andUlrichNeumann. Point-nerf: Point-
2015. 11 based neural radiance fields. In IEEE Conf. Comput. Vis.
[37] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, PatternRecog.,pages5438–5448,2022. 2
HongwenZhang,andYebinLiu. Tensor4d: Efficientneural [50] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto
4d decomposition for high-fidelity dynamic reconstruction Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Invert-
andrendering. InIEEEConf.Comput.Vis.PatternRecog., ingneuralradiancefieldsforposeestimation. InIEEE/RSJ
pages16632–16642,2023. 2 InternationalConferenceonIntelligentRobotsandSystems
[38] ChengSun,MinSun,andHwann-TzongChen.Directvoxel (IROS),pages1323–1330.IEEE,2021. 2
gridoptimization:Super-fastconvergenceforradiancefields [51] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.
reconstruction. InIEEEConf.Comput.Vis.PatternRecog., pixelnerf: Neural radiance fields from one or few images,
pages5459–5469,2022. 2 2021. 2
[39] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil,NithinRaghavan,UtkarshSinghal,RaviRa-
mamoorthi,JonathanBarron,andRenNg. Fourierfeatures
let networks learn high frequency functions in low dimen-
sionaldomains. InAdv.NeuralInform.Process.Syst.,pages
7537–7547,2020. 2
[40] MatthewTancik,VincentCasser,XinchenYan,SabeekPrad-
han,BenMildenhall,PratulPSrinivasan,JonathanTBarron,
andHenrikKretzschmar. Block-nerf: Scalablelargescene
neuralviewsynthesis. InIEEEConf.Comput.Vis.Pattern
Recog.,pages8248–8258,2022. 2
[41] OmerSahinTasandChristophStiller. Limitedvisibilityand
10A.CARLADatasets effective reconstruction of pose conditioning in the latent
space, and the regularization of latents. Regularization
Acompletefigureoftheactorandegoconfigurationsacross
pushesthelatentstowardGaussiandistributionsandkeeps
scenes and the progression of timestamps for the Single-
the non-expressive latents in an over-parameterized latent
SceneApproachingIntersection,Multi-SceneApproaching
space to be standard normal. This stabilizes the sampling
Intersection,andtheMulti-SceneTwoLaneMergeisvisu-
processandensuresstochasticbehavioroflatentsamplesin
alizedinFig.14.
case of occlusion. To achieve this balance, we use a lin-
ear KLD weight scheduler, where the weight is initialized
B.RelatedWorkComparison
atalowvalueforKLDincrementstartepoch(seeTab.5).
In this section, we draw a comparison between CARFF Thisallowsthemodeltoinitiallyfocusonachievinghighly
andothermethodsthatperformtaskssimilartoourmodel. accurate conditioned reconstructions. The KLD weight is
However,thesemethodsdonotpreciselyalignwiththeob- then steadily increased until KLD increment end epoch is
jectivesweaimtoachieve. Forinstance,whilesomemeth- reached, ensuring probabilistic behavior under partial ob-
odsintegrate3Dreasoning,othersmayomitthisaspect. To servability.
establishafaircomparisonbetweenourmodelandcurrent
C.2.MixtureDensityNetwork
methods, we have conducted an in-depth analysis of their
qualitativedifferences,asdelineatedinTab.4. Weprimar- Themixturedensitynetwork(MDN)takesinthemeanand
ily compare to other NeRF works based on their ability to variancesofthelatentdistributionsq (z |It−1)andout-
ϕ t−1 c
model uncertainty (state and dynamics) and perform fore- puts the estimated posterior distribution as a mixture of
castingintheenvironment.Ourworksurpassesallthelisted Gaussianq′(z |It−1)throughamulti-headedMLP.
ϕ t c
previousworksandisonparwith2D-basedforecastingap-
Architecture: The shared backbone simply contains 2
proachesinfunctionality[25]. Thiscomparisonhighlights
fullyconnectedlayersandrectifiedlinearunits(ReLU)ac-
thatourmodelcomprehensivelyencompassesthekeyqual-
tivation with hidden layer size of 512. Additional heads
itativefactorsthatshouldbepresenttoreasonfromtheoc-
with 2 fully connected layers are used to generate µ and
cludedashumansdo. i
σ2. The mixture weight, π , is generated from a 3 layer
i i
MLPnetwork. WelimitthenumberofGaussians,K =2.
C.ImplementationDetails
Optimization: We train our network for 30,000 epochs
C.1.Pose-ConditionalVAE
usingthebatchsizeof128andaninitialLRof0.005,and
Architecture: We implement PC-VAE on top of a stan- apply LR decay to optimize training. This takes approxi-
dard PyTorch VAE framework. The encoder with convo- mately30minutestotrainutilizingtheGPU.Duringtrain-
lutionallayersisreplacedwithasingleconvolutionallayer ing, the dataloader outputs the means and variances at the
andaVisionTransformer(ViT)Large16[9]pre-trainedon current timestamp and indexed view, and the means and
ImageNet[36].Wemodifyfullyconnectedlayerstoproject variances for the next timestamp, at a randomly sampled
ViToutputofsize1000tomeanandvarianceswithsizeof neighboring view. This allows the MDN to learn how oc-
thelatentdimension,8. Duringtraining,thedataloaderre- cluded views advance into all the possible configurations
turnstheposeofthecameraanglerepresentedbyaninteger from potentially unoccluded neighboring views, as a mix-
value. This value is one-hot encoded and concatenated to tureofGaussian.
the re-parameterized encoder outputs, before being passed Ateachiteration,thenegativelog-likelihoodlossiscom-
to the decoder. The decoder input size is increased to add puted for 1000 samples drawn from the predicted mixture
the number of poses to accommodate the additional pose ofdistributionsq ϕ′(z t|I ct−1)withrespecttothegroundtruth
information. distribution q (z |It). While the MDN is training, addi-
ϕ t c
tional Gaussian noise, given by ϵ ∼ N(0,σ2), is added to
Optimization: We utilize a single RTX 3090 graphics
the means and variances of the current timestamp t − 1,
cardforallourexperiments. ThePC-VAEmodeltakesap-
whereσ ∈[0.001,0.01]. TheGaussiannoiseandLRdecay
proximately 22 hours to converge using this GPU. During
helppreventoverfittingandreducemodelsensitivitytoen-
this phase, we tune various hyperparameters including the
vironmentalartifactslikemovingtrees,movingwater,etc.
latent size, learning rate and KL divergence loss weight to
establishoptimaltrainingtailoredtoourmodel(seeTab.5).
C.3.NeRF
Inordertooptimizeforthevariedactorconfigurationsand
scenarios generated within the CARLA [8] simulator, we Architecture: We implement our NeRF decoder uti-
slightlyadjusthyperparametersdifferentlyforeachdataset. lizing an existing PyTorch implementation of Instant-
Thelearningrate(LR)andKLdivergence(KLD)weight NGP[24]. Weconcatenatethelatentstotheinputsoftwo
are adjusted to find an appropriate balance between the parts of the Instant-NGP architecture: the volume density
11Realistic State Dynamics Code
Method 3D Application Uncertainty Uncertainty Prediction Planning Released
CARFF ✓ ✓ ✓ ✓ ✓ ✓ ✓
NeRF-VAE[17] ✓ ✓
NeRF for Visuomotor Con- ✓ ✓ ✓ ✓
trol[18]
NeRFNavigation[1] ✓ ✓ ✓ ✓
AVP[25] ✓ ✓ ✓ ✓ ✓
Table4. QualitativecomparisonofCARFFtorelatedworks. CARFFaccomplishesallthehighlightedobjectivesasopposedtothe
similarworkslisted.Acheckmarkindicatesthattheassociatedmethodincorporatesthequalitativefeatureineachcolumn,whereasempty
spacesindicatethatthemethoddoesnotaccountforit. Here,3Dreferstomethodsthatreasonina3Denvironmentandperformnovel
viewsynthesis. Realisticapplicationreferstowhetherthemethodhasbeendemonstratedinrealisticandcomplexscenarios. Stateand
dynamicuncertaintyrefertowhetherthemodelpredictsprobabilisticallyundertheseconditions. Predictionreferstoforecastingintothe
future,whileplanningreferstousingmodelpredictionsfordecision-making.
Pose Inputs PC- VAE Decoded Images From Set of New Pose
Figure9. PC-VAEencoderinputs, groundtruthtimestamps, andreconstructions. Theencoderinput, It, amongtheotherground
c
truthimagesI viewedfromcameraposecatdifferenttimestamps,isreconstructedacrossanewsetofposesc′′respectingtimestampt,
c
generatingIt .Thisisafullgridofthereconstructions.
c′′
12Figure10. NeRFgraphicaluserinterface. TheGUIallowsustotoggleandpredictwithaninputimagepath. Theprobeandpredict
functionprobesthecurrentlocationofthecarandpredictsthenext.Thescreenshotissharpenedforvisualclarityinthepaper.
PC-VAEHyperparameters optimization. Inouroptimizationprocedure,weuseanLR
of0.002alongwithanLRdecayandstartwithpre-defined
LatentSize 8
latent samples. Then we slowly introduce the re-sampled
LR 0.004
latents. We believe that this strategy progressively dimin-
KLDWeightStart 0.000001
ishes the influence of a single sample, while maintaining
KLDWeightEnd 0.00001−0.00004*
efficient training. Based on our observations, this strategy
KLDIncrementStart 50epochs
contributestowardsInstant-NGP’sabilitytorapidlyassim-
KLDIncrementEnd 80epochs
ilate fundamental conditioning and environmental recon-
struction, while simultaneously pushing the learning pro-
Table 5. PC-VAE experimental setup and hyperparameters.
cesstobelessskewedtowardsasinglelatentsample.
The main hyperparameters in PC-VAE training on the three
datasetsarelatentsize,LR,andKLDweight. ForKLDschedul-
D.GUIInterface
ing, the KLD increment start refers to the number of epochs at
which the KLD weight begins to increase from the initial KLD
For ease of interaction with our inference pipeline, our
weight. KLDincrementendisthenumberofepochsatwhichthe
NeRF loads a pre-trained MDN checkpoint, and we build
KLDweightstopsincreasingatthemaximumKLDweight. The
a graphical user interface (GUI) using DearPyGUi for vi-
asterisk(*)marksthehyperparameterthatisdataset-dependent.
sualization purposes. We implement three features in the
GUI:(a)predict,(b)probeandpredict,and(c)toggle.
network, σ(x), for the density values, and the color net-
Predict: We implement the function to perform predic-
work, C(r), for conditional RGB generation. While the
tion directly from a given image path in the GUI. We use
overall architecture is kept constant, the input dimensions
thedistributionq (z |It−1)fromPC-VAEencoder,cor-
ofeachnetworkaremodifiedtoallowadditionallatentcon- ϕ t−1 c
responding to the input image It−1, to predict the latent
catenation. c
distribution for the next timestamp q′(z |It−1). This pro-
ϕ t c
cess is done on the fly through the MDN. A sample from
Optimization: Empirically, we observe that it is essen-
thepredicteddistributionisthengeneratedandusedtocon-
tialtotraintheNeRFsuchthatitlearnsthedistributionof
ditiontheNeRF.Thisadvancestheentirescenetothenext
scenes within the PC-VAE latent space. Using only pre-
timestamp.
definedlearnedsamplestotrainmayruntheriskofrelying
onnon-representativesamples.Ontheotherhand,directre- Probe and predict: The sampled latent from the pre-
samplingduringeachtrainingiterationinInstant-NGPmay dicteddistributiondoesnotcorrespondtoasingulardistri-
lead to delayed training progress, due to NeRF’s sensitive butionandhencewecannotdirectlypredictthenexttimes-
1325
20
15
10
5
0
0 20 40 60 80 100120140
Epochs
(a)Single-SceneApproachingIntersection
25
20
Figure 12. Latent sample distribution clustering. The distri-
15
butions of latent samples for the Multi-Scene Two Lane Merge
10 datasetareseparablethrought-SNEclustering. Inthefigure,the
clustersforScene0,Timestamp0andScene1,Timestamp0over-
5 lapindistributionbecausetheyrepresentthesameinitialstateof
theenvironmentunderdynamicsuncertainty.
0
0 20 40 60 80 100120140
Epochs
EncoderArchitectures Train SVM NV
(b)Multi-SceneApproachingIntersection PSNR Accuracy PSNR
25 Multi-SceneApproachingIntersection
20 PC-VAE 26.47 89.17 26.37
PC-VAEw/oCL 26.20 83.83 26.16
15
VanillaPC-VAE 25.97 29.33 25.93
10
PC-VAEw/oFreezing 24.82 29.83 24.78
5 PC-VAEw/MobileNet 19.37 29.50 19.43
VanillaVAE 26.04 14.67 9.84
0
0 20 40 60 80 100120140 Multi-SceneTwoLaneMerge
Epochs
PC-VAE 25.50 88.33 25.84
(c)Multi-SceneTwoLaneMerge
PC-VAEw/oCL 24.38 29.67 24.02
Figure11. AveragetrainPSNRplotforallCARLAdatasets. VanillaPC-VAE 24.75 29.67 24.96
TheplotshowstheincreaseinaveragetrainingPSNRofallimages PC-VAEw/oFreezing 23.97 28.33 24.04
foreachdataset,overtheperiodofthetrainingprocess. PC-VAEw/MobileNet 17.70 75.00 17.65
VanillaVAE 25.11 28.17 8.49
tamp. Tomakeourmodelauto-regressiveinnature,weper-
Table 6. PC-VAE metrics and ablations across Multi-Scene
form density probing. We probe the density of the NeRF
datasets. CARFF’sPC-VAEoutperformsotherencoderarchitec-
atthepossiblelocationcoordinatesofthecartoobtainthe
turesacrosstheMulti-Scenedatasetsinreconstructionandpose-
currenttimestampandscene. Thisisthenusedtomatchthe
conditioning.
latenttoacorrespondingdistributioninthePC-VAEspace.
Thenewdistributionenablesauto-regressivepredictionsus- E.CARFFEvaluation
ingthepredictfunctiondescribedabove.
E.1.Pose-ConditionalVAE
Reconstruction Quality: To analyze the reconstruction
Toggle: TheNeRFgeneratesascenecorrespondingtothe performance of the model during training, we periodically
provided input image path using learned latents from PC- plot grids of reconstructed images. These grids consist
VAE.Whentheinputimageisafullyobservableview,the of (a) randomly selected encoder inputs drawn from the
NeRFrendersclearactorandegoconfigurationsrespecting dataset,(b)thecorrespondinggroundtruthimagesforthose
theinput. Thisallowsustovisualizethesceneatdifferent inputsateach timestampatthesamecamera pose, and(c)
timestampsandindifferentconfigurations. reconstructedoutputsatrandomlysampledposesrespecting
14
RNSPegarevA
RNSPegarevA
RNSPegarevA1 10-foldvalidationonthelatentstocalculatetheaccuracyas
ametricforclustering. SeeTab.6fortheresults.
Beyondseparability,weanalyzetherecallandaccuracy
0.8
of the learned latents directly from PC-VAE under par-
tial and full observations. This achieves very high accu-
0.6 racy even under a large number of samples while retrain-
Accuracy ingdecentrecall,enablingdownstreamMDNtraining.(See
Recall
Fig.13)
0.4
0 10 20 30 40 50
NumSamples E.2.FullyObservablePredictions
(a)ApproachingIntersection
OneofthetasksoftheMDNistoforecastthefuturescene
1
configurations under full observation. We quantitatively
evaluate our model’s ability to forecast future scenes by
0.8 comparing bird’s-eye views rendered from the NeRF with
chosen ground truth images of the scene for the various
timestamps(seeTab.7). Thevaluesarecalculatedanddis-
0.6
playedforallthreedatasets. InTab.7, imagesaremarked
Accuracy
Recall as either toggled (I˜ ) or predicted (Iˆ ). Toggled images
0.4 in the table cannot
bti
e predicted
determti
inistically due to it
0 10 20 30 40 50
NumSamples being the first timestamp in the dataset, or the state of the
(b)TwoLaneMerge previous timestamps across scenes being the same in case
of dynamics uncertainty. Due to the same reason, in the
Figure 13. Multi-Scene dataset accuracy and recall curves Multi-SceneTwoLaneMergeDataset,thereareadditional
fromlearnedlatents. Wetestourframeworkacrossn = 1and boldedPSNRvaluesforthepairs(I ,I˜ )and(I ,I˜ ).
n = 50 samples from PC-VAE’s latent distributions from ego-
t1 t4 t4 t1
centricimageinput. Acrossthenumberofsamplesn,weachieve
an ideal margin of belief state coverage generated under partial
observation(recall),andtheproportionofcorrectbeliefssampled
under full observation (accuracy) for the MDN to learn. As we
significantlyincreasethenumberofsamples, theaccuracystarts
todecreaseduetorandomnessinlatentdistributionresampling.
theinputsceneandtimestamp. Anexamplereconstruction
grid is provided in Fig. 9. The grid enables visual assess-
ment of whether the model is capable of accurately recon-
structingreasonableimagesusingtheencoderinputs, con-
ditionedontheposes. Thisevaluationprovidesuswithvi-
sualevidenceofimprovementinreconstructionquality. We
alsoquantitativelyanalyzetheprogressiveimprovementof
reconstruction through the average PSNR calculated over
thetrainingdata(seeFig.11).
Latent Space Analysis To assess the quality of the la-
tentsgeneratedbyPC-VAE,weinitiallyuset-SNEplotsto
visualizethelatentdistributionsasclusters. Fig.12shows
that the distributions of the latent samples for the Multi-
SceneTwoLaneMergedatasetareseparable. Whilet-SNE
is good at retaining nearest-neighbor information by pre-
serving local structures, it performs weakly in preserving
global structures. Therefore, t-SNE may be insufficient in
capturingthedifferencesindistributionsforallourdatasets.
Instead,wepivottoSupportVectorMachinetoperform
a quantitative evaluation of the separability of the latents.
We utilize a Radial Basis Function (RBF) kernel with the
standard regularization parameter (C = 1). We perform
15
%
%Single-SceneApproachingIntersection
Result I I I I I I I I I I
t1 t2 t3 t4 t5 t6 t7 t8 t9 t10
I˜ 29.01 −5.97 −6.08 −6.52 −6.44 −6.03 −6.31 −6.36 −6.26 −6.28
t1
Iˆ −5.42 27.51 −3.07 −4.67 −4.58 −4.17 −4.43 −4.51 −4.39 −4.39
t2
Iˆ −6.06 −2.81 28.12 −4.47 −4.68 −4.19 −4.05 −4.61 −4.47 −4.52
t3
Iˆ −7.01 −5.37 −5.03 29.40 −4.99 −5.08 −5.03 −5.41 −5.28 −5.32
t4
Iˆ −6.87 −5.2 −4.93 −5.00 29.44 −4.53 −4.46 −5.19 −5.05 −5.09
t5
Iˆ −6.29 −4.55 −4.27 −4.8 −4.24 29.02 −4.02 −4.53 −4.38 −4.44
t6
Iˆ −6.76 −5.05 −4.76 −5.31 −5.14 −4.36 29.50 −4.50 −4.86 −4.93
t7
Iˆ −6.73 −5.02 −4.74 −5.25 −5.10 −4.64 −4.76 29.46 −4.41 −4.86
t8
Iˆ −6.75 −5.00 −4.70 −5.23 −5.07 −4.64 −4.85 −4.52 29.55 −4.42
t9
Iˆ −6.79 −5.06 −4.75 −5.30 −5.15 −4.69 −4.93 −5.01 −4.34 29.55
t10
Multi-SceneApproachingIntersection
Result I I I I I I
t1 t2 t3 t4 t5 t6
I˜ 28.10 −5.24 −5.50 −1.67 −3.29 −3.92
t1
Iˆ −5.23 28.02 −6.11 −4.70 −3.21 −4.84
t2
Iˆ −5.43 −6.03 27.97 −4.85 −4.53 −2.93
t3
I˜ −1.71 −4.73 −5.00 28.26 −2.25 −3.08
t4
I˜ −3.68 −3.24 −4.91 −2.76 28.21 −2.99
t5
Iˆ −4.02 −4.91 −3.27 −3.13 −2.61 28.26
t6
Multi-SceneTwoLaneMerge
Result I I I I I I
t1 t2 t3 t4 t5 t6
I˜ 28.27 −5.31 −6.41 28.23 −4.77 −5.42
t1
I˜ −5.22 28.23 −5.17 −5.27 −2.91 −4.01
t2
Iˆ −6.32 −5.09 28.14 −6.33 −5.01 −4.28
t3
I˜ 28.27 −5.27 −6.37 28.23 −4.72 −5.37
t4
I˜ −4.64 −2.73 −5.01 −4.71 28.08 −5.29
t5
Iˆ −5.32 −4.02 −4.32 −5.33 −5.34 28.17
t6
Table7. CompletePSNRvaluesforfullyobservablepredictionsforallCARLAdatasets. ThetablecontainsPSNRvaluesbetween
thegroundtruthimagesandeitheratoggledimage(markedasI˜ ),orapredictedimage(markedasIˆ ).Toggledorpredictedimagesthat
ti ti
correspondtothecorrectgroundtruthareboldedandhaveasignificantlyhigherPSNRvalue. ThePSNRvaluesforincorrectcorrespon-
dancesarereplacedwiththedifferencebetweentheincorrectPSNRandtheboldedPSNRassociatedwithacorrectcorrespondance.
1617
noitcesretnI
gnihcaorppA
enecS
-elgniS
sah
rotcA
:9
pmatsemiT
8
pmatsemiT
7
pmatsemiT
6 pmatsemiT
5 pmatsemiT
4
pmatsemiT
3
pmatsemiT
2
pmatsemiT
1
pmatsemiT
rotcA
:0
pmatsemiT
noitcesretni
eht
dessorc
ot
tuoba
si
ecnalubma
noitcesretni
ssorc
noitcesretnI
gnihcaorppA
enecS
-itluM
5
pmatsemiT
4 pmatsemiT
3
pmatsemiT
2
pmatsemiT
1 pmatsemiT
0
pmatsemiT
ylno
rac ogE
:2
enecS
htiw
rac
ogE
:1
enecS
ecnalubma
rotca
egreM
enaL
owT
enecS
-itluM
5
pmatsemiT
4 pmatsemiT
3
pmatsemiT
2
pmatsemiT
1 pmatsemiT
0
pmatsemiT
htiw
rac ogE
:2
enecS
htiw
rac
ogE
:1
enecS
ecnalubma
gnivom
-tsaf
ecnalubma
gnivom
-wols
snoitarugfinocracogednarotcaehT.stesataDegreMenaLowTenecS-itluMdna,noitcesretnIgnihcaorppAenecS-itluM,noitcesretnIgnihcaorppAenecS-elgniS.41erugiF neebevahnoitcesretnIgnihcaorppAenecS-itluMehtrofsracehtfosrolocehT.esoparemacelgnisatadezilausiverastesatadALRACeerhtehtfosenecsdnaspmatsemitehtrof
.repapehtniytiralclausivdnatsartnocretaergrofdefiidomylthgils