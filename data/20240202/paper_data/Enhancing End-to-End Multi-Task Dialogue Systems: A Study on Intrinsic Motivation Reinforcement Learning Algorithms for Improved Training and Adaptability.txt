1
Enhancing End-to-End Multi-Task Dialogue
Systems: A Study on Intrinsic Motivation
Reinforcement Learning Algorithms for
Improved Training and Adaptability
1st Navin Kamuni 2nd Hardik Shah 3rd Sathishkumar Chintala 4th Naveen 5th Sujatha Alla
AI ML M.Tech Department of Department of IT Kunchakuri Old Dominion
BITS Pilani WILP IT Fisher Investments Department of IT University,
USA Rochester USA Eudoxia Resarch Norfolk, Virginia
navin.kamuni@gmail.com Institute of sathishkumarchintala9@gmail.com University USA
Technology
USA salla001@odu.edu
USA
Knav18@gmail.com
hds6825@rit.edu
with intrinsic rewards for novelty and encouraging policy
Abstract— End-to-end multi-task dialogue systems are usually exploration beyond simple models such as Proximal Policy
designed with separate modules for the dialogue pipeline. Among Optimization.
these, the policy module is essential for deciding what to do in response
to user input. This policy is trained by reinforcement learning
II. BACKGROUND: TASK-ORIENTED DIALOGUE SYSTEMS
algorithms by taking advantage of an environment in which an agent
receives feedback in the form of a reward signal. The current dialogue Unlike conversational AI or chatbots, task-oriented dialogue
systems, however, only provide meagre and simplistic rewards. systems are computer programs that employ natural language
Investigating intrinsic motivation reinforcement learning algorithms is to assist users in achieving certain goals. Intent analysis,
the goal of this study. Through this, the agent can quickly accelerate domain recognition, and slot filling are some of the phases these
training and improve its capacity to judge the quality of its actions by
systems go through when users supply the information needed
teaching it an internal incentive system. In particular, we adapt
for things like taxi reservations. Until the user's wants are
techniques for random network distillation and curiosity-driven
satisfied or the discussion is over, the process keeps going.
reinforcement learning to measure the frequency of state visits and
Component interactions are coordinated by the conversation
encourage exploration by using semantic similarity between
utterances. Experimental results on MultiWOZ, a heterogeneous manager's conversation State Tracker (DST) and policy. To
dataset, show that intrinsic motivation-based debate systems help express user goals, these systems make use of datasets that
outperform policies that depend on extrinsic incentives. By adopting contain transcripts of human conversations divided into
random network distillation, for example, which is trained using domains, intents, and slots.
semantic similarity between user-system dialogues, an astounding The user simulator (US), dialogue manager, DST, policy,
average success rate of 73% is achieved. This is a significant
natural language generation (NLG), natural language
improvement over the baseline Proximal Policy Optimization (PPO),
understanding (NLU), and dialogue manager are important
which has an average success rate of 60%. In addition, performance
parts. The US is available in agenda-based and model-based
indicators such as booking rates and completion rates show a 10% rise
forms, with the latter providing domain scalability and
over the baseline. Furthermore, these intrinsic incentive models help
improve the system's policy's resilience in an increasing amount of mirroring user goals [9], [10]. Using models such as RNNs,
domains. This implies that they could be useful in scaling up to settings LSTMs, and BERT-NLU for intent categorization and slot
that cover a wider range of domains. value extraction, NLU focuses on transforming user utterances
into conversation acts [11], [12]. With innovations like TripPy
Index Terms— Dialogue Systems, MultiWOZ, Proximal Policy and TRADE, DST manages multi-domain complexity.
Optimization, Reinforcement Learning, Web Accessibility Algorithms are used by policy components, which are essential
for system-user interactions, in a Markov Decision Process
I. INTRODUCTION framework [13]. NLG generates sentences directly from data,
Human-computer interaction is a major area of focus for AI ranging from template-based to corpus-based approaches [17],
research, especially when it comes to creating conversational [18]. Notably, issues with sparse reward and learning efficiency
systems [1]–[8]. Reward scarcity in reinforcement learning are addressed by pre-training protocols and various reward
poses a significant obstacle to the effectiveness and complexity functions in task-oriented dialogue systems, like Q-learning and
of these systems, particularly task-oriented dialogue systems. In least-squares optimization [14] to [16]. Metrics for evaluation,
order to address these problems, this work introduces language such as information and success rates, evaluate the capabilities
input into training and investigates strategies for lowering of the system; however, difficulties in conducting a thorough
reward sparsity. It explores random network distillation and assessment still exist, particularly when poor US can mask
curiosity-based reinforcement learning, providing dialogues policy performance [19]. The present study highlights the2
intricate components, procedures, and assessment challenges B. Convlab-2
associated with task-oriented dialogue systems. It specifically An improvement to Convlab [27], Convlab-2 is an open-source
focuses on user interaction simulation, natural language dialogue systems library created by [12] that facilitates the
understanding, dialogue management, and the generation of quick construction, training, and assessment of dialogue
human-like responses. systems. It supports four main datasets: DealOrNoDeal [28],
MultiWOZ [26], CamRest676 [17], and CrossWOZ [29].
III. RELATED WORKS Convlab-2 provides contemporary models for dialogue system
A number of novel approaches to improving discussion policies components such as Natural Language Understanding (NLU),
have been introduced in the field of dialogue systems through Natural Language Generation (NLG), and Dialogue State
innovative studies. While study [21] created PETAL, a transfer Tracking (DST). It supports both monolithic and pipeline
reinforcement learning framework for personalizing dialogue dialogue systems. To reduce problems from upstream and
systems by leveraging individual interactions and collective downstream processes, the NLU module is not included in this
user data, study [20] introduced ACER, which uses study. Instead, utterance creation is done using a basic template
reinforcement learning for efficient training of dialogue NLG module. Additionally embedded is a simple rule-based
systems. Additionally, [22] introduced an algorithm that DST module. ConvLab-2 is made to make it simple to set up
enhances the exploration efficiency of deep Q-learning agents and evaluate different approaches to dialogue systems, such as
in dialogue systems. [23] carried out a thorough evaluation of reinforcement learning methods. It has an environment module
deep learning-based dialogue systems, covering model kinds, that links the system for reinforcement learning policy training
approaches, and future expectations. In order to improve and the user simulator (US), and it is built on OpenAI Gym
learning from user interactions, study [24] presented a hybrid [30]:
learning approach that combines imitation and reinforcement 1. To sample an initial belief state, which includes the
learning. Furthermore, [25] developed a dialogue management randomly chosen first user goal, the function env.reset() is
architecture based on feudal reinforcement learning. This utilized.
architecture addresses scalability concerns by segregating 2. The function next state, reward, done = env.step(action) is
decision-making into two categories: master policy selection used to take actions, retrieve instant rewards, advance the
and primitive action choices. MDP to the next state, and determine whether the
These methods do have some drawbacks, though. Long-term discussion has ended.
dependencies and complex dialogue contexts may be difficult Moreover, independent of the environment variable used, the
for ACER [20]. PETAL [21] makes the assumption that evaluator module can be activated within the environment to
individual interactions are good target domains for evaluate the efficacy of the interaction. When the dialogue fails
personalization, which may mean that it ignores a range of user (done = False), the reward function is intended to offer a -1
preferences and the evolution of dialogue context. The reward; conversely, when the dialogue succeeds (done = True
exploration efficiency algorithm proposed by [22] needs to be and evaluator.task success = True), the reward function is
validated in a variety of user interactions and confronts scaling intended to assign a positive reward of L. In ConvLab-2, the
issues. Generalization and adaptation problems limit the maximum number of turns in a conversation instance is denoted
potential of deep learning-based dialogue systems [23]. by L and is set to 40. Each phase's actions, which are often the
Sensitive to the quality of the training data, the hybrid learning Policy module's output, must be sent to the environment. The
approach [24] needs to strike a balance between imitation and following policies are included in ConvLab-2:
reinforcement learning. Finally, due to presumptions regarding 1. A rule-based policy in which each user dialogue act is
domain ontology and information sharing, the Feudal met with a tailored system reaction.
reinforcement learning-based architecture [25] may face 2. A Maximum Likelihood Estimation (MLE) policy that
difficulties in real-world deployment. For dialogue systems to
uses imitation to directly learn state-action pairs from
be robust and be used in real-world scenarios, these issues must
the dataset.
be resolved.
3. PPO [31], Guided Dialog Policy Learning (GDPL)
[12], and REINFORCE [32] are the three
IV. EMPIRICAL STUDY
reinforcement learning policies.
A. MultiWOZ Dataset
About 8000 human-to-human dialogue examples from an The following performance metrics can be calculated using
information center, spanning seven categories such as ConvLab-2's analyzer module:
attraction, hotel, and restaurant, are included in the MultiWOZ
dataset [26]. Task-oriented dialogue system research often 1. Complete rate: The ratio of finished conversations to
makes use of it. The conversations, which range in type from total conversations shows that, regardless of
single to multi-domain, typically center around two domains; correctness, the system gave the user all the
over 30% of the conversations include a single domain, while information they asked for:
three domains are the least prevalent. Annotations for starting
aims, user utterances, and system conversation acts are included 𝑐𝑜𝑚𝑝𝑙𝑒𝑡𝑒𝑑 𝑑𝑖𝑎𝑙𝑜𝑔𝑢𝑒𝑠
𝐶𝑜𝑚𝑝𝑙𝑒𝑡𝑒 𝑟𝑎𝑡𝑒 = (1)
in the dataset. 𝑡𝑜𝑡𝑎𝑙 𝑑𝑖𝑎𝑙𝑜𝑔𝑢𝑒𝑠3
2. Success rate: The ratio of successful dialogues to all Sample action a given s;
i i
dialogues, where success denotes that all user requests Compute embeddings 𝑓 𝜃𝑖(𝑠) 𝑎𝑛𝑑 𝑓 𝜃′(𝑠 𝑖)
were appropriately handled by the system: Optimise the loss||𝑓′(𝑠𝑖 )−𝑓𝜃(𝑠)||2w.r.t
𝜃𝑖 𝑖 𝑖
𝜃 𝑏𝑦 𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡 𝑑𝑒𝑠𝑐𝑒𝑛𝑡;
𝑠𝑢𝑐𝑐𝑒𝑠𝑠𝑓𝑢𝑙 𝑑𝑖𝑎𝑙𝑜𝑔𝑢𝑒𝑠
𝑆𝑢𝑐𝑐𝑒𝑠𝑠 𝑟𝑎𝑡𝑒 = (2) C o m p u t e i n t r i n s i c r e w a r d 𝑟 𝑖 𝑛 𝑡 = 𝜂 | | 𝑓 ′ ( 𝑠 ) −
𝑡𝑜𝑡𝑎𝑙 𝑑𝑖𝑎𝑙𝑜𝑔𝑢𝑒𝑠 𝑖 𝜃𝑖 𝑖
𝑓𝜃(𝑠)||2;
𝑖
3. Book rate: The ratio of booked dialogues that are 𝜂 ⟵(1−𝛼)𝜂
successful to all of the dialogues that are bookable: for end
END
𝑠𝑢𝑐𝑐𝑒𝑠𝑠𝑓𝑢𝑙 𝑏𝑜𝑜𝑘𝑖𝑛𝑔𝑠
𝐵𝑜𝑜𝑘 𝑟𝑎𝑡𝑒 = (3)
𝑡𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑏𝑜𝑜𝑘𝑎𝑏𝑙𝑒 𝑑𝑖𝑎𝑙𝑜𝑔𝑢𝑒𝑠
V. MODEL IMPLEMENTATION ENVIRONMENT
ConvLab-2 is an open-source library that has been updated
C. Algorithms
since its February 2020 launch and is used in this study. It
PPO algorithm in the context of reinforcement learning, involves changes to a number of different parts, including
drawing parallels with Trust Region Policy Optimization models, NLG, and user agents. Models are trained and assessed
(TRPO). PPO aims to control policy parameter updates to in a manner consistent with reinforcement learning techniques,
prevent drastic changes between iterations, akin to TRPO's meaning that test and evaluation data are treated equally.
approach of limiting the Kullback-Leibler (KL) divergence ConvLab-2's DST uses a belief state encoder to process user
between the updated and old policies. However, PPO goals and conversation history into fixed-length arrays. This
introduces a clipped surrogate function to address instability covers the agent's unprocessed actions as well. Table II lists
issues associated with TRPO. The surrogate function constrains common hyperparameters for all models, while Table I shows
the ratio of probabilities for actions under the updated and old data overviews and output array sizes. The batch size and
policies, ensuring it remains within a predefined interval. discount factors were not changed during the hyperparameter
tuning process; they remain at standard values. Due to resource
In exploration strategies, traditional methods like 𝜀-greedy and and time constraints—which are explained in more detail in the
entropy bonuses with more sophisticated approaches. Pseudo- following section—each experiment was restricted to one
count models, introduced by [33], estimate state visitation million training steps.
frequencies using density models, extending to high- TABLE I
dimensional spaces with hash mapping. Intrinsic motivation THE INPUT AND OUTPUT OF THE BELIEF STATE AND ACTION
methods [34], [35] introduce novelty as a measure, rewarding ENCODERS
Input (structure) Output (structure, dimension)
less frequently visited states. In [36], intrinsic motivation is
Belief state (dict) (array, 340)
quantified as information gain via KL divergence. However,
Action (dict) (array, 209)
traditional reinforcement learning algorithms face challenges
due to reward sparsity, prompting the introduction of intrinsic TABLE II
rewards. Episodic memory [37] appends large intrinsic reward HYPERPARAMETERS
states to facilitate learning from novel states. Curiosity-driven Hyperparameter Value
Number of training steps 1M
learning [38] employs an Intrinsic Curiosity (IC) module,
Discount factor 0.99
comprising forward and inverse estimation models, to generate Dialogue sample batch size 32
step-wise intrinsic rewards. Random Network Distillation Optimizer batch size 32
(RND) [39] measures prediction error for novel states, Optimizer model AdamW ([31])
Activation function ReLU
encouraging exploration. In order to predict state features and
actions, IC combines forward and inverse dynamics models,
using feature vector differences as intrinsic rewards. It seeks to A. PPO
balance extrinsic and intrinsic rewards in order to maximize The PPO algorithm is implemented similarly to Convlab-2,
policy actions. RND promotes exploration by rewarding except for the hyperparameters. Following an analysis of the
prediction errors and employing random neural networks to learning performance at different learning rates, optimizer
predict states. As their pseudocode and Algorithm 1 models, and algorithm-specific hyperparameters, these
demonstrate, in reinforcement scenarios, both IC and RND can modifications were made. In this implementation, the actor-
integrate with or separate from policy learning. critic architecture consists of two neural networks; hence, since
there are no shared parameters, two different optimizers are
Algorithm 1 used. Tables III and IV list the hyperparameters for the actor
START and critic models of PPO. The gradient for the actor loss is
Randomly initialize target and predictor network trimmed at a value of the same policy and is trained for five
parameters, 𝜃 0′ and 𝜃 0; iterations of every sampled batch of conversations. All
Freeze 𝜃 0′; exploration comes from the models of intrinsic motivation
Get initial state s 0; because the entropy benefit is ignored. These PPO
Set 𝜂 and 𝛼; hyperparameters don't change while the RND and IC models
for i = 0, 1, 2,…..do are being trained.
TABLE III4
HYPERPARAMETERS FOR PPO (ACTOR) 3) Random Network Distillation General Hyperparameter
Hyperparameter Value Table VII lists the hyperparameters used for RND in this work,
Actor input dimension 340
which includes both modes that use DAs and utterances. There
Actor hidden dimension 100
are notable differences in some hyperparameters between RND
Actor output dimension 209
Learning rate 5e-6 (DAs) and RND (Utt), all of which have been painstakingly
Surrogate clip (η) 0.1 adjusted via a grid search procedure.
GAE (λ) 0.95
TABLE VII
TABLE IV
HYPERPARAMETERS FOR RND (DAS AND UTT) TRAINING
HYPERPARAMETERS FOR PPO (CRITIC)
Hyperparameter Value
Hyperparameter Value
Learning rate (both) 1e-3
Critic input dimension 340 Warmup episodes (DAs) 100
Critic hidden dimension 50
Warmup episodes (Utt) 200
Critic output dimension 1
Moving average steps period (DAs) 2
Learning rate 1e-5 Moving average steps period (Utt) 10
B. Random Network Distillation Update rounds (DAs) 5
Algorithm 1 provides two operational modes for the Update rounds (Utt) 1
RND gradient clip value (DAs) 10
functionality of the RND model, guiding its implementation.
rint multiplier initial value (Utt) 1.0
The model can calculate step-wise intrinsic rewards using NLU rint multiplier initial value (both) 5.0
(called Utt, for example, RND (Utt)) or dialogue acts (called rint multiplier initial value (both) 0.001
rint linear annealing steps (DAs) 20000
DAs, such RND (DAs)) that are exchanged between the user
rint linear annealing steps (Utt) 50000
and the system. The RND model's two modes go through warm-
up stages before working together to train the policy.
C. Intrinsic Curiosity
1) Random Network Distillation from Dialogue Acts There are two modes of operation for intrinsic curiosity (IC):
When using dialogue acts to produce intrinsic rewards, two IC (DAs) and IC (Utt). Whereas IC (Utt) processes the tuple
feed-forward neural networks are used: one for the target (state, action, future state) as dialogue acts, IC (DAs) employs
model, which has fixed parameters throughout training, and NLG to generate utterances from conversation acts. As outlined
another for the predictor model. Table V contains information in Algorithm 1, IC provides two training methods: combined
about the hyperparameters for these networks. optimization with the policy and pre-training with a predefined
policy for intrinsic incentives. The specific hyperparameters
TABLE V needed for each of these approaches are listed in the Results
HYPERPARAMETERS FOR TARGET NETWORKS AND RND (DAS)
section. The IC models are feed-forward neural networks with
PREDICTION
ReLU activations; IC (Utt) uses a BERT base uncased
Hyperparameter Value
Input dimension 418 tokenizer, and IC (DAs) uses a two-layer network to encode
Hidden dimension 524 dialogue actions. In pre-training, the intrinsic reward is
Output 340
determined by scaling the forward model's loss with
dimension
hyperparameter η.
TABLE VIII
2) Random Network Distillation from Utterances
HYPERPARAMETERS FOR IC (DAS AND UTT) MODEL
When using utterances for intrinsic reward creation, the Hyperparameter Value
predictor and target models are trained using a pre-trained Number of steps for pre-train 1000
BERT model. Although the entire pre-trained BERT model can Learning rate (pre-train) 1e-3
Learning rate (joint) 1e-5
be optimized during training, the effect of freezing all BERT
Update rounds 1
layers—aside from the last layer, the classification head, which
Gradient clipping parameter 10
is learned simultaneously with the policy—was studied. Even 𝜂 0.01
with comparable outcomes, the earlier approach requires far 𝛽 𝐷𝐴𝑠 0.2
𝛽 0.2
more training wall time. As a result, it was decided to train the 𝑈𝑡𝑡
𝛽 0.8
BERT classifier head in all scenarios. Table VI provides a full 𝑗𝑜𝑖𝑛𝑡
𝜆 0.5
breakdown of the hyperparameters for the BERT-based 𝑝𝑜𝑙
Inverse model hidden dimension 524
predictor and target models. Forward model hidden dimension 524
State encoder output dimension 256
TABLE VI State encoder max length 200
HYPERPARAMETERS FOR RND (U ) PREDICTOR AND TARGET
TT
NETWORKS VI. RESULT ANALYSIS
Hyperparameter Value
Since Convlab-2's training does not provide direct performance
Critic input dimension BertForSequenceClassification
Critic hidden dimension bert-base-uncased monitoring, an Analyzer wrapper is used to evaluate dialogue
Critic output dimension 512 sample size in real-time using a hyperparameter. Although it
Learning rate 340
takes longer overall, simultaneous training and evaluation are
essential for hyperparameter tuning. Less dialogue samples
increase training speed but decrease accuracy. 1000 random5
dialogues are used in this study to assess performance in a performance metric computations. Future research aims to
consistent and trustworthy manner. improve user interfaces for a wider audience, customize
interactions to users' accessibility needs, and use intrinsic
A. Baseline Models
motivation reinforcement learning to multi-task dialogue
Within its system policy module, Convlab-2 provides four systems for web accessibility [41, 42].
reinforcement learning algorithms; Table IX shows the average
success rates. Although GDPL [40] is regarded as the best VIII. DECLARATIONS
model in terms of performance on the MultiWOZ dataset in
[12], the authors of Convlab-2 report that PPO has a success A. Funding: No funds, grants, or other support was received.
rate of 0.74 and GDPL has a measly 0.58, with the latter just
B. Conflict of Interest: The authors declare that they have no
outperforming MLE and REINFORCE. Additionally, using the
known competing for financial interests or personal
same dataset, the success rate stated in [40] is 0.59. This
relationships that could have appeared to influence the work
suggests that there may be problems with specific library
reported in this paper.
components or that the default training parameters
recommended by Convlab-2 aren't the best ones. C. Data Availability: Data will be made on reasonable
request.
TABLE IX D. Code Availability: Code will be made on reasonable request.
CONVLAB-2 BASELINE MODEL SUCCESS RATE
Model Success rate
MILE 0.56
REFERENCES
REINFORCE 0.54
PPO 0.74 [1] N. Marwah, V. K. Singh, G. S. Kashyap, and S. Wazir, “An analysis
GDPL 0.58 of the robustness of UAV agriculture field coverage using multi-
agent reinforcement learning,” International Journal of Information
Technology (Singapore), vol. 15, no. 4, pp. 2317–2327, May 2023,
B. Effect of the Number of Sampled Dialogues in Evaluation doi: 10.1007/s41870-023-01264-0.
[2] G. S. Kashyap, K. Malik, S. Wazir, and R. Khan, “Using Machine
Here, we provide empirical evidence in support of the
Learning to Quantify the Multimedia Risk Due to Fuzzing,”
qualitative insight that any taught policy can be evaluated by
Multimedia Tools and Applications, vol. 81, no. 25, pp. 36685–
fixing the number of dialogue samples, or n eval. The success rate 36698, Oct. 2022, doi: 10.1007/s11042-021-11558-9.
and standard deviation during the evaluation of a PPO model [3] G. S. Kashyap, A. E. I. Brownlee, O. C. Phukan, K. Malik, and S.
that was trained for 1 million steps while modifying n are Wazir, “Roulette-Wheel Selection-Based PSO Algorithm for Solving
eval the Vehicle Routing Problem with Time Windows,” Jun. 2023,
shown in Fig. 1. For every n , twenty calculations of the
eval Accessed: Jul. 04, 2023. [Online]. Available:
success rate were made. The results show that the performance https://arxiv.org/abs/2306.02308v1
measures vary very little when n = 1000 is set. As a result, [4] G. S. Kashyap, D. Mahajan, O. C. Phukan, A. Kumar, A. E. I.
eval
Brownlee, and J. Gao, “From Simulations to Reality: Enhancing
we determine n = 1000 for all experiments to provide
eval Multi-Robot Exploration for Urban Search and Rescue,” Nov. 2023,
consistent and extremely trustworthy evaluations of model
Accessed: Dec. 03, 2023. [Online]. Available:
performance during training. https://arxiv.org/abs/2311.16958v1
[5] M. Kanojia, P. Kamani, G. S. Kashyap, S. Naz, S. Wazir, and A.
Chauhan, “Alternative Agriculture Land-Use Transformation
Pathways by Partial-Equilibrium Agricultural Sector Model: A
Mathematical Approach,” Aug. 2023, Accessed: Sep. 16, 2023.
[Online]. Available: https://arxiv.org/abs/2308.11632v1
[6] H. Habib, G. S. Kashyap, N. Tabassum, and T. Nafis, “Stock Price
Prediction Using Artificial Intelligence Based on LSTM– Deep
Learning Model,” in Artificial Intelligence & Blockchain in Cyber
Physical Systems: Technologies & Applications, CRC Press, 2023,
pp. 93–99. doi: 10.1201/9781003190301-6.
[7] S. Wazir, G. S. Kashyap, and P. Saxena, “MLOps: A Review,” Aug.
2023, Accessed: Sep. 16, 2023. [Online]. Available:
Fig. 1. Effect of the number of sampled dialogues in evaluation https://arxiv.org/abs/2308.10908v1
[8] S. Wazir, G. S. Kashyap, K. Malik, and A. E. I. Brownlee, “Predicting
VII. CONCLUSION & FUTURE WORKS the Infection Level of COVID-19 Virus Using Normal Distribution-
Based Approximation Model and PSO,” Springer, Cham, 2023, pp.
Using the MultiWOZ dataset and Convlab-2, this study 75–91. doi: 10.1007/978-3-031-33183-1_5.
examined intrinsic incentive models with an emphasis on RND [9] L. El Asri, J. He, and K. Suleman, “A sequence-to-sequence model
and IC strategies to improve dialogue system performance. The for user simulation in spoken dialogue systems,” in Proceedings of
the Annual Conference of the International Speech Communication
system policy performance was significantly enhanced by
Association, INTERSPEECH, Jun. 2016, vol. 08-12-Sept, pp. 1151–
RND, especially with its utterance-based approach, which 1155. doi: 10.21437/Interspeech.2016-1175.
increased the success rate from 60% to 73% and increased the [10] F. L. Kreyssig, I. Casanueva, P. Budzianowski, and M. Gašić,
completion and book rates by 10%. However, the study's “Neural user simulation for corpus-based policy optimisation for
spoken dialogue systems,” in SIGDIAL 2018 - 19th Annual Meeting
dependence on Convlab-2 and the ease of use of the dialogue
of the Special Interest Group on Discourse and Dialogue -
system components presented challenges. Future developments Proceedings of the Conference, May 2018, pp. 60–69. doi:
will involve upgrading the user simulator, implementing more 10.18653/v1/w18-5007.
complex NLG and DST algorithms, and resolving problems [11] G. Mesnil et al., “Using recurrent neural networks for slot filling in
spoken language understanding,” IEEE Transactions on Audio,
with Convlab-2, such as user objective assignment and
Speech and Language Processing, vol. 23, no. 3, pp. 530–539, Mar.6
2015, doi: 10.1109/TASLP.2014.2383614. [27] S. Lee et al., “ConvLab: Multi-domain end-to-end dialog system
[12] Q. Zhu et al., “ConvLab-2: An open-source toolkit for building, platform,” in ACL 2019 - 57th Annual Meeting of the Association for
evaluating, and diagnosing dialogue systems,” in Proceedings of the Computational Linguistics, Proceedings of System Demonstrations,
Annual Meeting of the Association for Computational Linguistics, Apr. 2019, pp. 64–69. doi: 10.18653/v1/p19-3011.
Feb. 2020, pp. 142–149. doi: 10.18653/v1/2020.acl-demos.19. [28] M. Lewis, D. Yarats, Y. N. Dauphin, D. Parikh, and D. Batra, “Deal
[13] S. Singh, M. Kearns, D. Litman, and M. Walker, “Reinforcement or no deal? End-to-end learning for negotiation dialogues,” in
learning for spoken dialogue systems,” in Advances in Neural EMNLP 2017 - Conference on Empirical Methods in Natural
Information Processing Systems, 2000, vol. 12, pp. 956–962. Language Processing, Proceedings, Jun. 2017, pp. 2443–2453. doi:
[14] G. Gordon-Hall, P. J. Gorinski, G. Lampouras, and I. Iacobacci, 10.18653/v1/d17-1259.
“Show Us the Way: Learning to Manage Dialog from [29] Q. Zhu, K. Huang, Z. Zhang, X. Zhu, and M. Huang, “CrossWOZ: A
Demonstrations,” Apr. 2020, Accessed: Dec. 09, 2023. [Online]. Large-Scale Chinese Cross-Domain Task-Oriented Dialogue
Available: https://arxiv.org/abs/2004.08114v1 Dataset”, doi: 10.1162/tacl.
[15] P. H. Su et al., “Learning from real users: Rating dialogue success [30] G. Brockman et al., “OpenAI Gym,” Jun. 2016, Accessed: Dec. 03,
with neural networks for reinforcement learning in spoken dialogue 2023. [Online]. Available: https://arxiv.org/abs/1606.01540v1
systems,” in Proceedings of the Annual Conference of the [31] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
International Speech Communication Association, INTERSPEECH, “Proximal Policy Optimization Algorithms,” Jul. 2017, Accessed:
Aug. 2015, vol. 2015-Janua, pp. 2007–2011. doi: Dec. 09, 2023. [Online]. Available:
10.21437/interspeech.2015-456. https://arxiv.org/abs/1707.06347v2
[16] P. Wesselmann, Y. C. Wu, and M. Gasic, “Curiosity-driven [32] R. J. Williams, “Simple statistical gradient-following algorithms for
Reinforcement Learning for Dialogue Management,” in ICASSP, connectionist reinforcement learning,” Machine Learning, vol. 8, no.
IEEE International Conference on Acoustics, Speech and Signal 3–4, pp. 229–256, May 1992, doi: 10.1007/bf00992696.
Processing - Proceedings, May 2019, vol. 2019-May, pp. 7210– [33] M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton,
7214. doi: 10.1109/ICASSP.2019.8683033. and R. Munos, “Unifying count-based exploration and intrinsic
[17] T. H. Wen, M. Gašić, N. Mrkšić, P. H. Su, D. Vandyke, and S. Young, motivation,” in Advances in Neural Information Processing Systems,
“Semantically conditioned lstm-based Natural language generation 2016, vol. 29, pp. 1479–1487.
for spoken dialogue systems,” in Conference Proceedings - EMNLP [34] A. G. Barto, “Intrinsic motivation and reinforcement learning,” in
2015: Conference on Empirical Methods in Natural Language Intrinsically Motivated Learning in Natural and Artificial Systems,
Processing, Aug. 2015, pp. 1711–1721. doi: 10.18653/v1/d15-1199. vol. 9783642323, Springer-Verlag Berlin Heidelberg, 2013, pp. 17–
[18] O. Vinyals and Q. Le, “A Neural Conversational Model,” Jun. 2015, 47. doi: 10.1007/978-3-642-32375-1_2.
Accessed: Dec. 09, 2023. [Online]. Available: [35] P. Y. Oudeyer, F. Kaplan, and V. V. Hafner, “Intrinsic motivation
https://arxiv.org/abs/1506.05869v3 systems for autonomous mental development,” IEEE Transactions
[19] J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young, “A survey on Evolutionary Computation, vol. 11, no. 2, pp. 265–286, Apr. 2007,
of statistical user simulation techniques for reinforcement- learning doi: 10.1109/TEVC.2006.890271.
of dialogue management strategies,” Knowledge Engineering [36] G. Ostrovski, M. G. Bellemare, A. Van Den Oord, and R. Munos,
Review, vol. 21, no. 2, pp. 97–126, Jun. 2006, doi: “Count-based exploration with neural density models,” in 34th
10.1017/S0269888906000944. International Conference on Machine Learning, ICML 2017, Jul.
[20] G. Weisz, P. Budzianowski, P. H. Su, and M. Gasic, “Sample 2017, vol. 6, pp. 4161–4175. Accessed: Dec. 09, 2023. [Online].
Efficient Deep Reinforcement Learning for Dialogue Systems with Available: https://proceedings.mlr.press/v70/ostrovski17a.html
Large Action Spaces,” IEEE/ACM Transactions on Audio Speech [37] N. Savinov et al., “Episodic curiosity through reachability,” in 7th
and Language Processing, vol. 26, no. 11, pp. 2083–2097, Nov. International Conference on Learning Representations, ICLR 2019,
2018, doi: 10.1109/TASLP.2018.2851664. Oct. 2019. Accessed: Dec. 09, 2023. [Online]. Available:
[21] K. Mo, Y. Zhang, S. Li, J. Li, and Q. Yang, “Personalizing a dialogue https://arxiv.org/abs/1810.02274v5
system with transfer reinforcement learning,” in 32nd AAAI [38] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven
Conference on Artificial Intelligence, AAAI 2018, Apr. 2018, vol. 32, Exploration by Self-supervised Prediction.” PMLR, pp. 2778–2787,
no. 1, pp. 5317–5324. doi: 10.1609/aaai.v32i1.11938. Jul. 17, 2017. Accessed: Sep. 17, 2023. [Online]. Available:
[22] Z. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, and L. Deng, “BBQ- https://proceedings.mlr.press/v70/pathak17a.html
networks: Efficient exploration in deep reinforcement learning for [39] Y. Burda, H. Edwards, A. Storkey, and O. K. Openai, “Exploration
task-oriented dialogue systems,” in 32nd AAAI Conference on by random network distillation,” in 7th International Conference on
Artificial Intelligence, AAAI 2018, Apr. 2018, vol. 32, no. 1, pp. Learning Representations, ICLR 2019, Oct. 2019. Accessed: Dec. 09,
5237–5244. doi: 10.1609/aaai.v32i1.11946. 2023. [Online]. Available: https://arxiv.org/abs/1810.12894v1
[23] J. Ni, T. Young, V. Pandelea, F. Xue, and E. Cambria, “Recent [40] R. Takanobu, H. Zhu, and M. Huang, “Guided dialog policy learning:
advances in deep learning based dialogue systems: a systematic Reward estimation for multi-domain task-oriented dialog,” in
survey,” Artificial Intelligence Review, vol. 56, no. 4, pp. 3055–3155, EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in
Aug. 2023, doi: 10.1007/s10462-022-10248-8. Natural Language Processing and 9th International Joint
[24] B. Liu, G. Tür, D. Hakkani-Tür, P. Shah, and L. Heck, “Dialogue Conference on Natural Language Processing, Proceedings of the
learning with human teaching and feedback in end-To-end trainable Conference, Aug. 2019, pp. 100–110. doi: 10.18653/v1/d19-1010.
task-oriented dialogue systems,” in NAACL HLT 2018 - 2018 [41] H. Shah, “Advancing Web Accessibility -- A guide to transitioning
Conference of the North American Chapter of the Association for sign Systems from WCAG 2.0 to WCAG 2.1,” Artificial Intelligence,
Computational Linguistics: Human Language Technologies - Soft Computing and Applications, pp. 233–245, Nov. 2023, doi:
Proceedings of the Conference, Apr. 2018, vol. 1, pp. 2060–2069. 10.5121/csit.2023.132218.
doi: 10.18653/v1/n18-1187. [42] H. Shah, “Harnessing customized built-in elements -- Empowering
[25] I. Casanueva et al., “Feudal reinforcement learning for dialogue Component-Based Software Engineering and Design Systems with
management in large domains,” in NAACL HLT 2018 - 2018 HTML5 Web Components,” Artificial Intelligence, Soft Computing
Conference of the North American Chapter of the Association for and Applications, pp. 247–259, Nov. 2023, doi:
Computational Linguistics: Human Language Technologies - 10.5121/csit.2023.132219.
Proceedings of the Conference, Mar. 2018, vol. 2, pp. 714–719. doi:
10.18653/v1/n18-2112.
[26] P. Budzianowski et al., “MultiWoz - A large-scale multi-domain
wizard-of-oz dataset for task-oriented dialogue modelling,” in
Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2018, Sep. 2018, pp. 5016–
5026. doi: 10.18653/v1/d18-1547.