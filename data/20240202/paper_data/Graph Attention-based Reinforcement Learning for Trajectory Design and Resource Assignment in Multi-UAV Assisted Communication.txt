1
Graph Attention-based Reinforcement Learning for
Trajectory Design and Resource Assignment in
Multi-UAV Assisted Communication
Zikai Feng, Student Member, IEEE, Di Wu, Member, IEEE, Mengxing Huang, Member, IEEE, Chau Yuen,
Fellow, IEEE
Abstract—In the multiple unmanned aerial vehicle (UAV)- the latest version brings substantial enhancements on mobile
assisteddownlinkcommunication,itischallengingforUAVbase devices [1]. However, due to the fast growth of emerging
stations (UAV BSs) to realize trajectory design and resource
terminals, ground backbone networks are facing serious chal-
assignmentinunknownenvironments.Thecooperationandcom-
lenges of data congestion [2] [3]. On account of geographical
petitionbetweenUAVBSsinthecommunicationnetworkleadsto
aMarkovgameproblem.Multi-agentreinforcementlearningisa environment, some outlying regions still lack sufficient wire-
significantsolutionfortheabovedecision-making.However,there less coverage. Traditional ground communication networks
arestillmanycommonissues,suchastheinstabilityofthesystem are increasingly difficult to meet the demand for high-quality
and low utilization of historical data, that limit its application.
services in wireless communication. Therefore, research must
In this paper, a novel graph-attention multi-agent trust region
be dedicated to the new generation of 6G communication [4].
(GA-MATR) reinforcement learning framework is proposed to
solve the multi-UAV assisted communication problem. Graph The integration of air-space-ground communication is a
recurrent network is introduced to process and analyze complex significant direction of 6G, which provides a solution for the
topology of the communication network, so as to extract useful currentinsufficientwirelesscommunicationcoverage[5].One
information and patterns from observational information. The
ofthesetechnologiesisthewidespreaduseofunmannedaerial
attention mechanism provides additional weighting for conveyed
vehicle (UAV) [6] [7].
information, so that the critic network can accurately evaluate
the value of behavior for UAV BSs. This provides more reliable UAVs hold tremendous promise due to affordability, strong
feedbacksignalsandhelpstheactornetworkupdatethestrategy flexibility, and easy deployment. In the field of wireless
moreeffectively.Ablationsimulationsindicatethattheproposed communication, they often play roles of aerial access nodes
approach attains improved convergence over the baselines. UAV
to support the downlink communication [8]. In remote and
BSs learn the optimal communication strategies to achieve their
disaster-prone areas, UAVs provide new ideas for inadequate
maximum cumulative rewards. Additionally, multi-agent trust
regionmethodwithmonotonicconvergenceprovidesanestimated orineffectiveinfrastructure[9];indenselypopulatedandheav-
Nash equilibrium for the multi-UAV assisted communication ily trafficked hotspots, UAVs are utilized to enhance wireless
Markov game. coverage and increase network throughput. Additionally, they
IndexTerms—UAV-assistedCommunication,TrajectoryDesign can also work in traffic control [10], industrial Internet of
and Resource Assignment, Graph Attention Network, Multi- Things[11],emergencyrescue[12],andotherfields[13][14].
agent Reinforcement Learning, Game Theory In order to fully leverage the advantages of drones in
communication,researchershaveinvestedasignificantamount
of interest in the design of UAV-assisted communication
I. INTRODUCTION
systems, such as resource deployment [15] [16] [17] [18],
IN modern communication, 5G mobile network is being trajectoryoptimization[19][20],thejointpowerandtrajectory
implemented globally. Compared to its previous iteration, optimization [21] [22], as well as its further combination
with other wireless technologies [23] [24]. For example, non-
*Correspondingauthor:MengxingHuangandChauYuen.
convex optimization theory is used in [25] to describe UAV
ThisworkissupportedbytheScientificResearchFundProjectofHainan
University (KYQD(ZR)-21007), the Natural Science Foundation of Hainan based trajectory optimization and resource allocation, and a
Province(621QN212),andtheNationalNaturalScienceFoundationofChina selectable optimized method is proposed to solve the lower
(62062030). This work is also supported under the State Scholarship Fund,
boundproblem.Ithasabetterefficiencythantheupperbound
ChinaScholarshipCouncil(202207565036).
Zikai Feng, Di Wu, and Mengxing Huang are with the School of In- methods. In [26], a block coordinate descent method was
formation and Communication Engineering, Hainan University, and State proposed to decompose non-convex problems such as task al-
Key Laboratory of Marine Resource Utilization in South China Sea,
location,resourcescheduling,andtrajectoryplanningofUAVs
Haikou, 570228, China (13523011824@163.com, hainuwudi@163.com, and
huangmx09@163.com). communication.Lagrangiandualityandconvexapproximation
Zikai Feng is also with the Engineering Product Development Pillar, are used to deal with the obtained two sub-problems.
SingaporeUniversityofTechnologyandDesign,487372,Singapore.
Unfortunately, most of the related work mainly focus on
Di Wu is also with the Department of Automation, Shanghai Jiao Tong
University,Shanghai,200240,China,andPRISMALab,UniversityofNaples the design and optimization of deterministic systems. In these
FedericoII,Naples,80125,Italy. work, the information of the system, such as the parameters
Chau Yuen is with the School of Electrical and Electronic Engi-
of channels, are known [27]. When it comes to the dynamics
neering, Nanyang Technological University, 639798, Singapore (e-mail:
chau.yuen@ntu.edu.sg). and unknowns of communication scenarios under real-world
4202
naJ
13
]AM.sc[
1v08871.1042:viXra2
condition, it may not be practical to simply use the traditional additional information transmission and weighting, so that the
optimization methods. criticnetworkcanmoreaccuratelyevaluatethebehaviorvalue
With the realistic demands of high dimensional, nonlinear, for UAV BSs. This provides more reliable feedback signals
and unknown environments in communication systems, the and helping the actor network to update the strategy more
traditional optimization method is becoming more and more effectively. Besides, the standard deviation regularization is
difficult to meet the decision-making [28]. It is necessary to introduced to avoid significant resource allocation gaps be-
use machine learning methods to explore new paths. Entities tween the paired GUs, so as to ensure the fairness in resource
withlearningabilitycanbettercopewiththedynamicchanges allocation. UAV BSs in the communication network employ
of the communication network. Among the most encouraging the enhanced method to optimize their own communication
solutions, reinforcement learning is a promising one [29]. strategy, so as to maximize the cumulative rewards. In addi-
The core idea of reinforcement learning is that the agent tion, with monotonic improvement guarantee of the MARL
optimize its strategy by learning interactive data with the framework, the joint optimal strategies of UAV BSs gradually
environment, thereby obtaining the maximum cumulative re- converge. This provides an approximate Nash equilibrium
ward[30].Recently,thesuccessofsingle-agentreinforcement solution for the UAV-assisted communication Markov game.
learning has spread to multi-agent scenarios [31] [32]. For The main contributions of this work are as bellow:
instance, multi-agent deep deterministic policy gradient algo- 1) A novel graph-attention multi-agent trust region rein-
rithm overcomes the obstacle of policy instability caused by forcement learning framework is proposed to address the
dynamic environmental [33]. Similarly, multi-agent proximal trajectory design and resource assignment in UAV-assisted
policy optimization algorithm extends the single-agent policy communication.Graphneuralnetworkisintroducedtoprocess
gradient methods to the field of multi-agent systems and and analyze complex topology and extract useful information
inherits its excellent convergence performance [34]. from communication data. The attention mechanism provides
Some research have already focused on the multi-agent re- additional weighting for transferred data, so that the critic
inforcementlearning(MARL)basedcommunicationnetworks network can more accurately evaluate the value of behavior
[35]. In [36], a combination of game theory and MARL is for UAV BSs. It provides more reliable feedback signals and
utilized to facilitate route mapping for UAV BSs. It intro- helps the actor network update parameters more effectively.
duces low computational complexity methods and distributed 2) The standard deviation regularization is introduced to
characteristics, and designs a dynamic multi-target sensing avoidsignificantresourceallocationgapsbetweenGUsserved
framework for unmanned aerial vehicles. The multi-agent by the same UAV BS. This ensures the fairness in resource
actor-critictechniqueisemployedin[37]toaddressthepower allocation for UAV BSs.
assignment within UAV-enhanced mobile networks, aiming to 3) Game theory is employed to model the UAV-assisted
maximize the capacity. communicationsystem,andthedownlinkcommunicationsys-
However, the above MARL algorithms share certain com- temwithmultipleUAVBSsisstatedasaMarkovgame.With
mon problems, such as the instability of the system and low monotonic convergence guarantee of multi-agent trust region
utilization of historical data. This is because, in multi-agent approach, the joint optimal polices of UAV BSs gradually
systems, the relationships and topology between agents are converge. This provides an approximate solution of Nash
usually dynamically changing. There are rich interactions and equilibrium.
dependencies between agents, as well as traditional optimiza- In this paper, the system representation and problem de-
tion methods cannot directly handle these structured data. scriptionaredescribedinSection2.Thegraph-attentionmulti-
Therefore, how to effectively capture the complex interaction agent trust region framework for the multi-UAV assisted
relationshipsbetweenagentsandaggregateglobalinformation communication Markov game can be found in Section 3.
in the topology network, so that each agent can obtain a The ablation simulations are analyzed in Section 4. Section
comprehensiveviewoftheentireenvironment,iscrucialforall 5 summarizes this paper.
agents. Besides, in large-scale communication scenarios, the
dynamicenvironmentinformationandtheinteractioninforma-
II. SYSTEMMODELINGANDPROBLEMDESCRIPTION
tion between agents are exponential. It is also a challenge for
agentstoextractthemostimportantinformationfrommassive This paper considers a multi-UAV assisted communication
information and eliminate useless information. system in the D region, D ⊂R3. In this system, M UAV BSs
Motivated by the above analysis, a novel graph-attention provide wireless communication services for N GUs. Define
multi-agent trust region reinforcement learning framework, the set of UAV BSs as M = {1,2,...,M}, and define the
GA-MATR, is proposed in this paper to execute the trajec- set of GUs as N = {1,2,...,N}. The schematic diagram of
tory design and resource assignment in multi-UAV assisted UAV-assisted communication system is shown as Figure 1.
communication system. Because of the cooperation and com- In this paper, all GUs move randomly on the two-
petition between UAV base stations (UAV BSs), the downlink dimensional(2D)ground,andeachUAVBSfliesinthethree-
communication system with multiple UAV BSs and ground dimensional (3D) space and gives wireless services for GUs
users (GUs) is modeled as a Markov game. After that, graph using frequency division multiple-plexing (FDM). At each
recurrent neural network is introduced to analyze and process time slot, UAV BSs will first independently and sequentially
complex communication topology data and extract useful in- decidetopairingwithGUs,andbroadcastthepairingdecision
formation and patterns. The attention mechanism can provide and position information to other UAV BSs. After that, they3
will design trajectories based on the positions of the paired where f is the carrier frequency; d = h/sin(φ (t))
c m,n(t) m,n
GUs, allocate power and bandwidth resources in real-time. is the straight-line distance from UAV BS m to GU n, and c
In order to reduce interference and improve the quality of is the speed of light. P (t) is the transmit power allocated
m,n
communication, each GU will only be paired with one UAV by UAV BS m to GU n at time t.
BS. This can also ensure that every GU can receive assisted Then, the received signal power available at GU n is
wirelesscommunicationservices.Inthispaper,toavoidwast- Prec(t)=P (t)−PL (t).
m,n m,n m,n
ing resources, we assume each UAV BS will serve at least Considering the flight safety of UAV BSs, their minimum
one GU. Multi-beamforming is used in power allocation. The flight altitude is set to 5 meters. In Cartesian coordinate sys-
total power and bandwidth resources for each UAV BS are tem,thevelocityofUAVBSmisv =(v ⇀ i+v ⇀ j+v ⇀ k).
m m m m
denoted as P total and B total respectively in this paper, which The location update equation of UAV BS m is:
are fixed and the same. Besides, each UAV BS has four
different resource allocation options to power and bandwidth P om s(t+1)=P om s(t)+v mt ∗∆t, ∆t=0.1s (3)
respectivelyateachtimeslot.EachUAVBSaimstomaximize
GUs move randomly at a speed lower than UAV BSs. At
the mean throughput of the paired GUs, as well as ensuring
every time slot, the GU will be paired by one UAV BS and
fairness.
be provided communication services. In addition, each UAV
BS must serve at least one GU at each time slot.
Then, the connection relationship between UAV BS m and
GU n is represented by binary variables σ (t) ∈ {0,1}.
m,n
When GU n is paired by UAV BS m, σ (t) is set to 1,
m,n
otherwise it is set to 0.
(cid:26)
1, if GU nis paired byUAVBSm
σ (t)= (4)
m,n 0, otherwise
In the air-ground communication system, the action
a ∈ A of UAV BS m include the moving vec-
m m
tors v , the option of whether to pair with every
m
GU onehot1 , the power allocation options onehot2 ,
m m
and the bandwidth allocation options onehot3 , i.e,
m
a ={v ,onehot1 ,onehot2 ,onehot3 }. The dimen-
m m m m m
sion of onehot1 is the number of GUs N.
m
For UAV BS m, there are four power allocation options
and four bandwidth allocation options respectively, which are
shown as below:
Fig. 1: Schematic diagram of UAV-assisted communication 1) Allocate communication resources (power and band-
system width) based on randomly generated proportions.
P (t)=p˜ (t)∗P ,
m,n mn total
(cid:26)
random(0,1), ifσ (t)=1
A. Air-ground communication Model
p˜ mn(t)=
0 ,ifσ
mm ,, nn
(t)=0
, (5)
s.t.
(cid:80)N
p˜ (t)=1.
Statistical probability-based air-ground communication i=0 mn
models have been applied in many works. Specifically, the
down-link channel can be separated into Line of Sight (LoS) B m,n(t)=˜b mn(t)∗B total,
(cid:26)
and Non-Line of Sight (NLoS) [38]. ˜b (t)= random(0,1) ,ifσ m,n(t)=1 , (6)
Inthiswork,thecoordinatesoftheUAVBSm,andtheGU mn 0 ,ifσ m,n(t)=0
n are expressed as Pm =(x ,y ,z ) and Pn =(x ,y ,0) s.t. (cid:80)N ˜b (t)=1.
os m m m os n n i=0 mn
respectively, where Pn =(x ,y ,0), x ,x ∈(−100,100),
os n n m n where p˜ (t) is the proportion of power obtained by GU n
and z ∈(0,100). Due to the limitations of the flight altitude mn
m fromUAVBSmtothetotalpower;B (t)isthebandwidth
ofUAVBSs,thedownlinkchannelistypicallyLoS.Therefore, m,n
obtained by GU n from UAV BS m; ˜b (t) is the proportion
at time t, the total path loss from UAV BS m to GU n in the mn
of bandwidth obtained by GU n from UAV BS m to the total
LoS link can be described as
bandwidth.
PL (t)=lLoS(t)+ςLoS (1) 2) Allocate communication resources evenly based on the
m,n m,n
total number of paired GUs.
where lLoS(t) is the instant path loss from UAV BS m to GU
m,n 1
nontheLoSlink,andςLoSistheadditionallossesunderthe P (t)= P (7)
m,n s (t) total
free space transmission. m
(cid:18) (cid:19)
4πf d (t) 1
lLoS(t)=20lg c m,n (2) B (t)= B (8)
m,n c m,n s (t) total
m4
where s (t) presents the number of GUs paired by UAV BS to the current topology and channel conditions, so that each
m
m at time slot t. GU can enjoy the same communication services as much as
3) Fully allocate communication resources based on the possible. For every UAV BS, the observation information in
distance. each time slot includes the positions of the paired GUs, the
P positions of other UAV BSs, and pairing information of other
P (t)= total (9)
m,n d (t)α UAV BSs.
mn
To ensure fairness in resource allocation for UAV BSs, this
paperintroducestheindexofstandarddeviationregularization
B
B m,n(t)= d to (t ta )l α (10) ε m as an evaluation index for UAV BS m
mn
(cid:115)
4) Allocate communication resources based on the distance
ε =
(cid:80)N n=1(c m,n−c)
, c =
1 (cid:88)N
c , n∈N (14)
between UAV BS m and GU n, plus the weighted sum of m n m N n=1 m,n
random proportions.
wherec istheaveragecommunicationrateofallGUsserved
m
P (t)=c ∗ P total +c ∗p˜ (t), c +c =1 (11) by UAV BS m. The smaller index ε m, the higher the fairness
m,n 1 d mn(t)α 2 mn 1 2 of resource allocation for UAV BS m, and the smaller the
difference in communication rates of GUs.
B (t)=c ∗ B total +c ∗˜b (t), c +c =1 (12) This paper defines the optimization goal of each UAV BS
m,n 3 d (t)α 4 mn 3 4 as:
mn
(cid:88) (cid:88) (cid:88)
For example, if N = 6 and onehot1 = [1,0,0,0,1,0], max c (t)− ε
m m,n m
UAV BS m selects the GU 1 and the GU 5. If onehot2 m = t∈Tn∈sm(t) t∈T
[1,0,0,0], UAV BS m selects the power allocation method 1; s.t.σ (t)∈{0,1},n∈N
m,n
if onehot3 m = [0,1,0,0], UAV BS m selects the bandwidth (cid:88)
σ (t)=1,n∈N
allocation method 2. m,n
Define n as the power spectral density, and b (t) is the m∈M
0 m,n (cid:88)
bandwidth resources form UAV BS m to GU n. Based on σ m,n(t)≥1,
the Shannon channel capacity theory, the communication rate n∈N
(cid:88)
provided by UAV BS m to GU n is shown as σ (t)=s (t), (15)
m,n m
Prec(t) n∈N
c (t)=σ (t)B (t)log (1+ m,n ) (13) (cid:88)
m,n m,n m,n 2 n B (t) p m,n(t)=P total,
0 m,n
n∈sm(t)
(cid:88)
B. Optimization Problem Description b m,n(t)=B total,
In this work, in order to maximize the throughput of n∈sm(t)
b (t)≥b ,p (t)≥p ,n∈N
the multi-UAV assisted communication system, as well as m,n min m,n min
ensuring the fairness in resource allocation, each UAV BS ∀t∈T,m∈M
needs to optimize the pairing strategy firstly, then to optimize
where p (t) and b (t) are the transmit power and band-
the flight trajectory, the power allocation strategy, and the m,n m,n
widthresourcesallocatedbyUAVBSmtoGUnrespectively;
bandwidth allocation strategy. Take T as the time period
max b is the minimum divisible bandwidth; p is the mini-
of one episode, and take t as the time interval for a decision min min
mum divisible power.
t∈T ={1,2,...,T }.
max
Figure 2 gives the time series decision-making process of
C. Multi-UAV Assisted Communication Markov Game
themulti-UAVassistedcommunicationnetwork.Ateachtime
slot t, it is first necessary for each UAV BS to decide to pair Consideringtheuncertaintyoftheenvironmentinthispaper,
whichGUs.Specifically,UAVBSswillselecttopairwithGUs intelligentUAVBSsneedtomakedecisionsineverytimeslot,
in sequence. In order to reduce interference and improve the and these decisions will affect the future time slot. Therefore,
quality of communication, each GU will only be paired with the Markov framework of sequential decision is adopted.
one UAV BS. The GU that has been paired with the previous 1) Markov Game
UAV BSs is no longer available to the next UAV BSs, which In a dynamic environment involving multiple UAV BSs,
can avoid repeated pairing. After determining the GUs to be each UAV BS sequentially makes decisions through interac-
paired, the UAV BS will broadcast the pairing decision and tionswiththeunknownsurrounding.Then,theprocessofjoint
position information to all other UAV BSs. In this paper, we decision-making involving multiple participants is commonly
assume each UAV BS will serve at least one GU, so as to referred to a stochastic game [39], alternatively defined a
avoid wasting resources. After that, each UAV BS will adjust Markov game [40].
it’s own trajectory in real-time based on the current positions Definition 1MarkovGameexpandstheconceptofMarkov
of the paired GUs (since GUs are moving as well) to reduce Decision Process into the multi-agent scenarios [41]. In this
the overall path loss. At the same time, each UAV BS will work, the problem of multi-UAV assisted communication is
adjust the power and bandwidth allocation strategy according described as a tuple of Markov game.5
Fig. 2: The time series decision-making process with three UAV BSs and nine GUs
N: the quantity of UAV BSs. Nash Equilibrium (NE) is a key notion in game theory and
S: the state set of UAV BSs. is first proposed by John Nash in the 1950s [42]. It describes
A:thejointactionsetofallUAVBSs,A:=A ×···×A . a scenario in which multiple parties participate in decision-
1 N
P:S×A→∆(S):whenUAVBSstakejointactionsa∈A, making.
thestatestransfertothenextstates’subjecttotheprobability VariousapproachesexistforaddressingMarkovgames,and
P. Nash equilibrium stands out as one of the most prominent
γ ∈[0,1] is the discount factor. solutions.
For UAV BS i: The following gives the definition of NE.
o is the observation. Lemma 1. In Markov game, Nash equilibrium is a set of
i
π (a |o ) is the action. strategies that meet the following attributes: no participant
i i i
r : S ×A → R is the reward function, rt is the instant can independently alter their own strategy to achieve achieve
i i
reward. greater profits.
The UAV BS engage with the environment following this The Markov game discussed in this work is a non-
protocol: at time slot t, based on the current state s ∈ S, cooperative game. In this non-cooperative game, NE indicates
t
UAV BS i takes an action ai ∈ Ai with its policy πi(·|s ). that every participant is rational. In order to distinguish UAV
t t
UAV BSs’ joint action a = (a1,...,an) ∈ A corresponds to BS i and other UAV BSs, the tuple of (·i,·−i) is introduced
t t t
the joint policy π(·|s ) = (cid:81)n πi(·i|s ). Then, the UAV BS to describe the variables. Given the optimal strategies π∗ of
t i=1 t −i
receives a return r = r(s ,a ) ∈ R, and moves to the next other UAV BSs, every rational UAV BS will not individually
t t t
state s subject to probability P(s |s ,a ). changetheoptimalstrategyπ∗.Inotherwords,eachUAVBS
t+1 t+1 t t i
The definitions of the state value and the state-action value has already made the optimal decision under the strategy of
are established: V (s) = E [(cid:80)∞ γtr |s = s] others.
and Q (s,a) =
Eπ a0:∞∼ [π (cid:80),s1∞:∞∼ γP
tr
|st=0
=
s,t
a
0
= a]. Then,(π∗,π∗ )composesaNashequilibriumforthemulti-
π s1:∞∼P,a1:∞∼π t=0 t 0 0 i −i
The formulation of the advantage function is expressed as UAV assisted communication Markov game G. Its mathemat-
A (s,a)=Q (s,a)−V (s). ical form is represented as:
π π π
The immediate reward for UAV BS m is set to: R (π∗,π∗ )≥R (π ,π∗ ), i∈N (18)
i i −i i i −i
r =
1 (cid:88)sm(t)
c (t)−ε ∗λ (16)
m s m(t) i=1 m,n m III. GRAPHATTENTION-BASEDTRUSTREGIONMARL
where λ is coefficient of the standard deviation regularization
SOLUTIONFORUAV-ASSISTEDCOMMUNICATION
index.
MARKOVGAME
Entitiesaimtoenhancetheirmovementstrategiesinpursuit In this section, we first introduce the proposed graph
of maximizing their overall returns. Furthermore, the descrip- attention-based trust region MARL algorithm, then analyze
tion of the optimization model is shown as: the computational complexity of the proposed algorithm and
benchmarkalgorithms,andfinallydiscussthesolutionofequi-
maxR m[a m,m∈M] (17)
librium for the UAV-assisted communication Markov game.
where R m = (cid:80) ∞ t=0γtr m(t) represents the cumulative A. Graph Attention-based Trust Region MARL
reward of the UAV BS m; γ is the discount factor. Trustregionreinforcementlearning(TRRL)isatrustregion
2) Nash Equilibrium basedpolicyoptimizationalgorithm,whichaimestosolvenon6
convex, high-dimensional problems [43]. The TRRL method This type of update ensures a monotonic enhancement for
can mathematically prove convergence under certain condi- strategy, i.e., J(π )>J(π ).
k+1 k
tions. For example, the monotonic convergence of Trust Re- 2) Trust region-MARL with monotonic convergence
gionPolicyOptimization(TRPO)reinforcementlearningalgo- This work presents a solution for Markov games using
rithmhasbeenstrictlyprovedinmathematics[44].Therefore, MARL, as there exists a natural integration between game
compared to some traditional gradient optimization methods, theory and MARL. In this section, the trust region algorithm
TRRL usually has better stability and convergence. is expand into the field of MARL.
Unfortunately, in terms of multi-agent scenario, the nature Acorrding to [45], the combined advantage can be decou-
of monotonic improvement is no longer simple and applica- pled as the sum of individual advantages. This decomposition
ble, and it mainly faces the challenges of instability, high- mechanism of advantage values provides key support for
dimensional state space and non-stationary. ensuring the monotonicity of subsequent sequential policy
This is because that policy updates for UAV BSs may lead updates.
to rapid changes of the multi-agent environment, leading to Corresponding to Theorem 1, in MARL, with the updates
instability in the training process. The TRPO algorithm itself ofπim =argmax[Li1:m(πi1:m−1,πim)−CDmax(πim,πim)],
isrelativelysensitivetostability.Inmulti-agentenvironments,
k+1
πim
πk k+1 KL k
we have
due to dynamic changes in the policies of other UAV BSs,
the algorithm may be more prone to falling into an unstable J(π )≥L (π )−CDmax(π ,π )
k+1 πk k+1 KL k k+1
state. Besides, the overall state space is often of substantial n
≥L (π )− (cid:80) CDmax(πim,πim )
size.Thisleadstoanexplosioninthedimensionsoftheprob- πk k+1 KL k k+1
m=1
l oe fm o, pi tn imcr ie za as tii on ng .t Ih ne rd ei sf pfi oc nu sl ety toan td hec seom cp hu alt la et nio gn ea sl
,
rc eo sm eap rl ce hx eit ry
s
=J(π K)+
(cid:80)n
(Li π1 k:m(π ki1 +:m 1−1,π kim +1)
m=1
have proposed many improved and variant TRPO algorithms, −CDmax(πim,πim )) (21)
KL k k+1
s pu oc lih cyas oh pe tit mer io zg ae tin oe nou [s 4- 6a ]g ,e en tt cT .,R aP imO e[ d45 a] t, em nhu alt ni ca ig ne gnt thp ero ax di am pa t-l
≥J(π K)+
(cid:80)n
(Li π1 k:m(π ki1 +:m 1−1,π kim)
m=1
abilityandperformanceofTRPOinmulti-agentenvironments. −CDmax(πim,πim))
KL k k
However, multi-agent reinforcement learning is still an active (cid:80)n
=J(π )+ 0=J(π )
research field, and more work is needed to solve its unique K K
m=1
challenges and problems.
In this paper, based on the stable convergence capability This proves that trust region achieves monotonic improve-
of trust region MARL, we further propose a novel graph ment in MARL. With the above theorem, the monotonic im-
attention-based trust region MARL framework, witch can provement property of TRPO has been successfully extended
address the instability of the system and low utilization of and introduced into MARL.
historical data for the UAV-enhanced communication task. 3) Graph attention network (GAN)
The monotonic improvement property of graph attention trust GANembedstheattentionmechanismintothegraphneural
region MARL have been justified in theory. There is no network [48]. In GAN, an undirected graph H < η,ζ >
need for UAV BSs to exchange parameters, and it is also no contains a vertex ξ for each agent i = 1,2,...,n and a set
necessity for limiting assumptions regarding the separability of undirected edges{i,j} ∈ ζ between connected vertices η i
of the value function. and η j.
1) Trust region reinforcement learning algorithms Attentionmechanismreferstomappingasetofqueriesand
The single-agent trust region method, such as TRPO, was key values to generate an output value. The key value in the
proposedtoguaranteethemonotonicimprovementJ ineach queryfunctionshouldbeassignedthroughanattention(weight
π
iteration. The following gives its description. allocation) mechanism. This can reflect which information
Theorem 1 [47]. Let π denotes the present policy and π¯ needs more attention at a certain moment. The functional
represents the candidate policy. expression is provided as follows:
There have L (π¯) = J + E [A (s,a)] and
π π s∼ρ,a∼π¯ π qκT
D Kma Lx(π,π¯)=max sD KL(π(·|s),π¯(·|s)). Then, the following Attention(q,κ,µ)=soft(√ )µ (22)
inequality holds d κ
J(π¯)≥L π(π¯)−CD Kma Lx(π,π¯) (19) where q is the query; κ and µ form a key-value pair; d κ is
the columns of the q, d matrix, i.e., the vector dimension.
κ
where C = 4γmaxs,a|Aπ(s,a)|. GAN integrates the attention mechanism with the graph
(1−γ)2
Asthegapbetweenthecurrentstrategyπ andthecandidate structure. When calculating the information of each node,
strategyπ¯ narrows,UAVBSsthatonlyinvolvethestatedistri- additional information of other nodes are introduced. Besides,
bution of the current strategy L π(π¯) will become increasingly weights are assigned through an attention network to display
accurate estimates of actual performance indicators J(π¯). theimpactofothernodesontheirown.Theweightexpression
Based on this, an iterative trust region method is formulated. is:
The strategy update formula is as follows:
exp(LeakyReLU(ϖT[Wh ||Wh ]))
π k+1 =arg πmax(L πk(π)−CD km lax(π k,π)) (20) ϖ ij = (cid:80) k∈Niexp(LeakyReLU(ϖT[Wi h i||Wj h j])) (23)7
Fig. 3: Framework of graph attention-based multi-agent trust region reinforcement learning
where ϖ and W are parameters; h presents eigenvectors of Algorithm 1 Graph-Attention Multi-Agent Trust Region Al-
i
information for node. ϖ is a weight scalar, which reveals gorithm
ij
the importance of node η i to node η j. 1: Initialize the joint policy π 0 =(π 01,...,π 0n)
4)GraphattentionbasedtrustregionMARLframework 2: for k = 0, 1, ..., K do
Basedonthemonotonicimprovementcapabilityoftrustre- 3: Encode the observation information o(s,a) as a graph
gionMARL,inordertofurtheraddresstheinstabilityandlow matrix H<η,ζ >
data utilization caused by dynamic environment, we propose 4: Compute attention value Attention(q,κ,µ)
a novel graph attention based trust region MARL framework 5: Compute the advantage function:
to coup with the multi-UAV assisted communication. A (Attention(q,κ,µ))
πk
On one hand, the use of graph recurrent network (GRN) 6: Compute Q=max s,a|A πk(s,a)| and C = (14 −γ γε
)2
helps to model and handle complex relationships between 7: Draw the arrangement i 1:n of UAV BSs randomly
intelligent UAV BSs. Due to the interaction and dependency 8: for m = 1 : n do
relationships between UAV BSs in multi-agent environments, 9: Update π kim
+1
= argmax πim[Li π1 k:m(π ki1 +:m 1−1,πim)−
GRN can effectively capture these relationships. By using CDmax(πim,πim)]
KL k
GRN for representation learning, the observation information 10: end for
of UAV BSs can be mapped to a shared representation space, 11: end for
thereby better expressing the relationships between UAV BSs
and GUs. In addition, the graph structure in multi-agent envi-
ronmentsisusuallydynamicallychangingwiththeactionsand Firstly,theconnectioninformationofintelligentUAVBSsand
interactions of the UAV BSs. GRN has the ability to process GUsareencodedintoagraphmatrixH(η,ζ).Then,thegraph
dynamic graphs and can adaptively update graph structures matrix and environmental information are assembled into the
and node representations, thereby better reflecting changes in attention network Attention(q,κ,µ) for feature extraction.
states and relationships between UAV BSs and GUs. Finally, they are input into the critical network for value
On the other hand, the main idea of combining attention evaluation.
mechanism with MARL is to help critic networks selectively
focus on interactions or important features related to the B. Complexity Analysis
currenttask.Bylearningattentionweights,criticnetworkscan This paper compares the performance of the baseline
focusmoreattentiononUAVBSsthatcontributetothecurrent MATR, the baseline IPPO [49], the baseline HATRPO [45],
decision based on task requirements, thereby improving the the MATR with single graph mechanism (Graph MATR), the
accuracy and generalization ability of state or action value MATR with single attention mechanism (Attention MATR),
functions. and the proposed Graph-Attention MATR. Meanwhile, the
Figure 3 gives the framework of graph attention-based computational complexity of these algorithms are compared,
multi-agent trust region reinforcement learning. As shown in which are summarized in Table I. The computational com-
Figure3,basedontheactor-criticstructure,thegraphattention plexity mainly includes the complexity per layer, the input /
network is introduced to revise the critic network for the trust output demensions of actor network, and the input / output
region MARL. Before the state information o (s,a) of the demensions of the critic network.
i
intelligent agent is input into the critical network, it will be Itcanbeobservedthatthedifferenceofcomputationalcom-
processed through the graph attention network in advance. plexity between Graph-Attention MATR and other methods8
TABLE I: Comparison of computational complexity. n is the sequence length, d is the representation dimension, r is the size
of transition in self-attention, obs dim is the dimension of observation information, and act dim is the the dimension of action
Algorithms Complexity Actorinput/outputdemensions Criticinput/outputdemensions
MATR O(n·d) obs dim/act dim obs dim/1
IPPO O(d) obs dim/act dim obs dim/1
HATRPO O(n·d) obs dim/act dim obs dim/1
GraphMATR O(n2· d) obs dim/act dim obs dim/1
AttentionMATR O(r·n · d) obs dim/act dim r· obs dim/1
Graph-AttentionMATR O(r·n2· d) obs dim/act dim r· obs dim/1
is not too significant. The computational complexity of the
proposed algorithm in this paper is mainly determined by the
size of transition, i.e. the number of GUs.
C. Equilibrium Solution for UAV-Assisted Communication
Markov Game
This part explores the presence of NE for Markov game.
Theorem 1: For the Markov game G with M UAV BSs,
the optimal policy set [Γ∗(a ,a ),m ∈ M] is a Nash
m −m
equilibrium.
Proof: To begin, as stated in reference [50], the Markov
game possesses a minimum of one NE. After that, the al-
gorithm’s monotonic improvement has been formally demon-
stratedinSectionIII.B.2).Eachagentisenabledtoindefinitely
converge towards the optimal strategy through the training
procedure. Once the strategies converges to Γ∗, all UAV
Fig.5:TrajectoriesandpairingwithtwoUAVBSs.GU1(2,1)
BSs can adopt optimal policies during each game round, and
means that GU 1 is paired with UAV BS 2 in the start and is
no single agent can enhance profits by altering the policy
paired with UAV BS 1 in the end, etc.
independently.ThismeetsthedefinitionforNashequilibrium.
When the following conditions hold:
Γ∗ =arg max R m,R −m, m∈M (24)
am,a−m
Γ∗(a ,a ) is a NE for the Markov game.
m −m
The proof is finished.
Remark 1: Reference [49] proves that Markov game has
at least one NE. But, it is a challenge to prove whether
Nash equilibrium is unique, as well as to find the quantity
of Nashequilibrium. In fact,checking for uniquenessof Nash
equilibriumisNP-hard.Inthepracticaltrainingofalgorithms,
it will cost too much to calculate the optimal strategies. So,
the agent will only conduct a limited episodes of training.
(a)UAVBS1 (b)UAVBS2
Accordingly, we obtain the approximate solution of Nash
Fig. 6: Mean reward box plot with two UAV BSs
equilibrium. This is where our work sets itself apart.
IV. PERFORMANCEEVALUATION
This part gives the simulation results with different number
of UAV BSs and GUs.
Tovalidatetheeffectivenessoftheproposedgraph-attention
MATR algorithm, five baseline methods are compared as: (1)
the baseline MATR; (2) the baseline IPPO; (3) the baseline
(a)UAVBS1 (b)UAVBS2
HATRPO; (4) the Graph MATR; (5) the Attention MATR.
Fig. 4: Mean reward curves with two UAV BSs Inthispaper,Adamoptimizerisadoptedtoupdatetheneu-
ral network [51]. The following provides a detailed analysis.9
(a)UAVBS1 (b)UAVBS2 (c)UAVBS3
Fig. 7: Mean reward curves with three UAV BSs
TABLE II: Final episode reward with 2 UAV BSs
UAVBS1 UAVBS2
MATR 2760.250 3083.469
IPPO 2581.505 3074.054
HATRPO 3029.567 2967.351
GraphMATR 2876.115 3046.190
AttentionMATR 2788.917 2819.708
Graph-AttentionMATR(Thispaper) 3073.919 3295.174
TABLE III: Final episode reward with 3 UAV BSs
UAVBS1 UAVBS2 UAVBS3
MATR 2636.052 4337.268 4796.336
IPPO 2428.499 3873.794 4835.297
HATRPO 2611.63 4522.892 4920.298
GraphMATR 2623.020 4584.723 4654.053
AttentionMATR 2172.944 3978.846 4160.447
Graph-AttentionMATR(Thispaper) 2741.980 4955.742 4471.564
TABLE IV: Final episode reward with 4 UAV BSs
UAVBS1 UAVBS2 UAVBS3 UAVBS4
MATR 1331.177 2498.194 3938.529 4243.466
IPPO 1407.159 2564.732 4059.404 4012.171
HATRPO 1401.452 2585.839 4107.441 4316.122
GraphMATR 1397.712 2592.799 4252.433 4142.618
AttentionMATR 1166.262 2506.635 3262.909 3783.110
Graph-AttentionMATR(Thispaper) 1488.718 2690.066 4403.375 4449.627
A. Parameter Settings
In this paper, the area D is set as a cuboid space of
200*200*100 m, and a Cartesian coordinate system is con-
structed based on this. The GU moves on the 2D ground of
(-100, 100)*(-100, 100). The UAV BS flies in the 3D space.
Forsafety,theflightheightissetto(10,100).Atthebeginning
ofeachepisode,allUAVBSstakeofffromrandompositions,
and each GU moves randomly at an interval speed of [0m/s,
10m/s]. In all experiments, each UAV BS has rated transmit
power P = 10dBm, shared total bandwidth B is
Fig. 8: Trajectories and pairing with three UAV BSs total total
30MHz, communication carrier frequency is f = 2GHz,
c
noise power spectral density is n = 10−17W/Hz, and
0
minimumdivisiblebandwidthisb =0.1MHz.Theservice
min
duration of each round is assumed to be T = 1000s, and
max
each decision interval is 1 s.10
(a)UAVBS1 (b)UAVBS2 (c)UAVBS3
Fig. 9: Mean reward box plot with three UAV BSs
(a)UAVBS1 (b)UAVBS2 (c)UAVBS3 (d)UAVBS4
Fig. 10: Mean reward curves with four UAV BSs
(a)UAVBS1 (b)UAVBS2 (c)UAVBS3 (d)UAVBS4
Fig. 11: Mean reward box plot with four UAV BSs
B. Result Analysis
In this part, the results of the graph attention-based trust
region MARL approach with different scenes are discussed.
1) Results with two UAV BSs and eight GUs
AsshowninFigure4,ablationexperimentswereconducted
to compare the proposed algorithm and other baseline al-
gorithms. As shown in the mean episode reward curve, the
convergence of the graph-attention MATR is the best. The
overall mean episode reward obtained is superior to the other
three methods. Table II provides the final episode reward
of two UAV BSs based on different algorithms. It can be
seen that among the 1000 episodes, based on the algorithm
proposedinthisarticle,alltwoUAVBSsachievedthehighest
cumulative reward. This validates the superior convergence
performance of the proposed method from another aspect.
Fig. 12: Trajectories and pairing with four UAV BSs
Figure 5 gives the trajectories, starting pairing, and ending11
pairing of last 25 steps for each UAV BS in the last episode. posed graph attention multi-agent trust region reinforcement
Starting from the initial position, UAV BSs make decision learning significantly improves the convergence performance,
basedontheexperiencelearnedfromhistoricaldata,changing which supports good solution for the trajectory design and
their spatial positions and pairing policies to achieve optimal resource allocation in multi-UAV assisted communication
communication performance. At the beginning, UAV BS 1 Markov game. This is because that trust region MARL guar-
chooses to pair with GU 2, GU 3, and GU 7; UAV BS 2 antees the monotonic convergence firstly. Secondly, graph
chooses to pair with GU 1, GU 4, GU 5, GU 6, and GU 8. recurrent network extracts useful information and patterns
At the end, UAV BS 1 adjusts its policy to pair with GU 1, from large amount of state information. Thirdly, the attention
GU 6, GU 7, and GU 8; UAV BS 2 adjusts its policy to pair mechanism provides additional information transmission and
with GU 2, GU 3, GU 4, and GU 5. Figure 6 gives the mean weighting, so that the critic network can more accurately
reward box plot of two UAV BSs under four algorithms. It evaluate the behavior value for UAV BSs. It provides more
can also be seen that, compared to other benchmark methods, reliable feedback signals and helping the actor network to
the proposed algorithm has complete advantages in terms of update the strategy more effectively.
mean reward distribution.
2) Results with three UAV BSs and nine GUs V. CONCLUTIONS
AsshowninFigure7,ablationexperimentswereconducted
Todealwiththedynamictrajectorydesignandresourceas-
to compare the proposed algorithm and other baseline al-
signmentinmulti-UAVassistedcommunication,Markovgame
gorithms. As shown in the mean episode reward curve, the
is used to model the communication network. Then, a novel
convergence effect of the proposed graph-attention MATR
graph attention-based multi-agent trust region reinforcement
is above other curves in general. The overall mean episode
learning framework is proposed to optimize pairing strategies
reward obtained is superior to the other three methods. Table
withGUs,mobiletrajectories,powerallocationstrategies,and
III provides the final episode reward of three UAV BSs based
bandwidthallocationstrategiesforallUAVBSs.Graphrecur-
on different algorithms. It can be seen that among the 1000
rent network is introduced to process and analyze complex
episodes, based on the algorithm proposed in this article,
topology and extract useful patterns from interactive data.
two out of three UAV BSs achieved the highest cumulative
The attention mechanism is designed to provide additional
rewards. This validates the superior convergence performance
weighting for conveyed information between UAV BSs, so
of the proposed method from another aspect. Figure 8 gives
that the critic network of each UAV BS can more accurately
the trajectories, starting pairing, and ending pairing of last
evaluate the value of behavior. It provides more reliable
25 steps for each UAV BS in the last episode. Starting from
feedback signals and helps the UAV BS update parameters of
the initial position, UAV BSs make decision based on the
theactornetworkmoreeffectively.ThisalsoallowsUAVBSs
experience learned from historical data, changing their spatial
to learn the moving policies and communication strategies of
positions and pairing policies to achieve optimal communica-
others, thereby optimizing their own policy. The introduced
tion performance. Figure 9 gives the mean reward box plot of
algorithm exhibits favorable performance and superior con-
three UAV BSs under four algorithms. It can also be seen
vergence compared to baseline strategies. Equally important
that, compared to other benchmark methods, the proposed
is that, well-trained intelligent UAV BSs can attain stable
algorithm has advantages at the highest median.
polices,therebyforminganapproximateNashequilibriumfor
3) Results with four UAV BSs and sixteen GUs
the downlink communication Markov game.
InFigure10,ablationexperimentswerecarriedouttocom-
pare the proposed algorithm with other baseline algorithms.
REFERENCES
As shown in the mean episode reward curve, the convergence
effect of the proposed algorithm is generally above other [1] A. M. Seid, A. Erbad, H. N. Abishu, A. Albaseer, M. Abdallah, and
M. Guizani, “Blockchain-empowered resource allocation in multi-uav-
curves.Specifically,theoverallmeanepisoderewardobtained
enabled 5g-ran: A multi-agent deep reinforcement learning approach,”
is superior to the other three methods. Table IV provides the IEEE Transactions on Cognitive Communications and Networking,
final episode reward based on different algorithms. It can be 2023.
[2] J.Wang,H.Han,H.Li,S.He,P.K.Sharma,andL.Chen,“Multiple
seen that among the 1000 episodes, based on the algorithm
strategiesdifferentialprivacyonsparsetensorfactorizationfornetwork
proposedinthispaper,allfourUAVBSsachievedthehighest traffic analysis in 5g,” IEEE Transactions on Industrial Informatics,
cumulative reward. This validates the superior convergence vol.18,no.3,pp.1939–1948,2021.
[3] T.-C. Chen, Y.-S. Liang, P.-S. Ko, P.-T. Ho, J.-C. Huang et al.,
performance of the proposed method from another aspect.
“Wirelesscommunicationusingembeddedmicroprocessor-5gembedded
Figure 11 gives the mean reward box plot of four UAV BSs e-commerce system oriented to fruit ordering, sales, and logistics,”
with four algorithms. It can also be seen that, compared WirelessCommunicationsandMobileComputing,vol.2022,2022.
[4] R.Zhu,G.Li,Y.Zhang,Z.Fang,andJ.Wang,“Load-balancedvirtual
to other benchmark methods, the proposed algorithm has
network embedding based on deep reinforcement learning for 6g re-
advantages in terms of mean reward distribution. Figure 12 gionalsatellitenetworks,”IEEETransactionsonVehicularTechnology,
gives the trajectories, starting pairing, and ending pairing of 2023.
[5] Z. Jia, Q. Wu, C. Dong, C. Yuen, and Z. Han, “Hierarchical aerial
last 25 steps for every UAV BS in the last episode. Starting
computing for internet of things via cooperation of haps and uavs,”
fromtheinitialposition,UAVBSsmakedecisionbasedonthe IEEEInternetofThingsJournal,vol.10,no.7,pp.5676–5688,2022.
experience learned from historical data, changing their spatial [6] H. Cao, N. Kumar, L. Yang, M. Guizani, and F. R. Yu, “Resource
orchestration and allocation of e2e slices in softwarized uavs-assisted
positionsandpairingpoliciestoachieveoptimalperformance.
6g terrestrial networks,” IEEE Transactions on Network and Service
Remark 2: To sum up, in different scenarios, the pro- Management,2023.12
[7] Y.Zeng,R.Zhang,andT.J.Lim,“Throughputmaximizationforuav- [30] C.Li,T.Wang,C.Wu,Q.Zhao,J.Yang,andC.Zhang,“Celebrating
enabled mobile relaying systems,” IEEE Transactions on communica- diversity in shared multi-agent reinforcement learning,” Advances in
tions,vol.64,no.12,pp.4983–4996,2016. NeuralInformationProcessingSystems,vol.34,pp.3991–4002,2021.
[8] Y. Xu, Z. Liu, C. Huang, and C. Yuen, “Robust resource allocation [31] H. Wang, C. H. Liu, H. Yang, G. Wang, and K. K. Leung, “Ensuring
algorithmforenergy-harvesting-basedd2dcommunicationunderlaying thresholdaoiforuav-assistedmobilecrowdsensingbymulti-agentdeep
uav-assistednetworks,”IEEEInternetofThingsJournal,vol.8,no.23, reinforcement learning with transformer,” IEEE/ACM Transactions on
pp.17161–17171,2021. Networking,2023.
[9] F.-H. Tseng and Y.-J. Hsieh, “Delanalty minimization with reinforce- [32] Z.Lv,L.Xiao,Y.Du,G.Niu,C.Xing,andW.Xu,“Multi-agentrein-
ment learning in uav-aided mobile network,” IEEE Transactions on forcementlearningbaseduavswarmcommunicationsagainstjamming,”
ComputationalSocialSystems,2023. IEEETransactionsonWirelessCommunications,2023.
[10] F.Fu,B.Xue,L.Cai,L.T.Yang,Z.Zhang,J.Luo,andC.Wang,“Live [33] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch,
traffic video multicasting services in uavs-assisted intelligent transport “Multi-agent actor-critic for mixed cooperative-competitive environ-
systems: A multi-actor attention critic approach,” IEEE Internet of ments,” Advances in neural information processing systems, vol. 30,
ThingsJournal,2023. 2017.
[11] W. Noh, S. Cho et al., “Sparse cnn and deep reinforcement learning- [34] C.Yu,A.Velu,E.Vinitsky,J.Gao,Y.Wang,A.Bayen,andY.Wu,“The
based d2d scheduling in uav-assisted industrial iot networks,” IEEE surprising effectiveness of ppo in cooperative multi-agent games,” Ad-
TransactionsonIndustrialInformatics,2023. vancesinNeuralInformationProcessingSystems,vol.35,pp.24611–
[12] Q. Shen, J. Peng, W. Xu, Y. Sun, W. Liang, L. Chen, Q. Zhao, and 24624,2022.
X.Jia,“Faircommunicationsinuavnetworksforrescueapplications,” [35] Z. Feng, M. Huang, D. Wu, E. Q. Wu, and C. Yuen, “Multi-agent
IEEEInternetofThingsJournal,2023. reinforcement learning with policy clipping and average evaluation
[13] W. Zhou, L. Fan, F. Zhou, F. Li, X. Lei, W. Xu, and A. Nallanathan, for uav-assisted communication markov game,” IEEE Transactions on
“Priority-awareresourceschedulingforuav-mountedmobileedgecom- IntelligentTransportationSystems,2023.
putingnetworks,”IEEETransactionsonVehicularTechnology,2023. [36] P. Nathan, F. Georgios, O. Kendric, and O. Meeko, “A uav-enabled
[14] B. Yang, X. Cao, C. Yuen, and L. Qian, “Offloading optimization in
dynamicmulti-targettrackingandsensingframework,”inProceedings
of the 2020 IEEE Global Communications Conference (GLOBECOM
edgecomputingfordeep-learning-enabledtargettrackingbyinternetof
uavs,”IEEEInternetofThingsJournal,vol.8,no.12,pp.9878–9893, 2020),Taipei,Taiwan,2020,pp.7–11.
2020. [37] M. C. Domingo, “Power allocation and energy cooperation for uav-
enabledmmwavenetworks:Amulti-agentdeepreinforcementlearning
[15] Q. Wu, Y. Zeng, and R. Zhang, “Joint trajectory and communication
approach,”Sensors,vol.22,no.1,p.270,2021.
designformulti-uavenabledwirelessnetworks,”IEEETransactionson
[38] N.Gao,Z.Qin,X.Jing,Q.Ni,andS.Jin,“Anti-intelligentuavjamming
WirelessCommunications,vol.17,no.3,pp.2109–2121,2018.
strategyviadeepq-networks,”IEEETransactionsonCommunications,
[16] Q. Wu and R. Zhang, “Delay-constrained throughput maximization in
vol.68,no.1,pp.569–581,2019.
uav-enabled ofdm systems,” in 2017 23rd Asia-Pacific Conference on
[39] G.WU,W.JIA,J.ZHAO,F.GAO,andM.YAO,“Marl-baseddesign
Communications(APCC). IEEE,2017,pp.1–6.
of multi-unmanned aerial vehicle assisted communication system with
[17] ——, “Common throughput maximization in uav-enabled ofdma sys-
hybrid gaming mode,” Journal of Electronics Information Technology,
temswithdelayconsideration,”IEEETransactionsonCommunications,
vol.44,no.3,pp.940–950,2022.
vol.66,no.12,pp.6614–6627,2018.
[40] M. L. Littman, “Markov games as a framework for multi-agent rein-
[18] M.L.Puterman,Markovdecisionprocesses:discretestochasticdynamic
forcementlearning,”inMachinelearningproceedings1994. Elsevier,
programming. JohnWiley&Sons,2014.
1994,pp.157–163.
[19] C. Dann, T. V. Marinov, M. Mohri, and J. Zimmert, “Beyond value-
[41] H.Liu,C.Zhang,Q.Chai,K.Meng,Q.Guo,andZ.Y.Dong,“Robust
functiongaps:Improvedinstance-dependentregretboundsforepisodic
regional coordination of inverter-based volt/var control via multi-agent
reinforcement learning,” Advances in Neural Information Processing
deepreinforcementlearning,”IEEETransactionsonSmartGrid,vol.12,
Systems,vol.34,pp.1–12,2021.
no.6,pp.5420–5433,2021.
[20] M. Simchowitz and K. G. Jamieson, “Non-asymptotic gap-dependent
[42] J.Nash,“Non-cooperativegames,”Annalsofmathematics,pp.286–295,
regretboundsfortabularmdps,”AdvancesinNeuralInformationPro-
1951.
cessingSystems,vol.32,2019.
[43] S. Kakade and J. Langford, “Approximately optimal approximate re-
[21] H. Pan, Y. Liu, G. Sun, P. Wang, and C. Yuen, “Resource scheduling
inforcement learning,” in Proceedings of the Nineteenth International
foruavs-aidedd2dnetworks:Amulti-objectiveoptimizationapproach,”
ConferenceonMachineLearning,2002,pp.267–274.
IEEETransactionsonWirelessCommunications,2023.
[44] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
[22] H.Pan,Y.Liu,G.Sun,J.Fan,S.Liang,andC.Yuen,“Jointpowerand region policy optimization,” in International conference on machine
3dtrajectoryoptimizationforuav-enabledwirelesspoweredcommuni- learning. PMLR,2015,pp.1889–1897.
cationnetworkswithobstacles,”IEEETransactionsonCommunications,
[45] J.G.Kuba,R.Chen,M.Wen,Y.Wen,F.Sun,J.Wang,andY.Yang,
vol.71,no.4,pp.2364–2380,2023. “Trustregionpolicyoptimisationinmulti-agentreinforcementlearning,”
[23] M. G. Azar, R. Munos, and B. Kappen, “On the sample complexity arXivpreprintarXiv:2109.11251,2021.
of reinforcement learning with a generative model,” arXiv preprint [46] C.Yu,A.Velu,E.Vinitsky,J.Gao,Y.Wang,A.Bayen,andY.Wu,“The
arXiv:1206.6461,2012. surprising effectiveness of ppo in cooperative multi-agent games,” Ad-
[24] C.Dann,L.Li,W.Wei,andE.Brunskill,“Policycertificates:Towards vancesinNeuralInformationProcessingSystems,vol.35,pp.24611–
accountable reinforcement learning,” in International Conference on 24624,2022.
MachineLearning. PMLR,2019,pp.1507–1516. [47] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
[25] A.ZanetteandE.Brunskill,“Tighterproblem-dependentregretbounds region policy optimization,” in International conference on machine
in reinforcement learning without domain knowledge using value learning. PMLR,2015,pp.1889–1897.
function bounds,” in International Conference on Machine Learning. [48] H.Ryu,H.Shin,andJ.Park,“Multi-agentactor-criticwithhierarchical
PMLR,2019,pp.7304–7312. graph attention network,” in Proceedings of the AAAI Conference on
[26] G. Wen, J. Fu, P. Dai, and J. Zhou, “Dtde: A new cooperative multi- ArtificialIntelligence,vol.34,no.05,2020,pp.7236–7243.
agentreinforcementlearningframework,”TheInnovation,vol.2,no.4, [49] C.S.deWitt,T.Gupta,D.Makoviichuk,V.Makoviychuk,P.H.Torr,
2021. M. Sun, and S. Whiteson, “Is independent learning all you need in
[27] H.Pan,Y.Liu,G.Sun,J.Fan,S.Liang,andC.Yuen,“Jointpowerand the starcraft multi-agent challenge?” arXiv preprint arXiv:2011.09533,
3dtrajectoryoptimizationforuav-enabledwirelesspoweredcommuni- 2020.
cationnetworkswithobstacles,”IEEETransactionsonCommunications, [50] J.B.Rosen,“Existenceanduniquenessofequilibriumpointsforconcave
vol.71,no.4,pp.2364–2380,2023. n-persongames,”Econometrica:JournaloftheEconometricSociety,pp.
[28] N. Aboueleneen, A. Alwarafy, and M. Abdallah, “Deep reinforcement 520–534,1965.
learningforinternetofdronesnetworks:Issuesandresearchdirections,” [51] D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimization,”
IEEEOpenJournaloftheCommunicationsSociety,2023. arXivpreprintarXiv:1412.6980,2014.
[29] G. Hu, Y. Zhu, D. Zhao, M. Zhao, and J. Hao, “Event-triggered
communication network with limited-bandwidth constraint for multi-
agent reinforcement learning,” IEEE Transactions on Neural Networks
andLearningSystems,2021.