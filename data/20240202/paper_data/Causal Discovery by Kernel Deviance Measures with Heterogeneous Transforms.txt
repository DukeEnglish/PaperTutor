Causal Discovery by Kernel Deviance Measures with Heterogeneous
Transforms
TimTse1 ZhitangChen1 ShengyuZhu1 YueLiu1
Abstract 1. Introduction
The discovery of causal relationships in a set The detection of causal relationships is important in sci-
of random variables is a fundamental objective ence as it allows one to infer the results of an outcome
of science and has also recently been argued as subjected to different interventions(Pearl, 2009). For ex-
being an essential component towards real ma- ample, itis commonin the medicalsciences to determine
chine intelligence. One class of causal discov- thegeneticcausationofcertaindiseaseswhileinthesocial
ery techniques are founded based on the argu- sciences one might seek to understand the driving factors
ment that there are inherent structural asymme- thatdeterminesocioeconomicstatus(Gelman,2011). His-
triesbetweenthecausalandanti-causaldirection torically,themajorityofcausalrelationshipsarediscovered
which could be leveraged in determiningthe di- throughrandomizedcontrolledexperimentsbutconstraints
rectionofcausation.Togoaboutcapturingthese suchasfinancialandethicallimitationshavemotivatedre-
discrepancies between cause and effect remains search into methodologies that operate solely on observa-
to be a challenge and many currentstate-of-the- tionaldata(Mitrovicetal.,2018).
art algorithms propose to compare the norms
In more recent times, causality has also been gaining
of the kernel mean embeddings of the condi-
more traction in the machine learning (ML) community.
tional distributions. In this work, we argue that
The recent advent of Deep Learning (DL) (LeCunetal.,
such approaches based on RKHS embeddings
2015; Goodfellowetal., 2016) has had a remarkable im-
areinsufficientincapturingprincipalmarkersof
pact, with DL-based models achieving state-of-the-art re-
cause-effect asymmetry involving higher-order
sultsintasksrangingfromnaturallanguageprocessingand
structuralvariabilitiesoftheconditionaldistribu-
speech recognition to computer vision and reinforcement
tions. We propose Kernel Intrinsic Invariance
learning.Despitethesesuccesses,thereremainssignificant
Measure with HeterogeneousTransform(KIIM-
challengestobeaddressedsuchasrobustnesstoadversarial
HT) which introduces a novel score measure
attacks,thedemandforenormousamountsoflabelleddata
basedonheterogeneoustransformationofRKHS
and the lack of generalization across domains (Waldrop,
embeddingsto extractrelevanthigher-ordermo-
2019). The application of causality to redress these limi-
mentsoftheconditionaldensitiesforcausaldis-
tationsappeartobepromisingastheinteractionsbetween
covery. Inference is made via comparing the
randomvariablescouldbeleveragedasapriortorenderan
scoreofeachhypotheticalcause-effectdirection.
algorithmmorerobusttoadversarialattacks,data-efficient
Tests and comparisons on a synthetic dataset, a
and generalize better to unseen domains where an other-
two-dimensional synthetic dataset and the real-
wise“associativemethod”couldnot.
worldbenchmarkdatasetTu¨bingenCause-Effect
Pairs verify our approach. In addition, we con- Some examples of recent work in this direction in-
duct a sensitivity analysis to the regularization clude (Dasguptaetal., 2019) which introduces a model-
parameter to faithfully compare previous work freemeta-reinforcementlearningalgorithmthatlearnsthe
to our method and an experiment with trials on causal structure of the environmentthereby providing the
variedhyperparametervaluestoshowcasethero- agentwiththeabilitytoperformandinterpretexperiments.
bustnessofouralgorithm. (Yangetal.,2019) proposesa causalinferenceframework
for robust visual reasoning via do-calculus, treating ad-
1Huawei Noah’s Ark Lab. Correspondence
versarial attacks as interventions. Finally, the work of
to: Tim Tse <tim.tse@huawei.com>, Zhitang
Chen <chenzhitang2@huawei.com>, Shengyu (Gongetal., 2018) propoundsCausal Domain Adaptation
Zhu <zhushengyu@huawei.com>, Yue Liu <li- Networks, an artificial network architecture which makes
uyue52@huawei.com>. use of causal representation of joint distributions for the
task of domain adaptation which essentially involves un-
4202
naJ
13
]LM.tats[
1v71081.1042:viXraCausalDiscoverybyKernelDevianceMeasureswithHeterogeneousTransforms
derstandingandmakinguseofdistributionchangesacross insteadproposeKernelInstrinsicInvarianceMeasurewith
domains.Wereferthereaderto(Scho¨lkopf,2019)foranin- HeterogeneousTransforamtion(KIIM-HT)whichpresents
depthdiscussionaswellasadditionexamplesonthetopic anovelscoremeasurebasedonheterogeneoustransforma-
ofcausalityformachinelearning. tionofRKHSembeddingstoextractrelevanthigher-order
deviancesoftheconditionaldistributionsforcausaldiscov-
Howtoactuallygoaboutlearningthecausalrelationships
ery.
of r.v.s is itself a broad topic but in general, the various
methodscouldberoughlygroupedbasedontheircommon The remainder of the paper is organized as follows: first
theme. A class of methods going by the name of struc- we provide some preliminaries then we discuss some ex-
turalequationmodelling(SEM)(Heinze-Demletal.,2018; isting work in causal discovery. Next, we describe our
Shimizuetal.,2006)seektomodelstructuralrelationships proposed method followed by experimentson a synthetic
through graphical models but are limited by their ability dataset,atwo-dimensionalsyntheticdataset,theTu¨bingen
to identifymodelsonly up to a Markovequivalenceclass. Cause-EffectPairs(TCEP)dataset,asensitivityanalysisto
Another group of methods (Spirtesetal., 1993; Sunetal., theregularizationparameterandademonstrationofthero-
2007)attempttoinferthecausalstructurebytestingforin- bustness of our method to different hyperparameters and
dependenceoftheconditionaldistributionsofthedatabut finally,wewillconclude.
are not robust to the choice of conditional independence
test. A final collection of techniques (Hoyeretal., 2009; 2. Preliminaries
Mooijetal., 2009; Zhang&Hyva¨rinen, 2009) conjecture
thatthere are innate asymmetriesbetweentwo interacting We briefly review some concepts on kernel methods and
r.v.s.whichcouldbeleveragedforcausaldiscoverybutare kernel mean embeddings, which form the bedrock of our
limited by the assumption of a particular functional form methodandfewotherrelatedworks.
andnoisemodel.
Kernelmethodsareaclassofmachinelearningalgorithms
Aparticularinterestinginterpretationofcause-effectasym- that work by mapping an input space to a possibly in-
X
metry argues that nature, tends from order to disorder in finite dimensional space where certain operations in
H
accordance with the second law of thermodynamics, sug- could be expressed as dot products in . This so-
X H
gestingthatonemaybeabletoinferthesequenceofcausa- called kernel trick allows one to bypass the explicit and
tioninasetofeventsbydeterminingasymmetriesintheir sometimes intractable (i.e., infinite) representation of co-
measuredcomplexity(Janzingetal.,2016).Fromastatisti- ordinates in which are instead mapped into , x
X H →
calpointofview,oneinterpretationofasymmetryincause- φ(x) := k(x, ) via positive definite kernel k(, ), satis-
· · ·
effect(Mooijetal.,2010)presentsitselfintheideathatthe fying φ(x),φ(y) = k(x,y). Effectively, (non)linear
H
h i
factorizationofthejointdistributionp(X,Y)issimplerin transformationsanddotproductoperationsarereplacedby
thecausaldirectionX Y asopposedtotheanti-causal kernelevaluationswhich,inonepossibleinterpretation,is
→
directionY X,i.e. apairwisesimilaritymeasureoftheinputs.
→
Given a probability distribution p(X) on , the kernel
(p(X))+ (p(Y X)) (p(Y))+ (p(X Y)) (1) X
K K | ≤K K | mean embedding (Songetal., 2013) of p(X) in is
X
H
givenby
where () denotes the Kolmogorov complexity (KC)
K ·
(Gru¨nwald&Vita´nyi,2008).Despiteitstheoreticalappeal,
µ :=E [k(X, )]=E [φ(X)]= φ(x)p(x)dx.
itiswell-knownthattheKCmeasureisessentiallyuncom- X X · X Z
X
putable and as a workaround, metrics such as Minimal
Oftentimes, we do not have access to the underlying true
Description Length (Lemeire&Dirkx, 2006) and RKHS
distributionand thuscalls foran estimation of the embed-
norms(Chenetal., 2014; Sunetal., 2008) have been pro-
dingwithfintesamples. Givensamples x i = 1,...,n
posed as tractable substitutes. A recent work of this { i | }
theempiricalestimateisgivenby
class (Mitrovicetal., 2018) proposes a score-based mea-
sure based on the variance of the RKHS norms while n
1
(Chenetal., 2019) propounds an extension amounting to µˆ X = k(x i, ).
n ·
a linear projection of the norms of the variance between Xi=1
theRKHSembeddings. (Scholkopf&Smola, 2001) show that if k is a character-
istic kernel then the mapping is injective. An example of
Ourworkbuildsontopofstate-of-the-artapproachesbased
such a kernelis the radialbasis function(RBF) kernelde-
onRKHSembeddings.Inthispaper,wearguethatmetrics
finedas
of current methods are insufficient in capturing structural
variabilities of the conditional distributions which are vi- x y 2
k(x,y)=σ2exp k − k2 ,
tal information for inferring cause-effect asymmetry. We f (cid:18)− 2ℓ2 (cid:19)CausalDiscoverybyKernelDevianceMeasureswithHeterogeneousTransforms
nism.Incontrasttootherworkwhichmeasuresasymmetry
intermsoftheKCofthejointdistributions(i.e.,Equation
1),KCDCproposesinsteadtointerpretthisasymmetryvia
theKolgomorovcomplexityoftheconditionaldistribution.
Morespecifically,KCDCarguesthattheirproposedasym-
metry metric is realized by the KC of the input distribu-
tion being independent of the causal mechanism whereas
−6 −4 −2 y0 2 4 6 −6 −4 −2 x0 2 4 6 o pn ent dh ee no ct eh ie nr th ha en id n, pi un tt ah ne da tn ht ei- mca iu ns imal ud mire dc et sio cn ri, pt th ioe nre leis ngd te h-
of the data-generating mechanism. This (in)dependence
Figure1.Conditionaldistributionsofthecausaldirection,p(y|x),
could be measured by looking at the variability in KC of
andtheanti-causaldirection,p(x|y).
the causal mechanismfor differentinputs. To circumvent
the uncomputabilityof the KC, KCDC resorts to compar-
whereℓ is the length-scaleand σ is the signalamplitude. ingthevariabilityofconditionaldistributionsembeddedin
f
Similarly,givenaconditionaldistributionp(X Y = y)in- RKHSgivenby
|
stantiated at y, the kernelconditionalmean embeddingis
definedas 1 n 1 n 2
= µ µ
µ :=E [φ(X)]= φ(x)p(xy)dx SX→Y n Xi=1(cid:16)(cid:13) Y|xi (cid:13)HY − n Xj=1(cid:13) X|yj (cid:13)HX(cid:17)
X|y X|y (cid:13) (cid:13) (cid:13) (cid:13)
Z |
X
where and are RKHSs defined on and , re-
andlikewise,thecorrespondingempiricalestimateisgiven HX HY X Y
spectivelyandµ isthe meanembeddingofthe condi-
by Y|xi
tionaldistributionp(y x)evaluatedatx . Thedirectionof
µˆ X|y =Ψ(K y +λI)−1k y causationismadeviaa| simpledecisionri ulethatcompares
where K y is the gram matrix (i.e., [K y] i,j = X→Y and Y→X.
k(y ,y )), Ψ = [ψ(x ),...,ψ(x )] and k = S S
i j 1 n y (Chenetal.,2019)bringstoattentionpotentialissueswith
[k(y,y ),...,k(y,y )]⊤.
1 n identifyingdistributionsbysimplycomparingthenormsof
the embeddings. Specifically, under the mild assumption
3. RelatedWork
thatdomain issymmetricabouttheoriginandtheentail-
X
ingkernelisstationary,thengivenarbitrarydensitiesp(x)
Thetwopriorworksthataremostrelevanttoourcurrentre-
and q(x) where p(x) = q( x), it can be shown by ap-
searchare1)KernelConditionalDevianceforCausalInfer- −
plying Bochner’s theorem (Rudin, 1990) that µ =
ence (KCDC) and 2) Kernel Intrinsic InvarianceMeasure k p kHX
µ . Thisresultimpliesthatp(x)andq(x)mayshare
(KIIM). k q kHX
the same norm despite exhibiting large structural variabil-
As aforementioned, KCDC belongs to the class of causal ity between the two distributions (i.e., for skewed distri-
discoveryalgorithmsthatpositaninherentasymmetrybe- butions, the structure of p(x) and q(x) are different). In
tweenthecausalandanti-causaldirection. Considertheil- general,givenanarbitraryprobabilitydensity,thereexists
lustrativetoyexample,y =x3+x+ǫwhereǫ (0,1). with high probability a different probability density with
∼N
Figure 1 depicts the asymmetry between the conditional thesamenorminRKHSembedding.Motivatedbythisob-
distributions in the causal direction p(y x) and the anti- servation,theyinsteadproposeKIIMwhichintroducesthe
|
causaldirectionp(xy) fordifferentvaluesof x andy, re- scoreinthedirectionofX Y givenby
| →
spectively.Notethestructuralvariabilityoftheconditional
distributions is larger in the anti-causal direction than the n 1 n
=min W⊤µ W⊤µ 2
causal direction. It is stressed that structural variability SX→Y W Y|xi − n Y|xj HY
does not refer to the variability in location and scale but Xi=1(cid:13) Xj=1 (cid:13)
(cid:13) (cid:13)
rathertoaspectssuchasthenumberofmodesandinhigher
whichcouldbe interpretedasthe calculationof thediffer-
ordermoments. Ifoneweretoimaginetheconditionaldis-
enceoftheconditionaldistributionsinstantiatedatdifferent
tributionsasfunctionsmappingxtoyandvice-versa,then
values. Thescore iszeroif and onlyif all theconditional
themappinginthecausaldirectionwouldproduceoutputs
distributionsare equalas a result of the injective property
withthesamestructurefordifferentinputswhereasin the
of the kernel mappings. W is a projection matrix which
anti-causaldirection,theoutputsexhibitdiversestructural
maps each conditional distribution into a subspace where
variationsacrossdifferentinputs.
thelocationandscaleareremovedwhileleavingthehigher-
Fromthisinsight, KCDCprovidesa newinterpretationof orderdevianceswhichare intendedfor use in quantifying
asymmetrybasedontheKCofthedata-generatingmecha- thestructuralvariabilityofsaidconditionaldistributions.CausalDiscoverybyKernelDevianceMeasureswithHeterogeneousTransforms
4. KernelIntrinsicInvarianceMeasure with where W () is a heterogeneous projection matrix. We
θ
·
Heterogeneous Transformation leveragethepowerofartificialneuralnetworksandparam-
eterize W () as a feedforward neural network with pa-
θ
Despitethepositiveresultsreported,wedemonstratethata rameters θ. · Equation 2 could be perceived as a pairwise
globallinearprojectionmaynotbeasufficienttransforma- comparisonofthekernelmeanembeddingsofconditional
tion to capturethe desiredstatistics in the conditionaldis- distributions in a subspace and optimizing for θ amounts
tributions. Toillustratethis,considerthefollowingsimple to learning a heterogeneousprojection into said subspace
example.Lety =x+ǫwithǫ (0,1). Theconditional whichpreservestherelevantstatisticsforcausaldiscovery.
∼N
distributionevaluatedatx i isgivenby Weunderscorethenotionthattheprojectionmatrixforthe
ithinstanceisafunctionoftheithinputasmotivatedbyour
1 (y x )2
p(y x )= exp − i . aforementionedobservationoftheinsufficiencyofaglobal
i
| √2π (cid:18)− 2 (cid:19) projection.
Now, define p (y) := p(y x ) and consider the transfor- TheempiricalestimateofEquation2isgivenby
Yi
|
i
mationofrandomvariableY givenbyZ :=Y +b . The
i j i j
pdfofZ j isgivenby Sˆ x→y =m θin Sx→y (3)
1 (z b j x i)2 where
p (z)= exp − −
Zj
√2π (cid:18)− 2 (cid:19)
n i−1
which implies that an arbitrary conditional distribution = W⊤(x )K (K +λI)−1k
Sx→y θ i y x xi
p(y x j)canbeobtainfromanotherconditionaldistribution Xi=1Xj=1(cid:13)
| (cid:13)f
p q( uy a| dx ri a) ticvi ka erth ne elo xffset φb (j x)= =x [j 1,− x,x xi 2. ]⊤N ,o thw e, nconsider the −W θ⊤(x j)K y(K x+λI)−1k xj 2 HY.
7→ (cid:13)
f (cid:13)
WeassumethatW ()liesinthespanofΨhencewecan
θ
µ Zj =
Z
Zφ(z)p Zj(z)dz write W θ( ·) = ΨW· θ( ·) and K
y
=: Ψ⊤Ψ is the gram
matrix for the effect (i.e., [K ] = k(y ,y )) while all
=M(b j)µ Yi other pf arametersremain the sy ami, ej as descri ibej d in the pre-
vioussection. We notethat a trivialsolutionexists where
where
W⊤(z) = 0forallz andtopreventtheoptimizationrou-
1 0 0 θ
tinefromconvergingtothisdegeneratesolution,weintro-
M(b j)=b j 1 0. f
b2 2b 1 ducearegularizationtermtoEquation3,resultingin
 j j 
This example demonstrates that even in the simple case λ n 1
reg =min + reg (4)
w anh oe tr he erth ae ndco tn hd eiti eo mn bal edd di is nt grib ku eti ro nn es
l
ia sre ao tf hf rs ee ets vo af riao bn le
e
Sx→y θ (cid:26)Sx→y n
Xi=1
kW θ(x i) k2 2(cid:27)
quadratic, µ and µ are related via a transformation
Zj Yi where λ
reg
is a hyperparameter which determines the de-
M(b ) which is a function of the input data-point b and
j j gree of penalization for the trivial solution. We take a
therefore,theredoesnotexista globaltransformationma-
straightforward approach in optimizing Loss 4 where the
trixW thatcapturesthestatisticalrelationsofallpairwise
scoreistakentobethelowestvalueattainedfromrunninga
kernelembeddingoftheconditionaldistributions.
stochasticgradientdescentalgorithmfor iterations.Hav-
I
Motivated by the previous observations, we propound to ingobtainedboth x→y and y→x thedirectionofcausa-
S S
rectify the limitations of a global linear projection with a tionD(X,Y)isdecidedviathedecisionrule
novelscoremetricbasedontheheterogeneousprojections
oftheembeddings. OurnewalgorithmwhichwecallKer- X Y if x→y < y→x
→ S S
nelIntrinsicInvarianceMeasurewithHeterogeneousTrans- D(X,Y)=Y X if > . (5)
formation(KIIM-HT)introducesthescoreinthedirection
 → Sx→y Sy→x
undecided if =
x→y y→x
ofx ygivenby S S
→ 
Algorithm1providesapseudocodesummaryoftheoverall
2 n i−1 algorithm.
∗ =min W⊤(x )µ
Sx→y θ n(n −1) Xi=1Xj=1(cid:13) θ i Y|xi KIIMadoptsaimportancere-weightingschememotivated
(cid:13)
by the observation that real-world data exhibits noise and
W⊤(x )µ 2 , (2)
− θ j Y|xj HY outliersthatoughttobere-weightedtominimizetheirdetri-
(cid:13)
(cid:13)CausalDiscoverybyKernelDevianceMeasureswithHeterogeneousTransforms
Algorithm1KIIM-HTAlgorithm (x3+x)exp(ǫ),4)MNM-2:(sin(10x)+exp(3x))exp(ǫ)
Input: Realizations (x ,y ) n of (X,Y), regular- and5)CNM:log(x6+5)+x5 sin(x2 ǫ). Weconduct
izationhyperparamterλ,n{ umi beri o} fi S= G1 Diterations experimentsfor two noise gener− atingpro| ce| sses, where in
Output: Causal direction (X Y or YI the first process ǫ (0,1) is sampled from the stan-
X) → → dard normal distribu∼ tioN n and in the second, ǫ (0,1)
∼ U
is sampled from the standard uniform distribution. The
1: ComputegrammatricesK xandK y
2: ComputeK y(K x+λI)−1k xi fori=1,...,n c da isu ts re ibux tio∼ nN an( d0, a1 s) amis ps la em sp izl eed of fr 1o 0m 0t ih se us st ea dnd fa or rd en aco hrm ea xl
-
3: MinimizeLoss4via iterationsofSGD
I perimental instance. We compare our algorithm against
4: Take x→y tobethelowestvalueencounteredinthe
S I ANM (Hoyeretal., 2009), IGCI (Daniusˇisetal., 2010),
iterationsofSGD
KCDC (Mitrovicetal., 2018), LiNGAM (Shimizuetal.,
5: Repeatsteps1through4fortheotherdirection
2006)andKIIM(Chenetal.,2019). InKCDC,KIIMand
6: DeterminecausaldirectionD(X,Y)usingEquation5
KIIM-HT,bothgrammatricesK andK aretakentobe
7: returnD(X,Y) x y
themultiplicationoftherationalquadratic(RQ)kernelde-
fined as k(x,y) = 1
kx−yk2
2 and the RBF kernelas
− kx−yk2 2+1
mentaleffects.There-weightingschemeisgivenas defined in the preliminaries section. The scale-length hy-
perparameter of the RBF kernel is selected using the me-
ref := φ(y) φ(x)p(y x)u(x)dx, dian heuristic and the kernel regularization parameter is
CYX Z X ⊗ | set to λ = 10−3. In addition, KIIM-HT utilizes a pro-
CXref
X
:=
Z
φ(x) ⊗φ(x)dx sje izc etio in npm ua ttrix diW mθ b( y·) npa hr iam dde ete nriz ae nd db thy ea nn aninp ou ut tpl ua tye lar yo ef
r
X
ofsizen hiddenbyrank n samples(i.e.,n)which
and ×
thengetsreshapedtoformtheprojectionmatrix.Thenum-
µref = ref ref −1 φ(x)
Y|x CYX CYX berofhiddenunitsissetton hidden = 20andtherank
(cid:0) (cid:1)
whereu(x)isthereferencedistribution.Thisre-weighting oftheprojectionmatrixissettorank = 100. Avalueof
schemecouldnaturallybeextendedtoKIIM-HTbysubsti- λ reg = 10−3 isusedfortheregularizationhyperparameter.
tutingtheempiricalestimationwith Arectifiedlinearunitactivationisusedforthehiddenlayer
while a linear activation is used for the output layer. We
µˆr Yef
|x
=ΨHR1 2(R1 2HK xHR21 +λI)−1R1 2Hk
x
usetheAdamoptimizer(Kingma&Ba,2014)withdefault
parameters,alearningrateof10−3 and = 100gradient
I
whereH =I 1~1~1⊤,~1isan 1vectorofonesandR steps for optimizing the loss function. IGCI is tested for
− n ×
isadiagonalre-weightingmatrixwith[R] = u(xi). The bothsettingswherethereferencemeasureparameterisset
i,i p(xi)
to the uniform reference measure and the Gaussian refer-
overall algorithm remains unchanged except for line 2 in
encemeasurewhicharedenotedIGCI- andIGCI- , re-
Algorithm1wheretheempiricalestimateoftheconditional
N U
spectively. Weranfivetrialsforeachexperimentalsetting
kernelmeanembeddingsisreplacedbyitsre-weightedver-
andwereporttheaverageperformanceandtheaccompany-
sion. WedenotethisvariantofKIIM-HTasRw-KIIM-HT
ingstandarddeviation(SD).
where“Rw”standsfor“re-weighted”.
TheresultsaresummarizedinTable1. Asexpected,ANM
5. Experiments performswell onthe ANM-1 andANM-2 datasetsdueto
themodelsassumptionofadditivenoise. However,thisas-
We describe our experimental setup and results on a syn- sumptionalsohampersitsperformanceondatagenerating
thetic dataset, a two-dimensional synthetic dataset and a mechanismsinvolvingnonadditivenoise as evidentin the
real-worldbenchmarkdatasetTu¨bingenCause-EffectPairs model’s poorer performance on the other datasets. IGCI
(TCEP).Wealsoconductanexperimenttoanalyzethesen- showsrobustperformancewithbothuniformandGaussian
sitivity of the kernel based methods to the regularization referencemeasurewhileontheotherhand,wewereunfor-
parameterandanexperimenttodemonstratetherobustness tunatelyunabletoreproducethepositiveresultsofKCDC
ofourmethodtodifferenthyperparameters. as reported in their paper. We conjecture KCDC may be
sensitive to the regularization hyperparameter λ which is
5.1.SyntheticData unspecifiedintheoriginalpaperandweinvestigatethisfur-
ther in our later experiments. LiNGAM exhibitsa perfor-
We test the effectivenessof our algorithm againsta set of
mance in the extremes where it achieves perfect on most
baseline on five sets of synthetic mappings from cause x
ofthefunctionalandnoisesettingsbutthentherearealsoa
to effect y. These mappings include 1) ANM-1: y =
fewsettingswhereitclassifieswithanaccuracyofzeroper-
x3 + x + ǫ, 2) ANM-2: y = x + ǫ, 3) MNM-1: y =CausalDiscoverybyKernelDevianceMeasureswithHeterogeneousTransforms
Table1.Performanceonthesyntheticdataset.
Models(%)
Function ǫ ANM IGCI- IGCI- KCDC LiNGAM KIIM KIIM-HT
∼ N U
ANM-1 99.6 0.49 97.8 0.98 100.0 0.0 74.8 3.6 100.0 0.0 100.0 0.0 100.0 0.0
N ± ± ± ± ± ± ±
100.0 0.0 99.6 0.49 100.0 0.0 0.0 0.0 100.0 0.0 96.2 1.47 100.0 0.0
U ± ± ± ± ± ± ±
ANM-2 47.8 2.86 52.6 4.8 48.6 7.63 57.2 3.12 0.0 0.0 80.4 3.61 96.4 0.49
N ± ± ± ± ± ± ±
70.4 4.92 52.2 4.26 48.4 5.75 48.4 7.81 0.0 0.0 48.8 4.26 67.2 3.12
U ± ± ± ± ± ± ±
MNM-1 0.0 0.0 100.0 0.0 100.0 0.0 0.0 0.0 100.0 0.0 100.0 0.0 100.0 0.0
N ± ± ± ± ± ± ±
0.0 0.0 99.8 0.4 100.0 0.0 0.0 0.0 100.0 0.0 99.6 0.49 100.0 0.0
U ± ± ± ± ± ± ±
MNM-2 2.4 1.02 100.0 0.0 100.0 0.0 0.0 0.0 100.0 0.0 100.0 0.0 100.0 0.0
N ± ± ± ± ± ± ±
29.2 6.65 100.0 0.0 100.0 0.0 0.0 0.0 100.0 0.0 99.2 1.17 100.0 0.0
U ± ± ± ± ± ± ±
CNM 47.4 4.13 100.0 0.0 100.0 0.0 0.0 0.0 100.0 0.0 100.0 0.0 100.0 0.0
N ± ± ± ± ± ± ±
30.8 5.56 100.0 0.0 100.0 0.0 0.0 0.0 100.0 0.0 100.0 0.0 100.0 0.0
U ± ± ± ± ± ± ±
cent.KIIMalsoperformsdecentlyacrossallsettings;com- We see again that both IGCI- and IGCI- are quite ro-
N U
paredtoIGCI,oneofthebestbaselines,KIIMsignificantly bust, consistently achieving performances in the 80s and
outperformsonǫ (0,1)forANM-2whileitarguably 90s with only a few in the 70s. Similarly, KCDC also re-
∼ N
matches the performancefor all other settings (i.e., slight mains consistent with previous findings, showing subpar
advantageforIGCIonǫ (0,1)forMNM-1andMNM- performance with the best performing setting at 56.4 per-
∼U
2). Similarly,KIIM-HTdisplaysverystrongperformance; cent. KIIM, on the otherhand, deviatesfrom previousre-
forallsettings,KIIM-HTeithermatchesoroutperformsall sultsandperformsverypoorlyonthetwo-dimensionalset-
otherbaselinesexceptforthesettingǫ (0,1)forANM- tings,withthebestperformingsettingonlyat38.2percent.
∼U
2 where KIIM-HT ranks second behind ANM. KIIM-HT We are unsure of KIIM’s large difference in performance
alsoshowsrobustnessinthestabilityofthealgorithmasit acrossthe two experiments, butit appearsthatKIIM may
has the least variance on the ANM-2 settings which were notperformwellunderalow-dataregimeandexperimental
thehardestandhadthemostvariedperformances. evidence,includedinAppendixA,corroboratesthisasthe
performance of KIIM improves with increased data sam-
5.2.Two-DimensionalSyntheticData ples. We conjecture KIIM’s sensitivity to sample size is
duetoitsusageofeigendecompositionasacorepartinits
We examinethe performanceof our algorithmon a set of
algorithm.Decomposingadatamatrixintoasetofeigven-
two-dimensionalvector valued cause-effectpairs. Indeed,
valuesandeigenvectorscouldbeviewedasadatasumma-
real-worldstatisticalexperimentsoftentimesmeasuremul-
rization technique expressed through said eigenpairs and
tiple predictor and response variables and it would be un-
indeed, one of the most elementary dimensionality reduc-
usual to observe the value of only one random variable.
tiontechnique,PCA,amountstothisprocedure.Ofcourse,
Thereareten setsofsyntheticmappingsbetweencausex
asummarizationislogicalwhentherearealotofdatasam-
andeffectywhicharegeneratedbytakingpairwisecombi-
ples, but a summary is an antithesis in a low-data regime,
nationsofthescalarmappingsofthepreviousexperiment
which may explain why KIIM is doing so poorly when
andthencollatingtherespectivecausesandeffectstoform
n = 5 as in such a setting, it may by operating on non-
a vector (i.e., x and the associated y corresponds to an
i i sensereturnedbyeigendecomposition. Lastly,weseethat
independentscalarmappingwherei 1,2 ).Weabstain
KIIM-HTdoesthebest,beatingallotherbaselines,includ-
∈{ }
from permuting the noise process of each dimension and
ingIGCI,ineverysingleinstance. TheaccuracyofKIIM-
instead each vector mappingis associated with noise gen-
HTisconsistentlyinthehigh90sanditachievesnolower
eratedfromeitherthestandardnormalorstandarduniform
than91percentinallthe settings. Similarto theprevious
distribution,resultinginatotaloftwentysettings. Weuse
experiment, KIIM-HT shows significantly low variability
asamplesizeofn = 5whileallotherexperimentalsetup
in all of the settings; the only clear loss for the model is
parametersremainthesame. Althoughtheoretically,there
on ANM-2 MNM-2 for ǫ (0,1) while for all other
arenoprohibitionsonextendingANMandLiNGAMtoop- ∼ N
settingsKIIM-HTconsistentlyhasthelowestvariability.
erateonvector-valuedinputs,wewereunabletofindsuch
implementationsandhence,weonlycompareagainstIGCI,
KCDC and KIIM. The results are summarized in Table 2.CausalDiscoverybyKernelDevianceMeasureswithHeterogeneousTransforms
Table2.Performanceonthetwo-dimensionalsyntheticdataset.
Models(%)
Functions ǫ IGCI- IGCI- KCDC KIIM KIIM-HT
∼ N U
ANM-1ANM-2 65.8 5.04 72.4 4.63 18.0 4.05 8.0 0.89 92.0 2.28
N ± ± ± ± ±
70.2 2.4 77.0 4.34 21.6 3.38 10.4 2.87 94.0 1.67
U ± ± ± ± ±
ANM-1MNM-1 75.6 4.54 82.6 2.94 9.2 1.17 2.8 1.17 98.0 1.1
N ± ± ± ± ±
76.4 4.13 86.0 2.1 3.2 2.48 0.4 0.49 100.0 0.0
U ± ± ± ± ±
ANM-1MNM-2 88.4 3.26 93.6 1.62 5.4 1.02 1.8 0.4 98.2 0.4
N ± ± ± ± ±
89.4 2.8 92.8 2.23 5.6 2.42 1.8 1.72 99.0 0.63
U ± ± ± ± ±
ANM-1CNM 87.8 2.4 94.0 1.79 25.2 2.71 9.8 2.71 99.2 0.98
N ± ± ± ± ±
91.2 1.47 95.2 0.98 29.0 2.76 11.2 4.87 100.0 0.0
U ± ± ± ± ±
ANM-2MNM-1 72.6 2.24 78.4 3.61 20.2 4.17 11.0 0.89 89.2 3.71
N ± ± ± ± ±
77.8 3.19 84.2 3.87 9.8 2.14 4.8 1.47 94.6 1.85
U ± ± ± ± ±
ANM-2MNM-2 88.6 1.85 92.0 1.26 18.0 1.79 7.6 1.2 95.6 2.06
N ± ± ± ± ±
90.0 3.52 91.2 1.47 9.0 2.61 5.4 2.8 96.6 1.85
U ± ± ± ± ±
ANM-2CNM 86.0 4.05 90.0 1.79 42.0 6.48 24.0 2.53 92.8 1.17
N ± ± ± ± ±
91.0 3.16 91.2 1.47 56.4 1.85 38.2 4.96 94.6 2.24
U ± ± ± ± ±
MNM-1MNM-2 93.2 1.17 94.0 2.61 11.2 2.14 3.6 1.02 98.6 0.49
N ± ± ± ± ±
91.8 3.12 91.6 1.85 2.6 0.8 0.0 0.0 100.0 0.0
U ± ± ± ± ±
MNM-1CNM 93.4 3.38 94.4 1.36 29.4 4.13 14.8 1.6 99.2 0.75
N ± ± ± ± ±
90.8 1.17 95.0 1.79 18.8 3.71 3.8 1.47 99.8 0.4
U ± ± ± ± ±
MNM-2CNM 96.0 1.41 96.4 2.24 21.8 5.78 9.0 1.79 99.4 0.8
N ± ± ± ± ±
97.2 1.33 97.2 1.17 18.0 3.03 3.6 0.8 99.4 0.8
U ± ± ± ± ±
5.3.Tu¨bingenCause-EffectPairs all models to this downsampling procedure and we show
in Appendix B that this did not adversely affect the non-
In this experiment we test our algorithm on real-world
kernelbasedmethods. Asaforementioned,real-worlddata
data. We use the open dataset Tu¨bingen Cause-Effect
are rife with noise and outliers and therefore, we use the
Pairs(TCEP)(Mooijetal.,2016)whichisapopularbench-
re-weightingschemesforKIIMandKIIM-HTtomitigate
mark consisting of approximately 108 (varying between
their effects. We test two differentreferencedistributions
versions) cause and effect pairs collected in various real-
whereoneistheGaussiandistributionandtheotheristhe
world domains (e.g., meteorology, biology, engineering,
Laplacedistributionwhichwe denote,respectively,byap-
etc.) with the ground-truth causal direction labelled by
pendinga“- ”and“- ”tothemodelname. Table3pro-
human experts. We use the version of TCEP providedin N L
videsasummaryoftheresults.ANM,IGCI- andIGCI-
theCausalDiscoveryToolboxopen-sourcePythonpackage N U
havesimilarperformances,consistentwithpreviousresults.
(Kalainathan&Goudet, 2019) which delivers the data al-
LiNGAM does poorly while KCDC performs just behind
readypreprocessede.g.,removalofmultivariatepairs(not
IGCI, consistent with results reported in one of the previ-
necessaryforallmodelsasevidentinprevioussectionbut
ousworks. Asexpected,bothvanillaKIIMandKIIM-HT
ratherforfaircomparisonstopreviouswork)andarrange-
performpoorlyduetothepresenceofnoiseandoutliersin
ment of variables so that x and y is always cause and ef-
real-worlddata,whereasre-weightedKIIManditshetero-
fect, respectively. Additionally,some pairsexhibitsignifi-
geneousextension,ingeneral,doeswell,withKIIM-HT-
cantlylargesamplesize(i.e.,thelargestsamplesizeofall L
performingthebest.
the pairs is 16382)and it is well-knownthat kernel meth-
ods,atleastwhennaivelyimplemented,doesnotscalewell
5.4.Sensitivitytoλ
with large training sets. Therefore, to cater to our kernel-
based method, in each experimentalinstance, the training Weexaminetheimpactofthekernelregularizationparam-
set is reduced via randomly sampling the first 400 sam- eter λ inherent to the kernel based methods (e.g., KCDC,
plesfromtheoriginalset. Forfaircomparisonswesubject KIIM and KIIM-HT). The motivation for this experimentCausalDiscoverybyKernelDevianceMeasureswithHeterogeneousTransforms
searchoverrandλ ,respectively.Foreachhyperparame-
ref
Table3.PerformanceonTCEP.
tercombination,werunthesuiteofexperimentspresented
Method TCEP(%) inthesyntheticdatasectionaswellasthetwo-dimensional
synthetic data section. The results are given in Appendix
ANM 53.4 2.42
± D.Forbothexperimentalsuites,weseethattheresultsare
IGCI- 61.6 0.49
N ± veryconsistent. Inthefirstsuiteofexperiments,weobtain
IGCI- 65.0 1.26
U ± unvarying results of one hundred across all experimental
KCDC 59.8 1.72
± settingsexceptforANM-2wherewithǫ (0,1)weob-
LiNGAM 34.2 1.60
∼N
KIIM
40.6±
2.33
tain ameanperformanceof94.04withSD 4.30while for
± ǫ (0,1)ameanof64.92withSD5.18,displayingquite
Rw-KIIM- 69.8 1.94
∼U
N ± robustperformances.Thesecondsuiteofexperimentssim-
Rw-KIIM- 62.4 0.80
L ± ilarlydisplaysconsistentperformanceswithmostofthefig-
KIIM-HT 37.8 0.75
± uresinthe90sandjustafewinthe80sand70s.
Rw-KIIM-HT- 71.2 1.72
N ±
Rw-KIIM-HT- 69.0 2.1
L ±
6. Conclusion
In this work, we focus on the problem of discovering the
partially stems from our inability to replicate the results
direction of causation of two potentially multivariate ran-
of KCDC and we suspect that KCDC’s performancemay
dom variables. We discuss some limitations of current
vary significantly with λ which is a value that is unspec-
state-of-the-art methods where specifically we show that
ified in the experiments of the original paper. We test
a global linear projection of the conditionalmean embed-
the performance of the kernel based methods for λ
dingisaninsufficientmetrictocomparetheintrinsicinvari-
∈
10−3,10−2,10−1,1,5,10,50 on the synthetic and the
ance/deviance of the conditional distributions. To rectify
{ }
two-dimensional synthetic dataset. We additionally test
this, weproposeKernelIntrinsicInvarianceMeasurewith
KCDC onthe two-dimensionalsyntheticdatasetwith 100
HeterogeneousTransformation(KIIM-HT)which uses an
samples. TheresultsaregiveninAppendixC.Forthethe
artificialneuralnetworktrainedviastochasticgradientde-
synthetic dataset, the performance of KCDC is positively
scent to heterogeneously and locally project each condi-
correlated, peaking and maintaining thereafter at λ = 1
tional mean embedding into a subspace where only rele-
while KIIM and KIIM-HT both exhibit performances in-
vanthigher-orderstatistics areextractedforcausaldiscov-
variant to λ. For the two-dimensional dataset, KCDC is
ery. Resultsandcomparisonsonasyntheticdataset,atwo-
similarlypositivelycorrelatedwithλwhereasKIIMisalso
dimensionalsyntheticdatasetandareal-worlddatasetver-
positively correlated while KIIM-HT is negatively corre-
ify the effectiveness of our approach while a sensitivity
lated. With 100 samples, KCDC is also positively corre-
analysis to the regularizationparameter attempts at a can-
latedwithλ,doingpoorlyinthebeginning,performingper-
didcomparisontopreviousworkandfinally,anexperiment
fectatλ=10−1thendecliningthereafter.Theevidenceis
with trials on various hyperparameter values demonstrate
infavorofourhypothesisthatKCDC’sstrongperformance
therobustnessofouralgorithm.
maybeduetolargeλ. Westressthatsuchasettingwould
be unusualbecausethe mainpurposeofλ is toregularize
References
the Gram matrix and an undulylarge λ oughtto interfere
withthedata. Furthermore,theeffectsofalargeλismag- Chen, Z., Zhang, K., Chan, L., and Scho¨lkopf, B.
nifiedforKCDCrelativetoKIIMandKIIM-HTasthefor- Causal discovery via reproducing kernel hilbert space
mer usesa regularizationterm givenbynλ as opposedto embeddings. Neural Computation, 26(7):1484–
justλforthelattertwo. 1517, 2014. doi: 10.1162/NECO a 00599. URL
\ \
https://doi.org/10.1162/NECO_a_00599.
5.5.RobustnesstoHyperparameters PMID:24708374.
ComparedtoKCDCandKIIM,KIIM-HTintroducesafew
Chen, Z., Zhu, S., Liu, Y., and Tse, T. Causal discovery
additional tunable hyperparametersand in this last exper-
by kernel intrinsic invariancemeasure. 2019. preprint,
iment, we demonstrate the robustness of our method to https://arxiv.org/abs/1909.00513.
different hyperparameter values. We focus on the rank
of the projection matrix r and the regularization hyperpa- Daniusˇis, P., Janzing, D., Mooij, J., Zscheischler,
rameter λ . A straightforward approach is taken where J., Steudel, B., Zhang, K., and Scho¨lkopf, B.
reg
we performa grid search of the hyperparametersover the Inferring deterministic causal relations. In Pro-
values A B where A = 5,10,20,80,100 and ceedings of the Twenty-Sixth Conference on Un-
B = r 10× −4,1λr 0eg −3,10−2,1r 0−1,1{ isthesetofvalu} eswe certainty in Artificial Intelligence, UAI’10, pp.
λreg
{ }CausalDiscoverybyKernelDevianceMeasureswithHeterogeneousTransforms
143–150, Arlington, Virginia, United States, 2010. Lemeire, J. and Dirkx, E. F. Causal models as minimal
AUAI Press. ISBN 978-0-9749039-6-5. URL descriptionsofmultivariatesystems. 2006.
http://dl.acm.org/citation.cfm?id=3023549.3023566.
Mitrovic, J., Sejdinovic, D., and Teh, Y. W. Causal
Dasgupta, I., Wang, J. X., Chiappa, S., Mitro- inference via kernel deviance measures. In Pro-
vic, J., Ortega, P. A., Raposo, D., Hughes, E., ceedings of the 32Nd International Conference on
Battaglia, P. W., Botvinick, M., and Kurth-Nelson, Neural Information Processing Systems, NIPS’18, pp.
Z. Causal reasoning from meta-reinforcement 6986–6994, USA, 2018. Curran Associates Inc. URL
learning. CoRR, abs/1901.08162, 2019. URL http://dl.acm.org/citation.cfm?id=3327757.3327802.
http://arxiv.org/abs/1901.08162.
Mooij, J., Janzing, D., Peters, J., and Scho¨lkopf, B.
Regression by dependence minimization and its appli-
Gelman, A. Causality and statistical learn-
cation to causal inference in additive noise models. In
ing. American Journal of Sociology, 117(3):
Proceedings of the 26th Annual International Confer-
955–966, 2011. doi: 10.1086/662659. URL
ence on Machine Learning, ICML ’09, pp. 745–752,
https://doi.org/10.1086/662659.
New York, NY, USA, 2009. ACM. ISBN 978-1-
Gong,M.,Zhang,K.,Huang,B.,Glymour,C.,Tao,D.,and 60558-516-1. doi: 10.1145/1553374.1553470. URL
Batmanghelich,K. Causalgenerativedomainadaptation http://doi.acm.org/10.1145/1553374.1553470.
networks. 042018.
Mooij, J., Stegle, O., Janzing, D., Zhang, K., and
Scho¨lkopf, B. Probabilistic latent variable models for
Goodfellow, I., Bengio, Y., and Courville,
distinguishingbetweencauseandeffect. volume23,pp.
A. Deep Learning. MIT Press, 2016.
1687–1695,012010.
http://www.deeplearningbook.org.
Mooij, J. M., Peters, J., Janzing, D., Zscheischler,
Gru¨nwald, P. D. and Vita´nyi, P. M. B. Algorithmic in-
J., and Scho¨lkopf, B. Distinguishing cause from
formation theory. CoRR, abs/0809.2754, 2008. URL
effect using observational data: Methods and
http://arxiv.org/abs/0809.2754.
benchmarks. J. Mach. Learn. Res., 17(1):1103–
1204, January 2016. ISSN 1532-4435. URL
Heinze-Deml, C., Maathuis, M. H., and Meinshausen,
http://dl.acm.org/citation.cfm?id=2946645.2946677.
N. Causal structure learning. Annual Review of
Statistics and Its Application, 5(1):371–391, 2018. Pearl, J. Causality: Models, Reasoning and Inference.
doi: 10.1146/annurev-statistics-031017-100630. URL CambridgeUniversityPress, New York,NY, USA,2nd
https://doi.org/10.1146/annurev-statisticse-d0it3io1n0,1270-091.0I0S6B3N00.52189560X,9780521895606.
Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., Rudin, W. Fourier Analysis on Groups. Wiley Classics
and Scho¨lkopf, B. Nonlinear causal discovery with Library. Wiley, 1990. ISBN 9780471523642. URL
additive noise models. In Koller, D., Schuurmans, https://books.google.ca/books?id=WlzLFUo0VzYC.
D., Bengio, Y., and Bottou, L. (eds.), Advances
Scho¨lkopf, B. Causality for Machine Learning. arXiv e-
in Neural Information Processing Systems 21, pp.
prints,art.arXiv:1911.10500,Nov2019.
689–696. Curran Associates, Inc., 2009. URL
http://papers.nips.cc/paper/3548-nonlineaSrch-oclakoupsfa,Bl.-adnidsScmoovlae,rAy.-J.wLietahrn-iangdwdiithtiKveren-enlso:iSsupe--models.pdf.
portVectorMachines,Regularization,Optimization,and
Janzing, D., Chaves, R., and Scho¨lkopf, B. Al- Beyond.MITPress,Cambridge,MA,USA,2001.ISBN
gorithmic independence of initial condition and 0262194759.
dynamical law in thermodynamics and causal in-
ference. New Journal of Physics, 18(9):093052, Shimizu, S., Hoyer, P. O., Hyva¨rinen, A., and Kermi-
sep 2016. doi: 10.1088/1367-2630/18/9/093052. URL nen, A. A linear non-gaussian acyclic model for
https://doi.org/10.1088%2F1367-2630%2F18%2cFa9u%sa2lF0d9is3co0v5e2ry .. J. Mach. Learn. Res., 7:2003–
2030, December 2006. ISSN 1532-4435. URL
Kalainathan,D.andGoudet,O. Causaldiscoverytoolbox: http://dl.acm.org/citation.cfm?id=1248547.1248619.
Uncovercausalrelationshipsinpython,032019.
Song, L., Fukumizu, K., and Gretton, A. Kernel em-
Kingma,D. P.andBa, J. Adam: A methodforstochastic beddings of conditional distributions: A unified ker-
optimization. CoRR,abs/1412.6980,2014. nelframeworkfornonparametricinferenceingraphical
models. IEEE Signal Processing Magazine, 30(4):98–
LeCun,Y.,Bengio,Y.,andHinton,G. Deeplearning. Na- 111, July 2013. ISSN 1053-5888. doi: 10.1109/MSP.
ture,521:436–44,052015. doi:10.1038/nature14539. 2013.2252713.CausalDiscoverybyKernelDevianceMeasureswithHeterogeneousTransforms
Spirtes, P., Glymour, C., and Scheines, R. Causation,
Prediction, and Search, volume 81. 01 1993. doi:
10.1007/978-1-4612-2748-9.
Sun, X., Janzing, D., Scho¨lkopf, B., and Fukumizu,
K. A kernel-based causal learning algorithm. In
Proceedings of the 24th International Conference
on Machine Learning, ICML ’07, pp. 855–862,
New York, NY, USA, 2007. ACM. ISBN 978-1-
59593-793-3. doi: 10.1145/1273496.1273604. URL
http://doi.acm.org/10.1145/1273496.1273604.
Sun, X., Janzing, D., and Scho¨lkopf, B. Causal rea-
soning by evaluating the complexity of conditional
densities with kernel methods. Neurocomput.,
71(7-9):1248–1256, March 2008. ISSN 0925-
2312. doi: 10.1016/j.neucom.2007.12.023. URL
http://dx.doi.org/10.1016/j.neucom.2007.12.023.
Waldrop, M. M. News feature: What are the limits
of deep learning? Proceedings of the National
Academy of Sciences, 116(4):1074–1077, 2019. ISSN
0027-8424. doi: 10.1073/pnas.1821594116. URL
https://www.pnas.org/content/116/4/1074.
Yang, C. H., Liu, Y., Chen, P., Ma, X., and
Tsai, Y. J. When causal intervention meets image
masking and adversarial perturbation for deep neu-
ral networks. CoRR, abs/1902.03380, 2019. URL
http://arxiv.org/abs/1902.03380.
Zhang, K. and Hyva¨rinen, A. On the identifiabil-
ity of the post-nonlinear causal model. In Pro-
ceedings of the Twenty-Fifth Conference on Un-
certainty in Artificial Intelligence, UAI ’09, pp.
647–655, Arlington, Virginia, United States, 2009.
AUAI Press. ISBN 978-0-9749039-5-8. URL
http://dl.acm.org/citation.cfm?id=1795114.1795190.