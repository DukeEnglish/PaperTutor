Rank Supervised Contrastive Learning for Time Series Classification
QianyingRen1, DongshengLuo2, DongjinSong1∗
1UniversityofConnecticut
2FloridaInternationalUniversity
{qianying.ren,dongjin.song}@uconn.edu,dluo@fiu.edu
Abstract intervention. Withthehugeamountoftimeseriesdata,how
tocategorizeandinterpretthestatusbecomesacriticalissue
Recently, various contrastive learning techniques toinvestigate.
havebeendevelopedtocategorizetimeseriesdata
and exhibit promising performance. A general Traditionally, one of the most popular time series classifica-
paradigm is to utilize appropriate augmentations tion approaches is to use the nearest neighbor (NN) classi-
and construct feasible positive samples such that fier based on a distance function [Lines and Bagnall, 2015].
theencodercanyieldrobustanddiscriminativerep- For instance, Dynamic Time Warping (DTW) distance has
resentations by mapping similar data points closer beenusedtogetherwithanNNclassifier(DTW-NN)topro-
togetherinthefeaturespacewhilepushingdissim- vide a strong baseline [Bagnall et al., 2016]. In addition,
ilar data points farther apart. Despite its efficacy, ensemble methods have been shown to outperform DTW-
the fine-grained relative similarity (e.g., rank) in- NN by ensembling individual NN classifiers with different
formation of positive samples is largely ignored, distance measures over the same or different feature spaces.
especially when labeled samples are limited. To More recently, Collective Of Transformation-based Ensem-
this end, we present Rank Supervised Contrastive bles (COTE) combines the strengths of multiple approaches
Learning (RankSCL) to perform time series clas- tohandlevariousaspectsoftimeseriesdatacanyieldbetter
sification. Different from conventional contrastive classificationaccuracy.Linesetal.furtherextendedCOTEto
learningframeworks,RankSCLaugmentsrawdata createHIVE-COTE[Linesetal.,2018]byincorporatingahi-
in a targeted way in the embedding space and erarchical vote system. These approaches, however, involve
adoptscertainfilteringrulestoselectmoreinforma- highcomplexityforbothtrainingandinference.
tive positive and negative pairs of samples. More-
Morerecently, deeplearningbasedtimeseriesclassification
over, a novel rank loss is developed to assign dif-
algorithms, e.g., InceptionTime [Ismail Fawaz et al., 2020],
ferent weights for different levels of positive sam-
are becoming more popular as they have shown promis-
ples,enabletheencodertoextractthefine-grained
ing performance and can obtain more effective representa-
informationofthesameclass,andproduceaclear
tions.However,supervisedlearningmayrequireasubstantial
boundaryamongdifferentclasses.Thoroughlyem-
amount of high-quality labeled time series data for training
pirical studies on 128 UCR datasets and 30 UEA
which could be infeasible in many real-world applications.
datasets demonstrate that the proposed RankSCL
Therefore,severalcontrastivelearningbasedtimeseriesrep-
canachievestate-of-the-artperformancecompared
resentationtechniqueshavebeendevelopedtoresolvethisis-
toexistingbaselinemethods.
sue. The key idea is to leverage appropriate augmentations
andconstructfeasiblepositivesamplessuchthattheencoder
1 Introduction
can yield robust and discriminative representations by map-
Nowadays, time series data are becoming ubiquitous in nu- ping similar data points closer together in the feature space
merous real-world applications. For instance, in a power while pushing dissimilar data points farther apart. For in-
plant [Prickett et al., 2011], a large number of sensors can stance, TimCLR [Yang et al., 2022] uses the DTW [Muller,
be employed to monitor the operation status in real time. 2007]toprovidephase-shiftandamplitude-changeaugmen-
Withafitnesstrackingdevice,atemporalsequenceofactions tations to make the representation learning tied to tempo-
[Parkka et al., 2006], e.g., walking for 5 minutes, running ral variations. TS2Vec [Yue et al., 2022a] jointly consid-
for 1 hour, and sitting for 15 minutes, etc, can be recorded ers instance-wise and patch-wise augmentations and sepa-
and detected with related sensors. In healthcare, we can de- ratesseveraltimeseriesintovariouspatches. InfoTS[Luoet
tectepilepticseizuresbyclassifyingEEGdataintotwocate- al.,2023a]utilizesinformationtheorytogenerateappropriate
gories,i.e.,normalandabnormal,andprovidetimelymedical timeseriesdataaugmentationsbypromotinghighfidelityand
variety. Despite their efficacy, the fine-grained relative sim-
∗CorrespondingAuthor ilarity (e.g., rank) information of positive samples is largely
4202
naJ
13
]GL.sc[
1v75081.1042:viXraignored,especiallywhenlabeledsamplesarelimited. wise and patch-wise levels by TS2Vec [Yue et al., 2022a]
framework, which separates several time series into various
Tothisend,weproposeRankSupervisedContrastiveLearn-
patches. Meanwhile, a novel temporal contrastive learning
ing (RankSCL) to tackle this issue and yield more effec-
taskisintroducedbyTS-TCC[Eldeleetal.,2021a],prompt-
tiverepresentationstofacilitatetimeseriesclassification. By
ingaugmentationstopredicteachother’sfuturesequence.
rankingtheimportanceofdifferentpositivesamples,wecan
betterunderstandthepotentiallandscapeofthefeaturespace.
To capture distinctive seasonal and trend patterns, CoST
Specifically, we make full use of the information of posi- [Wooetal.,2022]appliescontrastivelossesacrosstimeand
tive samples by leveraging their relatively similarity infor- frequencydomains. Additionally,TF-C[Zhangetal.,2022]
mation in terms of rank. We encode the rank by taking
aimstooptimizetime-basedandfrequency-basedrepresenta-
account of the number of triplets in which the distance of
tionsofthesameexampletobecloserandintroducesaninno-
anchor-negative pairs is smaller than anchor-positive pairs. vative time-frequency consistency framework. InfoTS [Luo
Based on that, a targeted data augmentation technique is et al., 2023a], leveraging information-theoretic principles to
designed to generate designated samples, aiming to enrich
generateappropriatetimeseriesdataaugmentationsbymax-
the information of samples from the same category and en-
imizinghighfidelityandvariety. Despitethesemethodolog-
hancetheboundaryamongsamplesfromdifferentcategories.
ical innovations, all of these methods exist a common lim-
Bycombiningthesetwotechniques,ourproposedRankSCL
itation: none directly address the generation of reasonable
has been thoroughly evaluated on 128 UCR datasets and 30
positive and negative samples nor learning of discriminative
UEA datasets. Our experiment results demonstrate that the
boundariesfordifferentclasses.
proposedRankSCLcanachievestate-of-the-artperformance
compared to existing baseline methods. Our main contribu-
2.2 TimeSeriesClassification
tionsinclude:
The field of time series classification is fundamental and
• We develop a novel rank supervised contrastive learning
rapidly evolving. Some conventional methods are not based
frameworkandpresentanovelranklossthatassignsdiffer-
on deep learning such as TS-CHIEF [Shifaz et al., 2020],
entweightstodifferentlevelsofpositivesamples.
ROCKET [Dempster et al., 2020], and DTW-NN [Iwana et
• We propose a targeted data augmentation technique based al., 2020], have been foundational. With the rapid develop-
onRSCLtogeneratedesignatedpositivesamplesthatcan ment of deep learning, an increasing number of models are
enrichtheinformationofsamplesfromthesamecategory emerginginthisdomain,exhibitingremarkableperformance.
andenhancetheboundaryamongdifferentcategories. Deeplearningalgorithmscangetbetterclassificationresults
thanstatistics-basedmethodsbecausetheyarebetteratlearn-
• Our empirical studies on 128 UCR datasets and 30 UEA ing rich representations during training. InceptionTime[Is-
datasets demonstrate that the proposed RankSCL outper- mail Fawaz et al., 2020] applied the Inception Networks in
formsthestate-of-the-art.
time series and can capture local patterns. The attention
mechanismwasemployedbyMACNN[LaiandWang,2020]
2 RelatedWork
toenhancethemulti-scaleCNNs’classificationperformance.
TheproposedRankSCLiscloselyrelatedtocontrastivetime To address multi-variate TSC difficulties, Hao et al. intro-
series representation learning, time series classification, and ducedCA-SFCN[HaoandCao, 2020], whichusedvariable
timeseriesbaseddataaugmentation. andtemporalattentionmodulation.
2.1 ContrastiveTimeSeriesRepresentation
2.3 TimeSeriesDataAugmentation
Learning
Dataaugmentationisessentialforthesuccessfuluseofdeep
Forcontrastivelearning,itisessentialtoenrichtherepresen-
learningmodelsontimeseriesdatasinceitisapowerfultech-
tation space through the manual generation of positive and
nique to increase the quantity and size of the training data.
negative pairs. In traditional, positive pairs representations
Conventional methods of time series data enhancement can
are aligned closely together, while negative pairs are delib-
be broadly categorized into three domains, the first method
erately distanced. As illustrated by the SimCLR [Chen et
isthetimedomain,thesecondisthefrequencydomain,and
al., 2020] framework, different views of augmentations for
thethirdisthetime-frequencydomain. Theeasiestdataaug-
thesamesamplearetreatedaspositivepairs,whileaugmen-
mentation techniques for time series data are the transforms
tations applied to different samples are regarded as negative
inthetimedomain. Themajorityofthemperformdirectma-
pairs.
nipulations on the original input time series, such as adding
However, recent works in time series contrastive learning Gaussian noise or more intricate noise patterns like spikes,
have introduced various pair designs to leverage the invari- steps,andslopes. However,thereareafewwaystoconsider
antfeaturesoftimeseriesdata. Forinstance,TimCLR[Yang dataaugmentationfromafrequencydomainview. Fortime-
et al., 2022] employs the DTW [Muller, 2007] to facili- frequency domain, the authors of [Yao et al., 2019] use the
tatephase-shiftandamplitude-changeaugmentations,which shortFouriertransform(STFT)tocreatetime-frequencyfea-
are better suited for time series context, to ensure represen- tures for sensor time series. They then supplement the data
tation learning is intrinsically connected to temporal varia- byusingthesefeaturessothatadeepLSTMneuralnetwork
tions. Thecontrastivelossisfurtherdefinedinbothinstance- canclassifyhumanactivities.Figure1:OverviewofRankSCLframework,consistingofthreecomponents:(1)aFullyConvolutionalNetworkthatcapturestheembeddings
ofrawtimeseriesinstances,(2)targeteddataaugmentationthatgeneratesmoresamplesintheembeddingspace,(3)selectionofvalidtriplets
andcalculationofranklosstotraintheencodernetwork. Eventhoughthisfigureshowsaunivariatetimeseriesinstanceasanexample,the
architecturesupportsmultivariateinstances.
3 Method Encoder Network, f (·). As shown in Figure 1, we utilize
θ
3-layer Fully Convolutional Networks (FCN) to map the in-
3.1 NotationsandProblemDefinition
putrawtimeseriesdatax intherepresentationr =f (x ).
i i θ i
A time series instance x is represented by a T ×F matrix,
i Each module consists of a convolutional layer followed by
whereT isthetimestepandF isthefeaturedimension.With a batch normalization layer [Ioffe and Szegedy, 2015] and a
F = 1,xisaunivariateinstance,otherwisexisknownasa Rectified Linear Units (ReLU) activation function [Agarap,
multivariateinstance. GivenasetofN timeseriesinstances 2018]. To minimize the number of weights, features are
X={x ,x ,x ,...,x },theobjectiveistolearnanonlinear
1 2 3 N routed into a global average pooling layer after the convo-
functionf thatmapseachxtoaD dimensionalvectorr ∈
θ lutionblocks.
RD, which preserves its semantics and D ≪ T × F. In
supervised settings, we have a subset of X, denoted by X , Projection Head, g (·). Inspired by the classic SimCLR
L θ
whereeachinstancexisassociatedwithalabely. framework [Chen et al., 2020], we included a small MLP
networkasaprojectionheadtofurthertransformr toanew
3.2 Framework
space that z = g (r ). We adopt a normalization function
i θ i
The overall architecture of RankSCL is shown in Figure 1. following the last layer to map representations in a unit hy-
Wefeedtherawtimeseriesdataintoanencodernetwork,f , perspheretofacilitatethedistanceandrankingcomputation.
θ
tolearnlow-dimensionalrepresentations,whicharethenfor- Theprojectionheadisonlyusedinthetrainingphaseandwill
ward propagation into a projection head. For each sample, bedroppedintheinference.
wetreatitasananchoranddefinethepositivesamplesbased
on the label information [Khosla et al., 2020]. Specifically, 3.4 EmbeddingSpaceDataAugmentation
instances with the same label as the anchor are considered
In the paradigm of contrastive learning, data augmentation
positivesamples. Otherwise, theyareusedasnegativesam-
emergesasapivotalcomponent,significantlyinfluencingthe
ples. To enrich the intra-class information, we utilize data
effectiveness of the model. The selection of a suitable aug-
augmentation techniques on embeddings of all samples. A
mentation strategy, particularly for time series data, stands
rank-supervised contrastive loss function is adopted to train
asacrucialchallenge. Intheliterature, acommonapproach
theencodernetworkandprojectionhead. Toinferrepresen-
involves selecting augmented data points that are substan-
tations for time series classification, we follow the existing
tially divergent from the original data, thereby introducing
contrastivelearningframeworks[Chenetal.,2020]toignore
increasedvariabilitytofosteramorerobustencoder.Yet,this
theprojectionheadandusehiddenrepresentationsproduced
strategyoftenleadstoissuesofdistributionalshifts,ashigh-
bytheencodernetwork. lightedinrecentstudies[Yueetal.,2022a].
3.3 ModelArchitecture.
Diverging from traditional contrastive learning approaches
Themaincomponentsofourmethodcontaintheencodernet- thatapplydataaugmentationdirectlytorawtimeseriesdata,
workandtheprojectionhead. ourmethodaddressesthesechallengeswithintheembeddingspace. For each training instance and its corresponding la-
bel(x ,y )inthelabeleddatasetX , weutilizetheencoder
i i L
network coupled with a projection head to generate a com-
pactembeddingz ofx withinaunithypersphere. Weintro-
i i
ducejitteringwithscaleα, denotedbyt asaselectedaug-
α
mentationoperation.Anaugmentedinstanceisthenobtained
throughz′ =t (z).
i α
Tofurtherenhancediversity,weconsiderasetoftwojittering
operationswithdistinctscales, T = {t ,t }. Inpractice,
α1 α2
we set α = 0.03 and α = 0.05. As indicated in prior re-
1 2
search [Yue et al., 2022a], augmentations performed in the
hidden space effectively preserve label information. Conse-
quently, both the augmented instances and their original la-
belsareincorporatedintothecurrenttrainingbatch.
Figure2:ValidTripletSelection
By applying small-scale jittering to embeddings in a com-
pact unit hypersphere, our augmentation technique not only
3.6 RankSupervisedContrastiveLearningLoss
enrichesvalidinformationbutalsomitigatespotentialissues
Intraditionalcontrastivelearning,akeyoverlookedaspectis
relatedtodistributiondriftorthecreationofoutliers.Thisap-
the variable distances of different positive samples from the
proach,inturn,enhancesthetrainingprocessoftheencoder
anchor.Treatingallpositivesamplesequally,withoutconsid-
networkwithinthecontrastivelearningframework.
eringtheirproximitytotheanchor, maynoteffectivelycap-
ture the nuanced potential representations of a class. Addi-
3.5 SelectionofValidTripletPairs
tionally, this approach is susceptible to the influence of out-
liers,potentiallyintroducingnoiseintothelearningprocess.
Intherealmofcontrastivelearning, thefoundationalgoalis
tomaximizethesimilaritybetweenpositivesamplesanden- Toaddressthisissue,weintroducetheconceptofrankingdif-
sure that negative samples remain distinctly separate. Early ferentpositivesamplesandproposeanovelrank-supervised
contrastive learning models typically incorporated a single contrastivelearninglossfunction.Thisfunctiondifferentially
positive and a negative pair within a minibatch, as estab- weightspositivesamplesbasedontheirutilityintrainingthe
lished by seminal works [Chopra et al., 2005]. Recent ad- model. Wefirstdefinethesetofvalidhardnegativesamples
vancements have introduced multiple positive and negative forananchor-positivepair(x ,x )as:
a p
pairs[Sohn,2016;Khoslaetal.,2020],leadingtosignificant
X(n) ={x |dist(x ,x )<dist(x ,x )} (1)
strides in diverse fields including computer vision [Khosla ap n a n a p
et al., 2020] and natural language processing [Gunel et al.,
The intuition behind this approach is that a positive sample
2021]. However,thesemethodsofteninvolveconsideringall
withfewervalidhardnegativesamplesislikelyclosertothe
anchor-negative pairs, which can be computationally inten-
anchorandthereforemoreinformativefortheclassrepresen-
sive.
tation. Such samples should be given more weight. Con-
versely,apositivesamplewithalargerX(n) mightbedistant
To optimize this process, we propose the novel concept of ap
from the class centroid and treated with lower weight, as it
a “valid hard negative pair” for contrastive learning. In
could potentially be an outlier introducing noise. To imple-
this context, for an anchor instance x and a correspond-
a
ment this, we rank positive samples based on the count of
ing positive sample x , a negative sample x is deemed
p n
theirvalidhardnegativesamples. Inthisframework,distinct
“validhard”ifthedistancedist(x ,x )betweentheanchor
a n
positive samples receive different weights during the learn-
x and the negative sample x is less than the distance be-
a n
ingprocess. Ahigh-rankpositivesample,indicatingalarger
tweentheanchorandthepositivesample,i.e.,dist(x ,x )<
a n
number of valid hard negatives, may offer less valuable in-
dist(x ,x ). Atripletformedbythesecriteria,(x ,x ,x ),
a p a p n
formationfortheclassandismorelikelytobeanoutlier. In
isthendefinedasavalidtripletpair. Thisapproachfocuses
contrast, a low-rank positive sample, with fewer valid hard
onmorechallengingandinformativenegativeexamplesdur-
negatives, is weighted more heavily, suggesting it provides
ing training, thereby enhancing the discriminative power of
morerelevantinformationfortheclass.
theresultantrepresentations.
Formally, wedefinetherankofapair(x ,x )basedonthe
a p
Theintroductionofvalidtripletpairssubstantiallyimproves sizeofitsvalidhardnegativesampleset:
class separation while maintaining closer distances between
|X(n)|
positive samples and their anchors. Moreover, the selective (cid:88)ap
focus on valid triplets in contrastive training not only hones R(x a,x p)= 1(dist(x a,x n)≤dist(x a,x p)). (2)
inonmoreinstructivenegativesamplesbutalsoreducescom- n
putational requirements, thereby enhancing the overall effi- Here, 1(·) is an indicator function, returning 1 if the con-
ciencyofthealgorithm. dition is met, otherwise 0. This rank-based approach aimsAccuracy
6 5 4 3 2 1
TS-TCC RankSCL
DTW TS2Vec
InfoTS T-Loss
Figure4: CriticalDifference(CD)diagramofUnivariateTimese-
riesclassificationtask
Accuracy
6 5 4 3 2 1
TS-TCC RankSCL
T-Loss TS2Vec
Figure3:PropertiesofrankfunctionR(·) DTW InfoTS
Figure 5: Critical Difference (CD) diagram of Multivariate Time
to enhance the model’s ability to discern between more and
seriesclassificationtask
less informative positive samples, thereby refining the train-
ingprocessandimprovingtheoverallqualityoflearnedrep-
resentations. Remark. The choice of mapping functions significantly in-
fluencestheresultsofourmodel. Inexploringdifferentfunc-
Thediscretenatureoftheindicatorfunctionintherankfunc- tions,wecomparedtheperformanceofthelog(1+z)func-
tion R(x a,x p) presents a challenge for optimization during tion against the arctan(x) function. Our findings indicated
thetrainingprocess. Toaddressthis,wereplacetheindicator thatthelog(1+z)functionledtoinferiorperformancecom-
functionwithacontinuousapproximationusingthesigmoid pared to the arctan(x) function. This discrepancy arises
function σ(·). Consequently, the revised rank function be- because, with an increasing rank value, the rate of change
comes: in the log(1 + z) function is more pronounced than in the
|X(n)| arctan function. Additionally, arctan has upper and lower
ap
(cid:88) bounds, meaning that for higher rank values, the samples
R(x ,x )= σ(dist(x ,x )−dist(x ,x )), (3)
a p a p a n with minimal derivative changes will have a slower rate of
n
change, thereby exerting less influence on the learning pro-
whereσ(k)= 1 isthesigmoidfunction. cess. Theconceptualframeworkofourlossfunctiondesign
1+e−k
shares some similarities with the approach in [Song et al.,
In order to assign differentiated weights to positive samples 2018], yet it remains distinct in application and methodol-
based on their ranks, we introduce a novel objective func-
ogy. Ourmethodutilizestheinversetangentfunctiontomap
tion that penalizes poorly ranked samples more stringently.
rankingvalues,applyingthisinthecontextofrepresentation
Samples with lower ranks, indicating a greater number of
learning. This approach effectively adjusts the weights of
closer negative samples to the anchor, should receive lesser
different positive samples, enhancing the model’s ability to
weight. This is because these samples may potentially be
distinguishclassboundariesandextractmeaningfullearning
outliers, providing noisy information that is not conducive
representationsforvariouscategories. Thisnuancedapplica-
forextractinggeneralclassrepresentations. Forthispurpose,
tionofthearctanfunctioncatersspecificallytothechallenges
we employ the arctan function to map the ranks. This func-
of rank-supervised contrastive learning, balancing the need
tionensuresthatasRincreaseslinearly,theincrementinthe
foreffectiveclassseparationwiththenuancesofrepresenta-
loss function gradually diminishes, reflecting the decreasing
tionlearning.
significance of higher-ranked (potentially noisier) samples.
Therefore,ourfinalobjectivefunctionis:
4 Experiments
(cid:88)(cid:88)
L(R(x ,x ))= arctan(R(x ,x )) (4) Weassessthelearnedrepresentationsofourmethodontime
a p a p
a p seriesclassificationtaskinthispart. Thecomprehensiveex-
perimentalresultsanddetailsareshownintheappendix.
This loss function can then be used to perform backpropa-
gation during the training process. Utilizing the Adam opti- 4.1 DatasetsandBaselines
mizer,thismethodologyfacilitatesthelearningofdiscrimina-
The classic two different kinds of benchmark datasets are
tiverepresentations, effectivelybalancingtheneedtopriori-
usedfortheevaluation1.Forthemultivariatetimeseriesclas-
tize informative samples while mitigating the impact of less
usefuloutliers. 1https://www.cs.ucr.edu/∼eamonn/time series data/Dataset RankSCL InfoTS TS2Vec T-Loss TS-TCC DTW
UCR 2.328 3.621 2.395 3.332 4.918 4.406
UEA 2.067 3.350 3.150 4.067 4.600 3.767
Table1:AverageRankvaluesfor128UCRdatasetsand30UEAdatasets
128UCR 30UEA
Method ACC↑ Prec. ↑ F1↑ Recall↑ ACC↑ Prec. ↑ F1↑ Recall↑
RankSCL 0.821 0.817 0.803 0.810 0.715 0.719 0.705 0.712
InfoTS 0.733 0.723 0.705 0.714 0.669 0.672 0.657 0.664
TS2Vec 0.822 0.816 0.799 0.807 0.695 0.687 0.672 0.677
TS-TCC 0.685 0.603 0.566 0.584 0.617 0.573 0.533 0.552
T-Loss 0.782 0.750 0.743 0.747 0.581 0.572 0.545 0.558
DTW 0.679 0.672 0.648 0.660 0.654 0.645 0.624 0.635
Table2:ClassificationResultson128UCRdatasetsand30UEAdatasets
sificatin task we use the UEA archive [Bagnall et al., 2018] datasets. IntermsofACC,ourmethodoutperformsTS2Vec
consists of 30 multivariate datasets, while the UCR archive in72outof128UCRdatasets,InfoTS[Luoetal.,2023a]in
[Dau et al., 2018] has 128 univariate time series datasets 96datasets,andT-loss[Franceschietal.,2019]in88datasets
for the univariate time series classficiation task. The ex- outof125datasets.Figure4showstheCriticalDifferencedi-
tensiveexperimentsareconductedcomparedwithotherSO- agramofdifferentmethodson128UCRdatasets. Thisresult
TAs,suchasInfoTS[Luoetal.,2023a],TS2Vec[Yueetal., indicates that RankSCL has similar results with the TS2Vec
2022a],T-Loss[Franceschietal.,2019],TNC[Tonekaboniet [Yueetal.,2022a]modelintermsofAccuracyscores,while
al., 2021a],TS-TCC [Eldele et al., 2021a],TST [Zerveas et significantly outperforming the other methods. More details
al., 2020] and DTW [Muller, 2007]. The appendix contains of128UCRdatasetsareshownintheAppendix.
thecompleteexperimentresultsofourmethodonalldatasets.
To better test the quality of the representations, we chose 4.4 MultivariateTimeSeriesResults
morecomprehensivemetricstomeasure,theaccuracyofthe
Table 2 also shows the comparison results on UEA datasets
classification, the Precision score, and the F1 score. All of
for the multivariate time series classification. The proposed
thebaselinemodelsaremeasuredbythesamemetrics.
RankSCL significantly outperforms all baseline methods in
termsofaccuracy,precisionscore,F1score,andRecallvalue.
4.2 ImplementDetails
Inparticular, RankSCLoutperformsthebestbaselinemodel
For all datasets, a three-layer Fully Convolutional Network TS2Vec [Yue et al., 2022a] by 2.0% in terms of Accuracy,
structure is used to encode and a projection head with two by 3.2% in precision score, by 3.3% in F1 score and 3.5%
linear layers is applied. After the upstream encoder is rea- by Recall value. The results indicate that the proposed aug-
sonably trained, the learned representation will be passed to mentationtechniquescoupledwiththeranklossfunctionare
the SVM classifier with RBF kernel for classification. The more effective on multivariate time series data compared to
learningrateissetto0.00001andthenumberofaugmented univariatetimeseriesdata.
positive samples is a parameter. Experiments are performed
onanNVIDIAGeForceRTX3090. 4.5 AblationStudy
4.3 UnivariateTimeSeriesResults A full comparison of different variants of RankSCL model
isevaluatedonPigCVPdatasettojustifytheeffectivenessof
Table 2 summarizes the experiment results from the UCR
ourmethodandtheresultsareshowninTable3.Toassessthe
datasets. The Appendix contains the complete results in de-
effectivenessofthedataaugmentation,weevaluatedifferent
tail. RankSCL outperforms other baselines in terms of Pre-
numbers of augmented positive samples. The average accu-
cision score, F1 score and Recall value. It improves 0.1%
racywithoutaugmentedpositivesamplessignificantlydrops
classificationprecisionscore,0.4%F1scoreand0.3%Recall
16.1%, andthisisevenmoreobviousintermsofothermet-
value. We noticed that these baseline methods just compare
rics.Afterthenumberofaugmentedsamplesexceeds5,there
the accuracy values of the two datasets in their papers. To
isasignificantdropinperformance,provingthatthenumber
evaluatetheperformanceofthedifferentmethodsmorerea-
ofaugmentedpositivesamplesdoesnotlinearlyincreasewith
sonablyandcomprehensively,wealsocalculateandcompare
performance. We also evaluate the experiment of data aug-
theF1valuesandthePrecisionscorestomakeacomparison.
mentation on raw time series data (w/o Data Augmentation
Theresultsofalltheexperimentsaretheresultsobtainedby
(RawData)). InTable3,wecanobserveitisnotaseffective
takingthemeanvalueaccordingtothe5differentseeds.
asaugmentingthesamplesintheembeddingspace(w/oData
In particular, the RankSCL model achieves a similar accu- Augmentation), which can demonstrate the effectiveness of
racy score over the best baseline TS2Vec on all 128 UCR ourproposedmethod.Differentencoderarchitecturesareem-Avg. Accuracy↑ Avg. Precision↑ Avg. F1↑ Avg. Recall↑
RankSCL 0.797 0.834 0.793 0.813
w/oDataAugmentation 0.636 0.663 0.62 0.641
w/DataAugmentation(RawData) 0.519 0.550 0.500 0.524
w/oFCN(ResnetBackbone) 0.318 0.319 0.293 0.305
w/oRankLoss(CE) 0.636 0.663 0.620 0.641
w/oRankLoss(CTL) 0.676 0.710 0.663 0.686
w/oRankLoss(TL) 0.506 0.532 0.509 0.520
Table3:AblationstudiesonPigCVPdataset
Number Avg. Accuracy↑ Avg. Precision↑ Avg. F1↑ Avg. Recall↑
0 0.636 0.663 0.62 0.641
5 0.797 0.834 0.793 0.813
10 0.517 0.544 0.503 0.523
15 0.520 0.560 0.504 0.530
Table4:DataAugmentationAnalysisonPigCVPdataset
(a) SyntheticControl (b) TS2Vec (c) RankSCL
Figure6:Thet-SNEvisualizationofthelearnedrepresentationsofSyntheticControldataset(with6classes).(Bestviewedincolor)
ployed for comparison in Table 3. We observe that FCN ar- clearboundariesamongthedifferentclasses.
chitecture is more effective than other variations (w/o FCN
(Resnet Backbone)) on the PigCVP dataset. We also make
comparisons of different loss functions, including cross en- 5 Conclusion
tropy(CE), contrastive loss(CTL), and triplet loss(TL). The
resultssuggestthatRankbasedarctanlossfunctionisbetter Inthispaper,weproposeanovelsupervisedcontrastivelearn-
thanotheralternatives. ing framework termed RankSCL which learns optimal time
seriesrepresentationsforclassficationtask. Anewdataaug-
4.6 QualitativeEvaluation
mentationmethodisproposedtogeneratemoretargetedpos-
Based on t-SNE [Jansen et al., 2017], we project Synthet- itive samples in the embedding space, which enriches the
icControl dataset (with 6 classes) from raw space to the intra-class information. Moreover, a certain mining rule is
two-dimensionalspacebasedont-SNE[Jansenetal., 2017] applied to capture valid triplet pairs to reduce the computa-
(Figure 6(a)). Similarly, for TS2Vec and RankSCL, we tionalcomplexity. Wefurtherproposeanovelranklossfunc-
alsoprojecttheirlearnedembeddingstothetwo-dimensional tionthatsortsdifferentpositivesamplestolearnoptimalrep-
spacewitht-SNEandcomparetheirvisualizationresultsac- resentations.Theevaluationresultsprovetheeffectivenessof
cordingly. We can observe for both TS2Vec and RankSCL, thesestrategiesandcertifythatwithproperdataaugmentation
different classes are well separated and clustered together. methodology, enough valid triplet pairs, and abundant intra-
Moreover,TS2Vechas7clusterswhileRankSCLhas6clus- class information, the representations learned by RankSCL
ters which is consistent with the number of classes 6. This are highly qualified to be applied to more future tasks and
demonstratesthatourproposedRankSCLcanpreservemore othermodalitiesofdata.References ProceedingsoftheTwenty-NinthInternationalJointCon-
ferenceonArtificialIntelligence,2020.
[Agarap,2018] AbienFredAgarap.Deeplearningusingrec-
tifiedlinearunits(relu). CoRR,abs/1803.08375,2018. [IoffeandSzegedy,2015] Sergey Ioffe and Christian
Szegedy. Batch normalization: Accelerating deep
[Bagnalletal.,2016] Anthony J. Bagnall, Aaron Bostrom,
network training by reducing internal covariate shift,
James Large, and Jason Lines. The great time series
2015.
classification bake off: An experimental evaluation of
recently proposed algorithms. extended version. CoRR, [IsmailFawazetal.,2020] Hassan Ismail Fawaz, Benjamin
abs/1602.01711,2016. Lucas, Germain Forestier, Charlotte Pelletier, Daniel F
Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane
[Bagnalletal.,2018] Anthony J. Bagnall, Hoang Anh Dau,
Idoumghar, Pierre-Alain Muller, and Francois Petitjean.
JasonLines,MichaelFlynn,JamesLarge,AaronBostrom,
Inceptiontime: Finding alexnet for time series classifica-
Paul Southam, and Eamonn J. Keogh. The uea multi-
tion.DataMiningandKnowledgeDiscovery,34(6):1936–
variate time series classification archive, 2018. CoRR,
1962,2020.
abs/1811.00075,2018.
[Iwanaetal.,2020] Brian Kenji Iwana, Volkmar Frinken,
[Chenetal.,2020] Ting Chen, Simon Kornblith, Moham-
and Seiichi Uchida. Dtw-nn: A novel neural network
madNorouzi,andGeoffreyHinton. Asimpleframework
for time series recognition using dynamic alignment be-
for contrastive learning of visual representations. In In- tween inputs and weights. Knowledge-Based Systems,
ternationalconferenceonmachinelearning,pages1597–
188:104971,2020.
1607.PMLR,2020.
[Jansenetal.,2017] Aren Jansen, Manoj Plakal, Ratheet
[Chopraetal.,2005] SumitChopra,RaiaHadsell,andYann Pandya, Daniel P. W. Ellis, Shawn Hershey, Jiayang Liu,
LeCun.Learningasimilaritymetricdiscriminatively,with R. Channing Moore, and Rif A. Saurous. Unsuper-
application to face verification. In 2005 IEEE computer vised learning of semantic audio representations. CoRR,
societyconferenceoncomputervisionandpatternrecog-
abs/1711.02209,2017.
nition(CVPR’05),volume1,pages539–546.IEEE,2005.
[Khoslaetal.,2020] Prannay Khosla, Piotr Teterwak, Chen
[Dauetal.,2018] HoangAnhDau,AnthonyBagnall,Kaveh Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Maschinot, Ce Liu, and Dilip Krishnan. Supervised con-
Gharghabi, Chotirat Ann Ratanamahatana, and Ea- trastivelearning. Advancesinneuralinformationprocess-
monn Keogh. The ucr time series archive. CoRR, ingsystems,33:18661–18673,2020.
abs/1810.07758,2018.
[KingmaandBa,2014] Diederik P Kingma and Jimmy Ba.
[Dempsteretal.,2020] AngusDempster,FrancoisPetitjean, Adam: A method for stochastic optimization. arXiv
and Geoffrey I Webb. Rocket: exceptionally fast and preprintarXiv:1412.6980,2014.
accurate time series classification using random convolu-
tional kernels. Data Mining and Knowledge Discovery, [LaiandWang,2020] Peng-Ren Lai and Jia-Shung Wang.
34(5):1454–1495,2020. Multi-stage attention convolutional neural networks for
hevc in-loop filtering. In 2020 2nd IEEE International
[Eldeleetal.,2021a] EmadeldeenEldele,MohamedRagab, ConferenceonArtificialIntelligenceCircuitsandSystems
Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli (AICAS),pages173–177,2020.
Li, and Cuntai Guan. Time-series representation learn-
ing via temporal and contextual contrasting. CoRR, [LinesandBagnall,2015] J.LinesandA.Bagnall. Timese-
riesclassificationwithensemblesofelasticdistancemea-
abs/2106.14112,2021.
sures.DataMiningandKnowledgeDiscovery,29(3):565–
[Eldeleetal.,2021b] EmadeldeenEldele,MohamedRagab, 592,2015.
Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li,
[Linesetal.,2018] J. Lines, S. Taylor, and A. Bagnall.
andCuntaiGuan. Time-seriesrepresentationlearningvia
Time series classification with hive-cote: The hierarchi-
temporalandcontextualcontrasting,2021.
cal vote collective of transformation-based ensembles.
[Franceschietal.,2019] Jean-Yves Franceschi, Aymeric ACM Transactions on Knowledge Discovery from Data,
Dieuleveut,andMartinJaggi. Unsupervisedscalablerep- 12(5):52:1–52:35,2018.
resentationlearningformultivariatetimeseries. Advances
[Luoetal.,2023a] Dongsheng Luo, Wei Cheng, Yingheng
inneuralinformationprocessingsystems,32,2019.
Wang,DongkuanXu,JingchaoNi,WenchaoYu,Xuchao
[Guneletal.,2021] Beliz Gunel, Jingfei Du, Alexis Con- Zhang, Yanchi Liu, Yuncong Chen, Haifeng Chen, and
neau,andVeselinStoyanov. Supervisedcontrastivelearn- Xiang Zhang. Time series contrastive learning with
ing for pre-trained language model fine-tuning. In Inter- information-awareaugmentations,2023.
nationalConferenceonLearningRepresentations,2021.
[Luoetal.,2023b] Dongsheng Luo, Wei Cheng, Yingheng
[HaoandCao,2020] YifanHaoandHuipingCao.Anewat- Wang,DongkuanXu,JingchaoNi,WenchaoYu,Xuchao
tentionmechanismtoclassifymultivariatetimeseries. In Zhang, Yanchi Liu, Yuncong Chen, Haifeng Chen, andXiang Zhang. Time series contrastive learning with [Yaoetal.,2019] ShuochaoYao,AilingPiao,WenjunJiang,
information-awareaugmentations,2023. YiranZhao,HuajieShao,ShengzhongLiu,DongxinLiu,
Jinyang Li, Tianshi Wang, Shaohan Hu, Lu Su, Jiawei
[Muller,2007] MeinardMuller. Dynamictimewarping. In- Han, andTarekF.Abdelzaher. Stfnets: Learningsensing
formation retrieval for music and motion, pages 69–84, signals from the time-frequency perspective with short-
2007. time fourier neural networks. CoRR, abs/1902.07849,
2019.
[Parkkaetal.,2006] J.Parkka,M.Ermes,KorpipaaP.,Man-
tyjarviJ.,andPeltolaJ. Activityclassificationusingreal- [Yueetal.,2022a] Zhihan Yue, Yujing Wang, Juanyong
istic data from wearable sensors. IEEE Transactions on Duan,TianmengYang,CongruiHuang,YunhaiTong,and
Information Technology in Biomedicine, 6883:119–128, BixiongXu. Ts2vec: Towardsuniversalrepresentationof
2006. timeseries.InProceedingsoftheAAAIConferenceonAr-
tificialIntelligence,volume36,pages8980–8987,2022.
[Pedregosaetal.,2011] FabianPedregosa,Gae¨lVaroquaux,
Alexandre Gramfort, Vincent Michel, Bertrand Thirion, [Yueetal.,2022b] Zhihan Yue, Yujing Wang, Juanyong
Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Duan,TianmengYang,CongruiHuang,YunhaiTong,and
Weiss, Vincent Dubourg, et al. Scikit-learn: Machine BixiongXu. Ts2vec: Towardsuniversalrepresentationof
learning in python. the Journal of machine Learning re- timeseries.InProceedingsoftheAAAIConferenceonAr-
search,12:2825–2830,2011. tificialIntelligence,volume36,pages8980–8987,2022.
[Prickettetal.,2011] P. Prickett, G. Davies, and Grosvenor [Zerveasetal.,2020] George Zerveas, Srideepika Jayara-
man, Dhaval Patel, Anuradha Bhamidipaty, and Carsten
R.Ascadabasedpowerplantmonitoringandmanagement
Eickhoff. A transformer-based framework for mul-
system.Knowledge-BasedandIntelligentInformationand
tivariate time series representation learning. CoRR,
EngineeringSystems,6883:433–442,2011.
abs/2010.02803,2020.
[Shifazetal.,2020] Ahmed Shifaz, Charlotte Pelletier,
[Zhangetal.,2022] Xiang Zhang, Ziyuan Zhao, Theodoros
Francois Petitjean, and Geoffrey I Webb. Ts-chief: a
Tsiligkaridis, and Marinka Zitnik. Self-supervised con-
scalable and accurate forest algorithm for time series
trastive pre-training for time series via time-frequency
classification. Data Mining and Knowledge Discovery,
consistency,2022.
34(3):742–775,2020.
[Zhouetal.,2021] Haoyi Zhou, Shanghang Zhang, Jieqi
[Sohn,2016] Kihyuk Sohn. Improved deep metric learning Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wan-
withmulti-classn-pairlossobjective. Advancesinneural cai Zhang. Informer: Beyond efficient transformer for
informationprocessingsystems,29,2016. long sequence time-series forecasting. In Proceedings of
theAAAIconferenceonartificialintelligence,volume35,
[Songetal.,2018] Dongjin Song, Ning Xia, Wei Cheng,
pages11106–11115,2021.
Haifeng Chen, and Dacheng Tao. Deep r-th root of
rank supervised joint binary embedding for multivariate
time series retrieval. In Proceedings of the 24th ACM
SIGKDDInternationalConferenceonKnowledgeDiscov-
ery&DataMining,pages2229–2238,2018.
[Tonekabonietal.,2021a] Sana Tonekaboni, Danny Eytan,
andAnnaGoldenberg.Unsupervisedrepresentationlearn-
ing for time series with temporal neighborhood coding.
CoRR,abs/2106.00750,2021.
[Tonekabonietal.,2021b] Sana Tonekaboni, Danny Eytan,
andAnnaGoldenberg.Unsupervisedrepresentationlearn-
ing for time series with temporal neighborhood coding,
2021.
[Wooetal.,2022] Gerald Woo, Chenghao Liu, Doyen Sa-
hoo, Akshat Kumar, and Steven C. H. Hoi. Cost: Con-
trastive learning of disentangled seasonal-trend represen-
tationsfortimeseriesforecasting.CoRR,abs/2202.01575,
2022.
[Yangetal.,2022] Xinyu Yang, Zhenguo Zhang, and
RongyiCui. Timeclr: Aself-supervisedcontrastivelearn-
ing framework for univariate time series representation.
Know.-BasedSyst.,245(C),jun2022.A Algorithms Accuracy
ThetrainingalgorithmofRankSCLisdescribedinAlgorithm 7 6 5 4 3 2 1
1. First,theparametersintheencoderf andprojectionhead
θ
g are randomly initialized (line 1). For a batch of training
θ
samplesX B ⊆X,wecangettherepresentationsr i =f θ(x i) TS-T TN CC C RankSCL
and then get the embeddings z = g (r ) (lines 4-5). Two TS2Vec
i θ i DTW
jitteringoperationswithdistinctscalesα 1 =0.03,α 2 =0.05 InfoTS T-Loss
are applied to the normalized embeddings. All augmented
samplesareassignedthesamelabelsastheoriginaldataand Figure7:CDdiagramofUnivariateTimeseriesclassificationtask.
thenwecombinethemtogetlargersamples(lines6-11). Af-
ter doing the normalization, all of the samples will be em- Accuracy
bedded in the unit hypersphere (line 12). We leverage la-
7 6 5 4 3 2 1
bel information to help us categorize different classes (line
13). The Euclidean distances are calculated for each pair
of samples in the batch and the rank value of each anchor-
TNC
RankSCL
positive pair is also calculated (lines 14-15). Parameters θ TS-TCC
TS2Vec
in the encoder and projection head are updated by minimiz- T-Loss
InfoTS
ingtherank-supervisedcontrastivelearningloss(line17-18). DTW
Wewillreleasethecodeupontheacceptanceofthiswork.
Figure8:CDdiagramofMultivariateTimeseriesclassificationtask.
Algorithm1:RankSCL’smainlearningalgorithm
Input: timeseriesdatasetX,thelabelsetY,number
ducted on a Linux machine with 4 NVIDIA GeForce RTX
ofaugmentationk
3090 Ti GPUs, each with 25GB memory. CUDA version is
Output: encodernetworkf ,projectionheadg
θ θ 11.7andDriverVersionis515.65.01. OurmethodRankSCL
Initializetheencoderf ,projectionheadg .;
1 θ θ is implemented with Python 3.7.6 an Pytorch 1.12.0. We
foreachepochdo
2 train and evaluate our methods with the following hyper-
3
foreachtrainingepochX
B
⊆Xdo
parameters and configurations: Optimizer: Adam opti-
4 r i ←f θ(x i); mizer [Kingma and Ba, 2014] with learning rate and de-
z ←g (r );
5 i θ i cay rates setting to 0.0001 and 0.0005. SVM: scikit-learn
6 whilej <kdo implementation [Pedregosa et al., 2011] with penalty C ∈
7 z i′ =t α1(z i); (cid:8) 10i|i∈{−4,−3,...,4}∪∞(cid:9) [Franceschi et al., 2019].
z′′ =t (z );
8 i α2 i Encoder architecture: We choose a 3-layer Fully Convo-
j ←j+1;
9 lutionalNetworktodesigntheencoder. Specifically,theout-
10 end putdimensionofthelinearprojectionlayerissetto320,the
11 Z B ←average(Z B,Z B′ ,Z B′′); sameforthenumberofchannelsinthefollowingprojection
12 NormalizeZ B; head. Classifier architecture: a fully connected layer that
13 CategorizeZ B bylabelsetY; mapstherepresentationstothelabelisadopted. Numberof
14 ComputeEuclideandistancematrix; augmentations: kissearchedin{0,5,10,15}.
ComputerankRforeachanchor-positivepair
15
withEq.(3); C FullResults
Computerank-supervisedcontrastivelearning
16
lossLwithEq.(4); Tables 5 and 6 show the full results of univariate time se-
Updateparametersθintheencoderf ; riesclassification ofourmethod, compared withotherbase-
17 θ
Updateparametersθintheprojectionheadg ; line models, including InfoTS [Luo et al., 2023b], TS2Vec
18 θ
[Yue et al., 2022b],T-Loss [Franceschi et al., 2019], TNC
end
19
[Tonekaboni et al., 2021b], TS-TCC [Eldele et al., 2021b].
end
20
T-Losscannothandledatasetswithmissingvalues,including
DodgerLoopDay, DodgerLoopGame and DodgerLoopWeek-
B ExperimentalSettings end.TheresultsofDTWandTNConInsectWingbeatdataset
inUEAarenotreported. Whencomputingaverageaccuracy
DataPreprocessing: Forunivariatetimeseriesdata,wenor-
scorestheseunavailableaccuracyscoresareconsideredas0.
malizethedatasetusingz-scorefollowing[Franceschietal.,
Figures 7 and 8 show the average rank results of all of the
2019;Yueetal.,2022b;Zhouetal.,2021]tomakesurethat
methods on 128 UCR and 30 UEA. Tables 7 and 8 present
eachofthedatasetshaszeromeanandunitvariance.Formul-
the full classification results of 30 UEA datasets. All meth-
tivariate time series data, we normalize each variable inde-
ods were experimented on the same five seeds, and the final
pendentlybyusingz-score. Someofthedatasetshavemiss-
resultswerereportedbytakingtheaverageofthefiveseeds.
ingvalues(NaN),thecorrespondingpositionofmissingvalue
TheresultsofDTWareupdatedandthedetailsareshownin
wouldbesettozero.
Tables5and6.
HardwareandImplementations: Allexperimentsarecon-Dataset RankSCL InfoTS TS2Vec T-Loss DTW TS-TCC TNC
ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec.
Adiac 0.822 0.821 0.753 0.768 0.765 0.772 0.645 0.642 0.573 0.624 0.241 0.148 0.068 0.029
ACSF1 0.896 0.906 0.772 0.797 0.870 0.883 0.896 0.896 0.590 0.630 0.438 0.367 0.166 0.065
ArrowHead 0.848 0.853 0.824 0.832 0.834 0.833 0.804 0.813 0.709 0.705 0.694 0.554 0.339 0.113
Beef 0.747 0.785 0.738 0.773 0.720 0.781 0.660 0.660 0.567 0.639 0.654 0.471 0.240 0.172
BeetleFly 0.940 0.947 0.826 0.871 0.890 0.916 0.880 0.560 0.700 0.738 0.561 0.673 0.580 0.540
BirdChicken 0.900 0.917 0.724 0.732 0.870 0.887 0.810 0.850 0.600 0.600 0.717 0.682 0.540 0.402
Car 0.847 0.853 0.770 0.765 0.730 0.744 0.713 0.718 0.517 0.546 0.728 0.667 0.217 0.062
CBF 0.997 0.997 0.991 0.992 0.996 0.996 0.987 0.987 0.997 0.997 0.822 0.830 0.384 0.352
ChlorineConcentration 0.817 0.799 0.779 0.750 0.823 0.804 0.724 0.682 0.548 0.484 0.565 0.306 0.549 0.447
CinCECGTorso 0.720 0.741 0.803 0.816 0.798 0.814 0.682 0.682 0.498 0.534 0.858 0.834 0.272 0.160
Coffee 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.929 0.941 0.881 0.825 0.507 0.254
Computers 0.763 0.764 0.655 0.669 0.652 0.653 0.669 0.577 0.684 0.687 0.562 0.513 0.552 0.506
CricketX 0.792 0.799 0.699 0.715 0.789 0.809 0.774 0.778 0.744 0.767 0.662 0.573 0.138 0.081
CricketY 0.789 0.793 0.693 0.711 0.762 0.775 0.733 0.733 0.685 0.718 0.637 0.534 0.175 0.117
CricketZ 0.823 0.823 0.725 0.731 0.800 0.800 0.760 0.755 0.754 0.763 0.680 0.585 0.112 0.094
DiatomSizeReduction 0.924 0.946 0.982 0.979 0.950 0.970 0.976 0.948 0.709 0.616 0.665 0.546 0.311 0.101
DistalPhalanxOutlineCorrect 0.766 0.760 0.759 0.762 0.764 0.767 0.766 0.781 0.732 0.754 0.649 0.552 0.595 0.414
DistalPhalanxOutlineAgeGroup 0.741 0.730 0.737 0.777 0.716 0.745 0.727 0.696 0.748 0.754 0.707 0.535 0.643 0.436
DistalPhalanxTW 0.692 0.543 0.670 0.497 0.682 0.507 0.668 0.515 0.662 0.498 0.689 0.490 0.579 0.205
Earthquakes 0.732 0.601 0.749 0.475 0.748 0.374 0.748 0.500 0.748 0.628 0.736 0.615 0.754 0.690
ECG200 0.920 0.917 0.864 0.857 0.908 0.891 0.862 0.873 0.790 0.781 0.841 0.805 0.684 0.633
ECG5000 0.942 0.680 0.939 0.733 0.938 0.699 0.935 0.514 0.938 0.655 0.941 0.874 0.873 0.391
ECGFiveDays 0.997 0.997 0.987 0.987 1.000 1.000 0.999 0.975 0.598 0.683 0.834 0.844 0.547 0.427
ElectricDevices 0.691 0.652 0.677 0.648 0.710 0.678 0.714 0.620 0.610 0.578 0.672 0.554 0.443 0.342
FaceAll 0.775 0.812 0.753 0.792 0.783 0.814 0.758 0.854 0.808 0.811 0.765 0.703 0.239 0.257
FaceFour 0.832 0.833 0.764 0.777 0.886 0.906 0.805 0.827 0.682 0.746 0.932 0.913 0.359 0.260
FacesUCR 0.914 0.919 0.848 0.851 0.932 0.934 0.875 0.826 0.886 0.880 0.893 0.837 0.295 0.158
FiftyWords 0.727 0.653 0.758 0.678 0.775 0.734 0.732 0.578 0.664 0.561 0.710 0.564 0.148 0.016
Fish 0.941 0.954 0.829 0.845 0.930 0.934 0.844 0.847 0.794 0.793 0.746 0.693 0.155 0.042
FordA 0.956 0.956 0.878 0.879 0.940 0.940 0.931 0.910 0.594 0.595 0.912 0.906 0.524 0.522
FordB 0.828 0.829 0.730 0.730 0.802 0.806 0.794 0.773 0.654 0.661 0.763 0.757 0.506 0.505
GunPoint 0.995 0.995 0.975 0.975 0.983 0.983 0.976 0.943 0.827 0.829 0.879 0.884 0.513 0.506
Ham 0.714 0.715 0.695 0.697 0.711 0.711 0.659 0.615 0.590 0.590 0.754 0.726 0.520 0.443
HandOutlines 0.913 0.906 0.924 0.922 0.928 0.931 0.909 0.896 0.889 0.900 0.663 0.671 0.641 0.370
Haptics 0.539 0.544 0.499 0.508 0.526 0.534 0.451 0.446 0.406 0.437 0.367 0.297 0.213 0.176
Herring 0.613 0.614 0.621 0.564 0.600 0.514 0.556 0.516 0.547 0.544 0.541 0.532 0.594 0.297
InlineSkate 0.371 0.384 0.350 0.376 0.389 0.426 0.357 0.357 0.356 0.388 0.286 0.227 0.169 0.049
InsectWingbeatSound 0.615 0.614 0.630 0.634 0.626 0.628 0.564 0.565 0.362 0.394 0.662 0.565 0.186 0.097
ItalyPowerDemand 0.960 0.960 0.957 0.957 0.958 0.959 0.946 0.901 0.946 0.946 0.940 0.935 0.746 0.783
LargeKitchenAppliances 0.912 0.915 0.771 0.773 0.865 0.871 0.820 0.820 0.800 0.805 0.585 0.445 0.386 0.424
Lightning2 0.830 0.833 0.743 0.781 0.852 0.856 0.852 0.799 0.820 0.855 0.709 0.712 0.577 0.454
Lightning7 0.786 0.820 0.762 0.811 0.844 0.880 0.784 0.760 0.726 0.852 0.768 0.689 0.260 0.159
Mallat 0.962 0.966 0.879 0.897 0.906 0.917 0.959 0.959 0.928 0.940 0.820 0.745 0.156 0.074
Meat 0.967 0.970 0.920 0.930 0.903 0.923 0.910 0.910 0.933 0.944 0.313 0.281 0.333 0.111
MedicalImages 0.793 0.786 0.765 0.754 0.788 0.808 0.762 0.714 0.709 0.694 0.690 0.501 0.513 0.118
MiddlePhalanxOutlineCorrect 0.846 0.846 0.801 0.798 0.789 0.786 0.777 0.756 0.732 0.730 0.619 0.551 0.570 0.285
MiddlePhalanxOutlineAgeGroup 0.445 0.447 0.651 0.874 0.641 0.813 0.644 0.472 0.558 0.474 0.636 0.413 0.554 0.330
MiddlePhalanxTW 0.579 0.458 0.589 0.441 0.586 0.425 0.574 0.417 0.552 0.350 0.576 0.308 0.500 0.178
MoteStrain 0.889 0.888 0.861 0.868 0.871 0.870 0.857 0.742 0.814 0.816 0.844 0.847 0.737 0.756
NonInvasiveFetalECGThorax1 0.951 0.949 0.934 0.932 0.925 0.923 0.890 0.889 0.786 0.808 0.863 0.784 0.167 0.108
NonInvasiveFetalECGThorax2 0.893 0.891 0.937 0.935 0.937 0.936 0.925 0.923 0.863 0.865 0.848 0.754 0.138 0.080
OliveOil 0.840 0.851 0.868 0.909 0.840 0.896 0.833 0.777 0.767 0.694 0.407 0.303 0.400 0.100
OSULeaf 0.926 0.927 0.659 0.658 0.831 0.843 0.746 0.734 0.545 0.498 0.498 0.433 0.234 0.174
PhalangesOutlinesCorrect 0.829 0.826 0.795 0.813 0.809 0.810 0.799 0.786 0.755 0.784 0.656 0.500 0.615 0.508
Phoneme 0.277 0.169 0.198 0.089 0.307 0.157 0.267 0.115 0.294 0.144 0.182 0.135 0.111 0.017
Plane 0.996 0.996 0.996 0.997 0.986 0.988 0.988 0.988 1.000 1.000 0.964 0.939 0.263 0.189
ProximalPhalanxOutlineCorrect 0.893 0.888 0.874 0.890 0.882 0.883 0.867 0.838 0.832 0.849 0.711 0.583 0.692 0.537
ProximalPhalanxOutlineAgeGroup 0.843 0.742 0.859 0.769 0.845 0.735 0.848 0.743 0.824 0.709 0.645 0.561 0.855 0.637
ProximalPhalanxTW 0.776 0.547 0.819 0.603 0.793 0.570 0.801 0.550 0.761 0.499 0.643 0.394 0.674 0.235
RefrigerationDevices 0.554 0.575 0.566 0.596 0.596 0.627 0.524 0.525 0.475 0.489 0.452 0.382 0.346 0.332
ScreenType 0.583 0.589 0.447 0.447 0.411 0.389 0.421 0.424 0.427 0.440 0.381 0.374 0.337 0.320
ContinuedonnextpageTable5continuedfrompreviouspage
Dataset RankSCL InfoTS TS2Vec T-Loss DTW TS-TCC TNC
ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec.
ShapeletSim 0.949 0.949 0.556 0.557 0.990 0.990 0.759 0.532 0.639 0.640 0.524 0.481 0.509 0.508
ShapesAll 0.908 0.921 0.789 0.799 0.896 0.909 0.851 0.852 0.708 0.687 0.718 0.563 0.072 0.018
SmallKitchenAppliances 0.813 0.822 0.738 0.735 0.727 0.724 0.686 0.686 0.688 0.684 0.610 0.441 0.513 0.435
SonyAIBORobotSurface1 0.831 0.858 0.864 0.877 0.885 0.892 0.835 0.973 0.617 0.764 0.557 0.575 0.448 0.364
SonyAIBORobotSurface2 0.935 0.929 0.860 0.852 0.870 0.862 0.920 0.813 0.803 0.801 0.813 0.800 0.681 0.663
StarLightCurves 0.974 0.962 0.962 0.943 0.968 0.952 0.961 0.947 0.914 0.871 0.939 0.901 0.818 0.614
Strawberry 0.972 0.966 0.960 0.952 0.963 0.955 0.945 0.931 0.919 0.906 0.897 0.820 0.644 0.336
SwedishLeaf 0.955 0.956 0.911 0.913 0.939 0.940 0.926 0.927 0.779 0.787 0.884 0.813 0.231 0.195
Symbols 0.859 0.875 0.955 0.956 0.971 0.972 0.958 0.958 0.929 0.943 0.834 0.775 0.262 0.189
SyntheticControl 0.995 0.995 0.995 0.995 0.995 0.995 0.984 0.984 0.983 0.984 0.990 0.969 0.695 0.707
ToeSegmentation1 0.961 0.962 0.808 0.818 0.914 0.918 0.943 0.790 0.741 0.787 0.576 0.514 0.485 0.370
ToeSegmentation2 0.780 0.714 0.816 0.718 0.868 0.790 0.852 0.762 0.815 0.701 0.574 0.551 0.527 0.422
Trace 1.000 1.000 1.000 1.000 0.998 0.998 1.000 1.000 0.963 1.000 0.702 0.616 0.350 0.248
TwoLeadECG 0.996 0.996 0.924 0.932 0.979 0.980 0.991 0.922 0.852 0.883 0.724 0.730 0.537 0.577
TwoPatterns 1.000 1.000 0.999 0.999 1.000 1.000 1.000 1.000 1.000 1.000 0.999 0.999 0.402 0.372
UWaveGestureLibraryX 0.804 0.801 0.804 0.796 0.804 0.799 0.802 0.801 0.736 0.708 0.767 0.703 0.289 0.179
UWaveGestureLibraryY 0.724 0.724 0.722 0.720 0.723 0.722 0.724 0.727 0.632 0.626 0.640 0.551 0.405 0.322
UWaveGestureLibraryZ 0.756 0.749 0.741 0.732 0.759 0.752 0.748 0.751 0.671 0.659 0.683 0.598 0.336 0.228
UWaveGestureLibraryAll 0.836 0.838 0.954 0.954 0.934 0.934 0.908 0.909 0.179 0.181 0.939 0.914 0.316 0.269
Wafer 0.996 0.990 0.995 0.991 0.996 0.991 0.992 0.987 0.975 0.975 0.994 0.985 0.895 0.716
Wine 0.789 0.790 0.813 0.820 0.763 0.713 0.830 0.815 0.574 0.575 0.496 0.473 0.500 0.300
WordSynonyms 0.663 0.616 0.654 0.581 0.690 0.659 0.674 0.500 0.596 0.575 0.608 0.430 0.237 0.028
Worms 0.821 0.834 0.646 0.702 0.719 0.751 0.709 0.612 0.403 0.381 0.537 0.417 0.424 0.131
WormsTwoClass 0.753 0.782 0.693 0.692 0.784 0.781 0.808 0.726 0.532 0.537 0.583 0.532 0.561 0.434
Yoga 0.895 0.895 0.829 0.829 0.881 0.882 0.871 0.836 0.785 0.785 0.733 0.732 0.516 0.554
AllGestureWiimoteX 0.752 0.761 0.109 0.029 0.785 0.788 0.784 0.785 0.679 0.687 0.514 0.325 0.112 0.017
AllGestureWiimoteY 0.774 0.774 0.148 0.085 0.777 0.779 0.789 0.789 0.676 0.694 0.546 0.335 0.108 0.022
AllGestureWiimoteZ 0.774 0.782 0.132 0.058 0.759 0.761 0.754 0.756 0.649 0.662 0.548 0.344 0.125 0.032
BME 0.991 0.991 0.998 0.998 0.980 0.981 0.971 0.971 0.847 0.844 0.880 0.779 0.467 0.449
Chinatown 0.937 0.906 0.975 0.960 0.972 0.955 0.943 0.909 0.971 0.952 0.855 0.743 0.717 0.575
Crop 0.775 0.777 0.754 0.754 0.753 0.752 0.731 0.732 0.659 0.668 0.634 0.448 0.370 0.350
EOGHorizontalSignal 0.677 0.710 0.512 0.559 0.537 0.589 0.532 0.537 0.434 0.486 0.428 0.348 0.188 0.105
EOGVerticalSignal 0.461 0.496 0.397 0.437 0.484 0.508 0.453 0.457 0.436 0.448 0.379 0.286 0.173 0.080
EthanolLevel 0.808 0.815 0.582 0.583 0.474 0.470 0.315 0.316 0.280 0.273 0.322 0.265 0.253 0.125
FreezerRegularTrain 0.994 0.994 0.987 0.988 0.983 0.984 0.979 0.961 0.839 0.839 0.894 0.715 0.705 0.719
FreezerSmallTrain 0.781 0.783 0.901 0.904 0.870 0.870 0.883 0.864 0.760 0.761 0.692 0.543 0.593 0.624
Fungi 0.855 0.897 0.887 0.936 0.938 0.952 0.990 0.995 0.177 0.032 0.444 0.375 0.084 0.009
GestureMidAirD1 0.726 0.752 0.091 0.029 0.623 0.678 0.562 0.565 0.438 0.466 0.624 0.490 0.052 0.004
GestureMidAirD2 0.605 0.645 0.065 0.032 0.534 0.556 0.482 0.485 0.362 0.368 0.512 0.373 0.054 0.004
GestureMidAirD3 0.266 0.270 0.075 0.033 0.312 0.315 0.277 0.271 0.169 0.099 0.330 0.231 0.051 0.020
GesturePebbleZ1 0.865 0.879 0.228 0.149 0.852 0.878 0.884 0.880 0.657 0.692 0.752 0.728 0.161 0.027
GesturePebbleZ2 0.851 0.860 0.209 0.098 0.857 0.860 0.843 0.837 0.582 0.653 0.727 0.700 0.155 0.037
GunPointAgeSpan 0.996 0.996 0.967 0.969 0.980 0.980 0.983 0.974 0.994 0.994 0.804 0.755 0.558 0.536
GunPointMaleVersusFemale 0.999 0.999 0.999 0.999 1.000 1.000 1.000 0.997 0.968 0.968 0.974 0.919 0.728 0.782
GunPointOldVersusYoung 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.999 0.998 0.840 0.878
HouseTwenty 0.943 0.941 0.846 0.851 0.902 0.910 0.933 0.843 0.882 0.883 0.804 0.620 0.578 0.654
InsectEPGRegularTrain 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.831 0.579
InsectEPGSmallTrain 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.916 0.911 0.714 0.548
MelbournePedestrian 0.958 0.958 0.928 0.933 0.955 0.955 0.938 0.938 0.831 0.849 0.807 0.672 0.253 0.200
MixedShapesRegularTrain 0.951 0.947 0.916 0.915 0.919 0.919 0.922 0.923 0.823 0.819 0.851 0.670 0.254 0.278
MixedShapesSmallTrain 0.878 0.872 0.831 0.839 0.852 0.848 0.872 0.877 0.739 0.748 0.789 0.602 0.240 0.239
PickupGestureWiimoteZ 0.800 0.836 0.142 0.072 0.840 0.868 0.804 0.808 0.680 0.687 0.739 0.610 0.140 0.028
PigAirwayPressure 0.533 0.577 0.205 0.258 0.626 0.656 0.465 0.464 0.053 0.041 0.088 0.073 0.031 0.002
PigArtPressure 0.978 0.983 0.444 0.463 0.964 0.972 0.924 0.922 0.322 0.278 0.164 0.143 0.040 0.008
PigCVP 0.797 0.834 0.192 0.196 0.811 0.821 0.740 0.738 0.202 0.198 0.119 0.102 0.040 0.005
PLAID 0.495 0.584 0.087 0.084 0.544 0.590 0.636 0.556 0.892 0.909 0.324 0.276 0.205 0.048
PowerCons 0.951 0.952 0.984 0.984 0.964 0.965 0.931 0.871 0.856 0.857 0.997 0.996 0.673 0.691
Rock 0.428 0.478 0.590 0.513 0.712 0.691 0.576 0.639 0.440 0.379 0.514 0.393 0.288 0.151
SemgHandGenderCh2 0.665 0.749 0.870 0.856 0.956 0.949 0.908 0.861 0.912 0.907 0.919 0.805 0.588 0.640
SemgHandMovementCh2 0.720 0.723 0.667 0.674 0.870 0.873 0.783 0.784 0.764 0.780 0.670 0.443 0.290 0.278
ContinuedonnextpageTable5continuedfrompreviouspage
Dataset RankSCL InfoTS TS2Vec T-Loss DTW TS-TCC TNC
ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec.
SemgHandSubjectCh2 0.876 0.883 0.852 0.854 0.948 0.949 0.859 0.861 0.849 0.855 0.883 0.715 0.439 0.413
ShakeGestureWiimoteZ 0.880 0.901 0.182 0.077 0.924 0.939 0.928 0.929 0.820 0.779 0.807 0.779 0.104 0.015
SmoothSubspace 0.975 0.975 0.824 0.805 0.976 0.976 0.943 0.943 0.860 0.862 0.926 0.768 0.788 0.788
UMD 0.990 0.990 0.990 0.991 0.994 0.994 0.992 0.992 0.854 0.854 0.932 0.878 0.407 0.236
DodgerLoopDay 0.537 0.581 0.556 0.587 0.495 0.540 – – 0.362 0.453 0.555 0.376 0.200 0.133
DodgerLoopGame 0.803 0.803 0.777 0.784 0.813 0.818 – – 0.862 0.896 0.750 0.750 0.571 0.424
DodgerLoopWeekend 0.896 0.858 0.972 0.960 0.952 0.936 – – 0.949 0.931 0.910 0.765 0.717 0.441
AVG 0.821 0.817 0.733 0.723 0.822 0.816 0.782 0.750 0.699 0.692 0.685 0.603 0.406 0.305
Table5:AccuracyscoresandPrecisionscoresofourmethodcomparedwiththoseofothermethodson128UCRdatasets.Therepresentation
dimensionsofTS2Vec,T-Loss,TNC,TS-TCCandTSTareallsetto320forfaircomparison.Dataset RankSCL InfoTS TS2Vec T-Loss DTW TS-TCC TNC
F1 Recall F1 Recall F1 Recall F1 Recall F1 Recall F1 Recall F1 Recall
Adiac 0.814 0.817 0.740 0.754 0.752 0.762 0.622 0.632 0.545 0.582 0.154 0.151 0.029 0.029
ACSF1 0.896 0.901 0.772 0.784 0.870 0.877 0.896 0.896 0.585 0.607 0.276 0.315 0.073 0.069
ArrowHead 0.845 0.849 0.823 0.828 0.835 0.834 0.805 0.809 0.705 0.705 0.485 0.517 0.169 0.135
Beef 0.745 0.764 0.733 0.753 0.706 0.742 0.647 0.654 0.576 0.606 0.382 0.422 0.140 0.154
BeetleFly 0.940 0.943 0.818 0.844 0.886 0.901 0.451 0.499 0.688 0.712 0.541 0.599 0.486 0.512
BirdChicken 0.899 0.908 0.721 0.727 0.869 0.878 0.847 0.849 0.600 0.600 0.606 0.642 0.431 0.416
Car 0.849 0.851 0.765 0.765 0.733 0.738 0.718 0.718 0.505 0.525 0.637 0.652 0.096 0.075
CBF 0.997 0.997 0.991 0.992 0.996 0.996 0.987 0.987 0.997 0.997 0.788 0.808 0.281 0.312
ChlorineConcentration 0.787 0.793 0.748 0.749 0.797 0.800 0.689 0.686 0.430 0.455 0.326 0.316 0.277 0.342
CinCECGTorso 0.722 0.731 0.795 0.805 0.795 0.804 0.675 0.679 0.504 0.519 0.813 0.824 0.187 0.173
Coffee 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.927 0.934 0.785 0.804 0.336 0.289
Computers 0.763 0.763 0.648 0.659 0.651 0.652 0.500 0.535 0.683 0.685 0.370 0.430 0.511 0.509
CricketX 0.795 0.797 0.699 0.707 0.795 0.802 0.773 0.775 0.748 0.757 0.558 0.565 0.082 0.081
CricketY 0.790 0.791 0.696 0.703 0.764 0.769 0.732 0.732 0.688 0.703 0.519 0.526 0.117 0.117
CricketZ 0.817 0.820 0.720 0.725 0.794 0.797 0.756 0.756 0.745 0.754 0.567 0.576 0.071 0.081
DiatomSizeReduction 0.856 0.899 0.970 0.974 0.947 0.959 0.961 0.954 0.562 0.588 0.543 0.544 0.133 0.115
DistalPhalanxOutlineCorrect 0.758 0.759 0.742 0.752 0.749 0.757 0.775 0.778 0.698 0.725 0.515 0.533 0.411 0.413
DistalPhalanxOutlineAgeGroup 0.721 0.725 0.738 0.757 0.689 0.716 0.703 0.700 0.755 0.754 0.545 0.540 0.452 0.444
DistalPhalanxTW 0.521 0.532 0.487 0.492 0.501 0.504 0.502 0.508 0.484 0.491 0.509 0.499 0.249 0.225
Earthquakes 0.535 0.566 0.434 0.453 0.428 0.399 0.428 0.461 0.478 0.543 0.578 0.596 0.483 0.568
ECG200 0.912 0.914 0.850 0.853 0.827 0.858 0.851 0.862 0.761 0.771 0.801 0.803 0.597 0.614
ECG5000 0.597 0.636 0.599 0.659 0.567 0.626 0.558 0.535 0.554 0.600 0.870 0.872 0.362 0.376
ECGFiveDays 0.997 0.997 0.987 0.987 1.000 1.000 0.975 0.975 0.548 0.608 0.813 0.828 0.433 0.430
ElectricDevices 0.620 0.636 0.604 0.625 0.628 0.652 0.624 0.622 0.542 0.559 0.485 0.517 0.308 0.324
FaceAll 0.801 0.806 0.776 0.784 0.799 0.806 0.772 0.811 0.810 0.810 0.684 0.693 0.201 0.225
FaceFour 0.824 0.828 0.766 0.772 0.894 0.900 0.809 0.818 0.640 0.689 0.895 0.904 0.247 0.253
FacesUCR 0.893 0.906 0.824 0.837 0.915 0.924 0.845 0.835 0.853 0.866 0.832 0.834 0.130 0.143
FiftyWords 0.598 0.624 0.613 0.644 0.646 0.687 0.578 0.578 0.497 0.527 0.564 0.564 0.016 0.016
Fish 0.942 0.948 0.831 0.838 0.931 0.932 0.846 0.846 0.783 0.788 0.668 0.680 0.053 0.047
FordA 0.956 0.956 0.878 0.879 0.940 0.940 0.911 0.910 0.587 0.591 0.894 0.900 0.520 0.521
FordB 0.828 0.828 0.730 0.730 0.802 0.804 0.768 0.771 0.652 0.656 0.735 0.746 0.498 0.502
GunPoint 0.995 0.995 0.975 0.975 0.983 0.983 0.944 0.943 0.827 0.828 0.866 0.875 0.387 0.439
Ham 0.714 0.714 0.695 0.696 0.710 0.710 0.568 0.590 0.588 0.589 0.635 0.677 0.430 0.437
HandOutlines 0.907 0.906 0.917 0.919 0.920 0.925 0.893 0.895 0.874 0.887 0.625 0.647 0.393 0.381
Haptics 0.532 0.538 0.484 0.496 0.517 0.525 0.429 0.437 0.397 0.416 0.280 0.288 0.151 0.162
Herring 0.590 0.602 0.493 0.526 0.511 0.513 0.435 0.472 0.541 0.542 0.501 0.516 0.373 0.331
InlineSkate 0.372 0.378 0.362 0.369 0.389 0.407 0.356 0.356 0.370 0.379 0.217 0.221 0.074 0.059
InsectWingbeatSound 0.613 0.613 0.628 0.631 0.625 0.627 0.561 0.563 0.358 0.375 0.549 0.557 0.114 0.105
ItalyPowerDemand 0.960 0.960 0.957 0.957 0.958 0.959 0.900 0.901 0.946 0.946 0.926 0.930 0.733 0.757
LargeKitchenAppliances 0.912 0.913 0.771 0.772 0.864 0.868 0.819 0.820 0.800 0.802 0.336 0.383 0.335 0.374
Lightning2 0.827 0.830 0.722 0.750 0.850 0.853 0.785 0.792 0.810 0.832 0.673 0.692 0.453 0.454
Lightning7 0.776 0.797 0.752 0.780 0.831 0.855 0.771 0.765 0.725 0.783 0.682 0.685 0.159 0.159
Mallat 0.962 0.964 0.871 0.884 0.903 0.910 0.958 0.958 0.928 0.934 0.745 0.745 0.065 0.069
Meat 0.967 0.968 0.918 0.924 0.902 0.912 0.907 0.908 0.935 0.939 0.292 0.286 0.167 0.133
MedicalImages 0.763 0.774 0.744 0.749 0.763 0.785 0.733 0.723 0.691 0.692 0.504 0.503 0.114 0.116
MiddlePhalanxOutlineCorrect 0.841 0.843 0.796 0.797 0.784 0.785 0.743 0.750 0.720 0.725 0.511 0.530 0.363 0.319
MiddlePhalanxOutlineAgeGroup 0.425 0.436 0.467 0.608 0.447 0.577 0.483 0.478 0.420 0.445 0.418 0.415 0.372 0.350
MiddlePhalanxTW 0.426 0.441 0.389 0.414 0.393 0.408 0.406 0.412 0.357 0.353 0.341 0.323 0.220 0.196
MoteStrain 0.888 0.888 0.858 0.863 0.870 0.870 0.737 0.739 0.811 0.813 0.822 0.834 0.728 0.742
NonInvasiveFetalECGThorax1 0.948 0.948 0.931 0.932 0.921 0.922 0.887 0.888 0.782 0.795 0.782 0.783 0.118 0.113
NonInvasiveFetalECGThorax2 0.888 0.889 0.934 0.935 0.934 0.935 0.921 0.922 0.859 0.862 0.753 0.753 0.094 0.087
OliveOil 0.792 0.820 0.849 0.878 0.784 0.836 0.784 0.781 0.642 0.667 0.321 0.312 0.143 0.118
OSULeaf 0.909 0.918 0.647 0.652 0.822 0.833 0.729 0.731 0.490 0.494 0.408 0.420 0.155 0.164
PhalangesOutlinesCorrect 0.815 0.820 0.765 0.788 0.789 0.800 0.776 0.781 0.708 0.744 0.499 0.499 0.452 0.479
Phoneme 0.151 0.159 0.079 0.084 0.142 0.149 0.107 0.111 0.119 0.130 0.065 0.088 0.016 0.017
Plane 0.996 0.996 0.996 0.996 0.987 0.987 0.989 0.989 1.000 1.000 0.938 0.938 0.176 0.182
ProximalPhalanxOutlineCorrect 0.877 0.882 0.841 0.865 0.856 0.869 0.834 0.836 0.779 0.812 0.583 0.583 0.433 0.480
ProximalPhalanxOutlineAgeGroup 0.754 0.748 0.777 0.773 0.734 0.734 0.742 0.742 0.709 0.709 0.525 0.542 0.601 0.618
ProximalPhalanxTW 0.518 0.532 0.527 0.563 0.524 0.546 0.523 0.536 0.508 0.503 0.426 0.409 0.271 0.252
RefrigerationDevices 0.557 0.566 0.569 0.582 0.590 0.608 0.530 0.527 0.479 0.484 0.242 0.296 0.279 0.303
ScreenType 0.583 0.586 0.443 0.445 0.377 0.383 0.409 0.416 0.422 0.431 0.207 0.266 0.255 0.284
ContinuedonnextpageTable6continuedfrompreviouspage
Dataset RankSCL InfoTS TS2Vec T-Loss DTW TS-TCC TNC
F1 Recall F1 Recall F1 Recall F1 Recall ACC Prec. ACC Prec. ACC Prec.
ShapeletSim 0.949 0.949 0.554 0.556 0.990 0.990 0.402 0.458 0.638 0.639 0.395 0.434 0.442 0.473
ShapesAll 0.907 0.914 0.782 0.790 0.895 0.902 0.850 0.851 0.681 0.684 0.505 0.532 0.022 0.020
SmallKitchenAppliances 0.811 0.816 0.735 0.735 0.720 0.722 0.684 0.685 0.681 0.682 0.344 0.387 0.423 0.429
SonyAIBORobotSurface1 0.831 0.844 0.864 0.870 0.885 0.888 0.971 0.972 0.594 0.668 0.479 0.522 0.357 0.361
SonyAIBORobotSurface2 0.931 0.930 0.855 0.853 0.866 0.864 0.769 0.790 0.783 0.792 0.775 0.787 0.639 0.651
StarLightCurves 0.962 0.962 0.946 0.945 0.954 0.953 0.945 0.946 0.890 0.880 0.895 0.898 0.603 0.608
Strawberry 0.970 0.968 0.957 0.955 0.960 0.958 0.915 0.923 0.914 0.910 0.791 0.805 0.377 0.355
SwedishLeaf 0.956 0.956 0.911 0.912 0.939 0.939 0.925 0.926 0.774 0.780 0.806 0.809 0.161 0.176
Symbols 0.856 0.865 0.955 0.955 0.971 0.972 0.958 0.958 0.926 0.934 0.759 0.767 0.163 0.175
SyntheticControl 0.995 0.995 0.995 0.995 0.995 0.995 0.984 0.984 0.983 0.983 0.967 0.968 0.676 0.691
ToeSegmentation1 0.961 0.961 0.805 0.811 0.913 0.915 0.788 0.789 0.724 0.754 0.369 0.430 0.376 0.373
ToeSegmentation2 0.726 0.720 0.738 0.728 0.820 0.805 0.799 0.780 0.712 0.706 0.408 0.469 0.406 0.414
Trace 1.000 1.000 1.000 1.000 0.998 0.998 1.000 1.000 1.000 1.000 0.635 0.625 0.243 0.246
TwoLeadECG 0.996 0.996 0.923 0.927 0.979 0.979 0.922 0.922 0.849 0.866 0.695 0.712 0.431 0.494
TwoPatterns 1.000 1.000 0.999 0.999 1.000 1.000 1.000 1.000 1.000 1.000 0.998 0.998 0.362 0.367
UWaveGestureLibraryX 0.800 0.800 0.798 0.797 0.799 0.799 0.801 0.801 0.693 0.700 0.687 0.695 0.200 0.189
UWaveGestureLibraryY 0.723 0.723 0.717 0.718 0.720 0.721 0.722 0.724 0.620 0.623 0.531 0.541 0.321 0.322
UWaveGestureLibraryZ 0.751 0.750 0.734 0.733 0.754 0.753 0.744 0.747 0.645 0.652 0.582 0.590 0.243 0.235
UWaveGestureLibraryAll 0.836 0.837 0.953 0.954 0.933 0.933 0.908 0.908 0.179 0.180 0.908 0.911 0.240 0.253
Wafer 0.991 0.990 0.987 0.989 0.990 0.991 0.977 0.982 0.930 0.952 0.984 0.984 0.569 0.634
Wine 0.789 0.789 0.812 0.816 0.729 0.721 0.814 0.815 0.573 0.574 0.465 0.469 0.362 0.328
WordSynonyms 0.537 0.574 0.503 0.539 0.553 0.602 0.524 0.512 0.450 0.505 0.435 0.432 0.033 0.031
Worms 0.805 0.819 0.619 0.658 0.682 0.715 0.656 0.633 0.293 0.331 0.288 0.341 0.154 0.142
WormsTwoClass 0.729 0.755 0.687 0.689 0.780 0.781 0.699 0.712 0.532 0.534 0.409 0.462 0.401 0.417
Yoga 0.894 0.894 0.827 0.828 0.880 0.881 0.829 0.833 0.783 0.784 0.700 0.716 0.432 0.485
AllGestureWiimoteX 0.754 0.757 0.033 0.031 0.786 0.787 0.785 0.785 0.680 0.683 0.232 0.271 0.028 0.021
AllGestureWiimoteY 0.774 0.774 0.066 0.074 0.776 0.778 0.787 0.788 0.678 0.686 0.250 0.287 0.033 0.026
AllGestureWiimoteZ 0.775 0.778 0.055 0.057 0.758 0.759 0.757 0.757 0.651 0.656 0.255 0.293 0.039 0.035
BME 0.991 0.991 0.998 0.998 0.980 0.980 0.971 0.971 0.845 0.844 0.744 0.761 0.331 0.381
Chinatown 0.925 0.915 0.969 0.964 0.966 0.960 0.852 0.880 0.965 0.958 0.697 0.719 0.578 0.576
Crop 0.775 0.776 0.753 0.753 0.752 0.752 0.730 0.731 0.656 0.662 0.381 0.412 0.348 0.349
EOGHorizontalSignal 0.686 0.698 0.528 0.543 0.552 0.570 0.555 0.546 0.448 0.466 0.260 0.298 0.120 0.112
EOGVerticalSignal 0.455 0.475 0.395 0.415 0.485 0.496 0.452 0.455 0.422 0.435 0.224 0.251 0.086 0.083
EthanolLevel 0.810 0.812 0.581 0.582 0.471 0.470 0.313 0.314 0.271 0.272 0.215 0.237 0.135 0.130
FreezerRegularTrain 0.994 0.994 0.987 0.987 0.983 0.983 0.961 0.961 0.839 0.839 0.694 0.704 0.696 0.707
FreezerSmallTrain 0.781 0.782 0.901 0.903 0.870 0.870 0.862 0.863 0.760 0.760 0.445 0.489 0.546 0.582
Fungi 0.854 0.875 0.901 0.918 0.943 0.948 0.993 0.994 0.052 0.040 0.337 0.355 0.014 0.011
GestureMidAirD1 0.722 0.737 0.038 0.033 0.625 0.650 0.565 0.565 0.418 0.441 0.491 0.491 0.007 0.005
GestureMidAirD2 0.594 0.618 0.030 0.031 0.523 0.539 0.469 0.477 0.339 0.353 0.378 0.375 0.007 0.005
GestureMidAirD3 0.252 0.261 0.039 0.036 0.294 0.304 0.259 0.265 0.115 0.106 0.230 0.231 0.013 0.015
GesturePebbleZ1 0.863 0.871 0.122 0.134 0.843 0.860 0.877 0.878 0.660 0.676 0.670 0.698 0.047 0.034
GesturePebbleZ2 0.850 0.855 0.099 0.099 0.854 0.857 0.838 0.837 0.599 0.625 0.623 0.659 0.047 0.041
GunPointAgeSpan 0.996 0.996 0.966 0.967 0.980 0.980 0.974 0.974 0.994 0.994 0.689 0.721 0.500 0.517
GunPointMaleVersusFemale 0.999 0.999 0.999 0.999 1.000 1.000 0.997 0.997 0.968 0.968 0.911 0.915 0.721 0.750
GunPointOldVersusYoung 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.997 0.997 0.835 0.856
HouseTwenty 0.941 0.941 0.839 0.845 0.898 0.904 0.854 0.848 0.882 0.882 0.563 0.590 0.511 0.574
InsectEPGRegularTrain 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.616 0.597
InsectEPGSmallTrain 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.889 0.900 0.499 0.523
MelbournePedestrian 0.958 0.958 0.928 0.930 0.955 0.955 0.938 0.938 0.831 0.840 0.631 0.651 0.193 0.197
MixedShapesRegularTrain 0.949 0.948 0.916 0.915 0.920 0.919 0.923 0.923 0.816 0.817 0.633 0.651 0.219 0.245
MixedShapesSmallTrain 0.875 0.873 0.832 0.835 0.848 0.848 0.874 0.876 0.722 0.735 0.551 0.575 0.166 0.196
PickupGestureWiimoteZ 0.787 0.811 0.072 0.072 0.843 0.855 0.802 0.805 0.654 0.670 0.554 0.581 0.043 0.034
PigAirwayPressure 0.528 0.551 0.206 0.229 0.614 0.634 0.453 0.458 0.037 0.039 0.041 0.052 0.004 0.003
PigArtPressure 0.977 0.980 0.427 0.444 0.962 0.967 0.920 0.921 0.281 0.279 0.081 0.103 0.008 0.008
PigCVP 0.793 0.813 0.179 0.187 0.798 0.809 0.727 0.732 0.173 0.185 0.065 0.079 0.005 0.005
PLAID 0.436 0.499 0.037 0.051 0.511 0.547 0.572 0.564 0.830 0.868 0.215 0.242 0.059 0.053
PowerCons 0.951 0.951 0.984 0.984 0.964 0.964 0.870 0.870 0.855 0.856 0.996 0.996 0.661 0.676
Rock 0.418 0.446 0.508 0.510 0.694 0.692 0.594 0.616 0.404 0.391 0.352 0.371 0.176 0.163
SemgHandGenderCh2 0.663 0.703 0.859 0.857 0.952 0.950 0.877 0.869 0.902 0.904 0.781 0.793 0.576 0.606
SemgHandMovementCh2 0.717 0.720 0.668 0.671 0.871 0.872 0.786 0.785 0.767 0.773 0.372 0.405 0.257 0.267
ContinuedonnextpageTable6continuedfrompreviouspage
Dataset RankSCL InfoTS TS2Vec T-Loss DTW TS-TCC TNC
F1 Recall F1 Recall F1 Recall F1 Recall ACC Prec. ACC Prec. ACC Prec.
SemgHandSubjectCh2 0.877 0.880 0.851 0.853 0.948 0.948 0.862 0.862 0.847 0.851 0.687 0.701 0.401 0.407
ShakeGestureWiimoteZ 0.876 0.888 0.092 0.084 0.922 0.930 0.935 0.932 0.789 0.784 0.722 0.750 0.025 0.019
SmoothSubspace 0.975 0.975 0.808 0.806 0.976 0.976 0.943 0.943 0.859 0.860 0.751 0.760 0.781 0.785
UMD 0.990 0.990 0.990 0.990 0.994 0.994 0.992 0.992 0.853 0.853 0.863 0.871 0.272 0.253
DodgerLoopDay 0.543 0.561 0.534 0.560 0.501 0.520 – – 0.320 0.375 0.331 0.352 0.099 0.114
DodgerLoopGame 0.802 0.802 0.774 0.779 0.811 0.815 – – 0.858 0.877 0.662 0.703 0.457 0.440
DodgerLoopWeekend 0.879 0.868 0.964 0.962 0.938 0.937 – – 0.935 0.933 0.741 0.753 0.466 0.453
AVG 0.803 0.810 0.705 0.714 0.799 0.807 0.743 0.747 0.668 0.680 0.566 0.584 0.291 0.298
Table6:F1scoresandRecallscoresofourmethodcomparedwiththoseofothermethodson128UCRdatasets.Dataset RankSCL InfoTS TS2Vec T-Loss DTW TS-TCC TNC
ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec. ACC Prec.
ArticularyWordRecognition 0.977 0.978 0.981 0.984 0.982 0.983 0.927 0.869 0.961 0.961 0.983 0.985 0.579 0.601
AtrialFibrillation 0.293 0.309 0.240 0.395 0.141 0.121 0.304 0.228 0.133 0.133 0.467 0.435 0.281 0.165
BasicMotions 1.000 1.000 0.975 0.977 0.985 0.986 0.945 0.903 0.955 0.955 0.775 0.882 0.500 0.523
CharacterTrajectories 0.996 0.995 0.993 0.993 0.682 0.808 0.979 0.961 0.060 0.050 0.982 0.981 0.197 0.158
Cricket 0.986 0.987 0.972 0.975 0.971 0.974 0.778 0.721 0.989 0.989 1.000 1.000 0.553 0.485
DuckDuckGeese 0.604 0.642 0.496 0.495 0.493 0.543 0.331 0.295 0.580 0.579 0.340 0.296 0.356 0.369
EigenWorms 0.727 0.786 0.841 0.791 0.660 0.661 0.311 0.320 0.846 0.790 0.588 0.474 0.369 0.108
Epilepsy 0.975 0.977 0.963 0.961 0.959 0.958 0.859 0.815 0.978 0.979 0.935 0.938 0.317 0.284
ERing 0.913 0.915 0.850 0.870 0.932 0.937 0.884 0.850 0.838 0.837 0.933 0.935 0.501 0.519
EthanolConcentration 0.304 0.309 0.282 0.285 0.258 0.258 0.291 0.283 0.234 0.231 0.289 0.302 0.308 0.199
FaceDetection 0.637 0.638 0.512 0.512 0.520 0.520 0.540 0.506 0.518 0.505 0.534 0.534 0.518 0.523
FingerMovements 0.528 0.529 0.480 0.480 0.483 0.484 0.476 0.458 0.536 0.533 0.620 0.620 0.476 0.473
HandMovementDirection 0.322 0.333 0.308 0.340 0.359 0.362 0.434 0.373 0.305 0.334 0.189 0.268 0.286 0.247
Handwriting 0.566 0.560 0.520 0.536 0.394 0.418 0.297 0.204 0.425 0.438 0.500 0.548 0.061 0.029
Heartbeat 0.740 0.678 0.711 0.675 0.745 0.710 0.724 0.717 0.713 0.647 0.722 0.630 0.694 0.589
JapaneseVowels 0.980 0.977 0.985 0.987 0.988 0.988 0.858 0.785 0.080 0.110 0.968 0.968 0.189 0.179
Libras 0.914 0.923 0.851 0.860 0.849 0.856 0.701 0.641 0.868 0.868 0.867 0.868 0.250 0.202
LSST 0.164 0.201 0.543 0.416 0.566 0.454 0.366 0.379 0.528 0.325 0.559 0.421 0.360 0.216
MotorImagery 0.550 0.557 0.498 0.249 0.472 0.472 0.526 0.418 0.526 0.500 0.490 0.490 0.534 0.536
NATOPS 0.927 0.928 0.916 0.916 0.922 0.924 0.729 0.631 0.912 0.912 0.878 0.877 0.301 0.341
PEMS-SF 0.881 0.886 0.684 0.692 0.705 0.714 0.676 0.570 0.635 0.638 0.555 0.564 0.301 0.302
PenDigits 0.986 0.987 0.989 0.989 0.987 0.987 0.910 0.871 0.986 0.986 0.979 0.979 – –
PhonemeSpectra 0.271 0.271 0.234 0.233 0.209 0.209 0.155 0.140 0.231 0.232 0.164 0.177 0.065 0.050
RacketSports 0.881 0.891 0.850 0.869 0.832 0.845 0.794 0.753 0.810 0.824 0.822 0.838 0.408 0.423
SelfRegulationSCP1 0.862 0.867 0.786 0.815 0.867 0.870 0.862 0.850 0.832 0.866 0.833 0.850 0.627 0.666
SelfRegulationSCP2 0.549 0.549 0.560 0.560 0.536 0.539 0.504 0.461 0.541 0.525 0.517 0.517 0.518 0.517
SpokenArabicDigits 0.989 0.989 0.988 0.988 0.915 0.921 0.959 0.941 0.100 0.100 0.980 0.981 0.123 0.060
StandWalkJump 0.533 0.529 0.480 0.418 0.403 0.400 0.379 0.357 0.320 0.320 0.267 0.103 0.320 0.227
UWaveGestureLib 0.873 0.874 0.898 0.901 0.780 0.780 0.755 0.650 0.893 0.892 0.897 0.903 0.351 0.328
InsectWingbeat 0.518 0.512 0.464 0.460 0.464 0.468 0.253 0.225 0.100 0.100 – – – –
AVG 0.715 0.719 0.695 0.687 0.669 0.672 0.617 0.573 0.581 0.572 0.654 0.645 0.345 0.311
Table7:AccuracyscoresandPrecisionscoresofourmethodcomparedwiththoseofothermethodson30UEAdatasets.Dataset RankSCL InfoTS TS2Vec T-Loss DTW TS-TCC TNC
F1 Recall F1 Recall F1 Recall F1 Recall F1 Recall F1 Recall F1 Recall
ArticularyWordRecognition 0.977 0.977 0.981 0.982 0.982 0.983 0.849 0.859 0.960 0.961 0.984 0.984 0.535 0.566
AtrialFibrillation 0.298 0.303 0.241 0.299 0.127 0.124 0.202 0.214 0.160 0.145 0.407 0.421 0.188 0.175
BasicMotions 1.000 1.000 0.975 0.976 0.985 0.986 0.887 0.895 0.955 0.955 0.718 0.792 0.456 0.487
CharacterTrajectories 0.995 0.995 0.993 0.993 0.690 0.745 0.955 0.958 0.010 0.017 0.980 0.980 0.129 0.142
Cricket 0.986 0.987 0.972 0.973 0.971 0.972 0.673 0.696 0.989 0.989 1.000 1.000 0.460 0.472
DuckDuckGeese 0.604 0.622 0.481 0.487 0.505 0.523 0.225 0.255 0.577 0.578 0.280 0.288 0.330 0.349
EigenWorms 0.655 0.714 0.786 0.789 0.608 0.634 0.285 0.301 0.793 0.791 0.386 0.425 0.132 0.119
Epilepsy 0.975 0.976 0.961 0.961 0.958 0.958 0.778 0.796 0.979 0.979 0.933 0.935 0.291 0.287
ERing 0.913 0.914 0.848 0.859 0.933 0.935 0.844 0.847 0.832 0.834 0.933 0.934 0.425 0.467
EthanolConcentration 0.305 0.307 0.282 0.283 0.247 0.252 0.180 0.220 0.235 0.233 0.286 0.294 0.198 0.198
FaceDetection 0.637 0.637 0.510 0.511 0.519 0.520 0.468 0.486 0.397 0.445 0.534 0.534 0.491 0.506
FingerMovements 0.527 0.528 0.480 0.480 0.483 0.483 0.416 0.436 0.473 0.501 0.620 0.620 0.469 0.471
HandMovementDirection 0.319 0.325 0.307 0.323 0.354 0.358 0.252 0.301 0.308 0.321 0.189 0.222 0.247 0.247
Handwriting 0.537 0.548 0.477 0.505 0.375 0.395 0.204 0.204 0.417 0.427 0.473 0.508 0.021 0.024
Heartbeat 0.659 0.668 0.547 0.604 0.639 0.673 0.664 0.690 0.613 0.630 0.584 0.606 0.542 0.564
JapaneseVowels 0.980 0.978 0.985 0.986 0.988 0.988 0.746 0.765 0.020 0.034 0.965 0.966 0.110 0.136
Libras 0.913 0.918 0.852 0.856 0.849 0.853 0.564 0.600 0.866 0.867 0.865 0.866 0.178 0.189
LSST 0.090 0.124 0.357 0.384 0.411 0.431 0.343 0.361 0.329 0.327 0.361 0.389 0.124 0.158
MotorImagery 0.531 0.544 0.332 0.285 0.464 0.468 0.415 0.417 0.333 0.400 0.490 0.490 0.516 0.526
NATOPS 0.926 0.927 0.916 0.916 0.922 0.923 0.633 0.632 0.912 0.912 0.877 0.877 0.284 0.310
PEMS-SF 0.879 0.882 0.682 0.687 0.702 0.708 0.569 0.570 0.626 0.632 0.525 0.544 0.296 0.299
PenDigits 0.986 0.986 0.989 0.989 0.987 0.987 0.870 0.870 0.985 0.985 0.979 0.979 – –
PhonemeSpectra 0.267 0.269 0.230 0.231 0.207 0.208 0.073 0.096 0.229 0.231 0.162 0.169 0.045 0.047
RacketSports 0.890 0.890 0.859 0.864 0.843 0.844 0.697 0.724 0.823 0.824 0.834 0.836 0.385 0.403
SelfRegulationSCP1 0.862 0.864 0.781 0.798 0.867 0.869 0.826 0.838 0.866 0.866 0.831 0.840 0.595 0.628
SelfRegulationSCP2 0.548 0.549 0.560 0.560 0.531 0.535 0.429 0.444 0.430 0.473 0.514 0.515 0.505 0.511
SpokenArabicDigits 0.989 0.989 0.988 0.988 0.916 0.919 0.929 0.935 0.020 0.033 0.980 0.980 0.053 0.056
StandWalkJump 0.524 0.526 0.426 0.422 0.398 0.399 0.293 0.322 0.301 0.310 0.148 0.121 0.250 0.238
UWaveGestureLib 0.871 0.873 0.895 0.898 0.778 0.779 0.586 0.616 0.890 0.891 0.895 0.899 0.316 0.321
InsectWingbeat 0.512 0.512 0.459 0.460 0.464 0.466 0.141 0.174 0.020 0.033 – – – –
AVG 0.705 0.712 0.672 0.679 0.657 0.664 0.533 0.552 0.545 0.558 0.624 0.635 0.286 0.298
Table8:F1scoresandRecallscoresofourmethodcomparedwiththoseofothermethodson30UEAdatasets.