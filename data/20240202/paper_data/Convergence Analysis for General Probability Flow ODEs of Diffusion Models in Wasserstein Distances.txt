Convergence Analysis for General Probability Flow ODEs of
Diffusion Models in Wasserstein Distances
Xuefeng Gao xfgao@se.cuhk.edu.hk
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong, Shatin, N.T. Hong Kong.
Lingjiong Zhu zhu@math.fsu.edu
Department of Mathematics
Florida State University, Tallahassee, FL, USA.
Abstract
Score-basedgenerativemodelingwithprobabilityflowordinarydifferentialequations(ODEs)
hasachievedremarkablesuccessinavarietyofapplications. WhilevariousfastODE-based
samplers have been proposed in the literature and employed in practice, the theoretical
understandings about convergence properties of the probability flow ODE are still quite
limited. In this paper, we provide the first non-asymptotic convergenceanalysis for a gen-
eral class of probability flow ODE samplers in 2-Wasserstein distance, assuming accurate
score estimates. We then consider various examples and establish results on the iteration
complexity of the corresponding ODE-based samplers.
Keywords: Probability flow ODEs, diffusion models, Wasserstein convergence.
1. Introduction
Score-based generative models(SGMs) (Sohl-Dickstein et al.,2015;Song and Ermon,2019;
Ho et al., 2020; Song et al., 2021), or diffusion models, have achieved remarkable suc-
cess in a range of applications, particularly in the realm of image and audio generation
(Rombach et al., 2022; Ramesh et al., 2022; Popov et al., 2021). These models employ a
unique approach where samples from a target data distribution are progressively corrupted
with noise through a forward process. Subsequently, the models learn to reverse this cor-
rupted process in order to generate new samples.
In this paper, we aim to provide theoretical guarantees for the probability flow ODE
(ordinary differential equation) implementation of SGMs, proposed initially in Song et al.
(2021). The forward process in SGMs, denoted by (x ) , follows the stochastic differ-
t t [0,T]
∈
ential equation (SDE):
dx = f(t)x dt+g(t)dB , x p , (1)
t t t 0 0
− ∼
wherebothf(t)andg(t) arescalar-valued non-negative continuous functions oftime t,(B )
t
is the standard d-dimensional Brownian motion, and p is the d dimensional (unknown)
0
−
target data distribution. Two popular choices of forward processes in the literature are
Variance Exploding(VE) SDEs and Variance Preserving (VP) SDEs Song et al. (2021); see
Section 2 for more details. If we denote p (x) as the probability density function of x in
t t
1
4202
naJ
13
]LM.tats[
1v85971.1042:viXra(1), then Song et al. (2021) showed that there exists an ODE:
dx˜ 1
t = f(T t)x˜
t
+ (g(T t))2 xlogp
T
t(x˜ t),
dt − 2 − ∇ −
x˜ p , (2)
0 T
∼
where the solution x˜ at time t [0,T], is distributed according to p . The ODE (2) is
t T t
∈ −
called theprobability flow ODE.NotethattheprobabilityflowODEinvolves thescore func-
tion, xlogp t(x), which is unknown. In practice, it can be approximated using neural net-
∇
workswhicharetrainedwithappropriatescorematchingtechniques(Hyv¨arinen and Dayan,
2005; Vincent, 2011; Song et al., 2020). Once the score function is estimated, one can sam-
ple x˜ from a normal distribution to initialize the ODE, and numerically solve the ODE
0
forward in time with any ODE solvers such as Euler (Song et al., 2021) or Heun’s 2nd order
method (Karras et al., 2022). The resulting sample generated at time T can be viewed as
an approximate sample from the target distribution since x˜ p .
T 0
∼
A large body of work on diffusion models has recently investigated various methods for
faster generation of samples based on the probability flow ODE; see, e.g., Lu et al. (2022);
Karras et al. (2022); Zhang and Chen (2023); Zhao et al. (2023); Song et al. (2023). While
these methods are quite effective in practice, the theoretical understandings of this proba-
bility flowODE approacharestill quitelimited. Toourbestknowledge, Chen et al.(2023e)
established the first non-asymptotic convergence guarantees for the probability flow ODE
sampler, but it did not provide concrete polynomial dependency on the dimension d and
1/ǫ, where ǫ is the prescribed error between the data distribution and the generated distri-
bution. Chen et al.(2023c)consideredtheaspecificVP-SDE astheforwardprocess(where
f 1andg √2), andprovidedpolynomial-timeguarantees forsomevariants oftheprob-
≡ ≡
ability flow ODE based implementation. Specifically, the algorithms they analyzed rely on
the use of stochastic corrector steps, so the samplers are not fully deterministic. Li et al.
(2023) also considered a specific VP-SDE as the forward processes and analyzed directly
a discrete-time version of probability flow ODEs and obtained convergence guarantees for
fully deterministic samplers.
These theoretical studies have focused on convergence analysis of probability flow ODE
samplers inTotal Variation (TV)distance or Kullback-Leibler (KL)divergence between the
datadistributionandthegenerateddistribution. However, practitionersareofteninterested
in the 2-Wasserstein ( ) distance. For instance, in image-related tasks, Fr´echet Inception
2
W
Distance (FID) is a widely adopted performance metric for evaluating the quality of gener-
ated samples, where FID measures the distance between the distribution of generated
2
W
images andthedistribution of realimages (Heusel et al.,2017). Inaddition, previousworks
of (Chen et al., 2023c) and (Li et al., 2023) have studied specific choices of f and g in their
convergence analysis of ODE-based samplers. However, it has been shown in Song et al.
(2021) that the empirical performance of probability flow ODE samplers depends crucially
onthechoice off andg in(2), which indicates theimportanceofselecting theseparameters
(noise schedules) of diffusion models. This leads to the following question which we study
in this paper:
Can we establish Wasserstein convergence guarantees for probability flow ODE samplers
with general functions f and g?
2Our Contributions.
• We establish non-asymptotic convergence guarantees for a general class of probability
flow ODEs in 2-Wasserstein distance, assuming that the score function can be accu-
ratelylearnedandthedatadistributionhasasmoothandstronglylog-concave density
(Theorem 4). In particular, we allow general functions f and g in the probability flow
ODE (2), and our results apply to both VP and VE SDE models. Theorem 4 di-
rectly translates to an upper bound on the iteration complexity, which is the number
of sampling steps needed for the ODE sampler to yield ǫ accuracy in 2-Wasserstein
−
distance between the data distribution and the generative distribution of the SGMs.
• We specialize our result to ODE samplers with specific functions f and g that are
commonly used in the literature, and we find that the complexity of VE-SDEs is
worse than that of VP-SDEs for the examples we analyze. See Table 3 for details.
This theoretical finding is consistent with the empirical observation in Song et al.
(2021), where they found that the empirical performance of probability flow ODE
samplers depends on the choice of f and g, and the sample quality for VE-SDEs is
much worse than VP-SDEs for high-dimensional data.
• We obtain an iteration complexity bound √d/ǫ of the ODE sampler for each
O
of the three examples of VP-SDEs studied, (cid:16)where(cid:17)d is the dimension of the data
distribution and ignores the logarithmicefactors and hides dependency on other
O
parameters. We also show that (see Proposition 9) under mild assumptions there
are no other choicees of f and g so that the iteration complexity can be better than
√d/ǫ .
O
(cid:16) (cid:17)
• Oeur main proof technique relies on spelling out an explicit contraction rate in the
continuous-time ODE and providing a careful analysis controlling the discretiza-
tion and score-matching errors. Our proof technique is inspired by iteration com-
plexity results in the context of Langevin algorithms for sampling in the literature
(Dalalyan and Karagulyan, 2019); yet our analysis is more sophisticated and subtle
due to the time-inhomogeneity that is inherent in the probability flow ODE, as well
as the usage of an exponential integrator in the discretization scheme.
In Table 1, we provide a concise summary of the comparison of our results with closely
related studies on convergence analysis for probability flow ODE samplers. We impose a
stronger assumption on the data distribution for convergence analysis in Wasserstein dis-
tance. Suchalog-concavity assumptionisnecessaryinordertoobtainthecontraction ofthe
continuous-time probability flow ODE, as well as guaranteeing that the discretization and
score-matching errors of our algorithm does not propagate and accumulate over sampling
steps/iterations in our analysis. We refer to the discussions at the end of Section 3.2 after
we present the main result for details.
1.1 Related Work
In addition to the deterministic sampler based on probability flow ODEs, another major
family of diffusion samplers is based on discretization of the reverse-time SDE, which is
3Table 1: Comparison with the existing literature on convergence rate analysis for prob-
ability flow ODEs, where d is the dimension of the data distribution and ǫ is the error
level.
Forward Data
Reference Sampling Distance Complexity
Process Distribution
General Fully Mild
Chen et al. (2023e) KL Non-explicit
SDE Deterministic Assumptions
Chen et al. (2023c) A specific Needs TV Mild ˜ √d
VP-SDE Stochasticity Assumptions O ǫ
(cid:16) (cid:17)
Li et al. (2023) A specific Fully TV Mild ˜ d2
VP-SDE Deterministic Assumptions O ǫ
General Fully Strong (f,g) d(cid:16)epe(cid:17)ndent
Our paper 2 −
(linear) SDE Deterministic W log-concavity See Table 3
obtained by reversing the forward process (1) in time (Anderson, 1982; Cattiaux et al.,
2023)). This leads to SDE-based stochastic samplers due to the noise in the reverse-
time SDE. Compared with SDE-based samplers, ODE-based deterministic samplers are
often claimed to converge much faster, at the cost of slightly inferior sample quality;
see e.g. Yang et al. (2023). In recent years, there has been a significant surge in re-
search focused on the convergence theory of SDE-based stochastic samplers for diffusion
models, particularly when assuming access to precise estimates of the score function; see,
e.g., Block et al. (2020); De Bortoli et al. (2021);De Bortoli (2022); Lee et al. (2022, 2023);
Chen et al. (2023a,d); Li et al. (2023); Chen et al. (2023b); Gao et al. (2023); Benton et al.
(2023a); Tang and Zhao (2024). These studies have mostly focused on the convergence
analysis of SDE-based stochastic samplers in TV or KL divergence. De Bortoli (2022);
Chen et al. (2023a,d) provided Wasserstein convergence bound for the SDE-based sampler
for the DDPM model in Ho et al. (2020) for bounded data distribution, in which case the
2-Wasserstein distance can be bounded by the TV distance. Gao et al. (2023) established
convergenceguaranteesforSDE-basedsamplersforageneralclassofSGMsin2-Wasserstein
distance, for unboundedsmoothlog-concave data distribution. Ourstudydiffersfromthese
studies in that we consider deterministic samplers based on the probability flow ODE im-
plementation of SGMs.
Ourwork is also broadly related to flow basedmethods. Recently, Benton et al. (2023b)
provided the error bounds for the flow matching methods (Albergo and Vanden-Eijnden,
2022) that covers probability flow ODEs in score-based diffusion models. However, they do
not consider discretization error in terms of time and their error boundscales exponentially
in a regularity parameter. Albergo et al. (2023) extended Albergo and Vanden-Eijnden
(2022) and introduced a framework that unifies flow-based and diffusion-based methods
using stochastic interpolants. Finally, Cheng et al. (2023) provided convergence guarantees
for a progressive flow model, which differs from score-based diffusion models in that the
flow model they consider is deterministic in the forward (data-to-noise) process.
Notations. For any d-dimensional random vector x with finite second moment, the
L -norm of x is defined as x = E x 2, where denotes the Euclidean norm.
2
k
kL2
k k k·k
p
4We denote (x) as the law of x.
For any twoL Borel probability measures µ ,µ (Rd), the space consisting of all the
1 2 2
∈ P
Borel probability measures on Rd with the finite second moment (based on the Euclidean
norm), the standard 2-Wasserstein distance (Villani, 2009) is defined by
(µ ,µ ) := infE[ x x 2],
2 1 2 1 2
W k − k
p
where the infimum is taken over all joint distributions of the random vectors x ,x with
1 2
marginal distributions µ ,µ .
1 2
A differentiable function F from Rd to R is said to be µ-strongly convex and L-smooth
(i.e. F is L-Lipschitz) if for every u,v Rd,
∇ ∈
µ L
u v 2 F(u) F(v) F(v) (u v) u v 2.
⊤
2k − k ≤ − −∇ − ≤ 2k − k
2. Preliminaries
Recall p (Rd)denotes theunknowndatadistribution whichhas adensity, where (Rd)
0
∈ P P
is the space of all probability measures on Rd. Given i.i.d samples from p , the problem
0
of generative modelling is to generate new samples that (approximately) follow the data
distribution.
We consider the probability flow ODE (see (2)) based implementation of SGMs for
samplegeneration, seee.g. Song et al.(2021). Thefunctionsf andg in(2)canbegeneralin
our study, and in particular, our study covers the following two classes of models commonly
used in the literature:
• f(t) 0 and g(t) = d[σ2(t)] for some nondecreasing function σ(t), e.g., g(t) = aebt
≡ dt
for some positive conqstants a,b. The corresponding forward SDE (1) is referred to as
Variance Exploding (VE) SDE.
• f(t)= 1β(t)andg(t) = β(t)forsomenondecreasingfunctionβ(t),e.g.,β(t) = at+b
2
for some positive constants a,b. The corresponding forward SDE (1) is referred to as
p
Variance Preserving (VP) SDE.
To implement the ODE (2), one needs to
(a) sample from a tractable distribution (aka prior distribution) to initialize the ODE,
(b) estimate the score function xlogp t(x), and
∇
(c) numerically solve/integrate the ODE.
We next discuss these three issues (or error sources).
First, we discuss (a), the prior distribution. Note that p is unknown. To provide an
T
unifying analysis for probability flow ODEs with general f and g (including both VE and
VP SDE models), we choose the prior distribution to be a normal distribution pˆ given as
T
follows:
T
pˆ
T
:= 0, e −2 tTf(s)ds(g(t))2dt I
d
, (3)
N ·
(cid:18) Z0 R (cid:19)
5and I is the d-dimensional identity matrix. To see why this is a reasonable choice, we note
d
that the forward SDE (1) has an explicit solution
t
x
t
= e
−
0tf(s)dsx
0+ e
−
stf(v)dvg(s)dB
s. (4)
R Z0 R
Hence, we can the take the distribution of the Brownian integral
0t
e
−
stf(v)dvg(s)dB
s
in
(4), whichis pˆ T, asan approximation of p T, sothat W2(p T,pˆ T)
≤
e
−R
0Tf(Rs)ds
kx
0
kL2.Hence,
we consider the ODE: R
dy 1
t = f(T t)y + (g(T t))2 logp (y ),
t T t t
dt − 2 − ∇ −
y pˆ , (5)
0 T
∼
as an approximation of the ODE (2) which starts from p .
T
Remark 1 If the forward process is a VP-SDE with a stationary distribution which is
normal, one can also take it as the prior distribution and our main result in this paper can
be easily adapted to this setting.
We next consider (b) score matching, i.e., approximate the unknown true score func-
tion xlogp t(x) using a time-dependent score model s θ(x,t), which is often a deep neural
∇
network parameterized by θ. To train the score model, one can use, for instance, denoising
score matching (Song et al., 2021), where the training objective for optimizing the neural
network is given by
T 2
m θin
Z0
hλ(t)E x 0E x t |x 0 (cid:13)s θ(x t,t) −∇x tlogp t |0(x t |x 0)
(cid:13)
idt. (6)
(cid:13) (cid:13)
Here, λ() : [0,T] R is some posi(cid:13)tive weighting function, x (cid:13)p is the data distri-
>0 0 0
· → ∼
bution, and p (x x ) is the density of x given x , which is Gaussian due to the choice
t0 t 0 t 0
| |
of the forward SDE in (1). Because one has i.i.d. samples from p , the distribution of x ,
0 0
the objective in (6) can be approximated by Monte Carlo methods and the resulting loss
function can be then optimized.
After the score function is estimated, we introduce a continuous-time process that ap-
proximates (5):
dz 1
t = f(T t)z + (g(T t))2s (z ,T t),
t θ t
dt − 2 − −
z pˆ , (7)
0 T
∼
where we replace the true score function in (5) by the estimated score s .
θ
Finally, we discuss (c) numerically solve the ODE (7) for generation of new samples.
Therearevarious methodsproposedand employed in practice, includingEuler (Song et al.,
2021), Heun’s 2nd order method (Karras et al., 2022), DPM solver (Lu et al., 2022), expo-
nential integrator (Zhang and Chen, 2023), to name just a few. In this paper, we consider
the following exponential integrator (i.e. exactly integrating the linear part) discretization
6of the ODE (7) for our theoretical convergence analysis. Let η > 0 be the stepsize and
without loss of generality, let us assume that T = Kη, where K is a positive integer. By
using the semi-linear structure of the ODE (7), one can multiply an exponential integrat-
ing factor to both hand sides of (7) and integrate from (k 1)η to kη, to obtain for any
−
k = 1,2,...,K:
z
kη
= e (k kη −1)ηf(T −t)dt z
(k 1)η
+ 1 kη e tkηf(T −s)ds(g(T t))2s θ(z t,T t)dt. (8)
R − 2 Z(k −1)η R − −
Therefore, one can introduce the following exponential integrator discretization of the ODE
(7). Let u pˆ and for any k = 1,2,...,K, we have
0 T
∼
kη f(T t)dt
u k = e (k−1)η − u k 1
R −
+ 1 kη e tkηf(T −s)ds(g(T t))2dt s θ(u
k
1,T (k 1)η). (9)
2 Z(k −1)η R − !· − − −
We are interested in the convergence of the generated distribution (u ) to the data
K
L
distribution p , where (u ) denotes the law or distribution of u . Specifically, our goal is
0 K K
L
to bound the 2-Wasserstein distance ( (u ),p ), and investigate the number of iterates
2 K 0
W L
K that is needed in order to achieve ǫ accuracy, i.e. ( (u ),p ) ǫ.
2 K 0
W L ≤
3. Main Results
In this section we state our main results. We discuss the assumptions in Section 3.1, and
presentthemainresultinSection3.2. Wethenspecializeourmainresulttoseveralconcrete
examples and obtain explicit iteration complexity of the ODE sampler for each example in
Section 3.3. Finally, in Section 3.4 we provide some further discussions about the obtained
results. The proofs of results are deferred to the Appendix.
3.1 Assumptions
We follow the same assumptions as in Gao et al. (2023). The first assumption is on the
density of data distribution p , which implies that x p is L -integrable.
0 0 0 2
∼
Assumption 1 Assume that the density p is differentiable and positive everywhere. More-
0
over, logp is m -strongly convex and L -smooth for some m ,L > 0.
0 0 0 0 0
−
Remark 2 Weneedtheassumptionofstrong-log-concavity datadistributionmainlybecause
we consider Wasserstein convergence, and the ODE may be not contractive and discretiza-
tion errors will accumulate over sampling steps without such an assumption. For further
technical details, see the discussion at the end of Section 3.2.
Our next assumption is about the true score function xlogp t(x) for t [0,T]. We
∇ ∈
assume that xlogp t(x) is Lipschtiz in time, where the Lipschitz constant has at most
∇
linear growth in x . Assumption 2 is needed in controlling the discretization error of the
k k
ODE (7).
7Table 2: Summary of quantities, their interpretations and the sources
Quantities Interpretations Sources/References
η¯ in Theorem 4 Upper bound for the stepsize (27)
µ(t) in (24) Contraction rate of ( (y ),p ) (35)
2 T 0
W L
L(t) in (26) Lipschitz constant of xlogp t(x) Lemma 14
∇
Contraction rate of discretization
γ in (17) Theorem 4
j,η and score-matching errors in u
j
A component in the discretization
φ in (15) Theorem 4
k,η and score-matching errors in u
j
A component in the discretization
ψ in (16) Theorem 4
k,η and score-matching errors in u
j
A component in the contraction rate of
δ (T t) in (30) Proposition 13
j − discretization and score-matching errors in u j
ω(T) in (32) sup x (58)
0 ≤t ≤T k t kL2
ν in (33) Bound for sup y y Lemma 15
k,η (k −1)η ≤t ≤kη t − (k −1)η L2
(cid:13) (cid:13)
(cid:13) (cid:13)
Assumption 2 There exists some constant L such that
1
sup logp (x) logp (x) L η(1+ x ), for all x. (10)
T t T (k 1)η 1
1 k K ∇ − −∇ − − ≤ k k
(k 1≤)η≤t kη(cid:13) (cid:13)
− ≤ ≤ (cid:13) (cid:13)
The next assumption is on the score-matching approximation. Recall (u ) are the
k
iterates defined in (9).
Assumption 3 Assume that there exists M > 0 such that
sup logp (u ) s (u ,T (k 1)η) M. (11)
T (k 1)η k 1 θ k 1
k=1,...,K ∇ − − − − − − − L2 ≤
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
Remark 3 It is largely an open (theoretical) question when networks can approximate the
score function accurately inSGMs. Moststudies on convergence analysis of diffusionmodels
assume some form of L error for score learning and focus on the sampling phase. We also
2
follow this approach. Some papers, e.g., Chen et al. (2023d), assume
sup logp (x ) s (x ,kη) M, (12)
k∇ kη kη − θ kη kL2 ≤
k=1,...,K
where (x ) is the continuous-time forward process in (1). The difference is that the expecta-
t
tion in the L norm of (11) is taken w.r.t. the algorithm iterates (u ). The main result
2 k 1
−
in our paper will still hold if Assumption 3 is to be replaced by (12) under the additional
assumption that s (,kη) is Lipschitz for every k, and the observation that logp () is
θ kη
· ∇ ·
Lipschitz under Assumption 1 (see Lemma 14).
83.2 Main Result
We are now ready to state our main result.
Theorem 4 Suppose that Assumptions 1, 2 and 3 hold and the stepsize η η¯, where η¯> 0
≤
has an explicit formula given in (27) in Appendix A. Then, we have
W2( L(u K),p 0)
≤
e
−
0Kηµ(t)dt
·kx
0 kL2
+ E(f,g,K,η,M,L 1) .
R
Initialization error Discretization and score matching errors
(13)
| {z } | {z }
Here, µ(t) is given in (24) in Appendix A and
K K
Kηf(T t)dt
E(f,g,K,η,M,L 1) := γ j,η e kη −
·
R
k=1j=k+1
X Y
L M √η
1
η(1+ x +ω(T))φ + φ + ν ψ , (14)
· 2 k
0 kL2 k,η
2
k,η
2
k,η k,η
!
p
where for any k = 1,2,...,K,
kη
φ
k,η
:= e tkηf(T −s)ds(g(T t))2dt, (15)
−
Z(k −1)η R
kη
ψ
k,η
:= e2 tkηf(T −s)ds (g(T t))4(L(T t))2dt, (16)
· − −
Z(k −1)η R
and for any j = 1,2,...,K,
jη L η jη
γ := 1 δ (T t)dt+ 1 (g(T t))2dt, (17)
j,η j
− − 2 −
Z(j −1)η Z(j −1)η
and L(t) is given in (26), δ (T t) is defined in (30), ω(T) is defined in (32) and ν is
j k,η
−
given in (33) in Appendix A.
We defer the expressions of L(t),µ(t),δ (T t),ω(T) and ν in Theorem 4 to the
j k,η
−
Appendix to save spaces. However, we provide the interpretations of these quantities in
Table 2 to facilitate readers’ understanding.
While the boundin Theorem 4looks quite complex, theboundcan beeasily interpreted
as follows.
Thefirsttermin(13),referredtoastheinitializationerror,characterizestheconvergence
of the continuous-time probability flow ODE (y ) in (5) to the distribution p without
t 0
discretization or score-matching errors. Specifically, it bounds the error ( (y ),p ),
2 T 0
W L
which is introduced due to the initialization of the probability flow ODE (y ) at pˆ instead
t T
of p (see Proposition 10). One can find from the definition (24) that µ(t) > 0 and hence
T
the initialization error goes to zero when we pick T = Kη to be sufficiently large, i.e.,
e
−
0Kηµ(t)dt
·kx
0 kL2
→
0, as Kη
→
∞.
R
9The second term in (13) quantifies the discretization and score-matching errors in run-
ning the algorithm (u ) in (9). Note that the assumption η η¯ in Theorem 4 implies that
k
≤
γ (0,1) in (17), which plays the role of a contraction rate of the error y u
j,η ∈ k kη − k kL2
over iterations (see Proposition 11). Conceptually, it guarantees that as the number of
iterations increases, the discretization and score-matching errors in the iterates (u ) do not
k
propagate and grow in time, which helps us control the overall discretization and score-
matching errors. In particular, one can show that for fixed T = Kη, the discretization and
score-matching errors go to zero as the stepsize decreaes to zero, i.e.
E(f,g,K,η,M,L ) 0, when η 0.
1
→ →
On combing the above two terms, we infer that we can first choose a large T = Kη, and
then choose a small stepsize η so that ( (u ),p ) can be made small.
2 K 0
W L
Discussions about Assumption 1. Assumption 1 plays two roles for obtaining the
upper bound in Theorem 4.
• First, the m -strong-convexity of logp guarantees that µ(t) > 0 (which appears
0 0
−∇
in the first term of (13) and it is defined in (24) in Appendix A), which guarantees
the 2-Wasserstein contraction and the convergence of the continuous-time probability
flow ODE (y ) in (5) to the distribution p without discretization or score-matching
t 0
errors. Indeed, one can easily verify from (24) that µ(t) 0 as m 0, which
0
→ →
indicates that strong-convexity of logp is necessary; otherwise the ODE (5) will
0
−∇
not have a contraction.
• Second, the m -strong-convexity of logp , together with the L -Lipschitzness of
0 0 0
−∇
logp in Assumption 1, guarantees that the discretization and score-matching error
0
∇
at each iterate k can be explicitly controlled as in Theorem 4. In particular, As-
sumption 1 guarantees that γ (0,1) when the stepsize η η¯, which controls the
j,η
∈ ≤
propagation of the discretization and score-matching errors as the number of iterates
grows. The m -strong-convexity of logp is necessary since one can easily verify
0 0
−∇
that η¯ 0 and γ / (0,1) as m 0.
j,η 0
→ ∈ →
3.3 Examples
In this section, we consider several examples of the forward processes and discuss the im-
plications of Theorem 4. In particular, we consider a variety of choices for f and g in the
forward SDE (1), and investigate the iteration complexity, i.e., the number of iterates K
that is needed in order to achieve ǫ accuracy, i.e. ( (u ),p ) ǫ. While the bound in
2 K 0
W L ≤
Theorem 4 is quite sophisticated in general, it can be made more explicit when we consider
special f and g.
Example 1. We first consider a VE-SDE example, where f 0 and g has exponential
≡
growth in time. This example is considered in Song et al. (2021). Let f(t) 0 and g(t) =
≡
aebt for some a,b > 0, we can obtain the following corollary from Theorem 4.
Corollary 5 Letf(t) 0andg(t) = aebt forsomea,b > 0. Then, wehave ( (u ),p )
2 K 0
≡ W L ≤
(ǫ) after K =
d3/2log(d/ǫ)
iterations provided that M
ǫ2
and η
ǫ3
.
O O ǫ3 ≤ √d ≤ d3/2
(cid:16) (cid:17)
10Table 3: Summary of the iteration complexity of the algorithm (9) for various examples
in terms of ǫ and dimension d. Here f,g correspond to the drift and diffusion terms in
the forward SDE (1), and a, b, ρ are positive constants. In addition, K is the number of
iterates, M is the score-matching approximation error, and η is the stepsize required to
achieve accuracy level ǫ (i.e. ( (u ),p ) ǫ).
2 K 0
W L ≤
f g K M η
0 aebt d3/2log(d ǫ) ǫ2 ǫ3
O ǫ3 O √d O d3/2
(cid:18) (cid:19)
0 (b+at)c
d(2c1 +1)+3
2
(cid:16)
ǫ2
(cid:17) (cid:16)
ǫ3
(cid:17)
O ǫ2c2 +1+3 O √d O d3 2
(cid:18) (cid:19)
(cid:16) (cid:17) (cid:16) (cid:17)
b √b √d(log(d))2 ǫ ǫ
2 O ǫ ǫ O log(√d/ǫ) O √dlog(√d/ǫ)
b+at √b+at (cid:16)√d(log(d))3 2(cid:17) (cid:16) ǫ (cid:17) (cid:16) ǫ (cid:17)
2 O ǫ ǫ O log(√d/ǫ) O √dlog(√d/ǫ)
(b+at)ρ (b+at)ρ
2
(cid:16)√d(log(d))ρ ρ+ +2 1(cid:17) (cid:16) ǫ (cid:17) (cid:16) ǫ (cid:17)
2 O ǫ ǫ O log(√d/ǫ) O √dlog(√d/ǫ)
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
Example 2. Next, we consider another VE-SDE example, where f 0 and g has
≡
polynomial growth in time. Then we can obtain the following corollary from Theorem 4.
Corollary 6 Let f(t) 0 and g(t) = (b + at)c for some a,b > 0, c 1/2. Then, we
≡ ≥
have ( (u ),p ) (ǫ) after K =
d(2c1 +1)+3
2
iterations provided that M
ǫ2
and
W2 L K 0 ≤ O O ǫ2c2 +1+3 ≤ √d
(cid:18) (cid:19)
η
ǫ3
.
≤ d3 2
This example is proposed in Gao et al. (2023), which is inspired by Karras et al. (2022)
that considers f(t) 0,g(t) = √2t with non-uniform discretization time steps, where the
≡
time steps are defined according to a polynomial noise schedule. Note that we consider a
uniform time step with constant stepsizes in our analysis.
Example 3. Next, we consider a VP-SDE example, with constant f,g. This includes
the special case f 1, g √2 that is considered in Chen et al. (2023c). In particular, we
≡ ≡
consider f b, g √b for some b > 0. We obtain the following corollary from Theorem 4.
≡ 2 ≡
Corollary 7 Assume f b, g √b forsome b > 0. Then, we have ( (u ),p ) (ǫ)
≡ 2 ≡ W2 L K 0 ≤O
after K = √d(log(d))2 iterations provided that M ǫ and η ǫ .
O ǫ ǫ ≤ log(√d/ǫ) ≤ √dlog(√d/ǫ)
(cid:16) (cid:17)
Example 4. Finally, we consider a VP-SDE example where f,g have polynomial
growth. We consider f(t) = 1(b + at)ρ and g(t) = (b+at)ρ, where a,b > 0, which
2
is proposed in Gao et al. (2023). This includes the special case f(t) = 1(b + at) and
p 2
g(t) = √b+at, i.e. ρ = 1, that is studied in Ho et al. (2020). Then we can obtain the
following corollary from Theorem 4.
Corollary 8 Assumef(t)= 1(b+at)ρ andg(t) = (b+at)ρ. Then, wehave ( (u ),p )
2 W2 L K 0 ≤
(ǫ)afterK =
√d(log(d))ρ ρ+ +2
1 iterationsprovipdedthatM ǫ andη ǫ .
O O ǫ ǫ ≤ log(√d/ǫ) ≤ √dlog(√d/ǫ)
(cid:16) (cid:17)
11We observe from Corollary 8 that the complexity K deceases as ρ increases. However,
this does not suggest that the optimal complexity is achieved when ρ since our
→ ∞
complexity only keeps track the dependence on d and ǫ, and ignores any pre-factor that
can depend on ρ which might go to infinity as ρ . Indeed, it follows from η η¯ in
log(2) 2log(2) → ∞ ≤
Theorem 4 that η = 0 as ρ so that the complexity will
≤ max0≤t≤Tf(t) (b+aT)ρ → → ∞
explode as ρ .
→ ∞
3.4 Further discussions
In Table 3, we summarize the results about the interation complexity for the examples
discussed in Section 3.3. We next provide some further discussions.
An immediate observation from Table 3 is that the iteration complexity depends on f,g
andthecomplexity ofVE-SDEsisworsethanthatofVP-SDEs,atleastfortheexampleswe
analyze. This theoretical finding is generally consistent with Song et al. (2021), where they
observed empirically that the performance of probability flow ODE samplers (with Euler
or Runge-Kutta solvers) depends on the choice of forward SDEs and the sample quality for
VE-SDEs is much worse than VP-SDEs for high-dimensional data.
Another observation from Table 3 is that the best iteration complexity is of order
(√d/ǫ) from the examples we studied. One natural question is whether there are other
O
choices of f,g so that the iteration complexity becomes better than (√d/ǫ). We next
O
sehow that the answer to this question is negative, if we use the result in Theorem 4.
e
Proposition 9 Under the assumptions in Theorem 4, we also assume min (g(t))2L(t)>
t 0
ρ ≥
t
0 and max µ(s) c µ(s)ds + c uniformly in t for some c ,c ,ρ > 0, where
0 ≤s ≤t ≤ 1 0 2 1 2
µ(s) is defined in (24). We(cid:16)fRurther as(cid:17)sume liminf
T
0T e −2 sTf(v)dv(g(s))2ds > 0. If we
→∞
use the upper bound (13), then in order to achieve ǫ accuracy,Ri.e. ( (u ),p ) ǫ, we
2 K 0
R W L ≤
must have K = Ω √d/ǫ , where Ω ignores the logarithmic dependence on ǫ and d.
(cid:16) (cid:17)
e e
The assumptions in Proposition 9 are mild and one can readily check that they are
satisfied for all the examples in Table 3 that achieve the iteration complexity O
√d
. If
ǫ
we ignore the dependence on the logarithmic factors of d and ǫ, we can see from(cid:16)Tab(cid:17)le 3
that all the VP-SDE examples achieve the lower bound in Proposition 9. e
4. Outline of the Proof of Theorem 4
Inthissection,weprovideanoutlinefortheproofofourmainresult(Theorem4). Atahigh
level, there are three sources of errors in analyzing the convergence: (1) the initialization
of the algorithm at pˆ instead of p , (2) the estimation error of the score function, and (3)
T T
the discretization error of the continuous-time ODE (7).
First, we study the error introduced due to the initialization at pˆ instead of p . Recall
T T
the probability flow ODE y given in (5), which has the same dynamics as x˜ (defined in
t t
(2)) butwith a different priordistribution y pˆ (in contrast to x˜ p ). Sincepˆ = p ,
0 T 0 T T T
∼ ∼ 6
(y ) = p , whose error is a result of the initialization of the algorithm at pˆ instead of
T 0 T
L 6
p . The following result bounds ( (y ),p ).
T 2 T 0
W L
12Proposition 10 Assume p is m -strongly-log-concave. Then, we have
0 0
W2( L(y T),p 0)
≤
e
−
0Tµ(t)dt
kx
0
kL2, (18)
R
where µ(t) is given in (24) in Appendix A.
The main challenge in analyzing the ODE y lies in studying the term logp (y ).
t T t t
∇ −
In general, this term is neither linear in y nor admits a closed-form expression. However,
t
whenp
0
isstronglylog-concave, itisknownthatthat xlogp
T
t(x)isalsostronglyconcave
∇ −
(see e.g. Gao et al. (2023)). This fact allows us to establish Proposition 10 whoseproof will
be given in Appendix B.1.2.
Now we consider the algorithm (9) with iterates (u ), and bound the errors due to
k
both score estimations and discretizations. For any k = 0,1,2,...,K, u has the same
k
distribution as uˆ , where uˆ is a continuous-time process with the dynamics:
kη t
duˆ 1
t = f(T t)uˆ + (g(T t))2s uˆ ,T t/η η , (19)
t θ t/η η
dt − 2 − ⌊ ⌋ −⌊ ⌋
(cid:0) (cid:1)
with the initial distribution uˆ pˆ . We have the following result that provides an upper
0 T
∼
bound for y uˆ in terms of y uˆ .
k
kη
−
kη kL2
k
(k −1)η
−
(k −1)η kL2
Proposition 11 Assume that p is m -strongly-log-concave, and logp is L -Lipschitz.
0 0 0 0
∇
Then, for any k = 1,2,...,K,
kη f(T t)dt
ky kη −uˆ kη kL2 ≤ γ k,η ·e R(k−1)η − ky (k −1)η −uˆ (k −1)η kL2
L M √η
1
+ η(1+ x +ω(T))φ + φ + ν ψ , (20)
2 k
0 kL2 k,η
2
k,η
2
k,η k,η
p
where γ , φ and ψ are defined in (17), (15) and (16) respectively, and ω(T) is defined
k,η k,η k,η
in (32) and ν is given in (33) in Appendix A.
k,η
Proposition 11 provides the guarantees on how the errors due to both score estimations
and discretizations propagate as the number of iterates k increases. By iterating over
k = 1,2,...,K, we immediately get:
y uˆ E(f,g,K,η,M,L ), (21)
k Kη − Kη kL2 ≤ 1
where E(f,g,K,η,M,L ) is given in (14).
1
Since uˆ has the same distribution as u , we have
kη k
( (y ), (u )) y uˆ . (22)
W2 L Kη L K ≤ k Kη − Kη kL2
Finally, by the triangle inequality for 2-Wasserstein distance, we can decompose the
2-Wasserstein error in terms of the 2-Wasserstein error due to the initialization of the
algorithm at pˆ instead of p and the 2-Wasserstein error due to both score estimations
T T
and discretizations, we obtain:
( (u ),p ) ( (u ), (y ))+ ( (y ),p ), (23)
2 K 0 2 K Kη 2 Kη 0
W L ≤ W L L W L
where we used T = Kη.
Hence, we conclude that Theorem 4 follows by applying (18), (21), (22) and (23).
135. Conclusion and Future Work
Inthis paper,weprovidethefirstnon-asymptotic convergence analysis forageneral class of
probabilityflowODEsamplersin2-Wasserstein distance,assumingaccuratescoreestimates
and smooth log concave data distribution. Our analysis provides some insights about the
iteration complexity of deterministic ODE-based samplers for different choices of forward
SDEs in diffusion models.
The analysis of ODE-based samplers is still relatively limited and there are many open
questions. While our work focuses on the sampling phase of diffusion models, a significant
open problem is to investigate the training phase, i.e., understand when the score function
canbeaccurately learned,andcombinetheresultswiththeanalysisofsamplingtoestablish
end-to-end guarantees for diffusion models. We leave it to a future work.
Acknowledgements
Xuefeng Gao acknowledges support from the Hong Kong Research Grants Council [GRF
14201421, 14212522, 14200123]. Lingjiong Zhu is partially supported by the grants NSF
DMS-2053454, NSF DMS-2208303.
References
M. S. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic inter-
polants. In International Conference on Learning Representations, 2022.
M. S. Albergo, N. M. Boffi, and E. Vanden-Eijnden. Stochastic interpolants: A unifying
framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.
B. D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their
Applications, 12(3):313–326, 1982.
J. Benton, V. De Bortoli, A. Doucet, and G. Deligiannidis. Linear convergence bounds for
diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686, 2023a.
J.Benton, G.Deligiannidis, andA.Doucet. Errorboundsforflowmatchingmethods. arXiv
preprint arXiv:2305.16860, 2023b.
A. Block, Y. Mroueh, and A. Rakhlin. Generative modeling with denoising auto-encoders
and Langevin sampling. arXiv preprint arXiv:2002.00107, 2020.
P. Cattiaux, G. Conforti, I. Gentil, and C. L´eonard. Time reversal of diffusion processes
under a finite entropy condition. Annales de l’Institut Henri Poincar´e (B) Probabilit´es et
Statistiques, 59(4):1844–1881, 2023.
H. Chen, H. Lee, and J. Lu. Improved analysis of score-based generative modeling: User-
friendly bounds under minimal smoothness assumptions. In International Conference on
Machine Learning, volume 202, pages 4764–4803. PMLR, 2023a.
M. Chen, K. Huang, T. Zhao, and M. Wang. Score approximation, estimation and distri-
bution recovery of diffusion models on low-dimensional data. In Proceedings of the 40th
International Conference on Machine Learning, volume 202. PMLR, 2023b.
14S.Chen,S.Chewi,H.Lee, Y.Li, J.Lu,andA.Salim. TheprobabilityflowODEisprovably
fast. In Advances in Neural Information Processing Systems, 2023c.
S. Chen, S. Chewi, J. Li, Y. Li, A. Salim, and A. R. Zhang. Sampling is as easy as learning
the score: Theory for diffusion models with minimal data assumptions. In International
Conference on Learning Representations, 2023d.
S. Chen, G. Daras, and A. Dimakis. Restoration-degradation beyond linear diffusions:
A non-asymptotic analysis for DDIM-type samplers. In International Conference on
Machine Learning, pages 4462–4484. PMLR, 2023e.
X. Cheng, J. Lu, Y. Tan, and Y. Xie. Convergence of flow-based generative models via
proximal gradient descent in Wasserstein space. arXiv preprint arXiv:2310.17582, 2023.
A. S. Dalalyan and A. G. Karagulyan. User-friendly guarantees for the Langevin Monte
Carlo with inaccurate gradient. Stochastic Processes and their Applications, 129(12):
5278–5311, 2019.
V. De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis.
Transactions on Machine Learning Research, 11:1–42, 2022.
V. De Bortoli, J. Thornton, J. Heng, and A. Doucet. Diffusion Schro¨dinger bridge with
applications to score-based generative modeling. In Advances in Neural Information
Processing Systems, volume 34, pages 17695–17709, 2021.
X. Gao, H. M. Nguyen, and L. Zhu. Wasserstein convergence guarantees for a general class
of score-based generative models. arXiv preprint arXiv:2311.11003, 2023.
M. Gu¨rbu¨zbalaban,X. Gao, Y.Hu, andL. Zhu. Decentralized stochastic gradient Langevin
dynamicsandHamiltonian MonteCarlo. Journal of Machine Learning Research, 22:1–69,
2021.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a
two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural
Information Processing Systems, volume 30, 2017.
J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Advances in
Neural Information Processing Systems, volume 33, 2020.
A. Hyv¨arinen and P. Dayan. Estimation of non-normalized statistical models by score
matching. Journal of Machine Learning Research, 6(4):695–708, 2005.
T.Karras, M.Aittala, T.Aila, andS.Laine. Elucidatingthedesignspaceof diffusion-based
generative models. In Advances in Neural Information Processing Systems, volume 35,
2022.
H.Lee,J.Lu,andY.Tan.Convergenceforscore-basedgenerativemodelingwithpolynomial
complexity. In Advances in Neural Information Processing Systems, volume 35, 2022.
15H. Lee, J. Lu, and Y. Tan. Convergence of score-based generative modeling for general
data distributions. In International Conference on Algorithmic Learning Theory, pages
946–985. PMLR, 2023.
G. Li, Y. Wei, Y. Chen, and Y. Chi. Towards faster non-asymptotic convergence for
diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023.
C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. DPM-solver: A fast ODE solver
for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural
Information Processing Systems, volume 35, pages 5775–5787, 2022.
V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov. Grad-TTS: A diffusion
probabilisticmodelfortext-to-speech. InInternational Conference on Machine Learning,
pages 8599–8608. PMLR, 2021.
A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional
image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 10684–10695, 2022.
J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine
Learning, volume 37, pages 2256–2265. PMLR, 2015.
Y.SongandS.Ermon.Generativemodelingbyestimatinggradientsofthedatadistribution.
In Advances in Neural Information Processing Systems, volume 32, 2019.
Y. Song, S. Garg, J. Shi, and S. Ermon. Sliced score matching: A scalable approach to
density and score estimation. In Uncertainty in Artificial Intelligence, pages 574–584.
PMLR, 2020.
Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based
generativemodelingthroughstochasticdifferentialequations. InInternational Conference
on Learning Representations, 2021.
Y. Song, P. Dhariwal, M. Chen, and I. Sutskever. Consistency models. In Proceedings of
the 40th International Conference on Machine Learning, pages 32211–32252, 2023.
W. Tang and H. Zhao. Contractive diffusion probabilistic models. arXiv preprint
arXiv:2401.13115, 2024.
C. Villani. Optimal Transport: Old and New. Springer, Berlin, 2009.
P. Vincent. A connection between score matching and denoising autoencoders. Neural
Computation, 23(7):1661–1674, 2011.
L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, Y. Shao, W. Zhang, B. Cui, and
M.-H. Yang. Diffusion models: A comprehensive survey of methods and applications.
ACM Copmuting Surveys, 56(4):1–39, 2023.
16Q. Zhang and Y. Chen. Fast sampling of diffusion models with exponential integrator. In
International Conference on Learning Representations, 2023.
W. Zhao, L. Bai, Y. Rao, J. Zhou, and J. Lu. UniPC: A unified predictor-corrector frame-
work for fast samplingof diffusion models. InAdvances in Neural Information Processing
Systems, 2023.
17Convergence Analysis for General Probability Flow ODEs of
Diffusion Models in Wasserstein Distances
APPENDIX
The Appendix is organized as follows:
• In Appendix A, we summarize the notations given in Table 2 in the main paper, that
are used in presenting the main results.
• In Appendix B, we provide the proofs of the main results of the paper.
• We present some additional technical proofs in Appendix C.
• We providethederivation of resultsfor variousexamples inSection 3.3 andadditional
details in Appendix D.
Appendix A. Key Quantities
In this section, we provide the definitions for the key quantities given in Table 2, that play
a major role in presenting the main results in the main paper.
For any 0 t T, we define:
≤ ≤
m (g(t))2
0
µ(t) := , (24)
2 e −2 0tf(s)ds +m
0
0t e −2 stf(v)dv(g(s))2ds
R R
(cid:16) (g(tR))2 (cid:17)
m(t) := 2f(t), (25)
m1 0e −2 0tf(s)ds + 0t e −2 stf(v)dv(g(s))2ds −
R R
L(t) := min t e −2 stR f(v)dv(g(s))2ds −1 , e 0tf(s)ds 2 L
0
, (26)
( (cid:18)Z0 R (cid:19)
(cid:16)
R
(cid:17)
)
We also define:
η¯:= min η¯ ,η¯ , (27)
1 2
{ }
1(g(t))2
4
log(2) 1 e−2R0tf(s)ds+ te−2Rstf(v)dv(g(s))2ds
η¯ := min , min  m0 0 , (28)
1   max 0 ≤t ≤T f(t) 0 ≤t ≤T  

1 4(g(t))4(L(Rt))2 + L 21(g(t))2    

η¯ :=
min   e−2R m0tf 0(s)ds + 0t e −2   stf(v)dv(g(s))2ds
.
    
(29)
2 0 ≤t ≤T

R1 2(g(t)R )2 

For any k = 1,2,...,K and (k 1)η t kη, we define:
− ≤ ≤
1e− (t k−1)ηf(T −s)ds (g(T t))2 η
δ (T t) := 2 − (g(T t))4(L(T t))2,
k − m1 0e −2 0T−tf(sR )ds + 0T −t e −2 sT−tf(v)dv(g(s))2ds − 4 − −
R R (30)
R
18and finally, let us define:
θ(T):= sup e −1 2 0tm(T −s)dse − 0Tf(s)ds kx 0 kL2, (31)
0 t T R R
≤ ≤
t 1/2
ω(T) := sup e −2 0tf(s)ds kx
0
k2
L2
+d e −2 stf(v)dv(g(s))2ds , (32)
0 ≤t ≤T (cid:18) R Z0 R (cid:19)
and for any k = 1,2,...,K,
kη 1
ν := (θ(T)+ω(T)) f(T s)+ (g(T s))2L(T s) ds
k,η
− 2 − −
Z(k −1)η(cid:20) (cid:21)
kη 1
+(L T + logp (0) ) (g(T s))2ds. (33)
1 0
k∇ k 2 −
Z(k −1)η
Appendix B. Proofs of the Main Results
B.1 Proof of Theorem 4
To prove Theorem 4, we study the three sources of errors discussed in Section 2 for conver-
gence analysis: (1) the initialization of the algorithm at pˆ instead of p , (2) the estimation
T T
error of the score function, and (3) the discretization error of the continuous-time process
(7).
First, we study the error introduced due to the initialization at pˆ instead of p . Recall
T T
the probability flow ODE y given in (5):
t
1
dy = f(T t)y + (g(T t))2 logp (y ) dt, y pˆ . (34)
t t T t t 0 T
− 2 − ∇ − ∼
(cid:20) (cid:21)
As discussed in Section 2, the distribution of y differs from p , because y pˆ = p .
T 0 0 T T
∼ 6
The following result provides a bound on ( (y ),p ).
2 T 0
W L
Proposition 12 (Restatement of Proposition 10) Assume that p is m -strongly-log-
0 0
concave. Then, we have
W2( L(y T),p 0)
≤
e
−
0Tµ(t)dt
kx
0
kL2, (35)
R
where µ(t) is given in (24).
Notice that the term x in Proposition 12 is finite since Assumption 1 implies that
k
0 kL2
x p is L -integrable (see e.g. Lemma 11 in Gu¨rbu¨zbalaban et al. (2021)).
0 0 2
∼
The key idea of the proof of Proposition 12 is to observe that when p is strongly log-
0
concave, the term xlogp
T
t(x) is also strongly concave (see e.g. Gao et al. (2023)). This
∇ −
fact allows us to establish Proposition 12. The proof of Proposition 12 will be given in
Section B.1.2.
Now we consider the algorithm (9) with iterates (u ), and bound the errors due to
k
score estimations and discretizations together. For any k = 0,1,2,...,K, u has the same
k
distribution as uˆ , where uˆ is a continuous-time process with the dynamics:
kη t
1
duˆ = f(T t)uˆ + (g(T t))2s uˆ ,T t/η η dt, (36)
t t θ t/η η
− 2 − ⌊ ⌋ −⌊ ⌋
(cid:20) (cid:21)
(cid:0) (cid:1)
19with the initial distribution uˆ pˆ . We have the following result that provides an upper
0 T
∼
bound for y uˆ in terms of y uˆ . This result plays a key role in
k
kη
−
kη kL2
k
(k −1)η
−
(k −1)η kL2
the proof of Theorem 4.
Proposition 13 (Restatement of Proposition 11) Assume that p is m -strongly-log-
0 0
concave, i.e. logp is m -strongly convex and logp is L -Lipschitz. For any k =
0 0 0 0
− ∇
1,2,...,K,
kη L kη
y uˆ 1 δ (T t)dt+ 1 η (g(T t))2dt
k kη − kη kL2 ≤ −
Z(k −1)η
k − 2
Z(k −1)η
−
!
kη f(T t)dt
·e R(k−1)η − ky (k −1)η −uˆ (k −1)η kL2
+ L 21 η(1+ kx
0 kL2
+ω(T)) kη e tkηf(T −s)ds(g(T −t))2dt
Z(k 1)η R
−
+ M kη e tkηf(T −s)ds(g(T t))2dt
2 −
Z(k −1)η R
1/2
+√ην
k,η
kη 1 e tkηf(T −s)ds(g(T t))2L(T t) 2 dt ,
2 − −
Z(k −1)η(cid:20) R (cid:21) !
(37)
where δ (t), 0 t T, is defined in (30), ω(T) is defined in (32) and ν is given in (33).
k k,η
≤ ≤
We remark that the coefficient in front of the term y uˆ in (37) lies
(k −1)η − (k −1)η L2
in between zero and one. Indeed, we can see that assumption η η¯ := min(η¯ ,η¯ ) in
(cid:13) ≤ (cid:13) 1 2
Theorem 4 implies that η η¯ , where η¯ is defined in (28(cid:13)) which yields tha(cid:13)t
1 1
≤
1e−ηmax0≤t≤Tf(t)(g(t))2
2
1 e−2R0tf(s)ds+ te−2Rstf(v)dv(g(s))2ds
η min  m0 0 , (38)
≤ 0 t T   1(g(t))4(L(Rt))2 + L1(g(t))2  
≤ ≤   4 2  
 
 
and assumption η η¯:= min(η¯ ,η¯ ) in Theorem 4 implies that η  η¯ , where η¯ is defined
1 2 2 2
≤ ≤
in (29) which yields that
η min
m1 0e −2 0tf(s)ds + 0t e −2 stf(v)dv(g(s))2ds
, (39)
≤0 ≤t ≤T ( R 1 2e −ηmin0R≤t≤Tf(Rt)(g(t))2 )
anditfollowsfrom(38)-(39)andthedefinitionofδ (t)in(30)thatδ (T t) L1η(g(T t))2
j j − ≥ 2 −
for every j = 1,2,...,K and (j 1)η t jη and ηmax δ (t) < 1 for every
(j 1)η t jη j
− ≤ ≤ − ≤ ≤
j = 1,2,...,K such that for any j = 1,2,...,K,
jη L jη
0 1 δ (T t)dt+ 1 η (g(T t))2dt 1.
j
≤ − − 2 − ≤
Z(j −1)η Z(j −1)η
Now we are ready to prove Theorem 4.
20B.1.1 Proof of Theorem 4
Proof Since uˆ has the same distribution as u , by applying (37) recursively, we have
kη k
( (y ), (u ))
2 Kη K
W L L
y uˆ
≤ k Kη − Kη kL2
K K jη L jη
1 δ (T t)dt+ 1 η (g(T t))2dt
j
≤ − − 2 −
Xk=1j= Yk+1
Z(j −1)η Z(j −1)η !
·e kK ηηf(T −t)dt L 21 η(1+ kx
0 kL2
+ω(T)) kη e tkηf(T −s)ds(g(T −t))2dt
R Z(k −1)η R
+ M kη e tkηf(T −s)ds(g(T t))2dt
2 −
Z(k −1)η R
1/2
+√ην
k,η
kη 1 e tkηf(T −s)ds(g(T t))2L(T t) 2 dt .
2 − −
Z(k −1)η(cid:20) R (cid:21) ! !
Moreover, we recall that T = Kη and by triangle inequality for 2-Wasserstein distance,
( (u ),p ) ( (u ), (y ))+ ( (y ),p ).
2 K 0 2 K Kη 2 Kη 0
W L ≤ W L L W L
The proof is completed by applying Proposition 12.
B.1.2 Proof of Proposition 12
Proof We recall that
1
dx˜ = f(T t)x˜ + (g(T t))2 logp (x˜ ) dt, (40)
t t T t t
− 2 − ∇ −
(cid:20) (cid:21)
with the initial distribution x˜ p and
0 T
∼
1
dy = f(T t)y + (g(T t))2 logp (y ) dt,
t t T t t
− 2 − ∇ −
(cid:20) (cid:21)
with the initial distribution y pˆ .
0 T
∼
It is proved in Gao et al. (2023) that logp (x) is a(T t)-strongly-concave, where
T t
− −
1
a(T t) := . (41)
− m1 0e −2 0T−tf(s)ds + 0T −t e −2 sT−tf(v)dv(g(s))2ds
R R
R
Next, let us recall from (25) the definition of m(T t):
−
m(T t):= (g(T t))2a(T t) 2f(T t), 0 t T, (42)
− − − − − ≤ ≤
21where a(T t) is defined in (41). We can compute that
−
d x˜
t
y
t
2e 0tm(T −s)ds
k − k
R
=(cid:16) m(T t)e 0tm(T −s)ds x(cid:17) ˜
t
y
t
2dt+2e 0tm(T −s)ds x˜
t
y t,dx˜
t
dy
t
− k − k h − − i
= m(T t)eR 0tm(T −s)ds x˜
t
y
t
2dt+2eR 0tm(T −s)ds x˜
t
y t,f(T t)(x˜
t
y t) dt
− k − k h − − − i
R R
+2e 0tm(T −s)ds x˜
t
y t,1 (g(T t))2( logp
T
t(x˜ t) logp
T
t(y t)) dt
− 2 − ∇ − −∇ −
R (cid:28) (cid:29)
e 0tm(T −s)ds m(T t)+2f(T t) (g(T t))2a(T t) x˜
t
y
t
2dt
≤ − − − − − k − k
R
= 0.
(cid:0) (cid:1)
This implies that
x˜
t
y
t
2e 0tm(T −s)ds x˜
0
y
0
2, (43)
k − k ≤ k − k
R
so that
E x˜
T
y
T
2 e
−
0Tm(T −s)dsE x˜
0
y
0
2. (44)
k − k ≤ k − k
R
Consider a coupling of (x˜ ,y ) such that x˜ p , y pˆ and E x˜ y 2 = 2(p ,pˆ ).
0 0 0 ∼ T 0 ∼ T k 0 − 0 k W2 T T
Next, we recall that p is the distribution of x which has the expression (see (4))
T T
T
x
T
= e
−
0Tf(s)dsx
0+ e
−
sTf(v)dvg(s)dB
s, (45)
R Z0 R
and pˆ (see (3)) is the distribution of
T
T
e
−
sTf(v)dvg(s)dB
s. (46)
Z0 R
Therefore, it follows from (45) and (46) that
W2(p T,pˆ T)
≤
e
−
0Tf(s)ds
kx
0
kL2. (47)
R
By combining (44) with (47), we conclude that
2( (y ),p ) = ( (y ), (x˜ )) E x˜ y 2
W2 L T 0 W2 L T L T ≤ k T − T k
≤
e
−
0Tm(T −s)ds W22(p T,pˆ T)
R
≤
e
−
0Tm(s)dse −2 0Tf(s)ds kx
0
k2
L2
= e −2 0Tµ(t)dt kx
0
k2 L2,
R R R
where
m(t) m (g(t))2
0
µ(t) =f(t)+ = ,
2 2 e −2 0tf(s)ds +m
0
0t e −2 stf(v)dv(g(s))2ds
R R
(cid:16) (cid:17)
R
and we have used (42). The proof is complete.
22B.1.3 Proof of Proposition 13
We first state a key technical lemma, which will be used in the proof of Proposition 13.
Lemma 14 (Gao et al. (2023)) Suppose that Assumption 1holds. Then, xlogp
T
t(x)
∇ −
is L(T t)-Lipschitz in x, where L(T t) is given in (26).
− −
Now, we are ready to prove Proposition 13.
Proof First, we recall that for any (k 1)η t kη,
− ≤ ≤
t 1
y = y + f(T s)y + (g(T s))2 logp (y ) ds,
t (k −1)η
Z(k −1)η(cid:20)
− s 2 − ∇ T −s s
(cid:21)
t 1
uˆ = uˆ + f(T s)uˆ + (g(T s))2s uˆ ,T (k 1)η ds,
t (k 1)η s θ (k 1)η
− Z(k −1)η(cid:20) − 2 − − − − (cid:21)
(cid:0) (cid:1)
which implies that
y
kη
= e (k kη −1)ηf(T −t)dt y
(k 1)η
+ 1 kη e tkηf(T −s)ds(g(T t))2 logp
T
t(y t)dt,
R − 2 Z(k 1)η R − ∇ −
−
kη f(T t)dt
uˆ kη = e (k−1)η − uˆ (k 1)η
R −
+ 21 Z(kkη 1)ηe Rtkηf(T −s)ds(g(T −t))2s
θ
uˆ
(k
−1)η,T −(k −1)η dt.
− (cid:0) (cid:1)
It follows that
y uˆ
kη kη
−
kη f(T t)dt
= e (k−1)η − y (k 1)η uˆ (k 1)η
R − − −
+ kη (cid:0)1 e tkηf(T −s)ds(g(T(cid:1) t))2 logp
T
t(y
(k
1)η) logp
T t
uˆ
(k 1)η
dt
Z(k 1)η 2 R − ∇ − − −∇ − −
−
+ kη 1 e tkηf(T −s)ds(g(T t))2 logp(cid:0)
T
t(y t) logp
T
t(y
(k
1)η) dt(cid:0) (cid:1)(cid:1)
Z(k −1)η 2 R − ∇ − −∇ − −
+ kη 1 e tkηf(T −s)ds(g(T t)(cid:0) )2 (cid:1)
2 −
Z(k −1)η R
logp uˆ s uˆ ,T (k 1)η dt.
T t (k 1)η θ (k 1)η
· ∇ − − − − − −
(cid:0) (cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
23This implies that
y uˆ
k kη − kη kL2
kη f(T t)dt
e (k−1)η − y (k 1)η uˆ (k 1)η
≤ (cid:13) R − − −
(cid:13)
(cid:13) (cid:13) (cid:13) + kη (cid:0) 1 e tkηf(T −s)ds(g(T(cid:1) t))2
2 −
Z(k −1)η R
logp (y ) logp uˆ dt
T t (k 1)η T t (k 1)η
· ∇ − − −∇ − − (cid:13)
(cid:0) (cid:0) (cid:1)(cid:1)
(cid:13) (cid:13)L2
+ kη 1 e tkηf(T −s)ds(g(T t))2 logp
T
t(y t) logp
T
t(y
(k
1)η(cid:13) (cid:13)) dt
(cid:13)
(cid:13)
(cid:13)Z(k −1)η 2 R − (cid:0)∇ − −∇ − −
(cid:1)
(cid:13)
(cid:13) (cid:13)L2
(cid:13) (cid:13) + kη 1 e tkηf(T −s)ds(g(T t))2 (cid:13) (cid:13)
(cid:13) (cid:13)Z(k −1)η 2 R −
(cid:13)
(cid:13)
(cid:13) logp uˆ s uˆ ,T (k 1)η dt . (48)
T t (k 1)η θ (k 1)η
· ∇ − − − − − − (cid:13)
(cid:0) (cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
(cid:13) (cid:13)L2
(cid:13)
(cid:13)
Next, we provide upper bounds for the three terms in (48).
Bounding the first term in (48). We can compute that
kη f(T t)dt
e (k−1)η − y (k 1)η uˆ (k 1)η
(cid:13) R − − −
(cid:13)
(cid:13) (cid:0) (cid:1) 2
(cid:13) (cid:13)
=
e2+
R(k
kηZ −( 1k
)k η−η
f1 () Tη
−21 te
)dR
ttkη yf( (T
k
−−
1s )) ηds −(g u( ˆT
(k
−−
1)t η))2 2(cid:0)∇logp
T
−t(y
(k
−1)η) −∇logp
T −t
(cid:0)uˆ
(k −1)η
(cid:1)(cid:1)dt
(cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:13)
2
+ kη 1 e(cid:13) (cid:13) tkηf(T −s)ds(g(T (cid:13) (cid:13)t))2 logp
T
t(y
(k
1)η) logp
T t
uˆ
(k 1)η
dt
(cid:13) (cid:13)Z(k −1)η 2 R − ∇ − − −∇ − − (cid:13)
(cid:13)
(cid:13) (cid:0) (cid:0) (cid:1)(cid:1) (cid:13)
k(cid:13)η kη f(T t)dt (cid:13)
+2 (cid:13) e (k−1)η − y (k 1)η uˆ (k 1)η , (cid:13)
Z(k −1)η* R − − −
(cid:0) (cid:1)
1 e tkηf(T −s)ds(g(T t))2 logp
T
t(y
(k
1)η) logp
T t
uˆ
(k 1)η
dt.
2 R − ∇ − − −∇ − − +
(cid:0) (cid:0) (cid:1)(cid:1)
24WeknowfromGao et al.(2023)thatlogp (x)isa(T t)-strongly-concave, wherea(T t)
T t
− − −
is given in (41). Hence we have
kη f(T t)dt
e (k−1)η − y (k 1)η uˆ (k 1)η
(cid:13) R − − −
(cid:13)
(cid:13) (cid:0) (cid:1) 2
(cid:13) (cid:13) + kη 1 e tkηf(T −s)ds(g(T t))2 logp
T
t(y
(k
1)η) logp
T t
uˆ
(k 1)η
dt
Z(k −1)η 2 R − ∇ − − −∇ − − (cid:13)
(cid:13)
(cid:0) (cid:0) (cid:1)(cid:1) (cid:13)
kη 2 kη f(T t)dt 2 (cid:13)
1 m k(T t)dt e (k−1)η − y (k 1)η uˆ (k 1)η (cid:13)
≤ − Z(k −1)η − ! R
(cid:13)
− − −
(cid:13)
+ Z(kk −η
1)η
1 2e Rtkηf(T −s)ds(g(T −t))2L(cid:13) (T −t) (cid:13)y
(k −1)η
−(cid:13) uˆ
(k −1)η
(cid:13)dt !2
kη η kη (cid:13) (cid:13)
1 m (T t)dt+ (g(T t))4(L(T t))2dt
k
≤ − − 2 − −
Z(k −1)η Z(k −1)η !
2 kη f(T t)dt 2
e (k−1)η − y (k 1)η uˆ (k 1)η ,
· R − − −
(cid:13) (cid:13)
(cid:13) (cid:13)
where we applied Cauchy-Schwartz inequality and Lemma 14, and m (T t) is defined as:
k
−
m k(T t):= e− (t k−1)ηf(T −s)ds (g(T t))2a(T t), (k 1)η t kη, (49)
− − − − ≤ ≤
R
for every k = 1,2,...,K. Hence, we conclude that
kη f(T t)dt
e (k−1)η − y (k 1)η uˆ (k 1)η
(cid:13) R − − −
(cid:13)
(cid:13) (cid:0) (cid:1)
(cid:13) (cid:13) + kη 1 e tkηf(T −s)ds(g(T t))2 logp
T t
y
(k 1)η
logp
T t
uˆ
(k 1)η
dt
Z(k −1)η 2 R − (cid:0)∇ −
(cid:0)
− (cid:1)−∇ −
(cid:0)
−
(cid:1)(cid:1)
(cid:13)
(cid:13) (cid:13)L2
≤
1
−
Z(kk −η 1)ηδ k(T −t)dt !e R(k kη −1)ηf(T −t)dt (cid:13)y
(k −1)η
−uˆ
(k −1)η
(cid:13)L2, ((cid:13) (cid:13)50)
(cid:13) (cid:13)
where we used the inequality √1 x 1 x for any 0 x 1 and the definition of
− ≤ − 2 ≤ ≤
δ (T t) in (30) which can be rewritten as
k
−
δ k(T t):= 1 e− (t k−1)ηf(T −s)ds (g(T t))2a(T t) η (g(T t))4(L(T t))2, (k 1)η t kη,
− 2 − − −4 − − − ≤ ≤
R
where a(T t) is given in (41).
−
25Bounding the second term in (48). Using Lemma 14, we can compute that
2
kη 1 e tkηf(T −s)ds(g(T t))2 logp
T
t(y t) logp
T
t(y
(k
1)η) dt
(cid:13) (cid:13)Z(k −1)η 2 R − ∇ − −∇ − − (cid:13)
(cid:13)
(cid:13) (cid:0) 2 (cid:1) (cid:13)
(cid:13) (cid:13)
≤
Z(kkη
1)η
1 2e Rtkηf(T −s)ds(g(T −t))2L(T −t) ky
t
−y
(k −1)η
kdt
!
(cid:13) (cid:13)
−
η kη 1 e tkηf(T −s)ds(g(T t))2L(T t) 2 y
t
y
(k 1)η
2dt,
≤ Z(k −1)η(cid:20)2 R − − (cid:21) k − − k
which implies that
kη 1 e tkηf(T −s)ds(g(T t))2 logp
T
t(y t) logp
T t
y
(k 1)η
dt
(cid:13)
(cid:13)
(cid:13)Z(k −1)η 2 R − (cid:0)∇ − −∇ −
(cid:0)
−
(cid:1)(cid:1)
(cid:13)
(cid:13) (cid:13)L2
(cid:13) (cid:13) E η kη 1 e tkηf(T −s)ds(g(T t))2L(T t) 2 y
t
y
(k 1)η
2dt 1(cid:13) (cid:13)/2
≤ " Z(k −1)η(cid:20)2 R − − (cid:21) k − − k #!
1/2
≤
η Z(kk −η 1)η(cid:20)1 2e Rtkηf(T −s)ds(g(T −t))2L(T −t) (cid:21)2 dt
·
(k
−1s )ηu ≤p
t
≤kηE ky
t
−y
(k −1)η
k2
!
1/2
= √η Z(kk −η 1)η(cid:20)1 2e Rtkηf(T −s)ds(g(T −t))2L(T −t) (cid:21)2 dt
! (k
−1s )ηu ≤p
t ≤kη
(cid:13)y
t
−y
(k −1)η
(cid:13)L2.
(cid:13) (cid:13)(51)
Bounding the third term in (48). We notice that
kη 1 e tkηf(T −s)ds(g(T t))2 logp
T
t(uˆ
(k
1)η) s
θ
uˆ
(k
1)η,T (k 1)η dt
(cid:13)
(cid:13)
(cid:13)Z(k −1)η 2 R − (cid:0)∇ − − −
(cid:0)
− − −
(cid:1)(cid:1)
(cid:13)
(cid:13) (cid:13)L2
(cid:13) (cid:13) kη 1 e tkηf(T −s)ds(g(T t))2 (cid:13) (cid:13)
≤ (cid:13) (cid:13)Z(k −1)η 2 R −
(cid:13)
(cid:13)
(cid:13) logp (uˆ ) s uˆ ,T (k 1)η dt
T (k 1)η (k 1)η θ (k 1)η
· ∇ − − − − − − − (cid:13)
(cid:0) (cid:0) (cid:1)(cid:1)
(cid:13) (cid:13)L2
+ kη 1 e tkηf(T −s)ds(g(T t))2 (cid:13) (cid:13)
(cid:13) (cid:13)Z(k −1)η 2 R −
(cid:13)
(cid:13)
(cid:13) logp uˆ logp uˆ dt .
T t (k 1)η T (k 1)η (k 1)η
· ∇ − − −∇ − − − (cid:13)
(cid:0) (cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
(cid:13) (cid:13)L2
(cid:13)
(cid:13)
26By Assumption 3, we have
kη 1 e tkηf(T −s)ds(g(T t))2
(cid:13) (cid:13)Z(k −1)η 2 R −
(cid:13)
(cid:13)
(cid:13) logp (uˆ ) s uˆ ,T (k 1)η dt
T (k 1)η (k 1)η θ (k 1)η
· ∇ − − − − − − − (cid:13)
(cid:0) (cid:0) (cid:1)(cid:1)
(cid:13) (cid:13)L2
M kη e tkηf(T −s)ds(g(T t))2dt. (cid:13) (cid:13) (52)
≤ 2 −
Z(k −1)η R
Moreover, by Assumption 2, we have
kη 1 e tkηf(T −s)ds(g(T t))2 logp
T t
uˆ
(k 1)η
logp
T (k 1)η
uˆ
(k 1)η
dt
(cid:13)
(cid:13)
(cid:13)Z(k −1)η 2 R − (cid:0)∇ −
(cid:0)
− (cid:1)−∇ − −
(cid:0)
−
(cid:1)(cid:1)
(cid:13)
(cid:13) (cid:13)L2
(cid:13) (cid:13)
≤≤ ZZ (( kk
kk
−−
ηη
11 )) ηη
11
2
2e
eR
Rt tk kη ηf f( (T T− −s s) )d ds s( (g g( (T
T
−
−t t) )) )2
2L(cid:13)
(cid:13)
1∇
ηl (cid:16)o 1g +p T
(cid:13)−
uˆt
((cid:0)
kuˆ −( 1k
)−
η1 (cid:13)) Lη
2(cid:1) (cid:17)− d∇
t
logp T −(k −1)η (cid:0)uˆ (k −1)η
(cid:1)(cid:13)
(cid:13)L(cid:13) (cid:13) 2dt
≤
L 21 η (cid:16)1+ (cid:13)y
(k −1)η
−uˆ
(k −1)η (cid:13)L2
+ ky
(k
−1)(cid:13)
η kL2
(cid:17)Z(k(cid:13)k −η 1)ηe Rtkηf(T −s)ds(g(T −t))2dt. (53)
(cid:13) (cid:13)
Furthermore, we can compute that
y y x˜ + x˜ , (54)
(k −1)η L2 ≤ (k −1)η − (k −1)η L2 (k −1)η L2
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
where x˜ is defined(cid:13)in (2). M(cid:13)oreov(cid:13)er, by (43) in the p(cid:13)roof o(cid:13)f Propo(cid:13)sition 12, we have
t
y
(k −1)η
−x˜
(k −1)η L2 ≤
E kx˜
0
−y
0
k2 1/2 = e
−
R0Tf(s)ds kx
0 kL2 ≤
kx
0
kL2, (55)
(cid:13) (cid:13) (cid:0) (cid:1)
wherewe(cid:13)applied(4)toobta(cid:13)intheequalityintheaboveequation. Moreover,sincex˜ = x
t T t
−
in distribution for any t [0,T], we have
∈
x˜ = x sup x =: ω(T). (56)
(k −1)η L2 T −(k −1)η L2 ≤
0 t T
k t kL2
≤ ≤
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
Next, let us show that ω(T) can be computed as given by the formula in (32). By
equation (4), we have
d x
t
2e2 0tf(s)ds = 2f(t) x
t
2e2 0tf(s)dsdt
k k k k
R R
(cid:16) (cid:17) +2e2 0tf(s)ds x t,dx
t
+e2 0tf(s)ds d (g(t))2dt.
h i · ·
R R
By taking expectations, we obtain
d E x
t
2e2 0tf(s)ds = e2 0tf(s)ds d (g(t))2dt,
k k · ·
R R
(cid:16) (cid:17)
27which implies that
t
E x
t
2 = e −2 0tf(s)dsE x
0
2+d e −2 stf(v)dv(g(s))2ds. (57)
k k k k
R Z0 R
Therefore, we conclude that
t 1/2
ω(T) = sup kx
t kL2
= sup e −2 0tf(s)ds kx
0
k2
L2
+d e −2 stf(v)dv(g(s))2ds . (58)
0 ≤t ≤T 0 ≤t ≤T (cid:18) R Z0 R (cid:19)
Therefore, by applying (53), (54), (55) and (56), we have
kη 1 e tkηf(T −s)ds(g(T t))2 logp
T t
uˆ
(k 1)η
logp
T (k 1)η
uˆ
(k 1)η
dt
(cid:13)
(cid:13)
(cid:13)Z(k −1)η 2 R − (cid:0)∇ −
(cid:0)
− (cid:1)−∇ − −
(cid:0)
−
(cid:1)(cid:1)
(cid:13)
(cid:13) (cid:13)L2
(cid:13) (cid:13)
≤
L 21 η (cid:13)y
(k −1)η
−uˆ
(k −1)η (cid:13)L2
Z(kk −η 1)ηe Rtkηf(T −s)ds(g(T −t))2dt (cid:13) (cid:13)
(cid:13) + L 21 η(1+(cid:13) kx
0 kL2
+ω(T)) kη e tkηf(T −s)ds(g(T −t))2dt.
Z(k −1)η R
It follows that the third term in (48) is upper bounded by
L 21 η (cid:13)y
(k −1)η
−uˆ
(k −1)η (cid:13)L2
Z(kk −η 1)ηe Rtkηf(T −s)ds(g(T −t))2dt
(cid:13) + L 21 η(1+ kx
0
kL2(cid:13) +ω(T)) kη e tkηf(T −s)ds(g(T −t))2dt
Z(k 1)η R
−
+ M kη e tkηf(T −s)ds(g(T t))2dt. (59)
2 −
Z(k −1)η R
Bounding (48). On combining (50), (51) and (59), we conclude that
y uˆ 2
k kη − kη kL2
kη L kη
1 δ (T t)dt+ 1 η (g(T t))2dt
k
≤ − − 2 −
( Z(k −1)η Z(k −1)η !
kη f(T t)dt
·e R(k−1)η − y (k −1)η −uˆ (k −1)η L2
+ L 21 η(1+ kx
0 kL2
+ω(T)) kη e tkηf(T −s)d(cid:13) (cid:13)s(g(T −t))2dt (cid:13) (cid:13)
Z(k −1)η R
+ M kη e tkηf(T −s)ds(g(T t))2dt
2 −
Z(k 1)η R
−
1/2
+√η kη 1 e tkηf(T −s)ds(g(T t))2L(T t) 2 dt
2 − −
Z(k 1)η(cid:20) R (cid:21) !
−
2
sup y y . (60)
·
(k 1)η t
kηk
t
−
(k −1)η kL2
)
− ≤ ≤
28Weneedonemoreresult,whichprovidesanupperboundforsup y y .
(k −1)η ≤t ≤kη
k
t
−
(k −1)η kL2
The proof of Lemma 15 is given in Appendix C.1.
Lemma 15 For any k = 1,2,...,K,
sup y y
(k 1)η t kη
t − (k −1)η L2
− ≤ ≤ (cid:13) (cid:13)
(cid:13) kη (cid:13) 1
(θ(T)+ω(T)) f(T s)+ (g(T s))2L(T s) ds
≤ − 2 − −
Z(k −1)η(cid:20) (cid:21)
kη 1
+(L T + logp (0) ) (g(T s))2ds.
1 0
k∇ k 2 −
Z(k −1)η
where we recall from (31)-(32) that
θ(T):= sup e −1 2 0tm(T −s)dse − 0Tf(s)ds kx 0 kL2,
0 t T R R
≤ ≤
t 1/2
ω(T) := sup e −2 0tf(s)ds kx
0
k2
L2
+d e −2 stf(v)dv(g(s))2ds .
0 ≤t ≤T (cid:18) R Z0 R (cid:19)
By applying Lemma 15, we conclude from (60) that
y uˆ 2
k kη − kη kL2
kη L kη
1 δ (T t)dt+ 1 η (g(T t))2dt
k
≤ − − 2 −
( Z(k −1)η Z(k −1)η !
kη f(T t)dt
·e R(k−1)η − y (k −1)η −uˆ (k −1)η L2
+ L 21 η(1+ kx
0 kL2
+ω((cid:13) (cid:13)T))+ M
2
kη (cid:13) (cid:13)e tkηf(T −s)ds(g(T −t))2dt
(cid:18) (cid:19)Z(k −1)η R
1/2 2
+√ην
k,η
kη 1 e tkηf(T −s)ds(g(T t))2L(T t) 2 dt ,
2 − −
Z(k 1)η(cid:20) R (cid:21) ! )
−
where ν is defined in (33). The proof of Proposition 13 is hence complete.
k,η
29B.2 Proof of Proposition 9
Proof First of all, we have
RHS of (13)
K K jη
≥
e
−
0Kηµ(t)dt
kx
0 kL2
+ 1
−
δ j(T −t)dt
R Xk=1j= Yk+1 Z(j −1)η !
1/2
kη 1 2
√ην (g(T t))2L(T t) dt
k,η
· 2 − −
Z(k −1)η(cid:20) (cid:21) !
≥
e
−
0Kηµ(t)dt
kx
0 kL2
R
K K k
+ 1 η max max δ (T t) − ην min 1 (g(t))2L(t)
j k,η
− 1 j K(j 1)η t jη − · 0 t T 2
k=1(cid:18) ≤ ≤ − ≤ ≤ (cid:19) ≤ ≤ (cid:18) (cid:19)
X
K K k
≥
e
−
0Kηµ(t)dt kx
0 kL2
+ 1 −0m tax
T
µ(T −t) − ·ην
k,η
0m tin
T
1 2(g(t))2L(t)
R k=1(cid:18) ≤ ≤ (cid:19) ≤ ≤ (cid:18) (cid:19)
X
= e − R0Kηµ(t)dt kx 0 kL2 + 1 −(1 − maη xm
0
≤a tx ≤0 T≤ µt ≤ (T t)µ(t))K ·ν k,η 0m ≤ti ≤n
T
(cid:18)1 2(g(t))2L(t)
(cid:19)
≥
e − R0Kηµ(t)dt kx 0 kL2 + 1 − me − aK
x
0η ≤m
t
≤ax T0≤ µt (≤ tT )µ(t) ·ν k,η 0m ≤ti ≤n
T
(cid:18)1 2(g(t))2L(t) (cid:19),
where µ(t) is given in (24), δ (T t) is defined in (30) and the equality above is due to the
j
−
formula for the finite sum of a geometric series and we used the inequality that 1 x e x
−
− ≤
for any 0 x 1 to obtain the last inequality above.
≤ ≤
Next, by the definition of ν in (33), we have
k,η
T 1/2
ν
k,η
η√d e −2 sTf(v)dv(g(s))2ds min (g(t))2L(t).
≥ (cid:18)Z0 R (cid:19) 0 ≤t ≤T
Therefore, in order for RHS of (13) ǫ, we must have
≤
e
−
0Kηµ(t)dt
kx
0 kL2
≤
ǫ,
R
which implies that Kη as ǫ 0 and in particular
→ ∞ →
T = Kη = Ω(1), (61)
(since under our assumptions on f and g, µ(t) which is defined in (24) is positive and
t
continuous so that µ(s)ds is finite for any t (0, ) and strictly increasing from 0 to
0 ∈ ∞ ∞
as t increases from 0 to ), and we also need
R ∞
1 −e −Kηmax0≤t≤Tµ(t) η√d T e −2 sTf(v)dv(g(s))2ds 1/2 1 min (g(t))2L(t) 2 ǫ.
max 0 ≤t ≤T µ(t) · (cid:18)Z0 R (cid:19) · 2 (cid:18)0 ≤t ≤T (cid:19) ≤
(62)
30Note that max
0 t T
µ(t) = max
0 t
Kηµ(t). Together with e
−
0Kηµ(t)dt = (ǫ/√d) (since
≤ ≤ ≤ ≤ O ρ
x (√d) from (72)) and the assumption that max R µ(s) c t µ(s)ds +c
k 0 kL2 ≤O 0 ≤s ≤t ≤ 1 0 2
ρ
uniformlyintforsomec ,c ,ρ > 0,itiseasytoseethatmax µ(t)
=(cid:16)
R log
√(cid:17)
d/ǫ .
1 2 0 t Kη
≤ ≤ O
Moreover, since we assumed min (g(t))2L(t) > 0, we have µ(t) > 0 for a(cid:16)ny(cid:16)t. (cid:16)Togeth(cid:17)e(cid:17)r (cid:17)
t 0
≥
with T = Kη = Ω(1) from (61), we have max µ(t) Ω(1). Since Kη as ǫ 0,
0 t T
≤ ≤ ≥ → ∞ →
we have 1 −e −Kηmax0≤t≤Tµ(t) = Ω(1). Therefore, it follows from (62) that η =
O
√ǫ
d
,
where ignores the logarithmic dependence on ǫ and d and we used the assum(cid:16)ptio(cid:17)n
min
t
0(O g(t))2L(t) > 0 and liminf
T
0T e −2 sTf(v)dv(g(s))2ds > 0. Hence, we ce onclude
that≥ wee
have the following lower
bo→ un∞
d for
theR
complexity: K = Ω
√d
, where Ω ignores
R ǫ
the logarithmic dependence on ǫ and d. This completes the proof. (cid:16) (cid:17)
e e
Appendix C. Additional Technical Proofs
C.1 Proof of Lemma 15
Proof We can compute that for any (k 1)η t kη,
− ≤ ≤
t 1
y y = f(T s)y + (g(T s))2 logp (y ) ds,
t (k 1)η s T s s
− − Z(k −1)η(cid:20) − 2 − ∇ − (cid:21)
and moreover
t 1
x˜ x˜ = f(T s)x˜ + (g(T s))2 logp (x˜ ) ds, (63)
t (k 1)η s T s s
− − Z(k −1)η(cid:20) − 2 − ∇ − (cid:21)
so that
y y
t (k 1)η
− −
= x˜ x˜
t (k 1)η
− −
t 1
+ f(T s)(y x˜ )+ (g(T s))2( logp (y ) logp (x˜ )) ds,
s s T s s T s s
Z(k −1)η(cid:20) − − 2 − ∇ − −∇ − (cid:21)
and therefore
y y
t − (k −1)η L2
(cid:13) (cid:13) t 1
(cid:13) x˜ x˜ (cid:13) + f(T s)+ (g(T s))2L(T s) y x˜ ds.
≤
(cid:13)
t − (k −1)η (cid:13)L2
Z(k −1)η(cid:20)
− 2 − − (cid:21)k s − s kL2
We obta(cid:13)ined in the pr(cid:13)oof of Proposition 12 that
ky
t
−x˜
t kL2
≤
e −21 0tm(T −s)ds ky
0
−x˜
0
kL2,
R
and moreover, from the proof of Proposition 12, we have ky
0
−x˜
0 kL2
≤
e
−
0Tf(s)ds
kx
0 kL2
so that R
ky t −x˜ t kL2 ≤e −1 2 0tm(T −s)dse − 0Tf(s)ds kx 0 kL2.
R R
31Therefore, we have
y y
t − (k −1)η L2
(cid:13) (cid:13) kη 1
(cid:13) x˜ x˜ (cid:13) +θ(T) f(T s)+ (g(T s))2L(T s) ds, (64)
≤ t − (k −1)η L2
Z(k 1)η(cid:20)
− 2 − −
(cid:21)
(cid:13) (cid:13) −
(cid:13) (cid:13)
where θ(T) bounds sup y x˜ and it is given in (31).
0 ≤t ≤T k t − t kL2
Next, it follows from (63) that
t 1
x˜ x˜ f(T s) x˜ + (g(T s))2 logp (x˜ ) ds, (65)
(cid:13)
t − (k −1)η (cid:13)L2 ≤
Z(k −1)η(cid:20)
− k s kL2 2 − k∇ T −s s kL2
(cid:21)
(cid:13) (cid:13)
and we have
logp (x˜ ) logp (x˜ ) logp (0) + logp (0)
k∇ T −s s kL2 ≤ k∇ T −s s −∇ T −s kL2 k∇ T −s kL2
L(T s) x˜ + logp (0)
≤ − k s kL2 k∇ T −s k
L(T s) x˜ +L T + logp (0) , (66)
≤ − k s kL2 1 k∇ 0 k
where we applied Assumption 2 to obtain the last inequality above.
Therefore, we have
t 1
x˜ x˜ ω(T) f(T s)+ (g(T s))2L(t s) ds
(cid:13) (cid:13)
t − (k −1)η
(cid:13)
(cid:13)L2 ≤
Z(k −1)η(cid:20)
− 2 −
t 1
−
(cid:21)
+(L T + logp (0) ) (g(T s))2ds, (67)
1 0
k∇ k 2 −
Z(k −1)η
where we recall that x˜ has the same distribution as x and we also recall from (58)
s T s
−
that ω(T) = sup x with an explicit formula given in (32) (see the derivation
0 ≤t ≤T k t kL2
that leads to (58) in the proof of Proposition 13) Hence, we conclude that uniformly for
(k 1)η t kη,
− ≤ ≤
kη 1
y y (θ(T)+ω(T)) f(T s)+ (g(T s))2L(T s) ds
(cid:13)
t − (k −1)η (cid:13)L2 ≤
Z(k −1)η(cid:20)
− 2 − −
(cid:21)
(cid:13) (cid:13) kη 1
+(L T + logp (0) ) (g(T s))2ds.
1 0
k∇ k 2 −
Z(k 1)η
−
This completes the proof.
Appendix D. Derivation of Results in Section 3.3
In this section, we prove the results that are summarized in Table 3 in Section 3.3. We dis-
cuss variance explodingSDEs in AppendixD.1, variance preservingSDEs in AppendixD.2.
32D.1 Variance-Exploding SDEs
In this section, we consider variance-exploding SDEs with f(t) 0 in the forward process
≡
(1). We can immediately obtain the following corollary of Theorem 4.
Corollary 16 Assume that Assumptions 1, 2, and 3 hold and η η¯, where η¯> 0 is defined
≤
in (27). Then, we have
( (u ),p )
2 K 0
W L
≤
e
−
0Kηµ(t)dt
kx
0 kL2
R
K K jη L jη
+ 1 δ(T t)dt+ 1 η (g(T t))2dt
− − 2 −
Xk=1j= Yk+1
Z(j −1)η Z(j −1)η !
L T 1/2 kη
1 η 1+2 x +√d (g(t))2dt (g(T t))2dt
· 2 k
0 kL2
−
(cid:18)Z0 (cid:19) ! Z(k −1)η
1/2
M kη kη 1
+ (g(T t))2dt+√ην (g(T t))4(L(T t))2dt , (68)
k,η
2 − 4 − −
Z(k −1)η Z(k −1)η ! !
where for any 0 t T:
≤ ≤
1(g(T t))2 η
δ(T t):= 2 − (g(T t))4(L(T t))2, (69)
− 1 + T −t (g(s))2ds − 4 − −
m0 0
R
1
where L(T −t):= min 0T −t (g(s))2ds − ,L
0
and
(cid:18) (cid:19)
(cid:16) (cid:17)
R
T 1/2 kη 1
ν := 2 x +√d (g(t))2dt (g(T s))2L(T s)ds
k,η
k
0 kL2
2 − −
(cid:18)Z0 (cid:19) ! Z(k 1)η
−
kη 1
+(L T + logp (0) ) (g(T s))2ds, (70)
1 0
k∇ k 2 −
Z(k −1)η
where µ(t) is defined as:
1(g(t))2
µ(t):= 2 . (71)
1 + t (g(s))2ds
m0 0
R
Theterm x inCorollary16hassquarerootdependenceonthedimensiond. Indeed,
k
0 kL2
by Lemma 11 in Gu¨rbu¨zbalaban et al. (2021), we have
x 2d/m + x , (72)
k
0 kL2
≤
0
k ∗k
p
where x is the unique minimizer of logp .
0
∗ −
In the next few sections, we consider special functions g in Corollary 16 and derive the
corresponding results in Table 3.
33D.1.1 Example: f(t) 0 and g(t) = aebt
≡
When g(t) = aebt for some a,b > 0, we can obtain the following result from Corollary 16.
Corollary 17 (Restatement of Corollary 5) Let g(t) = aebt for some a,b > 0. Then,
we have
( (u ),p ) (ǫ),
2 K 0
W L ≤ O
after
d3/2log(d/ǫ)
K =
O ǫ3
!
iterations, provided that M
ǫ2
and η
ǫ3
.
≤ √d ≤ d3/2
Proof Let g(t) = aebt for some a,b > 0. First, we can compute that
(g(t))2 2be2bt a2
(g(t))2L(t)= min ,L (g(t))2 = min ,L (e2bt 1)2 .
t (g(s))2ds 0 ! e2bt 1 0 4b2 −
0 (cid:18) − (cid:19)
R
If e2bt 2, then e2bt 1 1e2bt and (g(t))2L(t) 4b. On the other hand, if e2bt < 2, then
≥ − ≥ 2 ≤
(g(t))2L(t) L a2 . Therefore, for any 0 t T,
≤
04b2
≤ ≤
L a2
(g(t))2L(t) max 4b, 0 .
≤ 4b2
(cid:18) (cid:19)
By the definition of µ(t) in (71), we can compute that
1m (g(t))2 1m a2e2bt
µ(t) = 2 0 = 2 0 . (73)
1+m t (g(s))2ds 1+m a2(e2bt 1)
0 0 02b −
R
This implies that
t 1 t 2bm a2e2bsds 1 2b m a2+m a2e2bt
0 0 0
µ(s)ds = = log − . (74)
2 2b m a2+m a2e2bs 2 2b
Z0 Z0 − 0 0 (cid:18) (cid:19)
By letting t = T = Kη in (74) and using (72), we obtain
e
−
R0Kηµ(t)dt
kx
0 kL2
≤ 2b m
0a√ 22 +b
m 0a2e2bKη
2d/m 0+ kx
∗k
.
− (cid:16)p (cid:17)
p
Moreover,
a 1/2 L a2 η
ν 2 2d/m +2 x +√d e2bT 1 max 4b, 0
k,η ≤ 0 k ∗k √2b − 4b2 2
(cid:18) (cid:19) (cid:18) (cid:19)
p (cid:16) a2 (cid:17)
+(L T + logp (0) ) e2b(T (k 1)η) e2b(T kη) , (75)
1 0 − − −
k∇ k 4b −
(cid:16) (cid:17)
34and for any 0 t T:
≤ ≤
jη L jη
0 1 δ(T t)dt+ 1 η (g(T t))2dt 1,
≤ − − 2 − ≤
Z(j −1)η Z(j −1)η
where δ() is defined in (69), provided that the condition η η¯ (where η¯ is defined in (27))
· ≤
holds, that is:
1(g(t))2
1 +2 t(g(s))2ds 1 + t (g(s))2ds
η min min  m0 0 , min m0 0 , (76)
≤   0 ≤t ≤T   1 4(g(t))4(L(Rt))2 + L 21(g(t))2 

0 ≤t ≤T ( 1 2R(g(t))2 ) 

   
   
   
which holds provided that
1a2e2bt
2
1 +a2 (e2bt 1) 1 + a2 (e2bt 1)
η min min  m0 2b − , min m0 2b − . (77)
≤   0 ≤t ≤T   1 4max 16b2,L 162 0 ba 44 + L 21a2e2bt 

0 ≤t ≤T ( 1 2a2e2bt ) 

(cid:16) (cid:17)
   
   
   
Since 1 x e x for any 0 x 1, we conclude that
−
− ≤ ≤ ≤
K jη L jη
1 δ(T t)dt+ 1 η (g(T t))2dt
− − 2 −
j= Yk+1
Z(j −1)η Z(j −1)η !
K
jη δ(T t)dt+L1η jη (g(T t))2dt Kηδ(T t)dt+L1η Kη(g(T t))2dt
e− (j−1)η − 2 (j−1)η − = e− kη − 2 kη − ,
≤
R R R R
j=k+1
Y
where δ() is defined in (69). Moreover,
·
Kη Kη 1m a2e2b(T t)dt 1 L2a4
δ(T t)dt 2 0 − (K k)η2max 16b2, 0
Zkη − ≥ Zkη 1+m 0a 22 b(e2b(T −t) −1) − 4 − (cid:18) 16b4 (cid:19)
1 2b m a2+m a2e2b(T kη) 1 L2a4
= log − 0 0 − (K k)η2max 16b2, 0 ,
2 2b m a2+m a2e2b(T Kη) − 4 − 16b4
− 0 0 − ! (cid:18) (cid:19)
and
L Kη L a2
1 η (g(T t))2dt = 1 η e2b(K k)η 1 .
−
2 − 2 2b −
Zkη
(cid:16) (cid:17)
35By applying Corollary 16 with T = Kη, we conclude that
( (u ),p )
2 K 0
W L
√2b 2d/m + x
0
k ∗k
≤ 2b (cid:16)pm a2+m a2e2bK(cid:17)η
0 0
−
pK √2b (K k)η21max 16b2,L2 0a4 +L1ηa2 (e2b(K−k)η 1)
+ e − 4 16b4 2 2b −
(cid:18) (cid:19)
2b m a2+m a2e2b(K k)η
Xk=1 − 0 0 −
pM L a
+ 1 η 1+2 2d/m + x +√d (e2bKη 1)1/2
0
· 2 2 k ∗k √2b −
(cid:18) (cid:18) (cid:19)(cid:19)
(cid:16)p (cid:17)
a2
e2b(K k+1)η e2b(K k)η
− −
· 2b −
η L(cid:16) a2 (cid:17)
0
+ max 4b,
2 4b2
(cid:18) (cid:19)
a 1/2 L a2 η
2 2d/m +2 x +√d e2bT 1 max 4b, 0
· 0 k ∗k √2b − 4b2 2
(cid:18) (cid:19) (cid:18) (cid:19)
p (cid:16) (cid:17)
a2
+(L T + logp (0) ) e2b(T (k 1)η) e2b(T kη) .
1 0 − − −
k∇ k 4b −
!!
(cid:16) (cid:17)
By the mean-value theorem, we have
e2b(K (k 1))η) e2b(K k)η 2be2b(K (k 1))ηη,
− − − − −
− ≤
which implies that
( (u ),p )
2 K 0
W L
√d 1Kη2max 16b2,L2 0a4 +L1ηa2 e2bKη
+ e4 16b4 2 2b
≤ O ebKη O (cid:18) (cid:19)
!
K
1
M +L η√debKη e2b(K (k 1))ηη+ηebKηη√d
1 − −
·
Xk=1
eb(K −k)η ·
(cid:16) (cid:17)
+η2e2b(K (k 1))η +e2b(K (k 1))ηη2(Kη)
− − − −
!!
√d
≤ O ebKη
!
Kη2max
16b2,L2 0a4
+ e 16b4 M +L η√debKη ebKη +Kη2√d+η(Kη)ebKη
(cid:18) (cid:19) 1
O ·
!!
(cid:16) (cid:17)
(ǫ),
≤ O
and (77) holds such that the condition η η¯ (where η¯ is defined in (27)) holds provided
≤
that
log(√d/ǫ) ǫ2 ǫ3
Kη = , M , η ,
b ≤ √d ≤ d3/2
36d3/2log(d/ǫ)
which implies that K . This completes the proof.
≥ O ǫ3
(cid:16) (cid:17)
D.1.2 Example: f(t) 0 and g(t) = (b+at)c
≡
Karras et al. (2022) considers f(t) 0,g(t) = √2t with non-uniform discretization time
≡
steps, where the time steps are defined according to a polynomial noise schedule. Inspired
by Karras et al. (2022), we next consider g(t) = (b+at)c for some a,b,c > 0, we can obtain
the following result from Corollary 16.
Corollary 18 (Restatement of Corollary 6) Let g(t) = (b + at)c for some a,b > 0,
c 1/2. Then, we have
≥
( (u ),p ) (ǫ)
2 K 0
W L ≤ O
after
1 +3
d(2c+1) 2
K =
O ǫ2c2 +1+3 !
iterations provided that M
ǫ2
and η
ǫ3
.
≤ √d ≤ d3 2
Proof When g(t) = (b+at)c for some a,b,c > 0, we can compute that
(b+at)c
(g(t))2L(t) = min ,L (b+at)2c . (78)
1 ((b+at)2c+1 b2c+1) 0
!
a(2c+1) −
If t b, then
≥ a
(b+at)c (b+at)c a(2c+1)
. (79)
1 ((b+at)2c+1 b2c+1) ≤ 1 (1 1 )(b+at)2c+1 ≤ (1 1 )b
a(2c+1) − a(2c+1) − 22c+1 − 22c+1
If t b, then
≤ a
L (b+at)2c L (2b)2c. (80)
0 0
≤
Therefore, it follows from (78), (79) and (80) that
a(2c+1)
(g(t))2L(t) max ,L (2b)2c .
≤ (1
−
22c1 +1)b 0 !
By (71), we have
e
−
0Kηµ(t)dt
2d/m
0
+ x =
2d/m 0 + kx
∗k .
R k ∗k 1+ m0p((b+aKη)2c+1 b2c+1)
(cid:16)p (cid:17) a(2c+1) −
q
37Furthermore,
(b+aT)2c+1 b2c+1 1/2
ν 2 2d/m + x +√d −
k,η 0
≤ k ∗k a(2c+1)
!
(cid:18) (cid:19)
(cid:16)p (cid:17)
1 a(2c+1)
max ,L (2b)2c η
· 2 (1
−
22c1 +1)b 0 !
1(b+a(T (k 1)η))2c+1 (b+a(T kη))2c+1
+(L T + logp (0) ) − − − − , (81)
1 0
k∇ k 2 a(2c+1)
and for any 0 t T:
≤ ≤
1(b+at)2c
δ(t) = 2
1 + 1 ((b+at)2c+1 b2c+1)
m0 a(2c+1) −
1 (b+at)2c
ηmin ,L2(b+at)4c ,
− 4 a2(2c1 +1)2((b+at)2c+1 −b2c+1)2 0
!
(where δ() is defined in (69)) satisfies
·
jη L jη
0 1 δ(T t)dt+ 1 η (g(T t))2dt 1,
≤ − − 2 − ≤
Z(j 1)η Z(j 1)η
− −
provided that the condition η η¯ (where η¯ is defined in (27)) holds.
≤
Since 1 x e x for any 0 x 1, we conclude that
−
− ≤ ≤ ≤
K jη L jη
1 δ(T t)dt+ 1 η (g(T t))2dt
− − 2 −
j= Yk+1
Z(j −1)η Z(j −1)η !
K
jη δ(T t)dt+L1η jη (g(T t))2dt Kηδ(T t)dt+L1η Kη(g(T t))2dt
e− (j−1)η − 2 (j−1)η − = e− kη − 2 kη − ,
≤
R R R R
j=k+1
Y
where δ() is defined in (69). Moreover,
·
Kη 1 m
δ(T t)dt log 1+ 0 (b+a(K k)η)2c+1 b2c+1
− ≥ 2 a(2c+1) − −
Zkη (cid:18) (cid:19)
(cid:0) (cid:1)
1 a2(2c+1)2
(K k)η2max ,L2(2b)4c ,
− 4 − (1
−
22c1 +1)2b2 0 !
and we can compute that
1 Kη 1 L η
L η (g(T t))2dt = 1 (b+a(K k)η)2c+1 b2c+1 .
1
2 − 2a(2c+1) − −
Zkη
(cid:0) (cid:1)
38By applying Corollary 16 with T = Kη and (72), we conclude that
( (u ),p )
2 K 0
W L
2d/m + x
0
k ∗k
≤ 1+ m0p((b+aKη)2c+1 b2c+1)
a(2c+1) −
q
1(K k)η2max a2(2c+1)2 ,L2(2b)4c +1 L1η ((b+a(K k)η)2c+1 b2c+1)
K e4 − (1− 22c1 +1)2b2 0 ! 2a(2c+1) − −
+
1+ m0 ((b+a(K k)η)2c+1 b2c+1)
Xk=1 a(2c+1) − −
L q (b+aKη)2c+1 b2c+1 1/2 M
1 η 1+2 2d/m + x +√d − +
0
· 2 k ∗k a(2c+1)
!
2
!
(cid:18) (cid:19)
(cid:16)p (cid:17)
(b+a(Kη (k 1)η))2c+1 (b+a(Kη kη))2c+1
− − − −
· a(2c+1)
1 a(2c+1)
+ ηmax ,L (2b)2c
2 (1
−
22c1 +1)b 0 !
(b+aKη)2c+1 b2c+1 1/2
2 2d/m + x +√d −
0
· k ∗k a(2c+1)
!
(cid:18) (cid:19)
(cid:16)p (cid:17)
1 a(2c+1)
max ,L (2b)2c η
· 2 (1
−
22c1 +1)b 0 !
1(b+a(Kη (k 1)η))2c+1 (b+a(Kη kη))2c+1
+(L Kη+ logp (0) ) − − − − .
1 0
k∇ k 2 a(2c+1)
!!
This implies that
( (u ),p )
2 K 0
W L
√d K 1
+e
((Kη)η+(Kη)2c+1η)
O
≤ O (Kη)2c 2+1
Xk=1
((K −k)η)2c 2+1
√d(Kη)2c 2+1 L 1η+M ((K k)η)2cη+η η√d(Kη)2c 2+1 +(Kη)η((K k)η)2c
· − −
!
(cid:16)(cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
√d
+e
((Kη)η+(Kη)2c+1η)
≤ O (Kη)2c 2+1 O
√d(Kη)2c 2+1 L 1η+M (Kη)c+ 21 +η Kη√d+(Kη)(Kη)c+1 2
·
!
(cid:16)(cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
(ǫ),
≤ O
and the condition η η¯ (where η¯ is defined in (27)) holds provided that
≤
1
d(2c+1) ǫ2 ǫ3
Kη = , M , η ,
ǫ2c2
+1
≤ √d ≤ d3
2
39so that K
d(2c1 +1)+3
2
. This completes the proof.
≥ O ǫ2c2 +1+3
(cid:18) (cid:19)
D.2 Variance-Preserving SDEs
In this section, we consider Variance-Preserving SDEs with f(t)= 1β(t) and g(t) = β(t)
2
in the forward process (1), where β(t) is often chosen as some non-decreasing function in
p
practice. We can obtain the following corollary of Theorem 4.
Corollary 19 Under the assumptions of Theorem 4, we have
( (u ),p )
2 K 0
W L
x
k
0 kL2
≤
m 0e
0Kηβ(s)ds+1
m
0
−
qK eR kK ηη 21β(Kη −t)dt+ kK ηηηmax(1,L2 0)(β(Kη −t))2dt+1 2L1η kK ηηβ(Kη −t)dt
+
Xk=1
R
m 0e
0(K−R
k)ηβ(s)ds
+1 m
0
1 2e−1 2ηmax0≤tR ≤Tβ(t)
−
R
(cid:16) (cid:17)
·
L 21 η 1+ kx
0 kL2
+ kx
0
k2
L2
+d 1/2 + M
2
2 e (k kη −1)η 21β(T −v)dv −1
(cid:18) (cid:19) (cid:18) R (cid:19)
(cid:16) (cid:17)
(cid:0) (cid:1)
1/2
kη β(T v)dv
+√ηmax(1,L 0) max β(Kη t) e (k−1)η − 1
(k −1)η ≤t ≤kη − (cid:18) R − (cid:19)
1 T (k 1)η
x + x 2 +d 1/2 +max(1,L ) − − β(s)ds
· k 0 kL2 k 0 kL2 2 0
(cid:16)
(cid:17)(cid:18) (cid:19)ZT −kη
(cid:0) (cid:1)
T (k 1)η 1
− −
+(L T + logp (0) ) β(s)ds . (82)
1 0
k∇ k 2
ZT −kη !!
Proof We apply Theorem 4 applied to the variance-preserving SDE (f(t) = 1β(t) and
2
g(t) = β(t)). First, we can compute that
p
L(T t)= min T −t e −2 sT−tf(v)dv(g(s))2ds −1 , e 0T−tf(s)ds 2 L
0
−
(cid:18)Z0 R (cid:19)
(cid:16)
R
(cid:17)
!
= min
1 e
−
01
T−tβ(s)ds,e
R0T−tβ(s)dsL
0
!.
−
R
If e R0T−tβ(s)ds
≥
2, then
1
e−R0T1
−tβ(s)ds ≤
2 and otherwise e R0T−tβ(s)dsL
0 ≤
2L 0. Therefore,
for any 0 t T, −
≤ ≤
L(T t) 2max(1,L ).
0
− ≤
40By applying Theorem 4, we have
W2( L(u K),p 0)
≤
e
−
0Kηµ(t)dt
kx
0 kL2
R
K K jη L jη
+ 1 δ (T t)dt+ 1 η (g(T t))2dt
j
− − 2 −
Xk=1j= Yk+1
Z(j −1)η Z(j −1)η !
·e kK ηηf(T −t)dt L 21 η(1+ kx
0 kL2
+ω(T)) kη e tkηf(T −s)ds(g(T −t))2dt
R Z(k −1)η R
+ M kη e tkηf(T −s)ds(g(T t))2dt
2 −
Z(k 1)η R
−
1/2
+√ην
k,η
kη 1 e tkηf(T −s)ds(g(T t))2L(T t) 2 dt ,
2 − −
Z(k −1)η(cid:20) R (cid:21) ! !
where
kη 1
ν := (θ(T)+ω(T)) f(T s)+ (g(T s))2L(T s) ds
k,η
− 2 − −
Z(k −1)η(cid:20) (cid:21)
kη 1
+(L T + logp (0) ) (g(T s))2ds,
1 0
k∇ k 2 −
Z(k −1)η
where
θ(T)= e
−
0T 1 2β(s)ds kx
0 kL2
≤
kx
0
kL2,
R
and
t 1/2
ω(T) = sup e −2 0tf(s)ds kx
0
k2
L2
+d e −2 stf(v)dv(g(s))2ds
0 ≤t ≤T(cid:18) R Z0 R (cid:19)
= sup e
−
0tβ(s)ds kx
0
k2
L2
+d 1 −e
−
0tβ(s)ds 1/2
≤
kx
0
k2
L2
+d 1/2 .
0 t T R R
≤ ≤ (cid:16) (cid:16) (cid:17)(cid:17)
(cid:0) (cid:1)
We can compute that
kη kη
e tkηf(T −s)ds(g(T t))2dt = e skη 1 2β(T −v)dvβ(T s)ds
− −
Z(k −1)η R Z(k −1)η R
kη 1β(T v)dv
= 2 e (k−1)η 2 − 1 .
−
(cid:18) R (cid:19)
41Furthermore, we have
1 kη
ν x + x 2 +d 1/2 +max(1,L ) β(T s)ds
k,η ≤ k 0 kL2 k 0 kL2 2 0 −
(cid:16)
(cid:17)(cid:18) (cid:19)Z(k −1)η
(cid:0) (cid:1)
T (k 1)η 1
− −
+(L T + logp (0) ) β(s)ds
1 0
k∇ k 2
ZT −kη
1 T (k 1)η
= x + x 2 +d 1/2 +max(1,L ) − − β(s)ds
k 0 kL2 k 0 kL2 2 0
(cid:16)
(cid:17)(cid:18) (cid:19)ZT −kη
(cid:0) (cid:1) T (k 1)η 1
− −
+(L T + logp (0) ) β(s)ds.
1 0
k∇ k 2
ZT −kη
Next, for VP-SDE, we have f(t)= 1β(t) and g(t) = β(t) so that we can compute:
2
p
1m β(t) 1m β(t)
µ(t)= 2 0 = 2 0 . (83)
e
−
0tβ(s)ds
+m
0
0t
e
−
stβ(v)dvβ(s)ds
e
−
0tβ(s)ds
+m 0(1 −e
−
0tβ(s)ds)
R R R R
R
It follows that
T
µ(t)dt =
21 R0Tβ(s)ds
m
+(m 10dx
m )e x
=
1
2
log m 0e
0Tβ(s)ds+1
−m
0
. (84)
Z0 Z0 0 − 0 − (cid:16) R (cid:17)
Hence, we obtain
e − R0Tµ(t)dt kx 0 kL2 =
m 0e
0Tk βx (s0 )k dL s2
+1 m
0. (85)
−
q R
Under the condition η η¯ (where η¯ is defined in (27)),
≤
jη L jη
0 1 δ (T t)dt+ 1 η (g(T t))2dt 1.
j
≤ − − 2 − ≤
Z(j −1)η Z(j −1)η
Since 1 x e x for any 0 x 1, we conclude that
−
− ≤ ≤ ≤
K jη L jη
1 δ (T t)dt+ 1 η (g(T t))2dt
j
− − 2 −
j= Yk+1
Z(j −1)η Z(j −1)η !
K
e−
(j jη −1)ηδj(T −t)dt+L 21η (j jη −1)η(g(T −t))2dt
≤
R R
j=k+1
Y
= e−
K
j=k+1
(j jη −1)ηδj(T −t)dt+L 21η kK ηη(g(T −t))2dt
,
P R R
42where
K jη
δ (T t)dt
j
−
j=
Xk+1Z(j −1)η
K jη 1e− (t j−1)ηf(T −s)ds (g(T t))2
= 2 − dt
j=
Xk+1Z(j −1)η m1 0e −2 0T−tf(sR )ds + 0T −t e −2 sT−tf(v)dv(g(s))2ds
R R
RK jη η
(g(T t))4(L(T t))2dt
− 4 − −
j=
Xk+1Z(j −1)η
K jη 1e−21 (t j−1)ηβ(T −s)ds β(T t)
= 2 − dt
j= Xk+1Z(j −1)η m1 0e − 0T−tβ(s)R ds + 0T −t e − sT−tβ(v)dvβ(s)ds
R R
R Kη η
(β(T t))2(L(T t))2dt
− 4 − −
Zkη
K jη 1e−1 2 (t j−1)ηβ(T −s)ds β(T t) Kη η
= 2 − dt (β(T t))2(L(T t))2dt.
j= Xk+1Z(j −1)η m1
0
R −1 e − 0T−tβ(s)ds +1 − Zkη 4 − −
R
(cid:16) (cid:17)
We can further compute that
K jη 1e−1 2 (t j−1)ηβ(T −s)ds β(T t)
2 − dt
R
j= Xk+1Z(j −1)η m1
0
−1 e − 0T−tβ(s)ds+1
R
(cid:16) (cid:17)Kη 1β(T t)
e −1 2ηmax0≤t≤Tβ(t)dt 2 − dt
≥ Zkη m1
0
−1 e − 0T−tβ(s)ds +1
R
= 1 e −1 2ηmax0≤t≤Tβ(t)dtlog (cid:16) m 0e 0(K−(cid:17) k)ηβ(s)ds +1 m
0
.
2 −
R
(cid:16) (cid:17)
Moreover, we can compute that
1/2
kη 1 e tkηf(T −s)ds(g(T t))2L(T t) 2 dt
2 − −
Z(k −1)η(cid:20) R (cid:21) !
1/2
kη
max(1,L 0) e tkηβ(T −v)dv(β(Kη t))2dt
≤ −
Z(k −1)η R !
1/2
kη
max(1,L 0) max β(Kη t) e tkηβ(T −v)dvβ(Kη t)dt
≤ (k −1)η ≤t ≤kη − Z(k −1)η R − !
1/2
kη β(T v)dv
= max(1,L 0) max β(Kη t) e (k−1)η − 1 .
(k −1)η ≤t ≤kη − (cid:18) R − (cid:19)
43By using T = Kη, we conclude that
( (u ),p )
2 K 0
W L
x
k
0 kL2
≤
m 0e
0Kηβ(s)ds+1
m
0
−
qK eR kK ηη 21β(Kη −t)dt+ kK ηηηmax(1,L2 0)(β(Kη −t))2dt+1 2L1η kK ηηβ(Kη −t)dt
+
Xk=1
R
m 0e
0(K−R
k)ηβ(s)ds +1 m
0
1 2e−1 2ηmax0≤tR ≤Tβ(t)
−
R
(cid:16) (cid:17)
·
L 21 η 1+ kx
0 kL2
+ kx
0
k2
L2
+d 1/2 + M
2
2 e (k kη −1)η 1 2β(T −v)dv −1
(cid:18) (cid:19) (cid:18) R (cid:19)
(cid:16) (cid:17)
(cid:0) (cid:1)
1/2
kη β(T v)dv
+√ηmax(1,L 0) max β(Kη t) e (k−1)η − 1
(k −1)η ≤t ≤kη − (cid:18) R − (cid:19)
1 T (k 1)η
x + x 2 +d 1/2 +max(1,L ) − − β(s)ds
· k 0 kL2 k 0 kL2 2 0
(cid:16)
(cid:17)(cid:18) (cid:19)ZT −kη
(cid:0) (cid:1)
T (k 1)η 1
− −
+(L T + logp (0) ) β(s)ds . (86)
1 0
k∇ k 2
ZT −kη !!
This completes the proof.
D.2.1 Example: β(t) b
≡
We consider thespecial case β(t) b forsome b > 0. Thisincludes thespecial case β(t) 2
≡ ≡
that is studied in Chen et al. (2023) Chen et al. (2023c).
Corollary 20 (Restatement of Corollary 7) Assume β(t) b. Then, we have
≡
( (u ),p ) (ǫ),
2 K 0
W L ≤ O
after
√d d 2
K = log
O ǫ ǫ
!
(cid:18) (cid:18) (cid:19)(cid:19)
iterations provided that M ǫ and η ǫ .
≤ log(√d/ǫ) ≤ √dlog(√d/ǫ)
44Proof When β(t) b for some b > 0, by Corollary 19 and (72), we can compute that
≡
( (u ),p )
2 K 0
W L
x
k
0 kL2
≤ m ebKη +1 m
0 0
−
pK e(K −k)η1 2b+(K −k)η2max(1,L2 0)b2+ 21L1η2(K −k)b
+
k=1 m eb(K k)η +1 m
21e−1 2ηb
X 0 − 0
−
·
L 21 η(cid:0) 1+ kx
0 kL2
+ kx
0
k2 L2(cid:1) +d 1/2 + M
2
2 e1 2bη −1
(cid:18) (cid:19)
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:0) (cid:1)
+√ηmax(1,L )b ebη 1 1/2 x + x 2 +d 1/2 1 +max(1,L ) bη
0 − · k 0 kL2 k 0 kL2 2 0
(cid:18) (cid:19)
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:0) (cid:1)
1
+(L T + logp (0) ) bη ,
1 0
k∇ k 2
!!
which implies that
W2( L(u K),p 0)
≤ O
e21√ bKd
η
+eKη2max(1,L2 0)b2+1 2L1η2Kb ·e21bKη (cid:16)e21ηb −1
(cid:17)
K M +L η√d η+η2 √d+Kη
1
·
!!
(cid:16) (cid:17) (cid:16) (cid:17)
(ǫ),
≤ O
and the condition η η¯ (where η¯ is defined in (27)) holds provided that
≤
2 √d ǫ
Kη = log , M η√d, η ,
b ǫ
!
≤ ≤ √d(Kη)
which implies that
ǫ ǫ
M , η
≤ log(√d/ǫ) ≤ √dlog(√d/ǫ)
and K
√d(log(d))2
. This completes the proof.
≥ O ǫ ǫ
(cid:16) (cid:17)
D.2.2 Example: β(t) = (b+at)ρ
We consider the special case β(t) = (b+at)ρ. This includes the special case β(t) = b+at
when ρ = 1 that is studied in Ho et al. (2020). Then we can obtain the following result
from Corollary 19.
Corollary 21 (Restatement of Corollary 8) Assume β(t) = (b+at)ρ. Then, we have
( (u ),p ) (ǫ),
2 K 0
W L ≤ O
45after
ρ+2
√d d ρ+1
K = log
O ǫ ǫ
!
(cid:18) (cid:18) (cid:19)(cid:19)
iterations provided that M ǫ and η ǫ .
≤ log(√d/ǫ) ≤ √dlog(√d/ǫ)
Proof When β(t) = (b+at)ρ, by Corollary 19 and (72), we can compute that
( (u ),p )
2 K 0
W L
2d/m + x
0
k ∗k
≤ m 0ea(ρ1 +1p )((b+aKη)ρ+1 −bρ+1) +1 m
0
−
q K 1+L1η ((b+a(K k)η)ρ+1 bρ+1)+ηmax(1,L2) 1 ((b+a(K k)η)2ρ+1 b2ρ+1)
e2a(ρ+1) − − 0 a(2ρ+1) − −
+
Xk=1 ea(ρ1 +1)((b+a(K −k)η)ρ+1 −bρ+1) +1 m
0
1 2e−1 2η(b+aKη)ρ
−
(cid:16) (cid:17)
M L
+ 1 η 1+2 2d/m + x +√d
0
· 2 2 k ∗k
(cid:18) (cid:19)
(cid:16) (cid:16)p (cid:17) (cid:17)
((b+a(K−k+1)η)ρ+1−(b+a(K−k)η)ρ+1)
1
2 e2 a(ρ+1) 1
· −
!
1
((b+a(K−k+1)η)ρ+1−(b+a(K−k)η)ρ+1) 1/2
+√η +max(1,L 0) e a(ρ+1) 1
2 · −
!
(cid:18) (cid:19)
1
2 2d/m + x +√d +max(1,L )
0 0
· k ∗k 2
(cid:18) (cid:19)
(cid:16) (cid:16)p (cid:17) (cid:17)
(b+a(K k+1)η)ρ+1 (b+a(K k)η)ρ+1
− − −
· a(ρ+1)
(cid:0) (cid:1)
1 (b+a(K k+1)η)ρ+1 (b+a(K k)η)ρ+1
+(L Kη+ logp (0) ) − − − .
1 0
k∇ k 2 a(ρ+1)
(cid:0) (cid:1)!!
46We can compute that
( (u ),p )
2 K 0
W L
√d
≤ O e2a(ρ1 +1)(b+aKη)ρ+1
K 1+L1η ((b+a(K k)η)ρ+1 bρ+1)+ηmax(1,L2) 1 (b+aKη)2ρ+1
e2a(ρ+1) − − 0 a(2ρ+1)
+
Xk=1 m 0ea(ρ1 +1)((b+a(K −k)η)ρ+1 −bρ+1) +1 m
0
1 2e−1 2η(b+aKη)ρ
−
(cid:16) (cid:17)
M +L 1η 1+2 2d/m
0
+ x +√d e21(b+a(K −k+1)η)ρη 1
· k ∗k −
(cid:16) (cid:16) (cid:16)p (cid:17) (cid:17)(cid:17)(cid:16) (cid:17)
1/2
+√η e(b+a(K −k+1)η)ρη 1 √d+Kη ((K k+1)η)ρη
− · −
!!
(cid:16) (cid:17) (cid:16)(cid:16) (cid:17) (cid:17)
√d ηmax(1,L2) 1 (b+aKη)2ρ+1 1 (b+aKη)ρ+1 e1 2η(b+aKη)ρ 1
+e 0 a(2ρ+1) e2a(ρ+1) −
≤ O e2a(ρ1 +1)(b+aKη)ρ+1 · (cid:16) (cid:17)
K M +L η√d (Kη)ρη+η2(Kη)2ρ √d+Kη (ǫ),
1
· ≤ O
!!
(cid:16) (cid:17) (cid:16) (cid:17)
and the condition η η¯ (where η¯ is defined in (27)) holds provided that
≤
1
(2a(ρ+1))ρ+1 1 b ǫ
Kη = log √d/ǫ ρ+1 , M η√d, η ,
a − a ≤ ≤ √d(Kη)ρ+1
(cid:16) (cid:16) (cid:17)(cid:17)
which implies that
ǫ ǫ
M , η
≤ log(√d/ǫ) ≤ √dlog(√d/ǫ)
and K
√d(log(d))ρ ρ+ +2
1 . This completes the proof.
≥ O ǫ ǫ
(cid:16) (cid:17)
47