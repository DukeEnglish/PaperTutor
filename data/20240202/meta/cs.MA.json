[
    {
        "title": "Game susceptibility, Correlation and Payoff capacity as a measure of Cooperative behavior in the thermodynamic limit of some Social dilemmas",
        "authors": "Rajdeep TahColin Benjamin",
        "links": "http://arxiv.org/abs/2401.18065v1",
        "entry_id": "http://arxiv.org/abs/2401.18065v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18065v1",
        "summary": "Analytically, finding the origins of cooperative behavior in infinite-player\ngames is an exciting topic of current interest. Previously, cooperative\nbehavior has been studied by considering game magnetization and individual\nplayer's average payoff as indicators. This paper shows that game\nsusceptibility, correlation, and payoff capacity can aid in understanding\ncooperative behavior in social dilemmas in the thermodynamic limit. In this\npaper, we compare three analytical methods, i.e., Nash equilibrium mapping\n(NEM), Darwinian selection (DS), and Aggregate selection (AS), with a\nnumerical-based method (ABM) via the game susceptibility, correlation, and\npayoff capacity as indicators of cooperative behavior. AS and DS fail compared\nto NEM and ABM by giving incorrect results for the indicators in question. The\nresults obtained via NEM and ABM are in good agreement for all three indicators\nin question, for both Hawk-Dove and the Public goods games. After comparing the\nresults obtained for all five indicators, we see that individual players'\naverage payoff and payoff capacity are the best indicators to study cooperative\nbehavior among players in the thermodynamic limit. This paper finds that NEM\nand ABM, along with the selected indicators, offer valuable insights into\ncooperative behavior in infinite-player games, contributing to understanding\nsocial dilemmas in the thermodynamic limit.",
        "updated": "2024-01-31 18:41:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18065v1"
    },
    {
        "title": "Distributed fixed-point algorithms for dynamic convex optimization over decentralized and unbalanced wireless networks",
        "authors": "Navneet AgrawalRenato L. G. CavalcanteSlawomir Stanczak",
        "links": "http://arxiv.org/abs/2401.18030v1",
        "entry_id": "http://arxiv.org/abs/2401.18030v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18030v1",
        "summary": "We consider problems where agents in a network seek a common quantity,\nmeasured independently and periodically by each agent through a local\ntime-varying process. Numerous solvers addressing such problems have been\ndeveloped in the past, featuring various adaptations of the local processing\nand the consensus step. However, existing solvers still lack support for\nadvanced techniques, such as superiorization and over-the-air function\ncomputation (OTA-C). To address this limitation, we introduce a comprehensive\nframework for the analysis of distributed algorithms by characterizing them\nusing the quasi-Fej\\'er type algorithms and an extensive communication model.\nUnder weak assumptions, we prove almost sure convergence of the algorithm to a\ncommon estimate for all agents. Moreover, we develop a specific class of\nalgorithms within this framework to tackle distributed optimization problems\nwith time-varying objectives, and, assuming that a time-invariant solution\nexists, prove its convergence to a solution. We also present a novel OTA-C\nprotocol for consensus step in large decentralized networks, reducing\ncommunication overhead and enhancing network autonomy as compared to the\nexisting protocols. The effectiveness of the algorithm, featuring\nsuperiorization and OTA-C, is demonstrated in a real-world application of\ndistributed supervised learning over time-varying wireless networks,\nhighlighting its low-latency and energy-efficiency compared to standard\napproaches.",
        "updated": "2024-01-31 17:49:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18030v1"
    },
    {
        "title": "Graph Attention-based Reinforcement Learning for Trajectory Design and Resource Assignment in Multi-UAV Assisted Communication",
        "authors": "Zikai FengDi WuMengxing HuangChau Yuen",
        "links": "http://arxiv.org/abs/2401.17880v1",
        "entry_id": "http://arxiv.org/abs/2401.17880v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17880v1",
        "summary": "In the multiple unmanned aerial vehicle (UAV)- assisted downlink\ncommunication, it is challenging for UAV base stations (UAV BSs) to realize\ntrajectory design and resource assignment in unknown environments. The\ncooperation and competition between UAV BSs in the communication network leads\nto a Markov game problem. Multi-agent reinforcement learning is a significant\nsolution for the above decision-making. However, there are still many common\nissues, such as the instability of the system and low utilization of historical\ndata, that limit its application. In this paper, a novel graph-attention\nmulti-agent trust region (GA-MATR) reinforcement learning framework is proposed\nto solve the multi-UAV assisted communication problem. Graph recurrent network\nis introduced to process and analyze complex topology of the communication\nnetwork, so as to extract useful information and patterns from observational\ninformation. The attention mechanism provides additional weighting for conveyed\ninformation, so that the critic network can accurately evaluate the value of\nbehavior for UAV BSs. This provides more reliable feedback signals and helps\nthe actor network update the strategy more effectively. Ablation simulations\nindicate that the proposed approach attains improved convergence over the\nbaselines. UAV BSs learn the optimal communication strategies to achieve their\nmaximum cumulative rewards. Additionally, multi-agent trust region method with\nmonotonic convergence provides an estimated Nash equilibrium for the multi-UAV\nassisted communication Markov game.",
        "updated": "2024-01-31 14:37:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17880v1"
    },
    {
        "title": "Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method",
        "authors": "Elissa MhannaMohamad Assaad",
        "links": "http://arxiv.org/abs/2401.17460v1",
        "entry_id": "http://arxiv.org/abs/2401.17460v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17460v1",
        "summary": "Federated learning (FL) is a novel approach to machine learning that allows\nmultiple edge devices to collaboratively train a model without disclosing their\nraw data. However, several challenges hinder the practical implementation of\nthis approach, especially when devices and the server communicate over wireless\nchannels, as it suffers from communication and computation bottlenecks in this\ncase. By utilizing a communication-efficient framework, we propose a novel\nzero-order (ZO) method with a one-point gradient estimator that harnesses the\nnature of the wireless communication channel without requiring the knowledge of\nthe channel state coefficient. It is the first method that includes the\nwireless channel in the learning algorithm itself instead of wasting resources\nto analyze it and remove its impact. The two main difficulties of this work are\nthat in FL, the objective function is usually not convex, which makes the\nextension of FL to ZO methods challenging, and that including the impact of\nwireless channels requires extra attention. However, we overcome these\ndifficulties and comprehensively analyze the proposed zero-order federated\nlearning (ZOFL) framework. We establish its convergence theoretically, and we\nprove a convergence rate of $O(\\frac{1}{\\sqrt[3]{K}})$ in the nonconvex\nsetting. We further demonstrate the potential of our algorithm with\nexperimental results, taking into account independent and identically\ndistributed (IID) and non-IID device data distributions.",
        "updated": "2024-01-30 21:46:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17460v1"
    },
    {
        "title": "Liquid Democracy for Low-Cost Ensemble Pruning",
        "authors": "Ben ArmstrongKate Larson",
        "links": "http://arxiv.org/abs/2401.17443v1",
        "entry_id": "http://arxiv.org/abs/2401.17443v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17443v1",
        "summary": "We argue that there is a strong connection between ensemble learning and a\ndelegative voting paradigm -- liquid democracy -- that can be leveraged to\nreduce ensemble training costs. We present an incremental training procedure\nthat identifies and removes redundant classifiers from an ensemble via\ndelegation mechanisms inspired by liquid democracy. Through both analysis and\nextensive experiments we show that this process greatly reduces the\ncomputational cost of training compared to training a full ensemble. By\ncarefully selecting the underlying delegation mechanism, weight centralization\nin the classifier population is avoided, leading to higher accuracy than some\nboosting methods. Furthermore, this work serves as an exemplar of how\nframeworks from computational social choice literature can be applied to\nproblems in nontraditional domains.",
        "updated": "2024-01-30 21:11:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17443v1"
    }
]