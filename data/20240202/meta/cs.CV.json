[
    {
        "title": "Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators",
        "authors": "Daniel GengAndrew Owens",
        "links": "http://arxiv.org/abs/2401.18085v1",
        "entry_id": "http://arxiv.org/abs/2401.18085v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18085v1",
        "summary": "Diffusion models are capable of generating impressive images conditioned on\ntext descriptions, and extensions of these models allow users to edit images at\na relatively coarse scale. However, the ability to precisely edit the layout,\nposition, pose, and shape of objects in images with diffusion models is still\ndifficult. To this end, we propose motion guidance, a zero-shot technique that\nallows a user to specify dense, complex motion fields that indicate where each\npixel in an image should move. Motion guidance works by steering the diffusion\nsampling process with the gradients through an off-the-shelf optical flow\nnetwork. Specifically, we design a guidance loss that encourages the sample to\nhave the desired motion, as estimated by a flow network, while also being\nvisually similar to the source image. By simultaneously sampling from a\ndiffusion model and guiding the sample to have low guidance loss, we can obtain\na motion-edited image. We demonstrate that our technique works on complex\nmotions and produces high quality edits of real and generated images.",
        "updated": "2024-01-31 18:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18085v1"
    },
    {
        "title": "Binding Touch to Everything: Learning Unified Multimodal Tactile Representations",
        "authors": "Fengyu YangChao FengZiyang ChenHyoungseob ParkDaniel WangYiming DouZiyao ZengXien ChenRit GangopadhyayAndrew OwensAlex Wong",
        "links": "http://arxiv.org/abs/2401.18084v1",
        "entry_id": "http://arxiv.org/abs/2401.18084v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18084v1",
        "summary": "The ability to associate touch with other modalities has huge implications\nfor humans and computational systems. However, multimodal learning with touch\nremains challenging due to the expensive data collection process and\nnon-standardized sensor outputs. We introduce UniTouch, a unified tactile model\nfor vision-based touch sensors connected to multiple modalities, including\nvision, language, and sound. We achieve this by aligning our UniTouch\nembeddings to pretrained image embeddings already associated with a variety of\nother modalities. We further propose learnable sensor-specific tokens, allowing\nthe model to learn from a set of heterogeneous tactile sensors, all at the same\ntime. UniTouch is capable of conducting various touch sensing tasks in the\nzero-shot setting, from robot grasping prediction to touch image question\nanswering. To the best of our knowledge, UniTouch is the first to demonstrate\nsuch capabilities. Project page: https://cfeng16.github.io/UniTouch/",
        "updated": "2024-01-31 18:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18084v1"
    },
    {
        "title": "Improved Scene Landmark Detection for Camera Localization",
        "authors": "Tien DoSudipta N. Sinha",
        "links": "http://arxiv.org/abs/2401.18083v1",
        "entry_id": "http://arxiv.org/abs/2401.18083v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18083v1",
        "summary": "Camera localization methods based on retrieval, local feature matching, and\n3D structure-based pose estimation are accurate but require high storage, are\nslow, and are not privacy-preserving. A method based on scene landmark\ndetection (SLD) was recently proposed to address these limitations. It involves\ntraining a convolutional neural network (CNN) to detect a few predetermined,\nsalient, scene-specific 3D points or landmarks and computing camera pose from\nthe associated 2D-3D correspondences. Although SLD outperformed existing\nlearning-based approaches, it was notably less accurate than 3D structure-based\nmethods. In this paper, we show that the accuracy gap was due to insufficient\nmodel capacity and noisy labels during training. To mitigate the capacity\nissue, we propose to split the landmarks into subgroups and train a separate\nnetwork for each subgroup. To generate better training labels, we propose using\ndense reconstructions to estimate visibility of scene landmarks. Finally, we\npresent a compact architecture to improve memory efficiency. Accuracy wise, our\napproach is on par with state of the art structure based methods on the\nINDOOR-6 dataset but runs significantly faster and uses less storage. Code and\nmodels can be found at https://github.com/microsoft/SceneLandmarkLocalization.",
        "updated": "2024-01-31 18:59:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18083v1"
    },
    {
        "title": "CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting",
        "authors": "Jiezhi YangKhushi DesaiCharles PackerHarshil BhatiaNicholas RhinehartRowan McAllisterJoseph Gonzalez",
        "links": "http://arxiv.org/abs/2401.18075v1",
        "entry_id": "http://arxiv.org/abs/2401.18075v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18075v1",
        "summary": "We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene\nForecasting, a method for predicting future 3D scenes given past observations,\nsuch as 2D ego-centric images. Our method maps an image to a distribution over\nplausible 3D latent scene configurations using a probabilistic encoder, and\npredicts the evolution of the hypothesized scenes through time. Our latent\nscene representation conditions a global Neural Radiance Field (NeRF) to\nrepresent a 3D scene model, which enables explainable predictions and\nstraightforward downstream applications. This approach extends beyond previous\nneural rendering work by considering complex scenarios of uncertainty in\nenvironmental states and dynamics. We employ a two-stage training of\nPose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we\nauto-regressively predict latent scene representations as a partially\nobservable Markov decision process, utilizing a mixture density network. We\ndemonstrate the utility of our method in realistic scenarios using the CARLA\ndriving simulator, where CARFF can be used to enable efficient trajectory and\ncontingency planning in complex multi-agent autonomous driving scenarios\ninvolving visual occlusions.",
        "updated": "2024-01-31 18:56:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18075v1"
    },
    {
        "title": "Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition",
        "authors": "Wei WeiTom De SchepperKevin Mets",
        "links": "http://arxiv.org/abs/2401.18054v1",
        "entry_id": "http://arxiv.org/abs/2401.18054v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18054v1",
        "summary": "Continual learning (CL) is the research field that aims to build machine\nlearning models that can accumulate knowledge continuously over different tasks\nwithout retraining from scratch. Previous studies have shown that pre-training\ngraph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)\nafter fine-tuning, a setting which is closely related to CL. Thus, we focus on\nstudying GNN in the continual graph learning (CGL) setting. We propose the\nfirst continual graph learning benchmark for spatio-temporal graphs and use it\nto benchmark well-known CGL methods in this novel setting. The benchmark is\nbased on the N-UCLA and NTU-RGB+D datasets for skeleton-based action\nrecognition. Beyond benchmarking for standard performance metrics, we study the\nclass and task-order sensitivity of CGL methods, i.e., the impact of learning\norder on each class/task's performance, and the architectural sensitivity of\nCGL methods with backbone GNN at various widths and depths. We reveal that\ntask-order robust methods can still be class-order sensitive and observe\nresults that contradict previous empirical observations on architectural\nsensitivity in CL.",
        "updated": "2024-01-31 18:20:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18054v1"
    }
]