[
    {
        "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
        "authors": "Coleman HooperSehoon KimHiva MohammadzadehMichael W. MahoneyYakun Sophia ShaoKurt KeutzerAmir Gholami",
        "links": "http://arxiv.org/abs/2401.18079v1",
        "entry_id": "http://arxiv.org/abs/2401.18079v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18079v1",
        "summary": "LLMs are seeing growing use for applications such as document analysis and\nsummarization which require large context windows, and with these large context\nwindows KV cache activations surface as the dominant contributor to memory\nconsumption during inference. Quantization is a promising approach for\ncompressing KV cache activations; however, existing solutions fail to represent\nactivations accurately in ultra-low precisions, such as sub-4-bit. In this\nwork, we present KVQuant, which addresses this problem by incorporating novel\nmethods for quantizing cached KV activations, including: (i) Per-Channel Key\nQuantization, where we adjust the dimension along which we quantize the Key\nactivations to better match the distribution; (ii) Pre-RoPE Key Quantization,\nwhere we quantize Key activations before the rotary positional embedding to\nmitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization,\nwhere we derive per-layer sensitivity-weighted non-uniform datatypes that\nbetter represent the distributions; (iv) Per-Vector Dense-and-Sparse\nQuantization, where we isolate outliers separately for each vector to minimize\nskews in quantization ranges; and (v) Q-Norm, where we normalize quantization\ncentroids in order to mitigate distribution shift, providing additional\nbenefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2,\nand Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit\nquantization on both Wikitext-2 and C4, outperforming existing approaches. Our\nmethod enables serving the LLaMA-7B model with a context length of up to 1\nmillion on a single A100-80GB GPU and up to 10 million on an 8-GPU system.",
        "updated": "2024-01-31 18:58:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18079v1"
    },
    {
        "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
        "authors": "Andreas OpedalAlessandro StolfoHaruki ShirakamiYing JiaoRyan CotterellBernhard SchölkopfAbulhair SaparovMrinmaya Sachan",
        "links": "http://arxiv.org/abs/2401.18070v1",
        "entry_id": "http://arxiv.org/abs/2401.18070v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18070v1",
        "summary": "There is increasing interest in employing large language models (LLMs) as\ncognitive models. For such purposes, it is central to understand which\ncognitive properties are well-modeled by LLMs, and which are not. In this work,\nwe study the biases of LLMs in relation to those known in children when solving\narithmetic word problems. Surveying the learning science literature, we posit\nthat the problem-solving process can be split into three distinct steps: text\ncomprehension, solution planning and solution execution. We construct tests for\neach one in order to understand which parts of this process can be faithfully\nmodeled by current state-of-the-art LLMs. We generate a novel set of word\nproblems for each of these tests, using a neuro-symbolic method that enables\nfine-grained control over the problem features. We find evidence that LLMs,\nwith and without instruction-tuning, exhibit human-like biases in both the\ntext-comprehension and the solution-planning steps of the solving process, but\nnot during the final step which relies on the problem's arithmetic expressions\n(solution execution).",
        "updated": "2024-01-31 18:48:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18070v1"
    },
    {
        "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
        "authors": "Parth SarthiSalman AbdullahAditi TuliShubh KhannaAnna GoldieChristopher D. Manning",
        "links": "http://arxiv.org/abs/2401.18059v1",
        "entry_id": "http://arxiv.org/abs/2401.18059v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18059v1",
        "summary": "Retrieval-augmented language models can better adapt to changes in world\nstate and incorporate long-tail knowledge. However, most existing methods\nretrieve only short contiguous chunks from a retrieval corpus, limiting\nholistic understanding of the overall document context. We introduce the novel\napproach of recursively embedding, clustering, and summarizing chunks of text,\nconstructing a tree with differing levels of summarization from the bottom up.\nAt inference time, our RAPTOR model retrieves from this tree, integrating\ninformation across lengthy documents at different levels of abstraction.\nControlled experiments show that retrieval with recursive summaries offers\nsignificant improvements over traditional retrieval-augmented LMs on several\ntasks. On question-answering tasks that involve complex, multi-step reasoning,\nwe show state-of-the-art results; for example, by coupling RAPTOR retrieval\nwith the use of GPT-4, we can improve the best performance on the QuALITY\nbenchmark by 20% in absolute accuracy.",
        "updated": "2024-01-31 18:30:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18059v1"
    },
    {
        "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
        "authors": "Yushi BaiXin LvJiajie ZhangYuze HeJi QiLei HouJie TangYuxiao DongJuanzi Li",
        "links": "http://arxiv.org/abs/2401.18058v1",
        "entry_id": "http://arxiv.org/abs/2401.18058v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18058v1",
        "summary": "Extending large language models to effectively handle long contexts requires\ninstruction fine-tuning on input sequences of similar length. To address this,\nwe present LongAlign -- a recipe of the instruction data, training, and\nevaluation for long context alignment. First, we construct a long\ninstruction-following dataset using Self-Instruct. To ensure the data\ndiversity, it covers a broad range of tasks from various long context sources.\nSecond, we adopt the packing and sorted batching strategies to speed up\nsupervised fine-tuning on data with varied length distributions. Additionally,\nwe develop a loss weighting method to balance the contribution to the loss\nacross different sequences during packing training. Third, we introduce the\nLongBench-Chat benchmark for evaluating instruction-following capabilities on\nqueries of 10k-100k in length. Experiments show that LongAlign outperforms\nexisting recipes for LLMs in long context tasks by up to 30\\%, while also\nmaintaining their proficiency in handling short, generic tasks. The code, data,\nand long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.",
        "updated": "2024-01-31 18:29:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18058v1"
    },
    {
        "title": "Rank Supervised Contrastive Learning for Time Series Classification",
        "authors": "Qianying RenDongsheng LuoDongjin Song",
        "links": "http://arxiv.org/abs/2401.18057v1",
        "entry_id": "http://arxiv.org/abs/2401.18057v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18057v1",
        "summary": "Recently, various contrastive learning techniques have been developed to\ncategorize time series data and exhibit promising performance. A general\nparadigm is to utilize appropriate augmentations and construct feasible\npositive samples such that the encoder can yield robust and discriminative\nrepresentations by mapping similar data points closer together in the feature\nspace while pushing dissimilar data points farther apart. Despite its efficacy,\nthe fine-grained relative similarity (e.g., rank) information of positive\nsamples is largely ignored, especially when labeled samples are limited. To\nthis end, we present Rank Supervised Contrastive Learning (RankSCL) to perform\ntime series classification. Different from conventional contrastive learning\nframeworks, RankSCL augments raw data in a targeted way in the embedding space\nand adopts certain filtering rules to select more informative positive and\nnegative pairs of samples. Moreover, a novel rank loss is developed to assign\ndifferent weights for different levels of positive samples, enable the encoder\nto extract the fine-grained information of the same class, and produce a clear\nboundary among different classes. Thoroughly empirical studies on 128 UCR\ndatasets and 30 UEA datasets demonstrate that the proposed RankSCL can achieve\nstate-of-the-art performance compared to existing baseline methods.",
        "updated": "2024-01-31 18:29:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18057v1"
    }
]