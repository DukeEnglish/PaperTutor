[
    {
        "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
        "authors": "Andreas OpedalAlessandro StolfoHaruki ShirakamiYing JiaoRyan CotterellBernhard SchölkopfAbulhair SaparovMrinmaya Sachan",
        "links": "http://arxiv.org/abs/2401.18070v1",
        "entry_id": "http://arxiv.org/abs/2401.18070v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18070v1",
        "summary": "There is increasing interest in employing large language models (LLMs) as\ncognitive models. For such purposes, it is central to understand which\ncognitive properties are well-modeled by LLMs, and which are not. In this work,\nwe study the biases of LLMs in relation to those known in children when solving\narithmetic word problems. Surveying the learning science literature, we posit\nthat the problem-solving process can be split into three distinct steps: text\ncomprehension, solution planning and solution execution. We construct tests for\neach one in order to understand which parts of this process can be faithfully\nmodeled by current state-of-the-art LLMs. We generate a novel set of word\nproblems for each of these tests, using a neuro-symbolic method that enables\nfine-grained control over the problem features. We find evidence that LLMs,\nwith and without instruction-tuning, exhibit human-like biases in both the\ntext-comprehension and the solution-planning steps of the solving process, but\nnot during the final step which relies on the problem's arithmetic expressions\n(solution execution).",
        "updated": "2024-01-31 18:48:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18070v1"
    },
    {
        "title": "SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition",
        "authors": "Yihan WuSoumi MaitiYifan PengWangyou ZhangChenda LiYuyue WangXihua WangShinji WatanabeRuihua Song",
        "links": "http://arxiv.org/abs/2401.18045v1",
        "entry_id": "http://arxiv.org/abs/2401.18045v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18045v1",
        "summary": "Recent advancements in language models have significantly enhanced\nperformance in multiple speech-related tasks. Existing speech language models\ntypically utilize task-dependent prompt tokens to unify various speech tasks in\na single model. However, this design omits the intrinsic connections between\ndifferent speech tasks, which can potentially boost the performance of each\ntask. In this work, we propose a novel decoder-only speech language model,\nSpeechComposer, that can unify common speech tasks by composing a fixed set of\nprompt tokens. Built upon four primary tasks -- speech synthesis, speech\nrecognition, speech language modeling, and text language modeling --\nSpeechComposer can easily extend to more speech tasks via compositions of\nwell-designed prompt tokens, like voice conversion and speech enhancement. The\nunification of prompt tokens also makes it possible for knowledge sharing among\ndifferent speech tasks in a more structured manner. Experimental results\ndemonstrate that our proposed SpeechComposer can improve the performance of\nboth primary tasks and composite tasks, showing the effectiveness of the shared\nprompt tokens. Remarkably, the unified decoder-only model achieves a comparable\nand even better performance than the baselines which are expert models designed\nfor single tasks.",
        "updated": "2024-01-31 18:06:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18045v1"
    },
    {
        "title": "Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability",
        "authors": "Navin KamuniHardik ShahSathishkumar ChintalaNaveen KunchakuriSujatha Alla Old Dominion",
        "links": "http://arxiv.org/abs/2401.18040v1",
        "entry_id": "http://arxiv.org/abs/2401.18040v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18040v1",
        "summary": "End-to-end multi-task dialogue systems are usually designed with separate\nmodules for the dialogue pipeline. Among these, the policy module is essential\nfor deciding what to do in response to user input. This policy is trained by\nreinforcement learning algorithms by taking advantage of an environment in\nwhich an agent receives feedback in the form of a reward signal. The current\ndialogue systems, however, only provide meagre and simplistic rewards.\nInvestigating intrinsic motivation reinforcement learning algorithms is the\ngoal of this study. Through this, the agent can quickly accelerate training and\nimprove its capacity to judge the quality of its actions by teaching it an\ninternal incentive system. In particular, we adapt techniques for random\nnetwork distillation and curiosity-driven reinforcement learning to measure the\nfrequency of state visits and encourage exploration by using semantic\nsimilarity between utterances. Experimental results on MultiWOZ, a\nheterogeneous dataset, show that intrinsic motivation-based debate systems\noutperform policies that depend on extrinsic incentives. By adopting random\nnetwork distillation, for example, which is trained using semantic similarity\nbetween user-system dialogues, an astounding average success rate of 73% is\nachieved. This is a significant improvement over the baseline Proximal Policy\nOptimization (PPO), which has an average success rate of 60%. In addition,\nperformance indicators such as booking rates and completion rates show a 10%\nrise over the baseline. Furthermore, these intrinsic incentive models help\nimprove the system's policy's resilience in an increasing amount of domains.\nThis implies that they could be useful in scaling up to settings that cover a\nwider range of domains.",
        "updated": "2024-01-31 18:03:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18040v1"
    },
    {
        "title": "Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models",
        "authors": "Mitodru NiyogiArnab Bhattacharya",
        "links": "http://arxiv.org/abs/2401.18034v1",
        "entry_id": "http://arxiv.org/abs/2401.18034v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18034v1",
        "summary": "We present Gyan AI Paramanu (\"atom\"), a family of novel language models for\nIndian languages. It is a collection of auto-regressive monolingual, bilingual,\nand multilingual Indic language models pretrained from scratch on a single GPU\nfor 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,\nOdia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,\nTamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are\npretrained with a context size of 1024 on a single GPU. The models are very\nefficient, small, fast, and powerful. We have also developed an efficient most\nadvanced Indic tokenizer that can even tokenize unseen languages. In order to\navoid the \"curse of multi-linguality\" in our multilingual mParamanu model, we\npretrained on comparable corpora by typological grouping using the same script.\nWe performed human evaluation of our pretrained models for open end text\ngeneration on grammar, coherence, creativity, and factuality metrics for\nBangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models\noutperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,\nGPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite\nbeing smaller in size by 66 to 20 times compared to standard 7B LLMs. To run\ninference on our pretrained models, CPU is enough, and GPU is not needed. We\nalso instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu\nmodels on 23k instructions in respective languages. Our pretrained and\ninstruction-tuned models which are first of its kind, most powerful efficient\nsmall generative language models ever developed for Indic languages, and the\nvarious results lead to the conclusion that high quality generative language\nmodels are possible without high amount of compute power and humongous number\nof parameters. We plan to release our models at https://www.bharatgpts.com.",
        "updated": "2024-01-31 17:58:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18034v1"
    },
    {
        "title": "Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI",
        "authors": "Mowafak AllahamNicholas Diakopoulos",
        "links": "http://arxiv.org/abs/2401.18028v1",
        "entry_id": "http://arxiv.org/abs/2401.18028v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18028v1",
        "summary": "Anticipating the negative impacts of emerging AI technologies is a challenge,\nespecially in the early stages of development. An understudied approach to such\nanticipation is the use of LLMs to enhance and guide this process. Despite\nadvancements in LLMs and evaluation metrics to account for biases in generated\ntext, it is unclear how well these models perform in anticipatory tasks.\nSpecifically, the use of LLMs to anticipate AI impacts raises questions about\nthe quality and range of categories of negative impacts these models are\ncapable of generating. In this paper we leverage news media, a diverse data\nsource that is rich with normative assessments of emerging technologies, to\nformulate a taxonomy of impacts to act as a baseline for comparing against. By\ncomputationally analyzing thousands of news articles published by hundreds of\nonline news domains around the world, we develop a taxonomy consisting of ten\ncategories of AI impacts. We then evaluate both instruction-based (GPT-4 and\nMistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3)\nusing a sample from this baseline. We find that the generated impacts using\nMistral-7B, fine-tuned on impacts from the news media, tend to be qualitatively\non par with impacts generated using a larger scale model such as GPT-4.\nMoreover, we find that these LLMs generate impacts that largely reflect the\ntaxonomy of negative impacts identified in the news media, however the impacts\nproduced by instruction-based models had gaps in the production of certain\ncategories of impacts in comparison to fine-tuned models. This research\nhighlights a potential bias in state-of-the-art LLMs when used for anticipating\nimpacts and demonstrates the advantages of aligning smaller LLMs with a diverse\nrange of impacts, such as those reflected in the news media, to better reflect\nsuch impacts during anticipatory exercises.",
        "updated": "2024-01-31 17:43:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18028v1"
    }
]