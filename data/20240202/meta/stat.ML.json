[
    {
        "title": "Variable selection for Naïve Bayes classification",
        "authors": "Rafael BlanqueroEmilio CarrizosaPepa Ramírez-CoboM. Remedios Sillero-Denamiel",
        "links": "http://dx.doi.org/10.1016/j.cor.2021.105456",
        "entry_id": "http://arxiv.org/abs/2401.18039v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18039v1",
        "summary": "The Na\\\"ive Bayes has proven to be a tractable and efficient method for\nclassification in multivariate analysis. However, features are usually\ncorrelated, a fact that violates the Na\\\"ive Bayes' assumption of conditional\nindependence, and may deteriorate the method's performance. Moreover, datasets\nare often characterized by a large number of features, which may complicate the\ninterpretation of the results as well as slow down the method's execution.\n  In this paper we propose a sparse version of the Na\\\"ive Bayes classifier\nthat is characterized by three properties. First, the sparsity is achieved\ntaking into account the correlation structure of the covariates. Second,\ndifferent performance measures can be used to guide the selection of features.\nThird, performance constraints on groups of higher interest can be included.\nOur proposal leads to a smart search, which yields competitive running times,\nwhereas the flexibility in terms of performance measure for classification is\nintegrated. Our findings show that, when compared against well-referenced\nfeature selection approaches, the proposed sparse Na\\\"ive Bayes obtains\ncompetitive results regarding accuracy, sparsity and running times for balanced\ndatasets. In the case of datasets with unbalanced (or with different\nimportance) classes, a better compromise between classification rates for the\ndifferent classes is achieved.",
        "updated": "2024-01-31 18:01:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18039v1"
    },
    {
        "title": "A cost-sensitive constrained Lasso",
        "authors": "Rafael BlanqueroEmilio CarrizosaPepa Ramírez-CoboM. Remedios Sillero-Denamiel",
        "links": "http://dx.doi.org/10.1007/s11634-020-00389-5",
        "entry_id": "http://arxiv.org/abs/2401.18023v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18023v1",
        "summary": "The Lasso has become a benchmark data analysis procedure, and numerous\nvariants have been proposed in the literature. Although the Lasso formulations\nare stated so that overall prediction error is optimized, no full control over\nthe accuracy prediction on certain individuals of interest is allowed. In this\nwork we propose a novel version of the Lasso in which quadratic performance\nconstraints are added to Lasso-based objective functions, in such a way that\nthreshold values are set to bound the prediction errors in the different groups\nof interest (not necessarily disjoint). As a result, a constrained sparse\nregression model is defined by a nonlinear optimization problem. This\ncost-sensitive constrained Lasso has a direct application in heterogeneous\nsamples where data are collected from distinct sources, as it is standard in\nmany biomedical contexts. Both theoretical properties and empirical studies\nconcerning the new method are explored in this paper. In addition, two\nillustrations of the method on biomedical and sociological contexts are\nconsidered.",
        "updated": "2024-01-31 17:36:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18023v1"
    },
    {
        "title": "Causal Discovery by Kernel Deviance Measures with Heterogeneous Transforms",
        "authors": "Tim TseZhitang ChenShengyu ZhuYue Liu",
        "links": "http://arxiv.org/abs/2401.18017v1",
        "entry_id": "http://arxiv.org/abs/2401.18017v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18017v1",
        "summary": "The discovery of causal relationships in a set of random variables is a\nfundamental objective of science and has also recently been argued as being an\nessential component towards real machine intelligence. One class of causal\ndiscovery techniques are founded based on the argument that there are inherent\nstructural asymmetries between the causal and anti-causal direction which could\nbe leveraged in determining the direction of causation. To go about capturing\nthese discrepancies between cause and effect remains to be a challenge and many\ncurrent state-of-the-art algorithms propose to compare the norms of the kernel\nmean embeddings of the conditional distributions. In this work, we argue that\nsuch approaches based on RKHS embeddings are insufficient in capturing\nprincipal markers of cause-effect asymmetry involving higher-order structural\nvariabilities of the conditional distributions. We propose Kernel Intrinsic\nInvariance Measure with Heterogeneous Transform (KIIM-HT) which introduces a\nnovel score measure based on heterogeneous transformation of RKHS embeddings to\nextract relevant higher-order moments of the conditional densities for causal\ndiscovery. Inference is made via comparing the score of each hypothetical\ncause-effect direction. Tests and comparisons on a synthetic dataset, a\ntwo-dimensional synthetic dataset and the real-world benchmark dataset\nT\\\"ubingen Cause-Effect Pairs verify our approach. In addition, we conduct a\nsensitivity analysis to the regularization parameter to faithfully compare\nprevious work to our method and an experiment with trials on varied\nhyperparameter values to showcase the robustness of our algorithm.",
        "updated": "2024-01-31 17:28:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18017v1"
    },
    {
        "title": "Causal Coordinated Concurrent Reinforcement Learning",
        "authors": "Tim TseIsaac ChanZhitang Chen",
        "links": "http://arxiv.org/abs/2401.18012v1",
        "entry_id": "http://arxiv.org/abs/2401.18012v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18012v1",
        "summary": "In this work, we propose a novel algorithmic framework for data sharing and\ncoordinated exploration for the purpose of learning more data-efficient and\nbetter performing policies under a concurrent reinforcement learning (CRL)\nsetting. In contrast to other work which make the assumption that all agents\nact under identical environments, we relax this restriction and instead\nconsider the formulation where each agent acts within an environment which\nshares a global structure but also exhibits individual variations. Our\nalgorithm leverages a causal inference algorithm in the form of Additive Noise\nModel - Mixture Model (ANM-MM) in extracting model parameters governing\nindividual differentials via independence enforcement. We propose a new data\nsharing scheme based on a similarity measure of the extracted model parameters\nand demonstrate superior learning speeds on a set of autoregressive, pendulum\nand cart-pole swing-up tasks and finally, we show the effectiveness of diverse\naction selection between common agents under a sparse reward setting. To the\nbest of our knowledge, this is the first work in considering non-identical\nenvironments in CRL and one of the few works which seek to integrate causal\ninference with reinforcement learning (RL).",
        "updated": "2024-01-31 17:20:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18012v1"
    },
    {
        "title": "Convergence Analysis for General Probability Flow ODEs of Diffusion Models in Wasserstein Distances",
        "authors": "Xuefeng GaoLingjiong Zhu",
        "links": "http://arxiv.org/abs/2401.17958v1",
        "entry_id": "http://arxiv.org/abs/2401.17958v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17958v1",
        "summary": "Score-based generative modeling with probability flow ordinary differential\nequations (ODEs) has achieved remarkable success in a variety of applications.\nWhile various fast ODE-based samplers have been proposed in the literature and\nemployed in practice, the theoretical understandings about convergence\nproperties of the probability flow ODE are still quite limited. In this paper,\nwe provide the first non-asymptotic convergence analysis for a general class of\nprobability flow ODE samplers in 2-Wasserstein distance, assuming accurate\nscore estimates. We then consider various examples and establish results on the\niteration complexity of the corresponding ODE-based samplers.",
        "updated": "2024-01-31 16:07:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17958v1"
    }
]