[
    {
        "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
        "authors": "Andreas OpedalAlessandro StolfoHaruki ShirakamiYing JiaoRyan CotterellBernhard SchölkopfAbulhair SaparovMrinmaya Sachan",
        "links": "http://arxiv.org/abs/2401.18070v1",
        "entry_id": "http://arxiv.org/abs/2401.18070v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18070v1",
        "summary": "There is increasing interest in employing large language models (LLMs) as\ncognitive models. For such purposes, it is central to understand which\ncognitive properties are well-modeled by LLMs, and which are not. In this work,\nwe study the biases of LLMs in relation to those known in children when solving\narithmetic word problems. Surveying the learning science literature, we posit\nthat the problem-solving process can be split into three distinct steps: text\ncomprehension, solution planning and solution execution. We construct tests for\neach one in order to understand which parts of this process can be faithfully\nmodeled by current state-of-the-art LLMs. We generate a novel set of word\nproblems for each of these tests, using a neuro-symbolic method that enables\nfine-grained control over the problem features. We find evidence that LLMs,\nwith and without instruction-tuning, exhibit human-like biases in both the\ntext-comprehension and the solution-planning steps of the solving process, but\nnot during the final step which relies on the problem's arithmetic expressions\n(solution execution).",
        "updated": "2024-01-31 18:48:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18070v1"
    },
    {
        "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
        "authors": "Parth SarthiSalman AbdullahAditi TuliShubh KhannaAnna GoldieChristopher D. Manning",
        "links": "http://arxiv.org/abs/2401.18059v1",
        "entry_id": "http://arxiv.org/abs/2401.18059v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18059v1",
        "summary": "Retrieval-augmented language models can better adapt to changes in world\nstate and incorporate long-tail knowledge. However, most existing methods\nretrieve only short contiguous chunks from a retrieval corpus, limiting\nholistic understanding of the overall document context. We introduce the novel\napproach of recursively embedding, clustering, and summarizing chunks of text,\nconstructing a tree with differing levels of summarization from the bottom up.\nAt inference time, our RAPTOR model retrieves from this tree, integrating\ninformation across lengthy documents at different levels of abstraction.\nControlled experiments show that retrieval with recursive summaries offers\nsignificant improvements over traditional retrieval-augmented LMs on several\ntasks. On question-answering tasks that involve complex, multi-step reasoning,\nwe show state-of-the-art results; for example, by coupling RAPTOR retrieval\nwith the use of GPT-4, we can improve the best performance on the QuALITY\nbenchmark by 20% in absolute accuracy.",
        "updated": "2024-01-31 18:30:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18059v1"
    },
    {
        "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
        "authors": "Yushi BaiXin LvJiajie ZhangYuze HeJi QiLei HouJie TangYuxiao DongJuanzi Li",
        "links": "http://arxiv.org/abs/2401.18058v1",
        "entry_id": "http://arxiv.org/abs/2401.18058v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18058v1",
        "summary": "Extending large language models to effectively handle long contexts requires\ninstruction fine-tuning on input sequences of similar length. To address this,\nwe present LongAlign -- a recipe of the instruction data, training, and\nevaluation for long context alignment. First, we construct a long\ninstruction-following dataset using Self-Instruct. To ensure the data\ndiversity, it covers a broad range of tasks from various long context sources.\nSecond, we adopt the packing and sorted batching strategies to speed up\nsupervised fine-tuning on data with varied length distributions. Additionally,\nwe develop a loss weighting method to balance the contribution to the loss\nacross different sequences during packing training. Third, we introduce the\nLongBench-Chat benchmark for evaluating instruction-following capabilities on\nqueries of 10k-100k in length. Experiments show that LongAlign outperforms\nexisting recipes for LLMs in long context tasks by up to 30\\%, while also\nmaintaining their proficiency in handling short, generic tasks. The code, data,\nand long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.",
        "updated": "2024-01-31 18:29:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18058v1"
    },
    {
        "title": "Multipath parsing in the brain",
        "authors": "Berta FranzluebbersDonald DunaganMiloš StanojevićJan BuysJohn T. Hale",
        "links": "http://arxiv.org/abs/2401.18046v1",
        "entry_id": "http://arxiv.org/abs/2401.18046v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18046v1",
        "summary": "Humans understand sentences word-by-word, in the order that they hear them.\nThis incrementality entails resolving temporary ambiguities about syntactic\nrelationships. We investigate how humans process these syntactic ambiguities by\ncorrelating predictions from incremental generative dependency parsers with\ntimecourse data from people undergoing functional neuroimaging while listening\nto an audiobook. In particular, we compare competing hypotheses regarding the\nnumber of developing syntactic analyses in play during word-by-word\ncomprehension: one vs more than one. This comparison involves evaluating\nsyntactic surprisal from a state-of-the-art dependency parser with LLM-adapted\nencodings against an existing fMRI dataset. In both English and Chinese data,\nwe find evidence for multipath parsing. Brain regions associated with this\nmultipath effect include bilateral superior temporal gyrus.",
        "updated": "2024-01-31 18:07:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18046v1"
    },
    {
        "title": "SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition",
        "authors": "Yihan WuSoumi MaitiYifan PengWangyou ZhangChenda LiYuyue WangXihua WangShinji WatanabeRuihua Song",
        "links": "http://arxiv.org/abs/2401.18045v1",
        "entry_id": "http://arxiv.org/abs/2401.18045v1",
        "pdf_url": "http://arxiv.org/pdf/2401.18045v1",
        "summary": "Recent advancements in language models have significantly enhanced\nperformance in multiple speech-related tasks. Existing speech language models\ntypically utilize task-dependent prompt tokens to unify various speech tasks in\na single model. However, this design omits the intrinsic connections between\ndifferent speech tasks, which can potentially boost the performance of each\ntask. In this work, we propose a novel decoder-only speech language model,\nSpeechComposer, that can unify common speech tasks by composing a fixed set of\nprompt tokens. Built upon four primary tasks -- speech synthesis, speech\nrecognition, speech language modeling, and text language modeling --\nSpeechComposer can easily extend to more speech tasks via compositions of\nwell-designed prompt tokens, like voice conversion and speech enhancement. The\nunification of prompt tokens also makes it possible for knowledge sharing among\ndifferent speech tasks in a more structured manner. Experimental results\ndemonstrate that our proposed SpeechComposer can improve the performance of\nboth primary tasks and composite tasks, showing the effectiveness of the shared\nprompt tokens. Remarkably, the unified decoder-only model achieves a comparable\nand even better performance than the baselines which are expert models designed\nfor single tasks.",
        "updated": "2024-01-31 18:06:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.18045v1"
    }
]