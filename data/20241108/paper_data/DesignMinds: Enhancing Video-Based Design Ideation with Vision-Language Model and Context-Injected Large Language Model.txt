DesignMinds: Enhancing Video-Based Design Ideation with
Vision-Language Model and Context-Injected Large
Language Model
TIANHAOHE,DelftUniversityofTechnology,TheNetherlands
ANDRIJASTANKOVIĆ,DelftUniversityofTechnology,Netherlands
EVANGELOSNIFORATOS,DelftUniversityofTechnology,Netherlands
GERDKORTUEM,DelftUniversityofTechnology,TheNetherlands
Fig.1. AdesignerintheexperimentalgroupisinteractingwithDesignMinds.
Ideationisacriticalcomponentofvideo-baseddesign(VBD),wherevideosserveastheprimarymedium
for design exploration and inspiration. The emergence of generative AI offers considerable potential to
enhancethisprocessbystreamliningvideoanalysisandfacilitatingideageneration.Inthispaper,wepresent
DesignMinds,aprototypethatintegratesastate-of-the-artVision-LanguageModel(VLM)withacontext-
enhancedLargeLanguageModel(LLM)tosupportideationinVBD.ToevaluateDesignMinds,weconducteda
between-subjectstudywith35designpractitioners,comparingitsperformancetoabaselinecondition.Our
resultsdemonstratethatDesignMindssignificantlyenhancestheflexibilityandoriginalityofideation,while
alsoincreasingtaskengagement.Importantly,theintroductionofthistechnologydidnotnegativelyimpact
userexperience,technologyacceptance,orusability.
CCSConcepts:•Human-centeredcomputing→EmpiricalstudiesinHCI;•Computingmethodologies
→Planningfordeterministicactions.
AdditionalKeyWordsandPhrases:DesignIdeation,GenerativeAI,Video-basedDesign,LargeLanguage
Model,VisionLanguageModel,Eye-tracking,Designer-AICollaboration
1 Introduction
Ideagenerationisthecornerstoneofinnovationandservesasthefoundationfornewdesigns
[11,41].Video-BasedDesign(VBD)enablesdesignerstoutilizevideocontentasakeytoolfor
generatingknowledge,inspiringnewideas,andidentifyingpotentialchallenges[19,63,65,71].
TheideationofVBDplaysacrucialroleinbrainstormingtoproduceawiderangeofideas,which
Authors’ContactInformation:TianhaoHe,DelftUniversityofTechnology,Delft,TheNetherlands,t.he-1@tudelft.nl;
AndrijaStanković,DelftUniversityofTechnology,Delft,Netherlands,andrija.stkvc@gmail.com;EvangelosNiforatos,Delft
UniversityofTechnology,Delft,Netherlands,e.niforatos@tudelft.nl;GerdKortuem,DelftUniversityofTechnology,Delft,
TheNetherlands,g.w.kortuem@tudelft.nl.
4202
voN
6
]CH.sc[
1v72830.1142:viXra2 Heetal.
arethenfilteredandrefinedtodevelopoptimalsolutions[15,42,43].However,generatingnovel
design ideas from videos is challenging for a large group of practitioners. It requires not only
a significant investment of time and effort but also extensive design experience to generate a
substantialnumberofrelatedideasforpractice[15].Consolidatingdesignproblemsandgenerating
feasiblesolutionsfromvideosusingtraditionalVBDmethodstypicallyrequiresextensivevideo
reviewandtheapplicationofprofessionaldivergentthinking[71].Thisprocessisoftenlabor-
intensiveandheavilydependentonthepractitioner’sdesignexperienceandknowledge,which
can be particularly challenging for novice designers with limited expertise and resources [77].
Additionally,previousresearchindicatedthatadvancedvideotoolscanpotentiallyenhancethe
designworkwithvideostoimprovethequalityoutcomes,andtofacilitateinteractions[73].
With the recent surge in Generative AI (GenAI), technologies such as the Large Language
Model(LLM)GPT-4[46]demonstratesignificantpotentialtoenhancecreativetasksacrossvarious
designdomains.AbaseLLMmodelcangenerateideasacrossdiversescopes.Itscapabilitiescan
befurtherrefinedbyincorporatingcontextualmaterialthroughaprocessknownasRetrieval-
AugmentedGeneration(RAG)tomakeitadaptableincurrentcircumstances[36].Additionally,
Vision-LanguageModels(VLMs)possesstheabilitytointerpretvideoswithhighdetail,reducing
theneedforextensivehumaneffort[7].Theseadvancementshavethepotentialtoassistdesigners
in overcoming challenges associated with generating efficient and effective ideas, particularly
whenfacedwithprolongedvideoviewingandlimiteddesignexperience[34,50].Assuch,this
paperexploresanapproachthatcombinesacustomizedVLMandLLM(DesignMinds)toenhance
the"watch-summarize-ideate"processinVBDtasksthroughdesigner-AIco-ideation.Wethen
presentourbenchmarksandevaluatethequalityofthegeneratedideas,cognitiveprocesses,user
experience (UX) and technology acceptance and use from VBD ideation. Our work makes the
followingcontributions:
• WeintroduceanovelGenAI-poweredchatbotthatfeaturesvideounderstandinganddesign-
context-based idea recommendations to enhance the ideation capabilities of new VBD
practitioners.
• Weinvestigatetheimpactofourprototypeintermsofideationquality,cognitiveprocessing
duringideation,andsubsequentUXandtechnologyacceptance.
• Ultimately,weproposeapotentialtool(DesignMinds)involvingtheuseofacustomized
VLMandLLMtoscaleuptheVBDideationprocessfornewdesigners.
Finally,ourfindingsindicatethatDesignMindsimprovestheflexibilityandoriginalityofdesign
ideasandboostdesigntaskengagement.Theadoptionofthistechnologyalsodidnotadversely
affecttheestablishedpatternsofUX,technologyacceptanceandusability.
2 Background
2.1 IdeationinDesign
Inthedesignprocess,ideationisakeyaspectofexperiencethatinfluencesboththeinitiationand
progressionintheearlystageofcreativeactivities.EckertandStaceyarticulatedthatideationisnot
merelyacatalystforcreativitybutalsoacriticalcomponentindevelopingdesignideas[18].They
claimedthatideationindesignprovidesacontextualframeworkthatenablesdesignerstoeffectively
communicateandpositiontheirwork.Itsparksdesigncreativity,offeringnewperspectivesand
triggeringthegenerationoforiginalideas[18].Similarly,SetchiandBoucharddefineideation
asamultifacetedphenomenonwheredesignersabsorbandreinterpretexistingideas,forms,and
concepts[57].Thisprocessisinfluencedbydesigners’individualexperiences,culturalbackgrounds,
andpersonalinterestsandservesasaguidingprincipleforcreativity.Thesubjectivityofideation
accelerates designers to explore a broader array of possibilities. Gonçalves et al. extended theDesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 3
understandingofideationintolaterstages,assertingthatdesignersmaintainalimitedrangeof
externalstimulipreferences.Bothdesignstudentsandprofessionalsoftenfavorvisualstimulisuch
asimages,objects,andvideosourcestoencouragecreativity[20].
However, relying on specific stimuli and designers’ own knowledge may cause the risk of
designfixation[30].Thisphenomenonoccurswhendesignersover-relyonspecificknowledge
directlyassociatedwithaproblemorthemselvesduringideation,eventuallyinhibitingthedesign
outcome[40,72].ViswanathanandLinseyclaimedthattheproblemoffixationispervasiveand
variesinverselywiththelevelofdesignexpertise.Theysuggestedthatitisespeciallyprominent
amongnovicedesigners,whotendtorelyheavilyontheirpredominantknowledgeduringideation
[66].Inaddition,novicedesignersoftenstruggletoanalyzeproblemscomprehensivelyandhave
difficultyseekinghelpfulinformationduringideation[14,17].Thisphenomenonoftenleadsto
failuresinframingproblemsanddirectingthesearchforsolutions,ultimatelydiminishingthe
designoutcome.Gonçalvespointedoutthatthelackofreflectioninideationcouldbeaddressedby
developingcomputationaltoolstohelpdesignersefficientlyfindrelevantstimuli.Suchtoolscould
assistinexperienceddesignersinexploringideasthataresemanticallydistantfromtheproblem
domainandexpandspaceforideation[20].Similarly,thestudybyDazkiretal.showedthatwhile
self-selectedcontextsindesignersledtogreaterinterestinthetopic,theyoftenfailedtodevelop
effectivedesignsolutions.Thisindicatesthat,althoughsomeautonomyisbeneficialfordeveloping
designideas,manyinexperienceddesignersstillneedexternalinterventionintheearlystages
toaidinideation[16].Assuch,designers,especiallythosewithlimitedexperience,oftenneed
additionalhelpandguidancefromoutsidesourcestoenhanceideation.
2.2 VideosforDesignIdeation
Theuseofvideoasacentraltoolforideation,knownasVBD,involvescapturinginformation
andanalyzingsolutionsindesignprocess.Thistechniqueisparticularlyprevalentinfieldslike
user experience UX design, interaction design, and ethnographic research [71]. By recording
user interactions with products or environments, videos provide a dynamic and context-rich
datasourcefordesigners.Designvideotapesareinformativeforpractitionerstodeepencontext
understandingsandgeneratefollow-upinterventions[70,71].DesignerfromtheAppleInc.utilized
videostoenvisionnewuserinterfaces(UIs)fortheirfuturecomputers[65].Theyutilizedvideosto
benchmarknewUIsandstudyuserbehavioralreactionsthroughvideotapes.Inthesameyear,Tatar
fromPARCexploredlearningfromrepeatedvideoobservationsofuserbehaviorthroughstationary
camerarecordingsandaimedtominimizeerroneousassumptionsinsoftwaredevelopment[63].
Tataralsoemphasizedtheimportantroleofusingvideosforideationtopinpointdesignsolutions.
Similarly, Ylirisku and Buur conceptualized the practice and highlighted that using videos for
designideationisinstrumentalforpractitioners.Videosareaneffectivetoolforlearningfrom
targetusers’dailyexperiencesandaugmentdesignersgenerateanabundanceofideasfordesign
artifacts[70].Moreover,designerscanideatefromthe"thickdescriptions"thatvideoscapture
about users’ movements, interactions, and emotional transitions, which help in constructing
designnarrativesandencapsulatingindividualthoughts.Whilevideo-baseddesignideageneration
presents significant opportunities, videos often contain complex content and frequent events
[70,71].Theprocessofwatchingthesevideoscanbelabor-intensiveandtime-consuming.Videos
withrichdetailsandrapidsequencesrequireviewerssubstantialinformationprocessingeffort
toanalyzeperceivedinformation.Asaresult,designermaysuffersrisksofdiminishingdecision-
makingcapabilityandresultinadeclineinideationeffectiveness[8,47].Therefore,itisessential
todevelopstrategiestomitigatefatigueandreducetheinformationprocessingeffortfordesigners
whousevideosforinspiration,whileensuringthattheyretainthevaluableinformationpresented
invideos.4 Heetal.
2.3 GenAIforDesignIdeation
RecentadvancementsinGenAIaredrivingsignificantchangesacrossmultipledisciplines.Large
LanguageModels(LLMs),suchasGPT-4[46],haveshownremarkablecapabilitiesinassisting
creativetasksfordesignpurposes[75].Xuetal.proposedanLLM-augmentedframeworkthatuses
LLMpromptstogenerateunifiedcognitionforpractitionersandoptimizethecreativedesignprocess
inaprofessionalproductdesign[68].AnothergroupofresearchersproposedJamplate,aprotocol
thatleveragesformattedpromptsinLLMstoguidenovicedesignersinreal-time.Thisapproach
enhancestheircriticalthinkingandimprovesideagenerationmoreeffectively[69].Makaturaetal.
exploredtheuseofGPT-4togeneratetextualdesignlanguageandspatialcoordinatesforproduct
designandadaptationinindustry[45].TheyhighlightedthatGPT-4’sreasoningcapabilitiesoffer
significantvalueinnoveldesigndomains.Whendesignersareinexperiencedwithaparticular
domainorworkingonanovelproblem,GPT-4cansynthesizeinformationfromrelatedareasto
providesuitableadvice.Inaddition,byextendingLLMswithvisualunderstandingcapabilities,
VLMs demonstrates a promising advancement for completing open-ended visual tasks using
informationextractedfromvideos[37].Moreover,manyresearchersrecentlymadeattemptsto
evolveVLMsintomorecomplexandcontext-awaresystems.Forexample,Zhouetal.introduced
NavGPT, an LLM-based navigation agent that uses visual cues detected by a VLM to provide
indoornavigationsuggestions[76].Theydemonstratedthattheirsystemcangeneratehigh-level
navigationalsuggestionsfromautomaticobservationsandmovinghistories.Moreover,Picardet
al.exploredtheuseofGPT-4V(ison)[46],aversionofGPT-4withvision-languagecapabilities,in
productdesign.Theyinvestigateditsapplicationindesigntasks,suchasanalyzinghandwritten
sketchesandprovidingfollow-upsuggestionsformaterialselection,drawinganalysis,andspatial
optimization.TheirfindingsdemonstratedthatthisLVMmodelcanhandlecomplexdesignidea
generationwithproficiency[49].
ThemergingofLLMsandVLMspresentsanopportunitytoenhancedesign,promptingusto
explorethisintegrationintherefinedfieldofVBD.WearecuriousonifcombiningLLMsand
VLMs can benefit VBD practitioners in their idea generations. To investigate, we prototyped
DesignMindsthatintegratesastate-of-the-art(SOTA)VLMandLLMmodelwithacontext-
injection technique. We conducted a study involving two video-based design tasks to
assesstheimpactondesignideation,focusingonideationquality,cognitiveprocesses,
userexperience,andtechnologyacceptance.
3 OurDesignMindsPrototype
ThedevelopmentofourprototypefollowedthenaturalprocessoftheidealizationofVBD,consist-
ingoftwomainparts:videocomprehensionandideareflectionandrefinement[70].Asshown
inFig.2,DesignMindsconsistsoftwoprimarycomponents:thebackendandthefront-end.The
backendincludesaVLMandaLLMintegratedwithadesignknowledgerepositoryforreference.
Thefront-endfeaturescorrespondingavideoplaybackregionalongsideaconversationalwin-
dow.Weadoptedblip2-opt-6.7b1,aSOTAVLM,tointerpretvideosintotextualdescriptions.
When processing a video, the VLM first extracts perceived objects from the video and utilizes
built-in language connection functions to generate comprehensive textual descriptions of the
entirevideo.ThesecompletevideodescriptionsthenwereprocessedbyanLLMthroughGPT-4
API(gpt-4-0125-preview)2.Togeneratemoredesign-groundedsuggestions,weimplemented
a RAG function using a text embedding model text-embedding-ada-0023 on a framework of
1https://huggingface.co/facebook/opt-6.7b(lastaccessed:November7,2024).
2https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4(lastaccessed:November7,2024).
3https://platform.openai.com/docs/guides/embeddings(lastaccessed:November7,2024).DesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 5
Video Description
Backend
Video The video shows a hand holding a
cucumber steady on a cutting board while
the other hand wields a knife, slicing
VLM through the vegetable. The cucumber is
cut into small bits. Each bit falls onto the
board. A smartphone stands upright in
front with its screen on. A bottle of beer
sits to the side, and a black sink is visible to
the left...
Design Books as a part of The person picks up a handful of
system
cucumber bits and places them into a
prompt
bowl. They set the knife down and reach
Context LLM for a small bowl of salt. They sprinkle some
Injection
salt over the cucumber bits, then stir them
gently with a spoon...
Video Playback Conversation Window
Query
Front-e nd
Designer Answer
Fig. 2. DesignMinds consists of two primary components: the backend and the front-end. The backend
includesaVLMandaLLMintegratedwithadesignknowledgerepository.Thefront-endfeaturesavideo
playbackregionalongsideaconversationalwindow.Thevideosarefirstprocessedtoextractkeyterms
(highlightedinpinkinvideodescription)andarethenconnectedintoacomprehensivedescription(bluein
videodescription)usingin-builtlanguagelinkingfunctions.Thesecompletedescriptionsarethenpassed
totheLLM,alongwithaknowledgerepositoryenrichedbyselecteddesignbooksfromacommitteevote.
Designerscanthenusethefeaturesinfront-endtowatchthevideoplaybacktoenhancetrustandgrounding
forthedesigncontext,andengageinideationthroughconversationsintheconversationalwindow.
LlamaIndex4 asourDesignMinds’sprofessionalknowledgerepositoryforconversations.Toensure
thattheknowledgerepositoryprovideddesigner-relevantinformationforourLLM,weconducted
a discussion on VBD literature within an independent community of designers (N = 30). This
discussionledtoavotethatselectedsixauthoritativebooks(1,966pagestotal)withhigh-level
methodologicalrigorandpracticaldesigncasesfortheVBDtraining.WethenutilizedtheRAG
functionandtokenizetheselecteddesignbookstofeedintotheknowledgerepositoryoftheLLM.
Wethenbuiltourfront-endinterfaceusingGradio5asillustratedinFig.3.Theinterfaceincludesa
videoplayerandanchatbotconversationwindow.Totestperformanceandenhanceconvenience
for test users in the later study, we allocated the right portion of the screen to included a text
boxwhereuserscouldrecordtheirideasandinspirations.Thissetupallowsuserstoreviewand
revisitthedesigncontextusingthevideoplayer,generateadditionalinsightsandideasthrough
thechatbot,andrecordtheircomprehensivethoughtsinthetextboxforlateruse.
4 Study
WeevaluatehowourproposedDesignMindsinfluencesideationinVBDtaskswithabetween-subject
studydesign.Specifically,weexaminewhetherandhowthetoolinfluencesdesigners’effectiveness
4https://docs.llamaindex.ai/en/stable/(lastaccessed:November7,2024).
5https://www.gradio.app/(lastaccessed:November7,2024)6 Heetal.
Note- taking Space
Video Playback Conversation Window
(Divergent Thinking)
Fig.3. TheinterfaceofDesignMindsprimarilyfeaturesavideoplayerontheleftandanLLMconversation
windowinthecenter.Tofacilitateorganizedideationrecordinginthelaterstudy,weadditionallyincludeda
note-takingspacebellowadescriptionofVBDtasksforrecordingparticipants’divergentthinkingduring
thestudytasks(seeSupplementaryText1)fordetailedtext.WhendesignersuseDesignMinds,thesystem
initiallyperformsabackgroundpre-analysisofthevideocontentontheleft,andtransitionsvideocontent
tothechatinterfaceinthecenter.Designerssubsequentlyinteractviachattingandgenerateinspirationas
DivergentThinkingnotesontheright.
andabilitytogenerateideasfromvideocontent.Ourassessmentisstructuredaroundthreekey
perspectives:thequalityofideasgeneratedbydesigners,thecognitiveprocessestheyundergo
duringtheideationtasks,andtheiroveralluserexperienceandacceptanceofthenewprototype.
Additionally,weanalyzehowdesignersinteractwithDesignMindsfromtheirconversationlogsto
betterunderstandtheideationprocess.WealsoexploreDesignMinds’potentialcognitiveeffects,
perceived usefulness and likelihood of adoption by designers. Finally, we investigate areas for
improvementandsuggestwaystoenhanceDesignMinds’usabilityandotherconcerns.Ourstudy
addressesthefollowingResearchQuestions(RQs):
RQ1 HowdoesDesignMindsinfluencethequalityofideasgeneratedintheVBDprocess?
Divergentthinking,aconceptintroducedbyGuilford[24,25]actsasafoundationalideain
creativityresearch.Divergentthinkingemphasizesthegenerationofnovel,free-flowing,and
unconventionalideas,allowingfortheexpansionofthedesignspacetoidentifyinnovative
solutionsbasedonavailableresources[1,6,24].Thisapproachisakeymodelindesign
ideation,whereeffectivedivergentthinkingisoftenregardedasindicativeofsuccessful
ideation[71].Buildingonthisfoundation,weinvestigatehowourDesignMindsimpactsthe
outcomesofdivergentthinkingbyaskingparticipantsintwoconditions(experimentalgroup
andcontrolgroup)togeneratecreativeideasduringthetask.WehypothesizethatdesignersDesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 7
withAIco-ideationwillexhibithigherDivergentThinkingscorescomparedto
ideationwithoutAI.
RQ2 HowdoesDesignMindsinfluencethewaydesignerspracticeideationinVBD?
ExamininguserbehaviorsisanothercriticalaspectofevaluatingtheVBDideationprocess,
inadditiontoassessingthefinaldeliverables.Thebehaviorsexhibitedduringtasksreflect
participants’approachestocompletingtheassignedtasks[4,5,26].Werecordtheireye
movementstoevaluatethelevelofengagementandcognitiveloadexperiencedbydesigners
inbothconditions.Additionally,weconductanin-depthanalysisofthechatloghistory
fromtheexperimentalgrouptounderstandhowparticipantsinteractedwithDesignMinds.
Wehypothesizethatdesignerswillexperiencegreaterengagementand,consequently,
aslightlyhighercognitiveloadintheAI-prototype-assistedcondition.
RQ3 WhatimpactdoesDesignMindshaveontheUserExperience(UX)andTechnologyAcceptance
andUseintheVBDideationprocess?
Theintroductionofnewtechnologiesortoolstoatraditionalmethodologycansometimes
causediscomfortanddecreasesinUX[61].Understandingandevaluatingtechnologyac-
ceptanceandusealsoprovidesinsightsintohowwellusersadapttonewtechnology,which
maypotentiallyimpacttheoriginalpractice.WefurthercomparetheUXandthelevelofac-
ceptanceanduseoftechnologybetweenourprototypeconditionandthecontrolcondition
duringtheVBDideationprocess.Wehypothesizethatthenewlyintroducedprototype
willnothaveadditionalnegativeinfluenceonUXandtechnologyacceptanceand
usecomparedtotraditionalpractices.
4.1 Participants
Table1. Thedemographicsofparticipants’designexperience,includingpossibleresponsesandtheirvalues,
arepresentedasanswerfrequencies(f),followedbythecorrespondingpercentages(%).
Variable Answer f %
Bachelor 12 34.29%
Current design educa-
Master 22 62.86%
tionallevel
PhD(ongoing) 1 2.86%
Definitelynot 15 42.86%
Probablynot 7 20.00%
Experienceofdesigning
Mightormightnot 9 25.71%
withvideos(VBD)
Probablyyes 2 5.71%
Definitelyyes 2 5.71%
Definitelynot 2 42.86%
Experienceofpracticing Probablynot 2 5.71%
design divergent think- Mightormightnot 9 25.71%
ing(ideation) Probablyyes 17 48.57%
Definitelyyes 5 14.29%
Neverusedbefore 2 5.71%
Proficiencyinusingchat- Beginner 7 20.00%
bot Intermediate 13 37.14%
Expert 13 37.14%8 Heetal.
We enlisted 35 design graduates (17 females and 18 males) from the design faculty at our
university,followingapprovalfromtheethicsboardandconfirmingthatnonehadanycognitive
impairments.Theparticipants,whoareeitheruniversitystudents(BSc&MSc)orPhDcandidates,
hadanaverageageof25.4years(SD=2.31)andanaverageof2.4yearsofdesignexperience(SD
=1.14).Table1presentsthedemographicsofparticipantsinvolvedinthestudy,includingtheir
educationallevels,self-assessedfamiliaritywithVBDexperienceandideation,aswellastheir
proficiencyinusingchatbotslikeChatGPT.Inaddition,participantswithvisualacuitybelow20/20
wereinstructedtowearcontactlensesbeforeparticipating.Allparticipantswerefullyinformed
andprovidedconsentbeforetheexperimentbegan.
4.2 Apparatus
In our experiment, we evaluated our system in an office setting with consistent lighting. The
systemwassetuptooperateaslocalhostonadesktopcomputerwithinthelabenvironment.Fig.1
illustratesthelabsetupwhereparticipantsengagedwiththesystem.Alongsidestandardoffice
equipmentsuchasakeyboard,mouse,andspeaker,participantswereaskedtoweareye-tracking
glasses(NEONtypefromPupilLabs6,samplingrate200Hz).Theseglasseswereconnectedtoan
AndroidphoneviaaUSB-Ccabletorecordeye-movementdata.Thedatacollectedincludedpupil
dilationchanges,gazepositions,andblinkpatterns.Themonitorwaspositionedinacomfortable
visualrangeof55centimetersfromtheuser-facingedgeofthetableandwasa22-inchscreen
tilted15degreesbelowtheparticipants’horizontallineofsight[21].Additionally,weplacedfour
AprilTags7oneachcornerofthescreen(seeFig.1)toallowtheeye-trackingglassestodetectthe
screen’ssurfaceaccuratelyanddefinetheDesignMinds’interfaceastheareaofinterest(AOI).
4.3 Measures
4.3.1 SubjectiveMeasures.
• EvaluationofDivergentThinking(RQ1):weemploytheconceptofdivergentthinking
outlinedbyGuilford[24,25],andassessitthroughthefollowingthreedimensions:
- Fluency: a measurement captures the quantity of comprehensive ideas generated.
Eachideamustbesufficientlydetailedintermsofpurposeandfunctionalitytobeclearly
understood.
-Flexibility:ameasurementevaluatestherangeofdifferentdomainsandsubdomains
coveredbytheideas,reflectingthediversityoftheideationprocess.
- Originality: a measurement evaluates the uniqueness of ideas, measured by their
statisticalinfrequency,andisevaluatedusinga7-pointLikertscale.
• ChatLoghistory(RQ2):theintermediateconversationhistorymadebyparticipantsin
theexperimentalgroupwithAIco-ideationwiththechatbotportionofDesignMinds.
• Unifiedtheoryofacceptanceanduseoftechnology(UTAUT)(RQ3):awidelyrecog-
nizedmodelforassessinghowusersacceptandadoptinformationtechnologyconsiders
theperceivedlikelihoodofadoption.Thislikelihoodisinfluencedbyfivekeyconstructs:
performanceexpectancy,effortexpectancy,attitudetowardusingtechnology,anxiety,and
behavioralintentiontousethesystem[3,64,67].
• UserExperienceQuestionnaire(UEQ)(RQ3)8:aquestionnairedesignedtomeasureUX
ininteractiveproductsusesabenchmarkingmethodthatorganizesrawUEQscoresinto
categoriessuchasefficiency,perspicuity,dependability,originality,andstimulation[55,56].
6https://pupil-labs.com/products/neon/(lastaccessed:November7,2024).
7https://april.eecs.umich.edu/software/apriltag(lastaccessed:November7,2024).
8https://www.ueq-online.org/(lastaccessed:November7,2024).DesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 9
Getting Familiar with
Testing
Experiment Settings
Consent
Preperation (~8min) Demographics
start
Task Introduction
Interacting with
Experimental Context 1 counter DesignMinds
(G Nr =o 1u 8p ) Context 2 balancing Interacting with
DesignMinds
Main Experiment
(~30min)
Context 1 Not interacting with
Control counter DesignMinds
Group balancing
(N=17) Context 2 Not interacting with
DesignMinds
UEQ
end UTAUT
Post Session (~12min) Q1: Overall Experience
Interview (~5min) Q2: Typical process of ideating
Q3: Attitude towards AI
Fig.4. Duringthestudy,participantswereinitiallyaskedtofamiliarizethemselveswithboththeenvironment
andDesignMinds(Testing).Theyreceivedinstructionsonthecomponentsoftheprototypeandhowtointeract
withit.Followingthis,participantscompletedconsentanddemographicformsforbackgroundinformation.
Theywerethenprovidedwithinstructionsforthetasks(Preparation).Participantswererandomlydivided
intotwogroups:theexperimentalgroup,whichinteractedwiththechatbotDesignMinds,andthecontrol
group,whereparticipantscontinuedtheirusualpracticefordesigninspiration.Eachparticipantgroupwas
assignedtwotaskswithdifferentdesigncontexts,presentedinacounterbalancedorder.Inthenextsession
(PostSession),participantswereaskedtocompletetheUEQandUTAUTquestionnaires.Finally,theywere
interviewedforabout5minutesonthreetopics:overallexperience,typicalideationprocess,andtheirattitudes
towardsAI.
4.3.2 ObjectiveMeasures(RQ2).
• Pupil Dilation: an involuntary physiological response where the pupils widen during
assignedtasks.
• FixationRateandDuration:measurementsdescribehowoftenandhowlongtheeyes
remainstationaryonaspecificpointduringtasks,withfixationrateindicatingthefrequency
ofthesepausesandfixationdurationindicatingthelengthoftimetheeyesstaystillinone
position.
• BlinkRateandDuration:measurementsdescribehowoftenandhowlongtheeyelids
rapidlyclosingandopeningduringtasks.
• SaccadeRateandSpeed:measurementsdescribehowoftenandhowquicklytheeyes
performfastandconjugatemovementsfromonefixedpointtoanother.
4.4 Procedure
4.4.1 PreparationandMainExperimentSession. Beforethestudybegan,participantswereassigned
toeitherthecontrolorexperimentalgroupusingde-identifiedIDs.Participantswereindividually
invitedtothelabaccordingtotheirscheduledtimesandtooktheirdesignatedpositionsinfrontof
themonitor(seeFig.1).Withassistance,theyadjustedthemonitor’sheightandtiltanglebased
ontheirmeasuredheightandseatingposture.Theywerethenintroducedtothestudyapparatus,10 Heetal.
includingtheuserinterface(showninFig.3)relevanttotheirassignedgroup,howtowearthe
eye-trackingglasses,andbriefedonthestudyprocedure.Afterthisintroduction,participantswere
askedtocompleteaconsentformandprovidedemographicinformation,includingtheirexperience
withdesignideationfromvideos,generaldesignexperience,andfamiliaritywithusingchatbots.
Thentheyreceivedinstructions(seeSupplementaryText1)onthetaskstheywererequiredto
complete.
Followingthepreparatoryphase,participantsineachgroupwereshowntwovideotasksdepicting
contextsofcookingandconstruction,withtheorderofpresentationcounterbalanced.Thesevideos
weresourcedfromEgo4D9,alarge-scalevideodatasetfrequentlyemployedforbenchmarkandHCI
research[22].Eachvideowasapproximately3minutesinlengthtoensurebrevity,consideringthe
totalmaximumtestingtimeof15minutespervideo.Intheexperimentalgroup,participantswere
instructedtousethedefinedUIshowninFig.3towatchvideoplayback,interactwiththechatbot,
andmakenotesinthedesignatednote-takingspacetorecorddivergentthinking.Incontrast,forthe
controlgroup,thechatbotwashidden,andparticipantswereaskedtoproceedwithdesignideation
onthenote-takingspacefromthevideosastheynormallywould.Participantswerenotifiedatthe
12-minutemarkofeachtaskthattheyhad3minutesremaining.Thisalertwasdesignedtokeep
theminformedofthetimeconstraintsandallowthemtopreparefortheconclusionofthecurrent
task.Thisprocesswasrepeatedforbothvideos.
4.4.2 PostSession. Uponcompletion,theeye-trackingrecordingswerehalted.Participantswere
then asked to evaluate their experience using UEQ and UTAUT questionnaires. Following the
questionnaires,abriefinterviewofapproximately5minuteswasconducted.Participantswere
askedthreemaintopics:theiroverallexperienceduringthetwovideotasks,theirperformancein
theideationprocess,andtheirattitudestowardAIafterhavingtheexperiment.
5 Results
5.1 DivergentThinkingAnalysis(RQ1)
Fluency Experimental Group
Control Group
7.5
5.0
2.5
Flexibility* Originality**
Fig.5. Radarchartdepictingtheevaluationscoresofdesignthinkingacrossratersfortheexperimentaland
controlgroups.Errorsareindicatedbyshadedregions.Attributesmarkedwithasterisks(*or**)represent
significantdifferences.*denotes0.01<p<.05,and**denotesp<.001.
9https://ego4d-data.org/(lastaccessed:November7,2024).DesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 11
ToaddressthequalityofideasgeneratedintheVBDprocessasproposedinRQ1,wecollected
the divergent thinking texts from both groups. We then recruited three independent raters to
evaluatetheideationresultsbasedonfluency,flexibility,andoriginality,usingapredeterminedset
ofcriteria(Seesupplementarytext2)[24].Wethenperformedaquantitativeanalysisoftherating
scoresforboththeexperimentalandcontrolgroups.AsshowninFig.5,weobservedasignificant
maineffectontheaverageratingsforflexibilityandoriginality(independentt-test𝑡(33)𝑡 =2.304,
𝑝 =.014;𝑡(33)𝑡 =4.674,𝑝 < .001).Theaveragescoresforbothflexibility(7.17±3.511points)
andoriginality(4.74±1.018points)intheexperimentalgroupweresignificantlyhigher
thanthoseinthecontrolgroup(flexibility:5.12±1.074points;originality:3.35±0.583points).
However,therewasnosignificantmaineffectontheratingforfluencybetweenthetwogroups
(independentt-test𝑡(33)𝑡 = 1.885,𝑝 = .068).Additionally,Krippendorff’sAlphawascalculated
toassesstheinternalconsistencyofthethreeraters’judgmentsonthecategoriesofdivergent
thinking.Weobservedamoderateagreementamongtheraters,withanaverageKrippendorff’s
Alphaof𝛼 =.702(95%CI,.245to1),p<.001.
5.2 DesignIdeationProcess(RQ2)
5.2.1 Eye-tracking measures. We first analyzed the eye-tracking results from both groups. As
showninFig.6c,asignificantmaineffectwasobservedintheaveragepupildilationbetweenthe
experimentalandcontrolgroups(independentt-test𝑡(33)𝑡 = 2.933,𝑝 = .021).Thedashedline
inthesubplotrepresents0millimeterswhichindicatestherewasnochangefromparticipant’s
baselinepupildiameterduringnon-taskedtime.Comparedtothebaseline,participantsinthe
experimentalgroupexhibitedanaveragedilationof0.15mmmorethanthoseinthe
controlgroupduringtheideationtask(experimental𝑠𝑡𝑑 = 0.206;control𝑠𝑡𝑑 = 0.152).We
then examined the gaze fixation rate per minute and the average fixation duration across the
twogroups. AsshowninFig.6b,nosignificantmaineffect(independentt-test𝑡(33)𝑡 = 0.795,
𝑝 =.986)wasobservedintheaveragefixationrate(seesubplot(a)).Interestingly,participants
intheexperimentalgroupexhibitedanaveragefixationdurationthatwassignificantly
120.31milliseconds(𝑠𝑡𝑑 =135.053)longerthanthatofthecontrolgroupwithintheAOI
(𝑠𝑡𝑑 = 193.366;independentt-test𝑡(33)𝑡 = 1.567,𝑝 = .039).Additionally,asshowninsubplot
(a) of Fig. 6c, we observed a significant main effect in the average blink rate (independent t-
test𝑡(33)𝑡 = 0.557,𝑝 = .004).Participantsintheexperimentalgrouponaverageblinked
5.23 (experimental 𝑠𝑡𝑑 = 4.459; control 𝑠𝑡𝑑 = 5.400) less times per minute than those in
thecontrolgroup.However,nosignificantdifferencewasfoundintheaverageblinkduration
betweenthetwogroups(independentt-test𝑡(33)𝑡 =0.226,𝑝 =.340).Nosignificantmaineffect
wasobservedintheaveragesaccaderatebetweenthetwogroupsshowninFig.6d(independent
t-test𝑡(33)𝑡 =0.252,𝑝 =.249).However,therewasasignificantincreaseinsaccadevelocityinthe
experimentalgroupcomparedtothecontrolgroup(independentt-test𝑡(33)𝑡 =3.171,𝑝 < .001).
Onaverage,participantsintheexperimentalgroupperformed662.45pixelspersecond
fastersaccadesthanthoseinthecontrolgroupwithintheAOI(experimental𝑠𝑡𝑑 =477.332;
control𝑠𝑡𝑑 =351.452).
5.2.2 Chat log analysis. In addition to eye-tracking measurements, we conducted an in-depth
analysisoftheconversationlogsfromtheexperimentalgroup.Weutilizedbothqualitativeand
quantitativemethodstobetterunderstandwhatoccurredduringtheaugmenteddesignideation
processeswithDesignMinds.Wecategorizedthequestionsthatparticipantsaskedasfollows:
(a) Questionsaboutdesignopportunities(N=16):Themajorityofquestionsposedbypartici-
pants(P1-3,P5,P7-17,andP19)focusedonsuggestionsorideasforimprovingtheprocesses
depictedinthevideos.Theseinquiriestypicallyemergedafterparticipantshadgainedan12 Heetal.
understandingofthevideo’scontentandidentifiedkeyareasofinterestforpotentialdesign
opportunities.Forinstance,somedesigners,suchasP2andP8,soughtinitialinspirationto
begintheirdesignsbyasking,"Howcantheprocessesshowninthevideobeimproved?"
(P2)and"Whatcanbeimproved?"(P8).Others(P3,P9,P12,andP19)aimedtobuildupon
existingideasandleveragedtheLLMtofurtherextendtheirconcepts.Theseparticipants
askedquestionssuchas,"Whatdoyousuggesttoavoidusinghandsdirectlywhenhandling
food during cooking?" (P9), "Can you recommend structures that allow a construction
workertoliftheavyobjectswithoutcarryingthem?"(P12),and"Whataretheconsequences
ofnotusingfittedkitchentoolsforthetask?"(P19).
(b) Generalvideocontentunderstanding(N=13):Manyparticipants(P2-4,P6-8,P10,P11,P13,
P15,P16,P17andP19)utilizedthevideocomprehensioncapabilitiesofDesignMinds to
gainacomprehensiveunderstandingofthecontentpresentedinthevideos.Participants
frequently inquired about the events occurring in the video or sought clarification on
specificactionsorobjectstheyfoundunclear.SomeparticipantsemployedaDesignMinds-
firststrategy,initiatingtheirideationprocessesbyqueryingtheLLMaboutthevideo’s
content.Forexample,commoninquiriesincluded,"Whatisthisvideoabout?"(P2),"Listthe
stepsoftheactivities."(P6),"Whatdishishemaking?"(P11),and"Canyoutellmewhat’s
happeninginthevideo?"(P15).OthersusedDesignMinds tovalidatetheirobservations,
askingquestionssuchas,"Aretheycuttingtheedgeinastraighterline?"(P10)and"This
videowasabouthowtocutanavocado,right?"(P17).Additionally,asubsetofparticipants
posedhigher-level,reflectivequestionsaboutthevideo’scontent,suchasP19,whoasked,
"Whatisthegoalofwhattheyaredoingduringtheconstructionwork?"
(c) UnderstandingandIdeationfromSpecificSceneSettings(N=10):Asubsetofparticipants
(P3,P6,P7,P9,P12-14,P16,P17andP19)soughttoutilizeDesignMindstogainadeeper
understandingofspecificscenesettingsdepictedinthevideos.Unlikethebroaderinquiries
incategory(b),theseparticipantsfocusedonmorenarrowlydefinedactionswithinagiven
context.Forexample,whenviewingascenewhereanindividualattemptstoretrievefood
fromasealedjar,P6askedtheLLM,"Whataresomewaystolockajarautomatically?"
Similarly,P9usedtheprototypeasatoolforidentifyingspecificitems,asking,"Whatisthe
toolcalledthatslicescheeseinthisvideo?"P14inquiredaboutstrategiesfororganizing
kitchenutensils,asking,"Canyoucombinetherelocationideasforkitchentools?"Inthe
contextofconstruction,P16soughtdetailedadvicebyasking,"HowcanImakesurethat
themenoperateheavymachinerysafely?"whileP17questioned,"Whichismoreefficient:
addinganextrastepintheprocessorusingtwodifferenttools?"
(d) CombiningImpressionswithOpinion-BasedQueries(N=4):Someparticipants(P6,P11,
P16,P19)wentastepfurtherbyintegratingtheirownimpressionswiththeirquestionsand
aksedforpinion-basedsuggestions.Forinstance,P11,whileobservingasceneinvolving
threeworkersinaconstructionsetting,asked,"Don’tyouthinkthespaceiscrowdedfor3
people?"Theparticipanthereshowcasedacriticalevaluationofthescene.Similarly,other
participantsframedtheirquestionsinawaythatencouragedcriticalthinking.Forexample,
P19asked,"Whathappensifyoudon’tusefittedkitchentoolsforthejob?"
Additionally,weconductedcorrelationteststoexploretherelationshipbetweentraitsfromthechat
logsduringideationandthequalityofthefinalideationoutcomes,measuredbythreeattributes:
fluency,flexibility,andoriginality(seeFig.5).Weanalyzedtheconversationhistoryandcomputed
theaveragenumberofchatturnsparticipantsmadewiththeprototype,theaveragenumberof
wordsineachquestionaskedandresponsegenerated,andthenumberoffollow-upideasgenerated
foreachparticipantintheexperimentalgroup.AsshowninTable2,Pearsonproduct-momentDesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 13
correlationtestswereconductedtomeasuretherelationshipbetweenchatlogvariablesandideation
quality.Therewasastrong,positivecorrelationbetweentheaveragenumberofwordsin
eachparticipant’squestionandtheoriginalityoftheideasultimatelygenerated,whichwas
statisticallysignificant(𝜌 =.500,𝑛=18,𝑝 =.034).Similarly,astrongandsignificantlypositive
correlationwasfoundbetweentheaveragenumberofwordsineachgeneratedanswer
andthefluency(𝜌 =.636,𝑛=18,𝑝 =.005),flexibility(𝜌 =.743,𝑛=18,𝑝 <.001),andoriginality
(𝜌 =.652,𝑛=18,𝑝 =.003)oftheideationquality.Inaddition,astrongandsignificantlypositive
correlationwasalsoobservedbetweentheaveragenumberofideasgeneratedfromthe
prototypeandboththefluencyandflexibilityoftheideationquality(𝜌 =.749,𝑛=18,𝑝 <.001;
𝜌 =.782,𝑛=18,𝑝 <.001).
Table2. TableofPearson’scorrelationcoefficients(𝜌)andtheirp-valuesforfourtestvariablesfromthe
analysisoftheintermediatechatlogandthreeideationqualityvariables(seeFig.5).Significantcorrelations
areindicatedby**or*basedonthep-values(seenotes).
Ideation Pearson’scorrelation
ChatLogVariable P-value
Quality coefficient
Fluency 0.261 0.296
Avg.Nr.ofChatTurns Flexibility 0.126 0.619
Originality -0.313 0.206
Fluency 0.218 0.385
Avg.Nr.ofWordsinEach
Flexibility 0.318 0.198
QuestionAsked
Originality .500* 0.034
Fluency .636** 0.005
Avg.Nr.ofWordsinEach
Flexibility .743** <.001
AnswerGenerated
Originality .652** 0.003
Fluency .794** <.001
Avg.Nr.ofIdeasGener-
Flexibility .782** <.001
ated
Originality 0.398 0.102
Notes:Pearson’scorrelationtest(two-tailed)issignificantatthe**p<0.01and*p<0.05.
5.3 UX,TechnologyAcceptanceandUse(RQ3)
To determine if the introduction of a new technology affected the ideation process from VBD,
we analyzed self-reported data on participants’ UX and technology acceptance from both the
experimentalandcontrolgroups,asshowninFig.7.Weconductedone-wayANOVAandKruskal-
WallisHtestsforeachattributepair.Thenullhypothesis(H0)forthesestatisticaltestsassumed
thattherewasnosignificantmaineffectbetweenthetwogroupsregardingattributesfromUXand
technologyacceptanceanduse,meaningthattheself-reportedperceptionsinbothgroupswerethe
same.FortheUEQwhichmeasuresUX(seeFig.7a),theanalysisrevealednosignificantdifference
intheattractivenessattributebetweentheexperimentalgroupthatusedDesignMinds andthe
controlgroupasabaseline(ANOVA𝐹
1,33
=.386,𝑝 =.538).Similarly,comparisonsoftheotherUEQ
attributes—perspicuity(ANOVA𝐹
1,33
=1.208,𝑝 =.332),efficiency(ANOVA𝐹
1,33
=.008,𝑝 =.944),
dependability (ANOVA 𝐹 1,33 = 0.200, 𝑝 = .665), stimulation (ANOVA 𝐹 1,33 = 0.376, 𝑝 = .553),
andnovelty(ANOVA𝐹
1,33
=1.639,𝑝 =.345)—betweentheexperimentalandcontrolgroupsalso14 Heetal.
0.6 (a)
Control Group
0.4
Experimental Group
0.2 0 Experim2 e0 ntal Group 40 60 80 100 (b)*
Control Group
800 800
0.0 600 600
0.2 400 400 No change in pupil
diameter from baseline
200 200
Experimental Group Control Group
Group
(a)Participantsintheexperimentalgroup 00 20 Ave4 r0 aged Fixation ra6 t0 e [fixations/min8 ]0 100 0 Experimental Group Control Group
exhibitedsignificantlygreaterpupildilation (b) In subplot (b-a), no significant difference was
comparedtothecontrolgroup.Thedashed observedintheaveragedfixationratebetweenthe
lineat0millimeteronyaxisrepresentsno groups.Insubplot(b-b)indicatedbyanasterisks(*),
changeinpupildiameterrelativetothebase- participantsintheexperimentalgroupexhibiteda
line,whenparticipantswerenotengagedin significantlyhigherfixationdurationcomparedby
ideationtasks. controlgroup.
(a)* (a)
Control Group Control Group
Experimental Group Experimental Group
0 5 10 15 20 25 30 (b) 0 25 50 75 100 125 150 175 (b)**
325 E Cx op ne trr oim l Ge rn ot ua pl Group 325 3000 E Cx op ne trr oim l Ge rn ot ua pl Group 3000
300 300
2500 2500 275 275
250 250 2000 2000
225 225 1500 1500
200 200 1000 1000
175 175
500 500
150 150
0 5 10 15 20 25 30 Experimental Group Control Group 00 25 50 75 100 125 150 175 0Experimental Group Control Group
Averaged Blink Rate* [blinks/min] Averaged Saccade Rate [saccades/min]
(c)Insubplot(c-a),participantsintheexperimental(d)Insubplot(d-a),nosignificantdifferencefound
groupexhibitedasignificantlylowerblinkrate,ascompared averaged saccade rate across the two
indicatedbyanasterisks(*).Incontrast,(c-b)showsgroups.Insubplot(d-b)withasterisks(**),partici-
nosignificantdifferencewasobservedinblinkdura-pantsintheexperimentalgroupexhibitedasignifi-
tionbetweenthegroups. cantlylowernumberofaveragevelocityinsaccade.
Fig.6. Plotsdisplayingtheaveragepupildilation(6a),fixationrateandduration(6b),averageblinkrate
and duration (6c), and average saccade rate and velocity (6d) for the experimental and control groups.
Accompanyinghistogramswitherrorbarsarealsoprovidedforeachmeasure.Attributesandsubplotsmarked
withasterisks(*or**)representsignificantdifferences.*denotes0.01<p<.05,and**denotesp<.001.
retainedthenullhypothesis(H0).Thus,allsixUEQattributescollectedfromtheexperimental
groupusingDesignMindsmeasuredUXhasthesameresultsasinthecontrolgroup.
Additionally,asshowninFig.7b,thenon-parametricKruskal-WallistestrevealedthatthePE
attribute(performanceexpectancy)fromUTAUTfailedtorejectthenullhypothesis(𝑥2(1) =.003,
𝑝 =.960),indicatingnosignificantdifferenceinperformanceexpectancybetweenthegroups.The
meanrankscoreswere17.92fortheexperimentalgroup,18.09forthecontrolgroup.Similarly,the
attributesofEE(effortexpectancy)(ANOVA𝐹
1,33
=1.413,𝑝 =.081),ATT(attitudetowardusing
technology)(ANOVA𝐹
1,33
=.699,𝑝 =.287),ANX(anxiety)(ANOVA𝐹
1,33
=.391,𝑝 =.442),andBI
]mm[
*noitaliD
lipuP
]sm[ noitaruD
knilB
degarevA
]sm[ *noitaruD
noitaxiF
degarevA
]s/slexip[
**yticoleV
edaccaS
degarevADesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 15
2.5
Experimental Group
Control Group
2.0
1.5
1.0
0.5
0.0
0.5
1.0
Attractiveness Perspicuity Efficiency Dependability Stimulation Novelty
Attribute
(a) Scaled average values for measuring UX (UEQ) which include at-
tributes—Attractiveness, Perspicuity, Efficiency, Dependability, Stimulation, and
Novelty—comparedbetweentheexperimentalandcontrolgroups.
Experimental Group
Control Group
4
3
2
1
0
PE EE ATT ANX BI
Attribute
(b)Scaledaveragevaluesformeasuringtechnologyacceptanceanduse(UTAUT)across
attributes:PE(PerformanceExpectancy),EE(EffortExpectancy),ATT(AttitudeToward
usingTechnology),ANX(Anxiety),andBI(BehavioralIntentiontousethesystem).
Fig.7. HistogramsshowcaseUXandtechnologyacceptanceanduse,measuredusingUEQandUTAUT
questionnaires,respectively.Standarddeviationsarerepresentedaserrorbars.Nosignificantmaineffectwas
foundbetweentheexperimentalandcontrolgroupsregardingtheintroductionofanewtypeoftechnology
(i.e.,DesignMinds).
(behavioralintention)(ANOVA𝐹
1,33
=.004,𝑝 =.938)alsoretainedthenullhypothesis.Assuch,
allattributesformeasuringtechnologyacceptanceanduseretainedthenullhypothesis
betweenthetwogroups.ThesefindingsindicatethatourexperimentalDesignMindsdid
notintroduceanynegativeeffectsonUXortechnologyacceptanceandusecomparedto
thenormalideationprocessinVBD(control).
erocS
delacS
erocS
delacS16 Heetal.
6 Discussion
Inthisstudy,weconductedanA/BtesttoevaluatetheimpactofourDesignMindsontheideation
processforVBD.Participantswereassignedtwosub-tasksandaskedtogenerateasmanydesign
ideasaspossiblerelatedtotheprovidedcontexts.OurfindingsindicatethatDesignMindssignifi-
cantlyenhancedparticipants’performanceintermsoftheflexibilityandoriginalityoftheirfinal
ideationoutputscomparedtothebaseline.Additionally,participantsusingDesignMindsdemon-
stratedgreaterengagementindecision-making,asevidencedbyeye-trackingdata,andtherewasa
strongpositivecorrelationbetweenthenumberofideasandwordsgeneratedwithDesignMinds
andtheoverallqualityoftheirideation.Furthermore,ourfindingssuggestthattheintroductionof
DesignMindsdidnotnegativelyimpactuserexperienceortechnologyacceptance.
6.1 IncreasedFlexibilityandOriginalityinDivergentThinking
Tomeasurecreativityduringideation,DivergentThinkingisawell-establishedmethodsupported
byboththeoryandpracticeinpriorstudies[51–53].Inthisstudy,weadoptedthisprovenapproach
toinvestigatehowanewtool(DesignMinds)incorporatingemergingtechnologiescanenhance
ideationwithinadesigncontextinvolvingvideos.Ourfirstresearchquestion(RQ1)exploresthe
impactofDesignMindsonideationoutcomes.Toaddressthis,wecollectedDivergentThinking
datafromourstudyandhadthreeindependentgraderswithan"internalconsistency"checkto
evaluatethequalityofideation,followingprinciplesoutlinedinwell-establishedliterature[24].At
theoutset,wereviewedhowideationisunderstoodandmeasuredintheliterature.Forinstance,
fluencyisusedtoassesstheproductivityofideation,whileflexibilityindicatesdiverseideasacross
differentconceptualcategories.Originalityisdefinedbythenoveltyorrarityofideaswithinagiven
task[24].Ourresultsshowthatparticipantsintheexperimentalgroup,supportedbyDesignMinds,
receivedhigherratingsinflexibilityandoriginalitycomparedtothecontrolgroup.Thissuggests
that with DesignMinds’ assistance, the ideation process generated more multifaceted
andnovelideas[52].Specifically,thetraitofflexibilitycouldimproveprofessionalpractitioners’
understandingoftasks(e.g.,theusabilityofanartifact)anddecision-makingindesignprojects(e.g.,
plansforimprovement)[2].Whereasoriginality,ontheotherhand,notonlystronglycorrelates
withinnovationbutalsoreflectsthequalityofauthenticityandintegrityofcreativetasks[23].
Similarly,otherstudiesconcludedthatideationfromindustrialdesigntasksshouldconsiderthree
keyaspects:"functionalvalue","aestheticvalue"(e.g.,visualform),and"originalityvalue"[10].Our
studyshowedthattheprototypenotablyimprovedoutcomesintwooftheseaspects—flexibilityand
originality.Assuch,theuseofDesignMindsenhancedthevarietyandnoveltyofideasincreative
VBDtasks.
6.2 GreaterEngagementinIdeationandPositiveCorrelationBetweenInteraction
HistoryandPerformance
Werecognizethatthefinaloutcomeofideation(i.e.,DivergentThinking)waspartiallyenhanced
bytheprototype.Toexplorefurther,wesoughttounderstandhowourprototypedDesignMinds
influenced the ideation processes in design tasks (RQ2). We began by measuring participants’
eyemovementsduringthetasksandobservedanincreasedinpupildilationintheexperimental
groupcomparedtothecontrolgroup.Previousstudieshaveshownthatdynamicchangesinpupil
dilationareassociatedwithhigh-levelcognitiveprocessing[27].Sincethestudywasconductedin
astablelightingenvironment,theobservedincreaseinpupildilationindicatesthatparticipants
voluntarily engaged in deeper, high-level decision-making prompted by the recommendations
generatedbyDesignMinds[32].Furthermore,theobservedincreaseingazefixationdurationand
fastersaccadespeedintheexperimentalgroupsuggeststhatparticipantsweremoreengagedinDesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 17
thetaskscomparedtothecontrolgroup[13,28,59].Supportedbyexistingliterature,longergaze
fixationdurationandquickersaccadicmovementstypicallyindicatehigherlevelsoffocusand
cognitiveengagement[29,74].ThismayalsosuggestthatourDesignMindscapturedparticipants’
attention more effectively within the design task context compared to the traditional practice
without additional helps in the control group. Similarly, the observed lower blink rate in the
experimentalgroupsuggeststhatparticipantsshowedgreateremotionalinterestinthegenerated
content,whichinturnincreasedtheirfocusandengagementwiththeprovideddesignusecase[44].
Ahighlevelofworkengagementhasalsobeenshowntoleadtomorepositiveandimprovedwork
performance[12,35].Inthisway,participantsfromtheexperimentalgrouptookthedesign-
specializedadviceandengagedinmoreiterativereflectionintheideationprocessing.
Followingtheeye-trackingmeasurements,weconductedafollow-upanalysisofthechatlogs.
WeexaminedhowtheDesignMinds’responsesinfluencedtheinteractionsandhowtheseexchanges
correlatedwiththequalityofideasproducedduringhumandivergentthinking.
AsnotedinSection5.2.2,manyparticipantsengagedwithDesignMindstoseekinspirationand
guidanceforpotentialdesignimprovementsbasedonthevideocontext.Thevideocomprehension
functionfromDesignMindsaugmentedthecasedebriefingprocesstoallowparticipantstobypass
theneedtointroducethedesigncasefromscratch.Instead,participantscoulddirectlypropose
questionsaboutbothgeneralandspecificcontentsfromthevideos.Thegenerativeresponsesthen
effectivelyaddressedthetopicsathandandenabledparticipantstocontinuetheconversation.
Interestingly,participantsalsotreatedDesignMindsasacompanionintheirdesigntasks.Theyoften
utilizeditscontextualunderstandingfromadesignprofessional’sperspectivetoseekconfirmation
about the use cases or video contents. Upon receiving positive feedback, participants became
moreintriguedandconfidentwhichleadtodeeperinsightsduringtheDivergentThinkingphase.
Additionally,someparticipantsincorporatedtheirpersonalperspectivesintothequestionsand
findingstheysoughttoconfirm.Thislikelyreflectsthenatureofdesignwork,whichisoftendriven
byemotionalandpersonalsentiments.
Inaddition,subsequentcorrelationtestsrevealseveralstrongandpositiverelationshipsbetween
the words and ideas generated in chat logs and the quality of ideation in Divergent Thinking
tasks.Thissuggeststhatparticipants’ideationholdspositiverelationshipwiththeassistancefrom
DesignMinds.Consequently,theideationphaseislikelytobeenhancedbyrichercontents
from generative answers in DesignMinds. Similarly, prior research has demonstrated that
well-structuredinstructionsindesigntaskscanplayasignificantroleinelicitinghigherlevelsof
originalityandfosteringabroaderrangeofideationamongpractitioners[54].Additionally,we
observedapositivecorrelationbetweenthelengthofthequestionsaskedbyparticipantsandthe
originalityoftheirideation.Thissuggeststhatthequalityandquantityofthegeneratedanswers
maybeinfluencedbythelevelofdetailinthequeryinput.Thisfindingbringsourattentiononthe
necessityofensuringthathumanpractitionersprovidemoredetailedrequests,clearlyexplaining
theirneedsinthecontextofthecurrentcircumstancesinfuturestudies.
6.3 NoDeclineinUXorTechnologyAcceptanceandUsewiththeIntroductionofNew
Technology
Whenintroducingnewtechnologyintoexistingpractices,practitionersmaystrugglewiththe
adaptation process. Technostress, for example, is a phenomenon where individuals are unable
toworkwithnewinformationandcommunicationtechnologies(ICT)intheirwork[61].This
difficultycanleadtoadecreaseinproductivityandcreativity[9].Previousliteraturehasshown
thatdiscomfortwithnewlyintroducedtoolsoftenmanifestsasadeclineinUXandintheratingsof
technologyacceptanceandusage[33,60,62].Suchadeclinecanpotentiallyleadtoineffectiveuse
ofthenewtechnologyandmismeasurementofitsactualfunctionality.GiventhattheDesignMinds18 Heetal.
integratesemergingICTcomponents,weareparticularlyinterestedinunderstandingwhetherthe
prototypeaffectsUXandtechnologyacceptanceandusescorescomparedtothebaseline(RQ3).
InSection5.3,theanalysisofself-reportedscoresfromtwoseparatequestionnairesrevealedno
significantdifferencesbetweentheexperimentalandcontrolgroups.Thissuggeststhatparticipants
inbothgroupsexhibitedsimilarlevelsoftasksatisfactionandwillingnesstoacceptandusethe
prototype.Assuch,theproposedDesignMindsdidnotnegativelyimpactthenormaldesign
ideationexperienceanddidnotaltertheoriginaluseandacceptanceofthetechnology.
Additionally,whileweobservedlowerratingsforcertainattributes,suchasperceiveddependability,
stimulation,andnoveltywithintheuserexperience,thesevariationsdonotimpactouroverall
findings of no significant difference in attribute scores. This may be attributable to individual
attitudestowardstheselecteddesignscenarios,asdesignisinherentlyinfluencedbysentiments
andemotions.WeanticipatethatfuturestudiesinvolvingdifferentVBDusecasesmayyieldhigher
scores,thoughthepatternofresultsisexpectedtoremainconsistent.
6.4 LimitationsandFutureWork
WhileDesignMindsshowssignificantpotentialforenhancingideationinVBD,severallimitations
warrantfurtherinvestigation.Ininformalpost-experimentdiscussions,someparticipantsexpressed
concernsaroundtransparencyandtrustwhenusingLLMsincreativeprocesses.Oneoftheprimary
challengesidentifiedistheriskof"hallucinations,"acommonissueinAI-driventoolswheremodels
provideconvincingyetincorrectinformation[31,39].Thismayincreaseconfidenceincreative
tasks but can also lead to biased or flawed outcomes [48]. To mitigate this risk, we integrated
theRAGmechanism[36]intoDesignMinds.Accordingtopriorliterature,RAGhelpsaddressthe
issue of generating inaccurate information by enabling the system to retrieve and incorporate
task-centric,contextual-relevant,andfactual-groundedcontent[58].Inthefuturework,weaimto
furtherenhanceDesignMinds’transparencybyintegratingmoreinterpretableoutputs,suchas
providingcitationlinkstocredibleliteratureinanswers[38]whichallowdesignerstotracethe
rationalebehindgeneratedsuggestions.
AnotherlimitationistheneedtotestDesignMindsacrossabroaderrangeofVBDusecases.While
DesignMindsprovedeffectiveinassistingdesignideationwithinthetwospecificcontextsofcooking
andconstruction,real-worldapplicationsinvolveamuchwiderdiversityofdesigntasksthatmay
demandmoreflexibletoolsandanexpandedknowledgebase.Inthisstudy,wepredefinedthe
designbooksforDesignMinds’sknowledgerepositorybasedonselectionsmadebyanindependent
committeetoalignwiththestudy’stasks.However,futureworkcouldallowdesignerstopersonalize
theknowledgebasebyselectinganduploadingtheirowndomain-specificresourcesthrougha
non-programmer-friendly interface. For example, platforms like AnythingLLM10 enable users
tochoosetheirownLLMmodelsandindexeddocumentswhichcouldpotentiallyofferamore
tailoredandflexibleapproachtoideationassistance.Furthermore,ourcurrentimplementation
of ideation assistance offers a fixed level of support to all users. However, design ideation is a
highlyindividualizedprocess,withvaryingneedsforinspirationandsuggestionsbasedonthe
designer’sexperience[70,71].Toaddressthisvariability,weallowedparticipantsinthestudyto
criticallyconsidertheirdependabilityoftheassistanceaccordingtotheirownpreferences,giving
themthefreedomtochoosewhichaspectsofideationassistancetoutilizeandwhattorecordin
theDivergentThinkingprocess.Theconsistentlevelofsupportwasmaintainedtoensureafair
comparisonandtoisolateDesignMinds’impactonideation.Infutureiterations,wecouldconsider
DesignMindsasaproductandimplementatunablefeaturethatallowsuserstoadjustthelevel
of"helpfulness"inguidingthedesignideationprocess.Weexpectthiswouldenabledesignersto
10https://anythingllm.com/(lastaccessed:November7,2024)DesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 19
controltheamountofinformationprovidedaccordingtotheirneedsandmakesthetoolmore
responsivetoindividualpreferences.
7 Conclusion
TheadvancementofgenerativeAIhassubstantiallytransformedhumanworkinrecentyears.In
VBDdesign,thereremainsanurgentneedtoreducetheburdenofmanualvideoanalysisand
accelerateprofessionalideation.Priorresearchacrossmultipledisciplineshasdemonstratedefforts
toharnessthepowerofgenerativeAItoaugmentdesignideation.Inthispaper,wepresentDesign-
Minds,aprototypethatelevatesideationassistanceforVBDtoahigherlevel.Utilizingadvanced
techniquesfromgenerativeAI,ourDesignMindscanautomaticallyextractinformationfromvideos,
integratewithprofessionaldesignguidelinesfromindexedliterature,andprovidedesign-and
case-centricrecommendationstoinspiredesigners.OurfindingsdemonstratethatDesignMinds
significantlyimprovesideationoutcomesintermsofflexibilityandoriginalityinDivergentThink-
ing.Throughcognitivemonitoringviaeye-trackingandchatloganalysis,weobservedincreased
engagementindesignideationwhenusingDesignMinds.Furthermore,assessmentsofUXand
technologyacceptanceanduseindicatedthattheintroductionofthistooldidnotcontributeto
increasedstressandensurestherewillbeasmoothintegrationintotheexistingVBDworkflowin
future.
References
[1] SelcukAcarandMarkA.Runco.2019.Divergentthinking:Newmethods,recentresearch,andextendedtheory.13,2
(2019),153–158. https://doi.org/10.1037/aca0000231Place:USPublisher:EducationalPublishingFoundation.
[2] ÖmerAkin.1994.CreativityinDesign.7,3(1994),9–21. https://doi.org/10.1111/j.1937-8327.1994.tb00633.x_eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1937-8327.1994.tb00633.x.
[3] SamuelAttuquayefioandHillarAddo.2014.UsingtheUTAUTmodeltoanalyzestudents’ICTadoption.10,3(2014).
https://www.learntechlib.org/p/148478/Publisher:OpenCampus,TheUniversityoftheWestIndies,WestIndies.
[4] HasanAyaz,PatriciaA.Shewokis,ScottBunce,KurtulusIzzetoglu,BenWillems,andBanuOnaral.2012.Opticalbrain
monitoringforoperatortrainingandmentalworkloadassessment. 59,1(2012),36–47. https://doi.org/10.1016/j.
neuroimage.2011.06.023
[5] FabioBabiloni.2019.MentalWorkloadMonitoring:NewPerspectivesfromNeuroscience.InHumanMentalWorkload:
ModelsandApplications(Cham)(CommunicationsinComputerandInformationScience),LucaLongoandMariaChiara
Leva(Eds.).SpringerInternationalPublishing,3–19. https://doi.org/10.1007/978-3-030-32423-0_1
[6] JohnBaer.2014.CreativityandDivergentThinking:ATask-SpecificApproach.PsychologyPress. https://doi.org/10.
4324/9781315806785
[7] FlorianBordes,RichardYuanzhePang,AnuragAjay,AlexanderC.Li,AdrienBardes,SuzannePetryk,OscarMañas,
ZhiqiuLin,AnasMahmoud,BargavJayaraman,MarkIbrahim,MelissaHall,YunyangXiong,JonathanLebensold,
CandaceRoss,SrihariJayakumar,ChuanGuo,DianeBouchacourt,HaiderAl-Tahan,KarthikPadthe,VasuSharma,Hu
Xu,XiaoqingEllenTan,MeganRichards,SamuelLavoie,PietroAstolfi,ReyhaneAskariHemmat,JunChen,Kushal
Tirumala,RimAssouel,MazdaMoayeri,ArjangTalattof,KamalikaChaudhuri,ZechunLiu,XilunChen,Quentin
Garrido,KarenUllrich,AishwaryaAgrawal,KateSaenko,AsliCelikyilmaz,andVikasChandra.2024-05-27. An
IntroductiontoVision-LanguageModeling. https://doi.org/10.48550/arXiv.2405.17247arXiv:2405.17247[cs]
[8] DonaldJ.Campbell.1988.TaskComplexity:AReviewandAnalysis.13,1(1988),40–52. https://doi.org/10.2307/258353
Publisher:AcademyofManagement.
[9] ShaliniChandra,AnuraginiShirish,andShirishSrivastava.2019.DoesTechnostressInhibitEmployeeInnovation?
ExaminingtheLinearandCurvilinearInfluenceofTechnostressCreators. 44,1(2019). https://doi.org/10.17705/
1CAIS.04419
[10] BoT.ChristensenandLindenJ.Ball.2016.Dimensionsofcreativeevaluation:Distinctdesignandreasoningstrategies
foraesthetic,functionalandoriginalityjudgments.45(2016),116–136. https://doi.org/10.1016/j.destud.2015.12.005
[11] VicenteChulvi,ElenaMulet,AmareshChakrabarti,BelindaLópez-Mesa,andCarmenGonzález-Cruz.2012.Comparison
ofthedegreeofcreativityinthedesignoutcomesusingdifferentdesignmethods.23,4(2012),241–269. https://doi.
org/10.1080/09544828.2011.624501Publisher:Taylor&Francis_eprint:https://doi.org/10.1080/09544828.2011.624501.
[12] CarletonCoffrin,LindaCorrin,PauladeBarba,andGregorKennedy.2014.Visualizingpatternsofstudentengagement
andperformanceinMOOCs.InProceedingsoftheFourthInternationalConferenceonLearningAnalyticsAndKnowledge20 Heetal.
(NewYork,NY,USA)(LAK’14).AssociationforComputingMachinery,83–92. https://doi.org/10.1145/2567574.2567586
[13] JohnColomboandD.WayneMitchell.2014.FixationLocationandFixationDurationasIndicesofCognitiveProcessing.
InIndividualDifferencesinEarlyVisualAttention:FixationTimeandInformationProcessing.PsychologyPress.
[14] NigelCross.2004.Expertiseindesign:anoverview.25,5(2004),427–441. https://doi.org/10.1016/j.destud.2004.06.002
[15] RikkeFriisDamandTeoYuSiang.2017.Whatisideation–andhowtoprepareforideationsessions.InteractionDesign
Foundation(2017).
[16] S.Dazkir,JenniferMower,Kellyreddybest,andElainePedersen.2013.Anexplorationofdesignstudents’inspiration
process.47(2013),404.
[17] CharlesEastman.2001.NewDirectionsinDesignCognition:StudiesofRepresentationandRecall.(2001).
[18] ClaudiaEckertandMartinStacey.2000.Sourcesofinspiration:alanguageofdesign.21,5(2000),523–538. https:
//doi.org/10.1016/S0142-694X(00)00022-3
[19] AnaFucs,JulianaJansenFerreira,ViníciusC.V.B.Segura,BeatrizdePaulo,RogerioAbreuDePaula,andRenato
Cerqueira.2020.Sketch-basedVideoAStorytellingforUXValidationinAIDesignforAppliedResearch.CHIExtended
Abstracts(2020). https://doi.org/10.1145/3334480.3375221
[20] MileneGonçalves,CarlosCardoso,andPetraBadke-Schaub.[n.d.].Inspirationchoicesthatmatter:theselectionof
externalstimuliduringideation.2([n.d.]),e10. https://doi.org/10.1017/dsj.2016.10
[21] CanadianCentreforOccupationalHealthGovernmentofCanadaandSafety.2023. CCOHS:OfficeErgonomics-
PositioningtheMonitor. https://www.ccohs.ca/oshanswers/ergonomics/office/monitor_positioning.htmlLastModified:
2023-06-13.
[22] KristenGrauman,AndrewWestbury,EugeneByrne,ZacharyChavis,AntoninoFurnari,RohitGirdhar,Jackson
Hamburger,HaoJiang,MiaoLiu,XingyuLiu,MiguelMartin,TusharNagarajan,IlijaRadosavovic,SanthoshKumar
Ramakrishnan,FionaRyan,JayantSharma,MichaelWray,MengmengXu,EricZhongcongXu,ChenZhao,Siddhant
Bansal,DhruvBatra,VincentCartillier,SeanCrane,TienDo,MorrieDoulaty,AkshayErapalli,ChristophFeichtenhofer,
AdrianoFragomeni,QichenFu,AbrhamGebreselasie,CristinaGonzalez,JamesHillis,XuhuaHuang,YifeiHuang,
WenqiJia,WeslieKhoo,JachymKolar,SatwikKottur,AnuragKumar,FedericoLandini,ChaoLi,YanghaoLi,Zhenqiang
Li,KarttikeyaMangalam,RaghavaModhugu,JonathanMunro,TullieMurrell,TakumiNishiyasu,WillPrice,PaolaRuiz
Puentes,MereyRamazanova,LedaSari,KiranSomasundaram,AudreySoutherland,YusukeSugano,RuijieTao,Minh
Vo,YuchenWang,XindiWu,TakumaYagi,ZiweiZhao,YunyiZhu,PabloArbelaez,DavidCrandall,DimaDamen,
GiovanniMariaFarinella,ChristianFuegen,BernardGhanem,VamsiKrishnaIthapu,C.V.Jawahar,HanbyulJoo,
KrisKitani,HaizhouLi,RichardNewcombe,AudeOliva,HyunSooPark,JamesM.Rehg,YoichiSato,JianboShi,
MikeZhengShou,AntonioTorralba,LorenzoTorresani,MingfeiYan,andJitendraMalik.2021.Ego4D:Aroundthe
Worldin3,000HoursofEgocentricVideo. https://arxiv.org/abs/2110.07058v3
[23] JoshuaGuetzkow,MichèleLamont,andGrégoireMallard.2004.WhatisOriginalityintheHumanitiesandtheSocial
Sciences?69,2(2004),190–212. https://doi.org/10.1177/000312240406900203Publisher:SAGEPublicationsInc.
[24] J.P.Guilford.1950.Creativity.5,9(1950),444–454. https://doi.org/10.1037/h0063487Place:USPublisher:American
PsychologicalAssociation.
[25] J.P.Guilford.1967. Creativity:Yesterday,TodayandTomorrow. 1,1(1967),3–14. https://doi.org/10.1002/j.2162-
6057.1967.tb00002.x_eprint:https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2162-6057.1967.tb00002.x.
[26] JohnM.Hinson,TinaL.Jameson,andPaulWhitney.2003.Impulsivedecisionmakingandworkingmemory.29,2
(2003),298–306. https://doi.org/10.1037/0278-7393.29.2.298
[27] BertHoeksandWillemJ.M.Levelt.1993-03-01.Pupillarydilationasameasureofattention:aquantitativesystem
analysis.25,1(1993-03-01),16–26. https://doi.org/10.3758/BF03204445
[28] JamesE.HoffmanandBaskaranSubramaniam.1995.Theroleofvisualattentioninsaccadiceyemovements.57,6
(1995),787–795. https://doi.org/10.3758/BF03206794
[29] DavidE.Irwin.2004.FixationLocationandFixationDurationasIndicesofCognitiveProcessing.InTheInterfaceof
Language,Vision,andAction.PsychologyPress. NumPages:29.
[30] DavidG.JanssonandStevenM.Smith.1991. Designfixation. 12,1(1991),3–11. https://doi.org/10.1016/0142-
694X(91)90003-F
[31] ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,DanSu,YanXu,EtsukoIshii,YeJinBang,AndreaMadotto,and
PascaleFung.2023-03-03.SurveyofHallucinationinNaturalLanguageGeneration.55,12(2023-03-03),248:1–248:38.
https://doi.org/10.1145/3571730
[32] OliviaE.Kang,KatherineE.Huffer,andThaliaP.Wheatley.2014. PupilDilationDynamicsTrackAttentionto
High-LevelInformation.9,8(2014),e102463. https://doi.org/10.1371/journal.pone.0102463Publisher:PublicLibrary
ofScience.
[33] ZuheirN.Khlaif,MageswaranSanmugam,andAbedulkarimAyyoub.2023.ImpactofTechnostressonContinuance
IntentionstoUseMobileTechnology.32,2(2023),151–162. https://doi.org/10.1007/s40299-021-00638-xDesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 21
[34] KyungsunKim,JeongyunHeo,andSanghoonJeong.2021.ToolorPartner:TheDesigner’sPerceptionofanAI-Style
GeneratingService.InArtificialIntelligenceinHCI:SecondInternationalConference,AI-HCI2021,HeldasPartof
the23rdHCIInternationalConference,HCII2021,VirtualEvent,July24–29,2021,Proceedings(Berlin,Heidelberg).
Springer-Verlag,241–259. https://doi.org/10.1007/978-3-030-77772-2_16
[35] WoocheolKim,JudithA.Kolb,andTaesungKim.2013.TheRelationshipBetweenWorkEngagementandPerformance:
AReviewofEmpiricalLiteratureandaProposedResearchAgenda.12,3(2013),248–276. https://doi.org/10.1177/
1534484312461635Publisher:SAGEPublications.
[36] PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichKüttler,
MikeLewis,Wen-tauYih,TimRocktäschel,SebastianRiedel,andDouweKiela.2020.Retrieval-AugmentedGeneration
forKnowledge-IntensiveNLPTasks.InProceedingsofthe34thInternationalConferenceonNeuralInformationProcessing
Systems(Vancouver,BC,Canada)(NIPS’20).CurranAssociatesInc.,RedHook,NY,USA,Article793,16pages.
[37] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.2023.BLIP-2:BootstrappingLanguage-ImagePre-trainingwith
FrozenImageEncodersandLargeLanguageModels.InProceedingsofthe40thInternationalConferenceonMachine
Learning(ProceedingsofMachineLearningResearch,Vol.202),AndreasKrause,EmmaBrunskill,KyunghyunCho,
BarbaraEngelhardt,SivanSabato,andJonathanScarlett(Eds.).PMLR,19730–19742. https://proceedings.mlr.press/
v202/li23q.html
[38] WeitaoLi,JunkaiLi,WeizhiMa,andYangLiu.2024. Citation-EnhancedGenerationforLLM-basedChatbots.
https://doi.org/10.48550/arXiv.2402.16063arXiv:2402.16063[cs]
[39] StephanieLin,JacobHilton,andOwainEvans.2022.TruthfulQA:MeasuringHowModelsMimicHumanFalsehoods.
InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers)
(Dublin,Ireland),SmarandaMuresan,PreslavNakov,andAlineVillavicencio(Eds.).AssociationforComputational
Linguistics,3214–3252. https://doi.org/10.18653/v1/2022.acl-long.229
[40] J.S.Linsey,I.Tseng,K.Fu,J.Cagan,K.L.Wood,andC.Schunn.2010.AStudyofDesignFixation,ItsMitigationand
PerceptioninEngineeringDesignFaculty.132,41003(2010). https://doi.org/10.1115/1.4001110
[41] BelindaLópez-Mesa,ElenaMulet,RosarioVidal,andGrahamThompson.2011. Effectsofadditionalstimulion
idea-findingindesignteams. 22,1(2011),31–54. https://doi.org/10.1080/09544820902911366 Publisher:Taylor&
Francis.
[42] WendyE.MackayandAnneLaureFayard.1999.Videobrainstormingandprototyping:techniquesforparticipatory
design.InCHI’99ExtendedAbstractsonHumanFactorsinComputingSystems(NewYork,NY,USA)(CHIEA’99).
AssociationforComputingMachinery,118–119. https://doi.org/10.1145/632716.632790
[43] WendyE.Mackay,AnneV.Ratzer,andPaulJanecek.2000. Videoartifactsfordesign:bridgingtheGapbetween
abstractionanddetail.InProceedingsofthe3rdconferenceonDesigninginteractivesystems:processes,practices,methods,
andtechniques(NewYorkCityNewYorkUSA).ACM,72–82. https://doi.org/10.1145/347642.347666
[44] AntonioMaffeiandAlessandroAngrilli.2019.Spontaneousblinkrateasanindexofattentionandemotionduring
filmclipsviewing.204(2019),256–263. https://doi.org/10.1016/j.physbeh.2019.02.037
[45] LianeMakatura,MichaelFoshey,BohanWang,FelixHähnLein,PingchuanMa,BoleiDeng,MeganTjandrasuwita,
AndrewSpielberg,CrystalElaineOwens,PeterYichenChen,AllanZhao,AmyZhu,WilJ.Norton,EdwardGu,Joshua
Jacob,YifeiLi,AdrianaSchulz,andWojciechMatusik.2023.HowCanLargeLanguageModelsHelpHumansinDesign
andManufacturing? https://doi.org/10.48550/arXiv.2307.14377arXiv:2307.14377[cs]
[46] OpenAI.2023.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL]
[47] LaurencePaquetteandThomasKida.1988.Theeffectofdecisionstrategyandtaskcomplexityondecisionperformance.
41,1(1988),128–142. https://doi.org/10.1016/0749-5978(88)90051-9
[48] RajaParasuramanandDietrichH.Manzey.2010.ComplacencyandBiasinHumanUseofAutomation:AnAttentional
Integration.52,3(2010),381–410. https://doi.org/10.1177/0018720810376055Publisher:SAGEPublicationsInc.
[49] CyrilPicard,KristenM.Edwards,AnnaC.Doris,BrandonMan,GiorgioGiannone,MdFerdousAlam,andFaezAhmed.
2023.FromConcepttoManufacturing:EvaluatingVision-LanguageModelsforEngineeringDesign. arXiv:2311.12668
[cs] http://arxiv.org/abs/2311.12668
[50] JebaRezwanaandMaryLouMaher.2023.DesigningCreativeAIPartnerswithCOFI:AFrameworkforModeling
InteractioninHuman-AICo-CreativeSystems.30,5(2023),1–28. https://doi.org/10.1145/3519026
[51] MarkA.Runco.2010. Chapter12:DivergentThinking,CreativityandIdeation. InTheCambridgeHandbookof
Creativity.CambridgeUniversityPress.
[52] Mark A. Runco and Selcuk Acar. 2024. Divergent Thinking as an Indicator of Creative Poten-
tial. 24, 1 (2024), 66–75. https://doi.org/10.1080/10400419.2012.652929 Publisher: Routledge _eprint:
https://doi.org/10.1080/10400419.2012.652929.
[53] MarkA.RuncoandShawnM.Okuda.1988.Problemdiscovery,divergentthinking,andthecreativeprocess.17,3
(1988),211–220. https://doi.org/10.1007/BF0153816222 Heetal.
[54] M. A. Runco and S. M. Okuda. 1991. The instructional enhancement of the flexibility and originality
scores of divergent thinking tests. 5, 5 (1991), 435–441. https://doi.org/10.1002/acp.2350050505 _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/acp.2350050505.
[55] MartinSchrepp,AndreasHinderks,andJörgThomaschewski.2014. ApplyingtheUserExperienceQuestionnaire
(UEQ)inDifferentEvaluationScenarios.383–392. https://doi.org/10.1007/978-3-319-07668-3_37
[56] MartinSchrepp,AndreasHinderks,andJörgThomaschewski.2017. ConstructionofaBenchmarkfortheUser
ExperienceQuestionnaire(UEQ).4(2017),40–44. https://doi.org/10.9781/ijimai.2017.445
[57] RossitzaSetchiandCaroleBouchard.2010.InSearchofDesignInspiration:ASemantic-BasedApproach.10,31006
(2010). https://doi.org/10.1115/1.3482061
[58] KurtShuster,SpencerPoff,MoyaChen,DouweKiela,andJasonWeston.2021. RetrievalAugmentationReduces
HallucinationinConversation. https://doi.org/10.48550/arXiv.2104.07567arXiv:2104.07567[cs]
[59] MiriamSpering.2022.EyeMovementsasaWindowintoDecision-Making.8(2022),427–448.IssueVolume8,2022.
https://doi.org/10.1146/annurev-vision-100720-125029Publisher:AnnualReviews.
[60] Monideepa Tarafdar, Ellen Bolman. Pullins, and T. S. Ragu-Nathan. 2015. Technostress: negative effect on
performance and possible mitigations. 25, 2 (2015), 103–132. https://doi.org/10.1111/isj.12042 _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1111/isj.12042.
[61] MonideepaTarafdar,QiangTu,BhanuS.Ragu-Nathan,andT.S.Ragu-Nathan.2007.TheImpactofTechnostresson
RoleStressandProductivity.24,1(2007),301–328. https://doi.org/10.2753/MIS0742-1222240109Publisher:Routledge
_eprint:https://doi.org/10.2753/MIS0742-1222240109.
[62] MonideepaTarafdar,QiangTu,andT.S.Ragu-Nathan.2010.ImpactofTechnostressonEnd-UserSatisfactionand
Performance. 27,3(2010),303–334. https://doi.org/10.2753/MIS0742-1222270311 Publisher:Routledge_eprint:
https://doi.org/10.2753/MIS0742-1222270311.
[63] DeborahTatar.1989.Usingvideo-basedobservationtoshapethedesignofanewtechnology.ACMSIGCHIBulletin
21,2(1989),108–111. https://doi.org/10.1145/70609.70628
[64] ViswanathVenkatesh,MichaelG.Morris,GordonB.Davis,andFredD.Davis.2003.UserAcceptanceofInformation
Technology:TowardaUnifiedView. https://papers.ssrn.com/abstract=3375136
[65] LaurieVertelney.1989. Usingvideotoprototypeuserinterfaces. ACMSIGCHIBulletin21,2(1989),57–61. https:
//doi.org/10.1145/70609.70615
[66] VimalK.ViswanathanandJulieS.Linsey.2013.DesignFixationandItsMitigation:AStudyontheRoleofExpertise.
135,51008(2013). https://doi.org/10.1115/1.4024123
[67] MichaelDWilliams,NripendraPRana,andYogeshKDwivedi.2015.Theunifiedtheoryofacceptanceanduseof
technology(UTAUT):aliteraturereview.28,3(2015),443–488. https://doi.org/10.1108/JEIM-09-2014-0088Publisher:
EmeraldGroupPublishingLimited.
[68] ShengyangXu,YaoWei,PaiZheng,JiaZhang,andChunyangYu.2024.LLMenabledgenerativecollaborativedesign
inamixedrealityenvironment.74(2024),703–715. https://doi.org/10.1016/j.jmsy.2024.04.030
[69] Xiaotong(Tone)Xu,JiayuYin,CatherineGu,JennyMar,SydneyZhang,JaneL.E,andStevenP.Dow.2024.Jamplate:
ExploringLLM-EnhancedTemplatesforIdeaReflection.InProceedingsofthe29thInternationalConferenceonIntelligent
UserInterfaces(NewYork,NY,USA)(IUI’24).AssociationforComputingMachinery,907–921. https://doi.org/10.
1145/3640543.3645196
[70] SaluYliriskuandJacobBuur.2007.Makingsenseandeditingvideos.InDesigningwithvideo:Focusingtheuser-centred
designprocess.SpringerLondon,86—-135. https://doi.org/10.1007/978-1-84628-961-3_2
[71] SaluYliriskuandJacobBuur.2007.Studyingwhatpeopledo.InDesigningwithvideo:Focusingtheuser-centreddesign
process.SpringerLondon,36–85. https://doi.org/10.1007/978-1-84628-961-3_2
[72] RobertJ.YoumansandThomazArciszewski.2014.Designfixation:Classificationsandmodernmethodsofprevention.
28,2(2014),129–137. https://doi.org/10.1017/S0890060414000043
[73] CarmenZahn,RoyPea,FriedrichW.Hesse,andJoeRosen.2010.ComparingSimpleandAdvancedVideoToolsasSup-
portsforComplexCollaborativeDesignProcesses.19,3(2010),403–440. https://doi.org/10.1080/10508401003708399
Publisher:Routledge_eprint:https://doi.org/10.1080/10508401003708399.
[74] MinZhao,TimothyM.Gersch,BrianS.Schnitzer,BarbaraA.Dosher,andEileenKowler.2012.Eyemovementsand
attention:Theroleofpre-saccadicshiftsofattentioninperception,memoryandthecontrolofsaccades.74(2012),
40–60. https://doi.org/10.1016/j.visres.2012.06.017
[75] ChuyiZhou,XiyuanZhang,andChunyangYu.2024.HowdoesAIpromotedesigniteration?Theoptimaltimeto
integrateAIintothedesignprocess. 0,0(2024),1–28. https://doi.org/10.1080/09544828.2023.2290915 Publisher:
Taylor&Francis_eprint:https://doi.org/10.1080/09544828.2023.2290915.
[76] GengzeZhou,YicongHong,andQiWu.2023.NavGPT:ExplicitReasoninginVision-and-LanguageNavigationwith
LargeLanguageModels. https://doi.org/10.48550/arXiv.2305.16986arXiv:2305.16986[cs]DesignMinds:EnhancingVideo-BasedDesignIdeationwithVision-LanguageModelandContext-InjectedLargeLanguage
Model 23
[77] J.Zimmerman.2005.Videosketches:exploringpervasivecomputinginteractiondesigns.IEEEPervasiveComputing4,
4(2005),91–94. https://doi.org/10.1109/MPRV.2005.91
You will be shown two videos. Your task is to analyze the videos and pinpoint
processesormethodsthatcouldbeenhanced.Focusontheactivitiesandconsider
alternativetools,interactions,orcontextualimprovements.Generateandwriteoutas
manyideasaspossible.Youareencouragedtothinkoutloud.
(Please use the provided chatbot to assist you. This tool offers insights and
suggestsimprovementsbasedonthevideocontent.Typeyourquestionsorthoughts
intothechatbotanduseitsresponsestoenhanceyourideation.Forexample,ask,
"Howcantheprocessshowninthevideobeimproved?")
You will have 15 minutes to engage with each video. Please use your time
effectivelyanddocumentasmanyideasaspossible.Pleasenotethatvideosdonot
havesound.Youwillbenotifiedafter12minutesofthetime.
When you are ready to proceed press the "Start" button and the arrow "→"
onthebottomrightsideofthescreen.
SupplementaryText1. InstructionaltextdisplayedintheNote-takingSpaceinFig.3.Textwithinparentheses
(thesecondparagraph)wasshownonlytoparticipantsintheexperimentalgroupwithaccesstoDesignMinds.
• Flexibility:Eachcomprehensiveideawhichportrayingthepurposeandfunction-
alityinsufficientdetailtobeunderstandablegivesa+1point.
• Flexibility:Givea+1pointforeachnewdomain/subdomainisspottedbasedon
theideationcontextacrossallparticipants.
• Originality:Agradebasedonthestatisticalinfrequencyofideasmeasuredona
7-pointLikertscale.
SupplementaryText2. PredeterminedcriteriabasedonGuilford’sstudy[24]forevaluatingfluency,flexibility,
andoriginalityindivergentthinkingtextsbyindependentraters.