Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For
Autonomous Visual Robot Navigation
Shreya Gummadi1, Mateus V. Gasparino1, Deepak Vasisht2, and Girish Chowdhary1
Abstract—Centralized learning requires data to be aggre- hance their learning by sharing model updates. With recent
gated at a central server, which poses significant challenges in advancements in the capabilities of edge devices, federated
terms of data privacy and bandwidth consumption. Federated
learningtakesadvantageofedgecomputationtotrainmodels
learning presents a compelling alternative, however, vanilla
locally and shares model parameters instead of raw data
federated learning methods deployed in robotics aim to learn
a single global model across robots that works ideally for all. with the server to learn a shared global model. FL also
Butinpracticeonemodelmaynotbewellsuitedforrobotsde- allows robots to send updates at intervals, rather than con-
ployedinvariousenvironments.ThispaperproposesFederated- tinuously streaming data reducing bandwidth usage. Further,
EmbedCluster (Fed-EC), a clustering-based federated learning
through federated learning, there is hope that robots can
frameworkthatisdeployedwithvisionbasedautonomousrobot
gatherinsightsfromtheirrespectiveenvironments,whilealso
navigation in diverse outdoor environments. The framework
addresses the key federated learning challenge of deteriorating contributingtoaglobalpoolofknowledgetolearnadaptable
modelperformanceofasingleglobalmodelduetothepresence models for varied environments on the go. Traditionally, FL
of non-IID data across real-world robots. Extensive real-world learns a single model that tries to minimize the average
experiments validate that Fed-EC reduces the communication
loss across robots. However, local data on deployed robots
size by 23x for each robot while matching the performance
is highly non-IID due to different usage and operating
of centralized learning for goal-oriented navigation and out-
performs local learning. Fed-EC can transfer previously learnt locations. During FL, the divergence of the local datasets
models to new robots that join the cluster. due to their non-IID nature leads to slower convergence
and worsening learning performance when the models are
I. INTRODUCTION
aggregated. In such cases, a singular global model suffers
Pooravailabilityofhigh-speedinternetislimitingoutdoor and may perform worse than local models for some robots.
robots from realizing their full potential. In today’s world, With non-IID data, it is improbable that there exists a single
robots are seamlessly deployed in diverse conditions all global model that fits the needs for all robots. The global
over the world, from bustling urban landscapes to rugged modelcanbebiasedandunfair.Currentroboticsystemsthat
terrains in the wild. Many of these robots are using visually use federated learning frameworks do so in simulation [14]
guidedautonomyarchitecturespoweredbymachinelearning orinstructuredindoorenvironments[15]anddonotaccount
and self-supervision. Recent works [1], [2], [3], [4], [5] for heterogeneity that arises in the real-world deployment of
have shown that with access to large amounts of data, robots.
robots can achieve state-of-the-art navigation performance One way to avoid biased global models is to learn per-
and can be deployed in various scenarios with minimum sonalized models by clustering robots with similar local
human intervention needed. These and other such methods data distributions and training one aggregate model for each
are driving tremendous progress in self-driving cars [6], [7], cluster. As a result, robots collaborate with only robots
robotsnavigationinindoor[8],[9]andoutdoorenvironments withsimilarexperiencesavoidingbiasesandnegativeperfor-
[10], [11], [12]. However, in practice, traditional learning mance. Previous clustered FL methods compare local model
approaches require access to all of the data in one place, weights or gradients that rely on indirect information of the
uploaded to a central server for model training requiring data distribution. [16] and [17] cluster the clients and learn
high speed internet. Furthermore, robots operating in the individual cluster models but incur a high communication
world experience diverse and varied environments requiring cost in doing so.
continuous upload of large amounts of data to the central In this paper, we highlight the first clustering-based sys-
server. While effective in controlled environments with high tem, Federated-EmbedCluster(Fed-EC) for self-supervised
bandwidths,uploadingbigchunksofdatacanbeachallenge visually guided autonomous navigation which overcomes
for robots in environments where high-speed internet is not the need for high bandwidth speeds. Fed-EC is deployed
available, is intermittent, outright denied or even leads to on two different visual navigation models to showcase its
significant battery power consumption. modularity. To overcome the negative affect of non-IID data
Federated learning (FL) [13] reduces the bandwidth re- on model performance, Fed-EC groups the aggregation of
quirement while enabling these robots to collectively en- local models by looking at similarity between the local
datasets. Within each cluster group, the data is similar
1Field Robotics Engineering and Science Hub (FRESH), Illinois Au- and mimics an IID set up ensuring that model aggregation
tonomous Farm, University of Illinois at Urbana-Champaign (UIUC), IL,
does not degrade performance. Unlike previous methods
USA(Email:gummadi4@illinois.edu)
2Dept.ofComputerScience,UIUC,IL,USA where multiple rounds are needed [16] or multiple models
4202
voN
6
]OR.sc[
1v21140.1142:viXraare communicated [17], in each communication round the Clustering Algorithm IFCA, that initializes k global models
mean embedding vector which does not incur any additional at the server which is sent to each client to compute
communication cost is shared along with the local model. loss. Clients are assigned to the cluster that produces the
Fed-EC does not know the cluster identities beforehand and smallest loss on the local client data. This method incurs
hence simultaneously identifies clusters within participating a high communication overhead due to k models being
robots and learns individual cluster models in the federated broadcast. [27] uses the difference between the local and
setting. the global model weights to perform hierarchical clustering.
In this paper, we consider robots in the wild that are [28]introducedFedGroupwhichusesEuclideandistanceof
constantly deployed with limited hardware on board, limited decomposedcosinesimilaritymetrictoclustertheclients.k-
communication bandwidth, and battery power. The main FED designed by [29] uses Llyod’s method for K-means
contributions of our papers are as follows: clustering to cluster the clients. However, the method is
• Weproposeaclustering-basedpersonalizedFLstrategy sensitive to the initialization of the centers and can take
Fed-EC, to overcome the problems generated by the superpolynomialtimetoconverge.Inallpreviousworks,the
heterogeneous nature of robotic operations. authorsusethelocalmodelweightstoindirectlyapproximate
• We implement and test the framework of Federated thelocaldatasets.Moreoverthepreviousworksarenottested
learning in the robotics settings, in particular on real forroboticsapplicationsandsufferfromhighcommunication
robotsusingtwodifferentnavigationmodelstonavigate costswhichisofhighimportancefordeployingrobotsinthe
to a given GPS point. wild.
• We validate through real-world robot experiments in Federated Learning in Robotics FL is increasingly used
diverse outdoor terrains that Fed-EC can perform as in numerous robotics applications. However, none of the FL
well as the centralized framework while reducing com- applications in robotics explicitly account for non-IID data,
munication size and is better than just local training. and deployment in real-world outdoor environments which
We also show that learning a personalized FL model leads to changing datasets and setting up server-robot com-
for each cluster is better than learning a singular global munication. [30] uses federated learning to achieve lifelong
FL model over all robots. federated reinforcement learning to improve the efficiency
• We also show the transferability properties of our sys- of robotic navigation. But the method is limited to testing in
tem to new robots that join the network. simulation and using the best model learnt in simulation to
testnavigationusingTurtle-botinanindoorspace.Similarly,
II. RELATEDWORK
[15] also uses federated learning for reinforcement learning
Federated Learning Federated Learning (FL) [13], [18], for robotic swarm navigation and tests in simulation and
[19], [20], [21] as mentioned in I is a distributed learning a turtle-bot in a limited indoor real-world scenario. The
framework addressing privacy concerns, communication ef- method accounts for communication in simulation using
ficiency and enabling collaborative model training. FedAvg communication volume but does not establish a server-robot
introduced by [13] is the most widely used FL algorithm communicationinrealtimedeployment.[31]and[32]apply
that learns one global model for all participating clients by vanillafederatedlearningtoautonomousdrivingbutonlytest
aggregating local model updates from them. A key problem it on simulated scenarios. [32] uses a server-less federated
in FL is that individual local data on clients are usually learning architecture by using peer-to-peer communication
non-IID. Several research has focused on providing possible instead. [33] also uses a server-less federated architecture
solutions for data heterogeneity in FL [22], [23], [24], [25]. and instead uses a gossip-based shared data structure for
These methods learn a single global model for non-IID trajectory forecasting. Previous methods do not personalize
data. One such algorithm, proposed by [25] bounds the the federated models for heterogeneous data. Our method
differences between the local and global optimizations by takes the problems of real-world federated deployment into
introducing a proximal term to the local objective function. account and learns a personal cluster model to deal with
[26] investigates the effect of non-IID data on FL and the heterogeneity.
degradation of performance.
Clustered Federated Learning Other research focuses
III. SYSTEMDESIGN
rather on learning personalized models for each client. Figure 1, shows an overview of Fed-EC. Data collected
Clustered Federated learning (CFL) instead of optimizing oneachrobotusinganonboardRGBDcameraandIMUare
a shared global model, partitions the clients into clusters usedtocalculatethemeanembeddingaswellastrainalocal
anddividestheoptimizationgoalintoseveralsub-objectives. traversability model. Fed-EC communicates the the local
CFLlearnsmultiplemodelsforeachcluster,whicharemore model weights and the mean embedding to the server over
specializedandachievebetteraccuracy.[16]proposedaCFL WiFi. The mean embeddings are used to cluster the robots
algorithm that bi-partitions the clients based on the cosine and a global cluster model is aggregated for each cluster.
similarity of their gradients and checks for congruent parti- Fed-EC sends the respective cluster model to each robot
tion using the gradient norm. Multiple rounds are required depending on its cluster identity and training continues until
to cluster incongruent clients which has high computational the local models converge on their test data. Once a good
and communication costs. [17] proposed Iterative Federated model is learnt, given a GPS goal point, the robot navigatesFig. 1: Workflow of Fed-EC. (a) Participating robots learn and communicates a mean embedding vector and local model
weight to the server. The server clusters the robots using the mean embedding vector and aggregates local models in each
cluster to learn a model which is shared with the robots based on their cluster identity. (b) The robots navigate to the a
given GPS goal using the learnt model which takes as input RGB and depth images from the front facing camera. (c) If a
new robot is deployed it computes a mean embedding and shares it with the server. The server assigns a cluster to the robot
and sends the respective cluster model to the robot to use.
to the goal by leveraging traversability predictions from the front-facingZED2stereo-inertialcamerathatcapturescolour
local models along with the robot’s kinodynamic model to and depth images with a field of view of 120 degrees. The
solve a non-linear model predictive controller. When a new robot is also equipped with an onboard WiFi router that is
robot joins the network, Fed-EC calculates a local mean used for federated learning.
embedding and communicates it to the server to identify the
robot’s cluster identity and the relevant cluster model is sent
to the robot.
Fig. 3: Sample Data and Traversability Labels collected
across different terrains.
Fig.2:TerrasentiaRobotwithZED2cameraandGNSSused B. Data Collection
as a testbed for our experiments.
We deploy the method described in [1] to collect data.
For initial training, we manually drive the robots around in
different terrains to collect input RGBD images and state
A. Robot Platform
estimations.Thisdatasetincludestraversabilityandcollision
The Terrasentia robot as shown in figure 2, is a small labels. The traversability label is created only for the path
lightweightskid-steermobilerobotdevelopedbyEarthSense traversedbytherobotandissetto1fortraversableand0for
Incasanagriculturalroboticplatform.WeintegratedaJetson untraversable areas. Figure 3 shows sample data collected in
AGX Orin computer to run model training, store data and three different terrains along with their traversability labels.
execute the navigation module. Terrasentia comes equipped During data collection, we intentionally drive the robot into
with a 6 DOF IMU, Global Navigation Satellite System untraversable areas and obstacles as shown in figure 4 to
(GNSS) and wheel encoders. On top of this, we installed a produce labels for failure cases.the best performance. Algorithm 1 and Algorithm 2 show
the process at robot and the server.
Algorithm 1 Robot
for each epoch e in E do
Train model over D
r
Update local model W
r
end for
Fig. 4: Common obstacles encountered by the robot. Left to
Initialize v =0
i
right: wall, fire hydrant,light pole, tree.
for each image I in D do
i r
v =v +f(I )
i i i
end for
C. Federated Learning with Embedding Clustering V
r
= |Dvi
r|
Upload W and V to server
r r
In this section, we present our method Fed-EC, a FL
framework that address non-IID local data by clustering
clients into distinct clusters based on their data distribution
distances. We consider a federated learning system that Algorithm 2 Server
consists of R robots deployed in the real world. Each robot for n=1 to N do
r in R has access to local data D r. Unlike vanilla federated for each robot r in R in parallel do
learningwhereaunifiedglobalmodelislearnt,inFed-ECthe Download W and V
r r
robotsaregroupedintokclustersattheserver,andindividual end for
cluster models are learnt for each cluster. k = DBSCAN(V .....V )
1 r
AtroundN ofFed-EC,Rrobotsparticipateinlocalmodel for each robot r in cluster k do
training.AttheNthround,robotrpartakesinmodeltraining G =Avg(W ) ∀r ∈k
k r
for E epochs on its local data D and updates local model Send G to robot r in cluster k
r k
W . Simultaneously, given an image I in D it is encoded end for
r i r
into an embedding v = f(I ) where f is a pretrained end for
i i
model. The robot is equipped with adequate computational
power that using a pretrained model to generate embeddings
is reasonable and not computationally expensive. The em- IV. IMPLEMENTATION
beddings are averaged to get the mean embedding V = WeimplementedFed-EConarealworldrobotdeployedin
r
(cid:80)|Dr|v
. Using embeddings Fed-EC directly encodes the semi-urban,forestlikeandurbanenvironments.Therobotas
i=1 i
visual information of the local datasets. Similar embeddings mentionedinSectionIII-Aisusedtocollectdataindifferent
represent robots deployed in similar regions or terrains. environments.Thetestbedconsistsof1realworldrobotand
The mean embeddings are a single vector with small data the collected data shuffled and then divided into 9 partitions
sizes which are easy to upload. Each robot r uploads the to mimic 9 robots in different environments to study non
updated local model W and the mean embedding V to the IID behavior. The local model is trained on the edge and
r r
server. The server deploys Density-Based Spatial Clustering communicated with the server using WiFi module installed
of Applications with Noise (DBSCAN) [34] to cluster the ontherobot.TheedgedevicehasNVIDIAJetsonAGXOrin
embeddings. We deploy DBSCAN as it is able to determine equipped with 12-core Arm CPU, 2048-core NVIDIA GPU
the number of clusters and doesn’t need to be manually and 64GB memory. It runs Ubuntu 18.04 and uses Python 3
defined beforehand. DBSCAN uses threshold distance be- and Pytorch for model training and inference. The server is
tween robots r and j as ∥V −V ∥ and minimum points in equippedwithaIntelCorei7-11800CPU,NVIDIAGeForce
r j
a cluster to identify regions of high density in the dataset RTX 3060 GPU and runs Ubuntu 18.04.
which are separated from each other by low density regions. Local Models: We deploy Fed-EC with two different
Afterdeterminingtherobotclusteridentities,thelocalmodel vision navigation models: 1. WayFAST [1], is a convolu-
weights within a cluster are aggregated to produce the kth tional neural network that takes as input RGB and depth
cluster model G . The aggregation rule is simply averaging images and outputs a traversability prediction map. The
k
as the robots in the cluster have similar data and mimic an traversability prediction represents a map in the same space
IID setting. The server sends respective cluster models to as the input image, that quantifies how well the robot can
the robots. The model embeddings are shared each round navigate in a given scenario. 2. BADGR [4], is an image
with the server to account for changing environments and based, action-conditioned predictive deep neural network
datasets. This allows the robots to be clustered in the best which uses RGB images and sequence of linear and angular
group for their needs. In case a robot is deployed in a vastly velocity commands to predict future collision events.
dissimilar area and is identified as a noise point instead of During deployment, a model predictive controller queries
being part of a cluster, the robot uses its local model to get thetraversabilityvaluesorcollisionvaluesdependingontheunderlying navigation model and uses them to minimize a even after several epochs due to its limited data. The central
cost function that drives the robot toward a goal point while baseline has access to all of the data from all the robots
jointly maximizing traversability. As a result, areas with low in one place and hence gives the ideal performance. The
traversability or collision such as trees and buildings are FedAvg model learns a single global model collaboratively,
avoided. however the average loss over all the robots is higher than
Baselines: We compare our Federated Learning frame- the local model. This is because FedAvg performs worse
work against three baseline frameworks: 1. Central Learn- than the local model for robots with smaller amounts of
ing,allofthedatacollectedattheedgeonindividualrobots data. Fed-EC on the other hand, clusters the robots based
is sent to the central server for model training, 2. Local on their mean data embedding and learns individual per-
training, model trained on the edge on local data without sonalized models for each cluster. In doing so, it reduces
collaboration between robots. 3. FedAvg [13], the server the loss over FedAvg and local models. Further, using Fed-
aggregates all local models to learn one global model. EC increases the incentive for robots to choose federated
Training Parameters: In all experiments, the baselines learning and improves the performance by diminishing the
and Fed-EC use the same model architecture. The local and negative transfer due to heterogeneity in data across robots.
central baselines are trained for 30 epochs with a batch In addition, Fed-EC takes 9mins18sec for local training and
size of 16. FedAvg and Fed-EC are trained locally for 5 computing mean embedding vector as the data is distributed
epochs with batch size 16. The size of the local training across robots which is faster than centralized method which
datasetvariesbetween450-1000samplesfromrobottorobot. takes 1hr12min52sec and is slightly slower than FedAvg’s
The minimum distance threshold for DBSCAN is manually 8min38sec because of the embedding vector computation.
tuned and the minimum points in each cluster is set to
2. A pretrained ResNet-18 is used to generate the data TABLE II: Time Taken to Communicate Data From the
embeddings. Robot to the Server for Centralized vs Federated Learning.
V. EVALUATION LearningType DataType DataSize UploadTime
Central Rosbag 1.8GB 00:20:08
TABLE I: Validation Loss and IPR of different models for
ModelWeights
Federated 78MB 00:00:42
Traversability Prediction. MeanEmbeddingVector
Model TrainingMode Avg.Loss IPR(%) LocalEpochs
Local 0.006853 - 30
Central 0.004021 - 30
WayFAST FedAvg 0.007871 60 5
Fed-EC(Ours) 0.004621 100 5
Local 0.934291 - 30
Central 0.226426 - 30
BADGR FedAvg 1.877938 40 5
Fed-EC(Ours) 0.530927 70 5
A. Model Performance
To evaluate the model performance of our method against
the baselines we use validation loss averaged over all the
Fig. 5: Effect of increasing computation per robot.
robots and incentivized participation rate (IPR) [35] which
is defined as
R B. Communication Cost
1 (cid:88)
IPR= 1(fFL(w)>flocal(w)) (1)
M r r The implementation of the Federated framework requires
r=1 robots to communicate their model updates to the server
where fFL(w) is the model performance on robot R using every few rounnds. Communication efficiency is essential
r
an FL strategy i.e. FedAvg or Fed-EC, flocal(w) is the for training. However, communicating over the internet can
r
local model performance on robot R and 1 is the indicator cause hold-ups and delays due to the asymmetric nature of
function. IPR indicates how many robots are incentivized to internetspeeds.Ingeneral,theuploadspeedsaremuchlower
participate in federated learning. than the download speeds. As seen in Table II outdoors
Table I shows the validation loss on the test data across with limited bandwidth of 12Mbps uploading raw data to
allconfigurations.Weperformasmanyepochsforlocaland the server per robot is 23x bigger than uploading model
central models as communication rounds for FedAvg and weights. This leads to centralized learning suffering delays
Fed-EC. The central model is trained at the server whereas and bottlenecks as all robots try to upload raw data. On
all the other models are trained on the edge of the robot. the other hand, model weights are uploaded in 42 seconds
Thesimplelocalbaselineperformspoorlyanddoesnotlearn and due to the nature of FL can be done asynchronously(a)
(b)
Fig. 6: Paths taken by Fed-EC and the baselines (Local, Central, FedAvg) to reach the goal in two different environments:
(a) Forest-like and (b) Parking Lot
TABLE III: Summary of Navigation Runs (successful
while still reducing the time and need for higher bandwidth.
runs/total runs) in three different environments. Trial 1:
Further,asshowninfigure5usingmorecomputationonthe
Forest like and Trial 2: Parking Lot.
edgedecreasesthenumberofcommunicationroundsneeded
to reach a target loss. We increase computation on the robot
Trial TrainingMode WayFAST BADGR
by increasing the number of epochs(E) while keeping the
Local 2/5 2/5
batch size(B) constant.
Central 5/5 4/5
Trial1 FedAvg 3/5 2/5
C. Navigation
Fed-EC(Ours) 5/5 4/5
Table III and figure 6 show results in a forest and urban Local 3/5 2/5
environment using two models WayFAST[1] and BADGR Central 4/5 4/5
Trial2 FedAvg 2/5 1/5
[4].
Fed-EC(Ours) 3/5 3/5
In the initial runs federated learning uses data previously
collectedthroughmanualdriving.Insubsequentrunsinclud-
ing in trial 2 the training incorporates data collected during
previous runs. During each run, the robot uses the newly notusedinthesamerunbutinsubsequentrunsofthetrials.
collected data to perform training and communicates with Thisensurescontinuous,incrementalimprovementwitheach
the server to update cluster models. As the training and successive trial.
communication take time, the updated cluster models are The local baseline performs poorly in both trials becauseofthelimitedtrainingdataontherobot.Thecentralbaseline
is able to reach the goal most cases for both models as it
has access to all of the data. FedAvg is able to reach the
goal in trial 1, however, it fails and performs worse than the
local baseline in the parking lot. This is due to the degraded
performance of the FedAvg model in this environment due
to the availability of limited data in this environment. Fed-
EC performs similarly to the central model and is able to
reach the goal in most runs without any intervention. It
outperforms the local model and the FedAvg model. All the
modelssufferintheparkinglotduetolimitedtrainingdatain
thatenvironment.Duetothenatureoffederatedlearningthe
datacollectedduringtheserunscanbefurtherusedtoupdate
the cluster models (Fed-EC) and global model (FedAvg) to
improve performance subsequently.
Fig. 8: New robot navigates to goal using previously learnt
global cluster model.
the respective cluster model. Fed-EC can transfer a model
which can be readily used without the need for retraining.
The new robot can participate in federated learning as well
andbettertheclustermodelwithitsnewdata.Figure8shows
a new robot navigating using model WayFAST [1] to a goal
without any intervention using the cluster model sent by the
server.
Fig. 7: Robot adapts and navigates around untraversable
Figure9showstheclustersidentifiedbyFed-ECtogroup
areas.
the robots. We observe that robots deployed in similar ter-
rainsaregroupedtogetherasthemeanembeddingvectorsof
thelocaldataareclosertoeachotherwhiledifferentterrains
D. Adaptation
occupy separate regions on the plot. The newly deployed
Due to thenature of federated learning,Fed-EC is contin-
robot is clustered in cluster 2 in the figure and is allocated
uallyupdatingandupdatingtheclustermodelwhennewdata
the cluster model of this group to perform navigation.
isavailable.Thelearningissharedacrossrobotsinthecluster
ensuring that other robots do not fail in the same scenario.
Figure 7 shows an example scenario where Fed-EC can
overcomeobstaclesbyusingupdatedmodelstoadapt.Inthis
case, the robot initially runs into a low-hanging tree branch,
assuming it can pass below it. The robot fails, however, it
recordsthisdatawhilenavigatingwhichisthenusedtotrain
the local model and in turn update the cluster model at the
server.Inasubsequentrun.therobotreceivesthenewcluster
model from the server and is able to recognize the branch
as untraversable and moves away from it to take a different
path.
Fig. 9: Visualization of 11 robots and their corresponding
E. Transferability to New Robot cluster identities using DBSCAN based on distance proxim-
In the real world, new robots can be acquired at any point ity of mean embedding vectors.
and deployed in different locations. Usually, this requires
training a new model for the robot data or sending the data
toaservertofinetuneabasemodel.ModelslikeFedAvgcan
VI. CONCLUSION
provide an initial model that can be finetuned on the local Toconclude,wepresentedafederatedlearningframework
data. However, if the initial model is bad it will not offer thatcanbedeployedwithvariousnavigationmodelsonreal-
significant benefit to just local training. When a new robot world robots to navigate to a given GPS point under limited
joins, Fed-EC can easily use the mean embedding vector of communicationconstraints.Wedemonstratethatourmethod
thenewrobottoidentifywhichclusteritbelongstoandshare performs comparatively to centralized learning without theneed for data to be uploaded to a server and reducing [15] S.Na,T.Roucˇek,J.Ulrich,J.Pikman,T.s.Krajn´ık,B.Lennox,and
the communication size. Fed-EC also performs better than F.Arvin,“Federatedreinforcementlearningforcollectivenavigation
ofroboticswarms,”IEEETransactionsonCognitiveandDevelopmen-
local models which are limited by no collaboration. Our
talSystems,pp.1–1,2023.
framework also tackles data imbalance and outperforms Fe- [16] F.Sattler,K.-R.Mu¨ller,andW.Samek,“Clusteredfederatedlearning:
dAvgbyclusteringrobotsbasedonsimilardatadistributions Model-agnosticdistributedmultitaskoptimizationunderprivacycon-
straints,”IEEEtransactionsonneuralnetworksandlearningsystems,
andlearningindividualpersonalizedclustermodelsresulting
vol.32,no.8,pp.3710–3722,2020.
in an unbiased global model. Further, Fed-EC continually [17] A. Ghosh, J. Chung, D. Yin, and K. Ramchandran, “An efficient
collects data and updates the cluster models which makes framework for clustered federated learning,” Advances in Neural
InformationProcessingSystems,vol.33,pp.19586–19597,2020.
it capable of adapting. Finally, when a new robot joins the
[18] J. Konecny´, H. B. McMahan, D. Ramage, and P. Richta´rik,
network, Fed-EC clusters the robot and assigns it an initial “Federated optimization: Distributed machine learning for on-device
model to use without the need for retraining. intelligence,” ArXiv, vol. abs/1610.02527, 2016. [Online]. Available:
https://api.semanticscholar.org/CorpusID:2549272
[19] A.Hard,K.Rao,R.Mathews,F.Beaufays,S.Augenstein,H.Eichner,
REFERENCES
C.Kiddon,andD.Ramage,“Federatedlearningformobilekeyboard
prediction,” ArXiv, vol. abs/1811.03604, 2018. [Online]. Available:
[1] M. V. Gasparino, A. N. V. Sivakumar, Y. Liu, A. E. B. Velasquez,
https://api.semanticscholar.org/CorpusID:53207681
V.A.H.Higuti,J.G.Rogers,H.Tran,andG.V.Chowdhary,“Wayfast:
[20] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,
Navigationwithpredictivetraversabilityinthefield,”IEEERobotics
V.Ivanov,C.Kiddon,J.Konecˇny`,S.Mazzocchi,B.McMahanetal.,
and Automation Letters, vol. 7, pp. 10651–10658, 2022. [Online].
“Towardsfederatedlearningatscale:Systemdesign,”Proceedingsof
Available:https://api.semanticscholar.org/CorpusID:251086090
machinelearningandsystems,vol.1,pp.374–388,2019.
[2] M. V. Gasparino, A. N. Sivakumar, and G. Chowdhary, “Wayfaster:
[21] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
a self-supervised traversability prediction for increased navigation
Challenges, methods, and future directions,” IEEE signal processing
awareness,”arXivpreprintarXiv:2402.00683,2024.
magazine,vol.37,no.3,pp.50–60,2020.
[3] A. Kouris and C.-S. Bouganis, “Learning to fly by myself: A self-
[22] M.Mohri,G.Sivek,andA.T.Suresh,“Agnosticfederatedlearning,”
supervised cnn-based approach for autonomous navigation,” in 2018
inInternationalConferenceonMachineLearning. PMLR,2019,pp.
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
4615–4625.
(IROS). IEEE,2018,pp.1–9.
[23] X.Li,K.Huang,W.Yang,S.Wang,andZ.Zhang,“Ontheconver-
[4] G. Kahn, P. Abbeel, and S. Levine, “Badgr: An autonomous
gence of fedavg on non-iid data,” arXiv preprint arXiv:1907.02189,
self-supervised learning-based navigation system,” IEEE Robotics
2019.
and Automation Letters, vol. 6, pp. 1312–1319, 2020. [Online].
[24] F.Sattler,S.Wiedemann,K.-R.Mu¨ller,andW.Samek,“Robustand
Available:https://api.semanticscholar.org/CorpusID:211096687
communication-efficientfederatedlearningfromnon-i.i.d.data,”IEEE
[5] D. Shah and S. Levine, “ViKiNG: Vision-Based Kilometer-
TransactionsonNeuralNetworksandLearningSystems,vol.31,no.9,
Scale Navigation with Geographic Hints,” in Proceedings of
pp.3400–3413,2020.
Robotics: Science and Systems, 2022. [Online]. Available: http:
[25] T.Li,A.K.Sahu,M.Zaheer,M.Sanjabi,A.Talwalkar,andV.Smith,
//www.roboticsproceedings.org/rss18/p019.html
“Federated optimization in heterogeneous networks,” Proceedings of
[6] T.-D.Do,M.-T.Duong,Q.-V.Dang,andM.-H.Le,“Real-timeself-
Machinelearningandsystems,vol.2,pp.429–450,2020.
drivingcarnavigationusingdeepneuralnetwork,”in20184thInterna-
[26] Y.Zhao,M.Li,L.Lai,N.Suda,D.Civin,andV.Chandra,“Federated
tionalConferenceonGreenTechnologyandSustainableDevelopment
learningwithnon-iiddata,”arXivpreprintarXiv:1806.00582,2018.
(GTSD),2018,pp.7–12.
[27] C.Briggs,Z.Fan,andP.Andras,“Federatedlearningwithhierarchical
[7] A. R. Fayjie, S. Hossain, D. Oualid, and D.-J. Lee, “Driverless
clustering of local updates to improve training on non-iid data,” in
car:Autonomousdrivingusingdeepreinforcementlearninginurban
2020 International Joint Conference on Neural Networks (IJCNN).
environment,” in 2018 15th International Conference on Ubiquitous
IEEE,2020,pp.1–9.
Robots(UR),2018,pp.896–901.
[28] M. Duan, D. Liu, X. Ji, R. Liu, L. Liang, X. Chen, and Y. Tan,
[8] Y.Zhu,R.Mottaghi,E.Kolve,J.J.Lim,A.K.Gupta,L.Fei-Fei,and
“Fedgroup:Efficientclusteredfederatedlearningviadecomposeddata-
A. Farhadi, “Target-driven visual navigation in indoor scenes using
drivenmeasure,”arXivpreprintarXiv:2010.06870,2020.
deep reinforcement learning,” 2017 IEEE International Conference
[29] D.K.Dennis,T.Li,andV.Smith,“Heterogeneityforthewin:One-
onRoboticsandAutomation(ICRA),pp.3357–3364,2016.[Online].
shot federated clustering,” in International Conference on Machine
Available:https://api.semanticscholar.org/CorpusID:2305273
Learning. PMLR,2021,pp.2611–2620.
[9] D. S. Chaplot, R. Salakhutdinov, A. K. Gupta, and S. Gupta,
[30] B. Liu, L. Wang, and M. Liu, “Lifelong federated reinforcement
“Neural topological slam for visual navigation,” 2020 IEEE/CVF
learning: A learning architecture for navigation in cloud robotic
ConferenceonComputerVisionandPatternRecognition(CVPR),pp.
systems,” IEEE Robotics and Automation Letters, vol. 4, no. 4, pp.
12872–12881,2020.[Online].Available:https://api.semanticscholar.
4555–4562,2019.
org/CorpusID:214754592
[31] Y.Li,X.Tao,X.Zhang,J.Liu,andJ.Xu,“Privacy-preservedfederated
[10] A. N. Sivakumar, S. Modi, M. V. Gasparino, C. Ellis, A. E. B. Ve-
learning for autonomous driving,” IEEE Transactions on Intelligent
lasquez,G.Chowdhary,andS.Gupta,“Learnedvisualnavigationfor
TransportationSystems,vol.23,no.7,pp.8423–8434,2022.
under-canopy agricultural robots,” in Robotics: Science and Systems,
[32] A. Nguyen, T. Do, M. Tran, B. X. Nguyen, C. Duong, T. Phan,
2021.
E.Tjiputra,andQ.D.Tran,“Deepfederatedlearningforautonomous
[11] S. P. Adhikari, G. Kim, and H. Kim, “Deep neural network-based
driving,” in 2022 IEEE Intelligent Vehicles Symposium (IV). IEEE,
systemforautonomousnavigationinpaddyfield,”IEEEAccess,vol.8,
2022,pp.1824–1830.
pp.71272–71278,2020.
[33] N. Majcherczyk, N. Srishankar, and C. Pinciroli, “Flow-fl: Data-
[12] D.Shah,B.Eysenbach,G.Kahn,N.Rhinehart,andS.Levine,“Ving:
drivenfederatedlearningforspatio-temporalpredictionsinmulti-robot
Learning open-world navigation with visual goals,” in 2021 IEEE
systems,” in 2021 IEEE International Conference on Robotics and
InternationalConferenceonRoboticsandAutomation(ICRA),2021,
Automation(ICRA),2021,pp.8836–8842.
pp.13215–13222.
[34] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, “A density-based
[13] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and
algorithm for discovering clusters in large spatial databases with
B. A. y Arcas, “Communication-efficient learning of deep networks noise,” in Knowledge Discovery and Data Mining, 1996. [Online].
from decentralized data,” in International Conference on Artificial
Available:https://api.semanticscholar.org/CorpusID:355163
Intelligence and Statistics, 2016. [Online]. Available: https://api.
[35] Y. J. Cho, D. Jhunjhunwala, T. Li, V. Smith, and G. Joshi,
semanticscholar.org/CorpusID:14955348
“To federate or not to federate: Incentivizing client participation
[14] B.Liu,L.Wang,M.Liu,andC.-Z.Xu,“Federatedimitationlearning: in federated learning,” ArXiv, vol. abs/2205.14840, 2022. [Online].
A novel framework for cloud robotic systems with heterogeneous Available:https://api.semanticscholar.org/CorpusID:249192165
sensor data,” IEEE Robotics and Automation Letters, vol. 5, no. 2,
pp.3509–3516,2020.