Hybrid Transfer Reinforcement Learning:
Provable Sample Efficiency from Shifted-Dynamics Data
Chengrui Qu∗ Laixi Shi† Kishan Panaganti† Pengcheng You∗
PKU Caltech Caltech PKU
Adam Wierman†
Caltech
November 7, 2024
Abstract
Online Reinforcement learning (RL) typically requires high-stakes online interaction data to learn a
policyforatargettask. Thispromptsinterestinleveraginghistoricaldatatoimprovesampleefficiency.
The historical data may come from outdated or related source environments with different dynamics.
It remains unclear how to effectively use such data in the target task to provably enhance learning and
sampleefficiency. Toaddressthis,weproposeahybridtransferRL(HTRL)setting,whereanagentlearns
inatargetenvironmentwhileaccessingofflinedatafromasourceenvironmentwithshifteddynamics. We
showthat–withoutinformationonthedynamicsshift–generalshifted-dynamicsdata,evenwithsubtle
shifts,doesnotreducesamplecomplexityinthetargetenvironment. However,withpriorinformationon
thedegreeofthedynamicsshift,wedesignHySRL,atransferalgorithmthatachievesproblem-dependent
sample complexity and outperforms pure online RL. Finally, our experimental results demonstrate that
HySRL surpasses state-of-the-art online RL baseline.
Keywords: Hybrid Tranfer RL, distribution shift, sample complexity, model-based RL
Contents
1 Introduction 2
1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Hybrid Transfer RL 4
2.1 Hybrid Transfer RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3 Minimax Lower Bound For HTRL 5
4 HTRL with Separable Shift 6
4.1 β-separable shfits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4.2 Algorithm design: HySRL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.3 Theoretical guarantees: sample complexity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5 Experiments 11
6 Conclusion 12
∗CollegeofEngineering,PekingUniversity,Beijing,100871,China.
†DepartmentofComputingMathematicalSciences,CaliforniaInstituteofTechnology,CA91125,USA.
1
4202
voN
6
]GL.sc[
1v01830.1142:viXraA Proof of the Minimax Lower Bound 18
A.1 Preliminaries and Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 Proof of Theorem 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B Proof of the Upper Bound 22
B.1 Preliminaries and Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 Proof of Theorem 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
C Auxiliary Proofs 33
C.1 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
C.2 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.3 Proof of Lemma 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
C.4 Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D Experiment Setup 42
1 Introduction
In online reinforcement learning (RL), an agent learns by continuously interacting with an unknown en-
vironment. While this approach has led to remarkable successes across various domains, such as robotics
(Espeholt et al., 2018), traffic control (He et al., 2023) and game playing (Silver et al., 2017), it often re-
quires billions of data from interactions to develop an effective policy (Li et al., 2023). Moreover, in many
real-world scenarios, such interactions can be costly, time-consuming, or unsafe (Eysenbach et al., 2021),
which significantly limits the broader application of RL in practice, highlighting the urgent need for more
sample-efficient paradigms.
OnepromisingdirectiontoaddresssampleinefficiencyinRListransferlearning(Zhuetal.,2023). When
developing an effective policy for a target environment, it is often possible to leverage experiences from a
similarsourceenvironmentwithshifteddynamics(Niuetal.,2024). Thesesourcesmayincludeanimperfect
simulator (Peng et al., 2018), historical operating data before external impacts (Luo et al., 2022), polluted
offline datasets (Wang et al., 2023), or data from other tasks in a multi-task setting (Sodhani et al., 2021).
Thisconcepthasledtovariousdomainsandpipelines,suchasmetaRL(Finnetal.,2017),cross-domainRL
(Eysenbachetal.,2021;Liuetal.,2022),anddistributionallyrobustRL(Shietal.,2023),whichdemonstrate
varying levels of effectiveness.
Fromapracticalstandpoint,sofartherearenoclearsignalsonhowtoperformtransferlearningsample-
efficientlywiththeoreticalguarantees. Whilesomestudiesshowthatusingshifteddynamicsdatacanreduce
the time required to achieve specific performance levels in the target environment (Liu et al., 2022; Serrano
et al., 2023; Zhang et al., 2024), others indicate that sometimes these transfers hinder rather than help
learning (Ammar et al., 2015; You et al., 2022), a phenomenon known as negative transfer.
These practical challenges highlight the need for theoretical insights, which have not been addressed in
existing frameworks. Recently, a new stream of research called hybrid RL (Xie et al., 2021) has emerged,
showing that, theoretically, an offline dataset with no dynamics shift can facilitate more efficient online
exploration. However, when the dataset is collected from a source environment with shifted dynamics, it
remainsunclearwhetherthisdatasetcanstillenablemoresample-efficientlearninginthetargetenvironment.
This brings us an interesting open question:
Can data from a shifted source environment be leveraged to provably enhance sample efficiency
when learning in a target environment?
To answer this question, we formulate a problem called hybrid transfer RL, where an agent learns in
a target environment while having access to an offline dataset collected from a source environment. The
source and target environments differ only in transition uncertainties in the same world (Doshi-Velez and
Konidaris, 2013). Since these differences are typically unknown before exploring the target environment, we
refer to them as an unknown dynamics shift. The learning goal is to find an optimal policy for the target
environment using minimal interactions.
2Contributions. In this work, we propose a hybrid transfer RL (HTRL) setting, where the source and
target environments share the same world structure, only differing in their transitions. We first present a
hardness result in terms of a minimax lower bound in general hybrid transfer RL and show provable sample
efficiency gains from the source environment dataset with additional prior information. To the best of our
knowledge, we are the first to look into the sample complexity of this transfer setting. Specifically:
• We formulate and focus on a new problem called hybrid transfer RL (HTRL). We find that even when
the target MDP is similar to the source MDP in dynamics, data from the source MDP cannot reduce the
sample complexity in the target MDP without further conditions, compared to state-of-the-art online RL
sample complexity (Theorem 1). This result demonstrates that general HTRL is not feasible, motivating
us to look into more practical yet meaningful settings.
• We study HTRL where prior information about the degree of the dynamics shift is available. A transfer
algorithm, HySRL, is designed, which achieves a problem-dependent sample complexity that is at least as
good as the state-of-the-art online sample complexity, offering sample efficiency gains in many scenarios
(Theorem 2). The key technical contributions involve extending the current reward-free and bonus-based
explorationtechniquestoaccommodatemoregeneralrewardsandincorporatingshifted-dynamicsdatainto
theanalysis. Inaddition,weconductexperimentsintheGridWorldenvironmenttoevaluatetheproposed
algorithmHySRL,demonstratingthatHySRLachievessuperiorsampleefficiencythanthestate-of-the-art
pure online RL baseline.
1.1 Related work
Finite-sample analysis of online, offline, and hybrid RL. Finite sample analysis in RL focuses on
understanding the sample complexity – how many samples are required to achieve a desired policy with
certain performance. In this line of research, a non-exhaustive list in online RL includes Azar et al. (2017);
Bai et al. (2019); Domingues et al. (2021b); Dong et al. (2019); He et al. (2021); Jafarnia-Jahromi et al.
(2020); Jin et al. (2018); Liu and Su (2021); Menard et al. (2021); Yang et al. (2021); Zanette and Brunskill
(2019); Zhang et al. (2020a, 2021b,b, 2020b), while offline RL has seen advances such as Duan et al. (2020,
2021); Jiang and Huang (2020); Jiang and Li (2016); Kallus and Uehara (2020); Li et al. (2014); Ren et al.
(2021); Thomas and Brunskill (2016); Uehara et al. (2020); Xu et al. (2021); Yang et al. (2020), and hybrid
RL frameworks are explored in Li et al. (2024); Song et al. (2023); Xie et al. (2021); Zhang and Zanette
(2023). ThemostcloselyrelatedsettingishybridRL,inwhichanagentlearnsinatargetenvironmentwith
accesstoanofflinedatasetcollectedfromthesameenvironment. OurworkextendshybridRLbyaddressing
cases where the offline dataset may also come from a related environment with shifted dynamics.
Transfer RL with dynamics shifts. Cross-domain RL with dynamics shifts is the most related setting,
which focuses on leveraging abundant samples from a source environment to reduce data requirements for
a target environment (Chen et al., 2024b; Eysenbach et al., 2021; Liu et al., 2022; Niu et al., 2023, 2022;
Wen et al., 2024). A major challenge in these transfers is the dynamics shift between the source and target
environments. Common approaches often involve training a classifier to distinguish between source and
target transitions, by techniques such as combining source and target datasets for policy training (Chen
et al., 2024b; Wen et al., 2024), or reshaping rewards by introducing a penalty term for dynamics shifts
(Eysenbach et al., 2021; Liu et al., 2022). While these methods show promising empirical performance, a
systematic study on sample complexity is missing. Our work fills this gap by offering a novel theoretical
perspective on cross-domain RL.
OtherrelatedtransferRLsettingsincludedistributionallyrobustofflineRL(LiuandXu,2024;Maetal.,
2023; Panaganti and Kalathil, 2022; Shi et al., 2023; Wang et al., 2024; Zhou et al., 2021), which focuses on
training a robust policy using only an offline dataset, without further exploration, to optimize performance
intheworst-casescenarioofthetargetenvironment. Anotherarea,metaRL(Chenetal.,2022;Duanetal.,
2016; Finn et al., 2017; Mutti and Tamar, 2024; Wang et al., 2017; Ye et al., 2023), trains an agent over a
distributionofenvironmentstoenhancegeneralizationcapabilities. Ourworkcontributestodistributionally
robustofflineRLbyaddressingthesamplecomplexitywhenexplorationinthetargetenvironmentisallowed
and complements meta RL by focusing on scenarios where the target environment lies outside the training
distribution.
3Reward-free RL. Reward-free RL seeks to collect sufficient data to achieve optimality for any potential
reward function. This paradigm was first proposed in Jin et al. (2020), with subsequent improvements in
sample complexity by Kaufmann et al. (2021); Ménard et al. (2021); Zhang et al. (2021a). Similar findings
have been developed for settings involving function approximation (Chen et al., 2024a; Qiu et al., 2021;
Wagenmaker et al., 2022; Wang et al., 2020; Zanette et al., 2020). While existing reward-free methods
efficiently estimate the transition kernel, they do not address the challenge of uniformly controlling high-
dimensional transition estimation errors that arises in transfer settings. Our work introduces new tools to
tackle this challenge in a sample-efficient manner.
Notation. We denote by [n] the set 1, ,n for any positive integer n, and use 1 to represent the
indicator function. For a function f de{fine·d··on } S, we define its expectation under the p{·r}obability measure
p as pf ≜ E f(s) and its variance as Var (f) ≜ E (f(s) E f(s′))2 = p(f pf)2. The total
s∼p p s∼p s′∼p
− −
variation distance between probability measures p and q is defined as TV(p,q)≜sup p(A) q(A). We
use standard O() and Ω() notation, where f = O(g) means there exists some consA t⊆ anS t| C >− 0 such|that
· ·
f Cg (similarly for Ω()), and use the tilde notation O(cid:101)() to suppress additional log factors. We denote
the≤cardinality of a set ·by . ·
X |X|
2 Hybrid Transfer RL
We begin by introducing the mathematical formulation of HTRL, benchmarking with standard online RL.
Online RL
Onlinedata
Rollout
Agent Targetenv
Hybrid Transfer RL (our setting)
Onlinedata
Offlinedata
Rollout
Sourceenv Agent Targetenv
Hybrid RL
Onlinedata
Offlinedata
Rollout
Targetenv Agent Targetenv
Figure 1: Comparison between different RL settings
Background: Markov decision process (MDP). WeconsiderepisodicMarkovDecisionProcess =
( , ,H,p,r,ρ), where is the state space with size S, is the action space with size A, H is the hoMrizon
leSngAth. p( s,a): S ∆( ) denotes the time-indepAendent transition probability at each step, and the
reward fun·c|tion isSde×teArm(cid:55)→inisticS 1, given by r : [0,1]. In this setting, a Markovian policy is given by
π := π H where π : ∆( ). AdditionSal×ly,Aw(cid:55)→e assume that each episode of the MDP starts from an
initia{l sth a}th e= g1 eneratedh froSm(cid:55)→an unAknown initial state distribution ρ ∆( ).
∈ S
1For simplicity, we consider deterministic rewards, as estimating rewards is not a significant challenge in deriving sample
complexityresults.
4For a given transition p, the value function for state s at step h is defined as the expected cumulative
future reward by executing policy π, which is given by Vp,π(s) := E [(cid:80)H r(s ,a ) s = s]. Similarly,
thestate-actionvaluefunction,orQ-function,isdefinedash Qp,π(s,a)=p,π
E
i= [(cid:80)h
H
i r(si ,|
a
)h
s =s,a =a].
We denote the weighted value function of policy π by: h p,π i=h i i | h h
Vp,π(ρ)=E [Vp,π(s)].
1 s∼ρ 1
As is well known, there exists at least one deterministic policy that maximizes the value function and
the Q-function simultaneously for all (s,a,h) [H] (Bertsekas, 2007). Let π⋆ denote an optimal
deterministicpolicy,andthecorrespondingopt∈imSal×vaAlue×functionV⋆ andoptimalQ-functionQ⋆ aredefined
h h
respectively by Vp,⋆ ≜Vp,π⋆ , Qp,⋆ ≜Qp,π⋆ , (s,a,h) [H].
h h h h ∀ ∈S×A×
2.1 Hybrid Transfer RL
In HTRL, the agent can directly interact with the target MDP = ( , ,H,p ,r,ρ) in episodes
tar tar
of length H. In an episode, at each step h [H], the agent obsMerves a stSateA s , takes an action
h
a [H], receives a reward r(s ,a ) and tr∈ansitions to a new state s accordi∈ngSto the underlying
h h h h+1
tran∈sition probability p ( s ,a ).
tar h h
Additionally, the agent·h|as access to an offline dataset = (s ,a ,r ,s′) pre-collected from a source
MDP = ( , ,H,p ,r,ρ). The target and sourceDMsrc DPs{shi arei thi e si a}me structure except for the
src src
transitMion probabSiliAties (i.e. p =p ). For simplicity, we assume the reward signals in and are
tar src src tar
the same; however, our analysis s̸ till holds when the reward signals differ. We assume p Mand p Mare both
src tar
unknown to the agent.
Goal. With access to both and , the goal in HTRL is to find an ε-optimal policy for .
src tar tar
Specifically, the agent learns toDfind a poMlicy πˆ for , which satisfies that: M
tar
M
Vptar,⋆(ρ) Vptar,πˆ(ρ) ε.
1 − 1 ≤
Benchmarking with standard online RL. ThebaselineforHTRLisonlineRL,wheretheagentlearns
from scratch by directly interacting with in episodes of length H. Different from HTRL, in online
tar
RL, the agent collects samples from wMithout any additional information, learning ε-optimal policy for
tar
. Offline RL cannot be a baselinMe because the differs from and in general it is impossible to
tar src tar
Mrun an offline algorithm on to obtain an ε-optiMmal policy for M.
src tar
Compared to online RLD, the introduction of additional accesMs to in HTRL naturally raises the
src
question: can we achieve better sample efficiency by leveraging ? UnDfortunately, the answer is negative,
src
which will be highlighted in the next section. D
3 Minimax Lower Bound For HTRL
In this section, we establish a minimax lower bound on the sample complexity for general HTRL, formally
demonstrating that sample complexity improvements for general HTRL are not feasible.
Specifically, when p is close to p , one might expect that fewer samples from are needed to
tar src tar
reach a given performance level by leveraging additional information about becausMe by the Simulation
src
Lemma, we can already obtain a good initial policy from . However,Mwe show in Theorem 1 that, in
src
the worst case, the same number of samples from is sMtill required compared with pure online RL. The
tar
proof can be found in Appendix A. M
Theorem 1 (Minimax lower bound for HTRL). Given an optimality gap ε, consider for any the
src
M
following set of possible MDPs:
≜ =( , ,H,p,r,ρ)
α
M {M S A |
max TV(p( s,a),p ( s,a)) α ,
src
(s,a)∈S×A ·| ·| ≤ }
5where 48ε/H2 α 1. Suppose S 3, H 3, A 2, ε 1/48. For any algorithm, there always
≤ ≤ ≥ ≥ ≥ ≤
exists a and a target MDP , if the number of samples n collected from the target MDP is
src tar α
M M ∈M
O(H3SA/ε2), then the algorithm suffers from an ε-suboptimality gap:
(cid:104) (cid:105)
E Vptar,⋆(ρ) Vptar,πˆ(ρ) ε,
tar 1 − 1 ≥
where E denotes the expectation with respect to the randomness during algorithm execution in the target
tar
MDP .
tar
M
Theorem 1 shows that the lower bound of sample complexity of general HTRL is Ω(H3SA/ε2), which
matchesthestate-of-the-artsamplecomplexityofpureonlineRL,O(cid:101)(H3SA/ε2)(e.g.,Ménardetal.(2021)2;
Wainwright (2019)). This demonstrates that, in general, practical transfer algorithms leveraging source en-
vironmentdatacannotreducethesamplecomplexityinthetargetenvironment. Nomatterwhatalgorithms
are used, there always exists a worst case where transfer learning cannot achieve better sample efficiency in
the target environment, compared to pure online RL. However, this lower bound is conservative, motivating
us to explore practically meaningful and feasible settings to derive problem-dependent sample complexity
bounds.
Comparisons to prior lower bounds. To the best of our knowledge, this is the first lower bound on
the sample complexity when leveraging information from a source environment to explore a new target
environment with an unknown dynamics shift. We highlight the novelty of our lower bound result by
comparing it with prior lower bounds:
Lower bounds for transfer learning in RL:MuttiandTamar(2024);O'Donoghue(2021);Yeetal.(2023)
provide lower bounds on regret in settings where the agent is trained on N source tasks and is fine-tuned
to the target task during testing. However, these lower bounds cannot be adapted to our setting as they
assume the target task is one of the source tasks – which is stronger than ours.
Lower bounds for pure online RL: The existing lower bound on the sample complexity of pure online RL
is also Ω(H3SA/ε2) (Gheshlaghi Azar et al., 2013). This demonstrates that the improvement in the sample
complexity lower bound from the introducing additional information from a source environment is at most
a constant factor. The construction of the lower bound for HTRL follows a procedure similar to existing
lower bounds for online RL (Lattimore and Hutter, 2012; Yin et al., 2020), offline RL (Rashidinejad et al.,
2021) and hybrid RL without dynamics shift (Xie et al., 2021). However, the new technical challenge in our
setting is to bound the maximum information gain of the data from . We address this difficulty using
src
a proper change-of-measure approach. See Appendix A for a detailedMcomparison.
4 HTRL with Separable Shift
Although improved sample efficiency is not achievable for general HTRL in the worst case, practical tasks
are typically more manageable than these difficult instances. Inspired by practical tasks such as hierarchical
RL(Chuaetal.,2023)andmetaRL(Chenetal.,2022),weinsteadfocusonaclassofHTRLwithseparable
shift in the following. This setting allows us to leverage prior information about the degree of dynamic shift
between the source and target environments. We then propose an algorithm, called HySRL, which achieves
provably superior sample complexity compared to pure online RL.
4.1 β-separable shfits
Wefirstintroducethedefinitionofseparableshift,characterizedbytheminimaldegreeofthedynamicsshift
between the source and target environments.
Definition 1 (β-separable shift). Consider a target MDP = ( , ,H,p ,r,ρ) and a source MDP
tar tar
M S A
=( , ,H,p ,r,ρ). The shift between and is β-separable if for some β (0,1], we have
src src tar src
M S A M M ∈
2Ménardetal.(2021)considerstime-dependenttransitionsandthesamplecomplexityresultisO(cid:101)(H4SA/ε2),whichinour
settingtranslatesintoO(cid:101)(H3SA/ε2)duetotime-independenttransitions.
6for all (s,a) ,
∈S×A
p ( s,a)=p ( s,a)
src tar
·| ̸ ·|
= TV(p ( s,a),p ( s,a)) β.
src tar
⇒ ·| ·| ≥
In other words, for any state-action pair (s,a), the transitions in and are either identical
src tar
or different by at least the degree of β w.r.t the TV distance metricM. This defiMnition is widely used to
characterizethe"distance"betweentasksinhierarchicalRL(Chuaetal.,2023),RLforlatentMDPs(Kwon
et al., 2024), multi-task RL Brunskill and Li (2013), and meta RL (Chen et al., 2022; Mutti and Tamar,
2024), serving the purpose of distinguishing different tasks with finite samples.
Such a minimal degree of dynamic shift, β, can often be estimated beforehand as prior information for
specificproblemsinpractice(BrunskillandLi,2013). Therefore, inthissection, wedesignalgorithmsunder
the assumption that p and p are β-separable.
tar src
Remark 1 (Separable shift makes HTRL feasible). The lower bound in Theorem 1 arises from potential
challenging target MDPs that subtly differ from the source MDP and are specified based on the optimality gap
ε. This subtlety requires extensive data to distinguish between them. However, in practice, the dynamic shift
betweensourceandtargetenvironmentsisusuallyindependentofε. ByfocusingonHTRLwithaβ-separable
shift, where β is independent of ε, we exclude over-conservative instances that are rare in practice.
In addition to the aforementioned key definition — β-separable dynamic shifts, we introduce another
assumption for the reachability of the target MDP. Note that it is not tailored for our Hybrid Transfer
RL setting, but widely adopted in extensive RL tasks such as standard RL, meta RL and multi-task RL
(Brunskill and Li, 2013; Chen et al., 2022; Jaksch et al., 2010), to ensure the agent’s access to the entire
environment (over all state-action pairs).
Assumption 1 (σ-reachability). We assume the target MDP has σ-reachability if there exists a con-
tar
M
stant σ (0,1] so that
∈
max max pπ(s,a) σ, (s,a) ,
π h∈[H] h ≥ ∀ ∈S×A
where pπ(s,a) is the probability of reaching (s,a) at step h by executing policy π in .
h Mtar
4.2 Algorithm design: HySRL
Focusing on HTRL with β-separable shift, now we are ready to introduce our algorithm HySRL, outlined in
Algorithm 1. To explicitly characterize the set of state-action pairs where p and p differ, we introduce
src tar
the following definition.
Definition 2 (Shifted region). We define the shifted region as the set of state-action pairs where the
B
transitions in and differ:
src tar
M M
≜ (s,a) p ( s,a)=p ( s,a) .
src tar
B { ∈S×A| ·| ̸ ·| }
Althoughp isunknowninadvance,itispossibletoinvestasmallnumberofonlinesamplestoestimate
tar
p andidentifytheshiftedregion . Thishelpsdeterminewhichpartof canimprovesampleefficiency
tar src
in , allowing us to focus furthBer exploration on the remaining areasDto learn an effective policy. Since,
tar
in Mmany practical applications, the dynamcis shift typically affects only a small portion of the state-action
space(Chuaetal.,2023),thisapproachcanenablemoresample-efficientexplorationin . Thisintuition
tar
drives the design of Algorithm 1. M
Algorithm 1: Hybrid separable-transfer RL (HySRL). At a high level, given a desired optimality
gap ε, if σβ is too small – implying that an excessive number of samples is required to identify the shifted
region – Algorithm 1 chooses to ignore the offline dataset and instead relies on pure online learning. Other-
wise, we proceed as follows: first, we run Algorithm 2 to obtain an estimated shifted region ˆ, which, with
high probability, matches the true shifted region . Next, we use the offline dataset andBonline data to
src
design exploration bonuses and execute AlgorithBm 3 to efficiently explore ˆ, ultimaDtely outputting a final
policy for . Below, we outline the key steps of Algorithm 1. B
tar
M
7Algorithm 1 Hybrid Separable-transfer RL (HySRL)
Require: Parameters β, δ, σ, ε, source dataset
src
(cid:112) D
1: if σβ S/Hε then
≤
ˆ // Abandon
2: src
B ←S×A D
3: else
ˆ call Algorithm 2// Estimate
4:
B ← B
5: end if
6: πfinal call Algorithm 3 with ˆ// Explore ˆ
← B B
7: return πfinal
Step 1: Reward-free shift identification (Algorithm 2). Evenwithknowledgeofβ andσ,accurately
estimating p to identify the shifted region is still challenging, as we need to control the errors in
tar
estimatinghigh-dimensionaltransitionswithfinBitesamples. Tothebestofourknowledge, noexistingworks
have addressed this issue.
Algorithm 2 Reward-free shift identification
Require: Parameters β, δ, σ, pˆ
src
1: for t=0,1,2, do
···
2: for h=H, ,1 do
3:
Update W·· t·using Eq. (1)
h
4: Update π ht+1( ·)=argmax a∈AW ht( ·,a)
5: end for
(cid:113)
6: Break if 3 ρπt+1Wt+ρπt+1Wt σβ/8
1 1 1 1 ≤
7: Rollout πt+1 and observe new online samples
8: for (s,a) do
9: Update∈ ntS (s× ,aA ), nt(s,a,s′) and pˆt ( s,a)
tar ·|
10: end for
11: end for
12: return Estimated shifted area ˆby Eq. (2)
B
To this end, sufficient online data coverage is required for each (s,a), which aligns with the motivation
behind reward-free exploration to collect enough data and achieve optimality for any reward signal r :
[0,1]. InspiredbyRF-Express from Ménard etal.(2021), we proposean algorithmin Algorithm2.
SSp×ecAifi(cid:55)→cally, we first define an uncertainty function Wt(s,a), which characterizes data sufficiency in the tth
h
episode, recursively (with Wt (s,a)=0) for all h [H] and (s,a) ,
H+1 ∈ ∈S×A
(cid:18) 4Hg (nt(s,a),δ)
Wt(s,a)≜min 1, 1
h nt(s,a)
(cid:19)
(cid:88)
+ pˆt (s′ s,a)maxWt (s′,a′) , (1)
tar | a′∈A h+1
s′
where g (n,δ) ≜ log(6SAH/δ)+Slog(8e(n+1)), nt(s,a) ≜ (cid:80)t (cid:80)H 1 (sτ,aτ)=(s,a) denotes the
visitatio1 ncountfor(s,a)inthefirsttepisodesandpˆt (s,a)denotτ e= s1 theh c= o1 rres{poh ndinh gempiric}altransitions.
tar
Accordingly, we select πt+1() = argmax Wt(,a) to collect online data from , update nt(s,a),
pˆt (s,a) and Wt(s,a), anh d st·op until: a∈A h · Mtar
tar h
(cid:113)
3 ρπt+1Wt+ρπt+1Wt σβ/8,
1 1 1 1 ≤
where ρπt+1Wt = (cid:80) ρ(s)Wt(s,πt+1(s)). This stopping criterion is designed to ensure that sufficient data
1 1 s 1 1
coverageisachievedwhenAlgorithm2stops. Beyondreward-freeexploration,ourdesignfurtherguarantees
the confidence intervals
TV(p ( s,a),pˆt ( s,a)) β/4
tar ·| tar ·| ≤
8is constructed for each (s,a). Then, we estimated the shifted region as:
ˆ≜ (s,a)
B { ∈S×A|
TV(pˆ ( s,a),pˆt ( s,a))>β/2(cid:9) , (2)
src ·| tar ·|
where pˆt denotes the empirical transitions in . We show that by executing Algorithm 2, the shifted
region
sr cc
an be identified with high
probabilityDws itrc
hin a sample size from that is independent of ε, as
tar
formallBy stated in Lemma 1. The proof of Lemma 1 can be found in AppeMndix B.2.
Lemma1(Sample-efficientshiftidentification). LetAssumption1hold, andδ (0,1)begiven. Supposethe
∈
shift between
tar
and
src
is β-separable, and
src
contains at least Ω(cid:101)(S/β2) samples for (s,a) .
M M D ∀ ∈S×A
With probability at least 1 δ/2, Algorithm 2 can output an estimate of p satisfying
tar
−
TV(p ( s,a),pˆt ( s,a)) β/4, (s,a) ,
tar ·| tar ·| ≤ ∀ ∈S×A
along with the estimated shifted region ˆ= , using O(cid:101)(H2S2A/(σβ)2) samples collected from tar.
B B M
The confidence interval for transitions with finite-sample guarantees in Lemma 1 is estabilished by ex-
tending reward-free exploration to accomodate more general reward functions r : [H] [0,1]
in the analysis. ×S ×A×S (cid:55)→
Step 2: Hybrid UCB value iteration (Algorithm 3). Once we have the estimated shifted region ˆ,
it is intuitive for the agent to focus more on exploring the estimated shifted region ˆ. To achieve this, wBe
introduce an algorithm that incorporates the additional source dataset in the deBsign of the exploration
src
bonus summarized in Algorithm 3. D
Algorithm 3 Hybrid UCB Value Iteration
Require: Parameters δ, ε, ˆ,
src
B D
1: for t=0,1,2, do
···
2: for h=H, ,1 do
3: Update
Q·t··
, Gt using Eqs. (3a) and (4)
h h
4: Update π ht+1( ·)=argmax a∈AQt h( ·,a)
5: end for
6: Break if ρπt+1Gt ε
7:
Rollout πt+11 and1 o≤bserve new online samples
8: for (s,a) ˆdo
9: Update∈ ntB (s,a), nt(s,a,s′) and pˆt ( s,a)
10: // Only update nt and pˆt insidetar ˆ·|
tar B
11: end for
12: end for
13: return πfinal =πt+1
This algorithm is inspired by BPI-UCBVIin Ménard et al. (2021); however, in our problem, we carefully
design the exploration bonus to leverage the additional offline dataset while controlling potential bias
src
that it introduces. To effectively use while avoiding potential biDas, we define the upper confidence
src
boundsoftheoptimalQ-functionsandvDaluefunctionsfortheestimatedshiftedregion ˆanditscomplement
/ ˆ, respectively: B
S×A B
(cid:115)
Qt (s,a)≜min(cid:18)
3 Var
(Vt )(s,a)g 2(n˜t(s,a),δ)
h p˜t h+1 n˜t(s,a)
+ 14H2g 1(n˜t(s,a),δ) + 1 p˜t(Vt Vt )(s,a)
n˜t(s,a) H h+1− h+1
(cid:19)
+p˜tVt (s,a)+r(s,a),H , (3a)
h+1
9Vt (s)≜max Qt (s,a), Vt (s)≜0, (3b)
h h H+1
a∈A
whereg (n,δ)≜log(6SAH/δ)+log(8e(n+1)), Vt isalowerboundoftheoptimalvaluedefinedsimilarly
2 h+1
in Appendix B.3, and Var () denotes the empirical variance under p˜t. Here, for (s,a) ˆ, we have
p˜t
· ∈ B
n˜t(s,a)≜nt(s,a) and p˜t( s,a)≜pˆt ( s,a); for (s,a) / ˆ, we have n˜t(s,a)≜n (s,a) and p˜t( s,a)≜
pˆ ( s,a), where n de·n|otes the vt ia sr it·at|ion count in ∈B. Note that we only ups dr ac te the visitati·o|n count
src src src
in ˆ·t|o remove statistical dependency. M
B Aiming to achieve optimality in , we choose πt+1() = argmax Qt (,a) to collect samples from
in Algorithm 2. Accordingly, wMe dta er fine the followh ing·function Gt(a s∈ ,A a) th o ·serve as an upper bound on
Mtar h
the optimality gap Vptar,⋆ Vptar,πt+1 (with G (s,a)=0):
h − h H+1
(cid:115)
(cid:18)
Gt(s,a)≜min H,6 Var (Vt )(s,a)g 2(n˜t(s,a),δ)
h p˜t h+1 n˜t(s,a)
(4)
35H2g (n˜t(s,a),δ) 3 (cid:19)
+ 1 +(1+ )p˜tπt+1Gt (s,a) ,
n˜t(s,a) H h+1 h+1
Algorithm 3 stops when ρπt+1Gt ε, indicating that ε-optimality is achieved in the target domain .
1 1 ≤ Mtar
This procedure requires at most O(cid:101)(H3 /ε2) samples from tar, as detailed in the final results in the next
section. |B| M
4.3 Theoretical guarantees: sample complexity
In this subsection, we discuss the total sample complexity of Algorithm 1, highlighting its sample efficiency
gainscomparedtothestate-of-the-artpureonlineRLsamplecomplexityandconnectionstopracticaltransfer
algorithms.
Theorem 2 (Problem-dependent sample complexity). Let Assumption 1 hold, and δ (0,1) and ε (0,1]
∈ ∈
begiven. Supposetheshiftbetween
tar
and
src
isβ-separable, and
src
containsatleastΩ(cid:101)(H3/ε2+S/β2)
M M D
samples for (s,a) . With probability at least 1 δ, the output policy πfinal of Algorithm 1 satisfies
∀ ∈S×A −
Vptar,⋆(ρ) Vptar,πfinal (ρ) ε, (5)
1 − 1 ≤
if the total number of online samples collected from is
tar
M
(cid:18) (cid:18) H3SA H3 H2S2A(cid:19)(cid:19)
O(cid:101) min , |B| + . (6)
ε2 ε2 (σβ)2
Theorem 2 provides a problem-dependent sample complexity of Algorithm 1 that is at least as good as
the state-of-the-art O(cid:101)(H3SA/ε2) in pure online RL (Ménard et al., 2021; Wainwright, 2019). Specifically,
for a given β:
• When ε Ω((cid:112) H/Sσβ): it captures the scenarios where the desired optimality gap ε is at least the
order of t≥he degree of the dynamics shift β. In this case, the sample complexity of Algorithm 1 becomes
O(cid:101)(H3SA/ε2), which matches the state-of-the-art pure online RL sample complexity, showing that our
framework provably avoids negative transfer in terms of sample efficiency.
• When ε < Ω((cid:112) H/Sσβ): the comparisons between the sample complexity of Algorithm 1 in Eq. (6) and
the state-of-the-art pure online RL is as follows:
(cid:18) H3 (cid:19) (cid:18) H3SA(cid:19)
O(cid:101) |B| v.s. O(cid:101) ,
ε2 ε2
where represents the cardinality of the shifted region, a problem-dependent parameter in HTRL that
isstrict|Bly|nolargerthanSA. ItindicatesthatAlgorithm1provablyachievesbettersampleefficiencythan
state-of-the-art pure online RL algorithms in HTRL tasks, as long as the shift does not cover the entire
state-action space, as validated in Section 5. In many practical scenarios, such as training cooking agents
(Beck et al., 2024) or autonomous driving (Xiong et al., 2016), environmental variations between source
10and target environments (e.g., different kitchen layouts or obstacle positions) typically affect only a small
portionofthestate-actionspace, meaning SA, withalargeseparableshift. Thisenablessignificant
sample efficiency gains from reusing the so|uBr|ce≪dataset.
Our results demonstrate that for HTRL tasks with β-separable shift between source and target environ-
ments, Algorithm 1 provably avoids harmful information transfer and enhances sample efficiency compared
to pure online RL. While Definition 1 depends on β, we evaluate Algorithm 1 in broader scenarios where an
inaccurate β is used, as discussed in Section 5, demonstrating the robustness of Algorithm 1.
Connectionswithpracticalcross-domaintransferalgorithms. Practicalalgorithmsforcross-domain
transfer RL often involve training a neural network classifier to distinguish between source and target tran-
sitions (Eysenbach et al., 2021; Liu et al., 2022; Niu et al., 2023; Wen et al., 2024) and reusing source data
accordingly. Our sample complexity results provide theoretical insights for determining the data collection
budgetinthetargetdomain. Theyalsodemonstratethattheestimatedtransitionshiftservesasaneffective
metric for utilizing the source data and can provably improve sample efficiency.
Extensions of Theorem 2: variants of source data. In Theorem 2, we assume abundant samples
from the source domain, which is a common assumption since we primarily focus on sample complexity in
the target domain . However, even when the source dataset is insufficient, similar results hold. In
tar src
particular, we consMider the set of state-action pairs where lacDks sufficient samples:
src
D
≜ (s,a) n src(s,a)<Ω(cid:101)(H3/ε2+S/β2) .
C { ∈S×A| }
By adjusting the input of Algorithm 3 to ˆ , Algorithm 1 can still achieve the identical optimality with
the sample complexity as below: B∪C
(cid:18) (cid:18) H3SA H3 H2S2A(cid:19)(cid:19)
O(cid:101) min , |B∪C| + .
ε2 ε2 (σβ)2
Similarly, when N datasets from N different source MDPs are available, Algorithm 1 can still function
byexecutingAlgorithm2oncetoidentifytheshiftsinthetargettransitionrelativetoeachsourcetransition
and selecting useful source data accordingly. Let denote the corresponding shifted region for each source
i
MDP i. Under the conditions of Theorem 2, the rBequired sample complexity in this setting becomes
(cid:18) (cid:18) H3SA H3 H2S2A(cid:19)(cid:19)
i∈[N] i
O(cid:101) min , |∩ B | + .
ε2 ε2 (σβ)2
5 Experiments
Weevaluateourproposedalgorithmbycomparingittothestate-of-the-artonlineRLbaseline, BPI-UCBVI
Ménard et al. (2021), in the GridWorld environment (S =16,A=4,H =20).
Inthesourceandthetargetenvironments,theagentmayfailtotakeanactionandgotoawrongdirection.
Compared with the source environment, the target environment includes three absorbing states. The source
dataset is collected by running Algorithm 2 in the source environment for T = 1 105 episodes, which
satisfies the conditions in Theorem 2. We implement both algorithms in the benchma×rk rlberry (Domingues
et al., 2021a), similar to that in Ménard et al. (2021). See Appendix D for a detailed introduction of the
experiment setup. The code is available at https://github.com/SilentEchoes77/hybrid-transfer-rl.
The results are presented in Fig. 2, averaged over 5 random seeds with a 95% confidence interval.
As shown in Fig. 2a, Algorithm 1 learns the optimal policy with approximately 1 106 samples from the
target environment. In contrast, BPI-UCBVI converges more slowly, highlighting t×he data inefficiency of
pure online RL. This demonstrates that transferring shifted-dynamics data from a source environment can
significantly improve sample efficiency.
To assess whether a correct β is necessary, we conduct an ablation study with the input β = 0.45,
while the true β ranges from 0.05 to 0.4. As shown in Fig. 2b, even when Definition 1 is not satisfied, the
performance degradation of the output policy from Algorithm 1 is minor and still outperforms BPI-UCBVI
within finite samples, demonstrating the robustness of our algorithm.
11(a) (b)
Figure 2: Fig. 2a shows the optimality gap of HySRL (ours) and BPI-UCBVI as the sample size varies.
Fig. 2b presents the percentage optimality gap of HySRL (ours) and BPI-UCBVI as the true β varies.
6 Conclusion
This paper introduces Hybrid Transfer RL, providing a framework to analyze the finite-sample guarantees
of practical transfer algorithms. We establish a worst-case lower bound for general HTRL, demonstrating
thatitcannotoutperformonlineRLinitsmostgeneralform. However,inmorepracticalscenarios,weshow
that transferring shifted-dynamics data can provably reduce sample complexity in the target environment,
offering theoretical insights for algorithm design.
Acknowledgment
The work of C. Qu is supported in part by NSFC through 723B1001 and by the Summer Undergraduate
Research Fellowships at California Institute of Technology. The work of L. Shi is supported in part by
the Resnick Institute and Computing, Data, and Society Postdoctoral Fellowship at California Institute of
Technology. K.PanagantiissupportedinpartbytheResnickInstituteandthe‘PIMCOPostdoctoralFellow
in Data Science’ fellowship at the California Institute of Technology. The work of P. You is supported in
part from NSFC through 723B1001, 72431001, 72201007, T2121002, 72131001. The work of A. Wierman is
supported in part from the NSF through CNS-2146814, CPS-2136197, CNS-2106403, NGSDI-2105648.
References
Ammar,H.B.,Eaton,E.,Luna,J.M.,andRuvolo,P.(2015). Autonomouscross-domainknowledgetransfer
in lifelong policy gradient reinforcement learning. In Proceedings of the 24th International Conference on
Artificial Intelligence, IJCAI’15, page 3345–3351. AAAI Press.
Azar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70,pages263–272.JMLR.
org.
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019). Provably efficient q-learning with low switching cost.
In Advances in Neural Information Processing Systems, pages 8002–8011.
Beck, J., Vuorio, R., Liu, E. Z., Xiong, Z., Zintgraf, L., Finn, C., and Whiteson, S. (2024). A survey of
meta-reinforcement learning. arXiv preprint arXiv:2301.08028.
Bertsekas,D.P.(2007). Dynamic Programming and Optimal Control, Vol. II. AthenaScientific,3rdedition.
12Brunskill, E. and Li, L. (2013). Sample complexity of multi-task reinforcement learning. In Proceedings of
the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, UAI’13, page 122–131, Arlington,
Virginia, USA. AUAI Press.
Chen, J., Modi, A., Krishnamurthy, A., Jiang, N., and Agarwal, A. (2024a). On the statistical efficiency of
reward-free exploration in non-linear rl. In Proceedings of the 36th International Conference on Neural
Information Processing Systems, NIPS ’22, Red Hook, NY, USA. Curran Associates Inc.
Chen, W., Mishra, S., and Paternain, S. (2024b). Domain adaptation for offline reinforcement learning with
limited samples. arXiv preprint arXiv:2408.12136.
Chen, X., Hu, J., Jin, C., Li, L., and Wang, L. (2022). Understanding domain randomization for sim-to-real
transfer. arXiv preprint arXiv:2110.03239.
Chua, K., Lei, Q., and Lee, J. (2023). Provable hierarchy-based meta-reinforcement learning. In Ruiz, F.,
Dy, J., and van de Meent, J.-W., editors, Proceedings of The 26th International Conference on Artificial
Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 10918–10967.
PMLR.
Domingues, O. D., Flet-Berliac, Y., Leurent, E., Ménard, P., Shang, X., and Valko, M. (2021a). rlberry - A
Reinforcement Learning Library for Research and Education.
Domingues, O. D., Ménard, P., Kaufmann, E., and Valko, M. (2021b). Episodic reinforcement learning in
finite MDPs: Minimax lower bounds revisited. In Algorithmic Learning Theory, pages 578–598. PMLR.
Domingues, O. D., Menard, P., Pirotta, M., Kaufmann, E., and Valko, M. (2021c). Kernel-based rein-
forcement learning: A finite-time analysis. In Meila, M. and Zhang, T., editors, Proceedings of the 38th
International Conference on Machine Learning, volume139ofProceedings of Machine Learning Research,
pages 2783–2792. PMLR.
Dong, K., Wang, Y., Chen, X., and Wang, L. (2019). Q-learning with UCB exploration is sample efficient
for infinite-horizon MDP. arXiv preprint arXiv:1901.09311.
Doshi-Velez,F.andKonidaris,G.D.(2013). Hiddenparametermarkovdecisionprocesses: Asemiparametric
regression approach for discovering latent task parametrizations. IJCAI : proceedings of the conference,
2016:1432–1440.
Duan, Y., Jia, Z., andWang, M.(2020). Minimax-optimaloff-policyevaluationwithlinearfunctionapprox-
imation. In International Conference on Machine Learning, pages 2701–2709. PMLR.
Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. (2016). Rl2: Fast reinforce-
ment learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779.
Duan, Y., Wang, M., and Wainwright, M. J. (2021). Optimal policy evaluation using kernel-based temporal
difference methods. arXiv preprint arXiv:2109.12002.
Durrett, R. (2019). Probability: Theory and Examples. Cambridge University Press.
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley,
T., Dunning, I., Legg, S., and Kavukcuoglu, K. (2018). IMPALA: Scalable distributed deep-RL with
importanceweightedactor-learnerarchitectures. InDy,J.andKrause,A.,editors,Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pages 1407–1416. PMLR.
Eysenbach, B., Asawa, S., Chaudhari, S., Levine, S., and Salakhutdinov, R. (2021). Off-dynamics reinforce-
ment learning: Training for transfer with domain classifiers. arXiv preprint arXiv:2006.13916.
Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep
networks. In Precup, D. and Teh, Y. W., editors, Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126–1135. PMLR.
13GheshlaghiAzar, M., Munos, R., andKappen, H.J.(2013). Minimaxpacboundsonthesamplecomplexity
of reinforcement learning with a generative model. Mach. Learn., 91(3):325–349.
He, J., Zhou, D., and Gu, Q. (2021). Nearly minimax optimal reinforcement learning for discounted mdps.
InRanzato,M.,Beygelzimer,A.,Dauphin,Y.,Liang,P.,andVaughan,J.W.,editors,AdvancesinNeural
Information Processing Systems, volume 34, pages 22288–22300. Curran Associates, Inc.
He,S.,Wang,Y.,Han,S.,Zou,S.,andMiao,F.(2023). Arobustandconstrainedmulti-agentreinforcement
learningelectricvehiclerebalancingmethodinamodsystems.In2023IEEE/RSJInternationalConference
on Intelligent Robots and Systems (IROS), pages 5637–5644.
Hsu, D., Kakade, S. M., and Zhang, T. (2012). A spectral algorithm for learning hidden markov models.
arXiv preprint arXiv:0811.4413.
Jafarnia-Jahromi,M.,Wei,C.-Y.,Jain,R.,andLuo,H.(2020). Amodel-freelearningalgorithmforinfinite-
horizon average-reward MDPs with near-optimal regret. arXiv preprint arXiv:2006.04354.
Jaksch, T., Ortner, R., andAuer, P.(2010). Near-optimalregretboundsforreinforcementlearning. Journal
of Machine Learning Research, 11(51):1563–1600.
Jiang, N. and Huang, J. (2020). Minimax value interval for off-policy evaluation and policy optimization.
Advances in Neural Information Processing Systems, 33:2747–2758.
Jiang, N. and Li, L. (2016). Doubly robust off-policy value evaluation for reinforcement learning. In Inter-
national Conference on Machine Learning, pages 652–661. PMLR.
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is q-learning provably efficient? In Bengio, S.,
Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., andGarnett, R., editors, Advances in Neural
Information Processing Systems, volume 31. Curran Associates, Inc.
Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T. (2020). Reward-free exploration for reinforcement
learning. InIII,H.D.andSingh,A.,editors,Proceedingsofthe37thInternationalConferenceonMachine
Learning, volume 119 of Proceedings of Machine Learning Research, pages 4870–4879. PMLR.
Jonsson, A., Kaufmann, E., Ménard, P., Domingues, O. D., Leurent, E., and Valko, M. (2020). Planning in
markov decision processes with gap-dependent sample complexity. arXiv preprint arXiv:2006.05879.
Kallus,N.andUehara,M.(2020). Doublereinforcementlearningforefficientoff-policyevaluationinmarkov
decision processes. Journal of Machine Learning Research, 21(167):1–63.
Kaufmann, E., Ménard, P., Darwiche Domingues, O., Jonsson, A., Leurent, E., and Valko, M. (2021).
Adaptive reward-free exploration. In Feldman, V., Ligett, K., and Sabato, S., editors, Proceedings of the
32nd International Conference on Algorithmic Learning Theory, volume 132 of Proceedings of Machine
Learning Research, pages 865–891. PMLR.
Kwon, J., Efroni, Y., Caramanis, C., and Mannor, S. (2024). Rl for latent mdps: regret guarantees and
a lower bound. In Proceedings of the 35th International Conference on Neural Information Processing
Systems, NIPS ’21, Red Hook, NY, USA. Curran Associates Inc.
Lattimore, T. and Hutter, M. (2012). Pac bounds for discounted mdps. In Proceedings of the 23rd Interna-
tional Conference on Algorithmic Learning Theory, ALT’12, page 320–334, Berlin, Heidelberg. Springer-
Verlag.
Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press.
Lehmann, E. L. and Casella, G. (2006). Theory of point estimation. Springer Science & Business Media.
Li,G.,Zhan,W.,Lee,J.D.,Chi,Y.,andChen,Y.(2024). Reward-agnosticfine-tuning: provablestatistical
benefits of hybrid reinforcement learning. In Proceedings of the 37th International Conference on Neural
Information Processing Systems. Curran Associates Inc.
14Li, L., Munos, R., and Szepesvári, C. (2014). On minimax optimal offline policy evaluation. arXiv preprint
arXiv:1409.3653.
Li, Q., Zhai, Y., Ma, Y., and Levine, S. (2023). Understanding the complexity gains of single-task RL
with a curriculum. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J.,
editors,Proceedings of the 40th International Conference on Machine Learning,volume202ofProceedings
of Machine Learning Research, pages 20412–20451. PMLR.
Liu,J.,Zhang,H.,andWang,D.(2022).Dara: Dynamics-awarerewardaugmentationinofflinereinforcement
learning. arXiv preprint arXiv:2203.06662.
Liu, S. and Su, H. (2021). Regret bounds for discounted mdps. arXiv preprint arXiv:2002.05138.
Liu, Z. and Xu, P. (2024). Minimax optimal and computationally efficient algorithms for distributionally
robust offline reinforcement learning. arXiv preprint arXiv:2403.09621.
Luo, F.-M., Jiang, S., Yu, Y., Zhang, Z., and Zhang, Y.-F. (2022). Adapt to environment sudden changes
by learning a context sensitive policy. Proceedings of the AAAI Conference on Artificial Intelligence,
36(7):7637–7646.
Ma, X., Liang, Z., Blanchet, J., Liu, M., Xia, L., Zhang, J., Zhao, Q., and Zhou, Z. (2023). Distributionally
robustofflinereinforcementlearningwithlinearfunctionapproximation. arXivpreprintarXiv:2209.06620.
Menard, P., Domingues, O. D., Shang, X., and Valko, M. (2021). Ucb momentum q-learning: Correcting
the bias without forgetting. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 7609–
7618. PMLR.
Mutti, M. and Tamar, A. (2024). Test-time regret minimization in meta reinforcement learning. arXiv
preprint arXiv:2406.02282.
Ménard, P., Domingues, O. D., Jonsson, A., Kaufmann, E., Leurent, E., and Valko, M. (2021). Fast active
learningforpureexplorationinreinforcementlearning. InProceedingsofthe38thInternationalConference
on Machine Learning, pages 7599–7608. PMLR. ISSN: 2640-3498.
Niu, H., Hu, J., Zhou, G., and Zhan, X. (2024). A comprehensive survey of cross-domain policy transfer
for embodied agents. In Larson, K., editor, Proceedings of the Thirty-Third International Joint Confer-
ence on Artificial Intelligence, IJCAI-24, pages 8197–8206. International Joint Conferences on Artificial
Intelligence Organization. Survey Track.
Niu, H., Ji, T., Liu, B., Zhao, H., Zhu, X., Zheng, J., Huang, P., Zhou, G., Hu, J., and Zhan, X. (2023).
H2o+: An improved framework for hybrid offline-and-online rl with dynamics gaps. arXiv preprint
arXiv:2309.12716.
Niu, H., sharma, s., Qiu, Y., Li, M., Zhou, G., HU, J., and Zhan, X. (2022). When to trust your simulator:
Dynamics-aware hybrid offline-and-online reinforcement learning. In Koyejo, S., Mohamed, S., Agarwal,
A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information Processing Systems,
volume 35, pages 36599–36612. Curran Associates, Inc.
O' Donoghue, B. (2021). Variational bayesian reinforcement learning with regret bounds. In Ranzato, M.,
Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information
Processing Systems, volume 34, pages 28208–28221. Curran Associates, Inc.
Panaganti,K.andKalathil,D.(2022). Samplecomplexityofrobustreinforcementlearningwithagenerative
model. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 9582–9602.
Peng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P. (2018). Sim-to-real transfer of robotic con-
trol with dynamics randomization. In 2018 IEEE International Conference on Robotics and Automation
(ICRA), pages 3803–3810.
15Qiu,S.,Ye,J.,Wang,Z.,andYang,Z.(2021). Onreward-freerlwithkernelandneuralfunctionapproxima-
tions: Single-agent mdp and markov game. In Meila, M. and Zhang, T., editors, Proceedings of the 38th
International Conference on Machine Learning, volume139ofProceedings of Machine Learning Research,
pages 8737–8747. PMLR.
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021). Bridging offline reinforcement learning
and imitation learning: A tale of pessimism. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang,
P., and Vaughan, J. W., editors, Advances in Neural Information Processing Systems, volume 34, pages
11702–11716. Curran Associates, Inc.
Ren,T.,Li,J.,Dai,B.,Du,S.S.,andSanghavi,S.(2021). Nearlyhorizon-freeofflinereinforcementlearning.
Advances in neural information processing systems, 34.
Serrano, S. A., Martinez-Carranza, J., and Sucar, L. E. (2023). Similarity-based knowledge transfer for
cross-domain reinforcement learning. arXiv preprint arXiv:2312.03764.
Shi,L.,Li,G.,Wei,Y.,Chen,Y.,Geist,M.,andChi,Y.(2023).Thecuriouspriceofdistributionalrobustness
in reinforcement learning with a generative model. In Oh, A., Naumann, T., Globerson, A., Saenko, K.,
Hardt, M., and Levine, S., editors, Advances in Neural Information Processing Systems, volume 36, pages
79903–79917. Curran Associates, Inc.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai,
M.,Bolton,A.,etal.(2017). Masteringthegameofgowithouthumanknowledge. nature,550(7676):354–
359.
Sodhani, S., Zhang, A., and Pineau, J. (2021). Multi-task reinforcement learning with context-based rep-
resentations. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on
Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9767–9779. PMLR.
Song, Y., Zhou, Y., Sekhari, A., Bagnell, J. A., Krishnamurthy, A., and Sun, W. (2023). Hybrid rl: Using
both offline and online data can make rl efficient. arXiv preprint arXiv:2210.06718.
Talebi, M. S. and Maillard, O.-A. (2018). Variance-aware regret bounds for undiscounted reinforcement
learning in mdps. In Janoos, F., Mohri, M., and Sridharan, K., editors, Proceedings of Algorithmic
Learning Theory, volume 83 of Proceedings of Machine Learning Research, pages 770–805. PMLR.
Thomas, P. and Brunskill, E. (2016). Data-efficient off-policy policy evaluation for reinforcement learning.
In International Conference on Machine Learning, pages 2139–2148. PMLR.
Uehara, M., Huang, J., and Jiang, N. (2020). Minimax weight and Q-function learning for off-policy evalu-
ation. In International Conference on Machine Learning, pages 9659–9668. PMLR.
Wagenmaker, A. J., Chen, Y., Simchowitz, M., Du, S., and Jamieson, K. (2022). Reward-free RL is no
harder than reward-aware RL in linear Markov decision processes. In Chaudhuri, K., Jegelka, S., Song,
L., Szepesvari, C., Niu, G., and Sabato, S., editors, Proceedings of the 39th International Conference on
Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 22430–22456. PMLR.
Wainwright,M.J.(2019).Variance-reducedq-learningisminimaxoptimal.arXivpreprintarXiv:1906.04697.
Wang, H., Shi, L., and Chi, Y. (2024). Sample complexity of offline distributionally robust linear markov
decision processes. arXiv preprint arXiv:2403.12946.
Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran,
D., and Botvinick, M. (2017). Learning to reinforcement learn. arXiv preprint arXiv:1611.05763.
Wang, R., Du, S. S., Yang, L., and Salakhutdinov, R. R. (2020). On reward-free reinforcement learning
with linear function approximation. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,
H., editors, Advances in Neural Information Processing Systems, volume 33, pages 17816–17826. Curran
Associates, Inc.
16Wang, Y., Zheng, Z., and Shen, M. (2023). Online pricing with polluted offline data. SSRN Electronic
Journal.
Wen, X., Bai, C., Xu, K., Yu, X., Zhang, Y., Li, X., and Wang, Z. (2024). Contrastive representation for
data filtering in cross-domain offline reinforcement learning. arXiv preprint arXiv:2405.06192.
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2021). Policy finetuning: Bridging sample-efficient
offline and online reinforcement learning. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and
Vaughan, J. W., editors, Advances in Neural Information Processing Systems, volume 34, pages 27395–
27407. Curran Associates, Inc.
Xiong, X., Wang, J., Zhang, F., and Li, K. (2016). Combining deep reinforcement learning and safety based
control for autonomous driving. arXiv preprint arXiv:1612.00147.
Xu, T., Yang, Z., Wang, Z., and Liang, Y. (2021). A unified off-policy evaluation approach for general value
function. arXiv preprint arXiv:2107.02711.
Yang, K., Yang, L., and Du, S. (2021). Q-learning with logarithmic regret. In International Conference on
Artificial Intelligence and Statistics, pages 1576–1584. PMLR.
Yang,M.,Nachum,O.,Dai,B.,Li,L.,andSchuurmans, D.(2020). Off-policyevaluationviatheregularized
Lagrangian. Advances in Neural Information Processing Systems, 33:6551–6561.
Ye, H., Chen, X., Wang, L., and Du, S. S. (2023). On the power of pre-training for generalization in RL:
Provable benefits and hardness. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and
Scarlett, J., editors, Proceedings of the 40th International Conference on Machine Learning, volume 202
of Proceedings of Machine Learning Research, pages 39770–39800. PMLR.
Yin,M.,Bai,Y.,andWang,Y.-X.(2020).Nearoptimalprovableuniformconvergenceinoff-policyevaluation
for reinforcement learning. arXiv preprint arXiv:2007.03760.
You, H., Yang, T., Zheng, Y., Hao, J., and Taylor, Matthew, E. (2022). Cross-domain adaptive trans-
fer reinforcement learning based on state-action correspondence. In Cussens, J. and Zhang, K., editors,
Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, volume 180 of Pro-
ceedings of Machine Learning Research, pages 2299–2309. PMLR.
Zanette, A. and Brunskill, E. (2019). Tighter problem-dependent regret bounds in reinforcement learning
withoutdomainknowledgeusingvaluefunctionbounds.InInternationalConferenceonMachineLearning,
pages 7304–7312. PMLR.
Zanette, A., Lazaric, A., Kochenderfer, M. J., and Brunskill, E. (2020). Provably efficient reward-agnostic
navigation with linear value iteration. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,
H., editors, Advances in Neural Information Processing Systems, volume 33, pages 11756–11766. Curran
Associates, Inc.
Zhang, G., Feng, L., Wang, Y., Li, M., Xie, H., and Tan, K. C. (2024). Reinforcement learning with
adaptive policy gradient transfer across heterogeneous problems. IEEE Transactions on Emerging Topics
in Computational Intelligence, 8(3):2213–2227.
Zhang, K., Kakade, S., Basar, T., and Yang, L. (2020a). Model-based multi-agent RL in zero-sum Markov
games with near-optimal sample complexity. Advances in Neural Information Processing Systems, 33.
Zhang, R. and Zanette, A. (2023). Policy finetuning in reinforcement learning via design of experiments
usingofflinedata. InAdvances in Neural Information Processing Systems, volume36, pages59953–59995.
Curran Associates, Inc.
Zhang, Z., Du, S., and Ji, X. (2021a). Near optimal reward-free reinforcement learning. In Meila, M. and
Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pages 12402–12412. PMLR.
17Zhang, Z., Ji, X., and Du, S. (2021b). Is reinforcement learning more difficult than bandits? a near-optimal
algorithm escaping the curse of horizon. In Belkin, M. and Kpotufe, S., editors, Proceedings of Thirty
Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages
4528–4531. PMLR.
Zhang, Z., Zhou, Y., and Ji, X. (2020b). Almost optimal model-free reinforcement learning via reference-
advantage decomposition. Advances in Neural Information Processing Systems, 33.
Zhou, Z., Zhou, Z., Bai, Q., Qiu, L., Blanchet, J., and Glynn, P. (2021). Finite-sample regret bound for
distributionally robust offline tabular reinforcement learning. In Banerjee, A. and Fukumizu, K., editors,
Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of
Proceedings of Machine Learning Research, pages 3331–3339. PMLR.
Zhu, Z., Lin, K., Jain, A. K., and Zhou, J. (2023). Transfer learning in deep reinforcement learning: A
survey. arXiv preprint arXiv:2009.07888.
A Proof of the Minimax Lower Bound
A.1 Preliminaries and Notations
We consider the following transfer setting: for a given source MDP = ( , ,H,p ,r,ρ), the target
src src
MDP =( , ,H,p ,r,ρ) is similar to the source MDP in the Msense thatS: A
tar tar
M S A
max TV(p ( s,a),p ( s,a)) α.
tar src
s,a∈S×A ·| ·| ≤
WeassumethatthetargetMDPtransitionp isunknowntous. ThusallthepossibletargetMDPscompose
tar
the following set , which is given by:
α
M
(cid:26) (cid:27)
≜ =( , ,H,p,r,ρ) max TV(p( s,a),p ( s,a)) α .
α src
M M S A |s,a∈S×A ·| ·| ≤
We use E to denote the expectation with respect to the randomness during algorithm execution in the
Mtar
target MDP .
tar
The proofMof Theorem 1 follows a similar construction as in Lattimore and Hutter (2012); Rashidinejad
et al. (2021); Xie et al. (2021). The new technical challenge in our setting is to bound the information gain
of the data from with different dynamics. Specifically,
src
M
• Although the minimax lower bound result in Rashidinejad et al. (2021) considers different dynamics,
they construct the MDP class based on the offline occupancy measures, which is quite different from
our construction. We instead put more emphasis on the source and target environmental gaps in our
construction.
• Lattimore and Hutter (2012); Rashidinejad et al. (2021); Xie et al. (2021) do not assume access to an
additional source offline dataset. In contrast, in our setting, we need to control the information gain
and bias from this additional source dataset.
As an overview of the proof, we construct a set of hard instance MDPs as the source MDP and the possible
target MDPs, and demonstrate that the maximum sample efficiency gain from can be bounded via a
src
change-of-measure approach. M
A.2 Proof of Theorem 1
Proof. We construct a set of MDPs with S+2 states, A actions and H steps, which are replicas of the hard
MDP presented in Fig. 3. Without loss of generality, we assume S 1 and H 3. The action space is [A].
For the state space, each hard MDP contains S bandit states deno≥ted by s ≥ , one good state s and
i i∈[S] g
{ }
18S S
1 g b 1
r =1 r =0
1(1 +γ1(a=a⋆)) 1(1 γ1(a=a⋆))
H 2 i H 2 − i
S
i
r =0
1 1
− H
Figure 3: Hard MDPs
onebadstates . Theinitialstatedistributionisauniformdistributionover s , givenbyρ(s )=1/S,
b i i∈[S] i
i [S]. { }
∀ ∈For each hard MDP, at each bandit state s , the transition probability is given by:
i
 1
p(s
i
|s i,a)=1
1−
1H, ∀a ∈[A],
p(s s ,a)= ( +γ1 a=a⋆ ), a [A],
p(sg | si
,a)=
H
1
(12 γ1{ a=a⋆i}
),
∀
a
∈
[A],
b | i H 2 − { i} ∀ ∈
where γ [0,1/3] is a constant to be specified later, and a⋆ [A] is the unique optimal action at each
bandit sta∈te s . We denote each hard MDP by , where ai ⋆ ∈ [A]S denotes the optimal action vector at
i a⋆
each bandit state. Therefore, we have [A]S diffeMrent hard MDP∈s with different optimal actions. The agent
receives zero reward at the bandit states, that is, r(s ,a)=0, a [A].
i
Apart from the bandit states, s and s are both absorbin∀g st∈ates with p(s s ,a) = p(s s ,a) = 1,
g b g g b b
a [A]. Theagentreceivereward1ats andzerorewardats . Thatis,r(s ,a)=| 1,r(s ,a)=| 0, a [A],
g b g b
∀ i ∈ [S]. ∀ ∈
∀ ∈Furthermore,wedefineaspecialMDP withnooptimalactions. Specifically,thetransitionprobability
0
at bandit states in is given by: M
0
M
 1
p(s
i
|s i,a)=1
1−
H, ∀a ∈[A],
p(s s ,a)= , a [A],
p(sg | si
,a)=
2 1H
,
∀
a
∈
[A],
b | i 2H ∀ ∈
while others remain the same as the hard MDPs. We choose this special MDP as our source MDP =
src
. By the definition of , we clearly see that when γ 48ε, each hard instance belongMto the
pMos0 sible target MDP set Ma s⋆ ince: ≤ H Ma⋆
α
M
γ 48ε
max TV(p ( s,a),p ( s,a))= α.
s,a∈S×A 0 ·| a⋆ ·| H ≤ H2 ≤
To prove Theorem 1, we only need to show that Theorem 1 holds on these hard MDPs. That is, we restrict
ourselves to the subcase where:
.
tar a⋆ α
M ∈{M }⊂M
In the following steps, we condider the average suboptimality gap over and link this gap to the
a⋆
number of samples from the target MDP. The key step is to bound the i{nMform}ation gain from the source
MDP via a change-of-measure approach.
19To begin with, we consider the uniform priori ν over a⋆(i.e., the hard MDPs), given by ν(a⋆) = 1/AS,
a⋆ [A]S. ForanyalgorithmALG,weaimtolinkthefollowingaveragesuboptimalitygapwiththenumber
∀of sa∈mples from the target MDP:
E E (cid:2) V⋆ Vπˆ (cid:3) ,
a⋆∼ν a⋆ 1,Ma⋆ − 1,Ma⋆
where E is taken with respect to the execution of ALG in .
a⋆ a⋆
{M }
Decompose suboptimality gaps. Let πˆ( ) denotes the output policy of ALG. First, note that the
state distribution dπ(s )=1/S (1 1/H)h−1· ≜| · d (s ) ( (h,s) [H] [S]) does not depend on the policy
π. Therefore, we hah vei · − h i ∀ ∈ ×
V⋆ Vπˆ
1,Ma⋆ − 1,Ma⋆
H S (cid:20) (cid:18) (cid:19) (cid:21)
(cid:88)(cid:88) 1 1 1
= dπˆ(s ) +γ (1 πˆ (a⋆ s )) (H h)
h i · H 2 − 2H · − h i | i · −
h=1i=1
(cid:88)H (cid:88)S
1
(cid:18)
1
(cid:19)h−1
= 1 (1 h/H)γ (1 πˆ (a⋆ s )).
S − H − · − h i | i
h=1i=1
Taking expectation with respect to the algorithm execution within the MDP M , we get
a⋆
E (cid:2) V⋆ Vπˆ (cid:3)
=(cid:88)H (cid:88)S
1
(cid:18)
1 1
(cid:19)h−1
(1 h/H)γ E (1 πˆ (a⋆ s ))
a⋆ 1,Ma⋆ − 1,Ma⋆ S − H − · a⋆ − h i | i
h=1i=1
(cid:88)H (cid:88)S
1
(cid:18)
1
(cid:19)h−1
= 1 (1 h/H)γ E (1 πˆ (a⋆ s ))
S − H − · a⋆ − h i | i
h=1i=1
H S
γ (cid:88)(cid:88)
(1 h/H)E (1 πˆ (a⋆ s )). ((1 1/H)h−1 1/e 1/3)
≥ 3S · − a⋆ − h i | i − ≥ ≥
h=1i=1
This gives us:
H S
E (cid:2) V⋆ Vπˆ (cid:3) γ (cid:88)(cid:88) (1 h/H)E (1 πˆ (a⋆ s )).
a⋆ 1,Ma⋆ − 1,Ma⋆ ≥ 3S · − a⋆ − h i | i
h=1i=1
Bound information gain. Let L (πˆ,a)≜E (1 πˆ (a⋆ s )) denotes the loss function. Therefore, we
have: h,i a⋆ − h i | i
H S
E E (cid:2) V⋆ Vπˆ (cid:3) γ (cid:88)(cid:88) (1 h/H)E L (πˆ,a). (7)
a⋆∼ν a⋆ 1,Ma⋆ − 1,Ma⋆ ≥ 3S · − a⋆∼ν h,i
h=1i=1
LetN (s ,a,s′)denotethevisitationof(s ,a,s′)in andN (s ,a,s′)denotethevisitationof(s ,a,s′)in
a⋆ i i a⋆ 0 i i
. Since the only data that reveal information abMout a⋆ is N (s , , ) and N (s , , ), the visitation count
Mats0 ,theposteriorofa⋆ dependsonlyonN (s , , )andNi (s ,a⋆ , ),i t·he· sufficien0 tsi ta·ti·sticsforthisposterior.
Supi pose the output poi licy πˆ (a⋆,s )=f(Na⋆ ,Ni · )·is given0 byi so·m· e measure function f :(N ,N ) [0,1].
By Lehmann and Casella (20h 06,i Thi eorem 10 .1 ofa⋆ Section 4), we have: 0 a⋆ (cid:55)→
E [L (πˆ,a⋆)] infE E (1 f(N ,N )).
a⋆∼ν h,i
≥ f
a⋆∼ν Na⋆∼(pa⋆,ALG),N0∼(p0,ALG)
−
0 a⋆
Weuse N (s ,a,s′) todenotetheprobabilitymeasureofrandomvariable N (s ,a,s′) whenthe
a⋆ i p,ALG a⋆ i
transitio{n probability}i|s p and the executing algorithm is ALG. By a change of mea{sure approach}, we have:
E [L (πˆ,a⋆)] infE E (1 f(N ,N ))
a⋆∼ν h,i
≥ f
a⋆∼ν Na⋆∼(pa⋆,ALG),N0∼(p0,ALG)
−
0 a⋆
infE E (1 f(N ,N ))
≥ f
a⋆∼ν Na⋆∼(p0,ALG),N0∼(p0,ALG)
−
0 a⋆
201 (cid:88)
E E sup f(N ,n)(P [N =n] P [N =n])
− 2 a⋆∼ν N0∼(p0,ALG) | 0 Na⋆∼(pa⋆,ALG) a⋆ − Na⋆∼(p0,ALG) a⋆ |
f n∈NA×S
(change of measure)
infE E (1 f(N ,N ))
≥ f
a⋆∼ν Na⋆∼(p0,ALG),N0∼(p0,ALG)
−
0 a⋆
E TV( N (s ,a,s′) , N (s ,a,s′) ) (definition of total variation)
−
a⋆∼ν
{
a⋆ i }|p0,ALG
{
a⋆ i }|pa⋆,ALG
=infE E (1 f(N ,N ))
f
Na⋆∼(p0,ALG),N0∼(p0,ALG) a⋆∼ν
−
0 a⋆
E TV( N (s ,a,s′) , N (s ,a,s′) )
− a⋆∼ν { a⋆ i }|p0,ALG { a⋆ i }|(p fa⋆ i, sAL bG ounded and measurable, Fubini’s theorem)
A
1 (cid:88)
=infE [(1 f(N ,N )) a⋆ =k]
f Na⋆∼(p0,ALG),N0∼(p0,ALG)A − 0 a⋆ | i
k=1
E TV( N (s ,a,s′) , N (s ,a,s′) ) (law of total expectation)
−
a⋆∼ν
{
a⋆ i }|p0,ALG
{
a⋆ i }|pa⋆,ALG
A 1
= A− −E a⋆∼νTV( {N a⋆(s i,a,s′) }|p0,ALG, {N a⋆(s i,a,s′) }|pa⋆,ALG),
where in the last equality we utilize the fact that (cid:80)A [f(N ,N ) a⋆ = k] = (cid:80)A πˆ (k,s ) = 1. This
demonstrates that the information gain from the
sourk c= e1 MDP0
is
ba o⋆ un|dei
d on
averagek .=1 h i
Link the average suboptimality gap to the sample complexity. Since we have already linked the
average loss function to the total variation distance, the rest we need to do is to link the total variation
distance to the number of samples. To achieve this, by Lattimore and Szepesvári (2020, Lemma 15.1), we
have:
(cid:114)
A 1 1
E a⋆∼ν[L h,i(πˆ,a⋆)]
≥
A− −E a⋆∼ν 2KL( {N a⋆(s i,a,s′) }|p0,ALG, {N a⋆(s i,a,s′) }|pa⋆,ALG)
(Pinsker’s inequality)
(cid:118)
(cid:117) A
A 1 (cid:117)1(cid:88)
≥
A− −E a⋆∼ν(cid:116)
2
E p0,ALG[N a⋆(s i,a)] ·KL(p 0( ·|s i,a),p a⋆( ·|s i,a))
a=1
(Lattimore and Szepesvári (2020, Lemma 15.1))
(cid:114)
A 1 1
≥
A− −E a⋆∼ν 2E p0,ALG[N a⋆(s i,a⋆ i)] ·KL(p 0( ·|s i,a⋆ i),p a⋆( ·|s i,a⋆ i))
(p and p only differ in a⋆ at s )
0 a⋆ i i
(cid:114)
A 1 2γ2
≥
A− −E a⋆∼ν E p0,ALG[N a⋆(s i,a⋆ i)]
· H
(KL(p ( s ,a),p ( s ,a))= 1 log( 1 ) 4γ2 for γ 1/3)
0 ·| i a⋆ ·| i 2H 1−4γ2 ≤ H ≤
(cid:118)
1 (cid:117) (cid:117)1 (cid:88)A 2γ2
(cid:116) E [N (s ,a)] (definition of ν and Jensen inequality)
≥ 2 − A p0,ALG a⋆ i · H
a=1
(cid:114)
1 2γ2
= E [N (s )].
2 − HA p0,ALG a⋆ i
Therefore, with Eq. (7) we can link the average suboptimality gap to the number of samples from the target
MDP .
a⋆
M
H S
E E (cid:2) V⋆ Vπˆ (cid:3) γ (cid:88)(cid:88) (1 h/H)E L (πˆ,a).
a⋆∼ν a⋆ 1,Ma⋆ − 1,Ma⋆ ≥ 3S · − a⋆∼ν h,i
h=1i=1
γ(H 1) γ(H 1)(cid:88)S (cid:114) 2γ2
≥
12−
−
6− HAE p0,ALG[N a⋆(s i)]
i=1
21 (cid:118) 
γ(H 1) 1 (cid:117) (cid:117) 2γ2 (cid:88)S
≥
6− 
2
−(cid:116)
SHA
E p0,ALG[N a⋆(s i)]
i=1
(cid:114)
γ(H 1) 1 2γ2n
≥
6− (
2 −
HSA), ((cid:80)S i=1E p0,ALG[N a⋆(s i)] ≤n)
where n denotes the number of samples from the target MDP . Recall that γ 48ε/H, we take
a⋆
(cid:113) M ≤
γ =48ε/H 1/3, for any ε 1/48, as long as 2γ2n 1, i.e.,
≤ ≤ HSA ≤ 4
HSA H3SA
n = ,
≤ 32γ2 32 482ε2
·
we have the average suboptimality gap greater than ε:
E E (cid:2) V⋆ Vπˆ (cid:3) 2ε(H 1)/H ε. (H 2)
a⋆∼ν a⋆ 1,Ma⋆ − 1,Ma⋆ ≥ − ≥ ≥
Therefore, for any algorithm ALG, there exists an a⋆ [A]S, as long as n H3SA , we have:
∈ ≤ 32·482ε2
E (cid:2) V⋆ Vπˆ (cid:3) ε.
a⋆ 1,Ma⋆ − 1,Ma⋆ ≥
This ends the proof.
B Proof of the Upper Bound
B.1 Preliminaries and Notations
Empirical MDP. Let (st,at,(st)′) denote the observed transitions in the target MDP in the t-th
{ h h h }h∈[H]
episode. For any state-action pair (s,a) , we let nt(s,a) ≜ (cid:80)t (cid:80)H 1 (sτ,aτ)=(s,a) denote
∈ S ×A τ=1 h=1 { h h }
the visitation count for (s,a) in the first t episodes in the target MDP, and correspondingly nt(s,a,s′) ≜
(cid:80)t (cid:80)H 1 (sτ,aτ,(sτ)′)=(s,a,s′) . With these definitions, we can define the empirical transitions as:
τ=1 h=1 { h h h }
nt(s,a,s′)
pˆt (s′ s,a)≜ if nt(s,a)>0,
tar | nt(s,a)
and pˆt (s′ s,a) ≜ 1/S otherwise. Besides, let ˆt ≜ ( , ,H,pˆt ,r,ρ) denote the empirical MDP. We
denotetar the Q| -function of a policy π evaluated onM ˆtar byS QpˆAt tar,π, at na dr the value function by Vpˆt tar,π.
Mtar h h
B.2 Proof of Lemma 1
B.2.1 Good Events
For Algorithm 2, we now start by considering good events inspired by Ménard et al. (2021). We consider
the following good event FRF:
(cid:26)
g
(nt(s,a),δ)(cid:27)
F ≜ t N+, (s,a) :KL(pˆt ( s,a),p ( s,a)) 1 ,
1 ∀ ∈ ∀ ∈S×A tar ·| tar ·| ≤ nt(s,a)
(cid:26) (cid:27)
1
F ≜ t N+, (s,a) :nt(s,a) nt (s,a) g (δ) ,
2 ∀ ∈ ∀ ∈S×A ≥ 2 tar − 3
F ≜ (s,a) :TV(p ( s,a),pˆ ( s,a)) β/4 ,
3 src src
{∀ ∈S×A ·| ·| ≤ }
3
(cid:92)
FRF ≜ F ,
i
i=1
22where the bonus functions are defined as follows:
6SAH
g (n,δ)≜log( )+Slog(8e(n+1)), (8)
1 δ
6SA
g (δ)≜log( ). (9)
3 δ
Let pt (s,a) denote the probability of visiting (s,a) at step h in the t-th episode under p . Furthermore,
h,tar tar
let nt (s,a) ≜ (cid:80)t (cid:80)H pt (s,a) denote the cummulative probability of visiting (s,a) in the first t
tar i=1 h=1 h,tar
episodes in Algorithm 2.
Lemma 2. We have P(FRF) 1 δ/2.
≥ −
Proof. By Lemma 11, we have P(F ) 1 δ/6. By Lemma 12, we have P(F ) 1 δ/6. By Lemma 10
1 2
and the assumption on in Lemm≥a 1,−we have P(F ) 1 δ/6. By union≥bou−nd, we have P(F) =
src 3
P((cid:84)3 F ) 1 δ/2. D ≥ −
i=1 i ≥ −
B.2.2 Key lemmas
Weadaptthereward-freeexplorationframeworktodirectlycontrolthetransitionestimationerrors,enabling
us to identify the shifted region within finite samples. Specifically, we consider a special class of reward
function r given by: B R
≜ r :[H] [0,1] some h⋆ [H], if h=h⋆,r (, , )=0 .
h
R { ×S×A×S (cid:55)→ |∃ ∈ ̸ · · · }
Concretely, any reward function r not only depends on the current state-action, but also on possible
next states. Moreover, r is non-zero∈oRnly at one step h⋆ (possibly different for different r ). For r ,
for any policy π and transition p, the Q-functions and value functions are defined as follow∈s:R ∈R
(cid:88)
Qp,π(s,a)≜ p(s′ s,a)(r (s,a,s′)+Vp,π(s′)), (s,a) ,h [H],
h | h h+1 ∀ ∈S×A ∈
s′
Vp,π(s)≜maxQp,π(s,a), s ,h [H],
h a h ∀ ∈S ∈
Qp,π (s,a)≜0,Vp,π (s)≜0, (s,a) .
H+1 H+1 ∀ ∈S×A
The specific definition of here helps us control the high-dimensional transition estimation errors for any
(s,a) and remove an H Rfactor from the sample complexity result. To achieve this goal, we define the
uncertainty function (with Wt (s,a)=0):
H+1
(cid:18) 4Hg (nt(s,a),δ) (cid:19)
Wt(s,a)=min 1, 1 +pˆt maxWt (s,a) . (10)
h nt(s,a) tar a′∈A h+1
We define the error term as:
eπ,t(s,a,r)≜ Qptar,π(s,a,r) Qˆpˆt tar,π (s,a,r).
h | h − h |
We first show that Wt(s,a) can be utilized to establish an upper bound on eπ,t(s,a,r) in the following
1 1
lemma.
Lemma 3. For any policy π and any r , in any episode t in Algorithm 2, w.p. at least 1 δ/2, we have:
∈R −
(cid:113)
[ρπ eπ,t](r) 3 ρπ Wt+ρπ Wt,
1 1 ≤ 1 1 1 1
where [ρπ eπ,t](r) = (cid:80) ρ(s)π (a s)eπ,t](s,a,r). The proof of Lemma 3 can be found in Ap-
1 1 s,a 1 | 1
pendix C.1. Next, we show that by choosing πt+1(s) = argmax Wt(s,a), Wt can be used to upper
1 a∈A 1 1
bound TV(p ,pˆt ) uniformly for any (s,a) , which helps us derive the stopping criterion for
Algorithm 2.tar tar ∈ S × A
23Lemma 4. Under Assumption 1, for any δ (0,1), w.p. 1 δ/2, for any t N, any (s,a) , there exists
∈ − ∈ ∈B
a policy π and reward r , such that:
∈R
TV(p ( s,a),pˆt ( s,a)) 2 [ρπ eπ,t](r) 2 (3(cid:113) ρmaxWt+ρmaxWt),
tar ·| tar ·| ≤ σ 1 1 ≤ σ a∈A 1 a∈A 1
where ρmax Wt =(cid:80) ρ(s)max Wt(s,a).
a∈A 1 s∈S a∈A 1
Proof. Inthislemma,sinceonly isconsidered,wedropthesubscript“tar” forclarity. Firstwebuildan
tar
upper bound on [ρπ eπ,t](r) for aMny π and r . By Lemma 3, if we take πt+1(s)=argmax Wt(s,a),
we can obtain that:1 1 ∈R 1 a∈A 1
(cid:113)
[ρπ eπ,t](r) 3 ρπt+1Wt+πt+1ρWt. (11)
1 1 ≤ 1 1 1 1
Now we build the connection between [ρπ eπ,t](r) and TV(p,pˆt). First by the definition of Qp,π, for r
we have: 1 1 1 ∈ R
H
(cid:88)
ρπ Qp,π =E [ r (s ,a ,s )]
1 1 π h h h h+1
h=1
=E [r (s ,a ,s )] (only r is non-zero)
π h⋆ h⋆ h⋆ h⋆+1 h⋆
(cid:88)
= pπ (s,a)p(s′ s,a)r (s,a,s′),
h⋆
|
h⋆
s,a,s′
where the last equality follows by recalling pπ (s,a) is the probability of reaching (s,a) at step h⋆ under
h⋆
policy π. Therefore, by triangle inequality, we have:
ρπ eπ,t(r) ρπ Qp,π ρπ Qpˆt,π
1 1 ≥| 1 1 − 1 1 |
(cid:88) (cid:88)
= pπ (s,a)p(s′ s,a)r (s,a,s′) pˆπ,t(s,a)pˆt(s′ s,a)r (s,a,s′), (12)
| h⋆ | h⋆ − h⋆ | h⋆ |
s,a,s′ s,a,s′
In the following step, we fix a given (s,a) for the analysis. For a given (s,a), we first take (π′,h⋆) =
argmax pπ(s,a), and choose r′ as:
π,h h ∈R
(cid:26) 1 if s=s,a=a,h=h⋆,
r′(s,a, )=
h · 0 otherwise.
Substituting r′ and π′ into Eq. (12) yields that:
pπ′ (s,a) pˆπ′,t(s,a) ρπ′eπ′,t(r′). (13)
| h⋆ − h⋆ |≤ 1 1
Then we choose the reward function r′′ to be:
∈R
(cid:26) 1 if s=s,a=a,h=h⋆,p(s′ s,a)>pˆt(s′ s,a),
r h′′(s,a,s′)= 0 otherwise. | |
Substituting r′′ and π′ into Eq. (12) yields that:
(cid:12) (cid:12)
ρπ′eπ′,t(r′′) (cid:12) (cid:12)pπ′ (s,a)(cid:88) p(s′ s,a)r′′ (s,a,s′) pˆπ′,t(s,a)(cid:88) pˆt(s′ s,a)r′′ (s,a,s′)(cid:12) (cid:12)
1 1 ≥(cid:12) (cid:12) h⋆ | h⋆ − h⋆ | h⋆ (cid:12) (cid:12)
s′ s′
(cid:12)
=(cid:12) (cid:12)pπ′ (s,a)((cid:88) p(s′ s,a)r′′ (s,a,s′) pˆt(s′ s,a)r′′ (s,a,s′))
(cid:12)
(cid:12)
h⋆
|
h⋆
− |
h⋆
s′
(cid:12)
(pˆπ′,t(s,a) pπ′ (s,a))(cid:88) pˆt(s′ s,a)r′′ (s,a,s′)(cid:12) (cid:12)
− h⋆ − h⋆ | h⋆ (cid:12) (cid:12)
s′
pπ′ (s,a) (cid:88) p(s′ s,a)r′′ (s,a,s′) pˆt(s′ s,a)r′′ (s,a,s′)
≥
h⋆
| |
h⋆
− |
h⋆
|
s′
24pπ′ (s,a) pˆπ′,t(s,a) pˆtr′′ (s,a) (triangle inequality)
−| h⋆ − h⋆ || h⋆ |
=pπ′ (s,a) (cid:88) (p(s′ s,a) pˆt(s′ s,a))
h⋆
| − |
s′:p(s′|s,a)>pˆt(s′|s,a)
pπ′ (s,a) pˆπ′,t(s,a) pˆtr′′ (s,a) (Definition of r′′)
−| h⋆ − h⋆ || h⋆ |
=pπ′ (s,a)TV(p( s,a),pˆt( s,a)) pπ′ (s,a) pˆπ′,t(s,a) pˆtr′′ (s,a)
h⋆ ·| ·| −| h⋆ − h⋆ || h⋆ |
σTV(p( s,a),pˆt( s,a)) ρπ′eπ′,t(r′), (Assumption 1. Eq. (13))
≥ ·| ·| − 1 1
Thus, we obtain that:
TV(p( s,a),pˆt( s,a)) 1 (ρπ′eπ′,t(r′)+ρπ′eπ′,t(r′′))
·| ·| ≤ σ 1 1 1 1
2
max
ρπ′eπ′,t(r)
≤ σ r∈{r′,r′′} 1 1
2 (cid:113)
(3 ρπt+1Wt+ρπt+1Wt), (Lemma 3)
≤ σ 1 1 1 1
where πt+1(s) = argmax Wt(s,a) as we said before. This ends the proof and yields the choice of the
1 a∈A 1
stopping criterion of Algorithm 2.
B.2.3 Proof of Lemma 1
Proof. Asaoverviewoftheproof,wefirstshowthatthestoppingcriterioncanguaranteethatwehave ˆ= .
Then we solve for the upper bound of the stopping time τ to get the sample complexity of AlgorithmB2. B
Identify the shift successfully. BythestoppingcriterioninAlgorithm2andLemma4, wehaveforany
(s,a) :
∈S×A 2 (cid:113)
TV(p ( s,a),pˆt ( s,a)) (3 ρπt+1Wt+ρπt+1Wt) β/4.
tar ·| tar ·| ≤ σ 1 1 1 1 ≤
On event FRF, we have:
TV(p ( s,a),pˆ ( s,a)) β/4,
src src
·| ·| ≤
which implies that if TV(pˆt ( s,a),pˆ ( s,a))>β/2, we have:
tar ·| src ·|
TV(p ( s,a),p ( s,a)) TV(pˆt ( s,a),pˆ ( s,a)) TV(p ( s,a),pˆt ( s,a))
src ·| tar ·| ≥ tar ·| src ·| − tar ·| tar ·|
TV(p ( s,a),pˆ ( s,a)) (triangle inequality)
src src
− ·| ·|
>β/2 β/4 β/4=0.
− −
This means that ˆ . On the contrary, if TV(p ( s,a),p ( s,a)) β, we have:
src tar
B ⊆B ·| ·| ≥
TV(pˆt ( s,a),pˆ ( s,a)) TV(p ( s,a),p ( s,a)) TV(p ( s,a),pˆt ( s,a))
tar ·| src ·| ≥ src ·| tar ·| − tar ·| tar ·|
TV(p ( s,a),pˆ ( s,a)) (triangle inequality)
src src
− ·| ·|
β β/4 β/4=β/2,
≥ − −
which implies that ˆ. Thus, by Lemma 2, we have = ˆw.p. 1 δ/2. This ends the proof of the first
part. B ⊆B B B −
Solveforthesamplecomplexity. NextwesolveforthesamplecomplexityAlgorithm2. Inthefollowing
steps, since only is considered, we drop the subscript ‘tar’ for clarity.
tar
As an overviewMof this part, we first upper bound ρπt+1W with the weighted sum of exploration bonus
1 1
under the true visitation probability. Further, we control this term by change the visitation count to the
cumulative visitation probability. Summing these upper bounds over t < τ, we get an inequality for τ.
Solving this inequality, we finally get an upper bound of τ, and in turn the sample complexity.
25To begin with, we establish an upper bound on Wt(s,a) for all (s,a,h). If nt(s,a)>0, we have:
h
Hg (nt(s,a),δ)
Wt(s,a) 4 1 +pˆtmaxWt (s,a)
h ≤ nt(s,a) a′∈A h+1
Hg (nt(s,a),δ)
=4 1 +pπt+1Wt (s,a)+(pˆt p)πt+1Wt (s,a)
nt(s,a) h+1 h+1 − h+1 h+1
(cid:115)
Hg (nt(s,a),δ) g (nt(s,a),δ) 2g (nt(s,a),δ)
4 1 +pπt+1Wt (s,a)+ 2Var (πt+1Wt )(s,a) 1 + 1
≤ nt(s,a) h+1 h+1 p h+1 h+1 nt(s,a) 3 nt(s,a)
(Lemma 14)
(cid:115)
Hg (nt(s,a),δ) 1 g (nt(s,a),δ)
5 1 +pπt+1Wt (s,a)+ 2 p(πt+1Wt )(s,a)H 1
≤ nt(s,a) h+1 h+1 H h+1 h+1 nt(s,a)
(Var (πt+1Wt )(s,a) pπt+1Wt (s,a))
p h+1 h+1 ≤ h+1 h+1
g (nt(s,a),δ) 1
7H 1 +(1+ )pπt+1Wt (s,a). ( √xy x+y)
≤ nt(s,a) H h+1 h+1 ≤
Since Wt(s,a) 1, we have for all nt(s,a) 0:
h ≤ ≥
g (nt(s,a),δ) 1
Wt(s,a) 7H( 1 1)+(1+ )pπt+1Wt (s,a).
h ≤ nt(s,a) ∧ H h+1 h+1
Apply this inequality recursively, we get:
ρπt+1Wt
7eH(cid:88)H (cid:88) pt+1(s,a)(g 1(nt(s,a),δ)
1). ((1+ 1)h e)
1 1 ≤ h nt(s,a) ∧ H ≤
h=1 s,a
Suppose the stopping time is τ, by the stopping criterion and Eq. (32) we have for t<τ,
(cid:118)
σβ/8
9(cid:117) (cid:117) (cid:116)eH(cid:88)H (cid:88) pt+1(g 1(nt(s,a),δ) 1)+7eH(cid:88)H (cid:88) pt+1g 1(nt(s,a),δ)
1.
≤ h nt(s,a) ∧ h nt(s,a) ∧
h=1(s,a)∈S×A h=1(s,a)∈S×A
Summing over all t<τ, we obtain that,
(cid:118)
(τ 1)σβ
72τ (cid:88)−1(cid:117) (cid:117) (cid:116)eH(cid:88)H (cid:88) pt+1(g 1(nt(s,a),δ) 1)+56eHτ (cid:88)−1 (cid:88)H (cid:88) pt+1g 1(nt(s,a),δ)
1
− ≤ h nt(s,a) ∧ h nt(s,a) ∧
t=1 h=1(s,a)∈S×A t=1h=1 s,a
(cid:118)
72(cid:117) (cid:117)
(cid:116)eH(τ
1)τ (cid:88)−1 (cid:88)H (cid:88) pt+1(g 1(nt(s,a),δ)
1)
≤ − h nt(s,a) ∧
t=1h=1(s,a)∈S×A
+56eHτ (cid:88)−1 (cid:88)H (cid:88) pt+1g 1(nt(s,a),δ)
1 (Cauchy-Schwarz inequality)
h nt(s,a) ∧
t=1h=1(s,a)∈S×A
(cid:118)
72(cid:117) (cid:117) (cid:116)eHττ (cid:88)−1 (cid:88)H (cid:88) pt+1(g 1(nt(s,a),δ) )+56eHτ (cid:88)−1 (cid:88)H (cid:88) pt+1g 1(nt(s,a),δ)
≤ h nt(s,a) 1 h nt(s,a) 1
t=1h=1(s,a)∈S×A ∨ t=1h=1(s,a)∈S×A ∨
(Lemma 9)
(cid:118)
(cid:117)
(cid:117)
τ (cid:88)−1 (cid:88)nt+1(s,a) nt(s,a) τ (cid:88)−1 (cid:88)nt+1(s,a) nt(s,a)
≤72(cid:116)eHτg 1(τ −1,δ) nt(s,a−
) 1
+56eHg 1(τ −1,δ) nt(s,a−
) 1
t=1 s,a ∨ t=1 s,a ∨
(cid:112)
144 eHτg (τ 1,δ)SAlog(τ +1)+224eHSAg (τ 1,δ)log(τ +1) (Lemma 18)
1 1
≤ − −
26S S S S
2 3 2 3
1 1 1 1 1+δ 1 δ 1 δ 1+δ
2 2 2 2 2 2− 2− 2
S S S S
2 3 2 3
1 1 1 1
2 2 2 2
S S
1 1
Figure 4: The MDP on the left is the real MDP, and the MDP on the right is the empirical MDP. For any
reward function r(, ) and any policy π, the simulation error Q Qˆ is zero, while the estimation error
1 1
can be significant w·i·th δ. | − |
(cid:114)
6SAH 6SAH
144 eHτSA(log( log(8eτ)+Slog2(8eτ)))+224eHSA(log( log(8eτ)+Slog2(8eτ))).
≤ δ δ
(log(τ +1) log(8eτ))
≤
Solving this inequality by Lemma 19, we obtain that:
HSA
τ C ,
≤
1(σβ)2
where C >0 only contains log factors. Therefore, Algorithm 2 can identify using at most C H2SA from
1
B
1(σβ)2
the target MDP. This concludes the proof.
Remark 2. Note that directly applying the proof technique in Ménard et al. (2021) or any other sample-
efficient algorithms cannot establish bounds on the estimation error of transitions and in turn identify the
shifted region. This can be illustrated through the example in Fig. 4. Besides, in terms of sample complexity,
simply apply the proof technique in Ménard et al. (2021) will result in an additional H factor in the sample
complexity result, while we improve upon this in our problem by considering a more general reward functions.
B.3 Proof of Theorem 2
B.3.1 Good Events
Since the sample complexity of the first step in Algorithm 1 is already studied in Lemma 1, here we mainly
focus on the episodes in the second step (i.e., Algorithm 3). We use the index t to denote the number of
episodes in the second step of Algorithm 1. For Algorithm 1, we consider the following good event FHybrid:
(cid:26)
g
(n˜t(s,a),δ)(cid:27)
F ≜ t N+, (s,a) :KL(p˜t( s,a),p ( s,a)) 1 ,
4 ∀ ∈ ∀ ∈S×A ·| tar ·| ≤ n˜t(s,a)
(cid:26) (cid:27)
1
F ≜ t N+, (s,a) ˆ:n˜t(s,a) nt (s,a) g (δ) ,
5 ∀ ∈ ∀ ∈B ≥ 2 tar − 3
(cid:26)
F ≜ t N+, h [H], (s,a) ,
6
∀ ∈ ∀ ∈ ∀ ∈S×A
(cid:115)
(cid:12) (cid:12)(p˜t −p tar)V hp +ta 1r,⋆(s,a)(cid:12) (cid:12) ≤min(cid:18) H, 2Var ptar(V hp +ta 1r,⋆)(s,a)g 2(n n˜ ˜t t( (s s, ,a a) ),δ) + 3Hg 2 n˜( tn˜ (t s( ,s a, )a),δ)(cid:19)(cid:27) ,
(cid:18) 6 (cid:19)
(cid:92) (cid:92)
FHybrid ≜FRF F ,
i
i=4
where the bonus functions g and g , event FRF, are defined in Appendix B.2, and g is defined as follows:
1 3 2
6SAH
g (n,δ)≜log( )+log(8e(n+1)).
2 δ
27In the definition of FHybrid,
(cid:40)
nt(s,a), for (s,a) ˆ,
n˜t(s,a)≜ ∈B
n (s,a), for (s,a) / ˆ,
src
∈B
(cid:40)
pˆt ( s,a), for (s,a) ˆ,
p˜t( s,a)≜ tar ·| ∈B
·| pˆ ( s,a), for (s,a) / ˆ,
src
·| ∈B
where nt(s,a) denotes the visitation count in Algorithm 3. Let pt (s,a) denote the probability of visiting
h,tar
(s,a)atstephinthet-thepisodeunderthetruedynamics. Furthermore,letnt (s,a)≜(cid:80)t (cid:80)H pt (s,a)
tar i=1 h=1 h,tar
denote the cumulative probability of visiting (s,a) in the first t episodes.
Lemma 5. We have P(FHybrid) 1 δ.
≥ −
Proof. ByLemma1,oneventFRF,wehave ˆ= ,whichimpliesthatfor(s,a) / ˆ,p˜t( s,a)=pˆ ( s,a)
src
is the unbiased estimate of p ( s,a), forB (s,a)B ˆ, p˜t( s,a)=pˆt ( s,a),∈tBhus by·|Lemma 11 w·e|have
P(F FRF) 1 δ/6. tar ·| ∈B ·| tar ·|
4
By Le|mma 1≥2, w−e have w.p. 1 δ/6,
−
1 6 ˆ 1 6SA
n˜t(s,a)=nt(s,a) nt (s,a) log( |B|) nt (s,a) log( ), t N, (s,a) ˆ,
≥ 2 tar − δ ≥ 2 tar − δ ∀ ∈ ∀ ∈B
thus we have P(F FRF) 1 δ/6.
5
Similarly, on event| FRF, w≥e h−ave ˆ = , which implies that for (s,a) / ˆ, p˜t( s,a) = pˆ ( s,a) is
src
the unbiased estimate of p ( s,aB ), forB (s,a) ˆ, p˜t( s,a) = pˆt ( ∈ s,B a), thu·s|by Lemma 13· |we have
P(F FRF) 1 δ/6.
Byta ur n· io|
n bound, we
hav∈
e
PB (FHy· br| id)=P((cid:84)t 6ar · F|
FRF) P(FRF) 1 δ.
6 | ≥ − i=4 i | · ≥ −
Remark 3. F differs from F in that F considers the sequence of p˜t in the execution of Algorithm 3, while
4 1 4
F considers the sequence of pˆt in the execution of Algorithm 2. A similar observation holds for F and F .
1 2 5
B.3.2 Key Lemmas
We define the following confidence intervals for the optimal value functions,
(cid:32) (cid:115)
Qt (s,a)≜min H,r(s,a)+3 Var (Vt )(s,a)g 2(n˜t(s,a),δ) +14H2g 1(n˜t(s,a),δ)
h p˜t h+1 n˜t(s,a) n˜t(s,a)
(cid:19)
+ 1 p˜t(Vt Vt )(s,a)+p˜tVt (s,a) , (s,a) , (14)
H h+1− h+1 h+1 ∀ ∈S×A
Vt (s)≜maxQt (s,a), (15)
h h
a∈A
Vt (s)≜0, (16)
H+1
(cid:32) (cid:115)
Qt(s,a)≜max 0,r(s,a) 3 Var (Vt )(s,a)g 2(n˜t(s,a),δ) 14H2g 1(n˜t(s,a),δ)
h − p˜t h+1 n˜t(s,a) − n˜t(s,a)
(cid:19)
1 p˜t(Vt Vt )(s,a)+p˜tVt (s,a) , (s,a) , (17)
−H h+1− h+1 h+1 ∀ ∈S×A
Vt(s)≜maxQt(s,a), (18)
h a∈A h
Vt (s)≜0. (19)
H+1
Lemma 6 (Algorithm ensures optimism). On event FHybrid, we have that for all t, all h [H], and any
∈
(s,a) ,
∈S×A Qt(s,a) Qptar,⋆(s,a) Qt (s,a), (20)
h ≤ h ≤ h
and
Vt(s) Vptar,⋆(s) Vt (s). (21)
h ≤ h ≤ h
28The proof of Lemma 6 can be found in Appendix C.2. We further define the following value functions to
bound Vptar,πt+1:
h
(cid:32) (cid:32) (cid:115)
Q˜t(s,a)≜min r(s,a)+p V˜t(s,a),max 0,r(s,a) 3 Var (Vt )(s,a)g 2(n˜t(s,a),δ)
h tar h − p˜t h+1 n˜t(s,a)
14H2g 1(n˜t(s,a),δ) 1 p˜t(Vt Vt )(s,a)+p˜tV˜t (s,a))(cid:19)(cid:19) , (s,a) ,
− n˜t(s,a) − H h+1− h+1 h+1 ∀ ∈S×A
V˜t(s)≜πt+1Q˜t(s,a),
h h h
V˜t (s)≜0.
H+1
With this definition, we are ready to lower bound Vptar,πt+1 in the following lemma.
h
Lemma 7. On event FHybrid, we have that for all (s,a,h),
(cid:16) (cid:17)
Q˜t(s,a) min Qt(s,a),Qptar,πt+1 (s,a) ,
h ≤ h h
(cid:16) (cid:17)
V˜t(s) min Vt(s),Vptar,πt+1 (s) .
h ≤ h h
Proof. Weproceedbyinduction. Forthebasecase,wehaveQ˜t (s,a)=Qt (s,a)=Qptar,πt+1 (s,a)=0,
H+1 H+1 H+1
sotheclaimtriviallyholds. Assumingthestatementholdsforsteph+1, thenforsteph, wefirstprovethat
Q˜t(s,a) Qptar,πt+1 (s,a). With the induction hypothesis, we have:
h ≤ h
Qptar,πt+1 (s,a) Q˜t(s,a) p (Vptar,πt+1 V˜t )(s,a) 0.
h − h ≥ tar h+1 − h+1 ≥
Then by definition, we have:
V˜t(s)=maxQ˜t(s,a) maxQptar,πt+1 (s,a)=Vptar,πt+1 (s).
h a∈A h ≤ a∈A h h
This ends the proof of the first part. For the second part, we have:
(cid:32) (cid:115)
Q˜t(s,a) max 0,r(s,a) 3 Var (Vt )(s,a)g 2(n˜t(s,a),δ)
h ≤ − p˜t h+1 n˜t(s,a)
14H2g 1(n˜t(s,a),δ) 1 p˜t(Vt Vt )(s,a)+p˜tV˜t (s,a))(cid:19)
− n˜t(s,a) − H h+1− h+1 h+1
(cid:32) (cid:115)
max 0,r(s,a) 3 Var
(Vt )(s,a)g 2(n˜t(s,a),δ)
≤ − p˜t h+1 n˜t(s,a)
14H2g 1(n˜t(s,a),δ) 1 p˜t(Vt Vt )(s,a)+p˜tVt (s,a))(cid:19)
− n˜t(s,a) − H h+1− h+1 h+1
=Qt(s,a). (22)
h
Then by definition we have:
V˜t(s)=maxQ˜t(s,a) maxQt(s,a)=Vt(s).
h a∈A h ≤ a∈A h h
This ends the proof.
B.3.3 Proof of Theorem 2
(cid:113)
Case 1: σβ Sε. Since by Lemma 1, we know that we can identify using O(cid:101)(H2SA/(σβ)2) samples
≥ H B
from the target MDP, so we focus on the sample complexity of the second step of Algorithm 1 (i.e., Algo-
rithm 3). As an overview, we first show that the stopping criterion in Algorithm 3 can yield an ε-optimal
policy for the target MDP, then we study its sample complexity.
29Optimality guarantee. We first define function Gt to upper bound the performance gap Vptar,∗(ρ)
h 1 −
Vptar,πt+1 (ρ). Specifically, let Gt ≜0 for all (s,a) , and for all h [H],
1 H+1 ∈S×A ∈
(cid:32) (cid:115)
Gt(s,a)≜min H,6 Var (Vt )(s,a)g 2(n˜t(s,a),δ) +35H2g 1(n˜t(s,a),δ)
h p˜t h+1 n˜t(s,a) n˜t(s,a)
(cid:19)
3
+(1+ )p˜tπt+1Gt (s,a) , (s,a) , (23)
H h+1 h+1 ∀ ∈S×A
We show in Lemma 8 that Gt can be used to upper bound the optimality gap. The proof of Lemma 8 can
1
be found in Appendix C.3.
Lemma 8. On event FUCBVI, for all t,
Vptar,⋆(ρ) Vptar,πt+1 (ρ) ρπt+1Gt. (24)
1 − 1 ≤ 1 1
By Lemma 8, the stopping criterion in Algorithm 3 will yield an ε-optimal policy for the target MDP.
This ends the proof of this part.
Sample complexity of Algorithm 3. As an overview of the proof for sample complexity result, we first
link Gt(s,a) with the summation of functions of the visitation count n˜t(s,a) for all (s,a) , then we
divide1 the summation into two parts. For (s,a) / ˆ, we can control the summation by t∈heSa×ssuAmption of
offlinedata. For(s,a) ˆ,wechangethevisitatio∈nBcountnt(s,a)intothecumulativeprobabilityofvisiting
(s,a) in the first t epis∈odBes and further control this term. Finally, to get the sample complexity result, we
sum the value gap upper bound ρπt+1Gt over all t < τ and get an inequality of the stopping time τ. By
1 1
solving the inequality, we get an upper bound of τ, and in turn the sample complexity result.
Link Gt(s,a) to n˜t(s,a). To link Gt(s,a) to n˜t(s,a), we need to change the empirical transition into the
1 1
true transition to make sure we can apply Eq. (23) recursively. Then we replace the optimistic estimate
Vt (s,a) with the true value function Vptar,πt to remove an H factor. For the first part, we have:
h+1 h+1
(cid:115)
g (n˜t(s,a),δ) 2 g (n˜t(s,a),δ)
(p˜t p)πt+1Gt (s,a) 2Var (πt+1Gt )(s,a) 1 + H 1 (Lemma 14)
| − h+1 h+1 |≤ ptar h+1 h+1 n˜t(s,a) 3 n˜t(s,a)
1 g (n˜t(s,a),δ)
≤
8H2Var ptar(π ht+ +1 1Gt h+1)(s,a)+17H2 1
n˜t(s,a)
( √xy ≤x+y)
1 g (n˜t(s,a),δ)
p πt+1Gt (s,a)+17H2 1 . (0 πt+1Gt (s,a) H)
≤ 8H tar h+1 h+1 n˜t(s,a) ≤ h+1 h+1 ≤
For the second part we have:
Var (Vt )(s,a) 2Var (Vt )(s,a)+4H2g 1(n˜t(s,a),δ) (Lemma 16)
p˜t h+1 ≤ ptar h+1 n˜t(s,a)
4Var
(Vptar,πt+1
)(s,a)+4Hp
(Vt Vptar,πt+1 )(s,a)+4H2g 1(n˜t(s,a),δ)
≤ ptar h+1 tar h+1− h+1 n˜t(s,a)
(Lemma 17)
4Var (Vptar,πt+1 )(s,a)+4Hp πt+1Gt (s,a)+4H2g 1(n˜t(s,a),δ) (Lemma 8)
≤ ptar h+1 tar h+1 h+1 n˜t(s,a)
Combining this, and by √x+y √x+√y and √xy x+y, we have:
≤ ≤
(cid:115)
g (n˜t(s,a),δ) g (n˜t(s,a),δ)
Gt(s,a) 12 Var (Vptar,πt+1)(s,a) 2 +403H2 1
h ≤ ptar h+1 n˜t(s,a) n˜t(s,a)
3031 1
+(1+ + )p πt+1Gt (s,a)
8H 8H2 tar h+1 h+1
(cid:115)
g (n˜t(s,a),δ) g (n˜t(s,a),δ) 4
12 Var (Vptar,πt+1)(s,a) 2 +403H2 1 +(1+ )p πt+1Gt (s,a)
≤ ptar h+1 n˜t(s,a) n˜t(s,a) H tar h+1 h+1
Since 0 Gt(s,a) H, then for all n˜t(s,a) 0, the following inequality holds:
≤ h ≤ ≥
(cid:115)
g (n˜t(s,a),δ) g (n˜t(s,a),δ)
Gt(s,a) 12 Var (Vptar,πt+1)(s,a)( 2 1)+403H2 1 1
h ≤ ptar h+1 n˜t(s,a) ∧ n˜t(s,a) ∧
4
+(1+ )p πt+1Gt (s,a) (25)
H tar h+1 h+1
Applying this inequality recursively, and notice that (1+ 1)x e, we have:
x ≤
(cid:115)
ρπt+1Gt
12e4(cid:88)H (cid:88)
pt+1 Var
(Vπt+1)(s,a)(g 2(n˜t(s,a),δ)
1)
1 1 ≤ h,tar ptar h+1 n˜t(s,a) ∧
h=1(s,a)∈S×A
+403e4H2(cid:88)H (cid:88)
pt+1
g 1(n˜t(s,a),δ)
1 (26)
h,tar n˜t(s,a) ∧
h=1(s,a)∈S×A
Furthermore, by Lemma 15 and Cauchy-Schwarz inequality, for (s,a) ˆ, we obtain:
∈B
(cid:115)
(cid:88)H (cid:88)
pt+1 Var
(Vptar,πt+1)(s,a)(g 2(n˜t(s,a),δ)
1)
h,tar ptar h+1 n˜t(s,a) ∧
h=1(s,a)∈Bˆ
(cid:118) (cid:118)
(cid:117) (cid:117) (cid:117)(cid:88)H (cid:88)
pt+1 Var
(Vptar,πt+1)(s,a)(cid:117) (cid:117) (cid:117)(cid:88)H (cid:88)
pt+1
(g 2(nt(s,a),δ)
1)
≤(cid:116) h,tar ptar h+1 (cid:116) h,tar nt(s,a) ∧
h=1(s,a)∈Bˆ h=1(s,a)∈Bˆ
(Cauchy-Swarchz inequality)
(cid:118) (cid:118)
≤(cid:117) (cid:117) (cid:116)(cid:88)H (cid:88) pt h+ ,t1 arVar ptar(V hp +ta 1r,πt+1)(s,a)(cid:117) (cid:117) (cid:117) (cid:116)(cid:88)H (cid:88) pt h+ ,t1 ar(g 2(n nt t( (s s, ,a a) ),δ) ∧1)
h=1(s,a)∈S×A h=1(s,a)∈Bˆ
(cid:118) (cid:118)
=(cid:117) (cid:117)
(cid:116)E
((cid:88)H
r
Vptar,πt+1)2(cid:117) (cid:117) (cid:117)(cid:88)H (cid:88)
pt+1
(g 2(nt(s,a),δ)
1) (Lemma 15)
πt+1 h − 1 (cid:116) h,tar nt(s,a) ∧
h=1 h=1(s,a)∈Bˆ
(cid:118)
H(cid:117) (cid:117) (cid:117)(cid:88)H (cid:88)
pt+1
(g 2(nt(s,a),δ)
1) (27)
≤ (cid:116) h,tar nt(s,a) ∧
h=1(s,a)∈Bˆ
Similarly, for (s,a) / ˆ, we have:
∈B
(cid:115)
(cid:88)H (cid:88)
pt+1 Var
(Vptar,πt+1)(s,a)(g 2(n˜t(s,a),δ)
1)
h,tar ptar h+1 n˜t(s,a) ∧
h=1(s,a)∈/Bˆ
(cid:118) (cid:118)
(cid:117) H (cid:117) H
(cid:117) (cid:117)(cid:88) (cid:88)
pt+1 Var
(Vptar,πt+1)(s,a)(cid:117) (cid:117)(cid:88) (cid:88)
pt+1
(g 2(n src(s,a),δ)
1)
≤(cid:116) h,tar ptar h+1 (cid:116) h,tar n (s,a) ∧
src
h=1(s,a)∈/Bˆ h=1(s,a)∈/Bˆ
(Cauchy-Swarchz inequality)
31(cid:118) (cid:118)
(cid:117) H (cid:117) H
≤(cid:117) (cid:116)(cid:88) (cid:88) pt h+ ,t1 arVar ptar(V hp +ta 1r,πt+1)(s,a)(cid:117) (cid:117) (cid:116)(cid:88) (cid:88) pt h+ ,t1 ar(g 2(n nsrc( (s s, ,a a) ),δ) ∧1)
src
h=1(s,a)∈S×A h=1(s,a)∈/Bˆ
(cid:118) (cid:118)
(cid:117) H (cid:117) H
=(cid:117)
(cid:116)E
((cid:88)
r
Vptar,πt+1)2(cid:117) (cid:117)(cid:88) (cid:88)
pt+1
(g 2(n src(s,a),δ)
1) (Lemma 15)
πt+1 h − 1 (cid:116) h,tar n (s,a) ∧
src
h=1 h=1(s,a)∈/Bˆ
(cid:118)
(cid:117) H
H(cid:117) (cid:117)(cid:88) (cid:88)
pt+1
(g 2(n src(s,a),δ)
1) (28)
≤ (cid:116) h,tar n (s,a) ∧
src
h=1(s,a)∈/Bˆ
Control the summation separately. For (s,a) / ˆ, by the assumption of Theorem 2 we have:
∈B
(cid:118)
12e4H(cid:117) (cid:117) (cid:117)(cid:88)H (cid:88)
pt+1
(g 2(n src(s,a),δ)
1)
H(cid:114)
H
ε2
ε/4
(cid:116) h,tar n (s,a) ∧ ≤ 4 H3 ≤
src
h=1(s,a)∈/Bˆ
403e4H2(cid:88)H (cid:88)
pt+1
g 1(n src(s,a),δ)
1 H3
ε2
ε2/4 ε/4
h,tar n (s,a) ∧ ≤ 4H3 ≤ ≤
src
h=1(s,a)∈/Bˆ
For (s,a) ˆ, we first change the visitation count into the cumulative visitation probability with Lemma 9:
∈B
(cid:118)
12e4H(cid:117) (cid:117) (cid:117)(cid:88)H (cid:88)
pt+1
(g 2(nt(s,a),δ) 1)+403e4H2(cid:88)H (cid:88)
pt+1
g 1(nt(s,a),δ)
1
(cid:116) h,tar nt(s,a) ∧ h,tar nt(s,a) ∧
h=1(s,a)∈Bˆ h=1(s,a)∈Bˆ
(cid:118)
24e4H(cid:117) (cid:117) (cid:117)(cid:88)H (cid:88)
pt+1
g 2(nt tar(s,a),δ) +1612e4H2(cid:88)H (cid:88)
pt+1
g 1(nt tar(s,a),δ)
≤ (cid:116) h,tar nt (s,a) 1 h,tar nt (s,a) 1
h=1(s,a)∈Bˆ tar ∨ h=1(s,a)∈Bˆ tar ∨
Notice that (cid:80)H pt+1 (s,a)=nt+1(s,a) nt (s,a), thus we obtain:
h=1 h,tar − tar
(cid:88)H (cid:88)
pt+1
g 2(nt tar(s,a),δ)
=
(cid:88) g 2(nt tar(s,a),δ)(nt t+ ar1(s,a) −nt tar(s,a))
h,tar nt (s,a) 1 nt (s,a) 1
h=1(s,a)∈Bˆ tar ∨ (s,a)∈Bˆ tar ∨
(cid:88)H (cid:88)
pt+1
g 1(nt tar(s,a),δ)
=
(cid:88) g 1(nt tar(s,a),δ)(nt t+ ar1(s,a) −nt tar(s,a))
h,tar nt (s,a) 1 nt (s,a) 1
h=1(s,a)∈Bˆ tar ∨ (s,a)∈Bˆ tar ∨
Solving for the sample complexity. With the stopping criterion, we notice that for all t<τ, we have:
(cid:118)
ε/2
≤24e4H(cid:117)
(cid:117) (cid:116)
(cid:88) g 2(nt tar(s,a n) t,δ () s(n ,at t+
a
)r1
−
1nt tar)(s,a)
(s,a)∈Bˆ tar ∨
+1612e4H2
(cid:88) g 1(nt tar(s,a),δ)(nt t+ ar1 −nt tar)(s,a)
nt (s,a) 1
(s,a)∈Bˆ tar ∨
Summing over t<τ, we get:
(cid:118)
(τ −1)ε/2
≤24e4Hτ (cid:88)−1(cid:117)
(cid:117) (cid:116)
(cid:88) g 2(nt tar(s,a n) t,δ () s(n ,at t+
a
)r1
−
1nt tar)(s,a)
t=1 (s,a)∈Bˆ tar ∨
32+1612e4H2τ (cid:88)−1 (cid:88) g 1(nt tar(s,a),δ)(nt t+ ar1 −nt tar)(s,a)
nt (s,a) 1
t=1(s,a)∈Bˆ tar ∨
(cid:118)
24e4H√τ
1(cid:117) (cid:117) (cid:117)τ (cid:88)−1 (cid:88) g 2(nt tar(s,a),δ)(nt t+ ar1 −nt tar)(s,a)
≤ − (cid:116) nt (s,a) 1
t=1(s,a)∈Bˆ tar ∨
+1612e4H2τ (cid:88)−1 (cid:88) g 1(nt tar(s,a),δ)(nt t+ ar1 −nt tar)(s,a)
(Cauchy-Schwarz inequality)
nt (s,a) 1
t=1(s,a)∈Bˆ tar ∨
(cid:118)
24e4H√τ
1(cid:117) (cid:117) (cid:117)τ (cid:88)−1 (cid:88) g 2(τ −1,δ)(nt t+ ar1 −nt tar)(s,a)
≤ − (cid:116) nt (s,a) 1
t=1(s,a)∈Bˆ tar ∨
+1612e4H2τ (cid:88)−1 (cid:88) g 1(τ −1,δ)(nt t+ ar1 −nt tar)(s,a)
(g (n,δ), g (n,δ) is increasing in n)
nt (s,a) 1 1 2
t=1(s,a)∈Bˆ tar ∨
(cid:113)
48e4H ˆ(τ 1)g (τ 1,δ)log(τ +1)+6448e4H2 ˆg (τ 1,δ)log(τ +1) (Lemma 18)
2 1
≤ |B| − − |B| −
(cid:114)
6SAH
48e4H ˆτ(log( )log(8eτ)+log2(8eτ))
≤ |B| δ
6HSA
+6448e4H2 ˆ(log( )log(8eτ)+Slog2(8eτ)) (log(τ +1) log(8eτ))
|B| δ ≤
Solving this inequality by Lemma 19, we obtain that:
H2 ˆ
τ C |B|, (29)
≤ 2 ε2
(cid:113)
where C >0 contains only log factors. By Lemma 1, when σβ Sε, the number of total samples from
2 ≤ H
in Algorithm 1 to get a ε-optimal policy for is:
tar tar
M M
H2S2A H3
O(cid:101)( + |B|).
(σβ)2 ε2
(cid:113)
Case 2: σβ Sε. In this case, because we choose ˆ= , due to the above proof, the total sample
≤ H B S×A
complexity is:
H3 ˆ H3SA
O(cid:101)( |B|)=O(cid:101)( ).
ε2 ε2
Combining the two cases ends the proof.
C Auxiliary Proofs
C.1 Proof of Lemma 3
Proof. In this lemma, since only is considered, we drop the subscript "tar" for clarity. From our
tar
construction of reward class weMcan deduce that 0 Vπ 1. For nt(s,a)>0, we have:
R ≤ h ≤
eπ,t(s,a,r)≜ Qπ(s,a,r) Qπ,pˆt (s,a,r)
h | h − h |
(p pˆt)Vπ +pˆt Vπ Vˆπ,t + (p pˆt)r (s,a)
≤| − h+1| | h+1− h+1| | − h |
33(cid:115)
g (nt(s,a),δ)
(p pˆt)Vπ +pˆt Vπ Vˆπ,t + 1
≤| − h+1| | h+1− h+1| 2nt(s,a)
(cid:113)
((p pˆt)r (s,a) TV(pˆt,p) 1KL(p,pˆt))
| − h | ≤ ≤ 2
(cid:115) (cid:115)
g (nt(s,a),δ) 2g (nt(s,a),δ) g (nt(s,a),δ)
2Var (Vπ )(s,a,r) 1 + 1 +pˆt Vπ Vˆπ,t + 1
≤ p h+1 nt(s,a) 3 nt(s,a) | h+1− h+1| 2nt(s,a)
(Lemma 14)
(cid:115)
g (nt(s,a),δ) 2g (nt(s,a),δ)
3 1 + 1 +pˆt Vπ Vˆπ,t
≤ nt(s,a) 3 nt(s,a) | h+1− h+1|
(cid:115)
1 Hg (nt(s,a),δ) 1 g (nt(s,a),δ)
3 ( 1 )+4H 1 +pˆtπ eπ,t (s,a,r),
≤ H nt(s,a) ∧ H nt(s,a) h+1 h+1
where in the last inequality we utilize that if Hg1(nt(s,a),δ) 1, we have:
nt(s,a) ≥ H
(cid:115) (cid:115)
g (nt(s,a),δ) 1 g (nt(s,a),δ) 1 g (nt(s,a),δ) g (nt(s,a),δ)
1 = H2 1 H2 1 =H 1 .
nt(s,a) H2 nt(s,a) ≤ H nt(s,a) nt(s,a)
Motivated by Ménard et al. (2021), we further define Wπ,t recursively by Wπ,t (s,a)=0 and:
h H+1
(cid:18) 4Hg (nt(s,a),δ) (cid:19)
Wπ,t(s,a)≜min 1, 1 +pˆtπ Wπ,t (s,a) .
h nt(s,a) h+1 h+1
Besides, we define Yπ,t(s,a) recursively by Yπ,t (s,a)=0 and:
h H+1
(cid:115)
1 Hg (nt(s,a),δ) 1
Yπ,t(s,a,r)≜3 ( 1 )+pˆtπ Yπ,t(s,a).
h H nt(s,a) ∧ H h+1 h+1
With the above definitions, we can show by induction that eπ,t(s,a,r) Yπ,t(s,a)+Wπ,t(s,a). For the
base case, the claim trivially holds. Assume the argument holdh s for step≤ h+h 1, we have: h
(cid:115)
1 Hg (nt(s,a),δ) 1 g (nt(s,a),δ)
eπ,t(s,a,r) 3 ( 1 )+4H 1 +pˆtπ (Yπ,t(s,a)+Wπ,t (s,a)).
h ≤ H nt(s,a) ∧ H nt(s,a) h+1 h+1 h+1
(induction hypothesis)
Because eπ,t(s,a,r) 1, we actually have:
h ≤
(cid:115)
1 Hg (nt(s,a),δ) 1
eπ,t(s,a,r) 3 ( 1 )+Wπ,t(s,a)+pˆtπ Yπ,t(s,a)
h ≤ H nt(s,a) ∧ H h h+1 h+1
Yπ,t(s,a)+Wπ,t(s,a). (30)
≤ h h
In the following step, the core idea is to obtain an upper bound on [ρπ eπ,t](r). By applying Eq. (30), we
1 1
obtain that:
[ρπ eπ,t](r) ρπ Yπ,t+ρπ Wπ,t
1 1 ≤ 1 1 1 1
(cid:115)
3(cid:88)H (cid:88)
pˆπ,t
1/H(Hg 1(nt(s,a),δ) 1
)+ρπ Wπ,t
≤ h nt(s,a) ∧ H 1 1
h=1(s,a)∈S×A
(cid:118) (cid:118)
≤3(cid:117) (cid:117) (cid:116)(cid:88)H (cid:88) pˆπ h,t(s,a)/H(cid:117) (cid:117) (cid:116)(cid:88)H (cid:88) pˆπ h,t(Hg 1 n(n t(t s(s ,a,a )),δ)
∧
H1 )+ρπ 1W 1π,t
h=1(s,a)∈S×A h=1(s,a)∈S×A
(Cauchy-Schwarz inequality)
34(cid:118)
=3(cid:117) (cid:117) (cid:116)(cid:88)H (cid:88) pˆπ h,t(Hg 1 n(n t(t s(s ,a,a )),δ)
∧
H1 )+ρπ 1W 1π,t. (31)
h=1(s,a)∈S×A
We recursively define Wˆπ,t(s,a) by Wˆπ,t (s,a)≜0 and:
h H+1
Hg (nt(s,a),δ) 1
Wˆπ,t(s,a)≜ 1 +pˆtπ Wˆπ,t (s,a).
h nt(s,a) ∧ H h+1 h+1
By definition, we know that:
ρπ Wˆπ,t =(cid:88)H (cid:88) pˆπ,tHg 1(nt(s,a),δ) 1 .
1 1 h nt(s,a) ∧ H
h=1(s,a)∈S×A
Thus Eq. (31) can be rewritten as:
(cid:113)
[ρπ eπ,t](r) 3 ρπ Wˆπ,t+ρπ Wπ,t.
1 1 ≤ 1 1 1 1
We now show that Wˆπ,t(s,a) Wπ,t(s,a) Wt(s,a). For the base case, the claim trivially holds since
h ≤ h ≤ h
Wˆπ,t (s,a)=Wπ,t (s,a)=Wt (s,a)=0. Assumetheclaimholdsforsteph+1, andforstephwehave:
H+1 H+1 H+1
(cid:18) 4Hg (nt(s,a),δ) (cid:19)
Wπ,t(s,a) min 1, 1 +pˆtπ Wt (s,a) (induction hypothesis)
h ≤ nt(s,a) h+1 h+1
(cid:18) 4Hg (nt(s,a),δ) (cid:19)
min 1, 1 +pˆtmaxWt (s,a)
≤ nt(s,a) a′∈A h+1
=Wt(s,a).
h
Since by our construction, we have Wˆπ,t(s,a) 1, thus:
h ≤
(cid:18) Hg (nt(s,a),δ) 1 (cid:19)
Wˆπ,t(s,a) min 1, 1 +pˆtπ Wˆπ,t (s,a)
h ≤ nt(s,a) ∧ H h+1 h+1
(cid:18) 4Hg (nt(s,a),δ) (cid:19)
min 1, 1 +pˆtπ Wπ,t (s,a) (induction hypothesis)
≤ nt(s,a) h+1 h+1
=Wπ,t(s,a).
h
Therefore by induction, we have:
ρπ Wˆπ,t ρπ Wπ,t ρπ Wt.
1 1 ≤ 1 1 ≤ 1 1
So we obtain that:
(cid:113)
[ρπ eπ,t](r) 3 ρπ Wt+ρπ Wt. (32)
1 1 ≤ 1 1 1 1
C.2 Proof of Lemma 6
Proof. We proceed by induction. For the base case, we have Qt (s,a) = Qptar,⋆(s,a) = Qt = 0, so
H+1 H+1 H+1
Eq. (20) and Eq. (21) trivially hold. Assuming the statement hold for step h+1, then for step h, we have:
(cid:115)
Qt (s,a) Qptar,⋆(s,a)=3 Var (Vt )(s,a)g 2(n˜t(s,a),δ) +14H2g 1(n˜t(s,a),δ)
h − h p˜t h+1 n˜t(s,a) n˜t(s,a)
+p˜t(Vt Vt )(s,a)+p˜t(Vt Vptar,⋆)(s,a)+(p˜t p )Vptar,⋆(s,a).
h+1− h+1 h+1− h+1 − tar h+1
35Then by the definition of F , we have:
4
(cid:115)
(cid:12) (cid:12)(p˜t −p tar)V hp +ta 1r,⋆(s,a)(cid:12) (cid:12)
≤
2Var ptar(V hp +ta 1r,⋆)(s,a)g 2(n n˜ ˜t t( (s s, ,a a) ),δ) + 3Hg 2 n˜( tn˜ (t s( ,s a, )a),δ) ,
then by Lemma 16 and Lemma 17, we can substitute the variance of optimal value function under the true
dynamics with the one under the empirical dynamics,
g (n˜t(s,a),δ)
Var (Vptar,⋆) 2Var (Vptar,⋆)+4H2 1
ptar h+1 ≤ p˜t h+1 n˜t(s,a)
4Var (V )+4H2g 1(n˜t(s,a),δ) +4Hp˜t(Vt Vptar,⋆)
≤ p˜t h+1 n˜t(s,a) h+1− h+1
4Var (V )+4H2g 1(n˜t(s,a),δ) +4Hp˜t(Vt Vt ). (Induction hypothesis)
≤ p˜t h+1 n˜t(s,a) h+1− h+1
Notice that g 1(n,δ) g 2(n,δ), and by inequalities √x+y √x+√y and √xy x+y we have:
≥ ≤ ≤
(cid:115)
(cid:12) (cid:12)(p˜t −p)V hp +ta 1r,⋆(s,a)(cid:12) (cid:12)
≤
8Var p˜t(Vt h+1)g 2(n n˜ ˜t t( (s s, ,a a) ),δ) +(3+2√2)Hg 1(n n˜ ˜t t( (s s, ,a a) ),δ)
(cid:115)
+ 8H2g 1(n˜t(s,a),δ) 1 p˜t(Vt Vt )
n˜t(s,a) H h+1− h+1
(cid:115)
3 Var (Vt )g 2(n˜t(s,a),δ) +14H2g 1(n˜t(s,a),δ) + 1 p˜t(Vt Vt ).
≤ p˜t h+1 n˜t(s,a) n˜t(s,a) H h+1− h+1
Then with the induction hypothesis, we have:
Qt (s,a) Qptar,⋆(s,a) p˜t(Vt Vt )(s,a) 0
h − h ≥ h+1− h+1 ≥
The proof for Qt(s,a) Qptar,⋆(s,a) follows a similar procedure and we omit this proof for brevity. Then
by definition,
weh
also
ha≤ve:h
Vt (s)=maxQt (s,a) maxQptar,⋆(s,a)=Vptar,⋆(s),
h a∈A h ≥ a∈A h h
Vt(s)=maxQt(s,a) maxQptar,⋆(s,a)=Vptar,⋆(s).
h a∈A h ≤ a∈A h h
This end the proof.
C.3 Proof of Lemma 8
Proof. By Lemma 7, we only need to show Qt (s,a) Q˜t(s,a) Gt(s,a), and we proceed by induction.
For the base case, the claim trivially holds
becah
use
Qt− (h s,a)=≤
Q˜t
h
(s,a)=Gt(s,a)=0. Assuming the
H+1 H+1 h
claim holds for step h+1, then for step h, if Gt(s,a) = H, the claim still trivially holds. If Gt(s,a) = H,
we discuss two cases. h h ̸
Case 1 Q˜t(s,a)=r(s,a)+p V˜t (s,a), then we have:
h tar h+1
(cid:115)
Qt (s,a) Q˜t(s,a) 3 Var (Vt )(s,a)g 2(n˜t(s,a),δ) +14H2g 1(n˜t(s,a),δ)
h − h ≤ p˜t h+1 n˜t(s,a) n˜t(s,a)
+ 1 p˜t(Vt Vt )(s,a)+(p˜t p )Vptar,∗(s,a)
H h+1− h+1 − tar h+1
+pˆt(Vt V˜t )(s,a)+(p p˜t)(Vptar,⋆ V˜t )(s,a).
h+1− h+1 tar − h+1 − h+1
36From the proof of Lemma 6, we have:
(cid:115)
(cid:12) (cid:12)(p˜t −p tar)V hp +ta 1r,⋆(s,a)(cid:12) (cid:12) ≤3 Var p˜t(Vt h+1)g 2(n n˜ ˜t t( (s s, ,a a) ),δ) +14H2g 1(n n˜ ˜t t( (s s, ,a a) ),δ) + H1 p˜t(Vt h+1−Vt h+1).
By Lemma 14 and inequality √xy x+y we have:
≤
(cid:115)
2 g (n˜t(s,a),δ) 2 g (n˜t(s,a),δ)
(p p˜t)(Vptar,∗ V˜t )(s,a) Var (Vptar,∗ V˜t )(s,a)H2 1 + H 1
| tar − h+1 − h+1 |≤ H2 ptar h+1 − h+1 n˜t(s,a) 3 n˜t(s,a)
1 g (n˜t(s,a),δ)
Var (Vptar,∗ V˜t )(s,a)+5H2 1
≤ 2H2 ptar h+1 − h+1 n˜t(s,a)
1 g (n˜t(s,a),δ)
Var (Vptar,∗ V˜t )(s,a)+7H2 1 (Lemma 16)
≤ H2 p˜t h+1 − h+1 n˜t(s,a)
1 g (n˜t(s,a),δ)
p˜t(Vptar,∗ V˜t )(s,a)+7H2 1
≤ H h+1 − h+1 n˜t(s,a)
1 g (n˜t(s,a),δ)
p˜t(Vptar,∗ V˜t )(s,a)+7H2 1 , (Lemma 6)
≤ H h+1 − h+1 n˜t(s,a)
then we have:
(cid:115)
Qt (s,a) Q˜t(s,a) 6 Var (Vt )(s,a)g 2(n˜t(s,a),δ) +35H2g 1(n˜t(s,a),δ) + 2 p˜t(Vt Vt )(s,a)
h − h ≤ p˜t h+1 n˜t(s,a) n˜t(s,a) H h+1− h+1
+ 1 p˜t(Vptar,∗ V˜t )(s,a)+p˜t(Vt V˜t )(s,a)
H h+1 − h+1 h+1− h+1
(cid:115)
6 Var
(Vt )(s,a)g 2(n˜t(s,a),δ) +35H2g 1(n˜t(s,a),δ)
≤ p˜t h+1 n˜t(s,a) n˜t(s,a)
+(1+ 3 )p˜t(Vt V˜t )(s,a) (Lemma 7)
H h+1− h+1
(cid:115)
6 Var
(Vt )(s,a)g 2(n˜t(s,a),δ) +35H2g 1(n˜t(s,a),δ)
≤ p˜t h+1 n˜t(s,a) n˜t(s,a)
3
+(1+ )p˜tπt+1Gt (s,a). (Induction hypothesis)
H h+1 h+1
=Gt(s,a)
h
Case 2 Q˜t(s,a)=r(s,a)+p V˜t (s,a), then we have:
h ̸ tar h+1
(cid:115)
Qt (s,a) Q˜t(s,a) 6 Var (Vt )(s,a)g 2(n˜t(s,a),δ) +28H2g 1(n˜t(s,a),δ)
h − h ≤ p˜t h+1 n˜t(s,a) n˜t(s,a)
+ 2 p˜t(Vt Vt )(s,a)+p˜t(Vt V˜t )(s,a)
H h+1− h+1 h+1− h+1
(cid:115)
6 Var
(Vt )(s,a)g 2(n˜t(s,a),δ) +35H2g 1(n˜t(s,a),δ)
≤ p˜t h+1 n˜t(s,a) n˜t(s,a)
(1+ 3 )p˜t(Vt V˜t )(s,a)
H h+1− h+1
(cid:115)
6 Var (Vt )(s,a)g 2(n˜t(s,a),δ) +35H2g 1(n˜t(s,a),δ) (Lemma 7)
≤ p˜t h+1 n˜t(s,a) n˜t(s,a)
3
(1+ )p˜tπt+1Gt (s,a) (Induction hypothesis)
H h+1 h+1
37=Gt(s,a).
h
Thus, we have:
Vptar,⋆(ρ) Vptar,πt+1 (ρ) E (Vptar,∗(s) Vptar,πt+1 (s))
1 − 1 ≤ s∼ρ 1 − 1
E (Vt (s) V˜t(s)) (Lemma 6 and Lemma 7)
≤ s∼ρ 1 − 1
ρπt+1(Qt Q˜t)
≤ 1 1− 1
ρπt+1Gt.
≤ 1 1
C.4 Auxiliary Lemmas
Lemma 9 (Visitation count to cumulative visitation probability). On event FRF, (s,a) ,
∀ ∈S×A
g (nt(s,a),δ) g (nt (s,a),δ)
t N+, 1 1 4 1 tar .
∀ ∈ nt(s,a) ∧ ≤ nt (s,a) 1
tar ∨
g (nt(s,a),δ) g (nt (s,a),δ)
t N+, 2 1 4 2 tar .
∀ ∈ nt(s,a) ∧ ≤ nt (s,a) 1
tar ∨
On event FHybrid, (s,a) ˆ,
∀ ∈B
g (n˜t(s,a),δ) g (nt (s,a),δ)
t N+, 1 1 4 1 tar .
∀ ∈ n˜t(s,a) ∧ ≤ nt (s,a) 1
tar ∨
g (n˜t(s,a),δ) g (nt (s,a),δ)
t N+, 2 1 4 2 tar .
∀ ∈ n˜t(s,a) ∧ ≤ nt (s,a) 1
tar ∨
Proof. Weonlyprovetheresultforg ,andtheproofforg isexactlythesame. OneventFRF, (s,a) ,
1 2
t N+, we have ∀ ∈S×A
∀ ∈ 1
nt(s,a) nt (s,a) g (δ)
≥ 2 tar − 3
Case 1 If g (δ) 1nt (s,a), then
3 ≤ 4 tar
g (nt(s,a),δ) g (nt(s,a),δ) g (1nt (s,a),δ) g (nt (s,a),δ)
1 1 1 1 4 tar 4 1 tar
nt(s,a) ∧ ≤ nt(s,a) ≤ 1nt (s,a) ≤ nt (s,a) 1
4 tar tar ∨
Here the second inequality is due to that g1(x,δ) is non-increasing for x 1, and the third inequality is due
to g (x,δ) is non-decreasing and nt (s,a) x 4g (δ) 1. ≥
1 tar ≥ 3 ≥
Case 2 If g (δ)> 1nt (s,a), similarly we can get:
3 4 tar
g (nt(s,a),δ) g (δ) g (nt (s,a),δ)
1 1 1<4 3 4 1 tar
nt(s,a) ∧ ≤ nt (s,a) 1 ≤ nt (s,a) 1
tar ∨ tar ∨
where we utilize the fact that g (x,δ) g (δ) 1 always holds for x 0. Combine the two cases and we
1 3
get the results. Similarly, on FHybrid, b≥ecause w≥e also have: ≥
1
n˜t(s,a) nt (s,a) g (δ)
≥ 2 tar − 3
So the same claim holds for FHybrid. This ends the proof.
38Lemma10(ConcentrationforDiscreteDistributions,(Hsuetal.,2012)). Letz beadiscreterandomvariable
that takes values in 1, ,d , distributed according to p. We write p as a vector where q =[Pr(z =j)]d .
{ ··· } j=1
Assume we have N iid samples, and our empirical estimate of q is [qˆ] =(cid:80)N 1 z =j /N. We have for
j i=1 { i }
ε>0:
∀ Pr(cid:16) qˆ q √d(1/√N +ε)(cid:17) e−Nε2 .
1
|| − || ≥ ≤
Next,weintroducethedeviationinequalityforcategoricaldistributionsbyJonssonetal.(2020,Proposi-
tion1). NotethatthisresulthelpsfortheconcentrationoftheeventsF ,F –whichrequiresuniformcontrol
1 4
over all t N+,
∈
Lemma 11 (Jonsson et al. (2020, Proposition 1)). For all p Σ and for all δ [0,1],
m
∈ ∈
(cid:18) (cid:18) (cid:19)(cid:19)
1 n
P n N+,nKL(pˆ ,p)>log( )+(m 1)log e(1+ ) 1 δ.
∀ ∈ n δ − m 1 ≥ −
−
Lemma12(ConcentrationforBernoullisequences). Let n beafiltrationandX ,...,X beasequence
of Bernoulli random variables with P(X = 1 ) = P{F wi } iti= h1 P being -measu1 rable ann d X being -
i i−1 i i i−1 i i
| F F F
measurable. It holds that
(cid:32) n n (cid:33)
(cid:88) (cid:88) 1
P n, X > P /2 log( ) 1 δ
∀ t t − δ ≥ −
t=1 t=1
Proof. We first define the following sequence:
M
n
=e(cid:80)n t=1(−Xt+Pt/2)
which is a supermartingale because (here we define f(p)=ep/2(1 p+ p), p [0,1]):
− e ∈
E(M
n
n−1)=e(cid:80)n t=− 11(−Xt+Pt/2)E(ePn/2−Xn n−1)
|F |F
=e(cid:80)n t=− 11(−Xt+Pt/2)f(P n) (definition of P t)
e(cid:80)n t=− 11(−Xt+Pt/2)f(0) (f(p) is decreasing in [0,1])
≤
=M
n−1
Withthewell-knownVille’sinequality(Durrett,2019,Exercise4.8.2),foranynon-negativesupermartingale
M we have:
n
1
P(sup M > ) δ E(M ).
n∈N
+
n δ ≤ · 1
Since f(x)≜xe−1+x/2+(1 x)ex/2 is non-increasing in [0,1], we have
−
E(M )=P e−1+P1/2+(1 P )eP1/2 1.
1 1 1
− ≤
This implies that P(sup M < 1) 1 δ. Thus, we obtain:
n∈N + n δ ≥ −
(cid:32) n n (cid:33)
(cid:88) (cid:88) 1
P n, X > P /2 log( ) 1 δ
∀ t t − δ ≥ −
t=1 t=1
Lemma 13 (Bernstein type inequality (Domingues et al., 2021c)). Consider the sequences of random vari-
ables (w t) t∈N∗ and (Y t) t∈N∗ adapted to a filtration ( t) t∈N. Let
F
t t t
S ≜(cid:88) w Y , V ≜(cid:88) w2E(cid:2) Y2 (cid:3) , and W ≜(cid:88) w ,
t s s t s s |Fs−1 t s
s=1 s=1 s=1
and h(x)≜(x+1)log(x+1) x. Assume that, for all t 1,
− ≥
39• w is measurable,
t t−1
F
• E[Y ]=0,
t t−1
|F
• w [0,1] almost surely,
t
∈
• there exists b>0 such that Y b almost surely.
t
| |≤
Then, for all δ >0,
(cid:18) (cid:19)
bS
P( t 1,(V /b2+1)h | t | log(1/δ)+log(4e(2t+1))) δ.
∃ ≥ t V +b2 ≥ ≤
t
The previous inequality can be weakened to obtain a more explicit bound: with probability at least 1 δ, for
−
all t 1,
≥ (cid:112)
S 2V log(4e(2t+1)/δ)+3blog(4e(2t+1)/δ) .
t t
| |≤
Lemma 14 (Bernstein transportation (Talebi and Maillard, 2018)). Let p,q Σ , where Σ denotes the
S S
∈
probability simplex of dimension S 1. For all α > 0, for all functions f defined on with 0 f(k) b,
− S ≤ ≤
for all s , if KL(p,q) α then
∈S ≤
(cid:113) 2
pf qf 2Var (f)α+ bα,
| − |≤ q 3
where we use the expectation operator defined as pf ≜ E f(s) and the variance operator defined as
s∼p
Var (f)≜E (f(s) E f(s′))2 =p(f pf)2.
p s∼p s′∼p
− −
Next, we prove a standard probability fact for our problem setting.
Lemma 15 (Law of total variance). For any policy π and for all h [H],
∈
(cid:32)
H
(cid:33)2
H
E π (cid:88) r(s h,a h) −V 1π(s 1) =(cid:88)(cid:88) pπ h(s,a)Var p(cid:0) V hπ +1(cid:1) (s,a).
h=1 h=1 s,a
Proof. We intend to prove the following claim:
(cid:32)
H
(cid:33)2(cid:12)
(cid:12)

H
E π (cid:88) r(s h,a h) −V hπ(s h) (cid:12) (cid:12) (cid:12)s h= (cid:88) (cid:88) [pπ h(s h′,a h′)Var p(cid:0) V hπ ′+1(cid:1) (s h′,a h′) |s h].
h′=h (cid:12) h′=hs h′,a h′
We proceed by induction. For the base case, the claim trivially holds since Vπ (s) = 0. We now assume
H+1
the claim holds for step h+1, and for step h, we have:
(cid:32)
H
(cid:33)2(cid:12)
(cid:12)

E π (cid:88) r(s h,a h) −V hπ(s h) (cid:12) (cid:12) (cid:12)s h
h′=h (cid:12)
(cid:32)
H
(cid:33)2(cid:12)
(cid:12)

=E π (cid:88) r(s h′,a h′) −V hπ +1(s h+1)+V hπ +1(s h+1) −pV hπ +1(s h,a h) (cid:12) (cid:12) (cid:12)s h
h′=h+1 (cid:12)
=E π(cid:104)(cid:0) V hπ +1(s h+1) −pV hπ +1(s h)(cid:1)2(cid:12) (cid:12) (cid:12)s h(cid:105) +E π (cid:32) (cid:88)H r h′(s h′,a h′) −V hπ +1(s h+1)(cid:33)2(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)s h 
h′=h+1 (cid:12)
+2E
(cid:34)(cid:32) (cid:88)H
r (s ,a ) Vπ (s
)(cid:33)
(cid:0) Vπ (s ) pVπ (s
)(cid:1)(cid:12) (cid:12)
(cid:12)s
(cid:35)
π h′ h′ h′ − h+1 h+1 h+1 h+1 − h+1 h (cid:12) (cid:12) h
h′=h+1
40For the cross term, due to the definition of Vπ , we have
h+1
(cid:34) (cid:88)H (cid:12) (cid:12) (cid:35)
E r (s ,a ) Vπ (s )(cid:12)s =0
π h′ h′ h′ − h+1 h+1 (cid:12) (cid:12) h+1
h′=h+1
Therefore by the law of total expectation we know that the cross term equals zero. This shows that:
(cid:32)
H
(cid:33)2(cid:12)
(cid:12)

E π (cid:88) r h′(s h′,a h′) −V hπ(s h) (cid:12) (cid:12) (cid:12)s h
h′=h (cid:12)
=E π(cid:104)(cid:0) V hπ +1(s h+1) −pV hπ +1(s h)(cid:1)2(cid:12) (cid:12) (cid:12)s h(cid:105) +E π (cid:32) (cid:88)H r h′(s h′,a h′) −V hπ +1(s h+1)(cid:33)2(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)s h 
h′=h+1 (cid:12)
H
=(cid:88) [pπ(s ,a )Var (Vπ )(s ,a ) s ]+ (cid:88) (cid:88) [pπ(s ,a )Var (cid:0) Vπ (cid:1) (s ,a ) s ]
h h h p h+1 h h | h h h′ h′ p h′+1 h′ h′ | h
ah h′=h+1s h′,a h′
(induction hypothesis)
H
= (cid:88) (cid:88) [pπ(s ,a )Var (cid:0) Vπ (cid:1) (s ,a ) s ]
h h′ h′ p h′+1 h′ h′ | h
h′=hs h′,a h′
This ends the proof. Particularly, for h=1, we have:
(cid:32)
H
(cid:33)2
H
E π (cid:88) r(s h,a h) −V 1π(s 1) =(cid:88)(cid:88) pπ h(s,a)Var p(cid:0) V hπ +1(cid:1) (s,a).
h=1 h=1 s,a
Lemma 16 (Varianceboundforchangeofmeasure(Ménardetal.,2021)). Let p,q Σ and f is a function
S
∈
defined on S such that 0 f(s) b for all s S. If KL(p,q) α then
≤ ≤ ∈ ≤
Var (f) 2Var (f)+4b2α and Var (f) 2Var (f)+4b2α.
q p p q
≤ ≤
Lemma 17 (variance bound for change of function (Ménard et al., 2021)). For p,q Σ , for f,g two
S
∈
functions defined on such that 0 g(s),f(s) b for all s , we have that
S ≤ ≤ ∈S
Var (f) 2Var (g)+2bpf g
p p
≤ | − |
Var (f) Var (f)+3b2 p q
q p 1
≤ ∥ − ∥
where we denote the absolute operator by f (s)= f(s) for all s .
| | | | ∈S
Lemma 18 (SummationtoIntegration(Ménard et al., 2021, Lemma 9)). Let a be a sequence taking values
t
in [0,1] and A
≜(cid:80)T
a then
T t=0 t
T
(cid:88) a t+1 4log(A +1).
A 1 ≤ T+1
T
t=0 ∨
Lemma19(Transcendentalinequalitytopolynomialinequality). LetA,B,C,D,E,andαbepositivescalars
such that 1 B E and α e. If τ 1 satisfies
≤ ≤ ≥ ≥
(cid:112)
τ C τ(Alog(ατ)+B(log(ατ))2)+D(Alog(ατ)+E(log(ατ))2), (33)
≤
then
(cid:16) (cid:17)
τ C2(AF +BF2)+ D+2C√D (AF +EF2)+1,
≤
where
F =4log(2α(A+E)(C+D)).
41Proof. Intuitively, the leading order of τ should be O˜(C2) since log(x) xk for any k > 0 and x 1. To
obtain this result, we first get a loose bound of τ to upper bound log(τ≤ ). Bk y doing this, we can tr≥ansform
theoriginalinequalityintoapolynomialinequality, whichiseasiertosolve. Followingthisidea, bychoosing
k =1/2,1/4,3/4,3/8, we obtain that:
(Alog(ατ)+B(log(ατ))2) 2A(ατ)21 +16B(ατ)1
2
≤
(2A+16E)(ατ)21
≤
4 64
(Alog(ατ)+E(log(ατ))2) A(ατ)3 4 + E(ατ)3 4
≤ 3 9
(2A+16E)(ατ)43 (34)
≤
This leads to:
τ C(cid:112) (2A+16E)α1 4τ3 4 +D(2A+16E)(ατ)43
≤
(C+D)(2A+16E)(ατ)3
4
≤
This yields that:
ατ ((C+D)(2A+16E))4α4
≤
(2α(C+D)(A+B))4 (35)
≤
Here we define F ≜4log(2α(A+B)(C+D)) for brevity, and substitute back to equation 33, we get:
(cid:112)
τ C (AF +BF2)τ +D(AF +EF2)
≤
Solving this, we get:
(cid:112) (cid:112)
C (AF +BF2)+ C2(AF +BF2)+4D(AF +EF2)
√τ
≤ 2
(cid:112) (cid:112)
C AF +BF2+√D AF +EF2 (√x+y √x+√y)
≤ ≤
Finally, this gives us the result:
τ C2(AF +BF2)+(D+2C√D)(AF +EF2)+1 (36)
≤
D Experiment Setup
We compare our algorithm with the state-of-the-art pure online RL algorithm BPI-UCBVI in Ménard et al.
(2021) on GridWorld environment (S = 16,A = 4,H = 20). The goal is to navigate in a room to collect
rewards. In the source and the target environments, the same structure includes:
• state-action space: the state space is a 4 4 room, and the action space is to go up/down/left/right.
×
• horizon: each episode has a horizon length 20.
• success probability, the agent may fail in taking an action and go to the wrong direction with uniform
probalibities. The success probability is set to be 0.95 in experiment 1.
• reward: r =1 at state (1,4), r =0.1 at state (2,3), r =0.01 at state (3,2), and r =1.5 at state (3,4).
The state (1,4) is an absorbing state, where the agent cannot escape once steps in and the reward can
only be obtained once.
• initial state: the agent starts from state (3,2) in each episode.
42Compared with the source environment, the target environment includes additional "traps" (absorbing
states), at states (2,2), (2,4) and (3,3), where the agent cannot escape once steps in. For experiment 1
and 2, the source dataset is collected by running Algorithm 2 in the source environment for T = 1 105
episodes, which satisfies the condition in Theorem 2. For both algorithms, ε=0.1, δ =0.1. We re-sca×le the
exploration bonus in BPI-UCBVI and Algorithm 3 with the same constant 2 10−3 to mitigate the effect
of the large hidden constant within O(cid:101)() (similarlily for Algorithm 2 with 1 1× 0−6). The optimality gap of
a policy in the target environment is e·valuated by running the policy for 10×0 episodes and calculating the
average the results.
Forexperiment1, werunbothalgorithmsinthetargetenvironmentforT =2 105 episodestoexamine
the relationship between optimality gaps and the sample size from the target envi×ronment. We set β =0.45
and σ =0.25 for Algorithm 1, satisfying Definition 1 and Assumption 1.
For experiment 2, we vary the success probability of taking an action in the target environment (not
accountedintheimplementationofAlgorithm1)toexaminetheeffectofmaximumunidentifiedshiftdegree.
The real success probability is set from 0.9 to 0.55 with a step size 0.05. Because β is still set to be 0.45 for
Algorithm 1, the maximum unidentified shift degree (real β) ranges from 0.05 to 0.4 with a step size 0.05.
For each success probability, we run the algorithms for 5 runs, each run contains T =1 105 episodes.
×
43