Weighted Sobolev Approximation Rates for Neural Networks
on Unbounded Domains
Ahmed Abdeljawad∗ ahmed.abdeljawad@oeaw.ac.at
Johann Radon Institute of Computational and Applied Mathematics (RICAM)
Austrian Academy of Sciences
Altenberger Straße 69, A-4040 Linz, Austria
Thomas Dittrich∗ thomas.dittrich@oeaw.ac.at
Johann Radon Institute of Computational and Applied Mathematics (RICAM)
Austrian Academy of Sciences
Altenberger Straße 69, A-4040 Linz, Austria
Abstract
In this work, we consider the approximation capabilities of shallow neural networks in
weightedSobolev spacesfor functions in the spectralBarronspace. The existing literature
alreadycoversseveralcases,in which the spectralBarronspace canbe approximatedwell,
i.e., without curse of dimensionality, by shallow networks and several different classes of
activationfunction. Thelimitationsoftheexistingresultsaremostlyontheerrormeasures
that were considered, in which the results are restricted to Sobolev spaces over a bounded
domain. We will here treat two cases that extend upon the existing results. Namely, we
treat the case with bounded domain and Muckenhoupt weights and the case, where the
domainisallowedtobe unboundedandthe weightsarerequiredtodecay. We firstpresent
embedding results for the more general weighted Fourier-Lebesgue spaces in the weighted
Sobolev spaces and then we establish asymptotic approximation rates for shallow neural
networks that come without curse of dimensionality.
Keywords: ApproximationRate, Neural Network,BarronSpace, Curse of Dimensional-
ity
1 Introduction
Over the last decade and a half, deep neural networks have enabled big breakthroughs
in various fields of machine learning. The focus of the present work will be on scientific
computing, where deep neural networks have contributed vastly to computational methods
for solving partial differential equations with methods such as deep learning backwards
stochastic differential equations (Han et al., 2018; E et al., 2017), physics informed neural
networks (Raissi et al., 2019), deep learning variational Monte Carlo (Hermann et al., 2023;
Gerard et al.,2024),andoperatorlearning(Anandkumar et al.,2019;Chen and Chen,1995;
Lu et al., 2021; Kovachki et al., 2024).
A central question in scientific computing is, which classes of functions can be ap-
proximated well by conventional numerical methods, or more recently by neural networks
(Bartolucci et al., 2023; DeVore et al., 2021). Conventional numerical methods like finite
element and finite difference approaches have been the go-to solutions for approximating
∗. Both authors contributed equally
1
4202
voN
6
]GL.sc[
1v80140.1142:viXrasolutions to partial differential equations in low dimensions. However, as the dimension-
ality increases, these traditional methods face prohibitive computational costs due to the
curse of dimensionality: achieving a desired accuracy ǫ requires computational resources
that grow exponentially with the problem’s dimension. The curse of dimensionality also
fundamentally impacts the numerical approximation of high-dimensional functions, not just
PDEs. Standard approaches such as polynomial approximations and splines suffer from
similar scaling issues, which severely restricts their applicability to high-dimensional prob-
lems. In contrast, neural networks provide a powerful alternative to overcome the curse of
dimensionality. Albeit not providing the best possible expressivity and approximation rates
in comparison to deep neural networks (Poole et al., 2016; Poggio et al., 2017), shallow neu-
ral networks are very interesting in this field from a theoretical perspective (Bach, 2017).
This is mostly due to their ’easy’ fit to existing theory such as sampling in Banach spaces
(Pisier, 1980/1981; Barron, 1993), approximation with dictionaries and variation spaces
(Kurkova and Sanguineti, Sept./2001; Siegel and Xu, 2023), and the analysis of ridge func-
tions via the Radon transform (Ongie et al., 2020; Parhi and Nowak, 2021; Unser, 2023).
A seminal work in this direction was thework of Barron onapproximation rates forshal-
low neural network (Barron, 1993). The class of functions that we consider in the present
work isnowadays known as thespectral Barronspace (Siegel and Xu,2022a)ortheFourier-
Analytic Barron space (Voigtlaender, 2022) and it can be seen as a polynomially weighted
Fourier-Lebesgue space with integrability exponent p = 1 (Abdeljawad and Dittrich, 2023).
The current literature on approximation theory with shallow networks mainly focuses on
cases,wheretheerrorismeasuredintermsofSobolevnormswith2 p < (Siegel and Xu,
≤ ∞
2020, 2022b) and Lebesgue spaces with p = (Ma et al., 2022). However, an important
∞
class of error measures that can be considered for Finite Element Simulations is the class of
weightedSobolevnorms(see(Agnelli et al.,2014;Nochetto et al.,2016;Heltai and Rotundo,
2019; Allendes et al., 2024)).
A trivial extension of the existing approximation theory for Barron spaces towards the
errorbeingmeasuredinweightedSobolevspacescanbeobtainedforthecasethattheweight
is bounded and the error is measured over a bounded domain. Then, one can bound the
weighted error measure by the unweighted measure using Hölder’s inequality
f f ω f f (1)
k −
N kWℓ,p(ω;U)
≤ k
kL∞(U)k
−
N kWℓ,p(U)
and apply the existing theory for the unweighted case. However, this excludes two major
advantages of weighted Sobolev spaces. Namely, unbounded weights and unbounded do-
mains. In the case of unbounded weights, the supremum of ω is infinite and, therefore, the
right side of (1) is infinite unless the approximation problem is trivial in the sense that f
can be approximated without error at a finite number of neurons.
A class of weights that is interesting for approximation on a bounded domain is the
so-called class of Muckenhoupt weights. The interest in this class initially came from the
fact that the Hardy-Littlewood maximal operators, as well as a broad range of Integral
operators are bounded on Muckenhoupt-weighted spaces (Grafakos, 2014) and that they
allow forsignificant generalizations ofsomeFourier inequalities suchastheHausdorff-Young
inequality (Heinig and Sinnamon, 1989). From a more application-oriented perspective,
Muckenhoupt-weighted spaces areof interest forfinite element methods where they are used
in form of weighted Sobolev spaces to study problems with singular sources (Agnelli et al.,
22014; Nochetto et al., 2016; Heltai and Rotundo, 2019; Allendes et al., 2024). Despite all
thisinterestinweighted Sobolevspaces,tothebestofourknowledge, theyhavenotyetbeen
studied in the context of neural networks and with a focus on the curse of dimensionality.
The case of unbounded domain is interesting insofar, as applications such as the ap-
proximation of the ground state of the electronic Schrödinger equation requires exactly this
setting (Gerard et al., 2024; Dusson et al., 2024) while the existing literature for neural net-
works solely provides universal approximation theorems (Wang and Qu, 2019; van Nuland,
2024) but no information about the approximation rate. In this context, (Wang and Qu,
2019) provides a universal approximation theorem for functions in Lp(R [0,1]d). They
×
also show that shallow networks with sigmoidal, ReLU, ELU, softplus, or LeakyReLU ac-
tivation cannot universally express non-zero functions in Lp(Rm [0,1]d) for m > 1. For
×
functions that asymptotically decay to zero, (van Nuland, 2024) generalizes the universal
approximation theorem to uniform convergence on Rd.
Note that weighted error measures are not tobeconfused with weighted variation spaces
(DeVore et al., 2025). In this type of spaces, the weight is applied to the dictionary of
functions with the aim of extending the space of target functions that can be approximated
withoutcurseofdimensionality. Contrary tothat,weighted errormeasuresaimatextending
the settings in which functions can be approximated well.
1.1 Contributions and Discussion
In this work, we will address both of the previously mentioned unbounded cases. To do
so, we will first present the necessary embedding results which show that weighted Barron
spaces are embedded in weighted Sobolev spaces in order to establish that approximation of
Barron functions is well defined. Second, we will then use Maurey’s sampling argument and
the theory of variation spaces in order to derive approximation rates in weighted Sobolev
spaces.
Forboundeddomain,werestricttheweightoftheSobolevspaceWℓ,p(ω; )tobeω(x)=
υ(x)− p1 ′ with υ being in the Muckenhoupt class A p′(Rd) with υ(x) 1/ xU −γp′ for some
γ Rd. Consequently,foraproperchoiceofγ wecanallowυ(x) =
x≥α,h whi| ch|i
isυ A p′(Rd)
∈ | | ∈
forα ( d,d(p′ 1))(seeSection2). Withthat,theweightωisallowedtohavesingularities
∈ − −
for negative α, as well as zeros for positive α.
The full embedding result for weighted Fourier-Lebesgue spaces in weighted Sobolev
spacescanbefoundinTheorem23,wesummarizethisresultinTheorem1(andTheorem26)
for the special case of Barron spaces.
Corollary 1. Let d,ℓ N, γ R with γ > d/2, p [2, ], q = 2(p/2)′, Rd have finite
∈ ∈ ∈ ∞ U ⊂
volume, and
ω(x) = υ(x)− p1 ′,
where υ is a radial non-decreasing function such that υ A p′(Rd) with υ(x) 1/ x −γp′ .
∈ ≥ h | |i
Then for any f ( ),
γ+ℓ
∈B U
kf kWℓ,p(ω;U)
≤
C d,ℓ kχ U kFLq γ(Rd)kf kBγ+ℓ(U),
where the constant C only depends on the number of dimensions d and the order ℓ of the
d,ℓ
Sobolev norm.
3A crucial requirement for this embedding result is χ FLq (Rd). As an example, in
U γ
∈
the unweighted one-dimensional setting with connected domain (i.e., γ = 0 and being
U
an interval) the Fourier-Transform of the characteristic function is the sinc function, which
is not L1-integrable. Therefore, for d = 1, it is essential that q > 1 and consequently
that p < . However, the proof of the approximation result Theorem 3 relies on Maurey’s
∞
sampling argument, which is validforBanach spaces of Rademacher-Type 2, which excludes
p = anyway.
∞
For unweighted and high dimensional settings, the work (Ko and Lee, 2016; Lebedev,
2013) provides conditions on the Fourier-Lebesgue integrability of characteristic functions,
which mostly depend on the smoothness on the boundary of the domain, with cubes be-
ing among the most well-behaved domains. For a discussion on these works, we refer the
interested reader to (Abdeljawad and Dittrich, 2023, Section 3.1.1).
For weighted settings, obviously, choosing a large polynomial weight (i.e., γ 0) pre-
≫
vents certain domains from being feasible. For a short discussion of this effect, we now
assume that is valid in the unweighted setting, that is, χ
FLp0,
and we want to
find a p
suchU
that χ
FLp1
for γ > 0. We further
assumeU t∈
hat χ
0
decays (fractional)
1 U
∈
γ1 1 U
polynomially which leads to χ
U
. h·i− pd 0−ε for some ε > 0 in order for χ
U
∈
FLp 00. We get
c
kχ U kp F1 Lp γ1
1
= c Rd( hξ iγ1χ U(ξ))p1dξ .
Rd
hξ iγ1− pd 0−ε p1 dξ,
Z Z (cid:16) (cid:17)
which is integrable, if p (γ d ) cd. We drop ε here as it can be chosen arbitrarily
1 1 − p0 ≤ −
small with the sole purpose of turning the inequality strict. The conclusion is that we can
at most choose γ < d , so that the left side is negative, and consequently have to choose
1 p0
p 1 .
1 ≥ 1 −γ1
p0 d
Over an unbounded domain, the embedding is straightforward and solely requires that
the weight decays sufficiently fast. We get the following result for the Barron space as a
special case of Theorem 31:
Lemma 2 (Embedding of Barron Space). Let d,ℓ N, u 0, p R such that up > d,
∈ ≥ ∈ +
then
kf kWℓ,p(h·i−u;Rd) . kf kBℓ(Rd)
for all f (Rd).
ℓ
∈ B
Subsequently, wecanderivetheapproximation resultsforbothcases. First,forfunctions
in the Barron space measured in the weighted Sobolev space over bounded domain the
approximation result is stated in Theorem 29 as follows:
Theorem 3 (Approximation in weighted Sobolev Space). Let d,ℓ N, γ 0 with γ > d/2,
p [2, ), q = 2(p/2)′, Rd such that χ FLq (Rd), and ∈ ≥
U γ
∈ ∞ U ⊂ ∈
ω(x) = υ(x)− p1 ′,
where υ is a radial non-decreasing function such that υ A p′(Rd) with υ(x) 1/ x −γp′ .
Further, let f (Rd) and let ̺ Wm,∞( s;R) b∈ e an activation funct≥ ioh n fo| r| si > 1.
γ+ℓ+1
∈ B ∈ h·i
4Then,
fNin ∈f Σ̺kf −f N kWℓ,p(ω;U) .
N−1
2 kω kLp(U)kf kBγ+ℓ+1(Rd)
where the implied constant only depends on the parameters of the setting but not on the
function itself.
The restrictions for this theorem come mostly from the conditions for the embedding
result. Additionally, we have to restrict to Barron functions of one order higher in order to
obtain a finite variation norm in the given dictionary.
Finally,theapproximationresultoverunboundeddomainfromTheorem33canbestated
as follows:
Theorem 4. Let d,ℓ,N,m N and p,r,u,v R such that 2 p < , 1 < r v, and
(u r)p > d. Furthermore,
∈
let ̺ Wℓ,∞(
v;∈
R) be an
activat≤
ion
func∞
tion and
D≤
be the
̺
− ∈ h·i
corresponding dictionary over Rd. For every target function f (Rd) we get
ℓ+r
∈ B
fN∈i Σn Nf
(D
̺)kf −f N kWℓ,p(h·i−u;Rd) .
N−1
2 kf kBℓ+r(Rd).
Note, that here we require that r > 1 and f (Rd). If we were to restrict to
ℓ+r
∈ B
functions f with supp f for some bounded Rd, we would obtain a special case
{ } ⊆ U U ⊂
of Theorem 3 with γ = 0. However, using directly the result for bounded domains would
only require the less restrictive assumption f (Rd). This difference in the asking for
ℓ+1
∈ B
a strict inequality comes from the fact, that over unbounded domain, we have to ensure
integrability of the polynomial bound on the activation function.
2 Preliminaries
In this section, we cover the mathematical preliminaries, that will be necessary throughout
our main contributions.
We let F and F−1 be the Fourier transform and the inverse Fourier transform, respec-
tively. For f L1 the pointwise definition of the Fourier transform is given by
∈
1
F f (ξ) = fˆ(ξ)= f(x)e−ihx,ξidx
{ } (2π)d 2 ZRd
and for fˆ L1, the pointwise definition of the inverse Fourier transform is given by
∈
1
F−1 f (x) = fˆ(ξ)eihx,ξidξ,
{ } (2π)d 2 ZRd
where , denotes the usual scalar product on Rd.
h· ··i
52.1 Weighted Function Spaces: Definition and Properties
The central elements of our work are weighted Sobolev spaces and Fourier-Lebesgue- (re-
spectively Barron-) spaces as classes of target functions. We will here continue by first
introducing the concept of weight functions and the special class of Muckenhoupt weights.
Secondly,weintroduceweightedSobolevspaces,weightedFourier-Lebesguespacesandspec-
tral Barron spaces. Finally, we present generalizations of the Housdorff-Young inequality
and Young’s convolution inequality.
Definition 5 (Muckenhoupt Weights (Heinig and Sinnamon, 1989)). Let d N. A non-
∈
negative and Lebesgue-measurable function ω : Rd R is called a Muckenhoupt Weight of
→
class A (Rd) for p (1, ), if it is locally integrable and there is a constant C > 0 such that
p
∈ ∞
for all d-balls Rd with volume it holds that
B ⊂ |B|
1 1
1
ω(x)dx
p ω(x)1−p′
dx
p′
C.
≤
|B|(cid:18)ZB (cid:19) (cid:18)ZB (cid:19)
If ω is solely nonnegative and Lebesgue-measurable, we refer to it as a weight function.
A special example of Muckenhoupt weights of the class A is given by ω : Rd R
p
defined by ω(x) = x α, where α ( d,d(p 1)) see, e.g., (Grafakos, 2014, Example 7.→ 1.7).
| | ∈ − −
Definition 6 (Weighted Function Spaces). Let p [1, ], d N, m Z , Rd, and
∈ ∞ ∈ ∈ + U ⊆
ω be a weight function on Rd. We define the weighted Lebesgue space, the weighted Sobolev
space, and the weighted Fourier-Lebesgue space as
Lp(ω; ) := f : R ωf Lp( ) ,
U { U → | ∈ U }
Wm,p(ω; ) := f : R α Zd with α mit holds∂αf Lp(ω; ) ,
U { U → |∀ ∈ + | |1 ≤ ∈ U }
and
FLp(ω; ) := f : R f L1(Rd)withf = fsuch thatfˆ Lp(ω; ) ,
e e U e
U { U → |∃ ∈ | ∈ U }
respectively. These spaces are normed spaces equipped with the norms
1
p
f := ω(x)f(x) pdx ,
k
kLp(ω;U)
| |
(cid:18)ZU (cid:19)
1
f := ω∂αf p p ,
k kWm,p(ω;U) |α|≤mk kLp(U)
(cid:18) (cid:19)
X
and
f := inf ωfˆ ,
k
kFLp(ω;U)
fe∈L1
e
Lp(Rd)
fe|U=f(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:13)
respectively. For p = , we make the obvious modifications for Lp(Rd) and replace the
∞
summation in the weighted Sobolev norm by a maximum.
6Note that in the case = Rd, the infimum in the Fourier-Lebesgue norm is over a single
U
element (of equivalence classes), which therefore results in the implicitly requirement that
f L1(Rd).
∈
Definition 7 (Spectral Barron Space). Let d N and Rd. In the special case of
∈ U ⊆
(fractional) polynomial weights of order s R together with p = 1, the weighted Fourier-
∈ 1
Lebesgue space is referred to as the so-called spectral Barron space. We denote this by
( ) := FL1( s; ).
s
B U h·i U
Remark 8 (Simplified Notation). Throughout the current work, we will consider the fol-
lowing simplifications in the notation:
• In case that = Rd, we drop the domain from the notation whenever the dimension
U
is clear from the context, i.e., FLp(ω) := FLp(ω;Rd).
• In the case of (fractional) polynomial weights of order s R, we reduce the notation
∈
for the weights in the Fourier-Lebesgue spaces such that we only denote the polynomial
degree, i.e., FLp ( ) := FLp( s; ).
s
U h·i U
• For the unweighted Fourier-Lebesgue spaces (i.e., polynomially weighted with s = 0)
we skip the weight all together, i.e., FLp( ) := FLp ( ).
U 0 U
A simple embedding result between Fourier-Lebesgue spaces and the Barron space can
be obtained similarly as in the proof of (Abdeljawad and Dittrich, 2023, Theorem 3.9) via
Jensen’s inequality.
Proposition 9 (Higher-Order Embedding (Abdeljawad and Dittrich, 2023)). Let d N,
∈
κ R, t 1, and σ = (d+1)(1 1), then
∈ ≥ − t
1
f f
k kBκ ≤ 1−1 k kFLt κ+σ
−(d+1) t
h·i L1(Rd)
(cid:13) (cid:13)
(cid:13) (cid:13)
In our result we require general(cid:13)izations o(cid:13)f the Hausdorff-Young inequalities between
weighted Fourier-Lebesgue norms and weighted Lebesgue norms. Namely, those generaliza-
tions are (Heinig and Sinnamon, 1989, Theorem 2.9 and Theorem 2.10), which strongly rely
on Muckenhoupt weights. In order to have a self-contained work we provide these results
here:
Lemma 10 (Generalized Hausdorff-Young Inequality: Type I (Heinig and Sinnamon, 1989,
Theorem 2.9)). Let d N, 1 < p q p′, and suppose υ is a radial function and radially
∈ ≤ ≤
non-decreasing, such that
ω(x) =
υ(x)p1
and ϑ(x) = x
d (cid:16)p1 ′− q1
(cid:17)ω
1
, (2)
| | x
(cid:18)| |(cid:19)
1. The existing literature sometimes differentiates between the polynomially weighted spectral Bar-
ron spaces and exponentially weighted spectral Barron spaces (see, e.g., (Siegel and Xu, 2022a;
Abdeljawad and Dittrich,2024)). Inthepresentwork,however,inthepresentworkweareonlydealing
with the polynomially weighted case and will therefore skip this prefixthroughout the work.
7then there is a constant C > 0 such that
f C f
k
kFLq(ϑ)
≤ k
kLp(ω)
if and only if υ A (Rd).
p
∈
Lemma11(GeneralizedHausdorff-YoungInequality: TypeII(Heinig and Sinnamon,1989,
Theorem 2.10)). Let d N, 1 < p′ q p, and suppose υ is a radial function and radially
∈ ≤ ≤
non-decreasing, such that
ω(x)= υ(x)− p1 ′ and ϑ(x) = x d (cid:16)p1 ′−1 q(cid:17)ω 1 , (3)
| | x
(cid:18)| |(cid:19)
then there is a constant C > 0 such that
f C f
k
kFLp(ω)
≤ k
kLq(ϑ)
if and only if υ A p′(Rd).
∈
Finally, we require the following result from (Toft et al., 2015) to obtain estimates for
convolution and multiplication in weighted Lebesgue and Fourier-Lebesgue spaces.
Assumption 12 (Toft-Young Functional). Let t R,τ [1, ],j = 0,1,2, and let
j j
∈ ∈ ∞
R(τ) = 2 1 1 1 . Assume that 0 R(τ) 1/2, and that
− τ0 − τ1 − τ2 ≤ ≤
0 t +t , j,k = 0,1,2, j = k, (4)
j k
≤ 6
0 t +t +t dR(τ), (5)
0 1 2
≤ −
hold true with strict inequality in (5) when R(τ) > 0 and t =dR(τ) for some j = 0,1,2.
j
Proposition 13 (Generalized Young Convolution Inequality (Toft et al., 2015, Theorem
2.2(1))). Let t ,t ,t R and τ ,τ ,τ [1, ] fulfill Assumption 12. Then the map
0 1 2 0 1 2
∈ ∈ ∞
(f ,f ) f f on C∞ Rd extends uniquely toa continuous mapfrom Lτ1 Rd Lτ2 Rd
to1 Lq2 7→ Rd1 ∗ wi2
th q
=0
τ′.
t1 × t2
−t0 0(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
(cid:0) (cid:1)
2.2 Smoothing by Convolution
One key element in the proof of Theorem 23 is to show the embedding result first for
functions in the Schwartz-Space of rapidly decaying functions S and then use the fact that
S is dense in for every s R in order to extend the embedding to the full spectral
s
B ∈
Barron space. In this section we will now present the standard technique of utilizing the
convolution with a mollifier function in order to represent a (possibly non-smooth) function
as a limit of functions in S.
In this regard, the first important concepts are the concepts of Mollifiers and smoothing
sequences (cf. (Tartar, 2007)):
Definition14. Afunctionφ S iscalled aMollifierifφ(x) = 0for x 1and φ = 1.
∈ | |≥ k
kL1
For ε > 0 the sequence
1 x
̺ (x) := φ
ε εd ε
(cid:16) (cid:17)
is called a smoothing sequence.
8Note that ̺ = 1 for all ε > 0.
k
ε kL1
Wewillapplythistypeofsmoothingsequencestocharacteristic functionsforthedomain
Ω Rd, for which we introduce the notation
⊂
χε := χ ̺ = χ ( τ)̺ (τ)dτ, (6)
Ω Ωε ∗ ε Rd Ωε ·− ε
Z
where
Ω := x Rd y Ωsuch that x y ε
ε
{ ∈ |∃ ∈ | − | ≤ }
is an extension of Ω by a margin of width ε.
In terms of the weighted Fourier-Lebesgue norm, by using Hölder’s inequality similar to
(Abdeljawad and Dittrich, 2023, Section 2.3), this now results in the following bound
Proposition 15. Let d N, p [1, ], and ω be a weight function on Rd. Then, for all
∈ ∈ ∞
measurable Ω Rd with finite volume and ε> 0 it holds that
⊆
χε = ω̺ χ ̺ ωχ ̺ ωχ = χ .
k
ΩkFLp(ω)
k
ε Ω kLp
≤ k
ε kL∞
k
Ω kLp
≤ k
ε kL1
k
Ω kLp
k
Ω kFLp(ω)
Regarding convergence in Lp, we get the following result:
b c b c c
Proposition 16 (Convergence in Lp). Let d N, p [1, ), Rd be bounded, and
∈ ∈ ∞ U ⊂
h L∞. Then,
∈
lim χεh = χ h .
ε→0k U kLp k U kLp
Proof The result follows immediately from Hölder’s inequality and (Tartar, 2007, Lemma
3.2) as
(χε χ )h χε χ h 0
k U − U kLp ≤ k U − U kLp k kL∞ −ε−→−→0
By combining smoothing sequences and truncation sequences (cf. (Tartar, 2007)) the
following density result holds:
Proposition 17. Let d N, s R, and Rd measurable. It holds
∈ ∈ U ⊆
S(Rd) ֒ FLp( )
→ s U
dense with the injection map ι :S(Rd) FLp ( ), ι(f) := f .
s U
→ U |
2.3 Maurey Approximation
In our work, we explore the approximation properties of shallow neural networks from the
prospective of nonlinear dictionary approximation. The dictionary D is often assumed to be
an arbitrary subset of a Banach space (see e.g., (DeVore, 1998)). A well known example
X
9for a dictionary with d-dimensional input is the set of ridge functions (see (Gordon et al.,
2001)), i.e., composition of non-linear univariate real valued function with a linear function
Dridge := h( w, ): w Rd,h L1 (R) .
d { h ·i ∈ ∈ loc }
Byrestricting hfurthertobeasingle(non-linear) function ̺ L1 andtranslations thereof,
∈ loc
we obtain the dictionary of shallow networks with activation function ̺. For a given activa-
tion function ̺: R R this dictionary is defined as
→
D := ̺( w, +b) w Rd, b R .
̺,d
h ·i ∈ ∈
n (cid:12) o
(cid:12)
Throughout the remainder of this work, we will(cid:12)omit the subscript d as the dimensionality
of the input space will be clear from the context.
Based on the concept of dictionaries we can now define an N-term approximation class:
Definition 18 (ApproximationClass). Let be a Banach space and D be a dictionary.
X ⊂ X
The corresponding N-term approximation class with bounded weights is
N N
Σ (D) = a g a R, g D, s.t. a M
N,M n n n n n
( (cid:12) ∈ ∈ | |≤ )
n X=1 (cid:12) n X=1
(cid:12)
(cid:12)
and the N-term approximation class wi(cid:12)th unbounded weights is
Σ (D) = Σ (D).
N N,M
M≥0
[
This indeed corresponds to the set of all shallow networks of width N and activation
function ̺ for D .
̺
By taking the union over all possible (finite) widths and taking the closure of that set,
we obtain the closure of the convex hull of the dictionary, i.e.,
conv D = Σ (D).
N,1
{± }
N∈N
[
This leads to a natural function-space for shallow neural networks of infinite width, namely,
the so-called variation space.
Definition 19 (Variation Space). Let be a Banach space, D be a dictionary and
X ⊂ X
conv D be the closure of the convex hull of the dictionary w.r.t. . Then, the variation
{± } X
space corresponding to the dictionary D is given by
(D) = f s.t. f <
K1
{ ∈ X k
kK1(D)
∞}
with the variation norm
f = inf t > 0f/t conv D .
k
kK1(D)
{ | ∈ {± }}
10For functions in the variation spaceof adictionary, a classical approximation result from
Maurey (see (Pisier, 1980/1981; Siegel and Xu, 2022b)) shows that they can be approxi-
mated well in type-2 Banach spaces. Before stating said approximation result, we recall the
definition of type-2 Banach spaces; further details and extensions regarding this theory can
be found e.g., in (Ledoux and Talagrand, 1991; Johnson and Lindenstrauss, 2001).
Definition 20 (Type-2 Banach Space). A Banach space is a type-2 Banach space if there
X
exists a positive constant C such that for any N 1 and f N we have
X ≥ { i }i=1 ⊂ X
2 1/2 1/2
N N
E ε f C f 2
 (cid:13) i i (cid:13)  ≤ X k i kX !
(cid:13)Xi=1 (cid:13)X Xi=1
(cid:13) (cid:13)
 (cid:13) (cid:13) 
where the expectation is tak(cid:13)en over (cid:13)independent Rademacher random variables ε ,...,ε ,
1 n
i.e.,
1
P(ε = 1) = P(ε = 1) = .
i i
− 2
The constant C is called the type-2 constant of the space .
X
X
Mostimportantlyforus,itisknownthatthespacesLp(µ)overameasurespace( , ,µ)
U A
are type-2 Banach spaces. A proof for this can be found in (Siegel and Xu, 2022b, Section
2), which can immediately be extended to weighted Sobolev spaces by replacing Lp(µ) with
Wℓ,p(ω; ).
U
Proposition21 (WeightedSobolevSpacesareofType-2). Let ℓ Z , 2 p < , Rd,
∈ + ≤ ∞ U ⊆
and ω be a weight function, then Wℓ,p(ω; ) is a type-2 Banach space.
U
Proof The proof is analogous to (Siegel and Xu, 2022a, Section 2) by replacing Lp(µ) with
Wℓ,p(ω; ).
U
Finally, theapproximation result(intheformulation of(Siegel and Xu,2022a))forfunc-
tions inthevariation spaceofadictionary with theerror beingmeasuredinatype-2 Banach
space is as follows:
Proposition 22 (Approximation Rate in Type-2 Banach Spaces (Siegel and Xu, 2022b,
(1.17))). Let
X
beatype-2BanachspaceandD
⊂ X
beadictionarywithKD := sup
d∈D
kd
kX
<
. Then for f (D), we have
∞ ∈ K
fN∈Σi Nn ,f Mf(D)kf −f
N kX ≤
4C 2,XK
D
kf kK(D)N− 21
with M = f .
f
k
kK(D)
3 Embedding Results for Fourier-Lebesgue Spaces
In this section we provide embedding results for Fourier-Lebesgue spaces in Sobolev spaces
for the case where the domain has finite volume but the weights are allowed to have singu-
larities. We have to split this analysis into two main parts:
11(i) Sobolev spaces with p [2, );
∈ ∞
(ii) Sobolev spaces with p [1,2).
∈
The reason for this split lies in the structure of the proof. In both cases, we need to
apply some variation of the Hausdorff-Young inequality, which requires that the lower part
of the inequality has an integrability exponent 2. For (i) we can therefore start with
≥
the Hausdorff-Young inequality and then continue with a generalized variant of Young’s
convolutioninequalitytomakethedependenceontheFourier-Lebesguespaceexplicitonthe
right side. For (ii) we first transition to high integrability exponents by means of Hölder’s
inequality and then transition to the Fourier-Lebesgue norm by means of the Hausdorff-
Young inequality.
3.1 Embedding in Sobolev Spaces with p [2, )
∈ ∞
Theorem 23 (General Embedding Result for p [2, )). Let d,ℓ N, γ,p,q R such
∈ ∞ ∈ ∈
that 1 < p′ q p and γ δ := d(1/p′ 1/q), Rd have finite volume, and
≤ ≤ ≥ − − − U ⊂
ω(x) = υ(x)− p1 ′
where υ is a radial non-decreasing function such that υ A p′(Rd) with υ(x) 1/ x −γp′ .
∈ ≥ h | |i
Furthermore, set τ := q′ and t := γ δ, and for j 1,2 let t R and τ [1, ] such
0 0 j j
− − ∈ { } ∈ ∈ ∞
that t ,t ,t (as degree of the polynomial weights) and τ ,τ ,τ (as integrability exponents)
0 1 2 0 1 2
fulfill Assumption 12.
Then, there exists a constant C which may only depend on d and ℓ such that
d,ℓ
kf kWℓ,p(ω;U)
≤
C d,ℓ kχ U kFLτ t11 kf kFLτ t22
+ℓ
(7)
for every f FLτ2 .
∈ t2+ℓ
The restriction on are more relaxed in the setting of Theorem 23 compared to the
U
setting of Theorem 29, in the sense that it is enough for Theorem 23 that is a measurable
U
set with finite volume, whereas the main Theorem 29 requires bounded measurable sets.
Proof The result is trivially true if the right-hand side of (7) is infinite, which is the case
if χ / FLτ1 and/or f / FLτ2 . In order to cover the non-trivial cases we start with
U ∈ t1 ∈ t2+ℓ
the assumptions that χ FLτ1 and that f is a Schwartz function, i.e., f S(Rd). As
U ∈ t1 ∈
a consequence of f S(Rd), we have that the right side of (7) is finite since S is stable
∈
under Fourier transform, i.e., F f S(Rd).
{ } ∈
Using Theorem 16 in the isotropic setting we rewrite the weighted Sobolev norm as
f p = χ ω ∂αf p = lim χε ω ∂αf p .
k kWℓ,p(ω;U) k U · · kLp ε→0k U · · x kLp
|α|≤ℓ |α|≤ℓ
X X
The fact that U Rd is bounded and that χε is constructed according to (6), implies that
⊂ U
χε L1 FL1 as aconsequence of Young’s convolution inequality and Hölder’s inequalities
U ∈ ∩
(see (Tartar, 2007)). The assumption f S also implies that ∂αf L1 FL1 for any
∈ ∈ ∩
α Zd. As a result, with (Jones, 2001, Section 13.B, Page 316) we conclude that
∈ +
F(χε∂αf)= F(χε) F(∂αf)
U U ∗
12and further with Young’s convolution inequality
F(χε∂αf) χε ∂αf ,
k U kL1 ≤ k UkFL1 k kFL1
for any α Zd. Namely, χε∂αf L1 FL1 for any α Zd. Therefore, the integral
∈ + U ∈ ∩ ∈ +
representation of the inverse Fourier transform is well defined. As we are dealing with real
valued functions and use the symmetric normalization of the Fourier transform, we can
equivalently write
χε∂αf = F−1[F (χε∂αf)]= F F χε∂αf ,
U U U
h i
(cid:0) (cid:1)
where the overline represents the complex conjugate. This enables the equivalence
ωχε∂αf = F−1[F (χε∂αf)]
k U kLp U Lp(ω)
= (cid:13)F F χε∂αf (cid:13) = F(hε )
(cid:13) U (cid:13) Lp(ω) α,β Lp(ω)
(cid:13) (cid:13) h (cid:0) (cid:1)i(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
where hε := F χε∂αf . Usin(cid:13) g Theorem 11 we(cid:13) get
α,β U
(cid:0) (cid:1)
F(hε ) = hε C hε = C χε∂αa ,
α,β Lp(ω) α,β FLp ω ≤ α,β Lq(ϑ) k U kFLq(ϑ)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
for (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
ϑ(ξ)= ξ
d (cid:16)p1 ′− q1
(cid:17)υ
1 − p1 ′
| | ξ
(cid:18)| |(cid:19)
The construction of υ, specifically the lower bound υ(x) 1/ x
−γp′
, immediately implies
≥ h | |i
ϑ(ξ) ξ d (cid:16)p1 ′− q1 (cid:17) ξ γ ξ −t0 where t = γ d 1 1 .
≤ | | h i ≤ h i 0 − − p′ − q
(cid:18) (cid:19)
AsanextstepweapplytheYoung-typeinequalityTheorem13andremovetheε-dependence
by using Theorem 15 as follows:
kχε U∂αf kFLq(ϑ) = kχε U∂αf kFLτ0′
(ϑ)
≤
kχε U∂αf
kFL−τ0′
t0
≤
kχε
UkFLτ t11
k∂αf
kFLτ t22
≤
kχ U kFLτ t11 k∂αf kFLτ t22 .
The remaining part of the proof is to remove the partial derivatives from k∂αf kFLτ t22 in
terms of as follows
k·kFLτ
ω
k∂αf kFLτ t22 = h·it2F (∂αf) Lτ2 ≤ h·it2 |·||α|fˆ Lτ2
(cid:13) (cid:13)
≤ (cid:13) (cid:13) h·it2+ℓfˆ Lτ2 =(cid:13) (cid:13) kf kF(cid:13) (cid:13)Lτ t22 +ℓ. (cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
13Combining all of the above steps leads to
f p = χ ω ∂αf p
k kWℓ,p(ω;U) k U · · kLp
|α|≤ℓ
X
max χ ω ∂αf p 1
≤ |α|≤ℓk
U
· ·
kLp
|α|≤ℓ
X
ℓ
d+n 1
max limC χε∂αf −
≤ |α|≤ℓ ε→0 k U kFLq(ϑ) n
n=0(cid:18) (cid:19)
X
l+1 d+l
= max limC χε∂αf
|α|≤ℓ ε→0 k U kFLq(ϑ) d d 1
(cid:18) − (cid:19)
≤
C d,ℓm |α|a ≤x
ℓ
kχ U kFLτ t11 kf kFLτ t22
+ℓ
= C d,ℓ kχ U kFLτ t11 kf kFLτ t22 +ℓ,
where C = Cl+1 d+l . Finally, the fact that t2+ℓ is a polynomial weight and the den-
d,ℓ d d−1 h·i
sity argument of S in the Fourier-Lebesgue space extend the argument from f S to
(cid:0) (cid:1) ∈
f FLτ .
∈ t2+ℓ
AsaspecialcaseofourresultinTheorem23,wecanobtaintheresultsof(Siegel and Xu,
2020, Lemma 2) as follows:
Corollary 24 (Barron space ֒ Hℓ). Let d,ℓ N and Rd be a domain with finite
→ ∈ U ⊂
volume. Then for any f we have
ℓ
∈ B
1
kf kHℓ(U)
≤
C d,ℓ |U|2 kf kBℓ.
Proof This is an immediate consequence of Theorem 23 with the choice γ = 0, ω υ 1,
≡ ≡
p = q = 2, t = t = 0, τ = 2, and τ = 1.
1 2 1 2
For a more general result for Barron spaces, we consider the case τ = 1 for which
2
Assumption 12 simplifies to
1 1 1 1 1
0 2 1= 1 . (8)
≤ − τ − τ − − τ − τ ≤ 2
0 1 0 1
We observe that the requirement 1 < p′ q p from the weighted Hausdorff-Young
≤ ≤
inequality implies that p [2, ) and furthermore with τ = q′ it follows that τ [p′,p] is
0 0
∈ ∞ ∈
necessary and sufficient to fulfill this condition.
It is immediately clear that p τ max p′,τ′ (or equivalently min p,τ τ′ p′)
≥ 0 ≥ { 1} { 1 } ≥ 0 ≥
is necessary and sufficient in order to satisfy the non-negativity in (8) and the assumption
for the weighted Hausdorff-Young inequality. We observe
• τ [1,p′) violates this condition;
1
∈
• from τ [p′,2] it follows that the upper bound in (8) is trivially fulfilled and that
1
∈
τ [τ′,p] is sufficient to satisfy the non-negativity in (8) and also the assumption
0 ∈ 1
from the weighted Hausdorff-Young inequality as p′ τ′;
≤ 1
14• for τ (2,p] we get the additional constraint that τ 2(τ /2)′ (i.e., the Hölder
1 0 1
∈ ≤
conjugate of τ /2). That is, we can choose τ [τ′,min p,2(τ /2)′ ];
1 0 ∈ 1 { 1 }
• for τ > p implies that τ′ < p′ and therefore we can further restrict the feasible set
1 1
to τ [p′,min p,2(τ /2)′ ]. Note, that this interval is never empty as 2(τ /2)′ 2,
0 1 1
∈ { } ≥
even in the case τ = .
1
∞
Formally, this is as follows:
Corollary 25 (General Embedding of Barron Spaces). Let d,ℓ N, p [2, ), τ [p′, ]
1
∈ ∈ ∞ ∈ ∞
[τ′,p], τ [p′,2],
1 1 ∈
τ [τ′,min p,2(τ /2)′ ], τ (2,p],
0 ∈  1 { 1 } 1 ∈
 [p′,min p,2(τ /2)′ ], τ (p, ]
1 1
{ } ∈ ∞
Rd with finite volume, an d
U ⊂
ω(x) = υ(x)− p1 ′
where υ is a radial non-decreasing function such that υ A p′(Rd) with υ(x) 1/ x
−γp′
,
∈ ≥ h | |i
where γ δ := d(1/p′ 1/τ′). Furthermore, let t := γ δ and let t ,t R such that
≥ − − 0 0 − − 1 2 ∈
Assumption 12 is fulfilled.
Then, for all f ( )
∈Bt2+ℓ
U
kf kWℓ,p(ω;U)
≤
C ℓ,d kχ U kFLτ t11 kf kBt2+ℓ(U).
In case that p 4, we can further simplify the expression in the last two cases as
≥
the expression (τ /2)′ is monotonically decreasing in τ . We get 2(τ /2)′ 2(p/2)′ =
1 1 1
≤
2p/(p 2) 2p/(4 2) = p.
− ≤ −
Ideally for the approximation result in Section 4.1 we consider Barron functions (i.e.,
τ = 1) and would like to minimize the polynomial degree t ,t of the weights as much as
2 1 2
possible such that the constraints on the domain are minimized and the class of functions
that is embedded in Wℓ,p(ω; ) is maximized. To do so, we observe first that necessarily
U
1 1
t ,t t =γ +δ = γ+d ,
1 2 ≥ − 0 p′ − τ′
(cid:18) 0(cid:19)
wheretheright-handsideisminimizedbychoosingτ = pandtheleft-handsidebychoosing
0
equality. Note that we are not necessarily free to choose γ as this might necessarily be
required to be strictly positive in order to accommodate for weights with singularities. For
the Toft-Young functional we get
1 1
R(τ) = 1 [0,1/2],
− p − τ ∈
1
which results in the necessary condition τ [p′,2(p/2)′]. In order to keep the restrictions
1
∈
on the domain as relaxed as possible, we choose
2p
τ = 2(p/2)′ = (with τ = for p = 2)
1 1
p 2 ∞
−
15and consequently get R(τ) = 1/2. As a last step, we have to make sure that either the
inequality (5) in Assumption 12 is strict or that γ > dR(τ) = d/2. Due to the choice
t = t = t = γ, we would get equality in (5) if γ = d/2, thus γ > d/2 is necessary and
1 2 0
−
sufficient for the other choices to be valid.
Corollary 26. Let d,ℓ N, γ R with γ > d/2, p [2, ], q = 2(p/2)′, Rd have
∈ ∈ ∈ ∞ U ⊂
finite volume, and
ω(x) = υ(x)− p1 ′,
where υ is a radial non-decreasing function such that υ A p′(Rd) with υ(x) 1/ x −γp′ .
∈ ≥ h | |i
Then for any f ( ),
γ+ℓ
∈B U
kf kWℓ,p(ω;U)
≤
C d,ℓ kχ U kFLq
γ
kf kBγ+ℓ(U),
where the constant C only depends on the number of dimensions d and the order ℓ of the
d,ℓ
Sobolev norm.
For more general choice for the space of target functions we can get the following result.
Corollary 27 (Conjugate FL-Spaces). Let d,ℓ N, γ > d/2, Rd have finite volume,
∈ U ⊂
and
ω(x) =
υ(x)−1
2
where υ is a radial non-decreasing function such that υ A (Rd) with υ(x) 1/ x −2γ.
2
∈ ≥ h | |i
Then for any f FLτ ( )
∈ γ+ℓ U
kf kHℓ(ω;U)
≤
C d,ℓ kχ U kFLτ γ′ kf kFLτ γ+ℓ(U).
Proof The statement immediately follows from Theorem 23 by choosing τ′ = τ = τ
1 2 ∈
[1, ], t = t = t = γ, and p = q = 2.
1 2 0
∞ −
3.2 Embedding in Sobolev Spaces with p [1,2]
∈
Lemma 28 (Low Degree Lemma). Let d N and p,q,r (1, ) such that 1 < q r q′
∈ ∈ ∞ ≤ ≤
and p r. Let
≤
1
ϑ(x)= x
d( q1 ′− r1)
ω(x) and ω(x) = υ
1 q
| | x
(cid:18)| |(cid:19)
with υ being a radial non-decreasing weight function such that υ A (Rd) and let Rd
q
∈ U ⊂
be a bounded set. Let f FLq(ω ℓ), then there exists a constant C that depends on
d,ℓ,p
∈ h·i
d,ℓ and p such that
1−1
kf kWℓ,p(ϑ;U)
≤
C d,ℓ,p |U|p r kf kFLq(ωh·iℓ;Rd)
for any ℓ Z .
∈ +
16Proof First, we assume that f S(Rd) and recall that
∈
f p := ϑ(x)∂αf(x) pdx
k kWℓ,p(ϑ;U) | x |
|α|≤ℓZU
X
max ϑ(x)∂αf(x) pdx 1
≤ |α|≤ℓ ZU | x |
|α|≤ℓ
X
ℓ+1 d+ℓ
max ϑ(x)∂αf(x)pdx
≤ d (cid:18)d −1 (cid:19)|α|≤ℓ ZU | x |
ℓ+1 d+ℓ
= max χ ϑ∂αf p ,
d d 1 |α|≤ℓ k U kLp
(cid:18) − (cid:19)
for any ℓ Z and multiindices α Zd.
∈ + ∈ +
We now assume α Zd to be fixed with α ℓ. By Hölder’s inequality with 1 = 1 + 1
∈ + | | ≤ p t r
we get
1
kχ Uϑ∂αf kLp
≤
|U|t kϑ∂αf kLr .
Since f S it follows that ∂αf belongs also to S. Hence ∂αf and F (∂αf) belong to L1.
∈
Therefore, we have pointwise equality in
∂αf = F−1(F (∂αf)).
Furthermore, ϑ is a weight of the form given in (2) and therefore by Theorem 10 there is a
constant C > 0 such that
∂αf = F (∂αf) C F (∂αf) ,
k
kLr(ϑ)
k
kFLr(ϑ)
≤ k
kLq(ω)
1
where 1 < q r q and ω(x) = v(1/ x )q. Consequently, we conclude that
≤ ≤ ′ | |
F (∂αf) ω |α|F(f) .
k
kLq(ω)
≤ |·| Lq
(cid:13) (cid:13)
(cid:13) (cid:13)
All together, we get (cid:13) (cid:13)
1
kf kWℓ,p(ϑ;U)
≤
ℓ+
d
1 dd+ 1ℓ p |U|1 t k∂αf kLr(ϑ)
≤
C d,ℓ,p |U|1 t kF (∂αf) kLq(ω)
(cid:18) (cid:18) − (cid:19)(cid:19)
C d,ℓ,p 1 t ω |α|F(f) C d,ℓ,p 1 t ω |α|F(f)
≤ |U| |·| Lq ≤ |U| h·i Lq
1 (cid:13) (cid:13) (cid:13) (cid:13)
≤ C d,ℓ,p |U|t (cid:13) (cid:13)kf kFLq(ωh·iℓ),(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
1
for any ℓ Z , where C = C ℓ+1 d+ℓ p. This concludes the proof of the lemma by
∈ + d,ℓ,p d d−1
using the fact that S is dense in F(cid:16) Lq((cid:0)ω h·i(cid:1)ℓ(cid:17)), for any ℓ ∈Z
+
and q
∈
[1, ∞].
174 Approximation of Fourier-Lebesgue Spaces
As a second part of our main contributions, we now deal with function approximation. For
doing so, we make use of Maurey’s sampling argument (Pisier, 1980/1981) and follow a
similar approach as in (Barron, 1993; Siegel and Xu, 2020; Abdeljawad and Dittrich, 2023).
Since the techniques used for bounded and unbounded domains are different, we split this
section into two parts. First, we address the approximation of functions with error in
bounded domains, and next we analyze the unbounded case, as seen in Theorem 29 and
Theorem 33, respectively.
4.1 Error Measure over Bounded Domain
In this section, we aim to approximate functions in a given weighted Barron space using
shallowneuralnetworks,wheretheerrormeasureistheweightedSobolevnormonabounded
domain. It is worth mentioning that our findings generalize the existing literature in the
sense that we allow the weights to exhibit singularities and permit arbitrary integrability
exponents such that the space is of Rademacher type 2.
Theorem29(ApproximationinweightedSobolevSpace). Letd,ℓ N, γ 0withγ = d/2,
p [2, ), q = 2(p/2)′, Rd such that χ FLq , and ∈ ≥ 6
U γ
∈ ∞ U ⊂ ∈
ω(x) = υ(x)− p1 ′,
where υ is a radial non-decreasing function such that υ A p′(Rd) with υ(x) 1/ x −γp′ .
Further, let f and let ̺ Wm,∞( s;R) be∈ an activation functi≥ onh for| s|i > 1.
γ+ℓ+1
∈ B ∈ h·i
Then,
fNin ∈f Σ̺kf −f N kWℓ,p(ω;U) .
N−1
2 kω kLp(U)kf kBγ+ℓ+1
where the implied constant only depends on the parameters of the setting but not on the
function itself.
Proof In this proof we take a similar approach to (Barron, 1993; Siegel and Xu, 2020;
Abdeljawad and Dittrich, 2023). That is, we first show that the target function can be
represented as a infinite convex combination of elements of some dictionary and second,
we use Maurey’s sampling argument (see (Pisier, 1980/1981; Barron, 1993)) to provide the
approximation rate.
The first step in this approach is to express the Fourier basis in terms of the target
function. The approach to do so is to start with a linear shift in the Fourier transform of
the activation function
1 1
̺ˆ(τ) = ̺(t)e−iτtdt = ̺( ξ,x +b)e−iτ(hξ,xi+b)db
√2π R √2π R h i
Z Z
which allows us to express the exponential term as
1
eiτhξ,xi = ̺( ξ,x +b)e−iτbdb
√2π̺ˆ(τ) R h i
Z
18under the assumption that τ = 0 and ̺ˆ(τ) = 0. Inserting this into the Fourier transform of
6 6
the target function leads to the representation
1 1 ξ,x
f(x)= eihξ,xifˆ(ξ)dξ = ̺ h i +b fˆ(ξ)e−iτbdbdξ.
(2π)d 2 ZRd (2π)d+ 21 ̺ˆ(τ) ZRd ZR (cid:18) τ (cid:19)
In the next step we split this representation into the elements of some dictionary and the
measure that represents our function in this dictionary. To do so, we introduce the modified
weight
ϕ(ξ,b) = (1+(b R ξ/τ ) )s with R = sup x
U + U
| |− | | | |
x∈U
and extend the reepresentation of f as follows
ϕ(ξ,b) ξ,x ξ γ+ℓ
f(x) = C ̺ h i +b h i fˆ(ξ)e−iτbdbdξ
̺,d Rd R ξ γ+ℓ τ ϕ(ξ,b)
Z Z h i (cid:18) (cid:19)
e
where the constant is
e
d+1 −1
C ̺,d = (2π) 2 ̺ˆ(τ) .
(cid:16) (cid:17)
In this representation, the elements of our dictionary are then
ϕ(ξ,b) ξ,x
̺˜(x;ξ,b) = ̺ h i +b
ξ γ+ℓ τ
h i (cid:18) (cid:19)
e
with parameters ξ and b and the (complex) measure, associated with the function f, is
ξ γ+ℓ
dµ (ξ,b) = C h i fˆ(ξ)e−iτbd(ξ,b).
f ̺,d
ϕ(ξ,b)
Based on this measure, we can calculate the variation norm of f as
e
f = d µ (ξ,b) = µ (, ) ,
k kK(Σ̺˜) ZRd×R | f | k f · ·· kL1(Rd×R)
for which we now first calculate the integral over b.
1 ∞ 1
I(ξ) = db = 2 db
ZR ϕ˜(ξ,b) Z0 (1+(b −R U |ξ/τ |) +)s
R ξ ∞ 1
U
= 2 + db C ξ .
τ b s ≤ U,τ,s h i
(cid:18)(cid:12) (cid:12) Z0 h i (cid:19)
(cid:12) (cid:12)
(cid:12) (cid:12)
The variation norm is then given by
(cid:12) (cid:12)
ξ γ+ℓ
f = C h i fˆ(ξ)e−iτb dbdξ = C I(ξ) ξ γ+ℓ fˆ(ξ) dξ. (9)
k kK(Σ̺˜) ̺,d Rd R(cid:12)ϕ(ξ,b) (cid:12) ̺,d Rd h i
Z Z (cid:12) (cid:12) Z (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)e (cid:12)
19For the upper bound on the dictionary we first consider the partial derivatives for fixed α
ϕ(ξ,b) ξα ξ
∂α̺˜ = ω∂α̺˜ = | | ω()̺(|α|) +b
k kLp(ω;U) k kLp(U) ξ γ+ℓ τ |α| (cid:13) · (cid:18)τ · (cid:19)(cid:13)Lp(U)
h ei | | (cid:13) (cid:13)
ω() (cid:13) (cid:13)
C ϕ(ξ,b) τ −|α| · (cid:13) = C τ −|α| ω(cid:13)
≤ ̺,ϕ | | ϕ(ξ,b) ̺,ϕ | | k kLp(U)
(cid:13) (cid:13)Lp(U)
(cid:13) (cid:13)
where e (cid:13) (cid:13)
(cid:13) (cid:13)
e
C = ̺ .
̺,ϕ
k
kWℓ,∞(h·is;R)
The final bound on the Sobolev-Norm is given by
̺˜ = ∂α̺˜
k
kWn,p(ω;U)
k
kLp(ω;U)
|α|≤ℓ
X
C ω τ −|α| (10)
≤
̺,ϕ
k
kLp(U)
| |
|α|≤ℓ
X
= C C ω .
̺,ϕ τ,ℓ
k
kLp(U)
With the assumptions on f and , namely
U
f and χ FLq,
∈ Bγ+ℓ+1 U ∈ γ
and Theorem 26, knowing that the weighted Fourier Lebesgue spaces decreasing when the
weight increases, we also get f Wℓ,p(ω; ). Therefore, we can apply Maurey’s approxi-
∈ U
mation (cf. Theorem 22) to get the result with = Wℓ,p(ω; ), f given by (9), KD
X U k kK(Σ̺˜)
given by (10), and M = f .
k kK(Σ̺˜)
Remark 30 (The theory covers unbounded weights). Our initial claim (see Section 1) was
that our theory is capable of treating unbounded weights. To see this, we take a more detailed
look at the assumption υ(x) 1/ x
−γp′
. This assumption allows us to choose the weight
υ = x γ and for d < γ < (≥ p h 1)| d| wi e get υ A (Rd). Thus, we can use Wℓ,p(ω; ) with
p
ω(x)| =| x − pγ ′ wei− ght in the err− or norm, which∈ indeed has a singularity at x =0 forU positive
| |
γ.
4.2 Error Measure over Unbounded Domain
Inthis final section we focusonthe casethat the error inthe Sobolev norm is measured over
an unbounded domain. In that regard, we have to consider a weighted Sobolev norm with
decayingweight. Asafirstresult,weextendknownembeddingresultsforthespectralBarron
space in the Sobolev space Hℓ( ) with bounded Rd to an embedding in Wℓ,p( −u).
U U ⊂ h·i
Lemma 31 (Embedding of Fourier-Lebesgue Spaces). Let d,ℓ N, u 0, 1 p′ q
∈ ≥ ≤ ≤ ≤
2 p such that 1 = 1 + 1 (we make the adaptation r = p if q = 1) and ur > d, then
≤ p r q′
kf kWℓ,p(h·i−u) . kf kFLq
ℓ
for all f .
ℓ
∈ B
20Proof Let f S, then we get with 1 = 1 + 1 via Hölder’s inequality and the Hausdorff-
∈ p r q′
Young inequality that
h·i−u∂αf
Lp ≤
h·i−u
Lr
k∂αf kLq′
≤
h·i−u
Lr
∂αf
Lq
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) ≤ (cid:13) (cid:13) h·i−u(cid:13) (cid:13) Lr h·iℓf Lq =(cid:13) (cid:13) h·i−u(cid:13) (cid:13) Lr(cid:13) (cid:13)kdf kF(cid:13) (cid:13)Lq ℓ .
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
With ur > d, the constant −u(cid:13) is(cid:13)finit(cid:13) (cid:13)e andb(cid:13) (cid:13)due to(cid:13)the d(cid:13)ensity of S in FLq( ℓ) we
h·i Lr h·i
can extend this bound to all f FLq( ℓ).
(cid:13) (cid:13)
∈ h·i
(cid:13) (cid:13)
As a steptowards our approximation result forshallow neural networks, we next provide
bounds on all possible neurons in the network which will later be used to develop a specific
dictionary that can be uniformly bounded in Wℓ,p( −u).
h·i
Lemma32(BoundonNeurons). Letd,ℓ,m N, 2 p < , andp,u,v R. Furthermore,
let ̺ Wℓ,∞( v;R). Then, for all (ξ,b) R∈ d R≤ , ∞ ∈
∈ h·i ∈ ×
g(;ξ,b) :Rd R, with g(x;ξ,b) := ̺( ξ,x /τ +b)
· → h i
is in Wℓ,p( −u) and we get
h·i
(i) for up > d:
kg( ·;ξ,b) kWℓ,p(h·i−u) . |ξ |ℓ h·id− p1−u h|ξ |/τ ·+b i−v Lp(R);
(cid:13) (cid:13)
(cid:13) (cid:13)
(ii) for 1 < r v and (u r)p > d: (cid:13) (cid:13)
≤ −
kg( ·;ξ,b) kWℓ,p(h·i−u) . |ξ |ℓ hmin {1, |τ |/ |ξ |}|b |i−r.
Proof In (i) we start off by calculating the Lp norm for fixed order α Zd of partial
∈ +
derivatives (α ℓ) and fixed parameters (ξ,b) Rd R
| | ≤ ∈ ×
k∂αg( ·;ξ,b) kLp(h·i−u;Rd) = k∂α̺( hξ, ·i/τ +b) kLp(h·i−u;Rd)
= ξα τ −|α| ̺(|α|)( ξ, /τ +b)
| || | h ·i Lp(h·i−u;Rd)
(cid:13) (cid:13)
ξ ℓ τ −|α| (cid:13)̺(|α|)( ξ, /τ +b)(cid:13)
≤ | | | | (cid:13) h ·i (cid:13) Lp(h·i−u;Rd)
(cid:13) (cid:13)
= ξ ℓ τ −|α|(cid:13) −u̺(|α|)( ξ, /τ(cid:13)+b)
(cid:13) (cid:13)
| | | | h·i h ·i Lp
(cid:13) (cid:13) 1
(cid:13) (cid:13) p
. ξ ℓ x(cid:13) −up ξ,x /τ +b −vpd(cid:13)x .
| | Rdh i hh i i
(cid:18)Z (cid:19)
The implied constant is τ −|α| ̺ . For ξ = 0 we can combine all partial deriva-
| | k
kWℓ,∞(h·iv;R)
tives with α ℓ to get
| | ≤
k∂α̺( hξ, ·i/τ +b) kLp(h·i−u;Rd) . hb i−v h·i−u
Lp
. |0 |ℓ h·id− p1−u h0+b i−v Lp(R),
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
21with the implied constant depending on τ, ℓ, and d.
The case ξ = 0 and d = 1 is trivial, as (11) is the same expression as the statement of
6
the lemma.
For the case ξ = 0 and d > 1 we split the integral over Rd into the one-dimensional
6
integral that is parallel to ξ and the remaining parts that are orthogonal to ξ. To do so, we
denote by V⊥ the basis for the d 1 dimensional subspace that is orthogonal to ξ. Then
ξ −
for every x Rd, there is y Rd−1 and t R such that x = V⊥y + ξ −1ξt. Overall, this
∈ ∈ ∈ ξ | |
transformation is unitary (i.e., no rescaling due to Jacobian determinant) and therefore
−up
x −up ξ,x /τ +b −vpdx = V⊥y+ ξ −1ξt ξ t/τ +b −vpdydt.
Rdh i hh i i R Rd−1 ξ | | h| | i
Z Z Z D E (12)
We will simplify this argument by calculating the (d 1)-dimensional integral over y. To do
−
so, we first use the the equivalence of finite-dimensional p-norms to bound
x = (x,1) √d+1 (x,1) = √d+1(1+ x
2)21
.
h i k k1 ≤ k k2 | |
Second,weusethefactthatthebasisformedbyV⊥ andξ areorthogonal. Third,weperform
ξ
a transformation to polar coordinates. And fourth, we substitute z = √1+t2tan(θ).
−up
2 2
1+ V⊥y+ ξ −1ξt dy
Rd−1 ξ | |
Z (cid:18) (cid:12) (cid:12) (cid:19)
(cid:12)
(cid:12)−up
= (cid:12) 1+t2+ y 2(cid:12) 2 dy
Rd−1 | |
=
Z 2πd− 21(cid:16)
∞
zd−2
1(cid:17)
+t2+z2
−u 2p
dy
Γ(d−1)
2 Z0
= 2πd− 21 π 2 (1+t(cid:0) 2)d− 22 tand−2(cid:1) (θ)(1+t2)−u 2p 1+tan2(θ) −u 2p √1+t2 dθ
Γ(d−1) cos2(θ)
2 Z0
d−1 π (cid:0) (cid:1)
=
2π 2 (1+t2)d−1 2−up 2
sind−2(θ)cos(θ)up−ddθ
Γ(d−1)
2 Z0
. t d−1−up.
h i
For the last bound, we used the assumption up > (u r)p > d > 1, which renders the
−
exponents in the integral positive and, therefore, allows us to bound the integral by π. The
2
implied constant then depends solely on d. Inserting (12) and (13) into (11) results in the
following scalar integral:
1
p
k∂αg( ·;ξ,b) kLp(h·i−u;Rd) . |ξ |ℓ Rht id−1−up h|ξ |t/τ +b i−vpdt
(cid:18)Z (cid:19)
= ξ ℓ
d− p1−u
ξ /τ +b −v .
| | h·i h| | · i Lp(R)
(cid:13) (cid:13)
The same asymptotic bound holds true for t(cid:13) (cid:13)he Sobolev norm Wℓ,p((cid:13) (cid:13) −u) with an additional
h·i
implied constant counting the number of partial derivatives up to order ℓ.
22For (ii), we extend the bound from (i). Observe, that the value of the norm is indepen-
dent of the sign of b and τ and therefore, we will limit our analysis to the case where bt < 0.
With 1 < r v we have
≤
t
d− p1−u
ξ t/τ +b −v t
d− p1−u
ξ t/τ +b −r
h i h| | i ≤ h i h| | i
r
−(u−r−d−1) 1
= t p
h i t ξ t/τ +b
(cid:18)h ih| | i(cid:19)
and define h(t) := t ξ t/τ +b . In order to find an upper bound on the reciprocal of h,
h ih| | i
we instead find a lower bound on h. Splitting into the three intervals ( ,0), [0, τb/ ξ ),
−∞ − | |
and[ τ b / ξ , ), weseethathismonotonically decayingonthefirstinterval, concave on
− | | | | ∞
the second interval, and monotonically increasing on the third interval. Thus, its minimum
is obtained either for t = 0 or t = τb/ ξ . That is
− | |
minh(t) = min h(0),h( τb/ ξ ) = min b , τb/ ξ = 1+min 1, τ / ξ b .
t∈R { − | | } {h i h | |i} { | | | |}| |
With the assumption (u r)p > d this implies
−
ht
id− p1−u
h|ξ |t/τ +b i−v
Lp(R) ≤
k1/hr
kL∞
ht
id− p1−u+r
Lp
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1 (cid:13)
(cid:13)
r 2 p1
=
1+min 1, τ / ξ b (u r)p d
(cid:18) { | | | |}| |(cid:19) (cid:18) − − (cid:19)
For a single partial derivative, this leads to
k∂αg( ·;ξ,b) kLp(h·i−u;Rd) . |ξ |ℓ hmin {1, |τ |/ |ξ |}|b |i−r
and by counting the number of partial derivatives up to order ℓ, we get the bound on in the
weighted Sobolev norm.
Finally, we can state the approximation result over unbounded domains.
Theorem 33 (Approximation over Unbounded Domain). Let d,ℓ,N,m N and p,r,u,v
R such that 2 p < , 1 < r v, and (u r)p > d. Furthermore,
let∈
̺ Wℓ,∞(
v;R∈
)
≤ ∞ ≤ − ∈ h·i
be an activation function and D be the corresponding dictionary over Rd. For every target
̺
function f we get
ℓ+r
∈B
fN∈i Σn Nf
(D
̺)kf −f N kWℓ,p(h·i−u) . N− 21 kf kBℓ+r.
Proof The space in which we measure the error is Wℓ,p( −u). For p = 2 we are dealing
h·i
with a Hilbert-space, which is by definition a type-2 Banach space. For 2 p < , we
know that Lp( −u;Rd) is a type-2 Banach space (see e.g., (Siegel and Xu, ≤ 2022b))∞ . This
immediately exh t·i ends to Wℓ,p( −u) by using the equivalence of finite-dimensional p-norms.
h·i
Thetype-2propertyallowsustouseMaurey’ssamplingargument(see(Pisier,1980/1981;
Barron, 1993; Siegel and Xu, 2022b)) in combination with the given error norm. Addition-
ally, we need to provide an integral representation of the target function in terms of some
23dictionary. We then need to show that the dictionary is bounded in Wℓ,p( −u) and that
h·i
the target function has finite variation norm for the given dictionary.
Fortheintegralrepresentationwetakeasimilarapproachto(Barron,1993;Siegel and Xu,
2020; Abdeljawad and Dittrich, 2023). The first step in this approach is to express the
Fourier basis in terms of the target function. To do so, we start with a linear shift in the
Fourier transform of the activation function
1 1
̺ˆ(τ) = ̺(t)e−iτtdt = ̺( ξ,x +b)e−iτ(hξ,xi+b)db
√2π R √2π R h i
Z Z
which allows us to express the exponential term as
1
eiτhξ,xi = ̺( ξ,x +b)e−iτbdb
√2π̺ˆ(τ) R h i
Z
under the assumption that τ = 0 and ̺ˆ(τ) = 0. As a second step, we insert this into the
6 6
Fourier transform of the target function, which leads to the representation
1
f(x)= eihξ,xifˆ(ξ)dξ
d
(2π)2 ZRd
1
= ̺( ξ,x /τ +b)fˆ(ξ)e−iτbdbdξ.
(2π)d+ 21 ̺ˆ(τ) ZRd ZR h i
The last step regarding the integral representation is to specify the dictionary D and the
corresponding measure of f in the variation space of K(D). In order to have a bounded
dictionary and finite variation norm, we extend the representation of f by the weight ω (as
defined in the statement of the theorem) and
ϑ(ξ,b) = ϑ ( ξ ,b) = b r ξ −r with 1 < r v
1
| | h i h i ≤
to get
ϑ(ξ,b) ξ ℓ
f(x)= c ̺( ξ,x /τ +b) h i fˆ(ξ)e−iτbdbdξ.
Rd R ξ ℓ h i ϑ(ξ,b)
Z Z h i
This now leads to the dictionary
ϑ(ξ,b)
D = ̺( ξ, /τ +b): Rd R (ξ,b) Rd R
( ξ ℓ h ·i → (cid:12) ∈ × )
h i (cid:12)
(cid:12)
(cid:12)
and the measure (cid:12)
ξ ℓ
dµ (ξ,b) = h i fˆ(ξ)e−iτbdbdξ
f
ϑ(ξ,b)
24ForthevariationnormweuseHölder’sinequalityforthenormovertheweightparameter,
which leads to the following bound in terms of the FL1-norm:
ℓ 1
f = h·i fˆ() = ℓ+rfˆ() db
k kK(D) (cid:13) (cid:13)ϑ( ·, ··) · (cid:13)
(cid:13)L1(Rd×R)
(cid:13)h·i · ZR hb ir (cid:13)L1(Rd)
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13) ℓ+rfˆ() (cid:13) (cid:13)
1(cid:13)
(cid:13) db
(cid:13)
(cid:13)
≤ (cid:13)h·i · (cid:13)L1(Rd) (cid:13)ZR hb ir (cid:13)L∞(Rd)
(cid:13) (cid:13)
(cid:13) 2(cid:13)
(cid:13) (cid:13)
= (cid:13)f (cid:13) . (cid:13) (cid:13)
k kBℓ+r r 1
−
The dictionary constant KD is defined as
ϑ(ξ,b)
KD = s hu ∈p Dkh kWℓ,p(h·i−u) =
ξ ℓ
(ξ,b)s ∈u Rp d×Rk̺( hξ, ·i/τ +b) kWℓ,p(h·i−u)
h i
and with the result (ii) from Theorem 32, we get
b r b r
KD . ξh ℓi
+r
|ξ |ℓ hmin {1, |τ |/ |ξ |}|b |i−r
≤ ξ r
minh i
1, |τ| b r
h i h i |ξ| | |
D n o E
For small ξ (i.e., ξ < τ ), we can simplify this to
| | | |
ξ −r b r min 1, τ / ξ b −r = ξ −r b r−r,
h i h i h { | | | |}| |i h i h i
which uniformly bounded by 1. Conversely for large ξ (i.e., ξ τ ) we get
| |≥ | |
b r ξ r b r
ξ −r b r min 1, τ / ξ b −r = h i | | = h i ,
h i h i h { | | | |}| |i ξ r( ξ + τ b )r (ξ + τ b )r
h i | | | || | | | | || |
which is monotonically decreasing in ξ and b , thus, uniformly bounded by τ −r (this is
| | | | | |
obtained by setting b = 0 and ξ = τ ).
| | | |
Overall, this results in the following bound on the dictionary:
KD . min 1,τ r (15)
{ }
where the implied constant depends on the number of dimensions d, the regularity ℓ, the
integrability p, the activation function ̺, and the weight ω.
Thus, by Maurey’s sampling argument in the formulation of (Siegel and Xu, 2022b,
(1.17)) with the variation norm being bounded as in (14) and the dictionary being bounded
as in (15), we have
fN∈Σi Nn ,f Mf(D)kf −f N kWℓ,p(h·i−u) .
N−1
2 kf kBℓ+r
with M = f In this formulation, the dictionary D is simply a rescaling of D , there-
f
k
kK(D) ̺
fore, Σ (D) Σ (D ) and finally,
N,M N ̺
⊂
fN∈i Σn Nf
(D
̺)kf −f N kWℓ,p(h·i−u)
≤
fN∈Σin Nf ,M(D)kf −f N kWℓ,p(h·i−u)
.
N−1
2 f .
k kBℓ+r
25References
A. Abdeljawad and T. Dittrich. Space-Time Approximation with Shallow Neural Networks
in Fourier Lebesgue Spaces. arXiv:2312.08461, Dec. 2023.
A. Abdeljawad and T. Dittrich. Approximation Rates in Fréchet Metrics: Barron Spaces,
Paley-Wiener Spaces, and Fourier Multipliers. In Preparation, 2024.
J.P.Agnelli,E.M.Garau,andP.Morin. A Posteriori ErrorEstimatesforEllipticProblems
with Dirac Measure Terms in Weighted Spaces. ESAIM: Mathematical Modelling and
Numerical Analysis, 48(6):1557–1581, Nov. 2014. doi: 10.1051/m2an/2014010.
A. Allendes, G. Campaña, and E. Otárola. Finite Element Discretizations of a Convective
Brinkman–Forchheimer Model Under Singular Forcing. Journal of Scientific Computing,
99(2):58, May 2024. doi: 10.1007/s10915-024-02513-5.
A. Anandkumar, K. Azizzadenesheli, K. Bhattacharya, N. B. Kovachki, Z. Li, B. Liu, and
A. Stuart. Neural Operator: Graph Kernel Network for Partial Differential Equations. In
ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations,
2019.
F. Bach. Breaking the Curse of Dimensionality with Convex Neural Networks. Journal of
Machine Learning Research, 18(19):1–53, 2017.
A. R.Barron. Universal Approximation Bounds for Superpositions of a Sigmoidal Function.
IEEE Transactions on Information Theory, 39(3):930–945, May 1993. doi: 10.1109/18.
256500.
F.Bartolucci, E.DeVito, L.Rosasco,andS.Vigogna. UnderstandingNeuralNetworkswith
Reproducing Kernel Banach Spaces. Applied and Computational Harmonic Analysis, 62:
194–236, Jan. 2023. doi: 10.1016/j.acha.2022.08.006.
T.ChenandH.Chen. UniversalApproximation toNonlinearOperatorsbyNeuralNetworks
with Arbitrary Activation Functions and Its Application to Dynamical Systems. IEEE
Transactions on Neural Networks, 6(4):911–917, July 1995. doi: 10.1109/72.392253.
R. A. DeVore. Nonlinear Approximation. Acta Numerica, 7:51–150, Jan. 1998. doi: 10.
1017/S0962492900002816.
R. A. DeVore, B. Hanin, and G. Petrova. Neural Network Approximation. Acta Numerica,
30:327–444, May 2021. doi: 10.1017/S0962492921000052.
R.A. DeVore, R. D. Nowak, R. Parhi, and J. W. Siegel. Weighted Variation Spaces and Ap-
proximation by Shallow ReLU Networks. Applied and Computational Harmonic Analysis,
74:101713, Jan. 2025. doi: 10.1016/j.acha.2024.101713.
G. Dusson, M.-S. Dupuy, and I.-M. Lygatsika. A Posteriori Error Estimates for Schrödinger
Operators Discretized with Linear Combinations of Atomic Orbitals. arXiv:2410.04943,
Oct. 2024.
26W. E, J. Han, and A. Jentzen. Deep Learning-Based Numerical Methods for High-
DimensionalParabolicPartialDifferentialEquationsandBackwardStochasticDifferential
Equations. Communications in Mathematics and Statistics, 5(4):349–380, Dec. 2017. doi:
10.1007/s40304-017-0117-6.
L. Gerard, P. Grohs, and M. Scherbela. Deep Learning Variational Monte Carlo for Solving
the Electronic Schrödinger Equation. In Handbook of Numerical Analysis, volume 25,
pages 231–292. Elsevier, 2024. ISBN 978-0-443-23984-7. doi: 10.1016/bs.hna.2024.05.010.
Y. Gordon, V. Maiorov, M. Meyer, and S. Reisner. On the Best Approximation by Ridge
Functions in the Uniform Norm. Constructive Approximation, 18(1):61–85, Jan. 2001.
doi: 10.1007/s00365-001-0009-5.
L. Grafakos. Classical Fourier Analysis, volume 249 of Graduate Texts in Mathematics.
Springer New York, New York, NY, 2014. ISBN 978-1-4939-1194-3. doi: 10.1007/
978-1-4939-1194-3.
J. Han, A. Jentzen, and W. E. Solving High-Dimensional Partial Differential Equations
Using Deep Learning. Proceedings of the National Academy of Sciences, 115(34):8505–
8510, Aug. 2018. doi: 10.1073/pnas.1718942115.
H.P. Heinig and G.J. Sinnamon. Fourier Inequalities andIntegral Representations of Func-
tions in Weighted Bergman Spaces over Tube Domains. Indiana University Mathematics
Journal, 38(3):603–628, 1989.
L. Heltai and N. Rotundo. Error Estimates in Weighted Sobolev Norms for Finite Element
Immersed Interface Methods. Computers & Mathematics with Applications, 78(11):3586–
3604, Dec. 2019. doi: 10.1016/j.camwa.2019.05.029.
J. Hermann, J. Spencer, K. Choo, A. Mezzacapo, W. M. C. Foulkes, D. Pfau, G. Carleo,
and F. Noé. Ab-Initio Quantum Chemistry with Neural-Network Wavefunctions. Nature
Reviews Chemistry, 7(10):692–709, Aug. 2023. doi: 10.1038/s41570-023-00516-8.
W. B. Johnson and J. Lindenstrauss. Basic Concepts in the Geometry of Banach Spaces. In
Handbook of the Geometry of Banach Spaces, volume 1, pages 1–84. Elsevier, 2001. ISBN
978-0-444-82842-2. doi: 10.1016/S1874-5849(01)80003-6.
F. Jones. Lebesgue Integration on Euclidean Space. Jones and Bartlett Books in Mathemat-
ics. Jones and Bartlett, Sudbury, Mass, rev. ed edition, 2001. ISBN 978-0-7637-1708-7.
H.KoandS.Lee. FourierTransformandRegularityofCharacteristicFunctions. Proceedings
of the American Mathematical Society, 145(3):1097–1107, Nov. 2016. doi: 10.1090/proc/
13435.
N. B. Kovachki, S. Lanthaler, and A. M. Stuart. Operator Learning: Algorithms and
Analysis. arxiv:2402.15715, 2024.
V. Kurkova and M. Sanguineti. Bounds on Rates of Variable-Basis and Neural-Network
Approximation. IEEE Transactions on Information Theory,47(6):2659–2665, Sept./2001.
doi: 10.1109/18.945285.
27V. V. Lebedev. On the Fourier Transform of the Characteristic Functions of Domains with
C1 Boundary. Functional Analysis and Its Applications, 47(1):27–37, Mar. 2013. doi:
10.1007/s10688-013-0004-1.
M. Ledoux and M. Talagrand. Probability in Banach Spaces. Springer Berlin Heidelberg,
Berlin, Heidelberg, 1991. ISBN 978-3-642-20212-4. doi: 10.1007/978-3-642-20212-4.
L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning Nonlinear Operators
Via DeepONet Based on the Universal Approximation Theorem of Operators. Nature
Machine Intelligence, 3(3):218–229, Mar. 2021. doi: 10.1038/s42256-021-00302-5.
L. Ma, J. W. Siegel, and J. Xu. Uniform Approximation Rates and Metric Entropy of
Shallow Neural Networks. Research in the Mathematical Sciences, 9(3):46, Sept. 2022.
doi: 10.1007/s40687-022-00346-y.
R. H. Nochetto, E. Otárola, and A. J. Salgado. Piecewise Polynomial Interpolation in
Muckenhoupt Weighted Sobolev Spaces and Applications. Numerische Mathematik, 132
(1):85–130, Jan. 2016. doi: 10.1007/s00211-015-0709-6.
G. Ongie, R. Willett, D. Soudry, and N. Srebro. A Function Space View of Bounded Norm
InfiniteWidthReluNets: TheMultivariateCase.InInternationalConference onLearning
Representations, 2020.
R. Parhi and R. D. Nowak. Banach Space Representer Theorems for Neural Networks and
Ridge Splines. Journal of Machine Learning Research, 22(1), Jan. 2021.
G. Pisier. Remarques Sur Un Résultat Non Publié De B. Maurey. Séminaire d’Analyse
fonctionnelle (dit" Maurey-Schwartz"), pages 1–12, 1980/1981.
T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, and Q. Liao. Why and When Can
Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality: A Review. In-
ternational Journal of Automation and Computing, 14(5):503–519, Oct. 2017. doi:
10.1007/s11633-017-1054-2.
B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential Expressivity
in Deep Neural Networks through Transient Chaos. In D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 29. Curran Associates, Inc., 2016.
M. Raissi, P. Perdikaris, and G. Karniadakis. Physics-Informed Neural Networks: A Deep
Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Par-
tial Differential Equations. Journal of Computational Physics, 378:686–707, Feb. 2019.
doi: 10.1016/j.jcp.2018.10.045.
J. W. Siegel and J. Xu. Approximation Rates for Neural Networks with General Activation
Functions. Neural Networks, 128:313–321, Aug. 2020. doi: 10.1016/j.neunet.2020.05.019.
J. W. Siegel and J. Xu. High-Order Approximation Rates forShallow Neural Networks with
Cosine and Relu Activation Functions. Applied and Computational Harmonic Analysis,
58:1–26, May 2022a. doi: 10.1016/j.acha.2021.12.005.
28J. W. Siegel and J. Xu. Sharp Bounds on the Approximation Rates, Metric Entropy, and
n-Widths of Shallow Neural Networks. Foundations of Computational Mathematics, Nov.
2022b. doi: 10.1007/s10208-022-09595-3.
J. W. Siegel and J. Xu. Characterization of the Variation Spaces Corresponding to
Shallow Neural Networks. Constructive Approximation, Feb. 2023. doi: 10.1007/
s00365-023-09626-4.
L. Tartar. An Introduction to Sobolev Spaces and Interpolation Spaces, volume 3 of Lecture
Notes of the Unione Matematica Italiana. Springer Berlin Heidelberg, Berlin, Heidelberg,
2007. ISBN 978-3-540-71482-8. doi: 10.1007/978-3-540-71483-5.
J. Toft, K. Johansson, S. Pilipović, and N. Teofanov. Sharp Convolution and Multiplication
Estimates in Weighted Spaces. Analysis and Applications, 13(05):457–480, Sept. 2015.
doi: 10.1142/S0219530514500523.
M.Unser. Ridges,NeuralNetworks,andtheRadonTransform. Journalof MachineLearning
Research, 24(37):1–33, 2023.
T. D. van Nuland. Noncompact Uniform Universal Approximation. Neural Networks, 173:
106181, May 2024. doi: 10.1016/j.neunet.2024.106181.
F. Voigtlaender. Lp sampling numbers for the Fourier-analytic Barron space.
arXiv:2208.07605, Aug. 2022.
M.-X. Wang and Y. Qu. Approximation Capabilities of Neural Networks on Unbounded
Domains. arXiv:1910.09293, 2019.
29