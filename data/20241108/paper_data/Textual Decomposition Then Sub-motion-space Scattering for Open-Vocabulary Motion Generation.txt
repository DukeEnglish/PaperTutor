Underreview
TEXTUAL DECOMPOSITION THEN SUB-MOTION-
SPACE SCATTERING FOR OPEN-VOCABULARY
MOTION GENERATION
KeFan JiangningZhang
ShanghaiJiaoTongUniversity TencentYoutuLab
RanYi JingyuGong
ShanghaiJiaoTongUniversity EastChinaNormalUniversity
YabiaoWang YatingWang
ZhejiangUniversity,TencentYoutuLab ShanghaiJiaoTongUniversity
XinTan ChengjieWang
EastChinaNormalUniversity ShanghaiJiaoTongUniversity,TencentYoutuLab
LizhuangMa
ShanghaiJiaoTongUniversity,EastChinaNormalUniversity
ABSTRACT
Text-to-motion generation is a crucial task in computer vision, which generates
the target 3D motion by the given text. The existing annotated datasets are lim-
ited in scale, resulting in most existing methods overfitting to the small datasets
and unable to generalize to the motions of the open domain. Some methods at-
tempt to solve the open-vocabulary motion generation problem by aligning to
the CLIP space or using the Pretrain-then-Finetuning paradigm. However, the
current annotated dataset’s limited scale only allows them to achieve mapping
from sub-text-space to sub-motion-space, instead of mapping between full-text-
space and full-motion-space (full mapping), which is the key to attaining open-
vocabulary motion generation. To this end, this paper proposes to leverage the
atomic motion (simple body part motions over a short time period) as an in-
termediate representation, and leverage two orderly coupled steps, i.e., Textual
Decomposition and Sub-motion-space Scattering, to address the full mapping
problem. For Textual Decomposition, we design a fine-grained description con-
version algorithm, and combine it with the generalization ability of a large lan-
guage model to convert any given motion text into atomic texts. Sub-motion-
spaceScatteringlearnsthecompositionalprocessfromatomicmotionstothetar-
get motions, to make the learned sub-motion-space scattered to form the full-
motion-space. For a given motion of the open domain, it transforms the ex-
trapolation into interpolation and thereby significantly improves generalization.
Our network, DSO-Net, combines textual decomposition and sub-motion-space
scatteringtosolvetheopen-vocabularymotiongeneration.Extensiveexperiments
demonstratethatourDSO-Netachievessignificantimprovementsoverthestate-
of-the-art methods on open-vocabulary motion generation. Code is available at
https://vankouf.github.io/DSONet/.
1 INTRODUCTION
1
4202
voN
6
]VC.sc[
1v97040.1142:viXraUnderreview
Text-to-motion(T2M)generation,aimingatgeneratingthe3Dtargetmotiondescribedbythegiven
text,isanimportanttaskincomputervisionandhasgarneredsignificantattentioninrecentresearch.
Itplaysacrucialroleinvariousapplications,suchasrobotics,animations,andfilmproduction.
Benefiting from advancements in GPT-
style(e.g.,LlamaGen(Sunetal.,2024))
text
anddiffusion-stylegenerativeparadigm atomic text
image
in text-to-image and text-to-video do- open text
motion
mains, some studies (Zhang et al., open motion
2023a; Jiang et al., 2023; Tevet et al., full-space
Simple Mapping CLIP-base Alignment
2022b; Shafir et al., 2023) have started sub-space
using these technologies to address the
T2M generation task. During the train-
LLM
ingprocess,pairedtext-motiondataare
utilized to align the text space with Pretrain-then-Finetuning Ours Textual Decomposition Sub-motion-space Scattering
the motion space. However, the open-
vocabularytext-to-motiongeneration
remains a challenging problem, requir- Figure 1: Compared with current text-to-motion
ing good motion generation quality for paradigms(Simplemapping,CLIP-basedalignment,and
unseenopen-vocaularytextatinference. Pretrain-then-Finetuning), our method proposes the tex-
Duetothelimitedscaleofrecenthigh- tual decomposition to decompose the raw motion text
quality annotated datasets (e.g., KIT- intoatomictextsandsub-motion-spacescatteringtolearn
ML (Plappert et al., 2016) and Hu- the composition process from atomic motions to tar-
manML3D(Guoetal.,2022)),asillus- get motions, which significantly improves the ability of
tratedinFig.1top-left,theSimpleMap- open-vocabularymotiongeneration.
ping paradigm only learns a mapping between a limited sub-text-space and a sub-motion-space,
ratherthanthemappingfromfull-text-spacetofull-motion-space. Consequently,generalizationto
unseenopen-vocabularytextisalmostimpossible.
To enhance the model’s generalization capabilities, two main strategies have been explored. As
shown in Fig. 1 top-right, the first paradigm is CLIP-based Alignment (e.g., MotionCLIP (Tevet
etal.,2022a)andOOHMG(Linetal.,2023)). Thiskindofapproachaimstoalignthemotionspace
withboththeCLIPtextspace(Radfordetal.,2021)andtheimagespace. Thecoreprocessinvolves
fittingthemotionskeletonontothemeshofthehumanbodySMPL(Loperetal.,2023)modeland
performingmulti-viewrenderingtoobtainposeimages,therebyachievingalignmentbetweenmo-
tionandimagespaces. ThesecondparadigmisPretrain-then-Finetuning(e.g.,OMG(Liangetal.,
2024a)),asillustratedinFig.1bottom-left. InspiredbythesuccessoftheStableDiffusion(Rom-
bachetal.,2022)modelinthetext-to-imagefield,thisparadigmfollowsapretrain-then-finetuning
process,alongwithscalingupthemodel,toenablegeneralizationtoopen-vocabularytext.
Although these two types of methods have achieved some progress in open-vocabulary text-to-
motiongeneration,theysufferfrominherentflaws: (1)TheCLIP-basedalignmentparadigmaligns
staticposeswiththeimagespace,whichresultsinthelossoftemporalinformationduringthelearn-
ing process. Consequently, this approach generates unrealistic motion. Furthermore, this method
overlooksthefeaturespacedifferencesbetweentheCLIPandT2Mtaskdatasets,potentiallyleading
to misalignment, unreliability, and inaccuracy in motion control. (2) Although the Pretrain-then-
Finetuningparadigmutilizesanadequatemotionpriorandexpandsthemotionspacebypretraining
onlarge-scalemotiondata,theannotatedpaireddatainthefinetuningstageisseverelylimited. The
significantimbalancebetweenlabeledandunlabeleddataresultsinthefine-tuningstageonlylearn-
ing the mapping from text to a condensed subspace of the full motion-space. Consequently, the
model has to perform extrapolation and has difficulties in generating motions that are outside the
subspacedistribution,asshowninFig.1.
Consequently, we conclude that the problem of insufficient generalization ability in current meth-
ods arises from the limited amount of high-quality labeled data and the inadequate utilization of
large-scaleunsupervisedmotiondataforpretraining. Existingmethodscanonlyestablishoverfitted
mappingswithinalimitedsubspace. Toachieveopen-vocabularymotiongeneration,itisessential
toestablishamappingfromthefull-text-spacetothefull-motion-space.
We observe that when understanding a motion, human beings tend to partition it into the combi-
nation of several simple body part motions (such as ”spine bend forward”, ”left hand up”) over a
2Underreview
shorttimeperiod,whichwedefineasatomicmotions. Alltheseatomicmotionsarecombinedspa-
tiallyandtemporallytoformtherawmotion. Thisobservationmotivatesustodecomposearaw
motionintoatomicmotionsandleverageatomicmotionsasanintermediaterepresentation. Since
rawmotiontextsoftencontainabstractandhigh-levelsemanticmeanings,directlyusingrawtexts
to guide motion generation can hinder the model’s ability to understand open-vocabulary motion
texts. Incontrast,atomicmotiontextsprovidealow-levelandconcretedescriptionofdifferentlimb
movements,whicharesharedacrossdifferentdomains.
Leveraging the atomic motions as an intermediate representation, we propose to address the full-
mapping problem through two orderly coupled steps: (1) Textual Decomposition. To enhance
generalization ability, we first design a textual decomposition process that converts a raw motion
textintoseveralatomicmotiontexts,subsequentlygeneratingmotionsfromtheseatomictexts. To
prepare training data, we develop a fine-grained description conversion algorithm to establish
atomictextsandmotionpairs. Specifically,wepartitiontheinputmotionintoseveraltimeperiods,
and describe the movements of each joint and spatial relationships from the aspects of velocity
(e.g., fast), magnitude (e.g., significant), and low-level behaviors (e.g., bending) for each period.
Thefine-graineddescriptionsandtherawtextaretheninputintoalargelanguagemodel(LLM)to
summarizetheatomicmotiontexts.Eachrawmotiontextisdecomposedintoatomicmotiontextsof
sixbodyparts: thespine,left/right-upper/lowerlimbs,andtrajectory. Bythisprocess,weguarantee
theconvertedatomicmotiontextsareconsistentwiththeactualmotionbehavior. Duringinference,
foranygiventext,weemploytheLLMtosplitthewholemotionintoseveralperiodsanddescribe
eachperiodusingatomicmotiontexts. Inthisway,weestablishamappingfromthefull-text-space
to the full-atomic-text-space. (2) Sub-motion-space Scattering. After obtaining the full-text-
spacetothefull-atomic-text-spacemappinginthefirststep, weaimtofurtherachievealignment
from the full-atomic-text-space to the full-motion-space, through the Sub-motion-space Scattering
step,therebyestablishingthemappingfromthefull-text-spacetothefull-motion-space. Giventhe
limitedlabeleddata,itisdifficultforthePretrain-then-Finetuningparadigmtolearnthefull-motion-
space,becauseitstrivialalignmentprocessonlylearnsamappingfromtexttoacondensedsubspace
(which we refer to as sub-motion-space), requiring extrapolation for out-of-domain motions. In
contrast, our approach scatters the sub-motion-space to form the full-motion-space, as shown in
thebottomrightofFig.1,transformingextrapolationintointerpolationandsignificantlyimproving
generalization. Thesub-motion-spacescatteringisachievedby learningthecombinationalprocess
ofatomicmotionstogeneratetargetmotions,withatext-motionalignment(TMA)moduletoextract
features for atomic motion texts, and a compositional feature fusion (CFF) module to fuse atomic
text features into motion features and learn the the combinational process from atomic motions to
targetmotions. AsshowninFig.1,Interpolatinganout-domainmotionisessentiallyacombination
ofseveralnearestclustersofscatteredsub-motion-space,whichishighlyconsistentwiththeprocess
ofCFFwedesign. Therefore,theCFFensuresforscatteringsub-motion-spacewelearned.
Overall, we adopt the discrete generative mask modeling and follow the pretrain-then-finetuning
pipeline for open-vocabulary motion generation. First, we pretrain a residual VQ-VAE (Martinez
etal.,2014)networkusingapre-processedlarge-scaleunlabeledmotiondataset,toenablethenet-
work to have prior knowledge of large-scale motion. For the fine-tuning stages, we first leverage
ourtextualdecompositionmoduletoconverttherawmotiontextintoatomictexts. Then,weutilize
both raw text and the atomic texts with our proposed TMA and CFF modules to train a text-to-
model generative model. Our network, abbreviated as DSO-Net, combines textual decomposition
and sub-motion-space scattering to solve the open-vocabulary motion generation. We conduct ex-
tensive experiments comparing our approach with previous state-of-the-art approaches on various
open-vocabularydatasetsandachieveasignificantimprovementquantitativelyandqualitatively.
Insummary,ourmaincontributionsinclude: (1)weproposetoleverageatomicmotionsasaninter-
mediate representation, and design textual decomposition and sub-motion-space scattering frame-
worktosolveopen-vocabularymotiongeneration. (2)Fortextualdecomposition,wedesignarule-
basedfine-graineddescriptionconversionalgorithmandcombineitwiththelargelanguagemodel
toobtaintheatomicmotiontextsforagivenmotion. (3)Forsub-motion-spacescattering,wepro-
posetoleverageatext-motionalignment(TMA)moduleandacompositionalfeaturefusion(CFF)
moduletolearnthegenerativecombinationofatomicmotions,therebysignificantlyimprovingthe
model’sgeneralizationability.
3Underreview
2 RELATED WORKS
2.1 TEXT-TO-MOTIONGENERATION
Text-to-Motion has been a long-standing concern. previous works (Guo et al., 2020; Petrovich
etal.,2021)usuallygenerateamotionbasedonthegivenactioncategories. Action2Motion(Guo
et al., 2020) uses a recurrent conditional variational autoencoder (VAE) for motion generation. It
useshistoricaldatatopredictsubsequentposturesandfollowstheconstraintsofactioncategories.
Subsequently, ACTOR (Petrovich et al., 2021) encodes the entire motion sequence into the latent
space, which significantly reduces the accumulated error. Using only action labels is not flexible
enough. Therefore, some works began to explore generating motion under text (i.e., natural lan-
guage). TEMOS(Petrovichetal.,2022)usesavariationalautoencoder(VAE)(Kingmaetal.,2019)
architecturetoestablishasharedlatentspaceformotionandtext. Thismodelalignsthetwodistri-
butions by minimizing the Kullback-Leibler (KL) divergence between the motion distribution and
the text distribution. Therefore, in the inference stage, only text input is needed as a condition to
generatethecorrespondingmotion. T2M(Guoetal.,2022)furtherlearnsatext-to-lengthestimator,
enabling the network to give the generated motion length automatically. T2M-GPT (Zhang et al.,
2023a) first introduces the VQ-VAE technique into text-to-motion tasks and leverages the autore-
gressiveparadigmtogeneratemotions. MotionGPT(Jiangetal.,2023;Zhangetal.,2024b)further
improves the motion quality under the autoregressive paradigm from the aspect of text encoder.
MDM(Tevetetal.,2022b)andMotionDiffuse(Zhangetal.,2022)arethefirstworkstosolvethe
motionsynthesistaskbyusingdiffusionmodels. Subsequentworks (Chenetal.,2023;Zouetal.,
2024; Zhang et al., 2023b; Dai et al., 2024; Zhang et al., 2024a; Karunratanakul et al., 2023; Xie
etal.,2023;Fanetal.,2024)furtherimprovethecontrollabilityandqualityofthegenerationresults
through some techniques such as database retrieval, spatial control, and fine-grained description.
However, all these methods essentially overfit the limited training data, thereby can not achieve
open-vocabularymotiongenerations.
2.2 OPEN-VOCABULARYGENERATION
Comparedtothepreviousmethodoftrainingandtestingonthesamedataset,theopen-vocabulary
generationtaskexpectstotrainononedatasetandtestontheother(out-domain)dataset.CLIP(Rad-
fordetal.,2021)ispre-trainedon400millionimage-textpairsusingthecontrastivelearningmethod
andhasstrongzero-shotgeneralizationability. Bycalculatingfeaturesimilaritywithagivenimage
andcandidatetextsinalist,itrealizesopen-vocabularyimage-texttasks. Therefore,onthisbasis,
manymethods inthefield oftext-to-imagegeneration, a series ofDiffusion-style-basedand GPT-
style-based methods (Rombach et al., 2022; Sun et al., 2024) are proposed to use CLIP to extract
featurestoimprovethegeneralizationabilityofthemodel. Basedonthis,MotionCLIP(Tevetetal.,
2022a) proposes to fit the motion data of the training set to the mesh of the human SMPL (Loper
et al., 2023) model in the preprocessing stage, thereby rendering multi-view static poses. In the
training stage, the motion features are aligned with the text and image features extracted by CLIP
simultaneously,therebyaligningthemotionfeaturestotheCLIPspaceandenhancingthegeneral-
ization ability of the model. AvatarCLIP (Hong et al., 2022) first synthesizes a key pose and then
matchesfromthedatabaseandfinallyoptimizestothetargetmotion. OOHGM(Linetal.,2023),
Make-An-Animation (Azadi et al., 2023), and PRO-Motion (Liu et al., 2023) first train a genera-
tive text-to-pose model with diverse text-pose pairs. Then, OOHGM further learns to reconstruct
full motion from the masked motion. Inspired by the success of AnimateDiff (Guo et al., 2023),
Make-An-Animationinsertsandfinetunesthetemporaladaptortoachievemotiongeneration. PRO-
Motionleveragesthelargelanguagemodeltogivethekey-posedescriptionsandsynthesizemotion
by a trained interpolation network. However, all these methods, aligning the static poses with the
imagespace, losethetemporalinformationduringthelearningandfinallyresultinthegeneration
of unrealistic motion. Recently, OMG (Liang et al., 2024a) try to use the successful paradigm,
pretrained-then-finetuning, intheLLMtoachieveopen-vocabulary. Therefore, itfirstpretraineda
un-condiditonaldiffusionmodelwithunannotatedmotiondata,thenfinetunesontheannotatedtext-
motion pairs by a ControlNet and MoE structure. However, due to the extremely limited labeled
data,thistypeofmethodcanultimatelyonlyachievetheeffectivemappingfromsub-text-spaceto
sub-motion-space,whichisstillfarfromsufficientforachievingopen-vocabularytasks.
4Underreview
Pretrain-then-Finetuning Textual Decomposition only in training stage
Paradigm TMA text motion alignment output motion feature
period 1 masked motion feature
spine bends forward
base/residual r ri ig gh …ht t u lop wp ee rr ll ii mm bb s fuli lg lyh t ely x tm eno dv se back TMA FC eao tm urp eo Fsi uti so in o n
codebook
period T
atomic text feature matrix
RVQ RVQ spine straighten briefly
Encoder Decoder right upper limb stretch outs Transformer Layer
right lower limb full extends atomic motion texts
…
raw text feaure ID
Mask
TMA
raw text: “Kick with right hand”
Fine-grained motion feature
Description Conversion
Sub-motion-space
Scattering
Figure2: Thearchitectureofourentireframework. Theoverallpipelineadoptsdiscretegener-
ativemodeling. 1)IntheMotionPre-Trainingstage(leftbluepart),weusetheResidualVQ-VAE
(RVQ)model,whichdesignsabaselayerandRresiduallayerstolearnlayer-wisecodebooks. By
tokenizing the motion sequence into multi-layer discrete tokens, we learn the large-scale motion
priors. 2) In the Motion Fine-tuning stage (right green part), we first leverage the large language
model(LLM)andthefine-graineddescriptionconversionalgorithmwedesign(onlyusedintrain-
ingstage)toperformtexutaldecomposition,whichconverttherawtextofamotionintotheatomic
texts. Then,forthebaselayerandresiduallayersinRVQ,weseparatelyusegenerativemaskmod-
elingandaneuralnetworkwithseveralTransformerlayerstolearnhowtopredictdiscretemotion
tokensaccordingtoagiventext. Furthermore,Wedesignatext-motionalignment(TMA)module
and a compositional feature fusion (CFF) module to learn the combinational process from atomic
motionstothetargetmotions.
2.3 GENERATIVEMASKMODELING
BERT (Devlin, 2018) as a very representative work in the field of natural language processing,
pretrainsatextencoderbyrandomlymaskingwordsandpredictingthesemaskedwords. Numerous
subsequent methods in the generative fields have borrowed this idea to achieve text-to-image or
video generation, such as MAGVIT (Yu et al., 2023), MAGE (Li et al., 2023), and Muse (Chang
etal.,2023). Comparedtoautoregressivemodeling,generativemaskedmodelinghastheadvantage
offasterinferencespeed. Inthemotionfield,MoMask(Guoetal.,2024)firstintroducedgenerative
masked modeling into the field of motion generation. It adopts a residual VQ-VAE (RVQ-VAE)
andrepresentshumanmotionasmulti-layerdiscretemotiontokenswithhigh-fidelitydetails. Inthe
trainingstage,themotiontokensofthebaselayerandtheresiduallayersarerandomlymaskedbya
maskingtransformerandpredictedaccordingtothetextinput. Inthegenerationstage,themasking
transformerstartsfromanemptysequenceanditerativelyfillsinthemissingtokens. Inthispaper,
ouroverallarchitecturealsoadoptsthesimilargenerativemaskedmodelingasMoMask(Guoetal.,
2024)toimplementourpretrain-then-finetuningstrategy.
3 METHOD
WeproposeanovelDSO-Netforopen-vocabularymotiongeneration,aimingtogeneratea3Dhu-
manmotionxfromanopen-vocabularytextualdescriptiondthatisunseeninthetrainingdataset.As
showninFig.2,ourframeworktakesthepretrain-then-finetuningparadigm,whichisfirstpretrained
on a large-scale unannotated motion dataset, and then finetuned on a small dataset of text-motion
pairs. AllthosemotionsareprocessedinaunifiedformatbyUniMocap(Chen&UniMocap,2023).
As analyzed before, the key to achieving open-vocabulary motion generation is to establish the
alignment between the full-text-space and the full-motion-space, which we call full-mapping. To
thisend,weleverageatomicmotionsasintermediaterepresentationsandconvertthefull-mapping
processintotwoorderlycoupledstages: 1)TextualDecomposition,and2)Sub-motion-spaceScat-
5Underreview
tering. The Textual Decomposition stage aims at converting any given motion text into several
atomic motion texts, each describing the motion of a simple body part over a short time period,
therebymappingthefull-text-spacetothefull-atomic-text-space. TheSub-motion-spaceScattering
stage is designed to learn the combinational process from atomic motions to the target motions,
whichscattersthesub-motion-spacelearnedfromlimitedpaireddatatoformthefull-motion-space,
viaatext-motionalignment(TMA)moduleandacompositionalfeaturefusion(CFF)module. The
scatteredsub-motion-spaceeventuallyimprovesthegeneralizationofourmotiongenerationability.
3.1 TEXTUALDECOMPOSITION
The textual decomposition is designed to convert any given motion text into several atomic mo-
tion texts (each describing the motion of a simple body part in a short time period). Different
fromtherawmotiontextsthatcontainabstractandhigh-levelsemanticmeanings,whichhinderthe
model’sabilitytounderstandopen-vocabularymotiontexts,atomicmotiontextsprovidealow-level
and concrete description of different limb movements, which are shared across different domains.
Therefore,weusethetextofatomicmotionsasanintermediaterepresentation,firstconvertingraw
motiontextsintoatomicmotiontexts,andthenlearningtheprocessofcombiningatomicmotions
togeneratetargetmotions.
First,weconstructatomicmotionandtextpairsfromalimitedrawmotion-textpaireddataset. Al-
thoughLargeLanguageModels(LLM)havethegeneralizationabilitytodescribeanygivenmotion
as an atomic motion, directly inputting a raw motion text into a large model will lead to a mis-
matchbetweenthegenerateddescriptionandtherealmotionbehavior,wheresomeworksalsosaid
before He et al. (2023); Shi et al. (2023). For this reason, we design a Fine-grained Description
Conversionalgorithmtodescribethemovementofeachjointandtherelativemovementrelation-
shipbetweenjointsinafine-grainedwayforagiven3Dmotion. Then,thisfine-graineddescription
and the raw motion text are input into the LLM to summarize it into the final atomic motion de-
scription. Theentirealgorithmdescribesthemovementofbodypartsintheinputmotionfromthree
aspects: speed, amplitude, and specific behavior; and divide the entire movement into at most P
timeperiods. Specifically,thisfine-graineddescriptionconversionconsistsoffoursteps:
PoseExtraction. Foreachframeinagiven3Dmotionx ,wecomputedifferentposedescriptors,
i
including the angle, orientation, and position of a single joint, and the distance between any two
joints. Taking the angle as an example, we can use three joint coordinates, J , J , and
shoulder elbow
J ,tocomputethebendingmagnitudeoftheupperlimb,whichisformulatedas:
wrist
J −J J −J
shoulder elbow ⊙ wrist elbow , (1)
||J −J || ||J −J ||
shoulder elbow wrist elbow
where⊙representstheinnerproduct. Foreachframeinamotion,wecomputeposedescriptorsfor
differentbodyparts.
Pose Aggregation. After obtaining the pose descriptors of each frame in a motion, we aggre-
gate adjacent frames into motion clips based on the pose descriptors, and obtain the descriptors
of the motion clips. Given the pose descriptors PD , PD , and PD of three consecu-
i−1 i i+1
tive frames, we first calculate the difference between two frames, ∆PD = PD − PD
i−1 i i−1
and ∆PD = PD − PD . We determine whether these three consecutive frames should
i i+1 i
be merged into a motion clip based on whether the signs of ∆PD and ∆PD are the same,
i i+1
i.e., both positive or both negative. In this way, we start from time i and continuously add the
{∆PD ,∆PD ,···}withthesamesignuntilthesignchanges,andtheresultofadditionisde-
i i+1
finedasS
=(cid:80)t=i+Ti∆PD
(T istheconsecutivetimelengthfromthestartingtimei),which
PDi t=i t i
representstheintensitychangeoftheposedescriptorduringthemotionclip. Wethencalculatethe
velocity as V
PDi
= |SP TD ii| , where |S PDi| is the absolute magnitude of S PDi. Finally, we obtain
theclipdescriptorforthemotionclipstartingfromPD ,whichisdefinedasthe(intensitychange,
i
velocity)pair: CD =(S ,V ).
PDi PDi PDi
ClipAggregation. Subsequently, weaggregatethemotionclipsbasedonthestarttimeofitsclip
descriptor. WeuniformlydivideamotionintoP binsintime,andputeachmotionclipintoabin
accordingtoitsstarttime. ThenumberofthebinsP issetempirically. Themotionclipsthatare
putinthesamebinwillberegardedasco-occurringmotionclips.
6Underreview
Description Conversion. We further classify the motion clips to some categories based on the
intensitychangeandvelocityintheclipdescriptor,andconvertitintothetextdescription. Forex-
ample,wefirstdeterminethebehaviorofamotionclipis“bending”or“extending”accordingtothe
clipdescriptorCD ofangle,wherethenegativeS means“bending”andviceversa. Then,
PDi PDi
when|S |exceedsathreshold,itwillbeclassifiedas“significant”;whilewhentheV isbelow
PDi PDi
somethreshold,itwillbeclassifiedas“slowly”. Finally,theconvertedtextis“Bending/Extending
significantlyslowly”.
Through this fine-grained description conversion algorithm, we first divide the given motion into
differentseveraltimeperiods(correspondingtoP bins),thenthemotionineachtimeperiodiscon-
vertedtoafine-graineddescriptioncomposedofsimplebehaviorsofbodyparts. Subsequently,we
inputallthesefine-graineddescriptionsandtherawtextofthecorrespondingmotionintotheLLM,
and make it perform simplifications to summarize L atomic texts for each period, where L is the
number of body parts, and each atomic text corresponds to a body part from spine, left/right up-
per/lowerlimbs,andtrajectory. AnexampleoftheatomicmotiontextsisshowninFig.2-right. The
atomic motion texts obtained by our algorithm are highly consistent with the real motion. During
theinference,foranygivenmotiontext,weprovidecorrespondingtextualdecompositionexamples
to ask the LLM to decompose the motion into several periods, and decompose each period into L
atomicmotiontextsdescribingsimplebodypartmovements.
3.2 SUB-MOTION-SPACESCATTERING
Asmentionedbefore, giventhelimitedpaireddata, wecanonlylearnasub-motion-space, i.e., a
condensedsubspaceofthefull-motion-space. Toenhancetheopen-vocabularygeneralizationabil-
ity,weproposetolearnthecombinationprocessfromatomicmotionstothetargetmotion,thereby
rearranging the sub-motion-space we learned in a much more scattered form. Although some tar-
getmotionsareout-of-distributionforthesub-motion-space,wescatterthesubspacetoformthe
full-motion-space,andconvertthegenerativeprocessfromextrapolationintointerpolation,thereby
significantlyenhancingthegenerativegeneralizationabilityofourmodel.
Specifically,toenableournetworktolearnascatteringsub-motion-space,weproposetoestablish
thecombinationprocessofatomicmotionsinsteadoflearningthetargetmotiondirectly,whichcon-
sistsoftwomainparts: (1)Text-MotionAlignment(TMA)and(2)CompositionalFeatureFusion
(CFF).
Text-Motion Alignment (TMA). Previous T2M generation methods usually leverage the CLIP
model to extract text features, and then align the text feature into the motion space through some
linear layers or attention layers during training. However, the CLIP method is trained on large-
scale text-image pair data, where the text is a description given for a static image, which has a
huge gap with the description of motion (including dynamic information over a time period). As
aresult,learningthealignmentduringtrainingbringsanextraburdenandseriouslyinterfereswith
ournetwork’sfocusonlearningthecombinationprocessfromatomicmotionstothetargetmotion.
Therefore, inspired by the text-motion retrieval method TMR (Petrovich et al., 2023), we first use
the contrastive learning method to pretrain a text feature extractor TMA on text-motion pair data.
ComparedtotheCLIPencodertrainedontext-imagepairdata,ourTMAistrainedontext-motion
pairdata,whichbetteralignsthetextfeaturestothemotionspace.Specifically, weusetheInfoNCE
lossfunctiontopullthepositivepair(x+,d+)(amotionanditscorrespondingtextualdescription)
closer, and push the negative pair (x+,d−) (a motion and another textual description) away, to
ensure that the text features are aligned with the motion space. For M positive pairs, the loss
functionisdefinedasfollows:
(cid:32) (cid:33)
L =−
1 (cid:88)
log
expA ii/τ
+log
expA ii/τ
, (2)
NCE 2M (cid:80) expA /τ (cid:80) expA /τ
i j ij j ji
whereA =(m+,d+),andτ isthetemperaturehyperparameter. Subsequently,asshowninFig.2,
ij
we use TMA as our text feature extractor for both the raw texts and decomposed atomic texts.
All these atomic text features are input to our next compositional feature fusion (CFF) module to
guidethemotiongeneration. Inthisway,wegreatlyreducetheinterferencecausedbytext-motion
misalignmentduringtheatomicmotioncombinationlearningprocess.
7Underreview
masked motion feature updated motion feature
1 1 2
� �
2 output motion feature
S R L ��
Q
1 2 N
CA
1 2 L 1 2 P
K, V
atomic text matrix TMA L
2
AT11 AT12 AT13AT1P
1 atomic text feature
AT21 AT22 AT23AT2P
S: split operation R: reshape operation CA: cross-attentio�n
ATL1 ATL2 ATL3ATLP N: frame number L: atomic number P: motion period number
Figure 3: Detailsof the compositional feature fusion (CFF)module, where the atomic text matrix
isinputintotheTMAmoduleforfeatureextraction,andisfusedwiththemotionfeaturebycross-
attention.
OverallArchitectureofMotionGenerativeModel. Ourmotiongenerativenetworkcontainstwo
differentgenerativemodels,onecorrespondstothebaselayerofResidualVQ-VAE(RVQ),andthe
othercorrespondstotheresiduallayers. AsdetailedinFig.2, eachgenerativemodel, eitherbase-
layer or residual-layer, contains sequentially stacked transformer layer and Compositional Feature
Fusion(CFF),i.e.,1transformerlayerfollowedby1CFFmodule,repeatedforK times. Different
residuallayerssharethesameparameters,buthavedifferentinputindicators,i.e.,V residuallayers
correspondtoindicator1toV. Sincetheindicatorsaretheminordifferencebetweenthebaseand
theresiduallayergenerativemodel,weomititforconvenience.
GivenamotionxofF motionframes,wefirstsampleitwithratior,andencodeeachof F down-
r
sampled frames with the motion encoder in RVQ. We then perform quantization by mapping the
encodedfeaturestothecodeindicesoftheirnearestcodesinthecodebookofbase/residuallayers,
denoted as I = [I ,I ,...I ], where N = F. Then, the code indices I is first converted into a
1 2 N r
one-hotembedding,andthenmappedintoamotionembeddingm = [m 1,m 2,...m N] ∈ RN×Dm
bylinearlayers,whichistakenastheinitialinputtothegenerativemodels,whereD isthechannel
m
dimension.
Givenatext,andtheatomicdescription(aL×P textmatrix),weencodetherawtextandtheatomic
textsbyourTMAtextencoder,andtheoutputsare: 1)rawtextfeatureT
r
∈RDT,whereD
T
isthe
channeldimension;and2)atomictextfeatureW ∈ RL×P×DW,whereListhenumberofatomic,
P isthenumberofmotionperiods,andD isthechanneldimension.
W
Themotionembeddingmisfirstrandomlymaskedoutwithavaryingratio,byreplacingsometo-
kenswithaspecial[MASK]token. Subsequently,themaskedembeddingm˜ = [m˜ ,m˜ ,...m˜ ]is
1 2 N
combinedwiththerawtextfeatureT ,whichistheninputintoatransformerlayerF .
r Transformer
The outputs are refined raw text feature and refined motion feature, denoted as To and m˜1 =
r
[m˜1,m˜1,...m˜1 ]∈RN×Dm,whichisformulatedas:
1 2 N
To,m˜1 =F (T ;m˜). (3)
r Transformer r
Thetransformerlayerenablestheoutputm˜1 tointegratebothglobalinformationandthetemporal
relationship.
Compositional Feature Fusion (CFF). The CFF module is designed to fuse atomic text features
intomotionfeatures,andguidethemodeltolearnthecombinationalprocessfromatomicmotionsto
thetargetmotions. AsshowninFig.3,weutilizethecross-attentionmechanismtofusetheatomic
motiontextfeatureW intothemotionfeatureinaspatialcombinationmanner.Specifically,wesplit
therefinedmotion featurem˜1 into L partsalongthe channeldimension(L isthenumber ofbody
parts),andreshapethesplittedmotionfeatureandinputitintoalinearlayertoobtaintheupdated
motionembeddingm˜2 ∈ RL×N×DW. Then,them˜2 istakenastheQuery,andthetheatomictext
feature W is taken as the Key and the Value to conduct the cross-attention calculation. Since the
atomic text feature W is extracted by our TMA model, which has aligned the text space with the
motionspace, theoutputmotionfeatureofthecross-attentionm˜3 ∈ RL×N×DW couldcomposite
8Underreview
theatomicmotionsexplicitlyanddirectly. TheoverallprocessofCFFmoduleisformulatedas:
m˜3 =F (m˜2;W). (4)
CFF
Eventually, the m˜3 goes through a linear layer and is reshaped to the final output motion feature
m˜o ∈RN×Dm. Them˜o isthencombinedwiththerefinedrawtextfeatureTo andinputtothenext
r
transformerlayerandtheCFFmodule. ThetransformerlayerandtheCFFmodulearesequentially
stackedforK times. TheoutputmotionfeatureofthefinalCFFmoduleisinputintoaclassification
headandpunishedbyacross-entropyloss.
3.3 INFERENCEPROCESS
Duringtheinferencestage,forgenerativemodelsofthebaselayerandRresiduallayers,weinitial-
izeallmotiontokensas[MASK]tokens. Duringeachinferencestep,wesimultaneouslypredictall
maskedmotiontokens,conditionedonboththerawmotiontextandtheatomicmotiontextsusing
in-contextlearning. Ourgenerativemodelsfirstpredicttheprobabilitydistributionoftokensatthe
maskedlocations,andsamplemotiontokensaccordingtotheprobability. Thenthesampledtokens
withthelowerconfidencesaremaskedagainforthenextiteration. Finally,allthepredictedtokens
aredecodedbacktomotionsequencesbytheRVQdecoder.
4 EXPERIMENTS
4.1 EXPERIMENTSETUP
DatasetDescription. Inthepre-trainingstage,weutilizevariouspubliclyavailablehumanmotion
datasets, including MotionX (Lin et al., 2024), MoYo (Tripathi et al., 2023), InterHuman (Liang
etal.,2024b),KIT-ML(Plappertetal.,2016),totalingover22Mframes. Inthesubsequentfinetun-
ingstage,wetrainourgenerativemotionmodelusingthetext-motionHumanML3Ddataset. Dur-
ingInference,weexperimentonthreedatasets,whereoneisthein-domaindataset(HumanML3D)
whiletheothertwoaretheout-domaindatasets(Idea400andMixamo). Idea400isthemonocular
dataset consisting of 400 daily motions, while the Mixamo includes various artist-created anima-
tions. Thedatasetsusedinpre-trainingonlycontainmotiondataanddonotcontaintextualdescrip-
tions,whilethedatasetsusedinfinetuningandinferencecontainmotiondataandannotatedtextual
descriptions. EvaluationMetricsandImplementationDetailsareintroducedintheappendix.
4.2 COMPARISON
Wefirstcompareourapproachquantitativelywithvariousstate-of-the-artmethods.FromtheTab.1,
itcanbeseenthatourmethodsignificantlyoutperformstheothermethodsonthetwoout-domain
datasets(Idea400andMixamo)andalsoachievescomparableresultsonthein-domaindatasets(Hu-
manML3D).AsshowninFig.4,comparedwithotherrepresentativemethods,thequalitativegener-
ationsofourmodelaremuchmoreconsistentwiththeopen-vocabularytexts. Boththequantitative
and qualitative results fully illustrate that our two orderly coupled designs (textual decomposition
andsub-motion-spacescattering)enablethemodelnottooverfitthedistributionofalimiteddataset
buttopossessstronggeneralizationability. Pleasechecktheappendixandthedemovideoformore
visualizationresultsongeneratedopenvocabularymotion.
HumanML3D Idea400 Mixamo
Method
FID↓ R-Prescion↑ Diversity↑ FID↓ R-Prescion↑ Diversity↑ FID↓ R-Prescion↑ Diversity↑
MDM 0.061±.001 0.817±.005 1.380±.004 0.821±.003 0.286±.006 1.329±.006 0.211±.002 0.380±.005 1.352±.005
T2M-GPT 0.013±.001 0.833±.002 1.382±.004 0.934±.002 0.314±.004 1.330±.004 0.221±.002 0.389±.004 1.350±.004
MoMask 0.011±.001 0.878±.002 1.390±.004 0.880±.001 0.340±.005 1.340±.004 0.209±.001 0.415±.005 1.346±.005
MotionCLIP 0.082±.002 0.331±.002 1.281±.004 1.112±.002 0.237±.005 1.195±.007 0.300±.002 0.228±.004 1.176±.005
Ours 0.027±.002 0.957±.002 1.388±.004 0.847±.001 0.703±.004 1.338±.004 0.186±.001 0.807±.004 1.360±.004
Table1: Comparisonwithstate-of-the-artsononein-domaindataset(HumanML3D)andtwoout-
domaindataset(Idea400andMixamo).
9Underreview
Capoeira Cartwheel To Roll Escape Dying Shot To Back Of Head Falling On Knees
gt
mdm
momask
t2m-gpt
ours
Figure4: Comparisonwithseveralstate-of-the-artsonopenvocabularytexts.
R-Precision↑
Methods FID↓ Diversity↑
R-Top1 R-Top2 R-Top3
Baseline 0.898±.002 0.160±.004 0.251±.005 0.314±.004 1.342±.005
Baseline+Pretrain 0.890±.002 0.162±.003 0.256±.005 0.323±.004 1.340±.005
Baseline+Pretrain+CFF 0.886±.002 0.170±.004 0.266±.004 0.337±.006 1.333±.006
Baseline+Pretrain+TMA 0.844±.002 0.380±.005 0.539±.005 0.630±.006 1.346±.004
Baseline+Pretrain+TMA+CFF 0.847±.001 0.449±.006 0.613±.004 0.703±.004 1.338±.004
Table 2: Ablation Study on the Idea400 dataset. The TMA and CFF represent the text-motion-
alignmentmoduleandthecompositionalfeaturefusionmodule.
4.3 ABLATIONSTUDY
To examine the specific function of each module in our novel DSO-Net, we conduct a series of
ablationstudiesfocusingontheeffectofpre-trainingonlarge-scalemotiondata,theeffectoftextual
decomposition,theeffectoftext-motionalignment,andtheeffectofcompositionalfeaturefusion.
EffectofPretrainingonLarge-scaleMotionData. Asweanalyzedbefore,foropen-vocabulary
motiongeneration,weneedtoestablishthemappingbetweentext-full-spaceandmotion-full-space.
Therefore,toenlargethemotionspacecontainedinourmodel,weleveragethelarge-scaleunanno-
tatedmotiondatatopre-trainaresidualvq-vae. AsillustratedinthesecondrowinTab.2,wegain
a1%and2%improvementinFIDandR-Top3,whichmeansenlargingthemotionspacelearnedin
themodelisuseful.
Effect of Compositional Feature Fusion (CFF). By generally comparing the results of the third
row and the second row, as well as the results of the last row and the fourth row, we can find
that through the CFF module we designed, the consistency between the generated motion and the
out-of-domain motion distributions(FID) and the similarity with their input text (R-Precision) on
out-of-domain datasets are significantly improved. This fully proves that by splitting the motion
feature channels and explicitly injecting the combination of atomic motions into the motion gen-
erationprocess,wecanlearnthecombinationprocessfromatomicmotionstotargetmotionswell
andscatterthesub-motion-spacewelearned. Eventually,thescatteredsub-motion-spacecaneffec-
10Underreview
tively convert the extrapolation generation process of out-of-domain motion into an interpolation
generationprocess,therebysignificantlyimprovingthegeneralizationofthemodel.
EffectofTextMotionAlignment(TMA).AsshownintheTab.2,afterleveragingthetextencoder
ofTMA(row4th),R-top3almostdoublescomparedtothesecondrow.Furthermore,wecanseethat
usingtheCFFmoduleontopofTMA,R-top3increasesby11%(4throwv.s. 5throw), whilethe
CFFmodulewithoutTMAincreasesbyonly3%(2ndrowv.s. 3rdrow). Bothresultsdemonstrate
thetextfeaturealignedwithmotionspaceindeedreleasetheburdenoflearningtheatomicmotion
compositionprocess.
5 CONCLUSION
In this paper, we propose our DSO-Net, i.e. Texutal Decomposition then Sub-motion-space
Scattering,tosolvetheopen-vocabularymotiongenerationproblem. Textualdecompositionisfirst
leveragedtoconverttheinputrawtextintoatomicmotiondescriptions,whichserveasourinterme-
diaterepresentations.Then,welearnedthecombinationprocessfromintermediateatomicmotionto
thetargetmotion,whichsubsequentlyscatteringthesub-motion-spacewelearnedandtransforming
extrapolation into interpolation and significantly improve generalization. Numerous experiments
areconductedtocompareourapproachwithpreviousstate-of-the-artapproachesonvariousopen-
vocabularydatasetsandachieveasignificantimprovementquantitativelyandqualitatively.
REFERENCES
SamanehAzadi,AkbarShah,ThomasHayes,DeviParikh,andSonalGupta. Make-an-animation:
Large-scale text-conditional 3d human motion generation. In Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision,pp.15039–15048,2023.
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan
Yang,KevinMurphy,WilliamTFreeman,MichaelRubinstein,etal. Muse: Text-to-imagegen-
erationviamaskedgenerativetransformers. arXivpreprintarXiv:2301.00704,2023.
Ling-Hao Chen and Contributors UniMocap. Unimocap: Unifier for babel, humanml3d, and kit.
https://github.com/LinghaoChan/UniMoCap,2023.
Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your
commandsviamotiondiffusioninlatentspace. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.18000–18010,2023.
Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Mo-
tionlcm: Real-time controllable motion generation via latent consistency model. arXiv preprint
arXiv:2404.19759,2024.
Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding.
arXivpreprintarXiv:1810.04805,2018.
Ke Fan, Junshu Tang, Weijian Cao, Ran Yi, Moran Li, Jingyu Gong, Jiangning Zhang, Yabiao
Wang, Chengjie Wang, and Lizhuang Ma. Freemotion: A unified framework for number-free
text-to-motionsynthesis. arXivpreprintarXiv:2405.15763,2024.
ChuanGuo,XinxinZuo,SenWang,ShihaoZou,QingyaoSun,AnnanDeng,MinglunGong,and
LiCheng. Action2motion: Conditionedgenerationof3dhumanmotions. InProceedingsofthe
28thACMInternationalConferenceonMultimedia,pp.2021–2029,2020.
Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating
diverseandnatural3dhumanmotionsfromtext. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.5152–5161,2022.
Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Gener-
ative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pp.1900–1910,2024.
11Underreview
Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh
Agrawala,DahuaLin,andBoDai. Animatediff: Animateyourpersonalizedtext-to-imagediffu-
sionmodelswithoutspecifictuning. arXivpreprintarXiv:2307.04725,2023.
Xin He, Shaoli Huang, Xiaohang Zhan, Chao Wen, and Ying Shan. Semanticboost: Elevating
motiongenerationwithaugmentedtextualcues. arXivpreprintarXiv:2310.20323,2023.
FangzhouHong,MingyuanZhang,LiangPan,ZhongangCai,LeiYang,andZiweiLiu. Avatarclip:
Zero-shottext-drivengenerationandanimationof3davatars. arXivpreprintarXiv:2205.08535,
2022.
BiaoJiang,XinChen,WenLiu,JingyiYu,GangYu,andTaoChen. Motiongpt:Humanmotionasa
foreignlanguage. AdvancesinNeuralInformationProcessingSystems,36:20067–20079,2023.
Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. Guided
motion diffusion for controllable human motion synthesis. In Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision,pp.2151–2162,2023.
DiederikPKingma,MaxWelling,etal. Anintroductiontovariationalautoencoders. Foundations
andTrends®inMachineLearning,12(4):307–392,2019.
TianhongLi, HuiwenChang, ShlokMishra, HanZhang, DinaKatabi, andDilipKrishnan. Mage:
Maskedgenerativeencodertounifyrepresentationlearningandimagesynthesis. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.2142–2152,2023.
Han Liang, Jiacheng Bao, Ruichi Zhang, Sihan Ren, Yuecheng Xu, Sibei Yang, Xin Chen, Jingyi
Yu, andLanXu. Omg: Towardsopen-vocabularymotiongenerationviamixtureofcontrollers.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
482–493,2024a.
HanLiang,WenqianZhang,WenxuanLi,JingyiYu,andLanXu. Intergen: Diffusion-basedmulti-
humanmotiongenerationundercomplexinteractions. InternationalJournalofComputerVision,
pp.1–21,2024b.
Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang.
Motion-x: A large-scale 3d expressive whole-body human motion dataset. Advances in Neural
InformationProcessingSystems,36,2024.
Junfan Lin, Jianlong Chang, Lingbo Liu, Guanbin Li, Liang Lin, Qi Tian, and Chang-wen Chen.
Beingcomesfromnot-being: Open-vocabularytext-to-motiongenerationwithwordlesstraining.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.
23222–23231,2023.
JinpengLiu,WenxunDai,ChunyuWang,YijiCheng,YansongTang,andXinTong. Plan,posture
andgo: Towardsopen-worldtext-to-motiongeneration. arXivpreprintarXiv:2312.14828,2023.
MatthewLoper,NaureenMahmood,JavierRomero,GerardPons-Moll,andMichaelJBlack.Smpl:
A skinned multi-person linear model. In Seminal Graphics Papers: Pushing the Boundaries,
Volume2,pp.851–866.AssociationforComputingMachineryNewYorkNYUnitedStates,2023.
JulietaMartinez, HolgerHHoos, andJamesJLittle. Stackedquantizersforcompositionalvector
compression. arXivpreprintarXiv:1411.2173,2014.
MathisPetrovich,MichaelJBlack,andGu¨lVarol. Action-conditioned3dhumanmotionsynthesis
with transformer vae. In Proceedings of the IEEE/CVF International Conference on Computer
Vision,pp.10985–10995,2021.
MathisPetrovich,MichaelJBlack,andGu¨lVarol. Temos:Generatingdiversehumanmotionsfrom
textualdescriptions. InEuropeanConferenceonComputerVision,pp.480–497.Springer,2022.
Mathis Petrovich, Michael J Black, and Gu¨l Varol. Tmr: Text-to-motion retrieval using con-
trastive3dhumanmotionsynthesis. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pp.9488–9497,2023.
12Underreview
Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big
data,4(4):236–252,2016.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pp.10684–10695,2022.
YonatanShafir,GuyTevet,RoyKapon,andAmitHBermano. Humanmotiondiffusionasagener-
ativeprior. arXivpreprintarXiv:2303.01418,2023.
XuShi,ChuanchenLuo,JunranPeng,HongwenZhang,andYunlianSun. Generatingfine-grained
humanmotionsusingchatgpt-refineddescriptions. arXivpreprintarXiv:2312.02772,2023.
Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.
Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint
arXiv:2406.06525,2024.
Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Ex-
posinghumanmotiongenerationtoclipspace. InEuropeanConferenceonComputerVision,pp.
358–374.Springer,2022a.
Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H. Bermano.
Humanmotiondiffusionmodel,2022b. URLhttps://arxiv.org/abs/2209.14916.
ShashankTripathi,LeaMu¨ller,Chun-HaoP.Huang,TaheriOmid,MichaelJ.Black,andDimitrios
Tzionas. 3Dhumanposeestimationviaintuitivephysics. InConferenceonComputerVisionand
PatternRecognition(CVPR),pp.4713–4725, 2023. URLhttps://ipman.is.tue.mpg.
de.
YimingXie,VarunJampani,LeiZhong,DeqingSun,andHuaizuJiang. Omnicontrol: Controlany
jointatanytimeforhumanmotiongeneration. arXivpreprintarXiv:2310.08580,2023.
Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose´ Lezama, Han Zhang, Huiwen Chang, Alexander G
Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video
transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.10459–10469,2023.
Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu,
Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete
representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition,pp.14730–14740,2023a.
Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei
Liu. Motiondiffuse: Text-drivenhumanmotiongenerationwithdiffusionmodel. arXivpreprint
arXiv:2208.15001,2022.
MingyuanZhang,XinyingGuo,LiangPan,ZhongangCai,FangzhouHong,HuirongLi,LeiYang,
and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of
theIEEE/CVFInternationalConferenceonComputerVision,pp.364–373,2023b.
Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, and Ziwei Liu. Finemogen:
Fine-grained spatio-temporal motion generation and editing. Advances in Neural Information
ProcessingSystems,36,2024a.
YaqiZhang,DiHuang,BinLiu,ShixiangTang,YanLu,LuChen,LeiBai,QiChu,NenghaiYu,and
WanliOuyang.Motiongpt:Finetunedllmsaregeneral-purposemotiongenerators.InProceedings
oftheAAAIConferenceonArtificialIntelligence,volume38,pp.7368–7376,2024b.
QiranZou,ShangyuanYuan,ShianDu,YuWang,ChangLiu,YiXu,JieChen,andXiangyangJi.
Parco: Part-coordinatingtext-to-motionsynthesis. arXivpreprintarXiv:2403.18512,2024.
13Underreview
Table3: ThepromptsusedintheLLMforobtainingtheatomicmotiontexts
systemprompt:Iwouldlikeyoutoplaytheroleofakinesiologyexperttoassistmeinaccuratelydescribinganmotion.
#CONTEXT#
Iwillprovideyouwithadescriptionofanindividual’smotion.Eachdescriptionencompassesinformationregardingtheactionsoftheperson.
Theactionsmightbedescribedtooabstractlyorcoarsely.Irequireyoutofurnishmewithadetailedaccountofthemotionbasedonyourkinesiologyexpertiseandthesubsequentinstructions.
Iexpectyouto:
(1)Segregatethisactionintoseveraldistinctstages.
(2)Foreachstage,provideadetaileddescriptionofthefollowingbodypartsforeachindividual.
Thebodypartsshouldinclude[”spine”,”leftupperlimb”,”rightupperlimb”,”leftlowerlimb”,”rightlowerlimb”,”trajectory”].
(3)Therulesandoutputrequirementsarelistedbelow.Pleaseadheretothemtoaccomplishthetask.
(4)Ihaveprovidedyouwithsomeexamplestofacilitateyourcomprehensionofthetask.Kindlyreviewthembeforecommencingthetask.
Theoutputmethodshouldbestrictlyintheformasintheexample,andforthedescriptionmethodsofdifferentstagebodyparts,pleaserefertotheexample.
#RULES#
(1)Avoidusinguncertainwordslike”may”inthesplitstatement.Also,refrainfromusingwordssuchas”also”,”too”inthesplitstatement.
(2)Theoutputdescriptionshouldbephysicallyplausible,
Thebehaviorofeachbodypartmustbecapableofreflectingthecomprehensive.
#OUTPUTREQUIREMENTS#
(1)ReturnFormat:JSON
(2)Pleasefollowtheformatoftheexamplebelowtoreturntheoutput,don’toutputotherinformation.
#Examples#
Example1:
<input>hestompshisleftfeet</input>
<output>{
”0”:{
”spine”:”remainsrelativelystableasthemotioninitiates”,
”leftupperlimb”:”leftarmmovesdownslightly”,
”rightupperlimb”:”nosignificantmovement”,
”leftlowerlimb”:”lefthipshiftspreparatorytostomp,anklebeginstoflex”,
”rightlowerlimb”:”stationary”,
”trajectory”:”preparingforstompingaction”
},
”1”:{...(×6)
},...
...
Example15:
...
A APPENDIX
In the following, we first provide additional implementation details. Then we introduce the large
languagemodel(LLM)foratomicmotiontextwherethepromptsandtherawmotiontextsareinput
toanLLMsimultaneouslytoobtaintheatomicmotiontextsduringinference.Finally,weshowmore
qualitativecomparisonsagainstpreviousstate-of-the-artonopen-vocabularymotiongeneration.
B ADDITIONAL IMPLEMENTATION DETAILS
EvaluationMetrics. Weevaluateourmodel’sperformancewiththreecommonlyusedmetrics: (1)
FrechetInceptionDistance(FID),whichevaluatesthesimilarityoffeaturedistributionsbetweenthe
generatedandrealmotions. (2)Motion-retrievalprecision(R-Precision),whichcalculatesthetext
andmotionmatchingaccuracy. (3)Diversity,whichmeasuringlatentvariance.
ImplementationDetails. Allthoseexperimentsarerunon4Tesla-V100GPU.Forthepre-traning
stage, we use 1 base layer and 5 residual layers for our residual VQ-VAE. The pre-traning epoch
is 100, and the corresponding learning rate and batch size on each GPU are 2e-4 and 512. The
codebooksizeanddownsampleratioare512and4. Forthefine-tuningstage,wetrainthegenera-
tivemodelsforbaseandresiduallayersrespectively. Allresiduallayersaresharedwiththename
parametersinthegenerativemodels,andonlydistinctfromeachotherwiththedifferentlayerID.
Thetrainingepochandlearningrateforbothgenerativemodelsare500and2e-4,andthebatchsize
foreachGPUis64.
C LLM FOR ATOMIC MOTION TEXT
Weusein-contextlearningtoguidetheLLMtodecomposethegivenrawtextaccordingtothegiven
examplesto obtainatomicmotion texts. Theexamples are15converted resultsobtainedfrom the
training set. We ask the LLM to split the given raw text according to the examples, where each
raw text should be split into several time periods, and each period contains the six atomic (spine,
left/right-upper/lowerlimbs,andtrajectory)motions. ThespecificpromptsareshowninTab.3.
D MORE QUALITATIVE COMPARISONS
AsshowninFig.5andFig.6,ourmethodssignificantlyoutperformtheotherstate-of-the-artresults.
Takethe”StandingtoKneelingDown”asanexample,Allothermethodsdonotunderstandthetime
14Underreview
Standing To Kneeling Down Getting Rocked By A Big Uppercut
gt
mdm
momask
t2m-gpt
ours
Figure5: Qualitativeresultscomparedwithpreviousstate-of-the-arts.
sequence of the two motions (standing and kneeling). Only our method meets the time sequence
requirements of motion. Textual Decomposition and Sub-motion-space Scattering are helpful for
ustopromotemotionperformancefortheopen-vocabularytext. Morevisualizationresultsarein
thedemovideo.
15Underreview
Lifting_Objects_Over_Head_And_Walk Cheer_Up_And_Walking_At_The_Same_Time
gt
mdm
momask
t2m-gpt
ours
Figure6: Qualitativeresultscomparedwithpreviousstate-of-the-arts.
16