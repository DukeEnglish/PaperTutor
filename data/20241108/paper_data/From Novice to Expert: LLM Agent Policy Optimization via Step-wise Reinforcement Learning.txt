From Novice to Expert: LLM Agent Policy Optimization via
Step-wise Reinforcement Learning
ZhiruiDeng YutaoZhu RuibinXiong
ZhichengDou Ji-RongWen MangWang
zrdeng@ruc.edu.cn yutaozhu94@gmail.com WeipengChen
dou@ruc.edu.cn jrwen@ruc.edu.cn xiongruibin18@mails.ucas.ac.cn
GaolingSchoolofArtificial GaolingSchoolofArtificial songmu@baichuan-inc.com
Intelligence Intelligence chenweipeng@baichuan-inc.com
RenminUniversityofChina RenminUniversityofChina BaichuanIntelligentTechnology
Beijing,China Beijing,China Beijing,China
Abstract OptimizationviaStep-wiseReinforcementLearning.InProceedingsofThe
Theoutstandingcapabilitiesoflargelanguagemodels(LLMs)ren- WebConference(WWW’25).ACM,NewYork,NY,USA,12pages.https:
//doi.org/XXXXXXX.XXXXXXX
derthemacrucialcomponentinvariousautonomousagentsystems.
Whiletraditionalmethodsdependontheinherentknowledgeof
LLMswithoutfine-tuning,morerecentapproacheshaveshiftedto-
1 Introduction
wardthereinforcementlearningstrategytofurtherenhanceagents’
abilitytosolvecomplexinteractivetaskswithenvironmentsand Largelanguagemodels(LLMs)havebegunarevolutionaryerain
tools.However,previousapproachesareconstrainedbythesparse artificialgeneralintelligence(AGI),duetotheirremarkablecapa-
rewardissue,whereexistingdatasetssolelyprovideafinalscalar bilitiesinhandlingcomplexinteractivetaskswithenvironments
rewardforeachmulti-stepreasoningchain,potentiallyleadingto andtools[42,46].Thetasksinvolvemultipleareasincludingweb
ineffectivenessandinefficiencyinpolicylearning.Inthispaper, browsing[12],webshopping[48],householding[35],andcomplex
weintroduceStepAgent,whichutilizesstep-wiserewardtoop- questionanswering[15,40,47].Althoughthesemodels(e.g.,Chat-
timizetheagent’sreinforcementlearningprocess.Inheritingthe GPT[24]andGPT-4[25])areendowedwithextensiveknowledge
spiritofnovice-to-experttheory,wefirstcomparetheactionsof duringpre-trainingonalarge-scalecorpus,theydemonstratea
theexpertandtheagenttoautomaticallygenerateintermediate tendencytogeneratehallucinatedcontent[21,54].Totacklethis
rewardsforfine-grainedoptimization.Additionally,wepropose issueandfurtheralignwithhumanpreferences,researchershave
implicit-rewardandinversereinforcementlearningtechniquesto introducedtrainingLLMagentswithreinforcementlearning(RL)to
facilitateagentreflectionandpolicyadjustment.Furthertheoretical enhancetheirabilityforcomplicatedtaskplanningandresolving.
analysisdemonstratesthattheactiondistributionoftheagentcan InitialeffortsindevelopingLLMagents[26,28,38,55]concen-
convergetowardtheexpertactiondistributionovermultipletrain- trated on maximizing the token-level generation probability of
ingcycles.Experimentalresultsacrossvariousdatasetsindicate theexpertactions,denotedinFigure1(a).Thesemethods,while
thatStepAgentoutperformsexistingbaselinemethods. straightforwardandreward-free,fallshortwhenconfrontedwith
thetrainingdatashortagesituationandstruggletogeneralizebe-
yondthetrainingdatadistribution.Recognizingtheseconstraints,
CCSConcepts
researchers[2,11,36,37,57]haveshiftedtowardsleveragingman-
• Computing methodologies → Planning and scheduling;
uallyannotatedpreferencesorthefinalenvironmentfeedbackas
Reinforcementlearning;Inversereinforcementlearning.
additionalrewardsignalsandconductingreinforcementlearning
trainingonthebasisofthesupervisedfine-tuning(SFT)model.
Keywords
Nevertheless,thesemethodsarerestrictedbythesparsityandde-
LLMAgentPlanning,ReinforcementLearning,Process-Reward
layoftherewardsignals.AsshowninFigure1(b),existingreward
Optimization
signalsarerepresentedasasinglescalarrewardforeachgener-
atedobservation-actiontrajectory.Suchsparsefeedbackrenders
ACMReferenceFormat:
ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,Mang itchallengingforthemodeltodiscernthequalityofeachaction,
Wang,andWeipengChen.2024.FromNovicetoExpert:LLMAgentPolicy particularlyfortaskswithlongreasoningchain.Consequently,the
modelstrugglestopreciselyrefinelow-qualityactions,resulting
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor inlowlearningefficiency.Thedelayedrewardfeedbackprevents
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation themodelfrommakingtimelycorrections,potentiallyleadingto
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe sub-optimalresponses.
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
Process-supervisedreinforcementlearning[27,41]presentsa
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. promisingsolutiontothesechallengesbyprovidingsupervision
WWW’25,April28-May02,2025,Sydney,Australia ateachintermediatereasoningstep.Throughouttheprocessof
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. agentreasoning,therewardofeachintermediatestepcanassist
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX inidentifyingunderperformedpoliciestimely,allowingforagent
4202
voN
6
]IA.sc[
1v71830.1142:viXraWWW’25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
agentandtheexpert.Webeginbyobservingexpertbehaviorpat-
ternsandthenforcetheagenttopracticeindependentlyateach
Expert SFT Observation step.Thisfacilitatesadeeperandfine-grainedcomprehensionof
LLM Agent
theexpert’sdecision-makingprocesses,spontaneouslyproviding
(a) Reward-Free Method Action step-wiserewardfeedback.Next,wedeviseareflectionmoduleto
effectivelyadjustandimproveagentpolicybasedonthepractice
Reward: 1.0 results.Wedevisetwostrategiesforagentreflection,including
Expert
implicit-rewardreinforcementlearningandinversereinforcement
learning.Tovalidatetheeffectivenessofourmodel,weconduct
Agent Reward: 0.2 RL
extensiveexperimentsonthreedifferentscenariosofagentinter-
LLM Agent
activetasks.Experimentalresultsconsistentlydemonstratethat
(b) Sparse Reward Method ourmodelStepAgentoutperformsthestate-of-the-artLLMagent
models.Thisclearlyindicatesthesuperiorityofapplyingstep-wise
rewardreinforcementlearningtoLLMagentpolicylearning.
Expert
Ourmaincontributionsarethree-fold:
Agent Agent Step-wise RL (1)Weproposeastep-wisereinforcementlearningframework
StepAgentthatautomaticallyconstructsintermediatefeedbackto
LLM Agent
progressivelyandefficientlyoptimizetheagentpolicytoeventually
(c) Our Method
alignwiththeexpertpolicy.
(2)Weintroducetwostagesencompassinginspectionandre-
Figure1:Thecomparisonbetweenourstep-wisefeedback
flection,andconstructprocess-supervisedtrainingdatawithout
LLMagentframeworkandpreviousapproaches.
humanannotationtofacilitatethenovicesbecomingexperts.
(3)Wedevisetworeflectionstrategiesforstep-wiseoptimization,
capabilityrapidimprovements.Inlightofthis,weproposetoop- includingimplicit-rewardandinversereinforcementlearning.
timizeagentpolicybyincorporatingstep-wisesupervision
intoreinforcementlearning.However,directlyapplyingstep-
2 RelatedWork
wisesupervisiontoLLMagentsintroducesitsownsetofchallenges.
First,valueassessmentsforindividualstepsareoftenabsentfrom 2.1 LLMsasAgent
thecurrentmulti-stepagentinteractiondatasets,leavingonlya Recently, the outstanding capabilities of large language models
finalevaluation.Evenforhumanannotators,fullyunderstanding (LLMs)haveledresearcherstoexploreadoptingthesemodelsas
thecontributionofeachsteptotheultimateoutcomepresentsa agentcorecontrollersandconstructingartificialintelligence(AI)
significantchallengethatcanbebothcostlyandlabor-intensive. agents.Thedevelopmentofexistingagentsystemscanberoughly
Furthermore,thenecessityforagentstointeractwiththedynami- dividedintotwoprimarycategories:prompt-basedmethodsand
callychangingenvironmentmakesthesituationevenmorecompli- fine-tuning-basedmethods.
cated.SamplingrewarddistributionsbasedonMCTS[7]requires Prompt-basedMethods.Prompt-basedmethods[22,30]fo-
theagenttointeractwiththeenvironmentuntilobtainingthefinal cused on carefully designing the prompt and directly utilizing
rewardwhichisnon-parallelizableandinefficient. closed-source large language models, such as ChatGPT [24] or
Consideringtheaforementionedconcerns,weaimtoefficiently GPT-4[25],fortaskplanningandreasoning.Chain-of-Thought
constructstep-wiserewardsupervisionwithoutadditionalhuman (CoT)prompting[45]wasthefundamentalofmostprompt-based
annotationtoaddresstheabilitygapbetweentheLLMagentand methodswhichintroducedintermediatereasoningstepsindemon-
theexpert.WetakeinspirationfromBenner’snovice-to-expertthe- strationstoenhancethecapacitytodosophisticatedreasoning.
ory[3,4]—novicescangraduallyalignwithexpertpolicythrough InheritthespiritofCoTprompting,ReAct[50]devisedathink-
repeatedlyobservingexpertbehaviorswithautonomouspractic- and-actformatprompttoinspireLLMstogeneratebothreasoning
ingandreflectionoftheircurrentpolicy[3,4].Intriguingly,even tracesandtask-specificactionsinaninterleavedmanner.ToT[49]
lackingexplicitprocess-supervisedrewardsignals,novicescanstill furthergeneralizedtotree-structureensuringtoexplorevarious
progressivelyapproximateexpertpolicyandrespondswiftlyto reasoningpathsandmakeglobaldecisionsbylookingaheador
externalstimuli.Thiscognitiveproficiencymirrorsthechallenge backtrackingwhennecessary.Drivenbyhumanrevisionbehavior,
ofadaptingstep-wisereinforcementlearninginagentinteraction SELF-REFINE[20]utilizedasingleLLMasthegenerator,refiner,
tasks—lackingstep-wisesupervisionandflexibility. andfeedbackprovider.Inaddition,Reflexion[34]leveragedlinguis-
Drawing on the above motivations, we propose a step-wise ticfeedbackmaintainedinamemorybuffertoreinforceagentsand
LLMAgentlearningframework(StepAgent),whichemulatesthe inducebetterdecision-making.
novice-to-expertlearningprocessbyautomaticallyconstructing Fine-tuning-basedMethods.Althoughprompt-basedmeth-
supervisionsignalsforstep-wisereinforcementlearning,thereby odscouldachievepromisingperformanceswithouttraining,they
approachingtheexpertpolicy.Wedelineatethenovice-to-expert heavilyrelyonwell-designedpromptsandadvancedclosed-source
processintotwodistinctstepsinthecontextofagenttasks,in- models(e.g.,ChatGPTandGPT-4)leadingtohighusagecosts.To
cludinginspectionandreflection.Specifically,fortheinspection addressthesechallenges,recentstudies[8,10,51,53]constructed
stage,ourtargetistorecognizethepolicydistinctionbetweenthe experttrajectorydatawithteacheragents(e.g.,GPT-4orhumans)FromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWW’25,April28-May02,2025,Sydney,Australia
andperformedsupervisedfine-tuningonopen-sourceLLMs(e.g., policy𝜋 𝜃(·|𝑠 𝑡), where𝑠 𝑡 = (Promptsys,𝑜1,𝑎1,···,𝑎 𝑡−1,𝑜 𝑡) ∈ S
LLaMA[39]andMistral[16]).Takingastepfurther,NAT[44]and isthecurrentstateoftheenvironment.Theinteractionprocess
ETO[36]introducednegativesamplesduringthefine-tuningto repeatsuntilthetaskcompletesorexceedsthemaximumsteps.
reducemodelhallucinationsandenhancerobustness.Furthermore, A reward 𝑟 ∈ [0,1] is then computed for the final trajectory
RejectionsamplingFine-Tuning(RFT)[52]collectedcorrectreason- (Promptsys,𝑜1,𝑎1,···,𝑜 𝑛,𝑎 𝑛), where𝑟 = 1 indicates the task is
ingpathsgeneratedbythesupervisedmodeltoenrichfine-tuning successand0meansfailure.1Theconditionalprobabilitydistribu-
datasetswhileSPIN[9]empoweredaweakAIagentleveragingits tionfortheoverallprocess𝜋 𝜃(𝑎 𝑛|𝑜1) canbedenotedthrougha
generateddatafortrainingwithoutadditionalhumanannotation. decompositionasfollows:
Inthispaper,wefocusonfine-tuningLLMswithreinforcement
𝑛
learninganddeviseastep-wiselearningstrategytoalignthecapa- (cid:214)
𝜋 𝜃(𝑎 𝑛|𝑜1)= 𝜋 𝜃(𝑎 𝑡|𝑠 𝑡). (1)
bilitiesoftheagentwiththeexpert.
𝑡=1
2.2 ReinforcementLearningforLLMs 3.2 SupervisedFine-tuning
WiththedevelopmentoftheLLMs,reinforcementlearning(RL)[11, Supervisedfine-tuning(SFT)entailsleveragingrelativelysmaller
57]playsavitalroleinimprovingthecapabilitiesofLLMs.Actor- labeledexpertdatatobetteradaptthepre-trainedLLMstospecific
Critic[17]wasthebasisofmanyadvancedRLalgorithmswhich domainsordownstreamtasks[26,55],providingasolidfoundation
leveragedtheactorpolicynetworktointeractwiththeenviron- forcreatingapowerfulagent.
mentandperformpolicyupdatesundertheguidanceofthecritic Givenanexpertinteractiontrajectory𝑡 𝑒 =(𝑜ˆ 1,𝑎ˆ 1,···,𝑜ˆ 𝑛,𝑎ˆ 𝑛)in
valuefunction.Basedontheactor-criticalgorithm,TrustRegion theexperttrajectorysetT,weleveragetheauto-regressivelossto
PolicyOptimization(TRPO)[32]introducedtrustregiontoensure fine-tunetheinitialLLMandobtainthebaseagent𝜋 asfollows:
𝜃0
monotonicperformanceofpolicylearningwhileProximalPolicy
Optimization(PPO)[33]furtherproposedpenaltyandclipstrate-
𝐿 SFT=−E 𝑡𝑒∼T[𝜋 𝜃(𝑎ˆ 𝑛|𝑜ˆ 1)]. (2)
g oi fe is nt so tas bim ilip tl yify duth rie na glg Ro er ii nth fom rcim empl ee nm te Ln et aa rt nio in n. gT fo roso mlv Het uh me ap nro Fb ele em d- F (𝑜o ˆ 1ll ,o 𝑎ˆw 1,i .n ..g ,𝑜ˆE 𝑡)q .u Wat eio fin rs( t1 c), o𝜋 n𝜃 ca( t𝑎ˆ e𝑛 n| a𝑜ˆ t1 e) t= he(cid:206) in𝑛 𝑡 s= t1 ru𝜋 c𝜃 ti( o𝑎ˆ n𝑡| p𝑠ˆ 𝑡 r) o, mw ph t,e are ct𝑠 iˆ o𝑡 n= s
back(RLHF)training,DirectPreferenceOptimization(DPO)[29] andobservationsintrajectory𝑡 𝑒asatokensequence𝑤 =(𝑤1,...,𝑤 𝑙)
adoptedasimpleclassificationlosstofine-tuningLLMsandachieve withlength𝑙.Then,theprobability𝜋 𝜃(𝑎ˆ 𝑛|𝑜ˆ 1)inEquation(2)can
higherefficiencyandbetterperformances.Sincetherewardsignal beformulatedasfollows:
isuncertainorsparseinreal-worldscenarios,researchersproposed
∑︁
behaviorcloning(BC)[38]toimitatethebehaviorsofexperts.Fur- 𝜋 𝜃(𝑎ˆ 𝑛|𝑜ˆ 1)=− log𝜋 𝜃(𝑤 𝑘|𝑤 <𝑘)×1𝑤𝑘∈A, (3)
thermore,GenerativeAdversarialImitationLearning(GAIL)[14] 𝑘
d agev enis te td oa fin ti tt he era et xiv pe err tew daa tr ad df iu sn trc it bi uo tn iol nea .rningstrategyforcingthe w inh de icr ae t𝑤 or< f𝑘 unin cd tii oca nte ins dt io ck ae tn ins gb wef ho ere tht eh re 𝑤𝑘-th ist aok toe kn ea nn od f1 a𝑤 c𝑘 ti∈ oA nsi gs ea nn
-
𝑘
Therewardfunctioninpreviousagentapproacheswaseither eratedbytheagent.Wemasktheobservationtokensandcompute
manuallyannotated[2,11,37]orlimitedtothefinalrewardfeed- theprobabilitysolelyfortheactiontokens.
backfromtheenvironment[35,43,48].Inthiswork,weproposea
step-wisereinforcementlearningmethodandautomaticallygener-
4 FromNovicetoExpert
aterewardsforeachstep.
Largelanguagemodel(LLM)agentshavedemonstratedsuperior
capabilitiesintacklingcomplexinteractivetasks,byleveraging
3 Preliminaries
reinforcementlearningstrategytoaligntheagentpolicywithhu-
Inthissection,wefirstformulatetheagenttaskandthenreview manpreferences.However,existingresearchonLLMagents[9,36]
supervisedfine-tuningforLLMs,acrucialstepbeforereinforcement encountersignificantchallengesstemmingfromrewardsignalspar-
learningthatpreparesthemodelforspecifictasks. sityandthecomplexitiesassociatedwithreasoningprocess.To
addresstheselimitations,inthissection,weintroduceastep-wise
3.1 ProblemFormulation reinforcementlearningframeworktooptimizetheagentpolicy
Theprocessofanagentinteractingwiththeenvironmentfortask withoutmanuallyannotatingtheproceduralrewards.Ourapproach
solvingcanbeformalizedasapartiallyobservableMarkovdecision isinspiredbytheprinciplesofBenner’snovicetoexpert[3,4],
process(POMDP)withthestatesetS,actionsetA,observation facilitatingprogressivelyself-iterativeexperienceacquisition.By
setO,transitionfunctionF :S×A →S,andrewardfunction constantlymonitoringtheexpert’sbehaviorsandpracticesponta-
R :S×A→ [0,1].Initially,theenvironmentprovidesageneral neously,theLLMagentcanaccumulateexperienceandeventually
taskinstructionPromptsysasthesystemprompt,alongwiththe advancefromnovicetoexpertproficiency.
agent’sinitialobservation𝑜1 ∈ Oasthespecifictaskinput,and The overall framework of StepAgent is depicted in Figure 2.
theagentneedstointeractwiththeenvironmentmultipletimes StepAgent comprises two major phases: (1) Inspection and (2)
forcompletingthetaskandgeneratingresponses. Reflection.Thedetailsofthetwostagesareintroducedinthe
Specifically,atthetimestep𝑡,thelargelanguagemodelagent followingsections.
parameterizedby𝜃 receivesanobservation𝑜 𝑡 ∈ O fromtheen-
vironmentanddecidestotakeanaction𝑎 𝑡 ∈Aaccordingtothe 1WeomitPromptsysforsimplificationinthefollowingexpressions.WWW’25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
Inspection LLM Agent Observe E shn ev lfi r so tan bm lee .n itt : n I e w edo su tl od bli ek e k ea t ob ,u gn ld ul te e no f f rc er ea ,c ak ne drs p, rs ip ci ec y lo b we ee rf ta hn ad n c 4h 0e .e 0s 0e dw oh lli ac rh s .i s
Expert Expert: Thought: I need to find ...
Inspection Inspection Practice Action: search[bundle of crackers, spicy beef and cheese]
Agent: Thought: The Spicy Beef Backpacking Bundle Environment: B0978Q1KK9, The Spicy Beef Back-packing Bundle …, $14.49;
(B0978P4L31) seems to match my criteria and is well B0978P4L31, …
Step-wise within my budget. Action: click[b0978p4l31] Expert: Thought: The Spicy Beef Backpacking Bundle (B0978Q1KK9) seems
Training Data Observe to match my criteria and is well within my budget. Action: click[b0978q1kk9]
LLM Agent Environment: flavor name: original beef backpack bundle; spicy beef backpack
Reflection Implicit Pair- bundle…. Price: $14.49. Rating: N.A. Description. Features. Reviews. Buy Now.
wise Feedback Reward Func Practice Expert: Now that I've clicked on the Spicy Beef Backpacking Bundle, I need to
PPO Agent: Thought: This is a spicy beef and cheese confirm that it's the spicy beef and cheese flavor, shelf stable, keto, and gluten-
bundle that's shelf stable. I should verify that it's keto free as required. Action: click[spicy beef backpack bundle]
LLM Agent LLM Agent and gluten-free, but since the price is affordable and it …
seems to match my needs, I'll go ahead and purchase
Implicit-Reward RL Inverse RL it. Action: click[buy now] Expert: Thought: …, I'll proceed with the purchase. Action: click[buy now]
Figure2:ThearchitectureofourproposedframeworkStepAgentcontainingtwostages:inspectionandreflection.Bluesnowfake
indicatesfrozenparameterswhileredflamemeanstrainableparameters.TheexamplecomesfromtheWebShopdataset.
4.1 Inspection:RecognizingCapabilityGaps thisspontaneousexerciseisthatthenovicegeneratesactionsbased
Inspection,inaccordancewithBenner’snovicetoexperttheory[3, onthepreviouslyestablishedlearningtargets.Specifically,foreach
4],involvesthenoviceinitiallyobservingexpertbehaviorsandat- learningobjectiveinTsample,wetreatthestate𝑠ˆ 𝑖 =(𝑜ˆ 1,𝑎ˆ 1,···,𝑜ˆ 𝑖)
temptingtoreplicatethesebehaviorsindependentlyunderthesame asthepromptandlettheagent𝜋 𝜃 parameterizedby𝜃 togenerate
circumstance.Thiscomparativepracticeaimstorecognizetheca- theappropriateactionasEquation(5)andobtainthecorresponding
pabilitygapbetweenthenoviceandtheexpert,therebyfacilitating agenttrajectory(𝑜ˆ 1,𝑎ˆ 1,···,𝑜ˆ 𝑖,𝑎𝜃 𝑖) ∈T sa𝜃 mple.
subsequentlynovicepolicyimprovements.Previousmethodsfor
constructingLLMagents[9,36]focusonobservingandimitating 𝑎𝜃 𝑖 ∼𝜋 𝜃(𝑎|𝑠). (5)
thecompletebehaviortrajectoryoftheexpertwiththefinalenvi-
ronmentalrewardfeedbackforoptimization.However,duetothe 4.2 Reflection:StrategizingPolicyRefinement
complexityoftheagenttasks,LLMagentsneedtoconstantlyinter- Innovice-to-experttheory,progressiontowardexpert-levelperfor-
actwiththeenvironmentandengageintrial-and-errortoarriveat mancerequiresnovicestoreflectontheirinteractiontrajectories.
theultimatereasoningoutcome.Theinherentmulti-stepreasoning Thisintrospectionisintendedtosummarizeandinternalizeexpe-
characteristicsofagenttasksbringdualchallengesofefficiencyand riences,ultimatelyleadingtothedevelopmentofindividualized
effectivenessforthenovice’sself-attemptsofthecompletetrajec- behaviorpatternsandpolicies.Therefore,inthissection,welever-
tory.First,emulatingthefulltrajectoryoftheexpertandacquiring ageinteractionsconstructedinSection4.1anddevisetwodistinct
thefinalenvironmentalfeedbackrequiretheagenttoconstantly reflectionstrategies,includingimplicit-rewardreinforcementlearn-
interactwiththeenvironment.Thisinteractionissequentialand ingandinversereinforcementlearning.
cannotbeparallelized,resultinginthesignificantconsumptionof
computationaltimeandresources.Besides,thenecessityforthe 4.2.1 Implicit-Reward Reinforcement Learning. We begin by di-
rectlycomparingtheactionsoftheexpertandthenoviceagent
novicetocomprehendeveryexpertactionsimultaneouslycanlead
withoutintroducingexplicitrewardestimation.Givenatrajectory
toinformationoverload.Thisoverloadcomplicatesthenoviceto
digestandmasterthespecificsofeachbehavior,oftenresultingin pair (𝑡 sample,𝑡 𝜃) where𝑡 sample = (𝑜ˆ 1,𝑎ˆ 1,···,𝑜ˆ 𝑖,𝑎ˆ 𝑖) istheexpert
inefficientlearningprocesses.Consequently,novicesmayrequire trajectorywhile𝑡 𝜃 =(𝑜ˆ 1,𝑎ˆ 1,···,𝑜ˆ 𝑖,𝑎𝜃 𝑖)isthecorrespondingagent
additionaltrainingdataoriterationstofullygrasptheinsights trajectory.Inheritingthespiritofpreviousworks[9,36],weutilize
derivedfromtheexpert’sexperiences. thedirectpreferenceoptimizationloss[29],definedasfollows:
Tat ht ieT sno et nia v ad e bd l lyr ee sos tbs hst eeh r ne v os ee va icl ni em d toi it ma idt ii t eo a nn t tes if, t yhit se hi es ox re p ts ces ore mtn ’s it ni aa gcl stf i ioo nnr ts hth est ie repn b- eo b hv y ai -c vse it oet rpo s. 𝐿 implicit(𝜋 𝜃,𝜋 𝑒)=−E[log𝜎(𝛽log𝜋 𝜋𝜃 𝑒(( 𝑎𝑎 ˆˆ 𝑖𝑖 || 𝑠𝑠 ˆˆ 𝑖𝑖 )) −𝛽log𝜋 𝜋𝜃 𝑒(( 𝑎𝑎
𝜃
𝑖𝜃 𝑖 || 𝑠𝑠 ˆˆ 𝑖𝑖 )) )],
andfacilitatethemasteryofcriticalskills.Specifically,considering (6)
an expert trajectory 𝑡 𝑒 = (𝑜ˆ 1, 𝑎ˆ 1, ···,𝑜ˆ 𝑛,𝑎ˆ 𝑛) with𝑛-steps, we where𝜋 𝜃 isthecurrentagentpolicyneededtobeoptimized,𝜋 𝑒 is
segmentthistrajectoryaftereachaction,treatingeachactionasa theexpertpolicy,𝜋 refisthereferencemodelinitializedwiththe
short-termlearningobjectiveforthenovice: agentpolicyand𝛽isahyper-parameter.
(𝑜ˆ 1,𝑎ˆ 1,···,𝑜ˆ 𝑖,𝑎ˆ 𝑖) ∈Tsample, 𝑖 =1,2,···,𝑛. (4) 4.2.2 InverseReinforcementLearning. Consideringthelackofre-
wardsignalsforeachreasoningstepinexistingdatasets,weintro-
Whenthenoviceestablisheslearningtargets,ittriggerstheprac- duceaninversereinforcementlearning(IRL)method[1,14,23,31].
ticestageintheexpert-novicelearningprocess.Thisspontaneous Thismethodfirstinfersthestep-wiserewardfunctionbasedon
exerciseisgearedtowardsidentifyingthebehavioraldiscrepan- theexpert’sandagent’sbehaviorsandthenleveragesthereward
cies between the novice agent and the expert, allowing for the functiontofine-grainedoptimizestheagentpolicy.
accumulationofexperienceandthegradualdevelopmentofthe Wefirstdefinetheoccupancymeasure𝜌 forapolicy𝜋,indi-
𝜋
novice’sbehavioralpatternsthroughrepeatedpractice.Centralto catingthenormalizeddistributionofstate-actionpairswhentheFromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWW’25,April28-May02,2025,Sydney,Australia
agentadoptspolicy𝜋 toexploretheenvironment: Algorithm1StepAgentwithInverseReinforcementLearning
𝜌 𝜋(𝑠,𝑎)=(1−𝛾)∑︁ 𝑡∞ =0𝛾𝑡𝑃 𝜋(𝑠 𝑡 =𝑠)𝜋(𝑎|𝑠), (7) 1 2 3: :
:
I On up tu pt u: taE :gx Fep in ne t art lpt aor gla i ecje nyc tit pno oir ti lie ia cs l yi( z𝑜 𝜋eˆ 1 𝜃d,𝑎 bˆ 1 y, 𝜋... 𝜃, 0𝑜ˆ 𝑛−1,𝑎ˆ 𝑛) ∈T,
w prh oe br ae bi1 li− ty𝛾 oi fs thth ee an geo nrm tia nli sz ta at ti eo 𝑠n af tac tit mor e,𝑃 𝑡𝜋 w( h𝑠 𝑡 en= a𝑠 d) or pe tp inre gs pe on lt is cyth 𝜋e . 4 5: : I fn oi rti ia tl ei rz ae ti𝜋 o𝜃 n1 𝑖← =𝜋 1,𝜃 20 ,...do
Toaccuratelyimitatetheexpertpolicy,itisessentialtoensure 6: //InspectionStage.
thatthepolicydistributiongeneratedbytheagentisassimilaras 7: Foreachsampledstep-wiseexperttrajectory(𝑜ˆ 1,𝑎ˆ 1,···,
possibletothatgeneratedbytheexpert.Thiscanbeachievedby 𝑜ˆ 𝑡,𝑎ˆ 𝑡) ∈Tsamplegeneratethecorrespondingagenttrajectory
m asa pin ot sa si in bi ln eg toth ta ht at th oe fa tg he en et x’s po erc tcu 𝜌pan .Wcy em ae da os pu tre Je𝜌 n𝜋 s𝜃 eni -s Sa hs ancl no os ne (𝑜ˆ 1,𝑎ˆ 1,···,𝑜ˆ 𝑡,𝑎𝜃 𝑡) ∈T sa𝜃 mplewithpolicy𝜋 𝜃𝑖
𝜋𝑒 8: //ReflectionStage.
divergence(JS)tomeasurethedistancebetweentwodistributions.
9: fordatain(Tsample,T sa𝜃 mple)do
m 𝜋inJS(𝜌 𝜋𝜃,𝜌 𝜋𝑒)−𝜆𝐻(𝜋 𝜃), (8)
10: trainthediscriminatorwiththefollowingloss:
where𝜆isthehyper-parameter,𝐻(𝜋 𝜃)=△E 𝜋𝜃[−log𝜋 𝜃(𝑎|𝑠)]isthe E 𝜋𝜃[log(𝐷𝑤(𝑠,𝑎))]+E 𝜋𝑒[log(1−𝐷𝑤(𝑠,𝑎)] (11)
𝛾-discountedcausalentropy[5]oftheagentpolicy. 11: Updatetheparameterofthediscriminator𝐷 𝑤 →𝐷 𝑤′
FollowingGAIL[14],theJensen-ShannondivergenceJS(𝜌 𝜋𝜃,𝜌 𝜋𝑒) 12: TakeapolicystepwithPPOruleandrewardfunction
andberepresentedbyaconvexcostfunctionregularizer𝜛(𝜌 𝜋𝜃 − log(𝐷 𝑤′(𝑠,𝑎))andupdatepolicy𝜋 𝜃𝑖 →𝜋 𝜃𝑖′.
𝜌 𝜋𝑒),uptoaconstantshiftandscaling.Thedefinitionoftheconvex
costfunctionregularizer𝜛:RS×A →R∪{∞}isdefinedas:
𝜛(𝑐) ≜
(cid:26) E +∞𝜋𝑒[−𝑐(𝑠,𝑎)−log(1−𝑒𝑐(𝑠,𝑎))] 𝑐
𝑐
< ≥0 0;
.
(9)
tribP ur to ioo nf. oT fh ste ato ec -c au cp tia on ncy pam ire sa .s Cu ore nsr ee qp ure es ne tn lyt ,s tt hh ee dn io scrm rea pl aiz ne cd yd bi es --
tween𝜌 and𝜌 canbemeasuredusingtheKullback-Leibleror
Accordingto[14],theoptimalsolutionoftheaboveregularizer
𝜋𝜃 𝜋𝑒
𝜛(𝜌
𝜋𝜃
−𝜌 𝜋𝑒)isdenotedasfollows: J se itn iose nn 5-S .1h ca an nno bn ed reiv fe or rg me un lc ae teK dL i/ nJS to(𝜌 P𝜋 r𝜃 o, p𝜌 o𝜋 s𝑒 i) ti. oIn nt 5h .2is .context,Propo □-
sup E 𝜋𝜃[log(𝐷(𝑠,𝑎))]+E 𝜋𝑒[log(1−𝐷(𝑠,𝑎))].
𝐷∈(0,1)S×A Proposition5.2. Provingthatoptimizingthelossfunctioncan
Therefore,theoptimizationproblemofEquation(8)canbetrans- beultimatelyequivalenttotheminimizedKL/JSdivergence.
formedintofindingasaddlepoint(𝜋,𝐷)ofthebelowEquation:
Intheremainingsection,wedemonstratethatthisproposition
E 𝜋𝜃[log(𝐷(𝑠,𝑎))]+E 𝜋𝑒[log(1−𝐷(𝑠,𝑎)]−𝜆𝐻(𝜋 𝜃). (10) isvalidforbothreflectionmechanisms,includingimplicit-reward
reinforcementlearningandinversereinforcementlearning.
Wedirectlytrainadiscriminatornetwork𝐷 :S×A→(0,1),
utilizingdatasampledfromtheexpertandagenttrajectories.The
Proof. Inthefollowingparts,wefirstprovethatProposition5.2
primaryobjectiveof𝐷istodifferentiatebetweenthedistribution
holdsforimplicit-rewardreinforcementlearning,andthenprove
ofdatageneratedbytheagentpolicy𝜋 andtheexpertpolicy𝜋 .
𝜃 𝑒 forinversereinforcementlearningoptimization.
When𝐷cannotdistinguishdatageneratedbytheagentfromthe
STEP1.AccordingtoRafailovetal.[29],theoptimalsolution
expert,thentheoccupancymeasureoftheagent𝜌 hassuccessfully
𝜋 oftheKL-constrainedrewardmaximizationobjectivecanberear-
matchedthatoftheexpert𝜌 .Thediscriminatornetwork𝐷can
𝜋𝑒 rangedsothattherewardfunctioncanbeexpressedas
beinterpretedasanimplicitrewardmodelprovidingstep-wise
learningsignalstotheagentpolicy.Thecompletelearningprocess
𝑟(𝑠,𝑎)=𝛽log
𝜋 𝜃(𝑎|𝑠)
+𝛽log𝑍(𝑠),
ofStepAgent-inverseisintroducedinAlgorithm1. 𝜋 ref(𝑎|𝑠)
where𝑍(𝑠) isthepartitionfunction[13,18].FollowingBradley-
5 TheoreticalAnalysis Terrymodel[6],wehave:
Inthissection,weprovideatheoreticalanalysistoprovethatthe
distributionofactionsgeneratedbytheagentcanconvergetoward
𝑝(𝑎1 >𝑎2|𝑠)=𝜎(𝑟(𝑠,𝑎1)−𝑟(𝑠,𝑎2)).
theexpertactiondistributionovermultipletrainingcycles. Then,thepolicyobjectivecanbeformulatedasEquation(6)which
isequivalenttominimizingtheKLdivergence.
Assumption 1. The loss function of Equation (6) and (10) is
STEP2.Inversereinforcementlearningfirsttrainsadiscrimina-
boundedandLipschitzcontinuous.
tornetwork,whichsubsequentlygeneratesscoresthatserveasthe
Sinceourpolicyupdatemethodemploysgradientdescent,under rewardfunctionforoptimizingthepolicynetwork.Itsoptimization
Assumption1,thepolicy𝜋 willconvergetoalocalminimumas targetcanbedenotedas:
𝜃
theiterationsincrease.Thefollowinganalysesareconductedunder
Assumption1.
𝐽(𝜃)=−E (𝑠,𝑎)∼𝜋𝜃[log𝐷(𝑠,𝑎)].
Accordingtothepolicygradienttheorem,thegradientof𝐽(𝜃)
Proposition5.1. Theoccupancymeasure𝜌 𝜋𝜃 fortheagentpolicy canbeexpressedas:
canconvergetocloselyapproximatetheexpert’soccupancymeasure
𝜌 𝜋𝑒,afterseveraliterations. ∇𝜃𝐽(𝜃)=E (𝑠,𝑎)∼𝜋𝜃 [∇𝜃log𝜋 𝜃(𝑎|𝑠)𝑅(𝑠,𝑎)].WWW’25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
Table1:Statisticofdatasetsinourexperiments. Wecomparethetwovariants(i.e.,implicitandinverse)ofour
methodStepAgentwithseveralbaselinesincluding(1)Supervised
Type Dataset #Train #Dev #Test #Turns
Fine-Tuning(SFT)[8,53]conductsbehavioralcloningonexperttra-
jectories,whichisthebaseagentforStepAgentandotherbaselines.
WebShop 1,938 - 200 4.9
Web
Mind2Web 1,009 - 912 7.3
(2)ProximalPolicyOptimization(PPO)[33]andDirectPreference
Optimization (DPO) [29] are two representative reinforcement
Agent ScienceWorld 1,483 194 241 14.4 learningmethods.Weutilizethefinaltaskrewardfromtheenvi-
ALFWorld 3,321 140 134 10.1
ronmentastherewardfeedbackforPPO.AsforDPO,weadoptthe
HotpotQA 90,447 7,405 7,405 7.0 trajectoriesgeneratedbytheagentasnegativesamples.(3)Rejec-
Multihop
QA
2WikiMultihopQA 167,454 12,576 12,576 8.2 tionsamplingFine-Tuning(RFT)[52]andSPIN[9]incorporatethe
MuSiQue 19,938 2,417 2,417 7.8 successtrajectoriesoftheagenttotheexperttrajectorydatasetand
trainstheagentonnewaugmentedtrajectories.(4)NAT[44]and
ETO[36]introducerejectedtrajectoriesintothetrainingprocess,
Weutilizetheoutputofthediscriminatorastherewardandthe
allowingtheagenttolearnfromitsfailureexperiences.Wealso
gradientofthepolicybecomes:
compareStepAgentwithClosed-SourceLLMsincludingGPT-3.5
∇𝜃𝐽(𝜃)=E (𝑠,𝑎)∼𝜋𝜃 [∇𝜃log𝜋 𝜃(𝑎|𝑠)𝐷(𝑠,𝑎)]. (GPT-3.5-turbo-1106)[24]andGPT-4(GPT-4-0125-preview)[25].
Theexpectedreturnofthepolicyupdate(i.e.,theoutputofthe
discriminator)isrelatedtothegradientofthepolicyparameters. 6.3 EvaluationMetrics
AccordingtoEquation(8-10),optimizingthelossfunctionofthe To align with previous methods [9, 36], we report the average
discriminatornetworkisequivalenttoreducingtheJSdivergence resultsofthetestset.ForWebShopandScienceWorld,weemploy
betweenthetwooccupancymeasures.Theapplicationofthepolicy thefinalrewardautomaticallyassessedbytheenvironmentasthe
gradienttheoremenablestheagenttooptimizeitsstrategyusing evaluationmetricwhileforALFWorld,weutilizethesuccessrate
feedbackfromthediscriminator.Thisprocessensuresthetrajectory forjudgement.IntermsofMind2Web,wereportmacroelement
generatedbytheagenttograduallyapproachtheexpert’strajectory accuracy.Additionally,forthethreemulti-hopquestion-answering
distributionbymaximizingtheoutputofthediscriminator. tasks,weleverageExactMatch(EM)forevaluation.
□
6.4 ImplementationDetails
6 ExperimentalSettings Consistentwithexistingworks[19,36],weemployReAct-form[50]
6.1 Datasets togeneratetheinteractiontrajectory,whichadditionallygenerates
Chain-of-Thought(CoT)rationales[45]beforeeachaction.Foreach
TothoroughlyevaluatetheabilityofourproposedmodelStepAgent,
task,aone-shotin-contextexampleisemployedintheinstruction
weutilizerepresentativetasksfromthreeaspects,includingweb
prompt.ThedetailsofpromptsaredescribedinAppendixA.For
tasks,agenttasks,andmulti-hopquestion-answeringtasks.The
thethreemulti-hopquestionansweringtasks,duetothelackof
statisticsofthesedatasetsaredelineatedinTable1.
intermediatereasoningstepsinthedatasets,weemployGPT-4[25]
WebtasksconsistofWebShop[48]foronlineshoppingand
astheexperttogeneratetrajectoriesandselecttrajectorieswiththe
Mind2Web[12]forcomplextasksonvariouswebsites.Rewardsin
exactmatchscoreequallingoneastheexperttrajectories.Welever-
thetwodatasetsaredensevariableandrangefrom0to1.
agegreedygenerationforourmethodandallbaselineapproaches.
AgenttaskscontainScienceWorld[43]forscienceexperiments,
IntheSFTstage,wesetthelearningrateas1e-5andthebatchsize
andALFWorld[35]forembodiedhousework.Theformercontains
as64.wechoosethecosineschedulerwitha0.03warmup.Wetrain
continuous final rewards from zero to one while the latter has
themodelforfourepochsonalldatasets.Forthereflectionstage,
binaryrewardsdemonstratingthecompletionofthetask.Forboth
thelearningrateis5e-7andthebatchsizeis16.Thetrainingepoch
datasets,wetreatthein-distributiontestsetsasthevalidationset
issetasone.WeleveragetheAdamWoptimizerinbothstages.All
andtheout-of-distributionunseenvariationswhichaimtoassess
experimentsarecarriedouton8NVIDIAA10080GGPUs.
thegeneralizationcapabilitiesofagentsasthetestset.
Multi-hopquestion-answeringtasksincludeHotpotQA[47],
7 ResultsandAnalysis
2WikiMultihopQA[15],andMuSiQue[40].Foreachdataset,we
leveragetheirassociatedWikipediaarticlescontextsasourretrieval 7.1 OverallResults
corpustoconductmulti-stepreasoning.Consideringtherestric- TheoverallperformanceofourproposedmethodsStepAgentand
tionsofexperimentalcosts,followingpreviousapproaches[50,56], allbaselinesareshowninTable2.Wecanobservethat:
weutilizeasubsetoftheentiredataset,selecting5,000samples (1)BothvariantsofStepAgentconsistentlyoutperformallbase-
fortrainingfromthetrainingsetand500sampleseachforthe linemethodsacrossthreedistincttaskcategoriesbyasignificant
validationandtestsetsfromthedevelopmentset. margin.IncomparisonwithETOandSPIN,whichintroducethe
entiretrajectoryfortraining,StepAgentachievesasignificantedge
6.2 BackboneModelsandBaselines withimprovementsoftheresultsoveralltasks.Thisperformance
WeverifytheeffectivenessandrobustnessofourStepAgentontwo demonstratestheeffectivenessofutilizingthestep-wiserewardsig-
widely-usedopen-sourcemodels:Mistral-7B(Mistral-7B-Instruct- nalstoemulatetheexpertpolicy.Evenwithouthuman-annotated
v0.1)andLlama-3-8B(Meta-Llama-3-8B-Instruct). step-wisepreferencedata,StepAgentstillcangraduallyalignwithFromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWW’25,April28-May02,2025,Sydney,Australia
Table2:Performancecomparisonofallmethods.Bothvariantscanoutperformallbaselinesbasedonopen-sourcedmodels.
WebTasks AgentTasks Question-AnsweringTasks
Backbone Methods
WebShop Mind2Web ScienceWorld ALFWorld HotpotQA 2WikiMultihopQA MuSiQue
GPT-3.5 Base 40.2 2.0 19.9 2.2 13.0 17.6 4.6
GPT-4 Base 58.0 26.7 53.6 36.6 39.4 64.8 28.2
Base 2.7 17.8 4.2 0.0 4.2 10.6 1.4
SFT 60.1 48.7 52.0 68.5 24.8 40.4 22.9
PPO 60.8 49.5 53.3 69.1 25.4 41.5 23.2
DPO 62.4 50.9 54.1 70.6 26.9 42.7 24.9
RFT 61.5 49.8 53.2 69.8 26.0 42.2 23.5
Mistral
7B SPIN 63.6 51.7 55.0 71.4 27.6 43.1 25.0
NAT 61.3 50.4 52.9 69.3 26.1 41.9 24.1
ETO 64.1 52.4 56.5 72.8 28.2 43.8 25.4
StepAgent-Implicit 66.2 53.3 59.6 74.2 31.0 46.8 27.7
StepAgent-Inverse 66.5 53.6 59.7 74.9 30.8 46.6 27.5
Base 7.2 23.6 32.3 0.0 15.6 13.8 9.0
SFT 62.6 50.3 54.5 67.8 33.0 47.8 30.6
PPO 63.2 51.0 55.0 67.9 33.2 47.6 30.4
DPO 64.0 52.6 56.9 70.3 35.1 48.5 31.6
RFT 63.6 50.8 54.7 68.0 33.5 47.9 30.8
Llama3
8B SPIN 65.4 53.9 60.3 71.9 34.8 48.9 31.9
NAT 63.2 50.9 55.6 68.3 33.4 48.0 31.0
ETO 65.7 54.0 62.5 73.4 35.2 49.4 32.3
StepAgent-Implicit 67.2 55.8 63.6 75.5 38.1 51.3 34.4
StepAgent-Inverse 67.6 55.9 64.1 76.1 37.8 52.0 34.1
Table3:AblationstudieswithdifferentrewardtypesbasedonLlama38Bandinversereinforcementlearning.
RewardType WebTasks AgentTasks Question-AnsweringTasks
Method
Step Final WebShop Mind2Web ScienceWorld ALFWorld HotpotQA 2WikiMultihopQA MuSiQue
StepAgent-inverse × ✓ 65.7 54.0 62.5 73.4 35.2 49.4 32.3
StepAgent-inverse ✓ × 67.6 55.9 64.1 76.1 37.8 52.0 34.1
StepAgent-inverse ✓ ✓ 68.0 56.4 64.8 76.2 38.9 52.0 34.8
theexpertpolicydistribution,leadingtosubstantialenhancements step-wiserewardscanfacilitatetheagent’sdeeperunderstanding
intheresponsequality. andinternalizationoftheexpertpolicy’sunderlyinglogic.
(2)InversereinforcementlearningstrategyStepAgent-Inverse Inthefollowingsections,weconductseveraladditionalexperi-
withexplicitrewardsdemonstratesaslightperformanceimprove- mentstoinvestigateStepAgentindepth.
mentcomparedtoimplicit-rewardreinforcementlearningmethods
StepAgent-Implicitonmostdatasets.Thisindicatesthatexplicit
7.2 AblationStudiesonRewardType
rewardscanprovidethemodelwithmuchcleareroptimizationob-
Inthissection,weconductablationstudiestoanalyzetheinfluence
jectives,therebyfacilitatingmoreeffectiveadjustmentsinbehavior.
ofdifferentrewardtypesonourStepAgentmodel.Weinvestigate
Consequently,theclarityoftheoptimizationtargetsenablesthe
ourmodelStepAgentwiththreevariants[27,41]:(1)Step-wise
noviceagenttomoreeffectivelyapproachexpert-levelperformance.
reward,whichconstructsstep-wiserewardbyobservingandim-
(3)Interestingly,StepAgenthasachievedmoresignificantim-
itating the expert behaviors and optimizes agent strategy with
provementsonthethreemulti-hopquestion-answeringtasks.Con-
step-wise rewards, as introduced in Section 4. (2) Final reward,
cretely,StepAgentcansurpassthestate-of-the-artmodelETObyan
whichutilizesthefinalenvironmentalfeedbackastherewardfor
absolutevalueimprovementof2.9%ontheHotpotQAdataset.Since
optimization.(3)Wealsoexplorethecombinationofthetworeward
thereasoningstepsinmulti-hopquestion-answeringtasksdemon-
typestoevaluatetheirimpactontheperformanceofStepAgent.
stratecomplexsemanticrelationships(i.e.,parallelorhierarchical),
ExperimentsareconductedbasedonLlama3 andinverseRLand
itischallengingfortheagenttoeffectivelyimitatesuchcomplex 8B
wecanobtainsimilarconclusionswithothersettings.
expertpolicybasedsolelyonfinalrewardsignals(e.g.,tasksuccess
FromtheresultsinTable3,wecanobservethatoptimizingthe
orfailureorexactmatchwiththecorrectanswer).Introducing
reinforcementlearningprocesssolelywiththefinalenvironmen-
talfeedbackasrewardsresultsinperformancedegradationonallWWW’25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
80 68 44 68 44
Mistral-7B-Implicit
70 Mistral-7B-Inverse 66 42 66 WS Implicit 42
60 M Mi is st tr ra al l- -1 13 3B B- -I Im nvp el ric seit 64 W WS S I Im nvp el ric seit 40 64 W HPS II mnv pe lir cs ie t 40
50 H HP P I Im nvp el ric seit HP Inverse
62 38 62 38
40
60 36 60 36
30
20 WebShop Mind2WebSci-World ALFWorldHotpotQA 2WikiQA MuSiQue 58 1 3 5 7 9 34 58 1 2 3 4 5 34
(a) Training Iteration (b) Practice Number
Figure3:Performancewithdifferentbackbonemodelparam-
etersonalldatasets. Figure4:Performancewithdifferenttrainingiterationsand
practicenumbers.“WS”isWebShopwhile“HP”isHotpotQA.
tasks.Concretely,eliminatingstep-wiserewardcausestheobvi-
ousdroponalltasks(e.g.,WebShop:68.0→−67.2andScienceWorld: TrainingIteration.Toidentifytheoptimaliterationnumber,
64.8→−63.6).Thisindicatesthatthestep-wiserewardcanfacilitate weincreasethetrainingiterationnumberfromonetonine,while
theagent’scapabilitytoalignwiththeexpert.Meanwhile,thefinal closelymonitoringtheperformancechangesassociatedwiththe
environmentalfeedbackalsocontributestothefinalresultswhich tworeflectionmechanisms.AsdepictedinFigure4(a),theperfor-
verifiesthatacombinationofthestep-wiseandthefinalreward manceofStepAgentimprovesprogressivelyasthetrainingiteration
supervisionisbeneficial.Step-wiserewardsupervisionprovidesim- numberincreasesforbothimplicit-rewardandinversereinforce-
mediatefeedback,boostingoptimizationefficiency,whilethefinal mentlearningstrategies.However,thepeakperformanceofthetwo
supervisionofferscleardirectionfortheoveralllearningobjectives. methodsdiffers.Specifically,ontheWebShopdataset,theimplicit-
Althoughthecombinationofthetworewardtypescanleadtobet- rewardstrategyreachesthepeakafterthreeiterationswhereas
terresults,obtainingthefinalrewardnecessitatesinteractionwith theinversereinforcementlearningmethodachievesitsbestperfor-
theenvironmentwhichcannotbeparallelized.Consequently,in manceattheseveniteration.Thisindicatesthatmoreiterationsare
thispaper,weexclusivelyfocusonadoptingthestep-wisereward requiredforthemodeltocorrectlylearntheexplicitrewardfunc-
tostrikeabalancebetweenefficiencyandeffectiveness. tion,whichleadstoslowerconvergence.Besides,theperformance
startstodegradewhentheiterationsexceedthepeak.Thisphenom-
7.3 PerformancewithDifferentModelSize enoncanbeattributedtothefactthatastheagent’scapabilities
TofurtherillustratetherobustnessofStepAgent,weconductexper- improve,ourself-playmethodforgeneratingstep-wisefine-tuning
imentswithdifferentbackbonemodelparametersizes.Weutilize data may struggle to provide contrasting positive and negative
Mistral andMistral forthisanalysis.Theresultsaredepicted samples.Theabsenceofcleardistinctionsbetweensuccessfuland
7B 13B
inFigure3.“-Implicit”indicatesStepAgentwithimplicit-reward unsuccessfulbehaviorsdisruptsthelearningprocess.
reinforcementlearningstrategywhile“-Inverse”representsinverse PracticeNumber.Inthispart,weconductexperimentstoinves-
reinforcementlearningmethod.WeabbreviateScienceWorldand tigatewhetherintroducingdiverseagenttrajectoriesisbeneficial
2WikiMultihopQAasSci-Worldand2WikiQAforlimitedspace. forperformanceimprovement.Toachievethis,weforcethenovice
First,wecanobservethatStepAgentdemonstratesconsistentand agenttopracticemultipletimesforeachlearningobjectiveduring
robustefficacyacrossmodelswithdifferentparameterscales.This theinspectionphase.Figure4(b)showstheresultsoftworeflection
performancestabilityhighlightsourmodel’sadaptabilitytodiffer- variants.Wecanobservethattheresultsofbothvariantsaregrad-
entconfigurations,ensuringthatitsreliabilityinachievingeffective uallyincreasingasthepracticenumbergrowsfromonetothree.
resultsregardlessoftheparameterscaleemployed.Second,com- Thisimpliesthatintroducingmorediversetrainingsamplescan
paredwithMistral ,Mistral achievessuperiorperformances. acceleratethenovice’sacquisitionoftheexpertpolicy.However,
7B 13B
Thisindicatestheimportanceofthebackbonemodel’scapability, theperformancedoesnotincreasewhenthepracticenumberex-
asitsignificantlyinfluencestheeffectivenessofpost-imitatedlearn- ceedsthree.Apotentialexplanationisthat,atthesamecognitive
ing.TheenhancedcapacityofMistral allowsformoreeffective level,thediversityofthesamplesremainslimiteddespitemultiple
13B
learningandadaptation,contributingtoimprovedperformances. attempts.Consequently,incorporatingmoresamplesmayleadto
informationredundancy,whichcanhinderlearningefficiencyand
7.4 ExplorationofParametersSettings alsoincreasecomputationalcosts.
In StepAgent, two important hyper-parameters will impact the
experimentalperformance–thenumberoftrainingiterationsin 8 ConclusionandFutureWork
Algorithm1andthepracticenumberoftheagentduringtheinspec- Reinforcementlearninghasbecomeaneffectiveapproachforalign-
tionstageofeachiteration.Inthissection,weconductexperiments ingagentbehaviorswithhumanpreferences.However,existing
toinvestigatetheirinfluences.Werandomlyselectedtworepresen- reinforcementlearningmethodsprimarilyadoptthefinalenviron-
tativedatasetsWebShopandHotpotQAforthisexperimentandwe mentalfeedbacktooptimizetheagentstrategy.Inthispaper,in-
candrawsimilarconclusionsonotherdatasets. spiredbyBenner’snovice-to-experttheory,weproposedStepAgent,
xirteM
noitaulavEFromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWW’25,April28-May02,2025,Sydney,Australia
astep-wisereinforcementlearningframeworkwithoutstep-wise [16] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,De-
humanannotation.Intheinspectionstage,thenoviceagentfirst vendraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,
observesthebehaviorsoftheexpertandthenrehearsesthedemon- GuillaumeLample,LucileSaulnier,etal.2023. Mistral7B. arXivpreprint
arXiv:2310.06825(2023).
stratedactions.Duringthereflectionstage,theagentcomparesits [17] VijayR.KondaandJohnN.Tsitsiklis.1999.Actor-CriticAlgorithms.InAdvances
actionswiththoseoftheexpertandadjustsitspolicytobetteralign inNeuralInformationProcessingSystems12,[NIPSConference,Denver,Colorado,
USA,November29-December4,1999],SaraA.Solla,ToddK.Leen,andKlaus-
withtheexpert’spolicydistribution.Experimentalresultsacross RobertMüller(Eds.).TheMITPress,1008–1014.http://papers.nips.cc/paper/1786-
threetypesoftasksconsistentlydemonstratethesuperiorityof actor-critic-algorithms
StepAgentoverexistingbaselines.Besides,weconductadditional [18] TomaszKorbak,HadyElsahar,GermánKruszewski,andMarcDymetman.2022.
Onreinforcementlearninganddistributionmatchingforfine-tuninglanguage
experimentstofurtherillustratetheeffectivenessandefficiencyof modelswithnocatastrophicforgetting.AdvancesinNeuralInformationProcessing
StepAgent.Inthefuture,weaimtoenhanceLLMagentsbyinte- Systems35(2022),16203–16220.
[19] XiaoLiu,HaoYu,HanchenZhang,YifanXu,XuanyuLei,HanyuLai,YuGu,
gratingmoreadvancedcognitivecapabilitiestobettersatisfyuser
HangliangDing,KaiwenMen,KejuanYang,etal.2023.Agentbench:Evaluating
demandsandrespondtodynamicenvironments. llmsasagents.arXivpreprintarXiv:2308.03688(2023).
[20] AmanMadaan,NiketTandon,PrakharGupta,SkylerHallinan,LuyuGao,Sarah
Wiegreffe,UriAlon,NouhaDziri,ShrimaiPrabhumoye,YimingYang,etal.
2024. Self-refine:Iterativerefinementwithself-feedback. AdvancesinNeural
References
InformationProcessingSystems36(2024).
[21] JoshuaMaynez,ShashiNarayan,BerndBohnet,andRyanMcDonald.2020.
[1] SaurabhAroraandPrashantDoshi.2021. Asurveyofinversereinforcement Onfaithfulnessandfactualityinabstractivesummarization. arXivpreprint
learning:Challenges,methodsandprogress. ArtificialIntelligence297(2021), arXiv:2005.00661(2020).
103500. [22] YoheiNakajima.2023. https://github.com/yoheinakajima/babyagi
[2] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,Nova [23] AndrewYNg,StuartRussell,etal.2000.Algorithmsforinversereinforcement
DasSarma,DawnDrain,StanislavFort,DeepGanguli,TomHenighan,etal.2022. learning..InIcml,Vol.1.2.
Trainingahelpfulandharmlessassistantwithreinforcementlearningfrom [24] OpenAI.2022.GPT-3.5. https://openai.com/index/chatgpt/
humanfeedback.arXivpreprintarXiv:2204.05862(2022). [25] OpenAI.2024.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL] https://arxiv.
[3] PatriciaBenner.1982. Fromnovicetoexpert. AJNTheAmericanJournalof org/abs/2303.08774
Nursing82,3(1982),402–407. [26] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,Pamela
[4] PatriciaBenneretal.1984.Fromnovicetoexpert.MenloPark84,1480(1984), Mishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal.2022.
10–1097. Traininglanguagemodelstofollowinstructionswithhumanfeedback.Advances
[5] MichaelBloemandNicholasBambos.2014. Infinitetimehorizonmaximum inneuralinformationprocessingsystems35(2022),27730–27744.
causalentropyinversereinforcementlearning.In53rdIEEEconferenceondecision [27] SarahPan,VladislavLialin,SherinMuckatira,andAnnaRumshisky.2023.Let’s
andcontrol.IEEE,4911–4916. ReinforceStepbyStep.arXivpreprintarXiv:2311.05821(2023).
[6] HeejongBongandAlessandroRinaldo.2022.Generalizedresultsfortheexistence [28] DeanAPomerleau.1991. Efficienttrainingofartificialneuralnetworksfor
andconsistencyoftheMLEintheBradley-Terry-Lucemodel.InInternational autonomousnavigation.Neuralcomputation3,1(1991),88–97.
ConferenceonMachineLearning.PMLR,2160–2177. [29] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherD.Manning,Ste-
[7] CameronBrowne,EdwardJackPowley,DanielWhitehouse,SimonM.Lucas, fanoErmon,andChelseaFinn.2023. DirectPreferenceOptimization:Your
PeterI.Cowling,PhilippRohlfshagen,StephenTavener,DiegoPerezLiebana, LanguageModelisSecretlyaRewardModel.InAdvancesinNeuralInfor-
SpyridonSamothrakis,andSimonColton.2012.ASurveyofMonteCarloTree mationProcessingSystems36:AnnualConferenceonNeuralInformationPro-
SearchMethods.IEEETrans.Comput.Intell.AIGames4,1(2012),1–43. https: cessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December10-16,
//doi.org/10.1109/TCIAIG.2012.2186810 2023,AliceOh,TristanNaumann,AmirGloberson,KateSaenko,MoritzHardt,
[8] BaianChen,ChangShu,EhsanShareghi,NigelCollier,KarthikNarasimhan, andSergeyLevine(Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/
and Shunyu Yao. 2023. FireAct: Toward Language Agent Fine-tuning. a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html
CoRR abs/2310.05915 (2023). https://doi.org/10.48550/ARXIV.2310.05915 [30] ToranBruceRichards.2023. https://github.com/Significant-Gravitas/AutoGPT
arXiv:2310.05915 [31] StuartRussell.1998.Learningagentsforuncertainenvironments.InProceedings
[9] ZixiangChen,YiheDeng,HuizhuoYuan,KaixuanJi,andQuanquanGu.2024. oftheeleventhannualconferenceonComputationallearningtheory.101–103.
Self-PlayFine-TuningConvertsWeakLanguageModelstoStrongLanguage [32] JohnSchulman,SergeyLevine,PieterAbbeel,MichaelI.Jordan,andPhilipp
Models. arXiv:2401.01335[cs.LG] https://arxiv.org/abs/2401.01335 Moritz.2015. TrustRegionPolicyOptimization.InProceedingsofthe32nd
[10] ZehuiChen,KuikunLiu,QiuchenWang,WenweiZhang,JiangningLiu,Dahua InternationalConferenceonMachineLearning,ICML2015,Lille,France,6-11July
Lin,KaiChen,andFengZhao.2024.Agent-FLAN:DesigningDataandMeth- 2015(JMLRWorkshopandConferenceProceedings,Vol.37),FrancisR.Bachand
odsofEffectiveAgentTuningforLargeLanguageModels. arXivpreprint DavidM.Blei(Eds.).JMLR.org,1889–1897. http://proceedings.mlr.press/v37/
arXiv:2403.12881(2024). schulman15.html
[11] PaulF.Christiano,JanLeike,TomB.Brown,MiljanMartic,ShaneLegg,and [33] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov.
DarioAmodei.2017.DeepReinforcementLearningfromHumanPreferences.In 2017. ProximalPolicyOptimizationAlgorithms. CoRRabs/1707.06347(2017).
AdvancesinNeuralInformationProcessingSystems30:AnnualConferenceonNeu- arXiv:1707.06347 http://arxiv.org/abs/1707.06347
ralInformationProcessingSystems2017,December4-9,2017,LongBeach,CA,USA, [34] NoahShinn,FedericoCassano,AshwinGopinath,KarthikNarasimhan,and
IsabelleGuyon,UlrikevonLuxburg,SamyBengio,HannaM.Wallach,RobFergus, ShunyuYao.2024.Reflexion:Languageagentswithverbalreinforcementlearning.
S.V.N.Vishwanathan,andRomanGarnett(Eds.).4299–4307.https://proceedings. AdvancesinNeuralInformationProcessingSystems36(2024).
neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html [35] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam
[12] XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamStevens,BoshiWang, Trischler,andMatthewJ.Hausknecht.2021. ALFWorld:AligningTextand
HuanSun,andYuSu.2024.Mind2web:Towardsageneralistagentfortheweb. EmbodiedEnvironmentsforInteractiveLearning.In9thInternationalConference
AdvancesinNeuralInformationProcessingSystems36(2024). onLearningRepresentations,ICLR2021,VirtualEvent,Austria,May3-7,2021.
[13] DongyoungGo,TomaszKorbak,GermánKruszewski,JosRozen,NahyeonRyu, OpenReview.net. https://openreview.net/forum?id=0IOX0YcCdTn
andMarcDymetman.2023.Aligninglanguagemodelswithpreferencesthrough [36] YifanSong,DaYin,XiangYue,JieHuang,SujianLi,andBillYuchenLin.2024.
f-divergenceminimization.arXivpreprintarXiv:2302.08215(2023). TrialandError:Exploration-BasedTrajectoryOptimizationofLLMAgents.In
[14] JonathanHoandStefanoErmon.2016.GenerativeAdversarialImitationLearning. Proceedingsofthe62ndAnnualMeetingoftheAssociationforComputational
InAdvancesinNeuralInformationProcessingSystems29:AnnualConferenceon Linguistics(Volume1:LongPapers),Lun-WeiKu,AndreMartins,andVivek
NeuralInformationProcessingSystems2016,December5-10,2016,Barcelona,Spain, Srikumar(Eds.).AssociationforComputationalLinguistics,Bangkok,Thailand,
DanielD.Lee,MasashiSugiyama,UlrikevonLuxburg,IsabelleGuyon,and 7584–7600. https://aclanthology.org/2024.acl-long.409
RomanGarnett(Eds.).4565–4573. https://proceedings.neurips.cc/paper/2016/ [37] NisanStiennon,LongOuyang,JeffreyWu,DanielZiegler,RyanLowe,Chelsea
hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html Voss,AlecRadford,DarioAmodei,andPaulFChristiano.2020. Learningto
[15] XanhHo,Anh-KhoaDuongNguyen,SakuSugawara,andAkikoAizawa.2020. summarizewithhumanfeedback. AdvancesinNeuralInformationProcessing
ConstructingAMulti-hopQADatasetforComprehensiveEvaluationofReason- Systems33(2020),3008–3021.
ingSteps.InProceedingsofthe28thInternationalConferenceonComputationalLin- [38] UmarSyed,MichaelH.Bowling,andRobertE.Schapire.2008.Apprenticeship
guistics.InternationalCommitteeonComputationalLinguistics,Barcelona,Spain learningusinglinearprogramming.InMachineLearning,Proceedingsofthe
(Online),6609–6625. https://www.aclweb.org/anthology/2020.coling-main.580WWW’25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
Twenty-FifthInternationalConference(ICML2008),Helsinki,Finland,June5- conditionalneuralsequencegeneration.arXivpreprintarXiv:2011.02593(2020).
9,2008(ACMInternationalConferenceProceedingSeries,Vol.307),WilliamW. [55] YujiaZhou,ZhichengDou,andJi-RongWen.2023.Enhancinggenerativeretrieval
Cohen,AndrewMcCallum,andSamT.Roweis(Eds.).ACM,1032–1039. https: withreinforcementlearningfromrelevancefeedback.InProceedingsofthe2023
//doi.org/10.1145/1390156.1390286 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.12481–12490.
[39] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yas- [56] YujiaZhou,ZhengLiu,JiajieJin,Jian-YunNie,andZhichengDou.2024.Metacog-
mineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhos- nitiveretrieval-augmentedlargelanguagemodels.InProceedingsoftheACMon
ale,etal.2023. Llama2:Openfoundationandfine-tunedchatmodels. arXiv WebConference2024.1453–1463.
preprintarXiv:2307.09288(2023). [57] DanielM.Ziegler,NisanStiennon,JeffreyWu,TomB.Brown,AlecRadford,Dario
[40] HarshTrivedi,NiranjanBalasubramanian,TusharKhot,andAshishSabharwal. Amodei,PaulF.Christiano,andGeoffreyIrving.2019.Fine-TuningLanguage
2022. MuSiQue:MultihopQuestionsviaSingle-hopQuestionComposition. ModelsfromHumanPreferences.CoRRabs/1909.08593(2019).arXiv:1909.08593
TransactionsoftheAssociationforComputationalLinguistics(2022). http://arxiv.org/abs/1909.08593
[41] JonathanUesato,NateKushman,RamanaKumar,FrancisSong,NoahSiegel,
LisaWang,AntoniaCreswell,GeoffreyIrving,andIrinaHiggins.2022.Solving
mathwordproblemswithprocess-andoutcome-basedfeedback.arXivpreprint
arXiv:2211.14275(2022).
[42] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,
A Prompts
ZhiyuanChen,JiakaiTang,XuChen,YankaiLin,WayneXinZhao,ZheweiWei,
andJirongWen.2024. Asurveyonlargelanguagemodelbasedautonomous
A.1 WebShop
agents.FrontiersofComputerScience18,6(March2024). https://doi.org/10.1007/
s11704-024-40231-1
[43] Ruoyao Wang, Peter A. Jansen, Marc-Alexandre Côté, and Prithviraj Am- TaskInstructionforWebShop
manabrolu.2022. ScienceWorld:IsyourAgentSmarterthana5thGrader?.
InProceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguage Youarewebshopping.Iwillgiveyouinstructionsabout
Processing,EMNLP2022,AbuDhabi,UnitedArabEmirates,December7-11,2022,
what to do. You have to follow the instructions. Every
YoavGoldberg,ZornitsaKozareva,andYueZhang(Eds.).AssociationforCom-
putationalLinguistics,11279–11298. https://doi.org/10.18653/V1/2022.EMNLP- roundIwillgiveyouanobservationandalistofavailable
MAIN.775 actions,youhavetorespondanactionbasedonthestate
[44] RenxiWang,HaonanLi,XudongHan,YixuanZhang,andTimothyBaldwin.
2024.LearningFromFailure:IntegratingNegativeExampleswhenFine-tuning and instruction. You can use search action if search is
LargeLanguageModelsasAgents.arXivpreprintarXiv:2402.11651(2024). available.Youcanclickoneofthebuttonsinclickables.
[45] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,
Anactionshouldbeofthefollowingstructure:
QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsreasoning
inlargelanguagemodels.Advancesinneuralinformationprocessingsystems35 search[keywords]
(2022),24824–24837. click[value]
[46] ZhihengXi,WenxiangChen,XinGuo,WeiHe,YiwenDing,BoyangHong,
Iftheactionisnotvalid,performnothing.Keywordsin
MingZhang,JunzheWang,SenjieJin,EnyuZhou,RuiZheng,XiaoranFan,Xiao
Wang,LimaoXiong,YuhaoZhou,WeiranWang,ChanghaoJiang,YichengZou, search are up to you, but the value in click must be a
XiangyangLiu,ZhangyueYin,ShihanDou,RongxiangWeng,WensenCheng, valueinthelistofavailableactions.Rememberthatyour
QiZhang,WenjuanQin,YongyanZheng,XipengQiu,XuanjingHuang,andTao
Gui.2023. TheRiseandPotentialofLargeLanguageModelBasedAgents:A keywordsinsearchshouldbecarefullydesigned.
Survey. arXiv:2309.07864[cs.AI] https://arxiv.org/abs/2309.07864 Yourresponseshouldusethefollowingformat:
[47] ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamW.Cohen,Ruslan
Thought:Ithink...
Salakhutdinov,andChristopherD.Manning.2018. HotpotQA:ADatasetfor
Diverse,ExplainableMulti-hopQuestionAnswering.InProceedingsofthe2018 Action:search[something]
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Brussels,Belgium,
October31-November4,2018,EllenRiloff,DavidChiang,JuliaHockenmaier,
andJun’ichiTsujii(Eds.).AssociationforComputationalLinguistics,2369–2380.
https://doi.org/10.18653/V1/D18-1259
[48] ShunyuYao,HowardChen,JohnYang,andKarthikNarasimhan.2022.WebShop: A.2 Mind2Web
TowardsScalableReal-WorldWebInteractionwithGroundedLanguageAgents.
InAdvancesinNeuralInformationProcessingSystems35:AnnualConferenceon TaskInstructionforMind2Web
NeuralInformationProcessingSystems2022,NeurIPS2022,NewOrleans,LA,USA,
November28-December9,2022,SanmiKoyejo,S.Mohamed,A.Agarwal,Danielle Youareahelpfulassistantthatisgreatatwebsitedesign,
Belgrave,K.Cho,andA.Oh(Eds.). http://papers.nips.cc/paper_files/paper/2022/
navigation,andexecutingtasksfortheuser.
hash/82ad13ec01f9fe44c01cb91814fd7b8c-Abstract-Conference.html
[49] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,TomGriffiths,YuanCao,and User:"<html><div><div><atockhomepage/><but-
KarthikNarasimhan.2024.Treeofthoughts:Deliberateproblemsolvingwith tonid=0bookareservation.toggleopen><span>Book
largelanguagemodels. AdvancesinNeuralInformationProcessingSystems36
areservation</span></button><buttonbookareserva-
(2024).
[50] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikR.Narasimhan, tion.toggleopen></button></div><div><selectid=1
andYuanCao.2023. ReAct:SynergizingReasoningandActinginLanguage type><optionreservationstrue>Dinein</option><op-
Models.InTheEleventhInternationalConferenceonLearningRepresentations,
ICLR2023,Kigali,Rwanda,May1-5,2023.OpenReview.net. https://openreview. tionpickup>Pickup</option><optiondelivery>Deliv-
net/pdf?id=WE_vluYUL-X ery </option> <option events> Events </option> <op-
[51] DaYin,FaezeBrahman,AbhilashaRavichander,KhyathiChandu,Kai-WeiChang,
tion wineries> Wineries </option> <option all> Every-
YejinChoi,andBillYuchenLin.2024. AgentLumos:UnifiedandModular
TrainingforOpen-SourceLanguageAgents. arXiv:2311.05657[cs.AI] https: thing </option> </select> <div id=2> <p> Celebrating
//arxiv.org/abs/2311.05657 andsupportingleadingwomenshakinguptheindustry.
[52] ZhengYuan,HongyiYuan,ChengpengLi,GuantingDong,ChuanqiTan,and
</p><span>Explorenow</span></div></div></div>
ChangZhou.2023.ScalingRelationshiponLearningMathematicalReasoning
withLargeLanguageModels.CoRRabs/2308.01825(2023). https://doi.org/10. </html>"BasedontheHTMLwebpageabove,trytocom-
48550/ARXIV.2308.01825arXiv:2308.01825 pletethefollowingtask:
[53] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong,
andJieTang.2023. AgentTuning:EnablingGeneralizedAgentAbilitiesfor Task:CheckforpickuprestaurantavailableinBoston,NY
LLMs.CoRRabs/2310.12823(2023). https://doi.org/10.48550/ARXIV.2310.12823 onMarch18,5pmwithjustoneguest
arXiv:2310.12823
Previousactions:None
[54] ChuntingZhou,GrahamNeubig,JiataoGu,MonaDiab,PacoGuzman,Luke
Zettlemoyer,andMarjanGhazvininejad.2020.DetectinghallucinatedcontentinFromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWW’25,April28-May02,2025,Sydney,Australia
Whatshouldbethenextaction?Pleaseselectfromthe </div></div></li><divid=1>EnterpriseFleetManage-
followingchoices(Ifthecorrectactionisnotinthepage ment</div></ul></nav><divregion><buttonid=2se-
above,pleaseselectA.’Noneoftheabove’): lectedpick-update03/19/2023><span><span>19</span>
A.Noneoftheabove <div><span>Mar</span><span>2023</span></div>
B.<buttonid=0bookareservation.toggleopen><span> </span></button></div></div></html>"Basedonthe
Booka HTMLwebpageabove,trytocompletethefollowingtask:
C.<selectid=1type><optionreservationstrue>Dinein Task:FindaminivanatBrooklynCityfromApril5thto
</option><option April8thfora22yearoldrenter.
D. <div id=2> <p> Celebrating and supporting leading Previousactions:[searchbox]Pick-up&ReturnLocation
womenshakingup (ZIP,CityorAirport)(...->TYPE:Brooklyn[option]Brook-
Assistant:Answer:C.Action:SELECT,Value:Pickup lyn,NY,USSelect->CLICK
User: "<html> <div> <main main> <section tabpanel> Whatshouldbethenextaction?Pleaseselectfromthe
<div><ultablist><litabheadinglevel3searchand></li> followingchoices(Ifthecorrectactionisnotinthepage
<li id=0 tab heading level 3 search and> <span> Hotel above,pleaseselectA.’Noneoftheabove’):
</span> </li> <li tab heading level 3 search and> </li> A.Noneoftheabove
<litabheadinglevel3searchand></li></ul><divtab- B.<divid=0><div><div><div>BuyACar</div><div>
panel><divid=1><div><span>Dates*</span><button C.<divid=1>EnterpriseFleetManagement</div>
buttoncleardates/></div><div><label>Travelers</la- D.<buttonid=2selectedpick-update03/19/2023><span>
bel> <div> <p> 1 Adult </p> <button button> 1 Adult <span>19</span>
</button><divdialog><buttonbuttontravelwithapet. Assistant:Answer:D.Action:CLICK
this><span>Travelwithapet</span></button><div>
<buttonbuttonclearallfields>Clearall</button><but-
tonbutton></button></div></div></div></div></div>
</div></div></section></main><footercontentinfo> A.3 ScienceWorld
<div><h3>StayConnected</h3><ulid=2><amobile TaskInstructionforScienceWorld
tools></a><aopenunited’stiktokfeedin></a><aopen
Youareahelpfulassistanttodosomescientificexperiment
united’sfacebookpagein></a><aopenunited’stwitter
inanenvironment.Intheenvironment,thereareseveral
feedin></a><aopenunited’syoutubepagein></a><a
rooms: kitchen, foundry, workshop, bathroom, outside,
openunited’sinstagramfeedin></a><aopenunited’s
livingroom,bedroom,greenhouse,artstudio,hallwayYou
linkedinprofilein></a></ul></div></footer></div>
shouldexploretheenvironmentandfindtheitemsyou
</html>"BasedontheHTMLwebpageabove,trytocom-
needtocompletetheexperiment.Youcanteleporttoany
pletethefollowingtask:
roominonestep.Allcontainersintheenvironmenthave
Task:Comparethefaretypestobooka1-adultticketfrom
alreadybeenopened,youcandirectlygetitemsfromthe
Springfiels,ILtoAustin,TXforApril29th2023
containers.Theavailableactionsare:
Previousactions:[combobox]Enteryourdepartingcity,
openOBJ:openacontainer
airportname,orairpor...->TYPE:SPRINGFIELD[button]
closeOBJ:closeacontainer
Springfield,IL,US(SPI)->CLICK[combobox]Enteryour
activateOBJ:activateadevice
destinationcity,airportname,orairp...->TYPE:AUSTIN
deactivateOBJ:deactivateadevice
[button]Austin,TX,US(AUS)->CLICK
connectOBJtoOBJ:connectelectricalcomponents
Whatshouldbethenextaction?Pleaseselectfromthe
disconnectOBJ:disconnectelectricalcomponents
followingchoices(Ifthecorrectactionisnotinthepage
useOBJ[onOBJ]:useadevice/item
above,pleaseselectA.’Noneoftheabove’):
lookaround:describethecurrentroom
A.Noneoftheabove
examineOBJ:describeanobjectindetail
B.<liid=0tabheadinglevel3searchand><span>Hotel
lookatOBJ:describeacontainer’scontents
C.<divid=1><div><span>Dates*</span><buttonbutton
readOBJ:readanoteorbook
cleardates
moveOBJtoOBJ:moveanobjecttoacontainer
D.<ulid=2><amobiletools></a><aopenunited’stiktok"
pickupOBJ:moveanobjecttotheinventory
Assistant:Answer:A.
pourOBJintoOBJ:pouraliquidintoacontainer
User:"<html><div><navmainmenu><ul><li><div
mixOBJ:chemicallymixacontainer
button>CarSales</div><divid=0><div><div><div>
teleporttoLOC:teleporttoaspecificroom
BuyACar</div><div>PlanYourPurchase</div></div>
focusonOBJ:signalintentonataskobject
<div><h4>ItsTaxRefundTime.TreatYourselftoanUp-
wait:tasknoactionfor10steps
grade.</h4><p>Withavarietyofoptions,investyour
wait1:tasknoactionforastep
refundinwhatyoureallywant-aquality,usedvehicle
Yourresponseshouldusethefollowingformat:
from Enterprise. </p> <a> View Inventory </a> </div>WWW’25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
A.5 HotpotQA,2WikimultihopQAandMusique
Thought:Ithink...
Action:openOBJ TaskInstructionforMultihop-QADatasets
Youareanexpertinthisfield.Pleaseanswerthequestion
assimplyandconciselyaspossible.EveryroundIwillgive
A.4 ALFWorld youanobservation,youhavetorespondwithinterleaving
ThoughtandActionsteps.Thoughtcanreasonaboutthe
TaskInstructionforALFWorld
currentsituation,andActioncanbetwotypes:
Interact with a household to solve a task. Imagine you (1) Search[entity], which searches the exact entity on
areanintelligentagentinahouseholdenvironmentand Wikipediaandreturnsthefirstparagraphifitexists.If
yourtargetistoperformactionstocompletethetaskgoal. not,itwillreturnsomesimilarentitiestosearch.
Atthebeginningofyourinteractions,youwillbegiven (2)Finish[answer],whichreturnstheanswerandfinishes
thedetaileddescriptionofthecurrentenvironmentand thetask.
yourgoaltoaccomplish.Foreachofyourturn,youwill Yourresponseshouldusethefollowingformat:
begiventheobservationofthelastturn.Youshouldfirst Thought:Ithink...
thinkaboutthecurrentconditionandplanforyourfuture Action:...
actions,andthenoutputyouractioninthisturn.
Theavailableactionsare:
1.gotorecep
2.taskobjfromrecep
3.putobjin/onrecep
4.openrecep
5.closerecep
6.toggleobjrecep
7.cleanobjwithrecep
8.heatobjwithrecep
9.coolobjwithrecep
whereobjandrecepcorrespondtoobjectsandreceptacles.
Afteryoureachturn,theenvironmentwillgiveyouim-
mediatefeedbackbasedonwhichyouplanyournextfew
steps.iftheenvrionmentoutput"Nothinghappened",that
meansthepreviousactionisinvalidandyoushouldtry
moreoptions.
Yourresponseshouldusethefollowingformat:
Thought:<yourthoughts>
Action:<yournextaction>