JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
GUIDE-VAE: Advancing Data Generation with
User Information and Pattern Dictionaries
Kutay Bo¨lat, Simon H. Tindemans
Abstract—Generative modelling of multi-user datasets has
become prominent in science and engineering. Generating a
data point for a given user requires employing user informa-
tion, and conventional generative models, including variational
autoencoders (VAEs), often ignore that. This paper introduces
GUIDE-VAE,anovelconditionalgenerativemodelthatleverages
user embeddings to generate user-guided data. By allowing the
modeltobenefitfromsharedpatternsacrossusers,GUIDE-VAE
enhances performance in multi-user settings, even under signifi-
cantdataimbalance.Inadditiontointegratinguserinformation,
GUIDE-VAEincorporatesapatterndictionary-basedcovariance
composition(PDCC)toimprovetherealismofgeneratedsamples
by capturing complex feature dependencies. While user embed-
dings drive performance gains, PDCC addresses common issues
such as noise and over-smoothing typically seen in VAEs.
Fig. 1. Conventional generative models disregard user information during
The proposed GUIDE-VAE was evaluated on a multi-user
training, treating the dataset anonymously, which limits their ability to
smartmeterdatasetcharacterizedbysubstantialdataimbalance generatedataforspecificusersduringinference.GUIDE-VAEaddressesthis
across users. Quantitative results show that GUIDE-VAE per- by incorporating user information in the training process, enabling control
forms effectively in both synthetic data generation and missing overthegeneratedoutputsforindividualusers.
recordimputationtasks,whilequalitativeevaluationsrevealthat
GUIDE-VAEproducesmoreplausibleandlessnoisydata.These
results establish GUIDE-VAE as a promising tool for controlled, Variationalautoencoders(VAEs)[4],alongwithconditional
realistic data generation in multi-user datasets, with potential
VAEs(CVAEs)[5],arewidelyadoptedgenerativemodelsthat
applications across various domains requiring user-informed
modelling. use probabilistic encoders and decoders. These models extend
variational inference methods through the power of deep
Index Terms—Covariance structure, generative modelling,
neuralnetworks,offeringnotableadvantages,e.g.efficientdata
multi-user datasets, user embedding, variational autoencoders
encodingandastrongprobabilisticfoundation.However,simi-
lar to many generative models, VAEs have certain limitations.
I. INTRODUCTION One of the well-known drawbacks is that they can produce
synthetic samples that lack fine-grained detail, often resulting
The landscape of generative artificial intelligence grows at
inblurryornoisyoutputs[6].Thisissueisparticularlystriking
anever-increasingpacelikeneverseenbefore.Theunderlying
whenthedatasetexhibitsstrongcorrelationsbetweenfeatures,
generative models get larger in size and capacity every year
as seen in time-series data, where failing to capture these
to generate novel content (text, images, audio, etc.) with
correlations can significantly reduce the realism of generated
a close-to-human-level performance, if not better. Beyond
samples.
content creation, generative models are now being leveraged
Multi-userdatasets,suchassmartmeterreadings[7],patient
by scientists, engineers, and businesses for applications such
health records [8], and user interaction data from digital plat-
as synthetic data generation [1] and scientific discovery [2].
forms [9], consist of records from multiple individuals, with
These applications often require some control over the gen-
eachusercontributingdatathatreflectstheiruniquebehaviours
eration mechanism to preserve the utility of the generated
and preferences. These datasets are crucial for capturing user-
samples. Conditional generative models offer such control by
level patterns, like individual energy consumption [10] or
introducingdesiredconditionstothegenerativeprocess.While
patient treatment responses [8], which are informative for
contemporary large language models incorporate text prompts
personalized recommendations and targeted solutions. In this
for this purpose, the datasets from various scientific domains
context,syntheticdatagenerationenablestheanalysisandsim-
(measurements, logs, surveys, etc.) may require more tailored
ulationofthesetrends[11].However,modellingthesedatasets
conditions [3].
presents significant challenges: training separate models for
each user is inefficient and impractical, while anonymizing
TheauthorsarewiththeDepartmentofElectricalSustainableEnergy,Delft
UniversityofTechnology,Mekelweg4,2628CDDelft,TheNetherlands. user identities leads to a loss of control over generating the
This research was undertaken as part of the InnoCyPES project, which specific patterns unique to each user, as shown in Fig. 1.
hasreceivedfundingfromtheEuropeanUnion’sHorizon2020researchand
One well-known data mining application parallel to multi-
innovation programme under the Marie Skłodowska-Curie grant agreement
No956433. user datasets is topic modelling in textual data. This tech-
4202
voN
6
]GL.sc[
1v63930.1142:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
nique analyzes a corpus of documents, each consisting of 2) Lack of Realism: VAEs utilize two probabilistic distri-
multiple words, and models the relationships between them butions during the generative process: the likelihood and the
by assigning abstract topics to documents. Latent Dirichlet priordistributions.Acommonlyusedlikelihooddistributionis
allocation (LDA) [12] is a widely used method for topic the multivariate Gaussian with a diagonal covariance matrix,
modelling, where it effectively captures the latent structure where the model assumes independence between features for
within the data. The resemblance between a document cor- a given latent variable [4]. While this assumption simplifies
pus and a multi-user dataset—where documents correspond the modelling process, it often leads to poor sample quality.
to users and words to individual data records—makes the Specifically,bytreatingfeaturesasindependent,VAEstendto
application of LDA in multi-user domains both intuitive and producenoisydata,asitfallsoutsidethemanifoldoftheorigi-
powerful. This analogy enables LDA to model the structure naldata[15].Thisproblemisparticularlysignificantfortime-
of multi-user datasets by grouping similar users based on series data generation, where capturing correlations between
their data characteristics. In [10], this approach is applied sequential data points is crucial for preserving informative
to smart meter data, yielding useful insights into consumer patterns, such as peaks and cycles in energy consumption
behaviour and consumption patterns. The ability of LDA to patterns.
reveal such underlying patterns makes it a promising tool A common workaround to improve sample quality is using
for extracting valuable information from multi-user datasets themeanvectorasthegeneratedsample,whichoftenresultsin
in diverse applications. overly smooth (“blurry”) outputs. These two options for the
generative process impose a significant limitation on VAEs
in generating realistic data, especially for applications that
A. Problem Statement
require fine-grained detail.
This work addresses two main challenges: (i) the absence Onenaturalquestionarises:whynotemployfullcovariance
of user-specific guidance during the generative process and matrices to better capture feature dependencies? While full
(ii) the lack of realism in the generated data. The following covariance matrices could, in theory, solve this issue, they
subsections delve deeper into these issues. introduce several practical challenges:
1) Lack of Guidance: Efficient application of conventional • Parameterization: Full covariance matrices are more dif-
generative models to multi-user datasets remains limited. ficult to parameterize as an output of a neural network,
These datasets contain user-specific information, which, if requiring elaborate mechanisms to ensure positivity.
overlooked, leads to synthetic data that fails to reflect indi- • Singularity: Full covariance matrices are prone to singu-
vidual patterns and behaviours. This problem becomes even lar solutions, particularly during training, making them
more severe in the presence of data imbalance, where users harder to constrain compared to their diagonal variants.
withlowerdatacountsareunder-represented[8],andpossibly
In [15] and [16], these problems are addressed by using
hinder fairness [13]. Moreover, without incorporating user
Cholesky and low-rank decompositions, respectively. How-
information,controllingthegenerativeprocesstoproducedata
ever, these require either sparsity assumptions [15] or re-
foraspecificuserbecomeshighlychallengingandmayrequire
gularization on the objective function [16], which limits
complex, ad-hoc methods [14].
the representative power. These challenges have limited the
The key issue lies in the absence of a mechanism to
adoption of full covariance matrices in VAEs, leaving a gap
represent user identities as mathematical objects that can be
in the literature and hindering progress toward generating
usedasconditionsduringthegenerativeprocess.Withoutsuch
high-quality, realistic synthetic data, particularly in fields that
a representation, generative models cannot efficiently encode
require accurate temporal or feature-based dependencies.
or utilize the user-specific information required to guide data
generation. A similar problem is encountered in the neural
B. Main Contributions
collaborative filtering field where the user and the items they
interact with are related to each other [9]. This requires In this work, we introduce Generalized User-Informed
representing users as inputs to neural networks. However, this DictionaryEnhancedVAE(GUIDE-VAE),anovelframework
is generally done by using one-hot encoding, which limits the that advances generative modelling for multi-user datasets
scalability and generalization. through four main contributions: (i) a user-guided data gen-
To address this challenge, user embeddings that represent eration process enabled by user embeddings, (ii) a novel
users based on their data offer a promising solution. These covariance structure (PDCC) that enhances sample realism,
embeddings map users into a metric space where their simi- and the applications of GUIDE-VAE for (iv) synthetic multi-
laritiescanbequantified[10],allowingthegenerativemodelto user dataset generation and (iv) missing record imputation
condition on these embeddings and generate data that reflects under data imbalance.
user-level patterns. However, conventional generative models (i) Userembeddingsforgenerativemodelling:Wepropose
donotemploysuchanapproach,leavingagapintheirability a novel methodology to condition generative models on
to accurately condition on user identities. The absence of this user identities by employing user embeddings. LDA is
capability not only limits the precision of the generated data utilized to create automated embeddings, transforming
butalsohindersthepotentialforinter-userknowledgetransfer, users into vectors in a metric space that captures the
which could improve both the quality and variability of the similarities between them. These embeddings serve as
synthetic outputs. conditions in a conditional generative model (in thisJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
case, a CVAE), enabling a user-guided data generation e.g. T=24 corresponds to daily profiles for hourly sampled
process. This model (GUIDE-VAE) allows the model to time series. These user datasets can be pooled (anonymized)
generate data specific to individual users. To the best of under X = {x |x ∈ (cid:83)U X } where U is the total
i i u=1 u
our knowledge, this is the first instance of data-driven number of users. Note that each element x has a one-to-one
i
user-information integration being applied to generative correspondence to a user-indexed data point x .1 We also
un
modelling. assume that pieces of contextual information c assigned to
un
(ii) Novelcovariancestructureandimprovedrealism:We each data point x , which can be used as conditions for the
un
introduce a new covariance matrix construction method, generative modelling as described in following sections.
PDCC, which is applied in the GUIDE-VAE likelihood
distribution to enhance sample realism. By learning de-
B. Latent Dirichlet Allocation
pendency patterns among features, PDCC effectively
mitigates the noisy sample issue commonly encountered LDA[12]isavariationalmethodfortopicdiscoveryapplied
inVAEswhileensuringpositivedefinitenessandavoiding to collections of discrete data. It is essentially a multi-level
singularity problems without sparsity and regularization Bayesian model that exploits the occurrence frequencies of
assumptions. This method maintains generalizability and data classes (words) in data collections (documents) to cluster
offers a scalable solution for realistic data generation. To both of these as mixture models. The mixture components
thebestofourknowledge,thisisthefirsttimesuchafull are conventionally named as topics, and a topic vector can be
covariancematrixparameterizationhasbeensuccessfully assigned to a given document using the posterior distribution
implemented in this context. of the model. In our context, LDA will allow us to discover
(iii) Synthetic multi-user time series data generation: We latent structures in multi-user time-series data, enabling us to
propose a novel approach to synthetic data generation model user profiles as distributions over latent topics.
thatexplicitlyaddressesthemulti-usernatureofdatasets. Here, we define a dataset consisting of U documents
GUIDE-VAE enables the generation of realistic time- W = {W }U where W = {w }Nu , and each w ∈
u u=1 u un n=1 un
series data across multiple users by conditioning on user {1,...W} represents a discrete variable out of W classes.2
embeddings, significantly improving modelling perfor- Sincethisstudyutilizesonlythetopicsassignedtodocuments,
mance compared to conventional unguided CVAEs. This the originally proposed Bayesian model in [12] is reduced to
approachoffersanewsolutionforgeneratingdiverseand a two-level hierarchy as
realistic synthetic time series data tailored to individual (cid:89) (cid:89)
p(W,θ,β;α,η)= p(W |θ ,β )p(θ ;α)p(β ;η)
users, addressing a key limitation in current generative u u k u k
models by incorporating the multi-user aspect. k∈[K]u∈[U]
(cid:89)(cid:89) (cid:88)
(iv) Inter-user information transfer and imputation: We = M(W | θ(k)β ;N )D(θ ;α)D(β ;η)
u u k u u k
introduce the problem of missing record imputation in k u k∈[K]
multi-userdatasetsunderdataimbalance,wherethenum- (1)
ber of missing records varies among users. GUIDE-VAE
where K is the number of topics. Here, each θ ∈ ∆K−1
leverages inter-user knowledge transfer through embed- u
represents the document-topic weights for the uth document
dings to mitigate this issue, utilizing data from similar
and is Dirichlet distributed with concentration parameter α>
users to improve imputation accuracy. As a result, the
0. Similarly, the word weights for each topic k, β ∈∆W−1,
model effectively handles users with fewer data points, k
is Dirichlet distributed with the concentration parameter η >
naturally improving imputation performance without be-
0. Lastly, each document W has a multinomial distribution
ing explicitly designed to address data imbalance. u
with N number of trials and an event probabilities vector of
We evaluated GUIDE-VAE using a multi-user smart meter u
(cid:80)Kθ(k)β ∈∆W−1.
dataset to demonstrate its performance and added benefits. To k k
Intuitively,thismodeldescribesagenerativeprocesstocre-
simulate the data imbalance, we created an enrollment model
ate documents as bags of words according to some latent top-
where users enrol in the data collection system randomly late,
ics. In order to reverse this process to find the corresponding
reflecting real-world conditions. In this use case, the dataset
topicweightsofagivendocument,onemustacquiretheposte-
owner (e.g., a utility company) aims to (1) create a synthetic
riordistributionp(θ |W ;α,η)∝p(W |θ ;η)p(θ ;α)which
versionofthedatasetand(2)completethedatasetbyimputing u u u u u
is inherently intractable. Instead, using variational inference,
the missing records for late-enrolling users.
this posterior can be approximated as q(θ ;γ ) = D(θ ;γ )
u u u u
II. PRELIMINARIES by estimating the parameter γ u after the convergence of the
proposed expectation-maximization algorithm in [12]. This
A. Multi-user Time-series Profiles
variational inference from documents to the posterior parame-
We consider datasets which consist of a collection of
ters γ is depicted as the core of the user embedding pipeline
u
regularly and synchronously sampled time series profile data
visualized in Fig. 2.
from multiple sources (users, for this study), as can be seen
i wn hF ei rg e. 2 x. We =den (cid:104)o xt (e t)t (cid:105)h Te uth ∈use Xr’ Ts d ⊆atas Ret Ta .s HX eu re=
,
N{x un is}N n t= hu 1
e
21 CO on ne vm ena tp iop nin ag llyc ,a tn heb se ei c= lassn es+ c(cid:80) orruu e′− s= p1 o0 nN du to′, aN ctu0 a=
l
w0 o.
rds. We keep them
un un t=1 u as integers for mathematical convenience. Note that the magnitude of these
number of recorded profiles, and T is the profile length, integervaluesdoesnotconveyanyinformation.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
Fig.2. Theuserembeddingframeworkformulti-usertime-seriesdatasets.Users’time-seriesdataaresegmentedintoprofilesandclusteredusingk-means.
Theseclustersaretreatedaswords,anduserdatasetsaretreatedasdocumentsforLDA.Aftertraining,LDAproducesauserdictionaryΓ,whereeachelement
correspondstotheparametersofaDirichletdistribution,servingasuserembeddings.
C. Variational Autoencoders such as dimensionality reduction and clustering. This study is
mainly interested in the first functionality of the VAEs, and
VAEs [4] are generative models that extend the variational
weinviteresearcherstoinvestigatetheeffectsoftheproposed
Bayesian inference methodology to realms of deep learning
methods on the latent variables.
and they operate by using three key distributions: the like-
lihood p (x|z), the approximate posterior q (z|x), and the 1) Conditional variational autoencoders: One natural ex-
ψ ϕ
prior p(z). Here, z is a latent variable while x is an observed tension of VAEs is their conditional counterparts, CVAEs
one. Even though the family selection of these distributions [5]. These generative models aim to model the conditional
is very flexible, it is common to choose the distributions as distributionp ψ(x|c),insteadofp ψ(x)asinconventionalVAEs
multivariate normal distributions as follows: as explained above. These conditions, c, are generally known
aspects of the random variables, such as labels and assigned
p (x|z)=N(x;µ=fµ(z);Σ=diag(fσ(z))), (2)
ψ ψ ψ contextual information pieces. Thanks to this additional infor-
q (z|x)=N(z;µ=fµ(x);Σ=diag(fσ(x))), (3) mation,CVAEscanlearntogeneratedatabelongingtoagiven
ϕ ϕ ϕ
condition, xˆ|c ∼ p (x|c), which provides additional control
p(z)=N(z;µ=0,Σ=I). (4) ψ
on the synthetic data generation process.
Here, the neural networks {f ψµ,f ψσ} and {f ϕµ,f ϕσ} form the Introducing the conditions into the VAEs requires only a
VAE’s decoder and encoder neural networks, whose parame- minimal adjustment to the setting and training. Derivation
tersarecollectedinψ andϕ,respectively.Aftersettingupthe
of ELBO remains the same except the structures of the
distributions and neural networks, the VAE can be trained to distributions which take the form p (x|z,c) and q (z|x,c).3
ψ ϕ
maximize the evidence lower bound (ELBO) as Thus,afterconcatenatingtheconditionsintotheobservedand
ψ∗,ϕ∗ =argmax
(cid:88)
E
(cid:20) logp ψ(x i|z)p(z)(cid:21) latent variables as inputs of encoder and decoder networks,
qϕ(z|xi) q (z|x ) respectively, (5) can be used for the CVAE training. The
ψ,ϕ xi∈X ϕ i corresponding overall structure is depicted in Fig. 3.4
=argmax
(cid:88)
(cid:80)N
l
MC
logp ψ(x i|zˆ il)
−D (q (z|x )∥p(z))
NMC KL ϕ i
ψ,ϕ xi∈X
III. METHODOLOGY
(5)
where zˆ ∼ q (z|x ) is the lth sample of the posterior The proposed GUIDE-VAE framework combines two core
il ϕ i
distributionforagivendatapointx andNMCisthenumberof components: user embeddings extracted via LDA for condi-
i
Monte Carlo samples to approximate E [logp (x |z)]. tioning on user-specific patterns and a CVAE enhanced with
qϕ(z|xi) ψ i
Since the ELBO objective is a lower bound for the ex- PDCC to capture feature dependencies and improve realism.
act log-likelihood logp (X) [4], maximizing it equips the Together, these components enable GUIDE-VAE to generate
ψ
VAE with two main functionalities. Firstly, one can take a personalized, high-quality time-series data.
sample from the marginal distribution model, xˆ ∼ p (x),
ψ
through ancestral sampling as xˆ ∼ p ψ(x|zˆ), zˆ ∼ p(z), since 3Theoretically, the prior also takes the form of p(z|c), but it usually left
p (x)=E [p (x|z)]. This provides the ability to generate untouchedduetotheextraamortizationload.
ψ p(z) ψ
unlimited data points and create synthetic datasets. Secondly, 4Note that this structure applies only to training. During the inference
for synthetic data generation, one does not aim to reconstruct a data point
the latent variables conditioned on observed data through
buttogenerateit.Thismeansapplyingancestralsamplingtogetherwiththe
the posterior distribution can be used in various applications, conditionsasxˆ|c∼p ψ(x|zˆ,c), zˆ∼p(z).JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
Fig.3. OverallcomputationaldiagramofGUIDE-VAE.GUIDE-VAEisaCVAE-basedmodelenhancedwithalearnablepatterndictionaryforPDCC,which
captures feature dependencies for improved realism. The user of the data point is selected from the user dictionary, and a sample from the corresponding
probabilisticembeddingisconcatenatedwithauxiliaryconditions(e.g.,metadataortimestamps)andappliedtoboththeencoderanddecoder.
A. Guidance: LDA-based user-embedding CVAEs.Thiscanbedonebyfindingakernelmeanembedding
[17] of the Dirichlet distribution and mapping the user distri-
AsitisdescribedinSectionII-B,LDAassignstopicweights
butions into vectors. However, this is mathematically cumber-
to each word in a vocabulary and uses these assignments to
some and outside of this study’s scope. Instead, a sampling-
calculate the posterior of a given document. Therefore, after
basedalternativemethodisproposedwhereeachdistributionis
training it, LDA can be used to map documents to Dirich-
representedwithacollectionof“uservectors”θˆ ∼Dir(γ ).
let distributions. This resembles the desired user-embedding us u
This sampling-based approach not only simplifies the process
scheme since documents are also non-mathematical entities
but also captures the variability and uncertainty inherent in
with varying sizes of (text-based) datasets. Following this
user distributions, making it a practical choice for large-scale
resemblance,LDAcanalsobeappliedtomulti-usertimeseries
datasets. In practice, if the users are revisited indefinitely like
datasets. However, most time series datasets do not consist of
in a neural network training, user vectors can be sampled per
words or discrete features in a finite domain, e.g. categories.
visit and storing only the user dictionary Γ is sufficient.
Instead, they tend to have records in continuous domains. In
[10], a data preprocessing pipeline is proposed to apply LDA
to a non-textual time series dataset. This “wording” process
B. Realism: Likelihood distributions using PDCC
enables the continuous time series data to be mapped into a
discrete space, making it compatible with LDA’s document- We propose a novel matrix composition method, termed
word structure. PDCC, to parameterize the full covariance matrix of the
The wording consists of three steps, as depicted in Fig. 2. GUIDE-VAE’s likelihood distribution. This section first gives
First, k-means clustering is applied to the pooled dataset X theintuitionbehindPDCC,thenitstheoreticalfoundation,and
described in Section II-A. This results in W cluster centres lastly, the useful properties that come with it.
corresponding to the “wording granularity”. Then, the user 1) Intuition: PDCC can be interpreted as a pattern-
time series profiles x are replaced with the respective dictionary scheme where the features of random variables
un
cluster labels to which they are assigned to, i.e. w = correlate by mapping high-dimensional uncorrelated noise.
un
argmin ||x − y || where y ∈ XT represent the wth To elaborate, let us investigate the sampling scheme of a
w un w w
cluster centre and w ∈ {1,...W}. Consequently, each user multivariate Gaussian distribution with a diagonal covariance
dataset consists of cluster labels that are interpreted as words, matrix, Σ = diag(σ) and zero mean vector, µ = 0. After
and “user documents” are formed as W ={w }Nu . having T i.i.d. samples from a unit normal distribution,
u un n=1
Treating users as documents allows LDA to generate {ϵˆ}T ∼T N(0,1), the final sample taken from the distri-
t t=1
approximate posterior distributions for each user dataset, bution can be acquired as xˆ = (cid:2) σ(t)ϵˆ(cid:3)T , which can be
D(θ u;γ u), where γ u ∈ RK + can be calculated iteratively represented as t t=1
using the document and the prior parameters as mentioned in
SectionII-B.Consequently,theresultingdistributionistreated (cid:88)T
xˆ = OneHot(t,T)σ(t)ϵˆ =I Σ[ϵˆ] (6)
astheuserembeddingthatisaimedfor,andtheconcentration t t t
parameters of these embeddings can be collected in a “user t=1
dictionary”Γ={γ u}U
u=1
asshowninFig.2.Notethat,unlike where OneHot(t,T) is a T dimensional one-hot-vector whose
a[1 s0 t] h, ethe emm be ea dn do inf gthe sid ni cs etri tb hu etio ren m,E ain[θ inu g]= mo(cid:80) mkγ eu γ nu(k ts), (i os rno st imus pe ld
y
a rell pre el se em ne tan tt is onar se ho0 we sxc the ept uO ncn oe uH po let( dt, nT at) u(t r) e= of1 th. eT ph ris ogo rn ese s- ih oo nt
the shape) of the distribution also contain information. For from the noise samples {ϵˆ t} to xˆ, which aligns with the
instance,userswithlowdatacountsresultindistributionswith independence of the features.
largervariances,andemployingonlythemeanvaluesofthese Now, let us denote a new sampling mechanism as
distributions neglects this inherited uncertainty.
V
Embedding users as distributions introduces the challenge xˆ =(cid:88) u σ˜(v)ϵˆ =UΣ˜[ϵˆ ] (7)
of applying these as conditions in mathematical models like v v v v
v=1JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
where {ϵˆ }V ∼V N(0,1) and V > T. Please note the 1+VT +V parameters in total, which is always larger
v v=1
resemblance to (6) but two main differences: The one-hot than T(T+1).Thiscanbeshowneasilybyusingbasical-
2
vectors are replaced with u , and the number of elements gebraicmanipulationsandtheinequalityT <V.Thus,it
v
to be summed increased from T to V. The former implies canbeconcludedthatPDCCoffersanoverparameterized
that each noise sample can now affect all the features through waytoconstructanypositivedefinitematrix.Aprooffor
the pattern vectors, u , that represent dependency patterns this is provided in Appendix A.
v
betweenthefeatures.Ontheotherhand,thelatterimpliesthat, • Parameterizability-PDCCisefficientlyparameterizable
unlike vanilla covariance matrix compositions, the number of for neural network training: We propose a partial param-
thesevectorsisnotlimitedbyT,whichcreatestheopportunity eterizationschemeforPDCCsinceitsoverparameterized
to employ (and learn) a large pattern dictionary, defined nature makes it challenging to model it as the output
as U = (cid:2) u⊤(cid:3)⊤ ∈ RT×V. This dictionary matrix maps of a neural network. In this scheme, only the auxiliary
v
high-dimensional uncorrelated noise vector [ϵˆ ] to a lower- standard deviations σ˜ are related to latent variables using
v
dimensional one. The resulting mapping can flourish arbitrary fσ˜(z) while the parameters of U are kept as global
ψ
correlations as it will be introduced theoretically in the rest of learnable parameters, i.e. a shared dictionary. Lastly,
the subsection. the base variance ξ is set as a hyper-parameter due
2) Definition: First, let σ˜ =(cid:2) σ˜(v)(cid:3)V ∈RV represent the to the reasons given earlier. Consequently, the resulting
v=1 +
auxiliary standard deviations in a higher dimensional space likelihood distribution is represented as
(V > T). Next, let us define a transformation matrix U =
p (x|z)=
(cid:2) u⊤(cid:3)⊤ ∈ RT×V, where each column u ∈ RT has a unit ψ,U (9)
norv m, i.e. ∥u v∥=1 ∀v. Lastly, let ξ ∈R +v represent the base N(x;µ=f ψµ(z),Σ=Udiag(f ψσ˜(z))2U⊤+ξI).
variance.Havingthesethreecomponents,acovariancematrix
One might question the benefit of having a dictionary
Σ∈RT +×T can be composed as with a size larger than T since the resulting covari-
ance matrix still has a rank of T. However, because
Σ=UΣ˜U⊤+ξI (8)
U is global, latent variables can influence the generated
where Σ˜ = diag(σ˜)2. Note that the composition UΣ˜U⊤ is data only through the auxiliary standard deviations. This
positive definite as long as rank(U) = T, and positive semi- means that U must learn the most frequent patterns
definiteelse.Itcanbeprovedeasilyusingpositivedefiniteness among the dataset. Thus, the dictionary size V allows
and matrix rank properties. In either way, (8) ensures Σ is controllingthepermissibleamountoffine-graineddetails
always positive definite due to ξ >0. in the generated data points.
3) Properties: MotivatedbytheproblemdefinitioninI-A2,
we claim that PDCC provides the following properties. C. GUIDE-VAE
• Constrainability - PDCC is spectrally constrainable for The proposed GUIDE-VAE model is a CVAE that is en-
avoiding singularities: Let {λ˜ }T represent the eigen-
t t=1 hanced with user embeddings and PDCC, as depicted in Fig.
values of UΣ˜U⊤ where λ˜ > 0 ∀t. Therefore, by the
t 3. Its likelihood and posterior distributions are defined as
definition of eigenvalue decomposition, the eigenvalues
of Σ are {λ |λ = λ˜ + ξ}T , which makes them p (x|z,c)=N(x;fµ(z,c),Udiag(fσ˜(z,c))2U⊤+ξI)
t t t t=1 ψ,U ψ ψ
bounded from below by ξ, preventing singularity. On (10)
the other hand, likelihood calculations require inverting and
Σ, and this inversion fails if det(Σ) ≈ 0 despite its
q (z|x,c)=N(z;fµ(x,c),diag(fσ(x,c))2), (11)
positivity. The constant ξ can also prevent this since ϕ ϕ ϕ
det(Σ)=(cid:81) λ =(cid:81) (λ˜+ξ)>ξT.However,notethatξ (cid:104) (cid:105)
t t t t respectively, where c = θˆ,c˜ is the condition vector. This
canbeinterpretedasanisotropicGaussiannoise(orden-
sity) added by default, hence the naming base variance. vector contains the user vector θˆ, sampled from the user em-
In order to preserve the realism aspect of the samples, bedding, and the auxiliary condition vector c˜, which includes
the base variance must be kept at moderate levels, e.g. additionalcontextualinformation,suchastimestampsorother
ξ < 1. Unfortunately, ξT diminishes exponentially with relevant features. The prior distribution remains unchanged
the increasing dimensionality, and one must sacrifice a and is kept as p(z)=N(z;0,I).
certain level of realism to guarantee numerical stability. To ensure numerical stability and prevent degenerate so-
• Flexibility-PDCCisflexibleenoughforcapturinginter- lutions, we apply a lower bound constraint on the auxiliary
dependencies: Every positive definite covariance matrix standard deviations, fσ˜(v)(z,c) > ε, ∀v. Without this con-
ψ
canbesubjectedtoCholeskydecompositionasΣ=LL⊤ straint, there is a risk that the decoder could disregard the
whereLisauniquelowertriangularmatrixwithpositive pattern vectors, leading the likelihood distribution to converge
diagonal elements. Vice versa, a valid matrix L can prematurely to N(x;fµ(z,c),ξI), thereby failing to learn
ψ
generate a unique covariance matrix. Therefore, it can meaningful patterns. By enforcing this constraint, we ensure
be claimed that a covariance matrix Σ has a degree that the decoder utilizes the pattern dictionary during training
of flexibility T(T+1), which is the number of lower and that the pattern dictionary learns informative correlations
2
triangular elements of L. Meanwhile, PDCC provides between features.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
IV. EXPERIMENTS
A. Data preperation
1) Dataset: Thedatasetusedinthisstudyconsistsofsmart
meterdatacollectedfromvariouselectricityconsumersacross
47provincesinSpain,includinghomes,offices,andbusinesses
[18]. The dataset includes 25,559 customers (users) and their
hourlyelectricityconsumptionmeasurements(kWh)collected
between November 2014 and June 2022.5 However, since
the consumers enrol the data collection system not at the
same time, there is an imbalance in the dataset between the
users in terms of data quantity that we want to stress in the
experimentation.
Fig.4. Thepmfofbeta-binomialdistribution(n=365,a=0.85)inlogarithmic
Thisstudyusesaspatiotemporalsubsetoftheentiredataset:
scalefordifferentbvalues.
Only the data collected from Gipuzkoa (the province with
the highest data density) between June 2021 and June 2022
is included in the experiments. Also, the users who enrolled
afterJune2021andwhofinishedtheirenrollmentbeforeJune
2022 are eliminated, i.e. only the users with at least one
full year enrollment are kept. Lastly, the users that constantly
consume 0 kWh or show negative consumption at least once
are eliminated, too. These resulted in a dataset with U=6830
users and N =365 daily (T=24) profiles for each, which
u
equates to a total number of ∼2.5M records.
Fig. 5. The data splitting scheme used in experimentation. A full dataset
As stated in Section I-B, it is desired to mitigate the where each user has an equal number of profiles first amputated according
user imbalance problem in a multi-user dataset. This problem to beta-binomial distribution, and these are reserved in the missing set (in
red). The remaining randomly split into training (green), validation (blues)
is already apparent in the data collection in [18], and the
andtesting(yellow)sets.
sub-dataset created above can be used to simulate the late
enrolmentproblem.Notethatthissimulateddatasethasground
truths for the missing records, unlike the original one.
In order to create an artificial missingness, the data of each
user’s first days are “amputated” randomly, corresponding to
randomly late enrolments. For this purpose, the beta-binomial
distribution with the probability mass function (pmf)
(cid:18) (cid:19)
n B(m−a,m−k+b)
P(M =m;a,b,n)= (12)
m B(a,b)
isemployedtotakei.i.d.lateenrolmentsamplesforeachuser,
i.e. N u = 365−M u where M u ∼ BetaBinom(a,b,n), ∀u. (a) (b)
Here, B is the Beta function, a > 0 and b > 0 are shape
Fig.6. Thedistributionsof(a)measurements(inWh)inthedatasetand(b)
parameters, and n represent the length of the integer support thezero-preservedlog-normalization(h0=-3,h+=1)appliedtothem.
m ∈ {0,1,...n}. In this study, the parameters n and a
are set to 365 and 0.85, respectively; thus, the “severity of
missingness” is controlled by the parameter b. In Fig. 4, prevent the direct use of the logarithmic transformation. To
the pmfs generated by different b values are plotted in a mitigatethis,weproposeanovelscalingtechniquecalledzero-
logarithmic scale. Note that the mean of the distribution is preserved log-normalization.
calculatedasE[M]= na anditisinverselyproportionaltob;
a+b Definition1(Zero-preservedlog-normalization). Givenanon-
hence it is named as data availability parameter. A depiction negative scalar dataset X={x }N ,x ∈R ∀i, two index
n i=1 i ≥0
of a possible missingness pattern is given in Fig. 5. sets can be created as I = {i|x > 0,x ∈ X} and I =
+ i i 0
2) Preprocessing: Energy consumption data tends to have {i|x = 0,x ∈ X}, representing the indices of positive and
i i
occasional peaks, and these cause a heavy tail in the data zero valued elements in X, respectively. Zero-preserved log-
distribution,ascanbeseeninFig.6a.Theconventionalwayto normalization transforms the dataset X into X¯ ={x¯ } where
i
deal with such heavy-tailed data is by applying a logarithmic
(cid:40)
transform. However, energy consumption data is inherently logxi−m +h , i∈I
“zero-inflated”,meaningthattherearedatainstances(intime) x¯ i =fZPLN(x i;m,s,h +,h 0)=
h
,s + i∈I+ .
that have no energy consumption. These exact zero values 0 0
(13)
Here, m = 1 (cid:80) logx , s2 = 1 (cid:80) (logx −m)2,
5The dataset has two versions: raw with missing values and the imputed |I +| i∈I + i |I +| i∈I + i
one.Thisstudyusestheimputedversion. and h · is a shifting constant of choice.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
The constants h and h are used to provide an effective
0 +
separation between zero and positive consumption values.6
The resulting data distribution after applying zero-preserved
log-normalization, with h =-3 and h =1, to the dataset is
0 +
given in Fig. 6b, and the desired log-normality can easily be
spotted. Also, choosing h smaller enough than h makes
0 +
the inverse transformation robust to small deviations around
h 0, i.e. if x¯ = h
0
± δ and h
+
≫ h
0
+ m
s
+ δ, then
x=exp(s(x¯−h )+m)≈0.
+
Now that an appropriate normalization technique is avail-
able, preprocessing can proceed. First, the dataset is created
for a given availability parameter b, and missing records
are stored in Xmissing. Then, the remaining data is split into
training, validation and test sets with a ratio of 8:2:2. This Fig.7. Theneuralnetworkarchitectureusedbothforencoderanddecoder.
splitting process is illustrated in Fig. 5. Note that the sizes of
these datasets also depend on the selected b. Then, the zero-
2) Constraints: Therearevariousconstraintsontheoutputs
preserved log-normalization (h =-3, h =1) is applied to each
0 + of both the encoder and decoder for numerical stability.
feature, x(t), individually. Note that the zero-excluded mean
Firstly, the base variance ξ is set to 10−2, which means the
m(t) and standard deviations s(t) are estimated using only the
marginal standard deviations of the likelihood distribution are
training set but applied to all sets.
boundedbelowby10−1.Similarly,thisboundfortheposterior
3) Conditioning: As mentioned in Section III-C, there are distribution is set to 5×10−1. Likewise, the mean parameters
two main components of the conditions (c ) used in the
un ofbothdistributionsareconstrainedbetween-3and5,causing
GUIDE-VAE training: the sampled user vector θˆ , and the
u betterconvergencewithouthurtingthemodellingperformance.
auxiliary condition vector c˜ . The generation of user dictio-
un 3) Optimization: Adam [20] is used for the optimization
nary Γ to take user vector samples is explained thoroughly in of the GUIDE-VAE with its default parameters8, and L2-
SectionsII-BandIII-A.Similartothenormalizationstep,only
regularization is applied over all parameters (including U)
the training data is used in LDA-based user embedding, and withacoefficientof10−5.Moreover,anadaptivelearningrate
theresultingdictionaryΓisstoredforagivenuserembedding
scheduler and early stopping are applied using the validation
hyper-parameter combination (W, K). The prior parameters set to further prevent overfitting. Lastly, NMC=16 is used for
always kept equal to η = 1 and α= 1.
W K Monte Carlo sampling that appears in the ELBO objective.
It is decided to conduct the experiments using two auxil-
iary conditions: months and weekdays. These conditions are
C. Performance metric
extracted using the data index n, encoded using the cyclic
In order to assess the performance of the trained generative
(sin-cos) transformation like in [19], and assigned to their
models, two datasets are used: Xtest for the synthetic data
respective data points x . Due to the inherited seasonality,
un generation and Xmissing for the imputation performance. The
we believe these are essential for energy consumption time
series modelling.7 log-likelihood of a given set on the generative models
1 (cid:88)
E [logp (x|c)]= logp (x |c ) (14)
p·(x,c) ψ |X·| ψ un un
B. Training
xun∈X·
is chosen as the performance metric. Here p(x,c) is
·
1) Neural networks: The neural network structure used in the empirical distribution representing either the testing
theexperiments,bothfortheencoderanddecoder,isdepicted or missing datasets and their respective conditions. Since
in Fig. 7. As can be seen, the input size (T +K +4) is the logp (x |c ) is intractable, it is approximated by the
ψ un un
same for both of the networks. This is because the size of the importance sampling as
latentspaceisfixedtoT=24inallexperimentssincethemain
(cid:20) (cid:21)
p (x ,z|c )
objective is data modelling, not dimensionality reduction. The logp (x |c )=logE ψ un un
ψ un un qϕ q (z|x ,c )
remainingpartoftheinput(K+4)comesfromtheconditions. ϕ un un
T ouh te puo tn sly izepa or ft tt hh eat Od uif tf pe ur ts Lb ae ytw eree (n σ/t σh ˜e ),t ww ho icn het ew qo ur ak ls
s
i Ts t fh oe
r
≈−logSMC+log(cid:88)SMC p ψ(x un|zˆ uns,θˆ us,c˜ un)p(zˆ uns)
q (zˆ |x ,θˆ ,c˜ )
the encoder and V for the decoder. Lastly, L and M are set s=1 ϕ uns un us un
(15)
to 3 and 1000, respectively, for all experiments.
where {zˆ } S ∼MC q (z|x ) and {θˆ } S ∼MC Dir(γ ). After
uns ϕ un us u
6Please note that the zero-preserved log-normalization differs from the sweeping different values, we set SMC=100 for all the ex-
(c x¯o inv =enti δo ,na ∀l io ∈ffse It 0ti )ng by(x¯ ei xcl= udix ngi + zerδ o, s∀ fri om∈ th[N
e
] e) sta imnd atiz oe nro o-r fep mlac ae nm den st
,
periments to balance the computational time for testing and
sothattheyarenotaffectedbytheunwantedskewnesscomeswiththezeros. estimation variance.
7Sincetheaimisnotfindingthebestgenerativemodelbutshowcasingthe
benefitsofGUIDE-VAE,thenumberofauxiliaryconditionsislimitedtotwo. 8β1=0.9,β2=0.999.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
Fig.8. AlearneduserdictionaryΓwithK=20.
Please note that the conventional sample comparison-based dataset, and we imputed its missing time series, Xmissing =
u′
performance metrics are inadequate for this task since they Xmissing ∩X , 104 times by sampling {zˆ } 1 ∼04 p(z), ∀n
u′ ns s
require preferably a larger sample population to represent the and {θˆ } 1 ∼04 Dir(γ ). Note that each sample s from the
distributiontocompare.Unfortunately,thenumberofsamples s u′
prior distribution corresponds to a likelihood distribution, and
for a given condition tends to be very limited when the
we aim to visualize the samples from these likelihood distri-
conditions get plenty. For instance, the dataset used in this
butions, xˆ ∼ p (x |zˆ ,θˆ ,c˜ ), since these are the final
study has three conditions: users, months and weekdays. This ns ψ n ns s n
outcomes during inference. For visualization we concatenated
means there are at most five instances of a given condition
t rr ei pp rl ee st ei nn tia ngye aa cr- olo nn dg itid oa nt aa ls det i, sta rn ibd utt ih oe ny .a Tr he isfa pr rofr bo lm emef gf ee tc sti ev ve el ny wth he ep rero (cid:12) (cid:12)fi Xle m us ′isso inf g(cid:12) (cid:12)a =giv Men u′.sa Tm hp el re ess ua lts inxˆ gs s= am[ pxˆ ln es s] fM n o=u r1′ e∈ achX mM ou d′T el,
are given in Fig. 10. In order to showcase the typical per-
more severe when continuous conditions are employed.
formance of each model, we calculated the log-likelihood of
V. RESULTS the ground truth time series on each sampled distribution, i.e.
(cid:80) logp (x |zˆ ,θˆ ,c˜ ), ∀s, and found the sample
shoT wh ce asr ee dsul hts ereo .f Tth he e e fox lp loer wim ine gnt hs yt ph ea rt -pw are ar me ec teo rnd vu ac lute ed s aa rr ee thax tn c∈ oX rrm u ei ′ss sin pg ondstψ omn edin as n(5s ×1n 03-thhighest)ofthesescores.
used as default unless it is stated otherwise: This “median best sample” is also given in Fig. 10.
• Data availability rate: b=10 As can be seen from Fig. 10, all models can generate
• User vector size: K =100 samplesthatareveryclosetothegroundtruth.Pleasenotethat
• Pattern dictionary size: V =100 these were generated using only the user embedding, month,
• Wording granularity: W =1000 and weekday information pieces. Also, recall that the ground
• Auxillary standard deviation lower-bound: ε=10−4 truth profile was never seen in the training. This showcases
• Marginal variance lower-bound: ξ =10−2 the generalization power of GUIDE-VAE. Another qualitative
Thesedefaultvalueswereselectedbasedonpreliminarytuning outcomeofthisexperimentistheplausibilityofthegenerated
to balance computational efficiency and performance. samples. As one can see, not only the median ones but all
samples get less noisy with the increasing pattern dictionary
A. Visual inspection size. We claim that the larger the pattern dictionary, the more
likely it is to observe realistic samples.
The user and pattern dictionaries and generated time series
samples after training are visualized in this section. We start
the visualization by showing a 20-dimensional (K=20) user
B. Effect of user embeddings
dictionary Γ in Fig. 8 as the output of the user embedding
scheme.9 The visual inspection suggests that the user vectors In order to analyse the effect of user embeddings, two
are dispersed enough to capture discrepancies between user independent hyper-parameter sweeps are considered:
behaviours. • K ∈{0, 5, 10, 20, 50, 100}
Next,weinvestigatethepatterndictionariesoftwoGUIDE- • W ∈{0, 250, 500, 1000, 2000}
VAE models with V=25 and V=100. For better visualization,
The results of the user vector size K and wording granularity
the pattern vectors (u ) are sorted with respect to their L -
v 1 W sweeps are given in Fig. 11 and 12, respectively.
norm, and the resulting matrices are given in Fig. 9. One
The first and the most obvious conclusion from Fig. 11 is
can immediately spot the richness of patterns coming with
that the introduction of user embeddings boosts the perfor-
the increasing dictionary size, which is expected since it
mance considerably. This showcases the importance of using
allows GUIDE-VAE to exploit finer details and more intrinsic
user information in modelling multi-user datasets. Further-
correlations between time steps. For instance, a large variety
more, it can be seen that the performance improvement is
ofswitchingbehavioursarecapturedbythemodelontheright
nearly monotonic with the increasing K. This makes sense
side of the dictionary in Fig. 9b, which are absent in Fig. 9a.
since the higher the user vector dimensionality, the more
Lastly, we visualize the data generated by GUIDE-VAE
informationcanbeconveyedtoGUIDE-VAE.Weseethatthis
with varying dictionary sizes. For this, we conducted a sub-
is subject to saturation after some K values. Practitioners can
experiment in which we selected a random user u′ from the
select among these values to not overwhelm the input sizes of
9Here,only10%oftheusersaregivenduetovisibilityconcerns. the neural networks with unnecessarily large user vectors.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
(a) (b)
Fig.9. PatterndictionariesUfromtwotrainedGUIDE-VAEmodelswith(a)V=25and(b)V=100,sortedaccordingtoL1-normofthepatternvectors.
Fig. 10. The effect of pattern dictionary size V on the sample quality. A selected user’s missing time series measurements (in blue) and the 10 closest
imputations(inred)outof1000generatedtimeseries(50ofwhichareinorange).
On the other hand, Fig. 12 reveals that cluster sizes above
250 are merely effective on the performance of this dataset.
Sincethisisacomputationallylightstepoftheuserembedding
scheme and does not affect the input sizes of the neural net-
works,moderatelyhighgranularitiescanbeusedforwording.
Another result given in these figures is the performance
improvement that comes with PDCC. As one can see, using
a pattern dictionary with size V =100 consistently increases
the performance over a diagonal covariance structure. This
shows that GUIDE-VAE brings out significant performance
Fig. 11. The effect of user vector size K on the modelling performance.
improvements by utilizing both of the proposed methods.
K=0meansnoconditioningonusers.
We also showcase that this performance improvement not
only comes at the global level but also at the user level by
calculatingthelog-likelihoodscoresindividuallyforeachuser.
We conducted this experiment with a user-guided (K=100)
and an unguided model by calculating the log-likelihood
scores of individual testing (Xtest = Xtest ∩X ) and missing
u u
datasets (Xmissing = Xmissing ∩X ). Then, we subtracted the
u u
individual unguided scores from the guided ones and called
the resulting differences “log-likelihood gain per user”. The
resulting histograms for both sets are given in Fig. 13. We
canseethatintroducinguserembeddingsbooststhemodelling
Fig.12. TheeffectofwordinggranularityW onthemodellingperformance.
of certain users significantly more than others. However, the
W=0meansnoconditioningonusers.
negative values in the log-likelihood gain on the missing set
imply that out-of-distribution generalization is not guaranteed
with GUIDE-VAE.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
Fig. 15. The effect of pattern dictionary size V on reconstruction log-
likelihood (RLL) and Kullback-Leibler divergence (KL-Div.). V=0 corre-
spondstoadiagonalcovariancematrix.
Fig.13. Distributionsoflog-likelihoodgainscomewithconditioningonuser
embeddings,peruser.
Fig. 16. The effect of expected missing days (controlled by b) on the
modellingperformance.
Fig.14. TheeffectofpatterndictionarysizeV onthemodellingperformance.
V=0correspondstoadiagonalcovariancematrix.
beobservedfromthedecreasingdeviationonthesampleswith
C. Effect of PDCC thepatterndictionarysizeinFig.10.Inotherwords,themodel
gets more confident about the likelihood distribution imposed
Two hyper-parameter value sets for the pattern dictionary
by the conditions, and the latent variable z poses a minimal
size V and the lower-bound ε are set as
effectongeneration.Inotherwords,alargepatterndictionary
• V ∈{0, 25, 50, 75, 100, 125, 150}
results in a more consistent sampling process.
•
ε∈{10−5, 10−4, 10−3}
andasweepconductedontheirproductset,i.e.allthepossible
V and ε combinations are tested. Like before, a value of 0 D. Effect of missingness
means no employment of PDCC, i.e. a diagonal covariance
Lastly, we investigate the effect of the missing set size to
matrix structure. In these cases, ε does not refer to anything.
the performance by sweeping the data availability rate b as
TheresultsofthissweeparegiveninFig.14.Asmentioned
before, the performance improvement that comes with PDCC • b={2, 3, 5, 10, 20, 30, 50}
is apparent. On the other hand, we see that the selection which corresponds to a series of expected missing days per
of neither the dictionary size nor the lower bound causes a user calculated as E[M ]=365 0.85 . Since this amputation
u 0.85+b
significant change in the performance. Please recall that the process is conducted at the very beginning of the entire train-
main motivation for proposing PDCC was to introduce more ing pipeline and is inherently stochastic, relying on a single
realism to the generated samples. will be investigated in the amputation scenario can be fallacious. Thus, this process is
qualitative results sections. repeated three times with different random number generator
A natural question that arises is why a larger dictionary kernels for each hyper-parameter configuration.
size does not affect the performance significantly if it means Theexperimentresultswithrespecttotheexpectednumber
higher representative power, as we claimed before. To answer of missing days are given in Fig. 16. One can immediately
this, we must investigate the two components of ELBO given spot the near monotonic decline in the performance with
in(5),namelythereconstructionlog-likelihoodandKullback- expected missing days on the missing set when the user
Leibler divergence. These two metrics were calculated for embeddings are provided while still being superior to its
trained models with varying V, and the results are given in unguidedcounterparts.Ontheotherhand,theperformanceon
Fig. 15. As can be seen, increasing the dictionary size results the testing set appears to be robust to the training data size.
in a decrease both in reconstruction and Kullback-Leibler Thisisexpectedsincethetestingsetisafixedportion(20%)of
divergence. It indicates that the pattern dictionary undertakes thenon-missingpart,andthetrainingdataisabundantenough
the representation burden from the latent space. This can also (∼1M, at worst) to “interpolate”.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
The robust behaviour in testing performance suggests that [8] C.Sun,J.vanSoest,andM.Dumontier,“Generatingsyntheticpersonal
GUIDE-VAE can be used confidently for synthetic data gen- healthdatausingconditionalgenerativeadversarialnetworkscombining
withdifferentialprivacy,”JournalofBiomedicalInformatics,vol.143,
eration tasks even under severe missingness. Unfortunately,
p.104404,2023.
this is not the case for the imputation performance since the [9] S.Zhang,L.Yao,A.Sun,andY.Tay,“Deeplearningbasedrecommen-
missingness is biased toward the first days of the year. The der system: A survey and new perspectives,” ACM computing surveys
(CSUR),vol.52,no.1,pp.1–38,2019.
increasing number of missing days per user creates a vacuum
[10] X.Chen,C.Zanocco,J.Flora,andR.Rajagopal,“Constructingdynamic
of ignorance about these periods, and the improved modelling residential energy lifestyles using latent dirichlet allocation,” Applied
capacity that comes with user embeddings inhibits the out- Energy,vol.318,p.119109,2022.
[11] A.J.Almansoori,S.Horva´th,andM.Taka´cˇ,“Padpaf:Partialdisentan-
of-distribution generalization power of GUIDE-VAE. Thus,
glementwithpartially-federatedgans,”2022.
we advise practitioners to use GUIDE-VAE cautiously for the [12] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
extrapolative missing value imputation tasks. Journal of machine Learning research, vol. 3, no. Jan, pp. 993–1022,
2003.
[13] B. Van Breugel, T. Kyono, J. Berrevoets, and M. Van der Schaar,
“Decaf: Generating fair synthetic data using causally-aware generative
VI. CONCLUSION
networks,”AdvancesinNeuralInformationProcessingSystems,vol.34,
pp.22221–22233,2021.
In this work, we introduced GUIDE-VAE, a novel frame-
[14] S. Hans, A. Sanghi, and D. Saha, “Tabular data synthesis with gans
work for generating realistic and user-guided time series data for adaptive ai models,” in Proceedings of the 7th Joint International
in multi-user settings. First, we proposed probabilistic user Conference on Data Science & Management of Data, 2024, pp. 242–
246.
embeddings as conditions for generative models, which can
[15] G. Dorta, S. Vicente, L. Agapito, N. D. F. Campbell, and I. Simpson,
beappliedtoanyconditionalgenerativemodel.Thisapproach “Structureduncertaintypredictionnetworks,”inProceedingsoftheIEEE
effectively addresses the challenges of user-agnostic data gen- ConferenceonComputerVisionandPatternRecognition(CVPR),June
2018.
eration and data imbalance by incorporating user identities,
[16] J. Langley, M. Monteiro, C. Jones, N. Pawlowski, and B. Glocker,
leading to significant performance improvements. Second, we “Structured uncertainty in the observation space of variational autoen-
introduced PDCC to enhance the realism of generated data, coders,”TransactionsonMachineLearningResearch,2022.
[17] K. Muandet, K. Fukumizu, B. Sriperumbudur, B. Scho¨lkopf et al.,
particularly by VAEs. PDCC mitigates the common issue of
“Kernel mean embedding of distributions: A review and beyond,”
noisy outputs, allowing the model to capture complex feature Foundations and Trends® in Machine Learning, vol. 10, no. 1-2, pp.
dependencies and generate realistic time series data. 1–141,2017.
[18] C. Quesada, L. Astigarraga, C. Merveille, and C. E. Borges, “An
WecombinedthesecontributionsintheGUIDE-VAEmodel
electricity smart meter dataset of spanish households: insights into
anddemonstrateditseffectivenessonamulti-usersmartmeter consumptionpatterns,”ScientificData,vol.11,no.1,p.59,2024.
dataset.Weconsideredtwotasks,namelysyntheticdatagener- [19] C.Wang,S.H.Tindemans,andP.Palensky,“Generatingcontextualload
profilesusingaconditionalvariationalautoencoder,”in2022IEEEPES
ation and missing records imputation, and the results showed
InnovativeSmartGridTechnologiesConferenceEurope(ISGT-Europe),
significant improvements over conventional CVAE models, 2022,pp.1–6.
both in modelling performance and sample quality. In overall, [20] D. P. Kingma, “Adam: A method for stochastic optimization,” arXiv
preprintarXiv:1412.6980,2014.
GUIDE-VAEoffersapowerfulsolutionforgeneratingrealistic
and user-guided data.
As future work, we plan to showcase the effect of user
APPENDIXA
embeddings in other conditional generative models. Also, we
PROOFS
desire to investigate other applications such as forecasting. Theorem 1. Any positive definite matrix can be constructed
Lastly, we plan to apply GUIDE-VAE on other multi-use using PDCC at least in V(V −T)+1 different ways.
datasets like healthcare and finance. Proof. Let M ∈ RT×T be a positive definite matrix with
eigenvalues λ >0, ∀t. Then, the Cholesky decomposition of
t
REFERENCES (M−ξI)resultsinauniquelowertriangularmatrixL∈RT×T
suchthatM−ξI=LL⊤ aslongasξ <min({λ }).Thus,we
i
[1] Z.Qian,T.Callender,B.Cebere,S.M.Janes,N.Navani,andM.vander shall perform the construction LL⊤ =UΣ˜U⊤. Let us define
Schaar,“Syntheticdataforprivacy-preservingclinicalriskprediction,” (cid:20) (cid:21)
medRxiv,pp.2023–05,2023. U = (cid:2) L 0(cid:3) Q where Q = (cid:2) Q Q˜(cid:3) . Here, Q ∈ RT×T
[2] X.Peng,S.Luo,J.Guan,Q.Xie,J.Peng,andJ.Ma,“Pocket2mol:Effi- U˜ ⊥ ⊥
cientmolecularsamplingbasedon3dproteinpockets,”inInternational is orthogonal, and U˜ ∈ R(V−T)×V and Q˜ ∈ RT×(V−T) are
ConferenceonMachineLearning. PMLR,2022,pp.17644–17655.
[3] J. Yoon, L. N. Drumright, and M. Van Der Schaar, “Anonymization free. Thus, by replacing U, we obtain
throughdatasynthesisusinggenerativeadversarialnetworks(ads-gan),”
IEEEjournalofbiomedicalandhealthinformatics,vol.24,no.8,pp. LL⊤ =LQΣ˜(LQ)⊤+0U˜Σ˜(0U˜)⊤ ⇒I=QΣ˜QT
2378–2388,2020.
[4] D. P. Kingma, “Auto-encoding variational bayes,” arXiv preprint ⇒I=Q Σ˜ Q−1+Q˜Σ˜ Q˜⊤
⊥ 1 ⊥ 2
arXiv:1312.6114,2013.
[5] K. Sohn, H. Lee, and X. Yan, “Learning structured output represen- (cid:20) Σ˜ (cid:21)
tation using deep conditional generative models,” Advances in neural where 1 Σ˜ = Σ˜. This equation always has at least
informationprocessingsystems,vol.28,2015. 1
[6] G. Bredell, K. Flouris, K. Chaitanya, E. Erdil, and E. Konukoglu, one solution, e.g. Σ˜ 1 =I, Q˜ = 0. Therefore, M can be
“Explicitlyminimizingtheblurerrorofvariationalautoencoders,”arXiv constructed at least V(V −T)+1 different ways by choosing
preprintarXiv:2304.05939,2023. ξ and the elements of U˜ independently.
[7] S.Chai,G.Chadney,C.Avery,P.Grunewald,P.VanHentenryck,and
P.L.Donti,“Defining‘good’:Evaluationframeworkforsyntheticsmart
meterdata,”arXivpreprintarXiv:2407.11785,2024.