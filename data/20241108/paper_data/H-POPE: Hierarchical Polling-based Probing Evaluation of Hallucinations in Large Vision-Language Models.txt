H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations in
Large Vision-Language Models
NhiPham∗ MichaelSchott∗
MaxPlanckInstituteforInformatics SaarlandUniversity
nhipham@mpi-inf.mpg.de misc00001@stud.uni-saarland.de
Abstract uralextensionistofurtherassesshallucinationongranular
details such as attributes. Therefore, the main objective of
By leveraging both texts and images, large vision lan- ourprojectistoconductamorefine-grainedevaluationon
guagemodels(LVLMs)haveshownsignificantprogressin not only the existence of objects but also the attributes as-
variousmulti-modaltasks.Nevertheless,thesemodelsoften sociated with them. To achieve this goal, we leverage the
sufferfromhallucinations,e.g.,theyexhibitinconsistencies discriminativeapproachoutlinedinPOPE[5].Foreachim-
betweenthevisualinputandthetextualoutput. Toaddress age, we start with coarse-grained questions about the ex-
this,weproposeH-POPE,acoarse-to-fine-grainedbench- istenceofobjectsandcontinuewithfine-grainedquestions
markthatsystematicallyassesseshallucinationinobjectex- about the existence of attributes. The resulting benchmark
istence and attributes. Our evaluation shows that models iscalledHierarchicalPOPE(H-POPE).
are prone to hallucinations on object existence, and even TheoriginalPOPEincludesthreesamplingstrategiesfor
more so on fine-grained attributes. We further investigate negativeobjects. Someofthemwerespeciallydesignedto
whether these models rely on visual input to formulate the formulatechallengingquestionsbasedonoccurrencestatis-
outputtexts. tics in the dataset. We utilise these settings in our study,
while also aiming to devise an additional sampling strat-
egy for attributes. To this end, we propose Image-based
1.Introduction Adversarialsetting,whichshiftsthefocustothelocalcon-
textofthegivenimageandselectsattributesthatappearin
Recent advances in multi-modal models have enabled
the image but do not describe the object in question. Our
a wide array of impressive capabilities. For example,
evaluation shows that models tend to hallucinate worst on
suchmodelscangenerateawell-formulateddescriptionof
questionssampledinthissetting,whichsuggeststhatexist-
a given image and thus reflect an understanding of both
ingLVLMsstruggletomatchattributescorrectlytoobjects
the textual and the visual domains. Despite their promis-
thatpossessthem.
ing progress, existing models often suffer from a phe-
Finally,itisnoticeablethatthecurrentworkcenterstheir
nomenoncalledhallucination,whichnotonlydegradesthe
evaluation metrics around the models’ textual output. We
model’sperformancebutalsoraisesquestionsaboutsafety
investigate relevance maps to see if the models make dif-
and reliability. To understand and assess hallucinations in
ferent use of visual information when giving hallucinated
LVLMs, several benchmarks have been proposed. In spe-
answersversuscorrectanswers.
cific, Caption Hallucination Assessment with Image Rele-
Insummary,ourmaincontributionsinclude:
vance (CHAIR) [9] demonstrates that when asked to pro-
vide an accurate description of a given image, responses • Introducing H-POPE, a benchmark to assess halluci-
from these models frequently include objects that are not nationonobjectsandattributesinLVLMs,includinga
actually present [9]. Moreover, the Polling-based Object newadversarialstrategytargetedatevaluatingwhether
Probing Evaluation (POPE) [5] shows that these models LVLMs can match attributes to the correct objects in
tendtoconfirmthepresenceofanobjectthatdoesnotexist, theimage.
ifitfrequentlyco-occurswithotherobjectsintheimage.
While these aforementioned benchmarks effectively • Evaluating three current LVLMs, namely mPLUG-
highlight the general issue of hallucination, they often fo- Owl[12],InstructBLIP[3]andLLaVa[7]onH-POPE.
cus on assessing hallucinationson object presence. A nat-
• Qualitative examples of the visual cues used by
*Authorscontributedequallytothiswork,listedinalphabeticalorder. LVLMs,whicharederivedfromLVLM-Interpret[10].
4202
voN
6
]VC.sc[
1v77040.1142:viXra2.RelatedWork 3.Method
2.1.HallucinationBenchmarks 3.1.H-POPEBenchmark
OurH-POPEbenchmarkextendsPOPE[5]toincludeat-
Hallucination evaluations of LVLMs, such as CHAIR
tributeevaluation. Givenaninputimage,H-POPEextracts
[9], traditionally follow the generative paradigm. Given
alistofground-truthobjectsfromtheannotations,andsam-
aninstruction,modelsgenerateatextualdescriptionwhich
plesanequalnumberofnegativeobjects. Theseareusedto
will then be assessed for hallucination. However, this ap-
formulatethefirsttier(coarser)ofquestions.Then,foreach
proachisoftenunstableduetoitsdependenceoninstruction
existent object, H-POPE extracts a list of ground-truth at-
designandcaptionlength. Toaddressthislimitation,POPE
tributesandsamplesanequalnumberofnegativeattributes.
introducesanoveldiscriminativeframeworkforevaluating
Theseformthesecondtier(finer)ofquestions.Notethatwe
coarse-grainedhallucinations[5].Itassessesmodels’object
sample objects and attributes with the same negative strat-
hallucinationwithasetofpollingbinaryquestionsbasedon
egy. OurpipelineiscapturedinFigure1.
objectspresentintheimagesandthosethataretakenfrom
random, popular and adversarial negative sampling strate- 3.2.NegativeSamplingStrategies
gies. Thishasnotonlybeenproventobestableacrossdif-
For object existence, our negative sampling strategies
ferent prompts, but also allows for flexibility in adapting
follow those that are from the POPE benchmark [5]. In
thisbenchmarkforothertypesofhallucinations.
specific: (i) random, which randomly samples any object
from the dataset that does not exist in the image; (ii) pop-
2.2.LargeScaleAttributes(LSA)Dataset
ular,whichselectstopfrequentlyappearingobjectsacross
the dataset that do not exist in the image; and (iii) adver-
Most hallucination benchmarks leverage datasets with
sarial (frequency-based), which selects top co-occurring
annotations for the evaluation. One such dataset is LSA,
objectswiththeground-truthobjectsthatdonotexistinthe
which combines attribute annotations extracted and aggre-
image. These three sampling strategies serve as a natural
gated from 6 different dataset: Visual Genome, GQA,
samplingschemeforattributes. Weintroduceanadditional
Flickr30K-Entities, MS-COCOCaptions2017, COCOAt-
adversarialscenario—(iv)adversarial(image-based)—
tributes, and Localized Narratives [8]. LSA differs from
which selects attributes that are present in the image, be-
existingworksthatonlyfocusonadjectiveobjectattributes
cause they describe other objects but do not describe the
and ignore attributes of visual relationship among objects.
objectinquestion. WeillustratethisinFigure2.
Moreover, it takes advantage of negative label expansion:
since many attributes are mutually exclusive, presence of 3.3.AttributeTypeSelection
suchanattribute(e.g.,clean)wouldautomaticallyimplyab-
senceofotherattributes(e.g.,dirty)[8]. LSAfitswellwith Our H-POPE benchmark formulates binary questions
the about object existence and object attributes. Negative at-
tributes are sampled from the list of attributes across the
dataset Ω, so we need to ensure mutual exclusion, i.e., if
2.3.LargeVision-LanguageModels(LVLMs)
an object has a list A of attributes, it should not have any
LVLMs leverage both textual and visual information to attributes from Ω\A. To this end, we restrict our setting
learn and generate content for different multi-modal tasks. to only three attribute types: color, material, and shape.
These tasks include image captioning, where the model WeusethesetypesfromtheLSAdataset,becausetheyare
generates descriptive text based on a visual input, and im- most frequent and typically mutually exclusive due to the
agegeneration,whereitproducesvisualcontentguidedby negativelabelexpansioninthedataset(e.g., ifanobjectis
textualprompts.SeveralprominentexamplesofLVLMsin- annotatedwithred andyellow, itislikelythatonecanuse
clude InstructBLIP [3], LLaVa [7] and mPLUG-Owl [12]. othercolorsasnegativeattributes).
On a high level, these models consist of a pre-trained lan-
4.HallucinationEvaluation
guageencoderandapre-trainedvisionencoder,whichgen-
erate corresponding text and visual embeddings that are
4.1.EvaluationSetting
then aligned through various methods, enabling the model
Models. WeevaluatethreepopularLVLMsonourbench-
to understand and generate coherent multimodal outputs
mark. TheseareInstructBLIP[3],LLaVa[7]andmPLUG-
[4]. Despite LVLMs’ great promise in tackling various Owl[12]. Wechoosethe7Bvariantsforeachofthem. We
tasks, most models are still prone to hallucinations [4]. In query the models with an input image and the following
ourstudy,wefocusoninvestigatinghallucinationsinmod- promptsforobjectexsistenceandattributesrespectively:
els that generate textual outputs given multi-modal input,
Q: Is there a(n) <object> in the image?
namelyanimageandatextualprompt. Q: Is the <object> of <attribute> in the image?Figure1.OverviewofourH-POPEbenchmark
SamplingSetting #Images
Random 994
Popular 994
Frequency-basedAdversarial 994
Image-basedAdversarial 926
Figure2.Overviewofourimage-basedadversarialsamplingstrat-
Table1. Numberofuniqueimagesusedinouranalysisforeach
egy.Foragivenobject,e.g.theobjectroadsign,werandomlyse-
samplingstrategy.
lectanattributefromanotherobjectintheimage,e.g.theattribute
greenfromtheobjecttree.Wethenaskwhethertheoriginalobject
hasthatattribute.
by any model across all settings is roughly 76.76%. This
number is lower than that in POPE, where InstructBLIP
achieved 88.73% accuracy in the random setting [5]. This
Metrics. Ourbenchmarkisformulatedasabinaryclassifi-
issomewhatexpectedsinceattributesarefinerdetailscom-
cationtask,andthuswereportaccuracy,F1-score,precision
paredtoobjectpresence,andthusquestionsaboutattributes
andrecall.
are harder than those about objects existence. In fact, the
Data. Our evaluation uses images and objects from the
accuracyscoresonobjectpresenceforLLaVaandInstruct-
MSCOCO (val. 2014) [6], and attribute annotations from
BLIP are approximately 10% higher than their scores on
theLSAdataset[8].Weselectallimagesforwhichwehave
attributepresence. Ontheotherhand,mPLUG-Owlalways
atleastsixquestions(twoquestionsonobjectexistence,and
performs worst, with its accuracy scores ranging between
fourquestionsonattributes). Forimagesforwhichwehave
50% and 60% across all settings. Surprisingly, this model
morethansixquestions,wesamplesixquestionsatrandom.
performs slightly better on attributes than on object exis-
Additionally,ifanimagehasmultipleannotatedobjects,we
tence. In addition, the most challenging setting is our ad-
selectarandomonetopreventsomeimagesfromappearing
versarial(image-based)setting,whereallthemodelssuffer
morefrequentlythanothers. Withthisweareleftwith994
theworstaccuracyscores.
imagesforrandom-,popularandfrequency-basedadversar-
It is likely that the difference in performance be-
ialsamplingand926imagesforourimage-basedsampling
tween mPLUG-Owl and the other two models arises from
strategy. TheexactnumbersareshowninTable1.
mPLUG-Owl using LLAMA [11] as its language model
backbone, while the other two models use Vicuna [2].
4.2.EvaluationResults
The latter is fine-tuned on GPT-4 and ChatGPT conver-
Our results are summarised in Table 2, which includes sations, while the former is not fine-tuned at all. While
the overall performance as well as a breakdown by ob- all three models fine-tune their final model in the multi-
jectpresenceandattributes. Thehighestaccuracyachieved modal setting, it seems plausible that the fine-tuned lan-guage backbone provides improved instruction following
ability,whichmakesLLaVaandInstructBLIPperformbet-
teronthisbenchmark.
5.FurtherAnalyses
5.1.Ablation1: PositiveAnswerBias
POPEfindsthatthemodels’answersarehighlyskewed
towards being positive (i.e., more likely to answer “yes”)
[5]. Our results show a similar tendency (Table 2), where
Figure 4. Pipeline for aggregating and visualizing the obtained
ahighoverallratioof“yes”to“no”isobserved. However,
relevancemaps. Weselecttherelevancemapsfortokensthatare
thispositiveanswerbiasonlyholdsforquestionsaboutob-
either“Yes”,“No”ornametheobjectorattributeweaskedabout.
ject existence. For questions about attributes, models are
The resulting maps are averaged. Afterwards, in line with [10],
morelikelytoanswer“no”inmostcases.
the maps are up-scaled to match the image size and plotted as a
heatmapontopoftheimage.
5.2.Ablation2: Context
In our evaluation, questions are asked sequentially in a
chat. We further investigate whether the models are sensi-
tive to the context, which might contain incorrect answers returns one relevance map per predicted token in the an-
in previous questions. An example of a chat (questions swer,whichindicateshowrelevanteachpatchintheimage
asked in context) is shown in Table 3. We therefore com- wastopredictingthecorrespondingtoken. Toobtainasin-
pare model performance with context vs. without context glerelevancemapperanswer,weselecttherelevancemaps
(Figure3). Similarresultsinbothsettingssuggestthatour correspondingtothetokens{“Yes”,“No”,“$object”,“$at-
benchmarkremainsstableregardlessofthepresenceofcon- tribute”}, where $object and $attribute are replaced with
textinformation. the object and attribute mentioned in the question. These
maps are then averaged and upscaled to match the image
size. To obtain the final visualization, they are plotted as
heatmapsontopoftheimage. Thelasttwostepsfollowthe
approach taken by LVLM-Interpret. In our qualitative ex-
amples(Figure5),relevantimagepatchesoftencorrespond
tothecorrectobject. Interestingly,thelocationsoftherel-
evant patches mostly remain the same, varying only in in-
tensity.Duetothisfact,nonotabledifferencesareobserved
betweencorrectandhallucinatedanswers.
6.Limitations
Ouraimistobetterunderstandhallucinationonafiner-
grained level, i.e. object attributes. Our H-POPE bench-
mark expects a clear yes or no answer from the model.
Figure 3. Difference in performance when asking our questions While this is usually the case in our experiments, we also
sequentiallyinachatvs.askingthemindividuallywithoutcontext.
encountersituationswherethemodelsfailtodoso. Inspe-
cific, InstructBLIP missed66 answersout of91912, while
LLaVa missed 58 answers, and mPLUG-Owl missed 417
5.3.VisualCueAnalysis
answers. Nevertheless, the number of misses is very low
Westudywhetherthemodelsusethecorrectvisualcues comparedtothetotalnumberofquestionsasked. Further-
in the image to arrive at their answers, and thus gain in- more, our current setting is limited to only a few types of
sightsintowhatcauseshallucination. Forexample,incases attributes,leavingothertypessuchastone,pattern,andori-
wherethemodeldoesnothallucinate,onewouldexpectthe entation unexplored. Our H-POPE requires mutual exclu-
imagepatchescorrespondingtotheobjectinquestiontobe sioninattributes,whichmakesitdifficulttoextendtosome
mostrelevant.WeuseLVLM-Interpret[10]tovisualizerel- attribute types. We leave it to future work to devise more
evancemaps[1]forLLaVa[7](figure4). LVLM-Interpret generalsamplingstrategies.Accuracy F1 Yes(%)
Sampling Model All Att. Obj. All Att. Obj. All Att. Obj.
mPLUG-Owl 56.04 57.21 53.81 60.35 55.08 67.16 62.00 45.00 93.00
Random LLaVa 71.36 66.62 80.85 72.55 66.85 82.84 54.00 51.00 62.00
InstructBLIP 76.76 72.22 85.83 74.96 67.95 86.51 43.00 37.00 55.00
mPLUG-Owl 62.63 66.32 55.51 64.61 61.58 68.27 56.00 38.00 92.00
Popular LLaVa 76.47 72.41 84.60 75.64 69.69 85.70 47.00 41.00 58.00
InstructBLIP 76.75 72.50 85.23 74.58 67.42 86.00 41.00 34.00 56.00
mPLUG-Owl 56.19 57.14 54.33 59.92 53.92 67.63 60.00 43.00 92.00
Adv. Img. LLaVa 68.26 64.82 75.12 69.46 64.02 78.61 54.00 48.00 66.00
InstructBLIP 67.48 64.05 74.31 66.81 59.76 77.70 48.00 39.00 65.00
mPLUG-Owl 56.16 57.49 53.61 59.83 54.02 67.16 60.00 42.00 93.00
Adv. Freq. LLaVa 67.89 64.53 74.59 69.29 63.78 78.45 55.00 48.00 68.00
InstructBLIP 70.04 67.44 75.23 69.14 62.91 78.57 47.00 38.00 66.00
Table2.Evaluationresultson(All)allquestionscombined,(Att.)questionsaboutattributesand(Obj.)questionsaboutobjects.
Position Question Answer Label
1 Isthereafenceintheimage? Yes,thereisafenceintheimage. yes
2 Isthefenceofwoodenmaterialintheimage? No,thefenceintheimageismadeofmetal. no
3 Isthefenceofironmaterialintheimage? No,thefenceintheimageismadeofmetal. yes
4 Isthefenceofmetalmaterialintheimage? Yes,thefenceintheimageismadeofmetal. yes
5 Isthefenceblackintheimage? Yes,thefenceintheimageisblack. no
Table3.Exampleofachatgiventothemodels.
7.Conclusion
In this work, we introduce H-POPE, an extension of
(a) POPE[5], toassessthetendencyofLVLMstohallucinate
attributes of objects in images. Our benchmark demon-
strates that current LVLMs hallucinate worse on attribute
questionsthanonquestionsaboutobjectexistence. Inaddi-
Istheceilingwhite Istheceilingbrown
InputImage Q: intheimage? Q: intheimage? tion,ournovelnegativesamplingstrategyshowsthatthese
Yes,theceiling No,theceiling models are likely to assign attributes to the wrong objects
A: intheimage A: intheimage .
iswhite. isnotbrown in the image. Finally, analysing relevance maps does not
provideinsightsintothesourceofhallucinations. Wehope
thatthisworkwillcontributetomoreaccuratehallucination
assessmentofLVLMs.
(b)
Acknowledgements. Our sincerest thanks goes to Haoran
Wangforhisvaluablefeedbackthroughoutthedevelopment
oftheproject.
Isthecurtainblue Isthecurtainwhite
InputImage Q: intheimage? Q: intheimage? The authors further thank the International Max Planck
Yes,thecurtain No,thecurtain Institute for Informatics (IMPRS-TRUST) for supporting
A: intheimageis A: intheimageis
blue. notwhite. Nhi Pham and the Zuse School ELIZA for supporting
MichaelSchott.
Figure 5. Examples of relevance maps for correct answers (in
green)andincorrectanswers(inred).References mplug-owl: Modularizationempowerslargelanguagemod-
elswithmultimodality,2024. 1,2
[1] Hila Chefer, Shir Gur, and Lior Wolf. Generic
Attention-model Explainability for Interpreting Bi-Modal
A.CompleteEvaluationMetrics
andEncoder-DecoderTransformers. In2021IEEE/CVFIn-
ternational Conference on Computer Vision (ICCV), pages We show complete tables including accuracy, f1-score,
387–396,Montreal,QC,Canada,Oct.2021.IEEE. 4
precision, recall and percentage of positive answers for all
[2] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
questionsinTable4,forquestionsaboutobjectsinTable5
haoWu,HaoZhang,LianminZheng,SiyuanZhuang,Yong-
andforquestionsaboutattributesinTable6.
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Wealsoshowtheprompttemplatesusedforeachmodel
Xing. Vicuna: An open-source chatbot impressing GPT-4
inTable7. Wefollowedthesuggestionsfromthehugging-
with90%*ChatGPTquality,Mar.2023. 3
facepages12 ofthecorrespondingmodels,withtheexcep-
[3] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat
tionofInstructBLIP,whichdidnotprovideone. Wethere-
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung,andStevenHoi.Instructblip:towardsgeneral-purpose
foreusethedefaulttemplatesuggestedbyhuggingface3.
vision-languagemodelswithinstructiontuning. InProceed-
ings of the 37th International Conference on Neural Infor-
mationProcessingSystems,NIPS’23,RedHook,NY,USA,
2024.CurranAssociatesInc. 1,2
[4] Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija
Jain, and Aman Chadha. Exploring the frontier of vision-
language models: A survey of current methodologies and
futuredirections. arXivpreprintarXiv:2404.07214,2024. 2
[5] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao,andJi-RongWen. Evaluatingobjecthallucinationin
largevision-languagemodels,2023. 1,2,3,4,5
[6] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence
Zitnick. Microsoft coco: Common objects in context. In
ComputerVision–ECCV2014: 13thEuropeanConference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
PartV13,pages740–755.Springer,2014. 3
[7] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improvedbaselineswithvisualinstructiontuning,2024. 1,
2,4
[8] KhoiPham,KushalKafle,ZheLin,ZhihongDing,ScottCo-
hen,QuanTran,andAbhinavShrivastava.Improvingclosed
andopen-vocabularyattributepredictionusingtransformers.
InEuropeanconferenceoncomputervision,pages201–219.
Springer,2022. 2,3
[9] AnnaRohrbach,LisaAnneHendricks,KayleeBurns,Trevor
Darrell, and Kate Saenko. Object Hallucination in Image
Captioning,Mar.2019. 1,2
[10] GabrielaBenMelechStan,EstelleAflalo,RaananYehezkel
Rohekar, Anahita Bhiwandiwalla, Shao-Yen Tseng,
Matthew Lyle Olson, Yaniv Gurwicz, Chenfei Wu, Nan
Duan,andVasudevLal. Lvlm-interpret: Aninterpretability
toolforlargevision-languagemodels,2024. 1,4
[11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet,Marie-AnneLachaux,TimotheeLacroix,Baptiste
Rozie`re,NamanGoyal,EricHambro,FaisalAzhar,Aurelien
Rodriguez, ArmandJoulin, EdouardGrave, andGuillaume
Lample. LLaMA:OpenandEfficientFoundationLanguage
1https://huggingface.co/docs/transformers/en/
Models. 3
model_doc/llava
[12] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,
2https://huggingface.co/MAGAer13/mplug-owl-
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, llama-7b
YayaShi,ChenliangLi,YuanhongXu,HehongChen,Jun- 3https://huggingface.co/docs/transformers/en/
fengTian,QiQian,JiZhang,FeiHuang,andJingrenZhou. chat_templatingAccuracy F1 Precision Recall Yes(%)
Sampling Model
mPLUG-Owl 56.04 60.35 54.28 67.95 62.00
Random LLaVa 71.36 72.55 69.68 75.66 54.00
InstructBLIP 76.76 74.96 81.18 69.63 43.00
mPLUG-Owl 62.63 64.61 60.74 69.00 56.00
Popular LLaVa 76.47 75.64 78.39 73.08 47.00
InstructBLIP 76.75 74.58 82.20 68.25 41.00
mPLUG-Owl 56.19 59.92 54.83 66.04 60.00
Adv. Img. LLaVa 68.26 69.46 66.91 72.21 54.00
InstructBLIP 67.48 66.81 68.15 65.52 48.00
mPLUG-Owl 56.16 59.83 54.64 66.12 60.00
Adv. Freq. LLaVa 67.89 69.29 66.39 72.46 55.00
InstructBLIP 70.04 69.14 71.22 67.18 47.00
Table4.Evaluationresultsonallquestionscombined.
Accuracy F1 Precision Recall Yes(%)
Sampling Model
mPLUG-Owl 53.81 67.16 50.85 98.90 93.00
Random LLaVa 80.85 82.84 75.10 92.35 62.00
InstructBLIP 85.83 86.51 82.36 91.10 55.00
mPLUG-Owl 55.51 68.27 52.09 99.03 92.00
Popular LLaVa 84.60 85.70 79.95 92.35 58.00
InstructBLIP 85.23 86.00 81.60 90.91 56.00
mPLUG-Owl 54.33 67.63 51.68 97.84 92.00
Adv. Img. LLaVa 75.12 78.61 68.91 91.47 66.00
InstructBLIP 74.31 77.70 68.49 89.78 65.00
mPLUG-Owl 53.61 67.16 50.96 98.47 93.00
Adv. Freq. LLaVa 74.59 78.45 68.11 92.48 68.00
InstructBLIP 75.23 78.57 69.08 91.09 66.00
Table5.Evaluationresultsonobjectquestions.
Accuracy F1 Precision Recall Yes(%)
Sampling Model
mPLUG-Owl 57.21 55.08 57.96 52.48 45.00
Random LLaVa 66.62 66.85 66.39 67.32 51.00
InstructBLIP 72.22 67.95 80.29 58.90 37.00
mPLUG-Owl 66.32 61.58 71.66 53.98 38.00
Popular LLaVa 72.41 69.69 77.30 63.44 41.00
InstructBLIP 72.50 67.42 82.69 56.92 34.00
mPLUG-Owl 57.14 53.92 58.31 50.14 43.00
Adv. Img. LLaVa 64.82 64.02 65.51 62.59 48.00
InstructBLIP 64.05 59.76 67.87 53.38 39.00
mPLUG-Owl 57.49 54.02 58.82 49.95 42.00
Adv. Freq. LLaVa 64.53 63.78 65.16 62.45 48.00
InstructBLIP 67.44 62.91 73.08 55.23 38.00
Table6.Evaluationresultsonattributequestions.Model Template
Human: $question
AI: $answer
mPLUG-Owl Human: $question
AI:
USER: $question ASSISTANT: $answer</s>USER: $question ASSISTANT:
LLaVa
<|user|>
$question
<|assistant|>
$answer
InstructBLIP <|user|>
$question
<|assistant|>
Table7.Chattemplatesusedforeachmodel.Thedollarsign$denotesthatthisisavariabletobereplacedwitheitherthequestionorthe
answer.