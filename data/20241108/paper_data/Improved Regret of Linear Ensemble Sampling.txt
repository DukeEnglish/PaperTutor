Improved Regret of Linear Ensemble Sampling
HarinLee Min-hwanOh
SeoulNationalUniversity SeoulNationalUniversity
Seoul,SouthKorea Seoul,SouthKorea
harinboy@snu.ac.kr minoh@snu.ac.kr
Abstract
Inthiswork,weclosethefundamentalgapoftheoryandpracticebyproviding
animprovedregretboundforlinearensemblesampling. Weprovethatwithan
ensemblesizelogarithmicinT,linearensemblesamplingcanachieveafrequen-
√
tistregretboundofO(cid:101)(d3/2 T),matchingstate-of-the-artresultsforrandomized
linearbanditalgorithms,wheredandT arethedimensionoftheparameterand
the time horizon respectively. Our approach introduces a general regret analy-
sisframeworkforlinearbanditalgorithms. Additionally,werevealasignificant
relationshipbetweenlinearensemblesamplingandLinearPerturbed-HistoryEx-
ploration (LinPHE), showing that LinPHE is a special case of linear ensemble
samplingwhentheensemblesizeequalsT. Thisinsightallowsustoderiveanew
√
regretboundofO(cid:101)(d3/2 T)forLinPHE,independentofthenumberofarms. Our
contributionsadvancethetheoreticalfoundationofensemblesampling,bringingits
regretboundsinlinewiththebestknownboundsforotherrandomizedexploration
algorithms.
1 Introduction
Ensemblesampling[16]hasemergedasanempiricallyeffectiverandomizedexplorationtechnique
invariousonlinedecision-makingproblems,suchasonlinerecommendation[17,27,26]anddeep
reinforcementlearning[18–20]. Despiteitspopularity,thetheoreticalunderstandingofensemble
samplinghaslaggedbehind,evenforthelinearbanditproblem,withpreviousresultsrevealingsub-
optimaloutcomes. Forinstance,apriorwork[21]demonstratedthatlinearensemblesamplingcould
√
achieveO( T)BayesianregretwithanensemblesizegrowingatleastlinearlywithT. However,
therequirementfortheensemblesizetobelinearinT ishighlyunfavorableandprohibitiveinmany
practicalsettings.Arecentwork[10]showedthatasymmetrizedversionoflinearensemblesampling
couldprovideanimprovementindependenceonensemblesizeofΘ(dlogT)andshowafrequentist
√
regretboundofO(cid:101)(d5/2 T). However,thisregretboundclearlyfallsshortoftheexistingfrequentist
regretachievedbystandardrandomizedalgorithmssuchasThompsonSampling(TS)[4,2]and
Perturbed-HistoryExploration(PHE)[11–13].1
In this work, we close this fundamental gap by providing an improved regret bound for linear
ensemble sampling. We prove that linear ensemble sampling with an ensemble size logarithmic
√
in T can still attain a frequentist regret bound of O(cid:101)(d3/2 T), marking the first time that linear
ensemblesamplingachievesastate-of-the-artresultforrandomizedlinearbanditalgorithms. Our
√ √ √
1LinTS[4,2]hasregretofO(cid:101)(cid:0) min(d3/2 T,d TlogK)(cid:1) ,andLinPHE[12]hasregretofO(cid:101)(d TlogK).
√ √
ForexchangesbetweenfactorsofO( d)andO( logK),werefertotheanalysisofAgrawalandGoyal[4].
Therefore,theexistingresultforlinearensemblesampling[10]hasagapofatleastO(d)comparedtoLinTS
andLinPHE.Besidesthisgapinregretbounds,thereappearstoberathercounter-intuitivedependenceon
ensemblesizeinJanzetal.[10](seeRemark2inSection4).
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
voN
6
]LM.tats[
1v23930.1142:viXraapproachnotonlyimprovesupontheregretboundbutalsosimplifiesthealgorithmbyavoidingthe
useofsymmetrizedperturbations,makingitmorepracticalforimplementation. Forregretanalysis,
wepresentageneral,conciseframeworkforanalyzinglinearbanditalgorithms,whichmaybeof
independentinterest.Furthermore,werigorouslyrevealthesignificantrelationshipbetweenensemble
samplingandPHEforthefirsttime,showingthatintheregimewheretheensemblesizeequalsT,
linearPHE(LinPHE)isaspecialcaseoflinearensemblesampling. Withthisnewinsight,wecanuse
√
theregretanalysisforensemblesamplingtoderiveanewregretboundofO(cid:101)(d3/2 T)forLinPHE,
whichisindependentofthenumberofarmsK.
Ourmaincontributionsaresummarizedasfollows:
√
• WeproveaO(cid:101)(d3/2 T)regretbound(Theorem1)forlinearensemblesamplingwithanensemble
sizeofm = Ω(KlogT),whereK denotesthenumberofarms. Importantly,ourregretbound
√
doesnotdependonK ormevenlogarithmically. OurresultisthefirsttoestablishO(cid:101)(d3/2 T)
regretforlinearensemblesamplingwithanensemblesizesublinearinT,improvingtheprevious
boundbythefactordwhilemaintainingtheensemblesizetobelogarithmicinT.
• Aspartoftheregretanalysis,wepresentageneralregretanalysisframework(Theorem2)forlinear
banditalgorithms. Thisframeworknotonlygeneralizestheregretanalysisofrandomizedalgo-
rithmssuchasensemblesamplingandPHEbutalsoappliestootheroptimism-baseddeterministic
algorithms. Thisresultcanbeofindependentinterestbeyondensemblesampling.
• WerigorouslyinvestigatetherelationshipbetweenlinearensemblesamplingandLinPHE.We
showthatintheregimeofensemblesizem=T,LinPHEisaspecialcaseofthelinearensemble
samplingalgorithm. Toourbestknowledge,thisisthefirstresulttoshowtheequivalencebetween
linearensemblesamplingandLinPHE.
• Asabyproduct,withthisnewinsightintotherelationshipbetweenlinearensemblesamplingand
LinPHE,weprovideanalternateanalysisforLinPHEasanextensionoftheanalysisforlinear
√
ensemblesampling,achievingaO(cid:101)(d3/2 T)regretboundwithnodependenceonK.
1.1 RelatedWork
Thestochasticlinearbanditproblem[5,1,14]isafoundationalsequentialdecision-makingproblem
andacoremodelformulti-armedbanditswithfeatures. Numerousalgorithmshavebeendeveloped
for this problem, including deterministic approaches such as UCB-based methods [5, 8, 1] and
randomizedalgorithmssuchasThompsonsampling[24,7,4,2]andPHE[11–13].
Thompsonsampling[24],aclassicalrandomizedmethod,utilizestheposteriordistributionofhidden
parameters based on observed data. Initially proposed for Bayesian settings [22, 23], it has also
demonstratedstrongperformanceinfrequentistsettings[3,4,2]. Forthestochasticlinearbandit,
AgrawalandGoyal[4]showedthatThompsonsamplingwithaGaussianpriorachievesaregret
√ √
boundofO(cid:101)(d3/2 T), whichcanbereducedtoO(cid:101)(d T logK)forsmallK. However, applying
Thompson sampling to more complex problems remains challenging, especially when posterior
computationbecomesintractable,thoughapproximatemethodshavebeenproposed[16,25].
PHE[11–13]isanotherclassofrandomizedalgorithmsthatdoesnotrelyonposteriordistributions,
makingitpotentiallyapplicabletomorecomplexsettings. Inthefinite-armedlinearbanditmodel,
√
PHEachievesaO(cid:101)(d T logK)regretbound,matchingtheperformanceofThompsonsamplingfor
finitearms[4]. However,therelationshipbetweenPHEandensemblesamplingremainsunexplored
inpreviousstudies.
Ensemblesampling[16]hasgainedpopularityasarandomizedexplorationmethodacrossvarious
decision-makingtasks[17,27,26,18–20]. Despiteitsempiricalsuccess,itstheoreticalfoundation,
particularlyforlinearbandits,isstillrelativelyunderdeveloped. Qinetal.[21]showedthatlinear
√
ensemblesamplingachievesO( T)Bayesianregretbutrequiresanimpracticallylargeensemble
sizethatscaleslinearlywithT. Morerecently,Janzetal.[10]reducedthedependenceonensemble
√
sizetoΘ(dlogT)andachievedafrequentistregretboundofO(cid:101)(d5/2 T). However,thefrequentist
√
regretboundofO(cid:101)(d3/2 T)forensemblesamplinghasyettobeachieved.
2Algorithm1LinearEnsembleSampling
Input: regularizationparameterλ>0,ensemblesizem∈N,
initialperturbationdistributionP onRd,reward-perturbationdistributionP onR,
I R
ensemblesamplingdistribution{J }T on[m]
t t=1
SampleWj ∼P foreachj ∈[m].
I
InitializeV =λI ,Sj =Wj,θj =V−1Sj foreachj ∈[m]
0 d 0 0 0 0
fort=1,2,...T do
Samplej ∼J
t t
PullarmX =argmax x⊤θjt andobserveY
t x∈X t−1 t
UpdateV =V +X X⊤
t t−1 t t
forj =1,2,...,mdo
SampleZj ∼P
t R
UpdateSj =Sj +X (Y +Zj) and θj =V−1Sj
t t−1 t t t t t t
endfor
endfor
2 Preliminaries
2.1 Notations
Ndenotesthesetofnaturalnumbersstartingfrom1. ForapositiveintegerM,[M]denotestheset
{1,2,...,M}. 0 denotesthezerovectorinRd andI denotestheidentitymatrixinRd×d. We
d d
defineandworkwithinaprobabilityspace(Ω,F,P),whereΩisthesamplespace,F istheeventset,
andPistheprobabilitymeasure.N(µ,Σ)denotestheuni-ormulti-variateGaussiandistributionwith
meanµandcovarianceΣ. ∧denoteslogicalconjunction(“and”)and∨denoteslogicaldisjunction
(“or”). Withslightabuseofnotation,wewrite{ω ∈Ω:A}andAinterchangeablywhenAissome
condition,forsimplicity. O(·)denotestheasymptoticgrowthratewithrespecttoproblemparameters
d,T,andK. O(cid:101)(·)furtherhideslogarithmicfactorsofT andd.
2.2 ProblemSetting
Weconsiderthestochasticlinearbanditproblem. Thelearningagentispresentedwithanon-empty
arm set X ⊂ Rd. For T time steps, where T ∈ N is the time horizon, the agent selects an arm
X ∈X andreceivesareal-valuedrewardY ,wheretherewardisgeneratedbasedonahiddentrue
t t
parametervector,θ∗ ∈Rd. Specifically,Y isdefinedasfollows:
t
Y =X⊤θ∗+η ,
t t t
whereη isazero-meanrandomnoise. Theobjectiveoftheagentistomaximizethecumulative
t
reward,orequivalently,tominimizethecumulativeregretR(T)definedas
T (cid:18) (cid:19)
(cid:88)
R(T):= supx⊤θ∗−X⊤θ∗ .
t
x∈X
t=1
3 EnsembleSamplingforLinearBandits
Algorithm1describeslinearensemblesampling. Thelearnermaintainsanensembleofmestimators,
whereeachestimatorfitsperturbedrewards. Forthej-thestimator,arandomvectorWj ∈Rdacts
asaninitialperturbationontheestimator,andarandomvariableZj perturbstherewardattimet.
t
Specifically,θj isthesolutionofthefollowingminimizationproblem:
t
t
mi θn ∈im
Rdizeλ(cid:13) (cid:13)θ−Wj/λ(cid:13) (cid:13)2 2+(cid:88)(cid:16)
X
i⊤θ−(cid:16)
Y i+Z
ij(cid:17)(cid:17)2
(1)
i=1
When selecting an arm, one of the m estimators is chosen according to an ensemble sampling
distributionJ andactsgreedilywithrespecttothesampledestimator. Previousensemblesampling
t
3algorithms[16,21,10]sampletheestimatorsuniformlyfromtheensemble,butweallowanypolicy
forselectingtheestimator. FurtherdistinguishingfromthealgorithmpresentedinJanzetal.[10],we
donotsampleRademacherrandomvariablesforsymmetrization,makingouralgorithmsimpler.
Ensemblesamplingiscapableofbeinggeneralizedtocomplexsettingswheneversolvingminimiza-
tionproblem(1)istractable. Especiallywhenincrementalupdatesoftheminimizationproblemare
cheap,forinstancewithneuralnetworksorothergradientdescent-basedmodels,ensemblesampling
canbeanefficientexplorationstrategy. Thealgorithmmaysimplystoreanensembleofmodels,
sample one to select an action, and then update the models incrementally based on the observed
rewardandgeneratedperturbation.
4 RegretBoundofLinearEnsembleSampling
Before we present the regret bound of linear ensemble sampling (Algorithm 1), we present the
followingstandardassumptionsontheproblemstructure.
Assumption1(Armsetandparameter). X isclosedandforallx ∈ X,∥x∥ ≤ 1. Thereexists
2
S >0suchthat∥θ∗∥ ≤S. Bothboundsareknowntotheagent.
2
Remark 1. Under Assumption 1, X is a compact set. Therefore, we can define x∗ :=
argmax x⊤θ∗andrewritethedefinitionofR(T)as(cid:80)T x∗⊤θ∗−X⊤θ∗.
x∈X t=1 t
Forarigorousstatementofthesecondassumption,wedefineseveralfiltrations. Fort∈[T]∪{0},
letFX :=σ(X ,...X )andFη :=σ(η ,...,η )betheσ-algebrasgeneratedbyX andη upto
t 1 t t 1 t i i
timetrespectively. Wealsodefinetheσ-algebrageneratedbythealgorithm’sinternalrandomness
up until the choice of X as FA. Let F := σ(cid:0) FA∪FX ∪Fη(cid:1) be the σ-algebra generated by
t t t t t t
the first t iterations of the interaction between the environment and the agent. In addition, let
F− :=σ(cid:0) FA∪FX ∪Fη (cid:1) betheσ-algebrageneratedinthesamewayasF ,butexcludingη .
t t t t−1 t t
Assumption2(Noise). Thereexistsσ ≥0suchthatη isF−-conditionallyσ-subGaussianforall
t t
t∈[T],i.e.,E(cid:2) exp(sη )|F−(cid:3) ≤exp(cid:0) σ2s2/2(cid:1) holdsalmostsurelyforalls∈R.
t t
Now, we define a value β to describe the variance of the generated perturbation values. Define
t
(cid:113) √
a sequence {β (δ)}∞ as β (δ) := σ dlog(cid:0) 1+ t (cid:1) +2log1 + λS. We may omit δ when
t t=0 t dλ δ
itsvalueisclearfromthecontext. Thedefinitionofβ comesfromAbbasi-Yadkorietal.[1]asa
t
confidenceradiusoftheridgeestimator,whichwelaterspecifyinLemma1. Wenowpresentthe
regretboundoflinearensemblesampling(Algorithm1).
Theorem1(Regretboundoflinearensemblesampling). Fixδ ∈(0,1]. Assume|X|=K <∞and
runAlgorithm1withλ≥1,m≥C(KlogT +log1),P =N(0 ,λβ2I ),P =N(0,β2),and
δ I d T d R T
J =Unif(m),whereC isauniversalconstantandUnif(m)denotestheuniformdistributionover
t
[m]. Then,withprobabilityatleast1−4δ,thecumulativeregretofAlgorithm1is
(cid:16) √ (cid:17)
R(T)=O (dlogT)23 T .
√
DiscussionofTheorem1. Theorem1showsthatAlgorithm1achievesaO(cid:101)(d3/2 T)frequentist
regretboundwithanensemblesizeofm = Ω(KlogT). Importantly,ourregretbounddoesnot
depend on K or m even logarithmically. Hence, this regret bound matches the state-of-the-art
frequentist regret bound of linear Thompson sampling [4, 2]. Our result is the first to establish
√
O(cid:101)(d3/2 T)regretforlinearensemblesamplingwithanensemblesizesublinearinT,improvingthe
previousboundbythefactordcomparedtotheexistingresultinJanzetal.[10]. Weconjecturethat
√
O(cid:101)(d3/2 T)regretishighlylikelytobethebestboundforlinearensemblesamplingbasedonthe
negativeresultinHamidiandBayati[9]forLinTS.2ComparingwiththealgorithminJanzetal.[10],
ourversionoflinearensemblesamplingalgorithmdoesnotutilizeRademacherrandomvariable
forsymmetrizedperturbation. ThisallowsouralgorithmtobesimplerthanthatofJanzetal.[10].
Partiallyduetothisalgorithmicdifference,ourregretanalysisisquitedistinctfromtheanalysisof
Janzetal.[10](seetheproofinSection5.2).
√
2Hamidi and Bayati [9] have shown that LinTS without the posterior variance inflation by d factor
(comparedtooptimism-basedalgorithmssuchasLinUCBorOFUL[1])canleadtoalinearregretinT.Thatis,
√
thefrequentistregretboundofO(cid:101)(d3/2 T)forLinTSisthebestonecanderiveforthealgorithm.
4Table1: Comparisonofregretboundsforlinearensemblesampling
Paper Frequentist/Bayesian RegretBound EnsembleSize
LuandVanRoy[16] Frequentist Invalid Invalid
√
Qinetal.[21] Bayesian O(cid:101)( dT logK) Ω(KT)
√
Janzetal.[10] Frequentist O(cid:101)(d5/2 T) Θ(dlogT)
√
Thiswork Frequentist O(cid:101)(d3/2 T) Ω(KlogT)
As in Lu and Van Roy [16] and Qin et al. [21], we study the finite-armed problem setting. Both
studiesanalyzetheexcessregretofensemblesamplingcomparedtoThompsonsamplingthroughan
informationtheoreticalapproach. However,directcomparisonsoftheregretboundsarenon-trivial.
TheanalysisbyLuandVanRoy[16]includesanerroradmittedbytheauthorsandQinetal.[21]
analyzetheBayesianregret,whichisaweakernotionofregretthanthefrequentistregretthatwe
analyzeinthiswork. AlongwithJanzetal.[10],ourresultalsomakesaprogressinreducingthe
sizeoftheensemblecomparedto LuandVanRoy[16]andQinetal.[21]. Thesizeoftheensemble
requiredbyJanzetal.[10]isΘ(dlogT).Thisrequirementimpliesthattheirensemblesizemaybe
smallerthanourswhenK islargerthandandalsoallowsK tobeinfinite. However,itisimportant
√
tonotethattheirresultingregretboundofO(cid:101)(d5/2 T)isclearlysub-optimalcomparedtoregret
boundsofotherrandomizedexplorationalgorithms. Theorem1achievesthetighterregretbound
whilesimultaneouslyreducingthesizeoftheensemble.
Remark2(Counter-intuitivedependenceonensemblesizeinJanzetal.[10]). Theregretbound
inJanzetal.[10]actuallygrowssuper-linearlywiththeensemblesize,whichiscounter-intuitive.
Theirregretboundimpliesthatastheensemblesizeincreases, theperformanceofthealgorithm
deteriorates. Thisfailstoexplainthesuperiorempiricalperformanceobservedforensemblesampling
evenwithalargeensemble. Onthecontrary,ourresultinTheorem1doesnotshowanyperformance
degradationastheensemblesizeincreases.
Remark3(Generalizabilityofperturbationdistributions). WeshowthatGaussiandistribution
forperturbationisnotessential. TheonlypropertiesoftheGaussiandistributionweutilizeareitstail
probabilityandanti-concentrationproperty,statedasLemma4andFact1inSection5.2. Therefore,
any other distributions exhibiting similar behaviors can instead be adopted. In Appendix H, we
rigorouslydemonstratethatanysymmetricsubGaussiandistributionwithlower-boundedvariancecan
beemployed,possiblyatacostofaconstantfactor. Alargeclassofdistributions,includinguniform
distribution,sphericaldistribution,Rademacherdistribution,andcenteredbinomialdistributionwith
p=1/2satisfythiscondition. Thisresultcanbeofindependentinterest.
5 Analysis
5.1 GeneralRegretAnalysisforLinearBandits
Webeginbypresentingageneralregretboundforanyalgorithmthatselectsthebestarmbasedonan
estimatedparameter.Thisresultcanbeofindependentinterest.Thisgeneralboundandanalysisserve
asageneralframeworkthatincludestheregretanalysisoflinearensemblesampling(Theorem1).
Theorem2(Generalregretboundforlinearbanditalgorithm). FixT ∈N. Assumethatateachtime
stept∈[T],theagentchoosesX =argmax x⊤θ ,whereθ ∈Rdischosenbytheagentunder
t x∈X t t
some(eitherdeterministicorrandom)policy. Letλ>0andV =λI+(cid:80)t X X⊤. Let{E }T
t i=1 i i 1,t t=1
and{E }T besequencesofeventsthatsatisfytwoconditions:
2,t t=1
1. (Concentration)Thereexistsaconstantγ >0suchthat
∥θ −θ∗∥ 1{E }≤γ (2)
t Vt−1 1,t
holdsalmostsurelyforallt∈[T].
2. (Optimism)E ∈F holdsandthereexistsaconstantp∈(0,1]suchthat
2,t t−1
P(cid:0)(cid:0) x∗⊤θ∗ ≤X⊤θ andE (cid:1) orEC |F (cid:1) ≥p (3)
t t 1,t 2,t t−1
holdsalmostsurelyforallt∈[T].
5TakeE = ∩T (E ∩E )andanyδ ∈ (0,1]. Then,undertheeventE andanadditionalevent
t=1 1,t 2,t
whoseprobabilityisatleast1−δ,thecumulativeregretisboundedasfollows:
(cid:18) (cid:19)(cid:115) (cid:18) (cid:19) (cid:114)
2 T γ 2T 1
R(T)≤γ 1+ 2dT log 1+ + log . (4)
p dλ p λ δ
DiscussionofTheorem2. Theaudiencewell-versedintheregretanalysisofrandomizedalgorithms
suchasTSandPHEwouldrecognizethatboundingtheregretusingtheprobabilityofbeingoptimistic
isastandardprocedure,alsopresentedinTheorem1ofKvetonetal.[12]andTheorem2ofJanz
etal.[10]asgeneralizationsoftheresultsinAgrawalandGoyal[4]andAbeilleandLazaric[2]
respectively. However, our regret analysis offers much more concise approach than the existing
techniques,whichcanbeofindependentinterestbeyondtheanalysisofensemblesampling.
Theorem2statesthatifthetwoconditions,specificallyconcentrationin(2)andoptimismin(3),are
√
met,thenthealgorithmachieves T regret. WeprovidetheproofofTheorem2inAppendixB.Our
prooftechniquegeneralizesthewell-studiedanalysisofAbeilleandLazaric[2]. Whiletheirwork
posesconditionsonad-dimensionalperturbationvectorthatisaddedtotheridgeestimator,wedo
notassumetheuseofridgeregressionnorweassumethattheestimatorisperturbed. Instead,weonly
poseconditionsonthefinalestimatorthealgorithmexploits. Duetothisgeneralization,Theorem2
isevencapableofinducingtheregretboundofLinUCB[1],whichalwaysoptsforanoptimistic
estimator,bysettingγ =2β andp=1withappropriateconcentrationeventsassignedtoE and
T 1,t
E . Inaddition,thereareseveralimprovementsthatsimplifytheproofwhichareworthnoting. To
2,t
exploittheoptimismcondition,weapplyMarkov’sinequalityonawell-definedrandomvariable.
TheproofofAbeilleandLazaric[2]reliesondefiningaconditionaldistribution,conditionedonboth
historyandtheeventofbeingoptimistic. However,suchdistributionmaynotbewell-definedifthe
probabilityoftheeventis0forgivenhistory. Janzetal.[10]trytosolvethisproblembyseparately
handlingsuchexceptionalcasesusingconditionalmeasures. However,theirconditionalmeasures
dependontherandomhistory,leadingtheprobabilityptobearandomvariable,whichcomplicates
theanalysis. WealsonotethatourproofdoesnotrequireconvexanalysisstudiedinAbeilleand
Lazaric[2].
Remark4(RoleofeventE ). Previousresultsthatutilizetheprobabilityofbeingoptimistic[4,2,
2,t
12,10]donotexplicitlydefineevents{E } . However,theirexistenceiscrucialinouranalysisof
2,t t
linearensemblesampling. Sincetheperturbationsequencesarealsopartofthehistoryinensemble
sampling,theprobabilityofθ beingoptimisticmaybeextremelysmallundersomeeventsinF
t t−1
thatsampleunfavorablesequences. TheroleofE istoconfineouranalysistothecasewheresuch
2,t
undesirableeventsdonotoccur.
5.2 ProofofRegretBoundinTheorem1
WeprovetheregretboundoflinearensemblesamplingstatedinTheorem1. ToapplyTheorem2,
high-probabilitiesofthesequencesofevents,namely{E ,E }T ,shouldbeguaranteedwithan
1,t 2,t t=1
appropriatevaluesofγ andp. Weshowthatseparateconstraintscanbeimposedontherandomness
oftherewardsandtheperturbationsrespectivelytoguaranteetheprobabilitiesoftheevents. We
beginbydecomposingtheestimatorintotwoparts: onethatfitstheobservedrewardsandtheother
thatperturbstheestimator.
(cid:32) t (cid:33) t (cid:32) t (cid:33)
(cid:88) (cid:16) (cid:17) (cid:88) (cid:88)
θj =V−1Sj =V−1 Wj + X Y +Zj =V−1 X Y +V−1 Wj + X Zj
t t t t i i i t i i t i i
i=1 i=1 i=1
=:θˆ t+θ(cid:101) tj, (5)
where we define θˆ
t
:= V t−1(cid:80)t i=1X iY
i
and θ(cid:101) tj := V t−1(Wj + (cid:80)t i=1X iZ ij). θˆ
t
is the ridge
regressionestimatoroftheobserveddata,anditsrandomnessmainlycomesfromthenoiseofthe
rewards,{η i}t i=1. θ(cid:101) tj istheperturbationaddedtoθˆ t,anditsrandomnesscomesfromthegenerated
perturbation,Wj and{Zj}t . Thefollowinglemmastatesthewell-knownconcentrationresultfor
i i=1
theridgeestimator.
Lemma 1 (Theorem 2 of Abbasi-Yadkori et al. [1]). Fix δ ∈ (0,1]. For t ∈ N∪{0}, define a
(cid:113) √
sequenceofeventswithβ (δ)=σ dlog(cid:0) 1+ t (cid:1) +2log1 + λS as
t dλ δ
Eˆ t :=(cid:110) ω ∈Ω:(cid:13) (cid:13)θˆ t−θ∗(cid:13) (cid:13) Vt ≤β t(δ)(cid:111)
6andtheirintersectionEˆ:=(cid:84)∞ Eˆ. Then,P(cid:0) Eˆ(cid:1) ≥1−δ.
t=0 t
Now,weaddressθ(cid:101)j. Defineaperturbationvectorthatrepresentstheperturbationsequenceforeach
t
modelasfollows:
(cid:16) (cid:17)⊤
Zj := √1 Wj⊤ Zj ··· Zj ∈Rd+t−1,∀j ∈[m]. (6)
t λ 1 t−1
LetZ := Zjt sothatZ istheperturbationvectorofthemodelchosenattimet. Thefollowing
t t t
lemmademonstratesthatoptimismcondition(3)canbesatisfiedbyananti-concentrationpropertyof
Z alone.
t
Lemma2(Sufficientconditionforoptimism). Fort∈[T],defineavectorU ∈Rd+t−1by
t−1
√
U⊤ :=x∗⊤V−1(cid:0) λI X ··· X (cid:1) .
t−1 t−1 d 1 t−1
Then,x∗⊤θ∗ ≤X⊤θ holdswheneverthereexistsaconstantc>0suchthatU⊤ Z ≥c∥U ∥
t t t−1 t t−1 2
and∥θ∗−θˆ ∥ ≤chold.
t−1 Vt−1
WepresentastraightforwardproofofLemma2inAppendixC.Wesignificantlydeviatefromthe
analysesofAbeilleandLazaric[2]andJanzetal.[10]inthemethodofguaranteeingtheoptimism
condition. Theiranalysesrequiread-dimensionalperturbationvectortohaveaconstantprobability
ofhavingpositivecomponent,so-calledanti-concentrated,in“every”possibledirectioninRdsince
theyonlyprovetheexistenceofadirectionthatimpliesoptimism. Weobserveandexploitthefact
thatitsufficestoconsiderjust“one”direction,specificallyU ,toproduceanoptimisticestimator.
t−1
Since U depends only on the sequence of selected arms, the dependency between U and
t−1 t−1
Z decoupleswhenthedependencybetween{X }T and{Z }T aredecoupled,whichwelater
t t t=1 t t=1
achievebytakingtheunionboundinauniqueway.
Thefollowinglemmashowsthatconcentrationandanti-concentrationpropertiesoftheperturbation
aresufficientconditionsforTheorem2. Weprovideasketchofitsproofanddefertheremaining
detailstoAppendixD.
Lemma3. SupposetheagentrunsAlgorithm1withsomeparameters. Fixγ >0andp∈(0,1]. For
(cid:101)
eacht∈[T],definetwoevents
E(cid:101)1,t :=(cid:110) ω ∈Ω:(cid:13) (cid:13)θ(cid:101) tj −t 1(cid:13) (cid:13)
Vt−1
≤γ (cid:101)(cid:111) ,
(cid:110) (cid:16) (cid:17) (cid:111)
E(cid:101)2,t := ω ∈Ω:P U t⊤ −1Z
t
≥β t−1∥U t−1∥
2
andE(cid:101)1,t |F
t−1
≥p ,
whereU t−1isdefinedasinLemma2. TakeE
1,t
=E(cid:101)1,t∩Eˆ t−1andE
2,t
=E(cid:101)2,t∩Eˆ t−1. Then,E 1,tand
E satisfyconcentrationcondition(2)withγ =γ+β andoptimismcondition(3)withthesame
2,t (cid:101) T
valueofp. Consequently,withprobabilityatleast1−2δ−P(E(cid:101)C),whereE(cid:101):= ∩T t=1(E(cid:101)1,t∩E(cid:101)2,t),
Algorithm1achievesregretbound(4)ofTheorem2.
Remark5. Lemma3appliestoanyperturbation-basedalgorithmthatexploitsθ
t
=θˆ t−1+θ(cid:101)t−1,
whereθ(cid:101)t−1isalineartransformofarandomperturbationvectorZ t. AversionoflinearThompson
sampling[2]andLinPHEalsofallintothiscategory.
Lemma3shiftstheproblemofconstructingtheregretboundofAlgorithm1tolower-boundingthe
probabilitiesofthetwosequencesofevents,{E(cid:101)1,t}T t=1and{E(cid:101)2,t}T t=1. NotethatE(cid:101)1,tandE(cid:101)2,tregard
{X }t−1andZ only,andareindependentoffurtherrandomnessof{η }T .
i i=1 t t t=1
SketchofProofofLemma3. The concentration condition follows immediately by the triangle in-
equality. Toshowtheoptimismcondition,weverifythefollowinglogicalimplicationrelationship:
(cid:16)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t(cid:17) ∨E 2C
,t
⇒(cid:0)(cid:0) x∗⊤θ∗ ≤X t⊤θ t(cid:1) ∧E 1,t(cid:1) ∨E 2C ,t,
whereLemma2bridgestheanti-concentrationonthelefthandsidetotheoptimismontherighthand
side. Thisimplicationrelationshipisconvertedtothefollowingprobabilityinequality:
P(cid:0)(cid:0) (x∗⊤θ∗ ≤X t⊤θ t)∧E 1,t(cid:1) ∨E 2C
,t
|F t−1(cid:1) ≥P(cid:16)(cid:16) (U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2)∧E(cid:101)1,t(cid:17) ∨E 2C
,t
|F t−1(cid:17) .
BythedefinitionofE(cid:101)2,t,therighthandsideisboundedbelowbyp,implyingoptimismcondition(3).
TheprobabilityoffailureisboundedbytheunionboundandLemma1.
7Theorem1employstheGaussianperturbationforaconcreteinstantiationofAlgorithm1. Wedefine
twovaluesγ
(cid:101)T
andγ T,whichserveastheconfidenceradiiofθ(cid:101)tandθ tfort∈[T]undertheGaussian
perturbation.
(cid:32)(cid:115) (cid:18) T (cid:19) 2T √ (cid:114) 2T(cid:33)
γ :=β dlog 1+ +2log + d+ 2log , γ :=γ +β . (7)
(cid:101)T T dλ δ δ T (cid:101)T T
NotethatintermsofdandT,bothγ andγ areinO(dlogT). Lemma4illustratestheconcentra-
(cid:101)T T
tionresult. ItsproofisasimpleapplicationofLemma1,andispresentedinAppendixE.
Lemma4. SupposeAlgorithm1isrunwithparametersspecifiedinTheorem1. Fixt ∈ [T]and
j ∈[m]. SupposethesequenceofarmsX ,...,X ischosenarbitrarilyrandomly,notnecessarily
1 t
bythetheagent.Letθ(cid:101) tj bedefinedasinEq.(5).Then,withprobabilityatleast1−δ/T,∥θ(cid:101) tj∥
Vt
≤γ
(cid:101)T
holds.
Thefollowingfactdescribesananti-concentrationpropertyofGaussiandistribution,whichfollows
fromthefactthatalinearcombinationofindependentGaussiansisagainGaussian.
Fact1. IfZ ∼ N(0,α2I )forsomeα ≥ 0andu ∈ Rn isafixedvectorforsomen ∈ N, then
n
P(cid:0) u⊤Z ≥α∥u∥ (cid:1) ≥P(z ≥1)=:p ,wherez ∼N(0,1). Wenotethatp ≥0.15.
2 N N
AllthebuildingblocksweneedtoproveTheorem1isready. Theproofillustratesthattheevents
specifiedinLemma3occurwithhighprobability.
ProofofTheorem1. DefineE(cid:101)1,t andE(cid:101)2,t asinLemma3withγ
(cid:101)
=γ
(cid:101)T
andp=p N/4,whereγ
(cid:101)T
is
definedinEq.(7)andp isdefinedinFact1. Weshowthattheseeventsoccurwithhighprobability,
N
andtherestfollowsfromLemma3. Forthesakeoftheanalysis,assumethatδ/T ≤p /2≈0.08,
N
whichholdswheneverT ≥14orδ <0.07.
AssumethattheperturbationvaluesWj andZj areFA-measurableforallj ∈[m]andt∈[T]. An
t 0
interpretationofthisassumptionisthatthealgorithmsamplesalltherequiredvaluesinadvance. Note
thatwestillobtainanequivalentalgorithmandthismodificationneednotactuallytakeplaceinthe
execution. Underthisassumption,theuniformsamplingofj ∼J istheonlysourceofrandomness
t t
regardingthechoiceofθ andX whenconditionedonthehistoryF . Itmayseemunintuitive,
t t t−1
butthismodificationsimplifiestheproofbecauseweonlyneedtodealwithj .
t
Wefirstlower-boundtheprobabilityofE(cid:101)1,t. Whenj ∈ [m]isfixed,wecanapplyLemma4and
obtainP(∥θ(cid:101) tj −1∥
Vt−1
≤γ (cid:101)T)≥1−δ/T. Sincej tissampledindependentlyofθ(cid:101) tj −1,itholdsthat
m m
P(cid:16) E(cid:101)1,t(cid:17) =(cid:88) P(cid:16) j t =j,(cid:13) (cid:13)θ(cid:101) tj −1(cid:13) (cid:13) Vt−1 ≤γ (cid:101)T(cid:17) =(cid:88) m1 P(cid:16)(cid:13) (cid:13)θ(cid:101) tj −1(cid:13) (cid:13) Vt−1 ≤γ (cid:101)T(cid:17) ≥1− Tδ . (8)
j=1 j=1
Now, we bound the probability of E(cid:101)2,t. Fix j ∈ [m]. Recall that Zj
t
is the perturbation vector
of the j-th model, defined in Eq. (6). The choice of Gaussian perturbation implies that Zj ∼
t
N(0 ,β2I ). Suppose that the sequence of arms X ,...,X is fixed. Then, we can
d+t−1 T d+t−1 1 T
applyFact1,obtainingthatP(U⊤ Zj ≥β ∥U ∥ )≥p ,wheretheprobabilityismeasured
t−1 t t−1 t−1 2 N
overtherandomnessoftheperturbationsequenceZj. LetIj := 1{(U⊤ Zj ≥ β ∥U ∥ )∧
t t t−1 t t−1 t−1 2
((cid:13) (cid:13)θ(cid:101) tj −1(cid:13) (cid:13)
Vt−1
≤γ (cid:101)T)}. Then,wehavethat
P(cid:0) I tj =1(cid:1) ≥P(cid:0) U t⊤ −1Zj t ≥β t−1∥U t−1∥ 2(cid:1) −P(cid:16)(cid:13) (cid:13)θ(cid:101) tj −1(cid:13) (cid:13) Vt−1 >γ (cid:101)T(cid:17) ≥p N −δ/T ≥p N/2, (9)
wherethefirstinequalityusesthatP(A∩B)≥P(A)−P(BC)holdsforanyeventsAandB,andthe
lastinequalityholdsbytheassumptionδ/T ≤p /2. However,asweassumedthattheperturbation
N
sequenceisFA-measurable,itisF -measurable,henceIj isalsoF -measurable. Itmeansthat
0 t−1 t t−1
thevalueofIj isdeterminedwhenthehistoryuptotimet−1isfixed. Theonlyremainingsourceof
t
randomnessinchoosingθ conditionedonF isthesamplingofj ∼J . Therefore,itholdsthat
t t−1 t t
m
P(cid:16)(cid:16) U t⊤ −1Zj tt ≥β t−1∥U t−1∥ 2(cid:17) ∧(cid:16)(cid:13) (cid:13)θ(cid:101) tj −t 1(cid:13) (cid:13)
Vt−1
≤γ (cid:101)T(cid:17) |F t−1(cid:17) = m1 (cid:88) I tj. (10)
j=1
8Algorithm2LinearPerturbed-HistoryExploration(LinPHE)
Input: regularizationparameterλ>0,initialperturbationdistributionP onRd,
I
reward-perturbationdistributionP onR
R
InitializeV =λI
0 d
fort=1,2,...T do
i.i.d.
SampleW ∼P ,Z ,...,Z ∼ P
t I t,1 t,t−1 R
(cid:16) (cid:17)
Updateθ =V−1 W +(cid:80)t−1X (Y +Z )
t t−1 t i=1 i i t,i
PullarmX =argmax x⊤θ ,andobserveY
t x∈X t t
UpdateV =V +X X⊤
t t−1 t t
endfor
Since we have verified that the expectation of the right hand side is greater than p /2, Azuma-
N
Hoeffdinginequality(Lemma12)impliesthatP(1 (cid:80)m Ij <p /4)≤exp(cid:0) −p2 m/8(cid:1) . Recall
m j=1 t N N
thatthisresultisobtainedassumingthatX ,...,X arefixed. Wetaketheunionboundoverall
1 T
possiblesequencesofarms. However,anaïveunionboundmultipliesKT tothefailureprobability,
whichleadstoanundesirableresultofmscalinglinearlywithT. Wepresentthefollowingclaim
inspired by an observation from Lu and Van Roy [16] that a permutation of selected arms can
be regarded as equivalent. We note that the strong result of Lemma 6 in Lu and Van Roy [16]
isnotapplicabletooursettingsincewedonotassumethat{η }T isdistributedidenticallynor
t t=1
independently. AlthoughwepresentallthemainideastosupportClaim1inthissection,theremay
beafewpointsthatreadersfindrequirefurtherjustification. Duetolimitedspace, weprovidea
full,rigorousjustificationofClaim1inAppendixF,wherewepresentadifferentperspectiveonthe
samplingofperturbation. Wenotethatthisclaimisnotaconjecturenoranassumption.
Claim1. ThereexistsaneventE∗suchthatunderE∗, 1 (cid:80)m Ij ≥p /4holdsfort=1,...,T
2 2 m j=1 t N
andP(E∗C)≤TKexp(−p2 m/8).
2 N
ThekeyobservationinClaim1isthattheperturbationvectorconsistsofi.i.d. components,hence
itsdistributionisinvariantunderindependentpermutations. Therefore,thedistributionsofθ(cid:101)j and
t−1
U⊤ Zj remaininvariantunderthepermutationofselectedarms. Althoughthesequenceofarms
t−1 t
andtheperturbationvectorarenotindependentasawhole,thepermutationthatsortstheselected
armspreservesthedistributionofZ sinceX andZ areindependentforallt∈[T]. Thenumberof
t t t
equivalenceclassesuptopermutationoversequencesofarmswithlengthsatmostT −1islessthan
TK,sinceeacharmcanbeselected0toT −1timesinclusively. Therefore,wetaketheunionbound
overtheTK sequencesofarmsandattainE∗.
2
Takingm= p8
2
(KlogT +log1 δ),weobtainthatP(E 2∗C)≤δ. Eq.(10)impliesthatP(∩T t=1E(cid:101)2,t)≥
P(E∗) ≥ 1−δN . Therefore,byLemma3,withprobabilityatleast1−4δ,thecumulativeregretis
2
boundedasfollows:
(cid:18) 8 (cid:19)(cid:115) (cid:18) T (cid:19) 4γ (cid:114) 2T 1 (cid:16) √ (cid:17)
R(T)≤γ T 1+ p 2dT log 1+ dλ + p T λ log δ =O (dlogT)3 2 T .
N N
6 EnsembleSamplingandPerturbed-HistoryExploration
In this section, we rigorously investigate the relationship between linear ensemble sampling and
LinPHE. A generalized version of LinPHE is described in Algorithm 2. Note that the perturbed
estimator, θ =
V−1(cid:0)
W +
(cid:80)t−1X
(Y + Z
)(cid:1)
, resembles the estimator of linear ensemble
t t−1 t i=1 i i t,i
sampling, which becomes evident when compared with Eq. (5). The main difference is that in
LinPHE(Algorithm2),theperturbationsequenceisgeneratedindependentlyofthehistoryatevery
timestep,whereasinlinearensemblesampling(Algorithm1),thesequenceisnotrenewedbutis
incrementedateachtimestep. However,wefurtherobservethatinlinearensemblesampling,aslong
asanestimatorisnotsampledforthearmselection,itsperturbationsequenceisindependentofthe
9selectedarmsandrewards. Thisimpliesthattheestimatorintheensemblethatisselectedforthe
firsttimeisequivalenttotheestimatorcomputedbythepolicyofLinPHE.Specifically,ifthej-th
estimatorisselectedforthefirsttimeattimestept,thentheperturbationvaluesoftheestimator,
Wj and{Zj}t−1,havehadnoeffectonpreviousinteractions. Therefore,newlysamplingthemas
i i=1
Wj ∼P and{Zj }t−1 i. ∼i.d. P ,asinLinPHE,doesnotalterfutureinteractions. Weconcludethat
t I t,i i=1 R
inthecasewheretheensemblesizeisgreaterthanorequaltoT,linearensemblesamplingbecomes
equivalenttoLinPHEbyselectingtheestimatorsinaroundrobin.
Proposition1. Linearensemblesampling(Algorithm1)withm = T anddeterministicpolicyof
choosingamodel,e.g.,J ≡tfort=1,...,T,isequivalenttoLinPHE(Algorithm2).
t
Proposition1showsthatLinPHEisaspecialcaseoflinearensemblesamplingandprovidesinsightful
consequencesinbothdirectionsoftheequivalence. Toourbestknowledge,Proposition1isthefirst
resulttoformallydemonstratetherelationshipbetweenlinearensemblesamplingandLinPHE.
LinearensemblesamplingwithT modelsisLinPHE: SinceanensembleofT modelsisequivalent
√
toLinPHEwhichachievesaregretboundO(cid:101)(d T logK),theensemblesizelargerthanT isnot
necessary. Thisimplicationcertainlyemphasizesthesub-optimalrequirementsoftheensemblesize
in LuandVanRoy[16],Qinetal.[21]. EvenwhenK >T inourproblemsetting,thisequivalence
providesthegroundforupperboundingtheensemblesizebyT.
LinPHEislinearensemblesamplingwithT models: Conversely,sinceLinPHEcanberegarded
aslinearensemblesamplingwithT models,itispossibletoderivearegretboundofLinPHEby
following the proof of Theorem 1. We present Corollary 1, which states a new regret bound of
√
O(cid:101)(d3/2 T)forLinPHE.Notethatinthiscase,theregretboundisindependentofK.
Corollary1(RegretboundofLinPHE). Fixδ ∈(0,1].Algorithm2withλ≥1,P =N(0 ,λβ2I )
√ I d T d
andP =N(0,β2)achievesO((dlogT)3/2 T)cumulativeregretwithprobabilityatleast1−3δ.
R T
√
DiscussionofCorollary1. Kvetonetal.[12]provideaO(cid:101)(d T logK)regretboundwhenthe
√
number of arms is finite. Our result is the first to prove that LinPHE achieves a O(cid:101)(d3/2 T)
regretboundthatisindependentofthenumberofarms. Hence, weviewouroverallresultsasa
generalizationofKvetonetal.[12]. Itiswidelyobservedthatassumingthesizeofthearmsettobe
√ √
K mayleadtoaninterchangingofa dfactorwitha logK factorintheregretbound[4],although
attaining such reduction may not always be done in a trivial manner [5, 8, 6]. Our focus is not
merelyonprovinganotherregretboundforLinPHE,butratheronhighlightingthecloserelationship
betweenlinearensemblesamplingandLinPHE,which,toourknowledge,hasbeenoverlookedinthe
literature.
TheproofofCorollary1followstheproofofTheorem1. Notethatthelatterpartoftheproofof
Theorem1focusesondecouplingthedependencybetween{X }T andZ . However,inthecaseof
t t=1 t
LinPHE,theyarealreadyindependentsincetheperturbationsequenceisfreshlysampledatevery
timestep,enablingamoreelegantandconciseproof. Especially,asitskipsthepartsthatrequirethe
numberofarmstobefinite,forinstancetheuseofClaim1,Corollary1holdsevenwhenthenumber
ofarmsisinfinite. ThewholeproofispresentedinAppendixG.
7 Conclusion
√
WeprovethatlinearensemblesamplingachievesaO(cid:101)(d3/2 T)regretbound,markingthefirstsuch
resultinthefrequentistsettingandmatchingthebest-knownregretboundforrandomizedalgorithms.
TherequiredensemblesizescaleslogarithmicallywiththetimehorizonasΩ(KlogT). Additionally,
weexpandouranalysistoLinPHE,demonstratingthatitisaspecialcaseoflinearensemblesampling
√
withanensembleofT models,achievingthesameregretboundofO(cid:101)(d3/2 T). Whileourwork
focuses on linear bandits, ensemble sampling applications have shown superior performance in
more complex settings. This suggests that theoretical extensions beyond the linear setting are
worthpursuing,withourresultsprovidinganimportantfoundationforpossiblyunderstandingthese
extensions. Extendingtheresultstogeneralcontextualsettings,wherethearmsetmaychangeover
timewithpotentiallynon-linearrewardfunctions,representsapromisingdirectionforfuturework.
10Acknowledgements
ThisworkwassupportedbytheNationalResearchFoundationofKorea(NRF)grantfundedbythe
Koreagovernment(MSIT)(No.2022R1C1C1006859,2022R1A4A1030579,andRS-2023-00222663)
andbyAI-BioResearchGrantthroughSeoulNationalUniversity.
References
[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear
stochasticbandits. AdvancesinNeuralInformationProcessingSystems,24:2312–2320,2011.
[2] MarcAbeilleandAlessandroLazaric.LinearThompsonSamplingRevisited.InAartiSinghand
JerryZhu,editors,Proceedingsofthe20thInternationalConferenceonArtificialIntelligence
and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 176–184.
PMLR,PMLR,20–22Apr2017.
[3] ShipraAgrawalandNavinGoyal. Analysisofthompsonsamplingforthemulti-armedbandit
problem. InConferenceonlearningtheory, pages39–1.JMLRWorkshopandConference
Proceedings,2012.
[4] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear
payoffs. InInternationalconferenceonmachinelearning,pages127–135.PMLR,2013.
[5] PeterAuer.Usingconfidenceboundsforexploitation-explorationtrade-offs.JournalofMachine
LearningResearch,3(Nov):397–422,2002.
[6] SébastienBubeck,NicoloCesa-Bianchi,andShamMKakade. Towardsminimaxpoliciesfor
onlinelinearoptimizationwithbanditfeedback. InConferenceonLearningTheory, pages
41–1.JMLRWorkshopandConferenceProceedings,2012.
[7] OlivierChapelleandLihongLi. Anempiricalevaluationofthompsonsampling. Advancesin
neuralinformationprocessingsystems,24,2011.
[8] WeiChu,LihongLi,LevReyzin,andRobertSchapire. Contextualbanditswithlinearpayoff
functions. InProceedingsoftheFourteenthInternationalConferenceonArtificialIntelligence
andStatistics,pages208–214.JMLRWorkshopandConferenceProceedings,2011.
[9] NimaHamidiandMohsenBayati. Onfrequentistregretoflinearthompsonsampling. arXiv
preprintarXiv:2006.06790,2020.
[10] DavidJanz,AlexanderELitvak,andCsabaSzepesvári. Ensemblesamplingforlinearbandits:
smallensemblessuffice. arXivpreprintarXiv:2311.08376,2023.
[11] BranislavKveton,CsabaSzepesvari,MohammadGhavamzadeh,andCraigBoutilier.Perturbed-
historyexplorationinstochasticmulti-armedbandits.InInternationalJointConferenceonArtifi-
cialIntelligence,2019. URLhttps://api.semanticscholar.org/CorpusID:67856126.
[12] BranislavKveton,CsabaSzepesvári,MohammadGhavamzadeh,andCraigBoutilier.Perturbed-
historyexplorationinstochasticlinearbandits. InUncertaintyinArtificialIntelligence,pages
530–540.PMLR,2020.
[13] BranislavKveton,ManzilZaheer,CsabaSzepesvari,LihongLi,MohammadGhavamzadeh,
andCraigBoutilier. Randomizedexplorationingeneralizedlinearbandits. InInternational
ConferenceonArtificialIntelligenceandStatistics,pages2066–2076.PMLR,2020.
[14] TorLattimoreandCsabaSzepesvári. Banditalgorithms. CambridgeUniversityPress,2020.
[15] BeatriceLaurentandPascalMassart. Adaptiveestimationofaquadraticfunctionalbymodel
selection. Annalsofstatistics,pages1302–1338,2000.
[16] XiuyuanLuandBenjaminVanRoy. Ensemblesampling. AdvancesinNeuralInformation
ProcessingSystems,30,2017.
11[17] XiuyuanLu,ZhengWen,andBranislavKveton. Efficientonlinerecommendationvialow-rank
ensemblesampling. InProceedingsofthe12thACMConferenceonRecommenderSystems,
pages460–464,2018.
[18] IanOsband,CharlesBlundell,AlexanderPritzel,andBenjaminVanRoy. Deepexplorationvia
bootstrappeddqn. AdvancesinNeuralInformationProcessingSystems,29,2016.
[19] IanOsband,JohnAslanides,andAlbinCassirer. Randomizedpriorfunctionsfordeeprein-
forcementlearning. AdvancesinNeuralInformationProcessingSystems,31,2018.
[20] Ian Osband, Benjamin Van Roy, Daniel J Russo, Zheng Wen, et al. Deep exploration via
randomizedvaluefunctions. JournalofMachineLearningResearch,20(124):1–62,2019.
[21] ChaoQin,ZhengWen,XiuyuanLu,andBenjaminVanRoy. Ananalysisofensemblesampling.
AdvancesinNeuralInformationProcessingSystems,35:21602–21614,2022.
[22] DanielRussoandBenjaminVanRoy.Learningtooptimizeviaposteriorsampling.Mathematics
ofOperationsResearch,39(4):1221–1243,2014.
[23] DanielJRusso,BenjaminVanRoy,AbbasKazerouni,IanOsband,ZhengWen,etal. Atutorial
onthompsonsampling. FoundationsandTrends®inMachineLearning,11(1):1–96,2018.
[24] WilliamRThompson. Onthelikelihoodthatoneunknownprobabilityexceedsanotherinview
oftheevidenceoftwosamples. Biometrika,25(3/4):285–294,1933.
[25] Runzhe Wan, Haoyu Wei, Branislav Kveton, and Rui Song. Multiplier bootstrap-based ex-
ploration. In International Conference on Machine Learning, pages 35444–35490. PMLR,
2023.
[26] JieZhou,BotaoHao,ZhengWen,JingfeiZhang,andWillWeiSun. Stochasticlow-ranktensor
bandits for multi-dimensional online decision making. Journal of the American Statistical
Association,pages1–14,2024.
[27] Zheqing Zhu and Benjamin Van Roy. Deep exploration for recommendation systems. In
Proceedingsofthe17thACMConferenceonRecommenderSystems,pages963–970,2023.
12Appendix
A Notations
WesummarizesthenotationsinthispaperinTable2andTable3
Table2: Notationsspecifictothispaper
LinearBandit
X Setofarms
θ∗ Trueparametervector
x∗ Optimalarm
X Chosenarmattimet
t
Y Observedrewardattimet
t
η Zero-meannoiseattimet
t
σ SubGaussianvarianceproxyofη
t
d Dimensionofarmsandtrueparametervector
K Numberofarms(if|X|<∞)
T Timehorizon
R(T) Cumulativeregret
Algorithm
λ Regularizationparameter
V λI +(cid:80)t X X⊤
t d i=1 i i
θ Perturbedestimator
t
Wj /W Initialperturbationvector
t
Zj /Z Rewardperturbation
t t,i
P DistributionforinitialperturbationW
I
P DistributionforrewardperturbationZ
R t
J Samplingpolicyattimet
t
Analysis
δ Probabilityoffailure
FX σ({X }t )
t i i=1
Fη σ({η }t )
t i i=1
FA σ-algebrageneratedbyalgorithm’sinternalrandomnessuntiltimet
t
F σ(FA∪FX ∪Fη),σ-algebrageneratedbythefirstt-iterationsofinteraction
t t t t
θˆ Ridgeregressionestimatorbytsamples
t
θ(cid:101)t Perturbationintheestimator,θ t+1−θˆ
t
β Confidenceradiusofθˆ
t t
γ Confidenceradiusofθ
t
γ
(cid:101)
Confidenceradiusofθ(cid:101)t
p Probabilityofoptimisticarmselection
p P(Z ≥1)≈0.15whereZ ∼N(0,1)
N
E Highprobabilityevent
E Concentrationevent
1,t
E Optimismevent
2,t
Eˆ Concentrationeventofθˆ
t t
E(cid:101)1,t Concentrationeventofθ(cid:101)t−1
E(cid:101)2,t Anti-concentrationeventofθ(cid:101)t−1
Z Perturbationvectorattimet
t √
Φ (cid:0) λI X ··· X (cid:1) ∈Rd+t
t d 1 t
U x∗⊤V−1Φ
t t t
13Table3: Genericnotations
Setsandfunctions
N Setofnaturalnumbers,startingfrom1
[M] SetofnaturalnumbersuptoM,i.e.,{1,2,...,M}
1 Indicatorfunction
Vectorandmatrices
∥·∥ ℓ normofavector
2 2
(·) i-thelementofavector
i
0 ZerovectorinRd
d
I IdentitymatrixinRd×d
d
Probability
(Ω,F,P) Probabilityspace
E Expectation
N(µ,Σ) GaussiandistributionwithmeanµandcovarianceΣ
∧ Logicalconjunction(“and”)
∨ Logicaldisjunction(“or”)
B ProofofTheorem2
Lemma5(Lemmas10and11inAbbasi-Yadkorietal.[1]). Letλ≥1,{X }T beanysequence
t t=1
ofd-dimensionalvectorssuchthat∥X ∥ ≤1forallt∈[T],andV =λI +(cid:80)t X X⊤. Then,
t 2 t i=1 i i
(cid:80)T ∥X ∥2 ≤2dlog(cid:0) 1+ T (cid:1) .
t=1 t V−1 dλ
t−1
AsalludedinthediscussionofTheorem2,weutilizeMarkov’sinequalityandtherandomvariableof
interestisX⊤θ 1{E ∩E }. However,thisrandomvariablemaynotbenon-negative,precluding
t t 1,t 2,t
the use of Markov’s inequality. The following lemma shows that adding an appropriate term
guaranteesitsnon-negativity.
Lemma 6. Assume the conditions of Theorem 2. Let J(θ) = sup x⊤θ, where θ ∈ Rd. Let
x∈X
Θ ={θ ∈Rd |∥θ−θ∗∥ ≤γ}. Defineθ− =argmin J(θ)andX− =argmax x⊤θ−.
t Vt−1 t θ∈Θt t x∈X t
Foranyθ ∈RdandaneventE′,weintroducethefollowingnotation:
g
(θ,E′)=(cid:0) J(θ)−J(θ−)(cid:1)1{E′}.
t t
Then,g (θ∗,E′) ≥ 0holdsforanyeventE′ ∈ F,andg (θ ,E′′) ≥ 0holdsalmostsurelyforany
t t t
eventsuchthatE′′ ⊂E .
1,t
Proof. Wefirstproveg (θ∗,E′)≥0. Sinceθ∗ ∈Θ ,
t t
J(θ∗)≥ inf J(θ)=J(θ−)
t
θ∈Θt
alwaysholds. Therefore,foranyeventE′,g (θ∗,E′)≥0holds.
t
WenowsupposeE′′ ⊂ E andproveg (θ ,E′′) ≥ 0. WeconsidertwocaseswhereE′′ doesand
1,t t t
doesnothold. UnderE′′,sinceE′′ ⊂E andbyconcentrationcondition(2),θ ∈Θ holdsalmost
1,t t t
surely. Then,J(θ )≥inf J(θ)=J(θ−)holds. UnderE′′C,g (θ ,E′′)=0≥0triviallyholds.
t θ∈Θt t t t
Therefore,g (θ ,E′′)≥0holdsalmostsurely.
t t
ProofofTheorem2. Weshowthatwithprobabilityatleast1−δ,itholdsthat
(cid:18) (cid:19)(cid:115) (cid:18) (cid:19) (cid:114)
2 T γ 2T 1
R(T)1{E}≤γ 1+ 2dT log 1+ + log .
p dλ p λ δ
WefirstboundtheinstantaneousregretundertheeventE attimet.
(cid:0) x∗⊤θ∗−X⊤θ∗(cid:1)1{E}=(cid:0) x∗⊤θ∗−X⊤θ +X⊤θ −X⊤θ∗(cid:1)1{E}
t t t t t t
=(cid:0) x∗⊤θ∗−X⊤θ (cid:1)1{E}+(cid:0) X⊤θ −X⊤θ∗(cid:1)1{E}
t t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I1 I2
14I isdirectlyboundedunderE.
2
I =X⊤(θ −θ∗)1{E}
2 t t
≤∥X ∥ ∥θ −θ∗∥ 1{E}
t V t− −1
1
t Vt−1
≤γ∥X ∥ ,
t V−1
t−1
wherethefirstinequalityisduetotheCauchy-Schwarzinequalityandthesecondinequalitycomes
fromcondition(2).
Now,weboundI . LetX−,θ−,andg bedefinedasinLemma6. ByLemma6,g (θ ,E)≥0holds
1 t t t t t
almostsurely. Then,
I =x∗⊤θ∗1{E}−X⊤θ 1{E}
1 t t
=x∗⊤θ∗1{E}−X−⊤θ−1{E}+X−⊤θ−1{E}−X⊤θ 1{E}
t t t t t t
=g (θ∗,E)−g (θ ,E)
t t t
≤g (θ∗,E)
t
≤g (θ∗,E ) ,
t 2,t
wherethelastinequalityholdssinceE ⊂E . AgainbyLemma6,g (θ∗,E )andg (θ ,E ∩E )
2,t t 2,t t t 1,t 2,t
arenon-negativealmostsurely. Notethatbythefirstpartofcondition(3),E isF -measurable
2,t t−1
andhenceg (θ∗,E )=(x∗⊤θ∗−X−⊤θ−)1{E }isalsoF -measurable. ApplyingMarkov’s
t 2,t t t 2,t t−1
inequalityconditionedonF ,weobtainthat
t−1
(cid:16) (cid:17) (cid:104) (cid:105)
g (θ∗,E )P g (θ∗,E )≤g (θ ,E ∩E )|F ≤E g (θ ,E ∩E )|F . (11)
t 2,t t 2,t t t 1,t 2,t t−1 t t 1,t 2,t t−1
Welower-boundtheprobabilityonthelefthandsideutilizingcondition(3). Supposetheeventof
interestincondition(3),namely((x∗⊤θ∗ ≤ X⊤θ )∧E )∨EC ,holds. Undertheevent,either
t t 1,t 2,t
EC or(x∗⊤θ∗ ≤X⊤θ )∧E ∧E holds. UnderEC ,g (θ∗,E )≤g (θ ,E ∩E )becomes
2,t t t 1,t 2,t 2,t t 2,t t t 1,t 2,t
0≤0,whichtriviallyholds. Otherwise,wehave(x∗⊤θ∗ ≤X⊤θ )∧E ∧E . Underthisevent,
t t 1,t 2,t
wehavethat
x∗⊤θ∗ ≤X⊤θ ⇔x∗⊤θ∗−X−⊤θ− ≤X⊤θ −X−⊤θ−
t t t t t t t t
⇔(cid:0) x∗⊤θ∗−X−⊤θ−(cid:1)1{E }≤(cid:0) X⊤θ −X−⊤θ−(cid:1)1{E ∩E }
t t 2,t t t t t 1,t 2,t
⇔g (θ∗,E )≤g (θ ,E ∩E ).
t 2,t t t 1,t 2,t
Therefore,wehaveshownthat
(cid:0)(cid:0) x∗⊤θ∗ ≤X⊤θ (cid:1) ∧E (cid:1) ∨EC ⇒g (θ∗,E )≤g (θ ,E ∩E ),
t t 1,t 2,t t 2,t t t 1,t 2,t
whichimplies
P(g (θ∗,E )≤g (θ ,E ∩E )|F )≥P(cid:0)(cid:0) x∗⊤θ∗ ≤X⊤θ ∧E (cid:1) ∨EC |F (cid:1)
t 2,t t t 1,t 2,t t−1 t t 1,t 2,t t−1
≥p
by condition (3). Therefore, we obtain that g (θ∗,E )≤ 1E[g (θ ,E ∩E )|F ] from in-
t 2,t p t t 1,t 2,t t−1
equality(11). Lastly,weboundg (θ ,E ∩E ).
t t 1,t 2,t
g (θ ,E ∩E )=(cid:0) X⊤θ −X−⊤θ−(cid:1)1{E ∩E }
t t 1,t 2,t t t t t 1,t 2,t
≤(cid:0) X⊤θ −X⊤θ−(cid:1)1{E ∩E }
t t t t 1,t 2,t
≤∥X t∥ V t− −1 1(cid:13) (cid:13)θ t−θ t−(cid:13) (cid:13) Vt−11{E 1,t}
≤∥X t∥ V t− −1 1(cid:16) ∥θ t−θ∗∥ Vt−1 +(cid:13) (cid:13)θ t−−θ∗(cid:13) (cid:13) Vt−1(cid:17) 1{E 1,t}
≤2γ∥X ∥ ,
t V−1
t−1
wherethefirstinequalityusesthatX− =sup x⊤θ−,thesecondinequalityisduetotheCauchy-
t x∈X t
Schwarzinequality,thethirdinequalityholdsbythetriangleinequality,andthelastinequalitycomes
15fromcondition(2)andthatθ− ∈Θ asdefinedinLemma6. Combiningall,theinstantaneousregret
t t
attimetundertheeventE isboundedasfollows:
(cid:0) x∗⊤θ∗−X⊤θ∗(cid:1)1{E}≤γ∥X
∥ +
2γ E(cid:104)
∥X ∥ |F
(cid:105)
t t V t− −1 1 p t V t− −1 1 t−1
(cid:18) 2γ(cid:19) 2γ (cid:16) (cid:104) (cid:105) (cid:17)
= γ+ ∥X ∥ + E ∥X ∥ |F −∥X ∥ .
p t V t− −1 1 p t V t− −1 1 t−1 t V t− −1 1
Then,thecumulativeregretunderE isboundedas
(cid:18) 2(cid:19) (cid:88)T 2γ (cid:88)T (cid:16) (cid:104) (cid:105) (cid:17)
R(T)1{E}≤γ 1+ ∥X ∥ + E ∥X ∥ |F −∥X ∥ .
p t V t− −1 1 p t V t− −1 1 t−1 t V t− −1 1
t=1 t=1
Toboundthefirstsum,weapplytheCauchy-SchwarzinequalityandthenLemma5.
(cid:118)
T (cid:117) T
(cid:88) ∥X ∥ ≤(cid:117) (cid:116)T (cid:88) ∥X ∥2
t V−1 t V−1
t−1 t−1
t=1 t=1
(cid:115)
(cid:18) (cid:19)
T
≤ 2dT log 1+ .
dλ
The second sum is bounded by Azuma-Hoeffding inequality. Note that 0 ≤ ∥X ∥ ≤
t V−1
(cid:113) t−1
λ max(V t− −1 1)∥X t∥
2
≤ √1 λ, where λ max(·) denotes the maximum eigenvalue. By Lemma 12,
withprobabilityatleast1−δ,itholdsthat
T (cid:114)
(cid:88)(cid:16) (cid:104) (cid:105) (cid:17) T 1
E ∥X ∥ |F −∥X ∥ ≤ log .
t V t− −1 1 t−1 t V t− −1 1 2λ δ
t=1
Therefore,withprobabilityatleast1−δ,thecumulativeregretunderE isboundedasfollows:
(cid:18) (cid:19)(cid:115) (cid:18) (cid:19) (cid:114)
2 T γ 2T 1
R(T)1{E}≤γ 1+ 2dT log 1+ + log .
p dλ p λ δ
C ProofofLemma2
ProofofLemma2. The proof is simple algebra utilizing a useful matrix Φ . Define Φ to be the
t t
matrixthatstacksX inadditiontoanidentitymatrixasfollows:
i
√
Φ :=(cid:0) λI X ··· X (cid:1) ∈Rd×(d+t).
t d 1 t
WecanexpressarelevantmatrixandvectorsusingΦ t,whichareV
t
= Φ tΦ⊤
t
,θ(cid:101) tj = V t−1Φ tZj t+1,
andU t⊤ =x∗⊤V t−1Φ t. Definingθ(cid:101)t−1 =θ(cid:101) tj −t 1tobetheperturbationintheselectedestimatorattime
t,weobtainthat
x∗⊤θ(cid:101)t−1 =x∗⊤V t− −1 1Φ t−1Z
t
=U⊤ Z .
t−1 t
Inaddition,itholdsthat
(cid:113)
∥U ∥ = x∗⊤V−1Φ Φ⊤ V−1x∗
t−1 2 t−1 t−1 t−1 t−1
(cid:113)
= x∗⊤V−1x∗
t−1
=∥x∗∥ .
V−1
t−1
16ByX⊤θ =sup x⊤θ ,itholdsthatx∗⊤θ ≤X⊤θ . Then,wehavethat
t t x∈X t t t t
X⊤θ −x∗⊤θ∗ ≥x∗⊤θ −x∗⊤θ∗
t t t
=x∗⊤(θ −θ∗)
t
(cid:16) (cid:17)
=x∗⊤ θ(cid:101)t−1+θˆ t−1−θ∗
(cid:16) (cid:17)
=U Z +x∗⊤ θˆ −θ∗ .
t−1 t t−1
Bytheconditionofthelemma,thereexistsapositiveconstantcsuchthat∥θˆ −θ∗∥ ≤cand
t−1 Vt−1
U⊤ Z −c∥U ∥ ≥0. BytheCauchy-Schwarzinequality,itholdsthat
t−1 t t−1 2
x∗⊤(cid:16) θˆ t−1−θ∗(cid:17) ≥−∥x∗∥
V t− −1
1(cid:13) (cid:13)θˆ t−1−θ∗(cid:13)
(cid:13) Vt−1
≥−c∥U ∥ .
t−1 2
Therefore,
X⊤θ −x∗⊤θ∗ ≥U⊤ Z −c∥U ∥
t t t−1 t t−1 2
≥0,
whereweprovedthatX⊤θ ≥x∗⊤θ∗.
t t
D ProofofLemma3
ProofofLemma3. UnderE
1,t
=E(cid:101)1,t∩Eˆ t−1,theconcentrationcondition,specificallycondition(2)
inTheorem2,holdsbythetriangleinequality.
∥θ t−θ∗∥ Vt−1 =(cid:13) (cid:13)θ(cid:101)t−1+θˆ t−1−θ∗(cid:13) (cid:13) Vt−1 ≤(cid:13) (cid:13)θ(cid:101)t−1(cid:13) (cid:13) Vt−1 +(cid:13) (cid:13)θˆ t−1−θ∗(cid:13) (cid:13) Vt−1 ≤γ (cid:101)+β t−1 ≤γ.
To show the optimism condition, condition (3) in Theorem 2, we first show that E ∈ F .
2,t t−1
Eˆ
t−1
∈F
t−1
holdssinceitregards{X i,η i}t i=− 11 only. SinceP((U t⊤ −1Z
t
≥ β t−1∥U t−1∥ 2)∧E(cid:101)1,t |
F t−1)isaF t−1-measurablerandomvariableandE(cid:101)2,tisaneventthatthespecifiedrandomvariable
isgreaterthanorequaltop,E(cid:101)2,t isinF t−1. Therefore,weobtainthatE
2,t
= Eˆ t−1∩E(cid:101)2,t ∈ F t−1.
To prove the remaining part of condition (3), we demonstrate the following logical implication
relationships:
(cid:16)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t(cid:17) ∨E 2C
,t
⇒(cid:16)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t∧E 2,t(cid:17) ∨E 2C
,t
⇒(cid:16)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t∧Eˆ t−1(cid:17) ∨E 2C
,t
⇒(cid:16)(cid:0) x∗⊤θ∗ ≤X t⊤θ t(cid:1) ∧E(cid:101)1,t∧Eˆ t−1(cid:17) ∨E 2C
,t
⇒(cid:0)(cid:0) x∗⊤θ∗ ≤X⊤θ (cid:1) ∧E (cid:1) ∨EC ,
t t 1,t 2,t
wherethefirstimplicationfollowsfromA∨BC ⇔(A∧B)∨BC,thesecondimplicationholdssince
E ⊂Eˆ ,thethirdimplicationholdsbyLemma2,whichstatesthat(U⊤ Z ≥β ∥U ∥ ∧
2,t t−1 t−1 t t−1 t−1 2
Eˆ t−1)⇒(x∗⊤θ∗ ≤X t⊤θ t),andthelastbyE
1,t
=E(cid:101)1,t∩Eˆ t−1. Thisimplicationrelationshipshows
that
P(cid:0)(cid:0)(cid:0) x∗⊤θ∗ ≤X t⊤θ t(cid:1) ∧E 1,t(cid:1) ∨E 2C
,t
|F t−1(cid:1) ≥P(cid:16)(cid:16)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t(cid:17) ∨E 2C
,t
|F t−1(cid:17) .
17WeboundtherighthandsideusingthedefinitionofE .
2,t
P(cid:16)(cid:16)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t(cid:17) ∨E 2C
,t
|F t−1(cid:17)
=E(cid:104) 1(cid:110)(cid:16)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t(cid:17) ∨E 2C ,t(cid:111) |F t−1(cid:105)
=E(cid:104) 1(cid:110)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t(cid:111) 1{E 2,t}+1(cid:8) E 2C ,t(cid:9) |F t−1(cid:105)
=E(cid:104) 1(cid:110)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t(cid:111) |F t−1(cid:105) 1{E 2,t}+1(cid:8) E 2C ,t(cid:9)
≥p1{E }+1(cid:8) EC (cid:9)
2,t 2,t
≥p,
wherethethirdequalityusesthatE
2,t
∈F t−1andthefirstinequalityholdssinceunderE
2,t
⊂E(cid:101)2,t,
E(cid:104) 1(cid:110)(cid:0) U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2(cid:1) ∧E(cid:101)1,t(cid:111) |F t−1(cid:105) ≥pholdsbythedefinitionofE(cid:101)2,t.
Therefore, E and E satisfy conditions (2) and (3). By Theorem 2, the regret bound stated in
1,t 2,t
inequality(4)holdswithprobabilityatleast1−δ−P(EC),wheretheunionboundistaken. Notethat
(cid:16) (cid:17)
E =∩T t=1(E 1,t∩E 2,t)=∩T
t=1
E(cid:101)1,t∩E(cid:101)2,t∩Eˆ
t−1
⊃Eˆ∩E(cid:101). Thefailureprobabilityisboundedas
δ+P(cid:0) EC(cid:1) ≤δ+P(cid:16) EˆC(cid:17) +P(cid:16) E(cid:101)C(cid:17)
(cid:16) (cid:17)
≤2δ+P E(cid:101)C ,
wherethefirstinequalitytakestheunionboundoverEC ⊂EˆC∪E(cid:101)Candthesecondinequalityisdue
toLemma1.
E ProofofLemma4
Lemma4isaspecialcaseofLemma9,whichgeneralizesGaussiandistributiontoanysubGaussian
distribution. Wefirstprovideageneralchi-squaredconcentrationresult,whichisrequiredtobound
theperturbationinducedbyW. AgeneralizedversionofLemma7ispresentedinLemma10.
Lemma7. IfZ ∼N(0,I )isad-dimensionalmultivariateGaussianvector,thenforanyδ ∈(0,1],
d
(cid:32) √ (cid:114) 1(cid:33)
P ∥Z∥ ≥ d+ 2log ≤δ.
2 δ
Proof. ByLemma13withx=log1,itholdsthat
δ
(cid:32) (cid:114) (cid:33)
1 1
P ∥Z∥2−d≥2 dlog +2log ≤δ.
2 δ δ
(cid:113) (cid:16)√ (cid:113) (cid:17)2
Sinced+2 dlog1 +2log1 ≤ d+ 2log1 ,itholdsthat
δ δ δ
(cid:32) √ (cid:114) 1(cid:33) (cid:32) (cid:114) 1 1(cid:33)
P ∥Z∥ ≥ d+ 2log ≤P ∥Z∥2 ≥d+2 dlog +2log
2 δ 2 δ δ
≤δ.
ProofofLemma4. ByLemma7,withδ/2T insteadofδ,yields
P(cid:32) (cid:13) (cid:13)Wj(cid:13)
(cid:13) 2
≥√
λβ T
(cid:32) √ d+(cid:114) 2log2 δT(cid:33)(cid:33)
≤
2δ
T ,
18sinceWj ∼N(0 ,λβ2I ). ApplyingLemma9withδ/2T insteadofδyieldsthat
d T d
(cid:13) (cid:13)θ(cid:101) tj −1(cid:13) (cid:13) Vt−1 ≤β T(cid:115) dlog(cid:18) 1+ dT λ(cid:19) +2log2 δT +β T (cid:32) √ d+(cid:114) 2log2 δT(cid:33)
holdswithprobabilityatleast1−δ/T. Notethattherighthandsideisequaltothedefinitionofγ ,
(cid:101)T
definedinEq.(7).
F RigorousJustificationofClaim1
Inthissection,werigorouslyjustifyClaim1thatisstatedintheproofofTheorem1. Todoso,we
presentadifferentviewpointontheperturbationsequences.
DenotethearmsasX ={x1,x2,...,xK}. Forthesakeoftheanalysis,assumethatδ/T ≤p /2≈
N
0.08,whichholdswheneverT ≥14orδ <0.07.
WereconstructtheperturbationsampledbyAlgorithm1. Assumethatinadditionto{Wj}m i. ∼i.d.
j=1
P ,Algorithm1samplesmKT samplesof{{Zj } }m i. ∼i.d. P atthebeginning,wherethe
I k,t (k,t) j=1 R
subscript (k,t) enumerates from (1,1) to (K,T). Define N =
(cid:80)t 1(cid:8)
X
=xk(cid:9)
to be the
k,t i=1 i
number of times arm k has been chosen up to time t. If the a t-th arm, xat, is selected at time t,
thenweassignZj =Zj . SinceZj isstillani.i.d. sampleofP conditionedonhistory,
t at,Nat,t at,Nat,t R
specificallyonσ(FX∪Fη∪σ({{Zj}t−1}m )),weattainanequivalentalgorithmwithAlgorithm1.
t t i i=1 j=1
Wenotethatthesemodificationsneednotbetakenintheexecutionofthealgorithm,andtheirpurpose
ispurelyfortheanalysis. DefineFA =σ({Wj,{Zj } }m ),whichreflectsthefactthatthey
0 k,t (k,t) j=1
aresampledinadvance. Fort∈[T],defineFA =σ(F ∪σ(j )),whichindicatesthattheonly
t t−1 t
additionalrandomnessofthealgorithmwhenchoosingθ isthesamplingofj ∼ J . Definethe
t t t
extendedperturbationvectorasfollows:
(cid:16) (cid:17)⊤
Zj = √1 Wj⊤ Zj ... Zj Zj ... Zj ∈Rd+KT
KT λ 1,1 1,T 2,1 K,T
RemovingsomecomponentsandreorderingZj yieldsZj,wheretheremovalandreorderingdepend
KT t
onthesequenceofchosenarms, namelya ,...,a . NotethatZj ∼ N(0 ,β2I ).
1 t−1 KT d+KT T d+KT
WealsodefinethecorrespondingextensionsofΦ andU . Defineamatrixthathasncolumns,firsta
t t
ofwhicharecopiesofv ∈Rdandtherestare0 asfollows.
d
rep(v,a,n):=(v ... v 0 ... 0 )∈Rd×n.
d d
DefinetheextendedversionofΦ asfollows:
t
√
Φ =(cid:0) λI rep(x1,N ,T) ... rep(xK,N ,T)(cid:1) ∈Rd×(d+KT).
t d 1,t K,t
Φ extendsΦ bypermutingthecolumnssothatthefeaturevectorsfromthesamearmappearin
t t
consecutive columns, then inserting multiple 0 appropriately. Then, we have V = Φ Φ⊤ and
d t t t
θ(cid:101)t = V t−1Φ tZj KT,sinceZj
KT
ispermutedandextendedfromZj
t
inasimilarmanner. Wedefine
theextendedversionofU asU =(x∗⊤V−1Φ )⊤ ∈Rd+KT. U isalsoapermutationofU with
t t t t t t
additionalzerosinserted. ItholdsthatU⊤Zj =U⊤Zj and∥U ∥ =∥U ∥ .
t t t KT t 2 t 2
LetX bethesetofallpossibleΦ . SinceΦ isfullydeterminedbyN ,...N andeachN
t t t 1,t K,t k,t
takesvaluebetween0andT−1inclusivelywhen0≤t≤T−1,weobtainthat(cid:12) (cid:12)∪T t=− 01X t(cid:12) (cid:12)≤TK.For
anyt∈[T],takeanyΦ ∈X . NotethatU =x∗⊤(Φ Φ⊤ )−1Φ isfullydetermined
t−1 t−1 t−1 t−1 t−1 t−1
byΦ t−1,andθ(cid:101) tj
−1
= (Φ t−1Φ⊤ t−1)−1Φ t−1Zj
KT
isdeterminedbyΦ
t−1
andZj KT. Assumingthat
Φ is fixed, U is also fixed, therefore we can apply Fact 1 and obtain that P(cid:0) U⊤ Zj ≥
t−1 t−1 t−1 KT
β ∥U ∥ (cid:1) ≥p ,wheretheonlysourceofrandomnesscomesfromZj . ApplyingLemma4,
T t−1 2 N KT
weobtainthatP(∥θ(cid:101) tj −1∥
Vt−1
≤γ (cid:101)T)≥1−δ/T. LetIj(Φ t−1)=1{(U⊤ t−1Zj
KT
≥β T∥U t−1∥ 2)∧
(∥θ(cid:101) tj −1∥
Vt−1
≤ γ (cid:101)T)}. Then,P(Ij(Φ t−1) = 1) ≥ p
N
−δ/T ≥ p N/2. Sincetheonlyrandomness
onchoosingthearmconditionedonF comesfromsamplingj ,itholdsthat
t−1 t
m
P(cid:16)(cid:16) U⊤ t−1Zj Kt T ≥β T ∥U t−1∥ 2(cid:17) ∧(cid:16)(cid:13) (cid:13)θ(cid:101) tj −t 1(cid:13) (cid:13) Vt−1 ≤γ (cid:101)T(cid:17) |F t−1(cid:17) = m1 (cid:88) Ij(Φ t−1) .
j=1
19We apply Azuma-Hoeffding inequality to show that 1 (cid:80)m Ij(Φ ) is bounded below with
m j=1 t−1
high probability. Since {Ij(Φ )}m are i.i.d. Bernoulli random variables with the associated
t−1 j=1
probabilitygreaterthanp /2,itholdsthat
N
 
m
P  m1 (cid:88) Ij(Φ t−1)≤ p 4N 
j=1
 
m m m
=P  m1 (cid:88) Ij(Φ t−1)− m1 (cid:88) E[Ij(Φ t−1)]≤ p 4N − m1 (cid:88) E[Ij(Φ t−1)]
j=1 j=1 j=1
 
m m
≤P  m1 (cid:88) Ij(Φ t−1)− m1 (cid:88) E[Ij(Φ t−1)]≤ p 4N − p 2N 
j=1 j=1
 
m m
=P  m1 (cid:88) Ij(Φ t−1)− m1 (cid:88) E[Ij(Φ t−1)]≤−p 4N 
j=1 j=1
(cid:18) p2 m(cid:19)
≤exp − N ,
8
whereLemma12isappliedattheend. Bytakingtheunionboundover∪T−1X ,weobtainthat
t=0 t
 
P ∃Φ∈∪T t=− 01X t,
m1 (cid:88)m
Ij(Φ)≤
p
4N
≤TKexp(cid:18) −p2
N
8m(cid:19)
.
j=1
TheeventE∗isdefinedasthecomplementoftheeventabove. Theproofiscomplete.
2
 
E∗
:=
ω ∈Ω:∀Φ∈∪T−1X ,
1 (cid:88)m
Ij(Φ)>
p N
.
2 t=0 t m 4
 
j=1
G ProofofCorollary1
ProofofCorollary1. LetE(cid:101)1,t andE(cid:101)2,t bedefinedasinLemma3withγ (cid:101)=γ
(cid:101)T
andp=p N/2. We
redefineacoupleofnotationstoadaptAlgorithm2. Let
(cid:16) (cid:17)⊤
Z t := √1 λW t⊤ Z t,1 ... Z t,t−1 ∈Rd+t−1
tobetheperturbationvectorattimet,and
(cid:32) t−1 (cid:33)
(cid:88)
θ(cid:101)t−1 :=V t− −1
1
W t+ X iZ
t,i
i=1
betheperturbationintheestimatorθ t. Regardingθ(cid:101)t−1asoneofθ(cid:101) tj −1intheproofofTheorem1,we
obtainthatP(E(cid:101)1,t)≥1−δ/T andP((U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2)∧E(cid:101)1,t)≥p N/2hold,analogouslyto
inequalities(8)and(9)respectively. Moreover,incontrasttotheproofofTheorem1,theperturbation
vectorZ isnowindependentofF . NotingthatU isF -measurable,italwaysholdsthat
t t−1 t−1 t−1
(cid:16) (cid:17) p
P (U t⊤ −1Z
t
≥β t−1∥U t−1∥ 2)∧E(cid:101)1,t |F
t−1
≥ 2N .
ThisprovesthatE(cid:101)2,t isinfactthewholeevent. Takingtheunionbound,weobtainthatP(E(cid:101)C) ≤
(cid:80)T P(E(cid:101)C )≤δ. ByLemma3,withprobabilityatleast1−3δ,thecumulativeregretisbounded
t=1 1,t
by
(cid:18) 4 (cid:19)(cid:115) (cid:18) T (cid:19) 2γ (cid:114) 2T 1 (cid:16) √ (cid:17)
R(T)≤γ T 1+ p 2dT log 1+ dλ + p T λ log δ =O (dlogT)3 2 T .
N N
20H GeneralizabilityofPerturbationDistributions
Inthissection,wedemonstratethatanydistributionthatissymmetric,subGaussian,andhaslower-
boundedvariancesatisfiestheresultsofLemma4andFact1,possiblyuptoaconstantfactor. As
mentionedinRemark3,itimpliesthatourresultsarevalidwhentheGaussiandistributionisreplaced
withanysymmetricnon-degeneratesubGaussiandistribution. Thefollowinglemmaisastandard
concentrationresultforvectormartingaleswithsubGaussiannoises.
Lemma8(Theorem1inAbbasi-Yadkorietal.[1]). Let{F }∞ beafiltration. Let{ξ }∞ bea
t t=0 t t=1
sequenceofreal-valuedrandomvariablessuchthatξ isF -measurableandisF -conditionallyσ-
t t t−1
subGaussianforsomeσ ≥0.Let{X }∞ beasequenceofRd-valuedrandomvectorssuchthatX is
t t=1 t
F -measurableand∥X ∥ ≤1almostsurelyforallt≥1.Fixλ≥1.LetV =λI+(cid:80)t X X⊤.
t−1 t 2 t i=1 t t
Then,foranyδ ∈(0,1],withprobabilityatleast1−δ,thefollowinginequalityholdsforallt≥0:
(cid:13) (cid:13) (cid:115)
(cid:13)(cid:88)t (cid:13) (cid:18) t (cid:19) 1
(cid:13) ξ X (cid:13) ≤σ dlog 1+ +2log .
(cid:13) i i(cid:13) dλ δ
(cid:13) (cid:13)
i=1 V−1
t
NextlemmaisasimpleapplicationofLemma8,whichprovestheconcentrationresultofθ(cid:101)tunder
thesubGaussianityofP .
R
Lemma 9 (Sufficient condition for concentration). Fix any δ ∈ (0,1] and t ∈ [T]. Assume
that P(cid:0) ∥W∥ >Lδ (cid:1) ≤ δ, and {Z }t−1 are mutually independent of each other and are FX-
2 t,0 i i=1 i
conditionallyσ2-subGaussianrespectivelyforeachi∈[t−1]. Then,withprobabilityatleast1−2δ,
R
itholdsthat
(cid:115)
(cid:13) (cid:13)θ(cid:101)t−1(cid:13) (cid:13) Vt−1 ≤σ R dlog(cid:18) 1+ dT λ(cid:19) +2log1 δ + L √δ t λ,0 .
Proof. Recallthatθ(cid:101)t−1 =V t− −1 1(W +(cid:80) it =− 11X iZ i). Therefore,
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)θ(cid:101)t−1(cid:13)
(cid:13)
=(cid:13) (cid:13)V t−1θ(cid:101)t−1(cid:13)
(cid:13)
Vt−1 V t− −1
1
(cid:13) (cid:88)t−1 (cid:13)
=(cid:13)W + X Z (cid:13)
(cid:13) i t,i(cid:13)
V−1
i=1 t−1
(cid:13)(cid:88)t−1 (cid:13)
≤∥W∥ +(cid:13) X Z (cid:13) ,
V t− −1 1 (cid:13) i i(cid:13) V−1
i=1 t−1
wherethetriangleinequalityisusedforthelastinequality. Toboundthefirstterm,weusethefact
thatλ (V−1)≤ 1,whereλ (·)denotesthemaximumeigenvalue. Itimpliesthat∥W∥ ≤
max t−1 λ max √ V t− −1 1
√1 λ∥W∥ 2. SinceP(∥W∥
2
>Lδ t,0)≤δbyassumption,∥W∥
V t− −1
1
≤Lδ t,0/ λholdswithprobability
atleast1−δ. ThesecondtermisboundedbyLemma8. Withprobability1−δ,itholdsthat
(cid:13) (cid:13) (cid:115)
(cid:13)(cid:88)t−1 (cid:13) (cid:18) t (cid:19) 1
(cid:13) X Z (cid:13) ≤σ dlog 1+ +2log .
(cid:13) i t,i(cid:13) R dλ δ
(cid:13) (cid:13)
i=1 V−1
t−1
Bytakingtheunionboundoverthetwoevents,weobtainthat
(cid:115)
(cid:13) (cid:13) Lδ (cid:18) t (cid:19) 1
(cid:13) (cid:13)θ(cid:101)t−1(cid:13)
(cid:13) Vt−1
≤ √t λ,0 +σ
R
dlog 1+
dλ
+2log
δ
holdswithprobabilityatleast1−2δ,whichprovesthethelemma.
We also provide that the ℓ -norm of the vector W whose components are i.i.d. samples of a
2
subGaussian distribution is upper-bounded with high-probability. This lemma, combined with
Lemma9,justifiesP tobeadistributionoverRdsuchthateachcomponentisani.i.d. sampleofa
I
subGaussiandistribution.
21Lemma 10. Suppose that W ∈ Rd and each component of W is sampled i.i.d. from a σ2-
I
subGaussiandistribution. Takeanyδ ∈(0,1]. Then,withprobability1−δ,itholdsthat
(cid:114)
1
∥W∥ ≤σ 2d+4log .
2 I δ
Proof. TakeX =e ,...,X =e ,where{e ,...,e }isthestandardbasisofRd. ByLemma8
1 1 d d 1 d
withξ =(W) andλ=1,itholdsthat
i i
(cid:13) (cid:13)(cid:88)d (cid:13) (cid:13) (cid:114) 1
(cid:13) (W) e (cid:13) ≤σ dlog2+2log
(cid:13) i i(cid:13) I δ
(cid:13) (cid:13)
i=1 (2Id)−1
with probability at least 1−δ. The proof is completed by noting that
∥(cid:80)d
(W) e ∥ =
i=1 i i (2Id)−1
√1 ∥W∥ 2.
2
Finally,wedemonstratethatsubGaussiandistributionwithlower-boundedvariancesatisfiestheanti-
concentrationconditionanalogoustoFact1. Wenormalizethedistributionsothatitis1-subGaussian.
Lemma11(Sufficientconditionforanti-concentration). SupposethatP isareal-valueddistribution
thatissymmetric,1-subGaussian,andhasvarianceatleast1/2. SupposeZ ∈Rnforsomen∈N
anditscomponentsarei.i.d. samplesofP. Then,foranyfixedu∈Rn,itholdsthat
(cid:18) (cid:19)
1
P u⊤Z ≥ ∥u∥ ≥0.01.
3 2
Proof. Withoutlossofgenerality,wemayassumethat∥u∥ =1. LetY =u⊤Z. Then,Var(Y)=
2
Var((cid:80)n (u) (Z) )=(cid:80)n (u)2Var((Z) )=Var(P)≥ 1. Ontheotherhand,weattainanupper
i=1 i i i=1 i i 2
boundofVar(Y)asfollows:
Var(Y)=E(cid:2) Y2(cid:3)
(cid:20) (cid:26) (cid:27)(cid:21) (cid:20) (cid:26) (cid:27)(cid:21)
=E Y21 |Y|≤ 1 +E Y21 1 <|Y|≤4 +E(cid:2) Y21{|Y|≥4}(cid:3)
3 3
(cid:18) (cid:19) (cid:18) (cid:19)
≤ 1 P |Y|≤ 1 +16P 1 <|Y|≤4 +E(cid:2) Y21{|Y|≥4}(cid:3)
9 3 3
(cid:18) (cid:19)
≤ 1 +16P 1 <|Y| +E(cid:2) Y21{|Y|≥4}(cid:3) . (12)
9 3
We upper-bound E[Y21{|Y| ≥ 4}] using its subGaussian property. Note that Y = u⊤Z is 1-
subGaussiansince∥u∥ =1andthecomponentsofZ areindependentand1-subGaussian. Applying
2
thestandardtailboundofsubGaussianrandomvariables,itholdsthatP(|Y|≥x)≤2exp(cid:0) −x2/2(cid:1)
,
orequivalently,P(cid:0) Y2 ≥x(cid:1) ≤2exp(−x/2). Then,itholdsthat
(cid:90) ∞
E(cid:2) Y21(cid:8) Y2 ≥16(cid:9)(cid:3) = P(cid:0) Y21(cid:8) Y2 ≥16(cid:9) ≥x(cid:1) dx
0
(cid:90) 16 (cid:90) ∞
= P(cid:0) Y21(cid:8) Y2 ≥16(cid:9) ≥x(cid:1) dx+ P(cid:0) Y21(cid:8) Y2 ≥16(cid:9) ≥x(cid:1) dx
0 16
(cid:90) ∞
=16P(cid:0) Y2 ≥16(cid:1) + P(cid:0) Y2 ≥x(cid:1) dx
16
(cid:90) ∞
≤32e−8+ 2e−x 2 dx
16
=32e−8+4e−8
≤0.0121. (13)
Bypluggingintheupperboundof(13)toinequality(12)andreorderingtheterms,weobtainthat
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1
P |Y|> ≥ − −0.0121 ≥0.023.
3 16 2 9
Finally, recall that Z is symmetric, therefore Y is symmetric. Therefore, we have
P(cid:0)
Y ≥
1(cid:1)
≥
3
0.023 ≥0.01.
2
22I AuxiliaryLemmas
Lemma12(Azuma-HoeffdingInequality). Fixn∈N. Let{Z }n beasequenceofreal-valued
i i=1
randomvariablesadaptedtoafiltration{F }n .Supposethatthereexistsa<bsuchthatZ ∈[a,b]
i i=0 i
holds almost surely for all i ∈ [n]. Then, for any δ ∈ (0,1], the following inequality holds with
probabilityatleast1−δ:
n (cid:114)
(cid:88) n 1
(Z −E[Z |F ])≤(b−a) log .
i i t−1 2 δ
i=1
Lemma13(Lemma1ofLaurentandMassart[15]). LetY ,...,Y bei.i.d. standardGaussian
1 d
variables. SetZ =(cid:80)d (Y2−1). Then,thefollowinginequalityholdsforanyx>0,
i=1 i
√
P(Z ≥ dx+2x)≤e−x.
23