A Comparative Study of Deep Reinforcement Learning for Crop
Production Management
JosephBalderasa, DongChenb, YanboHuang*c, LiWanga and Ren-CangLia
aDepartmentofMathematics,UniversityofTexasatArlington,Arlington,TX76019,USA
bDepartmentofAgricultural&BiologicalEngineering,MississippiStateUniversity,MississippiState,MS39762,USA
cUSDA-ARSGeneticsandSustainableAgricultureResearchUnit,MississippiState,MS39762,USA
*YanboHuang(Yanbo.Huang@usda.gov)isthecorrespondingauthor.
ARTICLE INFO ABSTRACT
Keywords: Cropproductionmanagementisessentialforoptimizingyieldandminimizingafield’senvironmental
Cropproductionmanagement impacttocropfields,yetitremainschallengingduetothecomplexandstochasticprocessesinvolved.
Reinforcementlearning Recently,researchershaveturnedtomachinelearningtoaddressthesecomplexities.Specifically,
Deeplearning reinforcement learning (RL), a cutting-edge approach designed to learn optimal decision-making
Machinelearning strategies through trial and error in dynamic environments, has emerged as a promising tool for
Proximalpolicyoptimization developingadaptivecropmanagementpolicies.RLmodelsaimtooptimizelong-termrewardsbycon-
DeepQ-networks tinuouslyinteractingwiththeenvironment,makingthemwell-suitedfortacklingtheuncertaintiesand
Agriculture variabilityinherentincropmanagement.StudieshaveshownthatRLcangeneratecropmanagement
policiesthatcompetewith,andevenoutperform,expert-designedpolicieswithinsimulation-based
cropmodels.Inthegym-DSSATcropmodelenvironment,oneofthemostwidelyusedsimulators
forcropmanagement,proximalpolicyoptimization(PPO)anddeepQ-networks(DQN)haveshown
promisingresults.However,thesemethodshavenotyetbeensystematicallyevaluatedunderidentical
conditions.Inthisstudy,weevaluatedPPOandDQNagainststaticbaselinepoliciesacrossthree
different RL tasks, fertilization, irrigation, and mixed management, provided by the gym-DSSAT
environment.Toensureafaircomparison,weusedconsistentdefaultparameters,identicalreward
functions,andthesameenvironmentsettings.OurresultsindicatethatPPOoutperformsDQNin
fertilizationandirrigationtasks,whileDQNexcelsinthemixedmanagementtask.Thiscomparative
analysisprovidescriticalinsightsintothestrengthsandlimitationsofeachapproach,advancingthe
developmentofmoreeffectiveRL-basedcropmanagementstrategies.
differentRLalgorithmsperformwhenappliedtothesevir-
1. Introduction
tual environments. One of the first crop RL environments
Crop production management is the process of taking
introduced was CropGym, an OpenAI gym environment
logicalactionsonacropfieldinordertoachievecroppro-
developed by Overweg et al. (2021) for the problem of
duction goals. Unfortunately, it can be difficult for farmers
Nitrogen fertilization management. Proximal policy opti-
tochooseoptimalpracticesduetocomplexphysical,chem-
mization(PPO)wasusedinthisstudytosuccessfullylearn
ical,andbiologicalphenomenawhichtakeplaceonacrop
fertilization policies which lowered negative environmen-
fieldandrandomexternalfactors,suchasweather(Husson
tal impact. Gautron et al. (2022b) then developed gym-
et al., 2021). With the advent of machine learning and its
DSSAT,aflexiblegymenvironmentthatutilizedthepopular
success in the field of agricultural science (Liakos et al.,
cropsimulatorDecisionSupportSystemforAgrotechnology
2018), researchers have begun to study how data-intensive
Transfer (DSSAT) to support fertilization and irrigation
algorithms can be used to learn optimal policies for crop
problems. PPO was shown to outperform expert baseline
management(Gautronetal.,2022a).Inparticular,methods
policiesinthegym-DSSATenvironment.Toaddresslong-
such as reinforcement learning (RL) have been applied to
term policy learning, Turchetta et al. (2022) introduced
cropmanagementproblemsandhaveproducedcompetitive
CyclesGym,acropenvironmentbasedonmulti-year,multi-
results against existing expert policies on crop simulators
crop CGM Cyles. This RL environment was also tested
(Gautron et al., 2022b; Tao et al., 2022). RL algorithms
usingPPO.Taoetal.(2022)implementeddeepQ-network
work by training an agent on a trial-and-error basis in an
(DQN)onthegym-DSSATenvironmenttoattainimproved
environment with the objective of maximizing cumulative
RL agent performance. They also implemented imitation
returns; after training, the algorithms output static policies
learning(IL)forthescenariowhereonlyafewstatevariables
thatcanbedeployedforreal-worldapplications.
areavailable.Wuetal.(2022)continuedthisstudybytesting
Recentresearchhasfocusedonthedevelopmentofcrop
DQNagainstsoftactor-critic(SAC)andotherbaselinepoli-
field RL environment software and the exploration of how
ciesongym-DSSATfortheNitrogenmanagementproblem.
Finally,Wuetal.(2024)combinedalanguagemodel(LM)
joseph.balderas@uta.edu(J.Balderas);dc2528@msstate.edu(D.
Chen);yanbo.huang@usda.gov(Y.Huang*);li.wang@uta.edu(L.Wang); with DQN to obtain a more optimal policy on the gym-
rcli@uta.edu(R.Li) DSSAT environment. This was achieved by using the LM
ORCID(s):
to convert state variables into more informative language.
While these studies demonstrated promising results, they
Balderas et al.: PreprintsubmittedtoElsevier Page1of10
4202
voN
6
]YS.ssee[
1v60140.1142:viXraDeep Reinforcement Learning for Crop Production Management
primarily focused on evaluating individual RL algorithms,
without systematically comparing different methods under
consistentsettings.
In this study, we conduct a comprehensive comparison
betweenthePPOandDQNalgorithms,focusingexclusively
onRLwithinthegym-DSSATenvironment.Toensureafair
comparison, we evaluate both algorithms using the reward
Figure 1: In the RL process, an agent makes an action in an
functionsandbaselinemethodsfromGautronetal.(2022b)
environment, and the environment in turn produces a new
forthefertilizationandirrigationtasks.Forthemixedprob-
state and a reward which informs the agent of its current
lem,whichconsidersfertilizationandirrigationsimultane-
performance. The goal of the agent is to use the environment
ously, we apply the economic profit reward function from
feedbacktomaximizeitscumulativerewards.Thislooprepeats
Taoetal.(2022).Additionally,bothmodelsaretrainedwith uptoaspecifiednumberofiterationsoruntilaterminalstate
random weather conditions, addressing a recommendation is reached (Gautron et al., 2022a).
from previous DQN studies (Tao et al., 2022; Wu et al.,
2022,2024),whichusednon-randomweatherbutsuggested
randomizationforfutureresearch.Thiscomparisonaimsto definedastheexpecteddiscountedreturn:
providevaluableinsightsintotherelativestrengthsofthese
algorithms, helping to inform the selection of appropriate [ 𝑇−1 ]
∑
RLmethodsforreal-worldcropmanagementapplications. 𝐽(𝜋)=𝔼 𝛾𝑡𝑟 , (1)
𝜏∼𝑝
𝜋
𝑡
The remainder of this paper is structured as follows. 𝑡=0
First, we introduce the fundamental concepts of RL and
where 𝜋 ∶ 𝑆 → (𝐴) is a policy mapping each state 𝑠 to
provide a detailed overview of the competing algorithms,
DQN and PPO. Next, we describe the gym-DSSAT en-
aprobabilitydistribution𝜋(𝑎 |𝑠)overactionsin𝐴,𝑝
𝜋
isthe
trajectory distribution under policy 𝜋, and 𝛾 ∈ (0,1] is a
vironment, including its default settings and the three RL
discountfactorthatreducestheinfluenceoffuturerewards
tasks it supports: fertilization, irrigation, and mixed man-
(Levineetal.,2020).ThegoalofRListolearnanoptimal
agement.Finally,wepresentandanalyzetheresultsofour
policy𝜋∗thatmaximizes𝐽(𝜋).Thepolicyisoftenmodeled
experiments, followed by a discussion of key findings and
usinganeuralnetworkwithparameters𝜃.
suggestionsforfutureresearchdirections.
InRL,thevaluefunction𝑉𝜋 ∶ 𝑆 → ℝassignsavalue
to each state 𝑠, representing the expected future rewards
2. PreliminariesofReinforcementLearning
whenstartingfromstate𝑠andfollowingpolicy𝜋.Thevalue
function𝑉𝜋 isformallydefinedas
Inthissection,wereviewsomepreliminariesofRL,fo-
cusingonMarkovDecisionProcess,andkeyRLalgorithms. [ ]
𝑇−1 |
𝑉𝜋(𝑠)=𝔼 ∑ 𝛾𝑡𝑟| |𝑠 =𝑠 . (2)
2.1. MarkovDecisionProcess 𝜏∼𝑝 𝜋 𝑡=0 𝑡| | 0
An RL environment can be modeled as a Markov de- |
cision process (MDP) (Sutton and Barto, 2018). An MDP Similarly, the state-action value function, or Q-function
is defined by the tuple  = ⟨𝑆,𝐴,p,r ⟩, where 𝑆 is the 𝑄𝜋 ∶ 𝑆 ×𝐴 → ℝ,assignsavaluetoeachstate-actionpair
set of environment states, 𝐴 is the set of actions, p is the (𝑠,𝑎), describing the expected future rewards when taking
transitionfunction,whichgivestheprobabilityp(𝑠′ |𝑠,𝑎)of action𝑎instate𝑠andfollowingpolicy𝜋.Itisdefinedas
transitioningtostate𝑠′ ∈ 𝑆 givenaction𝑎 ∈ 𝐴istakenin
[ ]
state 𝑠 ∈ 𝑆, and r is the reward function, which provides 𝑄𝜋(𝑠,𝑎)=𝔼 𝑇 ∑−1 𝛾𝑡𝑟| | |𝑠 =𝑠,𝑎 =𝑎 . (3)
t 𝑎he ise px ep re foct re md er dew inar sd tatr e(𝑠 𝑠,𝑎 a, n𝑠 d′) re= sul𝔼 ts[𝑟 i| n𝑠, s𝑎 ta, t𝑠 e′] 𝑠′w (h Pe un tea rc mti ao nn , 𝜏∼𝑝 𝜋 𝑡=0 𝑡| | | 0 0
2014). At each time step 𝑡 ∈ 0,1,2,…,𝑇 −1, where 𝑇 An optimal policy 𝜋∗ can be derived by first learning
can be infinite, an agent interacts with the environment by theoptimalQ-function𝑄∗andthenchoosingtheactionthat
observingthecurrentstate𝑠,takinganaction𝑎,receiving maximizesthisfunction:
𝑡 𝑡
a reward 𝑟, and observing the next state 𝑠 . A trajectory
𝑡 𝑡+1
isthefullsequenceofstates,actions,andrewards,denoted
⎧ 1 if𝑎=argmax𝑄∗(𝑠,𝑎′),
b 𝑆y 𝑓𝜏 ⊂= 𝑆( ,𝑠 c0 o,𝑎 n0 si, s𝑟 t0 s, o𝑠 1 f, s𝑎 ta1, te𝑟 s1, t… hat, t𝑠 e𝑇 r) m. iT nh ae tes ae nt o ef pifi sn oa dl es wta hte es n, 𝜋∗(𝑎 |𝑠)=⎪
⎨
⎪0
otherwis𝑎 e′ .∈𝐴 (4)
reached. ⎩
AnMDP,togetherwithanobjectivefunction𝐽 thatthe
An RL algorithm that combines policy gradient estimation
agentseekstooptimize,isreferredtoasaMarkovdecision
with Q-learning is referred to as an actor-critic method. In
problem(Gautronetal.,2022a).Theobjectivefunction𝐽 is
the following section, we will explore how DQN operates
asaQ-learningmethod,whilePPOservesasanactor-critic
method.
Balderas et al.: PreprintsubmittedtoElsevier Page 2 of 10Deep Reinforcement Learning for Crop Production Management
2.2. DQNalgorithm Algorithm1DQN
DeepQ-Network(DQN)(Mnihetal.,2015)isabreak- 1: Initializereplaymemory𝐷tosize𝑁
throughRLalgorithmdesignedtoapproximatetheoptimal 2: InitializeQ-network𝑄withrandomweights𝜃
Q-function in environments with a discrete action space 3: Initializetargetnetwork𝑄̂ withweights𝜃− =𝜃
using a neural network, or Q-network. The objective is 4: forepisode=1,…,𝑀 do
to approximate the optimal action-value function 𝑄∗(𝑠,𝑎), 5: Initializestate𝑠
0
whichisdefinedas 6: for𝑡=0,1,…,𝑇 −1do
[ ] 7: Withprobability𝜀,selectrandomaction𝑎
𝑇−1 | 𝑡
𝑄∗(𝑠,𝑎)=m 𝜋ax𝔼 𝜏∼𝑝 𝜋 ∑
𝑡=0
𝛾𝑡𝑟 𝑡| | | |𝑠 0 =𝑠,𝑎 0 =𝑎 , (5) 98 :: EO xth ee cr uw teis 𝑎e 𝑡, as ne dle oct b𝑎 se𝑡 r= vea 𝑟r 𝑡gm ana dx 𝑠𝑎𝑄 𝑡+1(𝑠 𝑡,𝑎 𝑡;𝜃)
| 10: Storetransition(𝑠,𝑎,𝑟,𝑠 )in𝐷
Once the optimal Q-function is learned, the agent follows 𝑡 𝑡 𝑡 𝑡+1
11: Sampletransitions(𝑠 ,𝑎 ,𝑟 ,𝑠 )from𝐷
a greedy policy by selecting the action that maximizes 𝑗 𝑗 𝑗 𝑗+1
12: Set𝑦 =𝑟 ifepisodeendsatstep𝑗+1
𝑄∗(𝑠,𝑎) for each state, as shown in (4). The key equation 𝑗 𝑗
13: Otherwise,set𝑦 =𝑟 +𝛾max𝑄̂(𝑠 ,𝑎′;𝜃−)
forlearning𝑄∗ istheBellmanequation(SuttonandBarto, 𝑗 𝑗
𝑎′
𝑗+1
2018):
14:
Dogradientdescenton(
𝑦
−𝑄(
𝑠 ,𝑎
;𝜃))2
𝑗 𝑗 𝑗
[ ] 15: Every𝐶 steps,set𝑄̂ =𝑄
𝑄∗(𝑠,𝑎)=𝔼 𝑟+𝛾 max𝑄∗(𝑠′,𝑎′)| |𝑠,𝑎 , (6)
𝑠′∼p 𝑎′ |
|
16: endfor
17: endfor
where𝑠′isthenextstate. 18: Output:Q-networkweights𝜃
Ateachtrainingstep,theQ-networkisupdatedbymin-
imizing the mean-squared error between the predicted Q-
valuesandthetargetvaluesderivedfromtheBellmanequa- where𝐴 = 𝐴𝜋 𝜃old(𝑠,𝑎)istheadvantagefunction,defined
𝑡 𝑡 𝑡
tion(Mnihetal.,2015).Thelossfunctionusedtoupdatethe as 𝐴𝜋(𝑠 𝑡,𝑎 𝑡) = 𝑄𝜋(𝑠 𝑡,𝑎 𝑡) − 𝑉𝜋(𝑠 𝑡), KL(⋅||⋅) is the KL
Q-networkatiteration𝑖isdefinedas divergence function, and 𝛿 is a hyperparameter (Schulman
et al., 2015). This problem is solved by approximating the
[ ]
𝐿 𝑖(𝜃 𝑖)=𝔼 𝑠,𝑎,𝑟,𝑠′ ( 𝑦 𝑖−𝑄(𝑠,𝑎;𝜃 𝑖))2 , (7) advantage function 𝐴 𝑡 and the KL constraint, followed by
the conjugate gradient method. TRPO finds large policy
where𝑦 = 𝑟+𝛾 max𝑄(𝑠′,𝑎′;𝜃−)representsthetargetQ- updates while ensuring the updates are constrained by the
𝑖 𝑎′ 𝑖
value, and 𝜃 and 𝜃− are the weights of the Q-network and KLdivergencetopreventthepolicyfromdeviatingtoofar
𝑖 𝑖
targetnetwork,respectively,atiteration𝑖. fromthepreviousone.
Two critical innovations improve the stability and con- PPOaddressestheneedforthequadraticapproximation
vergenceofDQN.First,theuseofexperiencereplayallows in TRPO’s KL constraint by introducing a clip function in
theagenttostorepasttransitions(𝑠,𝑎,𝑟,𝑠 )inareplay theloss:
𝑡 𝑡 𝑡 𝑡+1
buffer𝐷 ={𝑒 ,…,𝑒 },whicharethenrandomlysampled [ (
during
training1
.
This𝑁
helps break the temporal correlation 𝐿CLIP (𝜃)=𝔼 min
𝜋 𝜃(𝑎 𝑡|𝑠 𝑡)
𝐴,
betweenconsecutivesamplesandreducesvariance.Second, 𝑡 𝑡 𝜋 𝜃old(𝑎 𝑡|𝑠 𝑡) 𝑡
(9)
atargetnetworkisusedtostabilizethelearningprocess.The ( ) )]
targetnetworkisacopyoftheQ-network,anditsweightsare clip
𝜋 𝜃(𝑎 𝑡|𝑠 𝑡)
,1−𝜀,1+𝜀 𝐴 .
updatedevery𝐶step.Byholdingthetargetnetworkconstant
𝜋 𝜃old(𝑎 𝑡|𝑠 𝑡) 𝑡
for several updates, DQN reduces the risk of divergence Here,theclipfunctionkeepstheratioofthenewpolicytothe
andpolicyoscillationsthatcanoccurwhenthetargetvalues oldpolicywithintherange[1−𝜀,1+𝜀],preventingthepolicy
changetoorapidly. fromupdatingtooaggressivelyandeliminatingtheneedfor
ThefullpseudocodefortheDQNalgorithmisprovided anexplicitoptimizationconstraint(Schulmanetal.,2017).
inAlgorithm1,adaptedfrom(Mnihetal.,2015). Ifweshareparametersbetweenthepolicynetworkandthe
Forournumericalexperiments,weusetheDQNimple- value function network, we can modify 𝐿CLIP by adding a
mentationfrom(Wuetal.,2024).Moredetailsaregivenin valuefunctionerrortermandanentropyte𝑡 rm:
Section4.2.
[ ]
2.3. PPOalgorithm
𝐿 𝑡(𝜃)=𝔼
𝑡
𝐿C 𝑡LIP (𝜃)−𝑐
1(
𝑉𝜋 𝜃(𝑠 𝑡)−𝑉
𝑡targ)2
+𝑐 2𝑆[𝜋 𝜃](𝑠 𝑡) .
Proximal Policy Optimization (PPO) is based on the
(10)
actor-criticmethod,TrustRegionPolicyOptimization(TRPO)
(Schulmanetal.,2015),whichseekstosolve In(10),𝑐 and𝑐 arehyperparameters,and𝑆 isanentropy
1 2
function.ThefullpseudocodeforPPOwithsharednetworks
[ ]
max 𝔼
𝜋 𝜃(𝑎 𝑡|𝑠 𝑡)
𝐴
isshowninAlgorithm2(Schulmanetal.,2017).
𝜃 𝑡
[
𝜋 𝜃o (ld(𝑎 𝑡|𝑠 𝑡) 𝑡
)]
(8) StabT lo e-Bim asp el le im nee sn 3t 1P .P 6O .0, (w Rae ffiu nse ett ah le .,P 2P 02O 1)p .a Mck oa rg ee def tr ao im
ls
s.t. 𝔼
𝑡
KL 𝜋 𝜃old(⋅|𝑠 𝑡)||𝜋 𝜃(⋅|𝑠 𝑡) ≤𝛿, aregiveninSection4.2.
Balderas et al.: PreprintsubmittedtoElsevier Page 3 of 10Deep Reinforcement Learning for Crop Production Management
Algorithm2PPO 3.2. Fertilizationproblem
1: Initializeparameters𝜃 old In the fertilization problem, the RL agent is tasked
2: foriteration=1,2,… do withdeterminingthedailyapplicationofnitrogenfertilizer
3: foractor=1,2,…,𝑁 do (kg/ha) within the range of [0,200]. During this task, the
4: Run𝜋 for𝑇 timesteps cropsarestrictlyrainfed,meaningtheagenthasnocontrol
5: Compu𝜃o tl ed advantagefunctions𝐴 ,…,𝐴 over irrigation, and planting operations are automatically
1 𝑇
6: endfor managedbyDSSATbasedonsoiltemperatureandhumidity
7: Optimize𝐿with𝐾epochs,minibatchsize≤𝑁𝑇 conditions. The primary goal is to maximize the nitrogen
8: Set𝜃 old =𝜃 uptake by the crops while minimizing excessive fertilizer
9: endfor use.
10: Output:Networkweights𝜃 Therewardfunctionforthefertilizationproblemencour-
agesefficientnitrogenusebybalancingnitrogenuptakewith
the cost of fertilization. Specifically, let trnu(𝑡) denote the
3. gym-DSSAT plant’s nitrogen uptake between day 𝑡 and 𝑡 + 1, and let
𝑁 represent the amount of nitrogen applied on day 𝑡. The
Inthissection,weoutlinethecropmanagementprocess, 𝑡
rewardfunctionisdefinedas
whichincludesthesimulatoraswellasthefertilization,irri-
gation,andcombinedfertilizationandirrigationproblems. 𝑟 (𝑡)=trnu(𝑡,𝑡+1)−0.5𝑁, (11)
𝐹 𝑡
3.1. Environmentdetails where the first term rewards nitrogen uptake, and the sec-
This paper utilizes the gym-DSSAT environment, an ondtermpenalizesexcessivefertilizationfromtheprevious
open-source RL framework built with the popular Python day. This reward structure reflects the trade-off between
RLtoolkitOpenAIGym1andbasedontheDecisionSupport promotingplantgrowthandavoidingunnecessaryfertilizer
SystemforAgrotechnologyTransfer(DSSAT)cropsimula- application(Gautronetal.,2022b).
tor(Gautronetal.,2022b).Thedefaultconfigurationofgym-
DSSATsimulatesamaizeexperimentconductedin1982at 3.3. Irrigationproblem
theUniversityofFloridafarminGainesville,Florida,USA. Theirrigationproblemfocusesonthedailyapplication
Eachtimestepwithinthesimulationcorrespondstooneday, of water (L/m2) within the range of [0,50]. The agent is
withafullepisoderepresentinganentiregrowingseasonof responsible for managing irrigation, while fertilization is
approximately160days.Theepisodeconcludeseitherwhen handled deterministically, with small amounts of nitrogen
the crops are ready to be harvested or when a crop failure fertilizer applied automatically at 40, 45, and 80 days after
occurs. Additionally, weather conditions are stochastically planting.Theagentmustoptimizewaterusagetomaximize
generatedforeachepisode,addingvariabilitytothesimula- cropgrowth,balancingthewaterprovidedagainsttheplant’s
tion. The gym-DSSAT environment provides three distinct biomassgain.
managementmodes:fertilization,irrigation,andacombined Lettopwt(𝑡,𝑡+1)representthechangeinabove-ground
mode that allows both fertilization and irrigation. In each biomass(kg/ha)betweenday𝑡and𝑡+1,andlet𝑊 denote
𝑡
mode, the RL agent interacts with the environment and the amount of water applied on day 𝑡. The reward function
makesdecisionsonadailybasis.Thesemodesreflectreal- fortheirrigationproblemisdefinedas
world agricultural challenges where farmers must decide
how much nitrogen fertilizer to apply, how much water to 𝑟 (𝑡)=topwt(𝑡,𝑡+1)−15𝑊, (12)
𝐼 𝑡
use for irrigation, or how to balance both inputs. Detailed
where the first term rewards increases in biomass, and the
descriptions of these modes are provided in Sections 3.2 -
second term penalizes excessive water use. This reward
3.4.
structure encourages the agent to apply water judiciously,
To mirror the real-world uncertainty that farmers face,
promotingcropgrowthwhileminimizingover-irrigation.
the gym-DSSAT environment limits the agent’s access to
the full set of state variables available in DSSAT, making
3.4. Mixedproblem
onlyasubsetofvariablesobservableateachtimestep.This
The mixed problem combines the tasks of fertilization
designintroducestheproblemofpartialobservability,where
and irrigation, requiring the RL agent to simultaneously
the RL agent must make decisions based on incomplete
managebothnitrogenapplicationandwaterusage.Theagent
information.Despitethispartialobservability,gym-DSSAT
canapplynitrogenfertilizerintherangeof[0,200]kg/haand
canstillbeeffectivelytreatedasaMarkovDecisionProcess
providewaterforirrigationwithintherangeof[0,50]L/m²
(MDP), with no significant complications in implementa-
eachday.Theobservationspaceinthismodeistheunionof
tion.Acomprehensivedescriptionoftheobservationspace
the observation spaces from the fertilization and irrigation
foreachmodecanbefoundin(Gautronetal.,2022b).
problems.
1 https://www.gymlibrary.dev/index.html The reward at each interaction is a vector (𝑟 (𝑡),𝑟 (𝑡)),
𝐹 𝐼
representing the rewards for fertilization and irrigation.
However,sincemanyRLalgorithms,suchasPPOandDQN,
Balderas et al.: PreprintsubmittedtoElsevier Page 4 of 10Deep Reinforcement Learning for Crop Production Management
require scalar rewards, a scalar reward function is used for Itisimportanttonotethatthesebaselinemethodsdiffer
numerical experiments, as suggested by (Tao et al., 2022). from those used in other RL studies such as (Tao et al.,
Therewardfunctionforthemixedproblemisdefinedas 2022), where fertilizers and irrigation were applied based
on crop growth indicators (Wright et al., 2022). For our
{ 𝑤 𝑌 −𝑤 𝑁 −𝑤 𝑊 −𝑤 𝑁 ifharvestat𝑡, fertilization and irrigation tasks, we used reward functions
𝑟 (𝑡)= 1 2 𝑡 3 𝑡 4 𝑙,𝑡 (11) and (12), while for the mixed experiment, we applied
𝑀 −𝑤 𝑁 −𝑤 𝑊 −𝑤 𝑁 otherwise,
2 𝑡 3 𝑡 4 𝑙,𝑡 therewardfunction(13),configuredtoprioritizeeconomic
(13) profit.EachtrainedRLpolicywastestedagainstthesebase-
lines over 1000 episodes, with different stochastic weather
where 𝑤 ,𝑤 ,𝑤 , and 𝑤 are weights that represent the
1 2 3 4 conditionstoensurerobustness.Themainevaluationmetric
importance of yield (𝑌), nitrogen usage, water usage, and
was the average cumulative reward, which allowed us to
nitrate leakage (𝑁 ), respectively. These weights can be
𝑙,𝑡 assess overall policy performance under varying environ-
adjusteddependingontheprioritiesofthemanagementtask.
mentalconditions.
For instance, when 𝑤 = 0.158,𝑤 = 0.79,𝑤 = 1.1,
1 2 3
and 𝑤 = 0, the reward function balances the competing
4 4.2. Implementationdetails
objectives of maximizing yield and minimizing resource
ToimplementPPO,weusedtheStable-Baselines31.6.0
use.
library (Raffin et al., 2021), with default hyperparameters.
Themixedproblempresentstheagentwithamorecom-
The discount factor was set to 𝛾 = 1.0, consistent with
plex decision-making environment, where it must balance
the gym-DSSAT environment settings in (Gautron et al.,
the trade-offs between nitrogen and water application to
2022b).PPOwastrainedfor106timesteps,withvalidation
maximize yield while minimizing negative environmental
checksevery1000steps.Duringevaluation,theactionswere
impacts,suchasnitrateleakage.Thiscombinedtaskclosely
selected stochastically for the fertilization and irrigation
mirrorsreal-worldagriculturalmanagement,wherefarmers
tasks. However, for the mixed problem, better results were
must simultaneously manage multiple inputs to optimize
achievedbyusingdeterministicactionselection,abehavior
cropproductionoutcomes.
controlledwithinthepredictfunctionofthePPOimplemen-
tation.
4. NumericalExperiments ForDQN,weutilizedthePythonimplementationfrom
(Wuetal.,2024)2,whichsupportsthediscretizationofgym-
In this section, we compare the performance of two
DSSAT’s continuous action space. Specifically, the action
RL algorithms, DQN and PPO, in the gym-DSSAT en-
spacewasdiscretizedasfollows:
vironment. Our focus is on assessing how these methods
performonfertilization,irrigation,andamixedstrategythat
incorporatesbothfertilizationandirrigationpolicies.
𝐴={40𝑖kg/haNfertilizer&6𝑗 L/m2irrigationwater},
(14)
4.1. Experimentalsetup
The experiments were conducted in gym-DSSAT’s de- where 𝑖,𝑗 = 0,1,2,3,4. The DQN model was trained for
fault environment, simulating the 1982 maize experiment 4000episodeswithadiscountfactorof𝛾 = 0.99,usingthe
at the University of Florida, as mentioned in Section 3.1.
𝜀-greedy method for action selection. The neural network
architectureconsistedofthreehiddenlayers,eachwith256
BothRLalgorithms—PPOandDQN—weretrainedtolearn
three distinct policies: a fertilization policy, an irrigation
units.Thelearningratewassetto10−5,andthebatchsize
was1024.
policy,andamixedpolicycombiningbothfertilizationand
irrigation. The performance of these learned policies was
compared against two baseline methods previously used in 4.3. Policytraining
The training curves for PPO and DQN across the three
(Gautronetal.,2022b):
tasks—fertilization, irrigation, and mixed—are shown in
• Nullpolicy:Thisbaselineinvolvesapplyingnonitro- Figures 2, 3, and 4, respectively. These curves plot the
genfertilizer(0kg/ha)andnoirrigationwater0L∕m2. cumulativerewardsasafunctionofthenumberoftraining
The Null policy serves as a control to evaluate crop iterations.
growthwithonlynaturalinputs,suchasnitrogenfrom For the fertilization problem, as shown in Figure 2,
thesoilandrainwater.Thiscomparisonisessentialto bothPPOandDQNconvergedearlyinthetrainingprocess,
determinetheeffectofRL-basedintervention(How- indicatingthatlearninganoptimalfertilizationpolicyisrel-
ell,2003;Vanlauweetal.,2011). ativelyquickforbothalgorithms.Thesteadyriseincumula-
tiverewardssuggeststhatbothmethodsquicklylearnedhow
• Expertpolicy:TheExpertpolicyreplicatestheorigi-
tooptimizenitrogenapplicationtoenhancecropgrowth.
nalfertilizationstrategyfromthe1982experimentand
Incontrast,Figure3showsmoreoscillationduringtrain-
approximatestheirrigationschedule.Fertilizationand
ingfortheirrigationproblem,particularlyforDQN.These
irrigation are applied deterministically based on the
numberofdaysafterplanting,representingthetradi- 2
https://github.com/jingwu6/LM_AG/tree/main
tionalagronomicapproach(HuntandBoote,1998).
Balderas et al.: PreprintsubmittedtoElsevier Page 5 of 10Deep Reinforcement Learning for Crop Production Management
PPO Training Curve (mode=fertilization) PPO Training Curve (mode=irrigation)
0
10000
2000 0
10000
4000
20000
30000
6000
40000
8000 50000
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Timestep 1e6 Timestep 1e6
DQN Training Curve (mode=fertilization) DQN Training Curve (mode=irrigation)
0 15000
1000 10000
2000 5000
3000 0
4000 5000
5000 10000
15000
6000
0 500 1000 1500 2000 2500 3000 3500 4000
0 500 1000 1500 2000 2500 3000 3500 4000 Episode
Episode
Figure 3: PPO and DQN training curves for the irrigation
Figure 2: PPO and DQN training curves for the fertilization
problem. The horizontal axis measures training iterations and
problem. The horizontal axis measures training iterations and
the vertical axis measures cumulative rewards.
the vertical axis measures cumulative rewards.
For the fertilization problem, PPO outperformed both
oscillations indicate instability in the learning process, po-
theExpertandDQNpolicies,achievingthehighestaverage
tentially due tothe increased complexity of watermanage-
cumulative rewards with lower variance than the Expert
mentcomparedtofertilization.Theoscillationssuggestthat
policy.DQNperformedpoorlyonthistask,withthelowest
irrigationpoliciesaremoredifficultforbothPPOandDQN
averagecumulativerewardsandthelargestvariance,indicat-
tolearneffectively,whichmaybeduetothestochasticnature
ingalessconsistentlearningprocess.ThissuggeststhatPPO
of rainfall in the simulation and the more delicate balance
was more efficient in learning a fertilization strategy that
requiredtooptimizewaterusage.
balancednitrogenuptakewithpenalizingexcessivefertilizer
Themixedproblem,depictedinFigure4,demonstrates
use.
slower convergence for both PPO and DQN compared to
Intheirrigationproblem,PPOagainachievedthehigh-
thefertilizationtask.Whiletheoscillationswerelesssevere
est cumulative rewards, though it exhibited much higher
than those seen in the irrigation problem, DQN exhibited
variance compared to the Expert policy. This high vari-
morefluctuationsincumulativerewardsthanPPO,thoughit
ancemayindicatethatwhilePPOfoundeffectiveirrigation
ultimatelyachievedhigherrewards.Thissuggeststhatwhile
strategies, its performance fluctuated due to the stochastic
DQNismoresensitivetoparametertuningandexperiences
nature of rainfall in the environment. DQN, while closer
more variability during training, it can outperform PPO
in performance to PPO, still trailed the Expert policy and
whenthemodelisproperlyoptimized.
showedthehighestvariance,suggestingthatitstruggledto
consistentlybalancewaterusageacrossepisodes.
4.4. Evaluationresults
Forthemixedproblem(fertilizationandirrigation),the
Table 1 summarizes the average cumulative rewards
Expert policy achieved the highest average cumulative re-
and their standard deviations over 1000 test episodes for
wards.DQNfollowedclosely,outperformingPPObyasig-
eachpolicy(Null,Expert,PPO,andDQN)acrossthethree
nificant margin. Interestingly, PPO performed much worse
RL problems: fertilization, irrigation, and the mixed (all)
in this combined task compared to its performance in the
problem.Theperformanceofeachpolicyismeasuredbythe
single-taskproblems(fertilizationandirrigation).Although
averagecumulativereward,withthehighestscorebolded.
Balderas et al.: PreprintsubmittedtoElsevier Page 6 of 10
draweR
evitalumuC
draweR
evitalumuC
draweR
evitalumuC
draweR
evitalumuCDeep Reinforcement Learning for Crop Production Management
PPO Training Curve (mode=all) DQN Training Curve (mode=all)
2000
0
0
2500
2000
5000
4000
7500
10000 6000
12500 8000
15000
10000
17500
0.0 0.2 0.4 0.6 0.8 1.0 0 500 1000 1500 2000 2500 3000 3500 4000
Timestep 1e6 Episode
Figure4:PPOandDQNtrainingcurvesforthemixedproblem.Thehorizontalaxismeasurestrainingiterationsandthevertical
axis measures cumulative rewards.
Table 1
Average cumulative rewards and standard deviations over 1000 test episodes. The highest scores are bolded.
Null Expert PPO DQN
Fertilization 42.52±5.87 55.24±30.12 57.0±20.6 21.23±45.81
Irrigation 8213.84±3191.97 12068.41±750.9 12389.76±1379.06 10765.19±1700.18
All 175.52±59.55 691.87±272.6 257.09±149.12 594.88±299.17
PPO managed to outperform the Null policy, its poor per- extremesparsityinitsactionsexplainswhyPPOperformed
formance in the mixed problem may be attributed to its similarlytotheNullpolicyinthistask.Ontheotherhand,
difficulty in managing the complexity of simultaneously DQN applied nitrogen primarily between days 60 and 80
optimizingbothnitrogenandwaterapplications. at a high rate (around 120 kg/ha), and it applied irrigation
Figure5presentstheseevaluationresultsintheformof water consistently between days 80 and 120. DQN’s more
boxplots,providingavisualcomparisonofthecumulative balancedapproachtomanagingbothfertilizationandirriga-
rewards for each policy across the three tasks. The box tioncontributedtoitsbetterperformancerelativetoPPOin
plots reveal the distribution of rewards across the 1000 themixedproblem.
testepisodes,highlightingtheperformanceconsistencyand In summary, PPO demonstrated superior performance
variabilityofeachapproach. in single-task problems like fertilization and irrigation, but
Figures 6 and 7 present 2D histograms that show the struggledinthemorecomplexmixedproblem.Ontheother
frequencyofnonzerofertilizationandirrigationapplications hand,DQNexhibitedhighervariancebutperformedbetter
acrossalltestepisodes.Thesehistogramsofferinsightsinto inthemixedtaskbyemployingamorebalancedapplication
how the RL agents applied nitrogen and water throughout strategy.TheseresultssuggestthatwhilePPOmaybebetter
thegrowingseason.Forthefertilizationproblem(Figure6), suitedforsimplermanagementtasks,DQNmayhaveanad-
PPO concentrated most of its nitrogen applications around vantageinscenariosthatrequiresimultaneousoptimization
day 60, typically applying between 0 and 50 kg/ha of ni- ofmultiplevariables.
trogen. In contrast, DQN displayed no clear pattern, with
nitrogen applications scattered across the growing season,
5. Discussion
likely contributing to its lower performance. This lack of
a focused strategy likely explains why DQN struggled to The goal of this study was to evaluate the performance
outperformPPOandtheExpertpolicyinthistask.Intheirri- of PPO and DQN in the gym-DSSAT environment using
gationproblem,bothPPOandDQNappliedwaterprimarily defaultsettingsandparameters.Ournumericalexperiments
between days 80 and 120. PPO tended to apply moderate showed that PPO outperformed DQN on individual tasks
amounts of water (0-20 L∕m2), whereas DQN consistently such as fertilization and irrigation, but it performed worse
applied smaller amounts of water (around 6 L∕m2). The thanDQNonthemixedtask,whichrequiredmanagingboth
uniformityofDQN’sirrigationapplicationmayhavelimited fertilization and irrigation simultaneously. Notably, PPO
itsabilitytoadapttostochasticweatherconditions,resulting failedtomakenonzeroirrigationdecisionsinthemixedtask,
initslowerperformancerelativetoPPO. leading to a performance close to that of the Null policy.
Forthemixedproblem(Figure7),PPOappliednitrogen This behavior could be attributed to suboptimal parameter
only in the first few days after planting and surprisingly selection, particularly in scenarios requiring simultaneous
appliednowaterthroughouttheentiregrowingseason.This optimization across multiple inputs. One potential solution
Balderas et al.: PreprintsubmittedtoElsevier Page 7 of 10
draweR
evitalumuC
draweR
evitalumuCDeep Reinforcement Learning for Crop Production Management
Evaluation Performance (mode=fertilization) Evaluation Performance (mode=irrigation) Evaluation Performance (mode=all)
150 1500
100 16000 1250
14000
50 1000 12000
0 10000 750
50
8000 500
100
6000 250
150 4000 0
200 2000 250
Null Expert PPO DQN Null Expert PPO DQN Null Expert PPO DQN
Policy Policy Policy
Figure 5: Evaluation results shown as box plots. The vertical axis measures cumulative rewards for the 1000 test episodes.
PPO Nitrogen Usage (mode=fertilization) DQN Nitrogen Usage (mode=fertilization)
100 160
102
140 2×101
80
120
100 60
80
40
101
60
40
20 101
20
0 100 0
0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 140 160
Day of Simulation Day of Simulation
PPO Water Usage (mode=irrigation) DQN Water Usage (mode=irrigation)
40 25
103
102 20
30
15
20 102
101 10
10
5
101
0 100 0
0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 140 160
Day of Simulation Day of Simulation
Figure6:2Dhistogramsshowingthefrequencyofnonzerofertilizationandirrigationapplicationsduringtestingforthefertilization
problem and the irrigation problem. Darker areas correspond to higher frequencies.
wouldbetointroduceparameterselectionorhyperparameter the lack of robust validation procedures during model se-
tuning as part of the PPO training phase to optimize its lection may have contributed to both algorithms’ inconsis-
performanceacrossmorecomplextasks. tent performances. Future experiments should incorporate
Suboptimal parameter choices also seem to have af- systematicvalidationtechniques,suchascross-validationor
fected DQN’s performance, particularly in the fertilization grid search, to ensure that the models are evaluated using
problem, where it failed to surpass the Expert policy in optimal configurations. Additionally, future studies should
cumulativerewards.Thisindicatesthatfurthertuningorad- investigate the impact of different reward functions and
justmentstotherewardstructurecouldhelpimproveDQN’s performancecriteriatobetterunderstandhowthesefactors
ability to generalize across different conditions. Moreover, influencelearningoutcomes.Testingalternativerewardfor-
mulations,suchasmoredetailedeconomicorenvironmental
metrics,mayyieldricherinsightsintotheperformanceofRL
Balderas et al.: PreprintsubmittedtoElsevier Page 8 of 10
draweR
evitalumuC
)ah/gk(
tnuomA
rezilitreF
)2m/L(
tnuomA
noitagirrI
draweR
evitalumuC
)ah/gk(
tnuomA
rezilitreF
)2m/L(
tnuomA
noitagirrI
draweR
evitalumuCDeep Reinforcement Learning for Crop Production Management
PPO Nitrogen Usage (mode=all) PPO Water Usage (mode=all)
5
60
50 4
102
40
3
30
2
101
20
1
10
0 100 0
0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 140 160
Day of Simulation Day of Simulation
DQN Nitrogen Usage (mode=all) DQN Water Usage (mode=all)
160
140 25
103
120 102
20
100
15
80
102
60 10
40
5
20 101
101
0 0
0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 140 160
Day of Simulation Day of Simulation
Figure 7: 2D histograms showing the frequency of nonzero fertilization and irrigation applications during testing for the mixed
problem. Note, the PPO water usage histogram is empty because PPO provided no irrigation throughout all test episodes.
algorithms in agricultural management. Overall, the mixed To address some of these shortcomings, offline rein-
resultsbetweenPPOandDQNhighlighttheneedformore forcement learning (offline RL) offers a promising alter-
adaptive algorithms that can efficiently handle both single- native. Unlike traditional RL, which requires continuous
taskandmulti-tasklearningscenariosincropmanagement. interactions with the environment, offline RL learns from
Despite their potential, RL algorithms like PPO and staticdatasetsthatarecollectedinadvance.Thesedatasets
DQNpresentseveralshortcomings,particularlyinagricul- canconsistofhistoricaldatafrompreviousgrowingseasons,
tural applications such as crop management (Levine et al., sensordata,andexpertdecisions,makingofflineRLmuch
2020). First, RL algorithms require extensive interaction more feasible for agriculture. Since farmers often have ac-
with the environment to learn optimal policies, which in cesstolargeamountsofhistoricaldata,offlineRLeliminates
real-worldagriculturalsettingswoulddemandasignificant the need for costly and time-consuming real-time interac-
amount of time, resources, and data. This is impractical tions. Additionally, offline RL allows algorithms to learn
for real farms, where each interaction corresponds to an from past mistakes and successes, potentially leading to
entiregrowingseason,makingitdifficulttosimulateenough more robust and well-informed decision-making processes
real-time data to train the models effectively. Moreover, (Levineetal.,2020).
RL algorithms are sensitive to hyperparameters, and opti-
mizingtheseparameterstypicallyrequiresatrial-and-error
6. Conclusion
approach, leading to inefficient training. Furthermore, RL
models may struggle to generalize when confronted with In this paper, we conducted a comprehensive study of
variability in environmental conditions (e.g., changes in RL algorithms applied to the gym-DSSAT crop simulator
weather, soil conditions) that were not encountered during environment.WeprovidedanoverviewofthePPOandDQN
training, making them less robust in practical scenarios algorithms,detailedthegym-DSSATenvironment,andper-
(ChenandHuang,2024). formednumericalexperimentstoevaluatetheabilityofPPO
and DQN to learn fertilization and irrigation policies. Our
results showed that PPO outperformed DQN in individual
tasks, such as fertilization and irrigation, but struggled to
Balderas et al.: PreprintsubmittedtoElsevier Page 9 of 10
)ah/gk(
tnuomA
rezilitreF
)ah/gk(
tnuomA
rezilitreF
)2m/L(
tnuomA
noitagirrI
)2m/L(
tnuomA
noitagirrIDeep Reinforcement Learning for Crop Production Management
outperform DQN when tasked with managing both simul- K.G.Liakos,P.Busato,D.Moshou,S.Pearson,andD.Bochtis. Machine
taneously. As discussed, one limitation of our study was learninginagriculture:Areview.Sensors,18(8):2674,2018.
the lack of parameter tuning during training, which likely V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Belle-
mare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovski,etal.
impacted performance. Future work should focus on opti-
Human-levelcontrolthroughdeepreinforcementlearning. Nature,518
mizinghyperparametersforbothPPOandDQN,aswellas
(7540):529–533,2015.
exploringalternativerewardfunctionsandcropmanagement H.Overweg,H.N.Berghuijs,andI.N.Athanasiadis. Cropgym:arein-
scenarios. We also suggest that future research consider forcementlearningenvironmentforcropmanagement. arXivpreprint
applyingofflineRLalgorithmsforcropmanagement.Offline arXiv:2104.04326,2021.
RL, which relies on static datasets rather than real-time
M.L.Puterman.MarkovDecisionProcesses:DiscreteStochasticDynamic
Programming.JohnWiley&Sons,2014.
interactions, aligns more closely with how farmers operate
A.Raffin,A.Hill,A.Gleave,A.Kanervisto,M.Ernestus,andN.Dormann.
and has the potential to deliver more practical solutions in Stable-baselines3: Reliable reinforcement learning implementations.
real-worldfarmingapplications(Levineetal.,2020). JournalofMachineLearningResearch,22(268):1–8,2021.URLhttp:
//jmlr.org/papers/v22/20-1364.html.
J.Schulman,S.Levine,P.Abbeel,M.Jordan,andP.Moritz. Trustregion
AuthorshipContribution policyoptimization.InInternationalConferenceonMachineLearning,
pages1889–1897.PMLR,2015.
JosephBalderas:Investigation,Software,Formalanal-
J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov.Proximal
ysis, Writing - original draft; Dong Chen: Conceptual- policyoptimizationalgorithms.arXivpreprintarXiv:1707.06347,2017.
ization, Methodology, Resources, Supervision, Writing - R.S.SuttonandA.G.Barto. ReinforcementLearning:AnIntroduction.
review & editing; Yanbo Huang: Conceptualization, Re- MITpress,2018.
R.Tao,P.Zhao,J.Wu,N.F.Martin,M.T.Harrison,C.Ferreira,Z.Kalan-
sources,Supervision,Writing-review&editing;LiWang:
tari,andN.Hovakimyan.Optimizingcropmanagementwithreinforce-
Resources, Supervision, Writing - review & editing; Ren- mentlearningandimitationlearning.arXivpreprintarXiv:2209.09991,
CangLi:Resources,Supervision,Writing-review&edit- 2022.
ing. M.Turchetta,L.Corinzia,S.Sussex,A.Burton,J.Herrera,I.Athanasiadis,
J.M.Buhmann,andA.Krause. Learninglong-termcropmanagement
strategieswithcyclesgym. AdvancesinNeuralInformationProcessing
Acknowledgement Systems,35:11396–11409,2022.
B. Vanlauwe, J. Kihara, P. Chivenge, P. Pypers, R. Coe, and J. Six.
Thisresearchwasfinanciallysupportedbyasummerin-
Agronomicuseefficiencyofnfertilizerinmaize-basedsystemsinsub-
ternofaPh.D.studentattheUniversityofTexasatArlington
saharanafricawithinthecontextofintegratedsoilfertilitymanagement.
throughtheUSDAARSResearchApprenticeshipProgram PlantandSoil,339:35–50,2011.
at the University of Texas at Arlington through a Non- D.Wright,I.Small,C.Mackowiak,Z.Grabau,P.Devkota,andS.Paula-
AssistanceCooperativeAgreement(ProjectNumber:6066- Moraes. Field corn production guide: Ss-agr-85/ag202, rev. 8/2022.
EDIS,2022(4),2022.
21310-006-021-S). The USDA-ARS scientist works under
J.Wu,R.Tao,P.Zhao,N.F.Martin,andN.Hovakimyan.Optimizingnitro-
the federal in-house appropriated project (Project Number:
genmanagementwithdeepreinforcementlearningandcropsimulations.
6064-21600-001-000-D) from USDA-ARS National Pro- InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
gram216-SustainableAgriculturalSystems.Additionally, PatternRecognition,pages1712–1720,2022.
the project was supported in part by NSF DMS-2407692, J.Wu,Z.Lai,S.Chen,R.Tao,P.Zhao,andN.Hovakimyan. Thenew
agronomists: Language models are experts in crop management. In
NIHR01AG075582,andNIHR21AG079309.
Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pages5346–5356,2024.
References
D. Chen and Y. Huang. Integrating reinforcement learning and large
languagemodelsforcropproductionprocessmanagementoptimization
andcontrolthroughanewknowledge-baseddeeplearningparadigm,
2024.URLhttps://arxiv.org/abs/2410.09680.
R.Gautron,O.-A.Maillard,P.Preux,M.Corbeels,andR.Sabbadin. Re-
inforcementlearningforcropmanagementsupport:Review,prospects
andchallenges.ComputersandElectronicsinAgriculture,200:107182,
2022a.
R. Gautron, E. J. Padrón, P. Preux, J. Bigot, O.-A. Maillard, and
D. Emukpere. gym-dssat: a crop model turned into a reinforcement
learningenvironment.arXivpreprintarXiv:2207.03270,2022b.
T.A.Howell. Irrigationefficiency. EncyclopediaofWaterScience,467:
500,2003.
L.HuntandK.Boote.Dataformodeloperation,calibration,andevaluation.
UnderstandingOptionsforAgriculturalProduction,pages9–39,1998.
O. Husson, J.-P. Sarthou, L. Bousset, A. Ratnadass, H.-P. Schmidt,
J. Kempf, B. Husson, S. Tingry, J.-N. Aubertot, J.-P. Deguine, et al.
Soilandplanthealthinrelationtodynamicsustainmentofehandph
homeostasis:Areview.PlantandSoil,466(1):391–447,2021.
S.Levine,A.Kumar,G.Tucker,andJ.Fu.Offlinereinforcementlearning:
Tutorial,review,andperspectivesonopenproblems. arXivpreprint
arXiv:2005.01643,2020.
Balderas et al.: PreprintsubmittedtoElsevier Page 10 of 10