Graphical Abstract
A Collaborative Content Moderation Framework for Toxicity De-
tection based on Conformalized Estimates of Annotation Disagree-
ment
Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz
4202
voN
7
]LC.sc[
2v09040.1142:viXraHighlights
A Collaborative Content Moderation Framework for Toxicity De-
tection based on Conformalized Estimates of Annotation Disagree-
ment
Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz
•
A novel Collaborative Content Moderation framework based on Con-
formal Prediction.
•
We propose using a multitask approach with auxiliary task annotation
disagreement to guide reviews.
•
We propose new metrics, CARE and F1 Review, to measure human
collaboration effectiveness.
•
Experimental results show improvements in uncertainty quantification
and calibration.
•
Multitask approach enhances composite single tasks in content moder-
ation.A Collaborative Content Moderation Framework for
Toxicity Detection based on Conformalized Estimates of
Annotation Disagreement
Guillermo Villate-Castilloa, Javier Del Sera,b, Borja Sanzc
a
TECNALIA, Basque Research and Technology Alliance (BRTA), 48160, Derio, Spain
b
University of the Basque Country (UPV/EHU), 48013, Bilbao, Spain
c
Faculty of Engineering, University of Deusto, 48007, Bilbao, Spain
Abstract
Content moderation typically combines the efforts of human moderators
and machine learning models. However, these systems often rely on data
where significant disagreement occurs during moderation, reflecting the sub-
jective nature of toxicity perception. Rather than dismissing this disagree-
ment as noise, we interpret it as a valuable signal that highlights the inherent
ambiguity of the content—an insight missed when only the majority label is
considered. In this work, we introduce a novel content moderation frame-
work that emphasizes the importance of capturing annotation disagreement.
Our approach uses multitask learning, where toxicity classification serves as
the primary task and annotation disagreement is addressed as an auxiliary
task. Additionally, we leverage uncertainty estimation techniques, specifi-
cally Conformal Prediction, to account for both the ambiguity in comment
annotations and the model’s inherent uncertainty in predicting toxicity and
disagreement. Theframework also allowsmoderatorstoadjust thresholds for
annotation disagreement, offering flexibility in determining when ambiguity
should trigger a review. We demonstrate that our joint approach enhances
model performance, calibration, and uncertainty estimation, while offering
greater parameter efficiency and improving the review process in comparison
to single-task methods.
Keywords: Conformal prediction, Uncertainty quantification, Collaborative
content moderation, Toxicity detection
Preprint submitted to Pattern Recognition November 8, 20241. Introduction
Content moderation (CM) has been an important pillar in maintaining
ethical online interactions. Given the large amount of user-generated text,
CM systems often employ moderation algorithms [1] to combat the spread of
online toxicity. Over the past decade, much research on toxicity detection in
text data has leveraged the use of machine learning (ML) models, which have
been shown to suffer from reliability androbustness issues, wrong predictions
due to spurious lexical features [2] and biases [3]. Robustness and reliability
are cornerstones, especially in sensitive areas such as CM, where subjectivity
and context significantly influence the results [1].
As observed in a recent survey on toxicity detection [4], most research
contributions reported in this area to date have largely overlooked a key
element in decision-making: the estimation of the model’s confidence in its
prediction. In the few cases when uncertainty quantification (UQ) has been
considered [5], models have shown remarkable improvements in robustness.
UQ not onlyoptimizes theaccuracy ofpredictions, but also facilitates human
moderation. By identifying the least reliable predictions, UQ techniques
allow moderators to focus their attention on cases where the model exhibits
thehighestuncertainty, thusoptimizingtheuseoftimeandresourcesdevoted
to human review.
Although incorporating uncertainty has advanced ML models in different
disciplines, such as traffic modelling [6] or medical imaging [7], it is crucial
to recognize that tasks like toxic language detection, which are inherently
subjective, require a different perspective due to their complexity. Disagree-
ments among annotators during labelling processes often occur in content
moderation reflecting this subjectivity, which can lead to bias and, in some
cases, to the censorship of minority opinions. The quantification of inter-
annotator disagreement is thus a valuable source of information about the
complexity inherent in comments, and is directly related to the aleatoric
uncertainty present in the large databases whose inputs require moderation.
Annotation disagreement quantification, together with the epistemic un-
certainty of the modelling process, allow us to identify the cases when the
model cannot provide a robust prediction, as well as those when, due to their
complexity and subjectivity, the intervention of a human moderator is nec-
essary. Accounting for both cases is critical, as ML models trained with a
majority vote disregard the inherent ambiguity of comments. Unfortunately,
considering different annotation perspectives alongside the data modelling
2pipeline has been observed not to scale well in diverse annotation processes
[8]. Additionally, as toxic language evolves over time, it is important to
endow content moderators with the tools to make final decisions.
This manuscript builds upon the above observations. Specifically, we
propose a novel modelling framework that unifies the primary task of toxic
language classification with the auxiliary task of annotation disagreement
quantification through a multitask framework architecture. The proposed
approach is augmented with UQ for both tasks, using Inductive Conformal
Prediction (CP) for this purpose. This proposal is grounded on a previ-
ous study by Fornaciari et al. [9], which observed that unifying annotation
disagreement and classification can be beneficial for the performance of the
primary task. Finally, it aims to address a need highlighted by Gillespie [10],
who stated: “Perhaps, automated tools are best used to identify the bulk of
the cases, leaving the less obvious or more controversial identifications to hu-
man reviewers, as societies hold together not by reaching perfect consensus,
but by keeping their values under constant and legitimate reconsideration”.
Theproposedframeworkisevaluatedoveranexperimentalsetupdesigned
to answer with evidence three research questions (RQ):
•
RQ1: How does the integration of annotation disagreement prediction as
an auxiliary task influence the performance and calibration of toxicity de-
tection as the primary task? Conversely, what is the impact of the primary
task on the auxiliary task?
•
RQ2: In what ways does predicting annotation disagreement affect the
quantification and interpretation of uncertainty in the toxicity detection
task? How does the integration of the primary task influence uncertainty
quantification and interpretation in the auxiliary task?
•
RQ3: What are the benefits, if any, of incorporating an auxiliary task
into the toxicity identification process compared to treating them as sep-
arate tasks, in terms of robustness and the need for human moderation or
revision?
These three research questions aim to analyze the improvements the mul-
titask approach yields compared to simple single-task models, or, in the case
ofthefullframework, comparedtoperformingquantificationseparatelyusing
a composite of single-task models (CoM).
The rest of the manuscript is organized as follows: Section 2 provides
an overview of content moderation and CP on text data, and highlights our
3main contributions. Section 3 describes the materials and methods employed
in our experiments. In Section 4, we outline the experimental setup used to
address the RQs. Section 5 presents the experimental results and includes a
discussion of key findings. Finally, Section 6 summarizes the contributions,
key insights and limitations, and suggests avenues for future research.
2. Related Work and Contribution
In this section, we first establish the fundamentals and perform a lit-
erature review of CP (Subsection 2.1), with a focus on text classification.
Then we define content moderation and revisit frameworks proposed in the
recent literature within the crossroads of toxicity detection and uncertainty
estimation (Subsection 2.2). Finally, we end the section with a statement on
the contribution of our work beyond the literature reviewed in the previous
subsections (Subsection 2.3).
2.1. Conformal Prediction
CP is a framework pioneered by Vovk et al. [11], which can be defined as
“a practical measure of the evidence found in support of a prediction, by esti-
mating how unusual a potential example looks with respect to previous ones”
Gammerman et al. [12]. Given a desired confidence level 1 − α, conformal
algorithms are proven to always be valid, meaning that the prediction inter-
vals or sets generated by these algorithms will, on average, correctly contain
the true outcomes at least 1−α of the time. This validity is achieved with-
out requiring any specific assumptions about the data distribution, except
for the assumption of independent and identically distributed (i.i.d.) data.
Conformal predictors can be used for both classification and regression tasks,
and can be constructed from any conformity score to indicate the similarity
between a new test example and the training examples. Thanks to their
model-agnostic nature, CP techniques can be employed alongside any ML
algorithm.
Since we deal with toxicity detection, we depart from a trained classifica-
tion model (CLASS), from which we create a set of possible predicted labels
C(x) associated to input x using a small amount of data called calibration
data. To construct the prediction set in conformal prediction for classifica-
tion, we first select a conformal score s, typically derived from the softmax
output of the CLASS. We then calculate the quantile q = (n+1)(1−α)/n
b
4(with n denoting the number of calibration samples), which represents a cor-
rected estimate of the 1−α quantile for our chosen confidence level. Finally,
we generate the prediction set C(x) by including all classes for which the con-
formal score does not exceed the computed quantile, following the specified
conformal prediction method.
For the regression models (REG), the description of CP algorithms shifts
from generating a prediction set to generating an interval that guarantees
coverage of the true value in future observations based on previous ones (i.e.,
the calibration data). Similar to classification, we need to select a conformity
score and compute the corresponding quantile that ensures coverage at the
desired confidence level.
Given its robust theoretical foundation and model-agnostic applicability,
CP has been widely utilized in various domains, including object detection,
medicine, and natural language processing (NLP). According to a recent sur-
vey on the use of CP in NLP [13], tasks often tackled with CP include binary
classification, particularly for paraphrase detection, sentiment analysis, and
Boolean question answering [14]; multiclass classification for part-of-speech
(POS) tagging [15]; and multilabel classification [16]. In the case of natu-
ral language generation, CP has been used at the sentence level to provide
guarantees for Large Language Model (LLM) planners in completing tasks
[17], and at the token level to mitigate overconfidence in nucleus sampling
text decoding strategies [18]. Although CP has been predominantly applied
to classification tasks, we have not observed any instances in the literature
where it has been specifically used for toxic language detection, except for
the introductory examples provided in Angelopoulos and Bates [19]. This
lack of research is evident in the next subsection, where we explore content
moderation in further detail.
2.2. Content Moderation
CM, as defined in Grimmelmann [20], refers to the governance mecha-
nisms that structure participation in a community to facilitate cooperation
and prevent abuse. In our context, when we talk about CM, we also refer
to algorithmic CM, which the same author defines as “systems that classify
user-generated content based on either matching or prediction, leading to a
decision and governance outcome”.
Before the rise of social media, CM was primarily performed by human
moderators. However, with the surge of interactions on new platforms, this
5approach became unfeasible for humans to manage alone. As a result, com-
panies like YouTube, Twitter, and Facebook began to develop their own
moderation tools. Other initiatives also emerged, such as Google’s Perspec-
tiveAPIandOpenAI’s CMtools, tosupport companies thatdidnot havethe
resources to create their own tools. These algorithmic moderation tools were
primarily designed to automate CM, often without fully considering the role
of human moderators. Moderators were left mainly to review content that
was unjustifiably censored due to biases in these models. This was shown by
Hutchinson et al. [21] where instances of biases in Perspective API are very
clearly outlined.
Up to this point, we have discussed automatic CM, in which there is no
interaction between the system and human moderators. In contrast, collabo-
rative CM can bedefined asa framework inwhich analgorithmic moderation
toolandahumanmoderatorworktogethertoclassifyuser-generatedcontent,
each compensating for the other’s weaknesses. While moderation tools excel
at processing large volumes of data, they often lack robustness, adaptability,
and the ability to contextualize. On the other hand, human moderators can
provide broader knowledge, deeper contextual understanding, and empathy.
Building on this definition of collaborative CM and drawing from the lit-
erature review in Villate-Castillo et al. [4], it becomes evident that very little
research has been conducted in this specific area. The few studies that do
explore aspects of collaborativeCM include thosein Wang[22] andKivlichan
et al. [5]. The former study leverages user interactions alongside the knowl-
edge of content moderators to perform classification. Although it introduces
the role of moderators and their expertise, it does not address human over-
sight or governance of the moderation process. In contrast, the latter study
is the only one focusing on human oversight. It employs various UQ tech-
niques (though CP was notably excluded) to determine when collaboration
between moderators and moderation tools is necessary, particularly in cases
where the model’s predictions are uncertain.
2.3. Main Contribution
Taking into account the analysed literature, the main contribution of
this manuscript is a collaborative CM framework where the moderator is in
control of the algorithmic CM tools by deciding when the moderator’s final
decision should be incorporated. This is achieved not only by considering
the model’s confidence through UQ, but also by incorporating information
about the level of annotation disagreement the model should handle in its
6predictions. Since toxicity is a subjective task, with opinions varying across
content annotators, it is crucial to leave the final judgment to those who are
familiar with CM policies in place, as they are the ones who best understand
these policies and their context of application. Additionally, given the evolv-
ing nature of language, it is of utmost importance to have mechanisms that
are aware of this variability, especially in what refers to the ambiguity of tex-
tual toxicity. Along this line, the collaborative CM framework proposed in
this work introduces information about the annotation disagreement into the
modelling process within a multitask learning setting, enriching the confor-
malized estimation of the model’s confidence, and ultimately yielding better
andmorereliabledecisions aboutthetoxicityofthetextdatafedatitsinput.
As our experiments in Section 5 will clearly show, our multitask architecture
yields competitive levels of detection performance, a more aligned and better
calibrated estimation of the model’s confidence, and expanded information
(predicted annotation ambiguity) that allows for more informed decisions
about the moderation of potentially toxic content.
3. Materials and Methods
In this section, we first introduce the dataset used for the experiments,
alongwith themodifications madeto itsoriginal version (see Subsection 3.1).
Next, we present the methods employed (Subsection 3.2), where we provide
details on model selection, hyperparameter configuration, the computation
of annotation disagreement, and the Uncertainty Quantification (UQ) tech-
niques applied during the experiments. Lastly, we conclude by defining the
metrics used to validate our proposed research questions and the collabora-
tive Confusion Matrix (CM) framework (Subsection 3.3).
3.1. Dataset
When working with toxicity modelling datasets, one of the elements that
hasbeenrecentlycriticizedbythecommunityisthelackofinformationabout
the annotation process, such as the number of annotators, their background,
the total number of annotations, and the diversity of the annotators [23].
Since disagreement calculation is a central focus of our implementation, it
is essential to have a dataset that includes sufficiently diverse opinions and
numerous annotators per comment to yield an heterogeneous distribution of
annotations.
7As exposed in Villate-Castillo et al. [4], only two datasets currently meet
these requirements: the Jigsaw Unintended Bias in Toxicity Classification
dataset[24], towhichtheJigsawSpecializedRaterPoolsdataset[25]wasalso
added, as the latter included a portion of this dataset with a higher number
of annotations and greater diversity; and Wikipedia Detox (WikiDetox) [26].
However, Wikipedia Detox is not considered in our study because it does not
provide the percentage of people who agreed that a given comment is toxic.
Furthermore, this latter dataset contains a majority of comments with a low
level of disagreement, which may lead to the detection model being overfit
to this data subset, rendering it undesirable. Consequently, the model can
struggle to learn the lexical features associated with comments that elicit
higher levels of disagreement due to its limited exposure to examples from
that fraction of the data.
The Jigsaw Unintended Bias in Toxicity Classification dataset, created
as part of a challenge organized by Google Jigsaw, provides a high-quality
dataset that is already divided into training, validation, and test sets. This
predefinedpartitioningfacilitatesthereproducibilityofourexperiments. Ad-
ditionally, compared to other datasets in the toxicity domain [4], this dataset
features a substantially larger group of annotators (8,899), as opposed to the
smaller groups of tens typically found in other datasets. This large number
of annotators is a significant advantage, as it provides a more diverse range
of opinions, not only in terms of numbers, but also in terms of background,
ethnicity, gender, and age. However, during the dataset preprocessing phase,
the following caveats are found:
•
The dataset contains comments, each annotated by between 3 and several
hundred annotators. Since we are targeting a diverse dataset in terms of
opinions, we select only the comments that have at least 10 annotators.
This yields 315,685 comments in the training set, 38,855 in the test set,
and 33,611 in the validation set, out of a total of approximately 2 million
comments.
•
Subsection 3.2.2 shows that annotation disagreement is measured on a
scale from 0 to 1, where 0 indicates no disagreement and 1 indicates high
disagreement. When applying the formulas explained in this section to the
chosen dataset, we observe that the dataset is imbalanced in terms of the
levelofdisagreement. Themajorityofcommentshaveadisagreement score
higher than 0.4, with most falling between 0.4 and 0.8. Despite this im-
balance, the dataset includes instances of medium and high disagreement,
8unlike WikiDetox, which primarily contains disagreement scores between
0 and 0.1.
•
Theoriginaldatasetwasimbalanced, with92.07%non-toxiccommentsand
7.93% toxic comments. However, by selecting only comments with at least
10 annotators, the imbalance is reduced to 61.48% non-toxic comments
and 38.52% toxic comments.
Besides adjusting thedataset size tosuit thepurposeof thestudy, we per-
form thorough data cleaning. This process involves removing HTML tags,
emojis, accented characters, IP addresses, and contractions. We also deob-
fuscate swear words to make the comments more easily understandable.
3.2. Methods
In this section we introduce the notation, selected model architecture,
learning objectives, and hyperparameters (Subsection 3.2.1), whereas Sub-
section 3.2.2 discusses various methods from the literature for computing
disagreement and explain our selected approach. Finally, Subsection 3.2.3
presents the CP algorithms considered in our experiments.
3.2.1. Notation, Model Selection, Learning Objectives and Hyperparameters
The proposed CM framework is defined as a multitask learning (MTL)
task on an annotated dataset D = {X,A,y,d}, where X is a set of text
instances, A is the set of annotations, y is the set of toxic labels, and d is
the annotation disagreement value. Each entry y ∈ {0,1} represents the
i
label assigned to x ∈ X based on the majority vote of the annotators in
i
a ∈ A; the label that receives the most votes is selected for each instance.
i
Meanwhile, d ∈ R [0,1] represents the computed annotation disagreement
i
for the set of annotations given to the text instance x .
i
As depicted in Figure 1, the MTL CM process consists of a primary
task (toxicity classification of the given comment x ) and an auxiliary task
i
(estimation of the annotation disagreement). The architecture is based on
DistillBERT [27], which serves to extract the embedding from the
baseuncased
CLS token (a sentence-level representation for classification) of the given
comment x to perform the MTL task. The generated predictions for each
i
task are defined as y from the primary task and d (x ) from the auxiliary
i i i
task. Wethengeneratethecorresponding predictionset C(x )andconfidence
i
interval I(x ) using the inductive CP techniques latber detailed in Subsection
i b
2.1 and a calibration subset D . Finally, the need for a human moderator’s
cal
9Figure 1: Review process. The STL review process begins by extracting the represen-
tation CLS(xi) of the content xi and generating its corresponding classification yi. After
calibratingtheCPalgorithm,wegeneratethepredictionsetC(xi)forclassification. Ifthe
sizeofC(xi)is2(i.e.,thetotalnumberofpossibleclassesintheproblem),thecom bmentis
flaggedfor review by a human moderator. Otherwise, the classifier’s output is considered
confident, and yi is deemed a reliable prediction of the toxicity of xi. For the composite
STL models, in addition to generating the classification yi, we also compute the annota-
tion disagreemebnt estimate di(xi), a regression value learned from X and A. Once the
CPalgorithmfor regressioniscalibrated,weproducethe bconfidence intervalI(xi). Ifthis
interval exceeds a predefinedbambiguity threshold γ, the comment is marked for human
review. Otherwise, we rely on C(xi) to assess the confidence of the classifier’s output.
In the MTL approach, the only difference from the composite model is that both yi and
di(xi) are generated from the same representation CLS(xi) of the content.
b
b
10revision is determined by whether prediction set length of C(x ) is 2 (namely,
i
the number of possible classes in our problem) or if the predicted confidence
interval I(x ) is larger than an ambiguity threshold γ. This criterion allows
i
consideringthemodel’sinherent uncertaintyinthepredictionsoftheprimary
and auxiliary tasks, as well as the ambiguity of the comments.
In the classification task (primary), we address class imbalance, as noted
inSubsection3.1, byutilizing theso-calledfocal loss [28]initsweighted form.
This loss function has been empirically shown to enhance predictive perfor-
mance and improve uncertainty calibration in class-imbalanced datasets. For
generating the final classification outcome, we employ the Sigmoid activation
function, and classify a comment as toxic if the activation output is equal to
or above 0.5.
Fortheauxiliaryregressiontask, weexplorevariousapproaches, including
Binary Cross-Entropy (BCE) and Mean Squared Error (MSE) regression
losses, as well as treating the regression problem as a classification task. For
BCE and MSE, we apply a sigmoid activation function, as the output values
are ranged between 0 and 1. When using the regression-as-classification
(RAC) approach, we divide the prediction space into 20 equidistant intervals.
This method is supported by previous research in Guha et al. [29], which
shows that RAC produces confidence intervals that more accurately reflect
the true data distribution.
In line with the recommendations from this study, we utilize their intro-
duced loss function, R2CPP, setting ψ = 0.5 and τ = 0.1. These parameters
are maintained constant throughout the experiments discussed later in this
paper. Additionally, for all regression loss functions, we implement their
weighted versions, applying weights in bins of 0.1 to account for the data
imbalance related to annotation disagreement noted in Subsection 3.1.
Lastly, we introduce single-task learning (STL) models as baselines for
both the classification and regression tasks: the single-task regression model
(STL REG) and the single-task classification model (STL CLASS). These
models are based on the same loss function and hyperparameters used in the
MTL framework, aimed to effectively address the research questions intro-
duced in Section 1. For RQ1 and RQ2, we utilize the STL models indepen-
dently to compare the performance, calibration, and uncertainty estimation
benefits of our MTL approach for both the primary and auxiliary tasks. Re-
garding RQ3, we employ CoM for STL REG and STL CLASS during the
content review process to assess the actual benefits of the MTL approach in
the context of collaborative CM.
113.2.2. Computation of the Annotation Disagreement
Annotation disagreement can be computed in different ways, e.g. as the
percentage of people in disagreement with the majority vote as in Wan et al.
[30]; by predicting the 5-point scale score as proposed by Ramponi and
Leonardelli [31]; or by using the concept of soft labels [9], i.e., the proba-
bility of a comment to be in the positive class. In addition to these methods,
we also consider the possibility of using the entropy of the annotations to
quantify disagreement. Specifically, given the mean of the annotation labels
a (which in our case can be regarded as the probability that x is annotated
i i
to belong to the toxic class), the entropy serves as a measure of uncertainty
or unpredictability in the annotation process. This measure is given by:
d(x ) = −a log (a )−(1−a )log (1−a ), (1)
i i 2 i i 2 i
where a is the proportion of annotators classifying content x as toxic.
i i
In our case we propose an alternative method to compute the annotation
disagreement, which draws inspiration from a combination of the works in
Wan et al. [30] and Fornaciari et al. [9]. Specifically, the annotation dis-
agreement is given by the distance between the mean of the annotations a
i
(assuming that a = 1 represents toxic and a = 0 represent non-toxic) and
i i
0.5, which represents the value of maximum disagreement. The resulting
value, ranging from 0 to 0.5, is linearly scaled to the range R [0,1] and then
inverted to yield the sought measure of disagreement. Mathematically:
d(x ) = 1−2·|a −0.5|, (2)
i i
where d(x ) represents the annotation disagreement score for x , and a is
i i i
the mean of the annotations associated to x .
i
This metric, the distance based annotation disagreement score, is priori-
tized over entropy because the latter produces values that are too similar in
cases of high uncertainty, and moreover requires a more complex conversion
to calculate the percentage of individuals in disagreement. Regarding the
scale-based method introduced by Ramponi and Leonardelli [31], we were
unable to use their proposed technique because the selected dataset lacks
the required 5-point scale score for each comment.
3.2.3. Conformal Algorithms under Consideration
For UQ in classification and regression models, our framework incorpo-
rates inductive CP algorithms due to their ability to offer guaranteed cov-
erage, their model-agnostic nature, the fact that they do not require any
12changes to the model to which they are applied, and because they do not in-
crease latency during inference. To properly validate the advantages offered
by using the proposed MTL framework w.r.t. STL baselines, a wide variety
of UQ techniques have been selected. For all CP methods, we set α = 0.1 to
ensure a guaranteed coverage of 90%.
For classification tasks, the following UQ methods are considered:
•
Least Ambiguous set-valued Classifier (LAC) [32]: In LAC, the conformity
score function is defined as one minus the model’s softmax probability
p(y ;x ) associated to the true label y , which is computed for each point
i i i
in the calibration set D . Once the conformity scores {s , ..., s } are
cal 1 n
˙
computedforthencalibrationinstances, wecalculatetheir(n+1)(1−α)/n
quantileq. Finally, tocomputetheuncertaintyassociatedtothesamplesin
thepredictionset, foreachtestinstancex weextractthemodel’spredicted
i
class probabilities p(y;x ) for each possible class label y. All classes with
b i
a probability score lower than or equal to the estimated quantile q, i.e.:
C(x ) = {y ∈ {1,...,C} : p(y;x ) ≥ 1−q}, (3)
i i b
where C denotes the number of classes in the task under consideration.
b
•
Class-Conditional LAC (CCLAC) [19]: Inthiscasewecomputetheconfor-
mity score s(x,y) for each sample x and class y as one minus the softmax
probability assigned by the model to the class, either toxic or non-toxic.
The quantile is then computed for each class, yielding a quantile value q
y
for each class y (toxic and not toxic). Finally, during inference we iterate
through the classes and include them in the prediction set associated to
b
query instance x if their conformity score s(x ,y) is below the correspond-
i i
ing quantile q , namely:
y
C(x ) = {y ∈ {1,...,C} : p(y;x ) ≤ q }, (4)
b i i y
where, as in LAC, p(y;x ) denotes the softmax probability output by the
i b
model for class y and input instance x .
i
•
Conformal Risk Control (CRC) [33]: It extends the conformal prediction
guarantees to any bounded loss function that decreases as |C(x )| (i.e. the
n
cardinality of the predicted subset of labels) grows. CRC finds a threshold
value λ that controls the fraction of missed classes, generating a prediction
set C (x ) that depends on the selected λ. In our case, we consider a loss
λ i
13function based on the false negative rate (FNR), which, when computed
over the calibration set D , is given by:
cal
n
|y ∩C (x )|
LFNR(λ) = 1− i λ i , (5)
n
i=1
X
where C (x ) = {y ∈ {1,...,C} : p(y;x ) ≥ λ} is the prediction set
λ i i
given the threshold λ. This choice ensures that the FNR during inference
(namely, when i > n) is controlled, i.e. E[LFNR(λ)] ≤ α.
Whenitcomestotheestimationofuncertaintyfortheauxiliaryregression
task (annotation disagreement), the following UQ methods are considered:
•
Absolute Residual Conformity Score (AR) [34]: The conformity score is
computed as the residual error between the true value and the predicted
value |d −d (x )|. Then, the quantile of the conformity scores is computed
i i i
and used to determine the prediction bounds:
b
I(x ) = d (x )−q,d (x )+q . (6)
i i i i i
h i
• Gamma Conformity Score (G)b [34]: Thbeb Gscore bnormalizes the residuals
by the predictions as |d −d (x )|/d (x ). As before, we first compute the
i i i i i
quantile of the distribution of normalized conformity scores computed over
the calibration set, which isbthen usbed to determine the prediction bounds
as:
I(x ) = d (x )·(1−q),d (x )·(1+q) . (7)
i i i i i
h i
•
Residual Normalized Conbformity Score (bRN) [35]: It is similar to the
b b
Gscore. However, instead of dividing by the model’s own predictions, this
conformal predictor resorts to an external model (K Nearest Neighbors re-
.
gressor) to predict the REG model residuals σ(x ) = d −d (x ). As before,
i i i i
the quantile is computed from the conformity scores, which is then used
to determine the prediction bounds: b
b
I(x ) = d (x )−q ·σ(x ),d (x )+q ·σ(x ) . (8)
i i i i i i i
h i
• Regression as Classificatb ion (R2bCCbP) [29b ]: It is abvebry recent CP method
that converts a regression problem into a classification problem, allowing
14CP methods devised for classification tasks to elicit conformal intervals for
regression. It introduces a new loss function, to preserve the ordering of
the continuous alphabet of the target variable. First, the continuous target
variable is divided into discrete bins b ∈ {1,...,B}. After binning, each
instance in the dataset is assigned a class label corresponding to the bin it
belongs to. R2CCP’s loss can be defined as:
LR2CCP(x i) = |d(x i)−d(b)|ψ ·p(d(b);x i)−τ ·H(p(d(x i);x i)), (9)
where d(b) denotes thecenter ofthebinb ∈ {1,...,B}; |d(x )−d(b)|repre-
i
sentsthedistancebetweentherealdisagreement valueandthecenterofbin
b; ψ > 0 is a hyperparameter; p(d(b);x ) is the probability of the comment
i
x belonging to bin b; τ > 0 is a regularization term; and H(p(d(x );x )) is
i i i
the Shannon entropy of the discrete probability distribution over the bins
of the input x .
i
To compute the quantile q, R2CCP uses a linear interpolation of the soft-
max probabilities as the conformity score s(x,d(b)). Finally, the final
prediction interval for the continuous variable d is the union of the bins
b i
within the predicted conformal set of bins, i.e.:
I(x ) = [d(min{b ∈ {1,...,B} : s(x ,d(b)) ≥ q}) ,
i i
d(max{b ∈ {1,...,B} : s(x ,d(b)) ≥ q})]. (10)
i
b
3.3. Performance Metrics
b
Traditional CM systems have been evaluated using performance metrics
usually considered in Machine Learning, including accuracy, F1 score, and
AUC (Area under the ROC Curve). However, these metrics only capture
model performance and do not account for the efficiency of models in a
collaborative environment with human moderators.
To overcome this, we henceforth define collaboration efficiency as the
model’s ability to express its own limitations. In Kivlichan et al. [5], this gap
inevaluationmetrics isaddressed byintroducing Review Efficiency, which we
rename in our study as Model Uncertainty-aware Review Efficiency (MURE)
to distinguish between review efficiency based on model uncertainty and an-
notation disagreement. MURE quantifies the proportion of examples deliv-
ered to a moderator in situations where the model’s toxicity classification
would otherwise be incorrect. In simpler terms, it measures how often com-
ments flagged as uncertain by CP methods correspond to cases where the
model would have made an incorrect prediction.
15We mathematically define this metric in the context of a classification
task comprising C classes, such that |C(x i)| > 1 denotes the case when the
classifier is not certain about its prediction. We define TP as the number
UQ
of instances where the model correctly delivers the content at its output to
the moderator based on its estimated uncertainty, i.e. when y 6= y and
i i
|C(x )| > 1. Conversely, FP refers to the number of examples where
i UQ
the model incorrectly sends the content for review based on their estimated
b
uncertainty despite its toxicity prediction being accurate, corr. y = y and
i i
|C(x )| > 1. As a result, MURE can be computed as:
i
b
TP
UQ
MURE = . (11)
TP +FP
UQ UQ
Likewise, to account for the efficiency in predicting when a comment
is annotated ambiguously, we propose a new metric coined as Comment
Ambiguity-aware Review Efficiency (CARE). CARE evaluates the ability
to identify all ambiguous comments, based on a threshold γ selected by the
moderator. Let TP be the number of comments that are truly ambiguous
⇔
asperγ (i.e., d(x ) ≥ γ) andwhich arecorrectly predicted asambiguous (i.e.,
i
I(x ) ≥ γ). Likewise, we define FN as the number of comments which are
i ⇔
truly ambiguous but are not predicted as such, i.e., d(x ) ≥ γ but I(x ) < γ.
i i
Based on these definitions, CARE is given by:
TP
⇔
CARE = , (12)
TP +FN
⇔ ⇔
namely, the True Positive Rate (TPR) associated to the conformalized an-
notation disagreement estimate resulting from the auxiliary modeling task.
In addition to CARE and MURE, we also consider Review F1 Score (R-
F1), which measures how well the framework manages model errors due to
both model’s uncertainty and comment ambiguity. R-F1 combines precision
(MURE)andrecall(CARE)toevaluatethesystem’seffectiveness inhandling
these errors, providing a balance between the two:
MURE·CARE
R-F1 = 2· . (13)
MURE+CARE
Furthermore, weintroducethepoint-biserialcorrelationr tocapturethe
pb
correlationbetweentheauxiliarytaskflaggingacommentasambiguousgiven
the threshold γ and the level of annotation disagreement for that comment,
16as well as the correlation between the UQ method flagging the primary task
prediction as uncertain and the annotation disagreement. As mentioned
before, a comment can be deemed ambiguous if I(x ) ≥ γ and the primary
i
task prediction can be deemed uncertain if |C(x )| > 1. The point-biserial
i
correlation provides us with a score ranging from -1 to 1, where -1 indicates
a negative correlation, 1 indicates a positive correlation, and 0 indicates
no correlation. This score allows us to analyse the correlation between a
binary variable—whether the comment is ambiguous or the prediction is
uncertain—anda continuous variable, theannotationdisagreement. Keeping
this in mind, the point-biserial correlation r can be expressed as:
pb
d (x)−d (x) |d (x)|·|d (x)|
0 1 1 0
r = (14)
pb σ(d(x))
s
|d(x)|2
where d (x) is the group of comments with their corresponding true an-
1
notation disagreement values that are ambiguous or for which the model is
uncertain in each prediction, and d (x) is the group of comments that are not
0
ambiguous and for which the model does not show uncertainty, along with
their corresponding true annotation disagreement values. |d (x)| denotes the
1
number of elements in the ambiguous or uncertain group. σ(d(x)) represents
the standard deviation of the annotation disagreement, and |d(x)|2 denotes
the total number of disagreement scores elevated to the power of 2.
The metrics presented so far measure how well the framework performs
in a CM context. However, we have also introduced additional metrics to
support the analysis of each research question. In RQ1, where we analyze the
general performance and calibration of the models, we use F1 score, logistic
loss, Expected Calibration Error (ECE), and Adaptive Calibration Error
(ACE) [36] for classification, and Mean Absolute Error (MAE) and Mean
Squared Error (MSE) for regression. For RQ2, where we measure model
uncertainty in classification and regression, the following metrics are used:
•
Classification: In this case we consider Marginal Coverage, MURE, cer-
tainty F1 score, and correlation. Marginal Coverage represents the proba-
bility that the prediction set contains the true label, which should ideally
approach1−α. The certainty F1scoreistheF1scorecalculatedfortheset
of comments where the CP method is certain, i.e., |C(x )| = 1. Correlation
i
refers to the point-biserial correlationbetween the model’s uncertainty and
annotation disagreement.
17•
Regression: In this second case we report mean interval size, Interval Cov-
erage Probability (ICP), distance to interval (DI), and correlation. ICP is
the probability that the true prediction lies within the predicted interval,
which should also approach 1−α. DImeasures the mean distance fromthe
interval when the true value d falls outside the estimated confidence in-
i
terval I(x ). Correlation, in this case, refers to the Pearson correlation r
i I,d
between the size of the estimated interval I(x ) and the true annotation
i
disagreement d(x ).
i
Table 1 summarizes the metrics used to evaluate the classification and
regression tasks considered in our framework when addressing each of the
formulated RQs.
Task RQ1 RQ2 RQ3
Marginal Coverage,
Content toxicity F1 score, MURE,
detection Logistic loss, prediction length, R-F1
(primary, classification) ECE, ACE certainty F1 score,
r
pb
Annotation ambiguity
Mean interval size,
estimation MAE, MSE R-F1, CARE, r
ICP, DI, r I,d
(auxiliary, regression) I,d
Table 1: Summary of the metrics considered for each research question RQ.
4. Experimental Setup
In this section, we describe the experiments conducted to address the re-
search questions outlined in Section 1. Section 4.1 and 4.2 detail the exper-
imental setup for RQ1 and RQ2, respectively. Finally, Section 4.3 describes
the experimental setup for RQ3.
For all experiments the same hyperparameter setting is used, and fixed
seeds are set for the sake of reproducibility. As introduced in Section3.2, we
use the DistillBERT model, which was fine-tuned on a H100 GPU
baseuncased
with a batch size of 32, a maximum sequence length of 512, a learning rate
equal to 2 ·
10−5,
a weight decay of 0.01, and a learning rate warm-up of
1 epoch. AdamW is used as the optimizer, and the model is trained until
no improvement is observed in the loss metric, with a patience parameter
of 2 epochs. All scripts and log files used to produce the results discussed
18in the paper are publicly available in https://github.com/TheMrguiller/
Collaborative-Content-Moderation.
4.1. RQ1: Impact of Annotation Disagreement Prediction on Primary Task
Performance and Calibration
The STL CLASS model, depicted in the upper section of Figure 1, is
trained using focal loss on hard labels. In contrast, the MTL approach, illus-
trated in the lower section of the same figure, applies the same loss function
but incorporates an auxiliary task. To evaluate potential improvements, we
compare the STL CLASS across various regression settings, which include
BCE, MSE and RAC, as detailed in Subsection 3.2. Given that previous
research observed improvements in classification [9], we hypothesize that a
similar effect might occur with the regression auxiliary task. To verify this,
we compare each multitask model with the STL REG using the correspond-
ingregression loss. WeuseMAE andMSE astheprimarymetrics toevaluate
improvements.
These comprehensive assessments are crucial for addressing subsequent
research questions related to model uncertainty, particularly because calibra-
tionplays asignificant roleinCP methods. Aproper calibrationensures that
the estimated prediction intervals or conformal sets provided by CP methods
accurately reflect the true uncertainty of the model’s predictions.
4.2. RQ2: Effects of Annotation Disagreement Integration on Uncertainty
Quantification
Since proper model calibration is closely related to accurate uncertainty
quantification (UQ) provided by CP methods, we hypothesize that multitask
settings could improve model uncertainty estimation for both STL classifica-
tion (CLASS) and regression (REG). To quantify improvements in the pri-
mary task (STL CLASS), we focus specifically on the MURE metric, which
assesses how well the uncertainty method predicts cases where the model
is incorrect. In addition to this metric, we also consider measures com-
monly used to evaluate uncertainty estimation methods, including marginal
coverage, the correlation between uncertainty and annotation disagreement
scores r (which refers to the Pearson correlation between the estimated
I,d
annotation disagreement based on the confidence interval and the true an-
notation disagreement), and the certainty F1 score. To compare uncertainty
improvements, we use the same classification and multitask models described
in Section 4.1, calibrated using the same validation/calibration dataset and
19evaluated on the same test set. Each model is calibrated with the three
uncertainty estimation methods introduced in Section 3.2: LAC, Class Con-
ditional LAC, and CRC.
For the auxiliary regression task, a similar approach is used, with the
uncertainty metrics focusing on general regression uncertainty quantification
metrics, such as interval size, ICP, correlation between interval size and an-
notation disagreement scores, and DI. Regarding UQ techniques, different
methods are employed depending on the type of regression loss. For models
based on MSE and BCE regression losses, we use AR Score, G Score, and
RN Score. Lastly, for RAC, the previous methods are applied together with
R2CCP.
Theseexperimentsarecrucialforassessingtheimpactofinaccuratemodel
uncertainty quantification oncollaborativeCM. Misestimation ofuncertainty
could increase the moderators’ workload. For instance, comments that are
relatively easy to classify, such as those containing slurs or swear words [37],
might be unnecessarily submitted to them for revision.
4.3. RQ3: Benefits of an Auxiliary Task versus Independent Models in Col-
laborative Content Moderation
Previous studies have assessed the efficiency of moderators and modera-
tion systems using review efficiency (MURE) [5], which considers only model
performance while disregarding label data quality and text ambiguity. To
address these factors, our moderation framework includes an auxiliary task
designed to measure this concept. Since this is a novel approach, we intro-
duce a new metric, CARE, to account for the system’s capacity to detect
ambiguous examples, as explained in Section 3.3. To determine if a com-
ment is ambiguous, a threshold γ must be selected that aligns with the
desired maximum level of annotation ambiguity. Given that the quantity of
uncertain examples varies depending on each model’s capabilities, which in
turn affects the CARE metric, we used the False Positive Rate (FPR) at a
given TPR/MURE value, a metric commonly used in other areas including
Out-of-Distribution (OoD) detection [38]. Different TPR values are applied
for RAC models due to the interval size, which affects the quality of the
ambiguity estimation.
Experiments for RQ3 consider the following models:
•
STL CLASS(upper section of Figure1): Inthiscase, classification model’s
uncertainty is used to measure how well the model performs in detecting
20ambiguous terms, as uncertainty is sometimes associated with hard exam-
ples—instances that the model has not learned well. Since the model is
not calibrated for the task, we could not compare it under the same true
positive rate (TPR) because it is only able to achieve a maximum TPR of
around 45%.
•
CoM (middle section of Figure 1): To analyse whether there is any addi-
tional advantage compared to those examined in RQ1 and RQ2, we use
STL CLASS and REG jointly to provide classification and regression un-
certainty values. The same procedure employed in the multitask setting is
applied to compute the ambiguity of the comment.
•
MTL (lower section of Figure 1): We use the auxiliary task’s confidence
interval obtained from RQ2 to quantify the ambiguity of comments based
on the threshold derived from computing the FPR at TPR. To determine
if a comment is ambiguous, we check whether the upper confidence interval
exceeds the selected threshold.
In addition to computing the CARE metric, we also calculate the Review
F1 score. Finally, as in previous experiments, we compute the correlation
between the ambiguity score predicted by the model and the true annotation
disagreement.
5. Results and Discussion
In this section we present and analyse the results for each research ques-
tion. In Section 5.1 we discuss our findings for RQ1, highlighting the benefits
of our approach in terms of model calibration and performance. Section 5.2
focuses on the advantages of incorporating the auxiliary task for improving
uncertainty estimation in the primary classification task. In Section 5.3 we
compare the multitask and composite approaches for content moderation,
with a focus on comment annotation disagreement. For each RQ, we begin
by summarizing the key performance metrics and findings.
5.1. RQ1: Impact of Annotation Disagreement Prediction on Primary Task
Performance and Calibration
Following the experimental setup presented in Section 4.1, Table 2 sum-
marizes the performance results for the primary task (toxicity detection) of
the STL CLASS approach and the MTL approach with different regression
21settings. By examining the F1 score for the classification task, it is evident
that the introduction of the auxiliary task improves the toxicity detection
performance, especially in the cases of MSE and BCE, and to a lesser ex-
tent, RAC. In terms of calibration, the BCE and MSE MTL models exhibit
better calibration and, consequently, achieve higher F1 scores compared to
their STL CLASS counterpart. However, for RAC MTL, we do not observe
any significant improvement in calibration as per the ACE and logistic loss
values when compared to the STL CLASS approach, despite RAC providing
more information about the distribution of annotation disagreement scores
(which closely relates to the toxicity score).
Model name F1 score ↑ ECE ↓ ACE ↓ Log loss ↓
STL CLASS 71.61 0.0960
1.039×10−4
0.6407
MTL + BCE 72.15 0.12 1.013×10−4 0.6326
MTL + MSE 72.48 0.12 1.002×10−4 0.6372
MTL + RAC 71.89 0.0726
1.058×10−4
0.6570
Table 2: Comparison of detection performance and calibration between STL CLASS and
MTL models for toxicity classification. The best model for each metric is in bold, and
models with worse metrics than the baseline are underlined.
Surprisingly, ECE does not show consistency with other calibration met-
rics, which supports the findings of Nixon et al. [36], who observed that ECE
is not a reliable metric.
In the case of STL for the regression task (STL REG), Table 3 exposes
a clear improvement in all cases, as indicated by the MAE and MSE met-
rics. The largest improvement compared to the STL REG model occurs in
RAC, where the classification task further supports the regression task. The
converse does not hold for the classification task.
In summary, we observe a clear improvement in the classification task for
the MSE and BCE multitask versions compared to the STL CLASS, with
RAC version showing a poorer calibration. In STL REG, there is a more
consistent overall improvement for the MSE and MAE metrics. Although
the improvements in classification are not substantial, our results are in line
with the findings of previous studies [9, 39].
22MSE ↓ MAE ↓
Model name
STL REG/ MTL STL REG/ MTL
BCE 0.0461/0.0444 0.1694/0.1662
MSE 0.0460/0.0458 0.1693/0.1688
RAC 0.0673/0.0521 0.2075/0.1831
Table 3: Comparison of performance between STL REG and MTL models for regression
tasks. The best model for eachmetric is in bold, and models with worsemetrics than the
baseline are underlined.
5.2. RQ2: Effects of Annotation Disagreement Integration on Uncertainty
Quantification
As described in the experimental setup corresponding to RQ2, we now
focus on analysing whether there is any clear improvement in uncertainty
quantification for the regression and classification tasks by including an aux-
iliary task in annotation disagreement prediction.
The results for the classification uncertainty are divided based on the
conformal prediction methods in use:
•
LAC: Table 4 presents a comparison between the STL CLASS and its
MTL counterparts. For the MURE metric, all MTL models show a clear
improvement over the STL CLASS, with the most substantial gains ob-
served in the RAC regression-based models. The F1 score also exhibits
a noticeable improvement across all MTL baselines. Furthermore, the
higher point-biserial correlation r achieved by the MTL models indicate
pb
a stronger alignment with the annotation disagreement, and thus a more
effective contextualization of uncertainty.
•
Class Conditional LAC: In Table 5, we compare the STL CLASS models
with the MTL architecture for the Class Conditional LAC uncertainty
estimation method. Similar to previous cases, we observe improvements
across all metrics compared to STL CLASS, except for the F1 score in the
case of R2CCP’s regression loss, which performs slightly worse than the
STL CLASS model.
•
CRC: Table 6 reports the results of a comparison between STL CLASS
with the MTL architecture for CRC. The results here are consistent with
those of LAC, leading to similar conclusions.
23When it comes to the results of the regression uncertainty quantifica-
tion, we have divided the analysis into the STL REG models and regression
uncertainty quantification methods:
•
BCE: In Table 7, we observe that the addition of a classification head
leads to better results in terms of interval size, while maintaining the same
distance to interval values across all uncertainty methods. However, we
generally see worse results in terms of r . The method with the best
I,d
r value is G (Gamma Conformity Score, as per Section 3.2.3), which
I,d
generates intervals that are more aligned with the inherent annotation
complexity of the comments.
•
MSE: InTable7 we noteslightly worse results interms ofinterval size than
in the previous case. However, we obtain smaller values for the distance
to the interval, likely due to the interval size being wider than in the BCE
case. In this case, the r metric shows marginally better results.
I,d
•
RAC: InTable7, we findthatresults areconsistent withthose encountered
in BCE. In terms of interval size, there is an overall improvement over
those corresponding to STL REG. For the distance to interval metric,
we observe smaller values in all cases except for the R2CCP uncertainty
quantification method. Interms ofr , improvements areobserved, except
I,d
for theAR uncertainty method, where theSTL REGmodel improves upon
the multitask model.
In summary, we observe improvements in classification across all uncer-
tainty estimation methods compared to the STL models. The model shows
betteralignmentbetweenuncertaintyquantificationandannotationdisagree-
ments, indicating improved handling of difficult examples. However, in re-
gression, the r metric lacks consistent improvement, depending on the un-
I,d
certainty method: AR is less aligned, while RN, G, and RC2CPP are more
aligned. This may be due to AR providing static intervals, while the others
adjust for each input.
Forintervalsizeanddistance, RACandBCElossesoutperformSTLREG
models, whileMSEyields inconsistent results. Wepositthattheeffectiveness
of the auxiliary task relies on the chosen regression loss, with less pronounced
improvements inregressionduetoitssensitivitytoannotationdisagreements.
24Model name MURE ↑ F1 score ↑ Marginal Coverage r ↑
pb
CLASS 0.4076 0.8046 0.901 0.2767
MTL + BCE 0.4084 0.8068 0.90 0.2830
MTL + MSE 0.4105 0.8123 0.898 0.2775
MTL + RAC 0.4196 0.8094 0.898 0.2770
Table 4: Comparisonof LAC uncertainty method performance between STL CLASS and
MTL models for classification tasks. The best model for each metric is in bold, while
models with worse metrics than the baseline are underlined.
Model name MURE ↑ F1 score ↑ Marginal Coverage r ↑
pb
STL CLASS 0.3528 0.8286 0.90 0.2424
MTL + BCE 0.3532 0.8304 0.8994 0.2484
MTL + MSE 0.3635 0.8294 0.8987 0.2495
MTL + RAC 0.3650 0.8251 0.8969 0.2476
Table5: ComparisonofClassConditionalLAC uncertaintymethodperformancebetween
STL CLASS and MTL models for classification tasks. The best model for each metric is
in bold, and models with worse metrics than the baseline are underlined.
5.3. RQ3: Benefits of an Auxiliary Task Versus Independent Models in Col-
laborative Content Moderation
In this section, we present the main results of our collaborative content
moderation framework, focusing on the outcomes of our CARE metric, F1
Review Efficiency, and Pearson correlation r . The results are categorized
I,d
by regression loss, comparing the MTL approach against the CoM approach,
where STL REG and STL CLASS work together to make a final decision.
•
MTL MSE: In Table 8, we observe a clear improvement in F1 Review Effi-
ciency and point-biserial correlation. Additionally, the FPR shows overall
improvement, except for the CCLAC using the AR uncertainty method.
•
MTL BCE: In Table 8, the BCE version shows clear improvement across
all metrics, except for the CCLAC using the RN uncertainty method.
•
MTL RAC: In Table 8, we observe better F1 Review Efficiency compared
to the CoM models, primarily due to the improvement in the MURE met-
ric achieved by incorporating the auxiliary task. For FPR, improvements
25Model name MURE ↑ F1 score ↑ Marginal Coverage r ↑
pb
STL CLASS 0.4078 0.8045 0.901 0.2763
MTL + BCE 0.4084 0.806 0.8981 0.2830
MTL + MSE 0.4105 0.8123 0.8989 0.2775
MTL + RAC 0.4196 0.8094 0.8986 0.2770
Table 6: Comparisonof CRC uncertainty method performance between STL CLASS and
MTL models for classification tasks. The best model for each metric is in bold, while
models with worse metrics than the baseline are underlined.
are seen only when using the R2CCP and RN uncertainty methods, while
the other cases show worse results. Regarding r , we see overall improve-
I,d
ments, except when using the R2CCP uncertainty method.
To sum up, the comparison shows that using a multitask approach in
our proposed moderation framework leads to improvements in F1 Review
Efficiency, CARE,andr , particularlyfortheMSEandBCEmodels. While
I,d
there are instances where the CoM version performs better, such as with the
RAC model, the overall superiority of MTL model – demonstrated by higher
F1 Review Efficiency and greater computational efficiency – outweighs these
exceptions.
Finally, in Table 9 we can see that the STL CLASS approach can also de-
tect some instances of ambiguous comments as part of its uncertainty quan-
tification process. Although its ability is limited, since it is not explicitly
trained to predict annotation disagreement, the method still offers valuable
insights into ambiguous cases.
6. Conclusions and Future Work
In this work, we introduced a novel framework for collaborative content
moderation that accounts for model uncertainty and addresses annotation
disagreement through the introduction of an auxiliary task. This approach
excels at capturing the inherent subjectivity of toxic comments, providing
a moderation scheme that reflects this intrinsic complexity. Additionally,
we proposed two new metrics to evaluate the comment review process: F1
Review Efficiency and CARE. These metrics incorporate the concept of an-
notation disagreement into the review process, which the SOTA MURE does
not consider.
26Model name Method CP Interval size ↓ DI ↓ ICP r ↑
I,d
STL REG BCE AR 0.6437 0.0096 0.8994 -0.0320
MTL BCE 0.6339 0.0094 0.8997 -0.0910
STL REG BCE G 0.7198 0.0123 0.8968 0.4963
MTL BCE 0.7018 0.0114 0.8979 0.5041
STL REG BCE RN 0.7347 0.0113 0.8977 0.0065
MTL BCE 0.7276 0.0113 0.8984 -0.0082
STL REG MSE AR 0.6424 0.0096 0.8990 -0.0357
MTL MSE 0.6408 0.0098 0.8996 -0.0600
STL REG MSE G 0.7166 0.0123 0.8964 0.4959
MTL MSE 0.7246 0.0124 0.8985 0.5047
STL REG MSE RN 0.7320 0.0117 0.8950 0.0000
MTL MSE 0.7338 0.0112 0.8990 0.0010
STL REG RAC AR 0.72 0.0140 0.9149 0.3659
MTL RAC 0.7053 0.0081 0.9437 0.2690
STL REG RAC G 0.8640 0.0217 0.8851 0.3949
MTL RAC 0.8087 0.0141 0.8898 0.4815
STL REG RAC RN 0.8197 0.0129 0.8972 0.0986
MTL RAC 0.7697 0.01149 0.9012 0.1082
STL REG RAC R2CPP 0.6130 0.0272 0.7989 0.0150
MTL RAC 0.5157 0.0418 0.7892 0.2486
Table 7: Comparison of regression uncertainty method performance between STL REG
and MTL models for classificationtasks. The best model for each metric is in bold, while
models with worse metrics than the baseline are underlined.
27R-F1 FPR r
REG Method Class CP Regre CP pb
(MTL/STL)↑ (MTL/STL)↓ (MTL/STL) ↑
LAC 0.553/0.551 0.562/0.564 0.462/0.457
MSE CCLAC AR 0.509/0.498 0.563/0.561 0.462/0.454
CRC 0.553/0.551 0.562/0.564 0.462/0.457
LAC 0.553/0.551 0.573/0.596 0.463/0.459
MSE CCLAC G 0.509/0.498 0.577/0.612 0.464/0.462
CRC 0.553/0.551 0.573/0.593 0.463/0.459
LAC 0.549/0.555 0.647/0.660 0.344/0.342
MSE CCLAC RN 0.509/0.498 0.647/0.647 0.349/0.344
CRC 0.553/0.551 0.647/0.660 0.344/0.342
LAC 0.551/0.551 0.562/0.564 0.462/0.457
BCE CCLAC AR 0.499/0.498 0.555/0.560 0.465/0.455
CRC 0.551/0.551 0.555/0.565 0.466/0.456
LAC 0.551/0.551 0.573/0.596 0.467/0.46
BCE CCLAC G 0.499/0.498 0.579/0.613 0.467/0.463
CRC 0.551/0.551 0.573/0.596 0.467/0.460
LAC 0.551/0.5551 0.653/0.661 0.347/0.340
BCE CCLAC RN 0.499/0.498 0.651/0.646 0.353/0.342
CRC 0.551/0.555 0.647/0.660 0.344/0.342
LAC 0.582/0.570 0.801/0.762 0.408/0.355
RAC CCLAC AR 0.527/0.514 0.801/0.645 0.408/0.373
CRC 0.582/0.57 0.801/0.762 0.408/0.355
LAC 0.582/0.570 0.801/0.762 0.408/0.355
RAC CCLAC G 0.527/0.514 0.801/0.645 0.408/0.373
CRC 0.582/0.57 0.801/0.762 0.408/0.355
LAC 0.582/0.570 0.427/0.794 0.196/0.344
RAC CCLAC R2CCP 0.527/0.514 0.403/0.792 0.152/0.352
CRC 0.582/0.570 0.427/0.794 0.196/0.344
LAC 0.582/0.570 0.789/0.845 0.334/0.252
RAC CCLAC RN 0.527/0.514 0.789/0.842 0.335/0.259
CRC 0.582/0.570 0.789/0.845 0.310/0.08
Table 8: Comparison of Performance in the Moderation Review Process between MTL
and CoM models. Best values for all metrics are highlighted in bold.
28Name Class CP R-F1 ↑ FPR↓ MURE ↑ r ↑
pb
LAC 0.422 0.237 0.432 0.258
STL CLASS CCLAC 0.4 0.281 0.451 0.258
CRC 0.422 0.237 0.432 0.258
Table 9: STL CLASS performance on the estimation of the annotation disagreement and
the moderation review process.
Throughout the manuscript we have analysed theimpact of incorporating
this auxiliary task in terms of calibration, model performance, and uncer-
tainty estimation for both the primary task (classification) and the auxiliary
task (regression), using various regression losses and uncertainty quantifica-
tion techniques. We have found out that the integration of the auxiliary task
improves not only the classification task, but also enhances the auxiliary task
in terms of calibration and performance.
By demonstrating improvements in uncertainty estimation through the
use of annotation disagreement, we have opened up an interesting new area
of research, where future studies could build on towards enhancing existing
classification-based tasks with better uncertainty estimation techniques. Fi-
nally, we have evaluated the quantitative benefits of employing a multitask
architecture over a single-task approach, obtaining promising results. The
multitask approach outperformed the single-task model in detecting incor-
rect predictions, and also quantified disagreement in comments as specified
by the chosen γ, namely, the targeted ambiguity threshold.
Findings. We summarize the key findings for each of the research questions
that have guided this study:
•
RQ1: We have observed that, consistently with previous studies, the incor-
poration of an auxiliary task focused on annotation disagreement enhances
the calibration and performance of the toxicity classification task. In the
case of the improvements of performance in the auxiliary task, an overall
improvement was observed for the MSE and MAE metrics.
•
RQ2: In terms of improving uncertainty estimation by adding an auxiliary
task, this addition has been shown to enhance the classification model’s
uncertainty estimation compared to a single-task classifier. Furthermore,
we observed a better point-biserial correlation between the model’s uncer-
tainty and annotation disagreement, indicating a stronger understanding
29of borderline cases. For the regression task (disagreement estimation),
the confidence intervals generated were wider than those of the single-task
regression model used for the same task, particularly for the RAC and
BCE models. However, the MSE model did not demonstrate consistent
performance differences, making the overall enhancement inconclusive and
highly dependent on the appropriate configuration based on the selection
of the uncertainty method and the regression loss.
•
RQ3: The multitask architecture generally outperforms CoM, which con-
sist of a single-task classification model and a single-task regression model.
Although in some cases the improvement reported in our results is small,
the MTL framework is more computationally efficient and offered advan-
tages regarding the responses to RQ1 and RQ2. Compared to the single-
task classifier, the multitask model provides more flexibility for modera-
tion, as it directly computes annotation ambiguity. However, we acknowl-
edge that the uncertainty estimated for the single-task classifier is still
reasonably aligned with comment ambiguity.
On an overall concluding note, the moderation framework is not only
aligned better with the inherent limitations of toxicity classification, but also
offers improved uncertainty quantification, model calibration, and perfor-
mance, which are beneficial for real-world moderation processes.
Limitations. Toxicity classification, and by extension content moderation,
faces a significant challenge due to the inherent subjectivity of the task. This
subjectivity complicates dataset creation and the annotation process, as we
must account for both explicit and implicit (contextualized) toxicity. Since
a few datasets provide prior comments or context for toxic comments, it be-
comes difficult to accurately assess the toxicity during annotation. This can
lead to several issues in the annotation process, as outlined by Zhang et al.
[23]. The main challenges include: rater heterogeneity, individual differences,
variations in working patterns, difficulties in understanding comment com-
plexity, unclear task descriptions, and issues related to randomness. These
challenges directly impact our proposed moderation framework, which lever-
ages annotation disagreement as a core element. Another limitation is that
a few datasets guarantee a sufficient number of annotators, which restricts
our approach due to economic constraints, as increasing the number of an-
notators significantly raises the costs derived from the annotation process.
Finally, our approachmaybelimited bytheheterogeneity anddistribution of
30annotation disagreement, as datasets like Wiki Detox predominantly contain
low-disagreement comments. This makes it more challenging to effectively
model annotationdisagreement andincorporate it to frameworks like the one
proposed in this manuscript.
Future Work. As part of our futurework, we intend to explore additional un-
certainty estimation techniques to investigate whether the performance gains
from introducing the auxiliary task extend to other uncertainty estimation
methods. This will help in assessing the robustness and generalizability of
our framework when incorporating other uncertainty estimation techniques.
Furthermore, we aim to evaluate its applicability to other tasks character-
ized by subjective annotations, such as sentiment analysis, where annotation
disagreement and ambiguity play a critical role. Additionally, we envision
that our approach holds potential in the crossroads of active learning and
generative text modelling, as it can generate more diverse datasets and even-
tually, improve the detection of evolving patterns in toxic language expres-
sion. In this envisioned approach, we will investigate how to effectively avoid
known issues with synthetic data generation in fine-tuning loops, including
the model degradation due to content autophagy [40].
Acknowledgments
J. Del Ser acknowledges funding support from the Basque Government
through EMAITEK/ELKARTEK grants (IKUN, KK-2024/00064, BEREZ-
IA, KK-2023/00012), as well as the consolidated research group MATH-
MODE (IT1456-22). Guillermo Villate-Castillo acknowledges the funding
support from TECNALIA Research & Innovation, provided to employees
pursuing their doctoral thesis.
References
[1] R. Gorwa, R. Binns, C. Katzenbach, Algorithmic content moderation:
Technical and political challenges in the automation of platform gover-
nance, Big Data & Society 7 (2020) 2053951719897945.
[2] Z. Wang, A. Culotta, Identifying spurious correlations for robust text
classification, in: Findings of the Association for Computational Lin-
guistics: EMNLP 2020, 2020, pp. 3431–3440.
31[3] J. Jiang, Acritical audit ofaccuracy anddemographicbiases within tox-
icity detection tools, Dartmouth College Undergraduate Theses (2020).
[4] G. Villate-Castillo, J. Del Ser, B. Sanz, A systematic review of toxicity
in large language models: Definitions, datasets, detectors, detoxification
methods and challenges, 2024.
[5] I. Kivlichan, Z. Lin, J. Liu, L. Vasserman, Measuring and improving
model-moderator collaboration using uncertainty estimation, in: Pro-
ceedings of the 5th Workshop on Online Abuse and Harms (WOAH
2021), Association for Computational Linguistics, 2021, pp. 36–53.
[6] I. Lan˜a, I. Olabarrieta, J. D. Ser, Measuring the confidence of single-
point traffic forecasting models: Techniques, experimental comparison,
and guidelines toward their actionability, IEEE Transactions on Intelli-
gent Transportation Systems 25 (2024) 11180–11199.
[7] K. Zou, Z. Chen, X. Yuan, X. Shen, M. Wang, H. Fu, A review of
uncertainty estimation and its application in medical imaging, Meta-
Radiology (2023) 100003.
[8] A. M. Davani, M. D´ıaz, V. Prabhakaran, Dealing with disagreements:
Looking beyond the majority vote in subjective annotations, Transac-
tions of theAssociation for Computational Linguistics 10 (2022)92–110.
[9] T. Fornaciari, et al., Beyond black & white: Leveraging annotator
disagreement via soft-label multi-task learning, in: Proceedings of the
2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2021.
[10] T. Gillespie, Content moderation, AI, and the question of scale, Big
Data & Society 7 (2020) 2053951720943234.
[11] V. Vovk, A. Gammerman, G. Shafer, Algorithmic learning in a random
world, volume 29, Springer, 2005.
[12] A. Gammerman, V. Vovk, V. Vapnik, Learning by transduction, in:
Proceedings of the Fourteenth Conference on Uncertainty in Artificial
Intelligence, UAI’98, Morgan Kaufmann Publishers Inc., San Francisco,
CA, USA, 1998, p. 148–155.
32[13] M. M. Campos, A. Farinhas, C. Zerva, M. A. Figueiredo, A. F. Martins,
Conformal prediction for natural language processing: A survey, arXiv
preprint arXiv:2405.01976 (2024).
[14] P. Giovannotti, Calibration of natural language understanding models
with venn–abers predictors, in: Conformal and Probabilistic Prediction
with Applications, PMLR, 2022, pp. 55–71.
[15] N. Dey, J. Ding, J. Ferrell, C. Kapper, M. Lovig, E. Planchon, J. P.
Williams, Conformal prediction for text infilling and part-of-speech pre-
diction, The New England Journal of Statistics in Data Science (2022).
[16] L. Maltoudoglou, A. Paisios, L. Lenc, J. Mart´ınek, P. Kr´al, H. Pa-
padopoulos, Well-calibrated confidence measures for multi-label text
classification with a large number of labels, Pattern Recognition 122
(2022) 108271.
[17] K. Liang, Z. Zhang, J. F. Fisac, Introspective planning: Guiding
language-enabled agents to refine their own uncertainty, arXiv preprint
arXiv:2402.06529 (2024).
[18] D. Ulmer, C. Zerva, A. Martins, Non-exchangeable conformal language
generation with nearest neighbors, in: Findings of the Association for
Computational Linguistics: EACL 2024, 2024, pp. 1909–1929.
[19] A. N. Angelopoulos, S. Bates, Conformal prediction: A gentle introduc-
tion, Found. Trends Mach. Learn. 16 (2023) 494–591.
[20] J. Grimmelmann, The virtues of moderation, Yale JL & Tech. 17 (2015)
42.
[21] B. Hutchinson, V. Prabhakaran, E. Denton, K. Webster, Y. Zhong,
S. Denuyl, Social biases in NLP models as barriers for persons with
disabilities, in: Proceedings of the 58th Annual Meeting of the Associ-
ation for Computational Linguistics, 2020, pp. 5491–5501.
[22] K. Wang, Social Media Content Moderation: User-Moderator Collab-
oration and Perception Biases, Ph.D. thesis, The University of North
Carolina at Charlotte, 2024.
33[23] W. Zhang, H. Guo, I. D. Kivlichan, V. Prabhakaran, D. Yadav, A. Ya-
dav, A taxonomy of rater disagreements: Surveying challenges & op-
portunities from the perspective of annotating online toxicity, arXiv
preprint arXiv:2311.04345 (2023).
[24] cjadams, D. Borkan, inversion, J. Sorensen, L. Dixon, L. Vasser-
man, N. Thain, Jigsaw unintended bias in toxicity classi-
fication, 2019. URL: https://kaggle.com/competitions/
jigsaw-unintended-bias-in-toxicity-classification.
[25] N. Goyal, I. D. Kivlichan, R. Rosen, L. Vasserman, Is your toxicity my
toxicity? exploring the impact of rater identity on toxicity annotation,
Proceedings of the ACM on Human-Computer Interaction 6 (2022) 1–
28.
[26] E. Wulczyn, N. Thain, L. Dixon, Wikipedia talk labels: Personal at-
tacks, 2017.
[27] V. Sanh, DistilBERT, a distilled version of BERT: Smaller, faster,
cheaper and lighter, arXiv preprint arXiv:1910.01108 (2019).
[28] J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. Torr, P. Dokania,
Calibrating deep neural networks using focal loss, Advances in Neural
Information Processing Systems 33 (2020) 15288–15299.
[29] E. Guha, S. Natarajan, T. M¨ollenhoff, M. E. Khan, E. Ndiaye,
Conformal prediction via regression-as-classification, arXiv preprint
arXiv:2404.08168 (2024).
[30] R. Wan, J. Kim, D. Kang, Everyone’s voice matters: Quantifying an-
notation disagreement using demographic information, in: Proceedings
of the AAAI Conference on Artificial Intelligence, volume 37, 2023, pp.
14523–14530.
[31] A. Ramponi, E. Leonardelli, Dh-fbk at semeval-2022 task 4: Leveraging
annotators’ disagreement and multiple data views for patronizing lan-
guage detection, in: Proceedings of the 16th International Workshop on
Semantic Evaluation (SemEval-2022), 2022, pp. 324–334.
34[32] M. Sadinle, J. Lei, L. Wasserman, Least ambiguous set-valued clas-
sifiers with bounded error levels, Journal of the American Statistical
Association 114 (2019) 223–234.
[33] A. N. Angelopoulos, S. Bates, A. Fisch, L. Lei, T. Schuster, Conformal
risk control, arXiv preprint arXiv:2208.02814 (2022).
[34] T. Cordier, V. Blot, L. Lacombe, T. Morzadec, A. Capitaine, N. Brunel,
Flexible and systematic uncertainty estimation with conformal predic-
tion via the MAPIE library, in: Conformal and Probabilistic Prediction
with Applications, PMLR, 2023, pp. 549–581.
[35] J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, L. Wasserman,
Distribution-free predictive inference for regression, Journal of the
American Statistical Association 113 (2018) 1094–1111.
[36] J. Nixon, M. W. Dusenberry, L. Zhang, G. Jerfel, D. Tran, Measuring
calibration in deep learning., in: CVPR Workshops, volume 2, 2019.
[37] A. Anand, N. Mokhberian, P. N. Kumar, A. Saha, Z. He, A. Rao,
F. Morstatter, K. Lerman, Don’t blame the data, blame the model:
Understanding noise and bias when learning from subjective annota-
tions, arXiv preprint arXiv:2403.04085 (2024).
[38] J. Henriksson, C. Berger, M. Borg, L. Tornberg, S. R. Sathyamoor-
thy, C. Englund, Performance analysis of out-of-distribution detection
on trained neural networks, Information and Software Technology 130
(2021) 106409.
[39] M. Sandri, E. Leonardelli, S. Tonelli, E. Jeˇzek, Why don’t you do
it right? analysing annotators’ disagreement in subjective tasks, in:
Proceedings of the 17th Conference of the European Chapter of the
Association for Computational Linguistics, 2023, pp. 2428–2441.
[40] X. Xing, et al., When AI eats itself: On the caveats of data pollution
in the era of Generative AI, arXiv preprint arXiv:2405.09597 (2024).
35