AdaSociety: An Adaptive Environment with Social
Structures for Multi-Agent Decision-Making
YizheHuang*2,1 XingboWang*‚Ä†2 HaoLiu‚Ä†3 FanqiKong2,1 AoyangQin4,1
MinTang5,1 XiaoxiWang1 Song-ChunZhu1,2 MingjieBi1 SiyuanQi1 XueFeng(cid:0)1
1StateKeyLaboratoryofGeneralArtificialIntelligence,BIGAI 2PekingUniversity
3NewYorkUniversity 4TsinghuaUniversity 5UniversityofScienceandTechnologyofChina
szhyz@pku.edu.cn, jacksimbol@stu.pku.edu.cn, fengxue@bigai.ai
Abstract
Traditionalinteractiveenvironmentslimitagents‚Äôintelligencegrowthwithfixed
tasks. Recently,single-agentenvironmentsaddressthisbygeneratingnewtasks
basedonagentactions,enhancingtaskdiversity. Weconsiderthedecision-making
probleminmulti-agentsettings,wheretasksarefurtherinfluencedbysocialcon-
nections,affectingrewardsandinformationaccess. However,existingmulti-agent
environments lack a combination of adaptive physical surroundings and social
connections,hinderingthelearningofintelligentbehaviors. Toaddressthis,we
introduceAdaSociety,acustomizablemulti-agentenvironmentfeaturingexpanding
stateandactionspaces,alongsideexplicitandalterablesocialstructures. Asagents
progress,theenvironmentadaptivelygeneratesnewtaskswithsocialstructuresfor
agentstoundertake. InAdaSociety,wedevelopthreemini-gamesshowcasingdis-
tinctsocialstructuresandtasks. Initialresultsdemonstratethatspecificsocialstruc-
turescanpromotebothindividualandcollectivebenefits,thoughcurrentreinforce-
mentlearningandLLM-basedalgorithmsshowlimitedeffectivenessinleveraging
socialstructurestoenhanceperformance. Overall,AdaSocietyservesasavaluable
researchplatformforexploringintelligenceindiversephysicalandsocialsettings.
Thecodeisavailableathttps://github.com/bigai-ai/AdaSociety.
1 Introduction
Classiclearningenvironments[55,41,9,42,34]haveagentstrainedinsmallandstationaryworlds,
which hinders the improvement of agents‚Äô intelligence. The learning process stagnates once the
environments can no longer provide novel data for agents‚Äô explorations. Additionally, agents
trained on a fixed task set may suffer from a loss of generalization ability [13]. Single-agent
environments[18,25,61]setouttosolvethisproblembyconstructingadaptiveenvironmentsthat
continuouslygeneratenewtasksbasedonagentactions,providingamultitudinoustaskset.
Inmulti-agentsettings,however,thetasksetisdeterminedbynotonlyphysicalsurroundingsbutalso
socialconnectionsamongagents. Socialconnectionsdramaticallyimpactagents‚Äôdecision-making
byshapingtheirrewardstructuresandinformationaccess[20],anddifferentsocialstructuresendow
theenvironmentswithradicallydifferentresearchproblems. Forexample,centralizedscenariosfocus
onissueslikecreditassignmentandconsensusestablishment [21,44],whiledecentralizedsettings
requireagentstoaddressopponentmodelingissuesandnon-stationarity[3,21,29,33].
‚àóEqualcontribution.‚Ä†ThisworkwasconductedwhileXWandHLwereinternsatBIGAI.(cid:0)Corresponding
author.
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024)TrackonDatasetsandBenchmarks.
4202
voN
6
]AM.sc[
1v56830.1142:viXraPhysicalComponent Map Social Component
Naturalresources ‚Ä¶ Socialconnection Example:dynamicconnections
Resource
Syntheticresources ‚Ä¶ non-deterministic&dynamic
event Socialaction
resources resource Event
e.g. + grid connect disconnect
agent organization
Heterogeneousinv.capacity
Agent‚Äôs Heterogeneouspreferencefor inventory
resourcesininv.
Hierarchy
Movement (üëÜ,üëá,üëà,üëâ) agent-agent&agent-
Pick( ‚Ä¶),Dump ( ‚Ä¶) Physical organizationconnections
action
Synthesize,Communicate
Connectionsemantics
Impassable wallorwater Block shareinformation/reward
Figure 1: An overview of AdaSociety, composed of physical component and social component.
PhysicalComponentconsistsofdiverseresourcesandeventsonthemapandheterogeneousagents‚Äô
inventories. SocialComponentdescribestheadaptiveconnectionsbetweenagentsandorganizations,
whichshapeinformationaccessandrewardstructure. Agentstakesocialactionstoaltertheirsocial
connections. Asshownintherightmostflowchart,agentsareinitiallyindependentandcanestablish
individualconnections(edgesbetweennodes)andformgroups(grayovals).
Whatmakestheproblemevenmorechallengingisthatsocialconnectionsarenotpredefinedbut
adaptive,whichmeansthere‚Äôsadynamicalinterplaybetweenthetopologyofsocialconnectionsand
agents‚Äôstates[23]. Theadaptivenatureofsocialconnectionsandphysicalsurroundingsrequires
agents to learn continuously, reason about other agents‚Äô policies, and balance between physical
explorationsandestablishingsocialconnections. Whilecontemporarymulti-agentdecision-making
environments[6,2,53,66,48]haveachievedgreatprogressinstimulatingandtestingcapabilitiesof
learningalgorithmsinfixedtasksets,theyfailtogeneratenewtasksbyconcurrentlyconsidering
expandingphysicalsurroundingsandadaptivesocialconnections.
Tobridgethisgap,weproposeAdaSociety,amulti-agentenvironmentwithmassiveanddiversetasks
generatedbyadaptivesocialconnectionsandexpandingphysicalsurroundings,whichareinfluenced
byagents‚Äôbehavior. Inparticular,tothebestofourknowledge,AdaSocietyfirstintroducessocial
states(expressedasamulti-layerdirectedgraph)toexplicitlyandquantitativelydescribetheadaptive
anddynamicconnectionsbetweenentities,includingagentsandemergedorganizations. Thisgreatly
enrichesthediversityoftasks,supportingtheestablishmentofstableandlong-termrelationsbetween
entitiesandthequantitativestudyofsocialintelligence,likecoalitionformationandtheemergence
ofhierarchy. Insuchanenvironment,agentsneedtobalancetheexplorationofphysicalsurroundings
andthealterationofsocialconnections,leadingtomultiplepossiblevictorypathsandsignificant
decision-makingchallenges. TostimulatealgorithmdesignandtheoreticalanalysisinAdaSociety,we
provideaformulationofthemulti-agentdecision-makingproblems,namedGrowing-MG(Sec.3).
AdaSocietyservesasaplatformforresearcherstocustomizetheenvironmentfordiverseresearch
needs. Specifically,asetoffundamentalelementsandmechanismscanbeused,andinterfacesare
provided to set environment attributes and hyper-parameters. Moreover, AdaSociety exhibits its
characteristicsbyofferingthreemini-games,wherebothtensor-andLLM-basedmethodsaretested.
Insummary,thispapermakesthreecontributions. 1)Weintroduceanovelmulti-agentgeneral-sum
environmentfeaturingexpandingphysicalsurroundingsandadaptivesocialconnections. 2)Weoffer
acustomizableenvironmentwiththreebuilt-inmini-games,supportingbothtensor-andLLM-based
methods. 3) We implement RL and LLM methods in these mini-games and provide preliminary
results,layingthegroundworkforfurtherresearchinthisenvironment.
2 Environment
2.1 BasicComponents
ThekeycomponentsofAdaSociety(Fig.1)includethephysicalcomponent,composedofresources,
events,andagents‚Äôinventories,andthesocialcomponentdescribingconnectionsbetweenagentsand
organizations. Agentscanobserveandacttomodifybothphysicalandsocialstates.
2
‚Ä¶ ‚Ä¶2.1.1 PhysicalComponent
Resource and Event Resources are natural or synthetic. Natural resources scatter randomly on
the map. Some natural resources are visible to everyone while others can only be seen when an
agenthasspecificresourcesinitsinventory. Forexample,onlytheagentpossessingahammercan
observecoal. Whenagentswithspecificresourcesintheirinventoriesstandonaneventgridand
takethe‚Äòsynthesize‚Äôaction,oneunitofnewresourceissynthesized. Syntheticresourceswillbe
automatically placed into agents‚Äô inventories. These resources and events can be systematically
described as a synthesis tree (see Fig. 4). Importantly, agents are unaware of this synthesis tree.
Theygraduallylearnthetreethroughinteractionwiththeenvironment. Resources,eventgrids,and
agentsareinitializedinrandomlocationsonthemapforeveryepisode. Whilethereareexisting3D
benchmarkenvironmentsfocusingonperceptionchallenges,ourresearchcentersonthedomainof
multi-agentdecision-making. Tothisend,themapisintentionallycraftedina2Dsymbolicformat.
Agent‚ÄôsInventoryEveryagenthasaninventorywithmaximalcapacitiesofeveryresource,implying
skill diversity. For example, an agent with a 0 capacity for hammers cannot possess hammers
and observe coal. Agents can collect resources from the map into their inventories and dump
resourcesonthemap. Agents‚Äôrewardsareattachedtotheresourcesintheirinventories,whilethey
exhibitheterogeneityinresourcepreferences. Specifically,foragenti,therewardofresourceœÅis
R (œÅ)=mœÅ¬∑h (œÅ)¬∑rœÅ,wheremœÅistheamountofresourceœÅini‚Äôsinventory,h (œÅ)‚ààRrepresents
i i i i i
i‚ÄôspreferenceforœÅ,rœÅistheobjectiverewardofaunitofœÅ(seedetailsinSec.A.1).
2.1.2 SocialComponent
Thesocialcomponentexplicitlyexhibitstheconnectionsbetweenagentsororganizations. These
connectionsdrasticallyinfluencemulti-agentdecision-makingbyaffectingagents‚Äôaccessibleinfor-
mationandrewardstructures. Centralizationanditscompleteopposite,decentralization,canbeseen
astwotypicalconnectionstructures,presentingverydifferentdecision-makingproblems. AdaSociety
supportsadaptiveconnections,withcorrespondinginteractionsbeingmodeledasgeneral-sumgames.
AdaSocietyconsidersnotonlytheconnectionsbetweenagentsbutalsothesubordinateconnections
betweenagentsandorganizationsestablishedautonomouslybyagents. Thismakeshierarchicalcon-
nectionspossible. Agentstakesocialactionstochangesocialstates,likeconnectingordisconnecting
withsomeone. Fig.1illustratesevolvingconnectionstructures,fromfullyindependentagentsto
sparselyconnectedagentswithseveralnon-overlappingsmallgroups,andfinallytoaunifiedlarge
group. Ontheotherhand,asacustomizedenvironment,AdaSocietyalsosupportsuserstopredefine
and/orfixsocialconnectionsfortheirspecificresearchproblems. Thesemanticsofconnectionsare
diverse,whichcanberewardsharing,informationsharing,ordivisionoflaborbetweeninvolved
agents. AdaSocietysupportsthatagentsnegotiatetheirconnectionsemantics(Sec.4).
Tomaintainconsistencywiththephysicalcomponent,werefertotheseconnectionsbetweenagents
andorganizationsassocialstates,whichareexpressedasamulti-layerdirectedgraph(Sec.3). Social
statesexplicitlyandquantitativelyexpressrelationsbetweenagentsororganizations. Forexample,
the cooperation level of two agents can be measured by the frequency of connections between
them. Moreover,thecombinationofsocialstateswithsuccessivetasksinAdaSocietysupportsthe
establishmentofstableandlong-termrelationsandthestudyofsocialintelligence,likecoalition
formationandtheemergenceofhierarchy.
2.1.3 ObservationandAction
ObservationEachagentnavigateswithapartiallyobservablewindow,reachingogridsinthefour
cardinal directions of its current position. Agents can get their own inventory states of collected
resources,butnotthoseofco-players. Thesocialstatesofalltheagentsareaccessibletoeveryone.
ActionActionspaceconsistsofsocialactionsandphysicalactions. Socialactionsaimtobuildand
breakconnectionswithothers,includingotheragentsororganizations. Connectionsaredirectional.
If agent i connects to agent j, but not vice versa, i shares its information or reward with j, but
getsnothingfromj. Physicalactionsincludemovement,pickinganddumpingspecificresources,
synthesizing resources on corresponding event grids, and communicating with someone. Newly
synthesizedresourcesenrichpickinganddumpingactionsandtheactionspace.
3Table1: Comparisonwithexistingenvironments. AdaSocietyisuniqueforitsadaptiveconnections
betweenentitiesandexpandinggamespaces.
Multi- Dynamic Adaptive Imperfect Multi- General Tensor
Environment Comm.
agent Spaces Connection Information task Sum &LLM
AIEconomist[66] ‚úì ‚úó ‚úó ‚úì ‚úó ‚úó ‚úì ‚úó
BoatRace[2] ‚úì ‚úó ‚úì ‚úó ‚úó ‚úì ‚úì ‚úó
Crafter[25] ‚úó ‚úì ‚úó ‚úì ‚úó ‚úì ‚úó ‚úó
Diplomacy[6] ‚úì ‚úó ‚úó ‚úó ‚úì ‚úó ‚úì ‚úì
MeltingPot[2] ‚úì ‚úó ‚úó ‚úì ‚úó ‚úì ‚úì ‚úó
MineDojo[18] ‚úó ‚úì ‚úó ‚úì ‚úó ‚úì ‚úó ‚úì
NeuralMMO[53] ‚úì ‚úó ‚úó ‚úì ‚úì ‚úì ‚úó ‚úó
Overcooked[11] ‚úì ‚úó ‚úó ‚úó ‚úó ‚úì ‚úó ‚úó
SMAC[48] ‚úì ‚úó ‚úó ‚úì ‚úó ‚úó ‚úì ‚úó
Xland[54] ‚úì ‚úì ‚úó ‚úì ‚úó ‚úì ‚úì ‚úó
AdaSociety ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì
2.2 EvaluationMetrics
AdaSocietyprovidesdiversemetricstoevaluatetheperformancesofagentsandorganizationsin-
cludingIndividualreward,Fairnessscore,Completionrate,andAveragedegreeandMaximum
degreeofthesocialnetwork. DefinitionsanddetailsofthemetricsarediscussedinSec.A.5.
2.3 EnvironmentCharacteristics
TherearevariouscharacteristicsofAdaSocietythatmakeitnovel(seeTab.1). AdaSocietyisamulti-
agentdecision-makingenvironment,whichprovidesbothmini-gamesforspecificresearchproblems
andacustomizableplatformtoresearchers(seedetailsinSec.A.4). Agentsdynamicallyconnect
withotheragentsororganizationsandautonomouslycommunicatetonegotiatethesemanticsof
connections,makingtheemergenceofhierarchicalsocialstructureanddiversesocialintelligence
possible. With these dynamic and non-deterministic connections, friends may become foes, and
viceversa. Thus,theinteractionsbetweenagentscanbemodeledasgeneral-sumgames,where
cooperationcoexistswithcompetition. Agentsnavigatethisplaygroundwithapartiallyobservable
windowcenteredontheircurrentposition. ThestateandactionspacesofAdaSocietydynamically
expand,adaptingtoagents‚Äô(physicalandsocial)behavior. Thatgeneratesmassiveanddiverse
tasks,supportinganevaluationofagents‚Äôabilitiesinmultipleaspects. AdaSocietyisfriendlyto
LLM-andtensor-basedagents. Weevaluatestate-of-the-artRLmethodsandLLMsinSec.5. In
addition, wewanttostressthatthemutualadaptationbetweenagentsandAdaSociety, which
generatesavarietyofsuccessivetasksandmultiplepossiblevictorypaths. Achievingsuccessin
AdaSocietyrequiresabalancebetweentheexplorationofphysicalcomponentsandthealterationof
socialconnections(seeFig.5). Agentscontinuallylearnpoliciestoefficientlyexploreandachieve
goalsinAdaSociety. Meanwhile,agents‚Äô(physicalandsocial)behaviorwillaffectthedynamicsof
AdaSociety. SynthesizingnewresourceswillgraduallyexpandAdaSociety‚Äôsphysicalstatespace
and the corresponding physical action space, transition function, and reward function. Updated
socialstateswillreshapeagents‚Äôobservationandrewardstructures. Thus,tasksandtasksequences
are influenced by agents‚Äô behavior and social states, not sampled according to some predefined
distributionoftasks. Thatistosay,AdaSocietyadaptsitstasksandtasksequencestoagents. Mutual
adaptationprovidesexceptionallymassiveanddiversecomplextasks. Thestochasticityandnon-
stabilityofAdaSocietyproducevariousenvironmentdynamics. Agentsneedtokeeplearningtoadapt
tochangingsituations.
2.4 ResearchChallenges
Asanadaptivemulti-agentenvironment,AdaSocietyprovidesacomprehensiveplatformthatpresents
plentyofresearchchallenges. Theadaptiveanddynamiccharacteristicsofthephysicalandsocial
componentsbringchallengesmainlylyingintheintricateandunpredictableinteractionsbetween
agents. Throughmulti-dimensionalexploration,agentslearntheabilityofdynamicenvironmental
adaptationandengageincommunication-enabledinteractions. Meanwhile,agentsmaydevelop
4socialcognitionandutilizethisinformationtoconductcollectivereasoning,whichmayresultinthe
emergenceofvariousbehaviors. DetailsofthesechallengesarestatedinAppendixB.
3 Formulation
WenowprovideacomprehensivedefinitionandanalysisoftheGrowing-MGwithasocialstructure,
whicharegeneralenoughtoencompassalltheresearchchallengesmentionedabove. Threeconcrete
scenarioswillbeinstantiatedinnextsection.
Thepredominantmodelinmulti-agentsequentialdecision-makingistheMarkovGame(MG)[37].
However,asignificantlimitationofMGistheassumptionofconstantstateandactionspacesand
unchangedMarkoviantransitionsorrewards,ensuringconvergencetosomeclassicalsolutionssuch
asglobaloptimalityorNashequilibrium[4,57]. Toaddressdynamicstateandactionspaces,we
introducetwonewstructures,Monotonic-MG-bundleandGrowing-MGasbelow. AGrowing-MG
yieldsamulti-agentnon-stationarydecision-makingframework. Attimestept,withstates and
t
actiona , theMonotonic-MG-bundleproducesS ,A ,T ,R = Œ≤(s ,a ), formingone
t t+1 t+1 t+1 t+1 t t
newMGinstance. Thisframeworkdiffersfromtime-varyinggames[10,64,5],whichonlymodel
payoff matrix dependent on past actions. On the other hand, both the transition probability and
rewardfunctioninGrowing-MGwillevolvetriggeredwithsomecertaintransitions. Forsimplicity,
wedenoteallpossibletransitionandrewardfunctionsonarbitrarystateandactionspaceS,A,as
T(S,A) = {T|T : S √óA ‚Üí S}andR(S,A) = {R|R : S √óA ‚Üí R}andthelargestpossible
spacessupportedbytheenvironmentasuniversalstatespaceS andactionspaceA .
w w
Definition1. Abase-MGisatupleMG = ‚ü®I,S ,A ,T ,R ,œÅ,Œ≥‚ü©, whereI = {1,...,I}isa
b b b b b
setofagents;S ={S1,...,SI}andA ={A1,...,AI}isthestatespaceandactionspaceofall
b b b b b b
agents;T : S √óA √óS (cid:55)‚Üí [0,1]andR : S √óA (cid:55)‚Üí RI isthetransitionandrewardfunction;
b b b b b b b
œÅ:S (cid:55)‚Üí[0,1]istheinitialstatedistributionandŒ≥ isthetemporaldiscountfactor.
b
Definition 2. A Monotonic-MG-bundle upon a base-MG MG within the universal state
b
and action space S = {S1,...,SI},A = {A1,...,AI} is a map Œ≤ : S √ó A ‚Üí
w w w w w w t t
{S ,A ,T ,R |Si ‚äÜ Si ‚äÜ Si ‚äÜ Si ,Ai ‚äÜ Ai ‚äÜ Ai ‚äÜ Ai ,T ‚àà
t+1 t+1 t+1 t+1 b t t+1 w b t t+1 w t+1
T(S ,A ),R ‚ààR(S ,A )}.
t+1 t+1 t+1 t+1 t+1
Definition3. AGrowing-MGuponabase-MGMG withintheuniversalstateandactionspace
b
S ,A isatupleMG =(MG ,Œ≤).
w w g b
Conceptually,eachalterationinthestateandactionspacerepresentsadistinctstagewhereinterre-
lationshipsamongagentsshouldalsochange. Inspiredbyresearchincomplexsystemslikesocial
sciencesandeconomics[15,52,14], weproposeenhancingtheGrowing-MGframeworkwitha
multilayergraphstructure[26]G = (V,E,C)(seeFig.6). C isasetoflayers,andV isthesetof
nodesinalllayers. E isthesetofedgesexistingbetweennodesinonelayerorneighboringlayers.
Westartwithanon-interconnectedmultiplexsystemofnetworks{G1,G2,¬∑¬∑¬∑ ,G|C|},whereeach
layercconsistsofanodesetVc andanedgesetEc,representedbyanadjacencymatrixAc with
ij
i,j ‚àà{1,¬∑¬∑¬∑ ,|Vc|}. NodesinthefirstlayerrepresentagentsinGrowing-MG,whilehigherlayers
representgroupsandhierarchiesofgroups. Todelineaterelationshipbetweennodesinneighboring
layerssuchasagent-groupmembership,weintroduceinter-layerconnectivityusinganadjacency
matrixAc,c+1 withi ‚àà {1,¬∑¬∑¬∑ ,|Vc|}andj ‚àà {1,¬∑¬∑¬∑ ,|Vc+1|}. Thisrepresentationmodelsboth
ij
static and time-varying networks, as inter-layer and intra-layer connectivity evolves with agents‚Äô
behavior, distinguishing it from existing multi-agent frameworks that predetermine interactions
throughrewardstructures[46,62]. Finally, wenotethatboththeenvironmentalandsocialstates
withintheframeworkcanbeextendedtoincludeobservationalinformation[38,39],therebyfurther
enhancingtheframework‚Äôsgeneralityandpracticalrelevance.
4 Mini-games
ToprovideacomprehensivebenchmarkandillustratethecharacteristicsofAdaSociety,wepropose
a set of mini-games (Fig. 2). The three mini-games are arranged in ascending order of the com-
plexityofdecision-making. Socialstructure,prescribingagents‚Äôpartnersandconnectionsemantics,
evaluatesagents‚Äôabilitytoadapttothechangeablesocialstructure. Contractpredefinesconnection
semantics,whereagentsneedtoselectpartnerswhilelearningcoordinationwithvariousco-players.
5InNegotiation, agentsindependentlyselectpartners, determinetherewarddistributionplanwith
theirpartners,andbehaveunderthenegotiatedrelationship. Allofthethreemini-gamessharethe
samephysicalcomponent(Sec.5.1),whichcontainsapartofthesynthesistree. Thefollowingtext
providesadetaileddescriptionofthesocialcomponentsofSocialstructure,Contract,Negotiation.
Toshowthefullcomplexityofourphysicalcomponents,anothermini-gameExploration,which
containsallbuilt-inresourcesandevents,isintroducedinSec.C.2.
SocialStructure. Theexplicitrepresen-
tationofsocialstructureallowsdynamic
1 2 3 4
changes as agents interact with the en-
vironment. Pre-defined rules for struc-
G1 G2 G3 G4
ture change could be designed to com- Round 1 Matching
Inequality
pel agents to alter their social relation- 1 2 3 4
shipswhileinteractingwiththeenviron-
Ind.group Bargaining
ment. We implement structure change G1 G2 G3 G4
Round 2
at certain steps: when step t reaches Ovlp.group ‚Ä¶ Matching
T ,T ,...,thesocialstructuresaremod-
1 2 (a)SocialStructure (b)Contract (c)Negotiation
ifiedtoG ,G ,...,respectively. Different
1 2
categoriesofsocialstructuresarestated Figure2: Overviewofthreemini-games.
inSec.C.1. Thisforcesagentstolearn
policiestoadapttothechangingsocialenvironment.
Contract. Theenvironmentisdividedintotwostages: thecontractformationstagefordetermining
socialconnectionsandthephysicalinteractionstagetointeractwiththephysicalcomponentand
co-playerswithdeterminedsocialconnections. ThecontractformationstagelastsforcN timesteps,
wherecisapositiveintegerandN isthenumberofagents,whilethephysicalinteractionstagehasa
durationofT. Therefore,thetotaldurationofeachepisodeiscN +T. Beforethecontractformation
stage(0‚â§t<cN),anorder(i ,i ,...,i )israndomlysampled. Attimet,agenti ,wherek =t
1 2 N k
mod N,takessocialaction,selectingagroupnodev ‚ààV toconnect. Anagentcanconnectwith
g g
onlyonegroupnode. Agentswithinthesamegroupareconsideredtohaveformedacontracttoshare
rewards. Inthephysicalinteractionstage(t‚â•cN),allagentsactsynchronouslywithinthephysical
component,andtherewardsreceivedareequallydividedamongtheagentswithinthesamegroup.
Negotiation. Thegamehasanegotiationstagefollowedbyaphysicalstage. Innegotiation,agents
seekcooperationbyselectinganopponentandsendinghimarequest. Aftermutualrequests,agents
bargainbyexchangingproposalsuntilagreementorbreakup. Inthebargainingsession, agentsi
and j take turns to perform one of the three actions: (i) PROPOSE a new scheme (w ,w ) s.t.
i j
w +w =1,wherew andw representthepartitionofrewardsobtainedbyiandj respectively
i j i j
in the physical stage. (ii) ACCEPT the proposal from one‚Äôs opponent and form a new group
(coalition). (iii) DECLINE the proposal and end this session without any commitment. Once a
new group is formed, the cooperative relationship between i and j represented by edge E with
ij
apayoffdistribution (w ,w )isestablished. Later, when iorj seekstonegotiatewithothers, it
i j
represents the group {i,j}. For example, if i and an out-group agent k reach a new distribution
plan(wnew,wnew),thenkisregardedasjoining{i,j}toformanewgroup{i,j,k}withanupdated
i k
distribution(w ¬∑wnew,w ¬∑wnew,wnew).
i i j j k
5 Experiments
5.1 EnvironmentSetup
We have designed two physical task settings, featuring different levels of difficulty, for Social
Structure,Contract,andNegotiation. TheparametersofthesetasksareprovidedinSec.C.3.
IntheEasytask,theenvironmentinvolvesasingleeventHammerCraft. Withinthistask,agentsare
categorizedintotwotypesbasedontheirinventorycapacityandvaluepreference: carpentersand
miners. Carpentershavetheabilitytogatherwoodandstone,whichtheycanthenusetoproduce
hammersthroughtheHammerCraftevent. However,theirinventoryislimitedtoholdingonlyone
hammeratatime. Ontheotherhand,minersareunabletocollectstone,makingthemincapableof
producinghammers. However,minerspossesstheadvantageofbeingabletostoreaconsiderable
numberofhammersintheirinventory. Additionally,hammersheldbyminersareassignedahigher
valuecomparedtothoseheldbycarpenters.
6IntheHardtask,theenvironmentbecomesmorecomplexwiththeinclusionofsixresources: wood,
stone,hammer,coal,torch,andiron,aswellastwoevents: HammerCraftandTorchCraft. Similarto
theEasytask,agentsaredividedintocarpentersandminers. Duetothelimitedcapacityofcertain
resources,onlycarpenterscanexecuteHammerCrafttoproducehammers,whileonlyminerscan
executeTorchCrafttoproducetorches. However,carpenters‚Äôinventoriescannotstorecoal,which
requiresahammertopickup,andminers‚Äôinventoriescannotstoreiron,whichrequiresatorchto
pickup. Consequently,inordertomaximizegrouprewards,carpentersandminersshouldengagein
resourceexchange,providingtheresourcestheycanproducetoeachother. Thiscollaborativeeffort
ensuresthatthegroupcanobtainmoreresourcescollectively.
5.2 BaselineMethods
Weuseseveraldeepreinforcementlearningalgorithmsasbaselines. ProximalPolicyOptimization
(PPO)[49]strikesabalancebetweensampleefficiencyandpolicystabilitybyconstrainingpolicy
updatesusingatrustregionapproachandaclippedsurrogateobjective. RecurrentPPO(RecPPO)
usesPPOfortrainingandaddLSTM[28]tomaintainmemoriesinthenetwork. Rainbow[27]isa
value-basedmethodthatincorporatesseveralkeyenhancementsintotheDeepQ-learningframework.
MAPPO is the multi-agent version of PPO. It learns a critic that takes the global state and other
agents‚Äôactionsasinputsduringtraining. Weemployaconvolutionalneuralnetworkforencoding
gridinformationandagraphconvolutionalnetwork[32]forencodingsocialstateinallRLmethods.
Theopen-sourcelibraryRLLib[36]isusedforRLtraining.
Additionally, we design a curriculum learning (CL) algorithm. It starts with shared rewards to
enhancecooperationstrategies,thengraduallyincreasessocialstaterandomnessforlearningunder
differentsocialstructures,andfinallyallowsagentstoperformsocialactionstoestablishtheirown
socialstate. RecPPOisusedforRLtrainingateachstage. WealsopresentaLargeLanguageModel
+controller(LLM-C)frameworkbasedonGPT-4[1],whichconvertsenvironmentalinformation
intopromptstoqueryanLLMforhigh-levelplansandthencallsarule-basedcontrollertoexecute
actions based on the generated plan. LLM has been shown to be effective in some single-agent
environments,suchasMineCraft[59,56,58,67,60]. Thedetailsofthelasttwoalgorithmsaregiven
inAppendixD.
5.3 Results
5.3.1 SocialStructure
IntheSocialStructuremini-game,variousstaticanddynamicsocialstructuresaretestedtoevaluate
baselinealgorithms. DetailedresultsarepresentedinAppendixE. Here,wediscusstheresultofone
Dynamicscenario,wherethesocialstructurestartswithInequality,thenswitchestoIndependent
(Ind.) groupatstep30,andalterstoOverlapping(Ovlp.) groupatstep60.
Fig.3apresentstherewardaccumulationasagentstakeactionswiththreestatic-structurescenarios
andonedynamic-structurescenario,respectively. Theresultsverifytheinfluenceofdynamicchange
insocialstructureonagentperformancesincetheDynamiccurveresemblestheInequalityscenario
initiallybutthenitdropsinlaterstepsandapproachesOvlp. groupscenario.
Fig.3bandFig.3cillustratetheperformanceofvariouslearningmethods. Sometraditionalmethods,
suchasPPO,RecPPO,andMAPPO,exhibitsimilarperformance,withMAPPOperformingworsedue
50 Dynamic CL 0.40
234 000 I I On n ve d lq . p u g . a r gol ri u oty p up 234 000 P R M
R
RP e
a
aAO c
i
nP nP dP bP oO oO
mw
00000 ..... 12233 50505 C
P RP
eL
O
cPPO
1 00 1 00 000 ... 001 050 M R Ra aA i nP n dP b oO o mw
0 25 50 75 100125150175200 0 25 50 75 100125150175200 0 8000 16000 24000 32000
Steps Steps Episodes
(a) (b) (c)
Figure3: Dynamicstructure: (a)Individualrewardperstepwithdifferentsocialstructuresusing100
samplesfromPPO-trainedpolicies,(b)Individualrewardperstepusing100samplesfromdifferent
policies(c)Learningcurvesusingdifferentlearningmethods.
7
sdraweR
laudividnI
sdraweR
laudividnI
sdraweR
laudividnI
dezilamroNTable2: AverageindividualrewardinContractandNegotiation,normalizedbytheOrcalereward.
CL PPO RecPPO MAPPO Rainbow Random
Con.
Easy 0.9136¬±0.0023 0.2286¬±0.0003 0.2276¬±0.0015 0.2271¬±0.0003 0.1987¬±0.0127 0.0046¬±0.0002
Hard 0.2773¬±0.0466 0.1151¬±0.0002 0.1149¬±0.0000 0.1137¬±0.0005 0.0868¬±0.0033 0.0021¬±0.0000
Nego.
Easy 0.3543¬±0.0229 0.2276¬±0.0006 0.2278¬±0.0004 0.2147¬±0.0001 0.1969¬±0.0105 0.0040¬±0.0001
Hard 0.1945¬±0.0109 0.1093¬±0.0027 0.1107¬±0.0019 0.0946¬±0.0032 0.0905¬±0.0024 0.0020¬±0.0001
tothedifficultyinlearninganeffectivecentralcriticforheterogeneousagents. Rainbowperformsthe
worst,likelybecauseofitsgeneralineffectivenessinexploration. Curriculumlearningdemonstrates
superior performance by leveraging prior knowledge of different structures to adapt to dynamic
scenarios effectively. Additionally, figures in Fig. 3 reveal significant deviations in most tests,
regardlessofsocialstructures,learningalgorithms,orperformancemetrics. Comparedtoscenarios
withoutagentgroups(Fig.10aandFig.11a),theresultsindicatethatthecurrentalgorithmsstruggle
tolearnstablepoliciesforscenarioswithagentgroups.
5.3.2 Contract
As depicted in Tab. 2, Contract presents a challenge for popular RL methods, as they are stuck
inalocalequilibriumofcompletinglimitedHammerCraftonbothtasks(seeFig.7b), whileCL
demonstratesnotableperformanceontheEasytasksandsurpassesgeneralRLmethodsontheHard
tasks. ThefirstcurriculuminCLequipstheagentwiththeabilitytolearneffectivepoliciesinthe
physicalrealm,andthesecondcurriculumempowerstheagenttomakeinformedjudgmentsabout
differentsocialstructureswhileconsideringrationalphysicalpolicies. Ultimately,thisknowledge
aidsCLinselectinganappropriatecontract. However,itappearsthatCLmayforgetthestrategies
acquired during the first curriculum, as the reward at the end of the second stage has dropped
significantlycomparedtotheendofthefirststage(seeTab.12fordetails). Thismighthamperthe
performanceofCLontheHardtask.
Sharingrewardshasbeenrecognizedasaneffectivemethodforagentgroupstoacquirecooperative
strategies,therebysupportingthefeasibilityofCL‚Äôsapproach. Fig.7candFig.7dalsoillustrates
that. InthecaseoftheEasytask,CLeventuallyestablishesastablegroupofthreeindividualswho
activelysharerewardsandformacooperativealliance. However,itisimportanttonotethatthesize
ofthegroupdoesnotdirectlycorrelatewithhighreturns. Rainbow,forinstance,frequentlyforms
largegroupsinbothtasksbutfailstoachievesubstantialreturns. Thisoutcomeprimarilystemsfrom
inherentlimitationsinthealgorithm‚Äôslearningcapabilities.
5.3.3 Negotiation
Traditional RL methods struggle to enable carpenters and miners to learn to cooperate through
negotiation, dumping some tools to increase the benefit of teammates with larger capacities on
thephysicalstageasshowninTab.2. Thischallengearisesfromthecomplexityofcouplingthe
negotiationandphysicalstages. Oncenegotiationfails,dumpingtoolsinthesubsequentphysical
stagewouldsubstantiallyreducetheagents‚Äôrewards. Meanwhile,thecomplexnegotiationprocess
exacerbatestheconvergenceprobleminmulti-agentsettings,andagentshavetheincentivetoclaim
alargershareforthemselvestoexploittheco-playersinbargaining,posingchallengestoreaching
a consensus agreement. Consequently, in both easy and hard tasks, the average and maximum
degreesarelow,withmostagentsoptingtocompletetasksindependently,leadingtolowcompletion
ratesinHammerCraftandevenacompletefailureinTorchCraft(Fig.8). Intheeasytask,miners‚Äô
rewardsheavilyrelyoncarpenters‚Äôcooperation,whichseverelycompromisesfairness. Incontrast,
byfirstlearningtheoptimalstrategiesinphysicalenvironmentsunderdifferentsocialstructures,CL
canidentifystructureswithhighercooperationdegreesasmorebeneficial,facilitatingconsensus
duringnegotiationlearningandachievinghighergrouprewards,fairness,andsuccessfulTorchCraft.
Additionally,weshowtheCarpenters/Miners(abbreviatedasC/M)splitratiowhenthenegotiation
(cid:80) (cid:80)
stageisdone,whichiscomputedby w / w .Allresultsexceed1,aligning
i‚àà{Carpenters} i i‚àà{Miners} j
withtheintuitionthatminersaredisadvantagedinnegotiationsastheycannotindependentlyproduce
themorerewardinghammers.
85.3.4 LLM-CinAdaSociety
LLM-C runs three times for each task. Tab. 3 and Tab. 9 presents the quantitative results across
various metrics. Benefiting from the embedded commonsense reasoning and social intelligence
of LLMs, LLM-C exhibits outstanding performance in all three mini-games, achieving average
rewardsnearlysurpassingallRL-basedmethods. Afterbeinginformedofthegamerulesandthe
capabilitydifferencesbetweencarpentersandminers,LLM-Ccanaccuratelyrecognizetheimpor-
tanceofcooperationandswiftlyformallianceswithotherplayersthroughnegotiationorcontract.
During the physical stage, manu-
ally coded controllers complement Table3: AveragerewardofLLM-Cacrossmini-games.
LLM‚Äôs deficiencies in path plan-
ningandpositionjudgment,precisely SocialStructure Contract Negotiation
and efficiently realizing the high-
levelplanninggeneratedbytheLLM
Easy - 0.8433¬±0.1312 0.8733¬±0.1116
basedonthecurrentsocialstructure
Hard 0.7894¬±0.0444 0.6499¬±0.1716 0.6862¬±0.1027
andphysicalenvironment. However,
due to common issues with LLMs
such as hallucinations, context length limitations, and randomness in outputs, LLM-C does not
achieveOracleperformance,anditunderperformscomparedtoCLinContract-Easy,furthervalidat-
ingtheeffectivenessofourproposedCLapproach.
6 RelatedWorks
Environments. Several craft-based environments like Malmo [31], Crafter [25], Minedojo [18]
and Conan [61] create dynamic state and action spaces that expand with the agent‚Äôs exploration,
which, however, mainly focuses on single-agent setting. Environments including MAgent [65],
XLand[54],andMiniworld[12]provideasetofdifferentandtransferrabletasksthatbuildfrom
basicelements,andtheyareopenforcustomization. MeltingPot[2]containsasetofover50MARL
learningsubstrateswith limitedcustomizability. Interactivegames includingAIEconomist[66],
Overcooked[11],MPE[42],NeuralMMO[53],andSMAC[48]placeagentsindiversesystems
allowingthemtocompeteorcooperate. OtherexampleslikeDiplomacy[6]focusoncommunication
betweenagents. Noneoftheseenvironmentscontainbothdynamicsocialconnectionsandadaptive
taskslikeAdaSociety.
UnsupervisedEnvironmentDesign(UED).IntheparadigmofUED[16,40,30],theenvironment
learns a policy Œì : Œ† ‚Üí ‚àÜ(ŒòT), which is a function from agent policy Œ† to the environment‚Äôs
parametersŒòT. Suchapolicywillautomaticallyproduceadistributionoversolvableenvironments
andfurthersupportthecontinuedlearningoftheagent‚Äôspolicy. AdaSocietydoesnotimplementUED
toproducediversetasks. UnlikeUED,AdaSocietyhasnogoalsorobjectives,likemostecological
systems, and produces multiple tasks through adaptive social structures and expanding physical
surroundings.
Structured multi-agent systems. In multi-agent systems, various connections may be formed
betweenagents, andtheseconnectionsmayformcertainstructures. [17], [51]and[45]focuson
findingcommunicationtopologyformulti-agentcoordination. Someresearchmodelsthelocalityof
interactionandlearnsajointvalueviacoordinationgraphs[24,8,35]. NetworkedMARL[63,46,
47,62,50]learnslocalizedpoliciesonenvironmentswhereagentinteractionsarecontingentupon
theirconnectionswithinastaticgraph. Wefocusondynamicagentconnectionswhichshapeagents‚Äô
rewardsandobservations,andtheseconnectionsaremodeledasamulti-layergraph.
7 Conclusion
WeintroduceAdaSociety,amulti-agentenvironmentfeaturingexpandingphysicalsurroundingsand
adaptivesocialconnections. Theenvironmentiscapableofgeneratingmultipletasksinadaptation
toagents‚Äôbehavior. AdaSocietyisfriendlytotensor-basedandLLM-basedmethods. AdaSociety
providesinterfacessupportingsuperbcustomizationandalsooffersasetofmini-gameswithdiverse
socialconnections. WetestseveralRLandLLM-basedalgorithmsinmini-games. Preliminaryresults
indicatethatAdaSocietymaintainsarationalcomplexitylevelforcurrentdecision-makingmethods.
9TherearesomelimitationsofAdaSocietyilluminingourfuturework. Human-machineinteractionis
crucialforthestudyofmulti-agentsystems,whichisoneofourkeyresearchobjectivesinAdaSociety.
Whiletheenvironmentistemporarilynotequippedwithhumaninterfaces,thecurrentarchitecture
doessupportthesubsequentdevelopmentofhuman-machineinterfaces. Inaddition,ourgamespaces
canbefurtherexpandedbyintroducingsurvivalpressures(needforfood,hostilecreatures,andso
on). Thesenegativelosseswillpenalizeundesirableactions,complementtherolesofpositiverewards
inreinforcingdesirablebehavior,andguidemorediversebehavior.
AcknowledgmentsandDisclosureofFunding
ThisworkissupportedbytheNationalScienceandTechnologyMajorProject(No.2022ZD0114904).
Thisworkisalsosupportedbytheproject‚ÄúWuhanEastLakeHigh-TechDevelopmentZone,National
ComprehensiveExperimentalBaseforGovernanceofIntelligentSociety‚Äù.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] JohnPAgapiou, AlexanderSashaVezhnevets, EdgarADu√©√±ez-Guzm√°n, JaydMatyas, Yi-
ran Mao, Peter Sunehag, Raphael K√∂ster, Udari Madhushani, Kavya Kopparapu, Ramona
Comanescu,etal. Meltingpot2.0. arXivpreprintarXiv:2211.13746,2022.
[3] StefanoVAlbrechtandPeterStone. Autonomousagentsmodellingotheragents: Acompre-
hensivesurveyandopenproblems. ArtificialIntelligence,258:66‚Äì95,2018.
[4] EitanAltmanandAdamShwartz. Constrainedmarkovgames: Nashequilibria. InAdvancesin
dynamicgamesandapplications,pages213‚Äì221.Springer,2000.
[5] Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On the
convergence of no-regret learning dynamics in time-varying games. Advances in Neural
InformationProcessingSystems,36,2024.
[6] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried,
Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik
Konath,MinaeKwon,AdamLerer,MikeLewis,AlexanderH.Miller,SashaMitts,Adithya
Renduchintala,StephenRoller,DirkRowe,WeiyanShi,JoeSpisak,AlexanderWei,DavidWu,
HughZhang,andMarkusZijlstra. Human-levelplayinthegameofdiplomacybycombining
languagemodelswithstrategicreasoning. Science,378(6624):1067‚Äì1074,December2022.
doi: 10.1126/science.ade9097. URLhttps://doi.org/10.1126/science.ade9097.
[7] RicoBerner,SimonVock,EckehardSch√∂ll,andSerhiyYanchuk. Desynchronizationtransitions
inadaptivenetworks. PhysicalReviewLetters,126(2):028301,2021.
[8] WendelinB√∂hmer,VitalyKurin,andShimonWhiteson. Deepcoordinationgraphs. InInterna-
tionalConferenceonMachineLearning,pages980‚Äì991.PMLR,2020.
[9] GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,
andWojciechZaremba. Openaigym. arXivpreprintarXiv:1606.01540,2016.
[10] AdrianRiveraCardoso,JacobAbernethy,HeWang,andHuanXu.Competingagainstequilibria
inzero-sumgameswithevolvingpayoffs. arXivpreprintarXiv:1907.07723,2019.
[11] MicahCarroll,RohinShah,MarkKHo,TomGriffiths,SanjitSeshia,PieterAbbeel,andAnca
Dragan. Ontheutilityoflearningabouthumansforhuman-aicoordination. Advancesinneural
informationprocessingsystems,32,2019.
[12] MaximeChevalier-Boisvert,BolunDai,MarkTowers,RodrigoPerez-Vicente,LucasWillems,
SalemLahlou,SumanPal,PabloSamuelCastro,andJTerry. Minigrid&miniworld: Modular
&customizablereinforcementlearningenvironmentsforgoal-orientedtasks. InAdvancesin
10NeuralInformationProcessingSystems,volume36,pages73383‚Äì73394.CurranAssociates,
Inc.,2023. URLhttps://proceedings.neurips.cc/paper_files/paper/2023/file/
e8916198466e8ef218a2185a491b49fa-Paper-Datasets_and_Benchmarks.pdf.
[13] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying
generalization in reinforcement learning. In International conference on machine learning,
pages1282‚Äì1289.PMLR,2019.
[14] ManlioDeDomenico. Moreisdifferentinreal-worldmultilayernetworks. NaturePhysics,19
(9):1247‚Äì1262,2023.
[15] FabioDellaRossa,LouisPecora,KarenBlaha,AfrozaShirin,IsaacKlickstein,andFrancesco
Sorrentino. Symmetriesandclustersynchronizationinmultilayernetworks. Naturecommuni-
cations,11(1):3179,2020.
[16] MichaelDennis,NatashaJaques,EugeneVinitsky,AlexandreBayen,StuartRussell,Andrew
Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised
environmentdesign. Advancesinneuralinformationprocessingsystems, 33:13049‚Äì13061,
2020.
[17] YaliDu,BoLiu,VincentMoens,ZiqiLiu,ZhichengRen,JunWang,XuChen,andHaifeng
Zhang. Learningcorrelatedcommunicationtopologyinmulti-agentreinforcementlearning.
InProceedingsofthe20thInternationalConferenceonAutonomousAgentsandMultiAgent
Systems,pages456‚Äì464,2021.
[18] LinxiFan,GuanzhiWang,YunfanJiang,AjayMandlekar,YuncongYang,HaoyiZhu,Andrew
Tang, De-AnHuang, YukeZhu, andAnimaAnandkumar. Minedojo: Buildingopen-ended
embodiedagentswithinternet-scaleknowledge. AdvancesinNeuralInformationProcessing
Systems,35:18343‚Äì18362,2022.
[19] CorradoGini.Variabilit√†emutabilit√†:contributoallostudiodelledistribuzioniedellerelazioni
statistiche. January1912. URLhttp://ci.nii.ac.jp/ncid/BB01068601.
[20] MarkGranovetter. Theimpactofsocialstructureoneconomicoutcomes. InThesociologyof
economiclife,pages46‚Äì61.Routledge,2018.
[21] SvenGronauerandKlausDiepold. Multi-agentdeepreinforcementlearning:asurvey. Artificial
IntelligenceReview,55(2):895‚Äì943,2022.
[22] ThiloGrossandBerndBlasius. Adaptivecoevolutionarynetworks: areview. Journalofthe
RoyalSocietyInterface,5(20):259‚Äì271,2008.
[23] ThiloGrossandHirokiSayama. AdaptiveNetworks: theory,modelsandapplications. August
2009. URLhttp://pubman.mpdl.mpg.de/pubman/item/escidoc:2220530.
[24] CarlosGuestrin,DaphneKoller,andRonaldParr. Multiagentplanningwithfactoredmdps.
Advancesinneuralinformationprocessingsystems,14,2001.
[25] DanijarHafner. Benchmarkingthespectrumofagentcapabilities. InInternationalConference
onLearningRepresentations,2021.
[26] ZaynabHammoudandFrankKramer. Multilayernetworks: aspects, implementations, and
applicationinbiomedicine. BigDataAnalytics,5(1):2,2020.
[27] MatteoHessel,JosephModayil,HadoVanHasselt,TomSchaul,GeorgOstrovski,WillDabney,
DanHorgan,BilalPiot,MohammadAzar,andDavidSilver. Rainbow: Combiningimprove-
ments in deep reinforcement learning. In Proceedings of the AAAI conference on artificial
intelligence,volume32,2018.
[28] SeppHochreiterandJ√ºrgenSchmidhuber. Longshort-termmemory. Neuralcomputation,9(8):
1735‚Äì1780,1997.
11[29] YizheHuang,AnjiLiu,FanqiKong,YaodongYang,Song-ChunZhu,andXueFeng. Efficient
adaptationinmixed-motiveenvironmentsviahierarchicalopponentmodelingandplanning.
In Proceedings of the 41st International Conference on Machine Learning, volume 235 of
ProceedingsofMachineLearningResearch,pages20004‚Äì20022.PMLR,21‚Äì27Jul2024. URL
https://proceedings.mlr.press/v235/huang24p.html.
[30] Minqi Jiang, Michael Dennis, Jack Parker-Holder, Andrei Lupu, Heinrich K√ºttler, Edward
Grefenstette, TimRockt√§schel, andJakobFoerster. Groundingaleatoricuncertaintyforun-
supervised environment design. Advances in Neural Information Processing Systems, 35:
32868‚Äì32881,2022.
[31] MatthewJohnson,KatjaHofmann,TimHutton,andDavidBignell. Themalmoplatformfor
artificialintelligenceexperimentation. InIjcai,volume16,pages4246‚Äì4247,2016.
[32] ThomasNKipfandMaxWelling. Semi-supervisedclassificationwithgraphconvolutional
networks. InInternationalConferenceonLearningRepresentations,2016.
[33] Fanqi Kong, Yizhe Huang, Song-Chun Zhu, Siyuan Qi, and Xue Feng. Learning to bal-
ance altruism and self-interest based on empathy in mixed-motive games. arXiv preprint
arXiv:2410.07863,2024.
[34] MarcLanctot,EdwardLockhart,Jean-BaptisteLespiau,ViniciusZambaldi,SatyakiUpadhyay,
JulienP√©rolat,SriramSrinivasan,FinbarrTimbers,KarlTuyls,ShayeganOmidshafiei,etal.
Openspiel:Aframeworkforreinforcementlearningingames.arXivpreprintarXiv:1908.09453,
2019.
[35] Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep
implicit coordination graphs for multi-agent reinforcement learning. In Proceedings of the
20thInternationalConferenceonAutonomousAgentsandMultiAgentSystems,pages764‚Äì772,
2021.
[36] EricLiang,RichardLiaw,RobertNishihara,PhilippMoritz,RoyFox,KenGoldberg,Joseph
Gonzalez,MichaelJordan,andIonStoica. Rllib: Abstractionsfordistributedreinforcement
learning. InInternationalconferenceonmachinelearning,pages3053‚Äì3062.PMLR,2018.
[37] MichaelLLittman. Markovgamesasaframeworkformulti-agentreinforcementlearning. In
Machinelearningproceedings1994,pages157‚Äì163.Elsevier,1994.
[38] Qinghua Liu, Csaba Szepesv√°ri, and Chi Jin. Sample-efficient reinforcement learning of
partiallyobservablemarkovgames. AdvancesinNeuralInformationProcessingSystems,35:
18296‚Äì18308,2022.
[39] XiangyuLiuandKaiqingZhang. Partiallyobservablemulti-agentrlwith(quasi-)efficiency:
theblessingofinformationsharing. InInternationalConferenceonMachineLearning,pages
22370‚Äì22419.PMLR,2023.
[40] Ishita Mediratta, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, and
TimRockt√§schel. Stabilizingunsupervisedenvironmentdesignwithalearnedadversary. In
ConferenceonLifelongLearningAgents,pages270‚Äì291.PMLR,2023.
[41] VolodymyrMnih, KorayKavukcuoglu, DavidSilver, AndreiARusu, JoelVeness, MarcG
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-levelcontrolthroughdeepreinforcementlearning. nature,518(7540):529‚Äì533,2015.
[42] IgorMordatchandPieterAbbeel. Emergenceofgroundedcompositionallanguageinmulti-
agentpopulations. InProceedingsoftheAAAIconferenceonartificialintelligence,volume32,
2018.
[43] DavidMoreno-Mateos,AnttonAlberdi,EllyMorri√´n,WimHvanderPutten,AsunRodr√≠guez-
U√±a,andDanielMontoya. Thelong-termrestorationofecosystemcomplexity. NatureEcology
&Evolution,4(5):676‚Äì685,2020.
[44] AfshinOroojlooyandDavoodHajinezhad. Areviewofcooperativemulti-agentdeepreinforce-
mentlearning. AppliedIntelligence,53(11):13677‚Äì13722,2023.
12[45] Emanuele Pesce and Giovanni Montana. Learning multi-agent coordination through
connectivity-drivencommunication. MachineLearning,112(2):483‚Äì514,2023.
[46] GuannanQu,YihengLin,AdamWierman,andNaLi. Scalablemulti-agentreinforcementlearn-
ingfornetworkedsystemswithaveragereward. AdvancesinNeuralInformationProcessing
Systems,33:2074‚Äì2086,2020.
[47] GuannanQu,AdamWierman,andNaLi. Scalablereinforcementlearningoflocalizedpolicies
formulti-agentnetworkedsystems. InLearningforDynamicsandControl,pages256‚Äì266.
PMLR,2020.
[48] MikayelSamvelyan,TabishRashid,ChristianSchroederdeWitt,GregoryFarquhar,Nantas
Nardelli,TimGJRudner,Chia-ManHung,PhilipHSTorr,JakobFoerster,andShimonWhite-
son. Thestarcraftmulti-agentchallenge. InProceedingsofthe18thInternationalConference
onAutonomousAgentsandMultiAgentSystems,pages2186‚Äì2188,2019.
[49] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximal
policyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
[50] XingyuSha,JiaqiZhang,KeyouYou,KaiqingZhang,andTamerBas¬∏ar. Fullyasynchronous
policyevaluationindistributedreinforcementlearningovernetworks. Automatica,136:110092,
2022.
[51] JunjieSheng,XiangfengWang,BoJin,JunchiYan,WenhaoLi,Tsung-HuiChang,JunWang,
andHongyuanZha. Learningstructuredcommunicationformulti-agentreinforcementlearning.
AutonomousAgentsandMulti-AgentSystems,36(2):50,2022.
[52] QiSu,AlexMcAvoy,YoichiroMori,andJoshuaBPlotkin. Evolutionofprosocialbehaviours
inmultilayerpopulations. NatureHumanBehaviour,6(3):338‚Äì348,2022.
[53] JosephSu√°rez,PhillipIsola,KyoungWhanChoe,DavidBloomin,HaoXiangLi,NikhilPinna-
paraju,NishaanthKanna,DanielScott,RyanSullivan,RoseS.Shuman,LucasdeAlc√¢ntara,
HerbieBradley,LouisCastricato,KirstyYou,YuhaoJiang,QimaiLi,JiaxinChen,andXiaolong
Zhu. Neuralmmo2.0: Amassivelymulti-taskadditiontomassivelymulti-agentlearning,2023.
[54] Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck,
JakobBauer,JakubSygnowski,MajaTrebacz,MaxJaderberg,MichaelMathieu,etal. Open-
endedlearningleadstogenerallycapableagents. arXivpreprintarXiv:2107.12808,2021.
[55] EmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-based
control. In2012IEEE/RSJinternationalconferenceonintelligentrobotsandsystems,pages
5026‚Äì5033.IEEE,2012.
[56] GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,
andAnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels.
TransactionsonMachineLearningResearch,2024. ISSN2835-8856.
[57] Xiaofeng Wang and Tuomas Sandholm. Reinforcement learning to play an optimal nash
equilibriuminteammarkovgames. Advancesinneuralinformationprocessingsystems,15,
2002.
[58] ZihaoWang,ShaofeiCai,GuanzhouChen,AnjiLiu,XiaojianMa,andYitaoLiang. Describe,
explain,planandselect: interactiveplanningwithllmsenablesopen-worldmulti-taskagents.
InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
[59] ZihaoWang,ShaofeiCai,AnjiLiu,YonggangJin,JinbingHou,BoweiZhang,HaoweiLin,
ZhaofengHe,ZilongZheng,YaodongYang,etal. Jarvis-1: Open-worldmulti-taskagentswith
memory-augmentedmultimodallanguagemodels. arXivpreprintarXiv:2311.05997,2023.
[60] ZihaoWang,ShaofeiCai,ZhancunMu,HaoweiLin,CeyaoZhang,XuejieLiu,QingLi,Anji
Liu,XiaojianMa,andYitaoLiang. Omnijarvis: Unifiedvision-language-actiontokenization
enablesopen-worldinstructionfollowingagents. arXivpreprintarXiv:2407.00114,2024.
13[61] Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, and Yixin Zhu. Ac-
tive reasoning in an open-world environment. In Advances in Neural Informa-
tion Processing Systems, volume 36, pages 11716‚Äì11736. Curran Associates, Inc.,
2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
2712b17bb58ea5b2b65c45857b024744-Paper-Conference.pdf.
[62] YuxuanYi,GeLi,YaoweiWang,andZongqingLu. Learningtoshareinnetworkedmulti-agent
reinforcementlearning. AdvancesinNeuralInformationProcessingSystems,35:15119‚Äì15131,
2022.
[63] KaiqingZhang,ZhuoranYang,HanLiu,TongZhang,andTamerBasar. Fullydecentralized
multi-agent reinforcementlearning withnetworkedagents. In International Conference on
MachineLearning,pages5872‚Äì5881.PMLR,2018.
[64] MengxiaoZhang,PengZhao,HaipengLuo,andZhi-HuaZhou. No-regretlearningintime-
varyingzero-sumgames. InInternationalConferenceonMachineLearning, pages26772‚Äì
26808.PMLR,2022.
[65] LianminZheng,JiachengYang,HanCai,MingZhou,WeinanZhang,JunWang,andYongYu.
Magent: Amany-agentreinforcementlearningplatformforartificialcollectiveintelligence. In
ProceedingsoftheAAAIconferenceonartificialintelligence,volume32,2018.
[66] StephanZheng,AlexanderTrott,SunilSrinivasa,DavidCParkes,andRichardSocher. The
ai economist: Taxation policy design via two-level deep multiagent reinforcement learning.
Scienceadvances,8(18):eabk2607,2022.
[67] XizhouZhu,YuntaoChen,HaoTian,ChenxinTao,WeijieSu,ChenyuYang,GaoHuang,Bin
Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for
open-worldenvironmentsvialargelanguagemodelswithtext-basedknowledgeandmemory.
arXivpreprintarXiv:2305.17144,2023.
14A EnvironmentElements
Inthissection,weelaboratetheenvironmentelementspredefinedinAdaSocietyincludingresources,
events,andtheirdependency.
A.1 Resources
Thereare15kindsofresourcesinAdaSociety,whichcanbedividedintoNaturalResourcesand
Synthesized Resources based on whether they can be produced through events. Some of the nat-
uralresourcescanonlybediscoveredandgatheredbyagentswithcertainresources(denotedby
Requirements)intheirinventories. ThedetailsofresourcesarelistedinTab.4.
Table4: ResourcespredefinedinAdaSociety. Synthesizedindicateswhethertheresourcecanbe
crafted through events. Requirement is an attribute of natural resources (Synthesized = False)
indicatingthattheresourceisobservableandcollectibletoagentscarryingtherequiredresources.
Objectiverewarddenotestheobjectiverewardsofresources.
Resource Wood Stone Hammer Coal Torch Iron Steel Shovel Pickaxe GemMine Clay Pottery Cutter Gem Totem
Synthesized ‚úó ‚úó ‚úì ‚úó ‚úì ‚úó ‚úì ‚úì ‚úì ‚úó ‚úó ‚úì ‚úì ‚úì ‚úì
Requirement None None - Hammer - Torch - - - Pickaxe Shovel - - - -
Objectivereward 1 1 5 2 20 3 30 100 150 4 4 40 100 200 1000
A.2 Events
Thereare9built-ineventsinAdaSocietyaslistedinTab.5. Eacheventtakes2to3kindsofresources
asinputandoutputs1kindofproduct. Eventscanonlybeobservedandexecutedbyagentswhose
inventoriesmeettheeventrequirements.
Table5: EventspredefinedinAdaSociety. TheingredientsofeacheventarecoveredinInput. Most
eventstake2or3differentkindsofinputresources. TheproductsarelistedinOutput. Requirement
denotestheresourcesanagentneedstocarryinitsinventorytoobserveandexecutetheevent.
fEvent Input1 Input2 Input3 Output Requirement1 Requirement2
HammerCraft 1Wood 1Stone - 1Hammer - -
TorchCraft 1Wood 1Coal - 1Torch Coal -
SteelMaking 1Iron 1Coal - 1Steel Iron -
Potting 2Clay 1Coal - 1Pottery Clay -
ShovelCraft 2Steel 2Wood - 1Shovel Steel -
PickaxeCraft 3Steel 2Wood - 1Pickaxe Steel -
CutterCraft 2Steel 3Stone - 1Cutter Steel -
GemCutting 1GemMine - - 1Gem Cutter GemMine
TotemMaking 2Gem 1Pottery 1Steel 1Totem Gem -
A.3 Synthesistree
AnillustrationofthesynthetictreeisshowninFig.4,whichisusedbyallthemini-gamesoffered
bythispaper. InFig.4,naturalandsyntheticresourcesaredepictedwithinagreencircleandblue
octagoniconsrespectively. Thesolidredarrowlineattachedbyasquareeventiconlinkslow-level
resourcestohigh-levelproducts. Theeyeiconsindicatethatsomeresourcescanhelptheirowner
discovernewresourcesorevents.
A.4 Customization
AdaSocietyisaversatilemulti-agentenvironmentplatformthatsupportsextensivecustomizationof
variouselements,features,andhyper-parameters. Researcherscaneasilycreatetailoredenvironments
fordifferentobjectiveswithoutneedingtodelveintotheunderlyingcode.
15Clay
Wood Pottery
Iron
Shovel
Hammer
Steel Totem
Pickaxe
Torch
Cutter
Coal
Stone
Gem mine Gem
Natural resources Events
Synthetic resources Discovery
Figure4: Illustrationofasynthesistree.
Built-inresourcesandevents(See‚ÄúSynthesistree"ofFig.4andTab.5)areincludedinAdaSocietyfor
userstooptionallyincorporateintotheirownscenarios. Usersarealsowelcometodefinetemporary
resourcesandevents. ThecustomizableelementsandcorrespondingparametersarelistedinTab.6.
Table6: Customizableelementsandparameters.
Element Parameter Description
Mapsize h,w Mapheightandmapwidth.
Terrain B TerrainsetB={b ,¬∑¬∑¬∑,b }.b representsablock.
1 |B| i
bpos:thepositionofblockb onthemapwhichcanbeassignedorrandomlygenerated.
i i
Resource œ± Setofresourcesœ±={œÅ ,¬∑¬∑¬∑,œÅ }.EachresourceœÅ hasanattributeœÅreq.
1 |œ±| i i
œÅreq:Necessaryresourcesinagents‚Äôinventoriestoobserve&collectœÅ.
i i
œÅ Temporaryresources(DefinedbyspecifyingœÅreq )
temp temp
Event E SetofeventsE ={œµ ,¬∑¬∑¬∑,œµ }.Eacheventœµ hasattributesœµin,œµout,œµreq.
1 |E| i i i i
œµin:Resourcesconsumedbyeventœµ.
i i
œµout:Resourcesproducedbyeventœµ.
i i
œµreq:Necessaryresourcesinagents‚Äôinventoriestoobserve&executeœµ.
i i
Epos EventpositionsEpos={œµp 1os,¬∑¬∑¬∑,œµ |E|pos}.Eachœµp iosrepresentsalistofpositionsofœµ i.
œµ Temporaryevents(Definedbyspecifyingœµin ,œµout ,œµreq )
temp temp temp temp
Agent P SetofagentsP ={1,¬∑¬∑¬∑,|P|}
m(0) Initialinventories.mœÅ(0)denotestheinitialnumberofresourceœÅininventories.
i i
icap Inventorycapacity.icap:œ±‚ÜíRdenotesmaximumquantitiesofresourcesicancarry.
h h:œ±‚ÜíRdenotesquantitiesofcreditsigetsbyacquiringresources.
i i
Theactualrewardobtainedbyiish multiplybytheobjectiverewardoftheresource.
i
ipos(0) Initialpositionsofagentswhichcanbepredefinedorgeneratedrandomly.
A.5 EvaluationMetrics
Individualreward iscalculatedas:
(cid:88)
Rc = R (œÅ), (1)
i i
œÅ‚ààœ±
representingagenti‚Äôssubjectiverewardofalltypesofresourcesœ±.
16Fairnessscore iscomputedbasedonGiniindex[19]toassessesthegroup-wisefairness:
(cid:80)N (cid:80)N |Rc‚àíRc|
F =1‚àí i=1 j=1 i j , (2)
2N(cid:80)N Rc
i=1 i
whereN isthenumberofagents. Intuitively,thegreaterthevalueofF agroupgets,thefaireritis.
Individualrewardisoneofthemostcommonmetricsfordecision-makingproblems. Itmeasures
agents‚Äôdecision-makingabilitiesinmaximizingself-interest. However,relyingsolelyonindividual
rewardscanberisky. Ingeneral-sumgames,agentsfocusonmaximizingtheirownrewardsmay
engageinshortsightedandexploitativebehaviorsthatharmtheirownlong-termrewardsandthe
collectivebenefit. Forexample,inPrisoner‚ÄôsDilemma,self-interestedagentsalwaysfallintothe
inefficient Nash equilibrium of defection, which minimizes one‚Äôs own reward and the collective
benefit. To tackle this issue, we introduce the fairness score calculated using the Gini index,
whichevaluatesfairnesswithinagroup. Inrealsocieties,fairnessisacrucialcomponentofsocial
justice,significantlyinfluencingthestabilityofsocialstructuresandthemaintenanceoflong-term
cooperation. This metric serves as a reference for selecting agents and algorithms that balance
efficiencyandfairness,ratherthanmerelypursuingindividualgains.
Completionrate pertainstotheratioofsuccessfulexecutionsofaneventtoitsmaximumpotential
executions. Itiscomputedseparatelyforeachevent. Thecompletionrateisintroducedtomeasure
agents‚Äô exploration within the synthesis tree. It is calculated as the ratio of actual executions to
the optimal executions of the oracle policy (computation of the oracle policy can be found in
Supplementary). The higher the dimension of the completion rate, the deeper the exploration.
Exploration is crucial in RL. The introduction of completion rate will guide decision-making
algorithms to avoid local optima, actively explore the environment, and find the optimal policy
effectively.
Averagedegree ofnodetypeŒì‚àà{agent,group}iscalculatedas:
1 (cid:88)
D = D , (3)
Œì |N | n
Œì
n‚ààNŒì
whereN isthesetofŒìnodesandD isthedegreeofnoden.
Œì n
Maximumdegree reflectsthemaximumdegreeofacertaintypeofnode,definedas:
Dmax = max D . (4)
Œì n
n‚ààNŒì
Inasymmetriccases(wherenotalledgesarebidirectional),themaximumdegreeandtheaverage
degreementionedabovearecalculatedseparatelyforin-degreesandout-degrees. Socialstructureis
thedistinctivefeatureofAdaSociety.Degree-basedmetrics,includingaveragedegreeandmaximum
degree,areproposedtodescribeandmeasurethetopologyofsocialstructure,whichsignificantly
influences agents‚Äô policies and performances by shaping their information streams and reward
functions. Agents‚Äôdegreedistributionisgenerallycorrelatedwiththeirrewards. Forexample,an
agent witha high degree canobtain more informationor participate inmore rewarddistribution,
therebygaininghigherreturns. Combiningdegree-basedmetricswithothermetrics,likeindividual
rewardandfairness,wecanrecognizetheeffectivesocialstructureforscenarios,guidingthelearning
ofalgorithms.
A.6 Supplementaryfiguresforenvironmentdescriptionandformulation
A.6.1 MutualadaptionbetweenagentsandAdaSociety
Based on complex network theory, we say AdaSociety is an adaptive environment. In complex
networktheory, anetworkiscalledanadaptivenetwork, ifthereisafeedbackloopbetweenthe
attributesorbehaviorofnodesandthetopologyofthenetwork[22,43,7]. InAdaSociety,agents
build or break connections with others and impact social structure. Conversely, social structure
influences agents‚Äô observations and reward structures and further influences their attributes and
behavior. Thus,followingthedefinitionofadaptivenetworks,thesocialstructureofAdaSocietyis
17adaptive. AsakeycomponentofAdaSociety,socialstructureinfluencesthegenerationofnewtasks.
For example, independentagents collect allkinds of availableresources to synthesizehigh-level
resources. However, the team-up agents will be mostly rewarded by collecting or synthesizing
somespecifickindofresources,accordingtothedivisionoflaborintheteam. Furthermore,agents
initiallycanonlyobserveverylimitedresources(woodandstoneinourmini-games)andevents
(hammercraft). ThroughexplorationinAdaSociety,agentsgraduallydiscovernewresourcesand
events.Theappearanceofanewkindofresourcedependsonagents‚Äôbehavior.Forinstance,asshown
bythesynthesistreeinFig.4,whichappearsnext,shovelorcutter,dependsonagents‚Äôbehavior. To
sumup,AdaSocietyisanadaptiveenvironment.
Fig.5describesthemutualadaptionbetweenagentsandAdaSociety. Toachievetheirgoals,agents
learnpoliciestoadapttheir(physicalandsocial)behaviortotheenvironment. Meanwhile,agents‚Äô
behaviorwillaffectandevenchangetheenvironment. Specifically,physicalactionswillexpandthe
physicalstatespaceandthecorrespondingphysicalactionspace,reward,andtransitionfunctionsby
synthesizingnewresources. Socialactionswillaltersocialconnections,andtheninfluenceagents‚Äô
informationaccessandrewardstructures. InAdaSociety,thereisafeedbackloopbetweenagentsand
theenvironment,makingtheircoevolutionpossibleandmayshedlightonthegenerationofinfinite
tasks.
Physicalcomponent
Physicalactions Ph sy tas ti eca Al
spct ai co en T fr ua nn cs ti it
of iR u
o
ne nnw cta iord n
Agents Information Reward
Socialactions shar Sin og cialcompona ello nc tation
AdaSociety
Figure5: IllustrationofthemutualadaptationbetweenagentsandAdaSociety.
A.6.2 Amulti-layerdirectedgraphexpressionforsocialstructure
AsshowninFig.6,AdaSocietyexpressessocialstatesasamulti-layerdirectedgraph. Eachlayer
showsalevelofsocialorganization. AdaSocietysupportsthedescriptionofsocialstructureswith
arbitrarylevels,dependingontheresearchproblemsandtherequiredgranularityofsocialstructures.
Thebottom0th-levelconsistsofindividualagents,whoarethefundamentalunitsofdecision-making.
Nodesineachlayerrepresententities/agentsinthecorrespondinglevel. Anyagentonthekth-level
(k ‚â• 1) is composed of its connected agents on the (k-1)th-level. Its decision-making relies on
groupnorms,likevoting,consensusdecision-makinganddelegation. Akth-levelagentwillaffect
its(k-1)th-levelneighbors‚Äôrewardfunctionsandobservations,therebyinfluencingtheirdecision-
makingandenablingtheirdivisionoflabourandcooperation. Oneagentonthe(k-1)th-levelmay
besimultaneouslysubordinatetoanynumberofagentsonthekth-level. Forexample,anindividual
employeeisthe0th-levelagent,aprojectteamcomposedofseveralemployeesisthe1st-levelagent,
acompanyconsistingofmanyteamsisthe2nd-levelagent,andabusinessgroupcomposedofmany
companiesisthe3rd-levelagent.
AdaSociety supports the emergence of high-level social organizations. Edges inside one layer
representcooperativeconnections,whichshareinformationorrewardsbetweeninvolvedentities.
Edgesacrosslayersrepresentsubordinateconnections,withlow-levelentitiescomplyingwiththe
policyimplementedbythehigh-levelentities. Modelingsocialstatesasamulti-layergraphwill
facilitatetheapplicationofexistinggraphtheoryknowledgetoourresearch.
B ResearchChallenges
Exploration Agentsstartwithafewresourcesandeventswithinasimpleenvironmentinitially.
Astheagentsexplorethesynthesistree,theirbehaviorstriggerthemechanismstodepictchangesin
thephysicalenvironment. Duringthisprocess,moreresourcesandeventsareunlockedgradually,
182nd-level
1st-level
0th-level
agent cooperation subordinate
Figure6: Anillustrationofsocialstatesexpressedasamulti-layerdirectedgraph.
increasingthecomplexityoftheexploration. Dependencybetweendifferentresourcesandevents
evaluatestheagents‚Äôabilitiestomakedeepexplorationsintheenvironmentactively.
Adaptation InAdaSociety,agents‚Äôbehaviorscouldtriggertheenvironmenttoevolvewhilethe
changed environment affects actions that agents can take. Apart from the physical environment,
thesocialstructureoftheagentscoulddynamicallychangeasaconsequenceofeitherpre-defined
rulesoragentsocialbehaviors. Thisrequiresagentstomakedecisionsaccordinglytoadapttoand
co-evolvewithdynamicenvironmentsandsocialrelationships.
SocialCognition Agentshavebeliefsintheirsocialstructures,whichexplicitlyrepresenthowthey
interactwithotheragents, suchasexchanginginformationandsharingrewards. Inthiscomplex
environment,severalachievementsrequirecollaborationwhileresourcesarelimited,forcingagentsto
cognitivelyinferothers‚Äôintentions,evaluatetheeffectivenessofsocialstructures,andtheninvestigate
better choices. This makes AdaSociety a suitable environment for studying social cognition and
behaviors,suchasheterogeneousroles,labordivision,ownership,andtrust/betrayal.
Communication Portal for communication is provided to agents for sharing information and
coordinating actions. A successful agent may learn various communication protocols, context
representations,andinformationprocessingforoptimalobjectives. ThusAdaSocietycouldbeused
forstudyingtheeffectivenessofagentcommunication-enabledinteractions,suchasnegotiationfor
resourcetrading,informationtransitivity,andsemanticinteroperability.
Collective Reasoning Agents are embedded with heterogeneous skills while they only know
theirownskills. Thecomplexsynthesistreerequiresagents‚Äôabilitiestomakegroupdecisionson
collaboration,suchasknowledgesharingandskilltransferringforgreatergroupbenefit. Additionally,
the dynamics of environments make collective reasoning harder, especially for temporal credit
assignments. Forinstance,agentsmayoffertools(negativeimmediatereward)tocollaboratorsto
exploit unexplored resources (greater delayed reward). Therefore, AdaSociety brings challenges
for collective reasoning, such as adaptive cooperation and coordination, consensus, and conflict
resolution.
Emergence Action space in multiple perspectives, including physical actions, social actions,
andcommunication,enablesmassivepossibilitiesofagentbehaviorswithoutexplicitpolicies. In
AdaSociety,onecouldobservetheemergenceofcoordinationandcooperation,socialstructuresand
norms,andevencommunicationprotocolsandlanguage.
19C Mini-GameDetails
C.1 SocialStructure
AsstatedinSec.3,agentsconnectedbyedgesshareobservations,therebyimprovingtheircollective
situational awareness. Agents connecting to the same group node share rewards based on the
edge attributes, incentivizing collaborative efforts to achieve greater rewards. With the social
structure, agents act synchronously in the physical environment, following the mechanism for
sharingobservationandrewarddefinedbythesocialstructure.
Inthismini-game,weconductedexperimentswithstaticanddynamicsocialstructures. Inthestatic
setting,agentsareinitializedwithacertainsocialstructureG andkeepthestructureuntiltheendof
oneepisode. Inthispaper,wecategorizesocialstructuresthatarelessthantwolayersintofivetypes
toexaminetheeffectsofvaryingstructuresonagentbehaviorandperformance: 1)Isolation: agents
arefullyunconnected,i.e.,C = 1;E = ‚àÖ;2)Connection: agentsareconnectedwithoutforming
groups,i.e.,C = 1;E =Ã∏ ‚àÖ;3)IndependentGroup: agentsaregroupedwhileeachagentjoinsat
mostonegroup,i.e.,C =2;(cid:80)V2 A =1‚àÄi‚ààV1;4)OverlappingGroup: agentsaregroupedand
j ij
canjoinmultiplegroups,i.e.,C = 2;‚àÉi ‚àà V1, (cid:80)V2 A > 1;5)Inequality: agentsaregrouped
j ij
withdifferentrewardweights.
Inthedynamicsetting,pre-definedrulesforstructurechangecouldbedesignedtocompelagentsto
altertheirsocialrelationshipswhiletheytakeactionswithinthephysicalenvironment. Inthistask,
wedesignaDynamicscenario,wherethesocialstructurestartswithInequality,thenswitchesto
Ind. groupatstep30,andalterstoOvlp. groupatstep60.
C.2 Exploration
Inthisscenario,allbuilt-inresourcesandeventsareincluded. Physicalactionsandsocialactions
areavailableateverystep. Allagentsshareacommonvaluepreference,where1)resourcesnear
theendofthesynthesistreeareassignedhighvalue,and2)syntheticresourcesarevaluedhigher
thannaturalresources. Duetopartialobservation,timelimitations,andthechallengesassociated
withexploringnewresources,agentsmaymanipulatethesocialstatetoencourageinterestbinding,
informationsharing,anddivisionoflabor,whichhelpstomaximizerewards.
C.3 ParametersofMini-Games
TheparametersoftheEasytaskandtheHardtaskofSocialStructure,ContractandNegotiation,
alongwiththeparametersofExplorationareshowninTab.7.
D BaselineDetails
D.1 CurriculumLearning
WedevelopedacurriculumlearningalgorithmforAdaSociety. Thealgorithmcontrolsthesocialstate
topromotegroupcooperationandguidestheagenttolearnrationalphysicalpoliciesbeforelearning
socialpolicies. Ourcurriculumconsistsofthreestages. WeuseRecPPOforRLtrainingineach
stage.
In the first stage, all individual nodes are compelled to connect to the same group node. This
arrangementensuresthatallagentsbelongtothesamegroup. Iftherewardassignedtoanindividual
increasesmonotonicallywithrespecttothegroupreward,whichisacommonsetting,agentsinthis
stageoptimizetheiractionstoenhancetheoverallbenefits. Thispracticalapproachencouragesthe
learningofcooperativepoliciesthatyieldhigherrewards,benefitingbothindividualsandthegroup.
Inthesecondstage,eachindividualnodeisforcedtoconnecttoaspecificgroupnodewithprobability
p ,whileitrandomlyconnectstoanyofthegroupnodeswithprobability1‚àíp . Thevalueof
K K
p graduallydecreaseswiththeepisodenumberK,resultinginthegradualemergenceofdiverse
K
socialstatestructures. Thissetupenablesagentstolearnphysicalpolicieswithdifferentsocialstates.
Duringthefirsttwostages,socialactionsarenotallowed,soagentsfocussolelyonlearningpolicies
relatedtophysicalactions.
20Table7: Parametersofmini-games. WhenaresourceœÅisnotspecified,icap(œÅ)defaultsto‚àûand
h (œÅ)defaultsto1.
i
Parameter Easy Hard Exploration
h,w 7,7 15,15 20,20
|B| 0 0 25
bpos - - random
œ± {wood,stone,hammer} {wood,stone,hammer,
allbuilt-inresources
coal,torch,iron}
E 41HammerCraft 98HammerCraft, 40HammerCraft,40TorchCraft,30SteelMaking,
98TorchCraft 30Potting,20ShovelCraft,20PickaxeCraft,
20CutterCraft,10GemCutting,10TotemMaking
Epos random random random
mi(0) empty empty empty
icap carpenter:{hammer:1} carpenter:{hammer:1,coal:0}
default
miner:{wood:0,stone:0} miner:{stone:0,torch:1,iron:0}
hi carpenter:default carpenter:{coal:5,torch:1.5,iron:20/3}
default
miner:{hammer:2} miner:{coal:5,torch:1.5,iron:20/3}
ipos(0) random random random
Finally,inthethirdstage,theagentgainsthefreedomtoperformallactionsdefinedinthescenario,
therebyacquiringacomprehensivepolicyforthegiventask.
D.2 LargeLanguageModelwithController
ConsideringthesocialnatureofEvoSociety,wealsotesttheLargeLanguageModelwithGPT4[1]
asexamples. TheLLMagentconsistsofthreemodules: observation,reasoning,andexecution. Inthe
observationmodule,wetransformthecomplexphysicalenvironmentinformationwithintheagent‚Äôs
field of view and the current social state into natural language form, merging it with the system
prompt including game rules and the agent‚Äôs tasks as inputs. In the reasoning module, the LLM
agentgeneratesahigh-levelplanthroughfew-shotlearning,wherewerequiretheagenttoprovide
onlylegalplansthatconformtothecurrentenvironmentintheprompt. Intheexecutionmodule,
wedecomposethehigh-levelplanintolow-levelatomicactionsthroughhandcraftedfunctionsfor
interactingwiththeenvironment. Theprocessrepeatswithanewreasoningsteptogenerateanew
planuntilthecurrentplaniscompleted. DuetotherandomnessofLLMsandtheuncontrollable
natureofmulti-agentinteractionsinEvoSociety,itispossibletogenerateunachievableplans,which
willbedetectedbyamonitoringfunction,promptingtheLLMtoregeneratetheplan.
D.3 ComputeResources
CPU:128Intel(R)Xeon(R)Platinum8369BCPU@2.90GHz;Totalmemory: 263729336kB.GPU:
8NVIDIAGeForceRTX3090;MemoryperGPU:24576MiB.EachRLbaselineexperimenttakes
12to48hours,dependingonthemini-gameandRLalgorithm. ForLLM-Cexperiments,eachagent
takesanaverageof5secondsperstep.
E AdditionalResults
ThissectionpresentstheevaluationresultsinSocialStructure,Contract,Negotiation,andthetestsof
variousbaselinesinExploration.
Table8: AverageindividualrewardinExploration,normalizedbytheOrcalereward. TraditionalRL
algorithmscanonlyexploretheearlierpartofthesynthesistree,resultinginpoorreturns.
PPO RecPPO MAPPO Rainbow Random
0.1744¬±0.0138 0.1697¬±0.0041 0.0420¬±0.0123 0.0051¬±0.0004 0.0001¬±0.0000
21 ≈ó «Ø ≈ñ  ≈ó «Ø ≈ñ  ≈ô «Ø ≈ñ  ≈ò «Ø ≈ñ
 ≈ñ «Ø ≈û  ≈ñ «Ø ≈û  ≈ò «Ø ≈ö  ≈ó «Ø ≈õ
 ≈ñ «Ø ≈ú  ≈ñ «Ø ≈ú  ≈ó «Ø ≈û
 ≈ó «Ø ≈ñ
 ≈ñ «Ø ≈ö  ≈ñ «Ø ≈ö  ≈ó «Ø ≈ò
 ≈ñ «Ø ≈ò  ≈ñ «Ø ≈ò  ≈ñ «Ø ≈ú  ≈ñ «Ø ≈õ
 ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ
 ≈ñ «Ø ≈ú
 ≈ö «Ø ≈ñ  ≈ö «Ø ≈ñ
 ≈ñ «Ø ≈û  ≈ñ «Ø ≈õ
 ≈ô «Ø ≈ò  ≈ô «Ø ≈ò
 ≈ñ «Ø ≈ú  ≈ñ «Ø ≈ö
 ≈ñ «Ø ≈ô  ≈ò «Ø ≈ö  ≈ò «Ø ≈ö
 ≈ñ «Ø ≈ö  ≈ñ «Ø ≈ò  ≈ó «Ø ≈ú  ≈ó «Ø ≈ú
 ≈ñ «Ø ≈ò  ≈ñ «Ø ≈ó  ≈ñ «Ø ≈û  ≈ñ «Ø ≈û
 ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ
(a)Fairness (b)Completionrate (c)Maxgroupdegree (d)Avg.groupdegree
Figure7: EvaluationresultsofContract. Upperrow: Easy;lowerrow: Hard.
 ≈ó «Ø ≈ñ  ≈ó «Ø ≈õ  ≈ö
 ≈ñ «Ø ≈õ  ≈ò «Ø ≈ñ  ≈ó «Ø ≈ò  ≈ñ ≈ñ ≈ñ  «Ø «Ø «Ø  ≈ö ≈ú ≈û  ≈ñ ≈ñ ≈ñ  «Ø «Ø «Ø  ≈ò ≈ô ≈ö  ≈ó ≈ó  «Ø «Ø  ≈ñ ≈õ  ≈ñ ≈ñ  «Ø «Ø  ≈ú ≈ü  ≈ò ≈ô
 ≈ñ «Ø ≈ò  ≈ñ «Ø ≈ó  ≈ñ «Ø ≈õ  ≈ñ «Ø ≈ô  ≈ó
 ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ
 ≈ó «Ø ≈ñ  ≈ö
 ≈ñ «Ø ≈ô  ≈ú  ≈ó «Ø ≈õ  ≈ñ «Ø ≈û  ≈ô
 ≈ñ «Ø ≈ú  ≈ñ «Ø ≈ò  ≈ö  ≈ó «Ø ≈ñ
 ≈ò
 ≈ñ «Ø ≈ö
 ≈ñ «Ø ≈ò  ≈ñ «Ø ≈ó  ≈ò  ≈ó  ≈ñ «Ø ≈õ
 ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ  ≈ñ «Ø ≈ñ          ¬é ¬å          ¬ä ¬í ¬ó ¬ã ¬ò ¬†  ¬ä ¬ó ¬ç ¬ò ¬ñ
(a)Fairness (b)Completionrate (c)Maxgroupdegree (d)Avg.groupdegree (e)C/Msplitratio
Figure8: EvaluationresultsofNegotiation. Upperrow: Easy;lowerrow: Hard.
0.8 1.0
0.8 0.6
0.6
0.4
0.4
0.2
0.2
0.0 0.0
CL PPO RecPPO CCPPORainbowrandom CL PPO RecPPO CCPPORainbowrandom
(a) (b)
Figure9: SocialStructure-Dynamic: (a)Fairnessofagentsusingdifferentmethods,(b)Completion
rateofeventsusingdifferentmethods.
14 0.12 1.0
12 0.8
10 PPO 00 .. 01 80 PPO 0.6 0.8
8 RecPPO RecPPO 0.6 6 Rainbow 0.06 Rainbow 0.4
4 Random 0.04 Random 0.4
2 0.02 0.2 0.2
0 0 25 50 75 St1 e0 p0 s125 150 175 200 0.00 0 8000 16 E0 p00 iso2 d4 e0 s00 32000 0.0 PPO RecPPO Rainbow random 0.0 PPO RecPPO Rainbow random
(a) (b) (c) (d)
Figure10:SocialStructure-Isolation:(a)Individualrewardsperstepusing100samplesfromdifferent
policies(b)Learningcurvesusingdifferentmethods. (c)Fairnessofagentsusingdifferentmethods,
(d)Completionrateofeventsusingdifferentmethods
22
 ¬é ¬õ ¬ò ¬å ¬ú »± ¬ú ¬ú ¬é ¬ó ¬õ ¬í ¬ä 
 ¬é ¬õ ¬ò ¬å ¬ú »± ¬ú ¬ú ¬é ¬ó ¬õ ¬í ¬ä 
 ¬é ¬õ ¬ò ¬å ¬ú »± ¬ú ¬ú ¬é ¬ó ¬õ ¬í ¬ä 
 ¬é ¬õ ¬ò ¬å ¬ú »± ¬ú ¬ú ¬é ¬ó ¬õ ¬í ¬ä 
sdraweR
laudividnI
erocs
ssenriaF
 ¬é ¬ù ¬ä ¬õ »± ¬ó ¬ò ¬í ¬ù ¬é ¬ï ¬ô ¬ñ ¬ò 
 ¬é ¬ù ¬ä ¬õ »± ¬ó ¬ò ¬í ¬ù ¬é ¬ï ¬ô ¬ñ ¬ò 
 ¬é ¬ù ¬ä ¬õ »± ¬ó ¬ò ¬í ¬ù ¬é ¬ï ¬ô ¬ñ ¬ò 
 ¬é ¬ù ¬ä ¬õ »± ¬ó ¬ò ¬í ¬ù ¬é ¬ï ¬ô ¬ñ ¬ò 
sdraweR
laudividnI
dezilamroN
 ¬é ¬é ¬õ ¬ê ¬é ¬ç »± ¬ñ ¬û ¬ñ ¬í ¬° ¬ä 
 ¬é ¬é ¬õ ¬ê ¬é ¬ç »± ¬ñ ¬û ¬ñ ¬í ¬° ¬ä 
 ¬é ¬é ¬õ ¬ê ¬é ¬ç »± ¬ñ ¬û ¬ñ ¬í ¬° ¬ä 
 ¬é ¬é ¬õ ¬ê ¬é ¬ç »± ¬ñ ¬û ¬ñ ¬í ¬° ¬ä 
etar
noitelpmoC
erocs
ssenriaF
 ¬é ¬é ¬õ ¬ê ¬é ¬ç »± ¬é ¬ê ¬ä ¬õ ¬é ¬ü 
 ¬é ¬é ¬õ ¬ê ¬é ¬ç »± ¬é ¬ê ¬ä ¬õ ¬é ¬ü 
 ¬é ¬é ¬õ ¬ê ¬é ¬ç »± ¬é ¬ê ¬ä ¬õ ¬é ¬ü 
 ¬é ¬é ¬õ ¬ê ¬é ¬ç »± ¬é ¬ê ¬ä ¬õ ¬é ¬ü 
etar
noitelpmoC
 ¬é ¬ù ¬ä ¬õ »±  »¶ 
 ¬é ¬ù ¬ä ¬õ »±  »¶ 14 0.12 1.0
12 0.8 10 PPO 00 .. 01 80 PPO 0.6 0.8
8 RecPPO RecPPO 0.6 6 Rainbow 0.06 Rainbow 0.4
4 Random 0.04 Random 0.4
2 0.02 0.2 0.2
0 0 25 50 75 St1 e0 p0 s125 150 175 200 0.00 0 8000 16 E0 p00 isod24 e0 s00 32000 0.0 PPO RecPPO Rainbow random 0.0 PPO RecPPO Rainbow random
(a) (b) (c) (d)
Figure11: SocialStructure-Connection: (a)Individualrewardsperstepusing100samplesfrom
differentpolicies(b)Learningcurvesusingdifferentmethods. (c)Fairnessofagentsusingdifferent
methods,(d)Completionrateofeventsusingdifferentmethods
PPO 0.8 1.0
40 RecPPO 0.35
30 R Ra ai nn db oo mw 00 .. 23 50 PPO 0.6 0.8
0.20 RecPPO 0.6 20 Rainbow 0.4
0.15 Random 0.4
10 0.10 0.2
0.05 0.2
0 0 25 50 75 St1 e0 p0 s125 150 175 200 0.00 0 8000 16 E0 p00 isod24 e0 s00 32000 0.0 PPO RecPPO Rainbow random 0.0 PPO RecPPO Rainbow random
(a) (b) (c) (d)
Figure12: SocialStructure-IndependentGroup: (a)Individualrewardsperstepusing100samples
fromdifferentpolicies(b)Learningcurvesusingdifferentmethods. (c)Fairnessofagentsusing
differentmethods,(d)Completionrateofeventsusingdifferentmethods
350 PPO 350 0.8 1.0
300 R Re ac inP bP oO w 300 0.8 250 Random 250 0.6
200 200 0.6
150 150 PPO 0.4 0.4
100 100 RecPPO
50 50 Rainbow 0.2 0.2
Random
0 0 25 50 75 Ste100 ps125 150 175 200 0 0 5000 100001500 E0 p200 i0 s0 o250 d00 e30 s00035000 0.0 PPO RecPPO Rainbow random 0.0 PPO RecPPO Rainbow random
(a) (b) (c) (d)
Figure13: SocialStructure-OverlappingGroup: (a)Individualrewardsperstepusing100samples
fromdifferentpolicies(b)Learningcurvesusingdifferentmethods. (c)Fairnessofagentsusing
differentmethods,(d)Completionrateofeventsusingdifferentmethods
50 PPO 1.0
40 R Re ac inP bP oO w 00 .. 33 05 0.8 0.8 30 Random 0.25 PPO 0.6 0.20 RecPPO 0.6
20 0.15 R Ra ai nn db oo mw 0.4 0.4
10 00 .. 01 50 0.2 0.2
0 0 25 50 75 St1 e0 p0 s125 150 175 200 0.00 0 8000 16 E0 p00 isod24 e0 s00 32000 0.0 PPO RecPPO Rainbow random 0.0 PPO RecPPO Rainbow random
(a) (b) (c) (d)
Figure 14: Social Structure-Inequality: (a) Individual rewards per step using 100 samples from
differentpolicies(b)Learningcurvesusingdifferentmethods. (c)Fairnessofagentsusingdifferent
methods,(d)Completionrateofeventsusingdifferentmethods
Table9: EvaluationresultsofLLM-Cacrossmini-games.
SocialStructure Contract Negotiation
Hard Easy Hard Easy Hard
Fairness 0.8356¬±0.0496 0.0000¬±0.0000 0.0000¬±0.0000 0.7046¬±0.1401 0.8143¬±0.0350
Completionrate(HammerCraft) 0.6583¬±0.0624 0.9333¬±0.0236 1.0000¬±0.0000 0.8833¬±0.1041 1.0000¬±0.0000
Completionrate(TorchCraft) 0.6833¬±0.1546 - 0.9167¬±0.1179 - 0.9167¬±0.1443
MaxGroupDegree 4 4.000¬±0.000 8.000¬±0.000 1.3333¬±0.5773 3.3333¬±0.5773
Avg.GroupDegree 4 4.000¬±0.000 8.000¬±0.000 1.1667¬±0.2886 1.625¬±0.6959
23
sdraweR
laudividnI
sdraweR
laudividnI
sdraweR
latoT
sdraweR laudividnI
sdraweR
laudividnI
dezilamroN
sdraweR
laudividnI
dezilamroN
sdraweR
latoT
sdraweR
laudividnI
dezilamroN
erocs
ssenriaF
erocs
ssenriaF
erocs
ssenriaF
erocs ssenriaF
etar
noitelpmoC
etar
noitelpmoC
etar
noitelpmoC
etar noitelpmoCTable10: Time(inhours)andthenumberofgamestepstakenbyalgorithmstoconverge.
Time(hours)|GameSteps CL PPO RecPPO MAPPO Rainbow
Negotiation-Easy 5.66¬±0.16|14M 0.12¬±0.03|0.47M 1.05¬±0.04|2.4M 21.21¬±0.35|1.8M 14.97¬±0.30|42M
Negotiation-Hard 12.96¬±0.39|15M 1.66¬±0.03|2.3M 0.30¬±0.01|0.49M 74.85¬±9.75|4.4M 40.54¬±0.54|53M
Contract-Easy 32.03¬±1.38|112M 0.19¬±0.03|0.80M 1.94¬±0.03|6.0M 9.65¬±0.02|1.2M 9.65¬±1.49|25M
Contract-Hard 48.58¬±0.56|78M 0.21¬±0.02|0.56M 0.74¬±0.01|1.0M 14.94¬±0.01|0.94M 19.07¬±1.25|37M
SocialStructure-Dynamic 6.98¬±0.64|4.8M 6.90¬±0.55|6.0M 10.16¬±0.12|6.0M 46.23¬±1.21|4.8M 2.34¬±33.76|2.0M
Table 11: Average number of game steps per second for different player counts (4, 8, 20, 100,
1000). Thenumberofgroupsisthesameasthenumberofplayers. Theexperimentisconductedin
Exploration,whereallbuilt-inresourcesandeventsareincluded(seedetailsinSec.C.2).
4p 8p 20p 100p 1000p
2495.58¬±26.33 1245.38¬±3.65 395.33¬±2.52 42.58¬±0.26 2.60¬±0.09
Table12: GrouprewardsinContractaftereachstageinCurriculumLearning.
Contract-Easy Contract-Hard
Stage1 0.9747¬±0.0059 0.6470¬±0.0313
Stage2 0.3435¬±0.0262 0.2566¬±0.0284
Stage3 0.9136¬±0.0023 0.2773¬±0.0466
24F BroaderImpact
Tocontributetothedevelopmentofmulti-agentdecision-makingalgorithms,weproposeAdaSociety,
acustomizableenvironmentwithmassiveanddiversetasksgeneratedbyexpandingstateandaction
spacesandadaptivesocialstructures. Duetothecomplexityoftasksandtheheterogeneityofagents‚Äô
capacities and preferences, agents need to team up and even cooperatively establish hierarchical
socialstructurestoachievegoals. However,agentsmayalsolearnsomestrategiesthatareharmfulto
theirco-players,asiscommoninmulti-agentresearch. Wehavemadesignificanteffortstomitigate
suchbehaviorsthroughthoughtfuldesignwithintheenvironment. Giventheheterogeneityamong
agentsandadaptivesocialstructures,harmfulbehaviorstendtobeshort-sightedandinferiorwhenit
comestomaximizinglong-termbenefits,withstablecooperationemergingastheoptimalstrategy.
ThemultipleevaluationmetricsintroducedinAdaSociety,likefairness,alsoempowerresearchers
to identify and exclude extreme or exploitative agents and facilitate the learning of cooperative
behaviors.
Nevertheless,someharmfulbehaviorsmaystillariseduringtraining. Weaskresearchersutilizing
ourplatformtometiculouslyobserveagents‚Äôbehaviorstoensuretheyalignwithhumanvaluesand
preferences. Shouldanymisalignmentormisrepresentationhappen,weencouragecontributionsto
thesourcecode(includingbutnotlimitedtonewevaluationmetrics,environmentaldynamicsor
incentivemechanisms)toenhancetheplatform.
25Checklist
1. Forallauthors...
(a) Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthepaper‚Äôs
contributionsandscope? [Yes]
(b) Didyoudescribethelimitationsofyourwork? [Yes]PleaseseeSec.7.
(c) Didyoudiscussanypotentialnegativesocietalimpactsofyourwork? [Yes]Pleasesee
AppendixF.
(d) Haveyoureadtheethicsreviewguidelinesandensuredthatyourpaperconformsto
them? [Yes]
2. Ifyouareincludingtheoreticalresults...
(a) Didyoustatethefullsetofassumptionsofalltheoreticalresults? [N/A]Wedonot
havetheoreticalanalysisandresults.
(b) Did you include complete proofs of all theoretical results? [N/A] We do not have
theoreticalanalysisandresults.
3. Ifyouranexperiments(e.g. forbenchmarks)...
(a) Didyouincludethecode,data,andinstructionsneededtoreproducethemainexper-
imentalresults(eitherinthesupplementalmaterialorasaURL)?[Yes]Pleasesee
abstract.
(b) Didyouspecifyallthetrainingdetails(e.g.,datasplits,hyperparameters,howthey
werechosen)? [Yes]Intheappendix.
(c) Didyoureporterrorbars(e.g.,withrespecttotherandomseedafterrunningexperi-
mentsmultipletimes)? [Yes]Wereporteveryerrorbars.
(d) Didyouincludethetotalamountofcomputeandthetypeofresourcesused(e.g.,type
ofGPUs,internalcluster,orcloudprovider)? [Yes]PleaseseeSec.D.3.
4. Ifyouareusingexistingassets(e.g.,code,data,models)orcurating/releasingnewassets...
(a) Ifyourworkusesexistingassets,didyoucitethecreators? [Yes]
(b) Didyoumentionthelicenseoftheassets? [Yes]Wehavecited[36],whoselicenseis
Apache-2.0
(c) DidyouincludeanynewassetseitherinthesupplementalmaterialorasaURL?[Yes]
(d) Didyoudiscusswhetherandhowconsentwasobtainedfrompeoplewhosedatayou‚Äôre
using/curating? [No]Wedonotuseanyhumandata,andweusetheopensourcecode
whichfollowsthelicense.
(e) Didyoudiscusswhetherthedatayouareusing/curatingcontainspersonallyidentifiable
informationoroffensivecontent? [No]Wedonotuseanyhumandata.
5. Ifyouusedcrowdsourcingorconductedresearchwithhumansubjects...
(a) Didyouincludethefulltextofinstructionsgiventoparticipantsandscreenshots,if
applicable? [N/A]Wedonotusecrowdsourcingorconductedresearchwithhuman
subjects.
(b) Did you describe any potential participant risks, with links to Institutional Review
Board(IRB)approvals,ifapplicable?[N/A]Wedonotusecrowdsourcingorconducted
researchwithhumansubjects.
(c) Didyouincludetheestimatedhourlywagepaidtoparticipantsandthetotalamount
spentonparticipantcompensation? [N/A]Wedonotusecrowdsourcingorconducted
researchwithhumansubjects.
26SupplementaryMaterial
OptimalEventExecutionsforCalculatingCompletionRate
Whenthesynthesistreebecomescomplicated,itisnotstraightforwardtocalculatethemaximum
potentialexecutionsforalltheevents,makingitdifficulttoevaluatetheperformancethroughthe
metricCompletionrate. Therefore,wedevelopanoptimizationformulationtocomputethenumber
ofeventexecutionsthatmaximizethecreditsobtainedbyagents. Thisoptimizationisformulatedina
single-agentsetting. Sinceitaimstoobtainmaximumpotentialcredits,multi-agentcasescanalsobe
appliedwiththesetofeventsbeingtheunionofagents‚Äôskills. Allnaturalresourcescaneventually
becollected. Tab.13showstheparametersandvariablesusedinthisoptimization.
Table13: Parametersandvariablesusedincreditoptimization.
KnownParameters Description
œ±=Rn‚à™Rs SetofresourcesincludingnaturalresourcesRnandsyntheticresourcesRs.
m Initialresources.midenotestheinitialnumberofnaturalresourcesinenvironments.
h hidenotesthequantitiesofcreditsthatanagentgetsbyacquiringresourcei.
E Setofevents.Notethat|E|=|Rs|
E SynthesismatrixE |E|√ó|œ±|.ElementEijrepresentsthenumberofresourcejusedtosynthesizeresourcei
Q Q:Rn‚Üí2Erepresentstherequiredoccurredeventstocollectcertainnaturalresources
P P :E‚ÜíRrepresentsthenumberofproducedresourcesbyperformingcertainevents
D D:E‚Üí2Erepresentstherequiredoccurredeventstoperformcertainevents.
DecisionVariables Description
r Finalresources.rœÅdenotesthenumberofleftresourceœÅinenvironments.
Œ± Œ±iisabinaryvariabledenotingthatnaturalresourcesœÅicanbecollected.
x Numberofoccurredevents.xidenotesthenumberofoccurredeventœµiinenvironments.
Œ≤ Œ≤iisabinaryvariabledenotingthateventœµihasoccurredinenvironments.
(cid:88) (cid:88)
max r h Œ± + r h Œ≤ (5a)
i i i i i i
r,x,Œ±,Œ≤
i‚ààRn i‚ààRs
(cid:88)
s.t. r =m ‚àí x E ,‚àÄi‚ààR , (5b)
i i j ji n
j‚ààE
(cid:88)
r =P(i)‚àí x E ,‚àÄi‚ààR , (5c)
i j ji s
j‚ààE
Œ± ‚â§x ,‚àÄi‚ààR ,j ‚ààQ(i) (5d)
i j n
Œ≤ ‚â§x ,‚àÄi‚ààE (5e)
i i
x ‚â§Œ≤ M,‚àÄi‚ààE,j ‚ààD(i) (5f)
i j
x ‚ààN,Œ±‚àà{0,1},Œ≤ ‚àà{0,1} (5g)
i
Eq.5presentstheoptimizationformulation,whereEq.5acalculatesthetotalcreditsgainedbythe
agentscollectingandsynthesizingresources;Eq.5bandEq.5crepresenttheeventuallyleftnatural
resourcesandsyntheticresourcesafterexecutingevents;Eq.5dindicatetherequiredeventstocollect
certainresources;Eq.5eindicatewhetheratypeofeventhasoccurredornot;Eq.5fstatetherequired
eventstoexecutecertainevents;Eq.5glimitthevaluesofdecisionvariables.
ExamplePromptforLLM-C
ThefollowingexamplesillustratethepromptsusedinLLM-Cforeachmini-game. Theprompts
vary slightly for different mini-games and also differ across stages within the same mini-game.
Specifically,thepromptforthedynamicscenarioinSocialStructureispresentedinListing1. For
thecontractformationstageinContract,thepromptisdisplayedinListing2. Similarly,theprompt
forthenegotiationstageinNegotiationcanbefoundinListing3. ThephysicalstageforContract
andthatforNegotiationarethesame. Therearetwophysicalstagesettings,featuringdifferentlevels
ofdifficulty. ThecorrespondingpromptsareprovidedinListing4andListing5.
27Listing1: PromptexamplefordynamicscenarioinSocialStructure.
Instructions:
- The AdaSociety game is an open-ended multi-agent environment. The game consists of
a complex crafting tree, where the agent needs to obtain as many resources as
possible in the limited time and craft tools to mine more advanced resources to
maximize its benefit. At the same time, agents can also take other actions to
help them increase their returns. The numbers of resources are limited.
- Map: AdaSociety is a 2D grid-world game. The map size is 15*15.
- Natural resources: [Wood, Stone, Coal, Iron]. Some of them can only be
discovered with some specific tools, which will be introduced next.
- Tools: [Hammer, Torch]
- Craft tree:
- 1 Wood + 1 Stone = 1 Hammer. With a Hammer, Coal can be gathered;
- 1 Coal + 1 Wood = 1 Torch. With a Torch, Iron can be discovered;
- All gathered and tools are stored in the agent‚Äôs inventory.
- All crafts must be done on the corresponding event grid on the map. For
example, your inventory must contain wood and stone to craft a hammer.
- Default amount of all units in crafts is 1.
- Player:
- There are two kinds of player in the AdaSociety, Carpenters and Miners.
- The Carpenter can gather many woods, stones and irons and craft hammer, but
can only own one hammer. The Carpenter CANNOT gather coal so it CANNOT craft
torch, but its inventory can hold a lot of torches.
- The Miner can gather many woods and coals, so it can craft torch, but can only
own one torch. The Miner CANNOT gather stone so it CANNOT craft hammer, but
its inventory can hold a lot of hammers.
- For all players, the value of wood is 1, the value of stone is 1, the value of
hammer is 5, the value of coal is 10, the value of torch is 30, the value
of iron is 20.
- Different players may be placed in the same coalition, and the rewards for
players in the same coalition are split equally, so given the heterogeneity
between carpenter and miner, players in the same coalition need to cooperate.
Suppose you are a Carpenter named <carpenter_0>. Your aim is to maximize your reward
, which can gain from the resource value and the craft value.
You can not craft torchs, but you can craft hammers.
At each round in action phase, you will receive the current state:
Step: ...
Current surrounding social environment: ...
Current surrounding physical environment: ...
Your current inventory: ...
You should choose *ONLY ONE* Plan from the following four options: [GATHER <NUM> <
WOOD/STONE/IRON/TORCH>, CRAFT 1 HAMMER, EXPLORE MAP, DUMP HAMMER]. Here are
explanations about them:
- GATHER <NUM> <WOOD/STONE/IRON/TORCH>: You shouldn‚Äôt try to gather items that aren‚Äô
t in your field of view because you don‚Äôt know where they are. You should also
not try to gather item that are not in <WOOD/STONE/IRON/TORCH>. You can only
choose one type of item in your plan.
- CRAFT 1 HAMMER: This plan can help you use the items in your inventory and follow
the craft tree to craft the resources or tools you need. You can only use this
plan if you have the corresponding event grid (i.e. the craft point) in your
view. You should make sure you have enough material to craft.
- EXPLORE MAP: This plan helps you move randomly to explore the map.
- DUMP HAMMER: The plan is to drop hammers on the ground because some agents have
hammer‚Äôs capacity of only 1. This action will decrease the corresponding item
in the inventory by 1. If the item is not in the inventory, please do not
choose this plan.
<NUM> should be an integer not greater than 10.
Please strictly follow the format above for the output.
28The response should obey the following format:
Thoughts: Your analysis about your inventory and the current environment.
Plan: One of the above four plans you will take.
Examples:
###
Step: 20
Current surrounding physical environment:
The resources in your observation are: [5 Wood, 5 Stone]. The distances of them are
[4,6] steps away.
The event grid in your observation are: [Hammer Event]. The distances of them are
[3] steps away.
You have nothing in your inventory.
Thoughts: I don‚Äôt have anything in my inventory. There are 5 woods and 5 stones in
my observation, the wood is closer to me, so I need to gather some wood first.
Plan: GATHER 5 WOOD.
###
Step: 40
Current surrounding physical environment:
The resources in your observation are: [2 Wood, 4 Stone]. The distances of them are
[8,10] steps away.
The event grid in your observation are: [Hammer Event]. The distances of them are
[3] steps away.
Your current inventory:
You have 4 Wood, 6 Stone, 0 Hammer.
Thoughts: I have some woods and stones in my inventory but no hammer. There is a
hammer event in my observation, which means I can craft the hammer.
Plan: CRAFT 1 HAMMER.
###
Step: 60
Current surrounding physical environment:
The resources in your observation are: [1 Wood, 3 Stone]. The distances of them are
[5,2] steps away.
The event grid in your observation are: [Hammer Event]. The distances of them are
[3] steps away.
Your current inventory:
You have 2 Wood, 3 Stone, 1 Hammer.
Thoughts: I have some woods and stones, and one hammer in my inventory. Accounting
for my inventory can only hold one hammer, and there are two miners in my
coalition who can hold lots hammers, I should dump my hammer to let my
teammates pick it up, and craft a new one later.
Plan: DUMP HAMMER.
###
Step: 80
Current surrounding physical environment:
The resources in your observation are: [2 Wood, 1 Torch]. The distances of them are
[2,4] steps away.
The event grid in your observation are: [Hammer Event]. The distances of them are
[3] steps away.
Your current inventory:
You have 2 Wood, 3 Stone, 1 Hammer.
Thoughts: I have some woods and stones, and one hammer in my inventory but no torch.
Torch is most valuable tool for me, and my inventory can hold a lot of torches,
so I need to gather the torch on the map.
Plan: GATHER 1 TORCH.
###
Step: 90
Current surrounding physical environment:
29The resources in your observation are: []. The distances of them are [] steps away.
The numbers of them are [] respectively.
The event grid in your observation are: [Hammer Event]. The distances of them are
[0] steps away.
The people in your observation are: [miner_0], The distances of them are [1] steps
away.
Your current inventory:
You have NOTHING in your inventory.
Thoughts: I am carpenter_0, and in the current coalition, there are both carpenters
and miners. The hammercraft event is right next to me. Since I have no wood and
no stone, I can also not craft hammer. I don‚Äôt see any resource in my field of
view, so I need to explore the map to find one.
Plan: EXPLORE MAP.
Listing2: PromptexampleforthecontractformationstageinContract.
Instructions:
- The AdaSociety game is an open-ended multi-agent environment. The game consists of
a complex crafting tree, where the agent needs to obtain as many resources as
possible in the limited time and craft tools to mine more advanced resources to
maximize its benefit. At the same time, agents can also take other actions to
help them increase their returns, such as negotiating with others to exchange
resources they need, or forming groups with others to share information and
rewards.
- Map: AdaSociety is a 2D grid-world game. The map size is 15*15.
- Natural resources: [Wood, Stone, Coal, Iron]. Some of them can only be
discovered with some specific tools, which will be introduced next.
- Tools: [Hammer, Torch]
- Craft tree:
- 1 Wood + 1 Stone = 1 Hammer. With a Hammer, Coal can be gathered;
- 1 Coal + 1 Wood = 1 Torch. With a Torch, Iron can be discovered;
- All gathered and tools are stored in the agent‚Äôs inventory.
- All crafts must be done on the corresponding event grid on the map. For
example, a Hammer can be crafted ONLY on <Hammer Event>.
- Default amount of all units in crafts is 1.
- for carpenter, the value of wood is 1, the value of stone is 1, the value of
hammer is 5, the value of coal is 10, the value of torch is 30, the value of
iron is 20.
- for miner, the value of wood is 1, the value of stone is 1, the value of
hammer is 5, the value of coal is 10, the value of torch is 30, the value of
iron is 20.
- Player:
- carpenter_0: You can pick many woods, stones and irons. You can not pick coal.
You can own many torchs. Your own inventory can ONLY own 1 hammer.
- carpenter_1: You can pick many woods, stones and irons. You can not pick coal.
You can own many torchs. Your own inventory can ONLY own 1 hammer.
- carpenter_2: You can pick many woods, stones and irons. You can not pick coal.
You can own many torchs. Your own inventory can ONLY own 1 hammer.
- carpenter_3: You can pick many woods, stones and irons. You can not pick coal.
You can own many torchs. Your own inventory can ONLY own 1 hammer.
- miner_0: You can pick many woods and coals. You can not pick stone and iron.
You can own many hammers. Your own inventory can ONLY own 1 torch.
- miner_1: You can pick many woods and coals. You can not pick stone and iron.
You can own many hammers. Your own inventory can ONLY own 1 torch.
- miner_2: You can pick many woods and coals. You can not pick stone and iron.
You can own many hammers. Your own inventory can ONLY own 1 torch.
- miner_3: You can pick many woods and coals. You can not pick stone and iron.
You can own many hammers. Your own inventory can ONLY own 1 torch.
Suppose you are a player named <carpenter_0> in the AdaSociety game. You are now in
the contract phase. Your aim is to maximize your reward, which can gain from
the resource value and the craft value.
People in a coalition share the rewards equally.
30At each round in contract phase, you will receive the current state:
Step: ...
Round: ...
Current surrounding social environment: ...
Information: ...
In contract phase, you should respond to me with
Thoughts: (Your analysis to the current state)
Action: (About which coalition you want to join)
The <Action> can ONLY be chosen from the following options:
1. I want to join in Coalition 0.
2. I want to join in Coalition 1.
3. I want to join in Coalition 2.
4. I want to join in Coalition 3.
5. I want to join in Coalition 4.
6. I want to join in Coalition 5.
7. I want to join in Coalition 6.
8. I want to join in Coalition 7.
Please strictly follow the format above for the output.
Examples:
###
Step: 8
Round: 2
Current surrounding social environment:
coalition 0:[miner_1, carpenter_0, miner_0, carpenter_1, miner_2, miner_3,
carpenter_2,carpenter_3].
coalition 1:None.
coalition 2:None.
coalition 3:None.
Coalition 4: None
Coalition 5: None
Coalition 6: None
Coalition 7: None
Information: It‚Äôs carpenter_0‚Äôs turn.
Thoughts: In this round, all players are currently in Coalition 0. As a carpenter, I
pick wood and stone but can not own many hammers, while hammer has higher
reward. Joining a coalition with miners might be beneficial for me to from
their ability to own more hammers to maximize rewards. Since miners are already
in Coalition 0, I choose to join in Coalition 0.
Action: I want to join in Coalition 0.
Listing3: PromptexampleforthenegotiationstageinNegotiation.
Instructions:
- The AdaSociety game is an open-ended multi-agent environment. The game consists of
a complex crafting tree, where the agent needs to obtain as many resources as
possible in the limited time and craft tools to mine more advanced resources to
maximize its benefit. At the same time, agents can also take other actions to
help them increase their returns, such as negotiating with others to exchange
resources they need, or forming groups with others to share information and
rewards.
- Map: AdaSociety is a 2D grid-world game. The map size is 15*15.
- Natural resources: [Wood, Stone, Coal, Iron]. Some of them can only be
discovered with some specific tools, which will be introduced next.
- Tools: [Hammer, Torch]
- Craft tree:
- 1 Wood + 1 Stone = 1 Hammer. With a Hammer, Coal can be gathered;
- 1 Coal + 1 Wood = 1 Torch. With a Torch, Iron can be discovered;
- All gathered and tools are stored in the agent‚Äôs inventory.
- All crafts must be done on the corresponding event grid on the map. For
example, a Hammer can be crafted ONLY on <Hammer Event>.
- Default amount of all units in crafts is 1.
31- for carpenter, the value of wood is 1, the value of stone is 1, the value of
hammer is 5, the value of coal is 10, the value of torch is 30, the value of
iron is 20.
- for miner, the value of wood is 1, the value of stone is 1, the value of
hammer is 5, the value of coal is 10, the value of torch is 30, the value of
iron is 20.
- Player:
- carpenter_0: You can pick many woods, stones and irons. You can not pick coal.
You can own many torchs. Your own inventory can ONLY own 1 hammer.
- carpenter_1: You can pick many woods, stones and irons. You can not pick coal.
You can own many torchs. Your own inventory can ONLY own 1 hammer.
- carpenter_2: You can pick many woods, stones and irons. You can not pick coal.
You can own many torchs. Your own inventory can ONLY own 1 hammer.
- carpenter_3: You can pick many woods, stones and irons. You can not pick coal.
You can own many torchs. Your own inventory can ONLY own 1 hammer.
- miner_0: You can pick many woods and coals. You can not pick stone and iron.
You can own many hammers. Your own inventory can ONLY own 1 torch.
- miner_1: You can pick many woods and coals. You can not pick stone and iron.
You can own many hammers. Your own inventory can ONLY own 1 torch.
- miner_2: You can pick many woods and coals. You can not pick stone and iron.
You can own many hammers. Your own inventory can ONLY own 1 torch.
- miner_3: You can pick many woods and coals. You can not pick stone and iron.
You can own many hammers. Your own inventory can ONLY own 1 torch.
Suppose you are a player named <carpenter_0> in the AdaSociety game. You are now in
the first phase: negotiation phase. Your aim is to maximize your reward, which
can gain from the resource value and the craft value.
Join the coalition to share profits with other members according to the agreed-upon
distribution ratio.
At each round in negotiation phase, you will receive the current state:
Step: ...
Current surrounding social environment: Specify within {} that it is in an coalition
.
NegoState: Indicate within [] that two people are negotiating.
Communication log: ...
In negotiation phase, you should respond to me with
Thoughts: (Your analysis to the current state)
Communication: (About who to negotiate with or how to allocate the rewards)
The <Communication> can ONLY be chosen from the following options:
1. End. I chose to end this bargain.
2. Accept <PLAYER_NAME>‚Äôs proposal. I will gain <NUM>% reward and <PLAYER_NAME>
will gain <NUM>% reward.
3. I will make a new proposal. I will propose that I gain <NUM>% reward and <
PLAYER_NAME> will gain <NUM>% reward.
4. I will negotiate with <PLAYER_NAME>. I will propose that I gain <NUM>% reward
and <PLAYER_NAME> will gain <NUM>% reward.
- <PLAYER_NAME> should be from other player names‚Äô set: [carpenter_1, carpenter_2,
carpenter_3, miner_0, miner_1, miner_2, miner_3]
- <NUM> should be an integer which is multiples of ten and is not greater than 100.
Please strictly follow the format above for the output.
!!!If you are in an coalition with someone, you cannot negotiate with them!!!
Examples:
###
Step: 1
Current surrounding social environment:
[{‚Äôcarpenter_0‚Äô}, {‚Äôcarpenter_1‚Äô}, {‚Äôcarpenter_2‚Äô}, {‚Äôcarpenter_3‚Äô}, {‚Äôminer_0‚Äô}, {‚Äô
miner_1‚Äô}, {‚Äôminer_2‚Äô}, {‚Äôminer_3‚Äô}]
NegoState:
None.
Communication log:
None.
32Thoughts: I am carpenter_0. miners can craft torch but I can‚Äôt. As a carpenter, I
can pick many woods and stones but can only own 1 hammer. miners have a higher
value for hammers. I have a higher value for torchs. I should negotiate with
miner to maximize my reward.
Communication: I will negotiate with miner_0. I will propose that I gain 40% reward
and miner_0 will gain 60% reward.
###
Step 4:
Current surrounding social environment:
[{‚Äôcarpenter_0‚Äô}, {‚Äôcarpenter_1‚Äô}, {‚Äôcarpenter_2‚Äô}, {‚Äôcarpenter_3‚Äô}, {‚Äôminer_0‚Äô}, {‚Äô
miner_1‚Äô}, {‚Äôminer_2‚Äô}, {‚Äôminer_3‚Äô}]
NegoState:
[‚Äôcarpenter_1‚Äô, ‚Äôminer_0‚Äô],
Communication log:
None
Thoughts: I am carpenter_0. Both miner_0 and carpenter_1 are currently negotiating
with each other. I can negotiate with miner except for miner_0.
Communication: I will negotiate with miner_1. I will propose that I gain 40% reward
and miner_0 will gain 60% reward.
###
Step: 10
Current surrounding social environment:
[{‚Äôcarpenter_0‚Äô}, {‚Äôcarpenter_1‚Äô}, {‚Äôcarpenter_2‚Äô}, {‚Äôcarpenter_3‚Äô}, {‚Äôminer_0‚Äô}, {‚Äô
miner_1‚Äô}, {‚Äôminer_2‚Äô}, {‚Äôminer_3‚Äô}]
NegoState:
(carpenter_0,miner_0)
Communication log:
miner_0 want to gain 40% reward and you will gain 60% reward.
Thoughts: I am carpenter_0. I‚Äôm in negotiate state with miner_0. I can get 60% of
the reward, which sounds like a good deal and I can accept it.
Communication: Accept miner_0‚Äôs proposal. I will gain 60% reward and miner_0 will
gain 40% reward.
Listing4: PromptexamplefortheEasytask.
Instructions:
- The AdaSociety game is an open-ended multi-agent environment. The game consists of
a complex crafting tree, where the agent needs to obtain as many resources as
possible in the limited time and craft tools to mine more advanced resources to
maximize its benefit. At the same time, agents can also take other actions to
help them increase their returns. The numbers of resources are limited.
- Map: AdaSociety is a 2D grid-world game. The map size is 7*7.
- Natural resources: [Wood, Stone].
- Tools: [Hammer]
- Craft tree:
- 1 Wood + 1 Stone = 1 Hammer
- All gathered resources and tools are stored in the agent‚Äôs inventory.
- When there are enough resources in the inventory, you can use the CRAFT <TOOL>
action to synthesize the corresponding tools. For example, your inventory
must contain wood and stone to craft a hammer.
- All crafts must be done on the corresponding event grid on the map. For
example, a Hammer can be crafted ONLY on <Hammer Event>.
- Default amount of all units in crafts is 1.
- for carpenter, the value of wood is 1, the value of stone is 1, the value of
hammer is 5.
- for miner, the value of wood is 1, the value of stone is 1, the value of
hammer is 10.
- Player:
- carpenter_0: can own many woods and stones but can own ONLY own 1 hammer in
inventory.
- carpenter_1: can own many woods and stones but can own ONLY own 1 hammer in
inventory.
33- miner_0: can NOT own wood and stone, buy can own many hammers in inventory.
- miner_1: can NOT own wood and stone, buy can own many hammers in inventory.
Suppose you are a player named <carpenter_0> in the AdaSociety game. Your aim is to
maximize your reward, which can gain from the resource value and the craft
value.
Join the coalition to share profits with other members according to the agreed-upon
distribution ratio.
At each round in action phase, you will receive the current state:
Step: ...
Current surrounding social environment: ...
payoff: The proportion of the split, shared within an coalition.
Current surrounding physical environment: ...
Your current inventory: ...
In action phase, You should respond to me with
Thoughts: (Your analysis to the current state)
Plan: (The action you plan to take)
You should choose *ONLY ONE* Plan from the following four options: [GATHER <NUM> <
RESOURCE>, CRAFT 1 <TOOL>, EXPLORE MAP, DUMP <TOOL>]. Here are explanations
about them:
- GATHER <NUM> <RESOURCE>: RESOURCE is chosen from the Natural resource list above.
You shouldn‚Äôt try to gather resources that aren‚Äôt in your field of view because
you don‚Äôt know where they are. You should also not try to gather resources
that are not natural resources.
- CRAFT 1 <TOOL>: TOOL is chosen from the Tools list above. This plan can help you
use the items in your inventory and follow the craft tree to craft the
resources or tools you need. You can only use this plan if you have the
corresponding event grid (i.e. the craft point) in your view. You should make
sure you have enough material to craft.
- EXPLORE MAP: This plan helps you move randomly to explore the map.
- DUMP <TOOL>: TOOL is chosen from the Tools list above. The plan is to drop tools
on the ground because some agents have a tool capacity of only 1. This action
will decrease the corresponding item in the inventory by 1. If the item is not
in the inventory, please do not choose this plan.
<NUM> should be an integer not greater than 10.
Please strictly follow the format above for the output.
!!!Before making your crafting choice, please carefully check your inventory to
ensure you have the necessary materials for crafting. And ensure that the tools
in the inventory are fewer than the tool capacity. If there are excess tools,
they should be discarded before crafting new tools. Random crafting selections
are not allowed!!!
!!!If your inventory don‚Äôt have hammers, please not dump hammers!!!
!!!craft hammer must need stone and wood, both stone and wood are indispensable.!!!
Examples:
###
Step: 50
Current surrounding social environment:
[{‚Äôcarpenter_0‚Äô, ‚Äôcarpenter_1‚Äô, ‚Äôminer_0‚Äô, ‚Äôminer_1‚Äô}]
Current surrounding physical environment:
The resources in your observation are: [Wood, Stone]. The distances of them are
[5,4] steps away. The numbers of them are [5,4] respectively.
The event grid in your observation are: [Hammer Event]. The distances of them are
[0] steps away.
The people in your observation are: [miner_1], The distances of them are [1] steps
away.
Your current inventory:
You have 3 wood.
Thoughts: I‚Äôm carpenter_0, and I currently have 3 woods in my inventory. In my
observation, there is wood and stone nearby, which I can gather. The
Hammercraft event is also close by, allowing me to craft a hammer. But I hanve
34no enough material to craft hammer, so I need to gather resources. Since I have
3 woods, so I need to gather 3 stones.
Plan: GATHER 3 STONE.
###
Step: 90
Current surrounding social environment:
[{‚Äôcarpenter_0‚Äô,‚Äôminer_1‚Äô}, {‚Äôcarpenter_1‚Äô, ‚Äôminer_0‚Äô}]
Current surrounding physical environment:
The resources in your observation are: [Wood, Stone]. The distances of them are
[5,4] steps away. The numbers of them are [4,5] respectively.
The event grid in your observation are: [Hammer Event]. The distances of them are
[3] steps away.
The people in your observation are: [miner_0, miner_1], The distances of them are
[3,1] steps away.
Your current inventory:
You have 4 wood, 6 Stone.
Thoughts: I‚Äôm carpenter_0. I have 4 wood, 6 Stone. I am in a coalition with both
carpenters and miners. The resources available are wood and stone, both of
which are nearby. The hammercraft event is right next to me, allowing me to
craft a hammer. I currently have more than 1 wood and more than 1 stone in my
inventory, which is enough to craft a hammer. I have no hammers in the
inventory. I choose to craft hammer.
Plan: CRAFT 1 HAMMER.
###
Step: 90
Current surrounding social environment:
[{‚Äôcarpenter_0‚Äô,‚Äôminer_1‚Äô}, {‚Äôcarpenter_1‚Äô, ‚Äôminer_0‚Äô}]
Current surrounding physical environment:
The resources in your observation are: [Wood, Stone]. The distances of them are
[5,4] steps away. The numbers of them are [4,5] respectively.
The event grid in your observation are: [Hammer Event]. The distances of them are
[3] steps away.
The people in your observation are: [miner_0, miner_1], The distances of them are
[3,1] steps away.
Your current inventory:
You have 4 wood, 6 Stone, 1 hammer.
Thoughts: I‚Äôm carpenter_0. I have 4 wood, 6 Stone, 1 hammer. I am in a coalition
with both carpenters and miners. The resources available are wood and stone,
both of which are nearby. The hammercraft event is right next to me, allowing
me to craft a hammer. I currently have more than 1 wood and more than 1 stone
in my inventory, which is enough to craft a hammer. Since I have one hammer and
miner_1 who is also in my coalition is closer to me than miner0 in my
observation, I should consider discarding my current hammer before crafting a
new one to maximize the coalition‚Äôs rewards, which miner_1 can pick the hammer
and if miner_1 own hammer, it will gain more rewards than I own hammers.
Plan: DUMP HAMMER.
Listing5: PromptexamplefortheHardtask.
Instructions:
- The AdaSociety game is an open-ended multi-agent environment. The game consists of
a complex crafting tree, where the agent needs to obtain as many resources as
possible in the limited time and craft tools to mine more advanced resources to
maximize its benefit. At the same time, agents can also take other actions to
help them increase their returns. The numbers of resources are limited.
- Map: AdaSociety is a 2D grid-world game. The map size is 15*15.
- Natural resources: [Wood, Stone, Coal, Iron]. Some of them can only be
discovered with some specific tools, which will be introduced next.
- Tools: [Hammer, Torch]
- Craft tree:
- 1 Wood + 1 Stone = 1 Hammer. With a Hammer, Coal can be gathered;
35- 1 Coal + 1 Wood = 1 Torch. With a Torch, Iron can be discovered;
- All gathered and tools are stored in the agent‚Äôs inventory.
- All crafts must be done on the corresponding event grid on the map. For
example, your inventory must contain wood and stone to craft a hammer.
- Default amount of all units in crafts is 1.
- for carpenter, the value of wood is 1, the value of stone is 1, the value of
hammer is 5, the value of coal is 10, the value of torch is 30, the value of
iron is 20.
- for miner, the value of wood is 1, the value of stone is 1, the value of
hammer is 5, the value of coal is 10, the value of torch is 30, the value of
iron is 20.- Player:
- carpenter_0: You can gather many woods, stones and irons. You can not gather
coal. You can own many torchs. Your own inventory can ONLY own 1 hammer.
- carpenter_1: You can gather many woods, stones and irons. You can not gather
coal. You can own many torchs. Your own inventory can ONLY own 1 hammer.
- carpenter_2: You can gather many woods, stones and irons. You can not gather
coal. You can own many torchs. Your own inventory can ONLY own 1 hammer.
- carpenter_3: You can gather many woods, stones and irons. You can not gather
coal. You can own many torchs. Your own inventory can ONLY own 1 hammer.
- miner_0: You can gather many woods and coals. You can not gather stone and
iron. You can own many hammers. Your own inventory can ONLY own 1 torch.
- miner_1: You can gather many woods and coals. You can not gather stone and
iron. You can own many hammers. Your own inventory can ONLY own 1 torch.
- miner_2: You can gather many woods and coals. You can not gather stone and
iron. You can own many hammers. Your own inventory can ONLY own 1 torch.
- miner_3: You can gather many woods and coals. You can not gather stone and
iron. You can own many hammers. Your own inventory can ONLY own 1 torch.
Suppose you are a player named <carpenter_0> in the AdaSociety game. You are now in
the action phase. Your aim is to maximize your reward, which can gain from the
resource value and the craft value.
You can not craft torchs, but you can craft hammers.
Join the coalition to share profits with other members according to the agreed-upon
distribution ratio.
At each round in action phase, you will receive the current state:
Step: ...
Current surrounding social environment: ...
payoff: The proportion of the split, shared within an coalition.
Current surrounding physical environment: ...
Your current inventory: ...
In action phase, You should respond to me with
Thoughts: (Your analysis to the current state)
Plan: (The action you plan to take)
You should choose *ONLY ONE* Plan from the following four options: [GATHER <NUM> <
WOOD/STONE/IRON/TORCH>, CRAFT 1 HAMMER, EXPLORE MAP, DUMP HAMMER]. Here are
explanations about them:
- GATHER <NUM> <WOOD/STONE/IRON/TORCH>: You shouldn‚Äôt try to gather items that aren‚Äô
t in your field of view because you don‚Äôt know where they are. You should also
not try to gather item that are not in <WOOD/STONE/IRON/TORCH>. You can only
choose one type of item in your plan.
- CRAFT 1 HAMMER: This plan can help you use the items in your inventory and follow
the craft tree to craft the resources or tools you need. You can only use this
plan if you have the corresponding event grid (i.e. the craft point) in your
view. You should make sure you have enough material to craft.
- EXPLORE MAP: This plan helps you move randomly to explore the map.
- DUMP HAMMER: The plan is to drop hammers on the ground because some agents have
hammer‚Äôs capacity of only 1. This action will decrease the corresponding item
in the inventory by 1. If the item is not in the inventory, please do not
choose this plan.
<NUM> should be an integer not greater than 10.
Please strictly follow the format above for the output.
36!!!Before making your crafting choice, please carefully check your inventory to
ensure you have the necessary materials for crafting. And ensure that the tools
in the inventory are fewer than the tool capacity. If there are excess tools,
they should be discarded before crafting new tools. Random crafting selections
are not allowed!!!
!!!If your inventory don‚Äôt have hammers, please do not dump hammers!!!
!!!craft hammer must need stone and wood, both stone and wood are indispensable.!!!
Examples:
###
Step: 50
Current surrounding social environment:
[{carpenter_0, carpenter_1, carpenter_2, carpenter_3, miner_0, miner_1, miner_2,
miner_3}].
payoff: 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2
Current surrounding physical environment:
The resources in your observation are: [Wood, Stone]. The distances of them are
[5,4] steps away. The numbers of them are [5,4] respectively.
The event grid in your observation are: [Hammer Event]. The distances of them are
[0] steps away.
The people in your observation are: [miner_1], The distances of them are [1] steps
away.
Your current inventory:
You have 3 wood.
Thoughts: I‚Äôm carpenter_0, and I currently have 3 woods in my inventory. In my
observation, there is wood and stone nearby, which I can gather. The
Hammercraft event is also close by, allowing me to craft a hammer. But I hanve
no enough material to craft hammer, so I need to gather resources. Since I have
3 woods, so I need to gather 3 stones.
Plan: GATHER 3 STONE.
###
Step: 90
Current surrounding social environment:
[{‚Äôcarpenter_0‚Äô, ‚Äôminer_1‚Äô}, {‚Äôcarpenter_1‚Äô, ‚Äôminer_1‚Äô,‚Äôminer_2‚Äô, ‚Äôcarpenter_2‚Äô, ‚Äô
miner_3‚Äô, ‚Äôcarpenter_3‚Äô}]
payoff: 0.6, 0.1, 0.1, 0.2, 0.2, 0.4, 0.2, 0.2
Current surrounding physical environment:
The resources in your observation are: [Wood, Stone]. The distances of them are
[5,4] steps away. The numbers of them are [5,4] respectively.
The event grid in your observation are: [Hammer Event]. The distances of them are
[0] steps away.
The people in your observation are: [miner_1], The distances of them are [1] steps
away.
Your current inventory:
You have 4 wood, 6 Stone, 1 hammer.
Thoughts: I‚Äôm carpenter_0. In my coalition, there are mostly stones and a minority
of wood. I can craft hammer heads first to help the coalition gain greater
profits. miner_1 is in my coalition and he is closer than other people in order
to my hammer don‚Äôt be gathered by other coalition, miner own hammers can bring
more rewards to the coalition, so I will dump hammer.
Plan: DUMP HAMMER.
###
Step: 50
Current surrounding social environment:
[{carpenter_0, carpenter_1, carpenter_2, carpenter_3, miner_0, miner_1, miner_2,
miner_3}].
payoff: 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2
Current surrounding physical environment:
The resources in your observation are: []. The distances of them are [] steps away.
The numbers of them are [] respectively.
37The event grid in your observation are: [Hammer Event]. The distances of them are
[0] steps away.
The people in your observation are: [miner_0], The distances of them are [1] steps
away.
Your current inventory:
You have NOTHING in your inventory.
Thoughts: I am carpenter_0, and in the current coalition, there are both carpenters
and miners. The hammercraft event is right next to me. Since I have no wood and
no stone, I can also not craft hammer. I don‚Äôt see any resource in my field of
view, so I need to explore the map to find one.
Plan: EXPLORE MAP.
38