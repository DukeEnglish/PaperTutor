RAVL: Discovering and Mitigating Spurious
Correlations in Fine-Tuned Vision-Language Models
MayaVarma Jean-BenoitDelbrouck
StanfordUniversity StanfordUniversity;HuggingFace
mayavarma@cs.stanford.edu jbdel@stanford.edu
ZhihongChen AkshayChaudhari† CurtisLanglotz†
StanfordUniversity StanfordUniversity StanfordUniversity
zhihongc@stanford.edu akshaysc@stanford.edu langlotz@stanford.edu
Abstract
Fine-tunedvision-languagemodels(VLMs)oftencapturespuriouscorrelations
between image features and textual attributes, resulting in degraded zero-shot
performanceattesttime. Existingapproachesforaddressingspuriouscorrelations
(i) primarily operate at the global image-level rather than intervening directly
onfine-grainedimagefeaturesand(ii)arepredominantlydesignedforunimodal
settings. Inthiswork,wepresentRAVL,whichtakesafine-grainedperspectiveon
VLMrobustnessbydiscoveringandmitigatingspuriouscorrelationsusinglocal
imagefeaturesratherthanoperatingattheglobalimagelevel. Givenafine-tuned
VLM, RAVL first discovers spurious correlations by leveraging a region-level
clusteringapproachtoidentifypreciseimagefeaturescontributingtozero-shot
classificationerrors. Then, RAVL mitigates theidentified spuriouscorrelation
withanovelregion-awarelossfunctionthatenablestheVLMtofocusonrelevant
regionsandignorespuriousrelationshipsduringfine-tuning. WeevaluateRAVL
on654VLMswithvariousmodelarchitectures,datadomains,andlearnedspurious
correlations.OurresultsshowthatRAVLaccuratelydiscovers(191%improvement
overtheclosestbaseline)andmitigates(8.2%improvementonworst-groupimage
classificationaccuracy)spuriouscorrelations. Qualitativeevaluationsongeneral-
domainandmedical-domainVLMsconfirmourfindings.1
1 Introduction
Contrastivevision-languagemodels(VLMs)(e.g.,CLIP[36]andALIGN[24])areapowerfulclass
ofmodelsthatjointlylearnrelationshipsbetweenimagesandtext. VLMsaregenerallypretrained
onweb-scaledatasetswithmillionsofimage-textpairsandhavebeenshowntoexhibitimpressive
capabilitiesonawiderangeofdownstreamtasks. Inparticular,VLMshavetheabilitytoperform
tasksinazero-shotmannerwithoututilizingexplicittask-specifictrainingdata;thisisaccomplished
by modeling downstream tasks (e.g., image classification, text-to-image retrieval) as image-text
matchingtasks[36].
However,pretrainedVLMscanexhibitpoorzero-shotperformancewhencomparedtostate-of-the-art
task-specificmodels,particularlyonchallengingorout-of-domaindownstreamtasks[36,7,17,19].
Asaresult,pretrainedVLMsareoftenfine-tunedondomain-specificvision-languagedatasetsinorder
†Equalseniorauthorship.
1Code:https://github.com/Stanford-AIMI/RaVL
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
voN
6
]VC.sc[
1v79040.1142:viXraStage 1: Discovery Stage 2: Mitigation
Our Approach: RaVL butterfly Global Image-Text Objective
bird
Setting: VLM is fine-tuned on a dataset
where the presence of butterflies is
correlated with the presence of flowers
butterfly
bird
A monarch butterfly
Vision sits on a pink flower
Encoder
butterfly Fine-Grained Region-
Aware Objective
bird
minimize maximize
A si tm s o on na ar c ph in b ku fltt oe wrfl ey r EnT ce ox dt e r b bu irt dterfly Goal: Identify learned spurious similarity butterfly similarity
correlations between image features
Goal: Mitigate the
(flower) and textual attributes (butterfly)
spurious correlation
Figure1: Region-awareVision-Languagelearning(RAVL).RAVLtakesafine-grainedperspective
onVLMrobustnessbydiscoveringandmitigatingspuriouscorrelationsusinglocalimagefeatures.
toimprovezero-shotperformanceontasksofinterest. Forinstance,recentworkshavefine-tuned
theCLIPVLM[36]onvision-languagedatasetsconsistingof(i)chestX-raysandpairedphysician
reports[45],(ii)pathologydataandpairedtext[17,19],and(iii)productimagesandpairedcaptions
fromonlinefashionretailers[7].
Domain-specificvision-languagedatasetsusedtofine-tuneVLMsmaybesmallinsize,preventing
VLMsfromgainingtherobustnessbenefitsthatcomewithtrainingondiverse,web-scaledata[6,14].
Asaresult,fine-tunedVLMsmaycapturespuriouscorrelationsbetweenimagefeaturesandtextual
attributes[56]. Forinstance,consideraVLMfine-tunedonananimalimage-textdatasetwherethe
presenceofbutterfliesiscloselycorrelatedwiththepresenceofflowers(Figure1). Consequently,
the VLM may learn to incorrectly associate the image features corresponding to flower with the
textualattributebutterfly. Attesttime,theVLMislikelytoexhibitdegradedzero-shotclassification
performanceon(i)imagesofbutterflieswithoutflowersand(ii)imagesofotheranimalswithflowers.
Improvingrobustnessoffine-tunedVLMstospuriouscorrelationsischallengingforthefollowingtwo
reasons. First,existingautomatedapproachesprimarilydiscoverandmitigatespuriouscorrelations
at the global image level rather than intervening directly on fine-grained image features. Such
approachesdiscoverspuriouscorrelationsbyidentifyingcoherentgroupsofmisclassifiedimages
inanautomatedfashion[13,43,22,42];then,theidentifiedspuriouscorrelationcanbemitigated
duringtrainingusingdataaugmentationorrobustoptimization[43,39,22,56]. However,recent
workshavesuggestedthatsuchglobalimage-levelstrategies(i)discoverspuriouscorrelationsthat
alignpoorlywithhuman-interpretableattributes[25]and(ii)maynoteffectivelyenablemodelsto
ignorespuriouscorrelationsduringtraining[15,18]. Second,existingapproachesfordiscovering
andmitigatingspuriouscorrelationsarepredominantlydesignedtoimproverobustnessofunimodal
imageclassificationmodels[39,43]orpretrainedVLMs[60,49]. Thesesettingsdiffersubstantially
fromthefine-tunedVLMsetting,whichpresentsseveraluniquechallengessuchastheabsenceof
classandsubgrouplabelsinthetrainingsetandtheinclusionoffree-formtext.
Inthiswork,weaddressthesechallengesbyintroducingRegion-awareVision-Languagelearning
(RAVL), an approach for improving the robustness of fine-tuned VLMs to spurious correlations.
RAVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spuri-
ous correlations using local image features, rather than operating at the global image level. Our
contributionsare:
• First,givenafine-tunedVLM, RAVL discoverslearnedspuriouscorrelationsbetweenimage
featuresandtextualattributes. Usingalabeledclassificationdataset,wedecomposeimagesinto
candidateregions,utilizetheVLMembeddingspacetogroupvisually-similarregionsintofeature
clusters,andquantitativelyevaluatetheeffectsofeachfeatureonzero-shotclassificationerrors.
• Second,givenarankedlistofimagefeaturesthattheVLMhaslearnedtospuriouslycorrelate
withoneormoretextualattributes, RAVL mitigatestheidentifiedspuriouscorrelations. Our
keyinsightisthatregion-levelinformationcanbeleveragedduringVLMfine-tuninginorder
to improve model robustness. To this end, we introduce a novel region-aware loss function
2thatencouragestheVLMtofocusonrelevantregionsandignorespuriousrelationshipsduring
fine-tuning.
Inordertoevaluate RAVL,weintroducealarge-scaleevaluationframeworkforcontrolled, fine-
grainedevaluationsofVLMrobustnessonsyntheticandreal-worlddata. Ourframeworkconsistsof
654fine-tunedVLMspairedwithannotationsfortheground-truthspuriouscorrelationslearnedby
eachVLM.Acrosstheseevaluationsettings,(i)RAVLaccuratelydiscoversspuriouscorrelations,
achievinga191%improvementovertheclosestbaseline,and(ii)RAVLeffectivelymitigatesspurious
correlations,achievinguptoan8.2%improvementonworst-groupimageclassificationaccuracy.
Qualitativeevaluationsongeneral-domainandmedical-domainVLMsconfirmtheutilityofRAVL.
Thispaperisorganizedasfollows. InSection2,weintroduceourproblemsetting. Then,inSection
3, we present Stage 1 of RAVL, including our proposed methodology for discovering spurious
correlations, our large-scale evaluation framework, and experimental results. In Section 4, we
introduceStage2ofRAVL,includingourproposedmethodologyformitigatingspuriouscorrelations
aswellasexperimentalresults. Finally,weconcludeinSection5.
RelatedWork: Ourworkbuildsonseveralrecentresearchdirectionsfordiscoveringandmitigating
spuriouscorrelations. WeprovideananalysisofrelatedworksinAppendixSectionA.
2 Preliminaries
Inthissection,weformallydescribeourproblemsetting. Datasetsusedforfine-tuningVLMscan
be expressed as D = {(I ,T )}m , where I represents image inputs and T represents paired
F i i i=1 i i
free-formtext. Wedonotassumeaccesstoanyclassorsubgrouplabels.
Theperformanceoffine-tunedVLMscanbecharacterizedwithzero-shotclassificationtasks. Inline
withpriorwork[13,22,56],weassumethatthezero-shotclassificationdatasetincludesavalidation
split D = {(I ,y )}n with images I and known ground-truth class labels y ∈ Y, where Y
V i i i=1 i i
denotesthesetofallpossibleclasslabels. Atevaluationtime,classificationperformanceiscomputed
byencodingclasslabelsinY astextandmatchingimagestotheclosestclasslabelusingembedding
similarity. Wedonotassumeaccesstoanysubgrouplabels.
Fine-tuned VLMs may learn spurious correlations between image features and textual attributes.
Lete representtheimagefeaturescorrespondingtoavisualconcepta(e.g.,flowersinFigure1)
a
and y ∈ Y represent a class label (e.g., “butterfly" in Figure 1) such that e and y share no
a
causalrelationship. Then,afine-tunedVLMthathaslearnedaspuriouscorrelationwillbeunableto
disentanglee andyatevaluationtime.Thiswillmanifestinlowzero-shotclassificationperformance
a
onthefollowingtwosubgroupsofdata: (i)imagesfromclasslabelywithoutthefeaturee and(ii)
a
imagesfromotherclasslabelsY \{y}withthefeaturee .
a
However,sinceneitherthefine-tuningdatasetD northeevaluationdatasetD includesubgroup
F V
labels corresponding to visual concepts a, discovering and mitigating such spurious correlations
posesachallenge. Forinstance,inFigure1,therearenoannotationsforflowersindatasetsD and
F
D ,makingitchallengingtoidentifyandaddressthelearnedspuriouscorrelationbetweenimage
V
featurescorrespondingtoflowersandthetextualattributecorrespondingto“butterfly".
Inthefollowingsections,wewilldiscussourautomatedapproachRAVL,whichaimstoaddressthis
challengebyemployingfine-grainedregion-levelinformationtodiscover(Section3)andmitigate
(Section4)spuriouscorrelationsinfine-tunedvision-languagemodels.
3 DiscoveringSpuriousCorrelationsinFine-TunedVision-LanguageModels
Inthissection,wepresentthefirststageofRAVL,whichaimstodiscoverlearnedspuriouscorrela-
tionsinVLMs. InSection3.1,wediscussourregion-awareapproachfordiscoveringfine-grained
spuriouscorrelations. Then,inordertoquantitativelyevaluatetheefficacyofspuriousfeaturedis-
coverymethods,weintroducealarge-scaleevaluationframeworkinSection3.2. Finally,inSection
3.3,weuseourevaluationframeworktodemonstratethatRAVLoutperformspriorapproachesin
discoveringfine-grainedspuriouscorrelationsbetweenimagefeaturesandtextualattributes.
33.1 OurApproach: DiscoveringSpuriousCorrelations
ThefirststageofRAVLaimstoidentifyspuriouscorrelationsbetweenimagefeaturesandtextual
attributeslearnedbyafine-tunedVLMM. Incontrasttopriorworksthathaveincorporatedhumans
intheloopinordertoidentifyspuriouscorrelations[56,30],RAVLisafullyautomatedapproach.
Additionally, whereas previous automated methods for discovering spurious correlations focus
predominantlyonidentifyinggroupsofimageswithhigherrorrates[22,13],ourapproachidentifies
specificimagefeaturesthatmodelMhaslearnedtospuriouslycorrelatewithatextualattribute. Our
goalistodiscoverprecisespuriouscorrelationsthatcanbeeasilyinterpretedbyhumans.
As discussed in Section 2, a model M that has learned a spurious correlation between an image
feature e and a textual attribute y will demonstrate low zero-shot performance on (i) images in
a
D withlabely withoutthefeaturee and(ii)imagesinD withotherlabelsY \{y}withthe
V a V
featuree . Thekeychallengeliesinidentifyingsuchrelationshipswhennoannotationsareprovided
a
forvisualconceptsa. RAVLaddressesthischallengeby(1)obtainingcandidateimagefeaturesin
D ,(2)identifyingthecandidateimagefeaturesthat,whenpresentinanimage,directlycontribute
V
toclassificationerrors,and(3)rankingtheidentifiedimagefeaturesbydegreeoflearnedspurious
correlations.
Obtainingcandidateimagefeatures. RAVLfirstutilizesthezero-shotclassificationdatasetD
V
to identify candidate image features. To this end, we use the fine-tuned VLM M to extract an
imageembeddingforeachimageI inD andatextembeddingforeachclassy ∈ Y. Zero-shot
i V
classificationisperformedusingthecomputedembeddings; thisresultsinasoftmax-normalized
image score distribution vector s ∈ R|Y|, where |Y| represents the number of classes. Then,
Ii
we decompose each image I in D into a set of candidate regions R . There are a variety of
i V i
waysinwhichanimagecanbedecomposedintoregions,suchasdividingimagesintoequal-sized
segments(e.g.,quadrants)orusingregionproposalnetworks(RPNs)[38]. Ideally,regionsshould
capturekeyfeaturesintheimage;however,weemphasizethatRAVLdoesnotrequireground-
truthregion-levelannotations. WethenapplyRoIAlign[16,63]totheimageencoderofMto
extractembeddingsforeachregion. Zero-shotclassificationisperformedusingthecomputedregion
embeddings,resultinginasoftmax-normalizedregionscoredistributionmatrixS
Ri
∈R|Ri|×|Y|.
Givenregion-levelembeddingsforallcandidateregionsinD ,wenextaimtoidentifycoherent
V
groupsofimagefeaturesthatoccurconsistentlythroughoutthedataset(e.g.,featurescorresponding
to“flower"or“butterfly"inFigure1). Tothisend,weclusterthecomputedregion-levelembeddings
usingtheK-Medoidsalgorithmwithcosinedistance. Theoptimalnumberofclustersisselectedin
anautomatedfashionusingSilhouettedistance. Theresultingclusters(denotedasC)capturekey
imagefeaturesinD . Forfeatureclusterc∈C,lete denotesthesetoffeaturesinclusterc.
V c
Identifyingcandidateimagefeaturesthatdirectlycontributetoclassificationerrors. Wenow
seektoidentifyfeaturesthat,whenpresentinanimage,aredirectlyresponsibleforpredictionerrors.
Let R represent the set of regions assigned to cluster c and let I represent the set of images
c c
associated with the regions in cluster c. We identify labels for images in I ; we designate this
c
label set as Y . For each class label y ∈ Y , we identify all images in I with label y, and we
c c c
designatezero-shotclassificationaccuracyonthissubsetofny imagesaspy . Then,weidentifyall
in in
imagesinD withlabelythatdonothavearegionincludedinclusterc,andwedesignatezero-shot
v
classificationaccuracyonthissubsetofny imagesaspy .
out out
We now introduce the cluster influence score, which evaluates the extent to which features e
c
contribute tomispredicted imageclassificationlabels. Werestrict ourevaluationto onlyinclude
mispredictedimagesinI withground-truthlabelsysuchthatpy <py ;wewillrefertothissubset
c in out
asIerr ⊂I . ForeachimageI ∈Ierr,weextract(i)theimagescoredistributionvectors and(ii)
c c i c Ii
theregionscoredistributionmatrixS . Weuses toidentifythepredictedimageclassyˆ,andwe
Ri Ii
thenidentifytheregionrmaxinR withthehighestscoreforclassyˆ.
i i
Definition1(ClusterInfluenceScore). Forclustercandlabely,theclusterinfluencescoreisthe
proportionofimagesI ∈Ierr withlabelywheretheidentifiedhighest-scoringregionrmaxispart
i c i
ofclusterc(i.e.,rmax ∈R ):
i c
1 (cid:88)
Hy = 1[rmax ∈R ] (1)
c |{I ∈Ierr|y =y}| i c
i c i Ii∈I cerr;yi=y
4The final cluster influence score for cluster c is computed as the maximum over all labels y as
H =max Hy. HighvaluesofH showthatfeaturese aresimilartotheincorrectlabelinthe
c y∈Yc c c c
vision-languageembeddingspace;thissuggeststhatforagivenimagewithanincorrectprediction,
featuree ismorelikelytocontributetothemispredictionthanotherfeaturesintheimage. Onthe
c
otherhand,lowvaluesofH arelikelytoindicatethatfeaturee representsacorefeatureassociated
c c
withtheclasslabeloraneutralfeaturethatdoesnotaffectpredictions.
GivenH foreachfeaturecluster,wepruneallclusterswithinfluencescoresbelowathresholdofτ ,
c l
whichwesetto0.25inallexperiments.
Rankingimagefeaturesbydegreeoflearnedspuriouscorrelation. Foreachremainingfeature
cluster,wenextaimtodeterminetheextenttowhichthepresenceorabsenceoffeaturese affects
c
classificationperformance;weintroducetheclusterperformancegapmetrictothisend.
Definition2(ClusterPerformanceGap). Forclustercandlabely,theclusterperformancegapisthe
weighteddifferencebetweenzero-shotclassificationaccuracyonimageswithfeaturese andimages
c
withoutfeaturese :
c
Gy =w ×(py −py ), (2)
c y in out
wherew isasimpleweightingfactorcomputedasw =2×[min(ny ,ny )/(ny +ny )].
y y in out in out
Sincespuriouscorrelationsresultinconsistenterrorsasopposedtoisolatedmisclassifications,the
weightingfactorisdesignedtoprioritizestrongerspuriouscorrelationsthatresultinalargernumber
oferrors. Gy rangesbetween0and1. Thefinalperformancegapmetricforclusterciscomputed
acrossalllabc elsasG =(cid:80) |Gy|. AhighvalueofG suggeststhatthepresenceorabsenceof
c y∈Yc c c
featurese contributetolargeclass-levelvariationsinimageclassificationperformance.
c
GivenG foreachfeaturecluster,werankclustersinorderfromhighesttolowestvalues. Theoutput
c
ofthisstageisarankedlistofimagefeaturesthatmodelMhaslearnedtospuriouslycorrelatewith
oneormoreclasslabelsinY.
3.2 ExperimentalSetup: DesigningaLarge-ScaleEvaluationFramework
WenowdiscussourapproachforevaluatingRAVL.Evaluatingtheaccuracyofpredictedspurious
correlationsischallengingbecausetheground-truthspuriouscorrelationslearnedbyamodelMare
typicallyunknown. PreviousworksonVLMrobustnessevaluatediscoveredspuriouscorrelations
withqualitativeexperiments,human-in-the-loopevaluations,orsmall-scaledatasets[56]. Ouraim
in this section is to introduce a large-scale experimental setup where the ground-truth spurious
correlations learned by VLMs are known and annotated in advance; this can then enable us to
determinewhetherthefeaturesdiscoveredbyRAVLinSection3.1accuratelyalignwiththeground-
truth. Ourevaluationframeworkismotivatedbypriorwork[26,13];however,incontrasttoexisting
approaches,weintroduceevaluationsettingsthataredesigned(i)forevaluatingrobustnessapproaches
atthefine-grainedregionlevelratherthantheglobalimage-level,and(ii)forevaluatingVLMsrather
thanunimodalmodels.
DesigningControlledEvaluations: Ourevaluationframeworkartificiallyinducesspuriouscorre-
lationsintheVLMfine-tuningdata;then,giventheknownpre-definedspuriouscorrelationanda
VLMthatlearnedthedesiredspuriouscorrelation,wecanquantitativelyevaluatetheextenttowhich
RaVLdiscoversthecorrelation.
Wecreateasetofevaluationsettingsusingdatafromtwodomains: (1)syntheticdata(MNIST[11]
andFashionMNIST[51])and(2)real-worlddata(COCO[27]). Eachevaluationsettingconsistsof
thefollowingcomponents:
1. Predefinedspuriouscorrelation: Wedefineaspuriousimagefeatureandtextualattributepair
(eeval,aeval).ForMNISTandFashionMNIST,eevalrepresentsaredrectangle;aevalisgenerated
fromthesetofclasslabels{zero,one,two,three,fourfive,six,seven,eight,nine}forMNISTand
{t-shirt,trouser,pullover,dress,coat,sandal,shirt,sneaker,bag,ankleboot}forFashionMNIST.
ForCOCO,wesampleeeval andaeval fromthelistofannotatedattributes.
2. Fine-tuningdataset: Weconstructavision-languagefine-tuningdatasetDeval = {(I ,T )}m
F i i i=1
withimagesI andtextT . DatasetDeval issampledfromthetrainingsetsofMNIST,Fashion-
i i F
MNIST,orCOCOsuchthatthepresenceofimagefeatureeeval iscloselycorrelatedwiththe
presenceoftextattributeaeval asmeasuredbyCramer’sV[57].
5Domain: Synthetic Data Domain: Real-World Data
1.0 1.0
RaVL (Ours)
0.8 0.8
Spurious-Aware
0.6 0.6 Domino
0.4 0.4 George
Distilling Failures
0.2 0.2
Random
0.0 0.0
10 20 30 40 10 20 30 40
Strength of Learned Spurious Correlation (𝛕eval) Strength of Learned Spurious Correlation (𝛕eval)
Figure2: RAVLaccuratelyidentifiesspuriouscorrelations. Usingourevaluationsettings,weshow
that RAVL consistently outperforms prior methods in discovering learned spurious correlations
betweenimagefeaturesandtextualattributes. Here,weprovidePrecision@10metricsforaCLIP-
RN50modelfine-tunedonsyntheticdata(129settings)andreal-worlddata(171settings).
Table 1: Mean Precision@10 metrics demon-
stratetheefficacyof RAVLindiscoveringspu-
Table 2: Ablations show the utility of the clus-
riouscorrelations. Onaverageacross654evalu-
terperformancegapandinfluencemetrics. We
ationsettings, RAVLconsistentlyoutperforms
reportPrecision@10metricsforaCLIP-RN50
baselines.
model fine-tuned on real-world data (171 set-
Method CorrelationStrength(τeval) tings).
10 20 30 40
Ablation CorrelationStrength(τ )
eval
Num.EvalSettings 654 369 234 168
10 20 30 40
Random 21.2 18.2 15.5 12.5
DistillingFailures 20.1 16.2 8.5 1.5 UnweightedG cOnly 21.2 30.0 36.2 55.0
George 19.3 15.9 10.9 7.7 G cOnly 40.9 51.7 63.8 66.7
Domino 17.1 15.0 11.7 9.0 G c&H c(RAVL) 46.0 54.8 72.4 83.3
Spurious-AwareDetection 20.0 25.3 32.1 42.0
RAVL(Ours) 61.8 76.2 84.2 91.1
3. Fine-tunedVLM:AVLMMisfine-tunedonDeval.
F
4. Evaluation dataset: Model M is evaluated using a zero-shot classification dataset Deval =
V
{(I ,y ,R ,L )}n withimagesI ,classlabelsy ,regionboundingboxesR ,andregion-level
i i i i i=1 i i i
labelsL . Inparticular,aeval mustbeincludedintheclasslabelset,andeeval mustbeannotated
i
intheregion-levellabelset. SinceDeval isdesignedtoreflectareal-worldsetting,weassume
V
thatacorrelationbetweenaeval andeeval doesnotexist. DatasetDeval isconstructedfromthe
V
testsetsofMNIST,FashionMNIST,orCOCO.
Giventhefourcomponentslistedabove,weclassifyanevaluationsettingasvalidifmodelMlearned
theintendedspuriouscorrelation. Inordertomeasurethis,wefirstidentifyimageswithlabelaeval
inDeval andcomputetheperformancedifferencebetweenimageswithfeatureeeval andimages
V
withoutfeatureeeval; wedesignatethisvalueasϵ . Then, forlabelsy ̸= aeval, wecomputethe
1
maximumperformancedifferencebetweenimageswithoutfeatureeeval andimageswithfeature
eeval;wedesignatethisvalueasϵ . Largevaluesofϵ andϵ suggestthatmodelMhaslearnedthe
2 1 2
desiredspuriouscorrelationbetweenimagefeatureeeval andtextualattributeaeval,asdefinedin
Section2. Weremovesettingswhereϵ orϵ arebelowsomepredefinedperformancethresholdτ .
1 2 eval
Theperformancethresholdτ servesasaquantitativeindicatoroflearnedcorrelationstrength.
eval
ImplementationDetails: Intotal,wegenerate620fine-tuningdatasetsDeval (100synthetic;520
F
real-world). Wethenfine-tunemodelMoneachdatasetwiththreerandomseeds,resultingin1860
candidateevaluationsettings. Finally,wefilteroutsettingswheremodelMdoesnotconsistently
learn the spurious correlation; to this end, we only retain settings where both ϵ and ϵ exceed
1 2
τ =10acrossallthreerandomseeds. WerepeatthisprocedureacrossvariouspretrainedVLMs
eval
M,resultingin654validexperimentalsettings. Additionalimplementationdetailsareprovidedin
AppendixB.
6
01@noisicerP 01@noisicerP3.3 Results: RaVLEffectivelyDiscoversSpuriousCorrelations
ComparisonstoPriorApproaches: Givenanevaluationsettingwithapredefinedspuriouscorrela-
tion(eeval,aeval),afine-tunedVLMM,andanevaluationdatasetDeval,ourgoalistodetermine
V
theextenttowhichRAVLcandiscoverthecorrelationbetweeneeval andaeval.
Tothisend,weusethelabeledzero-shotclassificationdatasetDeval,whichincludesground-truth
V
regionboundingboxesandassociatedregionlabels. Weprovidetheground-truthboundingboxes
asinputto RAVL,whichreturnsasingletop-rankedclusterofregionslikelytoincludespurious
features. Werankregionswithintheclusterbasedonsimilaritytotheclustermedoid,andweutilize
theprovidedregion-levellabelsinDeval toevaluatetheproportionoftop-K regionsthatcontainthe
V
desiredspuriousfeatureeeval. Inlinewithpriorwork[13],wereportperformancewithPrecision@K
metrics. Wenotethatgivenanidentifiedspuriousfeatureeeval,thecorrelatedtextualattributeaeval
canbedetectedbyidentifyingtheclasslabelinDeval wheretheabsenceoffeatureeeval leadsto
V
degradedperformance.
There are few existing approaches for performing automated detection of fine-grained spurious
features learned by VLMs. Here, we compare RAVL with four previously-developed methods:
DistillingFailures[22],George[43],Domino[13],andSpurious-AwareDetection[56]. Distilling
Failures,George,andDominoarestate-of-the-artapproachesforautomaticidentificationofmodel
failuresresultingfromspuriouscorrelations;althoughthesemethodsoperateattheglobalimagelevel
andaredesignedforunimodalsettings,weadapttheseapproachesforoursettingbyutilizingregions
andzero-shotclassificationscoresasinput. Spurious-AwareDetectionoperatesatthefine-grained
regionlevelbycomputingclass-basedperformancegapsresultingfromthepresenceorabsenceof
particularfeatures. ToenableafaircomparisonwithRAVL,weprovidethesamesetofregionsand
associatedembeddingsasinputtoallbaselines. WealsocompareRAVLwitharandombaseline,
wheretherankedlistofregionsisshuffledrandomly.
Table1summarizesmeanPrecision@10metricsacrossall654evaluationsettings. Resultsdemon-
strate that RAVL consistently outperforms prior approaches in discovering spurious correlations
betweenimagefeaturesandtextualattributes,contributingtoa191%improvementovertheclosest
baseline. InTable1,weevaluatetheeffectsoflearnedspuriouscorrelationstrengthbyvaryingtheer-
rorthresholdτ from10to40andreportingperformanceforthesubsetofvalidevaluationsettings.
eval
ResultsshowthatRAVLisparticularlyeffectivewhenVLMMlearnsastrongspuriouscorrelation;
aslearnedcorrelationstrengthincreases, performanceof RAVL increasesby47%whereasmost
baselinesdegradeinperformance. WealsoobservethatDomino,George,andDistillingFailures
oftenachieveperformancenearorbelowtherandombaselineacrossourevaluationsettings;this
suggeststhatmethodsdesignedfordetectingerrorsresultingfromspuriouscorrelationsattheglobal
image-levelcannotbeeasilyadaptedforfine-grainedregion-leveldiscovery. Figure2demonstrates
thatourfindingsholdforbothsyntheticandreal-worlddata.
Ablations: OurablationstudyevaluatestheroleoftheclusterinfluencescoreH andthecluster
c
performancegapmetricG (Section3.1)inenablingaccuratediscoveryofspuriouscorrelations
c
betweenimagefeaturesandtextualattributes. Wecomparethefollowingthreemetricsforranking
clusters: (1) an unweighted cluster performance gap metric where w is set to 1, (2) the cluster
y
performancegapwithw computedasinSection3.1,and(3)acombinationoftheclusterperformance
y
gap and cluster influence metric as used in RAVL. As shown in Table 2, the metrics utilized by
RAVLconsistentlydemonstratethebestperformanceacrossvariouslearnedcorrelationstrengths
(τ ). Ourresultssuggesttheutilityofboththeperformancegapmetricandtheinfluencescorein
eval
identifyingfine-grainedspuriouscorrelations.
EvaluationsintheWild: Inadditiontoourcontrolledevaluations,weevaluatetheabilityofRAVL
tosurfacespuriouscorrelationslearnedby12off-the-shelfVLMs[12,36,20];thispresentsarealistic
anduncontrolledevaluationsetting. Weconsidertwozero-shotclassificationtasksD : (1)a397-
V
classsceneclassificationtaskonSUN397[52]and(2)binaryclassificationofcardiomegalyinchest
X-raysfromObjectCXR[23]. WeusetheclusterperformancegapmetricG ,introducedinSection
c
3.1,toquantifythedegreeofthelearnedspuriouscorrelation.
Ourresultsdemonstratethatallevaluatedmodels,whichspanarangeofarchitecture,trainingdata,
andparametercounts,showevidenceofhavinglearnedspuriouscorrelations;thisisdemonstratedby
nonzerovaluesoftheclusterperformancegapmetricG .Onaverageacrosstheevaluatedmodels,the
c
top-rankedspuriousfeatureclusterdiscoveredbyRAVLonSUN397achievesaclusterperformance
7MMooddeell Classification Task Spurious Features Identified by RaVL Zero-Shot Accuracy
Images with RaVL- Images without RaVL-
CLIP Scene Classification 100 identified features 0 identified features 100
(ViT-B/16) (SUN397) 79.4 31.2
Class label: fast food restaurant
Images with RaVL- Images without RaVL-
PubMedCLIP Chest X-ray 100 identified features 0 identified features 100
Classification
(ResNet-50) 35.320.0
(Object CXR)
Class label: cardiomegaly
Figure3: RAVLsurfacesspuriouscorrelationsinoff-the-shelfVLMs. RAVLidentifiesaspurious
correlation learned by CLIP ViT-B/16 between the presence of text-based retail signage and the
classlabelfast food restaurantinasceneclassificationtask. RAVLalsosurfacesaspurious
correlationlearnedbyPubMedCLIPResNet-50betweenmetalclips(foundinclothing)andtheclass
labelcardiomegaly(aheartcondition)onachestX-rayclassificationtask.
OPENCLIP-ViTL14
- Candidates: Cluster 461
CLIP-ViTB16
Table3: RAVL effectivelymitigatesspuriouscorrelations. Here,werepor-t CmaenadnidImataegse: COlvuesrtaelrls, 31, 5, 51
ImageWorst-Group(Img. WG),RegionOverall,andRegionWorst-Group(Reg. WG)metricsacross
ourreal-worldevaluationsettings. Sinceperformanceofmitigationmethodsisdependentonthe
resultsofStage1,wereportmetricsacrosssettingswhereStage1Precision@10>0.6andStage1
Precision@10>0.8.
Method Stage1DiscoveryPrecision@10>0.6 Stage1DiscoveryPrecision@10>0.8
Img.Overall Img.WG Reg.Overall Reg.WG Img.Overall Img.WG Reg.Overall Reg.WG
StandardFT 64.0 31.4 72.0 46.9 64.6 31.0 72.9 47.4
UpsampledFT 66.6 37.8 74.3 52.2 66.7 37.7 74.7 52.8
VL-ERM 68.8 32.2 75.6 50.3 68.7 30.9 75.9 50.6
VL-GDRO 69.1 33.7 75.6 50.4 68.8 31.1 76.0 51.0
Spurious-Aware 69.8 33.6 76.5 50.6 69.2 30.7 76.8 50.5
RAVL(Ours) 69.8 39.1 78.9 57.8 70.2 40.8 79.5 58.5
gaps(G )of9.9 (minimum=5.1,maximum=14.0). OnObjectCXR,themeanvalueofG is
c ±3.2 c
0.08 (minimum=0.04,maximum=0.12).2 Ourresultssupportfindingsfrompreviouswork
±0.04
suggestingthatallmodelsmaylearnspuriouscorrelations[30].
InFigure3,weprovidequalitativeexamplesofdiscoveredspuriousfeaturesfortheCLIPViT-B/16
modelevaluatedonSUN397andthePubmedCLIPResNet-50modelevaluatedonObjectCXR.For
theCLIPViT-B/16model,RAVLsurfacesafeatureclusterconsistingoftext-basedretailsignage.
WeobservesignificantperformancegapsbetweenimagescontainingtheRAVL-identifiedfeature
andimagesthatdonotcontainthefeature. Forinstance,wenotea48.2pointdifferenceinzero-shot
classificationaccuracyfortheclasslabelfast food restaurant,suggestingthataCLIPViT-B/16
modelcanbetterclassifyasceneofafastfoodrestaurantwhenatext-basedretailsignispresent. For
thePubmedCLIPResNet-50model,RAVLdiscoversthatthepresenceofmetalclips(foundinthe
patient’sclothing)isspuriouslycorrelatedwithcardiomegaly. Weobservethatthepresenceofclips
improveszero-shotclassificationaccuracyfortheclasslabelcardiomegalyby15.3points.
OurevaluationsshowthatRAVLcansurfacefine-grainedspuriouscorrelationsinrealisticsettings.
AdditionalimplementationdetailsandqualitativeexamplesareprovidedinAppendixD.
4 MitigatingSpuriousCorrelationsinFine-TunedVision-LanguageModels
In this section, we present the second stage of RAVL, which aims to mitigate learned spurious
correlations in VLMs. In Section 4.1, we discuss our methodology for mitigating fine-grained
spuriouscorrelationswithanovelregion-awarelossfunction. InSection4.2,weusetheevaluation
frameworkpreviouslyintroducedinSection3.2todemonstratethatRAVLsubstantiallyoutperforms
priorapproachesinmitigatingspuriouscorrelationsbetweenimagefeaturesandtextualattributes.
2WenotethatsincetheformulaforG involvesasummationoverclasslabels,rawvaluesofG forour
c c
2-classchestX-rayclassificationtaskarelowerthanthoseforour397-classsceneclassificationtask.
84.1 OurApproach: MitigatingSpuriousCorrelations
As described in Section 3, Stage 1 of RAVL discovers image features that VLM M has learned
to spuriously correlate with textual attributes. We next aim to mitigate the spurious correlation.
Motivatedbypriorworkonfine-grainedVLMs[58,46],ourkeyinsightisthatutilizingregion-level
informationduringVLMtrainingcanenablemodelstofocusonrelevantimage-textrelationships
andignorespuriouscorrelations.
SincedatasetD exclusivelyconsistsofimagesandtext,ground-truthsubgroupandclasslabelsare
F
notavailable.Asaresult,wefirstassignplausible(i)region-levelsubgrouplabelsand(ii)image-level
classlabelstothevision-languagefine-tuningdatasetD . Toassignsubgrouplabels,wedecompose
F
eachimageI indatasetD intoasetofcandidateregionsR . WethenfitthetrainedK-Medoids
i F i
clusteringmodelfromSection3.1onR andidentifyallspuriousregionsassociatedwiththetop
i
ranked cluster. We represent the identified spurious regions as Rs and remaining non-spurious
i
regionsasRr suchthatRs∪Rr =R . Inordertoassignplausibleclasslabels,weparsethepaired
i i i i
textT associatedwitheachimagetoidentifysamplesthatreferencetheclasslabelsincludedinthe
i
zero-shotclassificationlabelsetY;werefertotheassignedclasslabelforimageI asyˆ.
i i
Wenowintroduceanovelregion-awarecontrastivelossfunctionfortrainingVLMM . Forbatch
new
B,wedefineRs asthesetofallspuriousregionsinthebatch: Rs =(cid:83) Rs. ForimageI ∈B,
B B Ii∈B i i
thefirstlosscomponentLi encourageshighembeddingsimilaritybetweennon-spuriousregionsRr
R i
andassignedclasslabelyˆ whencomparedtootherclasslabels.
i
σ (Rr,yˆ)
Li =−log m i i (3)
R (cid:80) σ (Rr,yˆ)+P(Rs)
yˆj∈B m i j B
Here, for region embedding function f and text embedding function g, σ (A,b) =
m
exp(max (⟨f(a),g(b)⟩/τ)) with temperature τ. The term P(Rs) is a penalty that enforces
a∈A B
embedding-leveldissimilaritybetweenspuriousregionsandcorrelatedclasslabels.
The second loss component Li encourages high embedding similarity between non-spurious re-
A
gions Rr and assigned class label yˆ when compared to other regions. We define σ(a,b) =
i i
exp(⟨f(a),g(b)⟩/τ)withtemperatureτ.
σ (Rr,yˆ)
Li =−log m i i . (4)
A
σ
(Rr,yˆ)+(cid:80)|B|
σ
(Rr,yˆ)+(cid:80)
σ(r ,yˆ)
m i i j=1,yˆj̸=yˆi m j i rj∈Rs
B
j i
ThefinallossisexpressedasL=λL +(1−λ)(cid:80)|B| (Li +Li ). Here,λisahyperparameter
CL i=1 R A
andL takestheformoftheoriginallossfunctionusedfortrainingM;inourexperiments,L is
CL CL
theCLIPobjective[36]. ExtendedformulationsofourlossfunctionareprovidedinAppendixC.
4.2 Results: RaVLEffectivelyMitigatesSpuriousCorrelations
ComparisonstoPriorApproaches: Weusetheevaluationframeworkpreviouslyintroducedin
Section3.2tocompareRAVLwithpriorapproaches.Therearefewexistingapproachesformitigating
spuriouscorrelationsinthesettingoffine-tunedVLMs. Here,wecompare RAVL withstandard
VLMfine-tuning,upsampledVLMfine-tuning,ERM,GDRO[39],andSpurious-AwareMitigation
[56]. SinceERMandGDROaretraditionallyusedinunimodalclassificationsettings,weadaptthese
approachesforoursettingbyaddingacontrastivevision-languageobjectiveandusingzero-shot
classificationscoresduringfine-tuning;werefertotheseapproachesasVL-ERMandVL-GDRO
respectively.
Table3summarizesmeanzero-shotclassificationresultsacrossourreal-worldevaluationsettings.
Sinceperformanceofmitigationmethodsisdependentontheaccuracyofthediscoveredspurious
correlationsinStage1,Table3displaysresultsfortwoevaluationcategories: (i)the192settings
whereRAVLStage1Precision@10isgreaterthan0.6,and(ii)the106settingswhereRAVLStage1
Precision@10isgreaterthan0.8. Inlinewithpriorworksonrobustness[39,56],wereportimage
overall performance and image worst-group performance. Additionally, in order to evaluate the
extenttowhichtheVLMunderstandsfine-grainedfeatures,weintroducetwonewmetrics: region
overallperformanceandregionworst-groupperformance. Region-levelaccuraciesarecomputedby
performingzero-shotclassificationwithregionembeddingsandcomparingpredictedlabelstothe
ground-truthregion-levellabelsprovidedinthezero-shotclassificationdataset.
9ResultsshowthatRAVLconsistentlyoutperformspriorapproachesinmitigatingspuriouscorrelations.
AcrossthetwoevaluationcategoriesinTable3,RAVLcontributestoanimprovementofupto8.2%
onimageworst-groupperformanceand10.8%onregionworst-groupperformanceoverthenearest
baseline. Improvementsinregionworst-groupperformanceareparticularlynotable,suggestingthat
RAVLcanbetterinterpretfine-grainedfeatureswhencomparedtopriorapproaches. Additionally,
astheaccuracyofthediscoveredspuriouscorrelationsinStage1increases,theperformanceofthe
RAVL mitigation approach increases proportionally. Our results demonstrate the efficacy of our
fine-tuningprocedureinmitigatingspuriouscorrelationswhencomparedtopriorapproaches.
5 Conclusion
Inthiswork,weintroducedRAVL,afine-grainedregion-awareapproachforaddressingspurious
correlationsinVLMs. Wedemonstratethroughlarge-scale,controlledexperimentsaswellasin-the-
wildevaluationsthatRAVLcandiscover(191%improvementinidentifiedcorrelations)andmitigate
(8.2%improvementonworst-groupperformance)spuriouscorrelationsinVLMs. Wehopethatour
workcanhelp(i)diagnoseandcorrectcriticalfailuremodesinVLMspriortodeploymentand(ii)
driveprogresstowardsthedevelopmentoffine-grainedapproachesformodelrobustness.
AcknowledgmentsandDisclosureofFunding
We are thankful to Sophie Ostmeier, Eduardo Reis, and Ashwin Kumar for helpful discussions
andfeedback. MV issupported bygraduate fellowship awardsfrom theDepartment ofDefense
(NDSEG),theKnight-HennessyScholarsprogramatStanfordUniversity,andtheQuadprogram.
ACissupportedbyNIHgrantsR01HL167974,R01HL169345,R01AR077604,R01EB002524,
R01 AR079431, P41 EB027060, AY2AX000045, and 1AYSAX0000024-01; and NIH contracts
75N92020C00008 and 75N92020C00021. CL is supported by NIH grants R01 HL155410, R01
HL157235,byAHRQgrantR18HS026886,andbytheGordonandBettyMooreFoundation. JBD
and CL are supported by the Medical Imaging and Data Resource Center (MIDRC), which is
fundedbytheNationalInstituteofBiomedicalImagingandBioengineering(NIBIB)undercontract
75N92020C00021andthroughTheAdvancedResearchProjectsAgencyforHealth(ARPA-H).
10References
[1] DyahAdila,ChanghoShin,LinrongCai,andFredericSala. Zero-shotrobustificationofzero-shotmodels
withfoundationmodels,2023.
[2] AmitAlfassy,AssafArbelle,OshriHalimi,SivanHarary,RoeiHerzig,EliSchwartz,RameswarPanda,
MicheleDolfi,ChristophAuer,KateSaenko,PeterW.J.Staar,RogerioFeris,andLeonidKarlinsky. Feta:
Towardsspecializingfoundationmodelsforexperttaskapplications,2022.
[3] SaraBeery,GrantVanHorn,andPietroPerona. Recognitioninterraincognita. InProceedingsofthe
EuropeanConferenceonComputerVision(ECCV),September2018.
[4] LouisBlankemeier,JosephPaulCohen,AshwinKumar,DaveVanVeen,SyedJamalSafdarGardezi,Mag-
daliniPaschali,ZhihongChen,Jean-BenoitDelbrouck,EduardoReis,CesarTruyts,ChristianBluethgen,
MalteEngmannKjeldskovJensen,SophieOstmeier,MayaVarma,JeyaMariaJoseValanarasu,Zhongnan
Fang,ZepengHuo,ZaidNabulsi,DiegoArdila,Wei-HungWeng,EdsonAmaroJunior,NeeraAhuja,
JasonFries,NigamH.Shah,AndrewJohnston,RobertD.Boutin,AndrewWentland,CurtisP.Langlotz,
JasonHom,SergiosGatidis,andAkshayS.Chaudhari. Merlin:Avisionlanguagefoundationmodelfor
3dcomputedtomography,2024.
[5] ZhihongChen,MayaVarma,Jean-BenoitDelbrouck,MagdaliniPaschali,LouisBlankemeier,DaveVan
Veen,JeyaMariaJoseValanarasu,AlaaYoussef,JosephPaulCohen,EduardoPontesReis,EmilyB.Tsai,
AndrewJohnston,CameronOlsen,TanishqMathewAbraham,SergiosGatidis,AkshayS.Chaudhari,and
CurtisLanglotz. Chexagent:Towardsafoundationmodelforchestx-rayinterpretation,2024.
[6] MehdiCherti,RomainBeaumont,RossWightman,MitchellWortsman,GabrielIlharco,CadeGordon,
Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive
language-imagelearning,2022.
[7] PatrickJohnChia,GiuseppeAttanasio,FedericoBianchi,SilviaTerragni,AnaRitaMagalhaes,Diogo
Goncalves,CiroGreco,andJacopoTagliabue. Contrastivelanguageandvisionlearningofgeneralfashion
concepts. ScientificReports,12(1),November2022.
[8] JosephPaulCohen, JosephDViviano, PaulBertin, PaulMorrison, ParsaTorabian, MatteoGuarrera,
MatthewPLungren,AkshayChaudhari,RupertBrooks,MohammadHashir,etal. Torchxrayvision:A
libraryofchestx-raydatasetsandmodels. InInternationalConferenceonMedicalImagingwithDeep
Learning,pages231–249.PMLR,2022.
[9] ElliotCreager,Jörn-HenrikJacobsen,andRichardZemel. Environmentinferenceforinvariantlearning. In
InternationalConferenceonMachineLearning,pages2189–2200.PMLR,2021.
[10] AlexJDeGrave, JosephDJanizek, andSu-InLee. AIforradiographicCOVID-19detectionselects
shortcutsoversignal. Nat.Mach.Intell.,3(7):610–619,May2021.
[11] LiDeng. Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch. IEEESignal
ProcessingMagazine,29(6):141–142,2012.
[12] SedighehEslami,ChristophMeinel,andGerardDeMelo. Pubmedclip:Howmuchdoesclipbenefitvisual
questionansweringinthemedicaldomain? InFindingsoftheAssociationforComputationalLinguistics:
EACL2023,pages1151–1163,2023.
[13] SabriEyuboglu,MayaVarma,KhaledKamalSaab,Jean-BenoitDelbrouck,ChristopherLee-Messer,Jared
Dunnmon,JamesZou,andChristopherRe. Domino: Discoveringsystematicerrorswithcross-modal
embeddings. InInternationalConferenceonLearningRepresentations,2022.
[14] AlexFang,GabrielIlharco,MitchellWortsman,YuhaoWan,VaishaalShankar,AchalDave,andLudwig
Schmidt. Datadeterminesdistributionalrobustnessincontrastivelanguageimagepre-training(clip),2022.
[15] IshaanGulrajaniandDavidLopez-Paz.Insearchoflostdomaingeneralization.InInternationalConference
onLearningRepresentations,2021.
[16] KaimingHe,GeorgiaGkioxari,PiotrDollár,andRossGirshick. Maskr-cnn,2017.
[17] ZhiHuang,FedericoBianchi,MertYuksekgonul,ThomasJMontine,andJamesZou. Avisual-language
foundation model for pathology image analysis using medical twitter. Nat. Med., 29(9):2307–2316,
September2023.
11[18] BadrYoubiIdrissi,MartinArjovsky,MohammadPezeshki,andDavidLopez-Paz. Simpledatabalancing
achievescompetitiveworst-group-accuracy. InBernhardSchölkopf,CarolineUhler,andKunZhang,edi-
tors,ProceedingsoftheFirstConferenceonCausalLearningandReasoning,volume177ofProceedings
ofMachineLearningResearch,pages336–351.PMLR,11–13Apr2022.
[19] Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva,
FatwirSheikhMohammed,PavanKumarAnand,RanjayKrishna,andLindaShapiro. Quilt-1m: One
millionimage-textpairsforhistopathology.InThirty-seventhConferenceonNeuralInformationProcessing
SystemsDatasetsandBenchmarksTrack,2023.
[20] GabrielIlharco,MitchellWortsman,RossWightman,CadeGordon,NicholasCarlini,RohanTaori,Achal
Dave,VaishaalShankar,HongseokNamkoong,JohnMiller,HannanehHajishirzi,AliFarhadi,andLudwig
Schmidt. Openclip,July2021. Ifyouusethissoftware,pleaseciteitasbelow.
[21] PavelIzmailov,PolinaKirichenko,NateGruver,andAndrewGWilson.Onfeaturelearninginthepresence
ofspuriouscorrelations. AdvancesinNeuralInformationProcessingSystems,35:38516–38532,2022.
[22] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. Distilling model failures as
directionsinlatentspace. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
[23] JFHealthcare. Object-cxr-automaticdetectionofforeignobjectsonchestx-rays. 2020.
[24] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocLe,Yun-HsuanSung,
ZhenLi,andTomDuerig. Scalingupvisualandvision-languagerepresentationlearningwithnoisytext
supervision. InMarinaMeilaandTongZhang,editors,Proceedingsofthe38thInternationalConference
onMachineLearning,volume139ofProceedingsofMachineLearningResearch,pages4904–4916.
PMLR,18–24Jul2021.
[25] NariJohnson,ÁngelAlexanderCabrera,GregoryPlumb,andAmeetTalwalkar. Wheredoesmymodel
underperform?ahumanevaluationofslicediscoveryalgorithms,2023.
[26] WeixinLiangandJamesZou. Metashift:Adatasetofdatasetsforevaluatingcontextualdistributionshifts
andtrainingconflicts. InInternationalConferenceonLearningRepresentations,2021.
[27] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,
andC.LawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InDavidFleet,TomasPajdla,
BerntSchiele,andTinneTuytelaars,editors,ComputerVision–ECCV2014,pages740–755,Cham,2014.
SpringerInternationalPublishing.
[28] EvanZLiu,BehzadHaghgoo,AnnieSChen,AditiRaghunathan,PangWeiKoh,ShioriSagawa,Percy
Liang,andChelseaFinn. Justtraintwice:Improvinggrouprobustnesswithouttraininggroupinformation.
InInternationalConferenceonMachineLearning,pages6781–6792.PMLR,2021.
[29] MazdaMoayeri,PhillipPope,YogeshBalaji,andSoheilFeizi. Acomprehensivestudyofimageclassifica-
tionmodelsensitivitytoforegrounds,backgrounds,andvisualattributes. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages19087–19097,2022.
[30] MazdaMoayeri,WenxiaoWang,SahilSingla,andSoheilFeizi. Spuriosityrankings: Sortingdatato
measureandmitigatebiases,2023.
[31] NicolasM.Müller,SimonRoschmann,ShahbazKhan,PhilipSperl,andKonstantinBöttinger. Shortcut
detectionwithvariationalautoencoders,2023.
[32] JunhyunNam,HyuntakCha,SungsooAhn,JaehoLee,andJinwooShin.Learningfromfailure:De-biasing
classifierfrombiasedclassifier. AdvancesinNeuralInformationProcessingSystems,33:20673–20684,
2020.
[33] Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin. Spread spurious attribute: Improving
worst-groupaccuracywithspuriousattributeestimation. InInternationalConferenceonLearningRepre-
sentations,2021.
[34] LukeOakden-Rayner,JaredDunnmon,GustavoCarneiro,andChristopherRé.Hiddenstratificationcauses
clinicallymeaningfulfailuresinmachinelearningformedicalimaging. September2019.
[35] ObiomaPelka,SvenKoitka,JohannesRückert,FelixNensa,andChristophM.Friedrich. Radiology
objectsincontext(roco):Amultimodalimagedataset. InDanailStoyanov,ZeikeTaylor,SimoneBalocco,
RaphaelSznitman, AnneMartel, LenaMaier-Hein, LucDuong, GuillaumeZahnd, StefanieDemirci,
ShadiAlbarqouni,Su-LinLee,StefanoMoriconi,VeronikaCheplygina,DianaMateus,EmanueleTrucco,
EricGranger, andPierreJannin, editors, IntravascularImagingandComputerAssistedStentingand
Large-ScaleAnnotationofBiomedicalDataandExpertLabelSynthesis,pages180–189,Cham,2018.
SpringerInternationalPublishing.
12[36] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InMarinaMeilaandTongZhang,editors,
Proceedingsofthe38thInternationalConferenceonMachineLearning,volume139ofProceedingsof
MachineLearningResearch,pages8748–8763.PMLR,18–24Jul2021.
[37] PranavRajpurkar,JeremyIrvin,AartiBagul,DaisyDing,TonyDuan,HershelMehta,BrandonYang,
KaylieZhu,DillonLaird,RobynL.Ball,CurtisLanglotz,KatieShpanskaya,MatthewP.Lungren,and
AndrewY.Ng. Mura:Largedatasetforabnormalitydetectioninmusculoskeletalradiographs,2018.
[38] ShaoqingRen,KaimingHe,RossGirshick,andJianSun. Fasterr-cnn:Towardsreal-timeobjectdetection
withregionproposalnetworks. InC.Cortes,N.Lawrence,D.Lee,M.Sugiyama,andR.Garnett,editors,
AdvancesinNeuralInformationProcessingSystems,volume28,pages91–99.CurranAssociates,Inc.,
2015.
[39] ShioriSagawa,PangWeiKoh,TatsunoriB.Hashimoto,andPercyLiang. Distributionallyrobustneural
networks. InInternationalConferenceonLearningRepresentations,2020.
[40] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,SrivatsaKun-
durthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. Laion-5b:Anopen
large-scaledatasetfortrainingnextgenerationimage-textmodels,2022.
[41] SahilSinglaandSoheilFeizi. Salientimagenet:Howtodiscoverspuriousfeaturesindeeplearning? In
InternationalConferenceonLearningRepresentations,2022.
[42] SahilSingla,BesmiraNushi,ShitalShah,EceKamar,andEricHorvitz. Understandingfailuresofdeep
networksviarobustfeatureextraction. InIEEEConferenceonComputerVisionandPatternRecognition,
CVPR2021.ComputerVisionFoundation/IEEE,2021.
[43] NimitSohoni,JaredDunnmon,GeoffreyAngus,AlbertGu,andChristopherRé. Nosubclassleftbehind:
Fine-grainedrobustnessincoarse-grainedclassificationproblems.InH.Larochelle,M.Ranzato,R.Hadsell,
M.F.Balcan,andH.Lin,editors,AdvancesinNeuralInformationProcessingSystems,volume33,pages
19339–19352.CurranAssociates,Inc.,2020.
[44] BartThomee,DavidA.Shamma,GeraldFriedland,BenjaminElizalde,KarlNi,DouglasPoland,Damian
Borth,andLi-JiaLi. Yfcc100m: thenewdatainmultimediaresearch. CommunicationsoftheACM,
59(2):64–73,January2016.
[45] EkinTiu,EllieTalius,PujanPatel,CurtisPLanglotz,AndrewYNg,andPranavRajpurkar. Expert-level
detectionofpathologiesfromunannotatedchestx-rayimagesviaself-supervisedlearning. Nat.Biomed.
Eng.,6(12):1399–1406,December2022.
[46] MayaVarma,Jean-BenoitDelbrouck,SarahHooper,AkshayChaudhari,andCurtisLanglotz. Villa:Fine-
grainedvision-languagerepresentationlearningfromreal-worlddata. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,2023.
[47] MayaVarma,MandyLu,RachelGardner,JaredDunnmon,NishithKhandwala,PranavRajpurkar,Jin
Long,ChristopherBeaulieu,KatieShpanskaya,LiFei-Fei,MatthewP.Lungren,andBhavikN.Patel.
Automatedabnormalitydetectioninlowerextremityradiographsusingdeeplearning. NatureMachine
Intelligence,1(12):578–583,December2019.
[48] JuliaKWinkler,ChristineFink,FerdinandToberer,AlexanderEnk,TeresaDeinlein,RainerHofmann-
Wellenhof, LucThomas, AimiliosLallas, AndreasBlum, WilhelmStolz, etal. Associationbetween
surgicalskinmarkingsindermoscopicimagesanddiagnosticperformanceofadeeplearningconvolutional
neuralnetworkformelanomarecognition. JAMAdermatology,155(10):1135–1141,2019.
[49] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,
RaphaelGontijo-Lopes,HannanehHajishirzi,AliFarhadi,HongseokNamkoong,andLudwigSchmidt.
Robustfine-tuningofzero-shotmodels. arXivpreprintarXiv:2109.01903,2021. https://arxiv.org/
abs/2109.01903.
[50] Shirley Wu, Mert Yuksekgonul, Linjun Zhang, and James Zou. Discover and cure: Concept-aware
mitigationofspuriouscorrelation. InICML,2023.
[51] HanXiao,KashifRasul,andRolandVollgraf. Fashion-mnist:anovelimagedatasetforbenchmarking
machinelearningalgorithms,2017.
13[52] JianxiongXiao,JamesHays,KristaA.Ehinger,AudeOliva,andAntonioTorralba. Sundatabase:Large-
scalescenerecognitionfromabbeytozoo. In2010IEEEComputerSocietyConferenceonComputer
VisionandPatternRecognition,pages3485–3492,2010.
[53] KaiYuanqingXiao,LoganEngstrom,AndrewIlyas,andAleksanderMadry. Noiseorsignal:Theroleof
imagebackgroundsinobjectrecognition. InInternationalConferenceonLearningRepresentations,2020.
[54] HanwenXu,NaotoUsuyama,JaspreetBagga,ShengZhang,RajeshRao,TristanNaumann,CliffWong,
ZelalemGero, JavierGonzález, YuGu, YanboXu, MuWei, WenhuiWang, ShumingMa, FuruWei,
Jianwei Yang, Chunyuan Li, Jianfeng Gao, Jaylen Rosemon, Tucker Bower, Soohee Lee, Roshanthi
Weerasinghe,BillJWright,AriRobicsek,BrianPiening,CarloBifulco,ShengWang,andHoifungPoon.
Awhole-slidefoundationmodelfordigitalpathologyfromreal-worlddata. Nature,630(8015):181–188,
June2024.
[55] MinghaoXu,JianZhang,BingbingNi,TengLi,ChengjieWang,QiTian,andWenjunZhang. Adversarial
domainadaptationwithdomainmixup. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume34,pages6502–6509,2020.
[56] YuYang,BesmiraNushi,HamidPalangi,andBaharanMirzasoleiman. Mitigatingspuriouscorrelationsin
multi-modalmodelsduringfine-tuning. InInternationalConferenceonMachineLearning,2023.
[57] HuaxiuYao,YuWang,SaiLi,LinjunZhang,WeixinLiang,JamesZou,andChelseaFinn. Improving
out-of-distributionrobustnessviaselectiveaugmentation. InProceedingoftheThirty-ninthInternational
ConferenceonMachineLearning,2022.
[58] LeweiYao,RunhuiHuang,LuHou,GuansongLu,MinzheNiu,HangXu,XiaodanLiang,ZhenguoLi,
XinJiang,andChunjingXu. Filip:Fine-grainedinteractivelanguage-imagepre-training,2022.
[59] LinjunZhang,ZhunDeng,KenjiKawaguchi,AmirataGhorbani,andJamesZou. Howdoesmixuphelp
withrobustnessandgeneralization? InInternationalConferenceonLearningRepresentations,2020.
[60] MichaelZhangandChristopherRe. Contrastiveadaptersforfoundationmodelgrouprobustness. In
Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural
InformationProcessingSystems,2022.
[61] MichaelZhang,NimitS.Sohoni,HongyangR.Zhang,ChelseaFinn,andChristopherRé. Correct-n-
contrast:Acontrastiveapproachforimprovingrobustnesstospuriouscorrelations,2022.
[62] ShengZhang,YanboXu,NaotoUsuyama,HanwenXu,JaspreetBagga,RobertTinn,SamPreston,Rajesh
Rao,MuWei,NaveenValluri,CliffWong,AndreaTupini,YuWang,MattMazzola,SwadheenShukla,Lars
Liden,JianfengGao,MatthewP.Lungren,TristanNaumann,ShengWang,andHoifungPoon.Biomedclip:
amultimodalbiomedicalfoundationmodelpretrainedfromfifteenmillionscientificimage-textpairs,
2024.
[63] YiwuZhong,JianweiYang,PengchuanZhang,ChunyuanLi,NoelCodella,LiunianHaroldLi,Luowei
Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based language-image
pretraining. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages16793–16803,June2022.
14Appendix
Contents
A RelatedWork 15
B ExtendedDetailsonEvaluationSettings 16
C ExtendedDetailsonRAVLMitigation 17
D ExtendedEvaluations 18
D.1 ExtendedResultsforRAVLStage1(Discovery) . . . . . . . . . . . . . . . . . . 18
D.1.1 ExtendedComparisonstoPriorApproaches . . . . . . . . . . . . . . . . . 18
D.1.2 Additionaldetailsforin-the-wildevaluations . . . . . . . . . . . . . . . . 18
D.2 ExtendedResultsforRAVLStage2(Mitigation) . . . . . . . . . . . . . . . . . . 21
D.3 ComputationalComplexityAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . 22
E ExtendedDiscussion 23
A RelatedWork
Machinelearningmodelsoftenlearnspuriouscorrelations(alsoknownasshortcuts)betweenimage
featuresandclasslabels. Forinstance, modelshavebeenshowntorelyonthepresenceofchest
tubesratherthandiseasefeatureswhenidentifyingcollapsedlungsinchestX-rays[34];surgical
skinmarkingswhendetectingmelanomafromskinlesions[48];andenvironmentalfeatureswhen
performingobjectrecognitiontasks[3].Modelsthatlearnspuriouscorrelationswillgeneralizepoorly
toreal-worldsettings. Ourworkbuildsonseveralrecentresearchdirectionsfor(i)discoveringand
(ii)mitigatingspuriouscorrelations.
DiscoveringSpuriousCorrelations. Intheunimodalsetting,priorworkshavedevelopedautomated
methods for identifying systematic errors resulting from learned spurious correlations in vision
models. Usingalabeledvalidationset,theseapproachesutilizeclusteringalgorithms[13,43]or
lightweightmodels[42,22,31]toidentifysubgroupsofimageswithhigherrorrates;forinstance,in
theexampleinFigure1,imagescontainingbutterflieswithoutflowersmaybeidentifiedasonesuch
subgroup. Givenasetofimagesintheidentifiedsubgroups,ausercanthenidentifythecommon
featuresandrectifythedataormodel. However,recentworkhassuggestedthatitisoftenchallenging
forhumanstointerpretidentifiedsubgroupsandaccuratelydeterminethesharedfeaturesresulting
in model failure [25]. Additionally, such methods often focus solely on identifying images with
higherrorrates(e.g. butterflieswithoutflowers)ratherthanidentifyingthespecificclassoffeatures
contributingtotheerror(e.g. flowers). Arelatedlineofworkhasaimedtoidentifyspuriousfeatures
usinghumansupervision[41]orexternalconceptbanks[50].
Inthevision-languagesetting,Yangetal. useanexternaloff-the-shelfobjectdetectortoannotate
features [56]. Then, for each feature, the difference in zero-shot classification accuracy between
imagescontainingthefeatureandthosewithoutthefeatureismeasured;highperformancegapsare
usedtosignalspuriousfeatures. However,theefficacyofthisapproachisreliantonthequalityof
theobjectdetectorandahuman-in-the-loopisusedtoverifyresults;also,asweshowinthiswork,
performancegapsalonearenotalwayssufficientfordiscoveringspuriousfeatures.
MitigatingSpuriousCorrelations. Thereisalineofworkaimingtomitigatespuriouscorrelations
inthecontextofdeeplearning[61,39,32,9,28,33,21]. Theseworksexplorestrategieslikedata
augmentation[55,59,57,22,50]andinstanceupsampling[39,43]. Whiletheseapproacheshave
beenexploredwidelyinunimodaltasks[29,53],mitigatingspuriouscorrelationsinvision-language
settingshasnotbeenextensivelystudied. Somepreviousworkshavestudiedthisproblemwithinthe
contextofpretrainedVLMs[60,49,1];however,theirsettingdiffersmarkedlyfromthefine-tuned
15VLMsetting,wheredatasetsarecomposedofimage-textpairswithnoclassorsubgrouplabels. In
thefine-tunedVLMsetting,existingworkspredominantlyoperateattheglobalimage-level[56],
whichisunlikelytobesufficientformitigatingfine-grainedspuriouscorrelations.
B ExtendedDetailsonEvaluationSettings
Wecreate654evaluationsettingsusingdatafromtwodomains: (1)syntheticdata(MNIST[11]
andFashionMNIST[51])and(2)real-worlddata(COCO[27]). Below,weprovideimplementation
detailsforthefourcomponentsincludedineachevaluationsetting:
1. Predefinedspuriouscorrelation: Wedefineaspuriousimagefeatureandtextualattribute
pair(eeval,aeval). ForMNISTandFashionMNIST,eeval representsaredrectangle;aeval
is generated from the set {zero, one, two, three, four five, six, seven, eight, nine} for
MNISTand{t-shirt,trouser,pullover,dress,coat,sandal,shirt,sneaker,bag,ankleboot}for
FashionMNIST.ForCOCO,wesampleeeval andaeval fromthelistofannotatedattributes.
2. Fine-tuning dataset: Vision-language fine-tuning datasets Deval are sampled from the
F
trainingsetsofMNIST,FashionMNIST,andCOCOsuchthatthepresenceoffeatureeeval
iscorrelatedwiththepresenceoftextattributeaevalasmeasuredbyCramer’sV.ForMNIST
andFashionMNIST,wesyntheticallygeneratetextcaptionsbyrandomlysamplingfromthe
followingpre-definedprompttemplates: THEIMAGESHOWSA[CLASSLABEL],THEDIGIT
APPEARSTOBE[CLASSLABEL],THEREISANIMAGESHOWINGA[CLASSLABEL],and
THENUMBERISA[CLASSLABEL]. Inordertoreflectreal-worldsettingswherespurious
features(e.g. skinmarkingsindermoscopicimages[48])maynotbeannotatedintext,text
captionsinoursyntheticsettingssolelyrefertoclasslabelsanddonotdescribethespurious
feature. ForCOCO,weusetheprovidedtextcaptions.
3. Fine-tuned VLM: We fine-tune each model M on dataset Deval using a single NVIDIA
F
A100GPUwithaninitiallearningrateof5e-5. Weuseabatchsizeof128andtrainfor
100 epochs with early stopping. We set the loss temperature as τ = 0.07. In line with
priorworksthatexplorethebenefitsoflockedimage-texttraining[2,46],wefreezethetext
encoderandonlylearnweightsfortheimageencoder.
4. Evaluationdataset: Weconstructzero-shotclassificationdatasetsDeval fromthetestsets
V
of MNIST, FashionMNIST, and COCO. For MNIST and FashionMNIST, we generate
regionboundingboxesusingequally-sizedquadrants. ForCOCO,weusetheground-truth
bounding boxes and associated labels. Evaluation datasets are sampled to ensure that
a correlation between aeval and eeval does not exist. For MNIST, we perform prompt
ensembling for zero-shot classification using the following prompts: A PHOTO OF THE
NUMBER[CLASSLABEL];THEDIGIT[CLASSLABEL];ANIMAGEOFA[CLASSLABEL];
[CLASSLABEL].ForFashionMNIST,weusethefollowingprompts: APHOTOOFA[CLASS
LABEL]; THE [CLASS LABEL]; AN IMAGE OF A [CLASS LABEL]; [CLASS LABEL]. For
COCO,weusethefollowingprompts: THERE IS A [CLASS LABEL]; A PHOTO OF THE
[CLASSLABEL];APHOTOOFA[CLASSLABEL];[CLASSLABEL].
Our654evaluationsettingsaresummarizedinTable4. DatasetsarelicensedunderCCBY,CC
BY-SA, CC BY-NC, or MIT licenses. In Figure 4, we provide examples of both synthetic and
real-worldevaluationsettings.
Table4:Evaluationsettings.Weevaluateourapproachon654settings,dividedacross2datadomains
and2modelinitializations.
Domain ModelMInitialization
CLIP-RN50 CLIP-RN101
SyntheticData 129 162
Real-WorldData 171 192
16Predefined Correlation Fine-Tuning Dataset (Vision-Language) Evaluation Dataset (Zero-Shot Classification)
Image Image Image Image Image Image Image Image
Samples with Samples with
Dataset: textual attribute textual attribute
MNIST (Synthetic) aeval Text Text Text Text aeval
Class Label Class Label Class LabelClass Label
there is an the digit the image there is an
Spurious Feature (eeval) image showing appears to shows a image showing nine nine nine nine
red rectangle a nine be nine nine a nine
Textual Attribute (aeval) Image Image Image Image Image Image Image Image
nine
Samples without Samples without
textual attribute textual attribute
aeval shth oe wT e i sm x aat tg we o at p bh pT ee ee zadx eri rgt s oi tt o th sheT fio e im vwx esat g ae imt ah ge aeT r z e se ehx i rs o ot wan in g aeval Clas ts w L oabel Clas es ig L ha tbelCla ss es v L ea nbelClas fs o uL rabel
Image Image Image Image Image Image
Samples with Samples with
textual attribute textual attribute
Dataset: aeval Text Text Text aeval
COCO (Real-World) A man lying down A man sitting on a An image of two Class Label Class Label Class Label
on a couch with a top of a green children playing couch couch couch
Spurious Feature (eeval) cat on top of him couch on the couch
person
Image Image Image Image Image Image
Textual Attribute (aeval)
couch
Samples without Samples without
textual attribute textual attribute
aeval aeval
Text Text Text
Banana slices and Black dog A wooden kitchen Class Label Class Label Class Label
pe aa n bu lut eb u pt late ter .on sju cm rep ei nn g te u lep v a ist ib oi ng . tab wle o a on dd e nb e fln oc oh r on dining table chair bed
Figure4:Exampleevaluationsettings.Here,weprovideexamplesofpredefinedspuriouscorrelations,
fine-tuningdatasets,andevaluationdatasetsassociatedwithasyntheticevaluationsetting(toprow)
andareal-worldevaluationsetting(bottomrow). Theexamplesyntheticevaluationsettingconsists
ofapredefinedspuriouscorrelationbetweenaredrectangle(spuriousimagefeatureeeval)andnine
(textualattributeaeval). Thisspuriouscorrelationisvisibleinthevision-languagefine-tuningdataset,
wherethepresenceofredrectanglesandninesarestronglycorrelated,butnotintheevaluationdataset.
Similarly,theexamplereal-worldevaluationsettingconsistsofapredefinedspuriouscorrelation
between a person (spurious image feature eeval) and couch (textual attribute aeval). Again, this
spuriouscorrelationisvisibleinthevision-languagefine-tuningdataset,wherethepresenceofpeople
andcouchesarestronglycorrelated,butnotintheevaluationdataset.
C ExtendedDetailson RAVL Mitigation
Inthissection,weextendSection4.1byprovidingadditionaldescriptionsofourregion-awareloss
function.
ForbatchB,wedefineRs asthesetofallspuriousregionsinthebatch:Rs =(cid:83) Rs.Forimage
B B Ii∈B i
I inbatchB, thefirstcomponentofourregion-awarelossfunctionLi isdesignedtomaximize
i R
embeddingsimilaritybetweennon-spuriousregionsRr andassignedclasslabelyˆ;simultaneously,
i i
Li willminimizeembeddingsimilaritybetweennon-spuriousregionsRr andotherclasslabelsin
R i
thebatch. WeformulateLi asfollows:
R
σ (Rr,yˆ)
Li =−log m i i , (5)
R (cid:80) σ (Rr,yˆ)+P(Rs)
yˆj∈B m i j B
whereP(Rs)isapenaltytermthatencouragesdissimilaritybetweenspuriousfeaturesandcorrelated
B
class labels as expressed below. Including this term in the denominator of Li is meant to pull
R
embeddingsofspuriousregionsawayfromcorrelatedclasslabels.
(cid:88)
P(Rs)= maxσ(r ,yˆ) (6)
B j k
rj∈Rs
Byˆk∈B
TheformulaforLi includestwosimilarityfunctions: σandσ . Wedefineσandσ asfollows.
R m m
Letf representaregionembeddingfunction(associatedwiththeimageencoderofVLMM)and
17letgrepresentatextembeddingfunction(associatedwiththetextencoderofVLMM). Then,for
anarbitraryregiona,thefunctionf(a)willgenerateregionembeddingf(a)∈Rdwithembedding
dimensiond. Foranarbitraryclasslabelb,g(b)willgeneratetextembeddingg(b)∈Rd. Giventhis
notation,weestablishthefollowingdefinitionsforσandσ :
m
σ(a,b)=exp(⟨f(a),g(b)⟩/τ) (7)
σ (A,b)=exp(max(⟨f(a),g(b)⟩/τ)) (8)
m
a∈A
InthelosstermLi ,thefunctionσ (Rr,yˆ)willcomputethemaximumsimilaritybetweenregions
R m i i
inRr andclasslabelyˆ. Wespecificallyusethemaximumoperationinthiscomputationsincethere
i i
arelikelytoberegionsincludedinRr thatdonotreflecttheclasslabel;forinstance,intheexample
i
providedinFigure1,theremaybenon-spuriousregionssuchastreesorleavesincludedinRr,which
i
donotalignwiththeanimalclasslabels. Themaximumoperationensuresthatthesimilaritybetween
atleastoneregioninRr andtheclasslabelshouldbehigh.
i
Thesecondcomponentofourregion-awarelossfunctionLi isdesignedtomaximizeembedding
A
similaritybetweennon-spuriousregionsRr andassignedclasslabelyˆ; simultaneously,Li will
i i A
minimizeembeddingsimilaritybetweenotherregionsinthebatchandclasslabelyˆ. Weformulate
i
Li asfollows:
A
σ (Rr,yˆ)
Li =−log m i i . (9)
A
σ
(Rr,yˆ)+(cid:80)|B|
σ
(Rr,yˆ)+(cid:80)
σ(r ,yˆ)
m i i j=1,yˆj̸=yˆi m j i rj∈Rs
B
j i
D ExtendedEvaluations
D.1 ExtendedResultsforRAVLStage1(Discovery)
Inthissection,weextendtheresultsprovidedinSection3.3withadditionalevaluationsofStage1of
RAVL.OurgoalistoevaluatetheabilityofRAVLtodiscoverfine-grainedspuriouscorrelations
betweenimagefeaturesandtextualattributes.
D.1.1 ExtendedComparisonstoPriorApproaches
WeimplementRAVLaccordingtothedetailsprovidedinSection3.1. RAVLincludesaclustering
stepthatidentifiesgroupsofvisually-similarregions. Forallevaluationsettings, weidentifythe
optimalnumberofclustersbysweepingallclustersizesrangingbetween|Y|∗2and|Y|∗5;wethen
selecttheoptimalnumberofclustersusingSilhouettescores. Weselecttheseboundstobelarger
thantheclasslabelsetsizebyseveralmultiplesinordertoensurethatclustersadequatelyseparate
distinctfeatures. Priorworks[13,43]havealsoutilizedoverclusteringapproachesforthisobjective.
We note that users can adjust the bounds based on the composition of their dataset; for instance,
complexdatasetswithdiversefeaturesmayrequirealargerrange. ForMNISTandFashionMNIST,
thesizeofthelabelset|Y|is10;ForCOCO,thesizeofthelabelsetrangesbetween2and5.
Forallbaselines,weutilizetheofficialimplementationsprovidedbytheauthors. WeadaptDomino,
George,andDistillingFailuresforoursettingbyprovidingregionembeddingsasinputratherthan
imageembeddings.
InFigure5,weprovidedanextendedversionofFigure2. WedemonstratethatRAVLconsistently
outperformsbaselinesacrosstwodomains(syntheticimagesandrealimages),twomodelinitializa-
tions(CLIP-RN50andCLIP-RN101),andfourlearnedcorrelationstrengths(measuredbyvarying
τ ∈{10,20,30,40}).
eval
D.1.2 Additionaldetailsforin-the-wildevaluations
Evaluationsonsceneclassification: Here,weprovidedextendeddetailsonourin-the-wildevalua-
tionsperformedonsceneimages(Section3.3).
Weleveragetenoff-the-shelfVLMsasourmodelM: CLIP-RN50,OpenCLIP-RN50,CLIP-RN101,
OpenCLIP-RN101,CLIP-ViTB/32,OpenCLIP-ViTB/32,CLIP-ViTB/16,OpenCLIP-ViTB/16,CLIP-
ViTL/14, andOpenCLIP-ViTL/14[36,20]. ThefourRNmodelsutilizeResNetvisionencoders
18RaVL (Ours) Spurious-Aware Domino George Distilling Failures Random
Domain: Synthetic Data Domain: Real-World Data
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
10 20 30 40 10 20 30 40
Strength of Learned Spurious Correlation (𝛕eval) Strength of Learned Spurious Correlation (𝛕eval)
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
10 20 30 40 10 20 30
Strength of Learned Spurious Correlation (𝛕eval) Strength of Learned Spurious Correlation (𝛕eval)
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
10 20 30 40 10 20 30 40
Strength of Learned Spurious Correlation (𝛕eval) Strength of Learned Spurious Correlation (𝛕eval)
Figure5: RAVLaccuratelyidentifiesspuriouscorrelations. Here,weprovideanextendedversion
ofFigure2,whichdemonstratesthatRAVLconsistentlyoutperformspriormethodsindiscovering
learned spurious correlations between image features and textual attributes. Here, we provide
Precision@10metricsforaCLIP-RN50modelfine-tunedonsyntheticdata(129settings)andreal-
world data (171 settings); a CLIP-RN101 model fine-tuned on synthetic data (162 settings) and
real-worlddata(192settings);andanaverageacrossbothmodelarchitectures.
and the six ViT models utilize Vision Transformer backbones. The CLIP models were trained
onaproprietarydatasetwith400Mimage-textpairs. OpenCLIPResNetmodelsweretrainedon
YFCC15M[44],andOpenCLIPViTmodelsweretrainedonLAION2B[40].
WeselectSUN397asourzero-shotclassificationdatasetD [52]. SUN397consistsofsceneimages
V
from397classes. Weusethetestdatafromofficialpartitionnumber1,whichconsists19,850images.
Wethenuseanoff-the-shelfregionproposalnetwork[63]toidentifycandidateregions.
ForeachVLMM,weperform397-classzero-shotsceneclassificationonSUN397. Weuseaprompt
ensembleconsistingoftwoprompttemplatesasprovidedbyCLIP[36]. Duetothelargesizeofthe
zero-shotclassificationdatasetD ,weperformclusteringusingtheCLARA(ClusteringforLarge
V
Applications)algorithm,whichisanefficientimplementationofK-Medoids,andfixthenumberof
clustersas|Y|∗2,whichis794inthiscase.
EvaluationsonchestX-rayclassification: Here,weprovidedextendeddetailsonourin-the-wild
evaluationsperformedonmedicalimages(Section3.3). Inrecentyears,arangeofvision[47,54,37]
andvision-language[45,5,4,62]modelshavebeenproposedforlearningdiagnosticpatternsin
medicalimages,andthereisacriticalneedformethodscapableofidentifyingspuriouscorrelations
inthisdomain. OurgoalistodetermineifRAVLcaneffectivelysurfacespuriouscorrelationslearned
byreal-worldfine-tunedVLMsdevelopedformedicalimageinterpretation.
Weleveragetwooff-the-shelfvariantsofthePubMedCLIPmodel[12]asourVLMM:PubMedCLIP-
RN50 and PubMedCLIP-ViTB/32. The PubMedCLIP-RN50 model utilizes a ResNet-50 vision
encoderandwasfine-tunedfromtheCLIP-RN50model. ThePubMedCLIP-ViTB/32modelutilizes
aVisionTransformerbackboneforthevisionencoderandwasfine-tunedfromtheCLIP-ViTB/32
19
05NR-PILC
101NR-PILC
egarevA
01@noisicerP
01@noisicerP
01@noisicerP
01@noisicerP
01@noisicerP
01@noisicerPmodel.BothvariantsofPubMedCLIParefine-tunedusingROCO,alargeradiologydatasetconsisting
ofimagesandcaptionscollectedfromPubMed[35].
WeselectObject-CXRasourzero-shotclassificationdatasetD . Object-CXRisadatasetof10,000
V
frontalchestX-rayscompiledfromaround300townshiphospitalsinChina [23]. Twelveradiologists
with1-3yearsofexperienceannotatedtheimages,identifyingforeignobjectswithinthelungfield
using bounding boxes, ellipses, or masks, excluding support devices. We retain only bounding
boxesandexcludechestX-rayswithoutannotations,resultingin8,726objectannotationsacross
4,372images. Forour evaluations, we usetheObject-CXRdevsplit, whichincludes974 object
annotationsacross489images. Weassignimage-levellabelstothedatasetusingtorchxrayvision[8],
alibrarythatincludesavarietyofpretrainedchestX-raymodels. Specifically, weusethe XRV-
DENSENET121-DENSENET121-RES224-ALLpretrainedmodeltoproducemulti-classlabelsfor
avarietyofdiseases,includingEnlargedCardiomediastinum,Cardiomegaly,LungOpacity,Lung
Lesion,Edema,Consolidation,Pneumonia,Atelectasis,Pneumothorax,PleuralEffusion,andFracture.
Adiseaseisidentifiedaspresentifitmeetsaconfidencethresholdof0.60.
For each PubMedCLIP VLM M, we perform binary zero-shot classification of cardiomegaly in
Object-CXR.Cardiomegalyisamedicalconditioncharacterizedbythepresenceofanenlargedheart.
After performing a manual search over the text prompt space, we identify CARDIOMEGALY and
NORMAL asthepromptsthatleadtothehighestzero-shotclassificationaccuracyforbothmodel
variants. ThePubMedCLIP-RN50modelachievesanoverallzero-shotclassificationaccuracyof
74.2,withanaccuracyof14.0onthegroupofimageswithcardiomegalyandanaccuracyof91.9
on the group of images without cardiomegaly. The PubMedCLIP-ViTB/32 achieves an overall
zero-shot classification accuracy of 39.4, with an accuracy of 80.4 on the group of images with
cardiomegalyandanaccuracyof28.0onthegroupofimageswithoutcardiomegaly. Interestingly,
giventheselectedprompts,wenotethatthePubMedCLIP-RN50andthePubMedCLIP-ViTB/32
modelsexhibitinversetrends,withPubMedCLIP-RN50achievinghigherperformanceontheclass
ofimageswithoutcardiomegalyandPubMedCLIP-ViTB/32achievinghigherperformanceonthe
classofimageswithcardiomegaly.
Given VLM M and zero-shot classification dataset Deval, we apply RAVL in order to surface
V
learnedspuriouscorrelations. Similartoourcontrolledevaluationsonsyntheticdatasets,weperform
K-Medoids clustering with the number of clusters ranging from 20 to 50. The optimal number
of clusters is selected using Silhouette distance; we use 24 clusters for PubMedCLIP-RN50 and
20 clusters for PubMedCLIP-ViTB/32. The final cluster performance gap metric G associated
c
with the top-ranked spurious feature cluster is 0.041 and 0.119 for the PubMedCLIP-RN50 and
PubMedCLIP-ViTB/32modelsrespectively.
MMooddeell Classification Task Spurious Features Identified by RaVL Zero-Shot Accuracy
Images with RaVL- Images without RaVL-
OpenCLIP Scene Classification 100 identified features 0 identified features 100
(ViT-L/14) (SUN397) 93.375.0
Class label: outdoor chicken coop
Images with RaVL- Images without RaVL-
CLIP Scene Classification 100 identified features 0 identified features 100
(ViT-B/32) (SUN397) 63.639.3
Class label: pub (indoor)
Images with RaVL- Images without RaVL-
OpenCLIP Scene Classification 100 identified features 0 identified features 100
(ResNet-101) (SUN397) 38.9 15.6
Class label: restaurant patio
Figure6: RAVLsurfacesspuriouscorrelationsinoff-the-shelfVLMs. Here,weextendFigure3with
additionalexamplesofspuriouscorrelationsdiscoveredbyRAVLinoff-the-shelf-VLMs.
ExtendedResults: InFigure6,weextendthequalitativeresultsprovidedinFigure3withadditional
examplesofspuriouscorrelationssurfacedbyRAVLinoff-the-shelfVLMs. Wemakethefollowing
observations:
• FortheOpenCLIPViT-L/14model,RaVLsurfacesafeatureclusterconsistingofgreen
plantsandfences. Weobserveaperformancegapof18.3pointsbetweenimageswithclass
20labeloutdoor chicken coopthatcontaintheRaVL-identifiedfeatureandthosethatdo
notcontainthefeature. ThissuggeststhattheOpenCLIPViT-L/14modelcanbetterclassify
anoutdoor chicken coopscenewhengreenplantsandfencesarepresent.
• FortheCLIPViT-B/32model,RaVLsurfacesafeatureclusterconsistingofpeople. We
observeaperformancegapof24.3pointsbetweenimageswithclasslabelpub (indoor)
that contain the RaVL-identified feature and those that do not contain the feature. This
suggeststhattheCLIPViT-B/32modelcanbetterclassifyapub (indoor)scenewhen
peoplearepresent.
• FortheOpenCLIPResNet-101model,RaVLsurfacesafeatureclusterconsistingofchairs.
Weobserveaperformancegapof23.3pointsbetweenimageswithclasslabelrestaurant
patiothatcontaintheRaVL-identifiedfeatureandthosethatdonotcontainthefeature.
ThissuggeststhattheOpenCLIPResNet-101modelcanbetterclassifyrestaurant patio
sceneswhenchairsarepresent.
D.2 ExtendedResultsforRAVLStage2(Mitigation)
WetrainmodelM usingasingleNVIDIAA100GPUwithaninitiallearningrateof5e-5. We
new
useabatchsizeof128andtrainfor100epochswithearlystopping. Wesetthelosstemperatureas
τ =0.07anduseλ=0.8inlossfunctionL. Inlinewithpriorworksthatutilizelockedimage-text
fine-tuning[2,46],wefreezethetextencoderandsolelylearnweightsfortheimageencoder. We
generatecandidateregionsforthefine-tuningdatasetDeval usingaregionproposalnetworkwith
F
identicalsettingstopriorwork[63].
Below,weprovideadditionalimplementationdetailsforthefivemitigationbaselinesweexplorein
thisstudy. Sincetherearealimitednumberofexistingapproachesdesignedformitigatingspurious
correlationsinfine-tunedVLMs,weadaptseveralexistingmethodsforoursettinginordertotrain
modelM :
new
• StandardVLMFine-Tuning: WeperformstandardVLMfine-tuningwiththeoriginalloss
functionL usedtotrainmodelM. Inourexperiments,L istheCLIPobjective[36].
CL CL
• UpsampledVLMFine-Tuning: WeperformVLMfine-tuningwiththeoriginallossfunction
L usedtotrainmodelM. Inourexperiments,L istheCLIPobjective[36]. Weutilize
CL CL
aweightedsamplertoupsampleminoritygroupsduringtraining;classandsubgrouplabels
arederivedfromStage1ofRAVLasdetailedinSection4.1.
• VL-ERM:Sinceempiricalriskminimization(ERM)istraditionallyusedinunimodalclas-
sification settings, we adapt ERM for our multimodal setting by incorporating an extra
contrastivevision-languageobjectivefunction;thislossfunctionisintendedtoensurethat
VLM M learns image-text relationships during training. Specifically, the final loss
new
functionduringtrainingisL =λL +(1−λ)L . Here,L takestheform
VLERM CL ERM CL
oftheoriginallossfunctionusedtotrainmodelM;inourexperimentsL istheCLIP
CL
objective[36]. Wesetλ=0.8. Duringtraining,weapplyERMtozero-shotclassification
logitscomputedusingimageembeddingsandtextembeddingsofclasslabels. Weutilizea
weightedsamplertoupsampleminoritysubgroups;classandsubgrouplabelsarederived
fromStage1ofRAVLasdetailedinSection4.1.
• VL-GDRO [39]: Since GDRO is traditionally used in unimodal classification settings,
weadaptGDROforourmultimodalsettingbyincorporatinganextracontrastivevision-
languageobjectivefunction;thislossfunctionisintendedtoensurethatVLMM learns
new
image-textrelationshipsduringtraining. Specifically,thefinallossfunctionduringtraining
isL = λL +(1−λ)L . Here,L takestheformoftheoriginalloss
VLGDRO CL GDRO CL
functionusedtotrainmodelM;inourexperimentsL istheCLIPobjective[36]. Weset
CL
λ=0.8. Duringtraining,weapplyGDROtozero-shotclassificationlogitscomputedusing
imageembeddingsandtextembeddingsofclasslabels. Inlinewithstandardpractice,we
utilizeaweightedsamplertoupsampleminoritysubgroups;classandsubgrouplabelsare
derivedfromStage1ofRAVLasdetailedinSection4.1.
• Spurious-AwareMitigation[56]: Spurious-awaremitigationaimstoaddressspuriouscorre-
lationsinVLMsusingacombinationoffivelossfunctions: oneCLIPobjectivefunction,
two contrastive image objective functions meant to address spurious correlations in the
imagespace,andtwocontrastivelanguageobjectivefunctionsmeanttoaddressspurious
21Table5: RAVLeffectivelymitigatesspuriouscorrelationsacrossvariousmodelinitializations. Here,
we provide an extended version of Table 3 with a breakdown of results by model initialization
(CLIP-RN50vs. CLIP-RN101). OurresultsdemonstratethatRAVLconsistentlyoutperformsprior
methodsinmitigatingspuriouscorrelations. WereportmeanImageOverall(Img. Overall),Image
Worst Group (Img. WG), Region Overall (Reg. Overall), and Region Worst Group (Reg. WG)
metricsacrossourreal-worldevaluationsettings.
Method DiscoveryPrecision@10>0.6 DiscoveryPrecision@10>0.8
Img.Overall Img.WG Reg.Overall Reg.WG Img.Overall Img.WG Reg.Overall Reg.WG
StandardFT 64.2 35.8 73.2 50.1 64.4 36.0 74.3 51.8
UpsampledFT 65.2 36.7 73.7 51.0 65.2 38.0 74.3 52.3
VL-ERM 66.0 30.0 73.4 45.2 65.4 28.9 73.7 45.4
VL-GDRO 66.8 31.6 74.1 45.8 66.1 29.6 74.7 46.3
Spurious-Aware 67.4 31.6 74.0 45.2 66.2 28.2 74.5 45.2
RAVL(Ours) 67.9 36.9 77.8 55.4 67.8 38.6 79.0 56.5
StandardFT 63.9 28.9 71.3 45.0 64.7 27.6 72.0 44.4
UpsampledFT 67.4 38.4 74.6 52.8 67.8 37.5 75.0 53.2
VL-ERM 70.5 33.5 77.0 53.4 70.8 32.2 77.4 54.0
VL-GDRO 70.5 34.9 76.5 53.1 70.5 32.1 76.8 54.1
Spurious-Aware 71.3 34.8 78.1 53.9 71.2 32.4 78.4 53.9
RAVL(Ours) 71.0 40.4 79.5 59.2 71.8 42.2 79.8 59.8
correlationsinthetextspace. WenotethatSpurious-AwareMitigationwasexplicitlyde-
signedforthefine-tunedVLMsetting. WefollowtheimplementationofSpurious-Aware
Mitigationprovidedby[56]. Inourwork,sincewesolelyfine-tunedthevisionencodersof
VLMsM,weuseaversionofSpurious-AwareMitigationwiththeCLIPobjectivefunction
andtwocontrastiveimageobjectivefunctions.
Priorworksonmodelrobustnesspredominantlyevaluatemodelperformanceusingimageworst-
group scores [39]. In addition to image worst-group accuracy, we also report region overall and
regionworst-groupaccuracies,whichevaluatetheextenttowhichtheVLMunderstandsfine-grained
features. Region-levelaccuraciesarecomputedbyperformingzero-shotclassificationwithregion
embeddingsandcomparingpredictedlabelstotheground-truthregion-levellabelsprovidedinthe
zero-shotclassificationdataset.
In Table 5, we provide an extended version of Table 3 with a breakdown of results by model
initialization(CLIP-RN50andCLIP-RN101). WedemonstratethatRAVLconsistentlyoutperforms
priormethodsacrossbothmodelinitializations. InTable6,weprovideanextendedversionofTable
3withabreakdownofresultsbythelearnedcorrelationstrengthoftheoriginalVLMM. RAVL
consistentlyoutperformspriormethodsacrossfourcorrelationstrengthsτ ∈{10,20,30,40}.
eval
D.3 ComputationalComplexityAnalysis
Inthissection,weprovideananalysisofthecomputationalcomplexityofRAVL.RAVLiscompu-
tationallyinexpensive;inparticular,theRAVLdiscoverystagecanberunefficientlyonCPUand
themitigationstageaddsonlyasmallcomputationaloverhead. Below,weprovideananalysisof
computationalcomplexityforeachstageofRAVL.
ComputationalcomplexityanalysisofRAVLStage1: ThediscoverystageofRAVLisspecifically
designedtoberunonalabeledvalidationdatasetD ;inreal-worldsettings,validationdatasetsare
V
oftenrelativelysmallinsizeduetothehumaneffortneededforsecuringlabels,renderingthisstage
ascomputationallyinexpensivefordiverseapplications. Evenifthevalidationdatasetislargeinsize,
RAVLoperatesefficientlyasfollows:
• First,RAVLpreprocessesimagesbydecomposingeachimageintocandidateregions;there
areavarietyofwaysinwhichausercandecomposeanimageintoregions,suchasbyusing
equal-sizedsegments(e.g. quadrants)orrunninginferencewithregionproposalnetworks
(RPNs). Bothmethodsareinexpensiveandonlyneedtoberunonceinanofflinemanner.
Similarapproacheshavebeenappliedtolarge-scaledatasetsinpriorwork[63].
• Then,embeddingsneedtobegeneratedforeachregion,whichcanbedonebyutilizing
VLMMforinference(forwardpassesonly). Acrossasetof10FashionMNISTandCOCO
22
05NR-PILC
101NR-PILCTable6: RAVLeffectivelymitigatesspuriouscorrelationsacrosslearnedcorrelationstrengths. Here,
weprovideanextendedversionofTable3withabreakdownofresultsbythelearnedcorrelation
strength(τ ∈ {10,20,30,40}oftheoriginalmodelM. Weusethesubsetof106evaluation
eval
settingswhereRAVLStage1Precision@10isgreaterthan0.8. OurresultsdemonstratethatRAVL
consistentlyoutperformspriormethodsinmitigatingspuriouscorrelationsacrossvariouscorrelation
strengthsandmodelinitializations. WereportmeanImageWorstGroup(Img. WG)andRegion
WorstGroup(Reg. WG)metricsacrossourreal-worldevaluationsettings. Wenotethatthereareno
validevaluationsettingsforCLIP-RN101whenthelearnedcorrelationstrengthτ oftheoriginal
eval
modelMissetto40.
Method τ =10 τ =20 τ =30 τ =40
eval eval eval eval
Img.WG Reg.WG Img.WG Reg.WG Img.WG Reg.WG Img.WG Reg.WG
StandardFT 36.0 51.8 29.0 43.8 31.4 46.6 26.0 39.5
UpsampledFT 38.0 52.3 27.7 43.5 33.4 47.3 33.4 46.5
VL-ERM 28.9 45.4 23.3 38.8 25.7 38.3 22.9 32.5
VL-GDRO 29.6 46.3 22.7 37.1 28.6 37.6 26.4 28.2
Spurious-Aware 28.2 45.2 22.2 37.9 24.6 38.4 17.1 30.6
RAVL(Ours) 38.6 56.5 35.3 56.8 41.0 60.2 38.0 53.5
StandardFT 27.6 44.4 26.4 43.1 17.3 35.1 – –
UpsampledFT 37.5 53.2 36.0 49.4 25.6 40.0 – –
VL-ERM 32.2 54.0 32.1 51.5 18.1 42.5 – –
VL-GDRO 32.1 54.1 30.7 52.8 16.6 45.3 – –
Spurious-Aware 32.4 53.9 30.4 51.2 19.3 48.3 – –
RAVL(Ours) 42.2 59.8 39.7 56.4 28.8 54.0 – –
evaluationsettings,weobserveembeddinggenerationtotakeameanof24.5secondsona
singleA100GPU.
• Finally,givencandidateregionsandcorrespondingembeddings,theremainderoftheRaVL
discovery procedure (clustering and computation of metrics) can be run completely on
CPU.Acrossasetof10evaluationsettingsonCOCOandFashionMNIST,weobservethat
clusteringandcomputationofmetricsrequireameanof3.4secondstorun.
ComputationalcomplexityanalysisofRAVLStage2: ThemitigationstageofRAVLrequires
finetuningaVLMM . Acrossasetof10evaluationsettingsonCOCOandFashionMNIST,we
new
observethattheinclusionofourfine-grainedregion-awarelossfunctionatthisstageaddsanaverage
of0.15secondspertrainingstep(onasingleA100GPU)incomparisontotheoriginalfine-tuning
procedureforM.
E ExtendedDiscussion
SocietalImpact: Thegoalofourworkistoimproverobustnessoffine-tunedVLMstospurious
correlations. AsVLMsbecomemorecommonplaceinsociety,wehopethatourapproachcanenable
userstobetterdetectandmitigatemodelfailurespriortodeployment. Wealsonotethatourwork
includes a series of evaluations on medical images; rigorous clinical testing is necessary before
robustnessapproachesaredeployedinhealthcaresettings.
Limitations: Inlinewithpriorworksinvision-onlyandvision-languagesettings,ourmethodis
specificallydesignedtosurfaceandmitigatelocal,fine-grainedspuriousfeatures. Theremaybesome
sourcesofspurioussignalthatdonotmanifestinthisway;forinstance,featureslikeimagebrightness
orgendercanbeconsideredglobalfeatures,wherethespurioussignalisnotlocalizedtoaparticular
imageregion. Ourapproachisnotdesignedfortheseglobalspuriousfeatures. Rather,ourproblem
settingisinspiredbythemanyreal-world,practicalexamplesofregion-levelspuriousfeaturesthat
havebeendemonstratedinliteraturetoaffectmodelperformance,suchasimage-levelmarkingsin
dermoscopicimages[48],medicaldevicesinradiographs[34],andtextmarkersinmedicalimages
[10].
23
05NR-PILC
101NR-PILC