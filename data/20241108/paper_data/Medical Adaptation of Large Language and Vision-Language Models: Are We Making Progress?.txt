Medical Adaptation of Large Language and Vision-Language Models:
Are We Making Progress?
DanielP.Jeong1,SaurabhGarg1,2,ZacharyC.Lipton1,4,MichaelOberst3,4
1MachineLearningDepartment,CarnegieMellonUniversity
2MistralAI
3DepartmentofComputerScience,JohnsHopkinsUniversity
4AbridgeAI
{danielje,sgarg2,zlipton}@cs.cmu.edu,moberst@jhu.edu
Correspondence:danielje@cs.cmu.edu
Abstract (e.g.,medicaldiagnosis,informationretrievalfrom
clinical documents, patient triaging) (Fries et al.,
Severalrecentworksseektodevelopfounda-
2022a;Mooretal.,2023a). State-of-the-artperfor-
tion models specifically for medical applica-
manceonvariousmedicalbenchmarksistypically
tions,adaptinggeneral-purposelargelanguage
achievedbymassive-scaleclosed-sourcemodels,
models (LLMs) and vision-language models
(VLMs)viacontinuedpretrainingonpublicly
suchas GPT-4 (OpenAI,2023a,b), MED-GEMINI
availablebiomedicalcorpora. Theseworkstyp- (Saab et al., 2024; Yang et al., 2024), and MED-
icallyclaimthatsuchdomain-adaptivepretrain- PALM (Singhal et al., 2023a,b; Tu et al., 2024),
ing (DAPT) improves performance on down- oftenperformingonparwithhumansonmedical
streammedicaltasks,suchasansweringmedi- licensingexamsandopen-endedconsumerhealth
callicensingexamquestions. Inthispaper,we
question-answering(QA)tasks. However,thegen-
comparesevenpublic“medical”LLMsandtwo
eral lack of transparency in these models, high
VLMsagainsttheircorrespondingbasemodels,
APIusagecosts,andpatientdataprivacyconcerns
arrivingatadifferentconclusion: allmedical
VLMsandnearlyallmedicalLLMsfailtocon- make their integration into routine clinical work-
sistentlyimproveovertheirbasemodelsinthe flowschallenging(MarksandHaupt,2023).
zero-/few-shotpromptingregimeformedical Toaddresssuchconcerns,recentworkshavepro-
question-answering(QA)tasks. Forinstance, posed cheaper, open-source alternatives through
acrossthetasksandmodelpairsweconsiderin
domain-adaptivepretraining(DAPT;Gururangan
the3-shotsetting,medicalLLMsonlyoutper-
et al., 2020), where a pretrained open-source
formtheirbasemodelsin12.1%ofcases,reach
general-domainmodel—suchas LLAMA(Touvron
a(statistical)tiein49.8%ofcases,andaresig-
nificantlyworsethantheirbasemodelsinthe
et al., 2023a,b; Meta, 2024) or MISTRAL (Jiang
remaining 38.2% of cases. Our conclusions et al., 2023) in the language space, and LLAVA
arebasedon(i)comparingeachmedicalmodel (Liuetal.,2023)orOPEN-FLAMINGO(Awadalla
head-to-head,directlyagainstthecorrespond- etal.,2023)inthevision-languagespace—iscon-
ing base model; (ii) optimizing the prompts tinuallypretrainedonbiomedical(image-)textcor-
for each model separately; and (iii) account-
porafrompublicsourcessuchasPubMedandmed-
ing for statistical uncertainty in comparisons.
icaltextbooks. Whilesomepriorworksshowthat
Whilethesebasicpracticesarenotconsistently
medical models pretrained from scratch only us-
adopted in the literature, our ablations show
thattheysubstantiallyimpactconclusions. Our ingdomain-specificcorporacanoutperformthose
findings suggest that state-of-the-art general- trained via DAPT, both in the context of BERT-
domainmodelsmayalreadyexhibitstrongmed- styleencoder-onlymodels(Devlinetal.,2019;Gu
icalknowledgeandreasoningcapabilities,and etal.,2021;Yangetal.,2022)anddecodermodels
offerrecommendationstostrengthenthecon-
(Taylor et al., 2022; Luo et al., 2022; Hernandez
clusionsoffuturestudies.
et al., 2023; Bolton et al., 2024), the DAPT ap-
proachhasbecomecommonpractice,resultingina
1 Introduction
trendwherethereleaseofamorecapablegeneral-
Recent advances in autoregressive large lan- domainmodelistypicallyfollowedbytherelease
guagemodels(LLMs)andvision-languagemodels ofitsmedicalcounterpart.
(VLMs)haveattractedinterestfrompractitioners Despite the widespread adoption of medical
inmedicine,wherethesemodelsholdgreatpoten- DAPT,theclaimedimprovementsinperformance
tialtotransformvariousaspectsofclinicalpractice are worth scrutinizing. While the story is intu-
4202
voN
6
]LC.sc[
1v81140.1142:viXra(a) (b)
Medical Model Wins Tie Medical Model Loses
100 General-Domain Head-to-Head Comparison: Zero-/Few-shot Prompting 18.8% 12.5%
VLM/LLM
 80 34.2% 38.2%
Select Best
 Select Best
 Prompt Format Prompt Format 60
Medical
 vs + DAPT 75.0% 81.3%
Select Best Select Best
 40 56.3% 49.8%
Examples Examples Medical

VLM/LLM
 20
+ 0 9.4% 6.3% 12.1% 6.3%
LLM VLM LLM VLM
Zero-shot 3-shot
Figure1: MedicalLLMsandVLMstrainedviadomain-adaptivepretraining(DAPT)showlimitedimprovement
overtheirgeneral-domaincounterparts. (a)Overviewofourhead-to-headevaluationapproachforeachpairof
general-domain(blue)andmedicallyadaptedLLM/VLM(red). (b)Win/tie/lossrate(%)ofmedicalmodelsvs.
theircorrespondingbasemodelsacrossall(modelpair,QAdataset)combinations. Winratereferstotheproportion
of(modelpair,QAdataset)combinationswhereamedicalmodelshowsastatisticallysignificantimprovement.
itive, more recent base models (e.g., LLAMA-3- evaluate do not consistently improve over their
8B (Meta, 2024)) already exhibit strong off-the- general-domain counterparts on various medical
shelfperformanceonmedicalbenchmarkswithout (visual) QA tasks (Figure 1). We compare sev-
anyadaptation(e.g.,OpenMedicalLLMLeader- eralpairsofgeneral-domainandmedicallyadapted
board(Paletal.,2024)),andgivenalackoftrans- LLMs/VLMs (see Table 1), whose only differ-
parencyaboutthepretrainingcorporausedtotrain ences lie in medical DAPT (i.e., one model is
the general-domain model in the first place, they the base model, from which the other is derived
mayalreadybetrainedonrelevantmedicaltext. via medical DAPT). For each pair, we compare
theirperformancesfromzero-/few-shotprompting
Perhapsmoreconcerningisthelackofapples-
(Radford et al., 2019; Brown et al., 2020), after
to-applescomparisonsintheliterature. First,med-
independentlyselectingthe“best”promptformat
ical models resulting from DAPT are often only
and few-shot examples for each model based on
comparedagainstbaselineswithdifferentarchitec-
thevalidationsetandaccountingforstatisticalun-
tures(e.g., CLINICAL-CAMEL-70B (Tomaetal.,
certaintyinmodelcomparison.
2023) vs. GPT-4 (OpenAI, 2023a)) and under
Our findings (Section 4) suggest that state-of-
inconsistent evaluation setups (e.g., MEDITRON-
the-art general-domain models may already ex-
70B(Chenetal.,2023)fine-tunedonMedQA(Jin
hibitstrongmedicalknowledgeandreasoningca-
etal.,2020)vs. non-fine-tuned MED42-V1-70B
pabilities that can be leveraged effectively when
(Christopheetal.,2024)),whichcanconfoundthe
promptedappropriately.
interpretationofresults. Second,thecommonprac-
Our main contributions can be summarized as
ticeofusingasingle,fixedpromptingsetup(e.g.,
follows:
prompt format, choice of few-shot examples) for
allmodelsunderevaluationalsowarrantsconcern, 1. We provide a comprehensive head-to-head
as LLM/VLM behavior is extremely sensitive to comparisonbetweenstate-of-the-artgeneral-
such design decisions (Jiang et al., 2020; Zhao domainLLMs/VLMsandtheirmedicalDAPT
etal.,2021;Ceballos-Arroyoetal.,2024),andthe counterpartsonvariousmedical(visual)QA
“optimal” choice of such details rarely correlates benchmarks, to investigate the effectiveness
betweendifferentmodels(Sclaretal.,2024). ofDAPTformedicalspecialization.
In this paper, we perform an apples-to-apples 2. We find that after optimizing the prompts
comparisonthataddressestheseconcerns,compar- for medical and general-domain models in-
ingsevenmedicalLLMsandtwomedicalVLMs dependently, all medical VLMs and nearly
againsttheirgeneral-domainbasemodels. Wefind all medical LLMs that we evaluate fail to
that,forallbutoneLLMpair—BIOMISTRAL-7B consistently improve over their correspond-
(Labraketal.,2024)vs. MISTRAL-7B-INSTRUCT- inggeneral-domainbasemodels.
V0.1 (Jiang et al., 2023), a pair of models that 3. Weshowthatusingasingle,fixedpromptfor-
performs fairly poorly in absolute terms—the matandchoiceoffew-shotexamplesforall
open-source medical LLMs and VLMs that we models without testing for statistical signif-
fo
noitroporP
)tesataD
AQ
,riaP
ledoM(
)%(
snoitanibmoCTable1: Summaryofopen-sourceautoregressiveVLMandLLMpairsusedforevaluation.
ModelClass GeneralDomain MedicalDomain MedicalAdaptationCorpora
LLAMA-3-70B-INSTRUCT(Meta,2024) OPENBIOLLM-70B(PalandSankarasubbu,2024) Undisclosed
ClinicalPracticeGuidelines(e.g.,CDC,WHO)
LLAMA-2-70B(Touvronetal.,2023b) MEDITRON-70B(Chenetal.,2023)
PubMedArticles(S2ORC;Loetal.,2020)
ShareGPT
LLAMA-2-70B(Touvronetal.,2023b) CLINICAL-CAMEL-70B(Tomaetal.,2023) 20kPubMedArticlesPublishedBefore2021
LLM Random4kSubsetofMedQA(Jinetal.,2020)
LLAMA-3-8B(Meta,2024) OPENBIOLLM-8B(PalandSankarasubbu,2024) Undisclosed
ClinicalPracticeGuidelines(e.g.,CDC,WHO)
LLAMA-2-7B(Touvronetal.,2023b) MEDITRON-7B(Chenetal.,2023)
PubMedArticles(S2ORC;Loetal.,2020)
MISTRAL-7B-INSTRUCT-V0.1(Jiangetal.,2023) BIOMISTRAL-7B(Labraketal.,2024) PubMedArticles(PMCOpenAccessSubset)
LLAMA-2-7B-CHAT(Touvronetal.,2023b) BIOMEDGPT-LM-7B(Luoetal.,2023) PubMedArticles(S2ORC;Loetal.,2020)
LLAVA-V0-7B(Liuetal.,2023) LLAVA-MED-7B(Lietal.,2023) PubMedArticles(PMC-15M;Zhangetal.,2023)
VLM
MedicalTextbooks(MTB;Mooretal.,2023b)
OPEN-FLAMINGO-9B(Awadallaetal.,2023) MED-FLAMINGO-9B(Mooretal.,2023b)
PubMedArticles(PMC-OA;Linetal.,2023)
icance can lead to overly optimistic conclu- and PubMed abstracts (Jin et al., 2019). Medi-
sionsaboutthebenefitsfrommedicalDAPT. calVLMssuchasLLAVA-MED(Lietal.,2023),
adaptedfromLLAVA(Liuetal.,2023);and MED-
2 RelatedWork
FLAMINGO (Moor et al., 2023b), adapted from
DAPT(Gururanganetal.,2020)isatransferlearn-
OPEN-FLAMINGO (Awadalla et al., 2023); also
performwellonvisualQAtasksbasedonradiol-
ingapproach,whereapretrainedmodelisfurther
ogy(Lauetal.,2018;Liuetal.,2021)andpathol-
pretrainedondomain-specificdataforbetteralign-
ogyimages(Heetal.,2020)andacademicexams
menttoatargetdomainofinterest(e.g.,medicine,
(Yueetal.,2024). Theseencouragingresultshave
law). Several studies show that language models
establishedDAPTasago-toapproachfortraining
trainedviaDAPToftenoutperformtheirgeneral-
a medically specialized model, a conclusion that
domaincounterpartsondomain-specifictasks,such
were-examineinthiswork.
as claim detection from blog posts (Chakrabarty
et al., 2019), named entity recognition from Ger-
3 ExperimentalSetup
mannovels(KonleandJannidis,2020),andjudg-
mentpredictionforlegalcases(Xiaoetal.,2021). ToinvestigatetheeffectivenessofmedicalDAPT
Inthemedicaldomain,priorworksbasedonBERT- inimprovingzero-/few-shotperformance,wecom-
styleencoder-onlylanguagemodels(Devlinetal., pare7medicalLLMsand2medicalVLMsagainst
2019), such as BIOBERT (Lee et al., 2019) and theirgeneral-domaincounterpartsinpairs(Figure
CLINICALBERT(Alsentzeretal.,2019),showthat 1(a)), on 13 textual QA datasets and 8 visual QA
medicalDAPTimprovesfine-tuningperformance datasets,respectively. Themodelsineachpairare
ontaskssuchasmedicalconceptextractionfrom exactly identical in model architecture and scale,
patientreports(Uzuneretal.,2011),identification andtheironlydifferenceliesinwhethertheywere
of gene-disease relations from PubMed abstracts additionally pretrained on medical data. We also
(Dog˘anetal.,2014;Bravoetal.,2015;Krallinger notethatwhilesomeofdatasetsusedforevaluation
et al., 2017), and natural language inference on containbothclosed-ended(i.e.,hasclearground-
clinicalnotes(RomanovandShivade,2018). truthanswers)andopen-endedquestions,wefocus
Morerecentworkssuggestthatdecoder-based ourevaluationsontheformer,whereanobjective,
autoregressiveLLMsandVLMstrainedviamed- quantitativeassessmentofmedicalknowledgeand
icalDAPTalsoshowstrongperformanceonvari- reasoningcapabilitiesispossible. Forreproducibil-
ous medical tasks. Medical LLMs such as MED- ityofourresults,weopen-sourcethesourcecode
ITRON (Chenetal.,2023),adaptedfrom LLAMA- usedforallofourevaluationsdescribedbelowvia
2 (Touvron et al., 2023b); and BIOMISTRAL ourGitHubrepository1.
(Labrak et al., 2024), adapted from MISTRAL-
Models. In Table 1, we provide a summary of
7B-INSTRUCT-V0.1 (Jiangetal.,2023);perform
all of the LLM and VLM pairs that we use for
well on knowledge-intensive QA tasks based on
evaluation,alongwithdetailsaboutthepretraining
medicallicensingandacademicexams(Jinetal.,
2020; Pal et al., 2022; Hendrycks et al., 2021) 1https://github.com/taekb/eval-medical-daptcorporausedforadaptationtothemedicaldomain. ing the training examples repeated in the test set
ForLLAVA(Liuetal.,2023),weusetheveryfirst andremovingallduplicatesinbothsets. Wethen
version(v0)thatusesVICUNA-V0 (Chiangetal., take a random 80–20 split on the training set to
2023)astheLLMbackbone,as LLAVA-MED (Li create a new train–validation split, as the official
etal.,2023)wasadaptedfromthatparticularver- splitdoesnotincludeavalidationset. ForMMMU-
sion. Forallmodels,weusethecheckpointsmade Medical,whichdoesnothaveapublictestset,we
availableviaHuggingFace. Inallexperiments,we randomlyselect5examplesfromtheofficialvali-
generatepredictionsfromeachmodelvia(i)greedy dationsetforvalidation,andreservetheremaining
decoding(i.e.,samplingwithtemperatureT = 0) 25examplesfortesting. Forallotherdatasets,we
and(ii)constraineddecoding. Forconstrainedde- use the official split as provided. We provide the
coding, we constrain the token vocabulary to be remainingdatasetdetailsinAppendixA.
oneoftheanswerchoiceletters(e.g.,oneof[“A”,
Evaluation Metric. Since we focus on closed-
“B”,“C”,“D”]forafour-choiceQAdataset)and
endedQAtasks,weuseexact-matchaccuracyas
treattheanswerchoicewiththehighesttokenprob-
ourmainevaluationmetric. FollowingtheHolistic
abilityasagivenmodel’sprediction.
Evaluation of Language Models (HELM) bench-
Textual QA Datasets. For textual QA, we use mark(Liangetal.,2023),whenweconsidergreedy
MedQA(Jinetal.,2020),MedMCQA(Paletal., decoding, we treat the text generated by a model
2022),PubMedQA(Jinetal.,2019),andMMLU- (withoutanyconstraintsonthevocabulary)tobeits
Medical (Hendrycks et al., 2021) for evaluation. prediction,andcheckforanexactmatchbetween
MMLU-Medical refers to a subset of MMLU thepredictionandthecorrectansweruptoprimi-
corresponding to 9 subjects related to medicine: tivestringoperations(e.g.,lower-casing,removing
anatomy,clinicalknowledge,collegebiology,col- whitespace/punctuation). Tohandlecaseswhere
legemedicine,highschoolbiology,medicalgenet- themodelsimplyrepeatsthelistofanswerchoices
ics,nutrition,professionalmedicine,andvirology. orproducesanambiguousanswer(e.g.,selecting
For MedQA, we use the official train-validation- multipleanswerchoices), wetakeaconservative
testsplitsasprovidedthroughBigBio(Friesetal., approach and treat the prediction to be incorrect,
2022b). We note that MedQA has two versions, even if there is a match. Meanwhile, to quantify
onewithfouranswerchoicesperquestionandthe theextentofimprovementfrommedicalDAPT,we
otherwithfive,andweusebothforevaluation. For alsoconsidertherelativeaccuracyofthemedical
MedMCQA,whichdoesnothaveapublictestset, model with respect to the general-domain model.
wefollowtheapproachtakenbyWuetal.(2024) Formally,wedefinerelativeexact-matchaccuracy
and Labrak et al. (2024), taking a random 80–20 as E[1[f (x) = y] − 1[f (x) = y]] ∈
medical general
train–validationsplitoftheofficialtrainingsetand [−1,1],wheref andf denotethemed-
medical general
usingtheofficialvalidationsetfortesting. ForPub- ical and general-domain models, x and y denote
MedQA,wefollowSinghaletal.(2023a),usingthe the input prompt and answer in a QA pair from
211kartificallygeneratedQAsamplesfortraining, thetestset,and1[·]denotestheindicatorfunction.
andtakinga50–50splitonthe1kexpert-labeled This metric quantifies the difference in accuracy
examples. ForMMLU-Medical,weusetheofficial betweenthemedicalmodelandthegeneral-domain
splitasprovided. Weprovidetheremainingdataset model. Todistinguishthetwometrics,wereferto
detailsinAppendixA. theformerastheabsoluteexact-matchaccuracyin
subsequentdiscussions.
Visual QA Datasets. For visual QA, we use
VQA-RAD(Lauetal.,2018),PathVQA(Heetal., AssessingStatisticalSignificance. Giventherel-
2020), SLAKE (Liu et al., 2021), and MMMU- atively small size of test datasets in medical QA
Medical(Yueetal.,2024)forevaluation. MMMU- benchmarks,itisimportanttoassesswhetherthe
MedicalreferstoasubsetofMMMUcorrespond- perceivedimprovementsinperformancefrommed-
ingto5subjectsrelevanttomedicine: basicmedi- icalDAPTareattributabletochance. Toaccount
calscience,clinicalmedicine,diagnosticsandlab- for statistical uncertainty, we use the percentile
oratorymedicine,pharmacy,andpublichealth. For bootstrap, re-sampling (with replacement) ques-
VQA-RAD,weaddressthetrain-testleakageand tionsfromthetestsettogetasampleofthesame
duplication issues in the official train–test splits, size as the original test set. Within each resam-
previouslynotedbyMooretal.(2023b),byremov- ple,wecomputethedifferenceinaccuracyforthePrompt Format Sampling Prompt Format & Few-Shot Example Selection
Sample Prompt Format Components
Question Header Options Header Answer Header Validation Data
‘Question’ ~ {‘Question’, ‘’}
 ‘Options’ ~ {‘Options’, ‘Choices’, ‘’}
 ‘Answer’ ~ {‘Answer’, ‘The answer is’, ‘’}
 Sampled Prompt Formats
‘QUESTION’ = Casing(‘Question’) ‘OPTIONS’ = Casing(‘Options’) ‘ANSWER’ = Casing(‘Answer’)
Best Format

Spacing Option Wrapper Option Spacing Header Casing Prompt Format 1 +

Best Examples
‘:\n’ ~ {‘:’, ‘:\n’, ‘::’, ...} ‘( )’ ~ {‘( )’, ‘[ ]’, ...} ‘; ’ ~ {‘; ’, ‘\n’, ‘, ’, ...} Casing ~ {f(x)=x.upper(), f(x)=‘### x’, ...}
Construct Prompt Format
Example Set 1
QUESTION :\n <Question> OPTIONS :\n (A) <Option 1> ; ··· ; (D) <Option 4> ANSWER :\n Sampled Few-Shot Examples
Figure2: Overviewofthepromptformatsampling(left)andpromptingstrategyselection(right)process.
pairedmodels,andrepeatthisprocessfor10,000 andselectthebestpairoutof(10+1)×10 = 110
iterations. Theresultingdistributionofrelativeac- thatresultsinthehighestvalidationexact-matchac-
curacyisusedtoderivea95%confidenceinterval, curacy. Giventhatagridsearchatthisscalecanbe
andwejudgeadifferencetobestatisticallysignif- computationallyexpensive,especiallyfordatasets
icant if this interval does not cross zero. We do like MedMCQA that contain 37k validation QA
not perform any type of multiple-testing correc- pairs(seeTableA1),werandomlysubsample500
tion,whichwouldhavetheeffectofloweringthe validationQApairsfordatasetsthathavemorethan
numberofcomparisonsdeemedtobesignificant. 500. Using the vLLM framework (Kwon et al.,
2023) for sampling model outputs, this leads to
3.1 Zero-/Few-shotPromptingwith
a runtime of around 5–15 minutes per trial, on 4
Model-SpecificPromptSelection
NVIDIAA6000GPUsforthe70Bmodelsand2
In this section, we provide an overview of our GPUsfortheothers. Wethengeneratepredictions
approach to assess whether medical DAPT leads onthetestsetusingtheselectedpromptformatand
to statistically significant improvements in zero- few-shotsamples. Inthezero-shotsetting,weonly
/few-shotmedicalQAperformance. Forfew-shot searchoverthepromptformats.
prompting,weconsiderthe3-shotsettingtoensure To define the prompt format search space, we
thattheinputpromptisshorterthanthecontextwin- follow the approach by Sclar et al. (2024) and
dowsizesforallmodelsevaluated. Forevaluation, constructacontext-freegrammarofsemantically
wepayspecialattentiontotwoaspects. First,lan- equivalentyetsyntacticallydistinctpromptformats
guagemodelsarehighlysensitivetothechoiceof (Figure2,left). Forthemedicalmodelsthathavea
promptingstrategy(e.g.,promptformat,choiceof specificpromptformatdesignedandrecommended
few-shotexamples),whereseeminglyinsignificant for closed-ended QA tasks (e.g., BIOMISTRAL
changes to the prompt can lead to idiosyncratic (Labrak et al., 2024)), we fix the prompt format
model behavior (Jiang et al., 2020; Zhao et al., towhatisprovidedandonlysearchoverthechoice
2021). Second,priorworksshowthatthe“optimal” of few-shot examples. In the case when such in-
choiceofpromptformatrarelycorrelatesbetween formation is missing or only partially available
different models (Sclar et al., 2024), suggesting (see Table C1), we search over both the prompt
thatusingasingle,fixedpromptforallmodelsfor formats and few-shot examples. For instruction-
comparisoncanresultinmisleadingconclusions. tuned models, which typically have a structured
To ensure a fair comparison that isolates the conversationalformat(e.g.,‘### User:...###
impact of medical DAPT, we treat the choice of Assistant:...”) that is expected, we use the
promptformatandfew-shotexamplesasadditional sampledquestionandanswertemplatestoformat
hyperparameterswhengeneratingpredictions,and each “user” query and “assistant” response. We
tailor them to each model independently (Figure providetheremainingdetailsinAppendixB–C.
2). Wefirstrandomlysample10plausibleprompt
formatsfromapredefinedsearchspaceand10dif- 4 Results
ferentsetsoffew-shotexamplesfromthetraining
set ofeach dataset. We then search over all pairs Here, we summarize the main findings from the
of prompt formats (plus one additional manually zero-/few-shotpromptingexperimentsoutlinedin
designed default format) and few-shot examples, Section 3. Unless specified otherwise, we focusOpenBioLLM-70B MediTron-70B OpenBioLLM-8B MediTron-7B BioMistral-7B BioMedGPT-LM-7B
Llama-3-70B-Instruct Clinical-Camel-70B Llama-3-8B Llama-2-7B Mistral-7B-Instruct-v0.1 Llama-2-7B-Chat
Llama-2-70B
1.0
0.8
0.6
0.4
0.2
0.0
No Improvement
0.4
0.2
0.0
0.2
0.4
MedQA MedQA MedMCQA PubMedQA MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU:
(4 Options) (5 Options) Anatomy Clinical College College Medical Professional High Virology Nutrition
Knowledge Biology Medicine Genetics Medicine School
Biology
Figure3: MedicalLLMsdonotconsistentlyshowastatisticallysignificantimprovementovertheirgeneral-domain
counterpartsinthe3-shotsetting,afterindependentlyselectingthebestpromptformatandexamplesforeachmodel.
Toprowshowstheabsoluteexact-matchaccuraciesonthetestset,andbottomrowshowstherelativeexact-match
accuraciesalongwith95%confidenceintervalsderivedviabootstrappingonthetestset(seeSection3). Here,we
showtheresultsforgreedydecoding. The3-shotresultsforconstraineddecodingaresimilar(seeFigureE1(b)).
onthegreedydecodingresultsinsubsequentdis- MedQA,asthemodelhasalreadybeentrainedon
cussions and include the results for constrained a subset of the official training split (see Table 1
decodinginAppendixE.Overall,wefindthatall inTomaetal.(2023)). ForVLMs,weshowboth
medical VLMs and nearly all medical LLMs fail zero-shot and 3-shot results, as LLAVA-V0-7B
toconsistentlyimproveovertheirgeneral-domain and LLAVA-MED-7B werenotpretrainedtohan-
counterpartsinthezero-shotandfew-shotprompt- dleinputswithmultipleimages. Wecalculatethe
ingregimes. Moreover,wedemonstratetheimpor- confidenceintervalsviabootstrappingonthetest
tanceofrigorousexperimentaldesigninsurfacing set,asdescribedinSection3.
thisfinding—performingpairwisemodelcompari- ThetoprowofFigure3showsthattheabsolute
sonwithasingle,fixedpromptoptimizedonlyfor exact-matchaccuraciesaremostlysimilarbetween
themedicalmodel,whileignoringstatisticaluncer- each model pair across all datasets and model
tainty,paintsamisleadinglyoptimisticpictureof scales,withmarginalperformanceimprovements.
medicalDAPTperformance. Infact,thebottomrowofFigure3showsthatonly
2outof7medicalLLMs—OPENBIOLLM-70B
Finding 1: After model-specific prompt selec- and BIOMISTRAL-7B—showstatisticallysignifi-
tion, the vast majority of medical models fail cantimprovementsinperformance,withthe95%
to consistently show a statistically significant confidence intervals crossing zero relative accu-
improvementoverthegeneral-domainmodels. racy in most cases for the other models. When
InFigures3–4,weshowtheabsoluteandrelative compared against their corresponding base mod-
exact-match accuracies achieved by the medical els, OPENBIOLLM-70B achieves a win rate of
andgeneral-domainLLMsandVLMsinthezero- 30.8%,tierateof69.2%,andlossrateof0%,while
/few-shotpromptingregime. ForLLMs,weonly BIOMISTRAL-7B achieves a win rate of 46.2%,
showthe3-shotpromptingresultsinthemaintext tierateof53.8%,andlossrateof0%(TableD2).
(see Appendix D for results in the zero-shot set- Notably, MEDITRON-7B and BIOMEDGPT-LM-
ting, which are similar). We exclude the results 7B actuallyshowsignificantlyworseperformance
for CLINICAL-CAMEL-70B on both versions of than their base models, with loss rates of 76.9%
ycaruccA
ycaruccA
evitaleRLLaVA-Med-7B LLaVA-v0-7B Med-Flamingo-9B Open-Flamingo-9B
(a) (b)
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.8 0.8
0.6 No Improvement 0.6 No Improvement
0.4 0.4
0.2 0.2
0.0 0.0
0.2 0.2
0.4 0.4
0.6 0.6
0.8 0.8
VQA-RAD PathVQA SLAKE MMMU: MMMU: MMMU: MMMU: MMMU: VQA-RAD PathVQA SLAKE MMMU: MMMU: MMMU: MMMU: MMMU:
Basic Clinical DiagnosticsPharmacy Public Basic Clinical DiagnosticsPharmacy Public
Medical Medicine & Health Medical Medicine & Health
Science Lab Medicine Science Lab Medicine
Figure4: MedicalVLMsdonotshowastatisticallysignificantimprovementovertheirgeneral-domaincounterparts
inthe(a)zero-shotand(b)3-shotsettings,afterindependentlyselectingthebestpromptformatandexamplesfor
eachmodel. Toprowshowstheabsoluteexact-matchaccuraciesonthetestset,andbottomrowshowstherelative
exact-matchaccuraciesalongwith95%confidenceintervalsderivedviabootstrappingonthetestset(seeSection3).
Here,weshowtheresultsforgreedydecoding. Theresultsforconstraineddecodingaresimilar(seeFigureE2).
and92.3%,respectively. Similartrendsholdforthe Wesimilarlyobservelimitedimprovementsover-
zero-shotsetting(FigureD1andTableD1),where allwithconstraineddecoding(seeAppendixE.1).
only CLINICAL-CAMEL-70B and BIOMISTRAL- AsshowninFigureE5(a),whenweaggregatethe
7B showstatisticallysignificantimprovements. results over all (model pair, QA dataset) combi-
nations,medicalLLMsachievewin/tie/lossrates
Wenotethat,whileOPENBIOLLM-70Bshows
of 16.9%/68.6%/14.5% in the zero-shot setting
improvement in the 3-shot setting, it does not
and 11.2%/74.1%/14.7% in the 3-shot setting,
show improvement in the zero-shot setting (win-
while medical VLMs achieve win/tie/loss rates
ning on 7.7% and losing on 23.1% of tasks, see
of 6.3%/87.5%/6.3% in the zero-shot setting and
TableD1),andviceversaforCLINICAL-CAMEL-
0%/93.8%/6.3% in the 3-shot setting. In fact, no
70B(winningon0%oftasksandlosingon36.4%
medicalVLMshowsimprovementovertheirbase
oftasksinthe3-shotsetting,seeTableD2),leav-
modelsregardlessofthedecodingstrategy. Mean-
ing BIOMISTRAL-7B astheonlymedicalLLM
while,asshowninTablesE1–E2,wefindthatsome
thatwinsmorethanitlosesagainstitsbasemodel
medicalLLMsshowlargerimprovementswithcon-
(MISTRAL-7B-INSTRUCT-V0.1)inbothsettings,
straineddecoding(notably,MEDITRON-70Band
albeitwithrelativelylowabsoluteperformance.
MEDITRON-7B), although the results are mixed
In Figure 4, we make similar observations for (e.g.,CLINICAL-CAMEL-70Bperformsworsein
medical VLMs in both zero-shot and 3-shot set- thezero-shotsettingwithconstraineddecoding).
tings, where both LLAVA-MED-7B and MED- In summary, these results suggest that when
FLAMINGO-9B are virtually indistinguishable prompted with the “right” set of examples in an
fromtheirbasemodelsintermsofperformance, appropriate format, general-domain models may
showing no statistically significant improve- alreadyexhibitthecapacitytoachieveperformance
ments. TablesD1–D2showthatLLAVA-MED-7B competitivewithmedicallyadaptedmodels,onvar-
achieveswin/tie/lossratesof12.5%/62.5%/25.0% iousmedicalQAtasks.
in both zero-shot and 3-shot settings, while
MED-FLAMINGO-9B achieves win/tie/loss rates Finding2: Usingasingle, fixedpromptforall
of 0%/87.5%/12.5% in the zero-shot setting and modelsandoverlookingstatisticaluncertainty
0%/100%/0%inthe3-shotsetting. Meanwhile,we may overestimate the performance benefits of
notethattheconfidenceintervalsfortheMMMU- medicalDAPT. BasedonFinding1,wefurther
Medical datasets tend to be much wider than for investigate whether the conclusions differ if the
theothervisualQAdatasets,asthetestsetsonlyin- samepromptisusedforeachpairofmedicaland
clude25QAexamplesforeachsubject(TableA1). general-domainmodels. Inparticular,weconsider
ycaruccA
ycaruccA
evitaleR
ycaruccA
ycaruccA
evitaleRMedical Model Wins Tie Medical Model Loses
(a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d)
100 12.1% 18.8% 12.5% 12.1% 12.5% 6.3%
80 34.3% 29.5% 37.5% 38.2% 46.2%
56.3% 56.3% 56.3% 43.1% 64.1% 69.0% 56.3% 60
62.5% 63.6%
75.0% 81.3%
40 56.3% 70.5% 49.8%
62.5%
53.9% 20 44.9% 35.9% 25.0% 43.8% 24.3% 31.0% 37.5% 43.8% 43.8%
0 9.4% 6.3% 12.1% 6.3%
LLM VLM LLM VLM
Zero-shot 3-shot
Figure5:Optimizingthepromptforonlythemedicalmodelandcomparingmodelswithoutaccountingforstatistical
uncertaintycanoverestimatetheperformanceimprovementsfrommedicalDAPT.Weshowthewin/tie/lossrate(%)
ofmedicalmodelsvs. theirbasemodelsacrossall(modelpair,QAdataset)combinations,when(a)independently
optimizingthepromptforeachmodelandperformingstatisticaltesting,(b)optimizingthepromptonlyforthe
medicalmodelandperformingstatisticaltesting,(c)independentlyoptimizingthepromptforeachmodelwithout
statisticaltesting,and(d)optimizingthepromptonlyforthemedicalmodelwithoutstatisticaltesting. Here,we
showtheresultsforgreedydecoding. Theresultsforconstraineddecodingaresimilar(seeFigureE5).
whether selecting a prompt only for the medical ontheirabsoluteaccuracies. Notably,inthezero-
model,followingSection3.1,andusingitforthe shot setting, the win rate increases from 9.4% to
corresponding general-domain model can widen 70.5%formedicalLLMsandfrom6.3%to62.5%
the performance gap between each pair. We also formedicalVLMs,whenonlyperformingprompt
assess whether this gap becomes amplified when selection for the medical model and comparing
modelsarecomparedwithoutaccountingforstatis- based on absolute accuracy. Figure E5 in Ap-
ticaluncertainty,whichisoftendoneinpractice. pendixE.2showsasimilartrendinthewin/tie/loss
InFigure5,weshowhowthewin/tie/lossrates rates, when the model predictions are generated
ofthemedicalmodels,computedoverall(model viaconstraineddecoding. Theseresultshighlight
pair,QAdataset)combinations,changeaswevary the importance of accounting for LLM/VLM sen-
thefollowingaspectsoftheexperimentalsetup: sitivity to the prompting details, as suggested by
Sclaretal.(2024),andthestatisticaluncertainty
1. selectpromptsforeachmodelindependently
inmodelcomparison,inordertodrawreliablecon-
vs. onlybasedonthemedicalmodel;
clusionsabouttheeffectivenessofmedicalDAPT.
2. determineawinforthemedicalmodelbased
onconfidenceintervalsinrelativeaccuracyvs. 5 DiscussionandConclusion
rawabsoluteaccuracy.
Inthis work, we investigatedtheeffectiveness of
We note that when comparing each model pair DAPT for training medically specialized LLMs
based on absolute accuracy, there are no ties, as andautoregressiveVLMssuitableforknowledge-
thereal-valuedabsoluteaccuraciesarerarelyiden- intensive medical (visual) QA tasks. To that end,
tical. InAppendixD,weincludeFiguresD2–D3to wecomparedseveralpairsofstate-of-the-artmedi-
showhowtheabsoluteandrelativeexact-matchac- calLLMs/VLMstotheirgeneral-domaincounter-
curacieschangewhenthepromptisonlyoptimized parts,whoseonlydifferenceslieinmedicalDAPT
forthemedicalmodel. WealsoincludeTablesD3– andareexactlyidenticalinmodelarchitectureand
D4toshowchangesinwin/tie/lossrates. Weshow scale. Ourworkdivergesfrompriorworksbypro-
thesamesetofresultsforconstraineddecodingin vidingadirectapples-to-applescomparisonofmed-
FiguresE3–E4andTablesE3–E4inAppendixE. icalandgeneral-domainmodelswhileaccounting
Overall,wefindthatforbothLLMsandVLMs, forLLM/VLMsensitivitytopromptingdetailsand
theperformanceimprovementfromusingamedi- assessingthestatisticalsignificanceoftheresults.
callyadaptedmodelinsteadofitsgeneral-domain Acrossbothmodelclassesandallmodelscales,
counterpart can be substantially overestimated wefoundthattheperformancebenefitsfrommedi-
when(i)thepromptisonlytailoredtothemedical calDAPTlargelydisappearwhenwe(i)tailorthe
model;and(ii)themodelsarecomparedonlybased prompt format and choice of few-shot examples
fo
noitroporP
)tesataD
AQ
,riaP
ledoM(
)%(
snoitanibmoCto each medical and general-domain model sepa- somenewlyreleasedmodelsdoinfactyieldbetter
rately; and (ii) account for statistical uncertainty zero-orfew-shotperformanceonmedicalQA.
inmodelcomparison. Inparticular,wefoundthat Second,wefocusinthispaperonthenarrower
whenweoptimizethepromptonlyforthemedical task of closed-ended medical QA. In part, this
modelandcompareeachmodelpairbasedontheir choice reflects the fact that such benchmarks are
absoluteaccuracieswithoutaccountingforuncer- well-standardizedandhighlypublicized. However,
tainty,theperformanceimprovementsfrommedi- they do not reflect the breadth of possible appli-
calDAPTcanbeoverestimated,potentiallyleading cations of LLMs and VLMs in medical domains.
tounreliableconclusionsaboutthebenefitsofmed- Forinstance,Singhaletal.(2023b)showthatmed-
ical DAPT. For example, in the zero-shot setting, ical LLMs such as MED-PALM-2 can produce
evaluationunderthissetupleadstotheconclusion physician-levelanswerstoopen-endedconsumer
thatmedicalLLMsandVLMs,onaverage,outper- health queries, and Agrawal et al. (2022) demon-
formthecorrespondinggeneral-domainmodelsin strate the potential of using LLMs for extracting
70.5% and 62.5% of all QA tasks, while the im- informationfromstructuredclinicalnotes. Some
provementsareinrealitystatisticallysignificantin would argue that such tasks are a more realistic
only9.4%and6.3%oftasksafteroptimizingthe applicationofsuchmodelsinpractice,anditiscer-
promptforeachmodeltoensureafaircomparison. tainlypossiblethatananalysislikeourswouldfind
Our findings suggest that for state-of-the-art improved performance on such tasks, though we
general-domainLLMsandVLMs,theperformance donotinvestigatethesetasksinthepresentwork.
benefitsfromadditionallypretrainingonmedical Third, we do not consider downstream fine-
datafrompublicsourcessuchasPubMedmaybe tuningofmodelssubjecttomedicalDAPT.Inpart,
limited. Notably,almostallofthemedicalmodels this reflects issues of computational cost (e.g., to
usedinourevaluationusePubMedastheprimary fine-tune 70B-parameter models) and the added
source of pretraining data for medical adaptation complexityofreproducingafine-tuningprocedure,
(Table 1), while open-source datasets commonly versususingpubliclyavailablemodelcheckpoints.
usedforpretrainingthegeneral-domainbasemod- However,weacknowledgethatzero-andfew-shot
elsinthefirstplace(e.g.,thePile(Gaoetal.,2020), performance are only part of a broader narrative
S2ORC (Lo et al., 2020)) often already include around the claimed benefits of medical DAPT,
PubMed data. Prior works also suggest that the whichgenerallyincludestheadditionalclaimthat
intrinsiccapacityofLLMstosolveadownstream it provides a better initialization for downstream
taskislargelyobtainedduringtheinitialpretrain- fine-tuning(Chenetal.,2023;Lietal.,2023).
ingphase,andthatpost-trainingadjustmentsand Whileweacknowledgethelimitationsabove,we
prompt engineering efforts may only help elicit do not believe they detract from the value of this
theexistingcapabilities(ReynoldsandMcDonell, work. Wehopethatourresultscallattentiontoa
2021; Min et al., 2022). Thus, we argue that any needforrigoroushead-to-headevaluationswhen
claimsaboutimprovementfromaproposedmed- making similar claims of improved performance
icalDAPTprocedureshouldbeevidencedbyrig- viamedicalDAPT,whetherwithothermodels,on
oroushead-to-headcomparisonsagainstthecorre- otherclinicaltasks,orwithrespecttofine-tuning
spondinggeneral-domainmodel,inordertodraw versuszero-/few-shotperformance.
reliableconclusionsaboutitseffectiveness.
Acknowledgments
6 Limitations We gratefully acknowledge DARPA (FA8750-
23-2-1015), ONR (N00014-23-1-2368), NSF
Wediscussourfindingswiththefollowingcaveats.
(IIS2211955),UPMC,HighmarkHealth,Abridge,
First,thereisavastandgrowingsetofpaperson
FordResearch,Mozilla,thePwCCenter,Amazon
applyingmedicalDAPTtovariousgeneral-domain
AI,JPMorganChase,theBlockCenter,theCenter
basemodels,andwecouldnothopetocompareall
for Machine Learning and Health, and the CMU
publiclyavailablemodelshere. Whileweselected
Software Engineering Institute (SEI) via Depart-
themodelstocoverawiderangeofgeneral-domain
mentofDefensecontractFA8702-15-D-0002,for
basemodelsandmodelscales(7B–70B)(Table1)
theirgeneroussupportofourresearch.
andincludedsomeofthelatestmodels(e.g.,OPEN-
BIOLLMandLLAMA-3),itisalwayspossiblethatReferences Salvi, Matteo Pagliardini, Simin Fan, Andreas
Köpf, Amirkeivan Mohtashami, Alexandre Salli-
Monica Agrawal, Stefan Hegselmann, Hunter Lang,
nen, Alireza Sakhaeirad, Vinitra Swamy, Igor
YoonKim,andDavidSontag.2022. LargeLanguage
Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle
ModelsareFew-ShotClinicalInformationExtractors.
Montariol, Mary-Anne Hartley, Martin Jaggi, and
InProceedingsofthe2022ConferenceonEmpirical
Antoine Bosselut. 2023. MediTron-70B: Scaling
MethodsinNaturalLanguageProcessing(EMNLP).
Medical Pretraining for Large Language Models.
arXiv:2311.16079.
Emily Alsentzer, John Murphy, William Boag, Wei-
Hung Weng, Di Jindi, Tristan Naumann, and
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
MatthewMcDermott.2019. PubliclyAvailableClin-
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
icalBERTEmbeddings. InProceedingsofthe2nd
Zhuang,YonghaoZhuang,JosephE.Gonzalez,Ion
ClinicalNaturalLanguageProcessingWorkshop.
Stoica, andEricP.Xing.2023. Vicuna: AnOpen-
SourceChatbotImpressingGPT-4with90%*Chat-
AnasAwadalla,IrenaGao,JoshuaGardner,JackHes-
GPTQuality.
sel,YusufHanafy,WanrongZhu,KalyaniMarathe,
Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon
ClémentChristophe,PraveenKKanithi,PrateekMun-
Kornblith,PangWeiKoh,GabrielIlharco,Mitchell
jal, Tathagata Raha, Nasir Hayat, Ronnie Ra-
Wortsman, and Ludwig Schmidt. 2023. Open-
jan, Ahmed Al-Mahrooqi, Avani Gupta, Muham-
Flamingo: An Open-Source Framework for Train-
mad Umar Salman, Gurpreet Gosal, Bhargav
ingLargeAutoregressiveVision-LanguageModels.
Kanakiya, Charles Chen, Natalia Vassilieva, Boul-
arXiv:2308.01390.
baba Ben Amor, Marco AF Pimentel, and Shadab
Khan. 2024. Med42 – Evaluating Fine-Tuning
Elliot Bolton, Abhinav Venigalla, Michihiro Ya-
Strategies for Medical LLMs: Full-Parameter vs.
sunaga, David Hall, Betty Xiong, Tony Lee, Rox-
Parameter-EfficientApproaches. arXiv:2404.14779.
ana Daneshjou, Jonathan Frankle, Percy Liang,
MichaelCarbin,andChristopherD.Manning.2024.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
BioMedLM: A 2.7B Parameter Language Model
Kristina Toutanova. 2019. BERT: Pre-training of
TrainedOnBiomedicalText. arXiv:2403.18421.
DeepBidirectionalTransformersforLanguageUn-
ÀBravo,JPiñero,NQueralt-Rosinach,MRautschka, derstanding. InProceedingsofthe2019Conference
and LI Furlong. 2015. Extraction of Relations Be- of the North American Chapter of the Association
tweenGenesandDiseasesfromTextandLarge-scale for Computational Linguistics: Human Language
Data Analysis: Implications for Translational Re- Technologies,Volume1(LongandShortPapers).
search. BMCBioinformatics,16(55).
RezartaIslamajDog˘an,RobertLeaman,andZhiyong
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Lu. 2014. NCBI Disease Corpus: A Resource for
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind DiseaseNameRecognitionandConceptNormaliza-
Neelakantan,PranavShyam,GirishSastry,Amanda tion. JournalofBiomedicalInformatics,47:1–10.
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
JasonFries,EthanSteinberg,ScottFleming,Michael
Gretchen Krueger, Tom Henighan, Rewon Child,
Wornow, Yizhe Xu, Keith Morse, Dev Dash, and
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
NigamShah.2022a. HowFoundationModelsCan
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
AdvanceAIinHealthcare.
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, ChristopherBerner, SamMcCandlish, Alec
JasonFries,LeonWeber,NatashaSeelam,GabrielAl-
Radford, Ilya Sutskever, and Dario Amodei. 2020.
tay,DebajyotiDatta,SamueleGarda,SunnyKang,
Language Models are Few-Shot Learners. In Ad-
RosalineSu,WojciechKusa,SamuelCahyawijaya,
vances in Neural Information Processing Systems
FabioBarth,SimonOtt,MatthiasSamwald,Stephen
(NeurIPS).
Bach, Stella Biderman, Mario Sänger, Bo Wang,
AlbertoMarioCeballos-Arroyo,MonicaMunnangi,Ji- Alison Callahan, Daniel León Periñán, Théo Gi-
udingSun,KarenZhang,JeredMcInerney,ByronC. gant,PatrickHaller,JennyChim,JosePosada,John
Wallace, and Silvio Amir. 2024. Open (Clinical) Giorgi,KarthikRangasaiSivaraman,MarcPàmies,
LLMsareSensitivetoInstructionPhrasings. InPro- Marianna Nezhurina, Robert Martin, Michael Cul-
ceedingsofthe23rdWorkshoponBiomedicalNatu- lan, Moritz Freidank, Nathan Dahlberg, Shubhan-
ralLanguageProcessing(BioNLP). shu Mishra, Shamik Bose, Nicholas Broad, Yanis
Labrak,ShlokDeshmukh,SidKiblawi,AyushSingh,
TuhinChakrabarty,ChristopherHidey,andKathyMcK- MinhChienVu,TrishalaNeeraj,JonasGolde,Albert
eown. 2019. IMHO Fine-Tuning Improves Claim VillanovadelMoral,andBenjaminBeilharz.2022b.
Detection. InProceedingsofthe2019Conference BigBio: AFrameworkforData-CentricBiomedical
of the North American Chapter of the Association NaturalLanguageProcessing. InAdvancesinNeural
for Computational Linguistics: Human Language InformationProcessingSystems(NeurIPS).
Technologies,Volume1(LongandShortPapers).
LeoGao,StellaBiderman,SidBlack,LaurenceGold-
Zeming Chen, Alejandro Hernández-Cano, Angelika ing, Travis Hoppe, Charles Foster, Jason Phang,
Romanou,AntoineBonnet,KyleMatoba,Francesco Horace He, Anish Thite, Noa Nabeshima, ShawnPresser, and Connor Leahy. 2020. The Pile: An LeonardKonleandFotisJannidis.2020. Domainand
800GBDatasetofDiverseTextforLanguageModel- TaskAdaptivePretrainingforLanguageModels. In
ing. arXiv:2101.00027. Workshop on Computational Humanities Research
(CHR).
YuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto
Martin Krallinger, Obdulia Rabal, Saber Ahmad
Usuyama,XiaodongLiu,TristanNaumann,Jianfeng
Akhondi, Martín Pérez Pérez, Jesus Santamaría,
Gao, and Hoifung Poon. 2021. Domain-Specific
GaelPérezRodríguez,GeorgiosTsatsaronis,Ander
LanguageModelPretrainingforBiomedicalNatural
Language Processing. Association for Computing Intxaurrondo,JoséAntonioBasoLópez,UmeshK.
Machinery (ACM) Transactions on Computing for Nandal,ErinM.vanBuel,AmbikaChandrasekhar,
Healthcare,3(1). Marleen Rodenburg, Astrid Lægreid, Marius A.
Doornenbal,JulenOyarzábal,AnáliaLourenço,and
AlfonsoValencia.2017. OverviewoftheBioCreative
Suchin Gururangan, Ana Marasovic´, Swabha
VIChemical-ProteinInteractionTrack. InProceed-
Swayamdipta,KyleLo,IzBeltagy,DougDowney,
ingsoftheBioCreativeVIWorkshop.
andNoahA.Smith.2020. Don’tStopPretraining:
AdaptLanguageModelstoDomainsandTasks. In
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
AnnualMeetingoftheAssociationforComputational
Sheng,LianminZheng,CodyHaoYu,JosephE.Gon-
Linguistics(ACL).
zalez, Hao Zhang, and Ion Stoica. 2023. Efficient
Memory Management for Large Language Model
XuehaiHe,YichenZhang,LuntianMou,EricXing,and
ServingwithPagedAttention. InACMSymposium
PengtaoXie.2020. PathVQA:30000+Questionsfor
onOperatingSystemsPrinciples.
MedicalVisualQuestionAnswering. arXivpreprint
arXiv:2003.10286. YanisLabrak,AdrienBazoge,EmmanuelMorin,Pierre-
Antoine Gourraud, Mickael Rouvier, and Richard
DanHendrycks,CollinBurns,StevenBasart,AndyZou, Dufour. 2024. BioMistral: A Collection of Open-
MantasMazeika,DawnSong,andJacobSteinhardt. SourcePretrainedLargeLanguageModelsforMedi-
2021. MeasuringMassiveMultitaskLanguageUn- calDomains. arXiv:2402.10373.
derstanding. InInternationalConferenceonLearn-
ingRepresentations(ICLR). Jason J. Lau, Soumya Gayen, Asma Ben Abacha,
and Dina Demner-Fushman. 2018. A Dataset of
Evan Hernandez, Diwakar Mahajan, Jonas Wulff, ClinicallyGeneratedVisualQuestionsandAnswers
MicahJSmith,ZacharyZiegler,DanielNadler,Pe- about Radiology Images. Nature Scientific Data,
terSzolovits,AlistairJohnson,andEmilyAlsentzer. 5(180251).
2023. Do We Still Need Clinical Language Mod-
JinhyukLee,WonjinYoon,SungdongKim,Donghyeon
els? In Proceedings of the Conference on Health,
Kim,SunkyuKim,ChanHoSo,andJaewooKang.
Inference,andLearning(CHIL).
2019. BioBERT: A Pre-trained Biomedical Lan-
guage Representation Model for Biomedical Text
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
Mining. Bioinformatics,36(4):1234–1240.
sch,ChrisBamford,DevendraSinghChaplot,Diego
de las Casas, Florian Bressand, Gianna Lengyel,
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto
Guillaume Lample, Lucile Saulnier, Lélio Re-
Usuyama,HaotianLiu,JianweiYang,TristanNau-
nard Lavaud, Marie-Anne Lachaux, Pierre Stock,
mann, Hoifung Poon, and Jianfeng Gao. 2023.
TevenLeScao,ThibautLavril,ThomasWang,Timo-
LLaVA-Med:TrainingaLargeLanguage-and-Vision
théeLacroix,andWilliamElSayed.2023. Mistral
AssistantforBiomedicineinOneDay. InAdvances
7B. arXiv:2310.06825.
inNeuralInformationProcessingSystems(NeurIPS)
DatasetsandBenchmarksTrack.
ZhengbaoJiang,FrankF.Xu,JunAraki,andGraham
Neubig.2020. HowCanWeKnowWhatLanguage Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
ModelsKnow? TransactionsoftheAssociationfor Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
ComputationalLinguistics,8:423–438. Zhang,DeepakNarayanan,YuhuaiWu,AnanyaKu-
mar,BenjaminNewman,BinhangYuan,BobbyYan,
DiJin,EileenPan,NassimOufattole,Wei-HungWeng, CeZhang,ChristianCosgrove,ChristopherD.Man-
Hanyi Fang, and Peter Szolovits. 2020. What Dis- ning,ChristopherRé,DianaAcosta-Navas,DrewA.
easeDoesThisPatientHave? ALarge-scaleOpen Hudson, Eric Zelikman, Esin Durmus, Faisal Lad-
DomainQuestionAnsweringDatasetfromMedical hak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue
Exams. arXiv:2009.13081. Wang,KeshavSanthanam,LaurelOrr,LuciaZheng,
Mert Yuksekgonul, Mirac Suzgun, Nathan Kim,
QiaoJin,BhuwanDhingra,ZhengpingLiu,WilliamCo- NeelGuha, NiladriChatterji, OmarKhattab, Peter
hen,andXinghuaLu.2019. PubMedQA:ADataset Henderson, Qian Huang, Ryan Chi, Sang Michael
for Biomedical Research Question Answering. In Xie, Shibani Santurkar, Surya Ganguli, Tatsunori
Conference on Empirical Methods in Natural Lan- Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav
guageProcessingandtheInternationalJointCon- Chaudhary,WilliamWang,XuechenLi,YifanMai,
ferenceonNaturalLanguageProcessing(EMNLP- YuhuiZhang,andYutaKoreeda.2023. HolisticEval-
IJCNLP). uationofLanguageModels. arXiv:2211.09110.WeixiongLin,ZihengZhao,XiaomanZhang,Chaoyi Ankit Pal, Pasquale Minervini, Andreas Geert
Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Motzfeldt,AryoPradiptaGema,andBeatriceAlex.
2023. PMC-CLIP:ContrastiveLanguage-ImagePre- 2024. OpenMedicalLLMLeaderboard.
trainingusingBiomedicalDocuments. InMedical
ImageComputingandComputerAssistedInterven- AnkitPalandMalaikannanSankarasubbu.2024. Open-
tion(MICCAI). BioLLMs: AdvancingOpen-SourceLargeLanguage
ModelsforHealthcareandLifeSciences.
BoLiu,Li-MingZhan,LiXu,LinMa,YanYang,and
Xiao-Ming Wu. 2021. SLAKE: A Semantically- Ankit Pal, Logesh Kumar Umapathi, and Malaikan-
LabeledKnowledge-EnhancedDatasetforMedical nan Sankarasubbu. 2022. MedMCQA: A Large-
VisualQuestionAnswering. arXiv:2102.09542. scaleMulti-SubjectMulti-ChoiceDatasetforMedi-
caldomainQuestionAnswering. InProceedingsof
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae theConferenceonHealth,Inference,andLearning
Lee.2023. VisualInstructionTuning. InAdvancesin (CHIL).
NeuralInformationProcessingSystems(NeurIPS).
Alec Radford, Jeff Wu, Rewon Child, David Luan,
KyleLo,LucyLuWang,MarkNeumann,RodneyKin- DarioAmodei,andIlyaSutskever.2019. Language
ney,andDanielWeld.2020. S2ORC:TheSemantic Models are Unsupervised Multitask Learners. In
ScholarOpenResearchCorpus. InProceedingsof OpenAIBlog.
the58thAnnualMeetingoftheAssociationforCom-
putationalLinguistics. LariaReynoldsandKyleMcDonell.2021. PromptPro-
grammingforLargeLanguageModels: Beyondthe
Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Few-Shot Paradigm. In Extended Abstracts of the
Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. 2021CHIConferenceonHumanFactorsinComput-
BioGPT: Generative Pre-trained Transformer for ingSystems.
BiomedicalTextGenerationandMining. InBrief-
ingsinBioinformatics. Alexey Romanov and Chaitanya Shivade. 2018.
LessonsfromNaturalLanguageInferenceintheClin-
Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, icalDomain. InProceedingsofthe2018Conference
Yushuai Wu, Mu Qiao, and Zaiqing Nie. onEmpiricalMethodsinNaturalLanguageProcess-
2023. BioMedGPT: Open Multimodal Gener- ing(EMNLP).
ative Pre-trained Transformer for Biomedicine.
KhaledSaab,TaoTu,Wei-HungWeng,RyutaroTanno,
arXiv:2308.09442.
David Stutz, Ellery Wulczyn, Fan Zhang, Tim
Mason Marks and Claudia E. Haupt. 2023. AI Chat- Strother,ChunjongPark,ElaheVedadi,JuanmaZam-
bots,HealthPrivacy,andChallengestoHIPAACom- brano Chaves, Szu-Yeu Hu, Mike Schaekermann,
pliance. JournalofAmericanMedicalAssociation AishwaryaKamath,YongCheng,DavidG.T.Bar-
(JAMA),330(4):309–310. rett, Cathy Cheung, Basil Mustafa, Anil Palepu,
Daniel McDuff, Le Hou, Tomer Golany, Luyang
Meta. 2024. Introducing Meta Llama 3: The Most Liu, Jean baptiste Alayrac, Neil Houlsby, Nenad
CapableOpenlyAvailableLLMtoDate. Tomasev, Jan Freyberg, Charles Lau, Jonas Kemp,
JeremyLai,ShekoofehAzizi,KimberlyKanada,Si-
SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe, WaiMan,KavitaKulkarni,RuoxiSun,SiamakShak-
MikeLewis,HannanehHajishirzi,andLukeZettle- eri,LuhengHe,BenCaine,AlbertWebson,Natasha
moyer.2022. RethinkingtheRoleofDemonstrations: Latysheva,MelvinJohnson,PhilipMansfield,Jian
WhatMakesIn-ContextLearningWork? InProceed- Lu, Ehud Rivlin, Jesper Anderson, Bradley Green,
ingsofthe2022ConferenceonEmpiricalMethods Renee Wong, Jonathan Krause, Jonathon Shlens,
inNaturalLanguageProcessing(EMNLP). Ewa Dominowska, S. M. Ali Eslami, Katherine
Chou,ClaireCui,OriolVinyals,KorayKavukcuoglu,
MichaelMoor,OishiBanerjee,ZahraShakeri,Harlan JamesManyika, JeffDean, DemisHassabis, Yossi
Krumholz, Jure Leskovec, Eric Topol, and Pranav Matias,DaleWebster,JoelleBarral,GregCorrado,
Rajpurkar.2023a. FoundationModelsforGeneral- Christopher Semturs, S. Sara Mahdavi, Juraj Got-
istMedicalArtificialIntelligence. Nature,616:259– tweis,AlanKarthikesalingam,andVivekNatarajan.
265. 2024. CapabilitiesofGeminiModelsinMedicine.
arXiv:2404.18416.
Michael Moor, Qian Huang, Shirley Wu, Michihiro
Yasunaga,CyrilZakka,YashDalmia,EduardoPontes MelanieSclar,YejinChoi,YuliaTsvetkov,andAlane
Reis,PranavRajpurkar,andJureLeskovec.2023b. Suhr.2024. QuantifyingLanguageModels’Sensitiv-
Med-Flamingo: A Multimodal Medical Few-shot itytoSpuriousFeaturesinPromptDesignor: HowI
Learner. arXiv:2307.15189. LearnedtoStartWorryingAboutPromptFormatting.
InInternationalConferenceonLearningRepresenta-
OpenAI. 2023a. GPT-4 Technical Report. tions(ICLR).
arXiv:2303.08774.
Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Mah-
OpenAI.2023b. GPT-4V(ision)SystemCard. davi, Jason Wei, Hyung Chung, Nathan Scales,AjayTanwani,HeatherCole-Lewis,StephenPfohl, TaoTu,ShekoofehAzizi,DannyDriess,MikeSchaek-
Perry Payne, Martin Seneviratne, Paul Gamble, ermann,MohamedAmin,Pi-ChuanChang,Andrew
Chris Kelly, Abubakr Babiker, Nathanael Schärli, Carroll,CharlesLau,RyutaroTanno,IraKtena,Anil
Aakanksha Chowdhery, Philip Mansfield, Dina Palepu,BasilMustafa,AakankshaChowdhery,Yun
Demner-Fushman, and Vivek Natarajan. 2023a. Liu, Simon Kornblith, David Fleet, Philip Mans-
LargeLanguageModelsEncodeClinicalKnowledge. field, Sushant Prakash, Renee Wong, Sunny Vir-
Nature,620:1–9. mani,ChristopherSemturs,S.SaraMahdavi,Bradley
Green, Ewa Dominowska, Blaise Aguera y Arcas,
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, JoelleBarral,DaleWebster,GregS.Corrado,Yossi
ElleryWulczyn,LeHou,KevinClark,StephenPfohl, Matias,KaranSinghal,PeteFlorence,AlanKarthike-
HeatherCole-Lewis,DarleneNeal,MikeSchaeker- salingam,andVivekNatarajan.2024. TowardsGen-
mann,AmyWang,MohamedAmin,SamiLachgar, eralist Biomedical AI. New England Journal of
PhilipMansfield, SushantPrakash, BradleyGreen, Medicine(NEJM)AI,1(3).
Ewa Dominowska, Blaise Aguera y Arcas, Nenad
Tomasev,YunLiu,ReneeWong,ChristopherSem- Özlem Uzuner, Brett R South, Shuying Shen, and
turs,S.SaraMahdavi,JoelleBarral,DaleWebster, ScottLDuVall.2011. 2010i2b2/VAChallengeon
Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Concepts,Assertions,andRelationsinClinicalText.
AlanKarthikesalingam,andVivekNatarajan.2023b. Journal of American Medical Informatics Associa-
TowardsExpert-LevelMedicalQuestionAnswering tion(JAMIA),18(5):552–556.
withLargeLanguageModels. arXiv:2305.09617.
ChaoyiWu,WeixiongLin,XiaomanZhang,YaZhang,
WeidiXie,andYanfengWang.2024. PMC-LLaMA:
RossTaylor,MarcinKardas,GuillemCucurull,Thomas
TowardBuildingOpen-SourceLanguageModelsfor
Scialom,AnthonyHartshorn,ElvisSaravia,Andrew
Medicine. JournaloftheAmericanMedicalInfor-
Poulton, Viktor Kerkez, and Robert Stojnic. 2022.
maticsAssociation(JAMIA),pageocae045.
Galactica: A Large Language Model for Science.
arXiv:2211.09085.
ChaojunXiao, XueyuHu, ZhiyuanLiu, CunchaoTu,
andMaosongSun.2021. Lawformer: APre-trained
AugustinToma,PatrickR.Lawler,JimmyBa,RahulG.
LanguageModelforChineseLegalLongDocuments.
Krishnan,BarryB.Rubin,andBoWang.2023. Clini-
AIOpen,2:79–84.
calCamel:AnOpenExpert-LevelMedicalLanguage
ModelwithDialogue-BasedKnowledgeEncoding.
Lin Yang, Shawn Xu, Andrew Sellergren, Timo
arXiv:2305.12031.
Kohlberger,YuchenZhou,IraKtena,AtillaKiraly,
Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroen-
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
sri,EricWang,ElleryWulczyn,FayazJamil,Theo
Martinet,Marie-AnneLachaux,TimothéeLacroix,
Guidroz,ChuckLau,SiyuanQiao,YunLiu,Akshay
BaptisteRozière,NamanGoyal,EricHambro,Faisal
Goel,KendallPark,ArnavAgharwal,NickGeorge,
Azhar,AurelienRodriguez,ArmandJoulin,Edouard
YangWang,RyutaroTanno,DavidG.T.Barrett,Wei-
Grave, and Guillaume Lample. 2023a. LLaMA:
HungWeng,S.SaraMahdavi,KhaledSaab,TaoTu,
Open and Efficient Foundation Language Models.
SreenivasaRajuKalidindi,MozziyarEtemadi,Jorge
arXiv:2302.13971.
Cuadros,GregorySorensen,YossiMatias,Katherine
Chou,GregCorrado,JoelleBarral,ShravyaShetty,
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
DavidFleet,S.M.AliEslami,DanielTse,Shruthi
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Prabhakara,CoryMcLean,DaveSteiner,RoryPil-
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
grim,ChristopherKelly,ShekoofehAzizi,andDaniel
Bhosale,DanBikel,LukasBlecher,CristianCanton
Golden.2024. AdvancingMultimodalMedicalCa-
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
pabilitiesofGemini. arXiv:2405.03162.
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
CynthiaGao,VedanujGoswami,NamanGoyal,An- XiYang,AokunChen,NimaPourNejatian,HooChang
thonyHartshorn,SagharHosseini,RuiHou,Hakan Shin, Kaleb E Smith, Christopher Parisien, Colin
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa, Compas, Cheryl Martin, AB Costa, Mona G Flo-
IsabelKloumann,ArtemKorenev,PunitSinghKoura, res,YingZhang,TanjaMagoc,ChristopherAHarle,
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di- GloriaLipori,DuaneAMitchell,WilliamRHogan,
anaLiskovich,YinghaiLu,YuningMao,XavierMar- Elizabeth A Shenkman, Jiang Bian, and Yonghui
tinet,TodorMihaylov,PushkarMishra,IgorMoly- Wu.2022. ALargeLanguageModelforElectronic
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- HealthRecords. npjDigitalMedicine,5(194).
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama- XiangYue,YuanshengNi,KaiZhang,TianyuZheng,
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Jiang,WeimingRen,YuxuanSun,CongWei,Botao
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan, Yu,RuibinYuan,RenliangSun,MingYin,Boyuan
Melanie Kambadur, Sharan Narang, Aurelien Ro- Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang,
driguez,RobertStojnic,SergeyEdunov,andThomas HuanSun,YuSu,andWenhuChen.2024. MMMU:
Scialom. 2023b. Llama 2: Open Foundation and AMassiveMulti-disciplineMultimodalUnderstand-
Fine-TunedChatModels. arXiv:2307.09288. ingandReasoningBenchmarkforExpertAGI. InConferenceonComputerVisionandPatternRecog-
nition(CVPR).
ShengZhang,YanboXu,NaotoUsuyama,HanwenXu,
Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh
Rao,MuWei,NaveenValluri,CliffWong,Andrea
Tupini,YuWang,MattMazzola,SwadheenShukla,
LarsLiden,JianfengGao,MatthewP.Lungren,Tris-
tanNaumann,ShengWang,andHoifungPoon.2023.
BiomedCLIP: A Multimodal Biomedical Founda-
tionModelPretrainedfromFifteenMillionScientific
Image-TextPairs. arXiv:2303.00915.
TonyZ.Zhao,EricWallace,ShiFeng,DanKlein,and
Sameer Singh. 2021. Calibrate Before Use: Im-
provingFew-ShotPerformanceofLanguageModels.
In International Conference on Machine Learning
(ICML).A AdditionalDetailsonDatasets (’e.g., “### Options:”), and H for the answer
a
header(e.g.,“### Answer:”) as
TableA1: Summaryofthenumberofexamplesinthe
train,validation,andtestsetsofalltextualandvisual H (f ,d ,s ) ::= f (d )s ⟨text⟩,
q case q 1 case q 1
QAdatasetsusedforevaluation,inthetopandbottom
H (f ,d ,s ) ::= f (d )s ,
c case c 1 case c 1
sections,respectively.
H (f ,d ,s ) ::= f (d )s ⟨text⟩,
a case a 1 case a 1
Dataset Train Validation Test
where f ∈ F denotes the casing function
MedQA(4&5Options) 10178 1272 1273 case case
MedMCQA 146257 36565 4183 (e.g.,x(cid:55)→“###”+x,x(cid:55)→x.upper()),d q ∈ D q
PubMedQA 211269 500 500 denotesthequestiondescriptor(e.g.,“Question”),
MMLU:Anatomy 5 14 135
d ∈ D denotestheanswerchoicedescriptor(e.g.,
c c
MMLU:ClinicalKnowledge 5 29 265
“Options”),d ∈ D denotestheanswerdescrip-
MMLU:CollegeBiology 5 16 144 a a
MMLU:CollegeMedicine 5 22 173 tor (e.g., “Answer”), s 1 ∈ S 1 denotes the header
MMLU:HighSchoolBiology 5 32 310 separator(e.g.,‘:’),and⟨text⟩denotesatextplace-
MMLU:MedicalGenetics 5 11 100
holder. For formatting the list of answer choices,
MMLU:Nutrition 5 33 306
we also define the basic fields C for formatting
MMLU:ProfessionalMedicine 5 31 272
MMLU:Virology 5 18 166 each answer choice (e.g., “(A) yes”) and L for
theconcatenationofallanswerchoicesasfollows:
VQA-RAD 820 205 272
PathVQA 9806 3135 3391
SLAKE 1943 422 415 C(f ,f ,i) ::= f (f (i))⟨text⟩,
wrap index wrap index
MMMU:BasicMedicalScience 5 5 25
L(f ,f ,n,s ) ::= C(f ,f ,0)s ...
MMMU:ClinicalMedicine 5 5 25 wrap index 2 wrap index 2
MMMU:Diag.&LabMedicine 5 5 25 s C(f ,f ,n−1),
2 wrap index
MMMU:Pharmacy 5 5 25
MMMU:PublicHealth 5 5 25
where f ∈ F denotes the wrapper func-
wrap wrap
tion for the answer choice letter (e.g., x (cid:55)→ “(”
Inthissection,weprovideadditionaldetailsonthe
+ x + “)”), f ∈ F denotes the number-
index index
textualandvisualQAdatasetsintroducedinSec-
ing function that converts an integer index into a
tion3. InTableA1,wesummarizethenumberof number format (e.g., 0 → “A”), i ∈ Z+ denotes
QAexamplesincludedinthetrain,validation,and
the index of a particular answer choice from the
testsetsofeachdataset,afterfollowingtheprepro-
list, s ∈ S denotes the answer choice separa-
2 2
cessingstepsdetailedinSection3. ForVQA-RAD
tor,ndenotesthenumberofanswerchoices,and
(Lauetal.,2018),PathVQA(Heetal.,2020),and
⟨text⟩denotesatextplaceholder. Thefullprompt
SLAKE(Liuetal.,2021),weonlyshowthenum-
formatP(f ,f ,f ,d ,d ,d ,s ,s ,n)is
case wrap index q c a 1 2
berofclosed-ended visualQAexamples,sinceour
thenconstructedbyconcatenatingalloftheheaders
evaluationsfocusonclosed-endedvisualQA.For
andtheanswerchoices,whileaddingspacet ∈ T
thedatasetsthatrequiredadditionalsplitsfromthe
(e.g.,“\n”)in-between:
official train-validation-test split (e.g., due to the
lackofapublictestset),weincludeallofthefixed
P ::= H tH tLtH , (1)
q c a
randomseedsinourrepositoryforreproducibility.
wherewehaveleftoutthenotationsfortheargu-
B AdditionalDetailsonModel-Specific
mentsfornotationalsimplicity.
PromptSelection
To define the prompt format search space, we
In this section, we provide additional details on instantiatethegrammarabovewiththedescriptors,
howwedefinethepromptformatsearchspacedis- separators,spaces,andfunctionsshownbelow.
cussedinSection3.1. Weconstructacontext-free
grammar of plausible prompt formats following Descriptors:
theapproachbySclaretal.(2024)(seeSection3.1
andAppendixAofthepaperforreference). Using
D = {“Question”,“”};
q
the Backus-Naur notation, we first define the ba-
D = {“Options”,“Choices”,“”};
sic fields H for the question header (e.g., “### c
q
D = {“Answer”,“The answer is”}.
Question:”), H for the answer choice header a
cSeparators: [C]Participantinterest;[D]Administrationof
thequestionnairebystaff
S = {“: ”, “ : ”, “ :: ”, “:\n”, “= ”,
1 ANSWER–[B]Establishmentofarepository
“ = ”, “ == ”, “=\n”, “ - ”, ofbiologicspecimens
“ – ”, “—”, “\n”, “\n\n”};
S = {“\n”, “\\”, “; ”, “ || ”, “ ” C AdditionalDetailsonZero-/Few-shot
2
“;\n”, “;\n\n”, “, ”}. Prompting
Spaces: Inthissection,wesummarizethepromptingdetails
madeavailableforthemedicalLLMsandVLMs
T = {“\n”, “\n\n”, “ || ”, “ ”}.
usedinourevaluation(AppendixC.1),andthede-
Casing,Wrapper,andNumberingFunctions: faultpromptformatsusedforeachLLM(Appendix
C.2)andVLM(AppendixC.3),whichhavebeen
F = {x (cid:55)→ x,x (cid:55)→ x.title(),
case reproducedbasedontheformer.
x (cid:55)→ x.upper(),x (cid:55)→ x.lower()
x (cid:55)→ “### ” + x C.1 ReproducibilityofPromptingDetails
x (cid:55)→ “**” + x + “**”};
In Table C1, we provide a summary of all of
F = {x (cid:55)→ “(” + x + “)”,x (cid:55)→ x + “.”
wrap the prompting details available (in the context of
x (cid:55)→ x + “)”,x (cid:55)→ “[” + x + “]” closed-endedmedicalQA)forallmedicalLLMs
x (cid:55)→ x + “ )”,x (cid:55)→ “<” + x + “>”}; andVLMsusedinourevaluation. Wesharethese
detailstodemonstrateourbesteffortswithrepro-
F = {x (cid:55)→ chr(ord(“A”) + x)}.
index
ducing the original prompting setups considered
To randomly sample a prompt format accepted for performing our evaluations. In particular, we
by the grammar, we randomly sample each of focus on whether the following four components
these components and construct the full prompt areexplicitlymadeavailable,eitherintheoriginal
format, following Equation (1). Below, we show publicationsorthe publiclyreleasedcodereposi-
an example QA pair from the MedQA dataset tory: (i)systemprompt;(ii)zero-/few-shotprompt
(fouranswerchoices),formattedaccordingtothe format(usedforclosed-endedQAtasks);(iii)the
formats sampled from the prompt format space choice of few-shot examples; and (iv) details on
definedbytheabovecontext-freegrammar. how the text generations are sampled (e.g., soft-
maxtemperature,top-p,beamsize,randomseeds
Example1: used for sampling). Below, we provide detailed
clarificationsforeachmodel.
A key factor facilitating the application of
nested case-control studies from the MACS
was:
OPENBIOLLM (PalandSankarasubbu,2024).
OPTIONS–A)Datacollection
FortheOPENBIOLLM models,wefollowthein-
structions provided in the model cards posted by
B)Establishmentofarepositoryofbiologic
theauthorsonHuggingFace,forthe70B-parameter
specimens
and 8B-parameter models. We use the recom-
C)Participantinterest
D ) Administration of the questionnaire by
mended system prompt and the LLAMA-3-based
conversationalpromptformat. Meanwhile,inTa-
staff
ble C1, we treat the prompt format as partially
THE ANSWER IS – B ) Establishment of a
missing, as the exact format that was used to for-
repositoryofbiologicspecimens
mateachquestion(“user”query)andanswer(“as-
Example2:
sistant”response)forevaluationonclosed-ended
QUESTION – A key factor facilitating the multiple-choicequestionsisnotprovided. Atthe
applicationofnestedcase-controlstudiesfrom timeofwriting,therearenoadditionaldetailsabout
theMACSwas: the models that have been publicly released, be-
CHOICES – [A] Data collection; [B] Estab- yond what is provided in the model cards. We
lishmentofarepositoryofbiologicspecimens; includethedefaultpromptformatusedfor OPEN-
BIOLLM inAppendixC.2.1.Table C1: Summary of all of the prompting details made available for each medical LLM and VLM used for
evaluation. Foreachcolumn,acheckmark(✓)indicatesthattheinformationwasfullyprovided,atriangle(▲)
indicatesthattheinformationwaspartiallyprovided(e.g.,randomsamplingwithoutinformationabouttheseeds),
and a cross (✗) indicates that the information was not provided at all. “N/A” indicates that the corresponding
information is not available due to its irrelevance to the evaluation setup considered in the paper (e.g., lack of
few-shotexampledetailsbecausethemodelwasonlyoriginallyevaluatedinzero-shotorfine-tuningregimes).
System Zero-/Few-Shot Few-Shot Sampling
Model Prompt PromptFormat Examples Details
OPENBIOLLM(PalandSankarasubbu,2024) ✓ ▲ ✗ ✗
CLINICAL-CAMEL(Tomaetal.,2023) ✗ ▲ ✗ ▲
BIOMISTRAL(Labraketal.,2024) ✓ ✓ ✗ ▲
MEDITRON(Chenetal.,2023) ✓ ▲ ✓ ✓
BIOMEDGPT-LM(Luoetal.,2023) ✗ ✗ N/A ✗
LLAVA-MED(Lietal.,2023) ▲ ▲ N/A ▲
MED-FLAMINGO(Mooretal.,2023b) ✓ ▲ ✗ ✗
CLINICAL-CAMEL (Toma et al., 2023). For tion3). Weincludethedefaultpromptformatused
CLINICAL-CAMEL, we use the conversational for BIOMISTRALinAppendixC.2.3.
promptformatusedintheofficialGitHubreposi-
tory,whichcorrespondstotheofficialchatformat MEDITRON (Chenetal.,2023). Forthe MED-
for LLAMA-2 (Touvronetal.,2023b). Asthesys- ITRON models, we use the system prompts—
tem prompts and few-shot examples used for the tailoredspecificallytoMedQA,MedMCQA,Pub-
main evaluations in the paper are not provided, MedQA, and the MMLU datasets—provided in
weuseourownmanuallydesigneddefaultsystem Table2ofthepaper. Forthepromptformats, we
prompt and search over different choices of few- usetheonesprovidedintheofficialGitHubrepos-
shotexamples. Forsampling,theevaluationcode itory, as the prompt formats (those with special
usesdefaulttemperaturesettingof0.7(albeitwith- ‘<|im_start|>’ and ‘<|im_end|>’ tokens, fol-
outtherandomseeds),whichdiffersfromoureval- lowing the ChatML format) shown in the paper
uationsetup. Weincludethedefaultpromptformat are only applicable to the fine-tuned models (see
usedfor CLINICAL-CAMELinAppendixC.2.2. thisdiscussionfromtheofficialGitHubrepository).
In particular, we refer to the prompt formats pro-
BIOMISTRAL (Labrak et al., 2024). For vided in the dataset preprocessing code and used
BIOMISTRAL,weusethesystempromptandzero- forevaluationtodeterminethedefaultpromptfor-
/few-shotpromptformatprovidedinAppendixF mat for both the 70B- and 7B-parameter models.
ofthepaper. Atthetimeofwriting,thecodereposi- However,wewereunabletoreliablyreproducethe
toryisnotpubliclyavailable,andthepaperdoesnot zero-/few-shotpromptingperformanceusingthis
provide details on what few-shot examples were promptformat,andthereforeperformagridsearch
used for evaluation. In Section 4.3 of the paper, overthepromptformatsaswellformodel-specific
Labrak et al. (2024) mention that the output vo- promptselection. Intheevaluationcode,Chenetal.
cabulary is constrained to be one of the answer (2023)providetherandomseedsusedforsampling
choicesinletteredformat(e.g.,oneof[A,B,C,D]) the few-shot examples; however, we also search
to force the model to avoid generating irrelevant over the set of few-shot examples to consider a
tokens in its output. Meanwhile, it is not explic- larger number of few-shot example choices. For
itly clear whether (i) the filtered token with the sampling, we use the same greedy decoding ap-
highestprobabilitywastreatedasthemodel’spre- proachasconsideredinthepaper(referredto“Top
dictionor(ii)atokenwasrandomlysampledbased TokenSelection”inSection4.3ofthepaper). We
on the renormalized token probabilities. We also includethedefaultpromptformatusedfor MED-
notethatthevocabularyfilteringproceduremakes ITRONinAppendixC.2.4.
their evaluation setup different from ours, as we
usegreedydecodingtosamplethemodeloutputs BIOMEDGPT-LM (Luo et al., 2023). While
withoutanyconstraintsonthevocabulary(seeSec- BIOMEDGPT-LMwasevaluatedontextualmed-ical QA datasets such as MedMCQA and Pub- seeds),whichdiffersfromourevaluationsetup. We
MedQA,theevaluationwasperformedonlyinthe includethedefaultpromptformatusedforLLAVA-
supervisedfine-tuningregime,andthepromptfor- MED inAppendixC.3.1andthatforLLAVA-V0
mats used for these datasets are not available, to inAppendixC.3.2.
thebestofourknowledge. Meanwhile,theofficial
MED-FLAMINGO (Moor et al., 2023b). For
GitHub repository provides Jupyter notebook ex-
MED-FLAMINGO,weusethesystempromptand
amplescontainingaconversationalpromptformat
prompt format provided in the demo code from
used in the context of other QA tasks. We there-
the official GitHub repository by default. How-
foreusethisformatbydefaultbutsearchoverthe
ever,wesearchoverthepromptformatswhenper-
prompt formats for model-specific prompt selec-
forming model-specific prompt selection, as the
tion,sinceitisnotspecificallydesignedforclosed-
examplepromptinthedemodoesnotshowdetails
endedmultiple-choiceQAtasks. Moreover,asthe
for formatting answer choices in a closed-ended
systempromptprovidedisnotsemanticallyappli-
QAcontext. Thechoiceoffew-shotexamplesand
cabletotheQAtasksthatweconsider(e.g.,“You
the sampling details used for the original evalua-
are working as an excellent assistant in
tions on VQA-RAD and PathVQA are not avail-
chemistry and molecule discovery.”,weuse
able. We include the default prompt format used
ourownmanuallydesigneddefaultsystemprompt.
for MED-FLAMINGO inAppendixC.3.3andthat
We include the default prompt format used for
for OPEN-FLAMINGOinAppendixC.3.4.
BIOMEDGPT-LM inAppendixC.2.5.
C.2 DefaultLLMPromptFormats
LLAVA-MED (Li et al., 2023). For LLAVA-
Inthissection,wesharethedefaultpromptformats
MED,weusethesystempromptandconversational
thatweuseforeachLLM,usingMMLU(Clinical
promptformatincludedinthe“simple_conv_med”
Knowledge)(Hendrycksetal.,2021)asarunning
template from the official GitHub repository (for
example. Wedenotethesystempromptinred,any
LLAVA-V0 (Liu et al., 2023), we use the “sim-
few-shotexamplesingreen,andthequestionbeing
ple_conv” template) by default. For formatting
askedofthemodelinpurple.
thevisualquestions,wealsorefertothisfilecon-
For models that do not have a specific system
taining the raw visual QA results on VQA-RAD
prompt and prompt format designed for closed-
(“Please choose from the following two
ended medical QA (see Section C.1), we use a
options: [yes, no]”). Meanwhile, we make
manuallydesignedpromptformatbydefault. This
these choices with the following caveats, to the
includesallofthegeneral-domainLLMs. Forex-
best of our knowledge. First, the exact choice of
ample,inthe1-shotsetting,thedefaultpromptfor
systempromptandconversationalpromptformat
non-instruction-tunedmodelsisasfollows:
used for evaluation are not discussed in the pa-
The following is a multiple-choice question
perorthecoderepository,andwechoosetheone
aboutmedicalknowledge. Answertheques-
thathasasystempromptspecificto LLAVA-MED
(“You are LLaVA-Med, a large language and tionbychoosingoneoftheoptionsfromAto
vision assistant trained by a group of D.
researchers at Microsoft ...”) andfollows ###Question: Glycolysisisthenamegivento
thepathwayinvolvingtheconversionof:
the conversational format used for VICUNA-V0
(A)glycogentoglucose-1-phosphate.
(Chiangetal.,2023),whichformsitsLLMback-
(B)glycogenorglucosetofructose.
bone. Second,detailsonhowtheanswerchoices
(C)glycogenorglucosetopyruvateorlactate.
shouldbeformattedinthecontextofclosed-ended
(D)glycogenorglucosetopyruvateoracetyl
QAtasksisonlyshownintheVQA-RADresults
CoA.
file. Giventheuncertaintyinsuchdetails,wealso
###Answer: (C)glycogenorglucosetopyru-
searchoverthepromptformatsformodel-specific
vateorlactate.
promptselection. WenotethatLLAVA-MEDwas
### Question: What size of cannula would
notpretrainedonmulti-imageinputsorevaluated
youuseinapatientwhoneededarapidblood
in few-shot setting, and therefore details on the
transfusion(asof2020medicalknowledge)?
choice of few-shot examples are irrelevant. For
(A)18gauge.
sampling, theevaluationcodeusesadefaulttem-
peraturesettingof0.7(albeitwithouttherandom(B)20gauge. were developed by Saama AI Labs. who’s
(C)22gauge. willing to help answer the user’s query with
(D)24gauge. explanation. In your explanation, leverage
###Answer: yourdeepmedicalexpertisesuchasrelevant
anatomicalstructures,physiologicalprocesses,
Forinstruction-tunedmodels,whichtypicallyex-
diagnostic criteria, treatment guidelines, or
pectaspecificconversationalformat,weapplythe
other pertinent medical concepts. Use pre-
aboveformattoeach“user”queryand“assistant”
cisemedicalterminologywhilestillaimingto
response and remove the ‘###’ and ‘Answer:’
maketheexplanationclearandaccessibletoa
tags. Forexample,theinputpromptto LLAMA-3-
generalaudience.
70B-INSTRUCTisasfollows:
[User]Question: Glycolysisisthenamegiven
<|begin_of_text|>
tothepathwayinvolvingtheconversionof:
<|start_header_id|>system<|end_header_id|>
(A)glycogentoglucose-1-phosphate.
The following is a multiple-choice question
(B)glycogenorglucosetofructose.
about medical knowledge. Answer the
(C)glycogenorglucosetopyruvateorlactate.
questionbychoosingoneoftheoptionsfrom
(D)glycogenorglucosetopyruvateoracetyl
AtoD.<|eot_id|>
CoA.
<|start_header_id|>user<|end_header_id|>
[Model](C)glycogenorglucosetopyruvate
Question: Glycolysisisthenamegiventothe
orlactate.
pathwayinvolvingtheconversionof:
[User]Question: Whatsizeofcannulawould
(A)glycogentoglucose-1-phosphate.
youuseinapatientwhoneededarapidblood
(B)glycogenorglucosetofructose.
transfusion(asof2020medicalknowledge)?
(C)glycogenorglucosetopyruvateorlactate.
(A)18gauge.
(D)glycogenorglucosetopyruvateoracetyl
(B)20gauge.
CoA.<|eot_id|>
(C)22gauge.
<|start_header_id|>assistant<|end_header_id|>
(D)24gauge.
(C) glycogen or glucose to pyruvate or lac-
tate.<|eot_id|> C.2.2 CLINICAL-CAMEL(Tomaetal.,2023)
<|start_header_id|>user<|end_header_id|>
<s>[INST]«SYS»
Question: What size of cannula would you
The following is a multiple-choice question
use in a patient who needed a rapid blood
about medical knowledge. Answer the
transfusion(asof2020medicalknowledge)?
questionbychoosingoneoftheoptionsfrom
(A)18gauge.
AtoD.
(B)20gauge.
«/SYS»
(C)22gauge.
(D)24gauge.<|eot_id|>
Question: Glycolysis is the name given
<|start_header_id|>assistant<|end_header_id|>
tothepathwayinvolvingtheconversionof:
In the following subsections, we show the sys- (A)glycogentoglucose-1-phosphate.
tem prompt and prompt formats used in the 1- (B)glycogenorglucosetofructose.
shot setting for models that have a dedicated for- (C)glycogenorglucosetopyruvateorlactate.
mat. Weexcludethemodel-specificspecialtokens (D)glycogenorglucosetopyruvateoracetyl
(e.g.,‘[INST]’)foreaseofpresentation,andadd CoA. [/INST] (C) glycogen or glucose to
‘[User]’and‘[Model]’todemarcateeachques- pyruvateorlactate.</s><s>[INST]Question:
tionandanswerfortheinstruction-tunedmodels. What size of cannula would you use in a
patientwhoneededarapidbloodtransfusion
C.2.1 OPENBIOLLM (Paland
(asof2020medicalknowledge)?
Sankarasubbu,2024)
(A)18gauge.
You are an expert and experienced from the
(B)20gauge.
healthcareandbiomedicaldomainwithexten-
(C)22gauge.
sivemedicalknowledgeandpracticalexperi-
(D)24gauge. [/INST]
ence. Your name is OpenBioLLM, and you
C.2.3 BIOMISTRAL(Labraketal.,2024)The following are multiple choice questions The following is a multiple-choice question
(withanswers)aboutmedicalknowledge. aboutmedicalknowledge. Answertheques-
**Question:** Glycolysis is the name given tionbychoosingoneoftheoptionsfromAto
tothepathwayinvolvingtheconversionof: D.
(A)glycogentoglucose-1-phosphate. ###Human: Glycolysisisthenamegivento
(B)glycogenorglucosetofructose. thepathwayinvolvingtheconversionof:
(C)glycogenorglucosetopyruvateorlactate. (A)glycogentoglucose-1-phosphate.
(D)glycogenorglucosetopyruvateoracetyl (B)glycogenorglucosetofructose.
CoA. (C)glycogenorglucosetopyruvateorlactate.
**Answer:**(C (D)glycogenorglucosetopyruvateoracetyl
**Question:** What size of cannula would CoA.
youuseinapatientwhoneededarapidblood ### Assistant: (C) glycogen or glucose to
transfusion(asof2020medicalknowledge)? pyruvateorlactate.
(A)18gauge. ###Human: Whatsizeofcannulawouldyou
(B)20gauge. useinapatientwhoneededarapidbloodtrans-
(C)22gauge. fusion(asof2020medicalknowledge)?
(D)24gauge. (A)18gauge.
**Answer:**( (B)20gauge.
(C)22gauge.
C.2.4 MEDITRON(Chenetal.,2023)
(D)24gauge.
Youareamedicaldoctoransweringreal-world ###Assistant:
medical entrance exam questions. Based
on your understanding of basic and clini- C.3 DefaultVLMPromptFormats
cal science, medical knowledge, and mecha-
Inthissection,wesharethedefaultpromptformats
nismsunderlyinghealth,disease,patientcare,
thatweuseforeachgeneral-domain/medicalVLM,
and modes of therapy, answer the following
using VQA-RAD (Lau et al., 2018) as a running
multiple-choicequestion. Selectonecorrect
example. Wedenotethesystempromptinred,any
answerfromAtoD.Baseyouransweronthe
few-shotexamplesingreen,andthequestionbeing
current and standard practices referenced in
askedofthemodelinpurple. Bydefault,weshow
medicalguidelines.
theformatusedinthe1-shotsetting.
Question: Glycolysisisthenamegiventothe
pathwayinvolvingtheconversionof:
C.3.1 LLAVA-MED(Lietal.,2023)
Options:
YouareLLaVA-Med,alargelanguageandvi-
A.glycogentoglucose-1-phosphate.
sionassistanttrainedbyagroupofresearchers
B.glycogenorglucosetofructose.
at Microsoft, based on the general domain
C.glycogenorglucosetopyruvateorlactate.
LLaVA architecture. You are able to under-
D.glycogenorglucosetopyruvateoracetyl
standthevisualcontentthattheuserprovides,
CoA.
and assist the user with a variety of medical
Theansweris: C
andclinicaltasksusingnaturallanguage.
Question: Whatsizeofcannulawouldyouuse
Followtheinstructionscarefullyandexplain
inapatientwhoneededarapidbloodtransfu-
youranswersindetail.
sion(asof2020medicalknowledge)?
###Human: Doesthispatienthavemultiple
Options:
lesionsintheirchest? Pleasechoosefromthe
A.18gauge.
followingoptions: [yes,no]. <image>
B.20gauge.
###Assistant: no
C.22gauge.
### Human: Is there evidence of an aortic
D.24gauge.
aneurysm? Pleasechoosefromthefollowing
Theansweris:
options: [yes,no]. <image>
C.2.5 BIOMEDGPT-LM (Luoetal.,2023) ###Assistant:
C.3.2 LLAVA-V0 (Liuetal.,2023)TableD1: Thewin,tie,andlossrates(%)ofallmedical
Achatbetweenacurioushumanandanartifi-
LLMs(top)andVLMs(bottom)inthezero-shotsetting,
cialintelligenceassistant. Theassistantgives
after independently optimizing the prompts for both
helpful, detailed, and polite answers to the
medicalandgeneral-domainmodels. Modelpredictions
human’squestions.
aregeneratedviagreedydecoding.
###Human: Doesthispatienthavemultiple
lesionsintheirchest? Pleasechoosefromthe Model Win Tie Loss
followingoptions: [yes,no]. <image> OPENBIOLLM-70B(PalandSankarasubbu,2024) 7.7 69.2 23.1
###Assistant: no
MEDITRON-70B(Chenetal.,2023) 0 61.5 38.5
CLINICAL-CAMEL-70B(Tomaetal.,2023) 27.3 63.6 9.1
### Human: Is there evidence of an aortic OPENBIOLLM-8B(PalandSankarasubbu,2024) 0 46.2 53.8
aneurysm? Pleasechoosefromthefollowing MEDITRON-7B(Chenetal.,2023) 0 69.2 30.8
BIOMISTRAL-7B(Labraketal.,2024) 30.8 69.2 0
options: [yes,no]. <image> BIOMEDGPT-LM-7B(Luoetal.,2023) 0 15.4 84.6
###Assistant: LLAVA-MED-7B(Lietal.,2023) 12.5 62.5 25.0
MED-FLAMINGO-9B(Mooretal.,2023b) 0 87.5 12.5
C.3.3 MED-FLAMINGO(Mooretal.,2023b)
TableD2: Thewin,tie,andlossrates(%)ofallmedical
You are a helpful medical assistant. You are
LLMs(top)andVLMs(bottom)inthe3-shotsetting,
beingprovidedwithimages,aquestionabout
after independently optimizing the prompts for both
theimageandananswer. Followtheexamples
medicalandgeneral-domainmodels. Modelpredictions
andanswerthelastquestion. aregeneratedviagreedydecoding.
<image> Does this patient have multiple le-
sionsintheirchest? Model Win Tie Loss
(A)yes OPENBIOLLM-70B(PalandSankarasubbu,2024) 30.8 69.2 0
MEDITRON-70B(Chenetal.,2023) 0 69.2 30.8
(B)no
CLINICAL-CAMEL-70B(Tomaetal.,2023) 0 63.6 36.4
Answer: (B)no<|endofchunk|> OPENBIOLLM-8B(PalandSankarasubbu,2024) 7.7 61.5 30.8
<image> Is there evidence of an aortic
MEDITRON-7B(Chenetal.,2023) 0 23.1 76.9
BIOMISTRAL-7B(Labraketal.,2024) 46.2 53.8 0
aneurysm? BIOMEDGPT-LM-7B(Luoetal.,2023) 0 7.7 92.3
(A)yes LLAVA-MED-7B(Lietal.,2023) 12.5 62.5 25.0
(B)no
MED-FLAMINGO-9B(Mooretal.,2023b) 0 100.0 0
Answer:
D.1 Finding1(Section4)
C.3.4 OPEN-FLAMINGO(Awadallaetal.,
2023) Figure D1 shows the absolute and relative exact-
match accuracies achieved by the medical and
Thefollowingisamultiple-choicevisualques-
general-domainLLMsinthezero-shotprompting
tionrequiringmedicalknowledge. Answerthe
regime,afterindependentlyoptimizingtheprompt
questionbychoosingoneoftheprovidedan-
for each model. In Tables D1–D2, we also show
sweroptions.
thezero-shotand3-shotwin/tie/lossratesachieved
<image> Does this patient have multiple le-
sionsintheirchest?
bythemedicalLLMsandVLMs. ForCLINICAL-
(A)yes
CAMEL-70B, we compute the win/tie/loss rates
whileexcludingtheMedQAdatasets,asdiscussed
(B)no
in Section 4. For each medical model, we bold-
Answer: (B)no<|endofchunk|> <image>Is
facethewinrateifitwinsmorethanitlosestoits
thereevidenceofanaorticaneurysm?
general-domainbasemodel,andviceversa.
(A)yes
As discussed in Finding 1 of Section 4, we
(B)no
findthatinboththezero-shotand3-shotsettings,
Answer:
only2outof7medicalmodelsshowstatistically
D AdditionalResultsforthe significant improvements over their correspond-
Zero-/Few-ShotPromptingEvaluations
ing base models (CLINICAL-CAMEL-70B and
withGreedyDecoding
BIOMISTRAL-7Bforzero-shot;OPENBIOLLM-
70B and BIOMISTRAL-7Bfor3-shot),albeitbya
Inthissection,weprovideadditionalresultsforthe limitedmarginintermsofabsoluteaccuracy. For
mainzero-/few-shotpromptingexperimentswith allothermodels,thewinratesarelessthanorequal
greedydecoding,whicharediscussedinSection4. to the loss rates, and the majority of cases resultOpenBioLLM-70B MediTron-70B OpenBioLLM-8B MediTron-7B BioMistral-7B BioMedGPT-LM-7B
Llama-3-70B-Instruct Clinical-Camel-70B Llama-3-8B Llama-2-7B Mistral-7B-Instruct-v0.1 Llama-2-7B-Chat
Llama-2-70B
1.0
0.8
0.6
0.4
0.2
0.0
No Improvement
0.4
0.2
0.0
0.2
0.4
MedQA MedQA MedMCQA PubMedQA MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU:
(4 Options) (5 Options) Anatomy Clinical College College Medical Professional High Virology Nutrition
Knowledge Biology Medicine Genetics Medicine School
Biology
FigureD1:MedicalLLMsdonotshowastatisticallysignificantimprovementovertheirgeneral-domaincounterparts
inthezero-shotsetting,afterindependentlyselectingthebestpromptformatandexamplesforeachmodel. Toprow
showstheabsoluteexact-matchaccuraciesonthetestset,andbottomrowshowstherelativeexact-matchaccuracies
alongwith95%confidenceintervalsderivedviabootstrappingonthetestset(seeSection3). Weshowtheresults
forwhenmodelpredictionsaregeneratedviagreedydecoding.
TableD3: Thewin,tie,andlossrates(%)ofallmedical TableD4: Thewin,tie,andlossrates(%)ofallmedical
LLMs(top)andVLMs(bottom)inthezero-shotsetting, LLMs(top)andVLMs(bottom)inthe3-shotsetting,
whenusingasingle, fixedpromptoptimizedonlyfor whenusingasingle, fixedpromptoptimizedonlyfor
themedicalmodel. Modelpredictionsaregeneratedvia themedicalmodel. Modelpredictionsaregeneratedvia
greedydecoding. greedydecoding.
Model Win Tie Loss Model Win Tie Loss
OPENBIOLLM-70B(PalandSankarasubbu,2024) 23.1 61.5 15.4 OPENBIOLLM-70B(PalandSankarasubbu,2024) 38.5 61.5 0
MEDITRON-70B(Chenetal.,2023) 23.1 69.2 7.7 MEDITRON-70B(Chenetal.,2023) 0 100.0 0
OPENBIOLLM-8B(PalandSankarasubbu,2024) 69.2 30.8 0 CLINICAL-CAMEL-70B(Tomaetal.,2023) 54.5 45.5 0
MEDITRON-7B(Chenetal.,2023) 76.9 23.1 0 OPENBIOLLM-8B(PalandSankarasubbu,2024) 30.8 69.2 0
BIOMISTRAL-7B(Labraketal.,2024) 30.8 69.2 0 MEDITRON-7B(Chenetal.,2023) 38.5 23.1 38.5
BIOMEDGPT-LM-7B(Luoetal.,2023) 0 38.5 61.5 BIOMISTRAL-7B(Labraketal.,2024) 0 100.0 0
BIOMEDGPT-LM-7B(Luoetal.,2023) 7.7 46.2 46.2
LLAVA-MED-7B(Lietal.,2023) 25.0 62.5 12.5
MED-FLAMINGO-9B(Mooretal.,2023b) 25.0 62.5 12.5 LLAVA-MED-7B(Lietal.,2023) 50.0 37.5 12.5
MED-FLAMINGO-9B(Mooretal.,2023b) 25.0 75.0 0
in a tie (i.e., the confidence interval crosses zero
relativeaccuracy). general-domainbasemodel,andviceversa.
Comparedtowhenthepromptisindependently
D.2 Finding2(Section4)
optimized for each model, we see that a greater
Figures D2–D3 show how the absolute and rela- number of medical models show statistically sig-
tiveexact-matchaccuracieschangeforLLMsand nificantimprovements. Notably,allmedicalVLMs
VLMs in the zero-shot and 3-shot settings, when outperform their general-domain counterparts in
weuseasingle,fixedpromptthatisonlyoptimized both zero-shot and few-shot accuracy under this
forthemedicalmodel. InTablesD3–D4,wealso setup, and all but one medical LLMs outperform
showthezero-shotand3-shotwin/tie/lossratesin theirgeneral-domaincounterpartsinthezero-shot
this scenario. For each medical model, we bold- setting. These results suggest that using a single,
facethewinrateifitwinsmorethanitlosestoits fixedpromptthatisonlytailoredtoonemodelcan
ycaruccA
ycaruccA
evitaleROpenBioLLM-70B MediTron-70B OpenBioLLM-8B MediTron-7B BioMistral-7B BioMedGPT-LM-7B
Llama-3-70B-Instruct Clinical-Camel-70B Llama-3-8B Llama-2-7B Mistral-7B-Instruct-v0.1 Llama-2-7B-Chat
Llama-2-70B
1.0
0.8
0.6
0.4
0.2
(a) 0.0
1.00
0.75
0.50
0.25
0.00
0.25
0.50
No Improvement
0.75
1.00
MedQA MedQA MedMCQA PubMedQA MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU:
(4 Options) (5 Options) Anatomy Clinical College College Medical Professional High Virology Nutrition
Knowledge Biology Medicine Genetics Medicine School
Biology
1.0
0.8
0.6
0.4
0.2
(b) 0.0
No Improvement
0.4
0.2
0.0
0.2
0.4
MedQA MedQA MedMCQA PubMedQA MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU:
(4 Options) (5 Options) Anatomy Clinical College College Medical Professional High Virology Nutrition
Knowledge Biology Medicine Genetics Medicine School
Biology
Figure D2: Using a single, fixed prompt format only optimized for the medical model can overestimate the
performanceimprovementsfrommedicalDAPT,inboth(a)zero-shotand(b)3-shotsettings. Foreachsetting,
toprowshowstheabsoluteexact-matchaccuraciesonthetestset,andbottomrowshowstherelativeexact-match
accuracies along with 95% confidence intervals derived via bootstrapping on the test set (see Section 3). For
LLAMA-2-70B,whichhasmultiplecorrespondingmedicalLLMs(MEDITRON-70B and CLINICAL-CAMEL-
70B),weincludeamin-maxerrorbarintheabsoluteaccuracyplotstoshowhowtheabsoluteaccuracychanges
withrespecttoeachprompt. Weshowtheresultsforwhenmodelpredictionsaregeneratedviagreedydecoding.
resultinanunfaircomparisonandcanpotentially E ResultsfortheZero-/Few-Shot
leadtoanoverestimationoftheperformancebene- PromptingEvaluationswith
fitsofmedicalDAPT. ConstrainedDecoding
Inthissection,weprovidealloftheresultsforthe
zero-/few-shotpromptingevaluationsdescribedin
Section3,wherethemodelpredictionsaregener-
atedviaconstrained insteadofgreedydecoding.
ycaruccA
ycaruccA
evitaleR
ycaruccA
ycaruccA
evitaleRLLaVA-Med-7B LLaVA-v0-7B Med-Flamingo-9B Open-Flamingo-9B
(a) (b)
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.8 0.8
0.6 No Improvement 0.6 No Improvement
0.4 0.4
0.2 0.2
0.0 0.0
0.2 0.2
0.4 0.4
0.6 0.6
0.8 0.8
VQA-RAD PathVQA SLAKE MMMU: MMMU: MMMU: MMMU: MMMU: VQA-RAD PathVQA SLAKE MMMU: MMMU: MMMU: MMMU: MMMU:
Basic Clinical DiagnosticsPharmacy Public Basic Clinical DiagnosticsPharmacy Public
Medical Medicine & Health Medical Medicine & Health
Science Lab Medicine Science Lab Medicine
Figure D3: Using a single, fixed prompt format only optimized for the medical model can overestimate the
performanceimprovementsfrommedicalDAPT,inboth(a)zero-shotand(b)3-shotsettings. Foreachsetting,
toprowshowstherawexact-matchaccuraciesonthetestset,andthebottomrowshowstherelativeexact-match
accuraciesalongwith95%confidenceintervalsderivedviaboostrappingonthetestset(seeSection3). Weshow
theresultsforwhenmodelpredictionsaregeneratedviagreedydecoding.
E.1 Finding1(Section4) TableE1: Thewin,tie,andlossrates(%)ofallmedical
LLMs(top)andVLMs(bottom)inthezero-shotsetting,
after independently optimizing the prompts for both
medicalandgeneral-domainmodels. Modelpredictions
Here, we show the constrained decoding results
aregeneratedviaconstraineddecoding.
for the medical LLMs and VLMs after indepen-
dently optimizing the prompt for each model. In Model Win Tie Loss
FiguresE1–E2,weshowtheabsoluteandrelative OPENBIOLLM-70B(PalandSankarasubbu,2024) 7.7 76.9 15.4
exact-matchaccuraciesachievedbyallLLMsand MEDITRON-70B(Chenetal.,2023) 30.8 46.2 23.1
CLINICAL-CAMEL-70B(Tomaetal.,2023) 18.2 72.7 9.1
VLMsinthe(a)zero-shotand(b)3-shotprompt- OPENBIOLLM-8B(PalandSankarasubbu,2024) 0 53.8 46.2
ingregimes. InTablesE1–E2,weshowthezero- MEDITRON-7B(Chenetal.,2023) 23.1 76.9 0
BIOMISTRAL-7B(Labraketal.,2024) 30.8 69.2 0
shotand3-shotwin/tie/lossratesachievedbyeach
BIOMEDGPT-LM-7B(Luoetal.,2023) 7.7 84.6 7.7
model. ForCLINICAL-CAMEL-70B,wecompute
LLAVA-MED-7B(Lietal.,2023) 0 100.0 0
thewin/tie/lossrateswhileexcludingtheMedQA MED-FLAMINGO-9B(Mooretal.,2023b) 12.5 75.0 12.5
datasets,asdiscussedinSection4. Foreachmedi-
TableE2: Thewin,tie,andlossrates(%)ofallmedical
calmodel,weboldfacethewinrateifitwinsmore
LLMs(top)andVLMs(bottom)inthe3-shotsetting,
thanitlosestoitsgeneral-domainbasemodel,and
after independently optimizing the prompts for both
viceversa.
medicalandgeneral-domainmodels. Modelpredictions
aregeneratedviaconstraineddecoding.
Figure E1(a) and Table E1 show that 4 out of
7 medical LLMs show improvements over their
Model Win Tie Loss
general-domain counterparts in the zero-shot set-
OPENBIOLLM-70B(PalandSankarasubbu,2024) 23.1 53.8 23.1
ting,albeitbyalimitedmargininabsoluteterms. In MEDITRON-70B(Chenetal.,2023) 15.4 69.2 15.4
CLINICAL-CAMEL-70B(Tomaetal.,2023) 9.1 72.7 18.2
the3-shotsetting,FigureE1(b)andTableE2show
OPENBIOLLM-8B(PalandSankarasubbu,2024) 7.7 69.2 23.1
thatonly2outof7medicalLLMs—MEDITRON- MEDITRON-7B(Chenetal.,2023) 7.7 92.3 0
BIOMISTRAL-7B(Labraketal.,2024) 7.7 92.3 0
7B and BIOMISTRAL-7B—showimprovements
BIOMEDGPT-LM-7B(Luoetal.,2023) 7.7 69.2 23.1
overtheirgeneral-domaincounterpart,butwitha
LLAVA-MED-7B(Lietal.,2023) 0 87.5 12.5
tieon92.3%ofalldatasets. Forallothermodels, MED-FLAMINGO-9B(Mooretal.,2023b) 0 100.0 0
thewinratesarelessthanorequaltothelossrates,
andthemajorityofcasesresultinatie. Meanwhile,
E.2 Finding2(Section4)
FigureE2andTablesE1–E2showthatnomedical
VLMshowsastatisticallysignificantimprovement Here,wepresenttheconstraineddecodingresults
over its general-domain counterpart in either the formedicalLLMsandVLMswhenusingasingle,
zero-shotor3-shotsetting. fixedpromptformatonlyoptimizedforthemedical
ycaruccA
ycaruccA
evitaleR
ycaruccA
ycaruccA
evitaleROpenBioLLM-70B MediTron-70B OpenBioLLM-8B MediTron-7B BioMistral-7B BioMedGPT-LM-7B
Llama-3-70B-Instruct Clinical-Camel-70B Llama-3-8B Llama-2-7B Mistral-7B-Instruct-v0.1 Llama-2-7B-Chat
Llama-2-70B
1.0
0.8
0.6
0.4
0.2
(a) 0.0
No Improvement
0.4
0.2
0.0
0.2
0.4
MedQA MedQA MedMCQA PubMedQA MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU:
(4 Options) (5 Options) Anatomy Clinical College College Medical Professional High Virology Nutrition
Knowledge Biology Medicine Genetics Medicine School
Biology
1.0
0.8
0.6
0.4
0.2
(b) 0.0
No Improvement
0.4
0.2
0.0
0.2
0.4
MedQA MedQA MedMCQA PubMedQA MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU:
(4 Options) (5 Options) Anatomy Clinical College College Medical Professional High Virology Nutrition
Knowledge Biology Medicine Genetics Medicine School
Biology
FigureE1:MedicalLLMsdonotshowastatisticallysignificantimprovementovertheirgeneral-domaincounterparts
inboth(a)zero-shotand(b)3-shotsettings,afterindependentlyselectingthebestpromptformatandexamplesfor
eachmodel. Toprowshowstheabsoluteexact-matchaccuraciesonthetestset,andbottomrowshowstherelative
exact-matchaccuraciesalongwith95%confidenceintervalsderivedviabootstrappingonthetestset(seeSection3).
Weshowtheresultsforwhenmodelpredictionsaregeneratedviaconstraineddecoding.
model. InFiguresE3–E4,weshowhowtheabso- optimized for each model, we see that a greater
luteandrelativeexact-matchaccuracieschangefor number of medical models show statistically sig-
all LLMs and VLMs in the zero-shot and 3-shot nificantimprovements. InFigureE5,wealsoshow
settings. InTablesE3–E4,wealsoshowthezero- how the win/tie/loss rates of the medical models,
shotand3-shotwin/tie/lossratesinthisscenario. computedoverall(modelpair, QAdataset)com-
Foreachmedicalmodel,weboldfacethewinrate binations,changeaswevarythepromptingsetups
ifitwinsmorethanitlosestoitsgeneral-domain as in Finding 2 of Section 4. As in the greedy
basemodel,andviceversa. decoding setup, we find that for both LLMs and
VLMs,theperformanceimprovementsfrommedi-
Comparedtowhenthepromptisindependently
ycaruccA
ycaruccA
evitaleR
ycaruccA
ycaruccA
evitaleRLLaVA-Med-7B LLaVA-v0-7B Med-Flamingo-9B Open-Flamingo-9B
(a) (b)
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.8 0.8
0.6 No Improvement 0.6 No Improvement
0.4 0.4
0.2 0.2
0.0 0.0
0.2 0.2
0.4 0.4
0.6 0.6
0.8 0.8
VQA-RAD PathVQA SLAKE MMMU: MMMU: MMMU: MMMU: MMMU: VQA-RAD PathVQA SLAKE MMMU: MMMU: MMMU: MMMU: MMMU:
Basic Clinical DiagnosticsPharmacy Public Basic Clinical DiagnosticsPharmacy Public
Medical Medicine & Health Medical Medicine & Health
Science Lab Medicine Science Lab Medicine
Figure E2: Using a single, fixed prompt format only optimized for the medical model can overestimate the
performanceimprovementsfrommedicalDAPT,inboth(a)zero-shotand(b)3-shotsettings. Foreachsetting,
toprowshowstherawexact-matchaccuraciesonthetestset,andthebottomrowshowstherelativeexact-match
accuraciesalongwith95%confidenceintervalsderivedviaboostrappingonthetestset(seeSection3). Weshow
theresultsforwhenmodelpredictionsaregeneratedviaconstraineddecoding.
TableE3: Thewin,tie,andlossrates(%)ofallmedical shotsetting,thewinrateincreasesfrom16.9%to
LLMs(top)andVLMs(bottom)inthezero-shotsetting,
68.0%formedicalLLMsandfrom6.3%to56.3%
whenusingasingle, fixedpromptoptimizedonlyfor
formedicalVLMs,whenonlyperformingprompt
themedicalmodel. Modelpredictionsaregeneratedvia
selection for the medical model and comparing
constraineddecoding.
basedonrawabsoluteaccuracy.
Model Win Tie Loss
OPENBIOLLM-70B(PalandSankarasubbu,2024) 30.8 69.2 0
MEDITRON-70B(Chenetal.,2023) 30.8 53.8 15.4
CLINICAL-CAMEL-70B(Tomaetal.,2023) 63.6 36.4 0
OPENBIOLLM-8B(PalandSankarasubbu,2024) 46.2 46.2 7.7
MEDITRON-7B(Chenetal.,2023) 7.7 76.9 15.4
BIOMISTRAL-7B(Labraketal.,2024) 23.1 76.9 0
BIOMEDGPT-LM-7B(Luoetal.,2023) 30.8 69.2 0
LLAVA-MED-7B(Lietal.,2023) 0 100.0 0
MED-FLAMINGO-9B(Mooretal.,2023b) 25.0 75.0 0
TableE4: Thewin,tie,andlossrates(%)ofallmedical
LLMs(top)andVLMs(bottom)inthe3-shotsetting,
whenusingasingle, fixedpromptoptimizedonlyfor
themedicalmodel. Modelpredictionsaregeneratedvia
constraineddecoding.
Model Win Tie Loss
OPENBIOLLM-70B(PalandSankarasubbu,2024) 23.1 61.5 15.4
MEDITRON-70B(Chenetal.,2023) 7.7 92.3 0
CLINICAL-CAMEL-70B(Tomaetal.,2023) 36.4 63.6 0
OPENBIOLLM-8B(PalandSankarasubbu,2024) 30.8 61.5 7.7
MEDITRON-7B(Chenetal.,2023) 7.7 92.3 0
BIOMISTRAL-7B(Labraketal.,2024) 7.7 92.3 0
BIOMEDGPT-LM-7B(Luoetal.,2023) 30.8 69.2 0
LLAVA-MED-7B(Lietal.,2023) 25.0 75.0 0
MED-FLAMINGO-9B(Mooretal.,2023b) 0 100.0 0
calDAPTcanbesubstantiallyoverestimatedwhen
(i)thepromptisonlytailoredtothemedicalmodel;
and (ii) the models are compared only based on
theirabsoluteaccuracies. Forexample,inthezero-
ycaruccA
ycaruccA
evitaleR
ycaruccA
ycaruccA
evitaleROpenBioLLM-70B MediTron-70B OpenBioLLM-8B MediTron-7B BioMistral-7B BioMedGPT-LM-7B
Llama-3-70B-Instruct Clinical-Camel-70B Llama-3-8B Llama-2-7B Mistral-7B-Instruct-v0.1 Llama-2-7B-Chat
Llama-2-70B
1.0
0.8
0.6
0.4
0.2
(a) 0.0
1.00
0.75
0.50
0.25
0.00
0.25
0.50
No Improvement
0.75
1.00
MedQA MedQA MedMCQA PubMedQA MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU:
(4 Options) (5 Options) Anatomy Clinical College College Medical Professional High Virology Nutrition
Knowledge Biology Medicine Genetics Medicine School
Biology
1.0
0.8
0.6
0.4
0.2
(b) 0.0
No Improvement
0.4
0.2
0.0
0.2
0.4
MedQA MedQA MedMCQA PubMedQA MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU: MMLU:
(4 Options) (5 Options) Anatomy Clinical College College Medical Professional High Virology Nutrition
Knowledge Biology Medicine Genetics Medicine School
Biology
Figure E3: Using a single, fixed prompt format only optimized for the medical model can overestimate the
performanceimprovementsfrommedicalDAPT,inboth(a)zero-shotand(b)3-shotsettings. Foreachsetting,
toprowshowstheabsoluteexact-matchaccuraciesonthetestset,andbottomrowshowstherelativeexact-match
accuraciesalongwith95%confidenceintervalsderivedviabootstrappingonthetestset(seeSection3).ForLLAMA-
2-70B,whichhasmultiplecorrespondingmedicalLLMs(MEDITRON-70BandCLINICAL-CAMEL-70B),we
includeamin-maxerrorbarintheabsoluteaccuracyplotstoshowhowtheabsoluteaccuracychangeswithrespect
toeachprompt. Weshowtheresultsforwhenmodelpredictionsaregeneratedviaconstraineddecoding.
ycaruccA
ycaruccA
evitaleR
ycaruccA
ycaruccA
evitaleRLLaVA-Med-7B LLaVA-v0-7B Med-Flamingo-9B Open-Flamingo-9B
(a) (b)
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.8 0.8
0.6 No Improvement 0.6 No Improvement
0.4 0.4
0.2 0.2
0.0 0.0
0.2 0.2
0.4 0.4
0.6 0.6
0.8 0.8
VQA-RAD PathVQA SLAKE MMMU: MMMU: MMMU: MMMU: MMMU: VQA-RAD PathVQA SLAKE MMMU: MMMU: MMMU: MMMU: MMMU:
Basic Clinical DiagnosticsPharmacy Public Basic Clinical DiagnosticsPharmacy Public
Medical Medicine & Health Medical Medicine & Health
Science Lab Medicine Science Lab Medicine
Figure E4: Using a single, fixed prompt format only optimized for the medical model can overestimate the
performanceimprovementsfrommedicalDAPT,inboth(a)zero-shotand(b)3-shotsettings. Foreachsetting,
toprowshowstherawexact-matchaccuraciesonthetestset,andthebottomrowshowstherelativeexact-match
accuraciesalongwith95%confidenceintervalsderivedviaboostrappingonthetestset(seeSection3). Here,we
showtheresultsforwhenmodelpredictionsaregeneratedviaconstraineddecoding.
Medical Model Wins Tie Medical Model Loses
(a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d)
100 14.5% 5.5% 6.3% 12.5% 14.7% 3.3% 6.3% 6.3%
80 49.2% 32.1% 50.0% 43.8% 52.4% 38.5% 50.0% 56.3%
61.2% 56.3% 60 62.5% 76.1%
68.6% 87.5% 74.1% 93.8%
40
20 33.3% 50.9% 68.0% 50.0% 56.3% 47.7% 61.5% 37.5% 50.0% 43.8%
0 16.9% 6.3% 25.0% 11.2% 20.6%
LLM VLM LLM VLM
Zero-shot 3-shot
Figure E5: Optimizing the prompt for only the medical model and comparing models without accounting for
statisticaluncertaintycanoverestimatetheperformanceimprovementsfrommedicalDAPT.Weshowthewin/tie/loss
rate (%) of medical models vs. their base models across all (model pair, QA dataset) combinations, when (a)
independentlyoptimizingthepromptforeachmodelandperformingstatisticaltesting,(b)optimizingtheprompt
onlyforthemedicalmodelandperformingstatisticaltesting,(c)independentlyoptimizingthepromptforeach
modelwithoutstatisticaltesting,and(d)optimizingthepromptonlyforthemedicalmodelwithoutstatisticaltesting.
Here,weshowtheresultswhenmodelpredictionsaregeneratedviaconstraineddecoding.
fo
noitroporP
)tesataD
AQ
,riaP
ledoM(
ycaruccA
ycaruccA
evitaleR
)%(
snoitanibmoC
ycaruccA
ycaruccA
evitaleR