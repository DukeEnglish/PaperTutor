JOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 1
CPEG: Leveraging Consistency Policy with
Consensus Guidance for Multi-agent Exploration
Yuqian Fu, Graduate Student Member, IEEE, Yuanheng Zhu, Senior Member, IEEE, Haoran Li, Member, IEEE,
Zijie Zhao, Graduate Student Member, IEEE, Jiajun Chai, Graduate Student Member, IEEE,
and Dongbin Zhao, Fellow, IEEE
Abstract—Efficientexplorationiscrucialincooperativemulti-
agent reinforcement learning (MARL), especially in sparse-
reward settings. However, due to the reliance on the unimodal
policy, existing methods are prone to falling into the local 1 2
optima, hindering the effective exploration of better policies.
Furthermore,tacklingmulti-agenttasksincomplexenvironments
requirescooperationduringexploration,posingsubstantialchal-
lenges for MARL methods. To address these issues, we propose Different
Targets
a Consistency Policy with consEnsus Guidance (CPEG), with
twoprimarycomponents:(a)introducingamultimodalpolicyto
enhance exploration capabilities, and (b) sharing the consensus Agent 1
4
among agents to foster agent cooperation. For component (a),
CPEGincorporatesaConsistencymodelasthepolicy,leveraging 3 Agent 2
its multimodal nature and stochastic characteristics to facili-
tate exploration. Regarding component (b), CPEG introduces a
Consensus Learner to deduce the consensus on the global state Fig.1. Anexampleofcooperativeexploration. Thetwo-agentarmrequires
collaborativeexplorationtoreachfourtargetsatdifferentlocations.Inasparse
from local observations. This consensus then serves as a guid-
reward setting, agents must reach all targets before receiving any reward,
ance for the Consistency Policy, promoting cooperation among
makingexplorationmorechallenging.
agents.Theproposedmethodisevaluatedinmulti-agentparticle
environments (MPE) and multi-agent MuJoCo (MAMuJoCo),
and empirical results indicate that CPEG not only achieves
improvements in sparse-reward settings but also matches the be concentrated around the modality. Besides, the unimodal
performance of baselines in dense-reward environments.
policy is prone to converging towards a suboptimal policy
Index Terms—deep reinforcement learning, diffusion model, due to the lack of expressiveness, overfitting the behavior
multi-agent reinforcement learning
of other agents. To address these challenges, some methods
[4], [10], [11] explore alternative policy classes to enhance
I. INTRODUCTION exploration. Although these methods improve exploration to
RECENT years have witnessed a growing body of appli- some extent, they often exhibit limitations in practice. For
cations in cooperative multi-agent reinforcement learn- example, Gaussian mixture models can only cover a limited
ing (MARL), such as multi-robot tasks [1] and autonomous numberofmodes,andnormalizingflowmethods,whileableto
driving [2]. Despite these successful applications, cooperative compute density values, suffer from numerical instability due
MARL still faces challenges in exploration due to limitations totheirdeterminantdependence.Withthegrowingprevalence
in the policy class regarding multi-modality and the necessity of diffusion models [12], a series of works [13]–[15] apply
for cooperation in multi-agent systems (MAS) [3]. them as a powerful and multi-modal policy class, primarily in
In single-agent RL, the exploration quality heavily depends the context of single-agent offline RL. However, the diffusion
on the chosen policy class of agents [4]–[6], with inap- model is time-intensive, involving multiple sampling steps
propriate policy classes potentially leading to local optima. (e.g., 1000 steps), which makes the training and execution
This issue becomes more pronounced in MARL owing to computationally expensive, especially for the online MARL
the complex interaction between agents. In MARL, popular heavily depends on sampling from environments. In order to
methods[7]–[9]commonlyformulatethecontinuouspolicyas accelerate the speed of the sampling process, a novel model,
aunimodaldensityfunction,typicallyaGaussiandistribution. theconsistencymodelisdesignedtomapanypointatanytime
While computationally efficient, these policies can signifi- stepbacktothestartofthetrajectory,basedontheprobability
cantly weaken the exploration, as the sampled actions tend to flow ordinary differential equation (PF-ODE) [12].
Besides,thecooperationamongagentsduringexplorationis
This work has been submitted to the IEEE for possible publication.
criticalincomplexenvironments[3],[16],[17].Toexemplify,
Copyright may be transferred without notice, after which this version may
nolongerbeaccessible. consider the Reacher4 task depicted in Fig. 1, wherein two
The authors are with the State Key Laboratory of Multimodal Artificial agents are required to coordinate their swinging motions to
IntelligenceSystems,InstituteofAutomation,ChineseAcademyofSciences,
ensure that the end-effector of the manipulator contacts one
Beijing 100190, China, and also with the School of Artificial Intelligence,
UniversityofChineseAcademyofSciences,Beijing100049,China. of the four targets. In a sparse reward setting, agents must
4202
voN
6
]AM.sc[
1v30630.1142:viXraJOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 2
reach all targets before receiving any reward, making explo- the effectiveness of our approach.
ration more challenging. In this scenario, the agents need to Our main contributions are summarized as follows:
coordinatetheiractionstosettheentireroboticarminmotion, 1) We introduce a Consistency Policy for agents, capable
instead of engaging in independent and meaningless actions. of completing the diffusion process in a single step, to
Additionally, the agents need to adapt to the multimodality explore the environment in a multimodal way. To the
brought by the four targets, avoiding premature convergence best of our knowledge, our study represents the first
to a unimodal policy and achieving simultaneous exploration effort in leveraging consistency policies in MARL.
of multiple targets. This challenge can be tackled through the 2) We propose a Consensus Learner to infer the global
use of communication-based MARL techniques [18]. How- consensus from local observations, aimed at guiding
ever, these techniques introduce challenges such as selecting efficient cooperative exploration by sharing the same
appropriate information to transmit and additional bandwidth consensus between agents.
requirements. Recently, some studies [19], [20] leverage a 3) WeincorporateaSelf-referenceMechanismtoconstrain
shared consensus among agents to promote cooperation. At the generated actions by leveraging past successful ex-
each timestep, although the local observations of each agent periences,therebyreducingtheprobabilityofgenerating
are unique,they representdifferent aspectsof the sameglobal invalid actions by policies.
state. In the given scenario, observations of the agents only 4) We evaluate our method on MPE and MAMuJoCo
cover their own information such as angular velocity, which with dense and sparse reward settings. The results in-
serves as partial representations of the global state. However, dicate that CPEG performs comparably to state-of-the-
these individual observations are merely projections of the art (SOTA) algorithms in dense reward environments,
state. Consequently, our objective is to extract the global state while in sparse reward environments, which are more
information through consensus, thereby fostering cooperation. challenging for exploration, our algorithm outperforms
Inspired by the aforementioned observations, we introduce SOTA algorithms by 20%.
anovelframeworktailoredtoMARL,termedtheConsistency
PolicywithconsEnsusGuidance(CPEG),aimedatfacilitating
II. RELATEDWORK
efficient cooperative exploration. In this work, we adopt a
powerful and effective model, the consistency model as the A. Exploration in MARL
policy class, with the objective of harnessing its stochastic Asoneofthemostcriticalissuesinreinforcementlearning,
nature and expressiveness to explore in a multimodal manner. the exploration has drawn great attention in single-agent RL
Compared with diffusion models, consistency models enable research. This can be even more challenging in complex
efficientone-stepgenerationwhileretainingtheadvantagesof environments with sparse reward [3]. In the multi-agent do-
multi-step iterative sampling. For cooperative exploration, we main, several works [16], [17], [24] have emerged to address
introduce a discrete consensus representation as the guidance exploration challenges. MAVEN [24], for instance, aims to
fortheconsistencypolicy.Specifically,weemployacodebook maximize the mutual information between the trajectories
from VQ-VAE [21] for a discrete, distinguishable consensus and latent variables, thereby enabling the learning of diverse
representation. This allows agents to derive the same estima- exploration policies. EITI and EDTI [16], on the other hand,
tion of the global state from a unified consensus codebook, are designed to capture the influence of one agent’s behaviors
thereby guiding cooperative behaviors. To strike a balance on others, encouraging agents to visit states that influence the
between exploration and exploitation during training, some behavior of others. CMAE [17] adopts a shared exploration
studies [6], [22] use the mask to decide on exploration or goal derived from multiple projected state spaces. Beyond
exploitation. Building on the insights from these work, we these considerations, recent works highlight that the choice
design a guidance mask to intermittently drop the guidance of policy class employed by the agents can significantly
with probability. Additionally, during the initial phase of impact exploration dynamics [4], [5]. However, these efforts
training, generative policies may produce unreliable actions, primarily concentrate on single-agent domains. In this paper,
which can affect the training of the policy. To address this, we highlight the equally crucial role of policy classes in
we introduce a self-reference mechanism during the training decision-making within multi-agent systems. In our proposed
phase, leveraging past successful experiences to constrain the approach, CPEG, we introduce an innovative policy class, the
actions generated by the consistency policy, thus facilitating consensus-guided consistency policy, specifically tailored to
exploration. enhance exploration in MARL.
We empirically evaluate the proposed method on two
distinct environments: the multiple-particle environment
B. Diffusion Models for Decision Making
(MPE) [7] and multi-agent MuJoCo (MAMuJoCo) [23].
Acrossallexperiments,weexplorebothdense-rewardsettings The application of diffusion models in single-agent RL has
and sparse-reward settings, where the latter poses significant emerged as a significant trend, showcasing their capability
challenges for exploration as agents receive rewards solely to enhance model expressiveness and improve the decision-
upon completion of a specified task. The results demon- making process [25]. Recognized for their powerful and flex-
strate that CPEG exhibits notable superiority over competitive ible representation, diffusion models find utility in various
baselines in sparse-reward settings and achieves comparable domains such as trajectory generation [26] and latent skill
performance in dense-reward settings, thereby underscoring extraction [27]. Moreover, owing to their exceptional capacityJOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 3
forrepresentingmultimodaldistributions,diffusionmodelsare boundary. To accelerate the sampling process of a diffusion
also employed as effective policy classes. In the realm of model, the consistency model reduces the required number of
offline RL, methods like Diffusion-QL [13] and EDP [14] samplingstepstoasignificantlysmallervaluewithoutsubstan-
replace conventional Gaussian policies with diffusion models. tially compromising model generation performance Specifi-
BuildinguponthefoundationalworklaidbyJanneretal.[28], cally, it approximates a parameterized consistency function
andWangetal.[16],recentstudies[29],[30]extenddiffusion f : (x ,τ) → x , which is defined as a map from the noisy
θ τ ϵ
models to MARL, applying them to trajectory generation and samplex atstepτ backtotheoriginalsamplex ,incontrast
τ ϵ
policy estimation. However, it is worth noting that diffusion to the step-by-step denoising function p (x |x ) as the
θ τ−1 τ
models typically entail multiple sampling steps, posing a reverse diffusion process in diffusion model. The consistency
considerable time constraint for diffusion policy methods, function possesses the property of self-consistency, meaning
particularly in online environments. This challenge is further its outputs are consistent for arbitrary pairs of (x ,τ) that
τ
exacerbatedinMARLduetotheheightenedcomputationalde- belong to the same PF-ODE trajectory. Besides, for modeling
mandsasthenumberofagentsincreases[31],[32].Toaddress the conditional distribution with condition variable c, the
this issue, some methods adopt faster sampling techniques consistency function is modified to f (c,x ,τ), representing
θ τ
such as DPM-Solver [33]. Additionally, certain methods like a slight deviation from the original consistency model.
CPQL [34] and Consistency-AC [35] incorporate the consis-
tency model [36], streamlining the sampling process within
IV. METHOD
just one or two diffusion steps. Inspired by these studies,
In this section, we introduce CPEG, a novel approach that
we introduce consistency models as policies into MARL for
integratestheconsistencypolicyguidedbyconsensusbetween
the first time. Overall, our method not only leverages the
agents for exploration. As illustrated in Fig. 2, our framework
multi-modal nature of diffusion models but also significantly
consistsofthreeprincipalcomponents:a)aconsistencypolicy
enhances the time efficiency.
that generates actions from Gaussian noise, b) a discrete con-
sensus representation with masks, which serves as a guidance
III. PRELIMINARIES
forcooperation,andc)aself-referencemechanismtoconstrain
A. Problem Formulation
consistency policies. Detailed discussions on each component
We consider a fully cooperative multi-agent task as a are provided in the subsequent sections.
decentralized partially observable Markov decision pro-
cess (Dec-POMDP) [37], represented as a tuple G =
A. Consistency Policy
⟨S,A,U,P,r,O,Ω,γ⟩, where A represents the set of agents
with |A|=n . At each time step, the environment generates a) Notation: Given the presence of two types of
a
the global state s ∈ S, and each agent a ∈ A receives a timesteps in this work, one for the diffusion process and one
uniquelocalobservationo ∈O,producedbytheobservation for reinforcement learning, we use superscripts τ ∈ [0,T] to
a
function Ω(s,a) : S ×A → O. Subsequently, each agent a denote diffusion timesteps and subscripts t ∈ {1,...,T e} to
selectsandexecutesitsownactionu ∈U,resultinginajoint denote environment timesteps.
a
actionu∈Una thatinducesastatetransitionaccordingtothe Theconsistencypolicyaimstoexploreinamultimodalway
state transition function P(s,u):S×Una →S. Meanwhile, andavoidfallingintolocaloptima.Tofacilitateunderstanding,
theenvironmentprovidesaglobalrewardsharedbyallagents, we first formulate the diffusion policy before introducing the
determined by the reward function r(s,u) : S ×Una → R. consistency policy, which serves as its successor. In diffusion
R = (cid:80)Te γtrt is the agent’s total return, where T denotes policy, the action of agent a is diffused with a SDE:
t=0 e
thetimehorizon,andγ ∈[0,1)representsthediscountfactor.
duτ =µ(uτ,o ,τ)dτ +σ(τ)dwτ, (1)
Q (o,u) = E [R|o,u,o ,u ] is the action a a a
a {o−a,u−a}∼B −a −a
value function for agent a, where o and u denotes the where µ(·,·) representing the drift coefficient, σ(·) represent-
−a −a
observations and actions of all agents except agent a. The ing the diffusion coefficient, and {wτ} denoting the
τ∈[0,T]
experience replay buffer B contains the tuples ⟨o,o′,u,r⟩. standard Brownian Motion. Beginning with uT, the diffusion
a
policy aims to recover the original action u0 =u by solving
a a
B. Consistency Models a reverse process from T to 0 using the reverse-time SDE:
The diffusion model [12] addresses the multimodal dis- duτ =(cid:2) µ(uτ,o ,τ)−σ(τ)2∇logp (uτ,o )(cid:3) dτ+σ(τ)dw¯τ,
a a a τ a a
tribution matching problem using a stochastic differential (2)
equation (SDE), while the consistency model [36] tackles where w¯ is the reverse Brown Motion, and the score function
a comparable probability flow ordinary differential equation ∇logp (uτ) is the sole unknown term at each diffusion
τ
(PF-ODE): dx τ/dτ = −τ∇logp τ(x), where τ ∈ [0,T], timestep. Solving the Eq. (2) is challenging in practice, as
T >0isafixedconstant.Here,p τ(x)=p data(x)⊗N(0,τ2I) an alternative, we solve the corresponding PF-ODE:
denotes the distribution of data x τ at step τ, where ⊗ denotes (cid:20) 1 (cid:21)
the convolution operation and p data(x)=p 0(x) represents the duτ
a
= µ(uτ a,o a,τ)− 2σ(τ)2∇logp τ(uτ a,o a) dτ. (3)
raw data distribution. The reverse process occurs along the
solution trajectory {xˆ } , with ϵ being a small constant Althoughthisdiffusionpolicycanbeusedforexploration,it
τ τ∈[ϵ,T]
close to 0, employed for handling the numerical issues at the is time-intensive and makes the policy inference unacceptablyJOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 4
Self-reference
Reference
Buffer
Consensus
Observation Codebook
Encoder Consensus-guided Consistency Policy
Action Output
ODE Trajectories
Gaussian Noise
Policy Learning
Replay
Buffer
Fig.2. TheoverallframeworkoftheproposedCPEG.Consensus-guideddiffusionpoliciesfacilitatecooperativeexplorationamongmultipleagents.The
consistencypolicyutilizestheagent’sobservationandGaussiannoiseasinputs,generatingactionsundertheguidanceofmaskedconsensus.Theconsensus
learnerconsistsofanobservationencoder,consensuscodebook,andstatedecoder,whichlearnsdiscreteconsensusrepresentationsbyreconstructingstates.
Duringthetrainingphase,inadditiontopolicylearning,theself-referencemechanismprovidesgradientback-propagationbasedonthedisparitybetweenthe
agent’sactionandthosefromthereferencebuffer,therebyimposingapolicyconstraint.
slow, especially in tasks that contain multiple agents. To
Training Phase
addressthisissue,weintroduceaconsistencypolicy[34],[36]
based on Eq. (3): concat
π (u |o ):=f (o ,uτn,τ )
θa a a θa a a n (4)
=c (τ )uτn +c (τ )F (o ,uτn,τ ), State Decoder
skip n a out n θa a a n Execution Phase
where the initial action uτ an ∼ N(0,τ nI), and θ a represents mask Co Pns oi ls it ce yncy
the parameters of the policy π of the agent a. We discretize
the diffusion horizon [ϵ,T] into a predetermined sequence Observation Encoder Consensus Codebook Masked Consensus Guidance
{τ |n ∈ N} of length N for determining the solution
n
Fig.3. TheframeworkoftheConsensusLearner.Itoperatesdifferently
trajectory of action. Moreover, τ is the n-th sub-intervals
n duringthetrainingandexecutionprocess.
of the time horizon, with n ∼ U(1,N −1). The consistency
functionF (o ,uτ,τ)isatrainabledeepneuralnetworkthat
θa a a
acceptstheobservationo ofagentaasaninputandgenerates by minimizing the clipped double Q-learning loss [38]:
a
da
e
fin ni xf
s
ef ua e drc r ,iet ni sno
g
mtn ia atb
h
lw ll eei pt ch of
o
su ind tn si ic vm ist ei te o en tn
n
is ms ci ,o
y
en sw ps teri pt om h
c
ϵa
e
.t c sc s sh ki i rpn e(g mϵ) au iτ a n=. sc ds 1 ik f,i fp ec ro ea u nn t td ( iaϵc ) bo leu =t aa tr 0 ae , (cid:20)L (cid:16)T (cid:0)D r( +β j γ)=
i∈m
{E
1in
,{ 2o },u Q− βa i⊺} (∼ oB ′, ,u ua a∼ ′π (cid:1)θ −a(o Qa)
βj(o,u
a)(cid:17)2(cid:21)
,j =1,2.
(7)
Inthiswork,weemploytheparadigmofcentralizedtraining where β,β⊺ are the parameter and target parameter of Q-
with decentralized execution (CTDE). Therefore, the joint
networks respectively.
policy is expressed as a composition of individual policies:
(cid:89)na
B. Consensus Guidance
π (u|o)= π (u |o ). (5)
θ θa a a
To guide cooperation in multi-agent exploration, we intro-
a=1
duce a consensus learner, where agents sharing the same con-
b) Policy Learning: To train the consistency policy, we sensus demonstrate cooperative behaviors. We first formulate
adopt an off-policy optimization approach similar to MAD- a set of K discrete consensuses Ψ := {ψ 1,...,ψ K}, where
DPG [7]. Following the approach of Wang et al. [13], we K ∈ N is a tunable hyperparameter. Each consensus ψ k is
integrate the Q-value function into the consistency model, defined by a tuple ⟨e ψk,I ψk⟩, where k ∈ {1,...,K} is the
training it to preferentially sample actions with higher values: identity of consensus and e ψk ∈ Rm is the code (or latent
embedding) of consensus ψ . I is the set of agents who
L policy(θ a)=−E oa∼B,ua∼πθa(oa)[Q(o a,u a)]. (6) have the same consensus ψ kk , anψ dk each agent can only have
one consensus at each timestep: I ∩I = ∅,∪ I = I
ψi ψj j ψj
Specifically, as an estimation of the action value of the for i,j ∈ {1,2,...,K} and i ̸= j. To represent consensus,
agent’scurrentpolicy,theparameterβofQ(·,·)canbelearned we design a consensus leaner with VQ-VAE [21], as shownJOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 5
in Fig. 3. The motivations for this include: a) The discrete corresponding vector z :
e,a
codebook in VQ-VAE can effectively extract state features
e ←µz +(1−µ)e , (10)
with similar semantics, providing better separation than tra- ka e,a ka
ditional continuous representation methods. b) The use of where the value of µ is a hyperparameter that controls the
discrete feature representation helps mitigate the impact of speed of the moving average updates.
unavoidable noise on exploration in stochastic environments.
c) State Decoder: To develop the ability to deduce
c) The discretization process ensures a distinct association,
consensusfromlocalobservations,weaggregatetheconsensus
where each agent is linked to a single, specific consensus.
ψ from all agents together and process them through a state
a
The consensus learner comprises four primary components: decoder z d(·;α d), parameterized by α d, to reconstruct the
anobservationencoder,aconsensuscodebook,astatedecoder, global state sˆ. We train the observation encoder and state de-
and a consensus mask. We assume the consensus can be de- coder by maximizing the log-likelihood of the reconstruction:
ducedfromthecurrentglobalstates.However,fordistributed
L (α )=logp(sˆ|z (ψ;α )), (11)
recon e,d d d
execution in CTDE, each agent a can only access its local
observation o . Considering o = Ω(s,a) as a projection of whichencouragestheencodinganddecodingprocessesofthe
a a
theglobalstates,wedesignatwo-phasemechanismtodeduce consensus learner to capture the key information from partial
consensus from local observations. To deduce the consensus observationsforaccurateconsensusdeduction.Inaddition,the
from local observations, the consensus learner has two modes reconstruction loss gradient is also passed to the encoder for
during the training and execution phases: during the training training α by straight-through gradient estimation.
e
phase, the consensus learner processes observations from all d) Mask Consensus Exploration: In this work, we em-
agents, obtains discrete consensus representations for each ploy two distinct exploration strategies: unguided exploration
agent’s observation, and uses them to reconstruct the global and consensus-guided exploration, aiming to balance explo-
state; in the execution phase, it processes agent a’s observa-
ration and exploitation to some degree. Unguided exploration
tion to obtain the consensus for guiding policy generation of
allowsagentstoactaccordingtotheircurrentpolicieswithout
action u . In this manner, the consensus learner develops the
a specific consensus guidance, emphasizing their self-directed
capacity to infer consensus based on local observations.
exploration capabilities. However, its effectiveness is limited
by the initial policy quality and the inherent complexities of
a) Observation Encoder: The encoder z (·;α ), param-
e e MAS, neglecting potential cooperative advantages. Consider-
eterized by ℵ , encodes the observation of agent o into an
e a ingthis,weintroduceanotherexplorationstrategy,consensus-
embedding z ∈ Rm, matching the length of the consensus
e,a guidedexplorationdesignedtosteerthecooperationeffortsof
representation m. In practice, we use historical observations
agents. The consensus-guided exploration instructs agents to
as inputs during each agent’s execution to capture sufficient
take actions based on the consistency policy guided by their
information about the global state.
assignedconsensus.Throughtheguidanceofsharedconsensus
among different agents, consistency policies can generate
b) Consensus Codebook: The process of inferring con-
cooperative exploration behaviors. Meanwhile, to incorporate
sensus for each input observation proceeds as follows. First,
consensus guidance, the consistency function F (o,u,τ) in
the embedding z is mapped to a discrete index k of the θ
e,a a
Eq. (4) is modified to F (o,u,ψ,τ).
codebook E: θ
To implement the above concept and combine the two ex-
k a = argmin ∥z e,a−e j∥ 2, (8) ploration mechanisms, we introduce a binary consensus mask
j∈{1,...,K}
M. This mask can modulate the influence of the consensus
wheree j ∈E isacodebookvector,andK denotesthesizeof vectorψ a onactiongenerationbyobscuringitthroughmasked
thecodebook.Notably,theperformanceofconsensuslearning attention. We achieve this through masked attention, setting
is relatively insensitive to the choice of K, allowing us to set the consensus mask M = 0,ψ = ∅ to prevent the down-
a
the number of discrete consensus in codebook K = 5 across stream generation of u from being guided by the consensus
a
all scenarios. Based on this discrete index k a, the discrete representation. Conversely, when M = 1, the consensus
consensus representation for the agent a is derived as ψ a = representation is incorporated alongside the agent’s observa-
e ka. The loss function for the consensus inference process is tion during action generation. During the training phase, the
defined as: consensus mask M is sampled from a Bernoulli distribution
with a probability p . We set a fixed value p = 0.2
∥sg(z )−e ∥2+β∥z −sg(e )∥2, (9) M=1 M=1
e,a ka 2 e,a ka 2 during training, resulting in a higher proportion of training
where sg denotes the straight-through gradient operation [39], samples for consensus-guided over unguided exploration. In
enabling back-propagation training of argmin operations, and execution phase, we adjust p =1.0, encouraging agents
M=1
β is a hyperparameter balancing the degree of alignment to prioritize consensus-driven actions. It’s noteworthy that a
between the codebook and input embeddings. In practice, recent work [34] demonstrates that consistency policies can
CPEG uses the exponential moving average (EMA) to update achievepolicyimprovementwithpreciseguidance.InMARL,
thecodebook,insteadofdirectlylearningthemasparameters. giventhejointpolicy’scomplexityandtheenvironment’snon-
Specifically, EMA updates the codebook e by replacing it stationarity, precise guidance can more significantly enhance
ka
withaweightedcombinationofitspreviousvaluee andthe the agents’ policy improvement.
kaJOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 6
C. Self-reference Mechanism Algorithm 1: Leveraging Consistency Policy with
Consensus Guidance for Multi-agent Exploration
Due to the nature of generative models, the consistency
policymaygenerateinvalidactionsduringtheinitialstagesof // Initialize
training [40]. In order to constrain the generated actions, we 1 Initialize the policy network π θ and critic networks
proposeaself-referencemechanism,inspiredbyself-imitation Q ,Q ;
β1 β2
learning [41]. The self-reference mechanism stores previous 2 Initialize the replay buffer B and reference buffer B +;
experiences in a replay buffer and learns to replicate actions 3 for episode j =1,...,M do
that led to high returns in the past. For each agent, we collect 4 Reset the environment and receive initial joint
observation-actionpairs(o ,u ),alongwiththeircorrespond- observation o;
a a
ing returns, and store them in the replay buffer B. We then 5 for time step t=1,...,H do
sample entries (o,u,R) with empirical returns exceeding the // Consensus guidance
action-value estimates Q(o,u a), creating a reference buffer 6 For each agent a, infer consensus ψ a by
B ={(o,u,R)|R≥Q(o,u )}, where u is generated from Eq. (8);
+ a a
the current policy π θa. With B +, the consistency policy of 7 Sample consensus mask M={0,1};
agent a can be trained with a reference loss function: 8 Generate u a by π θa with consensus ψ a,
consensus mask M, and observation o ;
L (θ )=E a
(cid:2) λre (f τ n)a d(cid:0) f θa(n o∼ aU ,( u1 τ a,N n+− 11 ,) ψ,( aoa ,, τu na +)∼ 1)B ,+ f θa⊺(o a,uτ an,ψ a,τ n)(cid:1)(cid:3) , 9 U //pd Bat ue fc fo en rsen usu ps dale tar ener by Eqs. (9) to (11);
(12) 10 Execute actions u and observe reward r and
where λ(·) denotes a step-dependent weight function, uτn = new observation o′;
u+τ z,z ∼N(0,I)andd(·,·)representsthedistancemetric. 11 Store ⟨o,o′,u,r⟩ in replay buffer B;
n
Additionally, f θ⊺ is the exponential moving average of f θ, 12 Update reference buffer B +;
introducedfortrainingstability.Theself-referencemechanism 13 for agent a=1,...,n a do
inouralgorithmoffersthreebenefits:a)Asgenerativemethods // Policy update
are prone to generating invalid actions [40], the self-reference 14 Sample minibatch B ∈B and B + ∈B +;
mechanism acts as a policy constraint, ensuring more reliable 15 Update policy π θa by Eq. (6);
action selection. b) Learning from successful transitions can // Self-reference
accelerate exploration during the early training stages. c) self- 16 Constrain policy π θa by self-reference
imitation learning can be viewed as a lower bound of soft Q- mechanism Eq. (12);
learning[42],promotingpolicyimprovement.Thepseudocode 17 end
of CPEG is illustrated in Algorithm 1. // Q-value update
18 Update Q-value networks Q β1,Q β2 by Eq. (7);
19 end
V. EXPERIMENTS
20 end
In this section, we empirically evaluate our method to
answer the following questions: (1) Does CPEG effectively
contribute to exploration and outperform baselines (See Sec-
we employ Navigation and Reference as the experi-
tion V-B, Section V-D)? (2) Can consistency policies alleviate
mental environments. In MAMuJoCo, the experimental en-
the time-consuming issue associated with diffusion policies
vironments include HalfCheetah(2x3), Hopper(3x1),
(SeeSectionV-C)?(3)Howdoestheconsensuslearnerplaya
Reacher4(2x1), and HalfCheetah(6x1).
roleincooperativeexploration(SeeSectionV-D)?(4)Whether
consensus guidance and self-reference mechanism contribute • Hopper (3x1): This environment features a Hopper in
MuJoCo,controlledbyagentsoperatingitsthreeprimary
collectively to the final performance (See Section V-E)?
segments: torso, thigh, and leg. The agents must collab-
orate effectively to propel the Hopper forward, neces-
A. Experimental Settings sitating intricate coordination as each segment’s move-
ment affects the entire structure’s balance and progress.
a) Multi-Agent Task Benchmark: We conduct experi-
In sparse-reward settings, the agent receives a reward
mentsintwowidely-usedmulti-agentcontinuouscontroltasks
including the Multi-agent Particle Environments1 (MPE) [7] only upon advancing a specified distance, which places
and high-dimensional and challenging Multi-Agent MuJoCo2 demands on cooperative exploration.
(MAMuJoCo) [23] tasks. In MPE, agents known as physical • HalfCheetah (2x3) and HalfCheetah (6x1): In this
environment, the HalfCheetah is a robotic model that
particlesneedtocooperatetosolvevarioustasks.MAMuJoCo
resembles a simplified, two-dimensional cheetah. These
is an extension for MuJoCo locomotion tasks, originally
scenarios involve two and six agents, each representing a
designed for single-agent scenarios, enabling robots to move
front or rear leg. In this shared environment, the agents
withthecoordinationofmultipleagents.Specifically,inMPE,
engage in interactions with the primary objective of
1https://pettingzoo.farama.org/environments/mpe advancing forward without falling. These environments
2https://github.com/schroederdewitt/multiagent mujoco parallel the Hopper (3x1) in that agents are rewarded inJOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 7
TABLEI
PERFORMANCEEVALUATIONOFRETURNSANDSTANDARDDEVIATIONINDIFFERENTSCENARIOSWITHDENSEREWARDSANDSPARSEREWARDS.
±CAPTURESTHESTANDARDDEVIATION.WECOMPUTEDTHEAVERAGESCORESOFEACHALGORITHMACROSSDIFFERENTSCENARIOSUNDERTWO
SETTINGS.HIGHLIGHTEDFIGURESSHOWTHEHIGHESTPERFORMANCEAMONGEACHROW.
Scenarios CPEG (Ours) HATD3 [43] MAPPO [9] HASAC [43] MAT [44] CMAVEN [24]
Navigation -68.1±2.0 -66.7±9.1 -84.7±9.7 -68.6±8.3 -66.9±6.8 -88.3±11.2
Reference -11.9±1.7 -14.3±4.2 -32.1±3.8 -12.6±2.7 -15.9±2.9 -37.6±4.5
Dense Reacher4 (2x1) -22.1±0.4 -22.3±0.8 -22.9±0.5 -21.8±0.2 -23.0±0.2 -26.9±0.3
Reward HalfCheetah (2x3) 6954.9±466.8 5651.3±441.3 5463.1±392.7 7541.3±472.6 6711.6±459.8 4526.8±314.2
Hopper (3x1) 3593.4±257.1 3349.9±187.6 3228.3±194.9 3613.2±201.4 2745.8±174.1 2201.4±222.8
HalfCheetah (6x1) 5692.7±339.4 4782.4±438.6 4083.1±352.1 5535.4±392.4 5057.4±334.7 3343.6±366.2
Average 2689.8±177.9 2279.9±180.3 2105.8±158.9 2764.4±215.4 2401.4±195.7 1983.8±153.2
Navigation 19.3±0.3 8.9±1.3 4.2±0.8 10.4±0.6 3.9±0.9 4.9±1.8
Reference 18.1±0.5 10.4±0.6 3.3±0.8 10.2±0.4 3.8±0.7 8.1±0.9
Sparse Reacher4 (2x1) 50.0±0.0 36.8±0.1 24.4±0.5 38.9±0.6 37.6±0.8 28.5±2.4
Reward HalfCheetah (2x3) 965.0±8.0 805.0±12.6 703.7±27.9 801.2±10.9 733.7±14.1 629.1±13.2
Hopper (3x1) -4.8±0.7 -33.6±3.8 -27.9±2.7 -16.9±0.9 -20.3±4.3 -18.4±3.3
HalfCheetah (6x1) 704.3±9.8 501.6±10.2 533.6±21.9 622.0±16.4 578.1±18.2 480.7±10.3
Average 291.9±3.8 221.5±4.7 206.8±9.0 244.3±13.9 222.7±6.5 188.7±5.3
(a) Hopper (3x1) (b) HalfCheetah (c) Reacher4 (d) Spread (e) Reference
Fig.4. Demonstrationsoffivesparse-rewardenvironments.BothHalfCheetah(2x3)andHalfCheetah(6x1)arerepresentedasHalfCheetah.
sparse-reward settings only when they achieve forward sparse-reward settings, running 2 million steps in MPE and
movement over a certain distance [45]. 5 million steps in MAMuJoCo. In the sparse-reward setting,
• Reacher4: To investigate cooperative exploration in a agents don’t receive any intermediate rewards, i.e., agents
more complex continuous control setting, we propose a receive rewards only upon reaching a target or covering a
two-agent version of the Reacher environment with four specified distance.
targets, Reacher4. The agents are comprised of a two-
b) Baselines: Wecomparedourresultswithseveralbase-
arm robot, with the goal being to move the robot’s end
lines as follows. HATD3 and HASAC are strong continuous
effector close to one of the four targets. In this scenario,
control multi-agent algorithms, which are newly extended
wesetupsparserewards,whichmeansthattheagentsare
from TD3 and SAC by HARL [43]. MAPPO [9] stands as a
rewarded only upon making contact with all four targets.
popular and effective MARL algorithm in the continuous do-
• Spread: In Spread, there are n agents and n landmarks. main,generatingactionsfromGaussiandistribution.MAT[44]
The agents’ goal is to occupy all landmarks. They incur
addresses the multi-agent challenge as a sequential modeling
penalties for collisions and are rewarded only when all
issue,usingtheTransformer[47]togeneratecomplexactions.
landmarks are simultaneously covered.
Notably, we observe that few MARL methods focus on
• Reference: The Reference scenario requires each agent explorationwithincontinuousactionspaces.Consequently,we
to approach their designated landmark, which is known
extend the classic MARL exploration algorithm MAVEN [24]
only to the other agents. Both agents are simultaneous
into its continuous version, termed CMAVEN. Drawing in-
speakers and listeners. The key challenge lies in the
spiration from FACMAC [23], CMAVEN adopts the cross-
sparsity of the environment’s rewards: agents receive a
entropy method (CEM) — a sampling-based, derivative-free
reward only when all of them reach their designated
heuristic search strategy—for approximate greedy action se-
landmarks simultaneously [46].
lection. CEM has demonstrated effectiveness in identifying
Across both environments, we consider dense-reward and near-optimal solutions in nonconvex Q-networks for single-JOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 8
agent robotic control tasks. the consistency policy with consensus guidance and a self-
c) Training Details and Hyper-parameters: We conduct inference mechanism, fosters more efficient exploration and
five independent runs with different random seeds in all significantly enhances performance.
scenarios. The experiments in this study were conducted
using five distinct seeds on a hardware setup comprising C. Time Efficiency
2 NVIDIA RTX A6000 GPUs and 1 AMD EPYC CPU.
In addition to its superior performance, CPEG also pos-
We utilized official implementations of baseline algorithms,
sesses higher time efficiency compared with methods based
adhering strictly to their original hyper-parameters. The em-
on diffusion models. This efficiency is essential for practical
ployed consistency policy involves a multi-layer perceptron
scalabilityinmulti-agentsystemsandforfacilitatingreal-time
(MLP),whichprocessesstateinputstogeneratecorresponding
decision-making processes. To demonstrate this, we replace
actions. Specifically, our policy networks incorporate a 3-
thepolicyclassinourmethodwithadiffusionmodel(CPEG-
layer MLP architecture with a hidden size of 256, using the
DM) and an accelerated variant of it (CPEG-DPM) [33], and
Mish activation function. The consensus learner consists of
compared the time required by the different algorithms for
a MLP, a consensus codebook module, and another MLP.
each training iteration, as shown in Table II.
TheMLPnetworkisa128-dimensionalfully-connectedlayer,
In Navigation and HalfCheetah(2x3) scenarios,
followedbyaGELUactivation,andfollowedbyanother128-
our method attains training speeds up to 6.6× and 6.8×
dimensional fully-connected layer.
faster than those of diffusion-based methods, rivaling the time
For the consistency policy, we define the diffusion step
efficiency of previous methods like HATD3. Despite using
k within the range of [0.002,80.0], setting the number of
DPM-Solver, CPEG-DPM still faces challenges in achieving
sub-intervals M = 40. Following Karras diffusion model,
satisfactory training efficiency. Moreover, the training speed
the sub-interval boundaries are determined with the formula
(cid:16) (cid:16) (cid:17)(cid:17)ρ of algorithms is greatly impacted by the scaling in MAS.
k
i
= ϵρ1 + Mi− −1
1
Tρ1 −ϵρ1 , where ρ = 7. The Euler
We selected Humanoid(17x1) scenario, which involves 17
method is employed as the ODE solver. In the consensus
agents,forevaluatingourmethodsunderlarge-scaleagents.In
learner, we set the number of discrete consensus in codebook
thisscenario,thetrainingtimeofdiffusion-basedmethodssig-
K = 5 in different scenarios, and β = 0.2. The performance
nificantlyexceedsthatofCPEG,demonstratingthescalability
of consensus learning is not sensitive to the setting of K.
of our method in MARL.
Regarding performance evaluation, we pause training every
Due to the substantial reduction in sampling steps, the
M steps and evaluate for N episodes with action selection.
consistency model is inherently less expressive than the diffu-
The (M,N) in MPE and MAMuJoCo are (1000,20) and
sion model [36]. To evaluate this, we conduct a performance
(10000,40), resepcetively. The training objective is optimized
comparison between CPEG and diffusion-based methods in
by Adam with a learning rate of 5×10−4.
three sparse scenarios. In comparison to CPEG-DM and
CPEG-DPM, CPEG exhibits a performance decrease in MPE
B. Performance Comparison scenarios, yet outperforms it in the HalfCheetah(2x3)
and Humanoid (17x1) environments. This phenomenon
We first compare CPEG with baselines across six scenar-
could potentially be attributed to that the consistency policy
ios. Table I outlines the comparison between our approach
can effectively transmit action gradients in one step, thereby
and various baseline algorithms in both dense reward and
alleviating the challenges of policy training via Q-function
sparse reward settings. In dense-reward environments, CPEG
in diffusion-based methods [34]. Moreover, the slight per-
demonstrates performance slightly below or comparable to
formance trade-off in CPEG is considered acceptable, given
baselines, surpassing them in specific tasks. In the context of
its significant improvements in training and online inference
online MARL in POMDPs, the theoretically optimal policy
efficiency.
tends to be deterministic, suggesting that expressive models
oftenreverttounimodaltoapproachoptimality.Consequently,
D. Qualitative Analysis and Visualization
expressiveness primarily aids in exploration rather than con-
tributes to the final optimal policy convergence. In sparse- To showcase the exploration capabilities of our method,
reward environments requiring exploration, CPEG approach consider a didactic example illustrated in Fig. 5(a), which
exhibits a significant performance advantage, outperforming demonstratesdifferentMARLalgorithmsappliedtothesparse
baseline algorithms by 20%. While methods like HATD3, Reacher4 task [23] featuring four targets. The agents con-
MAPPO,andMATshowproficiencyindenserewardcontexts, sist of two-arm robots tasked with moving the robot’s end
theirlimitedfocusonexplorationhinderstheireffectivenessin effector close to one of the four targets. The depth of color
reaching sparse reward states. Although the HASAC method represents the magnitude of the state visitation probability.
improves exploration capability through entropy regulariza- In this scenario, we configure sparse rewards, meaning that
tion, the disparity between its employed Gaussian policy and the agents receive rewards only upon making contact with
the assumed theoretical Boltzmann policy leads to local op- all four targets. Besides, the agents need to cooperate in
tima, thereby constraining exploration. Notably, despite being exploration to reach the target, and the dispersion of targets
tailoredforMARLexploration,CMAVENstrugglesconsider- often leads to agents adopting one single policy, underscoring
ably in continuous action spaces, resulting in underwhelming the necessity for multimodal policies and cooperative explo-
performance. In contrast, our proposed method, employing ration. In Fig. 5(b), our method learns a multimodal jointJOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 9
TABLEII
PERFORMANCE(HIGHERISBETTER)ANDTIMEEFFICIENCY(LOWERISBETTER)COMPARISON.WECOMPARECPEG,HATD3,AND
DIFFUSION-BASEDMETHODSINTHREESPARSE-REWARDSCENARIOS,WHERETIMEDENOTESTHEDURATIONOFASINGLEITERATION.
Performance ↑ Time (ms) ↓
Scenarios
HATD3 CPEG CPEG-DM CPEG-DPM HATD3 CPEG CPEG-DM CPEG-DPM
Navigation 8.9±1.3 19.3±0.3 25.9±1.8 20.5±1.1 21 48 316 166
HalfCheetah (2x3) 805.0±12.6 965.0±8.0 943.6±7.2 914.3±7.4 12 37 247 129
Humanoid (17x1) 902.8±4.7 1143.6±9.1 991.5±8.4 878.9±9.5 155 249 2133 1645
2 2
1 1
0 0
1 1
(a) (b)
2 2
4 2 0 2 4 3 2 1 0 1 3 2 1 0 1 2 3 2 1 0 1 2 3
(a) Navigation (b) HalfCheetah (2x3)
Fig. 7. Visualization of the observation embeddings. We choose the last
30k stepsforvisualization.Thecolorsdenotedifferentconsensuses.
(c) (d) (e)
20.0
17.5
Fig.5. ExampleresultsobtainedbyCPEGandbaselinesonexploration
visitation. (a) shows a Reacher4 task, with four targets in different colors. 15.0
(b)-(d)showsthestatevisitationofCPEG,HASACandMAPPO.(e)counts
12.5
thetargetscoveredbypoliciesduringexploration.
10.0
CPEG (ours)
7.5
CPEG-w/o-CP
5.0
CPEG-w/o-CG
2.5 CPEG-w/o-SR
0.0
0.0 0.4 0.8 1.2 1.6 2.0
Timesteps(M)
A B C
Fig.8. Theablationperformanceofthethreekeycomponentsinourmethod
Fig. 6. Trajectory visualization in HalfCheetah (2x3). Different colors
underthesparseNavigationscenario.
representthecorrespondingconsensusbetweenagents.
they converge to a common consensus (the same color). This
policy, enabling the effector to reach four different targets
indicates that the consensus effectively reflects the global
relatively uniformly, without converging to a unimodal policy.
state, enabling multiple agents to coordinate their actions.
Conversely, in Fig. 5(c) and (d), baselines primarily focus
By leveraging this shared consensus, multiple agents can
on visiting certain targets, thereby limiting exploration in the
cooperatemoreeffectively,therebyenhancingtheirexploration
complex environment. Additionally, we quantify the targets
oftheenvironment.Additionally,wepresentthevisualizations
covered by the policy during exploration in Fig. 5(e). The
obtained by applying the PCA technique to the observation
curvesindicatethatourmethodachievesastatevisitationrate
embeddings. The goal is to show whether the consensus
of 83%, compared to 62% for HASAC, reflecting a 20% im-
learner can deduce distinguishable consensus from the local
provement.TheseresultsdemonstratethatCPEGcanfacilitate
observations, aiding in the guidance of policy during the
a more efficient exploration of the four different targets in the
exploration process. As shown in Fig. 7, the visualizations
scenario, highlighting the importance of multimodal policies
demonstrate that the consensus learner can generate informa-
and cooperative exploration capabilities among agents.
tive discrete representations by identifying potential correla-
In order to gain a better understanding of the consensus tions within individual observations of agents.
learner within CPEG, we generate visualizations of consensus
guidance and observation embeddings. In Fig. 6, we visualize
E. Ablation Study
the true trajectories generated by the consensus-guided con-
sistency policy in the HalfCheetah(2x3) scenario. The In this subsection, we conduct ablation studies in
trajectories are color-coded to represent the corresponding Navigationscenariotoinvestigatetheimpactofthreemain
consensus generated during execution. Across stages A, B, components in CPEG: (1) replacing the consistency policy
and C, although the observations of the two agents differ, with a deterministic policy (similar to MADDPG), while
draweR
egarevAJOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 10
incorporating consensus guidances and observations directly [7] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch,
as the inputs (CPEG-w/o-CP), (2) removing the consensus “Multi-agent actor-critic for mixed cooperative-competitive environ-
ments,”inAdvancesinNeuralInformationProcessingSystems,2017.
learner module, forcing agents to rely solely on their own
[8] J.G.Kuba,R.Chen,M.Wen,Y.Wen,F.Sun,J.Wang,andY.Yang,
observations for exploration during training (unguided explo- “Trustregionpolicyoptimisationinmulti-agentreinforcementlearning,”
ration)andachievingobjectivesduringexecution(CPEG-w/o- inInternationalConferenceonLearningRepresentations,2022.
CG), and (3) removing the self-reference mechanism, which [9] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu,
“ThesurprisingeffectivenessofPPOincooperativemulti-agentgames,”
relaxes the constraint on consistency policies (CPEG-w/o-
in Thirty-sixth Conference on Neural Information Processing Systems
SR). The experimental results, as shown in Fig. 8, reveal DatasetsandBenchmarksTrack,2022.
that CPEG-w/o-CP quickly converges to a locally optimal [10] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine, “Stabilizing off-
policy q-learning via bootstrapping error reduction,” in Advances in
policy. This result indicates that the consistency policy sig-
NeuralInformationProcessingSystems,2019.
nificantly enhances the exploration, preventing agents from [11] J.Ren,Y.Li,Z.Ding,W.Pan,andH.Dong,“Probabilisticmixture-of-
converging to local optima. Additionally, CPEG outperforms expertsforefficientdeepreinforcementlearning,”arXivpreprintarXiv:
2104.09122,2021.
CPEG-w/o-CG, which does not use consensus guidance, in
[12] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and
both convergence rate and final performance. Similarly, when B. Poole, “Score-based generative modeling through stochastic differ-
using only consensus guidance, CPEG-w/o-SR exhibits a lack entialequations,”inInternationalConferenceonLearningRepresenta-
tions,2021.
of exploitation of past successes, reducing sample efficiency
[13] Z.Wang,J.J.Hunt,andM.Zhou,“Diffusionpoliciesasanexpressive
and leading to performance degradation. Overall, the results
policyclassforofflinereinforcementlearning,”inInternationalConfer-
indicate that these elements collectively contribute to the enceonLearningRepresentations,2023.
improved performance of our method. [14] B.Kang,X.Ma,C.Du,T.Pang,andS.Yan,“Efficientdiffusionpolicies
for offline reinforcement learning,” in Advances in Neural Information
ProcessingSystems,2023.
VI. CONCLUSIONSANDFUTUREWORK [15] H. Li, Y. Zhang, H. Wen, Y. Zhu, and D. Zhao, “Stabilizing diffusion
model for robotic control with dynamic programming and transition
Inthispaper,weproposeCPEG,anovelmulti-agentexplo- feasibility,”IEEETransactionsonArtificialIntelligence,vol.1,no.01,
rationmethodwithaconsensus-guidedconsistencypolicythat pp.1–11,2024.
[16] T.Wang,J.Wang,Y.Wu,andC.Zhang,“Influence-basedmulti-agent
facilitates cooperative exploration and a self-reference mecha-
exploration,”inInternationalConferenceonLearningRepresentations,
nism that constrains the generated actions. To the best of our 2020.
knowledge, our study represents the first effort in leveraging [17] I.-J.Liu,U.Jain,R.A.Yeh,andA.Schwing,“Cooperativeexploration
for multi-agent deep reinforcement learning,” in International Confer-
consistency policies in MARL. The empirical results from six
enceonMachineLearning,2021.
environmentsdemonstratethatCPEGperformscomparablyto
[18] G. Hu, Y. Zhu, D. Zhao, M. Zhao, and J. Hao, “Event-triggered
baseline algorithms in dense reward settings while in sparse communication network with limited-bandwidth constraint for multi-
reward settings, which pose more challenges for exploration, agent reinforcement learning,” IEEE Transactions on Neural Networks
andLearningSystems,vol.34,no.8,pp.3966–3978,2023.
our algorithm outperforms baselines by 20%.
[19] Z. Xu, B. Zhang, D. Li, Z. Zhang, G. Zhou, and G. Fan, “Consensus
This work is the first to showcase the advantages of learning for cooperative multi-agent reinforcement learning,” in AAAI
consensus-guided consistency policies in the realm of multi- ConferenceonArtificialIntelligence,2022.
[20] J. Ruan, X. Hao, D. Li, and H. Mao, “Learning to collaborate by
agent exploration. On the other hand, the present work is
grouping: A consensus-oriented strategy for multi-agent reinforcement
confined to exploration within single-task scenarios. In the learning,”inEuropeanConferenceonArtificialIntelligence,2023.
future,weintendtostudyhowtobetterharnessthemultimodal [21] A. Van Den Oord, O. Vinyals et al., “Neural discrete representation
learning,”inAdvancesinNeuralInformationProcessingSystems,2017.
nature of consistency policies to devise a faster and more
[22] V. Zangirolami and M. Borrotti, “Dealing with uncertainty: Balancing
efficient algorithm for multi-task MARL.
exploration and exploitation in deep recurrent reinforcement learning,”
Knowledge-BasedSystems,vol.293,p.111663,2024.
[23] B. Peng, T. Rashid, C. Schroeder de Witt, P.-A. Kamienny, P. Torr,
REFERENCES
W. Bo¨hmer, and S. Whiteson, “FACMAC: Factored multi-agent cen-
tralisedpolicygradients,”inAdvancesinNeuralInformationProcessing
[1] G. Hu, H. Li, S. Liu, Y. Zhu, and D. Zhao, “NeuronsMAE: a novel
Systems,2021.
multi-agent reinforcement learning environment for cooperative and
[24] A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson, “MAVEN:
competitive multi-robot tasks,” in International Joint Conference on
Multi-agent variational exploration,” Advances in Neural Information
NeuralNetworks. IEEE,2023,pp.1–8.
ProcessingSystems,2019.
[2] Q. Zhang, Y. Gao, Y. Zhang, Y. Guo, D. Ding, Y. Wang, P. Sun, and
D. Zhao, “Trajgen: Generating realistic and diverse trajectories with [25] Z. Zhu, H. Zhao, H. He, Y. Zhong, S. Zhang, Y. Yu, and W. Zhang,
reactive and feasible agent behaviors for autonomous driving,” IEEE “Diffusionmodelsforreinforcementlearning:Asurvey,”arXivpreprint
TransactionsonIntelligentTransportationSystems,vol.23,no.12,pp. arXiv:2311.01223,2023.
24474–24487,2022. [26] A. Ajay, Y. Du, A. Gupta, J. B. Tenenbaum, T. S. Jaakkola, and
[3] J. Hao, T. Yang, H. Tang, C. Bai, J. Liu, Z. Meng, P. Liu, and P.Agrawal,“Isconditionalgenerativemodelingallyouneedfordecision
Z. Wang, “Exploration in deep reinforcement learning: From single- making?” in International Conference on Learning Representations,
agent to multiagent domain,” IEEE Transactions on Neural Networks 2023.
andLearningSystems,pp.1–21,2023. [27] S. Venkatraman, S. Khaitan, R. T. Akella, J. Dolan, J. Schneider, and
[4] B.Mazoure,T.Doan,A.Durand,J.Pineau,andR.D.Hjelm,“Lever- G. Berseth, “Reasoning with latent diffusion in offline reinforcement
aging exploration in off-policy algorithms via normalizing flows,” in learning,” in International Conference on Learning Representations,
ConferenceonRobotLearning,2020. 2024.
[5] Z.Huang,L.Liang,Z.Ling,X.Li,C.Gan,andH.Su,“Reparameterized [28] M.Janner,Y.Du,J.Tenenbaum,andS.Levine,“Planningwithdiffusion
policylearningformultimodaltrajectoryoptimization,”inInternational forflexiblebehaviorsynthesis,”inInternationalConferenceonMachine
ConferenceonMachineLearning,2023. Learning,2022.
[6] A.Sridhar,D.Shah,C.Glossop,andS.Levine,“NoMaD:Goalmasked [29] Z. Zhu, M. Liu, L. Mao, B. Kang, M. Xu, Y. Yu, S. Ermon, and
diffusion policies for navigation and exploration,” in NeurIPS 2023 W. Zhang, “MADiff: Offline multi-agent learning with diffusion mod-
FoundationModelsforDecisionMakingWorkshop,2023. els,”arXivpreprintarXiv:2305.17330,2023.JOURNALOFLATEXCLASSFILES,VOL.1,NO.1,NOVEMBER2024 11
[30] Z.Li,L.Pan,andL.Huang,“Beyondconservatism:Diffusionpolicies
in offline multi-agent reinforcement learning,” arXiv preprint arXiv:
2307.01472,2023.
[31] J.Chai,Y.Zhu,andD.Zhao,“NVIF:Neighboringvariationalinforma-
tionflowforcooperativelarge-scalemultiagentreinforcementlearning,”
IEEETransactionsonNeuralNetworksandLearningSystems,pp.1–13,
2023.
[32] J.Chai,Y.Fu,D.Zhao,andY.Zhu,“Aligningcreditformulti-agentco-
operationviamodel-basedcounterfactualimagination,”inInternational
Conference on Autonomous Agents and Multiagent Systems, 2024, pp.
281–289.
[33] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, “DPM-solver: A
fastODEsolverfordiffusionprobabilisticmodelsamplinginaround10
steps,”inAdvancesinNeuralInformationProcessingSystems,2022.
[34] Y. Chen, H. Li, and D. Zhao, “Boosting continuous control with
consistencypolicy,”inInternationalConferenceonAutonomousAgents
andMultiagentSystems,2024.
[35] Z. Ding and C. Jin, “Consistency models as a rich and efficient
policyclassforreinforcementlearning,”inInternationalConferenceon
LearningRepresentations,2024.
[36] Y.Song,P.Dhariwal,M.Chen,andI.Sutskever,“Consistencymodels,”
inInternationalConferenceonMachineLearning,2023.
[37] F.A.Oliehoek,C.Amatoetal.,Aconciseintroductiontodecentralized
POMDPs. Springer,2016,vol.1.
[38] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-
mation error in actor-critic methods,” in International Conference on
MachineLearning,2018.
[39] Y. Bengio, N. Le´onard, and A. Courville, “Estimating or propagating
gradientsthroughstochasticneuronsforconditionalcomputation,”arXiv
preprintarXiv:1308.3432,2013.
[40] C.Chen,R.Karunasena,T.H.Nguyen,A.Sinha,andP.Varakantham,
“Generativemodellingofstochasticactionswitharbitraryconstraintsin
reinforcementlearning,”inAdvancesinNeuralInformationProcessing
Systems,2023.
[41] J. Oh, Y. Guo, S. Singh, and H. Lee, “Self-imitation learning,” in
InternationalConferenceonMachineLearning,2018.
[42] Y. Tang, “Self-imitation learning via generalized lower bound q-
learning,”inAdvancesinNeuralInformationProcessingSystems,2020.
[43] Y. Zhong, J. G. Kuba, X. Feng, S. Hu, J. Ji, and Y. Yang,
“Heterogeneous-agent reinforcement learning,” Journal of Machine
LearningResearch,vol.25,no.32,pp.1–67,2024.
[44] M.Wen,J.G.Kuba,R.Lin,W.Zhang,Y.Wen,J.Wang,andY.Yang,
“Multi-agent reinforcement learning is a sequence modeling problem,”
inAdvancesinNeuralInformationProcessingSystems,2022.
[45] Y. Guo, J. Choi, M. Moczulski, S. Feng, S. Bengio, M. Norouzi, and
H.Lee,“Memorybasedtrajectory-conditionedpoliciesforlearningfrom
sparserewards,”inAdvancesinNeuralInformationProcessingSystems,
2020.
[46] Z. Zhang, Y. Liang, Y. Wu, and F. Fang, “MESA: Cooperative meta-
explorationinmulti-agentlearningthroughexploitingstate-actionspace
structure,” in International Conference on Autonomous Agents and
MultiagentSystems,2024,pp.2085–2093.
[47] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in
AdvancesinNeuralInformationProcessingSystems,2017.