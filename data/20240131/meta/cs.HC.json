[
    {
        "title": "Beyond Automated Evaluation Metrics: Evaluating Topic Models On Practical Social Science Content Analysis Tasks",
        "authors": "Zongxia LiAndrew MaoDaniel StephensPranav GoelEmily WalpoleAlden DimaJuan FungJordan Boyd-Graber",
        "links": "http://arxiv.org/abs/2401.16348v1",
        "entry_id": "http://arxiv.org/abs/2401.16348v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16348v1",
        "summary": "Topic models are a popular tool for understanding text collections, but their\nevaluation has been a point of contention. Automated evaluation metrics such as\ncoherence are often used, however, their validity has been questioned for\nneural topic models (NTMs) and can overlook the benefits of a model in real\nworld applications. To this end, we conduct the first evaluation of neural,\nsupervised and classical topic models in an interactive task based setting. We\ncombine topic models with a classifier and test their ability to help humans\nconduct content analysis and document annotation. From simulated, real user and\nexpert pilot studies, the Contextual Neural Topic Model does the best on\ncluster evaluation metrics and human evaluations; however, LDA is competitive\nwith two other NTMs under our simulated experiment and user study results,\ncontrary to what coherence scores suggest. We show that current automated\nmetrics do not provide a complete picture of topic modeling capabilities, but\nthe right choice of NTMs can be better than classical models on practical\ntasks.",
        "updated": "2024-01-29 17:54:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16348v1"
    },
    {
        "title": "Momentary Stressor Logging and Reflective Visualizations: Implications for Stress Management with Wearables",
        "authors": "Sameer NeupaneMithun SahaNasir AliTimothy HnatShahin Alan SamieiAnandatirtha NandugudiDavid M. AlmeidaSantosh Kumar",
        "links": "http://dx.doi.org/10.1145/3613904.3642662",
        "entry_id": "http://arxiv.org/abs/2401.16307v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16307v1",
        "summary": "Commercial wearables from Fitbit, Garmin, and Whoop have recently introduced\nreal-time notifications based on detecting changes in physiological responses\nindicating potential stress. In this paper, we investigate how these new\ncapabilities can be leveraged to improve stress management. We developed a\nsmartwatch app, a smartphone app, and a cloud service, and conducted a 100-day\nfield study with 122 participants who received prompts triggered by\nphysiological responses several times a day. They were asked whether they were\nstressed, and if so, to log the most likely stressor. Each week, participants\nreceived new visualizations of their data to self-reflect on patterns and\ntrends. Participants reported better awareness of their stressors, and\nself-initiating fourteen kinds of behavioral changes to reduce stress in their\ndaily lives. Repeated self-reports over 14 weeks showed reductions in both\nstress intensity (in 26,521 momentary ratings) and stress frequency (in 1,057\nweekly surveys).",
        "updated": "2024-01-29 17:08:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16307v1"
    },
    {
        "title": "\"You tell me\": A Dataset of GPT-4-Based Behaviour Change Support Conversations",
        "authors": "Selina MeyerDavid Elsweiler",
        "links": "http://arxiv.org/abs/2401.16167v1",
        "entry_id": "http://arxiv.org/abs/2401.16167v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16167v1",
        "summary": "Conversational agents are increasingly used to address emotional needs on top\nof information needs. One use case of increasing interest are counselling-style\nmental health and behaviour change interventions, with large language model\n(LLM)-based approaches becoming more popular. Research in this context so far\nhas been largely system-focused, foregoing the aspect of user behaviour and the\nimpact this can have on LLM-generated texts. To address this issue, we share a\ndataset containing text-based user interactions related to behaviour change\nwith two GPT-4-based conversational agents collected in a preregistered user\nstudy. This dataset includes conversation data, user language analysis,\nperception measures, and user feedback for LLM-generated turns, and can offer\nvaluable insights to inform the design of such systems based on real\ninteractions.",
        "updated": "2024-01-29 13:54:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16167v1"
    },
    {
        "title": "Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers",
        "authors": "Amr GomaaGuillermo ReyesMichael FeldAntonio Krüger",
        "links": "http://arxiv.org/abs/2401.16123v1",
        "entry_id": "http://arxiv.org/abs/2401.16123v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16123v1",
        "summary": "The rapid advancement of the automotive industry towards automated and\nsemi-automated vehicles has rendered traditional methods of vehicle\ninteraction, such as touch-based and voice command systems, inadequate for a\nwidening range of non-driving related tasks, such as referencing objects\noutside of the vehicle. Consequently, research has shifted toward gestural\ninput (e.g., hand, gaze, and head pose gestures) as a more suitable mode of\ninteraction during driving. However, due to the dynamic nature of driving and\nindividual variation, there are significant differences in drivers' gestural\ninput performance. While, in theory, this inherent variability could be\nmoderated by substantial data-driven machine learning models, prevalent\nmethodologies lean towards constrained, single-instance trained models for\nobject referencing. These models show a limited capacity to continuously adapt\nto the divergent behaviors of individual drivers and the variety of driving\nscenarios. To address this, we propose \\textit{IcRegress}, a novel\nregression-based incremental learning approach that adapts to changing behavior\nand the unique characteristics of drivers engaged in the dual task of driving\nand referencing objects. We suggest a more personalized and adaptable solution\nfor multimodal gestural interfaces, employing continuous lifelong learning to\nenhance driver experience, safety, and convenience. Our approach was evaluated\nusing an outside-the-vehicle object referencing use case, highlighting the\nsuperiority of the incremental learning models adapted over a single trained\nmodel across various driver traits such as handedness, driving experience, and\nnumerous driving conditions. Finally, to facilitate reproducibility, ease\ndeployment, and promote further research, we offer our approach as an\nopen-source framework at \\url{https://github.com/amrgomaaelhady/IcRegress}.",
        "updated": "2024-01-29 12:48:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16123v1"
    },
    {
        "title": "AccessLens: Auto-detecting Inaccessibility of Everyday Objects",
        "authors": "Nahyun KwonQian LuMuhammad Hasham QaziJoanne LiuChanghoon OhShu KongJeeeun Kim",
        "links": "http://dx.doi.org/10.1145/3613904.3642767",
        "entry_id": "http://arxiv.org/abs/2401.15996v1",
        "pdf_url": "http://arxiv.org/pdf/2401.15996v1",
        "summary": "In our increasingly diverse society, everyday physical interfaces often\npresent barriers, impacting individuals across various contexts. This\noversight, from small cabinet knobs to identical wall switches that can pose\ndifferent contextual challenges, highlights an imperative need for solutions.\nLeveraging low-cost 3D-printed augmentations such as knob magnifiers and\ntactile labels seems promising, yet the process of discovering unrecognized\nbarriers remains challenging because disability is context-dependent. We\nintroduce AccessLens, an end-to-end system designed to identify inaccessible\ninterfaces in daily objects, and recommend 3D-printable augmentations for\naccessibility enhancement. Our approach involves training a detector using the\nnovel AccessDB dataset designed to automatically recognize 21 distinct\nInaccessibility Classes (e.g., bar-small and round-rotate) within 6 common\nobject categories (e.g., handle and knob). AccessMeta serves as a robust way to\nbuild a comprehensive dictionary linking these accessibility classes to\nopen-source 3D augmentation designs. Experiments demonstrate our detector's\nperformance in detecting inaccessible objects.",
        "updated": "2024-01-29 09:27:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.15996v1"
    }
]