[
    {
        "title": "Synchformer: Efficient Synchronization from Sparse Cues",
        "authors": "Vladimir IashinWeidi XieEsa RahtuAndrew Zisserman",
        "links": "http://arxiv.org/abs/2401.16423v1",
        "entry_id": "http://arxiv.org/abs/2401.16423v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16423v1",
        "summary": "Our objective is audio-visual synchronization with a focus on 'in-the-wild'\nvideos, such as those on YouTube, where synchronization cues can be sparse. Our\ncontributions include a novel audio-visual synchronization model, and training\nthat decouples feature extraction from synchronization modelling through\nmulti-modal segment-level contrastive pre-training. This approach achieves\nstate-of-the-art performance in both dense and sparse settings. We also extend\nsynchronization model training to AudioSet a million-scale 'in-the-wild'\ndataset, investigate evidence attribution techniques for interpretability, and\nexplore a new capability for synchronization models: audio-visual\nsynchronizability.",
        "updated": "2024-01-29 18:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16423v1"
    },
    {
        "title": "Strategic Usage in a Multi-Learner Setting",
        "authors": "Eliot ShekhtmanSarah Dean",
        "links": "http://arxiv.org/abs/2401.16422v1",
        "entry_id": "http://arxiv.org/abs/2401.16422v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16422v1",
        "summary": "Real-world systems often involve some pool of users choosing between a set of\nservices. With the increase in popularity of online learning algorithms, these\nservices can now self-optimize, leveraging data collected on users to maximize\nsome reward such as service quality. On the flipside, users may strategically\nchoose which services to use in order to pursue their own reward functions, in\nthe process wielding power over which services can see and use their data.\nExtensive prior research has been conducted on the effects of strategic users\nin single-service settings, with strategic behavior manifesting in the\nmanipulation of observable features to achieve a desired classification;\nhowever, this can often be costly or unattainable for users and fails to\ncapture the full behavior of multi-service dynamic systems. As such, we analyze\na setting in which strategic users choose among several available services in\norder to pursue positive classifications, while services seek to minimize loss\nfunctions on their observations. We focus our analysis on realizable settings,\nand show that naive retraining can still lead to oscillation even if all users\nare observed at different times; however, if this retraining uses memory of\npast observations, convergent behavior can be guaranteed for certain loss\nfunction classes. We provide results obtained from synthetic and real-world\ndata to empirically validate our theoretical findings.",
        "updated": "2024-01-29 18:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16422v1"
    },
    {
        "title": "Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation",
        "authors": "Zhenyu HeGuhao FengShengjie LuoKai YangDi HeJingjing XuZhi ZhangHongxia YangLiwei Wang",
        "links": "http://arxiv.org/abs/2401.16421v1",
        "entry_id": "http://arxiv.org/abs/2401.16421v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16421v1",
        "summary": "In this work, we leverage the intrinsic segmentation of language sequences\nand design a new positional encoding method called Bilevel Positional Encoding\n(BiPE). For each position, our BiPE blends an intra-segment encoding and an\ninter-segment encoding. The intra-segment encoding identifies the locations\nwithin a segment and helps the model capture the semantic information therein\nvia absolute positional encoding. The inter-segment encoding specifies the\nsegment index, models the relationships between segments, and aims to improve\nextrapolation capabilities via relative positional encoding. Theoretical\nanalysis shows this disentanglement of positional information makes learning\nmore effective. The empirical results also show that our BiPE has superior\nlength extrapolation capabilities across a wide range of tasks in diverse text\nmodalities.",
        "updated": "2024-01-29 18:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16421v1"
    },
    {
        "title": "Semi-parametric Expert Bayesian Network Learning with Gaussian Processes and Horseshoe Priors",
        "authors": "Yidou WengFinale Doshi-Velez",
        "links": "http://arxiv.org/abs/2401.16419v1",
        "entry_id": "http://arxiv.org/abs/2401.16419v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16419v1",
        "summary": "This paper proposes a model learning Semi-parametric rela- tionships in an\nExpert Bayesian Network (SEBN) with linear parameter and structure constraints.\nWe use Gaussian Pro- cesses and a Horseshoe prior to introduce minimal nonlin-\near components. To prioritize modifying the expert graph over adding new edges,\nwe optimize differential Horseshoe scales. In real-world datasets with unknown\ntruth, we gen- erate diverse graphs to accommodate user input, addressing\nidentifiability issues and enhancing interpretability. Evalua- tion on\nsynthetic and UCI Liver Disorders datasets, using metrics like structural\nHamming Distance and test likelihood, demonstrates our models outperform\nstate-of-the-art semi- parametric Bayesian Network model.",
        "updated": "2024-01-29 18:57:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16419v1"
    },
    {
        "title": "Boolean Logic as an Error feedback mechanism",
        "authors": "Louis Leconte",
        "links": "http://arxiv.org/abs/2401.16418v1",
        "entry_id": "http://arxiv.org/abs/2401.16418v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16418v1",
        "summary": "The notion of Boolean logic backpropagation was introduced to build neural\nnetworks with weights and activations being Boolean numbers. Most of\ncomputations can be done with Boolean logic instead of real arithmetic, both\nduring training and inference phases. But the underlying discrete optimization\nproblem is NP-hard, and the Boolean logic has no guarantee. In this work we\npropose the first convergence analysis, under standard non-convex assumptions.",
        "updated": "2024-01-29 18:56:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16418v1"
    }
]