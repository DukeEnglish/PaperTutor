[
    {
        "title": "Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation",
        "authors": "Zhenyu HeGuhao FengShengjie LuoKai YangDi HeJingjing XuZhi ZhangHongxia YangLiwei Wang",
        "links": "http://arxiv.org/abs/2401.16421v1",
        "entry_id": "http://arxiv.org/abs/2401.16421v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16421v1",
        "summary": "In this work, we leverage the intrinsic segmentation of language sequences\nand design a new positional encoding method called Bilevel Positional Encoding\n(BiPE). For each position, our BiPE blends an intra-segment encoding and an\ninter-segment encoding. The intra-segment encoding identifies the locations\nwithin a segment and helps the model capture the semantic information therein\nvia absolute positional encoding. The inter-segment encoding specifies the\nsegment index, models the relationships between segments, and aims to improve\nextrapolation capabilities via relative positional encoding. Theoretical\nanalysis shows this disentanglement of positional information makes learning\nmore effective. The empirical results also show that our BiPE has superior\nlength extrapolation capabilities across a wide range of tasks in diverse text\nmodalities.",
        "updated": "2024-01-29 18:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16421v1"
    },
    {
        "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model",
        "authors": "Xiaoyi DongPan ZhangYuhang ZangYuhang CaoBin WangLinke OuyangXilin WeiSongyang ZhangHaodong DuanMaosong CaoWenwei ZhangYining LiHang YanYang GaoXinyue ZhangWei LiJingwen LiKai ChenConghui HeXingcheng ZhangYu QiaoDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2401.16420v1",
        "entry_id": "http://arxiv.org/abs/2401.16420v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16420v1",
        "summary": "We introduce InternLM-XComposer2, a cutting-edge vision-language model\nexcelling in free-form text-image composition and comprehension. This model\ngoes beyond conventional vision-language understanding, adeptly crafting\ninterleaved text-image content from diverse inputs like outlines, detailed\ntextual specifications, and reference images, enabling highly customizable\ncontent creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach\nthat applies additional LoRA parameters exclusively to image tokens to preserve\nthe integrity of pre-trained language knowledge, striking a balance between\nprecise vision understanding and text composition with literary talent.\nExperimental results demonstrate the superiority of InternLM-XComposer2 based\non InternLM2-7B in producing high-quality long-text multi-modal content and its\nexceptional vision-language understanding performance across various\nbenchmarks, where it not only significantly outperforms existing multimodal\nmodels but also matches or even surpasses GPT-4V and Gemini Pro in certain\nassessments. This highlights its remarkable proficiency in the realm of\nmultimodal understanding. The InternLM-XComposer2 model series with 7B\nparameters are publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.",
        "updated": "2024-01-29 18:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16420v1"
    },
    {
        "title": "Scaling Sparse Fine-Tuning to Large Language Models",
        "authors": "Alan AnsellIvan VulićHannah SterzAnna KorhonenEdoardo M. Ponti",
        "links": "http://arxiv.org/abs/2401.16405v1",
        "entry_id": "http://arxiv.org/abs/2401.16405v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16405v1",
        "summary": "Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with\ninstructions or human feedback) due to their sheer number of parameters. A\nfamily of parameter-efficient sparse fine-tuning (SFT) methods have proven\npromising in terms of performance but their memory requirements increase\nproportionally to the size of the LLMs. In this work, we scale sparse\nfine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. At any given\ntime, for a desired density level, we maintain an array of parameter indices\nand the deltas of these parameters relative to their pretrained values. We\niterate among: (a) updating the active deltas, (b) pruning indices (based on\nthe change of magnitude of their deltas) and (c) regrowth of indices. For\nregrowth, we explore two criteria based on either the accumulated gradients of\na few candidate parameters or their approximate momenta estimated using the\nefficient SM3 optimizer. We experiment with instruction-tuning of LLMs on\nstandard dataset mixtures, finding that SFT is often superior to popular\nparameter-efficient fine-tuning methods like LoRA (low-rank adaptation) in\nterms of performance and comparable in terms of run time. We additionally show\nthat SFT is compatible with both quantization and efficient optimizers, to\nfacilitate scaling to ever-larger model sizes. We release the code for SFT at\nhttps://github.com/AlanAnsell/peft and for the instruction-tuning experiments\nat https://github.com/ducdauge/sft-llm.",
        "updated": "2024-01-29 18:43:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16405v1"
    },
    {
        "title": "ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media Text",
        "authors": "Thanh-Nhi NguyenThanh-Phong LeKiet Van Nguyen",
        "links": "http://arxiv.org/abs/2401.16403v1",
        "entry_id": "http://arxiv.org/abs/2401.16403v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16403v1",
        "summary": "Lexical normalization, a fundamental task in Natural Language Processing\n(NLP), involves the transformation of words into their canonical forms. This\nprocess has been proven to benefit various downstream NLP tasks greatly. In\nthis work, we introduce Vietnamese Lexical Normalization (ViLexNorm), the\nfirst-ever corpus developed for the Vietnamese lexical normalization task. The\ncorpus comprises over 10,000 pairs of sentences meticulously annotated by human\nannotators, sourced from public comments on Vietnam's most popular social media\nplatforms. Various methods were used to evaluate our corpus, and the\nbest-performing system achieved a result of 57.74% using the Error Reduction\nRate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline.\nFor extrinsic evaluation, employing the model trained on ViLexNorm demonstrates\nthe positive impact of the Vietnamese lexical normalization task on other NLP\ntasks. Our corpus is publicly available exclusively for research purposes.",
        "updated": "2024-01-29 18:41:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16403v1"
    },
    {
        "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
        "authors": "Pratyush MainiSkyler SetoHe BaiDavid GrangierYizhe ZhangNavdeep Jaitly",
        "links": "http://arxiv.org/abs/2401.16380v1",
        "entry_id": "http://arxiv.org/abs/2401.16380v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16380v1",
        "summary": "Large language models are trained on massive scrapes of the web, which are\noften unstructured, noisy, and poorly phrased. Current scaling laws show that\nlearning from such data requires an abundance of both compute and data, which\ngrows with the size of the model being trained. This is infeasible both because\nof the large compute costs and duration associated with pre-training, and the\nimpending scarcity of high-quality data on the web. In this work, we propose\nWeb Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an\noff-the-shelf instruction-tuned model prompted to paraphrase documents on the\nweb in specific styles such as \"like Wikipedia\" or in \"question-answer format\"\nto jointly pre-train LLMs on real and synthetic rephrases. First, we show that\nusing WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training\nby $\\sim3x$. At the same pre-training compute budget, it improves perplexity by\nmore than 10% on average across different subsets of the Pile, and improves\nzero-shot question answer accuracy across 13 tasks by more than 2%. Second, we\ninvestigate the impact of the re-phrasing style on the performance of the\nmodel, offering insights into how the composition of the training data can\nimpact the performance of LLMs in OOD settings. Our gains are attributed to the\nfact that re-phrased synthetic data has higher utility than just real data\nbecause it (i) incorporates style diversity that closely reflects downstream\nevaluation style, and (ii) has higher 'quality' than web-scraped data.",
        "updated": "2024-01-29 18:19:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16380v1"
    }
]