[
    {
        "title": "Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation",
        "authors": "Zhenyu HeGuhao FengShengjie LuoKai YangDi HeJingjing XuZhi ZhangHongxia YangLiwei Wang",
        "links": "http://arxiv.org/abs/2401.16421v1",
        "entry_id": "http://arxiv.org/abs/2401.16421v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16421v1",
        "summary": "In this work, we leverage the intrinsic segmentation of language sequences\nand design a new positional encoding method called Bilevel Positional Encoding\n(BiPE). For each position, our BiPE blends an intra-segment encoding and an\ninter-segment encoding. The intra-segment encoding identifies the locations\nwithin a segment and helps the model capture the semantic information therein\nvia absolute positional encoding. The inter-segment encoding specifies the\nsegment index, models the relationships between segments, and aims to improve\nextrapolation capabilities via relative positional encoding. Theoretical\nanalysis shows this disentanglement of positional information makes learning\nmore effective. The empirical results also show that our BiPE has superior\nlength extrapolation capabilities across a wide range of tasks in diverse text\nmodalities.",
        "updated": "2024-01-29 18:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16421v1"
    },
    {
        "title": "Semi-parametric Expert Bayesian Network Learning with Gaussian Processes and Horseshoe Priors",
        "authors": "Yidou WengFinale Doshi-Velez",
        "links": "http://arxiv.org/abs/2401.16419v1",
        "entry_id": "http://arxiv.org/abs/2401.16419v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16419v1",
        "summary": "This paper proposes a model learning Semi-parametric rela- tionships in an\nExpert Bayesian Network (SEBN) with linear parameter and structure constraints.\nWe use Gaussian Pro- cesses and a Horseshoe prior to introduce minimal nonlin-\near components. To prioritize modifying the expert graph over adding new edges,\nwe optimize differential Horseshoe scales. In real-world datasets with unknown\ntruth, we gen- erate diverse graphs to accommodate user input, addressing\nidentifiability issues and enhancing interpretability. Evalua- tion on\nsynthetic and UCI Liver Disorders datasets, using metrics like structural\nHamming Distance and test likelihood, demonstrates our models outperform\nstate-of-the-art semi- parametric Bayesian Network model.",
        "updated": "2024-01-29 18:57:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16419v1"
    },
    {
        "title": "Boolean Logic as an Error feedback mechanism",
        "authors": "Louis Leconte",
        "links": "http://arxiv.org/abs/2401.16418v1",
        "entry_id": "http://arxiv.org/abs/2401.16418v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16418v1",
        "summary": "The notion of Boolean logic backpropagation was introduced to build neural\nnetworks with weights and activations being Boolean numbers. Most of\ncomputations can be done with Boolean logic instead of real arithmetic, both\nduring training and inference phases. But the underlying discrete optimization\nproblem is NP-hard, and the Boolean logic has no guarantee. In this work we\npropose the first convergence analysis, under standard non-convex assumptions.",
        "updated": "2024-01-29 18:56:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16418v1"
    },
    {
        "title": "ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift",
        "authors": "Hwanwoo KimXin ZhangJiwei ZhaoQinglong Tian",
        "links": "http://arxiv.org/abs/2401.16410v1",
        "entry_id": "http://arxiv.org/abs/2401.16410v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16410v1",
        "summary": "The presence of distribution shifts poses a significant challenge for\ndeploying modern machine learning models in real-world applications. This work\nfocuses on the target shift problem in a regression setting (Zhang et al.,\n2013; Nguyen et al., 2016). More specifically, the target variable y (also\nknown as the response variable), which is continuous, has different marginal\ndistributions in the training source and testing domain, while the conditional\ndistribution of features x given y remains the same. While most literature\nfocuses on classification tasks with finite target space, the regression\nproblem has an infinite dimensional target space, which makes many of the\nexisting methods inapplicable. In this work, we show that the continuous target\nshift problem can be addressed by estimating the importance weight function\nfrom an ill-posed integral equation. We propose a nonparametric regularized\napproach named ReTaSA to solve the ill-posed integral equation and provide\ntheoretical justification for the estimated importance weight function. The\neffectiveness of the proposed method has been demonstrated with extensive\nnumerical studies on synthetic and real-world datasets.",
        "updated": "2024-01-29 18:47:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16410v1"
    },
    {
        "title": "Is K-fold cross validation the best model selection method for Machine Learning?",
        "authors": "Juan M GorrizF SegoviaJ RamirezA OrtizJ. Suckling",
        "links": "http://arxiv.org/abs/2401.16407v1",
        "entry_id": "http://arxiv.org/abs/2401.16407v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16407v1",
        "summary": "As a technique that can compactly represent complex patterns, machine\nlearning has significant potential for predictive inference. K-fold\ncross-validation (CV) is the most common approach to ascertaining the\nlikelihood that a machine learning outcome is generated by chance and\nfrequently outperforms conventional hypothesis testing. This improvement uses\nmeasures directly obtained from machine learning classifications, such as\naccuracy, that do not have a parametric description. To approach a frequentist\nanalysis within machine learning pipelines, a permutation test or simple\nstatistics from data partitions (i.e. folds) can be added to estimate\nconfidence intervals. Unfortunately, neither parametric nor non-parametric\ntests solve the inherent problems around partitioning small sample-size\ndatasets and learning from heterogeneous data sources. The fact that machine\nlearning strongly depends on the learning parameters and the distribution of\ndata across folds recapitulates familiar difficulties around excess false\npositives and replication. The origins of this problem are demonstrated by\nsimulating common experimental circumstances, including small sample sizes, low\nnumbers of predictors, and heterogeneous data sources. A novel statistical test\nbased on K-fold CV and the Upper Bound of the actual error (K-fold CUBV) is\ncomposed, where uncertain predictions of machine learning with CV are bounded\nby the \\emph{worst case} through the evaluation of concentration inequalities.\nProbably Approximately Correct-Bayesian upper bounds for linear classifiers in\ncombination with K-fold CV is used to estimate the empirical error. The\nperformance with neuroimaging datasets suggests this is a robust criterion for\ndetecting effects, validating accuracy values obtained from machine learning\nwhilst avoiding excess false positives.",
        "updated": "2024-01-29 18:46:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16407v1"
    }
]