[
    {
        "title": "Computer Vision for Primate Behavior Analysis in the Wild",
        "authors": "Richard VoggTimo LüddeckeJonathan HenrichSharmita DeyMatthias NuskeValentin HasslerDerek MurphyJulia FischerJulia OstnerOliver SchülkePeter M. KappelerClaudia FichtelAlexander GailStefan TreueHansjörg ScherbergerFlorentin WörgötterAlexander S. Ecker",
        "links": "http://arxiv.org/abs/2401.16424v1",
        "entry_id": "http://arxiv.org/abs/2401.16424v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16424v1",
        "summary": "Advances in computer vision as well as increasingly widespread video-based\nbehavioral monitoring have great potential for transforming how we study animal\ncognition and behavior. However, there is still a fairly large gap between the\nexciting prospects and what can actually be achieved in practice today,\nespecially in videos from the wild. With this perspective paper, we want to\ncontribute towards closing this gap, by guiding behavioral scientists in what\ncan be expected from current methods and steering computer vision researchers\ntowards problems that are relevant to advance research in animal behavior. We\nstart with a survey of the state-of-the-art methods for computer vision\nproblems that are directly relevant to the video-based study of animal\nbehavior, including object detection, multi-individual tracking, (inter)action\nrecognition and individual identification. We then review methods for\neffort-efficient learning, which is one of the biggest challenges from a\npractical perspective. Finally, we close with an outlook into the future of the\nemerging field of computer vision for animal behavior, where we argue that the\nfield should move fast beyond the common frame-by-frame processing and treat\nvideo as a first-class citizen.",
        "updated": "2024-01-29 18:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16424v1"
    },
    {
        "title": "Synchformer: Efficient Synchronization from Sparse Cues",
        "authors": "Vladimir IashinWeidi XieEsa RahtuAndrew Zisserman",
        "links": "http://arxiv.org/abs/2401.16423v1",
        "entry_id": "http://arxiv.org/abs/2401.16423v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16423v1",
        "summary": "Our objective is audio-visual synchronization with a focus on 'in-the-wild'\nvideos, such as those on YouTube, where synchronization cues can be sparse. Our\ncontributions include a novel audio-visual synchronization model, and training\nthat decouples feature extraction from synchronization modelling through\nmulti-modal segment-level contrastive pre-training. This approach achieves\nstate-of-the-art performance in both dense and sparse settings. We also extend\nsynchronization model training to AudioSet a million-scale 'in-the-wild'\ndataset, investigate evidence attribution techniques for interpretability, and\nexplore a new capability for synchronization models: audio-visual\nsynchronizability.",
        "updated": "2024-01-29 18:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16423v1"
    },
    {
        "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model",
        "authors": "Xiaoyi DongPan ZhangYuhang ZangYuhang CaoBin WangLinke OuyangXilin WeiSongyang ZhangHaodong DuanMaosong CaoWenwei ZhangYining LiHang YanYang GaoXinyue ZhangWei LiJingwen LiKai ChenConghui HeXingcheng ZhangYu QiaoDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2401.16420v1",
        "entry_id": "http://arxiv.org/abs/2401.16420v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16420v1",
        "summary": "We introduce InternLM-XComposer2, a cutting-edge vision-language model\nexcelling in free-form text-image composition and comprehension. This model\ngoes beyond conventional vision-language understanding, adeptly crafting\ninterleaved text-image content from diverse inputs like outlines, detailed\ntextual specifications, and reference images, enabling highly customizable\ncontent creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach\nthat applies additional LoRA parameters exclusively to image tokens to preserve\nthe integrity of pre-trained language knowledge, striking a balance between\nprecise vision understanding and text composition with literary talent.\nExperimental results demonstrate the superiority of InternLM-XComposer2 based\non InternLM2-7B in producing high-quality long-text multi-modal content and its\nexceptional vision-language understanding performance across various\nbenchmarks, where it not only significantly outperforms existing multimodal\nmodels but also matches or even surpasses GPT-4V and Gemini Pro in certain\nassessments. This highlights its remarkable proficiency in the realm of\nmultimodal understanding. The InternLM-XComposer2 model series with 7B\nparameters are publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.",
        "updated": "2024-01-29 18:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16420v1"
    },
    {
        "title": "Endo-4DGS: Distilling Depth Ranking for Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting",
        "authors": "Yiming HuangBeilei CuiLong BaiZiqi GuoMengya XuHongliang Ren",
        "links": "http://arxiv.org/abs/2401.16416v1",
        "entry_id": "http://arxiv.org/abs/2401.16416v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16416v1",
        "summary": "In the realm of robot-assisted minimally invasive surgery, dynamic scene\nreconstruction can significantly enhance downstream tasks and improve surgical\noutcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to\nprominence for their exceptional ability to reconstruct scenes. Nonetheless,\nthese methods are hampered by slow inference, prolonged training, and\nsubstantial computational demands. Additionally, some rely on stereo depth\nestimation, which is often infeasible due to the high costs and logistical\nchallenges associated with stereo cameras. Moreover, the monocular\nreconstruction quality for deformable scenes is currently inadequate. To\novercome these obstacles, we present Endo-4DGS, an innovative, real-time\nendoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting\n(GS) and requires no ground truth depth data. This method extends 3D GS by\nincorporating a temporal component and leverages a lightweight MLP to capture\ntemporal Gaussian deformations. This effectively facilitates the reconstruction\nof dynamic surgical scenes with variable conditions. We also integrate\nDepth-Anything to generate pseudo-depth maps from monocular views, enhancing\nthe depth-guided reconstruction process. Our approach has been validated on two\nsurgical datasets, where it has proven to render in real-time, compute\nefficiently, and reconstruct with remarkable accuracy. These results underline\nthe vast potential of Endo-4DGS to improve surgical assistance.",
        "updated": "2024-01-29 18:55:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16416v1"
    },
    {
        "title": "A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect",
        "authors": "Yunkang CaoXiaohao XuJiangning ZhangYuqi ChengXiaonan HuangGuansong PangWeiming Shen",
        "links": "http://arxiv.org/abs/2401.16402v1",
        "entry_id": "http://arxiv.org/abs/2401.16402v1",
        "pdf_url": "http://arxiv.org/pdf/2401.16402v1",
        "summary": "Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the\nconcept of normality in visual data, widely applied across diverse domains,\ne.g., industrial defect inspection, and medical lesion detection. This survey\ncomprehensively examines recent advancements in VAD by identifying three\nprimary challenges: 1) scarcity of training data, 2) diversity of visual\nmodalities, and 3) complexity of hierarchical anomalies. Starting with a brief\noverview of the VAD background and its generic concept definitions, we\nprogressively categorize, emphasize, and discuss the latest VAD progress from\nthe perspective of sample number, data modality, and anomaly hierarchy. Through\nan in-depth analysis of the VAD field, we finally summarize future developments\nfor VAD and conclude the key findings and contributions of this survey.",
        "updated": "2024-01-29 18:41:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.16402v1"
    }
]