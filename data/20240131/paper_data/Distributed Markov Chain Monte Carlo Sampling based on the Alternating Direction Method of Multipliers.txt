Distributed MCMC Sampling via ADMM
Distributed Markov Chain Monte Carlo Sampling based on
the Alternating Direction Method of Multipliers
Alexandros E. Tzikas alextzik@stanford.edu
Department of Aeronautics and Astronautics
Stanford University
Stanford, CA 94305, USA
Licio Romao licio@stanford.edu
Department of Aeronautics and Astronautics
Stanford University
Stanford, CA 94305, USA
Mert Pilanci pilanci@stanford.edu
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
Alessandro Abate aabate@cs.ox.ac.uk
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, United Kingdom
Mykel J. Kochenderfer mykel@stanford.edu
Department of Aeronautics and Astronautics
Stanford University
Stanford, CA 94305, USA
Abstract
Many machine learning applications require operating on a spatially distributed dataset.
Despitetechnologicaladvances,privacyconsiderationsandcommunicationconstraintsmay
prevent gathering the entire dataset in a central unit. In this paper, we propose a dis-
tributed sampling scheme based on the alternating direction method of multipliers, which
is commonly used in the optimization literature due to its fast convergence. In contrast
to distributed optimization, distributed sampling allows for uncertainty quantification in
Bayesian inference tasks. We provide both theoretical guarantees of our algorithm’s con-
vergence and experimental evidence of its superiority to the state-of-the-art. For our the-
oretical results, we use convex optimization tools to establish a fundamental inequality on
the generated local sample iterates. This inequality enables us to show convergence of the
distribution associated with these iterates to the underlying target distribution in Wasser-
stein distance. In simulation, we deploy our algorithm on linear and logistic regression
tasks and illustrate its fast convergence compared to existing gradient-based methods.
Keywords: Markov chain Monte Carlo, distributed algorithms, sampling, alternating
direction method of multipliers, proximal operator
1
4202
naJ
92
]LM.tats[
1v83851.1042:viXraTzikas et al.
1 Introduction
Since the 1950s, with the foundational works by Dantzig (1963) and later developments in
the1980s(BertsekasandTsitsiklis,2015),variousresearchcommunitieshaverecognizedthe
importance of distributing computation to improve scalability. For example, the robotics
community has explored multi-robot simultaneous localization and planning, multi-robot
target tracking, and multi-robot task assignment (Shorinwa et al., 2023a). The machine
learningcommunityhasexploredfederatedlearning(Lietal.,2019)anddistributedtraining
of neural networks (Yu et al., 2022).
In order to select parameters in statistical models, optimization methods can be used to
generate point estimates that aim at maximizing a given performance metric. A different
approach for selecting parameters relies on a Bayesian treatment, whose goal is to obtain
samplesfromtheposteriordistributionontheparameterspace(Andrieuetal.,2003;Sekkat,
2022). Sampling methods constitute an important module in such a Bayesian paradigm,
becausetheyallowustoretainafullprobabilisticframeworkoftheuncertaintyaffectingour
statisticalmodel. SuchBayesianapproachesareusuallyemployedtoavoidoverfitting(Bhar
et al., 2023; Andrieu et al., 2003) and perform uncertainty quantification (Bhar et al., 2023;
Andrieu et al., 2003). Techniques based on Markov chain Monte Carlo (MCMC) (Andrieu
et al., 2003), variational inference (Blei et al., 2017), and Langevin dynamics (Welling and
Teh, 2011) have been proposed to perform sampling.
Inthispaper,weseekadistributedsamplingmechanismfortworeasons. First,thereare
applications in which the available data is spatially distributed, and collecting such data in
a central processing unit is not feasible due to privacy issues, communication constraints or
simply the size of the data. Second, the agents that hold the local, private data are usually
equippedwithcomputationalpower,thusenablinglocalcomputationsusinga(small)subset
of the entire dataset.
Our proposed sampling scheme leverages the consensus alternating direction method of
multipliers (ADMM), termed C-ADMM, presented by Mateos et al. (2010); Shorinwa et al.
(2023a,b), and Shi et al. (2014). A unique and crucial feature of the proposed sampling
scheme, which we refer to as the distributed ADMM-based sampler (D-ADMMS), is the
addition of a noise term in the proximal step of C-ADMM. As opposed to existing litera-
ture (Gu¨rbu¨zbalaban et al., 2021) that relies on gradient computations for the parameter
updates, the added noise and proximal updates of D-ADMMS are essential to its superior
convergence behavior. In summary, our main contributions are as follows:
• We show how to adapt C-ADMM to perform sampling in a distributed manner, by
designing a new distributed sampling algorithm, which is termed D-ADMMS.
• We develop a new analysis to show convergence of the distribution of the generated
iterates of D-ADMMS to the target distribution.
• We study the performance of the proposed scheme on regression tasks and discuss
advantages with respect to standard Langevin dynamics.
The remainder of the paper is structured as follows. In section 2, we review related
work. In section 3, we formulate the problem, while in section 4 we describe our proposed
approach. In section 5, we detail the convergence analysis of our proposed algorithm.
2Distributed MCMC Sampling via ADMM
Section 6 contains the numerical experiments. Finally, we conclude with closing remarks
and a discussion of future work in section 7.
2 Related Work
Langevin and Hamiltonian Gradient MCMC. When the goal is to sample from a
target distribution in Gibbs form (∝ exp−U(x)), discretizations of stochastic differential
equations (SDEs) are an attractive sampling technique because the stationary distribution
of these discretizations is usually close to the target distribution. Langevin algorithms are
MCMC methods based on the discretization of the overdamped Langevin diffusion, while
HamiltonianalgorithmsareMCMCmethodsbasedontheunderdampedLangevindiffusion.
Both Langevin and Hamiltonian methods scale well with high-dimensional sampling spaces
(Kungurtsev et al., 2023). They use first-order information of the target distribution to
guide the dynamics towards the relevant regions of the parameter space. The stationary
distribution of such algorithms, for example the unadjusted Langevin algorithm (ULA)
(Parayil et al., 2021), may contain a bias with respect to the stationary distribution of the
underlying SDE. To mitigate this bias, a Metropolis-Hastings correction can be introduced
attheexpenseofincreasingcomputation. Stochasticoptimizationtoolswerecombinedwith
Langevin dynamics by Welling and Teh (2011) to obtain a single-agent MCMC algorithm
that uses mini-batches of data at every iteration (to obtain gradient estimates), along with
a variable step-size and noise variance to guide convergence to the desired distribution.
These methods have recently been extended to the distributed setting, where the goal
(cid:80)
is to sample from a distribution, which is proportional to exp− f (x), in a network of
i∈V i
a set V of agents. The function f , for i ∈ V, is only known to the corresponding agent i.
i
Agents can communicate with their direct neighbors, as defined by the set of edges in the
communication graph. The methods that have been studied include: a modified version
of distributed stochastic gradient descent (Nedic and Ozdaglar, 2009), namely distributed
SGLD (D-SGLD) (Gu¨rbu¨zbalaban et al., 2021), and a method called distributed stochas-
tic gradient Hamiltonian Monte Carlo (D-SGHMC) (Gu¨rbu¨zbalaban et al., 2021), which is
an adaptation of the SGHMC method to the distributed setting. SGHMC can be faster
than SGLD, because it is based on the discretization of the underdamped inertial Langevin
diffusion, which converges to the stationary distribution faster than its overdamped coun-
terpart due to a momentum-based accelerating step. Gu¨rbu¨zbalaban et al. (2021) provide
convergence guarantees of the probability distribution of the local iterates of each agent to
the target distribution in terms of Wasserstein distance.
A distributed Hamiltonian Monte-Carlo algorithm with a Metropolis acceptance step
was recently derived (Kungurtsev et al., 2023). Each agent estimates both first-order and
second-order information of the global potential function of the target distribution, making
thisapproachdifferentfromours. TheULA,which is gradient-based, hasalsobeenmodified
for the distributed case, giving rise to the D-ULA scheme (Parayil et al., 2021). Assuming
conditional independence among the data, the posterior is given as a product of local
posteriors that are used to define the local dynamics. The distributed gradient-based ULA
(Parayil et al., 2021) has been modified to reduce the communication requirements between
the agents by Bhar et al. (2023). The difference is that the local iterate is not shared at
3Tzikas et al.
every iteration but only asynchronously by a triggering mechanism, which is based on the
iterate’s variation.
It should be evident from this discussion that most distributed sampling algorithms are
gradient-based. The optimization literature suggests that stochastic first-order methods are
usuallynotthefastesttoconverge(RyuandYin,2022),andcanbesensitivetothechoiceof
hyperparameters (Toulis et al., 2021). At the same time, C-ADMM offers fast convergence
in distributed optimization tasks (Shorinwa et al., 2023b). Motivated by this, we focus on
developing an ADMM-inspired distributed sampling scheme.
Distributed MCMC. In the case of large datasets, data is divided among agents.
Sampling from the global posterior in this case can be done using parallelized MCMC.
Neiswanger et al. (2014) develop an MCMC sampling framework where each processing
unit contains part of the dataset. Each agent deploys an MCMC method independently
(without communicating) to sample from the product term of the global posterior related
to its dataset. Then, by appropriately combining these individual samples, samples from
the global posterior can be obtained, in the spirit of a divide-and-conquer scheme. The
combination procedure nevertheless requires a central coordinator. An alternative approach
is to use several Markov chains. Ahn et al. (2014) propose such a distributed sampling
algorithm based on stochastic gradient Langevin dynamics (SGLD). However, it requires a
central coordinator.
MCMC and Federated Learning. The paradigm of SGLD has been used in the field
of federated learning (Deng et al., 2022) to convert optimization algorithms into sampling
algorithms. In federated learning, multiple agents aim to jointly optimize an objective
withoutsharingtheirprivatedata. Theagentscontainingtheprivatedatacancommunicate
with a centralized unit. Every iteration consists of a broadcast step, multiple local gradient
stepsbyeachagentusingitslocalfunction, andfinallyaconsensusstep. InspiredbySGLD,
the federated averaging optimization algorithm was modified into a gradient-based sampling
algorithmbyaddingnoiseinthelocalgradientupdates(Dengetal.,2022). Acombinationof
gradient-basedLangevindynamics andcompressiontechniquestoreducethecommunication
cost has been studied (Karagulyan and Richt´arik, 2023). Nevertheless, the algorithm is not
applicable in a distributed setting because it assumes a centralized processing unit.
A generalization of the federated averaging algorithm to compute the mode of the pos-
terior distribution has been explored (Al-Shedivat et al., 2020). The centralized processing
unit performs gradient steps on a suitable objective, while the agents employ a variant of
stochastic gradient MCMC in order to compute local covariances and expectations. Finally,
a distributed method that minimizes the Kullback-Leibler (KL) divergence with the data
likelihood function extends federated learning (Lalitha et al., 2019). It consists of a local
Bayesian update, a projection onto the allowed family of posteriors, and a consensus step.
Proximal Langevin Algorithms. ADMM has a proximal update at each iteration.
Theliteraturesuggeststhatproximaloperatorsaremorestablethansubgradients(Bauschke
and Combettes, 2011). Proximal optimization algorithms can be viewed as discretizations
of gradient flow differential equations, whose equilibria are the minimizers of the consid-
ered function (Parikh and Boyd, 2014). Analogously, although common forms of Langevin
MCMC methods employ subgradients, proximal MCMC methods possess favorable conver-
gence and efficiency properties (Durmus et al., 2018; Salim et al., 2019; Pereyra, 2016).
While the classical ULA is based on a forward Euler approximation of the Langevin SDE
4Distributed MCMC Sampling via ADMM
thathasthetargetdistributionasthestationarydistribution,proximalLangevinalgorithms
are based on discretizing an SDE whose stationary distribution equals the target distribu-
tion’s Moreau approximation (Pereyra, 2016). The regularity properties of the Moreau
approximation function lead to discrete approximations of the SDE with favourable sta-
bility and convergence qualities. To correct for the incurred error, a Metroplis-hastings
accept-reject step has been proposed (Pereyra, 2016). However, this algorithm requires
knowing the full energy function of the desired distribution at every iteration and therefore
it is not suitable for distributed computation.
A proximal stochastic Langevin algorithm has been proposed to sample from a distri-
(cid:80)
bution ∝ exp−U(x) with U(x) = F(x) + G (x), where F is smooth convex and G
i i i
are (possibly non-smooth) convex functions (Salim et al., 2019). The authors assume that
F(x) and G (x) can be written as expectations of functions f(x,ξ) and g (x,ξ), where ξ is
i i
a random variable. This allows the use of stochastic information on F(x) and G (x) when
i
designing the algorithm. At every iteration, a stochastic gradient step is taken with respect
to F(x) and Gaussian noise is added. Then, stochastic proximal operators of each G are
i
deployed sequentially. Although our problem fits in this problem formulation, the algorithm
by Salim et al. (2019) is not suitable for deployment in a distributed network setting, as it
requires the sequential deployment of the proximal operators.
Connections between ADMM, Sampling, and SDEs. ADMM is an optimization
method that has been developed to combine the robustness and convergence of the method
of multipliers and the decomposability of dual ascent (Boyd et al., 2011). Vono et al. (2019)
propose fast and efficient variations of the Gibbs sampler, based on the idea of variable
(cid:80)
splitting and variable augmentation, in order to sample from exp− f (x). The meth-
i i
ods employ surrogate distributions, which converge in the limit to the target distribution,
and consist of sequentially sampling a variable from its conditional distribution on the re-
maining ones. If, instead of sampling, we perform MAP optimization at each step of their
proposed algorithm, we recover the ADMM optimization method. Although this method is
closely related to our proposed scheme, it requires centralized communication at every step,
as discussed in Vono et al. (2019, appendix B). Other distributed sampling methods have
been proposed that are inspired by ADMM and the variable-splitting idea in optimization.
Rendell et al. (2020) introduce auxiliary variables and construct an MCMC algorithm on
an extended state space. Their algorithm can be partly deployed in a distributed manner
among agents, because the auxiliary variables are conditionally independent given the vari-
able of interest. The auxiliary variables can be independently sampled at each agent given
the variable of interest. The algorithm requires a centralized machine to sample the variable
of interest because the method consists of alternating between sampling the agent auxiliary
variables given the variable of interest and vice versa. An instance of this approach, the
split Gibbs sampler, has been studied by Vono et al. (2022).
Connections between ADMM and SDEs are made for the case of a stochastic optimiza-
tion problem where the sum of a function g(x) and the expected value of another function
f(x,ξ), where ξ is a random variable, is to be minimized (Zhou et al., 2020). At every
iteration, the primal variables are updated using a random sample of f, namely f(x,ξ ).
i
Zhouetal.(2020)provethattheprimaliterateconvergesasthestepsizedecreases, insome
sense, to the stochastic process satisfying a given SDE. Different from our formulation, the
5Tzikas et al.
only noise in the setting of Zhou et al. (2020) stems from the noisy sample of fat every
iteration.
3 Problem Formulation
We consider a network of agents. Each agent i possesses a local function f (x), where
i
x ∈ Rd. In machine learning applications, f (x) could pertain to the loss, as a function of
i
the model’s parameter x, on the agent i’s dataset. As such, f (x) is considered unknown
i
to all agents other than agent i. This is due to privacy considerations and the high cost of
transmittingagenti’sdataacrossthenetwork. Theoverallgoalistosampleinadistributed
manner from the distribution
(cid:88)
µ∗(x) ∝ exp−F(x), F(x) = f (x). (1)
i
i∈V
Suchalog-concavefunctionarisesastheposteriordistributioninvariousBayesianinference
problems, such as distributed Bayesian linear and logistic regression (Gu¨rbu¨zbalaban et al.,
2021).
The communication topology of the network is characterized by an undirected graph
G = (V,E), where V = {1,...,N}, for some integer N, is the set of agents, and E ⊂ V×V is
thesetofcommunicationlinks,i.e.,(i,j) ∈ E ifandonlyifi ̸= j andnodeicancommunicate
directly with node j. The neighborhood of agent i is denoted N = {j | (i,j) ∈ E}. The
i
cardinalityofN isdenotedN . ComplementarytotheundirectedgraphG, wealsodescribe
i i
the existing communication topology via a directed graph, G = (V,A). Every edge e ∈ E
d
is associated with two directed links in A that connect the same nodes as e. Therefore the
cardinality of A is twice that of E, i.e., |A| = 2|E|, and G describes the same topology as
d
G.
4 Description of the Proposed Method
In this section, we introduce our proposed method, D-ADMMS. We start by providing
background material on distributed optimization and C-ADMM. We then describe how we
modifyC-ADMM,whichisusedfordistributedoptimization,inordertoobtainD-ADMMS,
which performs distributed sampling.
4.1 Background on C-ADMM for Distributed Optimization
In distributed optimization problems, we consider the set-up introduced in section 3. How-
ever, in distributed optimization we aim to solve the optimization problem
(cid:88)
minimize f (x), (2)
i
x∈Rd
i∈V
instead of sample from eq. (1). We may introduce a local optimization variable, x for each
i
agent i, and consensus constraints in order to obtain the optimization problem
(cid:88)
minimize f (x )
i i
{z{ i,x ji }} (i i∈ ,jV )∈,
E
i∈V (3)
subject to x = x ∀(i,j) ∈ E.
i j
6Distributed MCMC Sampling via ADMM
If G is connected, x∗ is an optimal point of problem (2) if and only if x = x∗, ∀i ∈ V, is an
i
optimal point of problem (3). Problem (3) lends itself to a distributed treatment.
C-ADMM is a distributed optimization algorithm inspired by the method of multipliers,
whichcomputesaprimal-dualsolutionpairfortheoptimizationproblemviatheaugmented
Lagrangian: the primal variables x are updated as the minimizers of the augmented La-
i
grangian and the dual variables are updated via (dual) gradient ascent on the augmented
Lagrangian Shorinwa et al. (2023b). C-ADMM introduces auxiliary optimization variables
to problem (3) for each consensus constraint, which allows for distributed update steps. At
(k+1)
step (k+1), the primal variable of agent i, x , is updated according to
i

(cid:13) (k)
(k)(cid:13)2
x(k+1) ← argmin f (x)+p(k)T x+ρ (cid:88) (cid:13) (cid:13)x− x i +x j (cid:13) (cid:13)  , (4)
i i i (cid:13) 2 (cid:13)
x  (cid:13) (cid:13) 
j∈Ni 2
(k+1)
while the dual variable of agent i at step (k+1), p , which corresponds to the consensus
i
constraints involving agent i and its neighbors, is updated according to
(k+1) (k) (cid:88) (cid:16) (k+1) (k+1)(cid:17)
p ← p +ρ x −x , (5)
i i i j
j∈Ni
with initialization at zero.
4.2 The Proposed Method: D-ADMMS
Our distributed MCMC algorithm, D-ADMMS, is a modified version of the C-ADMM
optimization algorithm (Shorinwa et al., 2023b). In constrast to C-ADMM, which is a
distributed optimization algorithm, D-ADMMS is a distributed sampling algorithm. D-
ADMMS is given in Algorithm 1. A key feature of the proposed sampling scheme is the
added noise in the update of the primal variables. The MCMC sample corresponds to the
(k)
local primal variable x . At every iteration, each agent updates its local primal iterate by
i
solving a proximal problem. The primal update of D-ADMMS can equivalently be written
as
x(k+1)
= prox
(cid:40)
(cid:88)
x( ik) +x( jk)
+
√
2 w(k+1)
+
p( ik) (cid:27)
, (6)
i γifi 2N 2ρ i 2ρN
i i
j∈Ni
where γ = 2/(ρN ). Each agent then communicates its primal variable to its neighbors
i i
(k)
and updates its dual variable p based on the disagreement of the primal variables of the
i
neighboring agents. The inspiration of the added noise in the proximal step is derived from
the algorithm by Salim et al. (2019). The first step at each iteration of the algorithm by
Salim et al. (2019) consists of a noiseless gradient step and then a proximal update with
added noise. In D-ADMMS, the noiseless gradient step corresponds to the update of the
dual variables, while the primal variables are updated with a noisy proximal step. The
scaling of the noise involved is however different between the two algorithms.
5 Theoretical Analysis of the Proposed Method
(k)
Westudytheconvergenceofthedistributionassociatedwiththeprimaliterates,x ,ofthe
i
proposed algorithm, D-ADMMS, to the target distribution µ∗(x), in terms of 2-Wasserstein
7Tzikas et al.
Algorithm 1: Proposed Algorithm (D-ADMMS)
Initialization: k ← 0,
x(k)
∈
Rd,p(k)
= 0 ∀i ∈ V
i i
Parameters: ρ > 0
(k+1)
Output: samples x ∀i ∈ V
i
do in parallel ∀i ∈ V
(k+1)
w ∼ N(0,I)
i
 (cid:13) √ (cid:13)2
x(k+1) ← argmin  f (x)+p(k)T x+ρ(cid:80) (cid:13) (cid:13)x− x( ik)+x( jk) + 2 w(k+1)(cid:13) (cid:13) 
i x i i j∈Ni(cid:13) 2 2ρ i (cid:13)
 (cid:13) (cid:13) 
2
(k+1)
Communicate x to neighbors j ∈ N
i i
(k+1)
Receive x from neighbors j ∈ N
j i
(cid:16) (cid:17)
(k+1) (k) (cid:80) (k+1) (k+1)
p ← p +ρ x −x
i i j∈Ni i j
k ← k+1
distance. The 2-Wasserstein distance between two probability measures µ and ν with finite
second moments is defined as
(cid:18) (cid:19)1/2
W(µ,ν) = inf E ∥x−y∥2 , (7)
(x,y)∼τ
τ∈Γ(µ,ν)
where Γ(µ,ν) is the set of all couplings between µ and ν (Villani and Villani, 2009). We
adopt a convex analysis perspective. We base our analysis of the convergence rate (with
respect to the iterates’ Wasserstein distance to the target distribution) of D-ADMMS on
the analysis of the convergence rate (with respect to the iterates’ Euclidian distance to the
optimal point) of C-ADMM by Shi et al. (2014). Modifying the analysis by Shi et al. (2014)
for our purposes is not trivial for two reasons: i) the added noise in the primal update
of D-ADMMS gives rise to terms that do not exist in C-ADMM, and ii) it is not straight
forward how to obtain a Wasserstein distance bound on the distribution of the iterates from
a Euclidian distance bound of the iterates. We use ∥·∥ for the standard Euclidean norm
and ∥·∥2 for the G-matrix norm (·)TG(·).
G
This section is organized as follows. We start the first subsection by stating our as-
sumptions and introducing helpful quantities. We finish it with a lemma that shows the
equivalence of the D-ADMMS updates and a different set of updates, which involve the
same primal variables. This second set of updates is used in our analysis. In the second
subsection, we prove a recursive inequality for the Wasserstein distance between the dis-
tribution of the primal iterates of D-ADMMS and the target distribution of eq. (1). Our
main result is Theorem 3, which is included in the third subsection. Theorem 3 states that
there exists a decreasing upper-bound on the Wasserstein distance between the distribution
of the primal iterates of D-ADMMS and the target distribution of eq. (1), as the iterations
evolve.
8Distributed MCMC Sampling via ADMM
5.1 Assumptions, Definitions, and an Equivalent Expression of D-ADMMS
Assumption 1 The local objective functions f (x) are strongly convex: ∀x ,x ∈ Rd,∀i ∈
i a b
V, it holds that
⟨∇f (x )−∇f (x ),x −x ⟩ ≥ m ∥x −x ∥2, m > 0.
i a i b a b fi a b fi
Assumption 2 The gradients of the local objective functions are Lipschitz continuous:
∀x ,x ∈ Rd,∀i ∈ V, it holds that
a b
∥∇f (x )−∇f (x )∥ ≤ M ∥x −x ∥, M > 0.
i a i b fi a b fi
Assumption 3 The graph topology G is connected.
We further define the consensus convex optimization problem associated with µ∗ as
(cid:88)
minimize f (x )
i i
{z{ i,x ji }} (i i∈ ,jV )∈,
A
i∈V (8)
subject to x = z , x = z ∀(i,j) ∈ A.
i i,j j i,j
Concatenating each x in X ∈ RNd and all z in Z ∈ R|A|d, we may write the constraint
i i,j
of eq. (8) as
AX +BZ = 0. (9)
Here, A = [A ;A ], where A ,A ∈ R|A|d×Nd. If (i,j) ∈ A and z is the q-th block of Z,
1 2 1 2 i,j
then the (q,i) block of A and the (q,j) block of A are the the d×d identity matrices. All
1 2
(cid:2) (cid:3)
other blocks of A ,A contain zero entries. Also B = −I ;−I , where I is the
1 2 |A|d |A|d |A|d
(cid:80)
|A|d×|A|d identity matrix. We define f(X) = f (x ). By Assumptions 1 and 2, f(X)
i∈V i i
is strongly convex with constant m = min m and its gradients are Lipschitz continuous
f i fi
with constant M = max M .
f i fi
(cid:80)
Assumption 4 f (x) admits a (unique) minimizer.
i∈V i
Problem(8)thenadmitsauniquesolution(X∗,Z∗),becauseG isconnectedand(cid:80) f (x)
i∈V i
admits a unique minimizer. It is evident that the optimization problem (8) is related to our
problem of interest, which is to sample from distribution (1) in a distributed manner, but
solving problem (8) is not our objective. In our analysis, we use the minimizer of problem
(8) as a fixed point to which the Euclidian distance of the primal iterates of D-ADMMS can
be bounded. By assigning a point mass distribution to this fixed point, we are then able
to obtain relations for the Wasserstein distance of the primal iterates of D-ADMMS to the
target distribution.
We now introduce matrices M ,M ,L ,L , and D, based on the network topology G.
− + − +
M andM aretheextendedunorientedandorientedincidencematricesofG ,respectively.
+ − d
L and L are the extended signless and signed Laplacian matrices of G, respectively. D
+ −
is the extended degree matrix of G. By “extended” we mean the Kronecker product with
I . We also denote w(k) = (w(k) ,...,w(k) ) ∈ RNd, and p(k) = (p(k) ,...,p(k) ) ∈ RNd.
d 1 N 1 N
9Tzikas et al.
Lemma 1 Define β ∈ R|A|d. The update equations of D-ADMMS in Algorithm 1 can be
derived from the iterates
√ (cid:16) (cid:17)
∇f(X(k+1))+M β(k+1)+ 2Dw(k+1) = ρM Z(k)−Z(k+1) , (10)
− +
ρ
β(k+1)−β(k)− MTX(k+1) = 0, (11)
2 −
1
MTX(k)−Z(k) = 0, (12)
2 +
where X(k) is the concatenation of the x(k) from Algorithm 1.
i
Proof The proof is included in Appendix A.
By the lemma above, we may analyze the primal iterates of D-ADMMS using eq. (10-12).
The Karush-Kuhn-Tucker (KKT) conditions for problem (8) are
∇f(X∗)+M β∗ = 0, (13)
−
MTX∗ = 0, (14)
−
1
MTX∗−Z∗ = 0, (15)
2 +
as described by Shi et al. (2014), where β∗ denotes the unique optimal multiplier that exists
in the column space of MT. Since the equality constraints of problem (8) are feasible, by
−
Slater’scondition(BoydandVandenberghe,2004), thereexistsanoptimalmultiplierβ˜that
satisfies the KKT conditions. Its projection onto the column space ofMT is β∗, as analyzed
−
by Shi et al. (2014).
Assumption 5 β(0) is in the column space of MT.
−
By inspection of eq. (11), we observe that under Assumption 5, β(k) is in the column space
of MT for all k ≥ 0.
−
5.2 A Recursive Inequality of Convergence for D-ADMMS
We define U = (Z,β), G = diag{ρI , 1I }, and U∗ = (Z∗,β∗). We also denote the
2|E|d ρ 2|E|d
largest singular value and the smallest nonzero singular value of a matrix M, as σ (M),
max
and σ (M) respectively. We may now compute recursive bounds on U(k) and X(k). Then
min
we can obtain recursive bounds on the Wasserstein distance of the iterate. We denote µ
(·)
as the probability distribution of random variable (·). Also µ∗(X) = (cid:81)N µ∗(x ). Finally
i=1 i
W2(µ ,µ ) = ρW2(µ ,µ )+(1/ρ)W2(µ ,µ ).
G U(k) U∗ Z(k) Z∗ β(k) β∗
Lemma 2 Under Assumptions (1-5), for any κ > 1, there exists a δ > 0, such that the
distribution of the iterates of D-ADMMS satisfies the relation
1
W(µ X(k+1),µ∗) ≤ √
m
W G(µ U(k),µ U∗)+
f
√1 (cid:113) E(cid:0) ∥Dw(k+1)∥2(cid:1) +W(µ ,µ∗), (16)
2m
f
X∗−√ 21 mfDw(k+1)
10Distributed MCMC Sampling via ADMM
and W (µ ,µ ) is recursively upper-bounded by
G U(k) U∗
(cid:118)
W (µ ,µ ) ≤
√
aW (µ ,µ )+
E(cid:0)
y
√(k+1)(cid:1) +(cid:117)
(cid:117)
(cid:116)|
E(cid:0) r(k+1)(cid:1)
−(cid:32) E(cid:0)
y
√(k+1)(cid:1)(cid:33)2
|,
G U(k+1) U∗ G U(k) U∗ 2 a 2 a
(17)
where
b √ cd
y(k+1) = 2√ w¯(k+1)+cσ max(M −) ρ∥Dw(k+1)∥+ √ ∥Dw(k+1)∥, (18)
m m
f f
√
2b (cid:16) (cid:17)2 b
r(k+1) = w¯(k+1)∥Dw(k+1)∥+b w¯(k+1) + ∥Dw(k+1)∥2
m 2m2
f f
√
2cd
+ ∥Dw(k+1)∥2−e∥Dw(k+1)∥2, (19)
m
1 √ f
w¯(k+1) = ∥√ Dw(k+1)∥+∥ 2Dw(k+1)∥,
2m
f
(cid:40) (cid:41)
(κ−1)σ2 (M ) m
δ = min min − , f > 0, (20)
κσ m2 ax(M +) ρσ2 (M )+ κM f2
4 max + ρσ m2 in(M−)
and
√
2m +1 1 2 2δ
f
a = , b = , c = ,
2m (1+δ) 2(1+δ) (1+δ)σ2 (M )
f min −
ρσ2 (M ) 2δ
d = max − , e = . (21)
2 (1+δ)ρσ2 (M )
min −
Proof The proof is included in Appendix B.
5.3 Our Main Result
By inspection of eq. (17-21), if there exists a ρ > 0 such that δ satisfies 2m δ > 1, then
f
(k)
a < 1 and we have convergence in terms of Wasserstein distance for the primal iterates x .
i
This idea is formalized in the following theorem, whose result depends on the condition
number of f(X), τ = M f, and the condition number of the graph topology, τ = σmax(M+) .
f m f G σmin(M−)
(cid:113)
Theorem 3 Assume Assumptions (1-5) hold and τ−1 τ−2+4τ−2 −τ−2 > m−1 is true.
f f G f f
Then, there exists a ρ > 0 such that a < 1 and
W(µ X(k+1),µ∗) ≤ √ m1 (cid:0)√ a(cid:1)k W G(µ U0,µ U∗)+ √ a1
m
1−Y √ a+
f f
√
√ m1 f 1−R √ a + √ 21 m f(cid:113) E(cid:0) ∥Dw(k+1)∥2(cid:1) +W(µ X∗−√ 21 mfDw(k+1),µ∗),
Y and R are upper bounds on the terms
E(cid:0) y(l)(cid:1)
and
E(cid:0) r(l)(cid:1)
respectively, and Y,R ≥ 0
holds.
11Tzikas et al.
Proof The proof is included in Appendix C.
Because of the definitions L = 1M MT and L = 1M MT, we have that τ =
+ 2 + + − 2 − − G
(cid:113)
σmax(L+)
. σ (L )isknownasthegraph’salgebraicconnectivity(Shietal.,2014), while
σmin(L−) min −
σ (L )isrelatedtothenodedegreesofG (Cvetkovi´cetal.,2007). Assumingaring-cyclic
max +
graph topology of 5 agents, we have τ = 1.7. If we further assume m = 2, we get the
G f
sufficient condition for convergence: τ < 1.23. Increasing the connectedness of the graph,
f
for a fully connected graph of 5 agents, we have τ = 1.26. Then, for m = 2, the sufficient
√ G f
condition becomes τ < 6. We observe that as the graph becomes more connected,
f
τ decreases. Then, fixing τ and m , Theorem 1 implies that D-ADMMS converges if
G f f
the connectivity is above a certain threshold. In addition, a decreases with increasing
connectivity. Theconvergenceupperboundhoweveralsocontainsterms,includingY andR,
which depend on noise and topology characteristics and can increase as the graph becomes
more connected (D has larger values in its diagonal and a decreases).
(cid:113)
We also note that the sufficient convergence condition m−1 < τ−1 τ−2+4τ−2 −τ−2
f f f G f
is not symmetric with respect to the condition number τ . In other words, scaling the
f
functions f by a common constant, which does not change τ , modifies the convergence
i f
condition. This is because the scaling constant affects the solution of the proximal update
(k+1)
for x .
i
SinceADMMispracticallystablewhenthef arenotstronglyconvex, e.g., forindicator
i
functions, our algorithm is also applicable in this case. However, the theoretical analysis
above assumes strong convexity.
6 Simulation Results
In this section, we test D-ADMMS in simulation. We first discuss the existing baselines
from the literature and then provide simulation results for two different scenarios: Bayesian
linear regression and Bayesian logistic regression.1
6.1 Description of Baseline Algorithms
We compare our proposed algorithm against D-SGLD (Gu¨rbu¨zbalaban et al., 2021), D-
SGHMC (Gu¨rbu¨zbalaban et al., 2021), and D-ULA (Parayil et al., 2021). We let S be
a doubly stochastic matrix associated with the network’s communication topology, i.e., a
matrix whose (i,j)-entry, which we denote by S , is non-negative and less than one, and is
ij
different from zero if and only if the entry j ∈ N ; whenever j = i, we let S be defined as
i ii
(cid:80)
1− S .
j∈Ni ij
D-SGLD is characterized by
(k+1) (cid:88) (k) (k) (cid:112) (k+1) (k+1)
x = S x −η ∇f (x )+ 2η w , w ∼ N(0,I). (22)
i ij j DSGLD i i DSGLD i i
j∈Ni∪{i}
1. The source code for the presented experiments can be found in the following repository: https://
github.com/sisl/Distributed_ADMM_Sampler
12Distributed MCMC Sampling via ADMM
D-SGHMC proceeds with
(k+1) (k) (cid:16) (k) (k) (cid:17) (cid:112) (k+1) (k+1)
v = v −η γv +∇f (x ) + 2γη w , w ∼ N(0,I),
i i DSGHMC i i i DSGHMC i i
(23)
(k+1) (cid:88) (k) (k+1)
x = S x +η v . (24)
i ij j DSGHMC i
j∈Ni∪{i}
Finally, D-ULA evolves through the update
x(k+1) = x(k) −ζ(k) (cid:88) (cid:16) x(k) −x(k)(cid:17) −α(k)N∇f (x(k) )
i i i j i i
j∈Ni
(cid:112)
+
2α(k)w(k+1) ,w(k+1)
∼ N(0,NI). (25)
i i
6.2 Bayesian Linear Regression
6.2.1 Problem
We study the distributed Bayesian linear regression setting. We consider a varying number
of agents (N = 5,20,100) and a varying network topology (fully connected, ring-cyclic, and
no-edge) in our results. We assume that the model parameter is x ∈ R2, and we simulate
IID data points (zl,yl) for each agent through
δl ∼ N(0,ξ2I), zl ∼ N(0,I), yl = xTzl +δl. (26)
We assign ξ = 4. The prior on x is N(0,λI) with λ = 10. Each agent is assigned n
i
independent samples. The global posterior, from which we aim to sample, by a simple
(cid:80)
application of Bayes’ rule, is of the form π(x) ∝ exp− f (x), where
i∈V i
1
(cid:88)ni
(cid:16) (cid:17)2 1
f (x) = yl −xTzl + ∥x∥2. (27)
i 2ξ2 i i 2λN
l=1
6.2.2 Algorithm
We do not perform hyper-parameter tuning and assume η = 0.009, η =
DSGLD DSGHMC
0.1, γ = 7, as done by Gu¨rbu¨zbalaban et al. (2021), while ρ = 5. For D-ULA, we follow the
logistic regression example by Parayil et al. (2021) with modifications to avoid divergence.
We assume α(k) = 0.00082/(230+k)χ2, ζ(k) = 0.48/(230+k)χ1 We assume χ
1
= χ
2
= 0.05
for the cyclic and no-edge topologies. For the case of the fully connected graph: when
N = 5,20 we assume χ = 0.55 and χ = 0.05. Note that the selection of parameters
1 2
for D-ULA does not satisfy the conditions presented in the original paper (Parayil et al.,
2021), but the selected parameter values perform well experimentally. The algorithm by
Parayil et al. (2021) required the most testing to determine suitable parameters. Note that
(0)
x ∼ N(0,I), ∀i ∈ V for all algorithms. We also test two cases for n equal to 50 and 200.
i i
We tune the parameters independently for both values of n .
i
13Tzikas et al.
6.2.3 Results
We present results with respect to the 2-Wasserstein distance between the empirical distri-
(k)
butionoftheiteratesx andthetrueposteriorinFigures1and2forn = 50andn = 200
i i i
respectively. The true posterior and the iterate distributions are Gaussian (Gu¨rbu¨zbalaban
et al., 2021). Furthermore, the true posterior is known in closed form (Gu¨rbu¨zbalaban
et al., 2021). The Gaussian distribution of each agent’s iterate is estimated by empirically
computing the expectation and covariance through 100 independent trials. The same holds
(cid:80) (k)
for the average agent iterate x /N. The closed form of the 2-Wasserstein distance
i∈V i
between Gaussians, in the case of non-singular covariances, only involves the covariances
and means of the distributions (Givens and Shortt, 1984).
Figures 1 and 2 include the Wasserstein distance between the average iterate (avg) and
thetargetdistributionandtheWassersteindistancebetweentheiterateofanagent(ag)and
the target distribution. We observe that our proposed algorithm converges faster than the
other presented schemes (D-SGLD, D-ULA, D-SGHMC) in the cases of sparsely connected
network topology (ring-cyclic graph), while it can become slower for more connected graphs
(fully connected). This agrees with results in the optimization literature, which show that
C-ADMM can be slower as the communication topology becomes more connected (Bof
et al., 2018). In the case of a highly connected graph, the dual variables take a longer time
to converge because of the larger number of inter-agent disagreement terms involved, which
slows convergence. Although a in Theorem 1 decreases with increasing connectedness, the
theoretical upper bound need not decrease, because it includes terms that increase with
increased connectivity. Therefore, the experimental behavior is not necessarily misaligned
with the theoretical results.
(k)
We have also included the performance of ADMM (i.e., w = 0 in Algorithm 1) in
i
(k)
Figures 1 and 2. ADMM is an optimization scheme and hence it drives x to a point with
i
(k)
an optimal value for the associated optimization problem, for any initialization of x . We
i
observe that in the no-edge topology, ADMM performs exactly the same as D-ADMMS.
This is attributed to the last term of the primal update, which vanishes in the case of
a no-edge topology because no agent has any neighbors. The superior performance (in
Wasserstein distance) of ADMM compared to the sampling schemes for the case N = 100
can be attributed to the very small (≤ 10−3) entries of the true posterior covariance in this
case, because of the large number of samples present. This means that simply finding the
MAP (which is also the mean because of Gaussian structure) with ADMM is enough for a
small Wasserstein distance to the true posterior distribution. For N = 100, the inability of
CVXPY(DiamondandBoyd,2016)toconvergejustifiestheexclusionofthefullyconnected
case.
The fast convergence (in cases) of D-ADMMS is intuitively attributed to the proxi-
mal primal update of the algorithm. D-ADMMS is able to take large steps in terms of
Wasserstein distance in the initial iterations because its primal update consists of com-
pletely solving an optimization problem. Even in the case of no-edge topology, we observe
the large reduction of Wasserstein distance in the early iterations. Nevertheless, because
eachagentdoesnotgatherinformationfromotheragentsintheupdateofitsdualvariables,
the trajectory in the primal space remains uncorrected in this case. This leads D-ADMMS
to converge farther from the target distribution.
14Distributed MCMC Sampling via ADMM
Results for 5 agents
Fully connected Cyclic No edges
D-SGLD, ag D-SGLD, ag D-SGLD, ag
5 D D- -S UG LALD , a, gavg 5 D D- -S UG LALD , a, gavg 5 D D- -S UG LALD , a, gavg
D-ULA, avg D-ULA, avg D-ULA, avg
D-SGHMC, ag D-SGHMC, ag D-SGHMC, ag
4 D D- -ASG DH MM MC S, , a av gg 4 D D- -ASG DH MM MC S, , a av gg 4 D D- -ASG DH MM MC S, , a av gg
D-ADMMS, avg D-ADMMS, avg D-ADMMS, avg
ADMM, ag ADMM, ag ADMM, ag
3 ADMM, avg 3 ADMM, avg 3 ADMM, avg
2 2 2
1 1 1
0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Iteration Iteration Iteration
Results for 20 agents
Fully connected Cyclic No edges
3.0 3.0 4.0
D-SGLD, ag D-SGLD, ag D-SGLD, ag
D-SGLD, avg D-SGLD, avg D-SGLD, avg
D-ULA, ag D-ULA, ag 3.5 D-ULA, ag
2.5 D-ULA, avg 2.5 D-ULA, avg D-ULA, avg
D-SGHMC, ag D-SGHMC, ag D-SGHMC, ag
D-SGHMC, avg D-SGHMC, avg 3.0 D-SGHMC, avg
D-ADMMS, ag D-ADMMS, ag D-ADMMS, ag
2.0 D-ADMMS, avg 2.0 D-ADMMS, avg D-ADMMS, avg ADMM, ag ADMM, ag 2.5 ADMM, ag
ADMM, avg ADMM, avg ADMM, avg
1.5 1.5 2.0
1.5
1.0 1.0
1.0
0.5 0.5
0.5
0.0 0.0 0.0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Iteration Iteration Iteration
Results for 100 agents
Cyclic No edges
5 D D- -S SG GL LD D, , a ag vg 5 D D- -S SG GL LD D, , a ag vg
D-ULA, ag D-ULA, ag
D-ULA, avg D-ULA, avg
4 D-SGHMC, ag 4 D-SGHMC, ag
D-SGHMC, avg D-SGHMC, avg
D-ADMMS, ag D-ADMMS, ag
D-ADMMS, avg D-ADMMS, avg
3 ADMM, ag 3 ADMM, ag
ADMM, avg ADMM, avg
2 2
1 1
0 0
0 10 20 30 40 50 0 10 20 30 40 50
Iteration Iteration
Figure 1: 2-Wasserstein distance to target distribution vs iteration for n = 50. Both the
i
distancetothetargetdistributionoftheaverageiterate(avg)andaspecificagent
iterate (ag) are provided for each method. For the sparsely connected (cyclic)
graph topology, our proposed algorithm (D-ADMMS) outperforms the baselines
(D-SGLD, D-ULA, D-SGHMC) in terms of Wasserstein distance between the
distribution of the agent iterate and the target distribution.
15
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaWTzikas et al.
Results for 5 agents
Fully connected Cyclic No edges
D-SGLD, ag D-SGLD, ag D-SGLD, ag
D-SGLD, avg D-SGLD, avg 5 D-SGLD, avg
5 D-ULA, ag 5 D-ULA, ag D-ULA, ag
D-ULA, avg D-ULA, avg D-ULA, avg
D-SGHMC, ag D-SGHMC, ag D-SGHMC, ag
D-SGHMC, avg D-SGHMC, avg D-SGHMC, avg
D-ADMMS, ag D-ADMMS, ag 4 D-ADMMS, ag
4 D-ADMMS, avg 4 D-ADMMS, avg D-ADMMS, avg
ADMM, ag ADMM, ag ADMM, ag
ADMM, avg ADMM, avg ADMM, avg
3 3 3
2 2 2
1 1 1
0 0 0
0 5 10 15 20 25 30 0 5 10 15 20 25 30 0 5 10 15 20 25 30
Iteration Iteration Iteration
Results for 20 agents
Fully connected Cyclic No edges
2.00
D-SGLD, ag D-SGLD, ag D-SGLD, ag
1.0 D-SGLD, avg D-SGLD, avg D-SGLD, avg
D-ULA, ag D-ULA, ag 1.75 D-ULA, ag
D-ULA, avg 0.8 D-ULA, avg D-ULA, avg
D-SGHMC, ag D-SGHMC, ag D-SGHMC, ag
0.8 D-SGHMC, avg D-SGHMC, avg 1.50 D-SGHMC, avg
D-ADMMS, ag D-ADMMS, ag D-ADMMS, ag
D-ADMMS, avg D-ADMMS, avg D-ADMMS, avg ADMM, ag 0.6 ADMM, ag 1.25 ADMM, ag
0.6 ADMM, avg ADMM, avg ADMM, avg
1.00
0.4
0.4 0.75
0.2 0.2 0.50
0.25
0.0 0.0
0.00
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Iteration Iteration Iteration
Results for 100 agents
Cyclic No edges
5 D-SGLD, ag 5 D-SGLD, ag
D-SGLD, avg D-SGLD, avg
D-ULA, ag D-ULA, ag
D-ULA, avg D-ULA, avg
4 D-SGHMC, ag 4 D-SGHMC, ag
D-SGHMC, avg D-SGHMC, avg
D-ADMMS, ag D-ADMMS, ag
D-ADMMS, avg D-ADMMS, avg
3 ADMM, ag 3 ADMM, ag
ADMM, avg ADMM, avg
2 2
1 1
0 0
0 10 20 30 40 50 0 10 20 30 40 50
Iteration Iteration
Figure 2: 2-Wasserstein distance to target distribution vs iteration for n = 200. Both the
i
distancetothetargetdistributionoftheaverageiterate(avg)andaspecificagent
iterate (ag) are provided for each method. For the sparsely connected (cyclic)
graph topology, our proposed algorithm (D-ADMMS) outperforms the baselines
(D-SGLD, D-ULA, D-SGHMC) in terms of Wasserstein distance between the
distribution of the agent iterate and the target distribution.
16
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaW
ecnatsiD
nietsressaWDistributed MCMC Sampling via ADMM
6.2.4 Ablation
We study the robustness of the proposed scheme with respect to its hyper-parameter ρ. We
notethatD-ADMMSonlyrequiressettingthehyper-parameterρ. InFigure3, weassumea
network of 20 agents with n = 50 and we compute the 2-Wasserstein distance between the
i
(k)
empirical distribution of an agent’s iterate, x , when D-ADMMS is deployed, and the true
i
posterior. Wedeploy D-ADMMS withdifferentvalues ofρinordertostudythealgorithm’s
robustness to the choice of hyper-parameter. We use 100 trials in order to determine the
iterate’s empirical distribution.
Figure 4 shows the convergence in terms of 2-Wasserstein distance between an agent’s
iterate,whenD-ADMMSisdeployed,andthetrueposteriorforacyclicnetworkof20agents
withn = 50. Inthiscase, however, wefixρ = 5andvarytheinitialprobabilitydistribution
i
of the 100 sample iterates. We perform 10 experiments. For the first experiment, we
(0)
assume that the initial distribution for each agent iterate is the standard normal (x ∼
i
N(0,I), ∀i ∈ V). In each remaining experiment q, the 100 sample iterates are initially
drawn from the distribution N ((−1,2),Σ ), where Σ = A AT, and the entries of A are
q q q q q
drawn from the uniform U(0,10). Figure 5 demonstrates the evolution of samples in D-
ADMMS for a cyclic topology of 5 agents with n = 50, x (0) ∼ N(0,I), ρ = 5, and
i i
x ∈ R1.
Results for 20 agents
Cyclic
5
4
3
=1.0
=5.0
2 =10.0
=20.0
=50.0
1 =100.0
=200.0
=500.0
0 =1000.0
0 10 20 30 40 50
Iteration
Figure 3: 2-Wasserstein distance of an agent’s iterate to the target distribution for varying
ρ in D-ADMMS.
6.3 Bayesian Logistic Regression
6.3.1 Problem
We consider the distributed Bayesian logistic regression setting. We consider a varying
number of agents (N = 5,20,50) and a varying network topology (fully connected, ring-
cyclic, and no-edge) in our results. We first create a dataset of IID (zl,yl) pairs indexed by
l, where yl is a binary label (0 or 1). The likelihood function of a given sample for model
parameters x is
P(y = 1 | z;x) = (cid:0) 1+exp−xTz(cid:1)−1 . (28)
17
ecnatsiD
nietsressaWTzikas et al.
Results for 20 agents
Cyclic
standard
14 random
12
10
8
6
4
2
0
0 10 20 30 40 50
Iteration
Figure 4: 2-Wasserstein distance of an agent’s iterate to the target distribution for varying
initial sample distribution in D-ADMMS. Standard refers to x (0) ∼ N(0,I).
i
Figure 5: Evolution of the agents’ sample distributions in D-ADMMS for a cyclic network
of five agents. Each color corresponds to the samples of a different agent. We
also include the true global posterior distribution up to a scaling factor.
We assume x ∈ R3. The prior distribution over the model parameters is N(0,λI), where
λ = 10. A data point is created as follows:
zl ∼ N(0,20I), pl ∼ U(0,1), (29)
while the label yl is 1 if pl ≤ (cid:0) 1+exp−xTzl(cid:1)−1 and 0 otherwise. The parameter x is
sampled from its prior distribution and U(0,1) denotes the uniform distribution between 0
and 1. Assume that each agent i possesses n independent data points (zl,yl), where the
i i i
first n˜ data points are those with label yl = 1. Then the goal in Bayesian logistic regression
i i
18
ecnatsiD
nietsressaWDistributed MCMC Sampling via ADMM
(cid:80)
is to sample from the global posterior, which is proportional to exp− f (x), where
i∈V i
(cid:88)ni (cid:16) (cid:17) ∥x∥2
f (x) = log 1+expψ xTzl + , (30)
i l i 2λN
l=1
and ψ = −1 if 1 ≤ l ≤ n˜ , while ψ = 1 otherwise.
l i l
6.3.2 Algorithm
We use n = 50, ρ = 5, η = 0.0003, η = 0.02, and γ = 30 (Gu¨rbu¨zbalaban
i DSGLD DSGHMC
et al., 2021). For D-ULA, we assume α(k) and ζ(k) follow the same equations as in Section
6.2. We assume χ = χ = 0.05 for the cyclic and no-edge topologies. For the case of the
1 2
fully connected graph: when N = 5,20 we assume χ = 0.55,χ = 0.05, while for N = 50
1 2
we set χ = χ = 0.9.
1 2
6.3.3 Discussion
Because the posterior does not admit a Gaussian explicit formula, instead of 2-Wasserstein
distance, we provide results for prediction accuracy. In Figure 6, we show the mean predic-
tion accuracy on the total dataset, along with the ± 1 standard deviation interval, of each
agent, based on its iterate x(k) , as a function of the iteration. If P(y = 1 | zl;x(k) ) ≥ 0.5,
i i
theagent assignslabel1tothedata-point. Themeanandstandarddeviationarecomputed
(k)
through 100 independent trials. We also include ADMM (i.e., w = 0 in Algorithm 1).
i
WeobservethatD-ADMMSisabletoobtainthehighestaccuracyoutofallthesampling
methodspresented,inasmallernumberofiterationsforthecyclictopology. Itsperformance
is similar to that of ADMM. ADMM possesses superior performance because it converges
to the MAP model parameter, which is expected to have the highest accuracy. In addition,
the deviation from the mean accuracy is smaller for D-ADMMS in the cyclic topology. For
theno-edgetopology, asinBayesianlinearregression, ADMMisidenticaltoD-ADMMS.In
the fully connected case for N equal to 20 and 50 the decline in performance of D-ADMMS
issimilartothatfoundintheBayesianlinearregressiontask. Overall, D-ADMMSperforms
worse as the number of agents and the connectivity of the graph increase. This behavior
matches the observed performance in the Bayesian linear regression task.
7 Conclusions and Future Work
We proposed a novel distributed sampling algorithm based on ADMM. By using proximal
updates to generate samples in a distributed manner for a log-concave distribution, our
approach outperforms existing gradient-based algorithms. We have shown convergence
of the distribution of the iterates of our algorithm to the target distribution and have
demonstrated practical advantages for our method in regression problems. Despite these
promising results, limitations of the proposed algorithm include synchronous and lossless
communication among the agents of the network.
Future directions include: i) analyzing the proposed scheme as the discretization of a
stochastic differential equation to improve the convergence guarantees, ii) exploring connec-
tions between gradient flows and MCMC algorithms in distributed settings, iii) designing
19Tzikas et al.
Results for 5 agents
Fully connected Cyclic No edges
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 0.7 0.7
0.6 0.6 0.6
0.5 0.5 0.5
0.4 D-SGLD, ag 0.4 D-SGLD, ag 0.4 D-SGLD, ag
D-ULA, ag D-ULA, ag D-ULA, ag
0.3 D D- -ASG DH MM MC S, , a ag g 0.3 D D- -ASG DH MM MC S, , a ag g 0.3 D D- -ASG DH MM MC S, , a ag g
ADMM, ag ADMM, ag 0.2 ADMM, ag
0 5 10 15 20 25 30 0 5 10 15 20 25 30 0 5 10 15 20 25 30
Iteration Iteration Iteration
Results for 20 agents
Fully connected Cyclic No edges
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 0.7 0.7
0.6 0.6 0.6
0.5 0.5 0.5
0.4 D-SGLD, ag 0.4 D-SGLD, ag 0.4 D-SGLD, ag
D-ULA, ag D-ULA, ag D-ULA, ag
0.3 D-SGHMC, ag 0.3 D-SGHMC, ag 0.3 D-SGHMC, ag
D-ADMMS, ag D-ADMMS, ag D-ADMMS, ag
0.2 ADMM, ag 0.2 ADMM, ag 0.2 ADMM, ag
0 5 10 15 0 5 10 15 0 5 10 15
Iteration Iteration Iteration
Results for 50 agents
Fully connected Cyclic No edges
1.0 1.0
0.9 0.9 0.9
0.8 0.8
0.8
0.7 0.7
0.6 0.7 0.6
0.5 0.6 0.5
0.4 D-SGLD, ag D-SGLD, ag 0.4 D-SGLD, ag
D-ULA, ag 0.5 D-ULA, ag D-ULA, ag
0.3 D-SGHMC, ag D-SGHMC, ag 0.3 D-SGHMC, ag
D-ADMMS, ag D-ADMMS, ag D-ADMMS, ag
ADMM, ag ADMM, ag ADMM, ag
0.2 0.4 0.2
0 5 10 15 0 5 10 15 0 5 10 15
Iteration Iteration Iteration
Figure 6: Predictionaccuracyperiterationoncompletedataset. Wedepicttheaccuracyper
iteration on the complete dataset for each agent along with a 1-std interval over
100 independent trials for varying network topology. For the sparsely connected
(cyclic) graph topology, our proposed algorithm (D-ADMMS) outperforms the
baselines (D-SGLD, D-ULA, D-SGHMC) in terms of prediction accuracy.
20
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccADistributed MCMC Sampling via ADMM
distributed sampling algorithms based on accelerated first-order or higher-order optimiza-
tion methods, iv) considering distributed sampling with constrained support.
Acknowledgments and Disclosure of Funding
The NASA University Leadership Initiative (grant # 80NSSC20M0163) provided funds to
assist the first author with their research, but this article solely reflects the opinions and
conclusions of its authors and not any NASA entity. For the first author, this work was
also partially funded through the Alexander S. Onassis Foundation Scholarship program.
Appendix A. Proof of Lemma 1
We substitute
1
MTX(k)−Z(k) = 0 (31)
2 +
into
(cid:16) (cid:17) √
∇f(X(k+1))+M β(k+1)−ρM Z(k)−Z(k+1) + 2Dw(k+1) = 0, (32)
− +
ρ
β(k+1)−β(k)− MTX(k+1) = 0, (33)
2 −
to obtain
ρ (cid:16) (cid:17) √
∇f(X(k+1))+M β(k+1)− M MT X(k)−X(k+1) + 2Dw(k+1) = 0, (34)
− 2 + +
ρ
β(k+1)−β(k)− MTX(k+1) = 0, (35)
2 −
We observe that eq. (34) depends on M β(k+1) rather than β(k+1). We thus multiply eq.
−
(35) by M and obtain
−
ρ
M β(k+1)−M β(k)− M MTX(k+1) = 0. (36)
− − 2 − −
We substitute eq. (36) into eq. (34) and get
ρ (cid:16)ρ ρ (cid:17) √
∇f(X(k+1))+M β(k)− M MTX(k)+ M MT + M MT X(k+1)+ 2Dw(k+1) = 0.
− 2 + + 2 + + 2 − −
(37)
We define p(k) = M β(k) and from eq. (36, 37) we obtain the equivalent updates
−
ρ (cid:16)ρ ρ (cid:17) √
∇f(X(k+1))+p(k)− M MTX(k)+ M MT + M MT X(k+1)+ 2Dw(k+1) = 0,
2 + + 2 + + 2 − −
(38)
ρ
p(k+1)−p(k)− M MTX(k+1) = 0.
2 − −
(39)
21Tzikas et al.
1
Notice that L = 1M MT,L = 1M MT and D = (L +L ). Hence eq. (38-39) can
+ 2 + + − 2 − − 2 + −
be written as
√
∇f(X(k+1))+p(k)−ρL X(k)+2ρDX(k+1)+ 2Dw(k+1) = 0, (40)
+
p(k+1)−p(k)−ρL X(k+1) = 0. (41)
−
Wehavethusestablishedthateq. (10-12)implyeq. (40-41). Byasimpleinspection, wecan
observe that eq. (40-41) are equivalent to the update equations of D-ADMMS in Algorithm
1, where X(k) is the concatenation of the x(k) from Algorithm 1.
i
Appendix B. Proof of Lemma 2
Let us start with the following auxiliary lemma.
Lemma 4 Assume x,y ∈ Rn. Then the inequality
(cid:18) (cid:19)
1
∥x+y∥2+(κ−1)∥x∥2 ≥ 1− ∥y∥2 (42)
κ
holds for any κ > 1.
Proof By expanding the left-hand side (LHS) of (42), we obtain
(cid:18) (cid:19)
1
∥x∥2+2⟨x,y⟩+∥y∥2+(κ−1)∥x∥2 ≥ 1− ∥y∥2.
κ
By bringing all terms to the LHS, we get
1
κ∥x∥2+2⟨x,y⟩+ ∥y∥2 ≥ 0.
κ
By multiplying both sides by κ and noticing that κ > 1 > 0, we get
∥κx∥2+2⟨κx,y⟩+∥y∥2 ≥ 0,
which can be written as
∥κx+y∥2 ≥ 0,
this concluding the proof of the lemma.
Ourproofstrategyleveragesequations(10-12)andtheKKTconditionsin(13-15),which
yield the relations:
(cid:16) (cid:17) (cid:16) (cid:17) √
∇f(X(k+1))−∇f(X∗) = ρM Z(k)−Z(k+1) −M β(k+1)−β∗ − 2Dw(k+1), (43)
+ −
ρ (cid:16) (cid:17)
β(k+1)−β(k) = MT X(k+1)−X∗ , (44)
2 −
1 (cid:16) (cid:17)
Z(k+1)−Z∗ = MT X(k+1)−X∗ . (45)
2 +
We now exploit the relations in eq. (43-45) to show inequalities that hold for the iterates
generated by Algorithm 1. To this end, we split the analysis into five steps, which are
presented in the sequel.
22Distributed MCMC Sampling via ADMM
Step 1: A basic inequality
Since function f(X) is strongly convex (under Assumption 1), we obtain
m ∥X(k+1)−X∗∥2 ≤ ⟨X(k+1)−X∗,∇f(X(k+1))−∇f(X∗)⟩ =
f
(cid:16) (cid:17) (cid:16) (cid:17) √
⟨X(k+1)−X∗,ρM Z(k)−Z(k+1) −M β(k+1)−β∗ − 2Dw(k+1)⟩ =
+ −
(cid:16) (cid:17)
⟨X(k+1)−X∗,ρM Z(k)−Z(k+1) ⟩
+
(cid:16) (cid:17) √
+⟨X(k+1)−X∗,−M β(k+1)−β∗ ⟩+⟨X(k+1)−X∗,− 2Dw(k+1)⟩ =
−
(cid:16) (cid:17)
ρ⟨MT X(k+1)−X∗ ,Z(k)−Z(k+1)⟩
+
(cid:16) (cid:17) √
−⟨MT X(k+1)−X∗ ,β(k+1)−β∗⟩−⟨X(k+1)−X∗, 2Dw(k+1)⟩. (46)
−
Thefirstequalityholdsduetoeq. (43), thesecondandthridonesholdbysimplyexpanding
the terms and performing trivial algebraic manipulations. We substitute eq. (44-45) into
the last equation of (46) to obtain
m ∥X(k+1)−X∗∥2 ≤
f
2 √
2ρ⟨Z(k+1)−Z∗,Z(k)−Z(k+1)⟩+ ⟨β(k)−β(k+1),β(k+1)−β∗⟩−⟨X(k+1)−X∗, 2Dw(k+1)⟩ =
ρ
(cid:16) (cid:17)T (cid:16) (cid:17) √
2 U(k)−U(k+1) G U(k+1)−U∗ −⟨X(k+1)−X∗, 2Dw(k+1)⟩, (47)
(cid:110) (cid:111)
where we recall that U(k) = (Z(k),β(k)) and G = diag ρI , 1I . Using the relation
2|E|d ρ 2|E|d
⟨a−b,b−c⟩ = ∥a−c∥2 −∥a−b∥2 −∥b−c∥2 for the first term on the right-hand side of
G G G G
eq. (47), with a = U(k), b = U(k+1), and c = U∗, we obtain
m ∥X(k+1)−X∗∥2 ≤
f
√
∥U(k)−U∗∥2 −∥U(k+1)−U∗∥2 −∥U(k)−U(k+1)∥2 −⟨X(k+1)−X∗, 2Dw(k+1)⟩. (48)
G G G
We now upper bound the last term
√ √
−⟨X(k+1)−X∗, 2Dw(k+1)⟩ ≤ ∥X(k+1)−X∗∥∥ 2Dw(k+1)∥
1 (cid:16) √ (cid:17)2
≤ ∥X(k+1)−X∗∥+∥ 2Dw(k+1)∥ , (49)
2
where we apply the inequality ab ≤ 1(a+b)2 in the last step. This yields
2
m ∥X(k+1)−X∗∥2 ≤ ∥U(k)−U∗∥2
f G
1 (cid:16) √ (cid:17)2
−∥U(k+1)−U∗∥2 −∥U(k)−U(k+1)∥2 + ∥X(k+1)−X∗∥+∥ 2Dw(k+1)∥ . (50)
G G 2
Eq. (50) is the basic inequality of our analysis. The next steps of the proof constitute in
further developing such inequality.
23Tzikas et al.
Step 2: Dealing with the term ∥U(k)−U(k+1)∥2 +m ∥X(k+1)−X∗∥2
G f
Observe that
∥U(k)−U(k+1)∥2 +m ∥X(k+1)−X∗∥2 =
G f
1
ρ∥Z(k)−Z(k+1)∥2+ ∥β(k)−β(k+1)∥2+m ∥X(k+1)−X∗∥2. (51)
f
ρ
We now focus on obtaining a lower bound for the right-hand side (RHS) term in eq. (51),
which will be done in two steps as described below.
Step 2.1: An intermediate inequality
We show that the inequality
ρκσ2 (M ) κM2
max + ∥Z(k+1)−Z(k)∥2+ f ∥X(k+1)−X∗∥2 ≥
(κ−1)σ2 (M ) ρσ2 (M )
min − √ min −
1 2 2 (cid:16) (cid:17)
∥β(k+1)−β∗∥2− ∥M β(k+1)−β∗ ∥∥Dw(k+1)∥+
ρ ρσ2 (M ) −
min −
2
∥Dw(k+1)∥2, (52)
ρσ2 (M )
min −
holds, for any κ > 1. The first step to obtain inequality (52) is to manipulate the relation
in (43) by means of the inequality ∥a+b∥2+(κ−1)∥a∥2 ≥ (cid:0) 1− 1(cid:1) ∥b∥2 (see Lemma 2 for a
κ
proof of this inequality), which holds for κ > 1. Indeed, setting a = ∇f(X(k+1))−∇f(X⋆)
√
and b = M (cid:0) β(k+1)−β⋆(cid:1) + 2Dw(k+1), we obtain
−
(cid:18) 1(cid:19) (cid:16) (cid:17) √
1− ∥M β(k+1)−β∗ + 2Dw(k+1)∥2
−
κ
(cid:16) (cid:17)
≤ ∥ρM Z(k)−Z(k+1) ∥2+(κ−1)∥∇f(X(k+1))−∇f(X∗)∥2
+
≤ ρ2σ2 (M )∥Z(k+1)−Z(k)∥2+(κ−1)M2∥X(k+1)−X∗∥2, (53)
max + f
where the first inequality follows from ∥a+b∥ = ∥ρM (Z(k)−Z(k+1)∥ due to (43), and the
+
second inequality follows by Lipschitz continuity of ∇f(X) and the fact that the largest
singular value of M equals the induced 2-norm ∥M ∥ = max
∥M+x∥
. By expanding the
+ + x̸=0 ∥x∥
squares in the LHS of (53) and using the inequality ⟨a,b⟩ ≥ −∥a∥∥b∥, we have that
(cid:18) 1(cid:19) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17) √ √ (cid:17)
1− ∥M β(k+1)−β∗ ∥2−2∥M β(k+1)−β∗ ∥∥ 2Dw(k+1)∥+∥ 2Dw(k+1)∥2
− −
κ
≤ ρ2σ2 (M )∥Z(k+1)−Z(k)∥2+(κ−1)M2∥X(k+1)−X∗∥2. (54)
max + f
We now use the fact that ∥M (cid:0) β(k+1)−β∗(cid:1) ∥2 ≥ σ2 (M )∥β(k+1) −β∗∥2 (because both
− min −
β∗ and β(k+1) lie in the column space of MT) and then multiply the resulting inequality by
−
κ to obtain eq. (52).
ρ(κ−1)σ m2 in(M−)
24Distributed MCMC Sampling via ADMM
Step 2.2: A trivial inequality
Notice that from eq. (45)
1 (cid:16) (cid:17) 1
∥Z(k+1)−Z∗∥2 = ∥MT X(k+1)−X∗ ∥2 ≤ σ2 (M )∥X(k+1)−X∗∥2. (55)
4 + 4 max +
By simple multiplication of both sides by ρ, we obtain
ρσ2 (M )
ρ∥Z(k+1)−Z∗∥2 ≤ max + ∥X(k+1)−X∗∥2. (56)
4
Step 2.3: Combining the results from steps 2.1 and 2.2 to obtain (52)
We add inequality (56) into (52) to obtain
(cid:32) (cid:33)
ρκσ2 (M ) κM2 ρ
max + ∥Z(k+1)−Z(k)∥2+ f + σ2 (M ) ∥X(k+1)−X∗∥2 ≥
(κ−1)σ2 (M ) ρσ2 (M ) 4 max +
min − min −
√
1 2 2 (cid:16) (cid:17)
ρ∥Z(k+1)−Z∗∥2+ ∥β(k+1)−β∗∥2− ∥M β(k+1)−β∗ ∥∥Dw(k+1)∥+
ρ σ2 (M ) −
min −
2
∥Dw(k+1)∥2. (57)
ρσ2 (M )
min −
We now let
(cid:40) (cid:41)
(κ−1)σ2 (M ) m
δ = min min − , f > 0, (58)
κσ m2 ax(M +) ρσ2 (M )+ κM f2
4 max + ρσ m2 in(M−)
(cid:18) (cid:19)
and notice that if δ = (κ−1)σ m2 in(M−) , then δ κM f2 + ρσ2 (M ) ≤ m . Similarly, if
κσ m2 ax(M+) ρσ m2 in(M−) 4 max + f
δ = m f , then δ ρκσ m2 ax(M+) ≤ ρ. Therefore, by multiplying eq. (57) with
ρ 4σ m2 ax(M+)+
ρσ
m2κ iM
n(f
M2
−)
(κ−1)σ m2 in(M−)
δ, we obtain
ρ∥Z(k+1)−Z(k)∥2+m ∥X(k+1)−X∗∥2 ≥
f
√
δ 2 2δ (cid:16) (cid:17)
ρδ∥Z(k+1)−Z∗∥2+ ∥β(k+1)−β∗∥2− ∥M β(k+1)−β∗ ∥∥Dw(k+1)∥+
ρ σ2 (M ) −
min −
2δ
∥Dw(k+1)∥2. (59)
ρσ2 (M )
min −
We add the positive term 1∥β(k+1)−β(k)∥2 to the LHS of the last equation and apply the
ρ
definition of ∥U(k+1)−U(k)∥2 to get
G
∥U(k+1)−U(k)∥2 +m ∥X(k+1)−X∗∥2 ≥
G f
√
2 2δ (cid:16) (cid:17)
δ∥U(k+1)−U∗∥2 − ∥M β(k+1)−β∗ ∥∥Dw(k+1)∥+
G σ2 (M ) −
min −
2δ
∥Dw(k+1)∥2. (60)
ρσ2 (M )
min −
25Tzikas et al.
The last equation is a lower bound for the term ∥U(k) −U(k+1)∥2 +m ∥X(k+1) −X∗∥2,
G f
thus concluding step 2 of the proof.
Step 3: Manipulating the lower bound (60) of ∥U(k)−U(k+1)∥2 +m ∥X(k+1)−X∗∥2
G f
We combine inequality (60) with eq. (50) and obtain
1 1 (cid:16) √ (cid:17)2
∥U(k+1)−U∗∥2 ≤ ∥U(k)−U∗∥2 + ∥X(k+1)−X∗∥+∥ 2Dw(k+1)∥ +
G 1+δ G 2(1+δ)
√
2 2δ (cid:16) (cid:17) 2δ
∥M β(k+1)−β∗ ∥∥Dw(k+1)∥− ∥Dw(k+1)∥2, (61)
(1+δ)σ2 (M ) − (1+δ)ρσ2 (M )
min − min −
by using a ≤ b ≤ c ⇒ a ≤ c, where b stands for ∥U(k+1)−U(k)∥2 +m ∥X(k+1)−X∗∥2, and
G f
some algebraic manipulations.
Step 3.1: An intermediate trick
Eq. (48) also gives us the upper bound
1 1 √
∥X(k+1)−X∗∥2 ≤ ∥U(k)−U∗∥2 + ⟨X(k+1)−X∗,− 2Dw(k+1)⟩, (62)
m G m
f f
which, by completing the square, is equivalently written as
1 1 1
∥X(k+1)−X∗+ √ Dw(k+1)∥2 ≤ ∥U(k)−U∗∥2 + ∥Dw(k+1)∥2. (63)
2m f m f G 2m2 f
We now work to reform the second and third terms on the (RHS) of eq. (61).
Step 3.2: Manipulating the third term on the RHS of eq. (61)
We first manipulate the third term of eq. (61). From eq. (44) we have that
ρ (cid:16) (cid:17)
β(k+1)−β∗ = β(k)−β∗+ MT X(k+1)−X∗ . (64)
2 −
26Distributed MCMC Sampling via ADMM
which gives
(cid:16) (cid:17)
∥M β(k+1)−β∗ ∥ ≤ σ (M )∥β(k+1)−β∗∥
− max −
(cid:18) (cid:19)
ρσ (M )
≤ σ (M ) ∥β(k)−β∗∥+ max − ∥X(k+1)−X∗∥
max −
2
≤ σ (M )·
max −
(cid:32) (cid:32) (cid:33)(cid:33)
ρσ (M ) 1 1
∥β(k)−β∗∥+ max − ∥X(k+1)−X∗+ √ Dw(k+1)∥+∥√ Dw(k+1)∥
2 2m 2m
f f
≤ σ (M )·
max −
(cid:32) (cid:32)(cid:115) (cid:33)(cid:33)
ρσ (M ) 1 1 1
∥β(k)−β∗∥+ max − ∥U(k)−U∗∥2 + ∥Dw(k+1)∥2+∥√ Dw(k+1)∥
2 m f G 2m2 f 2m f
√ (cid:113)
≤ σ (M ) ρ ∥U(k)−U∗∥2 +
max − G
(cid:32)(cid:115) (cid:33)
ρσ2 (M ) 1 1 1
max − ∥U(k)−U∗∥2 + ∥Dw(k+1)∥2+∥√ Dw(k+1)∥ . (65)
2 m f G 2m2 f 2m f
The first step in the reasoning above applies the Euclidian norm and uses the properties of
the singular values of a matrix. The second step uses the triangle inequality property of the
Euclidian norm, eq. (64), and the properties of the singular values of a matrix. The third
step uses the triangle inequality as well. The fourth step uses eq. (63). Finally, the fifth
step uses the fact that
(cid:114) (cid:114)
√ 1 √ 1
∥β(k)−β∗∥ = ρ ∥β(k)−β∗∥2 ≤ ρ ∥β(k)−β∗∥2+ρ∥Z(k)−Z∗∥2, (66)
ρ ρ
and the definition of ∥U(k)−U∗∥2.
G
Step 3.3: Manipulating the second term on the RHS of eq. (61)
For the second term of eq. (61), we get
(cid:16) √ (cid:17)2
∥X(k+1)−X∗∥+∥ 2Dw(k+1)∥ ≤
 2
  ∥X(k+1)−X∗+ √1 Dw(k+1)∥+∥√1 Dw(k+1)∥+∥√ 2Dw(k+1)∥   ≤
 2m 2m 
 f f 
(cid:124) (cid:123)(cid:122) (cid:125)
w¯(k+1)≥0
1 1 (cid:16) (cid:17)2 1
∥U(k)−U∗∥2 +2w¯(k+1)∥X(k+1)−X∗+√ Dw(k+1)∥+ w¯(k+1) + ∥Dw(k+1)∥2 ≤
m f G 2m f 2m2 f
(cid:115)
1 1 1 (cid:16) (cid:17)2 1
∥U(k)−U∗∥2+2w¯(k+1) ∥U(k)−U∗∥2 + ∥Dw(k+1)∥2+ w¯(k+1) + ∥Dw(k+1)∥2,
m G m G 2m2 2m2
f f f f
(67)
27Tzikas et al.
by using the triangle inequality of the Euclidian norm, and then simply developing the
square and using eq. (63).
Step 4: Simplifying the recursive relationship (61) for ∥U(k+1)−U∗∥2
G
By replacing the third and second term on the RHS of eq. (61), with their upper bounds,
eq. (65) and eq. (67) respectively, and by combining like terms, we obtain
2m +1
∥U(k+1)−U∗∥2 ≤ f ∥U(k)−U∗∥2+
G 2m (1+δ) G
f
(cid:124) (cid:123)(cid:122) (cid:125)
a
(cid:32) (cid:115) (cid:33)
1 1 1 (cid:16) (cid:17)2 1
2w¯(k+1) ∥U(k)−U∗∥2 + ∥Dw(k+1)∥2+ w¯(k+1) + ∥Dw(k+1)∥2 +
2(1+δ) m G 2m2 2m2
f f f
(cid:124) (cid:123)(cid:122) (cid:125)
b
√ (cid:32)
2 2δ √ (cid:113)
∥Dw(k+1)∥ σ (M ) ρ ∥U(k)−U∗∥2 +
(1+δ)σ2 (M ) max − G
min −
(cid:124) (cid:123)(cid:122) (cid:125)
c
(cid:32)(cid:115) (cid:33)(cid:33)
ρσ2 (M ) 1 1 1
max − ∥U(k)−U∗∥2 + ∥Dw(k+1)∥2+∥√ Dw(k+1)∥
(cid:124) (cid:123)2 (cid:122) (cid:125) m f G 2m2 f 2m f
d
2δ
− ∥Dw(k+1)∥2. (68)
(1+δ)ρσ2 (M )
min −
(cid:124) (cid:123)(cid:122) (cid:125)
e
√ √ √
We will now use the fact a+b ≤ a+ b for a,b ≥ 0. Eq. (68) then gives us
b b
∥U(k+1)−U∗∥2
G
≤ a∥U(k)−U∗∥2 G+2√
m
fw¯(k+1)∥U(k)−U∗∥ G+2√
2m
fw¯(k+1)∥Dw(k+1)∥
(cid:16) (cid:17)2 b √
+b w¯(k+1) + ∥Dw(k+1)∥2+cσ (M ) ρ∥Dw(k+1)∥∥U(k)−U∗∥
2m2 max − G
f
cd cd cd
+ √ ∥Dw(k+1)∥∥U(k)−U∗∥ G+ √ ∥Dw(k+1)∥2+ √ ∥Dw(k+1)∥2−e∥Dw(k+1)∥2
m f 2m f 2m f
= a∥U(k)−U∗∥2+
G
(cid:32) (cid:33)
b √ cd
2√ w¯(k+1)+cσ max(M −) ρ∥Dw(k+1)∥+ √ ∥Dw(k+1)∥ ∥U(k)−U∗∥ G
m m
f f
(cid:124) (cid:123)(cid:122) (cid:125)
y(k+1)
(cid:32)√ √ (cid:33)
2b (cid:16) (cid:17)2 b 2cd
+ w¯(k+1)∥Dw(k+1)∥+b w¯(k+1) + ∥Dw(k+1)∥2+ ∥Dw(k+1)∥2−e∥Dw(k+1)∥2 .
m 2m2 m
f f f
(cid:124) (cid:123)(cid:122) (cid:125)
r(k+1)
(69)
28Distributed MCMC Sampling via ADMM
By simple substitution of the labeled quantities, we have
(cid:113)
∥U(k+1)−U∗∥2 ≤ a∥U(k)−U∗∥2 +y(k+1) ∥U(k)−U∗∥2 +r(k+1). (70)
G G G
An important fact is that y(k+1),r(k+1) only depend on the noise at iteration (k+1), i.e.,
these terms are a function of w(k+1).
Step 5: Obtaining bounds for the Wasserstein distances
We now choose the coupling2 between the marginal probability distribution of Z(k) and Z∗
(which is seen as a random variable with point mass at the value Z∗) to be that for which
their normed distance squared is minimized. We do the same for the coupling between
β(k+1) and β∗ (which is also seen as a random variable with point mass at value β∗). Using
Jensen’s inequality for concave functions and the independence between w(k+1) and U(k),
we get
W2(µ ,µ ) ≤ ∥U(k+1)−U∗∥2 ≤ aW2(µ ,µ )+
G U(k+1) U∗ G G U(k) U∗
(cid:16) (cid:17)(cid:113) (cid:16) (cid:17)
E y(k+1) W2(µ ,µ )+E r(k+1)
G U(k) U∗
(cid:32)
√
E(cid:0) y(k+1)(cid:1)(cid:33)2
(cid:16) (cid:17)
(cid:32) E(cid:0) y(k+1)(cid:1)(cid:33)2
≤ aW (µ ,µ )+ √ +E r(k+1) − √ . (71)
G U(k) U∗ 2 a 2 a
√ √ √
We can now bound the Wasserstein distance, by using a+b ≤ a+ b for a,b ≥ 0,
(cid:118)
W (µ ,µ ) ≤
√
aW (µ ,µ )+
E(cid:0)
y
√(k+1)(cid:1) +(cid:117)
(cid:117)
(cid:116)|
E(cid:0) r(k+1)(cid:1)
−(cid:32) E(cid:0)
y
√(k+1)(cid:1)(cid:33)2
|.
G U(k+1) U∗ G U(k) U∗ 2 a 2 a
(cid:124) (cid:123)(cid:122) (cid:125)
h(k+1)
(72)
From eq. (63) and by applying the same reasoning as before, we obtain
1 1 (cid:16) (cid:17)
W2(µ ,µ ) ≤ W2(µ ,µ )+ E ∥Dw(k+1)∥2 . (73)
X(k+1) X∗−√1 Dw(k+1) m G U(k) U∗ 2m2
2mf f f
√ √ √
By using a+b ≤ a+ b for a,b ≥ 0, we finally get
W(µ X(k+1),µ X∗−√ 21 mfDw(k+1)) ≤ √
m1
fW G(µ U(k),µ U∗)+ √
21
m
f(cid:113) E(cid:0) ∥Dw(k+1)∥2(cid:1)
. (74)
The triangle inequality of the 2-Wasserstein distance yields
W(µ ,µ∗) ≤ W(µ ,µ )+W(µ ,µ∗), (75)
X(k+1) X(k+1) X∗−√1 Dw(k+1) X∗−√1 Dw(k+1)
2mf 2mf
2. Assume that x and y are two random variables with marginal distributions p(x) and p(y) respectively.
(cid:80)
Thenthecouplingbetweenxandyisgivenbythejointdistributionp(x,y),suchthatp(x)= p(x,y)
(cid:80) y
and p(y) = p(x,y). The coupling is determined by the conditional distribution p(x | y) such that
x
p(x,y)=p(x|y)p(y).
29Tzikas et al.
and by eq. (71,74), we obtain the final bound
W(µ X(k+1),µ∗) ≤ √ m1 fW G(µ U(k),µ U∗)+ √ 21
m
f(cid:113) E(cid:0) ∥Dw(k+1)∥2(cid:1) +
W(µ ,µ∗), (76)
X∗−√1 Dw(k+1)
2mf
where the last two terms in the RHS are constants.
Appendix C. Proof of Theorem 3
The proof of Theorem 3 is based on two steps: i) we telescopically expand the inequality of
eq. (17), and ii) we find sufficient conditions for the telescopic sum to be decreasing with
increasing iteration number.
Step 1: Telescopically expanding eq. (13)
We start from eq. (17) of the main body, which is also given below for convenience,
(cid:118)
W (µ ,µ ) ≤
√
aW (µ ,µ )+
E(cid:0)
y
√(k+1)(cid:1) +(cid:117)
(cid:117)
(cid:116)|
E(cid:0) r(k+1)(cid:1)
−(cid:32) E(cid:0)
y
√(k+1)(cid:1)(cid:33)2
|.
G U(k+1) U∗ G U(k) U∗ 2 a 2 a
(77)
We first manipulate the last term on the RHS. By the triangle inequality of the absolute
value, we have that
(cid:118) (cid:118)
(cid:117)
(cid:117)
(cid:116)|
E(cid:0) r(k+1)(cid:1)
−(cid:32) E(cid:0)
y
√(k+1)(cid:1)(cid:33)2
| ≤
(cid:117)
(cid:117)
(cid:116)|
E(cid:0) r(k+1)(cid:1)
| + |
(cid:32) E(cid:0)
y
√(k+1)(cid:1)(cid:33)2
|. (78)
2 a 2 a
√ √ √
By the property a+b ≤ a+ b for a,b ≥ 0, we further have
(cid:118) (cid:118)
(cid:117)
(cid:117)
(cid:116)|
E(cid:0) r(k+1)(cid:1)
| + |
(cid:32) E(cid:0)
y
√(k+1)(cid:1)(cid:33)2
| ≤
(cid:113)
|
E(cid:0) r(k+1)(cid:1)
|+(cid:117)
(cid:117)
(cid:116)|
(cid:32) E(cid:0)
y
√(k+1)(cid:1)(cid:33)2
|. (79)
2 a 2 a
Therefore eq. (17) can be written as
√
E(cid:0) y(k+1)(cid:1)
(cid:113)
W (µ ,µ ) ≤ aW (µ ,µ )+ √ + |
E(cid:0) r(k+1)(cid:1)
|, (80)
G U(k+1) U∗ G U(k) U∗ a
since y(k+1) ≥ 0. We recursively apply the inequality above to obtain
√
(cid:0) (cid:1)k+1
W (µ ,µ ) ≤ a W (µ ,µ )+
G U(k+1) U∗ G U0 U∗
(cid:88)k+1 (cid:0)√ a(cid:1)k−lE(cid:16) y(l)(cid:17) +(cid:88)k+1 (cid:0)√ a(cid:1)k+1−l(cid:113)
|
E(cid:0) r(l)(cid:1)
|. (81)
l=1 l=1
30Distributed MCMC Sampling via ADMM
From eq. (18-21), we note that for given a,b,c,d, and e, the terms
E(cid:0) y(l)(cid:1)
and
E(cid:0) r(l)(cid:1)
are
bounded, as they only depend on the noise at iteration l and the Euclidian norm, as well
as the square of the Euclidian norm, of a Gaussian random variable are bounded. Assume
the terms
E(cid:0) y(l)(cid:1)
and
E(cid:0) r(l)(cid:1)
are bounded by Y ≥ 0 and R ≥ 0 respectively. Then, eq.
(81) becomes
(cid:0)√
(cid:1)k+1
(cid:88)k+1 (cid:0)√
(cid:1)k−l
(cid:88)k+1 (cid:0)√ (cid:1)k+1−l√
W (µ ,µ ) ≤ a W (µ ,µ )+ a Y + a R. (82)
G U(k+1) U∗ G U0 U∗
l=1 l=1
Combining eq. (82) with eq. (16) in the main body, we obtain
W(µ X(k+1),µ∗) ≤ √ m1 (cid:0)√ a(cid:1)k W G(µ U0,µ U∗)+
f
1 (cid:88)k (cid:0)√ (cid:1)k−l 1 (cid:88)k (cid:0)√ (cid:1)k−l√
√ a Y + √ a R+
m m
f f
l=1 l=1
√1 (cid:113) E(cid:0) ∥Dw(k+1)∥2(cid:1) +W(µ ,µ∗), (83)
2m
f
X∗−√ 21 mfDw(k+1)
wherethelasttwotermsontheRHSareconstants. Assumingthata < 1, then, sincea > 0,
we have (cid:80)∞ ak = 1 , which leads to the inequality
l=0 1−a
W(µ X(k+1),µ∗) ≤ √ m1 (cid:0)√ a(cid:1)k W G(µ U0,µ U∗)+
f
√
1 Y 1 R
√ √ + √ √ +
am 1− a m 1− a
f f
√1 (cid:113) E(cid:0) ∥Dw(k+1)∥2(cid:1) +W(µ ,µ∗). (84)
2m
f
X∗−√ 21 mfDw(k+1)
From the last equation, we observe that if a < 1, the upper bound for the Wasserstein
distance W(µ ,µ∗) decreases as the iteration number increases. This is because the
X(k+1)
first term on the RHS of eq. (84) decreases as k increases, while the remaining terms on
the RHS of eq. (84) are constants for all iterations k ≥ 0.
Step 2: Finding sufficient conditions for a < 1
We start with the definition of δ from eq. (20) of the main body. We observe that δ is a
function of both κ and ρ, given by
(cid:40) (cid:41)
(κ−1)σ2 (M ) m
δ(κ,ρ) = min min − , f . (85)
κσ m2 ax(M +) ρσ2 (M )+ κM f2
4 max + ρσ m2 in(M−)
We observe that only the second argument in the definition of δ(κ,ρ) depends on ρ. As-
suming a κ > 1 is selected, then the value
1
2κ2M
f
ρ(κ) = (86)
σ (M )σ (M )
min − max +
31Tzikas et al.
maximizes the second term in the min of eq. (85) and therefore δ for the given κ. In other
words, ρ(κ) from eq. (86) maximizes δ(κ,ρ) for the given κ. The maximum δ as a function
of κ, termed δ (κ), is hence given by
max
(cid:40) (cid:41)
(κ−1)σ2 (M ) m σ (M )
δ (κ) = min min − , f min − . (87)
max κσ m2 ax(M +) κ1 2M fσ max(M +)
In order to find the maximum δ(κ,ρ) we need to maximize δ (κ) in eq. (87) with respect
max
to κ. We observe that the first term in eq. (87) is monotonically increasing as a function κ,
while the second term in eq. (87) is monotonically decreasing as a function κ. Therefore,
to obtain the maximum δ, we choose κ such that the two terms are equal. Such a κ exists
and comes out to be
(cid:115)
1 τ2 τ4 τ2
κ = 1+ 4 G + G + G > 1, (88)
2 τ2 τ4 2τ2
f f f
where
σ (M ) M
max + f
τ = , τ = . (89)
G f
σ (M ) m
min − f
By plugging in κ from eq. (88) to eq. (87), we get the maximum possible value of δ(κ,ρ)
to be
(cid:115)
1 1 4 1
δ = + − . (90)
max 2τ τ2 τ2 2τ2
f f G f
We turn our attention to the definition of a in eq. (21). a < 1 if and only if 2m δ > 1.
f
Therefore, for convergence we need
1
δ > . (91)
max
2m
f
A sufficient condition for convergence is thus the following
(cid:118)
m f (cid:117) (cid:117) (cid:116)m2 f
+
4σ m2 in(M −)
−
m2 f
>
1
. (92)
M M2 σ2 (M ) M2 m
f f max + f f
The last equation can equivalently be written as
(cid:113)
τ−1 τ−2+4τ−2−τ−2 > m−1. (93)
f f G f f
References
SungjinAhn, BabakShahbaba, andMaxWelling. DistributedStochasticGradientMCMC.
In International Conference on Machine Learning, 2014.
Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated
Learning via Posterior Averaging: A New Perspective and Practical Algorithms. In
International Conference on Learning Representations, 2020.
32Distributed MCMC Sampling via ADMM
Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An Intro-
duction to MCMC for Machine Learning. Machine Learning, 50:5–43, 2003.
Heinz Bauschke and Patrick Combettes. Convex Analysis and Monotone Operator Theory
in Hilbert Spaces. Springer, 2011.
Dimitri Bertsekas and John Tsitsiklis. Parallel and Distributed Computation: Numerical
Methods. Athena Scientific, 2015.
Kinjal Bhar, He Bai, Jemin George, and Carl Busart. Distributed Event-Triggered Unad-
justed Langevin Algorithm for Bayesian Learning. Automatica, 156:111221, 2023.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational Inference: A Review for
Statisticians. Journal of the American Statistical Association, 112(518):859–877, 2017.
Nicoletta Bof, Ruggero Carli, and Luca Schenato. Is ADMM Always Faster than Average
Consensus? Automatica, 91:311–315, 2018.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed
OptimizationandStatisticalLearningviatheAlternatingDirectionMethodofMultipliers,
volume 3. Now Publishers, Inc., 2011.
Stephen P Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University
Press, 2004.
Dragoˇs Cvetkovi´c, Peter Rowlinson, and Slobodan K Simi´c. Signless Laplacians of Finite
Graphs. Linear Algebra and its Applications, 423(1):155–171, 2007.
George Dantzig. Linear Programming and Extensions. Princeton University Press, 1963.
Wei Deng, Qian Zhang, Yi-An Ma, Zhao Song, and Guang Lin. On Convergence of Feder-
ated Averaging Langevin Dynamics. arXiv preprint arXiv:2112.05120, 2022.
Steven Diamond and Stephen Boyd. CVXPY: A Python-Embedded Modeling Language
for Convex Optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.
Alain Durmus, Eric Moulines, and Marcelo Pereyra. Efficient Bayesian Computation by
Proximal Markov Chain Monte Carlo: When Langevin Meets Moreau. SIAM Journal on
Imaging Sciences, 11(1):473–506, 2018.
Clark R Givens and Rae Michael Shortt. A Class of Wasserstein Metrics for Probability
Distributions. Michigan Mathematical Journal, 31(2):231–240, 1984.
Mert Gu¨rbu¨zbalaban, Xuefeng Gao, Yuanhan Hu, and Lingjiong Zhu. Decentralized
Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo. Journal of Ma-
chine Learning Research, 22(1):10804–10872, 2021.
AvetikKaragulyanandPeterRicht´arik. ELF:FederatedLangevinAlgorithmswithPrimal,
Dual and Bidirectional Compression. arXiv preprint arXiv:2303.04622, 2023.
33Tzikas et al.
Vyacheslav Kungurtsev, Adam Cobb, Tara Javidi, and Brian Jalaian. Decentralized
Bayesian Learning with Metropolis-Adjusted Hamiltonian Monte Carlo. Machine Learn-
ing, pages 1–29, 2023.
Anusha Lalitha, Xinghan Wang, Osman Kilinc, Yongxi Lu, Tara Javidi, and Fari-
naz Koushanfar. Decentralized Bayesian Learning over Graphs. arXiv preprint
arXiv:1905.10466, 2019.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Con-
vergence of FedAvg on Non-IID Data. In International Conference on Learning Repre-
sentations, 2019.
Gonzalo Mateos, Juan Andr´es Bazerque, and Georgios B Giannakis. Distributed Sparse
Linear Regression. IEEE Transactions on Signal Processing, 58(10):5262–5276, 2010.
Angelia Nedic and Asuman Ozdaglar. Distributed Subgradient Methods for Multi-Agent
Optimization. IEEE Transactions on Automatic Control, 54(1):48–61, 2009.
Willie Neiswanger, Chong Wang, and Eric P Xing. Asymptotically Exact, Embarrassingly
Parallel MCMC. In Conference on Uncertainty in Artificial Intelligence, pages 623–632,
2014.
Anjaly Parayil, He Bai, Jemin George, and Prudhvi Gurram. A Decentralized Approach to
Bayesian Learning. arXiv preprint arXiv:2007.06799, 2021.
Neal Parikh and Stephen Boyd. Proximal Algorithms, volume 1. Now Publishers, Inc.,
2014.
Marcelo Pereyra. Proximal Markov Chain Monte Carlo Algorithms. Statistics and Com-
puting, 26:745–760, 2016.
Lewis J Rendell, Adam M Johansen, Anthony Lee, and Nick Whiteley. Global Consensus
Monte Carlo. Journal of Computational and Graphical Statistics, 30(2):249–259, 2020.
Ernest K Ryu and Wotao Yin. Large-Scale Convex Optimization: Algorithms & Analyses
via Monotone Operators. Cambridge University Press, 2022.
AdilSalim,DmitryKovalev,andPeterRicht´arik. StochasticProximalLangevinAlgorithm:
PotentialSplittingandNonasymptoticRates. AdvancesinNeuralInformationProcessing
Systems, 32, 2019.
Inass Sekkat. Large Scale Bayesian Inference. PhD thesis, E`cole des Ponts ParisTech, 2022.
Wei Shi, Qing Ling, Kun Yuan, Gang Wu, and Wotao Yin. On the Linear Convergence
of the ADMM in Decentralized Consensus Optimization. IEEE Transactions on Signal
Processing, 62(7):1750–1761, 2014.
Ola Shorinwa, Trevor Halsted, Javier Yu, and Mac Schwager. Distributed Optimization
MethodsforMulti-RobotSystems: PartI–ATutorial. arXiv preprint arXiv:2301.11313,
2023a.
34Distributed MCMC Sampling via ADMM
Ola Shorinwa, Trevor Halsted, Javier Yu, and Mac Schwager. Distributed Optimization
Methods for Multi-Robot Systems: Part II – A Survey. arXiv preprint arXiv:2301.11361,
2023b.
Panos Toulis, Thibaut Horel, and Edoardo M Airoldi. The Proximal Robbins–Monro
Method. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83
(1):188–212, 2021.
C´edric Villani and C´edric Villani. The Wasserstein Distances. Optimal Transport: Old and
New, pages 93–111, 2009.
Maxime Vono, Nicolas Dobigeon, and Pierre Chainais. Split-and-Augmented Gibbs Sam-
pler—Application to Large-Scale Inference Problems. IEEE Transactions on Signal Pro-
cessing, 67(6):1648–1661, 2019.
Maxime Vono, Daniel Paulin, and Arnaud Doucet. Efficient MCMC Sampling with
Dimension-Free Convergence Rate using ADMM-type Splitting. Journal of Machine
Learning Research, 23(1):1100–1168, 2022.
Max Welling and Yee W Teh. Bayesian Learning via Stochastic Gradient Langevin Dynam-
ics. In International Conference on Machine Learning, pages 681–688, 2011.
Javier Yu, Joseph A Vincent, and Mac Schwager. DINNO: Distributed Neural Network
Optimization for Multi-Robot Collaborative Learning. IEEE Robotics and Automation
Letters, 7(2):1896–1903, 2022.
Xiang Zhou, Huizhuo Yuan, Chris Junchi Li, and Qingyun Sun. Stochastic Modified Equa-
tions for Continuous Limit of Stochastic ADMM. arXiv preprint arXiv:2003.03532, 2020.
35