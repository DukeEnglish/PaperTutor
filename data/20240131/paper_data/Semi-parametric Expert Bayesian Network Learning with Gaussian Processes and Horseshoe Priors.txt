Semi-parametric Expert Bayesian Network Learning
with Gaussian Processes and Horseshoe Priors
YidouWeng1,FinaleDoshi-Velez2
1NationalUniversityofSingapore
2HarvardUniversity
yidouw@u.nus.edu,finale@seas.harvard.edu
Abstract manandNachman2013).Non-parametricmodelsdonotas-
sumeaspecificdatadistribution,providingmoreflexibility.
ThispaperproposesamodellearningSemi-parametricrela-
However,alimitationisthattheirrepresentationcomplexity
tionshipsinanExpertBayesianNetwork(SEBN)withlinear
oftengrowswiththeamountofavailabledata(Stone1980).
parameter and structure constraints. We use Gaussian Pro-
cesses and a Horseshoe prior to introduce minimal nonlin- Semi-parametricBNcombinesthesimplicityofparametric
ear components. To prioritize modifying the expert graph assumptions when appropriate with the flexibility of non-
over adding new edges, we optimize differential Horseshoe parametricmodelswhennecessary.
scales. In real-world datasets with unknown truth, we gen- Theobjectiveofourworkistolearnsemi-parametricad-
eratediversegraphsto accommodateuserinput,addressing ditions to an EBN to better explain the data, while min-
identifiability issues and enhancing interpretability. Evalua- imally modifying the expert graph. Doing so keeps our
tion on synthetic and UCI Liver Disorders datasets, using
modelalignedwithexpertunderstandingandallowsanex-
metricslikestructuralHammingDistanceandtestlikelihood,
perttoeasilyreviewproposedchanges.Specifically,SEBN
demonstrates our models outperform state-of-the-art semi-
usesGaussianprocesses(GPs)asadditionalcomponentsin
parametricBayesianNetworkmodel.
EBNstocapturenonlinearcomponentsofthemodel.Addi-
tionally,itusesaHorseshoepriortopenalizeexcessiveGP
Introduction
edges,especiallythosenotinthelinearEBN.
In various disciplines, models have been developed to de- On a large number of randomly-generated synthetic ex-
scribephenomenaofinterest.Mechanisticmodelsarepreva- amples, we demonstrate that SEBN can learn minimal
lent in medicine, epidemiology, physics, and other fields. changes and additions to an expert network to maximize
Expert Bayesian Networks (EBNs) serve as powerful tools predictive performance. We optimized the Horseshoe prior
for representing relationships between multiple variables over a spectrum of scales and weights based on validation
andsupportingdecision-making. StructuralHammingDistanceandtestlikelihood.Wefound
However,themostcommonapproach—utilizingstandard that differential Horseshoe priors work the best to penalize
structure learning algorithms for data-driven relationship especially non-expert edges and maintain the expert struc-
discovery—hasdrawbacks.Bayesiannetworks(BN)areof- ture. SEBN outperformed state-of-the-art Semi-Parametric
tennon-identifiable,andexpertknowledgeencapsulatesthe BayesianNetwork(SPBN)(Atienza,Bielza,andLarran˜aga
scientificunderstandingofanentirefield.Especiallyindo- 2022) in learning the correct edges and predicting unseen
mains where expert knowledge and model interpretability data.
arecrucial,andthegoalistoadvancescientificunderstand- LastlyweevalutedSEBNonareal-worlddataset,where
ing,itisvitalthatalearnednetworkextendsandrefinescur- thegroundtruthstructuresarenotknown.Whenthesingle
rent relationships between variables rather than completely rightchangeisnotobvious,weprovideddiversereasonable
rewritingthem. optionsforexperts.AlltheseoptionsoutperformedSPBNin
However, EBNs do have limitations: experts may face held-outlikelihood.
challengesinspecifyingedgesbetweenvariablesanddefin-
ingtheparametricformoftherelationshipalonganedge,es- RelatedWork
pecially for nonlinear relationships. This becomes a signif-
ExpertGraphAsHardandSoftConstraints Expertbe-
icant restriction when dealing with domains featuring non-
liefs can be incorporated into Bayesian Network Structure
linearinteractionsorwherethenatureofinteractionsisun-
learningashardorsoftconstraints.
known.
De Campos, Zeng, and Ji (2009) and Tsamardinos,
AnapproachunderexplorationislearningBNconditional
Brown,andAliferis(2006)usedexpertgraphsashardcon-
probabilitydistributions(CPDs)withnon-parametricdensi-
straints–nodeletionsand/oradditions.Theseworksarelim-
ties (Hofmann and Tresp 1995; Ickstadt et al. 2011; Fried-
ited to discrete or linear settings. While we preserve linear
Copyright©2024,AssociationfortheAdvancementofArtificial expert edges as hard constraints, we allow the addition of
Intelligence(www.aaai.org).Allrightsreserved. nonlinearcomponents.Thisapproachenablescapturingnu-
4202
naJ
92
]GL.sc[
1v91461.1042:viXraanced relationship in the data that may be challenging for thresholdtestingandregularization.Secondly,asaBayesian
expertstospecify. method,GPsincorporatedomainknowledgethroughchoice
Expert beliefs have also been incorporated a priori as ofcovariancefunctionsorhyperparameterpriors.Thisfacil-
softconstraints.Heckerman,Geiger,andChickering(1995) itatesexpert-guidedlearningandposteriorprobabilityinter-
penalized differences between expert graph and posterior pretationfornetworkstructures.
structure using a single confidence value for all specified Thirdly,GPslearnhyperparametersbothlocallyandglob-
edges.Inourmodel,whenpenalizingtheadditionofnonlin- ally (Piironen and Vehtari 2017). Locally, for each parent,
earcomponents,weallowforafiner-grainedspecificationof theGPmodeladoptsadifferentsetofhyperparametersand
confidencetopenalizevariousedgetypesdifferently. priors. Globally, an overarching regularization level for a
specificnodecanbeposedbyaprioronallitshyperparam-
Parametric, Nonparametric and Semiparametric
eters.
Bayesian Network Parametric continuous Bayesian
Networks, such as Gaussian Bayesian Networks (GBN), Regularizing Gaussian Process with Horseshoe Prior
are limited to learning linear dependencies among vari- To preserve the expert structure and manage model com-
ables. This restriction becomes significant in domains with plexity, regularization methods have been explored. Com-
non-linearinteractionsorunknowninteractionnature. mon approaches involve statistical testing to prune edges
When parametric assumptions are not met, the perfor- (Boukabour and Masmoudi 2021) and regularization terms
mance of parametric models may suffer. Nonparametric on structure complexity or differences, such as BDe score
methods,suchassplines(Imotoetal.2003),andKerneles- (Heckerman,Geiger,andChickering1995).
timators(GunnandKandola2002),havebeenexplored. GaussianProcesses,inherentlybiasedtowardidentifying
Alimitationofnon-parametricmodelsisthattheirrepre- functional dependencies, can be effectively and efficiently
sentationcomplexityusuallygrowingwiththedataavailable regularized by applying priors over their amplitudes. We
(Atienza, Larran˜aga, and Bielza 2022). Semi-parametric optedfortheHorseshoe(HS)priorforitsheavytailsandca-
methods,consideringsomepartsasparametricandothersas pacity to accommodate both zero and large values (Ghosh,
nonparametric,offergreaterprecisionformultidimensional Yao, and Doshi-Velez 2019). The heavy tails of HS effec-
cases(Gentle,Ha¨rdle,andMori2012). tivelydeactivateGPwithasmallamplitude,whileGPswith
Boukabour and Masmoudi (2021) and Atienza, alargeamplitudecanbeinterpretedasactive.
Larran˜aga, and Bielza (2022) proposed Semi-parametric
Bayesian Network as SBN and SPBN, respectively, out- BackgroundandNotation
performing parametric BN in predicting complicated
Bayesian Network A Bayesian network is represented
relationship. However, they restrict nodes to be exclusively
by a directed acyclic graph (DAG) G with nodes V =
eitherlinearornon-parametric,posingachallengeforincor-
{1,...,n} and directed edges. This network provides a
porating expert graphs. Graphs with mostly nonparametric
structure for factorizing the joint probability distribution
nodes tend to be sparse and significantly differ from linear
P(x) involving n random variables x = (x ,...,x ). θ is
EBN, making the learned nonparametric relationships hard 1 n
thesetofparameters,withθ denotingthesetofparameters
to interpret. In contrast, our approach introduces flexibility i
for the Conditional Probability Distribution (CPD) of node
by allowing both linear and non-linear components for
i.
each edge. We align the linear component with expert
belief, while learning non-linear components to capture Parameter Learning Assuming a fixed structure for a
nuances potentially missed by the expert. With the learned BN,wherethegraph’sedgesandthetypeofCPDforeach
structurelargelyconsistentwiththeexpertgraph,ourmodel nodeareknown,weestimatetheparametersforeachnode’s
facilitateseasyinterpretationofaddednon-linearedges. CPDtocompletethemodelusingmaximumlikelihoodcri-
AnotherlimitationofSBNisthatitoptimizeslinearand terion.
non-linearcomponentsseparately,whichnotberesultinthe GivenN independentandidenticallydistributedsamples
globaloptimum.Wecotrainbothcomponents,outperform- D ={x1,...,xN},thelikelihoodfunctionisdefinedas:
ingtheoriginaltwo-steplearningprocess.
N n
Nonparametric Method: Gaussian Process Nonpara- P(D|θ,G)= (cid:89)(cid:89) P(xj|θ ,xj ),
metric BN estimate all CPDs with nonparametric meth- i i Pa(i)
j=1i=1
ods. Various nonparametric methods have been explored
wherex isthesetoftheparentsofnodei.Theglobal
for nonparametric BN, such as Kernel Density Estimation Pa(i)
likelihooddecompositionpropertyensuresthattomaximize
(KDE)models(HofmannandTresp1995),aninfinitemix-
P(D|θ,G),wecanmaximize
turemodel(Ickstadtetal.2011),theNadaraya–Watsonesti-
mator(Nadaraya1964;Watson1964;BoukabourandMas-
P (D|θ,G)=P(X |θ ,X )
i i i Pa(i)
moudi2021),andGaussianProcesses(GPs)(Friedmanand
Nachman2013).Inourwork,weuseGPstomodelnonlin- independentlyforeachnodei.
earcomponents. To prevent overfitting, we use the early-stopping crite-
GPs provide flexible learning of continuous dependen- rion (Prechelt 2002). We randomly split the data D into
cies with advantages over other non-linear models. Firstly, twodisjointdatasetscalledthetrainingandvalidationsets,
GPshaveanamplitudehyperparameterthatcanbeusedfor D = D ∪ D . The parameter optimization on D
train val traincontinues until the point where θ that improves the likeli- Integratingthelikelihoodtimestheprior,thelogmarginal
i
hoodP(D |θ,G)cannotbefoundforλiterations.Here,λ likelihoodisgivenby:
val
represents the patience parameter. When λ > 0, the search
is permitted to explore less favorable parameters for up to logp(x1:N |x1:N
)=−1(cid:0)
(K+σ2I)−1
i Pa(i) 2 n
λ iterations. This approach helps mitigate the risk of being
trappedatlocalmaxima. + logdet(K+σ
n2I)+Nlog(2π)(cid:1)
HorseshoePrior TheHorseshoepriorissetfortheampli-
Structure Learning Structure learning methods can be
tudesσ2whereτ isthelocalscale.
broadly categorized into constraint-based and score-based
approaches.Constraint-basedmethodsuseconditionalinde-
σ2 |τ ∼N(0,τ2I), τ ∼C+(0,b)
pendence tests to eliminate and orient edges. Score-based
Specifically,thelogarithmoftheprobabilitydensityfunc-
methods, on the other hand, aim to maximize an objective
tion for the amplitudes for p parents with respect to τ is
functionoverdifferentgraphs.Score-basedmethodscanbe
givenby:
further classified into approximate and exact search meth-
ods.Approximatemethods,likegreedyhill-climbing,itera-
tivelymodifyagraph’sstructuretomaximizeascorefunc- (cid:88)p (cid:32) τ (cid:33)
tion.Incontrast,exactalgorithmsguaranteethereturnofthe logP(σ2 |τ)= log (cid:112)
(σ2)2+τ2
highest-scoringDAGbytreatingstructurelearningasacon- i=1 i
strained combinatorial optimization problem. (Kitson et al. (cid:88)p (cid:18) τ2 (cid:19) p
2023)Weuseanexactscorealgorithm. − log 1+ − log(2π)
(σ2)2 2
i=1 i
Gaussian Process We model the non-linear relationship
Thevalueofτ determinesthelevelofregularization:with
between x and its candidate parents x as a Gaussian
i Pa(i) alargeτ,allthevariableshaveverydiffusepriorswithvery
Process(GP)(FriedmanandNachman2013).
little shrinkage towards zero, but letting τ → 0 will shrink
AstochasticprocessoverX isafunctionthatassigns
Pa(i) the amplitude σ2 to zero (Piironen and Vehtari 2017). We
to each x ∈ X a random variable x . The process
Pa(i) Pa(i) i willexplorearangeofHSscalestoproducedifferentregu-
is said to be a GP if for each finite set of value x1 Pa:N (i), the larizationandthusvaryinglearnedstructures.
distribution over the corresponding random variables x1:N
i
is a multivariate normal distribution, whose mean and co- Model
variance functions we specify as E[x i] = µ(x Pa(i)) and Wemodelobservedvaluesx
i
ofeachnodeiwithcandidate
Cov[x ,x′] = K(x ,x′ ). As the mean can be con- parentsx inaBayesiannetworkas:
i i Pa(i) Pa(i) Pa(i)
sideredseparatelyasalinearcomponent,forsimplicityand (cid:88)
x =w·x +b+ f (x )+ϵ
withoutthelossofgenerality,wecanassumemeanofGPis i Pa(i) i j i
0.Thejointdistributionofx1:N istherefore: xj∈x Pa(i)
i
wherew·x +bisthelinearterm,f (x)isazero-mean
Pa(i) i
GPwithSEcovariancefunctionwithhyperparametersam-
(cid:18) (cid:19)
P(x1:N | x1:N ) ∝ exp −1 (x1:N)TK−1 x1:N plitudeσ2andlengthscalelandϵ iisthenoisewithassumed
i Pa(i) 2 i 1:N i knownvarianceσ2.
n
where J is the vector of means
1:N
(µ(x1 ),...,µ(xM )) and K is the covariance Learning
Pa(i) Pa(i) 1:N
matrixwiththe(p,q)-thentryK(xp ,xq ). Parameter Learning Using global likelihood decompo-
Pa(i) Pa(i) sition,tomaximizeP(D |θ,G),weindependentlymaxi-
Thecovariancebetweenpointsdeterminespropertieslike train
mizeP (D |θ,G)=P(X |θ ,X )foreachnodei.
periodicity,smoothness,andamplitudeofthelearnedfunc- i train i i Pa(i)
The posterior likelihood is the sum of probability of X
tions. These aspects of the covariance functions are con- i
given X and of the learned GP amplitude given its
trolledbyitshyperparameters.Forexample,intheSquared Pa(i)
Horseshoeprior,inlogarithm:
Exponential (SE) function, one of the mostly commonly
usedcovariancefunctions(WilliamsandRasmussen2006):
logP(X |X ,τ)=logP(X |X ,σ2)+w logP(σ2|τ)
(cid:32) 1(cid:32) (xp −xq )2(cid:33)(cid:33) i Pa(i) i Pa(i) HS
k(xp ,xq )=σ2exp − Pa(i) Pa(i) where w HS is the weight for the Horseshoe prior term.
Pa(i) Pa(i) 2 l2 The prior term is weighted to have a reasonable effect on
the likelihood as sample size increases. The parameters to
The hyperparameter σ2 controls the amplitude of variation
be optimized include the linear coefficients, as well as GP
of the function, and the length scale l controls the smooth-
lengthscalesandamplitudes.
ness/wigglenessofthefunction.Eachparentjofnodeihas
Wecomparetwolearningmodes,eithertwo-steporone-
itsownsetofhyperparametersσ2andl .
j j step. In two-step learning, the linear parameters are deter-
In noisy observations, assuming additive independent
minedbeforehand,eithersettogroundtruthorfittedassum-
identically distributed Gaussian noise ϵ with variance σ2,
n ing LGBN. Then they are fixed throughout optimizing GP
theprioronobservationsis:
parameters solely. In one-step learning, linear and GP pa-
cov(X )=K+σ2I. rametersareoptimizedtogether.
i nStructure Learning Parameter learning removes candi- ExperimentalSetup
dateparentswithlearnedGPamplitudessmallerthanade-
Synthetic Datasets We created two synthetic datasets.
finedthreshold.Inotherword,ourmodelconsidersallpos-
Both allow either modifying expert linear edges to contain
sibleparentsandretainsonlythosedeemedsignificant.This
GP components or adding new GP edges, with the second
approacheliminatestheneedtoiteratethroughedgeopera-
datasetfavoringmodification.
tions in approximate search algorithm. Exact search is fur-
ther plausible, given that the acyclic constraint imposed by 1. Independent-addition Dataset (ID): A dataset with a
the partial topological order provided by the expert graph uniformprobabilityof0.5formodificationandaddition.
substantiallyreducesthesearchspace.
2. Expert-guided Dataset (ED): A dataset where there’s
We implement an exact search in node-ordering space
a higher likelihood of observing nonlinearity in edges
modifying Dynamic Programming (DP). DP breaks down
specifiedbyexperts.Specifically,eachexpertedgehasa
theproblembysolvingsmallsub-problemsfirst.EveryDAG
probabilityof0.5formodificationand0.01foraddition.
musthaveatleastoneleafnode.ADAGwithnodesX can
be constructed from a leaf node X leaf and a sub-DAG with Generate random structure. Following the topological
nodesX−{X leaf}(SinghandMoore2005).Themaximum orderofnodes,foreachcurrentnode:
graphscoreisexpressedasarecurrencerelation:
1. Randomlysamplefromitsancestorstobeitslinearpar-
entswitha0.5probability.
score (X)=score (X−{X })
max max leaf 2. RandomlysamplefromitsancestorstobeitsGPparents
+score (X |Pa(X ))
max leaf leaf withcorrespondingprobabilityasspecifiedinIDorED
DPexploitsthisrecurrencerelationship,ensuringaguar- case.
anteed search for the highest-scoring DAG. DP is feasible
Setupparameters
forn ≤ 26andisusefulforlearningmoderatelysizednet-
works(SinghandMoore2005).Additionally,tosavecom- • Theinterceptβ =0
0
putationalcost,weprunepathsthatviolatethepartialtopo-
• Foreachlinearcomponentcoefficientβ =1
j
logicalorderprovidedbyexperts.
Onlynodeslastinthepartialtopologicalorderinthecur- • Foreachnon-linearcomponentγ j =cos2πX j
rent subgraph can be leaf nodes, as all other nodes can be • Thenoisevarianceσ2 ∼0.01
n
theirparentswithoutcreatingcycles.Thisensuresconsider-
ation of all possible edges without violating the hard con- Exampleofasyntheticdataset
straint. Note that only expert linear edges, not learned GP
0 0 0 0 0
edges, are hard constraints. Hence, pruning away insignifi-
0 0 0 0 0
cantGPedgesbasedonamplitudedoesnotaffectthesound-  
L=1 1 0 0 0
nessofourmodifiedDPalgorithm.  
1 1 1 0 0
The pseudocode for DP is as follows (Singh and Moore
1 0 0 0 0
2005):
InmatrixL,foreachrowiandforeachcolumnj where
Algorithm 1: OptOrd(S) – Store the score of the best net-
j <i,theonesrepresentlinearedgesfromX toX .
workonSinCache(S) j i
1: bestscore←∞ 0 0 0 0 0
2: ifS =∅then 0 0 0 0 0
 
3: return0.0 G =1 1 0 0 0
 
4: endif 1 0 0 0 0
5: forx∈S∧x∈LeafNodedo 1 1 0 1 0
6: ifCached(S−x)then
7: s←Cache(S−x) InmatrixG,foreachrowiandforeachcolumnj where
j <i,theonesrepresentnon-linearedgesfromX toX .
8: else j i
9: s←OptOrd(S−x) ThesematricesoflinearandGPcomponentsreflectagen-
10: endif eratedBNstructurewiththefollowingrelationships:
11: s←s+BestScore(S,x)
12: ifbestscore>sthen
X ∼N(0,0.1)
13: Leaf(S)←x 1
14: bestscore←s X 2 ∼N(0,0.1)
15: endif X =X +X +cos(2πX )+cos(2πX )+N(0,0.1)
3 1 2 1 2
16: Cache(S)←bestscore
X =X +X +X +cos(2πX )+N(0,0.1)
17: endfor 4 1 2 3 1
X =X +cos(2πX )+cos(2πX )+cos(2πX )+N(0,0.1)
18: returnbestscore 5 1 1 2 4SHD for Two- and One-stage Learning for Different Number of Nodes Test Likelihood for Two- and One-stage Learning for Different Number of Nodes
SHD for SEBN with or without Prior and SPBN for Different Number of Nodes
Number of Nodes: 6 90
01 .. 80 N N N N Nu u u u um m m m mb b b b be e e e er r r r
r
o o o o of f f f
f
N N N N No o o o od d d d de e e e es s s s s: : : :
:
7 8 9 1 110 788 505 22 .. 05 N N N N N Nu u u u u um m m m m mb b b b b be e e e e er r r r r
r
o o o o o of f f f f
f
N N N N N No o o o o od d d d d de e e e e es s s s s s: : : : :
:
6 7 8 9 1 110
Number of Nodes: 12 Number of Nodes: 12
0.6 70 Number of Nodes: 6 1.5
65 Number of Nodes: 7
0.4 60 N Nu um mb be er r o of f N No od de es s: : 8 9 1.0
Number of Nodes: 10
0.2 55 05 N Nu um mb be er r o of f N No od de es s: : 1 11 2 0.5
Train GP with Truth LIN Cotrain LIN+GP Train GP with Fixed LIN 0.0
0.0 Models No HS HS5 SPBN
Train GP with Truth LIN Cotrain LIN+GP Train GP with Fixed LIN Models
Models (b)TestLikelihoodvs.NumberOfNodes
(c)SHDvs.NumberofNodesforSPBN
(a)SHDvs.NumberOfNodesformodels for models with ground-truth linear pa-
baselineandSEBNwithnoHorseshoe,or
withground-truthlinearparameters,two- rameters, two-step trained linear and GP
with Horseshoe prior scale τ = 5 and
stage trained linear and GP parameters, parameters,andco-trainedlinearandGP
weightw =1.
andco-trainedlinearandGPparameters. parameters. HS
Figure 1: In Independent-addition Dataset, (a) and (b) At all network sizes, jointly-training linear and GP parameters got
about50%closertotheoraclesolutionthanthetwo-stageapproach.(c)SEBN,withorwithoutHorseshoeprior,significantly
outperformedthestate-of-the-artbaselineSPBN
Dataset Generation and Sampling To compare the per- forallnodesisnonparametric,i.e.,theCPDsareKDEs.The
formance of the models, we generated 100 different syn- startinggraphhadnoedges.Patienceλis15.
theticBayesiannetworkswith6≤n≤12continuousvari-
Optimization Expert graph is given as ground truth in
ablesandatmostn·(n−1)/2edges.ForeachBayesiannet-
synthetic dataset and learned using the Linear Gaussian
work,wesampled500instancesfortraining,100instances
Bayesian Network (LGBN) algorithm (Lauritzen and Wer-
forevaluationandanother100fortest.
muth1989)inUCIdataset.Noiselevelsareassumedknown
UCI Liver Disorders From the UCI Machine Learning for each variable. Initial values of GP amplitudes and GP
Repository,wechosetheLiverDisordersdataset(mis1990), lengthscalesweresetto0.2and0.4,respectively,approxi-
which comprises 345 samples of 7 continuous variables. matingthegroundtruth.ToassessedtheeffectoftheHorse-
Oneconstantvariablewasexcludedforanalysis.Thetrain- shoe prior on GP and identify the optimal scale, we ex-
ingandtestsampleswererandomlysplitwitharatioof9:1. perimented with no prior, or a range of Horseshoe scales
from 1 to 20. Additionally, we experimented with Horse-
Baselines
shoe weights of 1 and 10 to determine the most effective
regularizationlevel.
Two-stepLearningBaseline Wecomparedtheperfor- A small GP amplitude suggests minimal non-parametric
mance of determining linear components first and fixing variation, indicating a potential lack of need for GP edges.
them throughout optimizing GP components solely, as op- Consequently, a lower limit was set. The optimal threshold
posedtocotrainingthem. wastunedto0.2forIndependent-additionDataset(ID),0.1
Given Ground-Truth Linear Terms: In this approach, we forExpert-guidedDataset(ED),and0.01forUCILiverDis-
assumeddomainexpertsprovidedthelinearterms,eliminat- orders.
ingtheneedtolearnthemfromthedata.Theoracleensures We performed parameter optimization using GPyTorch.
GPcomponentsonlyfitnonlinearityinthedata,creatingan The maximum number of iterations is 200, with a patience
idealscenariotoassessupperlimitperformance. parameter(λ)of20.Thebestmodelisdeterminedbasedon
Fitted and Fixed Linear Terms: In the second approach, thehighestlikelihoodinthevalidationsetD .
val
we fitted the linear terms to the data and learned the GP
Metrics
based on the fitted and fixed linear terms. This allowed us
to obtain the best possible linear terms for our dataset and
learnaGPthatcapturedtheremainingnon-lineartrends.A Structural Hamming Distance (SHD): SHD
potentialissueisthatthefittedlineartermsmayattemptto (Tsamardinos, Brown, and Aliferis 2006) measures the
explain nonlinear components, hindering learning the true numberofedgeadditions,removals,orreversalsrequiredto
GPparameters. transform one graph into another. SHD results are reported
asmeans.
SPBN Baseline SPBN-CKDE (Atienza, Bielza, and
Larran˜aga 2022) learns nodes as either parametric (linear TestLikelihood: Testlikelihoodestimatestheexpected
Gaussian,LG)ornon-parametric(KDE)type.Theparamet- performanceofthemodelsonnewandunseendatabycal-
ricandnonparametriccomponentsareoptimizedtogetherin culatingthelog-likelihoodofthetestdataset.All”testlikeli-
a modified structure learning hill climbing algorithm with hood”intheResultssectionarelog-likelihoodandreported
anextraoperator–nodetypechange.Theinitialnodetype asmedians.
ecnatsiD
gnimmaH
larutcurtS
doohilekiL
tseT
ecnatsiD
gnimmaH
larutcurtSSHD for SEBN with HS Priors of Various Weights
SHD for SEBN with No Prior or HS Priors with Varying Scales 0.5 Number of Nodes: 6
Number of Nodes: 7
0.6 Number of Nodes: 6 Number of Nodes: 8
Number of Nodes: 7 Number of Nodes: 9 Number of Nodes: 8 0.4
0.5 Number of Nodes: 9 Number of Nodes: 10
Number of Nodes: 11
Number of Nodes: 10 Number of Nodes: 12
0.4 Number of Nodes: 11 0.3 Number of Nodes: 12
0.3
0.2
0.2
0.1
0.1
0.0
0.0 No HS HS5 W1 HS5 W10
NoHS HS1 HS5 HS20
Models
Models
(c) SHD vs. Number of Nodes for models with
(a)SHDvs.NumberofNodesformodelswithoutHorse-
Horseshoepriorscaleτ = 5,eachwithaweight
shoepriororwithaHorseshoepriorscaleτ = 1,5,20,
w =1,10.
eachwithw S =1. HS
H
Test Likelihood for SEBN with HS Priors of Various Weights
Test Likelihood for SEBN with No Prior or HS Priors with Varying Scales
90 90
85 85
80 80
75 75
70 Number of Nodes: 6
Number of Nodes: 7 70 Number of Nodes: 6
65 Number of Nodes: 8 Number of Nodes: 7
Number of Nodes: 9 65 Number of Nodes: 8
60 Number of Nodes: 10 Number of Nodes: 9
Number of Nodes: 11 60 Number of Nodes: 10
55 Number of Nodes: 12 Number of Nodes: 11
No HS HS1 HS5 HS20 55 Number of Nodes: 12
Models
No HS HS5 W1 HS5 W10
(b)TestLikelihoodvs.NumberofNodesformod- Models
els without Horseshoe prior or with a Horseshoe
(d)TestLikelihoodvs.NumberofNodesformod-
priorscaleτ =1,5,20,eachwithw S =1.
H elswithaweightw = 1orw = 10,each
HS HS
withHorseshoepriorscaleτ =5
Figure 2: In Independent-addition dataset, (a) and (b): In SHD and Test likelihood, all models with the Horseshoe prior out-
performedonewithout.TheperformancetrendsacrossdifferentHorseshoescaleshadaU-shapedpattern.Toosmallorlarge
scalespenalizedGPedgestooseverelyorlightly,resultinginincorrectstructure.τ =5emergedastheoptimalchoice.(c)and
(d):InSHDandTestlikelihood,w =1consistentlyoutperformedw =10.
HS HS
Results scales on learning the true structure. All models with the
Horseshoepriordemonstratedimprovedperformancecom-
Finding Structure: In the independent-addition setting,
paredtothemodelwithoutit.Theperformancetrendsacross
ourSEBNaddedaminimalnumberofGPedgesacross
different Horseshoe scales had a U-shaped pattern. Too
allnetworksizes. Figure1(a)showstheStructuralHam-
small or large scales penalized GP edges too severely or
ming Distance (SHD) as the network size grows for our
lightly,resultinginincorrectstructure.Forourindependent
SEBNapproach,theoraclewiththeground-truthlinearpa-
addition dataset, a Horseshoe prior scale of 5 emerged as
rameters(wouldnotbeavailableinrealsettings),andatwo-
the optimal choice. That said, the results were fairly stable
stageprocessthatfitsthelinearmodelbeforethenonlinear
acrossscales,suggestingthatthisparametershouldberela-
model.Overall,SHDincreasedwiththenetworksize.Atall
tivelyeasytotune.
network sizes, our jointly-trained approach got about 50%
closer to the oracle solution than the two-stage approach. Additionally, the heavier regularization (τ = 1) yielded
We used the jointly-trained approach for the remaining ex- better performance in smaller networks. Conversely, in
periments. larger networks, lighter regularization (τ = 20) performed
TheSEBNmodelinFigure1(a)wasnotregularizedbya better, although results were somewhat mixed. A detailed
Horseshoeprior. analysisofindividuallearnedmodelsacrossvariousnetwork
Figure2(a)showstheeffectofdifferentHorseshoeprior sizes revealed that in smaller networks, our SEBN learned
ecnatsiD
gnimmaH
larutcurtS
doohilekiL
tseT
ecnatsiD
gnimmaH
larutcurtS
doohilekiL
tseTSHD for SEBN with Uniform or Differential HS Scales
0.5 Test Likelihood for SEBN with Uniform or Differential HS Scales
Number of Nodes: 6
90.0
Number of Nodes: 7
Number of Nodes: 8
0.4 Number of Nodes: 9 87.5
Number of Nodes: 10
Number of Nodes: 11 85.0
0.3 Number of Nodes: 12
82.5
Number of Nodes: 6
0.2 80.0 Number of Nodes: 7
Number of Nodes: 8
77.5 Number of Nodes: 9
0.1 Number of Nodes: 10
75.0 Number of Nodes: 11
Number of Nodes: 12
0.0 NOHS HS5 HS5HS0.001 HS0.001
No HS HS5 HS5HS0.001 HS0.001 Models
Models
(b)TestLikelihoodvs.NumberofNodesforfour
(a) SHD vs. Number of Nodes for four models: models: one with no Horseshoe prior, one with a
onewithnoHorseshoeprior,onewithadifferen- differentialHorseshoepriorscalesτ =5forexpert
tialHorseshoepriorscalesτ =5forexpertedges edges and τ = 0.001 for non-expert edges, and
andτ = 0.001fornon-expertedges,andanother another two with a uniform scale τ = 5 or τ =
twowithauniformscaleτ = 5orτ = 0.001for 0.001forallpossibleedges.
allpossibleedges.
Figure3:InExpert-guideddataset,HorseshoepriorimprovedSHD(a)andtestlikelihood(b)inmostcases.Themodelwith
differentialHorseshoepriorscalesconsistentlyoutperformsthosewithuniformscales,beittoosmall(τ =0.001)ortoolarge
(τ =5).
extra edges, potentially overfitting to the training data. In of Horseshoe prior w = 1 consistently outperformed
HS
more complex networks, our SEBN learned a combination w =10(Fig.2(d)).
HS
ofextraandmissingedges,andasaresult,lighterregulariza-
tion (τ = 20) showed relatively better performance. A suit- In expert-guided setting, our SEBN with differential
ablemiddleground,achievedwithτ = 5,strikesabalance Horseshoe scales correctly prioritizes modifying expert
betweenextraandmissingedges. edgesoveraddingextraedgesandexplainsthedatabet-
Besides scales, Fig. 2(c) showed the impact of different ter. For Expert-guided Dataset (ED), Fig. 3(a) and Fig.
weights on the Horseshoe prior w . w = 1 consis- 3(b)showSHDandtestlikelihoodforSEBNwithnoHorse-
HS HS
tently outperformed w = 10, especially in larger net- shoe prior or a prior with a uniform scale τ = 5 or τ =
HS
works.Thisresultalignedwithourearlierobservationsthat 0.001, or differential scales τ = 5 for expert edges and
in complex networks, lighter regularization showed better
τ =0.001fornon-expertedges.
performance.Consequently,weusedw = 1forallsub- Horseshoe prior improved SHD and test likelihood in
HS
sequentresults. most cases. Further, the model with differential Horseshoe
Lastly, our SEBN, with or without Horseshoe prior, sig- prior scales consistently outperforms those with uniform
nificantly outperformed the state-of-the-art baseline SPBN scales,beittoosmall(τ =0.001)ortoolarge(τ =5).
(Fig. 1(c)). SEBN, but not SPBN, allows edges to contain The models ranked differently in SHD and Test Like-
both linear and GP components. This feature aligns well lihood. The model with a uniform scale τ = 0.001 per-
withoursyntheticdatasetting,whichaimstocloselymimic formsrelativelyworseinTestLikelihoodthanSHD,indicat-
real-worldscenarios. ingthatlargeregularizationprunesexcessiveedgesbutalso
pushesparametersdown,resultinginanacceptablestructure
In the independent-addition setting, our SEBN predicts butincorrectparameters.Conversely,themodelwithauni-
unseendatawell. Figure1(b)showsthetestlikelihoodas form scale τ = 5 performs relatively worse in SHD than
thenetworksizegrowsforourSEBNapproach,theoracle, Test Likelihood, suggesting that small regularization tends
and a two-stage process. Mirroring the SHD results, over- to learn more edges, leading to an incorrect structure but a
all, test likelihood increased with the network size. At all hightestlikelihood.
network sizes, our jointly-trained approach got about 50%
closertotheoraclesolutionthanthetwo-stageapproach. On UCI Liver Disorders Dataset, SEBN generated di-
Similarly, Fig. 2(b) shows the predictive performance verse graphs with varying Horseshoe scale settings. A
of different Horseshoe prior scales. The incorporation of smallerHorseshoescaleaddedfewerGPedges,anddif-
Horseshoepriorimprovedtestlikelihood.Theperformances ferentialscalesimprovedtestlikelihood.AllSEBNmod-
across various Horseshoe scales had an inverse U-shaped elsoutperformedSPBNintestlikelihood. ForUCILiver
pattern, indicating suboptimal performance for both ex- Disorders dataset, ”expert graph” learned using the Linear
cessively small and large scales. Furthermore, the weight Gaussian Bayesian Network (LGBN) algorithm (Lauritzen
ecnatsiD
gnimmaH
larutcurtS
doohilekiL
tseTalkphos alkphos alkphos
sgpt sgpt sgpt
sgot sgot sgot
drinks drinks drinks
mcv gammagt mcv gammagt mcv gammagt
(a)Expertgraph (b)NoHorseshoe (c)UniformHSscaleτ =5
alkphos
alkphos alkphos
sgpt
sgpt sgpt
sgot
sgot sgot
drinks
drinks drinks
mcv gammagt
mcv gammagt mcv gammagt
(d) Differential HS scales τ = 5
(e)UniformHSscaleτ =5 (f)SPBN
andτ =0.001
Figure4:LearnedgraphsforUCILiverDisorders:(a)ExpertgraphlearnedasalinearGBN.(b)GraphlearnedwithNoHS,
incorporating 6 additional GP edges with a test likelihood of -72.31. (c) Uniform HS scale τ = 5 , adding 3 GP edges and
achieving a test log likelihood of -72.31. (d) Differential HS scales τ = 5 and τ = 0.001 for expert and non-expert graphs
respectively,introducing3GPedgeswithatestlikelihoodof-60.56.(e)UniformHSscaleτ =0.001,adding1GPedgewith
atestlikelihoodof-60.57.(f)SPBN,learning4non-parametricedgeswithatestlikelihoodof-213.06.
and Wermuth 1989) consists of six edges in the six-node DiscussionandConclusion
BN (Fig. 4(a)). SEBN learned four candidate graphs with
We proposed a model for learning both linear and nonlin-
no Horseshoe prior (Fig. 4(b)), a uniform Horseshoe prior
ear edges within the Expert Bayesian Network framework.
τ = 5 (Fig. 4(c)) or τ = 0.001 (Fig. 4(d)), and differ-
Specifically,weemployedGaussianProcesses(GP)tocap-
ential Horseshoe prior scales τ = 5 for expert edges and
ture non-parametric nuances in data that are challenging to
τ =0.001fornon-expertedges(Fig.4(e)).
specifybyexperts.GPsoffertheadvantageofhavinganam-
The use of a uniform Horseshoe prior, and more signif- plitudeparametertoindicateedgestrengthandallowforef-
icantly, one with a smaller scale, resulted in reduced addi- fectiveregularization,aswellastheabilitytoindependently
tionsofGPedgesandimprovedtestlikelihood. learnhyperparametersforeachparent.
ToregularizeGPamplitudes,weintroducedaHorseshoe
Differential Horseshoe prior scales further improved test
likelihood. Although models with τ = 5 and differential prior.Horseshoeprior,withitsthick-taileddensitydistribu-
scalesτ = 5orτ = 0.001learnthesamenumberofedges, tion, can deactivate weak GP edges and retain significant
ones. We optimized the Horseshoe prior parameters based
the latter had a higher test likelihood. Differential scales,
on Structural Hamming Distance (SHD) and test likeli-
which regularizedparameters morestrongly for non-expert
hoodinsyntheticdata.Inreal-worldscenarioswhereground
edges,performedbetterinpredictingunseendata.
truthisunknown,weproducedarangeofcandidatelearned
The baseline model, SPBN, learned a structure signifi-
graphs with varying regularization, providing enhanced in-
cantlydifferentfromourmodels(Fig.4(f)),asitcontained
terpretability.
non-parametric edges only. While a direct structural com-
Leveraging GP hyperparameter independence for each
parison lacks a common standard, SEBN consistently and
parent, we developed an exact structure learning algorithm
significantlyoutperformtheSPBNmodelintestlikelihood
basedonDynamicProgramming.Ouralgorithmefficiently
(Table1).
prunes the search space with the acyclic constraint derived
fromthepartialtopologicalorderprovidedbyexperts.
Ourapproachhaslimitations.Theexactsearchalgorithm
Model TestLikelihood
hasrelativehighcomputationalcostsandhindersintegration
SPBN -213.06
commonpackagesthatuseapproximatescoremethods.Fu-
NoHS -72.31
tureworkwillfocusonenhancingcomputationalefficiency
HS5 -72.31
andscalability.
HS5+HS0.001 -60.56
Gaussian Processes offer promise in modeling non-
HS0.001 -60.57
parametric components with regularization, as their hyper-
Table 1: Test likelihoods of SPBN and our models across parameters provide information on edge inclusion. While
Horseshoescales. amplitudeintuitivelysignalsedgestrength,thelengthscale
can also serve as a threshold for detecting nonlinearity. A
large length scale suggests linearity in the variable’s scaleand thus a lack of need for nonlinear components. A small Ickstadt, K.; Bornkamp, B.; Grzegorczyk, M.; Wieczorek,
lengthscaleimpliesexcessivewiggling,potentiallyoverfit- J.; Rahuman Sheriff, M.; Grecco, H.; and Zamir, E. 2011.
tingthenoise.Furthertheoreticalproofandoptimizationof Nonparametric Bayesian networks. Bayesian. Statistics, 9:
thesethresholdsarerequired. 1–40.
Beyondthefullyobservedcase,weintendtoexplorepar- Imoto, S.; Kim, S.; Goto, T.; Aburatani, S.; Tashiro, K.;
tiallyobservedsettings.Ourgoalistorecoverthejointdis- Kuhara, S.; and Miyano, S. 2003. Bayesian network
tributionovertheobservedvariableswithminimalchanges and nonparametric heteroscedastic regression for nonlinear
to the expert graph, while maintaining computational effi- modelingofgeneticnetwork. Journalofbioinformaticsand
ciency. computationalbiology,1(02):231–252.
Ourmodelisavaluableandinterpretabletoolforhealth-
Kitson, N. K.; Constantinou, A. C.; Guo, Z.; Liu, Y.; and
careprofessionalsandresearchers.Itcapturesnuancednon-
Chobtham, K. 2023. A survey of Bayesian Network struc-
linear relationships aligned with existing knowledge and
turelearning. ArtificialIntelligenceReview,1–94.
offers flexibility in choosing from a spectrum of learned
Lauritzen,S.L.;andWermuth,N.1989. Graphicalmodels
graphs.Theincorporationofexpertknowledgeasbothprior
forassociationsbetweenvariables,someofwhicharequali-
beliefandposteriorrequirementmakesitpromisingforad-
tativeandsomequantitative.TheannalsofStatistics,31–57.
vancingtheapplicationofartificialintelligenceinhealthcare
andrelateddomains. Nadaraya,E.A.1964. Onestimatingregression. Theoryof
Probability&ItsApplications,9(1):141–142.
Acknowledgements Piironen,J.;andVehtari,A.2017. Onthehyperpriorchoice
YW and FDV acknowledge support for this work from the fortheglobalshrinkageparameterinthehorseshoeprior. In
NationalScienceFoundationunderGrantNo.IIS-1750358. ArtificialIntelligenceandStatistics,905–913.PMLR.
Any opinions, findings, and conclusions or recommenda- Prechelt,L.2002. Earlystopping-butwhen? InNeuralNet-
tionsexpressedinthismaterialarethoseoftheauthor(s)and works:Tricksofthetrade,55–69.Springer.
donotnecessarilyreflecttheviewsoftheNationalScience
Singh, A. P.; and Moore, A. W. 2005. Finding optimal
Foundation.
Bayesian networks by dynamic programming. Carnegie
MellonUniversity.CenterforAutomatedLearningandDis-
References
covery.
1990. LiverDisorders. UCIMachineLearningRepository.
Stone,C.J.1980.Optimalratesofconvergencefornonpara-
DOI:https://doi.org/10.24432/C54G67.
metricestimators. TheannalsofStatistics,1348–1360.
Atienza,D.;Bielza,C.;andLarran˜aga,P.2022. Semipara-
Tsamardinos,I.;Brown,L.E.;andAliferis,C.F.2006. The
metricbayesiannetworks. InformationSciences,584:564–
max-minhill-climbingBayesiannetworkstructurelearning
582.
algorithm. Machinelearning,65:31–78.
Atienza, D.; Larran˜aga, P.; and Bielza, C. 2022. Hybrid
Watson,G.S.1964. Smoothregressionanalysis. Sankhya¯:
semiparametricBayesiannetworks. TEST,31(2):299–327.
TheIndianJournalofStatistics,SeriesA,359–372.
Boukabour, S.; and Masmoudi, A. 2021. Semiparametric
Williams,C.K.;andRasmussen,C.E.2006. Gaussianpro-
Bayesiannetworksforcontinuousdata. Communicationsin
cesses for machine learning, volume 2. MIT press Cam-
Statistics-TheoryandMethods,50(24):5974–5996.
bridge,MA.
DeCampos,C.P.;Zeng,Z.;andJi,Q.2009.Structurelearn-
ingofBayesiannetworksusingconstraints. InProceedings
of the 26th Annual International Conference on Machine
Learning,113–120.
Friedman,N.;andNachman,I.2013. Gaussianprocessnet-
works. arXivpreprintarXiv:1301.3857.
Gentle,J.E.;Ha¨rdle,W.K.;andMori,Y.2012. Howcom-
putational statistics became the backbone of modern data
science. Springer.
Ghosh,S.;Yao,J.;andDoshi-Velez,F.2019. ModelSelec-
tioninBayesianNeuralNetworksviaHorseshoePriors. J.
Mach.Learn.Res.,20(182):1–46.
Gunn,S.R.;andKandola,J.S.2002. Structuralmodelling
withsparsekernels. Machinelearning,48:137–163.
Heckerman, D.; Geiger, D.; and Chickering, D. M. 1995.
Learning Bayesian networks: The combination of knowl-
edgeandstatisticaldata. Machinelearning,20:197–243.
Hofmann,R.;andTresp,V.1995. Discoveringstructurein
continuousvariablesusingBayesiannetworks. Advancesin
neuralinformationprocessingsystems,8:500–506.