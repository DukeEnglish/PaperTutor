ViLexNorm: A Lexical Normalization Corpus
for Vietnamese Social Media Text
Thanh-NhiNguyen∗and Thanh-PhongLe∗and KietVanNguyen
FacultyofInformationScienceandEngineering,
UniversityofInformationTechnology,HoChiMinhCity,Vietnam
VietnamNationalUniversity,HoChiMinhCity,Vietnam
{21521232, 21520395}@gm.uit.edu.vn,kietnv@uit.edu.vn
Abstract
Original comment
Lexical normalization, a fundamental task in c h ế t trong tôi, một ty chưa nóiii
NaturalLanguageProcessing(NLP),involves
Normalized comment
thetransformationofwordsintotheircanon-
ical forms. This process has been proven to chết trong tôi, một tình yêu chưa nói
benefitvariousdownstreamNLPtasksgreatly.
Inthiswork,weintroduceVietnameseLexical English
Normalization (VILEXNORM), the first-ever
dying within me, a love not yet spoken
corpus developed for the Vietnamese lexical
normalizationtask. Thecorpuscomprisesover
dying within me, a love yet unspoken
10,000pairsofsentencesmeticulouslyanno-
tatedbyhumanannotators,sourcedfrompublic
Figure1: Examplenormalizationof“chếttrongtôi,
comments on Vietnam’s most popular social
mộttychưanóiii”.
mediaplatforms. Variousmethodswereused
toevaluateourcorpus,andthebest-performing
systemachievedaresultof57.74%usingthe
Thispresentssignificantchallengesfornatural
Error Reduction Rate (ERR) metric (van der
languageprocessingsoftware(e.g.,Baldwinetal.,
Goot,2019a)withtheLeave-As-Is(LAI)base-
2013;Eisenstein,2013),whichisprimarilyaimed
line. For extrinsic evaluation, employing the
atanalyzingcanonicaltext. Onepossibleapproach
modeltrainedonVILEXNORM demonstrates
thepositiveimpactoftheVietnameselexical toenhancetheperformanceofthesesystemsisto
normalization task on other NLP tasks. Our normalize the text, thereby increasing its resem-
corpusispubliclyavailableexclusivelyforre- blancetothedata tbhỉa btỉN nLgạPn s đyỏst e ờm hsồwi eứrce đoaroig tihnưaơlnlyg
searchpurposes1.
developed and trained. This task is also called
bỉ ngạn đỏ hồi ức đau thương
Disclaimer: This paper contains real com- lexicalnormaliza t ion;s eeF i gu r e 1fo rth eno rmal-
mentswithexplicitorpotentiallysensitivecon- ization of “c h ế t trong tôi, một ty chưa nóiii”
tent. (English: dyingwithinme,aloveyetunspoken).
Inthispaper,wedefineourtaskoflexicalnor-
1 Introduction
malizationbyvanderGootetal.(2021),expressed
bythefollowingformulation:
In2022,thereweremorethan72millionusersof
socialnetworksinVietnam,accountingforapprox-
Definition-LexicalNormalization
imately73.7%ofthetotalpopulation2. Therapid
Lexicalnormalizationisthetaskoftransform-
growthofsocialmediahasresultedinasignificant
increaseinthevolumeofdataexchangedoverthe
ing an utterance into its standard form, word
byword,includingbothone-to-many(1-n)and Bình luận gốc
Internet. However,becausethedataisspontaneous,
many-to-one(n-1)replacements. c h ế t trong tôi, một ty chưa nóiii
itnaturallycontainsawiderangeoflinguisticvari-
ances,bothintended(e.g.,slang,leetspeak,puns)
Bình luận đã được chuẩn hoá
andunintended(e.g.,mistakes). In other words, throughout this paper, out-of-
chết trong tôi, một tình yêu chưa nói
vocabulary(OOV)andwrongin-vocabulary(IV)
∗*Equalcontribution.
tokenscanbenormalizedtotheirstandardlexical
1https://github.com/ngxtnhi/ViLexNorm
formsandtheirin-vocabularycounterpart’slexical
2https://www.statista.com/statistics/278341/
number-of-social-network-users-in-selected-countries/ items,respectively.
4202
naJ
92
]LC.sc[
1v30461.1042:viXraThelexicalnormalizationtaskhasbeenexten- thetaskwasfollowedbysubsequentcorporasuch
sively studied in various languages; however, re- as LexNorm1.2 (Yang and Eisenstein, 2013) and
searchspecifictoVietnamese,alow-resourcelan- LexNorm2015(Baldwinetal.,2015). Movingto
guage,isnotablylacking. Recognizingtheurgent languagesotherthanEnglish,severalcorporawere
needfortheearly-stageexplorationoflexicalnor- established. CroatiansawthecreationofReLDI-
malizationforVietnamese,wehavepainstakingly NormTagNER-hr2.0(Ljubesˇic´ etal.,2017),while
createdacorpusnamedVILEXNORM,encompass- SerbianhadReLDI-NormTagNER-sr2.0(Ljubesˇic´
ingbothOOVandIVreplacements. Wehopethis etal.,2017). Slovenian,too,haditsrepresentation
workwillserveasacatalyst,encouragingfurther withJanes-Tag2.0(Erjavecetal.,2017). Danish
initiatives to tackle this crucial task for the Viet- wasaddressedbythedevelopmentofDaN+(Plank
nameselanguage. etal.,2020). Italianalsohadadatasetintroduced
Ourprincipalcontributionsinthisstudyconsist by van der Goot et al. (2020). Shifting the fo-
ofthefollowing: custoAsianlanguages,Higashiyamaetal.(2021)
introduced a notable corpus for Japanese. Addi-
1. TheestablishmentofVILEXNORM,theinitial
tionally,Bariketal.(2019)presentedacorpusfor
corpusforVietnamesesocialmediadatanor-
code-mixedIndonesian-English,andMakhijaetal.
malization, which encompasses 10,467 sen-
(2020) developed HinglishNorm for code-mixed
tence pairs. Additionally, we provide a de-
Hindi-English. Remarkably,asharedtaskonmul-
taileddescriptionofourrigorousannotation
tilinguallexicalnormalization(MULTILEXNORM
process. Corpusanalysiswasthoroughlycon-
byvanderGootetal.,2021)hasprovidedabench-
ductedtograspthenoteworthyphenomenaof
markincluding12languagevariants.
Vietnameseobservedinthedomainofsocial
Alongside the establishment of corpora, ad-
media.
vancements in normalization systems, as exem-
2. The implementation of two approaches to plified by MoNoise by van der Goot, 2019a and
evaluate the efficacy of our corpus, includ- Mulleretal.,2019,haveshowcasedpromisingout-
ingPre-transformerModelsandTransformer- comes. Furthermore, lexical normalization has
based Models. Interestingly, the pre-trained been demonstrated to boost various downstream
model for Vietnamese achieved the highest NLPtasks,suchasnamedentityrecognition(Plank
performancealongwiththerelativelycompet- etal.,2020),POStagging(Zupanetal.,2019),de-
itiveperformanceofthevanillaTransformer, pendencyandconstituencyparsing(vanderGoot
especiallyconsideringthatitwastrainedfrom etal.,2020),sentimentanalysis(Sidorenko,2019),
scratch. andmachinetranslation(Bhatetal.,2018).
However, the studies have yet to be applied to
3. The extrinsic evaluation conducted on vari-
Vietnamese. Research efforts have primarily fo-
ous downstream NLP tasks highlights how
cused on the detection and correction of Viet-
efficienttheVietnameselexicalnormalization
namesespellingerrors(e.g.,Nguyenetal.,2015;
taskisinimprovingthesetasks’performance.
Nguyenetal.,2016;Doetal.,2021;Nguyenetal.,
2023), which are mostly unintended. To the best
2 RelatedWork
ofourknowledge,VILEXNORMstandsasthefirst
Thelandscapeoflexicalnormalizationresearchhas work to examine both advertent and inadvertent
witnessed significant growth and diversification variationsinspelling,encompassingallclassifica-
acrossvariouslanguagesoverthepastdecade. This tionsoutlinedbyvanderGootetal.(2018)except
section provides an overview of the foundational phrasalabbreviations.
work in English and extends to include develop-
mentsinlanguagesotherthanEnglish,highlighting 3 CorpusCreation
the emergence of corpora, advancements in nor-
In this section, we illustrate our corpus creation.
malizationsystems,andthedownstreamimpactof
TheoverviewprocessisdepictedinFigure2.
lexicalnormalizationondiverseNLPtasks.
Since the foundational work of Han and Bald-
3.1 DataCollectionandPre-processing
win(2011)withLexNorm1.1adecadeago,lexical
normalizationhassparkedinterestinEnglishand Datacollectionwasconductedontwowell-known
several other languages. In the realm of English, social media platforms including Facebook andDATA COLLECTION GUIDELINES
definitions scope examples reference
sources
PREPROCESSING ANNOTATION
@username N
s/s ≥ 4 & ≥ 75%?Y
annotator training annotating authors’ normalized subsets
sentence splitting word count filtering removing recruitment feedback
filtering
VILEXNORM
subsets
Figure2: TheoverviewprocessofcreatingVILEXNORM.
TikTok, due to their wide usage and popularity in literature surpassing 8.0 on a scale of 10.
amongVietnameseusers3. Furthermore, their academic backgrounds are
Wedeliberatelypickedawiderangeofcontent diverse,spanningfieldssuchascomputerscience,
categoriesandexclusivelyincludedcommentsfrom Vietnamesestudies,economics,andconstruction,
highlyengagingpublicposts. Thisstrategyaimed contributing to a broad spectrum of perspectives
to amplify the richness and variety of the Viet- duringtheannotationprocess.
namese language expressed across social media.
During the pre-processing stage, we divided the Annotation Guidelines As already stated,
commentsinparagraphformintoindividualsen- our objective was to engage annotators from
tences. Subsequently, we filtered out sentences various backgrounds to ensure diverse language
with fewer than four words to maintain a reason- perspectives. Consequently, we constructed
ableannotationdensityandoptimizetheannotation guidelines encompassing comprehensive def-
process. Furthermore, all usernames in the com- initions of related terms in the task, such as
ments were removed to ensure anonymity. Any Vietnamese word, non-canonical sentence, and
emojicharacterspresentinthesentenceswerealso the annotator’s role. This strategy aimed to
eliminated. In order to avoid overlooking social facilitate a clear understanding of the annotating
meaning, as pointed out by Nguyen et al. (2021) task. We explicitly outlined the scope of lexical
andcapturesocialphenomena,weretainedallcom- normalizationandpresentedillustrativeexamples
mentsthatmightincludetoxicoroffensivecontent, that demonstrated how to normalize each case
andallannotatorswerefullyawareofthat. and common mistakes correctly. In cases where
difficulties arose, annotators were recommended
3.2 AnnotationProcess
to consult reputable resources4. Furthermore,
AnnotatorRecruitment Theannotationprocess annotatorswereencouragedtoprovidesuggestions
involvedsixnativeVietnamesespeakers,including toenhancethefeasibilityoftheguidelines.
two of the authors, encompassing both male and
female individuals aged between 20 and 22. The Training Phase In the initial phase of the
annotators possess extensive familiarity with a annotation stage, the annotators were provided
widerangeofdiversesocialmediaplatformsand withguidelinesandunderwentatrainingsession.
exhibit university entrance examination results They were assigned to a subset of 100 sentences
3Statisticssourcedfrom andaskedtoestimatethenumberofsubsetsthey
https://www.similarweb.com/top-websites/
vietnam/computers-electronics-and-technology/ 4WeutilizedTraTu(afree,openonlineprofessionalViet-
social-networks-and-online-communities/inMay2023 namesedictionary)andGoogleforthispurpose.couldannotateinaday. Weallowedtheannotators namesecanconsistofmultiplesyllablesseparated
to freely determine their workload in order to byspaces. Thisseparationaidsinproperpronunci-
ensuretheannotations’quality. ationandcomprehensionofwords,reflectingthe
Vietnamese language’s unique phonological and
Main Annotation For each annotated sub- orthographic features. In light of this, we under-
set of 100 sentences, the annotators received took an analysis of VILEXNORM by considering
feedback on a sample of 20 random sentences theelementofsyllablecounts,aimingtodelveinto
fromtheauthors. Wecalculatedthepercentageof thedistinctivecharacteristicsofVietnamese.
sentencesforwhichtheauthorsagreedonthenor-
Athoroughdistributionanalysisofthewordsis
malization,specificallywhentheymutuallyagreed
providedinTable1. Itisindisputablethatthema-
that the sentence was completely normalized.
jorityofVietnameseindividualsutilize1-syllable
If the agreement score between the annotator’s
canonicalwordswiththeutmostfrequencywhen
annotationsandtheauthors’annotationswasbelow
engagingonvarioussocialmediaplatforms. More-
75%, the annotator was requested to re-annotate
over, we observe a noteworthy pattern in the 2-
the entire subset. Notably, agreement between
syllable and 3-syllable categories. The count of
subsetsannotatedbyeitherofthetwoauthorswas
normalizedwords(2,741for2-syllableand104for
evaluatedbytheotherauthor.
3-syllable) surpasses the count of non-canonical
ThroughoutboththeTrainingandMainAnnota-
words(396for2-syllableand7for3-syllable),sug-
tionphases,nosubsetrequiredre-annotationmore
gestingthatindividualsdeliberatelyoptforshorter
thanonce;thus,noannotatorswereeliminated.
variationsofwordswhencommunicatingthrough
onlinechannels. Thisinclinationtowardsbrevity
Inter-annotator Agreements The agree-
andefficiencyinconveyingmessagesalignswith
ment between annotators was averaged across
thetypicalcharacteristicsofonlinediscourse.
all subsets during the main annotation phase.
Additionally, as the authors were involved in To assess the extent of linguistic diversity ob-
the annotation task, the agreement between servedonsocialnetworks,weconductedananaly-
them was computed separately. The averaged sisofthestandardwordsthatdisplayedthehighest
inter-annotatoragreementforallsubsetsduringthe numberofvariations,asdepictedinTable2. The
main annotation phase was 88.46% between the resultsyieldedfascinatingstatistics. Forexample,
authorsandotherannotatorsand93.54%between theword"không"(no)demonstratedanimpressive
the two authors. The observed scores reflect a totalof53variations,whichunderscoresthecre-
high level of concordance between annotations, ativelanguageusedbyVietnameseindividualsin
demonstrating strong agreement between the the online sphere. Additionally, we explored the
annotatorsandtheauthorsinourtask. toptenmostfrequentlynormalizedterms,detailed
inAppendixA.
Filtering Following the main annotation
phase,weexcludedsentencesthatdidnotcontain
anywordsrequiringnormalizationinourdefined
4 IntrinsicEvaluation
scope. Afterward, the VILEXNORM corpus
comprisesatotalof10,467pairsofsentences.
This section focuses on the intrinsic evaluation
3.3 CorpusStatistics
of VILEXNORM, examining its empirical perfor-
ThecorpusVILEXNORMconsistsof10,467com- mance through diverse experiments and method-
mentpairsfollowingtheannotationprocess. These ologies. We explore methods ranging from pre-
arefurtherpartitionedintothreesubsets: training, transformerstructurestotransformer-basedstruc-
development,andtest,distributedinan8:1:1ratio. tures in the lexical normalization task. Subse-
The corpus encompasses a total of 20,061 word quently,weoutlinetheexperimentalsetup,includ-
pairs,comprisingatotalof3,489distinctpairsde- ing data configurations, training procedures, and
rivedfromthecomments. metrics. Finally,wepresentevaluationresults,an-
Vietnameseisamonosyllabiclanguagewherein alyzing each method’s performance and offering
every syllable is distinctly separated by a space insights into the efficiency and effectiveness of
initswrittenform. Alternatively,awordinViet- VILEXNORM.Non-canonicalWords NormalizedWords
NumberofSyllables
Total Distinct Total Distinct
1 19,658 3,188 17,207 2,707
2 396 295 2,741 736
3 7 6 104 41
4 - - 9 5
Total 20,061 3,489 20,061 3,489
Table1: VILEXNORMstatisticsshowingthenumberofwordscategorizedbysyllablecount. Non-canonicalWords
referstowordsfoundintheoriginalsentencesthatneedednormalization. NormalizedWordsrepresentsthecount
ofwordsnormalizedfromtheirnon-canonicalforms. Totaldenotesthetotalcountofwords,andDistinctsignifies
thecountofdistinctwords.
Standardword Numberofvariants advancementsindeeplearning,withthean-
không(no) 53 ticipationthattheywouldoptimizetaskper-
rồi(already) 50
formance.
vậy(so) 34
quá(very) 34
4.2 ExperimentalSetup
thôi(stop) 33
ơi(hey) 31 In this setup, we approached the lexical normal-
biết(know) 24
ization task as a sequence-to-sequence problem,
trời(god) 23
wheretheinputcomprisedasentencecontaining
được(okay) 22
at least one word in its unnormalized form, and
đi(go) 21
the objective was to generate the corresponding
Table 2: Standard words with the most variations in normalizedsentence. ExceptforBARTpho,which
VILEXNORM. inherently provides options for syllable-level and
word-levelinput,weassessedthemodelsonboth
segmentedandunsegmentedversionsofthecorpus
4.1 Methods
usingVnCoreNLP(Vuetal.,2018)tounderstand
To establish empirical performances on theinfluenceofwordsegmentationontheirperfor-
VILEXNORM, we conducted various experi- mance. Additionally,weappliedByte-Pairencoder
mentsusingdifferentmethods: (Sennrich et al., 2016) with a vocabulary size of
7000.
• Pre-transformer Structures: We initi- For the BiGRU and LSTM models, the model
atedbyemployingwell-establishedarchitec- trainingcommencedwithabatchsizeof32,em-
tures predating the widespread adoption of ploying the Adam optimizer along with cross-
transformer-basedmodelsinNLPtasks. This entropy loss. The training spanned 40 epochs,
categoryincludesLongShort-TermMemory utilizing a learning rate of 0.01. The same ex-
(LSTM;HochreiterandSchmidhuber,1997) perimentalsetupwasappliedtothevanillaTrans-
andBidirectionalGatedRecurrentUnits(Bi- former, albeit with a learning rate of 0.0001.
GRU;Choetal.,2014)withAttentionmecha- We explored both versions of BARTpho, namely
nism(Bahdanauetal.,2014). Wechosethese BARTpho andBARTpho ,publiclyavail-
syllable word
architectures due to their proven effective- able on Hugging Face5. Within this method, we
nessinsequencemodelingandtheirhistorical designatedtheepochcountas10,utilizingalearn-
prominenceinNLPtasks. ingrateof5e-5.
We utilized a system with 13GB RAM and an
• Transformer-basedStructures: Wefurther
NVIDIA Tesla T4 GPU to train all initial mod-
delvedintotransformer-basedstructures,in-
els. The manual seed for BARTpho was set to
cludingtrainingofvanillaTransformerfrom
42,whereasfortheremainingmodels,itwasestab-
scratch(Vaswanietal.,2017)andfine-tuning
lishedas0. Thiswasdonetoensurereproducibility
BARTpho (Tran et al., 2022), a pre-trained
andconsistencyintheresults.
Sequence-to-SequencemodelforVietnamese.
Theseselectionsweremotivatedbytherapid 5https://huggingface.co/vinai4.3 Metrics of data. Remarkably, the BARTpho model
syllable
showcased a significant positive ERR of 0.5774,
This paper employed the Error Reduction Rate
emphasizing its capacity to substantially reduce
(ERR) proposed by van der Goot (2019a) as the
errorsandenhancebothprecision(0.9332)andre-
primary metric. ERR assesses the reduction in
call(0.9193). Fortheword-leveldata,thevanilla
errors compared to a previous model and serves
TransformerandBARTpho alsodisplayedim-
asanormalizedmeasureoftoken-levelaccuracy, word
provementovertheLAIbaseline,achievingERRs
consideringthepercentageoftokensrequiringnor-
of0.2903and0.2269,respectively. However,this
malization. Since there is currently no standard
improvement was less pronounced compared to
normalization model for Vietnamese, the Leave-
theirsyllable-levelcounterparts. Theseoutcomes
As-Is(LAI)baseline,whichretainstheinputword,
underscorethattransformer-basedstructuresper-
wasutilized.
formexceptionallywell,evenwithoutthenecessity
TheERRformulaisasfollows:
ofwordsegmentation,reaffirmingtheiralignment
Accuracy −Accuracy
ERR = system baseline (1) withVietnameselinguisticfeaturesandsuggesting
1.0−Accuracy
baseline anenhancedcapabilitytocaptureandprocessthese
TheERRtypicallyfallswithintherangeof0.0 linguisticnuances.
to1.0,wherebyanegativeERRsuggestsmorein- Overall, despite encountering challenges with
correcttokennormalizationsthancorrectones. Itis pre-transformer structures resulting in higher er-
worthnotingthattheLeave-As-Isbaseline,which ror ratesthan the baseline, theadvancements ob-
returnstheinputwordswithoutanyalterations,will served with transformer-based architectures, par-
inevitablyproduceanERRvalueof0.0. ticularlyBARTpho syllable,demonstratepotentialfor
In the context of 1-n and n-1 transformations, substantialerrorreduction,offeringanencourag-
we utilize the Levenshtein distance metric (Lev- ingoutlookforfurtheradvancementsinthelexical
enshtein et al., 1966) to calculate accuracy at the normalizationtaskforVietnamese.
tokenlevel.
4.5 EffectsofNon-canonicalWordRatioin
AsstatedbyvanderGoot(2019b),ERRhasthe
SentencesonNormalizationEfficiency
limitationofnotprovidinginsightintothedistinc-
tion between false positives (FP) and false nega- Inordertogaininsightsintohowtheratioofwords
tives(FN).Thismetricdoesnotinformuswhether necessitatingnormalizationwithinasentenceaf-
the system normalizes excessively or cautiously. fects the efficiency of the normalization process,
Therefore, we also incorporated two additional weconductedathoroughanalysisonthedevelop-
metrics: PrecisionandRecall. ment set using the ERR score of BARTpho syllable
duetoitssuperiorperformance.
4.4 EvaluationResults
Figure 3 provides a graphical insight into the
Table3displaystheintrinsicevaluationresultsfor relationshipbetweennon-canonicalwordratiosand
various methods regarding Error Reduction Rate thecorrespondingERRperformances. Thewidth
(ERR),Precision,andRecall. of the columns is proportional to the number of
In terms of pre-transformer structures, using samplesineachcategory.
LSTM with both data versions resulted in ERR The ERR performance followed a distinct pat-
valuesof-4.3781and-4.1319,respectively. These tern with respect to the ratio of words requiring
negativeERRvaluesindicatethatthemodelshad normalization. Specifically,thenormalizationeffi-
ahighererrorratethanthebaselineLAIapproach. ciencyappearedtoimproveastheratioofwordsto
However, transitioning to BiGRU with the At- be normalized increased, peaking in the range of
tention mechanism showed improvement, bring- 20-30%. Beyondthisrange,theefficiencyslightly
ing ERR closer to zero, with -0.2483 for syllable decreased, though it remained higher than the 0-
leveland-0.3025forwordlevel. Notably,BiGRU 10%and10-20%categories.
achieved positive precision and recall of around Thispatternsuggeststhatsentenceswithamod-
0.80to0.84. erate proportion of words needing normalization
Moving to transformer-based structures, the (20-30%)areoptimallysuitedforthenormalization
vanilla Transformer displayed intriguing results, process. Thenormalizationsystemmayhavebeen
achievinganERRof0.3394,aprecisionof0.9090, effectively trained and fine-tuned to handle this
and a recall of 0.9104 for the syllable version range,resultinginenhancedefficiency. However,Method Level ERR Precision Recall
Syllable -4.3781 0.1178 0.1187
LSTM
Word -4.1319 0.1225 0.1222
Pre-transformerstructures
Syllable -0.2483 0.8350 0.8369
BiGRU+Attention
Word -0.3025 0.8182 0.8015
Syllable 0.3394 0.9090 0.9104
VanillaTransformer
Word 0.2903 0.8944 0.8950
Transformer-basedstructures
BARTphosyllable Syllable 0.5774 0.9332 0.9193
BARTphoword Word 0.2269 0.8912 0.8735
Table3:IntrinsicevaluationofmodelstrainedonVILEXNORM,showcasingErrorReductionRate(ERR),Precision,
andRecall. Resultsarepresentedacrosspre-transformerandtransformer-basedarchitectures,consideringbothword
andsyllable-leveldataconfigurations.
1.0 5.1 TheImpactofLexicalNormalizationon
DownstreamNLPTasksPerformance
0.8 Tovalidateournormalizationsystem’spracticalap-
plicabilityandeffectiveness,weconductedextrin-
sic evaluations across three specific tasks. These
0.6 tasks consisted of emotion recognition using the
UIT-VSMECcorpus(Hoetal.,2019),hatespeech
detection using the ViHSD dataset (Luu et al.,
0.4
2021), and spam detection using the ViSPAM
dataset(VanDinhetal.,2022). TheUIT-VSMEC
corpuscomprises6,927sentencesfromFacebook,
0.2
categorizedintosevenemotionlabelsthroughhu-
man annotation. Conversely, the ViHSD dataset,
consisting of 33,400 comments, was annotated
0-10% 10-20% 20-30% 30-40%>40%
into three labels specifically for hate speech de-
Proportion of Words Requiring Normalization
tection on various social networking platforms.
Figure3: PerformanceanalysisofBARTpho syllable on Lastly, the ViSPAM dataset, with its 19,868 re-
the development set of VILEXNORM, demonstrating
views,wascuratedtoidentifyspamreviews,par-
an association between non-canonical word ratio and
ticularly opinion-based ones, on Vietnamese E-
normalizationefficiency.
commerce platforms. In our assessment of ViS-
PAM,wefocusedsolelyonthebinaryclassification
as the ratio of words needing normalization ex- task,determiningwhetherareviewisspamornot.
ceedsthisrange,thesystemencounterschallenges, It is important to note that emoji characters were
potentiallyduetoincreasedlinguisticcomplexity excludedfromallthreedatasetsasournormaliza-
ornoisewithinthesentence. tionsystemisincapableofhandlingemojis.
Fortheextrinsicevaluation,weleveragedadi-
5 ExtrinsicEvaluation verse set of models for all three tasks. TextCNN
(Kim, 2014), recognized for its efficiency in text
This section extends the assessment of classification, was one of the key models. We
VILEXNORM beyond intrinsic measures, alsoincorporatedBidirectionalLSTM(BiLSTM)
exploring its impact on downstream NLP tasks. andGatedRecurrentUnit(GRU),bothrenowned
Through experiments, we investigate how the for their proficiency in sequence modeling. Fur-
normalization system enhances performance in thermore, we utilized PhoBERT (Nguyen and
emotion recognition, hate speech detection, and Tuan Nguyen, 2020), a state-of-the-art monolin-
spam detection. We also assess its efficacy in gual language model pre-trained specifically for
scenarioswithoutVietnamesediacritics,providing Vietnamese,forthisevaluation. SeeAppendixC
insights into its adaptability and real-world fordetailsonhyperparametersandtraining.
effectiveness. Ourchosennormalizationsystemforthiseval-
etaR
noitcudeR
rorrEuationwastheBARTpho duetoitssuperior PhoBERTintheabsenceofVietnamesediacritics.
syllable
performanceobservedintheintrinsicevaluation. We conducted experiments by removing varying
In this setup, we employed the normalized ver- percentages of diacritics from each comment in
sions of the input texts generated by our chosen the UIT-VSMEC and ViHSD datasets. The re-
normalizationsystemastheinputforthemodels. sultsdepictedinTable5showcasetheperformance
Notably,wetrainedthemodelsthreetimeswhile ofPhoBERTbeforeandafternormalizationusing
keepingthenormalizationmodelfrozen,underlin- BARTpho under various diacritic removal
syllable
ingtheeffectivenessofournormalizationsystem percentages: 25%,50%,75%,and100%.
inenhancingdownstreamtaskperformance. The PhoBERTexhibitedaconsistentdeclineinper-
averaged results from these experiments are de- formanceasdiacriticswereremoved,comparedto
tailedinTable4,providingacomprehensiveview the performance discussed in Section 5.1 where
oftheperformanceofthesemodelsbeforeandafter diacritics were retained. This decrease in perfor-
normalization. manceisananticipatedoutcomegiventhatdiacrit-
Theresultsdemonstratethattheapplicationof ics carry essential linguistic information in Viet-
ournormalizationsystemexhibitedimprovedF1- namese,andtheirremovalcanimpactthemodels’
macro scores in both UIT-VSMEC and ViHSD ability to process and understand the text accu-
cases. These findings indicate the potential af- rately.
firmativeimpactofournormalizationsystemson Uponexaminingspecificdiacriticremovalper-
improvingemotionrecognitionandhatespeechde- centages, an interesting pattern emerged. Both
tection. However, the outcomes for ViSPAM did datasets, UIT-VSMEC and ViHSD, exhibited an
not exhibit significant promise, showing a slight increaseinperformanceafternormalizationwhen
decreaseinhalfofthecases. Thissuggeststhatthe 25% and 50% of diacritics were removed. How-
binaryclassificationtaskofidentifyingspammes- ever, the increase was notably higher at the 50%
sagesisrelativelyuncomplicated,enablingmodels removalmark,indicatingamoresignificantimpact
tocomprehendessentialcharacteristicswithoutre- ofnormalizationatthislevel.
quiring a normalization stage. Another potential
On the other hand, as the diacritic removal in-
reason for this outcome may be attributed to the
creased to 75% and 100%, both datasets demon-
lossofimportantfeaturesthroughthenormaliza-
strated a decrease in performance after normal-
tion of non-standard input, which is crucial for
ization. Interestingly, the F1-macro score before
spamdetection.
normalizationat100%diacriticremovalsurpasses
In summary, the extrinsic evaluation strongly that at 75%, a surprising observation. This pat-
affirms that integrating our normalization sys- tern suggests that the near-complete removal of
tem enhances input data quality, resulting in im- diacriticscouldintroduceadditionalnoiseormod-
provedperformanceacrossdiverseNLPtasks,es- ifythelinguisticcontextinamannerdetrimental
pecially in complex tasks requiring sophisticated tothemodel’sperformance,evenafternormaliza-
pre-processingstrategies,highlightingtheversatile tion. Anotherplausiblefactorcouldbethelimited
applicabilityofournormalizationapproach. presence of non-diacritic samples in our corpus.
Expandingthecorpustoincludemorenon-diacritic
5.2 NormalizationImpactwhenLacking
samples could potentially enhance model perfor-
VietnameseDiacritics
mance across varying diacritic removal levels, a
Vietnamese diacritics, commonly known as dia-
directionworthconsideringinfutureresearch.
critical marks or accents, play a pivotal role in
theorthographyandsemanticinterpretationofthe 6 ConclusionandFutureWork
language. These diacritics, encompassing tones
andadditionalmarkers,areindispensableindiffer- Our paper introduced VILEXNORM, a novel cor-
entiatingwordswithsimilarspellingsbutdistinct pusexpresslydesignedforthelexicalnormalization
meanings. Forexample,theterm"ma"candenote taskofVietnamesesocialmediadata. Thecorpus
"ghost,""mother,""riceseedling,"or"which,"con- analysisdemonstratedcaptivatingcharacteristicsof
tingentupontheemployedtone. Inthissection,our theVietnameselanguageusedonsocialmedia. We
objectivewastoinvestigatetheefficacyofthenor- conductedempiricalevaluationsemployingvarious
malization system, particularly BARTpho , methods on this corpus, and the BARTpho
syllable syllable
in augmenting downstream task efficiency using modelemergedasthetopperformer,achievinganUIT-VSMEC ViHSD ViSPAM
Before After Before After Before After
TextCNN 29.48 29.85 57.38 58.92 78.29 78.31
BiLSTM 23.43 25.23 58.10 60.88 76.93 77.91
GRU 27.85 30.10 60.92 61.23 79.35 78.93
PhoBERT 59.15 62.03 65.91 66.54 89.28 88.21
Table4: F1-macroscoresofmodelsbeforeandafterlexicalnormalizationonthreeNLPdownstreamtasks: emotion
recognition(UIT-VSMEC),hatespeechdetection(ViHSD),andspamdetection(ViSPAM).
25% 50% 75% 100%
Before After Before After Before After Before After
UIT-VSMEC 53.63 53.94 40.79 43.50 32.62 29.59 32.77 31.91
ViHSD 61.59 62.27 53.92 58.81 57.08 56.78 57.18 56.85
Table5: PhoBERT’sF1-macroscorecomparisonbeforeandafterlexicalnormalizationonUIT-VSMEC(emotion
recognition)andViHSD(hatespeechdetection)datasetsacrossvaryingdiacriticremovallevels(25%,50%,75%,
100%).
impressiveERRscoreof57.74%andaPrecision vancingthedevelopmentofhighlyeffectivemul-
score of 93.32%. Additionally, we harnessed the tilinguallexicalnormalizationmodelsthatcanef-
potentialofVILEXNORM toassesstheimpactof fectively bridge language-specific gaps. Another
lexical normalization on downstream NLP tasks, important aspect of our future work involves ex-
andtheresultswereencouraging. Asthepioneer- panding the scope of extrinsic evaluations to en-
ingeffortinthelexicalnormalizationtaskforViet- compassabroaderrangeofNLPtasks,including
namese,wehopethatourcorpuscontributestothe dependencyparsingandPOStagging(vanderGoot
diversityofthemultilinguallexicalnormalization etal.,2021;vanderGoot,2019b). Thesetasksre-
task. Furthermore, we expect this work to moti- quirelabeladjustmentsduringnormalizationdue
vateandinspirefurtherexplorationandresearchin tothemonosyllabicnatureofVietnamese,neces-
handlingnoisydataontheInternet,advancingthe sitating the investigation of adaptive methods for
fieldoflexicalnormalizationinVietnameseNLP monosyllabiclanguagesandcontributingtoamore
research. diverse language landscape in practical language
processingscenarios.
Promising avenues for advancement in this
task are considered for our future research. Our
roadmapincludesnotonlyexpandingthecorpusin
bothscaleanddiversitybutalsoincorporatingava-
rietyofVietnameselanguagevariantsfoundacross
theInternet(e.g.,textlackingdiacriticmarks). Ad-
ditionally,weintendtoconductathoroughanalysis
ofagreement,exploringmetricslikeCohen’skappa
score (Cohen, 1960), to gain deeper insights into
the quality and consistency of the corpus. More-
over,weareinclinedtowardsacomprehensiveex-
plorationandadaptationofstate-of-the-artmodels
and methods, including MoNoise (van der Goot,
2019a), with the goal of identifying optimal so-
lutions for the lexical normalization task and ad-LimitationsandEthicalConsiderations Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay,andLiWang.2013. Hownoisysocial
Limitations mediatext,howdiffrntsocialmediasources? InPro-
In addition to the mentioned contributions, ceedingsoftheSixthInternationalJointConference
on Natural Language Processing, pages 356–364,
it is important to acknowledge the presence of
Nagoya, Japan. Asian Federation of Natural Lan-
severallimitationsinourwork. TheVILEXNORM guageProcessing.
corpus was formed within six months during
the research, potentially failing to represent Timothy Baldwin, Marie Catherine de Marneffe,
Bo Han, Young-Bum Kim, Alan Ritter, and Wei
the broader linguistic developments throughout
Xu. 2015. Shared tasks of the 2015 workshop on
time accurately. Additionally, the presence of
noisyuser-generatedtext: Twitterlexicalnormaliza-
incomprehensible comments in our corpus due tionandnamedentityrecognition. InProceedingsof
to the lack of context, showcasing the diverse theWorkshoponNoisyUser-generatedText,pages
126–135,Beijing,China.AssociationforComputa-
language used on the Internet, could potentially
tionalLinguistics.
influence the overall performance in real-world
applications. The inter-annotator agreement, AnabMaulanaBarik, RahmadMahendra, andMirna
whileanalyzedtosomeextent,remainsrelatively Adriani.2019. NormalizationofIndonesian-English
code-mixedTwitterdata. InProceedingsofthe5th
shallow,andfurtherexplorationisneededtogain
Workshop on Noisy User-generated Text (W-NUT
amorein-depthunderstandingofthequalityand
2019), pages 417–424, Hong Kong, China. Asso-
consistencyofourcorpus. ciationforComputationalLinguistics.
Irshad Bhat, Riyaz A. Bhat, Manish Shrivastava, and
EthicalConsiderations
DiptiSharma.2018. UniversalDependencyparsing
During the recruitment stage, we clearly in-
forHindi-Englishcode-switching. InProceedingsof
formedtheannotatorsthatthetaskswouldinvolve the2018ConferenceoftheNorthAmericanChap-
sensitiveandpotentiallyharmfulcontent. Thepur- teroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,Volume1(LongPa-
poseofgrantingannotatorstheabilitytomanage
pers),pages987–998,NewOrleans,Louisiana.As-
their workload, as mentioned in Section 3.2, was
sociationforComputationalLinguistics.
to prioritize their mental well-being. If, at any
point,theannotatorsfoundtheannotationtasksto KyunghyunCho,BartvanMerrie¨nboer,DzmitryBah-
beoverwhelming,theywerestronglyencouraged danau,andYoshuaBengio.2014. Ontheproperties
ofneuralmachinetranslation: Encoder–decoderap-
to notify the authors. Annotators received com-
proaches. InProceedingsofSSST-8,EighthWork-
pensationof$0.02foreachcommentnormalized, shoponSyntax,SemanticsandStructureinStatistical
whichtypicallyrequiredanaveragedurationof10 Translation,pages103–111,Doha,Qatar.Associa-
secondstofinish. tionforComputationalLinguistics.
Jacob Cohen. 1960. A coefficient of agreement for
Acknowledgements
nominalscales. Educationalandpsychologicalmea-
surement,20(1):37–46.
We would like to express our gratitude to the an-
notatorsfortheirdiligentefforts. Additionally,we Dinh-TruongDo,HaThanhNguyen,ThangNgocBui,
extend our sincere appreciation to the reviewers andHieuDinhVo.2021. Vsec: Transformer-based
modelforvietnamesespellingcorrection. InPRICAI
fortheirvaluablefeedbackandinsights,whichsig-
2021: TrendsinArtificialIntelligence: 18thPacific
nificantly contributed to the improvement of this
Rim International Conference on Artificial Intelli-
paper. gence,PRICAI2021,Hanoi,Vietnam,November8–
This research was supported by the Scientific 12, 2021, Proceedings, PartII18, pages259–272.
Springer.
ResearchSupportFundattheUniversityofInfor-
mationTechnology,VietnamNationalUniversity-
JacobEisenstein.2013. Whattodoaboutbadlanguage
HoChiMinhCity. ontheinternet. InProceedingsofthe2013Confer-
enceoftheNorthAmericanChapteroftheAssoci-
ation for Computational Linguistics: Human Lan-
References guageTechnologies,pages359–369,Atlanta,Geor-
gia.AssociationforComputationalLinguistics.
DzmitryBahdanau,KyunghyunCho,andYoshuaBen-
gio. 2014. Neural machine translation by jointly Tomazˇ Erjavec, Darja Fisˇer, Jaka Cˇibej, Sˇpela
learning to align and translate. arXiv preprint Arhar Holdt, Nikola Ljubesˇic´, and Katja Zupan.
arXiv:1409.0473. 2017. Cmctrainingcorpusjanes-tag2.0.BoHanandTimothyBaldwin.2011. Lexicalnormali- the28thInternationalConferenceonComputational
sationofshorttextmessages: Maknsensa#twitter. Linguistics: IndustryTrack,pages136–145,Online.
In Proceedings of the 49th Annual Meeting of the InternationalCommitteeonComputationalLinguis-
AssociationforComputationalLinguistics: Human tics.
Language Technologies, pages 368–378, Portland,
Oregon,USA.AssociationforComputationalLin- Benjamin Muller, Benoit Sagot, and Djamé Seddah.
guistics. 2019. Enhancing BERT for lexical normalization.
InProceedingsofthe5thWorkshoponNoisyUser-
ShoheiHigashiyama,MasaoUtiyama,TaroWatanabe, generatedText(W-NUT2019),pages297–306,Hong
andEiichiroSumita.2021. User-generatedtextcor- Kong, China. Association for Computational Lin-
pusforevaluatingJapanesemorphologicalanalysis guistics.
andlexicalnormalization. InProceedingsofthe2021
Conference of the North American Chapter of the Dat Quoc Nguyen and Anh Tuan Nguyen. 2020.
AssociationforComputationalLinguistics: Human PhoBERT: Pre-trained language models for Viet-
LanguageTechnologies,pages5532–5541,Online. namese. InFindingsoftheAssociationforComputa-
AssociationforComputationalLinguistics. tionalLinguistics: EMNLP2020,pages1037–1042,
Online.AssociationforComputationalLinguistics.
Vong Anh Ho, Duong Huynh-Cong Nguyen,
DanhHoangNguyen,LinhThi-VanPham,Duc-Vu DongNguyen,LauraRosseel,andJackGrieve.2021.
Nguyen, Kiet Van Nguyen, and Ngan Luu-Thuy OnlearningandrepresentingsocialmeaninginNLP:
Nguyen.2019. Emotionrecognitionforvietnamese asociolinguisticperspective. InProceedingsofthe
socialmediatext. CoRR,abs/1911.09339. 2021ConferenceoftheNorthAmericanChapterof
theAssociationforComputationalLinguistics: Hu-
Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. manLanguageTechnologies,pages603–612,Online.
Long short-term memory. Neural Comput., AssociationforComputationalLinguistics.
9(8):1735–1780.
ThienHaiNguyen,ThinhPham,KhoiMinhLe,Manh
ArmandJoulin,EdouardGrave,PiotrBojanowski,and Luong,NguyenLuongTran,HieuMan,DangMinh
Tomas Mikolov. 2017. Bag of tricks for efficient Nguyen,TuanAnhLuu,ThienHuuNguyen,Hung
textclassification. InProceedingsofthe15thCon- Bui,DinhPhung,andDatQuocNguyen.2023. A
ferenceoftheEuropeanChapteroftheAssociation vietnamesespellingcorrectionsystem. InCompan-
forComputationalLinguistics: Volume2,ShortPa- ionProceedingsofthe28thInternationalConference
pers, pages427–431, Valencia, Spain.Association onIntelligentUserInterfaces,IUI’23Companion,
forComputationalLinguistics. page158–161,NewYork,NY,USA.Associationfor
ComputingMachinery.
Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the VuHNguyen,HienTNguyen,andVaclavSnasel.2015.
2014ConferenceonEmpiricalMethodsinNatural Normalization of vietnamese tweets on twitter. In
LanguageProcessing(EMNLP),pages1746–1751, IntelligentDataAnalysisandApplications: Proceed-
Doha,Qatar.AssociationforComputationalLinguis- ingsoftheSecondEuro-ChinaConferenceonIntel-
tics. ligent Data Analysis and Applications, ECC 2015,
pages179–189.Springer.
VladimirILevenshteinetal.1966. Binarycodescapa-
bleofcorrectingdeletions,insertions,andreversals. VuHNguyen,HienTNguyen,andVaclavSnasel.2016.
InSovietphysicsdoklady,volume10,pages707–710. Textnormalizationfornamedentityrecognitionin
SovietUnion. vietnamesetweets. Computationalsocialnetworks,
3:1–16.
NikolaLjubesˇic´,Tomazˇ Erjavec,MajaMilicˇevic´,and
Tanja Samardzˇic´. 2017. Croatian twitter training Barbara Plank, Kristian Nørgaard Jensen, and Rob
corpusReLDI-NormTagNER-hr2.0. Slovenianlan- van der Goot. 2020. DaN+: Danish nested named
guageresourcerepositoryCLARIN.SI. entities and lexical normalization. In Proceedings
of the 28th International Conference on Compu-
NikolaLjubesˇic´,Tomazˇ Erjavec,MajaMilicˇevic´,and tational Linguistics, pages 6649–6662, Barcelona,
TanjaSamardzˇic´.2017. Serbiantwittertrainingcor- Spain(Online).InternationalCommitteeonCompu-
pusreldi-normtagner-sr2.0. tationalLinguistics.
Son T. Luu, Kiet Van Nguyen, and Ngan Luu-Thuy Rico Sennrich, Barry Haddow, and Alexandra Birch.
Nguyen.2021. Alarge-scaledatasetforhatespeech 2016. Neuralmachinetranslationofrarewordswith
detectiononvietnamesesocialmediatexts. InAd- subword units. In Proceedings of the 54th Annual
vancesandTrendsinArtificialIntelligence.Artifi- MeetingoftheAssociationforComputationalLin-
cial Intelligence Practices, pages 415–426, Cham. guistics(Volume1: LongPapers),pages1715–1725,
SpringerInternationalPublishing. Berlin,Germany.AssociationforComputationalLin-
guistics.
PiyushMakhija,AnkitKumar,andAnujGupta.2020.
hinglishNorm-acorpusofHindi-Englishcodemixed WladimirSidorenko.2019. Sentimentanalysisofger-
sentencesfortextnormalization. InProceedingsof mantwitter. arXivpreprintarXiv:1911.13062.Nguyen Luong Tran, Duong Minh Le, and Dat Quoc YiYangandJacobEisenstein.2013. Alog-linearmodel
Nguyen.2022. BARTpho: Pre-trainedSequence-to- forunsupervisedtextnormalization. InProceedings
Sequence Models for Vietnamese. In Proceedings of the 2013 Conference on Empirical Methods in
ofthe23rdAnnualConferenceoftheInternational NaturalLanguageProcessing,pages61–72,Seattle,
SpeechCommunicationAssociation. Washington, USA. Association for Computational
Linguistics.
RobvanderGoot.2019a. MoNoise: Amulti-lingual
andeasy-to-uselexicalnormalizationtool. InPro- Katja Zupan, Nikola Ljubesˇic´, and Tomazˇ Erjavec.
ceedingsofthe57thAnnualMeetingoftheAssocia- 2019. Howtotagnon-standardlanguage: Normali-
tionforComputationalLinguistics: SystemDemon- sationversusdomainadaptationforslovenehistorical
strations,pages201–206,Florence,Italy.Association and user-generated texts. Natural Language Engi-
forComputationalLinguistics. neering,25:651–674.
RobvanderGoot.2019b. Normalizationandparsing
algorithmsforuncertaininput. Ph.D.thesis,Univer-
sityofGroningen.
Rob van der Goot, Alan Ramponi, Tommaso Caselli,
Michele Cafagna, and Lorenzo De Mattei. 2020.
Norm it! lexical normalization for Italian and its
downstreameffectsfordependencyparsing. InPro-
ceedings of the Twelfth Language Resources and
EvaluationConference,pages6272–6278,Marseille,
France.EuropeanLanguageResourcesAssociation.
RobvanderGoot,AlanRamponi,ArkaitzZubiaga,Bar-
baraPlank,BenjaminMuller,In˜akiSanVicenteRon-
cal,NikolaLjubesˇic´,O¨zlemC¸etinog˘lu,RahmadMa-
hendra, Talha C¸olakog˘lu, Timothy Baldwin, Tom-
masoCaselli,andWladimirSidorenko.2021. Mul-
tiLexNorm: A shared task on multilingual lexical
normalization. InProceedingsoftheSeventhWork-
shoponNoisyUser-generatedText(W-NUT2021),
pages 493–509, Online. Association for Computa-
tionalLinguistics.
RobvanderGoot,RikvanNoord,andGertjanvanNo-
ord. 2018. A taxonomy for in-depth evaluation of
normalization for user generated content. In Pro-
ceedingsoftheEleventhInternationalConferenceon
LanguageResourcesandEvaluation(LREC2018),
Miyazaki,Japan.EuropeanLanguageResourcesAs-
sociation(ELRA).
CoVanDinh,SonT.Luu,andAnhGia-TuanNguyen.
2022. Detecting spam reviews on vietnamese e-
commercewebsites. InIntelligentInformationand
DatabaseSystems,pages595–607,Cham.Springer
InternationalPublishing.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser,andIlliaPolosukhin.2017. Attentionisall
youneed. Advancesinneuralinformationprocessing
systems,30.
ThanhVu,DatQuocNguyen,DaiQuocNguyen,Mark
Dras,andMarkJohnson.2018. VnCoreNLP:AViet-
namesenaturallanguageprocessingtoolkit. InPro-
ceedingsofthe2018ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputational
Linguistics: Demonstrations,pages56–60,NewOr-
leans,Louisiana.AssociationforComputationalLin-
guistics.A MostCommonlyNormalizedWordsinVILEXNORM
To identify words commonly substituted with their variants, we investigated the ten most frequently
occurring1-syllableand2-syllablenormalizedwordsalongwiththeirfrequenciesandrespectivevariants
(refertoTable6and7).
1-syllable Distribution Variants
không(no) 27% k, hong, hông, ko, hỏng, kg, hok, hem, khum, hổng, kh,
khong,hk,hõng,hog,khom,honggggggg,hogg,khun,hẻm,
khưm,k-ko,ứ,khôm,hum,o,hơm,khummm,0,honk,hỏk,
hăm,hongg,kô,khumm,hongggg,hôngggg,hỏg,hôk,ko,
hg,khoonng,khôg,khoeng,khok,hôn,khônh,kog,kó,ki,
hoq,hônnn,hống
tôi(me) 16% t,toi,tuôi,toy,toai,tôy,tui,toyy,toii,tôiiiiii,tao,tuiii
được(okay) 6% đc,dc,dk,đựt,đượt,đk,đx,đươic,đượttt,đươc,ddc,đuọce,
đuọc,duoc,đuoc,đực,đfc,dcd,đượtttt,đv,đượk,duocc
rồi(already) 4% ồi,r,oy,ròi,òii,gòi,roi,ời,gồi,rùi,òi,roy,zòyyyy,ròiii,ùi,
roài,rồu,gòy,gùi,rồy,rùiii,gòiii,zòi,roàiii,rồiii,dòii,rầu,
roii, goy, rôi, ui, dồi, rui, dồii, gòyy, ròy, roiif, dzồi, rùiiii,
dòi,rồiiiii,ròii,ròy,royyy,rùii,rrrrr,gồy,ruid,òy
vậy(so) 4% dẫy,zậy,v,z,dậy,dị,vậyyy,dzị,vị,d,zị,vay,dzậy,dzi,dạ,
dzọ,zay,dãy,zịk,dzayy,dợ,zẫy,zạy,dọ,dì,zz,vạiiii,vầy,
zayyy,vạy,vậii,zạyy,vại,dzạy
em(you/he/she) 3% e,emk,iêm,iêmmmm,iem,ẻm,emm,eim,eng,kem,êm,
3m,êmmmm
người(person) 2% ng,ngừi,ngta,nguoi,ngừiii,nguời,ny,ngừ,n,ngườii,ngừoi
mày(you) 2% m,mài,mài,may,màiii,mèy,m
với(with) 2% vs,zới,dứi,dí,dới,vứi,dzới,zí,vz,zs,vớiiii,vưới,vớii,v,
zdí,zúii,w,zứi,voi,va,dứ
anh(you/he) 2% ank,a,ânh,ah,an,ăng,ann
Table6: Themostcommonly1-syllablenormalizedwordsinVILEXNORMalongwiththeirrespectivedistribution
percentagesandvariants.2-syllable Distribution Variants
ngườita(people) 9% ngta,nta,ngt
ngườiyêu(lover) 9% ny,ngyeu,ngyo,ngiu,ngy,ngiu,ngừiiu,any,eo,ngừieo,
ngyêu,ngêu,nyêu
mọingười(everyone) 6% mn,mngg,mng,m.ng,mụing,m.n,m.n,mậungừ,mụingừi,
mụingườiiii
nhưngmà(but) 5% nhma,nma,nhmà,nhm,nmà
anhem(brothers) 3% ae,ae
bìnhthường(normal) 2% bthf,bthg,bt,bth,binhthuong,bthuong,binhthukng
giađình(family) 2% gđ,gd
điệnthoại(phone) 2% dthoai,đth,đt,dt
sinhnhật(birthday) 2% sn,snhat,xưnnhựt
baogiờ(whenever) 2% baoh,bg,bh,bjo,bgio
Table7: Themostcommonly2-syllablenormalizedwordsinVILEXNORMalongwiththeirrespectivedistribution
percentagesandvariants.B ErrorAnalysis
Toexplorethelinguisticchallengesposedbythelexicalnormalizationtask,weexaminedBARTpho ’s
syllable
predictionfailuresonthedevelopmentset. Astonishingresultswereobserved,highlightingthemodel’s
difficultyinhandlingtheusageofdialectsandslangwordsonsocialmediaplatforms. Thisreaffirms
the diverse linguistic practices employed by Vietnamese speakers online. Our system also struggled
withobfuscatedwords,apersistentissueinoffensivelanguagedetection. Furthermore,weencountered
instancesofword-choiceambiguity. RefertoTable8fordetailedexamplesanddiscussion. Importantly,
alloftheseerrorcasesinvolveintentionalspellingvariations,thusreinforcingthecoreobjectiveofour
research: toencompassthedeliberatelinguisticvariationsprevalentinsocialmediausage.
Examples Discuss
Original:
Dìa Bình định mă hống uống cà phơ là sai lầm
nhá
Themodeldidnotrecognizewordswrit-
Ground-truth:
ten in the phonetic accent of Central
Về Bình định mà không uống cà phê là sai lầm
Vietnamese("Dìa,""mă,""hống"). Con-
nhá
sequently,itretainedthesewordswith-
BARTpho predicted:
syllable
out normalization and incorrectly nor-
Dìa Bình định mă hống ác´ng cà phê là sai lầm
malizedacanonicalword("uống").
nhá
(English: VisitingBinhDinhwithoutdrinkingcoffeeis
amistake)
Original:
Quảbạnnhiệttìnhgửiđgìcũngcợt nhạ
Ground-truth: Asimilarmistakewasobservedwiththe
Quảbạnnhiệttìnhgửiđéogìcũngcợt nhả syllable "nhạ" that BARTpho in-
syllable
BARTpho predicted: correctlychose"nhại"toreplaceinstead
syllable
Quảbạnnhiệttìnhgửiđéogìcũngcợt nhại of"nhả".
(English: An enthusiastic friend, sent anything will
joke")
Original:
em bừn tữn ngke ăng lóy i mò
In this case, the model struggled with
Ground-truth:
out-of-vocabularyslangwords,leading
em bình tĩnh nghe anh nói đi mà
totheselectionofincorrectnormalized
BARTpho predicted:
syllable
counterparts.
em bừn thiếp ngườita ăn nói đii mò
(English:pleasestaycalmandlistentome)
Original:
maimốthongcógiànhănzịnha hôn
Ground-truth: Conversely, BARTpho failed to
syllable
maimốtkhôngcógiànhănvậynha không identifytheslangterm"hôn"duetoits
BARTpho predicted: presenceintheformalvocabularywith
syllable
maimốtkhôngcógiànhănvậynha hôn adifferentmeaning.
(English:don’tcompeteforfoodlikethatinthefuture,
okay?)
gnitirwtcelaiD
sdrowgnalSOriginal:
Mailạicógỏigàdưahấu,sầuriêngthì t.o.i
Ground-truth: Thedeliberateseparationofcharacters
Mailạicógỏigàdưahấu,sầuriêngthì toi in the word "toi" (dead) using dots
BARTpho predicted: causedconfusionforthemodelduring
syllable
Mailạicógỏigàdưahấu,sầuriêngthì tôi.o.i normalization.
(English:I’mdeadwiththeideaofwatermelon-chicken
anddurian-chickensalad)
Original:
Suynghĩcủamấycon thieunang khóhiểulắm
Ground-truth: Likewise, intentionally omitting the
Suynghĩcủamấycon thiểunăng khóhiểulắm spacebetweentwosyllablesanddiacrit-
BARTpho predicted: icsoftheword"thiểunăng"(retarded)
syllable
Suynghĩcủamấycon thieunang khóhiểulắm hasfooledoursystem.
(English:Thethoughtsofretardedguysareveryhard
toget)
Original: This example highlights the
Khổthânmấycongà,mai m làmconănđã BARTpho ’s challenge in ac-
syllable
Ground-truth: curately selecting the appropriate
Khổthânmấycongà,mai mình làmconănđã pronoun. Inparticular,itchose"mày,"a
BARTpho predicted: second-person pronoun, instead of the
syllable
Khổthânmấycongà,mai mày làmconănđã correctnormalization"mình,"whichis
(English:Poorchickens,tomorrowIwilleatone) afirst-personpronoun.
Original:
Chảhiểusaomìnhvẫnsốngđượcđến bh nhỉ
Inanothercaseofambiguity,themodel
Ground-truth:
incorrectly used "bao giờ" (whenever)
Chảhiểusaomìnhvẫnsốngđượcđến bâygiờ nhỉ
instead of "bây giờ" (now), illustrating
BARTpho predicted:
syllable
its struggle in distinguishing relative-
Chảhiểusaomìnhvẫnsốngđượcđến baogiờ nhỉ
timewords.
(English: I don’t know how I can still be alive until
now)
Table8: ChallenginginstancesintheDevelopmentsetfromVILEXNORMfortheBARTpho syllablemodel.
sdrowdetacsufbO
ytiugibmadroWC ExtrinsicExperimentalSettings
Trainingepochs 40
Learningrate 1e-4
Optimizer Adam
TextCNN,BiLSTM,GRU
Lossfunction CrossEntropy
Embeddings FastText(Joulinetal.,2017)
Batchsize 256
Version base6
Trainingepochs 2
PhoBERT
Learningrate 5e-5
Sequencelength 256
Batchsize 16
Table9: Trainingsettingsforthemodelsintheextrinsicevaluation.
6PhoBERT ispubliclyavailableonhttps://huggingface.co/vinai/phobert-base.
base