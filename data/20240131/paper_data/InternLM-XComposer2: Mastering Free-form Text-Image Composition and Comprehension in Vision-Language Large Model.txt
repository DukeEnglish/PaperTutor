InternLM-XComposer2: Mastering Free-form Text-Image
Composition and Comprehension in Vision-Language Large Models
XiaoyiDong∗1,2,PanZhang∗1,YuhangZang∗1,YuhangCao1,2,BinWang1,LinkeOuyang1,XilinWei1,
SongyangZhang1,HaodongDuan1,MaosongCao1,WenweiZhang1,YiningLi1,HangYan1,YangGao1,
XinyueZhang1,WeiLi1,JingwenLi1,KaiChen1,ConghuiHe3,XingchengZhang3,
YuQiao1,DahuaLin1,2,JiaqiWang1,(cid:66)
1ShanghaiArtificialIntelligenceLaboratory,2TheChineseUniversityofHongKong,3SenseTimeGroup
internlm@pjlab.org.cn
MathVista
Answer the two
questions in the AI2D ChartVQA
following figure. Provide 57.6
intermediate steps. 78.7
MMMU 72.6 HallB
To find the length of the edge, we can use the Pythagorean theorem,
pPl re oa vs ide e w dr mite a ta en r ia ar lsti ：cle based on the given requirements with the title 'Introduction to Chinese New Year Customs,' using the sw h qyh upi ac oh rt ee s snt a u ot s fe e ts h ( t eth h la e et ns i gin d t e ha s or i opg fph tot h st eir t i e oa n tt hhg el ee r , r t it wgh h oe t ss a iq dnu ega sler . e ) I n io s t f e h t q ih sue ca al le stn eog , tt whh e e o s hf u at mh vee o af rt ih ge h t 42.0 60.3
1. Chinese New Year customs include pasting couplets, setting off firecrackers, and paying respects to ancestors. triangle with one side of length 4 and the hypotenuse being the edge we
2. New Year delicacies include eating dumplings and glutinous rice balls, expressing the symbolic meaning of family reunion. want to find the length of. MME 2242.7 51.2 MM-Vet Let's denote the length of the edge we want to find as x. Then, according
to the Pythagorean theorem, we have:
u F T gf tC e ie ehsh ms u b nei tn eI a r e iv Se un ,l r la ps a amyt l re r t f iir ay i nn a oNo ngl 1 nlCe ys sd sFw h t c .b e. i uu T n e sY s htae tc tiw e. va o t sI ar e mt,i el e o ia h ss cnl a usn hc lo s sae a t at vlt k oe e e no h mb Jo eir a s saw mC tn t io nn ue eh r c ad rya lgi r us on oy e dnt f dae h e m nt ae hs pd n oS e ae drmp s ef tr bi N i iri d t nen s h‐t gege F a nd e cnFw a obe p 4y urs a, ut po 0 sY i av s lf 0 ee r ea 0t ty dl h s a , . y e ,Ti d esr s h ol a e ua i wC rs tn s tm na y ,u in er aa t g s a nj c ho rt a od r,r o lo f ie d ft t u nr m u fgwa id r rhd i eia nl s i l crt g b ri ao atenn ch d o ka is e l n r s, S F p a rii ne r tr o ut e dt d aci bun lra r dcg ic e n uO k g re l if o ngrf us goF d owi tr hde ne ec or lur e i Sa s c peoc k rsk r . ii ne Tg a gr hi nns e Fda r el ebl sy f tr o iic vg rr aehe l,t .a st l eie g td h tit nt so g,ws oc fha fir fce ih rea a cw r re aa cy t khe eov rsuil g is shp ctir oti nt os sd ia drn eivd re em dao w an nas y it me br pas o. d rT tlh u ae c nky t e x xTxx x o ^^ ^ q = ≈22 2 u f √ 8 i a== = n 8 .t 9 d i84 1 0o 4 0^ 6 t n 4h 2 :+ e + 6 l e8 4 n^ g2 th of the edge, we take the square root of both sides of the MMBench79.6 77.6 75.9 81.8 72.5 QBenchT
paying respects to ancestors, eating dumplings, etc. So, the length of the edge is approximately 8.944. MMBench-CN LLaVAW
Now, to find the angle at the vertex of the triangle, we can use the tangent SEEDImage
I l o Win o ft hna e id lnd ed a ai snt c ei cco toe tn ir m nst go pa a orf e n fir fie ea fc il d rsr eoa b cc yu rk adse ce rr kus d e, m rto so st , ih sc ce r y ser tma in lt lbo e wais a l ise d,f‐ eem a ls n ya t di pk v ri oen atg ca h tt ed im cre eo mv di sc up te s ohs i dces a aru e yl ,c . inh sT s oh ta mres us emd er cea p itng e ieo tr sf sn o . hrd m aa vn a ec n be c ar es ns na an er dd e atf ou h dfn e jt a c h a ct e ni eo gs nn i l td e. te .T oI h o n te p ht pt h ea o i sn as ng cite gae ln s et et h , io s e t f h 8 aa e .nn sg ia l den e g d ol ie v p ii d pn e o a d s i r tb eig y h t t ht h et e r a i la e nn n gg lg el te h i s is o 4 fe atq h nu e da l s t hit do ee t sh a idde ej l a e cn eg nt th t o GP OT- u4 rV s G Pe rm evin ioi- uP sr oo pen-sQ ouw re cn e V SL O- TP Alus
a o yt P C t ps p PI f w T
s
Hn oh o ti r or aeaa nog ei oa rte n uspt rs eu wn rti d rsh tst ed ru e 'C epi ii oi t sa fi n ton n r
o
ss lh si ir niec sfg
.
w. e og wadi aa a a tn T a. vn c msr C sn l
n
oe h a D nt oa , po ac i f s e ii clc uhu f lle rr ele u ya oy yl ee por e,e
x
p ', b uip f slpa as vn, p eerl l peo re e mi i kew go trlneo
l
t , ne sd est e gt e psp ah c onp th im p lsl n stow ea wcie e o ra d em nst i. ii oflp S t l s eoby wl ss fm p pe . do r t o.t aor ri i a gu t o T on mi swnhk o e
p
n ng sce g e d oa l l it aa ly y
b
w a psua F f nn la yu e a uc eu d oc , mb ks s
l,
l s f ah o t lee l oe rii a ii ro tn as ldt v rd in ete Sewe m ta e t rd
h
pfr ss l aor eo,
e r
l oh rm e p vruo ie ry na f a ee no wx ta gp d sc ro hdk p y, k rh p e p Fae r t ia ic, i t ce h el nlr n li ht eu os s sn se e bo tt ss .r tg ro sah a
i
u uo Te vb sfn te ca yhst
a
l iaotd ees oel ie lol nre un sy n crt e t gs p e oh t c ow pi b h al fnu
p
ue ea ouue lr g
r
s pstsr i rt esit t os s li t r e o et ec ‐a, e e ,b
w
hh m m
t
lw t isn :c eo th roc aos i cm is eb iu
t
n su aa h a tmyl e yu eptn c e w u sf s nl u d dsa e b. er a, si m oa t a t f ta yrs fh l ns oe e no ehe to emd rdu odei n e sr s ai r g n d s o
f
di rp rot it n yso oba o o c p o e mg te od l olre at li s oes y wo rva of s ren r io tdd ef t p a P T t a M t f T h E D soo hh yahn n h ee a uog meae ted d ey lo mp ide n iip nr bnc pS satu y pl w ghy r op crges t leam f te lioe r Di inaR s tyim zr on hm p gb ufe i eo ont fg sea a s mtof i wsg hpy cl hl F atis pte ofpc e ee ee rso oie ls enc e am rn s . ip r ntr t o lrs aSw tast t i .f bt gv r ha ooe sit l sif a pi nme lr eoo an all m ogs nr d ni eA as ps st . doe i ly n ua en s wa nlp pon lce gl aoi ese re c lc s rl f o'o vess .t se sur t i fs sa oo o pavo t ni orn t e nr ef t ias t di ni c rtc m ylf ieh ti s irr as yr tee i te e t e ,t nai ouc r atp mdyr o ra sla a sa o bl tdshc nc v ei h,k s uoe c e l ee i oe rm n an r yr is nno ncs te r rgir l go eg eun id o r nmh sdt tsu ns et g hi ' ie n me n ea ao gg' dt msr rt bSeao b t p las ohv eru ri n e e nms ir o anc sa n gfc nep i gf i t e fe consl e ay Fm itt g n es my o eee nei sr tn i'n s tc tto ls yc e. io j Cvo re rF cn m a hoyy na lc l,e io .em s ne t ba te Tm ehr sni r,n hl se ii bano ees et en. gs yfs h sfo rdieeI int s a ln u y vmr .g h rn ip een it b sd i rh gc u t bes ia oe p nedsls ro gal ld ye y iaf oec .in nrg v td Ia i d stga efrr .iu t dae c aphm e la n sa te ss oodys r ,, a a Sst t Tt a aa o i on ndn nn ,g gef ( (( tl lia aa s e en h n nn d o e≈=g gg f t al ll a 2he ee t nh r 6e) )) c g e . = == t 5 la ea e 7n4 0o n q ag .p d(/ 5 tul 0p e e 8 a t.o g, 5 ht rs w i ) eeoit e en ve s e: c/ ra ta n ed xtj aa okc fee t n htht ee t rin iav ne grs lee it sa an pg pe rn ot x bT( i ea em lr
l
pc a mt rta ie nen l ty g
w e
e 2 dhn 6
a
bt . t5)
y
7 wo f d
i
lb le o gt rh e es. MMEPer 1712M .0MM 4U 2.0 Mat 5h 7V .6ista 60H .3al 5l 1B .2 MM-Vet
this program. MMECog530.7 72.5
QBenchT
The program will print a dictionary with three key‐value pairs:
EatingdumplingsisalsoassociatedwiththelegendofNian,amonsterthatwould 'woodman':98, 'Alan':89, and 'Bobo':56.
上 下 横 T " eph nri联 联 批 ts o irs e: : : t pr一 万 财 cea on r帆 事 源 o us u pla风 如 广 s lt ee b t顺 意 进 s .ust io ne" ss sm ."o To hth ehsa oi rli in zog na tah le ca hd a," ra" ce tev rer ry et ph ri en sg eng to se ts hesm tho eo mth ely, o" fa tn hd e a l a w T t ta h oht pr ht eepg oa ic se r uc So h ek tpa sf s rav e c iic dr ni ha al e el gesa r rt e jdg sFu, d ,e est tis t sh hn hte i ad c i em vlu yu afv er ad i l pi l .wn l wi lan aWg g ag yeye ht x ah ., iad nle emtu h imS mspep olp per mvli osi in n l e rlg oa tg af ogsF ne fte h trf ts si e rht l oli ee mv m lsd ea a eal. id n nw pT e yo r pi at l c rh od c eu tue ss idm ectfe o e rea vn m sat io nmst iN gs a aei a Ca n ysnn hdd sb, ie ntyv er evi al m sbel d ea ag ig n te s cie tg ut o rar i ln as n tb unsglw e g ra ep eso s. ou as ot o nl Wd rs c di uahp a inn der te e fd en ap d nmpa tN wa ir i tle i n i yia t a .s hna r, T Ado
fe
td
l e
e
d
rl ie ecI xt
t
w e
[ e‘
cBi "s uB oh tbo it n’bo
]
go d " te
h
fl ire sot cme
o
B dtho eeb
,
o td
h
if c er to i dom icn td a ioric y nt , . ay ro yu
w
c ia lln
b
u ese
u
pt dhe
at
f eo dllo ww iti hn og
u
c to td he e: MMBen7 c9 h.6 MMBe7 n7 c.6
h-CN
7 S5 E.9 EDImage81. L8 LaVAW
key"Bobo". Qwen-VL-Chat LLaVA-1.5 Monkey
Free‐form Text‐Image Composition Free‐formComprehension Ours InternLM-XC CogVLM-17B
Figure1.Overviewoffree-formtext-imagecompositionandcomprehensionofInternLM-XComposer2.OurmodelbasedonInternLM2-
7B[77]notonlysignificantlyoutperformsexistingmultimodalmodelsbutalsomatchesorevensurpassesGPT-4V[58]andGemini
Pro[76]incertainassessments.(Pleasezoom-intoseethedetails.)
Abstract ters exclusively to image tokens to preserve the integrity
of pre-trained language knowledge, striking a balance be-
We introduce InternLM-XComposer2, a cutting-edge tween precise vision understanding and text composition
vision-language model excelling in free-form text-image with literary talent. Experimental results demonstrate the
composition and comprehension. This model goes be- superiorityofInternLM-XComposer2basedonInternLM2-
yond conventional vision-language understanding, adeptly 7Binproducinghigh-qualitylong-textmulti-modalcontent
crafting interleaved text-image content from diverse in- and its exceptional vision-language understanding perfor-
puts like outlines, detailed textual specifications, and ref- mance across various benchmarks, where it not only sig-
erence images, enabling highly customizable content cre- nificantlyoutperformsexistingmultimodalmodelsbutalso
ation. InternLM-XComposer2 proposes a Partial LoRA matchesorevensurpassesGPT-4VandGeminiProincer-
(PLoRA) approach that applies additional LoRA parame- tainassessments.Thishighlightsitsremarkableproficiency
in the realm of multimodal understanding. The InternLM-
*indicatesequalcontribution.
1
4202
naJ
92
]VC.sc[
1v02461.1042:viXraXComposer2 model series with 7B parameters are pub- Thisinvolvesfeedingforwardimagetokenswithadditional
liclyavailableathttps://github.com/InternLM/ LoRA[33](Low-RankAdaptation)parameters, whilelan-
InternLM-XComposer. guage tokens retain the original architecture. This selec-
tiveenhancementensuresrobustperformanceinbothvisual
and textual domains. (2) High-quality and Diverse Data
1.Introduction Foundataion: Thequalityanddiversityofthetrainingdata
arepivotal. Ourdatasetforfree-formtext-imagecomposi-
In recent years, there has been a remarkable evolution in
tionexcelsin:adheringtocomplexinstructions,customiza-
the field of large language models (LLMs) [8, 16, 17, 57,
tion with text and image for tailored content, high-quality
58, 68, 79]. Foremost among these, models like Chat-
and stylistically diverse writing, and versatile text editing
GPT [57] have completely altered human interaction with
including condensing, expanding, and revising. For ex-
technology. Concurrently, avarietyofopen-sourceLLMs,
ceptional vision-language comprehension capabilities, we
suchasLlama[78],Mistra[37],InternLM[77],QWen[65],
gather a wide range of high-quality pretraining and super-
GLM [25], and Baichuan [7], have empowered the cus-
vised fine-tuning multimodal data. This collection spans
tomizationofLLMs. Buildingontheseopen-sourcefoun-
various aspects and types, such as captions, general QA,
dations, the community has seen substantial progress in
scientific QA, chat-based QA, mathematical QA, concept
multimodal large language models (MLLMs) [6, 21, 29,
knowledge,conversation,andtext-imagecomposition.
48, 49, 82, 95, 100]. These MLLMs are adept at in-
InternLM-XComposer2 surpasses existing benchmarks
terpreting images and engaging in text-image dialogues,
in both composition and comprehension. In the creation
showcasing impressive multimodal understanding. Unlike
benchmarkofOpenCompass[18]forevaluatingthecreativ-
traditional MLLMs, a recent innovation, i.e., InternLM-
ity of LLMs, InternLM-XComposer2 showcases outstand-
XComposer [95], has focused on using MLLMs for text-
ingperformance. Todemostrateourmultimodalcomphren-
imagecompositionandcomprehension,markinganoveldi-
sion capility, we compare our InternLM-XComposer2 on
rectioninMLLMresearch. However,thispioneeringwork
a list of benchmarks with both open-source MLLMs and
is currently limited to generating text-image articles based
closed-source APIs, e.g., GPT4V [58], Gemini Pro [76],
ontitlesalone,lackingthesophisticationtomeetmorecom-
and Qwen-VL Plus [19]. We report results in Math-
plexcompositionrequirements.Furthermore,whileachiev-
Vista [52], MMMU [91], AI2D [40], MME [27], MM-
ing leading performance at its inception, this model still
Bench [51], MMBench-Chinese [51], SEED-Bench (Im-
possesses significant potential for enhancement in detailed
age)[41],LLaVA-Bench(In-the-Wild)[49],QBench[85],
perception and complex reasoning capabilities to advance
MM-Vet [90], HallusionBench [31], ChartQA [56], and
itsvision-languagecomprehensionperformance.
POPE [45]. InternLM-XComposer2 based on InternLM2-
Thisobservationmotivatesthedevelopmentofmoread-
7Bsignificantlyexceedstheperformanceofexistingopen-
vancedvision-languagemodelscapableofpracticalandpo-
source models by an impressive margin. Remarkably, it
tenttext-imagecompositionandcomprehension. Inthispa-
demonstrates superior performance to GPT4V [58], Gem-
per, we introduce InternLM-XComposer2, a cutting-edge
iniPro[76]acrosssixbenchmarks.
model excelling in free-form text-image composition and
comprehension,builtbasedonInternLM2[77]. InternLM-
2.RelatedWorks
XComposer2representsasignificantadvancementoverits
predecessor,InternLM-XComposer[95],inbothtext-image Large Language Models (LLMs). Recent LLM archi-
composition and comprehension. InternLM-XComposer2 tectures have marked a transition from encoder-decoder
isadeptatproducinghigh-quality,integratedtext-imagear- frameworks (e.g., BERT [22], T5 [68]) to an emphasis on
ticles from a variety of free-form inputs, such as detailed decoder-only models employed with autoregressive train-
specifications, structured outlines, and reference images, ing techniques for next-token prediction (e.g., GPT [67]).
servingtoawiderangeofapplicationcontexts.Intherealm The following works (e.g., GPT3 [8], InstructGPT [60],
of multimodal understanding, it demonstrates exceptional ChatGPT [57], PaLM [17]) have seen the integration of
capabilities in detailed perception, logical reasoning, and advanced techniques such as instruction-tuning and Rein-
extensive knowledge integration. Its performance signifi- forcement Learning from Human Feedback (RLHF). Cou-
cantlysurpassesthatofexistingopen-sourceMLLMs,and pled with expansive parameter sizes and extensive train-
itstandsonparwith,orevenexceeds,advancedmodelslike ingdata,theseLLMmodelshaveachievedsubstantialper-
GPT-4V[58]andGeminiPro[76]invariousbenchmarks. formance enhancements across a diverse range of Nat-
The appealing capabilities of InternLM-XComposer2 ural Language Processing (NLP) tasks. Other notable
are primarily due to two critical design elements. (1) LLMs encompass a range of developments, such as the
Partial LoRA: The Partial LoRA (P-LoRA) design har- OPT [96], LLaMA series [78, 79], e.g., Mistral [37, 38],
monizes its abilities in composition and comprehension. InternLM [77], GLM series [25, 93], Qwen series [6, 65],
2Baichuan[7],Skywork[84]andFalcon[61]havemadesig-
nificantcontributionstothefield.
MultimodalLargeLanguageModels(MLLMs). Vision-
language models (VLMs), exemplified by CLIP [66] and
itssubsequentworks[26,36,43,44,50,75,94],alignim- 𝑊 ∈ ℝ𝐶 𝑜𝑢𝑡×𝐶 𝑟
𝐵 Pretrained
age and text features in a unified embedding space. This
alignment is achieved through contrastive learning objec- Weights
tives applied to extensive image-text pair datasets. VLMs
achieve strong zero-shot and few-shot performance, show- 𝑊 𝐴 ∈ ℝ𝐶 𝑟×𝐶 𝑖𝑛 𝑊 0 ∈ ℝ𝐶 𝑜𝑢𝑡×𝐶 𝑖𝑛
casing significant generalization abilities across a range of
downstreamtasks.
Benefiting from existing large language models and
VLMsasthevisualencoder,recentMultimodalLargeLan-
guage Models (MLLMs) [12, 14, 15, 24, 28, 58] achieve
visual perception, understanding and reasoning abilities, Figure2. TheillustrationofthePartial-LoRA.Thebluetokens
showsuperbperformanceindiversevision-languagetasks. represent the visual tokens and the gray tokens are the language
tokens.OurPartial-LoRAisonlyappliedtothevisualtokens.
A series of studies [2, 5, 9, 10, 20, 21, 42, 46, 49, 62,
64, 80, 86, 92, 97, 98, 100] have explored further im-
This model boasts exceptional multi-lingual capabilities
prove the MLLM in different dimensions, such as instruc-
andhasdemonstratedimpressiveresultsinbenchmarks. In
tion tuning [11, 49, 98], efficient fine-tuning [33], high-
practical applications, we utilize the InternLM2-7B-Chat-
resolution image inputs [6, 82, 83], hallucination mitiga-
SFTvariantasourLLM.
tion[34,87,99],imagegeneration[23,30,74,89],3Dun-
Partial Low-Rank Adaptation. In the realm of multi-
derstanding [63] and image-text comprehension and com-
modal Language Learning Models (LLMs), one insuffi-
position[95].
cientlyexploredareaistheeffectivealignmentofdifferent
To enable highly customizable content creation, our
modalities. A desired alignment should potentially enrich
modelisdesignedforfree-formtext-imagecompositionand
the LLM with new modality-specific knowledge, while si-
comprehension based on MLLMs. We use Intern-LM2 as
multaneously preserving its inherent capabilities. Current
the LLM and CLIP ViT-Large as the visual encoder and
methodspredominantlyadoptoneoftwoapproaches: they
proposeanewpartialLoRAtoalignthetext-imagemodal-
either treat the visual token and language token equally or
ities. Given flexible and multi-modal user inputs such as
as entirely distinct entities. We contend that the first ap-
specifications, outlines, and reference images, our model
proachoverlookstheinherentpropertydistinctionsbetween
iscapableofgeneratinghigh-qualityinterleavedtext-image
modalities,whilethesecondapproachresultsinasubstan-
writtencontent.
tialalignmentcost.
Inourpursuitofeffectivemodalityalignment,weintro-
3.Method
duce Partial LoRA, a versatile plug-in module designed to
3.1.ModelArchitecture alignknowledgefromanewmodalitytotheLLM.Asillus-
tratedinFigureX,PartialLoRAdrawsinspirationfromthe
Ourproposedmodel,InternLM-XComposer2,incorporates
originalLoRAandincorporatesalow-rankadaptationthat
a vision encoder and a Language Learning Model (LLM).
isexclusivelyappliedtothenewmodalityportionofthein-
Thesetwocomponentsareinterconnectedviaaninnovative
put tokens. In our specific configuration, Partial LoRA is
Partial LoRA module. Given a set of images and text, the
appliedtoallvisualtokens.
LLM utilizes the output from the vision encoder as visual
Formally, for each linear layer L in the LLM blocks,
0
tokens and the tokenized text as language tokens. These we denote its weight matrix W
0
∈ R(Cout×Cin) and bias
tokensarethenconcatenatedtoformtheinputsequence. B
0
∈ RCout,whereC
in
andC
out
aretheinputandoutput
Vision Encoder. The vision encoder in our model is de-
dimension. Its corresponding Parital LoRA contains two
signed to extract high-level visual features from raw im- low-rank matrix W
A
∈ RCr×Cin and W
B
∈ RCout×Cr.
ages.Itispretrainedinanimage-languagecontrastiveman-
Withagiveninputx=[x ,x ],wehavetheoutputfeature
v t
ner(CLIP). Our findings indicate that, when used in con-
xˆby:
junction with our Partial LoRA module, a lightweight vi-
sionmodelperformseffectively. Forthesakeofefficiency, xˆ =W x +B
t 0 t 0
wehaveoptedtousetheOpenAIViT-Largemodel.
xˆ =W x +W W x +B
Large Language Model. We employ the recently intro- v 0 v B A v 0
xˆ=[xˆ ,xˆ ]
duced InternLM-2 as our Large Language Model (LLM). v t
3Task Dataset
GeneralSemanticAlignment ShareGPT4V-PT[11],COCO[13],Nocaps[1],TextCaps[73],LAION400M[69],SBU[59],CC3M[72]
WorldKnowledgeAlignment ConceptData[95]
VisionCapabilityEnhancement WanJuan[32],Flicker[88],MMC-Instruction[47]
Table1.DatasetsusedforPre-Training.Thedataarecollectedfromdiversesourcesforthethreeobjectives.
wherex v andx t arethevisualtokensandlanguagetokens Task Dataset
oftheinputsequencerespectively.
Multi-tasktraining
Caption ShareGPT4V[11],COCO[13],Nocaps[1]
3.2.Pre-Training GeneralQA VQAv2[4],GQA[35],OK-VQA[55]
ScienceQA AI2D[40],SQA[54]
During the pre-training phase, the LLM remains constant ChartQA DVQA[39],ChartQA[56]
while both the vision encoder and Partial LoRA are fine- MathQA MathQA[3],Geometry3K[53]
tuned to align the visual tokens with the LLM. The pre- WorldKnowledgeQA A-OKVQA[70],,KVQA[71]
Conversation LLaVA-150k[49],LVIS-Instruct4V[81]
trainingdataismeticulouslycuratedwiththreeobjectives
in mind: 1) general semantic alignment, 2) world knowl- Instructiontuning
Free-fromComposiiton In-housedata(RefertoSec.3.4)
edgealignment,3)visioncapabilityenhancement.
Conversation LLaVA-150k[49],LVIS-Instruct4V[81]
GeneralSemanticAlignment.Theobjectiveofgeneralse- ShareGPT-en&zh [16],InternLM-Chat[77]
manticalignmentistoequiptheMLLMwiththefundamen-
Table 2. Datasets used for Supervised Fine-Tuning. We collect
tal ability to comprehend image content. For instance, the
data from diverse sources to empower the model with different
MLLMshouldbeabletorecognizethatapictureofEinstein
capabilities.
represents‘ahuman’. Weutilizeimagecaptiondatafroma
variety of sources for this purpose, including high-quality,
detailedcaptionsfromShareGPT4V-PT,aswellasconcise
tionto490×490forimprovedperformance. ForthePartial
and precise captions from COCO, NoCaps, TextCaps, etc.
LoRA, we set a rank of 256 for all the linear layers in the
Duringthepre-trainingphase,weemployasimpleinstruc-
LLMdecoderblock. Ourtrainingprocessinvolvesabatch
tion: Describethisimagebriefly/indetail.
sizeof4906andspansacross2epochs. Thelearningrateis
World Knowledge Alignment. World knowledge repre- initiallysettoincreaseto2×10−4withinthefirst1%ofthe
sents an advanced capability of the MLLM. For instance,
trainingsteps. Followingthis,itdecreasesto0accordingto
theMLLMshouldbeabletoidentifythemaninthefigure
acosinedecaystrategy.Topreservethepre-existingknowl-
mentionedaboveas‘AlbertEinstein’andfurthertalksome-
edgeofthevisionencoder, weapplyalayer-wiselearning
thingabouthim. Toaligntheworldknowledgedepictedin
rate (LLDR) decay strategy and the decay factor is set to
theimagewiththeknowledgealreadyacquiredbytheLLM,
0.90.
wehaveconstructedaconceptdataset. Thisdatasetiscare-
fully filtered from the concept data utilized in InternLM-
3.3.SupervisedFine-tuning
XComposer [95]. Given that the text in the concept data
only partially describes the content in the image and their The pre-training phase aligns the visual feature with the
relationshipiscomplextomodel,weemployamorebroad language, enabling the Language Learning Model (LLM)
instruction: Tellmesomethingaboutthisimage. tocomprehendthecontentoftheimages. However, itstill
Vision Capability Enhancement. Finally, an advanced lackstheabilitytoeffectivelyutilizetheimageinformation.
MLLM necessitates certain vision-specific capabilities, Toovercomethislimitation,weintroducearangeofvision-
such as Optical Character Recognition (OCR), object lo- languagetasksthatthemodelengagesinduringthesubse-
calization(grounding),andtheunderstandingofstructured quentSupervisedFine-TuningStage. Thisstagecomprises
images(e.g.,charts,tables). Toachievethis,wehavecom- two sequential steps: Multi-task Training and Free-form
piledrelevantdatasets,asoutlinedinTable.1,andhaveim- Text-ImageComposition.Duringthisstage,wejointlyfine-
plementedcorrespondinginstructionsfortraining. tunethevisionencoder,LLM,andPartialLoRA.
Thanks to the design of Partial LoRA, the LLM is able Multi-task Training. As delineated in Table 2, the multi-
toadapttovisualtokenswhilemaintainingitsoriginallan- task training dataset is assembled from various sources,
guageprocessingcapabilities. ThefixedLLMalsoenables aimingtoequipthemodelwithabroadspectrumofcapabil-
ustodirectlyusein-contextlearningperformanceasamea- ities. Eachtaskisstructuredasaconversationalinteraction,
sureofpre-trainingquality. andtheinstructionsareaugmentedwithGPT-4toenhance
In our implementation, we employ the OpenAI CLIP diversity. Concurrently, to maintain the original language
ViT-L-14-336asthevisionencoder. Weincreaseitsresolu- capability, we also incorporate the supervised fine-tuning
4Method MathVista AI2D MMMU MME MMB MMBCN SEEDI LLaVAW QBenchT MM-Vet HallB ChartVQA
Open-Source SPH-MOE Monkey Yi-VL WeMM L-Int2 L-Int2 SPH-2 CogVLM Int-XC CogVLM Monkey CogAgent
PreviousSOTA 8x7B 10B 34B 6B 20B 20B 17B 17B 8B 30B 10B 18B
42.3 72.6 45.9 2066.6 75.1 73.7 74.8 73.9 64.4 56.8 58.4 68.4
Closed-sourceAPI
GPT-4V 49.9 78.2 56.8 1926.5 77.0 74.4 69.1 93.1 74.1 67.7 65.8 78.5
Gemini-Pro 45.2 73.9 47.9 1933.3 73.6 74.3 70.7 79.9 70.6 64.3 63.9 74.1
QwenVL-Plus 43.3 75.9 46.5 2183.3 67.0 70.7 72.7 73.7 68.9 55.7 56.4 78.1
Ours 57.6 78.7 42.0 2242.7 79.6 77.6 75.9 81.8 72.5 51.2 60.3 72.6
Table3. Comparisonwithclosed-sourceAPIsandpreviousopen-sourceSOTAs. OurInternLM-XComposer2getsSOTAresultsin
6ofthe12benchmarkswithonly7Bparameters,showingcompetitiveresultswithcurrentclosed-sourceAPIsandpreviousopen-source
SOTAMLLMs.Thebestresultsareboldandthesecond-bestresultsareunderlined.
datafromInternLM2,whichconstitutesafixed10%ofthe modificationssuchasshortening,expanding,andrewriting.
totalSupervisedFine-Tuning(SFT)data. Complex Instruction Adherence. We also capture in-
Free-form Text-Image Composition. To further enhance stances of adhering to complex instructions to create con-
the model’s ability to follow instructions and compose tent that caters to diverse demands like titles and outlines,
free-form image-text content, we employ data from both encompassingbothtextandimage-basedcompositions.
pure-text conversation corpora and vision-language con- CustomizationwithMaterials. Ourcollectionextendsto
versations, as outlined in Table 2. The dataset for free- materials used for personalized content creation, covering
form image-text composition is constructed following the both text and images, enabling customizable and unique
methodologydetailedinSection3.4. contentcreationexperiences.
Inourapproach,wejointlytrainallthecomponentswith Thedistributionofdataacrossthefourdimensionsisap-
a batch size of 2048 over 3000 steps. Data from multiple proximately equal, with a ratio of approximately 1:1:1:1.
sourcesaresampledinaweightedmanner,withtheweights Our method follows previous work [95] to identify suit-
basedonthenumberofdatafromeachsource. Themaxi- able positions for image insertion after generating the text
mumlearningrateissetto5×10−5,andeachcomponent content. A notable distinction in our approach is that
hasitsownuniquelearningstrategy.Forthevisionencoder, when users provide their own image materials, these im-
wesettheLayer-wiseLearningRateDecay(LLDR)to0.9, age materials are used for insertion instead of relying on
whichalignswiththepretrainingstrategy.FortheLLM,we retrievedimages[95]. Wealsoobservethathavingahigh-
employafixedlearningratescalefactorof0.2. Thisslows resolutionimageinputisnotessentialfortext-imagecom-
downtheupdateoftheLLM,achievingabalancebetween position. Therefore, following the pre-training phase, we
preserving its original capabilities and aligning it with vi- opttodown-sampletheimageinputresolutionto224x224
sionknowledge. duringtheSFTstageoffree-formtext-imagecomposition.
3.4.Free-formText-ImageComposition 4.Experiments
Free-formtext-imagecompositionreferstothecombination In this section, we validate the benchmark performance
oftextualcontentandvisualelementsinaflexibleandunre- of our InternLM-XComposer2 after the supervised fine-
strictivemanner. Ourmodelgeneratesinterleavedtextand tuning.
images, specifically customized to align with the text re-
4.1.MLLMBenchmarkresults.
quirementsprovidedbyusers,whichmayincludeelements
suchasatitle,outline,andwritingmaterial,andoptionally, In Table.3 and Table.4, we compare our InternLM-
anyvisualrequirementslikeimageresources. XComposer2 on a list of benchmarks with both SOTA
To facilitate free-form text-image composition, we col- open-source MLLMs and closed-source APIs. Here we
lectawiderangeofhigh-qualityanddiversein-housedata report results in MathVista[52], MMMU[91], AI2D[40],
acrossfourkeydimensions. Thesedimensionsencompass: MME Perception (MMEP) [27], MME Cognition
VariedWritingStyles. Ourdataspansamultitudeofwrit- (MMEC)[27], MMBench (MMB) [51], MMBench-
ingstyles,fromacademicpaperstosocialmediapostsand Chinese (MMBCN) [51], SEED-Bench Image Part
poems, ensuring a rich and diverse collection of text and (SEEDI)[41], LLaVA-Bench In-the-Wild (LLaVAW)
imagecontents. [49], QBench-Testset (QBenchT)[85], MM-Vet [90],
Flexible Text Editing. Our dataset includes extensive ex- HallusionBench(HallB)[31],ChartQA[56],POPE[45].
amples of text editing, encompassing a wide spectrum of Comparison with Closed-Source APIs. As shown in
5Method LLM MathVista MMMU MMEP MMEC MMB MMBCN SEEDI LLaVAW QBenchT MM-Vet HallB
BLIP-2 FLAN-T5 - 35.7 1,293.8 290.0 - - 46.4 38.1 - 22.4 -
InstructBLIP Vicuna-7B 25.3 30.6 - - 36.0 23.7 53.4 60.9 55.9 26.2 53.6
IDEFICS-80B LLaMA-65B 26.2 24.0 - - 54.5 38.1 52.0 56.9 - 39.7 46.1
Qwen-VL-Chat Qwen-7B 33.8 35.9 1,487.5 360.7 60.6 56.7 58.2 67.7 61.7 47.3 56.4
LLaVA Vicuna-7B 23.7 32.3 807.0 247.9 34.1 14.1 25.5 63.0 54.7 26.7 44.1
LLaVA-1.5 Vicuna-13B 26.1 36.4 1,531.3 295.4 67.7 63.6 68.2 70.7 61.4 35.4 46.7
ShareGPT4V Vicuna-7B 25.8 36.6 1,567.4 376.4 68.8 62.2 69.7 72.6 - 37.6 49.8
CogVLM-17B Vicuna-7B 34.7 37.3 - - 65.8 55.9 68.8 73.9 - 54.5 55.1
LLaVA-XTuner InernLM2-20B 24.6 39.4 - - 75.1 73.7 70.2 63.7 - 37.2 47.7
Monkey-10B Qwen-7B 34.8 40.7 1,522.4 401.4 72.4 67.5 68.9 33.5 - 33.0 58.4
InternLM-XC InernLM-7B 29.5 35.6 1,528.4 391.1 74.4 72.4 66.1 53.8 64.4 35.2 57.0
Ours InernLM2-7B 57.6 42.0 1,712.0 530.7 79.6 77.6 75.9 81.8 72.5 51.2 60.3
Table4. Comparisonwithopen-sourceSOTAmethods. InternLM-XComposer2outperformscompetitorsin10outof11benchmarks.
Thebestresultsareboldandthesecond-bestresultsareunderlined.
Method LLM POPE HallusionBench* w/oRef wRef
Method
Avg. C R UDF LC Avg. C R UDF LC
Closed-sourceAPI
GPT-4V - - 65.8 GPT-4 6.32 5.22 5.98 7.17 7.47 5.98 5.30 5.55 6.51 7.08
Gemini-Pro - - 63.9
QWen-72b-Chat 5.70 4.78 5.16 6.37 7.13 5.31 4.94 4.72 5.71 6.50
QwenVL-Plus - - 56.4
Yi-34b-Chat 6.03 4.91 5.68 6.79 7.35 5.71 5.03 5.22 6.18 6.87
Open-sourceMLLMs
Ours 6.24 5.11 6.12 7.03 7.45 5.90 5.21 5.76 6.27 6.93
InstructBLIP Vicuna-7B 78.9 53.6
IDEFICS-80B LLaMA-65B - 46.1
Table 6. Comparison on CreationBench [18]. We report the
Qwen-VL-Chat Qwen-7B - 56.4
results with and without the GPT-4 referenced answer. We re-
LLaVA Vicuna-7B 80.2 44.1
LLaVA-1.5 Vicuna-13B 85.9 46.7 porttheaveragescoreandothermetricsincludingCreativity(C),
InternLM-XC InernLM-7B - 57.0 Richness(R), User Demand Fulfillment (UDF), and Logical Co-
herence(LC).
Ours InernLM2-7B 87.7 60.3
crucialmetricintheevaluationofanMLLM.Inthisreport,
Table 5. Hallucination Evaluation on POPE and Hallu- we present the results obtained on both POPE and Hallu-
sionBench. IntenrLM-XComposer2 outperforms open-source sionBench. AsindicatedinTable.5,ourmodelachievesan
MLLMsandperformsonparwithclosed-sourceAPIs.*Weskip average F1-score of 87.7 across the three tracks of POPE,
thenon-visualquestions,followingthesettinginVLMEvalKit[18] setting a new state-of-the-art (SOTA) benchmark. In the
caseofHallusionBench, ourmodelsurpassestheaccuracy
Table.3,InternLM-XComposer2demonstratescompetitive- of all open-source models, establishing itself as the new
ness with Closed-Source APIs across numerous bench- SOTA.Furthermore,itoutperformstheclosed-sourceAPI,
marks. Forinstance, ourmodelachievesascoreof 57.6% QwenVL-Plus.
onMathVistaand78.9onAI2D,outperformingtheseAPIs
4.2.CreationBenchResults
by a significant margin. Meanwhile, despite having only
7Bparameters, ourmodelattainsaslightlyworsescoreof We use the CreationBench benchmark from OpenCom-
43.0%onthechallengingcollege-levelbenchmarkMMMU. pass [18] to assess the writing ability of our InternLM-
The strong performance can be attributed to the superb XComposer2. As shown in Table 6, the results indicate
knowledge acquired by the new InternLM2 LLM and the that our approach not only excels in overall creativity but
efficientPLoRAtrainingstrategy,whichenabledustoalign alsosignificantlyimprovesuponkeymetricsoverprevious
theLLMwithimagefeatureswhilepreservingitslanguage open-source LLMs. When compared without the GPT-4
capability. referenced answer, our method scored an impressive 6.24
ComparisonwithOpen-SourceModels. Wealsoconduct overall.EvenwhenevaluatedwiththeGPT-4reference,our
a comprehensive comparison with open-source MLLMs method maintained strong performance, achieving scores
under a similar model scale. As shown in Table.4, our that underscore its ability to generate responses with high
modelsignificantlyoutperformsexistingopen-sourcemod- levelsofcreativityandlogicalstructure,criticalforuseren-
els, achieving state-of-the-art results across all bench- gagementandsatisfactioninconversationalAIapplications.
marks. Notably, InternLM-XComposer2 is the first model
4.3.Qualitativeresults.
toachieveascoreexceeding1700ontheMME-Perception
benchmark. Furthermore, itattainedanaccuracyofnearly PleaserefertotheAppendixforourqualitativeresultsofthe
80%ontheMMBench. free-formimage-textcompositionsandmultimodalconver-
HallucinationEvaluation. Visualhallucinationservesasa sations.
65.Conclusion References
In this paper, we present InternLM-XComposer2, which [1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,
demonstrates its exceptional capabilities in the field of Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,
vision-language understanding and free-form text-image Stefan Lee, and Peter Anderson. Nocaps: Novel object
captioningatscale. InProceedingsoftheIEEE/CVFinter-
composition. Our proposed innovative Partial LoRA
nationalconferenceoncomputervision,pages8948–8957,
(PLoRA) approach, which applies additional LoRA pa-
2019. 4
rameters exclusively to image tokens, has proven effective
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
in preserving the integrity of pre-trained language knowl-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
edge while striking a balance between precise vision un- Mensch,KatieMillican,MalcolmReynolds,RomanRing,
derstanding and text composition with literary talent. Our ElizaRutherford,SerkanCabi,TengdaHan,ZhitaoGong,
model’s performance across various benchmarks not only SinaSamangooei,MarianneMonteiro,JacobMenick,Se-
significantly outperforms existing multimodal models but bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
also matches or even surpasses GPT-4V and Gemini Pro hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
in certain assessments, underscoring its remarkable profi- Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: avisuallanguagemodelforfew-shotlearning,
ciency in the realm of multimodal understanding. This
2022. 3
research opens up new possibilities for highly customiz-
[3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-
ablecontentcreationandpavesthewayforfutureadvance-
Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
ments in the MLLM field. The potential applications of
Mathqa: Towards interpretable math word problem solv-
InternLM-XComposer2 are vast and exciting, promising a ing with operation-based formalisms. arXiv preprint
future where AI can understand and generate high-quality arXiv:1905.13319,2019. 4
long-textmulti-modalcontentwitheaseandprecision. [4] StanislawAntol,AishwaryaAgrawal,JiasenLu,Margaret
Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi
Parikh. Vqa: Visualquestionanswering. InInternational
ConferenceonComputerVision(ICCV),2015. 4
[5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,
Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan
Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon
Kornblith,PangWeiKoh,GabrielIlharco,MitchellWorts-
man, and Ludwig Schmidt. Openflamingo: An open-
sourceframeworkfortraininglargeautoregressivevision-
languagemodels. arXiv.org,2023. 3
[6] JinzeBai, ShuaiBai, ShushengYang, ShijieWang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: A frontier large vision-language model
withversatileabilities. arXiv.org,2023. 2,3
[7] Baichuan. Baichuan2: Openlarge-scalelanguagemodels.
arXiv.org,2023. 2,3
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Languagemodelsarefew-shotlearners. AdvancesinNeu-
ral Information Processing Systems (NeurIPS), 33:1877–
1901,2020. 2
[9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,
VikasChandra,YunyangXiong,andMohamedElhoseiny.
Minigpt-v2: large language model as a unified interface
for vision-language multi-task learning. arXiv preprint
arXiv:2310.09478,2023. 3
[10] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
FengZhu,andRuiZhao. Shikra: Unleashingmultimodal
llm’sreferentialdialoguemagic. arXiv.org,2023. 3
[11] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui
He, JiaqiWang, FengZhao, andDahuaLin. Sharegpt4v:
Improvinglargemulti-modalmodelswithbettercaptions.
arXivpreprintarXiv:2311.12793,2023. 3,4
7[12] XiChen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, 2023. 2,3
Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, [22] JacobDevlin,Ming-WeiChang,KentonLee,andKristina
Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak- Toutanova. Bert: Pre-trainingofdeepbidirectionaltrans-
eri,MostafaDehghani,DanielSalz,MarioLucic,Michael formersforlanguageunderstanding. arXiv.org,2018. 2
Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, [23] RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,Zheng
Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ge,JinrongYang,LiangZhao,JianjianSun,HongyuZhou,
Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng
Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Ma, and Li Yi. Dreamllm: Synergistic multimodal com-
Beyer, JulienAmelot, KentonLee, AndreasPeterSteiner, prehensionandcreation. arXivpreprintarXiv:2309.11499,
Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, 2023. 3
Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhos- [24] DannyDriess,FeiXia,MehdiS.M.Sajjadi,CoreyLynch,
seini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
RaduSoricut. Pali-x: Onscalingupamultilingualvision Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
andlanguagemodel,2023. 3 Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna worth,SergeyLevine,VincentVanhoucke,KarolHausman,
Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence MarcToussaint, KlausGreff, AndyZeng, IgorMordatch,
Zitnick.Microsoftcococaptions:Datacollectionandeval- andPeteFlorence. Palm-e: Anembodiedmultimodallan-
uationserver,2015. 4 guage model. In arXiv preprint arXiv:2303.03378, 2023.
[14] XiChen,XiaoWang,LucasBeyer,AlexanderKolesnikov, 3
Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebas- [25] ZhengxiaoDu,YujieQian,XiaoLiu,MingDing,Jiezhong
tian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Qiu, Zhilin Yang, and Jie Tang. Glm: General language
DanielSalz,XiXiong,DanielVlasic,FilipPavetic,Keran model pretraining with autoregressive blank infilling. In
Rong,TianliYu,DanielKeysers,XiaohuaZhai,andRadu Proceedingsofthe60thAnnualMeetingoftheAssociation
Soricut. Pali-3 vision language models: Smaller, faster, for Computational Linguistics (Volume 1: Long Papers),
stronger,2023. 3 pages320–335,2022. 2
[15] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio- [26] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,
vanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Cao. Eva: Exploringthelimitsofmaskedvisualrepresen-
Kolesnikov,JoanPuigcerver,NanDing,KeranRong,Has- tation learning at scale. In Proceedings of the IEEE/CVF
sanAkbari,GauravMishra,LintingXue,AshishThapliyal, Conference on Computer Vision and Pattern Recognition
James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, (CVPR),pages19358–19369,2023. 3
ChaoJia,BurcuKaragolAyan,CarlosRiquelme,Andreas [27] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Steiner,AneliaAngelova,XiaohuaZhai,NeilHoulsby,and Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-
RaduSoricut.Pali:Ajointly-scaledmultilinguallanguage- rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-
imagemodel,2023. 3 grong Ji. Mme: A comprehensive evaluation benchmark
[16] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhang- for multimodal large language models. arXiv preprint
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, arXiv:2306.13394,2023. 2,5
Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and [28] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,
EricP.Xing. Vicuna: Anopen-sourcechatbotimpressing Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang
gpt-4with90%*chatgptquality,March2023. 2,4 Shen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-
[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, hui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul sheng Li, and Xing Sun. A challenger to gpt-4v? early
Barham, Hyung Won Chung, Charles Sutton, Sebastian explorationsofgeminiinvisualexpertise. arXivpreprint
Gehrmann, et al. Palm: Scaling language modeling with arXiv:2312.12436,2023. 3
pathways. arXiv.org,2022. 2 [29] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
[18] OpenCompass Contributors. Opencompass: A univer- Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xi-
sal evaluation platform for foundation models. https: angyu Yue, Hongsheng Li, and Yu Jiao Qiao. Llama-
//github.com/open-compass/opencompass, adapter v2: Parameter-efficient visual instruction model.
2023. 2,6 ArXiv,abs/2304.15010,2023. 2
[19] QWen Contributors. Qwen-vl-plus. https:// [30] YuyingGe,YixiaoGe,ZiyunZeng,XintaoWang,andYing
huggingface.co/spaces/Qwen/Qwen-VL- Shan. Plantingaseedofvisioninlargelanguagemodel. 3
Plus,year=2023. 2 [31] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,
[20] XTuner Contributors. Xtuner: A toolkit for efficiently Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,
fine-tuningllm. https://github.com/InternLM/ FurongHuang,YaserYacoob,DineshManocha,andTianyi
xtuner,2023. 3 Zhou. Hallusionbench: An advanced diagnostic suite for
[21] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat entangledlanguagehallucination&visualillusioninlarge
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale vision-languagemodels,2023. 2,5
Fung, and Steven Hoi. Instructblip: Towards general- [32] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin
purpose vision-language models with instruction tuning, Wang, WeiLi, HangYan, JiaqiWang, andDaLin. Wan-
8juan: A comprehensive multimodal dataset for advancing Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang,
englishandchineselargemodels. ArXiv,abs/2308.10755, LuYuan, LeiZhang, Jenq-NengHwang, Kai-WeiChang,
2023. 4 andJianfengGao. Groundedlanguage-imagepre-training.
[33] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen- InProceedingsoftheIEEE/CVFConferenceonComputer
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu VisionandPatternRecognition(CVPR),2022. 3
Chen. LoRA:Low-rankadaptationoflargelanguagemod- [45] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXin
els. InInternationalConferenceonLearningRepresenta- Zhao,andJi-RongWen. Evaluatingobjecthallucinationin
tions,2022. 2,3 largevision-languagemodels,2023. 2,5
[34] QidongHuang,XiaoyiDong,PanZhang,BinWang,Con- [46] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,
ghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.
Nenghai Yu. Opera: Alleviating hallucination in multi- Monkey: Image resolution and text label are important
modal large language models via over-trust penalty and things for large multi-modal models. arXiv preprint
retrospection-allocation. arXivpreprintarXiv:2311.17911, arXiv:2311.06607,2023. 3
2023. 3 [47] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,
[35] DrewAHudsonandChristopherDManning. Gqa:Anew Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong
dataset for real-world visual reasoning and compositional Yu. Mmc: Advancing multimodal chart understand-
question answering. Conference on Computer Vision and ing with large-scale instruction tuning. arXiv preprint
PatternRecognition(CVPR),2019. 4 arXiv:2311.10774,2023. 4
[36] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana [48] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.
Parekh, HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, Improved baselines with visual instruction tuning. arXiv
and Tom Duerig. Scaling up visual and vision-language preprintarXiv:2310.03744,2023. 2
representationlearningwithnoisytextsupervision. InPro- [49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
ceedingsoftheInternationalConferenceonMachinelearn- Lee. Visualinstructiontuning. arXiv.org,2023. 2,3,4,5
ing(ICML),pages4904–4916.PMLR,2021. 3 [50] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
[37] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun
Chris Bamford, Devendra Singh Chaplot, Diego de las Zhu,etal. Groundingdino: Marryingdinowithgrounded
Casas,FlorianBressand,GiannaLengyel,GuillaumeLam- pre-trainingforopen-setobjectdetection. arXiv.org,2023.
ple, Lucile Saulnier, Le´lio Renard Lavaud, Marie-Anne 3
Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, [51] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
Thomas Wang, Timothe´e Lacroix, and William El Sayed. Songyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,
Mistral7b,2023. 2 ConghuiHe, ZiweiLiu, KaiChen, andDahuaLin. Mm-
[38] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, bench: Is your multi-modal model an all-around player?
Arthur Mensch, Blanche Savary, Chris Bamford, Deven- arXiv:2307.06281,2023. 2,5
draSinghChaplot,DiegodelasCasas,EmmaBouHanna, [52] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-
FlorianBressand,etal. Mixtralofexperts. arXivpreprint yuanLi,HannanehHajishirzi,HaoCheng,Kai-WeiChang,
arXiv:2401.04088,2024. 2 Michel Galley, and Jianfeng Gao. Mathvista: Evaluating
[39] Kushal Kafle, Brian Price, Scott Cohen, and Christopher mathematicalreasoningoffoundationmodelsinvisualcon-
Kanan. Dvqa: Understandingdatavisualizationsviaques- texts. InInternationalConferenceonLearningRepresen-
tionanswering. InProceedingsoftheIEEEconferenceon tations(ICLR),2024. 2,5
computervisionandpatternrecognition,pages5648–5656, [53] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan
2018. 4 Huang,XiaodanLiang,andSong-ChunZhu. Inter-gps:In-
[40] AniruddhaKembhavi,MikeSalvato,EricKolve,Minjoon terpretablegeometryproblemsolvingwithformallanguage
Seo, HannanehHajishirzi, andAliFarhadi. Adiagramis andsymbolicreasoning.InThe59thAnnualMeetingofthe
worth a dozen images. In Computer Vision–ECCV 2016: AssociationforComputationalLinguistics(ACL),2021. 4
14th European Conference, Amsterdam, The Netherlands, [54] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-
October11–14,2016,Proceedings,PartIV14,pages235– WeiChang,Song-ChunZhu,OyvindTafjord,PeterClark,
251.Springer,2016. 2,4,5 and Ashwin Kalyan. Learn to explain: Multimodal rea-
[41] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix- soning via thought chains for science question answer-
iaoGe,andYingShan. Seed-bench: Benchmarkingmulti- ing. AdvancesinNeuralInformationProcessingSystems,
modalllmswithgenerativecomprehension,2023. 2,5 35:2507–2521,2022. 4
[42] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, [55] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
JingkangYang,andZiweiLiu.Otter:Amulti-modalmodel RoozbehMottaghi. Ok-vqa: Avisualquestionanswering
within-contextinstructiontuning. arXiv.org,2023. 3 benchmarkrequiringexternalknowledge. InProceedings
[43] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. oftheIEEE/cvfconferenceoncomputervisionandpattern
Blip: Bootstrapping language-image pre-training for uni- recognition,pages3195–3204,2019. 4
fiedvision-languageunderstandingandgeneration. InPro- [56] AhmedMasry, DoXuanLong, JiaQingTan, ShafiqJoty,
ceedingsoftheInternationalConferenceonMachinelearn- and Enamul Hoque. Chartqa: A benchmark for question
ing(ICML),pages12888–12900.PMLR,2022. 3 answering about charts with visual and logical reasoning.
[44] LiunianHaroldLi*,PengchuanZhang*,HaotianZhang*, arXivpreprintarXiv:2203.10244,2022. 2,4,5
9[57] OpenAI. Chatgpt. https://openai.com/blog/ Partha Pratim Talukdar. Kvqa: Knowledge-aware visual
chatgpt,2022. 2 questionanswering.InProceedingsoftheAAAIconference
[58] OpenAI. Gpt-4technicalreport,2023. 1,2,3 onartificialintelligence,2019. 4
[59] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. [72] PiyushSharma,NanDing,SebastianGoodman,andRadu
Im2text: Describing images using 1 million captioned Soricut. Conceptualcaptions:Acleaned,hypernymed,im-
photographs. In Neural Information Processing Systems agealt-textdatasetforautomaticimagecaptioning.InPro-
(NIPS),2011. 4 ceedingsofthe56thAnnualMeetingoftheAssociationfor
[60] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,Car- ComputationalLinguistics(Volume1:LongPapers),pages
roll Wainwright, Pamela Mishkin, Chong Zhang, Sand- 2556–2565,2018. 4
hini Agarwal, Katarina Slama, Alex Ray, et al. Training [73] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and
language models to follow instructions with human feed- AmanpreetSingh. Textcaps: adatasetforimagecaption-
back. AdvancesinNeuralInformationProcessingSystems ing with reading comprehension. In Computer Vision–
(NeurIPS),35:27730–27744,2022. 2 ECCV2020:16thEuropeanConference,Glasgow,UK,Au-
[61] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, gust23–28,2020,Proceedings,PartII16,pages742–758.
Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- Springer,2020. 4
dli,BaptistePannier,EbtesamAlmazrouei,andJulienLau- [74] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
nay. Therefinedwebdatasetforfalconllm:Outperforming Zhang,YuezeWang,HongchengGao,JingjingLiu,Tiejun
curatedcorporawithwebdata,andwebdataonly,2023. 3 Huang,andXinlongWang. Generativepretraininginmul-
[62] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan timodality. Jul2023. 3
Huang,ShumingMa,andFuruWei.Kosmos-2:Grounding [75] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,
multimodallargelanguagemodelstotheworld. arXiv.org, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang.
2023. 3 Alpha-CLIP:Aclipmodelfocusingonwhereveryouwant.
[63] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong arXivpreprintarXiv:2312.03818,2023. 3
Wu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao. [76] GeminiTeam. Gemini: Afamilyofhighlycapablemulti-
Gpt4point: Aunifiedframeworkforpoint-languageunder- modalmodels,2023. 1,2
standingandgeneration,2023. 3 [77] InternLMTeam. Internlm: Amultilinguallanguagemodel
[64] ZhangyangQi,YeFang,MengchenZhang,ZeyiSun,Tong with progressively enhanced capabilities. https://
Wu, Ziwei Liu, Dahua Lin, Jiaqi Wang, and Hengshuang github.com/InternLM/InternLM,2023. 1,2,4
Zhao. Gemini vs gpt-4v: A preliminary comparison and [78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
combinationofvision-languagemodelsthroughqualitative Martinet, Marie-Anne Lachaux, Timothe´e Lacroix, Bap-
cases,2023. 3 tiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar,
[65] Qwen.Introducingqwen-7b:Openfoundationandhuman- etal. Llama:Openandefficientfoundationlanguagemod-
alignedmodels(ofthestate-of-the-arts),2023. 2 els. arXiv.org,2023. 2
[66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya [79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
ingtransferablevisualmodelsfromnaturallanguagesuper- Llama 2: Open foundation and fine-tuned chat models,
vision. InProceedingsoftheInternationalConferenceon 2023. 2
Machinelearning(ICML),pages8748–8763.PMLR,2021. [80] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping
3 Zhong, PanZhang, XiaoyiDong, WeijiaLi, WeiLi, Jiaqi
[67] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Wang,etal.Vigc:Visualinstructiongenerationandcorrec-
Sutskever,etal.Improvinglanguageunderstandingbygen- tion. arXiv.org,2023. 3
erativepre-training. 2018. 2 [81] JunkeWang,LingchenMeng,ZejiaWeng,BoHe,Zuxuan
[68] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Wu, andYu-GangJiang. Toseeistobelieve: Prompting
Lee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi, gpt-4vfor bettervisual instructiontuning. arXivpreprint
and Peter J Liu. Exploring the limits of transfer learning arXiv:2311.07574,2023. 4
withaunifiedtext-to-texttransformer. JournalofMachine [82] WeihanWang,QingsongLv,WenmengYu,WenyiHong,Ji
LearningResearch(JMLR),21(1):5485–5551,2020. 2 Qi,YanWang,JunhuiJi,ZhuoyiYang,LeiZhao,Xixuan
[69] ChristophSchuhmann,RichardVencu,RomainBeaumont, Song,JiazhengXu,BinXu,JuanziLi,YuxiaoDong,Ming
RobertKaczmarczyk,ClaytonMullis,AarushKatta,Theo Ding,andJieTang. Cogvlm: Visualexpertforpretrained
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion- languagemodels,2023. 2,3
400m: Opendatasetofclip-filtered400millionimage-text [83] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,
pairs. arXivpreprintarXiv:2111.02114,2021. 4 Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and
[70] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Xiangyu Zhang. Vary: Scaling up the vision vocab-
Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A ulary for large vision-language models. arXiv preprint
benchmark for visual question answering using world arXiv:2312.06109,2023. 3
knowledge. InEuropeanConferenceonComputerVision, [84] TianwenWei, LiangZhao, LichangZhang, BoZhu, Lijie
pages146–162.Springer,2022. 4 Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu¨,
[71] Sanket Shah, Anand Mishra, Naganand Yadati, and RuiHu,etal. Skywork:Amoreopenbilingualfoundation
10model. arXivpreprintarXiv:2310.19341,2023. 3 35:36067–36080,2022. 3
[85] HaoningWu,ZichengZhang,ErliZhang,ChaofengChen, [95] PanZhang,XiaoyiDongBinWang,YuhangCao,ChaoXu,
LiangLiao,AnnanWang,ChunyiLi,WenxiuSun,Qiong LinkeOuyang,ZhiyuanZhao,ShuangruiDing,Songyang
Yan, Guangtao Zhai, et al. Q-bench: A benchmark for Zhang, Haodong Duan, Hang Yan, et al. Internlm-
general-purpose foundation models on low-level vision. xcomposer: A vision-language large model for advanced
arXivpreprintarXiv:2309.14181,2023. 2,5 text-imagecomprehensionandcomposition.arXivpreprint
[86] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan, arXiv:2309.15112,2023. 2,3,4,5
YiyangZhou,JunyangWang,AnwenHu,PengchengShi, [96] Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Yaya Shi, et al. mplug-owl: Modularization empowers Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
largelanguagemodelswithmultimodality.arXiv.org,2023. Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open
3 pre-trained transformer language models. arXiv preprint
[87] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao arXiv:2205.01068,2022. 2
Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, [97] HaozheZhao,ZefanCai,ShuzhengSi,XiaojianMa,Kaikai
and Enhong Chen. Woodpecker: Hallucination correc- An,LiangChen,ZixuanLiu,ShengWang,WenjuanHan,
tionformultimodallargelanguagemodels. arXivpreprint andBaobaoChang. Mmicl: Empoweringvision-language
arXiv:2310.16045,2023. 3 model with multi-modal in-context learning. arXiv.org,
[88] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock- 2023. 3
enmaier. From image descriptions to visual denotations: [98] Zhiyuan Zhao, Linke Ouyang, Bin Wang, Siyuan Huang,
New similarity metrics for semantic inference over event Pan Zhang, Xiaoyi Dong, Jiaqi Wang, and Conghui He.
descriptions. TransactionsoftheAssociationforComputa- Mllm-dataengine: An iterative refinement approach for
tionalLinguistics,2:67–78,2014. 4 mllm. arXiv.org,2023. 3
[89] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin [99] ZhiyuanZhao,BinWang,LinkeOuyang,XiaoyiDong,Ji-
Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh aqi Wang, and Conghui He. Beyond hallucinations: En-
Tang,BrianKarrer,ShellySheynin,CandaceRoss,Adam hancing lvlms through hallucination-aware direct prefer-
Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hov- enceoptimization.arXivpreprintarXiv:2311.16839,2023.
hannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen 3
Li, Susan Zhang, Gargi Ghosh, Yaniv Taigman, Maryam [100] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-
Armen Aghajanyan. Scaling autoregressive multi-modal language understanding with advanced large language
models:Pretrainingandinstructiontuning. 3 models. arXiv.org,2023. 2,3
[90] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
KevinLin,ZichengLiu,XinchaoWang,andLijuanWang.
Mm-vet: Evaluating large multimodal models for inte-
gratedcapabilities.arXivpreprintarXiv:2308.02490,2023.
2,5
[91] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,
Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,
Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin
Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu
Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and
Wenhu Chen. Mmmu: A massive multi-discipline multi-
modal understanding and reasoning benchmark for expert
agi. arXivpreprintarXiv:2311.16502,2023. 2,5
[92] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and
Chen Change Loy. Contextual object detection with
multimodal large language models. arXiv preprint
arXiv:2305.18279,2023. 3
[93] AohanZeng,XiaoLiu,ZhengxiaoDu,ZihanWang,Hanyu
Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,
XiaoXia,WengLamTam,ZixuanMa,YufeiXue,Jidong
Zhai, WenguangChen, ZhiyuanLiu, PengZhang, Yuxiao
Dong, and Jie Tang. GLM-130b: An open bilingual pre-
trained model. In The Eleventh International Conference
onLearningRepresentations(ICLR),2023. 2
[94] HaotianZhang,PengchuanZhang,XiaoweiHu,Yen-Chun
Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan,
Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying
localizationandvision-languageunderstanding. Advances
in Neural Information Processing Systems (NeurIPS),
11Please write a blog based on the title: French Pastries: A Sweet Indulgence
French Pastries: A Sweet Indulgence
Frenchpastriesaresomethingthateveryoneshouldindulge
in at least once in their lifetime. The delicate flavors and
beautiful presentation of these pastries make them the
perfecttreatforanyoccasion.Fromcroissantstomacarons,
éclairstotarts,Frenchpastriescomeinavarietyofshapes
Éclairs
and sizes. These sweet delights are not only delicious but
What are Eclairs? Èclairs (French pronunciation: (eklɛʁ))
also incredibly addictive. It's hard to resist just one pastry
are a type of pastry that originated in France and is still
when there are so many to choose from! Whether you're
widely consumed there. They are long, thin pastries that
looking for a quick breakfast on the go or a decadent
are filled with cream or pastry cream. The exterior has a
dessertafterdinner,Frenchpastriesaresuretosatisfyyour
crispy coating while the interior is soft. There are many
cravings.SowhynottreatyourselftosomeFrenchpastries
ways to make eclairs, but one thing remains constant—
today?Youwon'tregretit!
theyhaveanunforgettabletaste!
Tarts
Croissants
Tartsareopen-facedpiesthataretypicallyfilledwithfruit
The croissant is a crescent-shaped French pastry that has
or custard. They can be either sweet or savory, but the
becomeoneofthemostwell-knownFrenchpastriesinthe
sweet versions are definitely more popular in France.
world. This buttery, flaky pastry is created by laminating
TherearemanytypesoftartsinFrenchcuisine:appletart
dough with layers of butter; this process creates beautiful (tarte Tatin), almond tarTtips f(ort Vaisitirngt Sehanghaaimandine), chocolate tart
layers and flakiness when the croissant bakes. In France,
(tarte au chocolat), cheese tart (tarte aux fromages) and
croissantsare typically served forbreakfastor as a snack.
manyothers.
Croissants can be eaten plain or topped with sugar, jam,
Nutella, or other spreads. While croissants may seem like
an indulgent treat, they're actually relatively easy to make
at home. It's important to note that there are some
variations on how to make croissants, including sweeter
versions such as chocolate croissants and almond
croissants.
PainauChocolat
Pain au chocolat is a French pastry that consists of a
crescent-shaped roll filled with chocolate. It's similar to a
croissantbuthasarichchocolatefillingthataddsanextra
layer of decadence. The texture of the chocolate can be
anything from soft and creamy to hard and crunchy
dependingonpersonalpreference.Thebuttery,flakycrust
Macarons makesitadeliciouslyindulgenttreatforanytimeofday.
Macarons are small, colorful cookies that are made with
almond flour and filled with a variety of fillings such as In conclusion, French pastries are a sweet indulgence that
ganache,buttercream,orjam.Thedelicatetextureofthese should not be missed. Whether you are looking for
pastries make them perfect for pairing with coffee or tea. something light like a croissant or something more
FrenchMacaronsaretraditionallyassociatedwithParisian substantial like a tart, there is sure to be a French pastry
cafes, where they can be found in an array of flavors that will satisfy your cravings. So why not head to your
rangingfromclassictoexperimental. localbakeryandindulgeinsomeFrenchpastrygoodness?
Figure3.InterleavedcompositiongeneratedbyInternLM-XComposer2.
12根据以下标题：“中国水墨画：流动的诗意与东方美学”，创作长文章，字数不少于800字。请结合以
下文本素材：
“水墨画是由水和墨调配成不同深浅的墨色所画出的画，是绘画的一种形式，更多时候，水墨画被视为
中国传统绘画，也就是国画的代表。也称国画，中国画。墨水画是中国传统画之一。墨水是国画的起源，
以笔墨运用的技法基础画成墨水画。线条中锋笔，侧锋笔，顺锋和逆锋，点染，擦，破墨，拨墨的技法。
墨于水的变化分为五色。画成作品，题款，盖章。就是完整的墨水画作品。
基本的水墨画，仅有水与墨，黑与白色，但进阶的水墨画，也有工笔花鸟画，色彩缤纷。后者有时也
称为彩墨画。在中国画中，以中国画特有的材料之一，墨为主要原料加以清水的多少引为浓墨、淡墨、干
墨、湿墨、焦墨等，画出不同浓淡（黑、白、灰）层次。别有一番韵味称为“墨韵”。而形成水墨为主的
一种绘画形式。”
中国水墨画：流动的诗意与东方美学
技法与表现力
中国的传统绘画，特别是水墨画，被誉为“墨韵之舞”，以
水墨画的技法非常多样化，常见的有泼墨、破墨、积墨、宿
笔墨挥洒、线条交织的形式，展现了中国特有的审美情趣和文化
墨、枯墨等。这些技法各有特色，能够产生不同的效果。例如，
内涵。水墨画是一种独特的艺术形式，它不仅体现了中国传统文
泼墨适用于表现山峦起伏的大气磅礴；破墨则适合描绘树木枝叶
化的精髓，也彰显了东方的美学观念和哲学思想。在这篇文章中，
的繁茂纷杂；积墨则多用于营造厚重深邃的山水气氛。此外，水
我们将深入探讨水墨画的艺术特点，及其在中国文化中的重要地
墨画还常常结合诗词歌赋，形成一种诗情画意的境界。这种融合
位。
不仅提升了画作的意境，也让欣赏者能够更好地领略到其中的文
化内涵。
起源与发展
水墨画的历史可以追溯到中国古代的战国时期（公元前475
年—公元前221年）。当时的绘画主要是用毛笔在绢或纸上绘制， 价值与意义
而颜料则是从矿物、植物和动物中提取。随着时代的发展，绘画 水墨画作为中国传统文化的瑰宝，不仅是中国艺术的代表，
技法不断丰富，其中最重要的贡献之一就是墨的发现和使用。据 更是世界文化艺术宝库中的一颗璀璨明珠。它的价值不仅仅在于
传，秦始皇统一六国后，下令烧毁书籍，但一位书生藏起了一本 其独特的艺术风格和技术手段，更在于其所承载的深厚文化底蕴
珍贵的古书。这本古书后来被无意中丢弃，落入水中被泡湿。当 和精神内涵。通过欣赏水墨画，人们不仅可以感受到艺术家对自
书生捞起书页时，意外地发现上面的文字并未被水破坏，反而更 然的感悟和对生命的思考，也能够领悟到中国哲学的智慧和道德
加清晰了。这个故事告诉我们，水与墨的搭配是创造奇迹的关键。 准则。因此，保护和传承水T墨ips fo画r Visiti这ng Sha一nghai宝贵的文化遗产显得尤为重
从此以后，中国人开始将水和墨结合起来创作出具有独特韵味的 要。
画作。
结语
核心元素 综上所述，水墨画作为一门古老的艺术形式，不仅具有独特
的审美价值，更是中国传统文化的重要组成部分。它的魅力不仅
水墨画的核心元素包括笔法、墨色和构图。首先，笔法在水
体现在技艺的高超上，更在于其背后所蕴含的深刻思想和文化内
墨画中至关重要。不同的笔触和笔力能够产生丰富的视觉效果，
涵。让我们共同努力，让这门古老而又鲜活的艺术形式得以传承
如粗犷豪放的皴擦，细腻柔美的点染等。其次，墨色的运用也是
并发扬光大。
水墨画的精髓所在。通过调节水的多少，可以产生浓淡干湿的变
化，创造出深邃神秘的氛围。最后，构图则决定了画面的整体布
局和意境表达。合理的构图能够引导观者的视线流动，增强画面
的艺术感染力。
Figure4.InterleavedcompositiongeneratedbyInternLM-XComposer2.
13Please write a English poetry based on
Rabindranath Tagore's classic poem: "If
you shed tears when you miss the sun, you
Please write a English poetry based on
also miss the stars.", no less than 10
Rabindranath Tagore's classic poem: "If
sentences.
you shed tears when you miss the sun, you
also miss the stars.", no less than 10
sentences.
Whenthesunsets,theskyturnstodark
Andthetearsthatyoushed,leaveamark
Themountainsrisemajestically,
Theirpeaksshroudedinmistandcloud,
Butifyoulookup,you'llseethestars Thevalleysbelowstretchfarandwide,
Twinklinginthenight,likepreciouspearls Atapestryofgreenandgold.
Thetearsthatyoushed,canblindyoursight Theriversflowwithgentlegrace,
Theircurrentscarvingthroughtheearth,
Andyoumaymissthebeautythat'sinsight
Carryinglifeandbeautytoall,
Butifyoulookup,you'llseethestars
Asymphonyofnature'sworth.
Theforestsstandinsolemngrandeur,
TheirleavesrustlinTipsg for Viisitning Shtanhghaei breeze,
Alivingtestamenttotime,
Shiningbright,likeabeaconfromafar Aplaceofpeaceandtranquility.
Sodon'tbesadwhenthesunisgone
Anddon'tshedtearsforwhat'sdone
Instead,lookupandseethestars
Thatlightupthenight,likeshiningstars
Thetearsthatyoushed,canbereplaced
Bythebeautyofthestars,thatyoucanembrace
Sodon'tmissthesun,butlookuptothestars
Soletusnotweepforwhatislost,
Andfindthebeautythat'swaiting,nearandfar.
Butcherishwhatremains,
Forlifeisfleeting,buttheworld,
Isaplaceofendlesswondersandcharms.
Figure5.InterleavedcompositiongeneratedbyInternLM-XComposer2.
14请根据给定标题：“熊猫：自然界的温柔使者”，写一篇长文章。首先，详细介绍熊猫的基本特征，如
外貌，习性等。然后，探讨熊猫的自然栖息地及其面临的环境挑战，包括栖息地丧失和气候变化等问题。
之后，详细讨论熊猫的保护工作现状，包括建立自然保护区，大熊猫的人工繁育等。最后，分析熊猫在中
国和世界范围内的文化影响。字数不少于800字。
熊猫：自然界的温柔使者
此外，气候变暖也是大熊猫目前所面临的重要问题之一。据
研究，自20世纪70年代以来，我国东部沿海地区出现了持续增温
大熊猫，熊科哺乳动物，体肥头大、圆脸圆耳，幼年体色为
现象，这种趋势仍在继续发展。高温会导致竹林生长缓慢，从而
灰白色，因其独立特征和憨态可掬的样子被人们誉为“活化石”
降低了竹子作为大熊猫主要食物的供应量。除此之外，夏季降雨
和“中国国宝”。
量增多还会造成泥石流、滑坡等灾害，进一步破坏大熊猫的栖息
地。这些因素共同作用，使得大熊猫的种群数量日益减少。
从外表来看，大熊猫似乎是自然界最“温柔”的生物：黑白
相间的花纹让它的外形看起来更像是一种卡通形象；圆滚滚的身
躯和大大的头部让人忍不住想要抱一抱，即使不熟悉的人也会因 熊猫的保护现状如何？
为其憨态可掬的外表而心生好感…… 针对上述问题，政府和企业相继采取了一系列措施保护大熊
但就是这样一种温顺的生物，却也正面临着种种危机——栖 猫及其栖息地。例如，建立自然保护区和森林公园，限制人类活
息地丧失、气候变化等因素都对其生存构成威胁。 动范围，改善大熊猫的食物来源等。同时，还通过科学管理、合
理开发利用等方式推动了当地经济发展。尽管如此，由于历史原
因和自然环境的恶化等原因，野生大熊猫仍处于濒危状态。根据
国际自然保护联盟最新发布的红色名录，目前全球现存的大熊猫
约有1864只，其中约90%以上为中国境内的大熊猫。
熊猫的自然栖息地在哪？
首先来了解一下什么是大熊猫的自然栖息地。一般来说，大
熊猫生活在海拔1000~3000米的高山深谷中，以竹子为主食，喜
阴湿，耐寒冷的生态环境是其生存的基础条件。在我国西南地区
Tips for Visiting Shanghai
的四川盆地、陕西南部及甘肃南部的岷山、邛崃山、大小凉山、
秦岭等地是大熊猫的典型栖息地。此外，在我国的湖北神农架、
除了对野外环境进行保护外，人工繁育也是一个重要的手段。
湖南张家界以及河南伏牛山等地的亚热带山地也有发现。
通过实施繁殖计划和救助项目，帮助更多的大熊猫存活下来。目
前国内已有多个动物园、科研机构开展了大熊猫的人工饲养研究
工作，许多雌性个体已经产下幼崽。虽然存在一定的技术难度和
风险，但是随着经验的积累和技术进步，未来有望实现更多的大
熊猫繁衍后代。
熊猫的文化影响力如何体现？
最后我们来聊聊大熊猫在国际上的文化影响力。近年来，随
熊猫所面临的环境挑战
着媒体报道和旅游业的发展，越来越多的人了解到了这个可爱又
在远古时期，中国的南方地区就生活着原始大熊猫（也叫始
珍贵的物种。许多人甚至不远万里前往四川卧龙、雅安等地的自
熊猫）。大约在距今800万年前，全球进入冰期时代，地球开始
然保护区观赏野生的熊猫。而在影视作品中，《功夫熊猫》系列
变冷，植被逐渐向低纬度方向移动，这使得原本生活在高寒环境
电影更是将大熊猫的形象推向了一个全新的高度。此外，各种与
中的大熊猫失去了适宜的生活环境，因此它们被迫迁移到温暖的
大熊猫相关的衍生品也越来越受到消费者的青睐，比如玩具、文
南方地区。随着冰川进退的变化，大熊猫也在不断地寻找适合自
具、食品等等。可以说，大熊猫已经成为中外人民友好交往和文
己的生存环境，它们的足迹遍布全国各地。
化交流的桥梁之一。
随着第四纪冰川期的结束，气温开始回升，北方森林面积不 总而言之，保护好大熊猫这一珍贵物种不仅关乎到我国的生
断扩大，这就为大熊猫提供了更加广阔的生存空间。不过与此同 态文明建设，同时也具有广泛的社会意义和价值。希望今后能够
时，人类的活动也开始影响到大熊猫的生存环境。大规模的采伐 有更多的力量加入到这项伟大的事业中来，共同努力，为维护人
森林导致栖息地面积大幅缩小，水土流失加剧，这些都直接或间 类共同的家园贡献一份力量！
接地影响到野生大熊猫的数量。
Figure6.InterleavedcompositiongeneratedbyInternLM-XComposer2.
15根据以下标题：“芭蕾舞：优雅与力量的舞蹈艺术”，创作长文章，全面探讨芭蕾舞这一优雅而富有力量
的舞蹈艺术形式，字数不少于800字。主要内容如下：1、介绍芭蕾舞的历史起源和在世界舞蹈艺术中的地
位。2、探讨芭蕾舞从文艺复兴时期到现代的发展过程，分析不同时期和国家的芭蕾舞风格及其影响。3、
描述芭蕾舞的技术和动作语言，以及其在表现力和情感传达上的特点。4、介绍一些经典的芭蕾舞剧目和著
名舞者，分析它们对芭蕾舞艺术的贡献。5、最后总结芭蕾舞作为艺术形式的美学和文化价值，并对其未来
的发展趋势进行展望。
芭蕾舞：优雅与力量的舞蹈艺术 总之，芭蕾舞是一门历史悠久、技术精湛、内涵丰富的舞蹈
艺术形式。它不仅展示了人类的身体之美，更蕴含着深刻的文化
舞蹈，是一种优雅的艺术。芭蕾舞，更是其中最优雅的一种 内涵和哲学思考。
艺术形式。
技术与情感
起源与发展 芭蕾舞的技术非常重要，因为只有掌握了正确的技巧才能让
在远古时期，中国的南方地区就生活着原始大熊猫（也叫始 舞蹈看起来更加优美、流畅。在芭蕾舞中，演员需要用脚尖来支
熊猫）。大约在距今800万年前，全球进入冰期时代，地球开始 撑身体的重量，这需要极高的平衡能力和柔韧性。此外，芭蕾舞
变冷，植被逐渐向低纬度方向移动，这使得原本生活在高寒环境 还涉及到跳跃、旋转等各种高难度动作，都需要强大的力量和耐
中的大熊猫失去了适宜的生活环境，因此它们被迫迁移到温暖的 力来支撑。因此，要想成为一名优秀的芭蕾舞演员，必须经过长
南方地区。随着冰川进退的变化，大熊猫也在不断地寻找适合自 时间的训练和磨砺。
己的生存环境，它们的足迹遍布全国各地。 虽然芭蕾舞的技术十分重要，但芭蕾舞的灵魂却是情感。通
过舞蹈动作的传递，芭蕾舞能够表达出各种复杂的情感和思想。
比如，在古典芭蕾舞剧中，常常会出现悲欢离合的情节，而这些
情节往往是通过细腻的舞蹈动作来呈现的。在这些作品中，演员
需要通过表情、手势等多种方式来传达角色的内心世界，从而使
观众更好地理解剧情和角色性格。除此之外，现代芭蕾舞也越来
越注重对社会现实问题的关注和探讨，用舞蹈来呼吁人们对某些
问题的关注和反思。
芭蕾舞的技巧要求非常高，动作需要非常精准、流畅，而且
力度要掌握得恰到好处。此外，芭蕾舞的动作语言也非常丰富，
能够表达出很多复杂的情感和意义。正因为如此，芭蕾舞被认为
是最具表现力的舞蹈之一。
芭蕾舞的历史可以追溯到十五世纪，当时人们在庆祝胜利时
会跳舞，这种舞蹈被称为“卡斯蒂利亚舞”（Castilian
dance）。随着时代的发展，人们开始将音乐与舞蹈结合起来， 总之，芭蕾舞既注重技 Tip术 s for 又 Visitin强 g Shan调 ghai情感，二者缺一不可。只有
于是便诞生了最早的芭蕾舞剧《帕凡》（Pavane）。从那时起， 将技术和情感完美地融合在一起，才能创造出真正优秀的芭蕾舞
芭蕾舞就开始了它的辉煌历史。 作品。
经典剧目与著名舞者
说到芭蕾舞的经典剧目，最著名的当属《天鹅湖》、《胡桃
夹子》、《吉赛尔》等。这些剧目不仅具有极高的艺术价值，而
且深受观众喜爱。除了这些传统剧目外，现代芭蕾舞也有很多优
秀之作，比如美国的《雷米约的梦》、英国的《睡美人》等等。
当然，除了这些优秀的作品外，还有许多著名的芭蕾舞演员
随着时间的推移，芭蕾舞逐渐演变成了一种高雅的宫廷娱乐 也给人们留下了深刻的印象。像美国的玛丽安娜·阿米莉亚
活动。在法国国王路易十四的推崇下，芭蕾舞成为了欧洲王室贵 （Mariana Amalia）、英国的玛莎·格雷厄姆（Martha
族们最为喜爱的表演艺术之一。而到了十九世纪，芭蕾舞又迎来 Graham）、俄罗斯的娜塔莎·梅尔科娃（Natalia Mekhonova）
了一个新的高峰期。这个时期出现了许多著名的芭蕾舞大师，如 等等。她们不仅是芭蕾舞界的翘楚，更是影响了整个舞蹈界的人
马里·德拉佩诺普（Marie Taglioni）、费德利克·塔图 物。她们的舞蹈风格各异，有的热情奔放，有的柔美婉约，但无
（Friedrich Tarantin）等。他们创作了许多经典的作品，推动 一例外都给观众留下了深刻的印象。
了芭蕾舞艺术的发展。 结语
除了这些重要的历史人物外，还有许多杰出的芭蕾舞团为芭 芭蕾舞作为一门艺术形式，它不仅仅代表着优雅和美丽，更
蕾舞艺术的繁荣做出了巨大的贡献。例如，英国皇家芭蕾舞团、 蕴含着深刻的美学和文化价值。它不仅让我们欣赏到人类身体的
巴黎歌剧院芭蕾舞团、美国芭蕾舞剧院等都是享誉世界的芭蕾舞 极致之美，也让我们领略到人类智慧的无限可能。相信在未来，
团体。 芭蕾舞将继续发扬光大，为人类文明做出更大的贡献。
Figure7.InterleavedcompositiongeneratedbyInternLM-XComposer2.
16最近寒冬之际，在互联网上突然刮起了去
哈尔滨旅游的热潮。无数南方游客争先恐后的
前往中国的极北方-哈尔滨游玩。哈尔滨本地的
居民们也踊跃参与招待游客的活动当中，热情
地招待这些来自南方的“小土豆”们。请你以
第一人称，用文言文的形式记录一个南方人在
冬季前往哈尔滨游玩的游记。该文言文游记可
能包含：入住酒店，参观索菲亚大教堂，吃当
地的特色美食（红肠，烤冷面），游玩冰雪大
世界（冰雪打造的游乐场）等内容。文言文形 以小红书博主的口吻，写一篇文章，每一句
式可以参考《醉翁亭记》 话含有适当的emoji表情。抽取3-6个seo关键词，
生成#标签并放在文章最后。 要求以“🧸哈尔滨
旅游|南方小土豆历险记🧣”为标题写一篇小红
书分享，介绍包括入住酒店，吃美食，逛景点等
内容。需要包括以下内容：冰雪大世界，圣索菲
亚大教堂，中央大街以及烤冷面，香肠。
🧸哈尔滨旅游|南方小土豆历险记🧣
余闻哈尔滨之冬，极寒而美丽。故于岁末之
❣【第一站】冰雪大世界
际，携友前往一探究竟。
💰人均：¥200-300，推荐指数 🌟🌟🌟🌟
初至，见市廛之盛，闾阎之人声喧哗。既入
(cid:10991)票价偏贵，但体验感很棒～一定要去打卡哦！
夜，灯火辉煌，如星斗之在天。遂寻得一舍宿焉。
🍸吃：冰雪大世界附近有很多美食小吃，比如烤冷
窗外望之，索菲亚大教堂之倩影依稀可见，美不胜
面、香肠等。建议先买好票，然后慢慢品尝美食哦~
收。
🚶 ♀逛：中央大街是一条非常有特色的商业街，两侧的
次日晨，天微明，心驰索菲亚。及门，巍峨壮
建筑非常有特色，逛逛买买拍拍拍都很不错~
观，叹为观止。入其中，庄严肃穆之气扑面而来，
❣【第二站】圣索菲亚大教堂
敬而远之。
💰免费参观，推荐指数 🌟🌟🌟🌟
美食者也，若红肠、烤冷面等诸物皆属此列。
(cid:10991)这座教堂是哈尔滨的标志性建筑之一，非常值得一
余尝啖之，其味甚美，色鲜且香，真不食则馋矣！ *五、特⾊美⻝**
游哦~
午游冰雪大世界。此乃以冰雪为主题的游乐 🍸吃敦煌的 ：特⾊美 在⻝也⾮ 圣常丰 的索富 喜， 爱其 。菲中 此最 外有 ，亚名 还的 有当 ⽺大属 ⾁驴 泡⾁ 教馍⻩ 、⾯ 胡了 堂⽺。 焖驴 饼⾁ 附、⻩ 烤⾯ 全近是 ⽺⼀ 等道 特有以 ⾊驴 美⾁ ⻝很为 ，主 都要 是多⻝ 不材 容的 小错⾯ 过⻝ 的， 吃美味 味道 佳鲜 店肴⾹ 。可 和⼝，深 餐受当地 厅⼈和游 ，客
场，其景致由冰雕制作而成。建筑、人物、动物无
可以尝试一下当地的特色美食哦~
所不包，从飞龙到凤凰，从狮子到亭台楼阁，无一
🚶 ♀逛：这里有很多小店和小巷子，可以发现很多有趣
不精美绝伦，形态各异。漫步其中，如入童话世
的小玩意儿哦~
界，让人流连忘返。
❣【第三站】中央大街
夜，独步于中央大街之上。两边商铺林立，霓
💰门票：免费，推荐指数 🌟🌟🌟🌟
虹闪烁，繁华喧嚣之声不绝于耳。街头艺人弹唱、
(cid:10991)这条街道非常有特色，两旁都是欧式建筑，非常适
杂耍者比肩接踵，皆聚精会神地观摩欣赏。其乐融
合拍照哦~
融，热闹非凡。不禁赞叹：哈尔滨真乃一座充满活
🍸吃：这里有各种美食，比如烤冷面、炸鸡排、铁板
力与魅力的城市！
鱿鱼等等，还可以尝试一些当地的小吃哦~
至，天已昏黑。余倚窗远眺，星如钻石镶嵌于
🚶 ♀逛：中央大街是一个购物天堂，有很多商场和店
黑布之上。浮想联翩：或漫步街头，感北国风光之
铺，可以找到很多喜欢的东西哦~
壮丽；或游中央大街，观欧式建筑之独特；又或登
#哈尔滨旅游 #冰雪大世界 #圣索菲亚大教堂 #中央大
亚布力滑雪场，尝雪地滑道之刺激。然最令余难以
街 #哈尔滨必玩景点 #哈尔滨旅行攻略 #南方小土豆
忘怀者，乃哈尔滨人民热情好客之情谊也。此行虽
历险记
已结束，而此段美好回忆，必镌刻于心间矣！
Figure8.InterleavedcompositiongeneratedbyInternLM-XComposer2.
17Please write an blog based on the title: how to learn swim.
The first part should cover the swimming equipments.
The second part should discuss warm-up exercises before swimming.
The last part tells us how to swim, including different swimming techniques.
In summer, people will be attracted by the clear water The correct swimming posture is: keeping your head
swimming pool, which makes them want to dive into it. slightly abovethe watersurface,keepingyour chindown,
However, many people don't know how to swim or are extending your arms forward and placing them behind
afraid of the water. In fact, there is no need to be afraid. yourears,rotatingyourshouldersbackwards,keepingyour
Today I would like to share some tips on how to learn chest high, extending your legs straightly, bending your
swimming. kneesslightly,keepingyourfeetpointed,andputtingyour
toestogether.
Onethingthatmostnoviceswimmersmaynotbeawareof
At the beginning of swimming, we should adopt floating
isthe selection of swimming equipment. When they go to
andkickingmethod,thatis,lyingflatonthewatersurface
the swimming pool, they may see other people swimming
with both hands extended forward, alternating between
in different costumes and wearing different accessories.
upper and lower kicks, allowing water to enter the mouth
Which one should you choose? The answer depends on
and nose, taking deep breaths when breathing. With
yourswimmingabilitylevel.
repeated practice, the frequency of alternating between
upper and lower kicks will become faster and more
Forthosewhocanswimwellbutdonotweargoggles,they
accurate.
will be able to quickly find objects underwater; for
beginners,thechoiceofgogglesisveryimportant,because Swimming techniques inTicps flour Visditineg Shangfhariont crawl, back crawl,
they helpusavoidhittingthingswhenswimming,prevent breaststroke, side stroke, butterfly kick, and frog kick.
waterfromenteringoureyes,reduceeyefatiguecausedby Eachtechniquehasitsownadvantagesanddisadvantages,
waterresistance,andprotecttheeyes.Gogglessuitablefor and mastering each technique requires different
beginners have good sealing performance and are movements. The swimming skills that we usually
relativelycomfortabletowear. encounterinlifeincludefrontcrawlandbackcrawl,which
arecommonlyknownas"freestyle".Wejustneedtolearn
thesetwoskillsinordertoenjoyswimming.
Frontcrawl:
1.Headposition
Whenwegoswimming,it'simportanttotakeabathfirstto
cleanourselves,sothatwecangetusedtothetemperature
of the water. Before swimming, we also need to warm up
to improve our body'sflexibility, reduce the risk of injury
during swimming exercise, and ensure better performance
Head movement determines the whole stroke rhythm and
after swimming. This process is often ignored by many
direction of freestyle swimming. Freestyle swimming
novice swimmers, but if you ignore this step, you will
consistsoftwostrokesperarm.Theheadandneckshould
easilyhurtyourselfwhileswimming.
always be in line with the center of gravity, turning
sideways to breathe evenly, and turning to the opposite
side at the end of each stroke. The head should move
upwardsnaturallywhenturning.
Figure9.InterleavedcompositiongeneratedbyInternLM-XComposer2.
182.Armmovement Generate a travel diary focusing on a trip
to Yellowstone National Park and
recommend Yellowstone National Park in
English.
Arm movement is divided into two stages, namely the
push-forward stage and the pull-back stage. The push-
forward stage refers to the time when the forearm enters Yellowstone National Park is a must-see destination for
thewater,movesdownward,andextendsforwarduntilthe any traveler. The park's natural beauty and geothermal
fingertips pass the hips. It is important to extend the wonders are truly awe-inspiring. From the stunning Old
forearm fully during this stage. At the same time, the Faithful geyser to the breathtaking Grand Canyon of
elbowsshouldbelowered,andthehandshouldstretchout Yellowstone,thereisnoshortageofamazingsightstosee
asfaraspossible. inthisnationaltreasure.
3.Legmovement
One of the most popular attractions in Yellowstone is the
Old Faithful geyser. This iconic geyser erupts every 90
minutes, shooting water up to 180 feet into the air. It's a
Leg movement mainly includes the kick and the foot
truly spectacular sight to behold, and visitors can watch
movement. First, flex the knee and lift the leg to 45
fromboardwalksthatcirclethegeyserbasin.
degrees, then stretch the legs slowly, and finally use the
heeltostartthekick,andmakesurethatthewholebodyis
completelyhorizontalbeforefinishingthekick.Inaddition,
the calf muscle is also very important for propulsion, so
keep flexing your calf muscles throughout the swimming
process.
Tips for Visiting Shanghai
4.Breathingmethod Another highlight of Yellowstone is the Grand Canyon of
Yellowstone. This massive canyon is over 20 miles long
and up to 2,000 feet deep. The canyon is home to several
waterfalls, including the impressive Lower Falls, which
drops308feetintothecanyonbelow.
When swimming, turn your head sideways to breathe in
naturalrhythm,andlookatthesideabout60cmawayfrom
yourface.Youcanalsoexhalewhileswimminginshallow
watertocontroltheairflowrateofthemouthandnose.
In addition to these two main attractions, Yellowstone is
Backcrawl: also home to numerous other geothermal features,
Themaindifferencebetweenbackcrawlandfrontcrawlis including hot springs, mud pots, and fumaroles. These
thatthelegsarekickedtogetherinsteadofalternately.The uniquelandscapesareunlikeanythingyou'llfindanywhere
back crawl is basically similar to the front crawl, except elseintheworld.
that the arms move backward instead of forward. Overall, Yellowstone National Park is a must-visit
Therefore, only one person uses his hands to carry the destination for anyone looking to experience the natural
whole weight of the body, and the other person is beauty and geothermal wonders of the American West.
supportedbythepalmsandthebackofthehandstomove Whether you're interested in hiking, wildlife watching, or
forward. As for the breathing method, you can follow the simply taking in the stunning scenery, Yellowstone has
exampleofotherstobreathenaturally. somethingforeveryone.
Figure10.InterleavedcompositiongeneratedbyInternLM-XComposer2.
19请告诉我图中代码的功能是什么？
这段代码是 Python 语言的一个函数，名为 bubble_sort。
它使用了冒泡排序算法来对给定的列表进行排序。该算法通过不断交换相邻的元素，将较大的数逐渐“浮”到列
表的末端，最终达到排序的目的。
下面是函数 bubble_sort 的代码：
def bubble_sort(arr):
n = len(arr)
#遍历所有可能的元素对
for i in range(n):
#最后 n - i - 1 个元素已排好序，不需要再比较
for j in range(0, n-i-1):
if arr[j] > arr[j+1] :
arr[j], arr[j+1] = arr[j+1], arr[j]
return arr **⼆、鸣沙⼭⽉⽛泉**
鸣沙⼭⽉⽛泉是⼀处⾃然奇观，位于敦煌市⻄北约40公⾥处的沙漠中。这⾥地势平坦，沙丘连绵起伏，形成了⼀⽚⼴袤⽆垠的沙漠景观。⽽⽉⽛泉则静静地镶嵌在这⽚沙漠之中，泉⽔清澈⻅底，形状如新⽉，故称“⽉⽛泉”。每到夜晚，⽉亮升起时，⽉⽛泉周围会发出阵阵清脆的响声，犹如天籁之⾳，
# 示例 令⼈⼼旷神怡。
my_list = [64, 34, 25, 12, 22, 11, 90]
bubble_sort(my_list)
#打印排序后的数字列表
print(", ".join(map(str, my_list)))
在这个示例中，my_list 是一个需要排序的数字列表。
函数 bubble_sort 接收一个列表作为参数，返回排序后的列表。
在 main 函数中，my_list 的数字列表被传递给冒泡排序函数并排序后，打印排序后的列表。
很好，现在能帮我将上面代码改成快速排序吗？
当然可以，以下是使用快速排序算法的 Python 代码示例：
def quick_sort(arr):
if len(arr) <= 1:
return arr
pivot = arr[len(arr)//2]
left = [x for x in arr if x < pivot]
middle = [x for x in arr if x == pivot]
right = [x for x in arr if x > pivot]
return quick_sort(left) + middle + quick_sort(right)
# 示例
my_list = [64, 34, 25, 12, 22, 11, 90]
sorted_list = quick_sort(my_list)
#打印排序后的数字列表
print(", ".join(map(str, sorted_list)))
Figure11.ChatwithInternLM-XComposer2.
20这让我想起了柳宗元的《江雪》：众鸟
飞绝，万籁俱**寂⼆、鸣；沙⼭唯⽉⽛泉独**有老翁独钓寒江
泉鸣 则沙 静⼭ 静⽉ 地⽛ 镶泉 嵌是 在⼀ 这处 ⽚⾃ 沙然 漠奇 之观 中雪， ，位 泉于。⽔敦 清煌 澈在市 ⻅⻄ 底北这，约 形40 状幅公 如⾥ 新处画⽉的 ，沙中故漠 称中 “也。 ⽉这 ⽛⾥ 泉有地 ”。势 每一平 到坦 夜，人晚沙 ，丘 ⽉在连 亮绵 升起独起伏 时， ，钓形 ⽉成 ⽛了于泉⼀ 周⽚ 围寒⼴ 会袤 发江⽆ 出垠 阵的 阵沙 清漠 脆景 的观 响。 声⽽ ，⽉ 犹⽛
如
天籁之⾳，令⼈⼼旷神怡。
雪，画面充满了孤寂和宁静的感觉。
请描述这张图片。
我看到了这样*一*⼆、幅鸣沙⼭美⽉⽛丽泉**的图像。画面中，
鸣 观沙 。⼭ ⽽⽉ ⽉⽛ ⽛泉 泉是 则⼀ 静处 静⾃ 地一然 镶奇 嵌只观 在， 这位 ⽚人于 沙敦 漠的煌 之市 中⻄ ，手北 泉约 ⽔掌4 清0公 澈⾥ ⻅伸处 底的 ，了沙 形漠 状中出如。 新这 ⽉来⾥ ，地 故，势 称平 “⽉坦手⽛， 泉沙 ”心。丘 每连 到绵里夜起 晚伏有，， ⽉形 亮成一升了 起⼀条时⽚ ，⼴ ⽉袤 ⽛⽆ 泉垠 周的 围沙 会漠 发景
出阵阵清脆的响声，犹如天籁之⾳，令⼈⼼旷神怡。
蜿蜒的公路。这条道路在绿草如茵的山
谷中延伸，一路通往远方。云雾环绕着 描述这张图片。
群山，创造出一个梦幻般的环境。
这是一张生态系**⼆统、鸣沙循⼭⽉环⽛泉*图* ，它详细描
这张图真实存在吗？ 鸣 观沙 。⼭ ⽽⽉ ⽉⽛ ⽛泉 泉是 则⼀ 静绘处 静⾃ 地然了镶奇 嵌观 在不， 这位 ⽚于 沙同敦 漠煌 之生市 中⻄ ，北 泉物约 ⽔4 清0之公 澈⾥ ⻅处 底间的 ，沙 形漠 状的中 如。 新相这 ⽉⾥ ，地 故互势 称平 “⽉作坦 ⽛， 泉沙用”。丘 每连 到绵。夜起 晚伏生，， ⽉形 亮成态升了 起⼀ 时⽚ ，⼴ ⽉袤 ⽛⽆ 泉垠 周的 围沙 会漠 发景
出阵阵清脆的响声，犹如天籁之⾳，令⼈⼼旷神怡。
环境中，兔子通过食用植物茁壮成长。
作为兔子的天敌，狐狸和鹰则通过食
用兔子来作为食品链的一部分。有毒
我分析这张图像，发现其中的手掌和道 蛇和鹰这两种大型捕食者都捕食老鼠，
路是人工添加的合成内容，而山谷和群 并且它们自己也在自然界中寻求生存。
山应该是现实生活中存在的环境。整个 鹰捕食蛇，毒蛇捕食蛙。植物和微生
场景是合成的，并不真实存在。 物在土壤中分解粪便，这些分解物循
环回土壤中，供草食动物消耗。在这
个循环图中，植物是生产者，兔子、
毒蛇、老鼠和狐狸是消费者，而鹰是
食物链中最直接的消费者。生态系统
保持生态平衡并维持生命体的持续存
在是至关重要的。
**⼆、鸣沙⼭⽉⽛泉**
鸣沙⼭⽉这⽛泉道是⼀处题⾃然的奇观，数位于学敦煌市答⻄北案约40是公⾥处多的沙少漠中。这⾥地势平坦，沙丘连绵起伏，形成了⼀⽚⼴袤⽆垠的沙漠景观。⽽
⽉⽛泉则静静地镶嵌在这⽚沙漠之中，泉⽔清澈⻅底，形状如新⽉，故称“⽉⽛泉”。每到夜晚，⽉亮升起时，⽉⽛泉周围会发出阵阵清脆的
响声，犹如天籁之⾳，令⼈⼼旷神怡。
这道数学题的答案是A、-23
根据上面的人物关系图回答：祁同伟是
谁的学生？ **⼆、鸣沙⼭⽉⽛泉**
鸣沙⼭⽉⽛泉是⼀处⾃然奇观，位于敦煌市⻄北约40公⾥处的沙漠中。这⾥地势平坦，沙丘连绵起伏，形成了⼀⽚⼴袤⽆垠的沙漠景观。⽽
⽉⽛泉则静静地镶嵌在这⽚沙漠之中，泉⽔清澈⻅底，形状如新⽉，故称“⽉⽛泉”。每到夜晚，⽉亮升起时，⽉⽛泉周围会发出阵阵清脆的
响声，犹如天籁之⾳，令⼈⼼旷神怡。
祁同伟是高育良的学生。
**⼆、鸣沙⼭⽉⽛泉**
鸣沙⼭⽉⽛泉是⼀处⾃然奇观，位于敦煌市⻄北约40公⾥处的沙漠中。这⾥地势平坦，沙丘连绵起伏，形成了⼀⽚⼴袤⽆垠的沙漠景观。⽽
⽉⽛泉则静静地镶嵌在这⽚沙漠之中，泉⽔清澈⻅底，形状如新⽉，故称“⽉⽛泉”。每到夜晚，⽉亮升起时，⽉⽛泉周围会发出阵阵清脆的
响声，犹如天籁之⾳，令⼈⼼旷神怡。 侯亮平是谁的学生？
侯亮平是高育良的学生。
这张图让你想到了哪首古诗？
Figure12.ChatwithInternLM-XComposer2.
21The first column of the image contains
two diamond-shaped objects. The first
diamond shape is completely filled in
with a solid color, while the second one
alsohasanadditionaldotwithinit.
Analyzing this pattern, it seems that the
toprowandbottomrowcontainthesame
typeoffigurebutwithdifferentfeatures-
thetoponeiscompletelyfilled(possibly
Givenacirclewithadiameterof10cm withadifferentcolorortexture),andthe
andtwosmallercirclesofequalradius,
bottomoneincl*五u、d特e⾊s美⻝a**dotinsideit.
敦煌的特⾊美⻝也⾮常丰富，其中最有名的当属驴⾁⻩⾯了。驴⾁⻩⾯是⼀道以驴⾁为主要⻝材的⾯⻝，味道鲜⾹可⼝，深受当地⼈和游客
findtheareaoftheshadedregioninthe 的喜爱。此外，还有⽺⾁泡馍、胡⽺焖饼、烤全⽺等特⾊美⻝，都是不容错过的美味佳肴。
picture. From this information, we can infer that
the missing image in the second column
shouldbesimilartotheleftmostimagein
the second row—a star-shaped object
First, we need to find the radius of the with a solid fill. Since the other three
largercircle.Sincethediameteris10cm, imagesarealreadypresentandthepattern
theradiusishalfofthediameter,whichis: suggests repetition of figures with
radius=diameter/2=10cm/2=5cm varying properties, the image in the
secondcolumnwilllikelyconsistofastar
Next,weneedt**o⼆、f鸣i沙n⼭d⽉⽛th泉*e*areasofthetwo withasolidfill.
鸣沙⼭⽉⽛泉是⼀处⾃然s奇m观，a位l于l敦e煌r市c⻄i北r约c4l0e公s⾥.处的T沙h漠e中。r这a⾥d地i势u平s坦，o沙f丘e连a绵c起h伏，s形m成了a⼀l⽚le⼴袤r⽆垠的沙漠景观。⽽
⽉⽛泉则静静地镶嵌在这⽚沙漠之中，泉⽔清澈⻅底，形状如新⽉，故称“⽉⽛泉”。每到夜晚，⽉亮升起时，⽉⽛泉周围会发出阵阵清脆的
circle is ha响l声f，犹o如f天籁t之h⾳e，令r⼈a⼼d旷i神u怡s。of the large
circle,whichis:
smallerradius=radius/2=5cm/2=
2.5cm
Sotheareaofeachsmallercircleis:
area of smaller circle = π * (smaller
radius)²
area of smaller circle = π * (2.5 cm)² ≈
19.63cm²
The sumoftheareasofthe twosmaller
circlesis:
2*areaofsmallercircle≈39.26cm²
Please read the text in this image and return the
Finally, we can find the area of the information in the following JSON format (note
shaded region by subtracting the sum of xxx is placeholder, if the information is not
theareasofthetwosmallercirclesfrom available in the image, put "N/A" instead).
theareaofthelargecircle: {"class":xxx,"DLN":xxx,"DOB":xxx,"Name":
shadedregionarea=areaoflargecircle- xxx, "Address": xxx, "EXP": xxx, "ISS": xxx,
sumofsmallercircleareas "SEX":xxx,"HGT":xxx,"WGT":xxx,"EYES":
shadedregionarea=π*(5cm)²-39.26 xxx,"HAIR":xxx,"DONOR":xxx}
*五、特⾊美⻝**
cm²≈78.54cm²-39.26cm²
敦煌的特⾊美⻝也⾮常丰富，其中最有名的当属驴⾁⻩⾯了。驴⾁⻩⾯是⼀道以驴⾁为主要⻝材的⾯⻝，味道鲜⾹可⼝，深受当地⼈和游客
shadedregionarea≈39.28cm² 的喜爱。此外，还有⽺⾁泡馍、胡⽺焖饼、烤全⽺等特⾊美⻝，都是不容错过的美味佳肴。
{"class":"D",
"DLN":"D08954796",
"DOB":"01/01/1974",
"Name":"Jelani",
"Address": "123 Main St, Phoenix, AZ
85007",
"EXP":"03/01/2024",
"ISS":"03/01/2016",
"SEX":"M",
Herearethreesubim**a⼆、g鸣e沙s⼭a⽉⽛rr泉a**ngedina2-by-2 "HGT":"5’09"",
鸣 ⽽沙 ⽉⼭ ⽛⽉ 泉⽛ 则泉m 静是 静a⼀ 地t处 镶r⾃ 嵌i然 在x奇 这.观 ⽚， 沙F位 漠i于 之r敦s 中t煌 ，,市 泉⻄ ⽔l北o 清约 澈o4 ⻅0k公 底⾥ ，a处 形t的 状沙 如t漠 新h中 ⽉e。 ，这 故t⾥ 称w地 “⽉势o ⽛平 泉坦 ”i 。，m 每沙 到丘a 夜连g 晚绵e ，起s ⽉伏 亮，i 升形n 起成 时了 ，t⼀h ⽉⽚e ⽛⼴ 泉袤 周⽆ 围垠 会的 发沙 出漠 阵景 阵观 清。 "WGT":"185lb",
first column, in脆的t响h声e，犹t如o天p籁之l⾳e，f令t⼈a⼼旷n神d怡。bottom left. "EYES":"brown",
Then, findoutthepatterninthefirstcolumn "HAIR":"brown",
based on the left two images. Next, use the "DONOR":"yes"}
foundpatternandtheimageinthetoprightto
inferthemissingfigure.Finally,describewhat
wouldthemissingimagelooklike?
Figure13.ChatwithInternLM-XComposer2.
22Please read the table in this image and return a
markdown-style reconstructed table in text
Describethisimageindetail.
Theimageisablackandwhitefloorplan | Country | Sales Volume | Revenue |
ofahouse,providingadetailedlayoutof Profit|
theroomsandtheirdimensions.Theplan |---------|--------------|---------|--------|
is oriented with the entrance on the | USA | 40.080 | $15.971.880 |
bottomleftand**t⼆h、e鸣沙t⼭e⽉r⽛ra泉c**eonthetopleft.
$3.086.421
**⼆、鸣沙⼭⽉⽛泉**
鸣沙⼭⽉⽛泉是⼀处⾃然奇观，位于敦煌市⻄北约40公⾥处的沙漠中。这⾥地势平坦，沙丘连绵起伏，形成了⼀⽚⼴袤⽆垠的沙漠景观。⽽ 鸣沙⼭⽉⽛泉是⼀处⾃然奇观，位于敦煌市⻄北约40公⾥处的沙漠中。这⾥地势平坦，沙丘连绵起伏，形成了⼀⽚⼴袤⽆垠的沙漠景
⽉⽛泉则静静地镶嵌在这S⽚t沙a漠r之t中in，泉g⽔清f澈r⻅o 响底m 声， ，形 犹状 如如th 天新 籁e⽉ 之， ⾳故e ，称n 令“⽉t ⼈⽛r ⼼a泉 旷”n。 神c每 怡e到 。夜,晚，th⽉亮e升re起时，i⽉s⽛泉a周围会发出阵阵清脆的 观。⽽⽉⽛泉则静静|地镶C嵌在h这in⽚沙a漠之 出中 阵，|阵泉 清3⽔ 脆清5的澈.响0⻅ 声底7，，0犹形 如状 天如 籁新 之⽉ ⾳， ，故 令称 ⼈|“⽉ ⼼$⽛ 旷泉 神1” 怡。5。每.8到夜6晚6，.⽉6亮7升0起时|，⽉⽛泉周围会发
livingroomtotheright,whichis14'6"x $3.032.162
20'5"insize.Adjacenttothelivingroom | Australia| 27.054 | $14.812.566 |
isthediningarea,whichis9'10"x13'0" $2.888.636
in size. The dining area is connected to | India | 23.046 | $10.608.174 |
the kitchen, which is 7'2" x 9'2" in size. $1.853.710
Nexttothekitchenisthesecondbedroom, | South Korea | 16.032 | $10.494.948 |
which is 10'10" x 14'7" in size. The $1.975.844
masterbedroomislocatedtotherightof
thesecondbedroomandis12'10"x17'9"
in size. The plan also includes a foyer,
which is 6'0" x 5'3" in size, and a
bathroom, which is 7'2" x 9'2" in size.
Theterrace,whichis21'6"x80"insize,
islocatedonthetopleftsideoftheplan.
Thefloorplanprovidesacomprehensive
view of the house's layout, allowing one
tounderstandthespatialarrangementand
dimensionsofeachroom.
Figure14.ChatwithInternLM-XComposer2.
23