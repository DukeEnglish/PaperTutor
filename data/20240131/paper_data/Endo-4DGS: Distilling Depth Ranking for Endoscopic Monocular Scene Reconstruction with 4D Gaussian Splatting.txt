Endo-4DGS: Distilling Depth Ranking for
Endoscopic Monocular Scene Reconstruction
with 4D Gaussian Splatting
Yiming Huang1 ⋆, Beilei Cui1 ⋆, Long Bai1 ⋆, Ziqi Guo1, Mengya Xu1, and
Hongliang Ren1,2,3 ⋆⋆
1 Department of Electronic Engineering, The Chinese University of Hong Kong
(CUHK), Hong Kong SAR, China
2 Shun Hing Institute of Advanced Engineering, CUHK, Hong Kong SAR, China
3 Shenzhen Research Institute, CUHK, Shenzhen, China
{yhuangdl, beileicui, b.long}@link.cuhk.edu.hk, hlren@ee.cuhk.edu.hk
Abstract. Intherealmofrobot-assistedminimallyinvasivesurgery,dy-
namic scene reconstruction can significantly enhance downstream tasks
and improve surgical outcomes. Neural Radiance Fields (NeRF)-based
methods have recently risen to prominence for their exceptional ability
toreconstructscenes.Nonetheless,thesemethodsarehamperedbyslow
inference, prolonged training, and substantial computational demands.
Additionally,somerelyonstereodepthestimation,whichisofteninfea-
sibleduetothehighcostsandlogisticalchallengesassociatedwithstereo
cameras.Moreover,themonocularreconstructionqualityfordeformable
scenes is currently inadequate. To overcome these obstacles, we present
Endo-4DGS, an innovative, real-time endoscopic dynamic reconstruc-
tion approach that utilizes 4D Gaussian Splatting (GS) and requires no
groundtruthdepthdata.Thismethodextends3DGSbyincorporatinga
temporalcomponentandleveragesalightweightMLPtocapturetempo-
ralGaussiandeformations.Thiseffectivelyfacilitatesthereconstruction
of dynamic surgical scenes with variable conditions. We also integrate
Depth-Anything to generate pseudo-depth maps from monocular views,
enhancing the depth-guided reconstruction process. Our approach has
been validated on two surgical datasets, where it has proven to render
in real-time, compute efficiently, and reconstruct with remarkable accu-
racy.TheseresultsunderlinethevastpotentialofEndo-4DGStoimprove
surgical assistance.
1 Introduction
Endoscopicprocedureshavebecomeacornerstoneinminimallyinvasivesurgery,
offering patients with reduced trauma and quicker recovery times [8,16,27]. In
this case, accurate and dynamic 3D reconstruction of the endoscopic scene is
⋆ Co-first authors.
⋆⋆ Corresponding author.
4202
naJ
92
]VC.sc[
1v61461.1042:viXra2 Y. Huang et al.
Reference Estimated depth Texture Renderedimage Predicted depth
Fig.1. 3D textures, rendered image, and predicted depth of our proposed method.
critical to enhancing the surgeon’s spatial understanding and navigation, facil-
itating more precise and efficient interventions [13]. However, the complex and
constrained nature of endoscopic scenes poses significant challenges for tradi-
tional 3D reconstruction techniques due to factors such as limited field-of-view,
occlusions, and dynamic tissue deformation [20,23,26].
Recent advancements in endoscopic 3D reconstruction have been boosted
by the capabilities of Deep Neural Networks (DNNs) [19] and Neural Radiance
Fields (NeRFs) [14]. Some studies have achieved strong performance in depth
estimation and reconstruction under endoscopy, particularly through stereo re-
construction[2,12],structurefrommotion[3],depthandposeestimation[15,18]
or extensive visual pre-training [6]. However, reconstructing high-dimensional
deformable scenes remains a challenge. EndoNeRF [20] marks a significant step
forward, being the first to leverage NeRF’s ability for implicit geometric mod-
eling in endoscopic scenes. It introduces a dual neural field approach to model
tissuedeformationandcanonicaldensity,achievingdynamicscenerenderingand
theremovalofinstrumentocclusionduringendoscopic-assistedsurgery.Building
on this,EndoSurf[26] further employssigned distance functionsto modeltissue
surfaces, imposing explicit self-consistency constraints on the neural field. To
tackle the real-time dynamic reconstruction challenge, LerPlane [23] constructs
a 4D volume by introducing 1D time to the existing 3D spatial space. This ex-
tension allows for the formulation of both static fields and dynamic fields by
utilizing the spatial and spatiotemporal planes, respectively, which leads to a
substantial decrease in computational resources.
NeRF-based methods are transformative in 3D scene reconstruction but
struggle with slow rendering and less-than-ideal localization accuracy [5]. To
circumvent these limitations, 3D Gaussian Splatting (GS) has emerged as a
promising technique, celebrated for its fast inference and high-quality 3D rep-
resentation learning [10]. 3D GS uses a collection of scene images to optimize
anisotropic 3D Gaussians, capturing their positions, orientations, appearances,
andalphablendingparameters.Thiseffectivelyreconstructsboththegeometric
structure and visual appearance of a scene. The tile-based rasterizer of 3D GS
ensures rapid rendering.
Drawinginspirationfrom[21],weintroducetheconceptoftimeasthe4th di-
mensiontomodeldynamicscenes,specificallytargetingthereconstructionchal-
lengesofdeformabletissuesinendoscopicsurgery.Traditionalmethodstypically
rely on multi-view reconstruction, which is often impractical in vivo, given theEndo-4DGS 3
prohibitivecostsandsizeconstraintsofstereocamerasinendoscope-assistedpro-
cedures.Furthermore,amassingacomprehensive,fullysuperviseddepthdataset
isnotfeasible[18].Depth-Anything[24],anoveltechniquewithextensivevisual
pre-training, has shown exceptional depth estimation performance in diverse
scenes. We harness Depth-Anything [24] to estimate depth maps from monocu-
larcameras,guidingandrefiningour3Dreconstruction.Figure1presentsthe3D
textures,renderedimages,andpredicteddepthinendoscopicviews.Specifically,
our contributions in this paper are threefold:
– WepresentEndo-4DGS,aninnovativetechniquethatadaptsGaussianSplat-
ting for endoscopic scene reconstruction. Utilizing Depth-Anything, Endo-
4DGSachievesremarkablereconstructionoutcomeswithoutneedingground
truth depth data.
– We use a lightweight MLP to predict the temporal dynamics of deformable
tissues, creating a 4D voxel model for dynamic scenes. Depth-Anything aids
in estimating depth from a single camera viewpoint, acting as pseudo-depth
supervision for convergence.
– OurextensivevalidationontworealsurgicaldatasetsshowsthatEndo-4DGS
attainshigh-qualityreconstruction,excelsinreal-timeperformance,reduces
training expenditures, and demands less GPU memory, which sets the stage
for advancements in robot-assisted surgery.
2 Methodology
In this section, we introduce the representation and rendering formula of 4D
Gaussians [21] in Sec. 2.1 and demonstrate our motivation and detailed imple-
mentation of the depth prior-based reconstruction in Sec. 2.2.
2.1 Preliminaries
3D GS [10] utilizes 3D differentiable Gaussians as the unstructured representa-
tion, allowing for a differentiable volumetric representation that can be rapidly
rasterizedandprojectedontoa2Dsurfaceforswiftrendering.Withacovariance
matrixΣ andacenterpointX,wecanrepresentthe3DGaussiansasfollowing:
G(X)=e−1 2XTΣ−1X, (1)
where the covariance Σ can be further decomposed into Σ = RSSTRT, which
includes a scaling S and rotation R. With the differential splatting rendering
method [25], 3D Gaussians [10] achieves real-time rendering for photo-realistic
results.
2.2 Proposed Methodology
Overview of the Endo-4DGS Pipline. The whole pipeline of Endo-4DGS
is illustrated in Figure 2. The Endo-4DGS method consists of four major parts,4 Y. Huang et al.
Mask 𝑀 Initialized 𝑥𝑡 𝑦𝑡 𝑧𝑡 4D Gaussians
point cloud 𝑃
MLP
Monocular
Input 𝐼 𝑥𝑦 𝑦𝑧 𝑥𝑧 Splatting
Reprojection
Depth
Ranking Loss
Pre-trained Estimated Rendered Rendered
model 𝑁 depth map 𝑍𝑒 depth map𝑍መ Image 𝐼መ
𝐿1+𝑡𝑣Loss
Fig.2. Illustration of our proposed Endo-4DGS framework.
i.e. the depth estimation module, the depth initialization module, the color re-
construction module, and the depth ranking distilling module. We employ 4D
Gaussians as the backbone for 4D scene reconstruction and utilize the Depth-
Anything [24] model pre-trained on 63.5M images to estimate the depth map
from monocular input. The training data of Depth-Anything [24] covers a wide
range of data, therefore it is robust on the relative distance for estimated depth
map which can provide the geometry guidance with our depth ranking distilling
module. With the implementation of the depth prior, we address the initializa-
tion and depth supervision problem under monocular scene reconstruction.
4DGaussianSplattingInspiredby[21],thestaticrepresentationof3DGS[10]
shallbeextendedto4DbyconstructinganewGaussianrepresentationwithmo-
tionsandshapedeformations.Giventhedifferentialsplattingrenderingformula
S,4DGaussiansrepresentationG′,andthecameraextrinsicmatrixK =[R,T],
2
the novel-view image Iˆcan be rendered as:
Iˆ=S(K ,G′), (2)
2
where the 4D Gaussians is formed as G′ = ∆G +G by a static 3D Gaussians
G and its deformation ∆G. With the deformation function F and time t, the
deformation can be described as ∆G =F(G,t).
Morespecifically,amulti-resolutionHexPlanevoxelmoduleR(I,j)∈Rh×lNi×lNj
isutilized,wherehisthehiddendimoffeatures,N istheresolutionofthevoxel
grids and l is the upsampling scale. With a tiny MLP ϕ , the voxel features
d
f ∈Rh∗l of time t is encoded as temporal and spatial features f :
h d
f =ϕ (f )
d d h
(cid:91)(cid:89)
f = interp(R(i,j)),
h (3)
l
{i,j}∈{(x,y),(x,z),(y,z),(x,t),(y,t),(z,t)}.Endo-4DGS 5
In addition, a multi-head Gaussian deformation decoder D ={ϕ ,ϕ ,ϕ } is
x r s
designedfordecodingthedeformationofposition,rotation,andscalingofthe3D
Gaussians with three tiny MLPs: ϕ ,ϕ ,ϕ , respectively. With the deformation
x r s
of position ∆X = ϕ (f ), rotation ∆r = ϕ (f ), and scaling ∆s = ϕ (f ), the
x d r d s d
final representation of 4D Gaussians can be presented as:
G′ ={X +∆X,r+∆r,s+∆s,σ,C}, (4)
where X,r,s are the original position, rotation, and scaling of the static 3D
Gaussian representation, while σ is the density and C is the color.
Gaussians Initialization with Depth Prior.Previouswork [10] hasdemon-
stratedtheimportanceofapplyingpointcloudfromShapefromMotion(SfM)[17]
as an initialization for the 3D Gaussians. However, retrieving accurate point
clouds in surgical scenes is challenging due to the hardware constraints and the
varying illumination conditions. Existing solutions includes generating sparse
3D points by COLMAP [17] or using Multi-View Stereo (MVS) algorithms [1,
9,11]. However, in real-world applications, the only visual information from the
consumer-levelendoscopesisthemonocularRGBimage.Withsuchalimitation,
weproposetousethepre-traineddepthtoimplementthepointcloudinitializa-
tion for the 4D Gaussians. With the pre-trained depth estimation model N and
the input image I of width W and height H, we estimate an inversed depth
0
map Z ∈ RH×W. Then we apply a scaling α to convert the estimated inverse
e
depth to the depth map Z in the camera coordinate as Z = α . Given the
c c Ze
camera intrinsic matrix K , and the extrinsic matrix K , we can reproject the
1 2
point cloud P ∈RHW×3 for initialization from the given image I as follows:
0 0
P =Z K−1K−1(I ⊙M ), (5)
0 c 2 1 0 0
whereM isthemaskfortheinputimage,and⊙istheelement-wisemultiplica-
0
tion. with the initialized point cloud from the pre-train depth map, the training
process of the 4D-GS can be more robust in terms of geometry.
Distilling Depth Ranking and Optimization. Single-image depth estima-
tion is a challenging task due to the ill-pose nature and the bias from various
datasets. To utilize the pre-train depth map more effectively as the pseudo-
ground truth, we propose to use a structure-guided ranking loss [22] L . With
rk
the depth distilling loss, we enable the constraint for the rendered depth with-
out knowing the shift and scale. With the L color loss and a grid-based total-
1
variational loss L [4,7], our final loss for optimizing can be represented as:
tv
L=L +L +λL , (6)
1 tv rk
where λ is the weight for the depth ranking loss.6 Y. Huang et al.
Table 1. Comparison experiments on the EndoNeRF dataset [20] against EndoN-
eRF [20], EndoSurf [26], and LerPlane [23].
EndoNeRF-Cutting EndoNeRF-Pulling Training GPU
Models FPS
PSNR SSIM LPIPS PSNR SSIM LPIPS Time Usage
EndoNeRF [20] 35.84 0.942 0.057 35.43 0.939 0.064 6 hours 0.2 4 GB
EndoSurf [26] 34.89 0.952 0.107 34.91 0.955 0.120 7 hours 0.04 17 GB
LerPlane [23] 34.66 0.923 0.071 31.77 0.910 0.071 8 mins 1.5 20 GB
Ours 36.84 0.954 0.040 37.08 0.955 0.050 4 mins 100 4GB
Table 2. Comparison experiments on the StereoMIS dataset [9] against EndoN-
eRF [20].
Training GPU
Models PSNR SSIM LPIPS FPS
Time Usage
EndoNeRF [20] 21.49 0.622 0.360 5 hours 0.2 4 GB
Ours 31.46 0.829 0.175 5 mins 100 4GB
t = 0.01 t = 0.33 t = 0.57 t = 0.93
F
R
eN
o
d
n
E
sru
O
ecn
erefeR
Fig.3. Qualitative comparison on the StereoMIS dataset [9] against EndoNeRF [20].
3 Experiments
3.1 Dataset
We evaluate the performance based on two publicly available datasets, Stere-
oMIS [9] and EndoNeRF [20]. The StereoMIS dataset is a stereo video dataset
captured by the da Vinci Xi surgical system, consisting of 11 surgical sequences
by in-vivo porcine subjects. The EndoNeRF dataset includes two samples of
prostatectomy via stereo cameras and provides estimated depth maps based on
stereo-matching techniques, they also include challenging scenes with tool oc-Endo-4DGS 7
EndoNeRF EndoSurf LerPlane Ours Reference
t = 0.01
g
n
illu t = 0.52
p
t = 0.90
t = 0.01
g
n
ittu t = 0.52
c
t = 0.94
Fig.4.QualitativecomparisonontheEndoNeRFdataset[20]againstEndoNeRF[20],
EndoSurf [26], and LerPlane [23].
clusion and non-rigid deformation. The training and validation splitting follows
the 7:1 strategy in [26]. We use PSNR, SSIM, and LPIPS to evaluate the 3D
scene reconstruction performance. We also report the results of training time,
inference speed, and GPU memory usage on one single RTX4090 GPU.
3.2 Implementation Details
All experiments are conducted on the RTX4090 GPU with the Python Py-
Torch framework. We adopt the Adam optimizer with an initial learning rate
of 1.6×10−3. We employ the Depth-Anything-Small model for pseudo-depth
map generation with depth scale α = 1000 and λ = 0.01 as the weight for the
depth ranking loss. We use an encoding voxel size of [64,64,64,75], where the
four dimensions are length, width, height, and time, respectively.
3.3 Results
Wecomparetheperformanceofourmethodagainstthestate-of-the-artmethods,
e.g., EndoNeRF [20], EndoSurf [26], and LerPlane [23]. The evaluation perfor-
mances on StereoMIS and EndoNeRF are shown in Table 2 and Table 1. We8 Y. Huang et al.
can observe that while maintaining relatively high performance for EndoNeRF
andEndoSurf,theybothrequirehoursoftrainingwhichistime-consuming.Ler-
Planegreatlyreducesthetrainingtimetoaround8minutesatthecostofslight
degradation in rendering performance. It is worth noting that all these state-of-
the-art methods suffer from very low FPS which limits their further application
in real-time surgical scene reconstruction tasks. Our proposed method not only
achieves the best performance in all evaluation metrics on two datasets but also
increasestheinferencespeedtoareal-timelevelof100FPSwithonly4minutes
of training and 4G of GPU usage. We also illustrate some qualitative results for
bothdatasetsinFigure3and 4.ItcanbeobservedthatforStereoMIS,EndoN-
eRF can not capture the details of tissues while our proposed method preserves
a large amount of visible details with good geometry features. Our proposed
method also rendered better visualizations of the EndoNeRF dataset compared
toothermethods.Theabovequantitativeandqualitativeresultscertificatethat
Endo-4DGS achieves high-quality 3d reconstruction scenes with real-time level
inference speed which reveals its strong potential in future real-time endoscopic
applications.
4 Conclusion
Inconclusion,ourstudymarksasubstantialprogressioninrobot-assistedsurgery
byofferinganinnovativeandeffectiveapproachtodynamicscenereconstruction.
Endo-4DGSharnesses4DGaussianSplattingandDepth-Anything,enablingthe
real-time, high-fidelity reconstruction of deformable tissues without relying on
groundtruthdepthinformation.OurstrategicapplicationofalightweightMLP
for predicting deformable tissue dynamics, combined with Depth-Anything for
monoculardepthestimation,allowsEndo-4DGStooutperformexistingmethods
in accuracy and computational efficiency. The decreased GPU memory require-
ment and independence from extensive depth data are key achievements that
promise wider application and further innovation in the domain. Furthermore,
our work presents practical benefits, as it can be easily integrated into clinical
practice to boost the surgeon’s spatial perception and decision-making during
surgeries.
Acknowledgements. This work was supported by Hong Kong RGC Collab-
orative Research Fund (C4026-21G), General Research Fund (GRF 14211420
& 14203323), Shenzhen-Hong Kong-Macau Technology Research Programme
(Type C) STIC Grant SGDX20210823103535014 (202108233000303).
References
1. Allan, M., Mcleod, J., Wang, C., Rosenthal, J.C., Hu, Z., Gard, N., Eisert, P.,
Fu, K.X., Zeffiro, T., Xia, W., et al.: Stereo correspondence and reconstruction of
endoscopic data challenge. arXiv preprint arXiv:2101.01133 (2021)Endo-4DGS 9
2. Bae, G., Budvytis, I., Yeung, C.K., Cipolla, R.: Deep multi-view stereo for dense
3d reconstruction from monocular endoscopic video. In: International Conference
on Medical Image Computing and Computer-Assisted Intervention. pp. 774–783.
Springer (2020)
3. Barbed, O.L., Montiel, J.M., Fua, P., Murillo, A.C.: Tracking adaptation to im-
prove superpoint for 3d reconstruction in endoscopy. In: International Conference
on Medical Image Computing and Computer-Assisted Intervention. pp. 583–593.
Springer (2023)
4. Cao,A.,Johnson,J.:Hexplane:Afastrepresentationfordynamicscenes.In:Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion. pp. 130–141 (2023)
5. Chen, G., Wang, W.: A survey on 3d gaussian splatting. arXiv preprint
arXiv:2401.03890 (2024)
6. Cui,B.,Islam,M.,Bai,L.,Ren,H.:Surgical-dino:Adapterlearningoffoundation
modelfordepthestimationinendoscopicsurgery.arXivpreprintarXiv:2401.06013
(2024)
7. Fang, J., Yi, T., Wang, X., Xie, L., Zhang, X., Liu, W., Nießner, M., Tian, Q.:
Fast dynamic radiance fields with time-aware neural voxels. In: SIGGRAPH Asia
2022 Conference Papers. pp. 1–9 (2022)
8. Gao,H.,Yang,X.,Xiao,X.,Zhu,X.,Zhang,T.,Hou,C.,Liu,H.,Meng,M.Q.H.,
Sun,L.,Zuo,X.,etal.:Transendoscopicflexibleparallelcontinuumroboticmech-
anism for bimanual endoscopic submucosal dissection. The International Journal
of Robotics Research p. 02783649231209338 (2023)
9. Hayoz,M.,Hahne,C.,Gallardo,M.,Candinas,D.,Kurmann,T.,Allan,M.,Sznit-
man, R.: Learning how to robustly estimate camera pose in endoscopic videos.
International Journal of Computer Assisted Radiology and Surgery pp. 1185—-
1192 (2023)
10. Kerbl, B., Kopanas, G., Leimku¨hler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics 42(4) (2023)
11. Li, Z., Liu, X., Drenkow, N., Ding, A., Creighton, F.X., Taylor, R.H., Unberath,
M.: Revisiting stereo depth estimation from a sequence-to-sequence perspective
with transformers. In: Proceedings of the IEEE/CVF international conference on
computer vision. pp. 6197–6206 (2021)
12. Long, Y., Li, Z., Yee, C.H., Ng, C.F., Taylor, R.H., Unberath, M., Dou, Q.: E-
dssr:efficientdynamicsurgicalscenereconstructionwithtransformer-basedstereo-
scopic depth perception. In: Medical Image Computing and Computer Assisted
Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France,
September 27–October 1, 2021, Proceedings, Part IV 24. pp. 415–425. Springer
(2021)
13. Mahmoud, N., Cirauqui, I., Hostettler, A., Doignon, C., Soler, L., Marescaux,
J.,Montiel,J.M.M.:Orbslam-basedendoscopetrackingand3dreconstruction.In:
Computer-AssistedandRoboticEndoscopy:ThirdInternationalWorkshop,CARE
2016,HeldinConjunctionwithMICCAI2016,Athens,Greece,October17,2016,
Revised Selected Papers 3. pp. 72–83. Springer (2017)
14. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.Commu-
nications of the ACM 65(1), 99–106 (2021)
15. Ozyoruk,K.B.,Gokceler,G.I.,Bobrow,T.L.,Coskun,G.,Incetan,K.,Almalioglu,
Y., Mahmood, F., Curto, E., Perdigoto, L., Oliveira, M., et al.: Endoslam dataset
and an unsupervised monocular visual odometry and depth estimation approach
for endoscopic videos. Medical image analysis 71, 102058 (2021)10 Y. Huang et al.
16. Psychogyios, D., Colleoni, E., Van Amsterdam, B., Li, C.Y., Huang, S.Y., Li, Y.,
Jia, F., Zou, B., Wang, G., Liu, Y., et al.: Sar-rarp50: Segmentation of surgical
instrumentation and action recognition on robot-assisted radical prostatectomy
challenge. arXiv preprint arXiv:2401.00496 (2023)
17. Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
(June 2016)
18. Shao, S., Pei, Z., Chen, W., Zhu, W., Wu, X., Sun, D., Zhang, B.: Self-supervised
monoculardepthandego-motionestimationinendoscopy:Appearanceflowtothe
rescue. Medical image analysis 77, 102338 (2022)
19. Stucker, C., Schindler, K.: Resdepth: Learned residual stereo reconstruction. In:
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
nition Workshops. pp. 184–185 (2020)
20. Wang, Y., Long, Y., Fan, S.H., Dou, Q.: Neural rendering for stereo 3d recon-
struction of deformable tissues in robotic surgery. In: International Conference
on Medical Image Computing and Computer-Assisted Intervention. pp. 431–441.
Springer (2022)
21. Wu,G.,Yi,T.,Fang,J.,Xie,L.,Zhang,X.,Wei,W.,Liu,W.,Tian,Q.,Xinggang,
W.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint
arXiv:2310.08528 (2023)
22. Xian,K.,Zhang,J.,Wang,O.,Mai,L.,Lin,Z.,Cao,Z.:Structure-guidedranking
loss for single image depth prediction. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 611–620 (2020)
23. Yang,C.,Wang,K.,Wang,Y.,Yang,X.,Shen,W.:Neurallerplanerepresentations
for fast 4d reconstruction of deformable tissues. arXiv preprint arXiv:2305.19906
(2023)
24. Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., Zhao, H.: Depth anything: Un-
leashing the power of large-scale unlabeled data. arXiv:2401.10891 (2024)
25. Yifan,W.,Serena,F.,Wu,S.,O¨ztireli,C.,Sorkine-Hornung,O.:Differentiablesur-
facesplattingforpoint-basedgeometryprocessing.ACMTransactionsonGraphics
(TOG) 38(6), 1–14 (2019)
26. Zha, R., Cheng, X., Li, H., Harandi, M., Ge, Z.: Endosurf: Neural surface re-
construction of deformable tissues with stereo endoscope videos. In: International
ConferenceonMedicalImageComputingandComputer-AssistedIntervention.pp.
13–23. Springer (2023)
27. Zia,A.,Bhattacharyya,K.,Liu,X.,Berniker,M.,Wang,Z.,Nespolo,R.,Kondo,S.,
Kasai,S.,Hirasawa,K.,Liu,B.,etal.:Surgicaltoolclassificationandlocalization:
results and methods from the miccai 2022 surgtoolloc challenge. arXiv preprint
arXiv:2305.07152 (2023)