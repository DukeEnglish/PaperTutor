Boolean Logic as an Error feedback mechanism
Louis Leconte
LISITE, Isep, Sorbonne University
Math. and Algorithmic Sciences Lab, Huawei Technologies, Paris, France
Abstract
The notion of Boolean logic backpropagation was introduced tobuild neural net-
works with weights and activations being Boolean numbers. Most of computations
can be done with Boolean logic instead of real arithmetic, both during training and
inference phases. But the underlying discrete optimization problem is NP-hard, and
the Boolean logic has no guarantee. In this work we propose the first convergence
analysis, under standard non-convex assumptions.
1 Introduction
Training machine learning models can often be a very challenging process, requiring sig-
nificant computationalresources andtime. The use of DNNs oncomputing hardware such
as mobile and IoT devices is becoming increasingly important. IoT devices often have
limitations in terms of memory and computational capacity. Quantization is a potential
solution to this problem (Courbariaux et al., 2015; Chmiel et al., 2021; Leconte et al.,
2023). And in particular, Binary Neural Networks (BNNs) is a remarkably promising
direction because it reduces both memory and inference latency simultaneously (Nguyen,
2023).
Formaly, BNN training can be formulated as minimising the training loss with binary
weights, i.e.,
minf(w); f(w) = E [ℓ(NN(x,w),y)], (1)
(w,y)∼pdata
w∈Q
where Q = {±1}d is a discrete codebook, d is the number of parameters (network weights
and biases), n is the total number of clients, ℓ is the training loss (e.g., cross-entropy or
square loss), NN(x,w) is the DNN prediction function, p is the training distribution.
data
The quantization constraints in the above program make it an extremely difficult task:
the underlying optimization problem is non-convex, non-differentiable, and combinatorial
in nature.
To the best of our knowledge, in the quantized neural network literature and in par-
ticular BNN, one can only prove the convergence up to an irreducible error floor Li et al.
(2017). This idea has been extended to SVRG De Sa et al. (2018), and recently to SGLD
in Zhang et al. (2022), which is also up to an error limit.
In this work we provide complexity bounds for Boolean Logic (Nguyen, 2023) in a
smooth non-convex environment. We first recap the simplified mechanism of a given
Boolean Logic (noted as B) in Algorithm 1. In the forward pass, at iteration t, input of
layer l, xl,t, is buffered for later use in the backward, and the jth neuron output at kth
1
4202
naJ
92
]LM.tats[
1v81461.1042:viXraAlgorithm 1: Pseudo-code for Boolean training with B = XNOR.
Input : Learning rate η, nb iterations T;
1 Initialize
2 ml,0 = 0; β0 = 1;
i,j
3 end
4 for t = 0,...,T −1 do
/* 1. Forward */
5 Receive and buffer xl,t;
6 Compute xl+1,t following Equation (2);
/* 2. Backward */
7 Receive gl+1,t;
/* 2.1 Backpropagation */
8 Compute and backpropagate gl,t following Equation (3);
/* 2.2 Weight update */
9 C := 0, C := 0;
tot kept
10 foreach wl do
i,j
11 Compute
ql,t+1
following Equation (4);
i,j
12 Update
ml,t+1
=
βtml,t +ηtql,t+1;
i,j i,j i,j
13 C ← C +1;
tot tot
14 if XNOR(ml,t+1,wl,t ) = True then
i,j i,j
15 wl,t+1 ← ¬wl,t ; /* FLIP */
i,j i,j
16
ml,t+1
← 0;
i,j
17 else
18 wl,t+1 ← wl,t ; /* NO FLIP */
i,j i,j
19 C ← C +1;
kept kept
20 end
21 end
22 Release buffer xl,t;
23 Update βt+1 ← C /C ;
kept tot
24 Update ηt+1;
25 end
sample is computed as:
m
xl+1,t = wl + B(xl ,wl ), (2)
k,j 0,j k,i i,j
i=1
X
∀k ∈ [1,K],∀j ∈ [1,n] where K, m, n are, respectively, the training mini-batch, layer
input and output size.
In the backward pass, layer l receives gl+1,t from downstream layer l + 1. Then,
backpropagated signal gl,t (line 8 in Algorithm 1), is computed following Nguyen (2023)
as:
n n
l,t l,t l,t
g = |g |− |g |, (3)
k,i {gl,t =True} k,i,j {gl,t =False} k,i,j k,i,j k,i,j
j=1 j=1
X X
l,t
∀k ∈ [1,K],∀i ∈ [1,m], where g is given according to Nguyen (2023) for the utilized
k,i,j
logic B. Optimization signal at line 11 in Algorithm 1 is given according to Nguyen (2023)
2
1 1as:
K K
ql,t+1
=
|ql,t
|−
|ql,t
|, (4)
i,j {ql,t =True} i,j,k {ql,t =False} i,j,k i,j,k i,j,k
k=1 k=1
X X
∀i ∈ [1,m],∀j ∈ [1,n]. Finally, the weights are updated in lines 14–20 of Algorithm 1
following the rule formulated in Nguyen (2023).
We now introduce an abstraction to model the optimization process and prove con-
vergence of the mechanism detailed in Algorithm 1.
2 Continuous Abstraction of Nguyen (2023)
Boolean optimizer is discrete, proving its convergence directly is a hard problem. The
idea is to find a continuous equivalence so that some proof techniques existing from the
BNN and quantized neural networks literature can be employed.
In existing frameworks, quantity ∇f(·) denotes the stochastic gradient computed on
a random mini-batch of data. Boolean Logic does not have the notion of gradient, it
l,t
however has an optimization signal e(q in Algorithm 1) that plays the same role as
i,j
∇f(·). Therefore, these two notions, i.e., continuous gradient and Boolean optimization
signal, can be encompassed into a generalized notion. That is the root to the following
ceontinuous relaxation in which ∇f(·) stands for the optimization signal computed on a
random mini-batch of data.
For reference, the original Boeolean optimizer as formulated in in the previous section
is summarized in Algorithm 2 in which flip(w ,m ) flips weight and reset(w ,m )
t t+1 t t+1
resets its accumulator when the flipping condition is triggered.
Algorithm 2: Boolean optimizer
1 m ← β m +ηq ;
t+1 t t t
2 w ← flip(w ,m );
t+1 t t+1
3 m ← reset(w ,m );
t+1 t t+1
Algorithm 3: Equivalent formulation of Boolean optimizer
Data: Q ,Q quantizer
0 1
1 m ← η∇f(w )+e ;
t t t
2 ∆ ← Q (m ,w );
t 1 t t
3 w ←eQ (w −∆ );
t+1 0 t t
4 e ← m −∆ ;
t+1 t t
Algorithm3describesanequivalentformulationofBooleanoptimizer. Therein, Q ,Q
0 1
are quantizers which are specified in the following. Note that EF-SIGNSGD (SIGNSGD
with Error-Feedback) algorithm from Karimireddy et al. (2019) is a particular case of this
formulation with Q () = Identity() and Q () = sign(). For Boolean Logic abstraction,
0 1
they are given by:
Q (m ,w ) = w (ReLu(w m −1)+ 1 sign(w m −1)+ 1),
1 t t t t t 2 t t 2 (5)
(Q 0(w t) = sign(w t).
3
1 1The combination of Q and Q is crucial to take into account the reset property of the
1 0
accumulator m . Indeed in practice, ∆ := Q (m ,w ) is always equal to 0 except when
t t 1 t t
|m | > 1 and sign(m ) = sign(w ) (i.e., when the flipping rule is applied). As w has only
t t t t
values in {±1}, Q acts as identity function, except when ∆ is non-zero (i.e., when the
0 t
flipping rule is applied). With the choices (5), we can identify flip(w ,m ) = Q (w −
t t 0 t
Q (m ,w )). We do not have closed-form formula for reset(w ,m ) from Algorithm 2,
1 t t t t+1
but the residual errors e play this role. Indeed, e = m except when ∆ is non-zero
t t+1 t t
(i.e., when the flipping rule is applied and e is equal to 0).
t+1
The main difficulty in the analysis comes from the parameters quantization Q ().
0
Indeed, we can follow the derivations in Appendix B.3 from Karimireddy et al. (2019) to
boundthe error term Eke k2, but we also have additional terms coming fromthe quantity:
t
h = Q (w −Q (m ,w ))−(w −Q (m ,w )). (6)
t 0 t 1 t t t 1 t t
3 Non-convex analysis
In the following, we prove that Boolean logic optimizer (Nguyen, 2023) converges towards
a first-order stationary point, as T the number of global epochs grows.
3.1 Preliminaries
Our analysis is based on the following standard non-convex assumptions on f:
A. 1. Uniform Lower Bound: There exists f ∈ R s.t. f(w) ≥ f , ∀w ∈ Rd.
∗ ∗
A. 2. SmoothDerivatives: The gradient ∇f(w) is L-Lipschitz continuous for some L > 0,
i.e., ∀w,∀v ∈ Rd: k∇f(w)−∇f(v)k ≤ Lkw−vk.
A. 3. Bounded Variance: The variance of the stochastic gradients is bounded by some
σ2 > 0, i.e., ∀w ∈ Rd: E [∇f(w)] = ∇f(w) and E [k∇f(w)k2] ≤ σ2.
A. 4. Compressor: There exists δ < 1 s.t. ∀w,∀v ∈ Rd, kQ (v,w)−vk2 ≤ δkvk2.
e e 1
A.5. BoundedAccumulator: Thereexistsκ ∈ R∗ s.t. ∀tand∀i ∈ [d],wehave|m | ≤ ηκ.
+ t i
A. 6. Stochastic Flipping Rule: For all w ∈ R, we have E [Q (w)|w] = w.
0
In particular, A. 5 and A. 6 enable us to obtain E [h ] = 0 and to bound the variance
t
of h . Based on all these assumptions, we prove the following:
t
Theorem 3.1. Assume A. 1 to A. 6. Boolean Logic applied to Boolean weights w con-
verges at rate:
1 T−1 A∗
Ek∇f (w )k2 ≤ +B∗η +C∗η2 +Lr , (7)
t d
T Tη
t=0
X
where A∗ = 2(f(w )−f ), B∗ = 2Lσ2, C∗ = 4L2σ2 δ , r = dκ.
0 ∗ (1−δ)2 d 2
Remark 3.2. Our analysis is independent of the quantization function Q . We impose a
0
weakassumptiononQ (assumptionA.6),whichholdsforstandardquantizationmethods
0
such as stochastic rounding.
4Remark 3.3. An important remark is that we only consider parameter quantization in the
analysis. Nonetheless, our results remain valid when an unbiased quantization function
is used to quantize computed gradients. Indeed, the stochastic gradients remain unbi-
ased under such quantization methods. The only effect of the quantization would be an
increased variance in the stochastic gradients.
Remark 3.4. Assumptions 1 to 3 are standard. Assumptions 4 to 6 are non-classic but
dedicated to Boolean Logic strategy. A. 4 is equivalent to assuming Boolean Logic op-
timization presents at least one flip at every iteration t. A. 4 is classic in the literature
of compressed SGD Karimireddy et al. (2019); Alistarh et al. (2017). Moreover, A. 5 and
A. 6 are not restrictive, but algorithmic choices. For example, rounding (Q function) can
0
be stochastic based on the value of the accumulator m . Similar to STE clipping strategy,
t
the accumulator can be clipped to some pre-defined value κ before applying the flipping
rule to verify A. 5.
Remark 3.5. Our proof assumes that the step size η is constant over iterations. But in
practice, we gently decrease the value of η at some time steps. Our proof can be adapted
to this setting by defining a gradient accumulator a such that a = a +q . When η is
t t+1 t t
constant we recover the accumulation definition and we obtain m = ηa . In the proposed
t t
algorithm, gradients are computed on binary weight w and accumulated in a . Then, one
t t
applies the flipping rule on the quantity w˜ = ηa (w˜ = m when η is constant), and one
t t t t
(may) reset the accumulator a .
t
We start by stating a key lemma which shows that the residual errors e maintained
t
in Algorithm 3 do not accumulate too much.
Lemma 3.6. Under A. 3 and A. 4, the error can be bounded as E [ke k2] ≤ 2δ(1+δ)η2σ2.
t (1−δ)2
Proof. We start by using the definition of the error sequence:
ke k2 = kQ (m ,w )−m k2.
t+1 1 t t t
Next we make use of A. 4:
ke k2 ≤ δkm k2.
t+1 t
We develop the accumulator update:
ke k2 ≤ δke +η∇f(w )k2.
t+1 t t
We thus have a recurrence relation on the bound of e . Using Young’s inequality, we have
t
e
that for any β > 0,
1
ke k2 ≤ δ(1+β)ke k2 +δ(1+ )η2k∇f(w )k2.
t+1 t t
β
Rolling the recursion over and using A. 3 we obtain: e
1
E [ke k2] ≤δ(1+β)E [ke k2]+δ(1+ )η2E [k∇f(w )k2]
t+1 t t
β
1
≤δ(1+β)E [ke k2]+δ(1+ )η2σ2 e
t
β
t
1
≤ (δ(1+β))rδ(1+ )η2σ2
β
r
X
δ(1+ 1)
≤ β η2σ2.
1−δ(1+β)
5Taking β = 1−δ and plugging it in the above bounds gives:
2δ
2δ(1+δ)
E [ke k2] ≤ η2σ2.
t+1
(1−δ)2
Then, the next Lemma allows us to bound the averaged norm-squared of the distance
between the Boolean weight and w −Q (m ,w ). We make use of the previously defined
t 1 t t
quantity Equation (6) and have:
Lemma 3.7. Under assumptions A. 5 and A. 6: E [kh k2] ≤ ηdκ.
t
Proof. Let consider a coordinate i ∈ [d]. Q | as −1 or +1 for value with some probability
0 i
p . For the ease of presentation, we will drop the subscript i. Denote u := w −
i,t t t
Q (m ,w ). Hence, h can take value (1−u ) with some probability p and (−1−u ) with
1 t t t t t t
probability 1−p . Assumption A. 6 yields 2p −1 = u . Therefore, we can compute the
t t t
variance of h as follows:
t
d
E [kh k2] = E [ 1+(w −Q (m ,w ))2 −2Q (w −Q (m ,w )(w −Q (m ,w )]
t t 1 t t 0 t 1 t t t 1 t t
i
X
d
= ((1−u )2p +(−1−u )2(1−p ))
t t t t
i
X
d
= (1+u2 −2u (2p −1))
t t t
i
X
d
= (1−u2).
t
i
X
The definition of u leads to
t
1−u2 = 1−(1+Q (m ,w )2 −2w Q (m ,w ))
t 1 t t t 1 t t
= Q (m ,w )(2w −Q (m ,w )).
1 t t t 1 t t
When |m | ≤ 1 or sign(m ) 6= sign(w ), we directly have Q (m ,w )(2w −Q (m ,w )) =
t t t 1 t t t 1 t t
0 ≤ ηκ. When |m | > 1 and sign(m ) = sign(w ), we apply the definition of Q to obtain:
t t t 1
Q (m ,w )(2w −Q (m ,w )) ≤ m (2w −m )
1 t t t 1 t t t t t
≤ |m |
t
≤ ηκ.
Therefore, we can apply this result to every coordinate, and conclude that:
E [kh k2] ≤ ηdκ.
t
63.2 Proof of Theorem 3.1
We now can proceed to the proof of Theorem 3.1.
Proof. Consider the virtual sequence x = w −e . We have:
t t t
x = Q (w −∆ )−(m −∆ )
t+1 0 t t t t
= (Q (w −∆ )+∆ −e )−η∇f(w ).
0 t t t t t
Considering the expectation with respect to the random variable Q and the gradient
e 0
noise, we have:
E [x |w ] = x −η∇f(w ).
t+1 t t t
We consider E [·] the expectation with respect to every random process know up to time
t
t. We apply the L-smoothness assumption A. 2, and assumptions A. 3, A. 6 to obtain:
L
E [f(x )−f(x )] ≤ −ηh∇f(x ),∇f(w )i+ E [k(Q (w −∆ )+∆ )−η∇f(w )−w k2].
t t+1 t t t t 0 t t t t t
2
We now reuse h t from Equation (6) and simplify the above: e
L
E [f(x )−f(x )] ≤ −ηh∇f(x ),∇f(w )i+ E [kh −η∇f(w )k2]
t t+1 t t t t t t
2
L
≤ −ηh∇f(x )−∇f(w )+∇f(w ),∇fe(w )i+ E [kh −η∇f(w )k2].
t t t t t t t
2
Using Young’s inequality, we have that for any β > 0, e
E [f(x )−f(x )] ≤−ηh∇f(x )−∇f(w )+∇f(w ),∇f(w )i
t t+1 t t t t t
L L 1
+ (1+β)E [kh k2]+ η2(1+ )σ2.
t t
2 2 β
Making use again of smoothness and Young’s inequality we have:
E [f(x )−f(x )] ≤−ηk∇f(w )k2 −ηh∇f(x )−∇f(w ),∇f(w )i
t t+1 t t t t t
L L 1
+ (1+β)E [kh k2]+ η2(1+ )σ2
t t
2 2 β
ηρ η
≤−ηk∇f(w )k2 + k∇f(w )k2 + k∇f(x )−∇f(w )k2
t t t t
2 2ρ
L L 1
+ (1+β)E [kh k2]+ η2(1+ )σ2
t t
2 2 β
ηρ ηL2
≤−ηk∇f(w )k2 + k∇f(w )k2 + kx −w k2
t t t t
2 2ρ
ketk2
L L 1
+ (1+β)E [kh k2]+ η2(1+ )σ2.| {z }
t t
2 2 β
Under the law of total expectation, we make use of Lemma 3.6 and Lemma 3.7 to obtain:
ρ ηL22δ(1+δ)
E [f(x )]−E [f(x )] ≤−η(1− )E [k∇f(w )k2]+ η2σ2
t+1 t
2
t
2ρ (1−δ)2
L L 1
+ (1+β)ηdκ+ η2(1+ )σ2.
2 2 β
7Rearranging the terms and averaging over t gives for ρ < 2 (we can choose for instance
ρ = β = 1):
T
1 2(f(w )−f ) δ(1+δ)
E [k∇f(w )k2] ≤ 0 ∗ +2Lσ2η +2L2σ2 η2 +2Ldκ.
t
T +1 η(T +1) (1−δ)2
t=0
X
4 Conclusion
The bound in Theorem 3.1 contains 4 terms. The first term is standard for a general
non-convex target and expresses how initialization affects convergence. The second and
third terms depend on the fluctuation of the minibatch gradients. Another important
aspect of the rate determined by Theorem 3.1 is its dependence on the quantization error.
Note that there is an "error bound" of 2Ldκ that remains independent of the number of
update iterations. The error bound is the cost of using discrete weights as part of the
optimization algorithm. Previous work with quantized models also includes error bounds
(Li et al., 2017; Li and De Sa, 2019).
5 Acknowledgments
LL would like to thank Van Minh Nguyen for the useful discussions that lead to the idea
of this project. We also thank Youssef Chaabouni for discussions, fixes and suggestions
on manuscript writing.
8References
D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. Qsgd: Communication-
efficient sgd via gradient quantization and encoding. Advances in neural information
processing systems, 30, 2017.
B. Chmiel, R. Banner, E. Hoffer, H. B. Yaacov, and D. Soudry. Logarithmic unbiased
quantization: Simple 4-bit training in deep learning. arXiv preprint arXiv:2112.10769,
2021.
M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect: Training deep neural net-
works with binary weights during propagations. In Advances in neural information
processing systems, pages 3123–3131, 2015.
C. De Sa, M. Leszczynski, J. Zhang, A. Marzoev, C. R. Aberger, K. Olukotun, and C. Ré.
High-accuracy low-precision training. arXiv preprint arXiv:1803.03383, 2018.
S. P. Karimireddy, Q. Rebjock, S. U. Stich, and M. Jaggi. Error feedback fixes
signsgd and other gradient compression schemes. CoRR, abs/1901.09847, 2019. URL
http://arxiv.org/abs/1901.09847.
L. Leconte, S. Schechtman, and E. Moulines. Askewsgd: an annealed interval-constrained
optimisation method to train quantized neural networks. In International Conference
on Artificial Intelligence and Statistics, pages 3644–3663. PMLR, 2023.
H. Li, S. De, Z. Xu, C. Studer, H. Samet, and T. Goldstein. Training quantized nets: A
deeper understanding. Advances in Neural Information Processing Systems, 30, 2017.
Z. Li and C. M. De Sa. Dimension-free bounds for low-precision training. Advances in
Neural Information Processing Systems, 32, 2019.
V. M. Nguyen. Boolean variation and boolean logic backpropagation. arXiv preprint
arXiv:2311.07427, 2023.
R. Zhang, A. G. Wilson, and C. De Sa. Low-precision stochastic gradient langevin dy-
namics. In International Conference on Machine Learning, pages 26624–26644. PMLR,
2022.
9