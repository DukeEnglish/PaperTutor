SYNCHFORMER:EFFICIENTSYNCHRONIZATIONFROMSPARSECUES
VladimirIashin1,3 WeidiXie2,3 EsaRahtu1 AndrewZisserman3
1TampereUniversity 2ShanghaiJiaoTongUniversity 3UniversityofOxford
ABSTRACT addition,weexploreevidenceattributiontechniquesforinter-
pretabilityandextendthecapabilityofthemodeltosynchro-
Our objective is audio-visual synchronization with a focus
nizability prediction, i.e. if it is possible to synchronize the
on ‘in-the-wild’ videos, such as those on YouTube, where
provided audio and visual streams. Finally, we scale up the
synchronization cues can be sparse. Our contributions in-
trainingtoAudioSet[8],amillion-scale‘in-the-wild’dataset,
cludeanovelaudio-visualsynchronizationmodel,andtrain-
and achieve state-of-the-art results in both dense and sparse
ing that decouples feature extraction from synchronization
settings.
modelling through multi-modal segment-level contrastive
pre-training. This approach achieves state-of-the-art perfor-
mance in both dense and sparse settings. We also extend 2. RELATEDWORK
synchronization model training to AudioSet a million-scale
‘in-the-wild’ dataset, investigate evidence attribution tech- Earlyworksprimarilyfocusedonsynchronizingthevideosof
niques for interpretability, and explore a new capability humanpresentations, forexample, [9,10]usedhand-crafted
for synchronization models: audio-visual synchronizability. featuresandstatisticalmodels. Inthedeeplearningera,[11]
robots.ox.ac.uk/~vgg/research/synchformer introduced a self-supervised two-stream architecture trained
using a contrastive loss. Subsequent enhancements included
IndexTerms— Audio-visualsynchronization,transform-
multi-way contrastive training [1] and the incorporation of
ers,multi-modalcontrastivelearning,evidenceattribution
Dynamic Time Warping [12, 13]. [14] showcased the ad-
vantages of spatio-temporal attention, while [15] introduced
1. INTRODUCTION a cross-modal embedding matrix to predict synchronization
offsets. Buildingonthisprogress,[16]presentedanarchitec-
Thetaskofaudio-visualsynchronizationistopredictthetem- ture featuring a set of transformers for cross-modal feature
poraloffsetbetweenaudioandvisualstreamsinavideo. Pre- extraction and fusion, while [17] explored contrastive pre-
vious work in this area has mostly addressed the scenario trainingtodetectifaspeechvideoisout-of-sync.
wheredensecuesareavailable,suchaslipmovementsintalk- Research in the synchronization of diverse classes of
ingheadsorinstrumentplaying[1,2,3,4,5].Whileforopen- videoswassparkedbyChenetal.[6]whoemployedatrans-
domain videos, e.g. YouTube videos of dogs barking or iso- formerandleveragedasubsetofVGGSound[18],addressing
lated actions, the synchronization cues are sparse, and only 160 distinct classes. Iashin et al. [7] explored synchroniza-
available at certain time intervals [6]. Naturally, this brings tion of videos with sparse temporal cues and proposed to
challenges in that an extended temporal window is required use learnable query vectors to pick useful features from the
sothatsparsecuesarenotmissed. audio and visual streams to reduce the computation burden,
Thesynchronizationofsparsesignalswasexploredin[7]. however,theapproachrequiredadatasetwithdensesynchro-
However, thisapproachhasseverallimitations, first, itrelies nization cues for pre-training, and the model can be trained
on the availability of a dataset with dense synchronization onlyend-to-endwhichposesahighcomputationburdenthat
cues for pre-training, second, it requires end-to-end training limitstheselectionoffeatureextractors.
which,inturn,posesahighmemoryfootprintandlimitsthe
selectionoffeatureextractors. Inthiswork,weaddressthese
limitationsbytrainingthemodelintwostages:first,segment- 3. METHOD
level contrastive pre-training of feature extractors, and sec-
ond, training of a light-weight synchronization module. In Given audio and visual streams A and V, a synchronization
model M predicts the temporal offset ∆ between them, i.e.
ThisresearchwasfundedbytheAcademyofFinlandprojects327910 ∆=M(A,V).Fig.1showsanoverviewofourSynchformer
and 324346, EPSRC Programme Grant VisualAI EP/T028572/1, and a (M) synchronization model. Instead of extracting features
RoyalSocietyResearchProfessorship.WealsoacknowledgeCSC(Finland)
fromtheentirevideo,weextractfeaturesfromshortertempo-
forawardingthisprojectaccesstotheLUMIsupercomputer,ownedbythe
EuroHPCJU,hostedbyCSCandtheLUMIconsortiumthroughCSC. ralsegments(0.64sec)ofthevideo. Thesegment-levelaudio
4202
naJ
92
]VC.sc[
1v32461.1042:viXraLoss
CLS SEP
Fig. 1. Synchformer (M). The audio and visual streams (A,V) are split into S segments of equal duration. Then, the
segment-levelinputsarefedintotheirrespectivefeatureextractors(F ,F ). Thestreamsareaggregatedalongthespace(or
a v
frequency)byG ,G ,andconcatenatedintoasinglesequencewithauxiliarytokens(CLSandSEP).Thesequenceisfedinto
a v
thesynchronizationmoduleT,whichpredictsthetemporaloffset∆ˆ. Thedashedlinesshowthetrainingofthemodel.
and visual inputs are fed into their respective feature extrac-
tors independently to obtain frequency and spatio-temporal
features.Then,thestreamsareaggregatedalongthespace(or
frequency)dimensionsinG ,G . Thesynchronizationmod-
a v
ule T inputs the concatenated sequence of audio and visual
aggregatedfeaturesandpredictsthetemporaloffset∆ˆ.
3.1. ArchitectureofSynchformer(M)
Fig. 2. Segment AVCLIP Pre-training. The audio (A)
Segment-level features. A video with audio (a mel spec- and visual (V) streams are split into S segments, which are
trogram)andvisualstreams(A,V)issplitintoSsegmentsof fedintotheirrespectivefeatureextractors(F ,F ). Theout-
a v
equal duration, A
s
∈ RF×Ta (F,T
a
are frequency and time puts of the feature extractors (a s,v s) are aggregated along
dimensions), and V
s
∈ RTv×H×W×3 (T v,H,W,3 are time, time (omitted for clarity) to obtain audio and visual features
height,width,andRGB).Foreachsegments,weobtainaudio (a˜ ,v˜ ).Thefeaturesfromcorrespondingsegmentsinavideo
s s
andvisualfeaturemaps,a
s
∈Rf×ta×d,v
s
∈Rtv×h×w×das arepulledtogether(↑),whilethefeaturesfromothersegments
arepushedapart(↓).
a =F (A ) v =F (V ) s∈{1,...,S}, (1)
s a s s v s
whereF isAST[19]andF isMotionformerwithdivided
a v
space-time attention [20, 21], a and v are outputs of last 3.2. Training
s s
layersofF andF . Toreducethesequencelength, weag-
a v The model is trained in two stages. First, audio and visual
gregatethefrequency(f)andspatial(h×w)dimensionsas,
feature extractors (Eq. (1), (2)) undergo pre-training with
(cid:0) (cid:1)
a s =G a [AGG,a s]) v s =G v([AGG,v s] (2) segment-levelcontrastivelearning, facilitatingtheextraction
of distinguishable features from each segment in a video.
where G are single-layer transformer encoders [22, 23],
a/v
Second, the synchronization module (Eq. (3)) is trained to
a
s
∈ Rta×d and v
s
∈ Rtv×d, [,] is a concatenation opera-
predictthetemporaloffsetbetweenaudioandvisualstreams,
tor, and AGG is learnable token whose output is used as the
usingfeaturesfromthepre-trainedandfrozenfeatureextrac-
aggregationa ,v . Trainablepositionalencodingsareadded.
s s
tors(whoseweightsarenotupdatedduringthisstage).
Synchronization module. The audio and visual features
a ,v from all segments S are concatenated with learnable Segment-levelaudio-visualcontrastivepre-training. Draw-
s s
tokens, alongthetimedimension, andthenfedintothesyn- ing on the success of CLIP [24], we train the model with
chronizationmoduleT thatpredictsthetemporaloffset∆ˆ, InfoNCE loss [25] to distinguish between positive and neg-
∆ˆ =T(cid:0) [CLS,a ,...,a ,SEP,v ,...,v ](cid:1) , (3) ative pairs. In our setting, a positive pair is a pair of audio
1 S 1 S andvisualsegmentsfromthesametimeintervalofthesame
whereSEPisaseparatingtoken, andT isatransformeren- video,whilenegativepairsconsistofsegmentsfromthesame
coderwith3layers,8heads,andd=768. Following[7],we videoandsegmentsfromothervideosinthebatch.
posesynchronizationasaclassificationproblemwithafixed Toobtainlogitsforeachsegment,weaveragea ,v along
s s
number of classes. We add trainable positional encoding to the time dimension (t ,t ), project them with a linear layer,
a v
the input sequence. To predict ∆ˆ, we apply a linear layer andnormalizetounitlength,yieldinga˜ ,v˜ .Thus,forabatch
s s
withsoftmaxactivationtotheoutputoftheCLStoken. ofBaudio-visualpairs(videos)withSsegmentsineach,thesegment-levelcontrastivelossisdefinedby masked input, we observe the model’s prediction and, if the
predictioniscorrect(±0.2sec),werecordthemask. Togeta
LI =−
1 (cid:88)BS
log
exp(a˜ i·v˜ i/τ)
, (4)
reliablescore,werepeatthisprocessmultipletimes(K).
a→v BS (cid:80)BS exp(a˜ ·v˜ /τ) Theattributionscoreofatemporalintervaltisdefinedas
i=1 j=1 i j (cid:80)K 1(M =1∧∆ˆ =∆ )/K. Thescoreiscloserto1if
k=1 k,t i i
whereτ isatrainabletemperatureparameter.Thecounterpart theintervalisimportant, andcloserto0.5ifitislessimpor-
loss(LI )isdefinedanalogously. Thetotalcontrastiveloss tant. Considering a large number of potential permutations,
v→a
is obtained by averaging the two, LI = (LI +LI )/2. wemaskchunksofaudioandvisual‘frames’atonce,i.e.0.1
a→v v→a
With B = 2 videos and S = 14 segments per video (8.96 sec, and operate on one modality at a time. As a masking
sec, in total), the effective batch size is BS = 28. In our strategy,wereplacethecontentinselectedintervalswiththe
experiments,higherB orusingthemomentumqueuedidnot contentfromarandom‘distractor’videofromthetestset.
translate to better synchronization performance. We refer to
thisasSegmentAVCLIPandoutlineitinFig.2. 4.2. PredictingSynchronizability
Incontrasttothegeneralassumptioninexistingwork,thatall
Audio-Visual synchronization module training. We as- videosaresynchronizable,weadditionallyexplorethepossi-
sume that the audio and visual streams in the training data bilityofinferringthesynchronizabilityofprovidedaudioand
are synchronized. We formulate synchronization as a clas- visualstreams,i.e.ifitispossibletosynchronizethematall.
sification problem with a fixed number of classes, follow- To train a model for this task, we fine-tune Synchformer by
ing [7]. During the training of this stage, we rely on the au- addinganotherbinarysynchronizabilitypredictionhead. We
dio and visual features a s,v s obtained before the time-wise usethesametrainingdataasforthesynchronizationtraining
within-segmentaggregation, asinEq.(2). Segmentsareex- and uniform sampling of the offset target class, but with 0.5
tracted with a temporal step size that overlaps half the seg- probability use an offset that is equal to the duration of the
ment length, as this was found to improve synchronization inputtracktoensurethatthestreamsarenotsynchronizable.
performance. We use a batch size of 16 videos with 14 seg- Themodelistrainedwithbinarycrossentropyloss.
ments (4.8 sec, in total) in each. The model is trained with
crossentropyloss.
5. EXPERIMENTS
3.3. Discussion Datasets. Forthedensesetting,weuse∼58kclipsfromthe
LRS3dataset[26]processedasin[7],i.e.thevideosarenot
Ourapproachyieldsseveraladvantagesoverpriordesigns[7]:
cropped to the face region (‘full scene’). For the sparse set-
(i)Sincethefeatureextractors(transformers)nowoperateon
ting,weusetheVGGSound[18]datasetthatcontains∼200k
shorter input sequences and the synchronization module is
‘in-the-wild‘ 10-second YouTube videos (∼186k after filter-
trained later, we can allocate more trainable parameters and
ingformissings). Wetrainbothstagesofourmodelonthis
get higher-quality features; (ii) Provided that the features
dataset(§3.2). Forevaluation,weuseVGGSound-Sparse[7],
from different segments in a video are distinguishable, a
a curated subset of 12 sparse data classes from VGGSound-
light-weightsynchronizationmodulecanbetrainedwhilethe
Sparse[7]whichconsistsof∼600videos(542afterfiltering)
weights of feature extractors are frozen (not updated); (iii)
and VGGSound-Sparse (Clean) where all input clips (439
It streamlines the temporal evidence attribution visualiza-
videos)haveoneormoresynchronizableevents. Inaddition,
tion for the synchronization task; (iv) It is adaptable to new
wealsousetheAudioSet[8]datasetwhichconsistsof∼2M
downstreamtasks,suchassynchronizabilityprediction.
(∼1.6Mafterfiltering)‘in-the-wild’YouTubevideos.
4. ADDITIONALCAPABILITIES
Metrics. Asameasureofsynchronizationperformance,we
usetheaccuracyofthetop-1predictionofthetemporaloffset
4.1. EvidenceAttribution
across 21 classes from −2 to +2 seconds, with 0.2 seconds
Weaimtodeterminethetemporalevidenceusedbythemodel granularity. Similarto[7], weallowfor±0.2secondstoler-
forsynchronizationpredictions,assigninganattributionscore ance (±1 class) for the prediction to be considered correct.
toeachtemporalintervaloftheaudioandvisualstreams. We Forsynchronizabilityprediction,weusebinaryaccuracy.
assumethatifapartoftheinputisnotimportant,themodel
canpredicttheoffsetcorrectlywithoutit. Baseline. We compare our approach to the state-of-the-art
To determine the importance of a temporal interval, we method for audio-visual synchronization, SparseSync [7],
randomlysampleamaskM ∈RT ∈[0,1](ifM =0the which relies on ResNet-18 [27] and S3D [28] as audio and
k k,t
contentismasked),whereT =S(t +t ),andapplyittothe visualfeatureextractors,andtwoDETR-like[29]transform-
v a
input of the synchronization module T. For each randomly erstopickusefulfeaturesfromtheaudioandvisualstreams,Dense↓
RGB Frames
Train Params Train/ LRS3(‘FullScene’)
Method dataset (×106) (×103) A@1/±1cls
AVST[6,7] LRS3-FS 32.3 0.2 58.6/85.3
SparseS[7] LRS3-FS 55.3 0.9 80.8/96.9
Synchformer LRS3-FS 214+22.6 0.4+0.6 86.5/99.6
RGB Attribution Score
Sparse↓ ''hhiitt''
Train Params Train/ VGS-Sp VGS-Sp(C) ooffffsseett
Method dataset (×106) (×103) A@1/±1cls A@1/±1cls
AVST[6,7] VGS-Sp∗ 32.3 0.3 14.1/29.3 16.6/32.1 0.00 0.64 1.28 1.92 2.56 3.20 3.84 4.48
SparseS[7] VGS-Sp∗ 55.3 1.0 26.7/44.3 32.3/50.0
Audio Attribution Score
SparseS[7] VGS∗ 55.3 1.8 33.5/51.2 43.4/62.1
''hhiitt''
SparseS[7]† AudioSet∗ 55.3 19.7 35.3/56.7 40.0/63.0
Synchformer VGS 214+22.6 0.5+2.1 43.7/60.2 52.9/70.1
Synchformer AudioSet 214+22.6 4.2+4.4 46.8/67.1 54.6/77.6
0.00 0.64 1.28 1.92 2.56 3.20 3.84 4.48
Table 1. Synchronization results for dense (top) and
Spectrogram (out of sync by -1.20 sec wrt video)
sparsesignals. Thedensesettingresultsarereportedonthe
test set of LRS3-FS (‘Full Scene’), and for the sparse set-
ting,thetestsetofVGGSound-Sparse(VGS-Sp)andVGS-Sp 0.00 0.64 1.28 1.92 2.56 3.20 3.84 4.48
Seconds
Clean(C)areused. Thetop-1accuracyisreportedacross21
offsetclasseswithoutandwith±1classtolerance(indicated
Fig.3. Visualizationofevidenceattribution. Themoment
by the value before and after ‘/’). The train time is shown
of ‘hitting the ball’ and the ground truth ‘offset’ are high-
in GPU hours. For Synchformer, we show the combined
lightedinbothstreams.
(+) number of parameters and training time for both stages.
†:trainedbyus;∗: pre-trainedonLRS3(‘FullScene’).
1.0
0.9
0.8
andfinally,therefinedfeaturesarepassedtothetransformer 0.6 0.8
encodertopredictthetemporaloffset. 0.4 0.7
0.2
Area = 0.83 0.6
0.0
5.1. SynchronizationResults 0.0 0.2 0.4 0.6 0.8 1.0 0.99 0.90 0.80 0.70 0.60 0.50
False Positive Rate Synchability Threshold
InTab.1(top),wereporttheresultsonthedensesetting,i.e. Fig. 4. Predicting synchronizability with Synchformer.
by training and testing on the LRS3 (‘Full Scene’) dataset. Left: ROCcurve. Right: thesynchronizationperformanceon
Accordingtotheresults,ourmodeloutperformsthestate-of- videosthatwererankedbythesynchronizabilitymodel. The
the-artmethodbyasignificantmargin. resultsarereportedonVGGSound-Sparse.
TheresultsonthesparsesettingareshowninTab.1(bot-
tom), i.e. by training VGGSound and AudioSet and evaluat-
ingonthetestsetVGGSound-Sparseoracleanversionofit. present in all videos, the model was able to benefit from the
Eventhoughthebaselinemethodswerepre-trainedondense largeamount,yetnoisy,trainingdata. Weprovidetheresults
data(LRS3(‘FullScene’)),ourmodeloutperformsthembya ofanextensiveablationstudyinanArXivversion.
significantmarginaftertrainingonlyonsparsedata,i.e. rows
3 vs.5 and 4 vs.6, which showcases the benefits of the pro- 5.2. ResultsonAdditionalCapabilities
posedapproach.Noticethat,withthetwo-stageapproach,we
Evidenceattribution. InFig.3,wevisualizetheevidence
can train much larger models (55.3M vs 200+M), which re-
used by the model to make the synchronization predictions
sultsinbetterperformance,yetthetrainingtimeiscompara-
foratestvideofromVGGSound-Sparseasdescribedin§4.1.
bleonsmallerdatasets(VGGSoundandVGGSound-Sparse)
Theattributionvaluesaremin-maxscaled. WeusetheSynch-
andmuchlessonthelargerone(AudioSet).
formerthatwastrainedontheVGGSounddataset. Thevideo
In addition, we report the results of training on the Au-
isout-of-syncby−1.2sec,andthemodelpredictsthecorrect
dioSet dataset, which is a million-scale dataset of ‘in-the-
offset. Themodelisabletoattributeevidencemoreprecisely
wild’ videos. Note that, this dataset has never been used
intheaudiostream,i.e. onlypeaksat∼3.15sec.
for training synchronization models, due to the low compu-
tationalefficiencyinpreviousmodels. Forafaircomparison,
we also trained the baseline method [7] on AudioSet. Al- Predicting syncronizability results. In Fig. 4 (left), we
thoughtheaudio-visualcorrespondenceinthisdatasetisnot use the model that was trained for synchronization, fine-
etaR
evitisoP
eurT
slc1±1@ccA
noitazinorhcnyStune this for synchronizability using AudioSet, and test it [8] J. Gemmeke, D. Ellis, D. Freedman, A. Jansen,
on VGGSound-Sparse. The area under the curve is 0.83, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter,
setting the baseline for the task. In Fig. 4 (right), we dis- “Audioset: Anontologyandhuman-labeleddatasetfor
play the offset prediction performance for videos from the audioevents,” inICASSP,2017.
VGGSound-Sparse dataset, filtered by the synchronizability
model based on the confidence of video synchronizability. [9] J.HersheyandJ.Movellan,“Audiovision:Usingaudio-
This gives an evaluation of the usefulness of synchroniz- visualsynchronytolocatesounds,” NeurIPS,1999.
ability. As anticipated, the offset prediction performance
[10] M. Slaney and M. Covell, “Facesync: A linear opera-
is superior for audio-visual clips that the synchronizability
torformeasuringsynchronizationofvideofacialimages
modelexpressesconfidencein.
andaudiotracks,” NeurIPS,2000.
6. CONCLUSION [11] J.S.ChungandA.Zisserman,“Lipreadinginthewild,”
inACCV,2016.
We proposed a novel transformer-based model for audio-
visualsynchronizationwhichoutperformsthestate-of-the-art [12] L. Rabiner and B.-H. Juang, Fundamentals of speech
methodbyasignificantmargininbothdenseandsparseset- recognition, Prentice-Hall,Inc.,1993.
tings. We achieve this by decoupling the training of feature
[13] T.Halperin,A.Ephrat,andS.Peleg, “Dynamictempo-
extractorsfromthesynchronizationmodule. Tothisend,we
ralalignmentofspeechtolips,” inICASSP,2019.
introducedanovelSegmentAVCLIPpre-trainingmethodfor
segment-level contrastive learning and a Synchformer syn-
[14] N. Khosravan, S. Ardeshir, and R. Puri, “On attention
chronization module which operates on the ‘frozen’ feature
modulesforaudio-visualsynchronization,”inWorkshop
extractors that promotes adaptability for other downstream
onSightandSound,CVPR,2019.
tasks, such as synchronizability prediction, which is a novel
taskintroducedinthispaper. Finally, weexploredamethod
[15] Y.J.Kim,H.S.Heo,S.-W.Chung,andB.-J.Lee,“End-
for evidence attribution that highlights the evidence used by
to-end lip synchronisation based on pattern classifica-
themodeltomakesynchronizationpredictions.
tion,” inSLTWorkshop,2021.
[16] V.S.Kadandale,J.F.Montesinos,andG.Haro, “Vocal-
7. REFERENCES
ist: Anaudio-visualsynchronisationmodelforlipsand
[1] S.-W. Chung, J. S. Chung, and H.-G. Kang, “Perfect voices,” inInterspeech,2022.
match: Improved cross-modal embeddings for audio-
[17] A. Gupta, R. Tripathi, and W. Jang, “ModEFormer:
visualsynchronisation,” inICASSP,2019.
Modality-preserving embedding for audio-video syn-
[2] J.S.ChungandA.Zisserman, “Outoftime: automated chronization using transformers,” in ICASSP. IEEE,
lip sync in the wild,” in Workshop on Multi-view Lip- 2023.
reading,ACCV,2016.
[18] H.Chen,W.Xie,A.Vedaldi,andA.Zisserman, “VGG-
[3] T. Afouras, A. Owens, J. S. Chung, and A. Zisserman, Sound: Alarge-scaleaudio-visualdataset,” inICASSP,
“Self-supervised learning of audio-visual objects from 2020.
video,” inECCV,2020.
[19] Y.Gong,Y.Chung,andJ.Glass, “AST:AudioSpectro-
[4] R. Arandjelovic and A. Zisserman, “Objects that gramTransformer,” inInterspeech,2021.
sound,” inECCV,2018.
[20] P.Mandela,D.Campbell,Y.Asano,I.Misra,F.Metze,
[5] A. Owens and A. Efros, “Audio-visual scene analysis C. Feichtenhofer, A. Vedaldi, and J. F. Henriques,
with self-supervised multisensory features,” in ECCV, “Keeping your eye on the ball: Trajectory attention in
2018. videotransformers,” inNeurIPS,2021.
[6] H. Chen, W. Xie, T. Afouras, A. Nagrani, A. Vedaldi, [21] G. Bertasius, H. Wang, and L. Torresani, “Is space-
andA.Zisserman, “Audio-visualsynchronisationinthe time attention all you need for video understanding?,”
wild,” inBMVC,2021. inICML,2021.
[7] V.Iashin,W.Xie,E.Rahtu,andA.Zisserman, “Sparse [22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
in space and time: Audio-visual synchronisation with L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,
trainableselectors,” inBMVC,2022. “Attentionisallyouneed,” inNeurIPS,2017.[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Pre-train Pre-train Frozen Segment VGS-Sparse
“BERT:Pre-trainingofdeepbidirectionaltransformers unimodal AVCLIP featextr overlap Acc@1/±1cls
forlanguageunderstanding,” inNAACL:HLT,2019.
✗ ✗ ✗ ✔ 5.3/10.7
✔ ✗ ✗ ✔ 8.4/22.8
[24] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,
✔ ✔ ✗ ✔ 21.0/40.9
S.Agarwal,etal., “Learningtransferablevisualmodels
✔ ✔ ✔ ✗ 40.1/57.0
fromnaturallanguagesupervision,” inICML,2021. ✔ ✔ ✔ ✔ 43.7/60.2
[25] A.Oord, Y.Li, andO.Vinyals, “Representationlearn-
Table 2. Ablation study: initialization, training, seg-
ing with contrastive predictive coding,” arXiv preprint
ment overlap. The results are reported on the test set of
arXiv:1807.03748,2018.
VGGSound-Sparse. The metrics are top-1 accuracy across
[26] T. Afouras, J. S. Chung, and A. Zisserman, “LRS3- 21 offset classes without and with ±1 class tolerance (indi-
TED: a large-scale dataset for visual speech recogni- catedbythevaluebeforeandafter‘/’). ‘Pre-trainunimodal’
tion,” arXivpreprintarXiv:1809.00496,2018. –ifaudioandvisualfeatureextractorsarepre-trainedonAu-
dioSet and Something-Something v.2 [30] for audio and ac-
[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual tion recognition; ‘Pre-train AVCLIP’ – if the feature extrac-
learningforimagerecognition,” inCVPR,2016. tors are pre-trained with Segment AVCLIP (§3.2); ‘Frozen
featextr’–iffeatureextractors’weightsare‘frozen’(notup-
[28] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy,
dated) during synchronization training; ‘Segment overlap’ –
“Rethinking spatiotemporal feature learning: Speed-
ifthesegmentsareoverlappedbyhalfofthesegmentlength
accuracy trade-offs in video classification,” in ECCV,
duringsynchronizationtraining. Theperformanceofthefinal
2018.
modelisreportedinthelastrow.
[29] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kir-
illov, and S. Zagoruyko, “End-to-end object detection
withtransformers,” inECCV,2020. than GPU, i.e. the training of the Stage II can be con-
ducted on a modest GPU. The code is publicly available
[30] R.Goyal,S.Kahou,V.Michalski,J.Materzynska,etal., at: github.com/v-iashin/Synchformer
“The "something something" video database for learn-
ing and evaluating visual common sense,” in ICCV,
2017.
B. ABLATIONSTUDIES
A. IMPLEMENTATIONDETAILS
Initialization, training, and segment overlap. As shown
We use 0.64 seconds long segments. The RGB and audio in Tab. 2, training the weights of the synchronization mod-
streams are resampled to 25 fps and 16 kHz, following best ule from scratch, i.e. without pre-training of the feature ex-
practices suggested in [7]. We extract 128 mel-spectrogram tractors, significantly reduces the performance. We also
channelsfromaudiosegmentsof25mswithahopof10ms. found that initializing feature extractors with pre-trained
RGBframesarecroppedto2242. Thisgivesustheinputof weightsfromtheunimodalpre-training,e.g.onAudioSetand
size(128×66×1)foraudioand(16×224×224×3)forRGB Something-Something v.2 [30] for audio and action recog-
segments(A ,V ). Afterapplyingthefeatureextractors, we nition, respectively, improves the performance, yet it took
s s
obtain audio and visual features (a ,v ) of size (12 × 6 × significantlymoreiterationsforthemodeltotrainanditstill
s s
768)and(8×14×14×768),respectively. Duringthefirst underperformedcomparedtoothersettings.Theinitialization
stage, we use the batch size B = 2 and S = 14 segments with the weights from Segment AVCLIP pre-training (§3.2)
pervideo(=8.96seconds),totallingBS =28elementsinthe allows the model to start learning faster and reach higher
contrastive pool. In our experiments, higher B or using the performance. However,wenoticedthatiftheweightsofthe
momentum queue did not translate to better synchronization feature extractors are kept trainable during the synchroniza-
performance. During the second stage, we use B = 16 and tiontraining,themodeloverfitsthetrainingdataandperforms
S = 14 half-overlapped segments per video (=4.8 seconds). poorlyonthetestset. Freezingthefeatureextractorsduring
ThetransformerT has3layersand8attentionheadsandd= the training of the synchronization module allows increased
768hiddenunits. batch size and learning rate that speeds up and stabilizes
Both stages were trained on 16 (32) AMD Instinct the training allowing better performance. Finally, we found
MI250 GPUs. Half precision was used during training. that training the model with half-overlapped segments dur-
We note, that the bottleneck for training the synchroniza- ing the synchronization training improves the performance
tion module (Stage II) was associated with data I/O rather significantly.Trainingdata:LRS3(‘FullScene’)↓
Segment #ofsegments Input
length (during length VGS-Sparse Stage LRS3(‘FullScene’)
(seconds) 2ndstage) (seconds) Acc@1/±1cls I II (N =5966×2)
2.56 3 5.12 26.2/49.3 i A 86.4/99.6
1.28 7 5.12 40.0/61.0 ii B 86.6/99.6
0.64 14 4.80 43.7/60.2
Trainingdata:VGGSound↓
0.64 15 5.12 44.7/62.1
Stage VGGSound-Sparse VGGSound-Sparse(Clean)
0.32 31 5.12 41.2/58.2
I II (N =542×25) (N =439×1)
0.16 63 5.12 31.2/49.7
0.08 127 5.12 25.9/46.0 iii C 43.4/60.3 51.8/72.1
iv D 44.1/60.2 54.0/68.1
Table 3. Ablation study: segment length. The results are iv E 43.8/60.2 55.1/71.4
reported on the test set of VGGSound-Sparse. The metrics Trainingdata:AudioSet↓
are top-1 accuracy across 21 offset classes without and with
Stage VGGSound-Sparse VGGSound-Sparse(Clean)
±1 class tolerance (indicated by the value before and after
I II (N =542×25) (N =439×1)
‘/’). The preferred model is in blue. The best model is in
v F 46.4/66.8 53.8/77.9
bold.
vi G 47.2/67.4 55.4/77.2
Table5. Ablationstudy: resultsreproducibility. Thesyn-
Feature LRS3-FS VGS-Sparse chronization results are reported on test sets of LRS3 (‘Full
Method extractors Acc@1/±1cls Acc@1/±1cls Scene’),VGGSound-Sparse,andVGGSound-Sparse(Clean).
SparseS[7] ResNet-18+S3D 80.8/96.9 33.5/51.2∗ ThecheckpointsinthetoptableweretrainedonLRS3(‘Full
Ours ResNet-18+S3D 82.6/98.9 33.9/54.7 Scene’), VGGSoundwasusedforthemiddletable, andAu-
Ours AST+Mformer 86.5/99.6 43.7/60.2 dioSetforthebottomtable.N indicatesthesizeofthedataset
multipliedbythenumberoftimesoffsetswererandomlysam-
Table4. Ablationstudy: featureextractors. Thesynchro- pled. Themetricsaretop-1accuracyacross21offsetclasses
nizationresultsarereportedonthetestsetsofLRS3-FS(‘Full without and with ±1 class tolerance (indicated by the value
Scene’)andVGGSound-Sparse(VGS-Sparse)datasets. The before and after ‘/’). The checkpoint IDs are i–vi and A–G
metrics are top-1 accuracy across 21 offset classes without forStagesIandII,respectively.
andwith±1classtolerance(indicatedbythevaluebeforeand
after ‘/’). ResNet-18 and S3D were initially pre-trained for
Feature extractors. In Table 4, we show the performance
audio recognition on VGGSound and action recognition on
of our approach with another set of audio and visual feature
Kinetics400,whileASTandMotionformer(Mformer)were
pre-trained on AudioSet and Something-Something v.2. ∗ – extractors. In particular, instead of the AST and Motion-
formerthatwereusedinthiswork,weemployS3D[28]and
the synchronization model was pre-trained on dense signals
ResNet-18 [27] as in SparseSync [7]. By matching feature
first(LRS3-FS).
extractors to [7], we aim to highlight the importance of the
trainingapproachintroducedinthiswork. Wenotethateven
though SparseSync was additionally pre-trained on LRS3
(‘Full Scene’) for synchronization before training on VG-
Segment length. We compare the model performance
GSound, our approach yields superior results on both dense
across different segment lengths and report the results in
andsparsedatasets. Also, ourmethodallowspre-trainingof
Table 3. For all selected segment lengths, except for 0.64
feature extractors to be separated from training the synchro-
seconds,weconductthefulltrainingpipelineasusedforthe
nization module, which gives a larger computation budget
finalmodel(see§3.2). Fortheexperimentwith0.64-second
forfeatureextractors. ByreplacingResNet-18andS3Dwith
segments, we trained only the synchronization module on
AST and Motionformer, that have almost one order of mag-
longer inputs (15 segments). Note that we conduct this ad-
nitude more trainable parameters, we improve performance
ditional experiment with 0.64-second segments to make the
significantly.
totalinputlengthinsecondsequalto5.12forafaircompari-
son. AllmodelsforthisablationweretrainedonVGGSound.
We make two observations: (i) the model trained with 0.64- C. NOTEONRESULTSREPRODUCIBILITY
secondsegmentsperformsthebest(44.7/62.1), (ii)extend-
ing the input length by one segment improves performance To get robust estimates on the performance of our model,
asmoreinformationisprovidedtothemodeltoperformthe we average performance across multiple training runs for
task(43.7/60.2vs44.7/62.1). LRS3 (‘Full Scene’), VGGSound, and AudioSet and reportthe mean in the main part of this paper. The original values
of experiments that were used for averaging are reported in
Table5.
In addition, we conduct multiple rounds of offset sam-
plingforeachvideointhetestsetsoftheevaluationdatasets
to increase the robustness of an individual model. For in-
stance,wesampleanoffsetandastartingpointofatemporal
crop2times(×2intheTable5)forLRS3(‘FullScene’)and
25times(×25)forVGGSound-Sparse.Theresultsacrossthe
roundsareaveraged. Noticethatweonlyuseonesetofoff-
setsforVGGSound-Sparse(Clean)consideringthedifficulty
ofannotatingitmanually.
Inourexperiments,wefoundthatonecouldreplicatethe
training dynamics (shapes of the loss curve) from previous
runs during Stage I (feature extractors pre-training) for all
datasets. For Stage II, the training dynamics are predomi-
nantlysimilaronLRS3(‘FullScene’)andAudioSetbutvary
slightlyforVGGSound.
Wefoundthatonecouldreplicatetheperformanceofthe
LRS3 model up to 0.1% even if both stages were re-trained
anew. However,theperformanceoftheVGGSoundandAu-
dioSet models vary which results in variations of up to 0.8
percentpointswhenevaluatedonVGGSound-Sparse.
Finally,wenoticedastrongvariation(±3%points)inthe
performanceofthemodelontheVGGSound-Sparse(Clean)
acrossrunsThisislikelyduetoitssize(439videos)andthe
factthatonlyoneoffsetpervideoisevaluated.
The variation in performance across runs comes from
multiplesourcesthatinclude(butnotlimitedto):
1. Variation in training during the first stage (segment-
levelcontrastivepre-training);
2. Variationintrainingduringthesecondstage(synchro-
nization module training), e.g. due to unique offset
sampling at every iteration, which we call ‘path de-
pendency’, and accumulated variation from the first
stage;
3. Variation in the evaluation due to random offset sam-
pling (e.g. for every video in VGGSound-Sparse, we
randomly sample 25 offsets and average the perfor-
manceacrossthemasdescribedabove).
WetestedifthevariationiscausedbytheweightsofStage
I.Tothisend, wetrainedasynchronizationmodel(StageII,
iv–E)withthefeatureextractorsthatwereinitializedwiththe
sameweightsastheotherrunofStageII(iv–D).However,the
resultsdidn’tshowanydifferenceinvariationwhichsuggests
thatvariationisnotcausedbythethedifferenceistheweights
ofStageIalone.