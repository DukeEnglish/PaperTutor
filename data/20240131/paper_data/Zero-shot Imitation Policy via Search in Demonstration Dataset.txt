ZERO-SHOTIMITATIONPOLICYVIASEARCHINDEMONSTRATIONDATASET
FedericoMalato∗1,FlorianLeopold*2,AndrewMelnik2,VilleHautama¨ki1
1SchoolofComputing,UniversityofEasternFinland,Finland
2BielefeldUniversity,Germany
federico.malato@uef.fi
ABSTRACT Previous literature has addressed agents adaptability problems
by combining time-step-wise kNN search in latent space with Lo-
Behavioralcloningusesadatasetofdemonstrationstolearnapol-
cally Weighted Regression [24] for action selection [25, 26, 27].
icy. To overcome computationally expensive training procedures
Nonetheless, studies in this direction have been conducted only in
and address the policy adaptation problem, we propose to use la-
controlledroboticenvironmentswithcontinuousactions. Addition-
tentspacesofpre-trainedfoundationmodelstoindexademonstra-
ally, in these environments a reward signal is either used, such as
tiondataset,instantlyaccesssimilarrelevantexperiences,andcopy
in [26, 27], or can be inferred, for instance in [25]. Our work ex-
behaviorfromthesesituations. Actionsfromaselectedsimilarsit-
tends the applicability of search-based methods to discrete-action
uation can be performed by the agent until representations of the
domains,andvalidatestheminopen-endedenvironment.Moreover,
agent’scurrentsituationandtheselectedexperiencedivergeinthe
wedemonstratezero-shotadaptationforsuchmethods.
latentspace. Thus,weformulateourcontrolproblemasadynamic
To this end, we introduce Zero-shot Imitation Policy (ZIP), a
searchproblemover adatasetofexperts’demonstrations. Wetest
search-basedapproachtoimitationlearningthatinstantlyadaptsto
our approach on BASALT MineRL-dataset in the latent represen-
new tasks in complex, discrete domains. ZIP encodes all experts’
tation of a Video Pre-Training model. We compare our model to
trajectories into a reasonable latent space. We define the concept
state-of-the-art,ImitationLearning-basedMinecraftagents.Ourap-
ofsituation,aself-containedtrajectorythatincludesbothstatesand
proachcaneffectivelyrecovermeaningfuldemonstrationsandshow
correspondingactions. Wecouldthinkaboutsituationasashotin
human-likebehaviorofanagentintheMinecraftenvironmentina
amovie. ZIPcomparesthecurrentsituationwiththepreviousex-
wide variety of scenarios. Experimental results reveal that perfor-
periencesencodedinlatentspace, searchesfortheclosestoneand
manceofoursearch-basedapproachclearlywinsintermsofaccu-
executes the actions in it until the situation changes. This allows
racyandperceptualevaluationoverlearning-basedmodels.
thepolicytoadapttochangesintheenvironmentbyjustcollecting
IndexTerms— imitationlearning,behavioralcloning,Minecraft, newexperttrajectoriesinthatconfiguration.Perceptualexperiments
MineRL,BASALT showthattheproposedmethodclearlyoutperformsotheragentsex-
ceptpurelyscriptedagents.
1. INTRODUCTION
2. METHODS
ImitationLearning(IL)[1]andReinforcementLearning(RL)[2,3,
4, 5] are commonly used methods to train agents to perform tasks
ThepresentstudyismotivatedbytheMineRLBASALT2022chal-
insimulatedandrealenvironments. Despitetheextensiveresearch
lenge[28,29].Inthechallenge,anagentmustsolvefourtasks:find
inthesefields,anumberofpersistentchallengesstillhold. Among
acave,buildananimalpen,buildavillagehouse,andmakeawa-
them, significant computational costs and lack of zero-shot adapt-
terfall[28,29]. Forthesetasks,norewardfunctionisprovided. To
ability are the most prominent. Recent works leverage large lan-
solvethetasks, participantsareprovidedwithadatasetofdemon-
guage[6,7,8]andvisionmodels[9,10]toachievefew-shotadapt-
strations,eachshowingahumanexpertsolvingoneofthetasks.
ability. Still, such models are computationally expensive to train.
Thecoreideabehindourapproachistoreformulatethecontrol
Therefore, exploringalternativeapproachestoILandRLmethods
problemasasearchproblemontheexperts’demonstrations.
for control problems can potentially address these challenges and
Tokeepthesearchcomputationallyfeasible,wetakeadvantage
yieldadvantagesinspecificapplicationdomains.
oflargepre-trained modelsoftheproblemdomain. Recentlypro-
BehavioralCloning(BC)[11]hasbeensuccessfullyappliedto
posedVideoPre-Training(VPT)model[30](seeFigure1)isafoun-
practicalcontrolproblemsrangingfromautonomousdriving[12,13,
dationmodelforBCtrainedon70k+hoursofvideocontentscraped
14] to playing video games [15, 16, 17]. Despite being tremen-
fromtheinternet[30]. VPTisbuiltonanIMPALA[31]convolu-
dously popular due to its simplicity, BC suffers from a range of
tionalneuralnetwork(CNN)backbone. TheCNNmapsanimage
problemssuchasdistributionalshiftandcausalconfusion[18,19].
inputtoa1024-dimensionalfeaturevector.VPTgeneratesabatchof
Such limitations have been addressed with inverse reinforcement
129vectorsandforwardthemtofourtransformerblocks.Eachblock
learning[20](IRL)orgenerativeadversarialimitationlearning[21]
islinkedtoamemoryblockcontainingthelast128frames. Atthe
(GAIL), which on the other hand tend to be computationally ex-
endofthetransformerpipeline,onlythelastframeisretainedand
pensive and hard to train [22, 23]. Moreover, in case of complex
forwarded to two heads based on Multilayer Perceptrons (MLPs).
scenarios,suchmethodsmightstruggleinlearningasuitablestrat-
Oneheadpredictsakeyboardaction,whilethesecondheadpredicts
egy[19,20].
acomputermouseaction.
∗Equalcontribution Weuseapre-trainedVPTmodel[30]toencodesituationsinla-
4202
naJ
92
]IA.sc[
1v89361.1042:viXratentspace.Thepre-trainedversionofthemodelusedinthisstudyis
availableattheofficialGitHubrepository[32]. Therepositoryfea-
turesthreefoundationmodels,namely1x,2xand3x. Thebackbone
ofthethreemodelsisthesame,andtheydifferonlyfortheweights
width[32].
Fig. 2. Our approach. (A) Latent space generation: trajectories
areextracted from thedemonstration dataset. Frames areencoded
throughaprovidedVPTmodel,andpairedwiththecorresponding
actions. (B) Evaluation loop: at each time-step, the new observa-
Fig.1. AschemeoftheVPTmodelusedinthisstudy. Animage
tionisforwardedtothesameVPTmodel. Then,L1distanceacross
input is encoded with an IMPALA CNN and passed through four
currentandreferenceembeddingsiscomputedandthemostsimilar
transformerheads. Then,twoMLPheadspredictakeyboardanda
situationisfound.ZIPactsintheenvironmentfollowingtheactions
mouseactionrespectively.
oftheselectedreferencesituation.
2.1. Zero-shotImitationPolicy
ZIP solves a control problem by retrieving relevant past expe-
riences from experts’ demonstrations. In this context, we de-
fine a situation as a set of consecutive observation-action pairs
{(o ,a ),...,(o ,a )},τ ∈N.
t t t+τ t+τ
WeillustrateourapproachinFigure2.WeuseVPTtoextractall
theembeddingsfromanarbitrarilychosensubsetS oftheavailable
demonstrationsdatasetD,S ⊆D. S constitutesthed-dimensional
latentspacethatZIPsearches.Moreover,weassumethatexperthas
Fig.3.Anexampleofthesearchmechanism.Ateachtime-step,we
completedthegiventask(findcave,buildwaterfall,etc),sooptimal-
keeptrackofthedistancebetweencurrentandreferenceembedding.
ityassumptioninILissatisfied[19].
Whenever the distance overcomes a threshold, a divergence-based
Duringtesting(seeFigure2B),wepassthecurrentobservation
search (red line) selects a new reference embedding; if the agent
through VPT. Then, ZIP selects the most similar reference trajec-
followsathresholdfortoolong, atime-basedsearch(blueline)is
tory embedding in the latent space, according to their L1 distance
triggered. For each segment of the episode a yellow, dashed line
withthecurrentobservation. Finally,itcopiestheactionsofthese-
indicatesthevalueofthereferencedistance.Abrowndiamondcor-
lectedreferencesituation.Ateachtime-step,weshiftthecurrentand
respondingtoeachredlineshowsthedistancevaluethattriggered
referencesituationsintimeandrecomputetheirdistance.Whensit-
thenewsearch.
uationsdivergeovertime,theapproachperformsanewdivergence-
triggeredsearch(redlinesinFigure3).Additionally,ifthereference
BC 1x
embeddingisfollowedformorethanntime-steps,ZIPperformsa
BC 2x
newtime-triggeredsearch(bluelinesinFigure3). BC 3x
GAIL
ZIP
3. EXPERIMENTS (Ours)
0 30 60 90 120150180210240270300330360
Time (min)
The full FindCave dataset from the MineRL BASALT competi-
tion [28] consists of 5466 experts’ trajectories demonstrating the Fig.4. Timeneededtotraineachagenton100trajectoriesonthe
task, or around 150GB of data. For each frame of an episode, FindCave task. In the case of BC models, the training procedure
onlytheunprocessedRGBimageandthecorrespondingactionare consistsoffine-tuningapre-trainedVPTmodel. ForZIP,training
available. That is, an episode is a set of image-action pairs. No meansencodingasubsetoftrajectoriesthroughthereferenceversion
rewardsignalisavailable. Similarly,nomeasureofperformanceis ofVPT.AllmodelshavebeentrainedonasingleTeslaT4GPU.
provided. Inourstudyweconsideredonlythefirst100trajectories,
asmallfractionoftheavailabledata.
For our comparison, we fine-tuned the three VPT-based, pre- settingupZIP(32minutes)requiresmuchlesstimethanfine-tuning
trained foundation models introduced in Section 2. Additionally, anyVPT-basedBCagent(respectively,108,135and212minutes),
wecompareZIPtoGAIL[21],astate-of-the-artILalgorithm. We ortrainingGAIL(347minutes).
train GAIL from scratch until convergence for almost 6 hours on Weperformtwosetsofexperiments. First, weanalysethere-
thesamesubsetofdata,usingVPTasencoder. WhileZIPdoesnot sultsoftheperceptualevaluationfromBASALT[29],wherehuman
requireanytraining,werefertotrainingastheprocessofencoding contractorsgradepairsofvideos.Second,weapproximateaquanti-
theexperts’trajectoriesthroughVPT(seeFigure2A).Acomparison tativeevaluationfortheFindCavetaskfromtheBASALTsuite. We
oftrainingtimesforourtestedagentsisshowninFigure4.Notably, decidedtoevaluateonlyFindCavebecause,despitetheothertasks,
tnegAwefoundasufficientlyreliablewayofestablishingagroundtruth
Table 1. Top-5 ranking of the NeurIPS BASALT 2022 competi-
for its completion. To this end, we train a simple binary classifier
tion [29]. Below, the TrueSkill [33] scores for two human expert
todetectthepresenceofacave. Ourclassifieriscomposedbyfour
players,aBCbaseline,andarandomagent.
convolutional layers and two fully-connected layers. The network
hasbeentrainedonadatasetofcave/non-caveframes,manuallyex- Team Find Make Build Build Average
tracted from the original FindCave data, and achieved a validation Cave Waterfall Pen House
accuracyof97.89%overit.
GoUp 0.31 1.21 0.28 1.11 0.73
Wetestouragentsonthree-minutes-longepisodes,similarlyto
theofficialBASALTrules. Weapplyminimalchangestothetermi- ZIP 0.56 -0.10 0.02 0.04 0.13
nalconditionofanepisode,toeasetheevaluationprocedurewhile voggite 0.21 0.43 -0.20 -0.18 0.06
keepingtheperformanceevaluationintact. First,wedisabletheter- JustATry -0.31 -0.02 -0.15 -0.14 -0.15
minalaction’ESC’inallagents. Wesupportthisdecisionbyhigh- TheRealMiners 0.07 -0.03 -0.28 -0.38 -0.16
lighting that terminal actions constitute a minimal fraction of the
Human2 2.52 2.42 2.46 2.34 2.43
trainingexamples,andaBCagentwouldlikelyignoreit[11]. Sec-
Human1 1.94 1.94 2.52 2.28 2.17
ond,theBASALTcompetitionconsidersanepisodetobesuccessful
wheneveranagentperformstheterminalactionwhilebeinginsidea BC-Baseline -0.43 -0.23 -0.19 -0.42 -0.32
cave.Instead,weconsideranepisodesuccessfulwheneveranagent
Random -1.80 -1.29 -1.14 -1.16 -1.35
spendsmorethanfiveseconds(thatis,100consecutivecaveframes)
inacave.Wejustifythischoicebyconsideringthatatimethreshold
BC 1x
keeps the original evaluation criterion intact, while accounting for
BC 2x falsepositivessuchasrandomcavesightings.
BC 3x
As in the evaluation process of the BASALT competition, we
testouragentontwentyseedvalues. Sincetheofficialvaluesused GAIL
ZIP
inBASALTevaluationarenotpubliclyavailable,wehaveselected
(Ours)
eachseedmanuallytoensurethepresenceofcaves.Werepeatthree 0 10 20 30 40 50
runsoveroursetofseeds. Success Rate (%)
Ourapproachreliesontwoparameters,maximumstepsanddi-
vergencescalingfactor,regulatingthefrequenciesofthetwotypes Fig.5.AveragesuccessratefortestedmodelsontheFindCavetask.
ofsearch. Maximumstepsregulatesthemaximumnumberofcon- Each agent has been evaluated on a batch of 20 seeds. Each run
secutiveactionsthatanagentcanusefromthesametrajectory,be- hasbeenrepeatedthreetimes. Baselinemodelishighlightedwitha
fore triggering a new time-based search. On the other hand, di- verticalblueline.
vergence scaling factor determines when a new divergence-based OurproposedZIPagentwasoverallrankedsecondplaceinthe
searchistriggered,basedonhowmuchthedistancebetweencurrent challenge.TheresultsofouragentaredescribedinTable1.Thefirst
andreferenceembeddingshasincreasedcomparedtothelastsearch. placewasawardedtoteamGoUp,wholeverageddetectionmethods
Fortheperceptualexperiment, wehavesetmaximumsteps = 128 andhumanknowledgeofthetaskandcombinedthemwithscript-
anddivergencescalingfactor=2.0. Weperformanablationstudy ing [29]. Notably, all the other learning-based methods achieved
overthesehyperparameters,selectingninevaluesforeach,centered lowerperformancethanZIPinthreeoutoffourtasks.Additionally,
aroundourreferencevalue. Wetestouragentforthreerunsoften ourmethodwasawardedwith2outof5researchinnovationprizes
episodeseach,usingafixedseed. fromtheorganizingcommittee.
Finally,wevisualiseandanalysethelatentspacegeneratedfrom
VPTusingat-SNEplot.Forvisualclarity,weencodeonly10ofthe
4.2. Quantitativeresults
100trajectoriesusedinourexperiments. Wedifferentiatetrajecto-
riesbymarkingtheirpointswithdifferentcolors. Additionally,we We report success rate for the tested models on the FindCave task
distinguishexplorationframesfromcaveframes,andanalysetheir in Figure 5. ZIP has obtained the best performance, being able to
distributionthroughthespace. completethetask43.32±4.71%ofthetimes. Notably, itsworst
performance on our repeated trials has reported a 38.59% success
rate,comparabletothebestrunamongalltheBCagents(38.88%,
4. RESULTS BC3x).
GAILwasneverabletocompletethetask. Wehavecompared
We report both the perceptual and quantitative evaluations for our oursetupwith[21]toaccountforerrorsinthehyperparametersval-
agent. Notably, perceptual results have been obtained by asking ues.WehavetrainedGAILfor6,12,18and22hours.Wehaveob-
anonymoushumancontractorstocomparerandomlyselectedpairs servedasaturationofpolicyanddiscriminatorlossesafter6hours.
of agents. For each comparison, agents have been evaluated on Bywatchingsomevideosofthetrainedpolicyplayingthegame,we
human-likelinessandsuccess.
noticedanimprovementoverarandomagent.Still,thetrainedagent
haddifficultiesincompletingbasicactionsconsistently.
4.1. Perceptualevaluation Onthecontrary,VPT-basedBCmodelshavebeenpre-trainedon
hugeamountofdatascrapedfromtheinternet. Asaconsequence,
The organization committee of the competition ranked the agents fine-tuningthemledtoquitesuccessfulperformance. Morespecif-
using the TrueSkill [33] ranking system, which is widely used in ically,BC1xsucceeded(onaverage)28.33±10.27%ofthetime,
theMicrosoftonlinegamingecosystem.Givenasetofcompetitors, while BC 2x reached 33.4±4.71% of success rate. Perhaps sur-
thesystemusesBayesianinferencetocomputeanELO-likescore, prisingly,BC3xcompletedthetaskonly23.5±15.46%oftimes.
accordingtothematchhistoryofeachcompetitor. WeexplainthisresultbyobservingthatourtestedBCagentshave
tnegAFig. 6. Ablation study over the hyperparameters of our proposed
method. (A) Maximum number of time-steps following the same
trajectory.X-axis:valuesofthetime-stepsthreshold;(B)Divergence
scalingfactor. X-axis: divergencefactorvalues. Forvisualclarity,
onlyaverageperformanceoftheBCmodelsisreported.
Fig. 7. Example of latent space generated by VPT. The example
generallyhighervariancethanZIP.Thus,webelievethatBC3xonly
showsonly10outof100trajectoriesforvisualclarity. Pointsbe-
sufferedfrompoorchoicesofactions.
longingtothesametrajectoryaremarkedwiththesamecolor. We
highlightcaveframesusingastar-shapedmarker.
4.3. Ablationstudy
ginsofthespaceareformedbypointsofoneortwodemonstrations.
In Figure 6 we report the results of the ablation study conducted
Thosesmallerclustersrepresentpeculiarandraresituationsthathap-
overthehyperparametersofZIP.Whilewewerenotabletoidentify
penedonlyonceortwice,forinstance,spawninginadesertarea.
aprecisepattern,itisclearthatsomevaluesyieldbetterresultsthan
others. Forinstance,maximumsteps(Figure6A)seemstoimprove Figure 7 also shows that cave frames are concentrated on the
ZIP’sperformancewhenitsvalueisbetween32and128. Thisre- outskirtsofthespace, mostlyontherightside. Interestingly, cave
sultsconfirmswhatwehavefoundempirically,thatis,128isagood frames belonging to one trajectory tend to be close to each other,
candidate. whilebeingwellseparatedfromcaveframesofanothertrajectory.
Wesuggestthatthedistributionofthoseframesmightallowformore
Asfordivergencescalingfactor,itappearsthateither1.0,2.25
refinedstrategiesrelatedtogoalconditioning,ortonavigatethela-
and3.0aregoodchoices. Inparticular,1.0yieldsnovariance,sug-
tentspace.
gestingveryconsistentperformance. Inourreferenceimplementa-
tion,wehaveusedavalueof2.0,whichdoesnotseemtobecom-
petitivewithothervalues.
Following the results of the ablation study, we ran again the
5. CONCLUSIONS
quantitativeexperiment,changingonlythehyperparametersvalues
tomaximumsteps = 32anddivergencescalingfactor = 1.0. We
foundthatZIPreplicatestheresultsobtainedbyourreferencesetup We present Zero-shot Imitation Policy (ZIP), a search-based ap-
almostperfectly(43.3±6.24%). Wejustifythisresultbyconsid- proach to behavioral cloning that efficiently adapts to new tasks,
ering that the ablation study has been conducted on a fixed seed, leveragingpretrainedmodelsincomplexdomains.Ourexperiments
while the quantitative study uses variable seed values. Therefore, showthatZIPisarobustalternativetoimitationlearningmethods,
in a complex environment such as Minecraft, changing conditions whilerequiringveryshortamountoftimetobesetup.
canleadtosubstantiallydifferentresults. Nonetheless,itisnotable ZIPismostlylimitedbythesizeofthelatentspaceandthequal-
how our agent was able to keep the same performance despite the ityofdataused. Futureworkscouldexploretheusageofexternal
changes. toolsforbetterdatacompression[34]ormeasuresforrankingavail-
abledataaccordingtotheirrelevance.
4.4. Latentspacevisualisation
InFigure7weshowanexampleoflatentspacegeneratedthrough
6. ACKNOWLEDGEMENTS
theVPTencoding. Eachpointrepresentoneframebelongingtoa
specific trajectory, separated by color. Star-shaped points refer to
caveframesofaspecifictrajectory. We thank Research Council Finland (project number 350093) and
Clustersofpointsareclearlydistinguished. Ingeneral,clusters Detuscher Akademischer Austauschdienst (DAAD) for supporting
are heterogeneous, even though some smaller clusters at the mar- thecollaborationbetweenourgroups.7. REFERENCES [18] PimdeHaan,DineshJayaraman,andSergeyLevine, “Causal
confusioninimitationlearning,” 2019,vol.32.
[1] StefanSchaal,“Learningfromdemonstration,”inAdvancesin
[19] StuartRussell, HumanCompatible, Penguin,2019.
NeuralInformationProcessingSystems,M.C.Mozer,M.Jor-
dan,andT.Petsche,Eds.1996,vol.9,MITPress. [20] AndrewY.NgandStuartJ.Russell, “Algorithmsforinverse
reinforcementlearning,” inProceedingsoftheSeventeenthIn-
[2] RichardS.SuttonandAndrewG.Barto,ReinforcementLearn-
ternationalConferenceonMachineLearning,SanFrancisco,
ing:AnIntroduction, TheMITPress,secondedition,2018.
CA, USA, 2000, ICML ’00, p. 663–670, Morgan Kaufmann
[3] MalteSchillingandal., “Anapproachtohierarchicaldeepre- PublishersInc.
inforcement learning for a decentralized walking control ar-
[21] JonathanHoandal., “Generativeadversarialimitationlearn-
chitecture,” in Biologically Inspired Cognitive Architectures
ing,”2016.
2018: ProceedingsoftheNinthAnnualMeetingoftheBICA
Society.Springer,2019,pp.272–282. [22] Saurabh Arora and al., “A survey of inverse reinforcement
learning:Challenges,methodsandprogress,”2020.
[4] NicolasBachandal., “Learntomovethroughacombination
ofpolicygradientalgorithms: Ddpg,d4pg,andtd3,” inInter- [23] Stephen Adams and al., “A survey of inverse reinforcement
nationalConferenceonMachineLearning,Optimization,and learning,” Artif.Intell.Rev.,vol.55,no.6,pp.4307–4346,aug
DataScience.Springer,2020,pp.631–644. 2022.
[5] Malte Schilling and al., “Decentralized control and local in- [24] W.S.ClevelandandS.J.Devlin,“Locallyweightedregression:
formationforrobustandadaptivedecentralizeddeepreinforce- Anapproachtoregressionanalysisbylocalfitting,” Journal
mentlearning,”NeuralNetworks,vol.144,pp.699–725,2021. of the American Statistical Association, vol. 83, no. 403, pp.
596–610,1988.
[6] Long Ouyang and al., “Training language models to follow
instructionswithhumanfeedback,” AdvancesinNeuralInfor- [25] JyothishPari and al., “The surprising effectivenessof repre-
mationProcessingSystems,vol.35,pp.27730–27744,2022. sentationlearningforvisualimitation,”2021.
[7] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and [26] SiddhantHaldarandal., “Teacharobottofish: Versatileimi-
Sheila McIlraith, “Steve-1: A generative model for text-to- tationfromoneminuteofdemonstrations,”2023.
behaviorinminecraft,”2023. [27] Shikhar Bahl and al., “Affordances from human videos as a
versatilerepresentationforrobotics,”2023.
[8] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,
ChaoweiXiao,YukeZhu,LinxiFan,andAnimaAnandkumar, [28] Rohin Shah and al., “The minerl BASALT competition on
“Voyager:Anopen-endedembodiedagentwithlargelanguage learningfromhumanfeedback,” CoRR,vol.abs/2107.01969,
models,”2023. 2021.
[9] MaximeOquabandal., “Dinov2: Learningrobustvisualfea- [29] StephanieMilaniandal., “Towardssolvingfuzzytaskswith
tureswithoutsupervision,” arXivpreprintarXiv:2304.07193, human feedback: A retrospective of the minerl basalt 2022
2023. competition,”2023.
[10] Krishan Rana and al., “Contrastive language, action, [30] Bowen Baker and al., “Video pretraining (vpt): Learning to
and state pre-training for robot learning,” arXiv preprint actbywatchingunlabeledonlinevideos,” AdvancesinNeu-
arXiv:2304.10782,2023. ralInformationProcessingSystems,vol.35,pp.24639–24654,
2022.
[11] FarazTorabiandal.,“Behavioralcloningfromobservation,”in
IJCAIInternationalJointConferenceonArtificialIntelligence, [31] LasseEspeholtandal.,“IMPALA:scalabledistributeddeep-rl
2018,vol.2018-July. withimportanceweightedactor-learnerarchitectures,” CoRR,
vol.abs/1802.01561,2018.
[12] Saumya Kumaar Saksena and al., “Towards behavioural
cloningforautonomousdriving,” 2019. [32] “Video-pre-training,” https://github.com/openai/
Video-Pre-Training/tree/main/lib, Accessed:
[13] Tanmay Vilas Samak and al., “Robust behavioral cloning
2023-08-16.
forautonomousvehiclesusingend-to-endimitationlearning,”
SAEInternationalJournalofConnectedandAutomatedVehi- [33] TomMinkaandal., “Trueskill2: Animprovedbayesianskill
cles,vol.4,2021. ratingsystem,”Tech.Rep.MSR-TR-2018-8,Microsoft,March
2018.
[14] Shivansh Beohar and al., “Planning with rl and episodic-
memorybehavioralpriors,” arXivpreprintarXiv:2207.01845, [34] “Faiss: a library for efficient similarity search and clustering
2022. of dense vectors,” https://ai.meta.com/tools/faiss/, Accessed:
06-09-2023.
[15] AnssiKanervistoandal., “Playingminecraftwithbehavioural
cloning,” CoRR,vol.abs/2005.03374,2020.
[16] Anssi Kanervisto and al., “Benchmarking End-to-End Be-
haviouralCloningonVideoGames,” inIEEEConferenceon
ComputatonalIntelligenceandGames,CIG,2020,vol.2020-
August.
[17] OriolVinyalsVinyalsandal., “Grandmasterlevelinstarcraft
iiusingmulti-agentreinforcementlearning,” Nature,vol.575,
2019.