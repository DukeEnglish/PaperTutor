Computer Vision for Primate Behavior Analysis in the Wild
RichardVogg*1,TimoLu¨ddecke*1,JonathanHenrich*2,SharmitaDey1,MatthiasNuske3,
ValentinHassler1,DerekMurphy4,5,JuliaFischer4,5,6,7,JuliaOstner6,8,9,OliverSchu¨lke6,8,9,
PeterM.Kappeler6,10,11,ClaudiaFichtel6,10,AlexanderGail6,7,12,13,StefanTreue6,7,12,13,
Hansjo¨rgScherberger6,7,14,15,FlorentinWo¨rgo¨tter3,6,7,AlexanderS.Ecker1,6,7,16
*equalcontribution
1InstituteofComputerScienceandCampusInstituteDataScience,UniversityofGo¨ttingen
2ChairsofStatisticsandEconometrics,FacultyofEconomics,UniversityofGo¨ttingen
3DepartmentforComputationalNeuroscience,ThirdPhysicsInstitute,UniversityofGo¨ttingen
4CognitiveEthologyLaboratory,GermanPrimateCenter,LeibnizInstituteforPrimateResearch,Go¨ttingen
5DepartmentforPrimateCognition,Johann-Friedrich-BlumenbachInstitute,UniversityofGo¨ttingen
6LeibnizScienceCampus,GermanPrimateCenter,LeibnizInstituteforPrimateResearch,Go¨ttingen
7BernsteinCenterforComputationalNeuroscience,UniversityofGo¨ttingen
8BehavioralEcologyDepartment,UniversityofGo¨ttingen
9SocialEvolutioninPrimatesGroup,GermanPrimateCenter,LeibnizInstituteforPrimateResearch,Go¨ttingen
10BehavioralEcology&SociobiologyUnit,GermanPrimateCenter,LeibnizInstituteforPrimateResearch,Go¨ttingen
11DepartmentofSociobiology/Anthropology,UniversityofGo¨ttingen
12CognitiveNeuroscienceLaboratory,GermanPrimateCenter,LeibnizInstituteforPrimateResearch,Go¨ttingen
13Georg-Elias-Mu¨ller-InstituteofPsychology,UniversityofGo¨ttingen
14FacultyofBiologyandPsychology,UniversityofGo¨ttingen
15NeurobiologyLaboratory,GermanPrimateCenter,LeibnizInstituteforPrimateResearch,Go¨ttingen
16MaxPlanckInstituteforDynamicsandSelf-Organization,Go¨ttingen
Abstract
Advancesincomputervisionaswellasincreasinglywidespreadvideo-basedbehavioralmonitoringhave
greatpotentialfortransforminghowwestudyanimalcognitionandbehavior. However,thereisstillafairly
largegapbetweentheexcitingprospectsandwhatcanactuallybeachievedinpracticetoday, especiallyin
videosfromthewild.Withthisperspectivepaper,wewanttocontributetowardsclosingthisgap,byguiding
behavioralscientistsinwhatcanbeexpectedfromcurrentmethodsandsteeringcomputervisionresearchers
towards problems that are relevant to advance research in animal behavior. We start with a survey of the
state-of-the-artmethodsforcomputervisionproblemsthataredirectlyrelevanttothevideo-basedstudyof
animalbehavior,includingobjectdetection,multi-individualtracking,(inter)actionrecognitionandindividual
identification.Wethenreviewmethodsforeffort-efficientlearning,whichisoneofthebiggestchallengesfrom
apracticalperspective. Finally, weclosewithanoutlookintothefutureoftheemergingfieldofcomputer
visionforanimalbehavior,wherewearguethatthefieldshouldmovefastbeyondthecommonframe-by-frame
processingandtreatvideoasafirst-classcitizen.
1 Introduction
Monitoringthebehaviorofanimalsisanimportantresearchtaskinmultiplescientificdisciplinesrangingfrom
behavioral ecology to neuroscience. While behavior is a wide-ranging term, including swarm behavior or
movementpatterns[1], andoftenstudiedinlaboratories, wewanttofocushereonindividualizedactionsand
interactionsrecordedinimagesorvideosinfree-ranginganimals, ofteninthewild. Traditionally, thecollec-
tionofbehavioraldatahasbeentime-consuming,laborious,orinvasive,asithasreliedonhumanobservation
and manual documentation or equipping animals with biologgers or tracking devices [2]. In the last decade,
the situation has changed as powerful computer vision methods have emerged that have enabled researchers
toautomatemanyimagerecognitiontasks. Thesedevelopmentshaveputfullyautomatedbehavioraltracking
andanalysisofanimalbehavioronthehorizon. Automatedannotationsusingcomputervisioncomplementthe
work of human observers and fundamentally change how data on animal behavior are collected, with impor-
tantconsequencesforresearchers: Unprecedentedamountsofdatacanbeprocessedandanalyzed,something
1
4202
naJ
92
]VC.sc[
1v42461.1042:viXrathatwasinfeasibletobedonemanually;potentialbiasesintroducedbyresearcherswhoknowtheresearchhy-
pothesiscouldbecircumvented; evenforsmallerquantitiesofvideosresearcherscanfocusonanalyzingand
interpretingratherthanspendingweeksormonthsonmanualscoringofvideos.
Oneoftheareaswherecomputervisionmethodshavealreadybeenadoptedatscaleismonitoringanimal
behavior in laboratory settings [3, 4, 5, 6]. Open-source pose estimation frameworks such as DeepLabCut
[7],SLEAP[8]andothers[9,10]havebeenadoptedbyalargeusercommunityinneuroscienceandbeyond,
andhavefacilitatedanongoingparadigmshifttowardsmoreunrestrainedandnaturalbehaviorinneuroscience
[11]. These tools work remarkably well in a laboratory setting, where the visual complexity of the scene is
manageable. However,thesituationismorechallenginginmorenaturaloutdoorenvironmentsorinthewild,
wherecomplexvisualscenes,changinglightingconditions,non-stationarycameras,occlusionandclutterdue
tolargenumbersofanimalsposesignificantchallengestothecomputervisionmachinery[12]. Moreover,the
behaviors that behavioral ecologists are interested in are often complex, not easily deduced from an animal’s
pose on a single video frame and often concern interactions between two or more individuals, or between
individuals and objects in their environment. The following examples from our own ongoing research work
illustraterepresentativetypesofproblemsinwhichcomputervisionmethodscanassistbehavioralecologists:
First, the identification of individual monkeys from a collection of camera trap images. The co-occurrences
of monkeys reveal insights into the social networks of the group of monkeys. Second, tracking of multiple
monkeys throughout a set of hand-recordedvideos and identification of their actions, e.g. resting, grooming,
sleeping. Third,theautomatedanalysisofsociallearningbyidentifyinginteractionsbetweenindividualswho
performcertaintasksandobserveeachother.
Eventhoughtherearesomefirstsuccessfulexamplesofhowtousecomputervisiontechniquestodetect,
trackandidentifyindividualmonkeysinthewild[13,12,14],theydonothavethesamematurityandgeneral
usability as models developed for laboratory settings. Similarly, the automated recognition of actions and
interactionswithhighaccuracyfromvideorecordingsinnaturalenvironmentsusingcomputervisionmethods
is still a challenging and unsolved problem. Yet, these settings are what we are particularly interested in.
Makinguseofcurrentlyexistingcomputervisionmethodswillbringusclosertosolvingtheseproblems, but
it will likely require further advances in machine learning and computer vision. Therefore, our aim with this
perspective is not only to highlight and discuss current developments in computer vision but also to sketch a
pathtowardsfuturedevelopments.
Weidentifyfourkeytasksonthepathtowardsautomatedbehavioranalysisfromimagesandvideos. These
taskscanbeunifiedtoobtainaholisticcharacterizationofanimalbehavior(Fig.1).
• Animaldetection: Tounderstandthebehaviorofindividualanimals,theseanimalsmustfirstbelocated
inimages.
• Multi-animal tracking: For videos, detections of the same individual across multiple frames must be
associatedwitheachothertotrackanindividualovertime.
• Individualidentification: Manyresearchquestionsrequireidentifyingindividualsbasedonface,bodyor
otherfeatureswhichhelptodistinguishthem.
• Actionunderstanding:Classifyingbehaviorsbasedonobservablepatternsofactionsisthecoreofbehav-
ioralanalysis.Tasksofinterestincludetheassignmentofactionsorbehaviorstoindividualsandtemporal
intervals,aswellasthedetectionofinteractionsbetweenindividualsorbetweenindividualsandobjects.
Althoughtherearemanymethodstoaccomplishthesefourtasksusingcomputervisionmethods,theyoften
requirelargeamountsoflabeleddata,whichlimitstheirpracticaluse. Therefore,anotherimportantquestionis
howtosolvethetasksinthefaceoflimitedtimeandresources. Wereviewcurrentideasaroundhowtowork
withcompletelyorpartiallyunlabeleddata,labelsthatarefastertoannotate,andhowtofindthebestsamples
forannotationtoimprovethemodeloptimallygivenacertainlabelingbudget.Basedonouranalysisofthestate
of the art in the key tasks and effort-efficient methods, we highlight promising future directions for computer
visioninanimalbehaviorandidentifypotentiallyhigh-impactdevelopments.
Withthispaper,weaimtoreviewandstructurethefieldofcomputervisionforanimalbehaviorresearchers
with a technical interest. While we write primarily from the perspective of scientists interested in primate
behavior in natural environments, many of the discussed principles are also applicable to other taxonomic
groups and settings. The paper is directed at both AI researchers and empirical scientists, aiming to make
significant contributions in two ways: (1) provide guidance to the deep learning community on critical open
problems with practical applications and (2) show biologists and ecologists a path towards transforming their
researchbyAI.
2videosequenceinput withboundingboxes behavioralanalysistasks
loca�on
jump
lookat
... ...
eat eat
�me
ac�on interac�on iden�ty tracks
Figure1: Structureofbehavioranalysistasks: Startingfromavideotakeninthewild,thegoalistoobtaina
structuredrepresentationofactionsandinteractionsbetweenthedepictedanimalswhilekeepingtrackoftheir
identities. The behavior analysis output on the right shows the behavior of two individuals (blue and violet)
overtime.
Wereviewmachinelearningmethodsforthekeytasksofbehavioralanalysis(Section2)andeffort-efficient
learning(Section3)inawaythatisaccessibletoresearchersfrombothfields. Amoredetailedtechnicalreview
of these methods can be found in Appendix B and C, while Appendix A introduces some computer vision
foundations. OuroutlookonpromisingresearchdirectionsisprovidedinSection4.
2 Methods for primate behavior analysis
In this section, we introduce the computer vision tasks that we expect to be crucial for a holistic analysis of
individualized primate behavior: animal detection (2.1), multi-animal tracking (2.2), individual identification
(2.3) and action understanding (2.4). For each of these tasks, we describe the expected inputs and outputs
and illustrate the key methods. While some methods operate on static images, others require video input. In
AppendixB,wereviewallthementionedmethodsinmoretechnicaldetail.
Sincethegoalistoanalyzethebehaviorofindividuals,alltasksintroducedinthissectionrequireacertain
wayofrepresentingthepresenceandpositionofobjectinstances(inourcase: individualanimals)appearingin
animageorvideoframe. Possibleinstancerepresentationsareboundingboxes, instancesegmentationmasks
andkeypoints.
Bounding boxes represent instances by the smallest possible rectangular box that encompasses all visible
parts of an animal in an image. Most commonly, axis-aligned bounding boxes are used where the sides of
the box are parallel to the image axes. Depending on the actual shape of the animal, axis-aligned bounding
boxes can also contain a lot of background but come with the advantage of being fast to annotate. There
are variations of this representation, such as amodal bounding boxes, which encompass even the parts of the
individualthatarenotvisible(e.g. becausetheyareoccluded). Instancesegmentationmasksindicateforeach
pixel of the image to which individual it belongs or whether it is part of the background. Labels for instance
segmentationcanbeverytime-consumingtoacquire, althoughrecentmethodsshowgreatpotentialtoreduce
this time [15]. A keypoint representation allows a more precise determination of the animal’s posture. For
each instance, a canonical set of relevant keypoints, such as nose, left hand or right knee is defined. The
annotation effort depends on the number of keypoints used, but is usually higher than for bounding boxes.
While many popular toolkits for animal behavior analysis are centered around keypoints [7, 16, 8], we argue
later (see discussion in Section 4) that bounding boxes are a more suitable intermediate representation for
building flexible, generalizable architectures. We will also see that most of the work on tracking and action
understanding is based on bounding boxes, which is why the following sections are mostly focused on such
methods.
2.1 Animaldetection
Given a single image, a natural goal for animal behavior researchers using computer vision is to detect all
animals of interest in the image. Having discussed different ways of representing the presence and position
of individual animals, this section deals with methods to automatically detect them. We focus on detection
methodsthatrepresentindividualsasboundingboxeswhicharecalledobjectdetectionmethods. Trainingan
objectdetectorusuallyrequiresimageswithgroundtruthlabelsofboundingboxes. Akeychallengeforobject
3detectionmethodsisthatthenumberofobjectscanvaryfromimagetoimage,aswedonotknowapriorihow
many animals there are in an image. Therefore, neural networks, which commonly have a fixed number of
parameters,mustbeequippedwithamechanismtogenerateavariablenumberofoutputboundingboxes. This
istypicallydonebypredictingalargenumberofboundingboxestogetherwithaconfidencescore,whileonly
boundingboxeswithasufficientlyhighconfidencearekept. Inthefollowing,weexplainthebasicprinciples
ofpopularobjectdetectionmethods.
Objectdetectionmethodscanbecategorizedintosingle-stageandtwo-stagemethods. Intwo-stagemeth-
ods, detection is separated into two steps: Region proposal, where a large number of potential object regions
are identified, and classification, where the regions are rated according to their plausibility of containing an
objectandclassifiedintoobjectcategories. Thisparadigmisemployedbythewell-knownR-CNNfamily[17,
18, 19, 20]. Two-stagemethodsperformverywellbutaregenerallyfairlycomplexpipelines. Insingle-stage
methods, boundingboxcoordinatesandconfidencescoresarepredictedwithoutin-betweensteps. Compared
to two-stage methods, single-stage methods are (1) faster (which is important for real-time applications), (2)
conceptuallysimpler,and(3)canbenaturallytrainedinanend-to-endway. NotableexamplesareSingle-Shot
Detector(SSD)[21],theYOLO-series[22,23,24]andheatmap-baseddetectors[25,26,27,28],wherebound-
ingboxesarepredictedwiththehelpofheatmapsforpre-definedobjectparts,suchascornersorcenters. The
methodsdescribedsofarareallbasedonConvolutionalNeuralNetworks. Morerecently, single-stageobject
detectionmethodsbasedonthetransformernetworkarchitecture[29]havegainedpopularity. Inthisparadigm,
objects are represented by so-called object queries, i.e. numerical vectors, that extract information from the
inputimagethroughatransformernetwork. Theseobjectqueriesarethentransformedintoboundingboxesand
confidencescorestoarriveatobjectdetections.Notableexamplesfollowingthisparadigmincludethedetection
transformer(DETR)[30]anditsfollow-upworks[31,32,33,34].
2.2 Multi-animaltracking
Sofar,wehavediscussedhowobjectdetectionallowsustolocateandclassifyobjectsofinterestinstillimages.
However,researchersstudyinganimalbehaviorfrequentlyrecordvideofootageofbehavior. Inthiscase,they
needamodelthatwillnotonlyfindallanimalsinagivenvideoframebutalsokeeptrackofthemovertime.
Thistrackingiscrucialforobservinglong-termbehavioralpatternsandtheassignmentofinstancesandentire
sequencesofactionstothesameanimalovertime.
The input for a tracking model is always a video, typically broken down into single frames in a temporal
sequence.Theoutputisalistofdetections(mostcommonlyboundingboxes)andtrackIDsthatlinkthedetected
instances across video frames. A good tracking algorithm will be able to follow an individual through the
sequenceandassignthesameidentitytoeachdetectionofthatindividual. Whenthedetectionsarerepresented
asboundingboxes,thismethodisreferredtoasMultipleObjectTracking(MOT).Althoughtrackingalgorithms
based on other instance representations exist (e.g. Video Instance Segmentation or keypoint-based tracking),
theyoftensolveMOTasafirststeporhavetosolveitimplicitly. ForthisreasonandbecauseMOTisthemost
widelystudiedandbest-understoodparadigm,wewillfocusonithere.
Thetrackingprocedurehassofarusuallybeenpartitionedintotwophases[35]: detectionandassociation.
Inthedetectionphase,theprimaryaimistoidentifyandlocateobjectsofinterestwithinframes,asdescribed
above. Theassociationphaselinksthedetectedobjectsacrossframestoproducetrajectoriesovertimeforeach
object. Thisisaccomplishedthroughthedetectedlocationsandtheextractionofdescriptivefeatures, suchas
motion and appearance, from the detected objects. The motion features estimate the position of an object in
future frames, while the appearance features represent its visual characteristics. After feature extraction, an
affinitycostiscalculated. Theaffinitycostmeasuresthepairwisesimilaritybetweenthedetectedobjectsacross
framesbasedontheirlocation,motionandappearance.Itisthenusedtolinkthedetectedobjectsacrosssucces-
siveframestogenerateobjecttrajectoriesovertime.ExistingMOTmethodscanbecategorizedintotwogroups:
(1)Theclassicaltracking-by-detectionapproachdescribedabove,wherethedetectionandassociationphaseare
independent of each other, and (2) more recent tracking-by-query methods with interdependent detection and
associationphases.
Aprimeexampleoftheclassicaltracking-by-detectionframeworkistheSimpleOnlineReal-timeTracker
(SORT)model[36].IntheSORTmodel,thedetectionsofeachframearecollectedindependentlyofeachother.
Thespatialshiftbetweendetectionsofsuccessiveframesisthenusedtoextractmotionfeatures,whichareused
tocalculatetheexpectedlocationofobjectsonthenextframe. Detectionsarethenlinkedacrossframesbased
ontheoverlapbetweentheexpectedlocationofobjectsandtheactualdetections. Follow-upworksimproved
the SORT model, for example by additionally including appearance features [37, 38, 39] or employing more
elaborateassociationstrategies[37,40].
4More recently, frameworkshave emerged that follow thetracking-by-query paradigmwhere thedetection
and association phases are interlinked, rather than independent. In this setup, the results of the association
phase, suchasmotionorappearancefeatures, helpimprovetheresultsofthedetectionphase, andviceversa.
Themostprominentexampleofthisparadigmisthemultiple-objecttrackingwithtransformer(MOTR)model
[41] and its follow-up works [42, 43, 44]. MOTR uses the DETR detection framework described above and
introduces the concept of track queries in addition to object queries. While object queries are used to detect
newly emerging objects, as is done in DETR, track queries represent tracked objects in terms of their motion
andappearancefeaturesandareusedtoproducetracksofobjects. Objectandtrackqueriesinteractwitheach
otherthroughthetransformerdesignwhichenablesinformationexchangebetweendetectionandassociation.
2.3 Individualidentification
Individual identification is an essential step for most studies on animal behavior and ecology, for example,
instudiesofsociality[45,6]orforpopulationestimation[46]. Itdescribesthetaskofdistinguishingbetween
individualsinananimalgroupbyconsistentlyassigninguniqueidentitylabelsacrossdifferentimagesorvideos.
Thelabelistypicallyapre-assignedidentity,e.g. anameorIDcode. Individualidentificationisdifferentfrom
multi-objecttracking,whereindividualsareassignedanewlabelforeachvideo/sceneinwhichtheyappear.
Therearetwomainapproachestoindividualidentification: closedset andopenset identification[47]. In
closed set identification, all individuals are known beforehand and need only be recognized again in new im-
ages,whileinopensetidentificationpreviouslyunseenindividuals(e.g. fromunhabituatedgroups)canappear
inthedataandmustbedistinguishedfromknownindividuals. Closedsetidentificationisaclassificationtask
and has been traditionally tackled with standard methods like convolutional neural networks (CNNs) with a
classificationlayerthatareappliedonimagesofthefullbody[48,49,9]orface[50]oftheanimals. Alterna-
tively,Bainetal.[51]classifywhichanimalsarepresentinaframewithoutlocalizingthem.Incontrasttothese
classicalapproaches, morerecentlytherehasbeenevidencethatdeepmetriclearningfortheidentificationof
animals outperforms classification approaches [47]. In deep metric learning, the goal is to learn features that
canbeusedtodistinguishindividualsfromeachother. Commonlyusedapproachesforlearningdiscriminative
featuresaretripletloss[52]andcontrastivelearningmethods[53,54]. Oneadvantageofdeepmetriclearning
is that unknown individuals can be identified based on feature distance. More precisely, when the calculated
featureofanewlydetectedindividualishighlydissimilartothefeaturesassociatedwithknownindividuals,this
individual can be classified as unknown. Therefore, deep metric learning is a promising avenue to tackle the
challengingtaskofopensetindividualidentification. Foranexhaustiveoverviewofcurrentopensetmethods,
werefertothereviewofSunetal. [55].
2.4 Actionunderstanding
Theanimallocalizationmethodsdiscussedintheprevioussections(detectionandtracking)arecrucialprereq-
uisites. Nowwefocusonmethodsforactionunderstanding. Thesemethodsdiffermainlyinwhetherandhow
theylocalizeactionsinspaceandtime(Fig. 2). Forvideoinputs,theycanbedividedintofourmaincategories:
(1)actionrecognition(alsoknownasactionclassification),(2)temporalactiondetection(alsoknownastempo-
ralactionlocalization),(3)spatio-temporalactiondetection(alsoknownasspatio-temporalactionlocalization)
and(4)dynamicscenegraphgeneration.Toexemplifywhatthesefourmethodsdo,supposethatthereisavideo
ofamonkeyfeedingfromabowl. Anactionrecognitionmethodwouldassignthelabel“feeding”totheentire
video. Amorepreciseoutputcanbeobtainedusingatemporalactiondetectionmethodwhichwouldidentify
thetemporalintervalduringwhichthemonkeyisfeeding. Bothofthesemethodsgeneratevideo-leveloutput
thatdoesnotrefertoindividualanimals. Therefore,thesemethodsusuallydonotmakeuseofanimallocaliza-
tion. Incontrast,aspatio-temporalactiondetectionmethodwouldlocalizethemonkeyusingboundingboxes
on individual video frames (frame-based approach) or a track (track-based approach). The action “feeding”
would then be assigned to the respective bounding boxes (frame-based approach) or sub-tracks (track-based
approach) where the monkey feeds. Similarly, a dynamic scene graph generation method would localize the
monkey and the bowl by means of bounding boxes (frame-based approach) or tracks (track-based approach).
Theaction“feeding”wouldthenbeassignedtotherespectiveboundingboxes(frame-basedapproach)orsub-
tracks (track-based approach) in the form of a relationship triplet (monkey–feeding from–bowl). It should be
notedthatframe-basedapproachesstillleveragetemporalinformationfortheprediction. However,predictions
are made separately for each frame, which is why it is not possible to directly infer whether two actions on
differentframesbelongingtothesameanimalrepresentthesameactioninstanceorwhethertheactionhasbeen
5video/imagelevel instancelevel
ac�onrecogni�on ac�onlocaliza�on
ac�onrecogni�on scenegraphs
onimages onimages
singleimage
objectdetec�on
aacc��oonnrreeccooggnnii��oonn kkeeyy--ffrraammee--bbaasseedd key-frame-based
videoinput
singleoutput ssppaa��oo--tteemmppoorraall dynamicscene
aacc��oonnllooccaalliizzaa��oonn graphs
temporalac�on
trtarcakclke-tb-absaesded track-based
videoinput localiza�on
frame-wiseoutput
objecttracking
Figure2: Categoriesofactionandinteractionrecognitiontasksbasedoninputandoutputtemporalformatas
wellasspatialgranularity.
interrupted in between. To achieve this, a tracking method would have to be applied additionally. Figure 2
categorizestheactionunderstandingmethodswithregardtotheirinputandoutputtype.
Advancesinactionunderstandingmethodshavebeenlargelydrivenbythedevelopmentofmoreandmore
powerfulgeneralvideoprocessingbackbones. Whileearlybackbonesprimarilyreliedon3Dconvolutions[56,
57, 58], there has been a shift towards transformer networks [59, 60, 61, 62, 63, 64, 65, 66, 67], enabling
theexchangeofinformationacrossspatiallyandtemporallydistantregionsoftheinputvideo. State-of-the-art
action understanding results can often be achieved by simply employing a powerful video backbone with a
classifier networkon the entire videoclip (action recognition) ora spatially croppedversion (spatio-temporal
action detection). The region for spatial cropping is usually determined by employing a separate detector for
actors. Explicitly modeling motion [68, 69] or action-related components [70, 71, 72], such as actors and
objectsinvolved,foractionrecognitionandspatio-temporalactiondetectionhasalsobeenattempted,butoften
performs worse than simply using strong general video processing backbone without any bells and whistles.
Temporal action detection methods are conceptually similar to object detection methods, with the difference
being that temporal instead of spatial boundaries are predicted. Just as in object detection, there is a trend
towards single-stage methods that process the input using some form of video backbone and directly predict
actionboundariesandconfidencescoreswithoutanyin-betweenproposalgenerationstep[73,74,75,76].
The methods described so far assign actions either to entire videos, subsequences of videos or detected
individuals,butresearchersareofteninterestedininteractionsbetweentwoindividualsorindividualsandspe-
cific objects. Such interactions can be described by a scene graph, where edges express interactions between
entities(objects/individuals). Comparablylittleresearchhasbeendevotedtothechallengingfieldofdynamic
scenegraphgeneration. Inprinciple,existingmethods[77,78,79,80]involvethefollowingsteps: First,fea-
tures are extracted for entities (e.g. monkey and bowl) and relations (e.g. feeding). The entity features are
usuallyobtainedfromimageorvideobackbones,whilerelationsare,forexample,representedbytherelative
motionoftheinvolvedentities. Inasecondstep,theextractedfeaturesarefurtherprocessedusingsomeform
ofspatio-temporalreasoning. Finally,thefeaturesareusedtoseparatelypredicttheentityandrelationclasses.
3 Methods for effort-efficient learning
The methods presented in the previous section have been developed using (usually) large, diverse image or
video datasets with high-quality annotations. Thus, while there has been a lot of progress and many strong
methodsexistinprinciple,theycannotbeeasilyappliedtosituationswherenooronlyafewlabelsexist–asis
thecaseinanimalbehavior. Annotatingimagesorvideoswithboundingboxes,keypoints,segmentationmasks
orinteractionstakesaconsiderableamountoftimeandoftenlabelingbudgetsarelimited. Wethereforenow
turn to effort efficiency: Researchers would like to maximize model performance while minimizing manual
annotationcosts. Whichtechniquescanweusetomakethemostofavailablelabelsorevenofunlabeleddata?
Figure3providesaschematicoverviewofsomeofthemostpromisingeffort-efficientlearningtechniques.Note
6TransferLearning Semi-supervisedLearning
X Y pretrainedmodel XX
weight
ini�aliza�on
X Y model X Y model
Self-supervisedLearning
Ac�veLearning
XX
model X
dataselec�on
Weakly-supervisedLearning model
Y
X Y model humanannotator
incomplete(weak)labels
Figure3: Schematicoverviewofeffort-efficientlearningtechniques.
thattheboundariesbetweenthesecategoriesarefluentandsomemethodsdonotstrictlyfallintoonlyasingle
category,butarehybridapproaches. Ithelps,however,tolookattheapproachesinisolationastheydealwith
differenttypesofinputdataandmodelarchitectures.Belowweprovideahigh-leveloverviewofthesemethods.
InAppendixCwereviewthecurrentlyusedmethodologiesforeffort-efficientlearninginmoretechnicaldetail.
Themosttraditionaleffort-efficientlearningmethodistransferlearning. Theideaistotransferknowledge
gainedfromonetasktoenhancetheperformanceonarelatedtask. Traditionally,amodeltrainedonananno-
tated dataset with a wide range of objects, such as ImageNet [81] or MS-COCO [82], is fine-tuned with data
from the task of interest. Since knowledge contained in the original model can be leveraged, this allows for
traininggoodmodelsevenwhenonlyasmallnumberoflabelsisavailable.
Self-supervisedlearningmodelslearnmeaningfulrepresentationsfromlarge,unlabeleddatasets.Oneofthe
mostpopularapproachesismaskedautoencoders[83],wherepartsoftheimagearemaskedoutandthemodel
hastolearntorestoretheimage. Anotherlineofself-supervisedmodelsiscontrastivemethods[84,85],where
twosimilarversionsofaninputimagearecreated(e.g. byrandomcropping,flipping, colorchanges)andthe
modelhastolearnthattheyshowthesameimagewhiledifferentiatingitfromotherimages. Bothideasrequire
the model to learn a general understanding of what the image shows. After self-supervised pre-training on a
large unlabeled dataset, the models are adapted to the target task using transfer learning with a small labeled
dataset. Thiscanyieldbetterresultsthansupervisedpre-training[83].
Weakly supervised learning is used in scenarios where perfect annotations are not available. Models are
trainedusingincomplete,inaccurateorcheaperlabels. Especiallythelatterisrelevantforouranimalbehavior
models,aswecantrainamodelwithouthavingtogothroughthefullannotationeffort. Annotationsofimage-
level labels, bounding boxes, keypoints and instance segmentations are increasingly more fine-grained and
expensive. For example, for individual identification, we need bounding box labels, but drawing bounding
boxesismoretime-consumingthancreatingmorecoarse-grainedimage-levellabels(e.g. simplystatingwhich
individualsarepresentintheimage).Weaklysupervisedmethodsaimtofindwaystotrainafine-grainedmodel
withcoarse-grainedlabels,thusminimizingannotationtime.
Semi-supervised learning is a strategy that utilizes both labeled and unlabeled data for model training.
A basic strategy is to train a first model using the labeled data and use this to create pseudo labels on the
unlabeled data, with which a better model can be trained. In the popular architectures MixMatch [86] and
FixMatch [87], the pseudo labeling strategy is combined with consistency-based methods, where the model
learnstoconsistentlyrecognizeanimageasthesame,evenifitisflipped,cropped,hascolorchangesorother
augmentationsappliedtoit.
Activelearningisexploredasastrategicapproachtoselectingwhichunlabeledsamplesaremostbeneficial
7toannotatetoimprovethemodel.Forinstance,whenannotatingvideos,randomlysamplingframescouldresult
inmanyframeswithoutinterestingindividualsoractions,repeatedsamplesofthesameindividual,toosimple
ortoocomplicatedframes.Activelearninghassofarmostlybeenstudiedintheareaofimageclassification,and
recentlyinobjectdetection. Twomainstrategiesareused: Uncertainty-basedmethodslookforsampleswhere
thecurrentmodelshowslowconfidence[88];diversity-basedmethodsselectsamplesforannotationsuchthat
theentiredatasetiswellrepresented[89]. Activelearningispartofanemerginglargerfieldcalleddata-centric
learning,motivatedbystudiesthatshowthatbetterdataareoftenmorevaluablethanbettermodels[90].
Another approach how to avoid manual annotation is to generate synthetic training data. This usually
involvesdataproducedusing3Drenderers,wherelabeledsamplescanbegeneratedatnocostonceasimulation
modelhasbeencreated(e.g.Kubric[91]).Dependingonthequalityofthesimulateddata,itmightbenecessary
to explicitly adapt to real-world data, for example using transfer learning. In some cases, directly using the
model trained on synthetic images on natural images has proven to be successful. This involves optical flow
[92]andpointtracking[93,94].
Another paradigm that has become popular recently is cross-modal learning, where observations from a
secondmodalityserveassupervisorysignals. Forinstance,thelanguage-visionmodelCLIP[95]usesthetext
accompanying images on websites as “labels” for the images. Similar approaches could also be applied to
animal behavior. For example, in addition to video data, researchers could collect audio recordings, thermal
images,accelerometerdata,GPSinformation,weatherdataandmore,andusethedatafromtheseextramodal-
itiesaslabelsforvideodatainacross-modallearningsetting. Incorporatingaudioinformationintoamachine
vision model can lead to good performance on tasks relevant to researchers interested in animal behavior, as
demonstrated by Bain and colleagues [13] who used such an approach to identify actions performed by wild
chimpanzees.
Finally, annotationefficiencycanbeimprovedbyusingcollaborativeeffortsbetweenhumansandalready
existingmodels. Integrationofdeeplearningmodelsintoannotationworkflowsandthedesignofuser-friendly
toolsaremeanstoenhancetheefficiencyofcollaborativeannotationprocesses.
4 Avenues for Future Research
Wehaveseenthatquantifyingbehaviorfromvideorecordingsisamulti-facetedproblemthatrequiresdifferent
computer vision approaches depending on the specific information we seek to extract from a video and/or
the scientific question we wish to answer (identity, action, etc.). While the computer vision community has
developedapproachestotacklealloftheseproblems,notallofthemhavereceivedthesamedegreeofattention.
For instance, the computer vision community has focused much more on images than on video data, mostly
for reasons of computational complexity and memory. Research in computer vision is driven strongly by the
availabilityofbenchmarksand(annotated)datasets. Thefocushasoftenbeenondevelopingarchitecturesthat
workwellincertaindomains(e.g. pedestriantracking)giventheconstraintsofthesizeandtypesofdatasets
available. Comparably less effort has gone into researching how to obtain the necessary labels in the most
efficientway,howtoreuselabelsacrosstasksanddomains,andhowtodesignarchitecturesthatfacilitatesuch
reuse. Inthisfinalsection,wewanttohighlightwhatweconsidertobethemostimportantopenquestionsin
computervisionforanimalbehavior(Fig.4).
Video as a first-class citizen. Actions and interactions – the primary quantity of interest in behavioral sci-
ence – are inherently spatio-temporal. We argue that, for this reason, video should be treated as a first-class
citizen. A lot of progress in computer vision has been achieved in the area of image recognition (e.g. image
classification, objectdetection, imagesegmentation). Asaresult, evenvideoprocessingisoftendonebyfirst
performing frame-level image recognition and then merging/tracking results over time. As a prime example
of this paradigm, take the tracking-by-detection approach with independent detection and association phases:
image-based object detectors have been optimized for years and large, high-quality annotated datasets exist,
butdatasetswithdenselyannotatedobjecttracksinvideoarerareandcomparablysmall. Asaresult,tracking
approaches,whicharebasedonstate-of-the-artdetectors,outperformmethodsdirectlytrainedonobjecttracks
on typical pedestrian-tracking datasets. However, these image-based approaches likely lead to locally, rather
thangloballyoptimalsolutions,especiallywhenitcomestoclassifyinganimalbehavior. Thatis,frame-based
recognitioncannotdealaswellwithlow-levelimagecorruptionslikemotionblurorpoorlightingconditions,
has fundamental difficulties with reasoning about appearance changes caused by non-rigid body movements
and is more error-prone when it comes to occlusion of similarly looking individuals. All of these issues are
8Videoasfirstclassci�zen Nointermediaterepresenta�ons
x frame-wiseprocessing ✓ jointprocessing
keypoints
Bjumps AlooksatB
Aeats
output output
✓
video video video boundingboxes behavioralanalysis
✓
direct
Effortefficientlearningeverywhere Founda�onmodelsareusefulforpseudolabellinganddis�lla�on
x ✓ ✓
x ✓
X Y premtoradienled dataseX
lec�on model video image "describethescene"
X
X Y model X Y model Y "analyzebehavior"
k dn iso �w llale �d og ne founda�onmodel
supervisedlearning XX model X Y model founda�onmodel founda�onmodel
effort-efficientlearning
p las be eu lsdo X Y model model
behavioralanalysis
Figure4: Selectionofimportantdevelopmentsincomputervisionandtheirimpactonbehavioralanalysis.
in principle also present in tasks involving tracking humans moving through a space, but are far more pro-
nouncedwhenusingvideosofanimals,becauseanimalslookmoresimilar,movefasterandmorenonlinearly.
The community has recently started to recognize and tackle these challenges. For instance, the DanceTrack
benchmark[96]broughtintofocusthechallengesofdealingwithsimilar-lookingindividuals,complexmotion
andocclusion. Fromourperspective,asignificantproportionofthesedifficultiescouldbealleviatedbyshifting
from image-level to video-level methods. Recent query-based models where detections are obtained by also
takingintoaccountpastinformationofalready-trackedobjectsareastepintherightdirection. However,these
approachesstillprocessthevideoinaframe-by-framemanner,thuspreventingthelearningofvideo-levelfea-
tures. Realizingashifttowardsmethodsthatconsidervideosinanintegratedwayrequiresthecomputervision
communitytoacknowledgetheneedtomakethistransition.
Trackingindividualsinavideoisanimportantaspectofusingcomputervisioninthestudyofanimalbe-
havior,butnotanendinandofitself–itisonlyanintermediatestep. Whenstudyingthebehaviorofagroup
of non-human primates, localizing actions and interactions in space and time, as well as identifying which
individuals are involved, are key goals for researchers. Formally speaking, we want to infer a scene graph,
wherenodesrepresentindividuals(inspaceandtime)andedgesrepresenttheiractionsandinteractions. Thus,
whatisultimatelyneededisaframeworkthatcombinesideasfromseveralfieldsintoastrongspatio-temporal
backbone,ideallytrainedself-orsemi-supervisedwithmodularheadsthatoutputtracksforindividuals,identi-
fications,pixel-levelsegmentationand/orkeypointtracks,spatio-temporallylocalizedactionsandinteractions,
or subsets/combinations of those. As of now, detection, tracking and action understanding tasks are usually
treatedseparatelyfromeachother. Whileactionunderstandingmethodsarelargelybasedonvideo-levelpro-
cessing, the same does not hold for tracking methods, which prevents the convergence of two research areas
thatshouldbeconsideredjointly. Whiletherearestillsomeissuesrelatedtomemorylimitationsandlong-term
modeling capabilitiesof video-processing models, there arealready some promisingrecent works thataim to
providemodelsforjointtrackingandactionunderstanding[97,98]. Comprehensivebenchmarksforbehavioral
analysisareneededtodrivemodeldevelopmentinamoreunifieddirection.
Intermediaterepresentations. Poseestimationhasbecomeapopularfirststepandintermediaterepresenta-
tioninvideo-basedanalysisofanimalbehavior–inparticularinlab-basedsettings,e.g. inneuroscience. Pose
estimationasafirststephasbeenfacilitatedbyimpressiveopen-sourceframeworkssuchasDeepLabCut[7],
SLEAP[8]andDeepPoseKit[99],whichprovideexcellenttoolsfordataannotation,arewell-documented,rel-
ativelyeasytousefornon-expertsandworkremarkablywellinlabsettingsorwhenoverfittoindividualvideo
sequences. Other works built upon these frameworks by providing behavioral analysis tools for laboratory
settingsthatarebasedonfeaturesderivedfromtrackedkeypoints[100,4].
Keypointsclearlycontainusefulinformationforsomedownstreamtasks,suchasactionunderstanding,but
9theydonotlendthemselvesasnaturallytootherimportantproblemssuchasidentification,trackingorclassi-
fying certain actions. For example, the action “monkey feeding” can be more unambiguously detected using
visual information (e.g. food in the hand). Thus, while being helpful sometimes, the extraction of visual in-
formation from an image or video does not necessitate keypoints. We therefore argue that there are several
reasonsforpreferringboundingboxesastheintermediaterepresentationsaroundwhichtobuildgeneralizable
architectures: First,boundingboxesarefasterandeasiertoobtaincomparedtootherinstancerepresentations.
Second, as deep learning models become more and more powerful, we expect them to be able to extract the
relevantfeaturesfortaskssuchasactionunderstandingdirectlyfromtheinputimageorvideo,withouttheneed
forcomplexinstancerepresentations. Ifmorefine-grainedinstancerepresentationsareneeded,theycanbeeas-
ilyaddedasasecondstepwhentheinstancesarealreadydetectedandlocalized. Third,state-of-the-artaction
understandingmethodsalreadyrelypredominantlyonboundingboxesasinstancerepresentations. Thisobser-
vation mirrors a general trend: Over the history of deep learning, models using intermediate representations
have disappeared and been replaced with models that are trained “end-to-end” directly on the task of interest
(e.g. actionunderstanding). Until2012,imagerecognitionclassifiersweretrainedonmanually-definedvisual
representations(Bag-of-visualwords[101],histogramoforientedgradients[102]). TheseminalAlexNet[103]
hasshownthatend-to-endtrainingdirectlyontheimageclassificationtaskoutperformsusinghandcraftedinter-
mediaterepresentations. Weexpectthistransitiontoalsohappeninanimalbehavioranalysis,whichcurrently
reliesheavilyonsomeformofintermediaterepresentation. Akeyquestionwillbeiflabeleddataisnecessary
asadriveroriffeaturescanalsobelearnedinanunsupervisedway–ournexttopic.
Effort-efficientlearningeverywhere. Progressinimagerecognitionwasinitiallydrivenbyend-to-endtrain-
ingonlargecollectionsoflabeleddatasuchasImageNet[81]andMS-COCO[82],buttheimagecharacteristics
of such commonly used datasets differ quite a bit from those encountered when studying animal behavior in
thewild,limitingthepoweroftransferlearningfromsuchdatasets. Collectingandannotatinglarge-scale“in-
domain” datasets (i.e. with relevant species in the appropriate environments and contexts) is costly. Hence,
suchdatasetsmightnotbeavailableanytimesoonformanyrelevantanimalbehaviortasks. However, recent
years have brought a lot of progress in training models without the need for manual data annotation, as dis-
cussedinSection3. Inourview,aparticularlypromisingresearchdirectioninthecontextofanimalbehavior
isself-supervisedlearningincombinationwithtransferlearning.
Self-supervisedlearningispromisingbecauselargecollectionsofvideofootageofanimalbehaviorarein-
creasinglyavailableasresearchersemployvideorecordingsforbehavioralobservationsorsetupcameratraps
inthewild. Suchdatasetscouldbeusedtopre-trainabackbonein-domain. However,itremainstobeinvesti-
gatedwhichpre-trainingobjectivesyieldfeaturesthatareusefulforthedownstreamtask: itisnotatallclear
whetherself-supervisedlearningapproachesdevelopedonImageNetorYouTubevideoswillgeneralizetothe
problemsposedbyanalyzinganimalbehavior.Forinstance,theself-supervisedparadigmsdevelopedusingdata
fromImageNetdonotreadilygeneralizetootherimagedomains,becauseImageNetimagesaremostlyfocused
on a single salient object and work by applying label-preserving transformations to these images. Such ap-
proachesmaynoteasilygeneralizetosettingswheretherearemanysimilar-lookingindividualsclosetogether.
Current video-based self-supervised learning strategies are largely inspired by their counterparts in the image
domain.Themostsuccessfulmodelsrelyonmaskedfeaturemodeling[104,105,106]andvideo-languagecon-
trastivelearning[107]. Puttinganimalbehaviorrecognitionontheagendaforevaluatingself-supervisedvideo
representation learning could inspire new basic research in computer vision by making limitations of current
approaches measurable and incentivizing the development of new methods to address these limitations. The
animalbehaviorcommunitycouldfacilitatesucheffortsbypublishingdatasetsandestablishingbenchmarks.
Oneofthebiggestleverstoenableprogressinapplicationdomainssuchasanimalbehavior(butalsomore
broadly) is to shift focus from developing architectures for existing tasks and datasets towards the process of
iteratively annotating (the right) data and training models until the accuracy requirements of the problem at
handaremet. Inotherwords: Giventhatwewanttoidentifycertainaspectsofanimalbehavior(e.g. identities
of individuals and grooming interactions), which parts (frames, snippets) of the dataset should we annotate,
how (boxes, tracks, keypoints, actions spatially and/or temporally localized?) and in what order to achieve
satisfactoryperformancewithaslittleeffortaspossible? Inmachinelearning,thequestionofwhichsamplesto
annotateisformalizedasactivelearning. Incomputervision,activelearningismostlystudiedinthecontextof
imageclassification,morerecentlyalsoinobjectdetection[108]andsegmentation(e.g.SAM[15]).Extending
researchonactivelearningmoretowardsspatio-temporaltaskssuchasactionandinteractionrecognitionwould
beextremelyhelpful. Tosomeextent, thisisgoingtobeamatterofadaptingestablishedprinciplesofactive
learningtothesemorecomplextasksanddevelopingtherighttools. However,thereisalsoadeeperscientific
10Figure5: Multi-modallargelanguagemodels(hereGPT4-V)canbeusedtounderstandchallengingscenes. It
succeeds in recognizing the general setting (monkeys sitting on a branch) but fails to detect details (the baby
monkey,spatialrelation,interaction). Whilecurrentmodelsarenotcapableofafullyautomatedanalysisthey
mightserveasatoolforpseudolabeling. MoreexamplescanbefoundinAppendixD.
question that has not received too much attention thus far: Action and interaction recognition is inherently
a multi-task problem. Individuals need to be detected and tracked, then actions and interactions associated
with them. Some of these tasks inform each other. For instance, it is well known that adding segmentation
labels helps object detection (Mask R-CNN [109]), but it is less clear whether investing the same amount of
labeling time in additional bounding boxes would not improve a detector more. Similarly, some research has
investigatedhowrepresentationslearnedbyonetaskgeneralizetoanother(Taskonomy[110]),butwithafocus
on purely spatial tasks, most of which are not directly relevant to the study of animal behavior. How much
annotationeffortshouldbeputintoeachoftherelevanttasks,andwhataregoodstrategiesforidentifyingthe
limitingfactorsthatprovidethemostleveragetoimprovementatanygivenpointintime? Thisproblemcould
bephrasedasmulti-taskactivelearning,andwearenotawareofanyresearchinthisdirection.
Foundationmodels. ChatGPThaspopularizedtheideaoffoundationmodels[111], verylargemodelsthat
addressseveraltasksatonce. Whilethefirstversionsoperatedexclusivelyontext,nowimageprocessingcapa-
bilityisavailable.Thisraisesthequestiontowhichdegreebehavioranalysistaskswillbesolvedinthefutureby
suchgeneral-purposemodels(Fig.5). Whileearlyexperiments[112]suggestremarkableimageunderstanding
capabilities,webelievebehavioralanalysiswillnotbeconductedbythesemodelsinthenearfutureformulti-
plereasons: (a)behaviorrequiresvideounderstandingandspatio-temporalreasoningand(b)thedatadomain
is very different from the typical internet videos. (c) Most importantly, tasks like individual identification, or
specifying a target ethogram will necessarily require tuning the model as the required information to master
thesetasksisnotaccessibletothemodel. Nonetheless,foundationmodelscanalreadynowenhancebehavioral
understanding: Even closed-source models such as OpenAI’s GPT4-V can be leveraged to generate pseudo
labelsonimages.Fromopenly-availablemodelssuchasSegmentAnythingorCLIP,itisalsopossibletoextract
theknowledgethroughdistillation[113].
5 Conclusion
Wegaveacomprehensiveoverviewofthecurrentstate-of-the-artcomputervisiontechniquesthatweconsider
crucial for the analysis of individualized animal behavior in natural settings, with the four key tasks being
object detection, multi-individual tracking, individual identification and action understanding. Additionally,
we reviewed methods for effort-efficient learning, highlighting their practical significance given the extensive
annotation requirements of computer vision models. We expect recent generic computer vision technologies,
from multi-object tracking to action recognition, to surpass the performance of abundantly used specialized
image-orkeypoint-basedmethodsforautomatedanimalbehavioranalysisinthewild.
At the same time, there are still several computer vision problems highly relevant to practical behavioral
researchers, which should – in our opinion – receive more attention from the community. Extending effort-
efficientmethodssuchasactivelearningfromimagestovideohasgreatpotentialforprimatebehavioranalysis.
Furthermore,unifyingtaskssuchastracking,individualidentificationandactionunderstandinginsteadoftreat-
ingthemseparatelywouldenableacomprehensivecharacterizationofhowindividualanimalsinagroupmove
11andwhat(inter)actionstheyengagein. Thecreationofsuchjointmodelsisalsodesirableintermsofthecon-
vergenceofdifferentresearchdirectionstowardsholisticvideoprocessing. Ultimately, collaborationbetween
behavioral scientists and computer vision researchers will play a vital role in finding the right path towards
developingmethodologiesthatarebothpowerfulandpracticallyapplicable.
Acknowledgements
This publication was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
– Project-ID 454648639 – SFB 1528 – Cognition of Interaction and with NextGenerationEU funds from the
EuropeanUnionbytheFederalMinistryofEducationandResearchunderthefundingcode16DKWN038. The
responsibilityforthecontentofthispublicationlieswiththeauthors.
12References
[1] IainDCouzinandConorHeins.“Emergingtechnologiesforbehavioralresearchinchangingenviron-
ments”.In:TrendsinEcology&Evolution38.4(2023),pp.346–354.
[2] MelissaBatesonandPaulMartin.Measuringbehaviour:anintroductoryguide.Cambridgeuniversity
press,2021.
[3] VassilisMPapadakisetal.“Acomputer-visionsystemandmethodologyfortheanalysisoffishbehav-
ior”.In:Aquaculturalengineering46(2012),pp.53–59.
[4] CristinaSegalinetal.“TheMouseActionRecognitionSystem(MARS)softwarepipelineforautomated
analysisofsocialbehaviorsinmice”.In:Elife10(2021),e63720.
[5] Kevin Luxem et al. “Open-source tools for behavioral video analysis: Setup, methods, and best prac-
tices”.In:Elife12(2023),e79305.
[6] Daniel P Schofield et al. “Automated face recognition using deep neural networks produces robust
primate social networks and sociality measures”. In: Methods in Ecology and Evolution 14.8 (2023),
pp.1937–1951.
[7] AlexanderMathisetal.“DeepLabCut:markerlessposeestimationofuser-definedbodypartswithdeep
learning”.In:Natureneuroscience21.9(2018),pp.1281–1289.
[8] Talmo D Pereira et al. “SLEAP: A deep learning system for multi-animal pose tracking”. In: Nature
methods19.4(2022),pp.486–495.
[9] TristanWalterandIainDCouzin.“TRex,afastmulti-animaltrackingsystemwithmarkerlessidentifi-
cation,and2Destimationofpostureandvisualfields”.In:eLife10(Feb.2021).Ed.byDavidLentink,
ChristianRutz,andSergiPujades,e64000.ISSN:2050-084X.URL:https://doi.org/10.7554/
eLife.64000.
[10] Jacob M. Graving et al. “DeepPoseKit, a software toolkit for fast and robust animal pose estimation
usingdeeplearning”.In:eLife8(2019).
[11] GiannaAJordanetal.“Automatedsystemfortrainingandassessingreachingandgraspingbehaviors
inrodents”.In:JournalofNeuroscienceMethods401(2024),p.109990.
[12] CharlotteWiltshireetal.“DeepWild:ApplicationoftheposeestimationtoolDeepLabCutforbehaviour
trackinginwildchimpanzeesandbonobos”.In:JournalofAnimalEcology(2023).
[13] MaxBainetal.“Automatedaudiovisualbehaviorrecognitioninwildprimates”.In:ScienceAdvances
7.46(2021),eabi4883.
[14] MarkusMarksetal.“Deep-learning-basedidentification,tracking,poseestimationandbehaviourclas-
sificationofinteractingprimatesandmiceincomplexenvironments”.In:NatureMachineIntelligence
4.4(2022),pp.331–340.
[15] AlexanderKirillovetal.“Segmentanything”.In:ProceedingsoftheIEEE/CVFinternationalconfer-
enceoncomputervision(ICCV)(2023).
[16] RizaAlpGu¨ler,NataliaNeverova,andIasonasKokkinos.“DensePose:DenseHumanPoseEstimation
in the Wild”. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018),
pp.7297–7306.
[17] Ross B. Girshick et al. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Seg-
mentation”.In:2014IEEEConferenceonComputerVisionandPatternRecognition(2013),pp.580–
587.
[18] ShaoqingRenetal.“FasterR-CNN:TowardsReal-TimeObjectDetectionwithRegionProposalNet-
works”.In:IEEETransactionsonPatternAnalysisandMachineIntelligence39(2015),pp.1137–1149.
[19] KaimingHeetal.“MaskR-CNN”.In:IEEETransactionsonPatternAnalysisandMachineIntelligence
42(2017),pp.386–397.
[20] JifengDaietal.“R-FCN:ObjectDetectionviaRegion-basedFullyConvolutionalNetworks”.In:ArXiv
abs/1605.06409(2016).
[21] Wei Liu et al. “Ssd: Single shot multibox detector”. In: Computer Vision–ECCV 2016: 14th Euro-
peanConference,Amsterdam,TheNetherlands,October11–14,2016,Proceedings,PartI14.Springer.
2016,pp.21–37.
13[22] JosephRedmonandAliFarhadi.“YOLO9000:Better,Faster,Stronger”.In:2017IEEEConferenceon
ComputerVisionandPatternRecognition(CVPR)(2016),pp.6517–6525.
[23] JosephRedmonandAliFarhadi.“YOLOv3:AnIncrementalImprovement”.In:ArXivabs/1804.02767
(2018).
[24] ZhengGeetal.“YOLOX:ExceedingYOLOSeriesin2021”.In:ArXivabs/2107.08430(2021).
[25] HeiLawandJiaDeng.“CornerNet:DetectingObjectsasPairedKeypoints”.In:InternationalJournal
ofComputerVision128(2018),pp.642–656.
[26] KaiwenDuanetal.“CenterNet:KeypointTripletsforObjectDetection”.In:2019IEEE/CVFInterna-
tionalConferenceonComputerVision(ICCV)(2019),pp.6568–6577.
[27] Xingyi Zhou, Dequan Wang, and Philipp Kra¨henbu¨hl. “Objects as Points”. In: ArXiv abs/1904.07850
(2019).
[28] ZhiTianetal.“Fcos:Fullyconvolutionalone-stageobjectdetection”.In:ProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision.2019,pp.9627–9636.
[29] AshishVaswanietal.“AttentionisAllyouNeed”.In:NeuralInformationProcessingSystems.2017.
[30] Nicolas Carion et al. “End-to-End Object Detection with Transformers”. In: European Conference on
ComputerVision(ECCV)abs/2005.12872(2020).
[31] Xizhou Zhu et al. “Deformable DETR: Deformable Transformers for End-to-End Object Detection”.
In:ArXivabs/2010.04159(2020).
[32] Feng Li et al. “DN-DETR: Accelerate DETR Training by Introducing Query DeNoising”. In: 2022
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)(2022),pp.13609–13617.
[33] Shilong Liu et al. “DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR”. In: ArXiv
abs/2201.12329(2022).
[34] HaoZhangetal.“DINO:DETRwithImprovedDeNoisingAnchorBoxesforEnd-to-EndObjectDe-
tection”.In:ArXivabs/2203.03605(2022).
[35] GioeleCiaparroneetal.“DeepLearninginVideoMulti-ObjectTracking:ASurvey”.In:Neurocomput-
ing(2020),pp.61–88.
[36] Alex Bewley et al. “Simple online and realtime tracking”. In: 2016 IEEE international conference on
imageprocessing(ICIP).IEEE.2016,pp.3464–3468.
[37] YifuZhangetal.“Bytetrack:Multi-objecttrackingbyassociatingeverydetectionbox”.In:Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
PartXXII.Springer.2022,pp.1–21.
[38] JinkunCaoetal.“Observation-centricsort:Rethinkingsortforrobustmulti-objecttracking”.In:arXiv
preprintarXiv:2203.14360(2022).
[39] GerardMaggiolinoetal.“DeepOC-SORT:Multi-PedestrianTrackingbyAdaptiveRe-Identification”.
In:arXivpreprintarXiv:2302.11813(2023).
[40] NirAharon,RoyOrfaig,andBen-ZionBobrovsky.“BoT-SORT:Robustassociationsmulti-pedestrian
tracking”.In:arXivpreprintarXiv:2206.14651(2022).
[41] Fangao Zeng et al. “Motr: End-to-end multiple-object tracking with transformer”. In: European Con-
ferenceonComputerVision.Springer.2022,pp.659–675.
[42] YuangZhang,TiancaiWang,andXiangyuZhang.“MOTRv2:BootstrappingEnd-to-EndMulti-Object
TrackingbyPretrainedObjectDetectors”.In:ArXivabs/2211.09791(2022).
[43] EnYuetal.“MOTRv3:Release-FetchSupervisionforEnd-to-EndMulti-ObjectTracking”.In:arXiv
preprintarXiv:2305.14298(2023).
[44] FengYanetal.“BridgingtheGapBetweenEnd-to-endandNon-End-to-endMulti-ObjectTracking”.
In:arXivpreprintarXiv:2305.12724(2023).
[45] SandraFreyetal.“Investigatinganimalactivitypatternsandtemporalnichepartitioningusingcamera-
trapdata:challengesandopportunities”.In:RemoteSensinginEcologyandConservation3.3(),pp.123–
132.
14[46] J.AndrewRoyle.“HierarchicalSpatialCapture–RecaptureModelsforEstimatingDensityfromTrap-
pingArrays”.In:CameraTrapsinAnimalEcology:MethodsandAnalyses.Ed.byAllanF.O’Connell,
JamesD.Nichols,andK.UllasKaranth.Tokyo:SpringerJapan,2011,pp.163–190.ISBN:978-4-431-
99495-4.
[47] Maxime Vidal et al. “Perspectives on individual animal identification from biology and computer vi-
sion”.In:Integrativeandcomparativebiology61.3(2021),pp.900–916.
[48] Andre´ C. Ferreira et al. “Deep learning-based methods for individual recognition in small birds”. In:
MethodsinEcologyandEvolution11.9(),pp.1072–1085.
[49] Francisco Romero-Ferrero et al. “Idtracker. ai: tracking all individuals in small or large collectives of
unmarkedanimals”.In:Naturemethods16.2(2019),pp.179–182.
[50] DebayanDebetal.“FaceRecognition:PrimatesintheWild”.In:2018IEEE9thInternationalConfer-
enceonBiometricsTheory,ApplicationsandSystems(BTAS).2018,pp.1–10.
[51] MaxBainetal.“Count,CropandRecognise:Fine-GrainedRecognitionintheWild”.In:2019IEEE/CVF
InternationalConferenceonComputerVisionWorkshop(ICCVW)(2019),pp.236–246.
[52] Florian Schroff, Dmitry Kalenichenko, and James Philbin. “Facenet: A unified embedding for face
recognition and clustering”. In: Proceedings of the IEEE conference on computer vision and pattern
recognition.2015,pp.815–823.
[53] YutoKodamaetal.“Open-setrecognitionwithsupervisedcontrastivelearning”.In:202117thInterna-
tionalConferenceonMachineVisionandApplications(MVA).IEEE.2021,pp.1–5.
[54] Jongjin Park et al. “Opencos: Contrastive semi-supervised learning for handling open-set unlabeled
data”.In:EuropeanConferenceonComputerVision.Springer.2022,pp.134–149.
[55] JiayinSunandQiuleiDong.“ASurveyonOpen-SetImageRecognition”.In:arXivpreprintarXiv:2312.15571
(2023).
[56] ZhaofanQiu,TingYao,andTaoMei.“Learningspatio-temporalrepresentationwithpseudo-3dresidual
networks”.In:proceedingsoftheIEEEInternationalConferenceonComputerVision.2017,pp.5533–
5541.
[57] Youngwan Lee et al. “Diverse temporal aggregation and depthwise spatiotemporal factorization for
efficientvideoclassification”.In:IEEEAccess9(2021),pp.163054–163064.
[58] ChristophFeichtenhoferetal.“Slowfastnetworksforvideorecognition”.In:ProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision.2019,pp.6202–6211.
[59] AnuragArnabetal.“Vivit:Avideovisiontransformer”.In:ProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision.2021,pp.6836–6846.
[60] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. “Is space-time attention all you need for video
understanding?”In:ICML.Vol.2.3.2021,p.4.
[61] ShenYanetal.“Multiviewtransformersforvideorecognition”.In:ProceedingsoftheIEEE/CVFCon-
ferenceonComputerVisionandPatternRecognition.2022,pp.3333–3343.
[62] AJPiergiovanni,WeichengKuo,andAneliaAngelova.“Rethinkingvideovits:Sparsevideotubesfor
jointimageandvideolearning”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition.2023,pp.2214–2224.
[63] HaoqiFanetal.“Multiscalevisiontransformers”.In:ProceedingsoftheIEEE/CVFInternationalCon-
ferenceonComputerVision.2021,pp.6824–6835.
[64] ChaitanyaRyalietal.“Hiera:AHierarchicalVisionTransformerwithouttheBells-and-Whistles”.In:
InternationalConferenceonMachineLearning(ICML)(2023).
[65] Chao-Yuan Wu et al. “Memvit: Memory-augmented multiscale vision transformer for efficient long-
termvideorecognition”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.2022,pp.13587–13597.
[66] Kunchang Li et al. “Uniformer: Unified transformer for efficient spatiotemporal representation learn-
ing”.In:arXivpreprintarXiv:2201.04676(2022).
[67] KunchangLietal.“UniFormerV2:SpatiotemporalLearningbyArmingImageViTswithVideoUni-
Former”.In:arXivpreprintarXiv:2211.09552(2022).
15[68] Mandela Patrick et al. “Keeping your eye on the ball: Trajectory attention in video transformers”. In:
Advancesinneuralinformationprocessingsystems34(2021),pp.12493–12506.
[69] FuchenLongetal.“Stand-aloneinter-frameattentioninvideomodels”.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.2022,pp.3192–3201.
[70] Joanna Materzynska et al. “Something-else: Compositional action recognition with spatial-temporal
interaction networks”. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition.2020,pp.1049–1059.
[71] YuanTianetal.“Ean:eventadaptivenetworkforenhancedactionrecognition”.In:InternationalJour-
nalofComputerVision130.10(2022),pp.2453–2471.
[72] Gueter Josmy Faure, Min-Hung Chen, and Shang-Hong Lai. “Holistic Interaction Transformer Net-
work for Action Detection”. In: Proceedings of the IEEE/CVF Winter Conference on Applications of
ComputerVision.2023,pp.3340–3350.
[73] Feng Cheng and Gedas Bertasius. “TallFormer: Temporal Action Localization with a Long-Memory
Transformer”.In:ComputerVision–ECCV2022:17thEuropeanConference,TelAviv,Israel,October
23–27,2022,Proceedings,PartXXXIV.Springer.2022,pp.503–521.
[74] Chen-Lin Zhang, Jianxin Wu, and Yin Li. “Actionformer: Localizing moments of actions with trans-
formers”.In:ComputerVision–ECCV2022:17thEuropeanConference,TelAviv,Israel,October23–
27,2022,Proceedings,PartIV.Springer.2022,pp.492–510.
[75] TuanNTang,KwonyoungKim,andKwanghoonSohn.“TemporalMaxer:MaximizeTemporalContext
withonlyMaxPoolingforTemporalActionLocalization”.In:arXivpreprintarXiv:2303.09055(2023).
[76] DingfengShietal.“TriDet:TemporalActionDetectionwithRelativeBoundaryModeling”.In:arXiv
preprintarXiv:2303.07347(2023).
[77] Jingyi Wang et al. “Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene
Graphs”.In:arXivpreprintarXiv:2305.08522(2023).
[78] Shengyu Feng et al. “Exploiting long-term dependencies for generating dynamic scene graphs”. In:
ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision.2023,pp.5130–
5139.
[79] KaifengGaoetal.“Classification-then-grounding:Reformulatingvideoscenegraphsastemporalbipar-
titegraphs”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
2022,pp.19497–19506.
[80] MengWeietal.“InDefenseofClip-basedVideoRelationDetection”.In:arXivpreprintarXiv:2307.08984
(2023).
[81] OlgaRussakovskyetal.“ImageNetLargeScaleVisualRecognitionChallenge”.In:InternationalJour-
nalofComputerVision115(2014),pp.211–252.
[82] Tsung-Yi Lin et al. “Microsoft COCO: Common Objects in Context”. In: European Conference on
ComputerVision.2014.
[83] KaimingHeetal.“Maskedautoencodersarescalablevisionlearners”.In:ProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition.2022,pp.16000–16009.
[84] TingChenetal.“Asimpleframeworkforcontrastivelearningofvisualrepresentations”.In:Interna-
tionalconferenceonmachinelearning.PMLR.2020,pp.1597–1607.
[85] XinleiChenandKaimingHe.“Exploringsimplesiameserepresentationlearning”.In:Proceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.2021,pp.15750–15758.
[86] David Berthelot et al. “Mixmatch: A holistic approach to semi-supervised learning”. In: Advances in
neuralinformationprocessingsystems32(2019).
[87] KihyukSohnetal.“Fixmatch:Simplifyingsemi-supervisedlearningwithconsistencyandconfidence”.
In:Advancesinneuralinformationprocessingsystems33(2020),pp.596–608.
[88] DonggeunYooandInSoKweon.“Learninglossforactivelearning”.In:ProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition.2019,pp.93–102.
[89] Ozan Sener and Silvio Savarese. “Active learning for convolutional neural networks: A core-set ap-
proach”.In:arXivpreprintarXiv:1708.00489(2017).
16[90] Daochen Zha et al. “Data-centric ai: Perspectives and challenges”. In: Proceedings of the 2023 SIAM
InternationalConferenceonDataMining(SDM).SIAM.2023,pp.945–948.
[91] KlausGreffetal.“Kubric:Ascalabledatasetgenerator”.In:ProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition.2022,pp.3749–3761.
[92] Alexey Dosovitskiy et al. “FlowNet: Learning Optical Flow with Convolutional Networks”. In: 2015
IEEEInternationalConferenceonComputerVision(ICCV)(2015),pp.2758–2766.
[93] NikitaKaraevetal.“CoTracker:ItisBettertoTrackTogether”.In:arXiv:2307.07635(2023).
[94] Carl Doersch et al. “TAPIR: Tracking Any Point with per-frame Initialization and temporal Refine-
ment”.In:ICCV (2023).
[95] Alec Radford et al. “Learning Transferable Visual Models From Natural Language Supervision”. In:
InternationalConferenceonMachineLearning.2021.
[96] PeizeSunetal.“Dancetrack:Multi-objecttrackinginuniformappearanceanddiversemotion”.In:Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,pp.20993–
21002.
[97] Jiaojiao Zhao et al. “Tuber: Tubelet transformer for video action detection”. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,pp.13598–13607.
[98] Alexey Gritsenko et al. “End-to-End Spatio-Temporal Action Localisation with Video Transformers”.
In:arXivpreprintarXiv:2304.12160(2023).
[99] Jacob M Graving et al. “DeepPoseKit, a software toolkit for fast and robust animal pose estimation
usingdeeplearning”.In:Elife8(2019),e47994.
[100] Simon RO Nilsson et al. “Simple Behavioral Analysis (SimBA)–an open source toolkit for computer
classificationofcomplexsocialbehaviorsinexperimentalanimals”.In:BioRxiv(2020),pp.2020–04.
[101] Gabriella Csurka et al. “Visual categorization with bags of keypoints”. In: European Conference on
ComputerVision.2002.
[102] NavneetDalalandBillTriggs.“Histogramsoforientedgradientsforhumandetection”.In:2005IEEE
ComputerSocietyConferenceonComputerVisionandPatternRecognition(CVPR’05)1(2005),886–
893vol.1.
[103] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.“Imagenetclassificationwithdeepconvolu-
tionalneuralnetworks”.In:Advancesinneuralinformationprocessingsystems25(2012).
[104] Rui Wang et al. “Masked video distillation: Rethinking masked feature modeling for self-supervised
videorepresentationlearning”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition.2023,pp.6312–6322.
[105] ZhanTongetal.“Videomae:Maskedautoencodersaredata-efficientlearnersforself-supervisedvideo
pre-training”.In:Advancesinneuralinformationprocessingsystems35(2022),pp.10078–10093.
[106] Limin Wang et al. “Videomae v2: Scaling video masked autoencoders with dual masking”. In: Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2023,pp.14549–
14560.
[107] Yi Wang et al. “InternVideo: General Video Foundation Models via Generative and Discriminative
Learning”.In:arXivpreprintarXiv:2212.03191(2022).
[108] Ismail Elezi et al. “Not all labels are equal: Rationalizing the labeling costs for training object detec-
tion”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,
pp.14492–14501.
[109] Kaiming He et al. “Mask r-cnn”. In: Proceedings of the IEEE international conference on computer
vision.2017,pp.2961–2969.
[110] Amir R Zamir et al. “Taskonomy: Disentangling task transfer learning”. In: Proceedings of the IEEE
conferenceoncomputervisionandpatternrecognition.2018,pp.3712–3722.
[111] Rishi Bommasani et al. “On the opportunities and risks of foundation models”. In: arXiv preprint
arXiv:2108.07258(2021).
[112] Zhengyuan Yang et al. “The dawn of lmms: Preliminary explorations with gpt-4v (ision)”. In: arXiv
preprintarXiv:2309.174219.1(2023).
17[113] GeoffreyE.Hinton,OriolVinyals,andJeffreyDean.“DistillingtheKnowledgeinaNeuralNetwork”.
In:ArXivabs/1503.02531(2015).
18Pretraining
pretrainedmodel classprobabili�es
weight feature
inputimage ini�aliza�on volumes
head1
model head2
...
Figure6: Beforefine-tuningforaspecifictask,oftenneuralnetworksareinitializedusingImageNetclassifica-
tionweights.
A Foundations
A.1 NeuralNetworks
Neuralnetworksaremachinelearningmodelswhoseparameters(weights)canbeefficientlylearnedfromdata
usingthebackpropagationalgorithm[1]. Deepneuralnetworksarecomposedofmultiplelayers,i.e. blocksof
theconceptuallysamebutdifferentlyparameterizedoperation. Forinstance,afully-connectedlayertransforms
an input vector by matrix multiplication. The transformed vector can then be used as input to another fully-
connectedlayer.
A.2 Backbones
In modern neural-network-based computer vision, it is common to use a backbone model to obtain general
image features, which can be applied to several tasks. Backbones incorporate best practices for the design of
theneuralnetworkarchitecturesuchthatthisdoesnothavetobereinventedforeverytask.Convolutionalneural
networks (CNNs) such as ResNets [2] are commonly used as backbones for image tasks. The idea here is to
applythesamesetoflearnablefiltersatagridoflocationsofanimageineachlayer. Sincethesameoperation
is repeated at different locations and the number of parameters is independent of the image size, CNNs are
parameter-efficientmodelsthatcanlearnfromsmalldatasets. Thefeaturepyramidnetwork[3]isaCNNthat
candealwithimagecontentatdifferentsizescales. Drivenbythesuccessinlanguagemodeling,transformer
models were recently successfully applied on images [4]. The input image is divided into a grid of patches
(commonlyofsize8to16pixels). Eachpatchismappedtoatokenusingalinearprojectionandencodedusing
apositionalembedding.Then,atransformerencoder[5]isusedtoprocesstheimage.Toextractasinglefeature
vectorforwhole-imageclassification,aCLStokencanbeaddedtotheinput(inadditiontothepatches)orall
tokenscanbepooled.
The choice of backbone strongly influences training and inference time, as the number of parameters and
computeoperationscanvarysubstantially. Backboneswithalargenumberofparameterstendtoachievebetter
performanceacrossdownstreamtasksbutrequiremoretimetotrainandtomakepredictionsattesttime. Such
backbonesmightalsonotfitontoembeddeddevicestobeusedinthefield(e.g. aRaspberryPi4)ormightnot
permithigherframeratestostudymotioninreal-time.
A.3 Pre-trainingandTransferLearning
Pre-training(Fig.6)describestheprocessoftrainingamodelonanexternal,oftenlarge,dataset,whichallows
themodeltolearngenericimageorvideofeaturesthatrepresentthevisualcontentoftheinput. Duringtransfer
learning, this pre-trained model is fine-tuned on a, typically smaller, dataset that is available for the task of
interest. Forexample,anobjectdetectionmodelthathasbeentrainedonalargedatasetthatconsistsofhumans
can be used for fine-tuning on a smaller object detection dataset that consists of primates. We provide more
detailontransferlearninganddifferentpre-trainingtechniquesinAppendixC.1.
A.4 TaskHeads
Startingfromapre-trainedbackbone,thedesiredtaskcanbetrainedasataskheadontopofit. Forexample,
ifourtargettaskisindividualidentification,wecanaddan“identification”headtothebackbone. Multipletask
19heads may be added to the same backbone (e.g. identification, localisation, action classification). Since we
expectthebackbonetohavelearnedgoodfeaturesbefore,thetargettaskdoesnotneedaverylargenumberof
parameters. If the downstream dataset is small and the pre-trained backbone has many learnable parameters,
fine-tuningtheentiremodelcanleadtooverfitting. Thismeansthatthemodelperformswellonthetrainingset,
butlosesitsabilitytogeneralizetounseendata. Therefore,itcanbehelpfultofreezemostlayersandonlytrain
asmallpartofthemodel,suchasthetaskheads.
B Methods for Primate Behavior Analysis
B.1 Animaldetection
Thegoalofdetectionistotakeanimageandpredictasetofboundingboxesthatindicatethelocationsofani-
malsintheimage.Oftentheboundingboxlocationsareaccompaniedbyobjectclasspredictionsandconfidence
scores.
Anchor-based Oneapproachforobjectdetectionistodefinealargesetofcandidateregions,calledanchors,
a priori and allow the model to determine which of these candidate regions matches an object. This means
the task of determining the presence of an object is reduced to predicting a confidence score that indicates if
eachcandidateregioncontainsanobjectornot.However,sincethenumberofallpossibleobjectregionsisvery
high,itiscomputationallyinfeasibletomakeapredictionforallofthem.Manyanchor-baseddetectionmethods
solvethisproblemusingasliding-windowapproach[6,7,8]: Theypre-defineasmallnumberofanchor-boxes
(usually not more than 10) by their width and height to cover objects of different scales. Then, the image is
dividedintoagridandforeachgridlocationandanchorbox,itisevaluatedwhetherthereisamatchingobject
or not. For computational efficiency, the grid does not cover every pixel. To compensate for the inaccuracies
introducedbyconsideringonlyafixedsetofanchorboxesandgridlocations,anoffsetispredictedinaddition
totheconfidencescoretorefinethelocationaswellasthewidthandheightofthedetectedobjects. Toobtain
asetofproblem-specificanchor-boxwidthsandheights, theirdistributioninthetrainingdatasetcanbeused,
e.g. apedestriandatasetwillresultinprimarilyelongatedanchors. Adisadvantageofanchor-basedapproaches
isthatthemodel’sperformancemightdeclineiftheboundingboxesinthetargetdatasetlookdifferentfromthe
training distribution. Notable examples of single- and two-stage methods employing anchors are the R-CNN
[6,9]andYOLOfamily[10,8].
Heatmap-based Heatmap-baseddetectionisanobjectdetectionparadigmthatdoesnotrequireanchorboxes.
Here,aheatmap(aone-channelfeaturemap)isgeneratedovertheentireimageindicatingthepredictedloca-
tionsofthecornersorcenterofeachobject. Thepredictedvaluesofthisheatmapshouldbehighatspecified
objectlocations,suchastheobjectcenterorobjectcorners,andzeroeverywhereelse.
InCornerNet[11],top-leftandbottom-rightpairsofkeypointsindicatingboundingboxlocationsarepre-
dictedastwosetsofheatmaps(onepercategory, suchasmonkeyorhuman, toenablemulti-classdetection).
Toresolvetheambiguouscorrespondencebetweenpairsofkeypointswhenmultipleobjectsofthesameclass
are present, an additional pixel-wise embedding is trained to be similar at locations of corresponding points.
CenterNet(byDuanetal. [12])extendsthisapproachbypredictingthecenterpointoftheobjectinadditionto
thetwocornerkeypoints.
Another method also called CenterNet (by Zhou et al. [13]) predicts the centerpoint of each object as
a heatmap (one per category) and generates bounding box width and height in a different prediction head.
The center heatmap indicates at which position the width and height information should be read out. The
Fully-ConvolutionalOne-StageObjectDetector(FCOS[14])employsasimilarapproachtotheheatmap-based
approaches but uses all pixels that pertain to an object to estimate the location and size of its bounding box
insteadofjustconsideringthecenterorcorners.
Detectiontransformer:DETR Theoutputofmanyofthepreviousdetectionmethodscannotbedirectlyused
butneedstobefurtherprocessedtoavoidpredictingthesameboundingboxmultipletimesatnearbylocations.
Thisheuristicfilteringofredundantboundingboxesisanon-differentiableoperationand,dependingonwhere
in the framework it is employed, it might prevent the network from being trainable in an end-to-end manner
[6]. Thiscomplicatesthetrainingprocedureandcanhaveanegativeimpactonoveralldetectionperformance
[15]. The detection transformer (DETR) [16] introduces the concept of object queries, which can be used
20to make unique object predictions without the need for heuristic filtering or other hand-crafted components,
such as anchors. Each object query is a predefined or learned numerical vector. At its core, DETR relies
on a transformer that decodes these object queries into bounding boxes using image features (specifically, a
high-levelfeaturemap)fromaCNNbackbone. Sinceinformationexchangebetweenobjectqueriesispossible
through self-attention, duplicate matches to ground truth objects can be prevented by penalizing them in the
trainingobjective.
In practice, DETR has slow convergence (i.e. requires many training iterations) and high computational
demands due to the quadratic complexity of the transformer. Subsequent works aimed to tackle these issues
[17, 18, 19]. For example, in Deformable-DETR [17] each object query learns to attend to a small number
oflocationsthatarerelevantforobjectdetection,whichreducescomputationalcomplexityandleadstofaster
convergence.VariousimprovementsofDETRhavebeencombined,resultingintheDINOdetector1[20],which
iscurrentlythestateoftheartinobjectdetection.
PracticalDetectorTrainingConsiderations Besidesthemodel’sarchitecture, i.e. backboneanddetection
method, the training algorithm, training data and loss function are crucial factors for detection performance.
Over the years, multiple techniques have been developed. For example, one common problem of object de-
tection datasets is class imbalance, i.e., some object classes occur much more frequently than others. This
imbalanceincentivizesamodeltoimproveonfrequentratherthanrareclassesduringtraining. Focalloss[21]
addsafactortoeachclassterminthelossfunction,suchthatclassesthatarenotyetclassifiedthatwellreceive
more weight. Augmentations, i.e. random modifications to the images and labels, increase the variability of
thetrainingdataandhaveemergedasacrucialcomponentinobjectdetection. Classicaugmentationsinclude
random cropping and change of contrast. More recently, mosaic augmentation and mixup proved successful
in the context of object detection [22]. Another recent advancement is SiMoTA [22], an algorithm that seeks
tooptimizewhichpredictionsareconsideredpositiveandnegativeduringtraining. Usingthisadvancedlabel
assignmentstrategyhasapositiveimpactonbothtrainingtimeandmodelperformance.
DatasetsandBenchmarks ThestandardbenchmarkforevaluatinganobjectdetectionmodelistheCommon
Objects in Context (COCO) dataset [23]. Since virtually all published detection methods provide metrics for
theirperformanceonCOCO,itisausefulbenchmarktohelpresearchersselecttherightmodelintermsofac-
curacyandcomputationalefficiency. However,modelperformanceonCOCOmightnotbedirectlycomparable
toperformanceon tasks involvingthedetectionandindividualidentification ofanimals, asCOCOcontainsa
diversesetof80classes,andismoresuitedforbroaderclassificationproblems(e.g. classifyinganobjectasa
dogorabicycle,ratherthandistinguishingbetweensimilarlylookingindividualsofthesameclass,asrequired
forprimatedetectiontasks). FurtherpopularobjectdetectionbenchmarksareObject365[24]andLVIS[25].
Published, annotated datasets of images of primates are available, although these are much smaller than
the more commonly used datasets, such as COCO, and are often focused on keypoints. The MacaquePose
dataset[26]consistsofover13,000imagesof(severalspeciesof)macaqueswithlabeledkeypointsandmasks.
Maskscanbeconvertedtoboundingboxesbasedontheirmaximumextension. OpenApePose[27]offersover
70,000imagesofsixapespecieswithannotatedposes. TheAP-10K[28]andAPT-36K[29]datasetscontain
keypoint annotations for several primate species, as well as species from other orders. The Objects365 [24]
andLVIS[25]datasetsinclude“monkey”asanobjectclass,althoughinbothcases,thisclassincludesawide
variety of taxonomic groups, for example platyrrhini, catarrhini (both cercopithecoids and non-human apes),
andprosimians,aswellasartificialobjects,suchastoys,thatresembleprimates.
B.2 Multi-animaltracking
Tracking-by-detection Most work in the tracking-by-detection paradigm is based on the seminal SORT
model [30]. In this model, detections of each frame are collected independently of each other using Faster
R-CNN[6]. Thespatialshiftbetweendetectionsofsuccessiveframesisthenusedtoextractmotionfeatures.
Next, the Kalman filter algorithm [31] is applied which uses these motion features and the previous location
of an object to determine the predicted bounding box on the next frame. The overlap between the predicted
and detected bounding boxes is used to compute the affinity cost for all possible pairings. Finally, the model
associates the detections across frames by minimizing the affinity cost using Hungarian matching [32]. This
associationprocessiscontinuedforeachframeofthevideotoobtaintracks.
1thisshouldnotbeconfusedwithself-supervisedlearningmethodDINO
21Within the SORT framework, a number of models have emerged, each tweaking different components of
themodeltoachievesuperiorresults. Mostmodelsnowimplementmorepowerfulbackbones,suchasDarknet
[33,34],tofacilitatesuperiorfeatureextraction. Toimproveassociation,modelsoftenincorporateappearance
characteristicsofdetections[33,34,35]. Kalmanfiltershavealsoseenimprovements[34,36,35],andadaptive
thresholdsforpairingdetectionshavebeenestablished[33,37]. Somemodelsaccountforcameramotion[37]
andhavemademodificationstotheoverlapcomputationfortwoboundingboxes[38,39]. Inotherworks,affin-
itycostsarecomputedindirectlybyagraphneuralnetworkbasedonalldetectionsovermultipleconsecutive
frames[40,41].
Tracking-by-query Apart from the classical tracking-by-detection framework, there are also methods with
informationexchangebetweenthedetectionandassociationphase. Thisinformationexchangehasbeenfacili-
tatedbytheadventoftransformerstructuresincomputervision.SpecificallywiththeintroductionofDETR[16]
inobjectdetection,thenumberofmodelsimplementingthistrackingapproachhasseenasignificantincrease
[42, 43, 44, 45]. When using the DETR architecture (see Section 2.1) for tracking, the resulting embeddings
fromthetransformerdecoderarenotonlymappedontoboundingboxesormasks. Rather,theyarealsoreused
asadditionaltrackqueries,whichguidethedetectionandassociationprocessinthefollowingframe. Thisway,
motionandappearanceinformationfrompreviousframesistakenintoaccountforbothdetectionandassocia-
tion. SignificantprogressinthisareahasbeenmadewithMOTRv2[46],whichusesexternaldetectionsfrom
theYOLOXdetectionmodel[22],thatareassociatedandpotentiallyrefinedusingtrackqueries. Additionally,
an improved variant of DETR [19] contributes to enhanced processing. Most recent works even eliminated
the need for an external object detector in this architecture by employing a specialized training strategy that
reconcilesthedetectionandassociationobjective[47,48].
DatasetsandBenchmarks Sincemosttrackingprojectsareinterestedintrackinghumans,theyaretypically
evaluatedonhumanmulti-objecttrackingbenchmarks, suchasMOT17[49], adatasetofannotatedvideosof
pedestrianswalkingthroughthestreets,orDanceTrack[50],whichcontainsannotatedvideosthatshowdancers
and,therefore,humansengagedinmorecomplex,non-linearmotionpatternsthaninMOT17. Furthermore,the
dancersinavideotendtobesimilarlyclothed,whichmakesthetrackingtaskevenharder. Modelsperforming
wellonDanceTrackmaybegoodcandidatesforanimaltrackingtasksbecauseinthisdomainwealsoexpect
similarappearancesandhighlynon-linearmotionpatterns. Recently,thefirstdatasetsolelydevotedtotracking
groupsofanimalswasintroduced(AnimalTrack,[51]).However,primatevideosarenotincludedinthisdataset.
B.3 Individualidentification
Unlikecorecomputervisiontaskslikedetectionandtracking,therearefeweranimalidentificationbenchmarks
and methods. Training a classification or metric-based model for individual identification requires annotated
imagesorvideosofanimalsaloneorinagroup. Annotationinvolvesthelocalizationofananimalinaframe,
e.g. by drawing a bounding box, and then assigning an identity to it. Previous studies have demonstrated
promisingidentificationresultswithannotatedtrainingsetscontainingmerelytenstohundredsofexamples[52,
53,54],whileotherworkshaveusedthousandsoftrainingexamplesperindividual[55,56]. Theexactnumber
ofrequiredannotatedexamplesdependsonthecomplexityofthetaskandthedesiredmodelaccuracy. When
applyingidentificationmodelstounseendata, itcanbechallengingtorecognizeindividualsinsingleframes,
duetoocclusionornon-standardposes. Therefore,researchersoftenutilizecameratrapswhichproducebursts
ofseveralimagestakeninquicksuccessionwhentheydetectmotion[57].Whenseveralimagesfromasequence
areavailable,themodelpredictionscanbeaggregated,whichcanleadtoimprovedperformancecomparedto
when model predictions are based on single images [58]. This aggregation technique can be combined with
multi-objecttrackingmethods, sothatthemodelonlyhastobeverycertainonafewframestopropagatethe
identificationresulttoallotherframesofthevideo.
DatasetsandBenchmarks Mostscientistsinterestedinidentificationtasksworkwiththeirowndatasets,as
their use cases tend to differ strongly with respect to animal species, research question, camera type, back-
groundsoftherecordingsandlighting. Primate-relatedbenchmarksincludeaface-basedidentificationdataset
of lemurs [52] and a dataset containing, among other animals, chimpanzees and gorillas with tracking and
identificationlabels[59],therecentlyreleasedChimpACT[60]containslongitudinaldataofagroupofchim-
panzeesinazoo. Additionally, benchmarksonpersonandotherspeciesre-identificationcaninspiremethods
fornon-humanprimateapplications[61,62,63].
22B.4 Actionunderstanding
Actionrecognition Actionrecognitionmodelsclassifywhatactionavideosequencecontains. Earlyaction
recognition methods primarily rely on 3D-convolutions on RGB inputs [64, 65, 66]. For example, SlowFast
[66]convolvesoverlowandhighframerateinputtocapturehigh-levelsemanticsandfinemotions. Themain
drawbackofconvolutionalnetworksistheirlimitedreceptivefieldwhichisofparticulardisadvantageinvideo-
understandingtasksthatoftenrequirelong-rangetemporalreasoning. Therefore,therehasbeenashifttowards
transformer networks which benefit from stronger temporal modeling capabilities. The pioneering works in
thisarea[67, 68]closelyfollowthedesignofVisionTransformer[4]. Totackletheadditionalcomputational
cost imposed by the time dimension, the authors proposed several methods to minimize the memory load,
suchastubeembedding,framesubsamplingandvariousattentionfactorizations. Subsequenteffortsextended
these transformer models by including feature pyramids [69, 70, 71], by enhancing their long-term modeling
capabilities[72]orbyimprovingefficiency[73,74]. Someworksalsocombinedthemeritsofconvolutionand
transformermodelsbymodelinglocalandglobalfeaturesinshallowanddeeplayersrespectively[75,76].
Anotherlineofresearchapproachesactionrecognitionbymodelingmotion-relatedfeatures. Earlyworks
incorporated optical flow features in convolutional architectures [77, 78, 79] while more recently there have
beenattemptstodesignneuralnetworkmodulestocomputemotionfeatures[80, 81]. Furthermore, thereare
also methods which use motion information to guide feature extraction [82, 83, 84, 85, 86, 87, 88, 89]. For
example, efforts have beenmade toaggregate featuresalong motion trajectories[82, 83, 84], whichaccounts
for the fact that the same object may appear in different locations in different frames across time due to the
movement of the object. Another strand of literature models interactions between objects [90, 91, 92, 93].
Forexample,theaction“feedingfrombowl”canbenefitfromexplicitlymodelingtheinteractionbetweenthe
feedingindividualandthebowl. Whileearlyapproachesrelyonexternalobjectdetectors[90,92],morerecent
frameworks leverage attention to model interactions implicitly [93]. Although the modeling of motion and
action-related features provides suitable inductive biases for action recognition, performance often falls short
compared to the more generic transformer-based video backbones. This is especially true if these backbones
arepre-trainedwithstate-of-the-artself-supervisedmethods[94,95].Nevertheless,theexplorationofinductive
biasesforvideo-understandingtasksremainsanimportantresearchtopicsincetheycanbehelpfulinensuring
adecentmodelperformanceinthefaceoflimitedtrainingdata. Incontrast, genericvideobackbonesusually
requirehugeamountsoflabeleddatawhenbeingtrainedfromscratch.
Just as for animal localization methods, benchmarks for action recognition typically focus on datasets of
videos of humans [96, 97, 98, 99, 100, 101, 102]. These benchmarks consist of short videos (a couple of
seconds)thatarelabeledwithoneormoreactionlabels. However,recentlyananimalactionrecognitionbench-
markhasbeenpublishedthatcontainsvideo-levelactionlabelsforadiversesetofanimalspecies[103]. The
datasetdoesnotincludeprimatevideos.
Temporalactiondetection Temporalactiondetectionmethodsassignactionlabelstotemporalintervalsin
avideo. Theycanbedividedintosingle-andtwo-stagemethods,analogoustodetectionmethods. Two-stage
methods first generate temporal proposals which are subsequently classified and potentially refined. Single-
stagemethodsdirectlyproducepredictionswithoutthein-betweenproposalgenerationstep.
Early proposal generation methods are based on anchors, i.e. multiscale temporal intervals centered at
varioustemporallocations[104,105].Forexample,[104]obtainproposalsbyclassifyinganchorsascontaining
ornotcontaininganactionbasedonthefeaturesassociatedtotheanchor.Sinceanchorsmightbetoorestrictive
tocapturethelargetemporalvarietyingroundtruthactioninstances,subsequentworksdevelopedanchor-free
proposalgenerationmethods. Intheirpioneeringwork, [106]predictstarting, endingandactionprobabilities
at individual temporal locations and arrive at proposals by grouping according to these scores. Empirically,
groupingbasedonlow-levelpredictionscanbeunreliable,forexampleduetosceneswitches. Thisissuecanbe
counteractedbyenhancingthetemporalcontextmodelingcapabilitiesoremployingregularizationtechniques,
which has been pursued in subsequent works [107, 108, 109, 110, 111, 112]. For example, [111] employ a
graph-convolutionalnetworkandatransformertomodellocalandglobalrelationshipsbetweenvideosnippets,
respectively. [112]regularizeproposalsbyenforcingconsistencybetweenandwithinstarting, actionnessand
endingphases. Methodsthattakeproposalsasinputeitheroperatedirectlyonproposal-levelfeatures[113]or
furtherenhancethembyproposal-contextmodeling[114,115,116]toproducethefinalpredictions.
Earlysingle-stagemethodswerealsobasedonanchors[117,118]. Forexample,thefirstworktopropose
single-stage processing [117] directly regressed action boundaries and class confidence from anchors similar
toSSDforobjectdetection[7]. Morerecently,anchor-freemethodshavedemonstratedsuperiorperformance
[119, 120, 121, 122, 123, 124, 125, 126]. In this line of research, attention-based methods that operate on
23snippet-levelfeaturescomputedbypre-trained3D-convolutionalnetworksareapopularapproach. Forexam-
ple,ActionFormer[124]successivelyemploystransformerandsubsamplingblockstosnippet-levelfeaturesto
createafeaturepyramid,whichfacilitatesthedetectionofactionsatdifferenttemporalscales. Theresultingto-
kensareusedtoregresstherelativeoffsettotheactionboundariesandclassconfidences.Recently,transformers
havebeencriticizedfortheirinabilitytoattendtocrucialframesamongframeswithsimilarappearance[125].
Morerecentapproacheshavethereforereplacedattentionwithsimplertemporalcontextmodelingtechniques
while achieving state-of-the-art results [125, 126]. For example, TemporalMaxer [125] follows the same de-
sign as ActionFormer but replaces the transformer layers with simple max-pooling operations. Remarkably,
theseapproachesevenachievestate-of-the-artperformanceindensepredictiontaskswheretherearemultiple
overlappingactions,outperformingmethodsspecificallydevelopedforthesetasks[127,128,129,130,131].
Therearevariousbenchmarksforregular[132,133]anddense[134,135,96,97]temporalactiondetection.
Asusual,theactorsinvolvedaretypicallyhumansandwearenotawareofanyanimal-relatedbenchmarks.
Spatio-temporalactiondetection Existingspatio-temporalactiondetectionmethodscanbedividedintotwo
categories. First,therearetrack-basedmethodsthatidentifytracksofactorsinavideoandassignactionlabels
tosub-tracks. Second,thereareframe-basedmethodsthatleveragespatio-temporalinformationtoassignone
ormoreactionstodetectionsinindividualvideoframes.
Earlymethodsfortrack-basedspatio-temporalactiondetectionusuallyemployed2D-convolutionsonframe-
levelRGBinputsandopticalflowfeaturestoobtainactionlocalizations,whicharethenlinkedviatrackingal-
gorithms[136,137,138]. Subsequentworksimproveduponthisapproachbyleveragingtemporalinformation
[139, 140, 141, 142, 143, 144]. Thisisdonebydirectlypredictingactiontracksonshortvideosegmentsand
subsequently linking them based on overlap to obtain video-level action tracks. For example, this paradigm
wasadoptedbySTAR[144],whichusesasetofspatio-temporalqueriestodecodeactiontracksfromsegment-
levelvideofeatures. Notably,themethodcanbesupervisedwithactiontrackssparselyannotatedonindividual
keyframes,whichareeasiertoobtainthanexhaustiveannotations. Anotherrecentwork[145]representsdetec-
tionsinindividualframesbytheirappearanceandestimated3Dposefeatures.Alldetectionsinavideosegment
aretheninputtoastandardtransformerarchitecturetopredictframewiseactionclasseswhicharelinkedwitha
3Dtrackingalgorithm.
In contrast to track-based methods, frame-based spatio-temporal action detection aims to detect actions
on individual keyframes. A separate tracking algorithm is needed to link these detections into action tracks.
The standard frame-based approach [146] can be divided into three steps. First, a pre-trained object detector
like faster R-CNN [147] is used to obtain bounding boxes of actors in a frame. Then, each bounding box is
symmetricallyextendedalongthetemporaldimensiontoobtaina3Dregionofinterest,whichisusedforfeature
extraction from a video backbone. Lastly, the extracted features are used for multi-class action classification.
Thisprocedurehasbeenemployedincombinationwithvariousvideobackbones[66,148,4,70,72]andself-
supervisedpre-training[94,95].
However,therehavealsobeeneffortstodevelopmoresophisticatedmodelingframeworksforframe-based
spatio-temporalactiondetection. Earlyapproachesfocusedonmodelingactor-context[149,150]orlong-term
temporal [151] relations. To illustrate why it is useful to model these two types of relations, consider an in-
dividual that is feeding. In this case, contextual cues, such as the proximity of a feeding bowl, or temporal
information,suchastheindividualtakingthefoodacoupleofsecondsbefore,areindicativeoftheperformed
action. Subsequentresearchacknowledgedthatbothcontextandtemporalrelationsmustbemodeledsimulta-
neouslytomakefurtherprogress[152,153]. Forexample,[153]constructaspatio-temporalgraphwithactor
and context detections from successive video snippets and employ attention-based message passing. The en-
riched actor-features are then used for action-classification. Other works include more intricate relationships
in their models, while employing a memory module for temporal reasoning. For example, ACAR-Net [154]
includes higher-order interactions between actors and contextual objects and HIT [155] uses both RGB and
posefeaturestomodelhand,objectandpersoninteractions. Notably,allofthesemoresophisticatedmodeling
attemptsperformworsethanthestandardthree-stepframe-basedapproachwithtransformer-basedbackbones
andself-supervisedpre-training.
Benchmarksfortrack-basedspatio-temporalactiondetection, includetheJ-HMDB-21[156], UCF101-24
[99], MultiSports [157] and AVA datasets [146, 158]. However, the AVA dataset only contains sparsely an-
notated action tracks at one frame per second and, therefore, has been primarily used for the evaluation of
frame-basedapproaches. Wearenotawareofanyanimalrelatedbenchmarks.
24Dynamic scene graph generation Methods for dynamic scene graph generation (DSGG) localize animals
and objects and predict relationship triplets of the form entity–relation–entity, where entities can be animals
or objects. There are two paradigms for DSGG: In track-based methods, entities are tracked over time and
relationsarepredictedforsub-tracksoftwoentities. Inframe-basedmethods,relationsareonlypredictedfor
individualframesofavideo. Itshouldbenotedthatthereisalsoanextensiveliteratureoninteractiondetection
and scene graph generation methods for still images, however, since most relations and actions can be more
easily identified when temporal information is available, we focus on video-based methods in this work and
refertorecentsurveypapersforanoverviewofimage-basedapproaches[159,160].
Irrespective of the paradigm, all DSGG methods roughly follow the same procedure: First, features are
extractedfor entitiesand relations. The entityfeaturesare usuallyobtained fromimage- orvideo-backbones,
whilerelationsareoftenrepresentedbytheclasslabels,unionfeatures(i.e. featuresofbothinvolvedentities)
andrelativemotionofsubjectsandobjects. Inasecondstep,theextractedfeaturesarefurtherprocessedusing
some form of spatio-temporal reasoning. Finally, the features are used to separately predict the entity and
relationclasses. Ajointpredictionoftherelationship-tripletisavoidedbecausethenumberofpossibleclasses
wouldbeintractable.
Earlytrack-basedmethodsdividetheinputvideointoshort,overlappingsegmentsandgeneratescenegraphs
foreachsegmentseparately[161,162,163]. Video-levelscenegraphsarethenobtainedbymergingsegment-
levelrelationship-tripletsifthecorrespondingtrackshaveasufficientoverlapandthepredictedrelationisthe
same. Many subsequent works generally maintained this paradigm, while trying to improve on individual
aspects [164, 165, 166, 167]. For example, [165] realized that the proposed merging algorithm is not robust
to segment-level errors. Therefore, they proposed to preserve multiple relationship-triplet hypotheses at each
mergingstepanddiscardunlikelyoneswhensufficientinformationisavailable. However,theproblemremains
thatscenegraphsaregeneratedseparatelyforeachsegmentwithoutusingtheinformationfromothersegments,
whichprevents thelearningof inter-segmentdependencies. For example, thereis oftenanatural sequenceof
relations(e.g. person–take–cupfollowedbyperson–lookat–cup). Thisproblemhasbeenaddressedby[168]
who used two separate graph-convolution-based modules for segment-level inter-entity modeling and video-
levelinter-relationmodelingtoachievestate-of-the-artresultsfortrack-basedDSGG.Inadditiontothiswork,
therehavealsobeeneffortstodiscardthesegment-levelparadigmaltogether[169,170,171,172].Forexample,
[172]performtrackingonthecompleteinputvideoandobtainsub-tracksusingaslidingwindowapproachwith
multiple kernel sizes. Then, a pair proposal module is used to identify pairs of sub-tracks that relate to each
other. In the last step, the entity and relation classes of the identified pairs are predicted. Although methods
thatoperateoncompletevideosfacilitateglobalprocessing,theyhaveproblemsoftheirown. Forexample,the
creationoflongtracksispronetoerrors,whichcanhaveanegativeimpactonDSGGperformance. Inaddition,
thefeaturesoflongtracksmustbeheavilycompressedtobeprocessable,resultinginalossofinformation.
Frame-basedmethodsaredominatedbytransformermodels. Inearlyapproaches,thedependencybetween
all relation or entity features of neighboring frames is modeled [173, 153, 174]. For example, [174] use two
separate but identical transformer architectures for intra- and inter-frame relation-modeling. Although this
approachisrefreshinglysimple,itsuffersfrommemoryissuessincethetemporalreasoningisdoneinafully-
connectedway. Methodsalongthislinearethereforerestrictedtooperatingonasmallnumberofframes. In
contrast, morerecent methodsrely on someform of trackingto perform temporalmodeling foreach subject-
objectpairseparately[175,176,177],thusallowingfortheconsiderationofabroadertemporalcontext.
Benchmarks for track-based DSGG include the VidVRD [163] and the VidOR [178] datasets, while for
frame-basedDSGGthereistheActionGenomedataset[179].Allofthesebenchmarkdatasetsconsistofvideos
containingvarioushuman-centeredrelations.Theserelationsincludeactions,butalsospatialarrangements.We
arenotawareofanyanimalrelatedbenchmarks.
C Methods for effort-efficient learning
C.1 TransferLearning
Transferlearninginvolvesleveragingknowledgegainedfromonetasktoimprovelearningandperformanceon
arelated,butdifferent,task.Byreusingpreviouslyacquiredfeaturesormodels,transferlearningaddressesdata
scarcityandacceleratesmodeltraining. Thisapproachisparticularlyvaluableinscenarioswherelabeleddata
forthetargettaskislimitedorexpensivetoobtain.
Traditionally,modelswerepre-trainedthroughsupervisedlearningonsourcetaskswithlabeleddata[180,
181, 182, 183], transferring knowledge to target tasks with limited labeled data. In the realm of computer
25vision,aprevalenttransferlearningapproachinvolvespre-trainingonanextensiveanddiversedatasetsuchas
ImageNet[184],MS-COCO[23],orOpenImagesDataset[185],followedbyfine-tuningonanarrowertarget
dataset. However, recent advancements have introduced the use of unlabeled data as a pivotal paradigm for
pre-trainingintransferlearning[186, 187, 188, 189]. Thesemethodsallowmodelstobepre-trainedonlarge
amountsofunlabeleddatabypredictingorreconstructingcertaindataaspects,eliminatingtheneedforexplicit
labels. Amoreelaboratediscussionoftheseself-supervisedlearningmethodsispresentedinAppendixC.2.
Modern successors of transfer learning have reached zero-shot transfer capabilities, which means that no
examples are required to fine-tune the model to the target task. Instead, the diversely-pretrained foundation
modelisabletoadapttoatargettaskdirectly. Language-visionmodelslikeCLIP[182],EVA[190],Florence
[191] and Flamingo [192] are big models which are trained on large-scale datasets, usually involving text
and associated images obtained from the internet. These models have a good general understanding of many
object classes and often achieve a zero-shot performance that is on par or better than smaller models trained
specifically for this task. An advantage of these models is their strong ability to use information encoded in
naturallanguage,sothatcomplexquestionsaboutanimageorvideocouldbeformulatedasatext,loweringthe
technicalrequirementsontheusersideforworkingwithsuchasystem.
C.2 Self-supervisedlearning
Theobjectiveofself-supervisedlearningistoderivelabelsdirectlyfromthedata,enablingthemodeltolearn
meaningful representations from large, unlabeled datasets. These representations serve as valuable resources
fortraininggeneral-purposemodelsthatcanbeappliedtoawiderangeofdownstreamtasks. Therefore,self-
supervisedlearningtechniquesproducemodelswhichcanbeusedasfoundationsfortransferlearning. Inthe
followingparagraphs, wedelveintoseveralstate-of-the-artapproachesinself-supervisedlearning, whichcan
bebroadlyclassifiedintothreecategories: pretexttasklearning,jointembeddingarchitectures,andgenerative
approaches.
Pretexttasklearningtrainsmodelsontaskslikepredictingimagepatchpositions[193]orimagerotations
[194] to learn data features without labels. These tasks promote understanding of object features and spatial
relationshipsbutposechallengesintaskdesignandpotentialover-specializationthatmayimpairfutureperfor-
manceontaskssuchasclassificationorregression.
In joint embedding architectures, representations are learned by minimizing the deviation between simi-
lar samples and maximizing the deviation between dissimilar samples provided as training data. Typically, a
contrastivelossisoptimizedbetweenaugmentedpairsofsimilaranddissimilarsamples. TheSimCLR[186]
framework is a popular contrastive learning framework that led to the development of many succeeding con-
trastivelearningapproachesbasedonasimilaridea. Themainideaoftheframeworkistomaximizesimilar-
itybetweendifferentversionsofsimilarsamples(createdthroughaugmentationtechniqueslikecroppingand
rotating),whileminimizingthesimilaritybetweenthedissimilarsamplepairs. However,thiscontrastiveself-
supervisedlearningmethodrequiresalargenumberofdissimilarexamplesfortrainingtopreventcollapseof
therepresentationintotrivialsolutions,whereallinputsmaptothesameoutput,anditrequiressubstantialbatch
sizesforoptimalperformance.Recentstudies,includingSimSiam[187],BYOL[195],BarlowTwins[188]and
DINO[196]havemadesignificantadvancementsinthefieldofself-supervisedlearningbyeliminatingtheneed
fordissimilarornegativesamples. Thesemethodsemployvarioustechniques,suchasmomentumencoderand
cross-batch memory banks, to learn representations without explicitly utilizing negative pairs. VicReg [197]
addressestheissueofcollapsingbyintroducingaregularizationtermthatfocusesonthevarianceoftheembed-
dings. Notably, these techniques have exhibited competitive performance when compared to SimCLR across
diversebenchmarkdatasets.
A noteworthy generative method, known as Masked Autoencoders (MAE) [198], has shown immense
promise in the realm of self-supervised learning. The fundamental concept underlying MAE involves mask-
ingrandompatcheswithinaninputimageandsubsequentlyreconstructingthemissingpixels. Buildingupon
the success of this approach in image-based downstream tasks, researchers have extended this idea to video-
based pre-training [95, 199]. A combination of masked image modeling and contrastive learning is found in
iBOT[200]andCMAE[201].
C.3 Weaklysupervisedlearning
Perfect annotations are beneficial for model training, but not always available. In general, imperfections can
haveseveralorigins,forexample,badannotationquality,partlywrongannotations,orlabelsthatwereproduced
forasimilarbutnotexactlythedesiredtaskamongothers. Thegoalofweaklysupervisedlearningistofind
26aneffectivewaytoextractinformationfromsuchimperfectdatatosolvethedesiredtask. Weaklysupervised
learning has been used in a wide array of computer vision tasks, including object localization [202], image
classification[203],poseestimation[204],actionlocalization[205]andsceneunderstanding[206].
Giventheextensiveannotationeffortnecessaryinthecontextindividualizedbehavioralanalysis,themost
importantapplicationofweaklysupervisedlearninginvolvestheuseofcoarse-grainedlabelsforatargettask
with a finer granularity. For example, the goal might be to train a model for object localization using only
information about how many individuals are present in an image. Current methods used for coarse-to-fine-
grained weakly supervised learning are mostly based on one of two paradigms: Multiple Instance Learning
(MIL)andClassActivationMaps(CAM).
InMIL-basedapproaches[207,208]severalimages(orframesofavideo){X ,...,X }arecombinedinto
1 n
bags. Labels must only be provided at the bag level, i.e. when any of the images contain the labeled object,
the complete respective bag receives this label. For example, the label “dog” would be given to the bag if
thebagcontainsatleastoneimagewithadog. Themodelshavetofindoutwhichsubsetofimagesandwhich
imageregionsareresponsibleforpredictingthegivenlabelbycomparingpositivetonegativebags.CAM-based
approaches[209,210,211]useimage-levellabelstotrainaCNN.Afterwards,thelastconvolutionallayerscan
beusedtoseewhichareasoftheimagearemostresponsibleforthecorrectpredictionandcanbeusedtolocate
theobjectofinterest. Thereareadditionalideasthathelptoimproveperformancefurther. Forexample,some
modelsaddadditionalstepstopreventthemodelfromonlydetectingthemostdiscriminativeregion[212,207].
Insteadofcreatingtheclassactivationmapsfromthelastfeaturemap,othermethodsgetimprovedresultsfrom
addingfeaturesfromearlierlayers[213].
C.4 Semi-supervisedlearning
Semi-supervisedlearninginvolvesusingbothlabeledandunlabeleddatatotrainamodel. Thisisusefulinsce-
narioswherelabeleddataisscarceorexpensivetoobtain,butunlabeleddataisreadilyavailable. Thelabeled
dataareusedtotrainamodeltorecognizearelationshipbetweenthemodelinputs(e.g. imagesofamonkey)
andlabels(e.g. actiondisplayedonanimage),whilethelargersetofunlabeleddata(e.g. furtherimagesshow-
ingactions)isusedtogeneralizethelearnedfeatures. Comparedtoonlyusinglabeleddata, amorepowerful
androbustmodelcanbeobtained. Acomprehensivereviewofsemi-supervisedlearningtechniquesisprovided
by Yang et al. [214]. Trivially, semi-supervised learning can be implemented by using self-supervised meth-
odslikecontrastivelearning(seeAppendixC.2)followedbysupervisedfine-tuning. Furthersemi-supervised
approaches that we consider especially relevant for the use of computer vision in the field of primate video
analysescanbecategorizedaspseudolabelingandconsistency-based.
Pseudo labeling follows the basic idea that, at first, only the labeled data are used to train a model (the
“teacher” model). After applying the model to the unlabeled data, the most confident predictions are used
as pseudo labels, and a new model (the “student” model) is trained with the labeled data and the part of the
unlabeleddataforwhichconfidentpseudolabelswerecreated[215]. Forexample,Chenetal.[216]integrates
thisstudent-teacherapproachinathree-steppipelinetomakeoptimaluseoftheavailableunlabeleddata. The
steps are (1) self-supervised pre-training using unlabeled data, (2) fine-tuning with the labeled data and (3)
usingpseudolabelsgeneratedbythefine-tunedmodeltotrainamorerobuststudentmodel. Consistency-based
methods aim to make a model invariant to input perturbations that do not change the semantic content. For
example,Rasmusetal.[217]combinedasupervisedtrainingobjectivewithareconstructiontaskwherenoise
isaddedinvariouslayersoftheneuralnetworktoobtainamorerobustmodel.
Many popular semi-supervised methods are hybrid. These methods typically generate pseudo labels by
feedingtheoriginal(e.g.MeanTeacher[218])orweaklyaugmented(e.g.MixMatch[219]andFixMatch[220])
imagestoateachermodel. Duringtrainingofthestudentmodel,consistencyisenforcedbyadding(stronger)
data augmentations to the input. FlexMatch [221] further develops FixMatch by adding curriculum pseudo
labeling. Insteadofdefiningafixedconfidencethresholdforwhichpseudolabelsshouldbetakenintoaccount
during training, the threshold is determined adaptively based on the current learning status of the model. In
recentyears,pseudolabelingandconsistencybasedmethodshaveimprovedsubstantiallyinseveralcomputer
visiontasksbyusingVisionTransformers[4]asstudentandteachermodels[222,223].
Other semi-supervised approaches use generative models, either with Generative Adversarial Networks
(GANs) [224] or Variational Autoencoders [225, 226]. The basic idea is to use the generative power of the
modeltocreateadditionaldatasampleswhichcanhelptoimprovethemodelperformance. Theunlabeleddata
areusedtohelpthegeneratortocreatesamplesthataresimilartorealdatapoints.
27C.5 Activelearning
In situations where researchers have large amounts of unlabeled data and a limited labeling budget, active
learning can help the researcher to choose, in an automated fashion, which samples should be annotated and
addedtothetrainingsettooptimallyimprovethemodel.Thebasicideabehindactivelearningistostrategically
choosedatapointsthatarelikelytoprovidethemostvaluableinformationforimprovingthemodel’saccuracy
orreducingitsuncertainty.Often,activelearninghappensinaniterativemanner,wheremodeltrainingandnew
sampleselectionalternate.However,italreadyshowsbenefitsduringthefirstiteration,aswell-selectedsamples
for labeling can yield substantial improvement over random samples [227]. Several active learning strategies
havebeenproposedandreviewedforclassification[227],andobjectdetection[228,229]. However,recently,
firstapplicationsinothercomputervisionfieldshaveemerged,suchasactivelearningfortracking[230],action
recognition [231] and scene graph generation [232]. Here, we structure currently existing methods into three
mainbranches: classicalquery-by-committee-based,uncertainty-based,diversity-based.
A classical approach is query-by-committee [233, 234, 235], which involves maintaining a committee of
severalmodelsandselectingunlabeleddatapointsthathavehighdisagreementamongthemodels.Uncertainty-
based strategies use a single model and rely on the idea that samples on which the model performs poorly
should be labeled and added to the pool of training data. In classification tasks, this can be based on the
overallentropyofthepredictedlabelsorthedifferencebetweenthetwomostconfidentpredictedlabels. One
ofthecurrentlybestperforminguncertainty-basedmethodsisLossPredictionLoss(LPL)[236]whichinvolves
attaching a small parametric module, called the “loss prediction module” to the target network. It learns to
predict target losses of unlabeled inputs and helps to identify data points that are likely to result in incorrect
predictions by the target model. Diversity-based (or representation-based) strategies follow the idea that, by
having representative samples from the whole dataset in the pool of labeled data, one has the best chance to
haveawell-performingmodelonunseendata. Theyareoftenbasedonclusteringtechniques,suchasKMeans
[237]. CoreSet[238]findsdatasamplesinamannerthattheselectedsubsetofsamplesisrepresentativeforthe
remaining dataset. Different strategies are recommended depending on the size of the labeling budget [239].
Hybridmethodscombinethepreviousideasinvariousways. Forexample,manyhybridmethodsarebasedon
two-stepoptimization,wheretheuncertainty-anddiversity-basedcriteriaaretakenintoaccountandoptimized
inanalternatingmanner[240,241].
C.6 Human-machinecollaborativeannotations
Finally, labelingeffortcanbereducedbyselectingthemostappropriatetoolsforannotation. Moreandmore
annotationtoolsincludeexistingdeeplearningmodelsintotheirannotationworkflowstoacceleratetheprocess
andreducemanualhumanlabor. Incontrasttoactivelearning,whereamodelrecommendswhichdatasamples
wouldbemostbeneficialforlabeling,orsemi-supervisedlearning,wherepseudolabelsaresuggested,herewe
presentsomeideasonhowannotationtoolsthemselvescansupporthumanannotatorsbyincorporatingmodels
inaninteractivemanner.
Effort-efficiencyinthiscontextcanbeachievedthroughsmartstrategiessuchashavingtheannotationtool
provide initial annotation guesses (e.g. through model outputs [242]), heuristics [243], gamification [244] of
user input [245] or linearly interpolating between two annotated frames in a video [246]. Effective and user-
friendlydesignofannotationtoolscanalreadycontributetoimprovements. OnepromisingmethodisSegment
Anything (SAM) [247], which automatically, or with minimal user input such as clicking on an object in an
image, generates segmentation masks of objects and has shown good results on a broad range of images and
object classes, significantly reducing manual annotation efforts. The labeling software CVAT [246] includes
theoptiontosegmentobjectswithSAM,dramaticallyreducinglabelingtimeforinstancemaskscomparedto
traditionalmethods,asanimaloutlinescanbeobtainedbyasingleclickratherthanmanualdelineation.
D GPT4-V Image Descriptions
Weconductedasmallstudy(Fig.7)toassessthecurrentcapabilitiesofGPT4-V,arguablythestrongestmulti-
modallargelanguageavailable,forimagestakeninthefield. Whilesomepartsofthedescriptionsareremark-
ablygood,thereareoftenmistakesindetailsorinthenumberofanimalspresent. Henceweconcludethat,at
thecurrentstate,suchmodelsareinsufficientasastandalonetoolforprimatebehavioranalysis.
28Figure7: GPT4-Voutputwhenaskedtodescribeimagesfromin-the-wildrecordings.
29Appendix References
[1] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. “Learning representations by back-
propagatingerrors”.In:nature323.6088(1986),pp.533–536.
[2] KaimingHeetal.“Deepresiduallearningforimagerecognition”.In:ProceedingsoftheIEEEconfer-
enceoncomputervisionandpatternrecognition.2016,pp.770–778.
[3] Tsung-Yi Lin et al. “Feature Pyramid Networks for Object Detection”. In: 2017 IEEE Conference on
ComputerVisionandPatternRecognition(CVPR)(2016),pp.936–944.
[4] Alexey Dosovitskiy et al. “An image is worth 16x16 words: Transformers for image recognition at
scale”.In:ICLR(2021).
[5] AshishVaswanietal.“AttentionisAllyouNeed”.In:NeuralInformationProcessingSystems.2017.
[6] ShaoqingRenetal.“FasterR-CNN:TowardsReal-TimeObjectDetectionwithRegionProposalNet-
works”.In:IEEETransactionsonPatternAnalysisandMachineIntelligence39(2015),pp.1137–1149.
[7] Wei Liu et al. “Ssd: Single shot multibox detector”. In: Computer Vision–ECCV 2016: 14th Euro-
peanConference,Amsterdam,TheNetherlands,October11–14,2016,Proceedings,PartI14.Springer.
2016,pp.21–37.
[8] JosephRedmonandAliFarhadi.“YOLOv3:AnIncrementalImprovement”.In:ArXivabs/1804.02767
(2018).
[9] KaimingHeetal.“MaskR-CNN”.In:IEEETransactionsonPatternAnalysisandMachineIntelligence
42(2017),pp.386–397.
[10] JosephRedmonandAliFarhadi.“YOLO9000:Better,Faster,Stronger”.In:2017IEEEConferenceon
ComputerVisionandPatternRecognition(CVPR)(2016),pp.6517–6525.
[11] HeiLawandJiaDeng.“CornerNet:DetectingObjectsasPairedKeypoints”.In:InternationalJournal
ofComputerVision128(2018),pp.642–656.
[12] KaiwenDuanetal.“CenterNet:KeypointTripletsforObjectDetection”.In:2019IEEE/CVFInterna-
tionalConferenceonComputerVision(ICCV)(2019),pp.6568–6577.
[13] Xingyi Zhou, Dequan Wang, and Philipp Kra¨henbu¨hl. “Objects as Points”. In: ArXiv abs/1904.07850
(2019).
[14] ZhiTianetal.“Fcos:Fullyconvolutionalone-stageobjectdetection”.In:ProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision.2019,pp.9627–9636.
[15] PeizeSunetal.“Whatmakesforend-to-endobjectdetection?”In:InternationalConferenceonMachine
Learning.PMLR.2021,pp.9934–9944.
[16] Nicolas Carion et al. “End-to-End Object Detection with Transformers”. In: European Conference on
ComputerVision(ECCV)abs/2005.12872(2020).
[17] Xizhou Zhu et al. “Deformable DETR: Deformable Transformers for End-to-End Object Detection”.
In:ArXivabs/2010.04159(2020).
[18] Feng Li et al. “DN-DETR: Accelerate DETR Training by Introducing Query DeNoising”. In: 2022
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)(2022),pp.13609–13617.
[19] Shilong Liu et al. “DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR”. In: ArXiv
abs/2201.12329(2022).
[20] HaoZhangetal.“DINO:DETRwithImprovedDeNoisingAnchorBoxesforEnd-to-EndObjectDe-
tection”.In:ArXivabs/2203.03605(2022).
[21] Tsung-YiLinetal.“FocalLossforDenseObjectDetection”.In:IEEETransactionsonPatternAnalysis
andMachineIntelligence42(2017),pp.318–327.
[22] ZhengGeetal.“YOLOX:ExceedingYOLOSeriesin2021”.In:ArXivabs/2107.08430(2021).
[23] Tsung-Yi Lin et al. “Microsoft COCO: Common Objects in Context”. In: European Conference on
ComputerVision.2014.
[24] Shuai Shao et al. “Objects365: A Large-Scale, High-Quality Dataset for Object Detection”. In: 2019
IEEE/CVFInternationalConferenceonComputerVision(ICCV)(2019),pp.8429–8438.
30[25] Agrim Gupta, Piotr Dolla´r, and Ross B. Girshick. “LVIS: A Dataset for Large Vocabulary Instance
Segmentation”.In:2019IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)
(2019),pp.5351–5359.
[26] Rollyn T. Labuguen et al. “MacaquePose: A Novel “In the Wild” Macaque Monkey Pose Dataset for
MarkerlessMotionCapture”.In:FrontiersinBehavioralNeuroscience14(2020).
[27] NisargP.Desaietal.“OpenApePose:adatabaseofannotatedapephotographsforposeestimation”.In:
ArXivabs/2212.00741(2022).
[28] HangYuetal.“AP-10K:ABenchmarkforAnimalPoseEstimationintheWild”.In:Thirty-fifthCon-
ferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack(Round2).2021.
[29] YuxiangYangetal.“APT-36K:ALarge-scaleBenchmarkforAnimalPoseEstimationandTracking”.
In:Thirty-sixthConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack.
2022.
[30] Alex Bewley et al. “Simple online and realtime tracking”. In: 2016 IEEE international conference on
imageprocessing(ICIP).IEEE.2016,pp.3464–3468.
[31] Rudolph Emil Kalman. “A new approach to linear filtering and prediction problems”. In: Journal of
BasicEngineering82(1960),pp.35–45.
[32] Harold W Kuhn. “The Hungarian method for the assignment problem”. In: Nav. res. logist. q. 2.1-2
(1955),pp.83–97.
[33] YifuZhangetal.“Bytetrack:Multi-objecttrackingbyassociatingeverydetectionbox”.In:Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
PartXXII.Springer.2022,pp.1–21.
[34] JinkunCaoetal.“Observation-centricsort:Rethinkingsortforrobustmulti-objecttracking”.In:arXiv
preprintarXiv:2203.14360(2022).
[35] GerardMaggiolinoetal.“DeepOC-SORT:Multi-PedestrianTrackingbyAdaptiveRe-Identification”.
In:arXivpreprintarXiv:2302.11813(2023).
[36] YunhaoDuetal.“Strongsort:Makedeepsortgreatagain”.In:IEEETransactionsonMultimedia(2023).
[37] NirAharon,RoyOrfaig,andBen-ZionBobrovsky.“BoT-SORT:Robustassociationsmulti-pedestrian
tracking”.In:arXivpreprintarXiv:2206.14651(2022).
[38] FanYangetal.“HardtoTrackObjectswithIrregularMotionsandSimilarAppearances?MakeItEasier
byBufferingtheMatchingSpace”.In:ProceedingsoftheIEEE/CVFWinterConferenceonApplications
ofComputerVision.2023,pp.4799–4808.
[39] Feng Yan et al. “Multiple Object Tracking Challenge Technical Report for Team MT IoT”. In: arXiv
preprintarXiv:2212.03586(2022).
[40] GuillemBraso´ andLauraLeal-Taixe´.“Learninganeuralsolverformultipleobjecttracking”.In:Pro-
ceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020, pp. 6247–
6257.
[41] OrcunCetintas,GuillemBraso´,andLauraLeal-Taixe´.“UnifyingShortandLong-TermTrackingwith
GraphHierarchies”.In:arXivpreprintarXiv:2212.03038(2022).
[42] Fangao Zeng et al. “Motr: End-to-end multiple-object tracking with transformer”. In: European Con-
ferenceonComputerVision.Springer.2022,pp.659–675.
[43] Tim Meinhardt et al. “TrackFormer: Multi-Object Tracking with Transformers”. In: 2022 IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR)(2021),pp.8834–8844.
[44] ZitongZhan,DanielMcKee,andSvetlanaLazebnik.“RobustOnlineVideoInstanceSegmentationwith
TrackQueries”.In:arXivpreprintarXiv:2211.09108(2022).
[45] PeizeSunetal.“Transtrack:Multipleobjecttrackingwithtransformer”.In:arXivpreprintarXiv:2012.15460
(2020).
[46] YuangZhang,TiancaiWang,andXiangyuZhang.“MOTRv2:BootstrappingEnd-to-EndMulti-Object
TrackingbyPretrainedObjectDetectors”.In:ArXivabs/2211.09791(2022).
[47] EnYuetal.“MOTRv3:Release-FetchSupervisionforEnd-to-EndMulti-ObjectTracking”.In:arXiv
preprintarXiv:2305.14298(2023).
31[48] FengYanetal.“BridgingtheGapBetweenEnd-to-endandNon-End-to-endMulti-ObjectTracking”.
In:arXivpreprintarXiv:2305.12724(2023).
[49] AntonMilanetal.“MOT16:Abenchmarkformulti-objecttracking”.In:arXivpreprintarXiv:1603.00831
(2016).
[50] PeizeSunetal.“Dancetrack:Multi-objecttrackinginuniformappearanceanddiversemotion”.In:Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,pp.20993–
21002.
[51] Libo Zhang et al. “AnimalTrack: A Benchmark for Multi-Animal Tracking in the Wild”. In: Interna-
tionalJournalofComputerVision131.2(2023),pp.496–513.
[52] DebayanDebetal.“FaceRecognition:PrimatesintheWild”.In:2018IEEE9thInternationalConfer-
enceonBiometricsTheory,ApplicationsandSystems(BTAS).2018,pp.1–10.
[53] MatthiasKo¨rschens,Bjo¨rnBarz,andJoachimDenzler.“Towardsautomaticidentificationofelephants
inthewild”.In:arXivpreprintarXiv:1812.04418(2018).
[54] OlgaMoskvyaketal.“Robustre-identificationofmantaraysfromnaturalmarkingsbylearningposein-
variantembeddings”.In:2021DigitalImageComputing:TechniquesandApplications(DICTA).IEEE.
2021,pp.1–8.
[55] Andre´ C. Ferreira et al. “Deep learning-based methods for individual recognition in small birds”. In:
MethodsinEcologyandEvolution11.9(),pp.1072–1085.
[56] Francisco Romero-Ferrero et al. “Idtracker. ai: tracking all individuals in small or large collectives of
unmarkedanimals”.In:Naturemethods16.2(2019),pp.179–182.
[57] Stefan Schneider et al. “Past, present and future approaches using computer vision for animal re-
identificationfromcameratrapdata”.In:MethodsinEcologyandEvolution10.4(),pp.461–470.
[58] MatthiasKo¨rschens,Bjo¨rnBarz,andJoachimDenzler.“TowardsAutomaticIdentificationofElephants
intheWild”.In:ArXivabs/1812.04418(2018).
[59] LudmilaIKunchevaetal.“Abenchmarkdatabaseforanimalre-identificationandtracking”.In:2022
IEEE5thInternationalConferenceonImageProcessingApplicationsandSystems(IPAS).IEEE.2022,
pp.1–6.
[60] XiaoxuanMaetal.“ChimpACT:ALongitudinalDatasetforUnderstandingChimpanzeeBehaviors”.
In:arXivpreprintarXiv:2310.16447(2023).
[61] Liang Zheng et al. “Mars: A video benchmark for large-scale person re-identification”. In: Computer
Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings,PartVI14.Springer.2016,pp.868–884.
[62] Liang Zheng et al. “Scalable person re-identification: A benchmark”. In: Proceedings of the IEEE in-
ternationalconferenceoncomputervision.2015,pp.1116–1124.
[63] ShuyuanLietal.“ATRW:abenchmarkforAmurtigerre-identificationinthewild”.In:arXivpreprint
arXiv:1906.05586(2019).
[64] ZhaofanQiu,TingYao,andTaoMei.“Learningspatio-temporalrepresentationwithpseudo-3dresidual
networks”.In:proceedingsoftheIEEEInternationalConferenceonComputerVision.2017,pp.5533–
5541.
[65] Youngwan Lee et al. “Diverse temporal aggregation and depthwise spatiotemporal factorization for
efficientvideoclassification”.In:IEEEAccess9(2021),pp.163054–163064.
[66] ChristophFeichtenhoferetal.“Slowfastnetworksforvideorecognition”.In:ProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision.2019,pp.6202–6211.
[67] AnuragArnabetal.“Vivit:Avideovisiontransformer”.In:ProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision.2021,pp.6836–6846.
[68] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. “Is space-time attention all you need for video
understanding?”In:ICML.Vol.2.3.2021,p.4.
[69] ShenYanetal.“Multiviewtransformersforvideorecognition”.In:ProceedingsoftheIEEE/CVFCon-
ferenceonComputerVisionandPatternRecognition.2022,pp.3333–3343.
32[70] HaoqiFanetal.“Multiscalevisiontransformers”.In:ProceedingsoftheIEEE/CVFInternationalCon-
ferenceonComputerVision.2021,pp.6824–6835.
[71] ChaitanyaRyalietal.“Hiera:AHierarchicalVisionTransformerwithouttheBells-and-Whistles”.In:
InternationalConferenceonMachineLearning(ICML)(2023).
[72] Chao-Yuan Wu et al. “Memvit: Memory-augmented multiscale vision transformer for efficient long-
termvideorecognition”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.2022,pp.13587–13597.
[73] AdrianBulatetal.“Space-timemixingattentionforvideotransformer”.In:AdvancesinNeuralInfor-
mationProcessingSystems34(2021),pp.19594–19607.
[74] WangmengXiangetal.“SpatiotemporalSelf-attentionModelingwithTemporalPatchShiftforAction
Recognition”.In:ComputerVision–ECCV2022:17thEuropeanConference,TelAviv,Israel,October
23–27,2022,Proceedings,PartIII.Springer.2022,pp.627–644.
[75] Kunchang Li et al. “Uniformer: Unified transformer for efficient spatiotemporal representation learn-
ing”.In:arXivpreprintarXiv:2201.04676(2022).
[76] KunchangLietal.“UniFormerV2:SpatiotemporalLearningbyArmingImageViTswithVideoUni-
Former”.In:arXivpreprintarXiv:2211.09552(2022).
[77] DuTranetal.“Learningspatiotemporalfeatureswith3dconvolutionalnetworks”.In:Proceedingsof
theIEEEinternationalconferenceoncomputervision.2015,pp.4489–4497.
[78] Joao Carreira and Andrew Zisserman. “Quo vadis, action recognition? a new model and the kinetics
dataset”.In:proceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.2017,
pp.6299–6308.
[79] SainingXieetal.“Rethinkingspatiotemporalfeaturelearning:Speed-accuracytrade-offsinvideoclas-
sification”. In: Proceedings of the European conference on computer vision (ECCV). 2018, pp. 305–
321.
[80] CanZhangetal.“Pan:Towardsfastactionrecognitionvialearningpersistenceofappearance”.In:arXiv
preprintarXiv:2008.03462(2020).
[81] Heeseung Kwon et al. “Motionsqueeze: Neural motion feature learning for video understanding”. In:
Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings,PartXVI16.Springer.2020,pp.345–362.
[82] Yue Zhao, Yuanjun Xiong, and Dahua Lin. “Trajectory convolution for action recognition”. In: Ad-
vancesinneuralinformationprocessingsystems31(2018).
[83] Mandela Patrick et al. “Keeping your eye on the ball: Trajectory attention in video transformers”. In:
Advancesinneuralinformationprocessingsystems34(2021),pp.12493–12506.
[84] FuchenLongetal.“Stand-aloneinter-frameattentioninvideomodels”.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.2022,pp.3192–3201.
[85] LiminWangetal.“Tdn:Temporaldifferencenetworksforefficientactionrecognition”.In:Proceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2021,pp.1895–1904.
[86] Heeseung Kwon et al. “Learning self-similarity in space and time as generalized motion for video
action recognition”. In: Proceedings of the IEEE/CVF International Conference on Computer Vision.
2021,pp.13065–13075.
[87] CeyuanYangetal.“Temporalpyramidnetworkforactionrecognition”.In:ProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition.2020,pp.591–600.
[88] YuanzhongLiu,JunsongYuan,andZhigangTu.“Motion-drivenvisualtempolearningforvideo-based
actionrecognition”.In:IEEETransactionsonImageProcessing31(2022),pp.4104–4116.
[89] Guoxi Huang and Adrian G Bors. “Busy-quiet video disentangling for video classification”. In: Pro-
ceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022, pp. 1341–
1350.
[90] XiaolongWangandAbhinavGupta.“Videosasspace-timeregiongraphs”.In:ProceedingsoftheEu-
ropeanconferenceoncomputervision(ECCV).2018,pp.399–417.
33[91] BoleiZhouetal.“Temporalrelationalreasoninginvideos”.In:ProceedingsoftheEuropeanconference
oncomputervision(ECCV).2018,pp.803–818.
[92] Joanna Materzynska et al. “Something-else: Compositional action recognition with spatial-temporal
interaction networks”. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition.2020,pp.1049–1059.
[93] YuanTianetal.“Ean:eventadaptivenetworkforenhancedactionrecognition”.In:InternationalJour-
nalofComputerVision130.10(2022),pp.2453–2471.
[94] Yi Wang et al. “InternVideo: General Video Foundation Models via Generative and Discriminative
Learning”.In:arXivpreprintarXiv:2212.03191(2022).
[95] ZhanTongetal.“Videomae:Maskedautoencodersaredata-efficientlearnersforself-supervisedvideo
pre-training”.In:Advancesinneuralinformationprocessingsystems35(2022),pp.10078–10093.
[96] Gunnar A Sigurdsson et al. “Hollywood in homes: Crowdsourcing data collection for activity under-
standing”.In:ComputerVision–ECCV2016:14thEuropeanConference,Amsterdam,TheNetherlands,
October11–14,2016,Proceedings,PartI14.Springer.2016,pp.510–526.
[97] DimaDamenetal.“Rescalingegocentricvision”.In:arXivpreprintarXiv:2006.13256(2020).
[98] WillKayetal.“Thekineticshumanactionvideodataset”.In:arXivpreprintarXiv:1705.06950(2017).
[99] KhurramSoomro,AmirRoshanZamir,andMubarakShah.“UCF101:Adatasetof101humanactions
classesfromvideosinthewild”.In:arXivpreprintarXiv:1212.0402(2012).
[100] Hildegard Kuehne et al. “HMDB: a large video database for human motion recognition”. In: 2011
Internationalconferenceoncomputervision.IEEE.2011,pp.2556–2563.
[101] RaghavGoyaletal.“The”somethingsomething”videodatabaseforlearningandevaluatingvisualcom-
monsense”.In:ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017,pp.5842–
5850.
[102] MathewMonfortetal.“Momentsintimedataset:onemillionvideosforeventunderstanding”.In:IEEE
transactionsonpatternanalysisandmachineintelligence42.2(2019),pp.502–508.
[103] Xun Long Ng et al. “Animal kingdom: A large and diverse dataset for animal behavior understand-
ing”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,
pp.19023–19034.
[104] ZhengShou,DongangWang,andShih-FuChang.“Temporalactionlocalizationinuntrimmedvideos
viamulti-stagecnns”.In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecogni-
tion.2016,pp.1049–1058.
[105] HuijuanXu,AbirDas,andKateSaenko.“R-c3d:Regionconvolutional3dnetworkfortemporalactivity
detection”.In:ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017,pp.5783–
5792.
[106] TianweiLinetal.“BSN:BoundarySensitiveNetworkforTemporalActionProposalGeneration”.In:
EuropeanConferenceonComputerVision.2018.
[107] HaishengSuetal.“BSN++:ComplementaryBoundaryRegressorwithScale-BalancedRelationMod-
elingforTemporalActionProposalGeneration”.In:AAAIConferenceonArtificialIntelligence.2020.
[108] TianweiLinetal.“BMN:Boundary-MatchingNetworkforTemporalActionProposalGeneration”.In:
2019IEEE/CVFInternationalConferenceonComputerVision(ICCV)(2019),pp.3888–3897.
[109] Chuming Lin et al. “Fast Learning of Temporal Action Proposal via Dense Boundary Generator”. In:
ArXivabs/1911.04127(2019).
[110] Qing-HuangSongetal.“FasterLearningofTemporalActionProposalviaSparseMultilevelBoundary
Generator”.In:ArXivabs/2303.03166(2023).
[111] Shuning Chang et al. “Augmented Transformer with Adaptive Graph for Temporal Action Proposal
Generation”.In:Proceedingsofthe3rdInternationalWorkshoponHuman-CentricMultimediaAnalysis
(2021).
[112] Peisen Zhao et al. “Bottom-Up Temporal Action Localization with Mutual Regularization”. In: Euro-
peanConferenceonComputerVision.2020.
34[113] Limin Wang et al. “UntrimmedNets for Weakly Supervised Action Recognition and Detection”. In:
2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR)(2017),pp.6402–6411.
[114] ZhengShouetal.“CDC:Convolutional-De-ConvolutionalNetworksforPreciseTemporalActionLo-
calizationinUntrimmedVideos”.In:2017IEEEConferenceonComputerVisionandPatternRecogni-
tion(CVPR)(2017),pp.1417–1426.
[115] RunhaoZengetal.“GraphConvolutionalNetworksforTemporalActionLocalization”.In:2019IEEE/CVF
InternationalConferenceonComputerVision(ICCV)(2019),pp.7093–7102.
[116] Zixin Zhu et al. “Enriching Local and Global Contexts for Temporal Action Localization”. In: 2021
IEEE/CVFInternationalConferenceonComputerVision(ICCV)(2021),pp.13496–13505.
[117] TianweiLin,XuZhao,andZhengShou.“Singleshottemporalactiondetection”.In:Proceedingsofthe
25thACMinternationalconferenceonMultimedia.2017,pp.988–996.
[118] QinyingLiuandZileiWang.“Progressiveboundaryrefinementnetworkfortemporalactiondetection”.
In:ProceedingsoftheAAAIconferenceonartificialintelligence.Vol.34.07.2020,pp.11612–11619.
[119] FuchenLongetal.“Gaussiantemporalawarenessnetworksforactionlocalization”.In:Proceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.2019,pp.344–353.
[120] Chuming Lin et al. “Learning salient boundary feature for anchor-free temporal action localization”.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021,
pp.3320–3329.
[121] XiaolongLiuetal.“End-to-endtemporalactiondetectionwithtransformer”.In:IEEETransactionson
ImageProcessing31(2022),pp.5427–5441.
[122] Yilong He et al. “GLFormer: Global and Local Context Aggregation Network for Temporal Action
Detection”.In:AppliedSciences12.17(2022),p.8557.
[123] Feng Cheng and Gedas Bertasius. “TallFormer: Temporal Action Localization with a Long-Memory
Transformer”.In:ComputerVision–ECCV2022:17thEuropeanConference,TelAviv,Israel,October
23–27,2022,Proceedings,PartXXXIV.Springer.2022,pp.503–521.
[124] Chen-Lin Zhang, Jianxin Wu, and Yin Li. “Actionformer: Localizing moments of actions with trans-
formers”.In:ComputerVision–ECCV2022:17thEuropeanConference,TelAviv,Israel,October23–
27,2022,Proceedings,PartIV.Springer.2022,pp.492–510.
[125] TuanNTang,KwonyoungKim,andKwanghoonSohn.“TemporalMaxer:MaximizeTemporalContext
withonlyMaxPoolingforTemporalActionLocalization”.In:arXivpreprintarXiv:2303.09055(2023).
[126] DingfengShietal.“TriDet:TemporalActionDetectionwithRelativeBoundaryModeling”.In:arXiv
preprintarXiv:2303.07347(2023).
[127] Rui Dai, Srijan Das, and Francois Bremond. “CTRN: Class-Temporal Relational Network for Action
Detection”.In:arXivpreprintarXiv:2110.13473(2021).
[128] Rui Dai et al. “Pdan: Pyramid dilated attention network for action detection”. In: Proceedings of the
IEEE/CVFWinterConferenceonApplicationsofComputerVision.2021,pp.2970–2979.
[129] PraveenTirupatturetal.“Modelingmulti-labelactiondependenciesfortemporalactionlocalization”.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021,
pp.1460–1470.
[130] Rui Dai et al. “MS-TCT: multi-scale temporal convtransformer for action detection”. In: Proceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,pp.20041–20051.
[131] JingTanetal.“PointTAD:Multi-LabelTemporalActionDetectionwithLearnableQueryPoints”.In:
AdvancesinNeuralInformationProcessingSystems35(2022),pp.15268–15280.
[132] HaroonIdreesetal.“Thethumoschallengeonactionrecognitionforvideos“inthewild””.In:Com-
puterVisionandImageUnderstanding155(2017),pp.1–23.
[133] Fabian Caba Heilbron et al. “Activitynet: A large-scale video benchmark for human activity under-
standing”.In:2015IEEEconferenceoncomputervisionandpatternrecognition(CVPR).IEEE.2015,
pp.961–970.
[134] SerenaYeungetal.“Everymomentcounts:Densedetailedlabelingofactionsincomplexvideos”.In:
InternationalJournalofComputerVision126(2018),pp.375–389.
35[135] RuiDaietal.“Toyotasmarthomeuntrimmed:Real-worlduntrimmedvideosforactivitydetection”.In:
IEEETransactionsonPatternAnalysisandMachineIntelligence45.2(2022),pp.2533–2550.
[136] Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid. “Learning to track for spatio-temporal
action localization”. In: Proceedings of the IEEE international conference on computer vision. 2015,
pp.3164–3172.
[137] GeorgiaGkioxariandJitendraMalik.“Findingactiontubes”.In:ProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition.2015,pp.759–768.
[138] SumanSahaetal.“Deeplearningfordetectingmultiplespace-timeactiontubesinvideos”.In:British
MachineVisionConference(BMVC)(2016).
[139] VickyKalogeitonetal.“Actiontubeletdetectorforspatio-temporalactionlocalization”.In:Proceed-
ingsoftheIEEEInternationalConferenceonComputerVision.2017,pp.4405–4413.
[140] XitongYangetal.“Step:Spatio-temporalprogressivelearningforvideoactiondetection”.In:Proceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2019,pp.264–272.
[141] YixuanLietal.“Actionsasmovingpoints”.In:ComputerVision–ECCV2020:16thEuropeanConfer-
ence,Glasgow,UK,August23–28,2020,Proceedings,PartXVI16.Springer.2020,pp.68–84.
[142] Jiaojiao Zhao et al. “Tuber: Tubelet transformer for video action detection”. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,pp.13598–13607.
[143] Gurkirt Singh et al. “Spatio-Temporal Action Detection Under Large Motion”. In: Proceedings of the
IEEE/CVFWinterConferenceonApplicationsofComputerVision.2023,pp.6009–6018.
[144] Alexey Gritsenko et al. “End-to-End Spatio-Temporal Action Localisation with Video Transformers”.
In:arXivpreprintarXiv:2304.12160(2023).
[145] Jathushan Rajasegaran et al. “On the Benefits of 3D Pose and Tracking for Human Action Recog-
nition”. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
2023,pp.640–649.
[146] Chunhui Gu et al. “Ava: A video dataset of spatio-temporally localized atomic visual actions”. In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2018,pp.6047–6056.
[147] Huaizu Jiang and Erik Learned-Miller. “Face detection with the faster R-CNN”. In: 2017 12th IEEE
international conference on automatic face & gesture recognition (FG 2017). IEEE. 2017, pp. 650–
657.
[148] ChristophFeichtenhofer.“X3d:Expandingarchitecturesforefficientvideorecognition”.In:Proceed-
ingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.2020,pp.203–213.
[149] ChenSunetal.“Actor-centricrelationnetwork”.In:ProceedingsoftheEuropeanConferenceonCom-
puterVision(ECCV).2018,pp.318–334.
[150] RohitGirdharetal.“Videoactiontransformernetwork”.In:ProceedingsoftheIEEE/CVFconference
oncomputervisionandpatternrecognition.2019,pp.244–253.
[151] Chao-Yuan Wu et al. “Long-term feature banks for detailed video understanding”. In: Proceedings of
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.2019,pp.284–293.
[152] Jiajun Tang et al. “Asynchronous interaction aggregation for action detection”. In: Computer Vision–
ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartXV16.
Springer.2020,pp.71–87.
[153] AnuragArnab,ChenSun,andCordeliaSchmid.“Unifiedgraphstructuredmodelsforvideounderstand-
ing”.In:ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.2021,pp.8117–
8126.
[154] Junting Pan et al. “Actor-context-actor relation network for spatio-temporal action localization”. In:
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2021,pp.464–
474.
[155] Gueter Josmy Faure, Min-Hung Chen, and Shang-Hong Lai. “Holistic Interaction Transformer Net-
work for Action Detection”. In: Proceedings of the IEEE/CVF Winter Conference on Applications of
ComputerVision.2023,pp.3340–3350.
36[156] HueihanJhuangetal.“Towardsunderstandingactionrecognition”.In:ProceedingsoftheIEEEinter-
nationalconferenceoncomputervision.2013,pp.3192–3199.
[157] Yixuan Li et al. “Multisports: A multi-person video dataset of spatio-temporally localized sports ac-
tions”.In:ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.2021,pp.13536–
13545.
[158] AngLietal.“Theava-kineticslocalizedhumanactionsvideodataset”.In:arXivpreprintarXiv:2005.00214
(2020).
[159] MayaAntounandDanielAsmar.“Humanobjectinteractiondetection:Designandsurvey”.In:Image
andVisionComputing(2022),p.104617.
[160] GuangmingZhuetal.“Scenegraphgeneration:Acomprehensivesurvey”.In:arXivpreprintarXiv:2201.00443
(2022).
[161] XufengQianetal.“Videorelationdetectionwithspatio-temporalgraph”.In:Proceedingsofthe27th
ACMinternationalconferenceonmultimedia.2019,pp.84–93.
[162] Yao-HungHubertTsaietal.“Videorelationshipreasoningusinggatedspatio-temporalenergygraph”.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019,
pp.10424–10433.
[163] Xindi Shang et al. “Video visual relation detection”. In: Proceedings of the 25th ACM international
conferenceonMultimedia.2017,pp.1300–1308.
[164] Xu Sun et al. “Video visual relation detection via multi-modal feature fusion”. In: Proceedings of the
27thACMInternationalConferenceonMultimedia.2019,pp.2657–2661.
[165] ZixuanSuetal.“Videorelationdetectionviamultiplehypothesisassociation”.In:Proceedingsofthe
28thACMInternationalConferenceonMultimedia.2020,pp.3127–3135.
[166] YicongLietal.“Interventionalvideorelationdetection”.In:Proceedingsofthe29thACMInternational
ConferenceonMultimedia.2021,pp.4091–4099.
[167] XindiShangetal.“Videovisualrelationdetectionviaiterativeinference”.In:Proceedingsofthe29th
ACMinternationalconferenceonMultimedia.2021,pp.3654–3663.
[168] MengWeietal.“InDefenseofClip-basedVideoRelationDetection”.In:arXivpreprintarXiv:2307.08984
(2023).
[169] ChenchenLiuetal.“Beyondshort-termsnippet:Videorelationdetectionwithspatio-temporalglobal
context”. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.
2020,pp.10840–10849.
[170] KaifengGaoetal.“Classification-then-grounding:Reformulatingvideoscenegraphsastemporalbipar-
titegraphs”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
2022,pp.19497–19506.
[171] ShuoChenetal.“Socialfabric:Tubeletcompositionsforvideorelationdetection”.In:Proceedingsof
theIEEE/CVFInternationalConferenceonComputerVision.2021,pp.13485–13494.
[172] KaifengGaoetal.“Videorelationdetectionviatrackletbasedvisualtransformer”.In:Proceedingsof
the29thACMinternationalconferenceonmultimedia.2021,pp.4833–4837.
[173] JingweiJi,RishiDesai,andJuanCarlosNiebles.“Detectinghuman-objectrelationshipsinvideos”.In:
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.2021,pp.8106–8116.
[174] YurenCongetal.“Spatial-temporaltransformerfordynamicscenegraphgeneration”.In:Proceedings
oftheIEEE/CVFinternationalconferenceoncomputervision.2021,pp.16372–16382.
[175] Jingyi Wang et al. “Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene
Graphs”.In:arXivpreprintarXiv:2305.08522(2023).
[176] YimingLi,XiaoshanYang,andChangshengXu.“Dynamicscenegraphgenerationviaanticipatorypre-
training”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
2022,pp.13874–13883.
[177] Shengyu Feng et al. “Exploiting long-term dependencies for generating dynamic scene graphs”. In:
ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision.2023,pp.5130–
5139.
37[178] XindiShangetal.“Annotatingobjectsandrelationsinuser-generatedvideos”.In:Proceedingsofthe
2019onInternationalConferenceonMultimediaRetrieval.2019,pp.279–287.
[179] Jingwei Ji et al. “Action genome: Actions as compositions of spatio-temporal scene graphs”. In: Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2020,pp.10236–
10247.
[180] AliSharifRazavianetal.“CNNfeaturesoff-the-shelf:anastoundingbaselineforrecognition”.In:Pro-
ceedingsoftheIEEEconferenceoncomputervisionandpatternrecognitionworkshops.2014,pp.806–
813.
[181] JontiTalukdaretal.“Transferlearningforobjectdetectionusingstate-of-the-artdeepneuralnetworks”.
In:20185thinternationalconferenceonsignalprocessingandintegratednetworks(SPIN).IEEE.2018,
pp.78–83.
[182] Alec Radford et al. “Learning Transferable Visual Models From Natural Language Supervision”. In:
InternationalConferenceonMachineLearning.2021.
[183] ShaokaiYeetal.“SuperAnimalmodelspretrainedforplug-and-playanalysisofanimalbehavior”.In:
arXivpreprintarXiv:2203.07436(2022).
[184] OlgaRussakovskyetal.“ImageNetLargeScaleVisualRecognitionChallenge”.In:InternationalJour-
nalofComputerVision115(2014),pp.211–252.
[185] Alina Kuznetsova et al. “The open images dataset v4: Unified image classification, object detection,
andvisualrelationshipdetectionatscale”.In:InternationalJournalofComputerVision128.7(2020),
pp.1956–1981.
[186] TingChenetal.“Asimpleframeworkforcontrastivelearningofvisualrepresentations”.In:Interna-
tionalconferenceonmachinelearning.PMLR.2020,pp.1597–1607.
[187] XinleiChenandKaimingHe.“Exploringsimplesiameserepresentationlearning”.In:Proceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.2021,pp.15750–15758.
[188] JureZbontaretal.“Barlowtwins:Self-supervisedlearningviaredundancyreduction”.In:International
ConferenceonMachineLearning.PMLR.2021,pp.12310–12320.
[189] Maxime Oquab et al. “DINOv2: Learning Robust Visual Features without Supervision”. In: ArXiv
abs/2304.07193(2023).
[190] YuxinFangetal.“Eva:Exploringthelimitsofmaskedvisualrepresentationlearningatscale”.In:Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2023,pp.19358–
19369.
[191] LuYuanetal.“Florence:Anewfoundationmodelforcomputervision”.In:arXivpreprintarXiv:2111.11432
(2021).
[192] Jean-BaptisteAlayracetal.“Flamingo:avisuallanguagemodelforfew-shotlearning”.In:Advances
inNeuralInformationProcessingSystems35(2022),pp.23716–23736.
[193] Carl Doersch, Abhinav Gupta, and Alexei A Efros. “Unsupervised visual representation learning by
context prediction”. In: Proceedings of the IEEE international conference on computer vision. 2015,
pp.1422–1430.
[194] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. “Unsupervised representation learning by pre-
dictingimagerotations”.In:arXivpreprintarXiv:1803.07728(2018).
[195] Jean-Bastien Grill et al. “Bootstrap your own latent-a new approach to self-supervised learning”. In:
Advancesinneuralinformationprocessingsystems33(2020),pp.21271–21284.
[196] MathildeCaronetal.“Emergingpropertiesinself-supervisedvisiontransformers”.In:Proceedingsof
theIEEE/CVFinternationalconferenceoncomputervision.2021,pp.9650–9660.
[197] Adrien Bardes, Jean Ponce, and Yann LeCun. “Vicreg: Variance-invariance-covariance regularization
forself-supervisedlearning”.In:arXivpreprintarXiv:2105.04906(2021).
[198] KaimingHeetal.“Maskedautoencodersarescalablevisionlearners”.In:ProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition.2022,pp.16000–16009.
[199] Limin Wang et al. “Videomae v2: Scaling video masked autoencoders with dual masking”. In: Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2023,pp.14549–
14560.
38[200] JinghaoZhouetal.“ibot:Imagebertpre-trainingwithonlinetokenizer”.In:arXivpreprintarXiv:2111.07832
(2021).
[201] ZhichengHuangetal.“Contrastivemaskedautoencodersarestrongervisionlearners”.In:IEEETrans-
actionsonPatternAnalysisandMachineIntelligence(2023).
[202] Feifei Shao et al. “Deep learning for weakly-supervised object detection and localization: A survey”.
In:Neurocomputing496(2022),pp.192–207.
[203] Dhruv Mahajan et al. “Exploring the limits of weakly supervised pretraining”. In: Proceedings of the
Europeanconferenceoncomputervision(ECCV).2018,pp.181–196.
[204] XingyiZhouetal.“Towards3dhumanposeestimationinthewild:aweakly-supervisedapproach”.In:
ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017,pp.398–407.
[205] PilhyeonLee,YoungjungUh,andHyeranByun.“Backgroundsuppressionnetworkforweakly-supervised
temporalactionlocalization”.In:ProceedingsoftheAAAIconferenceonartificialintelligence.Vol.34.
07.2020,pp.11320–11327.
[206] Jing Shi et al. “A simple baseline for weakly-supervised scene graph generation”. In: Proceedings of
theIEEE/CVFInternationalConferenceonComputerVision.2021,pp.16393–16402.
[207] ZhongzhengRenetal.“Instance-aware,context-focused,andmemory-efficientweaklysupervisedob-
jectdetection”.In:ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecogni-
tion.2020,pp.10598–10607.
[208] Fang Wan et al. “C-mil: Continuation multiple instance learning for weakly supervised object detec-
tion”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2019,
pp.2199–2208.
[209] BoleiZhouetal.“Learningdeepfeaturesfordiscriminativelocalization”.In:ProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition.2016,pp.2921–2929.
[210] Soufiane Belharbi et al. “F-cam: Full resolution class activation maps via guided parametric upscal-
ing”.In:ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision.2022,
pp.3490–3499.
[211] ShakeebMurtazaetal.“Discriminativesamplingofproposalsinself-supervisedtransformersforweakly
supervised object localization”. In: Proceedings of the IEEE/CVF Winter Conference on Applications
ofComputerVision.2023,pp.155–165.
[212] YanGaoetal.“C-midn:Coupledmultipleinstancedetectionnetworkwithsegmentationguidancefor
weakly supervised object detection”. In: Proceedings of the IEEE/CVF International Conference on
ComputerVision.2019,pp.9834–9843.
[213] JunWeietal.“Shallowfeaturemattersforweaklysupervisedobjectlocalization”.In:Proceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.2021,pp.5993–6001.
[214] XiangliYangetal.“Asurveyondeepsemi-supervisedlearning”.In:IEEETransactionsonKnowledge
andDataEngineering(2022).
[215] Dong-Hyun Lee et al. “Pseudo-label: The simple and efficient semi-supervised learning method for
deepneuralnetworks”.In:Workshoponchallengesinrepresentationlearning,ICML.Vol.3.2.Atlanta.
2013,p.896.
[216] Ting Chen et al. “Big self-supervised models are strong semi-supervised learners”. In: Advances in
neuralinformationprocessingsystems33(2020),pp.22243–22255.
[217] AnttiRasmusetal.“Semi-supervisedlearningwithladdernetworks”.In:Advancesinneuralinforma-
tionprocessingsystems28(2015).
[218] AnttiTarvainenandHarriValpola.“Meanteachersarebetterrolemodels:Weight-averagedconsistency
targetsimprovesemi-superviseddeeplearningresults”.In:Advancesinneuralinformationprocessing
systems30(2017).
[219] David Berthelot et al. “Mixmatch: A holistic approach to semi-supervised learning”. In: Advances in
neuralinformationprocessingsystems32(2019).
[220] KihyukSohnetal.“Fixmatch:Simplifyingsemi-supervisedlearningwithconsistencyandconfidence”.
In:Advancesinneuralinformationprocessingsystems33(2020),pp.596–608.
39[221] BowenZhangetal.“Flexmatch:Boostingsemi-supervisedlearningwithcurriculumpseudolabeling”.
In:AdvancesinNeuralInformationProcessingSystems34(2021),pp.18408–18419.
[222] ZhaoweiCaietal.“Semi-supervisedvisiontransformersatscale”.In:AdvancesinNeuralInformation
ProcessingSystems35(2022),pp.25697–25710.
[223] ZhenXingetal.“Svformer:Semi-supervisedvideotransformerforactionrecognition”.In:Proceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2023,pp.18816–18826.
[224] Zihang Dai et al. “Good semi-supervised learning that requires a bad gan”. In: Advances in neural
informationprocessingsystems30(2017).
[225] DurkPKingmaetal.“Semi-supervisedlearningwithdeepgenerativemodels”.In:Advancesinneural
informationprocessingsystems27(2014).
[226] MEhsanAbbasnejad,AnthonyDick,andAntonvandenHengel.“Infinitevariationalautoencoderfor
semi-supervised learning”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition.2017,pp.5888–5897.
[227] XueyingZhanetal.“Acomparativesurveyofdeepactivelearning”.In:arXivpreprintarXiv:2203.13450
(2022).
[228] Zhanpeng Feng et al. “ALBench: a framework for evaluating active learning in object detection”. In:
arXivpreprintarXiv:2207.13339(2022).
[229] Ismail Elezi et al. “Not all labels are equal: Rationalizing the labeling costs for training object detec-
tion”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,
pp.14492–14501.
[230] Di Yuan et al. “Active learning for deep visual tracking”. In: IEEE Transactions on Neural Networks
andLearningSystems(2023).
[231] Aayush J Rana and Yogesh S Rawat. “Hybrid Active Learning via Deep Clustering for Video Action
Detection”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
2023,pp.18867–18877.
[232] Shuzhou Sun et al. “Evidential Uncertainty and Diversity Guided Active Learning for Scene Graph
Generation”.In:TheEleventhInternationalConferenceonLearningRepresentations.2022.
[233] IdoDaganandSeanPEngelson.“Committee-basedsamplingfortrainingprobabilisticclassifiers”.In:
MachineLearningProceedings1995.Elsevier,1995,pp.150–157.
[234] RayLiereandPrasadTadepalli.“Activelearningwithcommitteesfortextcategorization”.In:AAAI/IAAI.
Citeseer.1997,pp.591–596.
[235] WilliamHBeluchetal.“Thepowerofensemblesforactivelearninginimageclassification”.In:Pro-
ceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2018,pp.9368–9377.
[236] DonggeunYooandInSoKweon.“Learninglossforactivelearning”.In:ProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition.2019,pp.93–102.
[237] StuartLloyd.“LeastsquaresquantizationinPCM”.In:IEEEtransactionsoninformationtheory28.2
(1982),pp.129–137.
[238] Ozan Sener and Silvio Savarese. “Active learning for convolutional neural networks: A core-set ap-
proach”.In:arXivpreprintarXiv:1708.00489(2017).
[239] GuyHacohen,AvihuDekel,andDaphnaWeinshall.“Activelearningonabudget:Oppositestrategies
suithighandlowbudgets”.In:arXivpreprintarXiv:2202.02794(2022).
[240] Lin Yang et al. “Suggestive annotation: A deep active learning framework for biomedical image seg-
mentation”. In: Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th
InternationalConference,QuebecCity,QC,Canada,September11-13,2017,Proceedings,PartIII20.
Springer.2017,pp.399–407.
[241] ChangjianShuietal.“Deepactivelearning:Unifiedandprincipledmethodforqueryandtraining”.In:
InternationalConferenceonArtificialIntelligenceandStatistics.PMLR.2020,pp.1308–1318.
[242] Mykhaylo Andriluka, Jasper RR Uijlings, and Vittorio Ferrari. “Fluid annotation: a human-machine
collaborationinterfaceforfullimageannotation”.In:Proceedingsofthe26thACMinternationalcon-
ferenceonMultimedia.2018,pp.1957–1966.
40[243] Manuel Haussmann, Fred A Hamprecht, and Melih Kandemir. “Deep active learning with adaptive
acquisition”.In:arXivpreprintarXiv:1906.11471(2019).
[244] Nicolai Spicher, Tim Wesemeyer, and Thomas M Deserno. “Crowdsourcing image segmentation for
deeplearning:integratedplatformforcitizenscience,paidmicrotask,andgamification”.In:Biomedical
Engineering/BiomedizinischeTechnik0(2023).
[245] Olga Russakovsky, Li-Jia Li, and Li Fei-Fei. “Best of both worlds: human-machine collaboration for
objectannotation”.In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
2015,pp.2121–2131.
[246] BorisSekachevetal.opencv/cvat:v1.1.0.Versionv1.1.0.Aug.2020.
[247] AlexanderKirillovetal.“Segmentanything”.In:ProceedingsoftheIEEE/CVFinternationalconfer-
enceoncomputervision(ICCV)(2023).
41