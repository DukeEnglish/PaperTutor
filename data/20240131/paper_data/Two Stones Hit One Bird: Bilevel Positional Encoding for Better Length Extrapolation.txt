Two Stones Hit One Bird: Bilevel Positional Encoding
for Better Length Extrapolation
ZhenyuHe*1 GuhaoFeng*1 ShengjieLuo*1 KaiYang1
DiHe1 JingjingXu2 ZhiZhang2 HongxiaYang2 LiweiWang1
Abstract PG-19textcorpus(Raeetal.,2020a)forvisualization. Itis
evidentthatthetokennumberdistributionineachsegment
In this work, we leverage the intrinsic segmen-
(i.e., sentence)remainsremarkablyconsistent, regardless
tation of language sequences and design a new
of the total sequence length. In contrast, the number of
positionalencodingmethodcalledBilevelPosi-
sentenceslinearlyincreasesasthesequencelengthgrows.
tional Encoding (BiPE). For each position, our
BiPE blends an intra-segment encoding and an Giventheaboveobservations,wearguethatapopularre-
inter-segment encoding. The intra-segment en- searchdirectioninlanguagemodeling,knownasthelength
codingidentifiesthelocationswithinasegment extrapolationproblem(Pressetal.,2022;Aniletal.,2022;
andhelpsthemodelcapturethesemanticinforma- Chietal.,2022;2023;Chowdhury&Caragea,2023;Chen
tionthereinviaabsolutepositionalencoding. The etal.,2023b),shouldbebetterpositionedasanumber-of-
inter-segmentencodingspecifiesthesegmentin- segmentextrapolationproblem. Intheliterature,previous
dex,modelstherelationshipsbetweensegments, studiesinthisdirectionhaveeitherdevelopedbetterposi-
andaimstoimproveextrapolationcapabilitiesvia tional encodings that can handle longer sequence (Raffel
relative positional encoding. Theoretical analy- etal.,2020;Suetal.,2021;Ruossetal.,2023a;Kazemnejad
sis shows this disentanglement of positional in- etal.,2023;Chenetal.,2023b;Lietal.,2023;Pengetal.,
formation makes learning more effective. The 2023;Zhuetal.,2023;Chenetal.,2023a;Liuetal.,2023)
empiricalresultsalsoshowthatourBiPEhassu- or proposed specific inductive biases associated with the
periorlengthextrapolationcapabilitiesacrossa attentionpatternsinlanguagemodels(Ratneretal.,2023;
widerangeoftasksindiversetextmodalities. Han et al., 2023; Xiao et al., 2023), or both (Press et al.,
2022;Chietal.,2022;2023;Sunetal.,2023). However,
none of these methods adequately consider or utilize the
1.Introduction intrinsicsegmentationoflanguagedata,nordotheyspecifi-
callyaddresstheextrapolationissueintermsofthenumber
Inmanyscenarios,textcanbeeffectivelydecomposedinto
ofsegments.
modular segments, each expressing a self-contained unit
ofthought(Halliday&Matthiessen,2013). Innaturallan- In this paper, we introduce BiPE (Bilevel Positional
guages, documents are typically composed of sentences. Encoding), a simple yet effective positional encoding
Eachsentencedescribesadistinctideaorargument. Inpro- scheme for improving length extrapolation. Different
gramminglanguages,codeisorganizedintolinesorfunc- from all existing schemes, BiPE employs two distinct
tionclassesthatdefinecoherentoperationorfunctionality. encodingsforeachposition: anintra-segmentencodingand
In mathematics, proofs unfold through a series of deduc- an inter-segment encoding. The intra-segment encoding
tivesteps,eachrepresentingalogicalprogressionfromits identifiesthelocationofthetokenwithinitssegment. As
predecessorstothefinalanswer. a complement, the inter-segment encoding specifies the
segmenttowhichitbelongs. Usingnaturallanguageasan
The lengths of different text sequences may vary signifi-
illustration,differentwordswithinthesamesentenceshare
cantly(Pressetal.,2022). Whatisintriguingisthatempiri-
the same inter-segment positional encoding but possess
cally,weobservedthatforsequenceswithdifferentlengths,
different intra-segment encodings. Conversely, words in
thedistributionofthetokennumberineachmodularseg-
differentsentencesbutoccupyingthesameinter-segment
mentusuallyhasboundedsupportandtendstobeapprox-
position(e.g., thefirsttokenindifferentsentences)share
imately similar. In Figure 1, we utilized the widely used
the same intra-segment encoding while having distinct
*Equalcontribution 1PekingUniversity2ByteDanceInc. inter-segmentencodings. SeeFigure2foranillustration.
BiPEdisentanglesthepositionmodeling,offeringgreater
WorkinginProgress.
1
4202
naJ
92
]GL.sc[
1v12461.1042:viXraBilevelPositionalEncodingforBetterLengthExtrapolation
Figure1.Left: Thedistributionofthetokennumberinonesegmentwithdifferentsequencelengths. Right: Thedistributionofthe
number of segments with different sequence lengths. We use the tokenizer of Llama 2 (Touvron et al., 2023) for tokenization on
PG-19(Raeetal.,2020a).Fullstop“.”andnewline“\n”areusedforsegmentation.Itcanbeseenthatevenwhenthesequencelengthis
around120k,thetokennumberinmostsentencesislessthan50,whilethenumberofsentencesgrowsupto10k.
flexibilityinaddressingthelengthextrapolationproblem. 2.RelatedWork
Attheintra-segmentlevel,theintra-segmentencodingspeci-
This work focuses on the length extrapolation problem
fiespositionswithinthesegment,helpingthemodelcapture
in language modeling, i.e., can a language model that is
thesemanticinformationcontainedtherein. Giventhatthe
trainedonsequenceswithmaximumlengthL stillper-
numberoftokenswithinasegmentisusuallybounded,we train
form well when being tested on sequences with length
discoveredthatutilizingtheoriginalabsolutepositionalen-
L >L ?(Pressetal.,2022). Hereweprovidealitera-
codings(APE,Vaswanietal.,2017)isalreadysufficientat test train
turereviewofexistingapproachesrelatedtothisproblem.
thislevel. Theinter-segmentencodingtargetstocapturethe
relationshipsbetweensegmentsandexhibitscertainextrapo-
2.1.ImprovedPositionalEncodings
lationcapabilities. Therefore,weemployrelativepositional
encodings(RPE,Suetal.,2021;Pressetal.,2022). Inthis
TheoriginalTransformermodel(Vaswanietal.,2017)en-
way, inductive biases in the two levels focus on different
codespositioninformationviaAbsolutePositionalEncod-
aspectsofpositionalinformationandcanbeappropriately
ing(APE),whereeachpositionisequippedwitha(learnable
incorporated into model architectures, leading to a better
orfixedsinusoidal)real-valuedembedding. Butneitherthe
learning process. We further give a theoretical justifica-
learnablenorthefixedsinusoidalembeddingcangeneralize
tionofBiPE,whichsuggeststhattheproposedpositional
welltolongersequences.
encoding scheme can make the Transformer model more
parameter-efficientundersomeconditions. Different from APE that assigns an embedding for each
positioni,Shawetal.(2018)introducedRelativePositional
Extensive experiments are conducted to demonstrate the
Encoding(RPE)whichencodestherelativedistancei−jfor
empiricaleffectivenessofBiPE.First,weempiricallyverify
eachpositionpair(i,j). MostmethodsincorporateRPEas
theexpressivenesspowerofourBiPEinmathematicalrea-
anadditivetermintheattentionmodule(Raffeletal.,2020;
soningtasks(Weietal.,2022;Fengetal.,2023),whichwell
Pressetal.,2022;Chietal.,2022;2023). Thesemethods
alignswithourtheoreticalresults. Second,forthelengthex-
canmitigatethelengthextrapolationproblemtosomeextent
trapolationproblem,weevaluateBiPEandstrongbaseline
butstillhaveseverallimitations. Forexample,Raffeletal.
methodsacrossdiversetaskscoveringlanguagemodeling
(2020)usesthesameattentionbiasforallquery-keypairs
[PG-19(Raeetal.,2020a),ArXivandGithub(Gaoetal.,
witharelativedistancelargerthanK,whichlimitsitsability
2020)]andlongcontextbenchmark[SCROLLS(Shaham
todistinguishdifferentpositionsinlongsequences.
etal.,2022)]. Finally,weconductexperimentsondatasets
ofnormal-lengthsentences. Wealsoconductablationstud- Oneofthemostpopularlyusedrelativepositionalencoding
iestoverifytheeffectivenessofeachmoduleinBiPE.Our inrecentlargelanguagemodelsisRotaryPositionEncoding
empiricalresultsshowthesuperiorperformanceofourBiPE (RoPE)(Suetal.,2021;Chowdheryetal.,2022;Touvron
onmostproblems. etal.,2023). RoPErotatesthequeryandkeyvectorswith
anangleproportionaltotheirabsolutepositionsbeforethe
attention,whichresultsintheattentionbeingafunctionof
therelativedistancebetweentokens.
2BilevelPositionalEncodingforBetterLengthExtrapolation
Whileencoder-onlyTransformers(e.g.,BERT(Devlinetal., 3.3,weconductatheoreticalstudyontheexpressivenessof
2019b))arepermutationequivariantwithoutpositionalen- ourBiPEtofurtherdemonstrateitssoundness.
coding,Havivetal.(2022)showthatdecoder-onlyTrans-
formers with causal attention masks can learn positional 3.1.ModularSegmentsofTextSequence
informationevenwithoutanyexplicitpositionalencoding.
Formally,weuseS = [w ,...,w ,...,w ]todenotethe
Recently,Kazemnejadetal.(2023)discoveredthattheno 1 l L
inputtextsequence,i.e.,anorderedcollectionoftexttokens
positionalencoding(NoPE)modelalsocanhandlelonger
where w denotes the l-th token and L denotes the total
sequencestosomeextentonsmall-scalesynthetictasks,but l
sequencelength. Aspreviouslyintroduced,textsequences
thereisnostronglypositiveevidenceonlarge-scalesettings.
can be decomposed into a series of non-overlapping
modularsegments,i.e.,S=S ⊕S ⊕...S ⊕···⊕S
2.2.ImprovedAlgorithms 1 2 n N
where S is the n-th segment, N is the total number of
n
Tohelppositionalencodingshandlelongersequences,Ru- segments, and ⊕ is the concatenation operation. Each
oss et al. (2023b) recently proposed a way to randomly segment S is defined by S = [w ,w ,...,w ].
n n an an+1 bn
selectasubsetofpositionsfromamuchlargerrangethan Here, a and b denote the starting and ending indices.
n n
those observed during training. The positional informa- Thesegmentationstrategycansimplyusesymboldetection
tionoflongersequencescanthusbesimulated. Zhuetal. (e.g.,newlineandfullstop).
(2023)proposedasimilarideacalledpositionalskip-wise
fine-tuning (PoSE), which requires additional efforts for 3.2.BilevelPositionalEncoding
fine-tuninglarge-scalemodels.
Asstatedintheintroduction,inmanypracticalscenarios,
Relativepositionalencoding,especiallyRoPE,cancapture thenumberoftokensineachsegmentS followsasimilar
n
therelativepositionalinformationwell,butitslengthextrap- distribution regardless of the value L, and the sequence
olationcapabilityisnotsatisfactoryyet. Duetothis,one length L has a major impact on the segment number N.
lineofworksintroducespriorsbiasedtowardlocalwindow Consequently,webelievemodelingthelengthextrapolation
attention via additive RPEs (Press et al., 2022; Chi et al., forN isamoreeffectiveapproachthanthatforactuallength
2022; 2023; Sun et al., 2023) or hard constraints (Ratner L. Tothisend,weproposeBiPE,anovelbilevelpositional
et al., 2023; Xiao et al., 2023; Han et al., 2023) to boost encodingthatblendstwodistinctencodingschemesforeach
lengthextrapolationcapabilities. Anotherlineofworkstai- position: anintra-segmentencodingandaninter-segment
loredtoRoPEcalledpositionalembeddingscaling(Chen encoding. (SeeFigure2).
etal.,2023b;Pengetal.,2023;Roziereetal.,2023;Chen
etal.,2023a;Liuetal.,2023)adjuststherangeofeitherthe Intra-SegmentEncoding. Inatextsequence,eachmod-
positionindexorthefrequencybasisinRoPE,achieving ularsegmentdescribesanindependentstatement,andthe
promisingextrapolationperformance. Recently,aconcur- intra-segment positional encoding serves as an anchor to
rent work (Jin et al., 2024) proposed a bilevel attention identifythelocationofeachtokeninthesegmentforcap-
mechanismforbetterlengthextrapolationofRoPE-based turingsemanticinformationtherein. Formally,withineach
languagemodels. Itkeepstheexactattentioncomputation segmentS = [w ,w ,...,w ],weencodethe(lo-
n an an+1 bn
withinapre-definedneighborrangeandusestheflooroper- cal)positionifortokenw ,where1≤i≤b −a +1.
an+i n n
ationtogroupandmapunseenlargerelativepositions. Notethatthenumberoftokenswithinasegmentisusually
bounded, i.e., there are few sentences that are extremely
BiPEaimstodevelopanewpositionalencodingscheme,
long. Wefindusingtheoriginalabsolutepositionalencod-
which is orthogonal to all the methods above. All these
ing(Vaswanietal.,2017)isenough. Foreachtokenw
advancementscanbeseamlesslycombinedwithBiPEfor an+i
inS , weassignareal-valuedembeddinge toit, which
betterlengthextrapolation. Ourexperiments(Section4)on n i
willbeaddedtotheinputtokenembedding. e isshared
severalrepresentativealgorithmsprovidestrongevidence i
amongtokensatthesamelocalpositioniindifferentS .
supportingthecompatibilityofBiPE. n
Inter-SegmentEncoding. Thoughtheintra-segmenten-
3.Method
codingcanprovidethelocationoftokenswithineachseg-
ment, the locations across segments are mixed, and the
In this section, we introduce BiPE (Bilevel Positional
contextualrelationshipsbetweensegmentsarenotcaptured.
Encoding), a new positional encoding scheme for better
Asacomplement,weusetheinter-segmentpositionalen-
lengthextrapolationcapabilities.First,weformallydescribe
codingtospecifythesegmenttowhicheachtokenbelongs.
themodularsegmentsoftextsequenceinSection3.1.Based
Keepinginmindthatthisencodingwillplayanotherrole
onthissegmentrepresentation,wethoroughlyillustratethe
inhandlinglongersequencesthatareunseenduringtrain-
derivationofourBiPEmethodinSection3.2. InSection
ing,weemployrelativepositionalencodings(Shawetal.,
3BilevelPositionalEncodingforBetterLengthExtrapolation
StandardPositionalEncoding
L
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 … Feed-Forward ×
Network
BilevelPositionalEncoding(ours)
Inter-Segment 1 2 3 Inter-Segment Multi-Head
(Relative)PositionalEncoding Attention
Intra-Segment
Intra-Segment 1 2 3 4 1 2 3 4 5 1 2 3 4 5 6 … (Absolute)PositionalEncoding Input Embedding
Figure2. Left:TheschematiccomparisonofStandardPositionalEncoding(top)andourproposedBilevelPositionalEncoding(BiPE,
bottom). BiPEdifferentiatespositionsusingbothintra-segmentandinter-segmentencodings. Right: Absolutepositionalencoding
isusedasIntra-SegmentEncodingaddedtotheinputembeddingandrelativepositionalencoding(e.g.,RoPEandALiBi)isusedas
Inter-SegmentEncodingintheTransformerattentionmodule.
2018;Raffeletal.,2020;Suetal.,2021;Pressetal.,2022). Markov model (Fine et al., 1998), hierarchical recur-
Different from previous RPEs that are defined using the rent model (Chung et al., 2016), and hierarchical topic
distancebetweentokenindexes,theinter-segmentencoding model(Bleietal.,2003;Griffithsetal.,2003). Wefollowto
isdefinedusingthedistancebetweensegmentindexes. usethisassumptiontoinvestigatetheparameterefficiency
ofBiPE.Inparticular,weleveragethe(non-deterministic)
finiteautomata(NFA),whichiswidelyusedinthefieldof
Instantiation. BiPEusesabsolutepositionalinformation
theoreticalcomputerscience. Aluretal.(1999)proposed
fortheintra-segmentencodingandcanleverageanyRPE
hierarchicalfiniteautomataasapracticalwaytorepresent
approachesfortheinter-segmentencoding. Inthiswork,we
suchlinguisticstructures. Inspiredbytheframework,we
instantiatetwoBiPEvariants,BiPE-RoPEandBiPE-ALiBi.
introduceasimplifiedmodel,Bi-NFA,whichrestrictsthe
BiPE-RoPEleveragesRoPE(Suetal.,2021)astheinter- hierarchylevelofhierarchicalfiniteautomatatotwo. We
segment encoding. For a pair of tokens (w l1,w l2) which comparetheparameterefficiencyofTransformerstorepre-
areinthen-thsegmentandthem-thsegmentrespectively, sentNFAandBi-NFAandshowthatBiPEhasatheoretical
tworotationmatricesR Θ,nandR Θ,mareassigned,where advantageoverexistingpositionalencodingschemes.
Θ denotes the pre-defined parameters of the rotation ma-
trix(Suetal.,2021). Givenquery-keypairq ,k ∈ Rd, NFA.Anondeterministicfiniteautomaton(NFA)isafun-
l1 l2
damentalandessentialcomputationalmodelincomputer
the attention score is computed by
ql1RΘ,n( √kl2RΘ,m)T
=
science(Eilenberg,1974). AnNFAN canbedefinedasa
d
ql1RΘ√,n−mk lT
2. BiPE-ALiBiusesALiBi(Pressetal.,2022)
tupleN =(Q,Σ,δ,q 0,F),whereQisasetofstates,Σis
d thealphabetofinputsymbols,δ :Q×Σ→P(Q)isatransi-
astheinter-segmentencoding. Similarly,therelativeseg-
tionfunction,q ∈Qistheinitialstate,andF ⊆Qisaset
mentdistancen−miscalculatedfortokenpair(w ,w ). 0
l1 l2 offinalstates. P(Q)denotesthepowersetofQ. Astring
Theattentionscorebetweenthetwotokensiscomputedby
S = [w ,w ,··· ,w ] ∈ Σ∗ isacceptedbyN ifthereex-
ql√1k lT
2 +r|n−m|,whererisapre-definedhyper-parameter.
istsase1 quen2 ceofstan
tesr ,r ,··· ,r ∈Qsuchthatr =
d 0 1 n 0
q ,r ∈δ(r ,w )fori=0,1,...,n−1,andr ∈F.
0 i+1 i i+1 n
Discussion. TheoriginalBERT(Devlinetal.,2019a)also Bi-NFA.Weutilizethehierarchicalautomatatocapturethe
includestwoencodingsforrepresentingpositions,butits structureofmodularsegmentsandintroducetheBi-NFAby
approachdifferssignificantlyfromBiPE.Primarily,BERT restrictingthehierarchyleveltotwo. ABi-NFAisatuple
onlyneedstospecifytwosegmentsusingabsoluteencoding, N =(Q,Σ,δ,q ,F),whereQisthecollectionofstatesets
0
tailoredforthenextsentencepredictiontasknotforlength Q ,Q ,··· ,Q , Σ is the symbol set that includes a seg-
1 2 n
extrapolation. Furthermore, BERTtreatsasequenceasa mentseparatorw∗,δisthetransitionkernel,q istheinitial
0
flatarrayoftokensanddefinesthesegmentsinanarbitrary state,andF isthesetofacceptstates. Themaindifference
way,ignoringintrinsicsegmentationoflanguagedata. An between Bi-NFA and NFA is that the transitions are con-
empiricalcomparisoncanbefoundinSection4.5. strainedbythestatesetsandthesegmentseparator. Specif-
ically, for any state q ∈ Q and any symbol w, we have
i
3.3.TheoreticalAnalysisofBiPE δ(q,w) ⊂ Q if w ̸= w∗ and δ(q,w∗) ⊂ {q∗,··· ,q∗},
i 1 n
where q∗ is the start state in Q and q ∈ {q∗,··· ,q∗}.
Many previous works are built upon an assumption i i 0 1 k
Thus,Bi-NFAstayswithinthesamestatesetuntilitreads
that tokens are generated in a hierarchical manner in
the segment separator, and then it can move to any other
natural language and develop the hierarchical hidden
4BilevelPositionalEncodingforBetterLengthExtrapolation
statesetinQ. TheBi-NFAcanbeviewedasavariantof hidden dimension=48 hidden dimension=64 hidden dimension=256
NFAthatprocessestheinputsequencesegmentbysegment. 100 94 100 97 100 97 100 100 97 98 100 95 100100
85
Theorem3.1(LowerboundforTransformerwithabsolute 80
positionalencodingtorepresentNFA). Foranysizeofstate 68 69
60
set, thereexistsanNFAN = (Q,Σ,δ,q ,F)suchthata
0
TransformerwithAPEneedsatleastO(|Q|2)embedding 40
sizetorepresenttheNFA.
20 15 15
Theorem3.2(UpperboundforTransformerwithBiPEto
0
representBi-NFA). ForanyBi-NFAN =(Q,Σ,δ,q ,F), Sinusoidal RoPE XPOS ALiBi BiPE-ALiBiBiPE-RoPE
0
Q = {Q ,Q ,··· ,Q } there exists a Transformer with
1 2 k Figure3. Accuracy of Transformer models with different posi-
BiPEandO(k2+(cid:80) |Q |2)embeddingsizecanrepre-
i∈[k] i tionalencodingmethodsontheArithmetictask.OurBiPEmethod
senttheBi-NFA. consistentlyperformsbestondifferentscalesofparameters.
The proof of Theorems 3.1 and 3.2 can be found in Ap-
pendixAandTheorem3.1canbeextendedtorelativepo- 4.1.CapacityExperiments
sitional encodings. For a Bi-NFA N = (Q,Σ,δ,q ,F),
0
denote T = (cid:80) |Q | as the number of states, and as- Tasks. To empirically verify the parameter efficiency
i√∈[k] i
broughtbyourBiPEmethod,weconductexperimentson
sume|Q |=O( T). IfnaivelytreatingN asanNFAwith
i the Arithmetic task (Feng et al., 2023), which is recently
T states,wecandirectlyobtainfromTheorem3.1thatan
usedasaproxytoexaminethemathematicalreasoningcapa-
absolute positional encoding-based Transformer requires
bilityoflanguagemodels. Givenanarithmeticalexpression
atleastO(T2)dimensionstorepresentit. However,from
consistingofnumbers,basicoperations(+,−,×,÷,=)and
Theorem3.2,bywellexploitingthehierarchicalstructure,a
TransformerwithBiPEonlyrequiresO(N23)dimensions.
brackets,e.g.,(1+2)×(3+5)=,thistaskrequireslanguage
modelstocalculateandgeneratethecorrectresult,e.g.,24.
This suggests the superiority of the BiPE over previous
Following Feng et al. (2023), we train all models using
methodfromatheoreticalperspective.
Chain-of-Thoughtdemonstrations(SeeAppendixB.1). The
evaluationmetricistheaccuracyofthefinalanswer.
4.Experiments
Inthissection,weempiricallystudytheeffectivenessofour Settings. TheArithmeticdatasetfromFengetal.(2023)
BiPEmethod. Inparticular,weaimtoanswerthefollowing consistsof1milliontrainingsamplesand100ktestsamples
questionsthroughexperiments: intotal. Wechoosethestandarddecoder-onlyTransformer
languagemodelasthebasemodelandcompareourBiPE
• Q1: Do the theoretical results regarding parameter method with the following competitive positional encod-
efficiencyofBiPEholdinpractice? (Section4.1) ings: 1)SinusoidalPE(Vaswanietal.,2017);2)RoPE(Su
etal.,2021);3)XPOS(Sunetal.,2023);4)ALiBi(Press
• Q2: Does BiPE bring superior length extrapolation
etal.,2022). Inparticular,weimplementtwoversionsof
capabilitiesinreal-worldtasks? (Section4.2)
ourBiPE,BiPE-RoPEandBiPE-ALiBi,whichinstantiates
• Q3:DoesBiPEhelpTransformer-basedlanguagemod- the inter-segment encoding via RoPE and ALiBi respec-
elsbetterunderstandlongtext? (Section4.3) tively. Thesegmentboundaryissimplydeterminedbythe
equalsign“=”. FollowingFengetal.(2023), wesetthe
• Q4: Does BiPE hurt performance on normal-length numberoflayersto3andthenumberofattentionheadsto
text? (Section4.4) 4. Toevaluatetheparameterefficiency,wevarythehidden
dimensionin[48,64,256]. Additionalexperimentaldetails
• Q5: IseachdesignchoiceinBiPEhelpful? (Section
arepresentedinAppendixB.1.
4.5)
We will thoroughly answer each question with carefully Results. In Figure 3, it can be easily seen that given a
designedexperimentsonwidelyusedbenchmarksasbelow. similaramountofparameters,BiPE-basedlanguagemodels
Wealsocoverdifferentmodalitiesintheexperiments, in- consistently outperform other baselines on this task. For
cludingmath(arithmeticalreasoningtaskinSection4.1), example,whenthehiddendimensionis48,otherpositional
natural language (PG19&ArXiv task in Section 4.2) and encodingmethodsachieveinferioraccuracy(below70%),
code (Github task Section 4.2). We run each experiment whileBiPE-ALiBiandBiPE-RoPEachievehighaccuracy
multipletimeswithdifferentrandomseedsandreporttheav- of97%and95%respectively. Thisresultindeedwellaligns
eragedresults. Duetospacelimits,wepresentmoredetails with our theoretical results in Section 3.3, which further
andadditionalresultsinAppendixB. servesasastrongsupportforthebileveldesignofourBiPE.
5
)%(
ycaruccABilevelPositionalEncodingforBetterLengthExtrapolation
PG19 Language Modeling ArXiv Language Modeling Github Language Modeling
80 20 20.0
Sinusoidal Sinusoidal Sinusoidal
70 RoPE 18 RoPE 17.5 RoPE
Randomized RoPE Randomized RoPE Randomized RoPE
XPOS 16 XPOS 15.0 XPOS
60
A BL iPiB E-i RoPE 14 A BL iPiB E-i RoPE 12.5 A BL iPiB E-i RoPE
50 BiPE-ALiBi 12 BiPE-ALiBi 10.0 BiPE-ALiBi
40 10 7.5
30 8 5.0
6
20 2.5
4
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
Number of Tokens (×103) Number of Tokens (×103) Number of Tokens (×103)
Figure4.Languagemodelingperplexitywithvaryingevaluationsequencelengthsformodelstrainedonsequencelength1024.
4.2.LengthExtrapolationExperiments ingthebetterlengthextrapolationcapabilityofourBiPEin
real-worldtasks.
Tasks. We test the length extrapolation capability of
Transformer-based language models with different posi- Integrating BiPE with fine-tuning strategies. One line
tionalencodingmethods. FollowingChietal.(2022),we of recent improvements on length extrapolation comes
usethePile(Gaoetal.,2020)datasetasthepre-trainingcor- fromcontinuedfine-tuningRoPE-basedlanguagemodels
pusandevaluatethelogperplexityofpre-trainedlanguage withPositionInterpolationtechniques(Chenetal.,2023b;
modelsonthetestsetofPG19(Raeetal.,2020b),arXivand Peng et al., 2023). To further investigate the compatibil-
Github(Gaoetal.,2020). Weconductthenon-overlapping ity of our BiPE method with Position Interpolation, we
evaluationwhencomputingtheperplexityscore. use YaRN (Peng et al., 2023) to finetune the language
modelpre-trainedonthePiledatasetwithRoPE(Suetal.,
Settings. Wesetthepre-trainingsequencelengthto1024,
2021) and our BiPE-RoPE, and check the improvements
andevaluatethezero-shotperplexityonsequencelengths
ondownstreamdatasets. TheresultsarepresentedinFig-
[1024,2048,3072,4096,5120,6144,7168,8192]ondown-
ure5. Similartothezero-shotevaluationsetting,ourBiPE-
streamdatasets.Wechoosethestandarddecoder-onlyTrans-
RoPEachievesconsistentlybetterperformanceonlonger
formerasthebasemodelandcompareourBiPEmethods
sequencescomparedtoRoPEafterfinetuning. Furthermore,
(BiPE-RoPEandBiPE-ALiBi)withthefollowingpositional
althoughYaRNimprovesthelengthextrapolationcapability
encodings: 1) Sinusoidal PE (Vaswani et al., 2017); 2)
ofRoPEtosomeextent,itstillsuffersfromperformance
RoPE(Suetal.,2021);3)RandomizedRoPE(Ruossetal.,
drop when being evaluated on very long sequences, e.g.,
2023a);4)XPOS(Sunetal.,2023);5)ALiBi(Pressetal.,
11k/16k/16kforPG19/ArXiv/Github. Incontrast,ourBiPE-
2022). Thesegmentboundaryisdeterminedbyfullstop“.”
RoPEcombinedwithYaRNyieldsmuchbetterlengthex-
andnewline“\n”forgeneralpurposes.FortheTransformer-
trapolationcapability, i.e. maintainingaconsistentlylow
basedlanguagemodel,wesetthenumberoflayersto12,the
perplexityacrosssequenceswithlengthsupto20k. Please
hiddendimensionto768,andthenumberofattentionheads
refertoAppendixB.3formoreexperimentaldetails.
to 12. The total number of model parameters is approxi-
mately155M.Additionalexperimentaldetailsarepresented
4.3.LongContextBenchmark
inAppendixB.2.
Tasksandsettings. Toevaluatethemodel’sperformance
Results. TheresultsarepresentedinFigure4. OurBiPE
oflongcontextunderstanding,wefurtherfine-tunethepre-
methodsachieveconsistentlysuperiorperformanceonse-
trainedcheckpointsonSCROLLS(Shahametal.,2022),a
quences with lengths larger than the training length. For
longtextbenchmarkthatconsistsofsevendistinctdatasets
example,ourBiPE-ALiBioutperformsitscounterpartAL-
coveringdifferenttasks. Following Shahametal.(2022);
iBi,whichisalsothebestbaselinemethod,by3.35points
Ainslie et al. (2023), we use three evaluation metrics for
(25.24v.s. 28.59perplexity)onPG19with8192sequence
differenttasks: Rgmscore(thegeometricmeanofROUGE-
length. Compared to RoPE which performs well on se-
1,2,L), unigram overlap (F1) and exact match (EM). The
quences with the in-distribution length but yields a sig-
averagescoreacrossdifferentdatasetsisalsoreported. We
nificantperformancedroponlongersequences,ourBiPE
finetunepre-trainedmodelsusingasequencelengthof8192
methodsubstantiallyimprovesitslengthextrapolationca-
andselectthemodelcheckpointthatachievesthebestperfor-
pabilities,e.g.,19.67v.s. 158perplexityonPG19withthe
manceonthevalidationsetforthefinalevaluation. Thetest
4096sequencelength. Notably,thebenefitbroughtbyour
resultsareobtainedfromtheofficialSCROLLSwebsite.Ad-
BiPEmethodisalsoconsistentacrossallthreeevaluation
ditionalexperimentaldetailsarepresentedinAppendixB.4.
datasetscoveringtextdataindifferentmodalities,underscor-
6
ytixelpreP
tseT
ytixelpreP
tseT
ytixelpreP
tseTBilevelPositionalEncodingforBetterLengthExtrapolation
PG19 Language Modeling ArXiv Language Modeling Github Language Modeling
80 20 20.0
RoPE RoPE RoPE
70 RoPEyarn 18 RoPEyarn 17.5 RoPEyarn
BiPE-RoPE BiPE-RoPE BiPE-RoPE
60 BiPE-RoPEyarn 16 BiPE-RoPEyarn 15.0 BiPE-RoPEyarn
14 12.5
50 12 10.0
40 10 7.5
30 8 5.0
6
20 2.5
4
1 3 5 7 9 11 13 15 17 19 1 3 5 7 9 11 13 15 17 19 1 3 5 7 9 11 13 15 17 19
Number of Tokens (×103) Number of Tokens (×103) Number of Tokens (×103)
Figure5. LanguagemodelingperplexitywithvaryingevaluationsequencelengthsforRoPEandBiPE-RoPEfinetunedwithYaRN.
Table1.PerformancecomparisononSCROLLSbenchmark. Abbreviationsfordatasetnames: Qasper(Qas),ContractNLI(CNLI),
QMSum(QMS),NarrativeQA(NQA),SummScreenFD(SumS),GovReport(GovR),andQuALITY(QuAL).Rgmdenotesthegeometric
meanofROUGE-1,2,L.ThestatisticsofmediansequencelengthsarefromAinslieetal.(2023);Lietal.(2023).Bestperformingresults
arehighlightedinbold.
QAS CNLI QMS NQA SumS GovR QuAL Average
Metric F1 EM Rgm F1 Rgm Rgm EM
Medianlength 5472 2148 14197 57829 9046 8841 7171
Sinusoidal 9.3 57.7 12.42 10.1 7.46 12.49 1.9 15.89
RandomizedRoPE 12.3 52.4 11.80 10.8 7.19 18.95 10.4 17.71
ALiBi 12.0 68.8 10.27 3.2 6.00 23.14 0.0 17.62
BiPE-ALiBi 12.7 67.8 10.44 2.6 7.89 27.52 0.0 18.34
RoPE 16.4 67.8 10.13 9.7 9.88 14.33 0.4 18.38
BiPE-RoPE 21.2 68.9 10.64 12.3 8.13 27.92 7.4 22.36
RoPE 14.7 66.9 9.04 12.2 8.48 27.56 22.2 23.01
yarn
BiPE-RoPE 20.9 69.0 10.57 13.3 9.40 28.31 20.3 24.53
yarn
Results. The empirical results are provided in Table 1. 2020),TruthfulQAmc2(Linetal.,2022),andPIQA(Gao
First, BiPE-RoPE and BiPE-ALiBi exhibit better perfor- etal.,2020)benchmarksforthezero-shotevaluation,and
mance than RoPE and ALiBi, respectively. For example, employ10-shotHellaSwag(Zellersetal.,2019)and5-shot
ourBiPE-RoPEoutperformsitscounterpartRoPE,which MMLU (Hendrycks et al., 2021) for the few-shot evalua-
isalsothebestbaselinemethod,by3.98points(22.36v.s. tion. Theevaluationmetricsaretask-specific: forRACE,
18.38 average score). Besides, BiPE-RoPE achieves the WinoGrande,TruthfulQA,PIQA,andMMLU,wereportac-
highestaveragescore,surpassingothermethodsbyamar- curacy;andforHellaSwag,wereportnormalizedaccuracy.
ginofover3points. Onatask-by-taskbasis,BiPE-RoPE
Results. The empirical results are provided in Table 2.
achievesthetopscorein4outofthe7tasks. Wealsocom-
It can be easily seen that BiPE-RoPE and BiPE-ALiBi
parethetwoYaRN-finetunedmodels,i.e.,BiPE-RoPE
yarn achievecomparableperformancewithotherpositionalen-
andRoPE . WecanseethatBiPE-RoPE stillcon-
yarn yarn codingmethodsonsequenceswithin-distributionlengths,
sistently outperforms RoPE across 6 of 7 tasks and
yarn which demonstrates that our BiPE methods achieve bet-
achievesabetteraveragescore. Theresultsstrengthenthe
ter length extrapolation performance without sacrificing
effectivenessofBiPEinlong-contextmodeling.
in-distributionperformance.
4.4.Normal-lengthBenchmark
4.5.AblationStudy
Tasksandsettings.Inthisexperiment,weevaluatethezero- Effectivenessofeachpositionalencoding. BiPEleverages
shot and few-shot performance (Gao et al., 2023) of pre- two positional encodings, with one corresponding to the
trainedmodelsonarangeof“in-distribution”benchmark tokenindexwithineachsegment(intra-segmentencoding)
taskswherethesequencelengthisnormal. Inparticular,we andanotherforthesegmentindex(inter-segmentencoding).
useRACE(Laietal.,2017),WinoGrade(Sakaguchietal., Tochecktheireffectiveness,weconductedanexperiment
7
ytixelpreP
tseT
ytixelpreP
tseT
ytixelpreP
tseTBilevelPositionalEncodingforBetterLengthExtrapolation
Table2.Zero-shotandfew-shotperformanceonstandardbenchmarks.BiPE-basedmodelsperformonparwithothermethods.
Zero-Shot Few-Shot
Model
RACE WinoGrande TruthfulQA PIQA HellaSwag MMLU
Sinusoidal 27.85 52.09 45.92 60.88 29.70 26.49
RandomizedRoPE 26.41 50.99 45.53 60.66 29.30 25.20
XPOS 27.56 52.96 45.22 60.88 30.86 25.96
ALiBi 27.08 52.72 46.24 60.50 31.47 26.49
BiPE-ALiBi 28.42 49.25 45.79 60.72 30.60 25.74
RoPE 29.00 51.54 44.67 60.66 30.86 26.43
BiPE-RoPE 28.04 52.01 45.64 59.74 30.93 26.91
RoPE 27.08 53.35 45.69 60.12 30.52 26.16
yarn
BiPE-RoPE 27.56 51.38 45.80 60.72 30.61 26.22
yarn
PG19 Language Modeling PG19 Language Modeling
80 80
BiPE-RoPE BiPE-RoPE
70 BiPE-RoPE w/o intra 70 BiPE-RoPE ( =16)
BiPE-RoPE w/o inter BiPE-RoPE ( =256)
60 60
50 50
40 40
30 30
20 20
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
Number of Tokens (×103) Number of Tokens (×103)
Figure6. Left:LanguagemodelingperplexitywithvaryingevaluationsequencelengthsforBiPE-RoPEwithoutintra-segmentencoding
orinter-segmentencodingonPG19dataset.Right:LanguagemodelingperplexitywithvaryingevaluationsequencelengthsforBiPE-
RoPEwithfixedsegmentlengthsonPG19dataset.
where we removed one encoding at a time to assess the encodingthatidentifiesthelocationwithinitssegmentvia
impact on model performance. We follow the same pre- APE, and 2) an inter-segment encoding that specifies the
training setting in Section 4.2 and evaluate perplexity on segment to which it belongs via RPE. The intra-segment
PG19. InFigure6left,thedegradationinperformanceis encoding assists the model in capturing the semantic in-
observedupontheremovalofeitherencoding,whichclearly formationwithineachsegmentandandtheinter-segment
demonstratesthatbothofthemareimportant. encodingmodelstherelationshipsbetweensegments. This
bileveldesignwellalignswiththeintrinsicsegmentationof
Segmentation choices. In Section 4.2, we use full stop
textdataandenhanceslengthextrapolation.OurBiPEisfur-
“.” and newline “\n” for segmenting the text. One may
thersupportedbytheoreticalanalysisofitsexpressiveness.
wonder whether using a fixed segment length instead of
Allexperimentsverifythelengthextrapolationcapabilityof
pre-definedsymbolsworksinpractice. Tocheckthis,we
ourBiPEacrosstasksofdifferenttextmodalities.
assumeeachsegmentlengthisconstantγ,pre-trainBiPE
languagemodelsandevaluatetheperformanceonthePG19 Therearealsoseveralfuturedirectionsworthinvestigating.
dataset.InFigure6right,wecanseethatthisnaiveapproach First,theintrinsicsegmentationoftextdatayieldsahierar-
does not achieve the same level of performance as using chicalstructure,e.g.,sentences→paragraphs→documents.
naturalsegmentation. It would be beneficial to confirm whether expanding our
bileveldesigntoahierarchicalversionresultsinimproved
lengthextrapolation. Second,thereexistsequencedatathat
5.ConclusionandFutureDirections
donothaveclearboundaryofsegmentations,e.g.,timese-
Inthispaper,weintroduceBiPE,anovelbilevelpositional ries,aminoacidandgenesequence. Futureresearchcould
encodingschemedesignedtoimprovelengthextrapolation. explorebetterandmorecomprehensivesegmentationmeth-
Foreachposition,ourBiPEcombines1)anintra-segment odsforgeneralpurposes.
8
ytixelpreP
tseT
ytixelpreP
tseTBilevelPositionalEncodingforBetterLengthExtrapolation
ImpactStatement Chi,T.-C.,Fan,T.-H.,Rudnicky,A.,andRamadge,P. Dis-
sectingtransformerlengthextrapolationviathelensof
Thispaperpresentsworkwhosegoalistoadvancethefield
receptivefieldanalysis.InProceedingsofthe61stAnnual
of Machine Learning. There are many potential societal
MeetingoftheAssociationforComputationalLinguistics
consequences of our work, none which we feel must be
(Volume1: LongPapers),pp.13522–13537,2023.
specificallyhighlightedhere.
Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,
References G., Roberts, A., Barham, P., Chung, H. W., Sutton,
C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
Ainslie,J.,Lei,T.,deJong,M.,Ontanon,S.,Brahma,S.,
S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
Zemlyanskiy,Y.,Uthus,D.,Guo,M.,Lee-Thorp,J.,Tay,
N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,
Y., Sung, Y.-H., and Sanghai, S. CoLT5: Faster long-
Pope,R.,Bradbury,J.,Austin,J.,Isard,M.,Gur-Ari,G.,
rangetransformerswithconditionalcomputation. InThe
Yin,P.,Duke,T.,Levskaya,A.,Ghemawat,S.,Dev,S.,
2023ConferenceonEmpiricalMethodsinNaturalLan-
Michalewski,H.,Garcia,X.,Misra,V.,Robinson,K.,Fe-
guageProcessing,2023.
dus,L.,Zhou,D.,Ippolito,D.,Luan,D.,Lim,H.,Zoph,
B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,
Alur,R.,Kannan,S.,andYannakakis,M. Communicating
S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
hierarchicalstatemachines. InAutomata,Languagesand
Lewkowycz,A.,Moreira,E.,Child,R.,Polozov,O.,Lee,
Programming:26thInternationalColloquium,ICALP’99
K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,
Prague,CzechRepublic,July11–15,1999Proceedings
Catasta,M.,Wei,J.,Meier-Hellstern,K.,Eck,D.,Dean,
26,pp.169–178.Springer,1999.
J., Petrov, S., and Fiedel, N. Palm: Scaling language
modelingwithpathways,2022.
Anil,C.,Wu,Y.,Andreassen,A.,Lewkowycz,A.,Misra,
V.,Ramasesh,V.,Slone,A.,Gur-Ari,G.,Dyer,E.,and
Chowdhury, J. R. and Caragea, C. Monotonic loca-
Neyshabur,B. Exploringlengthgeneralizationinlarge
tionattentionforlengthgeneralization. arXivpreprint
languagemodels. AdvancesinNeuralInformationPro-
arXiv:2305.20019,2023.
cessingSystems,35:38546–38556,2022.
Chung, J., Ahn, S., and Bengio, Y. Hierarchical mul-
Blei,D.M.,Ng,A.Y.,andJordan,M.I. Latentdirichlet
tiscale recurrent neural networks. arXiv preprint
allocation. JournalofmachineLearningresearch,3(Jan):
arXiv:1609.01704,2016.
993–1022,2003.
Dasigi,P.,Lo,K.,Beltagy,I.,Cohan,A.,Smith,N.A.,and
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
Gardner,M. Adatasetofinformation-seekingquestions
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
andanswersanchoredinresearchpapers. InProceedings
Askell,A.,etal. Languagemodelsarefew-shotlearners.
ofthe2021ConferenceoftheNorthAmericanChapterof
Advancesinneuralinformationprocessingsystems,33:
theAssociationforComputationalLinguistics: Human
1877–1901,2020.
LanguageTechnologies,June2021.
Chen,G.,Li,X.,Meng,Z.,Liang,S.,andBing,L. Clex:
Continuouslengthextrapolationforlargelanguagemod- Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.BERT:
els. arXivpreprintarXiv:2310.16450,2023a. Pre-training of deep bidirectional transformers for lan-
guageunderstanding. InProceedingsofthe2019Confer-
Chen,M.,Chu,Z.,Wiseman,S.,andGimpel,K. Summ- enceoftheNorthAmericanChapteroftheAssociation
Screen: Adatasetforabstractivescreenplaysummariza- forComputationalLinguistics: HumanLanguageTech-
tion. InProceedingsofthe60thAnnualMeetingofthe nologies,Volume1(LongandShortPapers),June2019a.
Association for Computational Linguistics (Volume 1:
LongPapers),pp.8602–8615,May2022. Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.BERT:
Pre-training of deep bidirectional transformers for lan-
Chen, S., Wong, S., Chen, L., and Tian, Y. Extending guageunderstanding. InProceedingsofthe2019Confer-
contextwindowoflargelanguagemodelsviapositional enceoftheNorthAmericanChapteroftheAssociationfor
interpolation. arXivpreprintarXiv:2306.15595,2023b. ComputationalLinguistics: HumanLanguageTechnolo-
gies,Volume1(LongandShortPapers),pp.4171–4186,
Chi,T.-C.,Fan,T.-H.,Ramadge,P.J.,andRudnicky,A.Ker- June2019b.
ple: Kernelizedrelativepositionalembeddingforlength
extrapolation. AdvancesinNeuralInformationProcess- Eilenberg, S. Automata, languages, and machines. Aca-
ingSystems,35:8386–8399,2022. demicpress,1974.
9BilevelPositionalEncodingforBetterLengthExtrapolation
Feng,G.,Zhang,B.,Gu,Y.,Ye,H.,He,D.,andWang,L. Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P.,
Towardsrevealingthemysterybehindchainofthought: and Reddy, S. The impact of positional encoding on
Atheoreticalperspective. InThirty-seventhConference length generalization in transformers. arXiv preprint
onNeuralInformationProcessingSystems,2023. arXiv:2305.19466,2023.
Fine, S., Singer, Y., and Tishby, N. The hierarchical hid- Kocˇisky´,T.,Schwarz,J.,Blunsom,P.,Dyer,C.,Hermann,
denmarkovmodel: Analysisandapplications. Machine K.M.,Melis,G.,andGrefenstette,E. TheNarrativeQA
learning,32:41–62,1998. reading comprehension challenge. Transactions of the
AssociationforComputationalLinguistics,6,2018.
Gao,L.,Biderman,S.,Black,S.,Golding,L.,Hoppe,T.,
Foster,C.,Phang,J.,He,H.,Thite,A.,Nabeshima,N., Koreeda,Y.andManning,C. ContractNLI:Adatasetfor
et al. The Pile: An 800GB dataset of diverse text for document-levelnaturallanguageinferenceforcontracts.
language modeling. arXiv preprint arXiv:2101.00027, InFindingsoftheAssociationforComputationalLinguis-
2020. tics: EMNLP2021,pp.1907–1919,November2021.
Gao,L.,Tow,J.,Abbasi,B.,Biderman,S.,Black,S.,DiPofi,
Lai, G., Xie, Q., Liu, H., Yang, Y., andHovy, E. RACE:
A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li,
Large-scale ReAding comprehension dataset from ex-
H.,McDonell,K.,Muennighoff,N.,Ociepa,C.,Phang,
aminations. InProceedingsofthe2017Conferenceon
J.,Reynolds,L.,Schoelkopf,H.,Skowron,A.,Sutawika,
EmpiricalMethodsinNaturalLanguageProcessing,pp.
L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,
785–794,September2017.
A. Aframeworkforfew-shotlanguagemodelevaluation,
12 2023. URL https://zenodo.org/records/ Li, S., You, C., Guruganesh, G., Ainslie, J., Ontanon, S.,
10256836. Zaheer,M.,Sanghai,S.,Yang,Y.,Kumar,S.,andBho-
janapalli, S. Functional interpolation for relative posi-
Griffiths,T.,Jordan,M.,Tenenbaum,J.,andBlei,D. Hier-
tionsimproveslongcontexttransformers. arXivpreprint
archicaltopicmodelsandthenestedchineserestaurant
arXiv:2310.04418,2023.
process. Advancesinneuralinformationprocessingsys-
tems,16,2003. Lin, S., Hilton, J., and Evans, O. TruthfulQA: Measur-
inghowmodelsmimichumanfalsehoods. InProceed-
Halliday, M. A. K. and Matthiessen, C. M. Halliday’s
ingsofthe60thAnnualMeetingoftheAssociationfor
introductiontofunctionalgrammar. Routledge,2013.
ComputationalLinguistics(Volume1: LongPapers),pp.
Han,C.,Wang,Q.,Xiong,W.,Chen,Y.,Ji,H.,andWang, 3214–3252,May2022.
S. Lm-infinite: Simpleon-the-flylengthgeneralization
Liu,B.,Ash,J.T.,Goel,S.,Krishnamurthy,A.,andZhang,
forlargelanguagemodels,2023.
C. Transformers learn shortcuts to automata. arXiv
Haviv,A.,Ram,O.,Press,O.,Izsak,P.,andLevy,O. Trans- preprintarXiv:2210.10749,2022.
former language models without positional encodings
Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.
stilllearnpositionalinformation. InFindingsoftheAsso-
Scalinglawsofrope-basedextrapolation. arXivpreprint
ciationforComputationalLinguistics: EMNLP2022,pp.
arXiv:2310.05209,2023.
1382–1390,December2022.
Hendrycks,D.,Burns,C.,Basart,S.,Zou,A.,Mazeika,M., Pang, R. Y., Parrish, A., Joshi, N., Nangia, N., Phang, J.,
Song,D.,andSteinhardt,J. Measuringmassivemultitask Chen, A., Padmakumar, V., Ma, J., Thompson, J., He,
languageunderstanding.ProceedingsoftheInternational H., and Bowman, S. QuALITY: Question answering
ConferenceonLearningRepresentations(ICLR),2021. withlonginputtexts,yes! InProceedingsofthe2022
ConferenceoftheNorthAmericanChapteroftheAssoci-
Huang,L.,Cao,S.,Parulian,N.,Ji,H.,andWang,L. Ef- ationforComputationalLinguistics: HumanLanguage
ficientattentionsforlongdocumentsummarization. In Technologies,pp.5336–5358,July2022.
Proceedingsofthe2021ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputationalLin- Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
guistics: HumanLanguageTechnologies,pp.1419–1436, Efficient context window extension of large language
June2021. models,2023.
Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.- Press,O.,Smith,N.,andLewis,M. Trainshort,testlong:
Y., Chen, H., and Hu, X. Llm maybe longlm: Self- Attentionwithlinearbiasesenablesinputlengthextrapo-
extendllmcontextwindowwithouttuning.arXivpreprint lation. InInternationalConferenceonLearningRepre-
arXiv:2401.01325,2024. sentations(ICLR),2022.
10BilevelPositionalEncodingforBetterLengthExtrapolation
Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., Sun,Y.,Dong,L.,Patra,B.,Ma,S.,Huang,S.,Benhaim,
andLillicrap,T.P. Compressivetransformersforlong- A., Chaudhary, V., Song, X., and Wei, F. A length-
rangesequencemodelling. InInternationalConference extrapolatable transformer. In Proceedings of the 61st
onLearningRepresentations(ICLR),2020a. Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for
Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,
ComputationalLinguistics,July2023.
andLillicrap,T.P. Compressivetransformersforlong-
rangesequencemodelling. InInternationalConference Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
onLearningRepresentations,2020b. A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale,S.,Bikel,D.,Blecher,L.,Ferrer,C.C.,Chen,
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
M.,Cucurull,G.,Esiobu,D.,Fernandes,J.,Fu,J.,Fu,W.,
Matena,M.,Zhou,Y.,Li,W.,andLiu,P.J. Exploring
Fuller,B.,Gao,C.,Goswami,V.,Goyal,N.,Hartshorn,
thelimitsoftransferlearningwithaunifiedtext-to-text
A.,Hosseini,S.,Hou,R.,Inan,H.,Kardas,M.,Kerkez,
transformer. TheJournalofMachineLearningResearch,
V.,Khabsa,M.,Kloumann,I.,Korenev,A.,Koura,P.S.,
21(1):5485–5551,2020.
Lachaux,M.-A.,Lavril,T.,Lee,J.,Liskovich,D.,Lu,Y.,
Mao,Y.,Martinet,X.,Mihaylov,T.,Mishra,P.,Molybog,
Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Magar, I.,
I.,Nie,Y.,Poulton,A.,Reizenstein,J.,Rungta,R.,Saladi,
Abend,O.,Karpas,E.,Shashua,A.,Leyton-Brown,K.,
K.,Schelten,A.,Silva,R.,Smith,E.M.,Subramanian,R.,
andShoham,Y. Parallelcontextwindowsforlargelan-
Tan,X.E.,Tang,B.,Taylor,R.,Williams,A.,Kuan,J.X.,
guagemodels.InProceedingsofthe61stAnnualMeeting
Xu,P.,Yan,Z.,Zarov,I.,Zhang,Y.,Fan,A.,Kambadur,
oftheAssociationforComputationalLinguistics(Volume
M.,Narang,S.,Rodriguez,A.,Stojnic,R.,Edunov,S.,
1: LongPapers),pp.6383–6402,2023.
andScialom,T.Llama2:Openfoundationandfine-tuned
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., chatmodels,2023.
Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
Code llama: Open foundation models for code. arXiv
L.,Gomez,A.N.,Kaiser,Ł.,andPolosukhin,I. Atten-
preprintarXiv:2308.12950,2023.
tion is all you need. Advances in Neural Information
Ruoss, A., Dele´tang, G., Genewein, T., Grau-Moya, J., ProcessingSystems(NeurIPS),30,2017.
Csorda´s,R.,Bennani,M.,Legg,S.,andVeness,J. Ran-
Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,brianichter,
domizedpositionalencodingsboostlengthgeneralization
Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of
oftransformers. InAssociationforComputationalLin-
thought prompting elicits reasoning in large language
guistics(ACL),July2023a.
models.InOh,A.H.,Agarwal,A.,Belgrave,D.,andCho,
Ruoss, A., Dele´tang, G., Genewein, T., Grau-Moya, J., K. (eds.), Advances in Neural Information Processing
Csorda´s,R.,Bennani,M.,Legg,S.,andVeness,J. Ran- Systems,2022.
domized positional encodings boost length generaliza-
Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Ef-
tionoftransformers. InProceedingsofthe61stAnnual
ficientstreaminglanguagemodelswithattentionsinks,
MeetingoftheAssociationforComputationalLinguistics
2023.
(Volume2: ShortPapers),pp.1889–1903,July2023b.
Zellers,R.,Holtzman,A.,Bisk,Y.,Farhadi,A.,andChoi,
Sakaguchi,K.,LeBras,R.,Bhagavatula,C.,andChoi,Y.
Y. Hellaswag: Can a machine really finish your sen-
Winogrande: Anadversarialwinogradschemachallenge
tence? InProceedingsofthe57thAnnualMeetingofthe
at scale. In Proceedings of the AAAI Conference on
AssociationforComputationalLinguistics,2019.
ArtificialIntelligence,pp.8732–8740,2020.
Zhong,M.,Yin,D.,Yu,T.,Zaidi,A.,Mutuma,M.,Jha,R.,
Shaham,U.,Segal,E.,Ivgi,M.,Efrat,A.,Yoran,O.,Haviv,
Awadallah,A.H.,Celikyilmaz,A.,Liu,Y.,Qiu,X.,and
A., Gupta, A., Xiong, W., Geva, M., Berant, J., et al.
Radev,D. QMSum: Anewbenchmarkforquery-based
Scrolls: Standardized comparison over long language
multi-domainmeetingsummarization. InProceedingsof
sequences. arXivpreprintarXiv:2201.03533,2022.
the2021ConferenceoftheNorthAmericanChapterof
Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention theAssociationforComputationalLinguistics: Human
with relative position representations. arXiv preprint LanguageTechnologies,pp.5905–5921,June2021.
arXiv:1803.02155,2018.
Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F.,
Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, andLi,S. Pose: Efficientcontextwindowextensionof
Y. Roformer: Enhancedtransformerwithrotaryposition llms via positional skip-wise training. arXiv preprint
embedding,2021. arXiv:2309.10400,2023.
11BilevelPositionalEncodingforBetterLengthExtrapolation
A.Proofs
A.1.TechnicalLemmas
Inthissubsection,wepresentsometechnicallemmasthatshowhowtheMLPandTransformercanperformbasicoperations.
WeshowthattheMLPwithGeLUactivationcanperformscalarmultiplication,selection,andBooleanmatrixmultiplication;
andthattheattentionlayercanperformtheCOPYoperation.
LemmaA.1(FromFengetal.(2023)). Foranyϵ>0andM >0,thereexistatwo-layerMLPf :R2 →RwithGeLU
activationandparameterswithℓ normupperboundedbyO(poly(M,1/ϵ))suchthat|f(a,b)−ab| ≤ ϵholdsforall
∞
a,b∈[−M,M].
LemmaA.2(FromFengetal.(2023)). Letg :Rd1 →Rd2 beatwo-layerMLPwithReLUactivation,andallparameter
valuesareupperboundedbyM. Foranyϵ>0,thereexistsatwo-layerMLPf ofthesamesizewithGeLUactivationand
parametersupperboundedbyO(poly(M,1/ϵ))intheℓ ∞norm,suchthatforallx∈Rd1,wehave∥f(x)−g(x)∥
∞
≤ϵ.
LemmaA.3(FromFengetal.(2023)). Definetheselectionfunctiong :Rd×Rd×R→Rdasfollows:
(cid:26)
x ift≥0,
g(x,y,t)= (1)
y ift<0.
Letf : Rd×Rd×R → Rd beatwo-layerMLPwithGeLUactivation. Then,foranyϵ > 0,α > 0,andM > 0,there
existMLPparameterswithℓ normboundedbyO(poly(M,1/α,1/ϵ)),suchthatforallx∈[−M,M]d,y ∈[−M,M]d,
∞
andt∈[−∞,−α]∪[α,+∞],wehave∥f(x,y,t)−g(x,y,t)∥ ≤ϵ.
∞
LemmaA.4. Letf : Rd1×d2 ×Rd2×d3× → Rd1×d3 beatwo-layerMLPwithGeLUactivation,andgivenaBoolean
matrix B, let e be the vector by flattening the matrix B. Then, for any ϵ > 0, α > 0, and M > 0, there exist MLP
B
parameterswithℓ
∞
normboundedbyO(poly(1/ϵ)),suchthatforallA ∈ {0,1}d1×d2 andB ∈ {0,1}d2×d3,wehave
∥f(e ,e )−e ∥ ≤ϵ.
A B A·B ∞
The proof of Lemmas A.1 to A.3 can be found in the appendix of Feng et al. (2023), and we will give the proof of
LemmaA.4.
ProofofLemmaA.4. GiventwoBooleanmatricesA ∈ {0,1}d1×d2 andB ∈ {0,1}d2×d3. Wecanrepresenttheoutput
A·Bbythefollowingformula:
(cid:95)
(A·B) = (A ∧B )
i,j i,k k,j
k∈[d2]
(cid:16) (cid:88) (cid:17) (cid:16) (cid:88) (cid:17)
=ReLU ReLU(A +B −1) −ReLU ReLU(A +B −1)−1
i,k k,j i,k k,j
k∈[d2] k∈[d2]
Therefore, we can implement the Boolean matrix multiplication by the MLP with ReLU activation, and according to
LemmaA.2,theMLPwithGeLUactivationcanperformtheBooleanmatrixmultiplication.
Thenweintroduceabasicoperationthatcanbeimplementedbytheattentionlayer. Andfollowingit,wegiveaspecial
formofthisoperationusedintheproofofourmaintheorems.
Letnbeanintegerande ,e ,··· ,e beasequenceofvectors,whoseℓ normisboundedbyalargeconstantM. Let
1 2 n ∞
K,Q∈Rd′×dbeanymatrices,andlet0<ρ<M beanyrealnumbers. Denoteq =Qx andk =Kx . Theoutputof
i i j j
theCOPYoperationisasequenceofvectorsu ,··· ,u withu =e ,wherepos(i)=argmax q ·k .Moreover,
1 n i pos(i) j∈[i] i j
weassumethatthematricesQ,Kandscalarsδsatisfythatforallconsideredsequencese ,e ,··· ,e ,wehaveforanyi
1 2 n
andj ∈[n]\{pos(i)},eitherq ·k −q ·k ≥δ. Thisassumptionguaranteesthattherearesufficientgapsbetweenthe
i pos(i) i j
attendedpositionandotherpositions. ThenweprovethattheattentionlayercanimplementtheCOPYoperation.
LemmaA.5(FromFengetal.(2023)). Foranyϵ>0,thereexistsanattentionlayerwithembeddingsizeO(d)andone
causalattentionheadthatcanapproximatetheCOPYoperationdefinedabove. Formally,foranyconsideredsequenceof
vectorse ,e ,...,e ,denotethecorrespondingattentionoutputaso ,o ,...,o . Then,wehave∥o −u ∥ ≤ϵforall
1 2 n 1 2 n i i ∞
i∈[n]. Moreover,theℓ normofattentionparametersisboundedbyO(poly(M,1/δ,log(n),log(1/ϵ))).
∞
12BilevelPositionalEncodingforBetterLengthExtrapolation
TheproofofthislemmacanbefoundinFengetal.(2023). BasedontheLemmasA.1andA.5,giventhetokenindexias
theabsolutePE,theattentionlayercancopytheembeddingatthespecificpositionbythefollowingconstruction. Given
aspecificpositioni,wecanconstructthequeryq =(1,i2,i)andthekeyofthej-thtokenk =(−j2,−1,2j)byMLP,
j
accordingtoLemmaA.1. Thenq·k =−(i−j)2getsthemaximumwheni=j,andwecanconcentratetheattentionon
j
thei-thtokenandcopyitsembeddingbyLemmaA.5.
A.2.ProofsofMainTheorems
Inthissubsection,wewillproveTheorems3.1and3.2,foreaseofreadingwerestatethetheoremshereandthengiveproof.
A.2.1.PROOFOFTHEOREM3.1
TheoremA.6(LowerBoundforTransformerwithAPEtoRepresentNFA). Foranysizeofstateset,thereexistsanNFA
N =(Q,Σ,δ,q ,F)suchthataTransformerwithAPEneedsatleastO(|Q|2)embeddingsizetorepresenttheNFA.
0
Weprovethistheoreminthelog-precisionsetting,whichisarealisticandpracticalsettingfortransformers. Inthissetting,
eachvalueinthetransformerisencodedbyO(logN)bits,whereN istheinputlength. Thiscorrespondstothepractical
scenariowherethetransformerhandlesinputsequencesofuptoafewthousandtokens,using16or32bitsfloating-point
numbers. Theoretically,thelog-precisionnumbercanapproximateanyrealnumberofmagnitudeO(poly(N))withan
errorofO(poly(1/N)). EachneuroninthetransformercanstoreonlyO(log(n))-bitsinformationandthuscannotretain
thefullinformationoftheentireinputsequence,whichisreasonableandalignedwithpracticalscenarios.
ProofofTheoremA.6. GiventhestatesetQ,wecanconstructaNFAN =(Q,Σ,δ,q ,F)asfollows:
0
• ThestatesetQisgiven,andassumingQ={q ,q ,··· ,q }.
1 2 n
• ΣisthesetofallmapsfromthestatesetQtoitspowerset,i.e. Σ={f :Q→P(Q)}.
• Givenastateq ∈Qandasymbolf ∈Σ,δ(q,f)=f(q).
• q =q .
0 1
• F ={q }.
n
ToprovethattheembeddingdimensionofthetransformertorepresentN isatleastO(n2),weusethefollowingargument.
The size of the alphabet is 2n2, so we need at least O(n2) bits to embed each input symbol. Suppose the embedding
dimensioniso(n2). Thenwecanfindtwoinputsequencesofconstantsizethathavethesameembeddingsinthetransformer
butdifferentoutcomesfortheNFA.Sincethelengthoftheinputsequenceisconstant,thetransformeruseso(n2)bitsto
representtheembeddingforeachtoken. Therefore,thereexisttwoinputtokensf andf′ thathavethesameembedding.
Letq ∈ f(q )andq ∈/ f′(q ). ThenwecanconstructtwoinputsequencesS = [f ,f,f ]andS′ = [f ,f′,f ],where
i j i j 1 2 1 2
f (q )={q },f (q )={q },andf (q)=∅forq ̸=q . Theembeddingsofthesetwosequencesinthetransformerarethe
1 0 j 2 i n 2 i
same,butoneisacceptedbyN whiletheotherisnot. Hence,thetransformerwithembeddingsizeo(n2)cannotrepresent
theNFAN. AtransformerwithAPEneedsatleastO(n2)embeddingsizetorepresenttheNFA.
A.2.2.PROOFOFTHEOREM3.2
TheoremA.7(UpperBoundforTransformerwithBiPEtoRepresentBi-NFA). ForanyBi-NFAN =(Q,Σ,δ,q ,F),Q=
0
{Q ,Q ,··· ,Q }thereexistsaTransformerwithBiPEandO(k2+(cid:80) |Q |2)embeddingsizecanrepresenttheBi-NFA.
1 2 k i∈[k] i
Inthisproof,weusethelog-precisiontransformerwiththeGeLUactivationfunctionandO(logN)layers,whereN is
theinputlength. Thischoiceoflayersiscrucialforthetransformertorepresentautomata,asaconstant-layertransformer
wouldrequireasuper-polynomialembeddingsize(intheinputlength)todoso(Liuetal.,2022). Withoutlossofgenerality,
wefocusontheBi-PEmodelwithT5-relativePEastheinter-segmentpositionalencodingandAPEastheintra-segment
positionalencoding. Moreover,ourproofcanbeeasilyextendedtoothervariantsofBi-PE,suchasthosewithRoPEor
AliBiastheinter-segmentpositionalencoding.
13BilevelPositionalEncodingforBetterLengthExtrapolation
T5-relativePE. WeusetheT5-relativePEmethodtocomputetheattentionlogitsbeforeapplyingthesoftmaxfunction.
Theattentionlogitsaregivenbythefollowingequation:
A (X)=XW (XW )⊤+B
RPE Q K
whereB = r ,K isahyper-parameter,and{r }K arelearnablescalarsthatrepresenttherelativeposition
i,j min(i−j,K) i i=0
embeddings.
ProofSketch. Inthisproof,weconstructatransformercontainingtwomodulestorepresenttheBi-NFA.Thefirstmodule
computesthestatetransitionsinthesegment,andthesecondmodulecomputesthestatetransitionsbetweenthesegments.
EachmodulecontainsO(logN)attentionlayersandimplementsaclassicdivide-and-conqueralgorithm.
ProofofTheoremA.7. GivenaBi-NFAN = (Q,Σ,δ,q ,F),wewillfirstintroducesomenotationsandthebasicidea
0
behindourconstruction.
The State Transition in Each Segment. Given Q = {Q ,··· ,Q }, we use q for the j-th state in Q and define
1 k i,j i
Q = (cid:83) Q , Q∗ = {q∗}. Without loss of generality, we assume that q is the start state q∗ in Q and q = q∗.
i∈[k] i i i,1 i i 0 1
Given an input symbol w, we view it as a map f from Q to P(Q) such that f (q) = δ(w,q), and an input string
w w
S = [w ,w ,··· ,w ] as the composition of f = f ·f ···f . Denoting the segment separator as w∗ ∈ Σ, for
any inpu1 t sy2 mbol wn ̸= w∗, we use a tuple of bs ooleaw n1 matw ri2 ces Mw (n w) = (cid:0) M (w),M (w),··· ,M (w)(cid:1) to represent
1 2 n
it, suchthatM (w) = (cid:0) M (w)(cid:1) = I[q ∈ δ(w,q )]. Wedefinethemultiplicationoftwotuplesofmatricesas
i,j,k i j,k i,j i,k
M(w)·M(w′)=(cid:0) M (w)·M (w′),··· ,M (w)·M (w′)(cid:1) ,whichisthecompositionoff ·f . Inthetransformer,we
1 1 n n w w′
flattenM(w)toavectorm(w). WeusetheMLPtoimplementthemultiplicationoftwotuplesofmatricesaccordingto
LemmaA.4. Moreover,astringwithoutasegmentseparatorcanbeviewedasthemultiplicationofthesetuplesofmatrices.
(cid:81)
GivenastringwithoutsegmentseparatorS =[w ,w ,··· ,w ],wecomputeM(S)= M(w )anddenotethevector
1 2 n i∈[n] i
flattenedfromM(S)asm(S). Giventhestartstateq∗,wegetthestatesetoftheBi-NFA{q |M (s)=1}aftertaking
i i,j i,1,j
inthestringS. Moreover,weusethenotationw :w andM(w :w )torepresentthesubstringfromw tow
i,j i,k i,j i,k i,j i,k
(cid:81)
anditsstatetransitionmatrixtuple. Wecanusetheclassicdivide-and-conqueralgorithmtocomputeM(s)= M(w ),
i∈[n] i
andthefirstmoduleofthetransformerweconstructimplementsthealgorithm.
The State Transition between Segments. A segment S = [w ,w ,··· ,w ,w∗] can be viewed as a map from
1 2 n
{q∗,q∗,··· ,q∗} to P({q∗,q∗,··· ,q∗}). For the segment separator w∗, we can also view it as a tuple of matrices
M(1 w∗2 ) = (cid:0) Mk (w∗),M1 (w∗2 ),··· ,Mk (w∗)(cid:1) , where M (w∗) = I[q∗ ∈ δ(w∗,q )]. Therefore, we can compute
1 2 k i,j,k k i,j
thestatetransitionmatrixA(S)ofthesegmentS suchthatA (S) = M (S). WehaveA (S) = I[q∗ ∈ δ(S,q∗)].
i,j i,1,j i,j j i
GivenasequenceofsegmentsS=S ⊕S ⊕···⊕S ,wecancomputeA(S)=Π A(S ). Thengiventhestartstate
1 2 n i∈[n] i
q ,wecangetthefinalstatesetQ(S)aftertheBi-NFAtakingintheinputSsuchthatQ(S)={q∗|A (S)=1}. Then
0 i 1,i
theBi-NFAacceptsSifandonlyifQ(S)∩F ̸=∅,andthisconditionjudgmentcanbeformulatedastheproductoftwo
Booleanvectorsandtherefore,canbeimplementedbyMLP.Forconvenienceandclarityinpresentingourproof,inthe
representationofthetransformer,weflattenthematricesA(S)andA(S)tovectorsandwedenotethemasa(S)anda(S),
respectively. WeusethenotationS : S andA(S : S )torepresentthesubstringfromS toS anditsstatetransition
i j i j i j
matrix.
TokenEmbeddings. GivenainputsequenceS=S ⊕S ⊕···⊕S andS =[w ,w ,···w ,w∗],assuming
1 2 n i i,1 i,2 i,li−1 i
thelengthofthesequenceisupperboumdedbyN. AsastandardwayinNLP,weadda<SOS>atthebeginningofthe
string. Foreachtokenw ̸=w∗,andw ̸=<SOS>,aftercombiningtheabsolutepositionalembedding,theembedding
i,j i,j
atthebeginningisx0 =(cid:0) m(w ),1,0,0,i(cid:1) ,andtheembeddingsforw∗and<SOS>is(0,0,1,0,l )and(0,0,0,1,1),
i,j i,j i
respectively. TheembeddingsizeatbeginingisO((cid:80) |Q |2).
i∈[k] i
ModuleI. Thefirstmodulecontains(⌈log(N)⌉+1)layers,andinthismodule,thetokenonlyattendstothetokensinthe
samesegment. Therefore,theT5-relativePEfortheselayersisr =−∞fori̸=0andr =0. Atthelayerl,theinput
i 0
embeddingsoftokenw isx1,l =(cid:0) m(w :w ),1,0,0,i(cid:1) ,wherew :w =[w ,w ,··· ,w ].
i,j i,j i,max(1,j−2l) i,j i,j1 i,j2 i,j1 i,j1+1 i,j2
Thefirstmodulecompletesthefollowingtasks:
• Copytheembeddingoftokenw ,notethatwhenj−2l <1,theembeddingcopiedismeaningless.
i,j−2l
14BilevelPositionalEncodingforBetterLengthExtrapolation
• Calculatethemultiplicationoftwotuplesoftransitionmatricesdefinedinthepreviousparagraph.
• Selecttheembedding,whenj −2l < 1,theoutputembeddingisthesameastheinput,andwhenj −2l ≥ 1,the
outputistheoutcomeofthemultiplication.
Therefore, the output embeddings of the token w at the layer l is x1,l+1 = (cid:0) m(w : w ),1,0,0,i(cid:1) .
i,j i,j i,max(1,j−2(l+1)) i,j
According to Lemmas A.3 to A.5 we can implement the COPY operation by the attention layer, and implement the
multiplication of two tuples of matrices, and the selection operation by the MLP. After ⌈log(N)⌉ layers, the output
embeddingofw isx1,⌈log(N)⌉ =(cid:0) m(w :w ),1,0,0,i(cid:1) . Atthefinallayer,thetokenw∗copiestheembeddingofthe
i,j i,j i,1 i,j
previoustokenw ,andusestheMLPtocomputethetransitionmatrixofthissegment. Thefinaloutputofthisblock
forthetokenw∗i i, sli(cid:0)− a1
(S ),0,1,0,l
(cid:1)
,forthetoken<SOS>is(0,0,0,1,1),andforthetokenw is(0,1,0,0,j). The
i i i i,j
embeddingsizeofthefirstmoduleisO((cid:80) |Q |2).
i∈[k] i
ModuleII. Thesecondmodulecontains⌈log(N)⌉layers,andweonlyneedtoconcentrateourattentionontheembedding
of the last token of each segment. Similar to the previous block, at the layer l, the input embeddings of token w∗ is
(cid:0) (cid:1) i
a(S : S ),0,1,0,l ,whereS : S = S ⊕S ⊕···⊕S . Thesecondmodulecompletesthefollowing
max(1,i−2l) i i i j i i+1 j
tasks:
• CopytheembeddingoflasttokenofsegmentS ,notethatwheni−2l <1,theembeddingcopiedismeaningless.
i−2l
• Calculatethemultiplicationoftwotransitionmatricesdefinedinthepreviousparagraph.
• Selecttheembedding: wheni−2l <1,theoutputembeddingisthesameastheinput,andwheni−2l ≥1,theoutput
istheoutcomeofthemultiplication.
Therefore,theoutputembeddingsofthetokenw∗is(cid:0)
a(S :S ),0,1,0,l
(cid:1)
. wedesigntherelativepositional
i max(1,i−2(l+1)) i i
embeddingasr =−∞fori̸=2l andr =0. Notethat,wheni−2l <1,thetokenwillgiveuniformattentiontothelast
i 2l
tokenofeachsegmentprevioustoit,therefore,wecanusetheembeddingof<SOS>todetectthiscase. Whenthevalueindi-
cates<SOS>isgreaterthan 1,wehavei−2l <1andmaintaintheembeddings. AccordingtoLemmasA.3toA.5wecan
N
implementthecopyoperationbytheattentionlayer,andimplementthemultiplicationoftwomatricesandtheselectionopera-
tionbytheMLP.Thenwecangetthefinalstatesetfromtheembeddingofthelasttokenoftheinputsequenceandwecanuse
anMLPtocomputetheoutcometodetermineacceptingtheinputornot. TheembeddingsizeofthesecondmoduleisO(k2).
Therefore,weconstructatransformerwithBiPE,andO(k2+(cid:80) |Q |2)embeddingsizecanrepresenttheBi-NFA.
i∈[k] i
15BilevelPositionalEncodingforBetterLengthExtrapolation
B.ExperimentalDetails
B.1.CapacityExperiments
Table3. Modelconfigurationsforcapacityexperiments. Table4. Trainingrecipesforcapacityexperiments.
Layers 3 Batchsize 512
Attentionheads 4 Epochs 100
Headdimensions {12,16,64} Dropout 0.1
Hiddendimensions {48,64,256} Weightdecay 0.01
FFNdimensions {192,256,1024} Optimizer AdamW
Modelparameters {87K,153K,2.4M} Learningrate 1e−4
Inthisexperiment,weusetheArithmetictask(Fengetal.,2023)toempiricallyverifytheparameterefficiencybrought
by our BiPE method. Given an arithmetical expression consisting of numbers, basic operations (+,−,×,÷,=) and
brackets,e.g.,(1+2)×(3+5) =,thistaskrequireslanguagemodelstocalculateandgeneratethecorrectresult,e.g.,
24. FollowingFengetal.(2023),wetrainallmodelsusingChain-of-Thoughtdemonstrations,e.g.,fortheinputsequence
(7+8)÷(5+2×7−2×8),theoutputsequenceis15÷(5+2×7−2×8)=15÷(5+14−2×8)=15÷(19−2×8)=
15÷(19−16) = 15÷3 = 5. Theevaluationmetricistheaccuracyofthefinalanswer. Wereferinterestedreadersto
AppendixHinFengetal.(2023)foradditionaldetails.
Modelconfigurations. Inthisexperiment, wetraindecoder-onlyTransformer-basedlanguagemodelswithdifferent
positionalencodingtechniqueswhilekeepingalltheotherconfigurationsthesame. ForSinusoidalPE,wefollowVaswani
etal.(2017)tosetthehyperparametersinsineandcosinefunctions. ForRoPEandXPOS,wefollowSuetal.(2021);Sun
etal.(2023)tosetthehyperparametersintherotarymatrixrespectively. ForALiBi,wefollowPressetal.(2022)tosetthe
slopevaluesineachattentionhead. FortheintrasegmentencodingofourBiPE,weusethelearnableabsolutepositional
encoding. FortheintersegmentencodingofourBiPE-RoPE,thehyperparametersarekeptthesameas(Suetal.,2021). For
theintersegmentencodingofourBiPE-ALiBi,theslopevaluesaresetto96timesoftheoriginalALiBi’ssetting. Other
modelconfigurationsareprovidedinTable3.
Trainingrecipes. Thenexttokenpredictionobjective(Brownetal.,2020)isadoptedforlanguagemodeltraining. The
numberofoperatorsinthearithmeticdatasetissetto6,whichyieldsatotalsequencelengthof223forChain-of-Thought
demonstrations. ThetrainingrecipesareprovidedinTable4. Allmodelsaretrainedon4NVIDIAA100GPUs.
B.2.LengthExtrapolationExperiments
Table5.Modelconfigurationsforlengthextrapolationexperiments. Table6.Trainingrecipesforlengthextrapolationexperiments.
Layers 12 Batchsize 256
Attentionheads 12 Totaltrainingsteps 500k
Headdimensions 64 Dropout 0.0
Hiddendimensions 768 Weightdecay 0.01
FFNdimensions 3072 Optimizer AdamW
Modelparameters 155M Learningrate 1e−4
Modelconfigurations. Inthisexperiment,wetraindecoder-onlyTransformerlanguagemodelswithdifferentpositional
encodingtechniqueswhilekeepingalltheotherconfigurationsthesame. ForSinusoidalPE,wefollowVaswanietal.(2017)
tosetthehyperparametersinsineandcosinefunctions. ForRoPEandXPOS,wefollowSuetal.(2021);Sunetal.(2023)to
setthehyperparametersintherotarymatrixrespectively.ForRandomizedRoPE,wesettheextendedpositions4timesofthe
traininglength. Wealsoconductedexperimentsontheextendedpositions16timesofthetraininglengthinFigure7,which
showsperformancedegradation. ForALiBi,wefollowPressetal.(2022)tosettheslopevaluesineachattentionhead. For
theintrasegmentencodingofourBiPE,weusethelearnableabsolutepositionalencoding. Fortheintersegmentencoding
ofourBiPE-RoPE,thehyperparametersarekeptthesameas(Suetal.,2021). FortheintersegmentencodingofourBiPE-
ALiBi,theslopevaluesaresetto96timesoftheoriginalALiBi’ssetting.OthermodelconfigurationsareprovidedinTable5.
16BilevelPositionalEncodingforBetterLengthExtrapolation
Trainingrecipes. Thenexttokenpredictionobjective(Brownetal.,2020)isadoptedforlanguagemodeltraining. All
modelsaretrainedonthePiledataset1 (Gaoetal.,2020)withatotalsequencelengthof1024. Thetrainingrecipesare
showninTable6. Allmodelsaretrainedon8NVIDIAA100GPUs.
PG19 Language Modeling ArXiv Language Modeling Github Language Modeling
450 200 800
Randomized RoPE (4 times) Randomized RoPE (4 times) Randomized RoPE (4 times)
400 Randomized RoPE (16 times) 175 Randomized RoPE (16 times) 700 Randomized RoPE (16 times)
350 150 600
300 125 500
250
100 400
200
75 300
150
50 200
100
50 25 100
0
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
Number of Tokens (×103) Number of Tokens (×103) Number of Tokens (×103)
Figure7.LanguagemodelingperplexitywithvaryingevaluationsequencelengthsforRandomizedRoPEtrainedonthePiledatasetwith
differenttimesofthetraininglengthforextendedpositions.
B.3.IntegratingBiPEwithFine-tuningStrategies
Table7. Fine-tuningrecipesfortheYaRNstrategy. Table8. Fine-tuningrecipesforlongcontextbenchmark.
Batchsize 64 Batchsize 64
TotaltrainingSteps 500 Totaltrainingsteps 5000
Dropout 0.0 Dropout 0.0
Weightdecay 0.01 Weightdecay 0.01
Optimizer AdamW Optimizer AdamW
Learningrate 2e−5 Learningrate 1e−5
Modelconfigurations. Inthisexperiment,weusetheYaRNstrategytofine-tunepre-trainedlanguagemodelswithRoPE
andourBiPE-RoPE.AllthemodelconfigurationsarethesameasthoseinTable5.
Fine-tuningrecipes. WesetthescalefactorinYaRNto16andfine-tunemodelsusingthenexttokenpredictiontaskfor
500stepsonthePile(Gaoetal.,2020)datasetwithasequencelengthof4096. ThefinetuningrecipesareshowninTable7.
Allmodelsarefine-tunedon8NVIDIAA100GPUs.
B.4.LongContextBenchmark
Modelconfigurations. Inthisexperiment,wefine-tunepretrainedlanguagemodelswithdifferentpositionalencoding
methods on SCROLLS (Shaham et al., 2022). It is a long context benchmark that consists of seven distinct datasets
coveringdifferenttasks,e.g,Question-Answering(Qasper(Dasigietal.,2021),NarrativeQA(Kocˇisky´ etal.,2018),and
QuALITY(Pangetal.,2022)),NaturalLanguageInference(ContractNLI(Koreeda&Manning,2021))andSummarization
(QMSum(Zhongetal.,2021),SummScreenFD(Chenetal.,2022),andGovReport(Huangetal.,2021)). Allthemodel
configurationsarethesameasthoseinTable5.
Fine-tuningrecipes. Wefine-tunemodelsusingthenexttokenpredictionobjectiveoneachtaskwithasequencelength
of8192. ThefinetuningrecipesareprovidedinTable8. Themodelcheckpointthatachievesthebestperformanceonthe
validationsetisselectedforthefinalevaluation. ThetestresultsareobtainedfromtheofficialSCROLLSwebsite2. All
modelsarefine-tunedon8NVIDIAA100GPUs.
1We use a copy of the Pile dataset with all copyrighted contents removed: https://huggingface.co/datasets/
monology/pile-uncopyrighted.
2https://www.scrolls-benchmark.com/submission
17
ytixelpreP
tseT
ytixelpreP
tseT
ytixelpreP
tseT