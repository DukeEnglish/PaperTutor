IS K-FOLD CROSS VALIDATION THE BEST MODEL SELECTION
METHOD FOR MACHINE LEARNING?
APREPRINT
J.M.Gorriz,F.Segovia,JRamirez∗ A.Ortiz
DepartmentofSignalTheoryandCommunications DepartmentofCommunicationsEngineering
UniversityofGranada UniversityofMalaga
Granada,Spain Malaga,Spain
aortiz@ic.uma.es
JohnSuckling InternationalInitiatives
DepartmentofPsychiatry fortheAlzheimer’sDiseaseNeuroimagingInitiative(ADNI)
UniversityofCambridge
Cambridge,UK
js369@cam.ac.uk
January30,2024
ABSTRACT
As a technique that can compactly represent complex patterns, machine learning has significant
potential for predictive inference. K-fold cross-validation (CV) is the most common approach to
ascertainingthelikelihoodthatamachinelearningoutcomeisgeneratedbychanceandfrequently
outperforms conventional hypothesis testing. This improvement uses measures directly obtained
frommachinelearningclassifications, suchasaccuracy, thatdonothaveaparametricdescription.
Toapproachafrequentistanalysiswithinmachinelearningpipelines, apermutationtestorsimple
statistics from data partitions (i.e. folds) can be added to estimate confidence intervals. Unfortu-
nately,neitherparametricnornon-parametrictestssolvetheinherentproblemsaroundpartitioning
small sample-size datasets and learning from heterogeneous data sources. The fact that machine
learningstronglydependsonthelearningparametersandthedistributionofdataacrossfoldsreca-
pitulatesfamiliardifficultiesaroundexcessfalsepositivesandreplication. Theoriginsofthisprob-
lem are demonstrated by simulating common experimental circumstances, including small sample
sizes,lownumbersofpredictors,andheterogeneousdatasources. Anovelstatisticaltestbasedon
K-foldCVandtheUpperBoundoftheactualerror(K-foldCUBV)iscomposed,whereuncertain
predictionsofmachinelearningwithCVareboundedbytheworstcasethroughtheevaluationof
concentrationinequalities. ProbablyApproximatelyCorrect-Bayesianupperboundsforlinearclas-
sifiersincombinationwithK-foldCVisusedtoestimatetheempiricalerror. Theperformancewith
neuroimaging datasets suggests this is a robust criterion for detecting effects, validating accuracy
valuesobtainedfrommachinelearningwhilstavoidingexcessfalsepositives.
Keywords K-foldcross-validation·linearsupportvectormachines·statisticallearningtheory·permutationtests·
MagneticResonanceImaging·UpperBounding.
1 Introduction
Machinelearning(ML)systems[1]aredesignedtosolvetaskswithoutknowledgeoftheunderlyingdistributionsof
thedata.Inotherwords,MLsystemsareagnosticmodels[2]thathaveclearpotentialforprocessingcompleximaging
∗gorriz@ugr.es,jg528@cam.ac.uk
4202
naJ
92
]LM.tats[
1v70461.1042:viXraIsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
datasets. The unknown probability density function (pdf) in high-dimensional spaces from which data samples are
drawn is modeled by fixing the ML system complexity, typically the number of parameters of the neural network
architectures [3], and optimising the parameters through learning on a training set. At the decision stage of ML
systems,highdimensionalfeaturesarecommonlyprojectedontolow-dimensionalsub-spaces[4]easingthelearning
task and permitting inference. System performance is frequently assessed by cross-validation (CV) techniques that
divide the overall data into partitions (i.e. folds) and then systematically train and test the ML system on each fold
averagingtheresultingoutcomemeasures. CVwithlimitedsamplesizescanleadtolargestandarderrors[5,6]. No
closed solution to this common problem in data analysis has been found in the literature, although there are several
resultswithinstatisticallearningtheory(SLT)thatcouldrelievetheissue[7].
Multivariate,data-drivenapproachesandMLsystemsofferanattractivealternativewithhighdetectionratesavailable
in several research fields, e.g. neuroscience [8, 9, 10, 11]. Between-group analyses in neuroimaging are typically
conductedwiththeGeneralLinearModel(GLM)andclassicalstatistics[12]. Althoughstraightforwardtointerpret
[13], the assumptions of the univariate GLM are frequently violated [14] making the test more conservative and
encouraginglessthanoptimalanalyticpractices. InflatedtypeIerrorrates,p-hacking,badexperimentaldesignshave
all become problematic affecting reproducibility and replicability across science and engineering [15], including NI
[16,17].
MigrationtoML[18,19]doesnotprecludefalsepositivesandmisleadingresults. Forexample, outputsfromsome
MLsystemswithmultivariatefeaturesarestronglyaffectedbyhigh-noiselevelsinthedata[20,21]. Here,although
questionableresearchpracticessuchasp-hacking2 arenotproblematicforML,thereisastrongdependenceofper-
formanceontheconstructionofthelearningandvalidationdatasets[6].
Therehavebeenseveralcommentariesintheliterature[5,16,20,22]aboutthehighvariabilityofperformancefound
acrossCVfoldsinseveralneuroimagingdataanalyseswithclearimplicationsforpredictiveinference. However,no
quantitativesolutionhasbeengiventothisissuebeyondincreasingsamplesizeandwarningoferrorsacrossCVfolds.
Inthecontextofsmallsample-sizesandheterogeneousdata,CVstronglyunderestimatestheactualerror,whichcan
beinterpretedastheviolationoftheassumptionofergodicity[23];thatis,theaveragebehavior(performance)ofany
systemcanbedescribedfromacollectionofrandomsamples(asetofrandomperformancesineachCVloop).
What would happen if our process was not ergodic? We can make use of the concept of a stable inducer [24] to
describetheconsequentialeffect. Aninduceristhealgorithmthatcreatesaclassfierfromatrainingset. Itisstable
foragivendatasetifitcreatesclassifiersthatmakethesamepredictionswhengivenperturbeddatasets. Ifthesample
is strongly perturbed, e.g. due to small-sized and heterogeneous samples drawn from multivariate distributions, the
processoflearningfromspecifictrainingfoldscannotbeefficientlyextrapolated,instatisticalterms,totestfolds.
Permutationanalyses[25]canaugmentMLsystemstotestforstatisticalsignificancebythecomputationofp-values
fromthedistributionofastatisticunderthenullhypothesis.Assumingthedataisi.i.d.,thisnulldistributionisobtained
by permuting the class labels a large number of times and training and testing the ML system each time. However,
theestimationofthestatisticscouldbebiasedwithheterogeneousdatasetsduetotheuseofonlyasingleinstanceof
theK-foldsintheCV.Ahighdegreeofconfidenceintheresultscannotbeobtainediftheessentialelementofthese
analyses,thepredictiveinferencebasedonanaveragedaccuracyfromspecificfolds,isflawed.
InthispaperweproposetheuseofK-foldCVincombinationwithastatisticaltestbasedontheanalysisoftheworst
case(upperboundingtheactualerror)toestimatetheprobabilitiesofuncertaintyfromasampledrawnfromapop-
ulation. Thismethod,namedK-foldCrossUpperBoundingValidation(CUBV)allowscontrolovertheperformance
ofthesystem. GiventheempiricalK-foldCV-errorwecalculatetheupper-boundoftheactualerrortoassessthede-
greeofsignificanceoftheclassifier’sperformance. Upper-boundingtheactualerrorisastatisticallearningtechnique
basedonconcentrationinequalities(CI)thatconstructsefficientalgorithmsforprediction,estimation,clustering,and
featurelearning. CIdealwithdeviationsoffunctionsofindependentrandomvariablesfromtheirexpectations;here,
thedeviationofempiricalerrorsineachtrainingfoldfromtherealerrorachievedbytheclassifiergiventhecomplete
dataset. This solution for evaluating the error variability is similar to the multiple comparisons p-value corrections,
suchasBonferroniorRandomFieldTheory,withinhypothesis-drivenmethodsforstatisticalinference[26,27].
2 K-foldCrossValidation: theoryandpractice
StatisticalinferenceusingMLsystemswithneuroimagingdataisusuallyperformedbymeansofpredictiveinference.
DatasetsaresplitintoCVfolds(trainingandtestsets)andtheabilityofstatisticalclassifierstoextrapolatefromthe
samples in the training sets is assessed on the test set. Performance is measured in terms of the classifier error that
isthenusedasastatistictotestforbetween-groupdifferences;e.g. byestimatingpvaluesfrompermutationtesting.
2thedeliberateselectionofthelevelofsignificanceintheseekofthedesiredeffect
2IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Thedegreeofreliabilityofthisapproachwilldependonthesamplesizeandthedatacomplexitywithbothaffecting
thequalityofthelearningprocedure.
2.1 PredictiveInferenceandMachineLearning
Giventheinputandoutputspacesinabinaryclassificationproblem,X andY respectively,weobserveNi.i.d. pairs
(x ,y ) ∈ (Rn,{0,1}) sampled according to an unknown P. We try to predict y from x by means of a function
i i
f ∈ F : X → Y thatischosenminimizingtheexpectedlossR(f) = E(L(y,f(x)) = P(f(x) ̸= y). Bydefinition,
theK-foldCVestimatederrorisgivenby:
1 (cid:88)
R (f)= 1−δ(f(D\D ),y ) (1)
N N i i
(xi,yi)∈D
whereδ(.)istheKronecker´sdelta. Typically, K = 10andrepresentsatrade-offbetweenvarianceandbiasofthe
estimationoftheerror. AnothercommonchoiceisK = N,knownastheleave-one-out(LOO)CV.D isthewhole
dataset,andD ,fori=1,...,K,arethemutuallyexclusivefolds.
i
As previously pointed out [24], a better estimation of this error would include all the possible combinations
(cid:0)N(cid:1)
,
N
K
althoughitisusuallytooexpensive computationally. Indeed, thevarianceoftheerrorstronglydepends onK ifthe
classifier f is unstable under perturbations. This happens when deleting instances of the training folds provokes a
substantialchangeintheoutcome,mainlyduetodataheterogeneity.
AmajordrawbackinusingapredictiveinferenceapproachisthattheCVissub-optimalwhenperformingahypothesis
testforagivensamplesizeandthreshold[28]3.
2.2 StatisticalInferencewithpermutationanalyses
Ap-valueestimationbymeansofapermutationtest[29]canbeconductedontheclassificationresultsfromK-fold
CVofaMLsystem[30]. Inthisway,theerrorofthetrainedMLsystemisthestatisticusedtotestforbetween-group
differences. In a pattern classification problem we evaluate how likely is the performance of the system using the
labeled data and empirical distribution compared to the distribution from a number, M, of label permutations that
simulate“noeffect’,i.e. samplesarei.i.d(thenulldistribution)[31]. Inotherwords,weassesstheprobabilityofthe
observedvalueoftheerrorunderthenullhypothesisbypermutationπ,asthefollowing:
#(R ≤R)
p = π (2)
value M +1
where#(.)isthenumberoftimestheerrorinthepermutationislessorequaltotheerrorobtainedwiththeobserved
effect. If this value is less than our level of significance, e.g. α = 0.05, then the distribution of data significantly
differsbetweengroups[25].
To sum up, the state-of-the-art in this field often employs only a single-instance estimation from a particular set of
CV folds and, at most, a subsequent permutation analysis to simulate the null distribution and test for statistical
significance. However, the whole procedure, i.e. the assessment of the true effect and the null distribution from
incompletedatasets,issubstantiallybiasedifthesamplesaresmallandheterogeneous.
3 PerformanceofK-foldCrossValidationinCommonExperimentalDesigns
ThefollowingexamplesillustratethekeyproblemsfoundintheliteraturethataffectthereproducibilityofMLresults
inneuroimagingandotherrelatedareas. Insummary,contemporaryresultsarebasedonasingleinstanceofthepdf
fromthetrainingfoldsaswellastheselectionofaparticularCVfoldasthetestsetwithwhichpredictiveinferenceis
assessed.Thus,evenwiththesamedatasets,theperformanceofonestudyinformedbyanothercandiffersubstantially
withpotentiallycontradictoryinterpretationsifthedatafoldsarenotidentical.
3.1 Thenullexperiment–controloftypeIerrors
AssumethattwolaboratoriesAandBacquireexperimentaldatafromtwopopulationsandseekaneffectinagroup
comparisonthatisactuallytriviallysmall,i.e.dependentonthesamplerealization(figure1).Bothlaboratoriesbelieve
that an effect exists although the patterns belonging to both classes are drawn from the same, unknown, Gaussian
3Proof:theNeyman-Pearsonlemma.
3IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure1: NulldistributionofaccuracyvaluesusingK-foldCV(ingreenfont)obtainedfromsamplingthepdf(onthe
left)andpermutingthefolddistribution(ontheright). InblueweshowtheproposedK-foldCUBVmethodtocontrol
FPinthisnullexperiment. Notethatinthisexampledimensionn=2andCohen’sd=0.
distribution: N(0,1). Giventheirsamplerealizations,theyapplytheirMLalgorithmsforpatternclassificationusing
a properly established CV-scheme and obtain accuracy values. The results obtained by the two laboratories are, of
course, not identical (a different sample realization in each lab) and normally distributed around 0.5. This expected
theoreticalresultisachievedifwefollowthesameschemewithanincreasingnumberoflaboratoriesandaveragethe
setofaccuracyvalues.
Howandwhendoesaspecificlaboratoryrejectthenullhypothesisthatthereisaneffect? Inordertoteststatistical
significancealaboratorycomparesitsobservedaccuracywiththesetofvaluesobtainedbypermutingconditionson
itsparticularsamplerealizationwiththeaimofmodellingthenulldistribution. Isthissufficientlyaccurate? Whatif
thesamplerealizationisrandomlyprovidingaccuraciesfromanoverfittedsolution? Eventhoughthenulldistribution
is properly modelled, the effect is being tested by assuming the alternative hypothesis in a classification task, thus
overfittingcouldaffecttheoutcomemeasure. ThisisthemainproblemofusingMLalgorithmsingroupcomparisons;
one can never be assured of the statistical significance of the averaged accuracy values obtained from folds using
empiricalmeasuresonly.
What if the null distribution is not properly modelled by a particular realization, e.g. by an insufficient number
of permutations, M? This situation compounds overfitting and non-reliable conclusions could be drawn from the
experiment.
4IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Whatifthepatternsaremorecomplexthanthosedrawnfromone-single-modeGaussianpdf? InthiscaseMLcould
beprovidinggoodclassificationresults,distinguishingsub-patternswithinthesameconditioninimbalanceddatasets.
This question is related to the assumptions, e.g. the Gaussianity and homogeneity assumptions, that are frequently
violatedinrandomeffectanalyseswherethemixingproportionoftheeffectshouldbeconsideredinstead[32].
Asanillustration,K-foldCVwasevaluatedinaclassificationtaskconsistingofsynthetici.i.d. datadrawnfromtwo
completely overlapped Gaussian distributions, representing two groups to be compared, in n = 2 dimensions; that
is, with Cohen’s d = 0 between the distribution centroids. We assessed the control of False Positives (FP) in this
experimentwhereanyeffectsdetectedaretypeIerrorsasafunctionofsamplesize.
Asshowninfigure1, theexperimentontheleftshowsasymmetricdistributionofthestatisticwhenwesamplethe
Gaussian pdf and directly compare the results. On the right of 1 is the estimated null-distribution of classifier error
obtainedwhenpermutinglabelsF =100timesusingasingle-pointrealizationofthesample. Itisnon-symmetrically
distributedandbiasedaround50%. Inshort,thestatisticaltestsperformedinapermutationanalysisresultinexcess
FP(orfalsenegativesdependingonthesignofthebias)aboveorbelowrandomchance.
3.2 Classificationvariabilityacrossindependent(multi-sample)experiments
AssumetwolaboratoriesAandBareexploringaspecificneurologicalconditionversusacontrolpatternbyaclassi-
ficationtask. Datasetsobtainedfromdifferentexperimentalsetupsandsignalprocessingpipelinesmaybemodelled
byafixed, butunknownpdf. Eachlaboratoryenrolsadifferentcohortofparticipantsandafterdataacquisitionand
processing they obtain noisy i.i.d samples (theoretically) drawn from different samples of the same pdf. We may
simulatethisscenariowithsyntheticdatabyrandomlygeneratingtwosamplesofsamplesizeN from2-dimensional
normaldistributionsN(0,1)andN(d,1),wheredistheCohen’sdistancebetweenthecentroids. Unfortunately,even
followingthesameprotocols,laboratoriesAandBobtaindifferentperformanceaccuraciesaveragedacrossCVfolds.
Ingeneral,classificationresultsobtainedfromK-foldCVreflectsthevariabilityinthesampleatdifferentsamplesizes,
N,evenwhentheeffectislarge(d = 2)asshowninfigure2. Noteinthisfigurehowtheaccuracyissymmetrically
distributedaroundtherealeffect(blackline)andthedecreasingvariabilitywithincreasingN,asexpected. However,
witharealizationof20sampleswereadilyseethattheeffectcouldbeoverestimatedinhalfofthelaboratoriesthat
undertakethisexperiment(valuesabove95%)orunderestimatedontheotherhalf(valuesaround85%)whenthetrue
effectisaround90%. Itwouldbedesirabletoprovideanadditionaltesttoevaluateifunderthespecificexperimental
conditions(samplesize,numberofpredictors,classifiercomplexity,etc.) laboratoriesAorBgiveperformancesthat
couldbeextrapolatedelsewhereandareclosetotherealeffect.
3.3 ClassificationvariabilityacrossCV-foldsinsinglesampleexperiments
Anincreasingnumberofinternationalchallengesandinitiativesaswellasopensourcedatabasesfosterthecommon
situationwherebylaboratoryAsharesdatawithBtotestthesamehypothesis[33,34,35]. Althoughbothlaboratories
havethesamesinglerealizationofthesample,neitherobtainedthesameclassificationresultsorestimatedthesame
effect level in data, if any. This is because the selected fold distribution in the CV was different, thus predictive
accuracyinthetestdataanditsaveragemaybeslightlyorsubstantiallydissimilar. Thisscenariocanbemodelledin
thesamemannerasintheprecedingexamplesbyrandomlypermutingthetrainingandtestfoldsF times.
Asanillustration, wesimulatedtwodatadistributionswithd = 1.57followingtheprocedureshownin[36](figure
3). Weobserveda“non-symmetric”distributionofaccuracyvaluesaroundtherealeffectlevelwithincreasingsample
size. The results obtained by many laboratories are similar (less variability than the previous example), but biased
mainlyduetosmallsamplesizesandthesinglerealizationofthesample.
Finally, data may be non-Gaussian distributed, e.g. see an example in n = 2 dimensions in figure 3, generating
imbalanced modes. In the previous examples, we assumed that samples were drawn from one dominant mode and
analmostsymmetricGaussianpdf. Complexdatafollowingimbalancedmulti-modalpdfs[36]furtherincreasesthe
variabilityoftheperformanceobtainedateachlab. Moreover,thesizeofeffectstronglyinfluencesthevariabilityof
theaccuracyresults,withsmallsamplesincreasingvariability,asshowninthefollowingsections;seefigure9.
4 TheK-foldCrossUpperBoundValidationtest
4.1 UpperBoundingtheactualerrorundertheworst-casescenario
ThemaingoalofSLTistoprovideaframeworkforaddressingtheproblemofstatisticalinference[37,38]. Oneof
itsmostnotableachievementsistoestablishsimpleandpowerfulconfidenceintervalsforboundingtheactualriskof
5IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure2:Bottomleft:exampleofaclassificationproblemusinglineardecisionfunctionsandsamplesdrawnfromtwo
GaussianpdfswithCohen’sd = 2similartotheproblemdescribedinsection3.1. Bottomright: averagedaccuracy
and its standard deviation versus sample size are displayed in green font for the standard K-fold CV, K = 10. The
theoreticalerrorachievedbylinearclassifiersandthewholedataset(2∗104 samples)inthisproblemisdisplayedby
theblackline.
6IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 3: Bottom left: Example of a classification problem with a single sample generated following the procedure
describedin[36]and,bottomright: aF =100-foldsdistributionforK =10. Seealsofigure2.
misclassification, R(f) [7]. In particular, we are interested in the estimation of the risk from an empirical quantity
withprobabilityatleast1−ηas:
R(f )≤R (f )+∆(N,F) (3)
N N N
wheref isestimatedtopreventoverfitting4,e.g. restrictingtheclassoffunctionswithf = argminR (f),and
N N N
f∈F
∆(N,F)isanupperboundoftheactualrisk. Intheworstcase,theinequalityturnsintoanequality. Thisdeviationor
inequalitycanbeinterpretedfromseveralperspectivesofclassicalprobabilitytheory[37]inordertoassesshowclose
thesumofindependentrandomvariables(empiricalrisk)aretotheirexpectations(actualrisk).
4.2 AProbablyApproximatelyCorrectBayesianbound
Here, weemployoneofthemajoradvancesinthisfieldbasedonProbablyApproximatelyCorrect(PAC)-Bayesian
theory [39]. In particular, we evaluate a dropout bound inspired by the recent success of dropout training in deep
neuralnetworks. Thebounddepictedinequation3isexpressedintermsoftheunderlyingdistributionQthatdraws
thefunctionf fromthesetof“rules”,F.
4Aclassifierthatperfectlypredictsthelabelsofthetrainingdatabutoftenfailstopredictthemonthetestset.
7IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Foranyconstant,λ>1/2,andclassoflinearclassifiers,f =ω ∈RnthatareselectedaccordingtothedistributionQ,
wehavethatwithprobabilityatleast1−ηoverthedrawofthesample,thefollowingCIholdforallthedistributions
Qwithdropoutrateδ:
1 (cid:18) 2λ2 (cid:18) k(cid:19)(cid:19)
R(ω)≤R (ω)+ min R (ω)+ i D(Q,Q ))+ln (4)
N 1≤i≤k 2λ i−1 N N u η
whereD(Q,Q )= 1−δ||ω||2istheKullback-LeiblerdivergencefromQtotheuniformdistributionQ =N(0,1)n
u 2 u
andλ∈(1/2,10)cantakekdifferentvalues[39].
4.3 AstatisticaltestbasedonK-foldCVandupperbounding
Thestatisticaltestproposedinthissection,theK-foldCUBVtest,formalizestheselectionoftheaccuracythreshold
(usually 50%) as the threshold for detection of an effect in a between-group analysis. Given a classifier with fixed
complexity and number of predictors (dimensions of the input pattern) in a small sample-size dataset, if the effect
is small we cannot guarantee statistical significance with just an empirical error. Instead, we propose to reject the
null-hypothesisiftheanalysisoftheworstcasewithatleastaprobability1−ηprovidesanupperboundthatsatisfies:
R(f )=R (f )+∆(N,F)≤η (5)
N N N
where we use CV to estimate the empirical risk R (f ). Note that this probability η refers to the analysis of the
N N
worstcaseandisnotrelatedtothelevelofsignificanceαofaclassicalstatisticaltest. Withanα < 0.05only1out
of20randomeffectswouldbeconsideredasarealeffectbyrejectingthenullhypothesis. Intheworstcaseanalysis
wecouldsoftenthisconditiontoη = 0.5asitreferstothesupremumofthedeviationbetweenactualandempirical
errors[40], avaluethatanefficientSLTalgorithm, suchasalinearSupportVectorMachine(SVM)[41], couldnot
achievegiventhesampleset.
In figure 4 we collect all the ideas presented in previous illustrations of different experimental scenarios. We show
the operation of the proposed CUBV test for detecting effects with good performance using the regular K-fold CV
andasingle-pointrealizationofthesample. Here,performanceisdescribedintermsofvariabilityoftheerrorandits
deviationfromtheactualrisk. Asshowninfigure4,theoreticallysamplingtheunknownpdf(ingreenfont)providesa
biasedestimationofthetheoreticalerror.Moreover,usingasinglerealization(dash-dottedlines)doesnotimprovethe
situation,butquitetheoppositeespeciallyatsmallsamplesizes. Finally,whenthedeviationobtainedbytheCUBV
techniqueisinthemajorityaboveη = 0.5(blue-shadedareaontheright)onecanbeassuredthatperformanceofa
single-pointrealizationbasedK-foldCVin“laboratory1”canbe“extrapolated”tootherlaboratories. Notethatthe
sizeofeffect(realclassificationratesofaround75%)influencesthisdecision.
5 MaterialsandMethods
5.1 Syntheticdatasetstomeasureheterogeneousandmulti-clusterdatasets
Currentbiomedicaldataprovidemulti-modal/dimensionalsourcesofinformationwhichresultincomplexandhetero-
geneousdatasets[42,43](seefigure4).Realdatainbetween-groupanalysesareusuallycharacterizedbyamultimodal
pdfthatdrawssamplesfromseveralsourcesofvariability(covariates)pergroupsuchassex,age,socioecomicstatus,
genetic profiles, and so on [44] that all influence the effect under observation [22, 46]. Assuming data from each
groupisdefinedbyabalancedsetofGaussianclusters,eachclusterrepresentingadatasource,wegeneraterealistic
data following the procedure described in [36], or simply by generating samples given each Gaussian cluster in a
n-dimensionalspace.
SLT provides additional statistical measures to control: i) the degree of reality of the pattern generated by these
procedures(typicallyusedintheextantliteraturetoevaluatemodelsinacontrolled,experimentalfashion),orsimply;
ii)theabilityoftheselectedclassifiercomplexitytoperformwellinaclassificationtask.
Heterogeneityindataisnotalwaysproportionaltothenumberof(unknown)sourcesgeneratingsampleswhen,forex-
ample,they“correlate”withinaclass. Theconceptofdata-(in)dependentmeasuresofcomplexityofsetsoffunctions
derivedbySLTcanbeevaluatedforthispurpose[7]. Ingeneral,capacitymeasures,e.g. theRademacheraverage,es-
tablishaconnectionbetweenempiricalerrors(derivedfromthetrainingset)andtheunobservedgeneralizationerror;
i.e. theactualerror. Oneofthesemeasuresisthewell-knownVCdimension[37]thatfortheclassoffunctionsF of
linearclassifiersiseasytocompute: h = n+1. Inanutshell, theVCdimensionoftheclassoflinearclassifiersis
thesizeofthelargestsetthatitcanshatter. Forexample,intwodimensionswecanshatteruptothreesamplesusing
linearclassifiers(seefigure5).
8IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 4: Performance of K-fold CV in common experimental designs. Typical large biobanks include data across
modalities including neuroimaging, biosensors, genetic, clinical, omics, etc. data. A set of synthetic and real MRI
samples, obtained from N sources and expressed as n dimensional features, are analysed. The theoretical error
c
achievedbya(linear)classifiercanbeassessedbyresubstitutionontheinfinitepopulation(pdf). Then,theK-foldCV
errorisestimatedby(theoretically)samplingthispdf(M times)orby(realistically)permutingthelearningfoldsusing
asinglerealizationofthesample(F times). TheproposedCUBVtestrejectsthenull-hypothesisintheblue-shaded
area.
Assume a number, N , of n-dimensional Gaussian pdfs (clusters) from which samples are drawn with centroids
c
randomlydistributed“faraway”fromoneanother(non-overlapping)intheinputspace. TheVCdimensionforlinear
classifiersestablishesthatoutof2Ncdifferentbinarycombinations,linearclassifierscanshatteronly2n+1simulations.
If the clusters are grouped into balanced binary classes {0,1} we have only (cid:0) Nc (cid:1) 5 different classification
Nc/2,Nc/2
simulationsinsteadof2Nc. Forexample,generating6clusters(3perclass)inn=2dimensions,wehave20different
classificationsimulations outof 64. Removingthe inversesimulations (invertinglabels) only10 remain andamong
them 7 represent a problem that a linear classifier cannot shatter (those described by multimodal pdfs or subsets
with intersecting convex hulls). This example is depicted in figures 5 and 6 (bottom). This procedure allows us to
selectwhichsimulateddatasamplesrepresentarealisticdatapoolforsubsequentsimulations. Ingeneral, anydata-
dependent capacity measure detects if the sample is likely to be drawn from a multimodal pdf. In this case, we can
visually/analyticallyidentifythenumberofsimulationsfollowingthisrealisticpattern.
5.2 SimulationofsingleclusterGaussianPDFs
Wegeneratedsyntheticdatatomodeldifferentscenariosinneuroimagingdataanalysiswherebyobservationvectors
xweresimulatedforeachgroupfromGaussiandistributions(similartobottomleftinfigure2)withdifferenteffect
sizes(d : 0−2),samplesizes(N : 20−500),anddimensions(n : 1−6). Tocomplementtheresultspresentedin
thepreliminaryexampleinsection3,wecomputedthepowerofthek-foldCV-basedpermutationtestandtheupper-
bounding test (considered as an independent test) in these commonly encountered experiments. We drew samples
from n = 2 Gaussian pdfs with only two clusters (one per group) and increasing d. Note that the aim of this paper
is the combination of both; that is, using the CUBV technique to validate the K-fold CV test within a permutation
analysis.
5Themultinomialcoefficientorthenumberofwaystoputmclustersintokclasses,(cid:0) m (cid:1) := m! ,where
m1,m2,...,mk m1!m2!...mk!
m isthenumberofclustersinclassj.
j
9IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 5: Data complexity and VC dimension in n = 2. In two dimensions the number of non-intersecting convex
hulls is, in general, 2h for a set of points or distant clusters with cardinality less than the Radon number (n + 2)
[45]. Assuming balanced sources and N > n+2 we have only Nc separable simulations whilst the number of
c
non-separablesimulationsgrowwithorder∼ √2Nc+1
2πNc
5.3 Simulationofcomplexmulti-clusterGaussianPDFs
Werepeatedthesimulationsincreasingthenumberofclusters,N ,pergroup(seefigure6)andconstructingpdfsfor
c
eachgroupfromanimbalancednumberofsamplesdrawnfromeachcluster. Thisprocedureincreasesthecomplexity
ofthesimulationaffectingthegeneralisationofperformanceofK-FoldCV.Theideabehindthissimulationisquite
simple:bychancethefittedclassifiersatthetrainingstagecannotperformwellonthetestsetbecausepatternsinboth
groups(trainingandtest)aredrawnfromdifferentpdfmodesatdifferentratios.
5.4 Neuroimagingdataset
DatasetsprovidedbytheInternationalchallengeforautomatedpredictionofmildcognitiveimpairment (MCI)from
MRIdata(https://inclass.kaggle.com/c/mci-prediction)wereconsideredfortheevaluationoftheproposedmethodin
aneuroimagingcontext.
MRI scans were selected from the Alzheimer’s disease Neuroimaging Initiative (ADNI, http://www.adni-info.org)
and preprocessed by Freesurfer (v5.3) [33]. The dataset consisted of 429 demographic, clinical, and cortical and
subcortical MRI features for each participant. Participants were in four groups according to their diagnostic status:
healthycontrol(HC),ADpatients, MCIindividualswhosediagnosisdidnotchangeinthefollow-up, andconverter
MCI (cMCI) individuals that progressed from MCI to AD in the follow-up period. The training dataset contained
10IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 6: We generate realistic datasets [36] including several modes by selecting a different number of clusters or
sources generating the data sample, N = {2,4,6}, and setting two balanced classes in all possible configurations:
c
#simulations= 1(cid:0) Nc (cid:1) ={1,3,10}. Bottom: threeseparablelinearsimulations1,4,10;sevennonseparablelinear
2 Nc/2
simulations: 2,3,5-9.
11IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
400ADNIindividuals(100HC,100MCI,100cMCIand100AD).Upto20-dimensionalpartialleastsquares(PLS)
based features were extracted from the MRI-study groups according to [36] and then combined to provide different
classificationproblemswithheterogeneoussources(N =4)bycombiningtheresultinggroups.
c
5.5 MonteCarloPerformanceEvaluationandPowerCalculations
Monte Carlo (MC) simulation was employed to determine the number of trials of a random variable, or statistic T,
requiredtoachievedetectionwithagivenathreshold[47],e.g. P(T >γ)>0.5.
TheprobabilitythataT-statistic,consistingofanaverageofnormallydistributedrandomvariables,isgreaterthana
(cid:18) (cid:19)
thresholdisequaltoP(T >γ)=Q √ γ ,whereQistheright-tailprobabilityandσisthestandarddeviation
σ2/N)
oftherandomvariable. Then,byestimatingthisprobabilityusinganumberofM trialswecanevaluatetheminimum
valuetoachievedetectionatagivensignificancelevelαas:
[Q−1(α/2)]2(1−P)
M ≥ (6)
ϵ2P
whereϵisthemaximumdeviationallowedbetweenP anditsestimationPˆ. Pˆ iscomputedbycountingthenumber
oftimesT isgreaterthantheselectedthresholdinM trials.
MCsimulationswereperformedincontrolledexperimentstoundertakepowercalculations.Weestimatedthepowerof
thetest(1−β)acrosspermutationsasthenumberoftimesthep-value(equation2)waslessthanthesignificancelevel,
0.05, divided by the total number of permutations. This was done for different sample sizes under ideal conditions,
i.e. mono-modal Gaussian pdf per class, and with imbalanced and multimodal Gaussian data (see the experimental
section6).
5.6 ExperimentalDesigns
Weperformedananalysisoftheschemeshowninfigure4undertheexperimentalconditionsdescribedintheillus-
tratedscenarios,3. Foreachexperimentaldesign,threeclassificationmethodswerecompared: K-foldCVandK-fold
CUBV, all with K = 10. The experimental designs were chosen to: i) evaluate the power of the tests; ii) assess
the theoretical (ensemble) and real (single realization) performance of the regular K-fold CV test; and iii) carry out
experimentsonaMRImulticlassdataset[33].
Power calculations for the null experiment, where d = 0 (3.1), were undertaken to understand nominal FP control.
Thesevalueswereincludedwithinpowercalculationsatdifferenteffectsizesd={0...4}inn=2dimensions,data
complexityN ={2,4},samplessizesN ={20...500},asshownintheexperimentalpart.Toassesstheprobability
c
ofdetectionweevaluatedtheMCperformanceoftheregularK-foldCV(numberoftrialsrequiredtodetecttheeffect)
andthedetectionabilityofthetestbasedonCUBV.
Alltheexperimentswerecarriedoutinthe“idealcase”basedonsamplingthetheoreticalpdfM ={100,1000}times
orlabelpermutingF ={100,1000}timesasinglerealizationofthesample(realcase). Inaddition,inbothcaseswe
generateddatafromsinglemodepdfandmultimodepdf,thusincreasingthecomplexityoftheproblem. Finally,real
MRIdatawasanalysedusingthismethodologytovalidatethefindingsachievedonastatisticalsignificantlevel.
6 Results
6.1 Resultsonthenullexperiment
Resultsforthenullexperiment,d=0,aredisplayedinfigures11and12. PoweroftheK-foldCV-basedpermutation
test was above the significance level confirming a FP rate in some experimental setups beyond an admissible level,
figures 17 and 18. This effect is partly controlled by increasing the sample size with a low data complexity. The
K-foldCUBV,usedasastatisticaltest,alwaysprovidedpowerbelowthesignificancelevelandthuscanbeconsidered
conservativewithinML-basedinferenceapproaches.
6.1.1 ControloftypeIerrorsacrossindependent(multi-sample)experiments
Infigure7wedisplaytheanalysisoftheFPrateunderthenull-hypothesiswithincreasingdimensionnandsample
size N. Performance is plotted for all the validation methods with M = 100 samples obtained from theoretical
GaussianpdfsandN = 2,theMCperformanceevaluationandtheworstcasedetectionanalysis. CUBVcontrolled
c
12IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
typeIerrorsinalltheanalysedsimulations. Notethatbothdetectionmethods(MCandCUBV)yieldssimilarresults
ofnoeffect,buttheFPrateinK-foldCVisalwaysabovethenominalvalue.
6.1.2 ControloftypeIerrorsinsinglesampleexperiments
In figure 8 we display the standalone analysis of the FP rate under the null-hypothesis with increasing dimension
n and sample size N. We plotted the performance of all the validation methods with F = 100 samples obtained
fromthefoldpermutationsofonesinglerealizationandN = 2,theMCperformanceevaluationandtheworstcase
c
detection analysis. This again highlights the ability of the K-fold CUBV to control type I errors unlike K-fold CV
whoseoptimisticperformanceisconfirmedbytheMCperformanceevaluation(detectionisachievedunderthenull
hypothesiswhencoloredlinesgobelowtheblueline: 8,bottom).
6.2 Classificationvariabilityacrossindependent(multi-sample)experiments
6.2.1 K-foldvariabilityvscomplexity
In figure 9 we show the classification values obtained averaging M = 1000 experiments and {1,3,10} simulations
usingK-foldCVasafunctionofthenumberofclusters(complexity)thatgeneratethedata,andsamplesizeforanup
to6dimensionalproblem. Thevariabilityoftheperformanceincreaseswithcomplexityandsamplesize,althoughthe
dimensionalityofthefeaturescanpartlyrelievethisproblem(seefigurescale). Unfortunately, mostofthemethods
usuallyemployedinneuroimagingareunivariate,typicallyvoxelwise,andtheconditionsmostfrequentlyencountered
arethosereflectedintheleftsideoffigure9. Thecurvesinthisfigurearecomparedtotheestimatednull-distribution
tomakeaninferenceaboutthedata,asshowninfigure11.
6.2.2 Poweranddetectionanalysisinsinglemodepdf
In figure 11 we show the behaviour of both approaches, K-fold CV and CUBV, with changes to Cohen’s d and
effect size in a n = 2 classification task. Statistical power is computed against d by evaluating the probability that
pvalue<0.05giventhesetoftrials,M.
When using the CUBV technique, the power of the test is computed by thresholding at the level of random chance
(equation 5). However, if we compute the number of required MC trials to achieve detection with a degree of con-
fidence α = 0.05 we obtain an unexpected result, as shown in figure 11 on the right. The number of required MC
trialstoachievedetectionforsmalleffectsizesisabout7timesthesamplesize. Thus,forexample,ifN = 500then
M ∼ 3700, even under the controlled experimental conditions of this simulation (samples drawn from a Gaussian
pdf).Conversely,theCUBVtechniqueachievesasignificantdetectionwithjustafewsamples. It’sclearthatbasedon
theinformationontherightoffigure11,wecanvalidatetheresultsobtainedwiththeclassicalML-basedpermutation
testusingthistechnique,unliketheMCevaluationasittheoreticallyrequiresmanytrialstoachievedetection.
6.2.3 Poweranddetectionanalysisinmulti-modepdf
Werepeatedthepoweranalysisoftheprevioussection(n = 2andN = 4)inthenon-separablecasepredictedby
c
thecapacitymeasure(seefigure12)assumingabalancedsampleperclusterandpergroup(i.e. sourceshavethesame
effectontheobservedvariable),andwitharatior =1/3perclusterinbothclasses(imbalancedcase).
Here,wereadilyseethatthedetectionabilityofthetestusingK-foldCVislesspowerfulthaninthe“ideal”caseand
greaterthanthelevelofsignificancewithnoeffect(d=0),thusprovidingaFPrategreaterthanthenominalvalue.
TheMCperformanceisworsethaninthepreviouscase,e.g. thecurvedoesnotdecreasewithsamplesizeatvalues
between2and3dBintheworstscenario(balancedsample),thusthenumberoftrialsneededtoachievedetectionat
thelevelofconfidenceisbetween7−20timesthesamplesize. TheCUBVtechniqueisexpectedtoachievedetection
withfewersamples((N,d) = (200,4)and(300,3),respectively). Intheimbalancedcase,thebenefitsoftheCUBV
approachareevenclearerasitpreservesdetectionabilitywhilstcontrollingFP.
Finally,figure13illustratestheoperationoftheCUBVapproachandtheregularK-foldCVagainstdforn=2. Sub-
stantialcontrolofFPisachievedbythecombinationofbothapproaches;figure13topleft. Optimisticorconservative
estimationsofthe“realeffect”(small,mediumandlargeeffects)arepredictedbytheCUBVmethodwitherrorsbelow
0.5atsmallsamplesizes.
13IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 7: Examples of performance, FP rates and MC performance evaluation across independent (multi-sample)
experiments
14IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure8: Examplesofperformance,FPratesandMCperformanceevaluationinsinglesampleexperiments.
15IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 9: The accuracy values (average and standard deviation) obtained in K-fold CV versus complexity (N ) and
c
samplesizeN withM =1000andalargeeffectsize,inan=1(top)andn=6(bottom)binaryclassificationtask.
16IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 10: The accuracy values (average and standard deviation) obtained in CUBV versus complexity (N ) and
c
samplesizeN withM =1000andalargeeffectsize,inan=1(top)andn=6(bottom)binaryclassificationtask.
Notethedashedlineinbluecorrespondingtotherandomthreshold.
17IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
6.3 ClassificationvariabilityacrossCV-foldsinsinglesampleexperiments
6.3.1 Performanceanalysisincludingcomplexity
This simulation is close to empirical studies commonly found in the contemporary literature. Given one, and only
onedatasetwithanincreasingnumberofclustersordatacomplexityweranapermutationtestbyrandomlyselecting
trainingandtestingfoldsrepeatedly(F =100)undervaryingsamplesize(N :20−500),dimension(n:1−6)and
complexity(N :2−6)usingapriorprocedure[36]. Theeffectsconsideredweremediumtolargeasshowninfigure
c
6.
Aspreviously,thebehaviouroftheestimatoroftheerroranditsaveragewerenotassymmetricastheidealcase,with
consequences for small sample-sizes and low dimensions, as predicted by the capacity measures detailed in section
5.1. Asanexample,infigure14forn=2andN =4therewas1outof3situations(numberofcasesoutofdifferent
c
simulationsthatcannotbeshattered)wheretheestimationwasbiasedtowardsunderestimatingtherealeffect. Inthis
case(2outof3),thetestdevelopedinsection4.3didnotrejectthenull-hypothesis(noeffect)foranyvalueofN. In
cases1and3,significantrejectionisachievedaboveN =50samplesusingK-foldCUBV.
Infigures15and16werepeatedthesameanalysiscarriedoutinsection6.2.1withasinglerealisationandaveraging
overthesetofsimulationswithincreasingcomplexity.
6.3.2 Poweranddetectionanalysisinsinglemodepdf
With the same parameter configuration at the previous section 6.2.2, we computed the same measures using only
asinglerealizationofthesample. Infigure17weshowtheexpectedbehaviourofbothapproaches, K-foldCVand
CUBV,varyingCohen’sdandsamplesizeinan=2classificationtask.Statisticalpowerwascomputedbyevaluating
theprobabilitythatp <0.05forasetoftrials,F =100. Surprisingly,thenumberofFPsforanumberofparameter
configurationsoftheK-foldCVmethodwasclearlyadmissibleevenatlargesamplesizes.
6.3.3 Poweranddetectionanalysisinmulti-modepdf
Withthesameparameterconfigurationthanprevioussection6.2.3wecomputedthesamemeasuresusingonlyasingle
realization of the sample. The power and detection analysis with increasing complexity is shown in figure 18. The
resultsareinlinewiththoseshownintheprevioussection,wherewehighlightedtheabilityoftheproposedCUBV
methodtoadequatelycontrolFPs.
6.4 ResultsonrealMRIdata
MRI datasets, described in section 5.4, were evaluated with the proposed CV methods. Multiclass classification
problems are usually divided into multiple, separate binary classifications. In this section we analyse three binary
classificationproblemsP1: HC+MCIvsAD+MCIc; P2: HC+MCIcvsMCI+AD;P3: HC+ADvsMCI+MCIc, that
arisebycombiningtheconditionsstudiedinonsetandprogressionofAlzheimer’sDisease(HC,AD,MCIandMCIc).
Cohen’sdistancedforthesetofproblemscanbeseeninfigure19. WeranF = 1000CVexperimentsandaveraged
theaccuracyvaluesobtainedfromK-fold,LOOandtheproposedCVmethods.
6.4.1 Realdataanalysis
In figure 19 we plotted the Cohen’s distance for the analysed groups. Note that the null distribution is not properly
modelledalthoughM = 1000permutationswereusedtosimulatetheconditionofnoeffect. Indeedthereisasmall
effecthiddenindataofabout0.1. Thiscanbeexplainedbythepresenceofmultipleclustersineachgroupandthe
similaritiesamongthem.
Dataobtainedafterfeatureextractionisrathersimilartothatanalysedintheprevioussimulatedexamplesasshown
in figure 20. The presence of non-Gaussian distributions with multiple modes represents the typical scenario where
standardCVmethodsareusuallyemployed.
6.4.2 MeanclassificationvariabilityandMCperformance
Infigure21weshowthevariabilityinthemeanaccuracyvaluesobtainedbyLOOandK-foldCV(F = 1000)that
are compared with a threshold of 0.5. The CUBV method shows an almost monotonic behaviour converging to the
theoreticalerrorvaluewithincreasingsamplesizeanddimension. Ontheotherhand,infigure22theMCanalysisis
shownforbothapproacheswhereitcanreadilybeseenthatalargernumberoftrialsisneededtoachievedetection
18IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
using the K-fold CV method (limited to 40 dB for visualization purposes and above M = N at all times) and the
increasedprobabilityofdetectionusingtheproposedapproachwithincreasingdimensionandsamplesize.
6.4.3 PowerAnalysis
Finally,weplottedinfigures23and24thenormalizedcumulativesumofbetavalueswithincreasingdimensionand
samplesizeforalltheanalyzedproblems(P1-3),andthenullexperiment.Anydeviationfromtheslopeequalto1line
intheK-foldCVmeansadetectionwithpowerlessthan1,unlikeCUBVthatdetectsaneffectwhenaccuracyexceeds
0.5. ObserveinthenullexperimentthatK-foldCVismainlyoperatingabovethesignificancelevel(α = 0.05)with
increasing sample size and dimension. In the latter experiment, CUBV only performs weakly at specific values of
smallsamplesizesandhighdimensions. Notethatthenulldistributionisincorrectlymodelledacrosssamplesizes.
7 Discussion
From the set of experiments undertaken here, we have demonstrated that the proposed CUBV method is effective
for assessing the variability of accuracy values obtained by K-fold CV. Moreover, the evidence derived from these
experimentssuggeststhatCUBVisarobustapproachtostatisticalinference.
Whenever accuracy values were associated with a large deviation between actual and empirical risks, the CUBV
methoddidnotgeneratesignificantresultsoverthatexpectedbyrandomchance;thatis,thenullhypothesiswasnot
erroneouslyrejected. Underthesameconditions, K-foldCVprovidedaccuraciesaboveandbelowthethresholdfor
significantacrossalleffectssizeswhereCUBVindicatedthattheunseenCVaccuracyvalues,drawnfromanunknown
pdf,wereexpectedtohavealarminguncertainty.Inthissense,CUBVisatrade-offbetweenthepropercontroloffalse
positivesandthepowertodetecttrueeffectsoftheK-foldCVtest.
Classicalmethodstoperformpowercalculationsandevaluatetherequirednumberoftrialstoachievedetection(the
MCevaluationmethod)revealedthatK-foldCVusuallyoperatesfarbelowitstheoreticalperformance.Inotherwords,
theMCmethodcanbeconsideredanover-conservativemethodandnovelcomplexAIapproachesthatleveragethese
methodstoagreatextentcannotnotberigorouslyvalidatedwithintheseframeworks.
ThetheoreticalfindingsarealsoapplicabletotheMRI-basedsamplesfromADpatients. Themodelsdevisedinthis
paper together with the simulation of realistic datasets create suitable exemplars for characterising performance in
neuroimaging applications. A simple comparison between the datasets from figure 20 with those in figures 12 and
6 reveals the similarity in the results obtained. Nevertheless, the scatter plots and data distributions projected in the
dimensions are clear examples demonstrating that the conditions to provide stable inducers are not met, and thus
explorationofalternativevalidationmethodsisapriority.
At the final stage of any (image) ML-based processing system classifiers learn from folds of the limited amount of
complexandmultimodalsamples.TheconservativenatureoftheCUBVmethodresultsinrobustdetectionthatshows
a monotonic behavior with sample size and feature dimension; see figure 22. The detection ability of K-fold CV
methodsinthesearchforrealeffectsisarguableinthelightoffalsepositivesfromthenull-experiment. Asshownin
figure23theirnormalizedcumulativesumofpowershowsalineardependenceonsamplesizeandfeaturedimension.
Thus,increasingsamplesizedoesnotcontroloftheriskoffalsepositivefindings,infactquitetheopposite. Oneof
thereasonsforthepoorperformanceisthedifficultyinmodellingthenulldistribution,asshowninfigure19. Even
withF = 1000labelpermutationsandgiventhe“single-point”N = 400sampledataset,thereisaneffectofabout
0.1hiddenindatathatprovokesaflawedstatisticalanalysis. Itisworthmentioningthat,withincreasingcomplexity
ofsimulateddataorwhentheproblemtobesolvedischallenging,e.g. P2andP3usingtheMRIdataset,theCUBV
methodisevenmoreusefulforcontrollingtheFPrateasshownintheexperimentalpart.
Finally, we emphasize the relevance of highlighting negative results in order to improve science6. Positive results
are often the main goal of any research paper and we seldom evaluate our algorithms on putative task designs with
no effect. This is the case described in section 3.1. This analysis is important because it is usually employed to
approximatelymodelthenull-distributionofthetest-statisticinpermutationanalyses,e.g. performanceoraccuracyin
aclassificationtaskusingMLtechniques. Inpermutationanalysistheperformanceobtainedfromthepaireddataand
labelsiscomparedtothatobtainedbyrandomlypermutingthegrouplabelsalargenumberoftimes, andshouldbe
distributedaround50%. Ifthedistributionoftheperformanceisnon-symmetricaroundrandomchance, andisthen
biased,theresultderivedfromthetestdataislikelytobeflawed. Thiswouldmeanthatthedistributionofdatadiffers
betweengroupsunderthenullhypothesis, whichviolatesthei.i.d. assumptionandtheestimationofp-valuescould
leadtoincorrectconclusionsatthefamily-wiselevel[25].
6PleaseseethecolumninNatureaboutthisissuehttps://www.nature.com/articles/d41586-019-02960-3
19IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
8 Conclusions
Standard CV methods in combination with ML were evaluated to ascertain whether statistical inferences made by
data-drivenapproachesaresufficientlyconsistent. Theyarefrequentlyclaimedtooutperformconventionalstatistical
approachessuchashypothesistesting. However,thisimprovementisbasedonmeasures(e.g. accuracy)derivedfrom
MLclassificationtasksthatdonothaveaparametricdescriptionanddependontheexperimentalsetup; i.e. derived
fromclassification/predictionfoldsofthedatasettoestablishconfidenceintervals.
Asshowninthispaper,smallsample-sizedatasetsandlearningfromheterogeneousdatasourcesstronglyinfluences
theirperformanceandresultsinpoorreplication. AnovelstatisticaltestbasedonK-foldCVandtheUpperBoundof
theactualerror(K-foldCUBV)wasproposedtotackletheuncertainpredictionsofMLandCVmethods.Theanalysis
oftheworstcaseobtainedbya(PAC)-BayesianupperboundforlinearclassifiersincombinationwiththeK-foldCV
estimation is a robust criterion to detect effects validating accuracy values obtained from ML models and avoiding
falsepositives,complementingtheregularK-foldmethodforCV.
Acknowledgments
ThisworkwassupportedbytheMCIN/AEI/10.13039/501100011033/andFEDER“UnamaneradehacerEuropa”
under the RTI2018-098913-B100 project, by the Consejería de Economía, Innovación, Ciencia y Empleo (Junta de
Andalucía)andFEDERunderCV20-45250,A-TIC-080-UGR18,B-TIC-586-UGR20andP20-00525projects.
References
[1] Y.LeCunetal.Deeplearning.Nature521,436–444(2015).
[2] MTRibeiro,etal.Model-agnosticinterpretabilityofmachinelearningarXivpreprintarXiv:1606.05386.2016.
[3] P Grohs, et al. Mathematical Aspects of Deep Learning. Cambridge University Press. ISBN 9781009025096.
https://doi.org/10.1017/9781009025096.
[4] L.vanderMaatenetal.VisualizingDatausingt-SNE.JournalofMachineLearningResearch2008vol9,num86,
2579–2605.
[5] G.Varoquaux.Cross-validationfailure:Smallsamplesizesleadtolargeerrorbars.NeuroImage180(2018)68-77.
[6] Gorgen, K., et al. The same analysis approach: Practical protection against the pitfalls of novel neuroimaging
analysismethods.NeuroImage,180,19-30.2018.
[7] S.Boucheronetal.ConcentrationInequalities:ANonasymptoticTheoryofIndependenceISBN:9780199535255
OxfordUniversityPress
[8] J.Mouro-Miranda, etal.Classifyingbrainstatesanddeterminingthediscriminatingactivationpatterns: Support
vectormachineonfunctionalMRIdata.NeuroImage,28,980-995.(2005).
[9] Y. Zhang et al. Multivariate lesion-symptom mapping using support vector regression. Hum Brain Mapp. 2014
Dec;35(12):5861-76.
[10] JMGorriz,etal.Aconnectionbetweenpatternclassificationbymachinelearningandstatisticalinferencewith
theGeneralLinearModel.IEEEJournalofBiomedicalandHealthInformatics2021.
[11] JMGorriz,etal.Ahypothesis-drivenmethodbasedonmachinelearningforneuroimagingdataanalysis.Neuro-
computingVolume510,21October2022,Pages159-171
[12] K.J.Friston, et al. Statistical Parametric Maps in functional imaging: A general linear approach Hum. Brain
Mapp.2:189-210(1995)
[13] K.J.Friston, et al. Classical and Bayesian inference in neuroimaging: theory NeuroImage, 16 (2) (2002), pp.
465-483
[14] J.D.Rosenblatt,etal.Revisitingmulti-subjectrandomeffectsinfMRI:Advocatingprevalenceestimation.Neu-
roImage84(2014): 113-121.
[15] NationalAcademiesofSciences, Engineering, andMedicine.(2019).ReproducibilityandReplicabilityinSci-
ence.Washington,DC:TheNationalAcademiesPress.https://doi.org/10.17226/25303.
[16] A.Eklund, et al. Cluster failure: Inflated false positives for fMRI. Proceedings of the National Academy of
SciencesJul2016,113(28)7900-7905.
20IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
[17] S. Noble, et al. Cluster failure or power failure? Evaluating sensitivity in cluster-level inference. NeuroImage,
209,116468,2020.
[18] ZWang,etal.Supportvectormachinelearning-basedfMRIdatagroupanalysis.NeuroImage36(4),1139-1151.
2007
[19] ZWang.AhybridSVM–GLMapproachforfMRIdataanalysis.Neuroimage46(3),608-615.2009.
[20] JollansL,etal.Quantifyingperformanceofmachinelearningmethodsforneuroimagingdata.Neuroimage.2019
Oct1;199:351-365.
[21] M.J.McKeownet.al.IndependentcomponentanalysisoffunctionalMRI:whatissignalandwhatisnoise?Curr
OpinNeurobiol.2003Oct;13(5): 620–629.
[22] J.M.Górriz,etal.AMachineLearningApproachtoRevealtheNeuroPhenotypesofAutisms.Internationaljour-
nalofneuralsystems,1850058.2019.
[23] G. Gallavotti. Ergodicity, ensembles, irreversibility in Boltzmann and beyond Springer March 1995 Journal of
StatisticalPhysics78(5):1571-1589
[24] R.Kohavi.Astudyofcross-validationandbootstrapforaccuracyestimationandmodelselection.International
JointConferenceonArtificialIntelligence(IJCAI),pp1–7,1995.
[25] B.Phipsonetal.PermutationP-valuesShouldNeverBeZero: CalculatingExactP-valuesWhenPermutations
Are Randomly Drawn. Statistical Applications in Genetics and Molecular Biology: Vol. 9: Iss. 1, Article 39.
(2010)
[26] R.S.J. Frackowiak, et al. Human Brain Function (Second Edition). Chap. 44. Introduction to Random Field
Theory.ISBN978-0-12-264841-0AcademicPress.867-879,2004.
[27] T.E.Nichols. Multiple testing corrections, nonparametric methods, and random field theory. NeuroImage 62
(2012)811-815
[28] K.J.Friston.Samplesizeandthefallaciesofclassicalinference.NeuroImage81(2013)503–504
[29] E T Bullmore et al. Global, voxel, and cluster tests, by theory and permutation, for a difference between two
groupsofstructuralMRimagesofthebrainIEEETransMedImaging(1999)Jan;18(1):32-42.
[30] P.T. Reiss, et al. Cross-validation and hypothesis testing in neuroimaging: an irenic comment on the exchange
betweenFristonandLindquistetal.Neuroimage.2015August1;116: 248-254
[31] C. Jimenez-Mesa et al. A non-parametric statistical inference framework for Deep Learning in current neu-
roimaging.InformationFusionVolume91,March2023,Pages598-611.
[32] J.D.Rosenblatt,etal.Better-than-chanceclassificationforsignaldetection.Biostatistics(2016).
[33] A. Sarica, et al. A machine learning neuroimaging challenge for automated diagnosis of Alzheimer’s disease.
Editorialonspecialissue: MachinelearningonMCI,vol302,JournalofNeuroscienceMethods.2018.
[34] C.C.Jack,Jr. ,et al. NIA-AA Research Framework: Toward a biological definition of Alzheimer’s disease.
AlzheimersDement.2018Apr;14(4): 535?562.
[35] J.M.Gorriz,etal.Artificialintelligencewithintheinterplaybetweennaturalandartificialcomputation:Advances
indatascience,trendsandapplications.NeurocomputingVolume410,14October237-2702020.
[36] J.M.Górriz,etal.Onthecomputationofdistribution-freeperformancebounds:Applicationtosmallsamplesizes
inneuroimaging.PatternRecognition93,1-13,2019.
[37] V.Vapnik.EstimationdependenciesbasedonEmpiricalData.Springer-Verlach.1982ISBN0-387-90733-5
[38] D.Haussler.DecisiontheoreticgeneralizationsofthePACmodelforneuralnetandotherlearningapplications.
InformationandComputationVolume100,Issue1,September1992,Pages78-150
[39] D.McAllester,APAC-BayesianTutorialwithADropoutBound.arXiv10.48550/ARXIV.1307.2118,2013
[40] J.M.Gorriz,etal.StatisticalAgnosticMapping: Aframeworkinneuroimagingbasedonconcentrationinequali-
ties.InformationFusionVolume66,February2021,Pages198-212
[41] C.J.C Burges. A tutorial on support vector machines for pattern recognition Data Mining and Knowledge Dis-
covery,2(2)(1998),pp.121-167
[42] ZhangYD,etal.Advancesinmultimodaldatafusioninneuroimaging: Overview,challenges,andnovelorien-
tation.InfFusion.2020Dec;64:149-187.
[43] J.N.Acostaetal.MultimodalbiomedicalAI.NatMed28,1773–1784(2022).
21IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
[44] C.S. Hyatt et al. The quandary of covarying: A brief review and empirical examination of covariate use in
structuralneuroimagingstudiesonpsychologicalvariables.Neuroimage205,116225
[45] H.Tverberg,AGeneralizationofRadon’sTheorem,JournaloftheLondonMathematicalSociety,Volumes1-41,
Issue1,1966,Pages123-128.
[46] M.Leming,etal.EnsembleDeepLearningonLarge,Mixed-SitefMRIDatasetsinAutismandOtherTasks.M
Leming,InternationalJournalofNeuralSystems.Vol.30,No.07,2050012.2020.
[47] S.M. Kay. Fundamentals of Statistical Signal Processing: Detection theory. Prentice-Hall PTR, 1998
013504135X,9780135041352.
22IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 11: Analysis of the ideal case. Top-up: statistical power of K-fold and CUBV (top-down) CV permutation
tests;bottom-up: MCperformanceevaluationandCUBVdetection(bottom-down).
23IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure12: Analysisofthenon-idealcase(N = 4andn = 2). Leftcolumn: samplesanddvalues-top-; statistical
c
powerofCVpermutationtests-middleupK-foldCV,middle-downCUBV;MCperformanceofK-foldCV-bottom
up-andCUBVdetection-bottomdown-usingabalanceddataset. Rightcolumn: thesamemeasuresusinganimbal-
ancedsamplewithr =1/3perclusterineachgroup.
24IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure13: Distributionofaccuracyvalues(M =100)vs. samplesizeanddforthenon-idealcase. Weshowan=2
classification problem sampling from N = 4 Gaussian pdfs (2 per cluster) using an imbalanced dataset (r = 1/3)
c
andd={0,1,2,4}. Notethebiasedregionswithinthegreenarea.
25IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure14: ExamplesofclassicalpermutationtestsbasedonregularCVandCUBVdecisionsdependingonsample
variabilityandclassifiercomplexity.Notethatwheneverthereisacleardeviationbetweenthegreenandblacklinesor
ahighvariabilityinK-foldCVisobtained,theCUBVdecisionisbelow0.5andthusthetestdoesnotrejectthenull
hypothesis. Thisdecisioniswarningthatthegreencurvevaluesobtainedcannotbeextrapolatedtootherexperiments
thustheanalysisbyanotherlaboratorycouldbeprovidingsubstantiallydifferentoutcomes,speciallyinproblem2out
of3.
26IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 15: The analysis depicted in figure 9 is replicated here using a single realization. Observe the theoretical
accuracyhighlightedbytheblueline
27IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure16: Thesameanalysisasinfigure10usingasinglerealisation
28IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure17: Examples,poweranddetectionanalysisinsingle-modepdfusingasinglesamplerealization
29IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure18: Examples,poweranddetectionanalysisinmulti-modepdfusingasinglesamplerealization
30IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure19:Cohen’sdistanceobtainedfrombinaryclasses(wholedatasets)versusdimension(PLSfeatures). Notethat
Problem1: HC+MCIvsAD+MCIc; Problem2: HC+MCIcvsMCI+AD;Problem3: HC+ADvsMCI+MCIc. The
nulldistributionisalsoplottedandwasobtainedwith1000labelpermutations. Notethatwiththissampleeventhe
nulldistributionhasasmall(incorrectlymodelled)effect.
31IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 20: Examples of data analyzed in this section with several dimensions and problems. Each classification
problemcorrespondswithadifferentCohen’sd,whilstfordimensionn>2onlythefirsttwovectorcomponentsare
displayed. Notethesimilaritiesoftheresultingsamples,afterfeatureextraction,withsimulateddata.
32IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure21: AccuracyvaluesforselectedCVmethodsasafunctionofnumberofdimensions. NotetheCUBVtech-
nique alerts on the variability of the results obtained by LOO and K-fold CV, especially in small effect and sample
sizes,andlowdimensions.Withincreasingsamplesizeestimations,classicalmethodsconvergetothetheoreticalerror
rate(obtainedbyresubstitutionofthewholedatasetinthesamemanneraswithsimulateddata).
33IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure22: Top: MCevaluationofK-foldCVinrealdatasetsbyaveragingtheresultsofProblems1, 2and3. Note
that values are limited to 40 db for visualization purposes. Bottom: detection analysis of the CUBV method versus
samplesize(N)andfeaturedimension(n).
34IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure23: Normalizedcumulativesumof(1−β)values(P :=
(cid:80)( jN =, 1n)(1−βj)
)forN andnversusdimension/sample
c #experiments
size,respectively. Top: nullexperiment;bottom: Problem1
35IsK-foldcrossvalidationthebestmodelselectionmethodforMachineLearning? APREPRINT
Figure 24: Normalized cumulative sum of (1−β) values for N and n versus dimension/sample size, respectively.
Top: Problem2;bottom: Problem3.
36