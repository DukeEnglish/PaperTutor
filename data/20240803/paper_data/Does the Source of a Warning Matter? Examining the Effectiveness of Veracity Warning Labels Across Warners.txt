Does the Source of a Warning Matter? Examining the Effectiveness of Veracity
Warning Labels Across Warners
BenjaminD.Horne
SchoolofInformationSciences,
DataScienceandEngineering,TheBredesenCenter,
UniversityofTennesseeKnoxville,Knoxville,TN,USA
bhorne6@utk.edu
Abstract stein et al. 2022; Lu et al. 2022), critical gaps in our un-
derstanding of label design and effectiveness still exist. As
Inthisstudy,weconductedanonline,between-subjectsex-
previous work from multiple disciplines has demonstrated:
periment(N=2,049)tobetterunderstandtheimpactofwarn-
trust in false information, its influence on decision mak-
inglabelsourcesoninformationtrustandsharingintentions.
ing, and its correction are complex (Lewandowsky et al.
Acrossfourwarners(thesocialmediaplatform,othersocial
2012; Swire-Thompson, DeGutis, and Lazer 2020; Penny-
mediausers,ArtificialIntelligence(AI),andfactcheckers),
wefoundthatallsignificantlydecreasedtrustinfalseinfor- cook et al. 2020; Brashier and Marsh 2020; Ecker et al.
mationrelativetocontrol,butwarningsfromAIweremod- 2022). Therefore, it is likely that human interaction with
estlymoreeffective.Allwarnerssignificantlydecreasedthe warninglabelsisalsocomplex.
sharingintentionsoffalseinformation,exceptwarningsfrom
Onethingthatiswell-knowninhuman-informationinter-
other social media users. AI was again the most effective.
actionisthatheuristics-rulesofthumbformakingdecisions
Theseresultsweremoderatedbypriortrustinmediaandthe
withoutexhaustivelycomparingalloptions-areusedwhen
information itself. Most noteworthy, we found that warning
making decisions about the information we trust (Broock-
labelsfromAIweresignificantlymoreeffectivethanallother
warning labels for participants who reported a low trust in manandKalla2016).Onecommonheuristicistrustinthe
news organizations, while warnings from AI were no more source of information, sometimes called source credibility
effective than any other warning label for participants who cues. That is, we often trust information and spend more
reportedahightrustinnewsorganizations. time reading information that comes from a trusted source
(Su¨lflow, Scha¨fer, and Winter 2019). This notion has been
well-studied in the context of trusting news stories (Go,
Introduction
Jung, and Wu 2014; Winter and Kra¨mer 2014), social me-
The rise of online disinformation and misinformation has dia posts (Kim and Dennis 2019; Kim, Moravec, and Den-
propelledscientiststofocusonunderstandingandmitigating nis2019;Su¨lflow,Scha¨fer,andWinter2019),politicalmes-
their impact. This nascent, but quickly growing, interdisci- sages (Berinsky 2017; Buchanan and Benson 2019), and
plinaryfieldofstudyhasproposedvarioussystemsofsafe- healthmessages(Hocevar,Metzger,andFlanagin2017),but
guards for online platforms to use. One particularly com- therehasbeenlessattentiontohowthesourceplaysarolein
mon,yetstillunderstudied,solutionistheuseofsoftcontent contentwarninglabels.Thewarner-theonewhogivesthe
moderation methods. Soft content moderation methods are warning-canvaryacrosslabels.Forinstance,ifawarning
interventionsthatdonotremovecontentbutinsteadlimitthe labelsays“FalseInformation.Checkedbyindependentfact-
visibility of content. Soft moderation can be done through checkers”,theindependentfact-checkersarethewarners.If
attachingveracitylabelstocontent(Zannettou2021),quar- a label is more generic, such as Reddit’s quarantined com-
antiningonlinecommunities(Chandrasekharanetal.2022), munitylabel,thewarnermaybeassumedtobetheplatform
ordemonetizingcontentproducers(Trujilloetal.2020). itself,Reddit.Givenhowimportantsourcecuesareininfor-
Inthisstudy,wefocusononeparticularmethod:veracity
mation consumption,it is reasonableto assume that source
warning labels. We choose to focus on warning labels be- cuesalsoplaysomeroleinwarninglabelconsumption.
causetheirimpactanddesignarepoorlyunderstood,despite
However, from the little work that has been done on
already being used in multiple real-life systems (Mosseri
sourcecuesinwarninglabels,thereismixedevidenceabout
2016; Morrow et al. 2022). While studies have character-
relative efficacy across sources (Martel and Rand 2023b).
izeduserengagementwithwarninglabelsonrealplatforms
For example, Yaqub et al. (2020) showed that labels from
(Zannettou 2021; Lees, McCarter, and Sarno 2022; Ling,
fact checkers were broadly more effective at deterring the
Gummadi,andZannettou2023)andhaveexperimentedwith
sharing of false news stories than warnings from news me-
how humans react to automatically generated warning la-
dia, the public, and Artificial Intelligence (AI). Similarly,
bels (Horne et al. 2019, 2020; Nevo and Horne 2022; Ep-
Seo,Xiong,andLee(2019)foundthatwarningsfromfact-
Copyright©2025,AssociationfortheAdvancementofArtificial checkers were more trusted than warnings from machines.
Intelligence(www.aaai.org).Allrightsreserved. Yet, other work has found that warning labels from fact
4202
luJ
13
]YC.sc[
1v29512.7042:viXracheckers, the public, and algorithms were equally effective organizations.Further,participantswhodistrustnewsorga-
at reducing the perceived accuracy of false information for nizationsaresignificantlymoreimpactedbywarninglabels
politicallyliberalinformationconsumers(Jiaetal.2022). from AI than all other warning label sources. On the other
Notably, across these studies the target metric of “effec- hand, AI is no more effective than any other warning label
tiveness” has differed. In some studies the target metric is source for participants who trust media organizations. We
trustininformation-i.e.,doesaninterventiondecreasetrust alsofindthatpoliticalleaningdidnotrobustlymoderatethe
infalseinformation?Similarly,somestudiesusethemetric effectivenessofwarners.Whilethereissomeevidencethat
of perceived accuracy - i.e., does an intervention decrease conservativeparticipantsarelessaffectedbywarningsfrom
perceivedaccuracyoffalseinformation?Inotherstudiesthe socialmediaplatformsandmoreaffectedbywarningsfrom
target metric is the sharing intention of information - i.e. AIintermsofeffectsize,theinteractionsbetweenpolitical
does an intervention decrease the likelihood that false in- leaningandeachwarningconditionwerenon-significant.
formation is shared on a social network? While these met- Sharingintentionsareonlyweaklycorrelatedwithinfor-
ricsarerelated,theyarenotnecessarilythesame.Somewhat mation trust (+0.3 Spearman’s ρ). While the majority of
paradoxically,perceptionsandactionsarenotalwayscorre- warning labels do significantly decrease sharing intentions
lated.Theclearestexampleofthisphenomenoncomesfrom relative to control, they have less of an impact on sharing
a 2021 report that found older people shared more misin- intentionsthanoninformationtrust.Inparticular,warnings
formationonsocialmediathanyoungpeople,andyetolder fromothersocialmediausershavenosignificantimpacton
peoplewereactuallylesslikelythanyoungpeopletobelieve sharingintentions,whilelabelsfromAI,factcheckers,and
that misinformation (Lazer et al. 2021). Further, the inten- theplatformdo.LabelsfromAIandfactcheckershavethe
tiontoshareinformationisnotnecessarilycapturedbyone’s largesteffectsonsharingintentions.
perceptionsofinformation(PennycookandRand2021),and
Broader Perspectives, Ethics, and Competing Interests
there is some evidence that frequent sharers on social me-
diainteractdifferentlywithwarninglabelinterventionsthan Ourfindingshaveimplicationsforcontentlabeldesignand
lessactivesocialmediausers(Horneetal.2019). information intervention experiment design. In the long-
term, we hope this work, and other work in the ICWSM
Hence,inthispaperweexaminetheimpactofawarning
community, helps build better content moderation meth-
label source on both the trust in and sharing of false infor-
odsandpoliciesforasaferinformationecosystem.Wefol-
mation.Weaskthreequestionsthatcombineandextendthe
lowedbestpracticesforconductingmisinformationresearch
priorworkonwarninglabelsourcecues:
(Greeneetal.2022).Forexample,giventheexperimentused
RQ1:Doesthesourceofawarninglabelchangetrustin
realinformation,bothtrueandfalse,weensuredthatallpar-
falseinformation?
ticipants went through a debriefing, in which information
RQ2:Doesthesourceofawarninglabelchangetheshar-
about the experiment, the veracity of headlines, and an ex-
ingintentionoffalseinformation?
planationofhowveracitywasdeterminedforeachheadline
RQ3: How do differences in media trust, political iden-
wasprovided.Additionalconsiderationsareprovidedinthe
tity,andcognitivereflectionchangewarnereffectiveness?
PaperChecklist.Theauthorsdeclarenocompetinginterests.
Tothisend,weconductafivecondition,between-subjects
experimenttoexaminetheimpactofthesourceofawarning
RelatedWork
labelontheinformationbehaviorsofFacebookandTwitter
users. Specifically, participants in the study scroll through This paper draws from two bodies of literature: 1. the
eightsocialmediaposts,halfofwhicharefalsestories,and quicklygrowingworkononlinecontentlabelingand2.the
indicate how much they trust the information in each post muchbroaderliteratureabouttrustandheuristics.
and how likely they would be to share the post. Partici-
pants are randomly assigned to one of five warning label Online Content Labels Online content labels are attach-
conditions: (1) all posts have no warning labels (control), mentstocontent-socialmediaposts,newsarticles,videos
(2)falsepostshavewarninglabelsfromothersocialmedia -thatareintendedtoaddinformationorcontexttothatcon-
users (crowd), (3) false posts have warning labels from the tent(Morrowetal.2022).Theselabelsfallintototwobroad
social media platform, (4) false posts have warning labels categories: 1. context labels and 2. veracity labels. Contex-
from fact-checkers, and (5) false posts have warning labels tuallabelsprovideadditionalinformationaboutthecontent,
fromanAItool.Examplesofpostsineachconditioncanbe often relating to the source of the content. For example,
foundinFigure1. YouTube’s source funding label, which labels videos from
Fromthisexperiment,wefindthatallwarninglabelssig- government funded outlets (Arnold, Reckendorf, and Win-
nificantly decrease trust in false information relative to the tersieck 2021). Veracity labels, the focus of this study, are
controlcondition,butwarningsfromAIaremodestlymore more direct than context labels. They are labels that warn
effective overall. AI had the largest effect relative to con- consumersofapieceofinformation’scorrectnessorreliabil-
trol, followed by warnings from fact-checkers, the crowd, ity.Forexample,FacebookandTwitterhavelabeledcontent
and the platform. These impacts are moderated by trust in withthird-partyfact-checks(MartelandRand2023b).
media and the information itself. For instance, participants Of the experimental evidence produced so far, both con-
whodistrustmediaorganizationsarelessaffectedbywarn- textual and veracity labels impact information behaviors
inglabelsfromthesocialmediaplatform,othersocialmedia (Morrowetal.2022).Inparticular,veracitywarninglabels
users, and fact checkers than participants who trust media seemtobegenerallyeffectiveatreducingbothbeliefinand(a)Platform (b)Crowd (c)AI (d)FactCheckers(FC)
Figure1:Examplefalsepostfromeachwarninglabelcondition.BoththeCrowdandPlatformconditionsmatchtherespective
population(FacebookorTwitter).Notethateachparticipantalsosawfourtruepostswithnolabelsineachthecondition.
engagement with false information (Pennycook and Rand introduction, one such heuristic is trust in the source infor-
2018; Horne et al. 2019, 2020; Mena 2020; Clayton et al. mation (Lewandowsky et al. 2012; Kim and Dennis 2019).
2020; Brashier et al. 2021; Epstein et al. 2022). However, Rather than deeply processing various facets of the infor-
as argued by Martel and Rand (2023b) and Morrow et al. mation’s truth-status, we may ask “do I trust the source of
(2022), the contexts and settings in which these findings this information?”. There are many other types of mental
holddeservefurtherstudy. shortcuts used in information selection and trust decisions,
One such setting change is the source of the veracity la- such as utilizing the writing style of information, perceiv-
bels. As briefly discussed in the introduction to this paper, ing social consensus about the information, trusting infor-
a few recent studies have directly or indirectly studied the mation because it forms a coherent story, trusting informa-
relative efficacy of warning label sources (Seo, Xiong, and tion shared by significant others, and trusting information
Lee 2019; Yaqub et al. 2020; Jia et al. 2022), each varying that aligns with prior beliefs or knowledge (Lewandowsky
in effectiveness metrics and results. The majority of these etal.2012;Anspach2017).
worksprovideevidencethattrustinthewarnerdoesmatter Veracity warning labels add another layer to informa-
tosomedegree,buttherelativeefficacyhasvaried.Notably, tion trust decisions. While processing the information it-
themajorityofthesestudieshavefocusedonspecifictopical self,whetherviaashortcutordeepinformationprocessing,
contexts:politicalinformationandCOVID-19information. likelystillplaysapartininformationbehaviorswhenwarn-
In our work, we want to broaden these results by studying inglabelsareattached,thosesameshortcutsmayalsoplaya
warning label source cues across metrics and topics (our roleinwarninglabelacceptance.Inthisstudy,wefocuson
headline selection is similar to that done by Yaqub et al. testing the influence of one particular heuristic on warning
(2020)). Further, our experiment adds an additional source label acceptance: trust in the source of the warning label.
notstudiedinthesepriorworks:thesocialmediaplatform. However, there are other shortcuts that may be used when
Wechoosetoaddthissourceasmanywarninglabelexperi- choosing to comply with a label. For example, one’s prior
mentsusegenericwarnings,wherethesourceisnotalways experience with warning labels may influence their future
clear. However, if those warnings were displayed on a spe- acceptanceofwarninglabels(likealgorithmaversionwhen
cificplatform,consumersmayassumetheplatformitselfis usersseeasystemmakeamistake(Dietvorst,Simmons,and
thesourceofthewarning.Theworkdoneinthispaperseeks Massey 2015)). Or social consensus about a warning label
to add both clarity to these prior results and add evidence systemmayimpacttrustinwarninglabelsproducedbythat
fromamoregenerictopicalcontext. system,muchlikehowsocialconsensuscaninfluencetrust
inindividualclaims.Inthecontextofwarninglabels,these
InformationTrustandHeuristics Animportantcompo- shortcuts have rarely been studied. This work begins to fill
nent of labeling content is understanding if labels will be this gap in our understanding of mental shortcuts relation-
acceptedandwhylabelsareorarenotaccepted.Thefactors shiptoawarninglabel’seffectiveness.
that influence label acceptance stem from a much broader
literature on information trust and heuristics. From this lit- Methods
erature,weknowthattrustininformationcanbeevaluated
ExperimentalSetup
alongbothaffectiveandcognitivedimensions(Adalı2013).
Trustmaychangeacrossfacetsoftheinformationbeingcon- Werecruited2,215U.S.adultswhoregularlyuseTwitteror
sumed,suchasthefamiliarityofinformationortheplatform Facebook on the survey platform Prolific. Specifically, we
informationisconsumedon. useProlifictorecruitparticipantswhoanswered‘Facebook’
Further, heuristics used in information trust decisions or ‘Twitter’ to the following question: “Which of the fol-
have been well-studied. When heuristics are used in lowing social media sites do you use on a regular basis (at
decision-making,wereplaceacomplex,difficult-to-answer leastonceamonth)?”ThisquestionwasapartofProlific’s
questionwithasimpler,easier-to-answerquestion(asubsti- various pre-screening questions to filter study populations.
tution (Kahneman 2011)). For instance, as discussed in the AccordingtoProlific,23,727peoplewereeligibletopartic-ipate in our experiment who regularly used Facebook and pants took a pre-survey to capture additional measures and
18,874peoplewereeligibletoparticipateinourexperiment demographics. The demographics captured included age,
whoregularlyusedTwitter.Whileitispossiblethatapartic- gender, race, education, and political leaning. Five addi-
ipantusedbothFacebookandTwitteratleastonceamonth, tional questions were asked to measure cognitive reflec-
weensurethatthetwosamplesaremutuallyexclusive.We tion (CRT), empirically validated by Frederick (2005) and
choosetosamplefromthesetwosetsofsocialmediausers Thomson and Oppenheimer (2016). Six more questions
instead of a general U.S. population for two reasons: 1. As wereaskedtocaptureparticipantsperceptionsoftheability,
discussedbelow,twoofourexperimentalconditionsrequire benevolence, and integrity (ABI) of news media organiza-
a platform to be mentioned, and 2. Prior work has shown tions and social media platforms (three questions for news
evidence that labels have different effectiveness across dif- media and three questions for social media). These ques-
ferentplatforms.Inparticular,labelsonTwitterwerefound tions were adapted from the model of trustworthiness pro-
tobemoreeffectivethanlabelsonFacebook(Arnold,Reck- posed by Mayer, Davis, and Schoorman (1995). A similar
endorf,andWintersieck2021;Morrowetal.2022). setofmeasuresforcognitivereflectionandperceivedtrust-
Using these two samples of social media users, we de- worthinesswerecapturedinapriorwarninglabelstudyby
ployedapilotsurveyfor15participants(acrossbothFace- Epsteinetal.(2022).Thesurveyquestionscanbefoundin
book and Twitter populations) to measure if our estimated thesupplementalmaterialforthepaper.
surveytimeandpaymentamountwerecorrect.Therewere Finally, after the discernment task, participants were
no issues found during the pilot, so the same payment pa- askedtwofactualmanipulationcheckstoensurethewarning
rameters and questions were used in two more batches of labelswereseen.Thentheparticipantsweredebriefedabout
survey deployment (one for Facebook users and one for whattheysawandwhatinformationwastrueandfalse.
Twitter users). All three batches were ran across the 18th
and19thofMay2023.Participantswerepaid$3forsurvey DataSampleandDemographics
completion.Acrossthesethreebatches166didn’tfinishthe
survey, leaving us with 2,049 total participants for analysis
(1,026fromFacebookand1,023fromTwitter).
All participants in the study scrolled through eight
randomly-selectedsocialmediaposts,halfofwhicharefalse
stories.Foreachpost,participantsindicatedhowmuchthey
trusttheinformation(5-pointscalefromTrustCompletelyto
DistrustCompletely)andhowlikelytheywouldbetoshare
theinformation(5-pointscalefromExtremelyLikelytoEx-
tremely Unlikely). Participants were also asked an open-
ended question to assess reasoning (“Is there a particular
reasonyoutrust(ordistrust)theinformationinthispost?”).
Intotal,16fact-checkedpostswererandomlyselectedfrom
recentheadlines/socialmediapostsfact-checkedbySnopes,
PolitiFact, FactCheck.org, or Reuters Fact Check. We only
includerecentlyfactcheckedheadlinestoensuretheinfor- Figure2:Correlationofdemographicvariablesacrossboth
mationisfreshandrelativelynovel(NevoandHorne2022). platforms.Detailsareinthesupplementalmaterials.
The source and image associated with each headline/post
were included in the experiment (as shown in Figure 1). ThesampleacrossboththeFacebookandTwitterpopula-
Each participant only saw a random selection of half of tionsskewedtowardsbeingmoreliberal,younger,andcol-
these,balancedbetweentrueandfalse.Intotal,thisgaveus legeeducated,withtheTwitterpopulationskewingslightly
16,392post-leveldatapoints,8,196forfalsepostsand8,196 more liberal and younger than the Facebook population.
fortrueposts.Importantly,theinformationcouldcomefrom Namely, among the Facebook users, 196 identified as very
any topic and was not limited to a particular type of false liberal, 324 as liberal, 290 as moderate, 162 as conserva-
information(i.e.,political,health,etc.).Theheadlinesused tive,and 53 as very conservative. Among the Twitter users,
canbefoundinthesupplementalmaterialforthepaper. 252identifiedasveryliberal,336asliberal,232asmoder-
Each participant was randomly assigned to one of five ate,154asconservative,and49asveryconservative.
conditions: (1) All posts have no warning labels (control), Using the six ABI questions described above, we com-
(2) False posts have warning labels from the social media pressed answers into an ABI score for news media and an
platform (Facebook or Twitter), (3) False posts have warn- ABIscoreforsocialmediaperparticipant.Specifically,for
inglabelsfromothersocialmediausers,(4)Falsepostshave each question, if a participant strongly disagreed with the
warning labels from an AI tool, and (5) False posts have prompt,wesubtracted2points,somewhatdisagreedwesub-
warning labels from fact-checkers. We choose these four tracted 1 point, neither agreed nor disagreed add 0 points,
warners because they all have been proposed in the litera- somewhat agreed we added 1 point, and strongly agreed
ture or used in real life systems (Epstein et al. 2022; Allen we added 2 points. This creates a score between −6 and
etal.2021;Morrowetal.2022;Pennycooketal.2020). 6, where −6 is the strongest distrust and 6 is the strongest
Prior to being randomly assigned to a condition, partici- trust. According to this score, participants skewed towardsdistrustingnewsmediaorganizationsandsocialmediaplat- thesummariesfortwomultilevelregressionmodels,onefor
forms.Themedianscorefornewsmediaorganizationswas trustinfalseinformationandanotherforthesharinginten-
−1 (µ = −1.45, σ = 2.56) for Facebook users and −1 tion of false information. In Figure 3, we show the effect
(µ=−1.48,σ =2.60)forTwitterusers.Socialmediaplat- sizesateachlevel(participantandpost)relativetocontrol.
formsweredistrustedevenmore,withthemedianscorefor
socialmediabeing−2(µ=−2.08,σ =2.38)forFacebook
usersand−2(µ=−2.08,σ =2.48)forTwitterusers.
Similar to our compression for the ABI questions, we
compressed the CRT questions into one score per partici-
pant. Specifically, for each question, if the participant an-
swered correctly, we add 1 point to their CRT score. This
processcreatedascorebetween0and5,where0isthelow-
est cognitive reflection and 5 is the highest. Overall, par-
ticipants skewed towards high cognitive reflection, with a
median CRT score of 4 (µ = 3.41, σ = 1.53) for Face-
book users and 4 (µ = 3.56, σ = 1.53) for Twitter users.
Distributions of each demographic variable across the two (a)Trust (b)SharingIntention
populationscanbefoundinthesupplementalmaterial.Cor-
relationsamongthesevariablescanbefoundinFigure2. Figure 3: Cohen’s d effect sizes relative to control across
conditions, where participant-level are effects between
DataAnalysis participant-level scores (-4 to 4) and post-level are effects
betweenpost-levelscores(-1to1).
Given the hierarchical structure of our data (repeated mea-
suresacrossindividuals),ourprimaryanalysisisdoneusing
mixedeffectsregression,alsoknownasamultilevelmodel.
TrustinFalseInformation
Specifically,wefittwomodels:onetopredicttrustinfalse
information and another model to predict sharing intention Groups Stat p-value AdjP 95%CI
of false information. Each model has two levels, where the Control/FC 0.121 0.000*** 0.000 [0.09,0.16]
firstlevelisthepostandthesecondlevelistheparticipant, Control/AI 0.144 0.000*** 0.000 [0.11,0.18]
therebyclusteringtherepeatedmeasureoftrustinpostsand Control/Platform 0.100 0.000*** 0.000 [0.06,0.14]
Control/Crowd 0.111 0.000*** 0.000 [0.08,0.15]
sharing of posts by participants. The dependent variable in
FC/AI 0.023 0.170 0.845 [-0.01,0.06]
eachmodelisapost-level score-anumberbetween
FC/Crowd -0.010 0.538 1.000 [-0.04,0.02]
−1(distrustornotshare)and+1(trustorshare)whichmaps FC/Platform -0.021 0.228 0.925 [-0.01,0.06]
directlyontothe5-pointLikertscaleusedinthesurveyfor AI/Crowd -0.033 0.043* 0.356 [-0.07,-0.01]
eachpost.Theindependentvariablesineachmodelinclude AI/Platform -0.044 0.010* 0.096 [0.01,0.08]
Crowd/Platform -0.011 0.538 1.000 [-0.02,0.05]
theconditionrelativetocontrol,theplatformgroupapartic-
ipantisin,andparticipant-leveldemographics. SharingofFalseInformation
In addition to the post-level trust and sharing Control/FC 0.070 0.000*** 0.000 [0.03,0.11]
score used in the multilevel models, we compute a Control/AI 0.064 0.001** 0.010 [0.03,0.10]
participant-level score for trust in and sharing Control/Platform 0.069 0.000*** 0.000 [0.03,0.11]
Control/Crowd 0.044 0.021* 0.191 [0.01,0.08]
intention of false posts. The participant-level score is a
FC/AI -0.006 0.743 1.000 [-0.04,0.03]
numberbetween−4and+4,where−4iscompletedistrust FC/Crowd -0.026 0.151 0.805 [-0.06,0.01]
in (or extremely unlikely to share) all false posts and FC/Platform -0.001 0.947 1.000 [-0.04,0.03]
+4 is complete trust in (or extremely likely to share) all AI/Crowd -0.020 0.273 0.959 [-0.06,0.02]
AI/Platform 0.005 0.794 1.000 [-0.03,0.04]
false posts. For example, a person’s trust score in false
Crowd/Platform 0.024 0.173 0.850 [-0.01,0.06]
information is computed as follows: for each false post,
add 1 if they answered ‘Trust Completely’, add 0.5 if
Table 1: Results of Tukey’s HSD Tests of post-level scores
they answered ‘Somewhat Trust’, add 0 if they answered
betweenconditiongroups.Significancecodesareshownfor
‘Neither Trust nor Distrust’, subtract 0.5 if they answered
p: *** p < 0.001, ** p < 0.01, * p < 0.05. Groups are
‘Somewhat Distrust’, and subtract 1 if they answered
in bold font if they are significant before adjusting p. Adj
‘CompletelyDistrust’.Thisscoreignoresdifferencesacross
pistheSidak-Bonferronifamily-wisecorrectionforeachp
individualpostsandinsteadcapturesthehigherlevelshifts
value, where α = 0.05. This correction suggests that the
in participant-level trust and sharing. We do not use this
difference between AI/Crowd and AI/Platform may not be
scoreformodeling,onlyforfurtherdescribingthedata.
significant.NotethatweshortenFactCheckerstoFC.
Results
Allwarninglabelsdecreasedtrustinfalseinformation
In Table 1, weshow the pairwise differences between each As both analysis at the post-level and participant-level in-
conditiongroupaccordingtoTukey’sHSDTestatthepost- dicate, all warning labels, no matter the source or platform
levelwithSidak-Bonferronicorrections.InTable2,weshow population,significantlydecreasedtrustinfalseinformationTrustinFalseInformation(Model1) SharingofFalseInformation(Model2)
Coef. Std.Err. P>|z| 95%CI Coef. Std.Err. P>|z| 95%CI
Intercept -0.261 0.037 0.000*** [-0.333,-0.189] -0.548 0.038 0.000*** [-0.622,-0.474]
AI -0.160 0.024 0.000*** [-0.207,-0.113] -0.068 0.025 0.006** [-0.117,-0.019]
Crowd -0.117 0.024 0.000*** [-0.165,-0.070] -0.045 0.025 0.071 [-0.095,0.004]
FC -0.127 0.024 0.000*** [-0.174,-0.081] -0.072 0.025 0.004** [-0.121,-0.023]
Platform -0.108 0.024 0.000*** [-0.155,-0.061] -0.070 0.025 0.005** [-0.118,-0.021]
PlatformGroup 0.009 0.015 0.578 [-0.022,0.039] -0.000 0.016 0.979 [-0.032,0.031]
Age -0.009 0.006 0.137 [-0.021,0.003] -0.009 0.006 0.170 [-0.021,0.004]
Education -0.015 0.006 0.007** [-0.026,-0.004] -0.002 0.006 0.692 [-0.014,0.009]
PoliticalLeaning -0.048 0.007 0.000*** [-0.061,-0.034] -0.019 0.007 0.011* [-0.033,-0.004]
CRT -0.025 0.005 0.000*** [-0.035,-0.015] -0.008 0.005 0.133 [-0.018,0.002]
ABINews -0.013 0.003 0.000*** [-0.020,-0.007] -0.001 0.004 0.800 [-0.008,0.006]
ABISocial 0.022 0.004 0.000*** [0.014,0.029] 0.010 0.004 0.014* [0.002,0.017]
ParticipantVar 0.076 0.010 - - 0.080 0.011 - -
Table 2: Summaries from two multilevel, random intercept linear models, one for trust in false information and another for
sharingintentionsoffalseinformation(statsmodelPython3package,version0.14.0).Thedependentvariableisthepost-level
trustscore,where−1iscompletelydistrust(extremelyunlikelytoshare)and+1iscompletelytrust(extremelylikelytoshare).
Eachmodelhastwolevels,whereleveloneisarethepostsandleveltwoaretheparticipants.Conditionsareinreferencetothe
controlconditionwherenolabelsarepresentedtotheparticipants.InterclassCorrelationCoefficient(ICC)formodel1is0.462
and for model 2 is 0.427, indicating a moderate level of correlation between both trust and share scores of false information
withinaparticipant’srepeatedmeasures.Theaveragetrustscorecanvaryby±0.276acrossparticipantsandtheaverageshare
scorecanvaryby±0.283acrossparticipants.Significancecodesare:***p<0.001,**p<0.01,*p<0.05.
relative to the control condition (p < 0.001 for all four tention (although they were found to be significant by the
warning label conditions in model 1). These effects, while Tukey’sHSDtestatthepost-levelinTable1priortoSidak-
significant, were relatively small. As shown in Figure 3a, Bonferroni corrections). Of the labels that did impact shar-
thelargesteffectsizeacrossparticipant-levelscoredistribu- ing,theeffectsagainweresmall(Figure3b).Warningsfrom
tionswas0.425betweencontrolandAIandthesmallestwas AIandfactcheckerswerethemosteffectiveintermsofthe
0.272betweencontrolandPlatform.Similar,butsmaller,ef- effectsizerelativetocontrol;however,nowarnerwasshown
fectsizeswerefoundatthepost-level.Atbothlevels,AIhad tobesignificantlymoreeffectivethananyotheraccordingto
the largest effect relative to control, followed by warnings Tukey’sHSDtest.
fromfact-checkers,thecrowd,andtheplatform. Importantly, while the shifts in sharing intention were
Whenexaminingthedifferencesbetweeneachwarner,we small, the baseline sharing intention is already quite low.
again see that the label from AI was modestly more effec- The average participant-level sharing score in the control
tive. In particular, when changing the condition reference conditionwas−3.26,wherethelowestascorecouldbewas
inmodel1,thedifferencebetweenwarningsfromtheplat- −4. Hence, by default, most participants were reluctant to
form and warnings from AI emerged as significantly dif- share information. This reflects prior work on sharing fake
ferent (p < 0.05), but with a very small effect size (0.122 news (Altay, Hacquin, and Mercier 2022). Unlike other in-
at the participant-level). This result is partially reflected by terventionstudies,wedidnotfilteroutparticipantswhodid
Tukey’s HSD test in Table 1, where labels from AI were notshareregularlyonsocialmedia(Pennycooketal.2021).
found to be significantly more effective than labels from Prior trust in media organizations moderated the ef-
boththecrowdandtheplatform(p<0.05).However,when fectiveness of warnings. As shown in model 1 of Ta-
adjustingthep-valuestocorrectforfamily-wiseerrorusing ble 2, participant’s prior perceptions of the ability, benev-
Sidak-Bonferroni, the differences between AI and the plat- olence,andintegrityofnewsorganizationsandsocialmedia
formandAIandthecrowdbecomenon-significant.Hence, platforms significantly impacted trust in false information.
we should emphasize that while labels from AI were very Namely, those who had higher trust in news organizations
modestlymoreeffectiveintermsofeffectsize,theywerenot hadlower trustinfalseinformation(p < 0.001),andthose
robustlymoreeffective.Allothersourcesshowednosignif- whohadhighertrustinsocialmediahadhighertrustinfalse
icantdifferencesfromeachother,onlyfromcontrol. information(p < 0.001).Sharingintentionswererelatedto
Warnings from other social media users were not ro- trust in social media platforms but not news organizations.
bustlyeffectiveatdeterringsharingoffalseinformation. Participantswhotrustedsocialmediamore,reportedhigher
While all warning labels were significantly effective at de- sharingintentionoffalseinformation(p<0.05).
creasing trust in false information, not all were effective in By examining the interactions between prior media trust
decreasingsharingoffalseinformation.Accordingtomodel andeachcondition,wecanexploretheseeffectsfurther.In
2inTable2,warningsfromtheplatform,factcheckers,and Table3,weshowtheresultsofthreeOrdinaryLeastSquares
AI significantly decreased sharing intentions for false in- (OLS) regressions on participant-level trust. The goal of
formation (p < 0.01 for all three labels). However, warn- these simpler models was to isolate the interaction effects
ings from the crowd did not significantly shift sharing in- fromparticipants’priormediatrustandpoliticalleaningon(a)ABINewsModel coef stderr P>|t|
Intercept -2.2324 0.075 0.000***
Platform -0.5106 0.106 0.000***
Crowd -0.4950 0.108 0.000***
AI -0.6006 0.105 0.000***
FC -0.5615 0.106 0.000***
ABINews 0.0388 0.024 0.101
ABINewsXPlatform -0.1096 0.033 0.001**
ABINewsXCrowd -0.0753 0.034 0.028*
ABINewsXAI -0.0097 0.035 0.779
ABINewsXFC -0.0852 0.033 0.011*
(a)ABINewsXPlatform (b)ABISocialXPlatform
(b)ABISocialModel coef stderr P>|t|
Intercept -1.9779 0.086 0.000***
Platform -0.7001 0.122 0.000***
Crowd -0.6289 0.122 0.000***
AI -0.7752 0.118 0.000***
FC -0.6881 0.121 0.000***
ABISocial 0.1522 0.026 0.000***
ABISocialXPlatform -0.1598 0.037 0.000***
ABISocialXCrowd -0.1082 0.038 0.004**
ABISocialXAI -0.0810 0.037 0.027*
ABISocialXFC -0.1165 0.036 0.001**
(c)PoliticalLeaningModel coef stderr P>|t|
(c)ABINewsXCrowd (d)ABISocialXCrowd
Intercept -1.5995 0.178 0.000***
Platform -0.3443 0.241 0.154
Crowd -0.6702 0.251 0.008**
AI -0.8879 0.245 0.000***
FC -0.4881 0.243 0.045*
PoliticalLeaning -0.2620 0.064 0.000***
PoliticalLeaningXPlatform -0.0299 0.087 0.732
PoliticalLeaningXCrowd 0.0970 0.091 0.284
PoliticalLeaningXAI 0.1100 0.089 0.218
PoliticalLeaningXFC 0.0085 0.087 0.922
Table 3: OLS regressions predicting the participant-level
(e)ABINewsXAI (f)ABISocialXAI
trust with interaction terms for (a) ABINews, (b) ABISo-
cial, and (c) PoliticalLeaning. Significance codes are: ***
p<0.001,**p<0.01,*p<0.05.
eachcondition(Wediscusspoliticalleaninginthenextsec-
tion). In Figure 4, we show the interactions of participant-
level trust across prior media trust subsets, where the high
ABI group (for news or social media) contained partici-
pantswhoscoredzerooraboveaccordingtoourcompressed
ABIscores.ThelowABIgroupcontainedparticipantswho (g)ABINewsXFC (h)ABISocialXFC
scoredbelowzero.
Within these participant subsets, warners differ in effec- Figure 4: Interaction plots of participant-level trust across
tiveness.Forinstance,participantswhotrustednewsorgani- high and low ABINews and ABISocial subgroups. All in-
zations less, trusted warning labels from social media plat- teractionsweresignificantexceptABINewsXAI.Note,the
formsless(Figure4a).InTable3a,theinteractionbetween y-axesareatdifferentscales.
prior trust in news organizations and warning labels from
the platform were significant (p < 0.01). Furthermore, ac-
cording to both a multilevel model and Tukey’s HSD Test and each condition was less than half of the effect size be-
on this subset, warning labels from the platform did not tweencontrolandtheAIcondition(0.213forFactCheckers
significantly shift trust in false information for those who and 0.198 for Crowd versus 0.471 for AI). In fact, for par-
didnottrustnewsorganizations.Thissubsetofparticipants ticipants who trusted news organizations less, warning la-
alsotrustedwarningsfromfactcheckersandthecrowdless belsfromAIweresignificantlymoreeffectivethanallother
(see Figure 4c, Figure 4g, and Table 3a). While there still warning labels (p < 0.01 for all pairs according to a mul-
wasasignificantshifttowardsdistrustingfalseinformation tilevelmodelonthissubset).Ontheotherhand,forpartici-
when labels from fact checkers or from the crowd were pantswhotrustednewsorganizationsmore,thispatternwas
showncomparedtocontrol,theeffectsizebetweencontrol not found. Warnings from AI were no more effective thanwarninglabelsfromanyothersourceandallwarninglabels
significantlyshiftedtrustrelativetocontrol.
Perhaps expected, prior trust in social media platforms
significantly moderated the effectiveness of warnings from
theplatform(p < 0.001inTable3).Infact,warninglabels
fromtheplatformdidnotsignificantlyshifttrustrelativeto
control for participants who trusted social media platforms
less(p=0.078accordingtoTukey’sHSD).Warningsfrom
(a)Trust-Conservatives (b)Share-Conservatives
the crowd and fact checkers were also significantly mod-
erated by prior trust in social media platforms. Saliently,
participantswhotrustedsocialmediaplatformsless,trusted
warning labels from AI modestly more than other warning
labels (p < 0.05 at post-level Tukey’s HSD), while labels
from AI were no more effective than other labels for those
whotrustedsocialmediaplatformsmore.
A similar pattern was true across both prior media trust
metricsforsharingintentions.Priortrustinnewsorganiza-
(c)Trust-Moderates (d)Share-Moderates
tions significantly interacted with warning labels from the
platform,withthosewhotrustednewsorganizationslessbe-
inglessimpactedbythoselabels.Priortrustinsocialmedia
platforms significantly moderated the effect of all warning
labels on sharing intentions, with those who trusted social
mediaplatformslessbeinglessimpactedbyalllabels.Due
to space restrictions, we show the interaction analysis for
sharingintentionsinthesupplementalmaterials.
Political identity did not robustly moderate the effec-
(e)Trust-Liberals (f)Share-Liberals
tiveness of warning labels. Despite the information pre-
sentednotalwaysbeingpoliticalintopic,self-reportedpo-
Figure5:Participant-leveltrustscoresacrossconditionsand
litical leaning still influenced trust in false information. As
self-reported political leaning. Conservatives includes par-
shown in Table 2, the more liberal a participant claimed to
ticipantswhoclaimedtobeeitherconservativeorverycon-
be, the less they trusted in false information (p < 0.001).
servative (a). Moderates includes participants who claimed
This result may be in part due to the imbalance between
to be moderate (b). Liberals includes participants who
conservative and liberals in the study population; however,
claimed to be either liberal or very liberal (c). In (d), we
ingeneral,thisfindingalignswiththeresultsofpriorstudies
showthedistributionsacrossallparticipants.
(Dobbsetal.2023).
In Figure 5, we show the distributions of participant-
level trust in false information across three groups of par-
ticipants:conservatives,moderates,andliberals.Fromthese Basedonthelargebodyofworkoninformationprocessing
sub-groups,wefoundsomeevidencethatconservativepar- and more recent work on warning labels, this role should
ticipantswerelessaffectedbywarninglabelsfromtheplat- be expected (Panizza et al. 2023). As indicated in Table 2
form.Namely,theeffectsizebetweencontrolandPlatform model1,participantswhoscoredhigheronthecognitivere-
conditionforconservativeparticipantswaslessthanhalfof flectiontest(measuringtendencytoengageinfurtherreflec-
theeffectsizeformoderatesandliberals(0.187forconser- tion)andweremoreeducated,trustedfalseinformationless
vatives, 0.457 for moderates, and 0.462 for liberals). Fur- (p < 0.001andp < 0.007,respectively).Thesetwofactors
ther, according to Tukey’s HSD, conservative participants suggestthattheinformationitselfcontinuestobeimportant
wereimpactedbywarningsfromAIsignificantlymorethan in trust decisions through deep information processing, in-
warningsfromtheplatform(p=0.038atthepost-levelafter formation familiarity, and topical expertise. Although, no-
Sidak-Bonferronicorrection),whilewarningsfromAIwere tablythesesamefactors(cognitivereflectionandeducation)
notsignificantlydifferentthananyotherwarningsourcefor didnotsignificantimpactsharingintentions.
liberal participants. However, in our interaction analysis at Tofurthershowtheroleofinformationintrust,wecom-
the participant-level, political leaning did not significantly puted a Chi-squared Test of Independence between each
play a role in the effectiveness of differing warning labels post and the frequency of trust on the 5-point Likert scale.
(see Table 3c). Across all political subgroups, all warning Acrossallconditions,wefoundasignificantrelationshipbe-
sourceswerestillsignificantlybetterthancontrol.Thesepat- tweenpostsandthefrequencyoftrust(allwithp < 0.001).
ternsheldtruewhenaskingaboutsharingintentions(Inter- When examining this frequency of trust across posts, each
actionanalysiscanbefoundinthesupplementalmaterial). posthadadifferentbaselinetrustacrossthepopulation,and
Informationprocessingstillplayedanimportantrole. thattrustshiftedtowardsdistrustwhenwarninglabelswere
Whiletherewasaclearimpactofwarninglabelsontrustin attached.Eventhoughthechangingskewacrossconditions
falseinformation,theinformationitselfwasstillimportant. wasclear,individualpoststhatweremoretrustedinthecon-trolconditioncontinuedtobemoretrustedinthewarningla- the control condition and the warning labels from the plat-
belconditions.However,therelationshipbetweenpostsand formweremorethandoubleinsizeforliberalsandmoder-
sharingintentionsdidnothavesignificantarelationship,fur- ates compared to conservatives. However, political leaning
ther suggesting that information processing occurred more did not significantly interact with the effectiveness of any
intrustdecisionsthaninsharingdecisions.Detailsaboutthe warner.Perhapsmoreimportantly,allwarnerssignificantly
headlinesusedcanbefoundinthesupplementalmaterial. decreasedtrustinfalseinformationcomparedtocontrol,no
matteraparticipantspoliticalleaning.
FactualManipulationTest For those who distrust news organizations, AI may be
seenasaneutralparty,particularlycomparedtosourcesthat
Afterthemaintrust task,weaskedparticipantsifthey saw
involveotherhumans(i.e.,crowd,fact-checkers,platform).
anywarninglabelsduringthesurveyandifso,whichhead-
This explanation is supported by the open-ended reasoning
lines. This manipulation test is done to ensure our inter-
fromourexperiment.Multipleparticipantsexplicitlystated
vention was correctly received by the participants. Overall,
trustinginAImorethanothersources.Forexample,onepar-
approximately 90% of participants in each warning label
ticipantsaid:“IhonestlytrustanAImorethanmainstream
condition said they saw warning labels (91% in No Label,
media”.OthersstatedthattheytrustinAIasanewsverac-
89% in Platform, 92% in AI, and 91% in Fact Checkers).
ity tool. For example, one participant said: “I trust that AI
When digging deeper into the headline/label recall, on av-
isagoodatconfirmingthevalidityofastory”andanother
erageparticipantsacrossallconditionsrecalled60%ofthe
stated: “I think AI would be great at determining if its fake
headlines with attached warning labels correctly (σ 0.425,
or not”. When ranking the most frequent bigrams used in
x˜ 0.75). These results are slightly better than but still re-
the open-ended reasoning across conditions, we found that
flectthefactualmanipulationresultsfoundinEpsteinetal.
“the AI” was the most mentioned bigram in the AI condi-
(2022), where many users did not recognize that they were
tion (mentioned 418 times), while in the other conditions,
presented with warning labels. In our case, the vast major-
themostmentionedbigramsweremoregenericanddidnot
ity of participants recognized warning labels, but recalling
mention the source of the warning, such as ”the warning”
what headlines were paired with warning labels soon after
(mentioned 453 times across all warning label conditions).
thetaskwasdonelesswell.
These results directly reflect the broader findings of prior
workontheperceptionsofAIinnews,whichdemonstrated
Discussion
thatindividualswithlowermediatrustandpoliticallyright-
In this study, we examined the impact of warning label leaning individuals had more positive views on AI’s use in
sourcesonthetrustinandsharingintentionoffalseinforma- news(Araujoetal.2023).
tion. For the most part, the results of this work support the Implications:Theresultsofthisworkhaveimplications
notion that warning labels are generally effective at chang- for both methods and practice. First, choosing a metric of
inginformationbehaviors(MartelandRand2023b).Inour intervention effectiveness is important when designing ex-
study, we found that warnings from AI had the largest ef- perimentalstudies.Wefoundthattrustinfalseinformation
fect on trust in and sharing intentions of false information, andsharingintentionsoffalseinformationwereonlyweakly
followedbywarningsfromfactcheckers.Thisefficacyrank- correlated with each other and that baseline sharing inten-
ingdiffersfrompriorwork,whichfoundthatwarningsfrom tionswereverydifferentthanbaselinetrustininformation.
fact checkers were most effective and warnings from AI To fully capture the effects of warning label interventions,
were least effective (Yaqub et al. 2020). Notably, this prior or any content moderation intervention, studies should use
work focused only on sharing intentions, and the impacts multipletargetmetrics.
onsharingintentionsacrosswarnersinourstudywerevery Second,wefoundthattheinformationitselfplayedasig-
small. These small differences mean that who the warner nificantroleintrustdecisions,althoughnotinsharinginten-
was didn’t seem to change sharing intentions much. How- tions.Thisresultagainsuggeststhattheinterventionmetric
ever, warning source did weakly impact trust in our study. matters, but it also suggest that information selection mat-
Specifically, warnings from AI decreased trust in false in- ters. As with other warning label studies, the hope is that
formation compared not only to the control condition, but withalargeenoughheadlinesample,theimpactofindivid-
alsocomparedtowarningsfromthecrowdandtheplatform. ual pieces of information on the larger pictures results will
Still,thedifferencesineffectivenessacrosswarners,nomat- benegligible.However,futureworkshouldexploretherela-
terthemetric,weresmall. tionshipbetweeninformationandinterventions.Fromprior
This efficacy difference between warnings from AI and research, we know that political congruence with informa-
warnings from other sources did not hold across all par- tion may matter in intervention effectiveness. For instance,
ticipants. Warnings from AI were no more effective than Berinsky(2017)foundthatifrumorsaredebunkedbyanun-
anyotherwarninglabelforparticipantswhoreportedahigh likelypartisansource,theyweremoreeffective(i.e.,aCEO
trust in news organizations. On the other hand, warning la- of a fossil fuel company correcting misinformation about
belsfromAIweresignificantlymoreeffectivethanallother climatechange(Morrowetal.2022)).Wealsoknowmany
warning labels for participants who reported a low trust things about information processing and trust, as described
in news organizations. We also found weak evidence that intheRelatedWorksectionofthispaper.However,contin-
self-reportedpoliticalleaningimpactedtheeffectivenessof ued exploration of how these factors interact with warning
warners.Inparticular,wefoundthattheeffectsizebetween labelsiscriticaltofullyunderstandbestdesignpractices.Third,thisworkaddstothealreadygrowingevidencethat These results should be thought of as the impact of warn-
AI-produced content labels can be effective (Horne et al. erswhenconsumptionisatleastsomewhatactive.Whilewe
2019; Epstein et al. 2022). The implications of this result maybeabletodoslightlybetteratsimulatingpassivebrows-
are mixed. Given that warnings from AI were significantly ingbybuildingdynamicsandboxenvironments,theselimi-
effective across media trust groups and political groups in tationswill,atleasttosomeextent,stillexist,asparticipants
ourstudy,AImaybeaperceivedasatrustedneutralsource knowtheyareinastudyenvironment.Betterunderstanding
for veracity information. However, this result should also therelativeknowledgegainsfromconstructingmorerealis-
point to the continue need to ensure when warning labels tictestingenvironmentsisanavenueforfutureresearch.
are used, they are used correctly. This caution is especially Thisexperiment, alongwith the majority of experiments
strong when using automated methods to determine when currently done in this space, should be considered some-
warninglabelsshouldbeattached,astheyarepronetoerror where between ‘Stage 1’ and ‘Stage 2’ of intervention de-
(Bozarth and Budak 2020; Horne, Nevo, and Smith 2023). velopment(asdescribedintheNIHStageModelforBehav-
If warnings from AI are broadly effective but incorrect, in- ioralInterventionDevelopment(NIH2022)).Thismeanswe
formation consumers may be harmed. Future work should are between refining existing intervention designs and test-
continue to establish the consequences of using automated ingtheefficacyofthosedesignsinresearchsettings(where
content moderation, both when those methods are correct our key limitations exist). The next stage (‘Stage 3’) for
andwhentheyarewrong(Horne,Nevo,andSmith2023). intervention work is efficacy testing with real-world plat-
forms. Unfortunately, this stage of research is often left
LimitationsandFutureWork to the platforms themselves, and occasionally select aca-
First,itisimportanttorecognizethattheseresultsmayeven- demics. Hence, it may not always be possible for research
tuallychange.Memoryandattention,bothcollectivelyand to get to this stage. Although, we would argue that having
individually, change over time (Wertsch and Roediger III somebarriertoentrytorunexperimentsonrealplatformsis
2008; Garimella et al. 2017). For example, trust in Twit- importanttoensureharmsarenotcausedbypoorlydesigned
terhaschangedasownershiphaschanged(Schulmanetal. andpoorlydebriefedexperiments.Fortunately,recentinter-
2023).Likewise,onecanreasonthatasAIapplicationsim- vention research that has made it to Stage 3 suggests that
proveandpeopleexperienceAIinmoreareasoftheirlives, the results from lab experiments (like the one in this pa-
theymaygaintrustinotherAItools.Orconversely,aspeo- per) may hold in real-life systems. In a working paper by
ple experience AI making mistakes in more areas of their Linetal.(2024),accuracypromptadvertisementswerede-
lives,theymaylosetrustinotherAItools.Similarly,while ployed to 33M Facebook users and 75K Twitter users. The
there is currently high distrust in fact checkers by the U.S. results from this at-scale experiment showed that accuracy
political right (Walker and Gottfried 2019), as the environ- messages reduce misinformation sharing, aligning with the
mentaroundfactcheckingandpoliticschanges,sotoomay resultspreviouslyfoundinresearchsettings.
trustinthembyparticulargroups(althoughassuggestedby
Conclusion
boththisstudyandothers,warningsfromfactcheckersstill
benefit those who distrust them (Martel and Rand 2023a)). In this work, we sought to better understand the impact of
Future work should continue to examine the relationships content warning label sources on the effectiveness of those
betweenwarningefficacyandconsumer’spriorexperiences. labels,bothintermsoftrustinandsharingintentionsoffalse
Further, future work should focus on longitudinal studies information.Usingafive-conditionbetween-subjectsexper-
giventheevolvingnatureinformationenvironments. iment,wefoundthat,comparedtohavingnowarninglabels,
Second,itmaybepossiblethatourmetricquestionsinflu- all warnings significantly shifted trust in false information
enced each other. That is, we asked each participant about but not all warnings significantly shifted sharing intentions
their trust in the information before asking about if they of false information. Still, with few exceptions, this work
would share the information. Given that accuracy prompts supports the view that content warning label interventions
influence sharing rates (Pennycook et al. 2021; Lin et al. are generally effective. Most notably, we found that warn-
2024), it may be possible that asking consumers to think ingsfromAIwereslightlymoreeffectiveoverallthanother
about their trust in information decreases their intention to warners. That is, warnings from AI were effective across
share that information. Future work can test this potential bothmetrics(trustandsharing),acrosspriortrustattitudes,
limitationthroughseparated,butcomparable,studiesofthe andacrossparticipantpoliticalleaning.Whilepriortrustin
twoconcepts. newsorganizationsmoderatedtheeffectivenessofwarnings
Third,akeylimitationwithanycontrolledexperimentis from the platform, the crowd, and fact checkers, it did not
the inability to perfectly simulate passive information con- significantlymoderatewarningsfromAI.Overall,wethink
sumption as one may do when scrolling through a news this key result is both promising and concerning. While on
feed.Whileourstudyensuredparticipantswerenotprimed onehand,ourresultsshowthepotentialusefulnessandneu-
to actively consume warning labels, this limitation still ex- tralityofAIinnewsveracitytasks,italsoimpliestheneed
ists. Further, given the simulated setting, there are no so- for highly accurate AI tools for news veracity tasks, given
cial consequences for sharing or not sharing information theirpowerinchanginginformationbehaviors.Together,we
(Yaqub et al. 2020). These active and passive consumption hopetheresultsofthisworkinformboththedesignofcon-
differencesmaypartiallychangetheeffectivenessofwarn- tentlabelsonreal-lifesystemsandfuturestudiesofveracity-
inglabelsandtheefficacydifferencesfromvaryingwarners. basedinformationinterventions.References Dobbs,M.;DeGutis,J.;Morales,J.;Joseph,K.;andSwire-
Thompson, B. 2023. Democrats are better than Republi-
Adalı,S.2013.Modelingtrustcontextinnetworks.Springer
cans at discerning true and false news but do not have bet-
Briefs.
termetacognitiveawareness. CommunicationsPsychology,
Allen, J.; Arechar, A. A.; Pennycook, G.; and Rand, D. G.
1(1):46.
2021.Scalingupfact-checkingusingthewisdomofcrowds.
Ecker,U.K.;Lewandowsky,S.;Cook,J.;Schmid,P.;Fazio,
Scienceadvances,7(36):eabf4393.
L.K.;Brashier,N.;Kendeou,P.;Vraga,E.K.;andAmazeen,
Altay,S.;Hacquin,A.-S.;andMercier,H.2022. Whydoso
M. A. 2022. The psychological drivers of misinformation
few people share fake news? It hurts their reputation. new beliefanditsresistancetocorrection. NatureReviewsPsy-
media&society,24(6):1303–1324.
chology,1(1):13–29.
Anspach, N. M. 2017. The new personal influence: How Epstein, Z.; Foppiani, N.; Hilgard, S.; Sharma, S.; Glass-
ourFacebookfriendsinfluencethenewsweread. Political man, E.; and Rand, D. 2022. Do explanations increase the
communication,34(4):590–606. effectivenessofAI-crowdgeneratedfakenewswarnings?In
Araujo, T.; Brosius, A.; Goldberg, A. C.; Mo¨ller, J.; and Proceedings of the International AAAI Conference on Web
deVreese,C.2023.Humansvs.AI:TheRoleofTrust,Polit- andSocialMedia,volume16,183–193.
icalAttitudes,andIndividualCharacteristicsonPerceptions Frederick,S.2005. Cognitivereflectionanddecisionmak-
About Automated Decision Making Across Europe. Inter- ing. JournalofEconomicperspectives,19(4):25–42.
nationalJournalofCommunication,17:28. Garimella, K.; De Francisci Morales, G.; Gionis, A.; and
Arnold,J.R.;Reckendorf,A.;andWintersieck,A.L.2021. Mathioudakis, M. 2017. The effect of collective attention
Source alerts can reduce the harms of foreign disinforma- oncontroversialdebatesonsocialmedia. InProceedingsof
tion. HarvardKennedySchoolMisinformationReview. the2017ACMonWebScienceConference,43–52.
Berinsky, A. J. 2017. Rumors and health care reform: Ex- Go,E.;Jung,E.H.;andWu,M.2014. Theeffectsofsource
perimentsinpoliticalmisinformation. Britishjournalofpo- cues on online news perception. Computers in Human Be-
liticalscience,47(2):241–262. havior,38:358–367.
Greene,C.M.;deSaintLaurent,C.;Murphy,G.;Prike,T.;
Bozarth, L.; and Budak, C. 2020. Toward a better perfor-
Hegarty,K.;andEcker,U.K.2022. BestPracticesforEth-
manceevaluationframeworkforfakenewsclassification. In
ical Conduct of Misinformation Research. European Psy-
Proceedings of the international AAAI conference on web
chologist.
andsocialmedia,volume14,60–71.
Hocevar, K. P.; Metzger, M.; and Flanagin, A. J. 2017.
Brashier,N.M.;andMarsh,E.J.2020. Judgingtruth. An-
Source credibility, expertise, and trust in health and risk
nualreviewofpsychology,71:499–515.
messaging. In Oxford Research Encyclopedia of Commu-
Brashier,N.M.;Pennycook,G.;Berinsky,A.J.;andRand, nication.
D. G. 2021. Timing matters when correcting fake news.
Horne, B. D.; Nevo, D.; Adali, S.; Manikonda, L.; and Ar-
Proceedings of the National Academy of Sciences, 118(5):
rington, C. 2020. Tailoring heuristics and timing AI inter-
e2020043118.
ventions for supporting news veracity assessments. Com-
Broockman,D.;andKalla,J.2016. Durablyreducingtrans- putersinHumanBehaviorReports,2:100043.
phobia:Afieldexperimentondoor-to-doorcanvassing. Sci-
Horne, B. D.; Nevo, D.; O’Donovan, J.; Cho, J.-H.; and
ence,352(6282):220–224.
Adalı, S. 2019. Rating Reliability and Bias in News Arti-
Buchanan,T.;andBenson,V.2019. Spreadingdisinforma- cles:DoesAIAssistanceHelpEveryone? InProceedingsof
tion on facebook: Do trust in message source, risk propen- theInternationalAAAIConferenceonWebandSocialMe-
sity,orpersonalityaffecttheorganicreachof“fakenews”? dia,volume13,247–256.
Socialmedia+society,5(4):2056305119888654. Horne,B.D.;Nevo,D.;andSmith,S.L.2023. Ethicaland
Chandrasekharan,E.;Jhaver,S.;Bruckman,A.;andGilbert, safetyconsiderationsinautomatedfakenewsdetection. Be-
E. 2022. Quarantined! Examining the effects of a haviour&InformationTechnology,1–22.
community-wide moderation intervention on Reddit. ACM Jia,C.;Boltz,A.;Zhang,A.;Chen,A.;andLee,M.K.2022.
Transactions on Computer-Human Interaction (TOCHI), UnderstandingEffectsofAlgorithmicvs.CommunityLabel
29(4):1–26. on Perceived Accuracy of Hyper-partisan Misinformation.
Clayton, K.; Blair, S.; Busam, J. A.; Forstner, S.; Glance, Proceedings of the ACM on Human-Computer Interaction,
J.;Green,G.;Kawata,A.;Kovvuri,A.;Martin,J.;Morgan, 6(CSCW2):1–27.
E.; et al. 2020. Real solutions for fake news? Measuring Kahneman,D.2011. Thinking,fastandslow. macmillan.
theeffectivenessofgeneralwarningsandfact-checktagsin Kim,A.;andDennis,A.R.2019. Sayswho?Theeffectsof
reducing belief in false stories on social media. Political presentationformatandsourceratingonfakenewsinsocial
behavior,42:1073–1095. media. Misquarterly,43(3):1025–1039.
Dietvorst, B. J.; Simmons, J. P.; and Massey, C. 2015. Al- Kim,A.;Moravec,P.L.;andDennis,A.R.2019.Combating
gorithmaversion:Peopleerroneouslyavoidalgorithmsafter fakenewsonsocialmediawithsourceratings:Theeffectsof
seeingthemerr. JournalofExperimentalPsychology:Gen- userandexpertreputationratings. JournalofManagement
eral,144(1):114. InformationSystems,36(3):931–968.Lazer, D.; Ruck, D. J.; Quintana, A.; Shugars, S.; Joseph, Pennycook, G.; Bear, A.; Collins, E. T.; and Rand, D. G.
K.;Grinberg,N.;Gallagher,R.J.;Horgan,L.;Gitomer,A.; 2020. Theimpliedtrutheffect:Attachingwarningstoasub-
Bajak,A.;Baum,M.;Ognyanova,K.;Qu,H.;Hobbs,W.R.; set of fake news headlines increases perceived accuracy of
McCabe,S.;andGreen,J.2021.TheCOVIDStatesProject# headlines without warnings. Management Science, 66(11):
18:FakeNewsonTwitter. Availableat:https://osf.io/vzb9t. 4944–4957.
Lees, J.; McCarter, A.; and Sarno, D. M. 2022. Twitter’s Pennycook, G.; Epstein, Z.; Mosleh, M.; Arechar, A. A.;
disputed tags may be ineffective at reducing belief in fake Eckles,D.;andRand,D.G.2021.Shiftingattentiontoaccu-
news and only reduce intentions to share fake news among racycanreducemisinformationonline. Nature,592(7855):
Democrats and Independents. Journal of Online Trust and 590–595.
Safety,1(3). Pennycook, G.; and Rand, D. G. 2018. Crowdsourcing
Lewandowsky, S.; Ecker, U. K.; Seifert, C. M.; Schwarz, Judgments of News Source Quality. Available at: short-
N.; and Cook, J. 2012. Misinformation and its correction: url.at/MNQ27.
Continuedinfluenceandsuccessfuldebiasing. Psychologi- Pennycook, G.; and Rand, D. G. 2021. The psychology of
calscienceinthepublicinterest,13(3):106–131. fakenews. Trendsincognitivesciences,25(5):388–402.
Lin, H.; Garro, H.; Wernerfelt, N.; Shore, J.; Hughes, A.; Schulman,J.;Qu,H.;Lazer,D.;Perlis,R.;Ognyanova,K.;
Deisenroth, D.; Barr, N.; Berinsky, A.; Eckles, D.; Penny- Baum, M.; Cadenasso, S.; Druckman, J.; Green, J.; Quin-
cook, G.; et al. 2024. Reducing misinformation sharing at tana,A.;etal.2023.TheCOVIDStatesProject#97:Twitter,
scaleusingdigitalaccuracypromptads. SocialMedia,andElonMusk.
Ling,C.;Gummadi,K.P.;andZannettou,S.2023. ”Learn Seo, H.; Xiong, A.; and Lee, D. 2019. Trust it or not: Ef-
the Facts About COVID-19”: Analyzing the Use of Warn- fects of machine-learning warnings in helping individuals
ing Labels on TikTok Videos. In Proceedings of the Inter- mitigate misinformation. In Proceedings of the 10th ACM
national AAAI Conference on Web and Social Media, vol- ConferenceonWebScience,265–274.
ume17,554–565. Su¨lflow,M.;Scha¨fer,S.;andWinter,S.2019. Selectiveat-
Lu,Z.;Li,P.;Wang,W.;andYin,M.2022. TheEffectsof tentioninthenewsfeed:Aneye-trackingstudyontheper-
AI-basedCredibilityIndicatorsontheDetectionandSpread ception and selection of political news posts on Facebook.
of Misinformation under Social Influence. Proceedings of newmedia&society,21(1):168–190.
the ACM on Human-Computer Interaction, 6(CSCW2): 1– Swire-Thompson, B.; DeGutis, J.; and Lazer, D. 2020.
27. Searching for the backfire effect: Measurement and design
Martel,C.;andRand,D.2023a.Fact-checkerwarninglabels considerations. Journalofappliedresearchinmemoryand
areeffectiveevenforthosewhodistrustfact-checkers. cognition,9(3):286–299.
Martel,C.;andRand,D.G.2023b.Misinformationwarning Thomson,K.S.;andOppenheimer,D.M.2016. Investigat-
labelsarewidelyeffective:Areviewofwarningeffectsand ingan alternateform ofthecognitive reflectiontest. Judg-
their moderating features. Current Opinion in Psychology, mentandDecisionmaking,11(1):99–113.
101710. Trujillo, M.; Gruppi, M.; Buntain, C.; and Horne, B. D.
Mayer,R.C.;Davis,J.H.;andSchoorman,F.D.1995. An 2020. WhatisBitChute?Characterizingthe”FreeSpeech”
integrativemodeloforganizationaltrust. Academyofman- Alternative to YouTube. In Proceedings of the 31st ACM
agementreview,20(3):709–734. Conference on Hypertext and Social Media, HT ’20, 139–
Mena, P. 2020. Cleaning up social media: The effect of 140.NewYork,NY,USA:AssociationforComputingMa-
warninglabelsonlikelihoodofsharingfalsenewsonFace- chinery. ISBN9781450370981.
book. Policy&internet,12(2):165–183. Walker, M.; and Gottfried, J. 2019. Republicans far more
Morrow,G.;Swire-Thompson,B.;Polny,J.M.;Kopec,M.; likelythanDemocratstosayfact-checkerstendtofavorone
and Wihbey, J. P. 2022. The emerging science of con- side.
tent labeling: Contextualizing social media content moder- Wertsch, J. V.; and Roediger III, H. L. 2008. Collec-
ation. Journal of the Association for Information Science tive memory: Conceptual foundations and theoretical ap-
andTechnology,73(10):1365–1386. proaches. Memory,16(3):318–326.
Mosseri,A.2016.Buildingabetternewsfeedforyou.Face- Winter, S.; and Kra¨mer, N. C. 2014. A question of
bookNewsroom,29:2016. credibility–Effectsofsourcecuesandrecommendationson
Nevo,D.;andHorne,B.D.2022.Howtopicnoveltyimpacts informationselectiononnewssitesandblogs. Communica-
theeffectivenessofnewsveracityinterventions. Communi- tions,39(4):435–456.
cationsoftheACM,65(2):68–75. Yaqub, W.; Kakhidze, O.; Brockman, M. L.; Memon, N.;
NIH. 2022. NIH stage model for behavioral interven- andPatil,S.2020. Effectsofcredibilityindicatorsonsocial
tion development. https://www.nia.nih.gov/research/dbsr/ media news sharing intent. In Proceedings of the 2020 chi
nih-stage-model-behavioral-intervention-development. conferenceonhumanfactorsincomputingsystems,1–14.
Panizza, F.; Ronzani, P.; Morisseau, T.; Mattavelli, S.; and Zannettou, S. 2021. “I Won the Election!”: An Empirical
Martini, C. 2023. How do online users respond to crowd- Analysis of Soft Moderation Interventions on Twitter. In
sourced fact-checking? Humanities and Social Sciences Proceedings of the International AAAI Conference on Web
Communications,10(1):1–11. andSocialMedia,volume15,865–876.PaperChecklist additional information and data in our supplemental
materialstoensurethattheworkisreproducible.
1. Formostauthors...
(i) Have you read the ethics review guidelines and en-
(a) Would answering this research question advance sci-
suredthatyourpaperconformstothem?Yes.Wehave
encewithoutviolatingsocialcontracts,suchasviolat-
carefullyreviewedtheethicsreviewguidelinesanden-
ingprivacynorms,perpetuatingunfairprofiling,exac-
suredthatthispaperconformstothem.
erbatingthesocio-economicdivide,orimplyingdisre-
specttosocietiesorcultures?Yes.Theresearchques- 2. Additionally,ifyourstudyinvolveshypothesestesting...
tionsansweredinthispaperadvanceresearchincon-
(a) Did you clearly state the assumptions underlying all
tentmoderationandlabeling,withoutviolatingsocial
theoreticalresults?NA
contracts.AlldatacollectedthroughProlificwasdone
(b) Haveyouprovidedjustificationsforalltheoreticalre-
soanonymouslywithIRBapprovalandwithfairpay.
sults?NA
Further, given the use of real, fact-checked informa-
(c) Didyoudiscusscompetinghypothesesortheoriesthat
tion,weensurethatallparticipantsaredebriefedabout
might challenge or complement your theoretical re-
theinformation’sveracity.
sults?NA
(b) Do your main claims in the abstract and introduction
(d) Haveyouconsideredalternativemechanismsorexpla-
accuratelyreflectthepaper’scontributionsandscope?
nationsthatmightaccountforthesameoutcomesob-
Yes,theclaimsmadethroughoutthepaperhavebeen
servedinyourstudy?NA
carefullyreviewedandcontextualized.Weensurethat
the claims made in the abstract and the introduction (e) Didyouaddresspotentialbiasesorlimitationsinyour
accuratelyreflecttheresultsofthepaper. theoreticalframework?NA
(c) Do you clarify how the proposed methodological ap- (f) Haveyourelatedyourtheoreticalresultstotheexisting
proach is appropriate for the claims made? Yes, we literatureinsocialscience?NA
haveworkedhardtoensureeachmethodologicalstep (g) Did you discuss the implications of your theoretical
is justified and appropriate for the claims made. The results for policy, practice, or further research in the
details of our methodological approach can be found socialsciencedomain?NA
intheSectionentitled“Methods”.
3. Additionally,ifyouareincludingtheoreticalproofs...
(d) Do you clarify what are possible artifacts in the data
used,givenpopulation-specificdistributions?Yes,we (a) Didyoustatethefullsetofassumptionsofalltheoret-
clarify where the imbalance in the population sam- icalresults?NA
plemayimpacttheresultsofouranalysis.Wediscuss (b) Didyouincludecompleteproofsofalltheoreticalre-
thepopulationsampleinthesubsectionentitled“Data sults?NA
Sample and Demographics”. In addition, we provide
4. Additionally,ifyouranmachinelearningexperiments...
detailsaboutalldemographicdistributionscapturedin
thestudyinoursupplementalmaterialsdocument. (a) Did you include the code, data, and instructions
neededtoreproducethemainexperimentalresults(ei-
(e) Did you describe the limitations of your work? Yes.
therinthesupplementalmaterialorasaURL)?While
We ensure that our work is contextualized by its lim-
we did not run machine learning experiments, we do
itations. Our discussion of the key limitations of the
document the statistical model implementation used
workcanbefoundinSection“LimitationsandFuture
to analyze the data in the capture of Table 2. Further
Work”.
weprovidedetailsofourmodelchoiceintheSection
(f) Did you discuss any potential negative societal im-
“Methods”.
pactsofyourwork?Yes.Inparticular,wediscusshow
(b) Didyouspecifyallthetrainingdetails(e.g.,datasplits,
our results have both positive and negative potential
hyperparameters,howtheywerechosen)?NA
implications for the use of warning labels in real-life
systems.Moregenerally,wedonotcreateanyartifacts (c) Didyoureporterrorbars(e.g.,withrespecttotheran-
thatcanbeusedoutsideofthiswork. dom seed after running experiments multiple times)?
NA
(g) Did you discuss any potential misuse of your work?
No.Wedonotcurateorreleaseanyresources,data,or (d) Did you include the total amount of compute and the
softwarethathavepotentialformisuse. type of resources used (e.g., type of GPUs, internal
cluster,orcloudprovider)?NA
(h) Didyoudescribestepstakentopreventormitigatepo-
tentialnegativeoutcomesoftheresearch,suchasdata (e) Do you justify how the proposed evaluation is suffi-
and model documentation, data anonymization, re- cient and appropriate to the claims made? Yes. We
sponsiblerelease,accesscontrol,andthereproducibil- carefully thought through and justified the choice of
ity of findings? Yes. We use best practices for both statisticalmodelbasedonthestructureofthedatacol-
human experiments and for misinformation research, lected.ThisjustificationisdescribedintheSectionen-
includinganonymizationanddebriefing.Asdescribed titled“Methods”.
intheSection“Methods”,wedescribeeachsteptaken (f) Doyoudiscusswhatis“thecost“ofmisclassification
in this work to ensure it is reproducible. Further, we andfault(in)tolerance?NA5. Additionally,ifyouareusingexistingassets(e.g.,code,
data, models) or curating/releasing new assets, without
compromisinganonymity...
(a) Ifyourworkusesexistingassets,didyoucitethecre-
ators?NA
(b) Didyoumentionthelicenseoftheassets?NA
(c) Did you include any new assets in the supplemental
materialorasaURL?NA
(d) Did you discuss whether and how consent was ob-
tainedfrompeoplewhosedatayou’reusing/curating?
NA
(e) Did you discuss whether the data you are us-
ing/curating contains personally identifiable informa-
tionoroffensivecontent?NA
(f) Ifyouarecuratingorreleasingnewdatasets,didyou
discuss how you intend to make your datasets FAIR?
NA
(g) Ifyouarecuratingorreleasingnewdatasets,didyou
createaDatasheetfortheDataset?NA
6. Additionally, if you used crowdsourcing or conducted
research with human subjects, without compromising
anonymity...
(a) Did you include the full text of instructions given to
participants and screenshots? Yes. We have provided
details about how the experiment was conducted (in
Section “Methods”) and we provided screenshots of
whatparticipantssawduringthestudy.Inaddition,we
providethefullsurveyinthesupplementalmaterials.
(b) Did you describe any potential participant risks, with
mentions of Institutional Review Board (IRB) ap-
provals? Yes. The experiment was IRB approved and
participants were provided with a consent form that
outlinesrisksandbenefitsofparticipatinginthestudy.
(c) Did you include the estimated hourly wage paid to
participants and the total amount spent on participant
compensation? Yes. The hourly wage paid to partici-
pants and the estimated time of completion was pro-
vided to participants prior to the study. Further, par-
ticipants were free to stop the study at any point and
were paid for completing at least 50% of the survey.
Thispaymentamountisdescribedinthepaper.
(d) Didyoudiscusshowdataisstored,shared,anddeiden-
tified? Yes. All data was collected and stored anony-
mously.Participantsweretoldthisinadvanced.