[
    {
        "title": "Modeling Urban Transport Choices: Incorporating Sociocultural Aspects",
        "authors": "Kathleen Salazar-SernaLorena CadavidCarlos J. Franco",
        "links": "http://arxiv.org/abs/2407.21307v1",
        "entry_id": "http://arxiv.org/abs/2407.21307v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21307v1",
        "summary": "This paper introduces an agent-based simulation model aimed at understanding\nurban commuters mode choices and evaluating the impacts of transport policies\nto promote sustainable mobility. Crafted for developing countries, where\nutilitarian travel heavily relies on motorcycles, the model integrates\nsociocultural factors that influence transport behavior. Multinomial models and\ninferential statistics applied to survey data from Cali, Colombia, inform the\nmodel, revealing significant influences of sociodemographic factors and travel\nattributes on mode choice. Findings highlight the importance of cost, time,\nsafety, comfort, and personal security, with disparities across socioeconomic\ngroups. Policy simulations demonstrate positive responses to interventions like\nfree public transportation, increased bus frequency, and enhanced security, yet\nwith modest shifts in mode choice. Multifaceted policy approaches are deemed\nmore effective, addressing diverse user preferences. Outputs can be extended to\ncities with similar sociocultural characteristics and transport dynamics. The\nmethodology applied in this work can be replicated for other territories.",
        "updated": "2024-07-31 03:19:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21307v1"
    },
    {
        "title": "Decentralized and Uncoordinated Learning of Stable Matchings: A Game-Theoretic Approach",
        "authors": "S. Rasoul EtesamiR. Srikant",
        "links": "http://arxiv.org/abs/2407.21294v1",
        "entry_id": "http://arxiv.org/abs/2407.21294v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21294v1",
        "summary": "We consider the problem of learning stable matchings in a fully decentralized\nand uncoordinated manner. In this problem, there are $n$ men and $n$ women,\neach having preference over the other side. It is assumed that women know their\npreferences over men, but men are not aware of their preferences over women,\nand they only learn them if they propose and successfully get matched to women.\nA matching is called stable if no man and woman prefer each other over their\ncurrent matches. When all the preferences are known a priori, the celebrated\nDeferred-Acceptance algorithm proposed by Gale and Shapley provides a\ndecentralized and uncoordinated algorithm to obtain a stable matching. However,\nwhen the preferences are unknown, developing such an algorithm faces major\nchallenges due to a lack of coordination. We achieve this goal by making a\nconnection between stable matchings and learning Nash equilibria (NE) in\nnoncooperative games. First, we provide a complete information game formulation\nfor the stable matching problem with known preferences such that its set of\npure NE coincides with the set of stable matchings, while its mixed NE can be\nrounded in a decentralized manner to a stable matching. Relying on such a\ngame-theoretic formulation, we show that for hierarchical markets, adopting the\nexponential weight (EXP) learning algorithm for the stable matching game\nachieves logarithmic regret with polynomial dependence on the number of\nplayers, thus answering a question posed in previous literature. Moreover, we\nshow that the same EXP learning algorithm converges locally and exponentially\nfast to a stable matching in general matching markets. We complement this\nresult by introducing another decentralized and uncoordinated learning\nalgorithm that globally converges to a stable matching with arbitrarily high\nprobability, leveraging the weak acyclicity property of the stable matching\ngame.",
        "updated": "2024-07-31 02:36:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21294v1"
    },
    {
        "title": "Non-Bayesian Social Learning with Multiview Observations",
        "authors": "Dongyan SuiWeichen CaoStefan VlaskiChun GuanSiyang Leng",
        "links": "http://arxiv.org/abs/2407.20770v1",
        "entry_id": "http://arxiv.org/abs/2407.20770v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20770v1",
        "summary": "Non-Bayesian social learning enables multiple agents to conduct networked\nsignal and information processing through observing environmental signals and\ninformation aggregating. Traditional non-Bayesian social learning models only\nconsider single signals, limiting their applications in scenarios where\nmultiple viewpoints of information are available. In this work, we exploit, in\nthe information aggregation step, the independently learned results from\nobservations taken from multiple viewpoints and propose a novel non-Bayesian\nsocial learning model for scenarios with multiview observations. We prove the\nconvergence of the model under traditional assumptions and provide convergence\nconditions for the algorithm in the presence of misleading signals. Through\ntheoretical analyses and numerical experiments, we validate the strong\nreliability and robustness of the proposed algorithm, showcasing its potential\nfor real-world applications.",
        "updated": "2024-07-30 12:16:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20770v1"
    },
    {
        "title": "Architectural Influence on Variational Quantum Circuits in Multi-Agent Reinforcement Learning: Evolutionary Strategies for Optimization",
        "authors": "Michael KölleKarola SchneiderSabrina EggerFelix ToppThomy PhanPhilipp AltmannJonas NüßleinClaudia Linnhoff-Popien",
        "links": "http://arxiv.org/abs/2407.20739v1",
        "entry_id": "http://arxiv.org/abs/2407.20739v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20739v1",
        "summary": "In recent years, Multi-Agent Reinforcement Learning (MARL) has found\napplication in numerous areas of science and industry, such as autonomous\ndriving, telecommunications, and global health. Nevertheless, MARL suffers\nfrom, for instance, an exponential growth of dimensions. Inherent properties of\nquantum mechanics help to overcome these limitations, e.g., by significantly\nreducing the number of trainable parameters. Previous studies have developed an\napproach that uses gradient-free quantum Reinforcement Learning and\nevolutionary optimization for variational quantum circuits (VQCs) to reduce the\ntrainable parameters and avoid barren plateaus as well as vanishing gradients.\nThis leads to a significantly better performance of VQCs compared to classical\nneural networks with a similar number of trainable parameters and a reduction\nin the number of parameters by more than 97 \\% compared to similarly good\nneural networks. We extend an approach of K\\\"olle et al. by proposing a\nGate-Based, a Layer-Based, and a Prototype-Based concept to mutate and\nrecombine VQCs. Our results show the best performance for mutation-only\nstrategies and the Gate-Based approach. In particular, we observe a\nsignificantly better score, higher total and own collected coins, as well as a\nsuperior own coin rate for the best agent when evaluated in the Coin Game\nenvironment.",
        "updated": "2024-07-30 11:16:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20739v1"
    },
    {
        "title": "Finite-Time Analysis of Asynchronous Multi-Agent TD Learning",
        "authors": "Nicolò Dal FabbroArman AdibiAritra MitraGeorge J. Pappas",
        "links": "http://arxiv.org/abs/2407.20441v1",
        "entry_id": "http://arxiv.org/abs/2407.20441v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20441v1",
        "summary": "Recent research endeavours have theoretically shown the beneficial effect of\ncooperation in multi-agent reinforcement learning (MARL). In a setting\ninvolving $N$ agents, this beneficial effect usually comes in the form of an\n$N$-fold linear convergence speedup, i.e., a reduction - proportional to $N$ -\nin the number of iterations required to reach a certain convergence precision.\nIn this paper, we show for the first time that this speedup property also holds\nfor a MARL framework subject to asynchronous delays in the local agents'\nupdates. In particular, we consider a policy evaluation problem in which\nmultiple agents cooperate to evaluate a common policy by communicating with a\ncentral aggregator. In this setting, we study the finite-time convergence of\n\\texttt{AsyncMATD}, an asynchronous multi-agent temporal difference (TD)\nlearning algorithm in which agents' local TD update directions are subject to\nasynchronous bounded delays. Our main contribution is providing a finite-time\nanalysis of \\texttt{AsyncMATD}, for which we establish a linear convergence\nspeedup while highlighting the effect of time-varying asynchronous delays on\nthe resulting convergence rate.",
        "updated": "2024-07-29 22:36:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20441v1"
    }
]