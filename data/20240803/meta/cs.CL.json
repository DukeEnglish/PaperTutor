[
    {
        "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
        "authors": "Richard RenSteven BasartAdam KhojaAlice GattiLong PhanXuwang YinMantas MazeikaAlexander PanGabriel MukobiRyan H. KimStephen FitzDan Hendrycks",
        "links": "http://arxiv.org/abs/2407.21792v1",
        "entry_id": "http://arxiv.org/abs/2407.21792v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21792v1",
        "summary": "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with upstream model capabilities, potentially\nenabling \"safetywashing\" -- where capability improvements are misrepresented as\nsafety advancements. Based on these findings, we propose an empirical\nfoundation for developing more meaningful safety metrics and define AI safety\nin a machine learning research context as a set of clearly delineated research\ngoals that are empirically separable from generic capabilities advancements. In\ndoing so, we aim to provide a more rigorous framework for AI safety research,\nadvancing the science of safety evaluations and clarifying the path towards\nmeasurable progress.",
        "updated": "2024-07-31 17:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21792v1"
    },
    {
        "title": "Vision-Language Model Based Handwriting Verification",
        "authors": "Mihir ChauhanAbhishek SatbhaiMohammad Abuzar HashemiMir Basheer AliBina RamamurthyMingchen GaoSiwei LyuSargur Srihari",
        "links": "http://arxiv.org/abs/2407.21788v1",
        "entry_id": "http://arxiv.org/abs/2407.21788v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21788v1",
        "summary": "Handwriting Verification is a critical in document forensics. Deep learning\nbased approaches often face skepticism from forensic document examiners due to\ntheir lack of explainability and reliance on extensive training data and\nhandcrafted features. This paper explores using Vision Language Models (VLMs),\nsuch as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By\nleveraging their Visual Question Answering capabilities and 0-shot\nChain-of-Thought (CoT) reasoning, our goal is to provide clear,\nhuman-understandable explanations for model decisions. Our experiments on the\nCEDAR handwriting dataset demonstrate that VLMs offer enhanced\ninterpretability, reduce the need for large training datasets, and adapt better\nto diverse handwriting styles. However, results show that the CNN-based\nResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach\nwith GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:\n71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings\nhighlight the potential of VLMs in generating human-interpretable decisions\nwhile underscoring the need for further advancements to match the performance\nof specialized deep learning models.",
        "updated": "2024-07-31 17:57:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21788v1"
    },
    {
        "title": "The Llama 3 Herd of Models",
        "authors": "Abhimanyu DubeyAbhinav JauhriAbhinav PandeyAbhishek KadianAhmad Al-DahleAiesha LetmanAkhil MathurAlan ScheltenAmy YangAngela FanAnirudh GoyalAnthony HartshornAobo YangArchi MitraArchie SravankumarArtem KorenevArthur HinsvarkArun RaoAston ZhangAurelien RodriguezAusten GregersonAva SpataruBaptiste RoziereBethany BironBinh TangBobbie ChernCharlotte CaucheteuxChaya NayakChloe BiChris MarraChris McConnellChristian KellerChristophe TouretChunyang WuCorinne WongCristian Canton FerrerCyrus NikolaidisDamien AllonsiusDaniel SongDanielle PintzDanny LivshitsDavid EsiobuDhruv ChoudharyDhruv MahajanDiego Garcia-OlanoDiego PerinoDieuwke HupkesEgor LakomkinEhab AlBadawyElina LobanovaEmily DinanEric Michael SmithFilip RadenovicFrank ZhangGabriel SynnaeveGabrielle LeeGeorgia Lewis AndersonGraeme NailGregoire MialonGuan PangGuillem CucurellHailey NguyenHannah KorevaarHu XuHugo TouvronIliyan ZarovImanol Arrieta IbarraIsabel KloumannIshan MisraIvan EvtimovJade CopetJaewon LeeJan GeffertJana VranesJason ParkJay MahadeokarJeet ShahJelmer van der LindeJennifer BillockJenny HongJenya LeeJeremy FuJianfeng ChiJianyu HuangJiawen LiuJie WangJiecao YuJoanna BittonJoe SpisakJongsoo ParkJoseph RoccaJoshua JohnstunJoshua SaxeJunteng JiaKalyan Vasuden AlwalaKartikeya UpasaniKate PlawiakKe LiKenneth HeafieldKevin StoneKhalid El-AriniKrithika IyerKshitiz MalikKuenley ChiuKunal BhallaLauren Rantala-YearyLaurens van der MaatenLawrence ChenLiang TanLiz JenkinsLouis MartinLovish MadaanLubo MaloLukas BlecherLukas LandzaatLuke de OliveiraMadeline MuzziMahesh PasupuletiMannat SinghManohar PaluriMarcin KardasMathew OldhamMathieu RitaMaya PavlovaMelanie KambadurMike LewisMin SiMitesh Kumar SinghMona HassanNaman GoyalNarjes TorabiNikolay BashlykovNikolay BogoychevNiladri ChatterjiOlivier DuchenneOnur ÇelebiPatrick AlrassyPengchuan ZhangPengwei LiPetar VasicPeter WengPrajjwal BhargavaPratik DubalPraveen KrishnanPunit Singh KouraPuxin XuQing HeQingxiao DongRagavan SrinivasanRaj GanapathyRamon CaldererRicardo Silveira CabralRobert StojnicRoberta RaileanuRohit GirdharRohit PatelRomain SauvestreRonnie PolidoroRoshan SumbalyRoss TaylorRuan SilvaRui HouRui WangSaghar HosseiniSahana ChennabasappaSanjay SinghSean BellSeohyun Sonia KimSergey EdunovShaoliang NieSharan NarangSharath RaparthySheng ShenShengye WanShruti BhosaleShun ZhangSimon VandenhendeSoumya BatraSpencer WhitmanSten SootlaStephane CollotSuchin GururanganSydney BorodinskyTamar HermanTara FowlerTarek SheashaThomas GeorgiouThomas ScialomTobias SpeckbacherTodor MihaylovTong XiaoUjjwal KarnVedanuj GoswamiVibhor GuptaVignesh RamanathanViktor KerkezVincent GonguetVirginie DoVish VogetiVladan PetrovicWeiwei ChuWenhan XiongWenyin FuWhitney MeersXavier MartinetXiaodong WangXiaoqing Ellen TanXinfeng XieXuchao JiaXuewei WangYaelle GoldschlagYashesh GaurYasmine BabaeiYi WenYiwen SongYuchen ZhangYue LiYuning MaoZacharie Delpierre CoudertZheng YanZhengxing ChenZoe PapakiposAaditya SinghAaron GrattafioriAbha JainAdam KelseyAdam ShajnfeldAdithya GangidiAdolfo VictoriaAhuva GoldstandAjay MenonAjay SharmaAlex BoesenbergAlex VaughanAlexei BaevskiAllie FeinsteinAmanda KalletAmit SanganiAnam YunusAndrei LupuAndres AlvaradoAndrew CaplesAndrew GuAndrew HoAndrew PoultonAndrew RyanAnkit RamchandaniAnnie FrancoAparajita SarafArkabandhu ChowdhuryAshley GabrielAshwin BharambeAssaf EisenmanAzadeh YazdanBeau JamesBen MaurerBenjamin LeonhardiBernie HuangBeth LoydBeto De PaolaBhargavi ParanjapeBing LiuBo WuBoyu NiBraden HancockBram WastiBrandon SpenceBrani StojkovicBrian GamidoBritt MontalvoCarl ParkerCarly BurtonCatalina MejiaChanghan WangChangkyu KimChao ZhouChester HuChing-Hsiang ChuChris CaiChris TindalChristoph FeichtenhoferDamon CivinDana BeatyDaniel KreymerDaniel LiDanny WyattDavid AdkinsDavid XuDavide TestuggineDelia DavidDevi ParikhDiana LiskovichDidem FossDingkang WangDuc LeDustin HollandEdward DowlingEissa JamilElaine MontgomeryEleonora PresaniEmily HahnEmily WoodErik BrinkmanEsteban ArcauteEvan DunbarEvan SmothersFei SunFelix KreukFeng TianFirat OzgenelFrancesco CaggioniFrancisco GuzmánFrank KanayetFrank SeideGabriela Medina FlorezGabriella SchwarzGada BadeerGeorgia SweeGil HalpernGovind ThattaiGrant HermanGrigory SizovGuangyiZhangGuna LakshminarayananHamid ShojanazeriHan ZouHannah WangHanwen ZhaHaroun HabeebHarrison RudolphHelen SukHenry AspegrenHunter GoldmanIgor MolybogIgor TufanovIrina-Elena VelicheItai GatJake WeissmanJames GeboskiJames KohliJaphet AsherJean-Baptiste GayaJeff MarcusJeff TangJennifer ChanJenny ZhenJeremy ReizensteinJeremy TeboulJessica ZhongJian JinJingyi YangJoe CummingsJon CarvillJon ShepardJonathan McPhieJonathan TorresJosh GinsburgJunjie WangKai WuKam Hou UKaran SaxenaKarthik PrasadKartikay KhandelwalKatayoun ZandKathy MatosichKaushik VeeraraghavanKelly MichelenaKeqian LiKun HuangKunal ChawlaKushal LakhotiaKyle HuangLailin ChenLakshya GargLavender ALeandro SilvaLee BellLei ZhangLiangpeng GuoLicheng YuLiron MoshkovichLuca WehrstedtMadian KhabsaManav AvalaniManish BhattMaria TsimpoukelliMartynas MankusMatan HassonMatthew LennieMatthias ResoMaxim GroshevMaxim NaumovMaya LathiMeghan KeneallyMichael L. SeltzerMichal ValkoMichelle RestrepoMihir PatelMik VyatskovMikayel SamvelyanMike ClarkMike MaceyMike WangMiquel Jubert HermosoMo MetanatMohammad RastegariMunish BansalNandhini SanthanamNatascha ParksNatasha WhiteNavyata BawaNayan SinghalNick EgeboNicolas UsunierNikolay Pavlovich LaptevNing DongNing ZhangNorman ChengOleg ChernoguzOlivia HartOmkar SalpekarOzlem KalinliParkin KentParth ParekhPaul SaabPavan BalajiPedro RittnerPhilip BontragerPierre RouxPiotr DollarPolina ZvyaginaPrashant RatanchandaniPritish YuvrajQian LiangRachad AlaoRachel RodriguezRafi AyubRaghotham MurthyRaghu NayaniRahul MitraRaymond LiRebekkah HoganRobin BatteyRocky WangRohan MaheswariRuss HowesRuty RinottSai Jayesh BonduSamyak DattaSara ChughSara HuntSargun DhillonSasha SidorovSatadru PanSaurabh VermaSeiji YamamotoSharadh RamaswamyShaun LindsayShaun LindsaySheng FengShenghao LinShengxin Cindy ZhaShiva ShankarShuqiang ZhangShuqiang ZhangSinong WangSneha AgarwalSoji SajuyigbeSoumith ChintalaStephanie MaxStephen ChenSteve KehoeSteve SatterfieldSudarshan GovindaprasadSumit GuptaSungmin ChoSunny VirkSuraj SubramanianSy ChoudhurySydney GoldmanTal RemezTamar GlaserTamara BestThilo KohlerThomas RobinsonTianhe LiTianjun ZhangTim MatthewsTimothy ChouTzook ShakedVarun VontimittaVictoria AjayiVictoria MontanezVijai MohanVinay Satish KumarVishal ManglaVlad IonescuVlad PoenaruVlad Tiberiu MihailescuVladimir IvanovWei LiWenchen WangWenwen JiangWes BouazizWill ConstableXiaocheng TangXiaofang WangXiaojian WuXiaolan WangXide XiaXilun WuXinbo GaoYanjun ChenYe HuYe JiaYe QiYenda LiYilin ZhangYing ZhangYossi AdiYoungjin NamYuWangYuchen HaoYundi QianYuzi HeZach RaitZachary DeVitoZef RosnbrickZhaoduo WenZhenyu YangZhiwei Zhao",
        "links": "http://arxiv.org/abs/2407.21783v1",
        "entry_id": "http://arxiv.org/abs/2407.21783v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21783v1",
        "summary": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "updated": "2024-07-31 17:54:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21783v1"
    },
    {
        "title": "ShieldGemma: Generative AI Content Moderation Based on Gemma",
        "authors": "Wenjun ZengYuchi LiuRyan MullinsLudovic PeranJoe FernandezHamza HarkousKarthik NarasimhanDrew ProudPiyush KumarBhaktipriya RadharapuOlivia SturmanOscar Wahltinez",
        "links": "http://arxiv.org/abs/2407.21772v1",
        "entry_id": "http://arxiv.org/abs/2407.21772v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21772v1",
        "summary": "We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers.",
        "updated": "2024-07-31 17:48:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21772v1"
    },
    {
        "title": "Adaptive Retrieval-Augmented Generation for Conversational Systems",
        "authors": "Xi WangProcheta SenRuizhe LiEmine Yilmaz",
        "links": "http://arxiv.org/abs/2407.21712v1",
        "entry_id": "http://arxiv.org/abs/2407.21712v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21712v1",
        "summary": "Despite the success of integrating large language models into the development\nof conversational systems, many studies have shown the effectiveness of\nretrieving and augmenting external knowledge for informative responses. Hence,\nmany existing studies commonly assume the always need for Retrieval Augmented\nGeneration (RAG) in a conversational system without explicit control. This\nraises a research question about such a necessity. In this study, we propose to\ninvestigate the need for each turn of system response to be augmented with\nexternal knowledge. In particular, by leveraging human judgements on the binary\nchoice of adaptive augmentation, we develop RAGate, a gating model, which\nmodels conversation context and relevant inputs to predict if a conversational\nsystem requires RAG for improved responses. We conduct extensive experiments on\ndevising and applying RAGate to conversational models and well-rounded analyses\nof different conversational scenarios. Our experimental results and analysis\nindicate the effective application of RAGate in RAG-based conversational\nsystems in identifying system responses for appropriate RAG with high-quality\nresponses and a high generation confidence. This study also identifies the\ncorrelation between the generation's confidence level and the relevance of the\naugmented knowledge.",
        "updated": "2024-07-31 16:04:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21712v1"
    }
]