Fitting Multilevel Factor Models
Tetiana Parshakova Trevor Hastie Stephen Boyd
September 19, 2024
Abstract
We examine a special case of the multilevel factor model, with covariance given
by multilevel low rank (MLR) matrix [PHDB24]. We develop a novel, fast implemen-
tation of the expectation-maximization (EM) algorithm, tailored for multilevel factor
models, to maximize the likelihood of the observed data. This method accommodates
any hierarchical structure and maintains linear time and storage complexities per it-
eration. This is achieved through a new efficient technique for computing the inverse
of the positive definite MLR matrix. We show that the inverse of an invertible PSD
MLR matrix is also an MLR matrix with the same sparsity in factors, and we use the
recursive Sherman-Morrison-Woodbury matrix identity to obtain the factors of the in-
verse. Additionally, we present an algorithm that computes the Cholesky factorization
of an expanded matrix with linear time and space complexities, yielding the covariance
matrixasitsSchurcomplement. Thispaperisaccompaniedbyanopen-sourcepackage
that implements the proposed methods.
1
4202
peS
81
]LM.tats[
1v76021.9042:viXraContents
1 Introduction 3
1.1 Prior work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Our contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2 Multilevel factor model 5
2.1 Multilevel low rank matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Problem setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3 Fitting 8
3.1 Expectation step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 Maximization step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4 Efficient computation 11
4.1 Inverse of PSD MLR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.1.1 Properties of structured matrices . . . . . . . . . . . . . . . . . . . . 12
4.1.2 Recursive SMW algorithm . . . . . . . . . . . . . . . . . . . . . . . . 13
4.2 EM iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.2.1 Selection matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.2.2 EM iteration computation . . . . . . . . . . . . . . . . . . . . . . . . 15
5 Numerical examples 17
5.1 Asset covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5.2 Synthetic multilevel factor model . . . . . . . . . . . . . . . . . . . . . . . . 18
A Second order approximation of log-likelihood 25
B Heuristic method for variance estimation 26
C Cholesky factorization 27
C.1 Schur complement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
C.2 Recursive Cholesky factorization . . . . . . . . . . . . . . . . . . . . . . . . . 27
C.2.1 Sparsity patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.3 Efficient computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C.4 Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D Product of MLR matrices 32
21 Introduction
Factor models are used to explain the variation in the observed variables through a smaller
number of factors. In fields like biology, economics, and social sciences, the data often has
hierarchical structures. To capture this structure specialized multilevel factor models were
developed. Existing methods for fitting these models do not scale well with large datasets.
In this work, we introduce an efficient algorithm for fitting multilevel factor models. Our
method is compatible with any hierarchical structure and achieves linear time and storage
complexity per iteration.
1.1 Prior work
Factor models. Factor analysis was initially developed to address problems in psycho-
metrics about 120 years ago [Spe04], and it later found applications in psychology, fi-
nance, economics, and statistics. The idea behind factor analysis is to describe variability
among the observed variables using a small number of unobserved variables called factors.
Factor models decompose a covariance matrix into a sum of a low rank matrix, associ-
ated with underlying factors, and a diagonal matrix, representing idiosyncratic variances.
Since the early 20th century, factor analysis has seen significant methodological advance-
ments [Fru54, Cat65, Jo¨r69, FF93, FWMS99], with several books dedicated to its theory
and application [Har76, Chi06].
Hierarchically structured data. Data from fields such as biology, economics, social
sciences, and medical sciences often exhibits a hierarchical, nested, or clustered structure.
This has led to the development of specialized techniques in factor analysis aimed specifically
at handling hierarchically structured data such as hierarchical factor models [SL57, Whe59]
and multilevel factor models [AAH81, MG89].
Hierarchical factor models. In hierarchical factor models, factors are organized into
a hierarchy, where general factors at the top influence more specific factors positioned be-
neath them [SL57, BNW12, YTM99, RB02]. This model type does not necessarily reflect
a hierarchy in the data (e.g., individuals within groups) but rather in the latent variables
themselves. Widely used in psychometrics, these models are crucial for distinguishing be-
tween higher-order and lower-order factors [Car93, McG09]. For instance, [DeY06] identified
a hierarchical structure of personality with two general factors, stability and plasticity, at
the top, and the so-called Big Five personality factors below them: neuroticism, agreeable-
ness, and conscientiousness are under stability, while extraversion and openness are under
plasticity.
Multilevel factor models. Multilevel factor models are statistical frameworks developed
in the 1980s to handle hierarchical data structures; see [AAH81, Gol86, MG89, RH98,
RHSP04a], and the books [DLMG08, Gol11]. These models partition factors into global
3and local components, allowing the decomposition of the variances of observed variables into
components attributable to each level of the hierarchy. There is a wide variety of multilevel
factor models discussed in the literature, with the general form for a 2-level factor model
presented in [Gol11, §8.2].
Multilevel (dynamic) factor models have also been applied to time series data [GH99,
BN02, Wan12, BW15]. They have been particularly effective in modeling the co-movement
of economic quantities across different levels [GH99, BW15]. For example, [KOW03, CKO11,
JS16] used these models to characterize the co-movement of international business cycles on
global, regional, and country levels.
In this paper we focus on a special case of a multilevel factor model, that has no inter-
cept and no linear covariates. Additionally, the observations follow a normal distribution
characterized by a covariance matrix that is a multilevel low rank (MLR) matrix [PHDB24].
In [PHDB24] authors consider two problems beyond fitting, namely, rank allocation and
capturing partition. Here, we assume that both rank allocation and hierarchical partition
are fixed, and focus solely on fitting.
Fitting methods. Several methods have been employed to fit multilevel models, each
with its advantages and challenges. Among the most prominent are maximum likelihood
and Bayesian estimation techniques [DFH+09], and Frobenius norm-based fitting meth-
ods [PHDB24]. Commonly utilized algorithms for these methods include the expectation-
maximization (EM) algorithm [RT82, Rau95], the Newton-Raphson algorithm [LB88], it-
erative generalized least squares [Gol86], the Fisher scoring algorithm, and Markov Chain
Monte Carlo [GB14]. Despite the efficacy of these approaches, no single method proves en-
tirely satisfactory under all possible data conditions encountered in research. As a result,
statisticians are continually developing alternative techniques to enhance model fitting and
accuracy [DFH+09, Lin10].
Software packages. Several commercial packages offer capabilities for handling multilevel
modeling, including LISREL [JS96], Mplus [AM06, MM17, Mut24] and MLwiN [RBG+00].
The open-source packages include lavaan [Ros12, Hua17], gllamm [RHSP04b]. Additional
resources and software recommendations can be found in [DLMG08, §1.7] and [Gol11, §18].
These tools are primarily designed for multilevel linear models [GH07], and most of them do
not support the specific requirements of factor analysis within multilevel frameworks that in-
volve an arbitrary number of levels in hierarchical structures. Although OpenMx [BNM+11,
PHvO+17], an open-source package that implements MLE-based fitting methods, does sup-
port multiple levels of hierarchy, it was unable to handle our large-scale examples. Ad-
ditionally, we found no high-quality, open-source implementations of MCMC-based fitting
methods; thus these were not included in our comparison.
In this paper, leveraging the MLR structure of the covariance matrix, we derive a novel
fast implementation of the EM algorithm for multilevel factor modeling that works with any
hierarchical structure and requires linear time and storage complexities per iteration.
4...
+ + + +
Figure 1: (Contiguous) PSD MLR matrix given as a sum of block diagonal matrices with each
block being low rank. The coefficients of the factors are depicted in green.
1.2 Our contribution
The main contributions of this paper are the following:
1. We present a novel computationally efficient algorithm for fitting multilevel factor
models, which operates with linear time and storage complexities per iteration.
2. WeshowthattheinverseofaninvertiblePSDMLRmatrixisalsoanMLRmatrixwith
the same sparsity in factors, and we use the recursive Sherman-Morrison-Woodbury
matrix identity to obtain the factors of the inverse.
3. We present an algorithm that computes the Cholesky factorization of an expanded
matrix with linear time and space complexities, yielding the covariance matrix as its
Schur complement.
4. We provide an open-source package that implements the fitting method, available at
https://github.com/cvxgrp/multilevel_factor_model.
We also provide several examples that illustrate our method.
2 Multilevel factor model
In this section we review the multilevel low rank (MLR) matrix along with notations neces-
sary for our method. We then present a variant of the multilevel factor model that will be
the focus of this paper.
2.1 Multilevel low rank matrices
An MLR matrix [PHDB24] is a row and column permutation of a sum of matrices, each
one a block diagonal refinement of the previous one, with all blocks low rank, given in the
factored form. We focus on the special case of symmetric positive semidefinite (PSD) MLR
matrices.
5An n n contiguous PSD MLR matrix A with L levels has the form
×
A = A1 + +AL, (1)
···
where Al is a PSD block diagonal matrix,
Al = blkdiag(A ,...,A ), l = 1,...,L,
l,1 l,p
l
where blkdiag is the direct sum of blocks A Rn l,k×n l,k for k = 1,...,p . Here p is the
l,k l l
∈
size of the partition at level l, with p = 1, and
1
p
(cid:88)l
n = n, l = 1,...,L.
l,k
k=1
Throughout this paper we consider L 2 and p = n, therefore AL is a diagonal matrix.
L
≥
The block dimensions on level l partition the n indices into p groups, which are contigu-
l
ous. We require that this partition be hierarchical, which means that the partition at level
l is a refinement of the partition at level l 1, for l = 2,...,L.
−
We require that blocks on level l have rank not exceeding r , given in the factored form
l
as
A = F FT , F Rn l,k×r l, l = 1,...,L 1, k = 1,...,p ,
l,k l,k l,k l,k ∈ − l
and refer to F as the factor (of block k on level l). Also define a diagonal matrix A =
l,k L,k
D 0 for k = 1,...,n, i.e., r = 1. See figure 1. We refer to r = r + +r +1 as
kk L 1 L−1
≥ ···
the MLR-rank of A. The MLR-rank of A is in general not the same as the rank of A. We
refer to (r ,...,r ,1) as the rank allocation.
1 L−1
Factor form. For each level l = 1,...,L 1 define
−
F = blkdiag(F ,...,F ) Rn×p lr l.
l l,1 l,p
l ∈
Then we have
Al = F FT, l = 1,...,L 1.
l l −
Define
F = (cid:2) F F (cid:3) Rn×s,
1 L−1
··· ∈
with s =
(cid:80)L−1p
r . Then we can write A as
l=1 l l
A =
(cid:2)
F D1/2
(cid:3)(cid:2)
F D1/2
(cid:3)T
= FFT +D,
where F has s columns, and a very specific sparsity structure, with column blocks that are
block diagonal, and D is diagonal, see figure 2.
6...
Figure 2: (Contiguous) PSD MLR matrix given as a product of two sparse structured matrices.
The coefficients of the factors are depicted in green.
... ...
Figure 3: (Contiguous) PSD MLR matrix given in compressed form.
7
...Compressed factor form. We can also arrange the factors into one dense matrix with
dimensions n r. We vertically stack the factors at each level to form matrices
×
 
F
l,1
F¯l =  . . .  Rn×r l, l = 1,...,L 1,
 
∈ −
F
l,p
l
and lastly a diagonal of matrix D, diag(D) Rn. We horizontally stack these matrices to
∈
obtain one matrix
F¯ = (cid:2) F¯1 F¯L−1 (cid:3) Rn×(r−1).
··· ∈
All of the coefficients in the factors of a contiguous MLR matrix are contained in this matrix
and vector diag(D), see figure 3. To fully specify a contiguous MLR matrix, we need to give
the block dimension n for l = 1,...,L, k = 1,...,p , and the ranks r ,...,r .
l,k l 1 L
PSD MLR matrix. We reviewed the contiguous PSD MLR matrix. PSD MLR matrix is
given by the symmetric permutation of rows and columns of a contiguous PSD MLR matrix.
Therefore, the PSD MLR matrix uses a general hierarchical partition of the index set.
2.2 Problem setting
We consider a multilevel factor model,
y = Fz +e, (2)
where F Rn×s is structured factor loading matrix, z Rs are factor scores, with z
∈ ∈ ∼
(0,I ), and e Rn are the idiosyncratic terms, with e (0,D).
s
N ∈ ∼ N
We assume that the n features can be hierarchically partitioned, with specific factors
explaining the correlations within each group of this hierarchical partition. This can be
modeled by taking F to be the factor matrix of PSD MLR. Then y Rn is a Gaussian
∈
random vector with zero mean and covariance matrix Σ that is PSD MLR,
Σ =
(cid:2)
F D1/2
(cid:3)(cid:2)
F D1/2
(cid:3)T
= FFT +D.
We assume that we have access to hierarchical partition and rank allocation. We seek
to fit the coefficients of F Rn×s and diagonal D Rn×n (with diag(D) > 0) from the
∈ ∈
observed samples.
We assume s n, i.e., number of factors is smaller than the number of features.
≪
3 Fitting
Maximum likelihood estimation. In this paper, we fit parameters F and D using max-
imum likelihood estimation (MLE). This approach is different from that in [PHDB24], which
8focuses on fitting the empirical covariance matrix by PSD MLR matrix using the Frobenius
norm-based loss. Notably, the Frobenius norm is not an appropriate loss for fitting covari-
ance models. First, the Frobenius norm is coordinate-independent, while in MLE changes
across different coordinates mean different things. Frobenius norm can lead to models with
small eigenvalues of the covariance matrix, whereas MLE inherently guards against this.
Second, the Frobenius norm-based loss is distribution-agnostic, while MLE takes advantage
of the known distribution. Nevertheless, there is an intrinsic connection between the MLE
and Frobenius norm, which we detail in §A of the appendix.
Suppose we observe samples y ,...,y Rn, organized in the matrix form as
1 N
∈
 
yT
1
.
Y =  . .  RN×n.
 
∈
yT
N
The log-likelihood based on N points is
nN N 1
ℓ(F,D;Y) = log(2π) logdet(FFT +D) Tr((FFT +D)−1YTY). (3)
− 2 − 2 − 2
For structured F direct maximization of log-likelihood ℓ(F,D;Y) is difficult. Instead, the
expectation-maximization (EM) algorithm [DLR77] is the preferred method for MLE.
Simplification via data augmentation. Difficult maximum likelihood problems can
be simplified by data augmentation. Suppose along with Y we also observed latent data
z ,...,z Rs, organized in matrix Z RN×s. Then the log-likelihood of complete data
1 N
∈ ∈
(Y,Z) for model (2) is
(n+s)N N 1 1
ℓ(F,D;Y,Z) = log(2π) logdetD D−1/2(Y ZFT) 2 Z 2. (4)
− 2 − 2 − 2∥ − ∥F − 2∥ ∥F
Maximizing the ℓ(F,D;Y,Z) with respect to F and D is now tractable. First, since D
is diagonal, when F is known solving for D is trivial. Second, note that ℓ(F,D;Y,Z) is
separable across the rows of F. The nonzero coefficients in each row of F can be found by
solving the least squares problem.
For example, consider a simple factor model, where F is just a dense low rank matrix.
Then from optimality conditions, we get
1
Fˆ = YTZ(ZTZ)−1, Dˆ = diag(diag((Y ZFˆT)T(Y ZFˆT))).
N − −
Since we only observe Y and Z is missing, we use the EM algorithm to simplify the
problem through data augmentation.
93.1 Expectation step
Intheexpectationstepwecomputetheconditionalexpectationoflog-likelihoodwithrespect
to the conditional distribution (Y,Z Y) governed by the the current estimate of parameters
|
F0 and D0:
(cid:0) (cid:1)
Q(F,D;F0,D0) = E ℓ(F,D;Y,Z) Y,F0,D0 (5)
|
(n+s)N N
= log(2π) logdetD
− 2 − 2
N
1 (cid:88) (cid:0) (cid:1)
E Tr(D−1(y Fz )(y Fz )T)+Tr(z zT) Y,F0,D0
−2 i − i i − i i i |
i=1
N
(n+s)N N 1 (cid:88) (cid:0) (cid:0) (cid:1)(cid:1)
= log(2π) logdetD Tr E z zT y ,F0,D0
− 2 − 2 − 2 i i | i
i=1
N
1 (cid:88) (cid:0) (cid:2) (cid:0) (cid:1)
Tr D−1 (y yT 2F E z y ,F0,D0 y )
−2 i i − i | i i
i=1
(cid:0) (cid:1) (cid:3)(cid:1)
+ F E z zT y ,F0,D0 FT .
i i | i
To evaluate the Q(F,D;F0,D0), we need to compute several expectations. First, using (2)
we have
cov(y,z) = EFzzT = F
cov(y,y) = FFT +D = Σ.
Thus (z,y) is a Gaussian random vector with zero mean and covariance
(cid:20) (cid:21)
I FT
cov((z,y),(z,y)) = s .
F Σ
Second, the conditional distribution (z y ,F0,D0) is Gaussian,
i i
|
(cid:16) (cid:17)
F0T (Σ0)−1y ,I F0T (Σ0)−1F0 .
i s
N −
And finally, we have
N N
(cid:88) (cid:0) (cid:1) (cid:88)
E z zT y ,F0,D0 = cov((z ,z ) y ,F0,D0))
i i | i i i | i
i=1 i=1
+E(cid:0)
z y
,F0,D0(cid:1) E(cid:0)
z y
,F0,D0(cid:1)T
i i i i
| |
= N(I F0T (Σ0)−1F0)+F0T (Σ0)−1YTY(Σ0)−1F0.
s
−
Hence, (5) becomes
(n+s)N N 1
Q(F,D;F0,D0) = log(2π) logdetD Tr(W)
− 2 − 2 − 2
1
(cid:0) (cid:1)
Tr D−1(YTY 2FV +FWFT) , (6)
−2 −
10where we defined matrices V Rs×n and W Rs×s as
∈ ∈
N
V = (cid:88) E(cid:0) z y ,F0,D0(cid:1) yT = F0T (Σ0)−1YTY (7)
i | i i
i=1
N
(cid:88) (cid:0) (cid:1)
W = E z zT y ,F0,D0 . (8)
i i | i
i=1
3.2 Maximization step
In the maximization step we find updated parameters F1 and D1 by solving the following
problem
maximize Q(F,D;F0,D0)
(cid:2) (cid:3) (9)
subject to F D1/2 is the factor of PSD MLR.
Similar to (4), the maximization problem (9) is tractable. Observe, Q(F,D;F0,D0) is
separable across the rows of F. The nonzero coefficients in each row of F can be determined
by solving the least squares problem. For efficiency, we can group the rows by their sparsity
pattern and instead solve the least squares problems for each row sparsity pattern of F at
once. Having F1, the diagonal matrix is then given by
1
D1 = diag(diag(YTY 2F1V +F1W(F1)T)).
N −
EM algorithm iterates expectation and maximization steps until convergence.
4 Efficient computation
4.1 Inverse of PSD MLR
Inthemaximizationstep,evaluatingmatricesV (7)andW (8)requiressolvinglinearsystems
with the PSD MLR matrix. We will first address the efficient computation of Σ−1, i.e.,
(F FT + +F FT +D)−1.
1 1 ··· L−1 L−1
We will show that the inverse of the PSD MLR matrix is the MLR matrix with the same
hierarchical partition and rank allocation, and
Σ−1 = H HT H HT +D−1,
− 1 1 −···− L−1 L−1
where H Rn×p lr l is a factor at level l with the same sparsity structure as F .
l l
∈
Let F denote the concatenation of left factors from levels l,...,L 1, and define F
l+ l−
−
similarly,
(cid:2) (cid:3) (cid:2) (cid:3)
F = F F , F = F F .
l+ l L−1 l− 1 l
··· ···
ThusthenumberofnonzerocoefficientsinF
isn(cid:80)L−1r
andinF
isn(cid:80)l
r . Wecom-
l+ l′=l l′ l− l′=1 l′
pute the coefficients of the inverse by recursively applying the Sherman-Morrison-Woodbury
(SMW) matrix identity.
114.1.1 Properties of structured matrices
We begin by giving useful properties of our structured matrices. Consider a factor matrix
on level l, F Rn×p lr l, with p blocks of size n r for all k = 1,...,p .
l l l,k l l
∈ ×
• Matrix F FT Rn×n is a block diagonal matrix with blocks of size n n , see
l l ∈ l,k × l,k
illustration below.
=
Further, sparsity of matrix F FT is a refinement of sparsity pattern of F FT for all
l′ l′ l l
l′ > l. Therefore, matrix
L−1
(cid:88)
F FT = F FT
(l+1)+ (l+1)+ l′ l′
l′=l+1
has the same sparsity pattern as F FT .
l+1 l+1
• The inverse of a block diagonal matrix is a block diagonal matrix consisting of the
inverses of each block. Thus matrix (F FT +D)−1 is block diagonal with p
(l+1)+ (l+1)+ l+1
blocks, each of size n n .
l+1,k l+1,k
×
• Since the row (also column) sparsity of (F FT +D)−1 refines the row sparsity
(l+1)+ (l+1)+
of F , the matrix
l
M = (F FT +D)−1F (10)
0 (l+1)+ (l+1)+ l
retains the sparsity pattern of F , see figure below.
l
=
Thus a matrix-vector product with M can be computed in the order of
(cid:80)p
l n r =
0 k=1 l,k l
nr operations. In the more general case where the left block diagonal matrix has
l
12column sparsity that is the same or finer than the row sparsity of the right matrix,
their product is a block diagonal matrix with the sparsity of the right matrix with
rescaled row dimensions of the blocks, as shown below.
= =
Then it is straightforward to see that the matrix M = MTF has the same column
1 0 (l−1)−
sparsity as the matrix F with row dimensions of the blocks equal to r . Further,
(l−1)− l
matrix (F FT +D)−1F has the same sparsity pattern as F .
(l+1)+ (l+1)+ (l−1)− (l−1)−
• Matrix FTM Rp lr l×p lr l is a block diagonal matrix with p blocks of size r r , see
l 0 ∈ l l × l
figure below.
= =
It is straightforward to check that each of the blocks is PSD.
4.1.2 Recursive SMW algorithm
We show that Σ−1 is an MLR matrix with factors having the same sparsity pattern as Σ.
To establish this, we employ SMW matrix identity, which is given by
(FFT +D)−1 = D−1 D−1F(I +FTD−1F)−1FTD−1.
s
−
Using this identity, we obtain
(F FT +D)−1 = (F FT +D)−1 (F FT +D)−1
l+ l+ (l+1)+ (l+1)+ − (l+1)+ (l+1)+
F (I +FT(F FT +D)−1F )−1FT(F FT +D)−1
l p lr l l (l+1)+ (l+1)+ l l (l+1)+ (l+1)+
= (F FT +D)−1 M (I +FTM )−1MT. (11)
(l+1)+ (l+1)+ − 0 p lr l l 0 0
Define the matrix
H = M (I +FTM )−1/2.
l 0 p lr l l 0
Using the properties of structured matrices above, we conclude that the sparsity of H is the
l
same as the sparsity of M which in turn is the same as the sparsity of F . Then we have
0 l
(F FT +D)−1 = (F FT +D)−1 H HT.
l+ l+ (l+1)+ (l+1)+ − l l
Applying this recursion from the bottom to the top level we get
Σ−1 = H HT H HT +D−1.
− 1 1 −···− L−1 L−1
Combining the above it follows that Σ−1 is an MLR matrix.
13Computing inverse. We now show that the complexity of computing the coefficients of
the MLR matrix Σ−1 is O(nr2 + p r r2) and extra memory used is less than 3nr +
L−1 max
2p r r, where r = max r ,...,r . To do so, we recursively compute the coefficients
L−1 max max 1 L
{ }
of the matrices
(F FT +D)−1F , H , (12)
l+ l+ (l−1)− l
from the bottom to the top level.
Suppose we have n(cid:80)l r coefficients of (F FT +D)−1F . This implies that
l′=1 l′ (l+1)+ (l+1)+ l−
we have the coefficients of M (10). We now show how to compute (12) using SMW matrix
0
identity (11).
1. Compute M = MTF in O(nr (cid:80)l−1 r ) and store its p r (cid:80)l−1 r coefficients,
1 0 (l−1)− l l′=1 l′ l l l′=1 l′
since for l′ l 1 computing MTF takes nr r operations, and compact form of
F has
(cid:80)≤l−1−
r columns.
0 l′ l l′
(l−1)− l′=1 l′
2. Compute M = (I +FTM )−1 in O(nr2+p r3) and store its p r2 coefficients. Com-
2 p lr l l 0 l l l l l
pute H = M (I +FTM )−1/2 in O(nr2 +p r3) and store its nr coefficients. Note
l 0 p lr l l 0 l l l l
that computing I +FTM requires O(nr2) operations, and its eigendecomposition
p lr l l 0 l
takes O(p r3) operations.
l l
3. Compute M = M M in O(p
r2(cid:80)l−1
r ) and store its p r
(cid:80)l−1
r coefficients, since
3 2 1 l l l′=1 l′ l l l′=1 l′
the sparsity of M refines the row sparsity of block diagonals in M , and compact form
2 1
of M has
(cid:80)l−1
r columns. Note that M has the same sparsity as M .
1 l′=1 l′ 3 1
4. Compute M = M M in O(nr
(cid:80)l−1
r ) and store its
n(cid:80)l−1
r coefficients, since the
4 0 3 l l′=1 l′ l′=1 l′
column sparsity of M refines the row sparsity of block diagonals in M , and compact
0 3
form of M has
(cid:80)l−1
r columns. Note that M has the same sparsity as F .
3 l′=1 l′ 4 (l−1)−
5. ComputeM = (F FT +D)−1F M inn(cid:80)l−1 r andstoreitsn(cid:80)l−1 r
5 (l+1)+ (l+1)+ (l−1)− − 4 l′=1 l′ l′=1 l′
coefficients.
Therefore, the complexity at the level l is
(cid:32) (cid:33)
l
(cid:88)
O (nr +p r2) r .
l l l l′
l′=1
Finally, we conclude that the total complexity is
(cid:32) (cid:33)
L−1 l
(cid:88) (cid:88)
T(n) = O (nr +p r2) r
l l l l′
l=1 l′=1
= O(nr2 +p r r2),
L−1 max
and extra storage used is less than 3nr+2p r r.
L−1 max
Recall that s =
(cid:80)L−1p
r n, therefore, we have p n. This implies that the time
l=1 l l ≪ L−1 ≪
complexity is linear in n.
14If we assume that the rank allocation is uniform r = = r = r˜and that each block
1 L−1
···
on one level is split into two nearly equal-sized blocks on the next level, p = 2l−1, then the
l
total complexity and storage are respectively
T(n) = O(nr˜2L2 +2Lr˜3L), 3nr˜L+2Lr˜2L.
Using the assumption that s n and s = (2L−1 1)r˜, we have
≪ −
L log (n/r˜+1)+1.
2
≪
4.2 EM iteration
4.2.1 Selection matrices
Let s be the ith row sparsity pattern of F, having a size of s . The number of unique
i i
| |
sparsity patterns of rows of F equals the number of groups at level L 1, i.e., p . Note
L−1
that we must have (cid:80)pL−1 s = n. Let S 0,1 |si|×n be a matrix th− at selects rows with
ith sparsity pattern.
i= S1 inc| ei |
any row
spar ri si∈ ty{ pat}
tern of F has
(cid:80)L−1r
= r 1 nonzero
l=1 l −
columns, we define ST 0,1 s×(r−1) as a matrix that selects those columns of F. Thus, we
ci ∈ { }
have c = r 1, and the matrices
i
| | −
S FST R|si|×(r−1), i = 1,...,p ,
ri ci ∈ L−1
are dense in the coefficients of F, see figure 4.
Using these definitions, it is evident that for any matrix M with s rows, we have
S FM = S FSTS M, i = 1,...,p .
ri ri ci ci L−1
Figure 4: Structured matrix F with p = 4 row sparsity patterns is shown on the left. The second
3
row sparsity pattern is highlighted in red. The dense matrix S FST is shown on the right.
r2 c2
4.2.2 EM iteration computation
Recall that Q(F,D;F0,D0) (6) is separable across the rows of F. Therefore, to find F1 we
solve the reduced least squares problem for each sparsity pattern of F.
15To find the coefficients of F in problem (9), it suffices to minimize the following
pL−1
(cid:88) (cid:0) (cid:1)
Tr(FWFT 2FV) = Tr S FWFTST 2S FVST
− ri ri − ri ri
i=1
pL−1
(cid:88) (cid:0) (cid:1)
= Tr (S FST)(S WST)(S FST)T 2(S FST)(S VST) ,
ri ci ci ci ri ci − ri ci ci ri
i=1
where we used §4.2.1. Then to recover the coefficients of F, we solve the least squares
problem,
S FST = (S VST)T(S WST)−1. (13)
ri ci ci ri ci ci
We now derive the computational complexity for calculating F1. We first compute coef-
ficients of MLR (Σ0)−1 in T(n). Recall
V = F0T (Σ0)−1YTY, W = N(I F0T (Σ0)−1F0)+F0T (Σ0)−1YTY(Σ0)−1F0.
s
−
Since F0ST Rn×(r−1), we compute (Σ0)−1(F0ST) Rn×(r−1) in O(nr2) using §4.1. Next
ci ∈ ci ∈
we compute
(cid:16) (cid:17)
(S F0T )(Σ0)−1 (F0ST) R(r−1)×(r−1)
ci ci ∈
(cid:16) (cid:17)
in O(nr2). To evaluate the product (S F0T )(Σ0)−1 YT R(r−1)×N we need O(nrN).
ci
∈
Combining the above, we obtain
(cid:16) (cid:17)
S VST = S F0T (Σ0)−1YT (YST) R(r−1)×|si|
ci ri ci ri ∈
in O( s rN). Also by computing
i
| |
(cid:16) (cid:17)
(S F0T )(Σ0)−1YT (cid:0) Y(Σ0)−1F0ST(cid:1) R(r−1)×(r−1)
ci ci ∈
inO(r2N), wethengetS WST R(r−1)×(r−1) inO(r2). GivenS VST andS WS , solving
ci ci ∈ ci ri ci ci
the linear system (13) takes O( s r3).
i
| |
When solving for each sparsity pattern s , the total complexity of the maximization step
i
is
pL−1
(cid:88)
T(n)+ O(nr2 +nrN + s rN +r2N + s r3),
i i
| | | |
i=1
which simplifies to
T(n)+O(p nr2 +p nrN +nr3).
L−1 L−1
Plugging in the complexity of the inverse computation we arrive at
O(p nr2 +nr3 +p nrN +p r r2).
L−1 L−1 L−1 max
Since p n, the time complexity is linear in n.
L−1
≪
16ˆ
Fit Model Σ Σ / Σ ℓ(F,D;Y)/N
F F
∥ − ∥ ∥ ∥
Frob FM 0.1538 11809
MLE FM 0.9495 11904
Frob MFM 0.1648 11956
MLE MFM 0.9506 12114
Table 1: Frobenius errors and average log-likelihoods for factors fitted using either the Frobenius
norm or MLE-based methods for the asset covariance matrix.
As a stopping criteria we use the relative difference between consecutive log-likelihoods of
observations (3). This involves the computation of the determinant of the covariance matrix.
In §C we show how to compute the Cholesky factorization of an expanded matrix with linear
time and space complexities, alongside the computation of the inverse in §4.1. Recognizing
that the covariance matrix is the Schur complement of this expanded matrix, we leverage
this factorization to compute logdet(FFT +D) in linear time. See §C.4 for details.
5 Numerical examples
We compare two factor fitting approaches based on Frobenius norm [PHDB24] and MLE.
In the first example, we compare a traditional factor model (FM) with a multilevel factor
model (MFM) using real data. We demonstrate that the multilevel factor model significantly
improves the likelihood of the observations. In the second example, we consider a synthetic
multilevel factor model to generate the observations. Our results show that the expected
log-likelihood distribution of the MLE-based method significantly outperforms the Frobenius
norm-based method.
5.1 Asset covariance matrix
We focus on the asset covariance matrix from [PHDB24, §8.1]. In this example the daily
returns of n = 5000 assets are found or derived from data from CRSP Daily Stock and
CRSP/CompustatMergedDatabase©2023CenterforResearchinSecurityPrices(CRSP®),
The University of Chicago Booth School of Business. We consider a N = 300 (trading) day
period ending 2022/12/30, and for hierarchical partition use Global Industry Classifica-
tion Standard (GICS) [BLO03] codes from CRSP/Compustat Merged Database – Security
Monthly during 2022/06/30 to 2023/01/31 which has L = 6 levels.
We use the GICS hierarchy and two different rank allocations; see figure 5 and table 1.
For a rank allocation of r = 29, r = = r = 0, r = 1 (i.e., a traditional factor model),
1 2 5 6
···
our method’s average log-likelihood of realized returns improves by 95 compared to the
Frobenius norm-based method. With ranks r = 14, r = 6, r = 4, r = 3, r = 2, r = 1
1 2 3 4 5 6
(i.e., multilevel factor model), it increases by 158. Thus the best log-likelihood is achieved
using the multilevel factor model fitted with MLE-based objective. Also note that a low
Frobenius error does not necessarily indicate a better log-likelihood, see table 1.
171.215 104
× MLE
1.21 104 Frob
×
1.205 104
×
1.2 104
×
1.195 104
×
1.19 104
×
1.185 104
× MLE
1.18 104 Frob
×
0 25 50 75 100 125 150 0 25 50 75 100 125 150
iteration iteration
Figure 5: Log-likelihood during the EM algorithm (red curve) and after Frobenius norm fitting
(blue curve), for FM (left) and MFM (right) of the asset covariance matrix.
To assess whether the log-likelihoods of the two methods are significantly different, we
can compare it to the standard deviation of the expectation of these log-likelihoods with
respect to the true model. Since we do not have the density of the true model, we assume
that the samples are drawn from (2). Under this assumption the standard deviation of the
average log-likelihood is 2.887, see §B. Therefore, we conclude that the log-likelihood for our
method MLE is significantly better.
5.2 Synthetic multilevel factor model
We generate samples from a synthetic multilevel factor model with n = 10000 features. We
create a random hierarchical partition with L = 6. Starting with a single group, we evenly
divide it across levels, resulting in 4, 8, 16, 32, and finally 10000 groups at the bottom level.
Each level is assigned ranks: r = 10, r = 5, r = 4, r = 3, r = 2, r = 1, respectively,
1 2 3 4 5 6
yielding s = 174 unique factors in total.
Followingthis,thecoefficientsofthestructuredfactormatrixF aresampledfrom (0,1).
N
Then we sample the noise variance in proportion to the average signal variance maintaining
a signal-to-noise ratio (SNR) of 4. This is achieved by sampling D uniformly from the
ii
interval
[0,2(1T diag(FFT)/n)/SNR], i = 1,...,n.
To evaluate how effectively we can fit the factors using MLE, we use the rank allocation
and hierarchical partition from the true model. The model is fitted with N = 80 samples
and evaluated using expected log-likelihood (based on the density of the true model).
Since in this example we have access to the true model Σtrue = FtrueFtrueT +Dtrue, we
can compute the expected log-likelihood
n 1 1
E(ℓ(F,D;y)) = log(2π) logdet(FFT +D) Tr((FFT +D)−1Σtrue).
−2 − 2 − 2
18
N/)Y;D,F(‘ N/)Y;D,F(‘Fit ℓ(F,D;Y)/N E(ℓ(F,D;y))
Frob 20864 24870
− −
MLE 20625 24552
− −
True 22031 22068
− −
Table 2: Log-likelihood for models fitted using the Frobenius norm, MLE-based methods and the
true model for a single Y in the synthetic example.
MLE
20650
− Frob
20700
−
20750
−
20800
−
20850
−
20900
−
20950
−
0 50 100 150 200 250 300
iteration
Figure 6: Log-likelihood during the EM algorithm (red curve) and after Frobenius norm fitting
(blue curve) for a single Y in the synthetic example.
We compare the average log-likelihood of two fitting approaches based on Frobenius
norm and MLE; see figure 6 and table 2. Our method outperforms the Frobenius norm-
based approach, showing a 239 higher average log-likelihood on the sampled data Y and a
318 greater expected log-likelihood.
We generate 200 samples Y, and for each Y, fit the model with two competing methods.
The resulting histograms of expected log-likelihoods E(ℓ(F,D;y)) are shown on figure 7.
The histogram of differences E(ℓ(FMLE,DMLE;y)) E(ℓ(FFrob,DFrob;y)) is displayed on
−
figure 8. The mean of the differences is 371, with a standard deviation of 136, and for
99.5% of the samples, the difference is positive. Based on these histograms, we conclude
that the distribution of the MLE-based method is significantly better than that of Frobenius
norm-based method.
19
N/)Y;D,F(‘0.006
Frob
MLE
0.005
0.004
0.003
0.002
0.001
0.000
25600 25400 25200 25000 24800 24600
− − − − − −
E(‘(F,D;y))
Figure 7: Histograms of expected log-likelihoods for MLE and Frobenius norm-based fitting
methods.
0.0035
0.0030
0.0025
0.0020
0.0015
0.0010
0.0005
0.0000
0 200 400 600 800
E(‘(FMLE,DMLE;y)) E(‘(FFrob,DFrob;y))
−
Figure 8: Histogram of differences in expected log-likelihoods between MLE and Frobenius norm-
based fitting methods.
20
ytisneD
ytisneDReferences
[AAH81] Murray Aitkin, Dorothy Anderson, and John Hinde. Statistical modelling of
data on teaching styles. Journal of the Royal Statistical Society Series A: Statis-
tics in Society, 144(4):419–448, 1981.
[AM06] Tihomir Asparouhov and Bengt Muth´en. Multilevel modeling of complex survey
data. In Proceedings of the Joint Statistical Meeting in Seattle, pages 2718–2726.
Citeseer, 2006.
[BLO03] Sanjeev Bhojraj, Charles M. C. Lee, and Derek K. Oler. What’s my line?
A comparison of industry classification schemes for capital market research.
Journal of Accounting Research, 41(5):745–774, 2003.
[BN02] Jushan Bai and Serena Ng. Determining the number of factors in approximate
factor models. Econometrica, 70(1):191–221, 2002.
[BNM+11] Steven Boker, Michael Neale, Hermine Maes, Michael Wilde, Michael Spiegel,
Timothy Brick, Jeffrey Spies, Ryne Estabrook, Sarah Kenny, Timothy Bates,
et al. Openmx: An open source extended structural equation modeling frame-
work. Psychometrika, 76:306–317, 2011.
[BNW12] Martin Brunner, Gabriel Nagy, and Oliver Wilhelm. A tutorial on hierarchically
structured constructs. Journal of Personality, 80(4):796–846, 2012.
[BV04] Stephen P. Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge
University Press, 2004.
[BW15] Jushan Bai and Peng Wang. Identification and Bayesian estimation of dynamic
factor models. Journal of Business & Economic Statistics, 33(2):221–240, 2015.
[Car93] John Carroll. Human Cognitive Abilities: A Survey of Factor-Analytic Studies.
Number 1. Cambridge University Press, 1993.
[Cat65] Raymond Cattell. A biometrics invited paper. Factor analysis: An introduction
to essentials I. The purpose and underlying models. Biometrics, 21(1):190–215,
1965.
[Chi06] Dennis Child. The Essentials of Factor Analysis. A&C Black, 2006.
[CKO11] MarioCrucini, AyhanKose, andChristopherOtrok. Whatarethedrivingforces
of international business cycles? Review of Economic Dynamics, 14(1):156–175,
2011.
[DeY06] Colin DeYoung. Higher-order factors of the big five in a multi-informant sample.
Journal of Personality and Social Psychology, 91(6):1138, 2006.
21[DFH+09] Robert Dedrick, John Ferron, Melinda Hess, Kristine Hogarty, Jeffrey Krom-
rey, Thomas Lang, John Niles, and Reginald Lee. Multilevel modeling: A re-
view of methodological issues and applications. Review of Educational Research,
79(1):69–102, 2009.
[DLMG08] Jan De Leeuw, Erik Meijer, and Harvey Goldstein. Handbook of Multilevel
Analysis, volume 401. Springer, 2008.
[DLR77] Arthur Dempster, Nan Laird, and Donald Rubin. Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society:
Series B (Methodological), 39(1):1–22, 1977.
[FF93] EugeneFamaandKennethFrench. Commonriskfactorsinthereturnsonstocks
and bonds. Journal of Financial Economics, 33(1):3–56, 1993.
[Fru54] Benjamin Fruchter. Introduction to Factor Analysis. Van Nostrand, 1954.
[FWMS99] Leandre Fabrigar, Duane Wegener, Robert MacCallum, and Erin Strahan. Eval-
uating the use of exploratory factor analysis in psychological research. Psycho-
logical Methods, 4(3):272, 1999.
[GB14] Harvey Goldstein and William Browne. Multilevel factor analysis modelling us-
ingMarkovchainMonteCarloestimation. InLatent variable and latent structure
models, pages 237–256. Psychology Press, 2014.
[GH99] Allan Gregory and Allen Head. Common and country-specific fluctuations in
productivity, investment, and the current account. Journal of Monetary Eco-
nomics, 44(3):423–451, 1999.
[GH07] Andrew Gelman and Jennifer Hill. Data Analysis Using Regression and Multi-
level/Hierarchical Models. Cambridge University Press, 2007.
[Gol86] Harvey Goldstein. Multilevel mixed linear model analysis using iterative gener-
alized least squares. Biometrika, 73(1):43–56, 1986.
[Gol11] Harvey Goldstein. Multilevel Statistical Models. John Wiley & Sons, 2011.
[Har76] Harry Harman. Modern Factor Analysis. University of Chicago Press, 1976.
[Hua17] Francis Huang. Conducting multilevel confirmatory factor analysis using R.
Working paper. https://doi.org/10.13140/RG.2.2.12391.34724, 2017.
[Jo¨r69] Karl Jo¨reskog. A general approach to confirmatory maximum likelihood factor
analysis. Psychometrika, 34(2):183–202, 1969.
[JS96] Karl J¨oreskog and Dag S¨orbom. LISREL 8: User’s Reference Guide. Scientific
Software International, 1996.
22[JS16] Breitung Jo¨rg and Eickmeier Sandra. Analyzing international business and fi-
nancial cycles using multi-level factor models: A comparison of alternative ap-
proaches. InDynamic Factor Models, volume35, pages177–214.EmeraldGroup
Publishing Limited, 2016.
[KOW03] Ayhan Kose, Christopher Otrok, and Charles Whiteman. International business
cycles: World, region, and country-specific factors. American Economic Review,
93(4):1216–1239, 2003.
[LB88] Mary Lindstrom and Douglas Bates. Newton-Raphson and EM algorithms for
linearmixed-effectsmodelsforrepeated-measuresdata. Journal of the American
Statistical Association, 83(404):1014–1022, 1988.
[Lin10] Ting Lin. A comparison of multiple imputation with EM algorithm and MCMC
method for quality of life missing data. Quality & Quantity, 44:277–287, 2010.
[McG09] KevinMcGrew. CHCtheoryandthehumancognitiveabilitiesproject: Standing
on the shoulders of the giants of psychometric intelligence research, 2009.
[MG89] Roderick McDonald and Harvey Goldstein. Balanced versus unbalanced designs
for linear structural relations in two-level data. British Journal of Mathematical
and Statistical Psychology, 42(2):215–232, 1989.
[MM17] Bengt Muth´en and Linda Muth´en. Mplus. In Handbook of Item Response The-
ory, pages 507–518. Chapman and Hall/CRC, 2017.
[Mut24] Bengt Muth´en. Mplus: A brief overview of its unique analysis capabilities.
In Cambridge Handbook of Research Methods and Statistics for the Social and
Behavioral Sciences: Volume Three. 2024. Forthcoming.
[PHDB24] Tetiana Parshakova, Trevor Hastie, Eric Darve, and Stephen Boyd. Factor
fitting, rank allocation, and partitioning in multilevel low rank matrices. In
M. Rassias, A. Nikeghbali, and P. Pardalos, editors, Optimization, Discrete
Mathematics, and Applications to Data Sciences, SOIA.Springer, October2024.
To appear.
[PHvO+17] Joshua Pritikin, Michael Hunter, Timo von Oertzen, Timothy Brick, and Steven
Boker. Many-level multilevel structural equation modeling: An efficient eval-
uation strategy. Structural Equation Modeling: A Multidisciplinary Journal,
24(5):684–698, 2017.
[Rau95] Stephen Raudenbush. Maximum likelihood estimation for unbalanced multilevel
covariance structure models via the EM algorithm. British Journal of Mathe-
matical and Statistical Psychology, 48(2):359–370, 1995.
[RB02] Stephen Raudenbush and Anthony Bryk. Hierarchical Linear Models: Applica-
tions and Data Analysis Methods, volume 1. SAGE, 2002.
23[RBG+00] JonRasbash,WilliamBrowne,HarveyGoldstein,MinYang,IanPlewis,Michael
Healy, Geoff Woodhouse, David Draper, Ian Langford, and Toby Lewis. A user’s
guide to MLwiN. London: Institute of Education, 286, 2000.
[RH98] Kenneth Rowe and Peter Hill. Modeling educational effectiveness in classrooms:
The use of multi-level structural equations to model students’ progress. Educa-
tional Research and Evaluation, 4(4):307–347, 1998.
[RHSP04a] Sophia Rabe-Hesketh, Anders Skrondal, and Andrew Pickles. Generalized mul-
tilevel structural equation modeling. Psychometrika, 69:167–190, 2004.
[RHSP04b] Sophia Rabe-Hesketh, Anders Skrondal, and Andrew Pickles. GLLAMM Man-
ual. Technical Report 1160, U.C. Berkeley Division of Biostatistics Working
Paper Series, 2004.
[Ros12] Yves Rosseel. lavaan: An R package for structural equation modeling. Journal
of statistical software, 48:1–36, 2012.
[RT82] Donald Rubin and Dorothy Thayer. EM algorithms for ML factor analysis.
Psychometrika, 47:69–76, 1982.
[SL57] JohnSchmidandJohnLeiman. Thedevelopmentofhierarchicalfactorsolutions.
Psychometrika, 22(1):53–61, 1957.
[Spe04] Charles Spearman. “General intelligence,” objectively determined and mea-
sured. The American Journal of Psychology, 15(2):201–292, 1904.
[Wan12] Peng Wang. Large dimensional factor models with a multi-level factor structure:
Identification, estimation, and inference. 2012.
[Whe59] Robert Wherry. Hierarchical factor solutions without rotation. Psychometrika,
24(1):45–51, 1959.
[YTM99] Yiu-Fai Yung, David Thissen, and Lori D. McLeod. On the relationship between
the higher-order factor model and the hierarchical factor model. Psychometrika,
64:113–128, 1999.
24A Second order approximation of log-likelihood
In this section we explain the intricate relationship between Frobenius norm and MLE-based
losses. Let S = YTY/N be a sample covariance matrix. Then the average log-likelihood of
N data points for a Gaussian model y N(0,Σ) is
∼
1 n 1 1
ℓ(Σ;Y) = log(2π) logdetΣ Tr(Σ−1S).
N −2 − 2 − 2
We now derive the second-order approximation of the average log-likelihood. We start
with finding the second-order approximation of the function f : Sn R,
→
f(Σ) = logdetΣ, domf = Sn .
++
Following the derivation of [BV04, §A.4], let ∆ Sn be such that (Σ+∆) Sn is close to
∈ ∈ ++
Σ. We have
(cid:0) (cid:1)
logdet(Σ+∆) = logdet Σ1/2(I +Σ−1/2∆Σ−1/2)Σ1/2
= logdetΣ+logdet(I +Σ−1/2∆Σ−1/2)
1
(cid:88)
= logdetΣ+ log(1+λ ),
i
i=1
where λ is the ith eigenvalue of Σ−1/2∆Σ−1/2. Since ∆ is small, then λ are small. Thus to
i i
second order, we have
λ2
log(1+λ ) λ i .
i i
≈ − 2
Combining the above we get
(cid:88)n λ2
logdet(Σ+∆) logdetΣ (λ i )
i
− ≈ − 2
i=1
1
(cid:0) (cid:1)
= Tr(Σ−1∆) Tr Σ−1∆Σ−1∆ .
− 2
We used the fact the sum of eigenvalues is the trace, and the eigenvalues of the product of
a symmetric matrix with itself are the squares of the eigenvalues of the original matrix, and
the cyclic property of trace.
Next we find the second-order approximation of the function g : Sn R,
→
g(Σ) = Tr(Σ−1S), domg = Sn .
++
Since Σ 0, we have
≻
(cid:0) (cid:1)
Tr((Σ+∆)−1S) = Tr Σ−1/2(I +Σ−1/2∆Σ−1/2)−1Σ−1/2S .
25Recall ∆ Sn is small, therefore the spectral radius of Σ−1/2∆Σ−1/2 is smaller than 1. Thus
∈
using the Neuman series to second order we have
(I +Σ−1/2∆Σ−1/2)−1 I Σ−1/2∆Σ−1/2 +Σ−1/2∆Σ−1∆Σ−1/2.
≈ −
Combining the above we get
(cid:0) (cid:1)
Tr((Σ+∆)−1S) Tr(Σ−1S) Tr(Σ−1∆Σ−1S)+Tr Σ−1∆Σ−1∆Σ−1S .
≈ −
Using the abovederivations, the second-orderapproximationof theaveragelog-likelihood
at S is the quadratic function of Σ given by
1 1 1
ℓ(Σ;Y) ℓ(S;Y) S−1(S Σ) 2
N ≈ N − 4∥ − ∥F
1 1
= ℓ(S;Y) I S−1Σ 2. (14)
N − 4∥ − ∥F
Finally, (14) gives the relationship between the log-likelihood and Frobenius norm.
B Heuristic method for variance estimation
In §5, we compare the log-likelihoods of models fitted using Frobenius-based loss or MLE.
To assess if the difference in the log-likelihoods is significant, we present a heuristic method
for estimating the variance of the average log-likelihood. We assume that the empirical data
is coming from model (2) with parameters F and D. Then the average log-likelihood of N
data points is
1 n 1 1
ℓ(F,D;Y) = log(2π) logdet(FFT +D) Tr((FFT +D)−1YTY)
N −2 − 2 − 2N
N
n 1 1 (cid:88)
= log(2π) logdet(Σ) yTΣ−1y .
−2 − 2 − 2N i i
i=1
Since y (0,Σ), then Σ−1/2y (0,I). This implies
i i
∼ N ∼ N
yTΣ−1y = (Σ−1/2y )T(Σ−1/2y ) χ2(n).
i i i i ∼
Let z = Σ−1/2y , thus
i i
(cid:32) (cid:33)
(cid:18) (cid:19) N N
1 1 (cid:88) 1 (cid:88) (cid:0) (cid:1) n
var ℓ(F,D;Y) = var zTz = var zTz = .
N 2N i i 4N2 i i 2N
i=1 i=1
Also the expectation is
(cid:18) (cid:19) N
1 n 1 1 (cid:88) (cid:0) (cid:1)
E ℓ(F,D;Y) = log(2π) logdet(Σ) E zTz
N −2 − 2 − 2N i i
i=1
n 1 n
= log(2π) logdet(Σ) .
−2 − 2 − 2
26In the asset covariance example, we have n = 5000 and N = 300. Therefore, the
approximation to the standard deviation is
(cid:114)
n
2.887,
2N ≈
and of the expectation is
n 1 n 1
(1+log(2π)) logdet(Σ) 7095 logdet(Σ).
−2 − 2 − 2 ≈ − − 2
C Cholesky factorization
In this section we present a Cholesky factorization for the expanded matrix.
C.1 Schur complement
Finding the inverse of Σ amounts to solving the linear system
(FFT +D)X = DX +F FT X + +F FTX = I ,
L−1 L−1 ··· 1 1 n
which is equivalent to solving expanded system of equations
  
D F F X
L−1 1
···
  F LT . . .−1 −I pL−1rL−1 ...     Y L . . .−1   = (cid:20) I 0n(cid:21) . (15)
  
FT I Y
1 − p1r1 1
Denote the expanded matrix (15) by E Sn+s. Note that E has the block sparsity pattern
∈
of the upward-left arrow.
Block Gaussian elimination on the matrix (15) leads to an LU decomposition
   
I F F I
n L−1 1 n
− ··· −
E =   

I pL−1rL−1 ...    (cid:20) FFT +D I s(cid:21)    −F . . .LT −1 I pL−1rL−1 ...    . (16)
−
I FT I
p1r1 − 1 p1r1
And FFT +D is Schur complement of the block I of the matrix E.
s
−
C.2 Recursive Cholesky factorization
Let s = (cid:80)l p r . Define E Sn+s l+ as the submatrix of E such that
l+ l′=L−1 l′ l′ l
∈
 
D F F
L−1 l
···
FT I 
E l =   L . . .−1 − pL−1rL−1 ...  .
 
FT I
l − p lr l
27We find the factors of E by recursively factorizing E ,...,E using the relation
L−1 1
 (cid:20) (cid:21)
F
E l
E l =  l+1 0 .
(cid:2) (cid:3)
FT 0 I
l − p lr l
C.2.1 Sparsity patterns
The block Gaussian elimination on E gives the following factorization
l
 
(cid:20) (cid:2) FI Tn+s 0(l+(cid:3)1 E)+ −1 I (cid:21) E l+1 (cid:18) I +(cid:2) FT 0(cid:3) E−1 (cid:20) F l(cid:21)(cid:19) (cid:20) (cid:2) FI Tn+s 0(l+(cid:3)1 E)+ −1 I (cid:21)T .
l l+1 p lr l − p lr l l l+1 0 l l+1 p lr l
(17)
Submatrices of E. We now show the sparsity pattern of matrices necessary for Cholesky
factorization. Assume that
(cid:20) (cid:21)
(cid:2) FT 0(cid:3) E−1 I n = FT (F FT +D)−1,
l− l+1 0 l− (l+1)+ (l+1)+
(cid:2) (cid:3)
and that matrix FT 0 E−1 has the same sparsity as matrix
l− l+1
(cid:2) (cid:3)
FT D F F .
l− L−1 ··· l+1
It is easy to check that these properties hold for the base case, E = D. Now we demonstrate
L
the properties of E .
l
Note that the (negative) bottom block in the block diagonal matrix in (17) is equal to
I +FT(F FT +D)−1F , (18)
p lr l l (l+1)+ (l+1)+ l
which is a positive definite matrix. Recall from §4.1.1, that this matrix is block diagonal,
consistingofp blocks,eachofwhichisofsizer r . LetR V RT betheCholeskyfactorization
l l × l l l l
of (18).
Using the relation from (17), we can express the inverse as
(cid:20) I (cid:21)T (cid:20) E−1 (cid:21)(cid:20) I (cid:21)
E−1 = (cid:2) n+s (l+(cid:3)1)+ l+1 (cid:2) n+s (l+(cid:3)1)+ .
l FT 0 E−1 I (R V RT)−1 FT 0 E−1 I
− l l+1 p lr l − l l l − l l+1 p lr l
Thus the matrix (cid:2) FT 0(cid:3) E−1 is equal to
(l−1)− l
(cid:20)(cid:18) (cid:20) (cid:21) (cid:19) (cid:20) (cid:21) (cid:21)
(cid:2) FT 0(cid:3) E−1 I F l (R V RT)−1(cid:2) FT 0(cid:3) E−1 F l (R V RT)−1 . (19)
(l−1)− l+1 n+s (l+1)+ − 0 l l l l l+1 0 l l l
Combining (19) and SMW (11) we get
(cid:20) (cid:21)
(cid:2) FT 0(cid:3) E−1 I n = FT (F FT +D)−1.
(l−1)− l 0 (l−1)− l+ l+
28The coefficients of matrix
(cid:20) (cid:21)
(cid:2) FT 0(cid:3) E−1 0 = FT (F FT +D)−1F (R V RT)−1 = MT (20)
(l−1)− l I (l−1)− (l+1)+ (l+1)+ l l l l 3
pr
l l
are obtained during the inverse computation, see §4.1.2. Furthermore, the sparsity of MT is
3
the same as the sparsity of FT F .
(l−1)− l
To simplify the argument, assume each group on level l is partitioned into K groups on
level l+1. This implies that for any l l , we have p /p = Kl2−l1. Since the row sparsity
1
≤
2 l2 l1
of F refines the row sparsity of F , matrix FTF is a block diagonal matrix with p blocks
l2 l1 l1 l2 l1
of size r Kl2−l1r . Thus the number of nonzero entries is p r r . Moreover, computing
FTF tal1 k× es O(nr rl2 ). Then for any ˜ l = l+1,...,L 1, matl r2 ixl1 l2
l1 l2 l1 l2 −
(cid:2) FT 0(cid:3) E−1(cid:2) 0 I 0(cid:3)T = (cid:0)(cid:2) FT 0(cid:3) E−1 MT (cid:2) FT 0(cid:3) E−1(cid:1)(cid:2) 0 I 0(cid:3)T ,
(l−1)− l p˜lr˜l (l−1)− l+1 − 3 l l+1 p˜lr˜l
(21)
has the sparsity of FT F FTF , which in turn has the sparsity of FT F . Here we used
(l−1)− l l ˜l (l−1)− ˜l
the fact that for any l′ = 1,...,l 1, the row sparsity of FTF refines the column sparsity of
− l ˜l
FTF , and therefore, FTF FTF is a block diagonal matrix with p blocks of size r K˜l−l′r ,
l′ l l′ l l ˜l l′ l′ × ˜l
which is the sparsity of FTF .
l′ ˜l
By induction we showed that for all l = L 1,...,1,
−
(cid:20) (cid:21)
(cid:2) FT 0(cid:3) E−1 I n = FT (F FT +D)−1,
(l−1)− l 0 (l−1)− l+ l+
and the matrix (cid:2) FT 0(cid:3) E−1 has the same sparsity as the matrix
(l−1)− l
(cid:2) (cid:3)
FT D F F .
(l−1)− L−1 ··· l
Cholesky factors. Let the Cholesky factorization of E be given by
l+1
E =
L(l+1)D(l+1)L(l+1)T
.
l+1
Technically, this is an LU factorization because E is not positive definite, and some of
l+1
the entries in D(l+1) are negative. However, we refer to it as Cholesky since the triangular
matrices are transposes of each other.
Using (17) we have
(cid:20) (cid:21)(cid:20) (cid:21)(cid:20) (cid:21)T
I E I
E l = (cid:2) FTn+s 0(l+(cid:3)1 E)+ −1 I l+1 R V RT (cid:2) FTn+s 0(l+(cid:3)1 E)+ −1 I
l l+1 p lr l − l l l l l+1 p lr l
(cid:20)
L(l+1)
(cid:21)(cid:20)
D(l+1)
(cid:21)(cid:20)
L(l+1)
(cid:21)T
= (cid:2) (cid:3) (cid:2) (cid:3) . (22)
FT 0 E−1L(l+1) R V FT 0 E−1L(l+1) R
l l+1 l − l l l+1 l
Note that the matrix R is a block diagonal matrix consisting of p blocks, each of which is a
l l
lower triangular matrix of size r r , see §4.1.1. Thus from (22), Cholesky factors of E are
l l l
×
(cid:20) L(l+1) (cid:21) (cid:20) D(l+1) (cid:21)
L(l) = , D(l) = . (23)
(cid:2) FT 0(cid:3) (D(l+1)L(l+1)T )−1 R V
l l − l
29Then we also have
(cid:20) (cid:21)
(L(l+1))−1
(L(l))−1 = (cid:2) (cid:3) .
R−1 FT 0 E−1 R−1
− l l l+1 l
˜
Assume for all l = L,...,l+1 matrices
(cid:2) (cid:3) (cid:2) (cid:3)
0 I 0 L(l+1), 0 I 0 (L(l+1))−1,
p˜lr˜l p˜lr˜l
(cid:2) (cid:3)
have the same sparsity as the matrix FT D F F .
˜l L−1 ··· l+1
It is easy to see that matrices (cid:2) FT 0(cid:3) (D(l+1)L(l+1)T )−1 and R−1(cid:2) FT 0(cid:3) E−1 have the
l l l l+1
(cid:2) (cid:3)
same sparsity as FT D F F . Thus matrices
l L−1 ··· l+1
(cid:2) (cid:3) (cid:2) (cid:3)
0 I L(l), 0 I (L(l))−1,
pr pr
l l l l
(cid:2) (cid:3)
have the same sparsity as the matrix FT D F F .
l L−1 ··· l
Evidently for the base case, L(L) = I and D(L) = D, these properties hold. By induction
n
we showed that matrices (L(1))−1 and (L(1))−1 have the same sparsity.
C.3 Efficient computation
Recurrent term. Using (23) we recursively compute
(cid:20) (cid:20) (cid:21) (cid:21)
(cid:2) FT 0(cid:3) (D(l)L(l)T )−1 = (cid:2) FT 0(cid:3) (D(l+1)L(l+1)T )−1 E−1 F l (V RT)−1 .
(l−1)− (l−1)− l+1 0 l l
From the previous section, we get
(cid:20) (cid:21)
(cid:2) FT 0(cid:3) E−1 F l (V RT)−1 = MTR .
(l−1)− l+1 0 l l 3 l
The product MTR requires (cid:80)l−1 O(p r2r ) = O(p rr2) operations. Thus we get
3 l l′=1 l l l′ l l
(cid:104) (cid:105)
(cid:2) FT 0(cid:3) (D(l)L(l)T )−1 = (cid:2) FT 0(cid:3) (D(l+1)L(l+1)T )−1 MTR , (24)
(l−1)− (l−1)− 3 l
whose sparsity is the same as that of
(cid:2) (cid:3)
FT D F F .
(l−1)− L−1 ··· l
Method. We now describe the algorithm for computing Cholesky factorization, that re-
cursively computes Cholesky factors of E ,E ,...,E . This process is accompanied by
L L−1 1
the recursive computation of coefficients in Σ−1, see §4.1. We include additional time and
space complexities beyond those discussed in §4.1.
We start with L(L) = I and D(L) = D. Then for each level l = L 1,...,1 repeat the
n
−
following steps.
301. Compute Cholesky decomposition of (18), R V RT, in O(p r3), and store its O(p r2)
l l l l l l l
coefficients. The coefficients of (18) and its inverse are obtained in §4.1.
2. Form L(l) and D(l) using stored coefficients of (cid:2) FT 0(cid:3) (D(l+1)L(l+1)T )−1 according to
l−
(23).
3. Form (cid:2) FT 0(cid:3) (D(l)L(l)T )−1 using (24). This requires O(p rr2) operations and
(l−1)− l l
(cid:80)l−1 r (n+(cid:80)L−1p r ) coefficients. We use (cid:80)l−1 p r r coefficients of M from §4.1.
l′=1 l′ ˜l=l ˜l ˜l l′=1 l l′ l 3
Cholesky factor of E, lower triangular matrix L(1), has less than
1
(cid:88)
n+ r (n+p r + +p r ) nr+p r2
l L−1 L−1 l l L−1
··· ≤
l=L−1
nonzero entries. The total cost for computing the factors is
1
(cid:88)
O(nr)+ O(p r3 +p rr2) = O(nr+p r3).
l l l l L−1
l=L−1
C.4 Determinant
Using the Cholesky decomposition of E we can easily compute the determinant of MLR
covariance matrix Σ. Specifically, using (16) we have
det(E) = det(FFT +D)( 1)s,
−
since the eigenvalues of a triangular matrix are exactly its diagonal entries and because the
determinant is a multiplicative map. Alternatively, using Cholesky decomposition, E =
L(1)D(1)L(1)T
, we have
det(E) = det(L(1))2det(D(1)) = det(D(1)).
Since
L−1
(cid:89)
det(D(1)) = ( 1)sdet(D) det(V ),
l
−
l=1
we obtain
n+s n+s
(cid:89) (cid:88)
det(FFT +D) = D(1) , logdet(FFT +D) = log D(1) .
| ii | | ii |
i=1 i=1
31D Product of MLR matrices
In this section we show that the product of two MLR matrices, A with MLR-rank r and
A′ with MLR-rank r′, sharing the same symmetric hierarchical partition, is also an MLR
matrix with the same hierarchical partition and an MLR-rank of (r+r′). We also show that
it can be computed using O(nmax r,r′ 2) operations.
{ }
Since the hierarchical partition is symmetric, without loss of generality assume A and A′
are contiguous MLR. Define
Al+ = Al + +AL,
···
then it is easy to check that
(cid:32) (cid:33)(cid:32) (cid:33)
L L
(cid:88) (cid:88)
AA′ = Al A′l
l=1 l=1
L−1
(cid:88)(cid:0) (cid:1)
= AlA′l+ +A(l+1)+A′l +ALA′L. (25)
l=1
We now show that each term in the sum above can be decomposed into a product of block
diagonal matrices, which are the factors of matrix AA′ on level l.
Recall the notation from [PHDB24],
Al = blkdiag(B CT ,...,B CT ), A′l = blkdiag(B′ C′T,...,B′ C′T )
l,1 l,1 l,p l l,p l l,1 l,1 l,p l l,p l
where B ,B′ ,C ,C′ Rn l,k×r l, for all k = 1,...,p , and l = 1,...,L.
l,k l,k l,k l,k ∈ l
Since for all levels l ˜ l, A′˜l refines the sparsity of Al, it follows that AlA′˜l has the same
sparsity as Al, see
§4.1.1≤
. Thus AlA′l+ has the same sparsity as Al. Similarly, A(l+1)+A′l has
the same sparsity as A′l.
˜
Consider levels l l. Let the kth group on level l (for k = 1,...,p ) be partitioned into
l
≤˜ ˜ ˜
p groups on level l, indexed by k,...,k + p 1. Let the partition of C into p
l,k,˜l l,k,˜l
−
l,k l,k,˜l
blocks for each group be defined as follows
 
C
l,k,1
.
C =  . . .
l,k  
C
l,k,p l,k,˜l
Then the kth diagonal block of the
AlA′˜l
is given by
(cid:16) (cid:17)
(AlA′˜l) = B CT blkdiag B′ C′T,...,B′ C′T
k l,k l,k ˜l,k˜ ˜l,k˜ ˜l,p
l,k,˜l
˜l,p
l,k,˜l
(cid:104) (cid:105)
= B
CT B′ C′T CT B′ C′T
l,k l,k,1 ˜l,k˜ ˜l,k˜ ··· l,k,p l,k,˜l ˜l,k˜+p l,k,˜l−1 ˜l,k˜+p l,k,˜l−1
T
= B C ,
l,k l,k,˜l
32where B ,C Rn l,k×r l are left and right factors of (AlA′˜l) . Computing
l,k l,k,˜l
∈
k
(C lT ,k,jB ˜l′ ,j)C ˜l′ ,T
k˜+j−1
∈
Rr l×n˜l,k˜+j−1, j = 1,...,p l,k,˜l,
where C
l,k,j
∈
Rn˜l,k˜+j−1×r l and C ˜l′ ,k˜+j−1,B ˜l′
,k˜+j−1
∈
Rn˜l,k˜+j−1×r˜l, takes O(n ˜l,k˜+j−1r lr ˜l) opera-
tions. Computing all coefficients of the right factor of
AlA′˜l
requires
(cid:88)p˜l p (cid:88)l,k,˜l
O(n r r ) = O(nr r ).
˜l,k˜+j−1 l ˜l l ˜l
k˜=1 j=1
Therefore, we have the following factorization
AlA′˜l
= blkdiag(B
CT
,...,B
CT
) = B
CT
,
l,1 l,1,˜l l,p l l,p l,˜l l l,˜l
where C has the same sparsity as B .
l,˜l l
˜
Similarly, for levels l l, we have
≥
AlA′˜l = blkdiag(B C′T,...,B C′T ) = B C′T,
˜l,1,l ˜l,1 ˜l,p˜l,l ˜l,p˜l ˜l,l ˜l
where the left factor B has the same sparsity as C , and it can be computed in O(nr r ).
˜l,l ˜l l ˜l
Thus
AlA′˜l
has the same sparsity as
A′˜l.
Combining the above we have the following factorization
L L
AlA′l+ +A(l+1)+A′l = (cid:88) B CT + (cid:88) B C′T
l l,˜l l,˜l l
˜l=l ˜l=l+1
(cid:104) (cid:105)(cid:104) (cid:105)T
= B (cid:80)L B (cid:80)L C C′ , (26)
l ˜l=l+1 l,˜l ˜l=l l,˜l l
which we can compute in
 
L L
(cid:88) (cid:88)
Onr
l
r ˜l′ +nr l′ r ˜l.
˜l=l+1 ˜l=l
Note that
(cid:80)L
B has the same row sparsity as B , and similarly,
(cid:80)L
C has the same
˜l=l+1 l,˜l l ˜l=l l,˜l
row sparsity as C′. Therefore, we can equivalently represent (26) as a product of two block
l
diagonal matrices by permuting the columns in the left and right factors accordingly. The
resulting two block diagonal matrices are the factors of AA′ on level l, and in the compressed
form have size n (r +r′) each.
× l l
Finally, from (25) we see that matrix AA′ is an MLR matrix with MLR-rank (r + r′).
Moreover, computing factors requires O(nmax r,r′ 2) operations.
{ }
33