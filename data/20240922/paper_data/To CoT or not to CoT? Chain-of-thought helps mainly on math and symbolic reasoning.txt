TO COT OR NOT TO COT? CHAIN-OF-THOUGHT HELPS
MAINLY ON MATH AND SYMBOLIC REASONING
ZayneSprague♠,FangcongYin♠,JuanDiegoRodriguez♠,DongweiJiang♢,
ManyaWadhwa♠,PrasannSinghal♠,XinyuZhao♠,
XiYe♡,KyleMahowald♠,GregDurrett♠
♠TheUniversityofTexasatAustin,♢JohnsHopkinsUniversity,♡PrincetonUniversity
zaynesprague@utexas.edu
ABSTRACT
Chain-of-thought(CoT)viapromptingisthedefactomethodforelicitingreason-
ingcapabilitiesfromlargelanguagemodels(LLMs). Butforwhatkindsoftasks
isthisextra“thinking”reallyhelpful?Toanalyzethis,weconductedaquantitative
meta-analysiscoveringover100papersusingCoTandranourownevaluationsof
20datasetsacross14models.OurresultsshowthatCoTgivesstrongperformance
benefits primarily on tasks involving math or logic, with much smaller gains on
othertypesoftasks.OnMMLU,directlygeneratingtheanswerwithoutCoTleads
toalmostidenticalaccuracyasCoTunlessthequestionormodel’sresponsecon-
tainsanequalssign,indicatingsymbolicoperationsandreasoning.Followingthis
finding, we analyze the behavior of CoT on these problems by separating plan-
ningandexecutionandcomparingagainsttool-augmentedLLMs.MuchofCoT’s
gaincomesfromimprovingsymbolicexecution, butitunderperformsrelativeto
usingasymbolicsolver. OurresultsindicatethatCoTcanbeappliedselectively,
maintainingperformancewhilesavinginferencecosts. Furthermore,theysuggest
aneedtomovebeyondprompt-basedCoTtonewparadigmsthatbetterleverage
intermediatecomputationacrossthewholerangeofLLMapplications.
Figure 1: Left: meta-analysis of CoT literature; each point is a reported delta of CoT over direct
answeringforsome(LLM,task)pair. Right: averageperformanceofusingzero-shotCoTv.s.di-
rect answer prompts across five general reasoning categories, covering 20 datasets with 14 LLMs
evaluatedoneach. Inbothsetsofresults, mathandotherkindsofsymbolicreasoningarethedo-
mainsthatconsistentlyseesubstantialimprovementsfromCoT(reddottedlineindicatesthemean
improvementfromCoTacrossexperiments).
1
4202
peS
81
]LC.sc[
1v38121.9042:viXra1 INTRODUCTION
Chain-of-thought (CoT) (Nye et al., 2022; Wei et al., 2022) has become a widely used prompting
technique for eliciting reasoning from language models. CoT can provide human-readable expla-
nations of how problems are solved (Joshi et al., 2023; Lanham et al., 2023), but most frequently
itisinvokedtoimproveanLLM’sabilitytoanswercomplexquestionsviaintermediatecomputa-
tion(Madaan&Yazdanbakhsh,2022;Wangetal.,2023a;Dzirietal.,2023). Currentpost-training
schemesforLLMsheavilyinfuseCoTcapabilitiesintomodels:systemslikeChatGPTorLlama3.1
defaulttoCoTwhengivenreasoningproblems(OpenAI,2023;Dubeyetal.,2024).
CoThasseenwidespreadusage,butitismostheavilyexploredinthedomainofmathematicalrea-
soning (Zhou et al., 2023a; Fu et al., 2023; Chae et al., 2024; Xu et al., 2024b; Qi et al., 2024).
Infact, many“reasoning”methodsforLLMsareevaluatedonlyinthemathdomain; forinstance,
Lightmanetal.(2024)frametheirpaperas“complexmulti-stepreasoning”andMixtral-Large2’sre-
lease1 citedeffort“enhancingthemodel’sreasoningcapabilities”,butperformanceisonlyreported
onGSM8KandMATH.CoTisreportedtobeeffectiveacrossawiderangeofstudies,butmanyof
thesestudiesfocusonanarrowsliceofthetaskspace. Inareasbeyondmath,resultsshowthatCoT
isnotasuseful(Kambhampatietal.,2024a)orcanevenhurtperformance(Wangetal.,2024).
In this work, we aim to evaluate where prompt-based CoT helps and why. We begin with a sys-
tematicmeta-analysisofrecentliteraturethatreportsperformanceofCoTversusdirectanswering
(DA).Wethenaugmentthispicturebyconductingexperimentson20datasetsand14contemporary
LLMs across zero-shot and few-shot prompt settings. Finding 1: CoT only helps substantially
on problems requiring mathematical, logical, or algorithmic reasoning. Figure 1 shows this
holds both across the literature and our own experiments. We find only a few cases of large gain
inotherkindsoftasks,andmanyoftheseoutliersfeaturesomecomponentofsymbolicreasoning.
Forinstance,onMMLU(Hendrycksetal.,2021a)andMMLUPro(Wangetal.,2024),weanalyze
theimprovementsfromCoTandfindthatCoTonlygivesbenefitonmathslicesofthedataset. As
much as 95% of the total performance gain from CoT on MMLU is attributed to questions
containing“=”inthequestionorgeneratedoutput. Fornon-mathquestions,wefindnofeatures
toindicatewhenCoTwillhelp.
How can we better understand why CoT improves on these questions and only these questions?
Themathandformallogicalreasoningdatasetsweconsidercanbebrokendownintotwostagesof
processing: aplanningstep(e.g.,parsingaproblemintoequations)andanexecutionstep(building
intermediateoutputsandworkingtowardsasolution)(Yeetal.,2023;Wangetal.,2023b;Sunetal.,
2024). Finding2:CoTprimarilyhelpswiththeexecutionstepthatperformscomputationand
symbolicmanipulation,butfallsshortofwhatLLMswithtoolaugmentationcando. Wefind
thatLMspromptedwithCoTcangenerateexecutableformalsolutionplansandexecutethoseplans
betterthandirectanswering. ButusingLMstogenerateasolutionplanandthenusinganexternal
symbolicsolvertosolvetheplanoutperformsusingCoTforbothstagesforthesetasks.
These results paint a picture that CoT’s utility is often circumscribed by tool augmentation: on
problems where CoT helps, we already have more powerful tools than CoT that we can employ,
and on “soft reasoning” problems like commonsense where no tools exist, we see limited benefit
from CoT. This characterization has two major implications. First, CoT is unnecessary for many
problems where it is widely employed: there exist more efficient prompting strategies that yield
similarperformanceformuchlowerinferencecost. Second,weseeacriticalneedtomovebeyond
prompt-basedCoTtomoresophisticatedapproachesbasedonsearch,interactingagents,ormodels
more heavily fine-tuned for CoT. Future work can explore how intermediate computation can be
betterusedtosolvechallengingproblemsoutsideofthemathandsymbolicreasoningdomains.
2 BACKGROUND: CHAIN-OF-THOUGHT
Thetasksweconsiderinthisworkconsistofaquestionq ∈ Σ∗ foravocabularyΣandananswer
a∈L(q)foralabelsetL(q). L(q)canconsistofadatatypelikebooleanorinteger,classification
labels, or problem-dependent labels like names of entities from q. One exception that we still
1https://mistral.ai/news/mistral-large-2407/
2explore is BiGGen Bench (Kim et al., 2024), which instead relies on an LLM-as-a-judge (Dubois
etal.,2023;Zhengetal.,2024b)toprovidealabelforgeneratedlong-formresponses.
Promptingandchain-of-thoughtforreasoning Alargelanguagemodelplacesdistributionsover
strings p(y) = (cid:81)n p (y ) where y ∈ Σ∗. In practice, we can interpret these as conditional
i=1 LM i
distributionsp(y | x)wherexisauser’sprompt. TypicalinvocationofanLLMinvolvesforming
apromptI(q)thatwrapsthequestionwithadditionalinstruction,thendrawingasampleresponse
y˜ ∼p(y|I(q)),andfinallyreturninga=extract(y˜)usingsomekindofanswerextractor.
For the reasoning tasks we consider in this work, the output y˜ can take one of multiple forms. A
direct answer only contains a string realization of a; e.g., y = ( 185,4) which is detokenized as
theanswera=1854. Achainofthoughtisalongersequenceyincludingothertokensbeyondthe
answer,e.g.,y = ( 185,6, minus, 2, equals, 185,4).2 Inbothcases,theextractfunctionmust
detokenizetheoutputandmaptheoutputtothecorrectdatatype;inchainofthought,thereissome
extraworktospotwheretheanswerisplaced,thoughthiscanusuallybedoneheuristically.
Ourpromptscanexplicitlyencourageuseofdirectanswerorchainofthoughtasstrategies,which
wedenoteasI andI . ForelicitingCoT,thisincludesstrategiesliketellingamodelto“think
da cot
stepbystep”(Kojimaetal.,2022). Fordirectlyansweringaquestion,apromptmaysay“immedi-
ately generate the answer”. In this latter case, we form I (q) = v (q) using a verbalizer v ,
da da da
which might concatenate the question with a description of the strategy like this (analogously for
I andv ). ItisnotguaranteedthatLLMspromptedtouseacertainstrategyactuallydo. How-
cot cot
ever, modern LLMs follow these instructions in the vast majority of cases. We track the average
locationoftheanswerinthegeneratedoutputforbothCoTanddirectpromptsinAppendixC.3to
ensurethatdirectanswerpromptsgivetheanswerearlyintheoutput. Wealsoensurethatextract
canparseanswersfromthegeneratedoutputforeachmodel,prompt,anddatasetcombinationused
inourexperiments,tailoringtheextractfunctionasneededtoensurelowunparseableratesforeach
model and task.3 All prompts and outputs per dataset per model have been uploaded to Hugging-
faceandweincludeexamplesofsomeofourpromptsintheAppendixH.Wealsoexperimentwith
few-shot CoT prompts, which we find perform similarly to zero-shot prompts; details about these
aregiveninAppendixB.
Symbolic reasoning Of key importance to this work is whether problems feature symbolic rea-
soning or not. We consider a problem to be symbolic if it can be grounded in a natural, well
agreed-uponformalsystem. “12×4”isanexampleofasymbolicproblem,whichcanbegrounded
inmathematics. Othersystemsincludefirst-orderlogic(Saparov&He,2023;Huaetal.,2024)or
planninglanguages(Liuetal.,2023a;Valmeekametal.,2023). Formally, forsymbolicproblems,
wedefineafunctionf thatactsasamapthatproducessomesymbolicexpressionS = f(q)from
thequestion. S canbeusedasinputforasolvertoderiveananswer,aˆ=solve(S).
Conversely,aproblemlikewhereonarivercanyouholdacupuprighttocatchwateronasunny
day? from CommonsenseQA (Talmor et al., 2019) is non-symbolic by our definition. While this
problemcouldbeformalizedwithsomekindofpredicatelogic(Zhouetal.,2022;Quanetal.,2024;
Zhouetal.,2024)orgroundedinsomekindofphysicalsimulation(Haoetal.,2023;Wongetal.,
2023),thereisnotanaturalnorwellagreed-uponframeworkforsolvingit.
We view non-symbolic to symbolic reasoning as a spectrum. MuSR (Sprague et al., 2024) is a
“semisymbolic” dataset in that it does contain an underlying formal system (e.g., for its murder
mysteriesportion,thenotionthatmotive(X)∧means(X)∧opportunity(X) =⇒ murderer(X)),
but also involves substantial commonsense reasoning that does not map onto a formal system. In
thesecases,wecanstillformS =f(q),butf mustrelyheavilyonalanguagemodelandinstantiate
newinformationforS thatisnotdirectlyrepresentedinq.
Centralclaims Figure1showsthattherearealargenumberofpositiveresultsonCoTreported
intheliterature. Informally,webelievemanyreadersoftheliteraturetoholdthefollowingview:
2Inthispaper,weonlyconsiderCoTsthatendwithananswer.Generatingapredictionfirstfollowedbyan
explanationdiffersonlyminorlyfromdirectanswerinthefew-shotsetting(Ye&Durrett,2022).
3Weexcludeanumberofother“CoT-like”approachesinouranalysissuchasdecomposedprompting(Khot
etal.,2023;Zhengetal.,2024a)andmulti-agentdebate(Duetal.,2023;Chenetal.,2024).Wefocusonsingle
promptapproaches.Wedealwithtool-augmentedapproachesinSection5.
3CentralConjecture
I willoutperformI onnearlyallreasoningproblems,whetherthoseproblemsinvolve
cot da
symbolicornon-symbolicreasoning.
Ourevidencedoesnotsupportthisconjecture.Wewillshowthatthisperformanceboostisstrongest
forsymbolicandsemi-symbolictasks, whilegivinglittletonoimprovement(orevenhurtingper-
formance)onquestionsthatarenon-symbolictasks.
3 RESULTS FROM THE LITERATURE
Wefirstperformameta-analysisofrecentpaperscomparingtheperformanceofpromptsI and
cot
I toidentifythetypesoftaskswhereCoThasbeenreportedtohelp.
direct
3.1 CRITERIAANDPROCESS
Automatic Selection and Paper Filtering We investigate all papers from ICLR 2024, a repre-
sentativeMLvenue,andtworepresentativeNLPvenues,EACL2024andNAACL2024(including
Findings and Workshop papers). We filtered all 4,642 papers (2,259 from ICLR 2024 and 2,382
fromthetwoACL-affiliatedconferences)forthosewithatleasttwooccurrencesof“CoT”,“chain-
of-thought”,or“chainofthought”,resultingin516papers. ThereareconceivablypapersusingCoT
called by another name (e.g., Scratchpads), but we believe these 516 give a representative sample
appropriateforsystematicanalysis.
Manual Paper Filtering and Results Extraction We then filter down to papers that perform a
comparisonofCoTpromptingvs.directprompting,whetherornotthisiscoretothepaper’sresearch
question. Wemanuallyfilteredthe516papersinquestionandextractedthekeyresultsfromthose
that remained. We excluded multimodal models, CoT-fine-tuned models, any experiments where
the“CoT”methodinvolvesmultipleforwardpasses(e.g.,self-consistency(Wangetal.,2023c)and
tree-of-thought(Yaoetal.,2023)),4 andsystemsthataugmentLLMswithexternaltools(discussed
moreinSection5).
Foreachpaperpassingthroughthesecriteria,wemanuallyextractedtheresultsfromkeytablescom-
paringCoTanddirectanswerprompts. WeonlyincluderesultswheretheCoTanddirectprompts
arerunonthesamemodelandsamedatasetwhilebeingonascaleof0to100(excludingLikertscale
evaluations,forexample)foramoredirectcomparison. WhenpapersincludevariousCoTordirect
answerprompts(includingzero/few-shotvariants),wealwaystakethebest-performingpromptfor
both.Wefocusonkeytestresultswhereapplicable,excludingdevsetsiftheyarereportedalongside
testandalsoexcludingnumbersfromablationsornonstandardsubsetsofdatasets.
Thisresultedinatotalof1,218experimentalcomparisonsacross110papers(35fromICLRand75
fromNAACLandEACL)covering264datasets. Detailsandmoreinformationcanbefoundinour
GitHubRepo: https://github.com/Zayne-sprague/To-CoT-or-not-to-CoT.
Categorization Giventhelargenumberoftasksanddatasetsbeingcompared, wegroupedeach
task into a set of 14 categories. These categories were determined based on the description (and
possibly examples) of the task, not taking into account system performance. These categories ab-
stract over traditional NLP task classifications (e.g., NER, reading comprehension) and take into
accountboththetaskformatandthekindsofreasoninginvolved. Definitionsforseveralcategories
areshowninTable1andthefulldescriptionisgiveninAppendixD.
3.2 RESULTS
Figure 2 shows the distribution of CoT deltas (CoT prompt minus the direct answer prompt per-
formance) across our categorization of different task types found in the literature. Compared to
4Thesesystemsusemorecomputethandirectanswer,andthereisnotaclearcomparisontobemadehere.
Moreover,ouranecdotalcoverageofthesemethodsshowsthattheyaremostusedformath,coding,andlogic
settings,forwhichwealreadyhavehighrepresentationamongreportedCoTmethods.
4Table 1: A few categories and their descriptions used to classify experimental comparisons. The
completelistofcategoriesandtheirdescriptionsisgiveninAppendixD.
Category Description
Symbolicandalgorithmic Tasksinvolvingsymbolmanipulationwhichcanbesolvedbyexecutinga
program. Thisincludesentitytrackingdatasets(e.g., SCONE,CoinFlip)
andalgorithmictasks(e.g.,BBHwordsortingorfindingshortestpathsina
graph).
Math Tasks requiring mathematical reasoning, from grade-school math to ad-
vancedmathematics,includingphysicsquestions.
Logicalreasoning Tasksdesignedtotestforlogicalreasoning,whetherdeductive(e.g.,PrOn-
toQA),inductive(Bowenetal.,2024)oranalogical(Maetal.,2024)rea-
soning,includingsyllogismsandlogicalpuzzles.
Encyclopedicknowledge Tasks requiring expert-level in-depth knowledge beyond mere common-
sense,usuallyinanopen-booksetting.
Mixeddatasets Datasetscontainingavarietyoftasks,suchasBIG-BenchHard(BBH)or
MMLU.
... ...
(5) Moral Scenarios
PoliMfact, Gossipcop, CoAID
(3) ScienceQA
(7) Legal Argument Reasoning
HotpotQA
(1) BBH
(9) BIG-bench Epistemic
(8) Commitment Bank
(6) Verbalized confidence
(2) Internal dialogue dataset
(10) BIG-bench Temporal
(4) BIG-bench Navigate
E-KAR
GSM8k
Le#er, Coin
Figure2: Resultsfromourmeta-analysis(greydots)aggregatedbypaperandcategory(bluedots).
Figure1,wetakethemeanresultsperpaperpercategory,indicatedbybluedots,showingthetrend
acrosspapersintheliterature. ThecategoriesarerankedinorderofascendingmedianCoTdelta.
ThethreecategorieswhichbenefitedthemostfromCoTaresymbolicreasoning,math,andlogical
reasoning, with average improvements of 14.2, 12.3, 6.9, respectively. Average performance on
these top three tasks with CoT was 56.9, whereas performance without CoT was 45.5. For other
categories, the average performance with CoT was 56.8, compared to 56.1 without CoT. We do
notconsiderthissmallimprovementavictoryforCoT.CoTinvolvesmorecomputationthandirect
5Table2: Models,datasets,andpromptingstrategiesusedinourexperiments. Modelsmarkedwith†
arerunwitha4kcontextsizewindow. NotethatGemmahasalargerthan4kcontextsizewindow,
but VLLM only supports up to a 4k context size window for it. Models marked with * indicate
closed-source models that cannot handle prefixed assistant messages. Datasets marked with △ do
nothaveafew-shotsetting.
Models Llama27BChat†(Touvronetal.,2023),Mistral7BInstructv0.3(Jiangetal.,2023),Llama
3.1 8B Instruct (Dubey et al., 2024), Llama 3.1 70B Instruct, Gemma 2 9B It† (Riviere &
et. al,2024),Phi-3Small8kInstruct(Abdinetal.,2024),gpt-4o-mini-2024-07-18*,gpt-4o-
2024-08-06*,Gemini1.5Flash*(Reid&et. al,2024),Gemini1.5Pro*(Reid&et. al,2024),
claude-3-haiku-20240307*(Anthropic,a),claude-3-5-sonnet-20240620*(Anthropic,b)
Datasets CommonsenseQA(Talmoretal.,2019), StrategyQA(Gevaetal.,2021), SiQA△ Sapetal.
(2019),PiQA△(Bisketal.,2019),Winogrande△(Sakaguchietal.,2021),GPQA(Reinetal.,
2023), MuSR(Spragueet al.,2024), ContextHub(Levels 1and 2only)(Hua etal., 2024),
ARC△ (Clarketal.,2018),AGIEvalLSAT(Zhongetal.,2023),MMLU(Hendrycksetal.,
2021a),MMLUPro(Wangetal.,2024),MATH(Hendrycksetal.,2021b),GSM8K(Cobbe
etal.,2021),GSM8K-hard(Gaoetal.,2023),FOLIO(Hanetal.,2022),MuSiQue△(Trivedi
etal.,2022),Big-BenchHard(Suzgunetal.,2023;Srivastavaetal.,2022),BiGGenBench
(Kimetal.,2024)
Prompts zero-shotdirectanswer,zero-shotCoT(Kojimaetal.,2022),few-shotdirectanswer(Brown
etal.,2020),few-shotCoT(Weietal.,2022)
answering,andatrulyfaircomparisonbetweenthemethodsshouldmatchthecomputeofthetwo
methods,e.g.,ensemblingacrossmultipleprompts.
Do any non-math datasets benefit from CoT? On the right side of Figure 2, we show the top
10 outliers from our observed trend, namely papers with high CoT deltas averaged across experi-
mentsintasksotherthanmath, symbolic, orlogicalreasoning. Althoughnotcategorizedasmath
or logic, several of these are related to logical, mathematical or symbolic reasoning in some way.
Fromthislist,thedatasetwhichbenefitsthemostmostfromCoTisBIG-benchHard(BBH)(Suz-
gun et al., 2023), a benchmark consisting largely of problems requiring algorithmic, arithmetic or
logical reasoning. For instance, BIG-bench Navigate is a spatial reasoning task, but relies heavily
on a mathematical primitive of counting steps taken to derive a final conclusion. Similarly, while
BIG-benchTemporalisatemporalreasoningtask(answeringquestionsaboutwhencertainevents
couldhaveoccurred),itrequiresdeductivereasoningtosolve. Inaddition,LegalArgumentReason-
ing (SemEval-2024 Task 5) (Bongard et al., 2022) was categorized as context-aware QA, but also
requires substantial reasoning ability. Finally, MMLU-Moral Scenarios (Hendrycks et al., 2021a)
requiresansweringtwoindependentquestionsatonce,whichessentiallyinvolvesasymboliccom-
binationoftwosimplerquestions.
There are a few outliers that less clearly follow the trend. ScienceQA (Lu et al., 2022) consists
of multiple choice questions across a range of natural and social science disciplines, though it is
hard to interpret gains without knowing breaking down performance by subject or question type.
The dialogue evaluation dataset from Jia et al. (2024) sees large improvements with CoT, but this
isaproprietarydataset,andwenotethatotheressayscoringresultsinourmeta-analysis(Lietal.,
2024;Stahletal.,2024)didnotshowimprovementswithCoT.Othernon-math,symbolicorlogical
datasets that benefit from CoT are Commitment Bank (de Marneffe et al., 2019) and the task of
elicitingverbalizedconfidence(Xiongetal.,2024).
Nevertheless,theseareexceptionstotherule. ThemajorityofthereportedbenefitsfromusingCoT
intheNLPandMLliteraturecomesfrommathormath-relatedtasks.
4 RESULTS FROM EXPERIMENTS
OuranalysisoftheliteratureshedslightonthebehaviorofCoT,butstillleavesopenquestionsabout
thebehaviorofthenewestmodelsandapples-to-applescomparisonsacrossmodelclasses,datasets,
andpromptingtechniques. Tofurtherourcharacterization, weperformaseriesofexperimentson
20datasetsacross14modelsinboththezero-shotandfew-shotsettingtocompareperformance.
6Figure3: Left: PerformancegainfromusingCoTforeachreasoningcategory. Right: Performance
gainfromusingCoTforeachdataset,averagedacrossmodelsandbrokenoutacross5representative
models. Red lines indicate median improvement. In both plots we see a consistent trend: most
improvements from using CoT are from math and symbolic reasoning. This trend remains true
acrossmodelcapabilities.
4.1 EXPERIMENTALSETUP
Dataset,Models,Prompts Table2liststhemodels,datasets,andpromptingtechniqueswecon-
sider for our experiments (more details, including the dataset composition of each reasoning cat-
egory, in Table 4 and Table 5 of Appendix A). We restricted our experiments to English models
commonlyusedandbenchmarkedongeneralreasoningdatasets. Thisexcludesmath-specificlan-
guagemodelslikeDeepSeekMath-Instructmodels(Shaoetal.,2024)aswellasnon-Englishmodels.
Wealsofocusoninstruction-tunedlanguagemodels. Ourdatasetsincludethosewhicharewidely
usedinCoTandreasoningliterature,includingamixofnon-symbolic,semisymbolic,andsymbolic
reasoning. Theyspandifferentformats,includingmultiple-choice,short-answer,andfree-response;
however, most of these datasets are multiple choice or short answer, as CoT is not typically used
inlong-formresponsesettings. Wealsocategorizeeachdatasetintoalargercategoryofreasoning
requiredtosolveit: Commonsense,Knowledge,Symbolic,Mathematical,andSoftReasoning. We
defineSoftReasoningasquestionsrelyingoncommonsenseandnaturallanguagebutgoingbeyond
simpleinferencesaboutthesestatements. Finally,weexploreseveralpromptingstrategiesforelic-
itingreasoningfromlanguagemodels, aspastworkhasemphasizedtheimportanceoftheprompt
(Yangetal.,2024). However,wegenerallyfoundslightperformancedifferences; seeAppendixG
for details. We therefore focus on prompts similar to Kojima et al. (2022) and Wei et al. (2022)
forzero-shotandfew-shotsettings, respectively, withalterationstoimprovethemodel’sabilityto
producedesiredbehavior(i.e.,formatsthatallowforeasilyparsedanswers).
ImplementationDetails Weuseahigh-throughputinferencepackage,vLLM(Kwonetal.,2023),
forthemodelinferenceprocess.Weusegreedydecodingonallmodels.Ourpromptsaretakenfrom
theLlama3.1evaluationswhenavailable(Dubeyetal.,2024),andminoradjustmentsaremadeto
unify prompting strategies. For other datasets, we either use the standard prompt for the dataset
fromthecorrespondingoriginalpaperorimplementourownprompt.5 Ouranswerparser(extract)
is tailored to each dataset and model. Specific details about each dataset, its prompts, and answer
extractorcanbefoundinAppendixA.
5PromptsandoutputsforourevaluationsareonHuggingface,https://huggingface.co/collections/
TAUR-Lab/cot-analysis-project-66bbb9e5e0156e65059895f5
74.2 RESULTS
Where does zero-shot CoT improve over direct prompts? On datasets that require math
(MATH,GSM8K)orformallogic(ContextHub,MuSRtoalesserdegree)toanswertheproblem.
Figure3ontheleftshowstheaverageCoTperformanceimprovementforeachreasoningcategory
fromFigure1(right);rawnumberscanbefoundinTable6oftheAppendix. Ontheright,Figure3
shows the performance gain from using CoT for each dataset, averaged across all models and for
a selection of individual models. On non-symbolic reasoning categories and datasets, specifically
thosethatcontainquestionsprimarilyinvolvingcommonsense(CSQA,PIQA,SiQA),languageun-
derstanding(WinoGrande),andreadingcomprehension(AGILSAT,ARC-Easy,ARC-Challenge),
thereislittletonoseparationbetweentheperformanceofzero-shotCoTandzero-shotdirectanswer.
Despitethesedatasetsinvolvingreasoning,CoTdoesnotyieldimprovement.
Bycontrast,themathematicalandsymboliccategoriesgetlargerboostsinimprovementsalongside
symbolic and many semi-symbolic datasets. MATH and GSM8k show gains as large as 41.6%
and66.9%,respectively.Thesemi-symbolicdatasetslikeContextHubandMuSRMurderMysteries
showmoderategains.Thesedatasetsrequiretheapplicationoflogicalrulestoreachtheanswer,e.g.,
first-orderlogicparsedfromsimplenaturallanguage(ContextHub)ormorecomplexcommonsense
statements(MuSRMurderMysteries). AllresultsareshownintheAppendixC.1aswellasafull
listofnumericresultsforbothCoTanddirectanswerpromptinginTable7. Wealsoexploredthe
few-shotsettingandfoundithadlittleimpactonwhenCoTwillhelp;seeAppendixB.
Does the answer format impact where CoT will help? Not much. Free response capabilities
maybehinderedbypre-planningorreasoningaboutthecorrectresponse.
Manyofthecommonly-useddatasetsforproblemsotherthanmatharemultiplechoice.Wehighlight
herethatCoThassimilarperformancetodirectansweracrossmodelsfortwodatasetsthatarenot
multiple-choiceandcontainvaryinglevelsofnon-symbolicreasoningtoanswerthequestion. First,
MuSiQue(Trivedietal.,2022)isashort-formQAtaskrequiringmulti-hopreasoning. Weconsider
thisasemi-symbolicdatasetasthequestionshaveanexplicitmulti-hopstructure. Becauseanswer
spansinMuSiQuecanbeparaphrasedinmanydifferentways,weuseGPT-4otojudgeiftwoanswer
spansareequivalent. Despitebeingsemi-symbolic,weseenooverallimprovementfromCoT.
Second,BiGGenBench(Kimetal.,2024)usesfree-formresponsesastheanswertoaquestion,and
an LLM-as-a-judge is used to evaluate these responses on a scale of 1 to 5. The free-form nature
oftheanswersblursthelinesbetweenCoTanddirectanswer. However,wedevisedaCoTprompt
for this setting where we ask the language model to generate a plan for the free-form response
(the reasoning part), and then we ask it to generate the full response (the answer part) all in one
generation. We then only give the response to the judge. We use GPT-4o mini as the judge with
thepromptfromKimetal.(2024). WealsoexcludeslicesfromBiGGenBenchthatasktheLLM
to“Thinkstepbystep”withinthequestion, ascomparingittodirectanswerisdifficultwiththese
prompts. We plot the performance of BiGGen Bench as the number of times a prompt receives a
scoreof4orbetteroneachquestion. CoTleadstomildoverallimprovementhere,whichweexpect
toseegiventhebenchmark’sinclusionofreasoningquestions(includingseveralcategoriesofmath)
andothercategoriessuchasplanning.
ArethegainsinKnowledge,SoftReasoning,andCommonsensesignificant? Mostlyno,except
forMMLU,StrategyQA,andMuSR.
WetestedthesignificanceoftheimprovementsfromCoTonthe13datasetsintheKnowledge,Soft
Reasoning, and Commonsense reasoning categories using paired bootstrapping to assess whether
CoTgivesasignificantimprovement.Toaccountformultiplecomparisons,weappliedaBonferroni
correction, setting the p-value to 0.00027 to account for the 14 models and 13 datasets. About
38% (58) of the datasets that show a benefit in these three reasoning categories were considered
significant. Nearlyhalfofthesecomparisons(26)areonMMLUandMMLUPro,whichwestudy
more closely in the next section. StrategyQA and MuSR also received a consistent performance
boostacross9and6modelsrespectively.StrategyQAisoftenusedtobenchmarkreasoningmethods
andisbuiltspecificallytogetabenefitfrommethodsthatdecomposethequestionintosteps, soa
gain in performance is not unprecedented. MuSR, similarly, was built to have multiple steps of
8Table3: Thetop3slicesbenefitingthemostfromCoTacrossMMLUandMMLUProforLlama
3.18band70b. 6outof12ofthesetopslicesdirectlycontain“math”or“mathematics.” Wedive
deeperintoeachcategorysubsequentlyandobservethatthequestionsleadingtoimprovementsin
theothercategoriesaremathematicalinnatureaswell.
MMLU MMLUPro
Model Subject Direct(%) CoT(%) Err.Red.(%) N Subject Direct(%) CoT(%) Err.Red.(%) N
Llama3.18b elementarymathematics 46.8 88.4 78.1 378 math 23.6 44.8 27.8 1350
Llama3.18b highschoolmathematics 39.6 71.5 52.8 270 business 29.4 45.6 23.0 789
Llama3.18b miscellaneous 83.9 89.9 37.3 783 physics 27.9 41.4 18.8 1299
Llama3.170b elementarymathematics 82.3 94.7 70.1 378 math 44.5 68.3 42.9 1351
Llama3.170b medicalgenetics 93.0 97.0 57.1 100 business 44.0 67.8 42.5 789
Llama3.170b highschoolmathematics 61.5 82.2 53.8 270 chemistry 40.5 64.0 39.6 1132
Figure4: CoTdeltasbetweenMMLUandMMLUProperformancewhenaquestionorgenerated
responsecontainsan“=”(With=)ornot(Without=). Wefilteroutanyquestionsthatdonotresult
inafinalanswer(degeneration,etc.). CoTprimarilyhelpsonthepairsofquestionsandgenerations
thatcontainan“=”,whichindicatesmath-relatedquestions.
complexnaturallanguagereasoning,whichmayreceivebenefitsfromCoT.Theremainingdatasets
thatreceivesignificantbenefitsarespreadacrossthedatasetsandmodels.
4.3 ZOOM-IN: MMLUANDMMLUPRO
MMLUandMMLUProshowgainsfromaddingCoT,butbecausethesedatasetsaresobroad,they
defy simple characterization. We explore the performance of CoT on each category of MMLU to
understanddivergencesinCoTperformancebetweenthesedomains.Welistthetopthreecategories
whereCoTgivesthelargesterrorreductionforLlama3.18Band70BonMMLUandMMLUPro
inTable3. Someofthesecategoriesareexplicitlymathematicalinnature,aswemightexpectfrom
Figure8. WecanalsoseethatCoTishelpingoncategorieslike“business”;uponcloserinspection,
wefoundthatthesecategoriesfrequentlyinvolvemathaswell(e.g.,businessquestionsmayinvolve
computationssurroundingwealth). WeneedtomorecarefullycharacterizeMMLUattheinstance
level. Indoingso,wecantestourhypotheseswithmuchfinergranularitythanpossiblebyrelying
onsubjectivegroupingsintotasksandcategories.
Breakdownbythepresenceofequations Weaimtodesignaninstance-levelclassifiertodeter-
mineifCoTisexpectedtohelponaquestionornot. Thatis, wewantafunctiong : q → {0,1}
whereg(q)returns1ifextract(y˜ ) = y∗ andextract(y˜ ) ̸= y∗ wherey∗ isthegoldanswer
cot da
to q. We explored different forms of g; however, we ultimately found it most effective to use a
classifierg : (q,y˜ ) → {0,1}whichalsoconsultsthechain-of-thoughtproducedbythemodel.
cot
This allows us to featurize how the LM solves the problem, particularly whether it uses symbolic
reasoningornot.
9Figure5: PromptvariantsthatseparateplanningandexecutionforGSM8K.Forallpromptvariants
besidesdirectanswerandCoT(notshown),wefirstfew-shotpromptanLLMtogenerateaPython
program as a solution plan. For Plan + Direct Solver, the LLM is prompted to directly give an
answerfromtheplan; forPlan+CoTSolver, theLLMispromptedtosolvetheplanstep-by-step
withCoTandgiveananswer;forPlan+ToolSolver,wefeedtheplanintoaPythoninterpreter.
We find that g can be implemented with a single feature: does q or y˜ contain a “=”? The “=”
cot
tokenverystronglyindicatesthepresenceofequationsintheproblemoritssolution,whichturnout
tobeastronghallmarkofsymbolicreasoning.6
We plot the overall CoT delta (performance of CoT minus the performance of direct answer) for
bothMMLUandMMLUProacrossmultiplemodelsbetweentwobinsaccordingtothisclassifier
g, labeled as “With =” and “Without =”, in Figure 4. We also report the amount of performance
gainexplainedbyquestionshavingan“=”vs.notinAppendixE.Wefindthatthemajorityofthe
performancegainfromCoTonMMLUandMMLUProcomesfromquestionsthathavean“=”in
the question or generated responses. Because “=” are usually found in math problems, we equate
this to CoT primarily benefiting MMLU and MMLU Pro on the math-related questions with very
littletonogain(dependingonthemodel)fornon-mathquestions.
5 STRENGTHS AND WEAKNESSES OF COT AT FORMAL REASONING
Previous sections establish that CoT primarily helps with symbolic reasoning tasks, but not why.
Manysymbolicandsemi-symbolictasksbebrokendownintotwostages(Yeetal.,2023;Panetal.,
2023;Jiangetal.,2024): planning,eitherviaaformalorinformalspecificationviaprompting(Sun
et al., 2024; Wang et al., 2023b), and execution, using the same LM or external solvers. In this
section,weattributetheperformancegainsfromCoTonsymbolictaskstothesetwostages.
Given a question that requires symbolic reasoning, we define the planning stage as extracting all
variables from the context into a formal specification and defining their relations. The execution
stageusesasolverthattakesasinputaplanandcanberuninanorderlyfashiontoderivethefinal
answer. UsingournotationfromSection2,letf(q) = Im (q)beamappingofthequestion
planning
qtoasymbolicplanS thatcanbeexecutedbythelanguagemodelorbyanexternalsymbolic
plan
solver,aˆ=solve(S ),whereaˆisthefinalanswerforq.
plan
Byseparatingplanningandexecutioninthisway,wecantesthowmuchalanguagemodelcangain
fromjustknowinghowtosolveaproblem(onlyhavingaplan),tohavingaplanandbeingableto
reasonaboutitsoutput(havingaplanandsolvingitwithCoT),ortohavingaplanandthensolveit
withanexternalsymbolicsolver. GivenaplanS ∼ Im (q),wecomparetheperformance
plan planning
ofthesettingsbelowtoevaluateatwhichstageLMismosteffectiveandfallsshort.
6We explored implementing g with a logistic regression classifier with tf-idf features over the (q,y˜ )
cot
pairs, trainedoverasubsetofthedatafromMMLUandMMLUPro. Thisclassifieractuallyallowedusto
discoverthe“=”feature,butitsaccuracydidnotexceedtheaccuracyofthatsinglefeature.
10Figure6: Performanceofpromptvariantsthatseparateplanningandexecutionformathandlogical
reasoningdatasets. Despiteoutperformingdirectanswerforsolvingaformalplanandderivingthe
finalanswer,CoTisstilllimitedinperformingsymboliccomputations: thereisalargeperformance
boostfromPlan+ToolSolveroverCoTandPlan+CoTSolveronaverageacrossallmodels.
5.1 SETTINGSEVALUATED
Settings1and2: Few-shotdirectanswerandCoT: Weusethefew-shotdirectanswerandCoT
promptsfromSection4.1asbaselines. Figure5includesanexampleofeachsettingonGSM8K.
Settings3and4: Plan+DirectSolverandPlan+CoTSolver: Hereweuseinspirationfrom
Xuetal.(2024a)andgenerateasymbolicplanusingthesamestrategyasYeetal.(2023). Specif-
ically, we use a few-shot prompt Im that is meant to generate a formal specification S
planning plan
thatshouldbeexecutablebyasymbolicsolver.7 InthesamepromptLMsareaskedtosolvetheir
generated specification S and derive the final answer y˜ ∼ p(y | I (S )), either directly
plan da plan
givingtheansweraftergeneratingthespecification(Plan+DirectSolver)orprovidingatraceof
the plan (step-by-step explanations and tracking of intermediate steps) for the derivation (Plan +
CoTSolver). Particularly, S isaPythonprogramforthemathdatasets, andisasetofformal
plan
specificationsinfirst-orderlogicforthelogicalreasoningdatasets.
Setting5:Plan+ToolSolver WethenevaluatehoweffectiveCoTcanbeatperformingsymbolic
computationscomparedwithexternalsymbolicsolvers. FollowingpriorworkonaugmentingLMs
with tools for math and logic questions (Ye et al., 2023; Pan et al., 2023; Gao et al., 2023; Chen
et al., 2023), we generate S the same way as in CoT Solver, but now feed in the plan into a
plan
symbolicsolver(PythoninterpreteroraSMTSolver),suchthataˆ=solve(S ).
plan
EvaluationSetup Wecomparetheperformanceofeachsettingonmath(GSM8KandGSM8K-
Hard) and formal logical reasoning (ContextHub and FOLIO) datasets. Recent LLMs might be
overfitting GSM8K by data contamination (Zhang et al., 2024), so we follow Gao et al. (2023) to
includeGSM8K-Hard,aminimallymodifiedharderversionthatreplacesnumbersofGSM8Kwith
largernumbers,toaccountforthepotentialcontamination.
ForPlan+DirectsolverandPlan+CoTsolver,weusethefew-shotpromptsfromYeetal.(2023).
ForPlan+Toolsolver,weusestate-of-the-arttool-augmentedpromptingmethodsusingsymbolic
solvers. Particularly,forGSM8K,weuseProgram-aidedLanguageModel(Gaoetal.,2023,PAL)
that executes the LM-generated plan with a Python interpreter. For the logical reasoning dataset,
weuseSatisfiability-AidedLanguageModel(Yeetal.,2023,SatLM)thatusesautomatedtheorem
proverZ3(DeMoura&Bjørner,2008)tosolvethegeneratedspecifications. IftheLM-generated
7Notethatwedonotenforceanycorrectnessontheoutputforthissetting. Instead,weassumethatgiven
theexamplesintheprompt,thelanguagemodelwillgenerateafairlycompletespecification.
11plan cannot be parsed by the tool, we use random guessing when the question is multiple choice,
andmarkitincorrectotherwise.
5.2 EVALUATIONRESULTS
Figure6showstheresultsacrossarepresentativeselectionofmodels. Detailednumericalresults,
includingtheunparseableratesofmodel-generatedplans,canbefoundinAppendixF.
When comparing direct answer with Plan + Direct solver and Plan + CoT solver, we note that for
manydatasetsandmodels, onlyhavingaplandoesnotaccountformostoftheperformancegain.
Compared with direct answer, CoT or Plan + CoT solver is needed for strong performance.
Tracking the execution with one of these methods gives the strongest accuracy benefit, espe-
ciallyformath-heavydatasets.
DespitetheirstrengthoverdirectanswerandPlan+Directsolver,CoTandPlan+CoTsolverare
dominated by Plan + Tool solver in most settings. LLMs are limited by their ability to execute
andtrackstepscomparedwithsymbolicsolvers.
WearguethattheseresultsprovideanexplanationofwhyCoThelpsonsymbolictasks. Whileall
taskscouldfeasiblybenefitfromadetaileddescriptionofhowtosolveeachindividualquestion(e.g.,
aplaninthecontextofthissection),CoTonlyoutperformsdirectanswerwhenthesestepsrequire
a substantial amount of tracing and computation. In these settings, we can see clear performance
benefitfromusingsymbolicsolvers;CoTappearstobeapoor(butuniversal)approximationto
suchsolvers. Whenpossible,LLMsshouldbepairedwithsymbolicsolverswhensolvingsymbolic
taskstoachieveconsistentlybetterperformanceoverdirectanswerandCoT.
6 DISCUSSION AND RELATED WORK
Where is CoT helping and why? Our results showing CoT improvement for math and logic
aligns well with early work on CoT for LLMs such as Scratchpads (Nye et al., 2022). As CoT
gained popularity, its application has broadened to tasks that canonically do not require multiple
steps. Itcanoftenyieldsmallimprovementsoverdirectanswering. Webelievethisledtothecur-
rentprevailingsentimentthatdeliberationshouldimproveperformanceonanytaskrequiringsome
typeofreasoning(ouroriginalclaimfromSection2). However,ourresultsshowaclearseparation
betweenperformanceonnon-symbolicandsymbolictasks. If,intheory,anyquestioncouldbenefit
from deliberation, why is CoT only benefiting the questions that can be solved through symbolic
manipulation? Our results from Section 5 suggest that the primary benefit of CoT comes in the
abilitytoexecutesymbolicstepsandtracktheiroutput. Notalltaskshavethisfeature: forexample,
questions from CommonsenseQA can hardly be translated into formally grounded and executable
solution plans. Datasets like StrategyQA may feature multiple steps of reasoning, but executing
those steps is not complex, so the benefits of CoT are small. It is unclear whether explicitly in-
stillingmodelswithparticularmodesofdeliberation,likeprocessofeliminationformultiplechoice
questions,mightmakethemmoreeffectivefornon-symbolictasks,orwhetherthere’safundamental
limitationimposedbytheirpre-trainingdata. Weleavethisdistinctionforfuturework.
LongHorizonPlanning Onesetoftaskswheresymbolicreasoninghelpssubstantiallythatour
experimentshaven’tcoveredasthoroughly(withtheexceptionofBiGGen-Bench)islong-horizon
planning(Valmeekametal.,2023;Xieetal.,2024;Gundawaretal.,2024;Valmeekametal.,2024).
There are two reasons we don’t treat it here. First, we are primarily interested in tasks that are
conveyedinlanguage,andweseelesscomplexplanninginlanguage-onlytasks. Second,therehas
alreadybeenalargedebateontheeffectivenessofCoT,bothpro(Huangetal.,2022;Huetal.,2023)
andagainst(Valmeekametal.,2023;Kambhampati,2024;Kambhampatietal.,2024b;Stechlyetal.,
2024a; Guan et al., 2024; Verma et al., 2024; Gundawar et al., 2024; Stechly et al., 2024b) using
CoTanditsderivativesliketree-of-thought(Yaoetal.,2023;Kangetal.,2024),thathasresultedin
complexsystemstohelpsolveplanningproblemsbetter. Whilestorygenerationandinterpretation
involveelementsofplanningwithnaturallanguage(Pengetal.,2022;Karpinskaetal.,2024),such
tasksarenotconventionallyformalizedandbenchmarkedasplanningandreasoning.
12Can we improve CoT further? Our work treats chain-of-thought variants that explicitly don’t
involve multiple inferences. But there is some evidence that using additional calls to LLMs can
help(Duetal.,2023;Yaoetal.,2023;Bestaetal.,2023;Chenetal.,2024). Onechallengeisthat
these methods use significantly increased computation; careful benchmarking sometimes reveals
thatnaivetechniquesareasgoodasiterativeones(Olaussonetal.,2024). However,pasttheoretical
resultshaveshownthatTransformersareaugmentedinafundamentalwaybyCoT(Liuetal.,2023b;
Merrill&Sabharwal,2024);webelievethisdoesindicatethepotentialforimprovedvariantsofCoT
beyondprompt-basedCoT.Ontheotherhand,recentmethodsshowingbenefitfrom“internalizing”
CoT(Dengetal.,2024)mayindicatethatexplicitgenerationofintermediatetokensisstillnotbeing
usedtoitsfullpotential.
Datasetcontamination Onelimitationofourstudyisthepresenceofpossibledatacontamination:
it is unknown which benchmarks may have been explicitly pre-trained on by language models.
If a model had memorized answers to benchmark questions, we would expect direct answering
to close some of the gap with CoT, as the model can just reproduce a known answer rather than
deriving it from scratch. We argue there are four reasons that our general conclusions are still
trustworthy. First,weusearangeoflanguagemodelscales,includingsmallmodelsthathaveless
capacitytomemorize. Second,datasetswithpoordirectansweringperformancelikeGSM8k-Hard
are unlikely to have been substantially memorized. Third, the inclusion of recent datasets such as
MuSR(Spragueetal.,2024)andBiGGenBench(Kimetal.,2024)helpstodefraythisrisk. Fourth,
oursurveyoftheliteratureincludespapersthatweresubmittedtoconferencesin2023,representing
arangeofolderLLMstrainedatvarioustimes.
7 CONCLUSION
In this work, we characterize the performance of prompt-based CoT through a meta-analysis of
the literature and experiments across different models, datasets, and prompts. We find that CoT
predominantlyhelpsonmathandformallogictasksregardlessofincludingexamplesintheprompt,
usingdifferentquestionformats,orrunningonstrongermodels. WeanalyzeCoT’sbehaviorfurther
andfindthatamajorityoftheperformancegainisconsistentlyattributedtotracingtheintermediate
steps of a problem, which symbolic solvers are better suited for and thus CoT rarely outperforms
them. WebelievethatCoTremainsapowerfultechnique,buttogiveimprovementacrossawider
rangeofNLPtasks,researchshouldmovebeyondprompt-basedCoTtonewparadigmslikesearch,
interactingagents,orbetterfine-tunedmodels.
8 REPRODUCIBILITY
For our experiments, we provide in-depth details of how we evaluated models on each dataset in
Section4.1andAppendixA.Furthermore,wereleaseallpromptsforeverydatasetonHuggingface,
including per model output and sampling parameters. For our meta-analysis of the literature, we
describe our filtering criteria and process of annotating experiments into high-level categories in
Section3andAppendixD.Wealsoreleasethefulllistofpapersinourmeta-analysistogetherwith
extractedexperimentalcomparisonsandtaskcategoryannotations.
ACKNOWLEDGMENTS
We acknowledge George Tsoukalas for providing insightful feedback throughout the project. We
also thank Kaj Bostrom and Eunsol Choi for reviewing and providing feedback on drafts of the
work. ThisworkwaspartiallysupportedbyNSFCAREERAwardIIS-2145280(toDurrett), NSF
CAREERAward2339729(toMahowald),theNSFAIInstituteforFoundationsofMachineLearn-
ing(IFML),theSloanFoundationviaaSloanResearchFellowship,andagrantfromOpenPhilan-
thropy.
REFERENCES
MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,AhmedAwadallah,HanyHas-
san Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Singh Behl, Alon Ben-
13haim,MishaBilenko,JohanBjorck,Se´bastienBubeck,MartinCai,CaioC’esarTeodoroMendes,
WeizhuChen,VishravChaudhary,ParulChopra,AllisonDelGiorno,GustavodeRosa,Matthew
Dixon,RonenEldan,DanIter,AbhishekGoswami,SuriyaGunasekar,EmmanHaider,Junheng
Hao,RussellJ.Hewett,JamieHuynh,MojanJavaheripi,XinJin,PieroKauffmann,NikosKaram-
patziakis,DongwooKim,MahoudKhademi,LevKurilenko,JamesR.Lee,YinTatLee,Yuanzhi
Li,ChenLiang,WeishungLiu,EricLin,ZeqiLin,PiyushMadan,ArindamMitra,HardikModi,
AnhNguyen,BrandonNorick,BarunPatra,DanielPerez-Becker,ThomasPortet,ReidPryzant,
Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olli Saarikivi, Amin Saied, Adil
Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xianmin Song, Olatunji
Ruwase, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Ji-
ahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Cheng-Yuan Zhang, Cyril Zhang,
JianwenZhang,LiLynaZhang,YiZhang,YunanZhang,andXirenZhou. Phi-3technicalreport:
A highly capable language model locally on your phone. ArXiv, abs/2404.14219, 2024. URL
https://api.semanticscholar.org/CorpusID:269293048.
Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku. a. URL https://www-cdn.
anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model Card Claude 3.
pdf.
Anthropic. Claude 3.5 Sonnet Model Card Addendum. b. URL https://www-cdn.anthropic.
com/fed9cc193a14b84131812372d8d5857f8f304c52/Model Card Claude 3 Addendum.pdf.
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Giani-
nazzi, JoannaGajda, TomaszLehmann, HubertNiewiadomski, PiotrNyczyk, andTorstenHoe-
fler. Graphofthoughts: Solvingelaborateproblemswithlargelanguagemodels. InAAAICon-
ferenceonArtificialIntelligence,2023. URLhttps://api.semanticscholar.org/CorpusID:
261030303.
YonatanBisk,RowanZellers,RonanLeBras,JianfengGao,andYejinChoi.Piqa:Reasoningabout
physicalcommonsenseinnaturallanguage. InAAAIConferenceonArtificialIntelligence,2019.
URLhttps://api.semanticscholar.org/CorpusID:208290939.
LeonardBongard,LenaHeld,andIvanHabernal. Thelegalargumentreasoningtaskincivilproce-
dure. InNikolaosAletras,IliasChalkidis,LeslieBarrett,Ca˘ta˘linaGoanta˘,andDanielPreotiuc-
, ,
Pietro (eds.), Proceedings of the Natural Legal Language Processing Workshop 2022, pp. 194–
207, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computa-
tionalLinguistics. doi: 10.18653/v1/2022.nllp-1.17. URLhttps://aclanthology.org/2022.
nllp-1.17.
ChenBowen,RuneSætre,andYusukeMiyao. Acomprehensiveevaluationofinductivereasoning
capabilitiesandproblemsolvinginlargelanguagemodels.InYvetteGrahamandMatthewPurver
(eds.), Findings of the Association for Computational Linguistics: EACL 2024, pp. 323–339,
St. Julian’s, Malta, March 2024. Association for Computational Linguistics. URL https://
aclanthology.org/2024.findings-eacl.22.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Asso-
ciates,Inc.,2020.URLhttps://proceedings.neurips.cc/paper files/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak,
MoohyeonKim,SeonghwanKim,TaeyoonKwon,JiwanChung,YoungjaeYu,etal. Language
Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in
LanguageModels. arXivpreprintarXiv:2404.02575,2024.
14ChihYaoChen,SwarnadeepSaha,andMohitBansal. Reconcile:Round-tableconferenceimproves
reasoningviaconsensusamongdiverseLLMs,2024. URLhttps://openreview.net/forum?
id=Yol6nUVIJD.
WenhuChen,XueguangMa,XinyiWang,andWilliamW.Cohen. Programofthoughtsprompting:
Disentanglingcomputationfromreasoningfornumericalreasoningtasks. TransactionsonMa-
chineLearningResearch,2023. ISSN2835-8856. URLhttps://openreview.net/forum?id=
YfZ4ZPt8zd.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,and
Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning
Challenge. arXiv:1803.05457v1,2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Trainingverifierstosolvemathwordproblems. ArXiv,abs/2110.14168,2021. URL
https://api.semanticscholar.org/CorpusID:239998651.
Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: In-
vestigatingprojectioninnaturallyoccurringdiscourse. InProceedingsofSinnundBedeutung23,
2019.
Leonardo De Moura and Nikolaj Bjørner. Z3: An efficient SMT solver. In Proceedings of the
TheoryandPracticeofSoftware,14thInternationalConferenceonToolsandAlgorithmsforthe
ConstructionandAnalysisofSystems, TACAS’08/ETAPS’08, pp.337–340, Berlin, Heidelberg,
2008.Springer-Verlag. ISBN3540787992.
Yuntian Deng, Yejin Choi, and Stuart Shieber. From Explicit CoT to Implicit CoT: Learning to
InternalizeCoTStepbyStep. arXivpreprintarXiv:2405.14838,2024.
Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improv-
ing factuality and reasoning in language models through multiagent debate. arXiv preprint
arXiv:2305.14325,2023.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 Herd of
Models. arXivpreprintarXiv:2407.21783,2024.
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A Simulation Framework
forMethodsthatLearnfromHumanFeedback,2023.
Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean
Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal,
Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits of trans-
formers on compositionality. In Thirty-seventh Conference on Neural Information Processing
Systems,2023. URLhttps://openreview.net/forum?id=Fkckkr3ya8.
YaoFu,HaoPeng,AshishSabharwal,PeterClark,andTusharKhot. Complexity-basedprompting
formulti-stepreasoning. InTheEleventhInternationalConferenceonLearningRepresentations,
2023. URLhttps://openreview.net/forum?id=yf1icZHC-l9.
LuyuGao, AmanMadaan, ShuyanZhou, UriAlon, PengfeiLiu, YimingYang, JamieCallan, and
GrahamNeubig. Pal: program-aidedlanguagemodels. InProceedingsofthe40thInternational
ConferenceonMachineLearning,ICML’23.JMLR.org,2023.
MorGeva,DanielKhashabi,EladSegal,TusharKhot,DanRoth,andJonathanBerant.DidAristotle
usealaptop? Aquestionansweringbenchmarkwithimplicitreasoningstrategies. Transactions
oftheAssociationforComputationalLinguistics,9:346–361,February2021. ISSN2307-387X.
doi: 10.1162/tacl a 00370.
15L.Guan,YifanZhou,DenisLiu,YantianZha,HeniBenAmor,andSubbaraoKambhampati. ”Task
Success” is not Enough: Investigating the Use of Video-Language Models as Behavior Critics
forCatchingUndesirableAgentBehaviors. ArXiv,abs/2402.04210,2024. URLhttps://api.
semanticscholar.org/CorpusID:267500077.
AtharvaGundawar,MuditVerma,L.Guan,KarthikValmeekam,SiddhantBhambri,andSubbarao
Kambhampati. Robust Planning with LLM-Modulo Framework: Case Study in Travel Plan-
ning. ArXiv, abs/2405.20625, 2024. URL https://api.semanticscholar.org/CorpusID:
270199944.
Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy
Sun,EkaterinaZubova,YujieQiao,MatthewBurtell,DavidPeng,JonathanFan,YixinLiu,Brian
Wong,MalcolmSailor,AnsongNi,LinyongNan,JungoKasai,TaoYu,RuiZhang,ShafiqJoty,
AlexanderR.Fabbri,WojciechKryscinski,XiVictoriaLin,CaimingXiong,andDragomirRadev.
FOLIO:NaturalLanguageReasoningwithFirst-OrderLogic. arXivpreprintarXiv:2209.00840,
2022. URLhttps://arxiv.org/abs/2209.00840.
Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reason-
ing with language model is planning with world model. In Houda Bouamor, Juan Pino, and
Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 8154–8173, Singapore, December 2023. Association for Computational
Linguistics.doi:10.18653/v1/2023.emnlp-main.507.URLhttps://aclanthology.org/2023.
emnlp-main.507.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,andJacob
Steinhardt. Measuring massive multitask language understanding. Proceedings of the Interna-
tionalConferenceonLearningRepresentations(ICLR),2021a.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song,andJacobSteinhardt. MeasuringMathematicalProblemSolvingWiththeMATHDataset.
NeurIPS,2021b.
HanxuHu,HongyuanLu,HuajianZhang,WaiLam,andYueZhang. Chain-of-symbolprompting
elicitsplanninginlargelangaugemodels,2023.
WenyueHua,KaijieZhu,LingyaoLi,LizhouFan,ShuhangLin,MingyuJin,HaochenXue,Zelong
Li, Jindong Wang, and Yongfeng Zhang. Disentangling Logic: The Role of Context in Large
Language Model Reasoning Capabilities. ArXiv, abs/2406.02787, 2024. URL https://api.
semanticscholar.org/CorpusID:270258104.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as
zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint
arXiv:2201.07207,2022.
Jinghan Jia, Abi Komma, Timothy Leffel, Xujun Peng, Ajay Nagesh, Tamer Soliman, Aram Gal-
styan, and Anoop Kumar. Leveraging LLMs for dialogue quality measurement. In Yi Yang,
Aida Davani, Avi Sil, and Anoop Kumar (eds.), Proceedings of the 2024 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies (Volume 6: Industry Track), pp. 359–367, Mexico City, Mexico, June 2024.
Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-industry.30. URL
https://aclanthology.org/2024.naacl-industry.30.
Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-
cile Saulnier, L’elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothe´e Lacroix, and William El Sayed. Mistral 7b. ArXiv,
abs/2310.06825,2023. URLhttps://api.semanticscholar.org/CorpusID:263830494.
Dongwei Jiang, Marcio Fonseca, and Shay B. Cohen. Leanreasoner: Boosting complex logical
reasoning with lean. In Kevin Duh, Helena Go´mez-Adorno, and Steven Bethard (eds.), Pro-
ceedingsofthe2024ConferenceoftheNorthAmericanChapteroftheAssociationforCompu-
tational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024,
16MexicoCity,Mexico,June16-21,2024,pp.7497–7510.AssociationforComputationalLinguis-
tics, 2024. doi: 10.18653/V1/2024.NAACL-LONG.416. URL https://doi.org/10.18653/
v1/2024.naacl-long.416.
Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang,
Yejin Choi, and Xiang Ren. Are Machine Rationales (Not) Useful to Humans? Measuring and
Improving Human Utility of Free-text Rationales. ArXiv, abs/2305.07095, 2023. URL https:
//api.semanticscholar.org/CorpusID:258676376.
Subbarao Kambhampati. Can large language models reason and plan? Annals of the New
York Academy of Sciences, 1534:15 – 18, 2024. URL https://api.semanticscholar.org/
CorpusID:268249961.
Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant
Bhambri,LucasPaulSaldyt,andAnilBMurthy.Position:LLMscan’tplan,butcanhelpplanning
in LLM-modulo frameworks. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian
Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st
InternationalConferenceonMachineLearning, volume235ofProceedingsofMachineLearn-
ing Research, pp. 22895–22907. PMLR, 21–27 Jul 2024a. URL https://proceedings.mlr.
press/v235/kambhampati24a.html.
Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant
Bhambri,LucasPaulSaldyt,andAnilBMurthy. Position: LLMscan’tplan,butcanhelpplan-
ninginLLM-moduloframeworks. InForty-firstInternationalConferenceonMachineLearning,
2024b. URLhttps://openreview.net/forum?id=Th8JPEmH4z.
LiweiKang,ZiruiZhao,DavidHsu,andWeeSunLee. Ontheempiricalcomplexityofreasoning
andplanninginllms. arXivpreprintarXiv:2404.11041,2024.
Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. One thousand and
onepairs: A”novel”challengeforlong-contextlanguagemodels. ArXiv,abs/2406.16264,2024.
URLhttps://api.semanticscholar.org/CorpusID:270703648.
Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish
Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In The
International Conference on Learning Representations, volume abs/2210.02406, 2023. URL
https://api.semanticscholar.org/CorpusID:252715485.
SeungoneKim,JuyoungSuk,JiYongCho,ShayneLongpre,ChaeeunKim,DongkeunYoon,Guijin
Son,YejinCho,SheikhShafayat,JinheonBaek,etal. TheBiGGenBench: APrincipledBench-
mark for Fine-grained Evaluation of Language Models with Language Models. arXiv preprint
arXiv:2406.05761,2024.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. In Proceedings of the 36th International Conference
on Neural Information Processing Systems, Red Hook, NY, USA, 2022. Curran Associates Inc.
ISBN9781713871088.
WoosukKwon,ZhuohanLi,SiyuanZhuang,YingSheng,LianminZheng,CodyHaoYu,JosephE.
Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementforlargelanguagemodel
servingwithpagedattention. InProceedingsoftheACMSIGOPS29thSymposiumonOperating
SystemsPrinciples,2023.
BrendenM.LakeandMarcoBaroni. Generalizationwithoutsystematicity: Onthecompositional
skillsofsequence-to-sequencerecurrentnetworks. InJenniferG.DyandAndreasKrause(eds.),
Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stock-
holmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine
LearningResearch,pp.2879–2888.PMLR,2018.URLhttp://proceedings.mlr.press/v80/
lake18a.html.
Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Her-
nandez,DustinLi,EsinDurmus,EvanHubinger,JacksonKernion,etal. Measuringfaithfulness
inchain-of-thoughtreasoning. arXivpreprintarXiv:2307.13702,2023.
17FangyuLei, QianLiu, YimingHuang, ShizhuHe, JunZhao, andKangLiu. S3Eval: Asynthetic,
scalable, systematic evaluation suite for large language model. In Kevin Duh, Helena Gomez,
andSteven Bethard(eds.), Proceedingsof the2024 Conferenceofthe NorthAmericanChapter
of the Association for Computational Linguistics: Human Language Technologies (Volume 1:
LongPapers),pp.1259–1286,MexicoCity,Mexico,June2024.AssociationforComputational
Linguistics. doi: 10.18653/v1/2024.naacl-long.69. URL https://aclanthology.org/2024.
naacl-long.69.
Tianwen Li, Zhexiong Liu, Lindsay Matsumura, Elaine Wang, Diane Litman, and Richard Cor-
renti. Using large language models to assess young students’ writing revisions. In Ekaterina
Kochmar, Marie Bexte, Jill Burstein, Andrea Horbach, Ronja Laarmann-Quante, Ana¨ıs Tack,
Victoria Yaneva, and Zheng Yuan (eds.), Proceedings of the 19th Workshop on Innovative Use
of NLP for Building Educational Applications (BEA 2024), pp. 365–380, Mexico City, Mex-
ico,June2024.AssociationforComputationalLinguistics. URLhttps://aclanthology.org/
2024.bea-1.30.
Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan
Leike,JohnSchulman,IlyaSutskever,andKarlCobbe. Let’sverifystepbystep. InTheTwelfth
InternationalConferenceonLearningRepresentations,2024. URLhttps://openreview.net/
forum?id=v8L0pN6EOi.
B. Liu, Yuqian Jiang, Xiaohan Zhang, Qian Liu, Shiqi Zhang, Joydeep Biswas, and Peter
Stone. Llm+p: Empowering large language models with optimal planning proficiency. ArXiv,
abs/2304.11477,2023a. URLhttps://api.semanticscholar.org/CorpusID:258298051.
BingbinLiu, JordanT.Ash, SurbhiGoel, AkshayKrishnamurthy, andCyrilZhang. Transformers
learnshortcutstoautomata. InTheEleventhInternationalConferenceonLearningRepresenta-
tions,2023b. URLhttps://openreview.net/forum?id=De4FYqjFueZ.
PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,
PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathoughtchainsfor
science question answering. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,
K.Cho,andA.Oh(eds.),AdvancesinNeuralInformationProcessingSystems35: AnnualCon-
ferenceonNeuralInformationProcessingSystems2022,NeurIPS2022,NewOrleans,LA,USA,
November28-December9,2022,2022. URLhttp://papers.nips.cc/paper files/paper/
2022/hash/11332b6b6cf4485b84afadb1352d3a9a-Abstract-Conference.html.
YingweiMa,YueLiu,YueYu,YuanliangZhang,YuJiang,ChangjianWang,andShanshanLi. At
Which Training Stage Does Code Data Help LLMs Reasoning? In The Twelfth International
ConferenceonLearningRepresentations,ICLR2024,Vienna,Austria,May7-11,2024.OpenRe-
view.net,2024. URLhttps://openreview.net/forum?id=KIPJKST4gw.
Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it
takestwototango. ArXiv,abs/2209.07686,2022. URLhttps://api.semanticscholar.org/
CorpusID:252355328.
WilliamMerrillandAshishSabharwal.Theexpressivepoweroftransformerswithchainofthought.
In The International Conference on Learning Representations, volume abs/2310.07923, 2024.
URLhttps://api.semanticscholar.org/CorpusID:263909434.
MaxwellNye,AndersJohanAndreassen,GuyGur-Ari,HenrykMichalewski,JacobAustin,David
Bieber,DavidDohan,AitorLewkowycz,MaartenBosma,DavidLuan,CharlesSutton,andAu-
gustusOdena.Showyourwork:Scratchpadsforintermediatecomputationwithlanguagemodels,
2022. URLhttps://openreview.net/forum?id=iedYJm92o0a.
Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-
Lezama.Isself-repairasilverbulletforcodegeneration? InTheTwelfthInternationalConference
onLearningRepresentations,2024. URLhttps://openreview.net/forum?id=y0GJXRungR.
OpenAI. GPT-4 Technical Report. ArXiv, abs/2303.08774, 2023. URL https://api.
semanticscholar.org/CorpusID:257532815.
18Liangming Pan, Alon Albalak, Xinyi Wang, and William Wang. Logic-LM: Empowering large
language models with symbolic solvers for faithful logical reasoning. In Houda Bouamor,
Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics:
EMNLP2023, pp.3806–3824, Singapore, December2023.AssociationforComputationalLin-
guistics. doi: 10.18653/v1/2023.findings-emnlp.248. URLhttps://aclanthology.org/2023.
findings-emnlp.248.
Xiangyu Peng, Siyan Li, Sarah Wiegreffe, and Mark Riedl. Inferring the reader: Guiding auto-
matedstorygenerationwithcommonsensereasoning. InYoavGoldberg,ZornitsaKozareva,and
YueZhang(eds.),FindingsoftheAssociationforComputationalLinguistics: EMNLP2022,pp.
7008–7029,AbuDhabi,UnitedArabEmirates,December2022.AssociationforComputational
Linguistics. doi: 10.18653/v1/2022.findings-emnlp.520. URL https://aclanthology.org/
2022.findings-emnlp.520.
ZhentingQi, MingyuanMa, JiahangXu, Li LynaZhang, FanYang, andMaoYang. MutualRea-
soningMakesSmallerLLMsStrongerProblem-Solvers. arXivpreprintarXiv:2408.06195,2024.
XinQuan,MarcoValentino,LouiseDennis,andAndreFreitas. Enhancingethicalexplanationsof
large language models through iterative symbolic refinement. In Yvette Graham and Matthew
Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 1–22, St. Julian’s, Malta, March
2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.
eacl-long.1.
Machel Reid and et. al. Gemini 1.5: Unlocking multimodal understanding across millions of to-
kens of context. ArXiv, abs/2403.05530, 2024. URL https://api.semanticscholar.org/
CorpusID:268297180.
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien
Dirani,JulianMichael,andSamuelR.Bowman. GPQA:AGraduate-LevelGoogle-ProofQ&A
Benchmark,2023.
GemmaTeamMorganeRiviereandet.al.Gemma2:Improvingopenlanguagemodelsatapractical
size. 2024. URLhttps://api.semanticscholar.org/CorpusID:270843326.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: an ad-
versarial winograd schema challenge at scale. Commun. ACM, 64(9):99–106, aug 2021. ISSN
0001-0782. doi: 10.1145/3474381. URLhttps://doi.org/10.1145/3474381.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Com-
monsense reasoning about social interactions. In Proceedings of the 2019 Conference on Em-
piricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceon
Natural Language Processing (EMNLP-IJCNLP), pp. 4463–4473, Hong Kong, China, Novem-
ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL
https://aclanthology.org/D19-1454.
AbulhairSaparovandHeHe. Languagemodelsaregreedyreasoners: Asystematicformalanalysis
of chain-of-thought. In The Eleventh International Conference on Learning Representations,
2023. URLhttps://openreview.net/forum?id=qFVVBzXxR2V.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
MingchuanZhang,Y.K.Li,Y.Wu,andDayaGuo. DeepSeekMath:PushingtheLimitsofMath-
ematical Reasoning in Open Language Models, 2024. URL https://arxiv.org/abs/2402.
03300.
ZayneReaSprague,XiYe,KajBostrom,SwaratChaudhuri,andGregDurrett. MuSR:Testingthe
limitsofchain-of-thoughtwithmultistepsoftreasoning. InTheTwelfthInternationalConference
onLearningRepresentations,2024. URLhttps://openreview.net/forum?id=jenyYQzue1.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria` Garriga-Alonso, et al. Beyond the
imitationgame:Quantifyingandextrapolatingthecapabilitiesoflanguagemodels.arXivpreprint
arXiv:2206.04615,2022.
19MajaStahl,LeonBiermann,AndreasNehring,andHenningWachsmuth. ExploringLLMprompt-
ingstrategiesforjointessayscoringandfeedbackgeneration.InEkaterinaKochmar,MarieBexte,
JillBurstein,AndreaHorbach,RonjaLaarmann-Quante,Ana¨ısTack,VictoriaYaneva,andZheng
Yuan (eds.), Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educa-
tionalApplications(BEA2024),pp.283–298,MexicoCity,Mexico,June2024.Associationfor
ComputationalLinguistics. URLhttps://aclanthology.org/2024.bea-1.23.
KayaStechly,KarthikValmeekam,andSubbaraoKambhampati.Ontheself-verificationlimitations
oflargelanguagemodelsonreasoningandplanningtasks. ArXiv,abs/2402.08115,2024a. URL
https://api.semanticscholar.org/CorpusID:267637077.
Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. Chain of thoughtlessness? an
analysis of cot in planning. 2024b. URL https://api.semanticscholar.org/CorpusID:
269626390.
Simeng Sun, Yang Liu, Shuohang Wang, Dan Iter, Chenguang Zhu, and Mohit Iyyer. PEARL:
Prompting large language models to plan and execute actions over long documents. In Yvette
GrahamandMatthewPurver(eds.),Proceedingsofthe18thConferenceoftheEuropeanChap-
ter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 469–486,
St. Julian’s, Malta, March 2024. Association for Computational Linguistics. URL https:
//aclanthology.org/2024.eacl-long.29.
MiracSuzgun,NathanScales,NathanaelScha¨rli,SebastianGehrmann,YiTay,HyungWonChung,
AakankshaChowdhery,QuocLe,EdChi,DennyZhou,andJasonWei. ChallengingBIG-bench
tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and
Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023,
pp.13003–13051,Toronto,Canada,July2023.AssociationforComputationalLinguistics. doi:
10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023.findings-acl.
824.
AlonTalmor,JonathanHerzig,NicholasLourie,andJonathanBerant. CommonsenseQA:Aques-
tion answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Con-
ferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human
LanguageTechnologies,Volume1(LongandShortPapers),pp.4149–4158,Minneapolis,Min-
nesota,June2019.AssociationforComputationalLinguistics.doi:10.18653/v1/N19-1421.URL
https://aclanthology.org/N19-1421.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas
Blecher, Cristian Canto´n Ferrer, Moya Chen, Guillem Cucurull, DavidEsiobu, Jude Fernandes,
JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyS.
Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa,IsabelM.Kloumann,A.V.Korenev,PunitSinghKoura,Marie-AnneLachaux,Thibaut
Lavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,
PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,
KalyanSaladi, AlanSchelten, RuanSilva, EricMichaelSmith, R.Subramanian, XiaTan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov,
Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert
Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat
models. ArXiv,abs/2307.09288,2023. URLhttps://api.semanticscholar.org/CorpusID:
259950998.
HarshTrivedi,NiranjanBalasubramanian,TusharKhot,andAshishSabharwal. MuSiQue: Multi-
hopquestionsviasingle-hopquestioncomposition. TransactionsoftheAssociationforCompu-
tationalLinguistics,2022.
Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the
planningabilitiesoflargelanguagemodels-acriticalinvestigation.InThirty-seventhConference
onNeuralInformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=
X6dEqXIsEW.
20Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kamb-
hampati. PlanBench:Anextensiblebenchmarkforevaluatinglargelanguagemodelsonplanning
and reasoning about change. In Proceedings of the 37th International Conference on Neural
InformationProcessingSystems,NIPS’23,RedHook,NY,USA,2024.CurranAssociatesInc.
Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati. Theory of mind abilities of
large language models in human-robot interaction: An illusion? Companion of the 2024
ACM/IEEE International Conference on Human-Robot Interaction, 2024. URL https://api.
semanticscholar.org/CorpusID:266902529.
BoshiWang, SewonMin, XiangDeng, JiamingShen, YouWu, LukeZettlemoyer, andHuanSun.
Towards understanding chain-of-thought prompting: An empirical study of what matters. In
Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2717–
2739, Toronto, Canada, July2023a. AssociationforComputationalLinguistics. doi: 10.18653/
v1/2023.acl-long.153. URLhttps://aclanthology.org/2023.acl-long.153.
LeiWang,WanyuXu,YihuaiLan,ZhiqiangHu,YunshiLan,RoyKa-WeiLee,andEe-PengLim.
Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language
models.InAnnualMeetingoftheAssociationforComputationalLinguistics,2023b.URLhttps:
//api.semanticscholar.org/CorpusID:258558102.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha
Chowdhery,andDennyZhou. Self-consistencyimproveschainofthoughtreasoninginlanguage
models. In The Eleventh International Conference on Learning Representations, 2023c. URL
https://openreview.net/forum?id=1PL1NIMMrw.
YuboWang,XueguangMa,GeZhang,YuanshengNi,AbhranilChandra,ShiguangGuo,Weiming
Ren,AaranArulraj,XuanHe,ZiyanJiang,TianleLi,MaxKu,KaiWang,AlexZhuang,Rongqi
Fan, Xiang Yue, and Wenhu Chen. MMLU-Pro: A More Robust and Challenging Multi-Task
LanguageUnderstandingBenchmark,2024.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
NeuralInformationProcessingSystems,35:24824–24837,2022.
Li Siang Wong, Gabriel Grand, Alexander K. Lew, Noah D. Goodman, Vikash K. Mansinghka,
JacobAndreas,andJoshuaB.Tenenbaum. Fromwordmodelstoworldmodels:Translatingfrom
natural language to the probabilistic language of thought. ArXiv, abs/2306.12672, 2023. URL
https://api.semanticscholar.org/CorpusID:259224900.
JianXie,KaiZhang,JiangjieChen,TinghuiZhu,RenzeLou,YuandongTian,YanghuaXiao,and
Yu Su. TravelPlanner: A Benchmark for Real-World Planning with Language Agents. ArXiv,
abs/2402.01622,2024. URLhttps://api.semanticscholar.org/CorpusID:267406800.
MiaoXiong, ZhiyuanHu, XinyangLu, YifeiLi, JieFu, JunxianHe, andBryanHooi. CanLLMs
ExpressTheirUncertainty? AnEmpiricalEvaluationofConfidenceElicitationinLLMs. InThe
TwelfthInternationalConferenceonLearningRepresentations,ICLR2024,Vienna,Austria,May
7-11,2024.OpenReview.net,2024. URLhttps://openreview.net/forum?id=gjeQKFxFpZ.
Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. Faithful logical
reasoning via symbolic chain-of-thought. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar
(eds.),Proceedingsofthe62ndAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pp.13326–13365,Bangkok,Thailand,August2024a.Associationfor
ComputationalLinguistics. URLhttps://aclanthology.org/2024.acl-long.720.
Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, and Jian-Guang
Lou. Re-readingimprovesreasoninginlanguagemodels, 2024b. URLhttps://openreview.
net/forum?id=3jXCF5dNpC.
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun
Chen.Largelanguagemodelsasoptimizers.InTheTwelfthInternationalConferenceonLearning
Representations,2024. URLhttps://openreview.net/forum?id=Bb4VGOWELI.
21Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. TreeofThoughts: Deliberateproblemsolvingwithlargelanguagemodels,2023.
Xi Ye and Greg Durrett. The Unreliability of Explanations in Few-shot Prompting for Textual
Reasoning. InAdvancesinNeuralInformationProcessingSystems,2022.
Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. Satisfiability-aided language models using
declarativeprompting. InAdvancesinNeuralInformationProcessingSystems,2023.
HughZhang,JeffDa,DeanLee,VaughnRobinson,CatherineWu,WillSong,TiffanyZhao,Pranav
Raja, DylanSlack, QinLyu, SeanHendryx, RussellKaplan, MicheleLunati, andSummerYue.
Acarefulexaminationoflargelanguagemodelperformanceongradeschoolarithmetic. ArXiv,
abs/2405.00332,2024. URLhttps://api.semanticscholar.org/CorpusID:269484687.
HuaixiuStevenZheng, SwaroopMishra, XinyunChen, Heng-TzeCheng, EdH.Chi, QuocVLe,
andDennyZhou. Takeastepback: Evokingreasoningviaabstractioninlargelanguagemodels.
In The Twelfth International Conference on Learning Representations, 2024a. URL https://
openreview.net/forum?id=3bq3jsvcQ1.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
ZiLin,ZhuohanLi,DachengLi,EricP.Xing,HaoZhang,JosephE.Gonzalez,andIonStoica.
Judging LLM-as-a-judge with MT-bench and Chatbot Arena. In Proceedings of the 37th Inter-
nationalConferenceonNeuralInformationProcessingSystems,NIPS’23,RedHook,NY,USA,
2024b.CurranAssociatesInc.
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
WeizhuChen,andNanDuan.AGIEval:AHuman-CentricBenchmarkforEvaluatingFoundation
Models,2023.
Ben Zhou, Kyle Richardson, Xiaodong Yu, and Dan Roth. Learning to decompose: Hypotheti-
cal question decomposition based on comparable texts. In Yoav Goldberg, Zornitsa Kozareva,
and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natu-
ral Language Processing, pp. 2223–2235, Abu Dhabi, United Arab Emirates, December 2022.
Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.142. URL
https://aclanthology.org/2022.emnlp-main.142.
BenZhou, HongmingZhang, SihaoChen, DianYu, HongweiWang, BaolinPeng, DanRoth, and
DongYu. Conceptualandunbiasedreasoninginlanguagemodels. ArXiv,abs/2404.00205,2024.
URLhttps://api.semanticscholar.org/CorpusID:268820105.
Denny Zhou, Nathanael Scha¨rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-
mans,ClaireCui,OlivierBousquet,QuocVLe,andEdH.Chi. Least-to-mostpromptingenables
complexreasoninginlargelanguagemodels.InTheEleventhInternationalConferenceonLearn-
ingRepresentations,2023a. URLhttps://openreview.net/forum?id=WZH7099tgfM.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and
Jimmy Ba. Large Language Models are Human-Level Prompt Engineers. In The Eleventh In-
ternationalConferenceonLearningRepresentations,2023b. URLhttps://openreview.net/
forum?id=92gvk82DE-.
22Table 4: List of datasets used in our experiments. We categorize each dataset into one of five
categories based on the type of reasoning required: Commonsense, Knowledge, Soft Reasoning,
Symbolic, or Mathematical. We also report answer formats. When we use few-shot prompts, we
markhowmanyexamplesthosepromptscontain. BiGGenBenchhasmanycategoriesofquestions
thatexplicitlyaskforCoTsintheresponse;weignorethosecategoriesforourevaluation.
Dataset Type AnswerFormat m-Shots
CommonsenseQA Commonsense Multiplechoice 7
StrategyQA Commonsense TrueorFalse 6
SIQA Commonsense Multiplechoice 0
PIQA Commonsense Multiplechoice 0
Winogrande Commonsense Multiplechoice 0
ArcEasy Knowledge Multiplechoice 0
ArcChallenge Knowledge Multiplechoice 0
AGIEvalLSAT SoftReasoning Multiplechoice 3
BiGGen-Bench SoftReasoning Freeresponse 0
MMLU Knowledge MultipleChoice 5
MMLUPro Knowledge MultipleChoice 5
BigBench-Hard Symbolic MultipleChoice 0
MuSR SoftReasoning MultipleChoice 1
GPQA Mathematical MultipleChoice 3
MuSiQue SoftReasoning ShortAnswer 0
GSM8K Mathematical ShortAnswer 8
GSM8K-Hard Mathematical ShortAnswer 8
FOLIO Symbolic True,False,orUnknown 4
ContextHub Symbolic True,False,orNeither 3
MATH Mathematical ShortAnswer 4
Table5: Listofmodelsforourexperiments. Wefocusoncontemporaryinstruction-tunedmodels;
althoughpretrainedandsmallerlanguagemodelscouldbeused,theyarenotthefocusofourstudy.
Prompts and outputs used for each model are available on Huggingface. ∗ Note that Gemma can
acceptmorethan4kinputtokens,butwearerestrictedto4kbyvLLM.
Model ContextLength IsOpenSource
Llama27BChat 4k True
Mistral7BInstructv0.3 8k True
Llama3.18BInstruct 128k True
Llama3.170BInstruct 128k True
Gemma29BIt 4k∗ True
Qwen7BInstruct 131k True
Qwen72BInstruct 131k True
GPT4o-Mini 128k False
GPT4o 128k False
Gemini1.5Pro 128k False
GeminiFlash 1m False
Claude3.5Sonnet 200k False
Claude3Haiku 200k False
A EXPANDED EXPERIMENTAL DETAILS
A full list of the datasets can be found in Table 4. Each model can be seen in Table 5. We use
one answer parser for all datasets of the same answer response format (one for multiple choice,
short answer, etc.); however, some datasets require special handling and have edge cases that we
handle separately from the rest of the datasets. Similarly, for each model, we use the exact same
promptacrossthem, exceptwhenclosedsourcemodelsrequiredifferentpromptsbecausetheydo
notallowforpartialcompletions(i.e., whenwecannotput“let’sthinkstepbystep”towarm-start
theassistant’sresponse).AllpromptsaregiveninourHuggingfacerepo,includingthemodeloutput
andwhatouranswerparserextractedastheanswer.
23Figure7: AverageperformanceimprovementfromusingCoTacrossdifferentmodelsinthezero-
shot and few-shot settings. Each bar represents how much CoT improves the accuracy for that
specificsetting. Ingeneral,CoTinthefew-shotsettingdoesnotchangethequalitativeperformance
ofCoTversuszero-shot,thoughitcanchangethemagnitudeforsymbolicdatasets.
ExperimentswereconductedeitherbyinvokingAPIsorbyrunningopen-sourcemodelsonourown
hardware,mostlyonamachinewith8A40sor4QuadroRTX8000s.Alllocallyhostedmodelswere
hostedwithvLLM.AllparametersgiventothevLLMAPIendpointaregivenintheHuggingface
repoaswell.
B FEW-SHOT EXPERIMENTS
Compared to a zero-shot prompt, a few-shot prompt additionally contains demonstrations of the
relevantreasoningmodeondifferentprobleminstances{(v(q ),y∗)}. Few-shotpromptsfordirect
i i
answersimplyencodetheanswera asy∗,whereasfew-shotpromptsforchain-of-thoughtinclude
i i
a reasoning trace ending in the correct answer. Now we can define the m-shot direct prompt as
Im(q) = v (q )a v (q )a ...v (q )a v (q) and the m-shot cot prompt as Im (q) =
da da 1 1 da 2 2 da m m da cot
v (q )y∗v (q )y∗...v (q )y∗ v (q).
cot 1 1 cot 2 2 cot m m cot
Figure 7 shows the difference between few-shot prompting and the zero-shot setting discussed in
the main text of the paper. We see that using CoT in the few-shot setting largely does not change
thedatasetsthatbenefitfromit. Onlyonedataset, MuSRTeamAllocation, startstoimprovewith
few-shot; however, we believe this to be an exception because the final step to derive the answer
iscomplexinthepromptandclearerintheexamples. Themagnitudeofimprovementoverdirect
answerpromptingwhenusingCoTisalsosimilartothezero-shotsetting.
C EXPANDED COT VS DIRECT EXPERIMENTAL RESULTS
C.1 FULLZERO-SHOTRESULTS
24Table6: DirectanswerandCoTaccuraciesforeachreasoningcategoryacrossmodels.
Model Commonsense Knowledge Mathematical Symbolic Soft
DA% CoT% DA% CoT% DA% CoT% DA% CoT% DA% CoT%
Claude-3Haiku 71.8 74.2 73.0 76.1 18.1 48.2 38.6 48.7 55.9 56.6
Claude-3.5Sonnet 80.7 82.6 83.8 88.8 38.7 59.0 53.2 67.1 67.6 75.7
GPT-4oMini 78.3 79.9 73.6 83.1 22.9 59.7 48.1 60.9 61.1 63.5
Gemini1.5Flash 76.8 73.6 78.2 81.0 27.2 55.7 47.0 59.7 60.6 62.6
Gemini1.5Pro 77.3 75.2 80.9 83.8 35.4 58.5 52.9 62.6 64.1 67.8
Gemma29b 71.5 72.9 74.9 76.9 18.5 50.5 46.7 55.8 58.2 60.5
GPT-4o 83.6 84.1 82.9 88.6 36.5 63.3 55.7 68.3 65.9 74.0
Llama27b 50.4 50.1 44.1 46.6 9.3 17.2 22.4 35.4 37.2 37.6
Llama3.170b 81.0 81.6 82.4 85.6 24.9 54.9 49.0 60.0 65.7 69.5
Llama3.18b 70.1 70.9 70.1 74.1 16.0 47.8 34.8 51.6 55.0 56.2
Mistral7b 62.4 66.0 62.0 64.5 10.9 28.9 41.8 45.0 48.6 49.7
Phi-3Small8k 70.2 72.5 76.1 79.7 17.8 47.1 51.2 58.7 57.9 56.4
Qwen272b 79.2 81.7 78.6 84.6 23.9 58.5 48.2 58.7 64.2 65.1
Qwen27b 68.3 70.6 65.2 71.3 15.9 53.5 43.8 52.3 54.4 49.4
Average 73.0 74.0 73.3 77.5 22.6 50.2 45.2 56.1 58.3 60.3
Table7: Zero-shotaccuracyfordirectansweringandCoTpromptsonalldatasets
Dataset Type Model zero-shotDAaccuracy zero-shotCoTaccuracy
MuSRTeamAllocations SoftReasoning Llama27b 34.8 37.2
MuSRTeamAllocations SoftReasoning Mistral7b 38.8 46.8
MuSRTeamAllocations SoftReasoning Llama3.18b 44.0 48.0
MuSRTeamAllocations SoftReasoning Llama3.170b 65.2 66.8
MuSRTeamAllocations SoftReasoning Gemma29b 47.2 44.8
MuSRTeamAllocations SoftReasoning Phi-3Small8k 47.2 61.6
MuSRTeamAllocations SoftReasoning Qwen27b 42.0 49.6
MuSRTeamAllocations SoftReasoning Qwen272b 58.0 66.8
MuSRTeamAllocations SoftReasoning GPT-4oMini 61.2 58.4
MuSRTeamAllocations SoftReasoning GPT-4o 64.0 63.6
MuSRTeamAllocations SoftReasoning Claude-3Haiku 56.8 59.2
MuSRTeamAllocations SoftReasoning Claude-3.5Sonnet 80.4 63.2
MuSRTeamAllocations SoftReasoning Gemini1.5Flash 48.8 55.2
MuSRTeamAllocations SoftReasoning Gemini1.5Pro 58.4 62.4
MuSiQue SoftReasoning Llama27b 40.1 36.1
MuSiQue SoftReasoning Mistral7b 47.3 47.2
MuSiQue SoftReasoning Llama3.18b 62.6 64.7
MuSiQue SoftReasoning Llama3.170b 74.0 72.2
MuSiQue SoftReasoning Gemma29b 67.7 68.7
MuSiQue SoftReasoning Phi-3Small8k 58.3 64.3
MuSiQue SoftReasoning Qwen27b 60.7 65.1
MuSiQue SoftReasoning Qwen272b 56.3 69.0
MuSiQue SoftReasoning GPT-4oMini 71.3 68.2
MuSiQue SoftReasoning GPT-4o 73.5 70.1
MuSiQue SoftReasoning Claude-3Haiku 54.8 56.0
MuSiQue SoftReasoning Claude-3.5Sonnet 66.9 70.4
MuSiQue SoftReasoning Gemini1.5Flash 69.8 66.2
MuSiQue SoftReasoning Gemini1.5Pro 69.8 71.3
AGIEvalLSATRC SoftReasoning Llama27b 31.2 36.4
AGIEvalLSATRC SoftReasoning Mistral7b 61.7 61.0
AGIEvalLSATRC SoftReasoning Llama3.18b 71.0 68.8
AGIEvalLSATRC SoftReasoning Llama3.170b 84.4 87.0
AGIEvalLSATRC SoftReasoning Gemma29b 75.1 78.1
AGIEvalLSATRC SoftReasoning Phi-3Small8k 68.8 69.9
AGIEvalLSATRC SoftReasoning Qwen27b 61.0 66.5
AGIEvalLSATRC SoftReasoning Qwen272b 83.6 84.4
AGIEvalLSATRC SoftReasoning GPT-4oMini 77.3 74.3
AGIEvalLSATRC SoftReasoning GPT-4o 88.1 81.4
AGIEvalLSATRC SoftReasoning Claude-3Haiku 71.7 65.1
AGIEvalLSATRC SoftReasoning Claude-3.5Sonnet 90.0 89.6
AGIEvalLSATRC SoftReasoning Gemini1.5Flash 78.1 81.0
AGIEvalLSATRC SoftReasoning Gemini1.5Pro 82.2 85.9
CommonsenseQA Commonsense Llama27b 49.4 54.6
CommonsenseQA Commonsense Mistral7b 68.0 68.0
CommonsenseQA Commonsense Llama3.18b 68.5 74.9
CommonsenseQA Commonsense Llama3.170b 83.5 84.4
CommonsenseQA Commonsense Gemma29b 79.2 80.1
CommonsenseQA Commonsense Phi-3Small8k 81.8 80.3
CommonsenseQA Commonsense Qwen27b 78.5 79.0
25Table7: Zero-shotaccuracyfordirectansweringandCoTpromptsonalldatasets
Dataset Type Model zero-shotDAaccuracy zero-shotCoTaccuracy
CommonsenseQA Commonsense Qwen272b 87.4 87.3
CommonsenseQA Commonsense GPT-4oMini 82.5 83.9
CommonsenseQA Commonsense GPT-4o 86.5 87.3
CommonsenseQA Commonsense Claude-3Haiku 80.6 79.0
CommonsenseQA Commonsense Claude-3.5Sonnet 85.1 84.3
CommonsenseQA Commonsense Gemini1.5Flash 79.7 82.6
CommonsenseQA Commonsense Gemini1.5Pro 79.9 82.9
GPQA Mathematical Llama27b 28.3 24.3
GPQA Mathematical Mistral7b 23.0 24.3
GPQA Mathematical Llama3.18b 24.1 25.9
GPQA Mathematical Llama3.170b 23.2 25.9
GPQA Mathematical Gemma29b 26.3 21.2
GPQA Mathematical Phi-3Small8k 22.3 20.8
GPQA Mathematical Qwen27b 24.1 24.6
GPQA Mathematical Qwen272b 21.0 18.1
GPQA Mathematical GPT-4oMini 21.0 24.0
GPQA Mathematical GPT-4o 23.7 25.9
GPQA Mathematical Claude-3Haiku 25.4 22.3
GPQA Mathematical Claude-3.5Sonnet 25.4 25.9
GPQA Mathematical Gemini1.5Flash 22.3 22.8
GPQA Mathematical Gemini1.5Pro 21.0 23.7
AGIEvalLSATLR SoftReasoning Llama27b 29.4 33.5
AGIEvalLSATLR SoftReasoning Mistral7b 44.1 47.8
AGIEvalLSATLR SoftReasoning Llama3.18b 59.0 53.9
AGIEvalLSATLR SoftReasoning Llama3.170b 81.4 81.0
AGIEvalLSATLR SoftReasoning Gemma29b 64.9 67.6
AGIEvalLSATLR SoftReasoning Phi-3Small8k 64.5 64.1
AGIEvalLSATLR SoftReasoning Qwen27b 50.6 58.4
AGIEvalLSATLR SoftReasoning Qwen272b 77.3 75.1
AGIEvalLSATLR SoftReasoning GPT-4oMini 65.3 68.2
AGIEvalLSATLR SoftReasoning GPT-4o 87.3 83.9
AGIEvalLSATLR SoftReasoning Claude-3Haiku 55.7 54.7
AGIEvalLSATLR SoftReasoning Claude-3.5Sonnet 83.7 82.7
AGIEvalLSATLR SoftReasoning Gemini1.5Flash 70.0 71.2
AGIEvalLSATLR SoftReasoning Gemini1.5Pro 79.4 80.4
SiQA Commonsense Llama27b 49.7 51.1
SiQA Commonsense Mistral7b 56.9 54.2
SiQA Commonsense Llama3.18b 60.6 59.5
SiQA Commonsense Llama3.170b 63.1 65.0
SiQA Commonsense Gemma29b 58.8 58.7
SiQA Commonsense Phi-3Small8k 38.0 37.5
SiQA Commonsense Qwen27b 60.0 61.1
SiQA Commonsense Qwen272b 64.5 61.9
SiQA Commonsense GPT-4oMini 62.6 62.6
SiQA Commonsense GPT-4o 63.8 63.3
SiQA Commonsense Claude-3Haiku 60.6 62.0
SiQA Commonsense Claude-3.5Sonnet 64.0 62.8
SiQA Commonsense Gemini1.5Flash 58.5 61.5
SiQA Commonsense Gemini1.5Pro 58.5 62.7
PiQA Commonsense Llama27b 62.1 64.7
PiQA Commonsense Mistral7b 78.6 77.7
PiQA Commonsense Llama3.18b 85.0 84.2
PiQA Commonsense Llama3.170b 91.8 90.6
PiQA Commonsense Gemma29b 84.0 84.8
PiQA Commonsense Phi-3Small8k 89.1 85.5
PiQA Commonsense Qwen27b 84.3 86.2
PiQA Commonsense Qwen272b 92.9 89.1
PiQA Commonsense GPT-4oMini 93.1 88.6
PiQA Commonsense GPT-4o 95.9 95.5
PiQA Commonsense Claude-3Haiku 85.9 86.6
PiQA Commonsense Claude-3.5Sonnet 94.6 94.5
PiQA Commonsense Gemini1.5Flash 84.6 89.8
PiQA Commonsense Gemini1.5Pro 88.1 91.3
ArcEasy Knowledge Llama27b 71.1 69.8
ArcEasy Knowledge Mistral7b 87.5 86.7
ArcEasy Knowledge Llama3.18b 93.0 92.5
ArcEasy Knowledge Llama3.170b 97.5 97.9
ArcEasy Knowledge Gemma29b 94.9 95.8
ArcEasy Knowledge Phi-3Small8k 96.0 96.3
ArcEasy Knowledge Qwen27b 89.5 84.7
ArcEasy Knowledge Qwen272b 97.9 97.4
ArcEasy Knowledge GPT-4oMini 96.8 94.6
ArcEasy Knowledge GPT-4o 98.9 98.1
ArcEasy Knowledge Claude-3Haiku 95.1 95.4
ArcEasy Knowledge Claude-3.5Sonnet 98.6 98.4
ArcEasy Knowledge Gemini1.5Flash 96.8 97.2
ArcEasy Knowledge Gemini1.5Pro 97.2 94.6
26Table7: Zero-shotaccuracyfordirectansweringandCoTpromptsonalldatasets
Dataset Type Model zero-shotDAaccuracy zero-shotCoTaccuracy
ArcChallenge Knowledge Llama27b 49.2 45.2
ArcChallenge Knowledge Mistral7b 78.3 76.6
ArcChallenge Knowledge Llama3.18b 86.0 82.6
ArcChallenge Knowledge Llama3.170b 95.0 93.6
ArcChallenge Knowledge Gemma29b 91.0 89.6
ArcChallenge Knowledge Phi-3Small8k 91.6 91.0
ArcChallenge Knowledge Qwen27b 83.9 75.3
ArcChallenge Knowledge Qwen272b 96.3 94.6
ArcChallenge Knowledge GPT-4oMini 93.3 82.6
ArcChallenge Knowledge GPT-4o 96.0 95.3
ArcChallenge Knowledge Claude-3Haiku 89.3 89.3
ArcChallenge Knowledge Claude-3.5Sonnet 96.0 95.3
ArcChallenge Knowledge Gemini1.5Flash 92.3 93.6
ArcChallenge Knowledge Gemini1.5Pro 91.6 90.6
AGIEvalLSATAR SoftReasoning Llama27b 17.0 17.4
AGIEvalLSATAR SoftReasoning Mistral7b 21.7 19.1
AGIEvalLSATAR SoftReasoning Llama3.18b 20.4 26.1
AGIEvalLSATAR SoftReasoning Llama3.170b 32.6 28.7
AGIEvalLSATAR SoftReasoning Gemma29b 24.8 23.0
AGIEvalLSATAR SoftReasoning Phi-3Small8k 28.3 26.5
AGIEvalLSATAR SoftReasoning Qwen27b 27.0 23.9
AGIEvalLSATAR SoftReasoning Qwen272b 29.1 28.3
AGIEvalLSATAR SoftReasoning GPT-4oMini 32.2 23.0
AGIEvalLSATAR SoftReasoning GPT-4o 37.8 30.0
AGIEvalLSATAR SoftReasoning Claude-3Haiku 24.8 23.5
AGIEvalLSATAR SoftReasoning Claude-3.5Sonnet 38.3 33.9
AGIEvalLSATAR SoftReasoning Gemini1.5Flash 27.8 27.8
AGIEvalLSATAR SoftReasoning Gemini1.5Pro 30.0 31.7
BiGGenBench SoftReasoning Llama27b 61.6 56.8
BiGGenBench SoftReasoning Mistral7b 70.1 68.1
BiGGenBench SoftReasoning Llama3.18b 66.5 67.7
BiGGenBench SoftReasoning Llama3.170b 78.9 76.9
BiGGenBench SoftReasoning Gemma29b 64.7 64.5
BiGGenBench SoftReasoning Phi-3Small8k 69.7 63.0
BiGGenBench SoftReasoning Qwen27b 46.2 69.9
BiGGenBench SoftReasoning Qwen272b 74.3 79.9
BiGGenBench SoftReasoning GPT-4oMini 70.3 77.7
BiGGenBench SoftReasoning GPT-4o 86.0 82.0
BiGGenBench SoftReasoning Claude-3Haiku 80.0 80.0
BiGGenBench SoftReasoning Claude-3.5Sonnet 91.4 79.3
BiGGenBench SoftReasoning Gemini1.5Flash 73.9 68.5
BiGGenBench SoftReasoning Gemini1.5Pro 78.7 67.1
Winogrande Commonsense Llama27b 49.9 50.4
Winogrande Commonsense Mistral7b 60.4 56.5
Winogrande Commonsense Llama3.18b 66.5 63.3
Winogrande Commonsense Llama3.170b 84.2 81.2
Winogrande Commonsense Gemma29b 68.7 67.7
Winogrande Commonsense Phi-3Small8k 81.5 81.6
Winogrande Commonsense Qwen27b 67.1 60.7
Winogrande Commonsense Qwen272b 81.9 80.7
Winogrande Commonsense GPT-4oMini 79.2 71.9
Winogrande Commonsense GPT-4o 89.7 86.5
Winogrande Commonsense Claude-3Haiku 70.7 66.2
Winogrande Commonsense Claude-3.5Sonnet 89.4 85.7
Winogrande Commonsense Gemini1.5Flash 72.5 74.8
Winogrande Commonsense Gemini1.5Pro 75.5 78.3
MMLU Knowledge Llama27b 46.3 41.7
MMLU Knowledge Mistral7b 60.5 56.5
MMLU Knowledge Llama3.18b 72.6 67.5
MMLU Knowledge Llama3.170b 85.0 83.2
MMLU Knowledge Gemma29b 73.8 71.4
MMLU Knowledge Phi-3Small8k 76.3 73.6
MMLU Knowledge Qwen27b 67.0 64.5
MMLU Knowledge Qwen272b 81.3 77.8
MMLU Knowledge GPT-4oMini 79.9 74.8
MMLU Knowledge GPT-4o 87.5 83.4
MMLU Knowledge Claude-3Haiku 72.2 68.4
MMLU Knowledge Claude-3.5Sonnet 87.2 84.0
MMLU Knowledge Gemini1.5Flash 76.3 74.7
MMLU Knowledge Gemini1.5Pro 81.3 81.1
StrategyQA Commonsense Llama27b 39.5 31.2
StrategyQA Commonsense Mistral7b 66.1 55.8
StrategyQA Commonsense Llama3.18b 73.7 68.6
StrategyQA Commonsense Llama3.170b 85.3 83.8
StrategyQA Commonsense Gemma29b 73.7 66.4
StrategyQA Commonsense Phi-3Small8k 72.3 66.0
StrategyQA Commonsense Qwen27b 63.2 54.8
27Table7: Zero-shotaccuracyfordirectansweringandCoTpromptsonalldatasets
Dataset Type Model zero-shotDAaccuracy zero-shotCoTaccuracy
StrategyQA Commonsense Qwen272b 81.7 76.9
StrategyQA Commonsense GPT-4oMini 82.2 84.5
StrategyQA Commonsense GPT-4o 84.5 85.5
StrategyQA Commonsense Claude-3Haiku 73.4 65.0
StrategyQA Commonsense Claude-3.5Sonnet 80.1 76.3
StrategyQA Commonsense Gemini1.5Flash 72.5 75.2
StrategyQA Commonsense Gemini1.5Pro 74.0 71.4
MuSRObjectPlacements SoftReasoning Llama27b 36.3 30.5
MuSRObjectPlacements SoftReasoning Mistral7b 50.8 43.4
MuSRObjectPlacements SoftReasoning Llama3.18b 55.5 53.5
MuSRObjectPlacements SoftReasoning Llama3.170b 65.6 43.8
MuSRObjectPlacements SoftReasoning Gemma29b 63.3 57.0
MuSRObjectPlacements SoftReasoning Phi-3Small8k 53.1 55.1
MuSRObjectPlacements SoftReasoning Qwen27b 48.8 48.4
MuSRObjectPlacements SoftReasoning Qwen272b 61.7 45.7
MuSRObjectPlacements SoftReasoning GPT-4oMini 59.0 55.0
MuSRObjectPlacements SoftReasoning GPT-4o 67.6 45.3
MuSRObjectPlacements SoftReasoning Claude-3Haiku 46.9 52.3
MuSRObjectPlacements SoftReasoning Claude-3.5Sonnet 69.5 51.2
MuSRObjectPlacements SoftReasoning Gemini1.5Flash 61.7 56.2
MuSRObjectPlacements SoftReasoning Gemini1.5Pro 66.4 50.0
FOLIO Symbolic Llama27b 36.5 33.0
FOLIO Symbolic Mistral7b 50.7 41.9
FOLIO Symbolic Llama3.18b 58.6 56.7
FOLIO Symbolic Llama3.170b 70.9 69.0
FOLIO Symbolic Gemma29b 66.0 55.7
FOLIO Symbolic Phi-3Small8k 68.0 59.6
FOLIO Symbolic Qwen27b 60.6 51.2
FOLIO Symbolic Qwen272b 65.0 65.0
FOLIO Symbolic GPT-4oMini 65.0 58.1
FOLIO Symbolic GPT-4o 79.8 62.6
FOLIO Symbolic Claude-3Haiku 61.6 48.8
FOLIO Symbolic Claude-3.5Sonnet 73.9 68.5
FOLIO Symbolic Gemini1.5Flash 74.9 69.5
FOLIO Symbolic Gemini1.5Pro 73.9 74.4
ContextHubDeductiveL2 Symbolic Llama27b 34.8 12.6
ContextHubDeductiveL2 Symbolic Mistral7b 48.8 55.1
ContextHubDeductiveL2 Symbolic Llama3.18b 52.8 21.5
ContextHubDeductiveL2 Symbolic Llama3.170b 50.0 41.1
ContextHubDeductiveL2 Symbolic Gemma29b 50.0 43.0
ContextHubDeductiveL2 Symbolic Phi-3Small8k 52.4 49.1
ContextHubDeductiveL2 Symbolic Qwen27b 51.3 39.8
ContextHubDeductiveL2 Symbolic Qwen272b 52.8 44.0
ContextHubDeductiveL2 Symbolic GPT-4oMini 47.0 42.0
ContextHubDeductiveL2 Symbolic GPT-4o 54.5 45.6
ContextHubDeductiveL2 Symbolic Claude-3Haiku 45.2 41.8
ContextHubDeductiveL2 Symbolic Claude-3.5Sonnet 53.0 46.2
ContextHubDeductiveL2 Symbolic Gemini1.5Flash 45.0 39.5
ContextHubDeductiveL2 Symbolic Gemini1.5Pro 57.3 43.3
ContextHubAbductiveL2 Symbolic Llama27b 34.3 31.9
ContextHubAbductiveL2 Symbolic Mistral7b 34.0 25.7
ContextHubAbductiveL2 Symbolic Llama3.18b 41.3 37.3
ContextHubAbductiveL2 Symbolic Llama3.170b 51.0 44.4
ContextHubAbductiveL2 Symbolic Gemma29b 41.5 32.9
ContextHubAbductiveL2 Symbolic Phi-3Small8k 44.3 32.8
ContextHubAbductiveL2 Symbolic Qwen27b 37.8 33.4
ContextHubAbductiveL2 Symbolic Qwen272b 45.5 32.2
ContextHubAbductiveL2 Symbolic GPT-4oMini 65.0 55.0
ContextHubAbductiveL2 Symbolic GPT-4o 57.5 46.8
ContextHubAbductiveL2 Symbolic Claude-3Haiku 37.0 31.4
ContextHubAbductiveL2 Symbolic Claude-3.5Sonnet 56.8 40.4
ContextHubAbductiveL2 Symbolic Gemini1.5Flash 53.1 32.2
ContextHubAbductiveL2 Symbolic Gemini1.5Pro 53.5 43.7
MMLUPro Knowledge Llama27b 19.9 19.6
MMLUPro Knowledge Mistral7b 31.6 28.4
MMLUPro Knowledge Llama3.18b 44.8 38.0
MMLUPro Knowledge Llama3.170b 64.9 55.0
MMLUPro Knowledge Gemma29b 48.1 42.7
MMLUPro Knowledge Phi-3Small8k 54.8 43.7
MMLUPro Knowledge Qwen27b 45.0 36.2
MMLUPro Knowledge Qwen272b 62.8 44.3
MMLUPro Knowledge GPT-4oMini 62.3 42.6
MMLUPro Knowledge GPT-4o 72.1 55.0
MMLUPro Knowledge Claude-3Haiku 47.6 39.0
MMLUPro Knowledge Claude-3.5Sonnet 73.4 57.2
MMLUPro Knowledge Gemini1.5Flash 58.5 47.2
MMLUPro Knowledge Gemini1.5Pro 65.3 57.4
28Table7: Zero-shotaccuracyfordirectansweringandCoTpromptsonalldatasets
Dataset Type Model zero-shotDAaccuracy zero-shotCoTaccuracy
MuSRMurderMysteries SoftReasoning Llama27b 50.0 50.0
MuSRMurderMysteries SoftReasoning Mistral7b 62.8 55.6
MuSRMurderMysteries SoftReasoning Llama3.18b 70.4 57.2
MuSRMurderMysteries SoftReasoning Llama3.170b 73.6 69.6
MuSRMurderMysteries SoftReasoning Gemma29b 76.8 61.6
MuSRMurderMysteries SoftReasoning Phi-3Small8k 61.6 58.8
MuSRMurderMysteries SoftReasoning Qwen27b 59.2 53.2
MuSRMurderMysteries SoftReasoning Qwen272b 80.8 64.4
MuSRMurderMysteries SoftReasoning GPT-4oMini 71.2 63.6
MuSRMurderMysteries SoftReasoning GPT-4o 87.6 70.8
MuSRMurderMysteries SoftReasoning Claude-3Haiku 62.4 56.8
MuSRMurderMysteries SoftReasoning Claude-3.5Sonnet 85.2 70.4
MuSRMurderMysteries SoftReasoning Gemini1.5Flash 70.8 58.4
MuSRMurderMysteries SoftReasoning Gemini1.5Pro 77.6 64.0
ContextHubDeductiveL1 Symbolic Llama27b 47.7 8.3
ContextHubDeductiveL1 Symbolic Mistral7b 50.3 67.3
ContextHubDeductiveL1 Symbolic Llama3.18b 50.7 23.3
ContextHubDeductiveL1 Symbolic Llama3.170b 53.8 40.7
ContextHubDeductiveL1 Symbolic Gemma29b 56.3 39.2
ContextHubDeductiveL1 Symbolic Phi-3Small8k 54.8 50.2
ContextHubDeductiveL1 Symbolic Qwen27b 59.3 43.3
ContextHubDeductiveL1 Symbolic Qwen272b 51.5 44.0
ContextHubDeductiveL1 Symbolic GPT-4oMini 49.3 41.5
ContextHubDeductiveL1 Symbolic GPT-4o 59.3 49.0
ContextHubDeductiveL1 Symbolic Claude-3Haiku 50.5 39.7
ContextHubDeductiveL1 Symbolic Claude-3.5Sonnet 54.5 47.0
ContextHubDeductiveL1 Symbolic Gemini1.5Flash 47.3 38.5
ContextHubDeductiveL1 Symbolic Gemini1.5Pro 57.3 46.0
ContextHubAbductiveL1 Symbolic Llama27b 29.4 16.4
ContextHubAbductiveL1 Symbolic Mistral7b 46.9 25.8
ContextHubAbductiveL1 Symbolic Llama3.18b 43.6 24.2
ContextHubAbductiveL1 Symbolic Llama3.170b 55.3 43.9
ContextHubAbductiveL1 Symbolic Gemma29b 61.9 58.9
ContextHubAbductiveL1 Symbolic Phi-3Small8k 62.5 60.3
ContextHubAbductiveL1 Symbolic Qwen27b 52.2 47.5
ContextHubAbductiveL1 Symbolic Qwen272b 61.9 45.0
ContextHubAbductiveL1 Symbolic GPT-4oMini 61.1 42.2
ContextHubAbductiveL1 Symbolic GPT-4o 74.2 65.6
ContextHubAbductiveL1 Symbolic Claude-3Haiku 35.3 22.8
ContextHubAbductiveL1 Symbolic Claude-3.5Sonnet 80.8 60.3
ContextHubAbductiveL1 Symbolic Gemini1.5Flash 66.4 47.2
ContextHubAbductiveL1 Symbolic Gemini1.5Pro 62.2 60.0
Big-BenchHard Symbolic Llama27b 29.8 31.9
Big-BenchHard Symbolic Mistral7b 39.3 35.1
Big-BenchHard Symbolic Llama3.18b 62.8 45.6
Big-BenchHard Symbolic Llama3.170b 78.9 54.8
Big-BenchHard Symbolic Gemma29b 58.7 50.8
Big-BenchHard Symbolic Phi-3Small8k 70.0 55.1
Big-BenchHard Symbolic Qwen27b 52.6 47.6
Big-BenchHard Symbolic Qwen272b 75.1 59.0
Big-BenchHard Symbolic GPT-4oMini 77.7 49.7
Big-BenchHard Symbolic GPT-4o 84.6 64.5
Big-BenchHard Symbolic Claude-3Haiku 62.4 47.3
Big-BenchHard Symbolic Claude-3.5Sonnet 83.6 56.9
Big-BenchHard Symbolic Gemini1.5Flash 71.3 55.4
Big-BenchHard Symbolic Gemini1.5Pro 71.6 50.3
MATH Mathematical Llama27b 4.2 4.0
MATH Mathematical Mistral7b 12.4 6.1
MATH Mathematical Llama3.18b 47.2 13.8
MATH Mathematical Llama3.170b 64.4 22.8
MATH Mathematical Gemma29b 45.6 19.1
MATH Mathematical Phi-3Small8k 43.2 18.5
MATH Mathematical Qwen27b 53.7 13.3
MATH Mathematical Qwen272b 63.5 23.8
MATH Mathematical GPT-4oMini 69.6 24.3
MATH Mathematical GPT-4o 73.3 35.2
MATH Mathematical Claude-3Haiku 32.7 17.4
MATH Mathematical Claude-3.5Sonnet 63.8 34.6
MATH Mathematical Gemini1.5Flash 54.5 31.3
MATH Mathematical Gemini1.5Pro 62.1 39.4
GSM8K-Hard Mathematical Llama27b 6.7 1.8
GSM8K-Hard Mathematical Mistral7b 21.0 3.0
GSM8K-Hard Mathematical Llama3.18b 34.4 6.0
GSM8K-Hard Mathematical Llama3.170b 46.6 14.0
GSM8K-Hard Mathematical Gemma29b 40.9 8.8
GSM8K-Hard Mathematical Phi-3Small8k 33.0 6.9
GSM8K-Hard Mathematical Qwen27b 48.4 5.0
29Table7: Zero-shotaccuracyfordirectansweringandCoTpromptsonalldatasets
Dataset Type Model zero-shotDAaccuracy zero-shotCoTaccuracy
GSM8K-Hard Mathematical Qwen272b 54.8 13.7
GSM8K-Hard Mathematical GPT-4oMini 53.9 11.7
GSM8K-Hard Mathematical GPT-4o 60.3 26.0
GSM8K-Hard Mathematical Claude-3Haiku 45.3 9.6
GSM8K-Hard Mathematical Claude-3.5Sonnet 50.8 32.3
GSM8K-Hard Mathematical Gemini1.5Flash 54.6 16.2
GSM8K-Hard Mathematical Gemini1.5Pro 58.2 26.2
GSM8K Mathematical Llama27b 29.6 6.9
GSM8K Mathematical Mistral7b 59.2 10.2
GSM8K Mathematical Llama3.18b 85.4 18.5
GSM8K Mathematical Llama3.170b 85.6 37.0
GSM8K Mathematical Gemma29b 89.2 24.9
GSM8K Mathematical Phi-3Small8k 90.0 24.9
GSM8K Mathematical Qwen27b 87.9 20.7
GSM8K Mathematical Qwen272b 94.6 40.1
GSM8K Mathematical GPT-4oMini 94.1 31.8
GSM8K Mathematical GPT-4o 95.8 58.8
GSM8K Mathematical Claude-3Haiku 89.4 22.9
GSM8K Mathematical Claude-3.5Sonnet 96.1 62.2
GSM8K Mathematical Gemini1.5Flash 91.4 38.6
GSM8K Mathematical Gemini1.5Pro 92.7 52.4
C.2 FULLFEW-SHOTRESULTS
Table8: Few-shotaccuracyfordirectansweringandCoTpromptsonalldatasets
Dataset Type Model few-shotDAaccuracy few-shotCoTaccuracy
AGIEvalLSATRC SoftReasoning Llama27b 33.1 38.7
AGIEvalLSATRC SoftReasoning Mistral7b 52.4 57.2
AGIEvalLSATRC SoftReasoning Llama3.18b 60.2 70.3
AGIEvalLSATRC SoftReasoning Llama3.170b 84.4 88.8
AGIEvalLSATRC SoftReasoning Gemma29b 74.3 79.2
AGIEvalLSATRC SoftReasoning Phi-3Small8k 63.2 65.1
AGIEvalLSATRC SoftReasoning Qwen27b 61.7 68.8
AGIEvalLSATRC SoftReasoning Qwen272b 85.9 85.9
AGIEvalLSATRC SoftReasoning GPT-4oMini 77.3 71.4
AGIEvalLSATRC SoftReasoning Gemini1.5Flash 79.2 81.8
AGIEvalLSATLR SoftReasoning Llama27b 33.7 34.7
AGIEvalLSATLR SoftReasoning Mistral7b 46.1 48.0
AGIEvalLSATLR SoftReasoning Llama3.18b 55.7 58.0
AGIEvalLSATLR SoftReasoning Llama3.170b 83.3 85.1
AGIEvalLSATLR SoftReasoning Gemma29b 65.7 68.2
AGIEvalLSATLR SoftReasoning Phi-3Small8k 64.7 59.2
AGIEvalLSATLR SoftReasoning Qwen27b 54.1 61.2
AGIEvalLSATLR SoftReasoning Qwen272b 77.5 79.6
AGIEvalLSATLR SoftReasoning GPT-4oMini 68.4 64.5
AGIEvalLSATLR SoftReasoning Gemini1.5Flash 68.6 72.9
GPQA Mathematical Mistral7b 23.0 25.9
GPQA Mathematical Llama3.18b 22.1 27.2
GPQA Mathematical Llama3.170b 24.8 24.3
GPQA Mathematical Gemma29b 19.9 22.3
GPQA Mathematical Phi-3Small8k 23.9 22.5
GPQA Mathematical Qwen27b 23.4 21.2
GPQA Mathematical Qwen272b 22.8 19.9
GPQA Mathematical GPT-4oMini 20.0 20.0
GPQA Mathematical Gemini1.5Flash 21.9 24.6
CommonsenseQA Commonsense Llama27b 18.2 19.2
CommonsenseQA Commonsense Mistral7b 73.6 70.4
CommonsenseQA Commonsense Llama3.18b 74.0 76.5
CommonsenseQA Commonsense Llama3.170b 84.7 84.6
CommonsenseQA Commonsense Gemma29b 81.8 80.8
CommonsenseQA Commonsense Phi-3Small8k 80.8 80.4
CommonsenseQA Commonsense Qwen27b 80.3 72.9
CommonsenseQA Commonsense Qwen272b 88.4 87.8
CommonsenseQA Commonsense GPT-4oMini 84.7 84.7
CommonsenseQA Commonsense Gemini1.5Flash 81.7 83.3
AGIEvalLSATAR SoftReasoning Llama27b 19.6 18.7
AGIEvalLSATAR SoftReasoning Mistral7b 20.9 22.6
AGIEvalLSATAR SoftReasoning Llama3.18b 24.8 26.1
AGIEvalLSATAR SoftReasoning Llama3.170b 36.1 30.9
AGIEvalLSATAR SoftReasoning Gemma29b 22.2 28.7
AGIEvalLSATAR SoftReasoning Phi-3Small8k 27.8 20.0
AGIEvalLSATAR SoftReasoning Qwen27b 24.3 23.0
30Table8: Few-shotaccuracyfordirectansweringandCoTpromptsonalldatasets
Dataset Type Model few-shotDAaccuracy few-shotCoTaccuracy
AGIEvalLSATAR SoftReasoning Qwen272b 27.0 30.0
AGIEvalLSATAR SoftReasoning GPT-4oMini 28.7 26.1
AGIEvalLSATAR SoftReasoning Gemini1.5Flash 28.3 20.4
MMLU Knowledge Llama27b 49.0 42.8
MMLU Knowledge Mistral7b 63.0 57.0
MMLU Knowledge Llama3.18b 71.7 69.3
MMLU Knowledge Llama3.170b 84.3 83.7
MMLU Knowledge Gemma29b 74.7 72.4
MMLU Knowledge Phi-3Small8k 77.3 75.2
MMLU Knowledge Qwen27b 69.9 68.6
MMLU Knowledge Qwen272b 82.7 81.8
MMLU Knowledge GPT-4oMini 82.3 77.8
MMLU Knowledge Gemini1.5Flash 78.1 79.0
StrategyQA Commonsense Llama27b 57.9 30.9
StrategyQA Commonsense Mistral7b 70.7 72.0
StrategyQA Commonsense Llama3.18b 74.4 65.8
StrategyQA Commonsense Llama3.170b 87.1 84.2
StrategyQA Commonsense Gemma29b 77.1 73.3
StrategyQA Commonsense Phi-3Small8k 75.0 71.1
StrategyQA Commonsense Qwen27b 71.9 58.9
StrategyQA Commonsense Qwen272b 83.2 80.1
StrategyQA Commonsense GPT-4oMini 83.0 86.2
StrategyQA Commonsense Gemini1.5Flash 77.0 80.3
ContextHubAbductiveL2 Symbolic Llama27b 36.2 35.0
ContextHubAbductiveL2 Symbolic Mistral7b 33.8 30.0
ContextHubAbductiveL2 Symbolic Llama3.18b 32.7 36.1
ContextHubAbductiveL2 Symbolic Llama3.170b 54.6 51.2
ContextHubAbductiveL2 Symbolic Gemma29b 44.8 33.2
ContextHubAbductiveL2 Symbolic Phi-3Small8k 49.8 34.2
ContextHubAbductiveL2 Symbolic Qwen27b 39.6 35.0
ContextHubAbductiveL2 Symbolic Qwen272b 54.7 34.9
ContextHubAbductiveL2 Symbolic GPT-4oMini 62.0 60.0
ContextHubAbductiveL2 Symbolic Gemini1.5Flash 48.6 47.8
ContextHubAbductiveL1 Symbolic Llama27b 21.4 16.7
ContextHubAbductiveL1 Symbolic Mistral7b 23.6 21.7
ContextHubAbductiveL1 Symbolic Llama3.18b 40.0 36.1
ContextHubAbductiveL1 Symbolic Llama3.170b 62.2 58.9
ContextHubAbductiveL1 Symbolic Gemma29b 48.9 59.4
ContextHubAbductiveL1 Symbolic Phi-3Small8k 59.2 56.4
ContextHubAbductiveL1 Symbolic Qwen27b 48.6 38.9
ContextHubAbductiveL1 Symbolic Qwen272b 53.3 56.1
ContextHubAbductiveL1 Symbolic GPT-4oMini 77.2 59.2
ContextHubAbductiveL1 Symbolic Gemini1.5Flash 79.7 68.6
MuSRMurderMysteries SoftReasoning Mistral7b 62.0 56.4
MuSRMurderMysteries SoftReasoning Llama3.18b 61.6 61.2
MuSRMurderMysteries SoftReasoning Llama3.170b 73.2 68.0
MuSRMurderMysteries SoftReasoning Gemma29b 81.6 62.0
MuSRMurderMysteries SoftReasoning Phi-3Small8k 62.0 53.6
MuSRMurderMysteries SoftReasoning Qwen27b 56.0 55.6
MuSRMurderMysteries SoftReasoning Qwen272b 80.4 66.0
MuSRMurderMysteries SoftReasoning GPT-4oMini 76.0 69.6
MuSRMurderMysteries SoftReasoning Gemini1.5Flash 70.0 66.4
MuSRTeamAllocations SoftReasoning Mistral7b 42.8 43.2
MuSRTeamAllocations SoftReasoning Llama3.18b 59.6 51.6
MuSRTeamAllocations SoftReasoning Llama3.170b 89.2 63.6
MuSRTeamAllocations SoftReasoning Gemma29b 48.4 45.6
MuSRTeamAllocations SoftReasoning Phi-3Small8k 66.0 46.4
MuSRTeamAllocations SoftReasoning Qwen27b 34.0 40.8
MuSRTeamAllocations SoftReasoning Qwen272b 56.0 66.4
MuSRTeamAllocations SoftReasoning GPT-4oMini 75.6 60.0
MuSRTeamAllocations SoftReasoning Gemini1.5Flash 90.0 54.4
MMLUPro Knowledge Llama27b 21.5 20.4
MMLUPro Knowledge Mistral7b 34.8 26.7
MMLUPro Knowledge Llama3.18b 44.7 38.0
MMLUPro Knowledge Llama3.170b 64.4 55.1
MMLUPro Knowledge Gemma29b 48.5 42.4
MMLUPro Knowledge Phi-3Small8k 54.8 43.2
MMLUPro Knowledge Qwen27b 46.6 39.0
MMLUPro Knowledge Qwen272b 62.5 51.6
MMLUPro Knowledge GPT-4oMini 63.0 45.0
MMLUPro Knowledge Gemini1.5Flash 59.4 50.6
MuSRObjectPlacements SoftReasoning Mistral7b 55.5 41.0
MuSRObjectPlacements SoftReasoning Llama3.18b 66.8 50.4
MuSRObjectPlacements SoftReasoning Llama3.170b 67.2 57.4
MuSRObjectPlacements SoftReasoning Gemma29b 68.0 58.2
MuSRObjectPlacements SoftReasoning Phi-3Small8k 62.1 51.6
MuSRObjectPlacements SoftReasoning Qwen27b 46.9 43.8
31Table8: Few-shotaccuracyfordirectansweringandCoTpromptsonalldatasets
Dataset Type Model few-shotDAaccuracy few-shotCoTaccuracy
MuSRObjectPlacements SoftReasoning Qwen272b 66.4 43.0
MuSRObjectPlacements SoftReasoning GPT-4oMini 67.0 47.0
MuSRObjectPlacements SoftReasoning Gemini1.5Flash 73.0 54.7
ContextHubDeductiveL2 Symbolic Llama27b 34.7 15.0
ContextHubDeductiveL2 Symbolic Mistral7b 63.8 51.4
ContextHubDeductiveL2 Symbolic Llama3.18b 76.1 27.3
ContextHubDeductiveL2 Symbolic Llama3.170b 82.6 53.6
ContextHubDeductiveL2 Symbolic Gemma29b 61.9 47.6
ContextHubDeductiveL2 Symbolic Phi-3Small8k 61.5 54.0
ContextHubDeductiveL2 Symbolic Qwen27b 55.3 36.4
ContextHubDeductiveL2 Symbolic Qwen272b 80.2 54.0
ContextHubDeductiveL2 Symbolic GPT-4oMini 59.0 41.0
ContextHubDeductiveL2 Symbolic Gemini1.5Flash 90.2 42.5
ContextHubDeductiveL1 Symbolic Llama27b 34.7 16.0
ContextHubDeductiveL1 Symbolic Mistral7b 46.2 59.2
ContextHubDeductiveL1 Symbolic Llama3.18b 73.0 23.0
ContextHubDeductiveL1 Symbolic Llama3.170b 67.5 50.0
ContextHubDeductiveL1 Symbolic Gemma29b 66.0 45.7
ContextHubDeductiveL1 Symbolic Phi-3Small8k 74.8 51.8
ContextHubDeductiveL1 Symbolic Qwen27b 58.8 37.5
ContextHubDeductiveL1 Symbolic Qwen272b 70.7 42.8
ContextHubDeductiveL1 Symbolic GPT-4oMini 59.2 44.3
ContextHubDeductiveL1 Symbolic Gemini1.5Flash 89.3 49.8
MATH Mathematical Llama27b 4.7 3.9
MATH Mathematical Mistral7b 13.7 7.1
MATH Mathematical Llama3.18b 41.2 14.2
MATH Mathematical Llama3.170b 61.9 24.2
MATH Mathematical Gemma29b 47.5 19.8
MATH Mathematical Phi-3Small8k 42.4 18.9
MATH Mathematical Qwen27b 55.0 15.0
MATH Mathematical Qwen272b 65.3 26.2
MATH Mathematical GPT-4oMini 71.7 24.6
MATH Mathematical Gemini1.5Flash 54.7 32.3
GSM8K Mathematical Llama27b 29.0 7.7
GSM8K Mathematical Mistral7b 56.2 12.5
GSM8K Mathematical Llama3.18b 86.4 20.1
GSM8K Mathematical Llama3.170b 96.1 39.1
GSM8K Mathematical Gemma29b 89.2 24.9
GSM8K Mathematical Phi-3Small8k 90.4 24.5
GSM8K Mathematical Qwen27b 87.6 21.4
GSM8K Mathematical Qwen272b 93.2 40.6
GSM8K Mathematical GPT-4oMini 94.2 32.8
GSM8K Mathematical Gemini1.5Flash 90.6 40.4
C.3 ANSWEREXTRACTORANDAVERAGEANSWERSPANRESULTS
In this section, we report the number of generations from each model on each dataset that our
answerparsercouldnotextract. “-1”denotesthatamodelwasnotrunonacertaindatasetdueto
contextlengthlimitationsinthefew-shotsetting. Weseethattheseunparseableratesaregenerally
lowacross theboard. Theweakest modelsstruggle onsome ofthemost challengingdatasets, but
unparseableratesareallatorbelow15%.
We also report the average character index of the beginning of the answer span that the answer
parser extracted. Of particular note is that the direct answer prompts all return an answer within
thefirst60characters,indicatingthattheanswersarereturnedalmostimmediately,asdesired. CoT
completionsaremuchlonger.
D QUANTITATIVE META-ANALYSIS
Seethefulllistofcategoriesandtheirdescriptionsthatweusedforthemeta-analysisinTable17.
E PERFORMANCE IMPACTS OF “=” ON MMLU AND MMLU PRO
Tables18and19showtheamountoftotalimprovementfromusingCoToverdirectpromptingthat
canbeexplainedbythepresenceof“=”onMMLUandMMLUProovermultiplemodels.
32Figure 8: Performance of zero-shot direct (blue) and zero-shot CoT (orange) across datasets and
models. Graphsaresortedinascendingorderbymediandelta(CoT,direct). Thedatasetsbenefiting
substantiallyareallsymbolicorsemi-symbolicinnature.
33Table9: Percentageofresponsesperdatasetpermodelthatouranswerparsercouldnotextractan
answerforinthezero-shotdirectanswersetting.Promptmodificationsweremadetodecreasethese
numbers. Nomodelisabove15%.
Zero-shotDirectAnswerUnparseableAnswerRatebyPercentage
dataset
Meta-Llama27b Mistral7b Meta-Llama3.18b Meta-Llama3.170b Gemma29b Phi-3Small8k Qwen27b Qwen272b GPT-4oMini
GPT-4o
Claude-3Haiku Claude-3.5Sonnet Gemini1.5Flash Gemini1.5Pro
CommonsenseQA 1.9 2.5 1.1 0.0 0.8 0.1 1.6 0.7 0.0 0.0 0.1 0.0 0.1 0.2
StrategyQA 0.0 1.9 0.1 0.0 11.7 0.5 4.9 2.7 0.0 0.0 0.0 0.0 0.0 0.2
SiQA 0.2 6.6 0.0 0.1 3.9 0.3 0.1 3.0 0.1 0.1 0.0 0.0 0.0 0.4
PiQA 0.4 6.0 0.0 0.1 3.3 2.1 0.0 5.5 0.2 0.0 0.1 0.0 0.1 0.9
Winogrande 0.0 3.0 0.1 0.0 2.1 0.2 5.1 0.4 0.0 0.0 0.0 0.0 0.0 3.6
ArcEasy 0.0 1.8 0.5 0.0 0.0 0.2 9.1 0.7 3.5 0.4 0.2 0.0 0.0 3.2
ArcChallenge 0.0 2.3 1.0 0.0 0.3 0.7 10.7 0.7 10.0 0.7 0.0 0.0 0.0 5.0
AGIEvalLSATLR 0.4 0.0 0.0 0.0 0.0 0.2 0.0 0.0 0.0 2.5 0.0 0.0 0.2 0.2
AGIEvalLSATAR 0.4 0.0 0.0 0.0 4.3 3.9 0.0 0.0 0.0 8.7 0.0 0.0 0.0 0.0
AGIEvalLSATRC 0.4 0.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 9.7 0.0 0.0 0.4 0.4
ContextHubDeductiveL1 0.0 0.0 0.0 0.0 1.2 0.0 2.3 0.0 0.0 0.0 0.2 0.0 0.0 0.2
ContextHubDeductiveL2 0.0 0.0 0.0 0.0 0.0 0.0 2.2 1.0 0.0 0.0 2.8 0.0 0.0 0.0
ContextHubAbductiveL1 0.0 0.0 0.0 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
ContextHubAbductiveL2 0.0 0.0 0.0 0.0 0.0 0.1 1.5 0.2 0.0 0.0 0.8 0.0 0.0 0.0
MuSRMurderMysteries 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
MuSRTeamAllocations 0.0 0.0 0.0 0.0 3.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 8.4 0.4
MuSRObjectPlacements 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
MMLU 0.1 0.0 0.0 0.0 0.1 0.2 3.6 1.2 0.6 0.0 1.3 0.3 0.2 0.7
MMLUPro 0.7 1.3 1.0 0.3 1.0 3.7 6.8 12.2 0.4 0.3 0.3 0.4 0.6 0.8
GPQA 1.3 7.1 0.0 0.0 8.7 12.7 5.4 15.2 0.0 0.0 1.6 0.0 0.0 0.7
MATH 0.6 6.9 0.3 0.2 0.1 0.1 3.5 3.0 0.8 0.0 0.3 0.0 0.4 0.6
GSM8K 0.2 4.1 2.5 0.0 2.7 0.0 1.7 0.2 0.0 0.0 12.7 5.5 0.0 0.0
BiGGenBench 4.6 0.3 0.9 0.1 0.5 1.0 1.3 1.0 1.3 0.0 0.0 0.1 0.4 0.3
GSM8K-Hard 4.8 7.6 2.0 2.4 0.4 0.2 3.2 1.1 0.1 0.5 5.2 0.5 0.2 0.0
MuSiQue 0.1 0.0 0.0 0.0 0.0 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.1
FOLIO 4.4 0.0 0.0 0.0 0.0 0.0 3.9 0.0 0.0 12.3 0.0 0.0 0.0 0.5
Big-BenchHard 0.0 0.0 0.0 7.4 0.0 0.2 0.0 0.0 0.0 0.3 0.0 4.5 0.2 12.8
Table10: Percentageofresponsesperdatasetpermodelthatouranswerparsercouldnotextractan
answerforinthezero-shotCoTsetting.Promptmodificationsweremadetodecreasethesenumbers.
Nomodelisabove15%.
Zero-shotCoTUnparseableAnswerRatebyPercentage
dataset
Meta-Llama27b Mistral7b Meta-Llama3.18b Meta-Llama3.170b Gemma29b Phi-3Small8k Qwen27b Qwen272b GPT-4oMini
GPT-4o
Claude-3Haiku Claude-3.5Sonnet Gemini1.5Flash Gemini1.5Pro
CommonsenseQA 2.9 1.3 8.6 0.0 0.6 0.1 0.0 0.0 1.6 0.0 0.2 0.3 2.4 2.6
StrategyQA 1.0 0.1 1.1 0.8 0.3 0.4 0.3 0.0 0.0 0.0 0.0 0.0 2.1 4.4
SiQA 0.8 1.8 0.3 0.1 1.6 0.0 0.1 0.1 0.0 0.0 0.3 0.1 3.5 4.0
PiQA 1.6 1.6 0.2 0.1 2.8 0.3 0.5 0.3 0.0 0.0 1.4 0.3 4.6 4.6
Winogrande 0.9 1.4 0.2 0.2 0.9 0.4 0.3 0.0 0.0 0.0 0.0 0.0 2.0 3.4
ArcEasy 0.2 0.4 0.2 0.0 0.5 1.6 1.6 0.0 0.5 0.0 0.0 0.0 0.4 0.5
ArcChallenge 0.0 0.7 0.0 0.0 0.0 0.0 0.7 0.0 0.0 0.0 0.0 0.0 0.7 0.7
AGIEvalLSATLR 3.3 2.2 0.0 0.0 1.2 0.0 2.0 0.0 0.0 0.0 0.0 0.0 0.8 0.2
AGIEvalLSATAR 4.8 7.0 6.1 2.2 5.7 5.2 4.3 0.4 1.3 1.3 0.0 0.4 4.8 1.7
AGIEvalLSATRC 7.1 1.1 0.0 0.0 0.7 3.0 6.7 0.0 0.0 0.0 0.0 0.0 0.0 0.4
ContextHubDeductiveL1 0.7 1.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3
ContextHubDeductiveL2 0.2 0.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.4
ContextHubAbductiveL1 0.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
ContextHubAbductiveL2 0.0 0.2 0.1 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.4
MuSRMurderMysteries 0.0 0.4 0.0 0.0 0.0 11.6 0.4 0.0 0.0 0.0 0.0 0.0 6.8 3.6
MuSRTeamAllocations 5.2 3.2 0.8 0.0 0.8 0.4 0.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0
MuSRObjectPlacements 0.0 1.6 0.0 0.0 0.4 0.8 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.4
MMLU 1.9 0.6 1.0 0.2 1.5 1.0 0.4 0.2 0.0 0.1 0.0 0.1 3.1 3.2
MMLUPro 4.4 5.4 13.1 3.3 12.5 3.6 5.4 2.0 2.4 1.9 0.4 0.4 5.0 4.4
GPQA 4.5 10.3 9.4 1.6 8.5 1.8 3.8 0.7 0.0 0.0 0.0 0.0 11.8 15.0
MATH 1.6 5.5 8.2 2.5 2.3 1.6 3.0 0.4 0.4 0.5 0.9 0.0 1.7 1.0
GSM8K 1.7 1.4 0.7 10.5 0.4 0.6 0.4 0.0 0.0 0.0 0.3 0.0 0.1 0.1
BiGGenBench 5.0 0.4 0.5 0.1 0.5 0.4 0.3 9.5 0.0 0.0 0.0 0.1 0.4 0.1
GSM8K-Hard 2.1 8.7 10.2 4.5 10.7 3.2 3.5 1.0 0.8 0.5 3.0 1.8 0.4 2.7
MuSiQue 1.4 0.0 8.3 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.7 3.1
FOLIO 0.0 0.0 1.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 1.5
Big-BenchHard 3.8 5.4 1.8 0.4 1.3 0.1 0.4 0.3 0.0 0.0 0.0 0.0 1.2 0.9
34Table11: Percentageofresponsesperdatasetpermodelthatouranswerparsercouldnotextractan
answerforinthefew-shotdirectanswersetting. Promptmodificationsweremadetodecreasethese
numbers. Nomodelisabove15%.
Few-shotDirectAnswerUnparseableAnswerRatebyPercentage
dataset
Meta-Llama27b
Mistral7b
Meta-Llama3.18b Meta-Llama3.170b
Gemma29b Phi-3Small8k Qwen27b Qwen272b GPT-4oMini
Gemini1.5Flash
CommonsenseQA 0.0 0.1 0.2 0.0 1.3 0.9 9.9 1.3 0.0 0.6
AGIEvalLSATLR 6.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2
AGIEvalLSATAR 2.6 0.0 0.0 0.0 3.5 5.2 0.0 0.0 0.0 0.0
AGIEvalLSATRC 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
ContextHubDeductiveL1 0.0 2.8 0.0 0.0 0.0 10.7 0.3 0.0 0.0 0.0
ContextHubDeductiveL2 0.0 0.1 0.0 0.0 0.0 0.3 0.2 0.0 0.0 0.0
ContextHubAbductiveL1 0.0 2.8 0.0 0.0 0.0 0.6 0.0 0.0 0.0 0.0
ContextHubAbductiveL2 0.0 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
MuSRMurderMysteries -1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.4
MuSRTeamAllocations -1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
MuSRObjectPlacements -1.0 0.0 0.0 0.0 0.4 1.2 0.0 0.0 0.0 0.0
MMLU 4.2 0.2 0.0 0.0 0.1 0.0 0.4 0.1 0.0 0.2
MMLUPro 5.1 1.2 2.4 0.3 1.0 9.1 0.5 2.6 0.4 0.5
GPQA -1.0 1.3 0.0 0.0 3.6 7.4 13.4 1.1 0.0 0.0
MATH 0.3 5.9 0.3 0.2 0.1 0.1 1.6 2.2 0.0 0.3
GSM8K 0.1 0.1 0.5 0.0 0.1 2.2 0.0 0.2 0.0 0.0
Table12: Percentageofresponsesperdatasetpermodelthatouranswerparsercouldnotextractan
answerforinthefew-shotCoTsetting.Promptmodificationsweremadetodecreasethesenumbers.
Nomodelisabove15%.
Few-shotCoTUnparseableAnswerRatebyPercentage
dataset
Meta-Llama27b
Mistral7b
Meta-Llama3.18b Meta-Llama3.170b
Gemma29b Phi-3Small8k Qwen27b Qwen272b GPT-4oMini
Gemini1.5Flash
CommonsenseQA 0.7 0.9 1.8 0.1 0.2 0.1 0.0 0.0 0.0 3.4
AGIEvalLSATLR 0.6 0.8 0.4 0.0 1.4 3.1 0.8 0.0 0.0 0.6
AGIEvalLSATAR 2.2 9.1 3.9 0.9 11.7 3.0 3.5 1.7 0.0 1.3
AGIEvalLSATRC 7.8 5.9 0.0 0.0 1.9 9.3 2.6 0.0 0.0 2.2
ContextHubDeductiveL1 0.2 0.0 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.3
ContextHubDeductiveL2 0.9 0.0 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.3
ContextHubAbductiveL1 0.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
ContextHubAbductiveL2 3.1 0.0 5.3 0.1 0.0 0.2 0.0 0.0 0.0 0.7
MuSRMurderMysteries -1.0 1.2 0.0 0.0 0.4 0.8 0.0 0.0 0.0 14.0
MuSRTeamAllocations -1.0 2.4 0.0 0.0 0.0 0.0 0.8 0.0 0.0 0.4
MuSRObjectPlacements -1.0 0.4 0.0 0.0 1.2 0.4 0.0 0.0 0.0 0.0
MMLU 0.6 0.8 1.1 0.2 1.5 0.7 0.3 0.2 0.2 2.5
MMLUPro 0.6 1.9 8.5 2.1 14.1 1.8 1.9 0.8 1.1 3.9
GPQA -1.0 12.1 10.3 0.9 12.9 6.0 5.6 3.3 0.0 13.6
MATH 1.5 6.8 8.2 2.4 11.1 2.6 2.9 1.1 0.5 1.8
GSM8K 0.8 1.3 1.0 0.1 0.5 0.5 0.1 0.0 0.1 0.1
35Table13:Averagecharacterindexofwheretheanswerspanbeginsinageneratedresponseforeach
datasetandmodelpairforthezero-shotdirectanswersetting. Weusethesenumbersasaproxyfor
themodelfollowinginstructions(i.e.generatingreasoningbeforeananswer).Promptmodifications
weremadetoensureCoTpromptsresultedinlongergenerationsanddirectanswerpromptsledto
shortgenerations.
Zero-shotDirectAnswerSpanLocationByCharacterIndex
dataset
Meta-Llama27b Mistral7b Meta-Llama3.18b Meta-Llama3.170b Gemma29b Phi-3Small8k Qwen27b Qwen272b GPT-4oMini
GPT-4o
Claude-3Haiku Claude-3.5Sonnet Gemini1.5Flash Gemini1.5Pro
CommonsenseQA 9 8 27 8 8 8 10 8 8 10 7 7 8 8
StrategyQA 44 45 27 44 44 44 46 44 8 8 42 41 8 7
SiQA 8 8 8 8 8 8 29 8 8 8 6 6 8 8
PiQA 7 8 8 8 8 8 25 8 8 8 4 5 8 8
Winogrande 8 9 8 8 8 8 9 8 8 9 5 4 8 8
ArcEasy 9 8 8 8 8 8 9 8 8 8 7 7 8 8
ArcChallenge 8 8 8 8 8 8 9 8 8 8 7 7 8 8
AGIEvalLSATLR 25 24 24 24 24 24 25 24 43 21 25 25 26 26
AGIEvalLSATAR 25 24 24 24 24 24 26 24 48 23 25 25 26 27
AGIEvalLSATRC 25 24 24 24 24 24 25 24 31 18 25 25 26 25
ContextHubDeductiveL1 19 19 19 19 20 19 19 19 19 19 20 20 19 20
ContextHubDeductiveL2 19 19 19 19 19 19 19 19 19 19 20 20 19 19
ContextHubAbductiveL1 19 19 19 19 20 19 19 19 19 19 20 20 19 19
ContextHubAbductiveL2 19 19 19 19 20 19 19 19 19 19 20 20 19 19
MuSRMurderMysteries 8 8 27 8 8 8 8 8 8 8 6 4 8 8
MuSRTeamAllocations 27 22 19 19 27 23 26 22 8 8 30 20 8 8
MuSRObjectPlacements 8 8 27 8 8 8 8 8 8 8 7 6 8 8
MMLU 19 18 19 19 20 18 18 18 19 19 19 19 19 20
MMLUPro 20 19 38 19 21 19 19 20 19 19 20 20 19 19
GPQA 19 19 19 19 21 19 19 19 19 19 20 20 19 20
MATH 30 31 28 28 28 30 30 33 28 28 31 29 28 28
GSM8K 22 29 30 28 28 37 24 28 28 28 29 28 28 28
GSM8K-Hard 9 57 11 21 9 13 40 20 7 8 8 8 8 8
FOLIO 39 8 8 8 8 8 31 13 8 16 5 6 8 70
Big-BenchHard 39 22 25 21 26 32 29 26 28 19 28 28 10 16
Table 14: Average character index of where the answer span begins in a generated response for
eachdatasetandmodelpairforthezero-shotCoTsetting. Weusethesenumbersasaproxyforthe
model following instructions (i.e. generating reasoning before an answer). Prompt modifications
weremadetoensureCoTpromptsresultedinlongergenerationsanddirectanswerpromptsledto
shortgenerations.
Zero-shotCoTAnswerSpanLocationByCharacterIndex
dataset
Meta-Llama27b Mistral7b Meta-Llama3.18b Meta-Llama3.170b Gemma29b Phi-3Small8k Qwen27b Qwen272b GPT-4oMini
GPT-4o
Claude-3Haiku Claude-3.5Sonnet Gemini1.5Flash Gemini1.5Pro
CommonsenseQA 441 564 845 1237 236 466 577 341 899 1086 626 1103 214 165
StrategyQA 726 434 996 1131 267 460 363 358 692 1033 754 1158 256 195
SiQA 569 423 841 965 235 528 472 420 847 1094 602 1016 196 169
PiQA 699 455 869 914 207 532 447 364 683 935 578 1092 200 150
Winogrande 377 324 645 694 187 326 391 298 634 750 408 889 200 173
ArcEasy 684 581 1154 1319 367 610 534 355 990 1239 789 1222 340 231
ArcChallenge 763 644 1178 1316 422 596 571 387 1020 1269 828 1240 372 267
AGIEvalLSATLR 2053 1324 1163 1675 524 689 1560 768 949 998 1561 728 906 886
AGIEvalLSATAR 1377 1791 1422 2182 712 1027 1819 1264 1230 1151 1202 849 817 871
AGIEvalLSATRC 1977 1032 1103 1575 779 590 1170 660 973 1079 1628 786 703 709
ContextHubDeductiveL1 694 368 759 711 383 327 539 402 540 580 542 556 320 254
ContextHubDeductiveL2 842 472 1095 990 614 442 789 585 840 758 777 655 515 503
ContextHubAbductiveL1 577 461 747 879 464 440 754 638 788 879 683 594 368 325
ContextHubAbductiveL2 861 600 1270 1229 686 571 976 856 1115 1113 894 894 601 551
MuSRMurderMysteries 495 1592 1958 1847 1210 1246 1241 1718 1961 1965 1671 1759 1349 1213
MuSRTeamAllocations 1212 1845 2294 2310 1513 1433 2021 2213 2562 2698 1479 1856 1596 1607
MuSRObjectPlacements 917 625 1354 1266 695 641 904 819 1593 1536 1210 1455 616 429
MMLU 834 512 663 622 503 277 497 407 400 461 447 409 630 413
MMLUPro 1371 513 788 716 640 518 954 699 926 940 590 653 660 774
GPQA 1034 778 917 901 806 500 1018 628 541 666 486 472 981 735
MATH 742 1118 1222 1179 748 670 1189 1145 1125 1153 677 675 679 698
GSM8K 572 637 834 719 453 521 709 645 1048 1035 708 680 541 437
GSM8K-Hard 916 939 1027 1069 555 766 1083 1053 1350 1266 594 815 605 512
FOLIO 724 765 1479 1379 733 668 919 488 1285 1583 907 1194 934 492
Big-BenchHard 596 230 876 861 429 349 315 443 877 973 545 863 455 346
36Table15:Averagecharacterindexofwheretheanswerspanbeginsinageneratedresponseforeach
datasetandmodelpairforthefew-shotdirectanswersetting. Weusethesenumbersasaproxyfor
themodelfollowinginstructions(i.e.generatingreasoningbeforeananswer).Promptmodifications
weremadetoensureCoTpromptsresultedinlongergenerationsanddirectanswerpromptsledto
shortgenerations.
Few-shotDirectAnswerSpanLocationByCharacterIndex
dataset
Meta-Llama27b
Mistral7b
Meta-Llama3.18b Meta-Llama3.170b
Gemma29b Phi-3Small8k Qwen27b Qwen272b GPT-4oMini
Gemini1.5Flash
CommonsenseQA 87 8 27 8 8 8 10 8 8 8
AGIEvalLSATLR 25 24 24 24 24 24 24 24 31 24
AGIEvalLSATAR 25 24 24 24 24 24 24 24 27 24
AGIEvalLSATRC 25 24 24 24 24 24 24 24 25 24
ContextHubDeductiveL1 19 19 19 19 19 19 19 19 19 19
ContextHubDeductiveL2 19 19 19 19 19 19 19 19 19 19
ContextHubAbductiveL1 19 19 19 19 19 19 19 19 19 19
ContextHubAbductiveL2 19 19 19 19 19 19 19 19 19 19
MuSRMurderMysteries -1 8 27 8 8 8 8 8 8 8
MuSRTeamAllocations -1 21 19 19 27 21 27 23 8 8
MuSRObjectPlacements -1 8 27 8 8 8 8 8 8 8
MMLU 19 18 19 19 19 18 18 18 19 19
MMLUPro 19 19 38 19 20 20 19 19 19 19
GPQA -1 19 19 19 19 19 19 19 19 19
MATH 29 36 29 29 28 30 30 41 28 28
GSM8K 22 23 23 22 22 23 22 24 27 28
Table 16: Average character index of where the answer span begins in a generated response for
eachdatasetandmodelpairforthefew-shotCoTsetting. Weusethesenumbersasaproxyforthe
model following instructions (i.e. generating reasoning before an answer). Prompt modifications
weremadetoensureCoTpromptsresultedinlongergenerationsanddirectanswerpromptsledto
shortgenerations.
Few-shotCoTAnswerSpanLocationByCharacterIndex
dataset
Meta-Llama27b
Mistral7b
Meta-Llama3.18b Meta-Llama3.170b
Gemma29b Phi-3Small8k Qwen27b Qwen272b GPT-4oMini
Gemini1.5Flash
CommonsenseQA 301 195 470 921 145 192 280 174 219 158
AGIEvalLSATLR 1037 510 464 539 437 359 530 599 894 523
AGIEvalLSATAR 1024 1247 886 768 573 1025 750 835 1033 670
AGIEvalLSATRC 799 378 131 206 164 111 241 205 1086 266
ContextHubDeductiveL1 383 386 406 376 359 376 388 364 416 366
ContextHubDeductiveL2 736 767 829 822 823 855 612 807 884 809
ContextHubAbductiveL1 301 386 428 450 431 413 541 447 575 379
ContextHubAbductiveL2 709 586 967 754 804 784 829 821 905 815
MuSRMurderMysteries -1 1280 1693 1702 1225 1338 1246 1719 1974 1419
MuSRTeamAllocations -1 2195 2087 2160 1628 1755 2181 2156 2632 1841
MuSRObjectPlacements -1 907 1104 1213 706 919 676 963 1351 853
MMLU 282 266 333 245 265 260 267 243 392 218
MMLUPro 429 397 424 411 516 425 541 325 681 396
GPQA -1 848 782 774 615 711 662 703 670 594
MATH 630 705 584 640 747 529 1074 848 1261 553
GSM8K 374 332 352 352 398 372 415 341 651 314
37Table17: Categoriesandtheirdescriptionsforthemeta-analysis.
Category Description
Symbolicandalgorithmic Tasksinvolvingsymbolmanipulationwhichcanbesolvedbyexecutinga
program. Thisincludesentitytrackingdatasets(e.g., SCONE,CoinFlip)
andalgorithmictasks(e.g.,BBHwordsortingorfindingshortestpathsina
graph).
Math Tasks requiring mathematical reasoning, from grade-school math to ad-
vancedmathematics,includingphysicsquestions.
Logicalreasoning Tasksdesignedtotestforlogicalreasoning,whetherdeductive(e.g.,PrOn-
toQA),inductive(Bowenetal.,2024)oranalogical(Maetal.,2024)rea-
soning,includingsyllogismsandlogicalpuzzles.
Commonsensereasoning Datasetsdesignedtotestforcommonsenseknowledgeandreasoning,i.e.,
world knowledge that most people would have, rather than specialized
expert-levelknowledgeinadisciplineacquiredafteryearsofstudy.
Encyclopedicknowledge Tasks requiring expert-level in-depth knowledge beyond mere common-
sense,usuallyinanopen-booksetting.
Spatial and temporal rea- Datasetsdesignedtotestforanunderstandingofspaceandspatialrelations
soning (e.g.,navigation)orreasoninginvolvingtimeandsequencesovertime.
Multi-hopQA Questionsinvolvingthecompositionofmultiplestepsofreasoninginorder
to arrive at an answer, such as “What is the capital of the country whose
scientistdiscoveredpenicillin?”
Context-awareQA Taskssuchasclosed-bookQAandreadingcomprehensioninvolvingrea-
soningaboutagiventextincontext. Thecontextisoftenashortpassage,
butcouldalsotaketheformofaknowledgegraph(KBQA)oratable.This
categoryalsoincludesinformationextractiontasks,suchasNERorrelation
extraction.
Entailment Tasksinvolvingestablishingtheinferentialrelationbetweentwotexts,pro-
totypicallyNLI,butalsoincludingfactverification.
Textclassification Tasks involving the classification of a text into a small set of categories,
such a topic or sentiment classification, but also involving tasks such as
hatespeechdetectionandmisinformationdetection.
Generation Tasks involving text generation, including machine translation, dialogue,
questiongeneration, aswellas codegeneration. Tasks suchasSQLexe-
cution(Leietal.,2024)orsystematictransformationsofdata(e.g.,SCAN
(Lake&Baroni,2018))areexcludedbecausetheycanbesolvedbyexecut-
ingaprogram.
Meta-linguistic Tasksprobingformodels’knowledgeoflinguistics,suchasidentifyingthe
mainsubjectofasentenceorsolvinglinguisticpuzzles.
Mixeddatasets Datasetscontainingavarietyoftasks,suchasBIG-BenchHard(BBH)or
MMLU.
Other Taskswhichdidnotfitinanyoftheothercategories,suchasevaluatingAI
safety,elicitingmodels’verbalizedconfidence,ormelodyretrieval.
F FULL RESULTS OF EVALUATIONS ON FORMAL REASONING DATASETS
AsdiscussedinSection5,weincludedetailedevaluationresultsoffew-shotdirectanswer,few-shot
CoT,directanswersolver,CoTsolver,andtool-augmentedpromptinginTable20. Theunparseable
rate stands for the rate of unparseable model responses that either fail to pass our answer extrac-
tion parser (for all methods except tool-augmented prompting) or fail to be executed by symbolic
solvers. ForFOLIOandContextHub,wecomputetheaccuracybymakingarandomguessforthe
unparseable responses; for GSM8K and GSM8K-Hard, we consider the unparseable responses as
incorrect.
38Table18: TotalCoTdeltasonMMLUbrokendownbythetotalgainfromquestionsandresponses
withan“=”vs.withoutan“=”.
Model TotalCoTDelta CoTdeltaw/= CoTdeltaw/o= Perf.Gainw/= FractionofNw/=
Llama27b 6.0 0.6 5.4 9.8% 10.9%
Mistral7b 4.1 1.2 2.9 28.6% 9.8%
Llama3.18b 5.5 2.9 2.6 52.9% 9.6%
Llama3.170b 1.9 1.8 0.1 94.0% 10.6%
Gemma29b 2.6 2.0 0.6 78.5% 10.0%
Phi-3Small8k 3.1 1.5 1.7 47.4% 8.3%
Qwen27b 2.5 3.0 -0.5 100.0% 9.8%
Qwen272b 3.5 2.4 1.1 67.8% 9.6%
GPT-4oMini 5.2 3.5 1.7 66.9% 10.5%
GPT-4o 4.2 2.4 1.8 57.6% 10.3%
Claude-3Haiku 3.7 2.4 1.3 64.4% 9.3%
Claude-3.5Sonnet 3.2 2.3 0.9 72.1% 10.7%
Gemini1.5Flash 3.0 1.7 1.2 59.0% 10.1%
Gemini1.5Pro 1.9 1.0 0.9 51.9% 9.6%
Table 19: Total CoT deltas on MMLU Pro broken down by the total gain from questions and re-
sponseswithan“=”vs.withoutan“=”.
Model TotalCoTDelta CoTdeltaw/= CoTdeltaw/o= Perf.Gainw/= FractionofNw/=
Llama27b 1.6 1.3 0.3 79.6% 43.6%
Mistral7b 3.8 1.9 1.9 50.7% 41.8%
Llama3.18b 12.4 10.0 2.4 80.8% 35.2%
Llama3.170b 11.4 11.1 0.3 97.6% 39.6%
Gemma29b 7.6 7.4 0.2 97.9% 40.2%
Phi-3Small8k 11.6 9.9 1.7 85.7% 42.7%
Qwen27b 10.0 8.9 1.1 88.6% 41.6%
Qwen272b 19.0 16.1 2.9 84.7% 41.4%
GPT-4oMini 20.6 18.4 2.3 89.0% 44.0%
GPT-4o 17.7 17.1 0.6 96.7% 44.1%
Claude-3Haiku 8.7 7.8 0.9 90.1% 42.0%
Claude-3.5Sonnet 16.2 14.8 1.3 91.9% 43.4%
Gemini1.5Flash 12.9 11.8 1.1 91.3% 42.3%
Gemini1.5Pro 10.0 8.6 1.4 85.7% 41.8%
39Table20:Performanceandunparseableratesforfew-shotdirectanswer,few-shotCoT,Plan+Direct
Solver,Plan+CoTSolver,andPlan+ToolSolverSolver. “Acc.” standsforaccuracyand“%Unp.”
stands for the rate of unparseable model responses that either fail to pass our answer extraction
parser (for all methods except Plan + Tool Solver prompting) or fail to be executed by symbolic
solvers. ForFOLIOandContextHub,wecomputetheaccuracybymakingarandomguessforthe
unparseable responses; for GSM8K and GSM8K-Hard, we consider the unparseable responses as
incorrect.
Dataset Method Mistral7b Llama3.18b Llama3.170b GPT-4oMini
Acc. %Unp. Acc. %Unp. Acc. %Unp. Acc. %Unp.
GSM8K DirectAnswer 12.5 0.1 20.1 0.5 39.1 0.0 32.8 0.0
GSM8K CoT 56.2 1.4 86.4 1.0 96.1 0.1 94.2 0.1
GSM8K Plan+CoTSolver 45.0 1.0 78.7 0.4 94.7 0.0 92.0 0.1
GSM8K Plan+DirectSolver 10.6 0.1 19.6 0.1 42.2 0.0 39.3 0.0
GSM8K Plan+ToolSolver 59.8 8.6 80.3 1.3 94.4 0.4 90.5 1.5
GSM8K-Hard DirectAnswer 2.9 0.7 4.4 0.6 12.8 0.7 12.3 7.6
GSM8K-Hard CoT 20.3 5.0 32.4 9.6 47.8 4.4 52.2 0.5
GSM8K-Hard Plan+CoTSolver 18.7 2.6 32.4 1.3 49.7 0.6 51.5 0.3
GSM8K-Hard Plan+DirectSolver 3.0 0.5 5.5 0.8 15.8 0.1 17.4 0.3
GSM8K-Hard Plan+ToolSolver 44.2 8.9 57.9 1.2 68.0 0.5 70.4 1.4
ContextHubDeductiveL1 DirectAnswer 59.2 2.8 23.0 0.0 50.0 0.0 44.3 0.0
ContextHubDeductiveL1 CoT 46.2 0.2 73.0 0.2 67.5 0.0 59.2 0.0
ContextHubDeductiveL1 Plan+CoTSolver 49.5 0.0 64.8 0.0 65.5 0.0 63.2 0.0
ContextHubDeductiveL1 Plan+DirectSolver 45.8 3.0 55.8 0.0 53.5 0.0 56.2 0.0
ContextHubDeductiveL1 Plan+ToolSolver 68.8 27.8 84.2 11.8 91.7 9.8 90.7 7.8
ContextHubAbductiveL1 DirectAnswer 21.7 2.8 36.1 0.0 58.9 0.0 59.2 0.0
ContextHubAbductiveL1 CoT 23.9 0.0 40.0 0.0 62.2 0.0 76.9 0.0
ContextHubAbductiveL1 Plan+CoTSolver 38.3 0.0 42.5 0.0 65.6 0.0 74.2 0.0
ContextHubAbductiveL1 Plan+DirectSolver 46.9 3.9 33.3 0.3 63.1 0.0 61.7 0.0
ContextHubAbductiveL1 Plan+ToolSolver 59.2 35.8 70.8 9.7 73.9 4.2 74.7 10.3
FOLIO DirectAnswer 56.2 12.3 59.6 0.0 69.5 0.0 64.0 0.0
FOLIO CoT 53.7 1.5 56.7 2.5 72.4 2.0 70.4 0.0
FOLIO Plan+CoTSolver 53.7 0.0 55.7 0.0 73.9 0.5 70.4 0.0
FOLIO Plan+DirectSolver 52.7 0.0 54.2 0.0 72.9 0.0 63.5 0.0
FOLIO Plan+ToolSolver 48.8 46.8 54.2 28.6 70.0 16.7 62.6 25.1
Wenotethatallmodelshavealowunparseablerate(<10%)forallmethodsexcepttool-augmented
prompting. Bymanuallyinspectingtheoutputs,weobservethatthehighunparseablerateforsome
models with tool-augmented prompting is caused by these models generating Python programs or
formalspecificationsthatfailtofollowtheformatoftheformallanguage(Pythonorz3)andthatlead
toexecutionerrors. Suchanissueisparticularlysevereforthesmallermodels. However,wenote
thatdespitethehighunparseablerate,theoverallaccuracyofthesemodelswithtoolaugmentation
isstillonparwithoroutperformsothermethods.
G OTHER COT PROMPT VARIANTS
G.1 TESTINGPERFORMANCEVOLATILITYACROSSPROMPTS
To test the impact of prompt choice on performance, we performed our zero-shot experiment on
Llama3.18Bwith7differentdatasetsand4differentzero-shotCoTpromptingstrategiescommon
in the literature (Kojima et al., 2022; Wang et al., 2023b; Zhou et al., 2023b; Yang et al., 2024).
Figure 9 shows variation due to prompts is typically small and no prompt gives a consistent gain
over the other. For our experiments, this suggests that different prompts have small effects on the
overalloutcomeonaverage.
40Figure9: PerformanceofmultiplepromptscommonlyusedtoelicitreasoningthroughCoTinthe
zeroshotsetting. Eachpromptstartstheassistantcompletionwithadifferentphrasemeanttoelicit
reasoning. AllresultsarefromusingLlama3.18BInstruct. FortheKojimavariant,weexplicitly
place“Let’sthinkstepbystep.” intheassistantmessage. Thereisverylittlevariationbetweenthe
CoTpromptsonaverage.
41H EXAMPLE PROMPTS
AllpromptscanbefoundonHuggingfaceathttps://huggingface.co/collections/TAUR-Lab/
cot-analysis-project-66bbb9e5e0156e65059895f5. Welistafewpromptresponsepairshere
inthissection.
AGIEvalLSATARzero-shotCoTpromptforLlama3.170B
<|start_header_id|>user<|end_header_id|>
Explain your reasoning step-by-step for each question before answering.
Give your final answer in the format \"The answer is therefore <A, B, C,
D, E>\". Failure to comply with the answer formatting will result in no
credit.
Of the eight students\u2014George, Helen, Irving, Kyle, Lenore, Nina,
Olivia, and Robert\u2014in a seminar, exactly six will give individual
oral reports during three consecutive days\u2014Monday, Tuesday, and
Wednesday. Exactly two reports will be given each day\u2014one in the
morning and one in the afternoon\u2014according to the following
conditions: Tuesday is the only day on which George can give a report.
Neither Olivia nor Robert can give an afternoon report. If Nina gives a
report, then on the next day Helen and Irving must both give reports,
unless Nina's report is given on Wednesday.Q: Which one of the following
could be the schedule of the students' reports? Answer Choices: (A)Mon.
morning: Helen; Mon. afternoon: Robert Tues. morning: Olivia; Tues.
afternoon: Irving Wed. morning: Lenore; Wed. afternoon: Kyle (B)Mon.
morning: Irving; Mon. afternoon: Olivia Tues. morning: Helen; Tues.
afternoon: Kyle Wed. morning: Nina; Wed. afternoon: Lenore (C)Mon.
morning: Lenore; Mon. afternoon: Helen Tues. morning: George; Tues.
afternoon: Kyle Wed. morning: Robert; Wed. afternoon: Irving (D)Mon.
morning: Nina; Mon. afternoon: Helen Tues. morning: Robert; Tues.
afternoon: Irving Wed. morning: Olivia; Wed. afternoon: Lenore (E)Mon.
morning: Olivia; Mon. afternoon: Nina Tues. morning: Irving; Tues.
afternoon: Helen Wed. morning: Kyle; Wed. afternoon: George
Let's think step by
step.<|eot_id|><|start_header_id|>assistant<|end_header_id|>
42AGIEvalLSATARzero-shotCoTresponseforLlama3.170B
To determine which one of the following could be the schedule of the
students' reports, let's analyze each answer choice in light of the given
conditions. First, let's recall the conditions: 1. Exactly six students
will give reports over three days. 2. Tuesday is the only day George can
give a report. 3. Neither Olivia nor Robert can give an afternoon report.
4. If Nina gives a report, then on the next day Helen and Irving must both
give reports, unless Nina's report is given on Wednesday. Let's evaluate
each answer choice: (A) Mon. morning: Helen; Mon. afternoon: Robert Tues.
morning: Olivia; Tues. afternoon: Irving Wed. morning: Lenore; Wed.
afternoon: Kyle - This choice does not violate any conditions directly.
However, we need to check if it aligns with all the rules, especially
considering the Nina rule, which isn't directly applicable here since Nina
isn't scheduled. George's requirement is also not met since George isn't
scheduled on Tuesday. (B) Mon. morning: Irving; Mon. afternoon: Olivia
Tues. morning: Helen; Tues. afternoon: Kyle Wed. morning: Nina; Wed.
afternoon: Lenore - This choice violates the condition that if Nina gives
a report, then on the next day Helen and Irving must both give reports,
unless Nina's report is given on Wednesday. Since Nina's report is on
Wednesday, this condition is technically not violated, but we need to
ensure all other conditions are met. George is not scheduled on Tuesday,
which is a requirement. (C) Mon. morning: Lenore; Mon. afternoon: Helen
Tues. morning: George; Tues. afternoon: Kyle Wed. morning: Robert; Wed.
afternoon: Irving - This choice meets the condition that George gives a
report on Tuesday. It also does not violate any other conditions directly.
However, we need to ensure that the Nina rule isn't applicable or is met,
which in this case, it's not applicable since Nina isn't scheduled. (D)
Mon. morning: Nina; Mon. afternoon: Helen Tues. morning: Robert; Tues.
afternoon: Irving Wed. morning: Olivia; Wed. afternoon: Lenore - This
choice violates the condition that if Nina gives a report, then on the
next day Helen and Irving must both give reports. Here, Helen is already
scheduled on the same day as Nina, and while Irving is scheduled the next
day, the condition specifies both Helen and Irving must give reports the
next day, which is not the case since Helen's report is on the same day as
Nina's. (E) Mon. morning: Olivia; Mon. afternoon: Nina Tues. morning:
Irving; Tues. afternoon: Helen Wed. morning: Kyle; Wed. afternoon: George
- This choice violates the condition that Tuesday is the only day George
can give a report, as George is scheduled on Wednesday. Given the
analysis, the only choice that does not directly violate any of the given
conditions and meets the specific requirements (especially considering
the placement of George and the handling of the Nina rule where
applicable) is: The answer is therefore (C).
43MATHzero-shotCoTpromptforLlama3.170B
<|begin_of_text|><|start_header_id|>user<|end_header_id|>
Solve the following math problem efficiently and clearly:
- For simple problems (2 steps or fewer):\nProvide a concise solution
with minimal explanation.
- For complex problems (3 steps or more):
Use this step-by-step format:\n\n## Step 1: [Concise description]\n[Brief
explanation and calculations]
## Step 2: [Concise description]
[Brief explanation and calculations]
...
Regardless of the approach, always conclude with:
Therefore, the final answer is: $\\boxed{answer}$. I hope it is correct.
Where [answer] is just the final number or expression that solves the
problem.
Problem: Chris labels every lattice point in the coordinate plane with
the square of the distance from the point to the origin (a lattice point
is a point such that both of its coordinates are integers). How many
times does he label a point with the number
$25$?<|eot_id|><|start_header_id|>assistant<|end_header_id|>
MATHzero-shotCoTresponseforLlama3.170B
## Step 1: Understand the problem Chris labels each lattice point in the
coordinate plane with the square of the distance from the point to the
origin. We need to find how many times he labels a point with the number
25. ## Step 2: Determine the equation for the distance The distance from
a point (x, y) to the origin (0, 0) is given by the equation $\sqrt{xˆ2 +
yˆ2}$. Since Chris labels the points with the square of the distance, we
are looking for points where $xˆ2 + yˆ2 = 25$. ## Step 3: Find the integer
solutions to the equation We need to find all integer values of x and y
that satisfy the equation $xˆ2 + yˆ2 = 25$. The possible values of $xˆ2$
and $yˆ2$ that add up to 25 are: (0, 25), (25, 0), (1, 24), (24, 1), (4,
21), (21, 4), (9, 16), (16, 9). ## Step 4: Count the number of solutions
However, we need to consider that some of these pairs have negative
counterparts, and we should also count those. For example, (-0, 5) and
(0, -5) are also solutions, as well as (-5, 0) and (5, 0). Similarly, (-3,
4), (3, -4), (-4, 3), and (4, -3) are also solutions. So, we have a total
of 12 solutions. The final answer is: $\boxed{12}$.
44CommonsenseQAzero-shotdirectanswerpromptforLlama3.170B
<|start_header_id|>system<|end_header_id|>
You are a helpful AI assistant that will answer reasoning questions. You
will always say at the end \"Answer: <Your Answer Letter Choice>\". You
must only pick one answer and you must end your response with \"Answer:
<Your Answer Letter Choice>\"
everytime!<|eot_id|><|start_header_id|>user<|end_header_id|>
Question: Where would you find magazines along side many other printed
works?
Answer Choices:
( A ) doctor
( B ) bookstore
( C ) market
( D ) train station
( E ) mortuary
Only write the answer. Write the answer in the following format: \"Answer:
<your answer>\". You must always give an answer. You may only pick one
answer choice, if you think multiple are correct only pick the one you
think is best.<|eot_id|><|start_header_id|>assistant<|end_header_id|>
CommonsenseQAzero-shotdirectanswerresponseforLlama3.170B
B
45