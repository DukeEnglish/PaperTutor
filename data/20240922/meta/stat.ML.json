[
    {
        "title": "Fitting Multilevel Factor Models",
        "authors": "Tetiana ParshakovaTrevor HastieStephen Boyd",
        "links": "http://arxiv.org/abs/2409.12067v1",
        "entry_id": "http://arxiv.org/abs/2409.12067v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12067v1",
        "summary": "We examine a special case of the multilevel factor model, with covariance\ngiven by multilevel low rank (MLR) matrix~\\cite{parshakova2023factor}. We\ndevelop a novel, fast implementation of the expectation-maximization (EM)\nalgorithm, tailored for multilevel factor models, to maximize the likelihood of\nthe observed data. This method accommodates any hierarchical structure and\nmaintains linear time and storage complexities per iteration. This is achieved\nthrough a new efficient technique for computing the inverse of the positive\ndefinite MLR matrix. We show that the inverse of an invertible PSD MLR matrix\nis also an MLR matrix with the same sparsity in factors, and we use the\nrecursive Sherman-Morrison-Woodbury matrix identity to obtain the factors of\nthe inverse. Additionally, we present an algorithm that computes the Cholesky\nfactorization of an expanded matrix with linear time and space complexities,\nyielding the covariance matrix as its Schur complement. This paper is\naccompanied by an open-source package that implements the proposed methods.",
        "updated": "2024-09-18 15:39:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12067v1"
    },
    {
        "title": "Cartan moving frames and the data manifolds",
        "authors": "Eliot TronRita FioresiNicolas CouellanStéphane Puechmorel",
        "links": "http://arxiv.org/abs/2409.12057v1",
        "entry_id": "http://arxiv.org/abs/2409.12057v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12057v1",
        "summary": "The purpose of this paper is to employ the language of Cartan moving frames\nto study the geometry of the data manifolds and its Riemannian structure, via\nthe data information metric and its curvature at data points. Using this\nframework and through experiments, explanations on the response of a neural\nnetwork are given by pointing out the output classes that are easily reachable\nfrom a given input. This emphasizes how the proposed mathematical relationship\nbetween the output of the network and the geometry of its inputs can be\nexploited as an explainable artificial intelligence tool.",
        "updated": "2024-09-18 15:31:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12057v1"
    },
    {
        "title": "Symmetry-Based Structured Matrices for Efficient Approximately Equivariant Networks",
        "authors": "Ashwin SamudreMircea PetracheBrian D. NordShubhendu Trivedi",
        "links": "http://arxiv.org/abs/2409.11772v1",
        "entry_id": "http://arxiv.org/abs/2409.11772v1",
        "pdf_url": "http://arxiv.org/pdf/2409.11772v1",
        "summary": "There has been much recent interest in designing symmetry-aware neural\nnetworks (NNs) exhibiting relaxed equivariance. Such NNs aim to interpolate\nbetween being exactly equivariant and being fully flexible, affording\nconsistent performance benefits. In a separate line of work, certain structured\nparameter matrices -- those with displacement structure, characterized by low\ndisplacement rank (LDR) -- have been used to design small-footprint NNs.\nDisplacement structure enables fast function and gradient evaluation, but\npermits accurate approximations via compression primarily to classical\nconvolutional neural networks (CNNs). In this work, we propose a general\nframework -- based on a novel construction of symmetry-based structured\nmatrices -- to build approximately equivariant NNs with significantly reduced\nparameter counts. Our framework integrates the two aforementioned lines of work\nvia the use of so-called Group Matrices (GMs), a forgotten precursor to the\nmodern notion of regular representations of finite groups. GMs allow the design\nof structured matrices -- resembling LDR matrices -- which generalize the\nlinear operations of a classical CNN from cyclic groups to general finite\ngroups and their homogeneous spaces. We show that GMs can be employed to extend\nall the elementary operations of CNNs to general discrete groups. Further, the\ntheory of structured matrices based on GMs provides a generalization of LDR\ntheory focussed on matrices with cyclic structure, providing a tool for\nimplementing approximate equivariance for discrete groups. We test GM-based\narchitectures on a variety of tasks in the presence of relaxed symmetry. We\nreport that our framework consistently performs competitively compared to\napproximately equivariant NNs, and other structured matrix-based compression\nframeworks, sometimes with a one or two orders of magnitude lower parameter\ncount.",
        "updated": "2024-09-18 07:52:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.11772v1"
    },
    {
        "title": "Recurrent Interpolants for Probabilistic Time Series Prediction",
        "authors": "Yu ChenMarin BilošSarthak MittalWei DengKashif RasulAnderson Schneider",
        "links": "http://arxiv.org/abs/2409.11684v1",
        "entry_id": "http://arxiv.org/abs/2409.11684v1",
        "pdf_url": "http://arxiv.org/pdf/2409.11684v1",
        "summary": "Sequential models such as recurrent neural networks or transformer-based\nmodels became \\textit{de facto} tools for multivariate time series forecasting\nin a probabilistic fashion, with applications to a wide range of datasets, such\nas finance, biology, medicine, etc. Despite their adeptness in capturing\ndependencies, assessing prediction uncertainty, and efficiency in training,\nchallenges emerge in modeling high-dimensional complex distributions and\ncross-feature dependencies. To tackle these issues, recent works delve into\ngenerative modeling by employing diffusion or flow-based models. Notably, the\nintegration of stochastic differential equations or probability flow\nsuccessfully extends these methods to probabilistic time series imputation and\nforecasting. However, scalability issues necessitate a computational-friendly\nframework for large-scale generative model-based predictions. This work\nproposes a novel approach by blending the computational efficiency of recurrent\nneural networks with the high-quality probabilistic modeling of the diffusion\nmodel, which addresses challenges and advances generative models' application\nin time series forecasting. Our method relies on the foundation of stochastic\ninterpolants and the extension to a broader conditional generation framework\nwith additional control features, offering insights for future developments in\nthis dynamic field.",
        "updated": "2024-09-18 03:52:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.11684v1"
    },
    {
        "title": "PieClam: A Universal Graph Autoencoder Based on Overlapping Inclusive and Exclusive Communities",
        "authors": "Daniel ZilbergRon Levie",
        "links": "http://arxiv.org/abs/2409.11618v1",
        "entry_id": "http://arxiv.org/abs/2409.11618v1",
        "pdf_url": "http://arxiv.org/pdf/2409.11618v1",
        "summary": "We propose PieClam (Prior Inclusive Exclusive Cluster Affiliation Model): a\nprobabilistic graph model for representing any graph as overlapping generalized\ncommunities. Our method can be interpreted as a graph autoencoder: nodes are\nembedded into a code space by an algorithm that maximizes the log-likelihood of\nthe decoded graph, given the input graph. PieClam is a community affiliation\nmodel that extends well-known methods like BigClam in two main manners. First,\ninstead of the decoder being defined via pairwise interactions between the\nnodes in the code space, we also incorporate a learned prior on the\ndistribution of nodes in the code space, turning our method into a graph\ngenerative model. Secondly, we generalize the notion of communities by allowing\nnot only sets of nodes with strong connectivity, which we call inclusive\ncommunities, but also sets of nodes with strong disconnection, which we call\nexclusive communities. To model both types of communities, we propose a new\ntype of decoder based the Lorentz inner product, which we prove to be much more\nexpressive than standard decoders based on standard inner products or norm\ndistances. By introducing a new graph similarity measure, that we call the log\ncut distance, we show that PieClam is a universal autoencoder, able to\nuniformly approximately reconstruct any graph. Our method is shown to obtain\ncompetitive performance in graph anomaly detection benchmarks.",
        "updated": "2024-09-18 00:49:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.11618v1"
    }
]