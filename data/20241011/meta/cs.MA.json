[
    {
        "title": "I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy",
        "authors": "Gian Maria CampedelliNicolò PenzoMassimo StefanRoberto DessìMarco GueriniBruno LepriJacopo Staiano",
        "links": "http://arxiv.org/abs/2410.07109v1",
        "entry_id": "http://arxiv.org/abs/2410.07109v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07109v1",
        "summary": "As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.",
        "updated": "2024-10-09 17:45:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07109v1"
    },
    {
        "title": "MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders",
        "authors": "Cheng LiMay FungQingyun WangChi HanManling LiJindong WangHeng Ji",
        "links": "http://arxiv.org/abs/2410.06845v1",
        "entry_id": "http://arxiv.org/abs/2410.06845v1",
        "pdf_url": "http://arxiv.org/pdf/2410.06845v1",
        "summary": "Mental health disorders are one of the most serious diseases in the world.\nMost people with such a disease lack access to adequate care, which highlights\nthe importance of training models for the diagnosis and treatment of mental\nhealth disorders. However, in the mental health domain, privacy concerns limit\nthe accessibility of personalized treatment data, making it challenging to\nbuild powerful models. In this paper, we introduce MentalArena, a self-play\nframework to train language models by generating domain-specific personalized\ndata, where we obtain a better model capable of making a personalized diagnosis\nand treatment (as a therapist) and providing information (as a patient). To\naccurately model human-like mental health patients, we devise Symptom Encoder,\nwhich simulates a real patient from both cognition and behavior perspectives.\nTo address intent bias during patient-therapist interactions, we propose\nSymptom Decoder to compare diagnosed symptoms with encoded symptoms, and\ndynamically manage the dialogue between patient and therapist according to the\nidentified deviations. We evaluated MentalArena against 6 benchmarks, including\nbiomedicalQA and mental health tasks, compared to 6 advanced models. Our\nmodels, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform\ntheir counterparts, including GPT-4o. We hope that our work can inspire future\nresearch on personalized care. Code is available in\nhttps://github.com/Scarelette/MentalArena/tree/main",
        "updated": "2024-10-09 13:06:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.06845v1"
    },
    {
        "title": "Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning",
        "authors": "Hao MaTianyi HuZhiqiang PuBoyin LiuXiaolin AiYanyan LiangMin Chen",
        "links": "http://arxiv.org/abs/2410.06101v1",
        "entry_id": "http://arxiv.org/abs/2410.06101v1",
        "pdf_url": "http://arxiv.org/pdf/2410.06101v1",
        "summary": "Reinforcement learning (RL) has emerged as a pivotal technique for\nfine-tuning large language models (LLMs) on specific tasks. However, prevailing\nRL fine-tuning methods predominantly rely on PPO and its variants. Though these\nalgorithms are effective in general RL settings, they often exhibit suboptimal\nperformance and vulnerability to distribution collapse when applied to the\nfine-tuning of LLMs. In this paper, we propose CORY, extending the RL\nfine-tuning of LLMs to a sequential cooperative multi-agent reinforcement\nlearning framework, to leverage the inherent coevolution and emergent\ncapabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is\ninitially duplicated into two autonomous agents: a pioneer and an observer. The\npioneer generates responses based on queries, while the observer generates\nresponses using both the queries and the pioneer's responses. The two agents\nare trained together. During training, the agents exchange roles periodically,\nfostering cooperation and coevolution between them. Experiments evaluate CORY's\nperformance by fine-tuning GPT-2 and Llama-2 under subjective and objective\nreward functions on the IMDB Review and GSM8K datasets, respectively. Results\nshow that CORY outperforms PPO in terms of policy optimality, resistance to\ndistribution collapse, and training robustness, thereby underscoring its\npotential as a superior methodology for refining LLMs in real-world\napplications.",
        "updated": "2024-10-08 14:55:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.06101v1"
    },
    {
        "title": "Concurrent-Learning Based Relative Localization in Shape Formation of Robot Swarms",
        "authors": "Jinhu LüKunrui ZeShuoyu YueKexin LiuWei WangGuibin Sun",
        "links": "http://arxiv.org/abs/2410.06052v1",
        "entry_id": "http://arxiv.org/abs/2410.06052v1",
        "pdf_url": "http://arxiv.org/pdf/2410.06052v1",
        "summary": "In this paper, we address the shape formation problem for massive robot\nswarms in environments where external localization systems are unavailable.\nAchieving this task effectively with solely onboard measurements is still\nscarcely explored and faces some practical challenges. To solve this\nchallenging problem, we propose the following novel results. Firstly, to\nestimate the relative positions among neighboring robots, a concurrent-learning\nbased estimator is proposed. It relaxes the persistent excitation condition\nrequired in the classical ones such as least-square estimator. Secondly, we\nintroduce a finite-time agreement protocol to determine the shape location.\nThis is achieved by estimating the relative position between each robot and a\nrandomly assigned seed robot. The initial position of the seed one marks the\nshape location. Thirdly, based on the theoretical results of the relative\nlocalization, a novel behavior-based control strategy is devised. This strategy\nnot only enables adaptive shape formation of large group of robots but also\nenhances the observability of inter-robot relative localization. Numerical\nsimulation results are provided to verify the performance of our proposed\nstrategy compared to the state-of-the-art ones. Additionally, outdoor\nexperiments on real robots further demonstrate the practical effectiveness and\nrobustness of our methods.",
        "updated": "2024-10-08 13:54:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.06052v1"
    },
    {
        "title": "Online Dynamic Pricing for Electric Vehicle Charging Stations with Reservations",
        "authors": "Jan MrkosAntonín KomendaDavid FiedlerJiří Vokřínek",
        "links": "http://arxiv.org/abs/2410.05538v1",
        "entry_id": "http://arxiv.org/abs/2410.05538v1",
        "pdf_url": "http://arxiv.org/pdf/2410.05538v1",
        "summary": "The transition to electric vehicles (EVs), coupled with the rise of renewable\nenergy sources, will significantly impact the electric grid. Unlike\nconventional fuel sources, electricity for EVs is constrained by grid capacity,\nprice fluctuations, and long EV charging times, requiring new pricing solutions\nto manage demand and supply. This paper proposes a model for online dynamic\npricing of reserved EV charging services, including reservation, parking, and\ncharging as a bundled service priced as a whole. Our approach focuses on the\nindividual charging station operator, employing a stochastic demand model and\nonline dynamic pricing based on expected demand. The proposed model uses a\nMarkov Decision Process (MDP) formulation to optimize sequential pricing\ndecisions for charging session requests. A key contribution is the novel\ndefinition and quantification of discretization error introduced by the\ndiscretization of the Poisson process for use in the MDP. The model's viability\nis demonstrated with a heuristic solution method based on Monte-Carlo tree\nsearch, offering a viable path for real-world application.",
        "updated": "2024-10-07 22:36:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.05538v1"
    }
]