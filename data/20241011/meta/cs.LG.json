[
    {
        "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
        "authors": "Hanrong YeHaotian ZhangErik DaxbergerLin ChenZongyu LinYanghao LiBowen ZhangHaoxuan YouDan XuZhe GanJiasen LuYinfei Yang",
        "links": "http://arxiv.org/abs/2410.07177v1",
        "entry_id": "http://arxiv.org/abs/2410.07177v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07177v1",
        "summary": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.",
        "updated": "2024-10-09 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07177v1"
    },
    {
        "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models",
        "authors": "Fei WangXingchen WanRuoxi SunJiefeng ChenSercan Ö. Arık",
        "links": "http://arxiv.org/abs/2410.07176v1",
        "entry_id": "http://arxiv.org/abs/2410.07176v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07176v1",
        "summary": "Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.",
        "updated": "2024-10-09 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07176v1"
    },
    {
        "title": "Neural Circuit Architectural Priors for Quadruped Locomotion",
        "authors": "Nikhil X. BhattasaliVenkatesh PattabiramanLerrel PintoGrace W. Lindsay",
        "links": "http://arxiv.org/abs/2410.07174v1",
        "entry_id": "http://arxiv.org/abs/2410.07174v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07174v1",
        "summary": "Learning-based approaches to quadruped locomotion commonly adopt generic\npolicy architectures like fully connected MLPs. As such architectures contain\nfew inductive biases, it is common in practice to incorporate priors in the\nform of rewards, training curricula, imitation data, or trajectory generators.\nIn nature, animals are born with priors in the form of their nervous system's\narchitecture, which has been shaped by evolution to confer innate ability and\nefficient learning. For instance, a horse can walk within hours of birth and\ncan quickly improve with practice. Such architectural priors can also be useful\nin ANN architectures for AI. In this work, we explore the advantages of a\nbiologically inspired ANN architecture for quadruped locomotion based on neural\ncircuits in the limbs and spinal cord of mammals. Our architecture achieves\ngood initial performance and comparable final performance to MLPs, while using\nless data and orders of magnitude fewer parameters. Our architecture also\nexhibits better generalization to task variations, even admitting deployment on\na physical robot without standard sim-to-real methods. This work shows that\nneural circuits can provide valuable architectural priors for locomotion and\nencourages future work in other sensorimotor skills.",
        "updated": "2024-10-09 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07174v1"
    },
    {
        "title": "Glider: Global and Local Instruction-Driven Expert Router",
        "authors": "Pingzhi LiPrateek YadavJaehong YoonJie PengYi-Lin SungMohit BansalTianlong Chen",
        "links": "http://arxiv.org/abs/2410.07172v1",
        "entry_id": "http://arxiv.org/abs/2410.07172v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07172v1",
        "summary": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to particular domains. This\nhas enabled the creation of powerful and adaptive routing-based \"Model\nMoErging\" methods with the goal of using expert modules to create an aggregate\nsystem with improved performance or generalization. However, existing MoErging\nmethods often prioritize generalization to unseen tasks at the expense of\nperformance on held-in tasks, which limits its practical applicability in\nreal-world deployment scenarios. We observe that current token-level routing\nmechanisms neglect the global semantic context of the input task. This\ntoken-wise independence hinders effective expert selection for held-in tasks,\nas routing decisions fail to incorporate the semantic properties of the task.\nTo address this, we propose, Global and Local Instruction Driven Expert Router\n(GLIDER) that integrates a multi-scale routing mechanism, encompassing a\nsemantic global router and a learned local router. The global router leverages\nLLM's advanced reasoning capabilities for semantic-related contexts to enhance\nexpert selection. Given the input query and LLM, the router generates semantic\ntask instructions that guide the retrieval of the most relevant experts across\nall layers. This global guidance is complemented by a local router that\nfacilitates token-level routing decisions within each module, enabling finer\ncontrol and enhanced performance on unseen tasks. Our experiments using\nT5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves\nsubstantially improved held-in performance while maintaining strong\ngeneralization on held-out tasks. We also perform ablations experiments to dive\ndeeper into the components of GLIDER. Our experiments highlight the importance\nof our multi-scale routing that leverages LLM-driven semantic reasoning for\nMoErging methods.",
        "updated": "2024-10-09 17:59:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07172v1"
    },
    {
        "title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
        "authors": "Fabian PaischerLukas HauzenbergerThomas SchmiedBenedikt AlkinMarc Peter DeisenrothSepp Hochreiter",
        "links": "http://arxiv.org/abs/2410.07170v1",
        "entry_id": "http://arxiv.org/abs/2410.07170v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07170v1",
        "summary": "Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across model\nweights. Recent works focus on weight-driven initialization or learning of\nadaptive ranks during training. Both approaches have only been investigated in\nisolation, resulting in slow convergence or a uniform rank distribution, in\nturn leading to sub-optimal performance. We propose to enhance LoRA by\ninitializing the new weights in a data-driven manner by computing singular\nvalue decomposition on minibatches of activation vectors. Then, we initialize\nthe LoRA matrices with the obtained right-singular vectors and re-distribute\nranks among all weight matrices to explain the maximal amount of variance and\ncontinue the standard LoRA fine-tuning procedure. This results in our new\nmethod Explained Variance Adaptation (EVA). We apply EVA to a variety of\nfine-tuning tasks ranging from language generation and understanding to image\nclassification and reinforcement learning. EVA exhibits faster convergence than\ncompetitors and attains the highest average score across a multitude of tasks\nper domain.",
        "updated": "2024-10-09 17:59:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07170v1"
    }
]