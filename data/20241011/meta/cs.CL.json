[
    {
        "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models",
        "authors": "Fei WangXingchen WanRuoxi SunJiefeng ChenSercan Ö. Arık",
        "links": "http://arxiv.org/abs/2410.07176v1",
        "entry_id": "http://arxiv.org/abs/2410.07176v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07176v1",
        "summary": "Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.",
        "updated": "2024-10-09 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07176v1"
    },
    {
        "title": "Do better language models have crisper vision?",
        "authors": "Jona RuthardtGertjan J. BurghoutsSerge BelongieYuki M. Asano",
        "links": "http://arxiv.org/abs/2410.07173v1",
        "entry_id": "http://arxiv.org/abs/2410.07173v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07173v1",
        "summary": "How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.",
        "updated": "2024-10-09 17:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07173v1"
    },
    {
        "title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
        "authors": "Fabian PaischerLukas HauzenbergerThomas SchmiedBenedikt AlkinMarc Peter DeisenrothSepp Hochreiter",
        "links": "http://arxiv.org/abs/2410.07170v1",
        "entry_id": "http://arxiv.org/abs/2410.07170v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07170v1",
        "summary": "Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across model\nweights. Recent works focus on weight-driven initialization or learning of\nadaptive ranks during training. Both approaches have only been investigated in\nisolation, resulting in slow convergence or a uniform rank distribution, in\nturn leading to sub-optimal performance. We propose to enhance LoRA by\ninitializing the new weights in a data-driven manner by computing singular\nvalue decomposition on minibatches of activation vectors. Then, we initialize\nthe LoRA matrices with the obtained right-singular vectors and re-distribute\nranks among all weight matrices to explain the maximal amount of variance and\ncontinue the standard LoRA fine-tuning procedure. This results in our new\nmethod Explained Variance Adaptation (EVA). We apply EVA to a variety of\nfine-tuning tasks ranging from language generation and understanding to image\nclassification and reinforcement learning. EVA exhibits faster convergence than\ncompetitors and attains the highest average score across a multitude of tasks\nper domain.",
        "updated": "2024-10-09 17:59:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07170v1"
    },
    {
        "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate",
        "authors": "Qidong HuangXiaoyi DongPan ZhangYuhang ZangYuhang CaoJiaqi WangDahua LinWeiming ZhangNenghai Yu",
        "links": "http://arxiv.org/abs/2410.07167v1",
        "entry_id": "http://arxiv.org/abs/2410.07167v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07167v1",
        "summary": "We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.",
        "updated": "2024-10-09 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07167v1"
    },
    {
        "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
        "authors": "Cheol Jun ChoNicholas LeeAkshat GuptaDhruv AgarwalEthan ChenAlan W BlackGopala K. Anumanchipalli",
        "links": "http://arxiv.org/abs/2410.07168v1",
        "entry_id": "http://arxiv.org/abs/2410.07168v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07168v1",
        "summary": "Syllables are compositional units of spoken language that play a crucial role\nin human speech perception and production. However, current neural speech\nrepresentations lack structure, resulting in dense token sequences that are\ncostly to process. To bridge this gap, we propose a new model, Sylber, that\nproduces speech representations with clean and robust syllabic structure.\nSpecifically, we propose a self-supervised model that regresses features on\nsyllabic segments distilled from a teacher model which is an exponential moving\naverage of the model in training. This results in a highly structured\nrepresentation of speech features, offering three key benefits: 1) a fast,\nlinear-time syllable segmentation algorithm, 2) efficient syllabic tokenization\nwith an average of 4.27 tokens per second, and 3) syllabic units better suited\nfor lexical and syntactic understanding. We also train token-to-speech\ngenerative models with our syllabic units and show that fully intelligible\nspeech can be reconstructed from these tokens. Lastly, we observe that\ncategorical perception, a linguistic phenomenon of speech perception, emerges\nnaturally in our model, making the embedding space more categorical and sparse\nthan previous self-supervised learning approaches. Together, we present a novel\nself-supervised approach for representing speech as syllables, with significant\npotential for efficient speech tokenization and spoken language modeling.",
        "updated": "2024-10-09 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07168v1"
    }
]