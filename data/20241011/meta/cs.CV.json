[
    {
        "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
        "authors": "Hanrong YeHaotian ZhangErik DaxbergerLin ChenZongyu LinYanghao LiBowen ZhangHaoxuan YouDan XuZhe GanJiasen LuYinfei Yang",
        "links": "http://arxiv.org/abs/2410.07177v1",
        "entry_id": "http://arxiv.org/abs/2410.07177v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07177v1",
        "summary": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.",
        "updated": "2024-10-09 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07177v1"
    },
    {
        "title": "Do better language models have crisper vision?",
        "authors": "Jona RuthardtGertjan J. BurghoutsSerge BelongieYuki M. Asano",
        "links": "http://arxiv.org/abs/2410.07173v1",
        "entry_id": "http://arxiv.org/abs/2410.07173v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07173v1",
        "summary": "How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.",
        "updated": "2024-10-09 17:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07173v1"
    },
    {
        "title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation",
        "authors": "Xinchen ZhangLing YangGuohao LiYaqi CaiJiake XieYong TangYujiu YangMengdi WangBin Cui",
        "links": "http://arxiv.org/abs/2410.07171v1",
        "entry_id": "http://arxiv.org/abs/2410.07171v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07171v1",
        "summary": "Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made\nnotable strides in compositional text-to-image generation. However, these\nmethods typically exhibit distinct strengths for compositional generation, with\nsome excelling in handling attribute binding and others in spatial\nrelationships. This disparity highlights the need for an approach that can\nleverage the complementary strengths of various models to comprehensively\nimprove the composition capability. To this end, we introduce IterComp, a novel\nframework that aggregates composition-aware model preferences from multiple\nmodels and employs an iterative feedback learning approach to enhance\ncompositional generation. Specifically, we curate a gallery of six powerful\nopen-source diffusion models and evaluate their three key compositional\nmetrics: attribute binding, spatial relationships, and non-spatial\nrelationships. Based on these metrics, we develop a composition-aware model\npreference dataset comprising numerous image-rank pairs to train\ncomposition-aware reward models. Then, we propose an iterative feedback\nlearning method to enhance compositionality in a closed-loop manner, enabling\nthe progressive self-refinement of both the base diffusion model and reward\nmodels over multiple iterations. Theoretical proof demonstrates the\neffectiveness and extensive experiments show our significant superiority over\nprevious SOTA methods (e.g., Omost and FLUX), particularly in multi-category\nobject composition and complex semantic alignment. IterComp opens new research\navenues in reward feedback learning for diffusion models and compositional\ngeneration. Code: https://github.com/YangLing0818/IterComp",
        "updated": "2024-10-09 17:59:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07171v1"
    },
    {
        "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate",
        "authors": "Qidong HuangXiaoyi DongPan ZhangYuhang ZangYuhang CaoJiaqi WangDahua LinWeiming ZhangNenghai Yu",
        "links": "http://arxiv.org/abs/2410.07167v1",
        "entry_id": "http://arxiv.org/abs/2410.07167v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07167v1",
        "summary": "We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.",
        "updated": "2024-10-09 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07167v1"
    },
    {
        "title": "AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation",
        "authors": "Yukang CaoLiang PanKai HanKwan-Yee K. WongZiwei Liu",
        "links": "http://arxiv.org/abs/2410.07164v1",
        "entry_id": "http://arxiv.org/abs/2410.07164v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07164v1",
        "summary": "Recent advancements in diffusion models have led to significant improvements\nin the generation and animation of 4D full-body human-object interactions\n(HOI). Nevertheless, existing methods primarily focus on SMPL-based motion\ngeneration, which is limited by the scarcity of realistic large-scale\ninteraction data. This constraint affects their ability to create everyday HOI\nscenes. This paper addresses this challenge using a zero-shot approach with a\npre-trained diffusion model. Despite this potential, achieving our goals is\ndifficult due to the diffusion model's lack of understanding of ''where'' and\n''how'' objects interact with the human body. To tackle these issues, we\nintroduce AvatarGO, a novel framework designed to generate animatable 4D HOI\nscenes directly from textual inputs. Specifically, 1) for the ''where''\nchallenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to\nidentify the contact body part from text prompts, ensuring precise\nrepresentation of human-object spatial relations. 2) For the ''how'' challenge,\nwe introduce correspondence-aware motion optimization that constructs motion\nfields for both human and object models using the linear blend skinning\nfunction from SMPL-X. Our framework not only generates coherent compositional\nmotions, but also exhibits greater robustness in handling penetration issues.\nExtensive experiments with existing methods validate AvatarGO's superior\ngeneration and animation capabilities on a variety of human-object pairs and\ndiverse poses. As the first attempt to synthesize 4D avatars with object\ninteractions, we hope AvatarGO could open new doors for human-centric 4D\ncontent creation.",
        "updated": "2024-10-09 17:58:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07164v1"
    }
]