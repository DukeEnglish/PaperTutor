[
    {
        "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
        "authors": "Hanrong YeHaotian ZhangErik DaxbergerLin ChenZongyu LinYanghao LiBowen ZhangHaoxuan YouDan XuZhe GanJiasen LuYinfei Yang",
        "links": "http://arxiv.org/abs/2410.07177v1",
        "entry_id": "http://arxiv.org/abs/2410.07177v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07177v1",
        "summary": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.",
        "updated": "2024-10-09 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07177v1"
    },
    {
        "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models",
        "authors": "Fei WangXingchen WanRuoxi SunJiefeng ChenSercan Ö. Arık",
        "links": "http://arxiv.org/abs/2410.07176v1",
        "entry_id": "http://arxiv.org/abs/2410.07176v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07176v1",
        "summary": "Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.",
        "updated": "2024-10-09 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07176v1"
    },
    {
        "title": "Neural Circuit Architectural Priors for Quadruped Locomotion",
        "authors": "Nikhil X. BhattasaliVenkatesh PattabiramanLerrel PintoGrace W. Lindsay",
        "links": "http://arxiv.org/abs/2410.07174v1",
        "entry_id": "http://arxiv.org/abs/2410.07174v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07174v1",
        "summary": "Learning-based approaches to quadruped locomotion commonly adopt generic\npolicy architectures like fully connected MLPs. As such architectures contain\nfew inductive biases, it is common in practice to incorporate priors in the\nform of rewards, training curricula, imitation data, or trajectory generators.\nIn nature, animals are born with priors in the form of their nervous system's\narchitecture, which has been shaped by evolution to confer innate ability and\nefficient learning. For instance, a horse can walk within hours of birth and\ncan quickly improve with practice. Such architectural priors can also be useful\nin ANN architectures for AI. In this work, we explore the advantages of a\nbiologically inspired ANN architecture for quadruped locomotion based on neural\ncircuits in the limbs and spinal cord of mammals. Our architecture achieves\ngood initial performance and comparable final performance to MLPs, while using\nless data and orders of magnitude fewer parameters. Our architecture also\nexhibits better generalization to task variations, even admitting deployment on\na physical robot without standard sim-to-real methods. This work shows that\nneural circuits can provide valuable architectural priors for locomotion and\nencourages future work in other sensorimotor skills.",
        "updated": "2024-10-09 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07174v1"
    },
    {
        "title": "Do better language models have crisper vision?",
        "authors": "Jona RuthardtGertjan J. BurghoutsSerge BelongieYuki M. Asano",
        "links": "http://arxiv.org/abs/2410.07173v1",
        "entry_id": "http://arxiv.org/abs/2410.07173v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07173v1",
        "summary": "How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.",
        "updated": "2024-10-09 17:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07173v1"
    },
    {
        "title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
        "authors": "Fabian PaischerLukas HauzenbergerThomas SchmiedBenedikt AlkinMarc Peter DeisenrothSepp Hochreiter",
        "links": "http://arxiv.org/abs/2410.07170v1",
        "entry_id": "http://arxiv.org/abs/2410.07170v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07170v1",
        "summary": "Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across model\nweights. Recent works focus on weight-driven initialization or learning of\nadaptive ranks during training. Both approaches have only been investigated in\nisolation, resulting in slow convergence or a uniform rank distribution, in\nturn leading to sub-optimal performance. We propose to enhance LoRA by\ninitializing the new weights in a data-driven manner by computing singular\nvalue decomposition on minibatches of activation vectors. Then, we initialize\nthe LoRA matrices with the obtained right-singular vectors and re-distribute\nranks among all weight matrices to explain the maximal amount of variance and\ncontinue the standard LoRA fine-tuning procedure. This results in our new\nmethod Explained Variance Adaptation (EVA). We apply EVA to a variety of\nfine-tuning tasks ranging from language generation and understanding to image\nclassification and reinforcement learning. EVA exhibits faster convergence than\ncompetitors and attains the highest average score across a multitude of tasks\nper domain.",
        "updated": "2024-10-09 17:59:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07170v1"
    }
]