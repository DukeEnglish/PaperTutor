Optimizing Estimators of Squared Calibration Errors
in Classification
Sebastian G. Gruber sebastian.gruber@dkfz.de
German Cancer Consortium (DKTK), partner site Frankfurt/Mainz,
a partnership between DKFZ and UCT Frankfurt-Marburg, Germany, Frankfurt am Main, Germany
German Cancer Research Center (DKFZ), Heidelberg, Germany
Goethe University Frankfurt, Germany
Francis Bach
Inria, Ecole Normale Supérieure, PSL Research University, Paris, France
Abstract
In this work, we propose a mean-squared error-based risk that enables the comparison and
optimization of estimators of squared calibration errors in practical settings. Improving the
calibration of classifiers is crucial for enhancing the trustworthiness and interpretability of
machine learning models, especially in sensitive decision-making scenarios. Although various
calibration (error) estimators exist in the current literature, there is a lack of guidance
on selecting the appropriate estimator and tuning its hyperparameters. By leveraging the
bilinear structure of squared calibration errors, we reformulate calibration estimation as a
regression problem with independent and identically distributed (i.i.d.) input pairs. This
reformulation allows us to quantify the performance of different estimators even for the most
challenging calibration criterion, known as canonical calibration. Our approach advocates for
a training-validation-testing pipeline when estimating a calibration error on an evaluation
dataset. We demonstrate the effectiveness of our pipeline by optimizing existing calibration
estimators and comparing them with novel kernel ridge regression-based estimators on
standard image classification tasks.
1 Introduction
In the field of machine learning, classification tasks involve predicting discrete class labels for given instances
(Bishop & Nasrabadi, 2006). As these models are increasingly being employed in critical applications such as
healthcare (Haggenmüller et al., 2021), autonomous driving (Feng et al., 2020), weather forecasting (Gneiting
&Raftery,2005), andfinancialdecision-making(Frydmanetal.,1985), theneedforreliableandinterpretable
predictions has become of critical importance. A key aspect of reliability in classification models is the
calibrationoftheirpredictedprobabilities(Murphy&Winkler,1977;Hekleretal.,2023). Calibrationrefersto
the alignment between predicted probabilities and the true likelihoods of outcomes, ensuring that predictions
are not only accurate but also meaningful in terms of their confidence scores (Murphy, 1973). Despite the
advancements in model architectures and learning algorithms, many modern classifiers, such as deep neural
networks, are prone to producing overconfident predictions (Minderer et al., 2021). This overconfidence can
be attributed to several factors, including the model’s complexity, training data limitations, and inherent
biases in learning processes (Guo et al., 2017). Consequently, even models that achieve high accuracy might
suffer from poor calibration, leading to potential misinterpretations and suboptimal decisions.
Toquantifytheextenttowhichamodelismiscalibrated,calibrationerrorshavebeenintroduced(Naeinietal.,
2015). However, their estimators are usually biased (Roelofs et al., 2022) and inconsistent (Vaicenavicius
et al., 2019). Other calibration errors with unbiased estimators exist but they lack theoretical derivation and
are difficult to interpret (Widmann et al., 2019; 2021; Marx et al., 2024). This, in turn, is highly problematic
since we cannot quantify how reliable a model is if we either do not know how reliable the metric is or how to
1
4202
tcO
9
]GL.sc[
1v41070.0142:viXrainterpret it. In this work, we tackle the former problem. We propose mean-squared error risk minimization
to find an optimal calibration estimator for any squared calibration error. This risk can be applied in any
practical scenario to compare and select different estimators, and works for all notions of calibration, even for
the notoriously difficult canonical calibration. Our contributions are as follows:
• We propose a novel risk applicable for squared calibration estimators in Section 3.1, which represents
an optimization objective for comparing calibration estimators proposed by the literature.
• We formulate a calibration-evaluation pipeline based on our risk in Section 3.2.2, which allows to
optimize calibration estimators used by the literature.
• We propose novel kernel ridge regression-based calibration estimators in Section 4, and compare
these with optimized baselines on common image classification models in Section 5.
2 Background
In the following, we offer an extensive introduction into the background of this work. First, we give briefly
measure theoretic preliminaries, followed by the different notions of calibration used for classification by the
literature. We then introduce and discuss commonly used estimators for these notions.
2.1 Measure Theoretic Preliminaries
Throughout this work we use measure theoretic definitions to formalise our contribution, its assumptions, and
itslimitations. Specifically,weassumeameasurespace(Ω,F,µ)isgiven. Weassociatewitharandomvariable
X∶Ω→X (ameasureablefunction)aprobabilityspace(X,F ,P )withσ-fieldF ={X(A)∣A∈F }and
X X X
pushforward measure P =µ◦X−1. Further, we may also assume a second random variable Y∶Ω→Y such
X
that we have a joint probability space (X ×Y,F ,P ), where F and P are defined analogous as
XY XY XY XY
F and P . Further, if a random variable Y is discrete, we use P to represent both its probability measure
X X Y
and the associated probability vector in the simplex ∆d ≔{(p ,...,p )⊺ ∈[0,1]d ∣∑d p =1}, where d is the
1 k i=1 i
number of unique outcomes. In the same sense we may use P ≔ dP XY as a measurable function X →∆d.
Y∣X dP
X
Finally, we say a statement holds µ-almost surely (µ-a.s.) if it is true except on a set of µ-measure zero
(Capiński & Kopp, 2004).
2.2 Mean-Squared Error Risk Minimization and Calibration
The mean-squared error (MSE) is one of the most common loss functions used in regression problems, see,
e.g., (Efron, 1994; Schölkopf & Smola, 2002; Bishop & Nasrabadi, 2006; Goodfellow et al., 2016; Murphy,
2022; Bach, 2024). Its expected loss for a vector-valued sample Y ∼P from a target distribution P and a
Y Y
prediction c∈Rd is defined by
L MSE(c,P Y)≔E Y∼P
Y
[∥c−Y∥2 ]. (1)
It holds that the expectation of the target term in the difference is the unique minimizer, i.e.,
E [Y]=argminL (c,P ). (2)
MSE Y
c∈Rd
Now, let’s introduce another random variable X such that (X,Y)∼P follows a joint distribution and X
XY
can be used as input for a regression model m∶X →Rd to approximate the target conditional distribution
m∗ (x)≔E [Y ∣X =x]. Then, the corresponding risk of m is defined by
R MSE(m)≔E X∼P
X
[L MSE(m(X),P Y∣X)]=E (X,Y)∼P
XY
[∥m(X)−Y∥2 ]. (3)
∗
Given m≠m , where the inequality holds for a set of positive probability mass, it follows that
R (m)>R (m∗ ). (4)
MSE MSE
2However, we have the measure theoretic limitation that there might be a null set A ∈ F and some m˜
X
such that m˜ (x)≠m∗ (x) for all x∈A while R (m˜)=R (m∗ ). This limitation of risk minimization is
MSE MSE
usually irrelevant in machine learning applications. However, in our case, it will require additional theoretical
assumptions on the target conditional distribution to make risk minimization a usable tool for assessing
calibration estimators.
The MSE can also be used for classification tasks with a one-hot encoded target in Equation 1, often referred
to as Brier score (Brier, 1950). Then, Murphy (1973) introduces the concept of calibration for a classifier
f∶X →∆d by showing that
R MSE(f)=E X∼P X [∥f(X)−P Y∣f(X)∥2 ]−E X∼P X [∥P Y −P Y∣f(X)∥2 ]+ ∥P Y∥2 . (5)
Thefirsttermontheright-handsideisusuallyreferredtoasthecalibrationterm,andthesecondassharpness
term, coining Equation (5) as calibration-sharpness decomposition of the Brier score (Gneiting et al., 2007;
Gruber & Buettner, 2022; Kuleshov & Deshpande, 2022; Gruber et al., 2024; Sun et al., 2024). In the
calibration term, f(X) is compared with the target distribution P (Y ∣f(X)) given the full predicted vector.
In current literature, this notion of calibration is referred to as canonical calibration (Vaicenavicius et al.,
2019; Popordanoska et al., 2022b; Gupta & Ramdas, 2022; Gruber et al., 2024). Formally, we say the model
f is canonically calibrated if and only if
P (Y =i∣f(X)=p)=p for all p∈∆d ,i=1...d. (6)
i
The corresponding L2 canonical calibration error is defined as
√
CCE (f)≔ E [∥f(X)−P ∥2 ], (7)
2 Y∣f(X)
whichisequaltothecalibrationterminEquation(5). ItholdsthatCCE (f)=0ifandonlyiff iscanonically
2
calibrated P -almost surely.
X
However,canonicalcalibrationerrorsarenotoriouslydifficulttoestimateandrepresentacalibrationstrictness
whichmaynotbenecessaryinpractice(Vaicenaviciusetal.,2019). Consequently, othernotionsofcalibration
have been proposed, which we discuss next.
2.3 Alternative Notions of Calibration
Besides canonical calibration, multiple notions of calibration have been introduced in the literature (Zadrozny
& Elkan, 2002; Vaicenavicius et al., 2019; Kull et al., 2019; Gupta & Ramdas, 2022). Respective calibration
errors assess the degree a classifier violates a given notion. In recent literature, the most common notion is
top-label confidence calibration (Naeini et al., 2015; Guo et al., 2017; Joo et al., 2020; Kristiadi et al., 2020;
Rahimi et al., 2020; Tomani et al., 2021; Minderer et al., 2021; Tian et al., 2021; Islam et al., 2021; Menon
et al., 2021; Morales-Álvarez et al., 2021; Gupta et al., 2021; Wang et al., 2021; Fan et al., 2022; Dehghani
et al., 2023; Chang et al., 2024). Here, we compare if the predicted top-label confidence max i∈Y f i(X)
matches the conditional accuracy P (Y =argmax i∈Y f i(X)∣max i∈Y f i(X)) given the prediction. Formally,
we say the classifier f is top-label confidence calibrated if and only if
P (Y =argm
i
axf
i(X)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)m
iaxf i(X)=p)=p for all p∈[0,1]. (8)
The corresponding L2 top-label confidence calibration error is defined as
√
TCE
2(f)≔√√√√√⎷E
[(maxf i(X)−P (Y =argmaxf i(X)∣maxf
i(X)))2
]. (9)
i i i
It holds that TCE (f)=0 if and only if f is top-label confidence calibrated P -almost surely. It is easier
2 X
to estimate than CCE since the target conditional distribution is only based on a scalar random variable
2
independent of the number of classes. However, top-label confidence calibration is a weaker condition than
3canonical calibration since the implication CCE (f) = 0 ⟹ TCE (f) = 0 does not generally hold in the
2 2
reverse direction (Gruber & Buettner, 2022).
Other notions of calibration exist, which also reduce the prediction to a scalar, such as class-wise calibration
(Zadrozny & Elkan, 2002; Kull et al., 2019; Kumar et al., 2019). Gupta & Ramdas (2022) introduce
further of such notions. In general, these notions are transformations of the full probability vectors to a
lower dimensional space. Similar to top-label confidence calibration, this makes them easier to estimate
than canonical calibration but also turns them into a weaker condition due to the information loss of the
transformation (Vaicenavicius et al., 2019; Gruber & Buettner, 2022).
2.4 Calibration Estimators
We assume a dataset of i.i.d. samples (X ,Y ),...,(X ,Y ) ∼ P to estimate the calibration of a given
1 1 n n XY
classifier f∶X →∆d. The most common approach to estimate calibration errors based on scalar conditionals
are binning schemes (Naeini et al., 2015; Guo et al., 2017; Minderer et al., 2021; Detlefsen et al., 2022). A
prominent estimator is the so-called expected calibration error (ECE), which is a binning-based estimator of
the L1 top-label confidence calibration error (Guo et al., 2017). In essence, the conditional target distribution
P (Y =argmax f (X)∣max f (X)) is estimated via a histogram binning scheme, which places all top-label
i i i i
confidence predictions into mutually distinct bins B ≔{i∣argmax f (X )∈I }, m=1,...,M, based on
m j j i m
a partition ⋃ I =[0,1]. The analogous L2 estimator is given by
m m
√
TCEb 2in
≔√√√√√√⎷∑M
∣B nm∣ (acc(B m)−conf(B m))2 (10)
m=1
with acc(B)= ∣B1 ∣∑ i∈B1 Yi=argmax jfj(Xi) and conf(B)= ∣B1 ∣∑ i∈Bargmax jf j(X i) (Kumar et al., 2019). This
estimator is primarily suitable for target distributions conditioned on a scalar random variable. The choice of
bin intervals I ,...,I is user-defined and the estimator only converges to TCE for an adaptive scheme
1 M 2
(Vaicenavicius et al., 2019). Patel et al. (2021) and Roelofs et al. (2022) propose approaches to automatically
select appropriate bins. However, in practice, it remains an open challenge to definitively select the optimal
choice for a specific dataset and classifier. Analogous binning-based estimators for class-wise calibration exist
in the literature and share these limitations (Kumar et al., 2019; Nixon et al., 2019; Vaicenavicius et al.,
2019).
Estimatingcanonicalcalibrationismoredifficultthanothernotionsofcalibrationduetothetargetdistribution
P (Y ∣f(X)) being conditioned on a vector-valued random variable. Popordanoska et al. (2022b) propose
to use a kernel density ratio estimator, which is closely related to the Nadaraya-Watson-estimator (Bierens,
1996). The estimator for CCE is given by
2
√
CCEk 2de
≔√√√√√√⎷n1
i∑
=n 1(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)f(X
i)−
∑ ∑jk
jd kir
d( irf (f(X (Xj) j; )f ;f(X (Xi)) i)e )Yj(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)(cid:194)2
, (11)
where e refers to the unit vector with a 1 at index i and k is chosen to be the Dirichlet kernel, which
i dir
is specifically suited for the simplex space (Ouimet & Tolosana-Delgado, 2022). The authors also propose
analogous kernel density based estimators for top-label and class-wise calibration errors, which we will denote
asTCEkde andCWCEkde. Eventhoughthekerneldensityapproachisadvantageouscomparedtothebinning
2 2
approach for higher dimensions, it still shares some of its limitations. Popordanoska et al. (2022b) show that
the estimator converges in the infinite data limit, but it is still not clear what is the optimal choice of kernel
and kernel hyperparameters in the finite data regime.
To summarize, a lot of different approaches have been proposed by the literature to estimate calibration
errors. However, it is not clear how to compare different estimators and pinpoint an optimal choice in a finite
data setup in practice.
43 A Mean-Squared Error Risk for Calibration Estimators
In this section, we present our main contribution: A mean-squared error based risk, which can be applied to
comparedifferentcalibrationestimatorsinapractical, finitedatasetup. Wefirstdiscussitsmeasuretheoretic
foundations in Section 3.1, and, then, proposea training-inference pipeline for estimating the calibration error
in practice in Section 3.2. This pipeline is analogous to how model training, model selection, and test error
evaluation is done in practice in machine learning (Bishop & Nasrabadi, 2006). All formulations are with
respect to canonical calibration, since this is the most general case. Other notions, like top-label confidence
calibration, can be derived by restricting the canonical case to binary classification. All missing proofs are
presented in Appendix C.
3.1 Theoretical Definition and Properties
Note that for the CCE calibration error it is sufficient to find a function h∗∶∆d×∆d →R such that
2
h∗ (p,p′ )=⟨p−P Y∣f(X)=p,p′−P Y∣f(X)=p′⟩, (12)
since from this follows that
E [h∗ (f(X),f(X))]=CCE . (13)
X 2
Indeed, in a later section, we will discover that current estimators already implicitly use such a form. In
general, we refer to a function h∶∆d×∆d →R as calibration estimation function. We now propose a
∗
risk, which quantifies how close such a h is to h . To achieve this, we slightly modify the mean-squared error
loss function of Equation (1) in the following.
Definition 1. For a prediction c∈R, a target product measure P ⊗P , and constants p,p′ ∈∆d we define
Y V
the calibration estimator loss by
L CE(c,P Y ⊗P V;p,p′ )≔E (Y,V)∼P Y⊗P V [(⟨p−e Y,p′−e V⟩−c)2 ], (14)
where e refers to the unit vector with a 1 at index i.
i
Similar to the mean-squared error, it holds that L has an unique minimizer given by
CE
⟨p−P ,p′−P ⟩=argminL (c,P ⊗P ;p,p′ ). (15)
Y V CE Y V
c∈R
We use this definition of a novel loss to define the respective risk in the following.
Definition 2. For a calibration estimator function h∶∆d×∆d →R, we define the calibration estimation
risk by
R CE(h)≔E X,X′[L CE(h(f(X),f(X′ )),P Y∣f(X)=f(X)⊗P Y∣f(X)=f(X′);f(X),f(X′ ))] (16)
with
X,X′ i∼idP
.
X
Similar to R in Equation (3), we may also express R in a simpler form, since it holds
MSE CE
R CE(h)=E X,X′,Y,Y′[(⟨f(X)−e Y,f(X′ )−e Y′⟩−h(f(X),f(X′ )))2 ] (17)
with
(X,Y),(X′ ,Y′ )i∼idP
. This formulation will be used in a later section to construct the empirical risk.
XY
Next, we establish that our proposed risk can distinguish the right solution almost surely.
Theorem 1. For any h∶∆d×∆d →R for which h=h∗ does not hold P ⊗P −almost surely we have
f(X) f(X)
that
R (h)>R (h∗ ). (18)
CE CE
This property would be sufficient in practice, if we were using h with arguments sampled from P ⊗P
f(X) f(X)
to estimate the calibration error. However, as demonstrated by Equation (13), we predict CCE via the same
2
5sample in both arguments. Mirroring the arguments results in a possible P ⊗P -null set since only
f(X) f(X)
elements in the diagonal D (∆d )≔{(p,p)∣p∈∆d }⊂∆d×∆d are considered. Consequently, the diagonal of
an optimum h′∗ identified via R may "slip through" the P ⊗P -a.s. guarantee in Theorem 1, and in
CE f(X) f(X)
turn may result in E [h′∗ (f(X),f(X))]≠CCE . To avoid such theoretical exceptions, we require additional
2
theoretical assumptions, which we state in the following.
Theorem 2. Assume a function h∶∆d×∆d →R is continuous in all points of the diagonal D (∆d \A) with
A being a P -null set, and the target as a function P ∶∆d → ∆d is continuous P -almost surely.
f(X) Y∣f(X) f(X)
Further, assume the boundary of the support of P does not involve a singular distribution, then it holds
f(X)
that
R (h)=R (h∗ ) ⟹ E [h(f(X),f(X))]=CCE . (19)
CE CE 2
This result states under which conditions we can expect that an optimal risk indicates a truthful calibration
estimation. We briefly discuss these conditions, which are of purely technical nature and should not influence
practical results. First, the continuity of h is non-problematic since it is user-defined and infinitely many
points of discontinuity are allowed (as long as they have no probability mass). The continuity of P may
Y∣f(X)
be considered the most relevant condition in practice, since we usually do not know about the nature of
the target distribution. However, again, infinitely many points of discontinuity are allowed, as long as their
overall probability mass is zero. The last assumption, namely that the boundary of the support is not allowed
to be part of a singular distribution, disqualifies certain theoretically crafted distributions. One such example
is the Cantor distribution, which consists of infinitely many disconnected points each of zero probability
(Teschl, 2014). In summary, the purpose of the conditions in Theorem 2 is to guarantee that samples from
P ⊗P can be arbitrarily close to the diagonal D (∆d ) and that this closeness indicates how well h
f(X) f(X)
matches h∗ on D (∆d ).
Remark. Alternatively to our approach, one might also use a loss for finding a probabilistic model gˆ(p)≈
P Y∣f(X)=p and then use hˆ (p,p′ )≔⟨p−gˆ(p),p′−gˆ(p′ )⟩≈⟨p−P Y∣f(X)=p,p′−P Y∣f(X)=p′⟩ as a solution. However,
learning a predictive space ∆d becomes increasingly more difficult for higher dimensions d than a regression
problem in R. For example, our approach is invariant to any orthogonal matrix M since ⟨Mx,My⟩=⟨x,y⟩.
3.2 Estimating Calibration for finite data
In this section, we propose a novel calibration evaluation pipeline for the finite data regime according to
our theory. First, we give an unbiased and consistent estimator of the risk in form of an U-statistic. Then,
we mimic the training-validation-testing pipeline of a conventional machine learning model for a debiased
calibration estimate. This procedure allows to select between different calibration estimators and optimize
their hyperparameters.
3.2.1 Risk Estimator
Note that the risk as formulated in Equation (17), is an expectation of two i.i.d. tuples of random variables
(X,Y) and (X′ ,Y′ ). Consequently, given an i.i.d. dataset (X ,Y ),...,(X ,Y )∼P , we can construct an
1 1 n n XY
U-statistic estimator (Shao, 2003) via
n n
Rˆ (h)≔ 1 ∑∑ (⟨f(X )−e ,f(X )−e ⟩−h(f(X ),f(X )))2 . (20)
CE n(n−1) i Yi j Yj i j
i=1j=1
i≠j
It holds that E [Rˆ (h)] = R (h), and Rˆ (h) → R (h) in distribution if n → ∞. The estimator has
CE CE CE CE
quadratic complexity in n. However, an estimator with linear complexity can be constructed as well by
excluding certain index combinations.
3.2.2 Calibration-Evaluation Pipeline
∗
In general, we cannot expect to find h . However, the empirical risk allows us to find a parametrized
∗
h close to h , where θ denote its parameters and η its hyperparameters. Consequently, similar to how
θ,η
6traditional machine learning works, we need a training-validation-test split to achieve unbiased estimation of
the calibration error once we optimized h . Specifically, we use a training set to find θ and a validation set
θ,η tr
to find η . The final calibration estimate is then computed by
val
Ĉ E (f)≔ 1 ∑ h (f(X),f(X)), (21)
2 n
te
θtr,ηval
X∈D
te
where D is the test set of size n , and Ĉ E is representative for different notions of calibration. We might
te te 2
also use multiple training, validation and test sets via (nested) cross validation. In that case, we average the
results of the calibration estimation functions fitted in each fold.
Remark. We encourage to never compare the test set risk of different calibration estimation functions, since
this would put a bias on the final calibration estimation. Neglecting this is equivalent to selecting an optimal
classifier based on the test accuracy in a classification task.
4 Calibration Estimation Functions
In this section, we first formulate the calibration estimation functions implicitly used in the literature. Then,
we introduce two novel calibration estimators based on kernel ridge regression, which minimize regularized
versions of the empirical risk in Equation (20). All calibration estimation functions can be seen as preliminary
to future research, since we may find function classes with lower validation risk by expanding the search space
and computational resources. All missing proofs are located in Appendix C.
4.1 Binning and Kernel Density
In this section, we show that the binning estimator TCEbin of Equation (10) and the kernel density ratio
2
estimator CCEkde of Equation (11) are the mean prediction of an implicit calibration estimator function of
2
the form
n
1
∑h(f(X ),f(X )) (22)
n i i
i=1
for some h∶∆d×∆d →R and an i.i.d. dataset (X ,Y ),...,(X ,Y )∼P . For the binning based estimator,
1 1 n n XY
define
M M
h bin(p,p′ )≔(∑ (conf(B m)−acc(B m))1 p∈Im)(∑ (conf(B m)−acc(B m))1 p′∈Im), (23)
m=1 m=1
where I ...I , acc(B ), and conf(B ) are defined as above in Equation (10). It holds that
1 M m m
n
(TCEbin )2 = 1 ∑h (f(X ),f(X )). (24)
2 n bin i i
i=1
Similarly, one may formulate the debiased (but not unbiased) estimator of Kumar et al. (2019).
Further,wecanalsoputtheestimatorofPopordanoskaetal.(2022b)intheformofEquation(22)bydefining
h kde(p,p′ )≔⟨p− ∑ ∑n i= n1e Y kik di (r f(f (X(X )i ,) p, )p) ,p′− ∑ ∑n i= n1e Y kik di (r f(f (X(X )i ,) p, ′p )′ ) ⟩, (25)
i=1 dir i i=1 dir i
where k is the Dirichlet kernel as defined above in Equation (11). Again, it holds that
dir
n
(CCEkde )2 = 1 ∑h (f(X ),f(X )). (26)
2 n kde i i
i=1
We will also use an analogous estimator TCEkde for estimating TCE in the experiment sections. Extending
2 2
the binning based estimator to CCE is in general not possible. Note that the runtime complexity of CCEkde
2 2
is in O(n2d), since we compare every evaluation instance with every training instance for every class. In the
following weintroducenovelestimators, whichareruntimeinvarianttothenumberofclasses. Theyarebased
on kernel ridge regression and directly minimize the empirical risk under kernel ridge regression assumptions.
74.2 Kernel Ridge Regression
Here, we propose two novel calibration estimators, which are derived as closed-form solutions under the
typical kernel ridge regression assumptions, see, e.g., (Schölkopf & Smola, 2002; Bach, 2024). The following
approachisbasedonthenotionofordinaryKroneckerkernelridgeregression(Stocketal.,2018). Specifically,
we require a reproducing kernel Hilbert space (RKHS) H with an associated feature map ϕ∶∆d →H, kernel
kH,innerproduct⟨.,.⟩H,andnorm∥.∥H. DenotewithH ⊗H thetensorproductoftheHilbertspacewith
itself, withrespectivefeaturemap(ϕ⊗ϕ)∶∆d×∆d →H ⊗H. Next, assumethat⟨f(X)−e Y,f(X′ )−e Y′⟩=
⟨g∗ ,(ϕ⊗ϕ)(p,p′ )⟩ +ϵ for some g∗ ∈ H ⊗H and zero-mean noise term ϵ. Define the kernel ridge
H⊗H
objective for a g ∈H ⊗H via
n n
Rˆ CE,λ(g)≔ n1
2
∑∑ (⟨f(X i)−e Yi,f(X j)−e Yj⟩−h(f(X j),f(X j)))2+λ∥g∥2
H⊗H
, (27)
i=1j=1
where
h(p,p′
) =
⟨g,(ϕ⊗ϕ)(p,p′
)⟩ . Then, a closed-form minimizer can be found, which results in the
H⊗H
predictor
h (p,p′ )≔vec⊺ (∆⊺ ∆ )(K ⊗K +λn2 I)−1 (k (p)⊗k (p′ )), (28)
kkr Yf(X) Yf(X) f(X) f(X) f(X) f(X)
w (kh Her (e f⊗ (Xb i)e ,c fom (Xe js ))t )h i,je ∈{K 1..r .o n}ne ∈ck Rer n×p nr ,o ad nu dct k, f∆ (XY )f (p(X )) ≔≔ (k( Hf( (X f1 () X− i)e ,Y p1
))
i∈⋯ {1...nf
}
( ∈X n R) n−
.
e WYn i) th∈ ouR td f× un r, thK erf(X m) o≔
d-
ifications, computing Equation (28) has runtime complexity O(n6 ) due to the matrix inverse, which is
practically infeasible. To reduce the complexity, we classically for Kronecker products make use of the
eigenvalue decomposition K =Q diag(λ ,...,λ )Q⊺ , which is in O(n3 ). Define Λ˜ ∈Rn×n with
f(X) f(X) 1 n f(X) f(X)
[Λ˜ ] = 1 and denote with ⊙ the Hadamard product. It holds that
f(X) ij λiλj+λn2
h (p,p′ )=k⊺ (p)Q (Λ˜ ⊙Q⊺ ∆⊺ ∆ Q )Q⊺ k (p′ ). (29)
kkr f(X) f(X) f(X) f(X) Yf(X) Yf(X) f(X) f(X) f(X)
This representation consists of multiplications of n×n matrices and in consequence is in O(n3 ). A more
general approach uses the Schur decomposition to achieve such a reduction in complexity (Moravitz Martin
& Van Loan, 2007). Naively computing the empirical generalization error Rˆ (h ) on an evaluation set
CE kkr
(X 1′ ,Y 1′ ),...,(X n′ ′,Y n′ ′)forsomen′ ∝nhascomplexityO(n5 ),whichisagainprohibitiveinpractice. However,
we can also reduce this complexity to O(n3 ) since it holds
n′ n′
Rˆ (h )= 1 ∑∑ (⟨f(X )−e ,f(X )−e ⟩−Hkkr )2 , (30)
CE kkr n′ (n′−1) i Yi j Yj ij
i=1j=1
j≠i
with
Hkkr ≔K⊺ f(X)f(X′)Q f(X)(Λ˜ f(X)⊙Q⊺ f(X)∆⊺ Yf(X)∆ Yf(X)Q f(X))Q⊺ f(X)K f(X)f(X′) ∈Rn′×n′ (31)
′
and [K f(X)f(X′)]
ij
≔kH (f(X i),f(X j)).
Alternatively, one may also fit a kernel ridge regressor for the problem assumption f(X)−e =g˜∗ ϕ(X)+ϵ
Y
with g˜∗ ∈ {g˜∶H →∆d }, and then use the result as plug-in for the inner product. This is referred to as
two-step kernel ridge regression (Stock et al., 2018) or U-statistic regression (Park et al., 2021). The model
then becomes
h
(p,p′ )≔k⊺
(p)(K
+λnI)−1 ∆⊺
∆ (K
+λnI)−⊺
k
(p′
). (32)
ukkr f(X) f(X) Yf(X) Yf(X) f(X) f(X)
It holds that h =h if the kernel used for h incorporates the regularisation constant λ or when λ=0
ukkr kkr kkr
(Stock et al., 2018).
In the next section, we perform top-label confidence and canonical calibration evaluations with the discussed
and proposed estimators. Specifically, we use the proposed calibration estimation risk in Equation (20) for
comparison and the proposed calibration-evaluation pipeline of Section 3.2.2 for calibration estimation.
8Figure1: SimulatedexperimentforestimatingCCE inataskwith5classesand500instances. Theempirical
2
riskcorrectlyidentifiestheidealcalibrationestimatorwithθ =1indicatedbytheredline. Standarddeviations
across multiple seeds indicate the empirical risk stability.
√
Table 1: Validation set square root risk Rˆ ×100 of TCE estimators for CIFAR10 models with optimized
CE 2
hyperparameters. Lower is better. The estimator TCEkde performs worse and TCEukkr better or equal than
2 2
the other estimators. The large risk of TCEkde translates to an outlier calibration estimation in Figure 2a.
2
Model LeNet-5 Densenet-40 ResNetWide-32 Resnet-110 Resnet-110 SD
Estimator
TCE15−bins 14.96 ± 0.31 6.14 ± 0.12 5.03 ± 0.2 5.4 ± 0.24 4.73 ± 0.25
2
TCEbins 14.96 ± 0.31 6.12 ± 0.12 5.03 ± 0.2 5.39 ± 0.23 4.72 ± 0.25
2
TCEkde 14.98 ± 0.31 13.7 ± 0.27 12.31 ± 0.65 7.46 ± 0.35 10.65 ± 0.58
2
TCEkkr 14.96 ± 0.31 6.13 ± 0.12 5.03 ± 0.2 5.39 ± 0.24 4.72 ± 0.25
2
TCEukkr 14.96 ± 0.31 6.11 ± 0.12 5.03 ± 0.2 5.38 ± 0.24 4.72 ± 0.25
2
5 Experiments
In this section, we demonstrate how to use our proposed risk framework in practice. We first run a simulation
with known ground truth. Then, we evaluate the risk of the different calibration estimation function defined
in Section 4 and the respective estimated calibration error across a variety of image classification datasets and
models. We focus on top-label confidence, since it is the most prominent, and canonical calibration, which is
the most general. The source code is publicly available at https://github.com/waiting-for-acceptance.
5.1 Simulation
Weconstructasimulationexperimentwithknowngroundtruthtodemonstratehowourproposedriskidentifies
thesolution. Forthis, wedrawi.i.d. samplesP ,...,P ∼Dir(α ,...,α )fromaDirichletdistributionwith
1 500 1 5
concentration parameters α =⋅⋅⋅=α =0.04. These samples represent the (in practice unknown) ground
1 5
truth probability vectors of a classification task with 500 instances and 5 classes. Then, we sample Y ∼P
i i
for i = 1...500 as target labels. We set the hypothetical model predictions to f(X ) = softmax( 3 logP )
i 10 i
for i = 1...500, which represent miscalibrated predictions. The concentration parameters were set such
that the model would have an accuracy of ≈ 90%. We then define a calibration estimation function
h (p,p′ )≔⟨p−softmax(10θlogp),p′−softmax(10θlogp′ )⟩, which has a single learnable parameter θ ∈R
sim 3 3
referred to as temperature. The estimation function was chosen such that it matches the ground truth
9√
Table 2: Validation set risk Rˆ ×100 of TCE for Cifar100 models. Lower is better. Similar as before, the
CE 2
estimator TCEkde performs worse than the other estimators except for LeNet-5. The best performing are
2
TCEukkr and TCEbins.
2 2
Model LeNet-5 Densenet-40 ResNetWide-32 Resnet-110 Resnet-110 SD
Estimator
TCE15−bins 18.51 ± 0.16 20.66 ± 0.40 18.40 ± 0.19 18.67 ± 0.43 17.18 ± 0.20
2
TCEbins 18.50 ± 0.16 20.43 ± 0.38 18.24 ± 0.17 18.61 ± 0.42 17.11 ± 0.20
2
TCEkde 18.50 ± 0.16 23.74 ± 0.41 23.28 ± 0.18 19.69 ± 0.47 18.21 ± 0.19
2
TCEkkr 18.50 ± 0.16 20.52 ± 0.39 18.29 ± 0.18 18.59 ± 0.42 17.11 ± 0.20
2
TCEukkr 18.50 ± 0.16 20.51 ± 0.40 18.28 ± 0.18 18.61 ± 0.43 17.12 ± 0.21
2
(a) CIFAR10 (b) CIFAR100
Figure 2: Different TCE estimates of different models. Most calibration estimates approximately agree with
2
each other. Only TCEkde is an outlier for Densenet-40, ResNetWide-32, and Resnet-110 SD. However, it also
2
shows an increased calibration estimation risk in these cases (c.f. Table 1).
∗
h =h if and only if θ =1. In Figure 1, we plot the mean results with standard deviations of the empirical
sim
risk according to 100 repetitions of the experiment. As can be seen, the empirical risk correctly identifies the
correct calibration estimation function with θ =1.
5.2 Real World Settings
Inthefollowing, weevaluateandcompareestimatorsdiscussedinSection4accordingtoournovelcalibration-
evaluation pipeline introduced in Section 3.2.2 on standard image classifier setups, like CIFAR and ImageNet.
Technical Setup
The experiments are conducted across several model-dataset combinations, whose logit sets are openly
accessible(Kulletal.,2019;Rahimietal.,2020;Gruber&Buettner,2022).1 Theimageclassificationdatasets
in use are CIFAR10 with 10 classes, CIFAR100 with 100 classes (Krizhevsky, 2009), and ImageNet with
1,000 classes (Deng et al., 2009). Since we restrict ourselves to evaluating the calibration error estimate of
the models, we only use the test set of each dataset (CIFAR: n=10,000, ImageNet: n=25,000). Modifying
or selecting models based on the calibration estimate would require using the validation set instead. The
1https://github.com/MLO-lab/better_uncertainty_calibration
10√
Table 3: Validation set square root risk Rˆ ×100 of CCE estimators for CIFAR100 models. Again,
CE 2
lower is better. Contrary to previous results, the estimator CCEkde manages to outperform the kernel ridge
2
regression based estimators in some scenarios.
Model LeNet-5 Densenet-40 ResNetWide-32 Resnet-110 Resnet-110 SD
Estimator
CCEkde 8.38 ± 0.06 5.61 ± 0.09 4.98 ± 0.05 5.14 ± 0.1 4.79 ± 0.03
2
CCEkkr 8.36 ± 0.06 5.56 ± 0.09 5.00 ± 0.05 5.16 ± 0.1 4.80 ± 0.03
2
CCEukkr 8.35 ± 0.06 5.56 ± 0.09 5.01 ± 0.05 5.16 ± 0.1 4.80 ± 0.02
2
Figure 3: Different CCE estimates for CIFAR100 models. The risk values of Table 3 do not relate to the
2
calibration estimate but only indicate which estimator to trust more (here: CCEkde).
2
included models are LeNet-5 (LeCun et al., 1998), ResNet-110, ResNet-110 SD, ResNet-152 (He et al., 2016),
Wide ResNet-32 (Zagoruyko & Komodakis, 2016), DenseNet-40, DenseNet-161 (Huang et al., 2017), and
PNASNet-5 Large (Liu et al., 2018). We did not conduct model training ourselves and refer to Kull et al.
(2019) and Rahimi et al. (2020) for further details. We evaluate top-label confidence calibration and canonical
calibration estimators for these classifiers.
We run the calibration-evaluation pipeline proposed in Section 3.2.2 with a random split of the original test
set, using 80% for tuning the calibration estimator function via cross-validation and 20% for the calibration
test set D , which computes the mean in Equation (21). In all experiments, we use 5-fold cross-validation to
te
optimize the hyperparameters of a calibration estimator function. For the best performing hyperparameter,
themodelsacrossallfoldsareusedasanensemblepredictorforthefinalcalibrationestimationonthesetD .
te
This approach also allows us to include error bars according to the cross validation folds.
As calibration estimator functions, we consider h , h , h , and h for top-label confidence, as well as
bin kde kkr ukkr
h , h , and h canonical calibration. For both kernel ridge regression models, we use the RKHS of the
kde kkr ukkr
RBF kernel k (x,y)=exp(−γ∥x−y∥2 ). We set γ = 1 based on preliminary evaluations. We also evaluate
rbf 2
h with 15 bins without hyperparameter optimization, which we refer to as
TCE15−bins.
This corresponds
bin 2
to a common default choice in current practice (Guo et al., 2017; Detlefsen et al., 2022). More details on the
hyperparameter search spaces are given in Appendix B.
11Results
We now discuss the experimental results. All reported risks are with respect to the holdout sets in the
cross-validation folds. The reported calibration estimations are with respect to the calibration test set (20%
of the original test set). The error bars for the risk and calibration estimations are the standard errors
according to the cross-validation folds. In Table 1 we show the performance across different models of
CIFAR10, and in Table 2 across models of CIFAR100. Here, we compare the calibration estimation functions
for top-label confidence calibration. As can be seen, no calibration estimation function dominates all others.
Specifically, TCEkde performs worst for all models, even when we consider the error bars. The estimator
2
TCEukkr outperforms the other estimators, however, the difference is too marginal with respect to the error
2
bars to come to a confident conclusion. For the CIFAR100 models, TCEkde performs more similar to the
2
other estimators. Further, the optimized binning estimator TCEbins shows the strongest performance and not
2
TCEukkr anymore. However, again, the error bars are too large to designate a definitive ranking. Further, for
2
the LeNet-5 model in CIFAR10 and CIFAR100, our risk is not sufficiently sensitive to rank the estimators.
In Figure 2 we depict the corresponding TCE estimations. As can be seen, only TCEkde is occasionally
2 2
an outlier relative to the other estimators, which is expected based on the reported risk values of Table 1
and Table 2. Even though, we cannot spot a direct connection between all risk values and the estimated
calibration values in Figure 2, the large risk of TCEkde is indicative of a worse estimation. However, it is not
2
surprising that differences in the risk do not always translate to differences in the estimated values, since the
loss does not measure in which direction a calibration estimator function gives wrong predictions.
In Table 3, we report the risk values of the canonical calibration estimator functions for CIFAR100 classifiers.
As can be seen, CCEkde performs better than in previous results, outperforming the other approaches in
2
some cases. We can also see that CCEukkr performs better than CCEkkr, which is a continuous trend across
2 2
all results. However, the error bars dominate the performance difference and no clear cut conclusion can be
made. In Figure 3, we show the respective calibration estimates.
In Appendix B, we offer additional results regarding top-label confidence calibration for ImageNet classifiers
and canonical calibration for CIFAR10 classifiers. In summary, no calibration estimator outperforms the
other approaches across all settings. Additionally, risk performance is often indicative of outlying calibration
estimates. This underlines the requirement of a risk to assess which estimator to use for evaluating the
calibration of a new model in practice. We may expect to find better estimators by extending the search
space (e.g., by considering different kernels), or by including other model classes, like boosted trees or neural
networks(Bishop&Nasrabadi,2006). However,theproposedriskmaynotbesufficientlysensitivetorankthe
estimators according to their performance. Future research may involve exploring alternative loss functions
for more sensitive results.
6 Conclusion
Inthiswork,weintroducedamean-squarederrorbasedrisktocomparedifferentcalibrationestimators. Thisis
thefirstapproachintheliteraturetocomparedifferentcalibrationestimatorsonreal-worlddatasets. Weoffer
measure theoretic conditions for when the risk identifies an ideal estimator. We also derive novel calibration
estimators as closed-form minimizers of the empirical risk based on kernel ridge regression assumptions.
Further, usinganempiricalriskenablestoperformhyperparameteroptimizationandestimatorselectionviaa
training-validation-testingpipeline, similartoconventionalmachinelearning. Intheexperiments, weoptimize
the hyperparameters of common calibration estimators in the literature on popular real-world benchmarks,
and compare the risks of different optimized estimators. No dominating calibration estimator was found,
which emphasises the requirement of using our risk to detect an appropriate estimator for new settings in
practice.
References
Francis Bach. Learning Theory from First Principles. MIT Press, 2024.
12HermanJ.Bierens. Topics in Advanced Econometrics: Estimation, Testing, and Specification of Cross-section
and Time Series Models. Cambridge University Press, 1996.
Christopher M. Bishop and Nasser M. Nasrabadi. Pattern Recognition and Machine Learning, volume 4.
Springer, 2006.
Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78(1):1 –
3, 1950.
Marek Capiński and Peter Ekkehard Kopp. Measure, Integral and Probability, volume 14. Springer, 2004.
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,
Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactions
on Intelligent Systems and Technology, 15(3):1–45, 2024.
MostafaDehghani,JosipDjolonga,BasilMustafa,PiotrPadlewski,JonathanHeek,JustinGilmer,AndreasPe-
ter Steiner, Mathilde Caron, Robert Geirhos, and Ibrahim Alabdulmohsin. Scaling vision transformers to
22 billion parameters. In International Conference on Machine Learning, pp. 7480–7512, 2023.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In Conference on Computer Vision and Pattern Recognition, pp. 248–255, 2009.
Nicki Skafte Detlefsen, Jiri Borovec, Justus Schock, Ananya Harsh Jha, Teddy Koker, Luca Di Liello, Daniel
Stancl, Changsheng Quan, Maxim Grechkin, and William Falcon. Torchmetrics - measuring reproducibility
in pytorch. Journal of Open Source Software, 7(70):4101, 2022.
Bradley Efron. An Introduction to the Bootstrap. CRC press, 1994.
HongxiangFan,MartinFerianc,ZhiqiangQue,XinyuNiu,MiguelL.Rodrigues,andWayneLuk. Accelerating
Bayesian neural networks via algorithmic and hardware optimizations. IEEE Transactions on Parallel and
Distributed Systems, 33(12):3387–3399, 2022.
Di Feng, Christian Haase-Schütz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner
Wiesbeck, and Klaus Dietmayer. Deep multi-modal object detection and semantic segmentation for
autonomous driving: Datasets, methods, and challenges. IEEE Transactions on Intelligent Transportation
Systems, 22(3):1341–1360, 2020.
Halina Frydman, Edward I. Altman, and Duen-Li Kao. Introducing recursive partitioning for financial
classification: the case of financial distress. The Journal of Finance, 40(1):269–291, 1985.
Tilmann Gneiting and Adrian E. Raftery. Weather forecasting with ensemble methods. Science, 310:248 –
249, 2005.
Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration and
sharpness. Journal of the Royal Statistical Society Series B: Statistical Methodology, 69(2):243–268, 2007.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
Sebastian G. Gruber and Florian Buettner. Better uncertainty calibration via proper scores for classification
and beyond. In Advances in Neural Information Processing Systems, 2022.
Sebastian G. Gruber, Teodora Popordanoska, Aleksei Tiulpin, Florian Buettner, and Matthew B. Blaschko.
Consistentandasymptoticallyunbiasedestimationofpropercalibrationerrors. InInternational Conference
on Artificial Intelligence and Statistics, pp. 3466–3474, 2024.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In
International Conference on Machine Learning, pp. 1321–1330, 2017.
Ashim Gupta, Giorgi Kvernadze, and Vivek Srikumar. Bert & family eat word salad: Experiments with text
understanding. ArXiv, abs/2101.03453, 2021.
13ChiragGuptaandAadityaRamdas.Top-labelcalibrationandmulticlass-to-binaryreductions.InInternational
Conference on Learning Representations, 2022.
Sarah Haggenmüller, Roman C. Maron, Achim Hekler, Jochen S. Utikal, Catarina Barata, Raymond L.
Barnhill, Helmut Beltraminelli, Carola Berking, Brigid Betz-Stablein, Andreas Blum, Stephan A. Braun,
Richard Carr, Marc Combalia, Maria-Teresa Fernandez-Figueras, Gerardo Ferrara, Sylvie Fraitag, Lars E.
French, Frank F. Gellrich, Kamran Ghoreschi, Matthias Goebeler, Pascale Guitera, Holger A. Haenssle,
SebastianHaferkamp,LucieHeinzerling,MarkusV.Heppt,FranzJ.Hilke,SarahHobelsberger,DieterKrahl,
Heinz Kutzner, Aimilios Lallas, Konstantinos Liopyris, Mar Llamas-Velasco, Josep Malvehy, Friedegund
Meier,CorneliaS.L.Müller,AlexanderA.Navarini,CristiánNavarrete-Dechent,AntonioPerasole,Gabriela
Poch, Sebastian Podlipnik, Luis Requena, Veronica M. Rotemberg, Andrea Saggini, Omar P. Sangueza,
Carlos Santonja, Dirk Schadendorf, Bastian Schilling, Max Schlaak, Justin G. Schlager, Mildred Sergon,
Wiebke Sondermann, H. Peter Soyer, Hans Starz, Wilhelm Stolz, Esmeralda Vale, Wolfgang Weyers,
Alexander Zink, Eva Krieghoff-Henning, Jakob N. Kather, Christof von Kalle, Daniel B. Lipka, Stefan
Fröhling, Axel Hauschild, Harald Kittler, and Titus J. Brinker. Skin cancer classification via convolutional
neural networks: systematic review of studies involving human experts. European Journal of Cancer, 156:
202–216, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.
Achim Hekler, Titus J. Brinker, and Florian Buettner. Test time augmentation meets post-hoc calibration:
uncertainty quantificationunder real-world conditions. InProceedings of the AAAI Conference on Artificial
Intelligence, volume 37, pp. 14856–14864, 2023.
GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKilianQ.Weinberger.Denselyconnectedconvolutional
networks. In Conference on Computer Vision and Pattern Recognition, pp. 4700–4708, 2017.
Ashraful Islam, Chun-Fu Chen, Rameswar Panda, Leonid Karlinsky, Richard J. Radke, and Rogério Schmidt
Feris. Abroadstudyonthetransferabilityofvisualrepresentationswithcontrastivelearning. International
Conference on Computer Vision (ICCV), pp. 8825–8835, 2021.
Taejong Joo, Uijung Chung, and Minji Seo. Being Bayesian about categorical probability. In ICML, 2020.
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being Bayesian, even just a bit, fixes overconfidence
in relu networks. In ICML, 2020.
AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Master’sthesis, UniversityofToronto,
2009.
Volodymyr Kuleshov and Shachi Deshpande. Calibrated and sharp uncertainties in deep learning via density
estimation. In International Conference on Machine Learning, pp. 11683–11693, 2022.
Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter Flach. Beyond
temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. Advances
in Neural Information Processing Systems, 32:12316–12326, 2019.
Ananya Kumar, Percy Liang, and Tengyu Ma. Verified uncertainty calibration. In Advances on Neural
Information Processing Systems, pp. 3792–3803, 2019.
YannLeCun,LéonBottou,YoshuaBengio,andPatrickHaffner. Gradient-basedlearningappliedtodocument
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille,
JonathanHuang,andKevinMurphy. Progressiveneuralarchitecturesearch. InProceedings of the European
conference on computer vision (ECCV), pp. 19–34, 2018.
Charlie Marx, Sofian Zalouk, and Stefano Ermon. Calibration by distribution matching: Trainable kernel
calibration metrics. Advances in Neural Information Processing Systems, 36, 2024.
14Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Seungyeon Kim, and Sanjiv Kumar. A
statistical perspective on distillation. In ICML, 2021.
MatthiasMinderer,JosipDjolonga,RobRomijnders,FrancesHubis,XiaohuaZhai,NeilHoulsby,DustinTran,
and Mario Lucic. Revisiting the calibration of modern neural networks. Advances in Neural Information
Processing Systems, 34, 2021.
Pablo Morales-Álvarez, Daniel Hernández-Lobato, Rafael Molina, and José Miguel Hernández-Lobato.
Activation-level uncertainty in deep neural networks. In ICLR, 2021.
Carla D. Moravitz Martin and Charles F. Van Loan. Shifted kronecker product systems. SIAM Journal on
Matrix Analysis and Applications, 29(1):184–198, 2007.
Allan H. Murphy. A new vector partition of the probability score. Journal of Applied Meteorology and
Climatology, 12(4):595 – 600, 1973.
Allan H. Murphy and Robert L. Winkler. Reliability of subjective probability forecasts of precipitation and
temperature. Journal of the Royal Statistical Society. Series C (Applied Statistics), 26(1):41–47, 1977.
Kevin P. Murphy. Probabilistic Machine Learning: An Introduction. MIT Press, 2022.
Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities
using Bayesian binning. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,
pp. 2901–2907, 2015.
Jeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring
calibration in deep learning. In CVPR Workshops, volume 2, 2019.
Frédéric Ouimet and Raimon Tolosana-Delgado. Asymptotic properties of dirichlet kernel density estimators.
Journal of Multivariate Analysis, 187:104832, 2022.
JunhyungPark,UriShalit,BernhardSchölkopf,andKrikamolMuandet. Conditionaldistributionaltreatment
effect with kernel conditional mean embeddings and u-statistic regression. In International Conference on
Machine Learning, pp. 8401–8412, 2021.
Kanil Patel, William H. Beluch, Bin Yang, Michael Pfeiffer, and Dan Zhang. Multi-class uncertainty
calibration via mutual information maximization-based binning. In International Conference on Learning
Representations, 2021.
TeodoraPopordanoska,RaphaelSayer,andMatthewB.Blaschko. AconsistentanddifferentiableL canonical
p
calibration error estimator. In Advances in Neural Information Processing Systems, 2022a.
TeodoraPopordanoska,RaphaelSayer,andMatthewB.Blaschko. AconsistentanddifferentiableL canonical
p
calibration error estimator. In Advances in Neural Information Processing Systems, 2022b.
Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron Boots. Intra order-preserving
functionsforcalibrationofmulti-classneuralnetworks. Advances in Neural Information Processing Systems,
33:13456–13467, 2020.
Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, and Michael C. Mozer. Mitigating bias in calibration error
estimation. In International Conference on Artificial Intelligence and Statistics, pp. 4036–4054, 2022.
BernhardSchölkopfandAlexanderJ.Smola. Learningwith Kernels: Support Vector Machines, Regularization,
Optimization, and Beyond. MIT Press, 2002.
Jun Shao. Mathematical statistics. Springer Science & Business Media, 2003.
Michiel Stock, Tapio Pahikkala, Antti Airola, Bernard De Baets, and Willem Waegeman. A comparative
study of pairwise learning methods based on kernel ridge regression. Neural Computation, 30(8):2245–2283,
2018.
15Zeyu Sun, Dogyoon Song, and Alfred Hero. Minimum-risk recalibration of classifiers. Advances in Neural
Information Processing Systems, 36, 2024.
Gerald Teschl. Mathematical Methods in Quantum Mechanics, volume 157. American Mathematical Soc.,
2014.
JunjiaoTian,DylanYung,Yen-ChangHsu,andZsoltKira. Ageometricperspectivetowardsneuralcalibration
via sensitivity decomposition. In NeurIPS, 2021.
Christian Tomani, Sebastian G. Gruber, Muhammed Ebrar Erdem, Daniel Cremers, and Florian Buettner.
Post-hocuncertaintycalibrationfordomaindriftscenarios. InConference on Computer Vision and Pattern
Recognition (CVPR), pp. 10124–10132, June 2021.
Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas Schön.
Evaluating model calibration in classification. In International Conference on Artificial Intelligence and
Statistics, pp. 3459–3467, 2019.
Xiao Wang, Hongrui Liu, Chuan Shi, and Cheng Yang. Be confident! towards trustworthy graph neural
networks via confidence calibration. In NeurIPS, 2021.
David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classification: A
unifying framework. Advances in Neural Information Processing Systems, 32:12257–12267, 2019.
David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests beyond classification. In Interna-
tional Conference on Learning Representations, 2021.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability
estimates. In International Conference on Knowledge Discovery and Data Mining, pp. 694–699. Association
for Computing Machinery, 2002.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference,
2016.
16√
Table 4: Validation set square root risk Rˆ ×100 of TCE for different ImageNet models. Lower is better.
CE 2
The estimator TCEkde performs worse than the other estimators for DenseNet-161 and ResNet-152. For
2
Pnasnet-5, all estimators perform similar.
Model DenseNet-161 Resnet-152 Pnasnet-5
Estimator
TCE15−bins 12.17 ± 0.16 12.58 ± 0.14 10.63 ± 0.16
2
TCEbins 12.17 ± 0.16 12.57 ± 0.14 10.63 ± 0.16
2
TCEkde 12.31 ± 0.17 12.77 ± 0.14 10.63 ± 0.16
2
TCEkkr 12.17 ± 0.16 12.57 ± 0.14 10.63 ± 0.16
2
TCEukkr 12.17 ± 0.16 12.57 ± 0.14 10.63 ± 0.16
2
A Overview
Here, we first give additional experimental results in Appendix B. The missing proofs are located in
Appendix C.
B Extended Experiments
Additional details
We use the implementation of the calibration estimator function h given by the original authors (Popor-
kde
danoska et al., 2022b). For a small fraction of inputs, this implementation returns NaN as prediction. We
remove these instances from the risk and calibration estimation calculation of h , which has a neglectable
kde
effect.
As hyperparameter search spaces for the TCE experiments, we consider {5i∣i=1,...,20} for the number of
binsinh ,abandwidthin{10−5(i−1)/14−(1−(i−1)/14)) ∣i=1,...,15}∪ {0.2i∣i=1,...,5}fortheDirichletkernel
bin
of h according to Popordanoska et al. (2022a), a regularization constant λ ∈
{n0.510−2i+1
∣i=1,...,9}
kde
for h , and λ ∈
{n0.510−i
∣i=1,...,9} for h . For the CCE experiments, we consider the same set of
kkr ukkr
bandwidths for the Dirichlet kernel of h , a regularization constant
λ∈{n0.510−i+9
∣i=1,...,18} for h ,
kde kkr
and
λ∈{n0.510−0.5i+4.5
∣i=1,...,18} for h .
ukkr
Additional results
In the following, we discuss the risks and calibration estimations of some left-out cases from the main paper.
In Table 4 we show the risk of the top-label confidence calibration estimators for ImageNet with various
models. AllcalibrationestimationfunctionsshowsimilarriskexceptTCEkde,whichisworseforDenseNet-161
2
and Resnet-152. This is in agreement with Figure 2b, where the estimated calibration values are also mostly
similar. The risks in Table 5 for canonical calibration estimators in the case of CIFAR10 show slightly
different results: Here, the risk fails to distinguish the performance between the different estimators. Only
CCEkde outperforms the other approaches for Resnet-110.
2
In summary, the results mimic the ones in the main paper and it is not apparent which estimator to use in
practice without considering our proposed risk. However, the risk may be insensitive regarding the various
estimator performances.
17√
Table 5: Validation set risk Rˆ ×100 of CCE for CIFAR10 models. Lower is better. All estimators
CE 2
perform fairly similar.
Model LeNet-5 Densenet-40 ResNetWide-32 Resnet-110 Resnet-110 SD
Estimator
CCEkde 13.53 ± 0.27 5.13 ± 0.13 4.22 ± 0.16 4.46 ± 0.14 3.89 ± 0.2
2
CCEkkr 13.53 ± 0.27 5.13 ± 0.13 4.22 ± 0.16 4.47 ± 0.14 3.89 ± 0.2
2
CCEukkr 13.53 ± 0.27 5.13 ± 0.13 4.22 ± 0.16 4.47 ± 0.14 3.89 ± 0.2
2
(a) TCE ImageNet (b) CCE CIFAR10
2 2
Figure 4: Different calibration estimates of different models. Most calibration estimates approximately agree
with each other. This is in agreement with the similar risk values for each estimator in Table 4 and Table 5.
C Missing Proofs
Here, we present the missing proofs of the main part. Specifically, we prove Theorem 1 in Section C.1,
Theorem 2 in Section C.2, and various statements of Section 3.2 in Section C.3.
C.1 Proof for Theorem 1
We show that R (h)>R (h∗ ).
CE CE
For this, we require that ⟨p−P ,p′−P ⟩ is the unique minimizer of L (.,P ⊗P ;p,p′ ), which holds since
Y V CE Y V
∂ L (c,P ⊗P ;p,p′ )
∂c CE Y V
= ∂∂ cE Y,V [(c− ⟨p−e Y,p′−e V⟩)2 ] (33)
=2E [(c− ⟨p−e ,p′−e
⟩)]
Y,V Y V
=2(c− ⟨p−P ,p′−P ⟩),
Y V
and ∂2 L (c,P ⊗P ;p,p′ )>0.
∂2c CE Y V
18Based on the assumption that ∃A∈F with P (A)>0 we have
f(X) f(X)
∀p,p′ ∈A∶ h(p,p′ )≠h∗ (p,p′
)
⟺∀p,p′ ∈A∶L CE(h(p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )>L CE(h∗ (p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )
⟹ ∫ L CE(h(p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )d(P f(X)⊗P f(X))(p,p′ ) (34)
A×A
>∫ L CE(h∗ (p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )d(P f(X)⊗P f(X))(p,p′ ),
A×A
w
L
Ch Eer (e .,Pth Ye ∣f(i Xne )=q pu ⊗al Pit Yy ∣ff (Xol )l =o pw ′;s p,s pi ′n ).ce h∗ (p,p′ ) = ⟨p−P Y∣f(X)=p,p′−P Y∣f(X)=p′⟩ is the unique minimizer of
From the unique minimizer property also follows that for all B ∈F f(X)⊗f(X) holds
∫ L CE(h(p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )d(P f(X)⊗P f(X))(p,p′ )
B (35)
≥∫ L CE(h∗ (p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )d(P f(X)⊗P f(X))(p,p′ ).
B
Since (∆d×∆d )\(A×A)∈F f(X)⊗f(X), it holds
R (h)
CE
=E X,X′[L CE(h(p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )]
=∫ L CE(h(p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )d(P f(X)⊗P f(X))(p,p′ )
(∆d×∆d)\(A×A)
+∫ L CE(h(p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )d(P f(X)⊗P f(X))(p,p′ )
A×A
(36)
>∫ L CE(h∗ (p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )d(P f(X)⊗P f(X))(p,p′ )
(∆d×∆d)\(A×A)
+∫ L CE(h∗ (p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )d(P f(X)⊗P f(X))(p,p′ )
A×A
=E X,X′[L CE(h∗ (p,p′ ),P Y∣f(X)=p⊗P Y∣f(X)=p′;p,p′ )]
=R (h∗ ).
CE
C.2 Proof for Theorem 2
A sketch of the necessity of Theorem 2 is given in Figure 5, which also illustrates the proof.
Proof. We use P ≔ f(X) and f(p,p′ ) ≔ {(h(p,p′ )−h∗ (p,p′ ))2 , p,p′ ∈P Y for simplicity. It is continuous
0, else,
at every point in which h and h∗ are continuous. We denote with D the P -null set of p’s for which h and h∗
P
are not continuous at point (p,p). Then,
E [f(P,P)]
=∫ f(p,p)dP (p)
P
Rd
=∫ f(p,p)dP (p)
supp(P P) P (37)
=∫ f(p,p)dP (p)+∫ f(p,p)dP (p)
P P
intsupp(P P) bdsupp(P P)
=∫ f(p,p)dP (p)+∫ f(p,p)dP (p).
P P
intsupp(P P)\D bdsupp(P P)
19d
d
Figure5: Blueindicatesapossiblesupportsetsupp(P ⊗P )⊆∆d×∆d,andredlineindicates{(p,p)∣p∈∆d }.
P P
During training we optimize all blue dots and areas, but during testing we only evaluate their intersection
with the red line (a possible null set).
The last line holds since the support of a measure is closed, and, consequently, splitting it up into in-
terior and boundary does not add new ’mass’ (Teschl, 2014). For the following, denote with B(p,ϵ) ≔
{x∈Rd ∣∥x−p∥ <ϵ} the open (euclidean) ball with center p and radius ϵ. Further, since h and h∗ are
2
by assumption continuous in the points {(p,p)∣p∈P \D}, it follows that f is also continuous at these
Y
points, and, consequently, also lower semicontinuous. We first deal with the interior term by using this
lower-semicontinuous property giving
∫ f(p,p)dP (p)
P
intsupp(P P)\D
=∫ liminf f(p ,p )dP (p)
1 2 P
intsupp(P P)\D(p1,p2)→(p,p)
=lim∫ inf{f(p ,p )∣(p ,p )∈B((p,p),ϵ)\{(p,p)}}dP (p) (38)
1 2 1 2 P
ϵ→0 intsupp(P P)\D
≤lim∫ essinf{f(p ,p )∣(p ,p )∈B((p,p),ϵ)\{(p,p)}}dP (p).
1 2 1 2 P
ϵ→0 intsupp(P P)\D
Since p∈intsupp(P ), it holds P (B(p,ϵ/2)\{p})>0 for all ϵ>0 following the definition of int and supp
P P
(Teschl, 2014). Let Sq(p,ϵ)={x∈Rd ∣∥p−x∥∞ <ϵ} be the open hypercube with center p and side length 2ϵ.
√
It holds B(p,ϵ)⊂Sq(p,ϵ) and Sq(p, dϵ)⊂B(p,ϵ). Then
√
0<P (B(p, 2dϵ)\p)
P
√
≤P (Sq(p, 2dϵ)\{p})
P
√
√ √
= P ⊗P ((Sq(p, 2dϵ)\{p})× (Sq(p, 2dϵ)\{p}))
P P
√ (39)
√ √
≤ P ⊗P ((Sq(p, 2dϵ)×Sq(p, 2dϵ))\{(p,p)})
P P
√
√
= P ⊗P (Sq((p,p), 2dϵ)\{(p,p)})
P P
√
≤ P ⊗P (B((p,p),ϵ)\{(p,p)}).
P P
20Consequently, B((p,p),ϵ)\{(p,p)} is not a null set (w.r.t. P ⊗P ), and, thus,
P P
essinf{f(p ,p )∣(p ,p )∈B((p,p),ϵ)\{(p,p)}}
1 2 1 2
≤∫ f(p ,p )d(P ⊗P )(p ,p )
1 2 P P 1 2
B((p,p),ϵ)\{(p,p)}
(40)
≤∫ f(p ,p )d(P ⊗P )(p ,p )
1 2 P P 1 2
B((p,p),ϵ)
≤E [f(P,P′ )]=0,
where we used the given assumption
h(P,P′
)
a=.s. h∗ (P,P′
) in the last line. Continuing Equation (38) it
follows
lim∫ essinf{f(p 1,p 2)∣(p 1,p 2)∈B((p,p),ϵ)\{(p,p)}}dP
P
(p)=0.
(41)
ϵ→0 intsupp(P P)\D(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) =(cid:210)(cid:210)(cid:209) 0(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)
Next, we deal with the boundary of the support. Since we assume that it consists of at most countably
infinite elements with probability mass (which we denote as {p ,...}, it holds
1
∫ f(p,p)dP (p)
P
bdsupp(P P)
=∫ f(p,p)dP (p)
P
{p1,...}
= ∑ f(p,p)P (p)
P
p∈{p1,...}
√ √
= ∑ ∑ 1 p=p′f(p,p′ ) P
P
(p) P
P
(p′ )
p∈{p1,...}p′∈{p1,...} (42)
√ √
=∫ 1 p=p′f(p,p′ )d( P
P
⊗ P P)(p,p′ )
{p1,...}×{p1,...}
√ √
=∫ f(p,p′ )d( P ⊗ P )(p,p′ )
P P
{(p,p)∣p∈{p1,...}}
√ √
≤∫ f(p,p′ )d( P ⊗ P )(p,p′ )
P P
Rd
=0,
√ √
where the last line holds since for all A∈F P×P we have P P ⊗ P P (A)=0 ⟺ P P ⊗P P (A)=0.
From Equation (41) and Equation (42) follows that f(P,P′ )a=.s. 0 ⟹ f(P,P)a=.s. 0. Consequently, we have
h(P,P′ )a=.s. h∗ (P,P′ ) ⟹ h(P,P)a=.s. h∗ (P,P).
Remark. Note that any random variable with outcomes restricted to ∆d =
{(p ,...,p )⊺ ∈[0,1]d ∣∑d p =1} ⊂ Rd has a singular distribution, since ∆d is a null set with re-
1 d i=1 i
spect to the d dimensional Lebesgue measure λd. This would then fall outside of the conditions stated in
Theorem 2. However, we can circumvent this simply by transforming (bijectively) the outcome space to
∆d
r
= {(p 1,...,p d−1)⊺ ∈[0,1]d−1 ∣0≤∑ id =− 11p
i
≤1} ⊂ Rd−1, which has non-zero mass according to the d−1
dimensional Lebesgue measure
λd−1.
C.3 Proofs for Section 3.2
We give proofs of various statements of Section 3.2.
21Proof for Equation (27)
Instead of proving the whole kernel ridge regression approach end-to-end, we bring Equation (27) into the
formofanordinarykernelridgeregressionobjectiveandthenshowthatoursolutioninEquation(28)matches
the ordinary solution.
For this, define Y˜
i
≔ vec i(∆⊺ Yf(X)∆ Yf(X)) = ⟨f(X imodn+1)−e Yimodn+1,f(X ⌈i/n⌉)−e
Y
⌈i/n⌉⟩ ∈ R and F˜
i
≔
(f(X imodn+1),f(X ⌈i/n⌉)) ∈ ∆d ×∆d with i = 1...n2, and h˜ (p˜) ≔ h(p˜ 1,p˜ 2) for p˜ ∈ ∆d ×∆d, as well as
H˜ ≔H ⊗H with ϕ˜ (p˜)≔(ϕ⊗ϕ)(p˜ ,p˜ ).
1 2
Then, we can write Equation (27) as
n n
Rˆ CE,λ(g)= n1
2
∑∑ (⟨f(X i)−e Yi,f(X j)−e Yj⟩−h(f(X j),f(X j)))2+λ∥g∥2
H⊗H
i=1j=1
(43)
n2
= n1
2
∑ (Y˜ i− ⟨g˜,ϕ˜ (F˜ i)⟩ H˜)2+λ∥g˜∥2 H˜,
i=1
which is ordinary kernel ridge regression in the last line (Bach, 2024). Bach (2024) shows its unique minimum
is reached under certain assumptions if
g˜=(Y˜ 1,...,Y˜ n2)(K˜ f(X)+λn2 I)−1 (ϕ˜ (F˜ 1),...,ϕ˜ (F˜ n2))⊺ (44)
with [K˜ f(X)] ij = ⟨ϕ˜ (F˜ i),ϕ˜ (F˜ j)⟩ H˜. Now, to reach our solution, note that it holds ⟨ϕ˜ (F˜ i),ϕ˜ (p˜)⟩ H˜ =
k(f(X imodn+1),p˜ 1)k(f(X ⌈i/n⌉),p˜ 2) and K˜
f(X)
=K f(X)⊗K f(X), which gives
⟨g˜,ϕ˜ (p˜)⟩
H˜
=(Y˜ 1,...,Y˜ n2)(K˜ f(X)+λn2 I)−1 (k(f(X 1modn+1),p˜ 1)k(f(X ⌈1/n⌉),p˜ 2),...,k(f(X n2modn+1),p˜ 1)k(f(X ⌈n2/n⌉),p˜ 2))⊺
=vec(∆⊺ ∆ )(K ⊗K +λn2 I)−1 (k (p˜ )⊗k (p˜ ))⊺ .
Yf(X) Yf(X) f(X) f(X) f(X) 1 f(X) 2
(45)
The last line is the predictor we stated in Equation (28).
Proof for Equation (29)
⊺
By definition of h and by using the eigenvalue decomposition K =Q Λ Q , we have
kkr f(X) f(X) f(X) f(X)
′
h (p,p)
kkr
≔vec⊺ (∆⊺ ∆ )(K ⊗K +λn2 I)−1 (k (p)⊗k (p′ ))
Yf(X) Yf(X) f(X) f(X) f(X) f(X)
=vec⊺ (∆⊺ ∆ )(Q ⊗Q )(Λ ⊗Λ +λn2 I)−1 (Q⊺ ⊗Q⊺ )(k (p)⊗k (p′ ))
Yf(X) Yf(X) f(X) f(X) f(X) f(X) f(X) f(X) f(X) f(X)
=((Q⊺ ⊗Q⊺ )vec(∆⊺ ∆ ))⊺ (Λ ⊗Λ +λn2 I)−1 (Q⊺ ⊗Q⊺ )(k (p)⊗k (p′ )).
f(X) f(X) Yf(X) Yf(X) f(X) f(X) f(X) f(X) f(X) f(X)
(46)
Note it holds that
(A⊗B)vec(C)=vec(BCA⊺
) for matrices A,B,C and
vec⊺ (A)vec(B)=tr(A⊺
B).
Then, with the Hadamard product ⊙ and Λ˜ ∈Rn×n with [Λ˜ ] ≔ 1 , we have
X f(X) ij (Λf(X)) ii(Λf(X)) jj+λn2
22((Q⊺ ⊗Q⊺ )vec(∆⊺ ∆ ))⊺ (Λ ⊗Λ +λn2 I)−1 (Q⊺ ⊗Q⊺ )(k (p)⊗k (p′ ))
f(X) f(X) Yf(X) Yf(X) f(X) f(X) f(X) f(X) f(X) f(X)
n2
=∑vec (Q⊺ ∆⊺ ∆ Q )vec (Q⊺ k (p)k⊺ (p′ )Q )[Λ˜ ]
i f(X) Yf(X) Yf(X) f(X) i f(X) f(X) f(X) f(X) f(X) ij (47)
i=1
=tr(Q⊺ k (p)k⊺ (p′ )Q (Λ˜ ⊙Q⊺ ∆⊺ ∆ Q ))
f(X) f(X) f(X) f(X) f(X) f(X) Yf(X) Yf(X) f(X)
=k⊺ (p′ )Q (Λ˜ ⊙Q⊺ ∆⊺ ∆ Q )Q⊺ k (p),
f(X) f(X) f(X) f(X) Yf(X) Yf(X) f(X) f(X) f(X)
which shows Equation (29).
Proof for Equation (30)
Given the definition of Hkkr we have
H ik jkr =[K⊺ f(X)f(X′)Q f(X)(Λ˜ f(X)⊙Q⊺ f(X)∆⊺ Yf(X)∆ Yf(X)Q f(X))Q⊺ f(X)K f(X)f(X′) ∈Rn′×n′ ]
ij (48)
=k⊺ (f(X′ ))Q (Λ˜ ⊙Q⊺ ∆⊺ ∆ Q )Q⊺ k (f(X′ ))∈Rn′×n′ ,
f(X) i f(X) f(X) f(X) Yf(X) Yf(X) f(X) f(X) f(X) j
′ ′
where the last line follows since [k f(X)(f(X j))]
i
=k(f(X i),f(X j))=[K f(X)f(X′)] ij.
23