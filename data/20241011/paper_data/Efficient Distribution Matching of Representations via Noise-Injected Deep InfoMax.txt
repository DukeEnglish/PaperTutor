Preprint. Underreview.
EFFICIENT DISTRIBUTION MATCHING OF REPRESEN-
TATIONS VIA NOISE-INJECTED DEEP INFOMAX
IvanButakov∗∗,1,2,AlexanderSememenko1,AlexanderTolmachev1,2,AndreyGladkov1,
MarinaMunkhoeva3,AlexeyFrolov1
1SkolkovoInstituteofScienceandTechnology;2MoscowInstituteofPhysicsandTechnology;
3ArtificialIntelligenceResearchInstitute;
{butakov.id, semenenko.av, tolmachev.ad, gladkov.ao}@phystech.edu,
munkhoeva@airi.net,al.frolov@skoltech.ru
ABSTRACT
DeepInfoMax(DIM)isawell-establishedmethodforself-supervisedrepresenta-
tionlearning(SSRL)basedonmaximizationofthemutualinformationbetween
theinputandtheoutputofadeepneuralnetworkencoder. DespitetheDIMand
contrastiveSSRLingeneralbeingwell-explored,thetaskoflearningrepresenta-
tionsconformingtoaspecificdistribution(i.e.,distributionmatching,DM)isstill
under-addressed.MotivatedbytheimportanceofDMtoseveraldownstreamtasks
(including generative modeling, disentanglement, outliers detection and other),
we enhance DIM to enable automatic matching of learned representations to a
selectedpriordistribution. Toachievethis, weproposeinjectinganindependent
noiseintothenormalizedoutputsoftheencoder,whilekeepingthesameInfoMax
trainingobjective. Weshowthatsuchmodificationallowsforlearninguniformly
and normally distributed representations, as well as representations of other ab-
solutelycontinuousdistributions. Ourapproachistestedonvariousdownstream
tasks. The results indicate a moderate trade-off between the performance on the
downstreamtasksandqualityofDM.
1 INTRODUCTION
Learningviablelow-dimensionalrepresentationsofcomplexdataplaysanimportantroleinmany
modern applications of artificial intelligence. This task arises in various domains, including im-
age (Haralick et al., 1973; Chen et al., 2020b; Rombach et al., 2022), audio (van den Oord et al.,
2019), and natural language processing (Mikolov et al., 2013; Radford et al., 2018; Devlin et al.,
2019). High-quality embeddings are particularly useful for multi-modal methods (Vinyals et al.,
2015;Radfordetal.,2021;Ho&Salimans,2021),statisticalandtopologicalanalysis(Mooretal.,
2020;Duong&Nguyen,2022;Butakovetal.,2024b), datavisualization(vanderMaaten&Hin-
ton,2008;McInnesetal.,2018), andtestingfundamentalhypotheses(Brownetal.,2023;Gurnee
&Tegmark,2024;Huhetal.,2024).
Existingapproachestorepresentationlearningcanbedividedintothreecategories(Ericssonetal.,
2022):supervised(requireslabeleddata),self-supervisedandunsupervised(nolabelingisrequired).
Inpractice,accesstolabeleddataislimited,whichhinderstheuseofsupervisedapproaches.There-
fore, unsupervised and self-supervised methods are of great importance. Contrastive learning is a
well-establishedparadigmofself-supervisedrepresentationlearning(SSRL),whichencouragesan
encodertolearnsimilarrepresentationsforvariousaugmentationsofthesamedatapoint,anddis-
similar–fordifferentdatapoints. DeepInfoMax(DIM)(Hjelmetal.,2019)leveragesinformation-
theoreticquantitiestoconstructadecentcontrastiveobjective,involvingadirectmaximizationofthe
usefulinformationcontainedintheembeddings. DIMisuniversal, flexible, anddeeplyconnected
totherigorousinformationtheory,whichallowsforagoodperformanceonavarietyofdownstream
tasks to be attained (Hjelm et al., 2019; Bachman et al., 2019; Velicˇkovic´ et al., 2019; Tschannen
etal.,2020;Yu,2024).
∗Correspondencetobutakov.id@phystech.edu
1
4202
tcO
9
]GL.sc[
1v39960.0142:viXraPreprint. Underreview.
Acquiringembeddingsadmittingaspecificdistribution(i.e.,distributionmatching,DM)isanaux-
iliary,yetimportanttaskinrepresentationlearning. Latentdistributionswithstraightforwardsam-
plingproceduresortractabledensitiesarecrucialfordownstreamgenerativemodeling(Kingma&
Welling,2014;Makhzanietal.,2016;Larsenetal.,2016;Papamakariosetal.,2021). Additionally,
specificdistributions(e.g.,Gaussian)exhibitproperties,whichareusefulforstatisticalanalysis(Tip-
ping&Bishop,1999;Duong&Nguyen,2022),disentanglement(Higginsetal.,2017;Balabinetal.,
2024),andoutliersdetection.
A classical approach to latent DM is to optimize a cheap and imprecise distribution dissimilarity
measureduringthetrainingorarchitecturesearch(Ng,2011;Makhzani&Frey,2014;Kingma&
Welling, 2014; Heusel et al., 2017; Higgins et al., 2017). Methods of this family have to rely on
several strong assumptions, such as embeddings already admitting a Gaussian distribution, which
eventually leads to suboptimal results. Another common approach employs adversarial networks
to push the learned representations towards a desired distribution (Makhzani et al., 2016; Hjelm
et al., 2019). One can also leverage generative models (e.g., normalizing flows or diffusion mod-
els) to explicitly perform DM “post hoc” (Bo¨hm & Seljak, 2022; Rombach et al., 2022). These
twoapproachesyielddecentresults,butrequiresupplementarynetworkstomatchthedistributions.
Finally,injectivelikelihood-basedmodelscanalsobeusedtoacquirelow-dimensionalrepresenta-
tionsadmittingadesireddistribution(Brehmer&Cranmer,2020;Sorrensonetal.,2024). However,
likelihoodmaximizationacrossdimensionsisnotoriouslyproblematicduetonon-squareJacobima-
trices.
Incontrasttothementionedapproaches,weproposeasimple,cost-effectiveandnon-intrusivemodi-
ficationtoDIM,whichallowsforanautomaticandexactDMoftherepresentations.Inthefollowing
text,weshowthatusingspecificactivationfunctionsandnoiseinjectionsattheoutletofanencoder,
combinedwiththeDIMobjective,allowsfornormallyanduniformlydistributedrepresentationsto
belearned. Ourcontributionsarethefollowing:
1. Weprovethatapplyingnormalizationandaddingasmallnoiseattheoutletofanencoder
andmaximizingtheDIMobjectiveminimizestheKullback-Leiblerdivergencebetweena
Gaussian(oruniform)distributionandthedistributionofembeddings.
2. Weconductexperimentsonseveraldownstreamtaskstoexplorethetrade-offbetweenthe
downstreamperformanceandtheaccuracyofDMviaourmethod.
3. WeconductadditionalexperimentstoassessthequalityofDMviaourmethodinthetasks
ofgenerativemodelling.
Thepaperisorganizedasfollows. InSection2,thenecessarybackgroundfrominformationtheory
andoriginalworksonDeepInfoMaxisprovided. InSection3,wedescribethegeneralmethodfor
DMviamodifiedDIM.InSection4,aconnectionbetweentheproposedapproachandotherSSRL
methodsisestablished.Section5isdedicatedtotheexperimentalevaluationofourmethod.Finally,
weconcludethepaperbydiscussingourresultsinSection6. Completeproofs,additionalanalysis
ofinfomax-basedDM,andtechnicaldetailsareprovidedinAppendicesAtoCcorrespondingly.
2 BACKGROUND
Inthissection,thebackgroundnecessarytounderstandourworkisprovided.Westartwiththebasic
definitionsfromtheinformationtheory. Then,themaximumentropytheoremsaregiven,whichare
crucial for understanding our approach. Finally, we outline the general variant of Deep InfoMax
representationlearningmethod,whichweaimtoenhancewithanautomaticdistributionmatching.
2.1 PRELIMINARIES
Let(Ω,F,P)beaprobabilityspacewithsamplespaceΩ,σ-algebraF,andprobabilitymeasureP
definedonF. ConsideranabsolutelycontinuousrandomvectorX: Ω → Rd withtheprobability
densityfunction(PDF)denotedasp(x). ThedifferentialentropyofX isdefinedasfollows:
(cid:90)
h(X)=−Elogp(X)=− p(x)logp(x)dx,
suppX
2Preprint. Underreview.
wheresuppX ⊆ Rn representsthesupport ofX,andlog(·)denotesthenaturallogarithm. Sim-
ilarly,wedefinethejointdifferentialentropyash(X,Y) = −Elogp(x,y)andconditionaldiffer-
entialentropyash(X | Y) = −Elogp(X|Y) = −E (cid:0)E logp(X |Y)(cid:1) . Finally,themutual
Y X|Y
information(MI)isgivenbyI(X;Y)=h(X)−h(X |Y),andthefollowingequivalenceshold
I(X;Y)=h(X)−h(X |Y)=h(Y)−h(Y |X),
I(X;Y)=h(X)+h(Y)−h(X,Y),
I(X;Y)=D (P ∥ P ⊗P ).
KL X,Y X Y
Mutualinformationcanalsobedefinedasanexpectationofthepointwisemutualinformation:
(cid:20) (cid:21)
p(x|y)
PMI (x,y)=log , I(X;Y)=EPMI (X,Y). (1)
X,Y p(x) X,Y
TheabovedefinitionscanbegeneralizedviaRadon-Nikodymderivativesandinduceddensitiesin
caseofdistributionssupportsbeingmanifolds,see(Spivak,1965). Weuseflexiblenotation,where
entropyanddivergencemayrefertorandomvectorsortheircorrespondingdistributions.
OurworkleveragesthemaximumentropypropertiesofGaussiananduniformdistributions:
Theorem 2.1 (Theorem 8.6.5 in Cover & Thomas (2006)). Let X be a d-dimensional absolutely
continuous random vector with probability density function p, mean m and covariance matrix Σ.
Then
h(X)=h(N(m,Σ))−D (p∥N(m,Σ)), h(N(m,Σ))=
1 log(cid:0) (2πe)ddetΣ(cid:1)
,
KL 2
whereN(m,Σ)isaGaussiandistributionofmeanmandcovariancematrixΣ.
Theorem2.2. LetX beanabsolutelycontinuousrandomvectorwithprobabilitydensityfunction
pandsuppX ⊆S,whereS hasfiniteandnon-zeroLebesguemeasureµ(S). Then
h(X)=h(U(S))−D (p∥U(S)), h(U(S))=logµ(S),
KL
whereU(S)isauniformdistributiononS.
Remark2.3. Notethath(X)̸=h(X′)−D (p ∥p )ingeneral.
KL X X′
Finally,wealsoutilizethefollowinglowerboundontheconditionalentropyofasumoftwocondi-
tionallyindependentrandomvectors:
Lemma 2.4. Let X and Z be random vectors of the same dimensionality, independent under the
conditioningvectorY. Then
h(X+Z |Y)=h(Z |Y)+I(X;X+Z |Y)≥h(Z |Y),
withequalityifandonlyifthereexistsameasurablefunctiongsuchthatX =g(Y).1
2.2 DEEPINFOMAX
Mutualinformation(MI)iswidelyconsideredasafundamentalmeasureofstatisticaldependence
betweenrandomvariablesduetoitskeyproperties,suchasinvarianceunderdiffeomorphisms,sub-
additivity,non-negativity,andsymmetry(Cover&Thomas,2006).TheseattributesmakeMIpartic-
ularlyusefulininformation-theoreticapproachestomachinelearning. Theconceptofmaximizing
MI between input and output, known as the infomax principle (Linsker, 1988; Bell & Sejnowski,
1995),servesasthefoundationforDeepInfoMax(DIM),afamilyofself-supervisedrepresentation
learningmethodsproposedbyHjelmetal.(2019).
Ana¨ıveDIMapproachsuggestslearningthemostinformativeembeddingsviaadirectmaximization
ofthemutualinformationbetweentheoriginaldataandcompressedrepresentations:
I(X;f(X))→max,
1Hereinafter,whencomparingrandomvariables,wemeanequalityorinequality“almostsure”.
3Preprint. Underreview.
where X is the random vector to be compressed, and f is the encoding mapping (being learned).
However, despite its simplicity and intuitiveness, this setting is rendered useless by the fact that
I(X;f(X))=∞forawiderangeofX andf (Bell&Sejnowski,1995;Hjelmetal.,2019).
Toavoidthislimitation,itisusuallysuggestedtoaugment(croprandomly,addnoise,etc.) original
datatoinjectstochasticityandmakeinformation-theoreticobjectivesnon-degenerate. Considerthe
followingMarkovchain:
f(X)−→X −→X′,
where X′ is augmented data. Now I(X′;f(X)) can be made non-infinite. Moreover, according
to the data processing inequality (Cover & Thomas, 2006), I(X′;f(X)) ≤ I(X;f(X)), which
connectsthisnewinfomaxobjectivewiththena¨ıveone.
TheonlyremainingproblemisthehighdimensionalityofX′,whichmakesMIestimationdifficult.
Toresolvethis,onecanapplyanadditional(ingeneral,random)dimensionality-reducingtransfor-
mationtoreplaceX′withY′:
f(X)−→X −→X′ −→Y′,
withI(Y′;f(X)) ≤ I(X′;f(X))beingthenewinfomaxobjective. Forthesakeofcomputational
simplicity,Y′canbedefinedasf(X′);thus,thesameencodernetworkisusedtocompressboththe
original and augmented data. Moreover, one can view I(f(X′);f(X)) as a contrastive objective:
this value is high when f yields similar representations for corresponding augmented and non-
augmentedsamples,anddissimilarforotherpairsofsamples.
Now, let us consider the task of mutual information maximization. As MI is notoriously hard to
estimate(McAllester&Stratos,2020),lowerboundsarewidelyusedtoreparametrizetheinfomax
objective(Belghazietal.,2021;vandenOordetal.,2019;Hjelmetal.,2019). Inthiswork,weonly
considerthetwomostpopularapproachesbasedonthevariationalrepresentationsoftheKullback-
Leiblerdivergence:
Donsker-Varadhan
(Donsk Be er l& ghV aza ira ed th aa l.n 21 09 28 13
)
I(X;Y)= T:su Ωp →R(cid:2)E P
X,Y
T −logE P X⊗P
Y
exp(T)(cid:3) (2)
Nguyen-Wainwright-Jordan
B(N elg gu hy ae zn iee tt aa ll .. 22 00 21 10 ), I(X;Y)= T:su Ωp →R(cid:2)E P
X,Y
T −E P X⊗P
Y
exp(T −1)(cid:3) (3)
whereΩisthesamplingspace,andT isameasurablecriticfunction.
Remark2.5. AssumingPMI exists,thesupremumin(2)and(3)isattainedatandonlyat
X,Y
T∗ =T∗(x,y)=PMI (x,y)+α (a.s. w.r.t. P ), (4)
X,Y X,Y
whereα =1for(3)andcanbeanyrealnumberfor(2)(followsfromTheorem1inBelghazietal.
(2021)andTheorem2.1inKeziou(2003)).
In practice, f and T are approximated via corresponding neural networks, with the parameters
being learned through the maximization of the Monte-Carlo estimate of (2) or (3). Although (2)
usually yields better estimates, the SGD gradients of this expression are biased in a mini-batch
setting(Belghazietal.,2021).
3 DEEP INFOMAX WITH AUTOMATIC DISTRIBUTION MATCHING
Inthepresentsection,wemodifyDIMtoenableautomaticdistributionmatching(DM)ofrepresen-
tations. Specifically, wepropose adding independentnoise Z to normalizedrepresentations of X,
thusreplacingf(X)byf(X)+Z intheinfomaxobjectivefromSection2.2. Thiscorrespondsto
thefollowingMarkovchain:
f(X)+Z −→f(X)−→X −→X′ −→f(X′).
By the data processing inequality, we know that I(f(X′);f(X)+Z) ≤ I(f(X′);f(X)), which
connectsourmethodtothefamilyofconventionalDIMapproachesdiscussedpreviously.
As we will demonstrate, the noise injection at the outlet of an encoder, in conjunction with The-
orems 2.1 and 2.2 on maximum entropy, allows us to achieve normally or uniformly distributed
4Preprint. Underreview.
embeddings. Toprovethis,weusethefollowingLemma3.1,whichdecomposestheproposedin-
fomaxobjectiveintothreecomponents: theentropyofrepresentationswithanadditivenoiseZ,the
entropyofthenoiseitself,andthemutualinformationbetweenf(X)andf(X)+Z conditionedon
f(X′)–representationsofaugmenteddata.
Lemma3.1. ConsiderthefollowingMarkovchainofabsolutelycontinuousrandomvectors:
f(X)+Z −→X −→X′ −→f(X′),
withZ beingindependentof(X,X′). Then
I(f(X′);f(X)+Z)=h(f(X)+Z)−h(Z)−I(f(X)+Z;f(X)|f(X′)).
Lemma3.1highlightstheimportanceofbothadditivenoiseandinputdataaugmentation. Specifi-
cally,ifnoaugmentationisapplied,i.e.,X = X′,thenI(f(X)+Z;f(X) | f(X′)) = 0. Inthis
case,maximizingtheinfomaxobjectiveisreducedtomaximizingtheentropyh(f(X)+Z). Under
thecorrespondingrestrictionsonf(X),thisisequivalenttodistributionmatching,seeTheorems2.1
and2.2.
However,DMalonedoesnotguaranteethatthelearnedrepresentationswillbemeaningfuloruseful
fordownstreamtasks.ThatiswhywealsoshowthatusingX ̸=X′allowsustorecovermeaningful
embeddings,asI(f(X)+Z;f(X)|f(X′))issettozerobylearningrepresentationsthatareweakly
invarianttoselecteddataaugmentations:
Definition3.2. Wecallanencodingmappingf weaklyinvarianttodataaugmentationX → X′ if
thereexistsafunctiongsuchthatf(X)=g(f(X))=g(f(X′))almostsurely.
Lemma3.3. UndertheconditionsofLemma3.1,letP(X = X′ | X) ≥ α > 0. Then,I(f(X)+
Z;f(X)|f(X′))=0preciselywhenf isweaklyinvarianttoX →X′.
Inwhatfollows,weformalizetheprovidedreasoningbyaddressingGaussiananduniformdistribu-
tionmatchingseparately. WealsobrieflydiscusshowageneralDMproblemcanbereducedtothe
normaloruniformone.
3.1 GAUSSIANDISTRIBUTIONMATCHING
LetusassumeZhavingfinitesecond-ordermoments.AccordingtoTheorem2.1,ifwerestrictf(X)
byfixingitscovariancematrix,h(f(X)+Z)attainsitsmaximalvaluepreciselywhenf(X)+Z is
distributednormally. Now,asZ isindependentofX,f(X)+Z isnormallydistributedifandonly
ifthedistributionoff(X)isalsoGaussian.
Thus, one can achieve the normality of f(X) via restricting second-order moments of f(X) and
maximizing the entropy h(f(X) + Z). In combination with Lemma 3.1, this forms a basis of
the proposed method for Gaussian distribution matching. Moreover, we show that the imposed
restrictionscanbepartiallyliftedviaonlyrequiringVar(f(X) )=1foreveryi∈{1,...,d}. This
i
approachispreferableinpractice,asthewidely-usedbatchnormalization(Ioffe&Szegedy,2015)
canbeemployedtorestrictthevariances. Weformalizeourfindingsinthefollowingtheorem.
Theorem3.4(Gaussiandistributionmatching). LettheconditionsofLemma3.3besatisfied. As-
sumeZ ∼ N(0,σ2I),Ef(X) = 0andVar(f(X) ) = 1foralli ∈ {1,...,d}. Then,themutual
i
informationI(f(X′);f(X)+Z)canbeupperboundedasfollows
(cid:18) (cid:19)
d 1
I(f(X′);f(X)+Z)≤ log 1+ , (5)
2 σ2
withtheequalityholdingexactlywhenf isweaklyinvariantandf(X)∼N(0,I). Moreover,
D (f(X)∥N(0,I))≤I(Z;f(X)+Z)−I(f(X′);f(X)+Z)−dlogσ.
KL
WhilethefirstinequalityinTheorem3.4suggeststhatexactdistributionmatchingisachievedonly
asymptotically, thesecondexpressionshowsthatD (f(X)∥N(0,I))canbeboundedusingthe
KL
value of the proposed infomax objective. Note that I(Z;f(X)+Z) is typically upper bounded,
excepttherarecaseswherethesupportoff(X)isdegenerate.
5Preprint. Underreview.
3.2 UNIFORMDISTRIBUTIONMATCHING
Now consider Z distributed uniformly on [−ε;ε]d and suppf(X) ⊆ [0;1]d. We can apply rea-
soning, similar to the Gaussian case, in conjunction with Theorem 2.2, to perform the uniform
distribution matching. However, unlike the previous case, the desired result can only be achieved
asymptotically (with ε → 0), as the sum of independent uniform random variables is no longer
uniform. ThesedetailsareoutlinedinTheorem3.5.
Theorem 3.5 (Uniform distribution matching). Under the conditions of Lemma 3.1, let Z ∼
U([−ε;ε]d) and suppf(X) ⊆ [0;1]d. Then, the mutual information I(f(X′);f(X) + Z) can
beupperboundedasfollows
(cid:18) (cid:19)
1
I(f(X′);f(X)+Z)≤dlog 1+ . (6)
2ε
with the equality if and only if 1/ε ∈ N, f is weakly invariant, and f(X) ∼ U(A), where the set
A={0,2ε,4ε,...,1}contains(1/(2ε)+1)elements.
Moreover,
D (cid:0) f(X)∥U([0;1]d)(cid:1) ≤I(Z;f(X)+Z)−I(f(X′);f(X)+Z)−dlog(2ε).
KL
In sharp contrast to Theorem 3.4, the equality in (6) is attained at f(X) conforming to a discrete
distribution. ThismakestheproposedobjectivelessattractiveincomparisontotheGaussiandistri-
butionmatching.However,onestillcanbeassuredthatthestandardcontinuousuniformdistribution
allowsustoapproachtheequalityin(6)withεapproachingzero:
Remark3.6(Butakovetal.2024a). Iff(X)∼U([0;1]d),f isweaklyinvariant,andε<1/2,then
I(f(X′);f(X)+Z)=d(ε−log(2ε))=dlog(1+1/(2ε))−(log(1+2ε)−ε).
(cid:124) (cid:123)(cid:122) (cid:125)
o(ε)
3.3 GENERALCASE
Our approach can be extended to a wide range of desired distributions of embeddings using the
probability integral transform (David & Johnson, 1948; Chen & Gopinath, 2000). Normalizing
flows (learnable diffeomorphisms) can also be leveraged to transform the distribution “post-hoc”
duetotheiruniversalityproperty(Huangetal.,2018;Jainietal.,2019;Kobyzevetal.,2020).
ThedataprocessinginequalitycanbeusedtoestimatethequalityofDMafterthetransformation:
Statement 3.7 (Corollary 2.18 in (Polyanskiy & Wu, 2024)). Let g be a measurable (w.r.t P and
Q)function. ThenD (cid:0)P◦g−1∥ Q◦g−1(cid:1) ≤ D (P ∥ Q),whereP◦g−1 andP◦g−1 denotethe
KL KL
push-forwardmeasuresofPandQafterapplyingg.
4 CONNECTION TO OTHER METHODS FOR SSRL
Inthissection,weexploretherelationofourapproachtoothermethodsforunsupervisedandself-
supervisedrepresentationlearning. Wearguethattheproposedtechniquefordistributionmatching
canbeappliedtoanyothermethod,giventhelattercanbeformulatedintermsofmutualinformation
maximization.
Autoencoders Inreconstruction-basedgenerativemodels(Makhzanietal.,2016),thereconstruc-
tionerroriscloselytiedtothemutualinformation(Theorem8.6.6in(Cover&Thomas,2006)):
d 1
I(X;Y)=h(X)−h(X |Y)≥h(X)− log(2πe)− logE[∥X−Xˆ(Y)∥2].
2 2
Here, X denotes the input, Y the latent representation produced by the encoder, and Xˆ(Y) the
reconstructed input. The last term involving the expected squared error2 is essentially the autoen-
coder’slossfunction. Byminimizingthisreconstructionloss, themutualinformationbetweenthe
inputdataanditsrepresentationismaximized. However,recallthatI(X;Y)candivergetoinfinity,
asdiscussedearlier,soitiscrucialtointroduceaugmentations.
2Hereandthroughout,∥·∥denotestheEuclideannorm.
6Preprint. Underreview.
InfoNCE Many conventional methods for self-supervised representation learning leverage
InfoNCEloss(vandenOordetal.,2019)inconjunctionwithasimilaritymeasureT(·, ·)tofor-
mulate the following contrastive objective (Tschannen et al., 2020; He et al., 2020; Chen et al.,
2020a):
eT(q,k+)
L =−[E T(Q,K)−logE exp(T(Q,K))], Lˆ =−log ,
InfoNCE P+ P− InfoNCE K1 (cid:80)K i=1eT(q,k i−)
where P+ and P− denote the distributions of positive and negative pairs of keys K and queries
Q correspondingly. By selecting P+ = P and P− = P ⊗P , we recover
f(X′),f(X)+Z f(X′) f(X)+Z
the Donsker-Varadhan bound (2) for our infomax objective I(f(X′);f(X)+Z). However, note
that in (2) and (3) the supremum is taken over all measurable functions. In contrast, non-infomax
methodstypicallyemployaseparablecritic: T(q,k)=⟨ϕ(q),ψ(k)⟩,whereϕandψareprojection
heads (Tschannen et al., 2020; Chen et al., 2020a). In special cases ϕ,ψ = Id, so similarity of
representationsismeasuredviaaplaindotproduct.
Despitethissignificantdifference,ourdistributionmatchingparadigmallowsustodropthesupre-
mum in (2) and (3) and establish a direct connection between DIM and traditional non-infomax
contrastiveSSRLmethods:
Theorem4.1(DualformofGaussiandistributionmatching). UndertheconditionsofTheorem3.4,
(cid:104) (cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
I(f(X′);f(X)+Z)≥E T∗ −logE exp T∗ ,
P+ N(0,σ2I) P− N(0,σ2I)
∥y∥2 ∥y−x∥2 1 (cid:18) ∥x∥2+∥y∥2/(1+σ2)(cid:19)
T∗ (x,y)= − = ⟨x,y⟩− ,
N(0,σ2I) 2(1+σ2) 2σ2 σ2 2
withtheequalityholdingpreciselywhenf isweaklyinvariantandf(X)∼N(0,I).
Note that ⟨x,y⟩ is widely used as a similarity measure, σ2 can be interpreted as temperature, and
theremainingpartoftheexpressionservesasaregularizationterm.
Covariance-based methods Barlow Twins (Zbontar et al., 2021) and VICReg (Bardes et al.,
2021) objective functions can be traced to information-theoretic terms either through Information
Bottleneck(Tishbyetal.,1999)ormulti-viewInfoMax(Federicietal.,2020)principles:
h(f(X′)|X)−λh(f(X′))→min (BarlowTwins)
I(f(X′);X′)≥h(f(X′))+E[logq(f(X′)|X′′)]→max (VICReg)
where λ > 0, X → X′′ is a separate augmentation path, independent of X → X′, and q is a
probabilitydensityfunctionofsomedistribution.Inbothcases,therepresentationentropyh(f(X′))
comesintoplay. Recallthatnormaldistributionisthemaximumentropydistributiongivenfirsttwo
moments (Theorem 2.1). As both methods employ covariance restriction terms in the respective
objectivefunctions,onecanrecoverthesemethodsbyassumingthenormalityoff(X′).
5 EXPERIMENTS
Inthissection,weevaluateourdistributionmatchingapproachonseveraldatasetsanddownstream
tasks. To assess the quality of the embeddings, we solve downstream classification tasks and cal-
culate clustering scores. To explore the relation between the magnitude of injected noise and the
qualityofDM,asetofstatisticalnormalitytestsisemployed. Fortheexperimentsrequiringnumer-
ousevaluationsorvisualization,weuseMNISThandwrittendigitsdatasetLeCunetal.(2010). For
otherexperiments,weuseCIFAR10andCIFAR100datasets(Krizhevsky,2009).
Multivariate normality and uniformity tests The key part of the experimental pipeline is to
estimate how much the distribution of embeddings acquired via the proposed method is sim-
ilar to the multivariate normal or uniform distribution. To do this, we leverage D’Agostino-
Pearson (D’Agostino, 1971; D’Agostino & Pearson, 1973), Shapiro-Wilk Gonza´lez-Estrada et al.
(2022)univariatetests,andHenze-Zirkler(Henze&Zirkler,1990;Trujillo-Ortizetal.,2007)multi-
variatetest. Toextendtheunivariateteststohigherdimensions,weutilizethefundamentalproperty
oftheGaussiandistribution,whichisthenormalityofanylinearprojection.
7Preprint. Underreview.
10 1
EstimatedMI
8 D’Agostino-Pearson 0.8
Shapiro-Wilk
6 Henze-Zirkler 0.6
4 0.4
2 0.2
0 0
1 0.4
0.9875 0.3
0.975 0.2
Gaussianna¨ıveBayes
0.9625
k-Nearestneighbors
0.1
Multi-layerperceptron
Silhouetteclusteringscore
0.95 0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
Capacity,nats
Figure 1: Results for MNIST dataset in the Gaussian DM setup for d = 2 with varying capacity
C =
dlog(cid:0) 1+1/σ2(cid:1)
. The dotted line denotes the minimal capacity required to preserve the
2
information about the class labels in f(X)+Z. The dashed line represents the upper bound on
themutualinformation(5). Werun5experimentsforeachpointandreportmeanvaluesand99%
asymptoticconfidenceintervals. InfoNCElossisusedtoapproximate(2).
Inpractice,wesamplerandomprojectorsfromad-dimensionalspheretoperformaunivariatenor-
malitytest. Wealsoemploybootsrappingwithsmallsubsamplingsizetogetlow-varianceaveraged
p-values and to smooth the transition from low to high p-values. Finally, we reduce the uniform
distributioncasetotheGaussianviatheprobabilityintegraltransform, seeSection3.3. Wereport
theresultsofthetestsinFigure1.Wealsovisualizethetwo-dimensionalembeddingsoftheMNIST
datasetinFigure2.
Classificationandclustering Toexplorethetrade-offbetweenthemagnitudeoftheinjectednoise
andthequalityofrepresentations,weevaluatevariousclusteringmetricsandperformdownstream
classificationusingconventionalMLmethods, suchasGaussianna¨ıveBayes, k-nearestneighbors
and shallow multilayer perceptron. In order to numerically measure the quality of clustering, we
computeSilhouettescore(Rousseeuw,1987). TheresultsarereportedinFigure1.
We also verify noise injection does not affect typical methods for SSRL such as SimCLR (Chen
etal.,2020a)andVICReg (Bardesetal.,2021). Tothisend,wetrainbothmethodsonCIFAR-10
and CIFAR-100 datasets with varying degree of noise. The linear probing performance does not
dropwhennoisemagnitudeisincreasedacrossσvalues(0.0,0.1,0.3,0.5)asseeninTable1.
Generation Generativeadversarialnetworks(Goodfellowetal.,2014)producesamplesfromthe
underlying latent distribution, which is usually Gaussian. A common approach to make the gen-
eration not purely random is to introduce a conditioning vector during the generation (Mirza &
Osindero,2014;Larsenetal.,2016). However,thedistributionoftheconditioningvectorisusually
unknownexceptforspecificcases,whichhinderstheunconditional(orpartiallyconditional)gener-
ationviathesamemodel. Thus,amodelconvertingdatatothecorrespondingconditioningvectors
withknownpriordistributionisofgreatuse. InAppendixD,weplugourencoderintoaconditional
GANandperformconditionalandunconditionalgeneration.
8
CUACOR
stan,IM eulav-p
erocsgniretsulCPreprint. Underreview.
(a)Normaldistribution,σ=0.1 (b)Uniformdistribution,ε=0.05
Figure2:Visualizationoftwo-dimensionalrepresentationsoftheMNISThandwrittendigitsdataset.
6 DISCUSSION
In this paper, we have proposed and thoroughly investigated a novel and efficient approach to
the problem of distribution matching of learned representations. Our technique falls into a well-
established family of Deep InfoMax self-supervised representation learning methods, and does
notrequiresolvingmin-maxoptimizationproblemsoremployinggenerativemodels“post-hoc”to
matchthedistributions. Theproposedapproachisgroundedintheinformationtheory,whichallows
for a rigorous theoretical justification of the method. In our work, we also explore the possibility
ofapplyingourtechniquetootherpopularmethodsforunsupervisedandself-supervisedrepresen-
tationlearning. Consequently,weassertthattheproposedapproachcanbeutilizedinconjunction
withanyothermethodforSSRL,providedthatitcanbeformulatedintermsofmutualinformation
orentropymaximization.
Toassessthequalityoftherepresentationsyieldedbyourmethod,we(a)visualizetheembeddings
andrunnormalitytests,and(b)solveasetofdownstreamtasksinvariousexperimentalsetups. The
resultsindicatethefollowing:
1. Increasing the noise magnitude facilitates better distribution matching, but only up to a
certainpoint; beyondthat, theentirerepresentationlearningprocessbeginstodeteriorate
duetoinsufficientinformationbeingtransmittedthroughthenoisychannel.
2. Experiments with MNIST and low-dimensional embeddings indicate the existence of a
moderate trade-off between the magnitude of the injected noise and the quality in down-
CIFAR-10 CIFAR-100
top-1 top-5 top-1 top-5
SimCLR 90.83 99.76 65.64 89.91
SimCLRσ =0.1 90.96 99.72 67.03 90.49
SimCLRσ =0.3 91.56 99.77 65.72 89.76
SimCLRσ =0.5 90.51 99.74 65.58 89.56
VICReg 90.63 99.67 65.71 88.96
VICRegσ =0.1 91.09 99.68 68.92 90.50
VICRegσ =0.3 90.75 99.61 67.31 89.89
VICRegσ =0.5 91.02 99.75 66.52 89.66
Table 1: Noise magnitude influence on linear probing on CIFAR-10 and CIFAR-100. Top-1 and
top-5accuracy(in%)isreported.
9Preprint. Underreview.
streamclassificationtasks. However,experimentswithotherinfomax-relatedmethodsfor
SSRLandhigherembeddingdimensionalitysuggestthisinfluencebeingnegligible.
3. Embeddings acquired via the proposed method can be used to condition generative mod-
els, allowing both conditional and unconditional generation due to the distribution of the
conditioningvectorbeingknown.
Futurework Asforfurtherresearch,weconsiderelaboratingonthedualformulationofinfomax-
based distribution matching. Theorem 4.1 suggests that Gaussian DM can be achieved through a
specific choice of the critic network T(x,y). We plan extending this result to other distributions.
Additionally, we consider conducting further experiments with generative models to comprehen-
sivelyassessthequalityofconditionalandunconditionalgeneration,utilizingconditioningvectors
obtainedviatheproposedmethod.
REFERENCES
PhilipBachman,RDevonHjelm,andWilliamBuchwalter.Learningrepresentationsbymaximizing
mutualinformationacrossviews. InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alche´-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_
files/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf.
NikitaBalabin,DariaVoronkova,IlyaTrofimov,EvgenyBurnaev,andSergueiBarannikov. Disen-
tanglementlearningviatopology.InRuslanSalakhutdinov,ZicoKolter,KatherineHeller,Adrian
Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st
InternationalConferenceonMachineLearning, volume235ofProceedingsofMachineLearn-
ingResearch,pp.2474–2504.PMLR,21–27Jul2024. URLhttps://proceedings.mlr.
press/v235/balabin24a.html.
AdrienBardes,JeanPonce,andYannLeCun.Vicreg:Variance-invariance-covarianceregularization
forself-supervisedlearning. arXivpreprintarXiv:2105.04906,2021.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: Mutual information neural estimation, 2021. URL
https://arxiv.org/abs/1801.04062.
AJBellandTJSejnowski. Aninformation-maximizationapproachtoblindseparationandblind
deconvolution. NeuralComput.,7(6):1129–1159,November1995.
Vanessa Bo¨hm and Urosˇ Seljak. Probabilistic autoencoder. Transactions on Machine Learning
Research, 2022. doi: 10.48550/ARXIV.2006.05479. URL https://openreview.net/
forum?id=AEoYjvjKVA.
Johann Brehmer and Kyle Cranmer. Flows for simultaneous manifold learning and density esti-
mation. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 442–453. Curran Associates, Inc.,
2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/
file/051928341be67dcba03f0e04104d9047-Paper.pdf.
Bradley CA Brown, Anthony L. Caterini, Brendan Leigh Ross, Jesse C Cresswell, and Gabriel
Loaiza-Ganem. Verifying the union of manifolds hypothesis for image data. In The Eleventh
InternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.
net/forum?id=Rvee9CAX4fi.
Ivan Butakov, Alexander Tolmachev, Sofia Malanchuk, A. M. Neopryatnaya, and Alexey Frolov.
Mutual information estimation via normalizing flows. ArXiv, abs/2403.02187, 2024a. URL
https://api.semanticscholar.org/CorpusID:268247846.
Ivan Butakov, Alexander Tolmachev, Sofia Malanchuk, Anna Neopryatnaya, Alexey Frolov, and
KirillAndreev. Informationbottleneckanalysisofdeepneuralnetworksvialossycompression.
In The Twelfth International Conference on Learning Representations, 2024b. URL https:
//openreview.net/forum?id=huGECz8dPp.
10Preprint. Underreview.
Scott Chen and Ramesh Gopinath. Gaussianization. In T. Leen, T. Dietterich, and
V. Tresp (eds.), Advances in Neural Information Processing Systems, volume 13. MIT Press,
2000. URL https://proceedings.neurips.cc/paper_files/paper/2000/
file/3c947bc2f7ff007b86a9428b74654de5-Paper.pdf.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In Hal Daume´ III and Aarti Singh (eds.),
Proceedings of the 37th International Conference on Machine Learning, volume 119 of Pro-
ceedings of Machine Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020a. URL
https://proceedings.mlr.press/v119/chen20j.html.
TingChen,SimonKornblith,KevinSwersky,MohammadNorouzi,andGeoffreyHinton. Bigself-
supervisedmodelsarestrongsemi-supervisedlearners. InProceedingsofthe34thInternational
ConferenceonNeuralInformationProcessingSystems,NIPS’20,RedHook,NY,USA,2020b.
CurranAssociatesInc. ISBN9781713829546.
Kai Lai Chung. A course in probability theory. Academic Press, 3rd ed edition, 2001. ISBN
0121741516;9780121741518;9780080522982;008052298X.
ThomasM.CoverandJoyA.Thomas. ElementsofInformationTheory(WileySeriesinTelecom-
municationsandSignalProcessing). Wiley-Interscience,USA,2006.
F. N. David and N. L. Johnson. The probability integral transformation when parameters are
estimated from the sample. Biometrika, 35(1/2):182–190, 1948. ISSN 00063444. URL
http://www.jstor.org/stable/2332638.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:pre-trainingofdeep
bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and
Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171–
4186. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1423. URL
https://doi.org/10.18653/v1/n19-1423.
M.D.DonskerandS.R.S.Varadhan.Asymptoticevaluationofcertainmarkovprocessexpectations
for large time. iv. Communications on Pure and Applied Mathematics, 36(2):183–212, March
1983. ISSN0010-3640. doi: 10.1002/cpa.3160360204.
B. Duong and T. Nguyen. Conditional independence testing via latent representation learning. In
2022IEEEInternationalConferenceonDataMining(ICDM),pp.121–130,LosAlamitos,CA,
USA,dec2022.IEEEComputerSociety. doi: 10.1109/ICDM54844.2022.00022. URLhttps:
//doi.ieeecomputersociety.org/10.1109/ICDM54844.2022.00022.
R.B.D’Agostino. Anomnibustestofnormalityformoderateandlargesamplesize. Biometrika,
58(1):341–348,1971.
R.B.D’AgostinoandE.S.Pearson.Testsfordeparturefromnormality.Biometrika,60(1):613–622,
1973.
LinusEricsson,HenryGouk,ChenChangeLoy,andTimothyM.Hospedales. Self-supervisedrep-
resentationlearning: Introduction,advances,andchallenges. IEEESignalProcessingMagazine,
39(3):42–62,2022. doi: 10.1109/MSP.2021.3134634.
Marco Federici, Anjan Dutta, Patrick Forre´, Nate Kushman, and Zeynep Akata. Learning robust
representationsviamulti-viewinformationbottleneck. InInternationalConferenceonLearning
Representations,2020. URLhttps://openreview.net/forum?id=B1xwcyHFDr.
Elizabeth Gonza´lez-Estrada, Jose´ Villasen˜or, and Roc´ıo Acosta-Pech. Shapiro-wilk test for
multivariate skew-normality. Computational Statistics, 37:1–17, 09 2022. doi: 10.1007/
s00180-021-01188-y.
IanJ.Goodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https:
//arxiv.org/abs/1406.2661.
11Preprint. Underreview.
WesGurneeandMaxTegmark. Languagemodelsrepresentspaceandtime. InTheTwelfthInterna-
tional Conference on Learning Representations, 2024. URL https://openreview.net/
forum?id=jE8xbmvFin.
RobertM.Haralick,K.Shanmugam,andIts’HakDinstein.Texturalfeaturesforimageclassification.
IEEETransactionsonSystems,Man,andCybernetics,SMC-3(6):610–621,1973. doi: 10.1109/
TSMC.1973.4309314.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
nition,2015. URLhttps://arxiv.org/abs/1512.03385.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervisedvisualrepresentationlearning. In2020IEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),pp.9726–9735,2020. doi: 10.1109/CVPR42600.2020.00975.
Norbert Henze and Bernd Zirkler. A class of invariant consistent tests for multivariate normality.
CommunicationsinStatistics-theoryandMethods,19:3595–3617,1990. URLhttps://api.
semanticscholar.org/CorpusID:120328121.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochre-
iter. Gans trained by a two time-scale update rule converge to a local nash equilibrium.
In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/
paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017. URLhttps://openreview.net/forum?id=Sy2fzU9gl.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estima-
tion and maximization. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=Bklr3j0cKX.
JonathanHoandTimSalimans. Classifier-freediffusionguidance. InNeurIPS2021Workshopon
DeepGenerativeModelsandDownstreamApplications,2021.URLhttps://openreview.
net/forum?id=qw8AKxfYbI.
Chin-WeiHuang, DavidKrueger, AlexandreLacoste, andAaronCourville. Neuralautoregressive
flows. InInternationalconferenceonmachinelearning,pp.2078–2087.PMLR,2018.
MinyoungHuh,BrianCheung,TongzhouWang,andPhillipIsola. Position: Theplatonicrepresen-
tationhypothesis. InRuslanSalakhutdinov,ZicoKolter,KatherineHeller,AdrianWeller,Nuria
Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International
Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research,
pp. 20617–20642. PMLR, 21–27 Jul 2024. URL https://proceedings.mlr.press/
v235/huh24a.html.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducinginternalcovariateshift,2015. URLhttps://arxiv.org/abs/1502.03167.
Priyank Jaini, Kira A. Selby, and Yaoliang Yu. Sum-of-squares polynomial flow. In Kamalika
ChaudhuriandRuslanSalakhutdinov(eds.),Proceedingsofthe36thInternationalConferenceon
Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 3009–3018.
PMLR,09–15Jun2019.
Amor Keziou. Dual representation of ϕ-divergences and applications. Comptes Ren-
dus Mathematique, 336(10):857–862, 2003. ISSN 1631-073X. doi: https://doi.org/10.
1016/S1631-073X(03)00215-2. URL https://www.sciencedirect.com/science/
article/pii/S1631073X03002152.
12Preprint. Underreview.
DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization,2017.
DiederikP.KingmaandMaxWelling.Auto-encodingvariationalbayes.InYoshuaBengioandYann
LeCun(eds.),2ndInternationalConferenceonLearningRepresentations,ICLR2014,Banff,AB,
Canada,April14-16,2014,ConferenceTrackProceedings,2014.URLhttp://arxiv.org/
abs/1312.6114.
Ivan Kobyzev, Simon Prince, and Marcus A. Brubaker. Normalizing flows: An introduction
and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence,43:3964–3979,2020. URLhttps://api.semanticscholar.org/CorpusID:
208910764.
AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Technicalreport,2009.
Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Au-
toencoding beyond pixels using a learned similarity metric. In Maria Florina Balcan and Kil-
ianQ.Weinberger(eds.),ProceedingsofThe33rdInternationalConferenceonMachineLearn-
ing,volume48ofProceedingsofMachineLearningResearch,pp.1558–1566,NewYork,New
York, USA, 20–22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/
larsen16.html.
YannLeCun,CorinnaCortes,andCJBurges.Mnisthandwrittendigitdatabase.ATTLabs[Online].
Available: http://yann.lecun.com/exdb/mnist,2,2010.
R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105–117, 1988. doi: 10.
1109/2.36.
Alireza Makhzani and Brendan Frey. k-sparse autoencoders. In Yoshua Bengio and Yann Le-
Cun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL https://arxiv.
org/abs/1312.5663.
AlirezaMakhzani,JonathonShlens,NavdeepJaitly,andIanGoodfellow.Adversarialautoencoders.
InInternationalConferenceonLearningRepresentations,2016. URLhttp://arxiv.org/
abs/1511.05644.
David McAllester and Karl Stratos. Formal limitations on the measurement of mutual informa-
tion. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third Interna-
tionalConferenceonArtificialIntelligenceandStatistics,volume108ofProceedingsofMachine
LearningResearch,pp.875–884.PMLR,26–28Aug2020. URLhttps://proceedings.
mlr.press/v108/mcallester20a.html.
Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger. Umap: Uniform manifold
approximation and projection. Journal of Open Source Software, 3(29):861, 2018. doi: 10.
21105/joss.00861. URLhttps://doi.org/10.21105/joss.00861.
TomasMikolov,KaiChen,GregCorrado,andJeffreyDean. Efficientestimationofwordrepresen-
tationsinvectorspace,2013. URLhttps://arxiv.org/abs/1301.3781.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets, 2014. URL https:
//arxiv.org/abs/1411.1784.
Michael Moor, Max Horn, Bastian Rieck, and Karsten Borgwardt. Topological autoencoders. In
HalDaume´ IIIandAartiSingh(eds.),Proceedingsofthe37thInternationalConferenceonMa-
chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7045–7054.
PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/moor20a.
html.
Andrew Ng. Sparse autoencoder, 2011. URL http://www.stanford.edu/class/
cs294a/sparseAutoencoder.pdf.
13Preprint. Underreview.
XuanLongNguyen,MartinJ.Wainwright,andMichaelI.Jordan.Estimatingdivergencefunctionals
andthelikelihoodratiobyconvexriskminimization. IEEETrans.Inf.Theor.,56(11):5847–5861,
nov2010. ISSN0018-9448. doi: 10.1109/TIT.2010.2068870. URLhttps://doi.org/10.
1109/TIT.2010.2068870.
GeorgePapamakarios,EricNalisnick,DaniloJimenezRezende,ShakirMohamed,andBalajiLak-
shminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Ma-
chine Learning Research, 22(57):1–64, 2021. URL http://jmlr.org/papers/v22/
19-1028.html.
Y. Polyanskiy and Y. Wu. Information Theory: From Coding to Learning. Cambridge Univer-
sityPress,2024. ISBN9781108832908. URLhttps://books.google.ru/books?id=
CySo0AEACAAJ.
AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever. Improvinglanguageunder-
standingbygenerativepre-training,2018.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision, 2021. URL
https://arxiv.org/abs/2103.00020.
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image syn-
thesis with latent diffusion models. In 2022 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 10674–10685, Los Alamitos, CA, USA, jun 2022.
IEEE Computer Society. doi: 10.1109/CVPR52688.2022.01042. URL https://doi.
ieeecomputersociety.org/10.1109/CVPR52688.2022.01042.
Peter J. Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of clus-
ter analysis. Journal of Computational and Applied Mathematics, 20:53–65, 11 1987. doi:
10.1016/0377-0427(87)90125-7.
Peter Sorrenson, Felix Draxler, Armand Rousselot, Sander Hummerich, Lea Zimmermann, and
UllrichKoethe. Liftingarchitecturalconstraintsofinjectiveflows. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024.URLhttps://openreview.net/forum?
id=kBNIx4Biq4.
M. Spivak. Calculus On Manifolds: A Modern Approach To Classical Theorems Of Advanced
Calculus. AvalonPublishing,1965. ISBN9780805390216.
Michael E. Tipping and Christopher M. Bishop. Mixtures of Probabilistic Principal Component
Analyzers. Neural Computation, 11(2):443–482, 02 1999. ISSN 0899-7667. doi: 10.1162/
089976699300016728. URLhttps://doi.org/10.1162/089976699300016728.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck
method. In Proc. of the 37-th Annual Allerton Conference on Communication, Control
and Computing, pp. 368–377, 1999. URL /brokenurl#citeseer.nj.nec.com/
tishby99information.html.
Antonio Trujillo-Ortiz, Rafael Hernandez-Walls, and K. Barba-Rojo. Hzmvntest. henze-zirkler’s
multivariatenormalitytest.,122007.
MichaelTschannen,JosipDjolonga,PaulK.Rubenstein,SylvainGelly,andMarioLucic. Onmu-
tualinformationmaximizationforrepresentationlearning.InInternationalConferenceonLearn-
ingRepresentations,2020. URLhttps://openreview.net/forum?id=rkxoh24FPH.
AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredic-
tivecoding,2019. URLhttps://arxiv.org/abs/1807.03748.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Ma-
chineLearningResearch,9(86):2579–2605, 2008. URLhttp://jmlr.org/papers/v9/
vandermaaten08a.html.
14Preprint. Underreview.
Petar Velicˇkovic´, William Fedus, William L. Hamilton, Pietro Lio`, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In International Conference on Learning Representations, 2019.
URLhttps://openreview.net/forum?id=rklz9iAcKQ.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural
imagecaptiongenerator. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pp.3156–3164,2015.
YangYou,IgorGitman,andBorisGinsburg. Largebatchtrainingofconvolutionalnetworks,2017.
URLhttps://arxiv.org/abs/1708.03888.
XuechuYu. Leveragingsuperfluousinformationincontrastiverepresentationlearning,2024. URL
https://arxiv.org/abs/2408.10292.
JureZbontar,LiJing,IshanMisra,YannLeCun,andSte´phaneDeny. Barlowtwins:Self-supervised
learningviaredundancyreduction. InInternationalconferenceonmachinelearning,pp.12310–
12320.PMLR,2021.
A COMPLETE PROOFS
Theorem 2.1 (Theorem 8.6.5 in Cover & Thomas (2006)). Let X be a d-dimensional absolutely
continuous random vector with probability density function p, mean m and covariance matrix Σ.
Then
h(X)=h(N(m,Σ))−D (p∥N(m,Σ)), h(N(m,Σ))=
1 log(cid:0) (2πe)ddetΣ(cid:1)
,
KL 2
whereN(m,Σ)isaGaussiandistributionofmeanmandcovariancematrixΣ.
ProofofTheorem2.1. Asforanym ∈ Rd itholdsh(X −m) = h(X),letusconsideracentered
randomvectorX. DenotingtheprobabilitydensityfunctionofN(0,Σ)byϕ ,wehave
Σ
(cid:90) p(x) (cid:90)
D (p∥N(0,Σ))= p(x)log dx=−h(X)− p(x)logϕ (x)dx.
KL ϕ (x) Σ
Rd Σ Rd
Now,considerthesecondterm:
(cid:90) 1
p(x)logϕ (x)dx=const+ E XTΣ−1X =
Σ 2 X
Rd
1 1 d
=const+ Tr(Σ−1E [XXT])=const+ Tr(Σ−1Σ)=const+ =
2 X 2 2
1 (cid:90)
=const+ E XTΣ−1X = ϕ (x)logϕ (x)dx.
2 N(0,Σ) Σ Σ
Rd
Here,inthesecondline,thecyclicpropertyofthetraceisused.
Theorem2.2. LetX beanabsolutelycontinuousrandomvectorwithprobabilitydensityfunction
pandsuppX ⊆S,whereS hasfiniteandnon-zeroLebesguemeasureµ(S). Then
h(X)=h(U(S))−D (p∥U(S)), h(U(S))=logµ(S),
KL
whereU(S)isauniformdistributiononS.
ProofofTheorem2.2.
(cid:90) p(x)
D (p∥U(S))= p(x)log dx=
KL 1/µ(S)
S
(cid:90)
=−h(X)+ p(x)logµ(S)dx=−h(X)+logµ(S)=−h(X)+h(U(S)).
S
15Preprint. Underreview.
Lemma 2.4. Let X and Z be random vectors of the same dimensionality, independent under the
conditioningvectorY. Then
h(X+Z |Y)=h(Z |Y)+I(X;X+Z |Y)≥h(Z |Y),
withequalityifandonlyifthereexistsameasurablefunctiongsuchthatX =g(Y).3
ProofofLemma2.4. BythedefinitionofmutualinformationbetweenZandX+Zconditionedon
Y,wehave
I(X;X+Z |Y)=h(X+Z |Y)−h(X+Z |X,Y),
whereh(X+Z |X,Y)=h(Z |Y)duetotheindependenceofX andZ givenY.
Now,letusprovethatI(X;X+Z |Y)=0ifandonlyif∃g :X =g(Y). Firstly,ifX =g(Y),
I(X;X+Z |Y)=I(g(Y);g(Y)+Z |Y)=I(0;Z |Y)=I(0;Z)=0
Next,considerI(X;X+Z |Y)=0. Thus,X isconditionallyindependentbothofX+Z andZ,
whichallowsforthemultiplicativepropertyofcharacteristicfunctionstobeused:
(cid:16) (cid:12) (cid:17) (cid:16) (cid:12) (cid:17) (cid:16) (cid:12) (cid:17) (cid:16) (cid:12) (cid:17) (cid:16) (cid:12) (cid:17)
E ei⟨t,Z⟩(cid:12)Y =E ei⟨t,−X+(Z+X)⟩(cid:12)Y =E e−i⟨t,X⟩(cid:12)Y E ei⟨t,Z⟩(cid:12)Y E ei⟨t,X⟩(cid:12)Y =
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:104) (cid:16) (cid:17)(cid:105)∗ (cid:16) (cid:17) (cid:16) (cid:17) (cid:12) (cid:16) (cid:12) (cid:17)(cid:12)2 (cid:16) (cid:12) (cid:17)
= E ei⟨t,X⟩ E ei⟨t,X⟩ E ei⟨t,Z⟩ =(cid:12)E ei⟨t,X⟩(cid:12)Y (cid:12) E ei⟨t,Z⟩(cid:12)Y
(cid:12) (cid:12) (cid:12) (cid:12)
Note that there exists ε(Y) > 0 such that
E(cid:0) ei⟨t,Z⟩(cid:12) (cid:12)Y(cid:1)
̸= 0 for ∥t∥ < ε(Y) (non-vanishing
property). Consequently,
(cid:12) (cid:12)E(cid:0) ei⟨t,X⟩(cid:12) (cid:12)Y(cid:1)(cid:12)
(cid:12) = 1 holds for ∥t∥ < ε(Y). This implies that the con-
ditionaldistributionofX givenY isaδ-distribution(Theorem6.4.7in(Chung,2001)),leadingto
X =E(X |Y),whereg(Y)≜E(X |Y)isaσ(Y)-measurablefunction.
Lemma3.1. ConsiderthefollowingMarkovchainofabsolutelycontinuousrandomvectors:
f(X)+Z −→X −→X′ −→f(X′),
withZ beingindependentof(X,X′). Then
I(f(X′);f(X)+Z)=h(f(X)+Z)−h(Z)−I(f(X)+Z;f(X)|f(X′)).
ProofofLemma3.1. Fromthedefinitionofmutualinformation,wehave
I(f(X′);f(X)+Z)=h(f(X)+Z)−h(f(X)+Z |f(X′)).
WeapplyLemma2.4torewritethesecondterm
h(f(X)+Z |f(X′))=h(Z |f(X′))+I(f(X);f(X)+Z |f(X′))
=h(Z)+I(f(X);f(X)+Z |f(X′)),
wheretheindependenceofZ andf(X′)isused.
Lemma3.3. UndertheconditionsofLemma3.1,letP(X = X′ | X) ≥ α > 0. Then,I(f(X)+
Z;f(X)|f(X′))=0preciselywhenf isweaklyinvarianttoX →X′.
ProofofLemma3.3. According to Lemma 2.4, I(f(X)+Z;f(X) | f(X′)) = 0 if and only if
f(X)=g(f(X′))forsomeg. Thus,weakinvarianceimpliesI(f(X)+Z;f(X)|f(X′))=0.
Ontheotherhand,iff(X)=g(f(X′)),
P(f(X)∈(g−1◦f)(X)|X)≥P(f(X)=f(X′)|X)≥P(X =X′ |X)≥α>0.
NotethatthepredicateP(X) ≜ “f(X) ∈ (g−1 ◦f)(X)”isnotrandomwhenconditionedonX.
Thus, P(P(X) | X) = ϕ(X), where ϕ is a function taking values in {0,1}. As ϕ(X) ≥ α > 0,
ϕ(X)=1. Thisimpliesg(f(X))=f(X)almostsurely.
3Hereinafter,whencomparingrandomvariables,wemeanequalityorinequality“almostsure”.
16Preprint. Underreview.
Theorem3.4(Gaussiandistributionmatching). LettheconditionsofLemma3.3besatisfied. As-
sumeZ ∼ N(0,σ2I),Ef(X) = 0andVar(f(X) ) = 1foralli ∈ {1,...,d}. Then,themutual
i
informationI(f(X′);f(X)+Z)canbeupperboundedasfollows
(cid:18) (cid:19)
d 1
I(f(X′);f(X)+Z)≤ log 1+ , (5)
2 σ2
withtheequalityholdingexactlywhenf isweaklyinvariantandf(X)∼N(0,I). Moreover,
D (f(X)∥N(0,I))≤I(Z;f(X)+Z)−I(f(X′);f(X)+Z)−dlogσ.
KL
ProofofTheorem3.4. ApplyingtheresultfromLemmaB.1givesus:
(cid:18) (cid:19)
D (cid:0) f(X)+Z∥N(0,(1+σ2)I)(cid:1) ≤ d log 1+ 1 −I(f(X′);f(X)+Z).
KL 2 σ2
Since KL-divergence is non-negative, we obtain the desired bound. Furthermore, equality holds
exactlywhenf isweaklyinvariantandf(X)∼N(0,I),asdiscussedintheproofofLemmaB.1.
ThesecondinequalityinvolvingD (f(X)∥N(0,I)),followsfromCorollaryB.3.
KL
Theorem 3.5 (Uniform distribution matching). Under the conditions of Lemma 3.1, let Z ∼
U([−ε;ε]d) and suppf(X) ⊆ [0;1]d. Then, the mutual information I(f(X′);f(X) + Z) can
beupperboundedasfollows
(cid:18) (cid:19)
1
I(f(X′);f(X)+Z)≤dlog 1+ . (6)
2ε
with the equality if and only if 1/ε ∈ N, f is weakly invariant, and f(X) ∼ U(A), where the set
A={0,2ε,4ε,...,1}contains(1/(2ε)+1)elements.
Moreover,
D (cid:0) f(X)∥U([0;1]d)(cid:1) ≤I(Z;f(X)+Z)−I(f(X′);f(X)+Z)−dlog(2ε).
KL
ProofofTheorem3.5. ProceedingsimilarlytotheGaussiancase,weapplyLemmaB.4,yielding
(cid:18) (cid:19)
D (cid:0) f(X)+Z∥U([−ε;1+ε]d)(cid:1) ≤dlog 1+ 1 −I(f(X′);f(X)+Z),
KL 2ε
where the left-hand side is non-negative, so we obtain the claimed inequality. The conditions for
equalityarealsoestablishedthroughLemmaB.4.
Toprovethesecondupper-bound,concerningD
(cid:0) f(X)∥U([0;1]d)(cid:1)
,itsufficestousetheresult
KL
fromCorollaryB.6.
Theorem4.1(DualformofGaussiandistributionmatching). UndertheconditionsofTheorem3.4,
(cid:104) (cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
I(f(X′);f(X)+Z)≥E T∗ −logE exp T∗ ,
P+ N(0,σ2I) P− N(0,σ2I)
∥y∥2 ∥y−x∥2 1 (cid:18) ∥x∥2+∥y∥2/(1+σ2)(cid:19)
T∗ (x,y)= − = ⟨x,y⟩− ,
N(0,σ2I) 2(1+σ2) 2σ2 σ2 2
withtheequalityholdingpreciselywhenf isweaklyinvariantandf(X)∼N(0,I).
ProofofTheorem4.1. By Remark 2.5 we know that the equality holds if and only if
T∗ (x,y)=PMI (x,y)+const.
N(0,σ2I) f(X′),f(X)+Z
Now,forindependentY ∼N(0,I)andZ ∼N(0,σ2I)
PMI (x,y)=logp (y |x)−logp (y)=
Y,Y+Z Y+Z|Y Y+Z
∥y∥2 ∥y−x∥2 d (cid:18) 1 (cid:19)
= − + log 1+ =
2(1+σ2) 2σ2 2 σ2
=T∗ (x,y)+I(Y;Y +Z).
N(0,σ2I)
Thus,theequalityholdsifandonlyifI(f(X′);f(X)+Z)= dlog(1+1/σ2),which,inturn,holds
2
preciselywhenf isweaklyinvariantandf(X)∼N(0,I)(seeTheorem3.4).
17Preprint. Underreview.
B SUPPLEMENTARY RESULTS ON DISTRIBUTION MATCHING
Inthissection,weexploretheconnectionbetweentheproposedinfomaxobjectiveandtheproblem
ofdistributionmatching,formulatedintermsoftheKullback-Leiblerdivergence.
GAUSSIANDISTRIBUTIONMATCHING
Lemma B.1. Assume the conditions of Theorem 3.4 are satisfied, then for Gaussian distribution
matching,wehave
(cid:18) (cid:19)
D (cid:0) f(X)+Z∥N(0,(1+σ2)I)(cid:1) ≤ d log 1+ 1 −I(f(X′);f(X)+Z),
KL 2 σ2
withequalityholdingexactlywhenf isweaklyinvariantandf(X)∼N(0,I).
Proof. FromLemma3.1weobtain
I(f(X′);f(X)+Z)=h(f(X)+Z)−h(N(0,σ2I))−I(f(X)+Z;f(X)|f(X′)).
UsingTheorem2.1,wecanrewritethefirstterm,whichyields
I(f(X′);f(X)+Z)=h(N(m,Σ))−h(N(0,σ2I))
−D (f(X)+Z∥N(m,Σ))−I(f(X)+Z;f(X)|f(X′)),
KL
wheremandΣarethemeanandcovariancematrixoff(X)+Z.
ToboundtheKL-divergence,notethattheconditionalmutualinformationisnon-negative:
D (f(X)+Z∥N(m,Σ))≤h(N(m,Σ))−h(N(0,σ2I))−I(f(X′);f(X)+Z).
KL
Equality holds exactly when I(f(X) + Z;f(X) | f(X′)) = 0, which is equivalent to f being
weaklyinvariant(seeLemma3.3).
Next,weestimatethedifferencebetweentheentropiesbyobservingthat
d
(cid:88)
h(N(m,Σ))≤ h(N(m ,Var(f(X) )+σ2))=d·h(N(0,1+σ2)),
i i
i=1
withtheequalityholdingifandonlyifΣisdiagonal,whichimpliesΣ = IsinceVar(f(X) ) = 1
i
foralli∈{1,...,d}. Finally,
(cid:18) (cid:19)
d·h(N(0,1+σ2))−h(N(0,σ2I))= d(cid:2) log(1+σ2)−logσ2(cid:3) = d log 1+ 1 ,
2 2 σ2
whichprovestheclaimedinequality.
LemmaB.2. UndertheconditionsofTheorem3.4,thefollowingholds:
D
(cid:0) f(X)+Z∥N(0,(1+σ2)I)(cid:1)
≤D (f(X)∥N(0,I))=
KL KL
=D
(cid:0) f(X)+Z∥N(0,(1+σ2)I)(cid:1)
+I(Z;f(X)+Z)−
d log(cid:0) 1+σ2(cid:1)
.
KL 2
Proof. The left-hand inequality follows directly from the data processing inequality for the
Kullback-Leiblerdivergence(Theorem2.17in(Polyanskiy&Wu,2024)).
Toestablishtheright-handside,wefirstapplyTheorem2.1,andthenusetheindependenceoff(X)
andZ,yieldingh(f(X))=h(f(X)+Z |Z). Thus,wehave
D (f(X)∥N(0,I))=h(N(0,I))−h(f(X)+Z |Z).
KL
Next,byusingthedefinitionofmutualinformationandagainapplyingTheorem2.1,onecanwrite
h(f(X)+Z |Z)=h(f(X)+Z)−I(f(X)+Z;Z)=
=h(N(0,(1+σ2)I))−D (cid:0) f(X)+Z∥N(0,(1+σ2)I)(cid:1) −I(f(X)+Z;Z).
KL
Finally,substitutingthisintotheexpansionforD (f(X)∥N(0,I)),andnotingthath(N(0,I))−
KL
N(0,(1+σ2)I))=−dlog(1+σ2)completestheproof.
2
18Preprint. Underreview.
CorollaryB.3. IntheGaussiandistributionmatchingsetup(Theorem3.4),wehave
D (f(X)∥N(0,I))≤I(Z;f(X)+Z)−I(f(X′);f(X)+Z)−dlogσ.
KL
Proof. ThisfollowsdirectlyfromcombiningLemmaB.1andLemmaB.2.
UNIFORMDISTRIBUTIONMATCHING
LemmaB.4. LettheconditionsofTheorem3.5hold,thenthefollowingboundappliesforuniform
distributionmatching:
(cid:18) (cid:19)
D (cid:0) f(X)+Z∥U([−ε;1+ε]d)(cid:1) ≤dlog 1+ 1 −I(f(X′);f(X)+Z),
KL 2ε
with the equality if and only if 1/ε ∈ N, f is weakly invariant, and f(X) ∼ U(A), where the set
A={0,2ε,4ε,...,1}contains(1/(2ε)+1)elements.
Proof. Similarly to the proof of Lemma B.1, we use the decomposition of the infomax objective
fromLemma3.1. Afterward,weapplyTheorem2.2tothetermh(f(X)+Z):
I(f(X′);f(X)+Z)=h(U([−ε;ε+1]d))−h(U([−ε;ε]d))
−D (cid:0) f(X)+Z∥U([−ε;ε+1]d)(cid:1) −I(f(X)+Z;f(X)|f(X′)).
KL
Therefore,theKL-divergencecanbeboundedas:
D (cid:0) f(X)+Z∥U([−ε;ε+1]d)(cid:1) ≤h(U([−ε;ε+1]d))−h(U([−ε;ε]d))−I(f(X′);f(X)+Z)
KL
(cid:18) (cid:19)
1
=dlog 1+ −I(f(X′);f(X)+Z),
2ε
withequalityachievedifandonlyifthemutualinformationbetweenf(X)+Z andf(X)condi-
tionedonf(X′)iszero,whichoccurspreciselywhenf isweaklyinvariant(seeLemma3.3).
Next,weshowwhentheequalityholds. Theprobabilitydensityfunctionofthesumofindependent
randomvectorsf(X)andZ isgivenbytheconvolution:
(cid:90) p (x)p (z−x)dx=(cid:89)d (cid:90) zi+ε(cid:32) 1 (cid:88) δ(z −a)(cid:33) dx i = 1 (cid:90) dx.
f(X) Z 1/(2ε)+1 i 2ε (1+2ε)d
Rd i=1 zi−ε a∈A [−ε;1+ε]d
Here, we used the independence of the components, with Z uniformly distributed on [−ε,ε] and
i
f(X) uniformlydistributedoverthediscretesetA={0,2ε,4ε,...,1}.
i
Therefore, (f(X)+Z) ∼ U([−ε;1+ε]d). Given this, one can calculate the mutual information
explicitly:
(cid:18) (cid:19)
1
I(f(X);f(X)+Z)=h(f(X)+Z)−h(Z)=dlog 1+ .
2ε
Thisconcludestheproof.
LemmaB.5. UndertheconditionsofTheorem3.5,thefollowingholds:
D
(cid:0) f(X)∥U([0;1]d)(cid:1)
=D
(cid:0) f(X)+Z∥U([−ε;ε+1]d)(cid:1)
+I(Z;f(X)+Z)−dlog(1+2ε).
KL KL
Proof. One can build on the reasoning from Lemma B.2 by applying Theorem 2.2 to express the
KL-divergencebetweenf(X)andU([0;1]d)asfollows:
D (cid:0) f(X)∥U([0;1]d)(cid:1) =h(U([0;1]d))−h(f(X)+Z |Z)=−h(f(X)+Z |Z).
KL
UsingTheorem2.2again,theconditionalentropycanbeexpressedas
h(f(X)+Z |Z)=h(U([−ε;ε+1]d))−D (cid:0) f(X)+Z∥U([−ε;ε+1]d)(cid:1) −I(f(X)+Z;Z).
KL
Toconcludeitremainstocalculateh(U([−ε;ε+1]d))=dlog(1+2ε).
CorollaryB.6. Intheuniformdistributionmatchingsetup(Theorem3.5),wehave
D (cid:0) f(X)∥U([0;1]d)(cid:1) ≤I(Z;f(X)+Z)−I(f(X′);f(X)+Z)−dlog(2ε).
KL
Proof. ApplyingLemmaB.4alongsideLemmaB.5isenough.
19Preprint. Underreview.
Table2: TheNNarchitecturesusedtoconductthetestsonMNISTimagesinSection5.
NN Architecture
×1: Conv2d(1,32,ks=3),MaxPool2d(2),BatchNorm2d,LeakyReLU(0.01)
ConvNet,
×1: Conv2d(32,64,ks=3),MaxPool2d(2),BatchNorm2d,LeakyReLU(0.01)
24×24
×1: Conv2d(64,128,ks=3),MaxPool2d(2),BatchNorm2d,LeakyReLU(0.01)
images
×1: Dense(128,128),LeakyReLU(0.01),Dense(128,dim)
CriticNN, ×1: Dense(dim+dim,256),LeakyReLU(0.01)
pairsofvectors ×1: Dense(256,256),LeakyReLU(0.01),Dense(256,1)
C DETAILS OF IMPLEMENTATION
For experiments on MNIST dataset, we use a simple ConvNet with three convolutional and two
fully connected layers. A three-layer fully-connected perceptron serves as a critic network for the
InfoNCEloss. WeprovidethedetailsinTable2. WeuseadditiveGaussiannoisewithσ = 0.6as
aninputaugmentation. Traininghyperparametersareasfollows: batchsize=1024, 2000epochs,
Adamoptimizer(Kingma&Ba,2017)withlearningrate10−3.
TheresultsonCIFARdatasets(Krizhevsky,2009)inTable1wereobtainedwiththestandardcon-
figurationofSSLmethods. Namely,weuseResNet-18(Heetal.,2015)backbone. Projectionhead
for SimCLR consists of two linear layers, for VICReg – 3 layers. Respective configurations are
[2048,256]and[2048,2048,2048],meaningembeddingdimensionsare256and2048,respectively.
Weapplyastandardsetofaugmentations:
PretrainTransform : Compose(
RandomResizedCrop(
size=(32, 32),
scale =(0.08, 1.0) ,
ratio =(0.75, 1.3333333333333333),
interpolation=InterpolationMode .BICUBIC,
antialias=True
)
RandomApply(
ColorJitter (
brightness =(0.6, 1.4) ,
contrast =(0.6, 1.4) ,
saturation =(0.8, 1.2) ,
hue=(−0.1, 0.1)
)
)
RandomGrayscale(p=0.2)
GaussianBlur(p=0.0)
Solarization (p=0.0)
RandomHorizontalFlip(p=0.5)
ToTensor()
Normalize(
mean=[0.4914, 0.4822, 0.4465],
std=[0.247, 0.2435, 0.2616],
inplace=False)
)
TestTransform: Compose(
ToTensor()
Normalize(
mean=[0.4914, 0.4822, 0.4465],
std=[0.247, 0.2435, 0.2616],
20Preprint. Underreview.
inplace=False
)
)
Traininghyperparametersareasfollows: batchsize256,800epochs,LARSoptimizer(Youetal.,
2017) with clipping, base learning rate 0.3, momentum 0.9, trust coefficient 0.02, weight decay
10−4. ForSimCLR,weusetemperature0.2,forVICReg–standardhyperparameters(25,25,1).
Wealsoprovidethesourcecodeinthesupplementarymaterials.
D CONDITIONING GENERATIVE ADVERSARIAL NETWORKS
Inthissection, weleverageourmethodtogenerateconditioningvectorsforaconventionalcGAN
setup(Mirza&Osindero,2014). Weusethetwo-dimensionalGaussianembeddingsoftheMNIST
dataset,acquiredfromthenoiselevelσ =0.1.Forconditionedgeneration,wegetembeddingsfrom
a batch of original images. For unconditioned generation, embeddings are sampled from N(0,I).
TheresultsarepresentedinFigures3and4.
(a)Encodedimages (b)Generatedimages
Figure3: Resultsofconditionalgeneration.
21Preprint. Underreview.
Figure4: Resultsofunconditionalgeneration.
22