DECIPHERING CROSS-MODAL ALIGNMENT IN LARGE
VISION-LANGUAGE MODELS WITH MODALITY INTE-
GRATION RATE
QidongHuang1,2 XiaoyiDong2,3 PanZhang2 YuhangZang2 YuhangCao2
JiaqiWang2 DahuaLin2 WeimingZhang1 NenghaiYu1
1USTC 2ShanghaiAILaboratory 3CUHK
ABSTRACT
WepresenttheModalityIntegrationRate(MIR),aneffective,robust,andgeneral-
izedmetrictoindicatethemulti-modalpre-trainingqualityofLargeVisionLan-
guageModels(LVLMs). Large-scalepre-trainingplaysacriticalroleinbuilding
capable LVLMs, while evaluating its training quality without the costly super-
visedfine-tuningstageisunder-explored. Loss,perplexity,andin-contextevalu-
ationresultsarecommonlyusedpre-trainingmetricsforLargeLanguageModels
(LLMs), while we observed that these metrics are less indicative when aligning
awell-trainedLLMwithanewmodality. Duetothelackofpropermetrics, the
researchofLVLMsinthecriticalpre-trainingstageishinderedgreatly,including
the training data choice, efficient module design, etc. In this paper, we propose
evaluatingthepre-trainingqualityfromtheinter-modaldistributiondistanceper-
spective and present MIR, the Modality Integration Rate, which is 1) Effective
torepresentthepre-trainingqualityandshowapositiverelationwiththebench-
markperformanceaftersupervisedfine-tuning. 2)Robusttowarddifferenttrain-
ing/evaluationdata. 3)Generalizeacrosstrainingconfigurationsandarchitecture
choices. Weconductaseriesofpre-trainingexperimentstoexploretheeffective-
nessofMIRandobservesatisfactoryresultsthatMIRisindicativeabouttraining
data selection, training strategy schedule, and model architecture design to get
better pre-training results. We hope MIR could be a helpful metric for building
capable LVLMs and inspire the following research about modality alignment in
differentareas. Ourcodeisat: github.com/shikiw/Modality-Integration-Rate.
1.3 59.9 60.0 60.1 59.8 60.0 59.8 5.00 4.6 59.9 60.0 60.1 59.8 60.0 59.8 5.00 59.9 60.0 60.1 59.8 60.0 59.8 5.00 1.2 59.5 59.6 4.75 59.5 59.6 4.75 45 59.5 59.6 4.75
4.50 4.4 4.50 40 4.50
1.1 Final Loss Value 4.25 PPL 4.25 In-Context Eval 4.25 1.0 58.1 M MI oR del Performance 4.00 4.2 58.1 M MI oR del Performance 4.00 35 58.1 M MI oR del Performance 4.00
3.75 4.0 3.75 30 3.75
0.9 3.50 3.50 3.50
3.8 25
0.8 56.5 3.25 56.5 3.25 56.5 3.25
100K200K400K600K800K 1M 1.2M1.4M1.6M1.8M 100K200K400K600K800K 1M 1.2M1.4M1.6M1.8M 100K200K400K600K800K 1M 1.2M1.4M1.6M1.8M
Pretrain Data Scale Pretrain Data Scale Pretrain Data Scale
Figure1: Finalloss, perplexity(PPL),andin-contextevaluationareinsufficientindicatorsof
LVLMpre-trainingquality.WetesttheeffectivenessofthesethreemethodsandourproposedMIR
on a pre-training data scaling experiment, where we curate ∼1.8M GPT-style data from ALLaVA
(Chen et al. (2024a)) and ShareGPT4V-PT (Chen et al. (2023)) and use different amount of data
to pre-train LLaVA-1.5 7B models (Liu et al. (2024c)). Note that “Model Performance” means
thepost-SFT(SupervisedFine-tuning)performanceon7multi-modalbenchmarksafterweequally
applySFTonthesepre-trainedmodelsonLLaVA’s665KSFTdata.In(a),wereporttheaverageloss
overthelast50pre-trainingstepsasthefinalloss. In(b),thePPLiscalculatedon1,000randomly
sampledimage-captionpairsfromShareGPT4V.In(c), weapply2-shotin-contextevaluationand
force thepre-trained models toresponse choice onMME (Fu etal. (2023)), MMBench(Liu et al.
(2023)),SEED-Img(Lietal.(2023a)),andreporttheaveragescores. Wecanfindthatthesethree
metricsfailtomeasurethepre-trainingqualitywhileMIRwellfitstheactualmodelperformance.
1
4202
tcO
9
]VC.sc[
1v76170.0142:viXra
eulaV
ssoL laniF RIM LPP RIM
)tohs-2(
erocS txetnoC-nI RIM15
10
5
0
5
10
15
15 10 5 0 5 10 15
t-SNE Dimension 1
Figure2: CurrentLVLMsshowobviousmodalitygapintheshallowlayers. (Left)Thet-SNE
visualizationdepictsthesignificantgapbetweenvision(warmcolors)andtext(coolcolors)tokens
atLLaVA-v1.5’sembeddingspace, whereweselectsixtypesofimages(fromDocVQA(Mathew
et al. (2021)), ChartQA (Masry et al. (2022)), InfoVQA (Mathew et al. (2022)) and ShareGPT4V
(Chen et al. (2023))) and three types of text data (from CNN News, Daily Mail (Nallapati et al.
(2016))andCodeSearchNet(Husainetal.(2019)). (Right)Themodalitygapindifferentlayersof
LVLM’slanguagemodel, whichisobtainedduringcomputingMIR.FormostofLVLMs, thefirst
severallayersstillstrivetonarrowthemodalitygaputilthemiddlelayersachievethealignment.
1 INTRODUCTION
In the past two years, the exploration of LVLMs (Liu et al. (2024c); Zhu et al. (2023); Dai et al.
(2023)) has exploded from multiple aspects, showcasing amazing usages in many areas. Putting
asidethediversityamongtheseexplorations,mostLVLMsfollowadual-stageparadigmthatusesa
pre-trainingstagetoalignthemodality,followedbyasupervisedfine-tuning(SFT)stagetoenable
diversemulti-modalcapabilities. WiththeprogressofLVLMs,thepre-trainingstageevolvedfrom
lightweight alignment with few image-caption pairs toward deep modality integration with large-
scalediversemulti-modaldata,playingamorecriticalroleinbuildingcapableLVLMs.
Despitetherapidevolution,howtoqualifytheLVLMsafterpre-trainingisstillanunder-explored
problem, which hinders the pre-training study from being more controllable and reliable. Bench-
markresultsaftersupervisedfine-tuningisthecommonlyusedsolutioninrecentstudies,whilethe
SFTstageintroducesnon-neglectablecomputationoverheadandcomplextheprocess.
AnothernaiveideaistoborrowthemetricsfromtheLargeLanguageModels(LLMs)pre-training
(Zhao et al. (2023)) and use the loss value, perplexity, or in-context evaluation results to evaluate
it. However, as shown in Figure 1, we empirically find that these metrics are neither stable nor
reliableinthemulti-modalarea(Yinetal.(2023);Baietal.(2023);Chenetal.(2024d);McKinzie
etal.(2024)). Specifically,weconductacontrollableexperimentaboutpre-trainingdataquantityto
studytherelationbetweenthesemetricsandthebenchmarkresultsaftersupervisedfine-tuning.The
traininglossandperplexitykeepdecreasingwithtrainingdataquantityincreasing,whilethemodel
performanceisalreadysaturatedwithlessthan1Mdata. Whenitcomestoin-contextevaluation,its
performanceshowsirregularjitterwithvaringtrainingdata,leadingtomeaninglessindication.
Wearguethein-effectivenessoftheaforementionedmetricsisbasedonthepre-trainingtargetdiffer-
encebetweenLLMsandLVLMs. Theformerlearnstomodelthelanguagewhilethelatterfocuses
on closing the domain gap between two modalities. We sample images and texts from multiple
sourcesandvisualizetheirdistributionbytheembeddinglayerfeatureoftheLLaVA-v1.5(Liuetal.
(2024a)). AsshowninFigure2(a),despitethecontentdiversitywithinimagesortexts,theyshowa
relativelyuniformdistributionwithineachmodalityandacleardistributiongapbetweenmodalities.
Wefurthercalculatethelayer-wisemodalitygaponseveralleadingLVLMs(Luetal.(2024);Wang
et al. (2023); Ye et al. (2024); Liu et al. (2024b); Tong et al. (2024)) in Figure 2(b), and we have
aconsistentobservationthatthegapisclosedwiththelayerincreasing,indicatingthattheLVLMs
learntoalignthedistributionstounderstandthenewlyintroducedimagemodality.
2
2
noisnemiD
ENS-tInspiredbythis,weintroducetheModalityIntegrationRate(MIR),whichmeasuresthepre-training
qualityofLVLMsfromthedistributiondistanceperspective. TheMIRenjoysseveraladvantages:
1) Effective in representing LVLM pre-training quality, exhibiting a strong correlation with post-
SFTbenchmarkperformance. MIRnaturallyconvergesduringpre-training,offeringausefulindi-
cator to aid in identifying critical points during pre-training, such as performance saturation. For
instance, asshowninFigure1and6, Table1, itcaneffectivelyidentifythesaturationpointwhen
improving the pre-training data scale or detailedness, making it a reliable tool for early stopping,
especiallywhentrainingonlarge-scalebuthomogeneousdata.
2)Robustacrossdiversetypesofimage-textpairingsusedintheevaluation.Benefitingfromitsdis-
tributionperspectivedesign,MIRreflectsthedistributiondistanceandberobusttowardthespecific
evaluationdatasample. WeexperimentallyfindMIRisstableregardlessoftheinputtype,including
different vision and text contents (e.g., natural or document images, news or mail text), and even
irrelevantimage-textpairs. Thisrobustnessextendstoitsstabilityinhandlingdifferentconversation
templates,andensuringreliabilityeveninthefaceofoverfittingduringtraining.
3) Generalized across various training recipes, strategies, and module designs, offering flexibility
in evaluating diverse settings or configurations of LVLM pre-training. Whether adjusting training
recipes, altering architectural choices, or unlocking specific model modules, MIR helps provide
valuable insights into how these variations impact on the quality of cross-modal alignment. For
example,MIRoffersinsightintounlockingstrategiesinLVLMpre-training,particularlyhighlight-
ingthatunlockingboththeprojectorandlanguagemodelsimultaneouslycanboostthecross-modal
alignmentandpreventearlysaturationwhenscalingthepre-trainingdata.
Ultimately, MIR proves to be a versatile and consistent metric, facilitating broader LVLM pre-
trainingoptimizations. BasedonthedesignofMIR,wefurtherproposeMoCa, alightweightand
learnable calibration module for each layer’s vision tokens, designed to enhance their alignment
withtexttokens. MoCaachievestheobviouslylowMIRsonbothLLaVA-v1.5andMini-Gemini,
with+1.5%and+0.9%averagegainsonpost-SFTbenchmarkperformance.
2 METHOD
Inthissection,wegivethedetaileddefinitionaboutmodalityintegrationrate(MIR)andinvestigate
itsbasicpropertiesincludinginput-agnostic,trainingconvergence,androbustnesstooverfitting.
2.1 MODALITYINTEGRATIONRATE
ProblemDefinition. Weaimtoestablishaneffectivemetricforquantifyingthecross-modalalign-
ment quality of LVLM pretraining. This metric should accurately reflect the impact of various
pretrainingconfigurations(suchasdata,strategies,recipes,andarchitecturalchoices)onmodelper-
formance,withoutrequiringsubsequentsupervisedfine-tuningevaluationonbenchmarks. Also,it
shouldbeapplicableacrossdifferentLVLMarchitectures.
Metric Overview. Consider a pretrained LVLM M = (E,P,D) where E is the vision encoder,
P is the vision-language projector and D = (D ,F) represents the language model that consists
t
oftokenizerD andK-layertransformerdecoderF. Weinputasetofimage-textpairs(V,T) =
t
({v }N ,{t }N )tothemodel,obtainingvisiontokensfvn andtexttokensftn fromthefirstk
n n=1 n n=1 k k
layersF ofthelanguagedecoderF,i.e.,forthenthsample:
k
fvn,ftn =F (P(E(v )),D (t )), (1)
k k k n t n
wherefvn ∈ Rrn×d,ftn ∈ Rsn×d representthevisionandtexttokenrepresentationsrespectively,
k k
fromthekthlayeroutputofthenthsample,withr ,s asthenumberofvision/texttokensanddas
n n
thedimensionofhiddenstates. Tocomputetheglobaldomaingapbetweenthetwomodalities,we
furtherconcatenatethevisionandtexttokensofallimage-textpairsatthefirstdimension,deriving
fv ∈Rr×d,ft ∈Rs×d wherer =(cid:80)N r ands=(cid:80)N s . Besides,wedefinefv ∈R1×d as
k k n=1 n s=1 n k,i
theithvisiontokeninfv andft ∈R1×dasthejthtexttokeninft.
k k,j k
Ourobjectiveistomeasurethemodalitygapbetweenvisiontokensfv andtexttokensft ateach
k k
layer. Giventhetypicaldiscrepancyinthenumberofvisionandtexttokens,i.e.,r ̸=s,weleverage
3Fre´chet Inception Distance (FID) (Heusel et al. (2017)) compute the domain divergence between
thesetokenrepresentations.
Text-CentricNormalization. FIDissensitivetotheabsolutevaluesoffeatures,whichisproblem-
atic for directly applying FID to evaluate modality gaps across layers, since deeper layers tend to
exhibitlargertokenmagnitudes(asindicatedbyincreasingℓ -normvalues)inLVLMs.
2
Anaivesolutionwouldbetonormalizealltokenstoacommonscaleandcomputethedistancelike
cosinesimilarity. However,itoverlooksthesignificantabsolutevaluedisparitybetweenvisionand
text tokens, which is particularly obvious in the shallower layers. While RMSNorm within each
transformerblocksignificantlyreducesthisdisparity,skipconnectionspartiallyretainitsinfluence,
alteringthedirectionoftokenrepresentations.Toaddressthis,weadoptatext-centricnormalization
that preserves the absolute value differences between vision and text tokens, while neutralizing
the effect and enabling cross-layer FID comparison reasonable. Specifically, we first perform ℓ
2
normalizationfortexttokensofeachlayer,derivingascalingfactorαas(Ablationisinappendix):
s
α = . (2)
k (cid:80)s ||ft ||
j=1 k,j 2
By multiplying text tokens with the scaling factor α , the average ℓ -norm of the text tokens is
k 2
normalizedto1.Thereby,weequallyscalebothvisionandtexttokenswiththefactorα ,tomaintain
k
the absolute value differences between vision and text tokens and facilitate more accurate cross-
layercomparisons. Duringimplementation, weobserveoccasionaloutliersinbothvisionandtext
tokens,characterizedbyunusuallyhighℓ -normvalues.Toaddressthis,weapplyanoutlierremoval
2
functionω(·)basedon“3σ”principle,focusingonthemajorityoftokenrepresentations.
Modality Integration Rate. After the scaling and outlier removal, we calculate the FID between
visionandtexttokensateachlayertoquantifythemodalitygap,thenaggregatethisacrossalllayers
toderivetheModalityIntegrationRate(MIR),i.e.,
(cid:88)
MIR=log FID(ω(α fv),ω(α ft))
k k k k
k
(3)
=log(cid:88)(cid:2) ||µ −µ ||2+Tr(Σ +Σ −2(Σ Σ )1/2)(cid:3)
v,k t,k v,k t,k v,k t,k
k
where µ =
(cid:80) iω(αkf kv ,i)
, µ =
(cid:80) jω(αkf kt ,j)
are the mean value of the processed vision to-
v,k r′ t,k s′
kens ω(α fv) and text tokens ω(α ft) at the kth language model layer. Σ and Σ are the
k k k k v,k t,k
correspondingcovariancematricesofω(α fv)andω(α ft),whichcanbeformalizedby
k k k k
(ω(α fv)−µ )⊤(ω(α fv)−µ ) (ω(α ft)−µ )⊤(ω(α ft)−µ )
Σ = k k v,k k k v,k , Σ = k k t,k k k t,k .
v,k r′−1 v,k s′−1
(4)
Tocomputethematrixsquarerootterm(Σ Σ )1/2 inEq.(3),wetypicallyfacehighcomputa-
v,k t,k
tionalcostsduetothelargematrixdimensionsinLVLMs. Therefore,wefurtherprovideasolution
usingNewton-Schulziterationtoapproximatethesquareroot,significantlyacceleratingtheprocess
and meeting the practical needs of training indicators. Empirically, this approximation introduces
minimalimpactontheoverallMIRvalue,witherrorsgenerallyremainingbelow1%.
2.2 INVARIANCETODIVERSEINPUTS
As a metric that requires some image-text pair data as inputs, MIR is generally input-agnostic for
differenttypesofinputs,nomatterwhattypesoftheimagesorquestion-answerpairs,whatconver-
sationtemplatesareused,whetherthemodelhasseenthedataduringpretraining,andwhetherthe
textisrelevantwiththeimage. ThispropertyisquitenecessaryforthepracticalapplicationofMIR
sincewecaneasilyorrandomlychoosesomesamplesforvalidationtocomputetheMIRwithout
any bias from data types or sources. This property also proves the proposed MIR exactly reflects
thedomaingapbetweenthevisionandtextrepresentationsforaparticularmodel,i.e.,itshouldbe
specifictoaparticularmodelbutnotsensitivetodifferenttypesofimage-textinputs.
Here we present several kinds of scenarios to show MIR’s invariance to diverse inputs, with the
pretrainedmodelinLLaVA-v1.57B:
43.0 Model-Seen
3.50 3.50
Model-Unseen
2.5
3.25 3.25
3.00 3.00 2.0
2.75 2.75 1.5
2.50 2.50 Plain Template 1.0
Vicuna-v1 Template
2.25 News Text 2.25 VL-relevant 0.5
2.00 Mail Text 2.00 VL-irrelevant
0.0
Human Art Landscape OCR Template Relevance 0 5 10 15 20 25 30
Layer idx
(a)Visual/TextContents (b)Templates&Relevance (c)Seenv.s.Unseen
Figure3:MIRisrobusttodiversekindsofdata.WecomputeMIRonpre-trainedLLaVA-v1.57B
modelwithvariousinputstoverifywhetheritisinput-agnostic.In(a),weselectfourkindsofvisual
contents(human,art,landscapeandocrimages)andtwokindsoftextcontents(news,mailtext). In
(b), we compare the MIR computed on the plain/vicuna v1 conversation template, and image-text
relevant/irrelevantpairs. In(c),weselecttheimage-textpairsinpre-trainingdataas“ModelSeen”
dataandunseenpairsas“ModelUnseen”data,todepicttheper-layerMIR.
Different contents. We typically target for four different vision domains (including human, art,
landscape,ocrtextimages)andtwolanguagedomains(includingnewstextandmailtext). Foreach
typeofcombination,werandomlysampleN =100image-textpairs,wherethedataisselectedfrom
VQA datasets (such as COCO (Lin et al. (2014)), ShareGPT4V (Chen et al. (2023)), ChartVQA
(Masryetal.(2022))andDocVQA(Mathewetal.(2021)))andtextdatasets(e.g.,CNN/DM(Nal-
lapatietal.(2016)))tocomputetheMIRscores,respectively. Figure3(a)showswhenweusevery
differenttypesofvisual/textcontentstocomputeMIR,thevaluesarerelativelyconsistentandinsen-
sitivetothediverseinputs,indicatingitsrobustnessandabilitytomeasurethedomaindivergence.
Conversation templates. Most of LVLMs use their particular template inherited from LLM to
support their instruction-following ability, where LLaVA-v1.5 adopts the vicuna v1 template by
default after SFT. We try two cases, i.e., with the template and without the template, to compute
MIRusingtheinputimagesfromTextVQA(Singhetal.(2019))validationset(LLaVAhasnever
seen)andtextdatafromCNN/DM.FromFigure3(b),itisclearlythattheintroductionoftemplate
hasrelativelittleimpactonthecomputationofMIR.
Relevance between image and text. Here we explore whether the MIR value can be influenced
bythecorrespondencebetweentheimagesandtextselectedforcomputation. Weselectsame100
imagesfromCOCOandpreparetwotypesoftext,oneisthevanillacaption,theotherisirrelevant
textinCNN/DM(wherewetruncateandkeepthesamenumberoftexttokens). FromFigure3(b),
wesurprisinglyfindthattheMIRscoreskeepsimilarunderthetwokindsofinputs.ItindicatesMIR
focusesonthedomaindivergencebetweenthemodalitiesratherthanspecificcontentdifferences.
Seenv.s. Unseen. Forpracticalusage, MIRisexpectedtobeinvariantwhethertheinputsamples
forevaluationareinvolvedinthepre-trainingdata,orelsetheMIRisnotreliabletotrulyshowthe
qualityofcross-modalalignment.Tothisend,weconductacomparisonregardingusingmodel-seen
andmodel-unseendatafromCOCOtocomputetheMIRscore,respectively. FromFigure3(c),we
canobtainthatthetwocurvesarehighlyoverlapped,thusMIRisrelativelyrobustinthiscase.
Bydefault,unlesswespeciallymentioned,allofMIRcomputationinthispaperusesrandomN =
100(Ablationisinappendix)imagesfromTextVQAvalidationsetandtextdatafromCNN/DM.
2.3 TRAININGCONVERGENCE
Inadditiontobeinginvarianttodiverseinputs,agoodpre-trainingmetricshouldexhibitclearcon-
vergence behavior, similar to training loss. Ideally, it should show a sharp decline in the early
stages and gradually approach an optimal point, with very slow improvements thereafter. To ex-
plore the convergence of MIR, we followed the vanilla pre-training setup of the LLaVA-v1.5 7B
modelandanalyzetherelationshipamongmodelperformance(measuredthepost-SFTbenchmark
performance1),trainingloss,andMIR.Figure4revealthatMIRdemonstratessimilarconvergence
propertiestotrainingloss. Inparticular,bothmetricssharplydecreaseintheearlypre-trainingsteps
1Inthispaper,weadopttheaveragescoreon7popularmulti-modalbenchmarksasthepost-SFTmodelper-
formance,includingMMStar,MMBench,MMBench-cn,SEED-Img,TextVQA,ScienceQA-Img,andGQA.
5
RIM RIM
RIM
reyaL-reP59.6 4.0 59.6
3.4
3.2 59.4 3.9 59.4
3.8
3.0 59.2 59.2
3.7
2.8 Loss 59.0 3.6 MIR 59.0
2.6 Model Performance Model Performance
2.4 58.8 3.5 58.8
3.4
2.2 58.6 58.6
3.3
2.0
58.4 3.2 58.4
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Pretraining Step Pretraining Step
Figure 4: MIR exhibits the similar convergence properties with training loss, closely corre-
spondswithpost-SFTmodelperformance. Wepre-trainLLaVA-v1.57Bmodelwithitsvanilla
settingandreportthetrainingloss,MIR,andpost-SFTperformanceon7LVLMbenchmarks.
and stabilize over time. Moreover, MIR closely corresponds with post-SFT model performance,
makingitastrongindicatorofeffectivepre-training.
Thisconvergencealsoreflectsthealignmentbetweenvisionandlanguagetokensthroughoutthelan-
guagemodellayers,indicatingthecross-modalalignment’sgraduallystabilizingduringpre-training.
Additionally, MIR’sabilitytotrackconvergenceprovidespracticalvalue. ByusingMIRasapre-
training monitor, we can identify when the model has reached sufficient cross-modal alignment,
allowingforearlystoppingandreducingunnecessarytrainingcosts.
60.75
3.6 60.50
60.25
3.4
MIR 60.00
3.2 Model Performance
59.75
3.0 59.50
59.25
2.8
59.00
0 2000 4000 6000 8000
Pretraining Step
Figure5:MIRisrobusttowardoverfitting.Weconduct2-epochpre-trainingbasedonthesettings
ofShareGPT4V7Bmodel,andreporttrainingloss,MIR,andpost-SFTmodelperformance(aver-
agedon7LVLMbenchmarks)ateachsteps. Thetraininglossshowsasharpdropatthebeginning
ofthesecondepochwhilethemodelperformancedoesnot. ItshowsthatMIRismoreconsistent
withthepost-SFTmodelperformancethantraining.
2.4 ROBUSTNESSAGAINSTMODELOVERFITTING
Thoughthetraininglosscanconvergeatthefirstepoch,itusuallyshowsasharpdropatthebegin-
ningofthesecondepoch(Figure5),duetotheoverfittingespeciallywhenunlockingboththevision
encoder and the language model during LVLM pre-training. This drop in loss, however, does not
correlatewithasignificantimprovementinmodelperformance,indicatingthatthelossmetricmay
notaccuratelyreflectthepre-trainingqualityatthisstage.
To explore the robustness of MIR in face of such model overfitting, we conduct the evaluation on
the stronger baseline ShareGPT4V, which curates around 1.2M detailed image captions for pre-
trainingandunlocksbothlatterhalfvisionencoderandthewholeLLMforbettercomprehending
the detailed semantics. Two epochs of pre-training are performed, and both MIR, training loss,
andpost-SFTmodelperformance(averagedacross7benchmarks)arerecordedateachstep. From
Figure5,itisevidentthatwhilethetraininglossdropssignificantlyatthestartofthesecondepoch,
themodelperformanceremainsstableaftertheconvergencereachedduringthefirstepoch.Thislack
ofcorrelationhighlightshowtraininglossfailstoserveasareliableindicatorofperformanceduring
overfitting scenarios. In contrast, MIR more closely aligns with model performance, maintaining
convergence from the end of the first epoch without drastic fluctuations. This result indicates the
robustnessofMIRinfaceofthemodeloverfitting,exhibitingamorereliableindicatorthantraining
lossformonitoringthetrainingstatesofLVLMs.
6
ssoL
ecnamrofreP
ledoM
RIM
RIM
ecnamrofreP
ledoM
ecnamrofreP
ledoM445 ... 570 050 59.5 59.9 60.0 59.6 60.1 59.8 60.0 59.8 556 990 ... 050 445 ... 570 050 67.5 67.9 67.8 67.8 67.8 68.1 67.8 66 78 .. 50 445 ... 570 050 62.8 62.9 62.8 62.8 62.9 62.8 62.9 66 22 .. 68
4.25 MIR 58.5 4.25 67.0 MIR 67.0 4.25 62.4 MIR 62.4
4.00 58.1 Avg 7 Benchmarks 58.0 4.00 SEED-Img 66.5 4.00 GQA 62.2 33 .. 57 05 55 77 .. 05 33 .. 57 05 66.2 66.0 33 .. 57 05 62.1 62.0
3.25 56.5 56.5 3.25 65.3 65.5 3.25 61.8 61.8
100K 200K 400K 600K 800K 1M 1.2M 1.4M 1.6M 1.8M 100K 200K 400K 600K 800K 1M 1.2M 1.4M 1.6M 1.8M 100K 200K 400K 600K 800K 1M 1.2M 1.4M 1.6M 1.8M
Pretrain Data Scale Pretrain Data Scale Pretrain Data Scale
61.2 61.2 61.2 69.2 63.4 63.4 63.4
3.2 61.0 3.2 68.8 69.0 3.2 63.3 63.3 3.1 60.7 60.8 3.1 68.4 68.5 3.1 63.2 63.2 223 ... 890
60.0 60.1
60.5 60.3 60.5 60.3 6666 0000 .... 0246 223 ... 890
67.4 67.6
68.2 68.1 68.0 67.9 66 78 .. 50 223 ... 890 63.0 62.9 63.0 63.0 66 23 .. 80
MIR 59.8 MIR 67.0 MIR 62.6
2.7 59.6 Avg 7 Benchmarks 59.6 2.7 66.7 SEED-Img 2.7 62.5 GQA
100K 200K 400K 600K 800K 1M 1.2M 1.4M 1.6M 1.8M 100K 200K 400K 600K 800K 1M 1.2M 1.4M 1.6M 1.8M 100K 200K 400K 600K 800K 1M 1.2M 1.4M 1.6M 1.8M
Pretrain Data Scale Pretrain Data Scale Pretrain Data Scale
Figure6: MIRasaneffectiveevaluatorwhenscalingpre-trainingdata. WeuseALLaVAand
ShareGPT4V-PTas∼1.8Mper-trainingdataandtraineachmodelwithdifferentscaleofdata. The
firstrowshowcasestheresultsofvanillaLLaVA-v1.57Bmodel,whereonlytheMLPprojectoris
trained. ThesecondrowshowcasestheresultsofShareGPT4V,wherethelatterhalfofViTandthe
wholeLLMarealsotrained. MIRwellalignswiththetrendofthepost-SFTmodelperformance.
3 EXPERIMENT
Inthissection, weexploreseveralkeyapplicationsofMIR,provinghowithelpsoptimizeconfig-
urationsforLVLMpre-training. Wefocusonfourscenarios: 1)Unveilingthe performanceupper
bound when scaling pre-training data; 2) Evaluating the impact of text detailedness on the quality
of LVLM pre-training; 3) Exploring the potential of MIR to select the optimal training recipes or
strategies;4)VerifyingtheeffectivenessofdifferentmoduledesignsforLVLMpre-training..
Our experiments involve three fully open-sourced LVLMs: LLaVA-v1.5, ShareGPT4V, and Mini-
Gemini(Lietal.(2024)),focusingontheir7Bmodelvariants.Forevaluation,weselected9popular
multi-modalbenchmarks, includingMMStar(Chenetal.(2024b)), MME(Fuetal.(2023)), MM-
Bench (Liu et al. (2023)), MMBench-cn, SEED-Img (Li et al. (2023a)), TextVQA (Singh et al.
(2019)), ScienceQA-Img (Lu et al. (2022)), POPE (Li et al. (2023c)), GQA (Hudson & Manning
(2019))andMM-Vet(Yuetal.(2023)). Thesebenchmarkscomprehensivelytestbothcoarse-and
fine-grainedcapabilitiesofLVLMs,providingaholisticviewofperformanceacrossvarioustasks.
3.1 SCALINGPRE-TRAININGDATAINLVLMS
Typically, increasingtheamountoftrainingdataimprovesmodelperformanceandgeneralization.
Here,wedemonstratetheeffectivenessofMIRinmeasuringpre-trainingqualitywhenscalingthe
amount of pre-training data. We use LLaVA-v1.5 7B as the base model, and curate two datasets,
ALLaVAandShareGPT4V-PT,comprising∼1.8Mimage-textpairs. Wethenpre-trainedmultiple
modelsusingdifferentsubsetsofthisdatasettoexploretherelationshipbetweenpre-trainingdata
scaleandmodelperformance,withMIRservingastheevaluationmetric.
Twodifferentpre-trainingstrategiesareemployedtoinvestigatethisscalinglaw: 1)VanillaLLaVA-
v1.5 setting: Only the MLP projector is trained while the rest of the model is frozen, with the
learningrateof1e-3.2)VanillaShareGPT4Vsetting:Weunlockthelatterhalfofthevisionencoder
aswellastheentireLLM,withalearningrateof2e-5. Afterpre-training,wefurtherequallyapply
supervisedfine-tuningtoallmodelsonLLaVA’s665KSFTdata,usingalearningrateof2e-5. Post-
SFTmodelperformanceisevaluatedacross7benchmarkstoverifytherelationshipbetweenMIR
andpre-trainingquality,showcasingtheeffectivenessofMIRasanindicator.
AsshowninFigure6,whenonlytheMLPprojectoristrained(thefirstrow,LLaVA-v1.5),thepost-
SFTperformancegraduallyimprovesbutplateausat800K∼1Mdatascale,indicatingabottleneck
infurtherenhancingcross-modalalignment. Incontrast, whenthevisionencoder, MLPprojector,
andLLMarealljointlytrained(thesecondrow,ShareGPT4V),continuestoincreasesignificantly,
7
RIM
RIM
skramhcneB
7 gvA
skramhcneB
7 gvA
RIM
RIM
gmI-DEES
gmI-DEES
RIM
RIM
AQG
AQGTable1: MIRasaneffectiveevaluatorwhenimprovingthedatadetailedness. Wetruncatethe
longcaptionsinALLaVAandShareGPT4V-PTonthesentence-level,toconstructthepre-training
datawithvaringdegreesofdetailedness.MIRshowsstrongcorrelationwithpost-SFTperformance.
CaptionLen MIR↓ Average MMStar MME MMB MMBCN SEEDI TQA SQAI POPE GQA
15.2 3.588 63.8 33.0 1488.1 65.8 59.5 66.8 57.7 69.4 86.0 62.4
49.1 3.442 64.0 33.7 1500.2 65.9 58.2 67.5 59.0 68.3 86.0 62.4
127.1 3.279 64.2 34.8 1472.6 66.2 58.9 67.6 59.0 68.7 85.8 62.7
181.2 3.218 64.4 35.5 1491.8 65.4 57.6 67.6 59.7 69.8 86.0 63.0
evenat1.8Mdatascale. Fromtheseresults,wecandrawthefollowinginsights: 1)MIRservesas
aneffectiveindicatorwhenscalingpre-trainingdata. 2)Appropriatelyunlockingvisionencoderor
LLMallowsforcontinuedimprovementforLVLMpre-trainingonlarger-scaledata.
3.2 IMPROVINGDATADETAILEDNESSINLVLMPRE-TRAINING
Leveraging the capabilities of MIR, we further explore how varying levels of caption detailedness
inpre-trainingdataaffectLVLMperformance. WeselectLLaVA-v1.5asthebasemodel, anduse
∼1.8Mimage-textpairsfromALLaVAandShareGPT4V-PTasthepre-trainingdata. Toconstruct
differentdegreesofcaptiondetailednessofpre-trainingdata,wetruncatetheoriginallongcaptions
tovariouslengthsonethesentence-level, thusgeneratingcaptionswithvaryinglevelsofdetailed-
ness. Thepre-trainingprocedurefollowsthedefaultconfigurationofLLaVA-v1.5,whereonlythe
MLPprojectoristrainedwithalearningrateof1e-3.
AsshowninTable1,theresultsindicatethatmodelstrainedonmoredetailedcaptionstendtohave
lowerMIRvalues,whichcorrelateswithimprovedpost-SFTperformance. Notably,whenincreas-
ingtheaveragecaptionlengthfrom15.2to49.1,theoverallmodelperformanceimprovessincethe
captionscanhelpmodelcomprehendmoresemanticsinthegivenvisualcontents. However,when
further increasing the average caption length from 127.1 to 181.2, the model’s global reasoning
abilityappearstoplateau(asseenintheSEED-Imgbenchmark),whileitsfine-grainedcapabilities,
particularlyintaskslikeTextVQA,continuetoshowsignificantimprovement.
3.3 OPTIMIZINGTRAININGRECIPESORSTRATEGIES
Differenttrainingstrategiesandconfigurationsarealsoacrucialfactorforenhancingthequalityof
LVLM pre-training (Lin et al. (2024)). Here we examine the effectiveness of MIR in optimizing
the hyper-parameters and selecting the optimal unlocking strategies during the pre-training phase.
To maintain consistency, we use the LLaVA-v1.5 7B model as the base model and standardize
the supervised fine-tuning (SFT) across all experiments using LLaVA-v1.5’s default setup, which
includes665KGPT-generatedSFTdata. Thisallowsustoisolateandanalyzetheimpactofvarious
trainingstrategiesandconfigurationsonthepre-trainingstage.
Training recipes. We investigate how MIR helps optimize training recipes without requiring ad-
ditionalwithoutfurtherSFT.ThebaselinesettingisLLaVA-v1.5’sofficialpre-trainingwith558K
BLIP-2-generatedimage-captionpre-trainingdata. Weconsiderthehyper-parametersincludingthe
learningrate(LR),warmupratioandtheschedulertypeoflearningratedecay.FromTable2,wecan
observethepositiverelationbetweenMIRandthepost-SFTbenchmarkperformance. Specifically,
a lower MIR reflects the effectiveness of different training configurations on pre-training quality,
particularlywithstablebenchmarkslikeSEED-Img.
Trainingstrategies. WeexplorehowMIRcanguidetheselectionofeffectiveunlockingstrategies
duringpre-training. Consideringthe558KBLIP-2generatedcaptionsusedinvanillaLLaVA-v1.5
arerelativelyshort,wecurateALLaVAandShareGPT4V-PTas∼1.9Mlong-captiondataforpre-
training,followingShareGPT4V’srecipewithalearningrateof2e-5. Forunlockingstrategies,we
focusonunlockingtheMLPprojectorandvariouspartsoftheLLM,includingLoRA,thefirsthalf
ofLLM,andtheentireLLM.AsshowninTable3, unlockingtheLLM’sparameterssignificantly
reducedMIRandenhancedthemodel’smulti-modalcapabilities,indicatingastrongcorrelationbe-
tweenMIRandpretrainingquality. Theseresultssuggestthat,whenpre-trainingonhighlydetailed
image-text data, unlocking the former half of LLM or the entire LLM can significantly improve
8Table2: MIRasaneffectiveevaluatorwhenoptimizingpre-trainingrecipes. Wetrydifferent
sets of hyper-parameters (learning rate (LR), warmup ratio, and learning rate decay scheduler) to
pre-train LLaVA-v1.5 7B models, following its official setting. MIR has strong positive relation
withthepost-SFTbenchmarkperformance.
LR Warmup LRscheduler MIR↓ Average MMStar MMB MMBCN SEEDI TQA SQAI GQA
1e-3 3e-2 cosine 3.182 59.5 33.8 65.9 59.0 66.5 58.5 69.6 62.9
1e-3 5e-2 cosine 3.043 59.6 34.2 65.7 60.0 66.9 58.5 69.0 62.7
1e-3 3e-2 cosine 3.182 59.5 33.8 65.9 59.0 66.5 58.5 69.6 62.9
1e-3 3e-2 linear 3.171 59.5 34.9 65.6 59.5 66.7 58.4 68.3 62.8
3e-3 3e-2 cosine 3.575 58.7 33.0 64.9 59.0 65.1 58.4 68.8 62.0
1e-3 3e-2 cosine 3.182 59.5 33.8 65.9 59.0 66.5 58.5 69.6 62.9
5e-4 3e-2 cosine 2.990 60.0 34.6 66.8 59.5 66.8 58.8 70.3 62.9
3e-4 3e-2 cosine 2.808 60.0 35.3 67.1 59.8 67.0 58.6 69.5 62.8
Table3: MIRasaneffectiveevaluatorwhenoptimizingpre-trainingstrategies. Using∼1.9M
datafromALLaVAandShareGPT4V-PT,wetrytounlocktheMLPprojectorandcariouspartsof
the LLM (including LoRA (Hu et al. (2021)), the former half of LLM, and the entire LLM), and
equallyapplySFTonLLaVA’s665KSFTdata. “w/merge”meansmergingtheLoRAweightswith
themodelweightsafterpre-training,viceversa.
UnlockLLM MIR↓ Average MMStar MME MMB MMBCN SEEDI TQA SQAI POPE GQA
- 3.001 64.0 33.3 1479.6 66.7 59.5 67.1 58.8 68.8 85.5 62.6
LoRAw/omerge 2.735 64.3 33.4 1504.6 65.5 59.5 67.7 59.0 69.2 86.0 62.8
LoRAw/merge 2.734 64.5 33.4 1502.6 66.4 60.5 67.5 58.9 69.5 86.0 62.8
FormerHalf 2.705 65.9 34.1 1564.5 66.8 62.1 69.2 60.1 72.4 86.5 63.3
AllLayers 2.656 65.9 36.0 1554.3 66.3 62.2 69.0 60.9 71.7 86.1 63.2
the model’s ability to bridge the modality gap between vision and language, facilitating the better
downstreamperformanceafterSFT.
3.4 EXPLORINGMODULEDESIGNSINLVLMS
Vision-language connector. The architectural design of vision-language connector in LVLMs is
critical,sinceitplaysaroleinprojectingvisionfeaturesintothelanguagespaceandnarrowingthe
modalitygap. PreviousLVLMstypicallyadopttwokindsofclassicalvisual-languageconnectors,
i.e., MLP and Q-Former (Li et al. (2023b)). The MLP utilizes several linear layers to map visual
tokensintothetextspace,whiletheQ-Formerleveragescross-attentiontoabsorbinstruction-aware
informationfromthevisualtokens.HereweleveragetheproposedMIRtoquantifytheeffectiveness
ofdifferenttypesofthevision-languageconnectoronLVLMtraining. FollowingtheLLaVA-v1.5
setup,weadoptCLIP-ViT-L/336(Radfordetal.(2021))asthevisionencoderandVicunav1.5asthe
LLMbydefault. Usingthepre-trainingandSFTdataofLLaVA-v1.5,wefirstpre-traineachtypeof
thevision-languageconnectorwiththelearningrateof1e-3,keepingthevisionencoderandLLM
frozen. Afterward, we unlock the LLM to allow joint training with the vision-language connector
during instruction tuning. For a fair comparison between MLP and Q-Former, we initialize Q-
Former using a BERT-Base checkpoint, without employing any additional warm-up stages to pre-
alignQ-FormerwiththevisionencoderlikeBLIP-2.
From Table 4, using an MLP as the vision-language connector significantly outperforms the Q-
Former. The2-layerMLPprojectorprovestobetheoptimalchoice,achievingthelowestMIRand
thehighestpost-SFTperformance. ThelowerMIRscoresuggeststhatMLPfacilitatesbettercross-
modalalignmentthantheQ-Former,allowingittomoreeffectivelycomprehendvisualinformation.
The positive correlation between MIR and the effectiveness of different vision-language connec-
torsindicatesthatMIRisareliablemetricforselectingoptimalmoduledesignsinLVLMtraining
withoutrelyingonSFT.
LearnableModalityCalibration(MoCa). TheobservationinFigure2showsthebaseLLMsof
LVLMstendtograduallynarrowthemodalitygapwhenvisionandtexttokensarepassedthrough
9Table 4: MIR as an effective evaluator in selecting module designs. We study the impact of
different vision-language connectors on LLaVA-v1.5 training, where we initialize Q-Former with
pre-trained BERT-Base (Devlin (2018)) and keep only two stages (pre-training and SFT) for fair
comparison. MIRpreciselyreflectstheoptimalmoduledesignwithoutSFT.
VLConnector MIR↓ Average MMStar MME MMB MMBCN SEEDI TQA SQAI POPE GQA
1-layerMLP 3.454 63.6 33.0 1439.7 65.9 59.0 66.1 58.3 69.4 86.0 62.5
2-layerMLP 3.182 63.9 33.8 1446.8 65.9 59.0 66.5 58.5 69.6 86.2 62.9
4-layerMLP 3.446 63.7 33.7 1436.4 65.6 59.2 66.2 58.7 69.6 86.1 62.5
Q-Former 3.673 53.7 26.3 1175.6 56.6 49.5 51.3 45.4 67.6 76.7 51.3
Table5: TheeffectivenessofMoCa. MoCaachieveslowerMIRsand+1.5%averagebenchmark
performance on LLaVA-v1.5, as well as +0.9% on Mini-Gemini. Here we report the reproduced
resultsofMini-Geminisincethedatalinksprovidedbytheofficialarepartiallyunavailable.
7BModel MIR↓ Average MMStar MME MMB MMBCN SEEDI TQA MM-Vet POPE GQA
LLaVA-v1.5 3.374 59.1 30.3 1510.7 64.3 58.3 66.1 58.2 31.1 85.9 62.0
+MoCa 3.162 60.6 36.5 1481.0 66.8 60.0 67.0 58.7 32.2 86.9 62.8
Mini-Gemini 2.667 62.1 34.1 1502.8 67.5 58.2 69.5 65.2 40.8 86.1 62.3
+MoCa 2.514 63.0 35.9 1520.5 68.3 60.2 69.6 65.6 42.9 86.5 62.4
the deeper layers, even though these tokens are not well-aligned when initially fed into the base
LLM. It drives us to rethink certain designs in LVLMs that are inherited from LLMs but may be
unsuitedforpromotingcross-modalalignment.Onesuchdesignistheuseofidenticalnormalization
forbothvisionandtexttokensateachLLMlayer.Sincethenormalizationispre-trainedonlanguage
data, it is inherently biased toward text processing, which disrupts vision information and hinders
effective cross-modal alignment during training. Therefore, we consider to insert a light-weight
learnable module to facilitate such alignment while preserving the language priors in the original
LLMnormalizationmodules.
To this end, we propose MoCa, a simple yet effective calibration method specially designed for
vision tokens, to help LVLMs automatically adjust the distribution of vision tokens to align more
closelywiththedistributionoftexttokens. Specifically,givenvisiontokensfv andtexttokensft
k k
inthehiddenstatesofkth LVLMlayeroutput,weapplyalearnablescalingvectoru∈R1×d tofv
k
andadditwithalearnableshiftingvectorv ∈R1×dbeforepassingittothenextlayer,i.e.,
ψ(fv)=u·fv+v, (5)
k k
whereψisthelearnablecalibrationmodule,appliedexclusivelytovisiontokensattheendofeach
LLMlayer. Thevectoru,vareinitializedasanall-onesvectorandanall-zerosvectorrespectively.
WeempiricallyvalidateMoCa’seffectivenessonthe7BmodelsofLLaVA-v1.5andMini-Gemini,
following their official training configurations. MoCa is integrated into both the pre-training and
supervisedfine-tuning(SFT)stages. FromTable5,MoCaachievessignificantlylowerMIRscores
on both models, with average post-SFT performance gains of 1.5% for LLaVA-v1.5 and 0.9% for
Mini-Gemini. It provides the significant gains on MMStar, demonstrating the learnable vectors
effectively bring vision features closer to the distribution of text tokens, ultimately enhancing the
model’sabilitytobettercomprehendandprocessvisualinputs.
4 CONCLUSION
ThispaperintroducesModalityIntegrationRate(MIR),anovelandeffectivemetricforevaluating
cross-modalalignmentduringthepre-trainingofLVLMs.Bycapturingdomaindifferencesbetween
vision and language features across all layers of the language model, MIR provides an effective
andreliablemeasureofpre-trainingqualitycomparedtotraditionalmetricslikelossorperplexity.
It demonstrates its good robustness, generalization ability, and the strong correlation with post-
SFTperformance,offeringvaluableinsightsforoptimizingarchitecturedesignsandtrainingsetup.
ComplementingMIR,weproposeMoCa,alightweight,learnablecalibrationmodulethatenhances
thealignmentofvisionandtexttokens,ultimatelydrivingbettermulti-modalcomprehension.
10REFERENCES
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguage
model for few-shot learning. Advances in neural information processing systems, 35:23716–
23736,2022.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou,andJingrenZhou.Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities.
arXivpreprintarXiv:2308.12966,2023.
GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,Zhi-
hongChen,JianquanLi,XiangWan,andBenyouWang. Allava: Harnessinggpt4v-synthesized
dataforalitevision-languagemodel. arXivpreprintarXiv:2402.11684,2024a.
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint
arXiv:2311.12793,2023.
LinChen, JinsongLi, XiaoyiDong, PanZhang, YuhangZang, ZehuiChen, HaodongDuan, Jiaqi
Wang,YuQiao,DahuaLin,etal. Areweontherightwayforevaluatinglargevision-language
models? arXivpreprintarXiv:2403.20330,2024b.
Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong
Duan,BinLin,ZhenyuTang,etal. Sharegpt4video: Improvingvideounderstandingandgenera-
tionwithbettercaptions. arXivpreprintarXiv:2406.04325,2024c.
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong,
KongzhiHu,JiapengLuo,ZhengMa,etal. Howfararewetogpt-4v? closingthegaptocom-
mercialmultimodalmodelswithopen-sourcesuites. arXivpreprintarXiv:2404.16821,2024d.
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligning
for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pp.24185–24198,2024e.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
BoyangLi,PascaleFung,andStevenHoi.Instructblip:Towardsgeneral-purposevision-language
modelswithinstructiontuning,2023.
Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding.
arXivpreprintarXiv:1810.04805,2018.
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
AyzaanWahid,JonathanTompson,QuanVuong,TianheYu,etal. Palm-e: Anembodiedmulti-
modallanguagemodel. arXivpreprintarXiv:2303.03378,2023.
ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,Xiawu
Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal
largelanguagemodels. arXivpreprintarXiv:2306.13394,2023.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. Advancesin
neuralinformationprocessingsystems,30,2017.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685,2021.
DrewAHudsonandChristopherDManning. Gqa: Anewdatasetforreal-worldvisualreasoning
andcompositionalquestionanswering. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pp.6700–6709,2019.
11Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint
arXiv:1909.09436,2019.
GabrielIlharco,MitchellWortsman,RossWightman,CadeGordon,NicholasCarlini,RohanTaori,
Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali
Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/
zenodo.5143773. Ifyouusethissoftware,pleaseciteitasbelow.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting lan-
guageandvisionusingcrowdsourceddenseimageannotations.Internationaljournalofcomputer
vision,123:32–73,2017.
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench: Bench-
marking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125,
2023a.
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-imagepre-
trainingforunifiedvision-languageunderstandingandgeneration.InInternationalconferenceon
machinelearning,pp.12888–12900.PMLR,2022.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InInternationalconference
onmachinelearning,pp.19730–19742.PMLR,2023b.
YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini: Miningthepotentialofmulti-modalityvisionlanguagemodels.
arXivpreprintarXiv:2403.18814,2024.
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating
objecthallucinationinlargevision-languagemodels. arXivpreprintarXiv:2305.10355,2023c.
JiLin,HongxuYin,WeiPing,PavloMolchanov,MohammadShoeybi,andSongHan.Vila:Onpre-
trainingforvisuallanguagemodels. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.26689–26699,2024.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings,PartV13,pp.740–755.Springer,2014.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.Improvedbaselineswithvisualinstruction
tuning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion,pp.26296–26306,2024a.
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,January2024b.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advances
inneuralinformationprocessingsystems,36,2024c.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,TongzhengRen,
ZhuoshuLi,YaofengSun,etal. Deepseek-vl:towardsreal-worldvision-languageunderstanding.
arXivpreprintarXiv:2403.05525,2024.
PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,
PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathoughtchainsfor
sciencequestionanswering.AdvancesinNeuralInformationProcessingSystems,35:2507–2521,
2022.
12AhmedMasry, DoXuanLong,JiaQingTan, ShafiqJoty, andEnamulHoque. Chartqa: Abench-
mark for question answering about charts with visual and logical reasoning. arXiv preprint
arXiv:2203.10244,2022.
MineshMathew,DimosthenisKaratzas,andCVJawahar. Docvqa: Adatasetforvqaondocument
images. InProceedingsoftheIEEE/CVFwinterconferenceonapplicationsofcomputervision,
pp.2200–2209,2021.
MineshMathew,VirajBagal,Rube`nTito,DimosthenisKaratzas,ErnestValveny,andCVJawahar.
Infographicvqa.InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputer
Vision,pp.1697–1706,2022.
BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,PhilippDufter,
DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis&insights
frommultimodalllmpre-training. arXivpreprintarXiv:2403.09611,2024.
RameshNallapati,BowenZhou,CaglarGulcehre,BingXiang,etal.Abstractivetextsummarization
usingsequence-to-sequencernnsandbeyond. arXivpreprintarXiv:1602.06023,2016.
Maxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Learning
robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu
Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824,2023.
BryanAPlummer,LiweiWang,ChrisMCervantes,JuanCCaicedo,JuliaHockenmaier,andSvet-
lanaLazebnik. Flickr30kentities: Collectingregion-to-phrasecorrespondencesforricherimage-
to-sentencemodels. InProceedingsoftheIEEEinternationalconferenceoncomputervision,pp.
2641–2649,2015.
YuxuanQiao,HaodongDuan,XinyuFang,JunmingYang,LinChen,SongyangZhang,JiaqiWang,
DahuaLin,andKaiChen. Prism: Aframeworkfordecouplingandassessingthecapabilitiesof
vlms. arXivpreprintarXiv:2406.14544,2024.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,
AarushKatta,TheoCoombes,JeniaJitsev,andAranKomatsuzaki. Laion-400m:Opendatasetof
clip-filtered400millionimage-textpairs. arXivpreprintarXiv:2111.02114,2021.
AmanpreetSingh, VivekNatarajan, MeetShah, YuJiang, XinleiChen, DhruvBatra, DeviParikh,
and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pp.8317–8326,2019.
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training
techniquesforclipatscale. arXivpreprintarXiv:2303.15389,2023.
Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha
Akula,JihanYang,ShushengYang,AdithyaIyer,XichenPan,etal. Cambrian-1: Afullyopen,
vision-centricexplorationofmultimodalllms. arXivpreprintarXiv:2406.16860,2024.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
layBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfounda-
tionandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,ZhuoyiYang,
Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv
preprintarXiv:2311.03079,2023.
13QinghaoYe, HaiyangXu, GuohaiXu, JiaboYe, MingYan, YiyangZhou, JunyangWang, Anwen
Hu,PengchengShi,YayaShi,etal.mplug-owl:Modularizationempowerslargelanguagemodels
withmultimodality. arXivpreprintarXiv:2304.14178,2023.
QinghaoYe,HaiyangXu,JiaboYe,MingYan,AnwenHu,HaoweiLiu,QiQian,JiZhang,andFei
Huang. mplug-owl2: Revolutionizingmulti-modallargelanguagemodelwithmodalitycollabo-
ration.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pp.13040–13051,2024.
ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,TongXu,andEnhongChen. Asurveyon
multimodallargelanguagemodels. arXivpreprintarXiv:2306.13549,2023.
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,
andLijuanWang. Mm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities. arXiv
preprintarXiv:2308.02490,2023.
XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguage
image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer
Vision,pp.11975–11986,2023.
Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao,
Haodong Duan, Songyang Zhang, Shuangrui Ding, et al. Internlm-xcomposer: A vision-
language large model for advanced text-image comprehension and composition. arXiv preprint
arXiv:2309.15112,2023.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv
preprintarXiv:2303.18223,2023.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592,2023.
14A RELATED WORK
A.1 VISION-LANGUAGEFOUNDATIONMODEL.
Vision-LanguageModels(VLMs)haveemergedasasignificantadvancementinnowadays’multi-
modallearning,capableofunderstandingandgeneratinghuman-likeresponsesbasedonvisualand
textualinputs. EarlymodelslikeCLIP(ContrastiveLanguage–ImagePre-training)(Radfordetal.
(2021))marksapivotalmomentbyaligningimagesandtextinasharedembeddingspace,enabling
thestrongcross-modalunderstanding.FollowingCLIP,modelslikeBLIP(BootstrappingLanguage-
ImagePre-training)(Lietal.(2022;2023b))extendsthisfoundation,enhancingthefusionofvision
andlanguagemodalitiesbyleveragingmorecomplexpre-trainingobjectives. Asthecapabilitiesof
LargeLanguageModels(LLMs)(Zhaoetal.(2023);Touvronetal.(2023))progressed,theirinte-
grationwithvisionmodelsgaverisetomorepowerfulinstruction-followingLargeVision-Language
Models(LVLMs)(Liuetal.(2024c;a;b);Zhuetal.(2023);Daietal.(2023);Baietal.(2023);Zhang
etal.(2023);Chenetal.(2024c);Qiaoetal.(2024)). EarlymodelssuchasFlamingo(Alayracetal.
(2022)) and PaLM-E (Driess et al. (2023)), and more recent ones like LLaVA (Liu et al. (2024c))
andQwen-VL(Baietal.(2023)),exemplifythistrend.
Most LVLMs share three essential components: the vision encoder, the vision-language connec-
tor, and the language decoder. The vision encoder is responsible for extracting precise features
fromimages,capturingbothdetailedandabstractvisualinformation. PopularchoicesincludeCLIP
(Radford et al. (2021)), OpenCLIP (Ilharco et al. (2021)), EVA-CLIP (Sun et al. (2023)), SigLIP
(Zhai et al. (2023)) and DINO series (Oquab et al. (2023)), which are designed to provide both
coarse-grainedandfine-grainedvisualguidance.Thevision-languageconnectorplaysacriticalrole
inmappingtheencodedvisualfeaturesintoaformatthatcanbeinterpretedbythelanguagemodel.
Common designs include simple MLP projectors and the Q-Former used in BLIP-2, while more
advancedsolutions,suchasthevisionabstractorinmPLUG-Owl(Yeetal.(2023))andQLLaMAin
Intern-VL(Chenetal.(2024e)),pushtheboundariesofcross-modalalignment. Thelanguagede-
coderistypicallyapre-trainedLLMdesignedtohandlelarge-scalelanguagedata,ensuringthatthe
modelhasrobustinstruction-followingandconversationalabilities. However,thecentralchallenge
inbuildingastrongLVLMliesinbridgingthemodalitygapbetweenvisionandlanguage. Thegoal
istoensurethatthelanguagedecodercanprocessvisualtokensasnaturallyasitdoeslanguageto-
kens,enablingsmoothandmeaningfulconversationswithmulti-modalinputs. Thiscrucialprocess
istypicallyaddressedduringthepre-trainingstageofLVLMdevelopment. Inthispaper,wefocus
on evaluating and improving cross-modal alignment during the pre-training of LVLMs, a critical
step in enhancing their overall performance and ensuring seamless interaction between visual and
textualmodalities.
A.2 CROSS-MODALALIGNMENTINLVLMS.
Cross-modalalignmentplaysapivotalroleinbuildingastrongLVLMthatcanwellsupportusers
toinputimages/videosandthemodelcanunderstandthemulti-modalcontents. Fortheconnector
module of cross-modal alignment, there are typically three types widely used in current LVLMs:
1)Flamingo-style(Alayracetal.(2022)). Theperceiverresamplerprojectsthevisionfeaturesinto
the fixed number of vision tokens, and the language decoder captures the vision information by
introducing cross-attention in Gated XATTN-DENSE layer. 2) BLIP-2-style (Li et al. (2023b)).
AQ-Formertoextracttheinstruction-awareinformationfromvisiontokensthroughcross-attention
andpasstheextractedtokenstothelanguagedecoder.3)LLaVA-style(Liuetal.(2024c)).Asimple
MLPprojectordirectlymapthevisiontokensintothetextembeddingspace.
Current Large Vision-Language Models (LVLMs) typically undergo a pre-training stage specifi-
cally designed for cross-modal alignment. As a result, the quality of the pre-training data and the
strategies employed are critical for enhancing this alignment. Early datasets, such as COCO (Lin
et al. (2014)), Flickr30k (Plummer et al. (2015)), and LAION-400M (Schuhmann et al. (2021)),
focus on short captions describing visual content. More recent datasets like ShareGPT4V (Chen
etal.(2023))andALLaVA(Chenetal.(2024a))featurelongercaptions, aimingtoprovidericher
descriptions to encourage the model to fully utilize the dense information of vision tokens. Be-
sides, some works have shown that incorporating grounding information (Peng et al. (2023)) or
densepriorsKrishnaetal.(2017)inthecaptionsfurtherenhancesLVLMs’abilitytocomprehend
15visualinputs.High-qualitydataplaysakeyroleinimprovingthecross-modalalignmentinLVLMs,
drivingadvancementsinmulti-modalunderstanding.
B APPENDIX EXPERIMENTS
B.1 THENECESSITYOFTEXT-CENTRICNORMALIZATIONINMIR
The computation of our MIR requires text-centric normalization for both vision tokens and text
tokens. Thisdesignensuresfairnessincross-layercomparisonsofMIR,asFIDvaluesaresensitive
totheabsolutemagnitudesoftheinputs. Toexplorethisfurther,weablatedthescalingfactorused
inMIRcomputation,andtheresultsareshownbelow:
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5 w/
w/o
0.0
0 5 10 15 20 25 30
Layer idx
Figure7:Text-centricnormalizationisnecessaryforMIRcomputation.WeablatetheαinMIRand
findthatitcanhelpMIRtorealizethefaircross-layercomparison.
Withouttext-centricnormalization,theMIRsacrossdifferentlayersofthelanguagemodelexhibit
apatternoffirstdecreasingandthenincreasing,withthefinalMIRevenhigherthanthatofthefirst
layer. Thisiscounterintuitivebecausethedeepestlayerisclosesttothelanguagesupervision,and
thevision/texttokensatthatlayershouldbemoretightlyaligned. Forexample,ifweattempttofind
theclosesttextembeddingsforthevisiontokensinthedeepestlayeracrossthevocabulary,wewill
observemuchmoresemanticalignmentcomparedtothevisiontokensinthefirstlayer. Therefore,
without text-centric normalization, MIRs across layers become incomparable due to differences
in absolute values, rendering cross-layer MIR comparisons unfair. Hence, applying text-centric
normalizationinMIRisessentialformeaningfulcomparisons.
B.2 ISMIRSENSITIVETOTHENUMBEROFDATASAMPLE?
As we clarified in the Method, we use 100 random selected images from TextVQA validation set
and text data from CNN/DM for MIR calculation. Hereby, we explore the sensitivity of MIR to
thenumberofdatasamples. Werandomlychoose10setsofthecertainnumberofdatasamplesto
computeMIRforpre-trainedLLaVA-v1.57Bmodel,reportingtheaveragevaluesandrangesunder
differentdatasamplenumbers.
Theresultsareasbelow:
Table6: ThemeanvalueofMIRgraduallybecomesstablewiththeincreaseofsamplenumber.
#Samples 1 5 10 20 50
LLaVA-v1.57B 3.380 3.358 3.377 3.379 3.374
#Samples 100 200 500 800 1000
LLaVA-v1.57B 3.375 3.376 3.376 3.376 3.376
16
RIM
reyaL-reP3.8
Average MIR
3.7 Range
3.6
3.5
3.4
3.3
3.2
3.1
3.0
1 5 10 20 50 100 500
Number of Samples
Figure8:ThefluctuationamplitudeofMIRgraduallydecreaseswiththeincreaseofsamplenumber.
Itcanbeconcludedthat,ifweusemorethan20samplestocomputeMIR,thefluctuationrangeis
relatively small and we just need to compute MIR for one times as the negligible error, instead of
computingformultipletimestogetaveragevalue. Overall,MIRisrelativelyrobusttothenumber
ofdatasamples,whichiseffectiveandreliablewhenN ≥20.
B.3 FURTHERDISCUSSIONw.r.tPERPLEXITY(PPL)INLVLMS
InFigure1,weshowthePPLisnotprecisetoindicatethepre-trainingquality. Thisresultisdraw
from computing PPL on LLaVA-v1.5 7B model that is pre-trained on GPT-style pre-training data
(i.e., ALLaVAandShareGPT4V-PT)andevaluatingwiththesamplesselectedfromShareGPT4V,
whichmeansthetrainingdataandtheevaluationsamplesarefromthesamedomain.Hereweshould
arguethatPPLismuchlessreliablewhenthepre-trainingdatahasdomaingapwiththeevaluation
samples. Tothisend,weconducttheexperimentsonthe∼1.2MdatabymixingLLaVA’sBLIP-2-
generated558KdataandALLaVA,topre-trainLLaVA-v1.57Bmodelwithdifferentscaleofdata.
ThenwefollowthesameevaluationsettingstocomputePPLandMIR,hereisthecomparison.
60.325 60.33 60.325 60.33
60.29 60.3 60.29 60.3
60.26 60.26
3.8 4.9
60.2 60.2
3.6
4.8 60.086 60.1 60.086 MIR 60.1
3.4 Model Performance
60.0 60.0
4.7
3.2
59.9 59.9
PPL
4.6
Model Performance 59.8 3.0 59.8
59.771 59.771
200K 400K 600K 800K 1M 1.2M 200K 400K 600K 800K 1M 1.2M
Pretrain Data Scale Pretrain Data Scale
Figure9: PPLismuchlessreliablewhenthepre-trainingdatahasdomaingapwiththeevaluation
samples.
It indicates that PPL is not appropriate for evaluating the pre-training quality of LVLMs, which
is struggling to deal with LVLMs’ diverse pre-training data from multiple domains nowadays. In
contrast,MIRoffersareliableevaluationforLVLMpre-trainingwithoutSFT.
B.4 LARGERLVLMS
WefurtherstudytheMIRsofLVLMsthathavedifferentscaleofbaseLLMs.Allofpre-trainingdata
andrecipesarethesamewiththeofficialsettingofLLaVA-v1.5. Theresultsareasthefollowing:
17
LPP
RIM
ecnamrofreP
ledoM
RIM
ecnamrofreP
ledoMTable7: MIRvaluesofLVLMsthathavedifferentscaleofLLMs.
BaseLLM VisionEncoder Projector Pretrain Epoch MIR
Data
Vicuna-13B-v1.5 CLIP-L/336 MLP-2x LCS-558K 1epoch 2.583
Vicuna-7B-v1.5 CLIP-L/336 MLP-2x LCS-558K 1epoch 3.374
LLaMA-2-13B-Chat CLIP-L/336 Linear LCS-558K 1epoch 2.477
LLaMA-2-7B-Chat CLIP-L/336 Linear LCS-558K 1epoch 3.699
The results above show that the 13B base LLM achieves a lower MIR than the 7B base LLM,
indicatingthatthelarger,well-trainedLLMhasastrongercapabilitytonarrowthemodalitygapin
theshallowlayers(asMIRisheavilyinfluencedbythelargermodalitygapintheshallowlayersof
thelanguagemodel). Thisisalsoconsistentwiththeimprovedpost-SFTmulti-modalperformance
ofthe13Bmodel.
18