Preprint.
ITERCOMP: ITERATIVE COMPOSITION-AWARE
FEEDBACK LEARNING FROM MODEL GALLERY FOR
TEXT-TO-IMAGE GENERATION
XinchenZhang1∗ LingYang2∗ GuohaoLi5 YaqiCai4 JiakeXie3 YongTang3
YujiuYang1† MengdiWang6 BinCui2†
1TsinghuaUniversity 2PekingUniversity 3LibAILab 4USTC
5UniversityofOxford 6PrincetonUniversity
https://github.com/YangLing0818/IterComp
ABSTRACT
Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made
notablestridesincompositionaltext-to-imagegeneration. However, thesemeth-
ods typically exhibit distinct strengths for compositional generation, with some
excelling in handling attribute binding and others in spatial relationships. This
disparityhighlightstheneedforanapproachthatcanleveragethecomplementary
strengths of various models to comprehensively improve the composition capa-
bility. To this end, we introduce IterComp, a novel framework that aggregates
composition-awaremodelpreferencesfrommultiplemodelsandemploysaniter-
ative feedback learning approach to enhance compositional generation. Specifi-
cally,wecurateagalleryofsixpowerfulopen-sourcediffusionmodelsandeval-
uatetheirthreekeycompositionalmetrics:attributebinding,spatialrelationships,
andnon-spatialrelationships. Basedonthesemetrics,wedevelopacomposition-
aware model preference dataset comprising numerous image-rank pairs to train
composition-awarerewardmodels. Then,weproposeaniterativefeedbacklearn-
ing method to enhance compositionality in a closed-loop manner, enabling the
progressive self-refinement of both the base diffusion model and reward models
overmultipleiterations. Theoreticalproofdemonstratestheeffectivenessandex-
tensiveexperimentsshowoursignificantsuperiorityoverpreviousSOTAmethods
(e.g., Omost and FLUX), particularly in multi-category object composition and
complex semantic alignment. IterComp opens new research avenues in reward
feedbacklearningfordiffusionmodelsandcompositionalgeneration.
1 INTRODUCTION
Therapidadvancementofdiffusionmodels(Sohl-Dicksteinetal.,2015;Hoetal.,2020;Songetal.,
2020;Peebles&Xie,2023)hasrecentlybroughtunprecedentedprogresstothefieldoftext-to-image
generation, with powerful models like DALL-E 3 (Betker et al., 2023), Stable Diffusion 3, (Esser
et al., 2024) and FLUX (BlackForest, 2024) demonstrating remarkable capabilities in generating
aesthetic and diverse images. However, these models often struggle to follow complex prompts
toachieveprecisecompositionalgeneration(Omost-Team,2024;Yangetal.,2024b;Zhangetal.,
2024b),whichrequiresthemodeltopossessrobust,comprehensivecapabilitiesinvariousaspects,
suchasattributebinding,spatialrelationships,andnon-spatialrelationships(Huangetal.,2023).
To enhance compositional generation, some works introduce additional conditions such as lay-
outs/boxes(Lietal.,2023;Zhouetal.,2024;Wangetal.,2024a;Zhangetal.,2024b). InstanceDif-
fusion(Wangetal.,2024a)controlsthegenerationprocessusinglayouts,masks,orotherconditions
through trainable instance masked attention layers. Although these layout-based methods demon-
stratestrongspatialawareness,theystrugglewithimagerealism,especiallyingeneratingnon-spatial
relationshipsandpreservingaestheticquality(Zhangetal.,2024b).Anotherpotentialsolutionlever-
∗Contributedequally.Contact:yangling0818@163.com
†Correspondingauthors.
1
4202
tcO
9
]VC.sc[
1v17170.0142:viXraPreprint.
Attribute Binding Spatial Relationship Non-Spatial Relationship Others
AmagicalnightsceneoftheHogwartsExpress,itracesthrough Astream,alongsidewithtreesandrocks,ontherocks,from
a moonlit landscape. The iconic red steamengine glows under lefttoright,isagrayBritishShorthair,ayellowAmerican
silvery moonlight, casting golden light from its windows as it robin, brown Maltipoo dog. The Maltipoo dog sit quietly,
crossesaarchedstoneviaduct.Anowlfliesalongside.Hogwarts showing an adorable smile.
Castletowersmajestically, with adarklakebelow.
FLUX-dev RPG InstanceDiffusion FLUX-dev RPG InstanceDiffusion
night scene night scene night scene British Shorthair British Shorthair British Shorthair
red engine red engine red engine brown dog brown dog brown dog
golden light golden light golden light from left to right from left to right from left to right
an owl an owl an owl on the rocks on the rocks on the rocks
castle castle castle sit quietly sit quietly sit quietly
a lake below a lake below a lake below adorable smile adorable smile adorable smile
Figure 1: Motivation of IterComp. We select three types of compositional generation methods.
Theresultsshowthatdifferentmodelsexhibitdistinctstrengthsacrossvariousaspectsofcomposi-
tionalgeneration. fig.3furtherdemonstratedthesedistinctstrengthsquantitatively.
agestheimpressivereasoningabilitiesofLargeLanguageModels(LLMs)todecomposecomplex
generationtasksintosimplersubtasks(Yangetal.,2024b;Omost-Team,2024;Wangetal.,2024b).
RPG(Yangetal.,2024b)employsMLLMsastheglobalplannertotransformtheprocessofgener-
atingcompleximagesintomultiplesimplergenerationtaskswithinsubregions.However,itrequires
designingcomplexpromptsforLLMs,anditischallengingtoachieveprecisegenerationresultsdue
totheirintricateoutputs(Yangetal.,2024b).
Weconductedextensiveexperimentstoexploretheuniquestrengthsofdifferentmodelsincompo-
sitionalgeneration.Asshownintheleftexampleinfig.1,text-to-imagemodelFLUX(BlackForest,
2024)demonstratesimpressiveperformanceinattributebindingandaestheticqualityduetoitsad-
vancedtrainingtechniquesandmodelarchitecture. Incontrast,layout-to-imagemodelInstanceDif-
fusion(Wangetal.,2024a)strugglestocapturefine-grainedvisualdetails,suchas’nightscene’or
’goldenlight.’Intherightexampleoffig.1,wherethetextpromptinvolvescomplexspatialrelation-
shipsbetweenmultipleobjects,FLUX(BlackForest,2024)exhibitslimitationsinspatialawareness.
Incontrast,InstanceDiffusion(Wangetal.,2024a)excelsinhandlingspatialrelationshipsthrough
layout guidance. This demonstrates that different models exhibit distinct strengths across various
aspectsofcompositionalgeneration. Moreover,fig.3furtherdemonstratedthesedistinctstrengths
quantitatively. Naturally, a pertinent question arises: Is there a method capable of excelling in all
aspectsofcompositionalgeneration?
In order to enable the diffusion model to improve compositional generation comprehensively, we
presentanewframework,IterComp,whichcollectscomposition-awaremodelpreferencesfromvar-
iousmodels,andthenemploysanovelyetsimpleiterativefeedbacklearningframeworktoachieve
comprehensiveimprovementsincompositionalgeneration.Firstly,weselectsixopen-sourcedmod-
elsexcellingindifferentaspectsofcompositionalitytoformourmodelgallery. Wefocusonthree
essentialcompositionalmetrics:attributebinding,spatialrelationships,andnon-spatialrelationships
to curate a new composition-aware model preference dataset, which consists of a large number of
image-rankpairs. Next,tocomprehensivelycapturediversecomposition-awaremodelpreferences,
wetrainrewardmodelstoprovidefine-grainedcompositionalguidanceduringthefinetuningofthe
basediffusionmodel. Finally,giventhatcompositionalgenerationisdifficulttooptimize,wepro-
poseiterativefeedbacklearning. Thisapproachenhancescompositionalityinaclosed-loopmanner,
allowingfortheprogressiveself-refinementofboththebasediffusionmodelandrewardmodelsin
multipleiterations.Wetheoreticallyandexperimentallydemonstratetheeffectivenessofourmethod
anditssignificantimprovementincompositionalgeneration.
Ourcontributionsaresummarizedasfollows:
• We propose the first iterative composition-aware reward-controlled framework IterComp,
tocomprehensivelyenhancethecompositionalityofthebasediffusionmodel.
• Wecurateamodelgalleryanddevelopahigh-qualitycomposition-awaremodelpreference
datasetcomprisingnumerousimage-rankpairs.
2Preprint.
• Weutilizeanewiterativefeedbacklearningframeworktoprogressivelyenhanceboththe
rewardmodelsandthebasediffusionmodel.
• Extensive qualitative and quantitative comparisons with previous SOTA methods demon-
stratethesuperiorcompositionalgenerationcapabilitiesofourapproach.
2 RELATED WORK
CompositionalText-to-ImageGeneration Compositionaltext-to-imagegenerationisacomplex
and challenging task that requires a model with comprehensive capabilities, including the under-
standingofcomplexpromptsandspatialawareness(Yangetal.,2024b;Zhangetal.,2024b). Some
methods enhance prompt comprehension by using more powerful text encoders or architectures
(Esseretal.,2024;Betkeretal.,2023;Huetal.,2024;Daietal.,2023). StableDiffusion3(Esser
etal.,2024)utilizesthreedifferent-sizedtextencoderstoenhancepromptcomprehension. DALL-E
3(Betkeretal.,2023)enhancestheunderstandingofrichtextualdetailsbyexpandingimagecap-
tionsthroughrecaptioning. However,compositionalcapabilitysuchasspatialawarenessremainsa
limitationofthesemodels(Lietal.,2023;Chenetal.,2024a). Othermethodsattempttoenhance
spatialawarenessbythecontrolofadditionalconditions(e.g., layouts)(Yangetal.,2023;Dahary
etal.,2024). BoxDiff(Xieetal.,2023)andLMD(Lianetal.,2023b)guidethegeneratedobjects
tostrictlyadheretothelayoutbydesigningenergyfunctionsbasedoncross-attentionmaps. Con-
trolNet (Zhang et al., 2023) and T2I-Adapter (Mou et al., 2024) specify high-level image features
tocontrolsemanticstructures. Althoughthesemethodsenhancespatialawareness,theyoftencom-
promiseimagerealism(Zhangetal.,2024b). Additionally,someapproachesleveragethepowerful
reasoningcapabilitiesofLLMstoassistinthegenerationprocess(Yangetal.,2024b;Omost-Team,
2024; Wang et al., 2024b). RPG (Yang et al., 2024b) employs MLLM to decompose complex
compositional generation tasks into simpler subtasks. However, these methods require designing
complexpromptsasinputstotheLLM,andthediffusionmodelstrugglestoproducepreciseresults
duetotheLLM’sintricateoutputs(Yangetal.,2024b).Incontrast,ourmethodextractstheseprefer-
encesfromdifferentmodelsinmodelgalleryandtrainscomposition-awarerewardmodelstorefine
thebasediffusionmodeliteratively,achievingrobustcompositionalityacrossmultipleaspects.
DiffusionModelAlignment Buildingonthesuccessofreinforcementlearningfromhumanfeed-
back(RLHF)inLargeLanguageModels(LLMs)(Ouyangetal.,2022;Baietal.,2022),numerous
methods in diffusion models have attempted to use similar approaches for model alignment (Lee
etal.,2023;Fanetal.,2024;Sunetal.,2023).Somemethodsuseapretrainedrewardmodelortrain
anewonetoguidethegenerationprocess(Zhangetal.,2024a;Blacketal.,2023;Dengetal.,2024;
Clark et al., 2023; Prabhudesai et al., 2023). For instance, ImageReward (Xu et al., 2024) manu-
allyannotatedalargedatasetofhuman-preferredimagesandtrainedarewardmodeltoassessthe
alignmentbetweenimagesandhumanpreferences. RewardFeedbackLearning(ReFL)isproposed
for tuning diffusion models with the ImageReward model. RAHF (Liang et al., 2024a) is trained
on RichHF-18K, a high-quality dataset rich in human feedback, and is capable of predicting the
unreasonablepartsingeneratedimages. Somemethodsbypassthetrainingofarewardmodeland
directly finetune diffusion models on human preference datasets (Yang et al., 2024a; Liang et al.,
2024b; Yang et al., 2024c). Diffusion-DPO (Wallace et al., 2024) reformulates Direct Preference
Optimization(DPO)toaccountforadiffusionmodel’snotionoflikelihood,utilizingtheevidence
lower bound to derive a differentiable objective. The potential for alignment in diffusion models
goes beyond this. We iteratively align the base model with composition-aware model preferences
fromthemodelgallery,effectivelyenhancingitsperformanceoncompositionalgeneration.
3 METHOD
Inthissection,wepresentourmethod,IterComp,whichcollectscomposition-awaremodelprefer-
encesfromthemodelgalleryandutilizesiterativefeedbacklearningtoenhancethecomprehensive
capability of the base diffusion model in compositional generation. An overview of IterComp is
illustrated in fig. 2. In section 3.1, we introduce the method for collecting the composition-aware
model preference dataset from the model gallery. In section 3.2, we describe the training process
for the composition-aware reward models and multi-reward feedback learning. In section 3.2, we
3Preprint.
Model Gallery
SDXL Img1 Img2
Im pg a- ir ra snk 3 Itera (t i v e F e e d b a c k L e )arning
Attribute Binding FLUX Img2 Img5
Non-spatial
Relationship SD 3 Img3 Img1
Pair-wise Composition-Aware Iteration 0 Compositionality
Reward Models
Composition-Aware SD 1.5 Img4 Img3 Training
Model Preference Ranking Iteration 1
RPG Img5 Img6 Looping 01 23
Spatial
Relationship Instance- Img6 Img4 …
Diffusion
1 Collect Composition-Aware Model Preference ModE ex lp Gan ad llery Img n
Iteration 2
Prompt: Fantastical world, towering
islands adorned with lush greenery
rise majestically from the ocean, Rewards
connected by futuristic bridges.
Whales and dolphins swim through
the air. Sailboats navigate the
pristine waters below.
2 Multi-Reward Feedback Learning
Feedback Optimization
Figure2: OverviewofIterComp. Wecollectcomposition-awaremodelpreferencesfrommultiple
modelsandemployaniterativefeedbacklearningapproachtoenabletheprogressiveself-refinement
ofboththebasediffusionmodelandrewardmodels.
propose the iterative feedback learning framework to enable the self-refinement of both the base
diffusionmodelandrewardmodels,progressivelyenhancingcompositionalgeneration.
3.1 COLLECTINGHUMANPREFERENCESOFCOMPOSITIONALITY
Compositional Metric and Model Gallery We focus on three key aspects of compositionality:
attributebinding,spatialrelationships,andnon-spatialrelationships(Huangetal.,2023),tocollect
composition-awaremodelpreferences.Weinitiallyselectsixopen-sourcedmodelsexcelindifferent
aspects of compositional generation as our model gallery: FLUX-dev (BlackForest, 2024), Stable
Diffusion3(Esseretal.,2024), SDXL(Podelletal.,2023), StableDiffusion1.5(Rombachetal.,
2022),RPG(Yangetal.,2024b),andInstanceDiffusion(Wangetal.,2024a).
Human Ranking on Attribute Binding For attribute binding, we randomly select 500 prompts
from each of the following categories: color, shape, and texture in the T2I-CompBench (Huang
et al., 2023). Three professional experts ranked the images generated by the six models for each
prompt, and their rankings were weighted to determine the final result. The primary criterion is
whether the attributes mentioned in the prompt were accurately reflected in the generated images,
especiallythecorrectrepresentationandbindingofattributestothecorrespondingobjects.
HumanRankingonComplexRelationships Forspatialandnon-spatialrelationships,weselect
1,000promptsforeachcategoryfromtheT2I-CompBench(Huangetal.,2023)andapplythesame
manual annotation method to obtain the rankings. For spatial relationships, the primary ranking
criterioniswhethertheobjectsarecorrectlygeneratedandwhethertheirspatialpositioningmatches
the prompt. For non-spatial relationships, the focus is on whether the objects display natural and
realisticactions.
Analysis of Composition-aware Model Preference Dataset For each prompt, we obtain 6 im-
ages and
(cid:0)6(cid:1)
= 15 image-rank pairs. As shown in table 1, in total, we collected a dataset with
2
22,500image-rankpairsformodelpreferenceinattributebinding,15,000forspatialrelationships,
and 15,000 for non-spatial relationships. We visualize the proportion of generated images ranked
firstforeachmodelinfig.3. Theresultsdemonstratethatdifferentmodelsexhibitdistinctstrengths
across various aspects of compositional generation, and this dataset effectively captures a diverse
rangeofcomposition-awaremodelpreferences.
3.2 COMPOSITION-AWAREMULTI-REWARDFEEDBACKLEARNING
Composition-awareRewardModelTraining Toachievecomprehensiveimprovementsincom-
positionalgeneration,weutilizethreetypesofcomposition-awaredatasetsdescribedinsection3.1,
decomposing compositionality into three subtasks and training a specific reward model for each.
4Preprint.
Table1: Statisticsonthecomposition-aware
model preference dataset. The dataset con- Attribute
Binding
sists of 3,500 text prompts, 27,500 images,
and52,500image-rankpairs.
Spatial
Relationship
Counts
Non-spatial
Category Texts Images Image-rankpairs Relationship
AttributeBinding 1,500 9,000 22,500
SpatialRelationship 1,000 6,000 15,000 0 20 40 60 80 100
Non-spatialRelationship 1,000 6,000 15,000 FLUX SDXL RPG
SD 3 SD 1.5 InstanceDiffusion
Total 3,500 21,000 52,500
Figure3:Theproportionofeachmodelrankedfirst.
Specifically, the reward model R (c,x ) is trained using the input format xw ≻ xl | c, where
θi 0 0 0
xw andxl denotingthe”winning”and”losing”images,cdenotingthetextprompt. Weselecttwo
0 0
imagescorrespondingtothesamepromptfromthecomposition-awaremodelpreferencedatasetsto
formaninputimage-rankpair,andtrainedtherewardmodelusingthefollowinglossfunction:
L(θ )=−E (cid:2) log(cid:0) σ(cid:0) R (c,xw)−R (cid:0) c,xl(cid:1)(cid:1)(cid:1)(cid:3) (1)
i (c,xw 0,xl 0)∼Di θi 0 θi 0
whereDdenotesthecomposition-awaremodelpreferencedataset,σ(·)isthesigmoidfunction.
Thethreecomposition-awarerewardmodelsapplyBLIP(Lietal.,2022;Xuetal.,2024)asfeature
extractors. Wecombinetheextractedimageandtextfeatureswithcrossattentionmechanism,and
usealearnableMLPtogenerateascorescalarforpreferencecomparison.
Multi-RewardFeedbackLearning Duetothemulti-stepdenoisingprocessindiffusionmodels,
yieldinglikelihoodsfortheirgenerationsisimpossible,makingtheRLHFapproachusedinlanguage
models unsuitable for diffusion models. Some existing methods (Xu et al., 2024; Zhang et al.,
2024a)finetunediffusionmodelsdirectlybytreatingthescoresoftherewardmodelasthehuman
preference loss. To optimize the base diffusion model using multiple composition-aware reward
models,wedesignthelossfunctionasfollows:
(cid:88)
L(θ)=λE (ϕ(R (c ,p (c )))) (2)
cj∼C i j θ j
i
whereC = {c ,c ,...,c }denotesthepromptset,p (c)denotesthegenerateimageofdiffusion
1 2 n θ
model with parameter θ under the condition of prompt c. We calculate the loss for each reward
modelR (·)andsumthemtoobtainthemulti-rewardfeedbackloss.
i
3.3 ITERATIVEOPTIMIZATIONOFCOMPOSITION-AWAREFEEDBACKLEARNING
Compositionalgenerationischallengingtooptimizeduetoitsinherentcomplexityandmultifaceted
nature,requiringbothourrewardmodelsandbasediffusionmodeltoexcelinaspectssuchascom-
plex text comprehension and the generation of complex relationships. To ensure more thorough
optimization, we propose an iterative feedback learning framework that progressively refines both
therewardmodelsandthebasediffusionmodelovermultipleiterations.
Atthe(k+1)-thiterationoftheoptimizationdescribedinsection3.2,wedenotetherewardmodels
andthebasediffusionmodelfromthepreviousiterationasRk(·)andpk(·),respectively. Foreach
θ
promptcinthedatasetsDk, wesampleanimagex∗ = pk(c)andexpandthecomposition-aware
0 θ
model preference dataset Dk with the sampled image. The image rankings for each prompt are
updatedusingthetrainedrewardmodelRk(·),whilepreservingtherelativeranksoftheinitialsix
θ
images. Following this process, we update the composition-aware model preference dataset to a
more comprehensive version, denoted as Dk+1. Using this dataset, we finetune both the reward
modelsandthebasediffusionmodeltogetRk+1(·)andpk+1(·). Thedetailedprocessofiterative
θ
feedbacklearningcanbefoundinalgorithm1.
Effectiveness of Iterative Feedback Learning Through this iterative feedback learning frame-
work,therewardmodelsbecomemoreeffectiveatunderstandingcomplexcompositionalprompts,
providingmorecomprehensiveguidancetothebasediffusionmodelforcompositionalgeneration.
Theoptimizationobjectiveoftheiterativefeedbacklearningprocessisformalizedinthefollowing
lemma(proofprovidedintheappendixA.2):
5Preprint.
Algorithm1IterativeComposition-awareFeedbackLearning
Dataset:Composition-awaremodelpreferencedatasetD ={((c ,xw,xl),...,(c ,xw,xl)}
0 1 0 0 n 0 0
PromptsetC ={c ,c ,...,c }
1 2 n
Input: Basemodelwithpretrainedparametersp ,rewardmodelR,reward-to-lossmapfunc-
θ
tionϕ,rewardre-weightscaleλ,iterativeoptimizationiterationsiter
Initialization: NumberofnoiseschedulertimestepsT,timesteprangeforfinetuning[T ,T ]
1 2
1: fork =0,...,iterdo
2: for(c i,xw 0,xl 0)∈D k do
3: L←log(cid:0) σ(cid:0) Rk (c,xw)−Rk (cid:0) c,xl(cid:1)(cid:1)(cid:1) //Rewardmodelloss
θi 0 θi 0
4: Rk
θi+1
←Rk θi(c i,xw 0,xl 0) //Updatetherewardmodels
5: endfor //GetRk+1aftertraining
6: forc i ∈C do
7: t←rand(T 1,T 2) //Pickarandomtimestept∈[T 1,T 2]
8: z T ∼N(0,I)
9: forj =T,...,t+1do
10: nograd: z j−1 ←pk θi(z j)
11: endfor
12: withgrad: z t−1 ←pk θi(z t)
13: x 0 ←VaeDec(z 0)←z t−1 //Predictimagefromtheoriginallatent
14: L←λϕ((cid:80) θRk θ+1(c i,x 0)) //Multi-rewardfeedbacklearningloss
15: pk ←pk //Updatethebasediffusionmodel
θi+1 θi
16: endfor //Getpk+1aftertraining
17: for(c i,xw 0,xl 0)∈D k do
18: x∗
0
←pk+1(c i) //Sampleimagesfromtheoptimizedbasediffusionmodel
19: endfor
20: D k+1 ←rank(D k∪x∗ 0) //Expandthedatasetandupdateranking
21: endfor
Lemma1. Theunifiedoptimizationframeworkofiterativefeedbacklearningcanbeformulatedas:
(cid:34) (cid:32) p∗(xw |c) p∗(cid:0) xl |c(cid:1)(cid:33)(cid:35)
max J(θ)=E logσ βlog θ 0:T −βlog θ 0:T (3)
θ [c∼C,(xw 0,xl 0)∼p∗ θ(·|c)] p ref(xw
0:T
|c) p ref(cid:0) xl
0:T
|c(cid:1)
wherep∗(·)denotestheoptimizedbasediffusionmodel.Wesimplifythebilevelproblemofiterative
feedback learning into a single-level objective. Based on this, we present the following theorem
regardingthegradientofthisobjective:
(cid:18) (cid:19)
Theorem1. AssumethatF θ(c,xw 0,xl 0) = logσ βlog pp re∗ θ f( (x xw 0
w
0:T :T| |c c)
)
−βlog pp r∗ θ ef( (x xl 0
l
0:T :T| |c c)
)
,thegra-
dientofoptimizationobjectcanbewrittenasthesumoftwoterms: ∇ J(θ)=T +T ,where:
θ 1 2
T =E(cid:2)(cid:0) ∇ logp (xw |c)+∇ logp (cid:0) xl |c(cid:1)(cid:1) F (cid:0) c,xw,xl(cid:1)(cid:3) (4)
1 θ θ 0:T θ θ 0:T θ 0 0
T =E [∇ [F (c,xw,xl)]] (5)
2 [c∼C,(xw,xl)∼p∗(·|c)] θ θ 0 0
0 0 θ
ItisevidentthatT representsthegradientformofdirectpreferenceoptimization. Inaddition,we
2
haveanothertermT ,whichguidesthegradientofoptimizationobjective. Asshownineq.(4),the
1
gradientdirectsthegenerationofxwandxwtooptimizetheimplicitrewardfunctionF (c,xw,xl).
0 0 θ 0 0
The gradient term T helps the model better distinguish between winning and losing samples, in-
1
creasingtheprobabilityofgeneratinghigh-qualityimageswhilereducingtheprobabilityofgener-
atinglow-qualityimages.Thisimprovesthemodel’salignmentwiththerewardmodel’spreferences
duringgeneration,therebyenhancingthecomprehensivecapabilitiesofcompositionalgeneration.
SuperiorityoverDiffusion-DPOandImageReward HereweclarifysomesuperioritiesofIter-
CompoverDiffusion-DPO(Wallaceetal.,2024)andImageReward(Xuetal.,2024).OurIterComp
firstfocusesoncomposition-awarerewardstooptimizeT2Imodelsforrealisticcomplexgeneration
6Preprint.
scenarios, and constructs a powerful model gallery to collect multiple composition-aware model
preferences. Thenournoveliterativefeedbacklearningframeworkcaneffectivelyachieveprogres-
siveself-refinementofbothbasediffusionmodelandrewardmodelsovermultipleiterations.
Text-controlled LLM-controlled Layout-controlled Reward-controlled
23.02 s/Img 15.57 s/Img 9.88 s/Img 5.63 s/Img
FLUX-dev RPG InstanceDiffusion IterComp (Ours)
A colossal, ancient tree with leaves made of ice towers over a mystical castle. Green trees line both
sides, while cascading waterfalls and an ethereal glow adorn the scene. The backdrop features towering
mountains and a vibrant, colorful sky.
On the rooftop of a skyscraper in a bustling cyberpunk city, a figure in a trench coat and neon-lit visor
stands amidst a garden of bio-luminescent plants, overlooking the maze of flying cars and towering
holograms. Robotic birds flit among the foliage, digital billboards flash advertisements in the distance.
In a magical seascape, a majestic ship sails through crystal blue waters surrounded by vibrant marine life
and soaring birds. Towering cliffs frame the scene, while a stunning rainbow arches across the sky,
blending with ethereal clouds. This enchanting journey captures the serene beauty of nature's wonders.
Under the luminous full moon, a serene Japanese garden with traditional pagodas and a tranquil pond
creates a magical night scene. The soft glow from the lantern-lit buildings reflects on the water, blending
nature and architecture in harmony. The moonlight bathes the landscape, enhancing the peaceful ambiance.
Figure 4: Qualitative comparison between our IterComp and three types of compositional genera-
tionmethods: text-controlled,LLM-controlled,andlayout-controlledapproaches. IterCompisthe
firstreward-controlledmethodforcompositionalgeneration,utilizinganiterativefeedbacklearning
framework to enhance the compositionality of generated images. Colored text denotes the advan-
tagesofIterCompingeneratedimages.
7Preprint.
Table 2: Evaluation results about compositionality on T2I-CompBench (Huang et al., 2023). Iter-
Comp consistently demonstrates the best performance regarding attribute binding, object relation-
ships, andcomplexcompositions. Wedenote thebestscorein blue andthe second-bestscorein
green. ThebaselinedataisquotedfromGenTron(Chenetal.,2024b).
AttributeBinding ObjectRelationship
Model Complex↑
Color↑ Shape↑ Texture↑ Spatial↑ Non-Spatial↑
StableDiffusion1.4(Rombachetal.,2022) 0.3765 0.3576 0.4156 0.1246 0.3079 0.3080
StableDiffusion2(Rombachetal.,2022) 0.5065 0.4221 0.4922 0.1342 0.3096 0.3386
Attn-Exctv2(Cheferetal.,2023) 0.6400 0.4517 0.5963 0.1455 0.3109 0.3401
StableDiffusionXL(Betkeretal.,2023) 0.6369 0.5408 0.5637 0.2032 0.3110 0.4091
PixArt-α(Chenetal.,2023) 0.6886 0.5582 0.7044 0.2082 0.3179 0.4117
ECLIPSE(Pateletal.,2024) 0.6119 0.5429 0.6165 0.1903 0.3139 -
Dimba-G(Feietal.,2024) 0.6921 0.5707 0.6821 0.2105 0.3298 0.4312
GenTron(Chenetal.,2024b) 0.7674 0.5700 0.7150 0.2098 0.3202 0.4167
GLIGEN(Lietal.,2023) 0.4288 0.3998 0.3904 0.2632 0.3036 0.3420
LMD+(Lianetal.,2023a) 0.4814 0.4865 0.5699 0.2537 0.2828 0.3323
InstanceDiffusion(Wangetal.,2024a) 0.5433 0.4472 0.5293 0.2791 0.2947 0.3602
IterComp(Ours) 0.7982 0.6217 0.7683 0.3196 0.3371 0.4873
4 EXPERIMENTS
4.1 EXPERIMENTALSETUP
DatasetsandTrainingSetting Therewardmodelsaretrainedonthecomposition-awaremodel
preferencedataset,consistingof3,500promptsand52,500image-rankpairs. Fortrainingthethree
rewardmodels,wefinetuneBLIPandthelearnableMLPwithalearningrateof1e−5andabatch
sizeof64. Duringtheiterativefeedbacklearningprocess,werandomlyselect10,000promptsfrom
DiffusionDB(Wangetal.,2022)anduseSDXL(Betkeretal.,2023)asthebasediffusionmodel,
finetuningitwithalearningrateof1e−5andabatchsizeof4. WesetT =40,[T ,T ]=[1,10],
1 2
ϕ=ReLU,andλ=1e−3. Allexperimentsareconductedon4NVIDIAA100GPUs.
BaselineModels Wecurateamodelgalleryofsixopen-sourcemodels,eachexcellingindifferent
aspects of compositional generation: FLUX (BlackForest, 2024), Stable Diffusion 3 (Esser et al.,
2024),SDXL(Betkeretal.,2023),StableDiffusion1.5(Rombachetal.,2022),RPG(Yangetal.,
2024b),andInstanceDiffusion(Wangetal.,2024a). Toensurethebasediffusionmodelthoroughly
and comprehensively learns composition-aware model preferences, we progressively expand the
modelgallerybyincorporatingnewmodels(e.g.,Omost(Omost-Team,2024),StableCascade(Per-
nias et al., 2023), PixArt-α (Chen et al., 2023)) at each iteration. For performance comparison in
compositionalgeneration,weselectseveralstate-of-the-artmethods,includingFLUX(BlackForest,
2024), SDXL (Betker et al., 2023), and RPG (Yang et al., 2024b) to compare with our approach.
We use GPT-4o (OpenAI, 2024) for the LLM-controlled methods and to infer the layout from the
promptforthelayout-controlledmethods.
4.2 MAINRESULTS
QualitativeComparison Asshowninfig.4, IterCompachievessuperiorcompositionalgenera-
tionresultscomparedtothethreemaintypesofcompositionalgenerationmethods: text-controlled,
LLM-controlled, and layout-controlled approaches. In comparison to text-controlled methods
FLUX (BlackForest, 2024), IterComp excels in handling spatial relationships, significantly reduc-
ingerrorssuchasobjectomissionsandinaccuraciesinnumeracyandpositioning. Whencompared
to LLM-controlled methods like RPG (Yang et al., 2024b), IterComp produces more reasonable
object placements, avoiding the unrealistic positioning caused by LLM hallucinations. Compared
to layout-controlled methods like InstanceDiffusion (Wang et al., 2024a), IterComp demonstrates
a clear advantage in both semantic aesthetics and compositionality, particularly when generating
undercomplexprompts.
Quantitative Comparison We compare IterComp with previous outstanding compositional
text/layout-to-imagemodelsontheT2I-CompBench(Huangetal.,2023)insixkeycompositional
scenarios. As shown in table 2, IterComp demonstrates a remarkable preference across all evalu-
ation tasks. Layout-controlled methods such as LMD+ (Lian et al., 2023a) and InstanceDiffusion
8Preprint.
Table3: Evaluationonimagerealism. Table4: Evaluationoninferencetime.
Model CLIPScore↑AestheticScore↑ImageReward↑ Model InferenceTime↓
StableDiffusion1.4(Rombachetal.,2022) 0.307 5.326 -0.065
StableDiffusion2.1(Rombachetal.,2022) 0.321 5.458 0.216 FLUX-dev 23.02s/Img
StableDiffusionXL(Betkeretal.,2023) 0.322 5.531 0.780 StableDiffusionXL(Betkeretal.,2023) 5.63s/Img
Omost(Omost-Team,2024) 21.08s/Img
GLIGEN(Lietal.,2023) 0.301 4.892 -0.077 RPG(Yangetal.,2024b) 15.57s/Img
LMD+(Lianetal.,2023a) 0.298 4.964 -0.072 InstanceDiffusion(Wangetal.,2024a) 9.88s/Img
InstanceDiffusion(Wangetal.,2024a) 0.302 5.042 -0.035
IterComp(Ours) 0.337 5.936 1.437 IterComp(Ours) 5.63s/Img
(Wang et al., 2024a) excel in generating accurate spatial relationships, while text-to-image mod-
els like SDXL (Betker et al., 2023) and GenTron (Chen et al., 2024b) exhibit particular strengths
in attribute binding and non-spatial relationships. In contrast, IterComp achieves comprehensive
improvementincompositionalgeneration. Itobtainsthestrengthsofvariousmodelsbycollecting
composition-aware model preferences, and employs a novel iterative feedback learning to enable
self-refinementofboththebasediffusionmodelandrewardmodelsinaclosed-loopmanner.
IterCompachievesahighlevelofcompositionalitywhilesimultaneouslyenhancingtherealismand
aesthetics of the generated images. As shown in table 3, we evaluate the improvement in image
realismbycalculatingtheCLIPScore,AestheticScore,andImageReward. IterCompsignificantly
outperformspreviousmodelsacrossallthreescenarios,demonstratingremarkablefidelityandpre-
cisioninalignmentwiththecomplextextprompt. Thesepromisingresultshighlighttheversatility
ofIterCompinbothcompositionalityandfidelity. Weprovidemorequantitativecomparisonresults
betweenIterCompandotherdiffusionalignmentmethodsinappendixA.3.
IterComprequireslesstimetogeneratehigh-qualityimages. Intable4,wecomparetheinference
time of IterComp with other outstanding models, such as FLUX (BlackForest, 2024), RPG (Yang
et al., 2024b) in generating a single image. Using the same text prompts and fixing the denois-
ing steps to 40, IterComp demonstrates faster generation, because it avoids the complex attention
computationsinRPGandOmost. Ourmethodcanincorporatecomposition-awareknowledgefrom
differentmodelswithoutaddinganycomputationaloverhead.Thisefficiencyhighlightsitspotential
forvariousapplicationsandoffersanewperspectiveonhandlingcomplexgenerationtasks.
User Study We conducted a compre-
hensive user study to evaluate the ef- Ours vs FLUX 52% 17% 31%
fectiveness of IterComp in compositional
Ours vs SDXL 70% 9% 21%
generation. Asillustratedinfig.5,weran-
domlyselected16promptsforeachcom- Ours vs Omost 63% 14% 23%
parison,andinvited23usersfromdiverse Ours vs RPG 60% 14% 26%
backgrounds to vote on image composi- Ours vs 84% 6%10%
tionality,resultinginatotalof1,840votes. InstanceDiffusion
0% 20% 40% 60% 80% 100%
The results show that IterComp received
Win Tie Loss
widespreaduserapprovalincompositional
Figure5: Resultsofuserstudy.
generation.
4.3 ABLATIONSTUDY
0.338 5.95 1.45
0.33680.3370 5.933 5.936
1.420 1.422
1.437
0.336 0.3358
5.90 5.894 1.40
0.3341 1.381
0.334
5.85 5.843 1.35 1.344
0.332 0.3316 5.831
0.330 5.80 1.30
2 3 4 5 6 2 3 4 5 6 2 3 4 5 6
Size of Model Gallery Size of Model Gallery Size of Model Gallery
(a)ImpactonCLIPScore. (b)ImpactonAestheticScore. (c)ImpactonImageReward.
Figure6: Ablationstudyonthemodelgallerysize.
Effect of Model Gallery Size In the ablation study on model gallery size, as shown in fig. 6,
we observe that increasing the size of the model gallery leads to improved performance for Iter-
Compacrossvariousevaluationtasks.Toleveragethisfindingandprovidemorefine-grainedreward
9
erocS
PILC
erocS
citehtseA
draweregamIPreprint.
SDXL Iteration 1 Iteration 2 Iteration 3 SDXL Iteration 1 Iteration 2 Iteration 3
In the heart of a bustling city, a colossal treehouse complex rises, Majestic sailing ships navigate stormy seas above, while beneath the
blending nature with urban life. Elevated wooden homes nestle among waves, a vibrant underwater world teems with marine life and ancient
lush branches, creating a serene, green oasis amidst modern ruins. A sea turtle glides through the crystal-clear waters, bridging
skyscrapers and busy streets below. the realms of oceanic mystery and maritime history.
Figure7: Ablationstudyontheiterationsoffeedbacklearning.
Omost Omost with IterComp Omost Omost with IterComp
A picturesque stone cottage, adorned with climbing
vines and sunflowers, sits peacefully along a
cobblestone path. Warm embrace of the afternoon sun.
RPG RPG with IterComp
Amidst a stormy, apocalyptic skyline, a masked warrior stands resolute, adorned
in intricate armor and a flowing cape. Lightning illuminates the dark clouds
Vibrant birds soar over a natural rock arch, with a behind him, highlighting his steely determination. With a futuristic city in ruins
lush river valley and towering red canyons below. The at his back and a red sword in hand, he embodies the fusion of ancient valor and
scene is framed by green vegetation and cacti, under advanced technology, ready to face the chaos ahead.
a blue sky with fluffy clouds.
RPG RPG with IterComp
Futuristic and prehistoric worlds collide: Dinosaurs roam near a medieval castle, flying cars and advanced skyscrapers dominate the
skyline. A river winds through lush greenery, blending ancient and modern civilizations in a surreal landscape.
Figure8: ThegenerationperformanceofintegratingIterCompintoRPGandOmost.
guidance, weprogressivelyexpandthemodelgalleryovermultipleiterationsbyincorporatingthe
optimizedbasediffusionmodelandnewmodelssuchasOmost(Omost-Team,2024).
Effectofcomposition-awareiterativefeedbacklearning Weconductedanablationstudy(see
fig.7)toevaluatetheimpactofcomposition-awareiterativefeedbacklearning.Theresultsshowthat
thisapproachsignificantlyimprovesboththeaccuracyofcompositionalgenerationandtheaesthetic
quality of the generated images. As the number of iterations increases, the model’s preferences
graduallyconverge. Basedonthisobservation,wesetthenumberofiterationsto3inIterComp.
4.4 GENERALIZATIONSTUDY
IterCompcanserveasapowerfulbackboneforvariouscompositionalgenerationtasks,leveraging
itsstrengthsinspatialawareness,complexpromptcomprehension,andfasterinference. Asshown
in fig. 8, we integrate IterComp into Omost (Omost-Team, 2024) and RPG (Yang et al., 2024b).
The results demonstrate that equipped with the more powerful IterComp backbone, both Omost
andRPGachieveexcellentcompositionalgenerationperformance, highlightingIterComp’sstrong
generalizationabilityandpotentialforbroaderapplications.
10Preprint.
5 CONCLUSION
In this paper, we propose a novel framework, IterComp, to address the challenges of complex
and compositional text-to-image generation. IterComp aggregates composition-aware model pref-
erencesfromamodelgalleryandemploysaniterativefeedbacklearningapproachtoprogressively
refine both the reward models and the base diffusion models over multiple iterations. For future
work,weplantofurtherenhancethisframeworkbyincorporatingmorecomplexmodalitiesasin-
putconditionsandextendingittomorepracticalapplications.
ACKNOWLEDGEMENT
TheauthorteamwouldliketodeliversincerethankstoRuihangChufromTsinghuaUniversityfor
hissignificantsuggestionsfortherefinementofthispaper.
REFERENCES
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,
2022.
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. Computer
Science.https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8,2023.
Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion
modelswithreinforcementlearning. arXivpreprintarXiv:2305.13301,2023.
BlackForest. Blackforestlabs;frontierailab,2024. URLhttps://blackforestlabs.ai/.
Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite:
Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on
Graphics(TOG),42(4):1–10,2023.
JunsongChen,JinchengYu,ChongjianGe,LeweiYao,EnzeXie,YueWu,ZhongdaoWang,James
Kwok,PingLuo,HuchuanLu,etal. Pixart-α: Fasttrainingofdiffusiontransformerforphotore-
alistictext-to-imagesynthesis. arXivpreprintarXiv:2310.00426,2023.
Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention
guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision,pp.5343–5353,2024a.
ShoufaChen,MengmengXu,JiaweiRen,YurenCong,SenHe,YanpingXie,AnimeshSinha,Ping
Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and
videogeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.6441–6451,2024b.
KevinClark, PaulVicol, KevinSwersky, andDavidJFleet. Directlyfine-tuningdiffusionmodels
ondifferentiablerewards. arXivpreprintarXiv:2309.17400,2023.
OmerDahary,OrPatashnik,KfirAberman,andDanielCohen-Or. Beyourself: Boundedattention
formulti-subjecttext-to-imagegeneration. arXivpreprintarXiv:2403.16990,2024.
XiaoliangDai,JiHou,Chih-YaoMa,SamTsai,JialiangWang,RuiWang,PeizhaoZhang,Simon
Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation
modelsusingphotogenicneedlesinahaystack. arXivpreprintarXiv:2309.15807,2023.
Fei Deng, Qifei Wang, Wei Wei, Tingbo Hou, and Matthias Grundmann. Prdp: Proximal reward
differencepredictionforlarge-scalerewardfinetuningofdiffusionmodels. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.7423–7433,2024.
11Preprint.
MucongDing,SouradipChakraborty,VibhuAgrawal,ZoraChe,AlecKoppel,MengdiWang,Am-
rit Bedi, and Furong Huang. Sail: Self-improving efficient online alignment of large language
models. arXivpreprintarXiv:2406.15567,2024.
PatrickEsser, SumithKulal, AndreasBlattmann, RahimEntezari, JonasMu¨ller, HarrySaini, Yam
Levi,DominikLorenz,AxelSauer,FredericBoesel,etal. Scalingrectifiedflowtransformersfor
high-resolution image synthesis. In Forty-first International Conference on Machine Learning,
2024.
Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel,
Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-
tuningtext-to-imagediffusionmodels. AdvancesinNeuralInformationProcessingSystems,36,
2024.
Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, and Junshi Huang.
Dimba: Transformer-mambadiffusionmodels. arXivpreprintarXiv:2406.01159,2024.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
XiweiHu,RuiWang,YixiaoFang,BinFu,PeiCheng,andGangYu. Ella: Equipdiffusionmodels
withllmforenhancedsemanticalignment. arXivpreprintarXiv:2403.05135,2024.
Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A compre-
hensivebenchmarkforopen-worldcompositionaltext-to-imagegeneration. AdvancesinNeural
InformationProcessingSystems,36:78723–78747,2023.
KiminLee,HaoLiu,MoonkyungRyu,OliviaWatkins,YuqingDu,CraigBoutilier,PieterAbbeel,
MohammadGhavamzadeh,andShixiangShaneGu. Aligningtext-to-imagemodelsusinghuman
feedback. arXivpreprintarXiv:2302.12192,2023.
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-imagepre-
trainingforunifiedvision-languageunderstandingandgeneration.InInternationalconferenceon
machinelearning,pp.12888–12900.PMLR,2022.
YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,JianweiYang,JianfengGao,ChunyuanLi,
andYongJaeLee. Gligen: Open-setgroundedtext-to-imagegeneration. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.22511–22521,2023.
Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt
understanding of text-to-image diffusion models with large language models. arXiv preprint
arXiv:2305.13655,2023a.
Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt
understanding of text-to-image diffusion models with large language models. arXiv preprint
arXiv:2305.13655,2023b.
YouweiLiang,JunfengHe,GangLi,PeizhaoLi,ArseniyKlimovskiy,NicholasCarolan,JiaoSun,
JordiPont-Tuset,SarahYoung,FengYang,etal. Richhumanfeedbackfortext-to-imagegenera-
tion. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pp.19401–19411,2024a.
Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, and Liang Zheng.
Step-aware preference optimization: Aligning preference with denoising performance at each
step. arXivpreprintarXiv:2406.04314,2024b.
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan.
T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion
models. InProceedingsoftheAAAIConferenceonArtificialIntelligence,pp.4296–4304,2024.
Omost-Team. Omostgithubpage,2024.
OpenAI. Hellogpt-4o,2024. URLhttps://openai.com/index/hello-gpt-4o/.
12Preprint.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to fol-
lowinstructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems, 35:
27730–27744,2022.
Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. Eclipse: A
resource-efficient text-to-image prior for image generations. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.9069–9078,2024.
WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,pp.4195–4205,2023.
Pablo Pernias, Dominic Rampas, Mats L Richter, Christopher J Pal, and Marc Aubreville.
Wu¨rstchen: An efficient architecture for large-scale text-to-image diffusion models. arXiv
preprintarXiv:2306.00637,2023.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023.
Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-
imagediffusionmodelswithrewardbackpropagation. arXivpreprintarXiv:2310.03739,2023.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684–10695,2022.
JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsupervised
learning using nonequilibrium thermodynamics. In International conference on machine learn-
ing,pp.2256–2265.PMLR,2015.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020.
JiaoSun,DeqingFu, YushiHu, SuWang, RoyiRassin, Da-ChengJuan, DanaAlon, CharlesHer-
rmann, Sjoerd van Steenkiste, Ranjay Krishna, et al. Dreamsync: Aligning text-to-image gen-
erationwithimageunderstandingfeedback. InSyntheticDataforComputerVisionWorkshop@
CVPR2024,2023.
Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam,
StefanoErmon,CaimingXiong,ShafiqJoty,andNikhilNaik. Diffusionmodelalignmentusing
directpreferenceoptimization. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.8228–8238,2024.
XudongWang,TrevorDarrell,SaiSakethRambhatla,RohitGirdhar,andIshanMisra. Instancedif-
fusion: Instance-levelcontrolforimagegeneration. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.6232–6242,2024a.
Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for
unifiedimagegenerationandediting. arXivpreprintarXiv:2407.05600,2024b.
ZijieJWang,EvanMontoya,DavidMunechika,HaoyangYang,BenjaminHoover,andDuenHorng
Chau. Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models.
arXivpreprintarXiv:2210.14896,2022.
Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and
MikeZhengShou.Boxdiff:Text-to-imagesynthesiswithtraining-freebox-constraineddiffusion.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pp.7452–7461,
2023.
Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao
Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation.
AdvancesinNeuralInformationProcessingSystems,36,2024.
13Preprint.
KaiYang,JianTao,JiafeiLyu,ChunjiangGe,JiaxinChen,WeihanShen,XiaolongZhu,andXiuLi.
Usinghumanfeedbacktofine-tunediffusionmodelswithoutanyrewardmodel.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.8941–8951,2024a.
LingYang,ZhaochenYu,ChenlinMeng,MinkaiXu,StefanoErmon,andCUIBin. Masteringtext-
to-imagediffusion: Recaptioning,planning,andgeneratingwithmultimodalllms. InForty-first
InternationalConferenceonMachineLearning,2024b.
ShentaoYang, TianqiChen, andMingyuanZhou. Adenserewardviewonaligningtext-to-image
diffusionwithpreference. arXivpreprintarXiv:2402.08265,2024c.
ZhengyuanYang,JianfengWang,ZheGan,LinjieLi,KevinLin,ChenfeiWu,NanDuan,Zicheng
Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.14246–
14255,2023.
Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao,
WeilinHuang, MinZheng, etal. Unifl: Improvestablediffusion viaunifiedfeedback learning.
arXivpreprintarXiv:2404.05595,2024a.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusionmodels.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pp.3836–3847,2023.
XinchenZhang,LingYang,YaqiCai,ZhaochenYu,JiakeXie,YeTian,MinkaiXu,YongTang,Yu-
jiuYang,andBinCui. Realcompo: Dynamicequilibriumbetweenrealismandcompositionality
improvestext-to-imagediffusionmodels. arXivpreprintarXiv:2402.12908,2024b.
Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation
controllerfortext-to-imagesynthesis. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.6818–6828,2024.
14Preprint.
A APPENDIX
This supplementary material is structured into several sections that provide additional details and
analysisrelatedtoIterComp. Specifically,itwillcoverthefollowingtopics:
• InappendixA.1,weprovideapreliminaryaboutStableDiffusion(SD)andRewardFeed-
backLearning(ReFL).
• InappendixA.2,weprovidedetailedtheoreticalproofoftheeffectivenessofiterativefeed-
backlearning.
• In appendix A.3, we present the quantitative comparison results between IterComp and
otherdiffusionalignmentmethods.
• InappendixA.4,weprovidemorevisualizationresultsforIterCompanditsbasediffusion
model,SDXL.
A.1 PRELIMINARY
StableDiffusion StableDiffusion(SD)(Rombachetal.,2022)performsmulti-stepdenoisingon
randomnoisez ∼ N(0,I)togenerateaclearlatentz inthelatentspaceundertheguidanceof
T 0
textpromptc. Duringthetraining, aninputimagex isprocessedbyapretrainedautoencoderto
0
obtainitslatentrepresentationz . Arandomnoiseϵ ∼ N(0,I)isinjectedintoz intheforward
0 0
processasfollow: √ √
z = α¯ z + 1−α¯ ϵ (6)
t t 0 t
whereα isthenoiseschedule. TheUNetϵ istrainedtopredicttheaddednoisewiththeoptimiza-
t θ
tionobjective:
(cid:104) (cid:105)
min L(θ)=E ∥ϵ−ϵ (z ,t,τ(c))∥2 (7)
θ
[z0∼E(x0),ϵ∼N(0,I),t] θ t 2
whereE(·)denotethepreteainedencoderofVAE,τ(·)denotesthepretrainedtextencoder.
RewardFeedbackLearning RewardFeedbackLearning(ReFL)(Xuetal.,2024)isproposedto
aligndiffusionmodelswithhumanpreferences.Therewardmodelservesasthepreferenceguidance
during the finetuning of the diffusion model. ReFL begins with an input prompt c and a random
noise z ∼ N(0,I). The noise z is progressively denoised until it reaches a randomly selected
T T
timestep t. The latent z is directly predicted from z , and the decoder from a pretrained VAE is
0 t
usedtogeneratethepredictedimagex . ThepretrainedrewardmodelR(·)providesarewardscore
0
asfeedback,whichisusedtofinetunethediffusionmodelasfollows:
min L(θ)=−E (R(c,x )) (8)
c∼C 0
θ
wherethepromptcisrandomlyselectedfromthepromptdatasetC.
A.2 THEORETICALPROOFOFTHEEFFECTIVENESSOFITERATIVEFEEDBACKLEARNING
A.2.1 PROOFOFLEMMA1
ProofofLemma1. ConsideringthegeneralformofRLHF,wechangetheoptimizationproblemof
iterativefeedbacklearningtoabileveloptimization(Wallaceetal.,2024;Dingetal.,2024):
min −E (cid:2) logσ(cid:0) R(c,xw)−R(cid:0) c,xl(cid:1)(cid:1)(cid:3)
[c∼C,(xw,xl)∼p∗(·|c)] 0 0
R 0 0 R (9)
s.t. p∗ :=argmaxE (cid:2)E R(c,x )(cid:3) −βD [p(x |c)||p (x |c)]
R
p
c∼C x0∼p(·|c) 0 KL 0:T ref 0:T
wherep∗ denotestheoptimizedbasemodelsundertheguidanceofrewardmodelR. Wehavethe
R
reparameterizationoftherewardmodel(alsoshowninpreviousworksby(Wallaceetal.,2024)):
(cid:20) p∗ (x |c)(cid:21)
R(c,x )=βE log R 0:T +βlogZ(c) (10)
0 pR(x1:T|x0,c) p (x |c)
ref 0:T
(cid:88)
Z(c)= p (x |c)exp(R(c,x )/β) (11)
ref 0:T 0
x
15Preprint.
Substitutingthisrewardreparameterizationintoeq.(9),wegetthenewoptimizationobjectiveas:
(cid:34) (cid:32) p∗ (xw |c) p∗ (cid:0) xl |c(cid:1)(cid:33)(cid:35)
min −E logσ βlog R 0:T −βlog R 0:T (12)
p∗
R
[c∼C,(xw 0,xl 0)∼p∗ R(·|c)] p ref(xw
0:T
|c) p ref(cid:0) xl
0:T
|c(cid:1)
ThisnewoptimizationobjectiveisdenotedasJ(p∗ ),weget:
R
(cid:34) (cid:32) p∗ (xw |c) p∗ (cid:0) xl |c(cid:1)(cid:33)(cid:35)
max J(p∗ )=E logσ βlog R 0:T −βlog R 0:T (13)
p∗
R
R [c∼C,(xw 0,xl 0)∼p∗ R(·|c)] p ref(xw
0:T
|c) p ref(cid:0) xl
0:T
|c(cid:1)
Weusep toparameterizethepolicyandformulatethefinaloptimizationobjectiveas:
θ
(cid:34) (cid:32) p∗(xw |c) p∗(cid:0) xl |c(cid:1)(cid:33)(cid:35)
max J(θ)=E logσ βlog θ 0:T −βlog θ 0:T (14)
θ [c∼C,(xw 0,xl 0)∼p∗ θ(·|c)] p ref(xw
0:T
|c) p ref(cid:0) xl
0:T
|c(cid:1)
A.2.2 PROOFOFTHEOREM1
ProofofTheorem1. Thegradientoftheoptimizationobjectiveineq.(14)canbewrittenas:
(cid:88) (cid:34) (cid:32) p∗(xw |c) p∗(cid:0) xl |c(cid:1)(cid:33)(cid:35)
∇ J(θ)=∇ p (xw |c)p (xl |c) logσ βlog θ 0:T −βlog θ 0:T
θ θ θ 0:T θ 0:T p (xw |c) p (cid:0) xl |c(cid:1)
c,xw,xl ref 0:T ref 0:T
0 0
(15)
Assumethat:
(cid:32) p∗(xw |c) p∗(cid:0) xl |c(cid:1)(cid:33)
F (c,xw,xl)=logσ βlog θ 0:T −βlog θ 0:T (16)
θ 0 0 p (xw |c) p (cid:0) xl |c(cid:1)
ref 0:T ref 0:T
pˆ (cid:0) xw ,xl |c(cid:1) =p (xw |c)p (xl |c) (17)
θ 0:T 0:T θ 0:T θ 0:T
Thegradientcanbedecomposedintotwoterms:
∇ J(θ)=∇ (cid:88) pˆ (cid:0) xw ,xl |c(cid:1) F (c,xw,xl)
θ θ θ 0:T 0:T θ 0 0
c,xw,xl
0 0
= (cid:88) ∇ pˆ (cid:0) xw ,xl |c(cid:1) F (c,xw,xl)+E [∇ [F (c,xw,xl)]]
θ θ 0:T 0:T θ 0 0 [c∼C,(xw,xl)∼p∗(·|c)] θ θ 0 0
0 0 θ
c,xw,xl (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) 0 0 (cid:123)(cid:122) (cid:125) T2
T1
(18)
Byexpandingthedistributionpˆ inT ,amorespecificformisobtained:
θ 1
T = (cid:88) ∇ pˆ (cid:0) xw ,xl |c(cid:1) F (c,xw,xl)
1 θ θ 0:T 0:T θ 0 0
c,xw,xl (19)
0 0
=E(cid:2)(cid:0) ∇ logp (xw |c)+∇ logp (cid:0) xl |c(cid:1)(cid:1) F (cid:0) c,xw,xl(cid:1)(cid:3)
θ θ 0:T θ θ 0:T θ 0 0
A.3 QUANTITATIVECOMPARISONWITHOTHERDIFFUSIONALIGNMENTMETHODS.
WecompareIterCompwithstate-of-the-artdiffusionalignmentmethods,Diffusion-DPO(Wallace
et al., 2024) and ImageReward (Xu et al., 2024) in terms of image compositionality and realism.
We calculate the average results of these models on T2I-CompBench (Huang et al., 2023), and
evaluateimagerealismviaCLIPScoreandAestheticScore. Asdemonstratedintable5,IterComp
significantlyoutperformspreviousdiffusionalignmentmethodsacrossallthreescenarios.IterComp
aggregates composition-aware model preferences from multiple models, which are used to train
reward models. Guided by these composition-aware reward models, it achieves comprehensive
improvementsincompositionalgeneration.Itssuperiorperformanceinimagerealismisattributedto
theeffectivenessofiterativefeedbacklearning,wheretheself-refinementofboththebasediffusion
modelandrewardmodelsacrossmultipleiterationsdrivessignificantgainsinbothcompositionality
andrealism.
16Preprint.
Table5: ComparisonbetweenIterCompandotherdiffusionalignmentmethods.
Model AverageResultonT2I-CB↑ CLIPScore↑ AestheticScore↑
StableDiffusionXL(Betkeretal.,2023) 0.4441 0.322 5.531
Diffusion-DPO(Wallaceetal.,2024) 0.4417 0.326 5.572
ImageReward(Xuetal.,2024) 0.4639 0.323 5.613
IterComp(Ours) 0.5554 0.337 5.936
A.4 MOREVISUALIZATIONRESULTS
SDXL IterComp (Ours) SDXL IterComp (Ours)
Figure9: MorevisualizationresultsforIterCompanditsbasediffusionmodel,SDXL.
17