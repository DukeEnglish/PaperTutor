MM-EGO: TOWARDS BUILDING EGOCENTRIC
MULTIMODAL LLMS
HanrongYe1†,HaotianZhang2†,ErikDaxberger2,LinChen2,ZongyuLin3,YanghaoLi2,
BowenZhang2,HaoxuanYou2,DanXu1,ZheGan2‡,JiasenLu2‡,YinfeiYang2‡
1CSE,HKUST 2Apple 3UCLA
Human "I combs cats fur with a comb." Compressed Visual Question Open-Response Language Multiple-Choice
Narration "woman puts cat treats on the plate."... Step 1. Global <vis>Em <b ved isd >ings <vis>Em <b te xd td >ings <pointer> QA Model QA
Glimpse
Language
Model LLM Question: What activity did I repeatedly do with the cat?
Top-k Select Options:
A: I brushed the cat's teeth with a toothbrush.
Egocentric QA Q A:: WW oh mat a d ni d p uw tso m caa tn t rd eo a tw s it oh n t th he e c pa lt a tt ere .ats? Step 2. Fallback <vis H> i Eg mh< - bRv ee di ss d > V ini gsu s< av lis> EmQ< but eex dstt di> o inn gs LLM B C D: : : I I I c w cl oi a mp sp h be e ed d d t t th h he e e c c ca a at t t' 's s 's n f faa uci rl e s w ww ii ti t ht hh a a a c w on ma ai sl bh c .cl li op tp he . r.
(a) Data: Narration to Egocentric QA
(b) Model: Memory Pointer Prompting (c) Benchmark: EgoMemoria
Data Engine
Figure1: Weintroduceafoundationmodelforegocentricvideounderstanding, contributingfrom
three key perspectives: (a) a data engine that can automatically transform human narrations into
7millionegocentricQAsamples, (b)amultimodallanguagemodeldesignedforegocentricvideo
comprehension,and(c)thecurationofachallengingegocentricvideounderstandingbenchmark.
ABSTRACT
Thisresearchaimstocomprehensivelyexplorebuildingamultimodalfoundation
modelforegocentricvideounderstanding. Toachievethisgoal,weworkonthree
fronts. First,asthereisalackofQAdataforegocentricvideounderstanding,we
developadataenginethatefficientlygenerates7Mhigh-qualityQAsamplesfor
egocentric videos ranging from 30 seconds to one hour long, based on human-
annotated data. This is currently the largest egocentric QA dataset. Second, we
contribute a challenging egocentric QA benchmark with 629 videos and 7,026
questions to evaluate the models’ ability in recognizing and memorizing visual
detailsacrossvideosofvaryinglengths.Weintroduceanewde-biasingevaluation
methodtohelpmitigatetheunavoidablelanguagebiaspresentinthemodelsbeing
evaluated. Third, we propose a specialized multimodal architecture featuring a
novel “Memory Pointer Prompting” mechanism. This design includes a global
glimpsesteptogainanoverarchingunderstandingoftheentirevideoandidentify
key visual information, followed by a fallback step that utilizes the key visual
information to generate responses. This enables the model to more effectively
comprehend extended video content. With the data, benchmark, and model, we
successfullybuildMM-Ego,anegocentricmultimodalLLMthatshowspowerful
performanceonegocentricvideounderstanding.
1 INTRODUCTION
Studyonegocentricvideosexploreshowmachinescanseeandunderstandtheworldfromafirst-
person,self-centeredperspective. Egocentricvideosdiffersignificantlyfromstatic-cameravideos,
such as movies or animations, both in terms of content and viewpoint. The content of egocentric
videosprimarilyrevolvesaroundhumandailyactivities. Thesevideostypicallyshareaperspective
similartohumanvision,wherethecameraandviewpointfrequentlymove.Asaresultofthesechar-
acteristics, egocentric videos exhibit a distinct data distribution compared to static-camera videos,
which has motivated a new area of research. In recent years, research interest in egocentric intel-
ligence has been on the rise (Sigurdsson et al., 2018; Damen et al., 2018; Grauman et al., 2022;
WorkdoneduringaninternshipatApple.†FirstAuthors.‡SeniorAuthors.
1
4202
tcO
9
]VC.sc[
1v77170.0142:viXraMangalametal.,2023;Plizzarietal.,2024). Thisgrowinginterestisdrivenbytherapidadvance-
ments in AR/VR headsets and robotics, where cameras capture long-form egocentric videos in a
manner akin to human vision. Research on egocentric videos will allow these devices to under-
stand their surroundings and human intentions, fostering more advanced machine intelligence and
improvingthehuman-machineinteractionexperience,withimmeasurableresearchandapplication
potential.
However,researchonunderstandingegocentricvideosremainsinitsearlystages,withpreviousre-
searchprimarilycenteredonspecializedtaskssuchasstorysummarization(Leeetal.,2012),hand-
objectrelationshipunderstanding(Caietal.,2016),actionclassification(Cartasetal.,2017;Lietal.,
2021),andtemporalorspatialgrounding(Graumanetal.,2022). Incontrast,worksfocusingonde-
velopingamoregeneralegocentricvideounderstandingmodelcapableofcomplexunderstanding
remain rare. Despite that video multimodal large language models (MLLMs) demonstrate strong
videounderstandingandreasoningability(Zhangetal.,2023;Wangetal.,2024;Linetal.,2024;
Zhang et al., 2024b), most of these works are unsuitable for egocentric video understanding from
data,benchmark,andmodeldesignperspectives.
(a) From a data standpoint, although many MLLMs use some egocentric videos from Activi-
tyNet (Yu et al., 2019), Ego4D (Grauman et al., 2022), and Charades (Sigurdsson et al., 2018) in
theirtraining,theyhavenotbeentrainedonlarge-scaleegocentricvideodatasets,whichinherently
restrictstheirabilitytocomprehendlengthyfirst-personvideosandaccuratelyextractvisualdetails.
WhileEgo4D(Graumanetal.,2022)offersvaluablehuman-annotatedvideosandlabelsforcertain
egocentricvideounderstandingtasks,particularlyepisodicmemory(whichassessesamodel’sabil-
itytoretainvisualdetailsinsuchvideos),itsannotationsarenotstructuredforgeneratinglanguage
responses,makingthemunsuitablefortrainingMLLMs. Therefore,alarge-scaleegocentricvideo
QA corpus is still needed. (b) In terms of benchmarking, exisiting video QA benchmarks either
focusonshortervideos–suchasEgoSchema(Mangalametal.,2023)andQaEgo4D,whicheval-
uate using around 3-minute and 8-minute videos, respectively – or concentrate on Internet video
content(e.g.,Video-MME(Fuetal.,2024)). Thiscreatesanotablegapinegocentricvideounder-
standing benchmarks that encompass videos ranging from seconds to an hour in length. (c) From
a model design perspective, previous video MLLMs have primarily addressed long videos in two
ways.Thefirstapproachinvolvesuniformlysamplingalimitednumberofvideoframesasvisualin-
put,asseeninLietal.(2024);Linetal.(2024). Despiteitssimplicity,thisapproachachievesbetter
performanceamongopen-sourcemodelsonpublicvideobenchmarks(Fuetal.,2024),largelybe-
causeitsdesignensureshightrainingefficiencyandgoodscalingproperties. Thesecondapproach
involvesfeedingalargevolumeofvisualtokensintothetransformerbackboneandemployingen-
gineeringtechniques,suchastensorparallelismandsequenceparallelism(Xueetal.,2024;Zhang
etal.,2024a), tofacilitatetrainingwithmillionsofvisualtokensincontext. However, theselong-
context transformers suffer from slow training speeds and small overall batch sizes, which hinder
performanceimprovementsgiventheconstraintsofcomputationalresourcesandtrainingtime. In-
tuitively, even humans cannot remember every detail of an hour-long video. We believe a more
effectiveapproachistounderstandthevideoprogressively:firstgetanoverviewoftheentirevideo,
thenfocusonspecificdetailswithparticularquestionsinmind.
Building on the observations mentioned above, we introduce MM-Ego, an egocentric MLLM de-
signedtoprocessandunderstandlongegocentricvideos. Ourcontributionsarethreefold:
(i) Data. To scale training data for MLLMs with egocentric understanding ability, we develop
anefficientdataengine, usinga“narrationtoegocentricQA”strategy, toautomaticallysynthesize
a large-scale egocentric QA dataset based on video narration data. Notably, rather than relying
on existing vision-language models as labelers, we generate egocentric QAs based on the human-
annotated fine-grained video clip narrations. This approach ensures that our data quality is not
constrained by the limitations of existing vision-language labeling models. In this way, we create
thefirstlarge-scaleegocentricQAdataset,consistingofover7millionegocentricQAsamplesthat
span video lengths from seconds to over an hour. This dataset enables the training of models to
recognizeandretainvisualdetailsfromegocentricvideos.
(ii) Benchmark. To evaluate the MLLMs’ performance in understanding and memorizing visual
details fromegocentric videos, we proposethe EgoMemoria benchmark. This challenging bench-
markincludes7,026multiple-choicequestionsfor629egocentricvideosrangingfrom30secondsto
1hour. IntheexperimentsonEgoMemoria,wefurtherinvestigatetheimpactofinevitablelanguage
2Key Frames Key Frames
Q: What activity did I repeatedly Q: Which hand did the man place
do with the cat? on his chest?
A: I combed the cat's fur with a A: The man placed both hands on
Video Narrations comb. his chest.
Video Clip 1: “I opens the rice cooker.” Q: What did I do with the pear aft
Q: How did the man adjust the
Video Clip 2: “I stirs the rice.” camera on his head? er slicing it?
A: I moved the pear on the tray w
Video Clip 3: “I closes the rice cooker.” A: The man adjusted the camera ith the knife in my right hand.
on his head with both hands.
LLM Prompt Q: What did I use to measure the Q: What object did I rinse at the
Egocentric QA wooden plank? tap?
A: I used the tape rule to measure A: I rinsed a spoon at the tap.
Video: [Clip 1, Clip 2, Clip 3]
the wooden plank.
Question: “Did I leave the rice cooker
open after using it?”
Answer: “No, I closed the rice cooker Q: Which hand did I use to turn on Q: Which hand did I use to pick a
after stirring the rice.” the tap initially? nother fabric from the table?
Key Frame: 3 A: I used my left hand. A: I used my left hand.
Figure2: “NarrationtoEgocentricQA”dataengine. Givenasequenceofhuman-annotatedvideo
narrations, we instruct a language model (GPT-4o) to generate egocentric understanding-related
questionsandanswers,alongwithidentifyingthekeyframesnecessarytoanswerthosequestions.
biasesacrossdifferentmodelsduringevaluationandintroduceadebiasedmetrictomoreaccurately
assessthemodels’trueegocentricunderstandingcapabilities.
(iii) Model. For our MM-Ego model, we develop a progressive approach to handle egocentric
videos by introducing a Memory Pointer Prompting method. It consists of two steps: “global
glimpse”and“fallback”. Intheglobalglimpsestep,weextractcompressedframe-levelvisualem-
beddingsfromtheentirevideotogetaglobalunderstanding. Then, weemployamemorypointer
embedding,designedtoexamineallcompressedframe-levelvisualembeddingsalongwiththeques-
tion embeddings, to aid in identifying key visual embeddings in a question-aware manner. In the
following fallback step, the selected key visual embeddings, in a higher-resolution form, are then
usedasfinalinputtotheLLMforprocessingandgeneration. Thisapproachallowsustoachievea
globalunderstandingoftheentirevideowhilealsoidentifyingandutilizingkeyvisualinformation
toanswerquestionsrelatedtovisualdetails.
2 METHOD
2.1 “NARRATIONTOEGOCENTRICQA”DATAENGINE
AsoutlinedinSection1,high-qualityegocentricQApairsarelackingfortraininganMLLMwith
egocentric video understanding ability. To address this gap, we develop an innovative “narration
to egocentric QA” data engine that automatically generates episodic memory-related QA samples
based on human-annotated video clip narrations from the Ego4D dataset (Grauman et al., 2022)
withouttheneedforadditionalmanualannotations.
Our approach leverages over 3,000 hours
of privacy-protected, de-identified egocentric 0.5-1 min 1-2 min 2-4 min 0.5-1 min 1-2 min 2-4 min
videos accompanied by more than 3 million 4-10 min 10-20 min 20-40 min 4-10 min 10-20 min 20-40 min
40-60 min 40-60 min
high-quality, human-created narrations. These
2%1%
fine-grained language descriptions provide a 5% 8% 3% 18%
richresourceforgeneratingQApairs. 11%
35%
17%
Theworkflowofthedataengineisillustratedin
18% 18%
Figure2. Byorganizingsequentialvideoclips
{Clip1, Clip2, ..., ClipN} and their corre- 18%
sponding narrations {Narration1, Narration2, 28% 18%
...,NarrationN}inproperchronologicalorder, (a) Before balancing (a) After balancing
we create comprehensive narration paragraphs
Figure3: Videolengthdistributioninouregocen-
that describe entire video sequences. We then
tricQAdataset.
employapowerfultext-onlylanguagemodel,
i.e.,GPT4-o,togeneratediverseandconfidentQApairsrelatedtoepisodicmemorybasedonthese
narrationparagraphs. Thelanguagemodelisinstructedtoattachtheindexofthenarrationsentence
uponwhicheachQApairisbased. ThisindexingallowsustomapeachQApairbacktothecorre-
spondingtimeframesintheoriginalvideos,enablingtheextractionofkeyframeinformationcrucial
forsubsequentmodeltraining.
3ApplyingthisdataenginetotheextensiveEgo4Ddatasetallowsustoefficientlyscalethecreation
ofegocentricQAdata. Wepartitionthedatasetintotrainingandtestingsetsaccordingtotheofficial
Ego4Depisodicmemorytask.TheegocentricQAdatasetprovidesmorethan7millionQAsamples
in938Kmulti-turnconversations. Thedataencompassesvideosofvaryingdurations,rangingfrom
30secondsto1hour,asillustratedinFigure3. Toensurecomprehensivecoverageandpreventbias
towards shorter videos, we balance the number of conversations across different video lengths in
training. Thisisthefirstlarge-scaleegocentricQAdatasetfeaturingvideosofsuchextendedranges
ofduration.
Throughthesesteps, our“narrationtoegocentricQA”dataengineaddressesthescarcityoflarge-
scale,high-qualityegocentricQAdataforegocentricscenes,andsetsasolidfoundationforbuilding
MM-Ego,asophisticatedegocentricMLLM,whichweintroduceinthefollowingsection.
2.2 MM-EGOMODEL
OurmodelinggoalistodevelopanMLLMforhandlingegocentricvideos, whicharelengthyand
rich in visual details. On the one hand, frame-level information is necessary to capture the full
contentofthevideo,asskippingframesduringsamplingcouldresultinasignificantlossofvisual
details. Ontheotherhand, processingallvisualtokensgeneratedbythevisualencoderiscompu-
tationally challenging for the transformer model. For instance, if each image is encoded into 729
visualembeddings(tokens),thetotalnumberofvisualembeddingsfora300-framevideowouldbe
218,700. However, most MLLMs are trained with a context length of less than 10,000 tokens (Li
etal.,2024). Takingthesefactorsintoaccount,weintroducetheMM-Egomodel,whichisbuiltfor
handling a large volume of egocentric video frames while maintaining manageable computational
costswithinthetransformerbackbone. MM-EgointroducesaninnovativeMemoryPointerPrompt-
ingmechanism,whichoperatesintwomainsteps: globalglimpseandfallback. Wewillintroduce
thedetailsofMM-Egointhefollowingsections.
2.2.1 VISUALANDTEXTUALEMBEDDING
Given an input video and the question, the first step is to embed them into visual and textual em-
beddings separately for later processing. We begin by uniformly sampling the video into up to N
frames,whereN canbeintherangeofhundreds. Then,weextractper-framevisualfeaturemaps
fromtheseframesusingarobustvisionencoder,SigLIP-so400m(Zhaietal.,2023). Followingthe
methodoutlinedbyLietal.(2024), weapplya2-layerMLPtoprojectthevisualfeaturemapsto
the LLM embedding space and use average pooling to reduce the height and width of the visual
featuremapsbyafactoroftwoandflattentheheightandwidthdimension,resultinginN relatively
high-resolutionvisualembeddings{Vi ∈RT×C,i∈[1,N]}whereT istheembeddinglengthand
C istheembeddingdimension. Forthetextualembedding,sinceweuseQwen2(Yangetal.,2024)
astheLLM,weuseitstokenizerandembeddinglayertotransformtheinputtextintotextualembed-
dings. Forquestionq,wedenotethecorrespondingtextualquestionembeddingas{Eq ∈RTq×C,
que
q ∈[1,Q]}whereQisthetotalnumberofquestionsandT istheembeddinglengthofquestionq.
q
2.2.2 MEMORYPOINTERPROMPTING
As processing all N high-resolution visual embeddings with the LLM is computationally diffi-
cult, we propose to identify key visual embeddings in a question-aware manner and only send
those selected embeddings to the subsequent LLM. Inspired by previous works on Pointer Net-
works (Vinyals et al., 2015; Merity et al., 2016), we propose a Memory Pointer Prompting mech-
anism, which is illustrated in Figure 4. Memory Pointer Prompting consists of two steps during
inference: globalglimpseandfallback. Intheglobalglimpsestep,keyvisualembeddingsareiden-
tifiedfromallframe-levelembeddings,guidedbythecontextofthequestion.Duringthesubsequent
fallbackstep,theimportantvisualembeddingsareselected,andtheirhigher-resolutionversionsare
providedtotheLLMtransformerbackboneforfurtherprocessingandlanguageresponsegeneration.
Global Glimpse Step. We begin by compressing the visual embeddings through average pooling
alongtheembeddinglengthdimension,resultinginasetofcompressedvisualembeddings{Ei ∈
vis
R1×C,i ∈ [1,N]}. Next,weintroducealearnablememorypointerpromptembeddingP ∈ R1×C,
4LLM Input in Training Phase
Question: Did I
close the rice Compr Ees mse bd e dV di is nu ga sl
cooker?
Question Embedding
Encoder 1 Encoder 2 ... Encoder N Tokenizer Memory Pointer Memory Pointer Embedding Input LLM
Compress Compress Compress Embed Embedding Ground-Truth Key High-
Res Visual Embeddings
Question Embedding
LLM Answer Embedding (b)
Softmax
Selected High-Res Input
Top- Visual Embeddings LLM
Correlation
Scores Selected Indices
Binary Cross-Entropy Loss Question Embedding
Ground-Truth Key Frame Indices Step 1. Global Glimpse Step 2. Fallback (a)
Figure4: (a)OverviewoftheproposedMemoryPointerPromptingmechanism. Itsinferencecon-
sistsoftwosteps: (1)GlobalGlimpse: Weconcatenatethecompressedvisualembeddingsfromall
frames,denotedasEi fori ∈ [1,N],withthequestionembeddingsE1 andthememorypointer
vis que
embeddingP1.ThiscombinedembeddingsequenceistheninputintotheLLM.Fromthelastlayer,
weextractembeddingsandcomputethedotproductbetweenthememorypointerembeddingandall
compressedvisualembeddingstogeneratethecorrelationscores.Theindicesoftheframeswiththe
topkscoresareselected. Duringtraining,thecorrelationscoresaresupervisedbyground-truthkey
frameindicesviaabinarycross-entropyloss. (2)Fallback: Thehigh-resolutionvisualembeddings
correspondingtotheselectedindicesarefedintotheLLMalongwiththequestionembeddingsfor
finalprocessingandresponsegeneration. (b)IllustrationofLLMinputsequenceduringtraining.
duplicateitQtimes,yielding{Pi ∈R1×C,i∈[1,Q]},andconcatenatetheembeddingsasfollows:
[E1 ,E2 ,...,EN,E1 ,P1].
vis vis vis que
HereQ = 1asMLLMsgenerateanswersforonlyonequestionatatime. Inthisway,thequestion
embedding is followed by a pointer embedding, which will be used to identify key visual embed-
dingswithknowledgeofthequestionembedding. Theentireembeddingsequenceisthenfedinto
theLLM,fromwhichweobtaintheoutputembeddingsequenceofthefinallayer:
[E′1,E′2,...,E′N,E′1 ,P′1].
vis vis vis que
We extract and stack the processed visual embeddings {E′i ∈ R1×C,i ∈ [1,N]} to obtain the
vis
matrixE ∈RN×C. WeconductasoftmaxdotproductoperationbetweenE andP′1:
vis vis
s=Softmax(E
·P′1T)∈RN.
(1)
vis
Heresisacorrelationscorevectorindicatingthecorrelationbetweenthequestionandeachframe.
BalancingExplorationandExploitation. Ourapproachtoselectingkeyvisualembeddingspar-
allels the principles of Bayesian Optimization (Frazier, 2018), where the objective function is ex-
pensive to evaluate. In such cases, it’s important to balance exploration (sampling in areas where
theuncertaintyishigh)andexploitation(samplinginareaswherethesurrogatemodelpredictshigh
performance). However,relyingsolelyontheaforementionedMemoryPointerPromptingmaylead
to overemphasizing certain areas of interest, potentially undermining the exploration process. To
mitigatethisissue,weintroduceperturbationsintothescoredistributionbyincorporatingauniform
samplingdistribution. Theprobabilityvectorofuniformsamplingcanbewrittenas:
(cid:26)
α ifi∈linspace(0,N,k),
ui = (2)
0 otherwise.
Hereαisanexplore-exploitbalancingparametertoadjusttheprobabilitydistribution. Weoverlap
theprobabilityvectorofuniformsamplingandscorematrixs:
s←s+u. (3)
We then identify the top-k indices as the set {S ,i ∈ [1,k]}. In this way, we find the key visual
i
embeddingsinaquestion-awaremanner.
5Table 1: Distribution of videos and QA samples
withdifferentlengths. Table2: Distributionofcorrectoptions
inMCQs.
Class Short Medium Long Sum
Minutes 0.5-1 1-2 2-4 4-10 10-20 20-40 40-60 - Option A B C D
Videos 100 100 100 100 100 100 29 629 Counts 1776 1751 1770 1729
QAs 500 498 987 997 1715 1792 537 7026
FallbackStep. Duringinference,asshowninFigure4,withthesetofindices{S ,i ∈ [1,k]}for
i
theselectedvisualembeddings,wenowassembletheLLMinputsequenceasfollows:
[ VS1,VS2,...,VSk ,E1 ].
que
(cid:124) (cid:123)(cid:122) (cid:125)
SelectedTop-kVisualEmbeedings
As previously introduced, VS1,VS2,...,VSk denote the selected top-k high-resolution visual em-
beddings, which provide more visual details than the compressed visual embeddings. This new
embedding sequence is fed into the LLM to generate the final language response. In summary,
the proposed Memory Pointer Prompting approach allows us to consider the full scope of video
informationwhilefilteringoutredundantdataintheLLMtransformer,ensuringcomputationaleffi-
ciency. ThenewinputservesasthefinalinputoftheLLMtogeneratethelanguageresponsegiven
thevisualandtextualinformation.
TrainingProcedure. GiventhenoveldesignofMM-Ego, itstrainingprocedureisdifferentfrom
popularMLLMs(Liuetal.,2023). Specifically, lettheanswerembeddingforquestionq ∈ [1,Q]
bedenotedasEq ,thentheinputembeddingsequenceduringthetrainingprocessisrepresentedas:
ans
[ E1 ,E2 ,...,EN ,E1 ,P1,...,EQ ,PQ, VS1,VS2,...,VSk ,E1 ,E1 ,...,EQ ,EQ ].
vis vis vis que que que ans que ans
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
CompressedVisualEmbeddings SelectedHigh-ResVisualEmbeddings
Wealsoprovideasimplifiedillustration(whereQ = 1)oftheinputembeddingsequencestructure
during training in Figure 4. Here, we begin by inputting the compressed visual embeddings for
all N frames, followed by the question embedding and memory pointer embedding. Next, we
integrate the k selected high-resolution visual embeddings (based on the ground-truth key frame
labels),andfinally,incorporateboththequestionandanswerembeddings. Oncetheinputsequence
ispreparedasoutlinedabove,wecantrainMM-Egosimilarlytotraditionallargelanguagemodels.
Thecompressedvisualembeddings,questionembedding,andmemorypointerembeddingsusedas
prefixesdonotcontributetothelanguagecross-entropyloss.
WhentrainingonsamplesfromourcuratedegocentricQAdatasetwherethereareground-truthkey
framelabelsforeachquestion,wecomputethecorrelationscorevectorsintheglobalglimpsestep,
andsuperviseitusingabinarycross-entropyloss. Fortrainingsamplesthatlackground-truthkey
framelabels,weomittheprefixes,whichresultsinthetraditionalMLLMtrainingprocess.
3 EXPERIMENTS
In the experiment section, we will first present a new egocentric video understanding benchmark,
specificallydesignedtoassessepisodicmemorycapabilities. Followingthis,wewillperformcom-
prehensive experiments to evaluate MM-Ego, utilizing both the newly introduced benchmark and
existingpublicbenchmarks.
3.1 EGOMEMORIABENCHMARK person video walkheld move
phone do
place
To evaluate the performance of egocentric floor hand
MLLMs,especiallyintermsofepisodicmem-
action use
ory ability, we propose a new benchmark
called EgoMemoria. Specifically, we generate times
drop pick
memory-related questions and answers from wood man interact
human-annotated narrations in the validation table put
Noun Verb
set of the Ego4D dataset. To ensure diversity,
Figure5:Themostfrequentlyoccurringverbsand
foreachvideoweonlygeneratealimitednum-
nounsinEgoMemoria.
ber of questions. We divide the videos into
6Question 1: Which hand did I use to touch the handle of the drilling machine?
Choices: A: I touched the handle of the drilling machine with my right hand. B: I touched the handle of the drilling machine
with both hands. C: I touched the handle of the drilling machine with my foot. D: I touched the handle of the drilling machine
with my left hand.
Prediction: D Selected
Label: D Frames
Scores
Question 2: Did I use a ladder during the process?
Choices: A: No, I used a step stool instead. B: No, I stood on a chair. C: Yes, I climbed down the ladder. D: No, I reached up
without assistance.
Prediction: C Selected
Label: C Frames
Scores
Question 3: What did I connect to the socket?
Choices: A: I connected an electric cleaner to the socket. B: I connected a digital clock to the socket. C: I connected a table
lamp to the socket. D: I connected a coffee maker to the socket.
Prediction: A Selected
Label: A Frames
Scores
Question 4: What color was the car that drove past last?
Choices: A: 1. The car that drove past last was red. B: 2. The car that drove past last was blue. C: The car that drove past last
was black. D: 3. The car that drove past last was white.
Prediction: A Selected
Label: C Frames
Scores
Question 5: Where did I place the knife after peeling the zucchini?
Choices: A: I left the knife in the sink. B: I put the knife back in the drawer. C: I placed the knife on the counter. D: I dropped
the knife on the cutting board.
Prediction: A Selected
Label: D Frames
Scores
Figure6:EgoMemoriaQAsvisualizationandpredictionanalysisoftheglobalglimpsestep.Wefind
highconsistencybetweentheidentifiedkeyframesandthequestions,demonstratingtheeffective-
ness of the proposed Memory Pointer Prompting method. The visualized correlation scores show
distinct distributions for different questions given the same video, indicating its question-specific
nature. The✓indicatesthattheselectedframesarerelevanttothequestions.
sevendifferentlengthranges: 0.5to1min,1to2min,2to4min,4to10min,10to20min,20to
40min,and40to60min. Weaimtobalancethenumberofsamplesindifferentvideolengths. The
distribution of videos and corresponding question-answer pairs (QAs) for each category is shown
in Table 1. Furthermore, we group these video lengths into three broader categories: short (0.5 to
2min),medium(2to20min),andlong(20to60min). Intotal,wecollect629videoswith7,026
questions.ThemostfrequentlyoccuringverbsandnounsinthequestionsarevisualizedinFigure5.
Since free-form answers are typically evaluated using a closed-source LLM as a judge, the evalu-
ation can be inconsistent and subject to significant variance, especially due to model version up-
dates. Toensuremorereliable,standardized,andconsistentperformanceevaluation,weconvertthe
free-formanswersintomultiple-choicequestions(MCQs),whichhelpsreducescoreinstability. In
practice, basedonthefree-formanswer, weinstructChatGPTtogeneratethreeadditionalchoices
that are plausible but incorrect, considering the original question and answer. We then randomize
theorderofthesechoicestoachieveauniformdistributionofcorrectoptions,asshowninTable2,
tominimizebiasinoptionplacement. WevisualizesomerandomlysampledexamplesinFigure6.
7Table3: PerformancecomparisonandlanguagebiasanalysisofdifferentmodelsontheEgoMemo-
ria benchmark. Our MM-Ego model demonstrates the best performance both before and after ex-
cludingthelanguagebiasofdifferentmodels.
LLaVA-OV(Lietal.,2024) EgoSFT MM-Ego
Method
Short Medium Long Avg Short Medium Long Avg Short Medium Long Avg
Original 70.24 64.94 61.19 65.45 79.06 76.34 73.51 76.30 79.96 79.64 79.09 79.56
ExcludeLLaVA-OVBias 56.44 49.64 44.83 50.30 66.37 64.15 60.03 63.52 71.97 70.68 68.15 70.26
ExcludeEgoSFTBias 55.75 49.27 45.21 50.08 61.73 59.59 54.50 58.61 67.70 66.33 63.89 65.97
ExcludeMM-EgoBias 47.41 42.11 35.22 41.58 50.60 46.39 40.38 45.79 49.80 49.11 43.81 47.58
MeanDebiasedAccuracy(MDA) 53.20 47.01 41.76 47.32 59.56 56.71 51.64 55.97 63.16 62.04 58.62 61.27
3.2 EXPERIMENTALSETUP
TrainingData. Weemployajointimage-videosupervisedfine-tuning(SFT)strategy. Toenhance
themodel’scapabilityinunderstandingabroaderrangeofvisualdata,wecombineouregocentric
QA dataset with a variety of multimodal datasets. We curate an SFT dataset mixture consisting
ofouregocentricQAdataset,Ego4Dnarrationdataset(Graumanetal.,2022),LLaVA-NeXTSFT
collection(includingChartQA(Masryetal.,2022),AI2D(Hiippalaetal.,2021),DocVQA(Mathew
et al., 2021), DVQA (Kafle et al., 2018), COCO (Lin et al., 2014)), ShareGPT4V (Chen et al.,
2023a),synthdog-en(Kimetal.,2021)),ShareGPT-4o(Chenetal.,2023b),ALLaVAinstruct(Chen
etal.,2024a),ShareGPT4Video(Chenetal.,2024b),sherlock(Hesseletal.,2022),ScienceQA(Lu
etal.,2022),NExT-QA(Xiaoetal.,2021),andActivityNet-QA(Yuetal.,2019).
ImplementationDetails. Themodelistrainedforoneepochwithabaselearningrateof1×10−5,
usingacosinescheduler. Thebatchsizeissetto128. Wesampleamaximumof300frames(N =
300)andselect32visualembeddingsintheproposedmemorypointerpromptingmechanism. By
default,wesettheexplore-exploitbalancingparameterαto0.1.
Pretrained Models. Our MM-Ego model is initialized from LLaVA-OV 7B (Li et al., 2024), a
state-of-the-artMLLMknownforitsgoodperformanceongeneralmultimodalunderstandingtasks.
Following the same architecture, we use the SigLip-so400M ViT (Zhai et al., 2023) as the visual
encoderforembeddingvideoframesandQwen2-7B(Yangetal.,2024)astheLLMarchitecture.
3.3 MAINRESULTS
We first conduct experiments on our EgoMemoria benchmark, primarily comparing three models:
LLaVA-OV(Lietal.,2024),itsfine-tunedversionusingourMM-EgoSFTdatamixture(referredto
as“EgoSFT”),andourMM-Egomodel,whichincorporatestheproposedMemoryPointerPrompt-
ingmentionedinSection2.2.2. WeshowtheEgoMemoriaaccuracyinthefirstrowofTable3. We
observeasignificantimprovementinthemodel’sperformanceonegocentricQAsaftertrainingon
ourMM-Egodatamixture,attributedtotherichegocentricknowledgeprovidedbyourcuratedego-
centric QA training data. Moreover, leveraging the MM-Ego model architecture further enhances
performance,thankstotheeffectiveMemoryPointerPromptingmechanism.
However, wenoticethattheoriginaloverallperformancemetricsarehigherthananticipated, rais-
ing curiosity about the extent to which language bias contributes to the models’ accuracy. To an-
swerthisquestion,weconductadditionalexperimentsaimedateliminatingtheselanguagebiases.
Specifically, we test the three model variants on the EgoMemoria benchmark without any visual
inputs, identifying questions that could be correctly answered without videos as “language-biased
questions”. Then, we evaluate the models’ performance on the subset of the benchmark without
language-biasedquestions. Forfairness,weapplythisdebiasingprocessacrossallthreemodelsso
that they are evaluated on the same sets of data. We calculate the mean accuracy of the debiased
variants,referredtoasthe“MeanDebiasedAccuracy(MDA)”. TheresultsarepresentedinTable3.
Asexpected,afterremovingthelanguage-biasedquestions,theaccuracyofallthreemodelsdrops
significantlytoamorereasonablelevel.Theperformancedeclineisnotablymorepronouncedinthe
“Medium” and “Long” classes compared to the “Short” class. For example, the average accuracy
of LLaVA-OV across the three classes (short, medium, and long) drops from 65.45 to 47.32. The
decrease in the “Short” class is 17.04, in the “Medium” class is 17.93, and in the “Long” class is
19.43. Despitethis,westillobserveimprovementsinMDAaftertrainingwithSFTdatagenerated
byourMM-Egodataengine(+8.65)andapplyingourMemoryPointerPromptingmethod(+13.95).
Theseresultsdemonstratetheeffectivenessofourapproachevenafterconsideringlanguagebias.
8Table4: Comparisonwithstate-of-the-artvideoMLLMs. MM-Egoshowsstrongperformanceon
egocentricunderstandingandcompetitiveperformanceonInternetvideounderstanding.
EgoMemoria(MDA) EgoSchema Video-MME(w/osubs)
Method
Short Medium Long Avg Full Short Medium Long Entire
GPT-4o 64.31 59.47 57.65 60.48 72.2 80.00 70.30 65.30 71.90
LLaVA-NeXT-Video-7B-DPO(Zhangetal.,2024b) 30.38 25.95 21.49 25.94 - - - - -
LLaVA-NeXT-Video-32B-Qwen(Zhangetal.,2024b) 43.78 33.76 31.04 36.19 60.85 - - - 60.20
LLaVA-OV7B(Lietal.,2024) 53.20 47.01 41.76 47.32 60.10 69.30 56.00 49.40 58.30
MM-Ego(ours) 63.16 62.04 58.62 61.27 69.03 67.60 55.70 47.80 57.00
Table 5: MDA on EgoMemoria when inferring with different numbers of frames. Our MM-Ego
modelshowsasmallerrelativedroponaveragewhendecreasingthenumberofsampledframes.
Short Medium Long Avg
Frames
LLaVA-OV EgoSFT MM-Ego LLaVA-OV EgoSFT MM-Ego LLaVA-OV EgoSFT MM-Ego LLaVA-OV EgoSFT MM-Ego
32 53.20 59.56 63.16 47.01 56.71 62.04 41.76 51.64 58.62 47.32 55.97 61.27
16 52.68 60.45 63.82 46.37 55.99 60.81 40.12 51.15 58.16 46.39 55.86 60.93
8 50.76 59.59 62.22 44.82 54.55 58.23 39.41 49.11 55.19 44.99 54.42 58.55
4 50.43 55.36 62.30 42.54 52.08 58.44 38.88 48.40 54.65 43.95 51.95 58.46
Rel.Diff 5.20% 7.07% 1.36% 9.49% 8.16% 5.81% 6.89% 6.26% 6.77% 7.12% 7.19% 4.59%
To better understand the capability of MM-Ego, we compare its performance with state-of-the-art
videoMLLMsonEgoMemoriaandprevalentlarge-scalevideoQAbenchmarks,includingthelong
egocentric video understanding benchmark EgoSchema (Mangalam et al., 2023) and the Internet-
video-based long-video understanding benchmark Video-MME (Fu et al., 2024). The results are
showninTable4. OnEgoMemoria,GPT-4oisevaluatedusing32uniformlysampledframesfrom
the videos, while other models follow their respective official inference settings. The MDA on
EgoMemoria is computed using the debiased subsets used in Table 3. Notably, MM-Ego exhibits
thehighestperformanceonEgoMemoria,particularlyinthe‘Medium’and‘Long’classes. Onthe
EgoSchemabenchmark, ourmodelachievesasubstantialperformancegainof+8.18overthepre-
viousstate-of-the-artopen-sourcemodel,underscoringtheeffectivenessofbothourdataandmodel
design for egocentric understanding. Additionally, on the challenging Internet video understand-
ingVideo-MMEbenchmark,ourmodelisonparwiththeleadingmodelofsimilarparametersize.
TheseresultsshowcaseMM-Ego’scapabilityinegocentricvideounderstandingwhilepreservingits
generalvideocomprehensionabilities.
61.0
3.4 MODELANALYSIS
60.5
60.0
Quantitative Analysis of Different Numbers of Frames. To
59.5 evaluate the influence of sampling different numbers of frames
59.0
for different models, we calculate the mean debiased accu-
racy(MDA)inTable5. Therelativeperformancedropfromsam- 58.5
pling 32 frames to sampling 4 frames is also calculated. As ex- 58.0 0.00 0.05 0.10 0.15 0.20
Alpha
pected, all models exhibit a decrease in performance with fewer
sampled frames. Notably, MM-Ego exhibits a smaller average Figure7: MDAscoreswithdif-
performance drop when the number of frames is reduced due to ferentαvaluesforexplore-and-
itsabilitytoidentifykeyframesgivenlowercomputationalbud- exploitbalancing.
get. The relative performance drop in the short category is con-
siderablysmallercomparedtothemediumandlongcategories,likelybecauseshortervideosrequire
fewerframestocomprehend.
QualitativeAnalysisofMemoryPointerPrompting. InFigure6,wepresentaqualitativeanalysis
of the accuracy of Memory Pointer Prompting on EgoMemoria. We randomly select samples and
visualize the key frames identified by the global glimpse step in Memory Pointer Prompting. The
resultsshowastrongalignmentbetweenthequestionsandtheselectedframes. Infailurecases,we
observethattheissuesareoftenduetotheambiguityofthequestions,causingthemodeltostruggle
withaccuratelylocalizingthekeyvisualembeddings.Furthermore,thevisualizedcorrelationscores
duringtheglobalglimpsestepshowdistinctpatternsacrossvariousvideosandquestions,confirming
itseffectivenessinselectingkeyvisualembeddingstailoredtothespecificquestions.
QuantitativeAnalysisofExplore-ExploitBalancingParameterα. AsdiscussedinSection2.2.2,
we design an explore-exploit balancing parameter α to fuse the uniform distribution and the sam-
9
)%(serocS
ADM1 2 3
4 5 6
Question 1: Where did I leave my wallet? 2 minutes video captured by a wearable device
MM-Ego: I left the wallet on the table. (Correct!)
Question 2: Did I see anything I can play with?
MM-Ego: Yes I have a guitar at the corner. (Correct!)
Question 3: How many times did I interact with the remoter?
MM-Ego: I interacted with the remoter three times. (Correct!)
Figure8: Real-worldconversationexamplesgeneratedbyMM-Ego. Theinputisa2-minutelong
egocentricvideorecordedusingacameraonanoff-the-shelfwearabledevice. MM-Egocanaccu-
ratelyidentifykeyvisualdetailsandprovidecorrectanswerstotheuser’smemory-relatedquestions.
pling probability computed by Memory Pointer Prompting. We illustrate MM-Ego’s performance
withvaryingvaluesofαinFigure7. Theresultsshowthatα=0.1achievesthebestperformance,
whilelargerorsmallervaluesofαtendtoeitherover-exploreorover-exploit.
ConversationExamplesbyMM-Ego. InFigure8,weshowareal-worlddemoofMM-Ego,where
theinputvideoisa2-minutelongegocentricvideocapturedbyacameraonanoff-the-shelfwearable
device (this video is not used in our dataset). MM-Ego is able to correctly answer the episodic
memory-relatedquestionsgiventheegocentricvideo,despitethedifferenceindatadomain.
4 RELATED WORK
Multimodal Large Language Models. Recent advancements in Large Language Models (Ope-
nAI,2023;Touvronetal.,2023)havesparkedsignificantinterestindevelopingMultimodalLarge
Language Models (MLLMs) that combine the language understanding capabilities of LLMs with
multi-modalperceptionabilities(Alayracetal.,2022;Daietal.,2023;Zhuetal.,2023;McKinzie
etal.,2024). Forvideo-basedMLLMs,mostworksfollowastructureakintoimage-basedMLLMs.
To handle the large volume of video frames, some methods reduce the number of frames (Zhang
etal.,2023;Wangetal.,2024;Maazetal.,2024;Xuetal.,2024),whichresultsinthelossofmany
visualdetails.OthersextendtheLLMs’contextlengthbyemployingparalleltechniques(Xueetal.,
2024),butthisoftenleadstolowtrainingefficiency. Unliketheseapproaches,ourmethodpreserves
globalawarenessoftheentirevideo,allowsforattentiontovisualdetails,andisefficientlytrainable.
Egocentric Video Understanding. While the growing field of egocentric video understanding is
stillinitsinfancy, therehavebeenmanyinfluentialworks. Foracomprehensiveoverviewofego-
centricvisionpleaserefertoPlizzarietal.(2024).Onthedata/benchmarkside,representativeworks
includeEgo4D(Graumanetal.,2022),Ego-Exo4D(Graumanetal.,2024),andEPIC-KITCHENS-
100(Damenetal.,2018).Whenalsoconsideringlanguage,priorworkonegocentricvideo-language
benchmarksincludeQaEgo4D(Ba¨rmann&Waibel,2022)andEgoSchema(Mangalametal.,2023).
For understanding long egocentric videos, prior modeling efforts include GroundVQA (Di & Xie,
2024), Encode-Store-Retrieve (Shen et al., 2023), and R-VLM (Xu et al., 2023). However, most
previousworksfocusonclassicvideounderstandingtaskssuchasactivityrecognitionandtemporal
grounding, andhencetheydonotinvolvealanguagemodel. Incontrast, weproposetodevelopa
multimodalLLMtotackleopen-endedegocentricvideo-languageunderstanding.
105 DISCUSSION
Limitation and Future Work. While MM-Ego demonstrates a strong ability in egocentric un-
derstanding, thereisstillroomforfurtherimprovement. Onthedataandbenchmarkside, wecan
introducemorediverseegocentricunderstandingcorpus. Forthemodelitself, weplantoenhance
itscapacitytoprocessalargernumberofframes,suchasattheorderofthousands,tobetterhandle
longerorevenalways-onegocentricvideos.
Conclusion. Inthispaper,wemakethreekeycontributionstowardsthedevelopmentofegocentric
foundationmodel: thecreationofthefirstlarge-scaleegocentricQAtrainingdataset,theintroduc-
tion of a novel model designed for effective long egocentric video comprehension, and the estab-
lishmentoftheEgoMemoriabenchmarkforassessingmodels’abilitytocapturevisualdetailsfrom
egocentricvideos. WehopethattheseeffortswillbenefitfurtherresearchonegocentricMLLMs.
ETHICS STATEMENT
Our proposed method does not involve the creation or introduction of any new video content. All
generateddataisderivedfrompubliclyavailable,privacy-protecteddatasets(Graumanetal.,2022).
Thedataisintendedexclusivelyforacademicresearchpurposesandwillnotbeusedforanycom-
mercialapplications. Wehaveadheredtoethicalstandardsbyensuringthatnoprivateorsensitive
datahasbeenusedorcompromised.
REPRODUCIBILITY STATEMENT
WeprovideadetailedexplanationofthedatasynthesisprocessinourdataengineinSection2.1.We
also elaborate on our model design in Section 2.2.2. Additionally, we outline the implementation
details,includingthetraininghyperparametersinSection3.2.
REFERENCES
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguage
modelforfew-shotlearning. InNeurIPS,2022.
LeonardBa¨rmannandAlexWaibel. Wheredidileavemykeys?-episodic-memory-basedquestion
answeringonegocentricvideos. InCVPR-W,2022.
Minjie Cai, Kris M Kitani, and Yoichi Sato. Understanding hand-object manipulation with grasp
typesandobjectattributes. InRobotics: ScienceandSystems,volume3,2016.
AlejandroCartas,JuanMar´ın,PetiaRadeva,andMariellaDimiccoli.Recognizingactivitiesofdaily
livingfromegocentricimages. InPatternRecognitionandImageAnalysis,2017.
GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,Zhi-
hongChen,JianquanLi,XiangWan,andBenyouWang. Allava: Harnessinggpt4v-synthesized
dataforalitevision-languagemodel. arXiv,2024a.
LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
Sharegpt4v: Improvinglargemulti-modalmodelswithbettercaptions. arXiv,2023a.
Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong
Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang.
Sharegpt4video: Improving video understanding and generation with better captions. arXiv,
2024b.
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong
Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl:
Scalingupvisionfoundationmodelsandaligningforgenericvisual-linguistictasks.arXiv,2023b.
11Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li,
Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models
withinstructiontuning. arXiv,2023.
Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evange-
los Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray.
Scalingegocentricvision: Theepic-kitchensdataset. InECCV,2018.
Shangzhe Di and Weidi Xie. Grounded question-answering in long egocentric videos. In CVPR,
2024.
PeterIFrazier. Atutorialonbayesianoptimization. arXiv,2018.
ChaoyouFu,YuhanDai,YondongLuo,LeiLi,ShuhuaiRen,RenruiZhang,ZihanWang,Chenyu
Zhou,YunhangShen,MengdanZhang,etal. Video-mme: Thefirst-evercomprehensiveevalua-
tionbenchmarkofmulti-modalllmsinvideoanalysis. arXiv,2024.
KristenGrauman,AndrewWestbury,EugeneByrne,ZacharyChavis,AntoninoFurnari,RohitGird-
har, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan,
Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray,
Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Car-
tillier,SeanCrane,TienDo,MorrieDoulaty,AkshayErapalli,ChristophFeichtenhofer,Adriano
Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang,
Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico
Landini,ChaoLi,YanghaoLi,ZhenqiangLi,KarttikeyaMangalam,RaghavaModhugu,Jonathan
Munro, TullieMurrell, TakumiNishiyasu, WillPrice, PaolaRuiz Puentes, Merey Ramazanova,
Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo,
Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Cran-
dall,DimaDamen,GiovanniMariaFarinella,ChristianFuegen,BernardGhanem,VamsiKrishna
Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva,
HyunSooPark, JamesM.Rehg, YoichiSato, JianboShi, MikeZhengShou, AntonioTorralba,
LorenzoTorresani,MingfeiYan,andJitendraMalik. Ego4d: Aroundtheworldin3,000hoursof
egocentricvideo. InCVPR,2022.
KristenGrauman,AndrewWestbury,LorenzoTorresani,KrisKitani,JitendraMalik,Triantafyllos
Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d:
Understandingskilledhumanactivityfromfirst-andthird-personperspectives. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.19383–19400,2024.
JackHessel,JenaDHwang,JaeSungPark,RowanZellers,ChandraBhagavatula,AnnaRohrbach,
KateSaenko,andYejinChoi. TheAbductionofSherlockHolmes: ADatasetforVisualAbduc-
tiveReasoning. InECCV,2022.
TuomoHiippala,MaliheAlikhani,JonasHaverinen,TimoKalliokoski,EvanfiyaLogacheva,Sera-
finaOrekhova,AinoTuomainen,MatthewStone,andJohnABateman. Ai2d-rst: Amultimodal
corpusof1000primaryschoolsciencediagrams. LanguageResourcesandEvaluation,55:661–
688,2021.
KushalKafle,BrianPrice,ScottCohen,andChristopherKanan. Dvqa: Understandingdatavisual-
izationsviaquestionanswering. InCVPR,2018.
Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung Park, Jinyeong Yim, Wonseok Hwang,
Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Donut: Document understanding trans-
formerwithoutocr. arXiv,2021.
YongJaeLee,JoydeepGhosh,andKristenGrauman. Discoveringimportantpeopleandobjectsfor
egocentricvideosummarization. InCVPR,2012.
BoLi,YuanhanZhang,DongGuo,RenruiZhang,FengLi,HaoZhang,KaichenZhang,YanweiLi,
ZiweiLiu,andChunyuanLi. Llava-onevision: Easyvisualtasktransfer. arXiv,2024.
Yanghao Li, Tushar Nagarajan, Bo Xiong, and Kristen Grauman. Ego-exo: Transferring visual
representationsfromthird-persontofirst-personvideos. InCVPR,2021.
12Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz,
MohammadShoeybi,andSongHan. Vila:Onpre-trainingforvisuallanguagemodels. InCVPR,
2024.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolla´r,andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InECCV,2014.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,
2023.
PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,
PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathoughtchainsfor
sciencequestionanswering. NeurIPS,2022.
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt:
Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. InACL,2024.
KarttikeyaMangalam,RaiymbekAkshulakov,andJitendraMalik.Egoschema:Adiagnosticbench-
mark for very long-form video language understanding. In NeurIPS Datasets and Benchmarks
Track,2023.
AhmedMasry, DoXuanLong,JiaQingTan, ShafiqJoty, andEnamulHoque. Chartqa: Abench-
markforquestionansweringaboutchartswithvisualandlogicalreasoning. arXiv,2022.
MineshMathew,DimosthenisKaratzas,andCVJawahar. Docvqa: Adatasetforvqaondocument
images. InProc.WACV,2021.
BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,PhilippDufter,
DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis&insights
frommultimodalllmpre-training. arXiv,2024.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv,2016.
OpenAI. ChatGPT:Optimizinglanguagemodelsfordialogue. https://openai.com/blog/
chatgpt,2023. Accessed: 2023.
Chiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Gio-
vanniMariaFarinella,DimaDamen,andTatianaTommasi. Anoutlookintothefutureofegocen-
tricvision. IJCV,pp.1–57,2024.
Junxiao Shen, John Dudley, and Per Ola Kristensson. Encode-store-retrieve: Enhancing memory
augmentationthroughlanguage-encodedegocentricperception. arXiv,2023.
Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari.
Charades-ego: Alarge-scaledatasetofpairedthirdandfirstpersonvideos. InarXiv,2018.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficientfoundationlanguagemodels. arXiv,2023.
OriolVinyals,MeireFortunato,andNavdeepJaitly. Pointernetworks. NeurIPS,28,2015.
YiWang,KunchangLi,XinhaoLi,JiashuoYu,YinanHe,GuoChen,BaoqiPei,RongkunZheng,
JilanXu,ZunWang,etal. Internvideo2: Scalingvideofoundationmodelsformultimodalvideo
understanding. arXiv,2024.
Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-
answeringtoexplainingtemporalactions. InCVPR,2021.
Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, and Yan Lu. Retrieval-based video language
modelforefficientlongvideoquestionanswering. arXiv,2023.
13Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang,
and Afshin Dehghan. Slowfast-llava: A strong training-free baseline for video large language
models. arXiv,2024.
FuzhaoXue,YukangChen,DachengLi,QinghaoHu,LigengZhu,XiuyuLi,YunhaoFang,Haotian
Tang,ShangYang,ZhijianLiu,etal. Longvila: Scalinglong-contextvisuallanguagemodelsfor
longvideos. arXiv,2024.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
ChengyuanLi,DayihengLiu,FeiHuang,etal. Qwen2technicalreport. arXiv,2024.
ZhouYu,DejingXu,JunYu,TingYu,ZhouZhao,YuetingZhuang,andDachengTao. Activitynet-
qa: Adatasetforunderstandingcomplexwebvideosviaquestionanswering. InAAAI,2019.
XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguage
imagepre-training. InICCV,2023.
Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language
model for video understanding. arXiv, 2023. URL https://arxiv.org/abs/2306.
02858.
Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue
Wang,HaoranTan,ChunyuanLi,andZiweiLiu. Longcontexttransferfromlanguagetovision.
arXiv,2024a.
YuanhanZhang,BoLi,haotianLiu,YongjaeLee,LiangkeGui,DiFu,JiashiFeng,ZiweiLiu,and
Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024b. URL
https://llava-vl.github.io/blog/2024-04-30-llava-next-video/.
DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. arXiv,2023.
14A MORE ANALYSIS OF MEMORY POINTER PROMPTING
TofurtherassesstheeffectivenessofMM-EgoandtheproposedMemoryPointerPromptingmech-
anism,wepresentadditionalvisualresultsofkeyframeidentificationduringtheglobalglimpsestep
inFigure9. MM-Egodemonstratesthecapabilitytoextractrelevantvisualinformationfromalarge
setofframesbasedonthegivenquestions.
Question 1: Where did I walk towards with the hose in my hands?
Choices: A: I walked backward towards a stone wall. B: I walked sideways towards a wooden shed. C: I walked around
towards a brick pathway. D: I walked forward towards an iron fence.
Prediction: D Selected
Label: D Frames
Question 2: What was the color of the tape I tried to remove from the wood?
Choices: A: The color of the tape was blue. B: The color of the tape was red. C: The color of the my was green. D: The color
of the tape was yellow.
Prediction: D Selected
Label: D Frames
Question 3: Which hand did I use to pick the frying pan from the boot of the pickup truck?
Choices: A: I picked a frying pan from the boot of the pickup truck with my right hand. B: 1. I picked a frying pan from the
boot of the pickup truck with my left hand. C: 2. I picked a fryingpan from the boot of the pickup truck with both hands. D:
3. I picked a frying pan from the boot of the pickup truck using a cloth in my left hand.
Prediction: A Selected
Label: A Frames
Question 4: What did I pass to my left hand?
Choices: A: I passed the cup to my left hand. B: I passed the plate to my left hand. C: I passed the book to my left hand. D: I
passed the remote to my left hand.
Prediction: B Selected
Label: B Frames
Question 5: What action did I take with the frying pan at the end?
Choices: A: A: I moved the frying pan on the cooker with my left hand and then stirred the content with the chopsticks. B: I
placed the frying pan in the sink and washed it using a sponge and dish soap. C: I transferred the frying pan to the dining
table and served the food onto plates. D: I hung the frying pan on the wall rack and wiped the stove clean.
Prediction: A Selected
Label: A Frames
Question 6: How did I add spice to the frying pan the first time?
Choices: A: I grabbed a handful of spice and sprinkled it over the frying pan. B: I shook the spice container directly above the
frying pan. C: I measured the spice with a teaspoon and added it to the frying pan. D: I scooped out some spice with the spoon
and poured it in the frying pan.
Prediction: D Selected
Label: D Frames
Figure9: MorekeyframeidentificationresultsoftheglobalglimpsesteponEgoMemoria. Wefind
highrelevancebetweentheidentifiedkeyframesandthequestions,demonstratingtheeffectiveness
of the proposed Memory Pointer Prompting method. The ✓ indicates that the selected frames are
relevanttothequestions.
15