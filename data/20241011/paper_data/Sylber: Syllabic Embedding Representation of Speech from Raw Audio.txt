UnderReview
SYLBER: SYLLABIC EMBEDDING REPRESENTATION
OF SPEECH FROM RAW AUDIO
CheolJunCho1,NicholasLee1,AkshatGupta1,DhruvAgarwal1,EthanChen1,
AlanWBlack2,GopalaK.Anumanchipalli1
1UniversityofCalifornia,Berkeley,2CarnegieMellonUniversity
ABSTRACT
Syllables are compositional units of spoken language that play a crucial role in
humanspeechperceptionandproduction. However,currentneuralspeechrepre-
sentationslackstructure,resultingindensetokensequencesthatarecostlytopro-
cess. Tobridgethisgap, weproposeanewmodel, Sylber, thatproducesspeech
representationswithcleanandrobustsyllabicstructure. Specifically,wepropose
aself-supervisedmodelthatregressesfeaturesonsyllabicsegmentsdistilledfrom
a teacher model which is an exponential moving average of the model in train-
ing. Thisresultsinahighlystructuredrepresentationofspeechfeatures,offering
three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) ef-
ficient syllabic tokenization with an average of 4.27 tokens per second, and 3)
syllabicunitsbettersuitedforlexicalandsyntacticunderstanding. Wealsotrain
token-to-speechgenerativemodelswithoursyllabicunitsandshowthatfullyin-
telligiblespeechcanbereconstructedfromthesetokens. Lastly,weobservethat
categorical perception, a linguistic phenomenon of speech perception, emerges
naturallyinourmodel,makingtheembeddingspacemorecategoricalandsparse
than previous self-supervised learning approaches. Together, we present a novel
self-supervisedapproachforrepresentingspeechassyllables,withsignificantpo-
tentialforefficientspeechtokenizationandspokenlanguagemodeling.
1 INTRODUCTION
Self-supervisedlearning(SSL)approacheshavebeensuccessfulinlearningspeechrepresentations
thatencoderichspeechcontentsusefulfordiversespeechdownstreamtasks(Baevskietal.,2020;
Hsu et al., 2021; Hu et al., 2024; Mohamed et al., 2022; Yang et al., 2021). In particular, speech
tokensobtainedbyquantizingSSLfeaturesarereceivingattentionforunderstandingandgenerating
spokenlanguage(Lakhotiaetal.,2021;Kharitonovetal.,2021;Hassidetal.,2024;Leeetal.,2022;
Zhangetal.,2023). SubstantialevidencesuggeststhatSSLfeaturesarehighlyphonetic(Hsuetal.,
2021; Cho et al., 2023; 2024a; Choi et al., 2024), which suggests that these quantized tokens are
sub-phonemicunitsthatdenselytilethephoneticspace(Sicherman&Adi,2023). Whilecapturing
fine-grained speech contents, most existing speech tokenization approaches yield high frequency
tokens(25-75Hz), resultinginalongsequenceoftokenstobeprocessed. Asprevailingattention
based neural networks (Vaswani, 2017) have a quadratic cost with respect to sequence length, it
becomesinfeasibletoprocesslongersequenceswithphoneme-levelgranularity.
Amajorbottleneckoftheinefficiencyinmodelingspokenlanguageisalackofstructureincurrent
neural speech representations. Unlike text, there is no clear delimiter nor orthographic symbol in
speechaudio,whicharecrucialinefficientandscalableprocessingasevidencedinthetextdomain.
However, human speech perception is structured as being segmented (Greenberg, 1998; Oganian
&Chang,2019;Gongetal.,2023)andcategorical(Libermanetal.,1957;Pisoni,1973;Pisoni&
Lazarus,1974).Wearguethatthemachinerepresentationofspeechshouldresemblethesecognitive
structurestoallowsimilarefficiencyastextprocessing.Anaturalsegmentedstructureofspeechisa
syllable,whichorganizesspeechsoundsintime(MacNeilage,1998;Greenberg,1998),andideally,
Correspondenceto:CheolJunCho<cheoljun@berkeley.edu>,GopalaK.Anumanchipalli
<gopala@berkeley.edu>
1
4202
tcO
9
]LC.sc[
1v86170.0142:viXraUnderReview
the embedding of a syllable should represent contents in a categorical way to avoid redundancy
prevailingincurrentSSL-basedtokens.
To this end, we propose self-segmentation distillation, a novel SSL framework that induces clean
androbustsyllabicstructuresinspeechrepresentations. Specifically,webuildontopofaprevious
self-supervisedsyllablelearningmodel, SDHuBERT(Choetal.,2024b), anditerativelyrefinethe
syllabic segments that naturally arise from the model. Unlike the original model, which induces
syllable structure as a byproduct of sentence-level SSL, we directly impose syllabic structures by
regressingfeaturesagainstunsupervisedsyllablesegmentsextractedfromateachermodelwhichis
a moving average of the training model. We call the resulting model Sylber (Syllabic embedding
representation).1
ThefeaturesfromSylberexhibitsalientsyllabicstructure—showingaflat,consistentoutputwithin
eachsegmentanddistinctivefromothersyllables(Figure2,right). Thisenablesafast,lineartime
algorithm for segmenting these features. Moreover, this allows more accurate boundary detection
andclusteringthatismorecoherentwithgroundtruthsyllablesthanpreviousapproaches. Syllabic
tokens quantized from Sylber features show significantly lower frequency at an average of 4.27
token/second,andcanbeusedtosynthesizefullyintelligiblespeech.2 Furthermore,unitLMsbased
onsyllabictokensoutperformthebaselineswithasimilarresourcesetting,inlearninglexiconsand
syntax.
TotestwhetherSylberiscategorical, weprobetheembeddingsofacontinuumofspeechsamples
that interpolate rhyming word pairs, inspired by linguistics (Liberman et al., 1957). We introduce
theDiscriminabilityIndex(DI)toquantifythedegreeofcategoricalperceptionofaspeechrepresen-
tationmodel. Surprisingly,weobserveatransientboundarydrawninthemiddleofthecontinuum,
showing the best DI across SSL models. This suggests that the learned features are discretized
in embedding space, contributing to the high efficiency of our syllabic tokens. To the best of our
knowledge,thisisthefirstdemonstrationofthevalidityandeffectivenessofspeechtokenizationat
thesyllablelevel,withatightconnectiontolinguistictheories.
Wesummarizeourcontributionsasfollows:
• We propose self-segmentation distillation, a novel SSL framework that imposes salient and
robustsyllabicstructureinspeechrepresentation.
• Theresultingmodel, Sylber, outperformspreviousapproachesinsyllabledetectionanddis-
coverywithasegmentationalgorithmwithO(n)timecomplexity.
• Weusethismodeltobuildasyllable-levelspeechtokenizationschemethathassignificantly
lowersamplingrateas4.27Tok/sonaverage,6-7timesimprovementoverpreviousHuBERT
tokens.
• We demonstrate that fully intelligible speech can be reconstructed from syllabic tokens, and
thattheseunitsarebettersuitedforspokenlanguageunderstanding.
• WedemonstratethatcategoricalperceptionarisesinSylber,projectingaudiotoamorecate-
goricalembeddingspacethanpreviousSSLmodels.
2 RELATED WORK
Self-supervised learning in speech Self-supervised learning (SSL) has been leveraged in speech
to learn representations from large, unlabeled speech corpuses (Hsu et al., 2021; Baevski et al.,
2020;Chenetal.,2022;Chungetal.,2021;Mohamedetal.,2022). Notably,HuBERT(Hsuetal.,
2021) and WavLM (Chen et al., 2022) are pretrained using masked prediction on audio signals in
ordertoextractrepresentationsontheaudioforeachframe. TheseSSLtechniquestypicallyextract
representationsatafixedframerateataround50Hz, whichisfairlyfinegrainedandsuggeststhat
theserepresentationsarehighlycorrelatedwithsub-phonemicstructures(Hsuetal.,2021;Choetal.,
2023;Abdullahetal.,2023;Sicherman&Adi,2023;Baevskietal.,2021).
Speech tokenization Clustering and/or quantizing these SSL representations can provide speech
tokens that are used for acoustic unit discovery (Hallap et al., 2022), speech recognition (Baevski
1Thecodeisavailablehere:https://github.com/Berkeley-Speech-Group/sylber.
2Audiosamplesareathttps://berkeley-speech-group.github.io/sylber.
2UnderReview
Frame-wise regression Target segment embedding
Stop Average
gradient
0 0 0 0 Unsupervised segmentation
Transformer
Encoder EMA Teacher
CNN Extractor
Speech audio
Figure1:Overviewofself-segmentationdistillation.Sylberistrainedwithframe-wiseregressionon
pseudosegmenttargets,obtainedbyanunsupervisedsegmentationalgorithmontheteacheroutputs.
etal.,2021;Changetal.,2024),speechsynthesis(Polyaketal.,2021;Hassidetal.,2024),language
modeling (Lakhotia et al., 2021; Borsos et al., 2022; Hassid et al., 2024; Zhang et al., 2023), and
translation (Lee et al., 2022; Li et al., 2023). However, these tokens severely suffer from high
samplingrates,whichmakesthedownstreammodelshardtoscale,andstruggletolearnlong-range
dependenciesandhigher-levellinguisticstructuresduetothelackofexplicitwordboundariesand
longer sequences. These caveats can be greatly improved by tokenizing speech at syllable-level
granularity.3
Syllabic structure in speech SSL Previous studies have demonstrated that syllabic structure can
beinducedbySSL(Pengetal.,2023;Choetal.,2024b;Komatsu&Shinozaki,2024). Pengetal.
(2023)showsthatsyllabicstructureinSSLfeaturescanbeinducedbyjointlytrainingwithimages
and spoken captions. SDHuBERT (Cho et al., 2024b) demonstrates that such syllabic induction
canbefreeofothermodalities,withasentence-levelSSL.Komatsu&Shinozaki(2024)combined
frame-wise distillation with speaker augmentation in order to derive syllabic segments. All these
methods then utilize an agglomeration algorithm on top of the learned features to infer syllable
boundaries,andextractsyllableembeddingsbyaveragingframeswithindetectedsegments. How-
ever, all of these prior studies induce syllabic structures through indirect ways, resulting in noisy
syllableboundaries. Moreover,itisunclearwhetherthediscoveredsyllablesarevalidspeechrep-
resentations or tokens. Our approach greatly improves the quality of the segments. Moreover, we
demonstratetheefficacyandvalidityofsyllabictokensthroughexperiments.
3 METHODS
3.1 SELF-SEGMENTATIONDISTILLATION
Sylber is trained by a novel SSL framework, self-segmentation distillation, that imposes more ex-
plicit inductive bias of segment structure in feature representations by directly solving the speech
segmentation problem. The diagram of our training process is depicted in Figure 1. Specifically,
we use SDHuBERT (Cho et al., 2024b) as a starting point, and leverage its unsupervised syllable
segmentsaspseudotargetsofsegmentation. Thetargetsegmentlabelsarecontinuousembeddings
averagedacrossframeswithineachsegmentthatarefoundbyanunsupervisedsegmentationalgo-
rithm. We use self-supervised knowledge distillation, where the teacher is an exponential moving
average (EMA) of the student model (Grill et al., 2020; Caron et al., 2021; He et al., 2020). The
targetsegmentandsegmentembeddingareextractedfromtheteacher,makingthelearningprocess
self-supervisedandfreeoflabels. Thelossobjectiveisaframe-wiseregressionlossthatminimizes
the Mean Squared Error (MSE) between the output features at each frame and the target from the
3InEnglish,thetypicalspeakingrateis4-5syllablespersecond.
3UnderReview
HuBERT SDHuBERT Sylber
T-UW HH-UW-M M-AY HH-AE-D R-IH T-AH-N R-OW-T B-AE-K T-UW HH-UW-M M-AY HH-AE-D R-IH T-AH-N R-OW-T B-AE-K T-UW HH-UW-M M-AY HH-AE-D R-IH T-AH-N R-OW-T B-AE-K
towhommayhad written wrote back towhommayhad written wrote back towhommayhad written wrote back
Figure2: Frame-wisesimilaritymatrixofrawfeaturesmeasuredbydotproduct. ForHuBERTand
SDHuBERT,featuresfromtheninthTransformerlayerareextracted. Aswecansee,Sylbershows
extremelysalientsyllabicstructurethatisalignedwiththegroundtruthsyllableboundaries.
correspondingsegment. Non-speechframesareregressedtozero. SeeAppendixA.1.1foraformal
definition.
OtherthanEMA,thislearningobjectiveisfreeoftechniquesthatpreventcollapse(e.g.,contrastive
learning, target recentering, masked prediction, etc). However, we find that initializing Sylber
weights with SDHuBERT can avoid collapse even though a naive regression is highly vulnerable
tocollapse. Additionally,weincludeadenoisingobjectivesimilartoChenetal.(2022)toimprove
robustnessofthemodel,where20%ofthebatchinputsforthestudentaremixedwithenvironmental
noise(Reddyetal.,2021)orotherspeechaudio. Thisadditionaldenoisingisnotaprimarysource
oflearningasasyllabicstructureisreadilyvisiblewithoutit(AppendixA.1.7).
3.2 LINEARTIMEGREEDYSEGMENTATIONALGORITHM
The result of our self-segmentation distillation induces a framewise speech representation that ex-
hibits a segmented structure as seen in the frame-wise similarity matrix (Figure 2, right). As we
cansee,ourmethodproducesacleanandrobustsegmentstructurethatwecantakeadvantageofto
designalinear-time,greedyaudiosegmentationalgorithm(alsoshowninAlgorithm1).
Thealgorithminvolvesthreelinearpassesthroughtheaudioembeddings. Thefirststepthresholds
all of the embeddings by their L2 norm. This step allows us to differentiate between speech and
non-speech segments. The second step is a monotonic agglomeration process where we sweep
througheachembeddingandaggregatethemintosegments. Adjacentframesaremergedtogether
intoasegmentaslongastheircosinesimilaritygoesaboveapredefinedmergethreshold. Thiscan
bedoneinsinglepasswithoutconstructingtheentiresimilaritymatrixbygreedilycreatinganew
segmentonceaframewithasimilaritybelowthethresholdisseen.
Thegreedysegmentationalgorithmcansometimesmakesomeerrorsbyshiftingsomeframes,soa
thirdpassisusedtorefinetheboundariesofadjacentsegments. Foreachboundary,alocalsearch
range is defined from the midpoint of the previous segment to the midpoint of the sub-sequence
segment. From here, we can compute the cosine similarity between each frame and the averages
of the two segments. For each candidate boundary in the search range, we compute an aggregate
cosinesimilarityscorebetweeneachframeandtheassignedsegmentandmaximizethissumtofind
theoptimalboundarybetweenadjacentsegments.
Each one of these steps can be implemented with O(n) complexity, so the entire segmentation
algorithmhaslinearcomplexitywithrespecttotheaudiosequencelength. AswecanseeinTable
1,thisissignificantlymoreefficientthatprevioussegmentationapproaches(Pengetal.(2023);Cho
etal.(2024b);Komatsu&Shinozaki(2024))whichallhaveO(n2)complexity.
4UnderReview
A C Mel HuBERT Sylber
1.00 1.00 1.00
down
0.96 0.88 0.88 town
0.91 0.77 0.75
1.00 1.00 1.00
zip
0.88 0.84 0.84 sip
0.75 0.67 0.68
1.00 1.00 1.00
ball
Categorical 0.95 0.82 0.88 mall
B Non-categorical 0.89 0.65 0.76
1.0 1.00 1.00 1.00
"lest" "rest" lest
0.94 0.86 0.78 rest
0.5 0.88 0.73 0.55
1.00 1.00 1.00
thin
0.95 0.85 0.73 thing
0.0 0.91 0.70 0.46
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Figure3: A.Overviewofarticulatoryinterpolationofrhymingwordswheninterpolatingα∈[0,1].
B.Hypotheticalcurvesofcategorical(solidlines)andnon-categorical(dashedlines)embeddings.
C. Similarity curves examples from Melspectrogram (Mel), HuBERT layer 9, and Sylber. Sylber
consistentlyshowshighlycategoricalperception,drawingasharpboundaryincontinuumbetween
words.
4 EVALUATING CATEGORICAL PERCEPTION IN SPEECH REPRESENTATION
Previous SSL-based tokens suffer from high redundancy in token vocabulary (Sicherman & Adi,
2023). ThisindicatesthattheSSLfeaturesdenselytilethephoneticspacewithoutaclearboundary,
resulting fine-grained, sub-phonemic units when clustered. Thus, to be better tokenized, the fea-
turesshouldhavedistinctboundariesintheirembeddingspace. Thisiswell-alignedwithcategori-
calperception, alinguistictheory, whicharguesthathumanspeechperceptiondrawsacategorical
boundaryinacontinuumofspeechsounds(Libermanetal.,1957;Pisoni&Lazarus,1974;Harnad,
2003).
Inspiredfromthislinguistictheory,wesimulateinterpolationbetweentworhymingwordstoprobe
the embeddings of speech SSL models to check whether they are categorical. Specifically, mono-
syllabicwordsarerecruitedwhereasingleconsonantisdifferentatthefrontorbackofthesyllable
(onset or coda). We make the contrast to be switching one of phonological properties: nasality,
voicedness,orplace(e.g.,“b”allvs“m”all,“d”ownvs”t”own,or“l”estvs“r”est,respectively). We
donotincludevowelcontrastssincecategoricalperceptionofvowelsisnotasconsistentasconso-
nants(Pisoni,1973;Pisoni&Lazarus,1974). Weconsider13typesofsuchdifferenceandsimulate
4 pairs for each type, resulting 52 word pairs in total. The details of the difference types and the
fulllistofwordpairscanbefoundinAppendixA.4. Tosimulateacontinuumbetweenwords,we
utilizeArticulatoryEncodec(Choetal.,2024c)whichallowsdirecteditinginthephysicalarticu-
latoryspace(Figure3-A).Wefirstgenerateaudiousingtheanoff-the-shelfTTSAPI.4 Weextract
articulatoryfeaturesfromthespeech,whicharethentemporallyalignedbydynamictimewarping
toeitherendifnecessary. Wesample51equidistantsamplesinthelinearinterpolationbetweentwo
words,whereeachendismanuallyadjustedtomaketheperceptualboundarydrawnapproximately
inthemiddle(α = 0.5),whichcanbeheardhere. Thepitchandloudnessarealsocontrolledtobe
atthesamelevel. MoredetailsaboutArticulatoryEncodeccanbefoundinSectionA.1.3.
Given a speech representation model, we extract features for each interpolating point between
words in each pair. We calculate the similarity between interpolating features with features from
either end, forming a likelihood curve along the interpolation. Hypothetically, if the represen-
tation is categorical, the likelihood curves should show a sharp transition at the boundary (Fig-
ure 3-B). If the embeddings are not categorical and tracing the interpolation, the curves would
4WeusetheTTSserviceinVertexAI(https://cloud.google.com/vertex-ai)withadefaultfemalevoice.
5
lautpecreP ytilibaborp
miS
miS
miS
miS
miSUnderReview
show “X” pattern as the dashed line in Figure 3-B. To quantify discriminability, we measure
an empirical risk of wrong discrimination. Given words at the left and right ends in interpo-
lation, x ,x ∈ W, the probability of being the left word given interpolating factor α is de-
L R
fined as p(x |x ) = sim(xL,xα)−offsetL . The probability of being the right
L α sim(xL,xα)−offsetL+sim(xR,xα)−offsetR
word is symmetrically defined. We need to subtract an offset due to the high base similarity,
as the words are rhyming pairs, such that offset = min sim(x ,x ). Then the empiri-
A α∈[0,1] A α
cal risk can be defined as L (q|x ,x ) := E 1 p(x |x ) + 1 p(x |x ), for the
Disc L R α∈[0,1] α<q R α α≥q L α
decision boundary at q ∈ [0,1]. The optimal boundary can be drawn by minimizing the risk,
α∗ = argmin L (q|x ,x ). Discriminability index (DI) is then defined as the risk at the
q∈[0,1] Disc L R
optimalboundary,averagedoverwordpairs:
1 (cid:88)
DI:= L (α∗|x ,x ) (1)
|W| Disc L R
xL,xR∈W
Iftheembeddingsarecategorical,DIwillbecloseto0. Iftheyarenon-categoricalwithX-shaped
curves,DIwillbe0.25. ThemaximumvalueofDIis0.5,whichwouldberandomchancediscrimi-
nation. Thiswillbediscussedlaterin§6.4indetail.
5 EXPERIMENTAL SETUP AND EVALUATION PROTOCOL
5.1 EXPERIMENTALSETUP
ArchitectureSylberhasthesamearchitectureasHuBERTwithaCNNfeatureextractorfollowed
byTransformerencoder. BasedontheobservationthattheninthlayerofSDHuBERTbestencodes
syllables(Choetal.(2024b)),weusea9layertransformerandinitializeweightswithSDHuBERT
up-tothatlayer.5 SeeAppendixA.1.4fortrainingdetails.
Tokenization To tokenize speech, we apply the aforementioned segmentation algorithm (Section
3.2) to get unsupervised speech segments. The features within segments are averaged to form
continuous speech tokens at a syllable granularity (4-5 syllables per second). We apply a simple
k-means clustering on the features with several different vocab sizes (5K, 10K, and 20K). These
clustersizesarelargerthanwhatisusedbyotherSSL-basedclusteringtechniques(usuallyaround
50-500clusters),whichisnecessarysinceourfeaturesaremoreclosertosyllablesthanphonemes;
similartohowvocabularysizesforBPEbasedtokenizersaresignificantlylargerthanthenumberof
characters. However,thesesyllabictokenshaveasignificantlylowertemporalresolutioncompared
topreviousSSL-basedtokens,whichleadstoimprovementstoefficiency(seeSection6.2).
Token-to-speech If our syllabic tokens are valid speech tokens, we should be able to reconstruct
intelligiblespeechfromthem. WetrainaConditionalFlow-matching(CFM)(Lipmanetal.,2022;
Leetal.,2024)modeltogeneratelow-levelspeechfeaturesthatcanbeconvertedtospeechaudio.
WeutilizeArticulatoryEncodec(Choetal.,2024c),anencoding-decodingframeworkthatencodes
speechintoarticulatoryfeaturesandspeakeridentity,anddecodesthembacktospeechwaveform.
The articulatory features are composed of spatiotemporal displacements of vocal tract articulators
and source features that represent loudness and pitch. Cho et al. (2024c) empirically prove that
these articulatory features are speaker agnostic provided that pitch is normalized, while allowing
full-reconstructiontospeech. Wenormalizepitchbydividingbyspeaker’smeanpitchandlogscal-
ing, following (Kharitonov et al., 2021). Since SSL-based speech tokens generally lack speaker
information(Polyaketal.,2021;Wangetal.,2023), weaimtoreconstructthesespeaker-agnostic
articulatory features from the syllabic tokens. The token embeddings are restored by the k-means
codebooks, and expanded to the durations of the original segments. The non-speech frames are
filled with zeros. For the case without quantization, the segment-averaged features are used. At
inferencetime,weinputthegeneratedarticulatoryfeatureswithspeakerembeddingsextractedfrom
originalspeechtothedecoderofArticulatoryEncodectogenerateaudio. SeeAppendixA.1.3for
theimplementationandtrainingdetails.
5Thecheckpointisretrievedfromhttps://github.com/cheoljun95/sdhubert.
6UnderReview
UnitLMFollowingLakhotiaetal.(2021),wetrainanautoregressiveunitlanguagemodel(uLM)
using the syllabic tokens. The model has the same architecture as GSLM (Lakhotia et al., 2021),
whichisadecoder-onlyTransformerwith12layers.
Datasets LibriSpeech (Panayotov et al., 2015) is used for pretraining Sylber, k-means clustering,
andfortrainingtheuLMs. LibriTTS-R(Koizumietal.,2023)isusedfortrainingtheCFMmodels.
FortrainingArticulatoryEncodec,weuseanextendeddatasetthatincludesLibriTTS-R,LibriTTS
(Zenetal.,2019),andEXPRESSO(Armougometal.,2006).
5.2 EVALUATION
Syllable detection and discovery We evaluate syllable boundaries with precision, recall, F1, and
R-valuewitha50mstolerance,following(Ra¨sa¨nenetal.,2009;Pengetal.,2023;Choetal.,2024b;
Komatsu & Shinozaki, 2024). Syllable discovery is evaluated by a separate clustering, where we
usethesameprocessasthepreviousworksthatuse4096clusters.Then,wemeasuresyllablepurity,
clusterpurity, andmutual informationbetween discoveredsyllablesand groundtruths (Choet al.,
2024b;Komatsu&Shinozaki,2024). ThesameLibriSpeechdev/testsetsareusedaspriorworks.
Speech resynthesis We use the extracted speaker embedding and mean pitch from the original
speakertosynthesizespeechfromarticulatoryfeaturespredictedfromtokens. Weusetheoriginal
segment durations for each token without predicting them since the duration information can be
easily tokenized. (For example, duration can be tagged for each token. See Appendix A.2.) We
remove randomness in CFM to yield consistent generation for evaluation purposes. We measure
reconstructionperformanceusingtheaveragePearsonCorrelationofeachcomponentinarticulatory
features. To evaluate intelligibility, we use an off-the-shelf speech recognition model, Whisper
(Radfordetal.,2023)6,andmeasureworderrorrate(WER)andcharactererrorrate(CER).Lastly,
we apply an automated speech quality measurement, UTMOS (Saeki et al., 2022), to evaluate the
qualityofgeneratedspeech. Theseareevaluatedonthetest-cleansplitofLibriTTS-R.
Coding efficiency We evaluate the coding efficiency of these tokens with Token/second (Tok/s),
bitrate,andcoding-rate. Thebitrateiscalculatedby(log (vocabsize))×Tok/s. Wedefinecoding-
2
rate as how much word information is preserved per bit: (1−WER/100)×total#ofwords. Likewise, the
total#ofbits
test-cleansplitofLibriTTS-Risused.
SpokenLanguageUnderstanding(SLU)Toevaluatethelanguageunderstanding,weusethezero-
shotmetricsoflexicallearning,sWUGGY,andsyntaxlearning,sBLIMP,followingLakhotiaetal.
(2021);Algayresetal.(2023).ThesemetricsareoriginallyfromtheZerospeechChallenge(Nguyen
etal.,2020),fordiscriminatingrealwords/phrasesandfakeones. Thetasksaresolvedzero-shotby
choosingwords/phraseswithhigherprobabilityinferredbyanLMandaccuracyisreported.
CategoricalperceptionForthemodelswithframe-wisefeatures,weusedynamictimewarpingto
findanalignmentthatmaximizessimilarity. Whilesomesamplepairsarealreadyaligned,wefind
thatadditionalwarpingyieldsbetterscores. Formodelswithsyllabicfeatures,weaverageacrossall
speech(ornormthresholded)partsofthefeaturesasallsamplesaremonosyllabic,yieldingasingle
embedding per sample. We use cosine similarity to measure similarity between embeddings from
samples,andevaluateDIasdefinedin§4.
5.2.1 BASELINES
Forsyllabledetectionanddiscovery,wecompareourmodelsagainstHuBERT,VGHuBERT,SDHu-
BERT,andKomatsu&Shinozaki(2024). Fortoken-to-speech,wetrainthebaselineCFMmodels
using HuBERT units with 50, 100, and 200 cluster sizes as used in Lakhotia et al. (2021), and
SDHuBERT tokens with 5K, 10K, and 20K cluster sizes. For coding efficiency, we apply Byte
Pair Encoding (BPE) using SentencePiece7 to merge frequent units to form larger vocabulary that
matches ours: 5K, 10K, and 20K, similar to Shen et al. (2024).8 For evaluating language under-
standing,weuseGSLM(Lakhotiaetal.,2021)andtGSLM(Algayresetal.,2023)asbaselines. We
6Weuse“openai/whisper-large-v3”fromHuggingface.
7https://github.com/google/sentencepiece
8ThecodingefficiencymetricsaresubstantiallyworseusingHuBERTwithoutBPEduetotheirsampling
granularity;thus,wecompareagainstHuBERTwithBPEtomakeamorefaircomparison.
7UnderReview
Table1: Syllabledetectionanddiscoveryresults. Pr: precision,Re: recall,R:R-value,SP:syllabic
purity, CP: cluster purity, and MI: mutual information. Complexity indicates time complexity of
post-hoc segmentation algorithm. n: the number of frames and k: the number of syllables. As
we can see, only Sylber uses a linear time algorithm while the other models use a quadratic time
algorithm.
SyllableDetection SyllableDiscovery
Model Complexity
Pr↑ Re↑ F1↑ R↑ SP↑ CP↑ MI↑
HuBERT O(kn2) 51.4 31.4 39.0 50.1 33.1 28.4 3.54
VGHuBERT O(kn2) 65.3 64.3 64.8 70.0 53.4 43.6 4.66
SDHuBERT O(n2/k) 64.3 71.0 67.5 70.7 54.1 46.2 4.76
Komatsu&Shinozaki(2024) O(kn2) 73.3 67.6 70.3 74.6 59.4 44.5 5.08
Sylber O(n) 76.6 68.3 72.2 75.9 64.0 43.9 5.28
Table2:Resynthesisresults. HB:HuBERT,SDHB:SDHuBERT,andKM:KMeanclustersize. Re-
constructionmetricsareaveragePearsonCorrelationandWERandCERarereportedinpercentage
(%). 95%confidenceintervalisreportedforreconstructionandquality. Bestscoresarehighlighted
withboldfontandbestscoreswithquantizationareunderlined.
Model Reconstruction Intelligibility Quality Frequency
Upstream KM Art↑ Loudness↑ Pitch↑ WER↓ CER↓ UTMOS↑ Tok/s↓
50 0.926±0.065 0.880±0.089 0.586±0.581 13.32 7.24 4.190±0.553 23.59
HB 100 0.941±0.046 0.878±0.098 0.594±0.560 7.78 3.89 4.177±0.548 26.68
200 0.944±0.044 0.886±0.090 0.608±0.573 6.34 3.10 4.197±0.543 28.97
5K 0.925±0.066 0.872±0.089 0.757±0.384 9.88 5.40 4.140±0.660
10K 0.927±0.064 0.879±0.083 0.759±0.412 9.25 4.99 4.173±0.600
SDHB 5.24
20K 0.930±0.061 0.883±0.081 0.784±0.373 8.63 4.62 4.180±0.609
∞ 0.959±0.035 0.948±0.042 0.906±0.217 4.94 2.56 4.190±0.552
5K 0.919±0.072 0.877±0.091 0.739±0.431 8.70 4.48 4.189±0.607
10K 0.922±0.064 0.876±0.088 0.753±0.424 8.07 4.28 4.155±0.624
Sylber 4.27
20K 0.924±0.066 0.882±0.084 0.774±0.374 7.95 4.06 4.210±0.547
∞ 0.957±0.037 0.950±0.045 0.918±0.216 4.88 2.42 4.199±0.539
excludeotheruLMsthatusemoreextensiveresourcesthanours(e.g.,includingalargerdatasetor
usingapretrainedtext-basedLM.)However,weincludetGSLMsinceithasasimilartokengranu-
larity(5Tok/s)asoursalthoughitistrainedona6×largerdataset. Forphoneticdiscriminability,
wecompareSylberwithtraditionalacousticfeatures(MelspectrogramandMFCC),representative
frame-wiseSSLmodels(HuBERT,Wav2Vec2,andWavLM),andSDHuBERT.Forframe-wiseSSL
models,thebestlayerswiththelowestDIsarechosen.
6 RESULTS
6.1 SYLLABLEDETECTIONANDDISCOVERY
Table1showsacomparisonofsyllabledetectionanddiscoveryperformance. Sylberoutperforms
allpreviousapproachesineverymetricotherthanrecallandclusterpurity. Asthesetwotermscan
beinflatedbyhavingmoresegments, itindicatesthatSDHuBERTisoversegmenting. Intermsof
discovery, we find the ground truth syllables are more purely mapped to ours than the baselines,
greatlyimprovingthepreviousSOTAbyhugemargin(59.4→64.0). Theresultsindicatethatour
modelcandetectanddiscoversyllablesbetterthanthepreviousapproaches. Moreover,theoutput
featuresfromourmodelaresignificantlycleanerthanHuBERTorSDHuBERTasshowninFigure
2,showinghighlyconsistentsimilaritieswithinsyllablespans. ThisallowsforamuchfasterO(n)
algorithm applicable, compared to the previous O(kn2) and O(n2/k) algorithms where n is the
numberofframesandkistheestimatenumberofsyllablescontrolledbyahyperparameter.
8UnderReview
Table3: Codingefficiencycomparison.
Token/second↓ Bitrate↓ Coding-rate↑
Model Vocabsize Vocabsize Vocabsize
5K 10K 20K 5K 10K 20K 5K 10K 20K
HB50-BPE 7.45 6.82 6.30 91.57 90.68 90.00 0.0283 0.0285 0.0287
HB100-BPE 14.78 14.40 14.10 181.56 191.37 201.46 0.0152 0.0144 0.0137
HB200-BPE 16.67 15.99 15.53 204.79 212.41 221.84 0.0136 0.0132 0.0126
SDHB 5.24 64.39 69.63 74.87 0.0253 0.0243 0.0234
Sylber 4.27 52.43 56.70 60.97 0.0315 0.0302 0.0289
6.2 RESYNTHESISPERFORMANCEANDCODINGEFFICIENCY
The results of token-to-speech resynthesis are shown in Table 2 and can be heard here. We find a
generaltrendinbothSDHuBERTandoursyllabictokensthatarticulatoryreconstructionandintel-
ligibility increase with finer clustering granularity. For intelligibility, our model outperforms SD-
HuBERTateveryvocabsizewhilerequiringlesstokenspersecond. Interestingly,thearticulatory
reconstructionisgenerallyhigherinSDHuBERT,butalsolessintelligible. Thisindicatesthatour
model marginalizes out some amount of articulatory variance which is orthogonal to orthographic
contents. Thismarginalizationisalsohappeninginintonationwhentheembeddingsarequantized,
asshowninthehugereductioninpitchcorrelationcomparedtonon-quantizedmodel,resultingina
flattenedspeechgeneration. ThispatternissharedinbothSDHuBERTandourmodel.
Compared to HuBERT units which have token granularity at sub-phonemic level, the articulation
isbetterreconstructedbyHuBERTunitswith200clustersthantheunitsfromSDHuBERTorour
model.Thisisnaturalgiventheirtemporalgranularityas28.97tokenspersecond,whichislikelyto
capturethelocaldynamicsofarticulationbetterthansyllabiclevel. Theintelligibilityisalsobetter
inthecaseof100and200clustersizes,WERsof7.78and6.34,comparedtothebestcaseofsyllabic
units,WERof7.95. Thoughthedifferenceismarginal,HuBERTunitsrequireatleast6timesmore
tokenspersecond.Also,wefindtheHuBERTisworseinrepresentingpitch,assuggestedbyPolyak
et al. (2021); Kharitonov et al. (2021). While no significant difference is perceived in quality, our
modelwith20KvocabsizequantizationshowsthebestaverageUTMOSof4.21.
Tobettercharacterizecodingefficiency,wecomparebandwidthandcoding-rateinTable3against
baselines with comparable settings. Our model outperforms each baseline in every metric, show-
ing about a 20% gain over the SDHuBERT tokens. In addition, Table 3 demonstrates the innate
inefficiencyinpreviousapproachesusingHuBERTunits. Thereisaminimalgaininsequencecom-
pressionwhileincreasingthevocabularysize, whereBPEisnotabletoreduceTok/sbyevenhalf
oftheoriginalwhenappliedto100and200clusters. TheonlycomparablebaselineisBPEon50
HuBERTclusters,whichcanreduceTok/sfrom23.59tobetween6.30and7.45. However,thereis
a huge information loss as shown in the high WER of 13.32, which results in a lower coding-rate
(0.0283, 0.0285, 0.0287) compared to ours (0.0315, 0.0302, 0.0289) for vocab size of (5K, 10K,
20K) respectively. Without quantization, the best performance is achieved by our model, with a
WERof4.88,Tok/sof4.27,andhighercorrelationsinloudnessandpitch(Table2). Thisindicates
thesignificantpotentialofsyllabictokensasanefficientspeechcodingthatcanbeharnessedbya
betterquantizationmethodlikevectorquantization(VQ)orresidualVQ(RVQ),whichweleavefor
futurework.
6.3 SPOKENLANGUAGEUNDERSTANDING(SLU)
Table4comparesthesWUGGYandsBLIMPscoresofunitLMswithdifferentvocabsizeofsyl-
labicunits(markedas“model-vocabsize”), andthebaselinemodels. Sylberwith20Kvocabsize
outperformsGSLMandtGSLMinsWUGGY,andoutperformsbaselinesatsBLIMP,whilenoneof
the SDHuBERT-based LMs outperform GSLM or tGSLM. This indicates that our syllabic tokens
havebetterutilityintermsoflanguagemodelingcomparedtothesyllabictokensfromSDHuBERT.
We also observe a general trend shared in SDHuBERT and ours that a larger vocab size yields a
higher sWUGGY score, indicating a finer clustering better covers the lexical space. However, no
such pattern was found in sBLIMP score. Notably, Sylber outperforms tGSLM which has a simi-
lar token granularity as 5 Hz but with a fixed pooling window, even though their model is trained
9UnderReview
Table4: SLUResults
Model sWUGGY↑ sBLIMP↑
GSLM 68.70 57.06
tGSLM 68.53 55.31
SDHB-5K 65.80 54.87
SDHB-10K 67.42 54.48
SDHB-20K 67.85 54.87
Sylber-5K 67.32 57.34
Sylber-10K 68.41 58.04
Sylber-20K 70.27 57.67
Table5: Phoneticdiscriminabilityresults. DI:DiscriminabilityIndex. Scoresfordifferentcontrast
postitions(onset,codaandall)areseparatelyreported.
DI↓
Model
Onset Coda All
Mel 0.198 0.193 0.196
Acoustic
MFCC 0.191 0.182 0.188
Wav2Vec2 0.172 0.178 0.174
HuBERT 0.136 0.152 0.141
Frame-wise Wav2Vec2Large 0.138 0.156 0.143
HuBERTLarge 0.166 0.180 0.170
WavLMLarge 0.136 0.148 0.140
SDHuBERT 0.133 0.126 0.131
Syllable-wise
Sylber 0.116 0.103 0.112
on a larger dataset. This suggests that a variable pooling window which is dynamically driven by
segmentationalgorithmisbetterthanusingafixedpoolingwindow.
6.4 DEMONSTRATIONOFEMERGENTCATEGORICALPERCEPTION
Figure3-Cillustratesthataclearboundaryisdrawnwheninterpolatingbetweentworhymingwords,
whereassuchaboundaryislessprominentinHuBERT.ThisindicatesthatSylberneedsonly2cate-
goriestorepresenttheinterpolatingcontinuum,whileHuBERTrequiresmultiplecategoriesorunits,
whichinduceshighlevelofinefficiencyandredundancyinclustering(Sicherman&Adi,2023).The
curves extracted from the melspectrogram resemble an X-shape, indicating a non-categorical em-
bedding space. This is the closest value to the hypothetical non-categorical score of 0.25 among
themodels(Table5). AsshowninTable5,ourmodel’sembeddingsdemonstratethebestdiscrim-
inability, with the lowest DIs across both onset and coda contrasts, resulting in an overall DI of
0.112. SDHuBERTalsoshowslowerDIsthanframe-wisemodels,thoughitstillfallsshortofour
model’s performance. The results are highly surprising since we only impose the model to learn
temporalstructure,andourlossobjectivedoesnotinvolvecategoricallearningatall. However,the
embeddingspaceofSylberisnaturallystructuredtobecategorical,indicatingtheself-segmentation
distillationmightbeanaturallearningalgorithmthatresembleshumanlanguagelearning. Amongst
theframe-wisemodels,WavLM-Largeexhibitsthebestdiscriminability,whichalignswiththefact
thatWavLMgenerallyhassuperiorrepresentationalpowercomparedtootherSSLmodels. Taken
together, these qualitative and quantitative results suggest that the embedding space of Sylber is
readilyquantized,contributingtotheperformanceimprovementsobservedinprevioussections.
7 CONCLUSION
We propose a novel self-supervised learning framework of speech that learns to transform speech
waveform to syllabic embedding. As results, we achieve a state-of-the-art unsupervised syllable
detectionanddiscoveryperformancewithasignificantreductionintimecomplexityofsegmentation
algorithm,downtoO(n). Theresynthesisexperimentsvalidatethatourmodelcanprovidesyllabic
tokens that encodes intelligible speech content in highly efficient way. Up to our knowledge this
isthefirstdemonstrationofspeechsynthesisfromsyllable-leveltokens. Furthermore,alinguistic-
theoryinspiredanalysisrevealsthatSylberalsosegmentstheembeddingspace,showingemergent
10UnderReview
categorical perception of speech. To sum, we introduce a new method for representing speech
featuresassyllables,offeringpromisingpotentialformoreefficientspeechtokenizationandspoken
languagemodeling.
LimitationsOurmodelisnotyetsuitableforuniversalspeechrepresentation,whichthemostspeech
SSLapproachesaimfor(Yangetal.,2021). WefindthatSylberdegradesinsomeSUPERBdown-
stream tasks, which we believe, is due to the parsimonious structure we are imposing (Appendix
A.3). Furthermore, our model has not been scaled to a larger speech corpus, which may provide
moregainsthanourcurrentsetting,asshownbypreviousstudies(Borsosetal.,2022;Hassidetal.,
2024). Weleavetheseforfuturedirectionstoexplore.
ACKNOWLEDGMENTS
This research is supported by the following grants to PI Anumanchipalli — NSF award 2106928,
BAIRCommons-MetaAIResearch,theRoseHillsInnovatorProgram,UCNoyceInitiativeatUC
Berkeley,andGoogleResearchScholarAward. SpecialthankstoShang-Wen(Daniel)LiandAb-
delrahmanMohamedforvaluablediscussionsandadvice.
ETHICS STATEMENT
WebelievethatSylberisasubstantialstepforwardforspeechmodelsandspokenlanguageunder-
standing. Ourtechniqueenablesefficientandeffectivespeechtokenizationwhichcanpotentiallybe
usedformaliciouspurposes. Itisimportantforusers,researchers,anddeveloperstousethismodel
andthisframeworkethicallyandresponsibly.
REPRODUCIBILITY STATEMENT
In the spirit of open research, we will be releasing all of the code associated with Sylber. We
will release the pretrained model weights as well as the code necessary to retrain the model. In
addition,wewillbereleasingalloftheinterpolationsamplessothatotherresearcherscanalsouse
ourDiscriminabilityIndexasanevaluationmetricforfutureresearch.
REFERENCES
Badr M Abdullah, Mohammed Maqsood Shaik, Bernd Mo¨bius, and Dietrich Klakow. An
information-theoretic analysis of self-supervised discrete representations of speech. arXiv
preprintarXiv:2306.02405,2023.
Robin Algayres, Yossi Adi, Tu Anh Nguyen, Jade Copet, Gabriel Synnaeve, Benoit Sagot, and
Emmanuel Dupoux. Generative spoken language model based on continuous word-sized audio
tokens. arXivpreprintarXiv:2310.05224,2023.
Fabrice Armougom, Sebastien Moretti, Olivier Poirot, Stephane Audic, Pierre Dumas, Basile
Schaeli, Vladimir Keduas, and Cedric Notredame. Expresso: automatic incorporation of struc-
tural information in multiple sequence alignments using 3d-coffee. Nucleic acids research, 34
(suppl 2):W604–W608,2006.
AlexeiBaevski, YuhaoZhou, AbdelrahmanMohamed, andMichaelAuli. wav2vec2.0: Aframe-
work for self-supervised learning of speech representations. Advances in neural information
processingsystems,33:12449–12460,2020.
AlexeiBaevski,Wei-NingHsu,AlexisConneau,andMichaelAuli. Unsupervisedspeechrecogni-
tion. AdvancesinNeuralInformationProcessingSystems,34:27826–27839,2021.
Zala´nBorsos,Raphae¨lMarinier,DamienVincent,EugeneKharitonov,OlivierPietquin,MattShar-
ifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a lan-
guagemodelingapproachtoaudiogeneration.(2022). arXivpreprintarXiv:2209.03143,2022.
11UnderReview
Mathilde Caron, Hugo Touvron, Ishan Misra, Herve´ Je´gou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of
theIEEE/CVFinternationalconferenceoncomputervision,pp.9650–9660,2021.
Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan
Sharma,JiatongShi,JinchuanTian,ShinjiWatanabe,etal. Exploringspeechrecognition,trans-
lation, and understanding with discrete speech units: A comparative study. In ICASSP 2024-
2024IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.
11481–11485.IEEE,2024.
SanyuanChen,ChengyiWang,ZhengyangChen,YuWu,ShujieLiu,ZhuoChen,JinyuLi,Naoyuki
Kanda,TakuyaYoshioka,XiongXiao,etal. Wavlm: Large-scaleself-supervisedpre-trainingfor
fullstackspeechprocessing. IEEEJournalofSelectedTopicsinSignalProcessing,16(6):1505–
1518,2022.
Cheol Jun Cho, Peter Wu, Abdelrahman Mohamed, and Gopala K Anumanchipalli. Evidence of
vocal tract articulation in self-supervised learning of speech. In ICASSP 2023-2023 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1–5. IEEE,
2023.
Cheol Jun Cho, Abdelrahman Mohamed, Alan W Black, and Gopala K Anumanchipalli. Self-
supervisedmodelsofspeechinferuniversalarticulatorykinematics. InICASSP2024-2024IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 12061–
12065.IEEE,2024a.
Cheol Jun Cho, Abdelrahman Mohamed, Shang-Wen Li, Alan W Black, and Gopala K Anu-
manchipalli.Sd-hubert:Sentence-levelself-distillationinducessyllabicorganizationinhubert.In
ICASSP2024-2024IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing
(ICASSP),pp.12076–12080.IEEE,2024b.
CheolJunCho,PeterWu,TejasSPrabhune,DhruvAgarwal,andGopalaKAnumanchipalli. Artic-
ulatoryencodec: Vocaltractkinematicsasacodecforspeech. arXivpreprintarXiv:2406.12998,
2024c.
KwangheeChoi,AnkitaPasad,TomohikoNakamura,SatoruFukayama,KarenLivescu,andShinji
Watanabe. Self-supervised speech representations are more phonetic than semantic. arXiv
preprintarXiv:2406.08619,2024.
Yu-AnChung, YuZhang, WeiHan, Chung-ChengChiu, JamesQin, RuomingPang, andYonghui
Wu. W2v-bert: Combining contrastive learning and masked language modeling for self-
supervisedspeechpre-training. In2021IEEEAutomaticSpeechRecognitionandUnderstanding
Workshop(ASRU),pp.244–250.IEEE,2021.
Xue L Gong, Alexander G Huth, Fatma Deniz, Keith Johnson, Jack L Gallant, and Fre´de´ric E
Theunissen. Phonemicsegmentationofnarrativespeechinhumancerebralcortex. Naturecom-
munications,14(1):4309,2023.
Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint
arXiv:1211.3711,2012.
StevenGreenberg. Asyllable-centricframeworkfortheevolutionofspokenlanguage. Behavioral
andbrainsciences,21(4):518–518,1998.
Jean-Bastien Grill, Florian Strub, Florent Altche´, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, CarlDoersch,BernardoAvilaPires,ZhaohanGuo,MohammadGheshlaghiAzar,
etal. Bootstrapyourownlatent-anewapproachtoself-supervisedlearning. Advancesinneural
informationprocessingsystems,33:21271–21284,2020.
MarkHallap,EmmanuelDupoux,andEwanDunbar.Evaluatingcontext-invarianceinunsupervised
speechrepresentations. arXivpreprintarXiv:2210.15775,2022.
StevanHarnad. Categoricalperception. 2003.
12UnderReview
Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet,
Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. Textually pretrained speech
languagemodels. AdvancesinNeuralInformationProcessingSystems,36,2024.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pp.9729–9738,2020.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,
andAbdelrahmanMohamed. Hubert: Self-supervisedspeechrepresentationlearningbymasked
predictionofhiddenunits. IEEE/ACMtransactionsonaudio,speech,andlanguageprocessing,
29:3451–3460,2021.
Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu
Li, Sunit Sivasankaran, Linquan Liu, et al. Wavllm: Towards robust and adaptive speech large
languagemodel. arXivpreprintarXiv:2404.00656,2024.
Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu-Anh
Nguyen,MorganeRivie`re,AbdelrahmanMohamed,EmmanuelDupoux,etal.Text-freeprosody-
awaregenerativespokenlanguagemodeling. arXivpreprintarXiv:2109.03264,2021.
YumaKoizumi,HeigaZen,ShigekiKarita,YifanDing,KoheiYatabe,NobuyukiMorioka,Michiel
Bacchiani, Yu Zhang, Wei Han, and Ankur Bapna. Libritts-r: A restored multi-speaker text-to-
speechcorpus. arXivpreprintarXiv:2305.18802,2023.
Ryota Komatsu and Takahiro Shinozaki. Self-supervised syllable discovery based on speaker-
disentangledhubert. arXivpreprintarXiv:2409.10103,2024.
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for
efficientandhighfidelityspeechsynthesis. Advancesinneuralinformationprocessingsystems,
33:17022–17033,2020.
Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte,
Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative
spoken language modeling from raw audio. Transactions of the Association for Computational
Linguistics,9:1336–1354,2021.
MatthewLe,ApoorvVyas,BowenShi,BrianKarrer,LedaSari,RashelMoritz,MaryWilliamson,
VimalManohar,YossiAdi,JayMahadeokar,etal. Voicebox: Text-guidedmultilingualuniversal
speechgenerationatscale. Advancesinneuralinformationprocessingsystems,36,2024.
Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak,
YossiAdi,QingHe,YunTang,etal. Directspeech-to-speechtranslationwithdiscreteunits. In
Proceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume
1: LongPapers),pp.3327–3339,2022.
Xinjian Li, Ye Jia, and Chung-Cheng Chiu. Textless direct speech-to-speech translation with dis-
cretespeechrepresentation. InICASSP2023-2023IEEEInternationalConferenceonAcoustics,
SpeechandSignalProcessing(ICASSP),pp.1–5.IEEE,2023.
Alvin M Liberman, Katherine Safford Harris, Howard S Hoffman, and Belver C Griffith. The
discriminationofspeechsoundswithinandacrossphonemeboundaries. Journalofexperimental
psychology,54(5):358,1957.
YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMattLe. Flowmatching
forgenerativemodeling. arXivpreprintarXiv:2210.02747,2022.
PeterFMacNeilage. Theframe/contenttheoryofevolutionofspeechproduction. Behavioraland
brainsciences,21(4):499–511,1998.
Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob D Havtorn, Joakim Edin, Christian
Igel,KatrinKirchhoff,Shang-WenLi,KarenLivescu,LarsMaaløe,etal. Self-supervisedspeech
representationlearning: Areview. IEEEJournalofSelectedTopicsinSignalProcessing,16(6):
1179–1210,2022.
13UnderReview
TuAnhNguyen,MaureendeSeyssel,PatriciaRoze´,MorganeRivie`re,EvgenyKharitonov,Alexei
Baevski,EwanDunbar,andEmmanuelDupoux.Thezeroresourcespeechbenchmark2021:Met-
ricsandbaselinesforunsupervisedspokenlanguagemodeling.arXivpreprintarXiv:2011.11588,
2020.
YuliaOganianandEdwardFChang. Aspeechenvelopelandmarkforsyllableencodinginhuman
superiortemporalgyrus. Scienceadvances,5(11):eaay6279,2019.
VassilPanayotov,GuoguoChen,DanielPovey,andSanjeevKhudanpur. Librispeech: anasrcorpus
basedonpublicdomainaudiobooks.In2015IEEEinternationalconferenceonacoustics,speech
andsignalprocessing(ICASSP),pp.5206–5210.IEEE,2015.
PuyuanPeng,Shang-WenLi,OkkoRa¨sa¨nen,AbdelrahmanMohamed,andDavidHarwath.Syllable
discoveryandcross-lingualgeneralizationinavisuallygrounded,self-supervisedspeechmodel.
arXivpreprintarXiv:2305.11435,2023.
David B Pisoni. Auditory and phonetic memory codes in the discrimination of consonants and
vowels. Perception&psychophysics,13:253–260,1973.
DavidBPisoniandJoanHouseLazarus.Categoricalandnoncategoricalmodesofspeechperception
alongthevoicingcontinuum. TheJournaloftheAcousticalSocietyofAmerica,55(2):328–333,
1974.
Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-
delrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled
self-supervisedrepresentations. arXivpreprintarXiv:2104.00355,2021.
AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,andIlyaSutskever.
Robustspeechrecognitionvialarge-scaleweaksupervision. InInternationalconferenceonma-
chinelearning,pp.28492–28518.PMLR,2023.
Okko Johannes Ra¨sa¨nen, Unto Kalervo Laine, and Toomas Altosaar. An improved speech seg-
mentationqualitymeasure: ther-value. InTenthAnnualConferenceoftheInternationalSpeech
CommunicationAssociation.Citeseer,2009.
Chandan KA Reddy, Harishchandra Dubey, Vishak Gopal, Ross Cutler, Sebastian Braun, Hannes
Gamper,RobertAichner,andSriramSrinivasan.Icassp2021deepnoisesuppressionchallenge.In
ICASSP2021-2021IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing
(ICASSP),pp.6623–6627.IEEE,2021.
Takaaki Saeki, Detai Xin, Wataru Nakata, Tomoki Koriyama, Shinnosuke Takamichi, and Hi-
roshi Saruwatari. Utmos: Utokyo-sarulab system for voicemos challenge 2022. arXiv preprint
arXiv:2204.02152,2022.
FeiyuShen,YiweiGuo,ChenpengDu,XieChen,andKaiYu. Acousticbpeforspeechgeneration
withdiscretetokens. InICASSP2024-2024IEEEInternationalConferenceonAcoustics,Speech
andSignalProcessing(ICASSP),pp.11746–11750.IEEE,2024.
Amitay Sicherman and Yossi Adi. Analysing discrete self supervised speech representation for
spokenlanguagemodeling. InICASSP2023-2023IEEEInternationalConferenceonAcoustics,
SpeechandSignalProcessing(ICASSP),pp.1–5.IEEE,2023.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-
hancedtransformerwithrotarypositionembedding. Neurocomputing,568:127063,2024.
AVaswani. Attentionisallyouneed. AdvancesinNeuralInformationProcessingSystems,2017.
ChengyiWang,SanyuanChen,YuWu,ZiqiangZhang,LongZhou,ShujieLiu,ZhuoChen,Yanqing
Liu,HuamingWang,JinyuLi,etal. Neuralcodeclanguagemodelsarezero-shottexttospeech
synthesizers. arXivpreprintarXiv:2301.02111,2023.
Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin,
Andy T Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, et al. Superb: Speech processing
universalperformancebenchmark. arXivpreprintarXiv:2105.01051,2021.
14UnderReview
HeigaZen,VietDang,RobClark,YuZhang,RonJWeiss,YeJia,ZhifengChen,andYonghuiWu.
Libritts: Acorpusderivedfromlibrispeechfortext-to-speech. arXivpreprintarXiv:1904.02882,
2019.
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.
Speechgpt: Empowering large language models with intrinsic cross-modal conversational abil-
ities. arXivpreprintarXiv:2305.11000,2023.
A APPENDIX
A.1 IMPLEMENTATIONDETAILS
A.1.1 SELF-SEGMENTATIONDISTILLATION
Givenaspeechaudio,x,weextractfeatures,M (x) = zS andM (x) = zT,whereM andM
S T S T
arethestudentandteachermodels, respectively. Theunsupervisedsegmentationalgorithm, Useg,
outputs segment boundaries from z as Useg(z) = {s}N, where N is the number of discovered
segments,ands ∈ N2 denotesstartandendframesofthesegment,indexedass ands forthe
j,0 j,1
j-thsegment. Wedefineanassignmentfunction,A(i) = j,thatgivestheindexofthesegment,j,
givenaframenumber,i,suchthats ≤i<s .Whenthereisnoassignablesegment,A(i)=−1,
j,0 j,1
meaningiisanon-speechframe. Thesegment-averagedfeature,v ,isdefinedbyaveragingacross
j
framesinthej-thsegment,v = 1 (cid:80) z ,where(p,q)=(s ,s ). Then,vT indicates
j p−q k∈[p,q] k j,0 j,1 A(i)
the teacher’s segment-averaged feature of the segment that i-th frame belongs to, being the target
of the regression, which is zero for non-speech frame, vT = 0. Finally, the loss function of the
−1
proposedself-segmentationdistillationisdefinedasL :=(cid:80) ||vT −zS||2.
SegDistill i A(i) i 2
Algorithm1GreedySegmentationAlgorithm
1: procedureGREEDY-SEGMENTATION(states,N thr,M thr)
2: ComputeL2normsandmarkspeechframes:speech =(∥s ∥ ≥N )
i i 2 thr
3: InitializeemptylistofsegmentsS
4: fori=1tondo
5: ifspeech and(nocurrentsegmentorsim(s ,s )<M )then
i i i−1 thr
6: StartnewsegmentS withs
k+1 i
7: elseifspeech then
i
8: Adds tocurrentsegmentS
i k
9: elseifcurrentsegmentS existsthen
k
10: FinalizecurrentsegmentS
k
11: endif
12: endfor
13: foreachboundaryjbetweensegmentsS andS do
k k+1
14: ifspeech then
j
15: Definelocalsearchrangefroma=midpoint(S )tob=midpoint(S )
k k+1
16: Findoptimalboundaryj∗=argmax (cid:80)j sim(s ,avg(S ))+(cid:80)b sim(s ,avg(S ))
j i=a i k i=j+1 i k+1
17: Updateboundarytoj∗
18: endif
19: endfor
20: returnS
21: endprocedure
A.1.2 NOISEAUGMENTATION
Fordenoisingobjective, wemixtheinputwitharandomlysampledenvironmentalsoundorother
speech audio. For mixing with environmental sound, we randomly select a clip from Reddy et al.
(2021)andsamplea5secondsclipfromit. Wefirstz-scorethewaveformandmultiplybyafactor
sampledfrom[0.05,0.7],andmixwithoriginalspeechaudio. Notethattheoriginalspeechisalso
z-scored. Formixingwithotherspeech,werandomlyselectanotherclipinthebatchandshiftfrom
left or right with a percentage sampled from [0.4,0.7], to make sure the original speech holds the
dominantinformationcontextinthemixture. Themagnitudeisalsomodulatedbymultiplyingbya
15UnderReview
factorsampledfrom[0.0,0.2]. Weapplythisaugmentationto20%ofthesamplesinthebatch,and
only to the inputs fed to the student model. Within the 20%, we have the source of noise be 75%
environmentalnoiseand25%otherspeech.
A.1.3 TOKEN-TO-SPEECH
ArticulatoryEncodecArticulatoryEncodec(Choetal.,2024c)iscomposedofarticulatoryencod-
inganddecoding. Theencodingpipelineoutputs14articulatoryfeaturesat50Hzareused,which
arecomposedoftheXYcoordinatesof6articulators(lowerincisor; upperandlowerlips; tongue
tip, bladeanddorsum;), andloudnessandpitch. Theseareinterpretableandgroundedrepresenta-
tions of speech that are fully informative of speech contents (Cho et al., 2024c). The decoder, or
articulatoryvocoder,isaHifi-GAN(Kongetal.,2020)conditionedonaspeakerembeddinginferred
fromaseparatespeakerencoder. Choetal.(2024c)showsthatArticulatoryEncodecsuccessfully
decomposesspeechcontentsandspeakeridentity,bynormalizingpitchtoremovespeakerspecific
pitch level. We replicate the implementation from Cho et al. (2024c), except that we change the
layer of WavLM from which speaker information is extracted from the CNN outputs to the sixth
Transformerlayer,basedontheobservationthatthislayercontributesthemosttothedownstream
speakeridentificationtask(Chenetal.,2022).
Conditionalflow-matching(CFM)TheinputmodelintheCFMiscomposedoftwofeedforward
networks(FFNs)andalinearlayer,whereeachFFNhastwolinearlayerswith512hiddenunitsand
residualconnection, withaReLUactivationanddropoutrateof0.05. Also, Layernormisapplied
totheoutputofeachFFN.Thefinallinearlayerprojectsthe512dimensionalfeatureto256. The
TransformerintheCFMhas8layersandeachlayerhas8headswith64dimensions, and512for
theencodingdimension. WeuseRotarypositionalembeddings(Suetal.,2024). Thefinaloutputis
projectedtothe14dimensionalflowinarticulatoryfeaturespace.
A.1.4 TRAININGDETAILS
We train Sylber in two stages. The first stage is training with segment boundaries inferred from
SDHuBERT which are extracted once at the beginning and fixed while in this stage. The second
stageutilizesonlinesegmentationusingtheteachermodel’soutputs,bythealgorithmin§3.2. Note
that this training is only possible since our model exhibits features clean enough for our greedy
segmentationtowork. Inthesecondstage,theL2normthresholdisupdatedonlinebyaggregating
thestatisticsofspeechandnon-speechsegments,andthemergethresholdisrandomlysampledfrom
[0.8,0.9]. Afterthetraining,thenormthresholdisfixedat3.09andthemergethresholdisfixedat
0.8. See Appendix A.1.5 for details about the thresholding. Sylber is trained for 1.15M steps in
thefirststageandfurthertrainedfor500kstepsinthesecondstage. Weuseabatchsizeof64and
each data point is randomly cropped to be 5 seconds, following Cho et al. (2024b). The learning
rateissetas1e-4withinitial500warmupupdatesforthefirststageand5e-5forthesecondstage.
TheEMAdecayratesaresetas0.9995and0.9999forthefirstandsecondstages,respectively. The
secondstagetrainingimprovesperformanceinsyllabledetectionanddiscovery(AppendixA.1.6).
For the CFM, the learning rate is fixed as 1e-4, with a batch size of 64 and 200k updates. For
ArticulatoryEncodecandunitLM,welargelyfollowChoetal.(2024c)andLakhotiaetal.(2021),
respectively.
A.1.5 THRESHOLDSSETTING
Thresholds in SDHuBERT segmentation For segmentation on SDHuBERT features, we apply
the minimum cut algorithm introduced by Peng et al. (2023) and modified by Cho et al. (2024b).
FollowingChoetal.(2024b),theinitialmaskisobtainedbythresholdingnormsoffeaturesfromthe
eleventhlayerofTransformer,wherewenormalizenormstobein[0,1]anduse0.1asthreshold.The
minimum cut refines each masked chunk to make it syllabic. Specifically, the algorithm conducts
within segment agglomerative clustering with a preset number of clusters. This preset number is
estimatedbyapre-definedspeakingrate. Asthispresetnumberofsyllablesmaybelargerthanthe
numberofsegments, apost-hocmergingprocessmergesadjacent segments withcosinesimilarity
higherthanathreshold,whichwecallthemergethreshold. Fortokenizationexperiments,weusea
moresensitivesegmentationconfigurationthantheoriginalsettingtopreventlossofspeechcontents
due to overly broad segments. Specifically, we halve the estimated syllable duration from 200ms
16UnderReview
to 100ms to cover speech with fast speaking rate, and increase the merge threshold from 0.3 to
0.4. However,theSDHuBERTisstillsensitivetonon-speechnoiseevents. Therefore,wefilterout
segmentswithaverageabsoluteamplitudeofwaveformlowerthan0.05
ThresholdsinSylbergreedysegmentationalgorithmUnlikethethresholdsinSDHuBERT,which
areheuristicallydriven,wetrytosetthethresholdsinouralgorithmmoreprincipledway,especially
inthesecondstageoftrainingwherethetargetsegmentsaredynamicallygenerated. Wefirstsetthe
normthresholdtobeoptimalboundarybetweensignal(speech)andnoise(non-speech),wherethe
likelihoodsofbeingsignalandnoiseareequal. Weassumebothsignalandnoisedistributionstobe
Gaussianandsolvetheequalitycondition. Afterthefirsttrainingstage,weusethepseudoground
truthsegmentsusedfortrainingtogetthedistributionofsegmentnormsandnon-segmentnormsin
thedevsplitofLibriSpeech.Tomakethedistributionreflectsnoise,weapplythenoiseaugmentation
as described in the denoising objective (Sec. A.1.2) to each sample. In the second training stage,
we update the mean and variance of noise distribution using the non-segment portions of student
outputsusinganexponentialmovingaveragewithadecayrateof0.9999,whilekeepingthesignal
distributionthesameasinitiallyset. Thisresultsinthethresholdof3.09aftertraining. Whilethese
segments may not require such frequent updates in threshold, we implement this to try to keep it
principledandempiricallydriven.
Ontheotherhand,westillremainlargelyheuristicallydrivenintermsofsettingourmergethreshold.
We use a particularly high threshold of 0.8 compared to 0.3 in the previous works. Such a high
threshold for merging is effective in Sylber since the features are much cleaner than SDHuBERT.
Insteadsettingthisthresholdtoafixednumber,wesampleavaluefrom[0.8,0.9]duringthesecond
stage training, which is somewhat arbitrarily set after visually inspecting multiple samples. We
found that 0.7 also generally works fine but we select 0.8 as the threshold for the inference since
thatisthelowestnumberintherangeweimposeduringtraining. Infact,thephonemerecognition
experimentempiricallyprovesthat0.8isoptimalwhenthresholdsof0.1incrementsaretested(Table
8).
A.1.6 EFFECTOFTHESECONDSTAGETRAININGWITHONLINESEGMENTATION
To check the effectiveness of the second stage training with the online segmentation, we compare
syllabledetectionanddiscoverymetricsbetweenthestage1andstage2models. AsshowninTable
6,weobservesomegainafterthesecondstagetraining,especiallyinprecisionofthesegmentation.
Table6: Syllabledetectionanddiscoveryperformancecomparisonbetweentwostages.
SyllableDetection SyllableDiscovery
Model
Pr↑ Re↑ F1↑ R↑ SP↑ CP↑ MI↑
Sylber-Stage-1 73.7 69.2 71.4 75.6 63.2 43.9 5.24
Sylber-Stage-2 76.6 68.3 72.2 75.9 64.0 43.9 5.28
A.1.7 EFFECTOFDENOISINGOBJECTIVE
As demonstrated in left two panels in Figure 4, the syllabic structures are already highly visible
withoutthedenoisingobjective,indicatingthatthemajorlearningsourceisself-segmentationdistil-
lationthanthedenoisingobjective. However,addingthedenoisingobjectivesignificantlyimproves
robustness;otherwise,themodelbecomeshighlysensitivetonoisyaudioasshownintherighttwo
panelsinFigure4.
A.2 CODINGEFFICIENCYWITHDURATION-INFORMEDTOKENIZATION
Whenwemeasurecodingefficiencyin§2,weignorethedurationinformation. Here,werecalculate
themetricsbyaddingdurationasseparatetokentaggedtoeachspeechtoken. Notethatdurationis
countedasthenumberofframes,soitalreadyliesondiscretespace. Wefindthat99%ofHuBERT
tokenshavedurationlessthan8,7,and6withthevocabsizeof50,100,and200,respectively. This
meansthatthedurationofeachtokencanbecodedby3bits. However,whenBPEisapplied,these
17UnderReview
Clean audio-W/o denoising Clean audio-W/ denosing Noisy audio-W/o denoising Noisy audio-W/ denosing
Figure 4: Frame-wise similarity matrix from with and without denosing objectives, using clean
signal(lefttwopanels)andnoisysignal(righttwopanel). Theorangewaveformdepictsthesource
noiseweaddtothecleanspeechsignal.
3bitswillbemultipliedbythemaximumnumberofunitsinsubwordstocountper-tokenduration
bits,whichis10to16dependingonthevocabsizeandclustergranularity.
Thesyllabictokensdonotdenselycovertheframes. Therefore,thedurationofsubsequentsilence
canbetaggedalongwiththedurationofthetokens. 98%ofsyllabictokenshavedurationlessthan
or equal to 16 (4 bits). We can also keep the subsequent silence duration up-to 7 frames (3 bits)
efficiently, and silence longer than 7 frames can be regarded as a separate “silence token”, adding
onemoretokentothek-meanscodebook.
Taking all these into consideration, we measure the coding efficiency metrics with the duration-
informed tokens as Table 7. Compared to Table 3, the gap between HuBERT-BPE and ours gets
even larger, where we achieve around or more than 4× gains compared to HuBERT baselines.
Moreover, evenafterappendingdurationtokens, weachievesignificantlylowerbitrateswhichare
beloworaround100.
Table7: Codingefficiencyofduration-informedtokens.
Token/second↓ Bitrate↓ Coding-rate↑
Model Vocabsize Vocabsize Vocabsize
5K 10K 20K 5K 10K 20K 5K 10K 20K
HB50-BPE 7.45 6.82 6.30 449.26 418.24 392.36 0.0058 0.0062 0.0066
HB100-BPE 14.78 14.40 14.10 624.82 666.64 709.06 0.0044 0.0041 0.0039
HB200-BPE 16.67 15.99 15.53 654.77 979.72 967.13 0.0043 0.0029 0.0029
SDHB 5.84 112.73 118.58 124.42 0.0239 0.0228 0.0219
Sylber 4.76 91.80 96.56 101.32 0.0297 0.0284 0.0271
A.3 GENERALREPRESENTATIONALPOWEROFSYLBER
Though the universal utility of our model is not of our focus, we evaluate and benchmark down-
stream tasks using SUPERB (Yang et al., 2021). First of all, to find the optimal merge threshold,
wetrainaphonemerecognition(PR)modelwithsyllabicembeddings,wherethemergethresholdis
sampledfrom[0.3,0.9]. TheregularCTCbasedapproachisnotapplicabletosyllabicgranularity,
since it requires that the input length must be no shorter than the target length. Instead, we adopt
RNN-T(Graves,2012)whichhasnorestrictiononsequencelength. Tokeepthemodelsizesimilar
to the PR model in SUPERB, we use a very simple, non-RNN transcriber, which is a Layernorm
followedbytwolinearlayerswheretheGELUactivationfunctionisappliedtothefirstlinearlayer’s
output. Theoutputsizeofthefirstlayerissetas768andsetasthevocabsizeofphonemes,73,for
thesecondlayer. Thepredictornetworkhasa3layerLSTMwithahiddensizeof1024,0.1dropout
rateandLayernormapplied. ThemodelistrainedwiththeRNN-TimplementationinPyTorch,and
we use beam size of 5 for decoding. The learning rate is set as 0.001 and AdamW is used. The
modelistraineduntilnoimprovementisfoundinvalidationloss. WeuseLibriSpeechcleansubsets
(train-clean, dev-clean, and test-clean), which is the dataset used in SUPERB PR task setting. As
18UnderReview
resultsinTable8,themergethresholdof0.8isselectedandusedthroughouttheSUPERBevalua-
tion. Thisnumbercoincideswiththethresholdweuseinthemainresultsaswell. Weusethecode
providedbyS3PRLfortheexperiment.9
Table8: PhonemerecognitiononLibriSpeech(LS)dev-celanwithdifferentmergethreshods.
PER↓
Dataset
Mthr=0.5 Mthr=0.6 Mthr=0.7 Mthr=0.8 Mthr=0.9
LSdev-clean 6.15 5.88 5.73 5.68 5.68
Weevaluate3versionsofSylber. WefreezethemodelfollowingtheSUPERBprotocol.
Sylber-AllLayerusesalllayerfeatureswithoutsegmentingwith50Hzfull-samplingrate,beinga
regularentrytoSUPERB.
Sylber-Segmentusessegmentembeddingaftersegmentation,withsyllablegranularity.
Sylber-Segment-Expandexpandssegmentembeddingtooriginallength.
Table9comparesthesewithaHuBERTbasemodel,whichhasacomparablemodelsizeandtrained
onthesamedata. SinceSylber-Segmenthasashortersequencelengththanthetarget,thusmaking
theCTC-basedrecognitiontaskinapplicable,wereplacethescoresusingtheaforementionedRNN-
Tmodel,andwefindareasonableperformanceinPRasPERof5.98,whileASRislaggingbylarge
margin. Asourmodelfeaturesaresyllabic,thisstructuremayneedtoberesolvedtobeconverted
tocharacters,addingadditionallayerofcomplexitythanmappingphonemicfeaturestocharacters
whichishardtoresolveinalimitedresourcesetting.
Anothernotablepointisthatourmodelsachievehigherkeywordspottingaccuracy(KS)andintent
classification(IC)comparedtotheHuBERTbasemodelinall3versions. Thisisalignedwiththe
improvedperformanceinlanguagelearningreportedin§6.3. Also,thereisahugedropinspeaker
identityaccuracy(SID)whenoursyllabicembeddingisused,indicatingthatthespeakerinformation
issomewhatmarginalizedout.
Also,thefailureinslotfilling(SF)andautomaticspeechverification(ASV)bySylber-Segmentis
attributed to the fact that S3PRL is tuned to lengthy input of speech representation with a regular
sampling rate. Further investigation is required, for a proper application of syllabic embedding to
thosetasks.
Table9: Performancecomparisonofvariousmodelsacrossdifferentmetrics
PR KS IC SID ER ASR ASR(w/LM) QbE SF ASV SD
Model
PER↓ Acc↑ Acc↑ Acc↑ Acc↑ WER↓ WER↓ MTWV↑ F1↑ CER↓ EER↓ DER↓
Hubert-base 5.41 96.3 98.34 81.42 64.92 6.42 4.79 0.0736 88.53 25.2 5.11 5.88
Sylber-AllLayer 11.78 96.75 98.44 76.16 64.34 11.76 8.32 0.0623 85.79 29.21 6.72 5.08
Sylber-Segment ∗5.98 97.08 98.92 50.59 64.50 ∗14.07 – 0.0139 – – – 13.21
Sylber-Segment-Expand 88.79 97.11 99.08 51.25 65.25 12.04 8.88 0.0591 85.66 29.49 8.75 15.55
A.4 RHYMINGWORDPAIRS
For the consonant at the onset, we constrain the difference to be phonologically adjacent: voiced
orvoicelesssounds(e.g.,“d”ownvs”t”own),non-nasalornasalsounds(e.g.,“b”allvs“m”all),or
spatiallyadjacentpairs(e.g.,“l”estvs“r”est). Fortheconsonantatthecoda,weconfinethewords
tohave“/I/”atthenucleusvoweltominimizedifferentcoarticulationpatterninducedbydifferent
endingconsonants. Weonlyconsidernasalitydifferenceatthecodaandweregardvoicedandun-
voicedconsonantsthesamesincevoiced-nessisrelativelysubtleatthecodaposition. Additionally,
weinclude“n-ng”contrast. Table10showsthefulllistofwordpairs.
9https://github.com/s3prl/s3prl
19UnderReview
Table10: Rhymingwordpairsusedinthediscriminabilitytask.
Onset
Voicedness
b-p v-f d-t z-s g-k
bay,pay vill,fill down,town zeal,seal goal,coal
bar,par vine,fine dall,tall zip,sip gap,cap
ban,pan vault,fault deen,teen zig,sig gain,cane
bad,pad vox,fox dime,time zoo,sue gauge,cage
Nasality Place
b-m d-n l-r t-s
ball,mall dose,nose lock,rock tank,sank
bean,mean dull,null lane,rain tale,sale
boon,moon dine,nine long,wrong tip,sip
bost,most deal,kneal lest,rest tell,sell
Coda
g/k-ng n-ng d/t-n b/p-m
pig,ping thin,thing kid,kin trip,trim
sick,sing bin,bing seed,seen deep,deem
dig,ding sin,sing chit,chin sip,seem
click,cling kin,king grid,grin rip,rim
20