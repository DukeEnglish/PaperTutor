Coevolving with the Other You: Fine-Tuning LLM
with Sequential Cooperative Multi-Agent
Reinforcement Learning
HaoMa1,2∗ TianyiHu1,2∗ ZhiqiangPu1,2† BoyinLiu3
XiaolinAi2 YanyanLiang4 MinChen2
1SchoolofArtificialIntelligence,UniversityofChineseAcademyofSciences
2InstituteofAutomation,ChineseAcademyofSciences
3Alibaba(China)Co.,Ltd.
4MacauUniversityofScienceandTechnology
{mahao2021, hutianyi2021, zhiqiang.pu, xiaolin.ai, chenmin2020}@ia.ac.cn
liuboyin.lby@alibaba-inc.com
yyliang@must.edu.mo
Abstract
Reinforcementlearning(RL)hasemergedasapivotaltechniqueforfine-tuning
large language models (LLMs) on specific tasks. However, prevailing RL fine-
tuningmethodspredominantlyrelyonPPOanditsvariants. Thoughthesealgo-
rithmsareeffectiveingeneralRLsettings,theyoftenexhibitsuboptimalperfor-
manceandvulnerabilitytodistributioncollapsewhenappliedtothefine-tuningof
LLMs. Inthispaper,weproposeCORY,extendingtheRLfine-tuningofLLMsto
asequentialcooperativemulti-agentreinforcementlearningframework,toleverage
the inherent coevolution and emergent capabilities of multi-agent systems. In
CORY,theLLMtobefine-tunedisinitiallyduplicatedintotwoautonomousagents:
apioneerandanobserver. Thepioneergeneratesresponsesbasedonqueries,while
theobservergeneratesresponsesusingboththequeriesandthepioneer’sresponses.
Thetwoagentsaretrainedtogether. Duringtraining,theagentsexchangeroles
periodically,fosteringcooperationandcoevolutionbetweenthem. Experiments
evaluateCORY’sperformancebyfine-tuningGPT-2andLlama-2undersubjective
andobjectiverewardfunctionsontheIMDBReviewandGSM8Kdatasets,respec-
tively. ResultsshowthatCORYoutperformsPPOintermsofpolicyoptimality,
resistancetodistributioncollapse,andtrainingrobustness,therebyunderscoringits
potentialasasuperiormethodologyforrefiningLLMsinreal-worldapplications.
1 Introduction
Largelanguagemodels(LLMs)haveachievedimpressivesuccessacrossdiversedownstreamtasks,
includingdialoguesystems[Ouyangetal.,2022,Touvronetal.,2023],codegeneration[Roziere
etal.,2023],androboticcontrol[Driessetal.,2023,Brohanetal.,2023].However,asthecapabilities
ofLLMsadvance,thechallengesassociatedwithfurtherperformancegainsbecomeincreasingly
intricate. Fine-tuning LLMs for specific tasks presents a significant challenge, prompting recent
explorationofLLMfine-tuningparadigmsuchassupervisedfine-tuning(SFT)[Wuetal.,2021],
reinforcementlearning(RL)fine-tuning[Shojaeeetal.,2023],anddirectpreferenceoptimization
(DPO)[Rafailovetal.,2024]. RLfine-tuningdemonstratespromisingpotentialforrefiningLLM.
∗Theseauthorscontributedequallytothiswork.
†Correspondingauthor:zhiqiang.pu@ia.ac.cn.
Preprint.Underreview.
4202
tcO
8
]IA.sc[
1v10160.0142:viXraCompared to SFT, RL fine-tuning offers a more direct optimization path, aligning training with
desired outcomes and potentially leading to better out-of-distribution performance [Kirk et al.,
2023]. ComparedtoDPO,RLfine-tuningallowsfine-tuningonrule-basedrewardfunctionswithout
requiringpreferencedata.
However,contemporaryRLalgorithmsarenotspecificallydesignedforLLMs. Whenfine-tuningan
LLMusingtheseRLalgorithms,theyexhibitinstabilityandvulnerabilitytodistributioncollapse,
whichmeansthattheLLMisover-optimizedandexhibitshighlybiasedbehavior[Zhengetal.,2023,
Yangetal.,2024b]. FromtheperspectiveofRL,LLMfine-tuninghasseveralchallenges,including
largediscreteactionspaceandsparserewards. TakingtheRLfine-tuningofLlama-2[Touvronetal.,
2023]asanexample,thedimensionoftheactionspaceofLlama-2canreachto32000,representing
32000potentialvocabularychoices. Moreover,therewardsignalisreceivedonlyaftergenerating
the complete response, which results in a sparse reward problem. The above challenges hinder
theexplorationinsuchavastsearchspace,causingtheinstabilityofpopularalgorithmslikePPO
[Schulmanetal.,2017].
Cooperativemulti-agentreinforcementlearning(MARL)representsaparadigmshiftinthefieldof
artificialintelligence(AI),wheremultipleautonomousagentscoevolvewithinacomplexsystem,
resulting in the emergence of new skills [Foerster, 2018, Yang and Wang, 2020, Oroojlooy and
Hajinezhad,2023,Zangetal.,2023]. Languageisanoutcomeofsuchmulti-agentcoevolution. Ina
society,numerousindividualsutilizelanguageforcommunication. Languagesdevelopthroughagent
interactionsandareshapedbysocietalandculturalinfluences. Aslanguagesprogress,theyinfluence
andareinfluencedbytheseinteractions[Cavalli-SforzaandFeldman,1981,Duéñez-Guzmánetal.,
2023]. Inspiredbythis,fine-tuninganLLMwithinacooperativeMARLframeworkmightleadto
theemergenceofsuperiorpoliciesduringcoevolution.
Inthispaper,weproposeaplug-and-playmethodnamedCORY,whichextendstheRLfine-tuningof
LLMstoasequentialcooperativeMARLframework. InCORY,theLLMtobefine-tunedisinitially
duplicatedintotwoautonomousagents3,assignedtworolesrespectively: apioneerandanobserver.
TherearetwofundamentalmechanismsinCORYtoenablethecoevolutionofthetwoLLMagents.
Thefirstisknowledgetransfer,wherethepioneergeneratesaresponseaccordingtoataskquery
independently,andtheobservergeneratesresponsebasedonthequeryaswellastheresponsefrom
thepioneer. Thesecondisroleexchange,wheretherolesofthetwoLLMagentsareexchanged
periodically during training. The two agents share a collective reward, calculated as the sum of
individualtaskrewards,andtheyaretrainedsimultaneouslywiththeirrespectivesamples. Ultimately,
CORYactsasaformofbootstrapping,whereinthecollaborativelearningbetweenLLMsenhances
the effectiveness of RL fine-tuning. Notably, this approach remains algorithm-agnostic, offering
flexibilityforintegrationwithvariousRLalgorithmsbeyondPPO,whilemaintainingsimplicityand
compatibilitywithexistingmethods.
Intheexperimentalevaluation,wesystematicallyinvestigatetheefficacyofourproposedmethod
across two types of reward functions: subjective and objective. Subjective reward functions are
modelstrainedtoalignhumanpreferences,whileobjectiverewardfunctionsarepre-definedfunctions
typicallyestablishedbydomainexperts. Fortheassessmentofsubjectiverewards,weleveragethe
IMDBreviewdataset[Tripathietal.,2020],awell-establishedbenchmarkforsentimentanalysis.
Meanwhile,theevaluationofobjectiverewardsisconductedusingtheGSM8Kdataset[Cobbeetal.,
2021a],whichfocusesonmathematicalwordproblemreasoning. Experimentresultsindicatethat
CORYsurpassesPPOregardingpolicyoptimality,resiliencetodistributioncollapse,androbustness
duringtraining,highlightingitspotentialasanadvancedmethodforimprovingLLMsinpractical
applications.
2 ProblemFormulation
To understand LLMs through the lens of RL, we present a sequential decision-making problem
formulation for the next-token prediction in causal language models. The next-token prediction
is precisely defined by the concept of language-augmented Markov decision process [Li et al.,
2022],denotedasM=<V,S,A,r,P,γ >. Here,V representsavocabularyofalanguagemodel,
encompassingallpossibletokens. Thew ∈V representsaspecifictokenwithinthisvocabulary. The
3The“agents”hererefertoindividualswhomakedecisionsandtakeactionsinthecontextofreinforcement
learning[SuttonandBarto,2018].
2statespaceS ⊂VM,whereVM isthecombinationspaceofM tokens. TheactionspaceA⊂VN,
whereVN isthecombinationspaceofN tokens. M andN arethemaxtokenlengthsforstateand
action, respectively. A state s ∈ S is a concatenation of token sequence s = (w ,w ,...,w ).
1 2 M
Anactiona ∈ Aistheoutputofacausallanguagemodel,construedasaconcatenationoftoken
sequencea=(w ,w ,...,w ). Thestatesandactionsarepaddedwithpadtokenifthereallength
1 2 N
islessthanthemaximumlength. Therewardfunctionr :S×A→Rassignsanumericalscoretoa
sequenceoftokens,whichcanbeconsideredasatypicalsparserewardproblemwithinthecontext
ofRL.ThestatetransitionfunctionP : S ×V → S describesadeterministictransitionofstates
accordingtotheauto-regressiveparadigm. Ateachstep,apredictedtokenisconcatenatedwiththe
stateoflaststep: s =(s ,w )=(s ,w ),wheres denotesatokenizeduser’sinputfora
i+1 i i+1 0 1:i+1 0
causallanguagemodel,andw =(w ,w ,...,w )denotesatokensequenceuptothei-thtoken.
1:i 1 2 i
Then,thetoken-levelpolicyofacausallanguagemodelcanbeencapsulatedwithinπ(w |s ,w ).
i 0 1:i−1
Andthesentence-levelpolicyisdefinedasajointpolicy:
N
(cid:89)
π(a|s )= π(w |s ,w ). (1)
0 i 0 1:i−1
i=1
Therewardfunctionr(·,·)isrelatedtoaspecifictask(e.g.,safetyalignment[Liu,2023,Jietal.,
2024],codegeneration[Shojaeeetal.,2023,Liuetal.,2023]). Ataskrewardisonlyobtainedafter
N steps of decision-making via token-level policy. Under such a sparse reward, RL is prone to
over-optimisation,resultingindistributionalcollapseofthelanguagemodel. Tomitigatetheriskof
distributionalcollapse,itiscommonpracticetoincorporatetoken-levelKLpenaltiesintothereward
function,whichservestoconstrainthedeviationofthelanguagemodelfromitsoriginaldistribution
[Goetal.,2023,Zhengetal.,2023].
(cid:40) −ηKL(π (·|s ,w ),π (·|s ,w )) i<N
θ 0 1:i−1 0 0 1:i−1
rˆ(s ,w )= (2)
i i
r(s ,a)−ηKL(π (·|s ,w ),π (·|s ,w )) i=N,
0 θ 0 1:i−1 0 0 1:i−1
whereηistheKLcoefficient,rˆ(s ,w )representsthetoken-levelcombinedrewardfunction.Foreach
i i
token,aKLpenaltyisimposedbasedontheKLdivergencebetweencurrentpolicyπ (·|s ,w )
θ 0 1:i−1
andinitialpolicyπ (·|s ,w ). Onlyafterpredictingthefinaltoken,doestherewardmodelyield
0 0 1:i−1
atask-specificrewardr(s ,a).
0
3 Method
3.1 CoevolvingwiththeOtherYou(CORY)
ToextendtheRLfine-tuningofLLMstoacooperativeMARLframework,theLLMtobefine-tuned
inCORYisinitiallyduplicatedintotwocopies,eachistreatedasanautonomousagent. Then,two
roles,apioneerandanobserver,areassignedtothesetwoLLMagents. Wedesigntwofundamental
mechanisms to facilitate the coevolution between the two agents. The first design is knowledge
transfer. TheLLMsasynchronouslytakeaction,withthepioneertransferringitsresponse(action)
totheobserver. Theobserverthenutilizesthisinformationtoguideitsowndecision. Thesecond
designisroleexchange. Oncetheobserverachievesasatisfactoryperformance,itexchangesroles
withthepioneer. Inthefollowing,weprovideacomprehensivedescriptionofeachelement,andthe
pipelineofourmethodisshowninFigure1.
KnowledgeTransfer. ToenablecollaborationbetweenthetwoLLMagentsforimprovedresponse
generation,weintroduceaknowledgetransfermechanism. Givenaquerydenotedass ,thepioneer
0
acts first and generates a response denoted as a . Subsequently, the observer receives both the
1
originalquerys andthepioneer’sresponsea togenerateitsownresponsea . Thissequential
0 1 2
interactionfacilitatesknowledgetransfer,wheretheobserverleveragesthepioneer’soutputtoguide
itsowngenerationprocess,potentiallyleadingtoasuperiorresponseduetothein-contextlearning
capabilitiesofLLMs. Thesentence-levelpoliciesofthepioneerandobservercanbeformulatedas
follows:
a ∼π (·|s ), a ∼π (·|s ,a ). (3)
1 pio 0 2 obs 0 1
Duringthetrainingprocess,theparametersofthepioneerandtheobserverareoptimizedseparately
throughanRLalgorithmsuchasPPO.AcooperativerelationshipexistsbetweenthetwoLLMagents.
3LORY
TraditionalRLFine-Tuning CoevolvingwiththeOtherYou (CORY)
Task Query Task Query
(':“Please help me solve the (': fo“ lP lole wa is ne g h mel ap t hm pe r oso bl lv ee m t .h ..e ”
following mathproblem...”
Role Exchange
Pioneer Observer
LLMAgent RewardModel RewardModel
Training Training Training
'(',)' '(',)! +'(',)& '(',)! +'(',)&
Response1 Response2
Response
)!:“Sure!Hereisasolution )&:“Sure!Hereisasolutionbased
)':“Sure!Hereisasolution tothisproblem...” onthereferenceanswer...”
tothisproblem...” Knowledge Transfer
Figure1: TheframeworkofCORY.AtraditionalRLfine-tuningmethodcanbesimplyextendedto
theCORYversionwithonlythreesteps. First,duplicatetheLLMintotwoLLMagents,oneacting
asapioneerandtheotherasanobserver;second,combinethetaskrewardsofthetwoLLMagentsto
replacetheoriginaltaskreward;third,periodicallyexchangetherolesofthetwoLLMagentsduring 7
training. Aftertraining,eithertheLLMagentcanperformthetaskindependently.
Tofacilitatethiscollaboration,CORYemploysacollectivetaskreward,calculatedasthesumof
individualtaskrewards:
r (s ,a ,a )=r(s ,a )+r(s ,a ), (4)
CORY 0 1 2 0 1 0 2
whichimpliesthatboththepioneerandtheobserverreceiverewardsfromeachother’simprovement.
FollowingtheformofEquation2,weaddr andtheKLpenaltytoconstructawholereward
CORY
signal. SimilartoNietal.[2022],wefindthatapartiallycorrectreferencecanalsobebeneficialfor
theobserver. Hence,itisnotnecessaryforthepioneertogenerateahigh-qualityresponse.
Role Exchange. During training, the observer may develop a prompt bias due to consistently
receivinginputsintheformof(s ,a ). Thisrelianceonpromptsthatcombinetheoriginalquery
0 1
withthepioneer’sresponse,hinderstheobserver’sabilitytogenerateresponsesindependently. To
addressthisissue,weintroducearoleexchangemechanism. Thismechanisminvolvesexchanging
therolesofthepioneerandobserverperiodicallyduringtraining:
π (·|s )=π (·|s ;θ ), π (·|s ,a )=π (·|s ,a ;θ ), ifswap=False
pio 0 pio 0 1 obs 0 1 obs 0 1 2
(5)
π (·|s )=π (·|s ;θ ), π (·|s ,a )=π (·|s ,a ;θ ), ifswap=True,
pio 0 pio 0 2 obs 0 1 obs 0 1 1
whereswapisinitializedasFalse,andreverseperiodically. Thisexchangeensuresthatboththe
LLMsexperiencebothroles(pioneerandobserver)multipletimesthroughoutthetrainingprocess.
Throughthisroleexchangemechanism,theyareforcedtoadapttobothpromptformats: s alone
0
andthecombinedformat(s ,a ). ThisallowsustouseeitherLLMindividuallyduringinference.
0 1
Fromarepresentationallearningperspective,thisroleexchangemechanismencouragestheLLMsto
developaunifiedrepresentationfors and(s ,a ). Thisunifiedrepresentationcapturestheessential
0 0 1
informationfromthetaskquery,regardlessofthespecificpromptformatpresentedduringtrainingor
inference.
ThesetwokeymechanismsinCORYactasaformofbootstrapping.ThetwoLLMagentscollaborate,
withtheobserverpotentiallylearningbetterpoliciesbyleveragingthepioneer’soutput.Roleexchange
ensuresboththeLLMsbenefitfromthiscollaborativelearning,similartocooperativelearningamong
humans. Importantly, CORY is an algorithm-agnostic approach, meaning it can be theoretically
compatible with various RL algorithms beyond PPO. Additionally, CORY offers the advantages
of simplicity in implementation and seamless integration with existing frameworks, making it a
plug-and-playsolution. ThederivationoftheCORY’spolicyupdatecanbefoundinAppendixB,
andthedetailedpseudocodesareprovidedinAppendixC.
4Pareto Frontier
Sub-optimal Frontier
Pareto Frontier Pareto SFurobn-toieprtimal 0
SPuabre-toop Ftriomnatiler Sub-opStuimba-loptimal Frontier of PPO
5
Sub-optimal Sub-opStuimba-lo Fprotinmtiaelr Forfo PnPtOier of DTSE
LK LK Sub-optimal Frontier of CORY 10
-LK- LK-- 15
20
PPO
25 CORY
0.12 0.14 0.16 0.18 0.20
TaTsaks kR Reewwaarrdd TaskT Raeswka Rrdeward Task Reward
(a) Paretofrontier (b) Sub-optimalfrontier (c) Empiricalresult
Figure2: TheempiricaldemonstrationofwhyCORYsurpassessingle-agentRLfine-tuning. In(c),
thevaluesofηfromlefttorightare1e-5,1e-4,1e-3,and1e-2.
3.2 UnderstandingCORY
FollowingtheexplanationofCORYinSection3.1,thissectionprovidesanempiricaldemonstration
ofwhytheproposedmethodsurpassesthesingle-agentRLfine-tuningmethod.
Infact,RLfine-tuningwithKLpenaltyinherentlyformulatesamulti-objectivereinforcementlearning
problem. TheLLMagentstrivestoconcurrentlymaximizethetaskrewardandminimizetheKL
divergence. Unfortunately,thesetwoobjectivesmaybeinoppositiontooneanother. Thisisbecause
maximizingthetaskrewardwillinevitablyleadtotheoutputdistributiondeviatingfromthepre-
trainedmodel,resultinginanincreaseinKLdivergence. Hence,theoptimizationprocessseeksa
trade-offbetweenthetaskrewardandtheKLdivergence,ideallydrivingthepolicytowardsaPareto
frontier[Ngatchouetal.,2005]. Thisfrontiercoversallachievablepolicieswherenopolicycan
improveononeobjectivewithoutsacrificingperformanceontheother. Formally,theParetofrontier
canbedefinedas:
F :={J (π)|π ∈Π∧∄π′ ̸=π :J (π′)≥J (π)}, (6)
r r r
where J (π) = E [(cid:80)T γr(s ,a )]. r(s,a) ∈ Rm is a vector-valued reward function and Π
r π t=0 t t
denotesthesetofallpolicies. Givenafixedreferencevectorω ∈ Ω ⊆ Rm,onecouldscalarize
themulti-objectiverewardintoasingleobjectivebyusingtheweightedsumωTr(s,a). Underthis
preferenceweighting,theidealoutcomeforthepolicyistoconvergetoapointontheParetofrontier,
asillustratedbytheblackdotsinFigure2(a).
However,duetotheinherentcomplexitiesofnaturallanguage,achievingperfectpolicyconvergence
totheParetofrontierisoftenintractable.Nevertheless,byadjustingthepreferences,thesesub-optimal
policiescanstillformafrontierasillustratedinFigure2(b).Forsimplicity,wetermitthesub-optimal
frontier. Ourhypothesisisthatthesub-optimalfrontierachievedbyCORYliesclosertothetrue
Paretofrontiercomparedtothatachievedbysingle-agentRLmethod.
To verify this hypothesis, we fine-tune the Llama-2-7b-chat model on the grade school math 8K
(GSM8K)dataset[Cobbeetal.,2021b]usingbothPPOandCORY.WemeasuretheKLdivergence
andthetaskrewardobtainedbyeachpolicyafterconvergence. Byadjustingthepreference,i.e.,η
inEquation2,weareabletogeneratesub-optimalfrontiersforboththemethods,asillustratedin
Figure2(c).ItisimportanttonotethattheY-axisrepresentsthenegativeKLdivergence(largervalues
indicatebetterperformance). Asexpected,thesub-optimalfrontierachievedbyCORYconsistently
outperformsthatofPPO,empiricallyvalidatingthehypothesis.
Ouranalysisthroughthelensofmulti-objectiveRLoffersvaluableinsightsintotheeffectivenessof
CORY.Theknowledgetransfermechanisminherentlyaddressestheoptimizationchallengesfacedby
theobserver. Byleveragingthereferenceresponseprovidedbythepioneer,theobserveractually
experiences a guided optimization process. Such guided process can alleviate the optimization
pressureonthetaskrewardside,andprioritizeimprovementontheKLpenaltyside. However,since
theobserver’spolicyduringtrainingtakesboththetaskqueryandthepioneer’sresponseasinputs,
theoptimizedpolicyisnottheonewereallywant(weneedthepolicywhichonlytakesthetaskquery
asinput),resultinginthepromptbiasissue. Theroleexchangemechanismcaneffectivelyaddress
thisissue,andtransfertheskillslearnedbytheobserverbacktothepioneer,reducingthepioneer’s
optimization pressure. Notably, CORY demonstrates significantly better stability and robustness
comparedtosingle-agentRLmethod(SeedetailsinSection4.2andAppendixE).Itconsistently
5
LK
-achievesalowerKLdivergencebetweenthefine-tunedandpre-trainedmodelswhilemaintaining
strongperformanceonthetargettask,signifyingabettertrade-offbetweenthetwoobjectives.
4 Experiments
ThissectionsystematicallyinvestigatetheperformanceofCORYacrosstwotypesofrewardfunctions:
subjectiverewardfunctionandobjectiverewardfunction. Subjectiverewardfunctionsarereward
modelstrainedondatacapturinghumanpreferences. Theyessentiallytranslatethehumansentiment
orjudgmentintoanumericalrewardsignalthatguidesalignment. Objectiverewardfunctionsare
pre-definedrule-basedfunctions,typicallyestablishedbydomainexperts. Thiscategorizationreflects
real-worldscenarioswhererewardfunctionsmightbelearnedfromhumanpreferencesormanually
craftedbydomainexperts. PromptsusedinexperimentsaredetailedinAppendixA.2.
4.1 SubjectiveRewardsonIMDBReview
TaskSetup. Toevaluateourmethodunderthesubjectiverewardsetting,weselecttheIMDBReview
dataset[Tripathietal.,2020]. Thisdatasetcontains50K<text,label>pairs,withthetrainingsetand
thetestseteachcontains25Kpiecesofdata. ThetextsintheIMDBdatasetaremoviereviews,and
thelabelsarethebinarysentimentclassificationlabels. Thedistilbert-imdbmodel4 trainedonthe
datasetisemployedastherewardmodel. Wefine-tuneGPT2-Large(774M)5byusingsingle-agent
PPO(single-PPO)andCORYrespectively. Inaddition,GPT2-XL(1.5B)6 isfine-tunedbyusing
single-PPOasanablationonmodelsize. Inthistask,werandomlysampletextsnippetsfromthe
IMDB dataset. The first 2 to 8 tokens (representing the beginning of the review) are retained as
promptsforsentimentcompletion. TheLLMsgeneratecontinuationsthattransformtheprompts
into positive sentiment comments. After that, the reward model evaluates the generated text to
assignasentimentscore. Theobjectiveistomaximizetheaveragesentimentscoreofthecompleted
comments. ExamplesofthistaskaredetailedinAppendixD.
Intheexperiments,eachmethodundergoes100trainingiterationsusingabatchsizeof256. For
simplicity,GPT2-LargeandGPT2-XLfine-tunedbysingle-PPOaretermedasPPO-GPT-2-land
PPO-GPT-2-xl,respectively. GPT-2-Largethatfine-tunedbyCORYarereferredtoCORY-LLM1and
CORY-LLM2,wheretheformeroneistheLLMthatinitializedasthepioneer,andthelatteroneisthe
LLMthatinitializedastheobserver.
PPO-GPT-2-l
2.5 PPO-GPT-2-xl 0.0
40 CORY-LLM1
2.0 CORY-LLM2 2.5
1.5 20 5.0
PPO-GPT-2-l PPO-GPT-2-l
1.0 PPO-GPT-2-xl 7.5 PPO-GPT-2-xl
CORY-LLM1 CORY-LLM1
0.5 CORY-LLM2 0 10.0 CORY-LLM2
20 40 60 80 100 20 40 60 80 100 25 50 75 100
Iteration Iteration Iteration
(a) Taskreward (b) KLdivergence (c) Combinedreward
Figure3: TrainingcurvesundersubjectiverewardsonIMDBReview.
Results and Analysis. We monitor the training process by visualizing task reward, KL diver-
gence, and a combined reward function that incorporates both the above objectives. Denoted
as r (s ,a), the combined reward function can be expressed as r (s ,a) = r(s ,a) + η ∗
c 0 c 0 0
KL(s ,π ,π ), where r(s ,a) and KL(s ,π ,π ) are the sentence-level task reward part and
0 θ 0 0 0 θ 0
theKLpenaltypart,respectively. AndtheKLpenaltypartcanbecalculatedasKL(s ,π ,π )=
0 θ 0
(cid:80)
−KL(π (·|s ,w ),π (·|s ,w )).
i=0,1,...,N θ 0 1:i−1 0 0 1:i−1
Itisimportanttonotethat,theactualrewardusedfortraininginCORYisnotthecombinedreward.
TheactualtrainingrewardnotonlyincludestheKLpenaltyandthetaskrewardfromthetargetagent,
4https://huggingface.co/lvwerra/distilbert-imdb
5https://huggingface.co/openai-community/gpt2-large
6https://huggingface.co/openai-community/gpt2-xl
6
drawer
ksaT
LK
drawer
denibmoCbut also includes the task reward from the other agent. In fact, the combined reward r (s ,a) is
c 0
therealoverallobjectivethatneedstobeoptimized,andcanbealignedwiththesingle-agentRL
fine-tuning,makingiteasiertocompareperformanceofallthemethods.
Thetrainingcurvesoftaskreward,KLdivergence,andthecombinedrewardareillustratedinFigure3.
Theresultsshowthatsingle-PPOandCORYachievesimilartaskrewardlevelsafter100training
iterations. However,thecurveofKLdivergencerelatedtosingle-PPOissignificantlyhigherthan
thatofCORY,reachingmorethantwicethelevelofCORYafterallthetrainingiterations. This
indicates CORY’s ability to achieve similar task reward levels with a smaller deviation from the
pre-trainedpolicy. Moreover,itcanbeobservedthatthecurvesofCORY-LLM1andCORY-LLM2are
veryclose,indicatingthatthetwoLLMagentsinitiallyplayingdifferentrolesfinallyachievevery
similarperformancelevelsattheendofthetraining. ConsistentwiththemotivationofCORY,both
thefine-tunedLLMagentscanbeusedtofinishtasksindividually,whichverifiestheeffectivenessof
thebootstrappedlearningandcoevolutionprinciplesinCORY.
Finally, Figure 3(c) visually confirms CORY’s advantage in combining the two objectives. The
combinedrewardcurveforCORYconsistentlyrises,indicatingitseffectivenessinsimultaneously
improvingtaskrewardandminimizingKLdivergence. Conversely,PPO’scombinedrewardcurve
exhibitsadecreasingtrend,suggestingitsstruggleinbalancingtheseobjectives. Hyperparameters
usedforbothsingle-PPOandCORYaredetailedinAppendixA.1.
4.2 ObjectiveRewardsonGSM8K
TaskSetup. Toevaluateourmethodunderarule-basedobjectiverewardfunction,weselectthe
GSM8Ktask[Cobbeetal.,2021a]. GSM8Kcomprises8.79Khigh-quality,linguisticallydiverse
gradeschoolmathwordproblems,with7.47Kallocatedfortrainingand1.32Kfortesting. Foreach
questioninthedataset,aresponseisobtainedviaLLM.Thepreciseanswerisextractedfromthe
responsesusingaregularexpression,typicallythefinalsetofnumbersintheresponse. Ifthenumber
inquestionmatchesthegroundtruthasrecordedinthedataset,arewardof1isawarded. Conversely,
ifthenumberisincorrect,arewardof0isgiven. TheLlama-2-7b-chat7 modelisselectedasthe
pre-trainedmodel.Toreducethetrainingoverhead,themodelisquantisedto4-bit. Forsimplicity,the
4-bitLlama-2-7b-chatmodelfine-tunedwithsingle-PPOisreferredtoasPPO-Llama-2. Thecopied
modelsfine-tunedwithCORYarereferredtoCORY-LLM1andCORY-LLM2,wheretheformeris
the LLM that initialized as the pioneer, and the latter is the LLM that initialized as the observer.
ExamplesofthistaskaredetailedinAppendixD.
0.20
PPO
0.20 10.0 CORY-LLM1
CORY-LLM2 0.15
7.5
0.15 0.10
5.0
PPO 2.5 0.05 PPO
0.10 CORY-LLM1 CORY-LLM1
CORY-LLM2 0.0 0.00 CORY-LLM2
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Iteration Iteration Iteration
(a) Taskreward (b) KLdivergence (c) Combinedreward
Figure4: TrainingcurvesunderobjectiverewardsonGSM8K.
ResultsandAnalysis. SimilartoSection4.1,Wemonitorthetrainingprocessbyvisualizingtask
reward, KL divergence, and the combined reward. As shown in Figure 4, the jitter observed in
allcurvessuggeststhechallengeposedbyGSM8K.Thevastexplorationspacepresentsinherent
instabilityfortheRLalgorithms. AsFigure4(a)illustrates,thetaskrewardcurveofsingle-PPO
peaksaround50trainingiterations,followedbyadecline. Single-PPO’sKLdivergenceexhibits
noconvergencetrend,reachingamaximumvalueduringtraining(Figure4(b)). Theinstabilityof
single-PPO results the high KL divergence after 50 iterations, leading to a poor performance on
combinedreward(Figure4(c)).
7https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
7
drawer
ksaT
LK
drawer
denibmoCIncontrast,CORYdemonstratesasignificantlymorestabletask
0.40
rewardcurve,consistentlyoutperformingsingle-PPO.What’s
0.35
more, CORY achieves a considerably lower KL divergence
comparedtosingle-PPO,facilitatingfasterconvergence. This 0.30
characteristicisparticularlyvaluableinthefine-tuningcontext, 0.25
asitallowsCORYtoachievesimilarorevenbettertaskrewards 0.20 Llama-2-7b-chat
PPO
withoutsignificantmodificationstotheoriginalparameterdis- 0.15 CORY
tributions.
1 2 3 4 5
Furthermore, the combined reward curves visually confirm Pass@k
CORY’ssuperiorityoversingle-PPO.CORY’sabilitytoeffec-
Figure 5: Evaluation results on
tivelybalancethetwoobjectivesisreflectedinitssteadilyin-
GSM8Ktestdataset.
creasingcombinedreward. Conversely,single-PPO’sstruggle
with balancing the objectives manifest as a decreasing com-
binedrewardandtraininginstability.
Inaddition,weconductacomparativeanalysisofmodelsfine-tunedwithdistinctmethodsanda
pre-trainedmodelontheGSM8KtestsetasshowninFigure5. Theevaluationmetricutilizedis
pass@k,whichgenerateskcorrespondingrepetitionsforasampleandpassesifatleastoneiscorrect.
ThetestresultsdemonstratethattheCORYfine-tuned4bitLlama-2-chat-7bcouldachieveapass@1
of18%onGSM8Ktestdataset.
4.3 Ablations
Inablationexperiments,weablatetheinfluenceofmodelsize,knowledgetransfer,androleexchange
underthesubjectiverewardsettingonIMDBreviewdataset. FormethodnamesdepictedinFigure6,
RExindicatesroleexchange,KT indicatesknowledgetransfer,LLM1andLLM2refertoLLMswho
areinitializedasthepioneerandtheobserverrespectively.
3.0 CORY-w/o-REx-LLM1
2.5 C CO OR RY Y- -w w/ /o o- -R KTE -x L- LL MLM 12 0.0
2.0 40 C C CO O OR R RY Y Y- - -w L LL L/o M M-K 1 2T-LLM2 2.5
1.5 5.0
CORY-w/o-REx-LLM1 20 CORY-w/o-REx-LLM1
1.0 CORY-w/o-REx-LLM2 7.5 CORY-w/o-REx-LLM2
CORY-w/o-KT-LLM1 CORY-w/o-KT-LLM1
0.5 C CO OR RY Y- -w LL/o M-K 1T-LLM2 0 10.0 C CO OR RY Y- -w LL/o M-K 1T-LLM2
CORY-LLM2 CORY-LLM2
20 40 60 80 100 20 40 60 80 100 25 50 75 100
Iteration Iteration Iteration
(a) Taskreward (b) KLdivergence (c) Combinedreward
Figure6: Trainingcurvesforablationsexperiments.
AblationonModelSize. Ourmethodemploystwomodelsduringtraining,withthetotalparameters
trainedbeingdoubledincomparisontosingle-PPO.Inordertoablatewhethertheenhancementof
CORYisderivedfromtheexpansionofthemodelparameters,anadditionalfine-tuningofGPT2-XL
(1.5B)withsingle-PPOisconductedontheIMDBdataset,whichhastwicethenumberofparameters
asGPT2-Large. TheresultsarepresentedinFigure3. Whilethetaskrewardofthemodelrapidly
reachesitsmaximumvalue,theKLpenaltypartdoesnotexhibitanotableimprovementcomparedto
GPT2-Large. TheKLdivergencecontinuestoincrease,leadingtothecollapseofthedistribution.
Ablation on Knowledge Transfer. We maintain role exchange, and the two models still share
a collective task reward (Equation 4), but disable knowledge transfer. This resembles PPO with
individual queries as inputs. However, without the observability of the pioneer’s outputs, this
equivalenttoaddingnoisetothePPOrewardsignal. Consequently,thetaskrewardsbecomeunstable,
andtheKLdivergencesarehighercomparedtoCORYasshowninFigure6. Thishighlightsthe
importanceofobservabilityforframingRLfine-tuningasatruemulti-agentcooperationproblem.
AblationonRoleExchange. Wemaintainknowledgetransferbutdisableroleexchange. Asevident
fromFigure6,bothLLMsachievegoodtaskrewards,buttheirKLdivergencesaremuchhigherthan
thatofCORY.Notably,theobserverLLMexhibitssignificantlylowerKLdivergencecomparedto
thepioneerLLM.Thisobservationhighlightsafascinatingphenomenonincooperativelearning:
8
drawer
ksaT
LK
ycaruccA
drawer
denibmoCbyreceivingthepioneer’sresponse,theobservercaneffectivelyoptimizetheKLdivergence. This
suggeststhattheobserverleveragesthepioneer’sexplorationtorefineitspolicywhilemaintaining
goodperformance,potentiallyleadingtoamorestablelearningprocess.
5 RelatedWork
Themostrelatedtopicisreinforcementleanringfromhumanfeedback(RLHF).InstructGPT[Ouyang
etal.,2022]fine-tunesGPT-3likemodels[Brownetal.,2020]toenhancehelpfulnessbycombining
SFTwithRLbasedonhumanpreferencedataset. Askelletal.[2021]trainsapreferencemodelfor
aligningtheLLMwithhumanvalues. Itarguesthatrankedpreferencemodelingprovestobethe
mosteffectivetrainingobjectivefordistinguishingbetweendesirableandundesirableLLMbehaviors.
Baietal.[2022]incorporatesaniterativeonlinetrainingmodewherepreferencemodelandLLM
areupdatedweeklyusingfreshhumanfeedbackdata. Existingresearchacknowledgestheinherent
complexity,instability,andhyperparametersensitivityofRLHF,particularlywhenemployingPPO
Zhengetal.[2023]. Severalworkshaveattemptedtoaddressthesechallengesbyintroducingmax-
entropyregularization[Wenetal.,2024],hyperparametertuning[Zhengetal.,2023],andreward
shaping[Yangetal.,2024a]. However,thesemethodsdoesnotshowsignificantimprovementover
thevanillaPPOalgorithm. Thisinspiresustoexplorealternativemethodfromadifferentperspective
thatextenttheRLfine-tuningofLLMstoacooperativeMARLproblem.
AnotherrelatedtopicisMARL.Undertheinteractionrelationship(cooperation,competition,mixed),
multi-agentcouldspontaneouslyemergecomplexanddiversepolicies,soastosolvethecomplex
problemsthatsingle-agentreinforcementlearningisdifficulttosolve. Forexample,inKimetal.
[2023],theRLbasedprompttuningisdecomposedintomulti-agentjointtuning. Thehugejoint
action space is equally split across agents, learning better and longer prompt. Such mechanisms
havealsobeenappliedinthefieldofcombinatorialoptimization. Thepaperthatismostsimilar
tousonthearchitectureofagenttrainingisGaoetal.[2023]. Itproposesanasymmetrictraining
symmetricexecutionframeworktodealwiththetwo-agentStackelberggameFangetal.[2021]. In
theStackelberggame,twoagentsmakedecisionsasynchronously. Theagentthatmakesthedecision
latercanobservetheformeragent,buttheformeragentcannotobservethelateragent. Thetraining
frameworkproposedbytheauthorsisabletoconvergeinStackelbergequilibriumempirically. This
inspiresustodesignthetrainingframeworkforLLMsunderasequentialcooperativesetting.
6 Discussion
ExperimentalevidencesuggeststhatCORYyieldsmorestableandsuperiorperformanceinRLfine-
tuning.Thiscanbeattributedtoourextensionofsingle-agentRLfine-tuningintoacooperativeMARL
version. Inthissection,wedelveintoadiscussionofhowthemulti-agentlearningcanbenefitLLM
fine-tuning. Theprimarybenefitisthatmulti-agentlearningencouragesthecoevolutionofLLMs
throughcollectiveliving,socialrelationshipsandmajorevolutionarytransitions[Duéñez-Guzmán
etal.,2023]. Thisprocessgeneratesavarietyofnewdata,whichfurtherfacilitatescoevolution. This
mechanismcontributestomanybreakthroughsingamesAI,suchasGo[Silveretal.,2016,2017,
ClarkandStorkey,2015],StarCraftII[Vinyalsetal.,2019],andDiplomacy[Bakhtinetal.,2022].
In this paper, we investigate the application of cooperative MARL to address challenges in RL
fine-tuning. CooperativeMARLfine-tuningappearstoincreasetrainingrobustnessandpreventdis-
tributioncollapse. Whileweconcentrateoncooperation,competitiveMARL,especiallypopulation-
basedmethods, representsapromisingdirectionforfutureresearch. Theseapproachescreatean
auto-curriculum mechanism driven by a natural arms race, which propels agent learning and en-
ablesmasteryofcomplextasks. Besidestheinteractionparadigm,thescaleofagentsiscrucialto
emergence. WhileweexamineasettinginvolvingtwoLLMs,incorporatingmoreLLMsinMARL
fine-tuningisanintriguingprospectforfuturestudies.
7 Conclusion
Inthispaper,weextendtheRLfine-tuningofLLMstoasequentialcooperativeMARLframework.
Tothisend,weduplicatethepre-trainedLLMintotwoLLMagentswithdifferentroles,anddesign
twokeymechanisms: knowledgetransferandroleexchange. ThesemechanismsenablethetwoLLM
9agentstolearncollaboratively,andafterthefine-tuningprocess,eithertheLLMagentcanbechosen
toperformthetaskindependently. Wealsoprovideanin-depthanalysisofRLfine-tunefromthe
perspectiveofmulti-objectiveRL,revealingtheexistenceofaParetofrontierbetweenKLdivergence
andtaskreward. WeempiricallyillustratethatCORYhasanadvantageoversingle-agentRLmethod
inapproachingtheParetofrontier. ExperimentresultsindicatethatCORYsurpassesPPOregarding
policyoptimality,resiliencetodistributioncollapse,androbustnessduringtraining,highlightingits
potentialasanadvancedmethodforimprovingLLMsinpracticalapplications.
8 Acknowledgement
ThisworkwassupportedbytheStrategicPriorityResearchProgramofChineseAcademyofScience
underGrantNo. XDA27030204,theNationalNaturalScienceFoundationofChinaunderGrant
62322316,theBeijingNovaProgramunderGrant20220484077and20230484435.
References
AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,DeepGanguli,TomHenighan,AndyJones,
NicholasJoseph,BenMann,NovaDasSarma,etal. Agenerallanguageassistantasalaboratory
foralignment. arXivpreprintarXiv:2112.00861,2021.
YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,DawnDrain,
StanislavFort,DeepGanguli,TomHenighan,etal. Trainingahelpfulandharmlessassistantwith
reinforcementlearningfromhumanfeedback. arXivpreprintarXiv:2204.05862,2022.
AntonBakhtin,NoamBrown,EmilyDinan,GabrieleFarina,ColinFlaherty,DanielFried,Andrew
Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by
combininglanguagemodelswithstrategicreasoning. Science,378(6624):1067–1074,2022.
AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,XiChen,KrzysztofChoromanski,
TianliDing, DannyDriess, AvinavaDubey, ChelseaFinn, etal. Rt-2: Vision-language-action
modelstransferwebknowledgetoroboticcontrol. arXivpreprintarXiv:2307.15818,2023.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
LuigiLucaCavalli-SforzaandMarcusWFeldman. Culturaltransmissionandevolution: Aquantita-
tiveapproach. Number16.PrincetonUniversityPress,1981.
ChristopherClarkandAmosStorkey. Trainingdeepconvolutionalneuralnetworkstoplaygo. In
Internationalconferenceonmachinelearning,pages1766–1774.PMLR,2015.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,ChristopherHesse,andJohn
Schulman. Trainingverifierstosolvemathwordproblems. arXivpreprintarXiv:2110.14168,
2021a.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifierstosolve
mathwordproblems. arXivpreprintarXiv:2110.14168,2021b.
DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,Ayzaan
Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal
languagemodel. InInternationalConferenceonMachineLearning,pages8469–8488.PMLR,
2023.
EdgarADuéñez-Guzmán,SuzanneSadedin,JaneXWang,KevinRMcKee,andJoelZLeibo. A
socialpathtohuman-likeartificialintelligence. NatureMachineIntelligence,5(11):1181–1188,
2023.
10Fei Fang, Shutian Liu, Anjon Basak, Quanyan Zhu, Christopher D Kiekintveld, and Charles A
Kamhoua. Introductiontogametheory. GameTheoryandMachineLearningforCyberSecurity,
pages21–46,2021.
JFoerster. Deepmulti-agentreinforcementlearning. PhDthesis,UniversityofOxford,2018.
YuanGao,JunfengChen,XiChen,ChongyangWang,JunjieHu,FuqinDeng,andTinLunLam.
Asymmetricself-play-enabledintelligentheterogeneousmultirobotcatchingsystemusingdeep
multiagentreinforcementlearning. IEEETransactionsonRobotics,2023.
DongyoungGo,TomaszKorbak,GermánKruszewski,JosRozen,NahyeonRyu,andMarcDymet-
man. Aligning language models with preferences through f-divergence minimization. arXiv
preprintarXiv:2302.08215,2023.
JiamingJi,MickelLiu,JosefDai,XuehaiPan,ChiZhang,CeBian,BoyuanChen,RuiyangSun,
YizhouWang,andYaodongYang. Beavertails: Towardsimprovedsafetyalignmentofllmviaa
human-preferencedataset. AdvancesinNeuralInformationProcessingSystems,36,2024.
Dong-KiKim,SungryullSohn,LajanugenLogeswaran,DongsubShim,andHonglakLee. Mul-
tiprompter: Cooperative prompt optimization with multi-agent reinforcement learning. arXiv
preprintarXiv:2310.16730,2023.
Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward
Grefenstette,andRobertaRaileanu. Understandingtheeffectsofrlhfonllmgeneralisationand
diversity. InTheTwelfthInternationalConferenceonLearningRepresentations,2023.
Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An
Huang,EkinAkyürek,AnimaAnandkumar,etal. Pre-trainedlanguagemodelsforinteractive
decision-making. AdvancesinNeuralInformationProcessingSystems,35:31199–31212,2022.
Jiate Liu, Yiqin Zhu, Kaiwen Xiao, QIANG FU, Xiao Han, Yang Wei, and Deheng Ye. Rltf:
Reinforcementlearningfromunittestfeedback. TransactionsonMachineLearningResearch,
2023.
YangLiu. Theimportanceofhuman-labeleddataintheeraofllms. arXivpreprintarXiv:2306.14910,
2023.
Patrick Ngatchou, Anahita Zarei, and A El-Sharkawi. Pareto multi objective optimization. In
Proceedings of the 13th international conference on, intelligent systems application to power
systems,pages84–91.IEEE,2005.
AnsongNi,JeevanaPriyaInala,ChenglongWang,OleksandrPolozov,ChristopherMeek,Dragomir
Radev,andJianfengGao. Learningmathreasoningfromself-sampledcorrectandpartially-correct
solutions. arXivpreprintarXiv:2205.14318,2022.
AfshinOroojlooyandDavoodHajinezhad. Areviewofcooperativemulti-agentdeepreinforcement
learning. AppliedIntelligence,53(11):13677–13722,2023.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems,35:27730–
27744,2022.
RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,andChelsea
Finn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel. Advances
inNeuralInformationProcessingSystems,36,2024.
BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,Yossi
Adi,JingyuLiu,TalRemez,JérémyRapin,etal. Codellama: Openfoundationmodelsforcode.
arXivpreprintarXiv:2308.12950,2023.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
11Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. Execution-based code
generationusingdeepreinforcementlearning. arXivpreprintarXiv:2301.13816,2023.
DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriessche,
JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal. Mastering
thegameofgowithdeepneuralnetworksandtreesearch. nature,529(7587):484–489,2016.
DavidSilver,JulianSchrittwieser,KarenSimonyan,IoannisAntonoglou,AjaHuang,ArthurGuez,
ThomasHubert,LucasBaker,MatthewLai,AdrianBolton,etal.Masteringthegameofgowithout
humanknowledge. nature,550(7676):354–359,2017.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
Sandesh Tripathi, Ritu Mehrotra, Vidushi Bansal, and Shweta Upadhyay. Analyzing sentiment
usingimdbdataset. In202012thInternationalConferenceonComputationalIntelligenceand
CommunicationNetworks(CICN),pages30–33.IEEE,2020.
OriolVinyals,IgorBabuschkin,WojciechMCzarnecki,MichaëlMathieu,AndrewDudzik,Junyoung
Chung,DavidHChoi,RichardPowell,TimoEwalds,PetkoGeorgiev,etal. Grandmasterlevelin
starcraftiiusingmulti-agentreinforcementlearning. Nature,575(7782):350–354,2019.
MuningWen,ChengDeng,JunWang,WeinanZhang,andYingWen.Entropy-regularizedtoken-level
policyoptimizationforlargelanguagemodels. arXivpreprintarXiv:2402.06700,2024.
JeffWu,LongOuyang,DanielMZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.
Recursivelysummarizingbookswithhumanfeedback. arXivpreprintarXiv:2109.10862,2021.
ShentaoYang,ShujianZhang,CongyingXia,YihaoFeng,CaimingXiong,andMingyuanZhou.
Preference-groundedtoken-levelguidanceforlanguagemodelfine-tuning. AdvancesinNeural
InformationProcessingSystems,36,2024a.
WanliYang,FeiSun,XinyuMa,XunLiu,DaweiYin,andXueqiCheng.Thebutterflyeffectofmodel
editing: Feweditscantriggerlargelanguagemodelscollapse. arXivpreprintarXiv:2402.09656,
2024b.
Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game
theoreticalperspective. arXivpreprintarXiv:2011.00583,2020.
YifanZang,JinminHe,KaiLi,HaoboFu,QiangFu,andJunliangXing. Sequentialcooperative
multi-agent reinforcement learning. In Proceedings of the 2023 International Conference on
AutonomousAgentsandMultiagentSystems,pages485–493,2023.
RuiZheng,ShihanDou,SongyangGao,YuanHua,WeiShen,BinghaiWang,YanLiu,SenjieJin,
QinLiu,YuhaoZhou,etal. Secretsofrlhfinlargelanguagemodelsparti: Ppo. arXivpreprint
arXiv:2307.04964,2023.
12A ImplementationDetails
ThecoderepositoryweutilizeisTRL8.Ourexperimentationemploys2AMDEPYC7773XCPUs
and 8 NVIDIA A6000 GPUs (48GB each). Leveraging a single GPU, CORY can achieve full-
precisionRLfine-tuningofGPT2-XLontheIMDBReviewdatasetwithin12hours. With4GPUs,
CORYcanaccomplishtheRLfine-tuningofa4-bitquantizedLlama-2-7b-chatmodelonGSM8K
within4hours.
A.1 Hyperparameters
Thehyperparametersettingsforfine-tuningGPT2followedthedefaultconfigurationinTRLfor
theIMDBdataset,whilethehyperparametersettingofLlama-2primarilyadheredtotheguidelines
providedbyStackLlama. Toensureafaircomparison,allhyperparameterswerecarefullyselectedto
balancethestabilityandperformanceofPPO.Agridsearchwasconductedoverαandη,withthesets
α1e-6,1e-5,1e-4andη1e-3,1e-2,1e-1,0.2,0.3,respectively,toidentifythehyperparameterthat
yieldedthemoststabletrainingforPPO.GivenCORY’srobustnesstohyperparameters(AppendixE),
mostPPOhyperparameters,exceptforthelearningrateα,wereapplieddirectlytoCORY.Forthe
GSM8Kdataset,IntheGSM8Kdataset,weadjustedthelearningrateαforCORY.
Table1: HyperparametersinIMDBReview
Hyperparameter PPO-GPT-2-l PPO-GPT-2-xl CORY
LearningRate(α) 1.41e-5 1.41e-5 1.41e-5
Epochs 1 1 1
PPOEpoch 4 4 4
BatchSize 256 256 256
MiniBatchSize 256 256 256
GradientAccumulationSteps 1 1 1
Iterations 100 100 100
InitialKLCoefficient(η) 0.3 0.3 0.3
EarlyStopping False False False
Discount(γ) 1 1 1
GAE(λ) 0.95 0.95 0.95
GradientClipRange 0.2 0.2 0.2
ValueClipRange 0.2 0.2 0.2
ValueLossCoefficient(β) 0.1 0.1 0.1
Periodofroleexchange(T ) - - 5iterations
REx
Table2: HyperparametersinGSM8K
Hyperparameter PPO PPO-13b CORY
LearningRate(α) 1e-5 1e-5 1e-4
Epochs 1 1 1
Batchsize 32 32 32
MiniBatchSize 2 2 2
GradientAccumulationSteps 16 16 16
Iterations 100 100 100
InitialKLCoefficient(η) 0.01 0.01 0.01
EarlyStopping False False False
Discount(γ) 1 1 1
GAE(λ) 0.95 0.95 0.95
GradientClipRange 0.2 0.2 0.2
ValueClipRange 0.2 0.2 0.2
ValueLossCoefficient(β) 0.1 0.1 0.1
Periodofroleexchange(T ) - - 5iterations
REx
8https://github.com/huggingface/trl
13A.2 PromptDetails
IMDBReview. ThepromptsusedinIMDBReviewareasfollows. ForPPOorCORY’spioneer,
sincethisisasentencecompletiontask,insteadofusingaprompttemplate,wedirectlyinputthefirst
fewwordsinthereview(brown).
Anotherfun,witty,frothy
ForCORY’sobserver,weusepioneer’sresponse(blue)tocompletethesentenceasareferencefor
observer, andretypethefirstfewwordsofthecommentattheendofthepromptforobserverto
complete.
Icanmakethissentence‘Anotherfun,witty,frothycutdifferentfromtheusual.’ more
positive: Anotherfun,witty,frothy
GSM8K.ThepromptsusedinGSM8Kareasfollows. ForPPOorCORY’spioneer,weprovidea
examplequestionandanswer. Thisisfollowedbyaquestionfromthedataset(brown). Thenthe
promptendswith‘Answer:’ toguidetheLLMtoanswer.
Question: Shawnhasfivetoys. ForChristmas,hegottwotoyseachfromhismomand
dad. Howmanytoysdoeshehavenow?
Answer: Shawnstartedwith5toys. Ifhegot2toyseachfromhismomanddad,thenthat
is4moretoys. 5+4=9.
Question:Thecivicleaguewashostingapancakebreakfastfundraiser.Astackofpancakes
was$4.00andyoucouldaddbaconfor$2.00. Theysold60stacksofpancakesand90
slicesofbacon. Howmuchdidtheyraise?
Answer:
ForCORY’sobserver,thequestionisfollowedby‘Reference’(blue),whichisthepioneer’sresponse.
Finally,italsoendswith‘Answer’toguidethemodeltoanswer.
Question: Shawnhasfivetoys. ForChristmas,hegottwotoyseachfromhismomand
dad. Howmanytoysdoeshehavenow?
Answer: Shawnstartedwith5toys. Ifhegot2toyseachfromhismomanddad,thenthat
is4moretoys. 5+4=9.
Question:Thecivicleaguewashostingapancakebreakfastfundraiser.Astackofpancakes
was$4.00andyoucouldaddbaconfor$2.00. Theysold60stacksofpancakesand90
slicesofbacon. Howmuchdidtheyraise?
Reference: TofindouthowmuchtheCivicLeagueraised,weneedtomultiplythenumber
ofstacksofpancakesbythecostofeachstack. So,60x$4=$240. Then,wemultiplythe
numberofslicesofbaconbythecostofeachslice. So,90x$2=$180. Therefore,the
CivicLeagueraisedatotalof$240+$180=$420.
Answer:
14B Token-LevelPolicyUpdateofCORY
WefirstderivetheformulaofQ-functionwhenfine-tuningLLMwithPPO.Thetoken-levelreward
functionrˆisgiveninEquation2.
(cid:34)N−i (cid:35)
(cid:88)
Q (s ,w )=E γkrˆ(s ,w )
π i i wi+1,...,wN∼π i+k i+k
k=0
(cid:34)N−i (cid:35) (cid:34)N−i (cid:35)
(cid:88) (cid:88)
=E γkr(s ,w ) −ηE γkKL[π(·|s ),π (·|s )]
wi+1,...,wN∼π i+k i+k wi+1,...,wN∼π i+k 0 i+k
k=0 k=0
(cid:34)N−i (cid:35)
=E (cid:2) γN−ir(s ,a)(cid:3) −ηE (cid:88) γkKL[π(·|s ),π (·|s )]
wi+1,...,wN∼π 0 wi+1,...,wN∼π i+k 0 i+k
k=0
(cid:34) N−i (cid:35)
(cid:88)
=E γN−ir(s ,a)−η γkKL[π(·|s ),π (·|s )] .
wi+1,...,wN∼π 0 i+k 0 i+k
k=0
(7)
For CORY, pioneer and observer share the same task reward r , but their Q-functions have
CORY
slightly different forms due to their different inputs. For simplicity, we define a uniform state
s˜ ≜(s ,a )fortheobserver,ands˜ ≜s forthepioneer. Then,denotingtheparameterizedpolicy
0 0 1 0 0
asπ ,theQ-functionsforthemcanbeexpressedinanuniformway.
θ
(cid:34) N−i (cid:35)
(cid:88)
Q (s˜,w )=E γN−ir (s ,a ,a )−η γkKL[π (·|s˜ ),π (·|s˜ )] .
πθ i i wi+1,...,wN∼πθ CORY 0 1 2 θ i+k 0 i+k
k=0
(8)
Similarly,CORY’suniformstatevaluefunctioncanbeexpressedas
(cid:88)
V (s˜)= π (w |s˜)Q (s˜,w ). (9)
πθ i θ i i πθ i i
wi∈V
In practice, both the pioneer and the observer in CORY are optimised using PPO independently.
Duringthetrainingphase,avalueheadisattachedtothelasthiddenlayerofthepolicynetworkto
predictthecurrentstatevalue. Thelossfunctionis:
LV =E [V (s˜)−V (s˜)]2, (10)
πθ πθ πθ i ϕ i
whereV (s˜)isthepredictedstatevalue,ϕrepresentstheparametersofthecorrespondingvalue
ϕ i
network. Forpolicyloss,theoptimisationobjectivewithclipisused.
(cid:20) (cid:18) (cid:19)(cid:21)
π (w |s˜) π (w |s˜)
LP =E min θ i i Aˆ (s˜,w ),clip( θ i i ,1−ϵ,1+ϵ)Aˆ (s˜,w ) ,
πθ π π (w |s˜) πθ i i π (w |s˜) πθ i i
θold i i θold i i
(11)
whereπ istheolderpolicythatcollectsdata. Theimportanceratio πθ(wi|s˜i) isusedtoestimate
θold πθold(wi|s˜i)
Aˆ underπ ondatacollectedviaπ . Itreflectshowmuchthecurrentpolicydeviatesrelativeto
πθ θ θold
theolderpolicy. Aˆ istheadvantagefunction,givenδ =rˆ(s˜,w )+γV (s˜ )−V (s˜),
πθ i i i ϕ i+1 ϕ i
Aˆ (s˜,w )=δ +(γλ)δ +···+(γλ)N−i+1δ . (12)
πθ i i i i+1 N−1
Ultimately,withavaluelosscoefficientβ,thepioneerandtheobserverarefine-tunedbymaximising
thefollowingobjective
L(θ,ϕ)=LP −βLV . (13)
πθ πθ
Ideally,aftertheoptimisation,theoptimaltoken-levelpolicyπ∗isobtained,whichinturnnaturally
leadstotheoptimalsentence-levelpolicy.
N
(cid:89)
π∗(a|s˜ )= π∗(w |s˜ ,w ). (14)
0 i 0 1:i−1
i=1
15C AlgorithmDetails
C.1 AlgorithmofCORY
Algorithm1CoevolvingwiththeOtherYou
Input: Pre-trainedLLMπ ,taskrewardmodelr,querydatasetD ,periodofroleexchangeT .
0 Q REx
Output: Fine-tunedLLMsπ andπ .
θ1 θ2
Initialization: Duplicateπ intoapioneerπ (·|·;θ )andanobserverπ (·|·,·;θ ),initializethe
0 pio 1 obs 2
pioneerbufferD ←∅andtheobserverbufferD ←∅.
pio obs
1: Setk ←0.
2: foreachiterationdo
3: SetD pio ←∅andD obs ←∅.
4: SampleataskquerybatchB QfromD Q.
5: foreachs 0inB Qdo
6: a 1 ∼π pio(·|s 0;θ 1).
7: r pio ←r(s 0,a 1).
8: a 2 ∼π obs(·|s 0,a 1;θ 2).
9: r obs ←r(s 0,a 1).
10: r CORY ←r pio+r obs.
11: Sets˜ 0 ←s 0andupdatememoryD pio ←D pio∪{(s˜ 0,a 1,r CORY)}.
12: Sets˜ 0 ←(s 0,a 1)andupdatememoryD obs ←D obs∪{(s˜ 0,a 2,r CORY)}.
13: endfor
14: Updateθ 1throughAlgorithmC.2onD pio.
15: Updateθ 2throughAlgorithmC.2onD obs.
16: if(k+1)%T REx =0 then
17: Setθ 1new ←θ 1andθ 2new ←θ 2.
18: θ 2 ←θ 1new.
19: θ 1 ←θ 2new.
20: endif
21: k ←k+1.
22: endfor
C.2 Token-LevelPolicyUpdate
Algorithm2PPO-basedToken-LevelPolicyUpdate
Input: TargetLLMπ ,referenceLLMπ ,sentence-leveldatabufferD,maxtokenlengthofaction
θ 0
N,learningrateα,KLcoefficientη.
Output: TheupdatedparametersofthetargetLLMθ.
Initialization: InitializethevaluenetworkV andthetoken-leveldatabufferDT ←∅.
ϕ
1: for(s˜ 0,a,r CORY)inDdo
2: DT ←∅.
3: fori=1,2,··· ,N do
4: r KL ←KL(π θ(·|s˜ 0,a[1:i−1]),π 0(·|s˜ 0,a[1:i−1])).
5: s i ←(s˜ 0,a[1:i−1]).
6: a i ←a[i].
7: s i+1 ←(s˜ 0,a[1:i]).
8: ifi<N then
9: r i ←−η·r KL.
10: else
11: r i ←r CORY−η·r KL.
12: endif
13: DT ←DT ∪{(s i,a i,r i,s i+1)}.
14: endfor
15: ComputeadvantageestimateAˆ πθ viaGAEonDT. (Equation12)
16: θ ←θ+α·∇ θL(θ,ϕ). (Equation13)
17: ϕ←ϕ+α·∇ ϕL(θ,ϕ). (Equation13)
18: endfor
16D QualitativeAnalysisofExperimentResults.
WecompareGPT2-Largemodelsfine-tunedwithPPOandCORYonIMDBReviewdataset,along
withtheoriginalmodel(Table3). Theinputreviewsnippetconsistsofthefirstfewwordsofamovie
review. ThegoalofLLMsistocompletethesentenceinapositivedirection. Comparingresults
beforeandafterfine-tuning,sentencesareoftenincompleteandoccasionallycontaingrammatical
errorsduetothelimitationsofGPT2-Large. However,thisdoesnotaffectourhorizontalcomparison
onthesamebaseline. Itisevidentthatthesentencesgeneratedbythefine-tunedmodelsareindeed
more positive. Comparing PPO and CORY, we find that PPO experiences distribution collapse.
WhileitstaskrewardiscomparabletoCORY,itsKLdivergenceissignificantlylarger(Figure3).
SentencesgeneratedbyCORYaremorepositive. Althoughthereareoccasionalgrammaticalerrors,
theyaresimilartothoseinthepre-trainedmodel,indicatingthatCORYeffectivelyavoidsdistribution
collapse.
WealsocomparedtheLlama-2-7B-chatmodelsfine-tunedwithPPOandCORYonGSM8K.Dueto
PPO’ssensitivitytoparameters,whichresultsineitherstabletrainingordistributioncollapse,we
dividedthecomparisonintotwotables.WhenPPOtrainsstably(Table4),thequalityofthegenerated
answerissimilartoCORY,thoughslightlylessaccurate.WhenPPOexperiencesdistributioncollapse
(Table5),ittendstogenerateparticularlylongoutputsuntilreachingthemaximumtokenlimit. This
is because the probability of the end-of-sentence token </s> in the token-level policy decreases
significantlycomparedtoitsinitialvalue,preventingsentencecompletion. Duetothedistribution
collapse,thein-contextlearningabilityofPPOisalsoimpaired. ItgeneratesanotherQuestionafter
generatinganAnswer. Incontrast,CORY’sperformanceismuchmorestable.
Table3: ExamplesofIMDBReview. GPT2-Largeisfine-tunedwithPPOandCORYrespectively.
Reviewsnippet ThisisoneofJackies
GPT2-Large ’lastregretsaboutrunning. (
PPO marvelousmarveloussplendidmarvelousmarvelousmagnificentawesome
marvelousstunningexcellentphenomenalspectacularbeautiful
CORY Cruisemoviesareworldfamousworldwidehas
Reviewsnippet Ifindithardtobelieve
GPT2-Large thatsomeonecouldhavesurvivedthreesets
PPO amazing marvelous marvelous marvelous marvelous spectacular terrific
beautifulmagnificentmarvelousfantasticwonderfulfabulous
CORY thatwe’veallhadwonderfulamazing
Reviewsnippet Thismoviewashighlyentertaining. The
GPT2-Large characterswerenotonlygreat,guests
PPO unacceptableTHANK!! wonderfulABEST
CORY filmwashighlyacclaimedcaptivatingenjoy
17Table4: ExamplesofGSM8KwhenPPOfine-tuningisstable.
Question Georgewantstoborrow$100fromaloanshark. Thefinancefeestartsat5%
anddoubleseveryweek. IfGeorgeplanstoborrowfor2weeks,howmuchin
feeswillhehavetopay?
PPO The finance fee starts at 5%, so after one week it would be 10%. After two
weeks,thefeewouldbe20%. So,Georgewouldhavetopay(100x10%)+(100
x20%)=200infees.
CORY Atfirst,thefinancefeeis5%. SoGeorgewillpay$5infees. Then,thenext
week, the fee will be doubled to $10. George will pay another $10 in fees.
Therefore,Georgewillpayatotalof$15infees.
Question Lorinhas4blackmarbles. Jimmyhas22yellowmarbles. Alexhadtwiceas
manyblackmarblesasLorinandonehalfasmanyyellowmarblesasJimmy.
HowmanymarblesdoesAlexhave?
PPO Alexhas8blackmarbles(twiceasmanyasLorin). SinceAlexhashalfasmany
yellowmarblesasJimmy,Alexhas11yellowmarbles(halfof22). SoAlexhas
atotalof8+11=19marbles.
CORY Alexhas8blackmarbles(twiceasmanyasLorin). SinceAlexalsohasonehalf
asmanyyellowmarblesasJimmy,Alexhas11yellowmarbles(halfof22). So,
intotal,Alexhas8+11=19marbles.
Question Billistryingtocontrolthepestsinhisgarden. Eachspiderheintroduceseats
7bugs,andeachtimehespraysthegardenhereducesthetotalbugpopulation
to80%ofwhatitwaspreviously. Ifthegardenhas400bugstostart,andBill
spraysonceandintroduces12spiders,howmanybugsareleft?
PPO ThenumberofbugsinthegardenbeforeBillsprayedis400. WhenBillsprays,
thetotalbugpopulationisreducedto80%ofitsoriginalvalue,whichmeans
thereare320bugsleft(400x80%). Then,Billintroduces12spiders,whicheat
7bugseach,sothereare7+84=91bugsleftinthegarden.
CORY First,let’sfindouthowmanybugsareleftafterBillspraysbymultiplyingthe
previousnumberofbugsby80%. So,400x0.8=320bugs. Now,let’sfindout
howmanybugseachspidercaneat. Eachspidercaneat7bugs,so12spiders
caneat7x12=84bugs. SinceBillintroduced12spiders,therearenow320-
84=236bugsleftinthegarden.
Question Mirajogseverymorning. Shejogs5milesperhour. Ifshejogsfor2hoursevery
morning,howmanymilescanshejogforfivedays?
PPO IfMirajogsatarateof5milesperhour,andshejogsfor2hourseachday,then
shewillhavejoggedatotaldistanceof10miles(2hoursx5miles/hour). Ifshe
doesthisfor5days,thenshewillhavejoggedatotaldistanceof50miles(10
milesx5days).
CORY Mirajogsatarateof5milesperhour,soifshejogsfor2hours,shewillhave
jogged10miles(2hoursx5miles/hour). Ifshedoesthis5daysinarow,she
willhavejoggedatotalof50miles(10miles/dayx5days).
18Table5: ExamplesofGSM8KwhenPPOleadstodistributioncollapse.
Question Adambought3kilogramsofnutsand2.5kilogramsofdriedfruitsatastore.
Onekilogramofnutscosts$12andonekilogramofdriedfruitcosts$8. How
muchdidhispurchasescost?
PPO First, we need to find out how many kilograms of each type of food Adam
bought. We can do this by dividing the amount of each type of food by its
priceperkilogram. 3/$12=x2.5/$8=y. Thenwecanfindthetotalcostby
multiplyingthenumberofkilogramsbytheirprices: xx$12+yx$8.
Question: Sarahhad30hoursofvacationtimeleftbeforeshecouldtakeanother
dayoff. Shewantedtoknowhowmanydaysshecouldgoonatripforwithout
usingupallhervacationtime. CanyouhelpSarahfigureouthowmanydays
shecangoonatrip?
Answer: To find out how many days Sarah can go on a trip, we first need to
dividehertotalvacationtime(30)bythenumberofdaysshewantstogoonthe
trip(let’scallitx). 30/x=5. NowwecanseehowmanyhoursSarahwillhave
leftafterhertripbymultiplyingthenumberofdaysbythenumberofhoursin
eachday: xx24(sincethereare24hoursineachday
CORY Sara’ssisterhad4smallnotebooksinherclosetatfirst. Thensheordered6more
notebooks,sonowshehas4+6=10notebooks. However,shelost2notebooks,
sonowshehas10-2=8notebooks.
Question Belindacanthrowaballataspeedof20feet/second.Iftheballfliesfor8seconds
beforehittingtheground,andBelinda’sbordercolliecanrun5feet/second,how
manysecondswillittakethebordercollietocatchuptotheball?
PPO To find out how long it will take the border collie to catch up to the ball, we
needtousemultiplicationanddivision. Weknowthattheballtravels20feetper
second,sofirstwewilldividebythespeedofthedogtofindouthowfarthedog
needstoruninonesecond. 20feet/5feet/second=4seconds.
Nowwewillfindouthowlongthistakesintotalbymultiplyingthenumberof
secondsbythetimetheballisintheair. 8secondsx4seconds=32seconds.
Therefore,itwilltakethebordercollie32secondstocatchuptotheball.
Question: Sarahcanpaintaroomin6hours. Ifsheworksonitfor4hoursevery
dayforaweek,howmanyhourswillittakehertopaintanothersimilar-sized
room?
Answer: TofindouthowmanyhoursitwilltakeSarahtopaintanotherroom,
weneedtousetheinformationwehaveaboutthefirstroom. Let’scallthesize
ofthefirstroom“x”. Wearetoldthatshecanpaint“x”in6hours. Tofindout
howmanyhoursitwill
CORY Weknowthattheballtravelsataspeedof20feet/second,soitwillcover20feet
in1second. Theballfliesfor8secondsbeforehittingtheground,soitcovers8
x20=160feet.
Thebordercollierunsataspeedof5feet/second, soitwillcover5feetin1
second. Tofindouthowlongittakesthebordercollietocatchuptotheball,we
dividethedistancetheballtraveledbythespeedofthebordercollie: 160feet/
5feet/second=32seconds.
19E SupplementaryExperimentsonRobustness
0.20 PPO-7B
CORY-LLM1-7B
0.15 15 CORY-LLM2-7B 0.15
0.10 P CP OO R- Y7 -B LLM1-7B 10 0.10 P CP OO R- Y7 -B LLM1-7B
CORY-LLM2-7B CORY-LLM2-7B
0.05 5 0.05
0.00 0
0.00
0 20 4 It0 eratio6 n0 80 100 0 20 4 It0 eratio6 n0 80 100 0 20 40 Iteratio6 n0 80 100
(a) Taskreward (b) KLdivergence (c) Combinedreward
Figure7: TrainingcurvesunderobjectiverewardsonGSM8K.Thefine-tunedmodelisLlama-2-7b-
chat. Learningrateαissetto1e-4.
PPO-7B
0.200 0.20 CORY-LLM1-7B
CORY-LLM2-7B
0.175 1.0 0.18
PPO-7B
0.150 CORY-LLM1-7B 0.16
0.5 CORY-LLM2-7B 0.14
0.125 PPO-7B
CORY-LLM1-7B 0.12
0.100 CORY-LLM2-7B 0.0 0.10
0 25 Iter5 a0 tion 75 100 0 20 4 It0 eratio6 n0 80 100 0 20 40 Iteratio6 n0 80 100
(a) Taskreward (b) KLdivergence (c) Combinedreward
Figure8: TrainingcurvesunderobjectiverewardsonGSM8K.Thefine-tunedmodelisLlama-2-7b-
chat. Learningrateαissetto1e-5.
20
0.30
0.30
15 0.25
0.25
PPO-13B
10 CORY-LLM1-13B 0.20
0.20 CORY-LLM2-13B
0.15
0.15 PPO-13B 5
CORY-LLM1-13B 0.10 PPO-13B
0.10 CORY-LLM2-13B 0 C CO OR RY Y- -L LL LM M1 2- -1 13 3B B
0 20 4 It0 eratio6 n0 80 100 0 20 4 It0 eratio6 n0 80 100 0.05 0 20 40 Iteratio6 n0 80 100
(a) Taskreward (b) KLdivergence (c) Combinedreward
Figure9: TrainingcurvesunderobjectiverewardsonGSM8K.Thefine-tunedmodelisLlama-2-13b-
chat. Learningrateαissetto1e-4.
0.30 PPO-13B 0.28 PPO-13B
4 CORY-LLM1-13B 0.26 C CO OR RY Y- -L LL LM M1 2- -1 13 3B B
CORY-LLM2-13B
0.25 PPO-13B 3 0.24
C CO OR RY Y- -L LL LM M1 2- -1 13 3B B 2 0.22
0.20 1 0.20
0.18
0
0 20 4 It0 eratio6 n0 80 100 0 20 4 It0 eratio6 n0 80 100 0.16 0 20 40 Iteratio6 n0 80 100
(a) Taskreward (b) KLdivergence (c) Combinedreward
Figure10: TrainingcurvesunderobjectiverewardsonGSM8K.Thefine-tunedmodelisLlama-2-
13b-chat. Learningrateαissetto1e-5.
20
drawer
ksaT
drawer
ksaT
drawer
ksaT
drawer
ksaT
LK
LK
LK
LK
drawer
denibmoC
drawer
denibmoC
drawer
denibmoC
drawer
denibmoCWeconductrobustnessexperimentsontheGSM8Kdataset,focusingontheimpactoflearningrate.
InFigures7and8,wesetthelearningratesto1e-4and1e-5,respectively,usingPPOandCORY
forfine-tuningtheLlama-2-7b-chatmodel,whilekeepingallotherhyperparametersconsistentwith
thoseinAppendixA.1. OurfindingsindicatethatCORYexhibitsrobustness,maintainingstable
trainingacrossdifferentlearningrates. ItsKLdivergenceandtaskrewardconvergearoundthe10th
iteration,withtheKLdivergenceremainingatarelativelylowvalue. Incontrast,withalearning
rateof1e-4,PPOleadstodistributioncollapse. PPOachievesstabletrainingandrelativelygood
performanceonlywithalearningrateof1e-5,butitsKLdivergenceshowsanacceleratingupward
trendevenafter100iterations,indicatinginstabilityandtheriskofdistributioncollapse.
InFigures9and10,weagainsetthelearningratesto1e-4and1e-5,respectively,usingPPOand
CORYtofine-tunetheLlama-2-13b-chatmodel,withallotherhyperparametersconsistentwiththose
inAppendixA.1. CORYensuresstabilityatbothlearningrates,achievinggoodtaskrewardand
lowKLdivergencewithalearningrateof1e-4. Althoughthetaskrewarddoesnotimprovewith
a learning rate of 1e-5, both the KL divergence and task reward curves stabilize, indicating that
CORYavoidsdistributioncollapseevenunderinappropriatehyperparametersettings. Incontrast,
PPOrapidlyleadstodistributioncollapsewithalearningrateof1e-4. Withalearningrateof1e-5,
thetaskrewardincreasessteadily,buttheKLdivergencecurveshowsanacceleratingupwardtrend,
indicatingtheriskofdistributioncollapse.
TheaboveanalysisdemonstratesthesuperiorrobustnessandstabilityofCORY.Furthermore,com-
paringtheKLdivergenceandtaskrewardcurvesacrossallfiguresrevealsthatPPOstrugglesto
balancetaskrewardandKLdivergence,whereasCORYconsistentlymaintainsabalancebetween
thetwo,asdiscussedinSection3.2.
F Limitations
Althoughourmethodshowspromisingresultsintrainingrobustness,policyoptimality,andavoiding
distributioncollapse,itrequiresduplicatingtheLLMintotwocopies,doublingthecomputational
resourcesneeded. Thisissuecouldbealleviatedthroughtechnicalsolutionslikeparametershar-
ing. Furthermore,asMoore’sLawpredictsthegrowingaffordabilityofcomputationalpower,this
limitationmaybecomelessimportantovertime.
G BroaderImpacts
AbetterRLfine-tuningmethodcanimprovetheperformanceofLLMsinspecializedtaskssuchas
robotcontrolandcodegeneration. Assumeawell-constructedrewardfunction,higherrewardsdo
leadtobetterpolicies. Thereexistsanoptimalpolicythatmaximizesthisfunction. IfRLfine-tuning
issufficientlyadvanced,itcouldtheoreticallyimprovethecapabilitiesofanLLMinaspecifictask
beyondthehumanlevel,oncetherewardexceedsacertainthreshold.
Amajorconcernisthepotentialofabuse,includingthegenerationofmisleadingandharmfulcontent.
Toaddressthisissue,valuealignmenttechniquescouldbeimplementedtoensurethatthemodel’s
goalsareinlinewithhumanvalues. Inaddition,implementingmonitoringmechanisms,suchas
real-timedetectionofLLM-generatedcontent,couldbebeneficial.
21NeurIPSPaperChecklist
1. Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper’scontributionsandscope?
Answer: [Yes]
Justification: Both the abstract and introduction clearly state that our main contribution
and scope: extending the RL fine-tuning of LLMs into a sequential cooperative MARL
framework.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
madeinthepaper.
• Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe
contributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor
NAanswertothisquestionwillnotbeperceivedwellbythereviewers.
• Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow
muchtheresultscanbeexpectedtogeneralizetoothersettings.
• Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals
arenotattainedbythepaper.
2. Limitations
Question: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?
Answer: [Yes]
Justification: WediscussthelimitationsinAppendixF.
Guidelines:
• TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat
thepaperhaslimitations,butthosearenotdiscussedinthepaper.
• Theauthorsareencouragedtocreateaseparate"Limitations"sectionintheirpaper.
• Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto
violationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,
modelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors
shouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe
implicationswouldbe.
• Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas
onlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften
dependonimplicitassumptions,whichshouldbearticulated.
• Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.
Forexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution
isloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe
usedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle
technicaljargon.
• Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms
andhowtheyscalewithdatasetsize.
• If applicable, the authors should discuss possible limitations of their approach to
addressproblemsofprivacyandfairness.
• Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby
reviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover
limitationsthataren’tacknowledgedinthepaper. Theauthorsshouldusetheirbest
judgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-
tantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers
willbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.
3. TheoryAssumptionsandProofs
Question: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand
acomplete(andcorrect)proof?
22Answer: [NA]
Justification: Thispaperdoesnotincludetheoreticalresults.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.
• Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-
referenced.
• Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.
• Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif
theyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort
proofsketchtoprovideintuition.
• Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
byformalproofsprovidedinappendixorsupplementalmaterial.
• TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.
4. ExperimentalResultReproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-
perimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
ofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?
Answer: [Yes]
Justification: Thepaperincludesdetaileddescriptionsoftheexperimentalsetup,algorithms,
modelarchitecturesinSection4.1andSection4.2. Hyperparametersusedaredetailedin
AppendixA.1. PseudocodesaredetailedinAppendixC.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived
well by the reviewers: Making the paper reproducible is important, regardless of
whetherthecodeanddataareprovidedornot.
• Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken
tomaketheirresultsreproducibleorverifiable.
• Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.
Forexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully
mightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay
benecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame
dataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften
onegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed
instructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase
ofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare
appropriatetotheresearchperformed.
• WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-
sionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe
natureofthecontribution. Forexample
(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow
toreproducethatalgorithm.
(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe
thearchitectureclearlyandfully.
(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould
eitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce
themodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct
thedataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.
Inthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin
someway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers
tohavesomepathtoreproducingorverifyingtheresults.
5. Openaccesstodataandcode
23Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-
tionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental
material?
Answer: [Yes]
Justification:Wehaveprovidedopenaccesstoboththedataandcodenecessarytoreproduce
ourmainexperimentalresults.
Guidelines:
• TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy)formoredetails.
• Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe
possible,so“No”isanacceptableanswer. Paperscannotberejectedsimplyfornot
includingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source
benchmark).
• Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto
reproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:
//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.
• Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow
toaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.
• Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew
proposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they
shouldstatewhichonesareomittedfromthescriptandwhy.
• Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized
versions(ifapplicable).
• Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe
paper)isrecommended,butincludingURLstodataandcodeispermitted.
6. ExperimentalSetting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: AllexperimentalsettingsareclearlystatedinSection4. Allhyperparameters
aredetailedinAppendixA.1.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
thatisnecessarytoappreciatetheresultsandmakesenseofthem.
• Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental
material.
7. ExperimentStatisticalSignificance
Question:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
informationaboutthestatisticalsignificanceoftheexperiments?
Answer: [Yes]
Justification: AlltrainingcurvesinSection4areplottedwiththemean±stdacrossthree
randomseeds.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confi-
denceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport
themainclaimsofthepaper.
24• Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for
example,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall
runwithgivenexperimentalconditions).
• Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,
calltoalibraryfunction,bootstrap,etc.)
• Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).
• Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror
ofthemean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
ofNormalityoferrorsisnotverified.
• Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative
errorrates).
• Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow
theywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.
8. ExperimentsComputeResources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-
puterresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce
theexperiments?
Answer: [Yes]
Justification: AllnecessaryinformationareprovidedinAppendixA.1.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,
orcloudprovider,includingrelevantmemoryandstorage.
• Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimentalrunsaswellasestimatethetotalcompute.
• Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute
thantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat
didn’tmakeitintothepaper).
9. CodeOfEthics
Question: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe
NeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: OurresearchfullyadherestotheNeurIPSCodeofEthics.
Guidelines:
• TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.
• IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea
deviationfromtheCodeofEthics.
• Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-
erationduetolawsorregulationsintheirjurisdiction).
10. BroaderImpacts
Question: Does the paper discuss both potential positive societal impacts and negative
societalimpactsoftheworkperformed?
Answer: [Yes]
Justification:Ourpaperthoroughlydiscussesboththepotentialpositiveandnegativesocietal
impactsofourworkinAppendixG.
Guidelines:
• TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.
25• IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal
impactorwhythepaperdoesnotaddresssocietalimpact.
• Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses
(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups),privacyconsiderations,andsecurityconsiderations.
• Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
anynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate
topointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto
generatedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
modelsthatgenerateDeepfakesfaster.
• Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis
being used as intended and functioning correctly, harms that could arise when the
technologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing
from(intentionalorunintentional)misuseofthetechnology.
• Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom
feedbackovertime,improvingtheefficiencyandaccessibilityofML).
11. Safeguards
Question: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible
releaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,
imagegenerators,orscrapeddatasets)?
Answer: [NA]
Justification: Ourpaperdoesnotinvolvethereleaseofdataormodels.
Guidelines:
• TheanswerNAmeansthatthepaperposesnosuchrisks.
• Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safetyfilters.
• DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors
shoulddescribehowtheyavoidedreleasingunsafeimages.
• Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo
notrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest
faitheffort.
12. Licensesforexistingassets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properlyrespected?
Answer: [Yes]
Justification: Allexistingassetsusedinthepaper,includingcode,data,andmodels,have
beenproperlycreditedtotheiroriginalcreatorsinSection4.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotuseexistingassets.
• Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.
• Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
• Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.
• Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof
serviceofthatsourceshouldbeprovided.
26• If assets are released, the license, copyright information, and terms of use in the
packageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasets
hascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe
licenseofadataset.
• Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof
thederivedasset(ifithaschanged)shouldbeprovided.
• Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto
theasset’screators.
13. NewAssets
Question:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
providedalongsidetheassets?
Answer: [NA]
Justification: Thepaperdoesnotreleaseanynewassets.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotreleasenewassets.
• Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir
submissions via structured templates. This includes details about training, license,
limitations,etc.
• Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
assetisused.
• Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither
createananonymizedURLorincludeananonymizedzipfile.
14. CrowdsourcingandResearchwithHumanSubjects
Question: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
wellasdetailsaboutcompensation(ifany)?
Answer: [NA]
Justification:Thepaperdoesnotinvolvecrowdsourcingexperimentsorresearchwithhuman
subjects.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-
tionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe
includedinthemainpaper.
• AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,
orotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata
collector.
15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
suchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution)wereobtained?
Answer: [NA]
Justification: Thepaperdoesnotinvolveresearchwithhumansubjects.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
shouldclearlystatethisinthepaper.
27• Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
andlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe
guidelinesfortheirinstitution.
• Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if
applicable),suchastheinstitutionconductingthereview.
28